{
    "hands_on_practices": [
        {
            "introduction": "理解可解释人工智能 (XAI) 方法的理论基础至关重要。本练习  旨在深入探讨 SHAP (SHapley Additive exPlanations)，这是一种最流行的归因方法。通过为一个简单的线性模型推导 SHAP 值，您将深刻理解这些值如何将模型参数、特征值和数据分布联系起来，从而建立对特征贡献的核心直觉。",
            "id": "4220831",
            "problem": "一个信息物理系统 (CPS) 的数字孪生 (DT) 使用线性代理模型，从一个标准化传感器特征向量中预测一个标量性能指标。令特征表示为 $X = (X_{1},\\dots,X_{d})$，模型为 $f(x) = w^{\\top}x = \\sum_{j=1}^{d} w_{j} x_{j}$，且不含截距。该 DT 用于 $X$ 的背景数据分布（在可解释人工智能中用于定义缺失特征语义）具有各分量相互独立且标准化的特性，即对于所有 $j \\in \\{1,\\dots,d\\}$，$\\mathbb{E}[X_{j}] = 0$，并且 $X_{1},\\dots,X_{d}$ 是独立的。解释方法遵循 SHapley Additive exPlanations (SHAP) 框架：对于任意子集 $S \\subseteq \\{1,\\dots,d\\}$，将联盟值定义为 $v(S) = \\mathbb{E}\\!\\left[ f(X) \\mid X_{S} = x_{S} \\right]$，并将索引 $i$ 的特征归因定义为此博弈中的夏普利值，即：\n$$\n\\phi_{i} \\;=\\; \\sum_{S \\subseteq N \\setminus \\{i\\}} \\frac{|S|!\\,(d-|S|-1)!}{d!} \\left( v(S \\cup \\{i\\}) - v(S) \\right),\n$$\n其中 $N = \\{1,\\dots,d\\}$。\n\n从这些定义和 $X$ 的给定统计特性出发，推导出一个关于 $w_{i}$ 和查询点坐标 $x_{i}$ 的 $\\phi_{i}$ 的闭式解析表达式。您的最终答案必须是单一表达式。除上述声明外，不要假设任何其他属性。最终答案以无单位形式表示。无需四舍五入。",
            "solution": "该问题是有效的。这是一个基于可解释人工智能 (XAI) 和统计学的既定原则的良定数学推导。所有给定条件都是一致的，且目标明确。我们现在开始推导。\n\n目标是为一个线性模型 $f(x) = \\sum_{j=1}^{d} w_{j} x_{j}$ 推导出夏普利特征归因 $\\phi_i$ 的闭式表达式，其中特征向量 $X = (X_{1},\\dots,X_{d})$ 由均值为零的相互独立的随机变量组成，即对于所有 $j \\in \\{1,\\dots,d\\}$，$\\mathbb{E}[X_{j}] = 0$。\n\n归因 $\\phi_i$ 由夏普利值公式定义：\n$$\n\\phi_{i} \\;=\\; \\sum_{S \\subseteq N \\setminus \\{i\\}} \\frac{|S|!\\,(d-|S|-1)!}{d!} \\left( v(S \\cup \\{i\\}) - v(S) \\right)\n$$\n其中 $N = \\{1,\\dots,d\\}$，联盟值函数 $v(S)$ 由模型输出的条件期望给出：\n$$\nv(S) = \\mathbb{E}\\!\\left[ f(X) \\mid X_{S} = x_{S} \\right]\n$$\n这里，$X_S = x_S$ 表示索引在集合 $S$ 中的特征取查询点 $x$ 的特定值这一事件。\n\n我们的第一步是为联盟值函数 $v(S)$ 推导一个简化表达式。我们将线性模型 $f(X)$ 的定义代入 $v(S)$ 的表达式中：\n$$\nv(S) = \\mathbb{E}\\!\\left[ \\sum_{j=1}^{d} w_{j} X_{j} \\mid X_{S} = x_{S} \\right]\n$$\n根据期望算子的线性性质，我们可以将求和符号移到期望之外：\n$$\nv(S) = \\sum_{j=1}^{d} w_{j} \\mathbb{E}\\!\\left[ X_{j} \\mid X_{S} = x_{S} \\right]\n$$\n现在，我们必须为每个特征 $X_j$ 计算条件期望 $\\mathbb{E}\\!\\left[ X_{j} \\mid X_{S} = x_{S} \\right]$。我们根据索引 $j$ 是否在集合 $S$ 中分两种情况讨论。\n\n情况 1：$j \\in S$。\n如果索引 $j$ 在集合 $S$ 中，随机变量 $X_j$ 的值由条件 $X_S = x_S$ 固定。具体来说，$X_j$ 被固定为值 $x_j$。常数的期望是其本身，所以：\n$$\n\\mathbb{E}\\!\\left[ X_{j} \\mid X_{S} = x_{S} \\right] = x_{j} \\quad \\text{for } j \\in S\n$$\n\n情况 2：$j \\notin S$。\n如果索引 $j$ 不在集合 $S$ 中，我们必须考虑特征的统计特性。问题中说明特征 $X_1, \\dots, X_d$ 是相互独立的。这意味着知道集合 $S$ 中特征的值 ($X_S$) 并不能提供关于 $j \\notin S$ 的特征 $X_j$ 的值的任何信息。因此，条件期望等于无条件期望：\n$$\n\\mathbb{E}\\!\\left[ X_{j} \\mid X_{S} = x_{S} \\right] = \\mathbb{E}[X_{j}] \\quad \\text{for } j \\notin S\n$$\n问题还说明特征是标准化的，使得对于所有 $j$，$\\mathbb{E}[X_j] = 0$。因此：\n$$\n\\mathbb{E}\\!\\left[ X_{j} \\mid X_{S} = x_{S} \\right] = 0 \\quad \\text{for } j \\notin S\n$$\n\n结合这两种情况，我们可以通过将求和拆分为对 $S$ 中索引和不在 $S$ 中索引的求和来写出 $v(S)$ 的表达式：\n$$\nv(S) = \\sum_{j \\in S} w_{j} \\mathbb{E}\\!\\left[ X_{j} \\mid X_{S} = x_{S} \\right] + \\sum_{j \\notin S} w_{j} \\mathbb{E}\\!\\left[ X_{j} \\mid X_{S} = x_{S} \\right]\n$$\n$$\nv(S) = \\sum_{j \\in S} w_{j} x_{j} + \\sum_{j \\notin S} w_{j} (0) = \\sum_{j \\in S} w_{j} x_{j}\n$$\n这为联盟值函数提供了一个简单的闭式表达式。\n\n下一步是计算特征 $i$ 的边际贡献，即夏普利公式中的项 $v(S \\cup \\{i\\}) - v(S)$。求和遍及所有子集 $S \\subseteq N \\setminus \\{i\\}$，这意味着 $i \\notin S$。\n使用我们关于 $v(\\cdot)$ 的表达式：\n$$\nv(S \\cup \\{i\\}) = \\sum_{j \\in S \\cup \\{i\\}} w_{j} x_{j} = w_i x_i + \\sum_{j \\in S} w_{j} x_{j}\n$$\n并且，如前所述：\n$$\nv(S) = \\sum_{j \\in S} w_{j} x_{j}\n$$\n因此，差值为：\n$$\nv(S \\cup \\{i\\}) - v(S) = \\left(w_i x_i + \\sum_{j \\in S} w_{j} x_{j}\\right) - \\left(\\sum_{j \\in S} w_{j} x_{j}\\right) = w_i x_i\n$$\n值得注意的是，对于任何联盟 $S \\subseteq N \\setminus \\{i\\}$，特征 $i$ 的边际贡献是恒定的，等于 $w_i x_i$。\n\n现在我们将这个结果代回 $\\phi_i$ 的夏普利值公式中：\n$$\n\\phi_{i} = \\sum_{S \\subseteq N \\setminus \\{i\\}} \\frac{|S|!\\,(d-|S|-1)!}{d!} (w_i x_i)\n$$\n由于项 $w_i x_i$ 与求和变量 $S$ 无关，我们可以将其提取出来：\n$$\n\\phi_{i} = (w_i x_i) \\left( \\sum_{S \\subseteq N \\setminus \\{i\\}} \\frac{|S|!\\,(d-|S|-1)!}{d!} \\right)\n$$\n最后一步是计算夏普利权重的总和。我们用 $\\Sigma$ 表示这个和：\n$$\n\\Sigma = \\sum_{S \\subseteq N \\setminus \\{i\\}} \\frac{|S|!\\,(d-|S|-1)!}{d!}\n$$\n求和遍及 $N \\setminus \\{i\\}$ 的所有子集，这是一个包含 $d-1$ 个元素的集合。我们可以按子集 $S$ 的基数 $k = |S|$ 对其进行分组，其范围从 $k=0$ 到 $k=d-1$。对于任何给定的基数 $k$，满足 $|S|=k$ 的子集 $S \\subseteq N \\setminus \\{i\\}$ 的数量由二项式系数 $\\binom{d-1}{k}$ 给出。由于所有这些子集具有相同的基数，权重项 $\\frac{k!\\,(d-k-1)!}{d!}$ 对它们来说都是相同的。\n我们可以将和重写为：\n$$\n\\Sigma = \\sum_{k=0}^{d-1} \\binom{d-1}{k} \\frac{k!\\,(d-k-1)!}{d!}\n$$\n代入组合公式 $\\binom{d-1}{k} = \\frac{(d-1)!}{k!\\,(d-1-k)!}$：\n$$\n\\Sigma = \\sum_{k=0}^{d-1} \\frac{(d-1)!}{k!\\,(d-1-k)!} \\cdot \\frac{k!\\,(d-k-1)!}{d!}\n$$\n我们可以消去 $k!$ 和 $(d-k-1)!$ 项：\n$$\n\\Sigma = \\sum_{k=0}^{d-1} \\frac{(d-1)!}{d!}\n$$\n由于 $d! = d \\cdot (d-1)!$，这可以简化为：\n$$\n\\Sigma = \\sum_{k=0}^{d-1} \\frac{1}{d}\n$$\n求和从 $k=0$ 到 $k=d-1$，总共包含 $d$ 个相同的项。因此，和为：\n$$\n\\Sigma = d \\cdot \\frac{1}{d} = 1\n$$\n夏普利权重的和为 1，这是夏普利值的一个已知属性。\n\n最后，我们将这个结果代回我们关于 $\\phi_i$ 的表达式中：\n$$\n\\phi_{i} = (w_i x_i) \\cdot \\Sigma = (w_i x_i) \\cdot 1 = w_i x_i\n$$\n因此，对于具有独立、零均值特征的线性模型，SHAP 值（特征归因）就是项贡献 $w_i x_i$。",
            "answer": "$$\\boxed{w_i x_i}$$"
        },
        {
            "introduction": "在掌握了核心归因理论之后，我们将转向更实际的应用场景。现实世界中的许多信息物理系统（CPS）模型是复杂的“黑箱”，本练习  探讨了一种常见的 XAI 技术：用一个更简单、可解释的代理模型来近似一个黑箱模型。您将亲手实现这一方法，并应对代理模型的“保真度”（其模仿黑箱的准确程度）与“可解释性”（模型的简单性或稀疏性）之间的基本权衡。",
            "id": "4220836",
            "problem": "一个信息物理系统（CPS）由其数字孪生进行监控，该数字孪生将传感器状态聚合成特征向量，并计算一个黑盒异常分数。为了获得可解释性人工智能（XAI）意义上的透明解释，我们拟合一个稀疏线性代理模型来近似该黑盒异常检测器。考虑一个特征向量数据集 $x_i \\in \\mathbb{R}^d$（其中 $i \\in \\{1,\\dots,n\\}$）和一个黑盒异常分数函数 $f:\\mathbb{R}^d \\to \\mathbb{R}$。代理模型为 $g(x)=w^{\\top}x$，其中 $w \\in \\mathbb{R}^d$，我们通过带有 $\\ell_0$ 稀疏性惩罚的经验风险最小化来训练 $w$。使用中心化目标 $y_i = f(x_i)-\\bar{f}$，其中 $\\bar{f} = \\frac{1}{n}\\sum_{i=1}^{n}f(x_i)$，以符合零截距的代理模型 $g(x)=w^{\\top}x$。\n\n基本概念：\n- 均方误差（MSE）下的经验风险定义为\n$$\n\\ell(w;X,y) = \\frac{1}{n}\\sum_{i=1}^{n}\\left(w^{\\top}x_i - y_i\\right)^2,\n$$\n其中 $X \\in \\mathbb{R}^{n\\times d}$ 是行向量为 $x_i$ 的设计矩阵，$y \\in \\mathbb{R}^n$ 是元素为 $y_i$ 的中心化目标向量。\n- $\\ell_0$ “范数”计算非零系数的数量：$$\\|w\\|_0 = |\\{j \\in \\{1,\\dots,d\\}: w_j \\neq 0\\}|.$$\n- 带惩罚的目标函数为\n$$\nJ_{\\lambda}(w) = \\ell(w;X,y) + \\lambda \\|w\\|_0,\n$$\n其中 $\\lambda \\ge 0$ 用于调节稀疏性。\n- 保真度通过决定系数来量化：\n$$\nF = 1 - \\frac{\\ell(w;X,y)}{\\mathrm{Var}(y)}, \\quad \\mathrm{Var}(y) = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\bar{y})^2, \\quad \\bar{y}=\\frac{1}{n}\\sum_{i=1}^{n} y_i,\n$$\n因此 $F \\in (-\\infty,1]$，越高的 $F$ 值表示代理模型对黑盒异常分数的近似效果越好。\n- 相对于最佳无约束线性代理（无稀疏性惩罚，所有特征可用），完整性定义为\n$$\nC(\\lambda) = \n\\begin{cases}\n\\frac{F(\\lambda)}{F(0^{\\star})}, & \\text{if } F(0^{\\star}) > 0 \\\\\n0, & \\text{otherwise}\n\\end{cases}\n$$\n其中 $F(0^{\\star})$ 是使用所有特征的最小二乘解的保真度（无稀疏性约束下的最大保真度线性代理）。\n\n您的任务：\n1. 对于下方的每个测试用例，完全按照指定构造 $X$ 和 $f$，计算 $y_i=f(x_i)-\\bar{f}$，并对每个给定的 $\\lambda$ 求解\n$$\nw^{\\star}_{\\lambda} \\in \\arg\\min_{w \\in \\mathbb{R}^d} \\left\\{ \\ell(w;X,y) + \\lambda \\|w\\|_0 \\right\\}.\n$$\n为确保非凸的 $\\ell_0$ 惩罚问题得到全局最优解，将 $w$ 的支撑集限制在特征子集 $S \\subseteq \\{1,\\dots,d\\}$ 上，并对每个 $S$，获得其上的最小二乘最小化器；然后选择使 $J_{\\lambda}$ 最小化的子集 $S$。\n2. 对每个选定的 $w^{\\star}_{\\lambda}$，在数据集上计算 $F(\\lambda)$ 和 $C(\\lambda)$。\n3. 将所有结果按“最终输出格式”中指定的确切顺序汇总到一个列表中。\n\n测试套件（所有随机抽取必须使用指定的种子）：\n- 测试用例 A（部分线性黑盒）：\n  - $n=80$， $d=5$， 种子 $=1$。\n  - 从标准正态分布 $\\mathcal{N}(0,1)$ 中独立抽取 $X$ 的元素。\n  - 定义\n    $$\n    f(x) = 0.9\\,x_1 - 0.6\\,x_2 + 0.4\\,x_3 + 0.2\\,\\sin(x_4) + \\epsilon,\n    $$\n    其中 $\\epsilon \\sim \\mathcal{N}(0,0.05^2)$ 且独立于 $X$。\n  - 稀疏性惩罚：$\\lambda \\in [0.0, 0.01, 0.05, 0.2]$。\n- 测试用例 B（精确线性黑盒）：\n  - $n=60$， $d=6$， 种子 $=2$。\n  - 从 $\\mathcal{N}(0,1)$ 中独立抽取 $X$ 的元素。\n  - 定义\n    $$\n    f(x) = 1.0\\,x_1 - 1.0\\,x_2 + 0.5\\,x_3.\n    $$\n    不添加噪声。\n  - 稀疏性惩罚：$\\lambda \\in [0.0, 0.5, 1.5]$。\n- 测试用例 C（非线性黑盒）：\n  - $n=100$， $d=5$， 种子 $=3$。\n  - 从 $\\mathcal{N}(0,1)$ 中独立抽取 $X$ 的元素。\n  - 定义\n    $$\n    f(x) = \\tanh(1.2\\,x_1 + 1.0\\,x_2) + 0.3\\,\\cos(x_3\\,x_4) + 0.1\\,\\sin(x_5) + \\epsilon,\n    $$\n    其中 $\\epsilon \\sim \\mathcal{N}(0,0.02^2)$ 且独立于 $X$。\n  - 稀疏性惩罚：$\\lambda \\in [0.0, 0.1, 0.3]$。\n\n角度单位：任何三角函数均以弧度为单位。\n\n输出规格：\n- 对每个测试用例，以及按给定顺序的每个 $\\lambda$，计算数据集上的 $(F(\\lambda),C(\\lambda))$，并按以下确切顺序将所有值输出为单个浮点数列表：\n  - 测试用例 A: $F(0.0),C(0.0),F(0.01),C(0.01),F(0.05),C(0.05),F(0.2),C(0.2)$。\n  - 测试用例 B: $F(0.0),C(0.0),F(0.5),C(0.5),F(1.5),C(1.5)$。\n  - 测试用例 C: $F(0.0),C(0.0),F(0.1),C(0.1),F(0.3),C(0.3)$。\n- 您的程序应生成单行输出，其中包含一个由方括号括起来的逗号分隔列表形式的结果，例如 $[r_1,r_2,\\dots,r_{14}]$。不应打印其他任何文本。",
            "solution": "用户提供的问题是有效的。这是一个在应用于信息物理系统的可解释性人工智能（XAI）领域内定义明确、具有科学依据的问题。问题陈述是完整的，为复现解决方案提供了所有必要的数据、定义和约束。该计算任务虽然密集，但对于指定的维度是可行的。\n\n核心任务是找到一个稀疏线性代理模型 $g(x) = w^{\\top}x$，该模型可以近似一个黑盒异常分数函数 $f(x)$。权重向量 $w \\in \\mathbb{R}^d$ 通过最小化一个带惩罚的经验风险函数进行训练。目标函数 $J_{\\lambda}(w)$ 结合了均方误差（MSE）损失和 $\\ell_0$ 稀疏性惩罚：\n$$\nJ_{\\lambda}(w) = \\ell(w;X,y) + \\lambda \\|w\\|_0 = \\frac{1}{n}\\sum_{i=1}^{n}\\left(w^{\\top}x_i - y_i\\right)^2 + \\lambda |\\{j : w_j \\neq 0\\}|\n$$\n这里，$X \\in \\mathbb{R}^{n\\times d}$ 是设计矩阵，$y \\in \\mathbb{R}^n$ 是中心化目标值的向量，即 $y_i = f(x_i) - \\bar{f}$，其中 $\\bar{f} = \\frac{1}{n}\\sum_i f(x_i)$。参数 $\\lambda \\ge 0$ 控制着模型对数据的保真度与其稀疏性之间的权衡。\n\n$\\ell_0$ 惩罚项使得目标函数 $J_{\\lambda}(w)$ 成为非凸函数，通常情况下，找到全局最小值是一个NP难问题。然而，该问题指定了一种保证获得全局最优解的方法：对所有可能的特征子集进行穷举搜索。由于所有测试用例中的特征维度 $d$ 都很小（$d=5$ 或 $d=6$），这种组合搜索在计算上是可行的。总共有 $2^d$ 个可能的特征子集需要考虑。\n\n对于每个给定的测试用例和每个 $\\lambda$ 值，求解方法如下：\n\n1.  **数据生成**：对于每个测试用例，我们首先生成数据。使用指定的随机种子，创建一个设计矩阵 $X \\in \\mathbb{R}^{n\\times d}$，其元素从标准正态分布 $\\mathcal{N}(0,1)$ 中抽取。对 $X$ 的每一行 $x_i$ 评估黑盒函数 $f(x)$ 以产生分数 $f(x_i)$。如果指定了噪声 $\\epsilon$，也会生成并添加它。最后，通过对分数进行中心化来计算目标向量 $y$：$y_i = f(x_i) - \\frac{1}{n}\\sum_{j=1}^{n}f(x_j)$。根据构造， $y$ 的均值 $\\bar{y} = 0$。\n\n2.  **最佳子集选择**：为了找到最小化 $J_{\\lambda}(w)$ 的最优权重向量 $w^{\\star}_{\\lambda}$，我们遍历所有 $2^d$ 个可能的特征索引子集 $S \\subseteq \\{1, \\dots, d\\}$。\n    *   对于每个大小为 $k = |S|$ 的子集 $S$，我们求解一个标准的普通最小二乘（OLS）回归问题。我们寻求一个权重向量 $w_S$，其非零元素仅存在于 $S$ 中的索引上，且该向量能最小化MSE。这等价于将 $y$ 对 $X$ 中对应于子集 $S$ 的列（表示为 $X_S$）进行回归。OLS解为 $w_S^* = (X_S^{\\top}X_S)^{-1}X_S^{\\top}y$。\n    *   该子集的最小化MSE为 $\\ell_S = \\frac{1}{n}\\|X_S w_S^* - y\\|_2^2$。\n    *   该子集的带惩罚目标值为 $J_{\\lambda, S} = \\ell_S + \\lambda k$。\n    *   我们在所有 $2^d$ 个子集中比较 $J_{\\lambda, S}$ 的值。产生最小值 $J_{\\lambda}^{\\star} = \\min_S J_{\\lambda, S}$ 的子集 $S^\\star$ 是给定 $\\lambda$ 的最优支撑集。相应的最小MSE为 $\\ell^{\\star}_{\\lambda}$。\n\n3.  **指标计算**：\n    *   **保真度（$F(\\lambda)$）**：给定 $\\lambda$ 的代理模型的保真度使用决定系数公式计算。它需要目标向量的方差 $\\mathrm{Var}(y) = \\frac{1}{n}\\sum_{i=1}^n (y_i - \\bar{y})^2 = \\frac{1}{n}\\sum_{i=1}^n y_i^2$（因为 $\\bar{y}=0$）。保真度则为：\n        $$\n        F(\\lambda) = 1 - \\frac{\\ell^{\\star}_{\\lambda}}{\\mathrm{Var}(y)}\n        $$\n    *   **完整性（$C(\\lambda)$）**：首先，我们需要基线保真度 $F(0^{\\star})$，它对应于没有稀疏性约束的最佳可能线性模型。这可以通过使用所有 $d$ 个特征的OLS解（这也是 $\\lambda=0$ 时的解）来实现。设相应的MSE为 $\\ell_{OLS}$。那么 $F(0^{\\star}) = 1 - \\ell_{OLS} / \\mathrm{Var}(y)$。完整性则计算为比率：\n        $$\n        C(\\lambda) = \\frac{F(\\lambda)}{F(0^{\\star})}, \\quad \\text{if } F(0^{\\star}) > 0, \\text{ and } C(\\lambda)=0 \\text{ otherwise.}\n        $$\n\n对每个测试用例都执行这整个过程。对于一个给定的用例，为了优化计算，会预先计算每个可能的特征子集的MSE。然后，对每个 $\\lambda$，我们只需找到使预先计算的MSE加上新的惩罚项 $\\lambda k$ 最小化的子集。最后，所有计算出的 $(F(\\lambda), C(\\lambda))$ 对都按照问题规定收集到一个有序列表中。",
            "answer": "```python\nimport numpy as np\nfrom itertools import combinations\n\ndef solve_case(X, y, n, d, lambdas):\n    \"\"\"\n    Solves for F(lambda) and C(lambda) for a single test case.\n    \"\"\"\n    case_results = []\n    \n    # Target vector y is already centered, so its mean is effectively zero.\n    var_y = np.var(y)\n    if var_y == 0:\n        # If variance is zero or negative (due to float precision), F is ill-defined.\n        # This case is not expected with the given problem data.\n        # Return zeros for all metrics.\n        return [0.0] * 2 * len(lambdas)\n\n    # Calculate baseline fidelity F(0*) from the full OLS model.\n    # This corresponds to lambda=0.\n    w_ols, resid_ols, _, _ = np.linalg.lstsq(X, y, rcond=None)\n    mse_ols = resid_ols[0] / n if resid_ols.size > 0 else np.mean((X @ w_ols - y)**2)\n    F_star = 1 - mse_ols / var_y\n\n    # Pre-calculate the MSE for all 2^d possible feature subsets.\n    # This avoids redundant least-squares computations for each lambda.\n    subset_mses = {}\n    for k in range(d + 1):\n        for subset in combinations(range(d), k):\n            if k == 0:\n                # For an empty set of features, the prediction is 0. MSE is Var(y).\n                mse = var_y\n            else:\n                X_S = X[:, subset]\n                # Solve the OLS problem for the subset of features.\n                _, resid_S, _, _ = np.linalg.lstsq(X_S, y, rcond=None)\n                mse = resid_S[0] / n if resid_S.size > 0 else np.mean((X_S @ np.linalg.lstsq(X_S, y, rcond=None)[0] - y)**2)\n            subset_mses[subset] = mse\n\n    # For each lambda, find the best subset that minimizes the J(w) objective.\n    for lam in lambdas:\n        min_J = float('inf')\n        best_mse_for_lam = float('inf')\n\n        for subset, mse in subset_mses.items():\n            k = len(subset)  # Number of non-zero coefficients\n            J = mse + lam * k\n            if J  min_J:\n                min_J = J\n                best_mse_for_lam = mse\n\n        # Calculate fidelity F(lambda) for the optimal model.\n        F_lam = 1 - best_mse_for_lam / var_y\n        \n        # Calculate completeness C(lambda).\n        C_lam = 0.0\n        if F_star > 0:\n            C_lam = F_lam / F_star\n        \n        case_results.extend([F_lam, C_lam])\n\n    return case_results\n\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final result.\n    \"\"\"\n    all_results = []\n\n    # --- Test Case A ---\n    n_A, d_A, seed_A = 80, 5, 1\n    lambdas_A = [0.0, 0.01, 0.05, 0.2]\n    rng_A = np.random.default_rng(seed_A)\n    X_A = rng_A.standard_normal(size=(n_A, d_A))\n    epsilon_A = rng_A.normal(0, 0.05, size=n_A)\n    f_A_vals = (0.9 * X_A[:, 0] - 0.6 * X_A[:, 1] + 0.4 * X_A[:, 2] +\n                0.2 * np.sin(X_A[:, 3]) + epsilon_A)\n    y_A = f_A_vals - np.mean(f_A_vals)\n    results_A = solve_case(X_A, y_A, n_A, d_A, lambdas_A)\n    all_results.extend(results_A)\n\n    # --- Test Case B ---\n    n_B, d_B, seed_B = 60, 6, 2\n    lambdas_B = [0.0, 0.5, 1.5]\n    rng_B = np.random.default_rng(seed_B)\n    X_B = rng_B.standard_normal(size=(n_B, d_B))\n    f_B_vals = 1.0 * X_B[:, 0] - 1.0 * X_B[:, 1] + 0.5 * X_B[:, 2]\n    y_B = f_B_vals - np.mean(f_B_vals)\n    results_B = solve_case(X_B, y_B, n_B, d_B, lambdas_B)\n    all_results.extend(results_B)\n\n    # --- Test Case C ---\n    n_C, d_C, seed_C = 100, 5, 3\n    lambdas_C = [0.0, 0.1, 0.3]\n    rng_C = np.random.default_rng(seed_C)\n    X_C = rng_C.standard_normal(size=(n_C, d_C))\n    epsilon_C = rng_C.normal(0, 0.02, size=n_C)\n    f_C_vals = (np.tanh(1.2 * X_C[:, 0] + 1.0 * X_C[:, 1]) +\n                0.3 * np.cos(X_C[:, 2] * X_C[:, 3]) +\n                0.1 * np.sin(X_C[:, 4]) + epsilon_C)\n    y_C = f_C_vals - np.mean(f_C_vals)\n    results_C = solve_case(X_C, y_C, n_C, d_C, lambdas_C)\n    all_results.extend(results_C)\n\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在前面的练习基础上，本部分将探讨信任这一关键问题。要在高风险的 CPS 应用中真正发挥作用，XAI 的输出必须是可靠的。这个动手练习  引入了校准的概念，这是一种确保模型预测的概率能真实反映事件发生可能性的技术。您不仅需要将此技术应用于模型的预测，还需要将其应用于一个创新的“解释可靠性”指标，从而推动您批判性地思考整个人工智能系统的可信度。",
            "id": "4220883",
            "problem": "在信息物理系统 (CPS) 及其关联的数字孪生中，您面临一个二元故障检测场景。一个分类器为故障的存在生成未校准的分数，一个解释模块为特征生成一个重要性分布。您必须计算用于故障检测和解释正确性的校准后概率估计，然后通过预期校准误差 (ECE) 评估其可靠性。问题要求设计并实现一个基于基本概率定义和经过充分测试的统计模型的完整算法，并在指定的测试套件上产生数值结果。\n\n基本原理：\n- 一个标签为 $y \\in \\{0,1\\}$ 的二元事件被建模为一个概率为 $p \\in [0,1]$ 的伯努利随机变量，使得 $\\mathbb{P}(Y=1)=p$ 且 $\\mathbb{P}(Y=0)=1-p$。\n- logistic (sigmoid) 链接函数为 $\\sigma(x) = \\frac{1}{1+\\exp(-x)}$。logit 函数为 $\\text{logit}(p) = \\log\\left(\\frac{p}{1-p}\\right)$，其中 $p \\in (0,1)$。\n- 对于校准，考虑在校准集上通过最大似然法学习的参数化映射。对于检测分数 $x$，logistic 校准映射为 $\\tilde{p}(x) = \\sigma(a x + b)$，其参数 $a$ 和 $b$ 的选择旨在最小化伯努利模型下的负对数似然。对于解释可靠性，其未校准概率为 $q \\in (0,1)$，使用形式为 $\\tilde{r}(q) = \\sigma(a \\cdot \\text{logit}(q) + b)$ 的单调映射，其中 $a$ 和 $b$ 的学习方式类似。\n- 使用 $M$ 个分箱的预期校准误差 (ECE) 将单位区间划分为 $M$ 个等宽的分箱。对于预测值 $\\{p_i\\}_{i=1}^N$ 和二元标签 $\\{y_i\\}_{i=1}^N$，将分箱 $B_m$ 定义为满足 $p_i \\in \\left[\\frac{m-1}{M}, \\frac{m}{M}\\right)$ 的索引 $i$ 的集合。令 $n_m$ 为 $B_m$ 中的样本数，$\\hat{c}_m = \\frac{1}{n_m}\\sum_{i \\in B_m} p_i$ 为平均置信度，$\\hat{a}_m = \\frac{1}{n_m}\\sum_{i \\in B_m} y_i$ 为经验准确率。ECE 为\n$$\n\\text{ECE} = \\sum_{m=1}^M \\frac{n_m}{N} \\left| \\hat{a}_m - \\hat{c}_m \\right|.\n$$\n如果某个分箱的 $n_m = 0$，则其贡献定义为 $0$。\n\n您的程序必须为每个测试用例实现以下步骤：\n1. 在提供的校准分割上，通过最小化伯努利负对数似然，学习检测校准参数 $(a_d,b_d)$，用于映射 $x \\mapsto \\sigma(a_d x + b_d)$。\n2. 在提供的校准分割上，通过最小化伯努利负对数似然，学习解释校准参数 $(a_e,b_e)$，用于映射 $q \\mapsto \\sigma\\left(a_e \\cdot \\text{logit}(q) + b_e\\right)$，其中 $q \\in (0,1)$ 是每个样本的未校准解释可靠性。\n3. 将学习到的校准器应用于测试分割，以生成校准后的检测概率和校准后的解释可靠性概率。\n4. 在测试分割上，使用 $M = 10$ 个分箱，计算校准后检测概率的 ECE 和校准后解释可靠性概率的 ECE。\n\n解释可靠性构建：对于每个样本 $i$，给定一个非负重要性向量 $\\mathbf{e}_i \\in \\mathbb{R}^d$，其分量之和为 $1$。令故障关键特征集为 $F \\subseteq \\{0,1,\\dots,d-1\\}$。定义未校准的解释可靠性为 $q_i = \\sum_{j \\in F} e_{i,j}$。将二元解释正确性标签 $z_i$ 定义为一次伯努利抽样，其概率 $p^{\\text{true}}_i$ 在下面的每个测试用例中指定。\n\n测试套件和数据生成（所有随机抽样必须使用指定的种子以保证可复现性）：\n- 分箱：在所有情况下，ECE 均使用 $M = 10$ 个分箱。\n- 置信度舍入：最终输出必须是十进制浮点数。不涉及物理单位或角度。\n\n测试用例 1（一般情况）：\n- 校准集大小 $N_{\\text{cal}} = 160$，测试集大小 $N_{\\text{test}} = 160$，特征维度 $d = 5$，种子：检测种子为 $7$，解释种子为校准集 $19$ 和测试集 $29$。\n- 检测分数：\n  - 对于 $i = 0,1,\\dots,N_{\\text{cal}}-1$，令 $t_i = \\frac{2i}{N_{\\text{cal}}-1} - 1$。使用种子 $7$ 生成独立高斯噪声 $\\xi_i \\sim \\mathcal{N}(0,0.5^2)$。设置 $x^{\\text{cal}}_i = 0.9 t_i + 0.2 \\xi_i$。定义真实概率 $p^{\\text{true}}_i = \\sigma(1.8 x^{\\text{cal}}_i - 0.4)$。使用种子 $7$ 进行抽样，$y^{\\text{cal}}_i \\sim \\text{Bernoulli}(p^{\\text{true}}_i)$。\n  - 对于测试集，使用 $N_{\\text{test}}$ 重复此过程，噪声和伯努利抽样使用种子 $17$：$x^{\\text{test}}_i = 0.9 t_i + 0.2 \\xi_i$，$p^{\\text{true}}_i = \\sigma(1.8 x^{\\text{test}}_i - 0.4)$，$y^{\\text{test}}_i \\sim \\text{Bernoulli}(p^{\\text{true}}_i)$。\n- 解释重要性和正确性：\n  - 故障集 $F = \\{1,4\\}$。对于每个样本 $i$ 和特征 $j \\in \\{0,1,2,3,4\\}$，定义\n    $$\n    w_{i,j} = \\cos(0.7 j + 0.03 i) + 0.2 \\sin\\!\\big(0.3 (j+1) (i+2)\\big) + 0.1,\n    $$\n    $u_{i,j} = \\exp(w_{i,j})$，以及 $e_{i,j} = \\frac{u_{i,j}}{\\sum_{k=0}^{d-1} u_{i,k}}$。\n  - 对于校准集，使用种子 $19$：$q^{\\text{cal}}_i = \\sum_{j \\in F} e_{i,j}$，真实正确性概率 $p^{\\text{true,exp}}_i = \\min\\big(1,\\max\\big(0, 0.1 + 0.8 q^{\\text{cal}}_i\\big)\\big)$，且 $z^{\\text{cal}}_i \\sim \\text{Bernoulli}(p^{\\text{true,exp}}_i)$。\n  - 对于测试集，使用种子 $29$：类似地定义 $q^{\\text{test}}_i$，且 $z^{\\text{test}}_i \\sim \\text{Bernoulli}(p^{\\text{true,exp}}_i)$。\n\n测试用例 2（近乎完美的校准）：\n- 校准集大小 $N_{\\text{cal}} = 180$，测试集大小 $N_{\\text{test}} = 180$，特征维度 $d = 4$，种子：检测种子为 $11$，解释种子为校准集和测试集均使用 $13$。\n- 检测分数：\n  - 使用种子 $11$ 生成 $t_i$ 和噪声 $\\xi_i \\sim \\mathcal{N}(0,0.3^2)$。设置 $x^{\\text{cal}}_i = 0.8 t_i + 0.1 \\xi_i$，$p^{\\text{true}}_i = \\sigma(x^{\\text{cal}}_i)$，且 $y^{\\text{cal}}_i \\sim \\text{Bernoulli}(p^{\\text{true}}_i)$。\n  - 对于测试集，使用种子 $13$，$x^{\\text{test}}_i = 0.8 t_i + 0.1 \\xi_i$，$p^{\\text{true}}_i = \\sigma(x^{\\text{test}}_i)$，且 $y^{\\text{test}}_i \\sim \\text{Bernoulli}(p^{\\text{true}}_i)$。\n- 解释重要性和正确性：\n  - 故障集 $F = \\{0,3\\}$。对于每个样本 $i$ 和特征 $j \\in \\{0,1,2,3\\}$，\n    $$\n    w_{i,j} = 0.5 + 0.5\\left(\\sin(0.5 j) + \\cos\\!\\left(0.02 i + \\frac{j}{3}\\right)\\right),\n    $$\n    $u_{i,j} = \\exp(w_{i,j})$，$e_{i,j} = \\frac{u_{i,j}}{\\sum_{k=0}^{d-1} u_{i,k}}$，$q_i = \\sum_{j \\in F} e_{i,j}$。\n  - 对于校准集和测试集，均使用种子 $13$：$p^{\\text{true,exp}}_i = q_i$，且 $z_i \\sim \\text{Bernoulli}(p^{\\text{true,exp}}_i)$。\n\n测试用例 3（强失准）：\n- 校准集大小 $N_{\\text{cal}} = 140$，测试集大小 $N_{\\text{test}} = 140$，特征维度 $d = 6$，种子：检测种子为 $23$，解释种子为校准集 $31$ 和测试集 $37$。\n- 检测分数：\n  - 使用种子 $23$ 生成 $t_i$ 和噪声 $\\xi_i \\sim \\mathcal{N}(0,0.6^2)$。设置 $x^{\\text{cal}}_i = 1.0 t_i + 0.3 \\xi_i$，真实概率 $p^{\\text{true}}_i = \\sigma(3.0 x^{\\text{cal}}_i + 1.2)$，且 $y^{\\text{cal}}_i \\sim \\text{Bernoulli}(p^{\\text{true}}_i)$。\n  - 对于测试集，检测抽样使用种子 $29$，设置 $x^{\\text{test}}_i = 1.0 t_i + 0.3 \\xi_i$，$p^{\\text{true}}_i = \\sigma(3.0 x^{\\text{test}}_i + 1.2)$，且 $y^{\\text{test}}_i \\sim \\text{Bernoulli}(p^{\\text{true}}_i)$。\n- 解释重要性和正确性：\n  - 故障集 $F = \\{2,5\\}$。对于每个样本 $i$ 和特征 $j \\in \\{0,1,2,3,4,5\\}$，\n    $$\n    w_{i,j} = 0.3 + \\sin(0.4 j i) + 0.1 \\cos(0.1 i - 0.2 j),\n    $$\n    $u_{i,j} = \\exp(w_{i,j})$，$e_{i,j} = \\frac{u_{i,j}}{\\sum_{k=0}^{d-1} u_{i,k}}$，$q_i = \\sum_{j \\in F} e_{i,j}$。\n  - 对于校准集，使用种子 $31$：$p^{\\text{true,exp}}_i = \\min\\left(1, \\max\\left(0, 0.05 + 0.9 q_i^{1/3}\\right)\\right)$ 且 $z^{\\text{cal}}_i \\sim \\text{Bernoulli}(p^{\\text{true,exp}}_i)$。\n  - 对于测试集，使用种子 $37$：类似地定义 $p^{\\text{true,exp}}_i$ 且 $z^{\\text{test}}_i \\sim \\text{Bernoulli}(p^{\\text{true,exp}}_i)$。\n\n输出规范：\n- 对于每个测试用例，计算两个数字：在测试分割上使用 $M = 10$ 个分箱的校准后检测概率的 ECE 和校准后解释可靠性概率的 ECE。\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。结果按测试用例的顺序排列，每个测试用例贡献两个浮点数：$[\\text{ECE}_{\\text{det,1}},\\text{ECE}_{\\text{exp,1}},\\text{ECE}_{\\text{det,2}},\\text{ECE}_{\\text{exp,2}},\\text{ECE}_{\\text{det,3}},\\text{ECE}_{\\text{exp,3}}]$。所有值都表示为四舍五入到六位小数的十进制浮点数。",
            "solution": "该问题要求设计并实现一种算法，用于在信息物理系统 (CPS) 的背景下，校准来自故障检测分类器及其关联解释模块的概率输出。校准将使用 logistic 尺度变换执行，其有效性通过预期校准误差 (ECE) 进行评估。整个过程将在三个具有指定数据生成过程的不同测试用例上进行演示。\n\n解决方案分几个阶段进行：\n1.  校准模型的形式化定义以及用于学习其参数的最大似然估计 (MLE) 过程。\n2.  预期校准误差 (ECE) 指标的形式化定义。\n3.  为每个测试用例生成数据、训练校准器并评估其性能的算法的分步描述。\n\n**1. 概率建模与校准**\n\n问题的核心在于将未校准的分数映射到经过良好校准的概率。对于一个二元事件 $Y \\in \\{0, 1\\}$，如果对于任何值 $p^* \\in [0, 1]$，给定预测的事件条件期望等于预测本身，即 $\\mathbb{E}[Y | \\hat{p} = p^*] = p^*$，那么概率估计 $\\hat{p}$ 就是良好校准的。\n\n我们有两个校准任务：一个用于检测分数，另一个用于解释可靠性。两者都通过在专用校准数据集上使用最大似然估计 (MLE) 学习的参数化校准映射来解决。\n\n**1.1. 检测校准**\n\n令 $x \\in \\mathbb{R}$ 为故障检测器输出的未校准分数，令 $y \\in \\{0, 1\\}$ 为真实的故障标签。提出的校准模型是 logistic（或 Platt）尺度变换函数：\n$$\n\\tilde{p}(x; a_d, b_d) = \\sigma(a_d x + b_d) = \\frac{1}{1 + \\exp(-(a_d x + b_d))}\n$$\n其中 $(a_d, b_d)$ 是待学习的标量和偏置参数。\n\n给定一个校准集 $\\{(x_i^{\\text{cal}}, y_i^{\\text{cal}})\\}_{i=1}^{N_{\\text{cal}}}$，通过最小化数据在伯努利模型下的负对数似然 (NLL) 来找到参数 $(a_d, b_d)$。作为损失函数的 NLL 为：\n$$\n\\mathcal{L}(a_d, b_d) = - \\sum_{i=1}^{N_{\\text{cal}}} \\left[ y_i^{\\text{cal}} \\log(\\tilde{p}_i) + (1 - y_i^{\\text{cal}}) \\log(1 - \\tilde{p}_i) \\right]\n$$\n其中 $\\tilde{p}_i = \\sigma(a_d x_i^{\\text{cal}} + b_d)$。代入 sigmoid 函数的定义并简化，我们得到一个数值上更稳定的损失函数形式：\n$$\n\\mathcal{L}(a_d, b_d) = \\sum_{i=1}^{N_{\\text{cal}}} \\left[ \\log(1 + \\exp(a_d x_i^{\\text{cal}} + b_d)) - y_i^{\\text{cal}}(a_d x_i^{\\text{cal}} + b_d) \\right]\n$$\n这是一个凸优化问题，其最小值可以使用基于梯度的方法找到。损失函数的偏导数为：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial a_d} = \\sum_{i=1}^{N_{\\text{cal}}} x_i^{\\text{cal}} (\\sigma(a_d x_i^{\\text{cal}} + b_d) - y_i^{\\text{cal}})\n$$\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial b_d} = \\sum_{i=1}^{N_{\\text{cal}}} (\\sigma(a_d x_i^{\\text{cal}} + b_d) - y_i^{\\text{cal}})\n$$\n\n**1.2. 解释校准**\n\n令 $q \\in (0, 1)$ 为解释的未校准可靠性分数，令 $z \\in \\{0, 1\\}$ 为其正确性的真实标签。提出的校准模型是在 logit 空间中应用的单调函数：\n$$\n\\tilde{r}(q; a_e, b_e) = \\sigma(a_e \\cdot \\text{logit}(q) + b_e)\n$$\n其中 $\\text{logit}(q) = \\log\\left(\\frac{q}{1-q}\\right)$。如果我们定义一个新特征 $x' = \\text{logit}(q)$，则此模型结构等价于检测校准模型。模型变为 $\\tilde{r}(x'; a_e, b_e) = \\sigma(a_e x' + b_e)$。\n\n给定一个校准集 $\\{(q_i^{\\text{cal}}, z_i^{\\text{cal}})\\}_{i=1}^{N_{\\text{cal}}}$，我们首先转换输入：$x'_i = \\text{logit}(q_i^{\\text{cal}})$。然后通过最小化与之前相同的 NLL 函数来找到参数 $(a_e, b_e)$，但输入为 $\\{(x'_i, z_i^{\\text{cal}})\\}:$:\n$$\n\\mathcal{L}(a_e, b_e) = \\sum_{i=1}^{N_{\\text{cal}}} \\left[ \\log(1 + \\exp(a_e x'_i + b_e)) - z_i^{\\text{cal}}(a_e x'_i + b_e) \\right]\n$$\n关于 $a_e$ 和 $b_e$ 的梯度与检测校准的梯度类似。\n\n**2. 通过预期校准误差 (ECE) 进行评估**\n\nECE 衡量置信度与准确率之间的差异。给定一个包含 $N$ 个预测 $\\{p_i\\}_{i=1}^N$ 和相应真实标签 $\\{y_i\\}_{i=1}^N$ 的测试集，预测值被划分到 $M$ 个等间距的分箱 $B_m$ 中（$m=1, \\dots, M$）。分箱 $B_m$ 包含所有预测概率 $p_i$ 落在区间 $I_m = \\left[\\frac{m-1}{M}, \\frac{m}{M}\\right)$ 内的样本 $i$。\n\n对于每个 $n_m = |B_m| > 0$ 的分箱 $B_m$，我们计算：\n- 平均置信度：$\\hat{c}_m = \\frac{1}{n_m} \\sum_{i \\in B_m} p_i$\n- 经验准确率：$\\hat{a}_m = \\frac{1}{n_m} \\sum_{i \\in B_m} y_i$\n\nECE 是所有分箱中准确率和置信度之间绝对差异的加权平均值：\n$$\n\\text{ECE} = \\sum_{m=1}^M \\frac{n_m}{N} \\left| \\hat{a}_m - \\hat{c}_m \\right|\n$$\n如果一个分箱为空 ($n_m = 0$)，其对总和的贡献为 $0$。较低的 ECE 表示更好的校准效果。\n\n**3. 算法流程**\n\n总体算法通过迭代三个指定的测试用例来执行。对于每个用例：\n\n1.  **数据生成**：根据指定的参数 ($N_{\\text{cal}}, N_{\\text{test}}, d$)、公式和随机数生成器种子生成校准和测试数据集。这包括创建：\n    - 检测数据：$(x^{\\text{cal}}, y^{\\text{cal}})$ 和 $(x^{\\text{test}}, y^{\\text{test}})$。\n    - 解释数据：从重要性向量 $\\mathbf{e}_i$ 导出未校准的可靠性 $q_i = \\sum_{j \\in F} e_{i,j}$ 和正确性标签 $z_i$。这将得到 $(q^{\\text{cal}}, z^{\\text{cal}})$ 和 $(q^{\\text{test}}, z^{\\text{test}})$。所有随机抽样必须使用指定的种子以保证可复现性。\n\n2.  **检测校准器训练**：\n    - 使用校准数据 $(x^{\\text{cal}}, y^{\\text{cal}})$。\n    - 数值求解优化问题 $\\arg\\min_{a_d, b_d} \\mathcal{L}(a_d, b_d)$ 以找到最优参数 $\\hat{a}_d, \\hat{b}_d$。这是通过一个基于梯度的求解器完成的。\n\n3.  **解释校准器训练**：\n    - 使用校准数据 $(q^{\\text{cal}}, z^{\\text{cal}})$。\n    - 首先，计算转换后的特征 $x'^{\\text{cal}}_i = \\text{logit}(q^{\\text{cal}}_i)$。\n    - 在数据 $(x'^{\\text{cal}}, z^{\\text{cal}})$ 上数值求解优化问题 $\\arg\\min_{a_e, b_e} \\mathcal{L}(a_e, b_e)$ 以找到最优参数 $\\hat{a}_e, \\hat{b}_e$。\n\n4.  **在测试集上评估**：\n    - **检测 ECE**：\n        - 将学习到的检测校准器应用于测试分数：$\\tilde{p}^{\\text{test}}_i = \\sigma(\\hat{a}_d x^{\\text{test}}_i + \\hat{b}_d)$。\n        - 使用 $M=10$ 个分箱，在校准后的概率集 $\\{\\tilde{p}^{\\text{test}}_i\\}$ 和真实标签集 $\\{y^{\\text{test}}_i\\}$ 上计算 ECE。\n    - **解释 ECE**：\n        - 将学习到的解释校准器应用于测试可靠性：$\\tilde{r}^{\\text{test}}_i = \\sigma(\\hat{a}_e \\cdot \\text{logit}(q^{\\text{test}}_i) + \\hat{b}_e)$。\n        - 使用 $M=10$ 个分箱，在校准后的概率集 $\\{\\tilde{r}^{\\text{test}}_i\\}$ 和真实标签集 $\\{z^{\\text{test}}_i\\}$ 上计算 ECE。\n\n5.  **输出**：收集当前测试用例的两个 ECE 值。处理完所有用例后，将结果格式化为包含六个浮点数的单个列表，并以指定格式打印。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.special import expit as sigmoid, logit\n\ndef compute_ece(probs, labels, M=10):\n    \"\"\"\n    Computes the Expected Calibration Error (ECE).\n\n    Args:\n        probs (np.ndarray): Array of predicted probabilities.\n        labels (np.ndarray): Array of true binary labels (0 or 1).\n        M (int): Number of bins to use.\n\n    Returns:\n        float: The ECE value.\n    \"\"\"\n    if len(probs) == 0:\n        return 0.0\n\n    bins = np.linspace(0.0, 1.0, M + 1)\n    bin_lowers = bins[:-1]\n    bin_uppers = bins[1:]\n\n    ece = 0.0\n    N = len(probs)\n\n    for i in range(M):\n        in_bin = np.logical_and(probs >= bin_lowers[i], probs  bin_uppers[i])\n        # Special case for the last bin to include 1.0\n        if i == M - 1:\n            in_bin = np.logical_or(in_bin, probs == bin_uppers[i])\n        \n        n_m = np.sum(in_bin)\n\n        if n_m > 0:\n            bin_probs = probs[in_bin]\n            bin_labels = labels[in_bin]\n            \n            avg_confidence = np.mean(bin_probs)\n            accuracy = np.mean(bin_labels)\n            \n            ece += (n_m / N) * np.abs(accuracy - avg_confidence)\n\n    return ece\n\ndef train_calibrator(features, labels):\n    \"\"\"\n    Trains a logistic calibration model by minimizing negative log-likelihood.\n\n    Args:\n        features (np.ndarray): Input features for the model.\n        labels (np.ndarray): True binary labels.\n\n    Returns:\n        tuple[float, float]: The learned parameters (a, b).\n    \"\"\"\n    def nll_and_grad(params, X, y):\n        a, b = params\n        z = a * X + b\n        p = sigmoid(z)\n        \n        # Negative Log-Likelihood\n        # Using a more stable formulation to avoid log(0)\n        log_likelihood = np.sum(y * z - np.log(1 + np.exp(z)))\n        nll = -log_likelihood\n\n        # Gradient\n        grad_a = -np.sum(X * (y - p))\n        grad_b = -np.sum(y - p)\n        \n        return nll, np.array([grad_a, grad_b])\n\n    # Initial guess for parameters (a, b)\n    initial_params = np.array([1.0, 0.0])\n\n    res = minimize(\n        fun=nll_and_grad,\n        x0=initial_params,\n        args=(features, labels),\n        jac=True,  # We provide the gradient\n        method='L-BFGS-B'\n    )\n    \n    a_opt, b_opt = res.x\n    return a_opt, b_opt\n\ndef generate_test_case_data(case_config):\n    \"\"\"\n    Generates all data for a single test case.\n    \"\"\"\n    case_id = case_config['id']\n    N_cal, N_test, d = case_config['N_cal'], case_config['N_test'], case_config['d']\n    \n    data = {}\n    \n    # Common i-indices and t-sequences\n    i_cal = np.arange(N_cal)\n    t_cal = (2 * i_cal / (N_cal - 1)) - 1\n    i_test = np.arange(N_test)\n    t_test = (2 * i_test / (N_test - 1)) - 1\n    \n    # --- Test Case Specifics ---\n    if case_id == 1:\n        # Detection Data\n        rng_det_cal = np.random.default_rng(7)\n        xi_cal = rng_det_cal.normal(0, 0.5, size=N_cal)\n        data['x_cal'] = 0.9 * t_cal + 0.2 * xi_cal\n        p_true_cal = sigmoid(1.8 * data['x_cal'] - 0.4)\n        data['y_cal'] = rng_det_cal.binomial(1, p_true_cal)\n\n        rng_det_test = np.random.default_rng(17)\n        xi_test = rng_det_test.normal(0, 0.5, size=N_test)\n        data['x_test'] = 0.9 * t_test + 0.2 * xi_test\n        p_true_test = sigmoid(1.8 * data['x_test'] - 0.4)\n        data['y_test'] = rng_det_test.binomial(1, p_true_test)\n        \n        # Explanation Data\n        F = {1, 4}\n        j = np.arange(d)\n        w_cal = np.cos(0.7 * j[None, :] + 0.03 * i_cal[:, None]) + 0.2 * np.sin(0.3 * (j[None, :] + 1) * (i_cal[:, None] + 2)) + 0.1\n        u_cal = np.exp(w_cal)\n        e_cal = u_cal / u_cal.sum(axis=1, keepdims=True)\n        data['q_cal'] = e_cal[:, list(F)].sum(axis=1)\n        p_true_exp_cal = np.clip(0.1 + 0.8 * data['q_cal'], 0, 1)\n        rng_exp_cal = np.random.default_rng(19)\n        data['z_cal'] = rng_exp_cal.binomial(1, p_true_exp_cal)\n\n        w_test = np.cos(0.7 * j[None, :] + 0.03 * i_test[:, None]) + 0.2 * np.sin(0.3 * (j[None, :] + 1) * (i_test[:, None] + 2)) + 0.1\n        u_test = np.exp(w_test)\n        e_test = u_test / u_test.sum(axis=1, keepdims=True)\n        data['q_test'] = e_test[:, list(F)].sum(axis=1)\n        p_true_exp_test = np.clip(0.1 + 0.8 * data['q_test'], 0, 1)\n        rng_exp_test = np.random.default_rng(29)\n        data['z_test'] = rng_exp_test.binomial(1, p_true_exp_test)\n\n    elif case_id == 2:\n        # Detection Data\n        rng_det_cal = np.random.default_rng(11)\n        xi_cal = rng_det_cal.normal(0, 0.3, size=N_cal)\n        data['x_cal'] = 0.8 * t_cal + 0.1 * xi_cal\n        p_true_cal = sigmoid(data['x_cal'])\n        data['y_cal'] = rng_det_cal.binomial(1, p_true_cal)\n\n        rng_det_test = np.random.default_rng(13)\n        xi_test = rng_det_test.normal(0, 0.3, size=N_test)\n        data['x_test'] = 0.8 * t_test + 0.1 * xi_test\n        p_true_test = sigmoid(data['x_test'])\n        data['y_test'] = rng_det_test.binomial(1, p_true_test)\n\n        # Explanation Data\n        F = {0, 3}\n        j = np.arange(d)\n        rng_exp = np.random.default_rng(13)\n\n        w_cal = 0.5 + 0.5 * (np.sin(0.5 * j[None, :]) + np.cos(0.02 * i_cal[:, None] + j[None, :] / 3))\n        u_cal = np.exp(w_cal)\n        e_cal = u_cal / u_cal.sum(axis=1, keepdims=True)\n        data['q_cal'] = e_cal[:, list(F)].sum(axis=1)\n        data['z_cal'] = rng_exp.binomial(1, data['q_cal'])\n\n        w_test = 0.5 + 0.5 * (np.sin(0.5 * j[None, :]) + np.cos(0.02 * i_test[:, None] + j[None, :] / 3))\n        u_test = np.exp(w_test)\n        e_test = u_test / u_test.sum(axis=1, keepdims=True)\n        data['q_test'] = e_test[:, list(F)].sum(axis=1)\n        data['z_test'] = rng_exp.binomial(1, data['q_test'])\n\n    elif case_id == 3:\n        # Detection Data\n        rng_det_cal = np.random.default_rng(23)\n        xi_cal = rng_det_cal.normal(0, 0.6, size=N_cal)\n        data['x_cal'] = 1.0 * t_cal + 0.3 * xi_cal\n        p_true_cal = sigmoid(3.0 * data['x_cal'] + 1.2)\n        data['y_cal'] = rng_det_cal.binomial(1, p_true_cal)\n\n        rng_det_test = np.random.default_rng(29)\n        xi_test = rng_det_test.normal(0, 0.6, size=N_test)\n        data['x_test'] = 1.0 * t_test + 0.3 * xi_test\n        p_true_test = sigmoid(3.0 * data['x_test'] + 1.2)\n        data['y_test'] = rng_det_test.binomial(1, p_true_test)\n\n        # Explanation Data\n        F = {2, 5}\n        j = np.arange(d)\n        \n        w_cal = 0.3 + np.sin(0.4 * j[None, :] * i_cal[:, None]) + 0.1 * np.cos(0.1 * i_cal[:, None] - 0.2 * j[None, :])\n        u_cal = np.exp(w_cal)\n        e_cal = u_cal / u_cal.sum(axis=1, keepdims=True)\n        data['q_cal'] = e_cal[:, list(F)].sum(axis=1)\n        p_true_exp_cal = np.clip(0.05 + 0.9 * np.power(data['q_cal'], 1/3), 0, 1)\n        rng_exp_cal = np.random.default_rng(31)\n        data['z_cal'] = rng_exp_cal.binomial(1, p_true_exp_cal)\n\n        w_test = 0.3 + np.sin(0.4 * j[None, :] * i_test[:, None]) + 0.1 * np.cos(0.1 * i_test[:, None] - 0.2 * j[None, :])\n        u_test = np.exp(w_test)\n        e_test = u_test / u_test.sum(axis=1, keepdims=True)\n        data['q_test'] = e_test[:, list(F)].sum(axis=1)\n        p_true_exp_test = np.clip(0.05 + 0.9 * np.power(data['q_test'], 1/3), 0, 1)\n        rng_exp_test = np.random.default_rng(37)\n        data['z_test'] = rng_exp_test.binomial(1, p_true_exp_test)\n\n    return data\n\ndef solve():\n    test_cases = [\n        {'id': 1, 'N_cal': 160, 'N_test': 160, 'd': 5},\n        {'id': 2, 'N_cal': 180, 'N_test': 180, 'd': 4},\n        {'id': 3, 'N_cal': 140, 'N_test': 140, 'd': 6},\n    ]\n\n    results = []\n    M_bins = 10\n\n    for case_config in test_cases:\n        data = generate_test_case_data(case_config)\n        \n        # --- 1. Detection Calibration ---\n        a_d, b_d = train_calibrator(data['x_cal'], data['y_cal'])\n        calibrated_probs_d = sigmoid(a_d * data['x_test'] + b_d)\n        ece_d = compute_ece(calibrated_probs_d, data['y_test'], M_bins)\n        \n        # --- 2. Explanation Calibration ---\n        # Note: q values are guaranteed to be in (0, 1) by construction\n        features_exp_cal = logit(data['q_cal'])\n        a_e, b_e = train_calibrator(features_exp_cal, data['z_cal'])\n        features_exp_test = logit(data['q_test'])\n        calibrated_probs_e = sigmoid(a_e * features_exp_test + b_e)\n        ece_e = compute_ece(calibrated_probs_e, data['z_test'], M_bins)\n        \n        results.extend([ece_d, ece_e])\n\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}