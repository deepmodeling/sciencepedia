{
    "hands_on_practices": [
        {
            "introduction": "在网络物理系统（CPS）中，精确定义奖励函数是设计智能控制器的首要挑战。逆强化学习（IRL）通过从专家演示中学习奖励函数，为解决这一问题提供了原则性方法。其中，最大熵（MaxEnt）IRL 框架尤为强大，它在解释专家行为的同时保持了最大的不确定性，有效解决了模糊性问题。这项练习  将引导你亲手实现最大熵 IRL 的核心计算引擎——前向-后向算法，用于计算配分函数和特征期望，这是理解并应用该学习范式的关键一步。",
            "id": "4212746",
            "problem": "考虑一个有限时域马尔可夫决策过程 (MDP)，它作为一个信息物理系统 (CPS) 的抽象数字孪生。该 MDP 有一个有限状态集 $\\mathcal{S}$、一个有限控制动作集 $\\mathcal{A}$、已知的转移概率 $P(s' \\mid s,a)$ 以及一个固定的时间范围 $T$。状态的初始分布为 $p_0(s)$，且 $\\sum_{s \\in \\mathcal{S}} p_0(s) = 1$。对于最大熵 (MaxEnt) 逆强化学习 (IRL)，假设轨迹上的奖励是可加的，其中每一步的奖励是特征的线性函数：$r(s,a) = \\boldsymbol{\\theta}^{\\top} \\mathbf{f}(s,a)$，其中 $\\mathbf{f}(s,a) \\in \\mathbb{R}^d$ 是一个固定的特征向量，$\\boldsymbol{\\theta} \\in \\mathbb{R}^d$ 是一个固定的参数向量。在最大熵分布下，一条轨迹 $(s_0,a_0,s_1,a_1,\\dots,s_T)$ 的未归一化权重是初始概率与转移概率的乘积，再乘以奖励总和的指数。配分函数 $Z$ 是所有可能轨迹的这些未归一化权重之和。期望特征计数是在归一化最大熵分布下，在时间步 $t \\in \\{0,\\dots,T-1\\}$ 上对 $\\mathbf{f}(s_t,a_t)$ 求和的期望值。\n\n从基本定义出发，实现一个前向-后向动态规划程序，用于计算：(i) 配分函数 $Z$，以及 (ii) 期望特征计数 $\\mathbb{E}\\left[\\sum_{t=0}^{T-1} \\mathbf{f}(s_t,a_t)\\right]$。你的实现必须从马尔可夫性质和最大熵轨迹分布（作为轨迹上的归一化吉布斯测度）的定义推导而来，不得假定任何快捷公式；从第一性原理推导所需的递归关系。\n\n你的程序必须使用以下参数值的测试套件。在所有情况下，状态集为 $\\mathcal{S} = \\{0,1,2\\}$，大小为 $|\\mathcal{S}| = 3$，动作集为 $\\mathcal{A} = \\{0,1\\}$，大小为 $|\\mathcal{A}| = 2$，转移概率 $P(s' \\mid s,a)$ 对每个 $(s,a)$ 指定为一个关于 $s' \\in \\{0,1,2\\}$ 的 3 维向量，其条目之和为 1。\n\n为每个动作定义转移动态 $P$ 如下：\n- 对于动作 $a = 0$：\n  - $P(0 \\mid 0,0) = 0.7$, $P(1 \\mid 0,0) = 0.2$, $P(2 \\mid 0,0) = 0.1$。\n  - $P(0 \\mid 1,0) = 0.1$, $P(1 \\mid 1,0) = 0.6$, $P(2 \\mid 1,0) = 0.3$。\n  - $P(0 \\mid 2,0) = 0.2$, $P(1 \\mid 2,0) = 0.3$, $P(2 \\mid 2,0) = 0.5$。\n- 对于动作 $a = 1$：\n  - $P(0 \\mid 0,1) = 0.2$, $P(1 \\mid 0,1) = 0.5$, $P(2 \\mid 0,1) = 0.3$。\n  - $P(0 \\mid 1,1) = 0.3$, $P(1 \\mid 1,1) = 0.4$, $P(2 \\mid 1,1) = 0.3$。\n  - $P(0 \\mid 2,1) = 0.4$, $P(1 \\mid 2,1) = 0.2$, $P(2 \\mid 2,1) = 0.4$。\n\n为每个 $(s,a)$ 定义特征映射 $\\mathbf{f}(s,a) \\in \\mathbb{R}^3$ 如下：\n- 如果 $a = 0$，$\\mathbf{f}(s,0) = [1,\\,0,\\,s]$。\n- 如果 $a = 1$，$\\mathbf{f}(s,1) = [0,\\,1,\\,s]$。\n\n三个测试用例如下：\n\n- 测试用例 1 (正常路径)：$T = 3$, $p_0 = [0.6,\\,0.3,\\,0.1]$, $\\boldsymbol{\\theta} = [0.5,\\,-0.2,\\,0.1]$。\n- 测试用例 2 (零奖励边界)：$T = 4$, $p_0 = [\\frac{1}{3},\\,\\frac{1}{3},\\,\\frac{1}{3}]$, $\\boldsymbol{\\theta} = [0,\\,0,\\,0]$。\n- 测试用例 3 (零时域边界)：$T = 0$, $p_0 = [0.2,\\,0.5,\\,0.3]$, $\\boldsymbol{\\theta} = [0.1,\\,0.2,\\,-0.3]$。\n\n要求：\n- 实现一个前向-后向动态规划程序，根据 MDP 的因子分解和最大熵轨迹加权，从第一性原理为每个测试用例计算 $Z$ 和期望特征计数向量。\n- 数值输出必须是浮点数，并四舍五入到 6 位小数。\n- 你的程序的最终输出必须是单行文本，其中包含一个结果列表，每个测试用例一个结果。每个结果是一个列表，包含 $Z$ 和其后的 3 个期望特征计数（按特征坐标顺序排列）。例如，格式应为 $[[Z_1,\\,e_{1,1},\\,e_{1,2},\\,e_{1,3}],\\,[Z_2,\\,e_{2,1},\\,e_{2,2},\\,e_{2,3}],\\,[Z_3,\\,e_{3,1},\\,e_{3,2},\\,e_{3,3}]]$.",
            "solution": "目标是在最大熵 (MaxEnt) 轨迹分布下，为有限时域马尔可夫决策过程 (MDP) 计算配分函数 $Z$ 和期望特征计数。这将通过从第一性原理推导并实现一个前向-后向动态规划算法来完成。\n\n设 MDP 由元组 $(\\mathcal{S}, \\mathcal{A}, P, p_0, T)$ 定义，其中 $\\mathcal{S}$ 是有限状态集，$\\mathcal{A}$ 是有限动作集，$P(s'|s,a)$ 是状态转移概率，$p_0(s)$ 是初始状态分布，$T$ 是时间范围。长度为 $T$ 的轨迹是一个序列 $\\tau = (s_0, a_0, s_1, a_1, \\ldots, a_{T-1}, s_T)$。\n\n奖励是特征的线性函数，$r(s,a) = \\boldsymbol{\\theta}^{\\top} \\mathbf{f}(s,a)$。根据最大熵原理，轨迹 $\\tau$ 的概率由吉布斯分布给出：\n$$p(\\tau) = \\frac{1}{Z} \\exp\\left(\\sum_{t=0}^{T-1} r(s_t, a_t)\\right) p_0(s_0) \\prod_{t=0}^{T-1} P(s_{t+1}|s_t, a_t)$$\n配分函数 $Z$ 是归一化常数，确保概率之和为一：\n$$Z = \\sum_{\\tau} p_0(s_0) \\prod_{t=0}^{T-1} P(s_{t+1}|s_t, a_t) \\exp\\left(\\sum_{t=0}^{T-1} r(s_t, a_t)\\right)$$\n求和遍历所有可能的轨迹 $\\tau$。求和项内部是轨迹的未归一化权重 $w(\\tau)$。\n\n问题要求计算 $Z$ 和期望特征计数 $\\mathbb{E}\\left[\\sum_{t=0}^{T-1} \\mathbf{f}(s_t,a_t)\\right]$。前向-后向算法适合此任务，因为它能在一个链式结构概率模型中高效地计算边缘概率。\n\n### 前向-后向算法的推导\n\n轨迹的未归一化权重可以按时间步进行分解：\n$$w(\\tau) = p_0(s_0) \\prod_{t=0}^{T-1} \\left[ P(s_{t+1}|s_t, a_t) \\exp(r(s_t, a_t)) \\right]$$\n\n**1. 前向传递：$\\alpha$ 消息**\n\n令 $\\alpha_t(s_t)$ 为从时间 $k=0$到 $k=t$ 且在状态 $s_t$ 结束的所有部分轨迹的未归一化权重之和。\n$$\\alpha_t(s_t) = \\sum_{s_0, a_0, \\ldots, s_{t-1}, a_{t-1}} p_0(s_0) \\prod_{k=0}^{t-1} \\left[ P(s_{k+1}|s_k, a_k) \\exp(r(s_k, a_k)) \\right]$$\n其中在 $k=t$ 时的最终状态 $s_k$ 被固定为 $s_t$。\n\n递归关系推导如下：\n\n-   **基例 ($t=0$):** 长度为 0 的部分轨迹仅为初始状态 $s_0$。其权重是其初始概率。\n    $$\\alpha_0(s_0) = p_0(s_0)$$\n\n-   **递归步骤 ($t \\to t+1$):** 在 $s_{t+1}$ 结束的部分轨迹的权重，是所有在时间 $t$ 结束于任意状态 $s_t$ 的轨迹权重之和，再通过一步 $(a_t, s_{t+1})$ 进行扩展。\n    $$\\alpha_{t+1}(s_{t+1}) = \\sum_{s_t \\in \\mathcal{S}} \\sum_{a_t \\in \\mathcal{A}} \\alpha_t(s_t) \\cdot P(s_{t+1}|s_t, a_t) \\exp(r(s_t, a_t))$$\n    此递归对 $t = 0, 1, \\ldots, T-1$ 执行。\n\n-   **配分函数 ($Z$):** 配分函数 $Z$ 是所有完整轨迹的权重之和。一条完整轨迹在时间 $T$ 结束。因此，$Z$ 是在最终时间步 $T$ 时所有 $\\alpha$ 消息之和。\n    $$Z = \\sum_{s_T \\in \\mathcal{S}} \\alpha_T(s_T)$$\n\n**2. 后向传递：$\\beta$ 消息**\n\n令 $\\beta_t(s_t)$ 为给定系统在时间 $t$ 处于状态 $s_t$ 的条件下，从时间 $k=t$ 到时间范围结束 $T$ 的所有未来部分轨迹的未归一化权重之和。\n$$\\beta_t(s_t) = \\sum_{a_t, s_{t+1}, \\ldots, a_{T-1}, s_T} \\prod_{k=t}^{T-1} \\left[ P(s_{k+1}|s_k, a_k) \\exp(r(s_k, a_k)) \\right]$$\n其中在 $k=t$ 时的初始状态 $s_k$ 被固定为 $s_t$。\n\n该递归通过时间上的后向推导得出：\n\n-   **基例 ($t=T$):** 在时间范围的末端，未来的部分轨迹为空。空集上的乘积为 1。\n    $$\\beta_T(s_T) = 1 \\quad \\forall s_T \\in \\mathcal{S}$$\n\n-   **递归步骤 ($t+1 \\to t$):** 为了计算 $\\beta_t(s_t)$，我们对所有可能的下一个动作 $a_t$ 和状态 $s_{t+1}$ 求和，将单步转移权重乘以未来路径其余部分的权重 $\\beta_{t+1}(s_{t+1})$。\n    $$\\beta_t(s_t) = \\sum_{a_t \\in \\mathcal{A}} \\sum_{s_{t+1} \\in \\mathcal{S}} P(s_{t+1}|s_t, a_t) \\exp(r(s_t, a_t)) \\cdot \\beta_{t+1}(s_{t+1})$$\n    此递归对 $t = T-1, T-2, \\ldots, 0$ 执行。\n\n**3. 计算期望特征计数**\n\n根据期望的线性性质，总期望特征计数是每个时间步的期望特征计数之和：\n$$\\mathbb{E}\\left[\\sum_{t=0}^{T-1} \\mathbf{f}(s_t,a_t)\\right] = \\sum_{t=0}^{T-1} \\mathbb{E}[\\mathbf{f}(s_t,a_t)]$$\n单个时间步的期望为：\n$$\\mathbb{E}[\\mathbf{f}(s_t,a_t)] = \\sum_{s \\in \\mathcal{S}} \\sum_{a \\in \\mathcal{A}} p(s_t=s, a_t=a) \\cdot \\mathbf{f}(s,a)$$\n我们需要边缘概率 $p(s_t=s, a_t=a)$。该概率是在时间 $t$ 经过状态-动作对 $(s,a)$ 的所有轨迹的归一化权重之和。该事件的未归一化权重 $w(s_t=s, a_t=a)$ 可以使用 $\\alpha$ 和 $\\beta$ 消息来表示。\n\n在时间 $t$ 导向状态 $s$ 的路径总权重是 $\\alpha_t(s)$。从时间 $t+1$ 的后续状态 $s'$ 到终点的路径权重是 $\\beta_{t+1}(s')$。它们之间的联系是从 $(s,a)$ 到 $s'$ 的转移。对所有可能的下一个状态 $s'$ 求和得到：\n$$w(s_t=s, a_t=a) = \\alpha_t(s) \\cdot \\exp(r(s,a)) \\cdot \\sum_{s' \\in \\mathcal{S}} P(s'|s,a) \\beta_{t+1}(s')$$\n通过 $Z$ 进行归一化得到边缘概率：\n$$p(s_t=s, a_t=a) = \\frac{w(s_t=s, a_t=a)}{Z} = \\frac{\\alpha_t(s) \\exp(r(s,a)) \\sum_{s' \\in \\mathcal{S}} P(s'|s,a) \\beta_{t+1}(s')}{Z}$$\n一个重要的恒等式 $\\sum_{s \\in \\mathcal{S}} \\alpha_t(s) \\beta_t(s) = Z$ (对于任意 $t \\in [0,T]$) 可用于验证计算的正确性。\n\n然后，通过对所有时间步求和来计算总期望特征计数：\n$$\\mathbb{E}_{\\text{total}} = \\sum_{t=0}^{T-1} \\sum_{s \\in \\mathcal{S}} \\sum_{a \\in \\mathcal{A}} p(s_t=s, a_t=a) \\cdot \\mathbf{f}(s,a)$$\n\n### 边界情况\n\n-   **时域 $T=0$：** 轨迹仅由一个初始状态 $s_0$ 组成。奖励总和与转移乘积均为空，其值分别为 0 和 1。轨迹 $(s_0)$ 的权重为 $p_0(s_0)$。配分函数为 $Z = \\sum_{s_0} p_0(s_0) = 1$。特征总和 $\\sum_{t=0}^{-1} \\mathbf{f}(s_t, a_t)$ 为空，因此期望特征计数是零向量。\n\n-   **零奖励 $\\boldsymbol{\\theta}=\\mathbf{0}$：** 如果 $\\boldsymbol{\\theta}=\\mathbf{0}$，则对于所有 $(s,a)$，$r(s,a)=0$，且 $\\exp(r(s,a))=1$。递归关系得以简化。一致性检查表明，$Z$ 的值应为 $|\\mathcal{A}|^T$，并且轨迹分布对应于执行一个均匀随机策略（以概率 $1/|\\mathcal{A}|$ 选择动作），然后在所有可能轨迹的集合上进行归一化。\n\n### 算法摘要\n\n1.  **初始化：** 给定 MDP 参数 $(\\mathcal{S}, \\mathcal{A}, P, p_0, T)$、特征 $\\mathbf{f}(s,a)$ 和权重 $\\boldsymbol{\\theta}$。令 $N_S = |\\mathcal{S}|$，$N_A = |\\mathcal{A}|$。\n2.  **处理 $T=0$：** 如果 $T=0$，则返回 $Z=1.0$ 和作为零向量的期望特征。\n3.  **前向传递：**\n    -   初始化大小为 $(T+1) \\times N_S$ 的 $\\alpha$ 矩阵。\n    -   对于 $s=0, \\ldots, N_S-1$，设置 $\\alpha[0, s] \\leftarrow p_0(s)$。\n    -   对于 $t$ 从 $0$ 到 $T-1$：\n        -   对每个 $s'$ 计算 $\\alpha[t+1, s'] \\leftarrow \\sum_{s,a} \\alpha[t, s] P(s'|s,a) \\exp(\\boldsymbol{\\theta}^{\\top}\\mathbf{f}(s,a))$。\n    -   计算 $Z \\leftarrow \\sum_s \\alpha[T, s]$。\n4.  **后向传递：**\n    -   初始化大小为 $(T+1) \\times N_S$ 的 $\\beta$ 矩阵。\n    -   对于 $s=0, \\ldots, N_S-1$，设置 $\\beta[T, s] \\leftarrow 1.0$。\n    -   对于 $t$ 从 $T-1$ 向下到 $0$：\n        -   对每个 $s$ 计算 $\\beta[t, s] \\leftarrow \\sum_{a,s'} P(s'|s,a) \\exp(\\boldsymbol{\\theta}^{\\top}\\mathbf{f}(s,a)) \\beta[t+1, s']$。\n5.  **计算期望：**\n    -   初始化 `total_expected_features` 向量为零。\n    -   对于 $t$ 从 $0$ 到 $T-1$：\n        -   对于每个状态 $s$ 和动作 $a$：\n            -   计算边缘概率 $p(s_t=s, a_t=a)$。\n            -   将 $p(s_t=s, a_t=a) \\cdot \\mathbf{f}(s,a)$ 加到 `total_expected_features` 中。\n6.  **返回** $Z$ 和 `total_expected_features`。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem for all test cases by implementing the forward-backward algorithm.\n    \"\"\"\n\n    # --- Problem Definition ---\n    num_states = 3\n    num_actions = 2\n    feat_dim = 3\n\n    # Transition probabilities P(s'|s,a) as a tensor of shape (s, a, s')\n    P = np.zeros((num_states, num_actions, num_states))\n    # a = 0\n    P[0, 0, :] = [0.7, 0.2, 0.1]\n    P[1, 0, :] = [0.1, 0.6, 0.3]\n    P[2, 0, :] = [0.2, 0.3, 0.5]\n    # a = 1\n    P[0, 1, :] = [0.2, 0.5, 0.3]\n    P[1, 1, :] = [0.3, 0.4, 0.3]\n    P[2, 1, :] = [0.4, 0.2, 0.4]\n\n    # Feature map f(s,a) as a tensor of shape (s, a, d)\n    F = np.zeros((num_states, num_actions, feat_dim))\n    for s in range(num_states):\n        # a = 0\n        F[s, 0, :] = [1, 0, s]\n        # a = 1\n        F[s, 1, :] = [0, 1, s]\n\n    # --- Test Cases ---\n    test_cases = [\n        {\n            \"T\": 3,\n            \"p0\": np.array([0.6, 0.3, 0.1]),\n            \"theta\": np.array([0.5, -0.2, 0.1]),\n        },\n        {\n            \"T\": 4,\n            \"p0\": np.array([1/3, 1/3, 1/3]),\n            \"theta\": np.array([0.0, 0.0, 0.0]),\n        },\n        {\n            \"T\": 0,\n            \"p0\": np.array([0.2, 0.5, 0.3]),\n            \"theta\": np.array([0.1, 0.2, -0.3]),\n        },\n    ]\n\n    results = []\n\n    for case in test_cases:\n        T = case[\"T\"]\n        p0 = case[\"p0\"]\n        theta = case[\"theta\"]\n\n        # Handle T=0 boundary case\n        if T == 0:\n            Z = 1.0\n            expected_features = np.zeros(feat_dim)\n            results.append([round(Z, 6)] + [round(ef, 6) for ef in expected_features])\n            continue\n\n        # Precompute reward-related term exp(theta^T * f(s,a))\n        # R has shape (s, a)\n        R = np.exp(np.einsum('sad,d->sa', F, theta))\n\n        # --- Forward Pass (alpha) ---\n        alpha = np.zeros((T + 1, num_states))\n        alpha[0, :] = p0\n\n        for t in range(T):\n            # alpha_{t+1}(s') = sum_{s,a} alpha_t(s) * R(s,a) * P(s'|s,a)\n            # einsum notation: s=state at t, a=action at t, p=state at t+1 (s')\n            alpha[t + 1, :] = np.einsum('s,sa,sap->p', alpha[t, :], R, P)\n        \n        # Partition function Z\n        Z = np.sum(alpha[T, :])\n\n        # --- Backward Pass (beta) ---\n        beta = np.zeros((T + 1, num_states))\n        beta[T, :] = 1.0\n\n        for t in range(T - 1, -1, -1):\n            # beta_t(s) = sum_{a,s'} R(s,a) * P(s'|s,a) * beta_{t+1}(s')\n            # einsum notation: s=state at t, a=action at t, p=state at t+1 (s')\n            beta[t, :] = np.einsum('sa,sap,p->s', R, P, beta[t + 1, :])\n        \n        # --- Expected Feature Counts ---\n        total_expected_features = np.zeros(feat_dim)\n        for t in range(T):\n            # sum_{s'} P(s'|s,a) * beta_{t+1}(s')\n            # einsum notation: p=state at t+1 (s')\n            beta_P_sum = np.einsum('p,sap->sa', beta[t + 1, :], P)\n            \n            # unnormalized probability of (s_t, a_t)\n            # using broadcasting for alpha[t,:]\n            prob_sa_unnorm = alpha[t, :, np.newaxis] * R * beta_P_sum\n            \n            # normalized probability p(s_t, a_t)\n            prob_sa = prob_sa_unnorm / Z\n            \n            # Expected features for this timestep\n            exp_feat_t = np.einsum('sa,sad->d', prob_sa, F)\n            total_expected_features += exp_feat_t\n\n        case_result = [round(Z, 6)] + [round(ef, 6) for ef in total_expected_features]\n        results.append(case_result)\n    \n    # Format the final output string as requested\n    output_str = \"[\" + \",\".join([f\"[{','.join(map(str, res))}]\" for res in results]) + \"]\"\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "在获得奖励函数后，可以利用基于模型的强化学习来规划最优策略，但网络物理系统的数字孪生模型终究是不完美的。这引出一个核心权衡：更长的规划视界可能带来更优的决策，但也可能因模型误差的累积而导致性能下降。这项实践  深入探讨了这一权衡，利用 Lipschitz 连续性来严格约束误差的传播，从而计算出保证策略单调改进所需的最小规划视界，这是实现安全可靠的基于模型强化学习的关键。",
            "id": "4212758",
            "problem": "考虑一个具有连续状态空间的确定性信息物理系统 (CPS)，其受真实动态 $f: \\mathbb{R}^{n} \\times \\mathbb{R}^{m} \\rightarrow \\mathbb{R}^{n}$ 控制，并由状态反馈策略 $\\pi: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{m}$ 调控。在一个基于模型的强化学习 (RL) 过程中，使用一个数字孪生模型 $\\hat{f}$ 进行规划。假设以下基本属性成立：\n\n- 真实动态在状态和动作上是利普希茨连续的，其利普希茨常数分别为 $L_{x} \\ge 0$ 和 $L_{a} \\ge 0$，因此对于所有的 $(x,a)$ 和 $(x',a')$，\n$$\n\\| f(x,a) - f(x',a') \\| \\le L_{x} \\| x - x' \\| + L_{a} \\| a - a' \\| .\n$$\n- 策略是利普希茨连续的，其利普希茨常数为 $L_{\\pi} \\ge 0$，因此对于所有的 $x,x'$，\n$$\n\\| \\pi(x) - \\pi(x') \\| \\le L_{\\pi} \\| x - x' \\| .\n$$\n- 数字孪生在任何状态-动作对上都有一个均匀的单步闭环模型缺陷上界 $\\epsilon_{m} \\ge 0$，因此对于所有的 $(x,a)$，\n$$\n\\| f(x,a) - \\hat{f}(x,a) \\| \\le \\epsilon_{m} .\n$$\n- 即时奖励 $r(x,a)$ 在状态和动作上是利普希茨连续的，其利普希茨常数分别为 $L_{r,x} \\ge 0$ 和 $L_{r,a} \\ge 0$。\n- 折扣因子满足 $\\gamma \\in (0,1)$，并且用于自举的评论家（状态价值函数） $V(x)$ 是利普希茨连续的，其利普希茨常数为 $L_{V} \\ge 0$。\n\n定义闭环收缩常数 $L_{c} \\triangleq L_{x} + L_{a} L_{\\pi}$ 和闭环策略下的奖励敏感度 $L_{r,c} \\triangleq L_{r,x} + L_{r,a} L_{\\pi}$。假设 $L_{c}  1$。\n\n一个基于模型的规划器在更新后的策略 $\\pi'$ 下，使用数字孪生模型 $\\hat{f}$ 执行长度为 $H$ 的轨迹展开，并在深度 $H$ 处使用评论家 $V$ 进行自举。设 $\\alpha  0$ 表示在初始状态分布上，规划器在使用数字孪生模型时的代理目标中经认证的最小改进裕度，这意味着代理目标预测至少有 $\\alpha$ 的改进。\n\n从上述假设出发，并仅使用利普希茨连续性、折扣回报和带自举的基于模型的轨迹展开的标准定义，通过使用 $L_{c}$、$L_{r,c}$ 和 $L_{V}$ 显式地界定在整个时域上的累积预测误差，推导出一个能保证真实折扣回报单调改进的最小整数规划时域 $H_{\\min}$ 的保守闭式表达式。假设在 $L_{V}  \\frac{L_{r,c}}{1 - \\gamma}$ 和\n$$\n\\frac{\\epsilon_{m}}{1 - L_{c}} \\cdot \\frac{L_{r,c}}{1 - \\gamma}  \\alpha  \\frac{\\epsilon_{m}}{1 - L_{c}} \\cdot L_{V} ,\n$$\n的情况下，因此需要一个非平凡的有限 $H_{\\min}$。\n\n你的最终答案必须是关于 $\\gamma$, $L_{x}$, $L_{a}$, $L_{\\pi}$, $L_{r,x}$, $L_{r,a}$, $L_{V}$ 和 $\\epsilon_{m}$ 的 $H_{\\min}$ 的单一闭式解析表达式。不需要数值。将最终答案表示为不带单位的单一解析表达式。如果需要使用实数的上取整来产生整数时域，请使用标准上取整运算符。最终答案必须是单一表达式。",
            "solution": "该问题要求在给定一个由基于模型的规划器预测的、经认证的最小改进 $\\alpha$ 的条件下，推导能保证真实策略价值单调改进的最小规划时域 $H_{\\min}$。如果新策略的真实价值不低于旧策略的价值，则单调改进得到保证。\n\n设策略 $\\pi$ 的规划器代理目标表示为 $\\hat{J}_H(x_0)$，其中 $H$ 是规划时域。该代理目标预测相对于某个基线（我们可以将其视为先前策略的价值）至少有 $\\alpha$ 的改进。为使保证成立，真实改进必须为非负。这可以表示为：\n$$ \\text{真实改进} = \\text{预测改进} + (\\text{真实改进} - \\text{预测改进}) \\ge 0 $$\n鉴于预测改进至少为 $\\alpha$，我们有：\n$$ \\text{真实改进} \\ge \\alpha - | \\text{预测误差} | $$\n因此，保证真实改进为非负的一个充分条件是预测误差的大小以 $\\alpha$ 为界。\n$$ | \\text{预测误差} | \\le \\alpha $$\n\n问题的核心是推导出此预测误差作为时域 $H$ 的函数的上界，然后找到满足该不等式的最小整数 $H$。\n\n让我们定义相关的价值函数。规划器使用数字孪生模型 $\\hat{f}$ 执行长度为 $H$ 的轨迹展开，并使用评论家 $V$ 进行自举。初始状态 $x_0$ 处的代理价值为：\n$$ \\hat{J}_H(x_0) = \\sum_{t=0}^{H-1} \\gamma^t r(\\hat{x}_t, \\pi(\\hat{x}_t)) + \\gamma^H V(\\hat{x}_H) $$\n其中预测轨迹 $\\{\\hat{x}_t\\}_{t=0}^H$ 由 $\\hat{x}_{t+1} = \\hat{f}(\\hat{x}_t, \\pi(\\hat{x}_t))$ 生成，且 $\\hat{x}_0 = x_0$。\n\n为了形式化预测误差，“真实”价值必须被一致地定义。最直接的比较是与一个使用真实动态 $f$ 但具有相同有限时域结构和自举函数 $V$ 的价值函数进行比较。设其为 $J_H(x_0)$:\n$$ J_H(x_0) = \\sum_{t=0}^{H-1} \\gamma^t r(x_t, \\pi(x_t)) + \\gamma^H V(x_H) $$\n其中真实轨迹 $\\{x_t\\}_{t=0}^H$ 由 $x_{t+1} = f(x_t, \\pi(x_t))$ 生成，且 $x_0 = \\hat{x}_0$。\n\n预测误差为 $|J_H(x_0) - \\hat{J}_H(x_0)|$。我们可以将其界定为：\n$$ |J_H(x_0) - \\hat{J}_H(x_0)| \\le \\left| \\sum_{t=0}^{H-1} \\gamma^t (r(x_t, \\pi(x_t)) - r(\\hat{x}_t, \\pi(\\hat{x}_t))) \\right| + \\gamma^H |V(x_H) - V(\\hat{x}_H)| $$\n$$ \\le \\sum_{t=0}^{H-1} \\gamma^t |r(x_t, \\pi(x_t)) - r(\\hat{x}_t, \\pi(\\hat{x}_t))| + \\gamma^H |V(x_H) - V(\\hat{x}_H)| $$\n\n为了界定这些项，我们首先需要界定状态偏差 $\\delta_t = \\|x_t - \\hat{x}_t\\|$。我们有 $\\delta_0 = 0$。对于 $t \\ge 0$:\n$$ \\delta_{t+1} = \\|x_{t+1} - \\hat{x}_{t+1}\\| = \\|f(x_t, \\pi(x_t)) - \\hat{f}(\\hat{x}_t, \\pi(\\hat{x}_t))\\| $$\n使用三角不等式：\n$$ \\delta_{t+1} \\le \\|f(x_t, \\pi(x_t)) - f(\\hat{x}_t, \\pi(\\hat{x}_t))\\| + \\|f(\\hat{x}_t, \\pi(\\hat{x}_t)) - \\hat{f}(\\hat{x}_t, \\pi(\\hat{x}_t))\\| $$\n第一项使用 $f$ 和 $\\pi$ 的利普希茨性质来界定：\n$$ \\|f(x_t, \\pi(x_t)) - f(\\hat{x}_t, \\pi(\\hat{x}_t))\\| \\le L_x\\|x_t - \\hat{x}_t\\| + L_a\\|\\pi(x_t) - \\pi(\\hat{x}_t)\\| \\le L_x\\delta_t + L_a(L_\\pi \\delta_t) = (L_x + L_a L_\\pi)\\delta_t = L_c \\delta_t $$\n第二项是模型缺陷，以 $\\epsilon_m$ 为界。这产生了递推关系：\n$$ \\delta_{t+1} \\le L_c \\delta_t + \\epsilon_m $$\n从 $\\delta_0 = 0$ 开始展开此递推关系得到 $\\delta_t \\le \\epsilon_m \\sum_{k=0}^{t-1} L_c^k = \\epsilon_m \\frac{1-L_c^t}{1-L_c}$。由于 $L_c  1$，我们可以为所有 $t \\ge 1$ 建立一个统一的保守上界：\n$$ \\delta_t \\le \\frac{\\epsilon_m}{1-L_c} $$\n\n现在，我们界定预测误差表达式中的各项。奖励差异使用其利普希茨性质来界定：\n$$ |r(x_t, \\pi(x_t)) - r(\\hat{x}_t, \\pi(\\hat{x}_t))| \\le L_{r,x}\\|x_t - \\hat{x}_t\\| + L_{r,a}\\|\\pi(x_t) - \\pi(\\hat{x}_t)\\| \\le (L_{r,x} + L_{r,a}L_\\pi)\\delta_t = L_{r,c}\\delta_t $$\n自举价值差异使用 $V$ 的利普希茨性质来界定：\n$$ |V(x_H) - V(\\hat{x}_H)| \\le L_V \\|x_H - \\hat{x}_H\\| = L_V \\delta_H $$\n将这些代入误差界并使用 $\\delta_t$ 的统一上界：\n$$ |J_H(x_0) - \\hat{J}_H(x_0)| \\le \\sum_{t=0}^{H-1} \\gamma^t (L_{r,c} \\delta_t) + \\gamma^H (L_V \\delta_H) $$\n$$ \\le \\sum_{t=0}^{H-1} \\gamma^t L_{r,c} \\left( \\frac{\\epsilon_m}{1-L_c} \\right) + \\gamma^H L_V \\left( \\frac{\\epsilon_m}{1-L_c} \\right) $$\n提取公因式：\n$$ |J_H(x_0) - \\hat{J}_H(x_0)| \\le \\frac{\\epsilon_m}{1-L_c} \\left( L_{r,c} \\sum_{t=0}^{H-1} \\gamma^t + L_V \\gamma^H \\right) $$\n等比数列的和为 $\\sum_{t=0}^{H-1} \\gamma^t = \\frac{1-\\gamma^H}{1-\\gamma}$。所以误差界为：\n$$ \\text{Error}(H) = \\frac{\\epsilon_m}{1-L_c} \\left( L_{r,c} \\frac{1-\\gamma^H}{1-\\gamma} + L_V \\gamma^H \\right) $$\n\n为保证改进，我们要求 $\\text{Error}(H) \\le \\alpha$:\n$$ \\frac{\\epsilon_m}{1-L_c} \\left( \\frac{L_{r,c}}{1-\\gamma} - \\frac{L_{r,c}\\gamma^H}{1-\\gamma} + L_V \\gamma^H \\right) \\le \\alpha $$\n整理各项以分离出 $H$：\n$$ \\frac{L_{r,c}}{1-\\gamma} + \\gamma^H \\left( L_V - \\frac{L_{r,c}}{1-\\gamma} \\right) \\le \\frac{\\alpha(1-L_c)}{\\epsilon_m} $$\n$$ \\gamma^H \\left( L_V - \\frac{L_{r,c}}{1-\\gamma} \\right) \\le \\frac{\\alpha(1-L_c)}{\\epsilon_m} - \\frac{L_{r,c}}{1-\\gamma} $$\n问题陈述了 $L_V  \\frac{L_{r,c}}{1-\\gamma}$，因此乘以 $\\gamma^H$ 的项是正的。令其为 $\\Delta_V = L_V - \\frac{L_{r,c}}{1-\\gamma}  0$。\n问题还陈述了 $\\alpha  \\frac{\\epsilon_m}{1-L_c} \\frac{L_{r,c}}{1-\\gamma}$，这意味着 $\\frac{\\alpha(1-L_c)}{\\epsilon_m}  \\frac{L_{r,c}}{1-\\gamma}$。因此，右侧是正的。\n设右侧为 $R = \\frac{\\alpha(1-L_c)}{\\epsilon_m} - \\frac{L_{r,c}}{1-\\gamma}  0$。不等式变为：\n$$ \\gamma^H \\Delta_V \\le R \\implies \\gamma^H \\le \\frac{R}{\\Delta_V} $$\n对两边取自然对数：\n$$ H \\ln(\\gamma) \\le \\ln\\left(\\frac{R}{\\Delta_V}\\right) $$\n由于 $\\gamma \\in (0,1)$，$\\ln(\\gamma)$ 是负数。除以 $\\ln(\\gamma)$ 会反转不等号的方向：\n$$ H \\ge \\frac{\\ln\\left(\\frac{R}{\\Delta_V}\\right)}{\\ln(\\gamma)} $$\n将 $R$ 和 $\\Delta_V$ 的表达式代回：\n$$ H \\ge \\frac{\\ln\\left( \\frac{\\frac{\\alpha(1-L_c)}{\\epsilon_m} - \\frac{L_{r,c}}{1-\\gamma}}{L_V - \\frac{L_{r,c}}{1-\\gamma}} \\right)}{\\ln(\\gamma)} $$\n为了简化对数的参数，我们可以将其分子和分母同乘以 $(1-\\gamma)$:\n$$ H \\ge \\frac{\\ln\\left( \\frac{\\frac{\\alpha(1-L_c)(1-\\gamma)}{\\epsilon_m} - L_{r,c}}{L_V(1-\\gamma) - L_{r,c}} \\right)}{\\ln(\\gamma)} $$\n满足此条件的最小整数 $H$ 是右侧的上取整。我们定义 $L_c = L_x + L_a L_\\pi$ 和 $L_{r,c} = L_{r,x} + L_{r,a} L_\\pi$。我们得到 $H_{\\min}$ 的最终表达式：\n$$ H_{\\min} = \\left\\lceil \\frac{\\ln\\left( \\frac{\\frac{\\alpha(1-(L_x+L_aL_\\pi))(1-\\gamma)}{\\epsilon_m} - (L_{r,x}+L_{r,a}L_\\pi)}{L_V(1-\\gamma) - (L_{r,x}+L_{r,a}L_\\pi)} \\right)}{\\ln(\\gamma)} \\right\\rceil $$\n条件 $\\alpha  \\frac{\\epsilon_m}{1-L_c} L_V$ 确保了对数内部的分子小于分母，使得该分数小于 1，其对数为负。这导致 $H$ 的一个正下界，与一个非平凡的规划时域是一致的。",
            "answer": "$$\\boxed{\\left\\lceil \\frac{\\ln\\left( \\frac{\\frac{\\alpha(1-(L_x+L_a L_\\pi))(1-\\gamma)}{\\epsilon_m} - (L_{r,x}+L_{r,a} L_\\pi)}{L_V(1-\\gamma) - (L_{r,x}+L_{r,a} L_\\pi)} \\right)}{\\ln(\\gamma)} \\right\\rceil}$$"
        },
        {
            "introduction": "在将新策略部署到实际物理系统之前，必须通过离线评估来确保其安全性和有效性，这是一个不可或缺的步骤。离策略评估（Off-Policy Evaluation, OPE）中的重要性采样（Importance Sampling, IS）是完成此任务的标准工具，它利用在现有策略下收集的数据来评估新策略。这项练习  提供了对不同重要性采样估计器统计特性（偏差与方差）的严谨分析，理解这些特性对于为网络物理系统选择合适的估计器和量化性能预测的不确定性至关重要。",
            "id": "4212729",
            "problem": "一个信息物理系统（CPS）的数字孪生（DT）被用于通过重要性采样（IS）在离线策略（off-policy）设置下评估一个深度目标策略。考虑一个有限时域强化学习（RL）任务，其时域长度为 $H \\in \\mathbb{N}$，折扣因子为 $\\gamma \\in (0,1)$，以及在行为策略下收集的 $N \\in \\mathbb{N}$ 条独立同分布的轨迹。为时间步 $t \\in \\{0,1,\\dots,H-1\\}$ 和轨迹索引 $i \\in \\{1,2,\\dots,N\\}$ 定义逐决策IS比率 $\\rho_{t}^{(i)} = \\pi(a_{t}^{(i)} \\mid s_{t}^{(i)}) / \\mu(a_{t}^{(i)} \\mid s_{t}^{(i)})$。设逐决策累积权重为 $W_{t}^{(i)} = \\prod_{k=0}^{t} \\rho_{k}^{(i)}$，每步奖励为 $r_{t}^{(i)}$。目标策略折扣回报的普通逐决策重要性采样（PDIS）估计量和加权（自归一化）PDIS估计量为\n$$\n\\hat{J}^{o} = \\sum_{t=0}^{H-1} \\gamma^{t} \\left( \\frac{1}{N} \\sum_{i=1}^{N} W_{t}^{(i)} r_{t}^{(i)} \\right),\n\\qquad\n\\hat{J}^{w} = \\sum_{t=0}^{H-1} \\gamma^{t} \\left( \\frac{ \\sum_{i=1}^{N} W_{t}^{(i)} r_{t}^{(i)} }{ \\sum_{i=1}^{N} W_{t}^{(i)} } \\right).\n$$\n假设以下适用于基于DT的CPS评估的基础建模条件：\n- 逐决策比率 $\\rho_{t}^{(i)}$ 在时间和轨迹上是独立的，对于所有 $t$，都有 $\\mathbb{E}[\\rho_{t}^{(i)}] = 1$ 和 $\\operatorname{Var}(\\rho_{t}^{(i)}) = \\sigma_{\\rho}^{2} \\in [0,\\infty)$，并且在 $t$ 上的独立性意味着 $W_{t}^{(i)}$ 具有 $\\mathbb{E}[W_{t}^{(i)}] = 1$ 和二阶矩 $\\mathbb{E}\\!\\left[(W_{t}^{(i)})^{2}\\right] = \\left(1 + \\sigma_{\\rho}^{2} \\right)^{t+1}$。\n- 随机变量 $r_{t}^{(i)}$ 在时间和轨迹上是独立的，并且对于每个固定的 $t$ 都独立于 $W_{t}^{(i)}$，其目标策略均值为 $m_{t} = \\mathbb{E}_{\\pi}[r_{t}]$，方差为 $v_{t} = \\operatorname{Var}_{\\pi}(r_{t})$，从而得到 $\\mathbb{E}\\!\\left[ (r_{t}^{(i)})^{2} \\right] = v_{t} + m_{t}^{2}$。\n- 目标策略的真实折扣回报为 $J^{\\pi} = \\sum_{t=0}^{H-1} \\gamma^{t} m_{t}$。\n\n从这些定义以及适用于比率估计量的大数定律和delta方法渐近性出发，执行以下操作：\n1. 以 $H$、$\\gamma$、$N$、$\\sigma_{\\rho}^{2}$、$\\{m_{t}\\}_{t=0}^{H-1}$ 和 $\\{v_{t}\\}_{t=0}^{H-1}$ 的闭式形式推导 $\\hat{J}^{o}$ 的偏差和方差。\n2. 在所述的独立性假设下，使用均值比展开法，推导 $\\hat{J}^{w}$ 的一阶（关于 $1/N$）偏差和方差。\n3. 使用你的结果，为普通和加权PDIS估计量之间的均方误差（MSE）的首项差异提供一个闭式表达式，定义为 $\\Delta_{\\mathrm{MSE}} = \\mathrm{MSE}(\\hat{J}^{o}) - \\mathrm{MSE}(\\hat{J}^{w})$，保留到 $1/N$ 阶的项并忽略更高阶的项。\n\n你的最终答案必须是 $\\Delta_{\\mathrm{MSE}}$ 作为 $H$、$\\gamma$、$N$、$\\sigma_{\\rho}^{2}$、$\\{m_{t}\\}$ 和 $\\{v_{t}\\}$ 的函数的单个闭式解析表达式，并且不应包含任何单位。无需四舍五入。",
            "solution": "总体目标是求出均方误差（MSE）的首项差异，$\\Delta_{\\mathrm{MSE}} = \\mathrm{MSE}(\\hat{J}^{o}) - \\mathrm{MSE}(\\hat{J}^{w})$。这需要推导每个估计量的偏差和方差。一个参数 $\\theta$ 的估计量 $\\hat{\\theta}$ 的MSE由 $\\mathrm{MSE}(\\hat{\\theta}) = \\operatorname{Var}(\\hat{\\theta}) + (\\operatorname{Bias}(\\hat{\\theta}))^2$ 给出，其中 $\\operatorname{Bias}(\\hat{\\theta}) = \\mathbb{E}[\\hat{\\theta}] - \\theta$。\n\n我们首先根据问题的给定条件建立必要的矩。\n随机变量是为每条轨迹 $i \\in \\{1,\\dots,N\\}$ 和时间步 $t \\in \\{0, \\dots, H-1\\}$ 定义的。期望 $\\mathbb{E}[\\cdot]$ 是相对于行为策略 $\\mu$ 导出的分布来计算的。\n-   重要性权重：$W_{t}^{(i)} = \\prod_{k=0}^{t} \\rho_{k}^{(i)}$。给定 $\\rho_{k}^{(i)}$ 是独立同分布的，其 $\\mathbb{E}[\\rho_{k}^{(i)}] = 1$ 和 $\\operatorname{Var}(\\rho_{k}^{(i)}) = \\sigma_{\\rho}^{2}$。\n    $\\mathbb{E}[W_{t}^{(i)}] = \\prod_{k=0}^{t} \\mathbb{E}[\\rho_{k}^{(i)}] = 1^{t+1} = 1$。\n    $\\mathbb{E}[(W_{t}^{(i)})^2] = \\prod_{k=0}^{t} \\mathbb{E}[(\\rho_{k}^{(i)})^2] = \\prod_{k=0}^{t} (\\operatorname{Var}(\\rho_{k}^{(i)}) + (\\mathbb{E}[\\rho_{k}^{(i)}])^2) = (\\sigma_{\\rho}^{2}+1)^{t+1}$。\n    $\\operatorname{Var}(W_{t}^{(i)}) = \\mathbb{E}[(W_{t}^{(i)})^2] - (\\mathbb{E}[W_{t}^{(i)}])^2 = (1+\\sigma_{\\rho}^{2})^{t+1} - 1$。\n-   奖励：$r_{t}^{(i)}$。问题假设对于固定的 $t$，$r_t^{(i)}$ 和 $W_t^{(i)}$ 是独立的。重要性采样的基本恒等式是 $\\mathbb{E}[W_{t}^{(i)} r_{t}^{(i)}] = \\mathbb{E}_{\\pi}[r_{t}] = m_{t}$。利用独立性假设，我们有 $\\mathbb{E}[W_{t}^{(i)} r_{t}^{(i)}] = \\mathbb{E}[W_{t}^{(i)}] \\mathbb{E}[r_{t}^{(i)}] = 1 \\cdot \\mathbb{E}[r_{t}^{(i)}]$。这意味着 $\\mathbb{E}[r_{t}^{(i)}] = m_{t}$。\n-   问题提供了 $\\mathbb{E}[(r_{t}^{(i)})^2] = v_{t} + m_{t}^{2}$。这是在行为策略分布下的二阶矩。因此，在行为策略下奖励的方差为 $\\operatorname{Var}(r_{t}^{(i)}) = \\mathbb{E}[(r_{t}^{(i)})^2] - (\\mathbb{E}[r_{t}^{(i)}])^2 = (v_{t}+m_{t}^2) - m_{t}^2 = v_{t}$。\n-   待估计的真实值为 $J^{\\pi} = \\sum_{t=0}^{H-1} \\gamma^{t} m_{t}$。\n\n我们继续处理问题的三个部分。\n\n1. $\\hat{J}^{o}$的偏差和方差的推导。\n\n普通PDIS估计量为 $\\hat{J}^{o} = \\sum_{t=0}^{H-1} \\gamma^{t} \\left( \\frac{1}{N} \\sum_{i=1}^{N} W_{t}^{(i)} r_{t}^{(i)} \\right)$。\n为了求偏差，我们计算其期望：\n$$\n\\mathbb{E}[\\hat{J}^{o}] = \\mathbb{E}\\left[ \\sum_{t=0}^{H-1} \\gamma^{t} \\left( \\frac{1}{N} \\sum_{i=1}^{N} W_{t}^{(i)} r_{t}^{(i)} \\right) \\right] = \\sum_{t=0}^{H-1} \\gamma^{t} \\frac{1}{N} \\sum_{i=1}^{N} \\mathbb{E}[W_{t}^{(i)} r_{t}^{(i)}]\n$$\n由于轨迹是独立同分布的，$\\mathbb{E}[W_{t}^{(i)} r_{t}^{(i)}]$ 对所有 $i$ 都是相同的。使用IS恒等式，$\\mathbb{E}[W_{t}^{(i)} r_{t}^{(i)}] = m_{t}$。\n$$\n\\mathbb{E}[\\hat{J}^{o}] = \\sum_{t=0}^{H-1} \\gamma^{t} \\frac{1}{N} (N \\cdot m_{t}) = \\sum_{t=0}^{H-1} \\gamma^{t} m_{t} = J^{\\pi}\n$$\n偏差为 $\\operatorname{Bias}(\\hat{J}^{o}) = \\mathbb{E}[\\hat{J}^{o}] - J^{\\pi} = 0$。$\\hat{J}^{o}$ 是一个无偏估计量。\n\n为了求方差，我们假设不同时间步的项是独立的，这是一个由问题陈述的简化假设。\n$$\n\\operatorname{Var}(\\hat{J}^{o}) = \\operatorname{Var}\\left( \\sum_{t=0}^{H-1} \\gamma^{t} \\frac{1}{N} \\sum_{i=1}^{N} W_{t}^{(i)} r_{t}^{(i)} \\right) = \\sum_{t=0}^{H-1} \\gamma^{2t} \\operatorname{Var}\\left( \\frac{1}{N} \\sum_{i=1}^{N} W_{t}^{(i)} r_{t}^{(i)} \\right)\n$$\n由于轨迹是独立同分布的，均值的方差是单个项的方差除以 $N$。\n$$\n\\operatorname{Var}(\\hat{J}^{o}) = \\sum_{t=0}^{H-1} \\gamma^{2t} \\frac{1}{N} \\operatorname{Var}(W_{t}^{(i)} r_{t}^{(i)})\n$$\n我们计算乘积项的方差：\n$$\n\\operatorname{Var}(W_{t}^{(i)} r_{t}^{(i)}) = \\mathbb{E}[(W_{t}^{(i)} r_{t}^{(i)})^2] - (\\mathbb{E}[W_{t}^{(i)} r_{t}^{(i)}])^2\n$$\n利用 $W_t^{(i)}$ 和 $r_t^{(i)}$ 的独立性，我们得到 $\\mathbb{E}[(W_{t}^{(i)} r_{t}^{(i)})^2] = \\mathbb{E}[(W_{t}^{(i)})^2] \\mathbb{E}[(r_{t}^{(i)})^2]$。代入已知的矩：\n$$\n\\mathbb{E}[(W_{t}^{(i)} r_{t}^{(i)})^2] = \\left( (1+\\sigma_{\\rho}^{2})^{t+1} \\right) \\left( v_{t} + m_{t}^{2} \\right)\n$$\n并且 $\\mathbb{E}[W_t^{(i)} r_t^{(i)}] = m_t$。综合起来：\n$$\n\\operatorname{Var}(W_{t}^{(i)} r_{t}^{(i)}) = (1+\\sigma_{\\rho}^{2})^{t+1} (v_{t} + m_{t}^{2}) - m_{t}^{2}\n$$\n所以，$\\hat{J}^{o}$ 的方差为：\n$$\n\\operatorname{Var}(\\hat{J}^{o}) = \\frac{1}{N} \\sum_{t=0}^{H-1} \\gamma^{2t} \\left[ (1+\\sigma_{\\rho}^{2})^{t+1} (v_{t} + m_{t}^{2}) - m_{t}^{2} \\right]\n$$\n\n2. $\\hat{J}^{w}$的一阶偏差和方差的推导。\n\n加权PDIS估计量为 $\\hat{J}^{w} = \\sum_{t=0}^{H-1} \\gamma^{t} \\hat{m}_{t}$，其中 $\\hat{m}_{t} = \\frac{ \\sum_{i=1}^{N} W_{t}^{(i)} r_{t}^{(i)} }{ \\sum_{i=1}^{N} W_{t}^{(i)} } = \\frac{A_{t}}{B_{t}}$。令 $A_t = \\frac{1}{N}\\sum_i W_t^{(i)}r_t^{(i)}$ 和 $B_t = \\frac{1}{N}\\sum_i W_t^{(i)}$。我们对两个随机变量之比使用泰勒展开。对于 $\\hat{m}_t = A_t/B_t$，一阶偏差为：\n$$\n\\operatorname{Bias}(\\hat{m}_{t}) \\approx \\frac{\\mathbb{E}[A_{t}]}{\\mathbb{E}[B_{t}]^3} \\operatorname{Var}(B_{t}) - \\frac{1}{\\mathbb{E}[B_{t}]^2} \\operatorname{Cov}(A_{t}, B_{t})\n$$\n我们有 $\\mathbb{E}[A_t] = m_t$ 和 $\\mathbb{E}[B_t] = 1$。$\\operatorname{Var}(B_{t}) = \\frac{1}{N} \\operatorname{Var}(W_t^{(i)}) = \\frac{1}{N}((1+\\sigma_{\\rho}^{2})^{t+1} - 1)$。\n协方差项为 $\\operatorname{Cov}(A_{t}, B_{t}) = \\frac{1}{N} \\operatorname{Cov}(W_{t}^{(i)} r_{t}^{(i)}, W_{t}^{(i)})$。\n$$\n\\operatorname{Cov}(W_{t}^{(i)} r_{t}^{(i)}, W_{t}^{(i)}) = \\mathbb{E}[W_{t}^{(i)} r_{t}^{(i)} \\cdot W_{t}^{(i)}] - \\mathbb{E}[W_{t}^{(i)} r_{t}^{(i)}] \\mathbb{E}[W_{t}^{(i)}]\n$$\n$$\n= \\mathbb{E}[(W_{t}^{(i)})^2 r_{t}^{(i)}] - m_{t} \\cdot 1 = \\mathbb{E}[(W_{t}^{(i)})^2] \\mathbb{E}[r_{t}^{(i)}] - m_{t} = (1+\\sigma_{\\rho}^{2})^{t+1} m_{t} - m_{t} = m_{t}((1+\\sigma_{\\rho}^{2})^{t+1} - 1)\n$$\n将这些代入 $\\hat{m}_t$ 的偏差公式：\n$$\n\\operatorname{Bias}(\\hat{m}_{t}) \\approx \\frac{m_{t}}{1^3} \\frac{1}{N}((1+\\sigma_{\\rho}^{2})^{t+1} - 1) - \\frac{1}{1^2} \\frac{1}{N}m_{t}((1+\\sigma_{\\rho}^{2})^{t+1} - 1) = 0\n$$\n$\\hat{m}_{t}$ 的一阶偏差为零。因此，$\\hat{J}^{w} = \\sum_t \\gamma^t \\hat{m}_t$ 的一阶偏差也为零。偏差的阶数为 $O(1/N^2)$。\n\n比率估计量 $\\hat{m}_t$ 的一阶方差为：\n$$\n\\operatorname{Var}(\\hat{m}_{t}) \\approx \\frac{1}{\\mathbb{E}[B_{t}]^2} \\operatorname{Var}(A_t - \\frac{\\mathbb{E}[A_t]}{\\mathbb{E}[B_t]} B_t) = \\operatorname{Var}(A_t - m_t B_t)\n$$\n$A_t - m_t B_t = \\frac{1}{N} \\sum_i (W_t^{(i)} r_t^{(i)} - m_t W_t^{(i)}) = \\frac{1}{N} \\sum_i W_t^{(i)}(r_t^{(i)} - m_t)$。由于轨迹是独立同分布的：\n$$\n\\operatorname{Var}(A_t - m_t B_t) = \\frac{1}{N} \\operatorname{Var}(W_t^{(i)}(r_t^{(i)} - m_t))\n$$\n该项的均值为 $\\mathbb{E}[W_t^{(i)}(r_t^{(i)} - m_t)] = \\mathbb{E}[W_t^{(i)}r_t^{(i)}] - m_t\\mathbb{E}[W_t^{(i)}] = m_t - m_t \\cdot 1 = 0$。\n所以它的方差就是它的二阶矩：\n$$\n\\operatorname{Var}(W_t^{(i)}(r_t^{(i)} - m_t)) = \\mathbb{E}[(W_t^{(i)}(r_t^{(i)} - m_t))^2] = \\mathbb{E}[(W_t^{(i)})^2 (r_t^{(i)} - m_t)^2]\n$$\n利用 $W_t^{(i)}$ 和 $r_t^{(i)}$ 的独立性，这变为：\n$$\n= \\mathbb{E}[(W_t^{(i)})^2] \\mathbb{E}[(r_t^{(i)} - m_t)^2] = (1+\\sigma_\\rho^2)^{t+1} \\operatorname{Var}(r_t^{(i)}) = (1+\\sigma_\\rho^2)^{t+1} v_t\n$$\n所以，$\\operatorname{Var}(\\hat{m}_t) \\approx \\frac{1}{N}(1+\\sigma_\\rho^2)^{t+1} v_t$。$\\hat{J}^w$ 的总方差是独立时间步上的总和：\n$$\n\\operatorname{Var}(\\hat{J}^{w}) \\approx \\sum_{t=0}^{H-1} \\gamma^{2t} \\operatorname{Var}(\\hat{m}_{t}) = \\frac{1}{N} \\sum_{t=0}^{H-1} \\gamma^{2t} (1+\\sigma_\\rho^2)^{t+1} v_t\n$$\n\n3. MSE首项差异的推导。\n\nMSE为 $\\operatorname{Var}(\\cdot) + (\\operatorname{Bias}(\\cdot))^2$。我们保留到 $O(1/N)$ 阶的项。\n对于 $\\hat{J}^o$，偏差恰好为零。\n$$\n\\mathrm{MSE}(\\hat{J}^{o}) = \\operatorname{Var}(\\hat{J}^{o}) = \\frac{1}{N} \\sum_{t=0}^{H-1} \\gamma^{2t} \\left[ (1+\\sigma_{\\rho}^{2})^{t+1} (v_{t} + m_{t}^{2}) - m_{t}^{2} \\right]\n$$\n对于 $\\hat{J}^w$，偏差是 $O(1/N^2)$ 阶的，所以偏差的平方是 $O(1/N^4)$ 阶的，可以忽略不计。\n$$\n\\mathrm{MSE}(\\hat{J}^{w}) \\approx \\operatorname{Var}(\\hat{J}^{w}) = \\frac{1}{N} \\sum_{t=0}^{H-1} \\gamma^{2t} (1+\\sigma_\\rho^2)^{t+1} v_t\n$$\n现在我们计算差异 $\\Delta_{\\mathrm{MSE}} = \\mathrm{MSE}(\\hat{J}^{o}) - \\mathrm{MSE}(\\hat{J}^{w})$，精确到 $O(1/N)$ 阶：\n$$\n\\Delta_{\\mathrm{MSE}} \\approx \\frac{1}{N} \\sum_{t=0}^{H-1} \\gamma^{2t} \\left( \\left[ (1+\\sigma_{\\rho}^{2})^{t+1} (v_{t} + m_{t}^{2}) - m_{t}^{2} \\right] - \\left[ (1+\\sigma_{\\rho}^{2})^{t+1} v_{t} \\right] \\right)\n$$\n$$\n\\Delta_{\\mathrm{MSE}} \\approx \\frac{1}{N} \\sum_{t=0}^{H-1} \\gamma^{2t} \\left( (1+\\sigma_{\\rho}^{2})^{t+1}v_{t} + (1+\\sigma_{\\rho}^{2})^{t+1}m_{t}^{2} - m_{t}^{2} - (1+\\sigma_{\\rho}^{2})^{t+1} v_{t} \\right)\n$$\n涉及 $v_t$ 的项相互抵消。\n$$\n\\Delta_{\\mathrm{MSE}} \\approx \\frac{1}{N} \\sum_{t=0}^{H-1} \\gamma^{2t} \\left( (1+\\sigma_{\\rho}^{2})^{t+1}m_{t}^{2} - m_{t}^{2} \\right)\n$$\n提出 $m_t^2$ 得到首项MSE差异的最终表达式。\n$$\n\\Delta_{\\mathrm{MSE}} = \\frac{1}{N} \\sum_{t=0}^{H-1} \\gamma^{2t} m_t^2 \\left( (1+\\sigma_{\\rho}^{2})^{t+1} - 1 \\right)\n$$\n这个表达式代表了加权估计量中的自归一化所实现的方差减少量，它消除了与平均奖励 $m_t$ 相关的方差分量。",
            "answer": "$$\n\\boxed{\\frac{1}{N} \\sum_{t=0}^{H-1} \\gamma^{2t} m_{t}^{2} \\left( (1+\\sigma_{\\rho}^{2})^{t+1} - 1 \\right)}\n$$"
        }
    ]
}