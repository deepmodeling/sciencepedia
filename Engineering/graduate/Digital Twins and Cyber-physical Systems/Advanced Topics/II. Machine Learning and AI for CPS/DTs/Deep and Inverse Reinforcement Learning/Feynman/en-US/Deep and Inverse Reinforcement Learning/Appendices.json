{
    "hands_on_practices": [
        {
            "introduction": "A core challenge in deploying reinforcement learning in cyber-physical systems is specifying a suitable reward function. Inverse Reinforcement Learning (IRL) addresses this by inferring rewards from expert demonstrations. This practice explores the geometric foundation of many IRL algorithms, showing how a reward function can be recovered by finding a hyperplane that separates the expert's behavior from alternatives in a feature space .",
            "id": "4212767",
            "problem": "A Cyber-Physical System (CPS) digital twin of a smart building operates heating, ventilation, and air conditioning actuators with a controller learned by Deep Reinforcement Learning (DRL). States are mapped to features that aggregate energy, comfort, and actuator wear into a feature vector of dimension $3$. The control problem is modeled as a Markov Decision Process (MDP) with unknown linear reward $r(s,a) = w^{\\top} \\phi(s,a)$, where $w \\in \\mathbb{R}^{3}$ is the reward weight to be recovered by Inverse Reinforcement Learning (IRL). Feature expectations are computed from long-run discounted trajectories under a fixed discount factor (undisclosed and not needed for the derivation) and are normalized so that each coordinate is comparable across policies.\n\nYou are given the expert policy feature expectation and three candidate DRL policy feature expectations, each in $\\mathbb{R}^{3}$:\n$$\n\\mu_{E} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}, \\quad\n\\mu_{1} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\quad\n\\mu_{2} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}, \\quad\n\\mu_{3} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}.\n$$\nConsider the convex hull $\\mathcal{C} = \\mathrm{conv}\\{\\mu_{E}, \\mu_{1}, \\mu_{2}, \\mu_{3}\\}$. Using only foundational principles of convex analysis and geometry (the definition of convex hull, Euclidean norm, Euclidean projection, and the Separating Hyperplane Theorem), recover the reward weight $w$ by:\n- First, computing the Euclidean projection of the origin $0 \\in \\mathbb{R}^{3}$ onto $\\mathcal{C}$.\n- Second, using a separating hyperplane argument to determine the unit normal that maximizes the margin between the origin and $\\mathcal{C}$, and identifying this unit normal with the recovered reward weight $w$.\n\nExpress your final answer for $w$ as a single closed-form analytic expression. No rounding is required. No physical units apply.",
            "solution": "The problem is first validated to be scientifically grounded, well-posed, objective, and self-contained. It presents a standard geometric problem within the context of Inverse Reinforcement Learning (IRL), providing all necessary data and a clear, mathematically sound procedure for finding the solution. No flaws were detected.\n\nThe core of the problem is to find the reward weight vector $w \\in \\mathbb{R}^{3}$. The procedure specified involves two main steps: first, computing the Euclidean projection of the origin onto a convex set, and second, determining the unit normal vector related to this projection.\n\nThe given feature expectations are:\n$$\n\\mu_{E} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}, \\quad\n\\mu_{1} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\quad\n\\mu_{2} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}, \\quad\n\\mu_{3} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}.\n$$\nThe convex set of interest is the convex hull of these four points:\n$$\n\\mathcal{C} = \\mathrm{conv}\\{\\mu_{E}, \\mu_{1}, \\mu_{2}, \\mu_{3}\\}.\n$$\nThis set is a tetrahedron in $\\mathbb{R}^{3}$ with vertices $\\mu_{E}$, $\\mu_{1}$, $\\mu_{2}$, and $\\mu_{3}$.\n\n**Step 1: Compute the Euclidean projection of the origin onto $\\mathcal{C}$.**\n\nWe are tasked with finding the point $p \\in \\mathcal{C}$ that is closest to the origin $0 \\in \\mathbb{R}^{3}$. This is the point that minimizes the Euclidean norm, or equivalently, the squared Euclidean norm. The problem is to find:\n$$\np = \\arg\\min_{x \\in \\mathcal{C}} \\|x\\|_{2}^{2}.\n$$\nSince $\\mathcal{C}$ is a closed and convex set, a unique solution for $p$ exists. The search for $p$ can be conducted by examining the projection of the origin onto the faces, edges, and vertices of the tetrahedron $\\mathcal{C}$.\n\nLet's consider the face $F$ of the tetrahedron formed by the vertices $\\mu_{1}$, $\\mu_{2}$, and $\\mu_{3}$:\n$$\nF = \\mathrm{conv}\\{\\mu_{1}, \\mu_{2}, \\mu_{3}\\} = \\mathrm{conv}\\left\\{\\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}\\right\\}.\n$$\nAny point $x = (x_1, x_2, x_3)^{\\top}$ in the affine hull of $F$ satisfies the equation $x_1 + x_2 + x_3 = 1$. The projection of the origin onto this plane is the point on the plane collinear with the plane's normal vector $n = (1, 1, 1)^{\\top}$. Let this projection be $p_{F} = k n$ for some scalar $k$. Substituting into the plane equation:\n$$\nk \\cdot 1 + k \\cdot 1 + k \\cdot 1 = 1 \\implies 3k = 1 \\implies k = \\frac{1}{3}.\n$$\nSo, the projection of the origin onto the affine hull of $F$ is $p_{F} = (\\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3})^{\\top}$.\n\nTo confirm this point lies within the face $F$, we must check if it can be expressed as a convex combination of the vertices of $F$. We seek coefficients $\\alpha_1, \\alpha_2, \\alpha_3 \\ge 0$ such that $\\sum_{i=1}^{3} \\alpha_i = 1$ and:\n$$\n\\begin{pmatrix} 1/3 \\\\ 1/3 \\\\ 1/3 \\end{pmatrix} = \\alpha_1 \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} + \\alpha_2 \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix} + \\alpha_3 \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} \\alpha_1 \\\\ \\alpha_2 \\\\ \\alpha_3 \\end{pmatrix}.\n$$\nThis gives $\\alpha_1 = \\alpha_2 = \\alpha_3 = 1/3$. These coefficients are non-negative and sum to $1$. Thus, the point $p_{F} = (\\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3})^{\\top}$ lies within the face $F$, and is the point in $F$ closest to the origin.\n\nNow, we must verify that this point is the projection onto the entire tetrahedron $\\mathcal{C}$. Let our candidate projection be $p = p_{F} = (\\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3})^{\\top}$. A point $p \\in \\mathcal{C}$ is the unique projection of the origin onto $\\mathcal{C}$ if and only if it satisfies the variational inequality:\n$$\n(x - p)^{\\top}(0 - p) \\le 0 \\quad \\forall x \\in \\mathcal{C},\n$$\nwhich simplifies to $p^{\\top}(x - p) \\ge 0$.\nSince any point $x \\in \\mathcal{C}$ is a convex combination of the vertices, $x = \\alpha_{E}\\mu_{E} + \\alpha_{1}\\mu_{1} + \\alpha_{2}\\mu_{2} + \\alpha_{3}\\mu_{3}$ where $\\alpha_{i} \\ge 0$ and $\\sum \\alpha_{i} = 1$. Due to linearity of the dot product, we only need to check the condition for the vertices of $\\mathcal{C}$.\nFor $x \\in \\{\\mu_1, \\mu_2, \\mu_3\\}$, the point $x$ is in the face $F$. Since $p$ is the projection onto $F$, the condition $p^{\\top}(x - p) \\ge 0$ is already satisfied for all $x \\in F$.\nWe only need to check the condition for the vertex $\\mu_{E}$:\n$$\np^{\\top}(\\mu_{E} - p) = \\begin{pmatrix} 1/3 \\\\ 1/3 \\\\ 1/3 \\end{pmatrix}^{\\top} \\left( \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 1/3 \\\\ 1/3 \\\\ 1/3 \\end{pmatrix} \\right) = \\begin{pmatrix} 1/3 \\\\ 1/3 \\\\ 1/3 \\end{pmatrix}^{\\top} \\begin{pmatrix} 2/3 \\\\ 2/3 \\\\ 2/3 \\end{pmatrix}.\n$$\n$$\np^{\\top}(\\mu_{E} - p) = \\frac{1}{3} \\cdot \\frac{2}{3} + \\frac{1}{3} \\cdot \\frac{2}{3} + \\frac{1}{3} \\cdot \\frac{2}{3} = 3 \\cdot \\frac{2}{9} = \\frac{2}{3}.\n$$\nSince $\\frac{2}{3} \\ge 0$, the condition holds for $\\mu_{E}$. Because all points in $\\mathcal{C}$ are convex combinations of its vertices, the condition holds for all $x \\in \\mathcal{C}$. Therefore, the Euclidean projection of the origin onto $\\mathcal{C}$ is indeed $p = (\\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3})^{\\top}$.\n\n**Step 2: Use a separating hyperplane argument to determine $w$.**\n\nThe problem states that the recovered reward weight $w$ is the unit normal that maximizes the margin between the origin and $\\mathcal{C}$. By the Separating Hyperplane Theorem, since the origin $0$ and the convex set $\\mathcal{C}$ are disjoint, there exists a hyperplane that separates them. The hyperplane providing the maximum margin of separation is the one normal to the line segment connecting the two closest points of the respective sets. In our case, these points are the origin $0$ and its projection $p$ onto $\\mathcal{C}$.\n\nThe vector normal to this hyperplane is simply $p - 0 = p$. The problem asks for the *unit* normal, which we identify with $w$.\n$$\nw = \\frac{p}{\\|p\\|_{2}}.\n$$\nWe have $p = (\\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3})^{\\top}$. We calculate its Euclidean norm:\n$$\n\\|p\\|_{2} = \\sqrt{\\left(\\frac{1}{3}\\right)^{2} + \\left(\\frac{1}{3}\\right)^{2} + \\left(\\frac{1}{3}\\right)^{2}} = \\sqrt{3 \\cdot \\frac{1}{9}} = \\sqrt{\\frac{1}{3}} = \\frac{1}{\\sqrt{3}}.\n$$\nFinally, we compute the unit vector $w$:\n$$\nw = \\frac{1}{1/\\sqrt{3}} \\begin{pmatrix} 1/3 \\\\ 1/3 \\\\ 1/3 \\end{pmatrix} = \\sqrt{3} \\begin{pmatrix} 1/3 \\\\ 1/3 \\\\ 1/3 \\end{pmatrix} = \\begin{pmatrix} 1/\\sqrt{3} \\\\ 1/\\sqrt{3} \\\\ 1/\\sqrt{3} \\end{pmatrix} = \\begin{pmatrix} \\frac{\\sqrt{3}}{3} \\\\ \\frac{\\sqrt{3}}{3} \\\\ \\frac{\\sqrt{3}}{3} \\end{pmatrix}.\n$$\nThis vector $w$ represents the recovered reward weights for the three features: energy, comfort, and actuator wear.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{\\sqrt{3}}{3} \\\\ \\frac{\\sqrt{3}}{3} \\\\ \\frac{\\sqrt{3}}{3} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Before deploying a new policy on a physical system, it is crucial to evaluate its performance safely and accurately. This often involves using historical data collected under a different policy, a setting known as off-policy evaluation. This exercise delves into the statistical trade-offs of this process by analyzing the bias and variance of two fundamental importance sampling estimators, providing insight into the reliability of off-policy estimates .",
            "id": "4212729",
            "problem": "A Digital Twin (DT) of a Cyber-Physical System (CPS) is used to evaluate a deep target policy in an off-policy setting via Importance Sampling (IS). Consider a finite-horizon Reinforcement Learning (RL) task with horizon length $H \\in \\mathbb{N}$, discount factor $\\gamma \\in (0,1)$, and $N \\in \\mathbb{N}$ independent and identically distributed trajectories collected under a behavior policy. Define the per-decision IS ratios $\\rho_{t}^{(i)} = \\pi(a_{t}^{(i)} \\mid s_{t}^{(i)}) / \\mu(a_{t}^{(i)} \\mid s_{t}^{(i)})$ for time step $t \\in \\{0,1,\\dots,H-1\\}$ and trajectory index $i \\in \\{1,2,\\dots,N\\}$. Let the per-decision cumulative weight be $W_{t}^{(i)} = \\prod_{k=0}^{t} \\rho_{k}^{(i)}$ and the per-step reward be $r_{t}^{(i)}$. The ordinary Per-Decision Importance Sampling (PDIS) estimator and the weighted (self-normalized) PDIS estimator for the discounted return of the target policy are\n$$\n\\hat{J}^{o} = \\sum_{t=0}^{H-1} \\gamma^{t} \\left( \\frac{1}{N} \\sum_{i=1}^{N} W_{t}^{(i)} r_{t}^{(i)} \\right),\n\\qquad\n\\hat{J}^{w} = \\sum_{t=0}^{H-1} \\gamma^{t} \\left( \\frac{ \\sum_{i=1}^{N} W_{t}^{(i)} r_{t}^{(i)} }{ \\sum_{i=1}^{N} W_{t}^{(i)} } \\right).\n$$\nAssume the following foundational modeling conditions suitable for DT-based CPS evaluation:\n- The per-decision ratios $\\rho_{t}^{(i)}$ are independent across time and trajectories with $\\mathbb{E}[\\rho_{t}^{(i)}] = 1$ and $\\operatorname{Var}(\\rho_{t}^{(i)}) = \\sigma_{\\rho}^{2} \\in [0,\\infty)$ for all $t$, and independence across $t$ implies $W_{t}^{(i)}$ has $\\mathbb{E}[W_{t}^{(i)}] = 1$ and second moment $\\mathbb{E}\\!\\left[(W_{t}^{(i)})^{2}\\right] = \\left(1 + \\sigma_{\\rho}^{2} \\right)^{t+1}$.\n- The random variables $r_{t}^{(i)}$ are independent across time and trajectories and independent of $W_{t}^{(i)}$ for each fixed $t$, with target-policy mean $m_{t} = \\mathbb{E}_{\\pi}[r_{t}]$ and variance $v_{t} = \\operatorname{Var}_{\\pi}(r_{t})$, yielding $\\mathbb{E}\\!\\left[ (r_{t}^{(i)})^{2} \\right] = v_{t} + m_{t}^{2}$.\n- The true discounted return of the target policy is $J^{\\pi} = \\sum_{t=0}^{H-1} \\gamma^{t} m_{t}$.\n\nStarting from these definitions and the law of large numbers and delta-method asymptotics for ratio estimators, perform the following:\n1. Derive the bias and variance of $\\hat{J}^{o}$ in closed form, expressed in terms of $H$, $\\gamma$, $N$, $\\sigma_{\\rho}^{2}$, $\\{m_{t}\\}_{t=0}^{H-1}$, and $\\{v_{t}\\}_{t=0}^{H-1}$.\n2. Derive the first-order (in $1/N$) bias and variance of $\\hat{J}^{w}$ using a ratio-of-means expansion, under the stated independence assumptions.\n3. Using your results, provide a closed-form expression for the leading-order difference in mean squared error (MSE) between the ordinary and weighted PDIS estimators, defined as $\\Delta_{\\mathrm{MSE}} = \\mathrm{MSE}(\\hat{J}^{o}) - \\mathrm{MSE}(\\hat{J}^{w})$, retaining terms up to order $1/N$ and neglecting higher-order terms.\n\nYour final answer must be the single closed-form analytic expression for $\\Delta_{\\mathrm{MSE}}$ as a function of $H$, $\\gamma$, $N$, $\\sigma_{\\rho}^{2}$, $\\{m_{t}\\}$, and $\\{v_{t}\\}$, and should not include any units. No rounding is required.",
            "solution": "The problem as stated is scientifically grounded, self-contained, and well-posed. The assumptions, while simplifying, are explicitly stated as foundational modeling conditions and do not contain internal contradictions. We may therefore proceed with a full derivation.\n\nThe overall goal is to find the leading-order difference in Mean Squared Error (MSE), $\\Delta_{\\mathrm{MSE}} = \\mathrm{MSE}(\\hat{J}^{o}) - \\mathrm{MSE}(\\hat{J}^{w})$. This requires deriving the bias and variance for each estimator. The MSE of an estimator $\\hat{\\theta}$ for a parameter $\\theta$ is given by $\\mathrm{MSE}(\\hat{\\theta}) = \\operatorname{Var}(\\hat{\\theta}) + (\\operatorname{Bias}(\\hat{\\theta}))^2$, where $\\operatorname{Bias}(\\hat{\\theta}) = \\mathbb{E}[\\hat{\\theta}] - \\theta$.\n\nLet's first establish the necessary moments based on the problem's givens.\nThe random variables are defined for each trajectory $i \\in \\{1,\\dots,N\\}$ and time step $t \\in \\{0, \\dots, H-1\\}$. Expectations $\\mathbb{E}[\\cdot]$ are taken with respect to the distribution induced by the behavior policy $\\mu$.\n-   Importance weights: $W_{t}^{(i)} = \\prod_{k=0}^{t} \\rho_{k}^{(i)}$. Given $\\rho_{k}^{(i)}$ are i.i.d. with $\\mathbb{E}[\\rho_{k}^{(i)}] = 1$ and $\\operatorname{Var}(\\rho_{k}^{(i)}) = \\sigma_{\\rho}^{2}$.\n    $\\mathbb{E}[W_{t}^{(i)}] = \\prod_{k=0}^{t} \\mathbb{E}[\\rho_{k}^{(i)}] = 1^{t+1} = 1$.\n    $\\mathbb{E}[(W_{t}^{(i)})^2] = \\prod_{k=0}^{t} \\mathbb{E}[(\\rho_{k}^{(i)})^2] = \\prod_{k=0}^{t} (\\operatorname{Var}(\\rho_{k}^{(i)}) + (\\mathbb{E}[\\rho_{k}^{(i)}])^2) = (\\sigma_{\\rho}^{2}+1)^{t+1}$.\n    $\\operatorname{Var}(W_{t}^{(i)}) = \\mathbb{E}[(W_{t}^{(i)})^2] - (\\mathbb{E}[W_{t}^{(i)}])^2 = (1+\\sigma_{\\rho}^{2})^{t+1} - 1$.\n-   Rewards: $r_{t}^{(i)}$. The problem assumes $r_t^{(i)}$ and $W_t^{(i)}$ are independent for a fixed $t$. The fundamental identity of importance sampling is $\\mathbb{E}[W_{t}^{(i)} r_{t}^{(i)}] = \\mathbb{E}_{\\pi}[r_{t}] = m_{t}$. Using the independence assumption, we have $\\mathbb{E}[W_{t}^{(i)} r_{t}^{(i)}] = \\mathbb{E}[W_{t}^{(i)}] \\mathbb{E}[r_{t}^{(i)}] = 1 \\cdot \\mathbb{E}[r_{t}^{(i)}]$. This implies $\\mathbb{E}[r_{t}^{(i)}] = m_{t}$.\n-   The problem provides $\\mathbb{E}[(r_{t}^{(i)})^2] = v_{t} + m_{t}^{2}$. This is the second moment under the behavior policy's distribution. The variance of the reward under the behavior policy is thus $\\operatorname{Var}(r_{t}^{(i)}) = \\mathbb{E}[(r_{t}^{(i)})^2] - (\\mathbb{E}[r_{t}^{(i)}])^2 = (v_{t}+m_{t}^2) - m_{t}^2 = v_{t}$.\n-   The true value to be estimated is $J^{\\pi} = \\sum_{t=0}^{H-1} \\gamma^{t} m_{t}$.\n\nWe proceed with the three parts of the problem.\n\n1. Derivation of the bias and variance of $\\hat{J}^{o}$.\n\nThe ordinary PDIS estimator is $\\hat{J}^{o} = \\sum_{t=0}^{H-1} \\gamma^{t} \\left( \\frac{1}{N} \\sum_{i=1}^{N} W_{t}^{(i)} r_{t}^{(i)} \\right)$.\nTo find the bias, we compute its expectation:\n$$\n\\mathbb{E}[\\hat{J}^{o}] = \\mathbb{E}\\left[ \\sum_{t=0}^{H-1} \\gamma^{t} \\left( \\frac{1}{N} \\sum_{i=1}^{N} W_{t}^{(i)} r_{t}^{(i)} \\right) \\right] = \\sum_{t=0}^{H-1} \\gamma^{t} \\frac{1}{N} \\sum_{i=1}^{N} \\mathbb{E}[W_{t}^{(i)} r_{t}^{(i)}]\n$$\nSince trajectories are i.i.d., $\\mathbb{E}[W_{t}^{(i)} r_{t}^{(i)}]$ is the same for all $i$. Using the IS identity, $\\mathbb{E}[W_{t}^{(i)} r_{t}^{(i)}] = m_{t}$.\n$$\n\\mathbb{E}[\\hat{J}^{o}] = \\sum_{t=0}^{H-1} \\gamma^{t} \\frac{1}{N} (N \\cdot m_{t}) = \\sum_{t=0}^{H-1} \\gamma^{t} m_{t} = J^{\\pi}\n$$\nThe bias is $\\operatorname{Bias}(\\hat{J}^{o}) = \\mathbb{E}[\\hat{J}^{o}] - J^{\\pi} = 0$. $\\hat{J}^{o}$ is an unbiased estimator.\n\nTo find the variance, we use the fact that terms at different time steps are independent, because $\\rho_t^{(i)}$ and $r_t^{(i)}$ are independent across time.\n$$\n\\operatorname{Var}(\\hat{J}^{o}) = \\operatorname{Var}\\left( \\sum_{t=0}^{H-1} \\gamma^{t} \\frac{1}{N} \\sum_{i=1}^{N} W_{t}^{(i)} r_{t}^{(i)} \\right) = \\sum_{t=0}^{H-1} \\gamma^{2t} \\operatorname{Var}\\left( \\frac{1}{N} \\sum_{i=1}^{N} W_{t}^{(i)} r_{t}^{(i)} \\right)\n$$\nSince trajectories are i.i.d., the variance of the mean is the variance of a single term divided by $N$.\n$$\n\\operatorname{Var}(\\hat{J}^{o}) = \\sum_{t=0}^{H-1} \\gamma^{2t} \\frac{1}{N} \\operatorname{Var}(W_{t}^{(i)} r_{t}^{(i)})\n$$\nWe compute the variance of the product term:\n$$\n\\operatorname{Var}(W_{t}^{(i)} r_{t}^{(i)}) = \\mathbb{E}[(W_{t}^{(i)} r_{t}^{(i)})^2] - (\\mathbb{E}[W_{t}^{(i)} r_{t}^{(i)}])^2\n$$\nUsing the independence of $W_t^{(i)}$ and $r_t^{(i)}$, we get $\\mathbb{E}[(W_{t}^{(i)} r_{t}^{(i)})^2] = \\mathbb{E}[(W_{t}^{(i)})^2] \\mathbb{E}[(r_{t}^{(i)})^2]$. Substituting the known moments:\n$$\n\\mathbb{E}[(W_{t}^{(i)} r_{t}^{(i)})^2] = \\left( (1+\\sigma_{\\rho}^{2})^{t+1} \\right) \\left( v_{t} + m_{t}^{2} \\right)\n$$\nAnd $\\mathbb{E}[W_t^{(i)} r_t^{(i)}] = m_t$. Putting it together:\n$$\n\\operatorname{Var}(W_{t}^{(i)} r_{t}^{(i)}) = (1+\\sigma_{\\rho}^{2})^{t+1} (v_{t} + m_{t}^{2}) - m_{t}^{2}\n$$\nSo, the variance of $\\hat{J}^{o}$ is:\n$$\n\\operatorname{Var}(\\hat{J}^{o}) = \\frac{1}{N} \\sum_{t=0}^{H-1} \\gamma^{2t} \\left[ (1+\\sigma_{\\rho}^{2})^{t+1} (v_{t} + m_{t}^{2}) - m_{t}^{2} \\right]\n$$\n\n2. Derivation of the first-order bias and variance of $\\hat{J}^{w}$.\n\nThe weighted PDIS estimator is $\\hat{J}^{w} = \\sum_{t=0}^{H-1} \\gamma^{t} \\hat{m}_{t}$, where $\\hat{m}_{t} = \\frac{ \\sum_{i=1}^{N} W_{t}^{(i)} r_{t}^{(i)} }{ \\sum_{i=1}^{N} W_{t}^{(i)} } = \\frac{A_{t}}{B_{t}}$. Let $A_t = \\frac{1}{N}\\sum_i W_t^{(i)}r_t^{(i)}$ and $B_t = \\frac{1}{N}\\sum_i W_t^{(i)}$. We use the Taylor expansion for the ratio of two random variables. For $\\hat{m}_t = A_t/B_t$, the first-order bias is:\n$$\n\\operatorname{Bias}(\\hat{m}_{t}) \\approx \\frac{\\mathbb{E}[A_{t}]}{\\mathbb{E}[B_{t}]^3} \\operatorname{Var}(B_{t}) - \\frac{1}{\\mathbb{E}[B_{t}]^2} \\operatorname{Cov}(A_{t}, B_{t})\n$$\nWe have $\\mathbb{E}[A_t] = m_t$ and $\\mathbb{E}[B_t] = 1$. $\\operatorname{Var}(B_{t}) = \\frac{1}{N} \\operatorname{Var}(W_t^{(i)}) = \\frac{1}{N}((1+\\sigma_{\\rho}^{2})^{t+1} - 1)$.\nThe covariance term is $\\operatorname{Cov}(A_{t}, B_{t}) = \\frac{1}{N} \\operatorname{Cov}(W_{t}^{(i)} r_{t}^{(i)}, W_{t}^{(i)})$.\n$$\n\\operatorname{Cov}(W_{t}^{(i)} r_{t}^{(i)}, W_{t}^{(i)}) = \\mathbb{E}[W_{t}^{(i)} r_{t}^{(i)} \\cdot W_{t}^{(i)}] - \\mathbb{E}[W_{t}^{(i)} r_{t}^{(i)}] \\mathbb{E}[W_{t}^{(i)}]\n$$\n$$\n= \\mathbb{E}[(W_{t}^{(i)})^2 r_{t}^{(i)}] - m_{t} \\cdot 1 = \\mathbb{E}[(W_{t}^{(i)})^2] \\mathbb{E}[r_{t}^{(i)}] - m_{t} = (1+\\sigma_{\\rho}^{2})^{t+1} m_{t} - m_{t} = m_{t}((1+\\sigma_{\\rho}^{2})^{t+1} - 1)\n$$\nSubstituting these into the bias formula for $\\hat{m}_t$:\n$$\n\\operatorname{Bias}(\\hat{m}_{t}) \\approx \\frac{m_{t}}{1^3} \\frac{1}{N}((1+\\sigma_{\\rho}^{2})^{t+1} - 1) - \\frac{1}{1^2} \\frac{1}{N}m_{t}((1+\\sigma_{\\rho}^{2})^{t+1} - 1) = 0\n$$\nThe first-order bias of $\\hat{m}_{t}$ is zero. Thus, the first-order bias of $\\hat{J}^{w} = \\sum_t \\gamma^t \\hat{m}_t$ is also zero. The bias is of order $O(1/N^2)$.\n\nThe first-order variance of the ratio estimator $\\hat{m}_t$ is:\n$$\n\\operatorname{Var}(\\hat{m}_{t}) \\approx \\frac{1}{\\mathbb{E}[B_{t}]^2} \\operatorname{Var}(A_t - \\frac{\\mathbb{E}[A_t]}{\\mathbb{E}[B_t]} B_t) = \\operatorname{Var}(A_t - m_t B_t)\n$$\n$A_t - m_t B_t = \\frac{1}{N} \\sum_i (W_t^{(i)} r_t^{(i)} - m_t W_t^{(i)}) = \\frac{1}{N} \\sum_i W_t^{(i)}(r_t^{(i)} - m_t)$. Since the trajectories are i.i.d.:\n$$\n\\operatorname{Var}(A_t - m_t B_t) = \\frac{1}{N} \\operatorname{Var}(W_t^{(i)}(r_t^{(i)} - m_t))\n$$\nThe mean of this term is $\\mathbb{E}[W_t^{(i)}(r_t^{(i)} - m_t)] = \\mathbb{E}[W_t^{(i)}r_t^{(i)}] - m_t\\mathbb{E}[W_t^{(i)}] = m_t - m_t \\cdot 1 = 0$.\nSo its variance is its second moment:\n$$\n\\operatorname{Var}(W_t^{(i)}(r_t^{(i)} - m_t)) = \\mathbb{E}[(W_t^{(i)}(r_t^{(i)} - m_t))^2] = \\mathbb{E}[(W_t^{(i)})^2 (r_t^{(i)} - m_t)^2]\n$$\nUsing independence of $W_t^{(i)}$ and $r_t^{(i)}$, this becomes:\n$$\n= \\mathbb{E}[(W_t^{(i)})^2] \\mathbb{E}[(r_t^{(i)} - m_t)^2] = (1+\\sigma_\\rho^2)^{t+1} \\operatorname{Var}(r_t^{(i)}) = (1+\\sigma_\\rho^2)^{t+1} v_t\n$$\nSo, $\\operatorname{Var}(\\hat{m}_t) \\approx \\frac{1}{N}(1+\\sigma_\\rho^2)^{t+1} v_t$. The total variance of $\\hat{J}^w$ is the sum over independent time steps:\n$$\n\\operatorname{Var}(\\hat{J}^{w}) \\approx \\sum_{t=0}^{H-1} \\gamma^{2t} \\operatorname{Var}(\\hat{m}_{t}) = \\frac{1}{N} \\sum_{t=0}^{H-1} \\gamma^{2t} (1+\\sigma_\\rho^2)^{t+1} v_t\n$$\n\n3. Derivation of the leading-order difference in MSE.\n\nThe MSE is $\\operatorname{Var}(\\cdot) + (\\operatorname{Bias}(\\cdot))^2$. We retain terms up to $O(1/N)$.\nFor $\\hat{J}^o$, the bias is exactly zero.\n$$\n\\mathrm{MSE}(\\hat{J}^{o}) = \\operatorname{Var}(\\hat{J}^{o}) = \\frac{1}{N} \\sum_{t=0}^{H-1} \\gamma^{2t} \\left[ (1+\\sigma_{\\rho}^{2})^{t+1} (v_{t} + m_{t}^{2}) - m_{t}^{2} \\right]\n$$\nFor $\\hat{J}^w$, the bias is of order $O(1/N^2)$, so the squared bias is $O(1/N^4)$, which is negligible.\n$$\n\\mathrm{MSE}(\\hat{J}^{w}) \\approx \\operatorname{Var}(\\hat{J}^{w}) = \\frac{1}{N} \\sum_{t=0}^{H-1} \\gamma^{2t} (1+\\sigma_\\rho^2)^{t+1} v_t\n$$\nNow we compute the difference $\\Delta_{\\mathrm{MSE}} = \\mathrm{MSE}(\\hat{J}^{o}) - \\mathrm{MSE}(\\hat{J}^{w})$ up to order $O(1/N)$:\n$$\n\\Delta_{\\mathrm{MSE}} \\approx \\frac{1}{N} \\sum_{t=0}^{H-1} \\gamma^{2t} \\left( \\left[ (1+\\sigma_{\\rho}^{2})^{t+1} (v_{t} + m_{t}^{2}) - m_{t}^{2} \\right] - \\left[ (1+\\sigma_{\\rho}^{2})^{t+1} v_{t} \\right] \\right)\n$$\n$$\n\\Delta_{\\mathrm{MSE}} \\approx \\frac{1}{N} \\sum_{t=0}^{H-1} \\gamma^{2t} \\left( (1+\\sigma_{\\rho}^{2})^{t+1}v_{t} + (1+\\sigma_{\\rho}^{2})^{t+1}m_{t}^{2} - m_{t}^{2} - (1+\\sigma_{\\rho}^{2})^{t+1} v_{t} \\right)\n$$\nThe terms involving $v_t$ cancel out.\n$$\n\\Delta_{\\mathrm{MSE}} \\approx \\frac{1}{N} \\sum_{t=0}^{H-1} \\gamma^{2t} \\left( (1+\\sigma_{\\rho}^{2})^{t+1}m_{t}^{2} - m_{t}^{2} \\right)\n$$\nFactoring out $m_t^2$ yields the final expression for the leading-order MSE difference.\n$$\n\\Delta_{\\mathrm{MSE}} = \\frac{1}{N} \\sum_{t=0}^{H-1} \\gamma^{2t} m_t^2 \\left( (1+\\sigma_{\\rho}^{2})^{t+1} - 1 \\right)\n$$\nThis expression represents the reduction in variance achieved by the self-normalization in the weighted estimator, which removes the component of variance related to the mean reward $m_t$.",
            "answer": "$$\n\\boxed{\\frac{1}{N} \\sum_{t=0}^{H-1} \\gamma^{2t} m_{t}^{2} \\left( (1+\\sigma_{\\rho}^{2})^{t+1} - 1 \\right)}\n$$"
        },
        {
            "introduction": "Digital twin models, while powerful, are always approximations of the real cyber-physical system, containing inherent uncertainties. To create truly reliable controllers, we must design policies that are robust to these model errors. This practice introduces the robust Markov Decision Process (MDP) framework, where you will solve for a policy that guarantees the best possible performance against a worst-case version of the uncertain model .",
            "id": "4212778",
            "problem": "Consider a Digital Twin (DT) of a Cyber-Physical System (CPS) modeled as a two-state Markov Decision Process (MDP), where the nominal operating mode is state $0$ and a fail-safe mode is state $1$. A controller trained via Deep Reinforcement Learning (DRL) uses a stationary stochastic policy in state $0$ that mixes two actions $a$ and $b$, and no control is available in state $1$ (which is absorbing). The rewards are $r(0,a)=2.0$, $r(0,b)=0.8$, and $r(1,\\cdot)=0$. The discount factor is $\\gamma=0.95$. Due to environment variability and model mismatch, the transition probabilities are uncertain but bounded by interval sets identified from operational logs via Inverse Reinforcement Learning (IRL). Specifically, when in state $0$, action $a$ yields a transition to state $1$ with probability $q_a \\in [0.35,\\,0.50]$, while action $b$ yields a transition to state $1$ with probability $q_b \\in [0.15,\\,0.25]$. The probability of remaining in state $0$ is $1-q_a$ under $a$ and $1-q_b$ under $b$. State $1$ is absorbing with $q(1 \\to 1)=1$.\n\nLet the stationary stochastic policy at state $0$ be parameterized by $p \\in [0,1]$, where action $a$ is chosen with probability $p$ and action $b$ is chosen with probability $1-p$. Under the minimax robust control paradigm for MDPs, the policy seeks to maximize the worst-case discounted value at state $0$ across all transition kernels consistent with the interval uncertainty. Formally, the robust value of a policy $\\pi$ is defined as $\\min_{P \\in \\mathcal{P}} V^{\\pi}_{P}(0)$, where $V^{\\pi}_{P}(0)$ is the usual discounted return at state $0$ under transition kernel $P$, and $\\mathcal{P}$ is the set of all kernels consistent with the rectangular intervals above.\n\nStarting only from the fundamental definitions of Markov Decision Processes (MDPs), discounted returns, and minimax robust control with rectangular uncertainty sets, and using the fact that the worst-case transition selection for a given policy will minimize the expected future value at each step, derive the robust discounted value at state $0$ as a function of $p$, and then solve the corresponding minimax optimization over $p \\in [0,1]$ to obtain the robust optimal mixing probability $p^{\\star}$. Provide $p^{\\star}$ as your final answer. No rounding is required. The final answer must be a single real number.",
            "solution": "The problem statement is evaluated to be valid. It is a well-posed problem in the domain of robust Markov Decision Processes (MDPs), a standard topic in control theory and reinforcement learning. All necessary parameters—states, actions, rewards, discount factor, and the uncertainty model (rectangular sets for transition probabilities)—are clearly defined and are mathematically and scientifically consistent. The objective is to find an optimal stochastic policy under a minimax criterion, which is a standard formulation. There are no contradictions, ambiguities, or factual unsoundness.\n\nWe are tasked with finding the optimal mixing probability $p^{\\star}$ for a stationary stochastic policy in a two-state MDP. The problem is formulated under the minimax robust control paradigm.\n\nThe state space is $S = \\{0, 1\\}$. State $0$ is the nominal operating mode, and state $1$ is an absorbing fail-safe mode.\nThe action space in state $0$ is $A_0 = \\{a, b\\}$. In state $1$, no control is possible.\nThe rewards are given as $r(0,a) = 2.0$, $r(0,b) = 0.8$, and $r(s=1, \\cdot) = 0$.\nThe discount factor is $\\gamma = 0.95$.\n\nThe stationary stochastic policy at state $0$, denoted $\\pi_p$, is defined by the probability $p \\in [0,1]$:\n$$ \\pi_p(a|0) = p $$\n$$ \\pi_p(b|0) = 1-p $$\n\nThe transition probabilities are uncertain and belong to intervals. Let $P(s'|s,u)$ be the probability of transitioning from state $s$ to $s'$ under action $u$.\nFor action $a$ in state $0$:\n$$ P(1|0,a) = q_a \\in [0.35, 0.50] $$\n$$ P(0|0,a) = 1 - q_a $$\nFor action $b$ in state $0$:\n$$ P(1|0,b) = q_b \\in [0.15, 0.25] $$\n$$ P(0|0,b) = 1 - q_b $$\nState $1$ is absorbing: $P(1|1,\\cdot) = 1$. The set of all valid transition kernels is denoted by $\\mathcal{P}$.\n\nThe objective is to find $p^{\\star}$ that solves the minimax problem:\n$$ p^{\\star} = \\arg\\max_{p \\in [0,1]} \\left( \\min_{P \\in \\mathcal{P}} V^{\\pi_p}_{P}(0) \\right) $$\nwhere $V^{\\pi_p}_{P}(0)$ is the value function at state $0$ for a policy $\\pi_p$ and a specific transition kernel $P$.\n\nLet $V(s)$ be the robust value function at state $s$, i.e., $V(s) = \\min_{P \\in \\mathcal{P}} V^{\\pi_p}_{P}(s)$.\nFirst, we determine the value of state $1$. Since it is an absorbing state with zero reward, its value is always zero, regardless of the policy or transition probabilities.\n$$ V(1) = r(1) + \\gamma \\sum_{s' \\in \\{0,1\\}} P(s'|1) V(s') = 0 + \\gamma \\cdot (1 \\cdot V(1)) $$\n$$ V(1) (1-\\gamma) = 0 $$\nSince $\\gamma=0.95 \\neq 1$, we must have $V(1) = 0$.\n\nNow, we write the robust Bellman equation for state $0$. The robust value for a given policy $\\pi_p$ is found by considering that for any action taken, nature will adversarially choose the transition probabilities from the allowed set to minimize the agent's expected future value.\nThe value function in state $0$ under policy $\\pi_p$ is the expectation over the actions in $\\pi_p$:\n$$ V_p(0) = \\pi_p(a|0) \\left( r(0,a) + \\gamma \\min_{P(\\cdot|0,a)} \\sum_{s' \\in \\{0,1\\}} P(s'|0,a) V_p(s') \\right) + \\pi_p(b|0) \\left( r(0,b) + \\gamma \\min_{P(\\cdot|0,b)} \\sum_{s' \\in \\{0,1\\}} P(s'|0,b) V_p(s') \\right) $$\nThis expression captures the minimax structure: the policy player maximizes, and for each of the policy's moves, an adversarial nature player minimizes.\n\nLet's evaluate the inner minimization terms for each action. We need the value of $V_p(0)$, but we can first establish its sign. Since all rewards $r(s,u)$ are non-negative, the discounted sum of rewards, which is the value function, must also be non-negative. Thus, $V_p(0) \\ge 0$.\n\nFor action $a$:\nThe term to be minimized by nature is the expected future value:\n$$ \\mathbb{E}_{P(\\cdot|0,a)}[V_p] = P(0|0,a)V_p(0) + P(1|0,a)V_p(1) = (1-q_a)V_p(0) + q_a \\cdot 0 = (1-q_a)V_p(0) $$\nNature chooses $q_a \\in [0.35, 0.50]$ to minimize $(1-q_a)V_p(0)$. Since $V_p(0) \\ge 0$, this is equivalent to maximizing $q_a$. The worst-case probability is the upper bound of the interval:\n$$ q_{a,\\text{worst}} = 0.50 $$\n\nFor action $b$:\nSimilarly, the term to be minimized by nature is:\n$$ \\mathbb{E}_{P(\\cdot|0,b)}[V_p] = (1-q_b)V_p(0) $$\nNature chooses $q_b \\in [0.15, 0.25]$ to minimize this. This is achieved by maximizing $q_b$. The worst-case probability is:\n$$ q_{b,\\text{worst}} = 0.25 $$\n\nNow we can write the Bellman equation for the robust value of policy $\\pi_p$, which we denote $J(p) = V_p(0)$.\n$$ J(p) = p \\left[ r(0,a) + \\gamma (1-q_{a,\\text{worst}}) J(p) \\right] + (1-p) \\left[ r(0,b) + \\gamma (1-q_{b,\\text{worst}}) J(p) \\right] $$\nWe need to solve this equation for $J(p)$.\n$$ J(p) = p \\cdot r(0,a) + (1-p) \\cdot r(0,b) + \\gamma \\left[ p(1-q_{a,\\text{worst}}) + (1-p)(1-q_{b,\\text{worst}}) \\right] J(p) $$\n$$ J(p) \\left( 1 - \\gamma \\left[ p(1-q_{a,\\text{worst}}) + (1-p)(1-q_{b,\\text{worst}}) \\right] \\right) = p \\cdot r(0,a) + (1-p) \\cdot r(0,b) $$\n$$ J(p) = \\frac{p \\cdot r(0,a) + (1-p) \\cdot r(0,b)}{1 - \\gamma \\left( p(1-q_{a,\\text{worst}}) + (1-p)(1-q_{b,\\text{worst}}) \\right)} $$\nNow, we substitute the numerical values: $r(0,a)=2.0$, $r(0,b)=0.8$, $\\gamma=0.95$, $q_{a,\\text{worst}}=0.50$, $q_{b,\\text{worst}}=0.25$.\n\nThe numerator is:\n$$ N(p) = p(2.0) + (1-p)(0.8) = 2.0p + 0.8 - 0.8p = 1.2p + 0.8 $$\nThe denominator is:\n$$ D(p) = 1 - 0.95 \\left[ p(1-0.50) + (1-p)(1-0.25) \\right] $$\n$$ D(p) = 1 - 0.95 \\left[ 0.50p + 0.75(1-p) \\right] $$\n$$ D(p) = 1 - 0.95 \\left[ 0.50p + 0.75 - 0.75p \\right] $$\n$$ D(p) = 1 - 0.95 \\left[ 0.75 - 0.25p \\right] $$\n$$ D(p) = 1 - (0.95 \\cdot 0.75) + (0.95 \\cdot 0.25)p $$\n$$ D(p) = 1 - 0.7125 + 0.2375p $$\n$$ D(p) = 0.2875 + 0.2375p $$\n\nSo, the function we want to maximize over $p \\in [0,1]$ is:\n$$ J(p) = \\frac{1.2p + 0.8}{0.2375p + 0.2875} $$\nThis is a fractional linear transformation (or homographic function) of $p$. To find its maximum, we analyze its derivative with respect to $p$.\nLet $J(p) = \\frac{N(p)}{D(p)}$. The derivative is $J'(p) = \\frac{N'(p)D(p) - N(p)D'(p)}{[D(p)]^2}$.\nThe derivatives of the numerator and denominator are:\n$$ N'(p) = 1.2 $$\n$$ D'(p) = 0.2375 $$\nThe sign of $J'(p)$ is determined by the sign of its numerator, since $[D(p)]^2 > 0$.\nThe numerator of $J'(p)$ is:\n$$ (1.2)(0.2875 + 0.2375p) - (1.2p + 0.8)(0.2375) $$\n$$ = (1.2 \\cdot 0.2875) + (1.2 \\cdot 0.2375)p - (1.2 \\cdot 0.2375)p - (0.8 \\cdot 0.2375) $$\n$$ = 1.2 \\cdot 0.2875 - 0.8 \\cdot 0.2375 $$\n$$ = 0.345 - 0.19 $$\n$$ = 0.155 $$\nSince the numerator of the derivative is a positive constant ($0.155$), we have $J'(p) > 0$ for all $p$ in its domain. This means that $J(p)$ is a strictly monotonically increasing function of $p$.\n\nTo maximize a monotonically increasing function on the closed interval $[0,1]$, we must choose the largest possible value for the variable. Therefore, the maximum value of $J(p)$ is attained at $p=1$.\nThe robust optimal mixing probability is $p^{\\star} = 1$.",
            "answer": "$$\\boxed{1}$$"
        }
    ]
}