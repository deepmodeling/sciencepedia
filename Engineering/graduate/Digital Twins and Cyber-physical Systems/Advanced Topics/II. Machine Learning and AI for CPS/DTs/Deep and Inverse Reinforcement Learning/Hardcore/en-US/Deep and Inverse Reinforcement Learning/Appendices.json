{
    "hands_on_practices": [
        {
            "introduction": "Defining a precise reward function for a complex Cyber-Physical System can be an immense challenge. Inverse Reinforcement Learning (IRL) offers a powerful solution by inferring the underlying objective from expert demonstrations. This exercise  guides you through the implementation of the forward-backward algorithm, a dynamic programming method that is fundamental to computing the key quantities in Maximum Entropy IRL, one of the most successful and widely-used IRL frameworks.",
            "id": "4212746",
            "problem": "Consider a finite-horizon Markov Decision Process (MDP) that serves as an abstract digital twin of a Cyber-Physical System (CPS). The MDP has a finite set of states $\\mathcal{S}$, a finite set of control actions $\\mathcal{A}$, known transition probabilities $P(s' \\mid s,a)$, and a fixed time horizon $T$. The initial distribution over states is $p_0(s)$ with $\\sum_{s \\in \\mathcal{S}} p_0(s) = 1$. For Maximum Entropy (MaxEnt) Inverse Reinforcement Learning (IRL), assume an additive reward over trajectories where the per-step reward is a linear function of features: $r(s,a) = \\boldsymbol{\\theta}^{\\top} \\mathbf{f}(s,a)$, where $\\mathbf{f}(s,a) \\in \\mathbb{R}^d$ is a fixed feature vector and $\\boldsymbol{\\theta} \\in \\mathbb{R}^d$ is a fixed parameter vector. Under the MaxEnt distribution, the unnormalized weight of a trajectory $(s_0,a_0,s_1,a_1,\\dots,s_T)$ is the product of the initial probability and transition probabilities multiplied by the exponential of the summed rewards. The partition function $Z$ is the sum of these unnormalized weights over all possible trajectories. The expected feature counts are the expectation of the sum of $\\mathbf{f}(s_t,a_t)$ over time steps $t \\in \\{0,\\dots,T-1\\}$ under the normalized MaxEnt distribution.\n\nStarting from fundamental definitions, implement a forward–backward dynamic program that computes: (i) the partition function $Z$, and (ii) the expected feature counts $\\mathbb{E}\\left[\\sum_{t=0}^{T-1} \\mathbf{f}(s_t,a_t)\\right]$. Your implementation must be derived from the Markov property and the definition of the MaxEnt trajectory distribution as a normalized Gibbs measure over trajectories, without assuming any shortcut formulas; derive the required recursions from first principles.\n\nYour program must use the following test suite of parameter values. In all cases, the state set is $\\mathcal{S} = \\{0,1,2\\}$ with size $|\\mathcal{S}| = 3$, the action set is $\\mathcal{A} = \\{0,1\\}$ with size $|\\mathcal{A}| = 2$, and the transition probabilities $P(s' \\mid s,a)$ are specified for each $(s,a)$ as a $3$-vector over $s' \\in \\{0,1,2\\}$, with entries summing to $1$.\n\nDefine the transition dynamics $P$ for each action as follows:\n- For action $a = 0$:\n  - $P(0 \\mid 0,0) = 0.7$, $P(1 \\mid 0,0) = 0.2$, $P(2 \\mid 0,0) = 0.1$.\n  - $P(0 \\mid 1,0) = 0.1$, $P(1 \\mid 1,0) = 0.6$, $P(2 \\mid 1,0) = 0.3$.\n  - $P(0 \\mid 2,0) = 0.2$, $P(1 \\mid 2,0) = 0.3$, $P(2 \\mid 2,0) = 0.5$.\n- For action $a = 1$:\n  - $P(0 \\mid 0,1) = 0.2$, $P(1 \\mid 0,1) = 0.5$, $P(2 \\mid 0,1) = 0.3$.\n  - $P(0 \\mid 1,1) = 0.3$, $P(1 \\mid 1,1) = 0.4$, $P(2 \\mid 1,1) = 0.3$.\n  - $P(0 \\mid 2,1) = 0.4$, $P(1 \\mid 2,1) = 0.2$, $P(2 \\mid 2,1) = 0.4$.\n\nDefine the feature map $\\mathbf{f}(s,a) \\in \\mathbb{R}^3$ for each $(s,a)$ by\n- If $a = 0$, $\\mathbf{f}(s,0) = [1,\\,0,\\,s]$.\n- If $a = 1$, $\\mathbf{f}(s,1) = [0,\\,1,\\,s]$.\n\nThe three test cases are:\n\n- Test Case $1$ (happy path): $T = 3$, $p_0 = [0.6,\\,0.3,\\,0.1]$, $\\boldsymbol{\\theta} = [0.5,\\,-0.2,\\,0.1]$.\n- Test Case $2$ (zero-reward boundary): $T = 4$, $p_0 = [\\frac{1}{3},\\,\\frac{1}{3},\\,\\frac{1}{3}]$, $\\boldsymbol{\\theta} = [0,\\,0,\\,0]$.\n- Test Case $3$ (zero-horizon boundary): $T = 0$, $p_0 = [0.2,\\,0.5,\\,0.3]$, $\\boldsymbol{\\theta} = [0.1,\\,0.2,\\,-0.3]$.\n\nRequirements:\n- Implement a forward–backward dynamic program that computes $Z$ and the expected feature counts vector for each test case from first principles based on the MDP factorization and the MaxEnt trajectory weighting.\n- Numerical outputs must be floats rounded to $6$ decimal places.\n- The final output of your program must be a single line containing a list of results, one per test case, where each result is a list consisting of $Z$ followed by the $3$ expected feature counts (in the order of the feature coordinates). For example, the format should be of the form $[[Z_1,\\,e_{1,1},\\,e_{1,2},\\,e_{1,3}],\\,[Z_2,\\,e_{2,1},\\,e_{2,2},\\,e_{2,3}],\\,[Z_3,\\,e_{3,1},\\,e_{3,2},\\,e_{3,3}]]$.",
            "solution": "The objective is to compute the partition function $Z$ and the expected feature counts for a finite-horizon Markov Decision Process (MDP) under the Maximum Entropy (MaxEnt) trajectory distribution. This will be accomplished by deriving and implementing a forward-backward dynamic programming algorithm from first principles.\n\nLet the MDP be defined by the tuple $(\\mathcal{S}, \\mathcal{A}, P, p_0, T)$, where $\\mathcal{S}$ is the finite set of states, $\\mathcal{A}$ is the finite set of actions, $P(s'|s,a)$ are the state transition probabilities, $p_0(s)$ is the initial state distribution, and $T$ is the time horizon. A trajectory of length $T$ is a sequence $\\tau = (s_0, a_0, s_1, a_1, \\ldots, a_{T-1}, s_T)$.\n\nThe reward is a linear function of features, $r(s,a) = \\boldsymbol{\\theta}^{\\top} \\mathbf{f}(s,a)$. Under the MaxEnt principle, the probability of a trajectory $\\tau$ is given by a Gibbs distribution:\n$$p(\\tau) = \\frac{1}{Z} \\exp\\left(\\sum_{t=0}^{T-1} r(s_t, a_t)\\right) p_0(s_0) \\prod_{t=0}^{T-1} P(s_{t+1}|s_t, a_t)$$\nThe partition function $Z$ is the normalization constant, ensuring that the probabilities sum to one:\n$$Z = \\sum_{\\tau} p_0(s_0) \\prod_{t=0}^{T-1} P(s_{t+1}|s_t, a_t) \\exp\\left(\\sum_{t=0}^{T-1} r(s_t, a_t)\\right)$$\nThe summation is over all possible trajectories $\\tau$. The term inside the sum is the unnormalized weight of a trajectory, $w(\\tau)$.\n\nThe problem requires the computation of $Z$ and the expected feature counts, $\\mathbb{E}\\left[\\sum_{t=0}^{T-1} \\mathbf{f}(s_t,a_t)\\right]$. A forward-backward algorithm is suitable for this task, as it efficiently computes marginals in a chain-structured probabilistic model.\n\n### Derivation of the Forward-Backward Algorithm\n\nThe unnormalized weight of a trajectory can be factored by time steps:\n$$w(\\tau) = p_0(s_0) \\prod_{t=0}^{T-1} \\left[ P(s_{t+1}|s_t, a_t) \\exp(r(s_t, a_t)) \\right]$$\n\n**1. Forward Pass: The $\\alpha$ Messages**\n\nLet $\\alpha_t(s_t)$ be the sum of unnormalized weights of all partial trajectories from time $k=0$ to $k=t$ that end in state $s_t$.\n$$\\alpha_t(s_t) = \\sum_{s_0, a_0, \\ldots, s_{t-1}, a_{t-1}} p_0(s_0) \\prod_{k=0}^{t-1} \\left[ P(s_{k+1}|s_k, a_k) \\exp(r(s_k, a_k)) \\right]$$\nwhere the final state $s_k$ at $k=t$ is fixed to $s_t$.\n\nThe recursion is derived as follows:\n\n-   **Base Case ($t=0$):** A partial trajectory of length $0$ is just the initial state $s_0$. The weight is its initial probability.\n    $$\\alpha_0(s_0) = p_0(s_0)$$\n\n-   **Recursive Step ($t \\to t+1$):** The weight of a partial trajectory ending at $s_{t+1}$ is the sum of weights of trajectories ending at any state $s_t$ at time $t$, extended by one step $(a_t, s_{t+1})$.\n    $$\\alpha_{t+1}(s_{t+1}) = \\sum_{s_t \\in \\mathcal{S}} \\sum_{a_t \\in \\mathcal{A}} \\alpha_t(s_t) \\cdot P(s_{t+1}|s_t, a_t) \\exp(r(s_t, a_t))$$\n    This recursion is performed for $t = 0, 1, \\ldots, T-1$.\n\n-   **Partition Function ($Z$):** The partition function $Z$ is the sum of weights of all full trajectories. A full trajectory ends at time $T$. Therefore, $Z$ is the sum of all $\\alpha$ messages at the final time step $T$.\n    $$Z = \\sum_{s_T \\in \\mathcal{S}} \\alpha_T(s_T)$$\n\n**2. Backward Pass: The $\\beta$ Messages**\n\nLet $\\beta_t(s_t)$ be the sum of unnormalized weights of all future partial trajectories from time $k=t$ to the end of the horizon $T$, given that the system is in state $s_t$ at time $t$.\n$$\\beta_t(s_t) = \\sum_{a_t, s_{t+1}, \\ldots, a_{T-1}, s_T} \\prod_{k=t}^{T-1} \\left[ P(s_{k+1}|s_k, a_k) \\exp(r(s_k, a_k)) \\right]$$\nwhere the initial state $s_k$ at $k=t$ is fixed to $s_t$.\n\nThe recursion is derived by working backward in time:\n\n-   **Base Case ($t=T$):** At the end of the horizon, the partial future trajectory is empty. The product over an empty set is $1$.\n    $$\\beta_T(s_T) = 1 \\quad \\forall s_T \\in \\mathcal{S}$$\n\n-   **Recursive Step ($t+1 \\to t$):** To compute $\\beta_t(s_t)$, we sum over all possible next actions $a_t$ and states $s_{t+1}$, multiplying the single-step transition weight by the weight of the rest of the future path, $\\beta_{t+1}(s_{t+1})$.\n    $$\\beta_t(s_t) = \\sum_{a_t \\in \\mathcal{A}} \\sum_{s_{t+1} \\in \\mathcal{S}} P(s_{t+1}|s_t, a_t) \\exp(r(s_t, a_t)) \\cdot \\beta_{t+1}(s_{t+1})$$\n    This recursion is performed for $t = T-1, T-2, \\ldots, 0$.\n\n**3. Computing Expected Feature Counts**\n\nThe total expected feature count is the sum of expected feature counts at each time step, by linearity of expectation:\n$$\\mathbb{E}\\left[\\sum_{t=0}^{T-1} \\mathbf{f}(s_t,a_t)\\right] = \\sum_{t=0}^{T-1} \\mathbb{E}[\\mathbf{f}(s_t,a_t)]$$\nThe expectation at a single time step is:\n$$\\mathbb{E}[\\mathbf{f}(s_t,a_t)] = \\sum_{s \\in \\mathcal{S}} \\sum_{a \\in \\mathcal{A}} p(s_t=s, a_t=a) \\cdot \\mathbf{f}(s,a)$$\nWe need the marginal probability $p(s_t=s, a_t=a)$. This probability is the sum of normalized weights of all trajectories passing through the state-action pair $(s,a)$ at time $t$. The unnormalized weight of this event, $w(s_t=s, a_t=a)$, can be expressed using the $\\alpha$ and $\\beta$ messages.\n\nThe total weight of paths leading to state $s$ at time $t$ is $\\alpha_t(s)$. The weight of paths from a subsequent state $s'$ at time $t+1$ to the end is $\\beta_{t+1}(s')$. The connection is the transition from $(s,a)$ to $s'$. Summing over all possible next states $s'$ gives:\n$$w(s_t=s, a_t=a) = \\alpha_t(s) \\cdot \\exp(r(s,a)) \\cdot \\sum_{s' \\in \\mathcal{S}} P(s'|s,a) \\beta_{t+1}(s')$$\nNormalizing by $Z$ gives the marginal probability:\n$$p(s_t=s, a_t=a) = \\frac{w(s_t=s, a_t=a)}{Z} = \\frac{\\alpha_t(s) \\exp(r(s,a)) \\sum_{s' \\in \\mathcal{S}} P(s'|s,a) \\beta_{t+1}(s')}{Z}$$\nAn important identity, $\\sum_{s \\in \\mathcal{S}} \\alpha_t(s) \\beta_t(s) = Z$ for any $t \\in [0,T]$, can be used to verify the correctness of the calculations.\n\nThe total expected feature counts are then calculated by summing over all time steps:\n$$\\mathbb{E}_{\\text{total}} = \\sum_{t=0}^{T-1} \\sum_{s \\in \\mathcal{S}} \\sum_{a \\in \\mathcal{A}} p(s_t=s, a_t=a) \\cdot \\mathbf{f}(s,a)$$\n\n### Boundary Cases\n\n-   **Horizon $T=0$:** A trajectory consists only of an initial state $s_0$. The reward sum and transition product are empty, evaluating to $0$ and $1$ respectively. The weight of a trajectory $(s_0)$ is $p_0(s_0)$. The partition function is $Z = \\sum_{s_0} p_0(s_0) = 1$. The sum of features $\\sum_{t=0}^{-1} \\mathbf{f}(s_t, a_t)$ is empty, so the expected feature counts are the zero vector.\n\n-   **Zero Reward $\\boldsymbol{\\theta}=\\mathbf{0}$:** If $\\boldsymbol{\\theta}=\\mathbf{0}$, then $r(s,a)=0$ for all $(s,a)$, and $\\exp(r(s,a))=1$. The recursions simplify. A consistency check shows that $Z$ should evaluate to $|\\mathcal{A}|^T$, and the trajectory distribution corresponds to executing a uniform random policy (choosing actions with probability $1/|\\mathcal{A}|$) and then normalizing over the set of all possible trajectories.\n\n### Algorithm Summary\n\n1.  **Initialization:** Given MDP parameters $(\\mathcal{S}, \\mathcal{A}, P, p_0, T)$, features $\\mathbf{f}(s,a)$, and weights $\\boldsymbol{\\theta}$. Let $N_S = |\\mathcal{S}|$, $N_A = |\\mathcal{A}|$.\n2.  **Handle $T=0$:** If $T=0$, return $Z=1.0$ and expected features as a zero vector.\n3.  **Forward Pass:**\n    -   Initialize $\\alpha$ matrix of size $(T+1) \\times N_S$.\n    -   Set $\\alpha[0, s] \\leftarrow p_0(s)$ for $s=0, \\ldots, N_S-1$.\n    -   For $t$ from $0$ to $T-1$:\n        -   Compute $\\alpha[t+1, s'] \\leftarrow \\sum_{s,a} \\alpha[t, s] P(s'|s,a) \\exp(\\boldsymbol{\\theta}^{\\top}\\mathbf{f}(s,a))$ for each $s'$.\n    -   Compute $Z \\leftarrow \\sum_s \\alpha[T, s]$.\n4.  **Backward Pass:**\n    -   Initialize $\\beta$ matrix of size $(T+1) \\times N_S$.\n    -   Set $\\beta[T, s] \\leftarrow 1.0$ for $s=0, \\ldots, N_S-1$.\n    -   For $t$ from $T-1$ down to $0$:\n        -   Compute $\\beta[t, s] \\leftarrow \\sum_{a,s'} P(s'|s,a) \\exp(\\boldsymbol{\\theta}^{\\top}\\mathbf{f}(s,a)) \\beta[t+1, s']$ for each $s$.\n5.  **Compute Expectations:**\n    -   Initialize `total_expected_features` vector to zeros.\n    -   For $t$ from $0$ to $T-1$:\n        -   For each state $s$ and action $a$:\n            -   Compute marginal probability $p(s_t=s, a_t=a)$.\n            -   Add $p(s_t=s, a_t=a) \\cdot \\mathbf{f}(s,a)$ to `total_expected_features`.\n6.  **Return** $Z$ and `total_expected_features`.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem for all test cases by implementing the forward-backward algorithm.\n    \"\"\"\n\n    # --- Problem Definition ---\n    num_states = 3\n    num_actions = 2\n    feat_dim = 3\n\n    # Transition probabilities P(s'|s,a) as a tensor of shape (s, a, s')\n    P = np.zeros((num_states, num_actions, num_states))\n    # a = 0\n    P[0, 0, :] = [0.7, 0.2, 0.1]\n    P[1, 0, :] = [0.1, 0.6, 0.3]\n    P[2, 0, :] = [0.2, 0.3, 0.5]\n    # a = 1\n    P[0, 1, :] = [0.2, 0.5, 0.3]\n    P[1, 1, :] = [0.3, 0.4, 0.3]\n    P[2, 1, :] = [0.4, 0.2, 0.4]\n\n    # Feature map f(s,a) as a tensor of shape (s, a, d)\n    F = np.zeros((num_states, num_actions, feat_dim))\n    for s in range(num_states):\n        # a = 0\n        F[s, 0, :] = [1, 0, s]\n        # a = 1\n        F[s, 1, :] = [0, 1, s]\n\n    # --- Test Cases ---\n    test_cases = [\n        {\n            \"T\": 3,\n            \"p0\": np.array([0.6, 0.3, 0.1]),\n            \"theta\": np.array([0.5, -0.2, 0.1]),\n        },\n        {\n            \"T\": 4,\n            \"p0\": np.array([1/3, 1/3, 1/3]),\n            \"theta\": np.array([0.0, 0.0, 0.0]),\n        },\n        {\n            \"T\": 0,\n            \"p0\": np.array([0.2, 0.5, 0.3]),\n            \"theta\": np.array([0.1, 0.2, -0.3]),\n        },\n    ]\n\n    results = []\n\n    for case in test_cases:\n        T = case[\"T\"]\n        p0 = case[\"p0\"]\n        theta = case[\"theta\"]\n\n        # Handle T=0 boundary case\n        if T == 0:\n            Z = 1.0\n            expected_features = np.zeros(feat_dim)\n            results.append([round(Z, 6)] + [round(ef, 6) for ef in expected_features])\n            continue\n\n        # Precompute reward-related term exp(theta^T * f(s,a))\n        # R has shape (s, a)\n        R = np.exp(np.einsum('sad,d-sa', F, theta))\n\n        # --- Forward Pass (alpha) ---\n        alpha = np.zeros((T + 1, num_states))\n        alpha[0, :] = p0\n\n        for t in range(T):\n            # alpha_{t+1}(s') = sum_{s,a} alpha_t(s) * R(s,a) * P(s'|s,a)\n            # einsum notation: s=state at t, a=action at t, p=state at t+1 (s')\n            alpha[t + 1, :] = np.einsum('s,sa,sap-p', alpha[t, :], R, P)\n        \n        # Partition function Z\n        Z = np.sum(alpha[T, :])\n\n        # --- Backward Pass (beta) ---\n        beta = np.zeros((T + 1, num_states))\n        beta[T, :] = 1.0\n\n        for t in range(T - 1, -1, -1):\n            # beta_t(s) = sum_{a,s'} R(s,a) * P(s'|s,a) * beta_{t+1}(s')\n            # einsum notation: s=state at t, a=action at t, p=state at t+1 (s')\n            beta[t, :] = np.einsum('sa,sap,p-s', R, P, beta[t + 1, :])\n        \n        # --- Expected Feature Counts ---\n        total_expected_features = np.zeros(feat_dim)\n        for t in range(T):\n            # sum_{s'} P(s'|s,a) * beta_{t+1}(s')\n            # einsum notation: p=state at t+1 (s')\n            beta_P_sum = np.einsum('p,sap-sa', beta[t + 1, :], P)\n            \n            # unnormalized probability of (s_t, a_t)\n            # using broadcasting for alpha[t,:]\n            prob_sa_unnorm = alpha[t, :, np.newaxis] * R * beta_P_sum\n            \n            # normalized probability p(s_t, a_t)\n            prob_sa = prob_sa_unnorm / Z\n            \n            # Expected features for this timestep\n            exp_feat_t = np.einsum('sa,sad-d', prob_sa, F)\n            total_expected_features += exp_feat_t\n\n        case_result = [round(Z, 6)] + [round(ef, 6) for ef in total_expected_features]\n        results.append(case_result)\n    \n    # Format the final output string as requested\n    output_str = \"[\" + \",\".join([f\"[{','.join(map(str, res))}]\" for res in results]) + \"]\"\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "Once a reward function is defined, we can train a controller. However, the digital twin used for training is only an approximation of the real system, containing inherent uncertainties. This practice  explores how to design controllers that are resilient to such model mismatch by using the framework of robust Markov Decision Processes. You will solve a minimax optimization problem to find a policy that guarantees the best possible performance under worst-case assumptions about the system's dynamics, a critical step in deploying agents in safety-conscious environments.",
            "id": "4212778",
            "problem": "Consider a Digital Twin (DT) of a Cyber-Physical System (CPS) modeled as a two-state Markov Decision Process (MDP), where the nominal operating mode is state $0$ and a fail-safe mode is state $1$. A controller trained via Deep Reinforcement Learning (DRL) uses a stationary stochastic policy in state $0$ that mixes two actions $a$ and $b$, and no control is available in state $1$ (which is absorbing). The rewards are $r(0,a)=2.0$, $r(0,b)=0.8$, and $r(1,\\cdot)=0$. The discount factor is $\\gamma=0.95$. Due to environment variability and model mismatch, the transition probabilities are uncertain but bounded by interval sets identified from operational logs via Inverse Reinforcement Learning (IRL). Specifically, when in state $0$, action $a$ yields a transition to state $1$ with probability $q_a \\in [0.35,\\,0.50]$, while action $b$ yields a transition to state $1$ with probability $q_b \\in [0.15,\\,0.25]$. The probability of remaining in state $0$ is $1-q_a$ under $a$ and $1-q_b$ under $b$. State $1$ is absorbing with $q(1 \\to 1)=1$.\n\nLet the stationary stochastic policy at state $0$ be parameterized by $p \\in [0,1]$, where action $a$ is chosen with probability $p$ and action $b$ is chosen with probability $1-p$. Under the minimax robust control paradigm for MDPs, the policy seeks to maximize the worst-case discounted value at state $0$ across all transition kernels consistent with the interval uncertainty. Formally, the robust value of a policy $\\pi$ is defined as $\\min_{P \\in \\mathcal{P}} V^{\\pi}_{P}(0)$, where $V^{\\pi}_{P}(0)$ is the usual discounted return at state $0$ under transition kernel $P$, and $\\mathcal{P}$ is the set of all kernels consistent with the rectangular intervals above.\n\nStarting only from the fundamental definitions of Markov Decision Processes (MDPs), discounted returns, and minimax robust control with rectangular uncertainty sets, and using the fact that the worst-case transition selection for a given policy will minimize the expected future value at each step, derive the robust discounted value at state $0$ as a function of $p$, and then solve the corresponding minimax optimization over $p \\in [0,1]$ to obtain the robust optimal mixing probability $p^{\\star}$. Provide $p^{\\star}$ as your final answer. No rounding is required. The final answer must be a single real number.",
            "solution": "The problem statement is evaluated to be valid. It is a well-posed problem in the domain of robust Markov Decision Processes (MDPs), a standard topic in control theory and reinforcement learning. All necessary parameters—states, actions, rewards, discount factor, and the uncertainty model (rectangular sets for transition probabilities)—are clearly defined and are mathematically and scientifically consistent. The objective is to find an optimal stochastic policy under a minimax criterion, which is a standard formulation. There are no contradictions, ambiguities, or factual unsoundness.\n\nWe are tasked with finding the optimal mixing probability $p^{\\star}$ for a stationary stochastic policy in a two-state MDP. The problem is formulated under the minimax robust control paradigm.\n\nThe state space is $S = \\{0, 1\\}$. State $0$ is the nominal operating mode, and state $1$ is an absorbing fail-safe mode.\nThe action space in state $0$ is $A_0 = \\{a, b\\}$. In state $1$, no control is possible.\nThe rewards are given as $r(0,a) = 2.0$, $r(0,b) = 0.8$, and $r(s=1, \\cdot) = 0$.\nThe discount factor is $\\gamma = 0.95$.\n\nThe stationary stochastic policy at state $0$, denoted $\\pi_p$, is defined by the probability $p \\in [0,1]$:\n$$ \\pi_p(a|0) = p $$\n$$ \\pi_p(b|0) = 1-p $$\n\nThe transition probabilities are uncertain and belong to intervals. Let $P(s'|s,u)$ be the probability of transitioning from state $s$ to $s'$ under action $u$.\nFor action $a$ in state $0$:\n$$ P(1|0,a) = q_a \\in [0.35, 0.50] $$\n$$ P(0|0,a) = 1 - q_a $$\nFor action $b$ in state $0$:\n$$ P(1|0,b) = q_b \\in [0.15, 0.25] $$\n$$ P(0|0,b) = 1 - q_b $$\nState $1$ is absorbing: $P(1|1,\\cdot) = 1$. The set of all valid transition kernels is denoted by $\\mathcal{P}$.\n\nThe objective is to find $p^{\\star}$ that solves the minimax problem:\n$$ p^{\\star} = \\arg\\max_{p \\in [0,1]} \\left( \\min_{P \\in \\mathcal{P}} V^{\\pi_p}_{P}(0) \\right) $$\nwhere $V^{\\pi_p}_{P}(0)$ is the value function at state $0$ for a policy $\\pi_p$ and a specific transition kernel $P$.\n\nLet $V(s)$ be the robust value function at state $s$, i.e., $V(s) = \\min_{P \\in \\mathcal{P}} V^{\\pi_p}_{P}(s)$.\nFirst, we determine the value of a state $1$. Since it is an absorbing state with zero reward, its value is always zero, regardless of the policy or transition probabilities.\n$$ V(1) = r(1) + \\gamma \\sum_{s' \\in \\{0,1\\}} P(s'|1) V(s') = 0 + \\gamma \\cdot (1 \\cdot V(1)) $$\n$$ V(1) (1-\\gamma) = 0 $$\nSince $\\gamma=0.95 \\neq 1$, we must have $V(1) = 0$.\n\nNow, we write the robust Bellman equation for state $0$. The robust value for a given policy $\\pi_p$ is found by considering that for any action taken, nature will adversarially choose the transition probabilities from the allowed set to minimize the agent's expected future value.\nThe value function in state $0$ under policy $\\pi_p$ is the expectation over the actions in $\\pi_p$:\n$$ V_p(0) = \\pi_p(a|0) \\left( r(0,a) + \\gamma \\min_{P(\\cdot|0,a)} \\sum_{s' \\in \\{0,1\\}} P(s'|0,a) V_p(s') \\right) + \\pi_p(b|0) \\left( r(0,b) + \\gamma \\min_{P(\\cdot|0,b)} \\sum_{s' \\in \\{0,1\\}} P(s'|0,b) V_p(s') \\right) $$\nThis expression captures the minimax structure: the policy player maximizes, and for each of the policy's moves, an adversarial nature player minimizes.\n\nLet's evaluate the inner minimization terms for each action. We need the value of $V_p(0)$, but we can first establish its sign. Since all rewards $r(s,u)$ are non-negative, the discounted sum of rewards, which is the value function, must also be non-negative. Thus, $V_p(0) \\ge 0$.\n\nFor action $a$:\nThe term to be minimized by nature is the expected future value:\n$$ \\mathbb{E}_{P(\\cdot|0,a)}[V_p] = P(0|0,a)V_p(0) + P(1|0,a)V_p(1) = (1-q_a)V_p(0) + q_a \\cdot 0 = (1-q_a)V_p(0) $$\nNature chooses $q_a \\in [0.35, 0.50]$ to minimize $(1-q_a)V_p(0)$. Since $V_p(0) \\ge 0$, this is equivalent to maximizing $q_a$. The worst-case probability is the upper bound of the interval:\n$$ q_{a,\\text{worst}} = 0.50 $$\n\nFor action $b$:\nSimilarly, the term to be minimized by nature is:\n$$ \\mathbb{E}_{P(\\cdot|0,b)}[V_p] = (1-q_b)V_p(0) $$\nNature chooses $q_b \\in [0.15, 0.25]$ to minimize this. This is achieved by maximizing $q_b$. The worst-case probability is:\n$$ q_{b,\\text{worst}} = 0.25 $$\n\nNow we can write the Bellman equation for the robust value of policy $\\pi_p$, which we denote $J(p) = V_p(0)$.\n$$ J(p) = p \\left[ r(0,a) + \\gamma (1-q_{a,\\text{worst}}) J(p) \\right] + (1-p) \\left[ r(0,b) + \\gamma (1-q_{b,\\text{worst}}) J(p) \\right] $$\nWe need to solve this equation for $J(p)$.\n$$ J(p) = p \\cdot r(0,a) + (1-p) \\cdot r(0,b) + \\gamma \\left[ p(1-q_{a,\\text{worst}}) + (1-p)(1-q_{b,\\text{worst}}) \\right] J(p) $$\n$$ J(p) \\left( 1 - \\gamma \\left[ p(1-q_{a,\\text{worst}}) + (1-p)(1-q_{b,\\text{worst}}) \\right] \\right) = p \\cdot r(0,a) + (1-p) \\cdot r(0,b) $$\n$$ J(p) = \\frac{p \\cdot r(0,a) + (1-p) \\cdot r(0,b)}{1 - \\gamma \\left( p(1-q_{a,\\text{worst}}) + (1-p)(1-q_{b,\\text{worst}}) \\right)} $$\nNow, we substitute the numerical values: $r(0,a)=2.0$, $r(0,b)=0.8$, $\\gamma=0.95$, $q_{a,\\text{worst}}=0.50$, $q_{b,\\text{worst}}=0.25$.\n\nThe numerator is:\n$$ N(p) = p(2.0) + (1-p)(0.8) = 2.0p + 0.8 - 0.8p = 1.2p + 0.8 $$\nThe denominator is:\n$$ D(p) = 1 - 0.95 \\left[ p(1-0.50) + (1-p)(1-0.25) \\right] $$\n$$ D(p) = 1 - 0.95 \\left[ 0.50p + 0.75(1-p) \\right] $$\n$$ D(p) = 1 - 0.95 \\left[ 0.50p + 0.75 - 0.75p \\right] $$\n$$ D(p) = 1 - 0.95 \\left[ 0.75 - 0.25p \\right] $$\n$$ D(p) = 1 - (0.95 \\cdot 0.75) + (0.95 \\cdot 0.25)p $$\n$$ D(p) = 1 - 0.7125 + 0.2375p $$\n$$ D(p) = 0.2875 + 0.2375p $$\n\nSo, the function we want to maximize over $p \\in [0,1]$ is:\n$$ J(p) = \\frac{1.2p + 0.8}{0.2375p + 0.2875} $$\nThis is a fractional linear transformation (or homographic function) of $p$. To find its maximum, we analyze its derivative with respect to $p$.\nLet $J(p) = \\frac{N(p)}{D(p)}$. The derivative is $J'(p) = \\frac{N'(p)D(p) - N(p)D'(p)}{[D(p)]^2}$.\nThe derivatives of the numerator and denominator are:\n$$ N'(p) = 1.2 $$\n$$ D'(p) = 0.2375 $$\nThe sign of $J'(p)$ is determined by the sign of its numerator, since $[D(p)]^2 > 0$.\nThe numerator of $J'(p)$ is:\n$$ (1.2)(0.2875 + 0.2375p) - (1.2p + 0.8)(0.2375) $$\n$$ = (1.2 \\cdot 0.2875) + (1.2 \\cdot 0.2375)p - (1.2 \\cdot 0.2375)p - (0.8 \\cdot 0.2375) $$\n$$ = 1.2 \\cdot 0.2875 - 0.8 \\cdot 0.2375 $$\n$$ = 0.345 - 0.19 $$\n$$ = 0.155 $$\nSince the numerator of the derivative is a positive constant ($0.155$), we have $J'(p) > 0$ for all $p$ in its domain. This means that $J(p)$ is a strictly monotonically increasing function of $p$.\n\nTo maximize a monotonically increasing function on the closed interval $[0,1]$, we must choose the largest possible value for the variable. Therefore, the maximum value of $J(p)$ is attained at $p=1$.\nThe robust optimal mixing probability is $p^{\\star} = 1$.",
            "answer": "$$\\boxed{1}$$"
        },
        {
            "introduction": "After developing a new policy, a critical step is to evaluate its performance before deployment, often using historical data collected under a different policy. Importance Sampling (IS) is a cornerstone of this off-policy evaluation, but its estimators come with their own statistical trade-offs. This problem  challenges you to analytically derive the bias and variance of two common IS estimators. Mastering this analysis is crucial for correctly interpreting evaluation results and understanding the fundamental trade-off between bias and variance in off-policy learning.",
            "id": "4212729",
            "problem": "A Digital Twin (DT) of a Cyber-Physical System (CPS) is used to evaluate a deep target policy in an off-policy setting via Importance Sampling (IS). Consider a finite-horizon Reinforcement Learning (RL) task with horizon length $H \\in \\mathbb{N}$, discount factor $\\gamma \\in (0,1)$, and $N \\in \\mathbb{N}$ independent and identically distributed trajectories collected under a behavior policy. Define the per-decision IS ratios $\\rho_{t}^{(i)} = \\pi(a_{t}^{(i)} \\mid s_{t}^{(i)}) / \\mu(a_{t}^{(i)} \\mid s_{t}^{(i)})$ for time step $t \\in \\{0,1,\\dots,H-1\\}$ and trajectory index $i \\in \\{1,2,\\dots,N\\}$. Let the per-decision cumulative weight be $W_{t}^{(i)} = \\prod_{k=0}^{t} \\rho_{k}^{(i)}$ and the per-step reward be $r_{t}^{(i)}$. The ordinary Per-Decision Importance Sampling (PDIS) estimator and the weighted (self-normalized) PDIS estimator for the discounted return of the target policy are\n$$\n\\hat{J}^{o} = \\sum_{t=0}^{H-1} \\gamma^{t} \\left( \\frac{1}{N} \\sum_{i=1}^{N} W_{t}^{(i)} r_{t}^{(i)} \\right),\n\\qquad\n\\hat{J}^{w} = \\sum_{t=0}^{H-1} \\gamma^{t} \\left( \\frac{ \\sum_{i=1}^{N} W_{t}^{(i)} r_{t}^{(i)} }{ \\sum_{i=1}^{N} W_{t}^{(i)} } \\right).\n$$\nAssume the following foundational modeling conditions suitable for DT-based CPS evaluation:\n- The per-decision ratios $\\rho_{t}^{(i)}$ are independent across time and trajectories with $\\mathbb{E}[\\rho_{t}^{(i)}] = 1$ and $\\operatorname{Var}(\\rho_{t}^{(i)}) = \\sigma_{\\rho}^{2} \\in [0,\\infty)$ for all $t$, and independence across $t$ implies $W_{t}^{(i)}$ has $\\mathbb{E}[W_{t}^{(i)}] = 1$ and second moment $\\mathbb{E}\\!\\left[(W_{t}^{(i)})^{2}\\right] = \\left(1 + \\sigma_{\\rho}^{2} \\right)^{t+1}$.\n- The random variables $r_{t}^{(i)}$ are independent across time and trajectories and independent of $W_{t}^{(i)}$ for each fixed $t$, with target-policy mean $m_{t} = \\mathbb{E}_{\\pi}[r_{t}]$ and variance $v_{t} = \\operatorname{Var}_{\\pi}(r_{t})$, yielding $\\mathbb{E}\\!\\left[ (r_{t}^{(i)})^{2} \\right] = v_{t} + m_{t}^{2}$.\n- The true discounted return of the target policy is $J^{\\pi} = \\sum_{t=0}^{H-1} \\gamma^{t} m_{t}$.\n\nStarting from these definitions and the law of large numbers and delta-method asymptotics for ratio estimators, perform the following:\n1. Derive the bias and variance of $\\hat{J}^{o}$ in closed form, expressed in terms of $H$, $\\gamma$, $N$, $\\sigma_{\\rho}^{2}$, $\\{m_{t}\\}_{t=0}^{H-1}$, and $\\{v_{t}\\}_{t=0}^{H-1}$.\n2. Derive the first-order (in $1/N$) bias and variance of $\\hat{J}^{w}$ using a ratio-of-means expansion, under the stated independence assumptions.\n3. Using your results, provide a closed-form expression for the leading-order difference in mean squared error (MSE) between the ordinary and weighted PDIS estimators, defined as $\\Delta_{\\mathrm{MSE}} = \\mathrm{MSE}(\\hat{J}^{o}) - \\mathrm{MSE}(\\hat{J}^{w})$, retaining terms up to order $1/N$ and neglecting higher-order terms.\n\nYour final answer must be the single closed-form analytic expression for $\\Delta_{\\mathrm{MSE}}$ as a function of $H$, $\\gamma$, $N$, $\\sigma_{\\rho}^{2}$, $\\{m_{t}\\}$, and $\\{v_{t}\\}$, and should not include any units. No rounding is required.",
            "solution": "The problem as stated is scientifically grounded, self-contained, and well-posed. The assumptions, while simplifying, are explicitly stated as foundational modeling conditions and do not contain internal contradictions. We may therefore proceed with a full derivation.\n\nThe overall goal is to find the leading-order difference in Mean Squared Error (MSE), $\\Delta_{\\mathrm{MSE}} = \\mathrm{MSE}(\\hat{J}^{o}) - \\mathrm{MSE}(\\hat{J}^{w})$. This requires deriving the bias and variance for each estimator. The MSE of an estimator $\\hat{\\theta}$ for a parameter $\\theta$ is given by $\\mathrm{MSE}(\\hat{\\theta}) = \\operatorname{Var}(\\hat{\\theta}) + (\\operatorname{Bias}(\\hat{\\theta}))^2$, where $\\operatorname{Bias}(\\hat{\\theta}) = \\mathbb{E}[\\hat{\\theta}] - \\theta$.\n\nLet's first establish the necessary moments based on the problem's givens.\nThe random variables are defined for each trajectory $i \\in \\{1,\\dots,N\\}$ and time step $t \\in \\{0, \\dots, H-1\\}$. Expectations $\\mathbb{E}[\\cdot]$ are taken with respect to the distribution induced by the behavior policy $\\mu$.\n-   Importance weights: $W_{t}^{(i)} = \\prod_{k=0}^{t} \\rho_{k}^{(i)}$. Given $\\rho_{k}^{(i)}$ are i.i.d. with $\\mathbb{E}[\\rho_{k}^{(i)}] = 1$ and $\\operatorname{Var}(\\rho_{k}^{(i)}) = \\sigma_{\\rho}^{2}$.\n    $\\mathbb{E}[W_{t}^{(i)}] = \\prod_{k=0}^{t} \\mathbb{E}[\\rho_{k}^{(i)}] = 1^{t+1} = 1$.\n    $\\mathbb{E}[(W_{t}^{(i)})^2] = \\prod_{k=0}^{t} \\mathbb{E}[(\\rho_{k}^{(i)})^2] = \\prod_{k=0}^{t} (\\operatorname{Var}(\\rho_{k}^{(i)}) + (\\mathbb{E}[\\rho_{k}^{(i)}])^2) = (\\sigma_{\\rho}^{2}+1)^{t+1}$.\n    $\\operatorname{Var}(W_{t}^{(i)}) = \\mathbb{E}[(W_{t}^{(i)})^2] - (\\mathbb{E}[W_{t}^{(i)}])^2 = (1+\\sigma_{\\rho}^{2})^{t+1} - 1$.\n-   Rewards: $r_{t}^{(i)}$. The problem assumes $r_t^{(i)}$ and $W_t^{(i)}$ are independent for a fixed $t$. The fundamental identity of importance sampling is $\\mathbb{E}[W_{t}^{(i)} r_{t}^{(i)}] = \\mathbb{E}_{\\pi}[r_{t}] = m_{t}$. Using the independence assumption, we have $\\mathbb{E}[W_{t}^{(i)} r_{t}^{(i)}] = \\mathbb{E}[W_{t}^{(i)}] \\mathbb{E}[r_{t}^{(i)}] = 1 \\cdot \\mathbb{E}[r_{t}^{(i)}]$. This implies $\\mathbb{E}[r_{t}^{(i)}] = m_{t}$.\n-   The problem provides $\\mathbb{E}[(r_{t}^{(i)})^2] = v_{t} + m_{t}^{2}$. This is the second moment under the behavior policy's distribution. The variance of the reward under the behavior policy is thus $\\operatorname{Var}(r_{t}^{(i)}) = \\mathbb{E}[(r_{t}^{(i)})^2] - (\\mathbb{E}[r_{t}^{(i)}])^2 = (v_{t}+m_{t}^2) - m_{t}^2 = v_{t}$.\n-   The true value to be estimated is $J^{\\pi} = \\sum_{t=0}^{H-1} \\gamma^{t} m_{t}$.\n\nWe proceed with the three parts of the problem.\n\n1. Derivation of the bias and variance of $\\hat{J}^{o}$.\n\nThe ordinary PDIS estimator is $\\hat{J}^{o} = \\sum_{t=0}^{H-1} \\gamma^{t} \\left( \\frac{1}{N} \\sum_{i=1}^{N} W_{t}^{(i)} r_{t}^{(i)} \\right)$.\nTo find the bias, we compute its expectation:\n$$\n\\mathbb{E}[\\hat{J}^{o}] = \\mathbb{E}\\left[ \\sum_{t=0}^{H-1} \\gamma^{t} \\left( \\frac{1}{N} \\sum_{i=1}^{N} W_{t}^{(i)} r_{t}^{(i)} \\right) \\right] = \\sum_{t=0}^{H-1} \\gamma^{t} \\frac{1}{N} \\sum_{i=1}^{N} \\mathbb{E}[W_{t}^{(i)} r_{t}^{(i)}]\n$$\nSince trajectories are i.i.d., $\\mathbb{E}[W_{t}^{(i)} r_{t}^{(i)}]$ is the same for all $i$. Using the IS identity, $\\mathbb{E}[W_{t}^{(i)} r_{t}^{(i)}] = m_{t}$.\n$$\n\\mathbb{E}[\\hat{J}^{o}] = \\sum_{t=0}^{H-1} \\gamma^{t} \\frac{1}{N} (N \\cdot m_{t}) = \\sum_{t=0}^{H-1} \\gamma^{t} m_{t} = J^{\\pi}\n$$\nThe bias is $\\operatorname{Bias}(\\hat{J}^{o}) = \\mathbb{E}[\\hat{J}^{o}] - J^{\\pi} = 0$. $\\hat{J}^{o}$ is an unbiased estimator.\n\nTo find the variance, we use the fact that terms at different time steps are independent, because $\\rho_t^{(i)}$ and $r_t^{(i)}$ are independent across time.\n$$\n\\operatorname{Var}(\\hat{J}^{o}) = \\operatorname{Var}\\left( \\sum_{t=0}^{H-1} \\gamma^{t} \\frac{1}{N} \\sum_{i=1}^{N} W_{t}^{(i)} r_{t}^{(i)} \\right) = \\sum_{t=0}^{H-1} \\gamma^{2t} \\operatorname{Var}\\left( \\frac{1}{N} \\sum_{i=1}^{N} W_{t}^{(i)} r_{t}^{(i)} \\right)\n$$\nSince trajectories are i.i.d., the variance of the mean is the variance of a single term divided by $N$.\n$$\n\\operatorname{Var}(\\hat{J}^{o}) = \\sum_{t=0}^{H-1} \\gamma^{2t} \\frac{1}{N} \\operatorname{Var}(W_{t}^{(i)} r_{t}^{(i)})\n$$\nWe compute the variance of the product term:\n$$\n\\operatorname{Var}(W_{t}^{(i)} r_{t}^{(i)}) = \\mathbb{E}[(W_{t}^{(i)} r_{t}^{(i)})^2] - (\\mathbb{E}[W_{t}^{(i)} r_{t}^{(i)}])^2\n$$\nUsing the independence of $W_t^{(i)}$ and $r_t^{(i)}$, we get $\\mathbb{E}[(W_{t}^{(i)} r_{t}^{(i)})^2] = \\mathbb{E}[(W_{t}^{(i)})^2] \\mathbb{E}[(r_{t}^{(i)})^2]$. Substituting the known moments:\n$$\n\\mathbb{E}[(W_{t}^{(i)} r_{t}^{(i)})^2] = \\left( (1+\\sigma_{\\rho}^{2})^{t+1} \\right) \\left( v_{t} + m_{t}^{2} \\right)\n$$\nAnd $\\mathbb{E}[W_t^{(i)} r_t^{(i)}] = m_t$. Putting it together:\n$$\n\\operatorname{Var}(W_{t}^{(i)} r_{t}^{(i)}) = (1+\\sigma_{\\rho}^{2})^{t+1} (v_{t} + m_{t}^{2}) - m_{t}^{2}\n$$\nSo, the variance of $\\hat{J}^{o}$ is:\n$$\n\\operatorname{Var}(\\hat{J}^{o}) = \\frac{1}{N} \\sum_{t=0}^{H-1} \\gamma^{2t} \\left[ (1+\\sigma_{\\rho}^{2})^{t+1} (v_{t} + m_{t}^{2}) - m_{t}^{2} \\right]\n$$\n\n2. Derivation of the first-order bias and variance of $\\hat{J}^{w}$.\n\nThe weighted PDIS estimator is $\\hat{J}^{w} = \\sum_{t=0}^{H-1} \\gamma^{t} \\hat{m}_{t}$, where $\\hat{m}_{t} = \\frac{ \\sum_{i=1}^{N} W_{t}^{(i)} r_{t}^{(i)} }{ \\sum_{i=1}^{N} W_{t}^{(i)} } = \\frac{A_{t}}{B_{t}}$. Let $A_t = \\frac{1}{N}\\sum_i W_t^{(i)}r_t^{(i)}$ and $B_t = \\frac{1}{N}\\sum_i W_t^{(i)}$. We use the Taylor expansion for the ratio of two random variables. For $\\hat{m}_t = A_t/B_t$, the first-order bias is:\n$$\n\\operatorname{Bias}(\\hat{m}_{t}) \\approx \\frac{\\mathbb{E}[A_{t}]}{\\mathbb{E}[B_{t}]^3} \\operatorname{Var}(B_{t}) - \\frac{1}{\\mathbb{E}[B_{t}]^2} \\operatorname{Cov}(A_{t}, B_{t})\n$$\nWe have $\\mathbb{E}[A_t] = m_t$ and $\\mathbb{E}[B_t] = 1$. $\\operatorname{Var}(B_{t}) = \\frac{1}{N} \\operatorname{Var}(W_t^{(i)}) = \\frac{1}{N}((1+\\sigma_{\\rho}^{2})^{t+1} - 1)$.\nThe covariance term is $\\operatorname{Cov}(A_{t}, B_{t}) = \\frac{1}{N} \\operatorname{Cov}(W_{t}^{(i)} r_{t}^{(i)}, W_{t}^{(i)})$.\n$$\n\\operatorname{Cov}(W_{t}^{(i)} r_{t}^{(i)}, W_{t}^{(i)}) = \\mathbb{E}[W_{t}^{(i)} r_{t}^{(i)} \\cdot W_{t}^{(i)}] - \\mathbb{E}[W_{t}^{(i)} r_{t}^{(i)}] \\mathbb{E}[W_{t}^{(i)}]\n$$\n$$\n= \\mathbb{E}[(W_{t}^{(i)})^2 r_{t}^{(i)}] - m_{t} \\cdot 1 = \\mathbb{E}[(W_{t}^{(i)})^2] \\mathbb{E}[r_{t}^{(i)}] - m_{t} = (1+\\sigma_{\\rho}^{2})^{t+1} m_{t} - m_{t} = m_{t}((1+\\sigma_{\\rho}^{2})^{t+1} - 1)\n$$\nSubstituting these into the bias formula for $\\hat{m}_t$:\n$$\n\\operatorname{Bias}(\\hat{m}_{t}) \\approx \\frac{m_{t}}{1^3} \\frac{1}{N}((1+\\sigma_{\\rho}^{2})^{t+1} - 1) - \\frac{1}{1^2} \\frac{1}{N}m_{t}((1+\\sigma_{\\rho}^{2})^{t+1} - 1) = 0\n$$\nThe first-order bias of $\\hat{m}_{t}$ is zero. Thus, the first-order bias of $\\hat{J}^{w} = \\sum_t \\gamma^t \\hat{m}_t$ is also zero. The bias is of order $O(1/N^2)$.\n\nThe first-order variance of the ratio estimator $\\hat{m}_t$ is:\n$$\n\\operatorname{Var}(\\hat{m}_{t}) \\approx \\frac{1}{\\mathbb{E}[B_{t}]^2} \\operatorname{Var}(A_t - \\frac{\\mathbb{E}[A_t]}{\\mathbb{E}[B_t]} B_t) = \\operatorname{Var}(A_t - m_t B_t)\n$$\n$A_t - m_t B_t = \\frac{1}{N} \\sum_i (W_t^{(i)} r_t^{(i)} - m_t W_t^{(i)}) = \\frac{1}{N} \\sum_i W_t^{(i)}(r_t^{(i)} - m_t)$. Since the trajectories are i.i.d.:\n$$\n\\operatorname{Var}(A_t - m_t B_t) = \\frac{1}{N} \\operatorname{Var}(W_t^{(i)}(r_t^{(i)} - m_t))\n$$\nThe mean of this term is $\\mathbb{E}[W_t^{(i)}(r_t^{(i)} - m_t)] = \\mathbb{E}[W_t^{(i)}r_t^{(i)}] - m_t\\mathbb{E}[W_t^{(i)}] = m_t - m_t \\cdot 1 = 0$.\nSo its variance is its second moment:\n$$\n\\operatorname{Var}(W_t^{(i)}(r_t^{(i)} - m_t)) = \\mathbb{E}[(W_t^{(i)}(r_t^{(i)} - m_t))^2] = \\mathbb{E}[(W_t^{(i)})^2 (r_t^{(i)} - m_t)^2]\n$$\nUsing independence of $W_t^{(i)}$ and $r_t^{(i)}$, this becomes:\n$$\n= \\mathbb{E}[(W_t^{(i)})^2] \\mathbb{E}[(r_t^{(i)} - m_t)^2] = (1+\\sigma_\\rho^2)^{t+1} \\operatorname{Var}(r_t^{(i)}) = (1+\\sigma_\\rho^2)^{t+1} v_t\n$$\nSo, $\\operatorname{Var}(\\hat{m}_t) \\approx \\frac{1}{N}(1+\\sigma_\\rho^2)^{t+1} v_t$. The total variance of $\\hat{J}^w$ is the sum over independent time steps:\n$$\n\\operatorname{Var}(\\hat{J}^{w}) \\approx \\sum_{t=0}^{H-1} \\gamma^{2t} \\operatorname{Var}(\\hat{m}_{t}) = \\frac{1}{N} \\sum_{t=0}^{H-1} \\gamma^{2t} (1+\\sigma_\\rho^2)^{t+1} v_t\n$$\n\n3. Derivation of the leading-order difference in MSE.\n\nThe MSE is $\\operatorname{Var}(\\cdot) + (\\operatorname{Bias}(\\cdot))^2$. We retain terms up to $O(1/N)$.\nFor $\\hat{J}^o$, the bias is exactly zero.\n$$\n\\mathrm{MSE}(\\hat{J}^{o}) = \\operatorname{Var}(\\hat{J}^{o}) = \\frac{1}{N} \\sum_{t=0}^{H-1} \\gamma^{2t} \\left[ (1+\\sigma_{\\rho}^{2})^{t+1} (v_{t} + m_{t}^{2}) - m_{t}^{2} \\right]\n$$\nFor $\\hat{J}^w$, the bias is of order $O(1/N^2)$, so the squared bias is $O(1/N^4)$, which is negligible.\n$$\n\\mathrm{MSE}(\\hat{J}^{w}) \\approx \\operatorname{Var}(\\hat{J}^{w}) = \\frac{1}{N} \\sum_{t=0}^{H-1} \\gamma^{2t} (1+\\sigma_\\rho^2)^{t+1} v_t\n$$\nNow we compute the difference $\\Delta_{\\mathrm{MSE}} = \\mathrm{MSE}(\\hat{J}^{o}) - \\mathrm{MSE}(\\hat{J}^{w})$ up to order $O(1/N)$:\n$$\n\\Delta_{\\mathrm{MSE}} \\approx \\frac{1}{N} \\sum_{t=0}^{H-1} \\gamma^{2t} \\left( \\left[ (1+\\sigma_{\\rho}^{2})^{t+1} (v_{t} + m_{t}^{2}) - m_{t}^{2} \\right] - \\left[ (1+\\sigma_{\\rho}^{2})^{t+1} v_{t} \\right] \\right)\n$$\n$$\n\\Delta_{\\mathrm{MSE}} \\approx \\frac{1}{N} \\sum_{t=0}^{H-1} \\gamma^{2t} \\left( (1+\\sigma_{\\rho}^{2})^{t+1}v_{t} + (1+\\sigma_{\\rho}^{2})^{t+1}m_{t}^{2} - m_{t}^{2} - (1+\\sigma_{\\rho}^{2})^{t+1} v_{t} \\right)\n$$\nThe terms involving $v_t$ cancel out.\n$$\n\\Delta_{\\mathrm{MSE}} \\approx \\frac{1}{N} \\sum_{t=0}^{H-1} \\gamma^{2t} \\left( (1+\\sigma_{\\rho}^{2})^{t+1}m_{t}^{2} - m_{t}^{2} \\right)\n$$\nFactoring out $m_t^2$ yields the final expression for the leading-order MSE difference.\n$$\n\\Delta_{\\mathrm{MSE}} = \\frac{1}{N} \\sum_{t=0}^{H-1} \\gamma^{2t} m_t^2 \\left( (1+\\sigma_{\\rho}^{2})^{t+1} - 1 \\right)\n$$\nThis expression represents the reduction in variance achieved by the self-normalization in the weighted estimator, which removes the component of variance related to the mean reward $m_t$.",
            "answer": "$$\n\\boxed{\\frac{1}{N} \\sum_{t=0}^{H-1} \\gamma^{2t} m_{t}^{2} \\left( (1+\\sigma_{\\rho}^{2})^{t+1} - 1 \\right)}\n$$"
        }
    ]
}