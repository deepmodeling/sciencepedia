{
    "hands_on_practices": [
        {
            "introduction": "深入理解高斯过程回归 (GPR) 的第一步是亲手实现其核心算法。本练习将指导你从基本原理出发，实现 GPR 的预测方程，重点在于使用 Cholesky 分解等数值稳定技术来处理协方差矩阵，这是在实践中避免数值错误和提高计算效率的关键。通过这个练习 ，你将能够将理论知识转化为可工作的代码，并探索不同超参数设置对模型预测的影响。",
            "id": "4224416",
            "problem": "考虑一个针对信息物理系统（CPS）数字孪生的代理建模任务，其中潜系统响应被建模为一个高斯过程（GP）。高斯过程（GP）定义为一组随机变量的集合，其中任何有限子集都服从一个多元高斯分布。设潜函数表示为 $f(\\cdot)$，观测输出为 $y$，其中 $y$ 是 $f(\\cdot)$ 的带噪声观测，噪声为附加的独立高斯噪声。\n\n假设先验为零均值 $m(x)=0$，协方差函数为具有已知超参数的 Matérn 协方差函数。观测模型为 $y_i = f(x_i) + \\epsilon_i$，其中对于 $i \\in \\{1,\\dots,n\\}$，$\\epsilon_i \\sim \\mathcal{N}(0,\\sigma_n^2)$ 独立同分布。Matérn 核 $k_\\nu(\\cdot,\\cdot)$ 通过欧几里得距离 $r = \\lVert x - x' \\rVert_2$ 定义为\n$$\nk_\\nu(r) \\;=\\; \\sigma_f^2 \\cdot \\frac{2^{1-\\nu}}{\\Gamma(\\nu)} \\left( \\frac{\\sqrt{2\\nu}\\, r}{\\ell} \\right)^\\nu K_\\nu \\!\\left( \\frac{\\sqrt{2\\nu}\\, r}{\\ell} \\right),\n$$\n其中 $\\sigma_f^2$ 是信号方差，$\\ell$ 是长度尺度，$\\nu$ 是平滑度参数，$\\Gamma(\\cdot)$ 是伽马函数，而 $K_\\nu(\\cdot)$ 是第二类修正贝塞尔函数。成立条件为 $k_\\nu(0) = \\sigma_f^2$。\n\n从高斯过程先验和线性高斯观测模型的定义出发，推导在一个新输入点 $x_*$ 处潜函数 $f(x_*)$ 的预测分布。实现一个程序，使用数值稳定的线性代数方法计算在 $x_*$ 处的预测均值和方差。\n\n您的实现必须：\n- 使用给定的 Matérn 核构建格拉姆矩阵 $K$，并将观测噪声方差 $\\sigma_n^2$ 加到其对角线上。\n- 使用数值稳定的求解器（例如，基于 Cholesky 分解的方法）以避免显式矩阵求逆。\n- 计算潜函数 $f(x_*)$ 的预测均值和方差。\n\n无需物理单位；所有量均为无量纲实数。不涉及角度。不涉及百分比。\n\n测试套件：\n使用以下测试用例。每个用例指定了 $X \\in \\mathbb{R}^{n \\times d}$，$y \\in \\mathbb{R}^n$，超参数 $(\\sigma_f,\\ell,\\nu,\\sigma_n)$，以及一个查询点 $x_* \\in \\mathbb{R}^d$。所有数值条目均为实数。\n\n- 用例 1（通用二维，中等长度尺度）：\n  $$\n  X^{(1)} = \\begin{bmatrix}\n  0.0  0.0\\\\\n  0.5  -0.2\\\\\n  1.0  0.3\\\\\n  1.5  -0.5\\\\\n  2.0  0.0\n  \\end{bmatrix},\\quad\n  y^{(1)} = \\begin{bmatrix}\n  0.0\\\\\n  0.329418342\\\\\n  0.807356092\\\\\n  0.782039086\\\\\n  0.999573603\n  \\end{bmatrix},\n  $$\n  $$\n  (\\sigma_f^{(1)},\\ell^{(1)},\\nu^{(1)},\\sigma_n^{(1)}) = (1.2,\\,0.7,\\,1.5,\\,0.1),\\quad\n  x_*^{(1)} = \\begin{bmatrix} 1.2\\\\ 0.1 \\end{bmatrix}.\n  $$\n\n- 用例 2（边缘情况：查询点等于一个训练输入）：\n  $$\n  X^{(2)} = X^{(1)},\\quad y^{(2)} = y^{(1)},\\quad\n  (\\sigma_f^{(2)},\\ell^{(2)},\\nu^{(2)},\\sigma_n^{(2)}) = (1.2,\\,0.7,\\,1.5,\\,0.1),\\quad\n  x_*^{(2)} = \\begin{bmatrix} 1.0\\\\ 0.3 \\end{bmatrix}.\n  $$\n\n- 用例 3（边界情况：极小长度尺度，强局部性）：\n  $$\n  X^{(3)} = X^{(1)},\\quad y^{(3)} = y^{(1)},\\quad\n  (\\sigma_f^{(3)},\\ell^{(3)},\\nu^{(3)},\\sigma_n^{(3)}) = (1.2,\\,0.05,\\,1.5,\\,0.05),\\quad\n  x_*^{(3)} = \\begin{bmatrix} 1.1\\\\ 0.25 \\end{bmatrix}.\n  $$\n\n- 用例 4（边界情况：极大长度尺度，近乎恒定的行为）：\n  $$\n  X^{(4)} = X^{(1)},\\quad y^{(4)} = y^{(1)},\\quad\n  (\\sigma_f^{(4)},\\ell^{(4)},\\nu^{(4)},\\sigma_n^{(4)}) = (1.2,\\,50.0,\\,1.5,\\,0.1),\\quad\n  x_*^{(4)} = \\begin{bmatrix} 10.0\\\\ -10.0 \\end{bmatrix}.\n  $$\n\n- 用例 5（一维输入，不同平滑度）：\n  $$\n  X^{(5)} = \\begin{bmatrix}\n  0.0\\\\ 1.0\\\\ 2.0\\\\ 3.0\n  \\end{bmatrix},\\quad\n  y^{(5)} = \\begin{bmatrix}\n  1.0\\\\ 0.540302306\\\\ -0.416146837\\\\ -0.989992497\n  \\end{bmatrix},\n  $$\n  $$\n  (\\sigma_f^{(5)},\\ell^{(5)},\\nu^{(5)},\\sigma_n^{(5)}) = (1.0,\\,1.0,\\,2.5,\\,0.01),\\quad\n  x_*^{(5)} = \\begin{bmatrix} 1.5 \\end{bmatrix}.\n  $$\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。列表中的每个测试用例结果是一个双元素列表 $[\\mu, s^2]$，包含对应 $x_*$ 的预测均值 $\\mu$ 和方差 $s^2$。具体而言，输出必须是：\n$$\n\\big[ [\\mu^{(1)}, s^{2,(1)}], [\\mu^{(2)}, s^{2,(2)}], [\\mu^{(3)}, s^{2,(3)}], [\\mu^{(4)}, s^{2,(4)}], [\\mu^{(5)}, s^{2,(5)}] \\big].\n$$",
            "solution": "用户在统计机器学习领域提供了一个定义明确的问题。任务是推导并实现高斯过程（GP）回归的预测方程。\n\n### 问题验证\n\n**第 1 步：提取已知条件**\n\n- **模型**：使用高斯过程来对潜函数 $f(\\cdot)$ 进行建模。\n- **先验**：GP 具有零均值先验，$m(x)=0$。\n- **协方差**：协方差函数是具有已知超参数 $\\sigma_f^2$（信号方差）、$\\ell$（长度尺度）和 $\\nu$（平滑度）的 Matérn 核 $k_\\nu(\\cdot,\\cdot)$。该核定义为：\n$$\nk_\\nu(r) \\;=\\; \\sigma_f^2 \\cdot \\frac{2^{1-\\nu}}{\\Gamma(\\nu)} \\left( \\frac{\\sqrt{2\\nu}\\, r}{\\ell} \\right)^\\nu K_\\nu \\!\\left( \\frac{\\sqrt{2\\nu}\\, r}{\\ell} \\right)\n$$\n其中 $r = \\lVert x - x' \\rVert_2$，$\\Gamma(\\cdot)$ 是伽马函数，而 $K_\\nu(\\cdot)$ 是第二类修正贝塞尔函数。一个给定的性质是 $k_\\nu(0) = \\sigma_f^2$。\n- **观测模型**：观测值 $y_i$ 是潜函数的带噪声版本，$y_i = f(x_i) + \\epsilon_i$，噪声 $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma_n^2)$ 是独立同分布的。\n- **目标**：推导在新输入点 $x_*$ 处潜函数值 $f(x_*)$ 的预测分布。\n- **实现约束**：\n    1. 构建格拉姆矩阵 $K$。\n    2. 将观测噪声方差 $\\sigma_n^2$ 加到对角线上。\n    3. 使用像 Cholesky 分解这样数值稳定的方法来避免显式矩阵求逆。\n    4. 计算 $f(x_*)$ 的预测均值和方差。\n- **数据**：提供了五个测试用例，每个用例都包含训练输入 $X$、训练输出 $y$、超参数 $(\\sigma_f, \\ell, \\nu, \\sigma_n)$ 和一个测试输入 $x_*$。\n\n**第 2 步：使用提取的已知条件进行验证**\n\n- **科学依据充分**：该问题是高斯过程贝叶斯推断的一个标准应用，这是现代机器学习和统计学的基石。所有定义和公式在该领域都是正确且标准的。\n- **定义明确**：该问题是良构的。给定模型、数据和超参数，后验预测分布是唯一确定的。对数值稳定求解器的要求承认并解决了协方差矩阵潜在的病态问题，确保可以计算出稳定的解。\n- **目标**：问题以精确的数学语言陈述，没有歧义，并为获得唯一解提供了所有必要的数值数据。\n- **完整性和一致性**：问题是自洽的。它提供了完整的模型规范（先验、似然、核函数）和计算所需的所有数据。没有矛盾之处。\n- **现实性**：该任务是代理建模的一个典型例子，这是数字孪生和信息物理系统中的一个关键功能。数值对于此类问题是现实的。\n\n**第 3 步：结论与行动**\n\n该问题是**有效**的，因为它科学上合理、定义明确且完整。我现在将继续进行解决方案的推导和实现。\n\n### 预测分布的推导\n\n高斯过程回归的核心原理是，任何有限的函数值集合都服从联合高斯分布。我们感兴趣的是在给定训练数据 $D = \\{X, \\mathbf{y}\\}$ 的条件下，测试点 $x_*$ 处的潜函数值 $f_* = f(x_*)$ 的条件分布，其中 $X = \\{x_1, \\dots, x_n\\}$ 是训练输入集，$\\mathbf{y} = [y_1, \\dots, y_n]^T$ 是相应带噪声观测值的向量。\n\n1.  **联合先验分布**：根据 GP 先验，训练点处的潜函数值向量 $\\mathbf{f} = [f(x_1), \\dots, f(x_n)]^T$ 和测试点处的值 $f_*$ 是联合高斯的。给定零均值先验，此分布为：\n    $$\n    \\begin{bmatrix} \\mathbf{f} \\\\ f_* \\end{bmatrix} \\sim \\mathcal{N} \\left( \\begin{bmatrix} \\mathbf{0} \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} K(X, X) & K(X, x_*) \\\\ K(x_*, X) & K(x_*, x_*) \\end{bmatrix} \\right)\n    $$\n    让我们用更简洁的符号表示分块协方差矩阵：$K = K(X, X)$（$n \\times n$ 的格拉姆矩阵），$k_* = K(X, x_*)$（训练点和测试点之间协方差的 $n \\times 1$ 向量），以及 $k_{**} = K(x_*, x_*)$（测试点处的先验方差）。因此，协方差矩阵为 $\\begin{bmatrix} K & k_* \\\\ k_*^T & k_{**} \\end{bmatrix}$。\n\n2.  **观测值与潜在测试值的联合分布**：观测模型为 $\\mathbf{y} = \\mathbf{f} + \\boldsymbol{\\epsilon}$，其中噪声向量 $\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma_n^2 I_n)$。由于 $\\mathbf{f}$ 和 $\\boldsymbol{\\epsilon}$ 是独立的，它们的和 $\\mathbf{y}$ 也是一个高斯随机变量。观测数据 $\\mathbf{y}$ 和潜函数值 $f_*$ 的联合分布为：\n    $$\n    \\begin{bmatrix} \\mathbf{y} \\\\ f_* \\end{bmatrix} \\sim \\mathcal{N} \\left( \\begin{bmatrix} \\mathbf{0} \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} K + \\sigma_n^2 I_n & k_* \\\\ k_*^T & k_{**} \\end{bmatrix} \\right)\n    $$\n    这里，$\\mathbf{y}$ 和 $f_*$ 之间的协方差是 $\\text{Cov}(\\mathbf{f} + \\boldsymbol{\\epsilon}, f_*) = \\text{Cov}(\\mathbf{f}, f_*) + \\text{Cov}(\\boldsymbol{\\epsilon}, f_*) = k_* + \\mathbf{0} = k_*$。$\\mathbf{y}$ 的方差是 $\\text{Var}(\\mathbf{f} + \\boldsymbol{\\epsilon}) = \\text{Var}(\\mathbf{f}) + \\text{Var}(\\boldsymbol{\\epsilon}) = K + \\sigma_n^2 I_n$。\n\n3.  **条件（后验）分布**：我们寻求分布 $p(f_* | \\mathbf{y}, X, x_*)$。使用多元高斯分布的条件分布标准公式，对于一个联合分布 $\\begin{pmatrix} \\mathbf{a} \\\\ \\mathbf{b} \\end{pmatrix} \\sim \\mathcal{N} \\left( \\begin{pmatrix} \\boldsymbol{\\mu}_a \\\\ \\boldsymbol{\\mu}_b \\end{pmatrix}, \\begin{pmatrix} \\Sigma_{aa} & \\Sigma_{ab} \\\\ \\Sigma_{ba} & \\Sigma_{bb} \\end{pmatrix} \\right)$，条件分布 $p(\\mathbf{b}|\\mathbf{a})$ 是一个高斯分布，其均值为 $\\boldsymbol{\\mu}_{b|a} = \\boldsymbol{\\mu}_b + \\Sigma_{ba} \\Sigma_{aa}^{-1} (\\mathbf{a} - \\boldsymbol{\\mu}_a)$，协方差为 $\\Sigma_{b|a} = \\Sigma_{bb} - \\Sigma_{ba} \\Sigma_{aa}^{-1} \\Sigma_{ab}$。\n\n    通过代入我们的项（$\\mathbf{a} \\to \\mathbf{y}$，$\\mathbf{b} \\to f_*$，零均值，以及推导出的协方差结构），我们得到 $f_*$ 的预测分布：\n    -   **预测均值** $\\mu_*(x_*)$:\n        $$\n        \\mu_*(x_*) = \\mathbf{0} + k_*^T (K + \\sigma_n^2 I_n)^{-1} (\\mathbf{y} - \\mathbf{0}) = k_*^T (K + \\sigma_n^2 I_n)^{-1} \\mathbf{y}\n        $$\n    -   **预测方差** $s^2_*(x_*)$:\n        $$\n        s^2_*(x_*) = k_{**} - k_*^T (K + \\sigma_n^2 I_n)^{-1} k_*\n        $$\n\n### 数值稳定的实现\n\n直接计算矩阵的逆 $(K + \\sigma_n^2 I_n)^{-1}$ 在数值上不稳定且效率低下。一个更优的方法是使用 Cholesky 分解。令 $K_{yy} = K + \\sigma_n^2 I_n$。由于 $K$ 是对称半正定的，且 $\\sigma_n^2 > 0$，因此 $K_{yy}$ 是对称正定的。于是，它有唯一的 Cholesky 分解 $K_{yy} = LL^T$，其中 $L$ 是一个下三角矩阵。\n\n预测均值和方差可以按如下方式计算：\n\n1.  为了计算均值 $\\mu_*(x_*) = k_*^T \\boldsymbol{\\alpha}$，其中 $\\boldsymbol{\\alpha} = K_{yy}^{-1}\\mathbf{y}$，我们求解线性系统 $K_{yy}\\boldsymbol{\\alpha} = \\mathbf{y}$。使用分解，这变为 $LL^T\\boldsymbol{\\alpha} = \\mathbf{y}$。我们用代入法分两步求解：\n    a. 求解 $L\\mathbf{v} = \\mathbf{y}$ 得到 $\\mathbf{v}$（前向代入）。\n    b. 求解 $L^T\\boldsymbol{\\alpha} = \\mathbf{v}$ 得到 $\\boldsymbol{\\alpha}$（反向代入）。\n\n2.  为了计算方差 $s^2_*(x_*) = k_{**} - k_*^T K_{yy}^{-1} k_*$，我们计算二次型。令 $\\mathbf{z} = L^{-1}k_*$。我们可以通过求解 $L\\mathbf{z} = k_*$（使用前向代入）来找到 $\\mathbf{z}$。然后，二次型可以写成：\n    $$\n    k_*^T K_{yy}^{-1} k_* = k_*^T (LL^T)^{-1} k_* = k_*^T (L^T)^{-1}L^{-1} k_* = (L^{-1}k_*)^T (L^{-1}k_*) = \\mathbf{z}^T\\mathbf{z} = \\lVert \\mathbf{z} \\rVert_2^2\n    $$\n    因此，预测方差为 $s^2_*(x_*) = k_{**} - \\mathbf{z}^T\\mathbf{z}$。这种方法避免了第二次三角求解，并且数值上很稳健。\n\n### Matérn 核的计算\n\nMatérn 核需要计算伽马函数 $\\Gamma(\\nu)$ 和第二类修正贝塞尔函数 $K_\\nu(z)$。这些函数在 `scipy.special` 中可用。一个关键细节是 $r=0$ 时的行为。此时，贝塞尔函数的参数为零，而 $K_\\nu(z)$ 在 $z \\to 0$ 时发散。然而，乘积 $z^\\nu K_\\nu(z)$ 的极限是一个有限值 $2^{\\nu-1}\\Gamma(\\nu)$。将此代入核函数公式，可以证实给定的性质 $k_\\nu(0) = \\sigma_f^2$。实现时必须单独处理 $r=0$ 的情况，以避免出现 $0 \\times \\infty$ 之类的数值错误。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import gamma, kv\nfrom scipy.linalg import cholesky, solve_triangular\n\ndef solve():\n    \"\"\"\n    Solves for the predictive mean and variance for a series of Gaussian Process\n    regression test cases.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"X\": np.array([\n                [0.0, 0.0], [0.5, -0.2], [1.0, 0.3], [1.5, -0.5], [2.0, 0.0]\n            ]),\n            \"y\": np.array([0.0, 0.329418342, 0.807356092, 0.782039086, 0.999573603]),\n            \"params\": (1.2, 0.7, 1.5, 0.1),\n            \"x_star\": np.array([1.2, 0.1])\n        },\n        {\n            \"X\": np.array([\n                [0.0, 0.0], [0.5, -0.2], [1.0, 0.3], [1.5, -0.5], [2.0, 0.0]\n            ]),\n            \"y\": np.array([0.0, 0.329418342, 0.807356092, 0.782039086, 0.999573603]),\n            \"params\": (1.2, 0.7, 1.5, 0.1),\n            \"x_star\": np.array([1.0, 0.3])\n        },\n        {\n            \"X\": np.array([\n                [0.0, 0.0], [0.5, -0.2], [1.0, 0.3], [1.5, -0.5], [2.0, 0.0]\n            ]),\n            \"y\": np.array([0.0, 0.329418342, 0.807356092, 0.782039086, 0.999573603]),\n            \"params\": (1.2, 0.05, 1.5, 0.05),\n            \"x_star\": np.array([1.1, 0.25])\n        },\n        {\n            \"X\": np.array([\n                [0.0, 0.0], [0.5, -0.2], [1.0, 0.3], [1.5, -0.5], [2.0, 0.0]\n            ]),\n            \"y\": np.array([0.0, 0.329418342, 0.807356092, 0.782039086, 0.999573603]),\n            \"params\": (1.2, 50.0, 1.5, 0.1),\n            \"x_star\": np.array([10.0, -10.0])\n        },\n        {\n            \"X\": np.array([\n                [0.0], [1.0], [2.0], [3.0]\n            ]),\n            \"y\": np.array([1.0, 0.540302306, -0.416146837, -0.989992497]),\n            \"params\": (1.0, 1.0, 2.5, 0.01),\n            \"x_star\": np.array([1.5])\n        }\n    ]\n\n    def matern_kernel(X1, X2, sigma_f, l, nu):\n        \"\"\"\n        Computes the Matérn covariance matrix between two sets of points.\n        \"\"\"\n        # Compute squared Euclidean distance matrix\n        # dist_sq = sum_i( (X1_i - X2_i)^2 )\n        dist_sq = np.sum(X1**2, axis=1)[:, np.newaxis] + np.sum(X2**2, axis=1) - 2 * np.dot(X1, X2.T)\n        # Numerical stability for small negative values from floating point errors\n        dist_sq = np.maximum(dist_sq, 0)\n        r = np.sqrt(dist_sq)\n\n        # Handle the r=0 special case\n        # For r=0, the kernel value is sigma_f^2\n        # For r>0, we use the standard formula\n        # A small epsilon is used for numerical stability\n        eps = 1e-9\n        r_is_zero = r  eps\n        \n        # Pre-compute constants\n        const_factor = (2**(1 - nu)) / gamma(nu)\n        sigma_f_sq = sigma_f**2\n\n        # Create a result matrix filled with the r=0 case\n        K = np.full(r.shape, sigma_f_sq)\n\n        # Compute arguments for the non-zero distance case\n        r_nz = r[~r_is_zero]\n        arg = (np.sqrt(2 * nu) * r_nz) / l\n        \n        # Calculate kernel values for r > 0\n        K[~r_is_zero] = sigma_f_sq * const_factor * (arg**nu) * kv(nu, arg)\n        \n        return K\n\n    results = []\n    for case in test_cases:\n        X, y = case[\"X\"], case[\"y\"]\n        sigma_f, l, nu, sigma_n = case[\"params\"]\n        x_star = case[\"x_star\"].reshape(1, -1) # Ensure x_star is 2D\n        n = X.shape[0]\n\n        # 1. Construct the Gram matrix K(X, X) and add noise\n        K = matern_kernel(X, X, sigma_f, l, nu)\n        K_yy = K + (sigma_n**2) * np.eye(n)\n\n        # 2. Compute the Cholesky decomposition of K_yy\n        # L L^T = K_yy\n        try:\n            L = cholesky(K_yy, lower=True)\n        except np.linalg.LinAlgError:\n            # Fallback for numerically challenging matrices\n            # Add a small jitter to the diagonal\n            jitter = 1e-6\n            L = cholesky(K_yy + jitter * np.eye(n), lower=True)\n\n\n        # 3. Compute the cross-covariance vector k_* = K(X, x_*)\n        k_star = matern_kernel(X, x_star, sigma_f, l, nu)\n\n        # 4. Compute the predictive mean\n        # Solve L v = y, then L^T alpha = v\n        v = solve_triangular(L, y, lower=True)\n        alpha = solve_triangular(L.T, v, lower=False)\n        mu_star = np.dot(k_star.T, alpha)\n\n        # 5. Compute the predictive variance\n        # Solve L z = k_*, then var = k_** - z^T z\n        z = solve_triangular(L, k_star, lower=True)\n        k_star_star = sigma_f**2 # k(x_*, x_*) where r=0\n        s2_star = k_star_star - np.dot(z.T, z)\n\n        # Ensure results are scalar floats\n        mu_val = float(mu_star)\n        s2_val = float(s2_star)\n        results.append([mu_val, s2_val])\n\n    # Format the final output string exactly as required\n    output_parts = [f\"[{mu},{s2}]\" for mu, s2 in results]\n    print(f\"[{','.join(output_parts)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "高斯过程回归的真正威力在于它能够量化预测的不确定性。在数字孪生和信息物理系统中，我们通常不仅关心某个量的瞬时值，更关心由它积分或变换而来的派生量（如总能耗）。本练习  将教你如何将 GPR 代理模型在多个时间点上的后验不确定性，通过线性变换（黎曼和）传播到对总能量的估计上，从而为系统级的决策提供带有置信区间的关键性能指标。",
            "id": "4224377",
            "problem": "考虑一个信息物理系统 (CPS) 的数字孪生 (DT)，它使用高斯过程回归 (GPR) 作为代理模型，来预测在固定工作模式下瞬时电功率消耗 $f(t)$（单位：瓦特）随时间 $t$（单位：秒）的变化。其目标是通过严格传播代理模型的不确定性，计算在整个工作周期内消耗总能量的不确定性边界。所有涉及物理单位的量必须以一致的单位报告：时间以秒为单位，能量以焦耳为单位。\n\n您将获得以下训练数据和模型假设：\n\n- 训练输入（时间样本，单位：秒）：$[0,2,4,6,8]$。\n- 训练输出（测量的功率，单位：瓦特，与上述时间对应）：$[120.0,135.0,128.0,140.0,132.0]$。\n- 核函数选择：平方指数协方差，其幅度参数为 $\\sigma_f^2$，长度尺度为 $\\ell$，由下式给出\n$$\nk(t,t') = \\sigma_f^2 \\exp\\left(-\\frac{(t-t')^2}{2\\ell^2}\\right).\n$$\n- 超参数：$\\sigma_f^2 = 900.0$（瓦特平方），$\\ell = 2.5$（秒），以及独立的测量高斯噪声方差 $\\sigma_n^2 = 9.0$（瓦特平方）。\n\n假设工作周期由一组有限的查询时间 $t_i$（单位：秒）指定，每个时间点代表在均匀时间间隔 $\\Delta t$（单位：秒）下对代理模型 $f(t)$ 的一次瞬时采样。总能量通过黎曼和 $\\sum_i f(t_i)\\,\\Delta t$ 近似计算，并且必须以焦耳为单位报告。由于代理模型的预测具有不确定性，需要传播 GPR 的后验不确定性，以获得总能量在 $0.95$ 水平上的双边中心可信区间的下界和上界。所有边界均以焦耳为单位，报告为实值小数。\n\n实现一个程序，该程序：\n- 对于每个工作周期案例，使用提供的训练数据和超参数，在指定的查询时间点上构建 $f(t)$ 的 GPR 后验分布。\n- 通过黎曼和传播代理模型的后验不确定性，以获得总能量（单位：焦耳）在 $0.95$ 水平上的中心可信区间。\n- 生成单行输出，包含每个案例的边界，其中每个案例的结果是一个双元素列表 $[L,U]$，分别代表 $0.95$ 可信区间的下界 $L$ 和上界 $U$。\n\n使用以下工作周期测试套件：\n1. 正常路径：查询时间为 $[1,2,3,4,5,6,7,8,9]$，均匀间隔 $\\Delta t = 1.0$ 秒。\n2. 边界情况：查询时间为空 $[]$，$\\Delta t = 1.0$ 秒（这代表零长度周期，应产生零能量和零不确定性）。\n3. 单样本边缘情况：查询时间为 $[5.0]$，$\\Delta t = 2.0$ 秒。\n4. 密集采样边缘情况：查询时间为 $[0.0,0.5,1.0,1.5,2.0,2.5,3.0,3.5,4.0,4.5,5.0,5.5,6.0,6.5,7.0,7.5,8.0]$，$\\Delta t = 0.5$ 秒。\n\n您的程序应生成单行输出，其中包含一个由方括号括起来的逗号分隔列表，形式如下\n$$\n[[L_1,U_1],[L_2,U_2],[L_3,U_3],[L_4,U_4]],\n$$\n其中每个 $L_i$ 和 $U_i$ 都是实数，代表以上述顺序列出的第 $i$ 个案例的 $0.95$ 可信区间的下界和上界（单位：焦耳）。",
            "solution": "该问题是有效的，因为它具有科学依据、客观、适定，并包含得出唯一解所需的所有必要信息。任务是将高斯过程回归 (GPR) 功耗代理模型的不确定性传播到总能耗的估计中。\n\n解决方案包括三个主要阶段：\n1.  基于提供的训练数据，为功率函数 $f(t)$ 构建 GPR 后验分布。\n2.  通过计算总能量（即黎曼和）的线性算子，传播该后验分布的均值和协方差。\n3.  使用得到的总能量分布来计算 $0.95$ 中心可信区间的边界。\n\n**1. 高斯过程回归模型**\n\n高斯过程 (GP) 定义了函数上的一个概率分布。我们将未知的功耗函数 $f(t)$ 建模为从一个 GP 中抽取的一个样本：\n$$\nf(t) \\sim \\mathcal{GP}(\\mu(t), k(t, t'))\n$$\n其中 $\\mu(t)$ 是均值函数，$k(t, t')$ 是协方差（或核）函数。按照标准实践，我们假设一个零均值先验，即 $\\mu(t) = 0$。问题指定了一个平方指数核函数：\n$$\nk(t, t') = \\sigma_f^2 \\exp\\left(-\\frac{(t-t')^2}{2\\ell^2}\\right)\n$$\n其超参数为 $\\sigma_f^2 = 900.0 \\, \\text{W}^2$（幅度）和 $\\ell = 2.5 \\, \\text{s}$（长度尺度）。\n\n训练数据包括 $N=5$ 个带噪声的功率测量值。训练输入是时间点 $X = [0, 2, 4, 6, 8]^T$ 秒，相应的训练输出是功率测量值 $Y = [120.0, 135.0, 128.0, 140.0, 132.0]^T$ 瓦特。观测模型为 $y_i = f(t_i) + \\epsilon_i$，其中测量噪声 $\\epsilon_i$ 假设为独立同分布的高斯噪声，$\\epsilon_i \\sim \\mathcal{N}(0, \\sigma_n^2)$，噪声方差为 $\\sigma_n^2 = 9.0 \\, \\text{W}^2$。\n\n给定训练数据 $(X, Y)$ 和一组 $N_*$ 个查询时间点 $X_* = [t_{*1}, \\dots, t_{*N_*}]^T$，GPR 框架提供了函数值向量 $\\mathbf{f}_* = [f(t_{*1}), \\dots, f(t_{*N_*})]^T$ 的后验分布。该后验分布是一个多变量高斯分布，$p(\\mathbf{f}_* | X, Y, X_*) = \\mathcal{N}(\\boldsymbol{\\mu}_*, \\boldsymbol{\\Sigma}_*)$，其均值向量为 $\\boldsymbol{\\mu}_*$，协方差矩阵为 $\\boldsymbol{\\Sigma}_*$。\n\n后验均值和协方差由下式给出：\n$$\n\\boldsymbol{\\mu}_* = K(X_*, X) [K(X, X) + \\sigma_n^2 I_N]^{-1} Y\n$$\n$$\n\\boldsymbol{\\Sigma}_* = K(X_*, X_*) - K(X_*, X) [K(X, X) + \\sigma_n^2 I_N]^{-1} K(X, X_*)\n$$\n其中：\n- $I_N$ 是 $N \\times N$ 单位矩阵。\n- $K(X, X)$ 是训练输入的 $N \\times N$ 协方差矩阵，其元素为 $[K(X, X)]_{ij} = k(t_i, t_j)$。\n- $K(X, X_*)$ 是训练输入和查询输入之间的 $N \\times N_*$ 互协方差矩阵，其元素为 $[K(X, X_*)]_{ij} = k(t_i, t_{*j})$。\n- $K(X_*, X) = K(X, X_*)^T$。\n- $K(X_*, X_*)$ 是查询输入的 $N_* \\times N_*$ 协方差矩阵，其元素为 $[K(X_*, X_*)]_{ij} = k(t_{*i}, t_{*j})$。\n\n**2. 总能量的不确定性传播**\n\n在工作周期内消耗的总能量 $E$（单位：焦耳）通过黎曼和近似计算：\n$$\nE = \\sum_{j=1}^{N_*} f(t_{*j}) \\Delta t\n$$\n其中 $\\Delta t$ 是均匀的时间间隔（单位：秒）。这是函数值向量 $\\mathbf{f}_*$ 的一个线性变换：\n$$\nE = \\mathbf{w}^T \\mathbf{f}_* \\quad \\text{其中} \\quad \\mathbf{w} = [\\Delta t, \\Delta t, \\dots, \\Delta t]^T\n$$\n由于 $\\mathbf{f}_*$ 服从多变量高斯分布，作为其分量线性组合的能量 $E$ 也服从单变量高斯分布：$E \\sim \\mathcal{N}(\\mu_E, \\sigma_E^2)$。\n\n能量的均值和方差使用随机变量线性变换的性质推导得出：\n- **平均能量**: $\\mu_E = \\mathbb{E}[E] = \\mathbb{E}[\\mathbf{w}^T \\mathbf{f}_*] = \\mathbf{w}^T \\mathbb{E}[\\mathbf{f}_*] = \\mathbf{w}^T \\boldsymbol{\\mu}_*$.\n- **能量方差**: $\\sigma_E^2 = \\text{Var}(E) = \\text{Var}(\\mathbf{w}^T \\mathbf{f}_*) = \\mathbf{w}^T \\text{Var}(\\mathbf{f}_*) \\mathbf{w} = \\mathbf{w}^T \\boldsymbol{\\Sigma}_* \\mathbf{w}$.\n\n代入 $\\boldsymbol{\\mu}_*$ 和 $\\boldsymbol{\\Sigma}_*$ 的表达式，我们可以计算出总能量的均值和方差。\n\n**3. 可信区间的计算**\n\n对于高斯随机变量 $E \\sim \\mathcal{N}(\\mu_E, \\sigma_E^2)$，其水平为 $1-\\alpha$ 的双边中心可信区间由下式给出：\n$$\n[L, U] = [\\mu_E - z_{\\alpha/2} \\sigma_E, \\mu_E + z_{\\alpha/2} \\sigma_E]\n$$\n其中 $\\sigma_E = \\sqrt{\\sigma_E^2}$ 是标准差，$z_{\\alpha/2}$ 是标准正态分布的上 $\\alpha/2$ 临界值。对于指定的 $0.95$ 水平，我们有 $1-\\alpha = 0.95$，因此 $\\alpha = 0.05$。所需的临界值是 $z_{0.025}$，即标准正态分布的 $0.975$ 分位数，$z_{0.025} \\approx 1.95996$。\n\n因此，总能量 $E$ 的 $0.95$ 可信区间的下界 $L$ 和上界 $U$ 为：\n$$\nL = \\mu_E - z_{0.025} \\sigma_E\n$$\n$$\nU = \\mu_E + z_{0.025} \\sigma_E\n$$\n\n每个测试案例的计算流程如下：\n1.  定义训练数据 $X, Y$ 和超参数 $\\sigma_f^2, \\ell, \\sigma_n^2$。\n2.  预先计算矩阵 $K_y = K(X, X) + \\sigma_n^2 I_N$ 和向量 $\\boldsymbol{\\alpha} = K_y^{-1} Y$。为保证数值稳定性，通过求解线性系统 $K_y \\boldsymbol{\\alpha} = Y$ 来找到 $\\boldsymbol{\\alpha}$。\n3.  对于每个工作周期（由查询时间 $X_*$ 和时间步长 $\\Delta t$ 定义）：\n    a. 如果 $X_*$ 是一个空集，能量 $E$ 为 $0$，不确定性也为零，所以 $L=U=0$。\n    b. 否则，构建矩阵 $K(X, X_*)$ 和 $K(X_*, X_*)$。\n    c. 计算后验平均功率向量：$\\boldsymbol{\\mu}_* = K(X_*, X) \\boldsymbol{\\alpha}$。\n    d. 计算后验协方差功率矩阵：$\\boldsymbol{\\Sigma}_* = K(X_*, X_*) - K(X_*, X) K_y^{-1} K(X, X_*)$。这也通过线性求解来计算，以避免直接求逆。\n    e. 计算平均能量 $\\mu_E = \\Delta t \\sum_{j} (\\boldsymbol{\\mu}_*)_j$。\n    f. 计算能量方差 $\\sigma_E^2 = (\\Delta t)^2 \\sum_{i,j} (\\boldsymbol{\\Sigma}_*)_{ij}$。\n    g. 使用标准差 $\\sigma_E = \\sqrt{\\sigma_E^2}$ 和 $z_{0.025}$ 计算边界 $L$ 和 $U$。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Computes the 95% credible interval for total energy consumption based on a\n    Gaussian Process Regression surrogate model of power.\n    \"\"\"\n    \n    # 1. Define GPR model parameters and training data\n    # Training inputs (time in seconds)\n    X_train = np.array([0., 2., 4., 6., 8.]).reshape(-1, 1)\n    \n    # Training outputs (power in Watts)\n    y_train = np.array([120.0, 135.0, 128.0, 140.0, 132.0]).reshape(-1, 1)\n    \n    # Hyperparameters\n    sigma_f_sq = 900.0  # Amplitude variance (Watts^2)\n    ell = 2.5          # Length-scale (seconds)\n    sigma_n_sq = 9.0   # Noise variance (Watts^2)\n    \n    # Test suite of operating cycles\n    test_cases = [\n        # Case 1: Happy path\n        (np.array([1., 2., 3., 4., 5., 6., 7., 8., 9.]), 1.0),\n        # Case 2: Boundary case (empty)\n        (np.array([]), 1.0),\n        # Case 3: Single-sample edge\n        (np.array([5.0]), 2.0),\n        # Case 4: Dense sampling edge\n        (np.arange(0.0, 8.01, 0.5), 0.5)\n    ]\n\n    def squared_exponential_kernel(xa, xb, sigma_f_sq_val, ell_val):\n        \"\"\"\n        Computes the squared-exponential kernel matrix between two sets of points.\n        Vectorized implementation for 1D inputs.\n        \"\"\"\n        # Broadcasting creates a matrix of pairwise differences (xa_i - xb_j)\n        # and then squares them.\n        sqdist = (xa - xb.T)**2\n        return sigma_f_sq_val * np.exp(-0.5 / ell_val**2 * sqdist)\n\n    # 2. Pre-computation for GPR posterior\n    # This part is independent of the query points and can be done once.\n    N = X_train.shape[0]\n    Ky = squared_exponential_kernel(X_train, X_train, sigma_f_sq, ell) + sigma_n_sq * np.eye(N)\n    \n    # For numerical stability, solve the linear system K_y * alpha = y_train\n    # instead of computing the inverse of K_y directly.\n    alpha = np.linalg.solve(Ky, y_train)\n\n    # 3. Process each test case\n    results = []\n    \n    # Get the z-score for the 95% confidence interval once.\n    # z such that P(-z  Z  z) = 0.95 for Z ~ N(0,1)\n    z_score = norm.ppf(0.975)\n\n    for X_query_vals, dt in test_cases:\n        # Handle the boundary case of an empty set of query times\n        if X_query_vals.size == 0:\n            results.append([0.0, 0.0])\n            continue\n\n        X_query = X_query_vals.reshape(-1, 1)\n        \n        # 4. GPR prediction at query points\n        K_star = squared_exponential_kernel(X_train, X_query, sigma_f_sq, ell)\n        K_star_star = squared_exponential_kernel(X_query, X_query, sigma_f_sq, ell)\n        \n        # Posterior mean of power at query points\n        mu_star = K_star.T @ alpha\n        \n        # Posterior covariance of power at query points\n        # Solve K_y * v = K_star to get v = (K_y^-1) * K_star\n        v = np.linalg.solve(Ky, K_star)\n        Sigma_star = K_star_star - K_star.T @ v\n\n        # 5. Uncertainty propagation for total energy\n        # Mean energy (Joules)\n        # E = sum(f(t_i) * dt) -> mu_E = sum(mu_star_i * dt)\n        mu_E = dt * np.sum(mu_star)\n\n        # Variance of energy (Joules^2)\n        # Var(E) = w^T * Sigma_star * w, where w is a vector of dt\n        # This simplifies to dt^2 * sum(all elements of Sigma_star)\n        var_E = dt**2 * np.sum(Sigma_star)\n        \n        # Handle potential small negative variance from numerical precision errors\n        if var_E  0:\n            var_E = 0.0\n        \n        sigma_E = np.sqrt(var_E)\n\n        # 6. Calculate the 95% credible interval\n        lower_bound = mu_E - z_score * sigma_E\n        upper_bound = mu_E + z_score * sigma_E\n        \n        results.append([lower_bound, upper_bound])\n\n    # 7. Format the final output string as specified\n    final_output_str = f\"[{','.join(map(str, results))}]\"\n    print(final_output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "在安全关键的信息物理系统中，任何决策或控制动作都必须在满足特定安全约束的条件下进行。本练习  介绍了一种将高斯过程回归用于安全验证的先进方法。你将学习如何利用 GPR 的后验分布来计算一个“安全证书”，该证书能够以指定的概率（例如 $0.99$）保证系统响应不会超出安全边界，这是在高风险环境中部署基于学习的控制器的核心技术。",
            "id": "4224426",
            "problem": "一个信息物理系统 (CPS) 控制器必须基于系统响应的高斯过程 (GP) 代理模型，高置信度地判断一个候选动作是否满足单边安全约束。您将构建一个高斯过程回归 (GPR) 模型，并为每个决策计算一个安全证书，该证书证明机会约束在概率水平 $0.99$ 上得到满足。任务是实现一个程序，从高斯条件化的第一性原理推导出所需的量。\n\n从以下基本依据出发：\n- 高斯过程 (GP) 的定义：它是一组随机变量的集合，其中任何有限子集都服从联合高斯分布，具有均值函数 $m(\\cdot)$ 和协方差函数 $k(\\cdot,\\cdot)$。\n- 联合高斯随机向量的条件分布也是高斯的，这一事实可以通过应用多元正态分布的标准条件化法则获得。\n- 对于标量高斯随机变量 $Z \\sim \\mathcal{N}(\\mu,\\sigma^2)$ 的经典结果：事件概率 $\\mathbb{P}[Z \\le b]$ 等于在 $(b-\\mu)/\\sigma$ 处求值的标准正态累积分布函数。\n\n考虑一个由零均值高斯过程建模的标量、无量纲的 CPS 输出，其协方差为平方指数协方差（也称为径向基函数协方差）。该协方差由下式给出：\n$$\nk(x,x') = \\sigma_f^2 \\exp\\left(-\\tfrac{1}{2}\\tfrac{(x-x')^2}{\\ell^2}\\right),\n$$\n其超参数为 $\\ell$（长度尺度）、$\\sigma_f$（信号标准差），以及方差为 $\\sigma_n^2$ 的独立加性高斯观测噪声。给定训练输入 $X \\in \\mathbb{R}^n$ 和含噪输出 $y \\in \\mathbb{R}^n$，在测试输入 $x_\\star \\in \\mathbb{R}$ 处的预测分布是高斯的，其均值 $m_\\star$ 和方差 $v_\\star$ 是通过将 $[y; f(x_\\star)]$ 的联合高斯先验以 $y$ 为条件来确定的。\n\n对于形式为 $f(x_\\star) \\le b$ 的单边安全约束，当预测分布为均值为 $m_\\star$、方差为 $v_\\star$ 的高斯分布时，要保证 $\\mathbb{P}[f(x_\\star) \\le b] \\ge p$（其中 $p \\in (0,1)$）的一个充分证书是以下带符号边界为非负：\n$$\nC(x_\\star; b, p) \\triangleq b - \\left(m_\\star + z_p \\sqrt{v_\\star}\\right),\n$$\n其中 $z_p$ 是满足 $\\Phi(z_p) = p$ 的标准正态分布的分位数。值 $C(x_\\star; b, p)$ 即为安全证书：如果 $C(x_\\star; b, p) \\ge 0$，则机会约束成立；如果 $C(x_\\star; b, p)  0$，则不成立。\n\n实现一个程序，该程序：\n- 通过对由平方指数核的 GP 先验和含噪训练观测值所蕴含的联合高斯分布进行条件化，为每个给定的测试输入 $x_\\star$ 构建 GPR 预测均值 $m_\\star$ 和预测方差 $v_\\star$。\n- 在概率水平 $p = 0.99$ 下，为每个测试用例计算安全证书 $C(x_\\star; b, p)$。\n\n使用以下固定的超参数和训练数据：\n- 超参数：$\\ell = 0.7$, $\\sigma_f = 1.0$, $\\sigma_n = 0.05$。\n- 训练输入：$X = [-1.5,\\,-1.0,\\,-0.25,\\,0.0,\\,0.5,\\,0.75,\\,1.25]$。\n- 训练输出：$y_i = \\sin(2 x_i)$，对于每个 $x_i \\in X$。\n\n决策和约束的测试套件：\n- 用例 1：$x_\\star = 0.25$, $b = 0.8$。\n- 用例 2：$x_\\star = 2.0$, $b = 0.4$。\n- 用例 3：$x_\\star = 0.75$, $b = 1.1$。\n\n计算要求和约定：\n- 所有计算都是无量纲的（没有物理单位）。\n- 您必须仅使用上面列出的基本原理和对核矩阵的线性代数运算来计算预测均值 $m_\\star$ 和方差 $v_\\star$。通过使用正则化核矩阵的 Cholesky 分解来确保矩阵求逆的数值稳定性。\n- 使用 $p = 0.99$ 以及与此 $p$ 值一致的精确标准正态分位数 $z_p$。\n- 对于每个用例，以实数形式输出安全证书值 $C(x_\\star; b, p)$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含三个证书值的结果，按用例顺序列出，以逗号分隔，并用方括号括起来。\n- 每个数字必须四舍五入到小数点后恰好六位。\n- 例如，一个有效的输出看起来像 $[0.123456,-0.123456,0.000000]$。",
            "solution": "该问题被评估为有效，因为它在科学上基于高斯过程回归的原理，是良定的（提供了所有必要的数据和约束），并且以客观、正式的语言表达。我们可以开始解决该问题。\n\n问题的核心是为一个候选动作 $x_\\star$ 和一个安全边界 $b$ 在置信水平 $p$ 下计算安全证书 $C(x_\\star; b, p)$。其定义如下：\n$$\nC(x_\\star; b, p) \\triangleq b - \\left(m_\\star + z_p \\sqrt{v_\\star}\\right)\n$$\n其中 $m_\\star$ 和 $v_\\star$ 是系统响应 $f(x_\\star)$ 的预测分布的均值和方差，而 $z_p$ 是对应于概率 $p$ 的标准正态分布的分位数。\n\n我们首先从多元高斯分布条件化的第一性原理出发，建立预测方程。\n\n一个零均值 GP 先验被置于潜函数 $f(x)$ 上。在训练位置 $X = \\{x_1, \\dots, x_n\\}$ 处的观测值 $y$ 与潜函数值 $f_X = [f(x_1), \\dots, f(x_n)]^T$ 通过模型 $y = f_X + \\epsilon$ 相关联，其中 $\\epsilon \\sim \\mathcal{N}(0, \\sigma_n^2 I)$ 是独立高斯噪声。\n\n观测到的训练输出 $y$ 和在新测试点 $x_\\star$ 处的潜函数值 $f_\\star = f(x_\\star)$ 的联合分布是一个多元高斯分布。在零均值先验下，其分布为：\n$$\n\\begin{pmatrix} y \\\\ f_\\star \\end{pmatrix} \\sim \\mathcal{N}\\left(\n\\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix},\n\\begin{pmatrix}\nK(X, X) + \\sigma_n^2 I  K(X, x_\\star) \\\\\nK(x_\\star, X)  K(x_\\star, x_\\star)\n\\end{pmatrix}\n\\right)\n$$\n其中 $K(A, B)$ 是集合 $A$ 和 $B$ 中各点之间的协方差函数求值矩阵。指定的协方差函数是平方指数核：\n$$\nk(x,x') = \\sigma_f^2 \\exp\\left(-\\frac{1}{2}\\frac{(x-x')^2}{\\ell^2}\\right)\n$$\n超参数为 $\\sigma_f = 1.0$, $\\ell = 0.7$，噪声标准差为 $\\sigma_n = 0.05$。\n\n为了符号清晰，我们定义协方差矩阵的各分块如下：\n- $K = K(X, X) \\in \\mathbb{R}^{n \\times n}$\n- $k_\\star = K(X, x_\\star) \\in \\mathbb{R}^{n \\times 1}$\n- $k_{\\star\\star} = K(x_\\star, x_\\star) \\in \\mathbb{R}$\n\n那么联合分布为：\n$$\n\\begin{pmatrix} y \\\\ f_\\star \\end{pmatrix} \\sim \\mathcal{N}\\left(\n\\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix},\n\\begin{pmatrix}\nK + \\sigma_n^2 I  k_\\star \\\\\nk_\\star^T  k_{\\star\\star}\n\\end{pmatrix}\n\\right)\n$$\n\n预测分布 $p(f_\\star | X, y, x_\\star)$ 是在给定观测值 $y$ 的条件下 $f_\\star$ 的条件分布。对于一个通用的分块高斯向量，如果 $\\begin{pmatrix} \\mathbf{x}_1 \\\\ \\mathbf{x}_2 \\end{pmatrix} \\sim \\mathcal{N}\\left(\\begin{pmatrix} \\mu_1 \\\\ \\mu_2 \\end{pmatrix}, \\begin{pmatrix} \\Sigma_{11}  \\Sigma_{12} \\\\ \\Sigma_{21}  \\Sigma_{22} \\end{pmatrix}\\right)$，则在给定 $\\mathbf{x}_1$ 的条件下 $\\mathbf{x}_2$ 的条件分布是 $\\mathcal{N}(\\mu_{2|1}, \\Sigma_{22|1})$，其中：\n- $\\mu_{2|1} = \\mu_2 + \\Sigma_{21} \\Sigma_{11}^{-1} (\\mathbf{x}_1 - \\mu_1)$\n- $\\Sigma_{22|1} = \\Sigma_{22} - \\Sigma_{21} \\Sigma_{11}^{-1} \\Sigma_{12}$\n\n将此规则应用于我们的 GP 情况（其中 $\\mathbf{x}_1=y, \\mathbf{x}_2=f_\\star, \\mu_1=0, \\mu_2=0$），我们得到预测均值 $m_\\star$ 和预测方差 $v_\\star$：\n$$\nm_\\star = k_\\star^T (K + \\sigma_n^2 I)^{-1} y\n$$\n$$\nv_\\star = k_{\\star\\star} - k_\\star^T (K + \\sigma_n^2 I)^{-1} k_\\star\n$$\n\n为了在保证稳定性的前提下对这些量进行数值计算，我们避免直接计算 $K_y \\triangleq K + \\sigma_n^2 I$ 的逆矩阵。相反，我们使用这个正定矩阵的 Cholesky 分解：$K_y = L L^T$，其中 $L$ 是一个下三角矩阵。\n\n计算过程如下：\n1.  构建训练数据：\n    - 输入：$X = [-1.5,\\,-1.0,\\,-0.25,\\,0.0,\\,0.5,\\,0.75,\\,1.25]$\n    - 输出：$y = [\\sin(2x_i)]_{i=1}^n$\n2.  计算核矩阵 $K = K(X, X)$ 和含噪核矩阵 $K_y = K + \\sigma_n^2 I$，其中 $\\sigma_n^2 = 0.05^2 = 0.0025$。\n3.  计算 Cholesky 分解 $K_y = L L^T$。\n4.  为了求得预测均值 $m_\\star$，我们首先求解中间向量 $\\alpha = K_y^{-1} y$。这通过求解两个三角系统来完成：\n    - 使用前向代入法求解 $L v = y$ 得到 $v$。\n    - 使用反向代入法求解 $L^T \\alpha = v$ 得到 $\\alpha$。\n    预测均值则为 $m_\\star = k_\\star^T \\alpha$。\n5.  为了求得预测方差 $v_\\star$，我们计算项 $k_\\star^T K_y^{-1} k_\\star$。我们首先使用前向代入法求解三角系统 $L w = k_\\star$ 得到 $w$。然后，该项变为 $w^T w$，因为 $w^T w = (L^{-1} k_\\star)^T (L^{-1} k_\\star) = k_\\star^T (L^{-1})^T L^{-1} k_\\star = k_\\star^T (L L^T)^{-1} k_\\star = k_\\star^T K_y^{-1} k_\\star$。\n    预测方差则为 $v_\\star = k_{\\star\\star} - w^T w$。注意，对于平方指数核，$k(x,x) = \\sigma_f^2 \\exp(0) = \\sigma_f^2$，所以 $k_{\\star\\star} = \\sigma_f^2 = 1.0$。\n6.  在为给定的测试用例 $(x_\\star, b)$ 计算出 $m_\\star$ 和 $v_\\star$ 后，我们找到 $p = 0.99$ 对应的标准正态分位数 $z_p$。该值为 $z_{0.99} \\approx 2.32634787$。\n7.  最后，我们计算安全证书：$C(x_\\star; b, p) = b - (m_\\star + z_{0.99} \\sqrt{v_\\star})$。\n\n对问题陈述中指定的三个测试用例重复此过程。然后报告所得的三个证书值。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import cholesky, cho_solve, solve_triangular\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Computes Gaussian Process Regression safety certificates from first principles.\n    \"\"\"\n    #\n    # Step 1: Define problem parameters and data\n    #\n    \n    # Hyperparameters for the squared-exponential kernel and noise\n    l_hyper = 0.7\n    sigma_f_hyper = 1.0\n    sigma_n_hyper = 0.05\n\n    # Training data\n    X_train = np.array([-1.5, -1.0, -0.25, 0.0, 0.5, 0.75, 1.25])\n    # The problem specifies generating y from the true function sin(2x)\n    y_train = np.sin(2 * X_train)\n    \n    # Test cases: each is a tuple of (x_star, b)\n    test_cases = [\n        (0.25, 0.8),  # Case 1\n        (2.0, 0.4),   # Case 2\n        (0.75, 1.1),  # Case 3\n    ]\n    \n    # Safety probability level and the corresponding standard normal quantile\n    p_safety = 0.99\n    z_p = norm.ppf(p_safety)\n    \n    #\n    # Step 2: Define the kernel function\n    #\n    def squared_exponential_kernel(x1, x2, l, sigma_f):\n        \"\"\"\n        Computes the squared-exponential covariance between two sets of points.\n        x1: np.ndarray of shape (N1,)\n        x2: np.ndarray of shape (N2,)\n        Returns a covariance matrix of shape (N1, N2).\n        \"\"\"\n        # Ensure inputs are 2D for broadcasting\n        x1 = np.asarray(x1).reshape(-1, 1)\n        x2 = np.asarray(x2).reshape(-1, 1)\n        \n        # Compute squared Euclidean distances\n        # dists is of shape (N1, N2)\n        dists = np.sum(x1**2, 1).reshape(-1, 1) + np.sum(x2**2, 1) - 2 * np.dot(x1, x2.T)\n        \n        return sigma_f**2 * np.exp(-0.5 / l**2 * dists)\n\n    #\n    # Step 3: Pre-compute fixed matrices for GPR\n    #\n    \n    # Compute the covariance matrix of training points\n    K = squared_exponential_kernel(X_train, X_train, l_hyper, sigma_f_hyper)\n    \n    # Add noise variance to the diagonal for the observed data covariance\n    K_yy = K + (sigma_n_hyper**2) * np.eye(len(X_train))\n    \n    # Compute the Cholesky factorization for stable inversion: K_yy = L * L^T\n    # The `lower=True` argument ensures L is lower-triangular.\n    try:\n        L = cholesky(K_yy, lower=True)\n    except np.linalg.LinAlgError:\n        print(\"Error: The covariance matrix is not positive definite.\")\n        return\n\n    #\n    # Step 4: Process each test case\n    #\n    \n    results = []\n    for x_star, b in test_cases:\n        #\n        # Step 4a: Compute kernel vectors for the test point\n        #\n        # Covariance between test point and training points\n        k_star = squared_exponential_kernel([x_star], X_train, l_hyper, sigma_f_hyper).flatten()\n        \n        # Covariance of the test point with itself (prior variance)\n        k_star_star = squared_exponential_kernel([x_star], [x_star], l_hyper, sigma_f_hyper).item()\n\n        #\n        # Step 4b: Compute predictive mean (m_star)\n        #\n        \n        # Solve for alpha = (K + sigma_n^2*I)^-1 * y using the Cholesky factor.\n        # cho_solve takes a tuple (L, True) indicating a pre-computed lower-triangular factor.\n        alpha = cho_solve((L, True), y_train)\n        \n        # Predictive mean: m_star = k_star^T * alpha\n        m_star = k_star.T @ alpha\n        \n        #\n        # Step 4c: Compute predictive variance (v_star)\n        #\n        \n        # Solve the triangular system L*v = k_star for v\n        v = solve_triangular(L, k_star, lower=True)\n        \n        # Predictive variance: v_star = k_star_star - v^T * v\n        v_star = k_star_star - v.T @ v\n        \n        #\n        # Step 4d: Compute the safety certificate\n        #\n        \n        # C = b - (m_star + z_p * sqrt(v_star))\n        certificate = b - (m_star + z_p * np.sqrt(v_star))\n        results.append(certificate)\n\n    #\n    # Step 5: Format and print the final output\n    #\n    \n    # Format results to exactly six decimal places\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n\n```"
        }
    ]
}