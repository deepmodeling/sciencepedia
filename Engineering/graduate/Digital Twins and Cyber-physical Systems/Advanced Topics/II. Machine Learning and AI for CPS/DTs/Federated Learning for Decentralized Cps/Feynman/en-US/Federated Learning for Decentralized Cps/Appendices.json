{
    "hands_on_practices": [
        {
            "introduction": "A core task in managing a Cyber-Physical System is to have an accurate model of its dynamics, which forms the basis of its digital twin. This exercise demonstrates how to combine local data from multiple clients to learn a global Linear Time-Invariant ($LTI$) model without sharing the raw data itself. By deriving a physics-informed, regularized estimator, you will practice a fundamental technique that merges prior physical knowledge with data-driven insights in a privacy-preserving manner .",
            "id": "4222039",
            "problem": "Consider a decentralized Cyber-Physical System (CPS) consisting of $K$ distributed clients, each maintaining a Digital Twin whose local dynamics are well-approximated by a Linear Time-Invariant (LTI) model. For client $k \\in \\{1,\\dots,K\\}$, the local state and input time series satisfy the discrete-time dynamics $x_{t+1}^{(k)} = A x_{t}^{(k)} + B u_{t}^{(k)} + \\epsilon_{t}^{(k)}$, where $x_{t}^{(k)} \\in \\mathbb{R}^{n}$, $u_{t}^{(k)} \\in \\mathbb{R}^{m}$, and $\\epsilon_{t}^{(k)} \\in \\mathbb{R}^{n}$ are model mismatch and process noise. Assume a physics-based prior model $(A_{0}, B_{0})$ obtained from first-principles modeling and linearization, with the concatenated parameter matrix defined as $\\Theta_{0} \\triangleq [A_{0}\\;B_{0}] \\in \\mathbb{R}^{n \\times (n+m)}$, and define the concatenated regressor $z_{t}^{(k)} \\triangleq \\begin{pmatrix} x_{t}^{(k)} \\\\ u_{t}^{(k)} \\end{pmatrix} \\in \\mathbb{R}^{n+m}$. Each client has a local trajectory of length $T_{k}$, with samples indexed by $t \\in \\{0,\\dots,T_{k}-1\\}$.\n\nYou are tasked with designing a physics-informed loss that penalizes violation of the LTI dynamics and formulating a federated estimator of $(A,B)$ that is consistent across clients while preserving data locality. The physics-informed loss must start from the following scientific bases:\n- The definition of an LTI system and its residual $r_{t}^{(k)} \\triangleq x_{t+1}^{(k)} - A x_{t}^{(k)} - B u_{t}^{(k)}$.\n- The well-tested assumption that $\\epsilon_{t}^{(k)}$ is independent and identically distributed with $\\epsilon_{t}^{(k)} \\sim \\mathcal{N}(0,\\sigma^{2} I_{n})$ for a known scalar $\\sigma^{2} > 0$.\n- The need to regularize the estimator toward the physics-based prior $\\Theta_{0}$ using a Tikhonov penalty with regularization strength $\\lambda > 0$.\n\nFormally, derive an additive objective over clients and time that penalizes dynamic residuals and regularizes toward $\\Theta_{0}$, and show that minimizing this objective over $\\Theta \\triangleq [A\\;B] \\in \\mathbb{R}^{n \\times (n+m)}$ admits a closed-form solution in terms of client-local sufficient statistics that can be aggregated without sharing raw data. Express your final estimator entirely in terms of the following aggregated quantities across clients:\n- $S_{ZZ} \\triangleq \\sum_{k=1}^{K} \\sum_{t=0}^{T_{k}-1} z_{t}^{(k)} z_{t}^{(k)\\top} \\in \\mathbb{R}^{(n+m) \\times (n+m)}$,\n- $S_{XZ} \\triangleq \\sum_{k=1}^{K} \\sum_{t=0}^{T_{k}-1} x_{t+1}^{(k)} z_{t}^{(k)\\top} \\in \\mathbb{R}^{n \\times (n+m)}$.\n\nYour derivation must rely only on the bases stated above and must not introduce untested shortcut formulas. The final answer must be a single closed-form analytical expression for the globally federated estimate of $\\Theta$ written as a function of $S_{ZZ}$, $S_{XZ}$, $\\lambda$, $\\sigma^{2}$, and $\\Theta_{0}$. Do not include any numerical values and do not provide an inequality. Do not include any units in your final answer.",
            "solution": "The problem is subjected to validation.\n\n### Step 1: Extract Givens\n- **System**: A decentralized Cyber-Physical System (CPS) with $K$ clients.\n- **Client Dynamics**: Discrete-time Linear Time-Invariant (LTI) model for client $k \\in \\{1, \\dots, K\\}$: $x_{t+1}^{(k)} = A x_{t}^{(k)} + B u_{t}^{(k)} + \\epsilon_{t}^{(k)}$.\n- **State Vector**: $x_{t}^{(k)} \\in \\mathbb{R}^{n}$.\n- **Input Vector**: $u_{t}^{(k)} \\in \\mathbb{R}^{m}$.\n- **Noise Term**: $\\epsilon_{t}^{(k)} \\in \\mathbb{R}^{n}$.\n- **Physics-Based Prior**: $(A_{0}, B_{0})$.\n- **Concatenated Prior Matrix**: $\\Theta_{0} \\triangleq [A_{0}\\;B_{0}] \\in \\mathbb{R}^{n \\times (n+m)}$.\n- **Concatenated Regressor**: $z_{t}^{(k)} \\triangleq \\begin{pmatrix} x_{t}^{(k)} \\\\ u_{t}^{(k)} \\end{pmatrix} \\in \\mathbb{R}^{n+m}$.\n- **Data Trajectory**: For each client $k$, a trajectory of length $T_{k}$ with time index $t \\in \\{0, \\dots, T_{k}-1\\}$.\n- **Residual Definition**: $r_{t}^{(k)} \\triangleq x_{t+1}^{(k)} - A x_{t}^{(k)} - B u_{t}^{(k)}$.\n- **Noise Model**: $\\epsilon_{t}^{(k)}$ are independent and identically distributed (i.i.d.) with a Gaussian distribution $\\mathcal{N}(0,\\sigma^{2} I_{n})$, where $\\sigma^{2} > 0$ is a known scalar.\n- **Regularization**: Tikhonov penalty towards $\\Theta_{0}$ with regularization strength $\\lambda > 0$.\n- **Parameter Matrix to Estimate**: $\\Theta \\triangleq [A\\;B] \\in \\mathbb{R}^{n \\times (n+m)}$.\n- **Aggregated Sufficient Statistics**:\n  - $S_{ZZ} \\triangleq \\sum_{k=1}^{K} \\sum_{t=0}^{T_{k}-1} z_{t}^{(k)} z_{t}^{(k)\\top} \\in \\mathbb{R}^{(n+m) \\times (n+m)}$.\n  - $S_{XZ} \\triangleq \\sum_{k=1}^{K} \\sum_{t=0}^{T_{k}-1} x_{t+1}^{(k)} z_{t}^{(k)\\top} \\in \\mathbb{R}^{n \\times (n+m)}$.\n- **Objective**: Derive a federated estimator for $\\Theta$ in closed form, expressed as a function of $S_{ZZ}$, $S_{XZ}$, $\\lambda$, $\\sigma^{2}$, and $\\Theta_{0}$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, well-posed, and objective.\n1.  **Scientific Grounding**: The problem is rooted in fundamental principles of system identification, statistical estimation, and control theory. The use of an LTI model, Gaussian noise assumption, maximum likelihood estimation, and Tikhonov regularization are standard and rigorously established techniques in engineering and statistics. The federated learning context is a modern, valid application area for these principles.\n2.  **Well-Posedness**: The problem asks for the derivation of a minimizer for an objective function. The objective, combining a sum-of-squares term (from the Gaussian likelihood) and a quadratic regularization penalty, is convex. This guarantees the existence and uniqueness of the solution. The Tikhonov regularization ensures the problem remains well-conditioned even if the data matrix sum $S_{ZZ}$ is not full rank.\n3.  **Objectivity and Completeness**: The problem is stated using precise mathematical language and notation. All necessary components to formulate the objective and derive its minimizer are provided. The definitions are self-contained and consistent.\n\nThe problem does not exhibit any flaws such as scientific unsoundness, missing information, or ambiguity.\n\n### Step 3: Verdict and Action\nThe problem is valid. A reasoned solution will be provided.\n\nThe task is to derive a federated estimator for the parameter matrix $\\Theta = [A\\;B]$ by minimizing a physics-informed loss function. This loss function must be constructed from the scientific bases provided: penalizing the dynamic residual and regularizing towards a prior model $\\Theta_0$. This formulation is equivalent to finding a Maximum A Posteriori (MAP) estimate of $\\Theta$.\n\nThe total objective function, which we seek to minimize with respect to $\\Theta$, is the sum of a data-fidelity term and a regularization term.\n\nThe data-fidelity term arises from the negative log-likelihood of the observed data. Given the i.i.d. Gaussian noise model $\\epsilon_{t}^{(k)} \\sim \\mathcal{N}(0, \\sigma^2 I_n)$, the likelihood of observing the state $x_{t+1}^{(k)}$ given the regressor $z_t^{(k)}$ and parameters $\\Theta$ is:\n$$p(x_{t+1}^{(k)} | z_t^{(k)}, \\Theta) = \\frac{1}{(2\\pi\\sigma^2)^{n/2}} \\exp\\left(-\\frac{1}{2\\sigma^2} \\|x_{t+1}^{(k)} - \\Theta z_t^{(k)}\\|_2^2\\right)$$\nSince all samples are independent, the total log-likelihood is the sum of the individual log-likelihoods. The negative log-likelihood, ignoring constant terms, is proportional to the sum of squared residuals, scaled by the noise variance:\n$$-\\ln \\mathcal{L}(\\Theta) \\propto \\frac{1}{2\\sigma^2} \\sum_{k=1}^{K} \\sum_{t=0}^{T_k-1} \\|x_{t+1}^{(k)} - \\Theta z_t^{(k)}\\|_2^2$$\n\nThe regularization term is specified as a Tikhonov penalty with strength $\\lambda$:\n$$\\mathcal{R}(\\Theta) = \\frac{\\lambda}{2} \\|\\Theta - \\Theta_0\\|_F^2$$\nwhere $\\|\\cdot\\|_F$ denotes the Frobenius norm. The factor of $1/2$ is included for convenience in differentiation.\n\nThe total objective function to minimize is the sum of the negative log-likelihood term and the regularization term, representing the negative log-posterior:\n$$\\mathcal{J}(\\Theta) = \\frac{1}{2\\sigma^2} \\sum_{k=1}^{K} \\sum_{t=0}^{T_k-1} \\|x_{t+1}^{(k)} - \\Theta z_t^{(k)}\\|_2^2 + \\frac{\\lambda}{2} \\|\\Theta - \\Theta_0\\|_F^2$$\nThis objective is additive over clients and time, which is essential for a federated approach. We can rewrite the objective using matrix trace notation, noting that for a vector $v$, $\\|v\\|_2^2 = v^\\top v = \\text{tr}(vv^\\top)$, and for a matrix $M$, $\\|M\\|_F^2 = \\text{tr}(MM^\\top)$.\n\nLet's expand the terms in the objective function:\n$$\\sum_{k,t} \\|x_{t+1}^{(k)} - \\Theta z_t^{(k)}\\|_2^2 = \\sum_{k,t} \\text{tr}\\left((x_{t+1}^{(k)} - \\Theta z_t^{(k)})(x_{t+1}^{(k)} - \\Theta z_t^{(k)})^\\top\\right)$$\n$$= \\sum_{k,t} \\text{tr}\\left(x_{t+1}^{(k)}(x_{t+1}^{(k)})^\\top - x_{t+1}^{(k)}(z_t^{(k)})^\\top\\Theta^\\top - \\Theta z_t^{(k)}(x_{t+1}^{(k)})^\\top + \\Theta z_t^{(k)}(z_t^{(k)})^\\top\\Theta^\\top\\right)$$\nUsing the linearity of the trace and sum operators, this becomes:\n$$= \\text{tr}\\left(\\sum_{k,t} x_{t+1}^{(k)}(x_{t+1}^{(k)})^\\top\\right) - \\text{tr}\\left(\\left(\\sum_{k,t} x_{t+1}^{(k)}(z_t^{(k)})^\\top\\right)\\Theta^\\top\\right) - \\text{tr}\\left(\\Theta \\left(\\sum_{k,t} z_t^{(k)}(x_{t+1}^{(k)})^\\top\\right)\\right) + \\text{tr}\\left(\\Theta \\left(\\sum_{k,t} z_t^{(k)}(z_t^{(k)})^\\top\\right) \\Theta^\\top\\right)$$\nUsing the provided definitions for $S_{XZ}$ and $S_{ZZ}$, and the property $\\text{tr}(A) = \\text{tr}(A^\\top)$:\n$$= C - 2\\text{tr}(S_{XZ} \\Theta^\\top) + \\text{tr}(\\Theta S_{ZZ} \\Theta^\\top)$$\nwhere $C$ is a constant independent of $\\Theta$.\n\nThe regularization term is:\n$$\\|\\Theta - \\Theta_0\\|_F^2 = \\text{tr}((\\Theta-\\Theta_0)(\\Theta-\\Theta_0)^\\top) = \\text{tr}(\\Theta\\Theta^\\top - \\Theta\\Theta_0^\\top - \\Theta_0\\Theta^\\top + \\Theta_0\\Theta_0^\\top)$$\n$$= \\text{tr}(\\Theta\\Theta^\\top) - 2\\text{tr}(\\Theta_0\\Theta^\\top) + \\text{const}$$\n\nSubstituting these back into the objective function $\\mathcal{J}(\\Theta)$:\n$$\\mathcal{J}(\\Theta) = \\frac{1}{2\\sigma^2}\\left(-2\\text{tr}(S_{XZ}\\Theta^\\top) + \\text{tr}(\\Theta S_{ZZ} \\Theta^\\top)\\right) + \\frac{\\lambda}{2}\\left(\\text{tr}(\\Theta\\Theta^\\top) - 2\\text{tr}(\\Theta_0\\Theta^\\top)\\right) + \\text{const}$$\nThis objective is a quadratic function of $\\Theta$ and is convex. The minimum is found by setting its gradient with respect to $\\Theta$ to zero. We use the matrix calculus identities $\\nabla_X \\text{tr}(AX^\\top) = A$ and $\\nabla_X \\text{tr}(X B X^\\top) = X(B+B^\\top)$.\n$$\\nabla_\\Theta \\mathcal{J}(\\Theta) = \\frac{1}{2\\sigma^2}\\left(-2S_{XZ} + \\Theta(S_{ZZ} + S_{ZZ}^\\top)\\right) + \\frac{\\lambda}{2}\\left(2\\Theta - 2\\Theta_0\\right)$$\nSince $S_{ZZ}$ is symmetric by definition ($S_{ZZ} = S_{ZZ}^\\top$), this simplifies to:\n$$\\nabla_\\Theta \\mathcal{J}(\\Theta) = \\frac{1}{\\sigma^2}(-S_{XZ} + \\Theta S_{ZZ}) + \\lambda(\\Theta - \\Theta_0)$$\nSetting the gradient to zero to find the optimal $\\hat{\\Theta}$:\n$$\\frac{1}{\\sigma^2}(-S_{XZ} + \\hat{\\Theta} S_{ZZ}) + \\lambda(\\hat{\\Theta} - \\Theta_0) = 0$$\nWe now solve for $\\hat{\\Theta}$. Multiply by $\\sigma^2$:\n$$-S_{XZ} + \\hat{\\Theta} S_{ZZ} + \\lambda\\sigma^2(\\hat{\\Theta} - \\Theta_0) = 0$$\n$$\\hat{\\Theta} S_{ZZ} + \\lambda\\sigma^2 \\hat{\\Theta} = S_{XZ} + \\lambda\\sigma^2 \\Theta_0$$\nFactor out $\\hat{\\Theta}$:\n$$\\hat{\\Theta}(S_{ZZ} + \\lambda\\sigma^2 I) = S_{XZ} + \\lambda\\sigma^2 \\Theta_0$$\nwhere $I$ is the identity matrix of size $(n+m) \\times (n+m)$.\nThe closed-form solution for the federated estimate $\\hat{\\Theta}$ is obtained by right-multiplying by the inverse of $(S_{ZZ} + \\lambda\\sigma^2 I)$. This matrix is invertible for $\\lambda > 0$ and $\\sigma^2 > 0$ because $S_{ZZ}$ is positive semi-definite and $\\lambda\\sigma^2 I$ is positive definite, making their sum positive definite and thus invertible.\n$$\\hat{\\Theta} = (S_{XZ} + \\lambda\\sigma^2 \\Theta_0)(S_{ZZ} + \\lambda\\sigma^2 I)^{-1}$$\nThis solution is compliant with the federated setting. Each client $k$ computes its local statistics $\\sum_{t} z_{t}^{(k)} z_{t}^{(k)\\top}$ and $\\sum_{t} x_{t+1}^{(k)} z_{t}^{(k)\\top}$, which are then aggregated at a central server to form $S_{ZZ}$ and $S_{XZ}$ without sharing the raw data $\\{x_t^{(k)}, u_t^{(k)}\\}$. The server then computes the final estimate using the derived formula.",
            "answer": "$$\n\\boxed{(S_{XZ} + \\lambda\\sigma^{2}\\Theta_{0})(S_{ZZ} + \\lambda\\sigma^{2}I)^{-1}}\n$$"
        },
        {
            "introduction": "In real-world decentralized networks, communication and computation delays are inevitable, leading to \"stale\" updates arriving at the central aggregator. This practice explores how to analytically bound the error caused by such asynchrony and how to design a staleness-aware weighting scheme to mitigate its impact. By working through this problem, you will gain insight into the theoretical analysis of asynchronous optimization and develop a practical strategy for improving the robustness of federated learning systems in the face of network latency .",
            "id": "4222082",
            "problem": "Consider a decentralized Cyber-Physical Systems (CPS) network in which a global Digital Twin (DT) model is trained via asynchronous Federated Learning (FL). At global iteration $t$, the server would ideally apply the synchronous update $-\\eta \\nabla f(x_t)$, where $f$ is the global objective, $x_t$ is the current global model state, and $\\eta>0$ is the step size. In practice, client updates arrive with staleness (lag) due to communication and computation delays. Let the random lag $K \\in \\{0,1,2,\\dots\\}$ have a geometric distribution with parameter $p \\in (0,1]$, that is $P(K=k)=\\pi_k$ with $\\pi_k = p(1-p)^k$.\n\nAssume the following fundamental properties:\n- The global objective $f$ has $L$-Lipschitz continuous gradients, i.e., $\\|\\nabla f(x)-\\nabla f(y)\\| \\leq L \\|x-y\\|$ for all $x,y$ and some constant $L>0$.\n- Each client’s stochastic gradient is uniformly bounded in norm by $G>0$, i.e., $\\|\\nabla f(x)\\| \\leq G$ for all relevant iterates.\n- Each global update step has magnitude at most $\\eta G$, i.e., $\\|x_{t+1}-x_t\\| \\leq \\eta G$.\n\nDefine the asynchronous aggregate update with normalized staleness weights $\\alpha_k$ as $-\\eta \\sum_{k=0}^{\\infty} \\alpha_k \\nabla f(x_{t-k})$, where the weights satisfy $\\alpha_k \\geq 0$ and $\\sum_{k=0}^{\\infty} \\alpha_k = 1$. The lag-induced error in the aggregate model update at iteration $t$ is the deviation\n$$\n\\Delta \\triangleq \\left\\|-\\eta \\sum_{k=0}^{\\infty} \\alpha_k \\nabla f(x_{t-k}) + \\eta \\nabla f(x_t)\\right\\|.\n$$\n- Case A (unweighted): $\\alpha_k = \\pi_k$.\n- Case B (staleness weighting): $\\alpha_k = \\frac{\\pi_k w(k)}{\\sum_{j=0}^{\\infty} \\pi_j w(j)}$ for a scheme $w(k) = a^k$ with parameter $a \\in (0,1)$.\n\nStarting from the above fundamental properties and definitions only, derive a worst-case upper bound on $\\Delta$ for Case A and Case B. Then, choose the exponential staleness weighting scheme $w(k)=a^k$ and determine the value of $a$ that makes the worst-case bound in Case B exactly half of that in Case A. Your final answer must be the single closed-form expression for $a$ (no units). Do not provide intermediate steps or any additional quantities in your final answer.",
            "solution": "The problem requires the derivation of a worst-case upper bound on the lag-induced error, $\\Delta$, for two different asynchronous update schemes, and then to find a parameter value for one scheme that halves the bound relative to the other.\n\nThe lag-induced error is defined as:\n$$\n\\Delta \\triangleq \\left\\|-\\eta \\sum_{k=0}^{\\infty} \\alpha_k \\nabla f(x_{t-k}) + \\eta \\nabla f(x_t)\\right\\|\n$$\nwhere $\\eta > 0$ is the step size, $f$ is the global objective, $x_t$ is the model state at global iteration $t$, and $\\alpha_k$ are normalized staleness weights satisfying $\\alpha_k \\geq 0$ and $\\sum_{k=0}^{\\infty} \\alpha_k = 1$.\n\nWe begin by simplifying the expression for $\\Delta$. We can factor out $\\eta$ and use the property that the weights sum to $1$:\n$$\n\\Delta = \\eta \\left\\| \\nabla f(x_t) - \\sum_{k=0}^{\\infty} \\alpha_k \\nabla f(x_{t-k}) \\right\\| = \\eta \\left\\| \\left(\\sum_{k=0}^{\\infty} \\alpha_k\\right) \\nabla f(x_t) - \\sum_{k=0}^{\\infty} \\alpha_k \\nabla f(x_{t-k}) \\right\\|\n$$\n$$\n\\Delta = \\eta \\left\\| \\sum_{k=0}^{\\infty} \\alpha_k (\\nabla f(x_t) - \\nabla f(x_{t-k})) \\right\\|\n$$\nApplying the triangle inequality for norms, we obtain an upper bound:\n$$\n\\Delta \\leq \\eta \\sum_{k=0}^{\\infty} \\alpha_k \\|\\nabla f(x_t) - \\nabla f(x_{t-k})\\|\n$$\nThe problem states that the global objective $f$ has $L$-Lipschitz continuous gradients, i.e., $\\|\\nabla f(x) - \\nabla f(y)\\| \\leq L \\|x - y\\|$. Applying this property, we get:\n$$\n\\Delta \\leq \\eta \\sum_{k=0}^{\\infty} \\alpha_k (L \\|x_t - x_{t-k}\\|)\n$$\nNext, we need to bound the term $\\|x_t - x_{t-k}\\|$. For $k>0$, we can express the difference as a telescoping sum:\n$$\nx_t - x_{t-k} = \\sum_{j=1}^{k} (x_{t-j+1} - x_{t-j})\n$$\nApplying the triangle inequality again:\n$$\n\\|x_t - x_{t-k}\\| \\leq \\sum_{j=1}^{k} \\|x_{t-j+1} - x_{t-j}\\|\n$$\nThe problem provides a worst-case bound on the magnitude of each global update step: $\\|x_{i+1} - x_i\\| \\leq \\eta G$ for any iteration $i$. Substituting this into the inequality:\n$$\n\\|x_t - x_{t-k}\\| \\leq \\sum_{j=1}^{k} \\eta G = k \\eta G\n$$\nFor $k=0$, the distance is $\\|x_t - x_t\\|=0$, which is consistent with this bound. Substituting this worst-case distance bound back into our expression for the bound on $\\Delta$:\n$$\n\\Delta \\leq \\eta \\sum_{k=0}^{\\infty} \\alpha_k (L (k \\eta G)) = \\eta^2 L G \\sum_{k=0}^{\\infty} k \\alpha_k\n$$\nThis gives us a general worst-case upper bound on $\\Delta$. The specific bound depends on the choice of weights $\\alpha_k$. The term $\\sum_{k=0}^{\\infty} k \\alpha_k$ represents the effective average staleness.\n\nCase A (unweighted): The weights are given by the geometric distribution of the lag, $\\alpha_k = \\pi_k = p(1-p)^k$.\nThe effective average staleness is the expectation of this geometric distribution.\n$$\n\\sum_{k=0}^{\\infty} k \\alpha_k = \\sum_{k=0}^{\\infty} k p(1-p)^k = E[K]\n$$\nFor a geometric distribution with success probability $p$ and support $\\{0, 1, 2, \\dots\\}$, the expected value is $E[K] = \\frac{1-p}{p}$.\nTherefore, the worst-case upper bound for Case A, denoted $B_A$, is:\n$$\nB_A = \\eta^2 L G \\left(\\frac{1-p}{p}\\right)\n$$\n\nCase B (staleness weighting): The weights are given by $\\alpha_k = \\frac{\\pi_k w(k)}{\\sum_{j=0}^{\\infty} \\pi_j w(j)}$ with $w(k) = a^k$ for $a \\in (0,1)$.\nFirst, we compute the normalization constant in the denominator:\n$$\n\\sum_{j=0}^{\\infty} \\pi_j w(j) = \\sum_{j=0}^{\\infty} p(1-p)^j a^j = p \\sum_{j=0}^{\\infty} (a(1-p))^j\n$$\nThis is a geometric series with ratio $r = a(1-p)$. Since $a \\in (0,1)$ and $p \\in (0,1]$, we have $0 \\leq 1-p < 1$, so $0 \\leq r < 1$. The series converges to $\\frac{1}{1-r}$.\n$$\n\\sum_{j=0}^{\\infty} \\pi_j w(j) = p \\left(\\frac{1}{1 - a(1-p)}\\right)\n$$\nNow we can write the expression for $\\alpha_k$:\n$$\n\\alpha_k = \\frac{p(1-p)^k a^k}{p / (1 - a(1-p))} = (1 - a(1-p))(a(1-p))^k\n$$\nThis expression for $\\alpha_k$ is the probability mass function of a new geometric distribution with success probability $p' = 1 - a(1-p)$ and support $\\{0, 1, 2, \\dots\\}$.\nThe effective average staleness for Case B is the expectation of this new distribution:\n$$\n\\sum_{k=0}^{\\infty} k \\alpha_k = \\frac{1-p'}{p'} = \\frac{1 - (1 - a(1-p))}{1 - a(1-p)} = \\frac{a(1-p)}{1 - a(1-p)}\n$$\nTherefore, the worst-case upper bound for Case B, denoted $B_B$, is:\n$$\nB_B = \\eta^2 L G \\left(\\frac{a(1-p)}{1 - a(1-p)}\\right)\n$$\n\nFinally, we need to find the value of $a$ such that $B_B = \\frac{1}{2} B_A$.\nWe set up the equation. For $p \\in (0,1)$, both bounds are non-zero.\n$$\n\\eta^2 L G \\left(\\frac{a(1-p)}{1 - a(1-p)}\\right) = \\frac{1}{2} \\left( \\eta^2 L G \\left(\\frac{1-p}{p}\\right) \\right)\n$$\nThe terms $\\eta^2 L G$ and $(1-p)$ (since $p<1$) are non-zero and can be cancelled from both sides. If $p=1$, both bounds are $0$ and the equality holds for any $a$, but the problem is most meaningful for $p<1$.\n$$\n\\frac{a}{1 - a(1-p)} = \\frac{1}{2p}\n$$\nNow we solve for $a$:\n$$\n2ap = 1 - a(1-p)\n$$\n$$\n2ap = 1 - a + ap\n$$\n$$\n2ap + a - ap = 1\n$$\n$$\nap + a = 1\n$$\n$$\na(p+1) = 1\n$$\n$$\na = \\frac{1}{1+p}\n$$\nThis is the required closed-form expression for $a$. Since $p \\in (0,1]$, $1+p \\in (1,2]$, which means $a \\in [\\frac{1}{2}, 1)$, consistent with the given constraint $a \\in (0,1)$.",
            "answer": "$$\n\\boxed{\\frac{1}{1+p}}\n$$"
        },
        {
            "introduction": "A global model optimized for overall performance might inadvertently disadvantage certain subgroups of clients, a critical issue in deploying fair and reliable Cyber-Physical Systems. This computational exercise provides a concrete framework using quadratic loss approximations to quantify the trade-off between overall model accuracy (risk) and fairness disparity across different client groups. By implementing and evaluating different aggregation strategies, you will gain hands-on experience in navigating the complex relationship between system-wide performance and equitable outcomes, a key skill for responsible AI and CPS design .",
            "id": "4222080",
            "problem": "Consider a decentralized Cyber-Physical Systems (CPS) environment modeled with Digital Twins, where Federated Learning (FL) aggregates client-side models without centralizing raw data. Let there be $K$ clients, indexed by $i \\in \\{1,2,\\dots,K\\}$, each with a local empirical risk function defined on a shared parameter vector $w \\in \\mathbb{R}^d$. Assume Empirical Risk Minimization (ERM) with twice-differentiable convex losses, and model each client’s empirical risk near its local minimizer $w_i^\\star$ by a positive-definite quadratic approximation:\n$$\nL_i(w) = (w - w_i^\\star)^\\top H_i (w - w_i^\\star) + c_i,\n$$\nwhere $H_i \\in \\mathbb{R}^{d \\times d}$ is symmetric positive definite and $c_i \\in \\mathbb{R}$ is a constant offset. Let the federated aggregator form a global parameter by a convex combination of local minimizers,\n$$\nw(\\alpha) = \\sum_{i=1}^K \\alpha_i w_i^\\star,\n$$\nwhere $\\alpha_i \\ge 0$ and $\\sum_{i=1}^K \\alpha_i = 1$. Let $p_i \\ge 0$ with $\\sum_{i=1}^K p_i = 1$ denote the normalized client data-size weights that define the overall risk as\n$$\nR(w) = \\sum_{i=1}^K p_i L_i(w).\n$$\nSuppose the $K$ clients are partitioned into two disjoint groups $G_1$ and $G_2$ that cover all clients. Define the group risks by averaging client risks within each group,\n$$\nR_{G_1}(w) = \\frac{1}{|G_1|} \\sum_{i \\in G_1} L_i(w), \\quad R_{G_2}(w) = \\frac{1}{|G_2|} \\sum_{i \\in G_2} L_i(w),\n$$\nand define the fairness disparity\n$$\nD(w) = \\left| R_{G_1}(w) - R_{G_2}(w) \\right|.\n$$\nThe fairness–accuracy trade-off is assessed by evaluating how $D(w(\\alpha))$ and $R(w(\\alpha))$ co-vary under different aggregation weights $\\alpha$.\n\nStarting from the fundamental base of ERM and convex quadratic loss approximations, write a complete program that, given the following test suite, computes for each specified aggregation weight vector the triple consisting of overall risk $R(w(\\alpha))$, fairness disparity $D(w(\\alpha))$, and the marginal fairness–accuracy slope relative to a specified baseline $\\alpha^{\\mathrm{base}}$,\n$$\nS(\\alpha) = \n\\begin{cases}\n\\frac{D(w(\\alpha)) - D(w(\\alpha^{\\mathrm{base}}))}{R(w(\\alpha)) - R(w(\\alpha^{\\mathrm{base}}))}, & \\text{if } R(w(\\alpha)) \\ne R(w(\\alpha^{\\mathrm{base}})), \\\\\n0, & \\text{otherwise}.\n\\end{cases}\n$$\n\nUse the following parameters, which are scientifically plausible and self-consistent for a CPS federated optimization scenario.\n\n- Dimension $d = 2$ and number of clients $K = 4$.\n- Client quadratic approximations:\n  - $H_1 = \\begin{bmatrix} 2.0 & 0.3 \\\\ 0.3 & 1.0 \\end{bmatrix}$, $w_1^\\star = \\begin{bmatrix} 1.0 \\\\ -0.5 \\end{bmatrix}$, $c_1 = 0$.\n  - $H_2 = \\begin{bmatrix} 1.5 & 0.1 \\\\ 0.1 & 1.2 \\end{bmatrix}$, $w_2^\\star = \\begin{bmatrix} -0.8 \\\\ 0.3 \\end{bmatrix}$, $c_2 = 0$.\n  - $H_3 = \\begin{bmatrix} 2.2 & -0.2 \\\\ -0.2 & 0.8 \\end{bmatrix}$, $w_3^\\star = \\begin{bmatrix} 0.5 \\\\ 1.2 \\end{bmatrix}$, $c_3 = 0$.\n  - $H_4 = \\begin{bmatrix} 0.9 & 0.0 \\\\ 0.0 & 1.8 \\end{bmatrix}$, $w_4^\\star = \\begin{bmatrix} -1.3 \\\\ -0.7 \\end{bmatrix}$, $c_4 = 0$.\n- Data-size weights $p = [0.4, 0.1, 0.3, 0.2]$.\n- Group partition $G_1 = \\{1, 2\\}$ and $G_2 = \\{3, 4\\}$.\n- Baseline aggregation weights $\\alpha^{\\mathrm{base}} = [0.25, 0.25, 0.25, 0.25]$.\n- Test suite of aggregation weights to evaluate:\n  - Case $1$: $\\alpha = [0.25, 0.25, 0.25, 0.25]$ (uniform baseline).\n  - Case $2$: $\\alpha = [0.45, 0.45, 0.05, 0.05]$ (emphasize group $G_1$).\n  - Case $3$: $\\alpha = [0.05, 0.05, 0.45, 0.45]$ (emphasize group $G_2$).\n  - Case $4$: $\\alpha = [0.4, 0.1, 0.3, 0.2]$ (data-size weighted).\n  - Case $5$: $\\alpha = [0.97, 0.01, 0.01, 0.01]$ (extremely skewed to client $1$).\n\nYour program must:\n- Compute $w(\\alpha)$ for each case.\n- Compute $L_i(w(\\alpha))$ for all clients $i$.\n- Compute $R(w(\\alpha))$, $R_{G_1}(w(\\alpha))$, $R_{G_2}(w(\\alpha))$, $D(w(\\alpha))$, and $S(\\alpha)$ as defined.\n- Produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is a list of three floats $[R, D, S]$ for the corresponding test case, in the given order. For example: $[[r_1,d_1,s_1],[r_2,d_2,s_2],\\dots]$.\n\nNo external input is required, and no physical units, angles, or percentages are involved. All numerical outputs must be real-valued floats.",
            "solution": "The problem statement is subjected to a rigorous validation process before a solution is attempted.\n\n### Step 1: Extract Givens\n\nThe following data and definitions are explicitly provided in the problem statement:\n- **Model Context**: A decentralized Cyber-Physical Systems (CPS) environment with $K$ clients using Federated Learning (FL). Client models are aggregated without centralizing raw data. The analysis is based on Empirical Risk Minimization (ERM).\n- **Client Risk Function**: Each client $i \\in \\{1, 2, \\dots, K\\}$ has a local empirical risk function $L_i(w)$ approximated by a positive-definite quadratic form near its local minimizer $w_i^\\star \\in \\mathbb{R}^d$:\n$$L_i(w) = (w - w_i^\\star)^\\top H_i (w - w_i^\\star) + c_i$$\nwhere $H_i \\in \\mathbb{R}^{d \\times d}$ is a symmetric positive definite matrix and $c_i \\in \\mathbb{R}$ is a constant.\n- **Aggregated Parameter Vector**: The global parameter vector $w(\\alpha)$ is a convex combination of local minimizers:\n$$w(\\alpha) = \\sum_{i=1}^K \\alpha_i w_i^\\star, \\quad \\text{with } \\alpha_i \\ge 0 \\text{ and } \\sum_{i=1}^K \\alpha_i = 1$$\n- **Overall Risk Function**: The overall risk $R(w)$ is a weighted average of client risks:\n$$R(w) = \\sum_{i=1}^K p_i L_i(w), \\quad \\text{with } p_i \\ge 0 \\text{ and } \\sum_{i=1}^K p_i = 1$$\n- **Group Risk and Fairness**: Clients are partitioned into two disjoint groups $G_1$ and $G_2$.\n  - Group risks: $R_{G_1}(w) = \\frac{1}{|G_1|} \\sum_{i \\in G_1} L_i(w)$ and $R_{G_2}(w) = \\frac{1}{|G_2|} \\sum_{i \\in G_2} L_i(w)$.\n  - Fairness disparity: $D(w) = |R_{G_1}(w) - R_{G_2}(w)|$.\n- **Fairness-Accuracy Slope**: The marginal trade-off slope relative to a baseline $\\alpha^{\\mathrm{base}}$ is:\n$$S(\\alpha) = \\begin{cases} \\frac{D(w(\\alpha)) - D(w(\\alpha^{\\mathrm{base}}))}{R(w(\\alpha)) - R(w(\\alpha^{\\mathrm{base}}))}, & \\text{if } R(w(\\alpha)) \\ne R(w(\\alpha^{\\mathrm{base}})) \\\\ 0, & \\text{otherwise} \\end{cases}$$\n- **Specific Parameters**:\n  - Dimension $d = 2$, Number of clients $K = 4$.\n  - Client $1$: $H_1 = \\begin{bmatrix} 2.0 & 0.3 \\\\ 0.3 & 1.0 \\end{bmatrix}$, $w_1^\\star = \\begin{bmatrix} 1.0 \\\\ -0.5 \\end{bmatrix}$, $c_1 = 0$.\n  - Client $2$: $H_2 = \\begin{bmatrix} 1.5 & 0.1 \\\\ 0.1 & 1.2 \\end{bmatrix}$, $w_2^\\star = \\begin{bmatrix} -0.8 \\\\ 0.3 \\end{bmatrix}$, $c_2 = 0$.\n  - Client $3$: $H_3 = \\begin{bmatrix} 2.2 & -0.2 \\\\ -0.2 & 0.8 \\end{bmatrix}$, $w_3^\\star = \\begin{bmatrix} 0.5 \\\\ 1.2 \\end{bmatrix}$, $c_3 = 0$.\n  - Client $4$: $H_4 = \\begin{bmatrix} 0.9 & 0.0 \\\\ 0.0 & 1.8 \\end{bmatrix}$, $w_4^\\star = \\begin{bmatrix} -1.3 \\\\ -0.7 \\end{bmatrix}$, $c_4 = 0$.\n  - Data-size weights: $p = [0.4, 0.1, 0.3, 0.2]$.\n  - Group partition: $G_1 = \\{1, 2\\}$, $G_2 = \\{3, 4\\}$.\n  - Baseline aggregation weights: $\\alpha^{\\mathrm{base}} = [0.25, 0.25, 0.25, 0.25]$.\n- **Task**: For a test suite of $\\alpha$ vectors, compute the triple $(R(w(\\alpha)), D(w(\\alpha)), S(\\alpha))$.\n- **Test Suite**:\n  - Case $1$: $\\alpha = [0.25, 0.25, 0.25, 0.25]$\n  - Case $2$: $\\alpha = [0.45, 0.45, 0.05, 0.05]$\n  - Case $3$: $\\alpha = [0.05, 0.05, 0.45, 0.45]$\n  - Case $4$: $\\alpha = [0.4, 0.1, 0.3, 0.2]$\n  - Case $5$: $\\alpha = [0.97, 0.01, 0.01, 0.01]$\n\n### Step 2: Validate Using Extracted Givens\n\nThe problem is evaluated against the established criteria:\n- **Scientifically Grounded**: The problem is well-grounded in the principles of optimization and machine learning. The use of a quadratic approximation for the loss function is a standard technique derived from the second-order Taylor expansion around a minimizer. The concepts of federated averaging, empirical risk, and fairness-accuracy trade-offs are central to current research in distributed machine learning. The framework is a valid and common simplification used to analyze the behavior of federated learning algorithms.\n- **Well-Posed and Consistent**: The problem is mathematically well-posed. All quantities are defined by explicit formulas. Given the input parameters, the output is uniquely determined through a deterministic computational procedure. The parameters are self-consistent:\n  - The sum of data-size weights is $\\sum_{i=1}^4 p_i = 0.4 + 0.1 + 0.3 + 0.2 = 1.0$.\n  - For each test case, the aggregation weights sum to one: e.g., for Case $2$, $\\sum_{i=1}^4 \\alpha_i = 0.45 + 0.45 + 0.05 + 0.05 = 1.0$. All $\\alpha_i$ are non-negative.\n  - The Hessian matrices $H_i$ are specified as symmetric positive definite. This is verified by checking their principal minors. For a $2 \\times 2$ matrix, this requires the top-left element and the determinant to be positive.\n    - $H_1$: $2.0 > 0$, $\\det(H_1) = (2.0)(1.0) - (0.3)^2 = 1.91 > 0$.\n    - $H_2$: $1.5 > 0$, $\\det(H_2) = (1.5)(1.2) - (0.1)^2 = 1.79 > 0$.\n    - $H_3$: $2.2 > 0$, $\\det(H_3) = (2.2)(0.8) - (-0.2)^2 = 1.72 > 0$.\n    - $H_4$: $0.9 > 0$, $\\det(H_4) = (0.9)(1.8) - (0.0)^2 = 1.62 > 0$.\n  All matrices are indeed positive definite.\n  - The definition of $S(\\alpha)$ explicitly handles the case where the denominator is zero, preventing ill-definedness.\n- **Objective and Formalizable**: The problem is stated using precise mathematical language, free from subjectivity or ambiguity. It is directly formalizable into a computational algorithm. The context of CPS and digital twins serves as a scientifically relevant motivation for the abstract mathematical problem.\n\nThe problem does not exhibit any of the defined flaws (e.g., scientific unsoundness, incompleteness, contradiction, or being ill-posed).\n\n### Step 3: Verdict and Action\n\nThe problem is **valid**. A solution will be derived and implemented.\n\n### Derivation of the Solution\n\nThe objective is to compute the triplet $(R(w(\\alpha)), D(w(\\alpha)), S(\\alpha))$ for each specified aggregation weight vector $\\alpha$ in the test suite. This requires a systematic application of the definitions provided. The procedure is as follows:\n\n**Step 1: Baseline Calculation**\nFirst, we compute the baseline overall risk $R^{\\mathrm{base}}$ and fairness disparity $D^{\\mathrm{base}}$ using the baseline aggregation weights $\\alpha^{\\mathrm{base}} = [0.25, 0.25, 0.25, 0.25]$. These values are necessary for calculating the slope $S(\\alpha)$ for all test cases.\n\n**Step 2: Iterating Through Test Cases**\nFor each test case defined by a vector $\\alpha$, we perform the following calculations:\n\n**2.1. Compute the Aggregated Parameter Vector $w(\\alpha)$**\nThe global parameter vector is the convex combination of the local minimizers:\n$$w(\\alpha) = \\sum_{i=1}^{4} \\alpha_i w_i^\\star$$\nThis is a vector sum where each local minimizer vector $w_i^\\star$ is scaled by its corresponding weight $\\alpha_i$.\n\n**2.2. Compute Client-Specific Losses $L_i(w(\\alpha))$**\nFor each client $i \\in \\{1, 2, 3, 4\\}$, the local loss at the aggregated parameter $w(\\alpha)$ is calculated. Given that $c_i = 0$ for all clients:\n$$L_i(w(\\alpha)) = (w(\\alpha) - w_i^\\star)^\\top H_i (w(\\alpha) - w_i^\\star)$$\nLet $\\Delta w_i(\\alpha) = w(\\alpha) - w_i^\\star$. The loss is computed as the quadratic form $L_i(w(\\alpha)) = (\\Delta w_i(\\alpha))^\\top H_i (\\Delta w_i(\\alpha))$.\n\n**2.3. Compute Overall Risk $R(w(\\alpha))$**\nThe overall risk is the weighted average of the client-specific losses, using the data-size weights $p_i$:\n$$R(w(\\alpha)) = \\sum_{i=1}^{4} p_i L_i(w(\\alpha))$$\nwhere $p = [0.4, 0.1, 0.3, 0.2]$.\n\n**2.4. Compute Group Risks and Fairness Disparity $D(w(\\alpha))$**\nThe clients are partitioned into $G_1 = \\{1, 2\\}$ and $G_2 = \\{3, 4\\}$. The group risks are averages of the losses within each group. Since $|G_1| = 2$ and $|G_2| = 2$:\n$$R_{G_1}(w(\\alpha)) = \\frac{1}{2} (L_1(w(\\alpha)) + L_2(w(\\alpha)))$$\n$$R_{G_2}(w(\\alpha)) = \\frac{1}{2} (L_3(w(\\alpha)) + L_4(w(\\alpha)))$$\nThe fairness disparity is the absolute difference between these two group risks:\n$$D(w(\\alpha)) = |R_{G_1}(w(\\alpha)) - R_{G_2}(w(\\alpha))|$$\n\n**2.5. Compute the Marginal Fairness-Accuracy Slope $S(\\alpha)$**\nUsing the pre-computed baseline values $R^{\\mathrm{base}} = R(w(\\alpha^{\\mathrm{base}}))$ and $D^{\\mathrm{base}} = D(w(\\alpha^{\\mathrm{base}}))$, the slope is calculated as:\n$$S(\\alpha) = \\frac{D(w(\\alpha)) - D^{\\mathrm{base}}}{R(w(\\alpha)) - R^{\\mathrm{base}}}$$\nIf the denominator $R(w(\\alpha)) - R^{\\mathrm{base}}$ is zero (which occurs for Case $1$ where $\\alpha = \\alpha^{\\mathrm{base}}$), the slope is defined to be $S(\\alpha) = 0$.\n\nThis multi-step procedure is applied to each of the $5$ test cases provided. The final output is an aggregation of the resulting $(R, D, S)$ triplets.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the federated learning fairness-accuracy trade-off problem.\n    \"\"\"\n    \n    # Define the problem parameters as specified.\n    # Dimension d=2, number of clients K=4.\n    \n    # Client Hessian matrices H_i\n    H_mats = [\n        np.array([[2.0, 0.3], [0.3, 1.0]]),\n        np.array([[1.5, 0.1], [0.1, 1.2]]),\n        np.array([[2.2, -0.2], [-0.2, 0.8]]),\n        np.array([[0.9, 0.0], [0.0, 1.8]])\n    ]\n\n    # Client local minimizers w_i^*\n    w_stars = [\n        np.array([1.0, -0.5]),\n        np.array([-0.8, 0.3]),\n        np.array([0.5, 1.2]),\n        np.array([-1.3, -0.7])\n    ]\n\n    # Constant offsets c_i (all zero)\n    c_vals = np.array([0.0, 0.0, 0.0, 0.0])\n\n    # Data-size weights p_i\n    p_vec = np.array([0.4, 0.1, 0.3, 0.2])\n\n    # Group partition G1={1, 2}, G2={3, 4}\n    # Indices are 0-based in the code, so G1={0, 1}, G2={2, 3}\n    G1_indices = [0, 1]\n    G2_indices = [2, 3]\n\n    # Baseline aggregation weights alpha_base\n    alpha_base = np.array([0.25, 0.25, 0.25, 0.25])\n\n    # Test suite of aggregation weights alpha\n    test_cases = [\n        np.array([0.25, 0.25, 0.25, 0.25]),  # Case 1 (uniform baseline)\n        np.array([0.45, 0.45, 0.05, 0.05]),  # Case 2 (emphasize G1)\n        np.array([0.05, 0.05, 0.45, 0.45]),  # Case 3 (emphasize G2)\n        np.array([0.4, 0.1, 0.3, 0.2]),       # Case 4 (data-size weighted)\n        np.array([0.97, 0.01, 0.01, 0.01])   # Case 5 (skewed to client 1)\n    ]\n\n    def compute_all_metrics(alpha_vec, H, w_s, c, p):\n        \"\"\"\n        Computes all required metrics for a given alpha vector.\n        \"\"\"\n        # Step 1: Compute global parameter w(alpha)\n        w_alpha = np.sum([alpha_vec[i] * w_s[i] for i in range(len(w_s))], axis=0)\n\n        # Step 2: Compute client-specific losses L_i(w(alpha))\n        losses = []\n        for i in range(len(w_s)):\n            delta_w = w_alpha - w_s[i]\n            loss = delta_w.T @ H[i] @ delta_w + c[i]\n            losses.append(loss)\n        \n        # Step 3: Compute overall risk R(w(alpha))\n        R = np.sum(p * losses)\n\n        # Step 4: Compute group risks and fairness disparity D(w(alpha))\n        R_G1 = np.mean([losses[i] for i in G1_indices])\n        R_G2 = np.mean([losses[i] for i in G2_indices])\n        D = np.abs(R_G1 - R_G2)\n\n        return R, D\n\n    # Compute baseline risk and disparity\n    R_base, D_base = compute_all_metrics(alpha_base, H_mats, w_stars, c_vals, p_vec)\n\n    results = []\n    for alpha_test in test_cases:\n        # Compute metrics for the current test case\n        R_test, D_test = compute_all_metrics(alpha_test, H_mats, w_stars, c_vals, p_vec)\n\n        # Step 5: Compute marginal fairness-accuracy slope S(alpha)\n        delta_R = R_test - R_base\n        if np.isclose(delta_R, 0):\n            S = 0.0\n        else:\n            delta_D = D_test - D_base\n            S = delta_D / delta_R\n        \n        # Append the triplet [R, D, S] to results\n        results.append([R_test, D_test, S])\n\n    # Format and print the final output string.\n    output_str = ','.join([str(res) for res in results])\n    print(f\"[{output_str}]\")\n\nsolve()\n\n```"
        }
    ]
}