{
    "hands_on_practices": [
        {
            "introduction": "A cornerstone of creating effective Digital Twins is accurately identifying the dynamics of the physical system. This first practice tackles this central challenge by demonstrating how to learn a system's model within a federated framework . You will derive a federated estimator for a Linear Time-Invariant (LTI) system, showing how to incorporate prior physical knowledge through regularization and aggregate local data into a globally consistent model without compromising data privacy.",
            "id": "4222039",
            "problem": "Consider a decentralized Cyber-Physical System (CPS) consisting of $K$ distributed clients, each maintaining a Digital Twin whose local dynamics are well-approximated by a Linear Time-Invariant (LTI) model. For client $k \\in \\{1,\\dots,K\\}$, the local state and input time series satisfy the discrete-time dynamics $x_{t+1}^{(k)} = A x_{t}^{(k)} + B u_{t}^{(k)} + \\epsilon_{t}^{(k)}$, where $x_{t}^{(k)} \\in \\mathbb{R}^{n}$, $u_{t}^{(k)} \\in \\mathbb{R}^{m}$, and $\\epsilon_{t}^{(k)} \\in \\mathbb{R}^{n}$ is model mismatch and process noise. Assume a physics-based prior model $(A_{0}, B_{0})$ obtained from first-principles modeling and linearization, with the concatenated parameter matrix defined as $\\Theta_{0} \\triangleq [A_{0}\\;B_{0}] \\in \\mathbb{R}^{n \\times (n+m)}$, and define the concatenated regressor $z_{t}^{(k)} \\triangleq \\begin{pmatrix} x_{t}^{(k)} \\\\ u_{t}^{(k)} \\end{pmatrix} \\in \\mathbb{R}^{n+m}$. Each client has a local trajectory of length $T_{k}$, with samples indexed by $t \\in \\{0,\\dots,T_{k}-1\\}$.\n\nYou are tasked with designing a physics-informed loss that penalizes violation of the LTI dynamics and formulating a federated estimator of $(A,B)$ that is consistent across clients while preserving data locality. The physics-informed loss must start from the following scientific bases:\n- The definition of an LTI system and its residual $r_{t}^{(k)} \\triangleq x_{t+1}^{(k)} - A x_{t}^{(k)} - B u_{t}^{(k)}$.\n- The well-tested assumption that $\\epsilon_{t}^{(k)}$ is independent and identically distributed with $\\epsilon_{t}^{(k)} \\sim \\mathcal{N}(0,\\sigma^{2} I_{n})$ for a known scalar $\\sigma^{2}  0$.\n- The need to regularize the estimator toward the physics-based prior $\\Theta_{0}$ using a Tikhonov penalty with regularization strength $\\lambda  0$.\n\nFormally, derive an additive objective over clients and time that penalizes dynamic residuals and regularizes toward $\\Theta_{0}$, and show that minimizing this objective over $\\Theta \\triangleq [A\\;B] \\in \\mathbb{R}^{n \\times (n+m)}$ admits a closed-form solution in terms of client-local sufficient statistics that can be aggregated without sharing raw data. Express your final estimator entirely in terms of the following aggregated quantities across clients:\n- $S_{ZZ} \\triangleq \\sum_{k=1}^{K} \\sum_{t=0}^{T_{k}-1} z_{t}^{(k)} z_{t}^{(k)\\top} \\in \\mathbb{R}^{(n+m) \\times (n+m)}$,\n- $S_{XZ} \\triangleq \\sum_{k=1}^{K} \\sum_{t=0}^{T_{k}-1} x_{t+1}^{(k)} z_{t}^{(k)\\top} \\in \\mathbb{R}^{n \\times (n+m)}$.\n\nYour derivation must rely only on the bases stated above and must not introduce untested shortcut formulas. The final answer must be a single closed-form analytical expression for the globally federated estimate of $\\Theta$ written as a function of $S_{ZZ}$, $S_{XZ}$, $\\lambda$, $\\sigma^{2}$, and $\\Theta_{0}$. Do not include any numerical values and do not provide an inequality. Do not include any units in your final answer.",
            "solution": "The problem is subjected to validation.\n\n### Step 1: Extract Givens\n- **System**: A decentralized Cyber-Physical System (CPS) with $K$ clients.\n- **Client Dynamics**: Discrete-time Linear Time-Invariant (LTI) model for client $k \\in \\{1, \\dots, K\\}$: $x_{t+1}^{(k)} = A x_{t}^{(k)} + B u_{t}^{(k)} + \\epsilon_{t}^{(k)}$.\n- **State Vector**: $x_{t}^{(k)} \\in \\mathbb{R}^{n}$.\n- **Input Vector**: $u_{t}^{(k)} \\in \\mathbb{R}^{m}$.\n- **Noise Term**: $\\epsilon_{t}^{(k)} \\in \\mathbb{R}^{n}$.\n- **Physics-Based Prior**: $(A_{0}, B_{0})$.\n- **Concatenated Prior Matrix**: $\\Theta_{0} \\triangleq [A_{0}\\;B_{0}] \\in \\mathbb{R}^{n \\times (n+m)}$.\n- **Concatenated Regressor**: $z_{t}^{(k)} \\triangleq \\begin{pmatrix} x_{t}^{(k)} \\\\ u_{t}^{(k)} \\end{pmatrix} \\in \\mathbb{R}^{n+m}$.\n- **Data Trajectory**: For each client $k$, a trajectory of length $T_{k}$ with time index $t \\in \\{0, \\dots, T_{k}-1\\}$.\n- **Residual Definition**: $r_{t}^{(k)} \\triangleq x_{t+1}^{(k)} - A x_{t}^{(k)} - B u_{t}^{(k)}$.\n- **Noise Model**: $\\epsilon_{t}^{(k)}$ are independent and identically distributed (i.i.d.) with a Gaussian distribution $\\mathcal{N}(0,\\sigma^{2} I_{n})$, where $\\sigma^{2}  0$ is a known scalar.\n- **Regularization**: Tikhonov penalty towards $\\Theta_{0}$ with regularization strength $\\lambda  0$.\n- **Parameter Matrix to Estimate**: $\\Theta \\triangleq [A\\;B] \\in \\mathbb{R}^{n \\times (n+m)}$.\n- **Aggregated Sufficient Statistics**:\n  - $S_{ZZ} \\triangleq \\sum_{k=1}^{K} \\sum_{t=0}^{T_{k}-1} z_{t}^{(k)} z_{t}^{(k)\\top} \\in \\mathbb{R}^{(n+m) \\times (n+m)}$.\n  - $S_{XZ} \\triangleq \\sum_{k=1}^{K} \\sum_{t=0}^{T_{k}-1} x_{t+1}^{(k)} z_{t}^{(k)\\top} \\in \\mathbb{R}^{n \\times (n+m)}$.\n- **Objective**: Derive a federated estimator for $\\Theta$ in closed form, expressed as a function of $S_{ZZ}$, $S_{XZ}$, $\\lambda$, $\\sigma^{2}$, and $\\Theta_{0}$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, well-posed, and objective.\n1.  **Scientific Grounding**: The problem is rooted in fundamental principles of system identification, statistical estimation, and control theory. The use of an LTI model, Gaussian noise assumption, maximum likelihood estimation, and Tikhonov regularization are standard and rigorously established techniques in engineering and statistics. The federated learning context is a modern, valid application area for these principles.\n2.  **Well-Posedness**: The problem asks for the derivation of a minimizer for an objective function. The objective, combining a sum-of-squares term (from the Gaussian likelihood) and a quadratic regularization penalty, is convex. This guarantees the existence and uniqueness of the solution. The Tikhonov regularization ensures the problem remains well-conditioned even if the data matrix sum $S_{ZZ}$ is not full rank.\n3.  **Objectivity and Completeness**: The problem is stated using precise mathematical language and notation. All necessary components to formulate the objective and derive its minimizer are provided. The definitions are self-contained and consistent.\n\nThe problem does not exhibit any flaws such as scientific unsoundness, missing information, or ambiguity.\n\n### Step 3: Verdict and Action\nThe problem is valid. A reasoned solution will be provided.\n\nThe task is to derive a federated estimator for the parameter matrix $\\Theta = [A\\;B]$ by minimizing a physics-informed loss function. This loss function must be constructed from the scientific bases provided: penalizing the dynamic residual and regularizing towards a prior model $\\Theta_0$. This formulation is equivalent to finding a Maximum A Posteriori (MAP) estimate of $\\Theta$.\n\nThe total objective function, which we seek to minimize with respect to $\\Theta$, is the sum of a data-fidelity term and a regularization term.\n\nThe data-fidelity term arises from the negative log-likelihood of the observed data. Given the i.i.d. Gaussian noise model $\\epsilon_{t}^{(k)} \\sim \\mathcal{N}(0, \\sigma^2 I_n)$, the likelihood of observing the state $x_{t+1}^{(k)}$ given the regressor $z_t^{(k)}$ and parameters $\\Theta$ is:\n$$p(x_{t+1}^{(k)} | z_t^{(k)}, \\Theta) = \\frac{1}{(2\\pi\\sigma^2)^{n/2}} \\exp\\left(-\\frac{1}{2\\sigma^2} \\|x_{t+1}^{(k)} - \\Theta z_t^{(k)}\\|_2^2\\right)$$\nSince all samples are independent, the total log-likelihood is the sum of the individual log-likelihoods. The negative log-likelihood, ignoring constant terms, is proportional to the sum of squared residuals, scaled by the noise variance:\n$$-\\ln \\mathcal{L}(\\Theta) \\propto \\frac{1}{2\\sigma^2} \\sum_{k=1}^{K} \\sum_{t=0}^{T_k-1} \\|x_{t+1}^{(k)} - \\Theta z_t^{(k)}\\|_2^2$$\n\nThe regularization term is specified as a Tikhonov penalty with strength $\\lambda$:\n$$\\mathcal{R}(\\Theta) = \\frac{\\lambda}{2} \\|\\Theta - \\Theta_0\\|_F^2$$\nwhere $\\|\\cdot\\|_F$ denotes the Frobenius norm. The factor of $1/2$ is included for convenience in differentiation.\n\nThe total objective function to minimize is the sum of the negative log-likelihood term and the regularization term, representing the negative log-posterior:\n$$\\mathcal{J}(\\Theta) = \\frac{1}{2\\sigma^2} \\sum_{k=1}^{K} \\sum_{t=0}^{T_k-1} \\|x_{t+1}^{(k)} - \\Theta z_t^{(k)}\\|_2^2 + \\frac{\\lambda}{2} \\|\\Theta - \\Theta_0\\|_F^2$$\nThis objective is additive over clients and time, which is essential for a federated approach. We can rewrite the objective using matrix trace notation, noting that for a vector $v$, $\\|v\\|_2^2 = v^\\top v = \\text{tr}(vv^\\top)$, and for a matrix $M$, $\\|M\\|_F^2 = \\text{tr}(MM^\\top)$.\n\nLet's expand the terms in the objective function:\n$$\\sum_{k,t} \\|x_{t+1}^{(k)} - \\Theta z_t^{(k)}\\|_2^2 = \\sum_{k,t} \\text{tr}\\left((x_{t+1}^{(k)} - \\Theta z_t^{(k)})(x_{t+1}^{(k)} - \\Theta z_t^{(k)})^\\top\\right)$$\n$$= \\sum_{k,t} \\text{tr}\\left(x_{t+1}^{(k)}(x_{t+1}^{(k)})^\\top - x_{t+1}^{(k)}(z_t^{(k)})^\\top\\Theta^\\top - \\Theta z_t^{(k)}(x_{t+1}^{(k)})^\\top + \\Theta z_t^{(k)}(z_t^{(k)})^\\top\\Theta^\\top\\right)$$\nUsing the linearity of the trace and sum operators, this becomes:\n$$= \\text{tr}\\left(\\sum_{k,t} x_{t+1}^{(k)}(x_{t+1}^{(k)})^\\top\\right) - \\text{tr}\\left(\\left(\\sum_{k,t} x_{t+1}^{(k)}(z_t^{(k)})^\\top\\right)\\Theta^\\top\\right) - \\text{tr}\\left(\\Theta \\left(\\sum_{k,t} z_t^{(k)}(x_{t+1}^{(k)})^\\top\\right)\\right) + \\text{tr}\\left(\\Theta \\left(\\sum_{k,t} z_t^{(k)}(z_t^{(k)})^\\top\\right) \\Theta^\\top\\right)$$\nUsing the provided definitions for $S_{XZ}$ and $S_{ZZ}$, and the property $\\text{tr}(A) = \\text{tr}(A^\\top)$:\n$$= C - 2\\text{tr}(S_{XZ} \\Theta^\\top) + \\text{tr}(\\Theta S_{ZZ} \\Theta^\\top)$$\nwhere $C$ is a constant independent of $\\Theta$.\n\nThe regularization term is:\n$$\\|\\Theta - \\Theta_0\\|_F^2 = \\text{tr}((\\Theta-\\Theta_0)(\\Theta-\\Theta_0)^\\top) = \\text{tr}(\\Theta\\Theta^\\top - \\Theta\\Theta_0^\\top - \\Theta_0\\Theta^\\top + \\Theta_0\\Theta_0^\\top)$$\n$$= \\text{tr}(\\Theta\\Theta^\\top) - 2\\text{tr}(\\Theta_0\\Theta^\\top) + \\text{const}$$\n\nSubstituting these back into the objective function $\\mathcal{J}(\\Theta)$:\n$$\\mathcal{J}(\\Theta) = \\frac{1}{2\\sigma^2}\\left(-2\\text{tr}(S_{XZ}\\Theta^\\top) + \\text{tr}(\\Theta S_{ZZ} \\Theta^\\top)\\right) + \\frac{\\lambda}{2}\\left(\\text{tr}(\\Theta\\Theta^\\top) - 2\\text{tr}(\\Theta_0\\Theta^\\top)\\right) + \\text{const}$$\nThis objective is a quadratic function of $\\Theta$ and is convex. The minimum is found by setting its gradient with respect to $\\Theta$ to zero. We use the matrix calculus identities $\\nabla_X \\text{tr}(AX^\\top) = A$ and $\\nabla_X \\text{tr}(X B X^\\top) = X(B+B^\\top)$.\n$$\\nabla_\\Theta \\mathcal{J}(\\Theta) = \\frac{1}{2\\sigma^2}\\left(-2S_{XZ} + \\Theta(S_{ZZ} + S_{ZZ}^\\top)\\right) + \\frac{\\lambda}{2}\\left(2\\Theta - 2\\Theta_0\\right)$$\nSince $S_{ZZ}$ is symmetric by definition ($S_{ZZ} = S_{ZZ}^\\top$), this simplifies to:\n$$\\nabla_\\Theta \\mathcal{J}(\\Theta) = \\frac{1}{\\sigma^2}(-S_{XZ} + \\Theta S_{ZZ}) + \\lambda(\\Theta - \\Theta_0)$$\nSetting the gradient to zero to find the optimal $\\hat{\\Theta}$:\n$$\\frac{1}{\\sigma^2}(-S_{XZ} + \\hat{\\Theta} S_{ZZ}) + \\lambda(\\hat{\\Theta} - \\Theta_0) = 0$$\nWe now solve for $\\hat{\\Theta}$. Multiply by $\\sigma^2$:\n$$-S_{XZ} + \\hat{\\Theta} S_{ZZ} + \\lambda\\sigma^2(\\hat{\\Theta} - \\Theta_0) = 0$$\n$$\\hat{\\Theta} S_{ZZ} + \\lambda\\sigma^2 \\hat{\\Theta} = S_{XZ} + \\lambda\\sigma^2 \\Theta_0$$\nFactor out $\\hat{\\Theta}$:\n$$\\hat{\\Theta}(S_{ZZ} + \\lambda\\sigma^2 I) = S_{XZ} + \\lambda\\sigma^2 \\Theta_0$$\nwhere $I$ is the identity matrix of size $(n+m) \\times (n+m)$.\nThe closed-form solution for the federated estimate $\\hat{\\Theta}$ is obtained by right-multiplying by the inverse of $(S_{ZZ} + \\lambda\\sigma^2 I)$. This matrix is invertible for $\\lambda  0$ and $\\sigma^2  0$ because $S_{ZZ}$ is positive semi-definite and $\\lambda\\sigma^2 I$ is positive definite, making their sum positive definite and thus invertible.\n$$\\hat{\\Theta} = (S_{XZ} + \\lambda\\sigma^2 \\Theta_0)(S_{ZZ} + \\lambda\\sigma^2 I)^{-1}$$\nThis solution is compliant with the federated setting. Each client $k$ computes its local statistics $\\sum_{t} z_{t}^{(k)} z_{t}^{(k)\\top}$ and $\\sum_{t} x_{t+1}^{(k)} z_{t}^{(k)\\top}$, which are then aggregated at a central server to form $S_{ZZ}$ and $S_{XZ}$ without sharing the raw data $\\{x_t^{(k)}, u_t^{(k)}\\}$. The server then computes the final estimate using the derived formula.",
            "answer": "$$\n\\boxed{(S_{XZ} + \\lambda\\sigma^{2}\\Theta_{0})(S_{ZZ} + \\lambda\\sigma^{2}I)^{-1}}\n$$"
        },
        {
            "introduction": "Decentralized Cyber-Physical Systems often rely on a network of sensors with varying quality and precision. This practice explores how Federated Learning provides an elegant solution for optimally fusing data from such heterogeneous sources . By deriving the steady-state performance of a federated Kalman filter, you will quantify the significant advantage of weighting sensor data by its quality (inverse noise variance) compared to naive averaging, a key principle in robust state estimation.",
            "id": "4222026",
            "problem": "Consider a decentralized Cyber-Physical System (CPS) with a Digital Twin (DT) running a Federated Learning (FL) estimation process of a scalar latent state. The physical process evolves according to a discrete-time linear time-invariant model $x_{k+1} = a x_k + w_k$, where $x_k \\in \\mathbb{R}$, $a \\in \\mathbb{R}$ with $|a|  1$ for stability, and the process noise $w_k$ is zero-mean Gaussian with variance $Q  0$. There are $m \\in \\mathbb{N}$ geographically distributed sensors; sensor $i \\in \\{1,\\dots,m\\}$ provides the measurement $y_{i,k} = x_k + v_{i,k}$, where $v_{i,k}$ is zero-mean Gaussian with variance $R_i  0$, independent across sensors and time, and independent of $w_k$.\n\nThe DT maintains a Kalman-style estimator using federated aggregation. In the federated setting, raw measurements are not centralized; instead, local sites send the necessary statistics for an information-form update. The aggregator constructs a measurement update that is equivalent (in the linear-Gaussian sense) to weighting each sensor’s contribution by the inverse of its noise covariance, $R_i^{-1}$. Let the posterior error covariance after the measurement update be denoted by $P_k^{+}$ and the prior error covariance before the measurement update by $P_k^{-}$. The time update obeys $P_{k+1}^{-} = a^2 P_k^{+} + Q$.\n\nStarting from fundamental Bayesian estimation principles for linear-Gaussian models and the independence of sensor noises, derive the steady-state posterior error covariance $P^{\\star}$ under inverse-covariance federated aggregation, expressed in closed form as a function of $a$, $Q$, and $\\{R_i\\}_{i=1}^{m}$. Then, using first principles, explain how inverse-covariance weighting compares with naive uniform averaging of measurements $ \\bar{y}_k = \\frac{1}{m} \\sum_{i=1}^{m} y_{i,k}$ in terms of steady-state error covariance, without relying on any shortcut formulas.\n\nYour final answer must be the single closed-form analytic expression for $P^{\\star}$, and must not include any inequality or equation comparison. No rounding is required, and no units are needed.",
            "solution": "The problem as stated is scientifically grounded, well-posed, objective, and self-contained. It describes a standard discrete-time linear state-space model and poses a question regarding the steady-state solution of the associated Kalman filter error covariance Riccati equation under a federated sensing scenario. The models and assumptions are standard in estimation theory and control systems. The problem is therefore deemed valid.\n\nThe physical process is described by the linear time-invariant (LTI) model:\n$$x_{k+1} = a x_k + w_k, \\quad w_k \\sim \\mathcal{N}(0, Q)$$\nwhere $x_k \\in \\mathbb{R}$ is the scalar state, the stability condition $|a|  1$ holds, and $w_k$ is the process noise.\nThere are $m$ sensors, with the $i$-th sensor providing a measurement:\n$$y_{i,k} = x_k + v_{i,k}, \\quad v_{i,k} \\sim \\mathcal{N}(0, R_i)$$\nThe measurement noises $v_{i,k}$ are mutually independent and independent of $w_k$.\n\nThe core of the federated aggregation scheme described is the principle of information fusion. For independent Gaussian measurements, the total information about the state is the sum of the information provided by each source. The information form of the Kalman filter is most natural here. The information update for the error covariance matrix (a scalar $P_k^{+}$ in this case) is given by:\n$$(P_k^{+})^{-1} = (P_k^{-})^{-1} + \\sum_{i=1}^{m} H_i^T R_i^{-1} H_i$$\nIn our case, the measurement matrix $H_i$ for each sensor is simply the scalar $1$. Thus, the update equation for the posterior error covariance becomes:\n$$(P_k^{+})^{-1} = (P_k^{-})^{-1} + \\sum_{i=1}^{m} R_i^{-1}$$\nLet us define the total measurement precision, $S$, as the sum of the individual sensor precisions:\n$$S = \\sum_{i=1}^{m} R_i^{-1}$$\nThe measurement update equation is then $(P_k^{+})^{-1} = (P_k^{-})^{-1} + S$. The quantity $S^{-1}$ can be interpreted as the effective measurement noise variance, $R_{eff}$, of a single equivalent sensor that optimally fuses the information from all $m$ sensors.\n\nThe time update for the error covariance is given as:\n$$P_{k+1}^{-} = a^2 P_k^{+} + Q$$\n\nWe seek the steady-state posterior error covariance, which we denote by $P^{\\star}$. At steady state, the covariances no longer change with time, so $P_k^{+} = P_{k+1}^{+} = P^{\\star}$ and $P_k^{-} = P_{k+1}^{-} = P^{-}$. The steady-state equations are:\n1. $(P^{\\star})^{-1} = (P^{-})^{-1} + S$\n2. $P^{-} = a^2 P^{\\star} + Q$\n\nWe can solve this system of two equations for the single unknown $P^{\\star}$. From equation (1), we can express $P^{-}$ in terms of $P^{\\star}$:\n$$(P^{-})^{-1} = (P^{\\star})^{-1} - S = \\frac{1 - S P^{\\star}}{P^{\\star}} \\implies P^{-} = \\frac{P^{\\star}}{1 - S P^{\\star}}$$\nSubstituting this into equation (2):\n$$\\frac{P^{\\star}}{1 - S P^{\\star}} = a^2 P^{\\star} + Q$$\nThis approach leads to algebraic difficulties. A more direct substitution is to substitute equation (2) into equation (1):\n$$(P^{\\star})^{-1} = (a^2 P^{\\star} + Q)^{-1} + S$$\n$$\\frac{1}{P^{\\star}} = \\frac{1}{a^2 P^{\\star} + Q} + S$$\nTo eliminate the fractions, we multiply the entire equation by $P^{\\star}(a^2 P^{\\star} + Q)$:\n$$a^2 P^{\\star} + Q = P^{\\star} + S P^{\\star}(a^2 P^{\\star} + Q)$$\n$$a^2 P^{\\star} + Q = P^{\\star} + a^2 S (P^{\\star})^2 + Q S P^{\\star}$$\nRearranging the terms yields a quadratic equation for $P^{\\star}$:\n$$a^2 S (P^{\\star})^2 + (1 + QS - a^2) P^{\\star} - Q = 0$$\nThis is a standard quadratic equation of the form $A x^2 + B x + C = 0$, with $x = P^{\\star}$, $A = a^2 S$, $B = 1-a^2+QS$, and $C = -Q$. The solution is given by the quadratic formula:\n$$P^{\\star} = \\frac{-B \\pm \\sqrt{B^2 - 4AC}}{2A}$$\nSubstituting our coefficients:\n$$P^{\\star} = \\frac{-(1-a^2+QS) \\pm \\sqrt{(1-a^2+QS)^2 - 4(a^2 S)(-Q)}}{2 a^2 S}$$\n$$P^{\\star} = \\frac{-(1-a^2+QS) \\pm \\sqrt{(1-a^2+QS)^2 + 4 a^2 Q S}}{2 a^2 S}$$\nSince $P^{\\star}$ represents an error variance, it must be a positive real number. The denominator $2a^2S$ is positive (assuming $a \\neq 0$, which is typical, and we know $S  0$). Let's analyze the numerator. The term under the square root is strictly positive. The square root term, $\\sqrt{(1-a^2+QS)^2 + 4 a^2 Q S}$, is strictly greater than $\\sqrt{(1-a^2+QS)^2} = |1-a^2+QS|$. Since $|a|1$, $Q0$, and $S0$, the term $1-a^2+QS$ is positive. Thus, the square root is greater than $1-a^2+QS$. To ensure a positive result for $P^{\\star}$, we must choose the `+' sign in the numerator:\n$$P^{\\star} = \\frac{-(1-a^2+QS) + \\sqrt{(1-a^2+QS)^2 + 4 a^2 Q S}}{2 a^2 S}$$\nThis is the closed-form expression for the steady-state posterior error covariance under inverse-covariance federated aggregation, with $S = \\sum_{i=1}^{m} R_i^{-1}$.\n\nNext, we compare this optimal fusion with naive uniform averaging of measurements. The averaged measurement is:\n$$\\bar{y}_k = \\frac{1}{m} \\sum_{i=1}^{m} y_{i,k} = \\frac{1}{m} \\sum_{i=1}^{m} (x_k + v_{i,k}) = x_k + \\frac{1}{m} \\sum_{i=1}^{m} v_{i,k}$$\nThis corresponds to an effective measurement model $\\bar{y}_k = x_k + \\bar{v}_k$, where the effective measurement noise is $\\bar{v}_k = \\frac{1}{m} \\sum_{i=1}^{m} v_{i,k}$. Since the noises $v_{i,k}$ are independent, the variance of their sum is the sum of their variances. The variance of the effective noise is:\n$$R_{avg} = \\text{Var}(\\bar{v}_k) = \\text{Var}\\left(\\frac{1}{m} \\sum_{i=1}^{m} v_{i,k}\\right) = \\frac{1}{m^2} \\sum_{i=1}^{m} \\text{Var}(v_{i,k}) = \\frac{1}{m^2} \\sum_{i=1}^{m} R_i$$\nThe effective measurement noise variance for the optimal inverse-covariance weighting is:\n$$R_{eff} = S^{-1} = \\left(\\sum_{i=1}^{m} R_i^{-1}\\right)^{-1}$$\nFrom first principles of mathematics, specifically the inequality of arithmetic and harmonic means for a set of positive numbers $\\{R_i\\}$, we have:\n$$\\frac{\\sum_{i=1}^{m} R_i}{m} \\ge \\frac{m}{\\sum_{i=1}^{m} R_i^{-1}}$$\nMultiplying both sides by $1/m$ gives:\n$$\\frac{1}{m^2}\\sum_{i=1}^{m} R_i \\ge \\frac{1}{\\sum_{i=1}^{m} R_i^{-1}}$$\nThis directly translates to:\n$$R_{avg} \\ge R_{eff}$$\nEquality holds if and only if all measurement noise variances are equal, i.e., $R_1=R_2=\\dots=R_m$.\n\nFrom first principles of estimation, the posterior error covariance is a monotonically increasing function of the measurement noise variance. A larger measurement noise variance implies that the measurement provides less reliable information about the state. Consequently, the filter places less weight on the measurement and more on the model prediction, leading to a larger uncertainty (error covariance) in the final state estimate. Therefore, since $R_{avg} \\ge R_{eff}$, the steady-state posterior error covariance resulting from naive averaging, $P_{avg}^{\\star}$, will be greater than or equal to the covariance from optimal inverse-covariance weighting, $P^{\\star}$. Naive uniform averaging is suboptimal unless all sensors are identically precise.",
            "answer": "$$\n\\boxed{\\frac{-(1-a^2+Q\\sum_{i=1}^{m} R_i^{-1}) + \\sqrt{(1-a^2+Q\\sum_{i=1}^{m} R_i^{-1})^2 + 4a^2Q(\\sum_{i=1}^{m} R_i^{-1})}}{2a^2(\\sum_{i=1}^{m} R_i^{-1})}}\n$$"
        },
        {
            "introduction": "Idealized models often assume perfect synchrony, but real-world distributed systems face communication delays and computational lags. This final practice confronts the practical challenge of asynchrony in Federated Learning, a critical consideration for deploying robust CPS . You will derive an upper bound on the model error induced by stale client updates and investigate how this error can be actively managed by a staleness-aware weighting scheme, linking theoretical analysis to practical system design.",
            "id": "4222082",
            "problem": "Consider a decentralized Cyber-Physical Systems (CPS) network in which a global Digital Twin (DT) model is trained via asynchronous Federated Learning (FL). At global iteration $t$, the server would ideally apply the synchronous update $-\\eta \\nabla f(x_t)$, where $f$ is the global objective, $x_t$ is the current global model state, and $\\eta0$ is the step size. In practice, client updates arrive with staleness (lag) due to communication and computation delays. Let the random lag $K \\in \\{0,1,2,\\dots\\}$ have a geometric distribution with parameter $p \\in (0,1]$, that is $P(K=k)=\\pi_k$ with $\\pi_k = p(1-p)^k$.\n\nAssume the following fundamental properties:\n- The global objective $f$ has $L$-Lipschitz continuous gradients, i.e., $\\|\\nabla f(x)-\\nabla f(y)\\| \\leq L \\|x-y\\|$ for all $x,y$ and some constant $L0$.\n- Each client’s stochastic gradient is uniformly bounded in norm by $G0$, i.e., $\\|\\nabla f(x)\\| \\leq G$ for all relevant iterates.\n- Each global update step has magnitude at most $\\eta G$, i.e., $\\|x_{t+1}-x_t\\| \\leq \\eta G$.\n\nDefine the asynchronous aggregate update with normalized staleness weights $\\alpha_k$ as $-\\eta \\sum_{k=0}^{\\infty} \\alpha_k \\nabla f(x_{t-k})$, where the weights satisfy $\\alpha_k \\geq 0$ and $\\sum_{k=0}^{\\infty} \\alpha_k = 1$. The lag-induced error in the aggregate model update at iteration $t$ is the deviation\n$$\n\\Delta \\triangleq \\left\\|-\\eta \\sum_{k=0}^{\\infty} \\alpha_k \\nabla f(x_{t-k}) + \\eta \\nabla f(x_t)\\right\\|.\n$$\n- Case A (unweighted): $\\alpha_k = \\pi_k$.\n- Case B (staleness weighting): $\\alpha_k = \\frac{\\pi_k w(k)}{\\sum_{j=0}^{\\infty} \\pi_j w(j)}$ for a scheme $w(k) = a^k$ with parameter $a \\in (0,1)$.\n\nStarting from the above fundamental properties and definitions only, derive a worst-case upper bound on $\\Delta$ for Case A and Case B. Then, choose the exponential staleness weighting scheme $w(k)=a^k$ and determine the value of $a$ that makes the worst-case bound in Case B exactly half of that in Case A. Your final answer must be the single closed-form expression for $a$ (no units). Do not provide intermediate steps or any additional quantities in your final answer.",
            "solution": "The problem requires the derivation of a worst-case upper bound on the lag-induced error, $\\Delta$, for two different asynchronous update schemes, and then to find a parameter value for one scheme that halves the bound relative to the other.\n\nThe lag-induced error is defined as:\n$$\n\\Delta \\triangleq \\left\\|-\\eta \\sum_{k=0}^{\\infty} \\alpha_k \\nabla f(x_{t-k}) + \\eta \\nabla f(x_t)\\right\\|\n$$\nwhere $\\eta  0$ is the step size, $f$ is the global objective, $x_t$ is the model state at global iteration $t$, and $\\alpha_k$ are normalized staleness weights satisfying $\\alpha_k \\geq 0$ and $\\sum_{k=0}^{\\infty} \\alpha_k = 1$.\n\nWe begin by simplifying the expression for $\\Delta$. We can factor out $\\eta$ and use the property that the weights sum to $1$:\n$$\n\\Delta = \\eta \\left\\| \\nabla f(x_t) - \\sum_{k=0}^{\\infty} \\alpha_k \\nabla f(x_{t-k}) \\right\\| = \\eta \\left\\| \\left(\\sum_{k=0}^{\\infty} \\alpha_k\\right) \\nabla f(x_t) - \\sum_{k=0}^{\\infty} \\alpha_k \\nabla f(x_{t-k}) \\right\\|\n$$\n$$\n\\Delta = \\eta \\left\\| \\sum_{k=0}^{\\infty} \\alpha_k (\\nabla f(x_t) - \\nabla f(x_{t-k})) \\right\\|\n$$\nApplying the triangle inequality for norms, we obtain an upper bound:\n$$\n\\Delta \\leq \\eta \\sum_{k=0}^{\\infty} \\alpha_k \\|\\nabla f(x_t) - \\nabla f(x_{t-k})\\|\n$$\nThe problem states that the global objective $f$ has $L$-Lipschitz continuous gradients, i.e., $\\|\\nabla f(x) - \\nabla f(y)\\| \\leq L \\|x - y\\|$. Applying this property, we get:\n$$\n\\Delta \\leq \\eta \\sum_{k=0}^{\\infty} \\alpha_k (L \\|x_t - x_{t-k}\\|)\n$$\nNext, we need to bound the term $\\|x_t - x_{t-k}\\|$. For $k0$, we can express the difference as a telescoping sum:\n$$\nx_t - x_{t-k} = \\sum_{j=1}^{k} (x_{t-j+1} - x_{t-j})\n$$\nApplying the triangle inequality again:\n$$\n\\|x_t - x_{t-k}\\| \\leq \\sum_{j=1}^{k} \\|x_{t-j+1} - x_{t-j}\\|\n$$\nThe problem provides a worst-case bound on the magnitude of each global update step: $\\|x_{i+1} - x_i\\| \\leq \\eta G$ for any iteration $i$. Substituting this into the inequality:\n$$\n\\|x_t - x_{t-k}\\| \\leq \\sum_{j=1}^{k} \\eta G = k \\eta G\n$$\nFor $k=0$, the distance is $\\|x_t - x_t\\|=0$, which is consistent with this bound. Substituting this worst-case distance bound back into our expression for the bound on $\\Delta$:\n$$\n\\Delta \\leq \\eta \\sum_{k=0}^{\\infty} \\alpha_k (L (k \\eta G)) = \\eta^2 L G \\sum_{k=0}^{\\infty} k \\alpha_k\n$$\nThis gives us a general worst-case upper bound on $\\Delta$. The specific bound depends on the choice of weights $\\alpha_k$. The term $\\sum_{k=0}^{\\infty} k \\alpha_k$ represents the effective average staleness.\n\nCase A (unweighted): The weights are given by the geometric distribution of the lag, $\\alpha_k = \\pi_k = p(1-p)^k$.\nThe effective average staleness is the expectation of this geometric distribution.\n$$\n\\sum_{k=0}^{\\infty} k \\alpha_k = \\sum_{k=0}^{\\infty} k p(1-p)^k = E[K]\n$$\nFor a geometric distribution with success probability $p$ and support $\\{0, 1, 2, \\dots\\}$, the expected value is $E[K] = \\frac{1-p}{p}$.\nTherefore, the worst-case upper bound for Case A, denoted $B_A$, is:\n$$\nB_A = \\eta^2 L G \\left(\\frac{1-p}{p}\\right)\n$$\n\nCase B (staleness weighting): The weights are given by $\\alpha_k = \\frac{\\pi_k w(k)}{\\sum_{j=0}^{\\infty} \\pi_j w(j)}$ with $w(k) = a^k$ for $a \\in (0,1)$.\nFirst, we compute the normalization constant in the denominator:\n$$\n\\sum_{j=0}^{\\infty} \\pi_j w(j) = \\sum_{j=0}^{\\infty} p(1-p)^j a^j = p \\sum_{j=0}^{\\infty} (a(1-p))^j\n$$\nThis is a geometric series with ratio $r = a(1-p)$. Since $a \\in (0,1)$ and $p \\in (0,1]$, we have $0 \\leq 1-p  1$, so $0 \\leq r  1$. The series converges to $\\frac{1}{1-r}$.\n$$\n\\sum_{j=0}^{\\infty} \\pi_j w(j) = p \\left(\\frac{1}{1 - a(1-p)}\\right)\n$$\nNow we can write the expression for $\\alpha_k$:\n$$\n\\alpha_k = \\frac{p(1-p)^k a^k}{p / (1 - a(1-p))} = (1 - a(1-p))(a(1-p))^k\n$$\nThis expression for $\\alpha_k$ is the probability mass function of a new geometric distribution with success probability $p' = 1 - a(1-p)$ and support $\\{0, 1, 2, \\dots\\}$.\nThe effective average staleness for Case B is the expectation of this new distribution:\n$$\n\\sum_{k=0}^{\\infty} k \\alpha_k = \\frac{1-p'}{p'} = \\frac{1 - (1 - a(1-p))}{1 - a(1-p)} = \\frac{a(1-p)}{1 - a(1-p)}\n$$\nTherefore, the worst-case upper bound for Case B, denoted $B_B$, is:\n$$\nB_B = \\eta^2 L G \\left(\\frac{a(1-p)}{1 - a(1-p)}\\right)\n$$\n\nFinally, we need to find the value of $a$ such that $B_B = \\frac{1}{2} B_A$.\nWe set up the equation. For $p \\in (0,1)$, both bounds are non-zero.\n$$\n\\eta^2 L G \\left(\\frac{a(1-p)}{1 - a(1-p)}\\right) = \\frac{1}{2} \\left( \\eta^2 L G \\left(\\frac{1-p}{p}\\right) \\right)\n$$\nThe terms $\\eta^2 L G$ and $(1-p)$ (since $p1$) are non-zero and can be cancelled from both sides. If $p=1$, both bounds are $0$ and the equality holds for any $a$, but the problem is most meaningful for $p1$.\n$$\n\\frac{a}{1 - a(1-p)} = \\frac{1}{2p}\n$$\nNow we solve for $a$:\n$$\n2ap = 1 - a(1-p)\n$$\n$$\n2ap = 1 - a + ap\n$$\n$$\n2ap + a - ap = 1\n$$\n$$\nap + a = 1\n$$\n$$\na(p+1) = 1\n$$\n$$\na = \\frac{1}{1+p}\n$$\nThis is the required closed-form expression for $a$. Since $p \\in (0,1]$, $1+p \\in (1,2]$, which means $a \\in [\\frac{1}{2}, 1)$, consistent with the given constraint $a \\in (0,1)$.",
            "answer": "$$\n\\boxed{\\frac{1}{1+p}}\n$$"
        }
    ]
}