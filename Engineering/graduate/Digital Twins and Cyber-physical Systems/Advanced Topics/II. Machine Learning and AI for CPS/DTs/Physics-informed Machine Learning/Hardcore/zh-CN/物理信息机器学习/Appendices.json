{
    "hands_on_practices": [
        {
            "introduction": "物理信息机器学习的核心在于其损失函数的巧妙设计。本练习将引导你从第一性原理出发，推导一个通用的混合损失函数，从而学会如何平衡基于物理的残差、观测数据、边界条件和正则化项 。对于设计适用于特定问题的定制化PINN模型而言，这是一项至关重要的基础技能。",
            "id": "4235602",
            "problem": "考虑一个赛博物理系统，其数字孪生状态是在一个带有边界 $\\partial \\Omega$ 的域 $\\Omega \\subset \\mathbb{R}^{d}$ 上定义的标量场 $u$。其控制物理学由一个非线性微分算子 $\\mathcal{L}$ 和一个已知的源项 $g$ 表示，因此物理定律是在 $\\Omega$ 上的偏微分方程 (PDE) $\\mathcal{L}(u) = g$。边界行为由一个边界算子 $\\mathcal{B}$ 和边界数据 $b$ 指定，使得在 $\\partial \\Omega$ 上有 $\\mathcal{B}(u) = b$。一个从粗糙数值求解器中获得的基于物理的代理模型 $u_{\\text{phys}}$ 是可用的，并且在其分辨率下满足 $\\mathcal{L}(u_{\\text{phys}}) = g$。为了提高保真度，我们引入一个由可训练参数 $\\theta$ 参数化的残差场 $r_{\\theta}$（例如，通过物理信息神经网络 PINN），并将校正场构造为 $u = u_{\\text{phys}} + r_{\\theta}$。\n\n假设在 $\\Omega$ 内部有含噪声的观测数据 $\\{(x_{i}, y_{i})\\}_{i=1}^{N_{d}}$，其中 $x_{i} \\in \\Omega$，$y_{i}$ 是真实状态的测量值。令 $w_{p}:\\Omega \\to \\mathbb{R}_{+}$ 和 $w_{b}:\\partial \\Omega \\to \\mathbb{R}_{+}$ 分别为编码物理定律和边界条件强制执行的空间重要性的非负权重函数。任务是，从基于控制定律和数据失配的最小二乘残差最小化的第一性原理出发，推导出一个训练目标函数 $\\mathcal{J}(\\theta)$ 的单一闭式解析表达式。该目标函数应惩罚对 $\\mathcal{L}(u) = g$ 的违反，同时拟合数据并遵守边界条件，并对残差场进行正则化以避免病态解。您的表达式必须包括：\n\n1. 一个物理项，用于度量在 $\\Omega$ 上对 $\\mathcal{L}(u) = g$ 的违反程度。\n2. 一个数据项，用于度量在位置 $\\{x_{i}\\}_{i=1}^{N_{d}}$ 处 $u$ 与观测值之间的失配。\n3. 一个边界项，用于度量在 $\\partial \\Omega$ 上对 $\\mathcal{B}(u) = b$ 的违反程度。\n4. 一个关于 $r_{\\theta}$ 的平滑度正则化项，其动机源于 Tikhonov 类型的考量。\n\n所有惩罚项必须与正常数 $\\lambda_{p}$、$\\lambda_{d}$、$\\lambda_{b}$ 和 $\\lambda_{s}$ 组合成一个加权和。从物理和边界定律通过最小化适当 $L^{2}$ 范数下的平方残差来强制执行，并且数据通过最小化逐点平方误差来拟合这一原理出发，明确地推导目标函数。以单一解析表达式的形式提供最终的目标函数 $\\mathcal{J}(\\theta)$。不需要进行数值评估，也不需要四舍五入。在最终答案中明确地写出权重、积分和求和，不要引入指定之外的任何额外项。",
            "solution": "问题陈述经评估为有效，具有科学依据、适定性、客观性和内部一致性。它描述了物理信息机器学习中的一个典型问题：为训练残差校正神经网络构建复合损失函数。我们接下来从指定的第一性原理出发推导训练目标函数 $\\mathcal{J}(\\theta)$。\n\n目标函数 $\\mathcal{J}(\\theta)$ 是四个不同惩罚项的加权和：一个基于物理的残差项、一个数据失配项、一个边界条件项和一个平滑度正则化项。其一般形式为：\n$$\n\\mathcal{J}(\\theta) = \\lambda_{p}\\mathcal{J}_{p}(\\theta) + \\lambda_{d}\\mathcal{J}_{d}(\\theta) + \\lambda_{b}\\mathcal{J}_{b}(\\theta) + \\lambda_{s}\\mathcal{J}_{s}(\\theta)\n$$\n其中 $\\lambda_{p}$、$\\lambda_{d}$、$\\lambda_{b}$ 和 $\\lambda_{s}$ 是正常数权重。我们分别推导每个分量。\n\n1.  **物理项 ($\\mathcal{J}_{p}(\\theta)$)**：此项惩罚对控制偏微分方程 (PDE) $\\mathcal{L}(u) = g$ 的违反。其原理是在域 $\\Omega$上最小化该方程的平方残差，并由 $w_{p}(x)$ 加权。物理残差的加权 $L^2$ 范数的平方是适当的度量。\n    物理残差为 $R_{p}(x) = \\mathcal{L}(u)(x) - g(x)$。代入校正场 $u = u_{\\text{phys}} + r_{\\theta}$，残差变为：\n    $$\n    R_{p}(x) = \\mathcal{L}(u_{\\text{phys}} + r_{\\theta})(x) - g(x)\n    $$\n    物理损失项是加权平方残差在域 $\\Omega$ 上的积分：\n    $$\n    \\mathcal{J}_{p}(\\theta) = \\int_{\\Omega} w_{p}(x) \\left( R_{p}(x) \\right)^2 dx = \\int_{\\Omega} w_{p}(x) \\left( \\mathcal{L}(u_{\\text{phys}} + r_{\\theta}) - g \\right)^2 dx\n    $$\n\n2.  **数据项 ($\\mathcal{J}_{d}(\\theta)$)**：此项度量模型预测与可用观测数据 $\\{(x_{i}, y_{i})\\}_{i=1}^{N_{d}}$ 之间的失配。其原理是最小化逐点平方误差之和。\n    对于每个数据点 $i$，误差是模型输出 $u(x_i)$ 和测量值 $y_i$ 之间的差值。\n    $$\n    \\text{Error}_i = u(x_i) - y_i = (u_{\\text{phys}}(x_i) + r_{\\theta}(x_i)) - y_i\n    $$\n    数据损失项是所有 $N_d$ 个数据点上这些误差的平方和：\n    $$\n    \\mathcal{J}_{d}(\\theta) = \\sum_{i=1}^{N_{d}} \\left( (u_{\\text{phys}}(x_i) + r_{\\theta}(x_i)) - y_i \\right)^2\n    $$\n\n3.  **边界项 ($\\mathcal{J}_{b}(\\theta)$)**：此项在边界 $\\partial \\Omega$ 上强制执行边界条件 $\\mathcal{B}(u) = b$。与物理项类似，这是通过在边界曲面 $\\partial \\Omega$ 上最小化边界条件的加权 $L^2$ 范数下的平方残差来实现的。\n    对于 $x \\in \\partial \\Omega$，边界残差为 $R_{b}(x) = \\mathcal{B}(u)(x) - b(x)$。代入 $u = u_{\\text{phys}} + r_{\\theta}$：\n    $$\n    R_{b}(x) = \\mathcal{B}(u_{\\text{phys}} + r_{\\theta})(x) - b(x)\n    $$\n    边界损失项是加权平方边界残差在 $\\partial \\Omega$ 上的曲面积分，其中 $dS$ 是曲面元：\n    $$\n    \\mathcal{J}_{b}(\\theta) = \\int_{\\partial \\Omega} w_{b}(x) \\left( R_{b}(x) \\right)^2 dS = \\int_{\\partial \\Omega} w_{b}(x) \\left( \\mathcal{B}(u_{\\text{phys}} + r_{\\theta}) - b \\right)^2 dS\n    $$\n\n4.  **平滑度正则化项 ($\\mathcal{J}_{s}(\\theta)$)**：引入此项是为了防止过拟合，并确保残差场 $r_{\\theta}$ 表现良好（即不是病态的或过度振荡的）。一个标准的 Tikhonov 类型平滑度正则化项会惩罚函数的复杂度，通常通过其导数的范数来衡量。强制平滑度的最常见选择是惩罰函数梯度的 $L^2$ 范数的平方，这对应于其 Dirichlet 能量。将此应用于残差场 $r_{\\theta}$ 得到：\n    $$\n    \\mathcal{J}_{s}(\\theta) = \\int_{\\Omega} \\| \\nabla r_{\\theta}(x) \\|^2 dx\n    $$\n    此处，$\\| \\cdot \\|$ 表示标准欧几里得范数，因此 $\\| \\nabla r_{\\theta} \\|^2 = \\sum_{j=1}^{d} (\\frac{\\partial r_{\\theta}}{\\partial x_j})^2$。\n\n最后，我们通过加权求和将这四个分量组合成一个单一的目标函数，得到完整的训练目标 $\\mathcal{J}(\\theta)$:\n$$\n\\mathcal{J}(\\theta) = \\lambda_{p} \\int_{\\Omega} w_{p}(x) \\left( \\mathcal{L}(u_{\\text{phys}} + r_{\\theta}) - g \\right)^2 dx + \\lambda_{d} \\sum_{i=1}^{N_{d}} \\left( u_{\\text{phys}}(x_i) + r_{\\theta}(x_i) - y_i \\right)^2 + \\lambda_{b} \\int_{\\partial \\Omega} w_{b}(x) \\left( \\mathcal{B}(u_{\\text{phys}} + r_{\\theta}) - b \\right)^2 dS + \\lambda_{s} \\int_{\\Omega} \\| \\nabla r_{\\theta} \\|^2 dx\n$$\n此表达式为训练残差场 $r_{\\theta}$ 的参数 $\\theta$ 提供了一个完整的、有原则的基础。",
            "answer": "$$\n\\boxed{\\mathcal{J}(\\theta) = \\lambda_{p} \\int_{\\Omega} w_{p}(x) \\left( \\mathcal{L}(u_{\\text{phys}} + r_{\\theta}) - g \\right)^2 dx + \\lambda_{d} \\sum_{i=1}^{N_{d}} \\left( u_{\\text{phys}}(x_i) + r_{\\theta}(x_i) - y_i \\right)^2 + \\lambda_{b} \\int_{\\partial \\Omega} w_{b}(x) \\left( \\mathcal{B}(u_{\\text{phys}} + r_{\\theta}) - b \\right)^2 dS + \\lambda_{s} \\int_{\\Omega} \\| \\nabla r_{\\theta} \\|^2 dx}\n$$"
        },
        {
            "introduction": "虽然许多PINN通过损失函数“软性”地施加边界条件，但在设计上“硬性”地强制执行它们通常是更优越的策略。本练习将带你亲手实践这一强大技术，在一个简化的设定下，你将能够获得一个精确的解析解 。这有助于深入理解神经网络的函数形式与物理约束之间是如何相互作用的。",
            "id": "4235640",
            "problem": "一个一维无量纲热棒（一种典型的赛博物理系统）的数字孪生，其稳态由区间 $\\Omega = [0,1]$ 上的二阶微分方程 $-u''(x) = q(x)$ 控制，并带有狄利克雷边界条件 $u|_{\\partial\\Omega} = h$，即 $u(0) = h(0)$ 和 $u(1) = h(1)$。为了在物理信息神经网络（PINN）中精确地施加边界条件，考虑以下参数化形式\n$$\nu_{\\theta}(x) = h(x) + \\eta(x)\\,N_{\\theta}(x)\n$$\n其中 $\\eta:\\Omega\\to\\mathbb{R}$ 满足 $\\eta(0)=\\eta(1)=0$，而 $N_{\\theta}$ 是一个可训练函数。设 $h(x) = \\alpha + (\\beta - \\alpha)\\,x$，其中 $\\alpha,\\beta\\in\\mathbb{R}$ 是固定常数，选择 $\\eta(x)=x(1-x)$，并选择可训练部分为二次多项式 $N_{\\theta}(x) = \\theta_{0} + \\theta_{1}\\,x + \\theta_{2}\\,x^{2}$。\n\n从第一性原理的角度出发，首先确定 $u_{\\theta}$ 所属的函数空间，以及精确的狄利克雷约束如何改变可训练函数的容许空间。然后，通过要求强形式残差恒等于零来施加物理约束，\n$$\n-u_{\\theta}''(x) \\equiv q(x) \\quad \\text{for all } x \\in [0,1]\n$$\n其中无量纲源项为 $q(x) = 12\\,x^{2} + 6\\,x + 2$。\n\n仅使用上述规范和一维热传导的基本定律，确定使残差恒等于零的参数向量 $\\theta = (\\theta_{0}, \\theta_{1}, \\theta_{2})$。请以精确值的形式提供您对 $(\\theta_{0}, \\theta_{1}, \\theta_{2})$ 的最终答案。由于问题是在无量纲变量中提出的，因此答案没有物理单位。",
            "solution": "用户希望我按照以下步骤解决问题：\n1. 验证问题。\n2. 如果有效，提供详细的解答。\n3. 解答必须对所有数学符号使用 LaTeX。\n4. 最终答案必须采用特定的方框格式。\n\n### 步骤 1：提取已知条件\n- **控制微分方程：** $-u''(x) = q(x)$，在区间 $\\Omega = [0,1]$ 上。\n- **边界条件：** $u|_{\\partial\\Omega} = h$，意味着 $u(0) = h(0)$ 和 $u(1) = h(1)$。\n- **物理信息神经网络 (PINN) 拟设：** $u_{\\theta}(x) = h(x) + \\eta(x)\\,N_{\\theta}(x)$。\n- **边界条件函数：** $h(x) = \\alpha + (\\beta - \\alpha)\\,x$，其中 $\\alpha, \\beta \\in \\mathbb{R}$ 为固定常数。\n- **乘子函数：** $\\eta(x) = x(1-x)$，满足 $\\eta(0)=\\eta(1)=0$。\n- **可训练函数（神经网络代理）：** $N_{\\theta}(x) = \\theta_{0} + \\theta_{1}\\,x + \\theta_{2}\\,x^{2}$。\n- **源项：** $q(x) = 12\\,x^{2} + 6\\,x + 2$。\n- **物理约束：** 强形式残差必须恒等于零：$-u_{\\theta}''(x) \\equiv q(x)$，对所有 $x \\in [0,1]$。\n- **目标：** 确定参数向量 $\\theta = (\\theta_{0}, \\theta_{1}, \\theta_{2})$。\n\n### 步骤 2：使用提取的已知条件进行验证\n1.  **科学依据：** 该问题描述了一个一维稳态热传导（泊松）问题。使用特定函数形式 $u_{\\theta}(x) = h(x) + \\eta(x) N_{\\theta}(x)$ 来精确满足狄利克雷边界条件的方法是物理信息机器学习领域中一种标准且有效的技术。该问题在科学上和数学上都是合理的。\n2.  **适定性：** 所有必要的函数（$h(x)$、$\\eta(x)$、$N_{\\theta}(x)$、$q(x)$）都已明确定义。任务是通过强制一个多项式恒等式来求得多项式的系数。这会得到一个关于参数 $\\theta_i$ 的适定线性方程组，预计有唯一解。\n3.  **目标：** 问题以精确的数学语言陈述，没有主观或模糊的术语。\n4.  **完整性：** 问题是自洽的，并提供了求得唯一解所需的所有信息。\n5.  **无其他缺陷：** 该问题未违反任何其他无效性标准。它是一个用于说明构建 PINN 解的原理的典型例子。\n\n### 步骤 3：结论与行动\n问题有效。将提供完整解答。\n\n### 解答推导\n\n问题要求找到参数向量 $\\theta = (\\theta_{0}, \\theta_{1}, \\theta_{2})$，使得函数 $u_{\\theta}(x)$ 对所有 $x \\in [0,1]$ 都精确满足微分方程 $-u_{\\theta}''(x) = q(x)$。\n\n首先，我们讨论解空间的结构。为了使微分方程的强形式成立，解 $u(x)$ 必须至少是二次连续可微的，即 $u \\in C^2([0,1])$。问题指定了非齐次狄利克雷边界条件 $u(0)=h(0)=\\alpha$ 和 $u(1)=h(1)=\\beta$。所有此类函数的集合构成一个仿射空间。参数化 $u_{\\theta}(x) = h(x) + \\eta(x) N_{\\theta}(x)$ 巧妙地转化了这个问题。根据构造，$u_{\\theta}(x)$ 对任何 $N_{\\theta}(x)$ 的选择都会自动满足边界条件，因为 $\\eta(x)$ 在边界处为零：\n$u_{\\theta}(0) = h(0) + \\eta(0) N_{\\theta}(0) = h(0) + 0 = h(0) = \\alpha$。\n$u_{\\theta}(1) = h(1) + \\eta(1) N_{\\theta}(1) = h(1) + 0 = h(1) = \\beta$。\n因此，边界条件被硬编码到拟设中。任务简化为找到满足微分方程本身的特定 $N_{\\theta}(x)$。在这个问题中，$N_{\\theta}(x)$ 不是一个神经网络，而是一个简单的多项式，这允许我们得到精确的解析解。\n\n我们首先通过代入给定的 $h(x)$、$\\eta(x)$ 和 $N_{\\theta}(x)$ 的形式来构建 $u_{\\theta}(x)$ 的完整表达式：\n$$\nu_{\\theta}(x) = \\left( \\alpha + (\\beta - \\alpha)x \\right) + \\left( x(1-x) \\right) \\left( \\theta_{0} + \\theta_{1}x + \\theta_{2}x^{2} \\right)\n$$\n展开乘积 $\\eta(x)N_{\\theta}(x)$：\n$$\n\\eta(x)N_{\\theta}(x) = (x - x^2)(\\theta_{0} + \\theta_{1}x + \\theta_{2}x^{2}) = \\theta_{0}x + \\theta_{1}x^2 + \\theta_{2}x^3 - \\theta_{0}x^2 - \\theta_{1}x^3 - \\theta_{2}x^4\n$$\n按 $x$ 的幂次对各项进行分组：\n$$\n\\eta(x)N_{\\theta}(x) = \\theta_{0}x + (\\theta_{1}-\\theta_{0})x^2 + (\\theta_{2}-\\theta_{1})x^3 - \\theta_{2}x^4\n$$\n现在，将其代回到 $u_{\\theta}(x)$ 的表达式中：\n$$\nu_{\\theta}(x) = \\alpha + (\\beta - \\alpha)x + \\theta_{0}x + (\\theta_{1}-\\theta_{0})x^2 + (\\theta_{2}-\\theta_{1})x^3 - \\theta_{2}x^4\n$$\n这可以写成一个单一的多项式：\n$$\nu_{\\theta}(x) = \\alpha + (\\beta - \\alpha + \\theta_{0})x + (\\theta_{1}-\\theta_{0})x^2 + (\\theta_{2}-\\theta_{1})x^3 - \\theta_{2}x^4\n$$\n接下来，我们计算二阶导数 $u_{\\theta}''(x)$。首先，一阶导数 $u_{\\theta}'(x)$ 是：\n$$\nu_{\\theta}'(x) = \\frac{d}{dx} \\left( \\alpha + (\\beta - \\alpha + \\theta_{0})x + (\\theta_{1}-\\theta_{0})x^2 + (\\theta_{2}-\\theta_{1})x^3 - \\theta_{2}x^4 \\right)\n$$\n$$\nu_{\\theta}'(x) = (\\beta - \\alpha + \\theta_{0}) + 2(\\theta_{1}-\\theta_{0})x + 3(\\theta_{2}-\\theta_{1})x^2 - 4\\theta_{2}x^3\n$$\n现在，我们计算二阶导数 $u_{\\theta}''(x)$：\n$$\nu_{\\theta}''(x) = \\frac{d}{dx} \\left( (\\beta - \\alpha + \\theta_{0}) + 2(\\theta_{1}-\\theta_{0})x + 3(\\theta_{2}-\\theta_{1})x^2 - 4\\theta_{2}x^3 \\right)\n$$\n$$\nu_{\\theta}''(x) = 2(\\theta_{1}-\\theta_{0}) + 6(\\theta_{2}-\\theta_{1})x - 12\\theta_{2}x^2\n$$\n控制方程是 $-u_{\\theta}''(x) = q(x)$。我们来计算 $-u_{\\theta}''(x)$：\n$$\n-u_{\\theta}''(x) = - \\left( 2(\\theta_{1}-\\theta_{0}) + 6(\\theta_{2}-\\theta_{1})x - 12\\theta_{2}x^2 \\right)\n$$\n$$\n-u_{\\theta}''(x) = (2\\theta_{0}-2\\theta_{1}) + (6\\theta_{1}-6\\theta_{2})x + 12\\theta_{2}x^2\n$$\n问题要求这个表达式恒等于源项 $q(x) = 12x^2 + 6x + 2$。\n$$\n(2\\theta_{0}-2\\theta_{1}) + (6\\theta_{1}-6\\theta_{2})x + 12\\theta_{2}x^2 \\equiv 2 + 6x + 12x^2\n$$\n要使两个多项式对所有 $x$ 都恒等，对应 $x$ 幂次的系数必须相等。这给了我们一个关于三个未知参数 $\\theta_0, \\theta_1, \\theta_2$ 的三元线性方程组。\n\n令 $x^2$ 的系数相等：\n$$\n12\\theta_{2} = 12 \\implies \\theta_{2} = 1\n$$\n令 $x^1$ 的系数相等：\n$$\n6\\theta_{1}-6\\theta_{2} = 6\n$$\n将值 $\\theta_{2}=1$ 代入此方程：\n$$\n6\\theta_{1}-6(1) = 6 \\implies 6\\theta_{1} = 12 \\implies \\theta_{1} = 2\n$$\n令常数项（$x^0$ 的系数）相等：\n$$\n2\\theta_{0}-2\\theta_{1} = 2\n$$\n将值 $\\theta_{1}=2$ 代入此方程：\n$$\n2\\theta_{0}-2(2) = 2 \\implies 2\\theta_{0} - 4 = 2 \\implies 2\\theta_{0} = 6 \\implies \\theta_{0} = 3\n$$\n因此，我们找到了使残差恒等于零的唯一参数向量 $\\theta = (\\theta_{0}, \\theta_{1}, \\theta_{2})$。正如预期的那样，这些参数与边界条件常数 $\\alpha$ 和 $\\beta$ 无关，因为线性函数 $h(x)$ 的二阶导数为零，因此对残差 $-u_{\\theta}''(x)$ 没有贡献。\n\n参数向量是 $(\\theta_{0}, \\theta_{1}, \\theta_{2}) = (3, 2, 1)$。",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 3 & 2 & 1 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "现实世界中的数字孪生不仅需要做出预测，还必须报告其预测的置信度。本练习将带你从确定性PINN模型迈向贝叶斯框架，其目标是推断解的完整概率分布。你将学习如何将贝叶斯线性回归与物理残差相结合，以计算后验预测分布，从而同时获得预测值及其不确定性 。",
            "id": "4235615",
            "problem": "考虑一个长度为 $1\\,\\mathrm{m}$ 的杆的一维稳态热传导模型，其热导率为 $k$（单位：$\\mathrm{W}/(\\mathrm{m}\\cdot\\mathrm{K})$），内部体积热生成率为 $q(x)$（单位：$\\mathrm{W}/\\mathrm{m}^3$）。其控制方程为二阶常微分方程 (ODE) $-k\\,\\frac{d^2 u}{dx^2}(x) = q(x)$，适用于 $x \\in (0,1)$，并带有狄利克雷边界条件 $u(0) = 0\\,\\mathrm{K}$ 和 $u(1) = 0\\,\\mathrm{K}$。在物理信息神经网络 (PINN) 中，通过一个作用于由 $\\theta$ 参数化的试解 $u_\\theta(x)$ 的算子 $\\mathcal{L}$，在一组配置点 $\\{x_i\\}_{i=1}^N$ 上强制执行基于物理的残差。似然函数使用残差 $\\mathcal{L}u_\\theta(x_i) - g(x_i)$ 来定义，其中 $g(x) = q(x)$，并处于异方差噪声（噪声方差取决于 $x$）下。您将构建一个贝叶斯 PINN，该 PINN 使用基于满足边界条件的基函数的线性参数化 $u_\\theta(x)$，并计算在指定传感器位置的后验预测分布。\n\n从以下基本原理出发：(i) 稳态热方程 $-k\\,\\frac{d^2 u}{dx^2}(x) = q(x)$ 和物理残差的定义 $\\mathcal{R}(x) := \\mathcal{L}u_\\theta(x) - g(x)$，其中 $\\mathcal{L} = -k\\,\\frac{d^2}{dx^2}$；(ii) 具有高斯先验和高斯似然的贝叶斯线性回归模型；以及 (iii) 高斯分布在线性变换下的性质。\n\n使用满足边界条件的线性展开对解进行参数化：$u_\\theta(x) = \\sum_{j=1}^{K} \\theta_j\\,b_j(x)$，其中 $b_j(x) = x^{n_j} - x^{n_j+1}$ 且 $n_j = j+1$。这些基函数对于所有 $j$ 都满足 $b_j(0) = 0$ 和 $b_j(1) = 0$，从而确保 $u_\\theta(0) = 0$ 和 $u_\\theta(1) = 0$。作用于 $u_\\theta$ 在点 $x$ 处的算子为 $\\mathcal{L}u_\\theta(x) = -k \\sum_{j=1}^{K} \\theta_j\\,b_j''(x)$，其中 $b_j''(x) = n_j(n_j-1)\\,x^{n_j-2} - (n_j+1)n_j\\,x^{n_j-1}$。将异方差残差噪声的方差定义为 $\\sigma^2(x) = \\sigma_0^2\\,(1 + \\beta x^2)$，其中 $\\sigma_0 > 0$ 和 $\\beta \\ge 0$ 是给定常数。假设参数服从零均值高斯先验 $\\theta \\sim \\mathcal{N}(0, \\lambda^{-1} I_K)$，其精度为 $\\lambda > 0$。\n\n在配置点 $\\{x_i\\}_{i=1}^N$ 处，定义线性系统 $\\mathbf{y} = A\\theta + \\varepsilon$，其中 $\\mathbf{y}_i = g(x_i)$，$A_{i,j} = -k\\,b_j''(x_i)$，且 $\\varepsilon \\sim \\mathcal{N}(0, \\Sigma)$，其中 $\\Sigma = \\mathrm{diag}(\\sigma^2(x_1), \\dots, \\sigma^2(x_N))$。据此，推导 $\\theta$ 的后验分布，然后计算在给定传感器位置 $\\{s_\\ell\\}_{\\ell=1}^M$ 处 $u_\\theta(x)$ 的后验预测分布。每个后验预测均值必须以 $\\mathrm{K}$ 为单位表示，每个后验预测方差必须以 $\\mathrm{K}^2$ 为单位表示。\n\n您的任务是实现一个完整、可运行的程序，该程序：\n- 使用指定的 $k$、$K$、配置点数 $N$、源项 $q(x)$ 和异方差方差函数 $\\sigma^2(x)$ 为每个测试用例构建矩阵 $A$ 和向量 $\\mathbf{y}$。\n- 在高斯先验和高斯似然模型下，计算 $\\theta$ 的后验均值向量 $m$ 和后验协方差矩阵 $S$。\n- 对于每个传感器位置 $s_\\ell$，计算后验预测均值 $\\mu_\\ell = b(s_\\ell)^\\top m$ 和方差 $\\nu_\\ell = b(s_\\ell)^\\top S\\,b(s_\\ell)$，其中 $b(s_\\ell) = [b_1(s_\\ell), \\dots, b_K(s_\\ell)]^\\top$。\n- 将每个测试用例的数值结果汇总为一个浮点数的一维列表，其顺序为 $[\\mu_1, \\nu_1, \\mu_2, \\nu_2, \\dots, \\mu_M, \\nu_M]$。\n\n使用以下源项 $q(x) = Q_0\\,\\sin(\\pi x)$，其中 $Q_0$ 是特定于用例的常数，单位为 $\\mathrm{W}/\\mathrm{m}^3$，并使用均匀分布的内部配置点 $x_i = \\frac{i}{N+1}$，其中 $i = 1, \\dots, N$。\n\n测试套件：\n- 用例 A (正常路径)：$K=3$, $N=25$, $k=10\\,\\mathrm{W}/(\\mathrm{m}\\cdot\\mathrm{K})$, $Q_0=100\\,\\mathrm{W}/\\mathrm{m}^3$, $\\sigma_0=0.3$, $\\beta=2$, $\\lambda=10$, 传感器位于 $[0.2, 0.5, 0.8]$。\n- 用例 B (边界情况，高噪声和少量点)：$K=5$, $N=10$, $k=5\\,\\mathrm{W}/(\\mathrm{m}\\cdot\\mathrm{K})$, $Q_0=50\\,\\mathrm{W}/\\mathrm{m}^3$, $\\sigma_0=1.0$, $\\beta=5$, $\\lambda=1$, 传感器位于 $[0.1, 0.9]$。\n- 用例 C (边界情况，强物理约束和多点)：$K=4$, $N=50$, $k=15\\,\\mathrm{W}/(\\mathrm{m}\\cdot\\mathrm{K})$, $Q_0=120\\,\\mathrm{W}/\\mathrm{m}^3$, $\\sigma_0=0.1$, $\\beta=1$, $\\lambda=50$, 传感器位于 $[0.33, 0.66]$。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，每个元素对应一个测试用例。对于每个测试用例，按 $[\\mu_1, \\nu_1, \\mu_2, \\nu_2, \\dots]$ 的顺序输出一个浮点数的一维列表，其中 $\\mu_\\ell$ 的单位是 $\\mathrm{K}$，$\\nu_\\ell$ 的单位是 $\\mathrm{K}^2$。例如，顶层输出必须类似于 $[[\\mu_1,\\nu_1,\\mu_2,\\nu_2], [\\dots], [\\dots]]$。",
            "solution": "该问题定义明确且具有科学依据，为一维稳态热传导问题构建贝叶斯物理信息神经网络 (PINN) 提供了完整的规范。我们将进行完整的推导和求解。\n\n问题在于确定温度场 $u(x)$ 的后验预测分布，该温度场由微分方程 $-k\\,\\frac{d^2 u}{dx^2}(x) = q(x)$ 在定义域 $x \\in (0,1)$ 上控制，并满足狄利克雷边界条件 $u(0) = 0\\,\\mathrm{K}$ 和 $u(1) = 0\\,\\mathrm{K}$。这一过程在贝叶斯线性回归框架内完成。\n\n首先，我们定义由权重向量 $\\theta = [\\theta_1, \\dots, \\theta_K]^\\top$ 参数化的试解 $u_\\theta(x)$。解表示为固有满足边界条件的基函数的线性组合：\n$$\nu_\\theta(x) = \\sum_{j=1}^{K} \\theta_j\\,b_j(x)\n$$\n基函数由 $b_j(x) = x^{n_j} - x^{n_j+1}$ 给出，其中 $n_j = j+1$。对于任何 $j \\geq 1$，我们有 $n_j \\geq 2$，所以 $b_j(0) = 0^{n_j} - 0^{n_j+1} = 0$ 且 $b_j(1) = 1^{n_j} - 1^{n_j+1} = 1 - 1 = 0$，因此对 $\\theta$ 的任何选择都满足边界条件。\n\nPINN 方法的核心是强制执行物理控制方程。我们定义一个基于物理的残差算子 $\\mathcal{L} = -k\\,\\frac{d^2}{dx^2}$。将此算子应用于我们的试解可得：\n$$\n\\mathcal{L}u_\\theta(x) = -k\\,\\frac{d^2}{dx^2} \\left( \\sum_{j=1}^{K} \\theta_j\\,b_j(x) \\right) = \\sum_{j=1}^{K} \\theta_j \\left( -k\\,b_j''(x) \\right)\n$$\n其中 $b_j''(x) = \\frac{d^2}{dx^2}(x^{n_j} - x^{n_j+1}) = n_j(n_j-1)x^{n_j-2} - (n_j+1)n_j x^{n_j-1}$。\n\n物理残差定义为差异 $\\mathcal{R}(x) = \\mathcal{L}u_\\theta(x) - q(x)$。模型假设控制方程在一组 $N$ 个配置点 $\\{x_i\\}_{i=1}^N$ 上在一定随机噪声下成立，即 $\\mathcal{L}u_\\theta(x_i) = q(x_i) + \\varepsilon_i$。这就构成了一个线性方程组。设 $\\mathbf{y}$ 为源项评估值的向量，其中 $\\mathbf{y}_i = q(x_i)$，并设 $A$ 为设计矩阵，其中 $A_{ij} = -k\\,b_j''(x_i)$。该系统可以写成向量形式：\n$$\n\\mathbf{y} = A\\theta + \\varepsilon\n$$\n其中 $\\mathbf{y} \\in \\mathbb{R}^N$, $A \\in \\mathbb{R}^{N \\times K}$, $\\theta \\in \\mathbb{R}^K$，且 $\\varepsilon \\in \\mathbb{R}^N$。噪声向量 $\\varepsilon$ 假设服从零均值高斯分布 $\\varepsilon \\sim \\mathcal{N}(0, \\Sigma)$，其对角协方差矩阵 $\\Sigma$ 反映了异方差（即位置相关的）和不相关的噪声。对角线元素由指定的方差函数给出 $\\Sigma_{ii} = \\sigma^2(x_i) = \\sigma_0^2(1 + \\beta x_i^2)$。\n\n这个设定定义了给定参数 $\\theta$ 时数据 $\\mathbf{y}$ 的似然分布：\n$$\np(\\mathbf{y}|\\theta) = \\mathcal{N}(\\mathbf{y} | A\\theta, \\Sigma) \\propto \\exp\\left(-\\frac{1}{2}(\\mathbf{y} - A\\theta)^\\top \\Sigma^{-1} (\\mathbf{y} - A\\theta)\\right)\n$$\n\n对于贝叶斯处理，我们引入参数 $\\theta$ 的先验分布。指定了一个零均值高斯先验 $p(\\theta) = \\mathcal{N}(\\theta | 0, \\lambda^{-1}I_K)$，其中 $\\lambda > 0$ 是精度参数，$I_K$ 是 $K \\times K$ 的单位矩阵。这种先验鼓励较小的参数值，起到一种正则化的作用。\n\n根据贝叶斯定理，$\\theta$ 的后验分布与似然和先验的乘积成正比：$p(\\theta|\\mathbf{y}) \\propto p(\\mathbf{y}|\\theta)p(\\theta)$。由于两个分布都是高斯分布，后验分布也是高斯分布 $p(\\theta|\\mathbf{y}) = \\mathcal{N}(\\theta|m, S)$，其中 $m$ 是后验均值，$S$ 是后验协方差。通过组合似然和先验的指数部分并对 $\\theta$ 进行配方，我们得到后验参数：\n后验精度矩阵 $S^{-1}$ 是先验精度与数据精度的和：\n$$\nS^{-1} = A^\\top \\Sigma^{-1} A + \\lambda I_K\n$$\n后验协方差是其逆矩阵：\n$$\nS = (A^\\top \\Sigma^{-1} A + \\lambda I_K)^{-1}\n$$\n后验均值向量 $m$ 由下式给出：\n$$\nm = S (A^\\top \\Sigma^{-1} \\mathbf{y})\n$$\n\n在建立了 $\\theta$ 的后验分布之后，我们可以计算在任何新的传感器位置 $s$ 处温度 $u(s)$ 的后验预测分布。在 $s$ 处的预测是 $\\theta$ 的线性函数：\n$$\nu_\\theta(s) = \\sum_{j=1}^K \\theta_j b_j(s) = b(s)^\\top \\theta\n$$\n其中 $b(s) = [b_1(s), \\dots, b_K(s)]^\\top$ 是在 $s$ 处求值的基函数向量。由于 $u_\\theta(s)$ 是高斯分布随机变量 $\\theta$ 的线性变换，因此得到的 $u_\\theta(s)$ 的后验预测分布也是高斯分布。其均值 $\\mu_s$ 和方差 $\\nu_s$ 可通过随机变量线性变换的性质求得：\n后验预测均值为：\n$$\n\\mu_s = \\mathbb{E}[u_\\theta(s) | \\mathbf{y}] = b(s)^\\top \\mathbb{E}[\\theta | \\mathbf{y}] = b(s)^\\top m\n$$\n后验预测方差为：\n$$\n\\nu_s = \\mathrm{Var}[u_\\theta(s) | \\mathbf{y}] = b(s)^\\top \\mathrm{Var}[\\theta | \\mathbf{y}] b(s) = b(s)^\\top S b(s)\n$$\n对于测试用例中的每个传感器位置 $s_\\ell$，我们将使用这些公式计算 $\\mu_\\ell$ 和 $\\nu_\\ell$。\n\n每个测试用例的计算过程如下：\n1.  通过生成 $N$ 个配置点 $x_i = \\frac{i}{N+1}$（其中 $i=1, \\dots, N$）来离散化定义域。\n2.  构建 $N \\times K$ 矩阵 $A$（其中 $A_{ij} = -k\\,b_j''(x_i)$）和 $N \\times 1$ 向量 $\\mathbf{y}$（其中 $\\mathbf{y}_i = Q_0\\,\\sin(\\pi x_i)$）。\n3.  构建对角逆协方差矩阵 $\\Sigma^{-1}$，其对角线元素为 $1/\\sigma^2(x_i) = 1/(\\sigma_0^2(1 + \\beta x_i^2))$。\n4.  计算后验协方差 $S = (A^\\top \\Sigma^{-1} A + \\lambda I_K)^{-1}$ 和后验均值 $m = S (A^\\top \\Sigma^{-1} \\mathbf{y})$。\n5.  对于每个传感器位置 $s_\\ell$，构建基向量 $b(s_\\ell)$ 并计算预测均值 $\\mu_\\ell = b(s_\\ell)^\\top m$ 和方差 $\\nu_\\ell = b(s_\\ell)^\\top S b(s_\\ell)$。\n6.  将结果 $[\\mu_1, \\nu_1, \\mu_2, \\nu_2, \\dots]$ 汇总成该测试用例的一个列表。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Bayesian PINN problem for the given test cases.\n    \"\"\"\n    \n    test_cases = [\n        # Case A: Happy path\n        {'K': 3, 'N': 25, 'k': 10.0, 'Q0': 100.0, 'sigma0': 0.3, 'beta': 2.0, 'lambda_': 10.0, 'sensors': [0.2, 0.5, 0.8]},\n        # Case B: Edge, high noise and few points\n        {'K': 5, 'N': 10, 'k': 5.0, 'Q0': 50.0, 'sigma0': 1.0, 'beta': 5.0, 'lambda_': 1.0, 'sensors': [0.1, 0.9]},\n        # Case C: Edge, strong physics, many points\n        {'K': 4, 'N': 50, 'k': 15.0, 'Q0': 120.0, 'sigma0': 0.1, 'beta': 1.0, 'lambda_': 50.0, 'sensors': [0.33, 0.66]},\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        K = case['K']\n        N = case['N']\n        k = case['k']\n        Q0 = case['Q0']\n        sigma0 = case['sigma0']\n        beta = case['beta']\n        lambda_ = case['lambda_']\n        sensors = case['sensors']\n\n        # 1. Generate collocation points\n        x_colloc = np.linspace(0, 1, N + 2)[1:-1] # Equally spaced interior points\n\n        # Define basis function and its second derivative\n        # n_j = j + 1\n        # b_j(x) = x^n_j - x^(n_j+1)\n        # b_j''(x) = n_j(n_j-1)x^(n_j-2) - (n_j+1)n_jx^(n_j-1)\n        \n        def basis_b(x, j):\n            n_j = j + 1\n            return x**n_j - x**(n_j + 1)\n\n        def basis_b_dd(x, j):\n            n_j = j + 1\n            term1 = 0\n            # For j=0 (n_j=1), n_j-2 = -1, which is ill-defined at x=0. But j starts from 1, so n_j >= 2.\n            # a*x^0 = a.\n            if n_j - 2 == 0:\n                term1 = n_j * (n_j - 1)\n            elif n_j - 2 > 0:\n                term1 = n_j * (n_j - 1) * x**(n_j - 2)\n\n            term2 = (n_j + 1) * n_j * x**(n_j - 1)\n            return term1 - term2\n\n        # 2. Construct matrix A and vector y\n        A = np.zeros((N, K))\n        y = np.zeros(N)\n\n        for i in range(N):\n            xi = x_colloc[i]\n            y[i] = Q0 * np.sin(np.pi * xi)\n            for j_idx in range(K):\n                j = j_idx + 1 # Basis function index from 1 to K\n                A[i, j_idx] = -k * basis_b_dd(xi, j)\n\n        # 3. Construct inverse noise covariance diagonal\n        sigma_sq = sigma0**2 * (1 + beta * x_colloc**2)\n        sigma_sq_inv_diag = 1.0 / sigma_sq\n        \n        # 4. Compute posterior mean and covariance\n        # S_inv = A.T @ Sigma_inv @ A + lambda * I\n        # Efficient computation without forming diagonal matrix Sigma_inv\n        S_inv = (A.T * sigma_sq_inv_diag) @ A + lambda_ * np.identity(K)\n        S = np.linalg.inv(S_inv)\n        \n        # m = S @ A.T @ Sigma_inv @ y\n        m = S @ (A.T @ (sigma_sq_inv_diag * y))\n\n        # 5. Compute posterior predictive mean and variance at sensor locations\n        case_results = []\n        for s_ell in sensors:\n            # Construct basis vector b(s_ell)\n            b_s = np.zeros(K)\n            for j_idx in range(K):\n                j = j_idx + 1\n                b_s[j_idx] = basis_b(s_ell, j)\n            \n            # Predictive mean\n            mu_ell = b_s @ m\n            \n            # Predictive variance\n            nu_ell = b_s.T @ S @ b_s\n            \n            case_results.extend([mu_ell, nu_ell])\n        \n        all_results.append(case_results)\n\n    # Final print statement in the exact required format.\n    # Convert each inner list to its string representation\n    # e.g., [1.23, 4.56] -> \"[1.23, 4.56]\"\n    # Then join these strings with commas.\n    str_results = [str(res) for res in all_results]\n    print(f\"[{','.join(str_results)}]\")\n\nsolve()\n```"
        }
    ]
}