{
    "hands_on_practices": [
        {
            "introduction": "Before constructing a Physics-Informed Neural Network (PINN), it is crucial to formulate the problem in a way that is amenable to gradient-based optimization. This practice explores nondimensionalization, a standard technique in scientific computing, to balance the terms in a partial differential equation. Properly scaling the problem can significantly improve training stability and convergence, as we will demonstrate by applying this technique to the viscous Burgers' equation, a classic model for nonlinear wave propagation and diffusion .",
            "id": "4235875",
            "problem": "A digital twin of a one-dimensional viscous transport process in a cyber-physical system is modeled by the viscous Burgers equation, which at the physical level reads $u_{t} + u u_{x} - \\nu u_{xx} = 0$ on a spatial interval of length $L$ and a time horizon of interest. A Physics-Informed Neural Network (PINN) is to be trained on this model by enforcing the partial differential equation residual at collocation points together with initial and boundary data. To promote robust optimization across different operating regimes, you will nondimensionalize the model.\n\nStarting from the physical equation and the core definitions of characteristic scales for space, time, and velocity magnitudes, introduce dimensionless variables by setting $x = L \\,\\xi$, $t = T \\,\\tau$, and $u = U \\,v$, where $L$ and $U$ are fixed characteristic length and velocity scales, and $T$ is a characteristic time scale to be determined. Derive the nondimensional form of the equation for $v(\\xi,\\tau)$ using only these scalings and the chain rule. Choose $T$ so that the nondimensional equation has order-one coefficients for both the temporal derivative and the convective nonlinearity. Identify the single dimensionless group multiplying the diffusion term that controls the balance between convection and diffusion.\n\nThen, using first principles about gradient-based optimization and conditioning, provide a concise argument for why training a Physics-Informed Neural Network (PINN) on the nondimensional form can improve optimization stability and convergence when implemented in the digital twin.\n\nYour final reported answer must be the analytic expression for the Reynolds number associated with this nondimensionalization in terms of $U$, $L$, and $\\nu$. Do not include units. If you choose to perform any intermediate numerical evaluations, do not report them. The final answer must be a single closed-form expression.",
            "solution": "The problem requires the nondimensionalization of the one-dimensional viscous Burgers equation, an analysis of the resulting dimensionless group, and an argument for why this procedure benefits the training of a Physics-Informed Neural Network (PINN).\n\nThe physical form of the viscous Burgers equation is given as:\n$$\nu_{t} + u u_{x} - \\nu u_{xx} = 0\n$$\nwhere $u(x,t)$ is the velocity, $\\nu$ is the kinematic viscosity, $u_t = \\frac{\\partial u}{\\partial t}$, $u_x = \\frac{\\partial u}{\\partial x}$, and $u_{xx} = \\frac{\\partial^2 u}{\\partial x^2}$.\n\nThe problem specifies the introduction of dimensionless variables $\\xi$, $\\tau$, and $v$ through the following scaling relations:\n$$\nx = L \\,\\xi, \\quad t = T \\,\\tau, \\quad u = U \\,v\n$$\nHere, $L$ is a characteristic length scale and $U$ is a characteristic velocity scale. The characteristic time scale $T$ is to be determined. The function $v$ is the dimensionless velocity, which depends on the dimensionless space and time variables, i.e., $v = v(\\xi, \\tau)$.\n\nTo transform the partial differential equation (PDE), we must express the partial derivatives of $u$ with respect to $x$ and $t$ in terms of the partial derivatives of $v$ with respect to $\\xi$ and $\\tau$. We use the chain rule for this transformation.\n\nFirst, we find the temporal derivative, $u_t$:\n$$\nu_{t} = \\frac{\\partial u}{\\partial t} = \\frac{\\partial (Uv)}{\\partial t} = U \\frac{\\partial v}{\\partial t} = U \\frac{\\partial v}{\\partial \\tau} \\frac{\\partial \\tau}{\\partial t}\n$$\nFrom the relation $t = T\\tau$, we have $\\frac{\\partial \\tau}{\\partial t} = \\frac{1}{T}$. Therefore, the temporal derivative becomes:\n$$\nu_{t} = \\frac{U}{T} \\frac{\\partial v}{\\partial \\tau} = \\frac{U}{T} v_{\\tau}\n$$\n\nNext, we find the first spatial derivative, $u_x$:\n$$\nu_{x} = \\frac{\\partial u}{\\partial x} = \\frac{\\partial (Uv)}{\\partial x} = U \\frac{\\partial v}{\\partial x} = U \\frac{\\partial v}{\\partial \\xi} \\frac{\\partial \\xi}{\\partial x}\n$$\nFrom the relation $x = L\\xi$, we have $\\frac{\\partial \\xi}{\\partial x} = \\frac{1}{L}$. Thus, the first spatial derivative is:\n$$\nu_{x} = \\frac{U}{L} \\frac{\\partial v}{\\partial \\xi} = \\frac{U}{L} v_{\\xi}\n$$\n\nFinally, we find the second spatial derivative, $u_{xx}$:\n$$\nu_{xx} = \\frac{\\partial^2 u}{\\partial x^2} = \\frac{\\partial}{\\partial x} \\left( \\frac{\\partial u}{\\partial x} \\right) = \\frac{\\partial}{\\partial x} \\left( \\frac{U}{L} v_{\\xi} \\right) = \\frac{U}{L} \\frac{\\partial v_{\\xi}}{\\partial x} = \\frac{U}{L} \\left( \\frac{\\partial v_{\\xi}}{\\partial \\xi} \\frac{\\partial \\xi}{\\partial x} \\right) = \\frac{U}{L} \\left( v_{\\xi\\xi} \\frac{1}{L} \\right)\n$$\nThis simplifies to:\n$$\nu_{xx} = \\frac{U}{L^2} v_{\\xi\\xi}\n$$\n\nNow, we substitute these expressions for the derivatives and for $u$ itself back into the original viscous Burgers equation:\n$$\n\\left( \\frac{U}{T} v_{\\tau} \\right) + (Uv) \\left( \\frac{U}{L} v_{\\xi} \\right) - \\nu \\left( \\frac{U}{L^2} v_{\\xi\\xi} \\right) = 0\n$$\nCombining the coefficients, we get:\n$$\n\\frac{U}{T} v_{\\tau} + \\frac{U^2}{L} v v_{\\xi} - \\frac{\\nu U}{L^2} v_{\\xi\\xi} = 0\n$$\nTo obtain the nondimensional form where coefficients are of order-one, we can divide the entire equation by one of the coefficients. A standard choice is to divide by the coefficient of the nonlinear convective term, $\\frac{U^2}{L}$, to make its coefficient exactly $1$:\n$$\n\\left( \\frac{U}{T} \\frac{L}{U^2} \\right) v_{\\tau} + \\left( \\frac{U^2}{L} \\frac{L}{U^2} \\right) v v_{\\xi} - \\left( \\frac{\\nu U}{L^2} \\frac{L}{U^2} \\right) v_{\\xi\\xi} = 0\n$$\nSimplifying the coefficients yields:\n$$\n\\left( \\frac{L}{UT} \\right) v_{\\tau} + v v_{\\xi} - \\left( \\frac{\\nu}{UL} \\right) v_{\\xi\\xi} = 0\n$$\nThe problem requires choosing the characteristic time scale $T$ such that the coefficients of the temporal derivative ($v_\\tau$) and the convective nonlinearity ($v v_\\xi$) are both of order-one. We have already set the coefficient of $v v_\\xi$ to $1$. To set the coefficient of $v_\\tau$ also to $1$, we must have:\n$$\n\\frac{L}{UT} = 1 \\implies T = \\frac{L}{U}\n$$\nThis choice for $T$ represents the convective time scale: the time it takes for a fluid parcel to travel the characteristic length $L$ at the characteristic velocity $U$.\n\nSubstituting this expression for $T$ back into the scaled equation, we obtain the final nondimensional form of the viscous Burgers equation:\n$$\nv_{\\tau} + v v_{\\xi} - \\left( \\frac{\\nu}{UL} \\right) v_{\\xi\\xi} = 0\n$$\nThe single dimensionless group multiplying the diffusion term ($v_{\\xi\\xi}$) is $\\frac{\\nu}{UL}$. This group is the reciprocal of the Reynolds number, a fundamental dimensionless parameter in fluid mechanics that quantifies the ratio of inertial forces to viscous forces. The Reynolds number, denoted $Re$, is defined as:\n$$\nRe = \\frac{\\text{inertial forces}}{\\text{viscous forces}} \\propto \\frac{U^2/L}{\\nu U/L^2} = \\frac{UL}{\\nu}\n$$\nThus, the dimensionless coefficient is $\\frac{1}{Re}$, and the equation is:\n$$\nv_{\\tau} + v v_{\\xi} - \\frac{1}{Re} v_{\\xi\\xi} = 0\n$$\nThe expression for the Reynolds number is therefore $\\frac{UL}{\\nu}$.\n\nThe argument for why training a PINN on the nondimensional form is advantageous relates to the numerical conditioning of the optimization problem. A PINN is trained by minimizing a loss function that includes the residual of the governing PDE. For the original physical equation, the PDE residual is $r = u_t + u u_x - \\nu u_{xx}$. The loss is typically the mean squared error of this residual over a set of collocation points, $\\mathcal{L}_{PDE} = \\langle (u_t + u u_x - \\nu u_{xx})^2 \\rangle$.\n\nIn a physical system, the magnitudes of the terms $u_t$, $u u_x$, and $\\nu u_{xx}$ can vary by many orders of magnitude. For instance, in a high-speed, low-viscosity flow, the convective term $u u_x$ may be far larger than the viscous term $\\nu u_{xx}$. When a gradient-based optimizer (e.g., Adam) minimizes the loss function, the gradients of the loss with respect to the neural network's parameters (weights and biases) will be dominated by the largest terms in the residual. This creates an ill-conditioned optimization landscape with steep \"canyons\" and flat \"plateaus,\" making it difficult for the optimizer to converge to a good solution. The network may learn to satisfy the dominant physics (e.g., convection) while largely ignoring the sub-dominant physics (e.g., diffusion), leading to an inaccurate model.\n\nNondimensionalization acts as a form of preconditioning. By scaling variables such that $v, \\xi, \\tau$ are all of order-one, we ensure that the terms in the nondimensional residual, $r' = v_\\tau + v v_\\xi - \\frac{1}{Re} v_{\\xi\\xi}$, are of comparable magnitude (unless $Re$ is extremely large or small, in which case the physics enters a distinct asymptotic regime). For a moderate $Re$, the coefficients are all of order-one. This balancing of terms leads to a better-conditioned loss landscape. The gradients originating from each physical term (temporal change, convection, diffusion) are more evenly scaled. As a result, the optimizer can make more uniform progress in minimizing the contributions from all physical effects simultaneously, leading to more stable and efficient training, and ultimately, a more accurate PINN-based digital twin.",
            "answer": "$$\n\\boxed{\\frac{UL}{\\nu}}\n$$"
        },
        {
            "introduction": "The core of a PINN is its ability to enforce physical laws directly within the loss function. This exercise demonstrates how to translate the governing equations of 2D linear elasticity into a PDE residual that the network must minimize. By manually calculating the required derivatives for a simple neural network ansatz, you will gain a concrete understanding of the mechanics behind how automatic differentiation computes the physics-informed loss term .",
            "id": "2668927",
            "problem": "Consider a two-dimensional, small-strain, linear elastic solid in a quasi-static setting with body force per unit volume $\\mathbf{b} = (b_x, b_y)$. Let the displacement field be $\\mathbf{u}(x,y) = (u_x(x,y), u_y(x,y))$. Starting only from the balance of linear momentum, the small-strain kinematic relation, and the linear isotropic constitutive law in terms of the Lamé parameters $(\\lambda, \\mu)$, do the following:\n\n1) Write the constitutive law for the Cauchy stress tensor $\\boldsymbol{\\sigma}$ in terms of the strain tensor $\\boldsymbol{\\varepsilon}$ and $(\\lambda,\\mu)$, and define the small-strain tensor in terms of the displacement field. Express all objects in Cartesian coordinates.\n\n2) Using your definitions, expand the interior equilibrium residual components that a Physics-Informed Neural Network (PINN) would enforce at interior collocation points, namely the two components of $\\nabla \\cdot \\boldsymbol{\\sigma} + \\mathbf{b}$, explicitly in Cartesian coordinates in terms of spatial derivatives of $u_x$ and $u_y$ and the Lamé parameters $(\\lambda,\\mu)$.\n\n3) Now consider a one-hidden-neuron neural network ansatz for the displacement field with hyperbolic tangent activation,\n$$\nz(x,y) = w_1 x + w_2 y + b_1,\\quad \\mathbf{u}(x,y) = \\mathbf{W}_2 \\,\\tanh\\!\\big(z(x,y)\\big) + \\mathbf{b}_2,\n$$\nwith $\\mathbf{W}_2 \\in \\mathbb{R}^{2 \\times 1}$ and $\\mathbf{b}_2 \\in \\mathbb{R}^{2}$. Take the specific parameters\n$$\nw_1 = 1,\\quad w_2 = 2,\\quad b_1 = 0,\\quad \\mathbf{W}_2 = \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix},\\quad \\mathbf{b}_2 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix},\n$$\nand assume zero body force $\\mathbf{b}=\\mathbf{0}$. Evaluate the two components of the interior equilibrium residual vector at the point $(x_0,y_0)=(1,0)$.\n\nAssume a nondimensionalized formulation so that all quantities are dimensionless. Express the final result as a single row vector containing the two residual components, written as a closed-form analytic expression. Do not approximate or round any constants; leave hyperbolic functions unevaluated (for example, write $\\tanh(1)$, $\\cosh(1)$ explicitly).",
            "solution": "The problem is validated as well-posed, scientifically grounded, and containing all necessary information for a unique solution. We proceed with the derivation.\n\nThe problem requires a step-by-step derivation, starting from fundamental principles of continuum mechanics, to evaluate the equilibrium residual for a given neural network ansatz for the displacement field.\n\nFirst, we address Task 1: defining the relevant kinematic and constitutive relations in Cartesian coordinates.\n\nIn a two-dimensional domain with coordinates $(x,y)$, the displacement field is $\\mathbf{u}(x,y) = (u_x(x,y), u_y(x,y))$. The small-strain (or infinitesimal strain) tensor $\\boldsymbol{\\varepsilon}$ is defined in terms of the gradient of the displacement field as:\n$$\n\\boldsymbol{\\varepsilon} = \\frac{1}{2} \\left[ \\nabla \\mathbf{u} + (\\nabla \\mathbf{u})^T \\right]\n$$\nIn Cartesian component form, this gives:\n$$\n\\varepsilon_{xx} = \\frac{\\partial u_x}{\\partial x}, \\quad \\varepsilon_{yy} = \\frac{\\partial u_y}{\\partial y}, \\quad \\varepsilon_{xy} = \\varepsilon_{yx} = \\frac{1}{2}\\left(\\frac{\\partial u_x}{\\partial y} + \\frac{\\partial u_y}{\\partial x}\\right)\n$$\nThe trace of the strain tensor, which represents the volumetric strain, is $\\text{tr}(\\boldsymbol{\\varepsilon}) = \\varepsilon_{kk} = \\varepsilon_{xx} + \\varepsilon_{yy}$.\n\nFor a linear, isotropic, and elastic material, the constitutive law relating the Cauchy stress tensor $\\boldsymbol{\\sigma}$ to the strain tensor $\\boldsymbol{\\varepsilon}$ is given by Hooke's Law, which can be expressed using the Lamé parameters $\\lambda$ and $\\mu$ (also known as the first and second Lamé parameters, where $\\mu$ is the shear modulus):\n$$\n\\boldsymbol{\\sigma} = \\lambda \\, \\text{tr}(\\boldsymbol{\\varepsilon}) \\, \\mathbf{I} + 2\\mu \\, \\boldsymbol{\\varepsilon}\n$$\nwhere $\\mathbf{I}$ is the second-order identity tensor. In Cartesian component form, the stress components are:\n$$\n\\sigma_{xx} = \\lambda(\\varepsilon_{xx} + \\varepsilon_{yy}) + 2\\mu \\varepsilon_{xx} = (\\lambda + 2\\mu)\\frac{\\partial u_x}{\\partial x} + \\lambda\\frac{\\partial u_y}{\\partial y}\n$$\n$$\n\\sigma_{yy} = \\lambda(\\varepsilon_{xx} + \\varepsilon_{yy}) + 2\\mu \\varepsilon_{yy} = \\lambda\\frac{\\partial u_x}{\\partial x} + (\\lambda + 2\\mu)\\frac{\\partial u_y}{\\partial y}\n$$\n$$\n\\sigma_{xy} = \\sigma_{yx} = 2\\mu \\varepsilon_{xy} = \\mu\\left(\\frac{\\partial u_x}{\\partial y} + \\frac{\\partial u_y}{\\partial x}\\right)\n$$\n\nNext, we address Task 2: deriving the explicit form of the interior equilibrium residual components. The balance of linear momentum in a quasi-static setting is the equilibrium equation $\\nabla \\cdot \\boldsymbol{\\sigma} + \\mathbf{b} = \\mathbf{0}$. A Physics-Informed Neural Network (PINN) seeks to minimize the residual of this equation, which is $\\mathbf{R} = \\nabla \\cdot \\boldsymbol{\\sigma} + \\mathbf{b}$. The components of this residual vector in Cartesian coordinates are:\n$$\nR_x = \\frac{\\partial \\sigma_{xx}}{\\partial x} + \\frac{\\partial \\sigma_{xy}}{\\partial y} + b_x\n$$\n$$\nR_y = \\frac{\\partial \\sigma_{yx}}{\\partial x} + \\frac{\\partial \\sigma_{yy}}{\\partial y} + b_y\n$$\nSubstituting the expressions for the stress components in terms of displacement derivatives yields the Navier-Cauchy equations. We assume the material is homogeneous, so $\\lambda$ and $\\mu$ are constants.\nFor the $x$-component of the residual:\n$$\nR_x = \\frac{\\partial}{\\partial x} \\left( (\\lambda + 2\\mu)\\frac{\\partial u_x}{\\partial x} + \\lambda\\frac{\\partial u_y}{\\partial y} \\right) + \\frac{\\partial}{\\partial y} \\left( \\mu\\left(\\frac{\\partial u_x}{\\partial y} + \\frac{\\partial u_y}{\\partial x}\\right) \\right) + b_x\n$$\n$$\nR_x = (\\lambda + 2\\mu)\\frac{\\partial^2 u_x}{\\partial x^2} + \\lambda\\frac{\\partial^2 u_y}{\\partial x \\partial y} + \\mu\\frac{\\partial^2 u_x}{\\partial y^2} + \\mu\\frac{\\partial^2 u_y}{\\partial y \\partial x} + b_x\n$$\nBy equality of mixed partials ($\\frac{\\partial^2 u_y}{\\partial x \\partial y} = \\frac{\\partial^2 u_y}{\\partial y \\partial x}$), we can group the terms:\n$$\nR_x = (\\lambda+2\\mu)\\frac{\\partial^2 u_x}{\\partial x^2} + \\mu \\frac{\\partial^2 u_x}{\\partial y^2} + (\\lambda+\\mu) \\frac{\\partial^2 u_y}{\\partial x \\partial y} + b_x\n$$\nFor the $y$-component of the residual:\n$$\nR_y = \\frac{\\partial}{\\partial x} \\left( \\mu\\left(\\frac{\\partial u_x}{\\partial y} + \\frac{\\partial u_y}{\\partial x}\\right) \\right) + \\frac{\\partial}{\\partial y} \\left( \\lambda\\frac{\\partial u_x}{\\partial x} + (\\lambda + 2\\mu)\\frac{\\partial u_y}{\\partial y} \\right) + b_y\n$$\n$$\nR_y = \\mu\\frac{\\partial^2 u_x}{\\partial x \\partial y} + \\mu\\frac{\\partial^2 u_y}{\\partial x^2} + \\lambda\\frac{\\partial^2 u_x}{\\partial y \\partial x} + (\\lambda + 2\\mu)\\frac{\\partial^2 u_y}{\\partial y^2} + b_y\n$$\nAgain, grouping terms:\n$$\nR_y = (\\lambda+\\mu) \\frac{\\partial^2 u_x}{\\partial x \\partial y} + \\mu \\frac{\\partial^2 u_y}{\\partial x^2} + (\\lambda+2\\mu) \\frac{\\partial^2 u_y}{\\partial y^2} + b_y\n$$\nThese are the explicit expressions for the residual components that a PINN would enforce.\n\nFinally, we address Task 3: evaluating these residuals for the specified neural network ansatz and parameters. The ansatz is given by:\n$z(x,y) = w_1 x + w_2 y + b_1$ and $\\mathbf{u}(x,y) = \\mathbf{W}_2 \\tanh(z(x,y)) + \\mathbf{b}_2$.\nWith the given parameters $w_1 = 1$, $w_2 = 2$, $b_1 = 0$, $\\mathbf{W}_2 = \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix}$, and $\\mathbf{b}_2 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$, we have:\n$z(x,y) = x + 2y$\nAnd the displacement components are:\n$$\nu_x(x,y) = 2 \\tanh(x+2y)\n$$\n$$\nu_y(x,y) = -1 \\tanh(x+2y)\n$$\nWe must compute the second-order partial derivatives of $u_x$ and $u_y$. Let $T(x,y) = \\tanh(x+2y)$ and $S(x,y) = \\text{sech}^2(x+2y) = 1-\\tanh^2(x+2y)$. We use the chain rule and the derivative identities $\\frac{d}{d\\alpha}\\tanh(\\alpha) = \\text{sech}^2(\\alpha)$ and $\\frac{d}{d\\alpha}\\text{sech}^2(\\alpha) = -2\\tanh(\\alpha)\\text{sech}^2(\\alpha)$.\n\nDerivatives of $u_x = 2T$:\n$\\frac{\\partial u_x}{\\partial x} = 2 \\frac{\\partial T}{\\partial x} = 2S \\cdot 1 = 2S$\n$\\frac{\\partial u_x}{\\partial y} = 2 \\frac{\\partial T}{\\partial y} = 2S \\cdot 2 = 4S$\n$\\frac{\\partial^2 u_x}{\\partial x^2} = \\frac{\\partial}{\\partial x}(2S) = 2(-2TS) \\cdot 1 = -4TS$\n$\\frac{\\partial^2 u_x}{\\partial y^2} = \\frac{\\partial}{\\partial y}(4S) = 4(-2TS) \\cdot 2 = -16TS$\n$\\frac{\\partial^2 u_x}{\\partial x \\partial y} = \\frac{\\partial}{\\partial y}(2S) = 2(-2TS) \\cdot 2 = -8TS$\n\nDerivatives of $u_y = -T$:\n$\\frac{\\partial u_y}{\\partial x} = -S \\cdot 1 = -S$\n$\\frac{\\partial u_y}{\\partial y} = -S \\cdot 2 = -2S$\n$\\frac{\\partial^2 u_y}{\\partial x^2} = \\frac{\\partial}{\\partial x}(-S) = -(-2TS) \\cdot 1 = 2TS$\n$\\frac{\\partial^2 u_y}{\\partial y^2} = \\frac{\\partial}{\\partial y}(-2S) = -2(-2TS) \\cdot 2 = 8TS$\n$\\frac{\\partial^2 u_y}{\\partial x \\partial y} = \\frac{\\partial}{\\partial y}(-S) = -(-2TS) \\cdot 2 = 4TS$\n\nWe are given zero body force, so $\\mathbf{b}=\\mathbf{0}$. We substitute these derivatives into the residual expressions:\n$$\nR_x = (\\lambda+2\\mu)(-4TS) + \\mu(-16TS) + (\\lambda+\\mu)(4TS)\n$$\n$$\nR_x = [-4(\\lambda+2\\mu) - 16\\mu + 4(\\lambda+\\mu)]TS = [-4\\lambda - 8\\mu - 16\\mu + 4\\lambda + 4\\mu]TS = -20\\mu TS\n$$\n$$\nR_y = (\\lambda+\\mu)(-8TS) + \\mu(2TS) + (\\lambda+2\\mu)(8TS)\n$$\n$$\nR_y = [-8(\\lambda+\\mu) + 2\\mu + 8(\\lambda+2\\mu)]TS = [-8\\lambda - 8\\mu + 2\\mu + 8\\lambda + 16\\mu]TS = 10\\mu TS\n$$\nThe residual vector is $\\mathbf{R}(x,y) = \\begin{pmatrix} -20\\mu & 10\\mu \\end{pmatrix} \\tanh(x+2y)\\text{sech}^2(x+2y)$.\n\nWe must evaluate this at the point $(x_0, y_0) = (1, 0)$. At this point, the argument of the hyperbolic functions is $z_0 = 1 + 2(0) = 1$.\nThe term $TS$ becomes $\\tanh(1)\\text{sech}^2(1)$. Using the identity $\\text{sech}^2(\\alpha) = 1/\\cosh^2(\\alpha)$, this is $\\frac{\\tanh(1)}{\\cosh^2(1)}$.\nThe residual components at $(1, 0)$ are:\n$$\nR_x(1,0) = -20\\mu \\frac{\\tanh(1)}{\\cosh^{2}(1)}\n$$\n$$\nR_y(1,0) = 10\\mu \\frac{\\tanh(1)}{\\cosh^{2}(1)}\n$$\nThe problem asks for the result as a single row vector.\n$$\n\\mathbf{R}(1,0) = \\begin{pmatrix} -20\\mu \\frac{\\tanh(1)}{\\cosh^{2}(1)} & 10\\mu \\frac{\\tanh(1)}{\\cosh^{2}(1)} \\end{pmatrix}\n$$\nThis is the final analytical expression.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n-20\\mu \\frac{\\tanh(1)}{\\cosh^{2}(1)} & 10\\mu \\frac{\\tanh(1)}{\\cosh^{2}(1)}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "A well-posed physical problem requires both a governing equation and a set of boundary conditions. This practice focuses on constructing the boundary loss terms for a PINN, covering both Dirichlet (prescribed value) and Neumann (prescribed derivative) conditions for Laplace's equation. You will learn how to formulate these losses from discrete data points, a common scenario in building digital twins for cyber-physical systems where sensors provide measurements only at specific locations .",
            "id": "4235901",
            "problem": "A cyber-physical system digital twin for a steady-state conductive field is modeled by Laplace’s equation, derived from conservation of flux with constant conductivity and no sources. Let the physical potential be $u:\\Omega \\to \\mathbb{R}$, where $\\Omega \\subset \\mathbb{R}^{d}$ is a bounded domain with Lipschitz boundary $\\partial \\Omega$. The governing law is $-\\Delta u = 0$ in $\\Omega$, with mixed boundary conditions $u=g$ on $\\Gamma_{D} \\subset \\partial \\Omega$ and $\\partial_{n} u = h$ on $\\Gamma_{N}=\\partial \\Omega \\setminus \\Gamma_{D}$, where $g:\\Gamma_{D} \\to \\mathbb{R}$ and $h:\\Gamma_{N} \\to \\mathbb{R}$ are measured by boundary sensors, and $\\partial_{n} u$ denotes the normal derivative.\n\nYou deploy a physics-informed neural network (PINN) $u_{\\theta}:\\Omega \\to \\mathbb{R}$ with parameters $\\theta$ to serve as the digital twin surrogate. For boundary training, you collect quadrature samples on each boundary segment:\n- Dirichlet samples $\\{x_{i}^{D}\\}_{i=1}^{N_{D}} \\subset \\Gamma_{D}$ with associated measurements $\\{g_{i}\\}_{i=1}^{N_{D}}$ and positive quadrature weights $\\{w_{i}^{D}\\}_{i=1}^{N_{D}}$ that approximate integration over $\\Gamma_{D}$.\n- Neumann samples $\\{x_{j}^{N}\\}_{j=1}^{N_{N}} \\subset \\Gamma_{N}$ with measured fluxes $\\{h_{j}\\}_{j=1}^{N_{N}}$, unit outward normals $\\{n_{j}\\}_{j=1}^{N_{N}} \\subset \\mathbb{R}^{d}$, and positive quadrature weights $\\{w_{j}^{N}\\}_{j=1}^{N_{N}}$.\n\nLet $W_{D}=\\sum_{i=1}^{N_{D}} w_{i}^{D}$ and $W_{N}=\\sum_{j=1}^{N_{N}} w_{j}^{N}$ be the total quadrature weights. Suppose you choose positive penalty coefficients $\\lambda_{D}$ and $\\lambda_{N}$ to balance the enforcement strength of Dirichlet and Neumann boundary conditions.\n\nStarting from the fundamental law $-\\Delta u = 0$ and the definitions of Dirichlet and Neumann boundary conditions, derive the separate boundary residual functionals on $\\Gamma_{D}$ and $\\Gamma_{N}$ that measure the squared mismatch between $u_{\\theta}$ and $g$, and between $\\nabla u_{\\theta} \\cdot n$ and $h$, respectively. Then, using quadrature approximations with the given samples and weights, construct a single composite, dimensionless boundary-only loss functional $L_{\\mathrm{bc}}(\\theta)$ whose value is a weighted sum of the two boundary residuals normalized by $W_{D}$ and $W_{N}$. Express this $L_{\\mathrm{bc}}(\\theta)$ explicitly as a closed-form analytic expression in terms of the given data $\\{x_{i}^{D},g_{i},w_{i}^{D}\\}_{i=1}^{N_{D}}$, $\\{x_{j}^{N},h_{j},n_{j},w_{j}^{N}\\}_{j=1}^{N_{N}}$, and the network $u_{\\theta}$. In your derivation, discuss how each boundary residual is enforced using automatic differentiation (AD) to obtain $\\nabla u_{\\theta}$ at boundary points.\n\nYour final answer must be the single explicit expression for $L_{\\mathrm{bc}}(\\theta)$ as a function of $\\theta$, containing only symbolic quantities and the given data. Do not include units. No rounding is required.",
            "solution": "The problem requires the derivation of a composite, dimensionless boundary-only loss functional, denoted $L_{\\mathrm{bc}}(\\theta)$, for a physics-informed neural network (PINN) surrogate $u_{\\theta}$ of a physical potential $u$. The system is governed by Laplace's equation $-\\Delta u = 0$ on a domain $\\Omega \\subset \\mathbb{R}^{d}$ with mixed Dirichlet and Neumann boundary conditions. The loss functional is constructed from discrete sensor data on the boundary.\n\nWe begin by formally defining the continuous residual functionals for each type of boundary condition. These functionals measure the extent to which the PINN surrogate $u_{\\theta}$ fails to satisfy the prescribed conditions on the boundary $\\partial \\Omega$.\n\nFirst, consider the Dirichlet boundary $\\Gamma_{D}$, where the potential is prescribed as $u=g$. The mismatch, or residual, at any point $x \\in \\Gamma_D$ is given by the difference $u_{\\theta}(x) - g(x)$. To measure the total error over this boundary segment, we integrate the square of this residual. This gives the continuous Dirichlet boundary residual functional, $R_{D}(\\theta)$:\n$$\nR_{D}(\\theta) = \\int_{\\Gamma_{D}} (u_{\\theta}(x) - g(x))^2 \\, \\mathrm{d}S\n$$\nwhere $\\mathrm{d}S$ is the surface measure on the boundary $\\partial\\Omega$.\n\nSecond, consider the Neumann boundary $\\Gamma_{N}$, where the normal derivative of the potential is prescribed as $\\partial_{n} u = h$. The normal derivative is the projection of the gradient onto the unit outward normal vector $n(x)$, i.e., $\\partial_{n} u(x) = \\nabla u(x) \\cdot n(x)$. The residual at a point $x \\in \\Gamma_N$ is the difference between the normal derivative of the PINN approximation and the prescribed function $h(x)$, which is $\\nabla u_{\\theta}(x) \\cdot n(x) - h(x)$. The total squared error over the Neumann boundary is then given by the continuous Neumann boundary residual functional, $R_{N}(\\theta)$:\n$$\nR_{N}(\\theta) = \\int_{\\Gamma_{N}} (\\nabla u_{\\theta}(x) \\cdot n(x) - h(x))^2 \\, \\mathrm{d}S\n$$\n\nIn a practical setting, we do not have the continuous functions $g$ and $h$ but rather discrete measurements at specific sensor locations. The problem provides sets of sample points and quadrature weights to approximate the integrals for $R_{D}(\\theta)$ and $R_{N}(\\theta)$.\n\nFor the Dirichlet boundary, we are given $N_D$ sample points $\\{x_{i}^{D}\\}_{i=1}^{N_{D}} \\subset \\Gamma_{D}$ with corresponding potential measurements $\\{g_{i}\\}_{i=1}^{N_{D}}$ (where $g_i = g(x_i^D)$) and positive quadrature weights $\\{w_{i}^{D}\\}_{i=1}^{N_{D}}$. The integral for $R_{D}(\\theta)$ is approximated by a weighted sum, which we denote as the discrete residual $\\hat{R}_{D}(\\theta)$:\n$$\n\\hat{R}_{D}(\\theta) = \\sum_{i=1}^{N_{D}} w_{i}^{D} (u_{\\theta}(x_{i}^{D}) - g_{i})^2\n$$\nThis is a numerical quadrature approximation of the integral $\\int_{\\Gamma_{D}} (u_{\\theta} - g)^2 \\, \\mathrm{d}S$.\n\nFor the Neumann boundary, we have $N_N$ sample points $\\{x_{j}^{N}\\}_{j=1}^{N_{N}} \\subset \\Gamma_{N}$ with flux measurements $\\{h_{j}\\}_{j=1}^{N_{N}}$, unit outward normals $\\{n_{j}\\}_{j=1}^{N_{N}}$, and positive quadrature weights $\\{w_{j}^{N}\\}_{j=1}^{N_{N}}$. The crucial calculation here is the term $\\nabla u_{\\theta}(x_j^N)$, the gradient of the neural network output with respect to its spatial inputs. This is computed using automatic differentiation (AD). The neural network $u_{\\theta}$ is a composition of differentiable elementary functions (e.g., affine transformations and activation functions). Its derivatives with respect to its inputs can therefore be computed algorithmically to machine precision. Deep learning frameworks provide this functionality natively, typically via reverse-mode AD (backpropagation), allowing for efficient evaluation of $\\nabla u_{\\theta}$ at any point $x_j^N$ required for the loss calculation.\n\nUsing AD to obtain $\\nabla u_{\\theta}(x_{j}^{N})$, we can then approximate the integral for $R_{N}(\\theta)$ with the discrete Neumann residual $\\hat{R}_{N}(\\theta)$:\n$$\n\\hat{R}_{N}(\\theta) = \\sum_{j=1}^{N_{N}} w_{j}^{N} (\\nabla u_{\\theta}(x_{j}^{N}) \\cdot n_{j} - h_{j})^2\n$$\n\nThe problem asks for a single composite loss functional $L_{\\mathrm{bc}}(\\theta)$ that is a weighted sum of the two boundary residuals, normalized by their respective total quadrature weights, $W_{D}=\\sum_{i=1}^{N_{D}} w_{i}^{D}$ and $W_{N}=\\sum_{j=1}^{N_{N}} w_{j}^{N}$. This normalization turns each discrete residual into a mean squared error approximation. We are also given positive penalty coefficients $\\lambda_{D}$ and $\\lambda_{N}$ to balance the relative importance of enforcing each boundary condition.\nThe normalized Dirichlet loss term is $\\lambda_{D} \\frac{\\hat{R}_{D}(\\theta)}{W_{D}}$.\nThe normalized Neumann loss term is $\\lambda_{N} \\frac{\\hat{R}_{N}(\\theta)}{W_{N}}$.\n\nThe problem specifies that the final loss functional $L_{\\mathrm{bc}}(\\theta)$ should be dimensionless. This typically implies that the governing equations and boundary conditions have been non-dimensionalized beforehand using characteristic scales for length, potential, etc. Under this standard assumption, all quantities in the loss function are dimensionless, and the penalty coefficients $\\lambda_D$ and $\\lambda_N$ are also dimensionless factors used for tuning the training process.\n\nCombining these components, the composite boundary loss functional is the sum of the normalized and weighted discrete residuals:\n$$\nL_{\\mathrm{bc}}(\\theta) = \\lambda_{D} \\frac{\\hat{R}_{D}(\\theta)}{W_{D}} + \\lambda_{N} \\frac{\\hat{R}_{N}(\\theta)}{W_{N}}\n$$\nSubstituting the expressions for $\\hat{R}_{D}(\\theta)$, $\\hat{R}_{N}(\\theta)$, $W_{D}$, and $W_{N}$:\n$$\nL_{\\mathrm{bc}}(\\theta) = \\lambda_{D} \\left( \\frac{\\sum_{i=1}^{N_{D}} w_{i}^{D} (u_{\\theta}(x_{i}^{D}) - g_{i})^2}{\\sum_{i=1}^{N_{D}} w_{i}^{D}} \\right) + \\lambda_{N} \\left( \\frac{\\sum_{j=1}^{N_{N}} w_{j}^{N} (\\nabla u_{\\theta}(x_{j}^{N}) \\cdot n_{j} - h_{j})^2}{\\sum_{j=1}^{N_{N}} w_{j}^{N}} \\right)\n$$\nThis can be written more concisely using the given definitions $W_{D}$ and $W_{N}$:\n$$\nL_{\\mathrm{bc}}(\\theta) = \\frac{\\lambda_{D}}{W_{D}} \\sum_{i=1}^{N_{D}} w_{i}^{D} (u_{\\theta}(x_{i}^{D}) - g_{i})^2 + \\frac{\\lambda_{N}}{W_{N}} \\sum_{j=1}^{N_{N}} w_{j}^{N} (\\nabla u_{\\theta}(x_{j}^{N}) \\cdot n_{j} - h_{j})^2\n$$\nThis expression represents the final, closed-form analytic form for the boundary-only loss functional $L_{\\mathrm{bc}}(\\theta)$ as a function of the network parameters $\\theta$ and the given data. Minimizing this loss functional with respect to $\\theta$ trains the network to satisfy the boundary conditions in a least-squares sense.",
            "answer": "$$\n\\boxed{\\frac{\\lambda_{D}}{W_{D}} \\sum_{i=1}^{N_{D}} w_{i}^{D} (u_{\\theta}(x_{i}^{D}) - g_{i})^2 + \\frac{\\lambda_{N}}{W_{N}} \\sum_{j=1}^{N_{N}} w_{j}^{N} (\\nabla u_{\\theta}(x_{j}^{N}) \\cdot n_{j} - h_{j})^2}\n$$"
        }
    ]
}