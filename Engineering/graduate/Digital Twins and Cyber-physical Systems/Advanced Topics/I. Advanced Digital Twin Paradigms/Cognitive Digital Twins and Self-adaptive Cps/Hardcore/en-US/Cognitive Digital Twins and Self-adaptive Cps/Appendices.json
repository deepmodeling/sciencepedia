{
    "hands_on_practices": [
        {
            "introduction": "Before a cognitive digital twin can learn, predict, or act, it must first establish an accurate understanding of the physical system's current state. This exercise focuses on the core of this challenge: optimally fusing noisy sensor measurements with a predictive model using the principles of Bayesian state estimation, as embodied by the celebrated Kalman Filter . By deriving the posterior covariance update from first principles, you will gain a deep appreciation for how the filter quantifies and reduces uncertainty with each new piece of information.",
            "id": "4208983",
            "problem": "A cognitive digital twin for a self-adaptive Cyber-Physical System (CPS) performs online state estimation of a two-dimensional latent state using a single scalar sensor. The digital twin employs the optimal linear Minimum Mean Square Error estimator derived from the linear-Gaussian Bayesian conditioning principle, commonly instantiated as the Kalman Filter (KF). At time index $k$, the prior belief over the state is Gaussian with mean $\\hat{x}_{k}^{-}$ and covariance $P_{k}^{-}$, and the measurement model is linear with additive Gaussian noise:\n$$\ny_{k} = H x_{k} + v_{k}, \\quad v_{k} \\sim \\mathcal{N}(0, R),\n$$\nwhere $H \\in \\mathbb{R}^{1 \\times 2}$ and $R \\in \\mathbb{R}$.\n\nStarting from the core definitions of linear-Gaussian models and Bayesian conditioning, derive the optimal linear gain $K_{k}$ and the posterior covariance $P_{k}^{+}$ of the state estimate after assimilating $y_{k}$, expressed solely in terms of $H$, $P_{k}^{-}$, and $R$. Then evaluate the posterior covariance numerically for the scientifically plausible parameters\n$$\nP_{k}^{-} = \\begin{bmatrix}2 & 0\\\\ 0 & 1\\end{bmatrix}, \\quad H = \\begin{bmatrix}1 & 0\\end{bmatrix}, \\quad R = 1.\n$$\nProvide the final posterior covariance as an exact matrix with no rounding. No physical units are required for this computation.",
            "solution": "The problem is valid. It is scientifically grounded in the principles of Bayesian state estimation and the Kalman Filter, is mathematically well-posed with all necessary information provided, and is formulated objectively. We will proceed with the derivation and numerical evaluation.\n\nThe goal is to find the optimal linear estimator for the state $x_k \\in \\mathbb{R}^2$ given a prior belief and a linear measurement. The posterior estimate, $\\hat{x}_k^+$, is a linear function of the prior estimate, $\\hat{x}_k^-$, and the new measurement, $y_k$. The update is of the form:\n$$\n\\hat{x}_k^+ = \\hat{x}_k^- + K_k (y_k - \\hat{y}_k^-)\n$$\nwhere $K_k$ is the gain matrix to be determined, and $\\hat{y}_k^-$ is the predicted measurement based on the prior. The predicted measurement is the expectation of $y_k$ given the prior information:\n$$\n\\hat{y}_k^- = E[y_k] = E[H x_k + v_k] = H E[x_k] + E[v_k] = H \\hat{x}_k^- + 0 = H \\hat{x}_k^-\n$$\nThe term $y_k - \\hat{y}_k^-$ is the innovation or measurement residual. The posterior estimate is thus:\n$$\n\\hat{x}_k^+ = \\hat{x}_k^- + K_k (y_k - H \\hat{x}_k^-)\n$$\nThe posterior estimation error is $e_k^+ = x_k - \\hat{x}_k^+$. Substituting the expression for $\\hat{x}_k^+$:\n$$\ne_k^+ = x_k - \\left( \\hat{x}_k^- + K_k (y_k - H \\hat{x}_k^-) \\right)\n$$\nSubstituting the measurement model $y_k = H x_k + v_k$:\n$$\ne_k^+ = (x_k - \\hat{x}_k^-) - K_k ( (H x_k + v_k) - H \\hat{x}_k^- )\n$$\nLet the prior estimation error be $e_k^- = x_k - \\hat{x}_k^-$. This simplifies the expression to:\n$$\ne_k^+ = e_k^- - K_k ( H(x_k - \\hat{x}_k^-) + v_k ) = e_k^- - K_k (H e_k^- + v_k)\n$$\nGrouping terms, we get:\n$$\ne_k^+ = (I - K_k H) e_k^- - K_k v_k\n$$\nThe posterior covariance matrix is defined as $P_k^+ = E[e_k^+ (e_k^+)^\\top]$. We substitute the expression for $e_k^+$:\n$$\nP_k^+ = E \\left[ \\left( (I - K_k H) e_k^- - K_k v_k \\right) \\left( (I - K_k H) e_k^- - K_k v_k \\right)^\\top \\right]\n$$\nExpanding the product:\n$$\nP_k^+ = E \\left[ (I - K_k H) e_k^- (e_k^-)^\\top (I - K_k H)^\\top - (I - K_k H) e_k^- v_k^\\top K_k^\\top - K_k v_k (e_k^-)^\\top (I - K_k H)^\\top + K_k v_k v_k^\\top K_k^\\top \\right]\n$$\nThe prior state error $e_k^-$ and the measurement noise $v_k$ are uncorrelated, so $E[e_k^- v_k^\\top] = 0$ and $E[v_k (e_k^-)^\\top] = 0$. Using the definitions $E[e_k^- (e_k^-)^\\top] = P_k^-$ and $E[v_k v_k^\\top] = R$, the expression simplifies to:\n$$\nP_k^+ = (I - K_k H) P_k^- (I - K_k H)^\\top + K_k R K_k^\\top\n$$\nThis is the Joseph form of the covariance update. To find the optimal gain $K_k$ that minimizes the mean square error (i.e., the trace of $P_k^+$), we differentiate $\\text{tr}(P_k^+)$ with respect to $K_k$ and set the result to zero. First, expand the expression for $P_k^+$:\n$$\nP_k^+ = P_k^- - K_k H P_k^- - P_k^- H^\\top K_k^\\top + K_k H P_k^- H^\\top K_k^\\top + K_k R K_k^\\top\n$$\n$$\nP_k^+ = P_k^- - K_k H P_k^- - P_k^- H^\\top K_k^\\top + K_k (H P_k^- H^\\top + R) K_k^\\top\n$$\nThe trace is a linear operator, so we can take the trace of each term. We then differentiate $\\text{tr}(P_k^+)$ with respect to $K_k$ using standard matrix calculus identities ($\\frac{\\partial}{\\partial X} \\text{tr}(AX) = A^\\top$, $\\frac{\\partial}{\\partial X} \\text{tr}(XA^\\top) = A$, $\\frac{\\partial}{\\partial X} \\text{tr}(XCX^\\top) = 2XC$ for symmetric $C$):\n$$\n\\frac{\\partial}{\\partial K_k} \\text{tr}(P_k^+) = 0 - (H P_k^-)^\\top - (P_k^- H^\\top) + 2 K_k (H P_k^- H^\\top + R)\n$$\nSince $P_k^-$ is symmetric, $(P_k^-)^\\top = P_k^-$.\n$$\n\\frac{\\partial}{\\partial K_k} \\text{tr}(P_k^+) = - (P_k^-)^\\top H^\\top - P_k^- H^\\top + 2 K_k (H P_k^- H^\\top + R) = -2 P_k^- H^\\top + 2 K_k (H P_k^- H^\\top + R)\n$$\nSetting the derivative to zero for optimality:\n$$\n-2 P_k^- H^\\top + 2 K_k (H P_k^- H^\\top + R) = 0\n$$\n$$\nK_k (H P_k^- H^\\top + R) = P_k^- H^\\top\n$$\nSolving for the optimal gain $K_k$:\n$$\nK_k = P_k^- H^\\top (H P_k^- H^\\top + R)^{-1}\n$$\nThis is the optimal Kalman gain. To find the posterior covariance, we can substitute this expression for $K_k$ back into a simplified form of the $P_k^+$ equation. Let's use $P_k^+ = P_k^- - K_k H P_k^- - P_k^- H^\\top K_k^\\top + K_k (H P_k^- H^\\top + R) K_k^\\top$. From the optimality condition, we have $P_k^- H^\\top = K_k (H P_k^- H^\\top + R)$. Substituting this into the third term:\n$$\nP_k^+ = P_k^- - K_k H P_k^- - \\left( K_k (H P_k^- H^\\top + R) \\right)^\\top K_k^\\top + K_k (H P_k^- H^\\top + R) K_k^\\top\n$$\nSince $(AB)^\\top = B^\\top A^\\top$ and $(H P_k^- H^\\top + R)$ is symmetric:\n$$\nP_k^+ = P_k^- - K_k H P_k^- - K_k (H P_k^- H^\\top + R) K_k^\\top + K_k (H P_k^- H^\\top + R) K_k^\\top\n$$\n$$\nP_k^+ = P_k^- - K_k H P_k^- = (I - K_k H) P_k^-\n$$\nThis is the simplest form for the updated posterior covariance. The problem asks for $P_k^+$ solely in terms of $H$, $P_k^-$, and $R$. Substituting the expression for $K_k$:\n$$\nP_{k}^{+} = \\left(I - \\left(P_k^- H^\\top (H P_k^- H^\\top + R)^{-1}\\right) H\\right) P_k^-\n$$\n\nNow, we evaluate this numerically with the given parameters:\n$$\nP_{k}^{-} = \\begin{bmatrix}2 & 0\\\\ 0 & 1\\end{bmatrix}, \\quad H = \\begin{bmatrix}1 & 0\\end{bmatrix}, \\quad R = 1\n$$\nFirst, we compute the innovation covariance, $S_k = H P_k^- H^\\top + R$.\n$$\nH P_k^- = \\begin{bmatrix}1 & 0\\end{bmatrix} \\begin{bmatrix}2 & 0\\\\ 0 & 1\\end{bmatrix} = \\begin{bmatrix}(1)(2)+(0)(0) & (1)(0)+(0)(1)\\end{bmatrix} = \\begin{bmatrix}2 & 0\\end{bmatrix}\n$$\n$$\nH P_k^- H^\\top = \\begin{bmatrix}2 & 0\\end{bmatrix} \\begin{bmatrix}1 \\\\ 0\\end{bmatrix} = (2)(1) + (0)(0) = 2\n$$\nSince $R=1$, the innovation covariance is a scalar:\n$$\nS_k = H P_k^- H^\\top + R = 2 + 1 = 3\n$$\nThe inverse is $S_k^{-1} = \\frac{1}{3}$.\n\nNext, we calculate the optimal Kalman gain $K_k = P_k^- H^\\top S_k^{-1}$.\n$$\nP_k^- H^\\top = \\begin{bmatrix}2 & 0\\\\ 0 & 1\\end{bmatrix} \\begin{bmatrix}1 \\\\ 0\\end{bmatrix} = \\begin{bmatrix}(2)(1)+(0)(0) \\\\ (0)(1)+(1)(0)\\end{bmatrix} = \\begin{bmatrix}2 \\\\ 0\\end{bmatrix}\n$$\n$$\nK_k = \\begin{bmatrix}2 \\\\ 0\\end{bmatrix} \\left(\\frac{1}{3}\\right) = \\begin{bmatrix}2/3 \\\\ 0\\end{bmatrix}\n$$\nFinally, we compute the posterior covariance $P_k^+ = (I - K_k H) P_k^-$.\nFirst, calculate $K_k H$:\n$$\nK_k H = \\begin{bmatrix}2/3 \\\\ 0\\end{bmatrix} \\begin{bmatrix}1 & 0\\end{bmatrix} = \\begin{bmatrix}(2/3)(1) & (2/3)(0) \\\\ (0)(1) & (0)(0)\\end{bmatrix} = \\begin{bmatrix}2/3 & 0 \\\\ 0 & 0\\end{bmatrix}\n$$\nNext, compute $(I - K_k H)$:\n$$\nI - K_k H = \\begin{bmatrix}1 & 0 \\\\ 0 & 1\\end{bmatrix} - \\begin{bmatrix}2/3 & 0 \\\\ 0 & 0\\end{bmatrix} = \\begin{bmatrix}1 - 2/3 & 0-0 \\\\ 0-0 & 1-0\\end{bmatrix} = \\begin{bmatrix}1/3 & 0 \\\\ 0 & 1\\end{bmatrix}\n$$\nNow, compute $P_k^+$:\n$$\nP_k^+ = (I - K_k H) P_k^- = \\begin{bmatrix}1/3 & 0 \\\\ 0 & 1\\end{bmatrix} \\begin{bmatrix}2 & 0\\\\ 0 & 1\\end{bmatrix} = \\begin{bmatrix}(1/3)(2)+(0)(0) & (1/3)(0)+(0)(1) \\\\ (0)(2)+(1)(0) & (0)(0)+(1)(1)\\end{bmatrix}\n$$\n$$\nP_k^+ = \\begin{bmatrix}2/3 & 0 \\\\ 0 & 1\\end{bmatrix}\n$$\nThe posterior covariance matrix is computed as shown. The uncertainty in the first state component is reduced from $2$ to $2/3$ due to the informative measurement, while the uncertainty in the unobserved second component remains unchanged at $1$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{2}{3} & 0 \\\\ 0 & 1 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "A truly cognitive system must adapt not only its actions but also its internal understanding of the world, which involves learning and refining the parameters of its predictive models based on observed behavior . This practice challenges you to derive the Ordinary Least Squares (OLS) estimator, a cornerstone of system identification, to determine a key plant parameter from operational data. Through this exercise, you will explore both the algebraic solution and its geometric interpretation, revealing how OLS finds the best possible model fit within the space of linear possibilities.",
            "id": "4209011",
            "problem": "A self-adaptive Cyber-Physical System (CPS) integrates a Cognitive Digital Twin (CDT) that continuously estimates a scalar plant gain to maintain performance under varying conditions. The CDT models the plant output as a linear response to a control feature with additive disturbance, adopting the regression model $y = \\Phi \\theta + \\varepsilon$, where $y \\in \\mathbb{R}^{n}$ is the measured output vector, $\\Phi \\in \\mathbb{R}^{n \\times 1}$ is the feature (design) vector, $\\theta \\in \\mathbb{R}$ is the unknown scalar parameter representing the plant gain, and $\\varepsilon \\in \\mathbb{R}^{n}$ models exogenous disturbances. The CDT updates $\\theta$ using Ordinary Least Squares (OLS), which is defined as the value that minimizes the residual sum-of-squares $\\sum_{i=1}^{n} (y_{i} - \\Phi_{i} \\theta)^{2}$.\n\nIn a calibration phase involving $n = 3$ trials, the CDT collects the feature vector and outputs\n$$\n\\Phi = \\begin{bmatrix}1\\\\ 2\\\\ 3\\end{bmatrix}, \\quad y = \\begin{bmatrix}2\\\\ 2\\\\ 5\\end{bmatrix}.\n$$\nStarting from the OLS definition as a residual minimizer (without assuming any pre-derived estimator), derive the estimator $\\hat{\\theta}$ and compute its numerical value exactly. Then, using first principles of geometric projection in Euclidean spaces, qualitatively interpret the fit quality by relating the residual vector to the column space of $\\Phi$.\n\nProvide the final numerical answer for $\\hat{\\theta}$ as an exact rational number. No rounding is required, and no units are associated with the parameter.",
            "solution": "The problem requires the derivation and computation of the Ordinary Least Squares (OLS) estimator for a scalar parameter $\\theta$ in the linear model $y = \\Phi \\theta + \\varepsilon$, followed by a geometric interpretation of the result.\n\nFirst, we perform the validation of the problem statement.\n\n### Step 1: Extract Givens\n- Model: $y = \\Phi \\theta + \\varepsilon$, where $y \\in \\mathbb{R}^{n}$, $\\Phi \\in \\mathbb{R}^{n \\times 1}$, $\\theta \\in \\mathbb{R}$, and $\\varepsilon \\in \\mathbb{R}^{n}$.\n- OLS definition: Minimize the residual sum-of-squares (RSS) $S(\\theta) = \\sum_{i=1}^{n} (y_{i} - \\Phi_{i} \\theta)^{2}$.\n- Number of trials: $n = 3$.\n- Feature vector: $\\Phi = \\begin{bmatrix}1\\\\ 2\\\\ 3\\end{bmatrix}$.\n- Output vector: $y = \\begin{bmatrix}2\\\\ 2\\\\ 5\\end{bmatrix}$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, utilizing the standard and fundamental statistical method of Ordinary Least Squares for linear regression. This is a core concept in data analysis, control theory, and machine learning, directly applicable to the context of parameter estimation in a Cyber-Physical System (CPS). The problem is well-posed; since the feature vector $\\Phi$ is not the zero vector, the term $\\Phi^\\top\\Phi$ will be a non-zero scalar, which is invertible, guaranteeing a unique solution for the estimator $\\hat{\\theta}$. The problem is objective, providing precise numerical data and a clear, formalizable objective. All necessary information is provided, and there are no internal contradictions or scientifically implausible claims. The problem is a standard exercise in linear algebra and statistics, making it a valid and solvable problem.\n\nHaving validated the problem, we proceed to the solution.\n\n### Part 1: Derivation and Computation of the OLS Estimator $\\hat{\\theta}$\n\nThe OLS estimator $\\hat{\\theta}$ is defined as the value of $\\theta$ that minimizes the residual sum of squares, $S(\\theta)$. The residual for the $i$-th observation is $e_i = y_i - \\Phi_i \\theta$. The sum of squared residuals is:\n$$\nS(\\theta) = \\sum_{i=1}^{n} e_i^2 = \\sum_{i=1}^{n} (y_i - \\Phi_i \\theta)^2\n$$\nIn vector notation, the residual vector is $e = y - \\Phi\\theta$. The RSS can be written as the squared Euclidean norm of the residual vector, $S(\\theta) = \\|e\\|^2 = e^\\top e$.\n$$\nS(\\theta) = (y - \\Phi\\theta)^\\top (y - \\Phi\\theta)\n$$\nExpanding this expression, we get:\n$$\nS(\\theta) = (y^\\top - (\\Phi\\theta)^\\top) (y - \\Phi\\theta) = (y^\\top - \\theta^\\top \\Phi^\\top) (y - \\Phi\\theta)\n$$\nSince $\\theta$ is a scalar, $\\theta^\\top = \\theta$.\n$$\nS(\\theta) = y^\\top y - y^\\top\\Phi\\theta - \\theta\\Phi^\\top y + \\theta\\Phi^\\top\\Phi\\theta\n$$\nThe terms $y^\\top\\Phi\\theta$ and $\\theta\\Phi^\\top y$ are both scalars. The transpose of a scalar is itself, so $(y^\\top\\Phi\\theta)^\\top = \\theta^\\top\\Phi^\\top y = \\theta\\Phi^\\top y$. This implies $y^\\top\\Phi\\theta = \\theta\\Phi^\\top y$. Thus, we can combine these terms:\n$$\nS(\\theta) = y^\\top y - 2\\theta(\\Phi^\\top y) + \\theta^2(\\Phi^\\top\\Phi)\n$$\nTo find the value of $\\theta$ that minimizes this quadratic function, we take the derivative of $S(\\theta)$ with respect to $\\theta$ and set it to zero.\n$$\n\\frac{dS}{d\\theta} = \\frac{d}{d\\theta} \\left( y^\\top y - 2\\theta(\\Phi^\\top y) + \\theta^2(\\Phi^\\top\\Phi) \\right)\n$$\n$$\n\\frac{dS}{d\\theta} = 0 - 2(\\Phi^\\top y) + 2\\theta(\\Phi^\\top\\Phi)\n$$\nSetting the derivative to zero and denoting the minimizing value as $\\hat{\\theta}$:\n$$\n-2(\\Phi^\\top y) + 2\\hat{\\theta}(\\Phi^\\top\\Phi) = 0\n$$\n$$\n2\\hat{\\theta}(\\Phi^\\top\\Phi) = 2(\\Phi^\\top y)\n$$\nSolving for $\\hat{\\theta}$ yields the OLS estimator:\n$$\n\\hat{\\theta} = (\\Phi^\\top\\Phi)^{-1} (\\Phi^\\top y)\n$$\nThis completes the derivation from first principles. Now we compute the numerical value using the provided data: $\\Phi = \\begin{bmatrix}1\\\\ 2\\\\ 3\\end{bmatrix}$ and $y = \\begin{bmatrix}2\\\\ 2\\\\ 5\\end{bmatrix}$.\n\nFirst, we compute the scalar term $\\Phi^\\top\\Phi$:\n$$\n\\Phi^\\top\\Phi = \\begin{bmatrix}1 & 2 & 3\\end{bmatrix} \\begin{bmatrix}1\\\\ 2\\\\ 3\\end{bmatrix} = (1)(1) + (2)(2) + (3)(3) = 1 + 4 + 9 = 14\n$$\nNext, we compute the scalar term $\\Phi^\\top y$:\n$$\n\\Phi^\\top y = \\begin{bmatrix}1 & 2 & 3\\end{bmatrix} \\begin{bmatrix}2\\\\ 2\\\\ 5\\end{bmatrix} = (1)(2) + (2)(2) + (3)(5) = 2 + 4 + 15 = 21\n$$\nSubstituting these values back into the expression for $\\hat{\\theta}$:\n$$\n\\hat{\\theta} = (14)^{-1} (21) = \\frac{21}{14}\n$$\nSimplifying the fraction gives the exact rational number:\n$$\n\\hat{\\theta} = \\frac{3}{2}\n$$\n\n### Part 2: Geometric Interpretation of Fit Quality\n\nIn Euclidean space $\\mathbb{R}^n$, OLS has a profound geometric interpretation. The set of all possible model predictions forms a subspace spanned by the columns of the design matrix. In this problem, the design \"matrix\" $\\Phi$ is a single vector, so its column space, denoted $\\text{Col}(\\Phi)$, is a one-dimensional subspace of $\\mathbb{R}^3$â€”specifically, a line passing through the origin and the point $(1, 2, 3)$.\n$$\n\\text{Col}(\\Phi) = \\{c \\cdot \\Phi \\mid c \\in \\mathbb{R}\\}\n$$\nThe OLS procedure finds the vector within this subspace that is closest to the observed output vector $y$. This \"closest\" vector is the orthogonal projection of $y$ onto $\\text{Col}(\\Phi)$. Let's call this projected vector $\\hat{y}$.\n$$\n\\hat{y} = \\Phi\\hat{\\theta} = \\begin{bmatrix}1\\\\ 2\\\\ 3\\end{bmatrix} \\left(\\frac{3}{2}\\right) = \\begin{bmatrix}3/2\\\\ 3\\\\ 9/2\\end{bmatrix}\n$$\nThe vector $\\hat{y}$ is the best approximation of $y$ that can be made using the linear model.\n\nThe \"fit quality\" is related to how close the approximation $\\hat{y}$ is to the actual data $y$. This is quantified by the residual vector $e$:\n$$\ne = y - \\hat{y} = \\begin{bmatrix}2\\\\ 2\\\\ 5\\end{bmatrix} - \\begin{bmatrix}3/2\\\\ 3\\\\ 9/2\\end{bmatrix} = \\begin{bmatrix}4/2 - 3/2\\\\ 4/2 - 6/2\\\\ 10/2 - 9/2\\end{bmatrix} = \\begin{bmatrix}1/2\\\\ -1\\\\ 1/2\\end{bmatrix}\n$$\nA fundamental property of OLS is that the residual vector $e$ is orthogonal to the column space $\\text{Col}(\\Phi)$. This means the dot product of $e$ with any vector in $\\text{Col}(\\Phi)$ is zero. It is sufficient to verify that $e$ is orthogonal to the spanning vector $\\Phi$:\n$$\n\\Phi^\\top e = \\begin{bmatrix}1 & 2 & 3\\end{bmatrix} \\begin{bmatrix}1/2\\\\ -1\\\\ 1/2\\end{bmatrix} = (1)\\left(\\frac{1}{2}\\right) + (2)(-1) + (3)\\left(\\frac{1}{2}\\right) = \\frac{1}{2} - 2 + \\frac{3}{2} = \\frac{4}{2} - 2 = 2 - 2 = 0\n$$\nThe orthogonality is confirmed. Geometrically, this means we have decomposed the observation vector $y$ into two orthogonal components: one component lying in the model subspace ($\\hat{y}$) and one component orthogonal to it ($e$). The fit is not perfect because $e$ is not the zero vector. The non-zero residual $e$ represents the portion of the data $y$ that cannot be explained by the linear model, which is attributed to the disturbance term $\\varepsilon$. The quality of the fit is inversely proportional to the magnitude (length) of this residual vector; a smaller magnitude indicates that the data lies closer to the line defined by $\\Phi$, implying a better fit.",
            "answer": "$$\n\\boxed{\\frac{3}{2}}\n$$"
        },
        {
            "introduction": "The ultimate purpose of a cognitive twin in a self-adaptive system is to make intelligent, forward-looking decisions that optimize performance. Model Predictive Control (MPC) is a powerful framework for achieving this by repeatedly solving a finite-horizon optimal control problem at each time step . In this exercise, you will apply the Bellman principle of optimality to solve an MPC problem from scratch, calculating the optimal sequence of actions to steer a system towards its goal, thus demonstrating how a CDT translates its predictive model into concrete, optimal actions.",
            "id": "4208970",
            "problem": "A Cognitive Digital Twin (CDT) supervising a self-adaptive Cyber-Physical System (CPS) uses Model Predictive Control (MPC) to compute the control action to be applied at each sampling step. The controlled plant is a scalar discrete-time integrator with dynamics $x_{k+1} = x_k + u_k$. At time $k=0$, the twin solves a finite-horizon, unconstrained MPC problem with horizon length $N=2$ using the quadratic stage cost $(x_k - x^{\\text{ref}})^2$ and control effort cost $u_k^2$, and a quadratic terminal cost $(x_N - x^{\\text{ref}})^2$. Assume $x^{\\text{ref}}=0$, $Q=1$, $R=1$, and terminal weight $Q_f=1$. The initial state is $x_0=2$, and there are no state or input constraints.\n\nStarting from the fundamental discrete-time dynamics $x_{k+1}=x_k+u_k$, the quadratic cost construction for finite-horizon optimal control, and the Bellman principle of optimality, formulate the resulting unconstrained MPC problem and solve it exactly to obtain the optimal first control move $u_0^\\star$ to be applied at $k=0$. Give your answer as an exact simplified rational number with no rounding and no units.",
            "solution": "The problem is valid as it is a well-posed, scientifically-grounded exercise in discrete-time optimal control, a standard topic in engineering and applied mathematics. It provides a complete and consistent set of givens, with no contradictions or ambiguities. We shall proceed with a formal solution.\n\nThe problem is to find the optimal first control action, $u_0^\\star$, for a finite-horizon, unconstrained Model Predictive Control (MPC) problem. The system is a scalar discrete-time integrator with dynamics given by:\n$$x_{k+1} = x_k + u_k$$\nThe optimization is performed over a horizon of length $N=2$. The objective is to minimize a quadratic cost function. The stage cost at step $k$ is $L(x_k, u_k) = (x_k - x^{\\text{ref}})^2 Q + u_k^2 R$, and the terminal cost is $V_f(x_N) = (x_N - x^{\\text{ref}})^2 Q_f$. The total cost to be minimized is:\n$$J = V_f(x_N) + \\sum_{k=0}^{N-1} L(x_k, u_k)$$\nThe given parameters are:\n- Horizon: $N=2$\n- Reference state: $x^{\\text{ref}}=0$\n- State weight: $Q=1$\n- Control weight: $R=1$\n- Terminal weight: $Q_f=1$\n- Initial state: $x_0=2$\n\nSubstituting these values, the cost function becomes:\n$$J(u_0, u_1) = x_2^2 + \\sum_{k=0}^{1} (x_k^2 + u_k^2) = x_2^2 + x_1^2 + u_1^2 + x_0^2 + u_0^2$$\nWe seek to find the control sequence $(u_0^\\star, u_1^\\star)$ that minimizes this cost, subject to the system dynamics. The MPC paradigm dictates that only the first element of this sequence, $u_0^\\star$, is applied to the system.\n\nThe standard method for solving such problems is dynamic programming, which applies the Bellman principle of optimality. We work backward in time from the final step, $k=N=2$.\n\nAt the final time step, $k=2$, the cost-to-go is the terminal cost:\n$$J_2(x_2) = x_2^2 Q_f = 1 \\cdot x_2^2 = x_2^2$$\nThis is a quadratic form $J_2(x_2) = P_2 x_2^2$, where $P_2=1$.\n\nNext, we step back to $k=1$. The optimal cost-to-go from state $x_1$ is given by the Bellman equation:\n$$J_1(x_1) = \\min_{u_1} \\{ L(x_1, u_1) + J_2(x_2) \\}$$\nSubstituting the expressions for the stage cost and the cost-to-go $J_2$, and using the system dynamics $x_2=x_1+u_1$:\n$$J_1(x_1) = \\min_{u_1} \\{ x_1^2 + u_1^2 + (x_1 + u_1)^2 \\}$$\nTo find the control $u_1$ that minimizes this expression, we take the derivative with respect to $u_1$ and set it to zero:\n$$\\frac{\\partial}{\\partial u_1} \\left( x_1^2 + u_1^2 + (x_1 + u_1)^2 \\right) = 2u_1 + 2(x_1 + u_1) = 0$$\n$$2u_1 + 2x_1 + 2u_1 = 0$$\n$$4u_1 = -2x_1$$\n$$u_1^\\star(x_1) = -\\frac{1}{2}x_1$$\nThis is the optimal feedback control law at $k=1$. Substituting this back into the expression for $J_1(x_1)$ gives the optimal cost-to-go from state $x_1$:\n$$J_1(x_1) = x_1^2 + \\left(-\\frac{1}{2}x_1\\right)^2 + \\left(x_1 - \\frac{1}{2}x_1\\right)^2 = x_1^2 + \\frac{1}{4}x_1^2 + \\left(\\frac{1}{2}x_1\\right)^2$$\n$$J_1(x_1) = x_1^2 + \\frac{1}{4}x_1^2 + \\frac{1}{4}x_1^2 = \\left(1 + \\frac{1}{2}\\right)x_1^2 = \\frac{3}{2}x_1^2$$\nThis is a quadratic form $J_1(x_1) = P_1 x_1^2$, where $P_1 = \\frac{3}{2}$.\n\nFinally, we step back to the initial time step, $k=0$. The optimal cost-to-go from the initial state $x_0$ is:\n$$J_0(x_0) = \\min_{u_0} \\{ L(x_0, u_0) + J_1(x_1) \\}$$\nUsing the system dynamics $x_1=x_0+u_0$ and the expression for $J_1(x_1)$:\n$$J_0(x_0) = \\min_{u_0} \\left\\{ x_0^2 + u_0^2 + \\frac{3}{2}(x_0+u_0)^2 \\right\\}$$\nTo find the optimal control $u_0^\\star$, we take the derivative with respect to $u_0$ and set it to zero:\n$$\\frac{\\partial}{\\partial u_0} \\left( x_0^2 + u_0^2 + \\frac{3}{2}(x_0+u_0)^2 \\right) = 2u_0 + \\frac{3}{2} \\cdot 2(x_0+u_0) = 0$$\n$$2u_0 + 3(x_0+u_0) = 0$$\n$$2u_0 + 3x_0 + 3u_0 = 0$$\n$$5u_0 = -3x_0$$\n$$u_0^\\star(x_0) = -\\frac{3}{5}x_0$$\nThis is the optimal feedback control law for the first step. The problem asks for the specific control action to be applied at $k=0$, given the initial state $x_0=2$. We substitute this value into the control law:\n$$u_0^\\star = -\\frac{3}{5} \\cdot 2 = -\\frac{6}{5}$$\nThe optimal first control move is $-\\frac{6}{5}$.",
            "answer": "$$\n\\boxed{-\\frac{6}{5}}\n$$"
        }
    ]
}