{
    "hands_on_practices": [
        {
            "introduction": "A cornerstone of predictive maintenance is the ability to mathematically model the process of failure. This exercise provides foundational practice with the Weibull distribution, a versatile and widely used model in reliability engineering for describing a component's lifetime. By deriving and interpreting the reliability and hazard functions, you will gain a deeper understanding of how to characterize different failure modes—such as infant mortality, random failures, and wear-out—and see how these statistical concepts directly inform the design of an effective maintenance policy .",
            "id": "4216187",
            "problem": "A rotating asset in a Cyber-Physical System (CPS) is monitored by a Digital Twin that performs predictive maintenance by estimating the instantaneous risk of failure from the asset’s inferred time-to-failure distribution. Suppose the time-to-failure random variable $T$ follows a Weibull distribution with shape parameter $k0$ and scale parameter $\\lambda0$, with probability density function $f(t)$ on $t \\ge 0$ given by\n$$\nf(t) = \\frac{k}{\\lambda}\\left(\\frac{t}{\\lambda}\\right)^{k-1}\\exp\\!\\left(-\\left(\\frac{t}{\\lambda}\\right)^{k}\\right).\n$$\nThe Digital Twin uses the fundamental definitions of reliability and hazard:\n- Reliability $R(t)$ is $R(t) = \\mathbb{P}(Tt)$.\n- Hazard $h(t)$ is $h(t) = \\frac{f(t)}{R(t)}$.\n\nA hazard-threshold policy is adopted: preemptive replacement is triggered at the first time $t_{\\eta}$ when the instantaneous hazard equals a given threshold $\\eta0$ (measured in events per hour). The system time unit is hours.\n\nTasks:\n1. Starting from the above $f(t)$ and the definitions $R(t) = \\mathbb{P}(Tt)$ and $h(t) = \\frac{f(t)}{R(t)}$, derive closed-form expressions for $R(t)$ and $h(t)$ for all $t \\ge 0$.\n2. For $k \\ne 1$, solve $h(t_{\\eta}) = \\eta$ for $t_{\\eta}$ and express $t_{\\eta}$ in hours as a closed-form symbolic expression in terms of $k$, $\\lambda$, and $\\eta$. Then compute $R(t_{\\eta})$ as a closed-form symbolic expression in terms of $k$, $\\lambda$, and $\\eta$.\n3. Briefly interpret, in the context of maintenance policy design, the qualitative meaning of the cases $k1$, $k=1$, and $k1$ in terms of how the hazard $h(t)$ evolves with time and what that implies for scheduling inspections and replacements.\n\nReport your final answer as a single composite expression containing both $t_{\\eta}$ and $R(t_{\\eta})$ (in that order) using the most simplified closed form. Express $t_{\\eta}$ in hours. No numerical rounding is required. The final answer must be a real-valued symbolic expression and must not include units.",
            "solution": "The problem statement is evaluated to be valid. It is scientifically grounded in the principles of reliability engineering and statistical modeling, using standard definitions for the Weibull distribution, reliability function, and hazard function. The problem is well-posed, with all necessary information and definitions provided to derive a unique, meaningful solution. It is free of ambiguity, contradiction, and factual error.\n\nThe tasks will be addressed in sequence.\n\nTask 1: Derivation of the reliability function $R(t)$ and the hazard function $h(t)$.\n\nThe reliability function $R(t)$ is defined as the probability that the time-to-failure $T$ is greater than some time $t$. It is the complement of the cumulative distribution function (CDF), $F(t) = \\mathbb{P}(T \\le t)$. Thus, $R(t) = 1 - F(t) = \\int_t^\\infty f(u) \\, du$.\nGiven the probability density function (PDF) $f(t)$ for the Weibull distribution:\n$$\nf(t) = \\frac{k}{\\lambda}\\left(\\frac{t}{\\lambda}\\right)^{k-1}\\exp\\!\\left(-\\left(\\frac{t}{\\lambda}\\right)^{k}\\right) \\quad \\text{for } t \\ge 0\n$$\nWe compute the integral for $R(t)$:\n$$\nR(t) = \\int_t^\\infty \\frac{k}{\\lambda}\\left(\\frac{u}{\\lambda}\\right)^{k-1}\\exp\\!\\left(-\\left(\\frac{u}{\\lambda}\\right)^{k}\\right) du\n$$\nTo solve this integral, we use the substitution method. Let $x = \\left(\\frac{u}{\\lambda}\\right)^{k}$. The differential is then $dx = k\\left(\\frac{u}{\\lambda}\\right)^{k-1} \\cdot \\frac{1}{\\lambda} \\, du$. We must also change the limits of integration. As $u \\to t$, the lower limit becomes $x = (t/\\lambda)^{k}$. As $u \\to \\infty$, the upper limit becomes $x \\to \\infty$.\nSubstituting into the integral:\n$$\nR(t) = \\int_{(t/\\lambda)^{k}}^{\\infty} \\exp(-x) \\, dx\n$$\nThis is a standard exponential integral:\n$$\nR(t) = \\left[ -\\exp(-x) \\right]_{(t/\\lambda)^{k}}^{\\infty} = \\lim_{b \\to \\infty} (-\\exp(-b)) - (-\\exp(-(t/\\lambda)^{k}))\n$$\nSince $\\lim_{b \\to \\infty} \\exp(-b) = 0$, this simplifies to:\n$$\nR(t) = \\exp\\!\\left(-\\left(\\frac{t}{\\lambda}\\right)^{k}\\right)\n$$\nThis is the closed-form expression for the reliability function for $t \\ge 0$.\n\nNext, we derive the hazard function $h(t)$, which is defined as $h(t) = \\frac{f(t)}{R(t)}$.\nUsing the given expression for $f(t)$ and the derived expression for $R(t)$:\n$$\nh(t) = \\frac{\\frac{k}{\\lambda}\\left(\\frac{t}{\\lambda}\\right)^{k-1}\\exp\\!\\left(-\\left(\\frac{t}{\\lambda}\\right)^{k}\\right)}{\\exp\\!\\left(-\\left(\\frac{t}{\\lambda}\\right)^{k}\\right)}\n$$\nThe exponential terms in the numerator and denominator cancel out, yielding:\n$$\nh(t) = \\frac{k}{\\lambda}\\left(\\frac{t}{\\lambda}\\right)^{k-1}\n$$\nThis is the closed-form expression for the instantaneous hazard function for $t \\ge 0$.\n\nTask 2: Solve for the replacement time $t_{\\eta}$ and the reliability at that time, $R(t_{\\eta})$, for $k \\ne 1$.\n\nThe replacement time $t_{\\eta}$ is defined by the condition $h(t_{\\eta}) = \\eta$, where $\\eta  0$ is a constant hazard threshold.\n$$\n\\frac{k}{\\lambda}\\left(\\frac{t_{\\eta}}{\\lambda}\\right)^{k-1} = \\eta\n$$\nWe solve this equation for $t_{\\eta}$. First, isolate the term containing $t_{\\eta}$:\n$$\n\\left(\\frac{t_{\\eta}}{\\lambda}\\right)^{k-1} = \\frac{\\eta \\lambda}{k}\n$$\nSince the problem specifies $k \\ne 1$, the exponent $(k-1)$ is non-zero, and we can take the $(1/(k-1))$-th power of both sides:\n$$\n\\frac{t_{\\eta}}{\\lambda} = \\left(\\frac{\\eta \\lambda}{k}\\right)^{\\frac{1}{k-1}}\n$$\nMultiplying by $\\lambda$ gives the final expression for $t_{\\eta}$ in hours:\n$$\nt_{\\eta} = \\lambda \\left(\\frac{\\eta \\lambda}{k}\\right)^{\\frac{1}{k-1}}\n$$\nNext, we compute the reliability at this specific time, $R(t_{\\eta})$. We use the derived expression for $R(t)$:\n$$\nR(t_{\\eta}) = \\exp\\!\\left(-\\left(\\frac{t_{\\eta}}{\\lambda}\\right)^{k}\\right)\n$$\nFrom the previous step, we have an expression for the term $t_{\\eta}/\\lambda$:\n$$\n\\frac{t_{\\eta}}{\\lambda} = \\left(\\frac{\\eta \\lambda}{k}\\right)^{\\frac{1}{k-1}}\n$$\nWe raise this expression to the power of $k$:\n$$\n\\left(\\frac{t_{\\eta}}{\\lambda}\\right)^{k} = \\left[ \\left(\\frac{\\eta \\lambda}{k}\\right)^{\\frac{1}{k-1}} \\right]^k = \\left(\\frac{\\eta \\lambda}{k}\\right)^{\\frac{k}{k-1}}\n$$\nSubstituting this back into the reliability function:\n$$\nR(t_{\\eta}) = \\exp\\!\\left( - \\left(\\frac{\\eta \\lambda}{k}\\right)^{\\frac{k}{k-1}} \\right)\n$$\nThis provides the closed-form symbolic expressions for both $t_{\\eta}$ and $R(t_{\\eta})$.\n\nTask 3: Interpretation of the shape parameter $k$.\n\nThe behavior of the hazard function $h(t) = \\frac{k}{\\lambda}(\\frac{t}{\\lambda})^{k-1}$ is determined by the sign of the exponent $k-1$.\n- Case $k  1$: In this case, the exponent $k-1$ is negative. Thus, $h(t)$ is a decreasing function of time $t$ for $t  0$. This models \"infant mortality\" or early-life failures. The risk of failure is highest at the beginning and decreases as the component operates. This implies that if an asset survives its initial period, it becomes more reliable. For maintenance policy, this suggests that age-based preventive replacement is counterproductive; a burn-in period might be beneficial, and other maintenance strategies (e.g., corrective maintenance upon failure) are more appropriate.\n- Case $k = 1$: The exponent $k-1$ is zero. The hazard function becomes constant: $h(t) = \\frac{1}{\\lambda}(\\frac{t}{\\lambda})^{0} = \\frac{1}{\\lambda}$. This corresponds to the exponential distribution. The risk of failure is constant over time, indicating that failures are random and memoryless. The asset does not \"age\" in terms of failure risk. For maintenance policy, this means that preventive replacement at a fixed age offers no advantage over replacement upon failure, as an old component is no more likely to fail in the next hour than a new one.\n- Case $k  1$: The exponent $k-1$ is positive. Thus, $h(t)$ is an increasing function of time $t$. This models failures due to aging and wear-out, which is common for mechanical components. The risk of failure increases with operational age. For maintenance policy, this is the classic scenario where predictive and preventive maintenance are most effective. A hazard-threshold policy is justified because it allows for component replacement before the risk of failure becomes unacceptably high, thereby optimizing the trade-off between utilizing the full life of a component and avoiding costly in-service failures.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\lambda \\left(\\frac{\\eta \\lambda}{k}\\right)^{\\frac{1}{k-1}}  \\exp\\left( - \\left(\\frac{\\eta \\lambda}{k}\\right)^{\\frac{k}{k-1}} \\right)\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "While statistical distributions describe the probability of failure over time, a digital twin must often track the unobservable health of a component in real-time. This practice introduces the Kalman filter, a fundamental algorithm for state estimation that lies at the heart of many digital twins. By deriving the steady-state Kalman gain for a simple health degradation model, you will uncover the elegant logic through which the filter optimally fuses predictions from an internal model with noisy sensor measurements to produce the best possible estimate of the true health state .",
            "id": "4216218",
            "problem": "A digital twin for predictive maintenance estimates a scalar health index that drifts due to gradual wear. The health dynamics and sensing are modeled as a linear Gaussian state-space system: the latent health state $x_{t} \\in \\mathbb{R}$ evolves according to $x_{t+1} = x_{t} + w_{t}$ and the sensor produces $y_{t} = x_{t} + v_{t}$, where the process noise $w_{t} \\sim \\mathcal{N}(0,Q)$ and the measurement noise $v_{t} \\sim \\mathcal{N}(0,R)$ are mutually independent, independent over time, and independent of the initial state, with $Q  0$ and $R  0$ known. In steady operation of the digital twin, the estimator is required to achieve optimal linear minimum mean-square error performance under these assumptions.\n\nStarting from the fundamental definitions of prediction and update error covariances for a linear estimator and the mean-square error optimality condition for the scalar linear Gaussian case, derive the steady-state error covariance and then the steady-state Kalman gain $K$ associated with the innovation-based linear update. Assume the steady-state limit of the error covariance exists and is finite, and do not assume any particular value for $Q$ or $R$ beyond positivity.\n\nProvide the final steady-state Kalman gain $K$ as a closed-form analytic expression in terms of $Q$ and $R$. No numerical evaluation is required, and no units are needed. The final answer must be a single analytic expression.",
            "solution": "The problem is first validated against the specified criteria.\n\n**Step 1: Extract Givens**\n- The latent health state $x_t \\in \\mathbb{R}$ evolves according to the process model: $x_{t+1} = x_{t} + w_{t}$.\n- The sensor measurement $y_t$ is given by the measurement model: $y_{t} = x_{t} + v_{t}$.\n- The process noise is a zero-mean Gaussian random variable: $w_{t} \\sim \\mathcal{N}(0,Q)$, with variance $Q  0$.\n- The measurement noise is a zero-mean Gaussian random variable: $v_{t} \\sim \\mathcal{N}(0,R)$, with variance $R  0$.\n- The noise processes $w_t$ and $v_t$ are mutually independent, independent over time, and independent of the initial state.\n- The objective is to find the optimal linear minimum mean-square error estimator in steady state.\n- It is assumed that the steady-state limit of the error covariance exists and is finite.\n- The task is to derive the steady-state Kalman gain $K$ as a closed-form analytic expression in terms of $Q$ and $R$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem describes a scalar linear Gaussian state-space model, a fundamental construct in estimation theory and control systems. The derivation of the steady-state Kalman filter is a standard and well-established procedure based on rigorous mathematical principles of probability and optimization. The application context of a digital twin for predictive maintenance is a modern and scientifically valid use of such models. The problem is scientifically sound.\n- **Well-Posed:** The problem is well-posed. The system is described by a standard random walk process with noisy measurements. For such a system where the process noise covariance $Q$ and measurement noise covariance $R$ are positive, the discrete-time algebraic Riccati equation associated with the Kalman filter has a unique, positive-definite (positive, in this scalar case) stabilizing solution. The problem explicitly assumes this steady-state limit exists, ensuring a unique solution can be found.\n- **Objective:** The problem is stated in precise, objective, and unambiguous mathematical language. Terms like \"linear minimum mean-square error,\" \"error covariance,\" and \"Kalman gain\" have precise definitions in the field.\n\n**Step 3: Verdict and Action**\nThe problem is valid. It is scientifically grounded, well-posed, and objective, with all necessary information provided for a unique solution. The derivation process can proceed.\n\n**Derivation of the Steady-State Kalman Gain**\n\nThe system is a linear time-invariant state-space model:\n$$x_{t+1} = F x_{t} + w_{t}$$\n$$y_{t} = H x_{t} + v_{t}$$\nFrom the problem statement, we have a scalar system where the state transition matrix is $F=1$ and the observation matrix is $H=1$. The noise covariances are the scalars $Q$ and $R$.\n\nThe Kalman filter recursively computes the state estimate by a two-step process: prediction and update. The error covariances associated with these steps are central to the filter's performance. Let $P_{t|t-1}$ be the predicted (a priori) error covariance and $P_{t|t}$ be the updated (a posteriori) error covariance.\n\nThe standard Kalman filter equations for the error covariances are:\n1.  **Prediction:** $P_{t|t-1} = F P_{t-1|t-1} F^T + Q$\n2.  **Update:** $P_{t|t} = (I - K_t H) P_{t|t-1}$\n\nThe Kalman gain $K_t$ is chosen to minimize the mean-square error, and is given by:\n$$K_t = P_{t|t-1} H^T (H P_{t|t-1} H^T + R)^{-1}$$\n\nFor our specific scalar case ($F=1$, $H=1$), these equations simplify to:\n1.  **Prediction:** $P_{t|t-1} = P_{t-1|t-1} + Q$\n2.  **Kalman Gain:** $K_t = P_{t|t-1} (P_{t|t-1} + R)^{-1} = \\frac{P_{t|t-1}}{P_{t|t-1} + R}$\n3.  **Update:** $P_{t|t} = (1 - K_t) P_{t|t-1}$\n\nWe can combine these to derive the discrete-time Riccati equation, which describes the propagation of the error covariance. Substituting the expression for $K_t$ into the update equation for $P_{t|t}$:\n$$P_{t|t} = \\left(1 - \\frac{P_{t|t-1}}{P_{t|t-1} + R}\\right) P_{t|t-1} = \\left(\\frac{P_{t|t-1} + R - P_{t|t-1}}{P_{t|t-1} + R}\\right) P_{t|t-1} = \\frac{R P_{t|t-1}}{P_{t|t-1} + R}$$\nNow, we substitute the prediction equation $P_{t|t-1} = P_{t-1|t-1} + Q$ into this result:\n$$P_{t|t} = \\frac{R (P_{t-1|t-1} + Q)}{P_{t-1|t-1} + Q + R}$$\nThis is the recursive relation for the a posteriori error covariance.\n\nIn steady state, the error covariance becomes constant. We define the steady-state a posteriori error covariance as $P = \\lim_{t \\to \\infty} P_{t|t}$. Thus, in the limit, $P_{t|t} = P_{t-1|t-1} = P$. The Riccati equation becomes the discrete-time algebraic Riccati equation (DARE):\n$$P = \\frac{R (P + Q)}{P + Q + R}$$\nTo solve for $P$, we rearrange this algebraic equation:\n$$P(P + Q + R) = R(P + Q)$$\n$$P^2 + PQ + PR = RP + RQ$$\n$$P^2 + PQ - RQ = 0$$\nThis is a quadratic equation in $P$ of the form $aP^2 + bP + c = 0$, with $a=1$, $b=Q$, and $c=-RQ$. Using the quadratic formula, $P = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$:\n$$P = \\frac{-Q \\pm \\sqrt{Q^2 - 4(1)(-RQ)}}{2} = \\frac{-Q \\pm \\sqrt{Q^2 + 4QR}}{2}$$\nSince $P$ represents a variance, it must be non-negative ($P \\ge 0$). Given that $Q  0$ and $R  0$, we have $\\sqrt{Q^2 + 4QR}  \\sqrt{Q^2} = Q$. Therefore, the root with the minus sign, $-Q - \\sqrt{Q^2 + 4QR}$, is negative and must be discarded as non-physical. The only valid solution is:\n$$P = \\frac{-Q + \\sqrt{Q^2 + 4QR}}{2}$$\nNow we must find the steady-state Kalman gain, $K = \\lim_{t \\to \\infty} K_t$. We can use the relationships derived earlier. An elegant approach is to first establish a direct link between $P$ and $K$.\nFrom the gain equation, $K_t = \\frac{P_{t|t-1}}{P_{t|t-1} + R}$, we have $K_t(P_{t|t-1} + R) = P_{t|t-1}$, which rearranges to $P_{t|t-1}(1 - K_t) = K_t R$.\nFrom the update equation, $P_{t|t} = (1 - K_t)P_{t|t-1}$.\nComparing these two expressions, we find a simple relationship:\n$$P_{t|t} = K_t R$$\nIn steady state, this becomes $P = KR$.\n\nWe can now substitute $P = KR$ into the quadratic equation we found for $P$:\n$$P^2 + QP - RQ = 0$$\n$$(KR)^2 + Q(KR) - RQ = 0$$\n$$K^2 R^2 + KQR - QR = 0$$\nSince $R  0$, we can divide the entire equation by $R$:\n$$R K^2 + Q K - Q = 0$$\nThis is a quadratic equation for the steady-state gain $K$. We solve for $K$ using the quadratic formula, with $a=R$, $b=Q$, $c=-Q$:\n$$K = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a} = \\frac{-Q \\pm \\sqrt{Q^2 - 4(R)(-Q)}}{2R}$$\n$$K = \\frac{-Q \\pm \\sqrt{Q^2 + 4QR}}{2R}$$\nThe Kalman gain $K_t$ must be in the range $[0, 1]$. In our case, since $P_{t|t-1}  0$ and $R  0$, $K_t = \\frac{P_{t|t-1}}{P_{t|t-1} + R}$ is strictly in $(0, 1)$, so the steady-state gain $K$ must also be positive. As before, the term $\\sqrt{Q^2 + 4QR}$ is greater than $Q$, so the negative root yields a negative value for $K$, which is physically invalid. Therefore, we must choose the positive root:\n$$K = \\frac{-Q + \\sqrt{Q^2 + 4QR}}{2R}$$\nThis expression for $K$ is the required closed-form solution in terms of $Q$ and $R$.",
            "answer": "$$\\boxed{\\frac{-Q + \\sqrt{Q^{2} + 4QR}}{2R}}$$"
        },
        {
            "introduction": "In real-world applications, the precise parameters of a system's degradation model are often unknown, and sensor data may be incomplete. This advanced exercise tackles the highly practical challenge of joint state and parameter estimation from missing data, a scenario where adaptive digital twins truly shine. You will implement the Expectation-Maximization (EM) algorithm, which cleverly combines the Kalman smoother (in the E-step) with maximum likelihood estimation (in the M-step), allowing the digital twin to simultaneously learn the underlying system dynamics while inferring the hidden health trajectory .",
            "id": "4216256",
            "problem": "A digital twin for predictive maintenance models a hidden scalar health state that evolves over discrete time and is only partially observed due to missing sensor readings. Consider the following Linear Gaussian State-Space Model (LGSSM) for a single asset over $T$ time steps, where the hidden health state is denoted by $x_t$ and the observation by $y_t$:\n$$\nx_1 \\sim \\mathcal{N}(\\mu_0, P_0), \\quad x_{t+1} = a x_t + u + w_t, \\quad w_t \\sim \\mathcal{N}(0,q), \\quad t = 1,\\dots,T-1,\n$$\n$$\ny_t = x_t + v_t, \\quad v_t \\sim \\mathcal{N}(0,r), \\quad t = 1,\\dots,T.\n$$\nSome $y_t$ values are missing according to a known binary mask $m_t \\in \\{0,1\\}$, where $m_t = 1$ indicates an observed $y_t$ and $m_t = 0$ indicates a missing observation. The goal is joint state and parameter estimation using Expectation-Maximization (EM), estimating the parameter vector $\\theta = (a,u,q,r,\\mu_0,P_0)$ while inferring the posterior distribution of the states.\n\nYou must:\n- Derive from first principles the Expectation step (E-step) and the Maximization step (M-step) for this model, starting from the complete-data log-likelihood and Bayes' rule for Gaussian distributions. Your derivation must begin with fundamental definitions of Gaussian densities and linear state-space propagation, and may use the well-tested principle that the posterior of a linear-Gaussian model is Gaussian. You must not invoke pre-quoted shortcut formulas without justification; instead, show how the sufficient statistics emerge from the linear-Gaussian structure and how they are computed by Kalman filtering and Rauch-Tung-Striebel smoothing.\n- Implement a complete, runnable program that performs EM with a fixed number of iterations $I$ on synthetic datasets defined below, handling missing observations rigorously in the Kalman update. Use numerically stable operations and enforce $q  0$, $r  0$, $P_0  0$ by lower-bounding with a small $\\epsilon$.\n\nImplementation requirements:\n- Use $I=50$ EM iterations, with no additional stopping criterion.\n- Initialize parameters with $(a^{(0)},u^{(0)},q^{(0)},r^{(0)},\\mu_0^{(0)},P_0^{(0)}) = (0.5,0.0,0.1,0.1,0.0,1.0)$.\n- In the E-step, compute the smoothed first and second moments $E[x_t \\mid y_{1:T}]$, $E[x_t^2 \\mid y_{1:T}]$, and $E[x_t x_{t+1} \\mid y_{1:T}]$ for $t=1,\\dots,T-1$ by combining a forward Kalman filter and a backward Rauch-Tung-Striebel smoother, correctly handling missing observations ($m_t=0$ implies no measurement update at time $t$).\n- In the M-step, maximize the expected complete-data log-likelihood with respect to $\\theta$, expressing the updates in terms of the sufficient statistics computed in the E-step. You must ensure the updates are derived from minimizing expected quadratic forms implied by the Gaussian model.\n- After $I$ iterations, output the final parameter estimates $(\\hat a,\\hat u,\\hat q,\\hat r,\\hat \\mu_0,\\hat P_0)$ for each test case, rounding each value to exactly $6$ decimal places.\n\nData generation and test suite:\n- For each test case, generate a trajectory by sampling from the LGSSM using the true parameters, with the specified random seed to set the pseudo-random number generator. The missingness mask is either random Bernoulli with a stated rate or deterministic as specified. Use the following three test cases:\n    1. Test A (mildly stable degradation with moderate noise): $T = 50$, true $(a,u,q,r,\\mu_0,P_0) = (0.98,0.05,0.01,0.04,0.0,0.5)$, missingness mask $m_t \\sim \\text{Bernoulli}(0.8)$ independently per time step with seed $7$ for both process and observation noise as well as missingness generation.\n    2. Test B (random walk with drift and low process noise, high missingness): $T = 80$, true $(a,u,q,r,\\mu_0,P_0) = (1.0,0.02,0.001,0.01,-0.5,1.0)$, missingness mask $m_t \\sim \\text{Bernoulli}(0.5)$ with seed $11$.\n    3. Test C (decaying health without drift, structured missingness): $T = 40$, true $(a,u,q,r,\\mu_0,P_0) = (0.90,0.0,0.05,0.02,1.0,0.2)$, missingness mask given by $m_t = 1$ for odd $t$ and $m_t = 0$ for even $t$ (deterministic), with seed $13$ for process and observation noise.\n\nRequired final output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element should be a list of six floats corresponding to $(\\hat a,\\hat u,\\hat q,\\hat r,\\hat \\mu_0,\\hat P_0)$ for one test case, in the order Test A, Test B, Test C. For example, the output must have the form\n\"[[a_A,u_A,q_A,r_A,mu0_A,P0_A],[a_B,u_B,q_B,r_B,mu0_B,P0_B],[a_C,u_C,q_C,r_C,mu0_C,P0_C]]\"\nwith each float rounded to exactly $6$ decimal places.",
            "solution": "The problem requires the derivation and implementation of the Expectation-Maximization (EM) algorithm to estimate the parameters $\\theta = (a, u, q, r, \\mu_0, P_0)$ of a scalar Linear Gaussian State-Space Model (LGSSM) with missing observations.\n\nThe model is defined by:\n- Initial state: $x_1 \\sim \\mathcal{N}(\\mu_0, P_0)$\n- State transition: $x_{t+1} = a x_t + u + w_t$, with $w_t \\sim \\mathcal{N}(0, q)$ for $t = 1, \\dots, T-1$\n- Observation: $y_t = x_t + v_t$, with $v_t \\sim \\mathcal{N}(0, r)$ for $t = 1, \\dots, T$\n- Missingness: A binary mask $m_t \\in \\{0, 1\\}$ indicates if $y_t$ is observed ($m_t=1$) or missing ($m_t=0$).\n\nThe EM algorithm is an iterative procedure for finding maximum likelihood estimates in models with latent variables. It alternates between an Expectation (E) step and a Maximization (M) step.\n\nLet $X = \\{x_1, \\dots, x_T\\}$ be the latent states and $Y_{obs} = \\{y_t \\mid m_t=1, t=1, \\dots, T\\}$ be the observed data. The goal is to maximize the marginal log-likelihood $\\log p(Y_{obs} | \\theta)$, which is generally intractable. EM maximizes the expected complete-data log-likelihood, $Q(\\theta | \\theta^{(k)}) = E_{X|Y_{obs}, \\theta^{(k)}}[\\log p(X, Y_{obs} | \\theta)]$, where $\\theta^{(k)}$ are the parameter estimates at iteration $k$.\n\n**1. Complete-Data Log-Likelihood**\n\nThe joint probability of the complete data $(X, Y_{obs})$ is given by the product of the probabilities of each component of the model:\n$$p(X, Y_{obs} | \\theta) = p(x_1 | \\mu_0, P_0) \\left( \\prod_{t=1}^{T-1} p(x_{t+1} | x_t, a, u, q) \\right) \\left( \\prod_{t=1, m_t=1}^{T} p(y_t | x_t, r) \\right)$$\nThe corresponding log-likelihood, $\\mathcal{L}_c(\\theta) = \\log p(X, Y_{obs} | \\theta)$, is:\n$$ \\mathcal{L}_c(\\theta) = \\log p(x_1 | \\mu_0, P_0) + \\sum_{t=1}^{T-1} \\log p(x_{t+1} | x_t, a, u, q) + \\sum_{t=1}^{T} m_t \\log p(y_t | x_t, r) $$\nSubstituting the Gaussian probability density functions, $p(z | \\mu, \\sigma^2) = (2\\pi\\sigma^2)^{-1/2} \\exp(-\\frac{(z-\\mu)^2}{2\\sigma^2})$, we get:\n$$ \\mathcal{L}_c(\\theta) = -\\frac{1}{2}\\log(2\\pi P_0) - \\frac{(x_1 - \\mu_0)^2}{2 P_0} - \\sum_{t=1}^{T-1} \\left( \\frac{1}{2}\\log(2\\pi q) + \\frac{(x_{t+1} - a x_t - u)^2}{2q} \\right) - \\sum_{t=1}^{T} m_t \\left( \\frac{1}{2}\\log(2\\pi r) + \\frac{(y_t - x_t)^2}{2r} \\right) $$\nThis can be rearranged by separating terms dependent on the parameters:\n$$ \\mathcal{L}_c(\\theta) = -\\frac{1}{2}\\log P_0 - \\frac{(x_1 - \\mu_0)^2}{2 P_0} - \\frac{T-1}{2}\\log q - \\frac{1}{2q}\\sum_{t=1}^{T-1}(x_{t+1} - a x_t - u)^2 - \\frac{\\sum m_t}{2}\\log r - \\frac{1}{2r}\\sum_{t=1}^{T} m_t (y_t - x_t)^2 + C $$\nwhere $C$ is a constant independent of the parameters $\\theta$.\n\n**2. The E-Step: Computing Sufficient Statistics**\n\nThe E-step computes the expectation of $\\mathcal{L}_c(\\theta)$ with respect to the posterior distribution of the latent states $p(X | Y_{obs}, \\theta^{(k)})$. We will denote this expectation as $E_k[\\cdot]$. The relevant terms in $E_k[\\mathcal{L}_c(\\theta)]$ involve expectations of functions of the latent states $x_t$. These are the sufficient statistics for the M-step.\nLet us define the following smoothed expectations, which are conditional on all observations $Y_{obs}$ and the current parameter set $\\theta^{(k)}$:\n- $\\hat{x}_{t|T} = E_k[x_t]$\n- $E_k[x_t^2] = \\text{Var}_k(x_t) + (E_k[x_t])^2 = P_{t|T} + \\hat{x}_{t|T}^2$\n- $E_k[x_t x_{t+1}] = \\text{Cov}_k(x_t, x_{t+1}) + E_k[x_t]E_k[x_{t+1}] = P_{t+1,t|T} + \\hat{x}_{t+1|T}\\hat{x}_{t|T}$\n\nFor an LGSSM, the posterior distribution $p(X | Y_{obs}, \\theta^{(k)})$ is Gaussian. Its moments can be computed efficiently using a two-pass algorithm consisting of a forward Kalman filter and a backward Rauch-Tung-Striebel (RTS) smoother. All parameters $(a, u, q, r, \\mu_0, P_0)$ in these equations are taken from the current estimate $\\theta^{(k)}$.\n\n**Kalman Filter (Forward Pass):**\nThe filter iteratively computes the filtering distribution $p(x_t|y_{1:t}, \\theta^{(k)}) = \\mathcal{N}(x_t | \\hat{x}_{t|t}, P_{t|t})$.\n- **Initialization ($t=1$):**\n  - Prediction: $\\hat{x}_{1|0} = \\mu_0$, $P_{1|0} = P_0$.\n- **For $t=1, \\dots, T$:**\n  - **Prediction Step (if $t1$):**\n    - $\\hat{x}_{t|t-1} = a \\hat{x}_{t-1|t-1} + u$\n    - $P_{t|t-1} = a^2 P_{t-1|t-1} + q$\n  - **Update Step:**\n    - If observation $y_t$ is available ($m_t = 1$):\n      - Residual: $\\tilde{y}_t = y_t - \\hat{x}_{t|t-1}$\n      - Residual Covariance: $S_t = P_{t|t-1} + r$\n      - Kalman Gain: $K_t = P_{t|t-1} / S_t$\n      - Updated Mean: $\\hat{x}_{t|t} = \\hat{x}_{t|t-1} + K_t \\tilde{y}_t$\n      - Updated Covariance: $P_{t|t} = (1 - K_t) P_{t|t-1}$\n    - If observation $y_t$ is missing ($m_t = 0$):\n      - $\\hat{x}_{t|t} = \\hat{x}_{t|t-1}$\n      - $P_{t|t} = P_{t|t-1}$\n\n**RTS Smoother (Backward Pass):**\nThe smoother computes the smoothing distribution $p(x_t|Y_{obs}, \\theta^{(k)}) = \\mathcal{N}(x_t | \\hat{x}_{t|T}, P_{t|T})$ and the lag-one cross-covariance $P_{t+1,t|T}$.\n- **Initialization ($t=T$):**\n  - $\\hat{x}_{T|T}$ and $P_{T|T}$ are taken from the final step of the Kalman filter.\n- **For $t = T-1, \\dots, 1$:**\n  - Smoother Gain: $J_t = P_{t|t} a / P_{t+1|t}$\n  - Smoothed Mean: $\\hat{x}_{t|T} = \\hat{x}_{t|t} + J_t (\\hat{x}_{t+1|T} - \\hat{x}_{t+1|t})$\n  - Smoothed Covariance: $P_{t|T} = P_{t|t} + J_t^2 (P_{t+1|T} - P_{t+1|t})$\n- **Lag-One Covariance Smoother:** After the backward pass for means and variances, we compute the smoothed lag-one covariances.\n- **For $t = 1, \\dots, T-1$:**\n  - $P_{t+1,t|T} = P_{t+1|T} J_t$\n\nThe E-step concludes by computing the sufficient statistics using these smoothed moments.\n\n**3. The M-Step: Parameter Maximization**\n\nThe M-step updates the parameters $\\theta^{(k+1)}$ by maximizing $Q(\\theta | \\theta^{(k)}) = E_k[\\mathcal{L}_c(\\theta)]$ with respect to $\\theta$.\n\n- **Update for $(\\mu_0, P_0)$:**\nThe relevant part of $Q$ is $E_k[-\\frac{1}{2}\\log P_0 - \\frac{(x_1 - \\mu_0)^2}{2 P_0}]$.\nSetting $\\frac{\\partial Q}{\\partial \\mu_0} = \\frac{1}{P_0}(E_k[x_1] - \\mu_0) = 0$ gives $\\mu_0^{(k+1)} = E_k[x_1] = \\hat{x}_{1|T}$.\nSetting $\\frac{\\partial Q}{\\partial P_0} = -\\frac{1}{2P_0} + \\frac{E_k[(x_1 - \\mu_0)^2]}{2P_0^2} = 0$, and plugging in the new $\\mu_0^{(k+1)}$, we get $P_0^{(k+1)} = E_k[(x_1 - \\hat{x}_{1|T})^2] = P_{1|T}$.\n\n- **Update for $(a, u)$:**\nThe relevant part of $Q$ involves $-\\frac{1}{2q}\\sum_{t=1}^{T-1}E_k[(x_{t+1} - a x_t - u)^2]$. Maximizing this is equivalent to minimizing the sum of squares $J(a,u) = \\sum_{t=1}^{T-1}E_k[(x_{t+1} - a x_t - u)^2]$.\nTaking derivatives w.r.t. $a$ and $u$ and setting to zero yields a linear system:\n$ \\frac{\\partial J}{\\partial u} = -2\\sum_{t=1}^{T-1}(E_k[x_{t+1}] - a E_k[x_t] - u) = 0 $\n$ \\frac{\\partial J}{\\partial a} = -2\\sum_{t=1}^{T-1}E_k[x_t(x_{t+1} - a x_t - u)] = 0 $\nThis leads to the normal equations:\n$ \\begin{pmatrix} \\sum E_k[x_t^2]  \\sum E_k[x_t] \\\\ \\sum E_k[x_t]  \\sum 1 \\end{pmatrix} \\begin{pmatrix} a \\\\ u \\end{pmatrix} = \\begin{pmatrix} \\sum E_k[x_{t+1}x_t] \\\\ \\sum E_k[x_{t+1}] \\end{pmatrix} $\nwhere sums are from $t=1$ to $T-1$. Solving this $2 \\times 2$ system gives the updates $a^{(k+1)}$ and $u^{(k+1)}$.\n\n- **Update for $q$:**\nThe relevant part of $Q$ is $-\\frac{T-1}{2}\\log q - \\frac{1}{2q}\\sum_{t=1}^{T-1}E_k[(x_{t+1} - a x_t - u)^2]$.\nSetting $\\frac{\\partial Q}{\\partial q} = -\\frac{T-1}{2q} + \\frac{1}{2q^2}\\sum E_k[\\dots]^2 = 0$ gives:\n$ q^{(k+1)} = \\frac{1}{T-1} \\sum_{t=1}^{T-1} E_k[(x_{t+1} - a^{(k+1)} x_t - u^{(k+1)})^2] $\nThe expectation can be expanded using the sufficient statistics and the newly computed $a^{(k+1)}, u^{(k+1)}$.\n\n- **Update for $r$:**\nLet $N_{obs} = \\sum_{t=1}^T m_t$. The relevant part of $Q$ is $-\\frac{N_{obs}}{2}\\log r - \\frac{1}{2r}\\sum_{t=1}^{T}m_t E_k[(y_t - x_t)^2]$.\nSetting $\\frac{\\partial Q}{\\partial r} = -\\frac{N_{obs}}{2r} + \\frac{1}{2r^2}\\sum m_t E_k[(y_t-x_t)^2] = 0$ gives:\n$ r^{(k+1)} = \\frac{1}{N_{obs}} \\sum_{t=1}^{T} m_t E_k[(y_t - x_t)^2] = \\frac{1}{N_{obs}} \\sum_{t=1}^{T} m_t ( (y_t - \\hat{x}_{t|T})^2 + P_{t|T} ) $\n\nThe algorithm proceeds by iterating these E and M steps for a fixed number of iterations, ensuring that variance parameters $q, r, P_0$ remain positive by enforcing a small positive lower bound $\\epsilon$.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases.\n    \"\"\"\n    \n    # Small epsilon to enforce positivity of variance parameters\n    EPSILON = 1e-9\n    \n    # Test case definitions\n    test_cases = [\n        {\n            'name': 'Test A',\n            'T': 50,\n            'true_params': {'a': 0.98, 'u': 0.05, 'q': 0.01, 'r': 0.04, 'mu0': 0.0, 'P0': 0.5},\n            'mask_spec': ('bernoulli', 0.8),\n            'seed': 7\n        },\n        {\n            'name': 'Test B',\n            'T': 80,\n            'true_params': {'a': 1.0, 'u': 0.02, 'q': 0.001, 'r': 0.01, 'mu0': -0.5, 'P0': 1.0},\n            'mask_spec': ('bernoulli', 0.5),\n            'seed': 11\n        },\n        {\n            'name': 'Test C',\n            'T': 40,\n            'true_params': {'a': 0.90, 'u': 0.0, 'q': 0.05, 'r': 0.02, 'mu0': 1.0, 'P0': 0.2},\n            'mask_spec': ('deterministic_alt',),\n            'seed': 13\n        }\n    ]\n\n    # Shared settings\n    iterations = 50\n    initial_params = {'a': 0.5, 'u': 0.0, 'q': 0.1, 'r': 0.1, 'mu0': 0.0, 'P0': 1.0}\n    \n    final_results = []\n    \n    def generate_data(T, true_params, mask_spec, seed):\n        \"\"\"Generates synthetic data from the LGSSM.\"\"\"\n        rng = np.random.default_rng(seed)\n        \n        a, u, q, r, mu0, P0 = true_params.values()\n        \n        x = np.zeros(T)\n        y = np.zeros(T)\n        \n        # Generate states\n        x[0] = rng.normal(mu0, np.sqrt(P0))\n        for t in range(T - 1):\n            x[t+1] = a * x[t] + u + rng.normal(0, np.sqrt(q))\n            \n        # Generate observations\n        y = x + rng.normal(0, np.sqrt(r), size=T)\n        \n        # Generate mask\n        mask_type = mask_spec[0]\n        if mask_type == 'bernoulli':\n            p = mask_spec[1]\n            m = rng.binomial(1, p, size=T)\n        elif mask_type == 'deterministic_alt':\n            m = np.zeros(T)\n            m[::2] = 1 # odd t (1, 3, ...) corresponds to index 0, 2, ...\n        \n        y[m == 0] = np.nan # Mark missing values\n        \n        return y, m\n\n    def kalman_filter(y, m, params, T):\n        \"\"\"Performs the forward Kalman filter pass.\"\"\"\n        a, u, q, r, mu0, P0 = params.values()\n\n        x_pred = np.zeros(T)\n        P_pred = np.zeros(T)\n        x_filt = np.zeros(T)\n        P_filt = np.zeros(T)\n\n        # Time t=1 (index 0)\n        x_pred[0] = mu0\n        P_pred[0] = P0\n        \n        if m[0] == 1:\n            K = P_pred[0] / (P_pred[0] + r)\n            x_filt[0] = x_pred[0] + K * (y[0] - x_pred[0])\n            P_filt[0] = (1 - K) * P_pred[0]\n        else:\n            x_filt[0] = x_pred[0]\n            P_filt[0] = P_pred[0]\n\n        # Time t=2...T (index 1...T-1)\n        for t in range(1, T):\n            # Prediction\n            x_pred[t] = a * x_filt[t-1] + u\n            P_pred[t] = a**2 * P_filt[t-1] + q\n            \n            # Update\n            if m[t] == 1:\n                S = P_pred[t] + r\n                if S  0:\n                   K = P_pred[t] / S\n                else:\n                   K = 0.0 # handle potential numerical issues\n                x_filt[t] = x_pred[t] + K * (y[t] - x_pred[t])\n                P_filt[t] = (1 - K) * P_pred[t]\n            else:\n                x_filt[t] = x_pred[t]\n                P_filt[t] = P_pred[t]\n                \n        return {\n            'x_pred': x_pred, 'P_pred': P_pred,\n            'x_filt': x_filt, 'P_filt': P_filt\n        }\n\n    def rts_smoother(filter_results, params, T):\n        \"\"\"Performs the backward RTS smoother pass.\"\"\"\n        a, q = params['a'], params['q']\n        x_pred, P_pred = filter_results['x_pred'], filter_results['P_pred']\n        x_filt, P_filt = filter_results['x_filt'], filter_results['P_filt']\n\n        x_smooth = np.zeros(T)\n        P_smooth = np.zeros(T)\n        P_cov_smooth = np.zeros(T - 1)\n\n        x_smooth[T-1] = x_filt[T-1]\n        P_smooth[T-1] = P_filt[T-1]\n\n        for t in range(T - 2, -1, -1):\n            if P_pred[t+1]  0:\n                J = P_filt[t] * a / P_pred[t+1]\n            else:\n                J = 0.0 # handle potential numerical issues\n\n            x_smooth[t] = x_filt[t] + J * (x_smooth[t+1] - x_pred[t+1])\n            P_smooth[t] = P_filt[t] + J**2 * (P_smooth[t+1] - P_pred[t+1])\n\n        # Lag-one covariance smoother pass\n        # P_t,t-1|T = P_t|T * J_{t-1}\n        for t in range(T - 1, 0, -1):\n            if P_pred[t]  0:\n                 J_prev = P_filt[t-1] * a / P_pred[t]\n            else:\n                 J_prev = 0.0\n            P_cov_smooth[t-1] = P_smooth[t] * J_prev\n\n        return {'x_smooth': x_smooth, 'P_smooth': P_smooth, 'P_cov_smooth': P_cov_smooth}\n\n    def e_step(y, m, params, T):\n        \"\"\"Performs the E-step.\"\"\"\n        filter_results = kalman_filter(y, m, params, T)\n        smoother_results = rts_smoother(filter_results, params, T)\n        \n        x_s = smoother_results['x_smooth']\n        P_s = smoother_results['P_smooth']\n        P_cov_s = smoother_results['P_cov_smooth']\n        \n        Ex = x_s\n        Ex2 = x_s**2 + P_s\n        Exx = np.zeros(T - 1)\n        for t in range(T-1):\n            Exx[t] = x_s[t] * x_s[t+1] + P_cov_s[t]\n\n        return {'Ex': Ex, 'Ex2': Ex2, 'Exx_pair': Exx}\n\n    def m_step(y, m, stats, T, params):\n        \"\"\"Performs the M-step.\"\"\"\n        Ex, Ex2, Exx_pair = stats['Ex'], stats['Ex2'], stats['Exx_pair']\n        \n        # Update mu0, P0\n        new_mu0 = Ex[0]\n        new_P0 = Ex2[0] - Ex[0]**2\n        \n        # Prepare sums for a, u updates\n        S_xx = np.sum(Ex2[:-1])\n        S_x = np.sum(Ex[:-1])\n        S_x_prime_x = np.sum(Exx_pair)\n        S_x_prime = np.sum(Ex[1:])\n        \n        # Update a, u\n        T_minus_1 = T - 1\n        denom = T_minus_1 * S_xx - S_x**2\n        if np.abs(denom)  EPSILON: # Avoid division by zero if states are constant\n            new_a = params['a']\n            new_u = params['u']\n        else:\n            new_a = (T_minus_1 * S_x_prime_x - S_x * S_x_prime) / denom\n            new_u = (S_x_prime * S_xx - S_x * S_x_prime_x) / denom\n        \n        # Update q\n        S_x_prime_x_prime = np.sum(Ex2[1:])\n        term1 = S_x_prime_x_prime\n        term2 = -2 * new_a * S_x_prime_x\n        term3 = -2 * new_u * S_x_prime\n        term4 = new_a**2 * S_xx\n        term5 = 2 * new_a * new_u * S_x\n        term6 = T_minus_1 * new_u**2\n        new_q = (term1 + term2 + term3 + term4 + term5 + term6) / T_minus_1\n        \n        # Update r\n        N_obs = np.sum(m)\n        sum_r = 0\n        for t in range(T):\n            if m[t] == 1:\n                sum_r += y[t]**2 - 2 * y[t] * Ex[t] + Ex2[t]\n        \n        if N_obs  0:\n            new_r = sum_r / N_obs\n        else:\n            new_r = params['r'] # Keep old r if no observations\n            \n        return {\n            'a': new_a, 'u': new_u, \n            'q': np.maximum(new_q, EPSILON), \n            'r': np.maximum(new_r, EPSILON),\n            'mu0': new_mu0, \n            'P0': np.maximum(new_P0, EPSILON)\n        }\n        \n    for case in test_cases:\n        y, m = generate_data(case['T'], case['true_params'], case['mask_spec'], case['seed'])\n        \n        current_params = initial_params.copy()\n        \n        for _ in range(iterations):\n            sufficient_stats = e_step(y, m, current_params, case['T'])\n            current_params = m_step(y, m, sufficient_stats, case['T'], current_params)\n            \n        final_params = list(current_params.values())\n        rounded_params = [f\"{p:.6f}\" for p in final_params]\n        final_results.append(f\"[{','.join(rounded_params)}]\")\n\n    print(f\"[[{','.join(final_results)}]]\")\n\nsolve()\n```"
        }
    ]
}