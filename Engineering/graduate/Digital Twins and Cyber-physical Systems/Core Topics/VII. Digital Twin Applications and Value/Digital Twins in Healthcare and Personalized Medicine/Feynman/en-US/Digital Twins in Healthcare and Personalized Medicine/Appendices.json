{
    "hands_on_practices": [
        {
            "introduction": "The first and most crucial step in creating a functional digital twin is personalization: adapting a general physiological model to reflect the unique biology of an individual. This exercise demonstrates this process by calibrating a standard one-compartment pharmacokinetic (PK) model. Using therapeutic drug monitoring (TDM) measurements, you will estimate key patient-specific parameters—the elimination rate constant $k_e$ and the volume of distribution $V$—transforming a generic model into a predictive, personalized tool.  This practice builds foundational skills in parameter estimation via nonlinear least squares, a core technique in pharmacometrics and digital twin development.",
            "id": "4217319",
            "problem": "A digital twin for individualized vancomycin therapy is constructed using a one-compartment pharmacokinetic (PK) model calibrated by Therapeutic Drug Monitoring (TDM) measurements. Starting from mass balance and first-order elimination, consider the drug amount in the body $A(t)$ and concentration $C(t)$ related by $C(t) = A(t)/V$, where $V$ is the apparent volume of distribution in liters. Intravenous infusion is modeled as a zero-order input with rate $R(t)$ in $\\mathrm{mg}/\\mathrm{h}$ during infusion intervals, and no input otherwise. The elimination process is first-order with elimination rate constant $k_e$ in $\\mathrm{h}^{-1}$. The mass balance yields the ordinary differential equation $dA/dt = -k_e A + R(t)$ and hence $dC/dt = -k_e C + R(t)/V$. Assume TDM observations $y_i$ at times $t_i$ satisfy $y_i = C(t_i) + \\epsilon_i$ with independent Gaussian noise $\\epsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$ of unknown variance.\n\nYour task is to implement a program that, for each provided test case, performs the following:\n\n- Estimate the individualized parameters $k_e$ and $V$ by maximum likelihood under the Gaussian noise assumption, which reduces to minimizing the sum of squared residuals between predicted concentrations $C(t_i)$ and observed $y_i$.\n- Using the estimated parameters, compute the individualized concentration trajectory $C(t)$ over the dosing schedule for the window $[0,T]$ with $T$ in hours.\n- Compute the following outputs:\n  1. The estimated elimination rate $k_e$ in $\\mathrm{h}^{-1}$, rounded to $3$ decimal places.\n  2. The estimated volume $V$ in $\\mathrm{L}$, rounded to $3$ decimal places.\n  3. The maximum concentration over $[0,T]$ in $\\mathrm{mg}/\\mathrm{L}$, rounded to $3$ decimal places.\n  4. The concentration at $t = T$ in $\\mathrm{mg}/\\mathrm{L}$, rounded to $3$ decimal places.\n  5. The area under the concentration-time curve over $[0,T]$ in $\\mathrm{mg}\\cdot\\mathrm{h}/\\mathrm{L}$, rounded to $3$ decimal places.\n\nUse only scientifically sound assumptions. Infusion schedules are piecewise-constant with known dose $D$ in $\\mathrm{mg}$ and infusion duration $\\tau$ in $\\mathrm{h}$ for each administration. For each test case, assume the initial amount $A(0) = 0$.\n\nImplement the calibration as nonlinear least squares with physically plausible parameter bounds and numerically stable propagation of $A(t)$ across the piecewise infusion and elimination intervals. The program must be self-contained and produce the exact final output format described below.\n\nAnswer units and rounding:\n- Express $k_e$ in $\\mathrm{h}^{-1}$, $V$ in $\\mathrm{L}$, concentrations in $\\mathrm{mg}/\\mathrm{L}$, and area under the curve in $\\mathrm{mg}\\cdot\\mathrm{h}/\\mathrm{L}$.\n- Round all outputs to $3$ decimal places.\n- Angles are not involved in this problem.\n\nTest suite:\nProvide results for the following cases. In all cases, the dosing schedule arrays list infusions as $(t_{\\text{start}}, \\tau, D)$ in units of $\\mathrm{h}$ for times and durations and $\\mathrm{mg}$ for dose.\n\n- Case $1$ (typical adult, $q12\\mathrm{h}$): $T = 24\\,\\mathrm{h}$, schedule $[(0, 1, 1000), (12, 1, 1000)]$, TDM observations at times and measured concentrations $[(1, 21.1), (11.5, 6.0), (13, 26.4), (23.5, 7.1)]$ in $\\mathrm{mg}/\\mathrm{L}$.\n- Case $2$ (single dose, short infusion): $T = 24\\,\\mathrm{h}$, schedule $[(0, 0.5, 1500)]$, TDM observations $[(0.5, 27.2), (2, 23.4), (8, 15.8), (24, 3.0)]$ in $\\mathrm{mg}/\\mathrm{L}$.\n- Case $3$ ($q8\\mathrm{h}$, long infusion): $T = 24\\,\\mathrm{h}$, schedule $[(0, 2, 750), (8, 2, 750), (16, 2, 750)]$, TDM observations $[(2, 15.9), (10, 21.5), (15.5, 9.4), (18, 22.2)]$ in $\\mathrm{mg}/\\mathrm{L}$.\n- Case $4$ ($q12\\mathrm{h}$, slower elimination, larger volume): $T = 24\\,\\mathrm{h}$, schedule $[(0, 1, 750), (12, 1, 750)]$, TDM observations $[(1, 10.6), (13, 15.6), (23.5, 8.1)]$ in $\\mathrm{mg}/\\mathrm{L}$.\n\nAlgorithmic requirements:\n- Propagate $A(t)$ exactly across each interval using the solution to $dA/dt = -k_e A + R$ on intervals with constant $R$, and $R=0$ off infusion.\n- Compute $C(t) = A(t)/V$.\n- For area under the curve, use the exact integral of $A(t)$ over each interval, divided by $V$.\n- Use nonlinear least squares with parameter bounds $k_e \\in [0.02, 1.0]$ and $V \\in [10, 100]$ and a reasonable initial guess.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list of per-case results, each per-case result being a list of $5$ floats in the order specified above, all rounded to $3$ decimal places, enclosed in square brackets. For example, the output should look like $[[k_{e,1},V_1,\\max C_1,C_1(T),\\mathrm{AUC}_1],[k_{e,2},V_2,\\max C_2,C_2(T),\\mathrm{AUC}_2],\\dots]$.",
            "solution": "The problem requires the development of a computational method to personalize a one-compartment pharmacokinetic (PK) model for vancomycin therapy using therapeutic drug monitoring (TDM) data. This involves estimating patient-specific parameters and using them to predict the drug's concentration profile and related clinical metrics. The problem is scientifically well-posed and provides all necessary information for a complete solution.\n\nThe fundamental principle is mass balance applied to the amount of drug, $A(t)$, in a single, well-mixed compartment representing the body. The apparent volume of this compartment is $V$. The relationship between the amount and concentration $C(t)$ is given by $C(t) = A(t)/V$. The dynamics of $A(t)$ are described by the first-order linear ordinary differential equation (ODE):\n$$\n\\frac{dA}{dt} = -k_e A(t) + R(t)\n$$\nwhere $k_e$ is the first-order elimination rate constant in units of $\\mathrm{h}^{-1}$, and $R(t)$ is the rate of drug administration in $\\mathrm{mg}/\\mathrm{h}$. For intravenous infusions, $R(t)$ is a piecewise-constant function. It is equal to a constant rate $R_{inf} = D/\\tau$ during an infusion of duration $\\tau$ and total dose $D$, and $R(t)=0$ otherwise. The initial condition is specified as $A(0) = 0$.\n\nThe core of the solution lies in solving this ODE and using it to predict concentrations, which are then used in a parameter estimation framework.\n\n**1. Analytical Solution of the Pharmacokinetic Model**\n\nThe ODE is solved over sequential time intervals where the infusion rate $R$ is constant. For an interval $[t_a, t_b]$, with initial amount $A(t_a)$ and constant rate $R$, the analytical solution for $A(t)$ where $t \\in [t_a, t_b]$ is:\n$$\nA(t) = \\frac{R}{k_e} + \\left(A(t_a) - \\frac{R}{k_e}\\right) e^{-k_e (t - t_a)}\n$$\nWhen there is no infusion, $R=0$, and the equation simplifies to pure exponential decay:\n$$\nA(t) = A(t_a) e^{-k_e (t - t_a)}\n$$\nThese equations allow us to propagate the drug amount $A(t)$ precisely from one time point to the next across a schedule comprised of infusion and elimination phases.\n\n**2. Parameter Estimation via Nonlinear Least Squares**\n\nThe individual PK parameters, $k_e$ and $V$, are unknown and must be estimated from the TDM data. The data consists of pairs $(t_i, y_i)$, where $y_i$ is the measured concentration at time $t_i$. The model for these observations is $y_i = C(t_i) + \\epsilon_i$, where $C(t_i) = A(t_i)/V$ is the true concentration and $\\epsilon_i$ are independent, identically distributed Gaussian random measurement errors with mean $0$.\n\nUnder this assumption, maximum likelihood estimation is equivalent to minimizing the sum of squared residuals (SSR) between the model-predicted concentrations and the observed concentrations:\n$$\n\\text{SSR}(k_e, V) = \\sum_{i} \\left( C(t_i; k_e, V) - y_i \\right)^2 = \\sum_{i} \\left( \\frac{A(t_i; k_e)}{V} - y_i \\right)^2\n$$\nThis is a nonlinear least squares problem. We seek the parameter values $(\\hat{k}_e, \\hat{V})$ that minimize this objective function, constrained by the physically plausible bounds $k_e \\in [0.02, 1.0]\\,\\mathrm{h}^{-1}$ and $V \\in [10, 100]\\,\\mathrm{L}$. This optimization is performed numerically, for which the `scipy.optimize.minimize` function with a bound-constrained algorithm (e.g., L-BFGS-B) is suitable.\n\n**3. Calculation of Derived Pharmacokinetic Metrics**\n\nOnce the optimal parameters $(\\hat{k}_e, \\hat{V})$ are determined for an individual, we can compute the required therapeutic metrics.\n\n- **Maximum Concentration ($C_{max}$)**: The concentration $C(t)$ increases during infusion intervals (assuming the initial concentration is below the steady state value for that infusion) and decreases during elimination intervals. Therefore, the peak concentration within the time window $[0, T]$ will occur at the end of one of the infusion periods. The procedure is to simulate the full concentration profile and identify the maximum value attained at the specific time points corresponding to the end of each infusion.\n\n- **Concentration at Final Time ($C(T)$)**: This is calculated by propagating the model forward to time $T$ using the estimated parameters $\\hat{k}_e$ and $\\hat{V}$.\n\n- **Area Under the Curve (AUC)**: The AUC over the interval $[0, T]$ is the integral of the concentration-time curve:\n$$\n\\text{AUC}_{[0,T]} = \\int_0^T C(t) dt = \\frac{1}{\\hat{V}} \\int_0^T A(t) dt\n$$\nTo compute this exactly as required, we integrate the analytical solution for $A(t)$ over each piecewise-constant interval. A numerically stable and elegant formula for the integral of $A(t)$ over an interval $[t_a, t_b]$ with constant rate $R$ can be derived by integrating the ODE itself:\n$$\n\\int_{t_a}^{t_b} \\frac{dA}{dt} dt = A(t_b) - A(t_a) = \\int_{t_a}^{t_b} (-k_e A(t) + R) dt = -k_e \\int_{t_a}^{t_b} A(t) dt + R(t_b - t_a)\n$$\nRearranging for the integral gives:\n$$\n\\int_{t_a}^{t_b} A(t) dt = \\frac{A(t_a) - A(t_b) + R(t_b - t_a)}{k_e}\n$$\nThe total integral $\\int_0^T A(t) dt$ is found by summing these interval integrals over a timeline composed of all infusion start/end times and the final time $T$. The final AUC is this total integral divided by the estimated volume $\\hat{V}$.\n\n**Algorithmic Implementation**\n\nA robust implementation will consist of three main components:\n1.  A core simulation function that, given parameters $(k_e, V)$, a dosing schedule, and a set of evaluation time points, propagates $A(t)$ and optionally computes the integral of $A(t)$ over the timeline. This function will use the analytical solutions for each interval.\n2.  An objective function for the optimizer that takes parameters $(k_e, V)$ as input, calls the simulation function to get predicted concentrations at TDM times, and returns the SSR.\n3.  A main loop that, for each test case, calls the optimization routine to find $(\\hat{k}_e, \\hat{V})$, and then uses these optimal parameters to compute and format the five required output metrics.\n\nThe entire process translates the principles of pharmacokinetics into a concrete computational workflow for model personalization and prediction, forming the foundation of a digital twin for drug therapy.",
            "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Solves the pharmacokinetic modeling problem for all test cases.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"T\": 24.0,\n            \"schedule\": [(0.0, 1.0, 1000.0), (12.0, 1.0, 1000.0)],\n            \"tdm_data\": [(1.0, 21.1), (11.5, 6.0), (13.0, 26.4), (23.5, 7.1)],\n        },\n        {\n            \"T\": 24.0,\n            \"schedule\": [(0.0, 0.5, 1500.0)],\n            \"tdm_data\": [(0.5, 27.2), (2.0, 23.4), (8.0, 15.8), (24.0, 3.0)],\n        },\n        {\n            \"T\": 24.0,\n            \"schedule\": [(0.0, 2.0, 750.0), (8.0, 2.0, 750.0), (16.0, 2.0, 750.0)],\n            \"tdm_data\": [(2.0, 15.9), (10.0, 21.5), (15.5, 9.4), (18.0, 22.2)],\n        },\n        {\n            \"T\": 24.0,\n            \"schedule\": [(0.0, 1.0, 750.0), (12.0, 1.0, 750.0)],\n            \"tdm_data\": [(1.0, 10.6), (13.0, 15.6), (23.5, 8.1)],\n        },\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        tdm_times = np.array([d[0] for d in case[\"tdm_data\"]])\n        tdm_obs = np.array([d[1] for d in case[\"tdm_data\"]])\n\n        def _propagate(ke, schedule, eval_times):\n            \"\"\"\n            Propagates the amount of drug A(t) over time.\n            \"\"\"\n            # Create a timeline of events (infusion starts/ends, evaluation points)\n            event_times = {0.0}\n            rate_changes = {}\n\n            for start, duration, dose in schedule:\n                if duration > 0:\n                    rate = dose / duration\n                    end = start + duration\n                    event_times.add(start)\n                    event_times.add(end)\n                    rate_changes[start] = rate_changes.get(start, 0) + rate\n                    rate_changes[end] = rate_changes.get(end, 0) - rate\n\n            if eval_times is not None:\n                for t in eval_times:\n                    event_times.add(t)\n\n            sorted_times = sorted(list(event_times))\n            \n            # Since eval_times might contain duplicates after adding to a set\n            eval_times_set = set(eval_times) if eval_times is not None else set()\n            \n            A = 0.0\n            current_rate = 0.0\n            last_t = 0.0\n            \n            amount_at_eval_times = {}\n\n            for t in sorted_times:\n                if t > last_t:\n                    dt = t - last_t\n                    A_start_interval = A\n                    if current_rate > 1e-9: # Infusion phase\n                        A = (current_rate / ke) + (A_start_interval - current_rate / ke) * np.exp(-ke * dt)\n                    else: # Elimination phase\n                        A = A_start_interval * np.exp(-ke * dt)\n\n                if t in rate_changes:\n                    current_rate += rate_changes[t]\n\n                if t in eval_times_set:\n                    amount_at_eval_times[t] = A\n                \n                last_t = t\n                \n            # Return amounts in the same order as eval_times\n            ordered_amounts = [amount_at_eval_times[t] for t in eval_times]\n            return ordered_amounts\n\n        def objective_function(params):\n            ke, V = params\n            predicted_amounts = _propagate(ke, case[\"schedule\"], tdm_times)\n            predicted_concentrations = np.array(predicted_amounts) / V\n            ssr = np.sum((predicted_concentrations - tdm_obs) ** 2)\n            return ssr\n        \n        # Parameter estimation\n        initial_guess = [0.1, 40.0]\n        bounds = [(0.02, 1.0), (10.0, 100.0)]\n        result = minimize(objective_function, initial_guess, method='L-BFGS-B', bounds=bounds)\n        ke_opt, V_opt = result.x\n\n        # Calculate final outputs with optimized parameters\n        T = case[\"T\"]\n        \n        # Create a timeline for final calculations (AUC, Cmax, C(T))\n        event_times = {0.0, T}\n        rate_changes = {}\n        infusion_end_times = []\n\n        for start, duration, dose in case[\"schedule\"]:\n            if duration > 0:\n                rate = dose / duration\n                end = start + duration\n                event_times.add(start)\n                event_times.add(end)\n                infusion_end_times.append(end)\n                rate_changes[start] = rate_changes.get(start, 0) + rate\n                rate_changes[end] = rate_changes.get(end, 0) - rate\n\n        sorted_times = sorted(list(event_times))\n        \n        A = 0.0\n        current_rate = 0.0\n        last_t = 0.0\n        auc_A = 0.0\n        amount_at_event_times = {}\n\n        for t in sorted_times:\n            if t > last_t:\n                dt = t - last_t\n                A_start_interval = A\n                if current_rate > 1e-9:\n                    A = (current_rate / ke_opt) + (A_start_interval - current_rate / ke_opt) * np.exp(-ke_opt * dt)\n                else:\n                    A = A_start_interval * np.exp(-ke_opt * dt)\n                \n                # Accumulate integral of A(t)\n                auc_A += (A_start_interval - A + current_rate * dt) / ke_opt\n\n            if t in rate_changes:\n                current_rate += rate_changes[t]\n\n            amount_at_event_times[t] = A\n            last_t = t\n        \n        # 1. and 2. Estimated parameters\n        ke_final = round(ke_opt, 3)\n        V_final = round(V_opt, 3)\n        \n        # 3. Maximum Concentration\n        C_at_inf_ends = [amount_at_event_times[t] / V_opt for t in infusion_end_times if t = T]\n        C_max = round(max(C_at_inf_ends) if C_at_inf_ends else 0.0, 3)\n\n        # 4. Concentration at T\n        C_T = round(amount_at_event_times[T] / V_opt, 3)\n\n        # 5. Area Under the Curve\n        AUC = round(auc_A / V_opt, 3)\n\n        all_results.append([ke_final, V_final, C_max, C_T, AUC])\n    \n    # Format the final output string\n    result_str = \"[\" + \",\".join([f\"[{r[0]},{r[1]},{r[2]},{r[3]},{r[4]}]\" for r in all_results]) + \"]\"\n    print(result_str)\n\nsolve()\n```"
        },
        {
            "introduction": "Once parameters have been estimated, a critical question arises: can we trust these values? This exercise introduces the rigorous concept of practical identifiability, which assesses whether the available data is informative enough to uniquely determine the model's parameters. Using the powerful profile likelihood method, you will investigate how factors like sparse sampling or structural redundancies in the model can prevent reliable parameter estimation for a pharmacokinetic digital twin.  This practice is essential for learning how to validate a model's reliability and understanding the limits of what can be inferred from a given dataset.",
            "id": "4217304",
            "problem": "A digital twin for individualized pharmacotherapy in healthcare is a computational replica of a patient’s drug concentration dynamics used to personalize dosing decisions, and it is a canonical example of a Cyber-Physical System (CPS) where the computational model is continuously updated with physical measurements. In order for the digital twin to be clinically actionable, the parameters of its dynamical model must be practically identifiable from available data. The goal of this problem is to operationalize practical identifiability for a one-compartment intravenous bolus pharmacokinetic model using profile likelihood and to implement a program that determines, for each parameter and each test case, whether its profile is bounded or flat under a clear criterion.\n\nFundamental base and setting: Consider a one-compartment intravenous bolus model with drug concentration $C(t)$ given by\n$$\nC(t) \\;=\\; \\frac{D}{V} \\, e^{-k_e \\, t},\n$$\nwhere $t$ is time, $D$ is dose, $V$ is volume of distribution, and $k_e$ is the first-order elimination rate constant. Assume measurements $y_i$ at times $t_i$ satisfy\n$$\ny_i \\;=\\; C(t_i) + \\varepsilon_i, \\quad \\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2),\n$$\nwith independent Gaussian noise of known variance $\\sigma^2$. The log-likelihood $\\ell(\\theta)$ under independent Gaussian errors is proportional to the negative of the sum of squared residuals,\n$$\n-2 \\log L(\\theta) \\;=\\; \\sum_{i=1}^{n} \\frac{(y_i - C(t_i;\\theta))^2}{\\sigma^2} + \\text{constant},\n$$\nwhere $\\theta$ denotes the parameter vector consisting of the parameters being estimated. The profile likelihood for a parameter $\\theta_j$ is defined as\n$$\n\\ell_p(\\theta_j) \\;=\\; \\max_{\\theta_{-j}} \\ell(\\theta_j,\\theta_{-j}),\n$$\nequivalently minimizing $-2\\log L$ over all parameters except $\\theta_j$, while $\\theta_j$ is held fixed.\n\nPractical identifiability criterion: For a single parameter profile, define the deviance\n$$\nD(\\theta) \\;=\\; \\sum_{i=1}^n \\frac{(y_i - C(t_i;\\theta))^2}{\\sigma^2}.\n$$\nLet $D_{\\min}$ be the minimum deviance over all parameters. For a confidence level of $95\\%$, let $q_{0.95}$ be the $0.95$ quantile of a chi-square distribution with $1$ degree of freedom. The $95\\%$ profile-based confidence set for $\\theta_j$ is\n$$\n\\mathcal{S}_j \\;=\\; \\left\\{\\theta_j \\,\\middle|\\, D_j(\\theta_j) \\le D_{\\min} + q_{0.95}\\right\\},\n$$\nwhere $D_j(\\theta_j)$ is the profiled deviance obtained by minimizing $D$ over all parameters except $\\theta_j$ with $\\theta_j$ fixed. In numerical practice, if the set $\\mathcal{S}_j$ is a bounded interval strictly contained within the explored grid for $\\theta_j$ (i.e., it does not touch the grid boundaries), we classify the parameter as having a bounded profile (practically identifiable). If $\\mathcal{S}_j$ touches a boundary or spans the entire grid without crossing the threshold $D_{\\min}+q_{0.95}$, then the profile is flat or unbounded (practically non-identifiable).\n\nProgram requirements: Implement a program that, for each test case below, generates synthetic data from the ground-truth parameters, computes the global minimum deviance $D_{\\min}$ by optimizing over the parameters to be estimated, constructs profile likelihoods for each parameter to be estimated by fixing a grid of values and optimizing the remaining parameters, and determines for each parameter whether its profile is bounded or flat according to the criterion above. Use the following grids for profiling:\n- For $k_e$: $50$ equally spaced points on $[0.05,1.0]$ in $\\text{hr}^{-1}$.\n- For $V$: $50$ equally spaced points on $[5.0,50.0]$ in $\\text{L}$.\n- For $D$: $50$ equally spaced points on $[50.0,500.0]$ in $\\text{mg}$.\nUse bounded optimization for the free parameters with bounds:\n- $k_e \\in [0.01,5.0]$ in $\\text{hr}^{-1}$,\n- $V \\in [1.0,200.0]$ in $\\text{L}$,\n- $D \\in [10.0,1000.0]$ in $\\text{mg}$.\n\nData generation: For each test case, set a fixed random seed to ensure reproducibility and draw $\\varepsilon_i$ from $\\mathcal{N}(0,\\sigma^2)$.\n\nUnits: Time $t$ in $\\text{hr}$, dose $D$ in $\\text{mg}$, volume $V$ in $\\text{L}$, concentration $C$ and $y_i$ in $\\text{mg}/\\text{L}$, and the elimination rate $k_e$ in $\\text{hr}^{-1}$.\n\nTest suite and coverage:\n- Case $1$ (happy path): Known dose $D$ estimated as fixed at $D = 100$ in $\\text{mg}$. Parameters to estimate: $k_e$ and $V$. Ground truth: $k_e = 0.4$ in $\\text{hr}^{-1}$, $V = 20$ in $\\text{L}$. Sampling times: $t = [0.0,0.5,1.0,2.0,4.0,6.0,8.0]$ in $\\text{hr}$. Noise standard deviation: $\\sigma = 0.2$ in $\\text{mg}/\\text{L}$.\n- Case $2$ (structural non-identifiability): Unknown dose $D$ included among parameters to estimate: $k_e$, $V$, $D$. Ground truth: $k_e = 0.4$ in $\\text{hr}^{-1}$, $V = 20$ in $\\text{L}$, $D = 100$ in $\\text{mg}$. Sampling times: $t = [0.0,0.5,1.0,2.0,4.0,6.0,8.0]$ in $\\text{hr}$. Noise standard deviation: $\\sigma = 0.2$ in $\\text{mg}/\\text{L}$.\n- Case $3$ (boundary and sparsity edge): Unknown dose $D$ included among parameters to estimate: $k_e$, $V$, $D$. Ground truth: $k_e = 0.4$ in $\\text{hr}^{-1}$, $V = 20$ in $\\text{L}$, $D = 100$ in $\\text{mg}$. Sampling times concentrated near zero: $t = [0.0,0.1,0.2,0.3]$ in $\\text{hr}$. Noise standard deviation: $\\sigma = 0.05$ in $\\text{mg}/\\text{L}$.\n- Case $4$ (high-noise stress): Known dose $D$ estimated as fixed at $D = 100$ in $\\text{mg}$. Parameters to estimate: $k_e$ and $V$. Ground truth: $k_e = 0.4$ in $\\text{hr}^{-1}$, $V = 20$ in $\\text{L}$. Sampling times: $t = [0.0,0.5,1.0,2.0,4.0,6.0,8.0]$ in $\\text{hr}$. Noise standard deviation: $\\sigma = 2.0$ in $\\text{mg}/\\text{L}$.\n\nRequired output: For each case, produce a list of booleans indicating, in order, whether each estimated parameter’s profile is bounded (true) or flat/unbounded (false), following the order of parameters as listed in the case definition. Specifically:\n- Case $1$: output $[b_{k_e}, b_V]$,\n- Case $2$: output $[b_{k_e}, b_V, b_D]$,\n- Case $3$: output $[b_{k_e}, b_V, b_D]$,\n- Case $4$: output $[b_{k_e}, b_V]$.\nThe program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each case’s list is itself enclosed in square brackets. For example, the output format should look like $[\\,[\\cdot,\\cdot],\\,[\\cdot,\\cdot,\\cdot],\\,\\ldots\\,]$.\n\nAngle unit: Not applicable.\n\nPercentages: Not applicable.\n\nFinal instructions: Implement the program in Python using numerical optimization to construct likelihood profiles and apply the identifiability criterion described above. Ensure all calculations follow the units provided and the program runs without external input, printing exactly one line in the specified format.",
            "solution": "The problem requires an evaluation of the practical identifiability of parameters in a one-compartment pharmacokinetic model using the profile likelihood method. This involves generating synthetic data, performing numerical optimization to find both global and profiled likelihoods, and applying a statistical criterion to classify each parameter's profile as either 'bounded' (identifiable) or 'flat/unbounded' (non-identifiable).\n\nThe pharmacokinetic model describes the drug concentration $C(t)$ over time $t$ following an intravenous bolus injection. The model is given by:\n$$\nC(t) \\;=\\; \\frac{D}{V} \\, e^{-k_e \\, t}\n$$\nHere, $D$ is the administered dose, $V$ is the volume of distribution, and $k_e$ is the first-order elimination rate constant. The parameter vector to be estimated is denoted by $\\theta$.\n\nThe data consist of measurements $y_i$ taken at times $t_i$, which are modeled as the true concentration plus independent and identically distributed Gaussian noise:\n$$\ny_i \\;=\\; C(t_i; \\theta) + \\varepsilon_i, \\quad \\text{where} \\quad \\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)\n$$\nThe noise variance $\\sigma^2$ is assumed to be known. Under this assumption, maximizing the likelihood function $L(\\theta)$ is equivalent to minimizing the sum of squared residuals, weighted by the variance. This gives rise to the deviance function $D(\\theta)$:\n$$\nD(\\theta) \\;=\\; \\sum_{i=1}^{n} \\frac{(y_i - C(t_i;\\theta))^2}{\\sigma^2}\n$$\nMinimizing $D(\\theta)$ with respect to all parameters $\\theta$ yields the maximum likelihood estimate $\\hat{\\theta}$ and the minimum deviance, $D_{\\min} = D(\\hat{\\theta})$.\n\nTo assess the identifiability of a single parameter, say $\\theta_j$, we employ the profile likelihood method. For a fixed value of $\\theta_j$, we minimize the deviance function with respect to all other parameters $\\theta_{-j}$. This procedure defines the profiled deviance for $\\theta_j$:\n$$\nD_j(\\theta_j) \\;=\\; \\min_{\\theta_{-j}} D\\left(\\theta = (\\theta_j, \\theta_{-j})\\right)\n$$\nBy computing $D_j(\\theta_j)$ over a grid of values for $\\theta_j$, we obtain the parameter's profile.\n\nThe practical identifiability criterion is based on the likelihood ratio test. The set of parameter values for $\\theta_j$ that are consistent with the data at a $95\\%$ confidence level is given by:\n$$\n\\mathcal{S}_j \\;=\\; \\left\\{\\theta_j \\,\\middle|\\, D_j(\\theta_j) \\le D_{\\min} + q_{0.95}\\right\\}\n$$\nwhere $q_{0.95}$ is the $0.95$ quantile of the chi-square distribution with $1$ degree of freedom (approximately $3.841$). This corresponds to the threshold for the change in deviance.\n\nA parameter $\\theta_j$ is deemed **practically identifiable** (having a 'bounded' profile) if the confidence set $\\mathcal{S}_j$, when evaluated on the specified numerical grid, forms an interval that does not include the grid's endpoints. This indicates that the data provide sufficient information to constrain the parameter within a finite range. Conversely, the parameter is **practically non-identifiable** (having a 'flat' or 'unbounded' profile) if the confidence set $\\mathcal{S}_j$ extends to one or both boundaries of the grid. This implies that the data are insufficient to constrain the parameter, which could be due to structural model properties (e.g., parameter redundancy) or practical limitations (e.g., sparse sampling, high noise).\n\nThe algorithm to solve this problem proceeds as follows for each test case:\n1.  **Data Synthesis**: Generate synthetic concentration data $y_i$ at specified time points $t_i$ using the ground-truth parameters, and add Gaussian noise with the specified standard deviation $\\sigma$. A fixed random seed ensures reproducibility.\n2.  **Global Deviance Minimization**: Numerically minimize the deviance function $D(\\theta)$ with respect to all estimable parameters, subject to the given bounds. This optimization yields the global minimum deviance $D_{\\min}$.\n3.  **Profiled Deviance Calculation**: For each parameter to be estimated:\n    a. Iterate through a predefined grid of values for that parameter.\n    b. At each grid point, fix the current parameter and numerically minimize the deviance with respect to the remaining free parameters, again subject to their bounds.\n    c. Store the resulting minimized deviance. This collection of values forms the profiled deviance for the parameter.\n4.  **Identifiability Assessment**:\n    a. Calculate the deviance threshold $T = D_{\\min} + q_{0.95}$.\n    b. For each parameter's profile, identify the grid points where the profiled deviance is less than or equal to $T$.\n    c. If this set of points is non-empty and its minimum and maximum indices are strictly within the grid boundaries (i.e., not the first or last index), the parameter is classified as 'bounded' (True). Otherwise, it is 'flat/unbounded' (False).\n\nThis procedure is applied to four distinct cases designed to test different identifiability scenarios:\n-   **Case 1**: A well-posed problem with low noise and sufficient data, where both $k_e$ and $V$ are expected to be identifiable.\n-   **Case 2**: A case exhibiting structural non-identifiability. The model $C(t) = (D/V) e^{-k_e t}$ is insensitive to individual values of $D$ and $V$, only their ratio $D/V$. Therefore, while $k_e$ should be identifiable, $D$ and $V$ are expected to be non-identifiable.\n-   **Case 3**: A case with sparse sampling concentrated at early time points. This practical limitation, combined with the structural non-identifiability of $D$ and $V$, is expected to render all three parameters ($k_e$, $V$, $D$) non-identifiable. The short time course provides little information about the exponential decay rate $k_e$.\n-   **Case 4**: A case identical to Case 1 but with high noise. The low signal-to-noise ratio is expected to flatten the likelihood surface, resulting in practical non-identifiability for both $k_e$ and $V$.",
            "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats import chi2\n\ndef solve():\n    \"\"\"\n    Performs practical identifiability analysis for a one-compartment pharmacokinetic model\n    using the profile likelihood method for four distinct test cases.\n    \"\"\"\n\n    # --- Configuration and Test Cases ---\n\n    # Chi-square quantile for 95% confidence interval with 1 degree of freedom\n    q_095 = chi2.ppf(0.95, df=1)\n\n    # Profiling grids\n    grids = {\n        'ke': np.linspace(0.05, 1.0, 50),\n        'V': np.linspace(5.0, 50.0, 50),\n        'D': np.linspace(50.0, 500.0, 50)\n    }\n\n    # Optimization bounds\n    bounds = {\n        'ke': (0.01, 5.0),\n        'V': (1.0, 200.0),\n        'D': (10.0, 1000.0)\n    }\n    \n    # Test cases defined in a structured list\n    test_cases = [\n        {\n            'name': 'Case 1 (Happy Path)',\n            'params_to_estimate': ['ke', 'V'],\n            'fixed_params': {'D': 100.0},\n            'ground_truth': {'ke': 0.4, 'V': 20.0, 'D': 100.0},\n            'times': np.array([0.0, 0.5, 1.0, 2.0, 4.0, 6.0, 8.0]),\n            'sigma': 0.2,\n            'seed': 42\n        },\n        {\n            'name': 'Case 2 (Structural Non-identifiability)',\n            'params_to_estimate': ['ke', 'V', 'D'],\n            'fixed_params': {},\n            'ground_truth': {'ke': 0.4, 'V': 20.0, 'D': 100.0},\n            'times': np.array([0.0, 0.5, 1.0, 2.0, 4.0, 6.0, 8.0]),\n            'sigma': 0.2,\n            'seed': 43\n        },\n        {\n            'name': 'Case 3 (Boundary and Sparsity)',\n            'params_to_estimate': ['ke', 'V', 'D'],\n            'fixed_params': {},\n            'ground_truth': {'ke': 0.4, 'V': 20.0, 'D': 100.0},\n            'times': np.array([0.0, 0.1, 0.2, 0.3]),\n            'sigma': 0.05,\n            'seed': 44\n        },\n        {\n            'name': 'Case 4 (High Noise)',\n            'params_to_estimate': ['ke', 'V'],\n            'fixed_params': {'D': 100.0},\n            'ground_truth': {'ke': 0.4, 'V': 20.0, 'D': 100.0},\n            'times': np.array([0.0, 0.5, 1.0, 2.0, 4.0, 6.0, 8.0]),\n            'sigma': 2.0,\n            'seed': 45\n        }\n    ]\n\n    # --- Helper Functions ---\n\n    def model(t, ke, V, D):\n        \"\"\" One-compartment IV bolus model \"\"\"\n        # Handle t=0 case to avoid issues with large negative exponents if ke is large\n        # Also, V cannot be zero. Optimizer bounds prevent this, but good practice.\n        if V == 0: return np.inf\n        C0 = D / V\n        return C0 * np.exp(-ke * t)\n\n    def deviance(params, param_names, t, y, sigma, fixed_params):\n        \"\"\" Calculates the deviance (proportional to -2 * log-likelihood) \"\"\"\n        all_params = dict(zip(param_names, params))\n        all_params.update(fixed_params)\n        \n        ke = all_params['ke']\n        V = all_params['V']\n        D = all_params['D']\n\n        y_model = model(t, ke, V, D)\n        residuals = y - y_model\n        return np.sum(residuals**2) / (sigma**2)\n\n    # --- Main Processing Loop ---\n    \n    final_results = []\n\n    for case in test_cases:\n        # 1. Generate synthetic data\n        np.random.seed(case['seed'])\n        gt = case['ground_truth']\n        y_true = model(case['times'], gt['ke'], gt['V'], gt['D'])\n        y_data = y_true + np.random.normal(0, case['sigma'], size=len(case['times']))\n\n        params_to_estimate = case['params_to_estimate']\n        \n        # 2. Global optimization to find D_min\n        initial_guess = [gt[p] for p in params_to_estimate]\n        opt_bounds = [bounds[p] for p in params_to_estimate]\n        \n        res_global = minimize(\n            deviance,\n            x0=initial_guess,\n            args=(params_to_estimate, case['times'], y_data, case['sigma'], case['fixed_params']),\n            method='L-BFGS-B',\n            bounds=opt_bounds\n        )\n        D_min = res_global.fun\n        threshold = D_min + q_095\n\n        case_results = []\n        # 3. Profile likelihood for each parameter\n        for i, p_fixed_name in enumerate(params_to_estimate):\n            profile_grid = grids[p_fixed_name]\n            profile_deviance = np.zeros_like(profile_grid)\n            \n            p_optim_names = [p for p in params_to_estimate if p != p_fixed_name]\n            \n            for j, p_fixed_value in enumerate(profile_grid):\n                \n                # Define objective for the nested optimization\n                def inner_objective(p_optim_values):\n                    current_fixed_params = {p_fixed_name: p_fixed_value}\n                    current_fixed_params.update(case['fixed_params'])\n                    return deviance(p_optim_values, p_optim_names, case['times'], y_data, case['sigma'], current_fixed_params)\n\n                # Set up for inner optimization\n                inner_initial_guess = [gt[p] for p in p_optim_names]\n                inner_bounds = [bounds[p] for p in p_optim_names]\n\n                # If no other params to optimize, just calculate deviance\n                if not p_optim_names:\n                    profile_deviance[j] = inner_objective([])\n                    continue\n                \n                res_profile = minimize(\n                    inner_objective,\n                    x0=inner_initial_guess,\n                    method='L-BFGS-B',\n                    bounds=inner_bounds\n                )\n                profile_deviance[j] = res_profile.fun\n\n            # 4. Assess identifiability\n            within_threshold_indices = np.where(profile_deviance = threshold)[0]\n            \n            is_bounded = False\n            if len(within_threshold_indices) > 0:\n                min_idx = np.min(within_threshold_indices)\n                max_idx = np.max(within_threshold_indices)\n                if min_idx > 0 and max_idx  len(profile_grid) - 1:\n                    is_bounded = True\n            \n            case_results.append(is_bounded)\n            \n        final_results.append(case_results)\n\n    # Format and print the final output\n    output_str = '[' + ','.join([str(res) for res in final_results]) + ']'\n    output_str = output_str.replace(\"'\", \"\") # Remove quotes around lists\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "With a personalized and validated model in hand, the digital twin can be deployed for its primary purpose: simulating outcomes to optimize therapeutic strategies. This exercise puts you in the role of a clinical modeler, using a digital twin of tumor growth to compare the efficacy of different chemotherapy dosing protocols. By simulating the response to metronomic, pulsed, and front-loaded schedules, you will use the model to run an in-silico clinical trial aimed at minimizing tumor burden under a fixed drug budget.  This practice provides hands-on experience in using a dynamic model for decision support, a key application of digital twins in personalized medicine.",
            "id": "4217334",
            "problem": "Consider a simplified patient-specific Digital Twin of tumor burden under cytotoxic chemotherapy in personalized medicine, modeled by a single-compartment tumor population with logistic growth and a dose-dependent kill term. Let the tumor population be $N(t)$ (in cells), and suppose its dynamics are governed by the Ordinary Differential Equation (ODE) $$\\dot N(t) = r\\,N(t)\\left(1 - \\frac{N(t)}{K}\\right) - \\kappa\\,u(t)\\,N(t),$$ where $r$ is the intrinsic growth rate (in $\\mathrm{day}^{-1}$), $K$ is the carrying capacity (in cells), $\\kappa$ is the drug kill sensitivity parameter (in $(\\mathrm{mg}/\\mathrm{m}^2)^{-1}$), and $u(t)$ is the chemotherapy infusion rate (in $\\mathrm{mg}/\\mathrm{m}^2/\\mathrm{day}$). Assume $t$ is measured in days and concentrations are folded into the effective infusion rate $u(t)$ so that $\\kappa\\,u(t)$ has units of $\\mathrm{day}^{-1}$. Let the initial tumor population be $N(0) = N_0$ (in cells). This ODE is a mass-balance model combining well-tested logistic growth and a first-order kill term proportional to dose rate.\n\nYour task is to write a complete program that, for each given test case representing a patient-specific parameter set, evaluates a set of dosing protocols $u(t)$ that all expend the same total dose budget $B$ (in $\\mathrm{mg}/\\mathrm{m}^2$) over a fixed horizon $[0,T]$ (in days). For each protocol, integrate the ODE to compute the normalized cumulative burden $$A = \\frac{1}{T}\\int_0^T \\frac{N(t)}{K}\\,dt,$$ which is dimensionless. If the tumor reaches extinction ($N(t)=0$) at some time $t_\\mathrm{ext} \\le T$, treat $N(t)=0$ for all $t \\in [t_\\mathrm{ext}, T]$ when computing $A$. Identify the dosing schedule that minimizes $A$ under the prescribed protocols. In case of ties where $|A_i - A_j| \\le 10^{-6}$ for the best two or more schedules, choose the schedule with the smallest index.\n\nThe dosing protocols to compare for each test case are:\n- Schedule $0$ (metronomic): $u(t) = \\frac{B}{T}$ for all $t \\in [0,T]$ and $u(t)=0$ otherwise.\n- Schedule $1$ (weekly pulses, $1$-day each): on days starting at $t=0,7,14,\\dots$ strictly less than $T$, infuse at rate $u(t) = \\frac{B}{n_\\mathrm{w}}$ for $1$ day, where $n_\\mathrm{w}$ is the number of pulses within $[0,T)$; otherwise $u(t)=0$.\n- Schedule $2$ (biweekly pulses, $1$-day each): on days starting at $t=0,14,28,\\dots$ strictly less than $T$, infuse at rate $u(t) = \\frac{B}{n_\\mathrm{b}}$ for $1$ day, where $n_\\mathrm{b}$ is the number of pulses within $[0,T)$; otherwise $u(t)=0$.\n- Schedule $3$ (front-loaded, first $3$ days): $u(t) = \\frac{B}{d}$ for $t \\in [0,d)$ with $d=\\min(3,T)$, and $u(t)=0$ otherwise.\n\nFor numerical realism, all parameters are scientifically plausible and self-consistent. Use the following test suite of parameter sets $(r,K,\\kappa,N_0,T,B)$:\n- Case $1$: $(0.03,\\,10^9,\\,0.002,\\,10^8,\\,42,\\,600)$\n- Case $2$: $(0.06,\\,10^9,\\,0.0015,\\,2\\times 10^8,\\,42,\\,600)$\n- Case $3$: $(0.03,\\,10^9,\\,0.0005,\\,10^8,\\,42,\\,600)$\n- Case $4$: $(0.005,\\,10^9,\\,0.002,\\,3\\times 10^8,\\,42,\\,600)$\n- Case $5$ (boundary condition with no treatment budget): $(0.03,\\,10^9,\\,0.002,\\,10^8,\\,42,\\,0)$\n\nAll time quantities must be handled in days, $N(t)$ must be interpreted in cells, infusion rates $u(t)$ in $\\mathrm{mg}/\\mathrm{m}^2/\\mathrm{day}$, and $A$ is dimensionless.\n\nYour program should output, for the above cases in order, the index of the optimal schedule for each case, aggregated into a single line of output as a comma-separated list enclosed in square brackets. For example, the output should be of the form $[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4,\\text{result}_5]$, where each $\\text{result}_i$ is an integer in $\\{0,1,2,3\\}$.",
            "solution": "The user-provided problem is a valid and well-posed task in computational systems biology and personalized medicine. It asks for the optimization of a chemotherapy dosing strategy by comparing a discrete set of protocols against a biologically relevant objective function. The underlying model is a standard logistic growth model modified with a first-order drug kill term, which is a scientifically grounded simplification of tumor dynamics under treatment. All parameters, constraints, and objective functions are clearly and consistently defined.\n\nThe problem is to find, for each patient-specific parameter set, which of four defined dosing schedules minimizes the normalized cumulative tumor burden, $A$. This requires solving an ordinary differential equation (ODE) for each schedule and then evaluating an integral of the resulting tumor population trajectory.\n\nThe governing ODE for the tumor population $N(t)$ is:\n$$\n\\dot N(t) = \\frac{dN}{dt} = r\\,N(t)\\left(1 - \\frac{N(t)}{K}\\right) - \\kappa\\,u(t)\\,N(t)\n$$\nwith the initial condition $N(0) = N_0$. The parameters are the intrinsic growth rate $r$, the carrying capacity $K$, the drug kill sensitivity $\\kappa$, and the initial tumor size $N_0$. The function $u(t)$ is the time-varying chemotherapy infusion rate.\n\nThe objective is to minimize the normalized cumulative tumor burden, a dimensionless quantity defined as:\n$$\nA[u] = \\frac{1}{T}\\int_0^T \\frac{N(t; u)}{K}\\,dt\n$$\nwhere $N(t; u)$ is the solution of the ODE under the control protocol $u(t)$ over the time horizon $[0, T]$. All protocols must adhere to a fixed total dose budget $B$, such that $\\int_0^T u(t)\\,dt = B$.\n\nThe four schedules to be compared are:\n1.  **Schedule $0$ (Metronomic):** A continuous, constant infusion over the entire horizon.\n    $$\n    u_0(t) = \\begin{cases} B/T  \\text{for } t \\in [0, T] \\\\ 0  \\text{otherwise} \\end{cases}\n    $$\n2.  **Schedule $1$ (Weekly Pulses):** Short, high-dose infusions at the beginning of each week. Let the period be $P_1 = 7$ days. The number of pulses, $n_\\mathrm{w}$, is the count of non-negative integers $k$ such that $k P_1  T$.\n    $$\n    u_1(t) = \\begin{cases} B/n_\\mathrm{w}  \\text{if } t \\in [kP_1, kP_1+1) \\text{ for some integer } k \\ge 0 \\text{ with } kP_1  T \\\\ 0  \\text{otherwise} \\end{cases}\n    $$\n3.  **Schedule $2$ (Biweekly Pulses):** Similar to weekly, but with a period of $P_2 = 14$ days. The number of pulses is $n_\\mathrm{b}$.\n    $$\n    u_2(t) = \\begin{cases} B/n_\\mathrm{b}  \\text{if } t \\in [kP_2, kP_2+1) \\text{ for some integer } k \\ge 0 \\text{ with } kP_2  T \\\\ 0  \\text{otherwise} \\end{cases}\n    $$\n4.  **Schedule $3$ (Front-loaded):** The entire dose is delivered at a high rate at the beginning of the treatment period. Let $d = \\min(3, T)$.\n    $$\n    u_3(t) = \\begin{cases} B/d  \\text{for } t \\in [0, d) \\\\ 0  \\text{otherwise} \\end{cases}\n    $$\nThe ODE is a Bernoulli equation and can be solved analytically for piecewise-constant $u(t)$. However, a numerical approach is more robust, general, and less prone to implementation errors with complex boundary conditions. We will employ a high-quality numerical ODE solver.\n\n**Numerical Integration Strategy:**\nThe core of the solution lies in the numerical integration of the ODE for each schedule.\n-   **ODE Solver:** We use `scipy.integrate.solve_ivp`, a modern adaptive step-size solver. This function is well-suited for the potentially stiff dynamics of the system (e.g., rapid cell kill followed by slow regrowth). We configure it to produce a dense output, which is a continuous interpolation of the solution, allowing for accurate evaluation at any time point.\n-   **Tumor Extinction:** The problem specifies that if $N(t)$ reaches zero, it remains zero. Since the cell population $N(t)$ cannot be negative, and the ODE ensures $N(t)$ only approaches zero asymptotically from a positive initial condition, we interpret \"reaches extinction\" as a practical threshold. We set this threshold at $N(t)  1$ cell, as a non-integer population is not physical. The `events` feature of `solve_ivp` is used to detect when the solution $N(t)$ crosses this threshold from above. When this event occurs, the integration is terminated at the event time, $t_\\mathrm{ext}$.\n-   **Objective Function Calculation:** The integral for $A$ is computed using `scipy.integrate.quad`. Given the dense output from `solve_ivp`, this provides a highly accurate evaluation of the integral. If an extinction event occurs at $t_\\mathrm{ext}  T$, the integral is computed only up to $t_\\mathrm{ext}$, as $N(t)=0$ for $t  t_\\mathrm{ext}$:\n    $$\n    A = \\frac{1}{TK} \\int_0^{t_\\mathrm{ext}} N(t)\\,dt\n    $$\n    where $t_\\mathrm{ext} = T$ if no extinction event occurs.\n\n**Algorithmic Procedure:**\nThe overall algorithm systematically evaluates each case and schedule:\n1.  Iterate through each of the five parameter sets (test cases).\n2.  For each case, iterate through the four dosing schedules (index $0$ to $3$).\n3.  For each schedule:\n    a. Construct the corresponding control function $u(t)$ based on the schedule's definition and the case parameters $T$ and $B$.\n    b. Define the ODE's right-hand-side function, which incorporates the specific $u(t)$.\n    c. Define a terminal event for the ODE solver to handle tumor extinction at $N(t)  1$.\n    d. Numerically solve the initial value problem over $[0, T]$ using `solve_ivp`.\n    e. Compute the objective function $A$ by integrating the resulting solution using `quad`.\n4.  After evaluating all four schedules for a given case, determine the minimum value of $A$.\n5.  Identify the index of the optimal schedule. In case of a tie, defined by $|A_i - A_j| \\le 10^{-6}$ for the best schedules, the one with the smallest index is chosen as per the problem's tie-breaking rule.\n6.  Collect the optimal indices for all test cases.\n7.  Format the final result as a comma-separated list enclosed in square brackets.\n\nThis principled and robust numerical approach ensures accurate and reliable comparison of the treatment protocols, leading to the identification of the optimal strategy for each patient-specific model.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.integrate import solve_ivp, quad\nimport math\n\ndef solve():\n    \"\"\"\n    Solves the tumor growth ODE for various chemotherapy schedules to find the optimal one.\n    \"\"\"\n\n    test_cases = [\n        # (r, K, kappa, N0, T, B)\n        (0.03, 1e9, 0.002, 1e8, 42, 600),\n        (0.06, 1e9, 0.0015, 2e8, 42, 600),\n        (0.03, 1e9, 0.0005, 1e8, 42, 600),\n        (0.005, 1e9, 0.002, 3e8, 42, 600),\n        (0.03, 1e9, 0.002, 1e8, 42, 0),\n    ]\n\n    final_results = []\n\n    for case in test_cases:\n        r, K, kappa, N0, T, B = case\n        \n        # Define the four dosing schedules u(t)\n        schedules = []\n\n        # Schedule 0: Metronomic\n        def u0(t, T_loc=T, B_loc=B):\n            if 0 = t  T_loc:\n                return B_loc / T_loc if T_loc > 0 else 0\n            return 0\n        schedules.append(u0)\n\n        # Schedule 1: Weekly pulses\n        def u1(t, T_loc=T, B_loc=B):\n            if t  0 or t >= T_loc or T_loc = 0:\n                return 0\n            period = 7\n            # Number of pulses: non-negative integers k where k*period  T\n            # (equivalent to floor((T-epsilon)/period) + 1)\n            num_pulses = math.floor((T_loc - 1e-9) / period) + 1\n            if num_pulses == 0: return 0\n            pulse_rate = B_loc / num_pulses\n            if (t / period) - math.floor(t / period)  (1 / period):\n                return pulse_rate\n            return 0\n        schedules.append(u1)\n        \n        # Schedule 2: Biweekly pulses\n        def u2(t, T_loc=T, B_loc=B):\n            if t  0 or t >= T_loc or T_loc = 0:\n                return 0\n            period = 14\n            num_pulses = math.floor((T_loc - 1e-9) / period) + 1\n            if num_pulses == 0: return 0\n            pulse_rate = B_loc / num_pulses\n            if (t / period) - math.floor(t / period)  (1 / period):\n                return pulse_rate\n            return 0\n        schedules.append(u2)\n        \n        # Schedule 3: Front-loaded\n        def u3(t, T_loc=T, B_loc=B):\n            d = min(3, T_loc)\n            if 0 = t  d:\n                return B_loc / d if d > 0 else 0\n            return 0\n        schedules.append(u3)\n\n        A_values = []\n        for u_func in schedules:\n            # Define the ODE system\n            def ode_func(t, y, r_loc, K_loc, kappa_loc, u_f):\n                N = y[0]\n                # Ensure N doesn't go below 0 for robustness\n                if N  0:\n                    N = 0\n                dN_dt = r_loc * N * (1 - N / K_loc) - kappa_loc * u_f(t) * N\n                return [dN_dt]\n\n            # Event function to detect tumor extinction (N  1 cell)\n            def extinction(t, y):\n                return y[0] - 1\n            extinction.terminal = True\n            extinction.direction = -1\n\n            # Solve the ODE\n            sol = solve_ivp(\n                lambda t, y: ode_func(t, y, r, K, kappa, u_func),\n                (0, T),\n                [N0],\n                dense_output=True,\n                events=extinction,\n                method='RK45',\n                atol=1e-8,\n                rtol=1e-8\n            )\n\n            # Determine the integration end time\n            t_end = T\n            if sol.t_events and len(sol.t_events[0]) > 0:\n                t_end = sol.t_events[0][0]\n\n            # Calculate the cumulative burden using quad for accuracy\n            # Integrate the dense solution N(t)\n            integral_val, _ = quad(lambda t: sol.sol(t)[0], 0, t_end)\n            \n            # Calculate normalized cumulative burden A\n            A = (1 / (T * K)) * integral_val if T > 0 else 0.\n            A_values.append(A)\n\n        # Find the best schedule index with tie-breaking\n        min_A = min(A_values)\n        \n        # Find all indices that are close to the minimum value\n        tied_indices = np.where(np.isclose(A_values, min_A, rtol=0, atol=1e-6))[0]\n        \n        # Choose the one with the smallest index\n        best_index = np.min(tied_indices)\n        final_results.append(best_index)\n\n    # Print the final results in the required format\n    print(f\"[{','.join(map(str, final_results))}]\")\n\nsolve()\n```"
        }
    ]
}