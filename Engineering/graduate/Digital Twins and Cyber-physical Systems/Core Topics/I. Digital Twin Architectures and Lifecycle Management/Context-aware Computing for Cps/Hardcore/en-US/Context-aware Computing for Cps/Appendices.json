{
    "hands_on_practices": [
        {
            "introduction": "The journey from a continuous physical context to a digital representation begins with sampling. This exercise grounds you in the foundational principles of this conversion, starting with the Nyquist-Shannon sampling theorem, which dictates the minimum rate to capture a signal without losing information. You will then move beyond ideal theory to a critical real-world challenge: quantifying how small random variations in sampling time, known as jitter, can introduce errors into the reconstructed signal within a Digital Twin .",
            "id": "4210122",
            "problem": "A cyber-physical system (CPS) with a Digital Twin (DT) must continuously estimate a time-varying context signal $c(t)$ for context-aware state estimation and control. The signal $c(t)$ is known to be real-valued, zero-mean, wide-sense stationary (WSS), and strictly bandlimited to a two-sided bandwidth $B$ Hz, meaning its Fourier transform vanishes outside $|f| \\leq B$. The CPS acquires $c(t)$ using a uniform sampler with nominal sampling period $T_{s}$ and nominal sampling frequency $f_{s} = 1/T_{s}$, and the DT reconstructs the signal using ideal bandlimited interpolation on the nominal uniform grid, i.e., it assumes uniform samples at times $t = n T_{s}$ and synthesizes the continuous-time estimate by ideal $\\operatorname{sinc}$ interpolation, where $\\operatorname{sinc}(x) = \\frac{\\sin(\\pi x)}{\\pi x}$.\n\nHowever, the actual sampling hardware exhibits small random timing jitter: the $n$-th sample is taken at time $t_{n} = n T_{s} + \\epsilon_{n}$, where $\\{\\epsilon_{n}\\}$ are independent, identically distributed zero-mean random variables with variance $\\sigma_{t}^{2}$, independent of $c(t)$, and satisfying $|\\epsilon_{n}| \\ll T_{s}$ almost surely. The DT is unaware of the jitter and applies ideal $\\operatorname{sinc}$ reconstruction as if the samples arrived at times $t = n T_{s}$. Let $P = \\mathbb{E}[c^{2}(t)]$ denote the signal power. Assume the two-sided power spectral density (PSD) of $c(t)$ is flat over the support $|f| \\leq B$, i.e., $S_{c}(f) = S_{0}$ for $|f| \\leq B$ and $S_{c}(f) = 0$ otherwise.\n\nTasks:\n- Using only the definitions of time-domain sampling, the Fourier transform, and bandlimitedness, derive a condition on $f_{s}$ that guarantees that $c(t)$ can be captured without aliasing under ideal uniform sampling. From that condition, determine the minimum sampling frequency $f_{s,\\min}$ that avoids aliasing.\n- In the small-jitter limit and under the stated statistical assumptions, derive from first principles the mean-square reconstruction error at the nominal sampling instants $t = m T_{s}$ incurred by the DT’s ideal bandlimited reconstruction that ignores jitter. Express this mean-square error in closed form as a function of $B$, $\\sigma_{t}$, and $P$.\n\nAnswer specification:\n- Express $f_{s,\\min}$ in Hz.\n- Express the mean-square error as an absolute mean-square quantity (same units as $c^{2}$).\n- Provide the final answer as a single row matrix with two entries $\\big(f_{s,\\min}, \\text{MSE}_{\\text{jitter}}\\big)$.",
            "solution": "The problem statement is parsed and validated against the specified criteria.\n\n**Step 1: Extract Givens**\n- Signal $c(t)$: Real-valued, zero-mean ($\\mathbb{E}[c(t)]=0$), wide-sense stationary (WSS), strictly bandlimited to $|f| \\leq B$.\n- Signal Power: $P = \\mathbb{E}[c^{2}(t)]$.\n- Power Spectral Density (PSD): $S_{c}(f) = S_{0}$ for $|f| \\leq B$ and $S_{c}(f) = 0$ otherwise.\n- Nominal Sampling: Period $T_{s}$, frequency $f_{s} = 1/T_{s}$.\n- Jittered Sampling Instants: $t_{n} = n T_{s} + \\epsilon_{n}$.\n- Jitter Statistics: $\\{\\epsilon_{n}\\}$ are independent and identically distributed (i.i.d.), with zero mean ($\\mathbb{E}[\\epsilon_{n}]=0$) and variance $\\sigma_{t}^{2} = \\mathbb{E}[\\epsilon_n^2]$. The jitter is independent of the signal $c(t)$.\n- Jitter Magnitude: $|\\epsilon_{n}| \\ll T_{s}$ almost surely.\n- Reconstruction: The Digital Twin (DT) uses ideal sinc interpolation assuming uniform samples at $t = nT_s$: $\\hat{c}(t) = \\sum_{n=-\\infty}^{\\infty} c(t_n) \\operatorname{sinc}\\left(\\frac{t - n T_s}{T_s}\\right)$.\n- Sinc function definition: $\\operatorname{sinc}(x) = \\frac{\\sin(\\pi x)}{\\pi x}$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded in the principles of digital signal processing, specifically sampling theory and the analysis of non-ideal sampling effects like jitter. The assumptions—a strictly bandlimited WSS signal, a flat PSD, and small i.i.d. zero-mean jitter—are standard idealizations used to make the problem analytically tractable. The problem is well-posed, with all necessary information provided to derive the requested quantities. The tasks are clearly defined, and the language is objective and formal. The problem does not violate any of the invalidity criteria.\n\n**Step 3: Verdict and Action**\nThe problem is deemed valid. A complete solution will be provided.\n\n**Derivation**\n\nThe solution is derived in two parts as requested by the problem statement.\n\n**Part 1: Minimum Sampling Frequency for Alias-Free Sampling**\n\nAn ideal uniform sampling process can be modeled as the multiplication of the continuous-time signal $c(t)$ with an impulse train $\\sum_{n=-\\infty}^{\\infty} \\delta(t - n T_{s})$. The resulting sampled signal is $c_s(t) = c(t) \\sum_{n=-\\infty}^{\\infty} \\delta(t - n T_{s})$.\n\nThe Fourier transform of the impulse train is another impulse train, given by the Poisson summation formula:\n$$ \\mathcal{F}\\left\\{\\sum_{n=-\\infty}^{\\infty} \\delta(t - n T_{s})\\right\\} = \\frac{1}{T_s} \\sum_{k=-\\infty}^{\\infty} \\delta\\left(f - \\frac{k}{T_s}\\right) = f_s \\sum_{k=-\\infty}^{\\infty} \\delta(f - kf_s) $$\nMultiplication in the time domain corresponds to convolution in the frequency domain. Let $C(f) = \\mathcal{F}\\{c(t)\\}$ be the Fourier transform of the original signal. The Fourier transform of the sampled signal, $C_s(f)$, is:\n$$ C_s(f) = C(f) * \\left( f_s \\sum_{k=-\\infty}^{\\infty} \\delta(f - kf_s) \\right) $$\nThe convolution with a shifted delta function results in a shifted version of the original function. Therefore, the spectrum of the sampled signal consists of scaled replicas of the original spectrum, centered at integer multiples of the sampling frequency $f_s$:\n$$ C_s(f) = f_s \\sum_{k=-\\infty}^{\\infty} C(f - kf_s) $$\nThe signal $c(t)$ is strictly bandlimited to $B$, which means its Fourier transform $C(f)$ is zero for $|f|  B$. Consequently, each spectral replica $C(f - kf_s)$ has its support within the interval $[kf_s - B, kf_s + B]$.\n\nAliasing occurs when these spectral replicas overlap. To ensure that the original signal can be perfectly recovered from its samples (by ideal low-pass filtering), the replicas must be disjoint. The condition for no overlap is that the upper edge of the baseband spectrum (for $k=0$, at $f=B$) must not exceed the lower edge of the adjacent spectrum (for $k=1$, at $f=f_s - B$).\n$$ B \\leq f_s - B $$\nRearranging this inequality gives the condition on the sampling frequency to avoid aliasing:\n$$ f_s \\geq 2B $$\nThis is the Nyquist-Shannon sampling criterion. The minimum sampling frequency that satisfies this condition is the Nyquist rate:\n$$ f_{s,\\min} = 2B $$\n\n**Part 2: Mean-Square Reconstruction Error due to Jitter**\n\nThe DT reconstructs the signal using ideal sinc interpolation based on the jittered samples $c(t_n) = c(nT_s + \\epsilon_n)$, but assumes these samples occurred at the nominal times $nT_s$. The reconstructed signal is:\n$$ \\hat{c}(t) = \\sum_{n=-\\infty}^{\\infty} c(t_n) \\operatorname{sinc}\\left(\\frac{t - n T_s}{T_s}\\right) $$\nWe are asked to find the mean-square error (MSE) at the nominal sampling instants, $t = m T_{s}$ for an integer $m$. Let's evaluate the reconstructed signal at such an instant:\n$$ \\hat{c}(mT_s) = \\sum_{n=-\\infty}^{\\infty} c(t_n) \\operatorname{sinc}\\left(\\frac{m T_s - n T_s}{T_s}\\right) = \\sum_{n=-\\infty}^{\\infty} c(t_n) \\operatorname{sinc}(m-n) $$\nThe sinc function has the property $\\operatorname{sinc}(k) = 1$ if $k=0$ and $\\operatorname{sinc}(k)=0$ for any non-zero integer $k$. Thus, the sum collapses to a single term where $n=m$:\n$$ \\hat{c}(mT_s) = c(t_m) \\operatorname{sinc}(0) = c(t_m) = c(mT_s + \\epsilon_m) $$\nThe error at the instant $t=mT_s$ is the difference between the true signal value and the reconstructed value:\n$$ e_m = c(mT_s) - \\hat{c}(mT_s) = c(mT_s) - c(mT_s + \\epsilon_m) $$\nThe problem states that the jitter is small, i.e., $|\\epsilon_n| \\ll T_s$. This allows us to use a first-order Taylor series expansion for $c(mT_s + \\epsilon_m)$ around the point $mT_s$:\n$$ c(mT_s + \\epsilon_m) \\approx c(mT_s) + c'(mT_s) \\epsilon_m $$\nwhere $c'(t)$ is the time derivative of $c(t)$. Substituting this approximation into the error expression gives:\n$$ e_m \\approx c(mT_s) - (c(mT_s) + c'(mT_s) \\epsilon_m) = -c'(mT_s) \\epsilon_m $$\nThe mean-square error, denoted $\\text{MSE}_{\\text{jitter}}$, is the expected value of the squared error:\n$$ \\text{MSE}_{\\text{jitter}} = \\mathbb{E}[e_m^2] \\approx \\mathbb{E}[(-c'(mT_s) \\epsilon_m)^2] = \\mathbb{E}[(c'(mT_s))^2 \\epsilon_m^2] $$\nThe jitter $\\{\\epsilon_n\\}$ is independent of the signal $c(t)$, and therefore also independent of its derivative $c'(t)$. This allows us to separate the expectation:\n$$ \\text{MSE}_{\\text{jitter}} \\approx \\mathbb{E}[(c'(mT_s))^2] \\mathbb{E}[\\epsilon_m^2] $$\nFrom the given information, $\\mathbb{E}[\\epsilon_m^2] = \\sigma_t^2$. The signal $c(t)$ is WSS, so the power of its derivative, $\\mathbb{E}[(c'(t))^2]$, is constant for all time. We can find this power using the PSD. The Fourier transform of the derivative is $\\mathcal{F}\\{c'(t)\\} = (j2\\pi f)C(f)$. The PSD of the derivative signal, $S_{c'}(f)$, is related to $S_c(f)$ by:\n$$ S_{c'}(f) = |j2\\pi f|^2 S_c(f) = (2\\pi f)^2 S_c(f) $$\nThe power of the derivative is the integral of its PSD over all frequencies:\n$$ \\mathbb{E}[(c'(t))^2] = \\int_{-\\infty}^{\\infty} S_{c'}(f) df = \\int_{-\\infty}^{\\infty} (2\\pi f)^2 S_c(f) df $$\nGiven $S_c(f) = S_0$ for $|f| \\leq B$ and $0$ otherwise, the integral becomes:\n$$ \\mathbb{E}[(c'(t))^2] = \\int_{-B}^{B} (2\\pi f)^2 S_0 df = 4\\pi^2 S_0 \\int_{-B}^{B} f^2 df = 4\\pi^2 S_0 \\left[\\frac{f^3}{3}\\right]_{-B}^{B} = 4\\pi^2 S_0 \\left(\\frac{B^3}{3} - \\frac{(-B)^3}{3}\\right) = \\frac{8\\pi^2 S_0 B^3}{3} $$\nNext, we relate the constant $S_0$ to the total signal power $P = \\mathbb{E}[c^2(t)]$. The power is the integral of the PSD of $c(t)$:\n$$ P = \\int_{-\\infty}^{\\infty} S_c(f) df = \\int_{-B}^{B} S_0 df = S_0 [f]_{-B}^{B} = 2BS_0 $$\nThis gives $S_0 = \\frac{P}{2B}$. Substituting this into the expression for the derivative's power:\n$$ \\mathbb{E}[(c'(t))^2] = \\frac{8\\pi^2 B^3}{3} \\left(\\frac{P}{2B}\\right) = \\frac{4\\pi^2 P B^2}{3} $$\nFinally, we substitute this back into the MSE equation:\n$$ \\text{MSE}_{\\text{jitter}} \\approx \\mathbb{E}[(c'(t))^2] \\mathbb{E}[\\epsilon_m^2] = \\left(\\frac{4\\pi^2 P B^2}{3}\\right) \\sigma_t^2 = \\frac{4\\pi^2 P B^2 \\sigma_t^2}{3} $$\nThis expression represents the mean-square error at the nominal sampling points due to the DT's ignorance of the sampling jitter, under the small-jitter approximation.",
            "answer": "$$ \\boxed{ \\begin{pmatrix} 2B  \\frac{4\\pi^{2} P B^{2} \\sigma_{t}^{2}}{3} \\end{pmatrix} } $$"
        },
        {
            "introduction": "No sensor is perfect; the data they provide is an imperfect reflection of reality. This practice provides a systematic framework for understanding and quantifying these imperfections by modeling distinct error sources such as bias, drift, precision noise, and quantization. By deriving the total estimation error from these components, you will develop a crucial skill for building robust cyber-physical systems: creating a comprehensive error budget for your context sources .",
            "id": "4210107",
            "problem": "A Cyber-Physical System (CPS) deploys a context sensor to monitor a scalar context state $z$ (for example, occupancy level encoded as a scalar latent variable in a digital twin). In this setting, the following fundamental definitions apply to the sensor: accuracy is the closeness of a measured or estimated value to the true state $z$; precision is the reproducibility of measurements under unchanged conditions and is quantified by the variance of zero-mean random fluctuations; bias is a fixed, systematic offset $b$ between the sensor output and the true state; drift is a slowly varying deviation $d(t)$ arising from internal sensor changes over time; resolution is the smallest distinguishable change $q$ in the sensor output, resulting in quantization of readings.\n\nOver a short estimation window of $N$ samples taken at uniform intervals $\\Delta t$, assume the true context state is constant at $z$ and the sensor produces measurements\n$$\nx_{k} \\;=\\; z \\;+\\; b \\;+\\; d_{k} \\;+\\; \\epsilon_{k} \\;+\\; \\eta_{k}, \\quad k=1,2,\\dots,N,\n$$\nwith the following characteristics:\n- The bias $b$ is a constant (unknown but time-invariant over the window).\n- The drift sequence $\\{d_{k}\\}$ is a discrete-time random walk with $d_{0}=0$ and $d_{k} = \\sum_{m=1}^{k} w_{m}$, where $\\{w_{m}\\}$ are independent, zero-mean Gaussian increments with $\\mathrm{Var}(w_{m}) = D\\,\\Delta t$, for a known diffusion coefficient $D0$.\n- The precision noise $\\{\\epsilon_{k}\\}$ are independent, zero-mean Gaussian with $\\mathrm{Var}(\\epsilon_{k}) = \\sigma_{p}^{2}$.\n- The resolution-induced quantization noise $\\{\\eta_{k}\\}$ are independent and identically distributed uniform random variables on $[-q/2,q/2]$, independent of all other terms.\n\nThe estimator used by the digital twin is the sample mean,\n$$\n\\hat{z} \\;=\\; \\frac{1}{N}\\sum_{k=1}^{N} x_{k}.\n$$\n\nStarting from the core definitions of mean squared error and properties of independent random processes, derive a closed-form expression for the mean squared error\n$$\nE\\!\\left[(z - \\hat{z})^{2}\\right]\n$$\nin terms of $b$, $D$, $\\Delta t$, $N$, $\\sigma_{p}^{2}$, and $q$. In your derivation, explicitly identify how bias, drift, precision, and resolution each contribute to the mean squared error. No rounding is required; present your final result as a single closed-form symbolic expression.",
            "solution": "The problem requires the derivation of the mean squared error (MSE) for the estimator $\\hat{z}$ of a true constant state $z$. The MSE is defined as $E[(z - \\hat{z})^{2}]$.\n\nThe estimator is the sample mean over $N$ measurements:\n$$\n\\hat{z} \\;=\\; \\frac{1}{N}\\sum_{k=1}^{N} x_{k}\n$$\nThe measurement $x_k$ is given by the model:\n$$\nx_{k} \\;=\\; z \\;+\\; b \\;+\\; d_{k} \\;+\\; \\epsilon_{k} \\;+\\; \\eta_{k}\n$$\nwhere $z$ and the bias $b$ are constants.\n\nFirst, we express the estimation error, $z - \\hat{z}$. Substituting the expressions for $\\hat{z}$ and $x_k$:\n$$\nz - \\hat{z} \\;=\\; z - \\frac{1}{N}\\sum_{k=1}^{N} (z + b + d_{k} + \\epsilon_{k} + \\eta_{k})\n$$\nBy distributing the summation and the factor $\\frac{1}{N}$:\n$$\nz - \\hat{z} \\;=\\; z - \\left(\\frac{1}{N}\\sum_{k=1}^{N}z + \\frac{1}{N}\\sum_{k=1}^{N}b + \\frac{1}{N}\\sum_{k=1}^{N}d_{k} + \\frac{1}{N}\\sum_{k=1}^{N}\\epsilon_{k} + \\frac{1}{N}\\sum_{k=1}^{N}\\eta_{k}\\right)\n$$\n$$\nz - \\hat{z} \\;=\\; z - \\left(\\frac{N z}{N} + \\frac{N b}{N} + \\frac{1}{N}\\sum_{k=1}^{N}d_{k} + \\frac{1}{N}\\sum_{k=1}^{N}\\epsilon_{k} + \\frac{1}{N}\\sum_{k=1}^{N}\\eta_{k}\\right)\n$$\n$$\nz - \\hat{z} \\;=\\; -b - \\frac{1}{N}\\sum_{k=1}^{N}d_{k} - \\frac{1}{N}\\sum_{k=1}^{N}\\epsilon_{k} - \\frac{1}{N}\\sum_{k=1}^{N}\\eta_{k}\n$$\nThe MSE is commonly decomposed into the sum of the variance of the estimator and its squared bias. For an estimator $\\hat{\\theta}$ of a parameter $\\theta$, $\\mathrm{MSE} = \\mathrm{Var}(\\hat{\\theta}) + (\\mathrm{Bias}(\\hat{\\theta}))^2$. Here, the error is $z-\\hat{z}$.\n$$\nE[(z - \\hat{z})^{2}] \\;=\\; \\mathrm{Var}(z - \\hat{z}) + (E[z - \\hat{z}])^{2}\n$$\nSince $z$ is a non-random constant, $\\mathrm{Var}(z - \\hat{z}) = \\mathrm{Var}(-\\hat{z}) = \\mathrm{Var}(\\hat{z})$. The expression becomes:\n$$\nE[(z - \\hat{z})^{2}] \\;=\\; \\mathrm{Var}(\\hat{z}) + (E[z - \\hat{z}])^{2}\n$$\nWe will compute these two components.\n\nFirst, let's determine the bias contribution, $(E[z - \\hat{z}])^{2}$.\n$$\nE[z - \\hat{z}] = E\\left[-b - \\frac{1}{N}\\sum_{k=1}^{N}d_{k} - \\frac{1}{N}\\sum_{k=1}^{N}\\epsilon_{k} - \\frac{1}{N}\\sum_{k=1}^{N}\\eta_{k}\\right]\n$$\nUsing the linearity of the expectation operator:\n$$\nE[z - \\hat{z}] = -b - \\frac{1}{N}\\sum_{k=1}^{N}E[d_{k}] - \\frac{1}{N}\\sum_{k=1}^{N}E[\\epsilon_{k}] - \\frac{1}{N}\\sum_{k=1}^{N}E[\\eta_{k}]\n$$\nThe problem states that the drift increments $\\{w_{m}\\}$, precision noise $\\{\\epsilon_{k}\\}$, and quantization noise $\\{\\eta_{k}\\}$ are all zero-mean random variables.\n- For drift, $d_k = \\sum_{m=1}^{k} w_m$, so $E[d_k] = \\sum_{m=1}^{k} E[w_m] = 0$.\n- For precision noise, $E[\\epsilon_k] = 0$.\n- For quantization noise, $\\eta_k \\sim U[-q/2, q/2]$, so $E[\\eta_k] = 0$.\nSubstituting these zero expected values:\n$$\nE[z - \\hat{z}] = -b - 0 - 0 - 0 = -b\n$$\nThe squared bias contribution to the MSE is therefore $(-b)^{2} = b^{2}$. This term arises from the systematic bias $b$ of the sensor.\n\nNext, we compute the variance contribution, $\\mathrm{Var}(\\hat{z})$.\n$$\n\\hat{z} = z + b + \\frac{1}{N}\\sum_{k=1}^{N}d_{k} + \\frac{1}{N}\\sum_{k=1}^{N}\\epsilon_{k} + \\frac{1}{N}\\sum_{k=1}^{N}\\eta_{k}\n$$\nThe variance of a random variable is unaffected by adding constants, so $z$ and $b$ do not contribute.\n$$\n\\mathrm{Var}(\\hat{z}) = \\mathrm{Var}\\left(\\frac{1}{N}\\sum_{k=1}^{N}d_{k} + \\frac{1}{N}\\sum_{k=1}^{N}\\epsilon_{k} + \\frac{1}{N}\\sum_{k=1}^{N}\\eta_{k}\\right)\n$$\nThe drift process, precision noise, and quantization noise are stated to be independent of each other. Therefore, the variance of their sum is the sum of their variances:\n$$\n\\mathrm{Var}(\\hat{z}) = \\mathrm{Var}\\left(\\frac{1}{N}\\sum_{k=1}^{N}d_{k}\\right) + \\mathrm{Var}\\left(\\frac{1}{N}\\sum_{k=1}^{N}\\epsilon_{k}\\right) + \\mathrm{Var}\\left(\\frac{1}{N}\\sum_{k=1}^{N}\\eta_{k}\\right)\n$$\nWe now compute each variance component, which correspond to the contributions from drift, precision, and resolution.\n\n1.  **Precision Contribution:** The precision noise terms $\\{\\epsilon_{k}\\}$ are independent and identically distributed with $\\mathrm{Var}(\\epsilon_{k})=\\sigma_{p}^{2}$.\n    $$\n    \\mathrm{Var}\\left(\\frac{1}{N}\\sum_{k=1}^{N}\\epsilon_{k}\\right) = \\frac{1}{N^2}\\mathrm{Var}\\left(\\sum_{k=1}^{N}\\epsilon_{k}\\right) = \\frac{1}{N^2}\\sum_{k=1}^{N}\\mathrm{Var}(\\epsilon_{k}) = \\frac{1}{N^2}(N\\sigma_{p}^{2}) = \\frac{\\sigma_{p}^{2}}{N}\n    $$\n2.  **Resolution Contribution:** The quantization noise terms $\\{\\eta_k\\}$ are i.i.d. uniform random variables on $[-q/2, q/2]$. The variance of a uniform distribution $U[a,b]$ is $(b-a)^2/12$.\n    $$\n    \\mathrm{Var}(\\eta_k) = \\frac{(q/2 - (-q/2))^2}{12} = \\frac{q^2}{12}\n    $$\n    Similar to the precision noise term:\n    $$\n    \\mathrm{Var}\\left(\\frac{1}{N}\\sum_{k=1}^{N}\\eta_{k}\\right) = \\frac{1}{N^2}\\sum_{k=1}^{N}\\mathrm{Var}(\\eta_{k}) = \\frac{1}{N^2}\\left(N\\frac{q^2}{12}\\right) = \\frac{q^2}{12N}\n    $$\n3.  **Drift Contribution:** This calculation is more involved because the drift samples $\\{d_k\\}$ are not independent.\n    $$\n    \\mathrm{Var}_{d} = \\mathrm{Var}\\left(\\frac{1}{N}\\sum_{k=1}^{N}d_{k}\\right) = \\frac{1}{N^2}\\mathrm{Var}\\left(\\sum_{k=1}^{N}d_{k}\\right)\n    $$\n    Let $S_d = \\sum_{k=1}^{N}d_{k}$. Since $E[d_k]=0$, we have $E[S_d]=0$. Thus, $\\mathrm{Var}(S_d)=E[S_d^{2}]$.\n    $$\n    \\mathrm{Var}(S_d) = E\\left[\\left(\\sum_{i=1}^{N}d_{i}\\right)\\left(\\sum_{j=1}^{N}d_{j}\\right)\\right] = \\sum_{i=1}^{N}\\sum_{j=1}^{N}E[d_{i}d_{j}]\n    $$\n    We must evaluate the covariance term $E[d_i d_j]$. Let $i \\le j$. We have $d_i = \\sum_{m=1}^{i}w_m$ and $d_j = \\sum_{l=1}^{j}w_l$. The increments $\\{w_m\\}$ are independent and zero-mean with $\\mathrm{Var}(w_m) = D\\Delta t$. This implies $E[w_m w_l] = \\delta_{ml} D\\Delta t$, where $\\delta_{ml}$ is the Kronecker delta.\n    $$\n    E[d_{i}d_{j}] = E\\left[\\left(\\sum_{m=1}^{i}w_{m}\\right)\\left(\\sum_{l=1}^{j}w_{l}\\right)\\right] = \\sum_{m=1}^{i}\\sum_{l=1}^{j}E[w_{m}w_{l}] = \\sum_{m=1}^{i}\\sum_{l=1}^{j}\\delta_{ml}D\\Delta t = \\sum_{m=1}^{i} D\\Delta t = i D\\Delta t\n    $$\n    Generalizing for any $i, j$, we have $E[d_i d_j] = \\min(i,j)D\\Delta t$. Substituting this into the variance expression:\n    $$\n    \\mathrm{Var}(S_d) = \\sum_{i=1}^{N}\\sum_{j=1}^{N}\\min(i,j)D\\Delta t = D\\Delta t \\sum_{i=1}^{N}\\sum_{j=1}^{N}\\min(i,j)\n    $$\n    The double summation is evaluated using the identity $\\sum_{i=1}^{N}\\sum_{j=1}^{N}\\min(i,j) = \\frac{N(N+1)(2N+1)}{6}$. Therefore:\n    $$\n    \\mathrm{Var}(S_d) = D\\Delta t \\frac{N(N+1)(2N+1)}{6}\n    $$\n    The drift's contribution to the estimator variance is then:\n    $$\n    \\mathrm{Var}_{d} = \\frac{1}{N^2}\\mathrm{Var}(S_d) = \\frac{D\\Delta t}{N^2}\\frac{N(N+1)(2N+1)}{6} = D\\Delta t \\frac{(N+1)(2N+1)}{6N}\n    $$\n\nFinally, we assemble all the components to find the total mean squared error:\n$$\nE[(z - \\hat{z})^{2}] = (E[z - \\hat{z}])^2 + \\mathrm{Var}(\\hat{z}) = b^2 + \\mathrm{Var}_{d} + \\mathrm{Var}_{\\epsilon} + \\mathrm{Var}_{\\eta}\n$$\n$$\nE[(z - \\hat{z})^{2}] = b^2 + D\\Delta t \\frac{(N+1)(2N+1)}{6N} + \\frac{\\sigma_{p}^{2}}{N} + \\frac{q^2}{12N}\n$$\nThis final expression is the sum of the contributions from bias, drift, precision, and resolution, as requested.",
            "answer": "$$\n\\boxed{b^{2} + D\\Delta t \\frac{(N+1)(2N+1)}{6N} + \\frac{\\sigma_{p}^{2}}{N} + \\frac{q^{2}}{12N}}\n$$"
        },
        {
            "introduction": "The ultimate purpose of context awareness is to drive intelligent action. This exercise bridges the gap between uncertain information and optimal decision-making by placing you in the role of designing a context-aware controller. You will use Bayesian inference to update your belief about the system's state based on noisy sensor data and then derive an optimal action policy that maximizes utility while rigorously adhering to a probabilistic safety constraint .",
            "id": "4210115",
            "problem": "A building Heating, Ventilation, and Air Conditioning (HVAC) controller is part of a Cyber-Physical System (CPS) equipped with a Digital Twin (DT) that performs context-aware energy management. At each decision epoch, the DT models a latent context variable $X$ (dimensionless) representing the thermal safety margin that would remain if an energy-saving mode were engaged. The prior model of $X$ is Gaussian with $X \\sim \\mathcal{N}(\\mu, \\sigma^{2})$. A sensor provides a noisy measurement $Y$ of $X$ according to $Y = X + W$, where $W \\sim \\mathcal{N}(0, \\tau^{2})$ is independent of $X$. The controller chooses an action $a \\in \\{0,1\\}$, where $a=1$ engages the energy-saving mode and $a=0$ keeps nominal operation.\n\nA safety hazard occurs if the energy-saving mode is engaged while the actual margin $X$ is below a fixed hazard threshold $x_{h}$, namely if $a=1$ and $X \\leq x_{h}$. The controller must satisfy a chance constraint that conditions on the observed context: whenever it engages the energy-saving mode based on $Y=y$, the conditional probability of hazard must be at most a prescribed level $\\alpha \\in (0,1)$, i.e., it must hold that $$\\mathbb{P}\\!\\left(X \\leq x_{h} \\,\\middle|\\, Y=y\\right) \\leq \\alpha$$ for every $y$ at which $a=1$ is chosen.\n\nThe instantaneous loss function is $\\ell(a) = c_{1}\\,\\mathbf{1}\\{a=1\\} + c_{0}\\,\\mathbf{1}\\{a=0\\}$ with constants $c_{1}  c_{0}$, so engaging the energy-saving mode strictly reduces loss. Among context-aware policies that are measurable functions of $Y$, consider threshold policies of the form $a(y) = \\mathbf{1}\\{y \\geq t\\}$, where $t$ is a threshold. Starting from the fundamental definitions of conditional probability and Bayesian inference for Gaussian models, derive the optimal threshold $t^{\\star}$ that minimizes the expected loss subject to the chance constraint stated above. Provide your final result as a single closed-form analytic expression for $t^{\\star}$ in terms of $\\mu$, $\\sigma$, $\\tau$, $x_{h}$, and $\\alpha$. No numerical evaluation is required, and all variables are dimensionless.",
            "solution": "The Digital Twin (DT) must choose a context-aware policy that minimizes expected loss while satisfying a chance constraint on conditional hazard. Since the instantaneous loss is lower for $a=1$ than for $a=0$ (because $c_{1}  c_{0}$), the optimal policy will engage the energy-saving mode on the largest possible set of measurements $y$ that satisfy the chance constraint. Under a threshold policy $a(y) = \\mathbf{1}\\{y \\geq t\\}$, this means choosing the smallest threshold $t$ for which the chance constraint holds for all $y \\geq t$.\n\nWe start from the fundamental definitions. The chance constraint requires, for each $y$ at which $a=1$ is chosen,\n$$\n\\mathbb{P}\\!\\left(X \\leq x_{h} \\,\\middle|\\, Y=y\\right) \\leq \\alpha.\n$$\nBy Bayes’ rule and the linear-Gaussian measurement model $Y = X + W$ with $W \\sim \\mathcal{N}(0, \\tau^{2})$ independent of $X \\sim \\mathcal{N}(\\mu, \\sigma^{2})$, the posterior distribution $X \\mid Y=y$ is Gaussian. The canonical derivation uses the product of exponentials in the Gaussian densities $p(y \\mid x)$ and $p(x)$, completing the square in $x$ to identify the posterior mean and variance. Specifically, the posterior $X \\mid Y=y$ is\n$$\nX \\mid Y=y \\sim \\mathcal{N}\\!\\left(m(y), v\\right),\n$$\nwhere\n$$\nm(y) = \\mu + K\\,(y - \\mu), \\quad K = \\frac{\\sigma^{2}}{\\sigma^{2} + \\tau^{2}}, \\quad v = \\frac{\\sigma^{2}\\,\\tau^{2}}{\\sigma^{2} + \\tau^{2}}.\n$$\nThe conditional hazard probability is thus\n$$\n\\mathbb{P}\\!\\left(X \\leq x_{h} \\,\\middle|\\, Y=y\\right) = \\Phi\\!\\left(\\frac{x_{h} - m(y)}{\\sqrt{v}}\\right),\n$$\nwhere $\\Phi(\\cdot)$ is the standard normal cumulative distribution function. The chance constraint $\\mathbb{P}(X \\leq x_{h} \\mid Y=y) \\leq \\alpha$ is equivalent to\n$$\n\\Phi\\!\\left(\\frac{x_{h} - m(y)}{\\sqrt{v}}\\right) \\leq \\alpha,\n$$\nand since $\\Phi$ is strictly increasing, this inequality is equivalent to\n$$\n\\frac{x_{h} - m(y)}{\\sqrt{v}} \\leq z_{\\alpha},\n$$\nwhere $z_{\\alpha} = \\Phi^{-1}(\\alpha)$ is the $\\alpha$-quantile of the standard normal distribution. Rearranging yields\n$$\nm(y) \\geq x_{h} - z_{\\alpha}\\,\\sqrt{v}.\n$$\nBecause $m(y)$ is an affine, strictly increasing function of $y$ with slope $K \\in (0,1)$, the set of $y$ satisfying the chance constraint is an interval of the form $[t, \\infty)$, where $t$ is the smallest $y$ for which the inequality holds with equality. Enforcing equality at the threshold $y = t$,\n$$\nm(t) = \\mu + K\\,(t - \\mu) = x_{h} - z_{\\alpha}\\,\\sqrt{v}.\n$$\nSolving for $t$,\n$$\nK\\,(t - \\mu) = x_{h} - z_{\\alpha}\\,\\sqrt{v} - \\mu\n\\quad \\Longrightarrow \\quad\nt = \\mu + \\frac{1}{K}\\,\\bigl(x_{h} - z_{\\alpha}\\,\\sqrt{v} - \\mu\\bigr).\n$$\nSubstituting $K = \\sigma^{2}/(\\sigma^{2} + \\tau^{2})$ and $\\sqrt{v} = \\sigma\\,\\tau / \\sqrt{\\sigma^{2} + \\tau^{2}}$ gives\n$$\nt^{\\star} = \\mu + \\frac{\\sigma^{2} + \\tau^{2}}{\\sigma^{2}}\\,\\left(x_{h} - \\mu - z_{\\alpha}\\,\\frac{\\sigma\\,\\tau}{\\sqrt{\\sigma^{2} + \\tau^{2}}}\\right).\n$$\nThis $t^{\\star}$ is the minimal threshold that ensures the conditional hazard probability is at most $\\alpha$ for all $y \\geq t^{\\star}$. Since engaging the energy-saving mode strictly reduces loss ($c_{1}  c_{0}$), the optimal policy that minimizes expected loss subject to the chance constraint is to engage the mode whenever $y \\geq t^{\\star}$ and refrain otherwise. Therefore, the optimal threshold $t^{\\star}$ is given by the closed-form expression above.",
            "answer": "$$\\boxed{\\mu + \\frac{\\sigma^{2} + \\tau^{2}}{\\sigma^{2}}\\left(x_{h} - \\mu - \\Phi^{-1}(\\alpha)\\,\\frac{\\sigma\\,\\tau}{\\sqrt{\\sigma^{2} + \\tau^{2}}}\\right)}$$"
        }
    ]
}