## Applications and Interdisciplinary Connections

The preceding chapters have established the core principles and mechanisms of threat modeling for cyber-physical systems. We have defined the unique characteristics of CPS—the tight coupling of computational and physical processes—and outlined foundational methodologies for identifying and analyzing threats. This chapter transitions from principle to practice, exploring how these foundational concepts are applied, extended, and integrated across a diverse spectrum of real-world systems and interdisciplinary contexts.

Our objective is not to reiterate the fundamentals, but to demonstrate their utility in addressing concrete engineering challenges. We will traverse the CPS stack, from low-level communication protocols to high-level control strategies, to see how threat models inform robust design. We will then examine how structured methodologies like STRIDE and ATT&CK® provide systematic frameworks for threat analysis in complex systems. Finally, we will broaden our perspective to address critical cross-cutting challenges, including [supply chain security](@entry_id:1132659), the human element in system operation, and the vital intersection of cybersecurity with functional safety and regulatory compliance. Through these applications, the abstract principles of [threat modeling](@entry_id:924842) become tangible tools for building more secure, resilient, and trustworthy cyber-physical systems.

### Threat Modeling Across the CPS Stack

A cyber-physical system can be conceptualized as a hierarchical stack, with physical [sensors and actuators](@entry_id:273712) at the bottom, and layers of communication, estimation, control, and human supervision built on top. A comprehensive threat model must consider vulnerabilities at each layer, as a compromise at any point can propagate and manifest as a hazardous physical event.

#### The Communication and Network Layer

The network is the nervous system of a CPS, transmitting sensor readings, control commands, and state information. Its compromise can lead to a catastrophic loss of control or situational awareness. Threat modeling at this layer focuses on the security properties of the communication protocols themselves.

Many operational technology (OT) and embedded systems rely on legacy protocols that were designed decades ago with an emphasis on performance and reliability, but with no consideration for security in an adversarial environment. For instance, protocols like Modbus/TCP, the de facto standard in many industrial environments, and the Controller Area Network (CAN) bus, ubiquitous in automotive systems, lack fundamental security properties. By design, they offer no native mechanisms for authentication, ensuring that a message came from a trusted source; confidentiality, protecting the content of a message from being read; or cryptographic integrity, ensuring a message has not been altered in transit. This exposes a vast attack surface. An adversary with network access can trivially send malicious commands, manipulate sensor readings, or replay valid messages, with the receiving device having no protocol-level means to detect the deception .

In contrast, modern protocols like OPC Unified Architecture (OPC UA) were designed with security as a core component, incorporating robust application-layer authentication via certificates, encryption for confidentiality, and [digital signatures](@entry_id:269311) for integrity. However, this does not eliminate risk. Security in these systems is often configurable, and a common vulnerability is the misconfiguration to a "None" security policy, which effectively disables all protections. Furthermore, architectural weaknesses, such as a gateway that bridges a secure OPC UA network to an insecure legacy fieldbus, can create pathways for threats to move laterally across the system .

Modeling threats against real-time networks like CAN requires an even deeper, more context-specific analysis. An attacker seeking to inject malicious messages must contend not only with the protocol's rules but also with the system's physical and [logical constraints](@entry_id:635151). Consider an attacker attempting to spoof wheel-speed sensor messages on a vehicle's CAN bus to degrade the performance of an anti-lock braking system. A successful attack is not as simple as injecting a frame with the correct identifier. First, due to the [bus arbitration](@entry_id:173168) mechanism where lower ID values have priority, and the "first-valid-sample" logic often used in controllers, the attacker must ensure their spoofed frame is transmitted and arrives *before* the legitimate frame. This often requires precise timing, initiating transmission microseconds before the legitimate node is scheduled to. Second, the payload of the spoofed message must be crafted to be plausible to the receiving controller. Modern ECUs often run sophisticated runtime checks, such as ensuring the reported values are within a physically possible range, that the rate of change does not exceed physical limits, and that the values correlate with predictions from a digital-twin-like model that uses other sensor inputs (e.g., engine speed). A successful attacker must therefore craft a payload that not only achieves a malicious objective (e.g., biasing the speed reading) but does so subtly enough to remain within the bounds of all these plausibility checks .

#### The Control and Estimation Layer

Moving up the stack, the control and estimation layer is where raw sensor data is fused into a [coherent state](@entry_id:154869) estimate and where control decisions are computed. Attacks at this layer are particularly insidious because they corrupt the "brains" of the CPS. Threat modeling here often leverages the [formal language](@entry_id:153638) of control theory.

A canonical threat is the False Data Injection (FDI) attack. In this scenario, an adversary subtly manipulates sensor readings to corrupt the state estimate computed by the system's observer (e.g., a Kalman filter), while evading detection by residual-based anomaly detectors. The stealthiness of such an attack can be formally modeled. Given a detector that triggers an alarm if a quadratic form of the residual, $r_k^\top W r_k$, exceeds a threshold $\gamma$, an attacker can inject an adversarial signal $a_k$ into the measurements. For the attack to remain stealthy, the magnitude of the injection, as measured by a norm weighted by the detector matrix $W$, must be constrained. Specifically, the "size" of the attack vector must be small enough that when added to the worst-case nominal residual, it still falls within the acceptance region. This leads to a precise mathematical condition, $\sqrt{a_k^\top W a_k} + \sqrt{\gamma_0} \le \sqrt{\gamma}$, where $\gamma_0$ defines the nominal residual bound. This formulation allows a security analyst to quantify the "stealthy attack space" as a function of the system's observer and detector design .

As with the network layer, a successful attack on the control loop often depends on satisfying both cyber and physical constraints simultaneously. Consider a man-in-the-middle attacker on a fieldbus intercepting sensor packets and forwarding forged versions to the controller. The attack's feasibility is a function of the real-time deadline, network delay and jitter, and the attacker's own computational overhead. The total time taken to intercept, forge, and forward the malicious packet must be less than the slack time remaining before the controller's deadline expires. Concurrently, the forged data must be plausible enough to pass the controller's residual-based checks. This demonstrates that in CPS, [threat modeling](@entry_id:924842) cannot treat the cyber and physical domains in isolation; feasibility is determined by their combined constraints .

While much of threat modeling focuses on reactive detection and response, a powerful mitigation strategy is to design control systems that are proactively resilient to disturbances. Robust control theory, particularly $H_\infty$ optimization, provides a framework for this. The goal is to design a controller that minimizes the worst-case amplification of external disturbances (which can include adversarial signals) to a critical performance output. For a given linear system, this can be formulated as an optimization problem to find a feedback gain $K$ that both stabilizes the system and minimizes the $H_\infty$ norm of the transfer function from the disturbance input to the performance output. The solution to this problem yields a controller that is, by design, less susceptible to a bounded class of adversarial inputs, representing a shift from "bolted-on" security to inherent [system resilience](@entry_id:1132834) .

#### The Sensing and Actuation Layer

At the lowest level of the stack, threats can target the physical sensors and actuators themselves. Threat identification at this layer often relies on analyzing operational data for anomalies that point to a specific adversarial model. In a swarm of UAVs, for example, distinct attack types leave distinct signatures in the [telemetry](@entry_id:199548) monitored by a digital twin.

-   **Jamming**, an attack on availability, manifests as a severe degradation of [wireless communication](@entry_id:274819) metrics. A broadband noise jammer will cause the signal-to-noise ratio to plummet, the packet delivery rate to collapse, and the channel to appear constantly busy, all while the content of successfully received (but likely corrupted) messages shows no sign of tampering .
-   **GNSS Spoofing**, an attack on the integrity of the sensing layer, presents a different signature. Communication metrics remain normal, and messages pass cryptographic authentication. The anomaly appears as a discrepancy between the spoofed sensor (the GNSS receiver reporting a false time or position) and other, independent sensors (like an IMU). This creates an inconsistency that can be detected by a multi-sensor fusion algorithm or a high-fidelity digital twin .
-   A sophisticated **False Data Injection** attack presents the most subtle signature. Here, an adversary who has already compromised a UAV's software injects a carefully crafted bias into the state data. Communication and sensor readings may appear normal to external observers. The only evidence might be a persistent, non-zero mean in the Kalman filter's residual sequence, indicating a [systematic mismatch](@entry_id:274633) between the model's prediction and the (corrupted) reality. This highlights the need for sophisticated, model-based anomaly detection within the CPS itself .

### Structured Frameworks and Methodologies in Practice

While analyzing threats at individual layers is crucial, the complexity of modern CPS demands systematic, structured methodologies that provide a holistic view of the system. Several frameworks have been developed to guide threat modeling efforts, ensuring completeness and aligning them with engineering and risk management processes.

#### Applying STRIDE to System Design

STRIDE, a mnemonic for Spoofing, Tampering, Repudiation, Information disclosure, Denial of service, and Elevation of privilege, is a widely used methodology for systematically identifying threats. It is often applied to a Data Flow Diagram (DFD) of the system, where analysts examine each data flow, process, and data store for susceptibility to these six threat categories, especially where data crosses a trust boundary.

In the context of an autonomous vehicle's sensor fusion pipeline, applying STRIDE forces a rigorous consideration of threats throughout the system. For instance, a threat modeler would assess the risk of sensor data being **spoofed** or **tampered with** on the in-vehicle network, **information disclosure** of telemetry sent to a cloud-based digital twin, **denial of service** against the critical fusion module, and **elevation of privilege** by a compromised low-level driver. A key insight from applying such frameworks to CPS is that proposed mitigations must be evaluated not only for their security effectiveness but also for their impact on system performance. For example, a proposal to encrypt and authenticate all high-rate sensor data must be quantitatively checked against the system's latency budget to ensure that the cryptographic processing overhead does not cause deadline misses in the real-time control loop .

#### Leveraging Threat Intelligence with MITRE ATT&CK® for ICS

While STRIDE helps in identifying broad categories of threats, the MITRE ATT&CK® for Industrial Control Systems (ICS) framework provides a knowledge base and common lexicon for describing specific adversary tactics and techniques observed in real-world attacks. It allows organizations to move from generic threats to concrete adversary behaviors, such as "Modify Controller Firmware" (T0832) or "Man-in-the-Middle" (T0829).

Applying ATT&CK for ICS to a specific CPS enables a more targeted and realistic threat assessment. For a system using a Programmable Logic Controller (PLC) on a hard real-time network like EtherCAT, one can evaluate the feasibility of specific ATT&CK techniques against the system's operational constraints. For instance, the technique "Modify Controller Firmware" for the purpose of "Persistence" might be deemed infeasible during operation because the required downtime for the update process (which often requires stopping the PLC) would far exceed the PLC's watchdog timer deadline, causing an immediate system fault. Similarly, a "Man-in-the-Middle" attack might be infeasible if the latency introduced by the attacker's interception device violates the microsecond-level timing budget of the real-time network protocol. This analysis grounds the abstract threat model in the physical and temporal realities of the specific CPS .

#### Risk-Centric Threat Modeling with PASTA

The Process for Attack Simulation and Threat Analysis (PASTA) is a seven-stage methodology that tightly aligns threat modeling with business objectives and [risk management](@entry_id:141282). It advocates a top-down approach, starting with the identification of business objectives (Stage 1) and translating them into technical security requirements. This risk-centric philosophy is particularly well-suited for CPS, where security efforts must be prioritized based on their ability to prevent unacceptable physical consequences.

Digital twins can be powerfully integrated into a PASTA-based workflow. For a power substation, the business objectives (e.g., maintaining voltage stability, ensuring operator safety) can be formalized as Key Performance Indicators (KPIs) and test oracles within the digital twin (Stage 1). After defining scope (Stage 2) and decomposing the system to identify the attack surface (Stage 3), threat intelligence is used to enumerate adversary goals (Stage 4) and identify system weaknesses (Stage 5). The digital twin then becomes the simulation platform for executing modeled attacks (Stage 6). By running executable adversarial scenarios on the twin, engineers can quantitatively measure the impact ($I_i$) of an attack on the business KPIs. This impact data, when combined with external estimates of likelihood ($p_i$), allows for a formal calculation of risk ($R = \sum p_i \cdot I_i$), which informs the prioritization of mitigations (Stage 7). This creates a direct, traceable line from threat simulation to risk-based business decisions .

### Cross-Cutting and Interdisciplinary Challenges

Effective threat modeling for CPS extends beyond the technical stack and structured methodologies. It requires addressing broader challenges that cut across disciplines, including the integrity of the supply chain, the role of human operators, and the complex interplay between security, safety, and regulation.

#### Securing the Supply Chain

The modern CPS is not a monolithic creation but an assembly of hardware and software components from numerous third-party suppliers. This creates a significant attack surface before the system is even deployed. A threat model must therefore encompass the entire supply chain.

A foundational step is recognizing the limitations of standard IT-centric vulnerability management tools. Metrics like the Common Vulnerability Scoring System (CVSS) score measure the intrinsic technical severity of a vulnerability in a component, but they do not and cannot capture the physical impact that exploiting that vulnerability might have within a specific CPS context. A high-CVSS vulnerability in a non-critical component might pose little physical risk, while a medium-CVSS flaw in a critical controller could be catastrophic. Similarly, the Exploit Prediction Scoring System (EPSS), which predicts the likelihood of a vulnerability being exploited "in the wild," may underestimate the risk for [targeted attacks](@entry_id:897908) against high-value, low-prevalence CPS assets. A principled workflow requires augmenting these tools with a physics-based consequence model—often derived from a digital twin—to understand the true, context-specific risk .

A comprehensive supply chain threat model considers multiple vectors, including malicious backdoors inserted into a compiler (the "Trusting Trust" problem), tampering with software binaries in a repository, and the introduction of malicious hardware implants during manufacturing or shipping. Mitigating these threats requires a defense-in-depth strategy. A Software Bill of Materials (SBOM) provides transparency into the software components, enabling monitoring for new vulnerabilities. Reproducible builds, where bit-for-bit identical binaries can be produced from source code in independent environments, help detect compiler-level or build-process tampering. Physical inspection and testing of incoming hardware components can help detect implants. A quantitative risk model can be used to evaluate the effectiveness of different combinations of these mitigations, allowing an organization to invest its resources in the controls that provide the greatest risk reduction .

#### The Human-in-the-Loop

In many systems, a human operator is an integral part of the control loop, especially in a supervisory role. This introduces the human element as both a potential line of defense and a potential attack vector.

The Human-Machine Interface (HMI) is a critical attack surface that sits at the intersection of human cognition and system control. An adversary can target it with deception attacks, manipulating the displayed information to trick an operator into making an incorrect assessment or taking an unsafe action. The effectiveness of such an attack can be analyzed using [signal detection theory](@entry_id:924366), which models the operator's decision as a choice between a "normal" and "attack" hypothesis based on a noisy indicator. The optimal decision threshold depends not only on the statistical properties of the signal but also on the relative costs of a missed detection versus a false alarm. In a high-consequence system where a missed detection is far more costly, the optimal strategy is to lower the detection threshold, making the system more sensitive at the expense of more frequent false alarms. HMI design also directly impacts susceptibility. A consolidated, clear interface with lower information complexity (quantifiable using metrics from information theory) can reduce the probability of operator misconfiguration. Furthermore, design features that increase the separability of normal and abnormal signals—for example, by using redundancy to reduce noise—directly make deception harder to achieve .

The human element also extends beyond the control room to the broader enterprise environment. The initial access for a sophisticated attack on a CPS often comes not from a direct assault on the operational network, but from a social engineering attack on an employee. A spear-phishing email can lead to credential theft, giving an attacker a foothold in the enterprise IT network. From there, they can attempt to pivot into the operational technology (OT) domain. Modeling this attack path, often using an attack graph with probabilistic steps, is crucial. This allows for the quantitative assessment of security controls like Multi-Factor Authentication (MFA), which introduces an additional, difficult step for the attacker after credential theft, thereby significantly reducing the overall probability of a successful compromise .

#### Intersection with Safety, Regulation, and Law

In many domains—including automotive, aviation, [industrial automation](@entry_id:276005), and medical devices—CPS are subject to stringent safety regulations. A central challenge is the co-assurance of safety and security. Safety engineering has a long history of managing risk from random hardware failures, using probabilistic methods. Security engineering deals with intelligent, adaptive adversaries, where likelihood is much harder to quantify.

For a safety-critical system, a formal safety case must be produced that argues that the system's risk is acceptably low, in compliance with standards like IEC 61508 or ISO 26262. To be valid for a connected CPS, this safety case must explicitly account for security-related hazards. A rigorous approach involves creating a formal, contractual interface between the safety and security arguments. This is achieved by introducing explicit, quantified assumptions in the safety case. For example, the safety argument might partition the total risk into contributions from random failures and from malicious tampering. It then proceeds under the assumption that security controls are effective enough to ensure the probability of a successful integrity-violating attack is below a small, acceptable threshold, $\alpha$. The security case must then provide the body of evidence—from penetration tests, cryptographic audits, [formal verification](@entry_id:149180), etc.—to justify this specific assumption. This structure maintains clear logical boundaries while allowing for a comprehensive risk assessment that correctly combines both stochastic and adversarial failure modes .

This convergence of threat modeling with legal and regulatory obligations is perhaps most evident in the field of medical devices. A modern medical CPS, such as an implantable neuromodulator connected to a cloud-based digital twin that optimizes therapy, is subject to comprehensive oversight by regulatory bodies like the U.S. Food and Drug Administration (FDA). For such a device, [threat modeling](@entry_id:924842) is not just an engineering best practice; it is a legal requirement. The manufacturer must submit extensive premarket documentation detailing its secure design process, a full system threat model, a [cybersecurity](@entry_id:262820) [risk assessment](@entry_id:170894), and a Software Bill of Materials (SBOM). Postmarket, the obligations are continuous and active: the manufacturer must monitor for new vulnerabilities, participate in information sharing organizations, remediate flaws in a timely manner, and report certain device malfunctions caused by cyber events to the FDA. In this context, the threat model becomes a central lifecycle artifact, driving design, validation, and ongoing maintenance in a tightly regulated environment .

### Chapter Summary

This chapter has journeyed through the practical landscape of [threat modeling](@entry_id:924842) for cyber-physical systems. We have seen how foundational principles are applied to dissect the security of low-level protocols and to formally analyze the stealth and impact of attacks on control systems. We have explored how structured methodologies like STRIDE, ATT&CK for ICS, and PASTA bring order and rigor to the analysis of complex systems, and how tools like digital twins can be integrated to provide high-fidelity simulation and quantitative impact assessment.

Crucially, we have also demonstrated that CPS [threat modeling](@entry_id:924842) is an inherently interdisciplinary endeavor. It requires not only deep technical expertise but also an understanding of the supply chain, the cognitive and behavioral characteristics of human operators, and the intricate legal and regulatory frameworks governing safety-critical domains. By bridging these disparate fields, threat modeling provides the holistic perspective necessary to secure the complex, interconnected systems that underpin modern society.