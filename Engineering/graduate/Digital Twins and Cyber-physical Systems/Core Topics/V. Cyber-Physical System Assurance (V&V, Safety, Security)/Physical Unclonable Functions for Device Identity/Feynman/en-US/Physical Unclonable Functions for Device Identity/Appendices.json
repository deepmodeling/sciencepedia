{
    "hands_on_practices": [
        {
            "introduction": "The practical utility of a Physical Unclonable Function hinges on its reliability—the ability to consistently reproduce the same response. However, environmental fluctuations and inherent physical randomness introduce noise, causing bits to flip. This first exercise provides a foundational model to quantify this challenge by treating each bit flip as an independent probabilistic event . By deriving the reliability of an entire $n$-bit response from first principles, you will uncover the fundamental scaling problem that makes error correction an essential component of any practical PUF system.",
            "id": "4235123",
            "problem": "A Physical Unclonable Function (PUF) is deployed for device identity within a Digital Twin (DT) of a Cyber-Physical System (CPS). Upon enrollment, the device’s PUF response is recorded as a reference string of length $n$ bits. On subsequent readouts, environmental noise causes each bit to independently flip relative to the enrolled bit with probability $p$, where $0 \\leq p \\leq 1$. Let the error vector be $E \\in \\{0,1\\}^{n}$, where $E_{i} = 1$ indicates a flip in bit $i$ and $E_{i} = 0$ indicates no flip. The Hamming weight of the error vector is defined as $H = \\sum_{i=1}^{n} E_{i}$, which counts the number of flipped bits.\n\nStarting only from the following foundational bases:\n- the definition of independent Bernoulli random variables,\n- the multiplication rule for independent events,\n- and combinatorial counting of outcomes,\n\nderive the probability mass function of $H$ by explicitly computing the distribution of the Hamming weight of errors. Then, define the reliability of the $n$-bit response, denoted $\\mathcal{R}(n,p)$, as the probability that a subsequent readout exactly matches the enrolled reference (that is, $H = 0$). Compute $\\mathcal{R}(n,p)$ as a closed-form analytic expression in $n$ and $p$.\n\nYour final answer must be a single closed-form expression. No numerical rounding is required, and no physical units apply to this probability.",
            "solution": "The problem statement is scientifically grounded, self-contained, and well-posed. It presents a standard model of bit-flip errors in a digital system, which can be analyzed using fundamental principles of probability theory. All variables and conditions are clearly defined, leading to a unique and meaningful solution. The problem is therefore deemed valid.\n\nWe are tasked with two objectives: first, to derive the probability mass function (PMF) of the Hamming weight of errors, $H$, and second, to compute the reliability, $\\mathcal{R}(n,p)$, defined as the probability of zero errors. The derivation must be constructed from first principles as specified.\n\nLet us model the state of each bit in the PUF response. For each bit $i \\in \\{1, 2, \\dots, n\\}$, there is an associated error variable $E_i$. The problem states that each bit flips independently with probability $p$. We can therefore define $E_i$ as a random variable such that $E_i=1$ if bit $i$ flips, and $E_i=0$ if bit $i$ does not flip.\n\nBased on the problem description, each $E_i$ is an independent Bernoulli random variable. The probability mass function for each $E_i$ is:\n$$ P(E_i = 1) = p $$\n$$ P(E_i = 0) = 1 - p $$\n\nThe Hamming weight of the error vector, $H$, is defined as the total number of flipped bits:\n$$ H = \\sum_{i=1}^{n} E_i $$\nWe seek to find the PMF of $H$, which is the probability $P(H=k)$ for any integer $k$ such that $0 \\le k \\le n$. The event $H=k$ corresponds to observing exactly $k$ bit flips and $n-k$ non-flips among the $n$ bits.\n\nLet us consider a specific sequence of errors that results in a Hamming weight of $k$. For instance, consider the sequence where the first $k$ bits flip and the remaining $n-k$ bits do not. The error vector would be $(\\underbrace{1, 1, \\dots, 1}_{k \\text{ times}}, \\underbrace{0, 0, \\dots, 0}_{n-k \\text{ times}})$. Since the bit flips are independent events, we can use the multiplication rule for independent events to find the probability of this specific sequence occurring:\n$$ P(E_1=1, \\dots, E_k=1, E_{k+1}=0, \\dots, E_n=0) = \\left( \\prod_{i=1}^{k} P(E_i=1) \\right) \\left( \\prod_{j=k+1}^{n} P(E_j=0) \\right) $$\nSubstituting the probabilities for the Bernoulli trials, we get:\n$$ p^k (1-p)^{n-k} $$\nThe probability of any other specific sequence of $k$ flips and $n-k$ non-flips is identical, as the multiplication is commutative.\n\nNext, we must use combinatorial counting to determine the total number of distinct sequences that result in a Hamming weight of $k$. This is equivalent to counting the number of ways to choose $k$ positions for the flips to occur out of a total of $n$ available bit positions. This quantity is given by the binomial coefficient, denoted $\\binom{n}{k}$:\n$$ \\binom{n}{k} = \\frac{n!}{k!(n-k)!} $$\n\nThe total probability of observing exactly $k$ flips, $P(H=k)$, is the sum of the probabilities of all such distinct sequences. Since each sequence has the same probability $p^k(1-p)^{n-k}$, we can multiply this probability by the number of such sequences:\n$$ P(H=k) = \\binom{n}{k} p^k (1-p)^{n-k} $$\nThis is the probability mass function for a binomial distribution, describing the random variable $H \\sim \\text{Binomial}(n,p)$. This completes the first part of the derivation.\n\nThe second part of the task is to compute the reliability, $\\mathcal{R}(n,p)$, which is defined as the probability that a readout exactly matches the enrolled reference. This corresponds to the case of zero bit flips, i.e., $H=0$. We can compute this by setting $k=0$ in the PMF derived above:\n$$ \\mathcal{R}(n,p) = P(H=0) $$\n$$ \\mathcal{R}(n,p) = \\binom{n}{0} p^0 (1-p)^{n-0} $$\nWe now evaluate each term in this expression.\nThe binomial coefficient for $k=0$ is:\n$$ \\binom{n}{0} = \\frac{n!}{0!(n-0)!} = \\frac{n!}{1 \\cdot n!} = 1 $$\nwhere we use the definition $0!=1$.\nThe term $p^0$ is:\n$$ p^0 = 1 $$\n(This holds for any $p \\in [0,1]$, including the edge cases).\nThe final term is:\n$$ (1-p)^{n-0} = (1-p)^n $$\nCombining these results, we obtain the closed-form analytic expression for the reliability:\n$$ \\mathcal{R}(n,p) = 1 \\cdot 1 \\cdot (1-p)^n = (1-p)^n $$\nThis expression gives the probability that an $n$-bit response is read out without any errors, given an independent bit-flip probability of $p$.",
            "answer": "$$\\boxed{(1-p)^n}$$"
        },
        {
            "introduction": "Having established the challenge of PUF unreliability, we now turn to a practical engineering solution: Error-Correcting Codes (ECC). Raw PUF responses are too noisy for direct use, but an ECC can correct a limited number of errors, enabling the robust reconstruction of a secret key. This exercise  places you in the role of a system designer, tasked with selecting parameters for a Bose–Chaudhuri–Hocquenghem (BCH) code. You will navigate the critical trade-off between error-correction capability and the length of the final, stable key, using statistical approximations to meet a stringent reliability target.",
            "id": "4235125",
            "problem": "A Physical Unclonable Function (PUF) in a Cyber-Physical System (CPS) is used to bind a device identity to its Digital Twin. The PUF produces a raw binary response that is enrolled once and then reconstructed on-demand. Each response bit flips independently relative to the enrolled reference with Bit Error Rate (BER) $p$. To make the reconstructed key robust across operating conditions, the system uses a binary primitive narrow-sense Bose–Chaudhuri–Hocquenghem (BCH) code as an Error-Correcting Code (ECC) over the PUF response. A reconstruction attempt fails if the number of bit errors in the noisy response exceeds the ECC’s error-correction capability.\n\nConsider a design in which the raw response length is fixed to $n = 511$ bits, so that the BCH code length equals the response length, with $n = 2^{m} - 1$ and integer $m$. The reconstructed key is the BCH code’s dimension $k$ (in bits). The design must ensure that the probability of a reconstruction failure per attempt is at most the desired key failure rate $\\delta$.\n\nAssume the following:\n- The bit-flip process is independent and identically distributed with BER $p = 0.02$.\n- The target key failure rate is $\\delta = 10^{-6}$.\n- The failure event is defined by a binomial model on the number of bit errors over $n$ trials.\n- The BCH family is restricted to primitive narrow-sense binary codes with length $n = 511$.\n\nSelect the error-correction capability $t$ so that the failure probability is at most $\\delta$, and then select the corresponding BCH parameters $[n, k, t]$ consistent with this choice. Compute the maximum reconstructed key length $k$ achievable under these constraints. Express the final key length in bits. If any intermediate approximation is required, justify it from first principles and use a statistically sound approximation method. No external data sources are available. The final answer must be a single number.",
            "solution": "The problem is well-defined and requires the application of standard engineering principles from statistics and coding theory. All provided data is necessary and consistent.\n\nThe first step is to determine the minimum error-correction capability, denoted by $t$, required to meet the specified key failure rate. The number of bit errors, $X$, in a raw response of length $n$ follows a binomial distribution, $X \\sim \\mathcal{B}(n, p)$, with parameters $n=511$ and $p=0.02$.\n\nA reconstruction failure occurs if the number of errors $X$ exceeds the code's capability $t$. The probability of this event must be at most $\\delta = 10^{-6}$. The mathematical constraint is:\n$$P(X > t) \\le \\delta$$\n\nThe exact calculation of this binomial tail probability is computationally intensive. We can use an approximation. First, we compute the mean $\\mu$ and variance $\\sigma^2$ of the binomial distribution:\n$$\\mu = np = 511 \\times 0.02 = 10.22$$\n$$\\sigma^2 = np(1-p) = 511 \\times 0.02 \\times (1 - 0.02) = 10.22 \\times 0.98 = 10.0156$$\nThe standard deviation is $\\sigma = \\sqrt{10.0156} \\approx 3.1647$.\n\nSince $n=511$ is large, and both $np = 10.22 > 5$ and $n(1-p) = 500.78 > 5$ are satisfied, the binomial distribution can be accurately approximated by a normal distribution $\\mathcal{N}(\\mu, \\sigma^2)$ according to the De Moivre-Laplace theorem.\n\nTo improve the accuracy of the approximation for a discrete distribution with a continuous one, we apply a continuity correction. The event $X > t$ for the discrete variable $X$ corresponds to the event $Y > t + 0.5$ for the continuous approximating variable $Y \\sim \\mathcal{N}(\\mu, \\sigma^2)$. The constraint becomes:\n$$P(Y > t + 0.5) \\le \\delta$$\n\nWe standardize this by converting to the standard normal variable $Z = \\frac{Y - \\mu}{\\sigma} \\sim \\mathcal{N}(0, 1)$:\n$$P\\left(Z > \\frac{(t + 0.5) - \\mu}{\\sigma}\\right) \\le \\delta$$\nLet $\\Phi(z)$ be the cumulative distribution function (CDF) of the standard normal distribution. The inequality can be written as:\n$$1 - \\Phi\\left(\\frac{t + 0.5 - \\mu}{\\sigma}\\right) \\le \\delta$$\n$$\\Phi\\left(\\frac{t + 0.5 - \\mu}{\\sigma}\\right) \\ge 1 - \\delta$$\nSubstituting $\\delta = 10^{-6}$, we need to find the quantile $z_{\\delta}$ such that $\\Phi(z_{\\delta}) \\ge 1 - 10^{-6} = 0.999999$. Without access to statistical tables, we must rely on known values from probability theory. The one-tailed probability of $10^{-6}$ corresponds to a z-score of approximately $z_{\\delta} \\approx 4.753$.\n\nNow we can solve for $t$:\n$$\\frac{t + 0.5 - \\mu}{\\sigma} \\ge z_{\\delta}$$\n$$t \\ge \\mu - 0.5 + z_{\\delta}\\sigma$$\nSubstituting the numerical values:\n$$t \\ge 10.22 - 0.5 + (4.753)(3.1647)$$\n$$t \\ge 9.72 + 15.0423$$\n$$t \\ge 24.7623$$\nSince $t$ must be an integer representing the number of correctable errors, we must select the smallest integer satisfying this condition, which is $t=25$.\n\nThe next step is to determine the maximum key length $k$ for a primitive narrow-sense binary BCH code with parameters $[n, k, t]$. The code length is given as $n=511$. For BCH codes, the length is of the form $n = 2^m - 1$ for some integer $m$.\n$$511 = 2^m - 1 \\implies 512 = 2^m \\implies m=9$$\nThe dimension $k$ of a BCH code designed to correct $t$ errors is related to $n$ and $m$ by the BCH bound on the number of parity-check bits, $n-k$. The number of parity bits required is at most $mt$. To maximize the information length $k$, we use the standard construction where this bound is an equality:\n$$k = n - mt$$\nSubstituting the determined values $n=511$, $m=9$, and $t=25$:\n$$k = 511 - 9 \\times 25$$\n$$k = 511 - 225$$\n$$k = 286$$\nThus, the maximum reconstructed key length achievable under the given constraints is $286$ bits. This corresponds to the BCH code $[511, 286, 25]$.",
            "answer": "$$\\boxed{286}$$"
        },
        {
            "introduction": "A reliable PUF response is necessary but not sufficient for a secure system; it must be integrated into a robust authentication protocol. The verifier's core task is to distinguish a genuine, noisy response from an impostor's attempt. This is typically achieved by accepting responses within a certain Hamming distance threshold, $\\tau$, of the enrolled value. This final exercise  delves into the critical task of setting this threshold to maximize usability for legitimate users while enforcing a strict security guarantee—a maximum False Accept Rate (FAR)—against a capable adversary. Your derivation will reveal how statistical analysis and a clear threat model directly inform the configuration of a secure system.",
            "id": "4235135",
            "problem": "In a cyber-physical system secured by a Physical Unclonable Function (PUF) identity, an $n$-bit response $\\mathbf{r} \\in \\{0,1\\}^{n}$ is enrolled at a nominal temperature $T_{0}$. At authentication time under a potentially manipulated ambient temperature $T$, the observed response $\\mathbf{r}'$ is produced by the same PUF. The bit error rate (BER) with respect to the enrolled response is modeled as independent per-bit flips with probability $p(T)$, where $p(T)$ is a known function of $T$ governed by a calibrated digital-twin model (e.g., dominated by thermally induced noise), with $0 \\le p(T) < \\tfrac{1}{2}$. The verifier accepts if and only if the Hamming distance $D_{H}(\\mathbf{r}',\\mathbf{r})$ does not exceed a threshold $\\tau(T)$, which the verifier may choose as a function of $T$.\n\nAn adversary manipulates $T$ to increase $p(T)$ in an attempt to induce the verifier to raise $\\tau(T)$. However, to preserve security, the verifier must enforce a fixed false accept rate (FAR) at or below a preset $\\alpha \\in (0,1)$ against an attacker who, via model-building, produces an $n$-bit guess $\\hat{\\mathbf{r}}$ whose bits are independent and match the enrolled $\\mathbf{r}$ with probability $q \\in [\\tfrac{1}{2},1)$ per bit. Thus, under an impostor attempt, the Hamming distance to $\\mathbf{r}$ is distributed as the number of mismatches between independent Bernoulli trials with per-bit mismatch probability $1-q$.\n\nAssume independence of bit errors across positions and across attempts, and that $n$ is large enough for a central limit approximation to be accurate. From first principles of binomial statistics and the central limit theorem, derive an asymptotic closed-form expression for the authentication threshold function $\\tau^{\\star}(T)$ that maximizes acceptance of genuine devices (equivalently, minimizes the false reject rate) subject to the FAR constraint $\\alpha$ against the model-building attacker described above. Express your final result in terms of $n$, $q$, and $\\alpha$ using the standard normal quantile function $\\Phi^{-1}(\\cdot)$, and write it as a real-valued function of $T$. Your final answer must be a single closed-form analytic expression. Do not round. Do not include units. Angles are not involved.",
            "solution": "The user wants to derive the optimal authentication threshold $\\tau^{\\star}(T)$ for a Physical Unclonable Function (PUF) based identity system. The goal is to maximize the probability of accepting a genuine device, subject to a constraint on the False Accept Rate (FAR).\n\nFirst, let us formalize the problem. We have two scenarios to consider: authentication of a genuine device and an attack by an impostor.\n\n1.  **Genuine Device Authentication**:\n    A genuine device's response $\\mathbf{r}'$ at temperature $T$ is compared to its enrolled response $\\mathbf{r}$. The number of differing bits, which is the Hamming distance $D_{H}(\\mathbf{r}', \\mathbf{r})$, follows a binomial distribution. Let's denote this distance as $D_{gen}$. The number of trials is the number of bits, $n$. The probability of a single bit flipping due to temperature is $p(T)$. Thus, the number of flipped bits is a random variable:\n    $$D_{gen} \\sim B(n, p(T))$$\n    The device is accepted if $D_{gen} \\le \\tau(T)$. The probability of acceptance for a genuine device, also known as the True Accept Rate (TAR), is $P(D_{gen} \\le \\tau(T))$. The False Reject Rate (FRR) is $P(D_{gen} > \\tau(T))$. Maximizing the TAR is equivalent to minimizing the FRR.\n\n2.  **Impostor Attack**:\n    An attacker presents a guess $\\hat{\\mathbf{r}}$. The problem states that the bits of $\\hat{\\mathbf{r}}$ independently match the corresponding bits of the true enrolled response $\\mathbf{r}$ with probability $q$. Therefore, the probability of a mismatch for any given bit is $1-q$. The Hamming distance between the attacker's guess and the enrolled response, $D_{H}(\\hat{\\mathbf{r}}, \\mathbf{r})$, represents the number of mismatched bits. Let's denote this distance as $D_{imp}$. This also follows a binomial distribution with $n$ trials and a success (mismatch) probability of $1-q$:\n    $$D_{imp} \\sim B(n, 1-q)$$\n    The system makes a false accept if an impostor is authenticated, which occurs if $D_{imp} \\le \\tau(T)$. The False Accept Rate (FAR) is the probability of this event, and it is constrained to be at most $\\alpha$:\n    $$\\text{FAR} = P(D_{imp} \\le \\tau(T)) \\le \\alpha$$\n\n**Optimization Problem**:\nOur goal is to choose the threshold function $\\tau(T)$ to maximize the genuine acceptance probability, $P(D_{gen} \\le \\tau(T))$, subject to the constraint $\\text{FAR} \\le \\alpha$.\n\nFor any fixed temperature $T$, the probability $P(D_{gen} \\le \\tau)$ is a monotonically non-decreasing function of the threshold $\\tau$. To maximize this probability, we must select the largest possible value for $\\tau(T)$ that still satisfies the FAR constraint. This means we should set the threshold such that the FAR is at its maximum allowed value, $\\alpha$. Let us denote this optimal threshold as $\\tau^{\\star}$. It must satisfy:\n$$P(D_{imp} \\le \\tau^{\\star}) = \\alpha$$\nNotice that the distribution of $D_{imp}$ depends only on $n$ and $q$, and is independent of the temperature $T$. Consequently, the optimal threshold $\\tau^{\\star}$ that solves this equation will also be independent of $T$. We are looking for a value $\\tau^{\\star}$ that we can express as a constant function $\\tau^{\\star}(T) = \\tau^{\\star}$.\n\nTo find $\\tau^{\\star}$, we use the central limit theorem (CLT) to approximate the binomial distribution of $D_{imp}$ with a normal distribution, as specified in the problem for large $n$.\nThe mean ($\\mu_{imp}$) and variance ($\\sigma_{imp}^2$) of the binomial distribution $B(n, 1-q)$ are:\n$$\\mu_{imp} = E[D_{imp}] = n(1-q)$$\n$$\\sigma_{imp}^2 = \\text{Var}[D_{imp}] = n(1-q)(1-(1-q)) = nq(1-q)$$\nThe standard deviation is:\n$$\\sigma_{imp} = \\sqrt{nq(1-q)}$$\nSo, the distribution of $D_{imp}$ is approximated by a normal distribution $\\mathcal{N}(\\mu_{imp}, \\sigma_{imp}^2)$.\n\nNow, we solve the equation $P(D_{imp} \\le \\tau^{\\star}) = \\alpha$ using this normal approximation. We start by standardizing the random variable $D_{imp}$:\n$$P\\left(\\frac{D_{imp} - \\mu_{imp}}{\\sigma_{imp}} \\le \\frac{\\tau^{\\star} - \\mu_{imp}}{\\sigma_{imp}}\\right) = \\alpha$$\nLet $Z = \\frac{D_{imp} - \\mu_{imp}}{\\sigma_{imp}}$ be the standard normal random variable, $Z \\sim \\mathcal{N}(0, 1)$. The equation becomes:\n$$P\\left(Z \\le \\frac{\\tau^{\\star} - \\mu_{imp}}{\\sigma_{imp}}\\right) = \\alpha$$\nBy definition, the cumulative distribution function (CDF) of the standard normal distribution, denoted $\\Phi(z)$, gives $P(Z \\le z)$. So, we have:\n$$\\Phi\\left(\\frac{\\tau^{\\star} - \\mu_{imp}}{\\sigma_{imp}}\\right) = \\alpha$$\nTo solve for $\\tau^{\\star}$, we apply the inverse of the standard normal CDF, which is the quantile function $\\Phi^{-1}(\\cdot)$:\n$$\\frac{\\tau^{\\star} - \\mu_{imp}}{\\sigma_{imp}} = \\Phi^{-1}(\\alpha)$$\nNow, we can isolate $\\tau^{\\star}$:\n$$\\tau^{\\star} = \\mu_{imp} + \\sigma_{imp} \\Phi^{-1}(\\alpha)$$\nFinally, we substitute the expressions for $\\mu_{imp}$ and $\\sigma_{imp}$:\n$$\\tau^{\\star} = n(1-q) + \\sqrt{nq(1-q)}\\Phi^{-1}(\\alpha)$$\nThis expression for $\\tau^{\\star}$ is independent of temperature $T$. Therefore, the optimal threshold function $\\tau^{\\star}(T)$ is a constant function equal to this value. This is the desired asymptotic closed-form expression for the authentication threshold that maximizes the genuine acceptance rate for any operating temperature $T$, while strictly enforcing the specified FAR constraint against a model-building attacker. The problem specifies $\\alpha \\in (0,1)$ and $q \\in [\\frac{1}{2}, 1)$. For typical security applications, $\\alpha$ is a small value (e.g., $10^{-6}$), which means $\\Phi^{-1}(\\alpha)$ will be a negative number, correctly setting the threshold $\\tau^{\\star}$ below the mean number of errors expected from an impostor, $\\mu_{imp}$.",
            "answer": "$$\\boxed{n(1-q) + \\sqrt{nq(1-q)} \\Phi^{-1}(\\alpha)}$$"
        }
    ]
}