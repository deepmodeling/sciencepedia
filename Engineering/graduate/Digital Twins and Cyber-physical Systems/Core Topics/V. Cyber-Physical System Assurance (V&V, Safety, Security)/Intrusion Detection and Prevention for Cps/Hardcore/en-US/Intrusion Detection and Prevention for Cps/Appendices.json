{
    "hands_on_practices": [
        {
            "introduction": "At the heart of many intrusion detection systems for Cyber-Physical Systems lies a statistical test that decides whether a deviation from normal behavior is significant. This practice lays the foundation by tasking you with designing a basic anomaly detector using the powerful Neyman-Pearson framework. By deriving a detection threshold $\\tau$ that guarantees a specific false alarm probability $\\alpha$, you will gain a concrete understanding of how to manage the fundamental trade-off between a detector's sensitivity and the operational cost of false alarms.",
            "id": "4228432",
            "problem": "A Cyber-Physical System (CPS) employs a Digital Twin to predict sensor outputs. Let the residual at discrete time index $k$ be defined as $r_{k} = y_{k} - \\hat{y}_{k}$, where $y_{k}$ is the measured output and $\\hat{y}_{k}$ is the Digital Twin prediction. An Intrusion Detection System (IDS) monitors $|r_{k}|$ to detect actuation or sensor spoofing attacks that manifest as a mean shift under the attack hypothesis. Under the no-attack hypothesis $H_{0}$, the residual $r_{k}$ is modeled by a zero-mean Gaussian distribution with variance $\\sigma^{2}$ due to sensor noise and model mismatch, i.e., $r_{k} \\sim \\mathcal{N}(0,\\sigma^{2})$. Under the attack hypothesis $H_{1}$, the residual distribution is shifted in mean, but the variance remains approximately $\\sigma^{2}$. Using the Neyman-Pearson framework, design a two-sided scalar threshold $\\tau$ for the test that rejects $H_{0}$ when $|r_{k}|  \\tau$, such that the false alarm probability satisfies $\\mathbb{P}(|r_{k}|  \\tau \\mid H_{0}) = \\alpha$ for a given design parameter $\\alpha \\in (0,1)$. Derive $\\tau$ explicitly in closed form in terms of $\\alpha$ and $\\sigma$, starting from fundamental definitions of Gaussian tail probabilities and the Neyman-Pearson criterion. Express your final answer symbolically; no numerical approximation is required.",
            "solution": "The problem statement is first validated to ensure it is scientifically sound, well-posed, and objective.\n\n### Step 1: Extract Givens\n- The residual at discrete time index $k$ is defined as $r_{k} = y_{k} - \\hat{y}_{k}$.\n- $y_{k}$ is the measured output from the Cyber-Physical System (CPS).\n- $\\hat{y}_{k}$ is the predicted output from the Digital Twin.\n- The null hypothesis, $H_{0}$, corresponds to the no-attack scenario.\n- Under $H_{0}$, the residual $r_{k}$ follows a zero-mean Gaussian distribution with variance $\\sigma^{2}$: $r_{k} \\sim \\mathcal{N}(0,\\sigma^{2})$.\n- The alternative hypothesis, $H_{1}$, corresponds to an attack scenario where the mean of the residual is shifted.\n- The Intrusion Detection System (IDS) employs a decision rule: reject $H_{0}$ if the absolute value of the residual exceeds a threshold $\\tau$, i.e., if $|r_{k}|  \\tau$.\n- The false alarm probability is constrained to be $\\mathbb{P}(|r_{k}|  \\tau \\mid H_{0}) = \\alpha$.\n- The parameter $\\alpha$ is a given design parameter such that $\\alpha \\in (0,1)$.\n- The objective is to derive a closed-form expression for the threshold $\\tau$ in terms of $\\alpha$ and $\\sigma$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is subjected to a critical review based on the extracted information.\n\n- **Scientific Groundedness**: The problem is well-grounded in statistical detection theory, a core discipline in signal processing and control engineering. The modeling of noise and model mismatch as a zero-mean Gaussian process is a standard and widely accepted practice. The use of the Neyman-Pearson framework to design a detector with a fixed false alarm rate is a fundamental concept in hypothesis testing. The problem is scientifically and mathematically sound.\n- **Well-Posedness**: The problem is well-posed. It provides sufficient information to determine a unique, stable, and meaningful solution for the threshold $\\tau$. The relationship between the false alarm probability $\\alpha$ and the threshold $\\tau$ for a Gaussian distribution is monotonic, ensuring a unique solution exists for any valid $\\alpha$.\n- **Objectivity**: The problem is stated using precise, unambiguous, and objective technical language. There are no subjective or opinion-based claims.\n\n### Step 3: Verdict and Action\nThe problem is deemed valid as it is scientifically sound, well-posed, and objective. No flaws listed in the validation criteria are present. Therefore, a full solution will be derived.\n\n### Solution Derivation\nThe goal is to determine the threshold $\\tau$ that satisfies the specified false alarm probability. The false alarm probability is the probability of rejecting the null hypothesis $H_{0}$ when it is actually true.\n\nThe decision rule is to reject $H_{0}$ if $|r_{k}|  \\tau$. The false alarm probability, $\\alpha$, is thus given by the condition:\n$$\n\\mathbb{P}(|r_{k}|  \\tau \\mid H_{0}) = \\alpha\n$$\nUnder the null hypothesis $H_{0}$, the residual $r_{k}$ is distributed as $r_{k} \\sim \\mathcal{N}(0, \\sigma^{2})$.\n\nThe event $|r_{k}|  \\tau$ is equivalent to the union of two disjoint events: $r_{k}  \\tau$ or $r_{k}  -\\tau$. Therefore, the probability can be written as:\n$$\n\\mathbb{P}(|r_{k}|  \\tau \\mid H_{0}) = \\mathbb{P}(r_{k}  \\tau \\mid H_{0}) + \\mathbb{P}(r_{k}  -\\tau \\mid H_{0})\n$$\nThe Gaussian distribution $\\mathcal{N}(0, \\sigma^{2})$ is symmetric about its mean of $0$. This symmetry implies that the probability in the two tails is equal:\n$$\n\\mathbb{P}(r_{k}  \\tau \\mid H_{0}) = \\mathbb{P}(r_{k}  -\\tau \\mid H_{0})\n$$\nSubstituting this into the expression for $\\alpha$, we get:\n$$\n\\alpha = 2 \\cdot \\mathbb{P}(r_{k}  \\tau \\mid H_{0})\n$$\nFrom this, we can isolate the one-sided tail probability:\n$$\n\\mathbb{P}(r_{k}  \\tau \\mid H_{0}) = \\frac{\\alpha}{2}\n$$\nTo evaluate this probability, we standardize the random variable $r_{k}$. Let $Z$ be a standard normal random variable, $Z \\sim \\mathcal{N}(0, 1)$. We can write $r_{k} = \\sigma Z$. The inequality $r_{k}  \\tau$ is then equivalent to $\\sigma Z  \\tau$, or $Z  \\frac{\\tau}{\\sigma}$.\n\nThe probability condition becomes:\n$$\n\\mathbb{P}\\left(Z  \\frac{\\tau}{\\sigma}\\right) = \\frac{\\alpha}{2}\n$$\nLet $\\Phi(z)$ be the cumulative distribution function (CDF) of the standard normal distribution, defined as $\\Phi(z) = \\mathbb{P}(Z \\le z)$. The tail probability can be expressed in terms of the CDF:\n$$\n\\mathbb{P}(Z  z) = 1 - \\mathbb{P}(Z \\le z) = 1 - \\Phi(z)\n$$\nApplying this to our equation, with $z = \\frac{\\tau}{\\sigma}$:\n$$\n1 - \\Phi\\left(\\frac{\\tau}{\\sigma}\\right) = \\frac{\\alpha}{2}\n$$\nWe can now solve for the argument of the CDF:\n$$\n\\Phi\\left(\\frac{\\tau}{\\sigma}\\right) = 1 - \\frac{\\alpha}{2}\n$$\nTo isolate $\\frac{\\tau}{\\sigma}$, we apply the inverse of the standard normal CDF, which is the quantile function, denoted by $\\Phi^{-1}$. The quantile function $\\Phi^{-1}(p)$ returns the value $z$ such that $\\mathbb{P}(Z \\le z) = p$.\n$$\n\\frac{\\tau}{\\sigma} = \\Phi^{-1}\\left(1 - \\frac{\\alpha}{2}\\right)\n$$\nFinally, solving for the threshold $\\tau$, we obtain the explicit closed-form expression:\n$$\n\\tau = \\sigma \\Phi^{-1}\\left(1 - \\frac{\\alpha}{2}\\right)\n$$\nThis expression provides the required two-sided scalar threshold $\\tau$ as a function of the known system parameter $\\sigma$ and the desired false alarm probability $\\alpha$. The term $\\Phi^{-1}(1 - \\frac{\\alpha}{2})$ represents the critical value from the standard normal distribution corresponding to an upper tail probability of $\\frac{\\alpha}{2}$.",
            "answer": "$$\n\\boxed{\\sigma \\Phi^{-1}\\left(1 - \\frac{\\alpha}{2}\\right)}\n$$"
        },
        {
            "introduction": "While a well-tuned statistical detector provides a first line of defense, a sophisticated adversary may not trigger it at all. This hands-on practice challenges you to think like an attacker by deriving and simulating a stealthy False Data Injection (FDI) attack. You will see how an adversary with knowledge of the system can carefully craft a malicious sensor signal that causes a growing deviation in the physical state while keeping the monitored residual at zero, thereby remaining invisible to the detector.  This coding exercise vividly illustrates the vulnerabilities of simple residual-based monitoring and the necessity of more robust security designs.",
            "id": "4228462",
            "problem": "Consider a scalar Cyber-Physical System (CPS) water level tank modeled in discrete time by the state update and measurement equations\n$$x_{k+1} = x_k + u_k + w_k,$$\n$$y_k = x_k + v_k + a_k,$$\nwhere $x_k$ is the true water level at discrete time $k$ in meters, $u_k$ is a known control input in meters, $w_k$ is process noise in meters, $v_k$ is measurement noise in meters, and $a_k$ is an additive sensor False Data Injection (FDI) attack signal in meters. A Digital Twin observer employs a Luenberger structure\n$$\\hat{x}_{k+1} = \\hat{x}_k + u_k + L \\left(y_k - \\hat{x}_k\\right),$$\nwhere $\\hat{x}_k$ is the estimated water level in meters and $L \\in \\mathbb{R}$ is a fixed observer gain. The attack detector monitors the residual\n$$r_k = y_k - \\hat{x}_k,$$\nand raises an alarm when $|r_k|$ deviates from zero. An omniscient adversary, with full knowledge of $x_k$, $\\hat{x}_k$, $u_k$, $v_k$, and the model, seeks to remain stealthy by enforcing $r_k = 0$ at every step.\n\nTasks:\n1. Using only the system definitions above, derive from first principles a sensor attack law for $a_k$ that enforces $r_k = 0$ for all $k$, and then derive the closed-loop estimation error dynamics for $e_k := x_k - \\hat{x}_k$ under this stealthy attack.\n2. From the derived error dynamics, obtain an expression for the final physical deviation $e_N = x_N - \\hat{x}_N$ after $N$ steps as a function of the initial deviation $e_0$ and the noise sequence $\\{w_k\\}$.\n3. Implement a simulation of the plant, observer, and stealthy FDI attack over a finite horizon. The simulation must exactly realize the derived attack policy, update $x_k$ and $\\hat{x}_k$ step-by-step, and compute the final deviation $e_N$.\n\nPhysical units: All $x_k$, $\\hat{x}_k$, $u_k$, $w_k$, $v_k$, and $a_k$ are in meters. You must report each requested final deviation $e_N$ in meters, rounded to six decimal places.\n\nAngle units: Not applicable.\n\nPercentages: Not applicable.\n\nTest Suite:\nSimulate the following parameter sets. Each set specifies $N$, $L$, $x_0$, $\\hat{x}_0$, and the sequences $\\{u_k\\}_{k=0}^{N-1}$, $\\{w_k\\}_{k=0}^{N-1}$, and $\\{v_k\\}_{k=0}^{N-1}$, all in meters.\n\n- Case A (happy path): $N=5$, $L=0.6$, $x_0=1.0$, $\\hat{x}_0=1.0$, $u=[0.1, 0.1, 0.1, 0.1, 0.1]$, $w=[0.0, 0.02, -0.01, 0.0, 0.03]$, $v=[0.0, 0.0, 0.0, 0.0, 0.0]$.\n- Case B (boundary, perfect gain with nonzero measurement noise): $N=4$, $L=1.0$, $x_0=1.5$, $\\hat{x}_0=1.7$, $u=[0.0, 0.0, 0.0, 0.0]$, $w=[0.0, 0.0, 0.0, 0.0]$, $v=[0.2, -0.1, 0.05, 0.0]$.\n- Case C (edge, zero horizon): $N=0$, $L=0.5$, $x_0=0.0$, $\\hat{x}_0=0.0$, $u=[\\,]$, $w=[\\,]$, $v=[\\,]$.\n- Case D (mixed inputs and process noise): $N=6$, $L=0.3$, $x_0=1.0$, $\\hat{x}_0=0.8$, $u=[0.1, -0.05, 0.0, 0.05, 0.1, -0.1]$, $w=[0.02, 0.02, -0.01, 0.0, 0.01, 0.03]$, $v=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0]$.\n- Case E (nonzero process and measurement noise): $N=3$, $L=0.8$, $x_0=2.0$, $\\hat{x}_0=2.0$, $u=[0.05, 0.0, -0.05]$, $w=[0.05, -0.02, 0.04]$, $v=[-0.1, 0.2, 0.0]$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with no spaces, where each entry is the final deviation $e_N$ in meters for one test case in the order A, B, C, D, E. Each entry must be rounded to six decimal places, for example, \"[0.040000,-0.200000,0.000000,0.270000,0.070000]\".",
            "solution": "The problem is valid as it is scientifically grounded in standard principles of linear systems and control theory, is well-posed, objective, and provides a complete and consistent setup for both derivation and simulation.\n\n### Part 1: Derivation of the Stealthy Attack Law and Error Dynamics\n\nThe problem statement provides the following discrete-time model for the Cyber-Physical System (CPS) and the Digital Twin observer:\n\nSystem state update:\n$$x_{k+1} = x_k + u_k + w_k$$\nSystem measurement:\n$$y_k = x_k + v_k + a_k$$\nObserver state update:\n$$\\hat{x}_{k+1} = \\hat{x}_k + u_k + L (y_k - \\hat{x}_k)$$\nThe residual is defined as:\n$$r_k = y_k - \\hat{x}_k$$\nand the estimation error is defined as:\n$$e_k = x_k - \\hat{x}_k$$\n\n**Derivation of the Attack Law $a_k$**\n\nAn omniscient adversary aims to remain stealthy by enforcing the condition $r_k = 0$ for all time steps $k$.\nStarting from the definition of the residual:\n$$r_k = y_k - \\hat{x}_k$$\nSetting the residual to zero:\n$$y_k - \\hat{x}_k = 0 \\implies y_k = \\hat{x}_k$$\nThis means the adversary must manipulate the sensor measurement $y_k$ to be exactly equal to the observer's current state estimate $\\hat{x}_k$.\nWe substitute the measurement equation for $y_k$:\n$$x_k + v_k + a_k = \\hat{x}_k$$\nSolving for the required attack signal $a_k$, we obtain the sensor attack law:\n$$a_k = \\hat{x}_k - x_k - v_k$$\nThis attack requires the adversary to have full knowledge of the system's true state $x_k$, the observer's estimated state $\\hat{x}_k$, and the measurement noise $v_k$ at each time step $k$, as specified by the problem's assumption of an omniscient adversary.\n\n**Derivation of the Closed-Loop Error Dynamics**\n\nNext, we derive the dynamics of the estimation error $e_k = x_k - \\hat{x}_k$ under this stealthy attack. The error at the next time step, $e_{k+1}$, is:\n$$e_{k+1} = x_{k+1} - \\hat{x}_{k+1}$$\nSubstitute the expressions for the state update $x_{k+1}$ and observer update $\\hat{x}_{k+1}$:\n$$e_{k+1} = (x_k + u_k + w_k) - \\left( \\hat{x}_k + u_k + L (y_k - \\hat{x}_k) \\right)$$\nThe control input $u_k$ cancels out:\n$$e_{k+1} = (x_k - \\hat{x}_k) + w_k - L (y_k - \\hat{x}_k)$$\nRecognizing that $e_k = x_k - \\hat{x}_k$ and $r_k = y_k - \\hat{x}_k$, we can write this as:\n$$e_{k+1} = e_k + w_k - L r_k$$\nThis is the general error dynamics equation. Now, we apply the stealthy attack condition, $r_k = 0$. The equation simplifies significantly:\n$$e_{k+1} = e_k + w_k - L(0)$$\n$$e_{k+1} = e_k + w_k$$\nThis is the closed-loop estimation error dynamics under the specified stealthy attack. The dynamics show that the error evolves as an integrator of the process noise $w_k$. The observer gain $L$ and the measurement noise $v_k$ have been rendered completely ineffective by the adversary's strategy.\n\n### Part 2: Expression for the Final Deviation $e_N$\n\nUsing the derived error dynamics $e_{k+1} = e_k + w_k$, we can find a closed-form expression for the final deviation $e_N$ after $N$ steps by unrolling the recursion:\nFor $k=0$:\n$$e_1 = e_0 + w_0$$\nFor $k=1$:\n$$e_2 = e_1 + w_1 = (e_0 + w_0) + w_1$$\nFor $k=2$:\n$$e_3 = e_2 + w_2 = (e_0 + w_0 + w_1) + w_2$$\nContinuing this pattern, we can see by induction that the error at any step $k$ is the initial error $e_0$ plus the sum of all process noise terms up to step $k-1$. For the final step $N$, this relationship is:\n$$e_N = e_0 + \\sum_{k=0}^{N-1} w_k$$\nThe initial deviation is $e_0 = x_0 - \\hat{x}_0$. Therefore, the final deviation is:\n$$e_N = (x_0 - \\hat{x}_0) + \\sum_{k=0}^{N-1} w_k$$\nThis expression demonstrates that under the stealthy attack, the final error is determined solely by the initial error and the cumulative process noise over the horizon.\n\n### Part 3: Simulation Implementation\n\nThe simulation will implement the system and observer dynamics step-by-step for a horizon of $N$ steps, from $k=0$ to $k=N-1$. For each step $k$, the simulation will perform the following actions:\n1.  Read the current states $x_k$, $\\hat{x}_k$ and the exogenous inputs $u_k$, $w_k$, $v_k$.\n2.  Calculate the stealthy attack signal $a_k$ using the derived law: $a_k = \\hat{x}_k - x_k - v_k$.\n3.  Calculate the compromised sensor measurement: $y_k = x_k + v_k + a_k$. By construction, this will ensure $y_k = \\hat{x}_k$.\n4.  Update the observer state to $\\hat{x}_{k+1}$ using its dynamics: $\\hat{x}_{k+1} = \\hat{x}_k + u_k + L (y_k - \\hat{x}_k)$. Since $y_k - \\hat{x}_k = 0$, this simplifies to $\\hat{x}_{k+1} = \\hat{x}_k + u_k$.\n5.  Update the true system state to $x_{k+1}$ using its dynamics: $x_{k+1} = x_k + u_k + w_k$.\n6.  The new states $x_{k+1}$ and $\\hat{x}_{k+1}$ become the current states for the next iteration.\n\nAfter $N$ iterations, the simulation will have computed the final states $x_N$ and $\\hat{x}_N$. The final deviation is then calculated as $e_N = x_N - \\hat{x}_N$. This value is computed for each test case.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Simulates a CPS, a Luenberger observer, and a stealthy FDI attack.\n    Calculates the final estimation error for several test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (N, L, x0, x_hat0, u, w, v)\n        # Case A: happy path\n        {'N': 5, 'L': 0.6, 'x0': 1.0, 'x_hat0': 1.0, \n         'u': [0.1, 0.1, 0.1, 0.1, 0.1], \n         'w': [0.0, 0.02, -0.01, 0.0, 0.03], \n         'v': [0.0, 0.0, 0.0, 0.0, 0.0]},\n        # Case B: boundary, perfect gain with nonzero measurement noise\n        {'N': 4, 'L': 1.0, 'x0': 1.5, 'x_hat0': 1.7, \n         'u': [0.0, 0.0, 0.0, 0.0], \n         'w': [0.0, 0.0, 0.0, 0.0], \n         'v': [0.2, -0.1, 0.05, 0.0]},\n        # Case C: edge, zero horizon\n        {'N': 0, 'L': 0.5, 'x0': 0.0, 'x_hat0': 0.0, \n         'u': [], 'w': [], 'v': []},\n        # Case D: mixed inputs and process noise\n        {'N': 6, 'L': 0.3, 'x0': 1.0, 'x_hat0': 0.8, \n         'u': [0.1, -0.05, 0.0, 0.05, 0.1, -0.1], \n         'w': [0.02, 0.02, -0.01, 0.0, 0.01, 0.03], \n         'v': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]},\n        # Case E: nonzero process and measurement noise\n        {'N': 3, 'L': 0.8, 'x0': 2.0, 'x_hat0': 2.0, \n         'u': [0.05, 0.0, -0.05], \n         'w': [0.05, -0.02, 0.04], \n         'v': [-0.1, 0.2, 0.0]}\n    ]\n\n    results = []\n    for case in test_cases:\n        N = case['N']\n        L = case['L']\n        x = case['x0']\n        x_hat = case['x_hat0']\n        u_seq = case['u']\n        w_seq = case['w']\n        v_seq = case['v']\n\n        # Simulation loop from k=0 to N-1\n        # This loop will not execute if N=0, which is the correct behavior.\n        for k in range(N):\n            # Get current inputs and noises for step k\n            u_k = u_seq[k]\n            w_k = w_seq[k]\n            v_k = v_seq[k]\n\n            # 1. Adversary computes the stealthy attack signal a_k to enforce r_k = 0\n            # a_k = x_hat_k - x_k - v_k\n            a_k = x_hat - x - v_k\n\n            # 2. System generates the compromised measurement y_k\n            # y_k = x_k + v_k + a_k\n            y_k = x + v_k + a_k\n\n            # 3. Observer updates its state estimate\n            # The residual r_k = y_k - x_hat is zero by construction of a_k\n            # x_hat_{k+1} = x_hat_k + u_k + L * (y_k - x_hat_k)\n            x_hat_next = x_hat + u_k + L * (y_k - x_hat)\n\n            # 4. Plant updates its true state\n            # x_{k+1} = x_k + u_k + w_k\n            x_next = x + u_k + w_k\n            \n            # 5. Advance time by updating states for the next iteration\n            x = x_next\n            x_hat = x_hat_next\n\n        # After the loop, x and x_hat are x_N and x_hat_N\n        final_deviation = x - x_hat\n        results.append(final_deviation)\n\n    # Format the results into the required single-line string\n    # E.g., \"[0.040000,-0.200000,0.000000,0.270000,0.070000]\"\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\n# Execute the simulation and print the final result.\nsolve()\n```"
        },
        {
            "introduction": "Developing a detector and understanding its vulnerabilities are crucial steps, but assessing its real-world performance requires a nuanced approach. This exercise shifts the focus to evaluation, particularly in the challenging context of class imbalance where attacks are rare events. You will calculate and interpret key performance metrics like precision, recall, and the $F_1$ score. This practice will demonstrate why a high accuracy score can be misleading and how low precision, driven by a high number of false positives, can render an otherwise sensitive detector practically useless in a CPS environment.",
            "id": "4228486",
            "problem": "A Digital Twin (DT) of a Cyber-Physical System (CPS) water distribution network is used to generate labeled operational data for evaluating an Intrusion Detection System (IDS). The DT runs a physics-based simulation of hydraulic dynamics and injects adversarial actuator command sequences and sensor spoofing traces to produce ground-truth anomalies. Over a single evaluation window covering $10000$ discrete control cycles, the DT labels $100$ cycles as truly anomalous and $9900$ cycles as truly normal. The IDS flags cycles as anomalous based on residuals between DT-predicted and observed states. The resulting confusion counts are: True Positive ($TP$) $=70$, False Positive ($FP$) $=300$, False Negative ($FN$) $=30$, and True Negative ($TN$) $=9600$.\n\nStarting only from the core definitions of conditional event classification quality in binary detection, compute the detector’s precision $P$, recall $R$, and $F_{1}$ score $F_{1}$. Express each of $P$, $R$, and $F_{1}$ as a decimal and round your answers to four significant figures.\n\nAfter computing the values, discuss how severe class imbalance in CPS—where true anomalies occur with a low base rate—skews these metrics, and explain why this can lead to misleading conclusions about detector performance. Your discussion should connect the computed values to the imbalance and reason from the fundamental definitions without invoking any shortcut formulas beyond those definitions.",
            "solution": "The problem requires the computation of three standard performance metrics for a binary classifier—precision ($P$), recall ($R$), and the $F_1$ score—and a subsequent discussion on the effects of class imbalance on these metrics.\n\nFirst, we establish the given quantities from the confusion matrix resulting from the Intrusion Detection System's (IDS) evaluation:\n- True Positives ($TP$): The number of anomalous cycles correctly identified as anomalous. $TP = 70$.\n- False Positives ($FP$): The number of normal cycles incorrectly identified as anomalous. $FP = 300$.\n- False Negatives ($FN$): The number of anomalous cycles incorrectly identified as normal. $FN = 30$.\n- True Negatives ($TN$): The number of normal cycles correctly identified as normal. $TN = 9600$.\n\nThe total number of actual positive instances (true anomalies) is $AP = TP + FN = 70 + 30 = 100$.\nThe total number of actual negative instances (true normal states) is $AN = TN + FP = 9600 + 300 = 9900$.\nThe total number of predictions is $N = TP + TN + FP + FN = 70 + 9600 + 300 + 30 = 10000$.\n\nWe proceed by computing precision, recall, and the $F_1$ score based on their fundamental definitions.\n\nPrecision ($P$) is defined as the ratio of true positives to the total number of instances classified as positive (i.e., the sum of true positives and false positives). It measures the accuracy of the positive predictions.\n$$P = \\frac{TP}{TP + FP}$$\nSubstituting the given values:\n$$P = \\frac{70}{70 + 300} = \\frac{70}{370} = \\frac{7}{37}$$\nAs a decimal rounded to four significant figures, this is:\n$$P \\approx 0.1892$$\n\nRecall ($R$), also known as sensitivity or the true positive rate, is defined as the ratio of true positives to the total number of actual positive instances (i.e., the sum of true positives and false negatives). It measures the ability of the classifier to find all the positive instances.\n$$R = \\frac{TP}{TP + FN}$$\nSubstituting the given values:\n$$R = \\frac{70}{70 + 30} = \\frac{70}{100} = 0.7$$\nExpressed to four significant figures, this is:\n$$R = 0.7000$$\n\nThe $F_1$ score is the harmonic mean of precision and recall. It seeks to balance the two metrics. The general formula for the $F_\\beta$ score is $(1+\\beta^2) \\frac{P \\cdot R}{(\\beta^2 \\cdot P) + R}$, and for $F_1$, $\\beta=1$.\n$$F_1 = 2 \\cdot \\frac{P \\cdot R}{P + R}$$\nAlternatively, it can be expressed directly in terms of the confusion matrix components:\n$$F_1 = \\frac{2TP}{2TP + FP + FN}$$\nUsing the latter formula for exactness:\n$$F_1 = \\frac{2 \\cdot 70}{2 \\cdot 70 + 300 + 30} = \\frac{140}{140 + 330} = \\frac{140}{470} = \\frac{14}{47}$$\nAs a decimal rounded to four significant figures, this is:\n$$F_1 \\approx 0.2979$$\n\nDiscussion on class imbalance:\nThe dataset exhibits severe class imbalance. The base rate of the positive class (anomalies) is extremely low:\n$$\\text{Base Rate} = \\frac{AP}{N} = \\frac{100}{10000} = 0.01$$\nThis means only $1\\%$ of the cycles are anomalous, while $99\\%$ are normal. This imbalance significantly skews the interpretation of performance metrics.\n\nThe impact on precision is profound. The formula $P = \\frac{TP}{TP + FP}$ shows that precision is sensitive to the number of false positives ($FP$). In a highly imbalanced scenario, the number of actual negatives ($AN=9900$) is vast. Even a low False Positive Rate ($FPR = \\frac{FP}{AN} = \\frac{300}{9900} \\approx 0.0303$) can generate a large absolute number of false positives ($FP=300$). These false positives overwhelm the true positives ($TP=70$) in the denominator of the precision calculation. The computed precision of $P \\approx 0.1892$ indicates that over $81\\%$ of the alarms raised by the IDS are false. For an operator in a control room, this means most of the time they spend investigating an alarm will be wasted, leading to alert fatigue and a loss of trust in the system.\n\nRecall, defined as $R = \\frac{TP}{TP + FN}$, is conditioned only on the actual positive instances. Its calculation is independent of the number of true negatives or false positives. Therefore, it is not directly skewed by the large number of negative samples. Our calculated recall of $R=0.7000$ signifies that the IDS successfully detects $70\\%$ of all true anomalies that occur. While this may seem like a reasonable performance, it provides no information about the cost of false alarms. A system could achieve high recall by being overly sensitive, flagging many normal events as anomalous, which is precisely what leads to low precision.\n\nThe $F_1$ score, as the harmonic mean of precision and recall, is designed to capture this trade-off. Because the harmonic mean is strongly influenced by smaller values, the low precision ($P \\approx 0.1892$) pulls the $F_1$ score down to $F_1 \\approx 0.2979$, despite the much higher recall ($R=0.7000$). The $F_1$ score thus correctly reflects that the overall utility of the detector is poor due to the unacceptable rate of false alarms relative to true detections.\n\nThis situation highlights why metrics like accuracy can be dangerously misleading in imbalanced contexts. The accuracy of this IDS is:\n$$\\text{Accuracy} = \\frac{TP + TN}{N} = \\frac{70 + 9600}{10000} = 0.9670$$\nAn accuracy of $96.7\\%$ appears excellent at first glance. However, a trivial \"always normal\" classifier would achieve an accuracy of $\\frac{AN}{N} = \\frac{9900}{10000} = 0.99$, or $99\\%$. The sophisticated IDS is thus performing worse than a trivial baseline in terms of overall accuracy. The analysis of precision and recall reveals this critical weakness, which accuracy conceals. The low precision, driven by the class imbalance, is the key indicator of the detector's poor practical performance, a fact appropriately summarized by the low $F_1$ score.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.1892  0.7000  0.2979\n\\end{pmatrix}\n}\n$$"
        }
    ]
}