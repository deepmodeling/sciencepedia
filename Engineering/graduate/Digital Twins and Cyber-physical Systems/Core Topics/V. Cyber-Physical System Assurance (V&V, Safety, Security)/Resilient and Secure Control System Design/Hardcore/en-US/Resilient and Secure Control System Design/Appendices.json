{
    "hands_on_practices": [
        {
            "introduction": "Before designing a secure system, one must first master the principles of optimal estimation under uncertainty. The Kalman filter is the quintessential tool for this task, providing the best possible state estimate for linear systems with Gaussian noise. This foundational exercise  will guide you through the derivation of the filter's recursive equations and challenge you to compute its steady-state performance, reinforcing the core concepts of prediction and correction in state estimation.",
            "id": "4241173",
            "problem": "A digital twin in a Cyber-Physical System (CPS) uses a linear time-invariant (LTI) state-space model to estimate the physical system state in the presence of stochastic disturbances and measurement noise. The discrete-time plant and measurement models are given by the fundamental stochastic dynamical relations\n$$x_{t+1} = A x_t + B u_t + w_t,$$\n$$y_t = C x_t + v_t,$$\nwhere $x_t$ is the state, $u_t$ is a known control input, and $y_t$ is the measurement. Assume the process noise $w_t$ and measurement noise $v_t$ are mutually independent, zero-mean Gaussian random variables with covariances $Q$ and $R$, respectively, and independent of the initial condition $x_0$. The control input $u_t$ is known to the estimator and does not carry uncertainty.\n\nStarting from the definitions of conditional expectation for jointly Gaussian random variables and the law of total expectation and variance, derive the optimal linear minimum mean-square error state estimator (the Kalman filter) for the above model. Then, under the standard detectability and stabilizability conditions ensuring convergence, specialize to the steady-state regime where the error covariance converges to a constant matrix $P$ and provide the algebraic relation that $P$ must satisfy.\n\nFinally, compute the steady-state error covariance $P$ for the scalar specialization\n$$A = a = 0.9,\\quad C = 1,\\quad Q = q = 0.5,\\quad R = r = 2,$$\nand express your final numeric value as a single real number. Round your answer to four significant figures. No physical units are required, and the angle unit specification is not applicable.",
            "solution": "The problem is valid as it presents a standard, well-posed problem in estimation theory, specifically the derivation and application of the Kalman filter. The provided model and parameters are scientifically sound, complete, and consistent.\n\nThe objective is to find the optimal linear estimator for the state $x_t$ that minimizes the mean-square error, $\\mathbb{E}[\\|x_t - \\hat{x}_t\\|^2]$. For a system with Gaussian noise, the optimal estimator is the conditional mean, $\\hat{x}_t = \\mathbb{E}[x_t | Y_t]$, where $Y_t = \\{y_0, y_1, \\dots, y_t\\}$ is the history of all measurements up to time $t$. The derivation of the Kalman filter is a recursive algorithm consisting of a time update step (prediction) and a measurement update step (correction). This derivation relies on the fundamental laws of probability, including the law of total expectation and the properties of conditional expectation for jointly Gaussian random variables.\n\nLet $\\hat{x}_{t|t-1}$ be the a priori state estimate at time $t$ given measurements up to $t-1$, and let $\\hat{x}_{t|t}$ be the a posteriori state estimate at time $t$ given measurements up to $t$. The corresponding error covariance matrices are defined as $P_{t|t-1} = \\mathbb{E}[(x_t - \\hat{x}_{t|t-1})(x_t - \\hat{x}_{t|t-1})^T]$ and $P_{t|t} = \\mathbb{E}[(x_t - \\hat{x}_{t|t})(x_t - \\hat{x}_{t|t})^T]$.\n\nFirst, we derive the time update equations (prediction). The a priori estimate $\\hat{x}_{t|t-1}$ is the conditional expectation of $x_t$ given the measurement history $Y_{t-1}$.\n$$ \\hat{x}_{t|t-1} = \\mathbb{E}[x_t | Y_{t-1}] = \\mathbb{E}[A x_{t-1} + B u_{t-1} + w_{t-1} | Y_{t-1}] $$\nBy linearity of expectation, and since $u_{t-1}$ is a known deterministic input and $w_{t-1}$ is zero-mean and independent of past states and measurements:\n$$ \\hat{x}_{t|t-1} = A \\mathbb{E}[x_{t-1} | Y_{t-1}] + B u_{t-1} + \\mathbb{E}[w_{t-1}] = A \\hat{x}_{t-1|t-1} + B u_{t-1} $$\nThe a priori error is $e_{t|t-1} = x_t - \\hat{x}_{t|t-1} = A(x_{t-1} - \\hat{x}_{t-1|t-1}) + w_{t-1} = A e_{t-1|t-1} + w_{t-1}$. The a priori error covariance $P_{t|t-1}$ is then:\n$$ P_{t|t-1} = \\mathbb{E}[e_{t|t-1}e_{t|t-1}^T] = \\mathbb{E}[(A e_{t-1|t-1} + w_{t-1})(A e_{t-1|t-1} + w_{t-1})^T] $$\nThe estimation error $e_{t-1|t-1}$ is a function of $x_0$ and the noise terms $\\{w_0, \\dots, w_{t-2}\\}$ and $\\{v_0, \\dots, v_{t-1}\\}$, so it is uncorrelated with $w_{t-1}$. The cross-terms in the expectation are zero.\n$$ P_{t|t-1} = A \\mathbb{E}[e_{t-1|t-1}e_{t-1|t-1}^T] A^T + \\mathbb{E}[w_{t-1}w_{t-1}^T] = A P_{t-1|t-1} A^T + Q $$\n\nNext, we derive the measurement update equations (correction). At time $t$, the new measurement $y_t$ becomes available. Because all variables are Gaussian, the state $x_t$ and measurement $y_t$, conditioned on the past data $Y_{t-1}$, are jointly Gaussian. Their conditional means and covariances are:\n$$ \\mathbb{E}[x_t | Y_{t-1}] = \\hat{x}_{t|t-1} $$\n$$ \\mathbb{E}[y_t | Y_{t-1}] = \\mathbb{E}[C x_t + v_t | Y_{t-1}] = C \\hat{x}_{t|t-1} $$\n$$ \\text{Cov}(x_t | Y_{t-1}) = P_{t|t-1} $$\n$$ \\text{Cov}(y_t | Y_{t-1}) = \\text{Cov}(C x_t + v_t | Y_{t-1}) = C P_{t|t-1} C^T + R \\equiv S_t $$\n$$ \\text{Cov}(x_t, y_t | Y_{t-1}) = \\text{Cov}(x_t, C x_t + v_t | Y_{t-1}) = P_{t|t-1} C^T $$\nFor two jointly Gaussian random vectors $X$ and $Y$, the conditional expectation of $X$ given $Y=y$ is $\\mathbb{E}[X|Y=y] = \\mathbb{E}[X] + \\text{Cov}(X,Y)\\text{Cov}(Y,Y)^{-1}(y - \\mathbb{E}[Y])$. Applying this rule to our problem:\n$$ \\hat{x}_{t|t} = \\mathbb{E}[x_t | Y_t] = \\mathbb{E}[x_t | y_t, Y_{t-1}] = \\hat{x}_{t|t-1} + (P_{t|t-1} C^T) S_t^{-1} (y_t - C \\hat{x}_{t|t-1}) $$\nThis defines the a posteriori state estimate. The term $K_t = P_{t|t-1} C^T S_t^{-1} = P_{t|t-1} C^T (C P_{t|t-1} C^T + R)^{-1}$ is the Kalman gain.\nThe a posteriori error covariance matrix $P_{t|t}$ is given by the formula for conditional covariance $\\text{Cov}(X|Y) = \\text{Cov}(X,X) - \\text{Cov}(X,Y)\\text{Cov}(Y,Y)^{-1}\\text{Cov}(Y,X)$:\n$$ P_{t|t} = P_{t|t-1} - (P_{t|t-1} C^T) S_t^{-1} (C P_{t|t-1}) = P_{t|t-1} - K_t S_t K_t^T $$\nUsing the definition of $K_t$, this can also be written as $P_{t|t} = (I - K_t C)P_{t|t-1}$.\n\nThe full Kalman filter recursion is thus:\nTime Update:\n$$ \\hat{x}_{t|t-1} = A \\hat{x}_{t-1|t-1} + B u_{t-1} $$\n$$ P_{t|t-1} = A P_{t-1|t-1} A^T + Q $$\nMeasurement Update:\n$$ K_t = P_{t|t-1} C^T (C P_{t|t-1} C^T + R)^{-1} $$\n$$ \\hat{x}_{t|t} = \\hat{x}_{t|t-1} + K_t (y_t - C \\hat{x}_{t|t-1}) $$\n$$ P_{t|t} = (I - K_t C) P_{t|t-1} $$\n\nUnder stabilizability and detectability conditions, the error covariance matrices converge to unique positive semi-definite solutions. Let $P = \\lim_{t\\to\\infty} P_{t|t-1}$ be the steady-state a priori error covariance and $P_{post} = \\lim_{t\\to\\infty} P_{t|t}$ be the steady-state a posteriori error covariance. The recursive equations for covariances become algebraic equations:\n$$ P = A P_{post} A^T + Q $$\n$$ P_{post} = P - P C^T (C P C^T + R)^{-1} C P $$\nSubstituting the expression for $P_{post}$ into the first equation yields the Discrete Algebraic Riccati Equation (DARE) for the steady-state a priori error covariance $P$:\n$$ P = A \\left( P - P C^T (C P C^T + R)^{-1} C P \\right) A^T + Q $$\nThis is the algebraic relation that $P$ must satisfy.\n\nFinally, we compute the scalar value for $P$ given the parameters $A=a=0.9$, $C=c=1$, $Q=q=0.5$, and $R=r=2$. The scalar DARE is:\n$$ P = a^2 \\left( P - \\frac{P^2 c^2}{c^2 P + r} \\right) + q = a^2 P \\left( \\frac{c^2 P + r - c^2 P}{c^2 P + r} \\right) + q = \\frac{a^2 Pr}{c^2 P + r} + q $$\nMultiplying by $(c^2 P + r)$ to clear the denominator:\n$$ P(c^2 P + r) = a^2 Pr + q(c^2 P + r) $$\n$$ c^2 P^2 + rP = a^2 rP + c^2 qP + qr $$\nRearranging into a standard quadratic form $k_2 P^2 + k_1 P + k_0 = 0$:\n$$ c^2 P^2 + (r - a^2 r - c^2 q)P - qr = 0 $$\nSubstituting the numerical values:\n$$ (1)^2 P^2 + (2 - (0.9)^2(2) - (1)^2(0.5))P - (0.5)(2) = 0 $$\n$$ P^2 + (2 - (0.81)(2) - 0.5)P - 1 = 0 $$\n$$ P^2 + (2 - 1.62 - 0.5)P - 1 = 0 $$\n$$ P^2 - 0.12 P - 1 = 0 $$\nWe solve this quadratic equation for $P$ using the quadratic formula, $P = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$:\n$$ P = \\frac{-(-0.12) \\pm \\sqrt{(-0.12)^2 - 4(1)(-1)}}{2(1)} = \\frac{0.12 \\pm \\sqrt{0.0144 + 4}}{2} = \\frac{0.12 \\pm \\sqrt{4.0144}}{2} $$\nSince the error covariance $P$ must be a positive quantity, we take the positive root:\n$$ P = \\frac{0.12 + \\sqrt{4.0144}}{2} \\approx \\frac{0.12 + 2.00359677}{2} \\approx \\frac{2.12359677}{2} \\approx 1.06179838 $$\nRounding the result to four significant figures gives $1.062$.",
            "answer": "$$\\boxed{1.062}$$"
        },
        {
            "introduction": "A resilient system must be robust not just to random noise, but also to intelligent adversaries. This practice explores a classic vulnerability through the lens of a stealthy replay attack, where an attacker manipulates sensor data to corrupt the state estimate while remaining invisible to standard anomaly detectors. By constructing the attack signal and analyzing its impact on the estimation error , you will gain critical insight into how seemingly secure monitoring systems can be compromised.",
            "id": "4241238",
            "problem": "Consider a discrete-time linear time-invariant plant and state observer in a Cyber-Physical System (CPS) with a cloud-hosted Digital Twin (DT). The plant dynamics and sensor model are given by the standard state-space representation\n$$\nx_{t+1} = A x_t + B u_t + w_t, \\quad y_t = C x_t + v_t,\n$$\nwhere $x_t \\in \\mathbb{R}^{n}$ is the plant state, $u_t \\in \\mathbb{R}^{m}$ is the control input, $y_t \\in \\mathbb{R}^{p}$ is the sensor measurement, $A \\in \\mathbb{R}^{n \\times n}$, $B \\in \\mathbb{R}^{n \\times m}$, $C \\in \\mathbb{R}^{p \\times n}$, and $w_t \\in \\mathbb{R}^{n}$, $v_t \\in \\mathbb{R}^{p}$ are arbitrary process and measurement noise sequences (not necessarily zero), respectively. The DT implements a Luenberger observer with gain $L \\in \\mathbb{R}^{n \\times p}$:\n$$\n\\hat{x}_{t+1} = A \\hat{x}_t + B u_t + L \\big( y_t^{a} - C \\hat{x}_t \\big),\n$$\nwhere $\\hat{x}_t \\in \\mathbb{R}^{n}$ is the state estimate and $y_t^{a} \\in \\mathbb{R}^{p}$ is the signal received from the sensor over the network, which may be altered by an adversary. The residual used for anomaly detection is defined by\n$$\nr_t = y_t^{a} - C \\hat{x}_t.\n$$\nAssume the adversary has compromised the sensor channel and can read the estimate $\\hat{x}_t$ from the DT over the network but cannot alter the estimatorâ€™s internal state or the control input $u_t$. The adversary recorded a contiguous past residual sequence $\\{ r_{t_0}, r_{t_0+1}, \\dots, r_{t_0+N-1} \\}$ corresponding to an earlier time window of length $N \\in \\mathbb{N}$. At a chosen attack start time $t_a$, the adversary launches a replay attack over the interval $t \\in \\{ t_a, t_a+1, \\dots, t_a+N-1 \\}$ that preserves the residual used for detection at each time by constructing $y_t^{a}$ as a function of the current $\\hat{x}_t$ and the recorded sequence.\n\nDefine the estimation error $e_t = x_t - \\hat{x}_t$. Using only fundamental definitions of the plant, observer, residual, and estimation error dynamics, perform the following:\n- Construct an explicit expression for the adversarial signal $y_t^{a}$ that guarantees $r_t = r_{t_0 + (t - t_a)}$ for all $t \\in \\{ t_a, t_a+1, \\dots, t_a+N-1 \\}$.\n- Derive the exact error dynamics under this replay attack and obtain a closed-form expression for the estimation error at the end of the attack window $e_{t_a+N}$ in terms of $A$, $L$, the initial error $e_{t_a}$, the process noise sequence $\\{ w_t \\}$, and the recorded residual sequence $\\{ r_{t_0}, \\dots, r_{t_0+N-1} \\}$.\n\nExpress the final answer as a single closed-form analytic expression for $e_{t_a+N}$; no numerical rounding is required and no physical units are involved.",
            "solution": "The problem as stated is scientifically grounded, well-posed, objective, and internally consistent. It presents a standard scenario in the analysis of secure control systems and is a valid problem in discrete-time linear systems theory. We may therefore proceed with a formal derivation.\n\nThe solution requires a three-step derivation: first, constructing the adversarial signal $y_t^{a}$; second, deriving the resulting dynamics of the estimation error $e_t$; and third, solving the error dynamics to find a closed-form expression for the error at the end of the attack window, $e_{t_a+N}$.\n\n**Step 1: Construction of the Adversarial Signal**\n\nThe adversary's goal is to inject a malicious sensor signal $y_t^{a}$ such that the residual computed by the Digital Twin (DT), $r_t$, matches a pre-recorded sequence $\\{r_{t_0}, r_{t_0+1}, \\dots, r_{t_0+N-1}\\}$. The attack occurs over the time interval $t \\in \\{t_a, t_a+1, \\dots, t_a+N-1\\}$.\n\nThe residual is defined as:\n$$\nr_t = y_t^{a} - C \\hat{x}_t\n$$\nTo execute the replay attack, the adversary must ensure that for each time step $t$ in the attack window, the current residual $r_t$ is equal to the recorded residual $r_{t_0 + (t - t_a)}$. Therefore, the adversary must force the following equality:\n$$\ny_t^{a} - C \\hat{x}_t = r_{t_0 + (t - t_a)}\n$$\nSince the adversary can read the current state estimate $\\hat{x}_t$ and has access to the recorded residual sequence, they can construct the required adversarial signal by solving for $y_t^{a}$:\n$$\ny_t^{a} = C \\hat{x}_t + r_{t_0 + (t - t_a)}\n$$\nThis expression defines the malicious signal injected by the adversary for each $t \\in \\{t_a, t_a+1, \\dots, t_a+N-1\\}$.\n\n**Step 2: Derivation of the Estimation Error Dynamics**\n\nThe estimation error is defined as $e_t = x_t - \\hat{x}_t$. We seek the dynamics of this error, i.e., an expression for $e_{t+1}$ in terms of $e_t$ and other known quantities. We start by writing the expression for $e_{t+1}$:\n$$\ne_{t+1} = x_{t+1} - \\hat{x}_{t+1}\n$$\nNow, we substitute the given expressions for the plant dynamics $x_{t+1}$ and the observer dynamics $\\hat{x}_{t+1}$:\n$$\nx_{t+1} = A x_t + B u_t + w_t\n$$\n$$\n\\hat{x}_{t+1} = A \\hat{x}_t + B u_t + L \\big( y_t^{a} - C \\hat{x}_t \\big)\n$$\nSubstituting these into the equation for $e_{t+1}$ yields:\n$$\ne_{t+1} = (A x_t + B u_t + w_t) - \\left( A \\hat{x}_t + B u_t + L \\big( y_t^{a} - C \\hat{x}_t \\big) \\right)\n$$\nThe term $y_t^{a} - C \\hat{x}_t$ is precisely the residual $r_t$. During the attack, the adversary ensures that $r_t = r_{t_0 + (t - t_a)}$. Substituting this into the observer dynamics gives:\n$$\n\\hat{x}_{t+1} = A \\hat{x}_t + B u_t + L r_{t_0 + (t - t_a)}\n$$\nNow, we can write the expression for $e_{t+1}$ again:\n$$\ne_{t+1} = (A x_t + B u_t + w_t) - ( A \\hat{x}_t + B u_t + L r_{t_0 + (t - t_a)} )\n$$\nSimplifying the expression by grouping terms:\n$$\ne_{t+1} = A x_t - A \\hat{x}_t + B u_t - B u_t + w_t - L r_{t_0 + (t - t_a)}\n$$\n$$\ne_{t+1} = A (x_t - \\hat{x}_t) + w_t - L r_{t_0 + (t - t_a)}\n$$\nUsing the definition of the estimation error $e_t = x_t - \\hat{x}_t$, we arrive at the error dynamics under the replay attack:\n$$\ne_{t+1} = A e_t + w_t - L r_{t_0 + (t - t_a)}\n$$\nThis linear recurrence relation holds for all $t \\in \\{t_a, t_a+1, \\dots, t_a+N-1\\}$. It shows that under this specific attack, the error dynamics are driven by the process noise $w_t$ and a forcing term constructed from the replayed residual, and are independent of the true sensor measurement noise $v_t$. It is crucial to note that the term $(A-LC)$ which characterizes a healthy observer's error dynamics has been replaced by $A$, indicating the feedback correction from the measurements has been compromised and replaced by the adversary's injected signal.\n\n**Step 3: Closed-Form Expression for the Final Error**\n\nWe now solve the derived linear recurrence relation to find a closed-form expression for the error at the end of the attack window, $e_{t_a+N}$. The recurrence is:\n$$\ne_{t+1} = A e_t + d_t, \\quad \\text{where } d_t = w_t - L r_{t_0 + (t - t_a)}\n$$\nWe can unroll this recurrence starting from the initial error at the beginning of the attack, $e_{t_a}$.\nFor $t = t_a$:\n$$\ne_{t_a+1} = A e_{t_a} + d_{t_a}\n$$\nFor $t = t_a+1$:\n$$\ne_{t_a+2} = A e_{t_a+1} + d_{t_a+1} = A (A e_{t_a} + d_{t_a}) + d_{t_a+1} = A^2 e_{t_a} + A d_{t_a} + d_{t_a+1}\n$$\nFor $t = t_a+2$:\n$$\ne_{t_a+3} = A e_{t_a+2} + d_{t_a+2} = A (A^2 e_{t_a} + A d_{t_a} + d_{t_a+1}) + d_{t_a+2} = A^3 e_{t_a} + A^2 d_{t_a} + A d_{t_a+1} + d_{t_a+2}\n$$\nBy induction, after $N$ steps, the error at time $t_a+N$ is given by:\n$$\ne_{t_a+N} = A^N e_{t_a} + \\sum_{j=0}^{N-1} A^{N-1-j} d_{t_a+j}\n$$\nNow, we substitute back the expression for the forcing term $d_{t_a+j}$:\n$$\nd_{t_a+j} = w_{t_a+j} - L r_{t_0 + (t_a+j - t_a)} = w_{t_a+j} - L r_{t_0+j}\n$$\nSubstituting this into the summation:\n$$\ne_{t_a+N} = A^N e_{t_a} + \\sum_{j=0}^{N-1} A^{N-1-j} (w_{t_a+j} - L r_{t_0+j})\n$$\nThis expression can be separated into two sums, one related to the process noise and the other to the replayed residuals:\n$$\ne_{t_a+N} = A^N e_{t_a} + \\sum_{j=0}^{N-1} A^{N-1-j} w_{t_a+j} - \\sum_{j=0}^{N-1} A^{N-1-j} L r_{t_0+j}\n$$\nThis is the final closed-form expression for the estimation error at time $t_a+N$. It is expressed in terms of the initial error $e_{t_a}$, the system matrix $A$, the observer gain $L$, the sequence of process noise $\\{w_{t_a}, \\dots, w_{t_a+N-1}\\}$ during the attack, and the replayed residual sequence $\\{r_{t_0}, \\dots, r_{t_0+N-1}\\}$.",
            "answer": "$$\n\\boxed{A^{N} e_{t_a} + \\sum_{j=0}^{N-1} A^{N-1-j} w_{t_a+j} - \\sum_{j=0}^{N-1} A^{N-1-j} L r_{t_0+j}}\n$$"
        },
        {
            "introduction": "Moving from vulnerability analysis to proactive defense, this final practice focuses on the design of a resilient observer. Instead of assuming a specific attack structure, we consider a bounded adversarial signal and aim to minimize its worst-case impact on the estimation error. This problem  introduces you to the powerful framework of robust control, where the design is formulated as a convex optimization problem to synthesize an observer gain that guarantees performance against a class of threats.",
            "id": "4241206",
            "problem": "Consider a continuous-time, linear time-invariant plant-observer pair modeling a cyber-physical system with a measurement-side adversarial injection, representative of a digital twin of a physical process. The plant dynamics are given by $\\dot{x}(t) = A x(t)$ and the measured output is $y(t) = C x(t) + a(t)$, where $x(t) \\in \\mathbb{R}^n$ is the plant state, $y(t) \\in \\mathbb{R}^p$ is the measurement, and $a(t) \\in \\mathbb{R}^p$ is a bounded adversarial signal satisfying $\\lVert a(t) \\rVert_{\\infty} \\leq \\bar{a}$ for all $t \\geq 0$. The observer is of Luenberger form,\n$$\n\\dot{\\hat{x}}(t) = A \\hat{x}(t) + L \\left( y(t) - C \\hat{x}(t) \\right),\n$$\nwhere $\\hat{x}(t) \\in \\mathbb{R}^n$ is the estimate and $L \\in \\mathbb{R}^{n \\times p}$ is the observer gain to be designed. The estimation error $e(t) = x(t) - \\hat{x}(t)$ then obeys\n$$\n\\dot{e}(t) = \\left( A - L C \\right) e(t) - L a(t).\n$$\n\nDefine the worst-case induced $\\ell_{\\infty}$ performance level $\\gamma_{\\infty}$ as the smallest nonnegative number satisfying\n$$\n\\sup_{t \\geq 0} \\lVert e(t) \\rVert_{\\infty} \\leq \\gamma_{\\infty} \\, \\bar{a}\n$$\nfor all adversarial signals $a(t)$ obeying the bound $\\lVert a(t) \\rVert_{\\infty} \\leq \\bar{a}$. The design objective is to find $L$ that minimizes $\\gamma_{\\infty}$ subject to internal stability of the error dynamics.\n\nTasks:\n\n1) Starting from the error dynamics and the definition of induced norms for linear time-invariant systems, derive a sufficient Linear Matrix Inequality (LMI) condition, in terms of a symmetric matrix variable $P = P^{\\top} \\succ 0$, an auxiliary variable $Y = L P$, and a scalar $\\gamma_2 > 0$, that guarantees the induced $\\ell_2$ gain from $a(t)$ to $e(t)$ is less than $\\gamma_2$. Then, using norm equivalence $\\lVert v \\rVert_{\\infty} \\leq \\lVert v \\rVert_{2}$ and $\\lVert a \\rVert_{2} \\leq \\sqrt{p} \\, \\lVert a \\rVert_{\\infty}$ for $v \\in \\mathbb{R}^n$ and $a \\in \\mathbb{R}^p$, conclude a sufficient bound $\\gamma_{\\infty} \\leq \\sqrt{p} \\, \\gamma_2$. Your derivation should start from the bounded-real lemma for continuous-time systems and use Schur complements to obtain an explicit LMI involving $A$, $C$, $P$, $Y$, and $\\gamma_2$.\n\n2) Specialize to the case $C = I_n$ (the identity matrix) and $A$ diagonal with entries $a_i \\in \\mathbb{R}$, $i = 1, \\dots, n$. In this decoupled case, consider the scalar error channel\n$$\n\\dot{e}_i(t) = \\left( a_i - \\ell_i \\right) e_i(t) - \\ell_i \\, a_i(t),\n$$\nwhere $\\ell_i$ is the $i$-th diagonal entry of $L$ and $a_i(t)$ is the $i$-th component of $a(t)$. Show that the induced $\\ell_{\\infty}$ gain from $a_i(t)$ to $e_i(t)$ equals\n$$\n\\gamma_{\\infty,i}(\\ell_i) = \\int_{0}^{\\infty} \\left| \\ell_i \\, e^{\\left( a_i - \\ell_i \\right) t} \\right| \\, dt = \n\\begin{cases}\n\\dfrac{|\\ell_i|}{\\ell_i - a_i}, & \\text{if } \\ell_i > a_i, \\\\\n+\\infty, & \\text{otherwise},\n\\end{cases}\n$$\nand deduce the minimized bound $\\gamma_{\\infty,i}^{\\star}$ over $\\ell_i$ that ensures stability of the scalar channel. Provide the final expression of $\\gamma_{\\infty}^{\\star} = \\max_{i=1,\\dots,n} \\gamma_{\\infty,i}^{\\star}$.\n\n3) Implement a program that, for the following test suite of diagonal systems with $C = I_n$ and $\\bar{a} = 1$ (unitless), computes the minimized worst-case induced $\\ell_{\\infty}$ bound $\\gamma_{\\infty}^{\\star}$ according to Task 2. Each test case is specified by the diagonal entries of $A$, as follows:\n- Test 1 (happy path, unstable plant requiring observer stabilization): $A = \\mathrm{diag}(0.1, 0.3)$.\n- Test 2 (mixed stability): $A = \\mathrm{diag}(-0.2, 0.4)$.\n- Test 3 (stable plant, best-case): $A = \\mathrm{diag}(-0.7, -0.1)$.\n- Test 4 (boundary case): $A = \\mathrm{diag}(0.0, -0.3)$.\n\nIn each case, compute the minimized $\\gamma_{\\infty}^{\\star}$ as a float. Your program should produce a single line of output containing the four results as a comma-separated list enclosed in square brackets, for example, \"[result1,result2,result3,result4]\".\n\nAll answers are unitless and should be provided as floats. No angle units or percentages are involved in this problem. The program must be self-contained, use only the specified libraries, take no user input, and follow the specified output format.",
            "solution": "The problem is validated as scientifically grounded, well-posed, and objective. It presents a standard problem in resilient control theory, specifically observer design to mitigate the effects of adversarial attacks. All provided information is self-contained and consistent.\n\n### Task 1: LMI Condition Derivation and Norm Bounding\n\nThe objective of this task is to derive a sufficient Linear Matrix Inequality (LMI) condition for a guaranteed level of performance and then relate different performance measures.\n\nThe estimation error dynamics are given by:\n$$\n\\dot{e}(t) = (A - LC) e(t) - L a(t)\n$$\nWe can view this as a linear time-invariant (LTI) system with input $w(t) = a(t)$ and state $e(t)$. If we define the output to be the state itself, $z(t) = e(t)$, the system can be written in state-space form as:\n$$\n\\begin{cases}\n\\dot{e}(t) = A_e e(t) + B_e w(t) \\\\\nz(t) = C_e e(t) + D_e w(t)\n\\end{cases}\n$$\nwhere $A_e = A - LC$, $B_e = -L$, $C_e = I_n$ (the $n \\times n$ identity matrix), and $D_e = 0$.\n\nThe induced $\\ell_2$ gain from the input $a(t)$ to the output $e(t)$ is the $\\mathcal{H}_{\\infty}$ norm of the system's transfer function $G(s) = C_e(sI-A_e)^{-1}B_e + D_e$. The Bounded-Real Lemma provides a condition for this gain to be less than a scalar $\\gamma_2 > 0$. It states that $\\|G\\|_{\\mathcal{H}_{\\infty}} < \\gamma_2$ if and only if there exists a symmetric positive definite matrix $P = P^\\top \\succ 0$ such that the following matrix inequality holds:\n$$\n\\begin{bmatrix}\nA_e^\\top P + P A_e + C_e^\\top C_e & P B_e \\\\\nB_e^\\top P & -\\gamma_2^2 I_p\n\\end{bmatrix} \\prec 0\n$$\nSubstituting the matrices $A_e$, $B_e$, $C_e$, and $D_e = 0$ for our error system, we obtain:\n$$\n\\begin{bmatrix}\n(A - LC)^\\top P + P (A - LC) + I_n & -PL \\\\\n-L^\\top P & -\\gamma_2^2 I_p\n\\end{bmatrix} \\prec 0\n$$\nThis inequality is linear in $P$ but contains the product $PLC$, which is non-linear in the design variables $P$ and $L$. To convert this into an LMI, we perform a standard change of variables. We define a new matrix variable $Y = PL$. Note that since $P$ must be invertible, we can recover the observer gain $L$ via $L=P^{-1}Y$ after solving for $P$ and $Y$. After substituting $L=P^{-1}Y$ into the inequality and rearranging terms, we arrive at:\n$$\n\\begin{bmatrix}\nA^\\top P + PA - C^\\top Y^\\top - YC + I_n & -Y \\\\\n-Y^\\top & -\\gamma_2^2 I_p\n\\end{bmatrix} \\prec 0\n$$\nThis is a Linear Matrix Inequality in the variables $P=P^\\top$, $Y$, and $\\nu = \\gamma_2^2$. The problem of minimizing the $\\ell_2$ gain can be posed as a convex optimization problem:\n$$\n\\min_{P \\succ 0, Y, \\nu > 0} \\nu \\quad \\text{subject to} \\quad \\begin{bmatrix} A^\\top P + PA - C^\\top Y^\\top - YC + I_n & -Y \\\\ -Y^\\top & -\\nu I_p \\end{bmatrix} \\prec 0\n$$\nThe minimum $\\ell_2$ gain is $\\gamma_{2}^{\\star} = \\sqrt{\\nu^{\\star}}$.\n\nNext, we relate the induced $\\ell_2$ gain $\\gamma_2$ to the induced $\\ell_\\infty$ gain $\\gamma_\\infty$. We are given the vector norm inequalities $\\lVert v \\rVert_{\\infty} \\leq \\lVert v \\rVert_{2}$ for $v \\in \\mathbb{R}^n$ and $\\lVert a \\rVert_{2} \\leq \\sqrt{p} \\, \\lVert a \\rVert_{\\infty}$ for $a \\in \\mathbb{R}^p$.\nThe induced $\\ell_\\infty$ norm of the error system is defined by $\\sup_{t \\geq 0} \\lVert e(t) \\rVert_{\\infty} \\leq \\gamma_{\\infty} \\sup_{t \\geq 0} \\lVert a(t) \\rVert_{\\infty} = \\gamma_\\infty \\bar{a}$.\nA key result for stable LTI systems is that the induced gain from an $\\mathcal{L}_\\infty$ input signal (with norm $\\sup_t \\|a(t)\\|_2$) to an $\\mathcal{L}_\\infty$ output signal (with norm $\\sup_t \\|e(t)\\|_2$) is also given by the $\\mathcal{H}_\\infty$ norm of the transfer function, $\\gamma_2$. That is:\n$$\n\\sup_{t \\geq 0} \\lVert e(t) \\rVert_2 \\leq \\gamma_2 \\sup_{t \\geq 0} \\lVert a(t) \\rVert_2\n$$\nUsing the provided inequalities, we can establish the bound. For any time $t$:\n$$\n\\lVert e(t) \\rVert_{\\infty} \\leq \\lVert e(t) \\rVert_2\n$$\nTaking the supremum over $t \\geq 0$ on both sides:\n$$\n\\sup_{t \\geq 0} \\lVert e(t) \\rVert_{\\infty} \\leq \\sup_{t \\geq 0} \\lVert e(t) \\rVert_2\n$$\nNow, we use the induced gain property:\n$$\n\\sup_{t \\geq 0} \\lVert e(t) \\rVert_2 \\leq \\gamma_2 \\sup_{t \\geq 0} \\lVert a(t) \\rVert_2\n$$\nNext, we apply the inequality for the input vector $a(t)$:\n$$\n\\lVert a(t) \\rVert_2 \\leq \\sqrt{p} \\, \\lVert a(t) \\rVert_{\\infty}\n$$\nTaking the supremum over $t \\geq 0$:\n$$\n\\sup_{t \\geq 0} \\lVert a(t) \\rVert_2 \\leq \\sup_{t \\geq 0} (\\sqrt{p} \\, \\lVert a(t) \\rVert_{\\infty}) = \\sqrt{p} \\sup_{t \\geq 0} \\lVert a(t) \\rVert_{\\infty} = \\sqrt{p} \\, \\bar{a}\n$$\nCombining these steps:\n$$\n\\sup_{t \\geq 0} \\lVert e(t) \\rVert_{\\infty} \\leq \\sup_{t \\geq 0} \\lVert e(t) \\rVert_2 \\leq \\gamma_2 \\sup_{t \\geq 0} \\lVert a(t) \\rVert_2 \\leq \\gamma_2 (\\sqrt{p} \\, \\bar{a})\n$$\nBy comparing this with the definition $\\sup_{t \\geq 0} \\lVert e(t) \\rVert_{\\infty} \\leq \\gamma_\\infty \\bar{a}$, we conclude that a sufficient bound on the induced $\\ell_\\infty$ gain is:\n$$\n\\gamma_\\infty \\leq \\sqrt{p} \\, \\gamma_2\n$$\n\n### Task 2: Analysis of a Decoupled Diagonal System\n\nFor this task, we specialize to the case where $A = \\mathrm{diag}(a_1, \\dots, a_n)$, $C = I_n$, and the observer gain $L$ is also chosen to be diagonal, $L = \\mathrm{diag}(\\ell_1, \\dots, \\ell_n)$. The error dynamics decouple into $n$ independent scalar systems:\n$$\n\\dot{e}_i(t) = (a_i - \\ell_i) e_i(t) - \\ell_i a_i(t) \\quad \\text{for } i=1, \\dots, n\n$$\nThe induced $\\ell_\\infty$ gain $\\gamma_{\\infty,i}$ from the scalar input $a_i(t)$ to the scalar output $e_i(t)$ is given by the $\\ell_1$ norm of the system's impulse response, $h_i(t) = -\\ell_i e^{(a_i - \\ell_i)t}$.\n$$\n\\gamma_{\\infty,i}(\\ell_i) = \\int_0^\\infty |h_i(t)| dt = \\int_0^\\infty |-\\ell_i e^{(a_i - \\ell_i)t}| dt = |\\ell_i| \\int_0^\\infty e^{(a_i - \\ell_i)t} dt\n$$\nFor the system to be stable, the integral must converge, which requires the exponent to be negative: $a_i - \\ell_i < 0$, or $\\ell_i > a_i$. If this condition is not met ($\\ell_i \\le a_i$), the impulse response does not decay to zero, and the gain is infinite, $\\gamma_{\\infty,i} = +\\infty$.\n\nAssuming stability, i.e., $\\ell_i > a_i$, the integral evaluates to:\n$$\n\\int_0^\\infty e^{(a_i - \\ell_i)t} dt = \\left[ \\frac{e^{(a_i - \\ell_i)t}}{a_i - \\ell_i} \\right]_0^\\infty = 0 - \\frac{1}{a_i - \\ell_i} = \\frac{1}{\\ell_i - a_i}\n$$\nThus, the gain is:\n$$\n\\gamma_{\\infty,i}(\\ell_i) = \\frac{|\\ell_i|}{\\ell_i - a_i}, \\quad \\text{for } \\ell_i > a_i\n$$\nThis confirms the expression given in the problem statement.\n\nNow we must find the observer gain $\\ell_i$ that minimizes this gain, $\\gamma_{\\infty,i}^\\star = \\min_{\\ell_i > a_i} \\gamma_{\\infty,i}(\\ell_i)$. We analyze this by cases based on the sign of $a_i$.\n\nCase 1: $a_i \\ge 0$.\nThe stability condition is $\\ell_i > a_i \\ge 0$, which implies $\\ell_i > 0$. Therefore, $|\\ell_i| = \\ell_i$. The gain function is:\n$$\n\\gamma_{\\infty,i}(\\ell_i) = \\frac{\\ell_i}{\\ell_i - a_i} = \\frac{(\\ell_i - a_i) + a_i}{\\ell_i - a_i} = 1 + \\frac{a_i}{\\ell_i - a_i}\n$$\nIf $a_i = 0$, then $\\gamma_{\\infty,i}(\\ell_i) = 1$ for any $\\ell_i > 0$. The minimum gain is $\\gamma_{\\infty,i}^\\star = 1$.\nIf $a_i > 0$, the term $\\frac{a_i}{\\ell_i - a_i}$ is positive. To minimize the gain, we must make this term as small as possible. This is achieved by making the denominator $\\ell_i - a_i$ as large as possible, which means taking $\\ell_i \\to \\infty$. The limit is:\n$$\n\\lim_{\\ell_i \\to \\infty} \\left( 1 + \\frac{a_i}{\\ell_i - a_i} \\right) = 1 + 0 = 1\n$$\nSo, the infimum of the gain is $1$. In this scenario, the minimized bound is $\\gamma_{\\infty,i}^\\star = 1$.\n\nCase 2: $a_i < 0$.\nThe stability condition is $\\ell_i > a_i$. We analyze the function $\\gamma_{\\infty,i}(\\ell_i) = \\frac{|\\ell_i|}{\\ell_i - a_i}$ on the domain $(a_i, \\infty)$.\nSubcase 2a: $\\ell_i \\ge 0$. Here, $|\\ell_i| = \\ell_i$. The gain is $\\gamma_{\\infty,i}(\\ell_i) = \\frac{\\ell_i}{\\ell_i - a_i}$. Its derivative is $\\frac{d\\gamma}{d\\ell_i} = \\frac{(\\ell_i-a_i) - \\ell_i}{(\\ell_i-a_i)^2} = \\frac{-a_i}{(\\ell_i-a_i)^2}$. Since $a_i < 0$, the derivative is positive, so the function is increasing for $\\ell_i \\ge 0$. The minimum value in this range is at $\\ell_i = 0$, where $\\gamma_{\\infty,i}(0) = \\frac{0}{0-a_i} = 0$.\nSubcase 2b: $a_i < \\ell_i < 0$. Here, $|\\ell_i| = -\\ell_i$. The gain is $\\gamma_{\\infty,i}(\\ell_i) = \\frac{-\\ell_i}{\\ell_i - a_i}$. Its derivative is $\\frac{d\\gamma}{d\\ell_i} = \\frac{-(\\ell_i-a_i) - (-\\ell_i)}{(\\ell_i-a_i)^2} = \\frac{a_i}{(\\ell_i-a_i)^2}$. Since $a_i<0$, the derivative is negative, so the function is decreasing for $\\ell_i \\in (a_i, 0)$. The infimum is approached as $\\ell_i \\to 0^-$, where the gain approaches $0$.\n\nCombining these subcases, for $a_i < 0$, the minimum gain is $\\gamma_{\\infty,i}^\\star = 0$, achieved by choosing $\\ell_i = 0$. This is intuitive: if a plant mode is already stable, no observation feedback is needed for stabilization, and by setting $\\ell_i=0$, the attack signal $a_i(t)$ is completely decoupled from the error dynamics for that mode.\n\nSummary of minimized scalar gains:\n$$\n\\gamma_{\\infty,i}^\\star =\n\\begin{cases}\n0, & \\text{if } a_i < 0 \\\\\n1, & \\text{if } a_i \\ge 0\n\\end{cases}\n$$\nThe overall worst-case induced $\\ell_\\infty$ gain for the vector system is the maximum of the individual scalar gains. This is because $\\|e(t)\\|_\\infty = \\max_i |e_i(t)|$. We have $|e_i(t)| \\le \\gamma_{\\infty,i} \\sup_t |a_i(t)| \\le \\gamma_{\\infty,i} \\|a(t)\\|_\\infty \\le \\gamma_{\\infty,i} \\bar{a}$. Therefore, $\\|e(t)\\|_\\infty \\le (\\max_i \\gamma_{\\infty,i}) \\bar{a}$.\nThe minimized overall gain is thus:\n$$\n\\gamma_\\infty^\\star = \\max_{i=1,\\dots,n} \\gamma_{\\infty,i}^\\star\n$$\nThis means that if all plant modes are stable (all $a_i < 0$), the optimal gain is $\\gamma_\\infty^\\star = 0$. If even one plant mode is unstable or marginally stable ($a_i \\ge 0$), the optimal gain is $\\gamma_\\infty^\\star = 1$.\n\n### Task 3: Implementation\n\nThis logic is directly applied to the test cases.\n- Test 1: $A = \\mathrm{diag}(0.1, 0.3)$. Contains $0.1 \\ge 0$ and $0.3 \\ge 0$. Result: $1.0$.\n- Test 2: $A = \\mathrm{diag}(-0.2, 0.4)$. Contains $0.4 \\ge 0$. Result: $1.0$.\n- Test 3: $A = \\mathrm{diag}(-0.7, -0.1)$. All entries are $< 0$. Result: $0.0$.\n- Test 4: $A = \\mathrm{diag}(0.0, -0.3)$. Contains $0.0 \\ge 0$. Result: $1.0$.\nThe expected output is $[1.0, 1.0, 0.0, 1.0]$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# No other non-standard libraries are used.\n\ndef solve():\n    \"\"\"\n    Computes the minimized worst-case induced l-infinity bound for a set of\n    decoupled linear systems based on the stability of their modes.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    # Each test case is a tuple of the diagonal entries of the matrix A.\n    test_cases = [\n        (0.1, 0.3),    # Test 1: Unstable plant\n        (-0.2, 0.4),   # Test 2: Mixed stability\n        (-0.7, -0.1),  # Test 3: Stable plant\n        (0.0, -0.3),   # Test 4: Boundary case (marginally stable)\n    ]\n\n    results = []\n    \n    # Process each test case.\n    for case in test_cases:\n        # Convert the tuple of diagonal entries into a numpy array for easy processing.\n        a_diag = np.array(case)\n        \n        # The logic derived in Task 2 is as follows:\n        # The minimized induced l-infinity gain for a single scalar channel i,\n        # gamma_infinity_i_star, is 0 if the plant mode a_i is stable (a_i < 0),\n        # and 1 if the plant mode a_i is marginally stable or unstable (a_i >= 0).\n        #\n        # The overall minimized gain for the vector system, gamma_infinity_star,\n        # is the maximum of the individual scalar gains.\n        # gamma_infinity_star = max(gamma_infinity_i_star for all i).\n        #\n        # This implies that if any mode a_i is non-negative (>= 0), the overall\n        # minimized gain will be 1. If all modes are strictly negative (< 0),\n        # the overall minimized gain will be 0.\n        \n        # Check if any diagonal element is non-negative (>= 0).\n        # The `any()` method on a boolean numpy array returns True if at least\n        # one element is True.\n        if np.any(a_diag >= 0):\n            gamma_infinity_star = 1.0\n        else:\n            gamma_infinity_star = 0.0\n            \n        results.append(gamma_infinity_star)\n\n    # Final print statement in the exact required format.\n    # The map(str, results) converts each float in the results list to a string.\n    # ','.join(...) concatenates these strings with a comma in between.\n    # The f-string places this comma-separated string inside square brackets.\n    print(f\"[{','.join(map(str, results))}]\")\n\n# Execute the main function.\nsolve()\n\n```"
        }
    ]
}