{
    "hands_on_practices": [
        {
            "introduction": "Understanding the mechanics of False Data Injection (FDI) attacks begins with the fundamental principle of stealthiness. This exercise guides you through the derivation of the core condition for an attack to remain invisible to standard residual-based detectors in a system using a Weighted Least Squares (WLS) estimator. By working through a concrete example, you will see how an attacker can craft a specific data injection vector $a=Hc$ to precisely manipulate a target state variable without triggering any alarms, a foundational concept in cyber-physical security .",
            "id": "4221503",
            "problem": "Consider a Digital Twin (DT) of a Cyber-Physical System (CPS) that performs state estimation using linear Weighted Least Squares (WLS). The measurement model is $y = H x + \\varepsilon$, where $y \\in \\mathbb{R}^{m}$ is the measurement vector, $x \\in \\mathbb{R}^{n}$ is the state vector, $H \\in \\mathbb{R}^{m \\times n}$ is the measurement matrix, and $\\varepsilon \\in \\mathbb{R}^{m}$ is zero-mean measurement noise with covariance $R \\in \\mathbb{R}^{m \\times m}$ that is symmetric positive definite. The DT’s estimator minimizes the quadratic form $(y - H x)^{\\top} R^{-1} (y - H x)$ and uses the same $H$ and $R$ throughout operation. \n\nAn adversary mounts a False Data Injection (FDI) attack by adding an attack vector $a \\in \\mathbb{R}^{m}$ to the measurement, producing $y' = y + a$. The adversary seeks a stealthy attack, meaning the estimator’s residual computed from $y'$ is identical to the residual computed from $y$. Starting from the WLS formulation and the definition of the residual under the $R^{-1}$ inner product, derive the constraint on $a$ that guarantees such stealthiness. Then, under this stealth constraint, argue how an attack can be constructed to shift the estimated state by a chosen vector without altering the residual, and in particular how to shift exactly one component of the state vector by a prescribed amount while leaving all other components unchanged.\n\nNow specialize to the following concrete instance with $m = 5$ and $n = 3$. The measurement matrix $H \\in \\mathbb{R}^{5 \\times 3}$ and the noise covariance $R \\in \\mathbb{R}^{5 \\times 5}$ are\n$$\nH = \\begin{pmatrix}\n1 & 2 & 0 \\\\\n0 & 3 & 1 \\\\\n0 & 0 & 4 \\\\\n5 & 0 & 0 \\\\\n0 & 6 & 7\n\\end{pmatrix}, \\quad\nR = \\operatorname{diag}(1, 2, 3, 4, 5).\n$$\nAssume $H$ has full column rank. The adversary wants to change the DT’s estimate of the second state component $x_{2}$ by a fixed amount $\\Delta = 1$, while keeping all other estimated state components unchanged, and remaining stealthy. Among all stealthy attack vectors that achieve this, determine the minimal $\\ell_{0}$ cardinality of the attack vector $a$, where the $\\ell_{0}$ cardinality $\\|a\\|_{0}$ is defined as the number of nonzero entries of $a$. Express your final answer as a single integer. No rounding is required, and no units are needed.",
            "solution": "The problem requires us to analyze a False Data Injection (FDI) attack on a linear Weighted Least Squares (WLS) state estimator. We must first derive the general condition for a stealthy attack that manipulates the state estimate and then apply this to a specific numerical case to find the minimum sparsity of the required attack vector.\n\nThe WLS state estimate $\\hat{x}$ is the vector that minimizes the objective function $J(x) = (y - Hx)^{\\top} R^{-1} (y - Hx)$. To find the minimum, we set the gradient of $J(x)$ with respect to $x$ to zero.\n$$\n\\frac{dJ(x)}{dx} = -2H^{\\top} R^{-1} (y - Hx) = 0\n$$\nThis leads to the normal equations:\n$$\nH^{\\top} R^{-1} H \\hat{x} = H^{\\top} R^{-1} y\n$$\nThe problem states that the measurement matrix $H \\in \\mathbb{R}^{m \\times n}$ has full column rank (rank $n$) and the noise covariance matrix $R \\in \\mathbb{R}^{m \\times m}$ is symmetric positive definite. Since $R$ is positive definite, its inverse $R^{-1}$ exists and is also positive definite. The matrix $G = H^{\\top} R^{-1} H$, known as the Gramian matrix, is an $n \\times n$ matrix. Because $H$ has full column rank and $R^{-1}$ is positive definite, $G$ is invertible. Therefore, a unique solution for the state estimate $\\hat{x}$ exists:\n$$\n\\hat{x} = (H^{\\top} R^{-1} H)^{-1} H^{\\top} R^{-1} y\n$$\nAn adversary injects a false data vector $a \\in \\mathbb{R}^{m}$, resulting in a corrupted measurement vector $y' = y + a$. The estimator, being unaware of the attack, computes a new state estimate $\\hat{x}'$ using $y'$:\n$$\n\\hat{x}' = (H^{\\top} R^{-1} H)^{-1} H^{\\top} R^{-1} y' = (H^{\\top} R^{-1} H)^{-1} H^{\\top} R^{-1} (y+a)\n$$\n$$\n\\hat{x}' = (H^{\\top} R^{-1} H)^{-1} H^{\\top} R^{-1} y + (H^{\\top} R^{-1} H)^{-1} H^{\\top} R^{-1} a\n$$\nThe change in the state estimate, $\\Delta\\hat{x} = \\hat{x}' - \\hat{x}$, is therefore:\n$$\n\\Delta\\hat{x} = (H^{\\top} R^{-1} H)^{-1} H^{\\top} R^{-1} a\n$$\nThe problem defines a stealthy attack as one for which the estimator's residual is unchanged. The residual vector corresponding to the original measurement $y$ is $r = y - H\\hat{x}$. The residual for the attacked measurement $y'$ is $r' = y' - H\\hat{x}'$. The stealthiness condition is $r' = r$.\nLet's express $r'$ in terms of $r$ and $a$:\n$$\nr' = y' - H\\hat{x}' = (y+a) - H(\\hat{x}+\\Delta\\hat{x}) = (y - H\\hat{x}) + (a - H\\Delta\\hat{x})\n$$\n$$\nr' = r + a - H(H^{\\top} R^{-1} H)^{-1} H^{\\top} R^{-1} a\n$$\nLet us define the projection-like matrix $\\Pi = H(H^{\\top} R^{-1} H)^{-1} H^{\\top} R^{-1}$. Then the expression for $r'$ becomes:\n$$\nr' = r + (I - \\Pi)a\n$$\nFor the residual to be unchanged ($r' = r$), we must have $(I - \\Pi)a = 0$. This is the mathematical condition for a perfectly stealthy attack. This equation implies $a = \\Pi a$, which means the attack vector $a$ must lie in the image of the matrix $\\Pi$. The image of $\\Pi$ is the same as the column space of $H$, denoted $\\text{Im}(H)$. Therefore, the condition for a stealthy attack is that the attack vector $a$ must be a linear combination of the columns of $H$. That is, there must exist a vector $c \\in \\mathbb{R}^n$ such that:\n$$\na = Hc\n$$\nNow we can determine the effect of such a stealthy attack on the state estimate. By substituting $a = Hc$ into the expression for $\\Delta\\hat{x}$:\n$$\n\\Delta\\hat{x} = (H^{\\top} R^{-1} H)^{-1} H^{\\top} R^{-1} (Hc) = (H^{\\top} R^{-1} H)^{-1} (H^{\\top} R^{-1} H) c = I_n c = c\n$$\nThis provides a powerful insight: for a stealthy attack of the form $a = Hc$, the resulting shift in the state estimate is precisely the vector $c$. An adversary can therefore engineer a specific, desired change in the state estimate, $\\Delta x_c$, by choosing $c = \\Delta x_c$ and constructing the attack vector as $a = H\\Delta x_c$. This attack is guaranteed to be stealthy.\n\nTo shift exactly one component of the state vector, say the $k$-th component $x_k$, by a prescribed amount $\\Delta$ while leaving all other components unchanged, the desired state shift vector is $\\Delta x_c = \\Delta \\cdot e_k$, where $e_k$ is the $k$-th standard basis vector in $\\mathbb{R}^n$. The required stealthy attack vector is then:\n$$\na = H(\\Delta \\cdot e_k) = \\Delta \\cdot (He_k)\n$$\nThe vector $He_k$ is the $k$-th column of the matrix $H$, which we denote as $h_k$. Thus, the unique stealthy attack vector that accomplishes this specific state modification is $a = \\Delta \\cdot h_k$.\n\nWe now specialize to the given concrete instance. We have $m=5$ and $n=3$, with\n$$\nH = \\begin{pmatrix} 1 & 2 & 0 \\\\ 0 & 3 & 1 \\\\ 0 & 0 & 4 \\\\ 5 & 0 & 0 \\\\ 0 & 6 & 7 \\end{pmatrix}\n$$\nThe adversary's goal is to change the DT's estimate of the second state component $x_2$ by $\\Delta = 1$, while keeping the estimates of $x_1$ and $x_3$ unchanged. This corresponds to the state index $k=2$ and the shift amount $\\Delta=1$. The desired state shift vector is:\n$$\n\\Delta x_c = 1 \\cdot e_2 = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}\n$$\nBased on our general derivation, there is a unique stealthy attack vector $a$ that achieves this state modification. This vector is given by $a = H\\Delta x_c$.\n$$\na = H e_2 = \\begin{pmatrix} 1 & 2 & 0 \\\\ 0 & 3 & 1 \\\\ 0 & 0 & 4 \\\\ 5 & 0 & 0 \\\\ 0 & 6 & 7 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 3 \\\\ 0 \\\\ 0 \\\\ 6 \\end{pmatrix}\n$$\nThis vector $a$ is the second column of $H$. Since this is the only vector that satisfies both the stealthiness and state modification objectives, the question \"determine the minimal $\\ell_0$ cardinality\" simply requires us to compute the $\\ell_0$ cardinality of this unique vector. The $\\ell_0$ cardinality, $\\|a\\|_0$, is the number of non-zero entries in the vector $a$.\nThe components of the attack vector are $a_1=2$, $a_2=3$, $a_3=0$, $a_4=0$, and $a_5=6$.\nThe non-zero entries are $a_1$, $a_2$, and $a_5$. Counting these, we find there are $3$ non-zero entries.\nTherefore, the minimal $\\ell_0$ cardinality of the required attack vector is $3$.",
            "answer": "$$\\boxed{3}$$"
        },
        {
            "introduction": "Building on the basic mechanism of stealthy attacks, this practice elevates the challenge to the level of strategic attack design. Here, you will formalize the attacker's objective as a constrained optimization problem: how to achieve a desired malicious impact on the system's state estimate while compromising the minimum number of sensors. This problem introduces you to the concept of minimizing the $\\ell_0$-\"norm\" and its practical convex relaxation using the $\\ell_1$-norm, connecting FDI attack design to the powerful field of sparse recovery and compressed sensing .",
            "id": "4221495",
            "problem": "A digital twin of a linear cyber-physical system (CPS) maintains a static measurement model for state estimation of the form $z = H x + w$, where $z \\in \\mathbb{R}^{m}$ are measurements, $x \\in \\mathbb{R}^{n}$ is the state, $H \\in \\mathbb{R}^{m \\times n}$ has full column rank with $m \\ge n$, and $w$ is zero-mean Gaussian noise with covariance $R \\succ 0$. The estimator employed by the digital twin is the Weighted Least Squares (WLS) estimator, and Bad Data Detection (BDD) uses the innovation (residual) $r = z - H \\hat{x}$, with tests on its norm calibrated for the Gaussian model. An adversary launches a false data injection attack by adding $a \\in \\mathbb{R}^{m}$ to the measurements, producing $z' = z + a$, with the goals of: (i) remaining stealthy to residual-based BDD and (ii) enforcing a desired linear impact on the estimator, specified by a vector $d \\in \\mathbb{R}^{n}$ and a scalar $\\delta \\in \\mathbb{R}$ through $d^{\\top} \\Delta x = \\delta$, where $\\Delta x = \\hat{x}' - \\hat{x}$ is the change in the WLS estimate under the attack. The attacker also wishes to minimize the number of compromised sensors, modeled by the $\\ell_{0}$-“norm” of $a$, denoted $\\|a\\|_{0}$.\n\nFrom first principles, derive the necessary and sufficient condition for stealthiness of a false data injection attack against residual-based BDD under the WLS geometry, and use it to justify the correct minimal-cardinality formulation. Then reason about convex relaxations that replace the $\\ell_{0}$ objective by an $\\ell_{1}$ objective, and discuss when such relaxations are exact. Select all options below that are correct and fully consistent with these derivations.\n\nA. The minimal-cardinality stealth attack is the combinatorial optimization problem $\\min \\|a\\|_{0}$ subject to $a$ lies in the column space of $H$ and $d^{\\top} \\Delta x = \\delta$. Under WLS with $H$ full column rank, this is equivalently $\\min \\|a\\|_{0}$ subject to $a = H c$ and $d^{\\top} c = \\delta$ for some $c \\in \\mathbb{R}^{n}$, because any stealth attack induces $\\Delta x = c$.\n\nB. A convex relaxation that preserves stealthness and impact is $\\min \\|c\\|_{1}$ subject to $a = H c$ and $d^{\\top} a = \\delta$, because promoting sparsity of $c$ directly controls the number of attacked sensors while the impact is a linear functional of $a$.\n\nC. Eliminating $a$ yields the convex relaxation $\\min \\|H c\\|_{1}$ subject to $d^{\\top} c = \\delta$, which is equivalent to the split formulation $\\min \\|a\\|_{1}$ subject to $a = H c$ and $d^{\\top} c = \\delta$.\n\nD. The stealthness requirement for residual-based BDD is $a \\in \\operatorname{null}(H^{\\top} R^{-1})$, i.e., $a$ is $R^{-1}$-orthogonal to the column space of $H$. Therefore, $\\min \\|a\\|_{0}$ subject to $H^{\\top} R^{-1} a = 0$ and $d^{\\top} \\Delta x = \\delta$ correctly models minimal-cardinality stealth attacks.\n\nE. Under the WLS geometry, an equivalent convex relaxation is $\\min \\|a\\|_{1}$ subject to $P_{\\perp} a = 0$ and $d^{\\top} (H^{\\top} R^{-1} H)^{-1} H^{\\top} R^{-1} a = \\delta$, where $P_{\\perp} \\triangleq I - H (H^{\\top} R^{-1} H)^{-1} H^{\\top} R^{-1}$ is the $R^{-1}$-orthogonal projector onto the residual subspace. Moreover, letting $G \\in \\mathbb{R}^{(m-n+1) \\times m}$ stack these linear constraints so that $G a = g$, if the Null Space Property (NSP) of order $k$ holds for $G$ with respect to the true support $S$ of a sparsest solution (that is, for all $h \\in \\operatorname{null}(G) \\setminus \\{0\\}$, $\\|h_{S}\\|_{1} < \\|h_{S^{c}}\\|_{1}$), then the $\\ell_{1}$ solution equals the $\\ell_{0}$ minimizer whenever $|S| \\le k$.\n\nSelect all that apply.",
            "solution": "The user has provided a problem statement regarding false data injection attacks on a linear cyber-physical system. I will first validate the problem statement and then proceed to a full derivation and evaluation of the options.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- Measurement model: $z = H x + w$, where $z \\in \\mathbb{R}^{m}$, $x \\in \\mathbb{R}^{n}$.\n- Measurement matrix: $H \\in \\mathbb{R}^{m \\times n}$, with full column rank and $m \\ge n$.\n- Noise: $w$ is zero-mean Gaussian with covariance $R \\succ 0$.\n- Estimator: Weighted Least Squares (WLS).\n- Bad Data Detection (BDD): Based on the residual $r = z - H \\hat{x}$.\n- Attack: $z' = z + a$, where $a \\in \\mathbb{R}^{m}$ is the false data injection vector.\n- Attacker goals:\n    1.  Stealth: Remain undetected by residual-based BDD.\n    2.  Impact: Enforce $d^{\\top} \\Delta x = \\delta$ for given $d \\in \\mathbb{R}^{n}$, $\\delta \\in \\mathbb{R}$, where $\\Delta x = \\hat{x}' - \\hat{x}$.\n    3.  Sparsity: Minimize the number of attacked sensors, i.e., minimize $\\|a\\|_{0}$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, well-posed, and objective.\n- **Scientific Grounding**: The setup describes a canonical problem in the field of control systems security and state estimation, specifically concerning stealthy false data injection attacks (FDIAs). The models used (linear system, WLS estimation, residual-based detection) are standard.\n- **Well-Posedness**: The problem is clearly structured with defined objectives and constraints. The assumptions ($H$ has full column rank, $R \\succ 0$) ensure that the WLS estimator is well-defined and unique. The task is to derive and analyze standard formulations, which is a mathematically tractable problem.\n- **Objectivity**: The problem is described using precise mathematical and engineering terminology, free from ambiguity or subjective content.\n\n**Step 3: Verdict and Action**\nThe problem statement is **valid**. I will proceed with the derivation and solution.\n\n### Derivation from First Principles\n\n**1. Weighted Least Squares (WLS) Estimator and Residual**\nThe WLS estimate $\\hat{x}$ minimizes the cost function $J(x) = (z - Hx)^{\\top} R^{-1} (z - Hx)$. Setting the gradient with respect to $x$ to zero gives the normal equations:\n$$ (H^{\\top} R^{-1} H) \\hat{x} = H^{\\top} R^{-1} z $$\nSince $H$ has full column rank and $R \\succ 0$, the matrix $H^{\\top} R^{-1} H$ is invertible. The WLS estimate is:\n$$ \\hat{x} = (H^{\\top} R^{-1} H)^{-1} H^{\\top} R^{-1} z $$\nThe corresponding residual is $r = z - H\\hat{x}$.\n\n**2. Attack Analysis: Impact and Stealthiness**\nAn adversary adds a vector $a$ to the measurements, yielding $z' = z + a$.\nThe new estimate $\\hat{x}'$ is:\n$$ \\hat{x}' = (H^{\\top} R^{-1} H)^{-1} H^{\\top} R^{-1} z' = (H^{\\top} R^{-1} H)^{-1} H^{\\top} R^{-1} (z + a) = \\hat{x} + (H^{\\top} R^{-1} H)^{-1} H^{\\top} R^{-1} a $$\nThe change in the state estimate, $\\Delta x = \\hat{x}' - \\hat{x}$, is therefore:\n$$ \\Delta x = (H^{\\top} R^{-1} H)^{-1} H^{\\top} R^{-1} a $$\nThe new residual $r'$ is:\n$$ r' = z' - H\\hat{x}' = (z+a) - H(\\hat{x} + \\Delta x) = (z - H\\hat{x}) + (a - H\\Delta x) = r + (a - H\\Delta x) $$\nA perfectly stealthy attack is one that leaves the residual unchanged, i.e., $r' = r$. This requires the change in the residual to be zero:\n$$ a - H\\Delta x = 0 $$\nSubstituting the expression for $\\Delta x$:\n$$ a - H \\left( (H^{\\top} R^{-1} H)^{-1} H^{\\top} R^{-1} a \\right) = 0 $$\nLet $P \\triangleq H (H^{\\top} R^{-1} H)^{-1} H^{\\top} R^{-1}$. $P$ is the projection matrix onto the column space of $H$, $\\operatorname{col}(H)$, with respect to the $R^{-1}$-inner product. The stealth condition becomes $(I - P)a = 0$, or $Pa = a$. This is true if and only if $a$ lies in the subspace onto which $P$ projects, which is $\\operatorname{col}(H)$.\nTherefore, the necessary and sufficient condition for a perfectly stealthy attack is:\n$$ a \\in \\operatorname{col}(H) $$\nThis means there must exist a vector $c \\in \\mathbb{R}^{n}$ such that $a = Hc$.\n\n**3. Minimal-Cardinality Attack Formulation**\nIf the attack is stealthy ($a = Hc$), we can find the resulting state error vector $\\Delta x$ in terms of $c$:\n$$ \\Delta x = (H^{\\top} R^{-1} H)^{-1} H^{\\top} R^{-1} (Hc) = (H^{\\top} R^{-1} H)^{-1} (H^{\\top} R^{-1} H) c = c $$\nThis is a critical result: for a stealthy attack of the form $a=Hc$, the induced error in the state estimate is precisely the vector $c$.\nThe attacker's goals can now be formulated as an optimization problem.\n- **Minimize sparsity**: $\\min \\|a\\|_{0}$\n- **Maintain stealth**: $a = Hc$ for some $c \\in \\mathbb{R}^{n}$.\n- **Achieve impact**: $d^{\\top} \\Delta x = \\delta \\implies d^{\\top} c = \\delta$.\n\nCombining these, the minimal-cardinality stealth attack problem is:\n$$ \\min_{a,c} \\|a\\|_{0} \\quad \\text{subject to} \\quad a=Hc, \\quad d^{\\top} c = \\delta $$\nThis can be written more compactly by eliminating $a$ from the objective:\n$$ \\min_{c \\in \\mathbb{R}^n} \\|Hc\\|_{0} \\quad \\text{subject to} \\quad d^{\\top} c = \\delta $$\n\n**4. Convex Relaxation**\nThe $\\ell_0$-\"norm\" is non-convex, leading to a combinatorial, NP-hard problem. The standard approach is to replace $\\| \\cdot \\|_{0}$ with its closest convex surrogate, the $\\ell_1$-norm, $\\| \\cdot \\|_{1}$. This yields the convex relaxation:\n$$ \\min_{c \\in \\mathbb{R}^n} \\|Hc\\|_{1} \\quad \\text{subject to} \\quad d^{\\top} c = \\delta $$\nThis problem can be rewritten using a \"split variable\" $a$, which is often useful for numerical algorithms:\n$$ \\min_{a,c} \\|a\\|_{1} \\quad \\text{subject to} \\quad a=Hc, \\quad d^{\\top} c = \\delta $$\n\n### Option-by-Option Analysis\n\n**A. The minimal-cardinality stealth attack is the combinatorial optimization problem $\\min \\|a\\|_{0}$ subject to $a$ lies in the column space of $H$ and $d^{\\top} \\Delta x = \\delta$. Under WLS with $H$ full column rank, this is equivalently $\\min \\|a\\|_{0}$ subject to $a = H c$ and $d^{\\top} c = \\delta$ for some $c \\in \\mathbb{R}^{n}$, because any stealth attack induces $\\Delta x = c$.**\nThis statement accurately summarises the derivation. The stealth condition is $a \\in \\operatorname{col}(H)$, which is $a = Hc$. For such an attack, the induced error is indeed $\\Delta x = c$. Substituting this into the impact constraint gives $d^{\\top}c = \\delta$. Therefore, the optimization problem is correctly stated.\n**Verdict: Correct.**\n\n**B. A convex relaxation that preserves stealthness and impact is $\\min \\|c\\|_{1}$ subject to $a = H c$ and $d^{\\top} a = \\delta$, because promoting sparsity of $c$ directly controls the number of attacked sensors while the impact is a linear functional of $a$.**\nThis statement contains multiple errors.\n1. The objective should be to promote sparsity in the attack vector $a$, not the state error vector $c$. Minimizing $\\|c\\|_{1}$ does not, in general, lead to a sparse $a=Hc$. The correct objective in a convex relaxation is $\\|a\\|_{1}$ or $\\|Hc\\|_{1}$.\n2. The impact constraint is incorrectly stated as $d^{\\top}a = \\delta$. Since $d \\in \\mathbb{R}^{n}$ and $a \\in \\mathbb{R}^{m}$, this expression is not even well-defined unless $m=n$. The correct impact constraint acts on the state error $\\Delta x$, which leads to $d^{\\top}c = \\delta$.\n**Verdict: Incorrect.**\n\n**C. Eliminating $a$ yields the convex relaxation $\\min \\|H c\\|_{1}$ subject to $d^{\\top} c = \\delta$, which is equivalent to the split formulation $\\min \\|a\\|_{1}$ subject to $a = H c$ and $d^{\\top} c = \\delta$.**\nThis statement correctly describes the convex relaxation of the minimal-cardinality problem. As derived above, relaxing the $\\ell_0$-norm to the $\\ell_1$-norm in $\\min \\|Hc\\|_{0}$ s.t. $d^{\\top}c = \\delta$ gives $\\min \\|Hc\\|_{1}$ s.t. $d^{\\top}c = \\delta$. The equivalence to the split formulation is a standard technique in optimization, where the term $Hc$ is replaced by an auxiliary variable $a$ with an additional equality constraint $a = Hc$. This is a correct statement.\n**Verdict: Correct.**\n\n**D. The stealthness requirement for residual-based BDD is $a \\in \\operatorname{null}(H^{\\top} R^{-1})$, i.e., $a$ is $R^{-1}$-orthogonal to the column space of $H$. Therefore, $\\min \\|a\\|_{0}$ subject to $H^{\\top} R^{-1} a = 0$ and $d^{\\top} \\Delta x = \\delta$ correctly models minimal-cardinality stealth attacks.**\nThe stealth condition derived above is $a \\in \\operatorname{col}(H)$. The condition stated here, $H^{\\top} R^{-1} a = 0$, means $a$ is in the $R^{-1}$-orthogonal complement of $\\operatorname{col}(H)$. If $H^{\\top} R^{-1} a = 0$, then $\\Delta x = (H^{\\top} R^{-1} H)^{-1} (H^{\\top} R^{-1} a) = 0$. Such an attack has zero impact on the state estimate and is designed to manipulate the residual, not be stealthy. It is the opposite of the attacker's stated goals.\n**Verdict: Incorrect.**\n\n**E. Under the WLS geometry, an equivalent convex relaxation is $\\min \\|a\\|_{1}$ subject to $P_{\\perp} a = 0$ and $d^{\\top} (H^{\\top} R^{-1} H)^{-1} H^{\\top} R^{-1} a = \\delta$, where $P_{\\perp} \\triangleq I - H (H^{\\top} R^{-1} H)^{-1} H^{\\top} R^{-1}$ is the $R^{-1}$-orthogonal projector onto the residual subspace. Moreover, letting $G \\in \\mathbb{R}^{(m-n+1) \\times m}$ stack these linear constraints so that $G a = g$, if the Null Space Property (NSP) of order $k$ holds for $G$ with respect to the true support $S$ of a sparsest solution (that is, for all $h \\in \\operatorname{null}(G) \\setminus \\{0\\}$, $\\|h_{S}\\|_{1} < \\|h_{S^{c}}\\|_{1}$), then the $\\ell_{1}$ solution equals the $\\ell_{0}$ minimizer whenever $|S| \\le k$.**\nThe first part presents a convex relaxation formulated entirely in terms of the attack vector $a$. The objective is the correct $\\ell_1$-norm relaxation. The first constraint, $P_{\\perp} a = 0$, is precisely the stealth condition $a \\in \\operatorname{col}(H)$ that we derived. The second constraint is the impact constraint $d^{\\top}\\Delta x = \\delta$ with the expression for $\\Delta x$ in terms of $a$ substituted. So this formulation is correct and equivalent to those in option C.\nThe second part discusses conditions for the exactness of this relaxation. The constraints are linear, of the form $Ga=g$. The number of independent linear constraints is $(m - \\operatorname{rank}(H)) + 1 = m-n+1$, so $G \\in \\mathbb{R}^{(m-n+1) \\times m}$ is correct. The Null Space Property is the fundamental condition in sparse recovery theory that guarantees that the unique $\\ell_0$-minimizer is also the unique $\\ell_1$-minimizer. The description of the NSP, although phrased in a slightly convoluted way, is mathematically sound and correctly applied to the problem. It states the core sufficient condition for the $\\ell_1$ relaxation to be exact.\n**Verdict: Correct.**",
            "answer": "$$\\boxed{ACE}$$"
        },
        {
            "introduction": "After exploring attack construction, we now pivot to the defender's perspective. This hands-on practice challenges you to design a defense mechanism against a known class of threats by deriving a statistically optimal detector from first principles. You will develop the Generalized Likelihood Ratio Test (GLRT) for an attack confined to a specific subspace and analyze its performance by calculating the false alarm rate, giving you direct experience with the essential trade-offs in designing robust and reliable detection systems for modern cyber-physical infrastructure .",
            "id": "4221476",
            "problem": "A networked Cyber-Physical System (CPS) is monitored by a Digital Twin that provides a physics-consistent predicted measurement vector $\\hat{y} \\in \\mathbb{R}^{m}$ at each sampling instant. The actual measurement vector $y \\in \\mathbb{R}^{m}$ may be corrupted by a False Data Injection (FDI) integrity attack $a \\in \\mathbb{R}^{m}$ and measurement noise $w \\in \\mathbb{R}^{m}$, satisfying the stochastic model\n$$\ny = \\hat{y} + a + w,\n$$\nwhere the noise $w$ is independent and identically distributed Gaussian with $w \\sim \\mathcal{N}(0, \\sigma^{2} I_{m})$, and the attack $a$ is unknown to the defender. Based on the system architecture and historical vulnerability analysis, the attacker’s actuation is known to be confined to a $r$-dimensional subspace $\\mathcal{S} = \\mathrm{col}(U)$ spanned by the columns of a matrix $U \\in \\mathbb{R}^{m \\times r}$, where $U^{\\top} U = I_{r}$ and $r \\leq m$.\n\nConsider the binary hypothesis testing problem\n$$\nH_{0}: a = 0 \\quad \\text{versus} \\quad H_{1}: a \\in \\mathcal{S} \\setminus \\{0\\}.\n$$\nStarting from the definition of the Generalized Likelihood Ratio Test (GLRT), which compares the maximum likelihood under the alternative hypothesis to the likelihood under the null hypothesis, derive the corresponding GLRT decision rule specialized to this Gaussian setting. Your derivation must be based on the Gaussian probability density function and must not assume any pre-derived detection statistic.\n\nLet the GLRT be implemented by thresholding a scalar test statistic $T(y)$ against a threshold $\\gamma > 0$. Compute the exact false alarm rate, defined as the probability of deciding $H_{1}$ under $H_{0}$, as a closed-form analytic expression that depends only on the threshold $\\gamma$ and the subspace dimension $r$. You may use standard special functions in your final expression. No numerical approximation is required. Express your final answer as a single symbolic expression; no units are involved.",
            "solution": "The problem asks for the derivation of a Generalized Likelihood Ratio Test (GLRT) for detecting a structured false data injection attack and for the calculation of the corresponding false alarm rate.\n\nFirst, we derive the GLRT decision rule. The stochastic model for the measurement vector $y \\in \\mathbb{R}^{m}$ is given by\n$$\ny = \\hat{y} + a + w,\n$$\nwhere $\\hat{y}$ is the predicted measurement, $a$ is the attack vector, and $w \\sim \\mathcal{N}(0, \\sigma^{2} I_{m})$ is Gaussian noise. The random component of this model is $w$. We can express $w$ as $w = y - \\hat{y} - a$. The probability density function (PDF) of $y$ given $a$ is therefore\n$$\np(y|a) = \\frac{1}{(2\\pi \\sigma^2)^{m/2}} \\exp\\left(-\\frac{1}{2\\sigma^2} \\|y - \\hat{y} - a\\|^2\\right).\n$$\nThe problem is a binary hypothesis test:\n$$\nH_{0}: a = 0 \\quad \\text{versus} \\quad H_{1}: a \\in \\mathcal{S} \\setminus \\{0\\},\n$$\nwhere $\\mathcal{S} = \\mathrm{col}(U)$ is an $r$-dimensional subspace spanned by the columns of $U \\in \\mathbb{R}^{m \\times r}$ with $U^{\\top} U = I_{r}$.\n\nThe GLRT statistic is the ratio of the maximum likelihood under $H_{1}$ to the likelihood under $H_{0}$:\n$$\nL(y) = \\frac{\\sup_{a \\in \\mathcal{S} \\setminus \\{0\\}} p(y|a)}{p(y|H_0)}.\n$$\nUnder the null hypothesis $H_{0}$, the attack $a=0$. The likelihood is\n$$\np(y|H_0) = p(y|a=0) = \\frac{1}{(2\\pi \\sigma^2)^{m/2}} \\exp\\left(-\\frac{1}{2\\sigma^2} \\|y - \\hat{y}\\|^2\\right).\n$$\nThere are no unknown parameters under $H_0$, so no maximization is needed.\n\nUnder the alternative hypothesis $H_{1}$, the attack vector $a$ is unknown but lies in the subspace $\\mathcal{S}$. We find the maximum likelihood estimate (MLE) of $a$. Maximizing $p(y|a)$ with respect to $a \\in \\mathcal{S}$ is equivalent to minimizing the squared norm in the exponent:\n$$\n\\hat{a}_{ML} = \\arg\\min_{a \\in \\mathcal{S}} \\|(y - \\hat{y}) - a\\|^2.\n$$\nThis is a standard least-squares problem, where the solution $\\hat{a}_{ML}$ is the orthogonal projection of the residual vector $z = y - \\hat{y}$ onto the subspace $\\mathcal{S}$. The projection matrix onto $\\mathcal{S} = \\mathrm{col}(U)$ is $P_{\\mathcal{S}} = U(U^{\\top}U)^{-1}U^{\\top}$. Given $U^{\\top}U = I_r$, this simplifies to $P_{\\mathcal{S}} = UU^{\\top}$.\nThus, the MLE of $a$ is\n$$\n\\hat{a}_{ML} = P_{\\mathcal{S}} (y - \\hat{y}) = UU^{\\top}(y - \\hat{y}).\n$$\nSince the likelihood function is continuous in $a$ and $\\mathcal{S}$ is a closed set, the supremum over $a \\in \\mathcal{S} \\setminus \\{0\\}$ is the same as the supremum over $a \\in \\mathcal{S}$, unless the maximum uniquely occurs at $a=0$. In that case, the likelihood ratio would be $1$, leading to a decision for $H_0$ for any reasonable threshold, so we can proceed by maximizing over $\\mathcal{S}$.\n\nSubstituting $\\hat{a}_{ML}$ back into the likelihood function gives the maximized likelihood under $H_1$:\n$$\n\\sup_{a \\in \\mathcal{S}} p(y|a) = \\frac{1}{(2\\pi \\sigma^2)^{m/2}} \\exp\\left(-\\frac{1}{2\\sigma^2} \\|(y - \\hat{y}) - UU^{\\top}(y - \\hat{y})\\|^2\\right).\n$$\nLet $z = y - \\hat{y}$. The term in the exponent is $\\|(I_m - UU^{\\top})z\\|^2$. The matrix $I_m - UU^{\\top}$ is the projection onto the orthogonal complement of $\\mathcal{S}$. By the Pythagorean theorem for orthogonal projections, $\\|z\\|^2 = \\|P_{\\mathcal{S}}z\\|^2 + \\|(I_m - P_{\\mathcal{S}})z\\|^2 = \\|UU^{\\top}z\\|^2 + \\|(I_m - UU^{\\top})z\\|^2$.\nTherefore, $\\|(I_m - UU^{\\top})z\\|^2 = \\|z\\|^2 - \\|UU^{\\top}z\\|^2$.\n\nThe likelihood ratio $L(y)$ is then\n$$\nL(y) = \\frac{\\exp\\left(-\\frac{1}{2\\sigma^2}(\\|y - \\hat{y}\\|^2 - \\|UU^{\\top}(y - \\hat{y})\\|^2)\\right)}{\\exp\\left(-\\frac{1}{2\\sigma^2} \\|y - \\hat{y}\\|^2\\right)} = \\exp\\left(\\frac{1}{2\\sigma^2} \\|UU^{\\top}(y - \\hat{y})\\|^2\\right).\n$$\nThe GLRT decides $H_1$ if $L(y) > \\gamma_L$ for some threshold $\\gamma_L > 1$. This is equivalent to comparing the log-likelihood ratio to a threshold:\n$$\n\\ln(L(y)) = \\frac{1}{2\\sigma^2} \\|UU^{\\top}(y - \\hat{y})\\|^2 > \\ln(\\gamma_L).\n$$\nA monotonic function of this statistic forms an equivalent test. The problem specifies a test statistic $T(y)$ and threshold $\\gamma$, such that the final false alarm rate depends only on $\\gamma$ and $r$. This implies the nuisance parameter $\\sigma^2$ must be absent from the distribution of $T(y)$ under $H_0$. We therefore define the test statistic $T(y)$ by normalizing the term in the exponent:\n$$\nT(y) = \\frac{1}{\\sigma^2} \\|UU^{\\top}(y - \\hat{y})\\|^2.\n$$\nThe GLRT decision rule is to decide $H_1$ if $T(y) > \\gamma$, where $\\gamma = 2\\ln(\\gamma_L)$.\n\nNext, we compute the false alarm rate, $P_{FA}$. This is the probability of deciding $H_1$ when $H_0$ is true.\n$$\nP_{FA} = P(T(y) > \\gamma | H_0).\n$$\nUnder $H_0$, $a=0$, so $y = \\hat{y} + w$. The test statistic becomes\n$$\nT(y)|_{H_0} = \\frac{1}{\\sigma^2} \\|UU^{\\top}((\\hat{y} + w) - \\hat{y})\\|^2 = \\frac{1}{\\sigma^2} \\|UU^{\\top}w\\|^2.\n$$\nLet $w' = \\frac{1}{\\sigma}w$. Since $w \\sim \\mathcal{N}(0, \\sigma^2 I_m)$, $w'$ is a standard normal random vector, $w' \\sim \\mathcal{N}(0, I_m)$. Substituting $w = \\sigma w'$ into the expression for $T(y)$, we get\n$$\nT(y)|_{H_0} = \\frac{1}{\\sigma^2} \\|UU^{\\top}(\\sigma w')\\|^2 = \\frac{\\sigma^2}{\\sigma^2} \\|UU^{\\top}w'\\|^2 = \\|UU^{\\top}w'\\|^2.\n$$\nThis can be written as a quadratic form $w'^{\\top}(UU^{\\top})^{\\top}(UU^{\\top})w' = w'^{\\top}U(U^{\\top}U)U^{\\top}w' = w'^{\\top}(UU^{\\top})w'$.\nThe statistic is a quadratic form in the standard normal vector $w'$, with the matrix $P_{\\mathcal{S}} = UU^{\\top}$. According to Cochran's theorem (or the general theory of quadratic forms of Gaussian vectors), if $X \\sim \\mathcal{N}(0, I_m)$ and $A$ is an $m \\times m$ symmetric and idempotent matrix with $\\mathrm{rank}(A) = k$, then the random variable $X^{\\top}AX$ follows a chi-squared distribution with $k$ degrees of freedom, $\\chi^2_k$.\n\nThe matrix $P_{\\mathcal{S}} = UU^{\\top}$ is:\n1.  Symmetric: $(UU^{\\top})^{\\top} = (U^{\\top})^{\\top}U^{\\top} = UU^{\\top}$.\n2.  Idempotent: $(UU^{\\top})(UU^{\\top}) = U(U^{\\top}U)U^{\\top} = U I_r U^{\\top} = UU^{\\top}$.\nThe rank of $P_{\\mathcal{S}}$ is $\\mathrm{rank}(UU^{\\top}) = \\mathrm{tr}(UU^{\\top}) = \\mathrm{tr}(U^{\\top}U) = \\mathrm{tr}(I_r) = r$.\n\nTherefore, under $H_0$, the test statistic $T(y)$ follows a chi-squared distribution with $r$ degrees of freedom:\n$$\nT(y)|_{H_0} \\sim \\chi^2_r.\n$$\n The false alarm rate is the probability that this $\\chi^2_r$-distributed random variable exceeds the threshold $\\gamma$:\n$$\nP_{FA} = P(\\chi^2_r > \\gamma).\n$$\nThis is the value of the complementary cumulative distribution function (CCDF), or survival function, of the $\\chi^2_r$ distribution, evaluated at $\\gamma$. The CCDF of a chi-squared random variable is commonly expressed using the regularized upper incomplete gamma function, $Q(s,x)$, defined as:\n$$\nQ(s, x) = \\frac{\\Gamma(s, x)}{\\Gamma(s)} = \\frac{1}{\\Gamma(s)} \\int_x^{\\infty} t^{s-1} e^{-t} dt,\n$$\nwhere $\\Gamma(s)$ is the gamma function and $\\Gamma(s,x)$ is the upper incomplete gamma function.\nFor a $\\chi^2_r$ random variable $X$, the probability $P(X > x_0)$ is given by $Q(\\frac{r}{2}, \\frac{x_0}{2})$.\nIn our case, the degrees of freedom are $r$ and the value $x_0$ is the threshold $\\gamma$. Hence, the false alarm rate is\n$$\nP_{FA} = Q\\left(\\frac{r}{2}, \\frac{\\gamma}{2}\\right).\n$$\nThis expression depends only on the subspace dimension $r$ and the threshold $\\gamma$, as required.",
            "answer": "$$\n\\boxed{Q\\left(\\frac{r}{2}, \\frac{\\gamma}{2}\\right)}\n$$"
        }
    ]
}