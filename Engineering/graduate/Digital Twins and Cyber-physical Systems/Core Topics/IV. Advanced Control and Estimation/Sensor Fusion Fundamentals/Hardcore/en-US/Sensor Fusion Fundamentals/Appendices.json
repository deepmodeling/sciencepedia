{
    "hands_on_practices": [
        {
            "introduction": "The Kalman Filter is the cornerstone of state estimation for linear systems. This exercise focuses on its most critical phase: the measurement update. By working through a concrete example of tracking an object's position and velocity, you will apply the core equations to see how a new measurement is used to correct a prior state prediction and reduce its uncertainty. Mastering this fundamental procedure  is essential for building and troubleshooting any sensor fusion system.",
            "id": "4245541",
            "problem": "A Digital Twin (DT) of a one-dimensional transport line within a Cyber-Physical System (CPS) estimates the kinematic state using a linear Gaussian sensor fusion model. The state is two-dimensional, $x_k = \\begin{pmatrix} p_k \\\\ v_k \\end{pmatrix}$, where $p_k$ is position in meters and $v_k$ is velocity in meters per second. The process model and measurement model are linear:\n- Process: $x_k = F x_{k-1} + w_{k-1}$ with zero-mean Gaussian process noise $w_{k-1} \\sim \\mathcal{N}(0, Q)$.\n- Measurement: $z_k = H x_k + v_k$ with zero-mean Gaussian measurement noise $v_k \\sim \\mathcal{N}(0, R)$.\n\nAt time step $k$, the following model and data are given:\n- State transition matrix $F = \\begin{pmatrix} 1 & 0.1 \\\\ 0 & 1 \\end{pmatrix}$.\n- Measurement matrix $H = \\begin{pmatrix} 1 & 0 \\end{pmatrix}$.\n- Process noise covariance is of continuous white-acceleration form with known spectral density, but only the discrete covariance is provided here for completeness: $Q = \\sigma_a^2 \\begin{pmatrix} \\frac{(0.1)^4}{4} & \\frac{(0.1)^3}{2} \\\\ \\frac{(0.1)^3}{2} & (0.1)^2 \\end{pmatrix}$ for some known $\\sigma_a$, which was used prior to this update to obtain the prior covariance below.\n- Measurement noise covariance $R = \\begin{pmatrix} 0.25 \\end{pmatrix}$.\n- Prior (predicted) state estimate $\\hat{x}_{k \\mid k-1} = \\begin{pmatrix} 10.0 \\\\ 1.0 \\end{pmatrix}$ with prior covariance $P_{k \\mid k-1} = \\begin{pmatrix} 2.0 & 0.4 \\\\ 0.4 & 0.5 \\end{pmatrix}$.\n- A new scalar position measurement $z_k = 10.2$.\n\nUsing first principles of linear Gaussian Bayesian estimation that underlie the Kalman Filter (KF), perform a single measurement update at time $k$. Specifically, derive and compute:\n- The innovation $\\nu_k$.\n- The innovation covariance $S_k$.\n- The Kalman gain $K_k$.\n- The posterior state estimate $\\hat{x}_{k \\mid k}$.\n- The posterior covariance $P_{k \\mid k}$.\n\nFinally, report as your final answer the updated position component, i.e., the first component of $\\hat{x}_{k \\mid k}$, expressed in meters and rounded to four significant figures. No other quantity should be reported as the final answer. All intermediate calculations should be internally consistent with the provided statistical model and data.",
            "solution": "The problem will first be validated to ensure it is scientifically sound, well-posed, and complete before a solution is attempted.\n\n### Problem Validation\n\n#### Step 1: Extract Givens\nThe data provided in the problem statement are:\n- State vector: $x_k = \\begin{pmatrix} p_k \\\\ v_k \\end{pmatrix}$, where $p_k$ is position and $v_k$ is velocity.\n- Process model: $x_k = F x_{k-1} + w_{k-1}$, with $w_{k-1} \\sim \\mathcal{N}(0, Q)$.\n- Measurement model: $z_k = H x_k + v_k$, with $v_k \\sim \\mathcal{N}(0, R)$.\n- State transition matrix: $F = \\begin{pmatrix} 1 & 0.1 \\\\ 0 & 1 \\end{pmatrix}$.\n- Measurement matrix: $H = \\begin{pmatrix} 1 & 0 \\end{pmatrix}$.\n- Process noise covariance context: $Q = \\sigma_a^2 \\begin{pmatrix} \\frac{(0.1)^4}{4} & \\frac{(0.1)^3}{2} \\\\ \\frac{(0.1)^3}{2} & (0.1)^2 \\end{pmatrix}$.\n- Measurement noise covariance: $R = \\begin{pmatrix} 0.25 \\end{pmatrix}$.\n- Prior state estimate at time $k$: $\\hat{x}_{k \\mid k-1} = \\begin{pmatrix} 10.0 \\\\ 1.0 \\end{pmatrix}$.\n- Prior error covariance at time $k$: $P_{k \\mid k-1} = \\begin{pmatrix} 2.0 & 0.4 \\\\ 0.4 & 0.5 \\end{pmatrix}$.\n- Measurement at time $k$: $z_k = 10.2$.\n\n#### Step 2: Validate Using Extracted Givens\nThe problem describes a standard application of the Kalman Filter for state estimation in a linear Gaussian system.\n- **Scientifically Grounded**: The problem is based on the fundamental principles of Bayesian estimation and the Kalman Filter, a cornerstone of modern control theory and signal processing. The physical model (constant velocity) is a standard and valid approximation for many systems over short time intervals.\n- **Well-Posed**: The problem is well-posed. All necessary inputs for the measurement update step of the Kalman Filter—the prior estimate ($\\hat{x}_{k \\mid k-1}$), prior covariance ($P_{k \\mid k-1}$), measurement ($z_k$), measurement matrix ($H$), and measurement noise covariance ($R$)—are provided. The dimensions of all matrices and vectors are consistent for performing the required matrix operations.\n- **Objective**: The problem is stated using precise, objective mathematical language and numerical values. There are no subjective or ambiguous terms.\n- **Completeness and Consistency**: The problem provides a self-contained set of information sufficient to perform the requested calculations. The matrices $F$ and $Q$ are provided for context regarding the prediction step that produced the prior, but are not needed for the measurement update step, which is the focus of the problem. This does not represent an inconsistency. The covariance matrices $P_{k \\mid k-1}$ and $R$ are symmetric and contain positive diagonal elements, consistent with their definition.\n\n#### Step 3: Verdict and Action\nThe problem is valid. It is a well-defined, standard exercise in applying the measurement update equations of the Kalman Filter. A solution will be derived.\n\n### Solution Derivation\n\nThe measurement update step of the Kalman Filter corrects the predicted state estimate ($\\hat{x}_{k \\mid k-1}$) using a new measurement ($z_k$). The five quantities to be computed are derived sequentially.\n\n1.  **Innovation ($\\nu_k$)**: The innovation, or measurement residual, is the difference between the actual measurement $z_k$ and the measurement predicted from the prior state estimate, $\\hat{z}_{k \\mid k-1} = H \\hat{x}_{k \\mid k-1}$.\n    $$\n    \\nu_k = z_k - H \\hat{x}_{k \\mid k-1}\n    $$\n    Substituting the given values:\n    $$\n    H \\hat{x}_{k \\mid k-1} = \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} 10.0 \\\\ 1.0 \\end{pmatrix} = 10.0\n    $$\n    Therefore, the innovation is:\n    $$\n    \\nu_k = 10.2 - 10.0 = 0.2\n    $$\n\n2.  **Innovation Covariance ($S_k$)**: This is the covariance of the innovation, representing the total uncertainty in the predicted measurement. It is the sum of the projected state uncertainty and the measurement noise uncertainty.\n    $$\n    S_k = H P_{k \\mid k-1} H^\\top + R\n    $$\n    Substituting the given values:\n    $$\n    H P_{k \\mid k-1} H^\\top = \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} 2.0 & 0.4 \\\\ 0.4 & 0.5 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 2.0 & 0.4 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = 2.0\n    $$\n    Adding the measurement noise covariance:\n    $$\n    S_k = 2.0 + 0.25 = 2.25\n    $$\n    Since the measurement is a scalar, $S_k$ is also a scalar.\n\n3.  **Kalman Gain ($K_k$)**: The Kalman gain is the optimal weight that minimizes the posterior state error covariance. It balances the confidence in the prior estimate against the confidence in the measurement.\n    $$\n    K_k = P_{k \\mid k-1} H^\\top S_k^{-1}\n    $$\n    First, we compute the term $P_{k \\mid k-1} H^\\top$:\n    $$\n    P_{k \\mid k-1} H^\\top = \\begin{pmatrix} 2.0 & 0.4 \\\\ 0.4 & 0.5 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 2.0 \\\\ 0.4 \\end{pmatrix}\n    $$\n    The inverse of the scalar innovation covariance is $S_k^{-1} = (2.25)^{-1} = \\frac{1}{2.25} = \\frac{1}{9/4} = \\frac{4}{9}$.\n    Now, we compute the gain:\n    $$\n    K_k = \\begin{pmatrix} 2.0 \\\\ 0.4 \\end{pmatrix} \\frac{4}{9} = \\begin{pmatrix} 8.0/9 \\\\ 1.6/9 \\end{pmatrix} = \\begin{pmatrix} 8/9 \\\\ 8/45 \\end{pmatrix}\n    $$\n\n4.  **Posterior State Estimate ($\\hat{x}_{k \\mid k}$)**: The updated state estimate is the prior estimate corrected by the innovation, weighted by the Kalman gain.\n    $$\n    \\hat{x}_{k \\mid k} = \\hat{x}_{k \\mid k-1} + K_k \\nu_k\n    $$\n    Substituting the calculated values:\n    $$\n    \\hat{x}_{k \\mid k} = \\begin{pmatrix} 10.0 \\\\ 1.0 \\end{pmatrix} + \\begin{pmatrix} 8/9 \\\\ 8/45 \\end{pmatrix} (0.2)\n    $$\n    Using $0.2 = \\frac{1}{5}$:\n    $$\n    \\hat{x}_{k \\mid k} = \\begin{pmatrix} 10.0 \\\\ 1.0 \\end{pmatrix} + \\begin{pmatrix} (8/9) \\times (1/5) \\\\ (8/45) \\times (1/5) \\end{pmatrix} = \\begin{pmatrix} 10 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} 8/45 \\\\ 8/225 \\end{pmatrix} = \\begin{pmatrix} 10 + 8/45 \\\\ 1 + 8/225 \\end{pmatrix} = \\begin{pmatrix} 450/45 + 8/45 \\\\ 225/225 + 8/225 \\end{pmatrix} = \\begin{pmatrix} 458/45 \\\\ 233/225 \\end{pmatrix}\n    $$\n    In decimal form, $\\hat{x}_{k \\mid k} \\approx \\begin{pmatrix} 10.1778 \\\\ 1.0356 \\end{pmatrix}$.\n\n5.  **Posterior Covariance ($P_{k \\mid k}$)**: The updated error covariance reflects the reduction in uncertainty due to the measurement. A common and numerically stable form is the Joseph form, but the simpler form $P_{k \\mid k} = (I - K_k H) P_{k \\mid k-1}$ is sufficient and exact here.\n    $$\n    P_{k \\mid k} = (I - K_k H) P_{k \\mid k-1}\n    $$\n    First, compute $I - K_k H$:\n    $$\n    K_k H = \\begin{pmatrix} 8/9 \\\\ 8/45 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\end{pmatrix} = \\begin{pmatrix} 8/9 & 0 \\\\ 8/45 & 0 \\end{pmatrix}\n    $$\n    $$\n    I - K_k H = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} - \\begin{pmatrix} 8/9 & 0 \\\\ 8/45 & 0 \\end{pmatrix} = \\begin{pmatrix} 1/9 & 0 \\\\ -8/45 & 1 \\end{pmatrix}\n    $$\n    Now, multiply by the prior covariance:\n    $$\n    P_{k \\mid k} = \\begin{pmatrix} 1/9 & 0 \\\\ -8/45 & 1 \\end{pmatrix} \\begin{pmatrix} 2.0 & 0.4 \\\\ 0.4 & 0.5 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{9}(2.0) & \\frac{1}{9}(0.4) \\\\ -\\frac{8}{45}(2.0) + 0.4 & -\\frac{8}{45}(0.4) + 0.5 \\end{pmatrix}\n    $$\n    Converting to fractions: $2.0 = 2$, $0.4 = \\frac{2}{5}$, $0.5 = \\frac{1}{2}$.\n    $$\n    P_{k \\mid k} = \\begin{pmatrix} \\frac{2}{9} & \\frac{2}{45} \\\\ -\\frac{16}{45} + \\frac{2}{5} & -\\frac{16}{225} + \\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} \\frac{2}{9} & \\frac{2}{45} \\\\ -\\frac{16}{45} + \\frac{18}{45} & -\\frac{32}{450} + \\frac{225}{450} \\end{pmatrix} = \\begin{pmatrix} 2/9 & 2/45 \\\\ 2/45 & 193/450 \\end{pmatrix}\n    $$\n\nThe question asks for the updated position component, which is the first element of the posterior state estimate $\\hat{x}_{k \\mid k}$.\n$$\np_k^{\\text{updated}} = (\\hat{x}_{k \\mid k})_1 = \\frac{458}{45} \\approx 10.1777...\n$$\nRounding this value to four significant figures gives $10.18$.",
            "answer": "$$\n\\boxed{10.18}\n$$"
        },
        {
            "introduction": "Real-world cyber-physical systems are often nonlinear, demanding techniques beyond the standard Kalman Filter. The Unscented Kalman Filter (UKF) offers a powerful solution by using a deterministic sampling method called the unscented transform to propagate uncertainty through nonlinear functions. This practice  will guide you through a single UKF measurement update, from generating sigma points to computing the posterior state, illustrating how to handle nonlinear models without the need for analytical Jacobians.",
            "id": "4245570",
            "problem": "A digital twin of a Cyber-Physical System (CPS) tracks a scalar latent state $x$ representing a physical quantity (for example, a normalized thermal parameter) using the Unscented Kalman Filter (UKF). The prior belief over $x$ just before receiving a new sensor reading is Gaussian with mean $\\mu$ and covariance (variance, since $x$ is scalar) $\\Sigma$. The sensor provides a nonlinear measurement modeled as $z = h(x) + v$ with $h(x) = x^2$, where $v$ is zero-mean Gaussian measurement noise with covariance $R$, independent of $x$.\n\nYou are given the following numerical parameters for a single measurement update:\n- Prior mean $\\mu = 1.2$ and prior covariance $\\Sigma = 0.25$.\n- Measurement $z = 1.8$.\n- UKF scaling parameters $\\alpha = 0.4$, $\\beta = 2$, and $\\kappa = 0$.\n- Measurement noise covariance $R = 0.04$.\n\nStarting from the principles that govern Bayesian estimation with Gaussian priors and the moment-matching rationale of the unscented transform, carry out one UKF measurement update for this scalar system. Compute the sigma points, propagate them through $h(x)$, and use them to determine the predicted measurement mean, the innovation covariance, the cross-covariance, the Kalman gain $K$, and then the posterior mean and covariance after the measurement update.\n\nReport your final results as a row matrix in the order $K$, posterior mean $\\mu^+$, and posterior covariance $\\Sigma^+$. Round your answer to six significant figures. Do not include any units in your final reported values.",
            "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\n### Step 1: Extract Givens\nThe following data and definitions are provided:\n- **Latent state:** A scalar state $x$. The state dimension is $L=1$.\n- **Prior belief:** Gaussian with mean $\\mu = 1.2$ and covariance $\\Sigma = 0.25$.\n- **Measurement model:** $z = h(x) + v$, with $h(x) = x^2$.\n- **Measurement noise:** Zero-mean Gaussian noise $v$ with covariance $R = 0.04$.\n- **Received measurement:** $z = 1.8$.\n- **UKF scaling parameters:** $\\alpha = 0.4$, $\\beta = 2$, $\\kappa = 0$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is a direct and standard application of the Unscented Kalman Filter (UKF) for a measurement update in a nonlinear system.\n- **Scientifically Grounded:** The UKF is a well-established and fundamental algorithm in nonlinear state estimation, widely used in engineering and science. The model provided is a canonical example used to illustrate the operation of such filters. All principles are sound.\n- **Well-Posed:** The problem is fully specified with all necessary parameters (prior state, measurement model, noise statistics, UKF parameters) required to compute a unique solution for one update step.\n- **Objective:** The problem is stated using precise mathematical and engineering terminology, free from subjective language.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. A full, reasoned solution will be provided.\n\n### Solution Derivation\nThe Unscented Kalman Filter measurement update is performed in three main stages:\n1.  Generation of sigma points and their weights.\n2.  Prediction of the measurement and relevant covariances using the unscented transform.\n3.  Update of the state estimate and its covariance.\n\n**1. Sigma Point Generation**\n\nFirst, the composite scaling parameter $\\lambda$ is calculated from the given UKF parameters $\\alpha$, $\\kappa$, and the state dimension $L=1$.\n$$ \\lambda = \\alpha^2(L + \\kappa) - L $$\nSubstituting the given values:\n$$ \\lambda = (0.4)^2(1 + 0) - 1 = 0.16 - 1 = -0.84 $$\nThere are $2L+1 = 3$ sigma points. They are generated as follows:\n$$ \\mathcal{X}_0 = \\mu $$\n$$ \\mathcal{X}_i = \\mu + \\left(\\sqrt{(L+\\lambda)\\Sigma}\\right)_i \\quad \\text{for } i=1, \\dots, L $$\n$$ \\mathcal{X}_{i+L} = \\mu - \\left(\\sqrt{(L+\\lambda)\\Sigma}\\right)_i \\quad \\text{for } i=1, \\dots, L $$\nFor our scalar case ($L=1$), the square root term is a scalar:\n$$ \\sqrt{(L+\\lambda)\\Sigma} = \\sqrt{(1 - 0.84) \\times 0.25} = \\sqrt{0.16 \\times 0.25} = \\sqrt{0.04} = 0.2 $$\nThe sigma points are:\n$$ \\mathcal{X}_0 = 1.2 $$\n$$ \\mathcal{X}_1 = 1.2 + 0.2 = 1.4 $$\n$$ \\mathcal{X}_2 = 1.2 - 0.2 = 1.0 $$\n\nNext, the weights for the mean ($W_m$) and covariance ($W_c$) are calculated:\n$$ W_{m,0} = \\frac{\\lambda}{L+\\lambda} = \\frac{-0.84}{1 - 0.84} = \\frac{-0.84}{0.16} = -5.25 $$\n$$ W_{c,0} = \\frac{\\lambda}{L+\\lambda} + (1 - \\alpha^2 + \\beta) = -5.25 + (1 - (0.4)^2 + 2) = -5.25 + (1 - 0.16 + 2) = -5.25 + 2.84 = -2.41 $$\n$$ W_{m,i} = W_{c,i} = \\frac{1}{2(L+\\lambda)} = \\frac{1}{2(1 - 0.84)} = \\frac{1}{2(0.16)} = \\frac{1}{0.32} = 3.125 \\quad \\text{for } i=1, 2 $$\n\n**2. Prediction using Unscented Transform**\n\nThe sigma points are propagated through the nonlinear measurement function $h(x) = x^2$ to obtain the predicted measurement sigma points, $\\mathcal{Z}_i$:\n$$ \\mathcal{Z}_0 = h(\\mathcal{X}_0) = (1.2)^2 = 1.44 $$\n$$ \\mathcal{Z}_1 = h(\\mathcal{X}_1) = (1.4)^2 = 1.96 $$\n$$ \\mathcal{Z}_2 = h(\\mathcal{X}_2) = (1.0)^2 = 1.00 $$\n\nThe predicted measurement mean, $\\hat{z}$, is the weighted average of these propagated points:\n$$ \\hat{z} = \\sum_{i=0}^{2L} W_{m,i} \\mathcal{Z}_i = W_{m,0}\\mathcal{Z}_0 + W_{m,1}\\mathcal{Z}_1 + W_{m,2}\\mathcal{Z}_2 $$\n$$ \\hat{z} = (-5.25)(1.44) + (3.125)(1.96) + (3.125)(1.00) = -7.56 + 6.125 + 3.125 = 1.69 $$\n\nThe innovation covariance, $S_{zz}$, is the weighted covariance of the propagated points, plus the measurement noise covariance $R$:\n$$ S_{zz} = \\sum_{i=0}^{2L} W_{c,i} (\\mathcal{Z}_i - \\hat{z})(\\mathcal{Z}_i - \\hat{z})^\\top + R $$\n$$ S_{zz} = W_{c,0}(\\mathcal{Z}_0 - \\hat{z})^2 + W_{c,1}(\\mathcal{Z}_1 - \\hat{z})^2 + W_{c,2}(\\mathcal{Z}_2 - \\hat{z})^2 + R $$\n$$ S_{zz} = (-2.41)(1.44 - 1.69)^2 + (3.125)(1.96 - 1.69)^2 + (3.125)(1.00 - 1.69)^2 + 0.04 $$\n$$ S_{zz} = (-2.41)(-0.25)^2 + (3.125)(0.27)^2 + (3.125)(-0.69)^2 + 0.04 $$\n$$ S_{zz} = (-2.41)(0.0625) + (3.125)(0.0729) + (3.125)(0.4761) + 0.04 $$\n$$ S_{zz} = -0.150625 + 0.2278125 + 1.4878125 + 0.04 = 1.565 + 0.04 = 1.605 $$\n\nThe state-measurement cross-covariance, $S_{xz}$, is calculated as:\n$$ S_{xz} = \\sum_{i=0}^{2L} W_{c,i} (\\mathcal{X}_i - \\mu)(\\mathcal{Z}_i - \\hat{z})^\\top $$\n$$ S_{xz} = W_{c,0}(\\mathcal{X}_0 - \\mu)(\\mathcal{Z}_0 - \\hat{z}) + W_{c,1}(\\mathcal{X}_1 - \\mu)(\\mathcal{Z}_1 - \\hat{z}) + W_{c,2}(\\mathcal{X}_2 - \\mu)(\\mathcal{Z}_2 - \\hat{z}) $$\nSince $\\mathcal{X}_0 - \\mu = 0$, the first term is zero.\n$$ S_{xz} = (3.125)(1.4 - 1.2)(1.96 - 1.69) + (3.125)(1.0 - 1.2)(1.00 - 1.69) $$\n$$ S_{xz} = (3.125)(0.2)(0.27) + (3.125)(-0.2)(-0.69) $$\n$$ S_{xz} = 0.16875 + 0.43125 = 0.6 $$\n\n**3. State Update**\n\nThe Kalman gain, $K$, is computed:\n$$ K = S_{xz} S_{zz}^{-1} $$\n$$ K = \\frac{0.6}{1.605} \\approx 0.3738317757... $$\n\nThe posterior state mean, $\\mu^+$, is updated using the Kalman gain and the measurement innovation $(z - \\hat{z})$:\n$$ \\mu^+ = \\mu + K(z - \\hat{z}) $$\n$$ \\mu^+ = 1.2 + (\\frac{0.6}{1.605})(1.8 - 1.69) = 1.2 + (\\frac{0.6}{1.605})(0.11) $$\n$$ \\mu^+ = 1.2 + \\frac{0.066}{1.605} \\approx 1.2 + 0.0411214953... = 1.2411214953... $$\n\nThe posterior state covariance, $\\Sigma^+$, is updated:\n$$ \\Sigma^+ = \\Sigma - K S_{zz} K^\\top = \\Sigma - K S_{xz}^\\top $$\n$$ \\Sigma^+ = 0.25 - \\left(\\frac{0.6}{1.605}\\right) (1.605) \\left(\\frac{0.6}{1.605}\\right)^\\top = 0.25 - \\frac{(0.6)^2}{1.605} $$\n$$ \\Sigma^+ = 0.25 - \\frac{0.36}{1.605} \\approx 0.25 - 0.2242990654... = 0.0257009345... $$\n\nRounding the final results to six significant figures as requested:\n- Kalman gain $K \\approx 0.373832$\n- Posterior mean $\\mu^+ \\approx 1.24112$\n- Posterior covariance $\\Sigma^+ \\approx 0.0257009$",
            "answer": "$$ \\boxed{ \\begin{pmatrix} 0.373832 & 1.24112 & 0.0257009 \\end{pmatrix} } $$"
        },
        {
            "introduction": "In practical applications, estimates from different sources are often correlated, for instance, due to shared sensors or environmental factors. Naively fusing such estimates as if they were independent leads to dangerous overconfidence in the result. This exercise  exposes this critical issue by tasking you to derive the correct fusion formula when error cross-correlation is known, using the powerful information form of estimation. Completing this practice will solidify your understanding of how to properly account for statistical dependencies to achieve robust and reliable fusion.",
            "id": "4245591",
            "problem": "A Digital Twin (DT) of a mobile robot in a Cyber-Physical System (CPS) must fuse two scalar position estimates produced by two perception pipelines that partially share upstream data from a Light Detection and Ranging (LiDAR) sensor. Because the pipelines reuse features and preprocessing stages, the estimation errors are statistically correlated. Let the latent true scalar state be $x \\in \\mathbb{R}$. Each pipeline produces a measurement modeled as $z_1 = x + n_1$ and $z_2 = x + n_2$, where the noises $n_1$ and $n_2$ are jointly zero-mean Gaussian with covariance matrix\n$$\nR = \\begin{pmatrix}\n\\sigma_{1}^{2} & \\sigma_{12} \\\\\n\\sigma_{12} & \\sigma_{2}^{2}\n\\end{pmatrix}.\n$$\nIn many engineering stacks, a naive fusion treats $n_1$ and $n_2$ as independent, which is inappropriate here. Starting from the fundamental definitions of multivariate Gaussian inference for linear models and the information form of estimation, explain from first principles why ignoring the cross-correlation induces overconfident fused covariances. Then, using the joint information matrix approach, derive the corrected fused posterior covariance of $x$ when the cross-covariance $\\sigma_{12}$ is known.\n\nFinally, for a concrete instantiation, suppose the DT knows that $\\sigma_{1}^{2} = 0.04$ (square meters), $\\sigma_{2}^{2} = 0.09$ (square meters), and $\\sigma_{12} = 0.018$ (square meters). Compute the corrected fused posterior variance of $x$ obtained by properly accounting for correlation via the joint information matrix. Express the final variance in square meters and round your answer to five significant figures.",
            "solution": "We begin from the multivariate Gaussian linear measurement model and its information form. The state $x \\in \\mathbb{R}$ is measured by two channels\n$$\nz = \\begin{pmatrix} z_{1} \\\\ z_{2} \\end{pmatrix} = H x + n, \\quad H = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix},\n$$\nwhere $n \\sim \\mathcal{N}(0, R)$ and\n$$\nR = \\begin{pmatrix}\n\\sigma_{1}^{2} & \\sigma_{12} \\\\\n\\sigma_{12} & \\sigma_{2}^{2}\n\\end{pmatrix}.\n$$\nWe assume a noninformative (improper flat) prior on $x$ so that the posterior is driven solely by the likelihood. The log-likelihood is, up to an additive constant,\n$$\n\\ell(x) = -\\frac{1}{2} (z - H x)^{\\top} R^{-1} (z - H x).\n$$\nBy the standard information form, the posterior precision (information) for $x$ is\n$$\nJ = H^{\\top} R^{-1} H,\n$$\nand the posterior covariance is\n$$\nP = J^{-1}.\n$$\n\nWhy naive fusion is overconfident: If one incorrectly assumes independence, one sets the off-diagonal entries of $R$ to zero, i.e., $R_{\\text{naive}} = \\operatorname{diag}(\\sigma_{1}^{2}, \\sigma_{2}^{2})$. Then\n$$\nJ_{\\text{naive}} = H^{\\top} R_{\\text{naive}}^{-1} H = \\frac{1}{\\sigma_{1}^{2}} + \\frac{1}{\\sigma_{2}^{2}},\n$$\nwhich double counts the shared information content present in the correlated errors. In the correct model, the off-diagonal terms $\\sigma_{12}$ reduce the effective information because the joint inversion $R^{-1}$ downweights redundant measurements. Mathematically, $H^{\\top} R^{-1} H \\leq \\frac{1}{\\sigma_{1}^{2}} + \\frac{1}{\\sigma_{2}^{2}}$ under positive correlation (i.e., $\\sigma_{12} > 0$), yielding larger (less overconfident) variance $P = J^{-1}$ compared to the naive case. Intuitively, correlation implies that the two channels do not provide independent evidence; treating them as independent overestimates precision and underestimates uncertainty.\n\nDerivation of the corrected fused covariance using the joint information matrix: Compute $R^{-1}$ for the symmetric $2 \\times 2$ covariance,\n$$\nR = \\begin{pmatrix}\na & b \\\\\nb & c\n\\end{pmatrix}, \\quad a = \\sigma_{1}^{2}, \\quad b = \\sigma_{12}, \\quad c = \\sigma_{2}^{2}.\n$$\nIts inverse is\n$$\nR^{-1} = \\frac{1}{\\det(R)} \\begin{pmatrix}\nc & -b \\\\\n-b & a\n\\end{pmatrix}, \\quad \\det(R) = a c - b^{2}.\n$$\nTherefore,\n$$\nJ = H^{\\top} R^{-1} H = \\begin{pmatrix} 1 & 1 \\end{pmatrix} \\left( \\frac{1}{a c - b^{2}} \\begin{pmatrix}\nc & -b \\\\\n-b & a\n\\end{pmatrix} \\right) \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\frac{a + c - 2 b}{a c - b^{2}}.\n$$\nThus, the corrected fused covariance is\n$$\nP = J^{-1} = \\frac{a c - b^{2}}{a + c - 2 b} = \\frac{\\sigma_{1}^{2} \\sigma_{2}^{2} - \\sigma_{12}^{2}}{\\sigma_{1}^{2} + \\sigma_{2}^{2} - 2 \\sigma_{12}}.\n$$\n\nSubstitute the given values $\\sigma_{1}^{2} = 0.04$, $\\sigma_{2}^{2} = 0.09$, and $\\sigma_{12} = 0.018$:\n$$\na = 0.04, \\quad c = 0.09, \\quad b = 0.018.\n$$\nCompute the determinant term:\n$$\na c - b^{2} = (0.04)(0.09) - (0.018)^{2} = 0.0036 - 0.000324 = 0.003276.\n$$\nCompute the denominator:\n$$\na + c - 2 b = 0.04 + 0.09 - 2 \\times 0.018 = 0.13 - 0.036 = 0.094.\n$$\nTherefore, the corrected fused posterior variance is\n$$\nP = \\frac{0.003276}{0.094} = 0.0348510638297872\\ldots\n$$\nRounded to five significant figures,\n$$\nP \\approx 0.034851.\n$$\nThis value, expressed in square meters, is larger than the naive independent fusion variance\n$$\nP_{\\text{naive}} = \\left( \\frac{1}{0.04} + \\frac{1}{0.09} \\right)^{-1} = (25 + 11.\\overline{1})^{-1} \\approx 0.027682,\n$$\nconfirming that ignoring correlation leads to an overconfident (too small) covariance estimate. The joint information matrix correctly accounts for the redundancy via the off-diagonal covariance and yields a more realistic uncertainty.",
            "answer": "$$\\boxed{0.034851}$$"
        }
    ]
}