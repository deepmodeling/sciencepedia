{
    "hands_on_practices": [
        {
            "introduction": "Effective state estimation in cyber-physical systems begins with accurate modeling. Since Moving Horizon Estimation (MHE) often relies on gradient-based optimization solvers, a crucial first step is to linearize any nonlinear system models to compute the required Jacobians. This exercise  provides foundational practice in this skill, guiding you from the first principles of a sensor's physical operation to the derivation of the explicit Jacobians needed to incorporate it into an MHE framework.",
            "id": "4231913",
            "problem": "A cyber-physical mobile robot is represented in its digital twin by the state vector $x_{k} \\in \\mathbb{R}^{2}$ containing planar position $x_{k} = \\begin{pmatrix} p_{x,k} \\\\ p_{y,k} \\end{pmatrix}$. The robot carries a time-of-flight acoustic sensor that measures the distance to a stationary beacon located at a known position $s = \\begin{pmatrix} s_{x} \\\\ s_{y} \\end{pmatrix}$. The sensor is heated by a control input $u_{k} \\in \\mathbb{R}$ that modifies the local speed of sound according to $c(u_{k}) = c_{0}\\sqrt{1 + \\beta u_{k}}$, where $c_{0}  0$ and $\\beta  0$ are known constants determined by calibration. The electronics apply a logarithmic amplifier to the raw time-of-flight, producing a dimensionless measurement output $y_{k}$ defined by the nonlinear function $y_{k} = h(x_{k}, u_{k})$, where $h$ must be constructed from the following physically grounded relationships:\n- The geometric distance between the robot and beacon is $d(x_{k}) = \\| x_{k} - s \\|_{2}$.\n- The time-of-flight is $t(x_{k}, u_{k}) = \\dfrac{d(x_{k})}{c(u_{k})}$.\n- The amplifier output is $y_{k} = \\ln\\!\\big(1 + \\alpha\\, t(x_{k}, u_{k})\\big)$, where $\\alpha  0$ is a known dimensionless gain factor.\nStarting from these fundamentals, form the explicit measurement model $y_{k} = h(x_{k}, u_{k})$ and derive the Jacobians with respect to the state and input that are needed for Moving Horizon Estimation (MHE). Specifically, compute $\\dfrac{\\partial h}{\\partial x_{k}}$ and $\\dfrac{\\partial h}{\\partial u_{k}}$ using first principles and the chain rule, then evaluate them at the operating point given by $s = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$, $x_{k} = \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix}$, $u_{k} = 1$, $\\alpha = 3$, $c_{0} = 1$ (normalized internal units of the digital twin), and $\\beta = 3$.\nProvide the final result as the single row vector $\\begin{pmatrix} \\dfrac{\\partial h}{\\partial p_{x,k}}  \\dfrac{\\partial h}{\\partial p_{y,k}}  \\dfrac{\\partial h}{\\partial u_{k}} \\end{pmatrix}$ in exact fractional form. For interpretability, the state Jacobian entries are to be understood in inverse meters and the input Jacobian entry in inverse-control units; do not include units inside the final boxed answer.",
            "solution": "The problem requires the derivation and evaluation of the Jacobians of a nonlinear measurement function $h(x_{k}, u_{k})$ for a cyber-physical system, as is common in estimation algorithms like Moving Horizon Estimation (MHE) or the Extended Kalman Filter (EKF). The Jacobians are the matrices of first-order partial derivatives of the measurement function with respect to the state vector $x_k$ and the control input $u_k$.\n\nFirst, we must construct the explicit form of the measurement function $y_{k} = h(x_{k}, u_{k})$. The problem provides the following relationships:\nThe state vector is $x_{k} = \\begin{pmatrix} p_{x,k} \\\\ p_{y,k} \\end{pmatrix}$.\nThe beacon is at a known position $s = \\begin{pmatrix} s_{x} \\\\ s_{y} \\end{pmatrix}$.\nThe geometric distance is $d(x_{k}) = \\| x_{k} - s \\|_{2} = \\sqrt{(p_{x,k} - s_{x})^{2} + (p_{y,k} - s_{y})^{2}}$.\nThe speed of sound is dependent on a control input $u_k$ as $c(u_{k}) = c_{0}\\sqrt{1 + \\beta u_{k}}$.\nThe time-of-flight is $t(x_{k}, u_{k}) = \\dfrac{d(x_{k})}{c(u_{k})}$.\nThe final measurement is $y_{k} = \\ln\\!\\big(1 + \\alpha\\, t(x_{k}, u_{k})\\big)$.\n\nSubstituting these expressions into one another, we obtain the complete measurement model:\n$$h(x_{k}, u_{k}) = \\ln\\left(1 + \\alpha \\frac{\\sqrt{(p_{x,k} - s_{x})^{2} + (p_{y,k} - s_{y})^{2}}}{c_{0}\\sqrt{1 + \\beta u_{k}}}\\right)$$\n\nOur task is to compute the partial derivatives of $h$ with respect to each component of the state $x_k$ (which are $p_{x,k}$ and $p_{y,k}$) and the control input $u_k$. We will use the chain rule for differentiation.\n\nLet's compute the state Jacobian, $\\dfrac{\\partial h}{\\partial x_{k}} = \\begin{pmatrix} \\dfrac{\\partial h}{\\partial p_{x,k}}  \\dfrac{\\partial h}{\\partial p_{y,k}} \\end{pmatrix}$.\nLet $g(d, c) = \\ln(1 + \\alpha \\frac{d}{c})$. Then $h(x_k, u_k) = g(d(x_k), c(u_k))$.\nThe derivative with respect to an intermediate variable, say $z$, is $\\dfrac{\\partial h}{\\partial z} = \\dfrac{\\partial g}{\\partial d}\\dfrac{\\partial d}{\\partial z} + \\dfrac{\\partial g}{\\partial c}\\dfrac{\\partial c}{\\partial z}$.\nFor the state variables $p_{x,k}$ and $p_{y,k}$, the speed of sound $c$ is constant, so $\\dfrac{\\partial c}{\\partial p_{x,k}} = 0$ and $\\dfrac{\\partial c}{\\partial p_{y,k}} = 0$.\nThe derivatives are thus simplified:\n$\\dfrac{\\partial h}{\\partial p_{x,k}} = \\dfrac{\\partial g}{\\partial d}\\dfrac{\\partial d}{\\partial p_{x,k}}$ and $\\dfrac{\\partial h}{\\partial p_{y,k}} = \\dfrac{\\partial g}{\\partial d}\\dfrac{\\partial d}{\\partial p_{y,k}}$.\n\nFirst, we compute $\\dfrac{\\partial g}{\\partial d}$:\n$$\\frac{\\partial g}{\\partial d} = \\frac{1}{1 + \\alpha \\frac{d}{c}} \\cdot \\frac{\\alpha}{c} = \\frac{\\alpha}{c + \\alpha d}$$\nNext, we compute the derivatives of the distance function $d(x_k) = \\sqrt{(p_{x,k} - s_{x})^{2} + (p_{y,k} - s_{y})^{2}}$:\n$$\\frac{\\partial d}{\\partial p_{x,k}} = \\frac{1}{2d} \\cdot 2(p_{x,k} - s_{x}) = \\frac{p_{x,k} - s_{x}}{d}$$\n$$\\frac{\\partial d}{\\partial p_{y,k}} = \\frac{1}{2d} \\cdot 2(p_{y,k} - s_{y}) = \\frac{p_{y,k} - s_{y}}{d}$$\nCombining these results, we get the components of the state Jacobian:\n$$\\frac{\\partial h}{\\partial p_{x,k}} = \\frac{\\alpha}{c + \\alpha d} \\cdot \\frac{p_{x,k} - s_{x}}{d} = \\frac{\\alpha (p_{x,k} - s_{x})}{d(c + \\alpha d)}$$\n$$\\frac{\\partial h}{\\partial p_{y,k}} = \\frac{\\alpha}{c + \\alpha d} \\cdot \\frac{p_{y,k} - s_{y}}{d} = \\frac{\\alpha (p_{y,k} - s_{y})}{d(c + \\alpha d)}$$\nThe state Jacobian row vector is $\\dfrac{\\partial h}{\\partial x_{k}} = \\dfrac{\\alpha (x_k - s)^T}{d(c+\\alpha d)}$.\n\nNow, we compute the input Jacobian, $\\dfrac{\\partial h}{\\partial u_{k}}$.\nFor the input $u_k$, the distance $d$ is constant, so $\\dfrac{\\partial d}{\\partial u_k} = 0$. The derivative simplifies to:\n$\\dfrac{\\partial h}{\\partial u_{k}} = \\dfrac{\\partial g}{\\partial c}\\dfrac{\\partial c}{\\partial u_{k}}$.\nFirst, we compute $\\dfrac{\\partial g}{\\partial c}$:\n$$\\frac{\\partial g}{\\partial c} = \\frac{1}{1 + \\alpha \\frac{d}{c}} \\cdot \\left(-\\frac{\\alpha d}{c^2}\\right) = \\frac{c}{c + \\alpha d} \\cdot \\left(-\\frac{\\alpha d}{c^2}\\right) = -\\frac{\\alpha d}{c(c + \\alpha d)}$$\nNext, we compute the derivative of the sound speed function $c(u_k) = c_{0}(1 + \\beta u_{k})^{1/2}$:\n$$\\frac{\\partial c}{\\partial u_{k}} = c_{0} \\cdot \\frac{1}{2}(1 + \\beta u_{k})^{-1/2} \\cdot \\beta = \\frac{c_{0}\\beta}{2\\sqrt{1 + \\beta u_{k}}}$$\nWe can express this in terms of $c$ itself: since $c = c_{0}\\sqrt{1 + \\beta u_{k}}$, we have $\\sqrt{1 + \\beta u_{k}} = c/c_0$.\n$$\\frac{\\partial c}{\\partial u_{k}} = \\frac{c_{0}\\beta}{2(c/c_0)} = \\frac{c_{0}^{2}\\beta}{2c}$$\nCombining these results gives the input Jacobian:\n$$\\frac{\\partial h}{\\partial u_{k}} = \\left(-\\frac{\\alpha d}{c(c + \\alpha d)}\\right) \\cdot \\left(\\frac{c_{0}^{2}\\beta}{2c}\\right) = -\\frac{\\alpha d c_{0}^{2}\\beta}{2c^{2}(c + \\alpha d)}$$\n\nThe final step is to evaluate these Jacobian expressions at the given operating point:\n$s = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$, $x_{k} = \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix}$, $u_{k} = 1$, $\\alpha = 3$, $c_{0} = 1$, $\\beta = 3$.\n\nWe first calculate the intermediate values at this point:\nThe state components are $p_{x,k} = 3$ and $p_{y,k} = 4$.\nThe distance is $d = \\sqrt{(3-0)^2 + (4-0)^2} = \\sqrt{9+16} = \\sqrt{25} = 5$.\nThe speed of sound is $c = c_{0}\\sqrt{1 + \\beta u_{k}} = 1\\sqrt{1 + 3 \\cdot 1} = \\sqrt{4} = 2$.\nA useful denominator term is $c + \\alpha d = 2 + 3 \\cdot 5 = 2 + 15 = 17$.\n\nNow, we substitute these values into the Jacobian expressions.\nFor the state Jacobian:\n$$\\frac{\\partial h}{\\partial p_{x,k}} = \\frac{\\alpha (p_{x,k} - s_{x})}{d(c + \\alpha d)} = \\frac{3(3 - 0)}{5(17)} = \\frac{9}{85}$$\n$$\\frac{\\partial h}{\\partial p_{y,k}} = \\frac{\\alpha (p_{y,k} - s_{y})}{d(c + \\alpha d)} = \\frac{3(4 - 0)}{5(17)} = \\frac{12}{85}$$\nFor the input Jacobian:\n$$\\frac{\\partial h}{\\partial u_{k}} = -\\frac{\\alpha d c_{0}^{2}\\beta}{2c^{2}(c + \\alpha d)} = -\\frac{3 \\cdot 5 \\cdot 1^2 \\cdot 3}{2 \\cdot 2^2 \\cdot (17)} = -\\frac{45}{2 \\cdot 4 \\cdot 17} = -\\frac{45}{136}$$\nThe resulting Jacobian row vector is $\\begin{pmatrix} \\dfrac{9}{85}  \\dfrac{12}{85}  -\\dfrac{45}{136} \\end{pmatrix}$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{9}{85}  \\frac{12}{85}  -\\frac{45}{136} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "One of the primary advantages of MHE over traditional filters like the EKF is its native ability to handle hard constraints on states and measurements. This practice  delves into a common real-world challenge: sensor saturation. You will use probabilistic reasoning to derive a condition that signals when a measurement is likely to saturate, justifying the inclusion of inequality constraints in the MHE optimization problem to maintain estimation accuracy in the presence of this strong nonlinearity.",
            "id": "4231959",
            "problem": "A discrete-time linear cyber-physical system, represented in a Digital Twin, evolves according to $x_{k+1} = A x_{k} + w_{k}$, where $x_{k} \\in \\mathbb{R}^{n}$ is the state, $A \\in \\mathbb{R}^{n \\times n}$ is the system matrix, and $w_{k}$ is a process disturbance modeled as a zero-mean Gaussian with covariance $Q \\succ 0$. The sensor reports a measured output $z_{k}$ obtained by saturating the raw signal $y_{k} = C x_{k} + v_{k}$ with $C \\in \\mathbb{R}^{m \\times n}$, where $v_{k}$ is a zero-mean Gaussian measurement noise with covariance $R \\succ 0$, and the saturation function is defined component-wise by\n$$\n\\operatorname{sat}(y) = \\begin{cases}\nL,  y \\leq L, \\\\\ny,  L  y  U, \\\\\nU,  y \\geq U,\n\\end{cases}\n$$\nfor fixed bounds $L  U$ applied to each scalar measurement channel. A Moving Horizon Estimation (MHE) problem is posed over a horizon of length $N$ with a quadratic arrival cost on the initial state of the horizon, quadratic process penalties, and quadratic measurement penalties, but no inequality constraints unless required by sensor physics. In practice, measurement saturation induces a piecewise-affine and non-differentiable measurement mapping, which can bias or destabilize unconstrained quadratic MHE if the predicted measurements are likely to be clipped at $L$ or $U$. To decide whether inequality constraints consistent with $L \\leq C x_{k} \\leq U$ must be included, consider the one-step-ahead predicted distribution of the scalar raw measurement $y_{k}$ at a candidate prior $\\hat{x}_{k \\mid k-1}$ with covariance $P_{k \\mid k-1} \\succeq 0$. Let the predicted raw measurement be modeled as Gaussian: $y_{k} \\sim \\mathcal{N}(\\mu, \\sigma^{2})$, where $\\mu = C \\hat{x}_{k \\mid k-1}$ and $\\sigma^{2} = C P_{k \\mid k-1} C^{\\top} + R$ for a single measurement channel. Define the saturation probability $p_{\\mathrm{sat}}(\\mu,\\sigma)$ as the probability that $y_{k}$ falls outside the interval $[L, U]$. In the absence of constraints, the MHE retains a purely quadratic least-squares structure; the system architect chooses a threshold $p_{\\ast} \\in (0,1)$ and includes constraints if $p_{\\mathrm{sat}}(\\mu,\\sigma) \\geq p_{\\ast}$. Starting from fundamental probabilistic definitions and the Gaussian model for $y_{k}$, derive the closed-form analytic expression for $p_{\\mathrm{sat}}(\\mu,\\sigma)$ in terms of the standard normal cumulative distribution function. Your final answer must be a single closed-form expression and must not be an inequality. No numerical approximation is required.",
            "solution": "The problem statement is evaluated for validity before attempting a solution.\n\n**Problem Validation**\n\n**Step 1: Extracted Givens**\n-   System model: $x_{k+1} = A x_{k} + w_{k}$\n-   State vector: $x_{k} \\in \\mathbb{R}^{n}$\n-   System matrix: $A \\in \\mathbb{R}^{n \\times n}$\n-   Process disturbance: $w_{k} \\sim \\mathcal{N}(0, Q)$, with covariance $Q \\succ 0$\n-   Raw measurement signal: $y_{k} = C x_{k} + v_{k}$\n-   Output matrix: $C \\in \\mathbb{R}^{m \\times n}$\n-   Measurement noise: $v_{k} \\sim \\mathcal{N}(0, R)$, with covariance $R \\succ 0$\n-   Saturated measurement: $z_{k} = \\operatorname{sat}(y_{k})$\n-   Saturation function (component-wise for a scalar $y$):\n    $$\n    \\operatorname{sat}(y) = \\begin{cases}\n    L,  y \\leq L, \\\\\n    y,  L  y  U, \\\\\n    U,  y \\geq U,\n    \\end{cases}\n    $$\n-   Saturation bounds: $L  U$\n-   Predicted state prior: $\\hat{x}_{k \\mid k-1}$ with covariance $P_{k \\mid k-1} \\succeq 0$\n-   Predicted raw measurement distribution (for a single scalar channel): $y_{k} \\sim \\mathcal{N}(\\mu, \\sigma^{2})$\n-   Mean of predicted measurement: $\\mu = C \\hat{x}_{k \\mid k-1}$\n-   Variance of predicted measurement: $\\sigma^{2} = C P_{k \\mid k-1} C^{\\top} + R$\n-   Definition of saturation probability: $p_{\\mathrm{sat}}(\\mu,\\sigma)$ is the probability that $y_{k}$ falls outside the interval $[L, U]$.\n-   Objective: Derive the closed-form expression for $p_{\\mathrm{sat}}(\\mu,\\sigma)$ in terms of the standard normal cumulative distribution function.\n\n**Step 2: Validation Using Extracted Givens**\nThe problem is scientifically grounded, well-posed, and objective. It poses a standard question from probability theory within the well-established context of state estimation for linear systems, specifically Moving Horizon Estimation (MHE). The models for the system dynamics, noise (Gaussian), and sensor non-linearity (saturation) are standard in control and estimation theory. The problem is self-contained, providing all necessary definitions and variables to derive the requested expression. There are no internal contradictions, ambiguities, or scientifically unsound premises.\n\n**Step 3: Verdict and Action**\nThe problem is **valid**. A solution will be derived.\n\n**Derivation of the Saturation Probability**\n\nThe objective is to find a closed-form expression for the saturation probability, $p_{\\mathrm{sat}}(\\mu,\\sigma)$. This is defined as the probability that the random variable $y_{k}$ falls outside the linear region of the saturation function, which is the interval $[L, U]$. The random variable $y_{k}$, representing the raw measurement for a single channel, is modeled as a Gaussian distribution with mean $\\mu$ and variance $\\sigma^2$, denoted as $y_{k} \\sim \\mathcal{N}(\\mu, \\sigma^{2})$.\n\nThe event that $y_{k}$ falls outside the interval $[L, U]$ can be expressed as the union of two disjoint events: $\\{y_{k} \\leq L\\}$ and $\\{y_{k} \\geq U\\}$. Since these events are mutually exclusive (as $L  U$), the probability of their union is the sum of their individual probabilities:\n$$\np_{\\mathrm{sat}}(\\mu,\\sigma) = P(y_{k} \\leq L \\text{ or } y_{k} \\geq U) = P(y_{k} \\leq L) + P(y_{k} \\geq U)\n$$\nTo evaluate these probabilities, we utilize the cumulative distribution function (CDF) of the normal distribution. The problem requires the final expression to be in terms of the standard normal CDF. Let $Z$ be a standard normal random variable, i.e., $Z \\sim \\mathcal{N}(0, 1)$. Its CDF is denoted by $\\Phi(z)$, where:\n$$\n\\Phi(z) = P(Z \\leq z) = \\int_{-\\infty}^{z} \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{t^2}{2}\\right) dt\n$$\nWe can standardize our random variable $y_{k}$ by subtracting its mean $\\mu$ and dividing by its standard deviation $\\sigma$. The resulting variable, $\\frac{y_{k} - \\mu}{\\sigma}$, follows the standard normal distribution $\\mathcal{N}(0, 1)$.\n\nLet us first evaluate the term $P(y_{k} \\leq L)$. By standardizing the variable, we get:\n$$\nP(y_{k} \\leq L) = P\\left(\\frac{y_{k} - \\mu}{\\sigma} \\leq \\frac{L - \\mu}{\\sigma}\\right)\n$$\nSince $\\frac{y_{k} - \\mu}{\\sigma}$ is a standard normal variable, this probability is given directly by the standard normal CDF:\n$$\nP(y_{k} \\leq L) = \\Phi\\left(\\frac{L - \\mu}{\\sigma}\\right)\n$$\nNext, let us evaluate the term $P(y_{k} \\geq U)$. We can express this using the complement event:\n$$\nP(y_{k} \\geq U) = 1 - P(y_{k}  U)\n$$\nSince the normal distribution is continuous, the probability of being less than a value is equal to the probability of being less than or equal to that value, i.e., $P(y_{k}  U) = P(y_{k} \\leq U)$. Therefore:\n$$\nP(y_{k} \\geq U) = 1 - P(y_{k} \\leq U)\n$$\nSimilar to the previous step, we standardize the variable inside the probability expression:\n$$\nP(y_{k} \\leq U) = P\\left(\\frac{y_{k} - \\mu}{\\sigma} \\leq \\frac{U - \\mu}{\\sigma}\\right) = \\Phi\\left(\\frac{U - \\mu}{\\sigma}\\right)\n$$\nSubstituting this back, we find the probability of the upper tail:\n$$\nP(y_{k} \\geq U) = 1 - \\Phi\\left(\\frac{U - \\mu}{\\sigma}\\right)\n$$\nFinally, we substitute the expressions for $P(y_{k} \\leq L)$ and $P(y_{k} \\geq U)$ into the equation for $p_{\\mathrm{sat}}(\\mu,\\sigma)$:\n$$\np_{\\mathrm{sat}}(\\mu,\\sigma) = \\Phi\\left(\\frac{L - \\mu}{\\sigma}\\right) + \\left[1 - \\Phi\\left(\\frac{U - \\mu}{\\sigma}\\right)\\right]\n$$\nRearranging the terms gives the final closed-form expression for the saturation probability:\n$$\np_{\\mathrm{sat}}(\\mu,\\sigma) = 1 + \\Phi\\left(\\frac{L - \\mu}{\\sigma}\\right) - \\Phi\\left(\\frac{U - \\mu}{\\sigma}\\right)\n$$\nThis expression depends only on the parameters of the predicted measurement distribution $(\\mu, \\sigma)$ and the saturation bounds $(L, U)$, as required. The function $\\Phi$ represents the standard normal cumulative distribution function.",
            "answer": "$$\n\\boxed{1 + \\Phi\\left(\\frac{L - \\mu}{\\sigma}\\right) - \\Phi\\left(\\frac{U - \\mu}{\\sigma}\\right)}\n$$"
        },
        {
            "introduction": "This final practice exercise synthesizes modeling, optimization, and implementation into a complete solution for a complex, realistic scenario. You will develop a Moving Horizon Estimator for a hybrid cyber-physical systemâ€”a grid-tied power inverter whose behavior depends on discrete switching modes. By enumerating potential mode sequences and solving a corresponding least-squares problem for each, this practice  demonstrates how MHE can effectively tackle mixed-integer estimation problems that are common in modern power electronics and other switched systems.",
            "id": "4232050",
            "problem": "Consider a digital twin (DT) of a single-phase grid-tied inverter in the broader context of Cyber-Physical Systems (CPS). The DT must perform Moving Horizon Estimation (MHE) to estimate the inductor current state while accounting for hybrid switching that induces mode-dependent measurement behavior. The inverter output stage is modeled as an $L$-$R$ filter driven by a direct-current (DC) voltage source and connected to an alternating-current (AC) grid that is well approximated as a Thevenin source over a short horizon.\n\nStart from fundamental electrical laws. The inductor obeys $v_L = L \\, \\frac{di}{dt}$, and by Kirchhoff's Voltage Law, $u - R \\, i - v_g - v_L = 0$, where $u$ is the inverter-side applied voltage, $R$ is the series resistance, $L$ is the inductance, $i$ is the inductor current, and $v_g$ is the grid voltage. Assume pulse-width modulation produces an average applied voltage $u = s \\, V_{\\mathrm{dc}}$, where $s \\in \\{-1, 0, +1\\}$ is the discrete switching mode and $V_{\\mathrm{dc}}$ is the DC bus voltage. Discretize the current dynamics with a forward Euler method with step $dt$ to obtain a discrete-time model suitable for MHE.\n\nLet the state be $x_k = i_k$ at discrete time $k$. Assume the DT receives measurements $y_k$ that depend on the switching mode via a mode-dependent sensor scaling and bias, so that $y_k = h_{s_k} \\, x_k + c_{s_k} + v_k$, where $h_{s_k}$ and $c_{s_k}$ are the measurement scale and bias determined by the current mode $s_k$, and $v_k$ is additive measurement noise.\n\nAssume additive process noise $w_k$ in the dynamics and additive measurement noise $v_k$ are independent, zero-mean Gaussian with known variances. The MHE must operate over a horizon of length $N$ with an arrival prior $x_0 \\sim \\mathcal{N}(\\bar{x}_0, \\sigma_P^2)$ to regularize the initial state. The objective is to jointly estimate the state trajectory $x_0, x_1, \\dots, x_N$ and the discrete mode sequence $s_0, s_1, \\dots, s_{N-1}$ by minimizing the negative log-likelihood implied by the Gaussian assumptions subject to the discretized dynamics and measurement models.\n\nFormally, derive the discrete dynamics from the laws above and assemble the MHE cost from first principles, then implement an algorithm that:\n- Enumerates all candidate hybrid mode sequences $\\{s_k\\}_{k=0}^{N-1}$ with $s_k \\in \\{-1, 0, +1\\}$ over the horizon.\n- For each candidate mode sequence, solves the weighted least-squares problem in the continuous variables $\\{x_k\\}_{k=0}^{N}$ that balances:\n  - The arrival regularization at $k=0$ using the prior.\n  - The process residuals defined by the discretized dynamics.\n  - The measurement residuals defined by the mode-dependent measurement model.\n- Selects the mode sequence and state trajectory that minimize the total cost.\n\nYour program must generate synthetic data for each test case by simulating the true dynamics for a specified true mode sequence and adding measurement noise. You may assume process noise is negligible in the data generation over the short horizon but must include the process residual weighting in the estimator. Use units consistently:\n- Inductance $L$ in henries (H), resistance $R$ in ohms ($\\Omega$), voltages $V_{\\mathrm{dc}}$ and $v_g$ in volts (V), time step $dt$ in seconds (s), and current $i$ in amperes (A).\n- Express the final current estimation error in amperes (A). No angle quantities are required.\n- All error outputs must be floats; mode misclassifications must be integers.\n\nDiscrete-time model to derive and use in the estimator:\n- Begin from $L \\, \\frac{di}{dt} = u - R \\, i - v_g$ with $u = s \\, V_{\\mathrm{dc}}$.\n- Use forward Euler with step $dt$ to obtain $x_{k+1}$ in terms of $x_k$, $s_k$, $V_{\\mathrm{dc}}$, $v_{g,k}$, $R$, and $L$.\n\nMeasurement model to use in the estimator:\n- $y_k = h_{s_k} \\, x_k + c_{s_k} + v_k$ with known $h_{s}$ and $c_s$ for each $s \\in \\{-1, 0, +1\\}$.\n\nNoise assumptions for the estimator:\n- $w_k \\sim \\mathcal{N}(0, \\sigma_Q^2)$ and $v_k \\sim \\mathcal{N}(0, \\sigma_R^2)$, with weights in the least-squares cost proportional to $1/\\sigma_Q^2$ and $1/\\sigma_R^2$, respectively.\n- Arrival prior $x_0 \\sim \\mathcal{N}(\\bar{x}_0, \\sigma_P^2)$ contributes a weight proportional to $1/\\sigma_P^2$.\n\nTest Suite:\nImplement the MHE and evaluate it on the following three test cases. For each case, generate synthetic measurements $y_k$ using the true mode sequence, simulate the true current using the discretized dynamics without process noise, and add independent Gaussian measurement noise with the specified standard deviation.\n\n- Case 1 (happy path):\n  - $L = 3 \\times 10^{-3}$ H, $R = 0.4$ $\\Omega$, $V_{\\mathrm{dc}} = 400$ V, $dt = 1 \\times 10^{-4}$ s, $N = 5$.\n  - Grid voltage $v_{g,k} = 170$ V for all $k$.\n  - True initial state $x_0^{\\mathrm{true}} = 5$ A; arrival prior mean $\\bar{x}_0 = 0$ A; arrival standard deviation $\\sigma_P = 3$ A.\n  - True mode sequence $(s_k)_{k=0}^{4} = [ +1, -1, +1, -1, +1 ]$.\n  - Measurement parameters: $h_{+1} = 1.0$, $h_{0} = 0.7$, $h_{-1} = 0.9$; $c_{+1} = 0.02$ A, $c_{0} = 0.0$ A, $c_{-1} = -0.02$ A.\n  - Estimator weights: $\\sigma_Q = 0.5$ A, $\\sigma_R = 0.05$ A, $\\sigma_P = 3$ A.\n  - Measurement noise standard deviation $\\sigma_{\\mathrm{meas}} = 0.05$ A.\n\n- Case 2 (includes zero-conduction mode and mild noise):\n  - $L = 2 \\times 10^{-3}$ H, $R = 0.3$ $\\Omega$, $V_{\\mathrm{dc}} = 350$ V, $dt = 1 \\times 10^{-4}$ s, $N = 5$.\n  - Grid voltage $v_{g,k} = 170$ V for all $k$.\n  - True initial state $x_0^{\\mathrm{true}} = 4$ A; arrival prior mean $\\bar{x}_0 = 6$ A; arrival standard deviation $\\sigma_P = 2$ A.\n  - True mode sequence $(s_k)_{k=0}^{4} = [ +1, 0, 0, -1, +1 ]$.\n  - Measurement parameters: $h_{+1} = 1.0$, $h_{0} = 0.7$, $h_{-1} = 0.9$; $c_{+1} = 0.02$ A, $c_{0} = 0.0$ A, $c_{-1} = -0.02$ A.\n  - Estimator weights: $\\sigma_Q = 0.5$ A, $\\sigma_R = 0.02$ A, $\\sigma_P = 2$ A.\n  - Measurement noise standard deviation $\\sigma_{\\mathrm{meas}} = 0.02$ A.\n\n- Case 3 (poor prior and low noise):\n  - $L = 4 \\times 10^{-3}$ H, $R = 0.5$ $\\Omega$, $V_{\\mathrm{dc}} = 450$ V, $dt = 2 \\times 10^{-4}$ s, $N = 5$.\n  - Grid voltage $v_{g,k} = 170$ V for all $k$.\n  - True initial state $x_0^{\\mathrm{true}} = 2$ A; arrival prior mean $\\bar{x}_0 = 8$ A; arrival standard deviation $\\sigma_P = 4$ A.\n  - True mode sequence $(s_k)_{k=0}^{4} = [ -1, -1, +1, +1, -1 ]$.\n  - Measurement parameters: $h_{+1} = 1.0$, $h_{0} = 0.7$, $h_{-1} = 0.9$; $c_{+1} = 0.02$ A, $c_{0} = 0.0$ A, $c_{-1} = -0.02$ A.\n  - Estimator weights: $\\sigma_Q = 0.5$ A, $\\sigma_R = 0.01$ A, $\\sigma_P = 4$ A.\n  - Measurement noise standard deviation $\\sigma_{\\mathrm{meas}} = 0.01$ A.\n\nYour program must:\n- Implement the estimator as described and select the mode sequence and state trajectory minimizing the cost over the horizon.\n- For each case, compute and return two quantities:\n  1. The root-mean-square (RMS) current estimation error over the entire horizon, including $k=0$ to $k=N$, in amperes (A), as a float.\n  2. The total number of mode misclassifications over the horizon, comparing the estimated discrete mode sequence to the true sequence, as an integer.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3]\").\n- Aggregate the results for the three cases in order and flatten them as \"[\\mathrm{err}_1,\\mathrm{mis}_1,\\mathrm{err}_2,\\mathrm{mis}_2,\\mathrm{err}_3,\\mathrm{mis}_3]\".",
            "solution": "The posed problem is valid as it is scientifically grounded in electrical engineering and control theory, well-posed, objective, and contains all necessary information for a unique solution. We are tasked with designing a Moving Horizon Estimator (MHE) for a hybrid system, specifically a grid-tied inverter, to jointly estimate the continuous inductor current state and the discrete switching mode.\n\n### 1. Derivation of the Discrete-Time System Dynamics\n\nThe continuous-time model for the inductor current $i(t)$ in the $L$-$R$ filter is derived from Kirchhoff's Voltage Law:\n$$L \\frac{di}{dt} = u - R i - v_g$$\nThe inverter's output voltage $u$ is determined by the switching mode $s \\in \\{-1, 0, +1\\}$ and the DC bus voltage $V_{\\mathrm{dc}}$, such that $u = s V_{\\mathrm{dc}}$. Substituting this into the dynamic equation yields:\n$$ \\frac{di}{dt} = -\\frac{R}{L}i + \\frac{sV_{\\mathrm{dc}} - v_g}{L} $$\nTo formulate a model suitable for a discrete-time estimator, we discretize this differential equation using the forward Euler method with a time step $dt$. Let $x_k = i(k \\cdot dt)$ be the state at discrete time $k$. The derivative is approximated as $\\frac{di}{dt} \\approx \\frac{x_{k+1} - x_k}{dt}$.\n$$ \\frac{x_{k+1} - x_k}{dt} = -\\frac{R}{L}x_k + \\frac{s_k V_{\\mathrm{dc}} - v_{g,k}}{L} $$\nRearranging to solve for $x_{k+1}$ gives the discrete-time state-space model:\n$$ x_{k+1} = \\left(1 - \\frac{R \\cdot dt}{L}\\right) x_k + \\frac{dt}{L} (s_k V_{\\mathrm{dc}} - v_{g,k}) $$\nThis equation is of the form $x_{k+1} = f(x_k, s_k)$, where $s_k$ is the control input (mode) applied during the interval $[k \\cdot dt, (k+1) \\cdot dt)$. The estimator must account for process noise $w_k \\sim \\mathcal{N}(0, \\sigma_Q^2)$, leading to the stochastic model:\n$$ x_{k+1} = f(x_k, s_k) + w_k $$\n\n### 2. MHE Cost Function Formulation\n\nThe objective is to find the state trajectory $X = \\{x_0, x_1, \\dots, x_N\\}$ and the mode sequence $S = \\{s_0, s_1, \\dots, s_{N-1}\\}$ that best explain the available measurements over a horizon of length $N$. Under the assumption of independent, zero-mean Gaussian noise, the maximum likelihood estimate is found by minimizing a weighted sum of squared residuals, which is equivalent to minimizing the negative log-likelihood. The total cost function $J(X, S)$ is composed of three parts:\n\n1.  **Arrival Cost ($J_P$)**: This term incorporates prior knowledge about the initial state $x_0 \\sim \\mathcal{N}(\\bar{x}_0, \\sigma_P^2)$. It penalizes the deviation of the estimated initial state $x_0$ from its prior mean $\\bar{x}_0$.\n    $$ J_P(x_0) = \\frac{(x_0 - \\bar{x}_0)^2}{\\sigma_P^2} $$\n\n2.  **Process Cost ($J_Q$)**: This term penalizes the process residuals, which represent the discrepancy between the estimated state trajectory and the dynamics predicted by the model. The process residual at step $k$ is $w_k = x_{k+1} - f(x_k, s_k)$.\n    $$ J_Q(X, S) = \\sum_{k=0}^{N-1} \\frac{w_k^2}{\\sigma_Q^2} = \\sum_{k=0}^{N-1} \\frac{\\left(x_{k+1} - f(x_k, s_k)\\right)^2}{\\sigma_Q^2} $$\n\n3.  **Measurement Cost ($J_R$)**: This term penalizes the measurement residuals, which are the differences between the actual measurements $y_k$ and the values predicted by the measurement model $y_k = h_{s_k} x_k + c_{s_k} + v_k$, where $v_k \\sim \\mathcal{N}(0, \\sigma_R^2)$. We assume measurements $y_k$ are available for $k=0, \\dots, N-1$.\n    $$ J_R(X, S) = \\sum_{k=0}^{N-1} \\frac{(y_k - (h_{s_k} x_k + c_{s_k}))^2}{\\sigma_R^2} $$\n\nThe total MHE cost to be minimized is the sum of these components:\n$$ \\min_{X, S} J(X, S) = \\min_{X, S} \\left( J_P(x_0) + J_Q(X, S) + J_R(X, S) \\right) $$\n\n### 3. Solution Algorithm\n\nThis is a mixed-integer optimization problem due to the continuous states $X$ and discrete modes $S$. Since the horizon $N$ is small ($N=5$), we can solve this by enumerating all possible mode sequences. For $s_k \\in \\{-1, 0, +1\\}$, there are $3^N$ candidate sequences.\n\nFor each fixed candidate mode sequence $S$, the cost function $J(X | S)$ is a quadratic function of the state vector $X = [x_0, x_1, \\dots, x_N]^T$. Minimizing this quadratic cost is a linear least-squares problem. We can structure this in the standard form $\\min_X \\| \\mathbf{M}X - \\mathbf{d} \\|_2^2$.\n\nThe vector of variables is $X \\in \\mathbb{R}^{N+1}$. The system is constructed from the residuals:\n-   **Prior residual**: $\\sqrt{W_P}(x_0 - \\bar{x}_0)$, where $W_P = 1/\\sigma_P^2$.\n-   **Measurement residuals ($k=0..N-1$):** $\\sqrt{W_R}(y_k - (h_{s_k} x_k + c_{s_k}))$, where $W_R = 1/\\sigma_R^2$. This is rewritten as $\\sqrt{W_R}(h_{s_k} x_k - (y_k - c_{s_k}))$.\n-   **Process residuals ($k=0..N-1$):** $\\sqrt{W_Q}(x_{k+1} - f(x_k, s_k))$, where $W_Q = 1/\\sigma_Q^2$. Let $f(x_k, s_k)=A x_k + B_k$, where $A = (1 - \\frac{R \\cdot dt}{L})$ and $B_k = \\frac{dt}{L}(s_k V_{\\mathrm{dc}} - v_{g,k})$. The residual is $\\sqrt{W_Q}(x_{k+1} - A x_k - B_k)$.\n\nThese $1+N+N=2N+1$ residuals are stacked to form the linear system $\\mathbf{M}X \\approx \\mathbf{d}$, where $\\mathbf{M}$ is a $(2N+1) \\times (N+1)$ matrix and $\\mathbf{d}$ is a $(2N+1)$-dimensional vector. This system is solved for $X$ using a standard least-squares solver. The minimum cost for the sequence $S$ is the resulting sum of squared residuals.\n\nThe overall algorithm is as follows:\n1.  Initialize $\\text{min\\_cost} = \\infty$.\n2.  Generate all $3^N$ candidate mode sequences $S$.\n3.  For each sequence $S$:\n    a. Construct the matrix $\\mathbf{M}$ and vector $\\mathbf{d}$ corresponding to $S$.\n    b. Solve for the optimal state trajectory $X_S = \\arg\\min_X \\| \\mathbf{M}X - \\mathbf{d} \\|_2^2$.\n    c. Calculate the cost $J_S = \\| \\mathbf{M}X_S - \\mathbf{d} \\|_2^2$.\n    d. If $J_S  \\text{min\\_cost}$, update $\\text{min\\_cost} = J_S$, and store $S$ and $X_S$ as the best-so-far solution.\n4.  The final estimate is the state and mode pair $(X_S, S)$ that achieved the overall minimum cost.\n\n### 4. Data Synthesis and Evaluation\n\nFor each test case, synthetic data is generated. A true state trajectory $\\{x_k^{\\mathrm{true}}\\}_{k=0}^{N}$ is created by simulating the deterministic discrete-time model with the given true mode sequence. Then, measurement data $\\{y_k\\}_{k=0}^{N-1}$ is generated by applying the mode-dependent measurement model to the true states and adding independent Gaussian noise with the specified standard deviation $\\sigma_{\\mathrm{meas}}$. The estimator's performance is quantified by the RMS current estimation error over the full horizon $[0, N]$ and the total number of misclassified modes over $[0, N-1]$.",
            "answer": "```python\nimport numpy as np\nimport itertools\n\ndef solve():\n    \"\"\"\n    Main function to define test cases, run MHE for each, and print results.\n    \"\"\"\n    # Set a fixed random seed for reproducibility of noise generation.\n    np.random.seed(42)\n\n    # Define the three test cases as specified in the problem.\n    test_cases = [\n        {\n            \"name\": \"Case 1 (happy path)\",\n            \"L\": 3e-3, \"R\": 0.4, \"Vdc\": 400.0, \"dt\": 1e-4, \"N\": 5,\n            \"vg\": 170.0,\n            \"x0_true\": 5.0, \"x0_prior_mean\": 0.0,\n            \"s_true\": [+1, -1, +1, -1, +1],\n            \"h_map\": {+1: 1.0, 0: 0.7, -1: 0.9},\n            \"c_map\": {+1: 0.02, 0: 0.0, -1: -0.02},\n            \"sigma_P\": 3.0, \"sigma_Q\": 0.5, \"sigma_R\": 0.05,\n            \"sigma_meas\": 0.05,\n        },\n        {\n            \"name\": \"Case 2 (includes zero-conduction mode)\",\n            \"L\": 2e-3, \"R\": 0.3, \"Vdc\": 350.0, \"dt\": 1e-4, \"N\": 5,\n            \"vg\": 170.0,\n            \"x0_true\": 4.0, \"x0_prior_mean\": 6.0,\n            \"s_true\": [+1, 0, 0, -1, +1],\n            \"h_map\": {+1: 1.0, 0: 0.7, -1: 0.9},\n            \"c_map\": {+1: 0.02, 0: 0.0, -1: -0.02},\n            \"sigma_P\": 2.0, \"sigma_Q\": 0.5, \"sigma_R\": 0.02,\n            \"sigma_meas\": 0.02,\n        },\n        {\n            \"name\": \"Case 3 (poor prior and low noise)\",\n            \"L\": 4e-3, \"R\": 0.5, \"Vdc\": 450.0, \"dt\": 2e-4, \"N\": 5,\n            \"vg\": 170.0,\n            \"x0_true\": 2.0, \"x0_prior_mean\": 8.0,\n            \"s_true\": [-1, -1, +1, +1, -1],\n            \"h_map\": {+1: 1.0, 0: 0.7, -1: 0.9},\n            \"c_map\": {+1: 0.02, 0: 0.0, -1: -0.02},\n            \"sigma_P\": 4.0, \"sigma_Q\": 0.5, \"sigma_R\": 0.01,\n            \"sigma_meas\": 0.01,\n        }\n    ]\n\n    results = []\n    for params in test_cases:\n        rms_error, misclassifications = run_mhe_for_case(params)\n        results.extend([rms_error, misclassifications])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef run_mhe_for_case(params):\n    \"\"\"\n    Solves the MHE problem for a single test case.\n    \n    Args:\n        params (dict): A dictionary containing all parameters for the case.\n        \n    Returns:\n        tuple: A tuple containing (rms_error, misclassifications).\n    \"\"\"\n    # Unpack parameters\n    L, R, Vdc, dt, N = params['L'], params['R'], params['Vdc'], params['dt'], params['N']\n    vg = params['vg']\n    x0_true = params['x0_true']\n    x0_prior_mean = params['x0_prior_mean']\n    sigma_P, sigma_Q, sigma_R = params['sigma_P'], params['sigma_Q'], params['sigma_R']\n    s_true = params['s_true']\n    h_map, c_map = params['h_map'], params['c_map']\n    sigma_meas = params['sigma_meas']\n\n    # --- 1. Generate True Data and Synthetic Measurements ---\n    \n    # Dynamics matrix A for discretization\n    A_dyn = 1.0 - (R * dt) / L\n    \n    # Generate true state trajectory (without process noise)\n    x_true = np.zeros(N + 1)\n    x_true[0] = x0_true\n    for k in range(N):\n        s_k = s_true[k]\n        B_dyn_k = (dt / L) * (s_k * Vdc - vg)\n        x_true[k + 1] = A_dyn * x_true[k] + B_dyn_k\n\n    # Generate synthetic measurements y_0, ..., y_{N-1}\n    y_meas = np.zeros(N)\n    for k in range(N):\n        s_k = s_true[k]\n        noise = np.random.normal(0, sigma_meas)\n        y_meas[k] = h_map[s_k] * x_true[k] + c_map[s_k] + noise\n\n    # --- 2. MHE Implementation ---\n    \n    modes = [-1, 0, 1]\n    mode_sequences = list(itertools.product(modes, repeat=N))\n\n    min_cost = float('inf')\n    best_x_est, best_s_est = None, None\n\n    # Pre-calculate weights for the cost function\n    W_P = 1.0 / (sigma_P**2)\n    W_Q = 1.0 / (sigma_Q**2)\n    W_R = 1.0 / (sigma_R**2)\n\n    # Least squares matrix dimensions\n    num_states = N + 1\n    num_residuals = 1 + N + N  # 1 prior, N dynamics, N measurements\n    \n    for s_candidate in mode_sequences:\n        # Construct the least-squares problem M*x = d for the current s_candidate\n        M = np.zeros((num_residuals, num_states))\n        d = np.zeros(num_residuals)\n        \n        row = 0\n\n        # Row for arrival cost (prior)\n        M[row, 0] = np.sqrt(W_P)\n        d[row] = np.sqrt(W_P) * x0_prior_mean\n        row += 1\n\n        # Rows for measurement cost\n        for k in range(N):\n            s_k = s_candidate[k]\n            h_k, c_k = h_map[s_k], c_map[s_k]\n            M[row, k] = np.sqrt(W_R) * h_k\n            d[row] = np.sqrt(W_R) * (y_meas[k] - c_k)\n            row += 1\n\n        # Rows for process cost (dynamics)\n        for k in range(N):\n            s_k = s_candidate[k]\n            B_dyn_k = (dt / L) * (s_k * Vdc - vg)\n            M[row, k] = -np.sqrt(W_Q) * A_dyn\n            M[row, k + 1] = np.sqrt(W_Q)\n            d[row] = np.sqrt(W_Q) * B_dyn_k\n            row += 1\n\n        # Solve the linear least squares problem\n        x_est, residuals, _, _ = np.linalg.lstsq(M, d, rcond=None)\n        \n        cost = residuals[0] if residuals.size > 0 else np.sum((M @ x_est - d)**2)\n\n        if cost  min_cost:\n            min_cost = cost\n            best_x_est = x_est\n            best_s_est = s_candidate\n    \n    # --- 3. Compute Output Metrics ---\n    \n    # Root-mean-square (RMS) current estimation error\n    rms_error = np.sqrt(np.mean((best_x_est - x_true)**2))\n    \n    # Total number of mode misclassifications\n    misclassifications = np.sum(np.array(best_s_est) != np.array(s_true))\n    \n    return rms_error, int(misclassifications)\n\nsolve()\n```"
        }
    ]
}