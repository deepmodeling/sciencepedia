{
    "hands_on_practices": [
        {
            "introduction": "The cornerstone of multi-sensor fusion lies in the principled combination of information under uncertainty. This first exercise provides a hands-on derivation of the fusion rule for the simplest non-trivial case: combining two independent, noisy measurements of a single, unknown quantity. By working through this problem , you will see how Bayesian inference naturally leads to a precision-weighted average, where more certain measurements contribute more to the final estimate.",
            "id": "4233215",
            "problem": "A Digital Twin (DT) of a Cyber-Physical System (CPS) monitors an unobserved scalar state $x$ (the internal core temperature of a battery module) using a centralized multi-sensor fusion architecture that aggregates a physical temperature sensor and a software-based observer. At a fixed time instant, the measurement models conditioned on $x$ are independent and described by the following Gaussian likelihoods: the physical sensor produces $y_1$ with $y_1 \\mid x \\sim \\mathcal{N}(x,\\sigma_1^2)$, and the DT observer produces $y_2$ with $y_2 \\mid x \\sim \\mathcal{N}(x,\\sigma_2^2)$. Assume a noninformative flat prior $p(x) \\propto 1$ over the range of interest, and conditional independence of $y_1$ and $y_2$ given $x$. Starting from Bayes’ rule and the definition of the Gaussian likelihood, derive the posterior density $p(x \\mid y_1,y_2)$ by completing the square, and identify the posterior mean as the minimum mean-squared error estimator for $x$ under the flat prior.\n\nAt a particular time instant, the measurements and known noise levels are:\n- $y_1 = 335.6$ K with $\\sigma_1 = 0.4$ K,\n- $y_2 = 336.9$ K with $\\sigma_2 = 0.9$ K.\n\nCompute the fused posterior mean estimate of $x$ in Kelvin. Express your final numerical answer in K and round to four significant figures.",
            "solution": "The objective is to derive the posterior probability density function $p(x \\mid y_1, y_2)$ for an unobserved state $x$ given two independent measurements $y_1$ and $y_2$, and then compute the posterior mean.\n\nAccording to Bayes' rule, the posterior distribution is proportional to the product of the likelihood and the prior distribution:\n$$\np(x \\mid y_1, y_2) \\propto p(y_1, y_2 \\mid x) p(x)\n$$\nGiven conditional independence of $y_1$ and $y_2$ and a noninformative flat prior ($p(x) \\propto 1$), this simplifies to:\n$$\np(x \\mid y_1, y_2) \\propto p(y_1 \\mid x) p(y_2 \\mid x)\n$$\nThe likelihoods are given as Gaussian distributions, $y_1 \\mid x \\sim \\mathcal{N}(x, \\sigma_1^2)$ and $y_2 \\mid x \\sim \\mathcal{N}(x, \\sigma_2^2)$. The posterior is proportional to the product of their PDFs:\n$$\np(x \\mid y_1, y_2) \\propto \\exp\\left( -\\frac{(x - y_1)^2}{2\\sigma_1^2} \\right) \\exp\\left( -\\frac{(x - y_2)^2}{2\\sigma_2^2} \\right) = \\exp\\left( -\\frac{1}{2} \\left[ \\frac{(x - y_1)^2}{\\sigma_1^2} + \\frac{(x - y_2)^2}{\\sigma_2^2} \\right] \\right)\n$$\nTo identify the form of this distribution, we analyze the argument of the exponential. By expanding the quadratic terms and grouping by powers of $x$, we complete the square. This reveals that the posterior is also a Gaussian distribution, $p(x \\mid y_1, y_2) \\sim \\mathcal{N}(\\mu_{post}, \\sigma_{post}^2)$.\nBy matching the coefficients of the quadratic expression in $x$ to the standard form of a Gaussian exponent, we find the posterior variance $\\sigma_{post}^2$ and mean $\\mu_{post}$:\n$$\n\\frac{1}{\\sigma_{post}^2} = \\frac{1}{\\sigma_1^2} + \\frac{1}{\\sigma_2^2}\n$$\n$$\n\\mu_{post} = \\sigma_{post}^2 \\left( \\frac{y_1}{\\sigma_1^2} + \\frac{y_2}{\\sigma_2^2} \\right) = \\frac{\\frac{y_1}{\\sigma_1^2} + \\frac{y_2}{\\sigma_2^2}}{\\frac{1}{\\sigma_1^2} + \\frac{1}{\\sigma_2^2}}\n$$\nThis shows that the posterior mean is the precision-weighted average of the measurements. For a Gaussian posterior, the mean is also the Minimum Mean-Squared Error (MMSE) estimator.\n\nNow, we compute the numerical value using the given data:\n- $y_1 = 335.6$ K\n- $\\sigma_1 = 0.4$ K $\\implies \\sigma_1^2 = 0.16$ K$^2$\n- $y_2 = 336.9$ K\n- $\\sigma_2 = 0.9$ K $\\implies \\sigma_2^2 = 0.81$ K$^2$\n\nFirst, calculate the precisions (inverse variances):\n$$\n\\frac{1}{\\sigma_1^2} = \\frac{1}{0.16} = 6.25 \\text{ K}^{-2}\n$$\n$$\n\\frac{1}{\\sigma_2^2} = \\frac{1}{0.81} \\approx 1.23457 \\text{ K}^{-2}\n$$\nNow, substitute these values into the formula for $\\mu_{post}$:\n$$\n\\mu_{post} = \\frac{y_1 \\left(\\frac{1}{\\sigma_1^2}\\right) + y_2 \\left(\\frac{1}{\\sigma_2^2}\\right)}{\\frac{1}{\\sigma_1^2} + \\frac{1}{\\sigma_2^2}} = \\frac{(335.6)(6.25) + (336.9)\\left(\\frac{1}{0.81}\\right)}{6.25 + \\frac{1}{0.81}}\n$$\n$$\n\\mu_{post} = \\frac{2097.5 + 415.9259...}{6.25 + 1.2345...} = \\frac{2513.4259...}{7.4845...} \\approx 335.8144... \\text{ K}\n$$\nRounding to four significant figures, the fused posterior mean estimate is $335.8$ K.",
            "answer": "$$\\boxed{335.8}$$"
        },
        {
            "introduction": "Building on the scalar case, we now generalize to the multi-dimensional systems common in robotics and aerospace, where state vectors and measurement models are represented by matrices. This practice  introduces the powerful concept of augmenting measurement models to handle multiple sensors in a single, elegant update step. You will derive the posterior covariance using the information form, which clearly illustrates that the information from a new measurement is additively combined with the prior information.",
            "id": "4233186",
            "problem": "A Digital Twin (DT) of a Cyber-Physical System (CPS) maintains a probabilistic state estimate of a two-dimensional latent state vector $x_{k} \\in \\mathbb{R}^{2}$ at discrete time $k$. At update time $k$, the DT receives two independent linear sensor measurements, each corrupted by zero-mean Gaussian noise, defined by\n$$\ny_{1,k} = C_{1} x_{k} + v_{1,k}, \\quad y_{2,k} = C_{2} x_{k} + v_{2,k},\n$$\nwhere $v_{1,k} \\sim \\mathcal{N}(0,R_{1})$, $v_{2,k} \\sim \\mathcal{N}(0,R_{2})$, and $v_{1,k}$ is statistically independent of $v_{2,k}$. The prior state estimate is Gaussian with covariance $P_{k|k-1}$. To perform multi-sensor fusion, stack the measurements and construct the augmented linear model with\n$$\n\\tilde{C} = \\begin{bmatrix} C_{1} \\\\ C_{2} \\end{bmatrix}, \\qquad \\tilde{R} = \\mathrm{diag}(R_{1}, R_{2}).\n$$\nStarting from the fundamental definitions of linear-Gaussian models and Bayes’ rule for Gaussian conditioning, derive the posterior covariance $P_{k|k}$ of the fused update. Use the following numerically specified, physically plausible quantities for a normalized, dimensionless state:\n$$\nP_{k|k-1} = \\begin{bmatrix} 2 & 0.5 \\\\ 0.5 & 1 \\end{bmatrix}, \\quad C_{1} = \\begin{bmatrix} 1 & 2 \\end{bmatrix}, \\quad C_{2} = \\begin{bmatrix} 2 & -1 \\end{bmatrix}, \\quad R_{1} = 0.2, \\quad R_{2} = 0.5.\n$$\nDemonstrate the fusion by explicitly forming $\\tilde{C}$ and $\\tilde{R}$, and carry out the full derivation to obtain $P_{k|k}$. As your final reported quantity, compute the determinant of $P_{k|k}$. Express the final determinant as an exact simplified fraction with no units and do not round.",
            "solution": "The task is to derive the posterior covariance matrix $P_{k|k}$ for a state estimate updated with two independent sensor measurements and then compute its determinant. We use the information form of the Kalman filter update, which is derived from the properties of Gaussian distributions under Bayesian conditioning.\n\nFor a linear-Gaussian system with prior $p(x_k) = \\mathcal{N}(x_k; \\hat{x}_{k|k-1}, P_{k|k-1})$ and likelihood $p(y_k | x_k) = \\mathcal{N}(y_k; C x_k, R)$, the posterior distribution $p(x_k | y_k)$ is also Gaussian with inverse covariance (information matrix) given by:\n$$\nP_{k|k}^{-1} = P_{k|k-1}^{-1} + C^T R^{-1} C\n$$\nThis equation shows that the posterior information is the sum of the prior information and the information gained from the measurement. For the multi-sensor fusion problem, we use the augmented measurement model with $\\tilde{C}$ and $\\tilde{R}$:\n$$\nP_{k|k}^{-1} = P_{k|k-1}^{-1} + \\tilde{C}^T \\tilde{R}^{-1} \\tilde{C}\n$$\nWe now substitute the given numerical values, using exact fractions for precision.\n$$\nP_{k|k-1} = \\begin{bmatrix} 2 & \\frac{1}{2} \\\\ \\frac{1}{2} & 1 \\end{bmatrix}, \\quad \\tilde{C} = \\begin{bmatrix} 1 & 2 \\\\ 2 & -1 \\end{bmatrix}, \\quad \\tilde{R} = \\begin{bmatrix} \\frac{1}{5} & 0 \\\\ 0 & \\frac{1}{2} \\end{bmatrix}\n$$\nFirst, calculate the required inverse matrices:\n$$\n\\det(P_{k|k-1}) = (2)(1) - (\\frac{1}{2})(\\frac{1}{2}) = \\frac{7}{4} \\implies P_{k|k-1}^{-1} = \\frac{4}{7} \\begin{bmatrix} 1 & -\\frac{1}{2} \\\\ -\\frac{1}{2} & 2 \\end{bmatrix} = \\begin{bmatrix} \\frac{4}{7} & -\\frac{2}{7} \\\\ -\\frac{2}{7} & \\frac{8}{7} \\end{bmatrix}\n$$\n$$\n\\tilde{R}^{-1} = \\begin{bmatrix} 5 & 0 \\\\ 0 & 2 \\end{bmatrix}\n$$\nNext, compute the information contribution from the measurements, $\\tilde{C}^T \\tilde{R}^{-1} \\tilde{C}$:\n$$\n\\tilde{C}^T \\tilde{R}^{-1} \\tilde{C} = \\begin{bmatrix} 1 & 2 \\\\ 2 & -1 \\end{bmatrix}^T \\begin{bmatrix} 5 & 0 \\\\ 0 & 2 \\end{bmatrix} \\begin{bmatrix} 1 & 2 \\\\ 2 & -1 \\end{bmatrix} = \\begin{bmatrix} 1 & 2 \\\\ 2 & -1 \\end{bmatrix} \\begin{bmatrix} 5 & 10 \\\\ 4 & -2 \\end{bmatrix} = \\begin{bmatrix} 13 & 6 \\\\ 6 & 22 \\end{bmatrix}\n$$\nNow, sum the prior information and the measurement information to get the posterior information matrix, $P_{k|k}^{-1}$:\n$$\nP_{k|k}^{-1} = P_{k|k-1}^{-1} + \\tilde{C}^T \\tilde{R}^{-1} \\tilde{C} = \\begin{bmatrix} \\frac{4}{7} & -\\frac{2}{7} \\\\ -\\frac{2}{7} & \\frac{8}{7} \\end{bmatrix} + \\begin{bmatrix} 13 & 6 \\\\ 6 & 22 \\end{bmatrix} = \\begin{bmatrix} \\frac{4+91}{7} & \\frac{-2+42}{7} \\\\ \\frac{-2+42}{7} & \\frac{8+154}{7} \\end{bmatrix} = \\begin{bmatrix} \\frac{95}{7} & \\frac{40}{7} \\\\ \\frac{40}{7} & \\frac{162}{7} \\end{bmatrix}\n$$\nThe problem asks for the determinant of the posterior covariance, $\\det(P_{k|k})$. Using the property $\\det(A^{-1}) = 1/\\det(A)$, we have $\\det(P_{k|k}) = 1/\\det(P_{k|k}^{-1})$.\n$$\n\\det(P_{k|k}^{-1}) = \\left(\\frac{95}{7}\\right)\\left(\\frac{162}{7}\\right) - \\left(\\frac{40}{7}\\right)\\left(\\frac{40}{7}\\right) = \\frac{95 \\times 162 - 40 \\times 40}{49} = \\frac{15390 - 1600}{49} = \\frac{13790}{49}\n$$\nThe determinant of the posterior covariance matrix is the reciprocal:\n$$\n\\det(P_{k|k}) = \\frac{1}{\\det(P_{k|k}^{-1})} = \\frac{49}{13790}\n$$\nTo simplify the fraction, we find the prime factors: $13790 = 10 \\times 1379 = 2 \\times 5 \\times 7 \\times 197$, and $49 = 7^2$.\n$$\n\\det(P_{k|k}) = \\frac{7 \\times 7}{2 \\times 5 \\times 7 \\times 197} = \\frac{7}{2 \\times 5 \\times 197} = \\frac{7}{1970}\n$$\nThis is the final, exact simplified fraction.",
            "answer": "$$\n\\boxed{\\frac{7}{1970}}\n$$"
        },
        {
            "introduction": "While linear models provide a powerful framework, many real-world sensors, such as radar or cameras, have an inherently nonlinear relationship with the state. This exercise  tackles this challenge by guiding you through the derivation and application of the Extended Kalman Filter (EKF) update for a common polar coordinate measurement model. You will perform a first-order Taylor-series linearization to approximate the nonlinear function, a fundamental technique for applying the principles of Gaussian filtering to a vast range of practical applications.",
            "id": "4233142",
            "problem": "A centralized multi-sensor fusion architecture is used in the Digital Twin (DT) of an underwater autonomous vehicle within a Cyber-Physical System (CPS). At a discrete time index $k$, the DT maintains a prior Gaussian state estimate for the vehicle’s planar position, modeled as $x \\in \\mathbb{R}^{2}$ with $x = \\begin{pmatrix} x_{1} \\\\ x_{2} \\end{pmatrix}$, having prior mean $\\hat{x}_{k|k-1}$ and prior covariance $P_{k|k-1}$. Two heterogeneous sensors independently observe a stationary target at the origin: a forward-looking sonar provides a range measurement and a monocular camera provides a bearing measurement. The fused measurement vector is\n$$\ny = \\begin{pmatrix} y_{r} \\\\ y_{\\theta} \\end{pmatrix} = h(x) + v,\n$$\nwhere $h(x) = \\begin{pmatrix} \\sqrt{x_{1}^{2} + x_{2}^{2}} \\\\ \\arctan2(x_{2}, x_{1}) \\end{pmatrix}$ and $v \\sim \\mathcal{N}(0, R)$ with $R \\in \\mathbb{R}^{2 \\times 2}$ positive definite. Assume the sensors are conditionally independent given the state, so $R$ is diagonal. The Extended Kalman Filter (EKF) measurement update is to be derived from first principles by linearizing the nonlinear measurement model about the prior mean and applying the linear-Gaussian Bayesian update.\n\nGiven:\n- Prior mean $\\hat{x}_{k|k-1} = \\begin{pmatrix} 30 \\\\ 40 \\end{pmatrix}$ (meters),\n- Prior covariance $P_{k|k-1} = \\begin{pmatrix} 9 & 3 \\\\ 3 & 16 \\end{pmatrix}$ (meters squared),\n- Measurement noise covariance $R = \\begin{pmatrix} 4.0 & 0 \\\\ 0 & 1.0 \\times 10^{-4} \\end{pmatrix}$ with range variance in meters squared and bearing variance in radians squared,\n- Actual fused measurement $y_k = \\begin{pmatrix} 49.0 \\\\ 0.93 \\end{pmatrix}$ with range in meters and bearing in radians.\n\nTasks:\n1. Starting strictly from the first-order Taylor linearization of $h(x)$ about $\\hat{x}_{k|k-1}$ and the laws of linear-Gaussian Bayesian estimation, derive the EKF measurement update for the posterior mean $\\hat{x}_{k|k}$ in terms of the Jacobian matrix $H = \\left.\\frac{\\partial h}{\\partial x}\\right|_{x=\\hat{x}_{k|k-1}}$, the prior covariance $P_{k|k-1}$, and the measurement noise covariance $R$.\n2. Compute the Jacobian $H$ at $x = \\hat{x}_{k|k-1}$ for the given $h(x)$.\n3. Use the derived update to compute the posterior mean $\\hat{x}_{k|k}$ numerically for the provided data.\n\nRound your final posterior state components to four significant figures. Express each component in meters. Provide your final answer as a single row vector.",
            "solution": "The problem requires the derivation and application of the Extended Kalman Filter (EKF) measurement update for a nonlinear measurement model.\n\n**1. Derivation of the EKF Measurement Update**\n\nThe EKF approximates the nonlinear measurement function $h(x)$ with a linear one by performing a first-order Taylor series expansion around the prior state estimate $\\hat{x}_{k|k-1}$:\n$$\nh(x_k) \\approx h(\\hat{x}_{k|k-1}) + H_k (x_k - \\hat{x}_{k|k-1})\n$$\nwhere $H_k = \\left.\\frac{\\partial h}{\\partial x}\\right|_{x=\\hat{x}_{k|k-1}}$ is the Jacobian matrix. Substituting this into the measurement equation $y_k = h(x_k) + v_k$ and rearranging yields an approximate linear measurement of the prior error $(x_k - \\hat{x}_{k|k-1})$:\n$$\ny_k - h(\\hat{x}_{k|k-1}) \\approx H_k (x_k - \\hat{x}_{k|k-1}) + v_k\n$$\nThis allows us to apply the standard linear Kalman filter update equations. The main steps are:\n- **Innovation (Residual):** $\\tilde{y}_k = y_k - h(\\hat{x}_{k|k-1})$\n- **Innovation Covariance:** $S_k = H_k P_{k|k-1} H_k^T + R_k$\n- **Kalman Gain:** $K_k = P_{k|k-1} H_k^T S_k^{-1}$\n- **Posterior Mean Update:** $\\hat{x}_{k|k} = \\hat{x}_{k|k-1} + K_k \\tilde{y}_k$\n\n**2. Computation of the Jacobian**\n\nThe measurement function is $h(x) = \\begin{pmatrix} \\sqrt{x_{1}^{2} + x_{2}^{2}} \\\\ \\arctan2(x_{2}, x_{1}) \\end{pmatrix}$. The Jacobian matrix $H(x)$ is:\n$$\nH(x) = \\begin{pmatrix} \\frac{\\partial h_1}{\\partial x_1} & \\frac{\\partial h_1}{\\partial x_2} \\\\ \\frac{\\partial h_2}{\\partial x_1} & \\frac{\\partial h_2}{\\partial x_2} \\end{pmatrix} = \\begin{pmatrix} \\frac{x_1}{\\sqrt{x_1^2 + x_2^2}} & \\frac{x_2}{\\sqrt{x_1^2 + x_2^2}} \\\\ \\frac{-x_2}{x_1^2 + x_2^2} & \\frac{x_1}{x_1^2 + x_2^2} \\end{pmatrix}\n$$\nWe evaluate this at $\\hat{x}_{k|k-1} = \\begin{pmatrix} 30 \\\\ 40 \\end{pmatrix}$. The range is $r = \\sqrt{30^2 + 40^2} = 50$ m.\n$$\nH_k = H(\\hat{x}_{k|k-1}) = \\begin{pmatrix} \\frac{30}{50} & \\frac{40}{50} \\\\ \\frac{-40}{50^2} & \\frac{30}{50^2} \\end{pmatrix} = \\begin{pmatrix} 0.6 & 0.8 \\\\ -0.016 & 0.012 \\end{pmatrix}\n$$\n\n**3. Numerical Computation of the Posterior Mean**\n\nWe follow the EKF update steps:\n\n- **Compute predicted measurement and innovation:**\nThe predicted measurement is $h(\\hat{x}_{k|k-1}) = \\begin{pmatrix} \\sqrt{30^2 + 40^2} \\\\ \\arctan2(40, 30) \\end{pmatrix} = \\begin{pmatrix} 50 \\\\ \\arctan(4/3) \\end{pmatrix} \\approx \\begin{pmatrix} 50 \\\\ 0.927295 \\end{pmatrix}$.\nThe innovation is $\\tilde{y}_k = y_k - h(\\hat{x}_{k|k-1}) = \\begin{pmatrix} 49.0 \\\\ 0.93 \\end{pmatrix} - \\begin{pmatrix} 50 \\\\ 0.927295 \\end{pmatrix} = \\begin{pmatrix} -1.0 \\\\ 0.002705 \\end{pmatrix}$.\n\n- **Compute innovation covariance $S_k = H_k P_{k|k-1} H_k^T + R_k$:**\n$$\nH_k P_{k|k-1} H_k^T = \\begin{pmatrix} 0.6 & 0.8 \\\\ -0.016 & 0.012 \\end{pmatrix} \\begin{pmatrix} 9 & 3 \\\\ 3 & 16 \\end{pmatrix} \\begin{pmatrix} 0.6 & -0.016 \\\\ 0.8 & 0.012 \\end{pmatrix} = \\begin{pmatrix} 16.36 & 0.0504 \\\\ 0.0504 & 0.003456 \\end{pmatrix}\n$$\n$$\nS_k = \\begin{pmatrix} 16.36 & 0.0504 \\\\ 0.0504 & 0.003456 \\end{pmatrix} + \\begin{pmatrix} 4.0 & 0 \\\\ 0 & 0.0001 \\end{pmatrix} = \\begin{pmatrix} 20.36 & 0.0504 \\\\ 0.0504 & 0.003556 \\end{pmatrix}\n$$\n\n- **Compute Kalman Gain $K_k = P_{k|k-1} H_k^T S_k^{-1}$:**\n$$\nP_{k|k-1} H_k^T = \\begin{pmatrix} 9 & 3 \\\\ 3 & 16 \\end{pmatrix} \\begin{pmatrix} 0.6 & -0.016 \\\\ 0.8 & 0.012 \\end{pmatrix} = \\begin{pmatrix} 7.8 & -0.108 \\\\ 14.6 & 0.144 \\end{pmatrix}\n$$\nThe inverse of the innovation covariance is $S_k^{-1} \\approx \\frac{1}{0.06984} \\begin{pmatrix} 0.003556 & -0.0504 \\\\ -0.0504 & 20.36 \\end{pmatrix}$.\n$$\nK_k = P_{k|k-1} H_k^T S_k^{-1} \\approx \\begin{pmatrix} 7.8 & -0.108 \\\\ 14.6 & 0.144 \\end{pmatrix} \\begin{pmatrix} 50.92 & -0.7216 \\\\ -0.7216 & 291.52 \\end{pmatrix} \\approx \\begin{pmatrix} 0.4751 & -37.11 \\\\ 0.6395 & 31.44 \\end{pmatrix}\n$$\n\n- **Compute the posterior mean $\\hat{x}_{k|k}$:**\nThe state update correction is $K_k \\tilde{y}_k$:\n$$\nK_k \\tilde{y}_k \\approx \\begin{pmatrix} 0.4751 & -37.11 \\\\ 0.6395 & 31.44 \\end{pmatrix} \\begin{pmatrix} -1.0 \\\\ 0.002705 \\end{pmatrix} \\approx \\begin{pmatrix} -0.5755 \\\\ -0.5544 \\end{pmatrix}\n$$\nFinally, add the correction to the prior mean:\n$$\n\\hat{x}_{k|k} = \\hat{x}_{k|k-1} + K_k \\tilde{y}_k \\approx \\begin{pmatrix} 30 \\\\ 40 \\end{pmatrix} + \\begin{pmatrix} -0.5755 \\\\ -0.5544 \\end{pmatrix} = \\begin{pmatrix} 29.4245 \\\\ 39.4456 \\end{pmatrix}\n$$\nRounding the final components to four significant figures, we get:\n$\\hat{x}_{k|k,1} = 29.42$ meters.\n$\\hat{x}_{k|k,2} = 39.45$ meters.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 29.42 & 39.45 \\end{pmatrix}}\n$$"
        }
    ]
}