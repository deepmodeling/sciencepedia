{
    "hands_on_practices": [
        {
            "introduction": "The first step in creating a data-driven digital twin is collecting high-quality data from the physical asset. This involves a critical trade-off governed by the Nyquist-Shannon Sampling Theorem; we must sample fast enough to capture the system's dynamics but also use an anti-aliasing filter to prevent high-frequency noise from corrupting our data. This exercise  provides hands-on practice in navigating this trade-off, allowing you to determine the minimum sampling frequency that satisfies both signal fidelity and anti-aliasing requirements—a fundamental skill in designing any data acquisition system.",
            "id": "4249810",
            "problem": "A mechanical cyber-physical system is operated with a Digital Twin that performs system identification from sampled sensor data. The underlying continuous-time plant is modeled as a linear time-invariant (LTI) system with dominant dynamics characterized by a highest significant eigenfrequency $f_{\\max}$, defined as the largest imaginary-part frequency among poles that contribute non-negligibly to the output spectrum within the operating envelope. The data acquisition chain consists of an analog anti-aliasing prefilter followed by an Analog-to-Digital Converter (ADC). The prefilter is a 2nd-order Butterworth low-pass with cutoff frequency $f_{c}$, and its magnitude response is given by $|H(j 2 \\pi f)| = \\left(1 + (f/f_{c})^{4}\\right)^{-1/2}$. The sampling frequency is $f_{s}$.\n\nTo ensure the fidelity of data-driven modeling for the Digital Twin, two constraints are imposed:\n1. Passband fidelity: At $f_{\\max}$, the magnitude response must satisfy $|H(j 2 \\pi f_{\\max})| \\geq 0.97$.\n2. Anti-alias suppression: At the Nyquist frequency $f_{s}/2$, the magnitude response must satisfy $|H(j 2 \\pi (f_{s}/2))| \\leq 0.01$.\n\nAssume $f_{\\max} = 45$ Hz. Determine the minimum sampling frequency $f_{s}$ that simultaneously satisfies both constraints by choosing $f_{c}$ to minimize $f_{s}$ subject to the constraints, starting from fundamental definitions including the Nyquist-Shannon Sampling Theorem (NSST) and standard filter response properties. Express your final answer in Hz and round your result to four significant figures. The answer must be a single real-valued number.",
            "solution": "The problem requires determining the minimum sampling frequency, $f_s$, for a data acquisition system under two constraints imposed on an anti-aliasing filter. The solution involves translating these system-level requirements into mathematical inequalities and solving an optimization problem.\n\nFirst, we establish the context. The Nyquist-Shannon Sampling Theorem (NSST) is a fundamental principle in digital signal processing. It states that to perfectly reconstruct a continuous-time signal from its samples, the sampling frequency $f_s$ must be strictly greater than twice the signal's highest frequency component, or bandwidth $f_{BW}$. In practice, signals are not perfectly band-limited, and anti-aliasing filters are used to attenuate frequencies above the Nyquist frequency, $f_s/2$, to prevent aliasing, where high-frequency components erroneously appear as lower frequencies in the sampled data.\n\nThe system uses a 2nd-order Butterworth low-pass filter with a magnitude response given by:\n$$|H(j 2 \\pi f)| = \\frac{1}{\\sqrt{1 + \\left(\\frac{f}{f_c}\\right)^4}}$$\nwhere $f$ is the frequency and $f_c$ is the cutoff frequency of the filter.\n\nThe problem presents two constraints on this filter's performance, which we will formalize into mathematical inequalities.\n\nConstraint 1: Passband fidelity.\nThe filter must pass the signal components of interest with minimal attenuation. The highest significant frequency of the plant is $f_{\\max} = 45$ Hz. At this frequency, the magnitude response must be at least $0.97$.\n$$|H(j 2 \\pi f_{\\max})| \\geq 0.97$$\nSubstituting the filter response equation and the value of $f_{\\max}$:\n$$\\frac{1}{\\sqrt{1 + \\left(\\frac{f_{\\max}}{f_c}\\right)^4}} \\geq 0.97$$\nTo solve for $f_c$, we manipulate the inequality. Since both sides are positive, we can square them:\n$$\\frac{1}{1 + \\left(\\frac{f_{\\max}}{f_c}\\right)^4} \\geq 0.97^2$$\nTaking the reciprocal reverses the inequality sign:\n$$1 + \\left(\\frac{f_{\\max}}{f_c}\\right)^4 \\leq \\frac{1}{0.97^2}$$\n$$\\left(\\frac{f_{\\max}}{f_c}\\right)^4 \\leq \\frac{1}{0.97^2} - 1$$\nSince the right-hand side is positive, we can proceed:\n$$\\frac{f_{\\max}^4}{f_c^4} \\leq \\frac{1 - 0.97^2}{0.97^2}$$\n$$f_c^4 \\geq \\frac{f_{\\max}^4 \\cdot 0.97^2}{1 - 0.97^2} = f_{\\max}^4 \\left(\\frac{1}{0.97^2} - 1\\right)^{-1}$$\nTaking the positive fourth root, we find a lower bound for $f_c$:\n$$f_c \\geq f_{\\max} \\left(\\left(\\frac{1}{0.97}\\right)^2 - 1\\right)^{-1/4}$$\n\nConstraint 2: Anti-alias suppression.\nThe filter must sufficiently attenuate frequencies at or above the Nyquist frequency, $f_s/2$, to prevent aliasing. At the frequency $f = f_s/2$, the magnitude response must be no more than $0.01$.\n$$|H(j 2 \\pi (f_s/2))| \\leq 0.01$$\nSubstituting the filter response equation:\n$$\\frac{1}{\\sqrt{1 + \\left(\\frac{f_s/2}{f_c}\\right)^4}} \\leq 0.01$$\nAgain, we square both sides:\n$$\\frac{1}{1 + \\left(\\frac{f_s}{2f_c}\\right)^4} \\leq 0.01^2$$\nTaking the reciprocal:\n$$1 + \\left(\\frac{f_s}{2f_c}\\right)^4 \\geq \\frac{1}{0.01^2} = 10000$$\n$$\\left(\\frac{f_s}{2f_c}\\right)^4 \\geq 10000 - 1 = 9999$$\nTaking the positive fourth root:\n$$\\frac{f_s}{2f_c} \\geq (9999)^{1/4}$$\nThis gives us a lower bound for $f_s$ as a function of $f_c$:\n$$f_s \\geq 2 f_c (9999)^{1/4}$$\n\nOptimization Problem.\nWe want to find the minimum possible value of $f_s$. The expression for the lower bound of $f_s$ is a monotonically increasing function of $f_c$. Therefore, to minimize $f_s$, we must choose the smallest possible value for $f_c$ that is permitted by Constraint 1. The minimal $f_c$ is achieved when the first inequality becomes an equality:\n$$f_{c, \\text{opt}} = f_{\\max} \\left(\\left(\\frac{1}{0.97}\\right)^2 - 1\\right)^{-1/4}$$\nSubstituting this optimal choice of $f_c$ into the inequality for $f_s$ gives the minimum required sampling frequency, $f_{s, \\text{min}}$:\n$$f_{s, \\text{min}} = 2 f_{c, \\text{opt}} (9999)^{1/4}$$\n$$f_{s, \\text{min}} = 2 \\left( f_{\\max} \\left(\\left(\\frac{1}{0.97}\\right)^2 - 1\\right)^{-1/4} \\right) (9999)^{1/4}$$\nWe can combine the terms under a single root:\n$$f_{s, \\text{min}} = 2 f_{\\max} \\left( \\frac{9999}{\\left(\\frac{1}{0.97}\\right)^2 - 1} \\right)^{1/4}$$\n\nNumerical Calculation.\nNow, we substitute the given value $f_{\\max} = 45$ Hz into the expression.\n$$f_{s, \\text{min}} = 2 \\cdot 45 \\left( \\frac{9999}{\\frac{1}{0.97^2} - 1} \\right)^{1/4}$$\n$$f_{s, \\text{min}} = 90 \\left( \\frac{9999}{\\frac{1 - 0.9409}{0.9409}} \\right)^{1/4}$$\n$$f_{s, \\text{min}} = 90 \\left( \\frac{9999 \\cdot 0.9409}{0.0591} \\right)^{1/4}$$\n$$f_{s, \\text{min}} = 90 \\left( 159187.80408... \\right)^{1/4}$$\n$$f_{s, \\text{min}} = 90 \\left( 19.974636... \\right)$$\n$$f_{s, \\text{min}} = 1797.7172... \\text{ Hz}$$\nThe problem requires the answer to be rounded to four significant figures.\n$$f_{s, \\text{min}} \\approx 1798 \\text{ Hz}$$\nThis value represents the minimum sampling frequency that allows for the design of a 2nd-order Butterworth filter satisfying both the passband fidelity and anti-aliasing suppression requirements.",
            "answer": "$$\\boxed{1798}$$"
        },
        {
            "introduction": "Once a suitable sampling period is chosen, we often need to convert a first-principles continuous-time model of our system into a discrete-time equivalent for simulation and control within the digital twin. This practice  delves into the exact discretization of a Linear Time-Invariant (LTI) system under a Zero-Order Hold (ZOH) assumption, a method that provides a precise mathematical bridge between continuous-world differential equations and discrete-world difference equations. By working through this calculation, you will gain a deeper understanding of how the dynamics of a physical system are mapped into the discrete domain, a crucial step for implementing model-based algorithms.",
            "id": "4249854",
            "problem": "A digital twin of a micro-electro-mechanical torsional actuator is modeled as a continuous-time Linear Time-Invariant (LTI) state-space system with two states capturing angular displacement and angular velocity. The continuous-time dynamics are given by $\\dot{x}(t)=A x(t)+B u(t)$ and $y(t)=C x(t)+D u(t)$, where $x(t)\\in\\mathbb{R}^{2}$ is the state, $u(t)\\in\\mathbb{R}$ is the control input, and $y(t)\\in\\mathbb{R}$ is the measured output. For controller emulation and state estimation inside the digital twin, the model is sampled under a Zero-Order Hold (ZOH), meaning $u(t)$ is constant over each sampling interval $[k T_{s},(k+1)T_{s})$ for integer $k$, with sampling period $T_{s}>0$.\n\nConsider the specific system matrices\n$$\nA=\\begin{pmatrix}\n-1 & 1 \\\\\n0 & -1\n\\end{pmatrix},\\quad\nB=\\begin{pmatrix}\n0 \\\\\n1\n\\end{pmatrix},\\quad\nC=\\begin{pmatrix}\n1 & 0\n\\end{pmatrix},\\quad\nD=0,\n$$\nand let the discrete-time model be $x[k+1]=A_{d} x[k]+B_{d} u[k]$ under ZOH sampling.\n\nStarting from the fundamental solution of linear time-invariant differential equations and the definition of Zero-Order Hold (ZOH) sampling, derive the exact discrete-time input mapping $B_{d}$ and compute the first entry of $B_{d}$, denoted $\\big(B_{d}\\big)_{1}$, as a closed-form analytic expression in terms of $T_{s}$.\n\nExpress your final answer as a single closed-form expression in terms of $T_{s}$. No rounding is required.",
            "solution": "The problem requires the derivation of the discrete-time input matrix $B_d$ for a continuous-time Linear Time-Invariant (LTI) system under Zero-Order Hold (ZOH) sampling, and the computation of its first element.\n\nThe continuous-time system is described by the state-space equation:\n$$ \\dot{x}(t) = A x(t) + B u(t) $$\nThe solution to this differential equation over an interval starting at time $t_0 = kT_s$ and ending at time $t = (k+1)T_s$ is given by the variation of parameters formula:\n$$ x((k+1)T_s) = \\exp(A((k+1)T_s - kT_s)) x(kT_s) + \\int_{kT_s}^{(k+1)T_s} \\exp(A((k+1)T_s - \\tau)) B u(\\tau) d\\tau $$\nLet $x[k] \\triangleq x(kT_s)$ and $x[k+1] \\triangleq x((k+1)T_s)$. The equation becomes:\n$$ x[k+1] = \\exp(AT_s) x[k] + \\int_{kT_s}^{(k+1)T_s} \\exp(A((k+1)T_s - \\tau)) B u(\\tau) d\\tau $$\nThe problem states that the input is sampled with a Zero-Order Hold (ZOH). This implies that the input $u(t)$ is held constant over each sampling interval, i.e., $u(\\tau) = u(kT_s) \\triangleq u[k]$ for all $\\tau \\in [kT_s, (k+1)T_s)$. Substituting this into the integral, we can factor out the constant term $u[k]$:\n$$ x[k+1] = \\exp(AT_s) x[k] + \\left( \\int_{kT_s}^{(k+1)T_s} \\exp(A((k+1)T_s - \\tau)) B d\\tau \\right) u[k] $$\nTo simplify the integral, we perform a change of variables. Let $\\sigma = (k+1)T_s - \\tau$. When $\\tau = kT_s$, $\\sigma = T_s$. When $\\tau = (k+1)T_s$, $\\sigma = 0$. The differential is $d\\sigma = -d\\tau$. The integral becomes:\n$$ \\int_{T_s}^{0} \\exp(A\\sigma) B (-d\\sigma) = \\int_{0}^{T_s} \\exp(A\\sigma) B d\\sigma = \\left(\\int_{0}^{T_s} \\exp(A\\sigma) d\\sigma\\right) B $$\nBy comparing the resulting equation,\n$$ x[k+1] = \\exp(AT_s) x[k] + \\left( \\left(\\int_{0}^{T_s} \\exp(A\\sigma) d\\sigma\\right) B \\right) u[k] $$\nwith the standard discrete-time form $x[k+1] = A_d x[k] + B_d u[k]$, we identify the discrete-time matrices as:\n$$ A_d = \\exp(AT_s) $$\n$$ B_d = \\left(\\int_{0}^{T_s} \\exp(A\\sigma) d\\sigma\\right) B $$\nOur task is to compute the first element of $B_d$ for the given matrices $A = \\begin{pmatrix} -1 & 1 \\\\ 0 & -1 \\end{pmatrix}$ and $B = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$.\n\nFirst, we must compute the matrix exponential $\\exp(A\\sigma)$. The matrix $A$ is a Jordan block. We can decompose it as $A = -1 \\cdot I + N$, where $I$ is the $2 \\times 2$ identity matrix and $N = \\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix}$.\nSince $-I$ and $N$ commute, we can write $\\exp(A\\sigma) = \\exp((-I+N)\\sigma) = \\exp(-I\\sigma)\\exp(N\\sigma) = \\exp(-\\sigma)I \\cdot \\exp(N\\sigma)$.\nThe matrix $N$ is nilpotent, with $N^2 = \\begin{pmatrix} 0 & 0 \\\\ 0 & 0 \\end{pmatrix}$. The Taylor series expansion of $\\exp(N\\sigma)$ terminates:\n$$ \\exp(N\\sigma) = I + N\\sigma + \\frac{(N\\sigma)^2}{2!} + \\dots = I + N\\sigma = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} + \\sigma\\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 1 & \\sigma \\\\ 0 & 1 \\end{pmatrix} $$\nTherefore, the matrix exponential of $A\\sigma$ is:\n$$ \\exp(A\\sigma) = \\exp(-\\sigma) \\begin{pmatrix} 1 & \\sigma \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} \\exp(-\\sigma) & \\sigma\\exp(-\\sigma) \\\\ 0 & \\exp(-\\sigma) \\end{pmatrix} $$\nNext, we compute the integral of $\\exp(A\\sigma)$ from $0$ to $T_s$:\n$$ \\int_{0}^{T_s} \\exp(A\\sigma) d\\sigma = \\int_{0}^{T_s} \\begin{pmatrix} \\exp(-\\sigma) & \\sigma\\exp(-\\sigma) \\\\ 0 & \\exp(-\\sigma) \\end{pmatrix} d\\sigma = \\begin{pmatrix} \\int_{0}^{T_s} \\exp(-\\sigma) d\\sigma & \\int_{0}^{T_s} \\sigma\\exp(-\\sigma) d\\sigma \\\\ 0 & \\int_{0}^{T_s} \\exp(-\\sigma) d\\sigma \\end{pmatrix} $$\nWe evaluate the two distinct integrals. The first integral is:\n$$ \\int_{0}^{T_s} \\exp(-\\sigma) d\\sigma = \\left[-\\exp(-\\sigma)\\right]_{0}^{T_s} = -\\exp(-T_s) - (-\\exp(0)) = 1 - \\exp(-T_s) $$\nThe second integral requires integration by parts, $\\int u \\, dv = uv - \\int v \\, du$. Let $u = \\sigma$ and $dv = \\exp(-\\sigma)d\\sigma$. This gives $du=d\\sigma$ and $v = -\\exp(-\\sigma)$.\n$$ \\int_{0}^{T_s} \\sigma\\exp(-\\sigma) d\\sigma = \\left[-\\sigma\\exp(-\\sigma)\\right]_{0}^{T_s} - \\int_{0}^{T_s} (-\\exp(-\\sigma)) d\\sigma $$\n$$ = \\left(-T_s\\exp(-T_s) - 0\\right) + \\int_{0}^{T_s} \\exp(-\\sigma) d\\sigma $$\n$$ = -T_s\\exp(-T_s) + \\left(1 - \\exp(-T_s)\\right) = 1 - T_s\\exp(-T_s) - \\exp(-T_s) = 1 - (1+T_s)\\exp(-T_s) $$\nSubstituting these results back into the matrix integral:\n$$ \\int_{0}^{T_s} \\exp(A\\sigma) d\\sigma = \\begin{pmatrix} 1 - \\exp(-T_s) & 1 - (1+T_s)\\exp(-T_s) \\\\ 0 & 1 - \\exp(-T_s) \\end{pmatrix} $$\nFinally, we compute $B_d$ by post-multiplying this matrix by $B$:\n$$ B_d = \\left(\\int_{0}^{T_s} \\exp(A\\sigma) d\\sigma\\right) B = \\begin{pmatrix} 1 - \\exp(-T_s) & 1 - (1+T_s)\\exp(-T_s) \\\\ 0 & 1 - \\exp(-T_s) \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} $$\n$$ B_d = \\begin{pmatrix} (1 - \\exp(-T_s)) \\cdot 0 + (1 - (1+T_s)\\exp(-T_s)) \\cdot 1 \\\\ 0 \\cdot 0 + (1 - \\exp(-T_s)) \\cdot 1 \\end{pmatrix} = \\begin{pmatrix} 1 - (1+T_s)\\exp(-T_s) \\\\ 1 - \\exp(-T_s) \\end{pmatrix} $$\nThe problem asks for the first entry of $B_d$, denoted as $(B_d)_1$. From the resulting vector, we have:\n$$ (B_d)_1 = 1 - (1+T_s)\\exp(-T_s) $$\nThis expression is the closed-form analytic result for the first element of the discrete-time input matrix $B_d$.",
            "answer": "$$\\boxed{1 - (1+T_{s})\\exp(-T_{s})}$$"
        },
        {
            "introduction": "After developing a model for a digital twin, whether from first principles or directly from data, a critical question remains: is the model accurate? A cornerstone of model validation is analyzing the model's one-step-ahead prediction errors, known as the innovations sequence; if a model has successfully captured the system's predictable dynamics, the remaining innovations should be unpredictable, resembling white noise. This hands-on coding exercise  guides you through implementing the Ljung-Box test, a powerful statistical tool for assessing whether the innovations are white, thereby providing quantitative evidence of your model's validity.",
            "id": "4249835",
            "problem": "Consider a discrete-time Linear Time-Invariant (LTI) Single-Input Single-Output (SISO) process described by an AutoRegressive with eXogenous input (ARX) model. Let the measured output sequence be $\\{y_k\\}_{k=0}^{N-1}$, the known input sequence be $\\{u_k\\}_{k=0}^{N-1}$, and the one-step-ahead output predictor be given by $\\hat{y}_{k|k-1}$, which is the prediction of $y_k$ using information available up to time $k-1$. The innovations sequence is defined by $\\tilde{y}_k = y_k - \\hat{y}_{k|k-1}$. Under correct modeling with Independent and Identically Distributed (IID) measurement noise, the innovations $\\{\\tilde{y}_k\\}$ should be a white-noise sequence. To assess whiteness, use the Ljung–Box (LB) statistic, which tests the joint null hypothesis that the autocorrelations of $\\{\\tilde{y}_k\\}$ at lags $1$ through $H$ are all equal to zero.\n\nStarting from fundamental definitions of discrete-time prediction and sample autocorrelation, compute the innovations sequence from a specified predictor and then compute the LB statistic and its associated decision at a given significance level. The LB test should be implemented in a way that is consistent with its asymptotic justification under the IID assumption when the model parameters used in the predictor are treated as given (not estimated from the data). The decision rule is to accept whiteness if the LB test $p$-value is greater than or equal to the significance level $\\alpha$.\n\nFor each test case below, the process is generated as\n$$\ny_k = a\\,y_{k-1} + b\\,u_{k-1} + e_k,\n$$\nwith initial condition $y_{-1} = 0$ and $u_{-1} = 0$. The predictor uses the known coefficients $a$ and $b$ and information up to $k-1$:\n$$\n\\hat{y}_{k|k-1} = a\\,y_{k-1} + b\\,u_{k-1}.\n$$\nThe input is deterministic:\n$$\nu_k = \\sin(\\omega k),\n$$\nwith angles in radians. The noise sequence is either white, $e_k \\sim \\mathcal{N}(0,\\sigma^2)$, or colored by an AutoRegressive of order one process, $e_k = c\\,e_{k-1} + w_k$, where $w_k \\sim \\mathcal{N}(0,\\sigma^2 (1 - c^2))$ so that the stationary variance of $e_k$ is $\\sigma^2$. The initial noise sample $e_0$ is drawn from $\\mathcal{N}(0,\\sigma^2)$. Use a fixed pseudo-random number generator seed to ensure reproducibility.\n\nFor each test case, perform the following tasks:\n- Generate $\\{u_k\\}$ and $\\{y_k\\}$ for the given parameters.\n- Compute the innovations $\\{\\tilde{y}_k\\}$ for $k=1,\\dots,N-1$.\n- Compute sample autocorrelations $\\hat{\\rho}_h$ for lags $h=1,\\dots,H$, based on the mean-centered innovations and a normalization consistent with asymptotic properties under the IID assumption.\n- Compute the Ljung–Box statistic using the sample autocorrelations and evaluate the $p$-value under a chi-square distribution with $H$ degrees of freedom, treating the predictor parameters as given.\n- Decide whiteness based on the rule \"accept whiteness if $p \\ge \\alpha$\".\n\nTest Suite:\n- Case A (correct model, white innovations, happy path):\n  - $N = 600$, $a = 0.7$, $b = 0.5$, $\\sigma = 0.2$, $\\omega = 0.1$, $c = 0.0$ (white), $H = 20$, $\\alpha = 0.05$ (expressed as a decimal).\n- Case B (correct model dynamics but colored noise, non-white innovations):\n  - $N = 600$, $a = 0.7$, $b = 0.5$, $\\sigma = 0.2$, $\\omega = 0.1$, $c = 0.6$ (colored), $H = 20$, $\\alpha = 0.05$ (expressed as a decimal).\n- Case C (correct model, white innovations, boundary condition with small data):\n  - $N = 40$, $a = 0.3$, $b = 0.2$, $\\sigma = 0.1$, $\\omega = 0.2$, $c = 0.0$ (white), $H = 5$, $\\alpha = 0.05$ (expressed as a decimal).\n\nImplementation Requirements:\n- Use a single fixed seed for the pseudo-random number generator to ensure deterministic outputs.\n- For each case, produce a boolean indicating whether whiteness is accepted at significance level $\\alpha$.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., `[resultA,resultB,resultC]`), where each result is a boolean value \"True\" or \"False\".\n\nFinal Output Format:\n- A single line string in the form `[b_A,b_B,b_C]`, where $b_A$, $b_B$, and $b_C$ are booleans corresponding to the whiteness decision for Case A, Case B, and Case C, respectively.",
            "solution": "The derivation proceeds from fundamental definitions in discrete-time stochastic modeling and hypothesis testing. The AutoRegressive with eXogenous input (ARX) model is a standard representation for discrete-time processes, and it is consistent with the modeling of digital twins for cyber-physical systems where a data-driven predictor is deployed to produce one-step-ahead output forecasts.\n\n1. Fundamental process and predictor definitions. The process is given by\n$$\ny_k = a\\,y_{k-1} + b\\,u_{k-1} + e_k,\n$$\nwith known deterministic input\n$$\nu_k = \\sin(\\omega k),\n$$\nwhere the angle is specified in radians. The one-step-ahead predictor uses information up to time $k-1$:\n$$\n\\hat{y}_{k|k-1} = a\\,y_{k-1} + b\\,u_{k-1}.\n$$\nThese relations follow from the definition of an AutoRegressive with eXogenous input model and the basic concept of one-step-ahead prediction in discrete time, which is derived from causality and the shift operator properties in time series.\n\n2. Innovations definition. The innovations are the prediction errors:\n$$\n\\tilde{y}_k = y_k - \\hat{y}_{k|k-1}.\n$$\nSubstituting the process and predictor,\n$$\n\\tilde{y}_k = \\left(a\\,y_{k-1} + b\\,u_{k-1} + e_k\\right) - \\left(a\\,y_{k-1} + b\\,u_{k-1}\\right) = e_k.\n$$\nUnder the assumption of white noise, $e_k$ is an Independent and Identically Distributed (IID) sequence with zero mean and finite variance, which implies that $\\{\\tilde{y}_k\\}$ should be a white-noise sequence. When the measurement noise is colored (for example, an AutoRegressive of order one sequence), the innovations inherit autocorrelation and are not white.\n\n3. Sample autocorrelation. To test whiteness, we compute sample autocorrelations from the observed innovations. Let $\\tilde{y}_k$ be the innovations for $k = 1, \\dots, n$, where $n = N-1$ is the available sample size for well-defined predictions. The sample mean is\n$$\n\\bar{\\tilde{y}} = \\frac{1}{n}\\sum_{k=1}^{n} \\tilde{y}_k.\n$$\nDefine mean-centered innovations $z_k = \\tilde{y}_k - \\bar{\\tilde{y}}$. The sample autocovariance at lag $h$ is\n$$\n\\hat{\\gamma}_h = \\frac{1}{n}\\sum_{k=h+1}^{n} z_k z_{k-h}.\n$$\nThe sample variance is $\\hat{\\gamma}_0 = \\frac{1}{n}\\sum_{k=1}^{n} z_k^2$. The sample autocorrelation at lag $h$ is then\n$$\n\\hat{\\rho}_h = \\frac{\\hat{\\gamma}_h}{\\hat{\\gamma}_0}.\n$$\nThese definitions are based on the standard estimators for autocovariance and autocorrelation under stationarity, and the normalization by $n$ aligns with asymptotic properties under the IID assumption.\n\n4. Ljung–Box (LB) statistic. The LB statistic jointly tests the null hypothesis that $\\hat{\\rho}_h = 0$ for $h = 1, 2, \\dots, H$. Under the null hypothesis and for large $n$, the LB statistic is approximately distributed as a chi-square random variable with $H$ degrees of freedom when the predictor parameters are treated as given. The LB statistic aggregates the squared autocorrelations with a finite-sample correction factor to mitigate bias:\n$$\nQ = n(n+2)\\sum_{h=1}^{H}\\frac{\\hat{\\rho}_h^2}{n - h}.\n$$\nThe $p$-value is computed under a chi-square distribution with $H$ degrees of freedom:\n$$\np = \\Pr\\left(\\chi^2_H \\ge Q\\right).\n$$\nThe decision rule is to accept whiteness if $p \\ge \\alpha$ and reject otherwise. This is grounded in large-sample theory, where under the IID assumption the vector of sample autocorrelations is asymptotically normal with zero mean, and the quadratic form above yields the chi-square approximation.\n\n5. Algorithmic steps. For each test case:\n- Generate the input $u_k = \\sin(\\omega k)$ for $k = 0, \\dots, N-1$.\n- Generate the noise $e_k$:\n  - White noise case: $e_k \\sim \\mathcal{N}(0,\\sigma^2)$ IID.\n  - Colored noise case: $e_k = c\\,e_{k-1} + w_k$ with $w_k \\sim \\mathcal{N}(0, \\sigma^2 (1 - c^2))$ and $e_0 \\sim \\mathcal{N}(0,\\sigma^2)$ to ensure stationarity with variance $\\sigma^2$.\n- Generate the output recursively using $y_k = a y_{k-1} + b u_{k-1} + e_k$ with $y_{-1} = 0$ and $u_{-1} = 0$.\n- Form the predictor $\\hat{y}_{k|k-1} = a y_{k-1} + b u_{k-1}$ and compute innovations $\\tilde{y}_k = y_k - \\hat{y}_{k|k-1}$ for $k = 1, \\dots, N-1$.\n- Compute the sample mean, autocovariances, and autocorrelations up to lag $H$, ensuring that $H \\le n - 1$.\n- Compute the LB statistic $Q$, the $p$-value using the chi-square distribution with $H$ degrees of freedom, and decide on whiteness using the threshold $\\alpha$.\n\n6. Test suite coverage and expectations. Case A is a correctly specified ARX model with white noise innovations, and the LB test at $H = 20$ and $\\alpha = 0.05$ is expected to accept whiteness. Case B is correctly specified in dynamics but with colored noise (AutoRegressive of order one), so the innovations are not white and the LB test is expected to reject whiteness. Case C uses a smaller sample size and correct model with white noise, acting as a boundary condition where the LB test is performed with $H = 5$ and $\\alpha = 0.05$; whiteness is expected to be accepted in this controlled setup.\n\n7. Output specification. For the three cases, produce booleans $b_A$, $b_B$, and $b_C$ indicating whether whiteness is accepted. The final output should be a single line string of the form `[b_A,b_B,b_C]`.",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import chi2\n\ndef ljung_box_whiteness(innovations, H, alpha):\n    \"\"\"\n    Compute Ljung-Box statistic for the innovations:\n    - innovations: 1D numpy array of innovations (mean will be removed)\n    - H: number of lags to test\n    - alpha: significance level (decimal)\n    Returns: boolean indicating whether whiteness is accepted (p >= alpha)\n    \"\"\"\n    n = innovations.shape[0]\n    if n < 2:\n        # Not enough data to perform test; conservatively reject whiteness\n        return False\n    # Mean-center innovations\n    z = innovations - np.mean(innovations)\n    # Effective maximum lag cannot exceed n-1\n    H_eff = min(H, n - 1)\n    # Sample variance (normalized by n)\n    gamma0 = np.dot(z, z) / n\n    if gamma0 == 0.0:\n        # All-zero innovations; autocorrelations are zero; accept whiteness\n        return True\n    # Compute sample autocorrelations up to H_eff\n    rhos = []\n    for h in range(1, H_eff + 1):\n        gamma_h = np.dot(z[h:], z[:-h]) / n\n        rhos.append(gamma_h / gamma0)\n    rhos = np.array(rhos)\n    # Ljung-Box Q statistic\n    denom = np.array([n - h for h in range(1, H_eff + 1)], dtype=float)\n    Q = n * (n + 2) * np.sum((rhos ** 2) / denom)\n    # p-value under chi-square with H_eff degrees of freedom\n    p_value = chi2.sf(Q, df=H_eff)\n    return p_value >= alpha\n\ndef generate_arx_series(N, a, b, sigma, omega, c, rng):\n    \"\"\"\n    Generate ARX(1,1) series:\n    y_k = a * y_{k-1} + b * u_{k-1} + e_k\n    with u_k = sin(omega * k) and noise e_k either white (c=0) or colored AR(1) (c != 0).\n    \"\"\"\n    u = np.sin(omega * np.arange(N))\n    y = np.zeros(N)\n    # Generate noise e_k\n    e = np.zeros(N)\n    if abs(c) < 1e-12:\n        # White noise\n        e = rng.normal(loc=0.0, scale=sigma, size=N)\n    else:\n        # Colored noise: e_k = c * e_{k-1} + w_k, w_k ~ N(0, sigma^2 * (1 - c^2))\n        e[0] = rng.normal(loc=0.0, scale=sigma)\n        w_std = sigma * np.sqrt(max(0.0, 1.0 - c ** 2))\n        for k in range(1, N):\n            w_k = rng.normal(loc=0.0, scale=w_std)\n            e[k] = c * e[k - 1] + w_k\n    # Generate y_k recursively\n    # y_0 uses y_{-1}=0 and u_{-1}=0 by convention\n    y[0] = a * 0.0 + b * 0.0 + e[0]\n    for k in range(1, N):\n        y[k] = a * y[k - 1] + b * u[k - 1] + e[k]\n    # Predictor: yhat_{k|k-1} = a y_{k-1} + b u_{k-1}\n    yhat = np.zeros(N)\n    yhat[0] = a * 0.0 + b * 0.0\n    for k in range(1, N):\n        yhat[k] = a * y[k - 1] + b * u[k - 1]\n    # Innovations tilde_y_k = y_k - yhat_{k|k-1}, for k >= 1\n    innovations = y[1:] - yhat[1:]\n    return innovations\n\ndef solve():\n    # Fixed seed for reproducibility\n    rng = np.random.default_rng(42)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A: Correct model, white innovations\n        {\"N\": 600, \"a\": 0.7, \"b\": 0.5, \"sigma\": 0.2, \"omega\": 0.1, \"c\": 0.0, \"H\": 20, \"alpha\": 0.05},\n        # Case B: Correct dynamics, colored noise (non-white innovations)\n        {\"N\": 600, \"a\": 0.7, \"b\": 0.5, \"sigma\": 0.2, \"omega\": 0.1, \"c\": 0.6, \"H\": 20, \"alpha\": 0.05},\n        # Case C: Boundary condition with small data, white innovations\n        {\"N\": 40,  \"a\": 0.3, \"b\": 0.2, \"sigma\": 0.1, \"omega\": 0.2, \"c\": 0.0, \"H\": 5,  \"alpha\": 0.05},\n    ]\n\n    results = []\n    for case in test_cases:\n        innovations = generate_arx_series(\n            N=case[\"N\"], a=case[\"a\"], b=case[\"b\"],\n            sigma=case[\"sigma\"], omega=case[\"omega\"],\n            c=case[\"c\"], rng=rng\n        )\n        decision = ljung_box_whiteness(innovations, H=case[\"H\"], alpha=case[\"alpha\"])\n        results.append(decision)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}