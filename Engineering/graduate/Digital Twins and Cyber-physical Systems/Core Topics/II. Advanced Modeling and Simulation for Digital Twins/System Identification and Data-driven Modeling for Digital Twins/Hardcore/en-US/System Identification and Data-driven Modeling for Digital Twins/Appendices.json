{
    "hands_on_practices": [
        {
            "introduction": "The fidelity of any data-driven model or digital twin begins with the quality of the data it is built upon. A critical first step in this process is setting the sampling frequency, a choice that involves a delicate trade-off. This exercise  moves beyond the textbook Nyquist-Shannon theorem to a practical engineering problem, where you must balance the need to capture a system's essential dynamics against the necessity of suppressing noise and preventing aliasing through a physical anti-aliasing filter.",
            "id": "4249810",
            "problem": "A mechanical cyber-physical system is operated with a Digital Twin that performs system identification from sampled sensor data. The underlying continuous-time plant is modeled as a linear time-invariant (LTI) system with dominant dynamics characterized by a highest significant eigenfrequency $f_{\\max}$, defined as the largest imaginary-part frequency among poles that contribute non-negligibly to the output spectrum within the operating envelope. The data acquisition chain consists of an analog anti-aliasing prefilter followed by an Analog-to-Digital Converter (ADC). The prefilter is a $2$nd-order Butterworth low-pass with cutoff frequency $f_{c}$, and its magnitude response is given by $|H(j 2 \\pi f)| = \\left(1 + (f/f_{c})^{4}\\right)^{-1/2}$. The sampling frequency is $f_{s}$.\n\nTo ensure the fidelity of data-driven modeling for the Digital Twin, two constraints are imposed:\n1. Passband fidelity: At $f_{\\max}$, the magnitude response must satisfy $|H(j 2 \\pi f_{\\max})| \\geq 0.97$.\n2. Anti-alias suppression: At the Nyquist frequency $f_{s}/2$, the magnitude response must satisfy $|H(j 2 \\pi (f_{s}/2))| \\leq 0.01$.\n\nAssume $f_{\\max} = 45$ Hz. Determine the minimum sampling frequency $f_{s}$ that simultaneously satisfies both constraints by choosing $f_{c}$ to minimize $f_{s}$ subject to the constraints, starting from fundamental definitions including the Nyquist-Shannon Sampling Theorem (NSST) and standard filter response properties. Express your final answer in Hz and round your result to four significant figures. The answer must be a single real-valued number.",
            "solution": "The problem requires determining the minimum sampling frequency, $f_s$, for a data acquisition system under two constraints imposed on an anti-aliasing filter. The solution involves translating these system-level requirements into mathematical inequalities and solving an optimization problem.\n\nFirst, we establish the context. The Nyquist-Shannon Sampling Theorem (NSST) is a fundamental principle in digital signal processing. It states that to perfectly reconstruct a continuous-time signal from its samples, the sampling frequency $f_s$ must be strictly greater than twice the signal's highest frequency component, or bandwidth $f_{BW}$. In practice, signals are not perfectly band-limited, and anti-aliasing filters are used to attenuate frequencies above the Nyquist frequency, $f_s/2$, to prevent aliasing, where high-frequency components erroneously appear as lower frequencies in the sampled data.\n\nThe system uses a $2$nd-order Butterworth low-pass filter with a magnitude response given by:\n$$|H(j 2 \\pi f)| = \\frac{1}{\\sqrt{1 + \\left(\\frac{f}{f_c}\\right)^4}}$$\nwhere $f$ is the frequency and $f_c$ is the cutoff frequency of the filter.\n\nThe problem presents two constraints on this filter's performance, which we will formalize into mathematical inequalities.\n\nConstraint 1: Passband fidelity.\nThe filter must pass the signal components of interest with minimal attenuation. The highest significant frequency of the plant is $f_{\\max} = 45$ Hz. At this frequency, the magnitude response must be at least $0.97$.\n$$|H(j 2 \\pi f_{\\max})| \\geq 0.97$$\nSubstituting the filter response equation and the value of $f_{\\max}$:\n$$\\frac{1}{\\sqrt{1 + \\left(\\frac{f_{\\max}}{f_c}\\right)^4}} \\geq 0.97$$\nTo solve for $f_c$, we manipulate the inequality. Since both sides are positive, we can square them:\n$$\\frac{1}{1 + \\left(\\frac{f_{\\max}}{f_c}\\right)^4} \\geq 0.97^2$$\nTaking the reciprocal reverses the inequality sign:\n$$1 + \\left(\\frac{f_{\\max}}{f_c}\\right)^4 \\leq \\frac{1}{0.97^2}$$\n$$\\left(\\frac{f_{\\max}}{f_c}\\right)^4 \\leq \\frac{1}{0.97^2} - 1$$\nSince the right-hand side is positive, we can proceed:\n$$\\frac{f_{\\max}^4}{f_c^4} \\leq \\frac{1 - 0.97^2}{0.97^2}$$\n$$f_c^4 \\geq \\frac{f_{\\max}^4 \\cdot 0.97^2}{1 - 0.97^2} = f_{\\max}^4 \\left(\\left(\\frac{1}{0.97^2}\\right) - 1\\right)^{-1}$$\nTaking the positive fourth root, we find a lower bound for $f_c$:\n$$f_c \\geq f_{\\max} \\left(\\left(\\frac{1}{0.97}\\right)^2 - 1\\right)^{-1/4}$$\n\nConstraint 2: Anti-alias suppression.\nThe filter must sufficiently attenuate frequencies at or above the Nyquist frequency, $f_s/2$, to prevent aliasing. At the frequency $f = f_s/2$, the magnitude response must be no more than $0.01$.\n$$|H(j 2 \\pi (f_s/2))| \\leq 0.01$$\nSubstituting the filter response equation:\n$$\\frac{1}{\\sqrt{1 + \\left(\\frac{f_s/2}{f_c}\\right)^4}} \\leq 0.01$$\nAgain, we square both sides:\n$$\\frac{1}{1 + \\left(\\frac{f_s}{2f_c}\\right)^4} \\leq 0.01^2$$\nTaking the reciprocal:\n$$1 + \\left(\\frac{f_s}{2f_c}\\right)^4 \\geq \\frac{1}{0.01^2} = 10000$$\n$$\\left(\\frac{f_s}{2f_c}\\right)^4 \\geq 10000 - 1 = 9999$$\nTaking the positive fourth root:\n$$\\frac{f_s}{2f_c} \\geq (9999)^{1/4}$$\nThis gives us a lower bound for $f_s$ as a function of $f_c$:\n$$f_s \\geq 2 f_c (9999)^{1/4}$$\n\nOptimization Problem.\nWe want to find the minimum possible value of $f_s$. The expression for the lower bound of $f_s$ is a monotonically increasing function of $f_c$. Therefore, to minimize $f_s$, we must choose the smallest possible value for $f_c$ that is permitted by Constraint 1. The minimal $f_c$ is achieved when the first inequality becomes an equality:\n$$f_{c, \\text{opt}} = f_{\\max} \\left(\\left(\\frac{1}{0.97}\\right)^2 - 1\\right)^{-1/4}$$\nSubstituting this optimal choice of $f_c$ into the inequality for $f_s$ gives the minimum required sampling frequency, $f_{s, \\text{min}}$:\n$$f_{s, \\text{min}} = 2 f_{c, \\text{opt}} (9999)^{1/4}$$\n$$f_{s, \\text{min}} = 2 \\left( f_{\\max} \\left(\\left(\\frac{1}{0.97}\\right)^2 - 1\\right)^{-1/4} \\right) (9999)^{1/4}$$\nWe can combine the terms under a single root:\n$$f_{s, \\text{min}} = 2 f_{\\max} \\left( \\frac{9999}{\\left(\\frac{1}{0.97}\\right)^2 - 1} \\right)^{1/4}$$\n\nNumerical Calculation.\nNow, we substitute the given value $f_{\\max} = 45$ Hz into the expression.\n$$f_{s, \\text{min}} = 2 \\cdot 45 \\left( \\frac{9999}{\\frac{1}{0.97^2} - 1} \\right)^{1/4}$$\n$$f_{s, \\text{min}} = 90 \\left( \\frac{9999}{\\frac{1 - 0.9409}{0.9409}} \\right)^{1/4}$$\n$$f_{s, \\text{min}} = 90 \\left( \\frac{9999 \\cdot 0.9409}{0.0591} \\right)^{1/4}$$\n$$f_{s, \\text{min}} = 90 \\left( 159187.80408... \\right)^{1/4}$$\n$$f_{s, \\text{min}} = 90 \\left( 19.974636... \\right)$$\n$$f_{s, \\text{min}} = 1797.7172... \\text{ Hz}$$\nThe problem requires the answer to be rounded to four significant figures.\n$$f_{s, \\text{min}} \\approx 1798 \\text{ Hz}$$\nThis value represents the minimum sampling frequency that allows for the design of a $2$nd-order Butterworth filter satisfying both the passband fidelity and anti-aliasing suppression requirements.",
            "answer": "$$\\boxed{1798}$$"
        },
        {
            "introduction": "Once a dynamic model has been identified from input-output data, it is often represented as a transfer function. However, for many modern control and simulation tasks, a state-space representation is far more powerful and versatile. This practice  guides you through the fundamental process of converting a given discrete-time transfer function into a controllable canonical state-space form, a standard and systematic procedure. You will then verify the controllability of the resulting model, a crucial property that determines if the system's state can be steered to any desired value by the input.",
            "id": "4249812",
            "problem": "A data-driven Digital Twin (DT) for a Cyber-Physical System (CPS) uses an identified discrete-time Single-Input Single-Output (SISO) Linear Time-Invariant (LTI) model to emulate a thermal-fluid subsystem under sampled actuation. From input-output data, the identified transfer function in the $z$-domain is\n$$\nG(z) \\;=\\; \\frac{b_0 + b_1 z^{-1} + b_2 z^{-2}}{1 + a_1 z^{-1} + a_2 z^{-2} + a_3 z^{-3}},\n$$\nwith coefficients $a_1 = 0.7$, $a_2 = -0.25$, $a_3 = 0.04$, and $b_0 = 0.15$, $b_1 = 0.05$, $b_2 = 0.02$. Starting from the definition of the causal discrete-time SISO LTI difference equation implied by $G(z)$ and the algebraic properties of the $z$-transform, derive a controllable canonical State-Space (SS) realization $\\{A, B, C, D\\}$ such that $G(z) = C (z I - A)^{-1} B + D$. Then, construct the controllability matrix $\\mathcal{C} = [\\,B \\;\\; A B \\;\\; A^{2} B\\,]$ and compute its rank. Provide the final rank as an integer. No rounding is required, and no units are needed for the final answer.",
            "solution": "The starting point is the standard causal discrete-time SISO Linear Time-Invariant (LTI) representation implied by a rational transfer function written in powers of $z^{-1}$. For\n$$\nG(z) \\;=\\; \\frac{b_0 + b_1 z^{-1} + b_2 z^{-2}}{1 + a_1 z^{-1} + a_2 z^{-2} + a_3 z^{-3}},\n$$\nthe corresponding time-domain difference equation follows from multiplying both sides by the denominator and applying the inverse $z$-transform (properties of the $z$-transform ensure shifts map to powers of $z^{-1}$). This yields\n$$\ny[k] + a_1 y[k-1] + a_2 y[k-2] + a_3 y[k-3] \\;=\\; b_0 u[k] + b_1 u[k-1] + b_2 u[k-2].\n$$\nWe seek a controllable canonical State-Space (SS) realization $\\{A,B,C,D\\}$ satisfying\n$$\nx[k+1] \\;=\\; A x[k] + B u[k], \\qquad y[k] \\;=\\; C x[k] + D u[k],\n$$\nwith $G(z) = C (z I - A)^{-1} B + D$.\n\nA standard controllable companion form uses the denominator coefficients to define a shift-register state with the companion matrix. For an order-$3$ denominator,\n$$\nA \\;=\\; \\begin{bmatrix}\n0 & 1 & 0 \\\\\n0 & 0 & 1 \\\\\n-a_3 & -a_2 & -a_1\n\\end{bmatrix}, \\qquad B \\;=\\; \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix}.\n$$\nSubstituting the identified parameters $a_1 = 0.7$, $a_2 = -0.25$, $a_3 = 0.04$, we obtain\n$$\nA \\;=\\; \\begin{bmatrix}\n0 & 1 & 0 \\\\\n0 & 0 & 1 \\\\\n-0.04 & 0.25 & -0.7\n\\end{bmatrix}, \\qquad B \\;=\\; \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix}.\n$$\nTo determine $C$ and $D$, we match $G(z) = C (z I - A)^{-1} B + D$ to the given rational form. Compute $(z I - A)^{-1} B$ explicitly by solving $(z I - A) x = B$ for $x$. Let $x = [x_1 \\;\\; x_2 \\;\\; x_3]^{\\top}$. The linear system is\n$$\nz x_1 - x_2 = 0, \\quad z x_2 - x_3 = 0, \\quad a_3 x_1 + a_2 x_2 + (z + a_1) x_3 = 1.\n$$\nFrom the first two equations, $x_2 = z x_1$ and $x_3 = z x_2 = z^2 x_1$. Substitute into the third:\n$$\nx_1 \\left( a_3 + a_2 z + (z + a_1) z^2 \\right) \\;=\\; 1 \\;\\;\\Rightarrow\\;\\; x_1 \\;=\\; \\frac{1}{z^3 + a_1 z^2 + a_2 z + a_3}.\n$$\nTherefore,\n$$\n(z I - A)^{-1} B \\;=\\; \\frac{1}{z^3 + a_1 z^2 + a_2 z + a_3} \\begin{bmatrix} 1 \\\\ z \\\\ z^2 \\end{bmatrix}.\n$$\nIt follows that\n$$\nG(z) \\;=\\; \\frac{C \\begin{bmatrix} 1 \\\\ z \\\\ z^2 \\end{bmatrix}}{z^3 + a_1 z^2 + a_2 z + a_3} + D \\;=\\; \\frac{c_1 + c_2 z + c_3 z^2}{z^3 + a_1 z^2 + a_2 z + a_3} + D,\n$$\nwhere $C = [c_1 \\;\\; c_2 \\;\\; c_3]$. To compare with a representation in powers of $z^{-1}$, multiply numerator and denominator by $z^{-3}$:\n$$\nG(z) \\;=\\; \\frac{c_1 z^{-3} + c_2 z^{-2} + c_3 z^{-1}}{1 + a_1 z^{-1} + a_2 z^{-2} + a_3 z^{-3}} + D.\n$$\nMatching coefficients with the identified model\n$$\nG(z) \\;=\\; \\frac{b_0 + b_1 z^{-1} + b_2 z^{-2}}{1 + a_1 z^{-1} + a_2 z^{-2} + a_3 z^{-3}},\n$$\nrequires\n$$\nc_1 \\;=\\; 0, \\quad c_2 \\;=\\; b_2, \\quad c_3 \\;=\\; b_1, \\quad D \\;=\\; b_0.\n$$\nThus,\n$$\nC \\;=\\; \\begin{bmatrix} 0 & b_2 & b_1 \\end{bmatrix} \\;=\\; \\begin{bmatrix} 0 & 0.02 & 0.05 \\end{bmatrix}, \\qquad D \\;=\\; 0.15.\n$$\nWe have the controllable canonical SS realization:\n$$\nA \\;=\\; \\begin{bmatrix}\n0 & 1 & 0 \\\\\n0 & 0 & 1 \\\\\n-0.04 & 0.25 & -0.7\n\\end{bmatrix}, \\quad\nB \\;=\\; \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix}, \\quad\nC \\;=\\; \\begin{bmatrix} 0 & 0.02 & 0.05 \\end{bmatrix}, \\quad\nD \\;=\\; 0.15.\n$$\n\nWe now verify controllability by computing the controllability matrix\n$$\n\\mathcal{C} \\;=\\; \\begin{bmatrix} B & A B & A^2 B \\end{bmatrix}.\n$$\nCompute the columns sequentially:\n$$\nB \\;=\\; \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix},\n\\quad\nA B \\;=\\; \\begin{bmatrix} 0 \\\\ 1 \\\\ -0.7 \\end{bmatrix},\n\\quad\nA^2 B \\;=\\; A \\begin{bmatrix} 0 \\\\ 1 \\\\ -0.7 \\end{bmatrix}\n\\;=\\; \\begin{bmatrix}\n1 \\\\\n-0.7 \\\\\n0.25 + 0.49\n\\end{bmatrix}\n\\;=\\; \\begin{bmatrix} 1 \\\\ -0.7 \\\\ 0.74 \\end{bmatrix}.\n$$\nTherefore,\n$$\n\\mathcal{C} \\;=\\; \\begin{bmatrix}\n0 & 0 & 1 \\\\\n0 & 1 & -0.7 \\\\\n1 & -0.7 & 0.74\n\\end{bmatrix}.\n$$\nThe rank equals the number of linearly independent columns, which we can confirm by computing the determinant (since $\\mathcal{C}$ is square for this SISO order-$3$ case):\n$$\n\\det(\\mathcal{C}) \\;=\\; 0 \\cdot \\det\\begin{bmatrix} 1 & -0.7 \\\\ -0.7 & 0.74 \\end{bmatrix}\n- 0 \\cdot \\det\\begin{bmatrix} 0 & -0.7 \\\\ 1 & 0.74 \\end{bmatrix}\n+ 1 \\cdot \\det\\begin{bmatrix} 0 & 1 \\\\ 1 & -0.7 \\end{bmatrix}\n\\;=\\; 1 \\cdot \\left( 0 \\cdot (-0.7) - 1 \\cdot 1 \\right) \\;=\\; -1.\n$$\nSince $\\det(\\mathcal{C}) = -1 \\neq 0$, the columns are linearly independent and the rank is $3$. This confirms the pair $(A,B)$ is controllable, as expected for controllable companion form.\n\nThe requested final answer is the rank of $\\mathcal{C}$ expressed as an integer.",
            "answer": "$$\\boxed{3}$$"
        },
        {
            "introduction": "A model is only as good as its ability to predict. A central tenet of system identification is that a good model should capture all the systematic, predictable behavior in a system, leaving behind only unpredictable, random noise in the residuals. This hands-on coding exercise  allows you to put this principle into practice by performing a residual analysis. You will compute the prediction errors (innovations) from a given model and use the formal Ljung-Box statistical test to verify if they are \"white\"—a key indicator of model adequacy.",
            "id": "4249835",
            "problem": "Consider a discrete-time Linear Time-Invariant (LTI) Single-Input Single-Output (SISO) process described by an AutoRegressive with eXogenous input (ARX) model. Let the measured output sequence be $\\{y_k\\}_{k=0}^{N-1}$, the known input sequence be $\\{u_k\\}_{k=0}^{N-1}$, and the one-step-ahead output predictor be given by $\\hat{y}_{k|k-1}$, which is the prediction of $y_k$ using information available up to time $k-1$. The innovations sequence is defined by $\\tilde{y}_k = y_k - \\hat{y}_{k|k-1}$. Under correct modeling with Independent and Identically Distributed (IID) measurement noise, the innovations $\\{\\tilde{y}_k\\}$ should be a white-noise sequence. To assess whiteness, use the Ljung–Box (LB) statistic, which tests the joint null hypothesis that the autocorrelations of $\\{\\tilde{y}_k\\}$ at lags $1$ through $H$ are all equal to zero.\n\nStarting from fundamental definitions of discrete-time prediction and sample autocorrelation, compute the innovations sequence from a specified predictor and then compute the LB statistic and its associated decision at a given significance level. The LB test should be implemented in a way that is consistent with its asymptotic justification under the IID assumption when the model parameters used in the predictor are treated as given (not estimated from the data). The decision rule is to accept whiteness if the LB test $p$-value is greater than or equal to the significance level $\\alpha$.\n\nFor each test case below, the process is generated as\n$$\ny_k = a\\,y_{k-1} + b\\,u_{k-1} + e_k,\n$$\nwith initial condition $y_{-1} = 0$ and $u_{-1} = 0$. The predictor uses the known coefficients $a$ and $b$ and information up to $k-1$:\n$$\n\\hat{y}_{k|k-1} = a\\,y_{k-1} + b\\,u_{k-1}.\n$$\nThe input is deterministic:\n$$\nu_k = \\sin(\\omega k),\n$$\nwith angles in radians. The noise sequence is either white, $e_k \\sim \\mathcal{N}(0,\\sigma^2)$, or colored by an AutoRegressive of order one process, $e_k = c\\,e_{k-1} + w_k$, where $w_k \\sim \\mathcal{N}(0,\\sigma^2 (1 - c^2))$ so that the stationary variance of $e_k$ is $\\sigma^2$. The initial noise sample $e_0$ is drawn from $\\mathcal{N}(0,\\sigma^2)$. Use a fixed pseudo-random number generator seed to ensure reproducibility.\n\nFor each test case, perform the following tasks:\n- Generate $\\{u_k\\}$ and $\\{y_k\\}$ for the given parameters.\n- Compute the innovations $\\{\\tilde{y}_k\\}$ for $k=1,\\dots,N-1$.\n- Compute sample autocorrelations $\\hat{\\rho}_h$ for lags $h=1,\\dots,H$, based on the mean-centered innovations and a normalization consistent with asymptotic properties under the IID assumption.\n- Compute the Ljung–Box statistic using the sample autocorrelations and evaluate the $p$-value under a chi-square distribution with $H$ degrees of freedom, treating the predictor parameters as given.\n- Decide whiteness based on the rule \"accept whiteness if $p \\ge \\alpha$\".\n\nTest Suite:\n- Case A (correct model, white innovations, happy path):\n  - $N = 600$, $a = 0.7$, $b = 0.5$, $\\sigma = 0.2$, $\\omega = 0.1$, $c = 0.0$ (white), $H = 20$, $\\alpha = 0.05$ (expressed as a decimal).\n- Case B (correct model dynamics but colored noise, non-white innovations):\n  - $N = 600$, $a = 0.7$, $b = 0.5$, $\\sigma = 0.2$, $\\omega = 0.1$, $c = 0.6$ (colored), $H = 20$, $\\alpha = 0.05$ (expressed as a decimal).\n- Case C (correct model, white innovations, boundary condition with small data):\n  - $N = 40$, $a = 0.3$, $b = 0.2$, $\\sigma = 0.1$, $\\omega = 0.2$, $c = 0.0$ (white), $H = 5$, $\\alpha = 0.05$ (expressed as a decimal).\n\nImplementation Requirements:\n- Use a single fixed seed for the pseudo-random number generator to ensure deterministic outputs.\n- For each case, produce a boolean indicating whether whiteness is accepted at significance level $\\alpha$.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[resultA,resultB,resultC]\"), where each result is a boolean value \"True\" or \"False\".\n\nFinal Output Format:\n- A single line string in the form $[b_A,b_B,b_C]$, where $b_A$, $b_B$, and $b_C$ are booleans corresponding to the whiteness decision for Case A, Case B, and Case C, respectively.",
            "solution": "The derivation proceeds from fundamental definitions in discrete-time stochastic modeling and hypothesis testing. The AutoRegressive with eXogenous input (ARX) model is a standard representation for discrete-time processes, and it is consistent with the modeling of digital twins for cyber-physical systems where a data-driven predictor is deployed to produce one-step-ahead output forecasts.\n\n1. Fundamental process and predictor definitions. The process is given by\n$$\ny_k = a\\,y_{k-1} + b\\,u_{k-1} + e_k,\n$$\nwith known deterministic input\n$$\nu_k = \\sin(\\omega k),\n$$\nwhere the angle is specified in radians. The one-step-ahead predictor uses information up to time $k-1$:\n$$\n\\hat{y}_{k|k-1} = a\\,y_{k-1} + b\\,u_{k-1}.\n$$\nThese relations follow from the definition of an AutoRegressive with eXogenous input model and the basic concept of one-step-ahead prediction in discrete time, which is derived from causality and the shift operator properties in time series.\n\n2. Innovations definition. The innovations are the prediction errors:\n$$\n\\tilde{y}_k = y_k - \\hat{y}_{k|k-1}.\n$$\nSubstituting the process and predictor,\n$$\n\\tilde{y}_k = \\left(a\\,y_{k-1} + b\\,u_{k-1} + e_k\\right) - \\left(a\\,y_{k-1} + b\\,u_{k-1}\\right) = e_k.\n$$\nUnder the assumption of white noise, $e_k$ is an Independent and Identically Distributed (IID) sequence with zero mean and finite variance, which implies that $\\{\\tilde{y}_k\\}$ should be a white-noise sequence. When the measurement noise is colored (for example, an AutoRegressive of order one sequence), the innovations inherit autocorrelation and are not white.\n\n3. Sample autocorrelation. To test whiteness, we compute sample autocorrelations from the observed innovations. Let $\\tilde{y}_k$ be the innovations for $k = 1, \\dots, n$, where $n = N-1$ is the available sample size for well-defined predictions. The sample mean is\n$$\n\\bar{\\tilde{y}} = \\frac{1}{n}\\sum_{k=1}^{n} \\tilde{y}_k.\n$$\nDefine mean-centered innovations $z_k = \\tilde{y}_k - \\bar{\\tilde{y}}$. The sample autocovariance at lag $h$ is\n$$\n\\hat{\\gamma}_h = \\frac{1}{n}\\sum_{k=h+1}^{n} z_k z_{k-h}.\n$$\nThe sample variance is $\\hat{\\gamma}_0 = \\frac{1}{n}\\sum_{k=1}^{n} z_k^2$. The sample autocorrelation at lag $h$ is then\n$$\n\\hat{\\rho}_h = \\frac{\\hat{\\gamma}_h}{\\hat{\\gamma}_0}.\n$$\nThese definitions are based on the standard estimators for autocovariance and autocorrelation under stationarity, and the normalization by $n$ aligns with asymptotic properties under the IID assumption.\n\n4. Ljung–Box (LB) statistic. The LB statistic jointly tests the null hypothesis that $\\hat{\\rho}_h = 0$ for $h = 1, 2, \\dots, H$. Under the null hypothesis and for large $n$, the LB statistic is approximately distributed as a chi-square random variable with $H$ degrees of freedom when the predictor parameters are treated as given. The LB statistic aggregates the squared autocorrelations with a finite-sample correction factor to mitigate bias:\n$$\nQ = n(n+2)\\sum_{h=1}^{H}\\frac{\\hat{\\rho}_h^2}{n - h}.\n$$\nThe $p$-value is computed under a chi-square distribution with $H$ degrees of freedom:\n$$\np = \\Pr\\left(\\chi^2_H \\ge Q\\right).\n$$\nThe decision rule is to accept whiteness if $p \\ge \\alpha$ and reject otherwise. This is grounded in large-sample theory, where under the IID assumption the vector of sample autocorrelations is asymptotically normal with zero mean, and the quadratic form above yields the chi-square approximation.\n\n5. Algorithmic steps. For each test case:\n- Generate the input $u_k = \\sin(\\omega k)$ for $k = 0, \\dots, N-1$.\n- Generate the noise $e_k$:\n  - White noise case: $e_k \\sim \\mathcal{N}(0,\\sigma^2)$ IID.\n  - Colored noise case: $e_k = c\\,e_{k-1} + w_k$ with $w_k \\sim \\mathcal{N}(0, \\sigma^2 (1 - c^2))$ and $e_0 \\sim \\mathcal{N}(0,\\sigma^2)$ to ensure stationarity with variance $\\sigma^2$.\n- Generate the output recursively using $y_k = a y_{k-1} + b u_{k-1} + e_k$ with $y_{-1} = 0$ and $u_{-1} = 0$.\n- Form the predictor $\\hat{y}_{k|k-1} = a y_{k-1} + b u_{k-1}$ and compute innovations $\\tilde{y}_k = y_k - \\hat{y}_{k|k-1}$ for $k = 1, \\dots, N-1$.\n- Compute the sample mean, autocovariances, and autocorrelations up to lag $H$, ensuring that $H \\le n - 1$.\n- Compute the LB statistic $Q$, the $p$-value using the chi-square distribution with $H$ degrees of freedom, and decide on whiteness using the threshold $\\alpha$.\n\n6. Test suite coverage and expectations. Case A is a correctly specified ARX model with white noise innovations, and the LB test at $H = 20$ and $\\alpha = 0.05$ is expected to accept whiteness. Case B is correctly specified in dynamics but with colored noise (AutoRegressive of order one), so the innovations are not white and the LB test is expected to reject whiteness. Case C uses a smaller sample size and correct model with white noise, acting as a boundary condition where the LB test is performed with $H = 5$ and $\\alpha = 0.05$; whiteness is expected to be accepted in this controlled setup.\n\n7. Output specification. For the three cases, produce booleans $b_A$, $b_B$, and $b_C$ indicating whether whiteness is accepted. The final output should be a single line string of the form\n$$\n[b_A,b_B,b_C].\n$$",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import chi2\n\ndef ljung_box_whiteness(innovations, H, alpha):\n    \"\"\"\n    Compute Ljung-Box statistic for the innovations:\n    - innovations: 1D numpy array of innovations (mean will be removed)\n    - H: number of lags to test\n    - alpha: significance level (decimal)\n    Returns: boolean indicating whether whiteness is accepted (p >= alpha)\n    \"\"\"\n    n = innovations.shape[0]\n    if n  2:\n        # Not enough data to perform test; conservatively reject whiteness\n        return False\n    # Mean-center innovations\n    z = innovations - np.mean(innovations)\n    # Effective maximum lag cannot exceed n-1\n    H_eff = min(H, n - 1)\n    # Sample variance (normalized by n)\n    gamma0 = np.dot(z, z) / n\n    if gamma0 == 0.0:\n        # All-zero innovations; autocorrelations are zero; accept whiteness\n        return True\n    # Compute sample autocorrelations up to H_eff\n    rhos = []\n    for h in range(1, H_eff + 1):\n        gamma_h = np.dot(z[h:], z[:-h]) / n\n        rhos.append(gamma_h / gamma0)\n    rhos = np.array(rhos)\n    # Ljung-Box Q statistic\n    denom = np.array([n - h for h in range(1, H_eff + 1)], dtype=float)\n    Q = n * (n + 2) * np.sum((rhos ** 2) / denom)\n    # p-value under chi-square with H_eff degrees of freedom\n    p_value = chi2.sf(Q, df=H_eff)\n    return p_value >= alpha\n\ndef generate_arx_series(N, a, b, sigma, omega, c, rng):\n    \"\"\"\n    Generate ARX(1,1) series:\n    y_k = a * y_{k-1} + b * u_{k-1} + e_k\n    with u_k = sin(omega * k) and noise e_k either white (c=0) or colored AR(1) (c != 0).\n    \"\"\"\n    u = np.sin(omega * np.arange(N))\n    y = np.zeros(N)\n    # Generate noise e_k\n    e = np.zeros(N)\n    if abs(c)  1e-12:\n        # White noise\n        e = rng.normal(loc=0.0, scale=sigma, size=N)\n    else:\n        # Colored noise: e_k = c * e_{k-1} + w_k, w_k ~ N(0, sigma^2 * (1 - c^2))\n        e[0] = rng.normal(loc=0.0, scale=sigma)\n        w_std = sigma * np.sqrt(max(0.0, 1.0 - c ** 2))\n        for k in range(1, N):\n            w_k = rng.normal(loc=0.0, scale=w_std)\n            e[k] = c * e[k - 1] + w_k\n    # Generate y_k recursively\n    # y_0 uses y_{-1}=0 and u_{-1}=0 by convention\n    y[0] = a * 0.0 + b * 0.0 + e[0]\n    for k in range(1, N):\n        y[k] = a * y[k - 1] + b * u[k - 1] + e[k]\n    # Predictor: yhat_{k|k-1} = a y_{k-1} + b u_{k-1}\n    yhat = np.zeros(N)\n    yhat[0] = a * 0.0 + b * 0.0\n    for k in range(1, N):\n        yhat[k] = a * y[k - 1] + b * u[k - 1]\n    # Innovations tilde_y_k = y_k - yhat_{k|k-1}, for k >= 1\n    innovations = y[1:] - yhat[1:]\n    return innovations\n\ndef solve():\n    # Fixed seed for reproducibility\n    rng = np.random.default_rng(42)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A: Correct model, white innovations\n        {\"N\": 600, \"a\": 0.7, \"b\": 0.5, \"sigma\": 0.2, \"omega\": 0.1, \"c\": 0.0, \"H\": 20, \"alpha\": 0.05},\n        # Case B: Correct dynamics, colored noise (non-white innovations)\n        {\"N\": 600, \"a\": 0.7, \"b\": 0.5, \"sigma\": 0.2, \"omega\": 0.1, \"c\": 0.6, \"H\": 20, \"alpha\": 0.05},\n        # Case C: Boundary condition with small data, white innovations\n        {\"N\": 40,  \"a\": 0.3, \"b\": 0.2, \"sigma\": 0.1, \"omega\": 0.2, \"c\": 0.0, \"H\": 5,  \"alpha\": 0.05},\n    ]\n\n    results = []\n    for case in test_cases:\n        innovations = generate_arx_series(\n            N=case[\"N\"], a=case[\"a\"], b=case[\"b\"],\n            sigma=case[\"sigma\"], omega=case[\"omega\"],\n            c=case[\"c\"], rng=rng\n        )\n        decision = ljung_box_whiteness(innovations, H=case[\"H\"], alpha=case[\"alpha\"])\n        results.append(decision)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}