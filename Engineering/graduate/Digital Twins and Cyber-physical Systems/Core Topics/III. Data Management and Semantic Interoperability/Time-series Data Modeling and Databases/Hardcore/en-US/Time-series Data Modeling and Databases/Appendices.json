{
    "hands_on_practices": [
        {
            "introduction": "Efficiently handling high-frequency sensor data is a cornerstone of digital twin applications. This first practice explores a common technique for managing data volume: timestamp compression using delta-of-delta encoding and quantization. By working through this problem, you will derive the worst-case reconstruction error, providing a quantitative understanding of the critical trade-off between storage efficiency and data fidelity. This analysis is essential for designing time-series databases that meet both performance and accuracy requirements. ",
            "id": "4251343",
            "problem": "A Cyber-Physical System (CPS) uses a Digital Twin to model and analyze a high-frequency sensor stream. The sensor emits a sequence of timestamps $\\{T_i\\}_{i=0}^{N}$ where $T_i \\in \\mathbb{R}$, with $N \\geq 2$. The physical sampling is nearly periodic with a nominal interval $P > 0$, but is subject to bounded timing fluctuations due to clock drift and scheduling jitter. Define the first differences $D_i = T_i - T_{i-1}$ for $i \\in \\{1,2,\\dots,N\\}$ and the second differences $S_i = D_i - D_{i-1}$ for $i \\in \\{2,3,\\dots,N\\}$. Empirical hardware characterization guarantees a bound $|S_i| \\leq R$ for all $i \\in \\{2,\\dots,N\\}$, where $R > 0$ is known.\n\nTo reduce storage, the system compresses the second differences $\\{S_i\\}_{i=2}^{N}$ using a uniform scalar quantizer with $2^b$ equally spaced reconstruction levels covering the interval $[-R, R]$, where $b \\in \\mathbb{N}$ is the fixed number of bits per second difference. The initial timestamp $T_0$ and the first difference $D_1$ are stored without loss. Let the quantizer step size be $\\Delta$ and denote the quantization error for $S_i$ by $e_i = \\hat{S}_i - S_i$, where $\\hat{S}_i$ is the quantized value reconstructed at the Digital Twin. Assume no clipping occurs (i.e., the dynamic range perfectly covers all realizable $S_i$ so that $|e_i| \\leq \\Delta/2$ for all $i$).\n\nReconstruction at the Digital Twin proceeds by integrating the compressed second differences:\n- Initialize $\\hat{T}_0 = T_0$ and $\\hat{D}_1 = D_1$.\n- For $i \\in \\{2,\\dots,N\\}$, compute $\\hat{D}_i = \\hat{D}_{i-1} + \\hat{S}_i$.\n- For $i \\in \\{1,\\dots,N\\}$, compute $\\hat{T}_i = \\hat{T}_{i-1} + \\hat{D}_i$.\n\nStarting from the definitions of first and second differences and the uniform quantization model, derive a closed-form expression for the worst-case absolute timestamp reconstruction error at index $N$, namely $\\max |\\hat{T}_N - T_N|$, as a function of $N$, $R$, and $b$. Your derivation must explicitly use the bounded error property $|e_i| \\leq \\Delta/2$ and the relationship between $\\Delta$, $R$, and $b$ implied by a uniform quantizer with $2^b$ levels over $[-R, R]$. Express your final answer as a single closed-form analytic expression in terms of $N$, $R$, and $b$. Do not provide a numerical approximation.",
            "solution": "The problem is first validated against the established criteria.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- Sequence of timestamps: $\\{T_i\\}_{i=0}^{N}$ where $T_i \\in \\mathbb{R}$ and $N \\geq 2$.\n- First differences: $D_i = T_i - T_{i-1}$ for $i \\in \\{1, 2, \\dots, N\\}$.\n- Second differences: $S_i = D_i - D_{i-1}$ for $i \\in \\{2, 3, \\dots, N\\}$.\n- Bound on second differences: $|S_i| \\leq R$ for all $i \\in \\{2, \\dots, N\\}$, with $R > 0$.\n- Quantizer parameters: $2^b$ levels for $b \\in \\mathbb{N}$ bits, covering the interval $[-R, R]$.\n- Lossless storage: $T_0$ and $D_1$ are known perfectly.\n- Quantizer step size: $\\Delta$.\n- Quantized second difference: $\\hat{S}_i$.\n- Quantization error: $e_i = \\hat{S}_i - S_i$.\n- Quantization error bound: $|e_i| \\leq \\Delta/2$.\n- Reconstruction initialization: $\\hat{T}_0 = T_0$ and $\\hat{D}_1 = D_1$.\n- Reconstruction steps:\n  - $\\hat{D}_i = \\hat{D}_{i-1} + \\hat{S}_i$ for $i \\in \\{2, \\dots, N\\}$.\n  - $\\hat{T}_i = \\hat{T}_{i-1} + \\hat{D}_i$ for $i \\in \\{1, \\dots, N\\}$.\n- Objective: Find a closed-form expression for the worst-case error $\\max |\\hat{T}_N - T_N|$ in terms of $N$, $R$, and $b$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded in digital signal processing and error analysis, representing a common scenario in Cyber-Physical Systems. It is mathematically well-posed, with all variables, conditions, and objectives clearly defined. The language is precise and objective. The problem is self-contained, providing all necessary information for derivation, including the relationship between quantizer parameters and the error bound. There are no contradictions or unrealistic physical assumptions. The problem is a valid and formalizable challenge in engineering mathematics.\n\n**Step 3: Verdict and Action**\nThe problem is deemed **valid**. A full solution will be provided.\n\n### Derivation of the Solution\n\nThe goal is to find the worst-case reconstruction error for the timestamp $T_N$, which is defined as $\\max |\\hat{T}_N - T_N|$. We begin by deriving closed-form expressions for both $T_N$ and $\\hat{T}_N$.\n\nFirst, we express the true timestamp $T_N$ in terms of its initial conditions $T_0$, $D_1$, and the sequence of second differences $\\{S_i\\}$.\nThe definition of the first difference $D_i = T_i - T_{i-1}$ implies that $T_N$ can be expressed as a sum:\n$$T_N = T_0 + \\sum_{i=1}^{N} D_i$$\nThe definition of the second difference $S_i = D_i - D_{i-1}$ implies that $D_i$ for $i \\geq 2$ can be expressed as:\n$$D_i = D_1 + \\sum_{j=2}^{i} S_j$$\nSubstituting this expression for $D_i$ into the equation for $T_N$:\n$$T_N = T_0 + D_1 + \\sum_{i=2}^{N} D_i = T_0 + D_1 + \\sum_{i=2}^{N} \\left(D_1 + \\sum_{j=2}^{i} S_j\\right)$$\n$$T_N = T_0 + D_1 + (N-1)D_1 + \\sum_{i=2}^{N} \\sum_{j=2}^{i} S_j$$\n$$T_N = T_0 + N D_1 + \\sum_{i=2}^{N} \\sum_{j=2}^{i} S_j$$\nThe double summation can be simplified by changing the order of summation. A term $S_k$ (for $k \\in \\{2, \\dots, N\\}$) is included in the sum for each $i$ such that $k \\leq i \\leq N$. The number of such values of $i$ is $N-k+1$. Therefore, the double summation is equivalent to:\n$$\\sum_{i=2}^{N} \\sum_{j=2}^{i} S_j = \\sum_{k=2}^{N} (N-k+1)S_k$$\nThis gives the closed-form expression for $T_N$:\n$$T_N = T_0 + N D_1 + \\sum_{k=2}^{N} (N-k+1)S_k$$\nThe reconstruction process for $\\hat{T}_N$ follows identical recursive definitions, starting from $\\hat{T}_0 = T_0$ and $\\hat{D}_1 = D_1$ and using the quantized second differences $\\{\\hat{S}_i\\}$. By direct analogy, the expression for $\\hat{T}_N$ is:\n$$\\hat{T}_N = \\hat{T}_0 + N \\hat{D}_1 + \\sum_{k=2}^{N} (N-k+1)\\hat{S}_k$$\nGiven that $\\hat{T}_0 = T_0$ and $\\hat{D}_1 = D_1$, the reconstruction error at index $N$, denoted $E_N = \\hat{T}_N - T_N$, is found by subtracting the two expressions:\n$$E_N = \\left(T_0 + N D_1 + \\sum_{k=2}^{N} (N-k+1)\\hat{S}_k\\right) - \\left(T_0 + N D_1 + \\sum_{k=2}^{N} (N-k+1)S_k\\right)$$\n$$E_N = \\sum_{k=2}^{N} (N-k+1)(\\hat{S}_k - S_k)$$\nUsing the definition of the quantization error, $e_k = \\hat{S}_k - S_k$, we get:\n$$E_N = \\sum_{k=2}^{N} (N-k+1)e_k$$\nTo find the worst-case absolute error, $\\max |E_N|$, we use the triangle inequality and the given error bound $|e_k| \\leq \\Delta/2$:\n$$|E_N| = \\left|\\sum_{k=2}^{N} (N-k+1)e_k\\right| \\leq \\sum_{k=2}^{N} |(N-k+1)e_k| = \\sum_{k=2}^{N} (N-k+1)|e_k|$$\nThe coefficients $(N-k+1)$ are positive for all $k \\in \\{2, \\dots, N\\}$. The maximum value of this sum occurs when each individual error term $|e_k|$ reaches its maximum possible value, $|e_k| = \\Delta/2$.\n$$\\max|E_N| = \\sum_{k=2}^{N} (N-k+1)\\frac{\\Delta}{2} = \\frac{\\Delta}{2} \\sum_{k=2}^{N} (N-k+1)$$\nThe sum is the sum of integers from $1$ to $N-1$:\n$$\\sum_{k=2}^{N} (N-k+1) = (N-1) + (N-2) + \\dots + 2 + 1 = \\sum_{j=1}^{N-1} j$$\nUsing the formula for the sum of the first $m$ integers, $\\sum_{j=1}^{m} j = \\frac{m(m+1)}{2}$, with $m=N-1$:\n$$\\sum_{j=1}^{N-1} j = \\frac{(N-1)((N-1)+1)}{2} = \\frac{N(N-1)}{2}$$\nSubstituting this back into the expression for the worst-case error:\n$$\\max|E_N| = \\frac{\\Delta}{2} \\left(\\frac{N(N-1)}{2}\\right) = \\frac{\\Delta N(N-1)}{4}$$\nThe final step is to relate the quantizer step size $\\Delta$ to the given parameters $R$ and $b$. The uniform quantizer has $2^b$ levels and covers the range $[-R, R]$. The total width of this range is $R - (-R) = 2R$. The step size $\\Delta$ is the total range width divided by the number of levels:\n$$\\Delta = \\frac{2R}{2^b} = R \\cdot 2^{1-b}$$\nSubstituting this expression for $\\Delta$ into the worst-case error formula yields the final answer:\n$$\\max|\\hat{T}_N - T_N| = \\frac{(R \\cdot 2^{1-b}) N(N-1)}{4} = \\frac{R N(N-1) \\cdot 2}{4 \\cdot 2^b} = \\frac{R N(N-1)}{2 \\cdot 2^b} = \\frac{R N(N-1)}{2^{b+1}}$$",
            "answer": "$$\\boxed{\\frac{R N(N-1)}{2^{b+1}}}$$"
        },
        {
            "introduction": "Beyond efficient storage, digital twins must process data streams in real time to provide timely insights, even when faced with unpredictable data lateness. This practice delves into watermarking, a fundamental mechanism in stream processing for balancing latency and completeness in event-time windowed aggregations. By modeling event arrivals and network delays, you will derive the expected error introduced by dropping late data, a crucial skill for configuring and validating the reliability of streaming analytics pipelines by tuning the lateness threshold $\\theta$. ",
            "id": "4251367",
            "problem": "Consider a streaming aggregation in a Digital Twin for a Cyber-Physical System (CPS) that computes event-time windowed sums of a scalar telemetry value. Events are timestamped by their event time, denoted by $t_{e}$, and arrive at the stream processor at arrival time $t_{a} = t_{e} + D$, where $D$ is a nonnegative random variable representing the end-to-end delay from production to processing. Windows are fixed-length, half-open event-time intervals of the form $[k\\Delta, (k+1)\\Delta)$ for $k \\in \\mathbb{Z}$ and window length $\\Delta > 0$. The stream processor uses a watermark with lateness threshold $\\theta > 0$: a window $[k\\Delta, (k+1)\\Delta)$ is finalized at watermark time $(k+1)\\Delta + \\theta$, and any event whose event time falls in the window but whose arrival time $t_{a}$ exceeds $(k+1)\\Delta + \\theta$ is permanently excluded from the aggregation for that window.\n\nAssume the following data-generation model for a single window $[k\\Delta, (k+1)\\Delta)$:\n\n- Event-time arrivals follow a homogeneous Poisson process with rate $\\lambda > 0$ events per unit time across event time. Therefore, the event-time locations inside the window are independent and identically distributed and uniform over the window conditional on the number of events, and the expected number of events in the window equals $\\lambda \\Delta$.\n\n- Each event carries a scalar value $X$ that is independent of $D$ and of the event-time process, with finite mean $\\mu = \\mathbb{E}[X]$.\n\n- Delays $D$ are independent and identically distributed with an exponential distribution of rate $\\alpha > 0$, so that $\\mathbb{P}(D > d) = \\exp(-\\alpha d)$ for $d \\geq 0$.\n\nThe semantics of aggregation under the watermark are defined as follows: an event with event time $t_{e} \\in [k\\Delta, (k+1)\\Delta)$ is included in the window sum if and only if $t_{a} \\leq (k+1)\\Delta + \\theta$, equivalently $D \\leq (k+1)\\Delta + \\theta - t_{e}$; otherwise, it is excluded and contributes no value to that window. The true event-time window sum is the sum of $X$ over all events with event time in the window, without regard to arrival time, while the watermark-based window sum excludes late arrivals beyond the watermark time.\n\nStarting from the core definitions of a homogeneous Poisson process, independence, expectation, and the exponential delay distribution, derive an analytic expression for the expected relative error (bias ratio) of the watermark-based window sum with respect to the true event-time window sum for a single window, defined as\n$$\n\\varepsilon_{\\mathrm{rel}} \\equiv \\frac{\\mathbb{E}\\left[\\text{true sum} - \\text{watermark-based sum}\\right]}{\\mathbb{E}\\left[\\text{true sum}\\right]}.\n$$\nExpress the final answer as a single closed-form symbolic expression in terms of the parameters $\\alpha$, $\\Delta$, and $\\theta$. The answer is dimensionless; no physical units are required.",
            "solution": "The problem as stated is scientifically grounded, well-posed, and objective. It is based on standard, verifiable models from probability theory and stream processing theory. A unique solution can be derived from the provided information. We proceed with the derivation.\n\nThe quantity to be determined is the expected relative error, defined as:\n$$\n\\varepsilon_{\\mathrm{rel}} = \\frac{\\mathbb{E}\\left[\\text{true sum}\\right] - \\mathbb{E}\\left[\\text{watermark-based sum}\\right]}{\\mathbb{E}\\left[\\text{true sum}\\right]}\n$$\nThis expression can be rewritten as:\n$$\n\\varepsilon_{\\mathrm{rel}} = \\frac{\\mathbb{E}\\left[\\text{sum of excluded values}\\right]}{\\mathbb{E}\\left[\\text{true sum}\\right]}\n$$\nWe will compute the numerator and the denominator separately. Let's analyze a single, representative window. Due to the time-homogeneous nature of the event-time process, we can choose the window $[k\\Delta, (k+1)\\Delta)$ with $k=0$, which is the interval $[0, \\Delta)$, without any loss of generality.\n\nFirst, let's compute the denominator, $\\mathbb{E}\\left[\\text{true sum}\\right]$. The true sum is the sum of values for all events whose event time $t_e$ falls within the window $[0, \\Delta)$, irrespective of their arrival time.\nLet $N$ be the number of events in the interval $[0, \\Delta)$. According to the definition of a homogeneous Poisson process with rate $\\lambda$, $N$ is a Poisson-distributed random variable with mean $\\mathbb{E}[N] = \\lambda \\Delta$.\nLet the values of these $N$ events be $X_1, X_2, \\ldots, X_N$. The problem states these values are independent and identically distributed with mean $\\mu = \\mathbb{E}[X]$. The true sum is $S_{\\text{true}} = \\sum_{i=1}^{N} X_i$.\nUsing the law of total expectation (or Wald's identity), we can compute the expected true sum:\n$$\n\\mathbb{E}[S_{\\text{true}}] = \\mathbb{E}\\left[\\mathbb{E}\\left[\\sum_{i=1}^{N} X_i \\bigg| N\\right]\\right]\n$$\nGiven $N=n$, the conditional expectation is $\\mathbb{E}\\left[\\sum_{i=1}^{n} X_i\\right] = \\sum_{i=1}^{n} \\mathbb{E}[X_i] = n\\mu$. Thus, $\\mathbb{E}[S_{\\text{true}} | N] = N\\mu$.\nTaking the expectation with respect to $N$:\n$$\n\\mathbb{E}[S_{\\text{true}}] = \\mathbb{E}[N\\mu] = \\mu\\mathbb{E}[N] = \\mu \\lambda \\Delta\n$$\n\nNext, we compute the numerator, which is the expected sum of values for events that are excluded from the aggregation. An event with event time $t_e \\in [0, \\Delta)$ is excluded if its arrival time $t_a$ is after the watermark time for the window. The watermark time is $(0+1)\\Delta + \\theta = \\Delta + \\theta$. The exclusion condition is $t_a > \\Delta + \\theta$.\nSince $t_a = t_e + D$, this condition becomes $t_e + D > \\Delta + \\theta$, or $D > \\Delta + \\theta - t_e$.\n\nLet's find the probability that a single event with event time $t_e$ is excluded. This probability, which we denote $p_{\\text{ex}}(t_e)$, depends on $t_e$.\n$$\np_{\\text{ex}}(t_e) = \\mathbb{P}(D > \\Delta + \\theta - t_e)\n$$\nThe delay $D$ follows an exponential distribution with rate $\\alpha$, so its survival function is $\\mathbb{P}(D > d) = \\exp(-\\alpha d)$ for $d \\geq 0$. Since $t_e \\in [0, \\Delta)$ and $\\theta > 0$, the argument $\\Delta+\\theta-t_e$ is always positive.\n$$\np_{\\text{ex}}(t_e) = \\exp(-\\alpha(\\Delta + \\theta - t_e))\n$$\nThe expected sum of excluded values, $\\mathbb{E}[S_{\\text{excluded}}]$, can be found by integrating the differential expected excluded value over the window interval. For a small time interval $dt_e$ around event time $t_e$, the expected number of events is $\\lambda dt_e$. The value $X$ of an event is independent of its event time $t_e$ and its delay $D$. Thus, the expected value of any single event is $\\mu$. The probability of an event at $t_e$ being excluded is $p_{\\text{ex}}(t_e)$.\nDue to the independence of the event process, value, and delay, the expected value contributed by excluded events in the interval $dt_e$ is:\n$$\nd\\mathbb{E}[S_{\\text{excluded}}] = (\\text{expected number of events}) \\times (\\text{expected value per event}) \\times (\\text{probability of exclusion})\n$$\n$$\nd\\mathbb{E}[S_{\\text{excluded}}] = (\\lambda dt_e) \\cdot \\mu \\cdot p_{\\text{ex}}(t_e) = \\lambda \\mu \\exp(-\\alpha(\\Delta + \\theta - t_e)) dt_e\n$$\nTo find the total expected sum of excluded values, we integrate this expression over the window length, from $t_e = 0$ to $t_e = \\Delta$:\n$$\n\\mathbb{E}[S_{\\text{excluded}}] = \\int_{0}^{\\Delta} \\lambda \\mu \\exp(-\\alpha(\\Delta + \\theta - t_e)) dt_e\n$$\nWe can factor out the constants and the part of the exponential that does not depend on $t_e$:\n$$\n\\mathbb{E}[S_{\\text{excluded}}] = \\lambda \\mu \\exp(-\\alpha(\\Delta + \\theta)) \\int_{0}^{\\Delta} \\exp(\\alpha t_e) dt_e\n$$\nThe integral is straightforward:\n$$\n\\int_{0}^{\\Delta} \\exp(\\alpha t_e) dt_e = \\left[ \\frac{1}{\\alpha} \\exp(\\alpha t_e) \\right]_{0}^{\\Delta} = \\frac{1}{\\alpha}(\\exp(\\alpha \\Delta) - \\exp(0)) = \\frac{1}{\\alpha}(\\exp(\\alpha \\Delta) - 1)\n$$\nSubstituting this back into the expression for $\\mathbb{E}[S_{\\text{excluded}}]$:\n$$\n\\mathbb{E}[S_{\\text{excluded}}] = \\lambda \\mu \\exp(-\\alpha(\\Delta + \\theta)) \\cdot \\frac{1}{\\alpha}(\\exp(\\alpha \\Delta) - 1)\n$$\n$$\n\\mathbb{E}[S_{\\text{excluded}}] = \\frac{\\lambda \\mu}{\\alpha} \\left( \\exp(-\\alpha(\\Delta + \\theta)) \\exp(\\alpha \\Delta) - \\exp(-\\alpha(\\Delta + \\theta)) \\right)\n$$\n$$\n\\mathbb{E}[S_{\\text{excluded}}] = \\frac{\\lambda \\mu}{\\alpha} (\\exp(-\\alpha \\theta) - \\exp(-\\alpha(\\Delta + \\theta)))\n$$\nNow we have both the numerator and the denominator for the relative error $\\varepsilon_{\\mathrm{rel}}$.\n$$\n\\varepsilon_{\\mathrm{rel}} = \\frac{\\mathbb{E}[S_{\\text{excluded}}]}{\\mathbb{E}[S_{\\text{true}}]} = \\frac{\\frac{\\lambda \\mu}{\\alpha} (\\exp(-\\alpha \\theta) - \\exp(-\\alpha(\\Delta + \\theta)))}{\\lambda \\mu \\Delta}\n$$\nThe terms $\\lambda$ and $\\mu$ cancel out, as they should, since the relative error should only depend on the temporal characteristics of the system, not on the event rate or value magnitude.\n$$\n\\varepsilon_{\\mathrm{rel}} = \\frac{\\exp(-\\alpha \\theta) - \\exp(-\\alpha(\\Delta + \\theta))}{\\alpha \\Delta}\n$$\nThis expression can be simplified by factoring out $\\exp(-\\alpha \\theta)$ from the numerator:\n$$\n\\varepsilon_{\\mathrm{rel}} = \\frac{\\exp(-\\alpha \\theta)(1 - \\exp(-\\alpha \\Delta))}{\\alpha \\Delta}\n$$\nThis is the final closed-form symbolic expression for the expected relative error in terms of the parameters $\\alpha$, $\\Delta$, and $\\theta$.",
            "answer": "$$\n\\boxed{\\frac{\\exp(-\\alpha\\theta)(1 - \\exp(-\\alpha\\Delta))}{\\alpha\\Delta}}\n$$"
        },
        {
            "introduction": "The ultimate goal of many digital twins is not just to process data, but to use it to infer the underlying state of a physical system. This practice focuses on the Kalman filter, a cornerstone algorithm for state estimation that optimally fuses noisy sensor measurements with a dynamic system model. By deriving the steady-state filter gain and verifying its stability conditions, you will gain hands-on experience with the core mathematical engine that powers state estimation in countless cyber-physical systems. ",
            "id": "4251389",
            "problem": "A cyber-physical system (CPS) digital twin ingests sensor time-series data from a time-series database to estimate the latent system state. The discrete-time linear time-invariant (LTI) state-space model for the scalar latent state is\n$$\nx_{k+1} = A\\,x_k + w_k,\\qquad y_k = C\\,x_k + v_k,\n$$\nwhere $w_k$ and $v_k$ are independent, zero-mean, white Gaussian sequences with covariances $\\mathbb{E}[w_k^2] = Q$ and $\\mathbb{E}[v_k^2] = R$. You are told that the parameters are $A = \\frac{6}{5}$, $C = 1$, $Q = \\frac{9}{100}$, and $R = \\frac{1}{4}$.\n\nStarting from the linear Gaussian model assumptions and the definition of the minimum mean-square error estimator, derive the steady-state Discrete-time Algebraic Riccati Equation (DARE) that the one-step-ahead error covariance $P$ must satisfy. Solve the resulting scalar DARE analytically to obtain the unique stabilizing steady-state covariance $P^{\\star}$, and then compute the steady-state Kalman gain\n$$\nK^{\\star} = \\frac{P^{\\star} C^{\\top}}{C P^{\\star} C^{\\top} + R}.\n$$\nVerify the convergence conditions for the Riccati recursion: $Q \\succeq 0$, $R \\succ 0$, detectability of the pair $(A,C)$, stabilizability of the pair $\\big(A, Q^{1/2}\\big)$, and Schur stability of the error dynamics map $(I - K^{\\star} C)A$. Conclude whether the steady-state filter exists and is stable for the given parameters.\n\nExpress your final Kalman gain $K^{\\star}$ in exact closed form. Do not round.",
            "solution": "The problem statement is found to be valid as it is scientifically grounded in established control and estimation theory, is well-posed with sufficient information, and uses objective, formal language. The tasks are standard procedures in the analysis of Kalman filters. All conditions for a valid problem are met.\n\nWe begin by deriving the Discrete-time Algebraic Riccati Equation (DARE) for the steady-state one-step-ahead prediction error covariance, denoted by $P$.\n\nThe state-space model is given by:\n$$x_{k+1} = Ax_k + w_k$$\n$$y_k = Cx_k + v_k$$\nwhere $w_k \\sim \\mathcal{N}(0, Q)$ and $v_k \\sim \\mathcal{N}(0, R)$ are independent white noise processes.\n\nThe Kalman filter propagates the state estimate and the error covariance in a two-step cycle: prediction and update.\n\n1.  **Prediction Step**: The predicted state estimate $\\hat{x}_{k|k-1}$ and its error covariance $P_k = \\mathbb{E}[(x_k - \\hat{x}_{k|k-1})(x_k - \\hat{x}_{k|k-1})^{\\top}]$ are propagated from the previous updated estimate $\\hat{x}_{k-1|k-1}$ and its covariance $P_{k-1|k-1}$.\n    The predicted state is $\\hat{x}_{k|k-1} = A\\hat{x}_{k-1|k-1}$.\n    The prediction error is $e_{k|k-1} = x_k - \\hat{x}_{k|k-1} = A(x_{k-1} - \\hat{x}_{k-1|k-1}) + w_{k-1} = A e_{k-1|k-1} + w_{k-1}$.\n    Thus, the prediction error covariance evolves as:\n    $$P_k = A P_{k-1|k-1} A^{\\top} + Q$$\n\n2.  **Update Step**: The predicted estimate is corrected using the new measurement $y_k$.\n    The updated state estimate is $\\hat{x}_{k|k} = \\hat{x}_{k|k-1} + K_k(y_k - C\\hat{x}_{k|k-1})$.\n    The Kalman gain $K_k$ is chosen to minimize the posterior error covariance $P_{k|k} = \\mathbb{E}[(x_k - \\hat{x}_{k|k})(x_k - \\hat{x}_{k|k})^{\\top}]$. The optimal gain is:\n    $$K_k = P_k C^{\\top} (C P_k C^{\\top} + R)^{-1}$$\n    The updated error covariance is given by:\n    $$P_{k|k} = (I - K_k C)P_k$$\n\nCombining these two steps, we can write the propagation equation for the prediction error covariance $P_k$:\n$$P_{k+1} = A P_{k|k} A^{\\top} + Q = A (I - K_k C) P_k A^{\\top} + Q$$\nSubstituting the expression for the optimal gain $K_k$:\n$$P_{k+1} = A \\left( P_k - P_k C^{\\top} (C P_k C^{\\top} + R)^{-1} C P_k \\right) A^{\\top} + Q$$\nThis is the Discrete-time Riccati Equation (DRE). The steady-state covariance $P = \\lim_{k \\to \\infty} P_k$ is found by setting $P_{k+1} = P_k = P$, which yields the Discrete-time Algebraic Riccati Equation (DARE):\n$$P = A P A^{\\top} - A P C^{\\top} (C P C^{\\top} + R)^{-1} C P A^{\\top} + Q$$\n\nThe problem provides scalar parameters: $A = \\frac{6}{5}$, $C = 1$, $Q = \\frac{9}{100}$, and $R = \\frac{1}{4}$. The DARE simplifies to:\n$$P = A^2 P - A^2 P C (C^2 P + R)^{-1} C P + Q$$\n$$P = A^2 P - \\frac{A^2 C^2 P^2}{C^2 P + R} + Q$$\nSubstituting the numerical values:\n$$P = \\left(\\frac{6}{5}\\right)^2 P - \\frac{\\left(\\frac{6}{5}\\right)^2 (1)^2 P^2}{(1)^2 P + \\frac{1}{4}} + \\frac{9}{100}$$\n$$P = \\frac{36}{25} P - \\frac{\\frac{36}{25} P^2}{P + \\frac{1}{4}} + \\frac{9}{100}$$\nTo solve for $P$, we rearrange the equation:\n$$0 = \\left(\\frac{36}{25} - 1\\right) P - \\frac{\\frac{36}{25} P^2}{P + \\frac{1}{4}} + \\frac{9}{100}$$\n$$0 = \\frac{11}{25} P - \\frac{\\frac{36}{25} P^2}{P + \\frac{1}{4}} + \\frac{9}{100}$$\nTo eliminate the denominators, we multiply by $100(P + \\frac{1}{4})$:\n$$0 = 4 \\times 11 P (P + \\frac{1}{4}) - 4 \\times 36 P^2 + (P + \\frac{1}{4}) \\times 9$$\n$$0 = 44 P^2 + 11 P - 144 P^2 + 9 P + \\frac{9}{4}$$\n$$0 = -100 P^2 + 20 P + \\frac{9}{4}$$\nMultiplying by $4$ to obtain integer coefficients:\n$$0 = -400 P^2 + 80 P + 9$$\nOr equivalently:\n$$400 P^2 - 80 P - 9 = 0$$\nThis is a quadratic equation for $P$. We use the quadratic formula $P = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$:\n$$P = \\frac{80 \\pm \\sqrt{(-80)^2 - 4(400)(-9)}}{2(400)}$$\n$$P = \\frac{80 \\pm \\sqrt{6400 + 14400}}{800}$$\n$$P = \\frac{80 \\pm \\sqrt{20800}}{800}$$\nWe simplify the radical: $\\sqrt{20800} = \\sqrt{100 \\times 208} = 10 \\sqrt{16 \\times 13} = 10 \\times 4 \\sqrt{13} = 40\\sqrt{13}$.\n$$P = \\frac{80 \\pm 40\\sqrt{13}}{800} = \\frac{2 \\pm \\sqrt{13}}{20}$$\nThe error covariance must be a non-negative quantity ($P \\ge 0$). Since $\\sqrt{13} \\approx 3.6$, the solution $\\frac{2 - \\sqrt{13}}{20}$ is negative. The unique positive solution, which corresponds to the stabilizing steady-state covariance $P^{\\star}$, is:\n$$P^{\\star} = \\frac{2 + \\sqrt{13}}{20}$$\n\nNext, we verify the convergence conditions. A unique, stabilizing, positive semi-definite solution $P^{\\star}$ to the DARE exists if the pair $(A, Q^{1/2})$ is stabilizable and the pair $(A, C)$ is detectable.\n1.  **$Q \\succeq 0$ and $R \\succ 0$**: $Q = \\frac{9}{100} > 0$ and $R = \\frac{1}{4} > 0$. The conditions are met.\n2.  **Detectability of $(A,C)$**: For the scalar case, the system is detectable if it is observable or if the unobservable dynamics are stable. The observability matrix is $\\mathcal{O} = \\begin{pmatrix} C \\\\ CA \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 6/5 \\end{pmatrix}$. This matrix has rank $1$, which is the dimension of the state space. Thus, the system is observable, which is stronger than detectable.\n3.  **Stabilizability of $(A, Q^{1/2})$**: Let $G = Q^{1/2} = \\sqrt{9/100} = 3/10$. The system is stabilizable if its uncontrollable dynamics are stable. The controllability matrix is $\\mathcal{C} = \\begin{pmatrix} G  AG \\end{pmatrix} = \\begin{pmatrix} 3/10  (6/5)(3/10) \\end{pmatrix} = \\begin{pmatrix} 3/10  18/50 \\end{pmatrix}$. This matrix has rank $1$. Thus, the system is controllable, which is stronger than stabilizable.\nSince both stabilizability and detectability conditions are met, a unique stabilizing solution $P^{\\star} \\ge 0$ exists.\n\nNow we compute the steady-state Kalman gain $K^{\\star}$:\n$$K^{\\star} = \\frac{P^{\\star} C^{\\top}}{C P^{\\star} C^{\\top} + R} = \\frac{P^{\\star} C}{C^2 P^{\\star} + R}$$\nSubstituting the parameters:\n$$K^{\\star} = \\frac{P^{\\star}}{P^{\\star} + R} = \\frac{\\frac{2 + \\sqrt{13}}{20}}{\\frac{2 + \\sqrt{13}}{20} + \\frac{1}{4}} = \\frac{\\frac{2 + \\sqrt{13}}{20}}{\\frac{2 + \\sqrt{13}}{20} + \\frac{5}{20}} = \\frac{\\frac{2 + \\sqrt{13}}{20}}{\\frac{7 + \\sqrt{13}}{20}} = \\frac{2 + \\sqrt{13}}{7 + \\sqrt{13}}$$\nTo rationalize the denominator, we multiply the numerator and denominator by the conjugate $7 - \\sqrt{13}$:\n$$K^{\\star} = \\frac{(2 + \\sqrt{13})(7 - \\sqrt{13})}{(7 + \\sqrt{13})(7 - \\sqrt{13})} = \\frac{14 - 2\\sqrt{13} + 7\\sqrt{13} - 13}{7^2 - (\\sqrt{13})^2} = \\frac{1 + 5\\sqrt{13}}{49 - 13} = \\frac{1 + 5\\sqrt{13}}{36}$$\n\nFinally, we verify the Schur stability of the error dynamics, governed by the map $(I - K^{\\star}C)A$. The filter is stable if all eigenvalues of this matrix have a magnitude less than $1$. In this scalar case, the eigenvalue $\\lambda_{err}$ is simply the value itself:\n$$\\lambda_{err} = (1 - K^{\\star}C)A = \\left(1 - \\frac{1 + 5\\sqrt{13}}{36} \\times 1\\right) \\times \\frac{6}{5}$$\n$$\\lambda_{err} = \\left(\\frac{36 - (1 + 5\\sqrt{13})}{36}\\right) \\times \\frac{6}{5} = \\frac{35 - 5\\sqrt{13}}{36} \\times \\frac{6}{5}$$\n$$\\lambda_{err} = \\frac{5(7 - \\sqrt{13})}{36} \\times \\frac{6}{5} = \\frac{6(7 - \\sqrt{13})}{36} = \\frac{7 - \\sqrt{13}}{6}$$\nTo check if $|\\lambda_{err}|  1$, we evaluate its bounds. We know that $3^2 = 9$ and $4^2 = 16$, so $3  \\sqrt{13}  4$.\n$$7 - 4  7 - \\sqrt{13}  7 - 3 \\implies 3  7 - \\sqrt{13}  4$$\n$$\\frac{3}{6}  \\frac{7 - \\sqrt{13}}{6}  \\frac{4}{6} \\implies \\frac{1}{2}  \\lambda_{err}  \\frac{2}{3}$$\nSince $0  \\frac{1}{2}  \\lambda_{err}  \\frac{2}{3}  1$, the condition $|\\lambda_{err}|  1$ is satisfied. The error dynamics are stable.\nAll conditions are satisfied, confirming that the steady-state filter exists, is unique, and is stable for the given parameters. The requested quantity is the steady-state Kalman gain $K^{\\star}$.",
            "answer": "$$\\boxed{\\frac{1 + 5\\sqrt{13}}{36}}$$"
        }
    ]
}