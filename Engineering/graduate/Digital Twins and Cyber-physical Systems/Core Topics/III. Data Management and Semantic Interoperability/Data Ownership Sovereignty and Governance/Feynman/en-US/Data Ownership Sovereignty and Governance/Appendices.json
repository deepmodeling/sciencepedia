{
    "hands_on_practices": [
        {
            "introduction": "In collaborative Digital Twin ecosystems, multiple sovereign data owners often contribute to a shared model. A fundamental governance challenge is to fairly attribute the value—such as improved model accuracy—generated by these contributions. This exercise introduces a principled approach from cooperative game theory, the Shapley value, to solve this problem . By calculating the fair contribution of each data owner based on their marginal impact across all possible collaborations, you will develop skills in data valuation, which is crucial for designing equitable and sustainable data-sharing agreements.",
            "id": "4212234",
            "problem": "A Cyber-Physical Systems (CPS) operator is constructing a Digital Twin (DT) model of a production line by federating data from three sovereign data owners, labeled $A$, $B$, and $C$. Each owner’s data may be included or excluded according to their data governance choices, subject to legal constraints such as consent and purpose limitation under the General Data Protection Regulation (GDPR). For any coalition $S \\subseteq \\{A,B,C\\}$, let $v(S)$ denote the DT’s validated out-of-sample accuracy (unitless fraction) when the model is trained exclusively on the data contributed by the owners in $S$, using a common pipeline and fixed hyperparameters to ensure comparability. Assume $v(\\varnothing) = 0$ and the following empirically obtained values:\n- $v(\\{A\\}) = 0.70$, $v(\\{B\\}) = 0.68$, $v(\\{C\\}) = 0.66$,\n- $v(\\{A,B\\}) = 0.78$, $v(\\{A,C\\}) = 0.77$, $v(\\{B,C\\}) = 0.75$,\n- $v(\\{A,B,C\\}) = 0.82$.\n\nTreating the owners as players in a cooperative game with characteristic function $v(\\cdot)$ and interpreting “fair contribution” as an allocation determined solely by coalition-dependent accuracy increments (subject to data governance constraints that determine admissible coalitions), do the following:\n\n1. Starting from the foundational fairness axioms of cooperative value allocation—symmetry (indistinguishable players receive equal allocation), efficiency (allocations sum to the grand coalition value), and the null player property (any player whose addition does not change any coalition’s value receives zero allocation)—derive a closed-form expression for the fair contribution of a player in terms of marginal improvements across all coalitions consistent with data governance orderings. Your derivation must establish why the resulting allocation depends on expectations over player orderings and why the axioms above constrain the allocation uniquely to that form.\n\n2. In the context of sovereign data ownership and governance, provide a justification for why each axiom is appropriate for attributing DT accuracy: symmetry when two owners’ data are functionally indistinguishable for model performance, efficiency to ensure no accuracy “surplus” or “deficit” remains unassigned, and the null player property when an owner’s data are either legally unusable (e.g., lack of valid consent) or statistically uninformative for the DT.\n\n3. Using your derived expression, compute the fair contribution for owner $A$ based on the values above. Round your final numeric answer to four significant figures and express it as a unitless decimal (no percentage sign).",
            "solution": "The problem asks for the derivation of a fair value allocation scheme based on specified axioms, a justification of these axioms in the context of data governance for Digital Twins, and the computation of this value for a specific data owner. The problem describes a cooperative game, where the players are the data owners $N = \\{A, B, C\\}$, and the value of any coalition $S \\subseteq N$ is given by the characteristic function $v(S)$, representing the accuracy of a Digital Twin model. The problem is valid as it is scientifically grounded in cooperative game theory and its application to data valuation, is well-posed with all necessary information provided, and is formulated objectively.\n\nThe fair allocation scheme sought is the Shapley value, which is uniquely determined by the axioms of symmetry, efficiency, and the null player property, along with linearity.\n\n1. Derivation of the Fair Contribution Expression (Shapley Value)\n\nLet $N$ be the set of players, in this case, the data owners $\\{A, B, C\\}$. A cooperative game is defined by the characteristic function $v: 2^N \\to \\mathbb{R}$, which assigns a value $v(S)$ to each coalition (subset) $S \\subseteq N$, with $v(\\varnothing)=0$. We seek an allocation vector $\\phi(v)$ with components $\\phi_i(v)$ for each player $i \\in N$, representing the fair contribution of player $i$ to the total value. This allocation must satisfy the following axioms:\n\n-   **Symmetry**: If two players $i$ and $j$ are interchangeable (i.e., for every coalition $S \\subseteq N \\setminus \\{i, j\\}$, it holds that $v(S \\cup \\{i\\}) = v(S \\cup \\{j\\})$), then their allocations must be equal: $\\phi_i(v) = \\phi_j(v)$.\n-   **Efficiency**: The sum of the individual allocations must equal the value of the grand coalition $N$: $\\sum_{i \\in N} \\phi_i(v) = v(N)$.\n-   **Null Player Property**: If a player $i$ contributes nothing to any coalition (i.e., for every coalition $S \\subseteq N$, $v(S \\cup \\{i\\}) = v(S)$), then their allocation is zero: $\\phi_i(v) = 0$.\n\nTo derive the expression for $\\phi_i(v)$, we consider the value player $i$ adds when joining a coalition of existing players. A fair way to account for all possibilities is to imagine the players arriving in a random sequential order. Let $\\pi$ be a permutation of the player set $N$, representing one such arrival ordering. Let $P_i(\\pi)$ be the set of players who precede player $i$ in the ordering $\\pi$. When player $i$ arrives, they join the coalition $P_i(\\pi)$ and create the new coalition $P_i(\\pi) \\cup \\{i\\}$. The marginal contribution of player $i$ for this specific ordering $\\pi$ is the value they add:\n$$\n\\text{MC}_i(\\pi) = v(P_i(\\pi) \\cup \\{i\\}) - v(P_i(\\pi))\n$$\nThe core idea of the Shapley value is that a player's fair contribution is their expected marginal contribution, averaged over all possible arrival orderings. Assuming each of the $|N|!$ permutations is equally likely (with probability $1/|N|!$), the fair contribution for player $i$, $\\phi_i(v)$, is the expectation of $\\text{MC}_i(\\pi)$:\n$$\n\\phi_i(v) = \\frac{1}{|N|!} \\sum_{\\pi \\in \\Pi_N} \\left[ v(P_i(\\pi) \\cup \\{i\\}) - v(P_i(\\pi)) \\right]\n$$\nwhere $\\Pi_N$ is the set of all permutations of $N$. This formulation establishes why the allocation depends on expectations over player orderings.\n\nTo obtain the closed-form expression in terms of marginal improvements across coalitions, we can group permutations based on the set of predecessors $P_i(\\pi)$. For any given coalition $S \\subseteq N \\setminus \\{i\\}$, let's count the number of permutations where $P_i(\\pi) = S$. For such a permutation, the players in $S$ must come first, followed by player $i$, followed by the remaining players in $N \\setminus (S \\cup \\{i\\})$.\n-   There are $|S|!$ ways to order the players within $S$.\n-   Player $i$ is in a fixed position after all players in $S$.\n-   There are $(|N| - |S| - 1)!$ ways to order the remaining players.\nThus, there are $|S|! (|N| - |S| - 1)!$ permutations for which player $i$ is preceded by precisely the coalition $S$. The marginal contribution for all these permutations is the same: $v(S \\cup \\{i\\}) - v(S)$.\n\nBy summing the contributions over all possible predecessor coalitions $S$, we can rewrite the expectation as:\n$$\n\\phi_i(v) = \\sum_{S \\subseteq N \\setminus \\{i\\}} \\frac{|S|! (|N| - |S| - 1)!}{|N|!} \\left[ v(S \\cup \\{i\\}) - v(S) \\right]\n$$\nThis is the closed-form expression for the fair contribution of player $i$ in terms of their marginal improvements to every possible coalition they can join. It can be proven that this is the *unique* allocation that satisfies the three stated axioms, as well as a fourth axiom called additivity ($\\phi(v+w) = \\phi(v) + \\phi(w)$). The uniqueness proof relies on showing that any game $v$ can be uniquely represented as a linear combination of simpler \"unanimity games\", and that the Shapley value is the only linear operator satisfying the axioms for these basis games.\n\n2. Justification of Axioms for Digital Twin Accuracy Attribution\n\n-   **Symmetry**: This axiom is appropriate because it embodies the principle of substitutability. If two data owners, $A$ and $B$, provide data that is functionally indistinguishable for the DT model (e.g., data from identical sensors on parallel production lines under identical conditions), such that replacing $A$'s data with $B$'s data in any data combination yields the same model accuracy, then their contribution to the model's performance is identical. It is therefore fair and logical that their attributed value be equal. The symmetry axiom formalizes this intuition, ensuring the allocation reflects functional equivalence, not arbitrary labels.\n\n-   **Efficiency**: This axiom ensures a rational and complete accounting of the total value created. The grand coalition's value, $v(N)$, represents the maximum achievable accuracy with the available data. The efficiency axiom, $\\sum_{i \\in N} \\phi_i(v) = v(N)$, mandates that the sum of the attributed contributions of all owners must precisely equal this total value. This prevents any accuracy \"surplus\" or \"deficit\" from being unassigned, ensuring that the allocation scheme provides a closed-loop decomposition of the total performance gain. It reflects the collaborative nature of the enterprise: all the value generated by the group must be distributed among its members.\n\n-   **Null Player Property**: This axiom is crucial for handling cases where data is unusable or uninformative, which are common concerns under data governance frameworks like GDPR. An owner might be a null player for two primary reasons. First, legal constraints, such as a lack of valid consent for a specific processing purpose required by the DT, may render their data legally unusable. In this scenario, they cannot contribute to any coalition, and their marginal value is correctly assessed as zero. Second, their data might be statistically uninformative—for example, it could be pure noise, completely redundant with data already provided by another owner, or irrelevant to the physical process being modeled. In this case, adding their data provides no improvement to the DT's accuracy. The null player axiom ensures that owners are rewarded for providing useful, legally compliant information, not simply for possessing data.\n\n3. Computation of Fair Contribution for Owner A\n\nWe apply the derived Shapley value formula to compute the fair contribution for owner $A$, denoted $\\phi_A(v)$. The set of players is $N = \\{A, B, C\\}$, so $|N| = 3$. The formula is:\n$$\n\\phi_A(v) = \\sum_{S \\subseteq \\{B, C\\}} \\frac{|S|! (3 - |S| - 1)!}{3!} \\left[ v(S \\cup \\{A\\}) - v(S) \\right]\n$$\nWe evaluate the term for each subset $S$ of $\\{B, C\\}$:\n\n-   For $S = \\varnothing$:\n    The weight is $\\frac{0! (2)!}{3!} = \\frac{1 \\times 2}{6} = \\frac{1}{3}$.\n    The marginal contribution is $v(\\{A\\}) - v(\\varnothing) = 0.70 - 0 = 0.70$.\n    The term is $\\frac{1}{3} \\times 0.70$.\n\n-   For $S = \\{B\\}$:\n    The weight is $\\frac{1! (1)!}{3!} = \\frac{1 \\times 1}{6} = \\frac{1}{6}$.\n    The marginal contribution is $v(\\{A, B\\}) - v(\\{B\\}) = 0.78 - 0.68 = 0.10$.\n    The term is $\\frac{1}{6} \\times 0.10$.\n\n-   For $S = \\{C\\}$:\n    The weight is $\\frac{1! (1)!}{3!} = \\frac{1 \\times 1}{6} = \\frac{1}{6}$.\n    The marginal contribution is $v(\\{A, C\\}) - v(\\{C\\}) = 0.77 - 0.66 = 0.11$.\n    The term is $\\frac{1}{6} \\times 0.11$.\n\n-   For $S = \\{B, C\\}$:\n    The weight is $\\frac{2! (0)!}{3!} = \\frac{2 \\times 1}{6} = \\frac{1}{3}$.\n    The marginal contribution is $v(\\{A, B, C\\}) - v(\\{B, C\\}) = 0.82 - 0.75 = 0.07$.\n    The term is $\\frac{1}{3} \\times 0.07$.\n\nSumming these terms gives the Shapley value for owner $A$:\n$$\n\\phi_A(v) = \\left(\\frac{1}{3} \\times 0.70\\right) + \\left(\\frac{1}{6} \\times 0.10\\right) + \\left(\\frac{1}{6} \\times 0.11\\right) + \\left(\\frac{1}{3} \\times 0.07\\right)\n$$\n$$\n\\phi_A(v) = \\frac{0.70}{3} + \\frac{0.10}{6} + \\frac{0.11}{6} + \\frac{0.07}{3}\n$$\n$$\n\\phi_A(v) = \\frac{2 \\times 0.70}{6} + \\frac{0.10}{6} + \\frac{0.11}{6} + \\frac{2 \\times 0.07}{6}\n$$\n$$\n\\phi_A(v) = \\frac{1.40 + 0.10 + 0.11 + 0.14}{6} = \\frac{1.75}{6}\n$$\n$$\n\\phi_A(v) \\approx 0.291666...\n$$\nRounding to four significant figures, the fair contribution for owner $A$ is $0.2917$.",
            "answer": "$$\\boxed{0.2917}$$"
        },
        {
            "introduction": "While sharing data is vital for building powerful Digital Twins, it must be done without compromising individual privacy. Basic anonymization techniques like $k$-anonymity can fail to protect against strong adversaries, leading to the development of more robust standards. This practice  places you in the role of a data steward, tasking you with auditing an anonymized dataset against two of these advanced criteria: distinctness $l$-diversity and $t$-closeness. You will learn to quantify privacy risks and propose concrete, governance-preserving adjustments to ensure compliance.",
            "id": "4212228",
            "problem": "A manufacturing Digital Twin (DT) deployed within a Cyber-Physical System (CPS) aggregates operational records and releases an anonymized dataset for collaborative analytics under strict data governance. The dataset uses quasi-identifiers to form equivalence classes and retains a single sensitive attribute, incident severity, with categories Minor, Moderate, and Critical. The data owner wishes to evaluate compliance with two anonymization criteria: distinctness $l$-diversity with $l=3$ and $t$-closeness with threshold $t=0.2$, under a categorical distance defined as the total variation distance. Use the following foundational definitions.\n\nDefinitions:\n- An equivalence class is the set of records identical on the chosen quasi-identifiers.\n- Distinctness $l$-diversity requires that each equivalence class contains at least $l$ distinct sensitive attribute values present.\n- $t$-closeness requires that for every equivalence class, the distance between the class-level sensitive attribute distribution and the global sensitive attribute distribution is bounded above by $t$. For a categorical sensitive attribute, use the total variation distance defined by $d(P,Q) = \\frac{1}{2}\\sum_{i} |P(i) - Q(i)|$, where $P$ and $Q$ are probability mass functions over the sensitive categories.\n\nThe anonymized dataset comprises four equivalence classes $\\mathcal{E}_1,\\mathcal{E}_2,\\mathcal{E}_3,\\mathcal{E}_4$, each containing $25$ records. The sensitive attribute counts within each class are:\n- $\\mathcal{E}_1$: Minor $20$, Moderate $5$, Critical $0$.\n- $\\mathcal{E}_2$: Minor $10$, Moderate $10$, Critical $5$.\n- $\\mathcal{E}_3$: Minor $15$, Moderate $7$, Critical $3$.\n- $\\mathcal{E}_4$: Minor $15$, Moderate $8$, Critical $2$.\n\nThe global sensitive attribute counts across the $100$ records are Minor $60$, Moderate $30$, Critical $10$.\n\nTask:\n1. Starting from the definitions above, derive the achieved distinctness $l$-diversity (i.e., the minimum number of distinct sensitive categories present in any equivalence class).\n2. Compute, for each equivalence class, the total variation distance between its sensitive attribute distribution and the global distribution, and identify the largest such distance.\n3. Evaluate whether the dataset satisfies distinctness $l$-diversity with $l=3$ and $t$-closeness with $t=0.2$.\n4. If any criterion fails, propose a minimal adjustment grounded in governance-preserving operations (e.g., microaggregation, swapping records between classes while preserving class sizes and global totals) to achieve compliance, and justify why the adjustment addresses the failure.\n\nExpress your final numeric outputs as the ordered pair consisting of the achieved distinctness $l$-diversity and the maximal total variation distance across classes. Provide exact values; no rounding is required. The outputs are dimensionless.",
            "solution": "The problem statement has been critically examined and found to be valid. It is scientifically grounded in the principles of data anonymization, well-posed with a complete and consistent set of data and definitions, and objective in its formulation. The total counts of sensitive attributes across the equivalence classes sum to the specified global counts, ensuring internal consistency.\nTotal Minor: $20+10+15+15 = 60$.\nTotal Moderate: $5+10+7+8 = 30$.\nTotal Critical: $0+5+3+2 = 10$.\nTotal Records: $4 \\times 25 = 100$.\nThe data is consistent and sufficient for a rigorous solution. We proceed with the analysis.\n\nThe problem requires a four-part analysis: evaluating distinctness $l$-diversity, computing the maximum $t$-closeness distance, assessing compliance with the given criteria, and proposing a minimal adjustment for any identified failures.\n\n**1. Derivation of Achieved Distinctness $l$-diversity**\n\nDistinctness $l$-diversity requires that each equivalence class contains at least $l$ distinct values for the sensitive attribute. We must find the minimum number of distinct sensitive attribute categories present in any single equivalence class. The sensitive attribute categories are Minor, Moderate, and Critical.\n\n-   For equivalence class $\\mathcal{E}_1$ with counts (Minor $20$, Moderate $5$, Critical $0$), there are $2$ distinct categories present: {Minor, Moderate}.\n-   For equivalence class $\\mathcal{E}_2$ with counts (Minor $10$, Moderate $10$, Critical $5$), there are $3$ distinct categories present: {Minor, Moderate, Critical}.\n-   For equivalence class $\\mathcal{E}_3$ with counts (Minor $15$, Moderate $7$, Critical $3$), there are $3$ distinct categories present: {Minor, Moderate, Critical}.\n-   For equivalence class $\\mathcal{E}_4$ with counts (Minor $15$, Moderate $8$, Critical $2$), there are $3$ distinct categories present: {Minor, Moderate, Critical}.\n\nThe achieved distinctness $l$-diversity is the minimum of these counts:\n$$l_{\\text{achieved}} = \\min(\\{2, 3, 3, 3\\}) = 2$$\n\n**2. Computation of Total Variation Distance**\n\nThe $t$-closeness criterion is evaluated using the total variation distance, $d(P, Q) = \\frac{1}{2}\\sum_{i} |P(i) - Q(i)|$, where $Q$ is the global distribution and $P$ is the class-level distribution of the sensitive attribute.\n\nFirst, we establish the global probability mass function, $Q$, over the categories $S = \\{\\text{Minor, Moderate, Critical}\\}$. The total number of records is $N=100$, with global counts of $60$ (Minor), $30$ (Moderate), and $10$ (Critical).\n$$Q(\\text{Minor}) = \\frac{60}{100} = 0.6$$\n$$Q(\\text{Moderate}) = \\frac{30}{100} = 0.3$$\n$$Q(\\text{Critical}) = \\frac{10}{100} = 0.1$$\nThus, the global distribution is $Q = (0.6, 0.3, 0.1)$.\n\nNext, we compute the probability mass function $P_k$ for each equivalence class $\\mathcal{E}_k$, each of which contains $N_k=25$ records.\n\nFor $\\mathcal{E}_1$: Counts are $(20, 5, 0)$.\n$$P_1 = \\left(\\frac{20}{25}, \\frac{5}{25}, \\frac{0}{25}\\right) = (0.8, 0.2, 0.0)$$\nThe total variation distance is:\n$$d(\\mathcal{E}_1, Q) = \\frac{1}{2} \\left( |0.8 - 0.6| + |0.2 - 0.3| + |0.0 - 0.1| \\right) = \\frac{1}{2} (0.2 + 0.1 + 0.1) = \\frac{1}{2} (0.4) = 0.2$$\n\nFor $\\mathcal{E}_2$: Counts are $(10, 10, 5)$.\n$$P_2 = \\left(\\frac{10}{25}, \\frac{10}{25}, \\frac{5}{25}\\right) = (0.4, 0.4, 0.2)$$\nThe total variation distance is:\n$$d(\\mathcal{E}_2, Q) = \\frac{1}{2} \\left( |0.4 - 0.6| + |0.4 - 0.3| + |0.2 - 0.1| \\right) = \\frac{1}{2} (0.2 + 0.1 + 0.1) = \\frac{1}{2} (0.4) = 0.2$$\n\nFor $\\mathcal{E}_3$: Counts are $(15, 7, 3)$.\n$$P_3 = \\left(\\frac{15}{25}, \\frac{7}{25}, \\frac{3}{25}\\right) = (0.6, 0.28, 0.12)$$\nThe total variation distance is:\n$$d(\\mathcal{E}_3, Q) = \\frac{1}{2} \\left( |0.6 - 0.6| + |0.28 - 0.3| + |0.12 - 0.1| \\right) = \\frac{1}{2} (0.0 + 0.02 + 0.02) = \\frac{1}{2} (0.04) = 0.02$$\n\nFor $\\mathcal{E}_4$: Counts are $(15, 8, 2)$.\n$$P_4 = \\left(\\frac{15}{25}, \\frac{8}{25}, \\frac{2}{25}\\right) = (0.6, 0.32, 0.08)$$\nThe total variation distance is:\n$$d(\\mathcal{E}_4, Q) = \\frac{1}{2} \\left( |0.6 - 0.6| + |0.32 - 0.3| + |0.08 - 0.1| \\right) = \\frac{1}{2} (0.0 + 0.02 + 0.02) = \\frac{1}{2} (0.04) = 0.02$$\n\nThe set of computed distances is $\\{0.2, 0.2, 0.02, 0.02\\}$. The largest such distance is:\n$$d_{\\max} = \\max(\\{0.2, 0.2, 0.02, 0.02\\}) = 0.2$$\n\n**3. Compliance Evaluation**\n\n-   **Distinctness $l$-diversity:** The requirement is $l=3$. The achieved level is $l_{\\text{achieved}}=2$. Since $2 < 3$, the dataset **fails** to satisfy distinctness $l$-diversity with $l=3$. The failure is in equivalence class $\\mathcal{E}_1$.\n-   **$t$-closeness:** The requirement is that the distance for every class must be bounded above by $t=0.2$. This means $d(\\mathcal{E}_k, Q) \\le 0.2$ for all $k \\in \\{1,2,3,4\\}$. The maximum computed distance is $d_{\\max}=0.2$. Since $0.2 \\le 0.2$, the condition holds for all equivalence classes. Therefore, the dataset **satisfies** $t$-closeness with $t=0.2$.\n\n**4. Minimal Adjustment Proposal**\n\nThe dataset fails the $l=3$ diversity criterion because $\\mathcal{E}_1$ lacks a 'Critical' incident record. A minimal adjustment to correct this is to introduce at least one 'Critical' record into $\\mathcal{E}_1$. This can be achieved via a record swap with another equivalence class to preserve class sizes and global distributions, as mandated by governance-preserving operations.\n\nWe can swap one 'Minor' record from $\\mathcal{E}_1$ with one 'Critical' record from any class that can spare one, for instance, $\\mathcal{E}_2$.\n-   Initial $\\mathcal{E}_1$: (Minor $20$, Moderate $5$, Critical $0$)\n-   Initial $\\mathcal{E}_2$: (Minor $10$, Moderate $10$, Critical $5$)\n\nThe proposed swap is: move one 'Critical' record from $\\mathcal{E}_2$ to $\\mathcal{E}_1$, and move one 'Minor' record from $\\mathcal{E}_1$ to $\\mathcal{E}_2$.\n\n-   Adjusted $\\mathcal{E}'_1$: (Minor $19$, Moderate $5$, Critical $1$)\n-   Adjusted $\\mathcal{E}'_2$: (Minor $11$, Moderate $10$, Critical $4$)\n\nLet us verify the effect of this adjustment.\n-   **$l$-diversity compliance:** After adjustment, $\\mathcal{E}'_1$ contains $3$ distinct categories. All other classes already had $3$. The minimum count is now $3$, so the dataset is compliant with $l=3$ diversity.\n-   **$t$-closeness compliance:** We must re-calculate the total variation distance for the modified classes, $\\mathcal{E}'_1$ and $\\mathcal{E}'_2$, to ensure they still meet the $t \\le 0.2$ threshold.\n    -   $P'_1 = \\left(\\frac{19}{25}, \\frac{5}{25}, \\frac{1}{25}\\right) = (0.76, 0.2, 0.04)$.\n    -   $d(\\mathcal{E}'_1, Q) = \\frac{1}{2} \\left( |0.76 - 0.6| + |0.2 - 0.3| + |0.04 - 0.1| \\right) = \\frac{1}{2} (0.16 + 0.1 + 0.06) = 0.16$.\n    -   $P'_2 = \\left(\\frac{11}{25}, \\frac{10}{25}, \\frac{4}{25}\\right) = (0.44, 0.4, 0.16)$.\n    -   $d(\\mathcal{E}'_2, Q) = \\frac{1}{2} \\left( |0.44 - 0.6| + |0.4 - 0.3| + |0.16 - 0.1| \\right) = \\frac{1}{2} (0.16 + 0.1 + 0.06) = 0.16$.\nSince $0.16 \\le 0.2$, both adjusted classes remain compliant with the $t$-closeness criterion. The other classes were unaffected and remain compliant. This minimal swap successfully rectifies the $l$-diversity failure while preserving all constraints and overall compliance.\n\nThe final numeric outputs requested are the achieved distinctness $l$-diversity and the maximal total variation distance for the original, unadjusted dataset.\nAchieved $l$-diversity: $2$.\nMaximal total variation distance: $0.2$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n2 & 0.2\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Data sovereignty regulations, such as the GDPR, impose strict rules on where data can be stored and processed, transforming legal requirements into hard engineering constraints. Designing a performant Cyber-Physical System architecture under these rules requires balancing regulatory compliance with technical objectives like minimizing latency. This final exercise  challenges you to solve a real-world architectural optimization problem. You will develop a placement strategy for microservices and data that respects both jurisdictional boundaries and infrastructure capacities, bridging the critical gap between high-level governance policy and low-level system design.",
            "id": "4212256",
            "problem": "A digital twin (DT) for a Cyber-Physical System (CPS) must comply with data sovereignty requirements while optimizing performance. Under jurisdictional constraints, each data partition must be stored only in permissible regions and each microservice must process only in permissible regions. Latency between regions is provided. The goal is to compute a placement strategy for microservices and data partitions that minimizes latency while meeting sovereignty and capacity constraints.\n\nFormulate the following mathematical optimization problem. Let the set of regions be $\\mathcal{L} = \\{0, 1, \\dots, L-1\\}$. Let the set of microservices be $\\mathcal{M} = \\{0, 1, \\dots, M-1\\}$ and the set of data partitions be $\\mathcal{P} = \\{0, 1, \\dots, P-1\\}$. For each region pair $(i,j)$, a nonnegative latency $D_{ij}$ (in milliseconds) is given, representing the network latency from processing in region $i$ to storage in region $j$. For each microservice $m \\in \\mathcal{M}$, an allowed processing region set $\\mathcal{S}_m \\subseteq \\mathcal{L}$ enforces sovereignty for processing. For each partition $p \\in \\mathcal{P}$, an allowed storage region set $\\mathcal{R}_p \\subseteq \\mathcal{L}$ enforces sovereignty for storage. Each region $i \\in \\mathcal{L}$ has capacity constraints $U^{\\text{proc}}_i$ (maximum number of microservices that can run in region $i$) and $U^{\\text{stor}}_i$ (maximum number of partitions that can be stored in region $i$). Let $A_{mp} \\ge 0$ be the number of data access operations per unit time by microservice $m$ to partition $p$, and let $B_{mn} \\ge 0$ be the number of inter-microservice calls per unit time from microservice $m$ to microservice $n$.\n\nDecision variables are the region assignments $x_m \\in \\mathcal{S}_m$ for processing and $y_p \\in \\mathcal{R}_p$ for storage. The objective is to minimize the total latency per unit time:\n$$\n\\text{Latency}(x,y) = \\sum_{m \\in \\mathcal{M}} \\sum_{p \\in \\mathcal{P}} A_{mp} \\, D_{x_m, y_p} \\;+\\; \\sum_{\\substack{m,n \\in \\mathcal{M} \\\\ m \\ne n}} B_{mn} \\, D_{x_m, x_n}.\n$$\nSubject to the capacity constraints:\n$$\n\\sum_{m \\in \\mathcal{M}} \\mathbf{1}\\{x_m = i\\} \\le U^{\\text{proc}}_i, \\quad \\forall i \\in \\mathcal{L},\n$$\n$$\n\\sum_{p \\in \\mathcal{P}} \\mathbf{1}\\{y_p = i\\} \\le U^{\\text{stor}}_i, \\quad \\forall i \\in \\mathcal{L},\n$$\nand the sovereignty constraints $x_m \\in \\mathcal{S}_m$ and $y_p \\in \\mathcal{R}_p$ for all $m,p$.\n\nAll latencies $D_{ij}$ are measured in milliseconds and the total latency objective must be expressed in milliseconds as a nonnegative integer or float. Angles are not used. Percentages are not used.\n\nImplement a program that, given test instances below, enumerates feasible assignments and returns the minimal total latency and the corresponding assignments.\n\nUse the following test suite, with explicit parameter values and sovereignty/capacity constraints:\n\n- Test case $1$:\n  - Regions: $L = 3$ with labels $0, 1, 2$.\n  - Latency matrix $D$ (milliseconds):\n    - $D_{00} = 1$, $D_{01} = 8$, $D_{02} = 16$,\n    - $D_{10} = 8$, $D_{11} = 1$, $D_{12} = 12$,\n    - $D_{20} = 16$, $D_{21} = 12$, $D_{22} = 1$.\n  - Microservices: $M = 3$ with allowed processing sets:\n    - $\\mathcal{S}_0 = \\{1\\}$,\n    - $\\mathcal{S}_1 = \\{1, 2\\}$,\n    - $\\mathcal{S}_2 = \\{0, 1\\}$.\n  - Partitions: $P = 3$ with allowed storage sets:\n    - $\\mathcal{R}_0 = \\{0\\}$,\n    - $\\mathcal{R}_1 = \\{0, 2\\}$,\n    - $\\mathcal{R}_2 = \\{2\\}$.\n  - Region capacities:\n    - Processing capacities $U^{\\text{proc}}_0 = 1$, $U^{\\text{proc}}_1 = 2$, $U^{\\text{proc}}_2 = 1$,\n    - Storage capacities $U^{\\text{stor}}_0 = 2$, $U^{\\text{stor}}_1 = 1$, $U^{\\text{stor}}_2 = 1$.\n  - Access demands $A$ (operations per unit time):\n    - Row $0$: $\\{100, 20, 0\\}$,\n    - Row $1$: $\\{50, 10, 40\\}$,\n    - Row $2$: $\\{0, 30, 70\\}$.\n  - Microservice call demands $B$:\n    - Row $0$: $\\{0, 40, 10\\}$,\n    - Row $1$: $\\{40, 0, 20\\}$,\n    - Row $2$: $\\{10, 20, 0\\}$.\n\n- Test case $2$ (boundary condition with forced cross-region activity):\n  - Regions: $L = 2$ with labels $0, 1$.\n  - Latency matrix $D$ (milliseconds):\n    - $D_{00} = 1$, $D_{01} = 5$,\n    - $D_{10} = 5$, $D_{11} = 1$.\n  - Microservices: $M = 1$ with $\\mathcal{S}_0 = \\{1\\}$.\n  - Partitions: $P = 1$ with $\\mathcal{R}_0 = \\{0\\}$.\n  - Region capacities:\n    - Processing capacities $U^{\\text{proc}}_0 = 0$, $U^{\\text{proc}}_1 = 1$,\n    - Storage capacities $U^{\\text{stor}}_0 = 1$, $U^{\\text{stor}}_1 = 0$.\n  - Access demands $A$:\n    - Row $0$: $\\{100\\}$.\n  - Microservice call demands $B$:\n    - Row $0$: $\\{0\\}$.\n\n- Test case $3$ (capacity-constrained trade-offs):\n  - Regions: $L = 4$ with labels $0, 1, 2, 3$.\n  - Latency matrix $D$ (milliseconds):\n    - $D_{00} = 1$, $D_{01} = 12$, $D_{02} = 20$, $D_{03} = 25$,\n    - $D_{10} = 12$, $D_{11} = 1$, $D_{12} = 8$, $D_{13} = 18$,\n    - $D_{20} = 20$, $D_{21} = 8$, $D_{22} = 1$, $D_{23} = 12$,\n    - $D_{30} = 25$, $D_{31} = 18$, $D_{32} = 12$, $D_{33} = 1$.\n  - Microservices: $M = 4$ with allowed processing sets:\n    - $\\mathcal{S}_0 = \\{1\\}$,\n    - $\\mathcal{S}_1 = \\{1, 2\\}$,\n    - $\\mathcal{S}_2 = \\{2\\}$,\n    - $\\mathcal{S}_3 = \\{1, 2\\}$.\n  - Partitions: $P = 3$ with allowed storage sets:\n    - $\\mathcal{R}_0 = \\{0\\}$,\n    - $\\mathcal{R}_1 = \\{0, 3\\}$,\n    - $\\mathcal{R}_2 = \\{3\\}$.\n  - Region capacities:\n    - Processing capacities $U^{\\text{proc}}_0 = 0$, $U^{\\text{proc}}_1 = 3$, $U^{\\text{proc}}_2 = 2$, $U^{\\text{proc}}_3 = 0$,\n    - Storage capacities $U^{\\text{stor}}_0 = 2$, $U^{\\text{stor}}_1 = 0$, $U^{\\text{stor}}_2 = 0$, $U^{\\text{stor}}_3 = 2$.\n  - Access demands $A$:\n    - Row $0$: $\\{200, 0, 0\\}$,\n    - Row $1$: $\\{10, 150, 40\\}$,\n    - Row $2$: $\\{0, 60, 140\\}$,\n    - Row $3$: $\\{5, 30, 20\\}$.\n  - Microservice call demands $B$:\n    - Row $0$: $\\{0, 50, 10, 5\\}$,\n    - Row $1$: $\\{50, 0, 80, 20\\}$,\n    - Row $2$: $\\{10, 80, 0, 30\\}$,\n    - Row $3$: $\\{5, 20, 30, 0\\}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each result must be a list of the form $[T, X, Y]$ where $T$ is the minimal total latency in milliseconds (an integer or float), $X$ is the list of assigned processing regions for microservices in index order, and $Y$ is the list of assigned storage regions for partitions in index order. The final single-line output must therefore have the form $[[T_1, X_1, Y_1],[T_2, X_2, Y_2],[T_3, X_3, Y_3]]$, with no spaces.",
            "solution": "The supplied problem is a mathematical optimization task concerning the optimal placement of microservices and data partitions in a distributed Cyber-Physical System. The goal is to minimize total network latency while respecting data sovereignty and regional capacity constraints.\n\nThe problem is first validated against the specified criteria.\n\n### Step 1: Extract Givens\n- **Sets**: Regions $\\mathcal{L} = \\{0, \\dots, L-1\\}$, microservices $\\mathcal{M} = \\{0, \\dots, M-1\\}$, data partitions $\\mathcal{P} = \\{0, \\dots, P-1\\}$.\n- **Parameters**:\n  - Latency matrix $D_{ij} \\ge 0$ for region pair $(i,j)$.\n  - Allowed processing regions $\\mathcal{S}_m \\subseteq \\mathcal{L}$ for each microservice $m$.\n  - Allowed storage regions $\\mathcal{R}_p \\subseteq \\mathcal{L}$ for each partition $p$.\n  - Processing capacity $U^{\\text{proc}}_i$ for each region $i$.\n  - Storage capacity $U^{\\text{stor}}_i$ for each region $i$.\n  - Microservice-to-partition access frequency $A_{mp} \\ge 0$.\n  - Inter-microservice call frequency $B_{mn} \\ge 0$.\n- **Decision Variables**:\n  - $x_m \\in \\mathcal{S}_m$: The region assigned to microservice $m$. The vector of all such assignments is $x = (x_0, \\dots, x_{M-1})$.\n  - $y_p \\in \\mathcal{R}_p$: The region assigned to data partition $p$. The vector of all such assignments is $y = (y_0, \\dots, y_{P-1})$.\n- **Objective Function**: Minimize total latency,\n  $$ \\text{Latency}(x,y) = \\sum_{m \\in \\mathcal{M}} \\sum_{p \\in \\mathcal{P}} A_{mp} \\, D_{x_m, y_p} \\;+\\; \\sum_{\\substack{m,n \\in \\mathcal{M} \\\\ m \\ne n}} B_{mn} \\, D_{x_m, x_n} $$\n- **Constraints**:\n  - **Processing Capacity**: $\\sum_{m \\in \\mathcal{M}} \\mathbf{1}\\{x_m = i\\} \\le U^{\\text{proc}}_i, \\quad \\forall i \\in \\mathcal{L}$.\n  - **Storage Capacity**: $\\sum_{p \\in \\mathcal{P}} \\mathbf{1}\\{y_p = i\\} \\le U^{\\text{stor}}_i, \\quad \\forall i \\in \\mathcal{L}$.\n  - **Sovereignty**: $x_m \\in \\mathcal{S}_m$ and $y_p \\in \\mathcal{R}_p$ are enforced by the definition of the decision variables.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is a well-defined combinatorial optimization problem. It combines features of the Generalized Assignment Problem (for the first term of the objective) and the Quadratic Assignment Problem (for the second term). The formulation is mathematically and scientifically sound, representing a realistic challenge in distributed systems design. All parameters, constraints, and objectives are clearly and objectively defined. The provided test cases are complete and internally consistent. The problem is therefore deemed **valid**.\n\n### Step 3: Verdict and Action\nThe problem is valid. A solution will be provided.\n\n### Principle-Based Design\nThe problem is to find integer-valued vectors $x$ and $y$ that minimize a nonlinear objective function subject to a set of linear and set-membership constraints. In general, problems of this structure are NP-hard, meaning no known algorithm can find the optimal solution in polynomial time for all instances.\n\nHowever, the specific test cases provided involve a small number of microservices ($M$), partitions ($P$), and regions ($L$). The number of possible choices for each assignment ($x_m$ and $y_p$) is also limited by the sovereignty constraints ($\\mathcal{S}_m$ and $\\mathcal{R}_p$). This results in a search space of manageable size. Therefore, the most direct and reliable method to find the guaranteed optimal solution is **complete enumeration**, also known as exhaustive search.\n\nThe solution algorithm is designed as follows:\n1.  **Generate Sovereign Assignments**: First, we generate all possible assignment vectors for microservices, denoted as $x = (x_0, \\dots, x_{M-1})$, where each $x_m$ is chosen from its allowed set $\\mathcal{S}_m$. Similarly, we generate all possible assignment vectors for data partitions, $y = (y_0, \\dots, y_{P-1})$, where each $y_p \\in \\mathcal{R}_p$. This step ensures that all considered assignments satisfy the sovereignty constraints by construction. The Cartesian product of the allowed sets for each entity, $\\prod_{m \\in \\mathcal{M}} \\mathcal{S}_m$ and $\\prod_{p \\in \\mathcal{P}} \\mathcal{R}_p$, provides these sets of candidate assignments.\n\n2.  **Filter by Feasibility**: We iterate through every pair of candidate assignments $(x, y)$. For each pair, we check its feasibility against the capacity constraints.\n    - An assignment $x$ is feasible if, for every region $i \\in \\mathcal{L}$, the number of microservices assigned to it, $\\sum_{m \\in \\mathcal{M}} \\mathbf{1}\\{x_m=i\\}$, does not exceed its processing capacity $U^{\\text{proc}}_i$.\n    - An assignment $y$ is feasible if, for every region $i \\in \\mathcal{L}$, the number of partitions assigned to it, $\\sum_{p \\in \\mathcal{P}} \\mathbf{1}\\{y_p=i\\}$, does not exceed its storage capacity $U^{\\text{stor}}_i$.\n    - Any pair $(x, y)$ that violates one or more of these capacity constraints is discarded.\n\n3.  **Evaluate Objective Function**: For each feasible assignment pair $(x, y)$, the total latency is calculated using the objective function:\n    $$ \\text{Total Latency} = \\underbrace{\\sum_{m=0}^{M-1} \\sum_{p=0}^{P-1} A_{mp} D_{x_m, y_p}}_{\\text{Data Access Latency}} + \\underbrace{\\sum_{m=0}^{M-1} \\sum_{n=0, n \\neq m}^{M-1} B_{mn} D_{x_m, x_n}}_{\\text{Inter-Service Latency}} $$\n\n4.  **Identify Optimum**: We maintain a record of the minimum latency found so far and the corresponding assignment $(x, y)$ that produced it. As we evaluate each feasible assignment, if its total latency is lower than the current minimum, we update the minimum latency and store the new optimal assignment.\n\nAfter enumerating all valid combinations, the final recorded assignment is the global optimum for the given problem instance. This brute-force approach, while inefficient for large-scale problems, is perfectly suited for the small, well-defined instances in the test suite and guarantees correctness.",
            "answer": "[[3270.0,[1,1,0],[0,0,2]],[500.0,[1],[0]],[5365.0,[1,2,2,1],[0,3,3]]]"
        }
    ]
}