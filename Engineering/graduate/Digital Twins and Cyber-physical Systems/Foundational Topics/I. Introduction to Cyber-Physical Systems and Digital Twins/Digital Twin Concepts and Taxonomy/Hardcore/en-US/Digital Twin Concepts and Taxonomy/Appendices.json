{
    "hands_on_practices": [
        {
            "introduction": "A digital twin's fidelity begins with the data it receives from its physical counterpart. This exercise explores the foundational principle of signal sampling, the Nyquist-Shannon theorem, which dictates the minimum rate at which a continuous signal must be sampled to be perfectly reconstructed in the digital domain. Mastering this principle and the associated concept of anti-aliasing is critical for designing the data acquisition layer of any cyber-physical system to prevent irreversible information loss. ",
            "id": "4213713",
            "problem": "A continuous-time sensor signal from a Cyber-Physical System (CPS) is streamed into the data acquisition layer of a Digital Twin (DT). The signal’s measured one-sided power spectral density is negligible beyond $120\\,\\mathrm{Hz}$, and system stakeholders report that the physically meaningful dynamics have dominant frequency content up to $120\\,\\mathrm{Hz}$. The DT requires that the sampled data be sufficient for perfect reconstruction under the assumptions of the Nyquist–Shannon Sampling Theorem (NSST), and that aliasing be practically suppressed by an anti-aliasing analog filter placed prior to sampling.\n\nStarting from the statement of the Nyquist–Shannon Sampling Theorem and the definition of aliasing in time-discrete sampling, reason from first principles to determine the minimum sampling frequency that guarantees perfect reconstruction for a strictly bandlimited signal whose highest non-zero frequency component is $120\\,\\mathrm{Hz}$.\n\nThen, discuss the qualitative requirements that the anti-aliasing filter must satisfy in the CPS-to-DT ingestion pipeline to make the bandlimited assumption true in practice. Address passband and stopband specifications in terms of the sampling frequency and the signal’s dominant content and explain how the transition band relates to the achievable suppression of out-of-band energy.\n\nExpress the final sampling frequency numerically in hertz ($\\mathrm{Hz}$). No rounding is required; report the exact value.",
            "solution": "The problem requires a determination of the minimum sampling frequency for a bandlimited signal based on the Nyquist–Shannon Sampling Theorem (NSST), followed by a qualitative discussion of the practical requirements for an anti-aliasing filter.\n\nFirst, we address the determination of the minimum sampling frequency from first principles.\n\nThe Nyquist–Shannon Sampling Theorem establishes the fundamental condition under which a continuous-time signal can be perfectly reconstructed from a sequence of its uniformly spaced samples. Let $x(t)$ be a continuous-time signal and its Fourier transform be $X(f)$. The signal $x(t)$ is said to be bandlimited to a maximum frequency $f_{\\text{max}}$ if its Fourier transform $X(f)$ is zero for all frequencies $|f| > f_{\\text{max}}$.\n$$X(f) = 0 \\quad \\text{for} \\quad |f| > f_{\\text{max}}$$\nSampling the signal $x(t)$ at a frequency $f_s$ with a sampling period $T_s = 1/f_s$ produces a discrete-time sequence $x[n] = x(nT_s)$. The process of ideal sampling can be modeled as multiplying the continuous-time signal by an impulse train. The Fourier transform of the sampled signal, denoted $X_s(f)$, is a periodic summation of the original signal's spectrum:\n$$X_s(f) = f_s \\sum_{k=-\\infty}^{\\infty} X(f - kf_s)$$\nThis equation shows that the spectrum of the sampled signal consists of replicas of the original spectrum $X(f)$, scaled by $f_s$ and shifted to be centered at integer multiples of the sampling frequency, $kf_s$.\n\nAliasing occurs when these spectral replicas overlap. If the original signal is bandlimited to $f_{\\text{max}}$, the central replica (for $k=0$) occupies the frequency band $[-f_{\\text{max}}, f_{\\text{max}}]$. The adjacent replicas (for $k = \\pm 1$) are centered at $\\pm f_s$ and occupy the bands $[f_s - f_{\\text{max}}, f_s + f_{\\text{max}}]$ and $[-f_s - f_{\\text{max}}, -f_s + f_{\\text{max}}]$. For the replicas not to overlap, the upper edge of the central replica ($f_{\\text{max}}$) must not exceed the lower edge of the first positive replica ($f_s - f_{\\text{max}}$). This leads to the condition:\n$$f_{\\text{max}} \\le f_s - f_{\\text{max}}$$\nRearranging this inequality gives the Nyquist criterion:\n$$f_s \\ge 2f_{\\text{max}}$$\nIf this condition holds, the original signal can be perfectly reconstructed from its samples by passing the sampled signal through an ideal low-pass filter with a cutoff frequency between $f_{\\text{max}}$ and $f_s - f_{\\text{max}}$. The frequency $2f_{\\text{max}}$ is known as the Nyquist rate. The sampling frequency $f_s$ must be at least the Nyquist rate. Therefore, the minimum sampling frequency, $f_{s, \\text{min}}$, that guarantees perfect reconstruction is precisely the Nyquist rate.\n\nThe problem states that the signal is strictly bandlimited with the highest non-zero frequency component at $f_{\\text{max}} = 120\\,\\mathrm{Hz}$. Applying the Nyquist criterion, the minimum sampling frequency is:\n$$f_{s, \\text{min}} = 2 \\times f_{\\text{max}} = 2 \\times 120\\,\\mathrm{Hz} = 240\\,\\mathrm{Hz}$$\n\nNext, we discuss the qualitative requirements for the anti-aliasing filter.\n\nThe premise of a \"strictly bandlimited\" signal is a mathematical idealization. Real-world signals, such as those from a Cyber-Physical System sensor, are typically not strictly bandlimited. The problem text acknowledges this by stating the signal's power spectral density is \"negligible\" beyond $120\\,\\mathrm{Hz}$. An anti-aliasing filter, which is an analog low-pass filter, is required to be placed before the analog-to-digital converter (sampler) to enforce the bandlimited condition in practice. Its function is to attenuate frequency components above a certain cutoff to a level where they will not cause significant aliasing upon sampling.\n\nThe specifications of this filter are defined by its behavior in three regions: the passband, the stopband, and the transition band.\n\n$1$. **Passband**: This is the range of frequencies that the filter must pass with minimal distortion. For this problem, the physically meaningful dynamics have content up to $120\\,\\mathrm{Hz}$. Therefore, the filter's passband must extend up to this frequency. The passband edge frequency, $f_p$, should be set at or just above the highest frequency of interest, so $f_p \\ge 120\\,\\mathrm{Hz}$. Within this band, $[0, f_p]$, the filter should ideally have a flat magnitude response (e.g., gain close to $1$) and a linear phase response (constant group delay) to avoid distorting the signal.\n\n$2$. **Stopband**: This is the range of frequencies that the filter must significantly attenuate. Aliasing folds frequencies above the Nyquist frequency, $f_s/2$, back into the baseband $[0, f_s/2]$. Therefore, to prevent aliasing, the filter's stopband must begin at or before the Nyquist frequency. The stopband edge frequency is denoted $f_{\\text{stop}}$. The attenuation in the stopband (e.g., measured in decibels, dB) must be sufficient to suppress any out-of-band noise and interference to below the noise floor or quantization error of the system.\n\n$3$. **Transition Band**: A physical filter cannot transition instantaneously from the passband to the stopband. This region of finite width, from $f_p$ to $f_{\\text{stop}}$, is the transition band. The steepness of the filter's response in this band is its \"roll-off\".\n\nThe relationship between these parameters and the sampling frequency is critical. If we were to sample at the theoretical minimum, $f_s = 240\\,\\mathrm{Hz}$, the Nyquist frequency would be $f_s/2 = 120\\,\\mathrm{Hz}$. In this case, the passband must extend up to $f_{\\text{max}} = 120\\,\\mathrm{Hz}$, and the stopband must begin at $f_s/2 = 120\\,\\mathrm{Hz}$. This implies $f_p = f_{\\text{stop}} = 120\\,\\mathrm{Hz}$, which corresponds to a zero-width transition band. Such a filter is a \"brick-wall\" filter, which is physically unrealizable.\n\nTo accommodate a real-world filter with a non-zero transition band, the sampling frequency must be chosen to be greater than the Nyquist rate ($f_s > 2f_{\\text{max}}$). This technique, known as oversampling, creates a \"guard band\" in the frequency domain between the highest frequency of interest, $f_{\\text{max}}$, and the Nyquist frequency, $f_s/2$. The width of this guard band is $(f_s/2) - f_{\\text{max}}$.\nThe filter can now be designed with its passband edge at $f_p \\approx f_{\\text{max}}~(=120\\,\\mathrm{Hz})$ and its stopband edge at $f_{\\text{stop}} \\le f_s/2$. The transition band, of width $f_{\\text{stop}} - f_p$, now fits entirely within the guard band.\n\nThe achievable suppression of out-of-band energy is determined by the filter's order and its roll-off characteristics. A steeper roll-off (associated with a higher-order filter) allows for a narrower transition band. This means that for a given level of stopband attenuation, a steeper filter allows the sampling frequency $f_s$ to be closer to the theoretical minimum of $2f_{\\text{max}}$. Conversely, using a higher sampling frequency (a wider guard band) relaxes the requirement on the filter's roll-off, potentially allowing for a simpler, lower-order filter to achieve the necessary suppression of aliased components.",
            "answer": "$$\\boxed{240}$$"
        },
        {
            "introduction": "A key function of a dynamic digital twin is to continuously synchronize its virtual state with its physical counterpart, a process complicated by sensor noise and model uncertainties. This practice delves into the heart of probabilistic state estimation by requiring a derivation of the Kalman filter from first principles, which provides the optimal solution for tracking linear systems under Gaussian noise assumptions. By working through this derivation and a concrete example, you will gain a deep understanding of how to fuse sensor data with a dynamic model to maintain a high-fidelity state representation. ",
            "id": "4213735",
            "problem": "A Digital Twin (DT) of a Cyber-Physical System (CPS) uses probabilistic state estimation to synchronize its virtual state with the physical asset. Consider a continuous-time Linear Time-Invariant (LTI) stochastic system with a Gaussian process model and sampled measurements:\n$$\\dot{x}(t) = A x(t) + B u(t) + w(t), \\quad y_{k} = C x(t_{k}) + v_{k},$$\nwhere $x(t) \\in \\mathbb{R}^{n}$ is the physical state, $u(t) \\in \\mathbb{R}^{m}$ is a known input, $y_{k} \\in \\mathbb{R}^{p}$ is the measurement at sampling instants $t_{k} = k \\Delta$, $w(t)$ is zero-mean Gaussian white process noise with spectral density (covariance rate) $Q \\succeq 0$, and $v_{k}$ is zero-mean independent and identically distributed Gaussian measurement noise with covariance $R \\succ 0$. Assume $x(0)$ is Gaussian, independent of $w(t)$ and $v_{k}$. The Digital Twin’s estimator must be derived from first principles (linear-Gaussian modeling, discretization of the continuous-time dynamics, and Bayesian conditioning).\n\nTask 1: Using the foundational laws of linear system discretization and Gaussian inference, derive the discrete-time Kalman Filter (KF) recursion implemented by the DT at sampling interval $\\Delta$, explicitly expressing the time update and measurement update equations in terms of $A_{\\Delta}$, $B_{\\Delta}$, and $Q_{\\Delta}$ obtained from the continuous-time model. Your derivation must start from the continuous-time stochastic model, produce the discrete-time equivalent with $x_{k+1} = A_{\\Delta} x_{k} + B_{\\Delta} u_{k} + w_{k}$, $y_{k} = C x_{k} + v_{k}$, and express $A_{\\Delta}$, $B_{\\Delta}$, and $Q_{\\Delta}$ from $A$, $B$, $Q$, and $\\Delta$ using scientifically standard integrals and operators. Then derive the Kalman time update for $(\\hat{x}_{k+1|k}, P_{k+1|k})$ and the measurement update for $(\\hat{x}_{k+1|k+1}, P_{k+1|k+1})$ and the gain $K_{k+1}$, all based on Gaussian conditioning, without invoking any unproven shortcut formulas.\n\nTask 2: Specialize your derived recursion to a scalar DT estimator for a single-input single-output CPS with parameters $A = -1$, $B = 0$, $C = 1$, $Q = 2$, $R = 1$, sampling interval $\\Delta = 0.5$, zero input $u_{k} = 0$ for all $k$, and initial posterior covariance $P_{0|0} = 3$. Compute the posterior covariance after two complete measurement updates, $P_{2|2}$. Round your final numerical answer to four significant figures. Express your answer as unitless.",
            "solution": "The problem presented is scientifically sound, well-posed, and contains all necessary information for a unique solution. It is a standard problem in state estimation for linear systems, central to control theory and its application in digital twins. We will proceed with a full derivation and calculation as requested.\n\nThe solution is divided into two tasks. First, the derivation of the general discrete-time Kalman Filter from the given continuous-time model. Second, the application of this filter to a specific scalar system to compute the posterior covariance after two steps.\n\n**Task 1: Derivation of the Discrete-Time Kalman Filter**\n\nThe derivation starts from the continuous-time stochastic differential equation and proceeds to the discretization of the system dynamics, followed by the derivation of the Kalman Filter equations based on Bayesian inference for linear-Gaussian systems.\n\n**Discretization of the Continuous-Time Model**\nThe state of the physical system is governed by the linear stochastic differential equation:\n$$\n\\dot{x}(t) = A x(t) + B u(t) + w(t)\n$$\nThe formal solution to this equation over the time interval $[t_k, t_{k+1}]$, where $t_{k+1} - t_k = \\Delta$, is given by the variation of constants formula:\n$$\nx(t_{k+1}) = \\exp(A\\Delta) x(t_k) + \\int_{t_k}^{t_{k+1}} \\exp(A(t_{k+1} - \\tau)) [B u(\\tau) + w(\\tau)] d\\tau\n$$\nWe define $x_k \\equiv x(t_k)$. Assuming a zero-order hold on the input, where $u(\\tau) = u_k$ for $\\tau \\in [t_k, t_{k+1})$, the equation becomes:\n$$\nx_{k+1} = \\exp(A\\Delta) x_k + \\left(\\int_{t_k}^{t_{k+1}} \\exp(A(t_{k+1} - \\tau)) d\\tau \\right) B u_k + \\int_{t_k}^{t_{k+1}} \\exp(A(t_{k+1} - \\tau)) w(\\tau) d\\tau\n$$\nBy performing a change of variable $\\sigma = t_{k+1} - \\tau$ in the first integral, we obtain the discrete-time state-space model:\n$$\nx_{k+1} = A_{\\Delta} x_k + B_{\\Delta} u_k + w_k\n$$\nwhere the discrete-time system matrix $A_{\\Delta}$, input matrix $B_{\\Delta}$, and process noise term $w_k$ are defined as:\n$$\nA_{\\Delta} = \\exp(A\\Delta)\n$$\n$$\nB_{\\Delta} = \\left(\\int_{0}^{\\Delta} \\exp(A\\sigma) d\\sigma\\right) B\n$$\n$$\nw_k = \\int_{t_k}^{t_{k+1}} \\exp(A(t_{k+1} - \\tau)) w(\\tau) d\\tau\n$$\nThe term $w_k$ is a zero-mean Gaussian random variable, as it is a linear transformation of the Gaussian process $w(t)$. Its covariance, $Q_{\\Delta}$, is given by:\n$$\nQ_{\\Delta} = \\mathbb{E}[w_k w_k^T] = \\mathbb{E}\\left[ \\left(\\int_{t_k}^{t_{k+1}} \\exp(A(t_{k+1} - \\tau_1)) w(\\tau_1) d\\tau_1 \\right) \\left(\\int_{t_k}^{t_{k+1}} \\exp(A(t_{k+1} - \\tau_2)) w(\\tau_2) d\\tau_2 \\right)^T \\right]\n$$\n$$\nQ_{\\Delta} = \\int_{t_k}^{t_{k+1}} \\int_{t_k}^{t_{k+1}} \\exp(A(t_{k+1} - \\tau_1)) \\mathbb{E}[w(\\tau_1) w(\\tau_2)^T] \\exp(A(t_{k+1} - \\tau_2))^T d\\tau_1 d\\tau_2\n$$\nThe continuous-time process noise $w(t)$ is white, meaning its autocorrelation is $\\mathbb{E}[w(\\tau_1) w(\\tau_2)^T] = Q \\delta(\\tau_1 - \\tau_2)$, where $\\delta(\\cdot)$ is the Dirac delta function. Substituting this and using the sifting property of the delta function, we get:\n$$\nQ_{\\Delta} = \\int_{t_k}^{t_{k+1}} \\exp(A(t_{k+1} - \\tau)) Q \\exp(A(t_{k+1} - \\tau))^T d\\tau\n$$\nWith the change of variable $\\sigma = t_{k+1} - \\tau$, this becomes:\n$$\nQ_{\\Delta} = \\int_{0}^{\\Delta} \\exp(A\\sigma) Q \\exp(A\\sigma)^T d\\sigma\n$$\nThe full discrete-time model is thus:\n$$\nx_{k+1} = A_{\\Delta} x_k + B_{\\Delta} u_k + w_k, \\quad w_k \\sim \\mathcal{N}(0, Q_{\\Delta})\n$$\n$$\ny_k = C x_k + v_k, \\quad v_k \\sim \\mathcal{N}(0, R)\n$$\n\n**Derivation of the Kalman Filter Recursion**\nThe Kalman filter is a recursive Bayesian estimator. We assume that at time $k$, the posterior distribution of the state $x_k$ given all measurements up to time $k$, denoted $y_{1:k}$, is Gaussian: $p(x_k | y_{1:k}) = \\mathcal{N}(x_k; \\hat{x}_{k|k}, P_{k|k})$. The recursion consists of two steps: time update and measurement update.\n\n**Time Update (Prediction):**\nThe goal is to find the distribution of $x_{k+1}$ given measurements $y_{1:k}$. This is the prior distribution for step $k+1$.\n$$\np(x_{k+1} | y_{1:k}) = \\int p(x_{k+1} | x_k) p(x_k | y_{1:k}) dx_k\n$$\nSince both distributions in the integral are Gaussian and the relationship $x_{k+1} = A_{\\Delta}x_k + B_{\\Delta}u_k + w_k$ is linear, the resulting distribution for $x_{k+1}$ is also Gaussian. Its mean and covariance are found as follows.\nThe predicted mean is the expectation of $x_{k+1}$ conditioned on $y_{1:k}$:\n$$\n\\hat{x}_{k+1|k} = \\mathbb{E}[x_{k+1} | y_{1:k}] = \\mathbb{E}[A_{\\Delta} x_k + B_{\\Delta} u_k + w_k | y_{1:k}]\n$$\n$$\n\\hat{x}_{k+1|k} = A_{\\Delta} \\mathbb{E}[x_k | y_{1:k}] + B_{\\Delta} u_k + \\mathbb{E}[w_k] = A_{\\Delta} \\hat{x}_{k|k} + B_{\\Delta} u_k\n$$\nThe predicted covariance is:\n$$\nP_{k+1|k} = \\mathbb{E}[(x_{k+1} - \\hat{x}_{k+1|k})(x_{k+1} - \\hat{x}_{k+1|k})^T | y_{1:k}]\n$$\n$$\nP_{k+1|k} = \\mathbb{E}[(A_{\\Delta}(x_k - \\hat{x}_{k|k}) + w_k)(A_{\\Delta}(x_k - \\hat{x}_{k|k}) + w_k)^T | y_{1:k}]\n$$\nSince the estimation error $(x_k - \\hat{x}_{k|k})$ is independent of the future process noise $w_k$, the cross-terms are zero:\n$$\nP_{k+1|k} = A_{\\Delta} \\mathbb{E}[(x_k - \\hat{x}_{k|k})(x_k - \\hat{x}_{k|k})^T | y_{1:k}] A_{\\Delta}^T + \\mathbb{E}[w_k w_k^T]\n$$\n$$\nP_{k+1|k} = A_{\\Delta} P_{k|k} A_{\\Delta}^T + Q_{\\Delta}\n$$\nSo, the prior distribution at step $k+1$ is $p(x_{k+1} | y_{1:k}) = \\mathcal{N}(x_{k+1}; \\hat{x}_{k+1|k}, P_{k+1|k})$.\n\n**Measurement Update (Correction):**\nUpon receiving the measurement $y_{k+1}$, we update the prior to obtain the posterior $p(x_{k+1} | y_{1:k+1})$. Using Bayes' rule:\n$$\np(x_{k+1} | y_{1:k+1}) \\propto p(y_{k+1} | x_{k+1}) p(x_{k+1} | y_{1:k})\n$$\nThe likelihood $p(y_{k+1} | x_{k+1})$ is obtained from the measurement model $y_{k+1} = C x_{k+1} + v_{k+1}$, which implies $p(y_{k+1} | x_{k+1}) = \\mathcal{N}(y_{k+1}; C x_{k+1}, R)$. We are multiplying two Gaussian distributions. The exponent of the resulting posterior is the sum of the exponents of the prior and the likelihood (ignoring constant terms):\n$$\nL = -\\frac{1}{2} (x_{k+1} - \\hat{x}_{k+1|k})^T P_{k+1|k}^{-1} (x_{k+1} - \\hat{x}_{k+1|k}) -\\frac{1}{2} (y_{k+1} - C x_{k+1})^T R^{-1} (y_{k+1} - C x_{k+1})\n$$\nExpanding and collecting terms quadratic and linear in $x_{k+1}$:\n$$\nL = -\\frac{1}{2} \\left( x_{k+1}^T(P_{k+1|k}^{-1} + C^T R^{-1} C)x_{k+1} - 2x_{k+1}^T(P_{k+1|k}^{-1} \\hat{x}_{k+1|k} + C^T R^{-1} y_{k+1}) + \\dots \\right)\n$$\nBy completing the square, we can identify the mean $\\hat{x}_{k+1|k+1}$ and covariance $P_{k+1|k+1}$ of the posterior Gaussian. The inverse of the posterior covariance is:\n$$\nP_{k+1|k+1}^{-1} = P_{k+1|k}^{-1} + C^T R^{-1} C\n$$\nAnd the posterior mean satisfies:\n$$\nP_{k+1|k+1}^{-1} \\hat{x}_{k+1|k+1} = P_{k+1|k}^{-1} \\hat{x}_{k+1|k} + C^T R^{-1} y_{k+1}\n$$\n$$\n\\hat{x}_{k+1|k+1} = P_{k+1|k+1} (P_{k+1|k}^{-1} \\hat{x}_{k+1|k} + C^T R^{-1} y_{k+1})\n$$\nTo get the standard filter equations, we apply the Woodbury matrix identity to $P_{k+1|k+1}^{-1}$:\n$$\nP_{k+1|k+1} = (P_{k+1|k}^{-1} + C^T R^{-1} C)^{-1} = P_{k+1|k} - P_{k+1|k} C^T (C P_{k+1|k} C^T + R)^{-1} C P_{k+1|k}\n$$\nWe define the Kalman Gain $K_{k+1}$ as:\n$$\nK_{k+1} = P_{k+1|k} C^T (C P_{k+1|k} C^T + R)^{-1}\n$$\nThe posterior covariance can then be written as:\n$$\nP_{k+1|k+1} = P_{k+1|k} - K_{k+1} C P_{k+1|k} = (I - K_{k+1} C) P_{k+1|k}\n$$\nSubstituting this back into the expression for $\\hat{x}_{k+1|k+1}$:\n$$\n\\hat{x}_{k+1|k+1} = (I - K_{k+1} C) P_{k+1|k} (P_{k+1|k}^{-1} \\hat{x}_{k+1|k} + C^T R^{-1} y_{k+1})\n$$\n$$\n\\hat{x}_{k+1|k+1} = (I - K_{k+1} C) \\hat{x}_{k+1|k} + (P_{k+1|k} - K_{k+1} C P_{k+1|k}) C^T R^{-1} y_{k+1}\n$$\nUsing $K_{k+1} (C P_{k+1|k} C^T + R) = P_{k+1|k} C^T$, which implies $K_{k+1} C P_{k+1|k} C^T + K_{k+1} R = P_{k+1|k} C^T$, we can show that $(P_{k+1|k} - K_{k+1} C P_{k+1|k}) C^T R^{-1} = K_{k+1}$.\nTherefore, the posterior mean update is:\n$$\n\\hat{x}_{k+1|k+1} = (I - K_{k+1} C) \\hat{x}_{k+1|k} + K_{k+1} y_{k+1} = \\hat{x}_{k+1|k} + K_{k+1}(y_{k+1} - C \\hat{x}_{k+1|k})\n$$\n\n**Task 2: Application to Scalar System**\n\nWe now apply these derived equations to the specific scalar system.\nThe given parameters are: $A = -1$, $B = 0$, $C = 1$, $Q = 2$, $R = 1$, $\\Delta = 0.5$.\nThe initial posterior covariance is $P_{0|0} = 3$.\n\n**Parameter Calculation**\nFirst, we compute the discrete-time parameters $A_{\\Delta}$ and $Q_{\\Delta}$.\n$$\nA_{\\Delta} = \\exp(A\\Delta) = \\exp((-1)(0.5)) = \\exp(-0.5)\n$$\nSince $B = 0$, we have $B_{\\Delta} = 0$.\nThe discrete process noise covariance $Q_{\\Delta}$ is:\n$$\nQ_{\\Delta} = \\int_{0}^{\\Delta} \\exp(A\\sigma) Q \\exp(A\\sigma)^T d\\sigma = \\int_{0}^{0.5} \\exp(-1\\sigma) \\cdot 2 \\cdot \\exp(-1\\sigma) d\\sigma = 2 \\int_{0}^{0.5} \\exp(-2\\sigma) d\\sigma\n$$\n$$\nQ_{\\Delta} = 2 \\left[ \\frac{\\exp(-2\\sigma)}{-2} \\right]_{0}^{0.5} = -[\\exp(-2 \\cdot 0.5) - \\exp(0)] = -(\\exp(-1) - 1) = 1 - \\exp(-1)\n$$\n\n**Covariance Propagation**\nWe need to compute $P_{2|2}$ starting from $P_{0|0} = 3$. The recursion involves propagation of the covariance only.\n\n**Step 1: from $k=0$ to $k=1$**\nTime Update (Prediction for $k=1$):\n$$\nP_{1|0} = A_{\\Delta} P_{0|0} A_{\\Delta}^T + Q_{\\Delta} = A_{\\Delta}^2 P_{0|0} + Q_{\\Delta}\n$$\n$$\nP_{1|0} = (\\exp(-0.5))^2 \\cdot 3 + (1 - \\exp(-1)) = 3\\exp(-1) + 1 - \\exp(-1) = 1 + 2\\exp(-1)\n$$\nMeasurement Update (Correction at $k=1$):\nFor the scalar case, the covariance update equation simplifies. The Kalman gain is $K_1 = P_{1|0}C^T(C P_{1|0} C^T + R)^{-1} = P_{1|0}(1 \\cdot P_{1|0} \\cdot 1 + 1)^{-1} = \\frac{P_{1|0}}{P_{1|0}+1}$.\nThe posterior covariance is $P_{1|1} = (1 - K_1 C)P_{1|0} = (1 - K_1)P_{1|0}$.\n$$\nP_{1|1} = \\left(1 - \\frac{P_{1|0}}{P_{1|0}+1}\\right) P_{1|0} = \\left(\\frac{1}{P_{1|0}+1}\\right) P_{1|0} = \\frac{P_{1|0}}{P_{1|0}+1}\n$$\nSubstituting the value of $P_{1|0}$:\n$$\nP_{1|1} = \\frac{1 + 2\\exp(-1)}{1 + 2\\exp(-1) + 1} = \\frac{1 + 2\\exp(-1)}{2 + 2\\exp(-1)}\n$$\n\n**Step 2: from $k=1$ to $k=2$**\nTime Update (Prediction for $k=2$):\n$$\nP_{2|1} = A_{\\Delta}^2 P_{1|1} + Q_{\\Delta} = \\exp(-1) P_{1|1} + (1 - \\exp(-1))\n$$\n$$\nP_{2|1} = \\exp(-1)\\left(\\frac{1 + 2\\exp(-1)}{2 + 2\\exp(-1)}\\right) + (1 - \\exp(-1)) = \\frac{\\exp(-1) + 2\\exp(-2)}{2(1 + \\exp(-1))} + \\frac{(1 - \\exp(-1)) \\cdot 2(1 + \\exp(-1))}{2(1 + \\exp(-1))}\n$$\n$$\nP_{2|1} = \\frac{\\exp(-1) + 2\\exp(-2) + 2(1 - \\exp(-2))}{2(1 + \\exp(-1))} = \\frac{\\exp(-1) + 2\\exp(-2) + 2 - 2\\exp(-2)}{2(1 + \\exp(-1))}\n$$\n$$\nP_{2|1} = \\frac{2 + \\exp(-1)}{2(1 + \\exp(-1))}\n$$\nMeasurement Update (Correction at $k=2$):\nSimilar to the previous step, the posterior covariance is $P_{2|2} = \\frac{P_{2|1}}{P_{2|1}+1}$.\n$$\nP_{2|2} = \\frac{\\frac{2 + \\exp(-1)}{2(1 + \\exp(-1))}}{\\frac{2 + \\exp(-1)}{2(1 + \\exp(-1))} + 1} = \\frac{2 + \\exp(-1)}{2 + \\exp(-1) + 2(1 + \\exp(-1))}\n$$\n$$\nP_{2|2} = \\frac{2 + \\exp(-1)}{2 + \\exp(-1) + 2 + 2\\exp(-1)} = \\frac{2 + \\exp(-1)}{4 + 3\\exp(-1)}\n$$\n\n**Numerical Result**\nWe now compute the final numerical value of $P_{2|2}$.\nUsing the value $\\exp(-1) \\approx 0.36787944$:\n$$\nP_{2|2} = \\frac{2 + 0.36787944}{4 + 3(0.36787944)} = \\frac{2.36787944}{4 + 1.10363832} = \\frac{2.36787944}{5.10363832} \\approx 0.463953503\n$$\nRounding to four significant figures, we get $0.4640$.\nThe posterior covariance after two measurement updates is $P_{2|2} \\approx 0.4640$.",
            "answer": "$$\n\\boxed{0.4640}\n$$"
        },
        {
            "introduction": "For a digital twin to operate in real-time, its underlying model must often be computationally efficient without sacrificing essential predictive accuracy. This exercise introduces Proper Orthogonal Decomposition (POD), a powerful technique for model order reduction that extracts the most dominant features from a dataset of system snapshots. By understanding how POD arises from the Singular Value Decomposition (SVD) and applying it to quantify captured energy, you will gain a practical tool for building data-driven surrogate models that balance fidelity and computational cost. ",
            "id": "4213771",
            "problem": "A cyber-physical system digital twin is constructed as a reduced-order surrogate of a high-dimensional state evolving under sensing and actuation. Consider a snapshot matrix $X \\in \\mathbb{R}^{m \\times n}$ whose columns collect $n$ zero-mean state snapshots in $\\mathbb{R}^{m}$. Proper Orthogonal Decomposition (POD) seeks an orthonormal reduced basis that maximizes the captured variation of the snapshots while minimizing the least-squares reconstruction error, consistent with the digital twin taxonomy requirement that a predictive twin maintains a specified fidelity threshold measured by the captured energy in the data. The energy associated with a set of snapshots can be defined by the squared norm of the data, and the projection onto a reduced orthonormal basis quantifies how much of this energy is captured by the reduced representation. Starting from the definitions of orthonormal projection and least-squares optimality, derive how the POD reduced basis arises from the Singular Value Decomposition (SVD) of $X$ and express the captured energy of a rank-$r$ POD model in terms of the singular values of $X$. Then, for the specific digital twin calibration dataset\n$$\nX \\;=\\; \\begin{pmatrix}\n5 & 0 & 0 \\\\\n0 & 3 & 0 \\\\\n0 & 0 & 1\n\\end{pmatrix} \\in \\mathbb{R}^{3 \\times 3},\n$$\ndetermine the minimal rank $r \\in \\{1,2,3\\}$ such that the fraction of captured energy by the POD model is at least $\\alpha = 0.95$. Express your final answer as the integer value of $r$. No rounding is required, and no physical units are involved.",
            "solution": "We begin from the fundamental definitions underpinning Proper Orthogonal Decomposition (POD) and orthonormal projections. Let $X \\in \\mathbb{R}^{m \\times n}$ collect $n$ zero-mean snapshots as columns. A rank-$r$ POD seeks an orthonormal basis $U \\in \\mathbb{R}^{m \\times r}$, with $U^{\\top}U = I_{r}$, that maximizes the captured snapshot energy under orthonormal projection while minimizing the least-squares reconstruction error of the data. The orthonormal projection of $X$ onto the column space of $U$ is $U U^{\\top} X$, and the least-squares reconstruction error in the Frobenius norm is\n$$\n\\|X - U U^{\\top} X\\|_{F}^{2}.\n$$\nEquivalently, the captured energy is\n$$\n\\|U^{\\top} X\\|_{F}^{2} \\;=\\; \\operatorname{trace}\\!\\left( (U^{\\top} X)(U^{\\top} X)^{\\top} \\right) \\;=\\; \\operatorname{trace}\\!\\left( U^{\\top} X X^{\\top} U \\right),\n$$\nsince $U^{\\top}U = I_{r}$ and the Frobenius norm is invariant under orthogonal transformations. The total energy in the snapshots is\n$$\n\\|X\\|_{F}^{2} \\;=\\; \\operatorname{trace}\\!\\left( X X^{\\top} \\right).\n$$\nTherefore, maximizing $\\|U^{\\top} X\\|_{F}^{2}$ over all $U$ with $U^{\\top}U = I_{r}$ is a Rayleigh–Ritz optimization. By the Courant–Fischer min–max principle, the maximizer $U$ is given by the $r$ leading eigenvectors of $X X^{\\top}$. If we write the Singular Value Decomposition (SVD) of $X$ as\n$$\nX \\;=\\; W \\Sigma V^{\\top},\n$$\nwhere $W \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthogonal, and $\\Sigma \\in \\mathbb{R}^{m \\times n}$ is diagonal (rectangular) with nonnegative singular values $\\sigma_{1} \\ge \\sigma_{2} \\ge \\cdots \\ge 0$ on the diagonal, then\n$$\nX X^{\\top} \\;=\\; W \\Sigma \\Sigma^{\\top} W^{\\top}.\n$$\nThe eigenvalues of $X X^{\\top}$ are $\\sigma_{i}^{2}$ and the corresponding eigenvectors are the left singular vectors (columns of $W$). Consequently, the optimal POD basis of rank $r$ is given by the first $r$ left singular vectors,\n$$\nU_{r} \\;=\\; W_{[:,1:r]},\n$$\nand the captured energy of the rank-$r$ POD model is\n$$\n\\|U_{r}^{\\top} X\\|_{F}^{2} \\;=\\; \\sum_{i=1}^{r} \\sigma_{i}^{2}.\n$$\nThe least-squares optimality follows from the equivalence between maximizing captured energy and minimizing reconstruction error, and is classically formalized by the Eckart–Young–Mirsky theorem: among all rank-$r$ approximations, $W_{[:,1:r]} \\Sigma_{[1:r,1:r]} V_{[:,1:r]}^{\\top}$ minimizes $\\|X - \\cdot \\|_{F}$, and the error is $\\sum_{i=r+1}^{\\rho} \\sigma_{i}^{2}$ where $\\rho$ is the rank of $X$.\n\nThe fraction of captured energy at rank $r$ is thus\n$$\n\\eta(r) \\;=\\; \\frac{\\sum_{i=1}^{r} \\sigma_{i}^{2}}{\\sum_{i=1}^{\\rho} \\sigma_{i}^{2}},\n$$\nand the minimal rank satisfying a fidelity threshold $\\alpha$ is\n$$\nr^{\\star} \\;=\\; \\min \\left\\{ r \\in \\{1,2,\\dots,\\rho\\} \\;:\\; \\eta(r) \\ge \\alpha \\right\\}.\n$$\n\nNow apply this to the given snapshot matrix\n$$\nX \\;=\\; \\begin{pmatrix}\n5 & 0 & 0 \\\\\n0 & 3 & 0 \\\\\n0 & 0 & 1\n\\end{pmatrix}.\n$$\nThis $X$ is already diagonal with nonnegative entries, so its singular values are the diagonal entries:\n$$\n\\sigma_{1} \\;=\\; 5,\\quad \\sigma_{2} \\;=\\; 3,\\quad \\sigma_{3} \\;=\\; 1.\n$$\nThe total energy is\n$$\n\\sum_{i=1}^{3} \\sigma_{i}^{2} \\;=\\; 5^{2} + 3^{2} + 1^{2} \\;=\\; 25 + 9 + 1 \\;=\\; 35.\n$$\nCompute the cumulative captured energy for increasing ranks:\n- For $r = 1$: $\\sum_{i=1}^{1} \\sigma_{i}^{2} = 25$, so $\\eta(1) = \\frac{25}{35} = \\frac{5}{7} \\approx 0.7142857$.\n- For $r = 2$: $\\sum_{i=1}^{2} \\sigma_{i}^{2} = 25 + 9 = 34$, so $\\eta(2) = \\frac{34}{35} \\approx 0.9714286$.\n- For $r = 3$: $\\sum_{i=1}^{3} \\sigma_{i}^{2} = 35$, so $\\eta(3) = 1$.\n\nGiven the threshold $\\alpha = 0.95$, the minimal rank satisfying $\\eta(r) \\ge 0.95$ is\n$$\nr^{\\star} \\;=\\; 2,\n$$\nsince $\\eta(1) \\approx 0.7142857 < 0.95$ and $\\eta(2) \\approx 0.9714286 \\ge 0.95$.",
            "answer": "$$\\boxed{2}$$"
        }
    ]
}