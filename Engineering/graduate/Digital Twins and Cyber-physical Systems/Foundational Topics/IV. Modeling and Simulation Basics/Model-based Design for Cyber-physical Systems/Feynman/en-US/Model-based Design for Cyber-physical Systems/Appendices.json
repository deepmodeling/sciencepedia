{
    "hands_on_practices": [
        {
            "introduction": "A cyber-physical system's model is only as good as its representation of the physical world. This practice begins at the fundamental interface between analog reality and digital computation: the Analog-to-Digital Converter (ADC). By deriving the quantization step $\\Delta$ and the signal-to-quantization-noise ratio (SQNR) from first principles, you will gain a core understanding of the information loss inherent in digitization and how to model these effects in a digital twin . This skill is crucial for predicting the real-world performance of control and estimation algorithms that rely on quantized sensor data.",
            "id": "4231794",
            "problem": "A cyber-physical system is instrumented with an Analog-to-Digital Converter (ADC), and its digital twin must replicate the measurement process to faithfully capture closed-loop behavior. The ADC is uniform and mid-tread, with $N$ bits and a full-scale peak-to-peak input range $V_{\\text{FS}}$. In normal operation, the input is a sinusoid that exercises the converter across all codes without clipping, and dithering is disabled. Assume the following foundational modeling facts:\n- A uniform $N$-bit quantizer partitions the full-scale range into $2^{N}$ disjoint code bins of equal width.\n- For an input that spans the converter nonlinearly but ergodically across all codes without overload, the instantaneous quantization error $e$ can be modeled as a random variable uniformly distributed on the interval $\\left[-\\Delta/2,\\,\\Delta/2\\right]$, where $\\Delta$ is the code bin width.\n- For a sinusoidal input $v(t)=A\\sin(\\omega t)$, the mean-square signal power equals $A^{2}/2$.\n- Signal-to-quantization-noise ratio (SQNR) in decibels is defined by $10\\log_{10}(\\text{signal power}/\\text{quantization noise power})$.\n\nStarting from these base definitions (and no other formulas), derive closed-form expressions for the quantization step $\\Delta$ and for SQNR in decibels for a full-scale sinusoidal input that just fits the ADC range without clipping. Then, evaluate your expressions for an ADC with $N=17$ bits and $V_{\\text{FS}}=2.5$ volts peak-to-peak. Round your numeric values as follows:\n- Round $\\Delta$ to six significant figures and express it in volts.\n- Round SQNR to four significant figures and express it in decibels.\n\nYour final numeric answer must be a single pair of values ordered as $\\left(\\Delta,\\ \\text{SQNR}\\right)$.",
            "solution": "The problem statement has been validated and is deemed valid. It is scientifically grounded, well-posed, and contains all necessary information to derive a unique, meaningful solution. The premises are standard, textbook-level assumptions used in the analysis of analog-to-digital converters.\n\nThe task is to derive expressions for the quantization step $\\Delta$ and the Signal-to-Quantization-Noise Ratio (SQNR) and then evaluate them for a specific ADC.\n\n**1. Derivation of the Quantization Step $\\Delta$**\n\nThe problem states that a uniform $N$-bit quantizer partitions the full-scale range $V_{\\text{FS}}$ into $2^{N}$ disjoint code bins of equal width. The width of a single code bin, also known as the quantization step or least significant bit (LSB), is denoted by $\\Delta$. It is calculated by dividing the total range by the number of bins.\n\n$$\n\\Delta = \\frac{V_{\\text{FS}}}{2^{N}}\n$$\n\nThis is the required closed-form expression for the quantization step.\n\n**2. Derivation of the Signal-to-Quantization-Noise Ratio (SQNR)**\n\nThe SQNR in decibels (dB) is defined as:\n\n$$\n\\text{SQNR} = 10\\log_{10}\\left(\\frac{\\text{signal power}}{\\text{quantization noise power}}\\right) = 10\\log_{10}\\left(\\frac{P_s}{P_q}\\right)\n$$\n\nTo find the expression for SQNR, we must first derive the expressions for the signal power $P_s$ and the quantization noise power $P_q$.\n\n**2a. Quantization Noise Power ($P_q$)**\n\nThe problem specifies that the instantaneous quantization error, $e$, can be modeled as a random variable uniformly distributed on the interval $\\left[-\\Delta/2, \\Delta/2\\right]$. The probability density function (PDF), $p(e)$, for such a distribution is:\n\n$$\np(e) = \n\\begin{cases} \n\\frac{1}{\\Delta}  \\text{for } -\\frac{\\Delta}{2} \\le e \\le \\frac{\\Delta}{2} \\\\\n0  \\text{otherwise}\n\\end{cases}\n$$\n\nThe quantization noise power, $P_q$, is the mean-square value of the error, $E[e^2]$. For a zero-mean distribution like this one, the mean-square value is equal to the variance.\n\n$$\nP_q = E[e^2] = \\int_{-\\infty}^{\\infty} e^2 p(e) \\, de = \\int_{-\\Delta/2}^{\\Delta/2} e^2 \\left(\\frac{1}{\\Delta}\\right) \\, de\n$$\n\nEvaluating the integral:\n\n$$\nP_q = \\frac{1}{\\Delta} \\left[ \\frac{e^3}{3} \\right]_{-\\Delta/2}^{\\Delta/2} = \\frac{1}{3\\Delta} \\left[ \\left(\\frac{\\Delta}{2}\\right)^3 - \\left(-\\frac{\\Delta}{2}\\right)^3 \\right] = \\frac{1}{3\\Delta} \\left[ \\frac{\\Delta^3}{8} - \\left(-\\frac{\\Delta^3}{8}\\right) \\right] = \\frac{1}{3\\Delta} \\left( \\frac{2\\Delta^3}{8} \\right)\n$$\n\n$$\nP_q = \\frac{\\Delta^2}{12}\n$$\n\n**2b. Signal Power ($P_s$)**\n\nThe input signal is a sinusoid $v(t) = A\\sin(\\omega t)$. The problem states this is a \"full-scale sinusoidal input that just fits the ADC range without clipping.\" The ADC has a full-scale peak-to-peak range of $V_{\\text{FS}}$. A sinusoid with amplitude $A$ has a peak-to-peak value of $2A$. For the signal to just fit the range, its peak-to-peak value must equal $V_{\\text{FS}}$.\n\n$$\n2A = V_{\\text{FS}} \\implies A = \\frac{V_{\\text{FS}}}{2}\n$$\n\nThe problem provides the formula for the mean-square signal power of a sinusoid: $P_s = A^2/2$. Substituting the expression for $A$ for a full-scale input:\n\n$$\nP_s = \\frac{1}{2}\\left(\\frac{V_{\\text{FS}}}{2}\\right)^2 = \\frac{V_{\\text{FS}}^2}{8}\n$$\n\n**2c. Assembling the SQNR Expression**\n\nNow we substitute the expressions for $P_s$ and $P_q$ into the SQNR definition:\n\n$$\n\\text{SQNR} = 10\\log_{10}\\left(\\frac{P_s}{P_q}\\right) = 10\\log_{10}\\left(\\frac{V_{\\text{FS}}^2/8}{\\Delta^2/12}\\right) = 10\\log_{10}\\left(\\frac{12}{8} \\frac{V_{\\text{FS}}^2}{\\Delta^2}\\right) = 10\\log_{10}\\left(\\frac{3}{2} \\frac{V_{\\text{FS}}^2}{\\Delta^2}\\right)\n$$\n\nTo obtain a closed-form expression in terms of $N$, we substitute $\\Delta = V_{\\text{FS}}/2^N$:\n\n$$\n\\text{SQNR} = 10\\log_{10}\\left(\\frac{3}{2} \\frac{V_{\\text{FS}}^2}{(V_{\\text{FS}}/2^N)^2}\\right) = 10\\log_{10}\\left(\\frac{3}{2} \\frac{V_{\\text{FS}}^2}{V_{\\text{FS}}^2 / (2^N)^2}\\right) = 10\\log_{10}\\left(\\frac{3}{2} (2^N)^2\\right)\n$$\n\n$$\n\\text{SQNR} = 10\\log_{10}\\left(\\frac{3}{2} \\cdot 2^{2N}\\right)\n$$\n\nThis is the final closed-form expression for the SQNR of an ideal $N$-bit ADC with a full-scale sinusoidal input.\n\n**3. Numerical Evaluation**\n\nThe problem provides the values $N=17$ bits and $V_{\\text{FS}}=2.5$ volts.\n\n**3a. Calculation of $\\Delta$**\n\nUsing the derived formula for $\\Delta$:\n\n$$\n\\Delta = \\frac{V_{\\text{FS}}}{2^N} = \\frac{2.5}{2^{17}} = \\frac{2.5}{131072} \\approx 1.90734863 \\times 10^{-5} \\, \\text{V}\n$$\n\nRounding to six significant figures as required:\n\n$$\n\\Delta \\approx 1.90735 \\times 10^{-5} \\, \\text{V}\n$$\n\n**3b. Calculation of SQNR**\n\nUsing the derived formula for SQNR with $N=17$:\n\n$$\n\\text{SQNR} = 10\\log_{10}\\left(\\frac{3}{2} \\cdot 2^{2 \\times 17}\\right) = 10\\log_{10}\\left(1.5 \\cdot 2^{34}\\right)\n$$\n\nUsing the properties of logarithms, $\\log(ab) = \\log(a) + \\log(b)$ and $\\log(x^y) = y\\log(x)$:\n\n$$\n\\text{SQNR} = 10\\left(\\log_{10}(1.5) + 34\\log_{10}(2)\\right)\n$$\n\nUsing the values $\\log_{10}(1.5) \\approx 0.176091$ and $\\log_{10}(2) \\approx 0.301030$:\n\n$$\n\\text{SQNR} \\approx 10\\left(0.176091 + 34 \\times 0.301030\\right) \\approx 10\\left(0.176091 + 10.23502\\right) = 10(10.411111) \\approx 104.11111 \\, \\text{dB}\n$$\n\nRounding to four significant figures as required:\n\n$$\n\\text{SQNR} \\approx 104.1 \\, \\text{dB}\n$$\n\nThe final numerical answer consists of the pair $(\\Delta, \\text{SQNR})$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 1.90735 \\times 10^{-5}  104.1 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Once sensor data is digitized, a CPS must often infer its internal state from these measurements. This is rarely a direct process, as sensor models are frequently nonlinear. This exercise focuses on a cornerstone technique for handling such systems: linearization . You will derive the measurement Jacobian for a common robotic sensor model and use it to analyze measurement sensitivity, providing direct, practical insight into the mechanics of model-based state estimators like the Extended Kalman Filter.",
            "id": "4231762",
            "problem": "A digital twin of a planar ground robot is used for model-based design of a state estimator in a cyber-physical system. The robot state is $x = [x, y, \\theta]^{\\top}$, where $x$ and $y$ are planar coordinates and $\\theta$ is the heading angle. The robot observes a fixed landmark at $(L_{x}, L_{y})$ through a bearing-range sensor whose measurement function is $h(x) = [b(x), r(x)]^{\\top}$ defined by\n$$\nb(x) = \\arctan2\\!\\big(L_{y} - y,\\, L_{x} - x\\big) - \\theta,\\quad r(x) = \\sqrt{\\big(L_{x} - x\\big)^{2} + \\big(L_{y} - y\\big)^{2}}.\n$$\nAssume $L_{x} = 5$, $L_{y} = -3$. The operating point is $x^{\\ast} = [1, -1, \\pi/4]^{\\top}$. The estimator is designed via first-order Taylor linearization of the measurement function about $x^{\\ast}$, yielding a measurement Jacobian that is used in the Extended Kalman Filter (EKF). \n\nStarting only from definitions of $h(x)$ and the first-order Taylor expansion for vector-valued functions, derive the measurement Jacobian $H(x) = \\frac{\\partial h(x)}{\\partial x}$ symbolically, and then evaluate $H(x^{\\ast})$. Treat $\\arctan2(\\cdot,\\cdot)$ and the square root as smooth away from singularities and ensure that all steps are justified by the chain rule.\n\nUsing your linearization, consider measurement sensitivity for the bearing component $b(x)$ under normalized state perturbations. Let the admissible perturbation set be\n$$\n\\mathcal{S} = \\left\\{\\, x \\in \\mathbb{R}^{3} : \\big\\|W\\big(x - x^{\\ast}\\big)\\big\\|_{\\infty} \\le \\delta \\,\\right\\},\n$$\nwhere $W = \\operatorname{diag}(0.5,\\, 0.5,\\, 1)$ and $\\delta = 3.1 \\times 10^{-2}$. Using only well-tested facts from norm duality and linearization, obtain the worst-case linearized bound on the magnitude of the bearing deviation $\\big|b(x) - b(x^{\\ast})\\big|$ induced by perturbations in $\\mathcal{S}$, expressed as a single real number in radians. Round your answer to four significant figures and express the bound in radians.",
            "solution": "The problem statement has been validated and is found to be scientifically grounded, well-posed, objective, and contains no discernible flaws. It constitutes a standard exercise in the analysis of nonlinear systems for state estimation. We may therefore proceed with a full solution.\n\nThe problem is divided into two parts. First, we derive the measurement Jacobian matrix for a given sensor model. Second, we perform a worst-case sensitivity analysis for the bearing measurement based on a first-order linearization.\n\nPart 1: Derivation and Evaluation of the Measurement Jacobian\n\nThe state vector is $x = [x, y, \\theta]^{\\top}$. The measurement function is $h(x) = [b(x), r(x)]^{\\top}$, where\n$$\nb(x) = \\arctan2(L_{y} - y, L_{x} - x) - \\theta\n$$\n$$\nr(x) = \\sqrt{(L_{x} - x)^{2} + (L_{y} - y)^{2}}\n$$\nThe measurement Jacobian is the $2 \\times 3$ matrix $H(x) = \\frac{\\partial h(x)}{\\partial x}$, defined as:\n$$\nH(x) = \\begin{pmatrix} \\frac{\\partial b}{\\partial x}  \\frac{\\partial b}{\\partial y}  \\frac{\\partial b}{\\partial \\theta} \\\\ \\frac{\\partial r}{\\partial x}  \\frac{\\partial r}{\\partial y}  \\frac{\\partial r}{\\partial \\theta} \\end{pmatrix}\n$$\nWe will compute each partial derivative symbolically. For clarity, let $\\Delta x = L_x - x$ and $\\Delta y = L_y - y$. The square of the range is $r^2(x) = (\\Delta x)^2 + (\\Delta y)^2$.\n\nDerivatives of the bearing component $b(x)$:\nThe derivative of $f(v, u) = \\arctan2(v, u)$ with respect to its arguments are $\\frac{\\partial f}{\\partial u} = \\frac{-v}{u^2+v^2}$ and $\\frac{\\partial f}{\\partial v} = \\frac{u}{u^2+v^2}$.\nUsing the chain rule with $v = \\Delta y$ and $u = \\Delta x$:\n$$\n\\frac{\\partial b}{\\partial x} = \\frac{\\partial}{\\partial x} \\left( \\arctan2(\\Delta y, \\Delta x) - \\theta \\right) = \\frac{\\partial \\arctan2(\\Delta y, \\Delta x)}{\\partial (\\Delta x)} \\frac{\\partial (\\Delta x)}{\\partial x} = \\frac{-\\Delta y}{(\\Delta x)^2 + (\\Delta y)^2} \\cdot (-1) = \\frac{\\Delta y}{(\\Delta x)^2 + (\\Delta y)^2} = \\frac{L_y - y}{(L_x - x)^2 + (L_y - y)^2}\n$$\n$$\n\\frac{\\partial b}{\\partial y} = \\frac{\\partial}{\\partial y} \\left( \\arctan2(\\Delta y, \\Delta x) - \\theta \\right) = \\frac{\\partial \\arctan2(\\Delta y, \\Delta x)}{\\partial (\\Delta y)} \\frac{\\partial (\\Delta y)}{\\partial y} = \\frac{\\Delta x}{(\\Delta x)^2 + (\\Delta y)^2} \\cdot (-1) = \\frac{-\\Delta x}{(\\Delta x)^2 + (\\Delta y)^2} = \\frac{-(L_x - x)}{(L_x - x)^2 + (L_y - y)^2}\n$$\nThe derivative with respect to $\\theta$ is straightforward:\n$$\n\\frac{\\partial b}{\\partial \\theta} = \\frac{\\partial}{\\partial \\theta} \\left( \\arctan2(\\Delta y, \\Delta x) - \\theta \\right) = -1\n$$\n\nDerivatives of the range component $r(x)$:\nThe range is $r(x) = ((\\Delta x)^2 + (\\Delta y)^2)^{1/2}$. Using the chain rule:\n$$\n\\frac{\\partial r}{\\partial x} = \\frac{1}{2} \\left((\\Delta x)^2 + (\\Delta y)^2\\right)^{-1/2} \\cdot \\frac{\\partial}{\\partial x}((\\Delta x)^2) = \\frac{1}{2r(x)} \\cdot 2(\\Delta x) \\frac{\\partial(\\Delta x)}{\\partial x} = \\frac{\\Delta x}{r(x)} \\cdot (-1) = \\frac{-\\Delta x}{r(x)} = \\frac{-(L_x - x)}{\\sqrt{(L_x - x)^2 + (L_y - y)^2}}\n$$\n$$\n\\frac{\\partial r}{\\partial y} = \\frac{1}{2} \\left((\\Delta x)^2 + (\\Delta y)^2\\right)^{-1/2} \\cdot \\frac{\\partial}{\\partial y}((\\Delta y)^2) = \\frac{1}{2r(x)} \\cdot 2(\\Delta y) \\frac{\\partial(\\Delta y)}{\\partial y} = \\frac{\\Delta y}{r(x)} \\cdot (-1) = \\frac{-\\Delta y}{r(x)} = \\frac{-(L_y - y)}{\\sqrt{(L_x - x)^2 + (L_y - y)^2}}\n$$\nThe range $r(x)$ does not depend on $\\theta$:\n$$\n\\frac{\\partial r}{\\partial \\theta} = 0\n$$\n\nAssembling the symbolic Jacobian $H(x)$:\n$$\nH(x) = \\begin{pmatrix}\n\\frac{L_y - y}{(L_x - x)^2 + (L_y - y)^2}  \\frac{-(L_x - x)}{(L_x - x)^2 + (L_y - y)^2}  -1 \\\\\n\\frac{-(L_x - x)}{\\sqrt{(L_x - x)^2 + (L_y - y)^2}}  \\frac{-(L_y - y)}{\\sqrt{(L_x - x)^2 + (L_y - y)^2}}  0\n\\end{pmatrix}\n$$\n\nNow, we evaluate $H(x)$ at the operating point $x^{\\ast} = [1, -1, \\pi/4]^{\\top}$ with landmark $(L_x, L_y) = (5, -3)$.\nFirst, calculate the differences and the range squared:\n$$\n\\Delta x^{\\ast} = L_x - x^{\\ast} = 5 - 1 = 4\n$$\n$$\n\\Delta y^{\\ast} = L_y - y^{\\ast} = -3 - (-1) = -2\n$$\n$$\n(r(x^{\\ast}))^2 = (\\Delta x^{\\ast})^2 + (\\Delta y^{\\ast})^2 = 4^2 + (-2)^2 = 16 + 4 = 20\n$$\n$$\nr(x^{\\ast}) = \\sqrt{20} = 2\\sqrt{5}\n$$\nSubstitute these values into the symbolic Jacobian entries:\n$$\n\\frac{\\partial b}{\\partial x}\\bigg|_{x^{\\ast}} = \\frac{\\Delta y^{\\ast}}{(r(x^{\\ast}))^2} = \\frac{-2}{20} = -\\frac{1}{10}\n$$\n$$\n\\frac{\\partial b}{\\partial y}\\bigg|_{x^{\\ast}} = \\frac{-\\Delta x^{\\ast}}{(r(x^{\\ast}))^2} = \\frac{-4}{20} = -\\frac{1}{5}\n$$\n$$\n\\frac{\\partial b}{\\partial \\theta}\\bigg|_{x^{\\ast}} = -1\n$$\n$$\n\\frac{\\partial r}{\\partial x}\\bigg|_{x^{\\ast}} = \\frac{-\\Delta x^{\\ast}}{r(x^{\\ast})} = \\frac{-4}{2\\sqrt{5}} = -\\frac{2}{\\sqrt{5}} = -\\frac{2\\sqrt{5}}{5}\n$$\n$$\n\\frac{\\partial r}{\\partial y}\\bigg|_{x^{\\ast}} = \\frac{-\\Delta y^{\\ast}}{r(x^{\\ast})} = \\frac{-(-2)}{2\\sqrt{5}} = \\frac{1}{\\sqrt{5}} = \\frac{\\sqrt{5}}{5}\n$$\n$$\n\\frac{\\partial r}{\\partial \\theta}\\bigg|_{x^{\\ast}} = 0\n$$\nThe evaluated Jacobian is:\n$$\nH(x^{\\ast}) = \\begin{pmatrix} -1/10  -1/5  -1 \\\\ -2\\sqrt{5}/5  \\sqrt{5}/5  0 \\end{pmatrix}\n$$\n\nPart 2: Measurement Sensitivity Analysis\n\nWe are asked to find the worst-case linearized bound on the magnitude of the bearing deviation, $|b(x) - b(x^{\\ast})|$, for state perturbations $x$ within the set $\\mathcal{S}$.\nThe first-order Taylor expansion of $b(x)$ about $x^{\\ast}$ is given by:\n$$\nb(x) \\approx b(x^{\\ast}) + \\nabla b(x^{\\ast})^{\\top} (x - x^{\\ast})\n$$\nwhere $\\nabla b(x^{\\ast})$ is the gradient of $b$ evaluated at $x^{\\ast}$. This gradient corresponds to the first row of the Jacobian $H(x^{\\ast})$ transposed:\n$$\n\\nabla b(x^{\\ast}) = \\begin{pmatrix} -1/10 \\\\ -1/5 \\\\ -1 \\end{pmatrix}\n$$\nThe linearized deviation is $\\Delta b = b(x) - b(x^{\\ast}) \\approx \\nabla b(x^{\\ast})^{\\top} (x - x^{\\ast})$. We want to find the maximum of $|\\Delta b|$ for $x \\in \\mathcal{S}$. Let the perturbation vector be $p = x - x^{\\ast}$. The problem is to find:\n$$\n\\max_{p} \\left| \\nabla b(x^{\\ast})^{\\top} p \\right| \\quad \\text{subject to} \\quad \\|W p\\|_{\\infty} \\le \\delta\n$$\nThis is equivalent to finding $\\max_{p} \\nabla b(x^{\\ast})^{\\top} p$ subject to the same constraint, as the set of admissible $p$ is symmetric around the origin.\nLet $c = \\nabla b(x^{\\ast})$. We want to maximize $c^{\\top}p$. Let $z = W p$, which implies $p = W^{-1} z$ since $W$ is invertible. The problem is transformed to:\n$$\n\\max_{z} c^{\\top} (W^{-1} z) \\quad \\text{subject to} \\quad \\|z\\|_{\\infty} \\le \\delta\n$$\nThis can be rewritten as $\\max_{z} ( (W^{-1})^{\\top} c )^{\\top} z$. Since $W$ is a diagonal matrix, $W^{-1}$ is also diagonal, thus $(W^{-1})^{\\top} = W^{-1}$. The problem is:\n$$\n\\max_{\\|z\\|_{\\infty} \\le \\delta} (W^{-1} c)^{\\top} z\n$$\nThe maximum value of an inner product $\\langle a, z \\rangle$ over the set $\\|z\\|_{\\infty} \\le \\delta$ is given by $\\delta \\|a\\|_1$, a fundamental result from norm duality (the dual norm of the $L_{\\infty}$-norm is the $L_1$-norm). Here, our vector $a$ is $W^{-1} c$.\nThe maximum bound is therefore $\\delta \\| W^{-1} c \\|_1$.\n\nLet's compute the components. We have:\n$$\nc = \\nabla b(x^{\\ast}) = \\begin{pmatrix} -0.1 \\\\ -0.2 \\\\ -1 \\end{pmatrix}\n$$\nThe weighting matrix is $W = \\operatorname{diag}(0.5, 0.5, 1)$, so its inverse is $W^{-1} = \\operatorname{diag}(2, 2, 1)$.\n$$\nW^{-1} c = \\begin{pmatrix} 2  0  0 \\\\ 0  2  0 \\\\ 0  0  1 \\end{pmatrix} \\begin{pmatrix} -0.1 \\\\ -0.2 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 2 \\times (-0.1) \\\\ 2 \\times (-0.2) \\\\ 1 \\times (-1) \\end{pmatrix} = \\begin{pmatrix} -0.2 \\\\ -0.4 \\\\ -1 \\end{pmatrix}\n$$\nNow, we compute the $L_1$-norm of this vector:\n$$\n\\| W^{-1} c \\|_1 = |-0.2| + |-0.4| + |-1| = 0.2 + 0.4 + 1.0 = 1.6\n$$\nThe perturbation bound is given as $\\delta = 3.1 \\times 10^{-2} = 0.031$.\nThe worst-case linearized bound on the bearing deviation is:\n$$\n\\max |b(x) - b(x^{\\ast})| \\approx \\delta \\| W^{-1} c \\|_1 = 0.031 \\times 1.6 = 0.0496\n$$\nThe problem asks for this value to be rounded to four significant figures. The calculated value $0.0496$ has three significant figures (the first non-zero digit is $4$, followed by $9$ and $6$). To represent this value with four significant figures, we append a zero, yielding $0.04960$. The units are in radians.",
            "answer": "$$\n\\boxed{0.04960}\n$$"
        },
        {
            "introduction": "With an accurate state estimate, the \"cyber\" part of a CPS can compute an optimal action for the \"physical\" part. Model Predictive Control (MPC) is a powerful paradigm for this task, systematically using a system model to optimize future behavior while respecting operational constraints. In this practice, you will solve a single-step MPC problem from first principles, determining the optimal control input for a linear system subject to state and actuator limits . This exercise illuminates the core optimization that lies at the heart of modern, model-based control design.",
            "id": "4231801",
            "problem": "Consider a discrete-time linear time-invariant state-space model used in Model Predictive Control (MPC) for a Cyber-Physical System (CPS) and its corresponding digital twin, given by\n$$\nx_{k+1} = A x_k + B u_k,\n$$\nwhere $x_k \\in \\mathbb{R}^2$ is the state at step $k$, $u_k \\in \\mathbb{R}$ is the control input at step $k$, $A \\in \\mathbb{R}^{2 \\times 2}$, and $B \\in \\mathbb{R}^{2 \\times 1}$. Assume dimensionless, normalized variables throughout.\n\nAt a particular time step, the digital twin provides the model matrices and cost weights\n$$\nA = \\begin{pmatrix} 1  \\frac{1}{4} \\\\ 0  1 \\end{pmatrix}, \\quad B = \\begin{pmatrix} \\frac{1}{2} \\\\ 1 \\end{pmatrix}, \\quad Q = \\begin{pmatrix} 9  0 \\\\ 0  1 \\end{pmatrix}, \\quad R = 1,\n$$\nand the current measured state\n$$\nx_k = \\begin{pmatrix} 0 \\\\ \\frac{1}{5} \\end{pmatrix}.\n$$\nFor a single-step horizon ($1$-step MPC), the stage cost is\n$$\nJ(u_k) = \\big(A x_k + B u_k\\big)^{\\top} Q \\big(A x_k + B u_k\\big) + u_k^{\\top} R u_k.\n$$\nThe actuator and state constraints are given by the input bound and a safety requirement on the predicted first state component:\n$$\n-1 \\leq u_k \\leq 1, \\quad -\\frac{1}{2} \\leq \\big(x_{k+1}\\big)_1 \\leq \\frac{1}{2},\n$$\nwhere $\\big(x_{k+1}\\big)_1$ denotes the first component of $x_{k+1}$.\n\nTasks:\n- Derive from first principles the unconstrained minimizer of $J(u_k)$ using the given model and cost.\n- Determine whether the unconstrained minimizer is feasible under the stated constraints. If it is infeasible, compute the constrained optimal $u_k$ by projecting onto the feasible set defined by the constraints.\n- Briefly discuss feasibility characterization for this single-step MPC problem and propose a warm-start strategy suitable for an iterative Quadratic Programming (QP) solver used in MPC implementation for CPS.\n\nProvide the single optimal control action $u_k^{\\star}$ as your final answer. Express your final numerical answer as an exact rational number (no rounding).",
            "solution": "The problem is first validated to ensure it is scientifically sound, well-posed, and complete.\n\n### Step 1: Extract Givens\n- State-space model: $x_{k+1} = A x_k + B u_k$\n- State vector: $x_k \\in \\mathbb{R}^2$\n- Control input: $u_k \\in \\mathbb{R}$\n- System matrix: $A = \\begin{pmatrix} 1  \\frac{1}{4} \\\\ 0  1 \\end{pmatrix}$\n- Input matrix: $B = \\begin{pmatrix} \\frac{1}{2} \\\\ 1 \\end{pmatrix}$\n- State cost weight matrix: $Q = \\begin{pmatrix} 9  0 \\\\ 0  1 \\end{pmatrix}$\n- Input cost weight: $R = 1$\n- Current state: $x_k = \\begin{pmatrix} 0 \\\\ \\frac{1}{5} \\end{pmatrix}$\n- Cost function for single-step MPC: $J(u_k) = \\big(A x_k + B u_k\\big)^{\\top} Q \\big(A x_k + B u_k\\big) + u_k^{\\top} R u_k$\n- Input constraint: $-1 \\leq u_k \\leq 1$\n- State constraint: $-\\frac{1}{2} \\leq \\big(x_{k+1}\\big)_1 \\leq \\frac{1}{2}$\n\n### Step 2: Validate Using Extracted Givens\nThe problem describes a standard single-step Model Predictive Control (MPC) formulation, which is a well-established method in control engineering.\n- **Scientific Grounding**: The use of a linear time-invariant state-space model, a quadratic cost function, and linear constraints is fundamental to modern control theory. The problem is scientifically and mathematically sound.\n- **Well-Posedness**: The cost function $J(u_k)$ needs to be examined. It can be expanded as a quadratic function of the scalar control input $u_k$.\n$J(u_k) = (x_k^\\top A^\\top + u_k B^\\top) Q (A x_k + B u_k) + u_k R u_k$\n$J(u_k) = x_k^\\top A^\\top Q A x_k + 2 u_k B^\\top Q A x_k + u_k^2 B^\\top Q B + u_k^2 R$\n$J(u_k) = (B^\\top Q B + R) u_k^2 + 2(B^\\top Q A x_k) u_k + (x_k^\\top A^\\top Q A x_k)$\nThis is a quadratic function of $u_k$. For this function to have a unique minimizer, the coefficient of the $u_k^2$ term must be positive.\n$B^\\top Q B + R = \\begin{pmatrix} \\frac{1}{2}  1 \\end{pmatrix} \\begin{pmatrix} 9  0 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{2} \\\\ 1 \\end{pmatrix} + 1 = \\begin{pmatrix} \\frac{9}{2}  1 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{2} \\\\ 1 \\end{pmatrix} + 1 = (\\frac{9}{4} + 1) + 1 = \\frac{13}{4} + 1 = \\frac{17}{4}$.\nSince $\\frac{17}{4}  0$, the cost function $J(u_k)$ is a strictly convex parabola, which guarantees the existence of a unique unconstrained minimizer. The constraints define a closed and convex set (an interval). Minimizing a strictly convex function over a convex set is a well-posed problem.\n- **Completeness and Consistency**: All necessary matrices, the initial state, the cost function, and constraints are explicitly provided and are dimensionally consistent.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n### Solution Derivation\n\n**Task 1: Unconstrained Minimizer**\n\nTo find the unconstrained minimizer of the convex quadratic cost function $J(u_k)$, we take its derivative with respect to $u_k$ and set it to zero.\n$$\n\\frac{dJ}{du_k} = \\frac{d}{du_k} \\left[ (B^\\top Q B + R) u_k^2 + 2(B^\\top Q A x_k) u_k + \\text{const} \\right] = 0\n$$\n$$\n2 (B^\\top Q B + R) u_k + 2(B^\\top Q A x_k) = 0\n$$\nSolving for the unconstrained optimal control input, $u_k^{\\text{unc}}$:\n$$\nu_k^{\\text{unc}} = - (B^\\top Q B + R)^{-1} (B^\\top Q A x_k)\n$$\nWe have already calculated $B^\\top Q B + R = \\frac{17}{4}$. Now we compute the term $B^\\top Q A x_k$.\nFirst, calculate the product $A x_k$:\n$$\nA x_k = \\begin{pmatrix} 1  \\frac{1}{4} \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ \\frac{1}{5} \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot 0 + \\frac{1}{4} \\cdot \\frac{1}{5} \\\\ 0 \\cdot 0 + 1 \\cdot \\frac{1}{5} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{20} \\\\ \\frac{1}{5} \\end{pmatrix}\n$$\nNext, calculate $B^\\top Q A x_k$:\n$$\nB^\\top Q = \\begin{pmatrix} \\frac{1}{2}  1 \\end{pmatrix} \\begin{pmatrix} 9  0 \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} \\frac{9}{2}  1 \\end{pmatrix}\n$$\n$$\nB^\\top Q A x_k = \\begin{pmatrix} \\frac{9}{2}  1 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{20} \\\\ \\frac{1}{5} \\end{pmatrix} = \\frac{9}{2} \\cdot \\frac{1}{20} + 1 \\cdot \\frac{1}{5} = \\frac{9}{40} + \\frac{8}{40} = \\frac{17}{40}\n$$\nNow, substitute these values back into the expression for $u_k^{\\text{unc}}$:\n$$\nu_k^{\\text{unc}} = - \\left( \\frac{17}{4} \\right)^{-1} \\left( \\frac{17}{40} \\right) = - \\frac{4}{17} \\cdot \\frac{17}{40} = - \\frac{4}{40} = -\\frac{1}{10}\n$$\nThe unconstrained minimizer is $u_k^{\\text{unc}} = -\\frac{1}{10}$.\n\n**Task 2: Feasibility and Constrained Optimum**\n\nWe must now determine if $u_k^{\\text{unc}} = -\\frac{1}{10}$ is feasible by checking it against the given constraints.\n\nFirst Constraint (Input bound):\n$$\n-1 \\leq u_k \\leq 1\n$$\nSubstituting $u_k^{\\text{unc}} = -\\frac{1}{10}$:\n$$\n-1 \\leq -\\frac{1}{10} \\leq 1\n$$\nThis inequality is true.\n\nSecond Constraint (State safety requirement):\n$$\n-\\frac{1}{2} \\leq \\big(x_{k+1}\\big)_1 \\leq \\frac{1}{2}\n$$\nThe predicted state is $x_{k+1} = A x_k + B u_k$. Its first component, $\\big(x_{k+1}\\big)_1$, is given by:\n$$\n\\big(x_{k+1}\\big)_1 = \\big(A x_k\\big)_1 + \\big(B u_k\\big)_1 = \\frac{1}{20} + \\frac{1}{2} u_k\n$$\nSubstituting $u_k^{\\text{unc}} = -\\frac{1}{10}$:\n$$\n\\big(x_{k+1}\\big)_1 = \\frac{1}{20} + \\frac{1}{2} \\left( -\\frac{1}{10} \\right) = \\frac{1}{20} - \\frac{1}{20} = 0\n$$\nChecking the constraint for this predicted state component:\n$$\n-\\frac{1}{2} \\leq 0 \\leq \\frac{1}{2}\n$$\nThis inequality is also true.\n\nSince the unconstrained minimizer satisfies all constraints, it lies within the feasible set. For a convex optimization problem, if the unconstrained optimum is feasible, it is also the constrained optimum.\nTherefore, the optimal control action is $u_k^{\\star} = u_k^{\\text{unc}} = -\\frac{1}{10}$.\n\nIf the unconstrained minimizer were infeasible, the optimal solution would be the projection of $u_k^{\\text{unc}}$ onto the feasible set for $u_k$. The feasible set for $u_k$ is the intersection of the two constraint intervals.\nFrom the first constraint, $u_k \\in [-1, 1]$.\nFrom the second constraint, $-\\frac{1}{2} \\leq \\frac{1}{20} + \\frac{1}{2} u_k \\leq \\frac{1}{2}$, which simplifies to:\n$-\\frac{11}{20} \\leq \\frac{1}{2} u_k \\leq \\frac{9}{20}$, or $-\\frac{11}{10} \\leq u_k \\leq \\frac{9}{10}$.\nThe feasible set is the intersection: $[-1, 1] \\cap [-\\frac{11}{10}, \\frac{9}{10}] = [-1, \\frac{9}{10}]$.\nSince our solution $u_k^{\\star} = -\\frac{1}{10}$ is in this interval, our conclusion is correct.\n\n**Task 3: Feasibility and Warm-Start Strategy**\n\n**Feasibility Characterization**: For this problem, a feasible control action $u_k$ exists if and only if the feasible set, defined by the intersection of all constraints on $u_k$, is non-empty. As derived above, the feasible set is the interval $[-1, \\frac{9}{10}]$. Since the interval is non-empty, feasible solutions exist. In general, for a given state $x_k$, feasibility requires the intersection of the input-domain constraints and the state-domain constraints (mapped back to the input space) to be non-empty. This can be checked analytically before solving the optimization problem, which is a key advantage of this simple MPC setup.\n\n**Warm-Start Strategy**: MPC involves solving a similar optimization problem at each time step. Iterative solvers, such as those used for Quadratic Programming (QP), can be significantly accelerated by providing a good initial guess, known as a warm start. A highly effective and common strategy is the temporal warm-start. For the optimization problem at time step $k$, the initial guess for the decision variable $u_k$ is set to the optimal solution found at the previous time step, $u_{k-1}^{\\star}$. This strategy leverages the fact that for many physical systems under MPC, the optimal control sequence does not change drastically between consecutive time steps. Thus, $u_{k-1}^{\\star}$ is often a very good approximation of $u_k^{\\star}$, reducing the number of iterations required by the QP solver to find the new optimum.",
            "answer": "$$\n\\boxed{-\\frac{1}{10}}\n$$"
        }
    ]
}