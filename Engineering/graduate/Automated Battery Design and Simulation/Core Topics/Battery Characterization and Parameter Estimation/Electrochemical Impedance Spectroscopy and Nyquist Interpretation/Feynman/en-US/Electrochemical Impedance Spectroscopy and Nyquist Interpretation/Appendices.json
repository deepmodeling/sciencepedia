{
    "hands_on_practices": [
        {
            "introduction": "To effectively analyze a battery's performance, we must understand how its internal resistances change with temperature. The high-frequency intercept ($R_s$) of a Nyquist plot provides a direct measure of the total ohmic resistance, which is dominated by the electrolyte. This practice  guides you through building and validating a predictive model that connects the temperature-dependent conductivity of the electrolyte to the observed changes in $R_s$, offering a fundamental skill in relating material properties to electrochemical performance.",
            "id": "3908796",
            "problem": "You are building an automated module to predict the high-frequency intercept in Electrochemical Impedance Spectroscopy (EIS) for a lithium-ion battery cell as a function of temperature. The high-frequency intercept, denoted by $R_s$, reflects the total ohmic resistance dominated by the electrolyte and electronic contacts. Starting from first principles of electrical conduction and a well-accepted interpretation of the EIS high-frequency intercept, derive and implement an algorithm to compute the expected change in $R_s$ across a specified temperature interval using measured electrolyte conductivity data, and validate the model against provided intercept measurements using a linear fit methodology.\n\nFundamental base:\n- Electrical conductivity $\\kappa(T)$ is the reciprocal of resistivity $\\rho(T)$, i.e., $\\rho(T) = 1/\\kappa(T)$.\n- Under uniform conduction, Ohm’s law in differential form relates current density $J$ and electric field $E$ by $J = \\kappa E$, and the resistance of a slab of thickness $L$ and cross-sectional area $A$ is obtained from $R = V/I$ with $V = E L$ and $I = J A$, yielding $R_{\\text{electrolyte}}(T) = \\dfrac{L}{\\kappa(T) A}$.\n- The EIS high-frequency intercept is modeled as $R_s(T) = \\dfrac{L}{\\kappa(T) A} + R_0$, where $R_0$ is a temperature-independent contact/electronic resistance over the narrow range considered.\n\nTask:\n1. Given an array of temperatures $\\{T_i\\}$ in kelvin, measured electrolyte conductivities $\\{\\kappa_i\\}$ in siemens per meter, geometry parameters $L$ in meters and $A$ in square meters, and a constant $R_0$ in ohms, perform a linear least-squares fit of the reciprocal conductivity versus reciprocal temperature:\n   $$y_i = \\frac{1}{\\kappa_i}, \\quad x_i = \\frac{1}{T_i}, \\quad y \\approx \\alpha x + \\beta,$$\n   where $\\alpha$ and $\\beta$ are fit parameters with units chosen to make the relation dimensionally consistent.\n2. Using the fitted relation, predict conductivity at each $T_i$ via\n   $$\\kappa_{\\text{pred}}(T_i) = \\frac{1}{\\alpha \\cdot \\frac{1}{T_i} + \\beta}.$$\n3. Compute the predicted high-frequency intercept at each temperature using\n   $$R_{s,\\text{pred}}(T_i) = \\frac{L}{\\kappa_{\\text{pred}}(T_i) \\, A} + R_0.$$\n4. Compute the expected change in $R_s$ over the interval spanned by the test temperatures as\n   $$\\Delta R_s = R_{s,\\text{pred}}(T_{\\max}) - R_{s,\\text{pred}}(T_{\\min}),$$\n   expressed in ohms.\n5. Validate the prediction against the provided measured intercepts $\\{R_{s,\\text{meas}}(T_i)\\}$ by computing the root-mean-square error (RMSE) in ohms:\n   $$\\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} \\left( R_{s,\\text{pred}}(T_i) - R_{s,\\text{meas}}(T_i) \\right)^2 }.$$\n\nPhysical units:\n- Temperatures $T$ must be in kelvin.\n- Conductivities $\\kappa$ must be in siemens per meter.\n- Thickness $L$ must be in meters.\n- Area $A$ must be in square meters.\n- Resistances $R_s$ and $R_0$ must be in ohms.\n\nAngle units are not applicable. Percentages are not applicable.\n\nTest suite:\nImplement the algorithm for the following parameter sets. All outputs must be expressed in ohms.\n\n- Case 1 (general case): $L = 25 \\times 10^{-6}$ m, $A = 1 \\times 10^{-2}$ m$^2$, $R_0 = 0.005$ $\\Omega$, temperatures $[293, 298, 303, 308, 313]$ K, measured conductivities $\\kappa$ $[0.9881, 0.9867, 1.0151, 1.0131, 1.0162]$ S/m, measured $R_s$ $[0.007529, 0.007557, 0.0074388, 0.0074936, 0.0074378]$ $\\Omega$.\n- Case 2 (boundary: minimal points for linear fit): $L = 20 \\times 10^{-6}$ m, $A = 5 \\times 10^{-3}$ m$^2$, $R_0 = 0.002$ $\\Omega$, temperatures $[295, 310]$ K, measured conductivities $\\kappa$ $[0.8825, 0.8869]$ S/m, measured $R_s$ $[0.006577, 0.006478]$ $\\Omega$.\n- Case 3 (edge: very small intercept due to high conductivity and large area): $L = 10 \\times 10^{-6}$ m, $A = 5 \\times 10^{-2}$ m$^2$, $R_0 = 0.0$ $\\Omega$, temperatures $[298, 299, 300]$ K, measured conductivities $\\kappa$ $[1.1516, 1.1559, 1.1515]$ S/m, measured $R_s$ $[0.0001737, 0.0001725, 0.0001743]$ $\\Omega$.\n\nFinal output format:\nYour program should produce a single line of output containing six comma-separated floats enclosed in square brackets representing the flattened list of per-case metrics in the order\n$$[\\Delta R_{s}^{(1)}, \\text{RMSE}^{(1)}, \\Delta R_{s}^{(2)}, \\text{RMSE}^{(2)}, \\Delta R_{s}^{(3)}, \\text{RMSE}^{(3)}],$$\nwhere superscripts indicate the case index. All values must be in ohms. No additional text should be printed.",
            "solution": "The problem requires the development and validation of a model to predict the high-frequency ohmic resistance, $R_s$, of a lithium-ion battery cell as a function of temperature, $T$. The model is based on fundamental principles of electrical conduction and a standard interpretation of Electrochemical Impedance Spectroscopy (EIS) data. The solution involves implementing a multi-step algorithm that includes data fitting, prediction, and error analysis.\n\nThe physical model for the high-frequency intercept is given by:\n$$R_s(T) = R_{\\text{electrolyte}}(T) + R_0$$\nwhere $R_0$ is a temperature-independent contact and electronic resistance, and $R_{\\text{electrolyte}}(T)$ is the temperature-dependent resistance of the electrolyte. For a slab of electrolyte with thickness $L$ and cross-sectional area $A$, the resistance is related to the electrolyte's electrical conductivity, $\\kappa(T)$, by:\n$$R_{\\text{electrolyte}}(T) = \\frac{L}{\\kappa(T) A}$$\nSubstituting this into the model for $R_s(T)$ yields:\n$$R_s(T) = \\frac{L}{\\kappa(T) A} + R_0$$\nThe task is to use measured conductivity data, $\\{\\kappa_i\\}$, at a series of temperatures, $\\{T_i\\}$, to create a predictive model for $\\kappa(T)$ and subsequently for $R_s(T)$.\n\nThe algorithmic process is executed as follows:\n\nStep 1: Empirical Modeling of Electrolyte Resistivity\nThe problem posits an empirical linear relationship between the reciprocal of conductivity (resistivity, $\\rho$) and the reciprocal of absolute temperature. This is a common approach for modeling thermally activated processes over a narrow temperature range. We define the variables for the linear fit as $y_i = 1/\\kappa_i$ and $x_i = 1/T_i$. The model to be fitted is:\n$$y \\approx \\alpha x + \\beta$$\nwhere $\\alpha$ and $\\beta$ are parameters to be determined. The units of $\\alpha$ are $\\Omega \\cdot m \\cdot K$ and the units of $\\beta$ are $\\Omega \\cdot m$ to ensure dimensional consistency. We are given $n$ data points $(x_i, y_i)$ corresponding to the measured temperatures and conductivities. The parameters $\\alpha$ and $\\beta$ are found by minimizing the sum of the squared residuals, $S = \\sum_{i=1}^{n} (y_i - (\\alpha x_i + \\beta))^2$. The optimal values are obtained by solving the normal equations, which gives a unique solution for $\\alpha$ and $\\beta$ provided there is variance in the $x_i$ values:\n$$ \\alpha = \\frac{n \\sum_{i=1}^{n} x_i y_i - (\\sum_{i=1}^{n} x_i)(\\sum_{i=1}^{n} y_i)}{n \\sum_{i=1}^{n} x_i^2 - (\\sum_{i=1}^{n} x_i)^2} $$\n$$ \\beta = \\bar{y} - \\alpha \\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n} y_i - \\alpha \\left( \\frac{1}{n}\\sum_{i=1}^{n} x_i \\right) $$\n\nStep 2: Prediction of Conductivity and High-Frequency Intercept\nOnce the parameters $\\alpha$ and $\\beta$ are determined from the fit, the model can be used to predict the electrolyte resistivity, $\\rho_{\\text{pred}}$, and conductivity, $\\kappa_{\\text{pred}}$, at any given temperature $T$ within the fitted range:\n$$ \\rho_{\\text{pred}}(T) = \\alpha \\cdot \\frac{1}{T} + \\beta $$\n$$ \\kappa_{\\text{pred}}(T) = \\frac{1}{\\rho_{\\text{pred}}(T)} = \\frac{1}{\\alpha \\cdot \\frac{1}{T} + \\beta} $$\nUsing this predicted conductivity, the high-frequency intercept, $R_{s,\\text{pred}}$, is calculated for each temperature $T_i$ in the original dataset:\n$$R_{s,\\text{pred}}(T_i) = \\frac{L}{\\kappa_{\\text{pred}}(T_i) A} + R_0 = \\frac{L}{A} \\rho_{\\text{pred}}(T_i) + R_0$$\n\nStep 3: Calculation of Performance Metrics\nThe performance of the model is evaluated using two metrics. First, the total change in the predicted resistance, $\\Delta R_s$, across the temperature range of the measurements is calculated. This is defined as the difference between the predicted resistance at the maximum temperature, $T_{\\max}$, and the minimum temperature, $T_{\\min}$, from the input set:\n$$\\Delta R_s = R_{s,\\text{pred}}(T_{\\max}) - R_{s,\\text{pred}}(T_{\\min})$$\nA negative value for $\\Delta R_s$ indicates that the resistance decreases as temperature increases, which is typical for liquid electrolytes.\n\nSecond, the model's accuracy is quantified by computing the Root-Mean-Square Error (RMSE) between the predicted intercepts, $\\{R_{s,\\text{pred}}(T_i)\\}$, and the experimentally measured intercepts, $\\{R_{s,\\text{meas}}(T_i)\\}$. The RMSE is given by:\n$$\\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} \\left( R_{s,\\text{pred}}(T_i) - R_{s,\\text{meas}}(T_i) \\right)^2 }$$\nThis metric provides a measure of the average deviation of the model's predictions from the measured values, in units of ohms.\n\nThe algorithm is implemented for each test case provided, yielding the two specified metrics, $\\Delta R_s$ and RMSE, for each case. These results are then compiled into a single output array as required.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the battery EIS modeling problem for all specified test cases.\n    \"\"\"\n    # Test cases defined as per the problem statement.\n    # Each case is a tuple: (L, A, R0, T_array, kappa_array, Rs_meas_array)\n    test_cases = [\n        # Case 1\n        (25e-6, 1e-2, 0.005,\n         np.array([293, 298, 303, 308, 313]),\n         np.array([0.9881, 0.9867, 1.0151, 1.0131, 1.0162]),\n         np.array([0.007529, 0.007557, 0.0074388, 0.0074936, 0.0074378])),\n        # Case 2\n        (20e-6, 5e-3, 0.002,\n         np.array([295, 310]),\n         np.array([0.8825, 0.8869]),\n         np.array([0.006577, 0.006478])),\n        # Case 3\n        (10e-6, 5e-2, 0.0,\n         np.array([298, 299, 300]),\n         np.array([1.1516, 1.1559, 1.1515]),\n         np.array([0.0001737, 0.0001725, 0.0001743]))\n    ]\n\n    results = []\n    for case in test_cases:\n        L, A, R0, T, kappa_meas, Rs_meas = case\n        n = len(T)\n\n        # Step 1: Perform a linear least-squares fit\n        # y = 1/kappa, x = 1/T\n        x = 1.0 / T\n        y = 1.0 / kappa_meas\n        \n        # Fit y = alpha * x + beta using numpy.polyfit\n        # alpha is the first coefficient, beta is the second\n        alpha, beta = np.polyfit(x, y, deg=1)\n\n        # Step 2: Predict conductivity at each temperature\n        # rho_pred = alpha * (1/T) + beta\n        rho_pred = alpha * x + beta\n        kappa_pred = 1.0 / rho_pred\n\n        # Step 3: Compute the predicted high-frequency intercept\n        Rs_pred = (L / A) * rho_pred + R0\n\n        # Step 4: Compute the expected change in Rs\n        # T_min and T_max are the first and last elements since T is sorted.\n        # R_s_pred is also sorted according to T.\n        delta_Rs = Rs_pred[-1] - Rs_pred[0]\n\n        # Step 5: Validate the prediction by computing the RMSE\n        rmse = np.sqrt(np.mean((Rs_pred - Rs_meas)**2))\n        \n        results.extend([delta_Rs, rmse])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The interpretation of electrochemical impedance spectra relies on the assumption that the data was collected from a linear, time-invariant, and stable system. Before fitting any equivalent circuit model, it is crucial to validate the quality and self-consistency of the measured data. This practice  introduces the Kramers-Kronig (K-K) relations, a powerful mathematical tool rooted in causality principles, allowing you to programmatically test whether an impedance dataset meets these fundamental requirements for valid analysis.",
            "id": "3908839",
            "problem": "Design and implement a Kramers–Kronig (K-K) compliance test for electrochemical impedance spectroscopy (EIS) data in the context of automated battery design and simulation. The EIS impedance is a complex function $Z(\\omega) = Z'(\\omega) + jZ''(\\omega)$ of angular frequency $\\omega$, where $Z'(\\omega)$ and $Z''(\\omega)$ denote the real and imaginary parts, respectively. The fundamental base you must start from is: a linear time-invariant, causal, and stable electrochemical system has a response function whose Fourier transform is analytic in the upper half of the complex-frequency plane, and therefore its real and imaginary parts are connected by Hilbert-transform-type integral relations (the Kramers–Kronig relations). From these principles, derive integral constraints that express $Z'(\\omega)$ in terms of $Z''(\\omega)$, and $Z''(\\omega)$ in terms of $Z'(\\omega)$, as principal value integrals over positive frequencies. Using these constraints, design a numerical test that reconstructs one part from the other and quantifies the residuals.\n\nYour program must do the following, using only the information in this statement:\n\n- Derive, from analyticity and causality, principal value integral expressions connecting $Z'(\\omega)$ and $Z''(\\omega)$. Discretize the integrals on a finite, strictly increasing, positive angular frequency grid $\\{\\omega_i\\}_{i=1}^N$ using a consistent quadrature rule. Implement the Cauchy principal value by excluding the singular contribution at $\\omega = \\omega_k$ when reconstructing at $\\omega_k$. Use trapezoidal-type weights $w_i$ in the $\\omega$-domain defined by one-sided differences at the endpoints and centered differences in the interior; that is, $w_1 = \\omega_2 - \\omega_1$, $w_N = \\omega_N - \\omega_{N-1}$, and for interior indices $i \\in \\{2,\\dots,N-1\\}$, $w_i = (\\omega_{i+1} - \\omega_{i-1})/2$.\n- Since the integrals require knowledge of asymptotic behavior, estimate the high-frequency limit $Z(\\infty)$ by $Z'(\\omega)$ evaluated at the highest measured frequency, i.e., $Z'(\\omega_N)$, and use it consistently in both reconstructions.\n- Compute the residuals $r'(\\omega_k) = Z'_{\\mathrm{meas}}(\\omega_k) - Z'_{\\mathrm{rec}}(\\omega_k)$ and $r''(\\omega_k) = Z''_{\\mathrm{meas}}(\\omega_k) - Z''_{\\mathrm{rec}}(\\omega_k)$ at every $\\omega_k$. Normalize pointwise by the magnitude $|Z(\\omega_k)| = \\sqrt{[Z'(\\omega_k)]^2 + [Z''(\\omega_k)]^2}$ to obtain dimensionless residuals.\n- Aggregate the normalized residuals into three metrics: the root-mean-square (RMS) of the normalized real residuals, the RMS of the normalized imaginary residuals, and the combined RMS defined as the square root of the average of the squares of the two RMS values. Explicitly, if the normalized residual vectors are $\\rho'$ and $\\rho''$, compute $\\mathrm{NRMS}_{\\mathrm{re}} = \\sqrt{\\frac{1}{N}\\sum_{k=1}^N [\\rho'(\\omega_k)]^2}$, $\\mathrm{NRMS}_{\\mathrm{im}} = \\sqrt{\\frac{1}{N}\\sum_{k=1}^N [\\rho''(\\omega_k)]^2}$, and $\\mathrm{NRMS}_{\\mathrm{all}} = \\sqrt{\\frac{1}{2}\\left(\\mathrm{NRMS}_{\\mathrm{re}}^2 + \\mathrm{NRMS}_{\\mathrm{im}}^2\\right)}$.\n- Propose a decision threshold $\\tau$ (dimensionless) to flag non-compliance due to non-linearity, noise, or drift. Base your proposal on reasonable EIS signal-to-noise assumptions in battery testing, and use it to return a boolean decision per dataset: return $\\mathrm{True}$ if $\\mathrm{NRMS}_{\\mathrm{all}} \\le \\tau$, else return $\\mathrm{False}$.\n\nPhysical units and numerical requirements:\n\n- The input frequencies must be interpreted as frequency $f$ in hertz (Hz) and converted to angular frequency by $\\omega = 2\\pi f$ in radians per second. Impedance is in ohms (Ω). Your outputs are dimensionless.\n- Round all floating-point outputs to $6$ decimal places.\n\nTest suite and datasets:\n\nUse the following four synthetic datasets as the test suite. For each dataset, construct $N$ logarithmically spaced frequencies $f_i$ in hertz between the specified $f_{\\min}$ and $f_{\\max}$, compute $\\omega_i = 2\\pi f_i$, and simulate the complex impedance $Z(\\omega)$ for a one-time-constant Randles circuit. The Randles circuit model is a series ohmic resistance in series with a parallel combination of a charge-transfer resistance and a double-layer capacitance; it is generated by standard circuit laws and linear time-invariant assumptions. Use these parameter values as specified per dataset:\n\n- Dataset A (happy path): $R_s = 0.5$ ohm, $R_{ct} = 1.0$ ohm, $C_{dl} = 1.5\\times 10^{-3}$ farad, $N = 64$, $f_{\\min} = 0.1$ hertz, $f_{\\max} = 10^4$ hertz. No additional perturbations.\n- Dataset B (noise): Same as Dataset A, but add independent, zero-mean Gaussian noise with standard deviation $\\sigma = 5\\times 10^{-3}$ ohm to both $Z'$ and $Z''$ at each frequency. Use a fixed random seed of $12345$ to ensure reproducibility.\n- Dataset C (drift/non-linearity surrogate): Same as Dataset A, but add a slowly varying baseline bias to the imaginary part: $Z'' \\leftarrow Z'' + b(f)$ with $b(f) = b_0 (f/f_{\\mathrm{ref}})^{-\\alpha}$, where $b_0 = 0.05$ ohm, $f_{\\mathrm{ref}} = 1.0$ hertz, and $\\alpha = 0.3$.\n- Dataset D (sparse grid boundary case): Same as Dataset A, but with $N = 12$.\n\nSpecification of the final program output:\n\n- For each dataset, compute and return a list of four items: $[\\mathrm{NRMS}_{\\mathrm{re}}, \\mathrm{NRMS}_{\\mathrm{im}}, \\mathrm{NRMS}_{\\mathrm{all}}, \\mathrm{Decision}]$, where the three RMS metrics are floats rounded to $6$ decimals, and the decision is a boolean as defined above using your proposed $\\tau$.\n- Your program should produce a single line of output containing the results for the four datasets as a comma-separated list of the four per-dataset lists, enclosed in square brackets and with no spaces. For example, the printed line must look like $[[a,b,c,True],[d,e,f,False],\\dots]$ with all floats rounded as specified and booleans spelled as $True$ or $False$.\n\nAll computations must be implemented by you; do not call any specialized K-K or EIS library functions. The program must be fully self-contained, take no input, and produce exactly one line of output in the specified format.",
            "solution": "The problem requires the design and implementation of a Kramers-Kronig (K-K) compliance test for electrochemical impedance spectroscopy (EIS) data. The solution involves deriving the K-K relations from fundamental principles, discretizing them for numerical computation, and applying the test to several synthetic datasets to evaluate their compliance.\n\n### Principle-Based Design\n\n#### 1. Derivation of the Kramers-Kronig Relations\nThe foundation of the test lies in the properties of the complex impedance, $Z(\\tilde{\\omega})$, as a response function of a linear, time-invariant (LTI), causal, and stable system. For such a system, $Z(\\tilde{\\omega})$ is analytic in the upper half of the complex frequency plane ($\\mathrm{Im}(\\tilde{\\omega})  0$). This analyticity allows the use of Cauchy's integral relations.\n\nLet the impedance at infinite frequency be a real value $Z_{\\infty} = \\lim_{\\omega\\to\\infty} Z(\\omega)$. The function $Z(\\tilde{\\omega}) - Z_{\\infty}$ vanishes as $|\\tilde{\\omega}| \\to \\infty$ in the upper half-plane. For a point $\\omega$ on the real axis, Cauchy's principal value integral formula gives:\n$$ Z(\\omega) - Z_{\\infty} = \\frac{1}{j\\pi} \\mathcal{P} \\int_{-\\infty}^{\\infty} \\frac{Z(\\omega') - Z_{\\infty}}{\\omega' - \\omega} d\\omega' $$\nwhere $\\mathcal{P}$ denotes the Cauchy principal value. Substituting $Z(\\omega) = Z'(\\omega) + jZ''(\\omega)$ yields:\n$$ Z'(\\omega) - Z_{\\infty} + jZ''(\\omega) = \\frac{1}{j\\pi} \\mathcal{P} \\int_{-\\infty}^{\\infty} \\frac{Z'(\\omega') - Z_{\\infty} + jZ''(\\omega')}{\\omega' - \\omega} d\\omega' = \\frac{1}{\\pi} \\mathcal{P} \\int_{-\\infty}^{\\infty} \\frac{Z''(\\omega')}{\\omega' - \\omega} d\\omega' - \\frac{j}{\\pi} \\mathcal{P} \\int_{-\\infty}^{\\infty} \\frac{Z'(\\omega') - Z_{\\infty}}{\\omega' - \\omega} d\\omega' $$\nEquating the real and imaginary parts separately, we get:\n$$ Z'(\\omega) - Z_{\\infty} = \\frac{1}{\\pi} \\mathcal{P} \\int_{-\\infty}^{\\infty} \\frac{Z''(\\omega')}{\\omega' - \\omega} d\\omega' $$\n$$ Z''(\\omega) = -\\frac{1}{\\pi} \\mathcal{P} \\int_{-\\infty}^{\\infty} \\frac{Z'(\\omega') - Z_{\\infty}}{\\omega' - \\omega} d\\omega' $$\nFor real physical systems, impedance satisfies the property $Z(-\\omega) = Z^*(\\omega)$, which implies that $Z'(\\omega)$ is an even function and $Z''(\\omega)$ is an odd function of frequency. This symmetry allows transforming the integrals to the domain of positive frequencies $[0, \\infty)$:\n$$ Z'(\\omega) - Z_{\\infty} = \\frac{2}{\\pi} \\mathcal{P} \\int_{0}^{\\infty} \\frac{\\omega' Z''(\\omega')}{(\\omega')^2 - \\omega^2} d\\omega' \\quad (1) $$\n$$ Z''(\\omega) = -\\frac{2\\omega}{\\pi} \\mathcal{P} \\int_{0}^{\\infty} \\frac{Z'(\\omega') - Z_{\\infty}}{(\\omega')^2 - \\omega^2} d\\omega' \\quad (2) $$\nThese are the integral relations that form the basis of our numerical test. They allow the reconstruction of one part of the impedance spectrum from the other.\n\n#### 2. Numerical Implementation\nThe test is implemented by discretizing the integrals (1) and (2) over a finite grid of measured angular frequencies $\\{\\omega_i\\}_{i=1}^N$.\n\n**Discretization:** The integrals are approximated by a weighted sum. Given a set of measured impedance values $(Z'(\\omega_i), Z''(\\omega_i))$, we reconstruct the spectra, denoted $Z'_{\\mathrm{rec}}(\\omega_k)$ and $Z''_{\\mathrm{rec}}(\\omega_k)$, at each frequency $\\omega_k$ in the grid. The problem specifies a particular quadrature rule with weights $w_i$:\n- $w_1 = \\omega_2 - \\omega_1$\n- $w_N = \\omega_N - \\omega_{N-1}$\n- $w_i = (\\omega_{i+1} - \\omega_{i-1})/2$ for $i \\in \\{2, \\ldots, N-1\\}$\n\nThe high-frequency limit $Z_{\\infty}$ is unknown and must be estimated from the available data. As specified, we use the real part of the impedance at the highest measured frequency: $Z_{\\infty} \\approx Z'_{\\mathrm{meas}}(\\omega_N)$.\n\nThe Cauchy principal value is handled by excluding the term where $i=k$ from the summation, as this is where the integrand's denominator becomes zero. The discretized forms of equations (1) and (2) are:\n$$ Z'_{\\mathrm{rec}}(\\omega_k) \\approx Z_{\\infty} + \\frac{2}{\\pi} \\sum_{i=1, i \\neq k}^{N} \\frac{\\omega_i Z''_{\\mathrm{meas}}(\\omega_i)}{(\\omega_i)^2 - \\omega_k^2} w_i \\quad (3) $$\n$$ Z''_{\\mathrm{rec}}(\\omega_k) \\approx -\\frac{2\\omega_k}{\\pi} \\sum_{i=1, i \\neq k}^{N} \\frac{Z'_{\\mathrm{meas}}(\\omega_i) - Z_{\\infty}}{(\\omega_i)^2 - \\omega_k^2} w_i \\quad (4) $$\n\n#### 3. Residual Analysis and Metrics\nThe validity of the K-K relations for the given dataset is assessed by quantifying the difference between the measured and reconstructed spectra.\n\n**Residuals:** The residuals are computed at each frequency point $\\omega_k$:\n- Real part residual: $r'(\\omega_k) = Z'_{\\mathrm{meas}}(\\omega_k) - Z'_{\\mathrm{rec}}(\\omega_k)$\n- Imaginary part residual: $r''(\\omega_k) = Z''_{\\mathrm{meas}}(\\omega_k) - Z''_{\\mathrm{rec}}(\\omega_k)$\n\n**Normalization:** To make the residuals dimensionless and comparable across different frequency ranges and impedance magnitudes, they are normalized by the impedance modulus $|Z(\\omega_k)| = \\sqrt{Z'_{\\mathrm{meas}}(\\omega_k)^2 + Z''_{\\mathrm{meas}}(\\omega_k)^2}$:\n- Normalized real residual: $\\rho'(\\omega_k) = r'(\\omega_k) / |Z(\\omega_k)|$\n- Normalized imaginary residual: $\\rho''(\\omega_k) = r''(\\omega_k) / |Z(\\omega_k)|$\n\n**Aggregate Metrics:** The set of normalized residuals is aggregated into three root-mean-square (RMS) metrics to provide a concise summary of the overall compliance:\n- $\\mathrm{NRMS}_{\\mathrm{re}} = \\sqrt{\\frac{1}{N}\\sum_{k=1}^N [\\rho'(\\omega_k)]^2}$\n- $\\mathrm{NRMS}_{\\mathrm{im}} = \\sqrt{\\frac{1}{N}\\sum_{k=1}^N [\\rho''(\\omega_k)]^2}$\n- $\\mathrm{NRMS}_{\\mathrm{all}} = \\sqrt{\\frac{1}{2}\\left(\\mathrm{NRMS}_{\\mathrm{re}}^2 + \\mathrm{NRMS}_{\\mathrm{im}}^2\\right)}$\n\n#### 4. Compliance Decision\nA decision on K-K compliance is made by comparing the combined metric, $\\mathrm{NRMS}_{\\mathrm{all}}$, against a predefined threshold, $\\tau$.\nIn high-quality EIS measurements, deviations from the ideal LTI behavior are small. Normalized residual levels below a few percent are typically considered indicative of good data quality. Residuals below $1\\%$ suggest excellent data, while those exceeding $5\\%$ often point to significant measurement artifacts such as noise, drift, or non-linearity. Based on these considerations, we propose a threshold $\\tau = 0.03$. This value represents an average relative error of $3\\%$, providing a reasonable balance for flagging potentially non-compliant data while tolerating minor experimental noise and numerical artifacts.\nThe decision rule is:\n- Return `True` (compliant) if $\\mathrm{NRMS}_{\\mathrm{all}} \\le \\tau$.\n- Return `False` (non-compliant) if $\\mathrm{NRMS}_{\\mathrm{all}}  \\tau$.\n\n#### 5. Test Data Generation\nThe test suite utilizes synthetic data from a Randles equivalent circuit, whose impedance is given by:\n$$ Z(\\omega) = R_s + \\frac{R_{ct}}{1 + j\\omega R_{ct} C_{dl}} $$\nThis can be separated into real and imaginary parts:\n$$ Z'(\\omega) = R_s + \\frac{R_{ct}}{1 + (\\omega R_{ct} C_{dl})^2} $$\n$$ Z''(\\omega) = - \\frac{\\omega R_{ct}^2 C_{dl}}{1 + (\\omega R_{ct} C_{dl})^2} $$\nwhere $R_s$ is the series resistance, $R_{ct}$ is the charge-transfer resistance, and $C_{dl}$ is the double-layer capacitance. This model is inherently LTI and thus K-K compliant. Perturbations are added to simulate measurement imperfections.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# No other libraries are imported, as per the rules.\n\ndef solve():\n    \"\"\"\n    Main function to run the Kramers-Kronig compliance test on a suite of datasets.\n    \"\"\"\n    \n    # Decision threshold for K-K compliance.\n    # A value of 0.03 (3%) is chosen as a reasonable threshold for distinguishing\n    # high-quality data from data with significant artifacts.\n    TAU_THRESHOLD = 0.03\n\n    # Define the parameters for the four test cases.\n    # (Rs, Rct, Cdl, N, fmin, fmax, noise_sigma, drift_params)\n    test_cases = [\n        # Dataset A (happy path)\n        (0.5, 1.0, 1.5e-3, 64, 0.1, 1e4, 0.0, None),\n        # Dataset B (noise)\n        (0.5, 1.0, 1.5e-3, 64, 0.1, 1e4, 5e-3, None),\n        # Dataset C (drift/non-linearity surrogate)\n        (0.5, 1.0, 1.5e-3, 64, 0.1, 1e4, 0.0, {'b0': 0.05, 'f_ref': 1.0, 'alpha': 0.3}),\n        # Dataset D (sparse grid boundary case)\n        (0.5, 1.0, 1.5e-3, 12, 0.1, 1e4, 0.0, None)\n    ]\n\n    all_results = []\n    \n    # Use a fixed seed for the random number generator for reproducibility.\n    rng = np.random.default_rng(12345)\n\n    for case in test_cases:\n        rs, rct, cdl, n, fmin, fmax, noise_sigma, drift_params = case\n        \n        # 1. Generate Data\n        f = np.logspace(np.log10(fmin), np.log10(fmax), n)\n        omega = 2 * np.pi * f\n        \n        # Randles circuit impedance\n        z_prime_ideal = rs + rct / (1 + (omega * rct * cdl)**2)\n        z_imag_ideal = - (omega * rct**2 * cdl) / (1 + (omega * rct * cdl)**2)\n        \n        z_meas_prime = z_prime_ideal\n        z_meas_imag = z_imag_ideal\n        \n        # Apply perturbations\n        if noise_sigma > 0:\n            noise_prime = rng.normal(scale=noise_sigma, size=n)\n            noise_imag = rng.normal(scale=noise_sigma, size=n)\n            z_meas_prime += noise_prime\n            z_meas_imag += noise_imag\n        \n        if drift_params:\n            b0, f_ref, alpha = drift_params['b0'], drift_params['f_ref'], drift_params['alpha']\n            bias = b0 * (f / f_ref)**(-alpha)\n            z_meas_imag += bias\n\n        # 2. K-K Transform\n        # Estimate high-frequency limit\n        z_inf = z_meas_prime[-1]\n        \n        # Calculate quadrature weights\n        w = np.zeros(n)\n        if n > 1:\n            w[0] = omega[1] - omega[0]\n            w[-1] = omega[-1] - omega[-2]\n            if n > 2:\n                w[1:-1] = (omega[2:] - omega[:-2]) / 2\n\n        z_rec_prime = np.zeros(n)\n        z_rec_imag = np.zeros(n)\n\n        # Perform reconstruction for each frequency point k\n        for k in range(n):\n            omega_k = omega[k]\n            \n            # Use boolean mask to exclude the singularity at i=k\n            i_not_k = np.arange(n) != k\n            omega_i = omega[i_not_k]\n            w_i = w[i_not_k]\n            \n            # Common denominator term\n            denom = omega_i**2 - omega_k**2\n            \n            # Reconstruct real part from imaginary part\n            integrand_prime = (omega_i * z_meas_imag[i_not_k]) / denom\n            sum_prime = np.sum(integrand_prime * w_i)\n            z_rec_prime[k] = z_inf + (2 / np.pi) * sum_prime\n            \n            # Reconstruct imaginary part from real part\n            integrand_imag = (z_meas_prime[i_not_k] - z_inf) / denom\n            sum_imag = np.sum(integrand_imag * w_i)\n            z_rec_imag[k] = (-2 * omega_k / np.pi) * sum_imag\n            \n        # 3. Residuals and Metrics\n        res_prime = z_meas_prime - z_rec_prime\n        res_imag = z_meas_imag - z_rec_imag\n        \n        # Impedance modulus for normalization\n        z_mod = np.sqrt(z_meas_prime**2 + z_meas_imag**2)\n        # Avoid division by zero, although not expected in these datasets\n        z_mod[z_mod == 0] = 1e-9 \n        \n        norm_res_prime = res_prime / z_mod\n        norm_res_imag = res_imag / z_mod\n        \n        # Calculate final metrics\n        nrms_re = np.sqrt(np.mean(norm_res_prime**2))\n        nrms_im = np.sqrt(np.mean(norm_res_imag**2))\n        nrms_all = np.sqrt(0.5 * (nrms_re**2 + nrms_im**2))\n        \n        # 4. Decision\n        decision = nrms_all = TAU_THRESHOLD\n        \n        # Store rounded results\n        all_results.append([\n            round(nrms_re, 6),\n            round(nrms_im, 6),\n            round(nrms_all, 6),\n            decision\n        ])\n\n    # 5. Final Output Formatting\n    case_outputs = []\n    for res in all_results:\n        # Format: [float,float,float,Bool] with 6 decimal places for floats\n        s = f\"[{res[0]:.6f},{res[1]:.6f},{res[2]:.6f},{res[3]}]\"\n        case_outputs.append(s)\n        \n    final_output_string = f\"[{','.join(case_outputs)}]\"\n    print(final_output_string)\n\nsolve()\n```"
        },
        {
            "introduction": "A common challenge in EIS analysis is that multiple different equivalent circuit models can often provide a visually good fit to the same experimental data. This ambiguity requires a more rigorous approach than simple visual inspection to select the most physically meaningful model. This exercise  demonstrates how two distinct models can produce nearly indistinguishable Nyquist plots and then equips you with a powerful statistical tool, the Bayesian Information Criterion (BIC), to objectively select the better model by balancing goodness-of-fit against model complexity.",
            "id": "3908855",
            "problem": "An automated battery design and simulation pipeline evaluates Electrochemical Impedance Spectroscopy (EIS) data from a lithium-ion cell using two candidate equivalent-circuit models. Electrochemical Impedance Spectroscopy (EIS) measures complex impedance $Z(\\omega)$ under a small-amplitude sinusoidal perturbation at angular frequency $\\omega$, and the Nyquist plot is the parametric plot of $\\operatorname{Re}\\{Z(\\omega)\\}$ versus $-\\operatorname{Im}\\{Z(\\omega)\\}$. Consider the following two models for the mid-frequency charge-transfer feature:\n\n- Model A: A series ohmic resistance $R_{s}$ in series with a parallel combination of charge-transfer resistance $R_{ct}$ and double-layer capacitance $C_{dl}$.\n- Model B: A series ohmic resistance $R_{s}$ in series with a parallel combination of charge-transfer resistance $R_{ct}$ and a Constant Phase Element (CPE) parameterized by $Q$ and $n$.\n\nBoth models are instantiated with the same resistances $R_{s} = 0.051\\,\\Omega$ and $R_{ct} = 0.189\\,\\Omega$. Model A uses $C_{dl} = 6.3 \\times 10^{-3}\\,\\mathrm{F}$, and Model B uses $n = 0.98$ and a $Q$ chosen so that the effective capacitance of the CPE locally matches the ideal capacitance of Model A at the characteristic angular frequency of Model A’s parallel branch. Specifically, define the characteristic angular frequency by $\\omega_{0} = \\frac{1}{R_{ct} C_{dl}}$ and choose $Q$ to satisfy $Q\\,\\omega_{0}^{n-1} = C_{dl}$. With these parameters:\n\n1. Starting from the definition of impedance for a series element and the admittance $Y(\\omega)$ of a parallel branch, and the properties of an ideal capacitor and a Constant Phase Element, derive the high-frequency ($\\omega \\to \\infty$) and low-frequency ($\\omega \\to 0$) limits of $\\operatorname{Re}\\{Z(\\omega)\\}$ for both models and show that the Nyquist arc intercepts coincide for the given $R_{s}$ and $R_{ct}$.\n2. Using the matching condition $Q\\,\\omega_{0}^{n-1} = C_{dl}$, evaluate the imaginary component $-\\operatorname{Im}\\{Z(\\omega_{0})\\}$ for both models and show that the difference in the apex height of the Nyquist arcs is within a typical instrument tolerance of $\\pm 5 \\times 10^{-3}\\,\\Omega$.\n\nThe pipeline fits both models to $M = 150$ frequencies, producing residuals in both the real and imaginary parts. Assume the residuals are independent and identically distributed Gaussian real-valued errors across both components, so the total number of scalar residuals is $N = 2M$. The least-squares residual sums are:\n- For Model A (ideal capacitor): $\\mathrm{RSS}_{A} = 1.35 \\times 10^{-4}\\,\\Omega^{2}$.\n- For Model B (CPE): $\\mathrm{RSS}_{B} = 1.28 \\times 10^{-4}\\,\\Omega^{2}$.\n\nThe number of fitted parameters (excluding any fixed measurement scaling) is $k_{A} = 3$ for Model A and $k_{B} = 4$ for Model B. Based on the Gaussian-noise assumption and maximum-likelihood estimation, derive the Bayesian Information Criterion (BIC) for each model and compute the difference $\\Delta \\mathrm{BIC} = \\mathrm{BIC}_{A} - \\mathrm{BIC}_{B}$. Round your final numerical answer to four significant figures and express it in nats.",
            "solution": "The problem is validated as scientifically grounded, well-posed, and objective. It presents a standard task in electrochemical analysis: deriving properties of equivalent circuit models and performing statistical model selection. All necessary data and definitions are provided. We will address the three parts of the problem in sequence.\n\nThe impedance of a resistor is $Z_R = R$. The impedance of an ideal capacitor is $Z_C(\\omega) = \\frac{1}{j\\omega C}$. The impedance of a Constant Phase Element (CPE) is $Z_{CPE}(\\omega) = \\frac{1}{Q(j\\omega)^n}$, where $j = \\sqrt{-1}$ is the imaginary unit, $\\omega$ is the angular frequency, and $Q$ and $n$ are the CPE parameters.\n\n**Part 1: High- and Low-Frequency Limits**\n\nFor Model A, the impedance consists of a series resistance $R_s$ and a parallel branch containing $R_{ct}$ and $C_{dl}$. The admittance of the parallel branch is the sum of the individual admittances:\n$$ Y_{p,A}(\\omega) = \\frac{1}{R_{ct}} + \\frac{1}{Z_{C_{dl}}(\\omega)} = \\frac{1}{R_{ct}} + j\\omega C_{dl} $$\nThe impedance of this parallel branch is $Z_{p,A}(\\omega) = \\frac{1}{Y_{p,A}(\\omega)} = \\frac{1}{\\frac{1}{R_{ct}} + j\\omega C_{dl}} = \\frac{R_{ct}}{1 + j\\omega R_{ct} C_{dl}}$.\nThe total impedance for Model A is the sum of the series components:\n$$ Z_A(\\omega) = R_s + Z_{p,A}(\\omega) = R_s + \\frac{R_{ct}}{1 + j\\omega R_{ct} C_{dl}} $$\nTo find the real part, we rationalize the complex fraction:\n$$ Z_A(\\omega) = R_s + \\frac{R_{ct}(1 - j\\omega R_{ct} C_{dl})}{1 + (\\omega R_{ct} C_{dl})^2} = \\left(R_s + \\frac{R_{ct}}{1 + (\\omega R_{ct} C_{dl})^2}\\right) - j\\left(\\frac{\\omega R_{ct}^2 C_{dl}}{1 + (\\omega R_{ct} C_{dl})^2}\\right) $$\nThe real part is $\\operatorname{Re}\\{Z_A(\\omega)\\} = R_s + \\frac{R_{ct}}{1 + (\\omega R_{ct} C_{dl})^2}$.\nWe now evaluate the limits:\nAs $\\omega \\to \\infty$, the term $(\\omega R_{ct} C_{dl})^2 \\to \\infty$, causing the fraction to go to $0$. Thus, the high-frequency limit is:\n$$ \\lim_{\\omega\\to\\infty} \\operatorname{Re}\\{Z_A(\\omega)\\} = R_s + 0 = R_s $$\nAs $\\omega \\to 0$, the term $(\\omega R_{ct} C_{dl})^2 \\to 0$. Thus, the low-frequency limit is:\n$$ \\lim_{\\omega\\to 0} \\operatorname{Re}\\{Z_A(\\omega)\\} = R_s + \\frac{R_{ct}}{1 + 0} = R_s + R_{ct} $$\n\nFor Model B, the capacitor is replaced by a CPE. The admittance of the parallel branch is:\n$$ Y_{p,B}(\\omega) = \\frac{1}{R_{ct}} + \\frac{1}{Z_{CPE}(\\omega)} = \\frac{1}{R_{ct}} + Q(j\\omega)^n $$\nThe total impedance for Model B is:\n$$ Z_B(\\omega) = R_s + \\frac{1}{\\frac{1}{R_{ct}} + Q(j\\omega)^n} = R_s + \\frac{R_{ct}}{1 + R_{ct}Q(j\\omega)^n} $$\nWe evaluate the impedance limits directly.\nAs $\\omega \\to \\infty$, the magnitude of the term $(j\\omega)^n$ goes to infinity (since $n=0.980$). The denominator of the fraction becomes dominated by this term, causing the fraction's magnitude to approach $0$. Thus, in the high-frequency limit:\n$$ \\lim_{\\omega\\to\\infty} Z_B(\\omega) = R_s + 0 = R_s $$\nThe real part is therefore $\\operatorname{Re}\\{Z_B(\\infty)\\} = R_s$.\nAs $\\omega \\to 0$, the term $(j\\omega)^n$ goes to $0$. Thus, in the low-frequency limit:\n$$ \\lim_{\\omega\\to 0} Z_B(\\omega) = R_s + \\frac{R_{ct}}{1+0} = R_s + R_{ct} $$\nThe real part is therefore $\\operatorname{Re}\\{Z_B(0)\\} = R_s + R_{ct}$.\nFor both models, the Nyquist plot arc intercepts the real axis at $R_s$ at high frequency and at $R_s + R_{ct}$ at low frequency. These intercepts are identical for both models since they share the same values for $R_s$ and $R_{ct}$.\n\n**Part 2: Apex Height Comparison**\n\nThe apex of the Nyquist arc is typically near the characteristic frequency $\\omega_0 = \\frac{1}{R_{ct} C_{dl}}$. We need to evaluate $-\\operatorname{Im}\\{Z(\\omega_0)\\}$ for both models.\n\nFor Model A, we found $\\operatorname{Im}\\{Z_A(\\omega)\\} = -\\frac{\\omega R_{ct}^2 C_{dl}}{1 + (\\omega R_{ct} C_{dl})^2}$. Thus:\n$$ -\\operatorname{Im}\\{Z_A(\\omega)\\} = \\frac{\\omega R_{ct}^2 C_{dl}}{1 + (\\omega R_{ct} C_{dl})^2} $$\nAt $\\omega = \\omega_0 = \\frac{1}{R_{ct} C_{dl}}$, the product $\\omega_0 R_{ct} C_{dl} = 1$. Substituting this in:\n$$ -\\operatorname{Im}\\{Z_A(\\omega_0)\\} = \\frac{(\\frac{1}{R_{ct} C_{dl}}) R_{ct}^2 C_{dl}}{1 + (1)^2} = \\frac{R_{ct}}{2} $$\nThis represents the radius of the semicircle, which is the apex height.\n\nFor Model B, we need the imaginary part of $Z_B(\\omega) = R_s + \\frac{R_{ct}}{1 + R_{ct}Q(j\\omega)^n}$.\nLet's use the property $j^n = \\exp(j\\frac{n\\pi}{2}) = \\cos(\\frac{n\\pi}{2}) + j\\sin(\\frac{n\\pi}{2})$.\n$$ Z_B(\\omega) = R_s + \\frac{R_{ct}}{1 + R_{ct}Q\\omega^n \\cos(\\frac{n\\pi}{2}) + j R_{ct}Q\\omega^n \\sin(\\frac{n\\pi}{2})} $$\nThe imaginary part of the fraction is:\n$$ \\operatorname{Im}\\left\\{\\frac{R_{ct}( (1 + R_{ct}Q\\omega^n \\cos(\\frac{n\\pi}{2})) - j R_{ct}Q\\omega^n \\sin(\\frac{n\\pi}{2}) )}{(1 + R_{ct}Q\\omega^n \\cos(\\frac{n\\pi}{2}))^2 + (R_{ct}Q\\omega^n \\sin(\\frac{n\\pi}{2}))^2}\\right\\} = \\frac{-R_{ct}^2Q\\omega^n \\sin(\\frac{n\\pi}{2})}{Denominator} $$\nSo, $-\\operatorname{Im}\\{Z_B(\\omega)\\} = \\frac{R_{ct}^2Q\\omega^n \\sin(\\frac{n\\pi}{2})}{(1 + R_{ct}Q\\omega^n \\cos(\\frac{n\\pi}{2}))^2 + (R_{ct}Q\\omega^n \\sin(\\frac{n\\pi}{2}))^2}$.\nWe evaluate this at $\\omega = \\omega_0$. The problem provides the matching condition $Q \\omega_0^{n-1} = C_{dl}$. We can use this to simplify the term $R_{ct}Q\\omega_0^n$:\n$$ R_{ct}Q\\omega_0^n = R_{ct}Q\\omega_0^{n-1}\\omega_0 = R_{ct}(C_{dl})\\omega_0 = R_{ct}C_{dl}\\left(\\frac{1}{R_{ct}C_{dl}}\\right) = 1 $$\nSubstituting this result into the expression for $-\\operatorname{Im}\\{Z_B(\\omega_0)\\}$:\n$$ -\\operatorname{Im}\\{Z_B(\\omega_0)\\} = \\frac{R_{ct} \\cdot (1) \\cdot \\sin(\\frac{n\\pi}{2})}{(1 + (1) \\cos(\\frac{n\\pi}{2}))^2 + (\\sin(\\frac{n\\pi}{2}))^2} $$\nThe denominator simplifies using $\\cos^2(x) + \\sin^2(x) = 1$:\n$$ \\text{Denominator} = 1 + 2\\cos(\\frac{n\\pi}{2}) + \\cos^2(\\frac{n\\pi}{2}) + \\sin^2(\\frac{n\\pi}{2}) = 2 + 2\\cos(\\frac{n\\pi}{2}) = 2(1 + \\cos(\\frac{n\\pi}{2})) $$\nUsing the half-angle identity $1+\\cos(x)=2\\cos^2(x/2)$, the denominator becomes $4\\cos^2(\\frac{n\\pi}{4})$.\nUsing the double-angle identity $\\sin(x)=2\\sin(x/2)\\cos(x/2)$, the numerator term is $\\sin(\\frac{n\\pi}{2}) = 2\\sin(\\frac{n\\pi}{4})\\cos(\\frac{n\\pi}{4})$.\n$$ -\\operatorname{Im}\\{Z_B(\\omega_0)\\} = \\frac{R_{ct} \\cdot 2\\sin(\\frac{n\\pi}{4})\\cos(\\frac{n\\pi}{4})}{4\\cos^2(\\frac{n\\pi}{4})} = \\frac{R_{ct}}{2} \\frac{\\sin(\\frac{n\\pi}{4})}{\\cos(\\frac{n\\pi}{4})} = \\frac{R_{ct}}{2} \\tan(\\frac{n\\pi}{4}) $$\nNow we calculate the values. Given $R_{ct} = 0.189\\,\\Omega$ and $n = 0.98$:\n$$ -\\operatorname{Im}\\{Z_A(\\omega_0)\\} = \\frac{0.189}{2} = 0.0945\\,\\Omega $$\n$$ -\\operatorname{Im}\\{Z_B(\\omega_0)\\} = \\frac{0.189}{2} \\tan\\left(\\frac{0.98\\pi}{4}\\right) \\approx 0.0945 \\times \\tan(0.76969) \\approx 0.0945 \\times 0.96767 \\approx 0.09144\\,\\Omega $$\nThe difference in apex height is:\n$$ \\Delta_h = |-\\operatorname{Im}\\{Z_A(\\omega_0)\\} - (-\\operatorname{Im}\\{Z_B(\\omega_0)\\})| \\approx |0.0945 - 0.09144| = 0.00306\\,\\Omega $$\nThis difference, $3.06 \\times 10^{-3}\\,\\Omega$, is within the specified instrument tolerance of $\\pm 5 \\times 10^{-3}\\,\\Omega$.\n\n**Part 3: Bayesian Information Criterion (BIC)**\n\nThe Bayesian Information Criterion (BIC) is defined as $\\mathrm{BIC} = -2\\ln(\\hat{L}) + k \\ln(N)$, where $\\hat{L}$ is the maximized value of the likelihood function, $k$ is the number of estimated parameters, and $N$ is the number of data points. For a model with independent and identically distributed Gaussian errors, the maximized log-likelihood is given by:\n$$ \\ln(\\hat{L}) = -\\frac{N}{2} \\ln(2\\pi) - \\frac{N}{2} \\ln(\\hat{\\sigma}^2) - \\frac{N}{2} $$\nwhere the maximum likelihood estimate of the error variance is $\\hat{\\sigma}^2 = \\frac{\\mathrm{RSS}}{N}$, with $\\mathrm{RSS}$ being the residual sum of squares. Substituting $\\hat{\\sigma}^2$ gives:\n$$ \\ln(\\hat{L}) = -\\frac{N}{2} \\left[ \\ln(2\\pi) + \\ln\\left(\\frac{\\mathrm{RSS}}{N}\\right) + 1 \\right] $$\nThus, the BIC is:\n$$ \\mathrm{BIC} = N\\left[\\ln(2\\pi) + \\ln\\left(\\frac{\\mathrm{RSS}}{N}\\right) + 1\\right] + k \\ln(N) $$\nWe are asked to compute the difference $\\Delta \\mathrm{BIC} = \\mathrm{BIC}_{A} - \\mathrm{BIC}_{B}$. When taking the difference between two models fitted to the same data (same $N$), terms that do not depend on the model-specific quantities ($k$, $\\mathrm{RSS}$) cancel out.\n$$ \\mathrm{BIC}_{A} = N\\left[\\ln\\left(\\frac{\\mathrm{RSS}_{A}}{N}\\right)\\right] + k_{A} \\ln(N) + C $$\n$$ \\mathrm{BIC}_{B} = N\\left[\\ln\\left(\\frac{\\mathrm{RSS}_{B}}{N}\\right)\\right] + k_{B} \\ln(N) + C $$\nwhere $C = N(\\ln(2\\pi) + 1)$ is a constant for both models.\n$$ \\Delta \\mathrm{BIC} = \\mathrm{BIC}_{A} - \\mathrm{BIC}_{B} = N\\left[\\ln\\left(\\frac{\\mathrm{RSS}_{A}}{N}\\right) - \\ln\\left(\\frac{\\mathrm{RSS}_{B}}{N}\\right)\\right] + (k_{A} - k_{B})\\ln(N) $$\n$$ \\Delta \\mathrm{BIC} = N \\ln\\left(\\frac{\\mathrm{RSS}_{A}}{\\mathrm{RSS}_{B}}\\right) + (k_{A} - k_{B})\\ln(N) $$\nNow we substitute the given values:\n$N=300$, $k_A=3$, $k_B=4$, $\\mathrm{RSS}_A = 1.35 \\times 10^{-4}\\,\\Omega^2$, $\\mathrm{RSS}_B = 1.28 \\times 10^{-4}\\,\\Omega^2$.\n$$ \\Delta \\mathrm{BIC} = 300 \\ln\\left(\\frac{1.35 \\times 10^{-4}}{1.28 \\times 10^{-4}}\\right) + (3 - 4)\\ln(300) $$\n$$ \\Delta \\mathrm{BIC} = 300 \\ln\\left(\\frac{1.35}{1.28}\\right) - \\ln(300) $$\nWe compute the numerical values:\n$$ \\ln(300) \\approx 5.70378 $$\n$$ \\frac{1.35}{1.28} \\approx 1.0546875 $$\n$$ \\ln\\left(\\frac{1.35}{1.28}\\right) \\approx \\ln(1.0546875) \\approx 0.053236 $$\n$$ \\Delta \\mathrm{BIC} \\approx 300 \\times 0.053236 - 5.70378 $$\n$$ \\Delta \\mathrm{BIC} \\approx 15.9708 - 5.70378 = 10.26702 $$\nRounding to four significant figures, we get $10.27$. The units are nats, as natural logarithm was used. A positive $\\Delta \\mathrm{BIC}$ indicates that Model B is preferred over Model A, as it has a lower BIC value.",
            "answer": "$$\n\\boxed{10.27}\n$$"
        }
    ]
}