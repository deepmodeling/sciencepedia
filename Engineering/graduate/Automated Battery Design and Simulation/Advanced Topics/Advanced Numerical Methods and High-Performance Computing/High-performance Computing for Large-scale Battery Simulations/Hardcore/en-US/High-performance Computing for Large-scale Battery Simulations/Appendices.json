{
    "hands_on_practices": [
        {
            "introduction": "Before we can optimize a simulation, we must understand the underlying physics and its mathematical structure. The Pseudo-two-Dimensional (P2D) model is a system of coupled partial differential equations whose behavior dictates the choice of numerical methods. This exercise guides you through nondimensionalization, a powerful technique to reveal the key physical timescales and identify dimensionless groups that quantify numerical stiffness, a major challenge in scientific computing. Understanding these groups is the first step in designing a robust and efficient High-Performance Computing (HPC) solver .",
            "id": "3918533",
            "problem": "Consider the pseudo-two-dimensional (P2D) model of a porous lithium-ion battery electrode, resolved in the through-thickness coordinate $x \\in [0,L]$ and the spherical particle radial coordinate $r \\in [0,R_{p}]$. The governing relations, based on Fickian transport, charge conservation, and interfacial kinetics, consist of:\n\n- Electrolyte mass balance: $\\varepsilon_{e}\\,\\dfrac{\\partial c_{e}}{\\partial t} = \\dfrac{\\partial}{\\partial x}\\!\\left(D_{e}\\,\\dfrac{\\partial c_{e}}{\\partial x}\\right) + \\left(1-t_{+}^{0}\\right)\\,\\dfrac{a_{s}\\,j}{F}$.\n- Solid particle diffusion: $\\dfrac{\\partial c_{s}}{\\partial t} = \\dfrac{D_{s}}{r^{2}}\\,\\dfrac{\\partial}{\\partial r}\\!\\left(r^{2}\\,\\dfrac{\\partial c_{s}}{\\partial r}\\right)$ with boundary condition $-D_{s}\\,\\left.\\dfrac{\\partial c_{s}}{\\partial r}\\right|_{r=R_{p}} = \\dfrac{j}{F}$ and symmetry at $r=0$.\n- Charge conservation in electrolyte: $\\dfrac{\\partial}{\\partial x}\\!\\left(\\kappa_{\\mathrm{eff}}\\,\\dfrac{\\partial \\phi_{e}}{\\partial x} + 2\\,\\dfrac{R\\,T}{F}\\,\\left(1-t_{+}^{0}\\right)\\,\\dfrac{\\partial \\ln c_{e}}{\\partial x}\\right) = a_{s}\\,j$.\n- Charge conservation in solid: $\\dfrac{\\partial}{\\partial x}\\!\\left(\\sigma_{\\mathrm{eff}}\\,\\dfrac{\\partial \\phi_{s}}{\\partial x}\\right) = -\\,a_{s}\\,j$.\n- Interfacial kinetics (generic), $j = j_{\\mathrm{ref}}\\,\\mathcal{K}(c_{e},c_{s}^{\\mathrm{surf}},\\eta)$, where $\\mathcal{K}$ is a dimensionless kinetic function arising from Butler–Volmer-type relations and $\\eta$ is the overpotential.\n\nHere, $\\varepsilon_{e}$ is the electrolyte porosity, $c_{e}$ is the electrolyte lithium concentration, $c_{s}$ is the solid concentration inside active particles, $D_{e}$ and $D_{s}$ are the effective electrolyte and solid diffusion coefficients, respectively, $t_{+}^{0}$ is the transference number, $a_{s}$ is the specific interfacial area, $j$ is the interfacial current density, $F$ is the Faraday constant, $\\kappa_{\\mathrm{eff}}$ and $\\sigma_{\\mathrm{eff}}$ are the effective electrolyte and solid conductivities, respectively, $\\phi_{e}$ and $\\phi_{s}$ are electrolyte and solid potentials, $R$ is the universal gas constant, and $T$ is the temperature.\n\nStarting from these equations and standard conservation principles, perform a systematic nondimensionalization using the following scales: $x$ with $L$, $r$ with $R_{p}$, $t$ with $t_{D,e} = L^{2}/D_{e}$ for through-thickness processes and with $t_{D,s} = R_{p}^{2}/D_{s}$ for intra-particle processes as appropriate, $c_{e}$ with $c_{e0}$, $c_{s}$ with $c_{s,\\max}$, $\\phi_{e}$ and $\\phi_{s}$ with $R\\,T/F$, and $j$ with $j_{\\mathrm{ref}}$. From your nondimensionalization, identify and interpret the following key dimensionless groups in terms of the original dimensional parameters:\n\n- An electrolyte Damköhler number $\\mathrm{Da}_{e}$ measuring the ratio of reaction to electrolyte diffusion.\n- A particle Thiele-type parameter $\\phi^{2}$ measuring the ratio of interfacial reaction to solid-state diffusion within particles.\n- A conductivity ratio $\\Lambda$ comparing electronic to ionic conduction pathways.\n\nExplain briefly how the magnitudes of these groups inform algorithmic stiffness and the selection of time integrators and preconditioners in high-performance computing (HPC) implementations for large-scale simulations.\n\nThen, using your derived expressions, evaluate the composite stiffness indicator\n$$\\Xi \\equiv \\max\\!\\left\\{\\mathrm{Da}_{e},\\,\\phi^{2}\\right\\}$$\nfor the following scientifically plausible parameter values at room temperature:\n$L = 70\\times 10^{-6}\\,\\mathrm{m}$, $R_{p} = 5\\times 10^{-6}\\,\\mathrm{m}$, $a_{s} = 8.0\\times 10^{4}\\,\\mathrm{m^{-1}}$, $D_{e} = 1.5\\times 10^{-10}\\,\\mathrm{m^{2}\\,s^{-1}}$, $D_{s} = 5.0\\times 10^{-14}\\,\\mathrm{m^{2}\\,s^{-1}}$, $c_{e0} = 1000\\,\\mathrm{mol\\,m^{-3}}$, $c_{s,\\max} = 2.5\\times 10^{4}\\,\\mathrm{mol\\,m^{-3}}$, $j_{\\mathrm{ref}} = 3.0\\,\\mathrm{A\\,m^{-2}}$, $F = 96485\\,\\mathrm{C\\,mol^{-1}}$. Assume $t_{+}^{0}$ and any kinetic symmetry factors are moderate and do not change the leading-order scaling of the identified groups. Round your final numerical answer for $\\Xi$ to three significant figures. Express the final result as a dimensionless number.",
            "solution": "The problem statement will first be validated for scientific and formal correctness.\n\n### Step 1: Extract Givens\nThe governing equations for a pseudo-two-dimensional (P2D) battery model are provided:\n- Electrolyte mass balance: $\\varepsilon_{e}\\,\\dfrac{\\partial c_{e}}{\\partial t} = \\dfrac{\\partial}{\\partial x}\\!\\left(D_{e}\\,\\dfrac{\\partial c_{e}}{\\partial x}\\right) + \\left(1-t_{+}^{0}\\right)\\,\\dfrac{a_{s}\\,j}{F}$\n- Solid particle diffusion: $\\dfrac{\\partial c_{s}}{\\partial t} = \\dfrac{D_{s}}{r^{2}}\\,\\dfrac{\\partial}{\\partial r}\\!\\left(r^{2}\\,\\dfrac{\\partial c_{s}}{\\partial r}\\right)$\n- Solid particle boundary condition: $-D_{s}\\,\\left.\\dfrac{\\partial c_{s}}{\\partial r}\\right|_{r=R_{p}} = \\dfrac{j}{F}$ and symmetry at $r=0$.\n- Charge conservation in electrolyte: $\\dfrac{\\partial}{\\partial x}\\!\\left(\\kappa_{\\mathrm{eff}}\\,\\dfrac{\\partial \\phi_{e}}{\\partial x} + 2\\,\\dfrac{R\\,T}{F}\\,\\left(1-t_{+}^{0}\\right)\\,\\dfrac{\\partial \\ln c_{e}}{\\partial x}\\right) = a_{s}\\,j$\n- Charge conservation in solid: $\\dfrac{\\partial}{\\partial x}\\!\\left(\\sigma_{\\mathrm{eff}}\\,\\dfrac{\\partial \\phi_{s}}{\\partial x}\\right) = -\\,a_{s}\\,j$\n- Interfacial kinetics: $j = j_{\\mathrm{ref}}\\,\\mathcal{K}(c_{e},c_{s}^{\\mathrm{surf}},\\eta)$\n\nThe characteristic scales for nondimensionalization are given:\n- $x \\sim L$\n- $r \\sim R_{p}$\n- $t \\sim t_{D,e} = L^{2}/D_{e}$ for through-thickness processes\n- $t \\sim t_{D,s} = R_{p}^{2}/D_{s}$ for intra-particle processes\n- $c_{e} \\sim c_{e0}$\n- $c_{s} \\sim c_{s,\\max}$\n- $\\phi_{e}, \\phi_{s} \\sim R\\,T/F$\n- $j \\sim j_{\\mathrm{ref}}$\n\nNumerical parameter values are:\n- $L = 70\\times 10^{-6}\\,\\mathrm{m}$\n- $R_{p} = 5\\times 10^{-6}\\,\\mathrm{m}$\n- $a_{s} = 8.0\\times 10^{4}\\,\\mathrm{m^{-1}}$\n- $D_{e} = 1.5\\times 10^{-10}\\,\\mathrm{m^{2}\\,s^{-1}}$\n- $D_{s} = 5.0\\times 10^{-14}\\,\\mathrm{m^{2}\\,s^{-1}}$\n- $c_{e0} = 1000\\,\\mathrm{mol\\,m^{-3}}$\n- $c_{s,\\max} = 2.5\\times 10^{4}\\,\\mathrm{mol\\,m^{-3}}$\n- $j_{\\mathrm{ref}} = 3.0\\,\\mathrm{A\\,m^{-2}}$\n- $F = 96485\\,\\mathrm{C\\,mol^{-1}}$\n\nThe task is to derive and interpret three dimensionless groups ($\\mathrm{Da}_{e}$, $\\phi^{2}$, $\\Lambda$), explain their implications for numerical simulation, and calculate the stiffness indicator $\\Xi \\equiv \\max\\!\\left\\{\\mathrm{Da}_{e},\\,\\phi^{2}\\right\\}$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded. The equations provided are the standard, widely accepted Doyle-Fuller-Newman (or P2D) model, a cornerstone of battery simulation based on fundamental principles of transport phenomena and electrochemistry. The problem is well-posed, self-contained, and objective. It presents a standard analysis (nondimensionalization) of a well-established physical model. The parameters provided are scientifically plausible for a typical lithium-ion battery system. The questions asked are directly related to the numerical analysis and high-performance computing aspects of solving these equations, a critical topic in the field. There are no contradictions, ambiguities, or factual inaccuracies.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n### Nondimensionalization and Derivations\n\nWe define the following dimensionless variables, denoted with a bar:\n$\\bar{x} = \\dfrac{x}{L}$, $\\bar{r} = \\dfrac{r}{R_{p}}$, $\\bar{c}_{e} = \\dfrac{c_{e}}{c_{e0}}$, $\\bar{c}_{s} = \\dfrac{c_{s}}{c_{s,\\max}}$, $\\bar{\\phi}_{e} = \\dfrac{\\phi_{e}}{RT/F}$, $\\bar{\\phi}_{s} = \\dfrac{\\phi_{s}}{RT/F}$, $\\bar{j} = \\dfrac{j}{j_{\\mathrm{ref}}}$.\nFor time, we use the electrolyte diffusion time scale, $\\bar{t} = \\dfrac{t}{t_{D,e}} = \\dfrac{t D_{e}}{L^2}$.\n\nThe derivatives transform as:\n$\\dfrac{\\partial}{\\partial t} = \\dfrac{D_{e}}{L^{2}}\\dfrac{\\partial}{\\partial \\bar{t}}$, $\\dfrac{\\partial}{\\partial x} = \\dfrac{1}{L}\\dfrac{\\partial}{\\partial \\bar{x}}$, $\\dfrac{\\partial}{\\partial r} = \\dfrac{1}{R_{p}}\\dfrac{\\partial}{\\partial \\bar{r}}$.\n\n1.  **Electrolyte Mass Balance Analysis**\n    The original equation is:\n    $$ \\varepsilon_{e}\\,\\dfrac{\\partial c_{e}}{\\partial t} = \\dfrac{\\partial}{\\partial x}\\!\\left(D_{e}\\,\\dfrac{\\partial c_{e}}{\\partial x}\\right) + \\left(1-t_{+}^{0}\\right)\\,\\dfrac{a_{s}\\,j}{F} $$\n    Substituting the dimensionless variables:\n    $$ \\varepsilon_{e}\\,\\dfrac{c_{e0} D_{e}}{L^2}\\dfrac{\\partial \\bar{c}_{e}}{\\partial \\bar{t}} = \\dfrac{1}{L}\\dfrac{\\partial}{\\partial \\bar{x}}\\!\\left(D_{e}\\,\\dfrac{c_{e0}}{L}\\dfrac{\\partial \\bar{c}_{e}}{\\partial \\bar{x}}\\right) + \\left(1-t_{+}^{0}\\right)\\,\\dfrac{a_{s}\\,(j_{\\mathrm{ref}}\\bar{j})}{F} $$\n    Assuming $D_e$ is constant and dividing by the diffusion term's scale, $\\dfrac{D_{e}c_{e0}}{L^2}$:\n    $$ \\varepsilon_{e}\\,\\dfrac{\\partial \\bar{c}_{e}}{\\partial \\bar{t}} = \\dfrac{\\partial^2 \\bar{c}_{e}}{\\partial \\bar{x}^2} + \\left(1-t_{+}^{0}\\right)\\,\\left(\\dfrac{a_{s}\\,j_{\\mathrm{ref}}L^2}{F\\,c_{e0}\\,D_{e}}\\right)\\bar{j} $$\n    The dimensionless group multiplying the reaction term $\\bar{j}$ is the electrolyte Damköhler number, $\\mathrm{Da}_{e}$. It represents the characteristic rate of reaction relative to the characteristic rate of diffusion in the electrolyte.\n    $$ \\mathrm{Da}_{e} = \\dfrac{a_{s}\\,j_{\\mathrm{ref}}L^2}{F\\,c_{e0}\\,D_{e}} $$\n    Interpretation: $\\mathrm{Da}_{e}$ is the ratio of the characteristic timescale for electrolyte diffusion across the electrode ($L^2/D_e$) to the timescale for electrolyte consumption by the reaction. If $\\mathrm{Da}_{e} \\gg 1$, the reaction is much faster than diffusion, leading to sharp concentration gradients.\n\n2.  **Solid Particle Diffusion Analysis**\n    The solid diffusion equation uses its own internal time scale, $\\bar{t_s} = t/t_{D,s} = tD_s/R_p^2$. The boundary condition at the particle surface is key.\n    $$ -D_{s}\\,\\left.\\dfrac{\\partial c_{s}}{\\partial r}\\right|_{r=R_{p}} = \\dfrac{j}{F} $$\n    Substituting dimensionless variables:\n    $$ -D_{s}\\,\\dfrac{c_{s,\\max}}{R_{p}}\\left.\\dfrac{\\partial \\bar{c}_{s}}{\\partial \\bar{r}}\\right|_{\\bar{r}=1} = \\dfrac{j_{\\mathrm{ref}}\\bar{j}}{F} $$\n    Rearranging to make the derivative term dimensionless:\n    $$ -\\left.\\dfrac{\\partial \\bar{c}_{s}}{\\partial \\bar{r}}\\right|_{\\bar{r}=1} = \\left(\\dfrac{j_{\\mathrm{ref}}R_{p}}{F\\,D_{s}\\,c_{s,\\max}}\\right)\\bar{j} $$\n    The group in parentheses is the Thiele-type parameter, $\\phi^{2}$. It measures the ratio of the reaction rate at the particle surface to the characteristic rate of diffusion within the particle.\n    $$ \\phi^{2} = \\dfrac{j_{\\mathrm{ref}}R_{p}}{F\\,D_{s}\\,c_{s,\\max}} $$\n    Interpretation: $\\phi^2$ is the ratio of the surface reaction flux ($j/F$) to the maximum possible diffusive flux within the particle ($D_s c_{s,max}/R_p$). If $\\phi^{2} \\gg 1$, the reaction is fast compared to solid-state diffusion, and lithium will be non-uniformly distributed in the particle, with steep gradients near the surface.\n\n3.  **Charge Conservation Analysis**\n    We analyze the two charge conservation equations to find the conductivity ratio.\n    Solid phase: $\\dfrac{\\partial}{\\partial x}\\!\\left(\\sigma_{\\mathrm{eff}}\\,\\dfrac{\\partial \\phi_{s}}{\\partial x}\\right) = -\\,a_{s}\\,j$\n    Electrolyte phase: $\\dfrac{\\partial}{\\partial x}\\!\\left(\\kappa_{\\mathrm{eff}}\\,\\dfrac{\\partial \\phi_{e}}{\\partial x} + \\dots\\right) = a_{s}\\,j$\n    Nondimensionalizing the solid-phase equation:\n    $$ \\dfrac{1}{L}\\dfrac{\\partial}{\\partial \\bar{x}}\\!\\left(\\sigma_{\\mathrm{eff}}\\,\\dfrac{RT}{FL}\\dfrac{\\partial \\bar{\\phi}_{s}}{\\partial \\bar{x}}\\right) = -\\,a_{s}\\,(j_{\\mathrm{ref}}\\bar{j}) \\implies \\dfrac{\\partial^2 \\bar{\\phi}_{s}}{\\partial \\bar{x}^2} = -\\left(\\dfrac{a_s j_{\\mathrm{ref}} L^2 F}{\\sigma_{\\mathrm{eff}} RT}\\right)\\bar{j} $$\n    The Ohm's law portion of the electrolyte phase equation, $\\dfrac{\\partial}{\\partial x}(\\kappa_{\\mathrm{eff}}\\,\\dfrac{\\partial \\phi_e}{\\partial x})$, scales similarly, yielding a dimensionless group $\\dfrac{a_s j_{\\mathrm{ref}} L^2 F}{\\kappa_{\\mathrm{eff}} RT}$.\n    The problem asks for a conductivity ratio $\\Lambda$ comparing electronic to ionic pathways. A direct ratio of the conductivities themselves is the most natural and meaningful definition.\n    $$ \\Lambda = \\dfrac{\\sigma_{\\mathrm{eff}}}{\\kappa_{\\mathrm{eff}}} $$\n    Interpretation: $\\Lambda$ compares the ease of electronic charge transport through the solid matrix to ionic charge transport through the electrolyte. If $\\Lambda \\gg 1$, the solid phase is much more conductive, and significant potential drops (and thus ohmic losses) will occur primarily in the electrolyte. If $\\Lambda \\ll 1$, the reverse is true. This balance governs the distribution of reaction current across the electrode thickness.\n\n### HPC Implications\nThe magnitudes of $\\mathrm{Da}_{e}$ and $\\phi^{2}$ are critical indicators of numerical stiffness. Stiffness arises in a system of differential equations when there is a wide separation of time scales between different processes.\n- A large $\\mathrm{Da}_{e}$ or $\\phi^2$ implies that the characteristic time for reaction is much shorter than the characteristic time for diffusion.\n- This creates a multi-scale temporal problem: the system has fast dynamics (reaction) and slow dynamics (diffusion). To capture the overall behavior over long times (e.g., a full charge/discharge cycle), the simulation is bottle-necked by the fast dynamics.\n- **Time Integrators**: Explicit time integration methods (e.g., Forward Euler, Runge-Kutta) have stability regions that require the time step size $\\Delta t$ to be on the order of the fastest time scale in the system. For a stiff problem, this results in a prohibitively large number of time steps. Therefore, large values of $\\mathrm{Da}_{e}$ or $\\phi^2$ mandate the use of implicit, A-stable methods (e.g., Backward Differentiation Formulas, BDF), which are stable for much larger time steps.\n- **Preconditioners**: A-stable implicit methods require the solution of a large, sparse, nonlinear system of algebraic equations at each time step, typically via a Newton-Raphson-type method. The core of this is solving a linear system $J\\delta x = -f$, where $J$ is the Jacobian matrix. For stiff problems, $J$ is often ill-conditioned, causing iterative linear solvers (like GMRES, common in HPC) to converge slowly or fail. An effective preconditioner $P$ is a matrix that approximates $J^{-1}$, transforming the system into $P J \\delta x = -P f$, which is better conditioned and easier to solve. The development of physics-based preconditioners, which exploit the known structure of the coupling in the P2D model, is crucial for achieving high performance in large-scale battery simulations.\n\n### Numerical Calculation of Stiffness Indicator\nWe now calculate $\\mathrm{Da}_{e}$ and $\\phi^{2}$ using the provided values to find $\\Xi = \\max\\!\\left\\{\\mathrm{Da}_{e},\\,\\phi^{2}\\right\\}$.\n\n-   Calculation of $\\mathrm{Da}_{e}$:\n    $$ \\mathrm{Da}_{e} = \\dfrac{a_{s}\\,j_{\\mathrm{ref}}L^2}{F\\,c_{e0}\\,D_{e}} = \\dfrac{(8.0\\times 10^{4}\\,\\mathrm{m^{-1}})(3.0\\,\\mathrm{A\\,m^{-2}})(70\\times 10^{-6}\\,\\mathrm{m})^2}{(96485\\,\\mathrm{C\\,mol^{-1}})(1000\\,\\mathrm{mol\\,m^{-3}})(1.5\\times 10^{-10}\\,\\mathrm{m^{2}\\,s^{-1}})} $$\n    $$ \\mathrm{Da}_{e} = \\dfrac{(8.0 \\times 10^4)(3.0)(4.9 \\times 10^{-9})}{96485 \\times 10^3 \\times 1.5 \\times 10^{-10}} = \\dfrac{1.176 \\times 10^{-3}}{1.447275 \\times 10^{-2}} \\approx 0.081256 $$\n\n-   Calculation of $\\phi^{2}$:\n    $$ \\phi^{2} = \\dfrac{j_{\\mathrm{ref}}R_{p}}{F\\,D_{s}\\,c_{s,\\max}} = \\dfrac{(3.0\\,\\mathrm{A\\,m^{-2}})(5\\times 10^{-6}\\,\\mathrm{m})}{(96485\\,\\mathrm{C\\,mol^{-1}})(5.0\\times 10^{-14}\\,\\mathrm{m^{2}\\,s^{-1}})(2.5\\times 10^{4}\\,\\mathrm{mol\\,m^{-3}})} $$\n    $$ \\phi^{2} = \\dfrac{1.5 \\times 10^{-5}}{96485 \\times (1.25 \\times 10^{-9})} = \\dfrac{1.5 \\times 10^{-5}}{1.2060625 \\times 10^{-4}} \\approx 0.12437 $$\n\n-   Evaluation of $\\Xi$:\n    $$ \\Xi = \\max\\!\\left\\{0.081256, 0.12437\\right\\} = 0.12437 $$\n    Rounding to three significant figures, the result is $0.124$.",
            "answer": "$$\\boxed{0.124}$$"
        },
        {
            "introduction": "The performance of a large-scale simulation is often limited by its most frequently executed computational kernels, such as the assembly of matrix operators. The Roofline model is an insightful visual tool for diagnosing whether a kernel's performance is limited by the processor's computational speed or the memory system's bandwidth. In this practice, you will calculate the operational intensity for a finite volume diffusion kernel and use it to predict performance bottlenecks, a crucial analysis for guiding optimization efforts and making efficient use of HPC resources .",
            "id": "3918481",
            "problem": "A three-dimensional lithium-ion battery electrolyte diffusion problem is discretized on a uniform Cartesian grid using a cell-centered finite volume method based on Fick’s law, which states that the molar flux density is proportional to the negative gradient of concentration. The diffusion operator is assembled into a sparse, symmetric positive-definite matrix by evaluating face-centered diffusive fluxes between each cell and its six axial neighbors in the $x$, $y$, and $z$ directions. For each interior cell, the assembly kernel performs the following operations per face: computes the harmonic average of the two neighboring cell diffusion coefficients using the formula $H = \\frac{2 D_{i} D_{j}}{D_{i} + D_{j}}$, multiplies $H$ by a precomputed geometric factor $f = \\frac{A}{d}$ (where $A$ is the face area and $d$ is the cell-center distance across the face), and updates the two affected sparse-matrix entries, adding $a = H f$ to the diagonal of the current cell and subtracting $a$ from the off-diagonal entry coupling the current cell to its neighbor. Assume double-precision floating-point arithmetic, where each floating-point number occupies $8$ bytes, and $32$-bit integers for index arrays, where each integer occupies $4$ bytes.\n\nAssume the following per-face arithmetic and memory behavior for the kernel:\n- Arithmetic per face: $H$ computed with $2$ multiplications, $1$ addition, and $1$ division; $a$ computed with $1$ multiplication; the two sparse-matrix entries updated with $2$ additions. Thus, there are $7$ floating-point operations per face and $6$ faces per cell for a total of $42$ floating-point operations per cell.\n- Memory per cell: read the current-cell diffusion coefficient $D_{i}$ once ($8$ bytes), read $6$ neighbor diffusion coefficients $D_{j}$ ($6 \\times 8$ bytes), read $6$ precomputed geometric factors $f$ ($6 \\times 8$ bytes), perform $12$ sparse-matrix updates each consisting of a read-modify-write of a double value ($12 \\times (8 + 8)$ bytes), and read $12$ integer offsets mapping local updates to global matrix positions ($12 \\times 4$ bytes). Thus, the total data movement per cell is $344$ bytes.\n\nThe target architecture is a single socket of a modern High-Performance Computing (HPC) node characterized by a peak double-precision throughput of $3\\,\\mathrm{TFLOP/s}$ and a sustained main-memory bandwidth of $100\\,\\mathrm{GB/s}$.\n\nUsing only general definitions of operational intensity as floating-point operations per byte transferred and the notion that a roofline performance bound is governed by the lesser of peak floating-point throughput and bandwidth-limited throughput, perform the following:\n- Derive the operational intensity of the described assembly kernel from the stated arithmetic and data-movement characteristics. Express the final operational intensity in floating-point operations per byte.\n- Explain where the kernel lies on a roofline plot for the given architecture and justify whether it is memory-bound or compute-bound.\n- Based on Amdahl’s Law and Gustafson’s Law, reason about the parallel scaling implications of optimizing this kernel and propose at least one scientifically sound optimization that changes the arithmetic-to-memory ratio to move the kernel toward the compute-bound regime.\n\nRound your operational intensity to four significant figures. Express the operational intensity in floating-point operations per byte.",
            "solution": "The problem statement is evaluated to be valid. It is scientifically grounded in the principles of numerical methods for partial differential equations and high-performance computing performance analysis. It is well-posed, objective, and provides a self-contained, consistent set of data and definitions required to derive the requested quantities and perform the analysis. There are no identifiable flaws such as scientific unsoundness, incompleteness, or ambiguity. Therefore, a full solution is presented.\n\nThe solution is presented in three parts as requested by the problem statement.\n\nFirst, we derive the operational intensity of the kernel. The operational intensity, denoted by $I$, is defined as the ratio of total floating-point operations (FLOPs) performed to the total bytes of data moved between the processor and main memory.\n\nThe problem provides the following values on a per-cell basis:\n- Total floating-point operations per cell: $42$ FLOPs.\n- Total data movement per cell: $344$ bytes.\n\nUsing the definition of operational intensity, we can calculate its value for this kernel:\n$$\nI = \\frac{\\text{Total Floating-Point Operations}}{\\text{Total Data Movement}} = \\frac{42 \\text{ FLOPs}}{344 \\text{ Bytes}}\n$$\nEvaluating this expression and rounding to four significant figures:\n$$\nI \\approx 0.122093... \\text{ FLOPs/Byte} \\approx 0.1221 \\text{ FLOPs/Byte}\n$$\n\nSecond, we analyze where the kernel lies on a roofline plot for the given architecture and determine if it is memory-bound or compute-bound. The roofline model establishes an upper bound on performance, $\\Pi$, given by the minimum of the architecture's peak floating-point throughput, $\\Pi_{\\text{peak}}$, and the performance limited by memory bandwidth, $\\beta$. The formula is:\n$$\n\\Pi \\le \\min(\\Pi_{\\text{peak}}, I \\cdot \\beta)\n$$\nThe problem specifies the architectural characteristics:\n- Peak double-precision throughput, $\\Pi_{\\text{peak}} = 3\\,\\mathrm{TFLOP/s} = 3 \\times 10^{12}$ FLOPs/second.\n- Sustained main-memory bandwidth, $\\beta = 100\\,\\mathrm{GB/s} = 1 \\times 10^{11}$ Bytes/second.\n\nThe boundary between the memory-bound and compute-bound regimes is determined by the machine balance, also known as the ridge point, $I_{\\text{ridge}}$. Kernels with an operational intensity lower than the ridge point are memory-bound, while those with a higher intensity are compute-bound. The ridge point is calculated as:\n$$\nI_{\\text{ridge}} = \\frac{\\Pi_{\\text{peak}}}{\\beta} = \\frac{3 \\times 10^{12} \\text{ FLOPs/s}}{1 \\times 10^{11} \\text{ Bytes/s}} = 30 \\text{ FLOPs/Byte}\n$$\nComparing the kernel's operational intensity to the machine balance:\n$$\nI \\approx 0.1221 \\text{ FLOPs/Byte} \\ll I_{\\text{ridge}} = 30 \\text{ FLOPs/Byte}\n$$\nSince the kernel's operational intensity is significantly lower than the machine's ridge point, the kernel is heavily **memory-bound**. This means that the execution speed is limited by the rate at which data can be fetched from main memory, not by the processor's ability to perform computations. On a roofline plot (log-log scale of Performance vs. Operational Intensity), this kernel lies on the sloped part of the roof, far to the left of the ridge point. Its attainable performance is dictated by the memory bandwidth:\n$$\n\\Pi_{\\text{attainable}} = I \\cdot \\beta \\approx 0.1221 \\frac{\\text{FLOPs}}{\\text{Byte}} \\times 1 \\times 10^{11} \\frac{\\text{Bytes}}{\\text{s}} \\approx 1.221 \\times 10^{10} \\text{ FLOPs/s} = 0.01221 \\text{ TFLOP/s}\n$$\nThis attainable performance is only about $0.41\\%$ of the machine's peak capability of $3\\,\\mathrm{TFLOP/s}$, which further highlights the severity of the memory bottleneck.\n\nThird, we reason about the parallel scaling implications and propose an optimization.\n\nThe memory-bound nature of the kernel has significant implications for parallel scaling.\n- According to **Amdahl’s Law**, which typically describes strong scaling (fixed problem size), the overall speedup is limited by the sequential portion of the code. In a shared-memory context, the memory bus is a shared resource. As more processing cores are used, they contend for this limited memory bandwidth. This contention acts as a bottleneck, analogous to a sequential part of the program, thus limiting the achievable speedup. The performance of all cores combined cannot exceed the bandwidth limit of $I \\cdot \\beta \\approx 0.01221\\,\\mathrm{TFLOP/s}$.\n- According to **Gustafson’s Law**, which describes weak scaling (problem size scales with the number of processors), performance can fare better. In a well-designed parallel implementation with domain decomposition, as the problem size and number of cores increase, most memory accesses become local to a core's cache or a processor's NUMA (Non-Uniform Memory Access) node. This reduces contention on the main memory bus or interconnects, allowing for better scalability. However, the fundamental low operational intensity remains a performance limiter within each subdomain.\n\nTo improve performance and parallel scalability, the operational intensity $I$ must be increased. This requires modifying the kernel to increase the ratio of arithmetic operations to memory accesses. A scientifically sound optimization is to **recompute the geometric factor $f$ on the fly** instead of reading it from memory. This is a classic space-time tradeoff.\n\n**Analysis of the proposed optimization:**\n- **Current state:** The kernel reads $6$ precomputed geometric factors per cell, contributing $6 \\times 8 \\text{ bytes} = 48$ bytes to the total memory traffic.\n- **Optimization:** We eliminate the storage and reading of $f$. Instead, for each face, we compute $f = A/d$, where $A$ is the face area and $d$ is the cell-center distance. This adds $1$ floating-point division per face.\n- **Impact on Arithmetic:** The FLOPs per face increase from $7$ to $7+1=8$. The total FLOPs per cell become $6 \\text{ faces} \\times 8 \\text{ FLOPs/face} = 48$ FLOPs.\n- **Impact on Memory:** The memory traffic per cell is reduced by the $48$ bytes previously used for reading $f$. The new total data movement is $344 - 48 = 296$ bytes.\n- **New Operational Intensity:** The new operational intensity, $I'$, becomes:\n$$\nI' = \\frac{48 \\text{ FLOPs}}{296 \\text{ Bytes}} \\approx 0.1622 \\text{ FLOPs/Byte}\n$$\nThis optimization increases the operational intensity by approximately $33\\%$ (from $0.1221$ to $0.1622$). While the kernel remains firmly in the memory-bound regime ($0.1622 \\ll 30$), this change shifts it to the right on the roofline plot. This increases its attainable performance ceiling to $I' \\cdot \\beta \\approx 0.01622\\,\\mathrm{TFLOP/s}$ and makes the kernel more arithmetically intensive, thereby improving its efficiency and potential for parallel scaling.",
            "answer": "$$\n\\boxed{0.1221}\n$$"
        },
        {
            "introduction": "On modern multi-node supercomputers, the time spent communicating data between nodes can easily dominate the time spent on actual computation. Effective parallel algorithm design focuses on hiding this communication latency by overlapping it with useful work, a technique known as communication-computation overlap. This final exercise asks you to model and quantify the benefits of such an overlapped schedule for a parallel Newton-Krylov solver, providing a quantitative understanding of how task-level parallelism is essential for achieving high efficiency and scalability in large-scale simulations .",
            "id": "3918527",
            "problem": "Design and implement a complete, runnable program that models and evaluates a scalable nonlinear solve workflow for a large-scale battery simulation, focusing on overlapping tasks to maximize parallel efficiency on multi-node clusters. The workflow uses Newton–Krylov (NK) methodology, where Newton's method uses Generalized Minimal Residual (GMRES) for solving linearized subproblems with a Jacobian-free approach, and block-preconditioning. The model must be framed as follows.\n\nStart from the following fundamental base:\n\n- Newton's method: given a nonlinear residual mapping $F(u): \\mathbb{R}^N \\to \\mathbb{R}^N$, the iterative update is $u_{k+1} = u_k + s_k$, where $s_k$ solves $J(u_k) s_k = -F(u_k)$ with $J(u_k)$ the Jacobian.\n- Jacobian-free Newton–Krylov (JFNK): the linear solve uses Generalized Minimal Residual (GMRES) without forming $J(u_k)$ explicitly, evaluating Jacobian-vector products as $J(u_k) v \\approx \\frac{F(u_k + \\epsilon v) - F(u_k)}{\\epsilon}$ for small $\\epsilon$.\n- Overlapped scheduling principle: if tasks $A$, $B$, and $C$ are independent within a stage, the stage wall-clock time equals $\\max(T_A, T_B, T_C)$, while a sequential schedule has $T_A + T_B + T_C$.\n- Latency–bandwidth communication model: the time to send a message of size $m$ bytes to one neighbor is $T_{\\text{msg}} = \\alpha + \\frac{m}{B}$, where $\\alpha$ is the latency and $B$ is the bandwidth. For $n_{\\text{nb}}$ neighbors, $T_{\\text{comm}} = n_{\\text{nb}} \\left(\\alpha + \\frac{m}{B}\\right)$.\n- Parallel decomposition: for $P$ nodes with a perfectly balanced one-dimensional domain decomposition, each node processes $N/P$ unknowns. Let the per-node effective floating-point throughput be $f_{\\text{cpu}} \\cdot e$, where $f_{\\text{cpu}}$ is the per-node peak in floating-point operations per second (FLOPS) and $e \\in (0,1]$ is the efficiency.\n- Per-cell work model: residual evaluation costs $w_{\\text{res}}$ floating-point operations (flops) per cell; Jacobian-vector product costs $w_{Jv}$ flops per cell; preconditioner setup costs $w_{\\text{ps}}$ flops per cell; preconditioner apply costs $w_{\\text{pa}}$ flops per cell; and the Newton update assembly costs $w_{\\text{upd}}$ flops per cell.\n\nAssume the following scheduling model for one Newton iteration:\n\n- Sequential schedule per Newton iteration:\n  $$T_{\\text{seq,Newton}} = T_{\\text{res}} + T_{\\text{comm}} + T_{\\text{ps}} + k \\left(T_{Jv} + T_{\\text{comm}} + T_{\\text{pa}}\\right) + T_{\\text{upd}},$$\n  where $k$ is the number of GMRES iterations.\n- Overlapped schedule per Newton iteration (residual evaluation, preconditioner setup, and communication overlapping; and, per GMRES iteration, overlap of Jacobian-vector product with communication):\n  $$T_{\\text{ov,Newton}} = \\max\\left(T_{\\text{res}}, T_{\\text{comm}}, T_{\\text{ps}}\\right) + k \\left(T_{\\text{pa}} + \\max\\left(T_{Jv}, T_{\\text{comm}}\\right)\\right) + T_{\\text{upd}}.$$\n\nFor $n_N$ Newton iterations, total times are $T_{\\text{seq,total}} = n_N \\cdot T_{\\text{seq,Newton}}$ and $T_{\\text{ov,total}} = n_N \\cdot T_{\\text{ov,Newton}}$. Compute the speed-up factor\n$$S = \\frac{T_{\\text{seq,total}}}{T_{\\text{ov,total}}}.$$\n\nAll time quantities must be expressed in seconds. The per-node times should be derived as:\n\n- Per-node compute rate: $r = f_{\\text{cpu}} \\cdot e$.\n- Local unknowns per node: $n_{\\text{local}} = N / P$.\n- Per-node compute times:\n  $$T_{\\text{res}} = \\frac{n_{\\text{local}} \\cdot w_{\\text{res}}}{r}, \\quad T_{Jv} = \\frac{n_{\\text{local}} \\cdot w_{Jv}}{r}, \\quad T_{\\text{ps}} = \\frac{n_{\\text{local}} \\cdot w_{\\text{ps}}}{r}, \\quad T_{\\text{pa}} = \\frac{n_{\\text{local}} \\cdot w_{\\text{pa}}}{r}, \\quad T_{\\text{upd}} = \\frac{n_{\\text{local}} \\cdot w_{\\text{upd}}}{r}.$$\n- Communication per exchange (one halo exchange per residual or per Jacobian-vector product): with $g$ ghost cells per boundary, message size per neighbor $m = g \\cdot 8$ bytes (assuming $64$-bit floating-point values). For $n_{\\text{nb}}$ neighbors,\n  $$T_{\\text{comm}} = n_{\\text{nb}} \\left(\\alpha + \\frac{m}{B}\\right).$$\n\nImplement a program that, for each provided test case, computes $S$ using the above model. Use double-precision floating-point arithmetic.\n\nYou are given the following test suite of $5$ parameter sets. For each case, all times must be computed in seconds, and the final outputs must be dimensionless floating-point speed-up factors. Each case provides:\n- $N$: total number of unknowns,\n- $P$: number of nodes,\n- $g$: ghost cells per boundary,\n- $n_{\\text{nb}}$: number of neighbors,\n- $w_{\\text{res}}$, $w_{Jv}$, $w_{\\text{ps}}$, $w_{\\text{pa}}$, $w_{\\text{upd}}$ (flops per cell),\n- $f_{\\text{cpu}}$ (FLOPS per node),\n- $e$ (efficiency as a decimal),\n- $\\alpha$ (seconds),\n- $B$ (bytes per second),\n- $k$: GMRES iterations per Newton step,\n- $n_N$: number of Newton iterations.\n\nTest cases:\n\n- Case 1 (balanced compute, low latency):\n  - $N = 10000000$, $P = 32$, $g = 2$, $n_{\\text{nb}} = 2$\n  - $w_{\\text{res}} = 3000$, $w_{Jv} = 3000$, $w_{\\text{ps}} = 20000$, $w_{\\text{pa}} = 2000$, $w_{\\text{upd}} = 500$\n  - $f_{\\text{cpu}} = 2.0 \\times 10^{11}$, $e = 0.7$\n  - $\\alpha = 2.0 \\times 10^{-6}$, $B = 2.5 \\times 10^{10}$\n  - $k = 40$, $n_N = 5$\n\n- Case 2 (high latency, lower bandwidth):\n  - $N = 10000000$, $P = 32$, $g = 2$, $n_{\\text{nb}} = 2$\n  - $w_{\\text{res}} = 3000$, $w_{Jv} = 3000$, $w_{\\text{ps}} = 20000$, $w_{\\text{pa}} = 2000$, $w_{\\text{upd}} = 500$\n  - $f_{\\text{cpu}} = 2.0 \\times 10^{11}$, $e = 0.7$\n  - $\\alpha = 1.0 \\times 10^{-3}$, $B = 5.0 \\times 10^{9}$\n  - $k = 40$, $n_N = 5$\n\n- Case 3 (tiny domain, many nodes, communication-dominated):\n  - $N = 32768$, $P = 64$, $g = 2$, $n_{\\text{nb}} = 2$\n  - $w_{\\text{res}} = 3000$, $w_{Jv} = 3000$, $w_{\\text{ps}} = 20000$, $w_{\\text{pa}} = 2000$, $w_{\\text{upd}} = 500$\n  - $f_{\\text{cpu}} = 2.0 \\times 10^{11}$, $e = 0.5$\n  - $\\alpha = 5.0 \\times 10^{-4}$, $B = 1.0 \\times 10^{10}$\n  - $k = 30$, $n_N = 4$\n\n- Case 4 (single node, no communication):\n  - $N = 10000000$, $P = 1$, $g = 2$, $n_{\\text{nb}} = 0$\n  - $w_{\\text{res}} = 3000$, $w_{Jv} = 3000$, $w_{\\text{ps}} = 20000$, $w_{\\text{pa}} = 2000$, $w_{\\text{upd}} = 500$\n  - $f_{\\text{cpu}} = 2.0 \\times 10^{11}$, $e = 0.7$\n  - $\\alpha = 2.0 \\times 10^{-6}$, $B = 2.5 \\times 10^{10}$\n  - $k = 40$, $n_N = 5$\n\n- Case 5 (heavy preconditioner, poor Krylov convergence):\n  - $N = 10000000$, $P = 16$, $g = 2$, $n_{\\text{nb}} = 2$\n  - $w_{\\text{res}} = 4000$, $w_{Jv} = 4000$, $w_{\\text{ps}} = 100000$, $w_{\\text{pa}} = 10000$, $w_{\\text{upd}} = 500$\n  - $f_{\\text{cpu}} = 2.0 \\times 10^{11}$, $e = 0.8$\n  - $\\alpha = 2.0 \\times 10^{-6}$, $B = 2.5 \\times 10^{10}$\n  - $k = 200$, $n_N = 6$\n\nYour program should produce a single line of output containing the speed-up factors $S$ for all five test cases as a comma-separated list enclosed in square brackets (for example, $[s_1,s_2,s_3,s_4,s_5]$). Express each speed-up as a floating-point number. No other output is permitted.",
            "solution": "The objective is to formulate and implement a performance model for a parallel nonlinear solver workflow pertinent to large-scale battery simulations. The model evaluates the speed-up, $S$, achieved by overlapping computation and communication tasks compared to a purely sequential execution schedule. The underlying numerical method is the Jacobian-Free Newton–Krylov (JFNK) algorithm with block preconditioning.\n\nThe model is constructed upon fundamental principles of parallel computing and numerical analysis. We begin by defining the system and parameters. The total problem size is $N$ unknowns, distributed across $P$ nodes in a perfectly balanced one-dimensional domain decomposition. Each node is thus responsible for a local problem of size $n_{\\text{local}} = N/P$.\n\nThe computational performance of a single node is characterized by its effective floating-point throughput, $r$, defined as the product of its peak theoretical throughput, $f_{\\text{cpu}}$, and its sustained efficiency, $e$:\n$$r = f_{\\text{cpu}} \\cdot e$$\nThe units of $r$ are floating-point operations per second (FLOPS).\n\nThe computational cost of different stages of the algorithm is modeled on a per-cell (or per-unknown) basis. The work, in flops per cell, is given for five distinct stages: residual evaluation ($w_{\\text{res}}$), Jacobian-vector product ($w_{Jv}$), preconditioner setup ($w_{\\text{ps}}$), preconditioner application ($w_{\\text{pa}}$), and the final solution update ($w_{\\text{upd}}$). The time, in seconds, for each stage on a single node is the total number of flops for that stage divided by the effective throughput, $r$:\n$$T_{\\text{res}} = \\frac{n_{\\text{local}} \\cdot w_{\\text{res}}}{r}$$\n$$T_{Jv} = \\frac{n_{\\text{local}} \\cdot w_{Jv}}{r}$$\n$$T_{\\text{ps}} = \\frac{n_{\\text{local}} \\cdot w_{\\text{ps}}}{r}$$\n$$T_{\\text{pa}} = \\frac{n_{\\text{local}} \\cdot w_{\\text{pa}}}{r}$$\n$$T_{\\text{upd}} = \\frac{n_{\\text{local}} \\cdot w_{\\text{upd}}}{r}$$\n\nCommunication between nodes is necessary to update halo or ghost regions of the domain. For a one-dimensional decomposition, each interior node communicates with $n_{\\text{nb}}$ neighbors (typically $n_{\\text{nb}}=2$). The problem specifies that each such exchange involves a message of size $m$, determined by the number of ghost cells, $g$, and the size of a double-precision floating-point number ($8$ bytes): $m = g \\cdot 8$. The time required for this communication is modeled using a standard latency-bandwidth model, where $\\alpha$ is the network latency and $B$ is the bandwidth:\n$$T_{\\text{comm}} = n_{\\text{nb}} \\left(\\alpha + \\frac{m}{B}\\right)$$\nThis time represents a single halo-exchange step, which is required for the residual evaluation and for each Jacobian-vector product within the Krylov solver.\n\nWith these component times defined, we can assemble the total wall-clock time for a single Newton iteration under two different scheduling policies. A Newton iteration consists of an initial setup phase, followed by $k$ iterations of the GMRES linear solver, and a final update.\n\nFirst, the sequential schedule, $T_{\\text{seq,Newton}}$, assumes each task is executed in series without any overlap. One halo exchange is needed for the initial residual calculation. Within the GMRES loop, each of the $k$ iterations requires a Jacobian-vector product (with its own halo exchange) and a preconditioner application.\n$$T_{\\text{seq,Newton}} = T_{\\text{res}} + T_{\\text{comm}} + T_{\\text{ps}} + k \\left(T_{Jv} + T_{\\text{comm}} + T_{\\text{pa}}\\right) + T_{\\text{upd}}$$\n\nSecond, the overlapped schedule, $T_{\\text{ov,Newton}}$, exploits task-level parallelism. In the setup phase, the residual evaluation ($T_{\\text{res}}$), its communication ($T_{\\text{comm}}$), and the preconditioner setup ($T_{\\text{ps}}$) are assumed to be independent and can be overlapped. The time for this phase is the maximum of the three. Within each of the $k$ GMRES iterations, the Jacobian-vector product ($T_{Jv}$) and its associated communication ($T_{\\text{comm}}$) can also be overlapped. The preconditioner application ($T_{\\text{pa}}$) is assumed to be sequential with this overlapped block. The final update ($T_{\\text{upd}}$) remains sequential.\n$$T_{\\text{ov,Newton}} = \\max\\left(T_{\\text{res}}, T_{\\text{comm}}, T_{\\text{ps}}\\right) + k \\left(T_{\\text{pa}} + \\max\\left(T_{Jv}, T_{\\text{comm}}\\right)\\right) + T_{\\text{upd}}$$\n\nThe total simulation time is the time per Newton step multiplied by the number of Newton iterations, $n_N$. The speed-up factor, $S$, is the ratio of the total sequential time to the total overlapped time.\n$$S = \\frac{T_{\\text{seq,total}}}{T_{\\text{ov,total}}} = \\frac{n_N \\cdot T_{\\text{seq,Newton}}}{n_N \\cdot T_{\\text{ov,Newton}}} = \\frac{T_{\\text{seq,Newton}}}{T_{\\text{ov,Newton}}}$$\nThe factor $n_N$ cancels, so the speed-up can be computed directly from the per-iteration times. The following program implements this model to calculate $S$ for each of the specified test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Calculates the speed-up factor for a set of test cases based on a performance\n    model of a parallel Newton-Krylov solver.\n    \"\"\"\n    \n    # Each dictionary in the list represents a distinct test case with all required parameters.\n    test_cases = [\n        # Case 1: balanced compute, low latency\n        {\n            \"N\": 10000000, \"P\": 32, \"g\": 2, \"n_nb\": 2,\n            \"w_res\": 3000, \"w_Jv\": 3000, \"w_ps\": 20000, \"w_pa\": 2000, \"w_upd\": 500,\n            \"f_cpu\": 2.0e11, \"e\": 0.7,\n            \"alpha\": 2.0e-6, \"B\": 2.5e10,\n            \"k\": 40, \"n_N\": 5\n        },\n        # Case 2: high latency, lower bandwidth\n        {\n            \"N\": 10000000, \"P\": 32, \"g\": 2, \"n_nb\": 2,\n            \"w_res\": 3000, \"w_Jv\": 3000, \"w_ps\": 20000, \"w_pa\": 2000, \"w_upd\": 500,\n            \"f_cpu\": 2.0e11, \"e\": 0.7,\n            \"alpha\": 1.0e-3, \"B\": 5.0e9,\n            \"k\": 40, \"n_N\": 5\n        },\n        # Case 3: tiny domain, many nodes, communication-dominated\n        {\n            \"N\": 32768, \"P\": 64, \"g\": 2, \"n_nb\": 2,\n            \"w_res\": 3000, \"w_Jv\": 3000, \"w_ps\": 20000, \"w_pa\": 2000, \"w_upd\": 500,\n            \"f_cpu\": 2.0e11, \"e\": 0.5,\n            \"alpha\": 5.0e-4, \"B\": 1.0e10,\n            \"k\": 30, \"n_N\": 4\n        },\n        # Case 4: single node, no communication\n        {\n            \"N\": 10000000, \"P\": 1, \"g\": 2, \"n_nb\": 0,\n            \"w_res\": 3000, \"w_Jv\": 3000, \"w_ps\": 20000, \"w_pa\": 2000, \"w_upd\": 500,\n            \"f_cpu\": 2.0e11, \"e\": 0.7,\n            \"alpha\": 2.0e-6, \"B\": 2.5e10,\n            \"k\": 40, \"n_N\": 5\n        },\n        # Case 5: heavy preconditioner, poor Krylov convergence\n        {\n            \"N\": 10000000, \"P\": 16, \"g\": 2, \"n_nb\": 2,\n            \"w_res\": 4000, \"w_Jv\": 4000, \"w_ps\": 100000, \"w_pa\": 10000, \"w_upd\": 500,\n            \"f_cpu\": 2.0e11, \"e\": 0.8,\n            \"alpha\": 2.0e-6, \"B\": 2.5e10,\n            \"k\": 200, \"n_N\": 6\n        },\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        # Unpack parameters from the case dictionary\n        N = case[\"N\"]\n        P = case[\"P\"]\n        g = case[\"g\"]\n        n_nb = case[\"n_nb\"]\n        w_res = case[\"w_res\"]\n        w_Jv = case[\"w_Jv\"]\n        w_ps = case[\"w_ps\"]\n        w_pa = case[\"w_pa\"]\n        w_upd = case[\"w_upd\"]\n        f_cpu = case[\"f_cpu\"]\n        e = case[\"e\"]\n        alpha = case[\"alpha\"]\n        B = case[\"B\"]\n        k = case[\"k\"]\n        # n_N is not needed as it cancels out in the speed-up ratio\n\n        # Step 1: Calculate intermediate values\n        r = f_cpu * e  # Effective FLOPS per node\n        n_local = N / P  # Local unknowns per node\n        m = g * 8.0  # Message size in bytes (8 bytes for a double)\n\n        # Step 2: Calculate time components\n        T_res = (n_local * w_res) / r\n        T_Jv = (n_local * w_Jv) / r\n        T_ps = (n_local * w_ps) / r\n        T_pa = (n_local * w_pa) / r\n        T_upd = (n_local * w_upd) / r\n        \n        # Communication time is zero if there are no neighbors\n        if n_nb == 0:\n            T_comm = 0.0\n        else:\n            T_comm = n_nb * (alpha + m / B)\n\n        # Step 3: Calculate total time per Newton iteration for both schedules\n        \n        # Sequential schedule time\n        T_seq_newton = T_res + T_comm + T_ps + k * (T_Jv + T_comm + T_pa) + T_upd\n        \n        # Overlapped schedule time\n        T_ov_newton = max(T_res, T_comm, T_ps) + k * (T_pa + max(T_Jv, T_comm)) + T_upd\n\n        # Step 4: Calculate speed-up\n        if T_ov_newton == 0:\n            # Avoid division by zero, although this case is unlikely with the given inputs\n            speedup = 1.0 \n        else:\n            speedup = T_seq_newton / T_ov_newton\n        \n        results.append(speedup)\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}