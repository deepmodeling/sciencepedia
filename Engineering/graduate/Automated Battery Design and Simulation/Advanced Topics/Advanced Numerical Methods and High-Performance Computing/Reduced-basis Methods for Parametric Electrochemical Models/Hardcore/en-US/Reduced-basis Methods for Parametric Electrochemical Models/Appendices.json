{
    "hands_on_practices": [
        {
            "introduction": "The fundamental trade-off in reduced-basis (RB) methods is between a computationally intensive 'offline' stage and a rapid 'online' stage. This exercise will guide you through deriving and quantifying these costs to understand the conditions under which RB methods provide a significant speedup. By implementing a cost model and calculating the break-even point for different scenarios, you will gain practical insight into designing efficient parametric simulation workflows. ",
            "id": "3945459",
            "problem": "Consider a parametric electrochemical model that, after spatial discretization, yields a family of symmetric positive definite linear systems of the form $A(\\mu) x(\\mu) = b(\\mu)$, where $A(\\mu) \\in \\mathbb{R}^{N \\times N}$ is an affine parameterization $A(\\mu) = \\sum_{q=1}^{Q} \\theta_q(\\mu) A_q$ with $Q$ parameter-independent matrices $A_q \\in \\mathbb{R}^{N \\times N}$ and scalar coefficient functions $\\theta_q(\\mu)$. A Reduced Basis (RB) approximation is constructed offline by generating $N_s$ full-order snapshots and then computing a Proper Orthogonal Decomposition (POD) basis via the Singular Value Decomposition (SVD) of the snapshot matrix. An affine reduced assembly is performed offline by projecting each $A_q$ onto the reduced space spanned by the RB basis $V \\in \\mathbb{R}^{N \\times N_r}$, producing reduced operators $A_q^r = V^\\top A_q V \\in \\mathbb{R}^{N_r \\times N_r}$. In the online stage, for a new parameter $\\mu$, the reduced operator $A^r(\\mu) = \\sum_{q=1}^{Q} \\theta_q(\\mu) A_q^r$ is assembled and solved for the reduced state $x^r(\\mu) \\in \\mathbb{R}^{N_r}$, and a linear functional of the state (for instance, a scalar output like terminal voltage) is evaluated from the reduced solution.\n\nStarting from fundamental linear algebra operation counts and standard computational facts, derive expressions for the offline computational cost to:\n- Generate $N_s$ full-order snapshots,\n- Construct the POD basis via an economy Singular Value Decomposition (SVD) of the $N \\times N_s$ snapshot matrix,\n- Precompute the affine reduced operators $A_q^r = V^\\top A_q V$ for all $q \\in \\{1,\\dots,Q\\}$,\n\nand derive expressions for the online computational cost per parameter query to:\n- Evaluate the parameter coefficients $\\theta_q(\\mu)$,\n- Assemble the reduced operator $A^r(\\mu) = \\sum_{q=1}^{Q} \\theta_q(\\mu) A_q^r$,\n- Solve the reduced system, and\n- Evaluate a linear functional of the reduced solution.\n\nUse only the following base facts in your derivation:\n- A dense matrix-matrix product of sizes $m \\times k$ and $k \\times n$ requires approximately $2 m k n$ floating-point operations (flops).\n- For an affine linear combination $A(\\mu) = \\sum_{q=1}^{Q} \\theta_q(\\mu) A_q$ with dense $A_q \\in \\mathbb{R}^{N \\times N}$, forming $A(\\mu)$ by direct scaling and summation costs $(Q$ multiplications $+ (Q-1)$ additions per entry$) \\times N^2 = (2Q-1) N^2$ flops.\n- For a symmetric positive definite matrix $M \\in \\mathbb{R}^{n \\times n}$, Cholesky factorization costs approximately $(1/3) n^3$ flops, and each triangular solve costs approximately $n^2$ flops. Solving $M y = f$ via Cholesky entails one factorization and two triangular solves, for a total of $(1/3) n^3 + 2 n^2$ flops.\n- For an economy Singular Value Decomposition (SVD) of a dense $N \\times N_s$ matrix with $N \\ge N_s$, a widely used flop model is approximately $4 N N_s^2 + 8 N_s^3$ flops.\n- Evaluating $Q$ scalar coefficient functions $\\theta_q(\\mu)$ has a cost that scales linearly in $Q$. Take a constant per-coefficient evaluation cost $C_\\theta = 1$ flop, so the total is $Q$ flops.\n- Evaluating a scalar linear functional $y = c^\\top x$ with $c \\in \\mathbb{R}^{n}$ costs approximately $2 n$ flops for the multiply-accumulate.\n\nAssume dense algebra throughout and that $A(\\mu)$ is symmetric positive definite for all $\\mu$ considered. Using the derived expressions, implement a program that, for each test case listed below, computes:\n- The total offline cost in flops,\n- The total online cost for $M_q$ queries in flops,\n- The total full-order (non-reduced) cost for $M_q$ queries in flops,\n- The per-query speedup ratio defined as $(\\text{full-order per-query flops}) / (\\text{online per-query flops})$,\n- The break-even number of queries $M_{\\text{break}} = \\left\\lceil \\frac{\\text{offline flops}}{\\text{full-order per-query flops} - \\text{online per-query flops}} \\right\\rceil$, with the convention that if the denominator is non-positive, report $-1$.\n\nAll outputs must be expressed in floating-point operations (flops) and must be numerically computed by your program. The test suite uses the following parameter sets $(N, N_s, N_r, Q, M_q)$:\n- $(2000, 50, 30, 5, 100)$\n- $(100, 10, 1, 1, 1)$\n- $(1500, 80, 40, 20, 250)$\n- $(3000, 300, 60, 10, 50)$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes its own list in the order: $[\\text{offline}, \\text{online\\_total}, \\text{full\\_total}, \\text{speedup}, \\text{break\\_even}]$. The final output format must be a single list of these lists with no spaces, for example $[[x_1,y_1,z_1,s_1,b_1],[x_2,y_2,z_2,s_2,b_2],\\dots]$. Angles are not involved. Express break-even as an integer. All other quantities may be floats. Units are flops.",
            "solution": "The problem statement is evaluated as valid. It is scientifically grounded in the principles of numerical linear algebra and model order reduction, specifically the Reduced Basis method. The problem is well-posed, objective, self-contained, and provides a clear, formalizable task.\n\nHerein, we derive the computational costs for the offline and online stages of the Reduced Basis (RB) method as described, based on the provided fundamental operation counts. The full-order model (FOM) is a parametric linear system $A(\\mu) x(\\mu) = b(\\mu)$, where $A(\\mu) = \\sum_{q=1}^{Q} \\theta_q(\\mu) A_q \\in \\mathbb{R}^{N \\times N}$ is symmetric positive definite (SPD). The reduced basis has dimension $N_r$, derived from $N_s$ snapshots.\n\n### Derivation of Computational Costs\n\nWe assume all matrix and vector operations involve dense algebra, as specified. All costs are measured in floating-point operations (flops).\n\n#### I. Offline Computational Cost ($C_{\\text{offline}}$)\n\nThe offline stage consists of three main tasks: snapshot generation, basis construction, and projection of operators.\n\n1.  **Generation of $N_s$ Full-Order Snapshots**:\n    For each of the $N_s$ training parameters, $\\mu_i$, a full-order solution $x(\\mu_i)$ must be computed. This involves assembling the matrix $A(\\mu_i)$ and solving the linear system $A(\\mu_i) x(\\mu_i) = b(\\mu_i)$.\n    *   Cost to assemble one matrix $A(\\mu_i) \\in \\mathbb{R}^{N \\times N}$: The affine assembly involves $Q$ scalings and $Q-1$ additions for each of the $N^2$ entries, given as $(2Q-1)N^2$ flops.\n    *   Cost to solve the $N \\times N$ SPD system using Cholesky factorization: This is given as $(1/3)N^3 + 2N^2$ flops.\n    *   The cost for a single snapshot is the sum of these two: $(2Q-1)N^2 + (1/3)N^3 + 2N^2 = (1/3)N^3 + (2Q+1)N^2$ flops.\n    *   Total cost for $N_s$ snapshots, $C_{\\text{snapshots}}$:\n        $$C_{\\text{snapshots}} = N_s \\left( \\frac{1}{3}N^3 + (2Q+1)N^2 \\right)$$\n\n2.  **Construction of the POD Basis**:\n    The basis is constructed via an economy Singular Value Decomposition (SVD) of the $N \\times N_s$ snapshot matrix. The cost for this operation, $C_{\\text{SVD}}$, is provided directly. Assuming $N \\geq N_s$:\n    $$C_{\\text{SVD}} = 4 N N_s^2 + 8 N_s^3$$\n    The resulting basis is denoted by the orthonormal matrix $V \\in \\mathbb{R}^{N \\times N_r}$, where $N_r$ is the number of retained singular modes.\n\n3.  **Precomputation of Affine Reduced Operators**:\n    Each parameter-independent matrix $A_q \\in \\mathbb{R}^{N \\times N}$ is projected onto the reduced basis to form $A_q^r = V^\\top A_q V \\in \\mathbb{R}^{N_r \\times N_r}$. This computation is done in two steps for efficiency:\n    *   First, compute the intermediate product $T_q = A_q V$. This is a product of an $N \\times N$ matrix and an $N \\times N_r$ matrix. The cost is $2 \\cdot N \\cdot N \\cdot N_r = 2N^2 N_r$ flops.\n    *   Second, compute the final product $A_q^r = V^\\top T_q$. This is a product of an $N_r \\times N$ matrix ($V^\\top$) and an $N \\times N_r$ matrix ($T_q$). The cost is $2 \\cdot N_r \\cdot N \\cdot N_r = 2N N_r^2$ flops.\n    *   The cost to form one reduced operator $A_q^r$ is $2N^2 N_r + 2N N_r^2$.\n    *   Total cost for all $Q$ operators, $C_{\\text{proj}}$:\n        $$C_{\\text{proj}} = Q \\left( 2N^2 N_r + 2N N_r^2 \\right)$$\n\nThe total offline cost, $C_{\\text{offline}}$, is the sum of these three components:\n$$C_{\\text{offline}} = C_{\\text{snapshots}} + C_{\\text{SVD}} + C_{\\text{proj}}$$\n$$C_{\\text{offline}} = N_s \\left( \\frac{1}{3}N^3 + (2Q+1)N^2 \\right) + \\left( 4 N N_s^2 + 8 N_s^3 \\right) + Q \\left( 2N^2 N_r + 2N N_r^2 \\right)$$\n\n#### II. Online Computational Cost ($C_{\\text{online, per-query}}$)\n\nThe online stage involves rapidly computing the solution for a new parameter $\\mu$.\n\n1.  **Evaluation of Parameter Coefficients**:\n    The $Q$ scalar functions $\\theta_q(\\mu)$ are evaluated. With a cost of $C_\\theta=1$ flop per function, the total cost $C_{\\text{coeffs}}$ is:\n    $$C_{\\text{coeffs}} = Q$$\n\n2.  **Assembly of the Reduced Operator**:\n    The reduced operator $A^r(\\mu) = \\sum_{q=1}^{Q} \\theta_q(\\mu) A_q^r$ is assembled. Since each $A_q^r$ is an $N_r \\times N_r$ matrix, the cost $C_{\\text{assemble_r}}$ is given by the affine combination rule for matrices of size $N_r \\times N_r$:\n    $$C_{\\text{assemble_r}} = (2Q-1)N_r^2$$\n\n3.  **Solution of the Reduced System**:\n    The $N_r \\times N_r$ reduced system $A^r(\\mu) x^r(\\mu) = b^r(\\mu)$ is solved. Since $A(\\mu)$ is SPD and $V$ has full column rank, $A^r(\\mu)=V^\\top A(\\mu) V$ is also SPD. Using the Cholesky method for an $N_r \\times N_r$ system, the cost $C_{\\text{solve_r}}$ is:\n    $$C_{\\text{solve_r}} = \\frac{1}{3}N_r^3 + 2N_r^2$$\n    (The cost of forming $b^r(\\mu)$ is assumed to be negligible, consistent with the problem's focus on system matrix operations).\n\n4.  **Evaluation of the Linear Functional**:\n    A linear functional of the reduced state $x^r(\\mu) \\in \\mathbb{R}^{N_r}$ is computed. For a dot product with a vector in $\\mathbb{R}^{N_r}$, the cost $C_{\\text{output}}$ is:\n    $$C_{\\text{output}} = 2N_r$$\n\nThe total online cost per query, $C_{\\text{online, per-query}}$, is the sum of these components:\n$$C_{\\text{online, per-query}} = C_{\\text{coeffs}} + C_{\\text{assemble_r}} + C_{\\text{solve_r}} + C_{\\text{output}}$$\n$$C_{\\text{online, per-query}} = Q + (2Q-1)N_r^2 + \\left( \\frac{1}{3}N_r^3 + 2N_r^2 \\right) + 2N_r = \\frac{1}{3}N_r^3 + (2Q+1)N_r^2 + 2N_r + Q$$\n\n#### III. Full-Order Model Cost ($C_{\\text{fom, per-query}}$)\n\nThe cost of solving the full-order model for a single query, without using model reduction, is:\n1.  **Assembly of $A(\\mu)$**: same as in the snapshot calculation, $(2Q-1)N^2$ flops.\n2.  **Solution of the FOM system**: same as in the snapshot calculation, $(1/3)N^3 + 2N^2$ flops.\n3.  **Evaluation of the Linear Functional**: the state $x(\\mu)$ is in $\\mathbb{R}^N$, so the cost is $2N$ flops.\n\nTotal FOM cost per query:\n$$C_{\\text{fom, per-query}} = ((2Q-1)N^2) + \\left( \\frac{1}{3}N^3 + 2N^2 \\right) + 2N = \\frac{1}{3}N^3 + (2Q+1)N^2 + 2N$$\n\n#### IV. Derived Quantities\n\nThe remaining quantities are calculated as follows:\n\n-   **Total Online Cost for $M_q$ queries**: $C_{\\text{online, total}} = M_q \\times C_{\\text{online, per-query}}$\n-   **Total Full-Order Cost for $M_q$ queries**: $C_{\\text{fom, total}} = M_q \\times C_{\\text{fom, per-query}}$\n-   **Per-Query Speedup Ratio ($S$)**: $S = \\frac{C_{\\text{fom, per-query}}}{C_{\\text{online, per-query}}}$\n-   **Break-Even Number of Queries ($M_{\\text{break}}$)**: This is the number of queries required for the total cost of the RB method to equal the total cost of the FOM.\n    $C_{\\text{offline}} + M_{\\text{break}} \\cdot C_{\\text{online, per-query}} = M_{\\text{break}} \\cdot C_{\\text{fom, per-query}}$\n    $$M_{\\text{break}} = \\left\\lceil \\frac{C_{\\text{offline}}}{C_{\\text{fom, per-query}} - C_{\\text{online, per-query}}} \\right\\rceil$$\n    If the denominator is non-positive, $M_{\\text{break}}$ is reported as $-1$.\n\nThese derived expressions will be implemented to compute the required values for the given test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes computational costs for a reduced basis method and compares them\n    to a full-order model based on provided cost formulas.\n    \"\"\"\n    # Test cases are defined as tuples: (N, N_s, N_r, Q, M_q)\n    test_cases = [\n        (2000, 50, 30, 5, 100),\n        (100, 10, 1, 1, 1),\n        (1500, 80, 40, 20, 250),\n        (3000, 300, 60, 10, 50),\n    ]\n\n    results_list = []\n\n    for case in test_cases:\n        N, Ns, Nr, Q, Mq = case\n\n        # Convert to float to ensure float arithmetic throughout\n        N, Ns, Nr, Q, Mq = float(N), float(Ns), float(Nr), float(Q), float(Mq)\n\n        # 1. Offline Cost Calculation\n        # Cost of generating Ns snapshots\n        cost_snapshots = Ns * (1/3 * N**3 + (2*Q + 1) * N**2)\n        # Cost of SVD for POD basis construction\n        cost_svd = 4 * N * Ns**2 + 8 * Ns**3\n        # Cost of projecting operators\n        cost_proj = Q * (2 * N**2 * Nr + 2 * N * Nr**2)\n        \n        offline_cost = cost_snapshots + cost_svd + cost_proj\n\n        # 2. Online Cost (per query) Calculation\n        online_per_query_cost = (1/3 * Nr**3 + (2*Q + 1) * Nr**2 + 2 * Nr + Q)\n        \n        # 3. Full-Order Model (FOM) Cost (per query) Calculation\n        fom_per_query_cost = (1/3 * N**3 + (2*Q + 1) * N**2 + 2 * N)\n\n        # 4. Total costs for Mq queries\n        online_total_cost = Mq * online_per_query_cost\n        fom_total_cost = Mq * fom_per_query_cost\n\n        # 5. Speedup Ratio\n        # Handle case where online cost could be zero, though unlikely with given formulas\n        if online_per_query_cost > 0:\n            speedup_ratio = fom_per_query_cost / online_per_query_cost\n        else:\n            speedup_ratio = float('inf') # Or handle as appropriate\n\n        # 6. Break-Even Number of Queries\n        cost_difference = fom_per_query_cost - online_per_query_cost\n        if cost_difference > 0:\n            # Ceiling of the division, converted to integer\n            break_even_queries = int(np.ceil(offline_cost / cost_difference))\n        else:\n            # If online is not faster, break-even is not achieved\n            break_even_queries = -1\n        \n        results_list.append([\n            offline_cost,\n            online_total_cost,\n            fom_total_cost,\n            speedup_ratio,\n            break_even_queries\n        ])\n\n    # Format the final output string exactly as required, with no spaces\n    inner_parts = []\n    for res in results_list:\n        # res[4] is break_even_queries, which is already an integer\n        res_str = f\"[{res[0]},{res[1]},{res[2]},{res[3]},{res[4]}]\"\n        inner_parts.append(res_str)\n    \n    final_output = f\"[{','.join(inner_parts)}]\"\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "The quality of a reduced-order model hinges on the basis used for projection, which should efficiently capture the system's behavior across the parameter space. This practice contrasts two primary strategies for basis construction: the data-centric Proper Orthogonal Decomposition (POD) and the error-centric Greedy algorithm. By implementing both methods and using a certified a posteriori error estimator to compare their performance, you will explore the trade-offs between optimality for training data and robustness for unseen parameters. ",
            "id": "3945577",
            "problem": "Consider a one-dimensional electrolyte concentration field $c_e(x;p)$ in a separator of length $L$, modeled for parameters $p = (D,k,j)$ by the stationary linear boundary value problem\n$$\n-\\frac{\\mathrm{d}}{\\mathrm{d}x}\\left(D\\,\\frac{\\mathrm{d} c_e}{\\mathrm{d}x}\\right) + k\\,c_e = s_j(x) \\quad \\text{for } x \\in (0,L), \\quad c_e(0;p) = 0, \\quad c_e(L;p) = 0,\n$$\nwhere $D$ is the electrolyte diffusivity, $k$ is a first-order reaction rate, and $s_j(x) = j \\sin\\left(\\frac{\\pi x}{L}\\right)$ is a volumetric source with amplitude $j$. Assume $D > 0$, $k > 0$, and $j > 0$. Use $L = 10^{-3}\\ \\text{m}$, with the domain discretized by $N$ internal nodes via a uniform mesh of spacing $\\Delta x = \\frac{L}{N+1}$, and central finite differences yielding a symmetric positive definite linear system\n$$\nA(p)\\,u(p) = f(p),\n$$\nwhere $u(p) \\in \\mathbb{R}^N$ represents the discrete concentration vector approximating $c_e(x;p)$ at the internal nodes, $A(p) = D\\,T + k\\,I$ with $T \\in \\mathbb{R}^{N \\times N}$ the standard tridiagonal stiffness matrix\n$$\nT = \\frac{1}{\\Delta x^2}\n\\begin{bmatrix}\n2 & -1 & 0 & \\cdots & 0 \\\\\n-1 & 2 & -1 & \\ddots & \\vdots \\\\\n0 & -1 & 2 & \\ddots & 0 \\\\\n\\vdots & \\ddots & \\ddots & \\ddots & -1 \\\\\n0 & \\cdots & 0 & -1 & 2\n\\end{bmatrix},\n$$\nand $f(p) = j\\,\\mathbf{s}$ with $\\mathbf{s}_i = \\sin\\left(\\frac{\\pi x_i}{L}\\right)$ and $x_i = i\\,\\Delta x$ for $i = 1,\\ldots,N$. This model admits an affine decomposition\n$$\nA(p) = \\theta_1(p)\\,A_1 + \\theta_2(p)\\,A_2, \\quad f(p) = \\theta_3(p)\\,f_1,\n$$\nwith $\\theta_1(p) = D$, $\\theta_2(p) = k$, $\\theta_3(p) = j$, $A_1 = T$, $A_2 = I$, and $f_1 = \\mathbf{s}$.\n\nLet the training parameter set $\\mathcal{P}_{\\mathrm{train}}$ be the Cartesian product\n$$\n\\mathcal{D} = \\{1.5\\times 10^{-10},\\, 3.0\\times 10^{-10},\\, 4.5\\times 10^{-10}\\}\\ \\text{m}^2/\\text{s}, \\quad\n\\mathcal{K} = \\{0.1,\\, 0.55,\\, 1.0\\}\\ \\text{s}^{-1}, \\quad\n\\mathcal{J} = \\{150,\\, 275,\\, 400\\}\\ \\text{mol}\\,(\\text{m}^{-3}\\,\\text{s}^{-1}),\n$$\nso that $\\mathcal{P}_{\\mathrm{train}} = \\mathcal{D} \\times \\mathcal{K} \\times \\mathcal{J}$. Using the full-order model above, compute snapshots $u(p)$ for all $p \\in \\mathcal{P}_{\\mathrm{train}}$.\n\nFrom these training snapshots, construct:\n- A Proper Orthogonal Decomposition (POD) basis of dimension $r_{\\mathrm{POD}}$ determined as the minimal $r$ such that the cumulative singular value energy of the snapshot matrix reaches or exceeds the threshold $\\tau = 0.999$.\n- A Greedy Reduced Basis (RB) of the same dimension $r_{\\mathrm{POD}}$ using the following certified a posteriori error metric for the $2$-norm:\n$$\n\\eta(p;u_r) = \\frac{\\|r(p;u_r)\\|_2}{\\lambda_{\\min}(A(p))}, \\quad r(p;u_r) = f(p) - A(p)\\,u_r,\n$$\nwhere $\\lambda_{\\min}(A(p))$ is the smallest eigenvalue of the symmetric positive definite matrix $A(p)$, and $u_r$ is the reduced-order approximation obtained by Galerkin projection onto the current RB space. Initialize the greedy algorithm with an empty basis. At each greedy iteration, over all $p \\in \\mathcal{P}_{\\mathrm{train}}$, compute $u_r(p)$ for the current basis, evaluate $\\eta(p;u_r)$, select $p^\\star$ with the largest $\\eta$, and append the orthonormalized full-order solution $u(p^\\star)$ to the basis. Continue until the basis has dimension $r_{\\mathrm{POD}}$.\n\nDefine the out-of-sample test suite $\\mathcal{P}_{\\mathrm{test}}$ by the four parameter tuples:\n$$\np_1 = (2.2\\times 10^{-10}\\ \\text{m}^2/\\text{s},\\ 0.2\\ \\text{s}^{-1},\\ 300\\ \\text{mol}\\,(\\text{m}^{-3}\\,\\text{s}^{-1})),\n$$\n$$\np_2 = (4.2\\times 10^{-10}\\ \\text{m}^2/\\text{s},\\ 0.9\\ \\text{s}^{-1},\\ 350\\ \\text{mol}\\,(\\text{m}^{-3}\\,\\text{s}^{-1})),\n$$\n$$\np_3 = (1.6\\times 10^{-10}\\ \\text{m}^2/\\text{s},\\ 0.15\\ \\text{s}^{-1},\\ 120\\ \\text{mol}\\,(\\text{m}^{-3}\\,\\text{s}^{-1})),\n$$\n$$\np_4 = (4.4\\times 10^{-10}\\ \\text{m}^2/\\text{s},\\ 0.95\\ \\text{s}^{-1},\\ 450\\ \\text{mol}\\,(\\text{m}^{-3}\\,\\text{s}^{-1})).\n$$\n\nFor each $p_i \\in \\mathcal{P}_{\\mathrm{test}}$, compute the POD-Galerkin solution $u_{r,\\mathrm{POD}}(p_i)$ and the greedy RB-Galerkin solution $u_{r,\\mathrm{RB}}(p_i)$. Using the certified metric $\\eta(p_i;u_r)$ as defined above, evaluate the bound for both reduced solutions. Express each bound in $\\text{mol}/\\text{m}^3$. Your program must output a single line containing a comma-separated list enclosed in square brackets, ordered as\n$$\n[\\eta_{\\mathrm{POD}}(p_1),\\ \\eta_{\\mathrm{RB}}(p_1),\\ \\eta_{\\mathrm{POD}}(p_2),\\ \\eta_{\\mathrm{RB}}(p_2),\\ \\eta_{\\mathrm{POD}}(p_3),\\ \\eta_{\\mathrm{RB}}(p_3),\\ \\eta_{\\mathrm{POD}}(p_4),\\ \\eta_{\\mathrm{RB}}(p_4)],\n$$\nwith each entry a floating-point number. Use $N = 200$ internal nodes and $L = 10^{-3}\\ \\text{m}$. Angles for the source term $\\sin\\left(\\frac{\\pi x}{L}\\right)$ are in radians. The final single-line output must adhere exactly to the described format.",
            "solution": "The user has provided a valid problem statement. It is scientifically grounded, well-posed, and contains all necessary details for a complete solution. I will now provide the reasoned solution.\n\nThe problem requires the construction and comparison of two reduced-order models (ROMs) for a parametric one-dimensional stationary reaction-diffusion equation, a common model in electrochemical systems. The full-order model (FOM) is derived from a finite difference discretization of the governing partial differential equation (PDE). The solution proceeds in several distinct stages: FOM setup, snapshot generation, basis construction via two methods (Proper Orthogonal Decomposition and a Greedy algorithm), and finally, evaluation of the resulting ROMs on a set of out-of-sample test parameters.\n\n**1. Full-Order Model (FOM) Discretization**\n\nThe governing PDE is a linear second-order boundary value problem:\n$$\n-\\frac{\\mathrm{d}}{\\mathrm{d}x}\\left(D\\,\\frac{\\mathrm{d} c_e}{\\mathrm{d}x}\\right) + k\\,c_e = j \\sin\\left(\\frac{\\pi x}{L}\\right) \\quad \\text{for } x \\in (0,L),\n$$\nwith homogeneous Dirichlet boundary conditions $c_e(0) = c_e(L) = 0$. The parameters are the diffusivity $D$, reaction rate $k$, and source amplitude $j$.\n\nThe spatial domain $(0, L)$ is discretized using $N=200$ internal nodes, creating a uniform mesh with spacing $\\Delta x = \\frac{L}{N+1}$. Let $x_i = i \\Delta x$ for $i=1, \\dots, N$ be the coordinates of the internal nodes. The concentration at these nodes is assembled into a vector $u(p) \\in \\mathbb{R}^N$, where $u_i(p) \\approx c_e(x_i; p)$.\n\nUsing a central finite difference scheme for the second derivative, we approximate $-\\frac{\\mathrm{d}^2 c_e}{\\mathrm{d}x^2}$ at node $x_i$ with $\\frac{-u_{i-1} + 2u_i - u_{i+1}}{\\Delta x^2}$. Incorporating the boundary conditions $u_0 = u_{N+1} = 0$, this leads to a system of linear equations $A(p)u(p)=f(p)$, where:\n- The system matrix is $A(p) = D\\,T + k\\,I$. Here, $T \\in \\mathbb{R}^{N \\times N}$ is the scaled stiffness matrix arising from the Laplacian operator, with entries $T_{ii} = \\frac{2}{\\Delta x^2}$ and $T_{i,i\\pm 1} = -\\frac{1}{\\Delta x^2}$. $I$ is the identity matrix.\n- The right-hand side vector is $f(p) = j\\,\\mathbf{s}$, where $\\mathbf{s}_i = \\sin\\left(\\frac{\\pi x_i}{L}\\right)$.\n\nThis structure admits an affine parameter-separability, which is crucial for efficient reduced-basis computations:\n$$\nA(p) = \\theta_1(p) A_1 + \\theta_2(p) A_2 = D\\,T + k\\,I,\n$$\n$$\nf(p) = \\theta_3(p) f_1 = j\\,\\mathbf{s}.\n$$\n\n**2. Snapshot Generation**\n\nTo build a ROM, we first sample the behavior of the FOM. We compute solutions, called \"snapshots,\" for a representative set of parameters. The training set $\\mathcal{P}_{\\mathrm{train}}$ is the Cartesian product of given values for $D$, $k$, and $j$, resulting in $3 \\times 3 \\times 3 = 27$ parameter tuples. For each $p \\in \\mathcal{P}_{\\mathrm{train}}$, we assemble $A(p)$ and $f(p)$ and solve the linear system $A(p)u(p)=f(p)$ to obtain the snapshot vector $u(p)$. These $27$ snapshots are collected as columns of a snapshot matrix $S \\in \\mathbb{R}^{N \\times 27}$.\n\n**3. Proper Orthogonal Decomposition (POD) Basis**\n\nPOD provides an optimal orthonormal basis for representing the snapshot data in a least-squares sense. This basis is computed via the Singular Value Decomposition (SVD) of the snapshot matrix $S$:\n$$\nS = U \\Sigma V^T.\n$$\nThe columns of the matrix $U \\in \\mathbb{R}^{N \\times 27}$ are the left-singular vectors, which form the POD modes. The singular values, on the diagonal of $\\Sigma$, quantify the \"energy\" or importance of each mode. To select a reduced basis of dimension $r_{\\mathrm{POD}}$, we find the minimum number of modes whose cumulative squared singular value energy meets a threshold $\\tau=0.999$:\n$$\nr_{\\mathrm{POD}} = \\min \\left\\{ r \\in \\{1,\\dots,27\\} \\left| \\frac{\\sum_{i=1}^r \\sigma_i^2}{\\sum_{i=1}^{27} \\sigma_i^2} \\ge \\tau \\right. \\right\\}.\n$$\nThe POD basis $V_{\\mathrm{POD}} \\in \\mathbb{R}^{N \\times r_{\\mathrm{POD}}}$ is then formed by the first $r_{\\mathrm{POD}}$ columns of $U$.\n\n**4. Greedy Reduced Basis (RB) Construction**\n\nThe Greedy algorithm constructs a basis iteratively by seeking to minimize the worst-case error over the training set at each step. This is guided by a certified a posteriori error estimator. For a given reduced solution $u_r(p)$ in a basis space $V_r$, the error bound is:\n$$\n\\eta(p;u_r) = \\frac{\\|r(p;u_r)\\|_2}{\\lambda_{\\min}(A(p))},\n$$\nwhere $r(p;u_r) = f(p) - A(p)u_r$ is the residual. The denominator, $\\lambda_{\\min}(A(p))$, is the smallest eigenvalue of $A(p)$ and serves as a lower bound for the coercivity constant of the problem. For the SPD matrix $A(p) = D\\,T + k\\,I$, we can efficiently compute $\\lambda_{\\min}(A(p)) = D\\,\\lambda_{\\min}(T) + k$. The smallest eigenvalue of the unscaled tridiagonal matrix $\\text{tridiag}(-1, 2, -1)$ is $2 - 2 \\cos(\\frac{\\pi}{N+1})$, so $\\lambda_{\\min}(T) = \\frac{2}{\\Delta x^2}(1 - \\cos(\\frac{\\pi}{N+1}))$.\n\nThe algorithm proceeds as follows for $k=1, \\dots, r_{\\mathrm{POD}}$:\n1.  Initialize with an empty basis $V_0 = \\emptyset$.\n2.  In iteration $k$, find the parameter $p_k^\\star$ that maximizes the error estimator over the training set, using the current basis $V_{k-1}$: $p_k^\\star = \\arg\\max_{p \\in \\mathcal{P}_{\\mathrm{train}}} \\eta(p; u_{r,k-1}(p))$.\n3.  Compute the full-order solution (snapshot) $u(p_k^\\star)$.\n4.  Orthonormalize this new snapshot against the existing basis vectors $V_{k-1}$ using the Gram-Schmidt process.\n5.  Append the resulting orthonormal vector to the basis, forming $V_k$.\nThis is repeated until the basis has dimension $r_{\\mathrm{POD}}$.\n\n**5. Evaluation on Test Parameters**\n\nWith both bases, $V_{\\mathrm{POD}}$ and $V_{\\mathrm{RB}}$, constructed, we evaluate their performance on the out-of-sample test set $\\mathcal{P}_{\\mathrm{test}}$. For a given test parameter $p_i \\in \\mathcal{P}_{\\mathrm{test}}$ and a basis $V_r$, the reduced solution $u_r(p_i)$ is found by Galerkin projection. This involves solving a much smaller linear system:\n$$\nA_r(p_i) u_{r, \\text{coeffs}} = f_r(p_i),\n$$\nwhere $A_r(p_i) = V_r^T A(p_i) V_r \\in \\mathbb{R}^{r \\times r}$ and $f_r(p_i) = V_r^T f(p_i) \\in \\mathbb{R}^r$. The high-dimensional approximate solution is then recovered as $u_r(p_i) = V_r u_{r, \\text{coeffs}}$.\n\nThe problem asks to compute the value of the error bound $\\eta(p_i; u_r)$ for both the POD and Greedy RB solutions for each of the four test parameters. This demonstrates the ability of the error estimator to provide a guaranteed error bound for new parameters without computing the (expensive) full-order solution. The units of $\\eta$ are $\\text{mol}/\\text{m}^3$, corresponding to concentration, as the units of the residual vector components are $\\text{mol}/(\\text{m}^3 \\cdot \\text{s})$ and the units of $\\lambda_{\\min}(A(p))$ are $\\text{s}^{-1}$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the reduced-basis method problem.\n    \"\"\"\n    # 1. Define constants and discretization\n    L = 1e-3  # m\n    N = 200\n    tau = 0.999\n    dx = L / (N + 1)\n    x_nodes = np.arange(1, N + 1) * dx\n\n    # 2. Build Full-Order Model (FOM) affine components\n    # A1 = T (tridiagonal stiffness matrix)\n    diag_main = 2.0 * np.ones(N)\n    diag_off = -1.0 * np.ones(N - 1)\n    A1 = (np.diag(diag_main) + np.diag(diag_off, k=1) + np.diag(diag_off, k=-1)) / dx**2\n    \n    # A2 = I (identity matrix)\n    A2 = np.identity(N)\n    \n    # f1 = s (source vector)\n    f1 = np.sin(np.pi * x_nodes / L)\n\n    # 3. Define training parameter set and generate snapshots\n    D_train_vals = [1.5e-10, 3.0e-10, 4.5e-10]\n    K_train_vals = [0.1, 0.55, 1.0]\n    J_train_vals = [150, 275, 400]\n    \n    P_train = []\n    for d in D_train_vals:\n        for k in K_train_vals:\n            for j in J_train_vals:\n                P_train.append((d, k, j))\n\n    num_snapshots = len(P_train)\n    S = np.zeros((N, num_snapshots))\n    \n    for i, p in enumerate(P_train):\n        D, k, j = p\n        A = D * A1 + k * A2\n        f = j * f1\n        u = np.linalg.solve(A, f)\n        S[:, i] = u\n\n    # 4. Construct Proper Orthogonal Decomposition (POD) basis\n    U, s_vals, Vh = np.linalg.svd(S, full_matrices=False)\n    energy = np.cumsum(s_vals**2) / np.sum(s_vals**2)\n    r_pod = np.where(energy >= tau)[0][0] + 1\n    V_pod = U[:, :r_pod]\n\n    # 5. Construct Greedy Reduced Basis (RB)\n    lambda_min_T = (2 - 2 * np.cos(np.pi / (N + 1))) / dx**2\n    \n    basis_vectors = []\n    for i in range(r_pod):\n        max_eta = -1.0\n        p_star = None\n        \n        # Search for p_star maximizing the error estimator\n        for p in P_train:\n            D, k, j = p\n            A = D * A1 + k * A2\n            f = j * f1\n            \n            if not basis_vectors:  # First iteration, basis is empty\n                residual = f\n            else:\n                V = np.array(basis_vectors).T\n                Ar = V.T @ A @ V\n                fr = V.T @ f\n                try:\n                    u_r_coeffs = np.linalg.solve(Ar, fr)\n                    u_r = V @ u_r_coeffs\n                    residual = f - A @ u_r\n                except np.linalg.LinAlgError:\n                    residual = f # Failsafe if matrix is singular, just use FOM\n            \n            lambda_min_A = D * lambda_min_T + k\n            eta = np.linalg.norm(residual) / lambda_min_A\n            \n            if eta > max_eta:\n                max_eta = eta\n                p_star = p\n\n        # Found p_star, get FOM solution\n        D_star, k_star, j_star = p_star\n        A_star = D_star * A1 + k_star * A2\n        f_star = j_star * f1\n        u_star = np.linalg.solve(A_star, f_star)\n        \n        # Orthonormalize u_star against current basis (Modified Gram-Schmidt)\n        new_vec = u_star\n        for v in basis_vectors:\n            new_vec = new_vec - np.dot(new_vec, v) * v\n        \n        norm_v = np.linalg.norm(new_vec)\n        if norm_v > 1e-12: # Avoid division by zero\n            basis_vectors.append(new_vec / norm_v)\n\n    V_rb = np.array(basis_vectors).T\n\n    # 6. Evaluate error bounds on the test set\n    test_cases = [\n        (2.2e-10, 0.2, 300),\n        (4.2e-10, 0.9, 350),\n        (1.6e-10, 0.15, 120),\n        (4.4e-10, 0.95, 450),\n    ]\n\n    def evaluate_error_bound(p, V_basis, A1, A2, f1, lambda_min_T_val):\n        D, k, j = p\n        A = D * A1 + k * A2\n        f = j * f1\n        \n        # Galerkin projection\n        Ar = V_basis.T @ A @ V_basis\n        fr = V_basis.T @ f\n        u_r_coeffs = np.linalg.solve(Ar, fr)\n        u_r = V_basis @ u_r_coeffs\n        \n        # Residual and error bound\n        residual = f - A @ u_r\n        lambda_min_A = D * lambda_min_T_val + k\n        eta = np.linalg.norm(residual) / lambda_min_A\n        return eta\n\n    results = []\n    for p_test in test_cases:\n        eta_pod = evaluate_error_bound(p_test, V_pod, A1, A2, f1, lambda_min_T)\n        eta_rb = evaluate_error_bound(p_test, V_rb, A1, A2, f1, lambda_min_T)\n        results.extend([eta_pod, eta_rb])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A reliable reduced model must be more than just fast and numerically accurate; it must also be physically consistent. This practice addresses the crucial concept of structure preservation by asking you to certify that the reduced model respects a fundamental thermodynamic law: non-negative entropy production. By comparing this critical quantity between the full and reduced models, you will learn how to verify that your ROM is not just a mathematical approximation but a valid physical surrogate. ",
            "id": "3945567",
            "problem": "Consider a one-dimensional, isothermal, electroneutral porous-electrode electrolyte model in the context of automated battery design and simulation. The reduced-basis method (RBM) is to be used for a parametric certification test of structure preservation that assesses whether thermodynamic irreversibility is preserved after model reduction by comparing entropy production. The electrolyte potential is taken as spatially varying only through its gradient that drives a uniform ionic current, and the salt concentration is determined by a steady-state reaction-diffusion balance. The fundamental base for this problem consists of linear irreversible thermodynamics and conservation laws.\n\nAssume the following one-dimensional steady-state parametric model on the spatial domain $x \\in [0,1]$ with dimensionless quantities. Let $c(x)$ denote the dimensionless salt concentration, normalized such that the equilibrium value is $1$. Let $\\phi(x)$ denote the dimensionless electrolyte potential. The material parameters are the dimensionless diffusivity $D$, the dimensionless electrolyte conductivity $\\kappa$, and the dimensionless first-order reaction rate $k$. A dimensionless applied current density $J$ is imposed uniformly, and the boundary conditions for the concentration are Dirichlet: $c(0)=1+\\delta$ and $c(1)=1-\\delta$, where $\\delta$ is a dimensionless small amplitude controlling the boundary concentration perturbation. The governing steady-state concentration equation is\n$$\nD \\frac{d^2 c}{dx^2} - k\\,(c-1) = 0,\n$$\nwith the specified Dirichlet boundary conditions. Under the assumption of uniform current $J$ and uniform conductivity $\\kappa$, the electrolyte potential obeys $\\frac{d\\phi}{dx} = -\\frac{J}{\\kappa}$ and does not require spatial solution beyond its constant gradient.\n\nFrom the base of linear irreversible thermodynamics, the local entropy production density is the sum of flux-force products and, near equilibrium, can be taken as a quadratic functional of the fields. In this model, the dimensionless total entropy production rate $\\Sigma$ is defined by\n$$\n\\Sigma(D,k,\\kappa,J,\\delta;c) \\;=\\; \\int_0^1 \\left( \\frac{J^2}{\\kappa} \\;+\\; D \\left|\\frac{dc}{dx}\\right|^2 \\;+\\; k\\,\\left(c-1\\right)^2 \\right)\\,dx,\n$$\nwhere the three terms correspond respectively to ohmic dissipation, diffusive mixing, and chemical reaction contributions. This quantity is nonnegative by construction, capturing thermodynamic irreversibility in a form that is structurally preserved under physically valid approximations near equilibrium.\n\nA reduced-basis method (RBM) is constructed using Proper Orthogonal Decomposition (POD) from full-model snapshots of the interior concentration solutions for a training set of parameter values. Let $V \\in \\mathbb{R}^{n \\times r}$ denote the column-orthonormal reduced basis obtained from the leading $r$ POD modes of the snapshot matrix, where $n$ is the number of interior degrees of freedom and $r \\ll n$. The full-order discretization of the steady-state boundary value problem is performed using a second-order centered finite-difference scheme on a uniform grid of $N$ nodes in $[0,1]$, with the two boundary nodes enforcing the Dirichlet data and $n=N-2$ interior unknowns. The discrete full-model linear system for the interior concentration vector $c_{\\mathrm{int}} \\in \\mathbb{R}^n$ takes the form\n$$\nA(D,k)\\,c_{\\mathrm{int}} = b(D,k,\\delta),\n$$\nwhere $A(D,k) \\in \\mathbb{R}^{n \\times n}$ is a symmetric, strictly diagonally dominant tridiagonal matrix arising from the discrete operator $D\\,\\partial_{xx} - k\\,I$, and $b(D,k,\\delta) \\in \\mathbb{R}^n$ collects the appropriate reaction source term and boundary contributions induced by the Dirichlet conditions. The reduced-order model is defined by the Galerkin projection\n$$\nA_r(D,k) \\, \\alpha \\;=\\; b_r(D,k,\\delta), \\quad A_r(D,k) \\;=\\; V^\\top A(D,k) V, \\quad b_r(D,k,\\delta) \\;=\\; V^\\top b(D,k,\\delta),\n$$\nwith the reduced coefficients $\\alpha \\in \\mathbb{R}^r$ and the reconstructed concentration $c_{\\mathrm{int}}^{(r)} \\;=\\; V \\alpha$. The boundary nodes are inserted to assemble the full reduced concentration vector $c^{(r)}(x)$ on all grid nodes. The reduced entropy production is then computed by numerically integrating the same functional,\n$$\n\\Sigma^{(r)}(D,k,\\kappa,J,\\delta) \\;=\\; \\int_0^1 \\left( \\frac{J^2}{\\kappa} \\;+\\; D \\left|\\frac{dc^{(r)}}{dx}\\right|^2 \\;+\\; k\\,\\left(c^{(r)}-1\\right)^2 \\right)\\,dx.\n$$\n\nCertification test objective: propose and implement a test that certifies structure preservation of the reduced model by computing the dimensionless reduced entropy production and comparing it to the full-model value across parameters. The certification criterion is quantified using the relative entropy production error,\n$$\n\\varepsilon_{\\Sigma} \\;=\\; \\frac{\\left|\\Sigma^{(r)} - \\Sigma\\right|}{\\Sigma},\n$$\nfor each parameter set. The test returns these errors and can be supplemented by checking nonnegativity of the reduced entropy production. All numerical integrals are to be computed using a uniform-grid trapezoidal rule consistent with the finite-difference scheme.\n\nYour task is to implement a complete program that:\n- Constructs a full-order finite-difference discretization for the steady-state boundary value problem for $c(x)$, solves for $c(x)$ at training parameter values, and builds a Proper Orthogonal Decomposition (POD) basis of rank $r$ from the interior snapshots.\n- For each test parameter set, solves both the full-order system and the reduced-order system via Galerkin projection and reconstructs the concentration field including boundary nodes.\n- Computes the dimensionless full-model entropy production $\\Sigma$ and the dimensionless reduced-model entropy production $\\Sigma^{(r)}$ using the discrete gradient and trapezoidal integration consistent with the discretization, and then computes the relative error $\\varepsilon_{\\Sigma}$ for each case.\n- Produces the required output containing the list of relative errors for the test suite.\n\nUse the following test suite of parameter values to ensure coverage:\n- Case $1$ (general case): $D=0.01$, $k=1.0$, $\\kappa=1.2$, $J=0.5$, $\\delta=0.1$.\n- Case $2$ (low diffusivity boundary condition stress): $D=0.0001$, $k=0.5$, $\\kappa=0.8$, $J=0.2$, $\\delta=0.05$.\n- Case $3$ (high reaction rate and high ohmic dissipation): $D=0.005$, $k=10.0$, $\\kappa=0.3$, $J=1.0$, $\\delta=0.1$.\n- Case $4$ (zero boundary concentration perturbation, purely ohmic and minimal reaction): $D=0.008$, $k=0.05$, $\\kappa=2.0$, $J=0.8$, $\\delta=0.0$.\n- Case $5$ (moderate mixed regime): $D=0.002$, $k=2.0$, $\\kappa=0.6$, $J=0.6$, $\\delta=0.15$.\n\nAll quantities are dimensionless, so no physical units are required. The final output format must be a single line containing the list of relative errors for the five cases as a comma-separated list enclosed in square brackets, expressed as decimal numbers. For example, the output must look exactly like $[e_1,e_2,e_3,e_4,e_5]$, where each $e_i$ is a decimal number.\n\nYour program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[e_1,e_2,e_3,e_4,e_5]$).",
            "solution": "The user has requested a solution to a problem involving model order reduction for a simplified electrochemical model. The task is to implement a certification test for structure preservation by comparing the entropy production of a full-order model (FOM) and a reduced-order model (ROM).\n\n### Step 1: Problem Validation\n\nI will first validate the problem statement according to the specified criteria.\n\n#### Extraction of Givens\n\n-   **Governing Equation (Concentration)**: $D \\frac{d^2 c}{dx^2} - k\\,(c-1) = 0$ on the spatial domain $x \\in [0,1]$.\n-   **Boundary Conditions (Concentration)**: $c(0)=1+\\delta$ and $c(1)=1-\\delta$.\n-   **Governing Equation (Potential)**: $\\frac{d\\phi}{dx} = -\\frac{J}{\\kappa}$ (uniform gradient).\n-   **Model Parameters**: Dimensionless diffusivity $D$, reaction rate $k$, electrolyte conductivity $\\kappa$, applied current density $J$, and boundary concentration perturbation $\\delta$.\n-   **Total Entropy Production Functional ($\\Sigma$)**: \n    $$\n    \\Sigma(D,k,\\kappa,J,\\delta;c) = \\int_0^1 \\left( \\frac{J^2}{\\kappa} + D \\left|\\frac{dc}{dx}\\right|^2 + k\\,\\left(c-1\\right)^2 \\right)\\,dx\n    $$\n-   **Numerical Discretization**: Second-order centered finite-difference scheme on a uniform grid of $N$ nodes. The number of interior nodes is $n=N-2$.\n-   **Full-Order Model (FOM)**: A discrete linear system $A(D,k)\\,c_{\\mathrm{int}} = b(D,k,\\delta)$ for the $n$ interior concentration values.\n-   **Reduced-Order Model (ROM)**:\n    -   **Basis**: A column-orthonormal basis $V \\in \\mathbb{R}^{n \\times r}$ from the leading $r$ Proper Orthogonal Decomposition (POD) modes of full-model solution snapshots.\n    -   **Galerkin Projection**: $A_r \\alpha = b_r$, where $A_r = V^\\top A V$, $b_r = V^\\top b$.\n    -   **Reconstruction**: The reduced interior solution is $c_{\\mathrm{int}}^{(r)} = V \\alpha$.\n-   **Reduced Entropy Production ($\\Sigma^{(r)}$)**: The same functional form as $\\Sigma$, evaluated with the reconstructed reduced solution $c^{(r)}$.\n-   **Certification Criterion**: Relative entropy production error $\\varepsilon_{\\Sigma} = \\frac{|\\Sigma^{(r)} - \\Sigma|}{\\Sigma}$.\n-   **Numerical Integration**: Uniform-grid trapezoidal rule.\n-   **Task**: Implement a program to build the POD basis, solve both FOM and ROM for a set of test parameters, compute the respective entropy productions $\\Sigma$ and $\\Sigma^{(r)}$, and calculate the relative error $\\varepsilon_{\\Sigma}$ for each test case.\n-   **Test Suite**:\n    1.  $D=0.01$, $k=1.0$, $\\kappa=1.2$, $J=0.5$, $\\delta=0.1$.\n    2.  $D=0.0001$, $k=0.5$, $\\kappa=0.8$, $J=0.2$, $\\delta=0.05$.\n    3.  $D=0.005$, $k=10.0$, $\\kappa=0.3$, $J=1.0$, $\\delta=0.1$.\n    4.  $D=0.008$, $k=0.05$, $\\kappa=2.0$, $J=0.8$, $\\delta=0.0$.\n    5.  $D=0.002$, $k=2.0$, $\\kappa=0.6$, $J=0.6$, $\\delta=0.15$.\n\n#### Validation Assessment\n\n1.  **Scientific Grounding**: The problem is well-grounded in established scientific principles. The governing equation is a standard linear reaction-diffusion equation. The entropy production functional is consistent with linear irreversible thermodynamics for processes near equilibrium, representing ohmic heating, diffusive mixing, and chemical dissipation. The proposed numerical methods (finite differences, POD, Galerkin projection) are standard techniques in computational science and engineering.\n2.  **Well-Posedness**: The governing boundary value problem is a linear, second-order ordinary differential equation with constant coefficients and Dirichlet boundary conditions, which guarantees a unique solution. The corresponding finite-difference discretization leads to a linear system $A c_{\\mathrm{int}} = b$. The operator $D \\frac{d^2}{dx^2} - kI$ with $D>0$, $k \\ge 0$ leads to a symmetric negative definite matrix $A$, which is non-singular, ensuring the discrete problem is also well-posed.\n3.  **Objectivity**: The problem is formulated using precise mathematical language and is devoid of subjective or ambiguous statements.\n4.  **Completeness and Consistency**: The problem statement is mostly self-contained. It omits the specification of the number of grid nodes $N$ (and thus interior nodes $n$), the rank of the reduced basis $r$, and the specific training set for the POD basis. These are necessary for a concrete implementation. However, this is a minor omission, not a fundamental flaw. It is standard practice in such problems to make reasonable assumptions. I will assume the provided test suite also serves as the training set for generating snapshots. I will select a sufficiently fine grid (e.g., $N=102$, so $n=100$) and a basis rank $r$ equal to the number of snapshots (i.e., $r=5$). These choices are consistent with the problem's context.\n\n#### Verdict\n\nThe problem is **valid**. The minor ambiguities regarding $N$, $r$, and the training set can be resolved with standard, explicitly stated assumptions. I will proceed with the solution.\n\n### Step 2: Solution\n\nThe solution will be implemented as a Python program structured to perform the requested certification test. The procedure is as follows:\n\n1.  **System Discretization**: The continuous problem is discretized on a uniform grid. For a grid of $N$ points, the spacing is $h=1/(N-1)$. The second-order centered finite difference approximation of the governing equation $D c'' - k(c-1)=0$ at an interior node $x_i$ is\n    $$\n    D \\frac{c_{i+1} - 2c_i + c_{i-1}}{h^2} - k c_i = -k.\n    $$\n    This can be rearranged into a linear system $A c_{\\mathrm{int}} = b$ for the $n=N-2$ interior nodal values, where $c_{\\mathrm{int}} = [c_1, \\dots, c_{N-2}]^\\top$. The matrix $A$ is tridiagonal and depends on $D$ and $k$:\n    $$\n    A_{ij} = \\begin{cases}\n    -\\left(\\frac{2D}{h^2} + k\\right) & \\text{if } i=j \\\\\n    \\frac{D}{h^2} & \\text{if } |i-j|=1 \\\\\n    0 & \\text{otherwise}\n    \\end{cases}\n    $$\n    The right-hand side vector $b$ incorporates the constant source term $-k$ and the boundary conditions $c_0 = 1+\\delta$ and $c_{N-1} = 1-\\delta$:\n    $$\n    b_i = \\begin{cases}\n    -k - \\frac{D}{h^2}(1+\\delta) & \\text{if } i=1 \\\\\n    -k & \\text{if } 1 < i < n \\\\\n    -k - \\frac{D}{h^2}(1-\\delta) & \\text{if } i=n\n    \\end{cases}\n    $$\n    where the indices for $b$ are from $1$ to $n=N-2$.\n\n2.  **POD Basis Construction**: A set of \"snapshot\" solutions is generated by solving the full-order model for each parameter set in the training data. Let the collection of these $n$-dimensional interior solution vectors form the columns of a snapshot matrix $S \\in \\mathbb{R}^{n \\times m}$, where $m$ is the number of training parameter sets. The POD basis $V \\in \\mathbb{R}^{n \\times r}$ is constructed from the first $r$ left singular vectors obtained from the Singular Value Decomposition (SVD) of $S$. The SVD is given by $S=U\\Sigma_{SVD}W^\\top$, and the basis is $V=U[:,:r]$.\n\n3.  **Reduced-Order Model (ROM) Solution**: For a given test parameter set, the FOM system matrices $A$ and $b$ are constructed. The ROM is then formulated via Galerkin projection onto the basis $V$:\n    $$\n    A_r = V^\\top A V \\quad \\text{and} \\quad b_r = V^\\top b.\n    $$\n    This yields a much smaller linear system $A_r \\alpha = b_r$ of size $r \\times r$, where $\\alpha \\in \\mathbb{R}^r$ is the vector of reduced coefficients. After solving for $\\alpha$, the approximate interior solution is reconstructed as $c_{\\mathrm{int}}^{(r)} = V \\alpha$. The full solution vector $c^{(r)}$ is then assembled by prepending $c_0=1+\\delta$ and appending $c_{N-1}=1-\\delta$.\n\n4.  **Entropy Production Calculation**: The total entropy production $\\Sigma$ is computed by numerically integrating its density functional. For a given full concentration vector $c$ (from either FOM or ROM), the integrand values are calculated at each grid point $x_i$:\n    $$\n    I_i = \\frac{J^2}{\\kappa} + D (\\nabla c)_i^2 + k (c_i - 1)^2.\n    $$\n    The gradient $(\\nabla c)_i$ is computed using a second-order finite difference approximation. The integral is then evaluated using the trapezoidal rule, which is consistent with the discretization scheme: $\\Sigma \\approx h \\sum_{i=0}^{N-2} \\frac{I_i + I_{i+1}}{2}$.\n\n5.  **Certification**: For each test case, the relative error $\\varepsilon_{\\Sigma} = |\\Sigma^{(r)} - \\Sigma| / \\Sigma$ is computed. Since the test cases provided are also used for training, we expect the ROM to be highly accurate, resulting in very small errors. The nonnegativity of $\\Sigma^{(r)}$ is also an implicit check; given the quadratic form of the integrand, this should be structurally guaranteed.\n\nThe implementation will follow these steps, using a grid size of $N=102$ ($n=100$) and a basis rank of $r=5$.",
            "answer": "```python\nimport numpy as np\nimport scipy.linalg\n\ndef solve():\n    \"\"\"\n    Implements the full-order and reduced-order models for a parametric\n    electrochemical system, and computes the relative error in entropy production\n    as a certification test for structure preservation.\n    \"\"\"\n    #\n    # --- Model and Numerical Parameters ---\n    #\n    N = 102  # Number of grid points\n    n = N - 2  # Number of interior degrees of freedom\n    r = 5      # Rank of the reduced basis\n    x = np.linspace(0.0, 1.0, N)\n    h = x[1] - x[0]\n\n    #\n    # --- Test Suite and Training Set Definition ---\n    #\n    # The problem provides 5 cases for testing. These will also be used\n    # as the training set to generate the POD basis.\n    test_cases = [\n        # Case 1: General case\n        {'D': 0.01, 'k': 1.0, 'kappa': 1.2, 'J': 0.5, 'delta': 0.1},\n        # Case 2: Low diffusivity\n        {'D': 0.0001, 'k': 0.5, 'kappa': 0.8, 'J': 0.2, 'delta': 0.05},\n        # Case 3: High reaction rate\n        {'D': 0.005, 'k': 10.0, 'kappa': 0.3, 'J': 1.0, 'delta': 0.1},\n        # Case 4: Zero boundary perturbation\n        {'D': 0.008, 'k': 0.05, 'kappa': 2.0, 'J': 0.8, 'delta': 0.0},\n        # Case 5: Moderate mixed regime\n        {'D': 0.002, 'k': 2.0, 'kappa': 0.6, 'J': 0.6, 'delta': 0.15},\n    ]\n\n    #\n    # --- Helper Functions ---\n    #\n\n    def build_fom_system(params):\n        \"\"\"Constructs the FOM linear system A c_int = b.\"\"\"\n        D, k, delta = params['D'], params['k'], params['delta']\n        \n        # Construct the matrix A\n        diag = -(2.0 * D / h**2 + k) * np.ones(n)\n        off_diag = (D / h**2) * np.ones(n - 1)\n        A = np.diag(diag) + np.diag(off_diag, k=1) + np.diag(off_diag, k=-1)\n        \n        # Construct the vector b\n        b = -k * np.ones(n)\n        b[0] -= (D / h**2) * (1.0 + delta)\n        b[-1] -= (D / h**2) * (1.0 - delta)\n        \n        return A, b\n\n    def compute_entropy(c, params):\n        \"\"\"Computes the total entropy production using the trapezoidal rule.\"\"\"\n        D, k, kappa, J = params['D'], params['k'], params['kappa'], params['J']\n        \n        # Compute gradient numerically\n        dc_dx = np.gradient(c, h)\n        \n        # Integrand: J^2/kappa + D*(dc/dx)^2 + k*(c-1)^2\n        integrand = (J**2 / kappa) + D * (dc_dx**2) + k * ((c - 1.0)**2)\n        \n        # Integrate using trapezoidal rule\n        sigma = np.trapz(integrand, x)\n        \n        return sigma\n\n    #\n    # --- Step 1: Build POD Basis from Snapshots ---\n    #\n    \n    snapshot_matrix = np.zeros((n, len(test_cases)))\n    fom_solutions = [] # Store FOM solutions to avoid re-computation\n\n    for i, params in enumerate(test_cases):\n        A, b = build_fom_system(params)\n        c_int = np.linalg.solve(A, b)\n        snapshot_matrix[:, i] = c_int\n        \n        c_full = np.concatenate(([1.0 + params['delta']], c_int, [1.0 - params['delta']]))\n        fom_solutions.append({'A': A, 'b': b, 'c_full': c_full})\n\n    # Perform SVD and extract the POD basis V\n    U, _, _ = scipy.linalg.svd(snapshot_matrix, full_matrices=False)\n    V = U[:, :r]\n\n    #\n    # --- Step 2: Run Certification Test ---\n    #\n\n    relative_errors = []\n\n    for i, params in enumerate(test_cases):\n        #\n        # --- Full-Order Model (FOM) Analysis ---\n        #\n        fom_data = fom_solutions[i]\n        c_full_fom = fom_data['c_full']\n        sigma_fom = compute_entropy(c_full_fom, params)\n\n        #\n        # --- Reduced-Order Model (ROM) Analysis ---\n        #\n        A_fom = fom_data['A']\n        b_fom = fom_data['b']\n        \n        # Galerkin projection\n        A_rom = V.T @ (A_fom @ V)\n        b_rom = V.T @ b_fom\n        \n        # Solve the small ROM system\n        alpha = np.linalg.solve(A_rom, b_rom)\n        \n        # Reconstruct the solution\n        c_int_rom = V @ alpha\n        c_full_rom = np.concatenate(([1.0 + params['delta']], c_int_rom, [1.0 - params['delta']]))\n        \n        # Compute reduced entropy production\n        sigma_rom = compute_entropy(c_full_rom, params)\n        \n        #\n        # --- Compute Relative Error ---\n        #\n        # The problem text uses |Sigma| in the denominator, but Sigma is a sum of\n        # squares and thus non-negative. For the given cases, J > 0, so Sigma > 0.\n        if sigma_fom > 1e-15:\n            error = np.abs(sigma_rom - sigma_fom) / sigma_fom\n        else:\n            # Handle the unlikely case of zero entropy\n            error = np.abs(sigma_rom - sigma_fom)\n            \n        relative_errors.append(error)\n\n    #\n    # --- Final Output ---\n    #\n    print(f\"[{','.join(map(str, relative_errors))}]\")\n\n\nsolve()\n\n```"
        }
    ]
}