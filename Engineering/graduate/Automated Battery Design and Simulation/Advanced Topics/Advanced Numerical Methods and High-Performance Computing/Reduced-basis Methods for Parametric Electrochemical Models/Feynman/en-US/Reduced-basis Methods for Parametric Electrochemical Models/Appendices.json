{
    "hands_on_practices": [
        {
            "introduction": "The primary motivation for reduced-basis methods is the dramatic acceleration of parametric queries. This is achieved by partitioning the computation into a one-time, computationally intensive \"offline\" stage and a rapid, lightweight \"online\" stage. This practice invites you to quantify this trade-off by deriving the computational costs, measured in floating-point operations, for each stage of a standard RB workflow for a linear parametric system. By analyzing these costs, you will gain a concrete understanding of the conditions under which RB methods provide significant speedups and be able to calculate the \"break-even\" point—the number of queries required to recoup the initial offline investment .",
            "id": "3945459",
            "problem": "Consider a parametric electrochemical model that, after spatial discretization, yields a family of symmetric positive definite linear systems of the form $A(\\mu) x(\\mu) = b(\\mu)$, where $A(\\mu) \\in \\mathbb{R}^{N \\times N}$ is an affine parameterization $A(\\mu) = \\sum_{q=1}^{Q} \\theta_q(\\mu) A_q$ with $Q$ parameter-independent matrices $A_q \\in \\mathbb{R}^{N \\times N}$ and scalar coefficient functions $\\theta_q(\\mu)$. A Reduced Basis (RB) approximation is constructed offline by generating $N_s$ full-order snapshots and then computing a Proper Orthogonal Decomposition (POD) basis via the Singular Value Decomposition (SVD) of the snapshot matrix. An affine reduced assembly is performed offline by projecting each $A_q$ onto the reduced space spanned by the RB basis $V \\in \\mathbb{R}^{N \\times N_r}$, producing reduced operators $A_q^r = V^\\top A_q V \\in \\mathbb{R}^{N_r \\times N_r}$. In the online stage, for a new parameter $\\mu$, the reduced operator $A^r(\\mu) = \\sum_{q=1}^{Q} \\theta_q(\\mu) A_q^r$ is assembled and solved for the reduced state $x^r(\\mu) \\in \\mathbb{R}^{N_r}$, and a linear functional of the state (for instance, a scalar output like terminal voltage) is evaluated from the reduced solution.\n\nStarting from fundamental linear algebra operation counts and standard computational facts, derive expressions for the offline computational cost to:\n- Generate $N_s$ full-order snapshots,\n- Construct the POD basis via an economy Singular Value Decomposition (SVD) of the $N \\times N_s$ snapshot matrix,\n- Precompute the affine reduced operators $A_q^r = V^\\top A_q V$ for all $q \\in \\{1,\\dots,Q\\}$,\n\nand derive expressions for the online computational cost per parameter query to:\n- Evaluate the parameter coefficients $\\theta_q(\\mu)$,\n- Assemble the reduced operator $A^r(\\mu) = \\sum_{q=1}^{Q} \\theta_q(\\mu) A_q^r$,\n- Solve the reduced system, and\n- Evaluate a linear functional of the reduced solution.\n\nUse only the following base facts in your derivation:\n- A dense matrix-matrix product of sizes $m \\times k$ and $k \\times n$ requires approximately $2 m k n$ floating-point operations (flops).\n- For an affine linear combination $A(\\mu) = \\sum_{q=1}^{Q} \\theta_q(\\mu) A_q$ with dense $A_q \\in \\mathbb{R}^{N \\times N}$, forming $A(\\mu)$ by direct scaling and summation costs $(Q$ multiplications $+ (Q-1)$ additions per entry$) \\times N^2 = (2Q-1) N^2$ flops.\n- For a symmetric positive definite matrix $M \\in \\mathbb{R}^{n \\times n}$, Cholesky factorization costs approximately $(1/3) n^3$ flops, and each triangular solve costs approximately $n^2$ flops. Solving $M y = f$ via Cholesky entails one factorization and two triangular solves, for a total of $(1/3) n^3 + 2 n^2$ flops.\n- For an economy Singular Value Decomposition (SVD) of a dense $N \\times N_s$ matrix with $N \\ge N_s$, a widely used flop model is approximately $4 N N_s^2 + 8 N_s^3$ flops.\n- Evaluating $Q$ scalar coefficient functions $\\theta_q(\\mu)$ has a cost that scales linearly in $Q$. Take a constant per-coefficient evaluation cost $C_\\theta = 1$ flop, so the total is $Q$ flops.\n- Evaluating a scalar linear functional $y = c^\\top x$ with $c \\in \\mathbb{R}^{n}$ costs approximately $2 n$ flops for the multiply-accumulate.\n\nAssume dense algebra throughout and that $A(\\mu)$ is symmetric positive definite for all $\\mu$ considered. Using the derived expressions, implement a program that, for each test case listed below, computes:\n- The total offline cost in flops,\n- The total online cost for $M_q$ queries in flops,\n- The total full-order (non-reduced) cost for $M_q$ queries in flops,\n- The per-query speedup ratio defined as $(\\text{full-order per-query flops}) / (\\text{online per-query flops})$,\n- The break-even number of queries $M_{\\text{break}} = \\left\\lceil \\frac{\\text{offline flops}}{\\text{full-order per-query flops} - \\text{online per-query flops}} \\right\\rceil$, with the convention that if the denominator is non-positive, report $-1$.\n\nAll outputs must be expressed in floating-point operations (flops) and must be numerically computed by your program. The test suite uses the following parameter sets $(N, N_s, N_r, Q, M_q)$:\n- $(2000, 50, 30, 5, 100)$\n- $(100, 10, 1, 1, 1)$\n- $(1500, 80, 40, 20, 250)$\n- $(3000, 300, 60, 10, 50)$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes its own list in the order: $[\\text{offline}, \\text{online\\_total}, \\text{full\\_total}, \\text{speedup}, \\text{break\\_even}]$. The final output format must be a single list of these lists with no spaces, for example $[[x_1,y_1,z_1,s_1,b_1],[x_2,y_2,z_2,s_2,b_2],\\dots]$. Angles are not involved. Express break-even as an integer. All other quantities may be floats. Units are flops.",
            "solution": "The problem statement is evaluated as valid. It is scientifically grounded in the principles of numerical linear algebra and model order reduction, specifically the Reduced Basis method. The problem is well-posed, objective, self-contained, and provides a clear, formalizable task.\n\nHerein, we derive the computational costs for the offline and online stages of the Reduced Basis (RB) method as described, based on the provided fundamental operation counts. The full-order model (FOM) is a parametric linear system $A(\\mu) x(\\mu) = b(\\mu)$, where $A(\\mu) = \\sum_{q=1}^{Q} \\theta_q(\\mu) A_q \\in \\mathbb{R}^{N \\times N}$ is symmetric positive definite (SPD). The reduced basis has dimension $N_r$, derived from $N_s$ snapshots.\n\n### Derivation of Computational Costs\n\nWe assume all matrix and vector operations involve dense algebra, as specified. All costs are measured in floating-point operations (flops).\n\n#### I. Offline Computational Cost ($C_{\\text{offline}}$)\n\nThe offline stage consists of three main tasks: snapshot generation, basis construction, and projection of operators.\n\n1.  **Generation of $N_s$ Full-Order Snapshots**:\n    For each of the $N_s$ training parameters, $\\mu_i$, a full-order solution $x(\\mu_i)$ must be computed. This involves assembling the matrix $A(\\mu_i)$ and solving the linear system $A(\\mu_i) x(\\mu_i) = b(\\mu_i)$.\n    *   Cost to assemble one matrix $A(\\mu_i) \\in \\mathbb{R}^{N \\times N}$: The affine assembly involves $Q$ scalings and $Q-1$ additions for each of the $N^2$ entries, given as $(2Q-1)N^2$ flops.\n    *   Cost to solve the $N \\times N$ SPD system using Cholesky factorization: This is given as $(1/3)N^3 + 2N^2$ flops.\n    *   The cost for a single snapshot is the sum of these two: $(2Q-1)N^2 + (1/3)N^3 + 2N^2 = (1/3)N^3 + (2Q+1)N^2$ flops.\n    *   Total cost for $N_s$ snapshots, $C_{\\text{snapshots}}$:\n        $$C_{\\text{snapshots}} = N_s \\left( \\frac{1}{3}N^3 + (2Q+1)N^2 \\right)$$\n\n2.  **Construction of the POD Basis**:\n    The basis is constructed via an economy Singular Value Decomposition (SVD) of the $N \\times N_s$ snapshot matrix. The cost for this operation, $C_{\\text{SVD}}$, is provided directly. Assuming $N \\geq N_s$:\n    $$C_{\\text{SVD}} = 4 N N_s^2 + 8 N_s^3$$\n    The resulting basis is denoted by the orthonormal matrix $V \\in \\mathbb{R}^{N \\times N_r}$, where $N_r$ is the number of retained singular modes.\n\n3.  **Precomputation of Affine Reduced Operators**:\n    Each parameter-independent matrix $A_q \\in \\mathbb{R}^{N \\times N}$ is projected onto the reduced basis to form $A_q^r = V^\\top A_q V \\in \\mathbb{R}^{N_r \\times N_r}$. This computation is done in two steps for efficiency:\n    *   First, compute the intermediate product $T_q = A_q V$. This is a product of an $N \\times N$ matrix and an $N \\times N_r$ matrix. The cost is $2 \\cdot N \\cdot N \\cdot N_r = 2N^2 N_r$ flops.\n    *   Second, compute the final product $A_q^r = V^\\top T_q$. This is a product of an $N_r \\times N$ matrix ($V^\\top$) and an $N \\times N_r$ matrix ($T_q$). The cost is $2 \\cdot N_r \\cdot N \\cdot N_r = 2N N_r^2$ flops.\n    *   The cost to form one reduced operator $A_q^r$ is $2N^2 N_r + 2N N_r^2$.\n    *   Total cost for all $Q$ operators, $C_{\\text{proj}}$:\n        $$C_{\\text{proj}} = Q \\left( 2N^2 N_r + 2N N_r^2 \\right)$$\n\nThe total offline cost, $C_{\\text{offline}}$, is the sum of these three components:\n$$C_{\\text{offline}} = C_{\\text{snapshots}} + C_{\\text{SVD}} + C_{\\text{proj}}$$\n$$C_{\\text{offline}} = N_s \\left( \\frac{1}{3}N^3 + (2Q+1)N^2 \\right) + \\left( 4 N N_s^2 + 8 N_s^3 \\right) + Q \\left( 2N^2 N_r + 2N N_r^2 \\right)$$\n\n#### II. Online Computational Cost ($C_{\\text{online, per-query}}$)\n\nThe online stage involves rapidly computing the solution for a new parameter $\\mu$.\n\n1.  **Evaluation of Parameter Coefficients**:\n    The $Q$ scalar functions $\\theta_q(\\mu)$ are evaluated. With a cost of $C_\\theta=1$ flop per function, the total cost $C_{\\text{coeffs}}$ is:\n    $$C_{\\text{coeffs}} = Q$$\n\n2.  **Assembly of the Reduced Operator**:\n    The reduced operator $A^r(\\mu) = \\sum_{q=1}^{Q} \\theta_q(\\mu) A_q^r$ is assembled. Since each $A_q^r$ is an $N_r \\times N_r$ matrix, the cost $C_{\\text{assemble_r}}$ is given by the affine combination rule for matrices of size $N_r \\times N_r$:\n    $$C_{\\text{assemble_r}} = (2Q-1)N_r^2$$\n\n3.  **Solution of the Reduced System**:\n    The $N_r \\times N_r$ reduced system $A^r(\\mu) x^r(\\mu) = b^r(\\mu)$ is solved. Since $A(\\mu)$ is SPD and $V$ has full column rank, $A^r(\\mu)=V^\\top A(\\mu) V$ is also SPD. Using the Cholesky method for an $N_r \\times N_r$ system, the cost $C_{\\text{solve_r}}$ is:\n    $$C_{\\text{solve_r}} = \\frac{1}{3}N_r^3 + 2N_r^2$$\n    (The cost of forming $b^r(\\mu)$ is assumed to be negligible, consistent with the problem's focus on system matrix operations).\n\n4.  **Evaluation of the Linear Functional**:\n    A linear functional of the reduced state $x^r(\\mu) \\in \\mathbb{R}^{N_r}$ is computed. For a dot product with a vector in $\\mathbb{R}^{N_r}$, the cost $C_{\\text{output}}$ is:\n    $$C_{\\text{output}} = 2N_r$$\n\nThe total online cost per query, $C_{\\text{online, per-query}}$, is the sum of these components:\n$$C_{\\text{online, per-query}} = C_{\\text{coeffs}} + C_{\\text{assemble_r}} + C_{\\text{solve_r}} + C_{\\text{output}}$$\n$$C_{\\text{online, per-query}} = Q + (2Q-1)N_r^2 + \\left( \\frac{1}{3}N_r^3 + 2N_r^2 \\right) + 2N_r = \\frac{1}{3}N_r^3 + (2Q+1)N_r^2 + 2N_r + Q$$\n\n#### III. Full-Order Model Cost ($C_{\\text{fom, per-query}}$)\n\nThe cost of solving the full-order model for a single query, without using model reduction, is:\n1.  **Assembly of $A(\\mu)$**: same as in the snapshot calculation, $(2Q-1)N^2$ flops.\n2.  **Solution of the FOM system**: same as in the snapshot calculation, $(1/3)N^3 + 2N^2$ flops.\n3.  **Evaluation of the Linear Functional**: the state $x(\\mu)$ is in $\\mathbb{R}^N$, so the cost is $2N$ flops.\n\nTotal FOM cost per query:\n$$C_{\\text{fom, per-query}} = ((2Q-1)N^2) + \\left( \\frac{1}{3}N^3 + 2N^2 \\right) + 2N = \\frac{1}{3}N^3 + (2Q+1)N^2 + 2N$$\n\n#### IV. Derived Quantities\n\nThe remaining quantities are calculated as follows:\n\n-   **Total Online Cost for $M_q$ queries**: $C_{\\text{online, total}} = M_q \\times C_{\\text{online, per-query}}$\n-   **Total Full-Order Cost for $M_q$ queries**: $C_{\\text{fom, total}} = M_q \\times C_{\\text{fom, per-query}}$\n-   **Per-Query Speedup Ratio ($S$)**: $S = \\frac{C_{\\text{fom, per-query}}}{C_{\\text{online, per-query}}}$\n-   **Break-Even Number of Queries ($M_{\\text{break}}$)**: This is the number of queries required for the total cost of the RB method to equal the total cost of the FOM.\n    $C_{\\text{offline}} + M_{\\text{break}} \\cdot C_{\\text{online, per-query}} = M_{\\text{break}} \\cdot C_{\\text{fom, per-query}}$\n    $$M_{\\text{break}} = \\left\\lceil \\frac{C_{\\text{offline}}}{C_{\\text{fom, per-query}} - C_{\\text{online, per-query}}} \\right\\rceil$$\n    If the denominator is non-positive, $M_{\\text{break}}$ is reported as $-1$.\n\nThese derived expressions will be implemented to compute the required values for the given test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes computational costs for a reduced basis method and compares them\n    to a full-order model based on provided cost formulas.\n    \"\"\"\n    # Test cases are defined as tuples: (N, N_s, N_r, Q, M_q)\n    test_cases = [\n        (2000, 50, 30, 5, 100),\n        (100, 10, 1, 1, 1),\n        (1500, 80, 40, 20, 250),\n        (3000, 300, 60, 10, 50),\n    ]\n\n    results_list = []\n\n    for case in test_cases:\n        N, Ns, Nr, Q, Mq = case\n\n        # Convert to float to ensure float arithmetic throughout\n        N, Ns, Nr, Q, Mq = float(N), float(Ns), float(Nr), float(Q), float(Mq)\n\n        # 1. Offline Cost Calculation\n        # Cost of generating Ns snapshots\n        cost_snapshots = Ns * (1/3 * N**3 + (2*Q + 1) * N**2)\n        # Cost of SVD for POD basis construction\n        cost_svd = 4 * N * Ns**2 + 8 * Ns**3\n        # Cost of projecting operators\n        cost_proj = Q * (2 * N**2 * Nr + 2 * N * Nr**2)\n        \n        offline_cost = cost_snapshots + cost_svd + cost_proj\n\n        # 2. Online Cost (per query) Calculation\n        online_per_query_cost = (1/3 * Nr**3 + (2*Q + 1) * Nr**2 + 2 * Nr + Q)\n        \n        # 3. Full-Order Model (FOM) Cost (per query) Calculation\n        fom_per_query_cost = (1/3 * N**3 + (2*Q + 1) * N**2 + 2 * N)\n\n        # 4. Total costs for Mq queries\n        online_total_cost = Mq * online_per_query_cost\n        fom_total_cost = Mq * fom_per_query_cost\n\n        # 5. Speedup Ratio\n        # Handle case where online cost could be zero, though unlikely with given formulas\n        if online_per_query_cost > 0:\n            speedup_ratio = fom_per_query_cost / online_per_query_cost\n        else:\n            speedup_ratio = float('inf') # Or handle as appropriate\n\n        # 6. Break-Even Number of Queries\n        cost_difference = fom_per_query_cost - online_per_query_cost\n        if cost_difference > 0:\n            # Ceiling of the division, converted to integer\n            break_even_queries = int(np.ceil(offline_cost / cost_difference))\n        else:\n            # If online is not faster, break-even is not achieved\n            break_even_queries = -1\n        \n        results_list.append([\n            offline_cost,\n            online_total_cost,\n            fom_total_cost,\n            speedup_ratio,\n            break_even_queries\n        ])\n\n    # Format the final output string exactly as required, with no spaces\n    inner_parts = []\n    for res in results_list:\n        # res[4] is break_even_queries, which is already an integer\n        res_str = f\"[{res[0]},{res[1]},{res[2]},{res[3]},{res[4]}]\"\n        inner_parts.append(res_str)\n    \n    final_output = f\"[{','.join(inner_parts)}]\"\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "The quality of a reduced model hinges on the construction of its basis. This practice delves into the heart of basis generation by contrasting two cornerstone strategies: the data-centric Proper Orthogonal Decomposition (POD) and the error-driven Greedy algorithm. You will implement both methods to construct bases for a parametric reaction-diffusion problem, a common motif in electrochemical modeling. A key element of this exercise is the use of a certified a posteriori error estimator to guide the Greedy selection process, introducing you to the powerful concept of building a reduced model that comes with a rigorous guarantee on its accuracy .",
            "id": "3945577",
            "problem": "Consider a one-dimensional electrolyte concentration field $c_e(x;p)$ in a separator of length $L$, modeled for parameters $p = (D,k,j)$ by the stationary linear boundary value problem\n$$\n-\\frac{\\mathrm{d}}{\\mathrm{d}x}\\left(D\\,\\frac{\\mathrm{d} c_e}{\\mathrm{d}x}\\right) + k\\,c_e = s_j(x) \\quad \\text{for } x \\in (0,L), \\quad c_e(0;p) = 0, \\quad c_e(L;p) = 0,\n$$\nwhere $D$ is the electrolyte diffusivity, $k$ is a first-order reaction rate, and $s_j(x) = j \\sin\\left(\\frac{\\pi x}{L}\\right)$ is a volumetric source with amplitude $j$. Assume $D > 0$, $k > 0$, and $j > 0$. Use $L = 10^{-3}\\ \\text{m}$, with the domain discretized by $N$ internal nodes via a uniform mesh of spacing $\\Delta x = \\frac{L}{N+1}$, and central finite differences yielding a symmetric positive definite linear system\n$$\nA(p)\\,u(p) = f(p),\n$$\nwhere $u(p) \\in \\mathbb{R}^N$ represents the discrete concentration vector approximating $c_e(x;p)$ at the internal nodes, $A(p) = D\\,T + k\\,I$ with $T \\in \\mathbb{R}^{N \\times N}$ the standard tridiagonal stiffness matrix\n$$\nT = \\frac{1}{\\Delta x^2}\n\\begin{bmatrix}\n2 & -1 & 0 & \\cdots & 0 \\\\\n-1 & 2 & -1 & \\ddots & \\vdots \\\\\n0 & -1 & 2 & \\ddots & 0 \\\\\n\\vdots & \\ddots & \\ddots & \\ddots & -1 \\\\\n0 & \\cdots & 0 & -1 & 2\n\\end{bmatrix},\n$$\nand $f(p) = j\\,\\mathbf{s}$ with $\\mathbf{s}_i = \\sin\\left(\\frac{\\pi x_i}{L}\\right)$ and $x_i = i\\,\\Delta x$ for $i = 1,\\ldots,N$. This model admits an affine decomposition\n$$\nA(p) = \\theta_1(p)\\,A_1 + \\theta_2(p)\\,A_2, \\quad f(p) = \\theta_3(p)\\,f_1,\n$$\nwith $\\theta_1(p) = D$, $\\theta_2(p) = k$, $\\theta_3(p) = j$, $A_1 = T$, $A_2 = I$, and $f_1 = \\mathbf{s}$.\n\nLet the training parameter set $\\mathcal{P}_{\\mathrm{train}}$ be the Cartesian product\n$$\n\\mathcal{D} = \\{1.5\\times 10^{-10},\\, 3.0\\times 10^{-10},\\, 4.5\\times 10^{-10}\\}\\ \\text{m}^2/\\text{s}, \\quad\n\\mathcal{K} = \\{0.1,\\, 0.55,\\, 1.0\\}\\ \\text{s}^{-1}, \\quad\n\\mathcal{J} = \\{150,\\, 275,\\, 400\\}\\ \\text{mol}\\,(\\text{m}^{-3}\\,\\text{s}^{-1}),\n$$\nso that $\\mathcal{P}_{\\mathrm{train}} = \\mathcal{D} \\times \\mathcal{K} \\times \\mathcal{J}$. Using the full-order model above, compute snapshots $u(p)$ for all $p \\in \\mathcal{P}_{\\mathrm{train}}$.\n\nFrom these training snapshots, construct:\n- A Proper Orthogonal Decomposition (POD) basis of dimension $r_{\\mathrm{POD}}$ determined as the minimal $r$ such that the cumulative singular value energy of the snapshot matrix reaches or exceeds the threshold $\\tau = 0.999$.\n- A Greedy Reduced Basis (RB) of the same dimension $r_{\\mathrm{POD}}$ using the following certified a posteriori error metric for the $2$-norm:\n$$\n\\eta(p;u_r) = \\frac{\\|r(p;u_r)\\|_2}{\\lambda_{\\min}(A(p))}, \\quad r(p;u_r) = f(p) - A(p)\\,u_r,\n$$\nwhere $\\lambda_{\\min}(A(p))$ is the smallest eigenvalue of the symmetric positive definite matrix $A(p)$, and $u_r$ is the reduced-order approximation obtained by Galerkin projection onto the current RB space. Initialize the greedy algorithm with an empty basis. At each greedy iteration, over all $p \\in \\mathcal{P}_{\\mathrm{train}}$, compute $u_r(p)$ for the current basis, evaluate $\\eta(p;u_r)$, select $p^\\star$ with the largest $\\eta$, and append the orthonormalized full-order solution $u(p^\\star)$ to the basis. Continue until the basis has dimension $r_{\\mathrm{POD}}$.\n\nDefine the out-of-sample test suite $\\mathcal{P}_{\\mathrm{test}}$ by the four parameter tuples:\n$$\np_1 = (2.2\\times 10^{-10}\\ \\text{m}^2/\\text{s},\\ 0.2\\ \\text{s}^{-1},\\ 300\\ \\text{mol}\\,(\\text{m}^{-3}\\,\\text{s}^{-1})),\n$$\n$$\np_2 = (4.2\\times 10^{-10}\\ \\text{m}^2/\\text{s},\\ 0.9\\ \\text{s}^{-1},\\ 350\\ \\text{mol}\\,(\\text{m}^{-3}\\,\\text{s}^{-1})),\n$$\n$$\np_3 = (1.6\\times 10^{-10}\\ \\text{m}^2/\\text{s},\\ 0.15\\ \\text{s}^{-1},\\ 120\\ \\text{mol}\\,(\\text{m}^{-3}\\,\\text{s}^{-1})),\n$$\n$$\np_4 = (4.4\\times 10^{-10}\\ \\text{m}^2/\\text{s},\\ 0.95\\ \\text{s}^{-1},\\ 450\\ \\text{mol}\\,(\\text{m}^{-3}\\,\\text{s}^{-1})).\n$$\n\nFor each $p_i \\in \\mathcal{P}_{\\mathrm{test}}$, compute the POD-Galerkin solution $u_{r,\\mathrm{POD}}(p_i)$ and the greedy RB-Galerkin solution $u_{r,\\mathrm{RB}}(p_i)$. Using the certified metric $\\eta(p_i;u_r)$ as defined above, evaluate the bound for both reduced solutions. Express each bound in $\\text{mol}/\\text{m}^3$. Your program must output a single line containing a comma-separated list enclosed in square brackets, ordered as\n$$\n[\\eta_{\\mathrm{POD}}(p_1),\\ \\eta_{\\mathrm{RB}}(p_1),\\ \\eta_{\\mathrm{POD}}(p_2),\\ \\eta_{\\mathrm{RB}}(p_2),\\ \\eta_{\\mathrm{POD}}(p_3),\\ \\eta_{\\mathrm{RB}}(p_3),\\ \\eta_{\\mathrm{POD}}(p_4),\\ \\eta_{\\mathrm{RB}}(p_4)],\n$$\nwith each entry a floating-point number. Use $N = 200$ internal nodes and $L = 10^{-3}\\ \\text{m}$. Angles for the source term $\\sin\\left(\\frac{\\pi x}{L}\\right)$ are in radians. The final single-line output must adhere exactly to the described format.",
            "solution": "The user has provided a valid problem statement. It is scientifically grounded, well-posed, and contains all necessary details for a complete solution. I will now provide the reasoned solution.\n\nThe problem requires the construction and comparison of two reduced-order models (ROMs) for a parametric one-dimensional stationary reaction-diffusion equation, a common model in electrochemical systems. The full-order model (FOM) is derived from a finite difference discretization of the governing partial differential equation (PDE). The solution proceeds in several distinct stages: FOM setup, snapshot generation, basis construction via two methods (Proper Orthogonal Decomposition and a Greedy algorithm), and finally, evaluation of the resulting ROMs on a set of out-of-sample test parameters.\n\n**1. Full-Order Model (FOM) Discretization**\n\nThe governing PDE is a linear second-order boundary value problem:\n$$\n-\\frac{\\mathrm{d}}{\\mathrm{d}x}\\left(D\\,\\frac{\\mathrm{d} c_e}{\\mathrm{d}x}\\right) + k\\,c_e = j \\sin\\left(\\frac{\\pi x}{L}\\right) \\quad \\text{for } x \\in (0,L),\n$$\nwith homogeneous Dirichlet boundary conditions $c_e(0) = c_e(L) = 0$. The parameters are the diffusivity $D$, reaction rate $k$, and source amplitude $j$.\n\nThe spatial domain $(0, L)$ is discretized using $N=200$ internal nodes, creating a uniform mesh with spacing $\\Delta x = \\frac{L}{N+1}$. Let $x_i = i \\Delta x$ for $i=1, \\dots, N$ be the coordinates of the internal nodes. The concentration at these nodes is assembled into a vector $u(p) \\in \\mathbb{R}^N$, where $u_i(p) \\approx c_e(x_i; p)$.\n\nUsing a central finite difference scheme for the second derivative, we approximate $-\\frac{\\mathrm{d}^2 c_e}{\\mathrm{d}x^2}$ at node $x_i$ with $\\frac{-u_{i-1} + 2u_i - u_{i+1}}{\\Delta x^2}$. Incorporating the boundary conditions $u_0 = u_{N+1} = 0$, this leads to a system of linear equations $A(p)u(p)=f(p)$, where:\n- The system matrix is $A(p) = D\\,T + k\\,I$. Here, $T \\in \\mathbb{R}^{N \\times N}$ is the scaled stiffness matrix arising from the Laplacian operator, with entries $T_{ii} = \\frac{2}{\\Delta x^2}$ and $T_{i,i\\pm 1} = -\\frac{1}{\\Delta x^2}$. $I$ is the identity matrix.\n- The right-hand side vector is $f(p) = j\\,\\mathbf{s}$, where $\\mathbf{s}_i = \\sin\\left(\\frac{\\pi x_i}{L}\\right)$.\n\nThis structure admits an affine parameter-separability, which is crucial for efficient reduced-basis computations:\n$$\nA(p) = \\theta_1(p) A_1 + \\theta_2(p) A_2 = D\\,T + k\\,I,\n$$\n$$\nf(p) = \\theta_3(p) f_1 = j\\,\\mathbf{s}.\n$$\n\n**2. Snapshot Generation**\n\nTo build a ROM, we first sample the behavior of the FOM. We compute solutions, called \"snapshots,\" for a representative set of parameters. The training set $\\mathcal{P}_{\\mathrm{train}}$ is the Cartesian product of given values for $D$, $k$, and $j$, resulting in $3 \\times 3 \\times 3 = 27$ parameter tuples. For each $p \\in \\mathcal{P}_{\\mathrm{train}}$, we assemble $A(p)$ and $f(p)$ and solve the linear system $A(p)u(p)=f(p)$ to obtain the snapshot vector $u(p)$. These $27$ snapshots are collected as columns of a snapshot matrix $S \\in \\mathbb{R}^{N \\times 27}$.\n\n**3. Proper Orthogonal Decomposition (POD) Basis**\n\nPOD provides an optimal orthonormal basis for representing the snapshot data in a least-squares sense. This basis is computed via the Singular Value Decomposition (SVD) of the snapshot matrix $S$:\n$$\nS = U \\Sigma V^T.\n$$\nThe columns of the matrix $U \\in \\mathbb{R}^{N \\times 27}$ are the left-singular vectors, which form the POD modes. The singular values, on the diagonal of $\\Sigma$, quantify the \"energy\" or importance of each mode. To select a reduced basis of dimension $r_{\\mathrm{POD}}$, we find the minimum number of modes whose cumulative squared singular value energy meets a threshold $\\tau=0.999$:\n$$\nr_{\\mathrm{POD}} = \\min \\left\\{ r \\in \\{1,\\dots,27\\} \\left| \\frac{\\sum_{i=1}^r \\sigma_i^2}{\\sum_{i=1}^{27} \\sigma_i^2} \\ge \\tau \\right. \\right\\}.\n$$\nThe POD basis $V_{\\mathrm{POD}} \\in \\mathbb{R}^{N \\times r_{\\mathrm{POD}}}$ is then formed by the first $r_{\\mathrm{POD}}$ columns of $U$.\n\n**4. Greedy Reduced Basis (RB) Construction**\n\nThe Greedy algorithm constructs a basis iteratively by seeking to minimize the worst-case error over the training set at each step. This is guided by a certified a posteriori error estimator. For a given reduced solution $u_r(p)$ in a basis space $V_r$, the error bound is:\n$$\n\\eta(p;u_r) = \\frac{\\|r(p;u_r)\\|_2}{\\lambda_{\\min}(A(p))},\n$$\nwhere $r(p;u_r) = f(p) - A(p)u_r$ is the residual. The denominator, $\\lambda_{\\min}(A(p))$, is the smallest eigenvalue of $A(p)$ and serves as a lower bound for the coercivity constant of the problem. For the SPD matrix $A(p) = D\\,T + k\\,I$, we can efficiently compute $\\lambda_{\\min}(A(p)) = D\\,\\lambda_{\\min}(T) + k$. The smallest eigenvalue of the unscaled tridiagonal matrix $\\text{tridiag}(-1, 2, -1)$ is $2 - 2 \\cos(\\frac{\\pi}{N+1})$, so $\\lambda_{\\min}(T) = \\frac{2}{\\Delta x^2}(1 - \\cos(\\frac{\\pi}{N+1}))$.\n\nThe algorithm proceeds as follows for $k=1, \\dots, r_{\\mathrm{POD}}$:\n1.  Initialize with an empty basis $V_0 = \\emptyset$.\n2.  In iteration $k$, find the parameter $p_k^\\star$ that maximizes the error estimator over the training set, using the current basis $V_{k-1}$: $p_k^\\star = \\arg\\max_{p \\in \\mathcal{P}_{\\mathrm{train}}} \\eta(p; u_{r,k-1}(p))$.\n3.  Compute the full-order solution (snapshot) $u(p_k^\\star)$.\n4.  Orthonormalize this new snapshot against the existing basis vectors $V_{k-1}$ using the Gram-Schmidt process.\n5.  Append the resulting orthonormal vector to the basis, forming $V_k$.\nThis is repeated until the basis has dimension $r_{\\mathrm{POD}}$.\n\n**5. Evaluation on Test Parameters**\n\nWith both bases, $V_{\\mathrm{POD}}$ and $V_{\\mathrm{RB}}$, constructed, we evaluate their performance on the out-of-sample test set $\\mathcal{P}_{\\mathrm{test}}$. For a given test parameter $p_i \\in \\mathcal{P}_{\\mathrm{test}}$ and a basis $V_r$, the reduced solution $u_r(p_i)$ is found by Galerkin projection. This involves solving a much smaller linear system:\n$$\nA_r(p_i) u_{r, \\text{coeffs}} = f_r(p_i),\n$$\nwhere $A_r(p_i) = V_r^T A(p_i) V_r \\in \\mathbb{R}^{r \\times r}$ and $f_r(p_i) = V_r^T f(p_i) \\in \\mathbb{R}^r$. The high-dimensional approximate solution is then recovered as $u_r(p_i) = V_r u_{r, \\text{coeffs}}$.\n\nThe problem asks to compute the value of the error bound $\\eta(p_i; u_r)$ for both the POD and Greedy RB solutions for each of the four test parameters. This demonstrates the ability of the error estimator to provide a guaranteed error bound for new parameters without computing the (expensive) full-order solution. The units of $\\eta$ are $\\text{mol}/\\text{m}^3$, corresponding to concentration, as the units of the residual vector components are $\\text{mol}/(\\text{m}^3 \\cdot \\text{s})$ and the units of $\\lambda_{\\min}(A(p))$ are $\\text{s}^{-1}$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the reduced-basis method problem.\n    \"\"\"\n    # 1. Define constants and discretization\n    L = 1e-3  # m\n    N = 200\n    tau = 0.999\n    dx = L / (N + 1)\n    x_nodes = np.arange(1, N + 1) * dx\n\n    # 2. Build Full-Order Model (FOM) affine components\n    # A1 = T (tridiagonal stiffness matrix)\n    diag_main = 2.0 * np.ones(N)\n    diag_off = -1.0 * np.ones(N - 1)\n    A1 = (np.diag(diag_main) + np.diag(diag_off, k=1) + np.diag(diag_off, k=-1)) / dx**2\n    \n    # A2 = I (identity matrix)\n    A2 = np.identity(N)\n    \n    # f1 = s (source vector)\n    f1 = np.sin(np.pi * x_nodes / L)\n\n    # 3. Define training parameter set and generate snapshots\n    D_train_vals = [1.5e-10, 3.0e-10, 4.5e-10]\n    K_train_vals = [0.1, 0.55, 1.0]\n    J_train_vals = [150, 275, 400]\n    \n    P_train = []\n    for d in D_train_vals:\n        for k in K_train_vals:\n            for j in J_train_vals:\n                P_train.append((d, k, j))\n\n    num_snapshots = len(P_train)\n    S = np.zeros((N, num_snapshots))\n    \n    for i, p in enumerate(P_train):\n        D, k, j = p\n        A = D * A1 + k * A2\n        f = j * f1\n        u = np.linalg.solve(A, f)\n        S[:, i] = u\n\n    # 4. Construct Proper Orthogonal Decomposition (POD) basis\n    U, s_vals, Vh = np.linalg.svd(S, full_matrices=False)\n    energy = np.cumsum(s_vals**2) / np.sum(s_vals**2)\n    r_pod = np.where(energy >= tau)[0][0] + 1\n    V_pod = U[:, :r_pod]\n\n    # 5. Construct Greedy Reduced Basis (RB)\n    lambda_min_T = (2 - 2 * np.cos(np.pi / (N + 1))) / dx**2\n    \n    basis_vectors = []\n    for i in range(r_pod):\n        max_eta = -1.0\n        p_star = None\n        \n        # Search for p_star maximizing the error estimator\n        for p in P_train:\n            D, k, j = p\n            A = D * A1 + k * A2\n            f = j * f1\n            \n            if not basis_vectors:  # First iteration, basis is empty\n                residual = f\n            else:\n                V = np.array(basis_vectors).T\n                Ar = V.T @ A @ V\n                fr = V.T @ f\n                try:\n                    u_r_coeffs = np.linalg.solve(Ar, fr)\n                    u_r = V @ u_r_coeffs\n                    residual = f - A @ u_r\n                except np.linalg.LinAlgError:\n                    residual = f # Failsafe if matrix is singular, just use FOM\n            \n            lambda_min_A = D * lambda_min_T + k\n            eta = np.linalg.norm(residual) / lambda_min_A\n            \n            if eta > max_eta:\n                max_eta = eta\n                p_star = p\n\n        # Found p_star, get FOM solution\n        D_star, k_star, j_star = p_star\n        A_star = D_star * A1 + k_star * A2\n        f_star = j_star * f1\n        u_star = np.linalg.solve(A_star, f_star)\n        \n        # Orthonormalize u_star against current basis (Modified Gram-Schmidt)\n        new_vec = u_star\n        for v in basis_vectors:\n            new_vec = new_vec - np.dot(new_vec, v) * v\n        \n        norm_v = np.linalg.norm(new_vec)\n        if norm_v > 1e-12: # Avoid division by zero\n            basis_vectors.append(new_vec / norm_v)\n\n    V_rb = np.array(basis_vectors).T\n\n    # 6. Evaluate error bounds on the test set\n    test_cases = [\n        (2.2e-10, 0.2, 300),\n        (4.2e-10, 0.9, 350),\n        (1.6e-10, 0.15, 120),\n        (4.4e-10, 0.95, 450),\n    ]\n\n    def evaluate_error_bound(p, V_basis, A1, A2, f1, lambda_min_T_val):\n        D, k, j = p\n        A = D * A1 + k * A2\n        f = j * f1\n        \n        # Galerkin projection\n        Ar = V_basis.T @ A @ V_basis\n        fr = V_basis.T @ f\n        u_r_coeffs = np.linalg.solve(Ar, fr)\n        u_r = V_basis @ u_r_coeffs\n        \n        # Residual and error bound\n        residual = f - A @ u_r\n        lambda_min_A = D * lambda_min_T_val + k\n        eta = np.linalg.norm(residual) / lambda_min_A\n        return eta\n\n    results = []\n    for p_test in test_cases:\n        eta_pod = evaluate_error_bound(p_test, V_pod, A1, A2, f1, lambda_min_T)\n        eta_rb = evaluate_error_bound(p_test, V_rb, A1, A2, f1, lambda_min_T)\n        results.extend([eta_pod, eta_rb])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Real-world electrochemical models are often nonlinear, posing a challenge to the efficiency of standard reduced-basis methods. This practice introduces the Empirical Interpolation Method (EIM), a powerful technique to handle nonlinearities without sacrificing the rapid online performance of the reduced model. You will first derive how approximation errors from EIM propagate into the final solution and then implement a complete workflow, from basis generation to solving a nonlinear reduced system via Newton's method. This exercise provides a crucial bridge from linear toy problems to the application of RB methods for more complex and realistic nonlinear systems .",
            "id": "3945440",
            "problem": "Consider a parametrized, semi-discrete, nonlinear electrochemical model motivated by porous electrode theory for a lithium-ion battery, cast in a purely mathematical form suitable for model reduction. Let $n \\in \\mathbb{N}$ be the dimension of a high-fidelity spatial discretization. For each parameter value $\\mu \\in \\mathbb{R}$, define two symmetric positive-definite matrices $K \\in \\mathbb{R}^{n \\times n}$ and $M \\in \\mathbb{R}^{n \\times n}$ representing linear operators, a forcing vector $b(\\mu) \\in \\mathbb{R}^{n}$ representing the applied current distribution, and a nonlinear term $N(u,\\mu) \\in \\mathbb{R}^{n}$ modeling Butler–Volmer-type reaction kinetics. The full-order steady model is given by the residual\n$$\nF(u;\\mu) \\;=\\; \\big(K + \\mu M\\big) u \\;+\\; N(u,\\mu) \\;-\\; b(\\mu),\n$$\nand the full-order solution $u^\\star(\\mu) \\in \\mathbb{R}^{n}$ satisfies $F(u^\\star(\\mu);\\mu) = 0$.\n\nAssume the nonlinearity is component-wise and smooth, specifically\n$$\nN(u,\\mu) \\;=\\; \\alpha(\\mu) \\, \\sinh(u),\n$$\nwhere $\\sinh(\\cdot)$ acts element-wise and $\\alpha(\\mu) > 0$ is a smooth scalar function of $\\mu$. Its Jacobian with respect to $u$ is\n$$\nD_u N(u,\\mu) \\;=\\; \\alpha(\\mu) \\, \\operatorname{diag}\\big(\\cosh(u)\\big).\n$$\n\nLet $V_r \\in \\mathbb{R}^{n \\times r}$ be an orthonormal Reduced Basis (RB) matrix constructed from snapshots of $u^\\star(\\mu)$ at a training set of parameter values, where $r \\ll n$. The RB (Galerkin) reduced system with the exact nonlinearity seeks $a^\\star(\\mu) \\in \\mathbb{R}^{r}$ such that\n$$\nF_r(a;\\mu) \\;=\\; V_r^\\top \\big(K + \\mu M\\big) V_r \\, a \\;+\\; V_r^\\top N\\!\\left(V_r a,\\mu\\right) \\;-\\; V_r^\\top b(\\mu) \\;=\\; 0,\n$$\nyielding the RB-exact state $u_r^\\text{exact}(\\mu) = V_r a^\\star(\\mu)$.\n\nTo enable fast online evaluation, approximate the nonlinearity $N(\\cdot,\\mu)$ using the Empirical Interpolation Method (EIM), in its discrete implementation known as the Discrete Empirical Interpolation Method (DEIM). Let $U_m \\in \\mathbb{R}^{n \\times m}$ be an orthonormal basis of nonlinear snapshots, with $m \\ll n$, and let $P \\in \\{0,1\\}^{n \\times m}$ be a column selection matrix that extracts $m$ empirically selected interpolation indices. The DEIM approximation is\n$$\n\\widetilde{N}(w,\\mu) \\;=\\; U_m \\big(P^\\top U_m\\big)^{-1} P^\\top N(w,\\mu),\n$$\nfor any $w \\in \\mathbb{R}^{n}$. The RB-DEIM reduced system seeks $a^{\\text{deim}}(\\mu) \\in \\mathbb{R}^{r}$ such that\n$$\n\\widetilde{F}_r(a;\\mu) \\;=\\; V_r^\\top \\big(K + \\mu M\\big) V_r \\, a \\;+\\; V_r^\\top \\widetilde{N}\\!\\left(V_r a,\\mu\\right) \\;-\\; V_r^\\top b(\\mu) \\;=\\; 0,\n$$\nyielding the RB-DEIM state $u_r^{\\text{deim}}(\\mu) = V_r a^{\\text{deim}}(\\mu)$.\n\nDefine a scalar output of interest $y(\\mu) \\in \\mathbb{R}$ by\n$$\ny(\\mu) \\;=\\; c^\\top u^\\star(\\mu),\n$$\nfor a fixed vector $c \\in \\mathbb{R}^{n}$ (for example, a linear functional modeling terminal voltage or an average state). In the reduced model, the RB-exact and RB-DEIM outputs are $y_r^\\text{exact}(\\mu) = c^\\top u_r^\\text{exact}(\\mu)$ and $y_r^\\text{deim}(\\mu) = c^\\top u_r^\\text{deim}(\\mu)$, respectively.\n\nTasks:\n1. Starting from the Newton linearization of $F_r(a;\\mu)$ and assuming $V_r$ has orthonormal columns, derive a first-order perturbation bound that quantifies how the DEIM approximation error in the nonlinearity\n$$\ne_N(\\mu) \\;=\\; \\big\\| N\\!\\left(V_r a^\\star(\\mu),\\mu\\right) \\;-\\; \\widetilde{N}\\!\\left(V_r a^\\star(\\mu),\\mu\\right) \\big\\|_2\n$$\npropagates into the RB solution coefficient error $\\big\\| a^{\\text{deim}}(\\mu) - a^\\star(\\mu) \\big\\|_2$, the RB state error $\\big\\| u_r^{\\text{deim}}(\\mu) - u_r^\\text{exact}(\\mu) \\big\\|_2$, and the output error $\\big| y_r^{\\text{deim}}(\\mu) - y_r^\\text{exact}(\\mu) \\big|$. Express the bounds in terms of the inverse of the RB Jacobian at $a^\\star(\\mu)$ and the norm of $c$.\n\n2. Implement a complete program that:\n   - Constructs reproducible symmetric positive-definite matrices $K$ and $M$ and vectors $b(\\mu)$ and $c$.\n   - Generates full-order training snapshots by solving $F(u;\\mu)=0$ for a training set of parameters, then builds $V_r$ by Proper Orthogonal Decomposition (POD) on the snapshot matrix of $u^\\star(\\mu)$.\n   - Builds the nonlinear snapshot basis $U_m$ by POD on the snapshot matrix of $N\\big(u^\\star(\\mu),\\mu\\big)$, and computes DEIM indices via pivoted QR to form $P$.\n   - For each test parameter, solves the RB-exact system and the RB-DEIM system via Newton’s method, computes $e_N(\\mu)$ at $u_r^\\text{exact}(\\mu)$, computes the actual RB state error and output error caused by DEIM, and evaluates the first-order perturbation bounds derived in Task 1.\n\nUse the following test suite of parameter values to exercise different regimes of nonlinearity:\n- Happy path: $\\mu = 0.20$ (moderate nonlinearity).\n- Weakly nonlinear boundary: $\\mu = 0.05$ (small $\\alpha(\\mu)$).\n- Strongly nonlinear regime: $\\mu = 1.50$ (large $\\alpha(\\mu)$).\n- Intermediate regime: $\\mu = 0.80$.\n\nAll quantities are dimensionless, and no physical units are required. Your program should produce a single line of output containing, for each test case in the order above, a list of five floats\n$$\n\\big[ e_N(\\mu), \\; \\|u_r^{\\text{deim}}(\\mu) - u_r^\\text{exact}(\\mu)\\|_2, \\; B_u(\\mu), \\; |y_r^{\\text{deim}}(\\mu) - y_r^\\text{exact}(\\mu)|, \\; B_y(\\mu) \\big],\n$$\nwhere $B_u(\\mu)$ is the first-order bound on the RB state error and $B_y(\\mu)$ is the first-order bound on the output error. Aggregate the four test-case lists into a single comma-separated list enclosed in square brackets, with no spaces, for example\n$$\n[ [e_1,s_1,b_{u,1},o_1,b_{y,1}], [e_2,s_2,b_{u,2},o_2,b_{y,2}], [e_3,s_3,b_{u,3},o_3,b_{y,3}], [e_4,s_4,b_{u,4},o_4,b_{y,4}] ].\n$$\nYour program must print exactly this aggregated format, with numeric values computed to six decimal places.",
            "solution": "The problem is found to be valid as it is scientifically grounded in the theory of model order reduction for parametric partial differential equations, is well-posed, objective, and contains sufficient information for a unique and meaningful solution to be constructed.\n\n### Task 1: Derivation of First-Order Perturbation Bounds\n\nThe objective is to derive first-order bounds on the error introduced by the Discrete Empirical Interpolation Method (DEIM) into the reduced-basis (RB) solution. We analyze how the error in approximating the nonlinear term propagates to errors in the RB coefficient vector, the RB state vector, and the output of interest.\n\nLet $a^\\star(\\mu) \\in \\mathbb{R}^r$ be the coefficient vector of the RB solution using the exact nonlinearity, satisfying the Galerkin-projected system:\n$$\nF_r(a^\\star;\\mu) \\;=\\; V_r^\\top \\big(K + \\mu M\\big) V_r a^\\star \\;+\\; V_r^\\top N(V_r a^\\star,\\mu) \\;-\\; V_r^\\top b(\\mu) \\;=\\; 0.\n$$\nLet $a^{\\text{deim}}(\\mu) \\in \\mathbb{R}^r$ be the coefficient vector for the RB-DEIM solution, which satisfies:\n$$\n\\widetilde{F}_r(a^{\\text{deim}};\\mu) \\;=\\; V_r^\\top \\big(K + \\mu M\\big) V_r a^{\\text{deim}} \\;+\\; V_r^\\top \\widetilde{N}(V_r a^{\\text{deim}},\\mu) \\;-\\; V_r^\\top b(\\mu) \\;=\\; 0.\n$$\nThe DEIM approximation of the nonlinearity is given by $\\widetilde{N}(w,\\mu) = U_m (P^\\top U_m)^{-1} P^\\top N(w,\\mu)$.\n\nOur analysis begins by considering the residual of the RB-DEIM solution $a^{\\text{deim}}$ when evaluated in the *exact* RB system $F_r$.\n$$\nF_r(a^{\\text{deim}};\\mu) \\;=\\; V_r^\\top \\big(K + \\mu M\\big) V_r a^{\\text{deim}} \\;+\\; V_r^\\top N(V_r a^{\\text{deim}},\\mu) \\;-\\; V_r^\\top b(\\mu).\n$$\nFrom the definition of the RB-DEIM system, we can substitute the linear and forcing terms:\n$$\nV_r^\\top \\big(K + \\mu M\\big) V_r a^{\\text{deim}} \\;-\\; V_r^\\top b(\\mu) = -V_r^\\top \\widetilde{N}(V_r a^{\\text{deim}},\\mu).\n$$\nSubstituting this into the expression for $F_r(a^{\\text{deim}};\\mu)$ yields:\n$$\nF_r(a^{\\text{deim}};\\mu) \\;=\\; V_r^\\top N(V_r a^{\\text{deim}},\\mu) \\;-\\; V_r^\\top \\widetilde{N}(V_r a^{\\text{deim}},\\mu) \\;=\\; V_r^\\top \\big[ N(V_r a^{\\text{deim}},\\mu) - \\widetilde{N}(V_r a^{\\text{deim}},\\mu) \\big].\n$$\nThis shows that the residual of the RB-DEIM solution in the exact system is the projection of the DEIM approximation error.\n\nNext, we perform a first-order Taylor expansion of $F_r(a;\\mu)$ around the exact solution coefficient $a^\\star(\\mu)$:\n$$\nF_r(a^{\\text{deim}};\\mu) \\;\\approx\\; F_r(a^\\star;\\mu) \\;+\\; J_r(a^\\star(\\mu);\\mu) \\left( a^{\\text{deim}}(\\mu) - a^\\star(\\mu) \\right),\n$$\nwhere $J_r(a^\\star;\\mu)$ is the Jacobian of $F_r$ with respect to $a$, evaluated at $a^\\star(\\mu)$. By definition, $F_r(a^\\star;\\mu) = 0$. Thus, we have:\n$$\nF_r(a^{\\text{deim}};\\mu) \\;\\approx\\; J_r(a^\\star(\\mu);\\mu) \\left( a^{\\text{deim}}(\\mu) - a^\\star(\\mu) \\right).\n$$\nCombining the two expressions for $F_r(a^{\\text{deim}};\\mu)$, we get:\n$$\nJ_r(a^\\star;\\mu) \\left( a^{\\text{deim}}(\\mu) - a^\\star(\\mu) \\right) \\;\\approx\\; V_r^\\top \\big[ N(V_r a^{\\text{deim}},\\mu) - \\widetilde{N}(V_r a^{\\text{deim}},\\mu) \\big].\n$$\nTo obtain a first-order bound, we approximate the error term on the right-hand side by evaluating it at the exact RB solution $V_r a^\\star$ instead of the RB-DEIM solution $V_r a^{\\text{deim}}$. This is justified by assuming that the DEIM approximation introduces a small perturbation, such that $a^{\\text{deim}} \\approx a^\\star$.\n$$\nJ_r(a^\\star;\\mu) \\left( a^{\\text{deim}}(\\mu) - a^\\star(\\mu) \\right) \\;\\approx\\; V_r^\\top \\big[ N(V_r a^{\\star},\\mu) - \\widetilde{N}(V_r a^{\\star},\\mu) \\big].\n$$\nSolving for the coefficient error $\\delta a(\\mu) = a^{\\text{deim}}(\\mu) - a^\\star(\\mu)$, we find:\n$$\n\\delta a(\\mu) \\;\\approx\\; J_r(a^\\star(\\mu);\\mu)^{-1} V_r^\\top \\big[ N(V_r a^{\\star}(\\mu),\\mu) - \\widetilde{N}(V_r a^{\\star}(\\mu),\\mu) \\big].\n$$\nTaking the Euclidean norm $\\|\\cdot\\|_2$ and applying the sub-multiplicative property of matrix norms:\n$$\n\\|\\delta a(\\mu)\\|_2 \\;\\lesssim\\; \\left\\| J_r(a^\\star(\\mu);\\mu)^{-1} \\right\\|_2 \\left\\| V_r^\\top \\right\\|_2 \\left\\| N(V_r a^{\\star}(\\mu),\\mu) - \\widetilde{N}(V_r a^{\\star}(\\mu),\\mu) \\right\\|_2.\n$$\nThe problem states that $V_r$ has orthonormal columns, so $V_r^\\top V_r = I_r$. The induced $2$-norm of $V_r^\\top$ is its largest singular value, which is $1$. The term $\\left\\| N(V_r a^{\\star}(\\mu),\\mu) - \\widetilde{N}(V_r a^{\\star}(\\mu),\\mu) \\right\\|_2$ is defined as the DEIM nonlinearity approximation error, $e_N(\\mu)$.\nThis leads to the first-order bound on the RB coefficient error:\n$$\n\\left\\| a^{\\text{deim}}(\\mu) - a^\\star(\\mu) \\right\\|_2 \\;\\lesssim\\; \\left\\| J_r(a^\\star(\\mu);\\mu)^{-1} \\right\\|_2 \\, e_N(\\mu).\n$$\nThe Jacobian $J_r(a;\\mu)$ is given by $D_a F_r(a;\\mu) = V_r^\\top (K + \\mu M) V_r + V_r^\\top D_uN(V_ra,\\mu) V_r$.\n\nNow, we derive the bound for the RB state error, $\\delta u_r(\\mu) = u_r^{\\text{deim}}(\\mu) - u_r^\\text{exact}(\\mu)$.\n$$\n\\delta u_r(\\mu) \\;=\\; V_r a^{\\text{deim}}(\\mu) - V_r a^\\star(\\mu) \\;=\\; V_r \\left( a^{\\text{deim}}(\\mu) - a^\\star(\\mu) \\right) = V_r \\delta a(\\mu).\n$$\nTaking the Euclidean norm:\n$$\n\\left\\| \\delta u_r(\\mu) \\right\\|_2 = \\left\\| V_r \\delta a(\\mu) \\right\\|_2.\n$$\nSince $V_r$ has orthonormal columns, it acts as an isometry on vectors in its range. Therefore, $\\left\\| V_r x \\right\\|_2 = \\left\\| x \\right\\|_2$ for any $x \\in \\mathbb{R}^r$.\n$$\n\\left\\| u_r^{\\text{deim}}(\\mu) - u_r^\\text{exact}(\\mu) \\right\\|_2 = \\left\\| a^{\\text{deim}}(\\mu) - a^\\star(\\mu) \\right\\|_2.\n$$\nThe first-order bound on the state error, $B_u(\\mu)$, is thus the same as the bound on the coefficient error:\n$$\nB_u(\\mu) \\;=\\; \\left\\| J_r(a^\\star(\\mu);\\mu)^{-1} \\right\\|_2 \\, e_N(\\mu).\n$$\n\nFinally, we derive the bound for the output error, $\\delta y_r(\\mu) = y_r^{\\text{deim}}(\\mu) - y_r^\\text{exact}(\\mu)$.\n$$\n\\delta y_r(\\mu) \\;=\\; c^\\top u_r^{\\text{deim}}(\\mu) - c^\\top u_r^\\text{exact}(\\mu) \\;=\\; c^\\top \\left( u_r^{\\text{deim}}(\\mu) - u_r^\\text{exact}(\\mu) \\right) = c^\\top \\delta u_r(\\mu).\n$$\nTaking the absolute value and applying the Cauchy-Schwarz inequality:\n$$\n\\left| \\delta y_r(\\mu) \\right| \\;=\\; \\left| c^\\top \\delta u_r(\\mu) \\right| \\;\\le\\; \\left\\| c \\right\\|_2 \\left\\| \\delta u_r(\\mu) \\right\\|_2.\n$$\nSubstituting the bound for the state error norm, we obtain the first-order bound on the output error, $B_y(\\mu)$:\n$$\nB_y(\\mu) \\;=\\; \\left\\| c \\right\\|_2 B_u(\\mu) \\;=\\; \\left\\| c \\right\\|_2 \\left\\| J_r(a^\\star(\\mu);\\mu)^{-1} \\right\\|_2 \\, e_N(\\mu).\n$$\n\n### Task 2: Implementation Details\n\nThe implementation constructs the specified mathematical objects and numerical methods to validate the derived bounds.\n1.  **Model Construction**: $K \\in \\mathbb{R}^{n \\times n}$ is taken as the second-order finite difference matrix for the $1$D negative Laplacian $(-1, 2, -1)$ on a grid of size $n$, which is symmetric positive definite (SPD). $M \\in \\mathbb{R}^{n \\times n}$ is the identity matrix, also SPD. The forcing vector $b(\\mu)$ and output functional vector $c$ are chosen to be deterministic functions of the spatial coordinate. The nonlinear scaling function is $\\alpha(\\mu) = 0.1 e^\\mu$.\n2.  **Offline Stage**:\n    *   **Snapshot Generation**: A set of training parameters $\\mu_{\\text{train}}$ is defined. For each $\\mu_i \\in \\mu_{\\text{train}}$, the high-fidelity system $F(u;\\mu_i)=0$ is solved for $u^\\star(\\mu_i)$ using Newton's method. The solutions form the columns of the state snapshot matrix $S_u$. The corresponding nonlinear terms $N(u^\\star(\\mu_i), \\mu_i)$ form the nonlinear snapshot matrix $S_N$.\n    *   **Basis Generation**: The state basis $V_r$ is computed via Proper Orthogonal Decomposition (POD), which corresponds to the first $r$ left singular vectors of $S_u$. Similarly, the nonlinear basis $U_m$ is computed from the first $m$ left singular vectors of $S_N$.\n    *   **DEIM Setup**: The DEIM interpolation indices are computed using the standard greedy algorithm, which is implemented efficiently via the pivoted QR decomposition of $U_m^\\top$. The matrix $(P^\\top U_m)^{-1}$ required for the online stage is pre-computed.\n3.  **Online Stage**:\n    *   For each test parameter $\\mu$, the RB-exact and RB-DEIM systems are solved using separate Newton solvers. These solvers operate on the small $r$-dimensional systems.\n    *   The RB-exact solution $a^\\star(\\mu)$ is used to compute the quantities needed for the error bounds: the nonlinearity error $e_N(\\mu)$ and the norm of the inverse RB Jacobian $\\|J_r(a^\\star(\\mu);\\mu)^{-1}\\|_2$.\n    *   The computed RB-DEIM solution $a^{\\text{deim}}(\\mu)$ is used to find the true state error $\\|u_r^{\\text{deim}} - u_r^\\text{exact}\\|_2$ and output error $|y_r^{\\text{deim}} - y_r^\\text{exact}|$.\n    *   These true errors are then compared against the derived a posteriori bounds $B_u(\\mu)$ and $B_y(\\mu)$. The results are collected and formatted as specified.",
            "answer": "```python\nimport numpy as np\nfrom scipy.linalg import svd, qr, inv, norm\n\ndef solve():\n    \"\"\"\n    Main function to perform the reduced-basis modeling and error analysis.\n    \"\"\"\n    # -- 0. Problem Setup --\n    n = 101  # Full-order dimension\n    r = 10   # Reduced basis dimension for state\n    m = 15   # Reduced basis dimension for nonlinearity\n    \n    test_cases = [0.20, 0.05, 1.50, 0.80]\n    \n    # Reproducible model construction\n    np.random.seed(0) # for reproducibility if any random part was added\n    # K: 1D discrete Laplacian, SPD\n    K = np.diag(2 * np.ones(n)) - np.diag(np.ones(n - 1), 1) - np.diag(np.ones(n - 1), -1)\n    K *= (n - 1)**2 # Scaling for grid spacing h=1/(n-1)\n    # M: Mass matrix (identity)\n    M = np.identity(n)\n    # Spatial coordinate\n    x_coords = np.linspace(0, 1, n)\n    # Output vector c: Point evaluation at center\n    c = np.zeros(n)\n    c[n // 2] = 1.0\n    \n    def alpha_func(mu):\n        return 0.1 * np.exp(mu)\n\n    def b_func(mu, x):\n        return (1.0 + mu) * np.sin(np.pi * x) * 100\n\n    def N_func(u, mu):\n        return alpha_func(mu) * np.sinh(u)\n\n    def DN_func(u, mu):\n        return alpha_func(mu) * np.cosh(u)\n\n    def newton_solver(F, J, u0, tol=1e-10, max_iter=50):\n        u = u0.copy()\n        for _ in range(max_iter):\n            res = F(u)\n            jac = J(u)\n            delta_u = np.linalg.solve(jac, -res)\n            u += delta_u\n            if np.linalg.norm(delta_u) < tol:\n                return u\n        raise RuntimeError(\"Newton solver did not converge.\")\n\n    # -- 1. Offline Stage: Basis Generation --\n    \n    # Training parameters\n    train_mu = np.linspace(0.05, 1.5, 20)\n    \n    # Generate full-order snapshots\n    S_u = np.zeros((n, len(train_mu)))\n    S_N = np.zeros((n, len(train_mu)))\n    \n    u_fom_prev = np.zeros(n)\n    for i, mu_train in enumerate(train_mu):\n        b_train = b_func(mu_train, x_coords)\n        \n        def fom_residual(u):\n            return (K + mu_train * M) @ u + N_func(u, mu_train) - b_train\n        \n        def fom_jacobian(u):\n            return K + mu_train * M + np.diag(DN_func(u, mu_train))\n\n        u_star = newton_solver(fom_residual, fom_jacobian, u_fom_prev)\n        S_u[:, i] = u_star\n        S_N[:, i] = N_func(u_star, mu_train)\n        u_fom_prev = u_star\n\n    # POD for state basis Vr\n    U_svd_u, _, _ = svd(S_u, full_matrices=False)\n    V_r = U_svd_u[:, :r]\n\n    # POD for nonlinearity basis Um\n    U_svd_n, _, _ = svd(S_N, full_matrices=False)\n    U_m = U_svd_n[:, :m]\n    \n    # DEIM indices via pivoted QR\n    _, _, p_indices = qr(U_m.T, pivoting=True)\n    p = p_indices[:m]\n\n    # Precompute online-efficient matrices\n    K_r = V_r.T @ K @ V_r\n    M_r = V_r.T @ M @ V_r\n    inv_PT_Um = inv(U_m[p, :])\n    VrT_Um = V_r.T @ U_m\n    norm_c = norm(c)\n    Vr_p = V_r[p, :]\n\n    # -- 2. Online Stage: Solve for test cases --\n    all_results = []\n    a_prev = np.zeros(r)\n\n    for mu_test in test_cases:\n        b_test = b_func(mu_test, x_coords)\n        b_r = V_r.T @ b_test\n        alpha_test = alpha_func(mu_test)\n\n        # -- Solve RB-exact system --\n        def rb_exact_res(a):\n            u_r = V_r @ a\n            return (K_r + mu_test * M_r) @ a + V_r.T @ N_func(u_r, mu_test) - b_r\n\n        def rb_exact_jac(a):\n            u_r = V_r @ a\n            d_N = DN_func(u_r, mu_test)[:, np.newaxis]\n            return K_r + mu_test * M_r + V_r.T @ (d_N * V_r)\n        \n        a_star = newton_solver(rb_exact_res, rb_exact_jac, a_prev)\n        u_r_exact = V_r @ a_star\n        y_r_exact = c.T @ u_r_exact\n\n        # -- Solve RB-DEIM system --\n        def rb_deim_res(a):\n            u_r_p = (V_r @ a)[p]\n            N_approx_proj = VrT_Um @ inv_PT_Um @ (alpha_test * np.sinh(u_r_p))\n            return (K_r + mu_test * M_r) @ a + N_approx_proj - b_r\n        \n        def rb_deim_jac(a):\n            u_r_p = (V_r @ a)[p]\n            diag_term = np.diag(alpha_test * np.cosh(u_r_p))\n            return K_r + mu_test * M_r + VrT_Um @ inv_PT_Um @ diag_term @ Vr_p\n\n        a_deim = newton_solver(rb_deim_res, rb_deim_jac, a_star)\n        u_r_deim = V_r @ a_deim\n        y_r_deim = c.T @ u_r_deim\n        a_prev = a_deim\n\n        # -- Compute errors and bounds --\n        # Nonlinearity error e_N\n        N_at_ur_exact = N_func(u_r_exact, mu_test)\n        N_tilde_at_ur_exact = U_m @ inv_PT_Um @ N_at_ur_exact[p]\n        e_N = norm(N_at_ur_exact - N_tilde_at_ur_exact)\n\n        # Actual state and output errors\n        state_err = norm(u_r_deim - u_r_exact)\n        output_err = abs(y_r_deim - y_r_exact)\n\n        # Error bounds B_u, B_y\n        J_r_at_astar = rb_exact_jac(a_star)\n        inv_Jr_norm = norm(inv(J_r_at_astar), 2)\n        \n        B_u = inv_Jr_norm * e_N\n        B_y = norm_c * B_u\n        \n        case_results = [\n            round(e_N, 6),\n            round(state_err, 6),\n            round(B_u, 6),\n            round(output_err, 6),\n            round(B_y, 6)\n        ]\n        all_results.append(case_results)\n\n    # -- 3. Format and Print Output --\n    output_str = str(all_results).replace(\" \", \"\")\n    print(output_str)\n\nsolve()\n```"
        }
    ]
}