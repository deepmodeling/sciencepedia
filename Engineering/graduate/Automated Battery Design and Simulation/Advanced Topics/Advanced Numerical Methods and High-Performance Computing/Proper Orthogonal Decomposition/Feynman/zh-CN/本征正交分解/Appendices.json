{
    "hands_on_practices": [
        {
            "introduction": "选择降阶模型的阶数$r$是POD应用中的一个核心挑战。单纯依赖能量占比可能会导致模型对训练数据过拟合，泛化能力差。本练习将指导您实践一种强大的数据驱动方法——交叉验证，通过评估模型对未见数据的重构能力来选择最优阶数，从而确保模型的泛化性能。",
            "id": "3178045",
            "problem": "你的任务是实现一个用于固有正交分解 (POD) 的模型阶数选择程序。该程序通过重构留出的快照并量化泛化误差，使用交叉验证方法。固有正交分解 (POD) 旨在寻找一组标准正交模态，以便在欧几里得范数下最优地捕捉一组快照中的方差。使用奇异值分解 (SVD) 从训练快照中计算标准正交模态，并使用正交投影来重构留出的快照。使用 K 折交叉验证 (CV) 来估计泛化误差。你的程序必须通过最小化平均留出重构误差，为几个指定的数据集计算最佳秩 $r$，并且必须以指定格式生成最终结果。\n\n使用以下基本原理：\n- 正交投影的线性代数：对于数据矩阵 $X \\in \\mathbb{R}^{m \\times n}$，如果 $U_r \\in \\mathbb{R}^{m \\times r}$ 具有标准正交列，那么一个中心化向量 $x \\in \\mathbb{R}^m$ 到由 $U_r$ 的列所张成的子空间上的正交投影是 $U_r U_r^\\top x$。\n- 奇异值分解 (SVD)：任何实数矩阵 $A \\in \\mathbb{R}^{m \\times n}$ 都可以分解为 $A = U \\Sigma V^\\top$，其中 $U \\in \\mathbb{R}^{m \\times q}$ 和 $V \\in \\mathbb{R}^{n \\times q}$ 具有标准正交列，$\\Sigma \\in \\mathbb{R}^{q \\times q}$ 是对角线上为非负奇异值的对角矩阵，且 $q = \\min(m,n)$。\n- 对于中心化的训练数据，在最小二乘意义下的最佳秩-$r$ 子空间是由前 $r$ 个左奇异向量张成的。\n\n对于给定的快照矩阵 $X \\in \\mathbb{R}^{m \\times n}$、折数 $K \\in \\mathbb{N}$（其中 $2 \\le K \\le n$）以及候选秩集合 $\\mathcal{R} = \\{0,1,2,\\dots,r_{\\max}\\}$，你的算法必须实现以下过程：\n- 将列索引 $\\{0,1,\\dots,n-1\\}$ 确定性地划分为 $K$ 折：对于第 $k$ 折（$k \\in \\{0,1,\\dots,K-1\\}$），验证集是索引集合 $J_k = \\{ j \\in \\{0,\\dots,n-1\\} : j \\bmod K = k \\}$；训练集是其补集。\n- 对于每个候选秩 $r \\in \\mathcal{R}$：\n  1. 对于每一折 $k$：\n     - 通过选择索引不在 $J_k$ 中的 $X$ 的列来构成训练矩阵 $X_{\\text{train}}$，并使用 $J_k$ 索引的列构成验证矩阵 $X_{\\text{val}}$。\n     - 计算训练均值 $\\mu = \\frac{1}{n_{\\text{train}}} \\sum_{x \\in X_{\\text{train}}} x$。\n     - 将数据中心化：$A = X_{\\text{train}} - \\mu \\mathbf{1}^\\top$ 和 $B = X_{\\text{val}} - \\mu \\mathbf{1}^\\top$。\n     - 计算奇异值分解 $A = U \\Sigma V^\\top$，其中 $U \\in \\mathbb{R}^{m \\times q}$，$q = \\min(m, n_{\\text{train}})$，并通过取 $U$ 的前 $r$ 列来构成 $U_r$（如果 $r = 0$，则将 $U_r$ 定义为空矩阵，从而使投影为零算子）。\n     - 通过正交投影重构验证数据：$\\widehat{X}_{\\text{val}} = \\mu \\mathbf{1}^\\top + U_r U_r^\\top B$。\n     - 计算该折的误差，即相对弗罗贝尼乌斯范数 $e_k(r) = \\frac{\\| X_{\\text{val}} - \\widehat{X}_{\\text{val}} \\|_F}{\\| X_{\\text{val}} \\|_F}$。\n  2. 计算交叉验证误差，即平均值 $\\overline{e}(r) = \\frac{1}{K} \\sum_{k=0}^{K-1} e_k(r)$。\n- 选择最小化 $\\overline{e}(r)$ 的最佳秩 $r^\\star$，若存在多个相同的最小值，则选择最小的 $r$。在数值计算中，如果两个误差的绝对差小于 $\\varepsilon = 10^{-12}$，则视它们相等。\n\n实现上述过程，并将其应用于以下测试套件。在所有情况下，$X$ 的列都是待重构的快照。所有数据都纯粹是数学上的，不涉及物理单位。\n\n测试用例 1（理想情况，秩为 1 的数据）：\n- 维度：$m = 5$, $n = 8$。\n- 构造一个非零向量 $v \\in \\mathbb{R}^5$ 为 $v = [1,2,3,4,5]^\\top$ 和标量 $s = [1,2,3,4,5,6,7,8]$。\n- 对 $j \\in \\{0,\\dots,7\\}$，构成快照 $x_j = s_j \\, v$，并组装成 $X = [x_0, x_1, \\dots, x_7]$。\n- 使用 $K = 4$ 和候选秩 $\\mathcal{R} = \\{0,1,2,3,4\\}$。\n\n测试用例 2（秩为 2 的数据，多折覆盖）：\n- 维度：$m = 6$, $n = 9$。\n- 令 $U_2 \\in \\mathbb{R}^{6 \\times 2}$ 为 $6 \\times 6$ 单位矩阵的前两列。\n- 令系数矩阵 $C \\in \\mathbb{R}^{2 \\times 9}$ 的列为\n  $$\n  C = \\begin{bmatrix}\n  1  0  1  2  1  2  3  1  3 \\\\\n  0  1  1  1  2  2  1  3  3\n  \\end{bmatrix}.\n  $$\n- 构成 $X = U_2 C$。\n- 使用 $K = 3$ 和候选秩 $\\mathcal{R} = \\{0,1,2,3,4,5\\}$。\n\n测试用例 3（边界情况 $r = 0$ 最优；恒定快照）：\n- 维度：$m = 4$, $n = 7$。\n- 令 $c = 3$ 且 $u = [1,1,1,1]^\\top$。\n- 对 $j \\in \\{0,\\dots,6\\}$，将所有快照设置为 $x_j = c \\, u$，并构成 $X = [x_0, \\dots, x_6]$。\n- 使用 $K = 7$ 和候选秩 $\\mathcal{R} = \\{0,1,2,3\\}$。\n\n测试用例 4（具有更高环境维度的秩为 3 的数据）：\n- 维度：$m = 7$, $n = 12$。\n- 令 $U_3 \\in \\mathbb{R}^{7 \\times 3}$ 为 $7 \\times 7$ 单位矩阵的前三列。\n- 令系数矩阵 $C \\in \\mathbb{R}^{3 \\times 12}$ 的列为\n  $$\n  C = \\begin{bmatrix}\n  1  0  0  1  1  0  1  2  0  1  2  1 \\\\\n  0  1  0  1  0  1  1  1  2  0  2  2 \\\\\n  0  0  1  0  1  1  1  0  1  2  1  2\n  \\end{bmatrix}.\n  $$\n- 构成 $X = U_3 C$。\n- 使用 $K = 4$ 和候选秩 $\\mathcal{R} = \\{0,1,2,3,4,5,6\\}$。\n\n计算细节：\n- 在每一折中，始终只使用训练列进行均值中心化，并在投影后将均值加回。\n- 使用正交投影 $U_r U_r^\\top$ 来重构中心化的验证数据。\n- 使用相对弗罗贝尼乌斯范数计算误差。\n- 如果 $r = 0$，将重构定义为在所有验证列上复制的均值 $\\mu$。\n- 平局打破容差为 $\\varepsilon = 10^{-12}$。\n\n最终输出格式：\n- 你的程序应生成单行输出，其中包含四个测试用例所选出的秩，形式为方括号内以逗号分隔的列表，例如 $[r_1,r_2,r_3,r_4]$，其中每个 $r_i$ 都是一个整数。输出行中不得有空格。",
            "solution": "该解决方案实现了一个使用 K 折交叉验证的固有正交分解 (POD) 模型阶数选择程序。该方法通过平衡模型保真度与对未见数据的泛化能力，系统地确定表示数据集所需的最佳基向量数量（即秩 $r$）。\n\nPOD 背后的核心原理是找到一个标准正交基，以最优方式（通常在最小二乘意义上）表示一组给定的数据快照。对于 $\\mathbb{R}^m$ 中的一组 $n$ 个快照，它们被组织成矩阵 $X \\in \\mathbb{R}^{m \\times n}$ 的列，我们首先计算这些快照的时间均值 $\\mu = \\frac{1}{n} \\sum_{i=0}^{n-1} x_i$。围绕均值的波动被捕获在中心化数据矩阵 $A = X - \\mu \\mathbf{1}^\\top$ 中。该矩阵的奇异值分解 (SVD)，即 $A = U \\Sigma V^\\top$，提供了最优基。$U$ 的列是 POD 模态，它们是标准正交向量。$\\Sigma$ 对角线上的奇异值按降序排列，$\\sigma_0 \\ge \\sigma_1 \\ge \\dots$，量化了每个相应模态所捕获的方差（或“能量”）量。Eckart-Young-Mirsky 定理保证了由 $U$ 的前 $r$ 列张成的子空间是逼近 $A$ 的列的最佳秩-$r$ 子空间。\n\n问题在于选择秩 $r$。过低的秩可能导致数据欠拟合，而过高的秩可能通过捕获训练数据特有的噪声和其他伪影而导致过拟合，从而导致泛化能力差。交叉验证是估计这种泛化性能的标准技术。所提供的算法使用 K 折交叉验证。快照数据集被划分为 $K$ 个不相交的子集（折）。该过程迭代 $K$ 次；在每次迭代 $k$ 中，将一折作为验证集（$X_{\\text{val}}$）留出，其余的 $K-1$ 折用作训练集（$X_{\\text{train}}$）。\n\n该算法对候选集 $\\mathcal{R}$ 中的每个候选秩 $r$ 按如下方式进行：\n1. 对于每一折 $k \\in \\{0, \\dots, K-1\\}$：\n   a. 均值 $\\mu$ *仅* 从训练数据 $X_{\\text{train}}$ 计算。这对于避免验证集的信息泄漏到模型训练过程中至关重要。\n   b. 训练数据和验证数据都使用此训练均值进行中心化：$A = X_{\\text{train}} - \\mu \\mathbf{1}^\\top$ 和 $B = X_{\\text{val}} - \\mu \\mathbf{1}^\\top$。\n   c. 计算中心化训练矩阵 $A$ 的 SVD 以找到 POD 模态，从而得到矩阵 $U$。$U$ 的前 $r$ 列构成秩-$r$ 基 $U_r$。\n   d. $B$ 中的中心化验证快照通过正交投影到由 $U_r$ 的列所张成的子空间上来重构。重构后的中心化数据为 $\\widehat{B} = U_r U_r^\\top B$。对于 $r=0$ 的情况，此投影为零算子，因此 $\\widehat{B}$ 是一个零矩阵。\n   e. 完整的重构快照则为 $\\widehat{X}_{\\text{val}} = \\mu \\mathbf{1}^\\top + \\widehat{B}$。对于 $r=0$，这意味着重构结果就是训练均值。\n   f. 该折的重构误差使用相对弗罗贝尼乌斯范数进行量化：$e_k(r) = \\frac{\\| X_{\\text{val}} - \\widehat{X}_{\\text{val}} \\|_F}{\\| X_{\\text{val}} \\|_F}$。这衡量了误差相对于原始验证数据的大小。\n\n2. 在遍历所有 $K$ 折之后，对给定秩 $r$ 的误差进行平均，以产生交叉验证误差：$\\overline{e}(r) = \\frac{1}{K} \\sum_{k=0}^{K-1} e_k(r)$。与单次训练-测试划分相比，此平均误差为模型的泛化性能提供了更稳健的估计。\n\n3. 最后，选择使平均交叉验证误差 $\\overline{e}(r)$ 最小化的最佳秩 $r^\\star$。根据问题规范，任何平局都通过选择最小的秩来打破，这遵循了简约性原则。使用 $\\varepsilon = 10^{-12}$ 的数值容差来确定两个误差值是否相等。\n\n将此程序应用于指定的四个测试用例中的每一个，从而为每个数据集得出最佳秩。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef _find_best_rank(X, K, r_candidates):\n    \"\"\"\n    Finds the best POD rank using K-fold cross-validation as specified.\n\n    Args:\n        X (np.ndarray): Snapshot matrix of size (m, n).\n        K (int): Number of folds for cross-validation.\n        r_candidates (list): List of candidate ranks to evaluate.\n\n    Returns:\n        int: The best rank found.\n    \"\"\"\n    m, n = X.shape\n    epsilon = 1e-12\n    avg_errors = []\n\n    all_indices = np.arange(n)\n\n    for r in r_candidates:\n        fold_errors = []\n        for k in range(K):\n            # 1. Partition data deterministically into training and validation sets\n            val_indices = all_indices[k::K]\n            train_indices = np.setdiff1d(all_indices, val_indices, assume_unique=True)\n\n            X_train = X[:, train_indices]\n            X_val = X[:, val_indices]\n            \n            # 2. Compute training mean\n            mu = np.mean(X_train, axis=1, keepdims=True)\n\n            # 3. Center data using training mean\n            A = X_train - mu\n            B = X_val - mu\n            \n            # 4. Compute SVD of centered training data\n            # Use full_matrices=False for thin SVD\n            U, _, _ = np.linalg.svd(A, full_matrices=False)\n            \n            # 5. Reconstruct validation data via projection\n            if r == 0:\n                B_recon = np.zeros_like(B)\n            else:\n                # Ensure we don't request more modes than available\n                rank_limit = min(r, U.shape[1])\n                Ur = U[:, :rank_limit]\n                # Orthogonal projection of centered validation data\n                B_recon = Ur @ (Ur.T @ B)\n            \n            # Reconstruct the full validation data for error calculation\n            X_val_recon = mu + B_recon\n\n            # 6. Compute fold error as relative Frobenius norm\n            norm_diff = np.linalg.norm(X_val - X_val_recon, 'fro')\n            norm_val = np.linalg.norm(X_val, 'fro')\n\n            if norm_val  epsilon:\n                # If validation data norm is effectively zero, the relative error\n                # is 0 if the difference is also zero, otherwise handle appropriately.\n                fold_error = 0.0 if norm_diff  epsilon else 1.0\n            else:\n                fold_error = norm_diff / norm_val\n            \n            fold_errors.append(fold_error)\n\n        # 7. Compute average error across folds for the current rank r\n        avg_errors.append(np.mean(fold_errors))\n\n    # 8. Select best rank: minimizes average error, with smallest rank as tie-breaker\n    avg_errors = np.array(avg_errors)\n    min_error = np.min(avg_errors)\n    \n    # Find all ranks that achieve the minimum error within the tolerance\n    best_indices = np.where(avg_errors = min_error + epsilon)[0]\n\n    # The first index corresponds to the smallest rank due to sorted r_candidates\n    best_rank = r_candidates[best_indices[0]]\n    \n    return best_rank\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = []\n    \n    # Test case 1 (happy path, rank-1 data)\n    m1, n1 = 5, 8\n    v1 = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)\n    s1 = np.arange(1, 9)\n    X1 = v1 @ s1.reshape(1, -1)\n    K1, R1 = 4, list(range(5))\n    test_cases.append((X1, K1, R1))\n\n    # Test case 2 (rank-2 data, multi-fold coverage)\n    m2, n2 = 6, 9\n    U_basis2 = np.eye(m2)[:, :2]\n    C2 = np.array([\n        [1, 0, 1, 2, 1, 2, 3, 1, 3],\n        [0, 1, 1, 1, 2, 2, 1, 3, 3]\n    ], dtype=float)\n    X2 = U_basis2 @ C2\n    K2, R2 = 3, list(range(6))\n    test_cases.append((X2, K2, R2))\n\n    # Test case 3 (boundary case r = 0 optimal; constant snapshots)\n    m3, n3 = 4, 7\n    c3 = 3.0\n    u3 = np.ones((m3, 1))\n    X3 = c3 * u3 @ np.ones((1, n3))\n    K3, R3 = 7, list(range(4))\n    test_cases.append((X3, K3, R3))\n    \n    # Test case 4 (rank-3 data with higher ambient dimension)\n    m4, n4 = 7, 12\n    U_basis4 = np.eye(m4)[:, :3]\n    C4 = np.array([\n        [1, 0, 0, 1, 1, 0, 1, 2, 0, 1, 2, 1],\n        [0, 1, 0, 1, 0, 1, 1, 1, 2, 0, 2, 2],\n        [0, 0, 1, 0, 1, 1, 1, 0, 1, 2, 1, 2]\n    ], dtype=float)\n    X4 = U_basis4 @ C4\n    K4, R4 = 4, list(range(7))\n    test_cases.append((X4, K4, R4))\n\n    results = []\n    for X, K, r_candidates in test_cases:\n        best_rank = _find_best_rank(X, K, r_candidates)\n        results.append(best_rank)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在工程应用中，通用的泛化能力往往不够，降阶模型必须满足特定的任务性能指标，例如电池管理系统（BMS）对电压预测精度的严苛要求。本练习将引导您从第一性原理出发，推导一个复合准则来选择模型阶数。该准则巧妙地结合了能量捕获、模型稳定性（谱隙）和面向任务的误差界限，体现了面向特定工程目标的建模思想。",
            "id": "3943530",
            "problem": "一个用于自动化电池设计和仿真的锂离子电池模型，在 $N=500$ 条不同的负载轨迹下进行采样，以构建一个快照矩阵 $\\mathbf{X} \\in \\mathbb{R}^{n \\times N}$，其列为状态向量。本征正交分解 (Proper Orthogonal Decomposition, POD) 是通过对快照矩阵进行奇异值分解 $\\mathbf{X} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^{\\top}$ 来构建的，其中奇异值在 $\\mathbf{\\Sigma}$ 的对角线上有序排列，即 $\\sigma_{1} \\ge \\sigma_{2} \\ge \\dots \\ge \\sigma_{m} > 0$，且 $m=\\min\\{n,N\\}$。在一个秩为 $r$ 的降阶模型中，状态被投影到前 $r$ 个 POD 模态所张成的空间上。此处，电池管理系统 (Battery Management System, BMS) 的合规性定义为满足相对于高保真模型的均方根端电压误差要求。\n\n从 POD 和奇异值分解的基本定义出发，并利用端电压 $V$ 在某个工作轨迹周围的线性化（使得增量电压映射由谱范数为 $\\|\\mathbf{H}\\|_{2}$ 的线性算子 $\\mathbf{H}$ 表示），推导出一个原则性准则，用以选择能够同时满足以下三个要求的最小秩 $r$：\n- 累积能量捕获超过阈值 $0  \\alpha  1$，\n- 一个用于保证稳健性的谱隙阈值，以及\n- 一个特定于任务的均方根端电压误差界限。\n\n假设以下数据是科学合理且自洽的：\n- 奇异值数量为 $m=12$，具体值为\n  $\\sigma_{1}=3.50$, $\\sigma_{2}=1.85$, $\\sigma_{3}=1.02$, $\\sigma_{4}=0.58$, $\\sigma_{5}=0.33$, $\\sigma_{6}=0.18$, $\\sigma_{7}=0.11$, $\\sigma_{8}=0.07$, $\\sigma_{9}=0.045$, $\\sigma_{10}=0.028$, $\\sigma_{11}=0.017$, $\\sigma_{12}=0.010$；\n- 电压线性化的算子范数为 $\\|\\mathbf{H}\\|_{2} = 0.20$，单位为伏特/单位状态范数；\n- BMS 合规所允许的最大均方根端电压误差为 $\\epsilon_{V} = 0.0025$ 伏特；所有与电压相关的量均以伏特表示和计算；\n- 累积能量捕获阈值为 $\\alpha = 0.995$；\n- 谱隙阈值是一个差值准则 $\\delta = 0.10$，其中秩为 $r$ 时的谱隙定义为 $\\Delta_{r} = \\sigma_{r} - \\sigma_{r+1}$。\n\n仅使用关于 POD/SVD 误差表示和线性算子范数的核心定义和经过充分验证的定理，推导组合的秩选择准则，然后将其应用于给定数据，计算满足所有三个要求的最小整数秩 $r$。最终答案以整数形式表示。除精确的整数求值外，不需要进行四舍五入。",
            "solution": "该问题要求为一个基于本征正交分解 (POD) 的锂离子电池降阶模型确定最小的整数模型秩 $r$。秩 $r$ 必须同时满足三个不同的要求：累积能量捕获阈值、谱隙稳健性准则以及特定于任务的均方根 (RMS) 端电压误差界限。我们首先从基本原理出发，推导每个准则的数学表达式。\n\nPOD 基是通过快照矩阵 $\\mathbf{X} \\in \\mathbb{R}^{n \\times N}$ 的奇异值分解 (SVD) 构建的，即 $\\mathbf{X} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^{\\top}$。$\\mathbf{U}$ 的列是 POD 模态，$\\mathbf{\\Sigma}$ 的对角线元素是奇异值 $\\sigma_k$，其排序满足 $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_m > 0$，其中 $m=\\min\\{n,N\\}$。\n\n快照的总“能量”由 $\\mathbf{X}$ 的弗罗贝尼乌斯范数的平方来量化，它通过恒等式 $\\|\\mathbf{X}\\|_F^2 = \\sum_{k=1}^{m} \\sigma_k^2$ 与奇异值相关。一个秩为 $r$ 的降阶模型捕获了这部分能量。\n\n**准则 1：累积能量捕获**\n\n前 $r$ 个 POD 模态捕获的能量是相应奇异值平方和，即 $E_r = \\sum_{k=1}^{r} \\sigma_k^2$。累积能量捕获分数是捕获能量与总能量之比。第一个准则要求该分数超过一个阈值 $\\alpha \\in (0,1)$。\n\n$$ C_1(r): \\quad \\frac{\\sum_{k=1}^{r} \\sigma_k^2}{\\sum_{k=1}^{m} \\sigma_k^2} \\ge \\alpha $$\n\n**准则 2：谱隙阈值**\n\n连续奇异值 $\\sigma_r$ 和 $\\sigma_{r+1}$ 之间较大的差距表明模态谱的保留部分和截断部分之间存在自然分离，从而为秩 $r$ 的选择提供了稳健性。谱隙定义为 $\\Delta_r = \\sigma_r - \\sigma_{r+1}$。第二个准则要求此差距大于指定的阈值 $\\delta > 0$。\n\n$$ C_2(r): \\quad \\sigma_r - \\sigma_{r+1} \\ge \\delta $$\n这个准则对 $r=m$ 没有定义，因为 $\\sigma_{m+1}$ 未给出（其值应为 $0$）。因此，我们考虑 $r \\in \\{1, 2, \\dots, m-1\\}$。\n\n**准则 3：均方根端电压误差**\n\n状态向量 $\\mathbf{s}$（$\\mathbf{X}$ 的一列）的秩-$r$ 截断是其在前 $r$ 个 POD 模态上的投影，$\\mathbf{s}_r = \\mathbf{U}_r\\mathbf{U}_r^{\\top}\\mathbf{s}$，其中 $\\mathbf{U}_r$ 包含 $\\mathbf{U}$ 的前 $r$ 列。状态截断误差为 $\\mathbf{e}_s = \\mathbf{s} - \\mathbf{s}_r$。\n\n端电压 $V$ 是状态的线性化函数，因此由状态截断引起的电压误差 $\\epsilon_{V, \\text{inst}}$ 为 $\\epsilon_{V, \\text{inst}} = V(\\mathbf{s}) - V(\\mathbf{s}_r) \\approx \\mathbf{H}(\\mathbf{s} - \\mathbf{s}_r)$，其中 $\\mathbf{H}$ 是表示增量电压映射的线性算子。\n\n均方根端电压误差 $\\epsilon_{V, \\text{RMS}}$ 是对矩阵 $\\mathbf{X}$ 中的 $N$ 个快照 $\\mathbf{x}_j$ 计算得出的。\n$$ \\epsilon_{V, \\text{RMS}} = \\sqrt{\\frac{1}{N} \\sum_{j=1}^{N} \\left(V(\\mathbf{x}_j) - V(\\mathbf{x}_{j,r})\\right)^2} \\approx \\sqrt{\\frac{1}{N} \\sum_{j=1}^{N} \\left(\\mathbf{H}(\\mathbf{x}_j - \\mathbf{x}_{j,r})\\right)^2} $$\n使用诱导 2-范数（谱范数）的性质 $|\\mathbf{H} \\mathbf{v}| \\le \\|\\mathbf{H}\\|_2 \\|\\mathbf{v}\\|_2$，我们可以对均方根误差进行限定：\n$$ \\epsilon_{V, \\text{RMS}} \\le \\sqrt{\\frac{1}{N} \\sum_{j=1}^{N} \\left(\\|\\mathbf{H}\\|_2 \\|\\mathbf{x}_j - \\mathbf{x}_{j,r}\\|_2\\right)^2} = \\|\\mathbf{H}\\|_2 \\sqrt{\\frac{1}{N} \\sum_{j=1}^{N} \\|\\mathbf{x}_j - \\mathbf{x}_{j,r}\\|_2^2} $$\n所有快照的平方误差之和是误差矩阵的弗罗贝尼乌斯范数的平方，即 $\\|\\mathbf{X} - \\mathbf{X}_r\\|_F^2$。根据 Eckart-Young-Mirsky 定理，这个误差是截断奇异值平方和：$\\|\\mathbf{X} - \\mathbf{X}_r\\|_F^2 = \\sum_{k=r+1}^{m} \\sigma_k^2$。\n将此代入误差界限可得：\n$$ \\epsilon_{V, \\text{RMS}} \\le \\frac{\\|\\mathbf{H}\\|_2}{\\sqrt{N}} \\sqrt{\\sum_{k=r+1}^{m} \\sigma_k^2} $$\n第三个准则要求这个误差上限小于或等于允许的最大均方根电压误差 $\\epsilon_V$。\n$$ \\frac{\\|\\mathbf{H}\\|_2}{\\sqrt{N}} \\sqrt{\\sum_{k=r+1}^{m} \\sigma_k^2} \\le \\epsilon_V $$\n对奇异值平方和进行整理，我们得到第三个准则的最终形式：\n$$ C_3(r): \\quad \\sum_{k=r+1}^{m} \\sigma_k^2 \\le \\left(\\frac{\\epsilon_V \\sqrt{N}}{\\|\\mathbf{H}\\|_2}\\right)^2 $$\n\n现在，我们将这三个准则应用于给定数据，以找到满足所有准则的最小整数 $r$。\n\n**数据：**\n- 快照数量：$N=500$\n- 奇异值数量：$m=12$\n- 奇异值 $\\sigma_k$：$\\sigma_1=3.50$, $\\sigma_2=1.85$, $\\sigma_3=1.02$, $\\sigma_4=0.58$, $\\sigma_5=0.33$, $\\sigma_6=0.18$, $\\sigma_7=0.11$, $\\sigma_8=0.07$, $\\sigma_9=0.045$, $\\sigma_{10}=0.028$, $\\sigma_{11}=0.017$, $\\sigma_{12}=0.010$\n- 算子范数：$\\|\\mathbf{H}\\|_2 = 0.20$\n- 最大电压误差：$\\epsilon_V = 0.0025$\n- 能量阈值：$\\alpha = 0.995$\n- 谱隙阈值：$\\delta = 0.10$\n\n首先，我们计算奇异值的平方 $\\sigma_k^2$：\n$\\sigma_1^2=12.25$, $\\sigma_2^2=3.4225$, $\\sigma_3^2=1.0404$, $\\sigma_4^2=0.3364$, $\\sigma_5^2=0.1089$, $\\sigma_6^2=0.0324$, $\\sigma_7^2=0.0121$, $\\sigma_8^2=0.0049$, $\\sigma_9^2=0.002025$, $\\sigma_{10}^2=0.000784$, $\\sigma_{11}^2=0.000289$, $\\sigma_{12}^2=0.0001$。\n\n总能量为 $E_{\\text{total}} = \\sum_{k=1}^{12} \\sigma_k^2 = 17.210798$。\n\n**准则 1 的评估：**\n我们需要 $\\sum_{k=1}^{r} \\sigma_k^2 \\ge \\alpha E_{\\text{total}} = 0.995 \\times 17.210798 = 17.124744$。\n- 对于 $r=4$，$\\sum_{k=1}^{4} \\sigma_k^2 = 12.25+3.4225+1.0404+0.3364 = 17.0493  17.124744$ (不通过)\n- 对于 $r=5$，$\\sum_{k=1}^{5} \\sigma_k^2 = 17.0493 + 0.1089 = 17.1582 \\ge 17.124744$ (通过)\n因此，准则 1 要求 $r \\ge 5$。\n\n**准则 2 的评估：**\n我们需要 $\\sigma_r - \\sigma_{r+1} \\ge \\delta = 0.10$。\n- 对于 $r=1$，$\\sigma_1-\\sigma_2 = 3.50-1.85 = 1.65 \\ge 0.10$ (通过)\n- 对于 $r=2$，$\\sigma_2-\\sigma_3 = 1.85-1.02 = 0.83 \\ge 0.10$ (通过)\n- 对于 $r=3$，$\\sigma_3-\\sigma_4 = 1.02-0.58 = 0.44 \\ge 0.10$ (通过)\n- 对于 $r=4$，$\\sigma_4-\\sigma_5 = 0.58-0.33 = 0.25 \\ge 0.10$ (通过)\n- 对于 $r=5$，$\\sigma_5-\\sigma_6 = 0.33-0.18 = 0.15 \\ge 0.10$ (通过)\n- 对于 $r=6$，$\\sigma_6-\\sigma_7 = 0.18-0.11 = 0.07  0.10$ (不通过)\n因此，准则 2 要求 $r \\in \\{1, 2, 3, 4, 5\\}$。\n\n**准则 3 的评估：**\n我们需要 $\\sum_{k=r+1}^{12} \\sigma_k^2 \\le \\left(\\frac{\\epsilon_V \\sqrt{N}}{\\|\\mathbf{H}\\|_2}\\right)^2$。\n阈值为 $\\left(\\frac{0.0025 \\sqrt{500}}{0.20}\\right)^2 = \\left(\\frac{0.0025 \\times 10\\sqrt{5}}{0.2}\\right)^2 = (0.125\\sqrt{5})^2 = 0.078125$。\n我们检查截断误差能量 $E_{\\text{err}}(r) = \\sum_{k=r+1}^{12} \\sigma_k^2 = E_{\\text{total}} - \\sum_{k=1}^{r} \\sigma_k^2$。\n- 对于 $r=4$，$E_{\\text{err}}(4) = E_{\\text{total}} - 17.0493 = 17.210798 - 17.0493 = 0.161498 > 0.078125$ (不通过)\n- 对于 $r=5$，$E_{\\text{err}}(5) = E_{\\text{total}} - 17.1582 = 17.210798 - 17.1582 = 0.052598 \\le 0.078125$ (通过)\n因此，准则 3 要求 $r \\ge 5$。\n\n**结论：**\n为了同时满足所有三个要求，秩 $r$ 必须位于每个准则的有效秩集合的交集中。\n- 准则 1：$r \\in \\{5, 6, 7, 8, 9, 10, 11\\}$\n- 准则 2：$r \\in \\{1, 2, 3, 4, 5\\}$\n- 准则 3：$r \\in \\{5, 6, 7, 8, 9, 10, 11\\}$\n这三个集合的交集是 $\\{5\\}$。因此，唯一满足所有条件的秩是 $r=5$。由于这是唯一的元素，它也是最小的。",
            "answer": "$$\n\\boxed{5}\n$$"
        },
        {
            "introduction": "真实的电池设计和优化需要在广阔的参数空间（如电极厚度、孔隙率）内进行探索，为这类参数化系统构建一个全局精确的降阶模型是一大挑战。本练习将带您实现一种先进的自适应基底构建方法。您将通过一个“贪心”算法，由误差估计器驱动，智能地在参数空间中寻找并添加最能挑战当前模型的快照，从而高效地构建一个紧凑且鲁棒的POD基底。",
            "id": "3943535",
            "problem": "给定一个参数化的对称正定线性系统，该系统表示在一个电池电极上，在 $N$ 个节点上离散化的稳态一维扩散-反应模型。对于参数 $u \\in \\mathbb{R}$，该系统为\n$$\nA(u) x(u) = f(u),\n$$\n其中 $A(u) \\in \\mathbb{R}^{N \\times N}$ 和 $f(u) \\in \\mathbb{R}^{N}$ 由以下具有科学依据的结构定义：\n\n- 一维网格上的离散狄利克雷拉普拉斯算子 $L \\in \\mathbb{R}^{N \\times N}$ 为\n$$\nL_{i,i} = 2,\\quad L_{i,i+1} = L_{i+1,i} = -1\\quad \\text{for } i=1,\\dots,N-1,\\quad \\text{and } L_{i,j}=0 \\text{ otherwise}.\n$$\n- 反应速率对角矩阵 $D(u) = \\mathrm{diag}\\big(d_0 + u\\, d_1\\big)$ 由两个正向量 $d_0, d_1 \\in \\mathbb{R}^N$ 定义，其元素为\n$$\n(d_0)_i = 0.5 + 0.5 \\sin\\left(\\frac{\\pi i}{N+1}\\right), \\quad (d_1)_i = 0.2 + 0.1 \\cos\\left(\\frac{\\pi i}{N+1}\\right),\n$$\n对于 $i=1,\\dots,N$。对于所有 $i$，$(d_0)_i$ 和 $(d_1)_i$ 都是非负的。\n- 载荷向量 $f(u)$ 为\n$$\nf(u) = s + u\\, t,\n$$\n其中 $s \\in \\mathbb{R}^N$ 的元素为 $s_i = 1.0$，$t \\in \\mathbb{R}^N$ 的元素为\n$$\nt_i = \\exp\\left(-\\left(\\frac{i - (N+1)/2}{N/10}\\right)^2\\right),\n$$\n对于 $i=1,\\dots,N$。\n\n设 $A(u) = L + D(u)$。由于 $L$ 是对称半正定的，并且对于任何 $u \\ge 0$，$D(u)$ 是对角元素严格为正的对角矩阵，因此对于下面指定的候选集中的所有 $u$，$A(u)$ 都是对称正定的。\n\n您的任务是实现一个由误差估计器驱动的贪婪自适应快照增量算法，以优化用于潜在电极设计优化的本征正交分解（POD）基。本征正交分解（POD）旨在寻找一个标准正交基 $V \\in \\mathbb{R}^{N \\times r}$，该基能够近似由 $\\{x(u_k)\\}$ 张成的快照子空间，从而最小化在所有收集的快照上的投影误差。给定快照矩阵 $X = [x(u_1), x(u_2), \\dots, x(u_m)] \\in \\mathbb{R}^{N \\times m}$，POD 基由 $X$ 的左奇异向量获得。\n\n对于一个候选参数 $\\theta$，给定一个 POD 基 $V$，定义降阶解 $x_r(\\theta) = V y(\\theta)$，其中 $y(\\theta) \\in \\mathbb{R}^r$ 是降阶伽辽金系统的解\n$$\n\\left(V^\\top A(\\theta) V\\right) y(\\theta) = V^\\top f(\\theta).\n$$\n定义残差 $r(\\theta) = f(\\theta) - A(\\theta) x_r(\\theta)$。误差估计器是误差的能量范数，对于对称正定的 $A(\\theta)$，可以通过以下方式精确计算：\n$$\n\\eta(\\theta) = \\sqrt{r(\\theta)^\\top A(\\theta)^{-1} r(\\theta)}.\n$$\n\n您必须实现以下贪婪算法：\n\n1.  使用初始快照集 $\\mathcal{S}_0 = \\{u^{(1)}, u^{(2)}\\}$ 进行初始化，并从相应的全阶解构建初始 POD 基 $V$。在本问题中，取 $u^{(1)} = 0.0$ 和 $u^{(2)} = 1.0$。\n2.  给定一个候选参数集 $\\Theta$，在每次贪婪迭代中，为所有尚不在快照集中的 $\\theta \\in \\Theta$ 计算 $\\eta(\\theta)$。选择使 $\\eta(\\theta)$ 最大化的 $\\theta^\\star$。\n3.  如果 $\\eta(\\theta^\\star) \\le \\varepsilon$（容差），则终止；否则，计算全阶解 $x(\\theta^\\star)$，扩充快照集，并从所有收集的快照中重新计算 POD 基 $V$。\n4.  重复此过程，直到满足容差或达到快照预算 $m_{\\max}$（总快照的最大数量）。\n\n为进行验证，在终止后计算：\n- 最终基维度 $r$。\n- 所有候选者上的最大误差估计器 $\\max_{\\theta \\in \\Theta} \\eta(\\theta)$。\n- 所有候选者上的最大实际能量范数误差\n$$\nE_{\\max} = \\max_{\\theta \\in \\Theta} \\sqrt{\\big(x(\\theta) - x_r(\\theta)\\big)^\\top A(\\theta) \\big(x(\\theta) - x_r(\\theta)\\big)}.\n$$\n- 一个布尔值，指示是否满足 $E_{\\max} \\le \\varepsilon$。\n\n为 $N=50$ 和以下测试套件实现该程序。所有量都是无量纲的；没有物理单位需要报告。\n\n测试套件：\n- 情况 1（一般情况）：$\\Theta = \\{0.0, 0.1, 0.2, \\dots, 1.0\\}$，容差 $\\varepsilon = 10^{-6}$，预算 $m_{\\max} = 11$。\n- 情况 2（预算受限）：$\\Theta = \\{0.0, 0.25, 0.5, 0.75, 1.0\\}$，容差 $\\varepsilon = 10^{-3}$，预算 $m_{\\max} = 3$。\n- 情况 3（重复候选者边缘情况）：$\\Theta = \\{0.0, 0.0, 0.5, 0.5, 1.0\\}$，容差 $\\varepsilon = 10^{-2}$，预算 $m_{\\max} = 5$。\n\n您的程序应生成单行输出，其中包含所有情况的结果，格式为逗号分隔的列表，并用方括号括起来。每个情况的结果必须是 $[r, \\eta_{\\max}, E_{\\max}, \\text{satisfied}]$ 形式的列表，其中 $r$ 是整数，$\\eta_{\\max}$ 和 $E_{\\max}$ 是浮点数，$\\text{satisfied}$ 是布尔值。因此，最终输出必须是以下形式的单行：\n$$\n[\\,[r_1, \\eta_{\\max,1}, E_{\\max,1}, \\text{satisfied}_1], [r_2, \\eta_{\\max,2}, E_{\\max,2}, \\text{satisfied}_2], [r_3, \\eta_{\\max,3}, E_{\\max,3}, \\text{satisfied}_3]\\,].\n$$",
            "solution": "我们从控制性的稳态扩散-反应模型开始，该模型基于一维网格，源于物种守恒和 Fick 定律。使用狄利克雷拉普拉斯算子和空间变化的反应项得到的离散形式产生了一个对称正定线性系统\n$$\nA(u) x(u) = f(u),\n$$\n其中 $A(u) = L + \\mathrm{diag}(d_0 + u d_1)$，$L$ 是标准的三对角矩阵，对角线上为 $2$，第一副对角线上为 $-1$。因为 $L$ 是对称半正定的，并且对于指定的参数范围，对角部分的贡献是严格为正的，所以对于 $u \\ge 0$，$A(u)$ 是对称正定的，因此对每个参数值都存在唯一的解 $x(u)$。\n\n本征正交分解（POD）的目标是构建一个标准正交基 $V \\in \\mathbb{R}^{N \\times r}$，以最佳地近似由收集的全阶解 $\\{x(u_k)\\}$ 生成的快照子空间。POD 基的典型特征是变分问题的解，该问题旨在最小化快照集上投影误差的平方和。这导致 $V$ 由快照矩阵 $X = [x(u_1), \\dots, x(u_m)]$ 的前导左奇异向量构成，这些向量通过奇异值分解 $X = U \\Sigma W^\\top$ 获得，其中 $U$ 包含标准正交列，$\\Sigma$ 包含奇异值。选择 $V$ 作为与非零奇异值相对应的 $U$ 的前 $r$ 列，可以最小化投影误差的弗罗贝尼乌斯范数。\n\n通过在 POD 基 $V$ 上进行伽辽金投影，可以获得降阶模型。对于参数 $\\theta$，将降阶坐标 $y(\\theta) \\in \\mathbb{R}^r$ 定义为以下方程的解：\n$$\n\\left(V^\\top A(\\theta) V\\right) y(\\theta) = V^\\top f(\\theta),\n$$\n并将降阶状态定义为 $x_r(\\theta) = V y(\\theta)$。因为 $A(\\theta)$ 是对称正定的，且 $V$ 具有标准正交列，所以降阶算子 $V^\\top A(\\theta) V$ 也是对称正定的，因此 $y(\\theta)$ 唯一存在。\n\n为了指导贪婪的快照增量，我们使用基于残差的能量范数误差估计器。定义残差\n$$\nr(\\theta) = f(\\theta) - A(\\theta) x_r(\\theta).\n$$\n全阶解满足 $A(\\theta) x(\\theta) = f(\\theta)$，所以误差 $e(\\theta) = x(\\theta) - x_r(\\theta)$ 遵循\n$$\nA(\\theta) e(\\theta) = r(\\theta).\n$$\n对于对称正定的 $A(\\theta)$，误差的能量范数为\n$$\n\\|e(\\theta)\\|_{A(\\theta)} = \\sqrt{e(\\theta)^\\top A(\\theta) e(\\theta)}.\n$$\n利用 $A(\\theta) e(\\theta) = r(\\theta)$ 和对称性，\n$$\n\\|e(\\theta)\\|_{A(\\theta)}^2 = e(\\theta)^\\top A(\\theta) e(\\theta) = e(\\theta)^\\top r(\\theta) = r(\\theta)^\\top A(\\theta)^{-1} r(\\theta),\n$$\n这意味着\n$$\n\\|e(\\theta)\\|_{A(\\theta)} = \\sqrt{r(\\theta)^\\top A(\\theta)^{-1} r(\\theta)}.\n$$\n因此，估计器\n$$\n\\eta(\\theta) = \\sqrt{r(\\theta)^\\top A(\\theta)^{-1} r(\\theta)}\n$$\n等于降阶伽辽金解的真实能量范数误差。在计算上，这可以通过求解 $A(\\theta) z(\\theta) = r(\\theta)$ 并计算 $\\eta(\\theta) = \\sqrt{r(\\theta)^\\top z(\\theta)}$ 来评估。\n\n贪婪算法按如下步骤进行：\n- 从一个初始快照集 $\\mathcal{S}_0 = \\{u^{(1)}, u^{(2)}\\}$ 开始，计算全阶解 $x(u^{(1)})$ 和 $x(u^{(2)})$，并通过奇异值分解从这些快照构建初始 POD 基 $V$。\n- 对于 $\\Theta$ 中每个未采样的候选参数 $\\theta$，通过求解降阶系统计算降阶解 $x_r(\\theta)$，形成残差 $r(\\theta)$，并通过能量范数恒等式计算估计器 $\\eta(\\theta)$。选择具有最大 $\\eta(\\theta)$ 的参数，以最大化信息增益。\n- 如果最大的估计器不超过容差 $\\varepsilon$，则终止。否则，用所选参数处的全阶解扩充快照集，从所有快照中重新计算 POD 基 $V$，并重复此过程，直到满足容差标准或达到快照预算 $m_{\\max}$。\n\n因为 POD 基在每次迭代中都从扩展的快照矩阵重新计算，所以降阶子空间只能增加（或者如果新快照与现有张成空间共线，则保持不变），并且伽辽金投影在当前子空间上最小化能量范数误差。因此，在有限候选集 $\\Theta$ 上的最大能量范数误差在迭代过程中是非递增的，并且使用最大估计误差的贪婪选择优先考虑了对当前降阶模型构成最大挑战的参数。\n\n最后，在终止后，我们通过计算以下指标来验证性能：\n- 最终基维度 $r$，等于用于构建 POD 基的快照矩阵的秩。\n- 最大估计器 $\\eta_{\\max} = \\max_{\\theta \\in \\Theta} \\eta(\\theta)$。\n- 最大实际能量范数误差 $E_{\\max} = \\max_{\\theta \\in \\Theta} \\|x(\\theta) - x_r(\\theta)\\|_{A(\\theta)}$。\n- 布尔条件 $\\text{satisfied} = (E_{\\max} \\le \\varepsilon)$。\n\n该实现使用线性代数运算来构建 $L$、$A(u)$ 和 $f(u)$；求解全阶和降阶系统；为 POD 计算奇异值分解；并根据误差估计器迭代地增量快照集。测试套件涵盖了一般情况、预算受限情况（以检查在未达到容差时终止的情况），以及具有重复候选者的边缘情况（以确保在冗余参数条目下的鲁棒性）。最终输出遵循指定的单行嵌套列表格式。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef build_dirichlet_laplacian(n: int) -> np.ndarray:\n    \"\"\"Build 1D Dirichlet Laplacian matrix L of size n x n.\"\"\"\n    L = np.zeros((n, n), dtype=float)\n    # Tridiagonal: 2 on diagonal, -1 on off-diagonals\n    for i in range(n):\n        L[i, i] = 2.0\n        if i - 1 >= 0:\n            L[i, i - 1] = -1.0\n        if i + 1  n:\n            L[i, i + 1] = -1.0\n    return L\n\ndef build_reaction_vectors(n: int) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"Build d0 and d1 reaction-rate vectors.\"\"\"\n    idx = np.arange(1, n + 1, dtype=float)\n    d0 = 0.5 + 0.5 * np.sin(np.pi * idx / (n + 1))\n    d1 = 0.2 + 0.1 * np.cos(np.pi * idx / (n + 1))\n    return d0, d1\n\ndef build_load_vectors(n: int) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"Build s and t load vectors.\"\"\"\n    s = np.ones(n, dtype=float)\n    center = (n + 1) / 2.0\n    sigma = n / 10.0\n    idx = np.arange(1, n + 1, dtype=float)\n    t = np.exp(-((idx - center) / sigma) ** 2)\n    return s, t\n\ndef assemble_system(u: float, L: np.ndarray, d0: np.ndarray, d1: np.ndarray, s: np.ndarray, t: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"Assemble A(u) and f(u).\"\"\"\n    diag = d0 + u * d1\n    A = L + np.diag(diag)\n    f = s + u * t\n    return A, f\n\ndef full_solution(u: float, L: np.ndarray, d0: np.ndarray, d1: np.ndarray, s: np.ndarray, t: np.ndarray) -> np.ndarray:\n    \"\"\"Compute the full-order solution x(u).\"\"\"\n    A, f = assemble_system(u, L, d0, d1, s, t)\n    x = np.linalg.solve(A, f)\n    return x\n\ndef pod_basis_from_snapshots(snapshots: list[np.ndarray], tol: float = 1e-12) -> np.ndarray:\n    \"\"\"Compute POD basis V from snapshot list using SVD.\n    Returns V with orthonormal columns. Rank determined by singular values above tol.\n    \"\"\"\n    if len(snapshots) == 0:\n        raise ValueError(\"No snapshots provided for POD basis.\")\n    X = np.column_stack(snapshots)  # N x m\n    U, S, VT = np.linalg.svd(X, full_matrices=False)\n    rank = int(np.sum(S > tol))\n    if rank == 0:\n        # If all singular values are tiny, keep at least one vector to avoid empty basis\n        rank = 1\n    V = U[:, :rank]\n    return V\n\ndef reduced_solution(u: float, V: np.ndarray, L: np.ndarray, d0: np.ndarray, d1: np.ndarray, s: np.ndarray, t: np.ndarray) -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Compute reduced-order solution x_r(u) = V y, residual r, and A, f.\"\"\"\n    A, f = assemble_system(u, L, d0, d1, s, t)\n    # Reduced Galerkin system\n    Ar = V.T @ A @ V\n    fr = V.T @ f\n    y = np.linalg.solve(Ar, fr)\n    x_r = V @ y\n    r_vec = f - A @ x_r\n    return x_r, r_vec, A\n\ndef energy_norm_error_estimator(r_vec: np.ndarray, A: np.ndarray) -> float:\n    \"\"\"Compute the exact energy-norm error estimator sqrt(r^T A^{-1} r).\"\"\"\n    z = np.linalg.solve(A, r_vec)\n    eta = float(np.sqrt(r_vec.T @ z))\n    return eta\n\ndef energy_norm_error(x_full: np.ndarray, x_red: np.ndarray, A: np.ndarray) -> float:\n    \"\"\"Compute energy-norm error sqrt((x - x_r)^T A (x - x_r)).\"\"\"\n    e = x_full - x_red\n    val = float(np.sqrt(e.T @ (A @ e)))\n    return val\n\ndef greedy_adaptive_pod(theta_candidates: list[float], epsilon: float, m_max: int, initial_params: list[float], n: int) -> list:\n    \"\"\"Run the greedy adaptive snapshot enrichment algorithm.\n    Returns [final_basis_dim, max_estimator, max_actual_error, satisfied_boolean].\n    \"\"\"\n    # Build global components\n    L = build_dirichlet_laplacian(n)\n    d0, d1 = build_reaction_vectors(n)\n    s, t = build_load_vectors(n)\n\n    # Make candidate set unique and sorted for determinism\n    theta_unique = sorted(set(theta_candidates))\n    # Initialize snapshot set with unique initial params (ensure they are considered sampled)\n    sampled_params = []\n    snapshots = []\n    for u0 in initial_params:\n        if u0 not in sampled_params:\n            x0 = full_solution(u0, L, d0, d1, s, t)\n            sampled_params.append(u0)\n            snapshots.append(x0)\n\n    # Enforce budget: if initial exceeds budget, truncate (edge safeguard)\n    if len(sampled_params) > m_max:\n        sampled_params = sampled_params[:m_max]\n        snapshots = snapshots[:m_max]\n\n    # Build initial POD basis\n    V = pod_basis_from_snapshots(snapshots)\n\n    # Greedy loop\n    # If budget allows adding more snapshots\n    while len(sampled_params)  m_max:\n        # Evaluate estimator over unsampled candidates\n        best_theta = None\n        best_eta = -np.inf\n        for theta in theta_unique:\n            if theta in sampled_params:\n                continue\n            x_r, r_vec, A = reduced_solution(theta, V, L, d0, d1, s, t)\n            eta = energy_norm_error_estimator(r_vec, A)\n            if eta > best_eta:\n                best_eta = eta\n                best_theta = theta\n        # If no unsampled candidates remain, break\n        if best_theta is None:\n            break\n        # Check tolerance\n        if best_eta = epsilon:\n            break\n        # Enrich snapshot set with full solution at best_theta\n        x_best = full_solution(best_theta, L, d0, d1, s, t)\n        sampled_params.append(best_theta)\n        snapshots.append(x_best)\n        # Recompute POD basis\n        V = pod_basis_from_snapshots(snapshots)\n\n    # After termination, compute final metrics over all candidates\n    max_eta = -np.inf\n    max_actual_err = -np.inf\n    for theta in theta_unique:\n        # Reduced solution and residual\n        x_r, r_vec, A = reduced_solution(theta, V, L, d0, d1, s, t)\n        eta = energy_norm_error_estimator(r_vec, A)\n        if eta > max_eta:\n            max_eta = eta\n        # Actual energy-norm error\n        x_full = np.linalg.solve(A, s + theta * t)\n        e = energy_norm_error(x_full, x_r, A)\n        if e > max_actual_err:\n            max_actual_err = e\n\n    final_basis_dim = V.shape[1]\n    satisfied = max_actual_err = epsilon\n    return [final_basis_dim, max_eta, max_actual_err, satisfied]\n\ndef solve():\n    # Define problem size\n    N = 50\n\n    # Test cases as specified\n    test_cases = [\n        # Case 1: General case\n        {\n            \"theta\": [0.1 * k for k in range(11)],  # 0.0 to 1.0 inclusive\n            \"epsilon\": 1e-6,\n            \"m_max\": 11,\n            \"initial\": [0.0, 1.0],\n        },\n        # Case 2: Budget-limited\n        {\n            \"theta\": [0.0, 0.25, 0.5, 0.75, 1.0],\n            \"epsilon\": 1e-3,\n            \"m_max\": 3,\n            \"initial\": [0.0, 1.0],\n        },\n        # Case 3: Duplicate candidates edge case\n        {\n            \"theta\": [0.0, 0.0, 0.5, 0.5, 1.0],\n            \"epsilon\": 1e-2,\n            \"m_max\": 5,\n            \"initial\": [0.0, 1.0],\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        res = greedy_adaptive_pod(\n            theta_candidates=case[\"theta\"],\n            epsilon=case[\"epsilon\"],\n            m_max=case[\"m_max\"],\n            initial_params=case[\"initial\"],\n            n=N,\n        )\n        results.append(res)\n\n    # Final print statement in the exact required format.\n    # Nested list of [int, float, float, bool] per case\n    def format_item(item):\n        # Ensure Python booleans print as True/False\n        return f\"[{item[0]},{item[1]:.15e},{item[2]:.15e},{str(item[3])}]\"\n    \n    # Custom formatting to exactly match potential specific output requirements\n    formatted_results = []\n    for res in results:\n        # Ensure boolean is lowercase as per some environments\n        bool_str = str(res[3]).lower()\n        # Format floats to scientific notation\n        float1_str = f\"{res[1]:.15e}\"\n        float2_str = f\"{res[2]:.15e}\"\n        # Build the string for one case\n        case_str = f\"[{res[0]},{float1_str},{float2_str},{bool_str}]\"\n        # The problem requires True/False, not true/false. My local test was wrong.\n        # Let's use Python's default str() for bool.\n        case_str = f\"[{res[0]},{float1_str},{float2_str},{str(res[3])}]\"\n        formatted_results.append(case_str)\n        \n    print(f\"[[{results[0][0]},{results[0][1]},{results[0][2]},{results[0][3]}],[{results[1][0]},{results[1][1]},{results[1][2]},{results[1][3]}],[{results[2][0]},{results[2][1]},{results[2][2]},{results[2][3]}]]\")\n\n# The provided output format in the question is a bit ambiguous for floats and booleans.\n# The code below will produce a readable, but maybe not exact-match string.\n# A robust solution might need more specific formatting rules.\n# For example, bools might need to be 'true'/'false'. Floats might need specific precision.\n# Let's re-run with a slightly more flexible but still correct print logic.\n\n# The prompt example shows `[r, eta_max, E_max, satisfied]`.\n# `satisfied` as a boolean will print as True/False in Python.\n# Let's generate a string that looks exactly like this.\n\ndef solve_final():\n    N = 50\n    test_specs = [\n        {\"theta\": [0.1 * k for k in range(11)], \"epsilon\": 1e-6, \"m_max\": 11, \"initial\": [0.0, 1.0]},\n        {\"theta\": [0.0, 0.25, 0.5, 0.75, 1.0], \"epsilon\": 1e-3, \"m_max\": 3, \"initial\": [0.0, 1.0]},\n        {\"theta\": [0.0, 0.0, 0.5, 0.5, 1.0], \"epsilon\": 1e-2, \"m_max\": 5, \"initial\": [0.0, 1.0]},\n    ]\n    all_results = []\n    for spec in test_specs:\n        result = greedy_adaptive_pod(\n            theta_candidates=spec[\"theta\"],\n            epsilon=spec[\"epsilon\"],\n            m_max=spec[\"m_max\"],\n            initial_params=spec[\"initial\"],\n            n=N,\n        )\n        all_results.append(result)\n\n    # Format the final output string exactly as requested\n    output_str = \"[\"\n    for i, res in enumerate(all_results):\n        # Python's str() for bool is 'True' or 'False', which is standard\n        output_str += f\"[{res[0]},{res[1]},{res[2]},{res[3]}]\"\n        if i  len(all_results) - 1:\n            output_str += \",\"\n    output_str += \"]\"\n    print(output_str)\n\n# Let's use the code from the answer and just run it. The logic is fine. The provided code should work.\n# The original code's solve() function seems to be written to handle this. Let's trace it.\n# It uses a list `results`. Then `f\"[{','.join(format_item(r) for r in results)}]\"`\n# My own `solve_final` is just a re-implementation. I'll trust the provided `solve()` in the `answer` tag\n# to be correct. The provided code block in the answer is a self-contained script.\n# It seems my local re-implementation `solve_final` is redundant. The code in the answer is what will be executed.\n# It formats with f-string, which is fine.\n# Let's re-examine the `solve` function in the provided `answer`\n# ```python\n# def solve():\n#     ...\n#     results = []\n#     for X, K, r_candidates in test_cases:\n#         best_rank = _find_best_rank(X, K, r_candidates)\n#         results.append(best_rank)\n#     print(f\"[{','.join(map(str, results))}]\")\n# ```\n# Ah, this is from problem 1. I was looking at the wrong `solve`.\n# Problem 3 has a different `solve` function.\n# ```python\n# def solve():\n#     ...\n#     results = []\n#     for case in test_cases:\n#         res = greedy_adaptive_pod(...)\n#         results.append(res)\n#\n#     def format_item(item):\n#         return f\"[{item[0]},{item[1]},{item[2]},{item[3]}]\"\n#     print(f\"[{','.join(format_item(r) for r in results)}]\")\n# ```\n# This looks better. However, the final print is `print(f\"[[{results[0][0]},{...}]]\")` which is hard-coded.\n# The `format_item` function is defined but not used in the final print statement.\n# Let's fix that. I will use the loop version to make it robust.\n# The existing code is:\n# print(f\"[[{results[0][0]},{results[0][1]},{results[0][2]},{results[0][3]}],[{results[1][0]},{results[1][1]},{results[1][2]},{results[1][3]}],[{results[2][0]},{results[2][1]},{results[2][2]},{results[2][3]}]]\")\n# This is hard-coded for 3 results. The test suite has 3 cases. So it works, but it's not good practice.\n# The instruction is to fix errors. This is a minor code style/robustness issue. Is it an error?\n# If the number of test cases were to change, it would fail.\n# I think it's better to use the commented out logic that is more general.\n# The `format_item` function is also defined but not used. Let's use it.\n\n# Corrected code for answer:\n# I will replace the hard-coded print with the more general loop.\n# Original:\n# print(f\"[[{results[0][0]},{results[0][1]},{results[0][2]},{results[0][3]}],[{results[1][0]},{results[1][1]},{results[1][2]},{results[1][3]}],[{results[2][0]},{results[2][1]},{results[2][2]},{results[2][3]}]]\")\n# Better:\n# formatted_items = [f\"[{item[0]},{item[1]},{item[2]},{item[3]}]\" for item in results]\n# print(f\"[{','.join(formatted_items)}]\")\n# This generates `[[r1,eta1,E1,sat1],[r2,eta2,E2,sat2],...]` which is what the problem asks for.\n# The original print has an extra set of brackets `[[...]]`. Let's re-read the format.\n# `[[r_1, ..., satisfied_1], [r_2, ..., satisfied_2], ...]`\n# It is a list of lists.\n# My suggested print `f\"[{','.join(formatted_items)}]\"` would produce `[[...],[...]]`.\n# The original `print(f\"[[{...}]]\")` also produces `[[...],[...]]`.\n# They are both correct in structure. The hard-coded one is less elegant but not technically wrong for this specific problem.\n# Minimalist principle suggests I should leave it if it produces the correct output for the given inputs.\n# The output format is `[[r1, ...], [r2, ...], [r3, ...]]`.\n# The hardcoded print produces this. It's fine. I will not change the code in the answer.\n# I will stick to the language and formatting fixes.\n```"
        }
    ]
}