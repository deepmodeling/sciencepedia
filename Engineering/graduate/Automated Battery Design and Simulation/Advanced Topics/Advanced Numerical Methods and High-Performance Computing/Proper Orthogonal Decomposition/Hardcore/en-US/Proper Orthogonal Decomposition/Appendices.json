{
    "hands_on_practices": [
        {
            "introduction": "This initial practice provides a crucial lesson on the fundamental behavior and limitations of Proper Orthogonal Decomposition (POD). By applying POD to an idealized translating Gaussian pulse, you will discover why global basis functions struggle to efficiently represent localized, advection-dominated phenomena, a common challenge in transport modeling . This coding exercise will help you build intuition about the slow decay of singular values and the resulting poor compressibility when dealing with moving features.",
            "id": "3265968",
            "problem": "Consider the family of snapshot functions of a translating Gaussian pulse defined by $u(x,t) = \\exp\\!\\left(-\\big(x - c t\\big)^{2}\\right)$ on the spatial interval $x \\in [-L,L]$ and discrete times $t \\in \\{t_{0}, t_{1}, \\dots, t_{m-1}\\}$. From first principles, Proper Orthogonal Decomposition (POD) is the procedure that, for a given rank $r$, selects an $r$-dimensional orthonormal basis in space that minimizes the total squared projection error of the snapshot set. Equivalently, it produces the best rank-$r$ approximation of the snapshot data (in the Euclidean least-squares sense across all grid points and times).\n\nYour task is to implement a program that:\n- Constructs the snapshot matrix $X \\in \\mathbb{R}^{N_x \\times m}$ whose $k$-th column is the sampled snapshot $u(x,t_k)$ at $N_x$ uniformly spaced grid points in $[-L,L]$, for a given speed $c$, number of snapshots $m$, and final time $T$ with $t_k$ equally spaced in $[0,T]$.\n- Computes, for ranks $r \\in \\{1,2,5,10\\}$, the best rank-$r$ approximation $X_r$ (as defined by POD) and the corresponding relative reconstruction error\n$$E_r = \\frac{\\lVert X - X_r \\rVert_F}{\\lVert X \\rVert_F},$$\nwhere $\\lVert \\cdot \\rVert_F$ denotes the Frobenius norm.\n- Reports the errors $E_r$ for each test case as floating-point numbers rounded to six decimal places.\n\nFundamental base to use:\n- Definitions of Euclidean inner product and Frobenius norm.\n- The defining optimization property of Proper Orthogonal Decomposition (POD): among all $r$-dimensional orthonormal bases, POD minimizes the total squared projection error of the snapshots. This yields the best rank-$r$ approximation of the snapshot matrix in the least-squares sense.\n\nTest suite:\nUse $L = 10$ and $N_x = 401$ for all cases. The four test cases are:\n1. Case A (stationary pulse): $c = 0$, $T = 5$, $m = 50$.\n2. Case B (slow translation): $c = 0.5$, $T = 10$, $m = 100$.\n3. Case C (fast translation): $c = 2.0$, $T = 4$, $m = 80$.\n4. Case D (few snapshots): $c = 0.5$, $T = 10$, $m = 5$.\n\nAnswer specification:\n- For each test case, output a list $[E_{1},E_{2},E_{5},E_{10}]$ of four floats rounded to six decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the bracketed list of four errors for one test case, in the order of Cases A, B, C, D. For example, an output of the correct format would look like\n`[[e_{A,1},e_{A,2},e_{A,5},e_{A,10}],[e_{B,1},e_{B,2},e_{B,5},e_{B,10}],[e_{C,1},e_{C,2},e_{C,5},e_{C,10}],[e_{D,1},e_{D,2},e_{D,5},e_{D,10}]]`,\nwith no spaces anywhere in the line.\n\nUnits:\n- There are no physical units required in this problem.\n\nAngle units:\n- Not applicable.\n\nPercentages:\n- Not applicable; express all quantities as decimals.\n\nYour implementation must be self-contained and require no user input, external files, or network access. It must run in a modern programming language and produce the exact final output format described above in a single line.",
            "solution": "The problem is subjected to validation.\n\n### Step 1: Extract Givens\n- **Snapshot function**: $u(x,t) = \\exp(-(x - c t)^2)$\n- **Spatial domain**: $x \\in [-L,L]$\n- **Time domain**: $t \\in \\{t_{0}, t_{1}, \\dots, t_{m-1}\\}$, which are $m$ equally spaced points in $[0,T]$.\n- **Spatial discretization**: $N_x$ uniformly spaced grid points in $[-L,L]$.\n- **Snapshot matrix**: $X \\in \\mathbb{R}^{N_x \\times m}$, where the $k$-th column is the sampled snapshot $u(x,t_k)$.\n- **Task**: Compute the best rank-$r$ approximation $X_r$ using Proper Orthogonal Decomposition (POD) for ranks $r \\in \\{1, 2, 5, 10\\}$.\n- **Error metric**: Relative reconstruction error $E_r = \\frac{\\lVert X - X_r \\rVert_F}{\\lVert X \\rVert_F}$, where $\\lVert \\cdot \\rVert_F$ is the Frobenius norm.\n- **Constants**: $L = 10$, $N_x = 401$.\n- **Test cases**:\n    1. Case A: $c = 0$, $T = 5$, $m = 50$.\n    2. Case B: $c = 0.5$, $T = 10$, $m = 100$.\n    3. Case C: $c = 2.0$, $T = 4$, $m = 80$.\n    4. Case D: $c = 0.5$, $T = 10$, $m = 5$.\n- **Output specification**: A single-line comma-separated list of lists, e.g., `[[e_{A,1},...,e_{A,10}],[e_{B,1},...,e_{B,10}],...]`, with numbers rounded to six decimal places and no spaces.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is a standard application of Proper Orthogonal Decomposition (POD), a cornerstone of reduced-order modeling in scientific computing. The mathematical foundation is the Singular Value Decomposition (SVD), which is a principal result in linear algebra. The physics is a simple translating Gaussian pulse, which is a common and valid test case. The problem is scientifically sound.\n- **Well-Posed**: All necessary parameters ($L, N_x, c, T, m$) are provided for each case. The function $u(x,t)$, the procedure for constructing the snapshot matrix $X$, and the error metric $E_r$ are all defined unambiguously. The SVD provides a unique construction for the best rank-$r$ approximation, ensuring a unique solution exists.\n- **Objective**: The problem is expressed in precise mathematical terms, devoid of any subjectivity, ambiguity, or opinion-based statements.\n\n### Step 3: Verdict and Action\nThe problem is valid. It is a well-defined, self-contained, and scientifically sound problem in the field of numerical methods. A complete solution will be provided.\n\n### Principle-Based Solution\nThe objective is to compute the relative reconstruction error for a rank-$r$ approximation of a set of data snapshots. The foundational principle is that the optimal rank-$r$ approximation, in the least-squares sense defined by the Frobenius norm, is obtained via the Singular Value Decomposition (SVD). This result is formally stated by the Eckart-Young-Mirsky theorem.\n\n**1. Snapshot Matrix Construction**\nFirst, we discretize the problem domain. The spatial domain $x \\in [-L, L]$ is sampled at $N_x$ uniformly spaced points, forming the grid $\\{x_j\\}_{j=0}^{N_x-1}$. The time interval $t \\in [0, T]$ is sampled at $m$ discrete, equally spaced points $\\{t_k\\}_{k=0}^{m-1}$. The snapshot data at each time point $t_k$ is a vector in $\\mathbb{R}^{N_x}$ whose entries are given by the function $u(x_j, t_k)$. The collection of these snapshots forms the columns of the snapshot matrix $X \\in \\mathbb{R}^{N_x \\times m}$. An element $X_{jk}$ of this matrix is given by:\n$$X_{jk} = u(x_j, t_k) = \\exp\\!\\left(-\\big(x_j - c t_k\\big)^{2}\\right)$$\n\n**2. Singular Value Decomposition and Optimal Approximation**\nThe SVD of the snapshot matrix $X$ is given by:\n$$X = U \\Sigma V^T$$\nwhere $U \\in \\mathbb{R}^{N_x \\times N_x}$ is an orthogonal matrix whose columns $u_i$ are the left-singular vectors (POD modes), $V \\in \\mathbb{R}^{m \\times m}$ is an orthogonal matrix whose columns $v_i$ are the right-singular vectors, and $\\Sigma \\in \\mathbb{R}^{N_x \\times m}$ is a rectangular diagonal matrix containing the singular values $\\sigma_i$. The singular values are non-negative and ordered by convention: $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_k \\ge 0$, where $k = \\min(N_x, m)$ is the rank of the matrix.\n\nThe Eckart-Young-Mirsky theorem states that the best rank-$r$ approximation of $X$ that minimizes the Frobenius norm of the difference, $\\lVert X - X_r \\rVert_F$, is the truncated SVD:\n$$X_r = \\sum_{i=1}^{r} \\sigma_i u_i v_i^T$$\nThis approximation is constructed using the first $r$ singular values and their corresponding left and right singular vectors.\n\n**3. Error Calculation**\nThe relative reconstruction error $E_r$ is defined as the ratio of the Frobenius norm of the error matrix $(X - X_r)$ to the Frobenius norm of the original matrix $X$. The Frobenius norm is related to the singular values by the identity $\\lVert A \\rVert_F^2 = \\sum_{i=1}^{\\text{rank}(A)} \\sigma_i(A)^2$.\nApplying this property, the squared norm of the original matrix is the sum of the squares of all its singular values:\n$$\\lVert X \\rVert_F^2 = \\sum_{i=1}^{k} \\sigma_i^2$$\nThe error matrix is $X - X_r = \\sum_{i=r+1}^{k} \\sigma_i u_i v_i^T$. Due to the orthogonality of the singular vectors, the squared Frobenius norm of the error matrix is the sum of the squares of the discarded singular values:\n$$\\lVert X - X_r \\rVert_F^2 = \\sum_{i=r+1}^{k} \\sigma_i^2$$\nCombining these results, the relative reconstruction error is given by:\n$$E_r = \\frac{\\lVert X - X_r \\rVert_F}{\\lVert X \\rVert_F} = \\frac{\\sqrt{\\sum_{i=r+1}^{k} \\sigma_i^2}}{\\sqrt{\\sum_{i=1}^{k} \\sigma_i^2}}$$\nNote that if the requested rank $r$ is greater than or equal to the actual rank of the matrix, $k$, the sum in the numerator is empty and evaluates to $0$, correctly yielding an error $E_r = 0$.\n\n**4. Computational Procedure**\nThe algorithm proceeds as follows for each test case:\n1.  Define the parameters $c$, $T$, and $m$, along with the fixed constants $L=10$ and $N_x=401$.\n2.  Construct the spatial grid $x$ and temporal grid $t$.\n3.  Assemble the $N_x \\times m$ snapshot matrix $X$ using the given function $u(x,t)$.\n4.  Compute the singular values $\\sigma_i$ of $X$ using a standard numerical library function for SVD. It is most efficient to compute only the singular values, not the full $U$ and $V$ matrices.\n5.  Calculate the total energy, represented by the squared Frobenius norm, $S_{total} = \\sum_{i=1}^{k} \\sigma_i^2$.\n6.  For each required rank $r \\in \\{1, 2, 5, 10\\}$, calculate the error energy, $S_{error} = \\sum_{i=r+1}^{k} \\sigma_i^2$.\n7.  The relative error is then $E_r = \\sqrt{S_{error} / S_{total}}$.\n8.  The calculated errors for each test case are collected and formatted according to the output specification.\nThis procedure provides a direct and numerically stable method for determining the required reconstruction errors based on fundamental principles of linear algebra.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Proper Orthogonal Decomposition problem for a translating Gaussian pulse.\n    \"\"\"\n    # Global parameters for all test cases\n    L = 10.0\n    Nx = 401\n    ranks_to_compute = [1, 2, 5, 10]\n\n    # Test suite: (c, T, m)\n    # c: speed, T: final time, m: number of snapshots\n    test_cases = [\n        (0.0, 5.0, 50),   # Case A: stationary pulse\n        (0.5, 10.0, 100), # Case B: slow translation\n        (2.0, 4.0, 80),   # Case C: fast translation\n        (0.5, 10.0, 5),   # Case D: few snapshots\n    ]\n\n    all_results = []\n\n    for c, T, m in test_cases:\n        # 1. Create spatial and temporal grids\n        x = np.linspace(-L, L, Nx)\n        t = np.linspace(0.0, T, m)\n\n        # 2. Construct the snapshot matrix X using broadcasting\n        # x_col has shape (Nx, 1) and t_row has shape (1, m)\n        # Broadcasting expands them to (Nx, m) for element-wise operations\n        x_col = x[:, np.newaxis]\n        t_row = t[np.newaxis, :]\n        X = np.exp(-((x_col - c * t_row) ** 2))\n\n        # 3. Compute the singular values of X\n        # We only need the singular values, so compute_uv=False is most efficient.\n        s = np.linalg.svd(X, compute_uv=False)\n        num_singular_values = s.shape[0]\n\n        # 4. Calculate the total energy (squared Frobenius norm of X)\n        # This is the sum of the squares of all singular values.\n        norm_X_sq = np.sum(s**2)\n\n        case_errors = []\n        for r in ranks_to_compute:\n            # 5. Calculate the reconstruction error for rank r\n            \n            # If norm_X_sq is zero, all errors are zero.\n            if norm_X_sq == 0.0:\n                 error = 0.0\n            # If rank r is >= number of singular values, the approximation is perfect.\n            elif r >= num_singular_values:\n                error = 0.0\n            else:\n                # The error norm is based on the truncated singular values (from r to end).\n                # s[r:] corresponds to sigma_{r+1}, sigma_{r+2}, ...\n                norm_err_sq = np.sum(s[r:]**2)\n                error = np.sqrt(norm_err_sq / norm_X_sq)\n            \n            case_errors.append(error)\n\n        all_results.append(case_errors)\n\n    # 6. Format the output string exactly as specified.\n    # e.g., [[err1,err2,...],[err1,err2,...]] with no spaces.\n    formatted_sublists = []\n    for res_list in all_results:\n        # Format each number to 6 decimal places.\n        formatted_numbers = [f\"{err:.6f}\" for err in res_list]\n        # Join numbers with commas and enclose in brackets.\n        formatted_sublists.append(f\"[{','.join(formatted_numbers)}]\")\n    \n    # Join the sublists with commas and enclose in outer brackets.\n    final_output = f\"[{','.join(formatted_sublists)}]\"\n\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "Building upon the insights from the translating pulse example, this exercise applies the same critical thinking to a physically relevant battery modeling scenario. You will analyze why a global POD basis struggles to capture the sharp, moving concentration fronts that develop during high-rate battery operation . This practice challenges you to move beyond simple application and evaluate sophisticated strategies, such as domain decomposition, that are necessary to create efficient and accurate reduced-order models for systems with localized dynamics.",
            "id": "3943763",
            "problem": "Consider a one-dimensional through-thickness model for electrolyte salt concentration in a lithium-ion battery electrode, with spatial coordinate $x \\in [0,L]$ measured from the current collector to the separator. The electrolyte salt concentration $c_e(x,t)$ satisfies species conservation and Fickian diffusion in the porous electrolyte, with a source term due to interfacial reaction:\n\n$$\n\\varepsilon_e \\frac{\\partial c_e}{\\partial t} = \\frac{\\partial}{\\partial x} \\left( D_e \\frac{\\partial c_e}{\\partial x} \\right) + \\Gamma j_n(x,t),\n$$\n\nwhere $\\varepsilon_e$ is the electrolyte volume fraction, $D_e$ is the effective electrolyte diffusivity, $\\Gamma = \\frac{(1-t_+^0) a_s}{F}$ with $t_+^0$ the cation transference number, $a_s$ the specific interfacial area, and $F$ Faraday’s constant. The interfacial molar flux $j_n(x,t)$ arises from electrode kinetics (e.g., via Butler–Volmer), and during a high-rate charge, $j_n(x,t)$ becomes sharply localized near the separator, leading to a steep concentration front. Assume that the boundary conditions are physically consistent with no flux at the current collector and continuity at the separator. Consider the high-rate regime where the diffusion length $\\ell_D(t) \\sim \\sqrt{D_e t}$ is small compared to $L$, so that the concentration field develops a localized “bump” of characteristic width $w \\ll L$ that propagates as the front moves with time. For a sequence of time instances $\\{t_k\\}_{k=1}^m$, the snapshots can be idealized as\n\n$$\nc_e(x,t_k) \\approx c_b + A h\\!\\left(x - \\xi(t_k)\\right),\n$$\n\nwhere $c_b$ is a slowly varying background, $A$ is an amplitude set by the applied current and kinetics, $h(\\cdot)$ is a localized profile of width $w$, and $\\xi(t)$ is the front position. A global Proper Orthogonal Decomposition (POD) basis is constructed from these snapshots over the full domain $[0,L]$.\n\nUsing first principles of species conservation, diffusion scaling, and the structure of the snapshot set as translations of a localized function, analyze why a global POD can struggle to capture sharp, moving concentration fronts with a small number of modes. Then, evaluate strategies to address this difficulty while preserving conservation across subdomains in a battery simulation. Select all correct statements.\n\nA. The poor performance of a global POD basis in this scenario is primarily due to insufficient “energy” in the snapshot ensemble, which can be remedied simply by adding more snapshots in time at the same spatial resolution.\n\nB. When snapshots are dominated by translations of a localized profile $h(x-\\xi)$ over $[0,L]$, the covariance operator is approximately a convolution with the autocorrelation of $h$, whose eigenfunctions are Fourier-like; representing a narrow front of width $w$ requires $\\mathcal{O}(L/w)$ global modes, leading to slow singular value decay and poor compression.\n\nC. Reducing the temporal sampling interval alone (i.e., taking snapshots more frequently in time without changing the spatial basis construction) ensures rapid singular value decay for a global POD basis because it resolves the front motion sufficiently.\n\nD. A non-overlapping domain decomposition with flux-continuity coupling at interfaces, combined with localized POD bases on subdomains that include and track the front, yields a reduced-order model whose required dimension scales like $\\mathcal{O}\\!\\left(\\max_i L_i / w\\right)$, where $L_i$ are subdomain sizes; enforcing interface conditions preserves mass conservation while efficiently capturing localized dynamics.\n\nE. Applying the Discrete Empirical Interpolation Method (DEIM) only to the nonlinear kinetic source term $j_n(x,t)$ resolves the difficulty with sharp fronts in the global POD, because front localization is primarily a consequence of nonlinearity rather than transport and diffusion scaling.\n\nSelect all options that are correct.",
            "solution": "The problem statement describes a one-dimensional diffusion-reaction equation for electrolyte concentration in a lithium-ion battery, a common component of a Doyle-Fuller-Newman (DFN) type model. The scenario of a sharp, moving concentration front under high-rate conditions is physically realistic, corresponding to significant concentration polarization. The problem asks for an analysis of the performance of a global Proper Orthogonal Decomposition (POD) method in this context and an evaluation of potential solutions. The physics and mathematics presented are standard and self-consistent. The problem is valid.\n\nThe core of the problem lies in understanding why POD, which is optimal for a given snapshot set in an energy sense, struggles with translating features. POD constructs an orthonormal basis $\\{\\phi_j(x)\\}_{j=1}^N$ that is optimal for representing the snapshot ensemble $\\{c_e(x,t_k)\\}_{k=1}^m$. The basis functions are the eigenfunctions of the spatial two-point correlation operator $K(x,x')$, defined as:\n$$\nK(x,x') = \\frac{1}{m} \\sum_{k=1}^m (c_e(x,t_k) - \\bar{c}_e(x))(c_e(x',t_k) - \\bar{c}_e(x')),\n$$\nwhere $\\bar{c}_e(x)$ is the time-averaged concentration. The efficiency of the POD basis is determined by the decay rate of the eigenvalues $\\lambda_j$ of $K$, which correspond to the squared singular values of the snapshot matrix. A rapid decay implies that most of the system's energy (variance) can be captured by a few modes, leading to a low-dimensional model.\n\nThe problem states that the snapshots can be idealized as $c_e(x,t_k) \\approx c_b + A h(x - \\xi(t_k))$, where $h$ is a localized function of width $w$ and $\\xi(t_k)$ is its moving position. For simplicity, let's analyze the fluctuating part, $u_k(x) = A h(x - \\xi(t_k))$. If the front positions $\\xi(t_k)$ sample a significant portion of the domain $[0,L]$, the summation in the correlation operator becomes an approximation of an integral over the spatial shift variable $\\xi$:\n$$\nK(x,x') \\propto \\int h(x-\\xi) h(x'-\\xi) \\,d\\xi.\n$$\nThis is a convolution operation. An operator of this form, acting on a domain that is large compared to the feature size, is a Toeplitz operator (or a circulant operator, if periodic boundary conditions are assumed). The eigenfunctions of such operators are known to be global, oscillatory functions, closely related to Fourier modes (e.g., sine/cosine functions).\n\nFrom Fourier analysis, to represent a localized feature of width $w$, one needs significant contributions from basis functions with wavelengths down to the order of $w$. On a domain of length $L$, this corresponds to modes with indices up to $n \\approx L/w$. Therefore, the energy of the signal is spread across approximately $\\mathcal{O}(L/w)$ Fourier-like modes. Because the POD modes are themselves Fourier-like in this scenario, the energy will be distributed among many POD modes as well. This implies that the singular values will decay slowly, and a large number of modes, scaling as $\\mathcal{O}(L/w)$, will be required to accurately capture the translating front. This poor compression is the fundamental reason a global POD basis is inefficient for such problems.\n\nNow, we evaluate each option based on this analysis.\n\n**A. The poor performance of a global POD basis in this scenario is primarily due to insufficient “energy” in the snapshot ensemble, which can be remedied simply by adding more snapshots in time at the same spatial resolution.**\n\nThis statement is **Incorrect**. The \"energy\" of the system is the sum of the squared $L_2$-norms of the snapshots. The problem is not a lack of energy or data, but the high dimensionality of the manifold on which the solution snapshots lie. The set of all translations of a localized function forms a high-dimensional space that cannot be efficiently approximated by a low-dimensional linear subspace. Adding more snapshots that show the front at new locations will generally make the POD representation *worse* for a fixed number of modes, because the basis must then be capable of representing the feature over a larger spatial range. The slow decay of singular values is an intrinsic property of the translating feature, not an artifact of insufficient temporal sampling.\n\n**B. When snapshots are dominated by translations of a localized profile $h(x-\\xi)$ over $[0,L]$, the covariance operator is approximately a convolution with the autocorrelation of $h$, whose eigenfunctions are Fourier-like; representing a narrow front of width $w$ requires $\\mathcal{O}(L/w)$ global modes, leading to slow singular value decay and poor compression.**\n\nThis statement is **Correct**. As derived above, the two-point correlation (or covariance) operator for a set of translating functions takes the form of a convolution operator. The eigenfunctions of convolution (Toeplitz) operators are known to be global, oscillatory, Fourier-like functions. To represent a localized feature of width $w$ using global Fourier-like modes over a domain of length $L$, one requires modes up to a spatial frequency of approximately $1/w$, which translates to needing a number of modes that scales as $\\mathcal{O}(L/w)$. This distribution of energy across a large number of modes is synonymous with slow singular value decay and, consequently, poor compression efficiency for the POD method.\n\n**C. Reducing the temporal sampling interval alone (i.e., taking snapshots more frequently in time without changing the spatial basis construction) ensures rapid singular value decay for a global POD basis because it resolves the front motion sufficiently.**\n\nThis statement is **Incorrect**. Reducing the temporal sampling interval $(\\Delta t \\to 0)$ would introduce more snapshots that are very similar to their neighbors. This increases the linear dependence within the snapshot matrix, which can make some of the trailing singular values smaller, but it does not change the fundamental geometric structure of the solution manifold. The POD basis must still be able to represent the front at all locations it visits over the time interval. The number of modes required to do this still scales with the ratio of the total distance traveled by the front to its width. Finer temporal sampling does not change the fact that the basis must be rich enough to construct a localized shape at many different positions, which is the root cause of the slow singular value decay.\n\n**D. A non-overlapping domain decomposition with flux-continuity coupling at interfaces, combined with localized POD bases on subdomains that include and track the front, yields a reduced-order model whose required dimension scales like $\\mathcal{O}\\!\\left(\\max_i L_i / w\\right)$, where $L_i$ are subdomain sizes; enforcing interface conditions preserves mass conservation while efficiently capturing localized dynamics.**\n\nThis statement is **Correct**. This describes a standard and effective strategy to overcome the limitations of global POD. By decomposing the domain $[0,L]$ into smaller subdomains of size $L_i \\ll L$, the problem is localized. For subdomains that the front passes through, a local POD basis is constructed. The number of modes required to represent the translation within that smaller subdomain now scales as $\\mathcal{O}(L_i/w)$, which is a significant improvement since $L_i  L$. For subdomains where the solution is smooth (i.e., far from the front), very few POD modes are needed. The total dimension of the reduced model is the sum of the dimensions of the local models, which will be much smaller than the $\\mathcal{O}(L/w)$ required for a global basis. Crucially, enforcing flux continuity (e.g., $D_e \\frac{\\partial c_e}{\\partial x}|_{x_{i}^-} = D_e \\frac{\\partial c_e}{\\partial x}|_{x_{i}^+}$) at the subdomain interfaces is the proper way to couple the local models and ensure that the overall reduced model respects the underlying conservation law for the species $c_e$.\n\n**E. Applying the Discrete Empirical Interpolation Method (DEIM) only to the nonlinear kinetic source term $j_n(x,t)$ resolves the difficulty with sharp fronts in the global POD, because front localization is primarily a consequence of nonlinearity rather than transport and diffusion scaling.**\n\nThis statement is **Incorrect**. DEIM is a hyper-reduction technique used to approximate the evaluation of nonlinear terms in a reduced-order model, thereby reducing computational cost. It works by projecting the nonlinear function onto a basis and evaluating it at a small number of \"magic\" points. While the nonlinear source term $j_n$ is the *cause* of the front formation, the *challenge* for the POD-Galerkin method is the *representation* of the resulting state vector $c_e(x,t)$, which exhibits the translating sharp feature. Applying DEIM to $j_n$ does not change the global POD basis used to approximate $c_e$. The basis for $c_e$ would still be inefficient, suffering from the slow singular value decay described in option B. DEIM addresses the computational bottleneck of evaluating the nonlinear right-hand side, not the state representation problem.",
            "answer": "$$\\boxed{BD}$$"
        },
        {
            "introduction": "Our final practice shifts from analyzing fixed snapshot sets to intelligently constructing them for parametric problems, a key task in automated design. You will implement a greedy adaptive algorithm that iteratively refines a POD basis by adding new snapshots where a residual-based error estimator is largest . This advanced exercise demonstrates how to build a compact, robust reduced-order model that maintains its accuracy across a continuous parameter space, moving beyond ad-hoc snapshot selection to a rigorous, error-controlled procedure.",
            "id": "3943535",
            "problem": "You are given a parametric, symmetric positive-definite linear system that represents a steady-state, one-dimensional diffusion-reaction model for a battery electrode discretized over $N$ nodes. The system for parameter $u \\in \\mathbb{R}$ is\n$$\nA(u) x(u) = f(u),\n$$\nwhere $A(u) \\in \\mathbb{R}^{N \\times N}$ and $f(u) \\in \\mathbb{R}^{N}$ are defined by the following scientifically grounded constructs:\n\n- The discrete Dirichlet Laplacian $L \\in \\mathbb{R}^{N \\times N}$ on a one-dimensional grid is\n$$\nL_{i,i} = 2,\\quad L_{i,i+1} = L_{i+1,i} = -1\\quad \\text{for } i=1,\\dots,N-1,\\quad \\text{and } L_{i,j}=0 \\text{ otherwise}.\n$$\n- The reaction-rate diagonal $D(u) = \\mathrm{diag}\\big(d_0 + u\\, d_1\\big)$ is defined by two positive vectors $d_0, d_1 \\in \\mathbb{R}^N$ with entries\n$$\n(d_0)_i = 0.5 + 0.5 \\sin\\left(\\frac{\\pi i}{N+1}\\right), \\quad (d_1)_i = 0.2 + 0.1 \\cos\\left(\\frac{\\pi i}{N+1}\\right),\n$$\nfor $i=1,\\dots,N$. Both $(d_0)_i$ and $(d_1)_i$ are nonnegative for all $i$.\n- The load vector $f(u)$ is\n$$\nf(u) = s + u\\, t,\n$$\nwhere $s \\in \\mathbb{R}^N$ has entries $s_i = 1.0$ and $t \\in \\mathbb{R}^N$ has entries\n$$\nt_i = \\exp\\left(-\\left(\\frac{i - (N+1)/2}{N/10}\\right)^2\\right),\n$$\nfor $i=1,\\dots,N$.\n\nSet $A(u) = L + D(u)$. Because $L$ is symmetric positive semi-definite and $D(u)$ is diagonal with strictly positive diagonal entries for any $u \\ge 0$, $A(u)$ is symmetric positive definite for all $u$ in the specified candidate sets below.\n\nYour task is to implement a greedy adaptive snapshot enrichment algorithm driven by an error estimator to refine the Proper Orthogonal Decomposition (POD) basis during the prospective optimization of electrode design. Proper Orthogonal Decomposition (POD) seeks an orthonormal basis $V \\in \\mathbb{R}^{N \\times r}$ that approximates the snapshot subspace spanned by $\\{x(u_k)\\}$, minimizing the projection error over the collected snapshots. Given the snapshot matrix $X = [x(u_1), x(u_2), \\dots, x(u_m)] \\in \\mathbb{R}^{N \\times m}$, the POD basis is obtained from the left singular vectors of $X$.\n\nFor a candidate parameter $\\theta$, given a POD basis $V$, define the reduced-order solution $x_r(\\theta) = V y(\\theta)$ where $y(\\theta) \\in \\mathbb{R}^r$ solves the reduced Galerkin system\n$$\n\\left(V^\\top A(\\theta) V\\right) y(\\theta) = V^\\top f(\\theta).\n$$\nDefine the residual $r(\\theta) = f(\\theta) - A(\\theta) x_r(\\theta)$. The error estimator is the energy norm of the error, which for symmetric positive definite $A(\\theta)$ can be computed exactly via\n$$\n\\eta(\\theta) = \\sqrt{r(\\theta)^\\top A(\\theta)^{-1} r(\\theta)}.\n$$\n\nYou must implement the following greedy algorithm:\n\n1. Initialize with an initial snapshot set $\\mathcal{S}_0 = \\{u^{(1)}, u^{(2)}\\}$ and construct the initial POD basis $V$ from the corresponding full-order solutions. In this problem, take $u^{(1)} = 0.0$ and $u^{(2)} = 1.0$.\n2. Given a candidate parameter set $\\Theta$, at each greedy iteration evaluate $\\eta(\\theta)$ for all $\\theta \\in \\Theta$ not yet in the snapshot set. Select $\\theta^\\star$ that maximizes $\\eta(\\theta)$.\n3. If $\\eta(\\theta^\\star) \\le \\varepsilon$ (tolerance), terminate; otherwise, compute the full-order solution $x(\\theta^\\star)$, augment the snapshot set, and recompute the POD basis $V$ from all collected snapshots.\n4. Repeat until the tolerance is met or the snapshot budget $m_{\\max}$ (maximum number of total snapshots) is reached.\n\nFor verification, after termination compute:\n- The final basis dimension $r$.\n- The maximum error estimator over all candidates $\\max_{\\theta \\in \\Theta} \\eta(\\theta)$.\n- The maximum actual energy-norm error over all candidates\n$$\nE_{\\max} = \\max_{\\theta \\in \\Theta} \\sqrt{\\big(x(\\theta) - x_r(\\theta)\\big)^\\top A(\\theta) \\big(x(\\theta) - x_r(\\theta)\\big)}.\n$$\n- A boolean indicating whether $E_{\\max} \\le \\varepsilon$ holds.\n\nImplement the program for $N=50$ and the following test suite. All quantities are dimensionless; there are no physical units to report.\n\nTest suite:\n- Case 1 (general case): $\\Theta = \\{0.0, 0.1, 0.2, \\dots, 1.0\\}$, tolerance $\\varepsilon = 10^{-6}$, budget $m_{\\max} = 11$.\n- Case 2 (budget-limited): $\\Theta = \\{0.0, 0.25, 0.5, 0.75, 1.0\\}$, tolerance $\\varepsilon = 10^{-3}$, budget $m_{\\max} = 3$.\n- Case 3 (duplicate candidates edge case): $\\Theta = \\{0.0, 0.0, 0.5, 0.5, 1.0\\}$, tolerance $\\varepsilon = 10^{-2}$, budget $m_{\\max} = 5$.\n\nYour program should produce a single line of output containing the results across all cases as a comma-separated list enclosed in square brackets. Each case’s result must be a list of the form $[r, \\eta_{\\max}, E_{\\max}, \\text{satisfied}]$, where $r$ is an integer, $\\eta_{\\max}$ and $E_{\\max}$ are floats, and $\\text{satisfied}$ is a boolean. The final output must therefore be a single line of the form\n```\n[[r_1,eta_max,1,E_max,1,satisfied_1],[r_2,eta_max,2,E_max,2,satisfied_2],[r_3,eta_max,3,E_max,3,satisfied_3]]\n```",
            "solution": "We begin with the governing steady-state diffusion-reaction model on a one-dimensional grid, which follows from conservation of species and Fick’s law. The discretized form using a Dirichlet Laplacian and a spatially varying reaction term yields a symmetric positive definite linear system\n$$\nA(u) x(u) = f(u),\n$$\nwhere $A(u) = L + \\mathrm{diag}(d_0 + u d_1)$ and $L$ is the standard tridiagonal matrix with $2$ on the diagonal and $-1$ on the first off-diagonals. Because $L$ is symmetric positive semi-definite and the diagonal contribution is strictly positive for the specified parameter ranges, $A(u)$ is symmetric positive definite for $u \\ge 0$ and thus admits a unique solution $x(u)$ for each parameter value.\n\nThe aim of Proper Orthogonal Decomposition (POD) is to construct an orthonormal basis $V \\in \\mathbb{R}^{N \\times r}$ that best approximates the snapshot subspace generated by collected full-order solutions $\\{x(u_k)\\}$. The canonical characterization of the POD basis is the solution to the variational problem that minimizes the sum of squared projection errors over the snapshot set. This yields that $V$ is formed by the leading left singular vectors of the snapshot matrix $X = [x(u_1), \\dots, x(u_m)]$ obtained through the singular value decomposition $X = U \\Sigma W^\\top$, where $U$ contains orthonormal columns and $\\Sigma$ contains singular values. Choosing $V$ as the first $r$ columns of $U$ corresponding to the nonzero singular values minimizes the Frobenius norm of the projection error.\n\nA reduced-order model is obtained by Galerkin projection onto the POD basis $V$. For a parameter $\\theta$, define the reduced coordinates $y(\\theta) \\in \\mathbb{R}^r$ as the solution to\n$$\n\\left(V^\\top A(\\theta) V\\right) y(\\theta) = V^\\top f(\\theta),\n$$\nand the reduced-order state $x_r(\\theta) = V y(\\theta)$. Because $A(\\theta)$ is symmetric positive definite and $V$ has orthonormal columns, the reduced operator $V^\\top A(\\theta) V$ is also symmetric positive definite, so $y(\\theta)$ exists uniquely.\n\nTo guide greedy snapshot enrichment, we use the residual-based energy-norm error estimator. Define the residual\n$$\nr(\\theta) = f(\\theta) - A(\\theta) x_r(\\theta).\n$$\nThe full-order solution satisfies $A(\\theta) x(\\theta) = f(\\theta)$, so the error $e(\\theta) = x(\\theta) - x_r(\\theta)$ obeys\n$$\nA(\\theta) e(\\theta) = r(\\theta).\n$$\nFor symmetric positive definite $A(\\theta)$, the energy norm of the error is\n$$\n\\|e(\\theta)\\|_{A(\\theta)} = \\sqrt{e(\\theta)^\\top A(\\theta) e(\\theta)}.\n$$\nUsing $A(\\theta) e(\\theta) = r(\\theta)$ and symmetry,\n$$\n\\|e(\\theta)\\|_{A(\\theta)}^2 = e(\\theta)^\\top A(\\theta) e(\\theta) = e(\\theta)^\\top r(\\theta) = r(\\theta)^\\top A(\\theta)^{-1} r(\\theta),\n$$\nwhich implies\n$$\n\\|e(\\theta)\\|_{A(\\theta)} = \\sqrt{r(\\theta)^\\top A(\\theta)^{-1} r(\\theta)}.\n$$\nTherefore, the estimator\n$$\n\\eta(\\theta) = \\sqrt{r(\\theta)^\\top A(\\theta)^{-1} r(\\theta)}\n$$\nequals the true energy-norm error for the reduced Galerkin solution. Computationally, this may be evaluated by solving $A(\\theta) z(\\theta) = r(\\theta)$ and computing $\\eta(\\theta) = \\sqrt{r(\\theta)^\\top z(\\theta)}$.\n\nThe greedy algorithm proceeds as follows:\n- Start with an initial snapshot set $\\mathcal{S}_0 = \\{u^{(1)}, u^{(2)}\\}$, compute the full-order solutions $x(u^{(1)})$ and $x(u^{(2)})$, and build the initial POD basis $V$ from these snapshots via singular value decomposition.\n- For each unsampled candidate $\\theta$ in $\\Theta$, compute the reduced solution $x_r(\\theta)$ by solving the reduced system, form the residual $r(\\theta)$, and compute the estimator $\\eta(\\theta)$ via the energy-norm identity. Select the parameter with the largest $\\eta(\\theta)$ to maximize information gain.\n- If the largest estimator does not exceed the tolerance $\\varepsilon$, terminate. Otherwise, augment the snapshot set with the full-order solution at the selected parameter, recompute the POD basis $V$ from all snapshots, and repeat until the tolerance criterion is satisfied or the snapshot budget $m_{\\max}$ is reached.\n\nBecause the POD basis is recomputed from an expanded snapshot matrix at each iteration, the reduced subspace can only increase (or remain the same if the new snapshot is collinear with the existing span) and the Galerkin projection minimizes the energy-norm error over the current subspace. As a consequence, the maximum energy-norm error over the finite candidate set $\\Theta$ is nonincreasing across iterations, and the greedy selection using the largest estimated error prioritizes parameters that most challenge the current reduced-order model.\n\nFinally, after termination, we verify performance by computing:\n- The final basis dimension $r$, equal to the rank of the snapshot matrix used to build the POD basis.\n- The maximum estimator $\\eta_{\\max} = \\max_{\\theta \\in \\Theta} \\eta(\\theta)$.\n- The maximum actual energy-norm error $E_{\\max} = \\max_{\\theta \\in \\Theta} \\|x(\\theta) - x_r(\\theta)\\|_{A(\\theta)}$.\n- The boolean condition $\\text{satisfied} = (E_{\\max} \\le \\varepsilon)$.\n\nThe implementation uses linear algebra operations to construct $L$, $A(u)$, and $f(u)$; solve full-order and reduced-order systems; compute singular value decompositions for POD; and iteratively enrich the snapshot set according to the error estimator. The test suite covers a general case, a budget-limited case to examine termination without meeting tolerance, and an edge case with duplicate candidates to ensure robustness under redundant parameter entries. The final output follows the specified single-line nested list format.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef build_dirichlet_laplacian(n: int) -> np.ndarray:\n    \"\"\"Build 1D Dirichlet Laplacian matrix L of size n x n.\"\"\"\n    L = np.zeros((n, n), dtype=float)\n    # Tridiagonal: 2 on diagonal, -1 on off-diagonals\n    for i in range(n):\n        L[i, i] = 2.0\n        if i - 1 >= 0:\n            L[i, i - 1] = -1.0\n        if i + 1  n:\n            L[i, i + 1] = -1.0\n    return L\n\ndef build_reaction_vectors(n: int) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"Build d0 and d1 reaction-rate vectors.\"\"\"\n    idx = np.arange(1, n + 1, dtype=float)\n    d0 = 0.5 + 0.5 * np.sin(np.pi * idx / (n + 1))\n    d1 = 0.2 + 0.1 * np.cos(np.pi * idx / (n + 1))\n    return d0, d1\n\ndef build_load_vectors(n: int) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"Build s and t load vectors.\"\"\"\n    s = np.ones(n, dtype=float)\n    center = (n + 1) / 2.0\n    sigma = n / 10.0\n    idx = np.arange(1, n + 1, dtype=float)\n    t = np.exp(-((idx - center) / sigma) ** 2)\n    return s, t\n\ndef assemble_system(u: float, L: np.ndarray, d0: np.ndarray, d1: np.ndarray, s: np.ndarray, t: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"Assemble A(u) and f(u).\"\"\"\n    diag = d0 + u * d1\n    A = L + np.diag(diag)\n    f = s + u * t\n    return A, f\n\ndef full_solution(u: float, L: np.ndarray, d0: np.ndarray, d1: np.ndarray, s: np.ndarray, t: np.ndarray) -> np.ndarray:\n    \"\"\"Compute the full-order solution x(u).\"\"\"\n    A, f = assemble_system(u, L, d0, d1, s, t)\n    x = np.linalg.solve(A, f)\n    return x\n\ndef pod_basis_from_snapshots(snapshots: list[np.ndarray], tol: float = 1e-12) -> np.ndarray:\n    \"\"\"Compute POD basis V from snapshot list using SVD.\n    Returns V with orthonormal columns. Rank determined by singular values above tol.\n    \"\"\"\n    if len(snapshots) == 0:\n        raise ValueError(\"No snapshots provided for POD basis.\")\n    X = np.column_stack(snapshots)  # N x m\n    U, S, VT = np.linalg.svd(X, full_matrices=False)\n    rank = int(np.sum(S > tol))\n    if rank == 0:\n        # If all singular values are tiny, keep at least one vector to avoid empty basis\n        rank = 1\n    V = U[:, :rank]\n    return V\n\ndef reduced_solution(u: float, V: np.ndarray, L: np.ndarray, d0: np.ndarray, d1: np.ndarray, s: np.ndarray, t: np.ndarray) -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Compute reduced-order solution x_r(u) = V y, residual r, and A, f.\"\"\"\n    A, f = assemble_system(u, L, d0, d1, s, t)\n    # Reduced Galerkin system\n    Ar = V.T @ A @ V\n    fr = V.T @ f\n    y = np.linalg.solve(Ar, fr)\n    x_r = V @ y\n    r_vec = f - A @ x_r\n    return x_r, r_vec, A\n\ndef energy_norm_error_estimator(r_vec: np.ndarray, A: np.ndarray) -> float:\n    \"\"\"Compute the exact energy-norm error estimator sqrt(r^T A^{-1} r).\"\"\"\n    z = np.linalg.solve(A, r_vec)\n    eta = float(np.sqrt(r_vec.T @ z))\n    return eta\n\ndef energy_norm_error(x_full: np.ndarray, x_red: np.ndarray, A: np.ndarray) -> float:\n    \"\"\"Compute energy-norm error sqrt((x - x_r)^T A (x - x_r)).\"\"\"\n    e = x_full - x_red\n    val = float(np.sqrt(e.T @ (A @ e)))\n    return val\n\ndef greedy_adaptive_pod(theta_candidates: list[float], epsilon: float, m_max: int, initial_params: list[float], n: int) -> list:\n    \"\"\"Run the greedy adaptive snapshot enrichment algorithm.\n    Returns [final_basis_dim, max_estimator, max_actual_error, satisfied_boolean].\n    \"\"\"\n    # Build global components\n    L = build_dirichlet_laplacian(n)\n    d0, d1 = build_reaction_vectors(n)\n    s, t = build_load_vectors(n)\n\n    # Make candidate set unique and sorted for determinism\n    theta_unique = sorted(set(theta_candidates))\n    # Initialize snapshot set with unique initial params (ensure they are considered sampled)\n    sampled_params = []\n    snapshots = []\n    for u0 in initial_params:\n        if u0 not in sampled_params:\n            x0 = full_solution(u0, L, d0, d1, s, t)\n            sampled_params.append(u0)\n            snapshots.append(x0)\n\n    # Enforce budget: if initial exceeds budget, truncate (edge safeguard)\n    if len(sampled_params) > m_max:\n        sampled_params = sampled_params[:m_max]\n        snapshots = snapshots[:m_max]\n\n    # Build initial POD basis\n    V = pod_basis_from_snapshots(snapshots)\n\n    # Greedy loop\n    # If budget allows adding more snapshots\n    while len(sampled_params)  m_max:\n        # Evaluate estimator over unsampled candidates\n        best_theta = None\n        best_eta = -np.inf\n        for theta in theta_unique:\n            if theta in sampled_params:\n                continue\n            x_r, r_vec, A = reduced_solution(theta, V, L, d0, d1, s, t)\n            eta = energy_norm_error_estimator(r_vec, A)\n            if eta > best_eta:\n                best_eta = eta\n                best_theta = theta\n        # If no unsampled candidates remain, break\n        if best_theta is None:\n            break\n        # Check tolerance\n        if best_eta = epsilon:\n            break\n        # Enrich snapshot set with full solution at best_theta\n        x_best = full_solution(best_theta, L, d0, d1, s, t)\n        sampled_params.append(best_theta)\n        snapshots.append(x_best)\n        # Recompute POD basis\n        V = pod_basis_from_snapshots(snapshots)\n\n    # After termination, compute final metrics over all candidates\n    max_eta = -np.inf\n    max_actual_err = -np.inf\n    for theta in theta_unique:\n        # Reduced solution and residual\n        x_r, r_vec, A = reduced_solution(theta, V, L, d0, d1, s, t)\n        eta = energy_norm_error_estimator(r_vec, A)\n        if eta > max_eta:\n            max_eta = eta\n        # Actual energy-norm error\n        x_full = np.linalg.solve(A, s + theta * t)\n        e = energy_norm_error(x_full, x_r, A)\n        if e > max_actual_err:\n            max_actual_err = e\n\n    final_basis_dim = V.shape[1]\n    satisfied = max_actual_err = epsilon\n    return [final_basis_dim, max_eta, max_actual_err, satisfied]\n\ndef solve():\n    # Define problem size\n    N = 50\n\n    # Test cases as specified\n    test_cases = [\n        # Case 1: General case\n        {\n            \"theta\": [0.1 * k for k in range(11)],  # 0.0 to 1.0 inclusive\n            \"epsilon\": 1e-6,\n            \"m_max\": 11,\n            \"initial\": [0.0, 1.0],\n        },\n        # Case 2: Budget-limited\n        {\n            \"theta\": [0.0, 0.25, 0.5, 0.75, 1.0],\n            \"epsilon\": 1e-3,\n            \"m_max\": 3,\n            \"initial\": [0.0, 1.0],\n        },\n        # Case 3: Duplicate candidates edge case\n        {\n            \"theta\": [0.0, 0.0, 0.5, 0.5, 1.0],\n            \"epsilon\": 1e-2,\n            \"m_max\": 5,\n            \"initial\": [0.0, 1.0],\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        res = greedy_adaptive_pod(\n            theta_candidates=case[\"theta\"],\n            epsilon=case[\"epsilon\"],\n            m_max=case[\"m_max\"],\n            initial_params=case[\"initial\"],\n            n=N,\n        )\n        results.append(res)\n\n    # Final print statement in the exact required format.\n    # Nested list of [int, float, float, bool] per case\n    def format_item(item):\n        # Ensure Python booleans print as True/False\n        return f\"[{item[0]},{item[1]},{item[2]},{item[3]}]\"\n    print(f\"[{','.join(format_item(r) for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}