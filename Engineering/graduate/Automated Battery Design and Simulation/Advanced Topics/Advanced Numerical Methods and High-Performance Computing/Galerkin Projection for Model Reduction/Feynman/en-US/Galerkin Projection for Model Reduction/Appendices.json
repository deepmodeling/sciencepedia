{
    "hands_on_practices": [
        {
            "introduction": "The cornerstone of Galerkin projection is the creation of a low-dimensional basis that effectively represents the system's behavior. In many physical systems, a standard Euclidean notion of orthogonality is insufficient. This exercise  introduces the critical concept of an inner product weighted by the system's mass matrix ($M$-inner product), which arises naturally from the weak formulation and helps preserve energy-like quantities. You will gain hands-on experience applying the Gram-Schmidt process within this generalized geometric framework to construct a basis that is truly \"orthonormal\" with respect to the system's intrinsic properties.",
            "id": "3915341",
            "problem": "In automated battery design and simulation, reduced-order models of electrolyte concentration dynamics are often derived by Galerkin projection of a high-dimensional semi-discrete system onto a low-dimensional subspace spanned by basis vectors. The Galerkin projection uses the mass matrix from the weak formulation to define the inner product and enforce orthonormality of the reduced basis. Consider a two-degree-of-freedom electrolyte diffusion state vector $x(t) \\in \\mathbb{R}^{2}$ representing spatially averaged concentration states at two finite volume nodes in a pouch cell. The semi-discrete weak form yields a symmetric positive definite (SPD) mass matrix $M \\in \\mathbb{R}^{2 \\times 2}$ such that the inner product is defined by $(u,v)_{M} = u^{\\top} M v$. Suppose that $M = \\mathrm{diag}(2,3)$ arises from mass-lumping of two control volumes with weights proportional to their volumes, and two linearly independent snapshot vectors $w_{1}, w_{2} \\in \\mathbb{R}^{2}$ are obtained from detailed simulation: $w_{1} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$ and $w_{2} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$.\n\nStarting from the definition of the $M$-inner product and the requirement of orthonormality for a basis $\\{v_{i}\\}$, construct an $M$-orthonormal basis $\\{v_{1}, v_{2}\\}$ from $\\{w_{1}, w_{2}\\}$ using the Gram–Schmidt process with respect to $(u,v)_{M} = u^{\\top} M v$. Form the matrix $V = \\begin{pmatrix} v_{1} & v_{2} \\end{pmatrix} \\in \\mathbb{R}^{2 \\times 2}$ and verify orthonormality by evaluating the Frobenius norm of the deviation from the identity, $\\|V^{\\top} M V - I\\|_{F}$, where $I$ is the $2 \\times 2$ identity matrix and $\\|\\cdot\\|_{F}$ denotes the Frobenius norm.\n\nReport the single real number $\\|V^{\\top} M V - I\\|_{F}$ as your final answer. Express the value as a pure number without units. If any numerical approximation is necessary, round your answer to four significant figures.",
            "solution": "The problem requires the construction of an orthonormal basis from a given set of vectors with respect to a non-standard inner product defined by a mass matrix $M$. This process is fundamental in Galerkin projection methods for model order reduction. After constructing the basis, we must verify its orthonormality by computing a specific matrix norm.\n\nFirst, we establish the given quantities. The mass matrix is $M = \\mathrm{diag}(2,3) = \\begin{pmatrix} 2 & 0 \\\\ 0 & 3 \\end{pmatrix}$. This matrix is symmetric and positive definite, with eigenvalues $2$ and $3$, thus it can properly define an inner product. The inner product is given by $(u,v)_{M} = u^{\\top} M v$ for any vectors $u, v \\in \\mathbb{R}^{2}$. The initial set of linearly independent vectors is $\\{w_{1}, w_{2}\\}$, where $w_{1} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$ and $w_{2} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$.\n\nWe apply the Gram-Schmidt process to the set $\\{w_{1}, w_{2}\\}$ to obtain an $M$-orthonormal basis $\\{v_{1}, v_{2}\\}$.\n\nStep 1: Construct the first orthonormal vector $v_{1}$.\nLet the first unnormalized vector be $u_{1} = w_{1} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$.\nWe compute its norm with respect to the $M$-inner product, denoted as $\\|u_{1}\\|_{M}$.\n$$ \\|u_{1}\\|_{M}^{2} = (u_{1}, u_{1})_{M} = u_{1}^{\\top} M u_{1} = \\begin{pmatrix} 1 & 1 \\end{pmatrix} \\begin{pmatrix} 2 & 0 \\\\ 0 & 3 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 & 1 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 3 \\end{pmatrix} = 2 \\cdot 1 + 3 \\cdot 1 = 5 $$\nThe norm is $\\|u_{1}\\|_{M} = \\sqrt{5}$.\nThe first orthonormal vector $v_{1}$ is obtained by normalizing $u_{1}$:\n$$ v_{1} = \\frac{u_{1}}{\\|u_{1}\\|_{M}} = \\frac{1}{\\sqrt{5}} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} $$\n\nStep 2: Construct the second orthonormal vector $v_{2}$.\nFirst, we find a vector $u_{2}$ that is $M$-orthogonal to $v_{1}$ by subtracting the projection of $w_{2}$ onto $v_{1}$ from $w_{2}$.\n$$ u_{2} = w_{2} - (w_{2}, v_{1})_{M} v_{1} $$\nWe compute the inner product $(w_{2}, v_{1})_{M}$:\n$$ (w_{2}, v_{1})_{M} = w_{2}^{\\top} M v_{1} = \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} 2 & 0 \\\\ 0 & 3 \\end{pmatrix} \\left( \\frac{1}{\\sqrt{5}} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\right) = \\frac{1}{\\sqrt{5}} \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 3 \\end{pmatrix} = \\frac{2}{\\sqrt{5}} $$\nNow we can compute $u_{2}$:\n$$ u_{2} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} - \\left( \\frac{2}{\\sqrt{5}} \\right) \\left( \\frac{1}{\\sqrt{5}} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\right) = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} - \\frac{2}{5} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 - \\frac{2}{5} \\\\ 0 - \\frac{2}{5} \\end{pmatrix} = \\begin{pmatrix} \\frac{3}{5} \\\\ -\\frac{2}{5} \\end{pmatrix} $$\nNext, we normalize $u_{2}$ to obtain $v_{2}$. We compute the squared norm $\\|u_{2}\\|_{M}^{2}$:\n$$ \\|u_{2}\\|_{M}^{2} = (u_{2}, u_{2})_{M} = u_{2}^{\\top} M u_{2} = \\begin{pmatrix} \\frac{3}{5} & -\\frac{2}{5} \\end{pmatrix} \\begin{pmatrix} 2 & 0 \\\\ 0 & 3 \\end{pmatrix} \\begin{pmatrix} \\frac{3}{5} \\\\ -\\frac{2}{5} \\end{pmatrix} $$\n$$ = \\begin{pmatrix} \\frac{6}{5} & -\\frac{6}{5} \\end{pmatrix} \\begin{pmatrix} \\frac{3}{5} \\\\ -\\frac{2}{5} \\end{pmatrix} = \\left(\\frac{6}{5}\\right)\\left(\\frac{3}{5}\\right) + \\left(-\\frac{6}{5}\\right)\\left(-\\frac{2}{5}\\right) = \\frac{18}{25} + \\frac{12}{25} = \\frac{30}{25} = \\frac{6}{5} $$\nThe norm is $\\|u_{2}\\|_{M} = \\sqrt{\\frac{6}{5}}$.\nThe second orthonormal vector $v_{2}$ is:\n$$ v_{2} = \\frac{u_{2}}{\\|u_{2}\\|_{M}} = \\frac{1}{\\sqrt{\\frac{6}{5}}} \\begin{pmatrix} \\frac{3}{5} \\\\ -\\frac{2}{5} \\end{pmatrix} = \\frac{\\sqrt{5}}{\\sqrt{6}} \\frac{1}{5} \\begin{pmatrix} 3 \\\\ -2 \\end{pmatrix} = \\frac{1}{\\sqrt{5}\\sqrt{6}} \\begin{pmatrix} 3 \\\\ -2 \\end{pmatrix} = \\frac{1}{\\sqrt{30}} \\begin{pmatrix} 3 \\\\ -2 \\end{pmatrix} $$\nThe resulting $M$-orthonormal basis is $\\{v_{1}, v_{2}\\}$ where $v_{1} = \\frac{1}{\\sqrt{5}}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$ and $v_{2} = \\frac{1}{\\sqrt{30}}\\begin{pmatrix} 3 \\\\ -2 \\end{pmatrix}$.\n\nStep 3: Form the matrix $V$ and evaluate the specified norm.\nThe matrix $V$ is formed by using $v_1$ and $v_2$ as its columns:\n$$ V = \\begin{pmatrix} v_{1} & v_{2} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{\\sqrt{5}} & \\frac{3}{\\sqrt{30}} \\\\ \\frac{1}{\\sqrt{5}} & -\\frac{2}{\\sqrt{30}} \\end{pmatrix} $$\nThe problem requires computing $\\|V^{\\top} M V - I\\|_{F}$. By the definition of an $M$-orthonormal basis, the matrix $V$ must satisfy the property $V^{\\top} M V = I$, where $I$ is the identity matrix. The elements of the matrix $V^{\\top} M V$ are given by $(V^{\\top} M V)_{ij} = v_i^{\\top} M v_j = (v_i, v_j)_M$. Since the basis $\\{v_1, v_2\\}$ is $M$-orthonormal, we have $(v_i, v_j)_M = \\delta_{ij}$, where $\\delta_{ij}$ is the Kronecker delta.\n\nLet's verify this property through explicit calculation:\n$$ V^{\\top} M V = \\begin{pmatrix} v_1^{\\top} \\\\ v_2^{\\top} \\end{pmatrix} M \\begin{pmatrix} v_1 & v_2 \\end{pmatrix} = \\begin{pmatrix} v_1^{\\top} M v_1 & v_1^{\\top} M v_2 \\\\ v_2^{\\top} M v_1 & v_2^{\\top} M v_2 \\end{pmatrix} = \\begin{pmatrix} (v_1, v_1)_M & (v_1, v_2)_M \\\\ (v_2, v_1)_M & (v_2, v_2)_M \\end{pmatrix} $$\nFrom our previous calculations:\n$(v_1, v_1)_M = \\|v_1\\|_M^2 = 1$\n$(v_2, v_2)_M = \\|v_2\\|_M^2 = 1$\nAnd by construction, $v_1$ and $v_2$ are orthogonal:\n$(v_1, v_2)_M = v_1^{\\top} M v_2 = \\left( \\frac{1}{\\sqrt{5}} \\begin{pmatrix} 1 & 1 \\end{pmatrix} \\right) \\begin{pmatrix} 2 & 0 \\\\ 0 & 3 \\end{pmatrix} \\left( \\frac{1}{\\sqrt{30}} \\begin{pmatrix} 3 \\\\ -2 \\end{pmatrix} \\right) = \\frac{1}{\\sqrt{150}} \\begin{pmatrix} 1 & 1 \\end{pmatrix} \\begin{pmatrix} 6 \\\\ -6 \\end{pmatrix} = \\frac{1}{\\sqrt{150}} (6-6) = 0$.\nSince the inner product is symmetric, $(v_2, v_1)_M = (v_1, v_2)_M = 0$.\n\nTherefore, the matrix product is:\n$$ V^{\\top} M V = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = I $$\nWe are asked to compute $\\|V^{\\top} M V - I\\|_{F}$. Substituting the result:\n$$ V^{\\top} M V - I = I - I = \\begin{pmatrix} 0 & 0 \\\\ 0 & 0 \\end{pmatrix} = O $$\nwhere $O$ is the $2 \\times 2$ zero matrix.\n\nThe Frobenius norm of a matrix $A$ is defined as $\\|A\\|_{F} = \\sqrt{\\sum_{i=1}^{m}\\sum_{j=1}^{n} |a_{ij}|^2}$. For the zero matrix, this is:\n$$ \\|O\\|_{F} = \\sqrt{0^2 + 0^2 + 0^2 + 0^2} = 0 $$\nThe calculation is exact and does not require any numerical approximation. The result is precisely $0$.",
            "answer": "$$\\boxed{0}$$"
        },
        {
            "introduction": "After projecting the governing equations onto a reduced basis, we are left with a system of ordinary differential equations (ODEs) that must be solved over time. This practice  bridges the gap between the static reduced model and a dynamic simulation. You will derive the update rule for the backward Euler method, a robust and widely used time-stepping scheme that is essential for handling the \"stiff\" dynamics characteristic of fast and slow processes in battery models. The exercise further explores the crucial topic of numerical stability by analyzing the method's amplification factor, providing insight into why the simulation remains reliable even with large time steps.",
            "id": "3915345",
            "problem": "Consider a linearized, semi-discrete reduced-order model for battery electrode-ion dynamics obtained by applying a Galerkin projection to the spatially discretized conservation laws and Fickian diffusion. Let the reduced state be $x_{r}(t) \\in \\mathbb{R}^{r}$, and the model be\n$$\nM_{r} \\,\\dot{x}_{r}(t) + K_{r} \\, x_{r}(t) = f_{r}(t),\n$$\nwhere $M_{r} \\in \\mathbb{R}^{r \\times r}$ is a symmetric positive definite mass matrix arising from the inner products of reduced basis functions, $K_{r} \\in \\mathbb{R}^{r \\times r}$ is a symmetric positive semidefinite stiffness matrix reflecting diffusive and reactive transport, and $f_{r}(t) \\in \\mathbb{R}^{r}$ encodes reduced forcing from boundary fluxes and source terms. Assume the reduced basis has been obtained via the standard Galerkin approach with test functions equal to trial functions, and that the linearization yields constant-in-time $M_{r}$ and $K_{r}$ over the interval of interest.\n\nYou will discretize in time using the backward Euler (also called implicit Euler) method with a uniform time step $\\Delta t > 0$. Starting from the above reduced-order dynamics, and using only the definition of backward Euler, derive the one-step, fully discrete update for the reduced state. Express the update in closed form as an explicit mapping that produces $x_{r}^{n+1}$ from $x_{r}^{n}$ and $f_{r}^{n+1}$.\n\nNext, analyze robustness for stiff dynamics characteristic of batteries, such as fast solid-phase diffusion modes. Consider the homogeneous case with $f_{r}(t) \\equiv 0$ and assume $(\\lambda_{i}, v_{i})$ are generalized eigenpairs satisfying $K_{r} v_{i} = \\lambda_{i} M_{r} v_{i}$ with $\\lambda_{i} \\ge 0$. Derive the per-mode amplification factor $g(\\lambda_{i}, \\Delta t)$ such that the modal coefficient updates as $\\alpha_{i}^{n+1} = g(\\lambda_{i}, \\Delta t)\\,\\alpha_{i}^{n}$ when $x_{r} = \\sum_{i} \\alpha_{i} v_{i}$. Provide this amplification factor in closed form.\n\nYour final output must be a pair of analytical expressions: the explicit backward Euler update mapping for $x_{r}^{n+1}$ in terms of $M_{r}$, $K_{r}$, $\\Delta t$, $x_{r}^{n}$, and $f_{r}^{n+1}$, and the per-mode amplification factor $g(\\lambda, \\Delta t)$. No numerical evaluation is required.",
            "solution": "The first task is to derive the one-step, fully discrete update for the reduced state $x_{r}(t)$ using the backward Euler method. The governing equation is the linearized reduced-order model:\n$$\nM_{r} \\,\\dot{x}_{r}(t) + K_{r} \\, x_{r}(t) = f_{r}(t)\n$$\nThe backward Euler method, an implicit first-order method, approximates the time derivative $\\dot{x}_{r}(t)$ at time $t_{n+1} = (n+1)\\Delta t$ using the states at time $t_{n+1}$ and $t_{n} = n\\Delta t$. Let $x_{r}^{n} \\approx x_{r}(t_n)$. The approximation of the derivative is:\n$$\n\\dot{x}_{r}(t_{n+1}) \\approx \\frac{x_{r}^{n+1} - x_{r}^{n}}{\\Delta t}\n$$\nSubstituting this approximation into the governing equation, evaluated at time $t_{n+1}$, we get the fully discrete system:\n$$\nM_{r} \\left( \\frac{x_{r}^{n+1} - x_{r}^{n}}{\\Delta t} \\right) + K_{r} x_{r}^{n+1} = f_{r}^{n+1}\n$$\nwhere $f_{r}^{n+1} = f_{r}(t_{n+1})$. Our goal is to derive an explicit expression for $x_{r}^{n+1}$. To do this, we rearrange the equation to group terms involving $x_{r}^{n+1}$:\n$$\n\\frac{1}{\\Delta t} M_{r} x_{r}^{n+1} - \\frac{1}{\\Delta t} M_{r} x_{r}^{n} + K_{r} x_{r}^{n+1} = f_{r}^{n+1}\n$$\n$$\n\\left( \\frac{1}{\\Delta t} M_{r} + K_{r} \\right) x_{r}^{n+1} = \\frac{1}{\\Delta t} M_{r} x_{r}^{n} + f_{r}^{n+1}\n$$\nTo simplify, we can multiply the entire equation by the time step $\\Delta t > 0$:\n$$\n(M_{r} + \\Delta t K_{r}) x_{r}^{n+1} = M_{r} x_{r}^{n} + \\Delta t f_{r}^{n+1}\n$$\nTo solve for $x_{r}^{n+1}$, we must invert the matrix $(M_{r} + \\Delta t K_{r})$. We must verify that this matrix is invertible. The problem states that $M_{r}$ is symmetric positive definite (SPD) and $K_{r}$ is symmetric positive semidefinite (SPSD). For any non-zero vector $v \\in \\mathbb{R}^{r}$, we have $v^{T}M_{r}v > 0$ and $v^{T}K_{r}v \\ge 0$. Since $\\Delta t > 0$, we also have $\\Delta t \\, v^{T}K_{r}v \\ge 0$.\nConsider the quadratic form of the matrix $(M_{r} + \\Delta t K_{r})$:\n$$\nv^{T} (M_{r} + \\Delta t K_{r}) v = v^{T} M_{r} v + \\Delta t (v^{T} K_{r} v)\n$$\nSince $v^{T} M_{r} v > 0$ and $\\Delta t (v^{T} K_{r} v) \\ge 0$, their sum is strictly positive: $v^{T} (M_{r} + \\Delta t K_{r}) v > 0$. This shows that the matrix $(M_{r} + \\Delta t K_{r})$ is symmetric positive definite, and therefore invertible.\nWe can now pre-multiply by the inverse to obtain the explicit mapping for $x_{r}^{n+1}$:\n$$\nx_{r}^{n+1} = (M_{r} + \\Delta t K_{r})^{-1} (M_{r} x_{r}^{n} + \\Delta t f_{r}^{n+1})\n$$\nThis is the first required expression.\n\nThe second task is to find the per-mode amplification factor $g(\\lambda, \\Delta t)$ for the homogeneous case, where $f_{r}(t) \\equiv 0$. The discrete update equation simplifies to:\n$$\n(M_{r} + \\Delta t K_{r}) x_{r}^{n+1} = M_{r} x_{r}^{n}\n$$\nWe express the state vector in the basis of generalized eigenvectors $v_{i}$, which satisfy $K_{r} v_{i} = \\lambda_{i} M_{r} v_{i}$. The state at time steps $n$ and $n+1$ can be written as modal expansions:\n$$\nx_{r}^{n} = \\sum_{i=1}^{r} \\alpha_{i}^{n} v_{i} \\quad \\text{and} \\quad x_{r}^{n+1} = \\sum_{i=1}^{r} \\alpha_{i}^{n+1} v_{i}\n$$\nSubstituting these expansions into the homogeneous discrete update equation:\n$$\n(M_{r} + \\Delta t K_{r}) \\sum_{i=1}^{r} \\alpha_{i}^{n+1} v_{i} = M_{r} \\sum_{i=1}^{r} \\alpha_{i}^{n} v_{i}\n$$\nBy linearity of the matrix-vector product:\n$$\n\\sum_{i=1}^{r} \\alpha_{i}^{n+1} (M_{r} + \\Delta t K_{r}) v_{i} = \\sum_{i=1}^{r} \\alpha_{i}^{n} M_{r} v_{i}\n$$\n$$\n\\sum_{i=1}^{r} \\alpha_{i}^{n+1} (M_{r}v_{i} + \\Delta t K_{r} v_{i}) = \\sum_{i=1}^{r} \\alpha_{i}^{n} M_{r} v_{i}\n$$\nNow, substitute the generalized eigenvalue relation $K_{r} v_{i} = \\lambda_{i} M_{r} v_{i}$ into the left side:\n$$\n\\sum_{i=1}^{r} \\alpha_{i}^{n+1} (M_{r}v_{i} + \\Delta t \\lambda_{i} M_{r} v_{i}) = \\sum_{i=1}^{r} \\alpha_{i}^{n} M_{r} v_{i}\n$$\nFactor out $M_{r}v_{i}$ on the left side:\n$$\n\\sum_{i=1}^{r} \\alpha_{i}^{n+1} (1 + \\Delta t \\lambda_{i}) M_{r} v_{i} = \\sum_{i=1}^{r} \\alpha_{i}^{n} M_{r} v_{i}\n$$\nThis equation holds for each mode independently. To isolate the update for a single modal coefficient $\\alpha_{j}^{n+1}$, we leverage the property that the generalized eigenvectors are $M_{r}$-orthogonal, i.e., $v_{j}^{T} M_{r} v_{i} = 0$ for $i \\neq j$. Pre-multiplying the equation by $v_{j}^{T}$:\n$$\nv_{j}^{T} \\sum_{i=1}^{r} \\left[ \\alpha_{i}^{n+1} (1 + \\Delta t \\lambda_{i}) - \\alpha_{i}^{n} \\right] M_{r} v_{i} = 0\n$$\n$$\n\\sum_{i=1}^{r} \\left[ \\alpha_{i}^{n+1} (1 + \\Delta t \\lambda_{i}) - \\alpha_{i}^{n} \\right] (v_{j}^{T} M_{r} v_{i}) = 0\n$$\nDue to $M_{r}$-orthogonality, all terms in the sum vanish except for $i=j$:\n$$\n\\left[ \\alpha_{j}^{n+1} (1 + \\Delta t \\lambda_{j}) - \\alpha_{j}^{n} \\right] (v_{j}^{T} M_{r} v_{j}) = 0\n$$\nSince $M_{r}$ is SPD and $v_{j}$ is a non-zero eigenvector, the scalar $v_{j}^{T} M_{r} v_{j} > 0$. Thus, the term in the brackets must be zero:\n$$\n\\alpha_{j}^{n+1} (1 + \\Delta t \\lambda_{j}) - \\alpha_{j}^{n} = 0\n$$\nSolving for $\\alpha_{j}^{n+1}$:\n$$\n\\alpha_{j}^{n+1} = \\frac{1}{1 + \\Delta t \\lambda_{j}} \\alpha_{j}^{n}\n$$\nThe problem defines the amplification factor $g(\\lambda_{i}, \\Delta t)$ such that $\\alpha_{i}^{n+1} = g(\\lambda_{i}, \\Delta t) \\alpha_{i}^{n}$. By comparing our derived relation with this definition, we can identify the amplification factor. Dropping the index subscript for generality, we have:\n$$\ng(\\lambda, \\Delta t) = \\frac{1}{1 + \\Delta t \\lambda}\n$$\nThis is the second required expression. Since $\\lambda \\ge 0$ and $\\Delta t > 0$, we have $0 < g(\\lambda, \\Delta t) \\le 1$, which confirms the unconditional stability of the backward Euler method for this class of problems.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} (M_{r} + \\Delta t K_{r})^{-1} (M_{r} x_{r}^{n} + \\Delta t f_{r}^{n+1}) & \\frac{1}{1 + \\Delta t \\lambda} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "For models that depend on variable parameters—such as manufacturing tolerances or operating temperatures—we need a basis that performs well across the entire parameter space. This is where the Reduced Basis Method (RBM) and its \"Greedy\" algorithm come into play. This exercise  outlines the logic behind this powerful technique, where an efficient error estimator is used to \"greedily\" find the parameter for which the current reduced model is least accurate. By understanding this workflow, you will see how to systematically and automatically build a compact, highly accurate basis for complex, parameterized engineering problems.",
            "id": "3915410",
            "problem": "Consider a parameterized, coercive linear variational model representative of lithium intercalation and ionic conduction in a battery electrode. Let $\\mathcal{P} \\subset \\mathbb{R}^p$ be a compact parameter domain encoding manufacturing and operating variability (for example, solid-phase diffusion coefficients and effective conductivities), and let $X$ be a finite-dimensional Hilbert space resulting from a convergent spatial discretization of the governing partial differential equations. For each parameter $\\mu \\in \\mathcal{P}$, the high-fidelity solution $u(\\mu) \\in X$ satisfies the variational problem: find $u(\\mu) \\in X$ such that\n$$\na\\big(u(\\mu), v; \\mu\\big) = \\ell\\big(v; \\mu\\big) \\quad \\text{for all } v \\in X,\n$$\nwhere $a(\\cdot,\\cdot;\\mu)$ is a continuous, coercive bilinear form with coercivity constant $\\alpha(\\mu) \\ge \\alpha_{\\mathrm{lb}}(\\mu) > 0$, and $\\ell(\\cdot;\\mu)$ is a continuous linear functional. Assume an affine parameter dependence such that $a(\\cdot,\\cdot;\\mu)$ and $\\ell(\\cdot;\\mu)$ admit accurate affine expansions over $\\mu$.\n\nFor reduced basis model reduction, let $X_r = \\mathrm{span}\\{\\phi_1,\\dots,\\phi_r\\} \\subset X$ be the reduced trial space. The Galerkin reduced solution $u_r(\\mu) \\in X_r$ is defined by\n$$\na\\big(u_r(\\mu), v_r; \\mu\\big) = \\ell\\big(v_r; \\mu\\big) \\quad \\text{for all } v_r \\in X_r.\n$$\nDefine the residual functional $r(\\cdot;\\mu) \\in X'$ associated with $u_r(\\mu)$ by $r\\big(v;\\mu\\big) := \\ell\\big(v;\\mu\\big) - a\\big(u_r(\\mu), v; \\mu\\big)$ for all $v \\in X$. Suppose one has available a residual-based a posteriori error estimator that upper-bounds the energy-norm error using a coercivity lower bound, and that this estimator can be evaluated efficiently for many parameters via an offline-online decomposition leveraging the affine expansions.\n\nYou are given a finite training set $\\Theta_{\\mathrm{train}} \\subset \\mathcal{P}$. The goal is to construct a basis enrichment strategy that, at each iteration, selects a new parameter sample for which the current reduced basis performs worst over $\\Theta_{\\mathrm{train}}$, computes the corresponding high-fidelity snapshot, and augments the basis, until a prescribed accuracy target is reached or a budget on the reduced dimension is exhausted.\n\nWhich of the following outlines correctly describes a Greedy algorithm that selects new parameter samples for basis enrichment based on the maximal residual-based error estimator over the training set, while maintaining offline-online efficiency?\n\nA. Initialize $X_r$ with one snapshot at an arbitrary $\\mu_0 \\in \\Theta_{\\mathrm{train}}$. Offline, precompute reduced operators and structures needed to evaluate the residual-based estimator quickly for any $\\mu$. Iterate: for each $\\mu \\in \\Theta_{\\mathrm{train}}$, compute $u_r(\\mu)$ by solving the reduced Galerkin problem and evaluate the residual-based estimator; select $\\mu^\\star$ as the parameter that maximizes the estimator over $\\Theta_{\\mathrm{train}}$; compute the high-fidelity solution $u(\\mu^\\star)$ and orthonormalize it against the current basis to enrich $X_r$; terminate when the maximal estimator over $\\Theta_{\\mathrm{train}}$ is below a tolerance or when $r$ reaches a budget.\n\nB. Initialize $X_r$ with one snapshot at an arbitrary $\\mu_0 \\in \\Theta_{\\mathrm{train}}$. At each iteration, randomly pick a subset of $\\Theta_{\\mathrm{train}}$, compute $u_r(\\mu)$ for these parameters, and select the parameter $\\mu^\\star$ that minimizes the residual-based estimator; enrich $X_r$ with $u_r(\\mu^\\star)$; stop when the average estimator over the subset falls below a tolerance.\n\nC. Initialize $X_r$ with one snapshot at an arbitrary $\\mu_0 \\in \\Theta_{\\mathrm{train}}$. At each iteration, for every $\\mu \\in \\Theta_{\\mathrm{train}}$ compute the high-fidelity solution $u(\\mu)$ to evaluate an error proxy; select the parameter with minimal coercivity lower bound to stress stability; enrich $X_r$ with the reduced solution $u_r(\\mu)$ at that parameter; stop when the smallest coercivity lower bound exceeds a threshold.\n\nD. Initialize $X_r$ with one snapshot at an arbitrary $\\mu_0 \\in \\Theta_{\\mathrm{train}}$. Offline, precompute reduced operators. Iterate: for each $\\mu \\in \\Theta_{\\mathrm{train}}$, compute $u_r(\\mu)$ and evaluate the norm of the reduced solution; select $\\mu^\\star$ as the parameter with the largest reduced solution norm; compute $u_r(\\mu^\\star)$ and add it to $X_r$ without orthonormalization; terminate when the maximal reduced solution norm over $\\Theta_{\\mathrm{train}}$ stops increasing.",
            "solution": "### Step 1: Extract Givens\n\nThe problem statement provides the following information:\n- A parameterized, coercive linear variational model representative of a battery electrode.\n- A compact parameter domain $\\mathcal{P} \\subset \\mathbb{R}^p$.\n- A finite-dimensional Hilbert space $X$ obtained from spatial discretization.\n- For each parameter $\\mu \\in \\mathcal{P}$, a high-fidelity solution $u(\\mu) \\in X$ satisfying the variational problem: $a(u(\\mu), v; \\mu) = \\ell(v; \\mu)$ for all $v \\in X$.\n- The bilinear form $a(\\cdot,\\cdot;\\mu)$ is continuous and coercive, with a coercivity constant $\\alpha(\\mu) \\ge \\alpha_{\\mathrm{lb}}(\\mu) > 0$.\n- The linear functional $\\ell(\\cdot;\\mu)$ is continuous.\n- Both $a(\\cdot,\\cdot;\\mu)$ and $\\ell(\\cdot;\\mu)$ admit accurate affine expansions with respect to the parameter $\\mu$.\n- A reduced trial space $X_r = \\mathrm{span}\\{\\phi_1,\\dots,\\phi_r\\} \\subset X$ is used for a reduced basis method.\n- The Galerkin reduced solution $u_r(\\mu) \\in X_r$ is defined by: $a(u_r(\\mu), v_r; \\mu) = \\ell(v_r; \\mu)$ for all $v_r \\in X_r$.\n- A residual functional is defined for a given reduced solution $u_r(\\mu)$ as $r(v;\\mu) := \\ell(v;\\mu) - a(u_r(\\mu), v; \\mu)$ for all $v \\in X$.\n- A residual-based a posteriori error estimator is available, which upper-bounds the energy-norm error. This estimator uses the coercivity lower bound $\\alpha_{\\mathrm{lb}}(\\mu)$ and can be evaluated efficiently due to an offline-online decomposition enabled by the affine expansions.\n- A finite training set $\\Theta_{\\mathrm{train}} \\subset \\mathcal{P}$ is provided.\n- The objective is to construct a basis enrichment strategy by iteratively selecting the parameter from $\\Theta_{\\mathrm{train}}$ for which the current reduced basis performs worst, computing the corresponding high-fidelity snapshot, and augmenting the basis. The process continues until a specified accuracy target or basis size budget is met.\n- The question asks for the correct description of a Greedy algorithm that accomplishes this while maintaining offline-online efficiency.\n\n### Derivation of the Correct Algorithm\n\nThe standard Greedy algorithm for reduced basis generation aims to build an accurate basis by iteratively correcting for the largest error. The key components are:\n1.  **Error Indication:** Since computing the true error $\\|u(\\mu) - u_r(\\mu)\\|$ is expensive (it requires $u(\\mu)$), a cheap and reliable *a posteriori* error estimator, $\\Delta_r(\\mu)$, is used as a proxy. The problem states such an estimator is available and can be computed efficiently (the \"online\" phase).\n2.  **Greedy Selection:** At each iteration $k$, with a basis of size $r$, the algorithm searches over the entire training set $\\Theta_{\\mathrm{train}}$ to find the parameter $\\mu^\\star$ that yields the maximum estimated error.\n    $$\n    \\mu^\\star = \\arg\\max_{\\mu \\in \\Theta_{\\mathrm{train}}} \\Delta_r(\\mu)\n    $$\n    This \"greedy\" choice identifies where the current reduced basis is performing the worst.\n3.  **Basis Enrichment:** The high-fidelity solution (\"snapshot\") $u(\\mu^\\star)$ is computed for this worst-case parameter. This snapshot contains the information most lacking in the current basis. The basis is then augmented with this new information. To ensure numerical stability and linear independence, the snapshot is typically orthonormalized (e.g., via Gram-Schmidt) against the existing basis vectors to form the next basis vector, $\\phi_{r+1}$. The new space becomes $X_{r+1} = \\mathrm{span}\\{X_r, \\phi_{r+1}\\}$.\n4.  **Offline-Online Strategy:** To make the greedy search efficient, computations that depend only on the basis functions are performed once \"offline\" after each enrichment. This includes computing the components of the reduced operators and the error estimator that arise from the affine expansion. The \"online\" stage then involves, for any given $\\mu$, rapidly assembling and solving the small $r \\times r$ reduced system and evaluating the error estimator.\n5.  **Termination:** The process is repeated until the maximum estimated error over the training set, $\\max_{\\mu \\in \\Theta_{\\mathrm{train}}} \\Delta_r(\\mu)$, falls below a prescribed tolerance $\\epsilon_{\\mathrm{tol}}$, or a maximum basis size $r_{\\mathrm{max}}$ is reached.\n\n### Option-by-Option Analysis\n\n**A. Initialize $X_r$ with one snapshot at an arbitrary $\\mu_0 \\in \\Theta_{\\mathrm{train}}$. Offline, precompute reduced operators and structures needed to evaluate the residual-based estimator quickly for any $\\mu$. Iterate: for each $\\mu \\in \\Theta_{\\mathrm{train}}$, compute $u_r(\\mu)$ by solving the reduced Galerkin problem and evaluate the residual-based estimator; select $\\mu^\\star$ as the parameter that maximizes the estimator over $\\Theta_{\\mathrm{train}}$; compute the high-fidelity solution $u(\\mu^\\star)$ and orthonormalize it against the current basis to enrich $X_r$; terminate when the maximal estimator over $\\Theta_{\\mathrm{train}}$ is below a tolerance or when $r$ reaches a budget.**\n\nThis option correctly describes all the key steps of the standard Greedy algorithm:\n-   **Initialization:** Correct.\n-   **Offline-Online Decomposition:** Correctly described for efficient evaluation of the estimator.\n-   **Greedy Selection:** Correctly states that for each parameter in the training set, the reduced solution and estimator are found, and the parameter maximizing the estimator is selected.\n-   **Enrichment:** Correctly states that the *high-fidelity solution* $u(\\mu^\\star)$ is computed and then used to enrich the basis, with orthonormalization for stability.\n-   **Termination:** Correctly identifies the standard stopping criteria based on accuracy tolerance or computational budget.\n\n**Verdict:** **Correct**.\n\n**B. Initialize $X_r$ with one snapshot at an arbitrary $\\mu_0 \\in \\Theta_{\\mathrm{train}}$. At each iteration, randomly pick a subset of $\\Theta_{\\mathrm{train}}$, compute $u_r(\\mu)$ for these parameters, and select the parameter $\\mu^\\star$ that minimizes the residual-based estimator; enrich $X_r$ with $u_r(\\mu^\\star)$; stop when the average estimator over the subset falls below a tolerance.**\n\nThis option has several fundamental flaws:\n-   It selects the parameter that *minimizes* the estimator. The goal is to find the *largest* error to correct it.\n-   It enriches the basis with the *reduced solution* $u_r(\\mu^\\star)$. Since $u_r(\\mu^\\star)$ is already in the span of the current basis $X_r$, adding it to the basis does not enrich the space or increase its dimension.\n-   It uses a randomized subset, which is a feature of some variants, but the subsequent steps are incorrect.\n-   The stopping criterion is based on an average over a subset, which is not the standard criterion for ensuring certified accuracy over the whole parameter domain.\n\n**Verdict:** **Incorrect**.\n\n**C. Initialize $X_r$ with one snapshot at an arbitrary $\\mu_0 \\in \\Theta_{\\mathrm{train}}$. At each iteration, for every $\\mu \\in \\Theta_{\\mathrm{train}}$ compute the high-fidelity solution $u(\\mu)$ to evaluate an error proxy; select the parameter with minimal coercivity lower bound to stress stability; enrich $X_r$ with the reduced solution $u_r(\\mu)$ at that parameter; stop when the smallest coercivity lower bound exceeds a threshold.**\n\nThis option is flawed for multiple reasons:\n-   It suggests computing the high-fidelity solution $u(\\mu)$ for *every* parameter in the training set at each iteration. This would be computationally prohibitive and defeats the purpose of model reduction, which is to *avoid* these expensive solves. The error estimator is specifically designed to be cheap.\n-   The selection criterion is to minimize the coercivity lower bound, which is not the greedy criterion. The greedy criterion is to maximize the error estimator, of which the coercivity is only one component.\n-   It enriches with the reduced solution $u_r(\\mu)$, which, as noted before, does not expand the basis space.\n\n**Verdict:** **Incorrect**.\n\n**D. Initialize $X_r$ with one snapshot at an arbitrary $\\mu_0 \\in \\Theta_{\\mathrm{train}}$. Offline, precompute reduced operators. Iterate: for each $\\mu \\in \\Theta_{\\mathrm{train}}$, compute $u_r(\\mu)$ and evaluate the norm of the reduced solution; select $\\mu^\\star$ as the parameter with the largest reduced solution norm; compute $u_r(\\mu^\\star)$ and add it to $X_r$ without orthonormalization; terminate when the maximal reduced solution norm over \\Theta_{\\mathrm{train}}$ stops increasing.**\n\nThis option is entirely incorrect:\n-   It uses the norm of the reduced solution, $\\|u_r(\\mu)\\|$, as the error proxy. This is not a reliable error indicator; a large solution norm does not imply a large error. The problem explicitly provides a residual-based estimator for this purpose.\n-   It enriches with the reduced solution $u_r(\\mu^\\star)$, which is a fatal flaw.\n-   It explicitly advocates *against* orthonormalization, which is poor numerical practice and leads to ill-conditioned systems.\n-   The stopping criterion is arbitrary and not related to the actual approximation error.\n\n**Verdict:** **Incorrect**.",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}