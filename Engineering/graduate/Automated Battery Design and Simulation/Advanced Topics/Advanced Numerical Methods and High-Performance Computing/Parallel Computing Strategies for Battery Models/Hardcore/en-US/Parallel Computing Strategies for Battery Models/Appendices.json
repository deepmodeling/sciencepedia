{
    "hands_on_practices": [
        {
            "introduction": "Before diving into complex parallel implementations, it is essential to understand the fundamental limits of performance scaling. Amdahl's Law provides a crucial theoretical framework for estimating the maximum speedup achievable by parallelizing a task. This exercise  challenges you to apply this law to a typical battery simulation workflow, identifying the serial bottlenecks that ultimately cap performance, no matter how many processors are used.",
            "id": "3936184",
            "problem": "Consider a strong-scaling study of a single outer iteration in an automated battery design loop that solves the porous-electrode electrochemical-thermal model using a fully implicit Newton–Krylov method on a distributed-memory cluster with Message Passing Interface (MPI). On a single processing element, the measured wall-clock breakdown for the dominant simulation phase is as follows (one end-to-end solve per design iteration):\n\n- Residual and Jacobian assembly over the mesh (parallelizable across spatial subdomains): $54.0$.\n- Local linear algebra kernels within Krylov iterations, including sparse matrix–vector operations and preconditioner application (parallelizable across subdomains): $30.0$.\n- Global convergence checks and line-search decision logic (treated as inherently serial for this strong-scaling limit because they require global reductions and coordination): $3.0$.\n- Time-step adaptivity controller decisions (serial): $1.5$.\n- Checkpoint input/output for restart files (serial): $1.5$.\n- Preconditioner setup and symbolic factorization constrained to a single rank due to thread-unsafe data structures (serial): $4.0$.\n- Miscellaneous serial overhead (initialization, parameter parsing, and logging): $1.0$.\n\nAssume ideal load balance for the parallelizable portions, no additional communication costs beyond those already accounted for in the listed serial items, and a fixed problem size (strong scaling). Let $T(1)$ denote the total time on one processing element, $T_{\\mathrm{s}}$ the total time spent in components that do not benefit from parallelization under the above assumptions, and $T_{\\mathrm{p}}$ the time spent in components that can be perfectly parallelized. Define the speedup on $N$ processing elements by $S(N) = T(1)/T(N)$.\n\nStarting from the definitions of $S(N)$, $T_{\\mathrm{s}}$, and $T_{\\mathrm{p}}$, and using only the ideal strong-scaling model $T(N) = T_{\\mathrm{s}} + T_{\\mathrm{p}}/N$, derive an expression for $S(N)$ in terms of the parallel fraction $P = T_{\\mathrm{p}}/T(1)$, and then compute the tight upper bound of $S(N)$ as $N$ grows without bound. Express your final bound as an exact rational number. Also, briefly justify which listed components dominate the runtime as $N$ becomes large under the given model. The final reported answer must be the single bound value (dimensionless).",
            "solution": "The problem is first validated against the specified criteria.\n\n**Step 1: Extracted Givens**\n- **Model:** Fully implicit Newton–Krylov method for a porous-electrode electrochemical-thermal model.\n- **Scaling Type:** Strong scaling (fixed problem size).\n- **Environment:** Distributed-memory cluster with MPI.\n- **Timing Data (on $1$ processing element):**\n  - Parallelizable components:\n    - Residual and Jacobian assembly: $54.0$.\n    - Local linear algebra kernels: $30.0$.\n  - Serial components:\n    - Global convergence checks and line-search: $3.0$.\n    - Time-step adaptivity controller: $1.5$.\n    - Checkpoint input/output: $1.5$.\n    - Preconditioner setup and symbolic factorization: $4.0$.\n    - Miscellaneous serial overhead: $1.0$.\n- **Assumptions:**\n  - Ideal load balance for parallelizable portions.\n  - No additional communication costs beyond those included in the serial components.\n  - Fixed problem size.\n- **Definitions:**\n  - $T(1)$: Total time on one processing element.\n  - $T(N)$: Total time on $N$ processing elements.\n  - $T_{\\mathrm{s}}$: Total time of serial components.\n  - $T_{\\mathrm{p}}$: Total time of parallelizable components.\n  - $S(N) = T(1)/T(N)$: Speedup on $N$ processing elements.\n  - $P = T_{\\mathrm{p}}/T(1)$: Parallel fraction.\n  - **Governing Model (Amdahl's Law):** $T(N) = T_{\\mathrm{s}} + T_{\\mathrm{p}}/N$.\n\n**Step 2: Validation Using Extracted Givens**\nThe problem is scientifically grounded, employing the well-established Amdahl's Law to model the performance of a parallel algorithm, a standard practice in computational science and engineering. The context of a battery simulation is realistic, and the breakdown of computational tasks into serial and parallelizable components is typical for such codes. The problem is well-posed, providing all necessary data and definitions to arrive at a unique, meaningful solution. It is objective and free of ambiguity. The problem is therefore deemed **valid**.\n\n**Solution Derivation**\nThe task is to derive an expression for speedup $S(N)$ and compute its upper bound.\n\nFirst, we calculate the total time for the serial and parallelizable components based on the provided data for a single processing element.\n\nThe total time for the parallelizable components, $T_{\\mathrm{p}}$, is the sum of the times for the residual/Jacobian assembly and the local linear algebra kernels:\n$$T_{\\mathrm{p}} = 54.0 + 30.0 = 84.0$$\n\nThe total time for the serial components, $T_{\\mathrm{s}}$, is the sum of the times for all components explicitly stated as serial:\n$$T_{\\mathrm{s}} = 3.0 + 1.5 + 1.5 + 4.0 + 1.0 = 11.0$$\n\nThe total time on a single processing element, $T(1)$, is the sum of the serial and parallelizable portions:\n$$T(1) = T_{\\mathrm{p}} + T_{\\mathrm{s}} = 84.0 + 11.0 = 95.0$$\n\nThe speedup on $N$ processing elements, $S(N)$, is defined as $S(N) = T(1)/T(N)$. Using the given ideal strong-scaling model, $T(N) = T_{\\mathrm{s}} + T_{\\mathrm{p}}/N$, we can write the speedup as:\n$$S(N) = \\frac{T(1)}{T_{\\mathrm{s}} + \\frac{T_{\\mathrm{p}}}{N}}$$\n\nTo express this in terms of the parallel fraction $P = T_{\\mathrm{p}}/T(1)$, we first recognize that the serial fraction is $1-P$.\n$1 - P = 1 - \\frac{T_{\\mathrm{p}}}{T(1)} = \\frac{T(1) - T_{\\mathrm{p}}}{T(1)} = \\frac{T_{\\mathrm{s}}}{T(1)}$.\nFrom these definitions, we can express $T_{\\mathrm{p}}$ and $T_{\\mathrm{s}}$ in terms of $T(1)$ and $P$:\n$$T_{\\mathrm{p}} = P \\cdot T(1)$$\n$$T_{\\mathrm{s}} = (1-P) \\cdot T(1)$$\n\nSubstituting these into the expression for $S(N)$:\n$$S(N) = \\frac{T(1)}{(1-P) \\cdot T(1) + \\frac{P \\cdot T(1)}{N}}$$\nDividing the numerator and the denominator by $T(1)$ yields the desired expression for $S(N)$ in terms of $P$:\n$$S(N) = \\frac{1}{(1-P) + \\frac{P}{N}}$$\nThis is the standard formulation of Amdahl's Law.\n\nTo find the tight upper bound of $S(N)$ as the number of processing elements $N$ grows without bound, we compute the limit of $S(N)$ as $N \\to \\infty$:\n$$S_{\\max} = \\lim_{N \\to \\infty} S(N) = \\lim_{N \\to \\infty} \\frac{1}{(1-P) + \\frac{P}{N}}$$\nAs $N \\to \\infty$, the term $P/N$ approaches $0$. Therefore, the limit is:\n$$S_{\\max} = \\frac{1}{1-P}$$\n\nThis shows that the maximum theoretical speedup is inversely proportional to the serial fraction of the code. We can also express this directly in terms of $T_{\\mathrm{s}}$ and $T(1)$:\n$$S_{\\max} = \\frac{1}{T_{\\mathrm{s}} / T(1)} = \\frac{T(1)}{T_{\\mathrm{s}}}$$\n\nUsing the calculated values for $T(1)$ and $T_{\\mathrm{s}}$:\n$$S_{\\max} = \\frac{95.0}{11.0} = \\frac{95}{11}$$\n\nAs $N$ becomes large, the time for the parallelizable portion, $T_{\\mathrm{p}}/N$, diminishes and approaches zero. The total wall-clock time $T(N) = T_{\\mathrm{s}} + T_{\\mathrm{p}}/N$ approaches $T_{\\mathrm{s}}$. Consequently, the execution time becomes dominated by the serial components, which constitute a performance bottleneck that cannot be reduced by adding more processing elements. In this problem, the serial components are the global convergence checks, time-step adaptivity, I/O, preconditioner setup, and other miscellaneous overhead. The single largest contributor to this serial bottleneck, with a time of $4.0$, is the \"Preconditioner setup and symbolic factorization constrained to a single rank\".",
            "answer": "$$\\boxed{\\frac{95}{11}}$$"
        },
        {
            "introduction": "While Amdahl's Law sets a theoretical ceiling, practical performance is governed by a more detailed balance of computation and communication. This exercise  moves from abstract principles to a concrete performance model for a multi-GPU diffusion solver, a common component in battery simulations. You will dissect the per-step runtime into computation, communication (including latency and bandwidth), and synchronization, learning how to quantify strong scaling efficiency in a realistic high-performance computing environment.",
            "id": "3936108",
            "problem": "Consider a multi-Graphics Processing Unit (GPU) electrolyte diffusion solver for a three-dimensional domain, implemented using a uniform explicit finite-volume discretization with a $7$-point stencil for Fickian diffusion. The domain has dimensions $N_x = 2048$, $N_y = 2048$, and $N_z = 1024$, and the solver advances for $N_t = 1000$ time steps. Each cell update performs $f = 20$ floating-point operations and stores one double-precision scalar of $8$ bytes. The baseline single-GPU implementation executes the entire domain on one GPU with sustained arithmetic throughput $R = 5.0 \\times 10^{12}$ floating-point operations per second.\n\nA multi-GPU implementation employs $G = 4$ GPUs with slab domain decomposition along the $z$-direction. The GPUs are connected in a ring topology by NVIDIA NVLink high-speed interconnect (NVLink) with sustained point-to-point bandwidth $B = 200 \\times 10^{9}$ bytes per second and per-message startup latency $L = 2.0 \\times 10^{-6}$ seconds. At each time step, each interior GPU exchanges two halo faces of thickness one cell (one with the neighbor above and one with the neighbor below). A halo face has size $N_x \\times N_y$ cells. Per face, packing and unpacking costs a fixed time $T_{\\mathrm{pack}} = 5.0 \\times 10^{-5}$ seconds, independent of message size. The communication/computation overlap fraction is $f_{\\mathrm{ov}} = 0.6$, meaning that a fraction $f_{\\mathrm{ov}}$ of the total communication time is fully overlapped by computation, while the remaining fraction is not overlapped and adds to the step time. A global synchronization barrier at the end of each time step costs $T_{\\mathrm{sync}} = 2.0 \\times 10^{-5}$ seconds.\n\nAssume perfect load balance and identical sustained arithmetic throughput $R$ on each GPU. For the purpose of determining the critical path per time step in the multi-GPU implementation, use the timing of an interior GPU (the GPUs at the ends communicate with only one neighbor and are not rate-limiting). Starting from first principles (definitions of runtime, speedup, and strong scaling) and using the given physical parameters and interconnect characteristics, derive the strong scaling efficiency of the multi-GPU implementation relative to the single-GPU baseline. Express the final efficiency as a decimal. Round your answer to $4$ significant figures.",
            "solution": "The problem is assessed to be valid as it is scientifically grounded in the principles of high-performance computing, well-posed with all necessary parameters provided, and articulated using objective, formal language. It presents a standard problem in parallel performance modeling.\n\nThe objective is to derive the strong scaling efficiency, $E_G$, of a multi-GPU implementation with $G$ GPUs relative to a single-GPU baseline. Strong scaling efficiency is defined as:\n$$E_G = \\frac{S_G}{G}$$\nwhere $S_G$ is the speedup on $G$ GPUs, defined as the ratio of the single-GPU execution time $T_1$ to the multi-GPU execution time $T_G$:\n$$S_G = \\frac{T_1}{T_G}$$\nCombining these, the efficiency is:\n$$E_G = \\frac{T_1}{G \\cdot T_G}$$\nSince the number of time steps $N_t$ is the same for both cases, we can analyze the performance based on the time required to complete a single time step. Let $T_{step,1}$ be the time per step for the single-GPU case and $T_{step,G}$ be the time per step for the multi-GPU case. The efficiency can then be written as:\n$$E_G = \\frac{N_t \\cdot T_{step,1}}{G \\cdot (N_t \\cdot T_{step,G})} = \\frac{T_{step,1}}{G \\cdot T_{step,G}}$$\n\nFirst, we determine the single-GPU time per step, $T_{step,1}$. This is purely computational work.\nThe total number of cells in the domain is $N_{cells} = N_x N_y N_z$.\nThe total number of floating-point operations (FLOPs) per time step is $W_{step} = N_{cells} \\cdot f$.\nThe time to perform these operations on a single GPU with sustained throughput $R$ is:\n$$T_{step,1} = \\frac{W_{step}}{R} = \\frac{N_x N_y N_z f}{R}$$\n\nNext, we model the time per step for the multi-GPU case, $T_{step,G}$. The problem states we should use the timing of an interior GPU as it represents the critical path. The time per step for an interior GPU is the sum of its computation time and any non-overlapped communication and synchronization overheads.\nThe per-step time is given by the model:\n$$T_{step,G} = T_{comp,G} + (1 - f_{\\mathrm{ov}}) T_{comm,G} + T_{\\mathrm{sync}}$$\nwhere $T_{comp,G}$ is the computation time per step on one of the $G$ GPUs, $T_{comm,G}$ is the total communication time per step for that GPU, $f_{\\mathrm{ov}}$ is the fraction of communication time overlapped with computation, and $T_{\\mathrm{sync}}$ is the final synchronization barrier cost.\n\nWith perfect load balancing, the computational workload is distributed evenly among the $G$ GPUs. The domain is decomposed along the $z$-direction, so each GPU handles a subdomain of size $N_x \\times N_y \\times (N_z/G)$.\nThe computation time per GPU is:\n$$T_{comp,G} = \\frac{N_x N_y (N_z/G) f}{R} = \\frac{N_x N_y N_z f}{G R} = \\frac{T_{step,1}}{G}$$\n\nNow, we model the communication time, $T_{comm,G}$. An interior GPU exchanges two halo faces, one with each of its two neighbors in the ring topology. The data for one face consists of $N_x \\times N_y$ cells, where each cell is a double-precision scalar of $s=8$ bytes.\nThe size of a single message (one halo face) is:\n$$M = N_x \\times N_y \\times s$$\nThe time to transfer one message is modeled as the sum of a fixed latency $L$ and a bandwidth-dependent term $M/B$. Additionally, there is a packing/unpacking overhead $T_{\\mathrm{pack}}$ per face exchange.\nThe time for a single halo exchange with one neighbor is $L + M/B + T_{\\mathrm{pack}}$.\nSince an interior GPU communicates with two neighbors, the total communication time per step is:\n$$T_{comm,G} = 2 \\left( L + \\frac{M}{B} + T_{\\mathrm{pack}} \\right) = 2 \\left( L + \\frac{N_x N_y s}{B} + T_{\\mathrm{pack}} \\right)$$\n\nSubstituting the expressions for $T_{step,1}$ and $T_{step,G}$ into the efficiency formula:\n$$E_G = \\frac{T_{step,1}}{G \\cdot T_{step,G}} = \\frac{G \\cdot T_{comp,G}}{G \\cdot T_{step,G}} = \\frac{T_{comp,G}}{T_{step,G}}$$\n$$E_G = \\frac{T_{comp,G}}{T_{comp,G} + (1 - f_{\\mathrm{ov}})T_{comm,G} + T_{\\mathrm{sync}}}$$\nThis formula represents the fraction of the total time that is spent on useful computation.\n\nNow, we substitute the given numerical values:\n- Domain: $N_x = 2048$, $N_y = 2048$, $N_z = 1024$\n- Operations: $f = 20$\n- GPUs: $G = 4$\n- Throughput: $R = 5.0 \\times 10^{12}$ FLOPS\n- Cell size: $s = 8$ bytes\n- Bandwidth: $B = 200 \\times 10^{9}$ bytes/s $= 2.0 \\times 10^{11}$ bytes/s\n- Latency: $L = 2.0 \\times 10^{-6}$ s\n- Packing time: $T_{\\mathrm{pack}} = 5.0 \\times 10^{-5}$ s\n- Overlap fraction: $f_{\\mathrm{ov}} = 0.6$\n- Sync cost: $T_{\\mathrm{sync}} = 2.0 \\times 10^{-5}$ s\n\nCalculate $T_{comp,G}$:\n$$T_{comp,G} = \\frac{2048 \\times 2048 \\times (1024/4) \\times 20}{5.0 \\times 10^{12}} = \\frac{2048 \\times 2048 \\times 256 \\times 20}{5.0 \\times 10^{12}}$$\n$$T_{comp,G} = \\frac{(2^{11}) \\times (2^{11}) \\times (2^8) \\times 20}{5.0 \\times 10^{12}} = \\frac{2^{30} \\times 20}{5.0 \\times 10^{12}} = \\frac{1073741824 \\times 20}{5.0 \\times 10^{12}}$$\n$$T_{comp,G} = \\frac{21474836480}{5.0 \\times 10^{12}} = 4.294967296 \\times 10^{-3} \\text{ s}$$\n\nCalculate $T_{comm,G}$:\nMessage size $M = 2048 \\times 2048 \\times 8 = 33554432$ bytes.\n$$T_{comm,G} = 2 \\left( 2.0 \\times 10^{-6} + \\frac{33554432}{2.0 \\times 10^{11}} + 5.0 \\times 10^{-5} \\right)$$\n$$T_{comm,G} = 2 \\left( 2.0 \\times 10^{-6} + 1.6777216 \\times 10^{-4} + 5.0 \\times 10^{-5} \\right)$$\n$$T_{comm,G} = 2 \\left( 0.02 \\times 10^{-4} + 1.6777216 \\times 10^{-4} + 0.5 \\times 10^{-4} \\right)$$\n$$T_{comm,G} = 2 \\left( 2.1977216 \\times 10^{-4} \\right) = 4.3954432 \\times 10^{-4} \\text{ s}$$\n\nCalculate the total overhead time added to the step, $T_{overhead}$:\n$$T_{overhead} = (1 - f_{\\mathrm{ov}})T_{comm,G} + T_{\\mathrm{sync}}$$\n$$T_{overhead} = (1 - 0.6) \\times (4.3954432 \\times 10^{-4}) + 2.0 \\times 10^{-5}$$\n$$T_{overhead} = 0.4 \\times 4.3954432 \\times 10^{-4} + 0.2 \\times 10^{-4}$$\n$$T_{overhead} = 1.75817728 \\times 10^{-4} + 0.2 \\times 10^{-4} = 1.95817728 \\times 10^{-4} \\text{ s}$$\n\nFinally, calculate the efficiency $E_G$:\n$$E_G = \\frac{T_{comp,G}}{T_{comp,G} + T_{overhead}}$$\n$$E_G = \\frac{4.294967296 \\times 10^{-3}}{4.294967296 \\times 10^{-3} + 1.95817728 \\times 10^{-4}}$$\n$$E_G = \\frac{42.94967296 \\times 10^{-4}}{42.94967296 \\times 10^{-4} + 1.95817728 \\times 10^{-4}}$$\n$$E_G = \\frac{42.94967296}{44.90785024} = 0.95639600...$$\n\nRounding the result to $4$ significant figures, we get:\n$$E_G \\approx 0.9564$$",
            "answer": "$$\\boxed{0.9564}$$"
        },
        {
            "introduction": "Effective parallel computing strategies involve trade-offs that extend beyond just runtime performance. This practice  explores the critical link between a parallel communication scheme and the numerical stability of the underlying simulation. By analyzing an explicit diffusion solver with delayed communication, you will derive how an attempt to optimize performance by reducing communication frequency introduces a new, more restrictive stability constraint on the simulation time step.",
            "id": "3936155",
            "problem": "Consider the electrolyte concentration $c_e(\\mathbf{x}, t)$ in a lithium-ion battery porous electrolyte, modeled at the cell scale by the diffusion equation $\\partial c_e/\\partial t = \\nabla \\cdot \\left(D_e \\nabla c_e\\right)$ with piecewise constant diffusion coefficient $D_e$ per subdomain. A standard explicit finite-difference update with an isotropic stencil in $d$ spatial dimensions uses $2d$ nearest neighbors on a uniform grid of spacing $h$. For sequential execution, stability of the explicit scheme is known to be constrained by the Courant–Friedrichs–Lewy (CFL) condition $\\Delta t \\leq h^2/(2 d D_e)$.\n\nNow suppose the computational domain is decomposed across processes using the Message Passing Interface (MPI) into $P$ nonoverlapping subdomains with uniform but potentially different grid spacings $\\{h_i\\}_{i=1}^{P}$ and subdomain-wise diffusion coefficients $\\{D_{e,i}\\}_{i=1}^{P}$. Each subdomain advances explicitly and maintains a ghost layer of width $w$ grid cells for stencil evaluation near interfaces. To reduce communication overhead, halo exchanges of ghost data occur every $m$ time steps rather than every step, so interface stencils may use neighbor values that are up to $m$ steps old.\n\nStarting from the diffusion equation and the explicit discretization, and taking as a well-tested fact the sequential CFL constraint, derive a conservative global stability bound for the maximum explicit time step $\\Delta t_{\\max}$ under this domain decomposition with delayed halo exchanges. Your derivation should clearly state the physical reasoning tying diffusion length over $m$ steps to the halo width $w$, and must account for heterogeneous $\\{h_i, D_{e,i}\\}$ across subdomains. Then, for the specific case with $d=3$, $P=4$, parameters\n- $h_1 = 6.0 \\times 10^{-6}$ m, $D_{e,1} = 2.2 \\times 10^{-10}\\ \\text{m}^2/\\text{s}$,\n- $h_2 = 5.0 \\times 10^{-6}$ m, $D_{e,2} = 2.5 \\times 10^{-10}\\ \\text{m}^2/\\text{s}$,\n- $h_3 = 4.0 \\times 10^{-6}$ m, $D_{e,3} = 1.8 \\times 10^{-10}\\ \\text{m}^2/\\text{s}$,\n- $h_4 = 5.0 \\times 10^{-6}$ m, $D_{e,4} = 2.4 \\times 10^{-10}\\ \\text{m}^2/\\text{s}$,\nand communication policy $w=1$ and $m=3$, compute the numerical value of $\\Delta t_{\\max}$ implied by your bound. Express the final time step in seconds and round your answer to four significant figures.",
            "solution": "The problem requires the derivation of a conservative global stability bound for the maximum explicit time step, $\\Delta t_{\\max}$, for a parallelized finite-difference simulation of a diffusion equation with delayed communication, followed by a numerical calculation for a specific case.\n\nThe governing equation is the diffusion equation for electrolyte concentration $c_e$:\n$$\n\\frac{\\partial c_e}{\\partial t} = \\nabla \\cdot (D_e \\nabla c_e)\n$$\nwhere the diffusion coefficient $D_e$ is piecewise constant over $P$ subdomains. The problem is discretized using an explicit finite-difference scheme on a grid with potentially heterogeneous spacing $h_i$ and diffusion coefficient $D_{e,i}$ in each subdomain $i$.\n\nThe stability of this parallel scheme is governed by two distinct constraints:\n1.  **Interior Stability**: The standard stability of the explicit scheme within the interior of each subdomain, far from the interfaces.\n2.  **Interface Stability**: A new constraint arising from the use of time-delayed ghost cell data at the interfaces between subdomains.\n\nA conservative global time step $\\Delta t$ must satisfy both constraints simultaneously across all subdomains.\n\n**1. Interior Stability Constraint**\n\nThe problem provides the well-tested sequential Courant–Friedrichs–Lewy (CFL) condition for an explicit finite-difference scheme for the diffusion equation in $d$ dimensions:\n$$\n\\Delta t \\leq \\frac{h^2}{2 d D_e}\n$$\nThis condition must hold within the interior of each subdomain $i \\in \\{1, \\dots, P\\}$. Since the subdomains are heterogeneous, each imposes its own limit on the time step:\n$$\n\\Delta t \\leq \\frac{h_i^2}{2 d D_{e,i}} \\quad \\text{for each } i=1, \\ldots, P\n$$\nTo ensure stability in all subdomains, the global time step $\\Delta t$ must be less than or equal to the minimum of these individual limits. This defines the interior stability bound, $\\Delta t_{\\text{interior}}$:\n$$\n\\Delta t_{\\text{interior}} = \\min_{i=1,\\dots,P} \\left( \\frac{h_i^2}{2 d D_{e,i}} \\right)\n$$\n\n**2. Interface Stability Constraint**\n\nAt the interface between subdomains, a process uses values from ghost cells to evaluate its finite-difference stencil. These ghost cells are populated with data from neighboring processes. The problem states that these halo exchanges occur every $m$ time steps. This means that for $m-1$ consecutive steps, the ghost cell data is \"stale\", i.e., it is from a previous time. The age of the information can be up to $m \\Delta t$.\n\nThe stability of an explicit scheme is fundamentally tied to the concept of the numerical domain of dependence. For a stable scheme, the numerical domain of dependence must contain the physical domain of dependence. The use of stale boundary data violates this principle if information from outside the known region propagates into the computational cell during the time the data is stale.\n\nThe physical reasoning for the CFL condition itself is that a diffusive signal should not propagate more than one grid cell $h$ in one time step $\\Delta t$. The characteristic diffusion length $L$ over a time $T$ is $L \\propto \\sqrt{D_e T}$. In $d$ dimensions on a grid, this is more accurately captured by $L^2 \\approx 2d D_e T$. Thus, for the standard CFL, we set $T=\\Delta t$ and require $L \\le h$, which gives $2d D_e \\Delta t \\le h^2$, recovering the given condition.\n\nWe apply the same physical reasoning to the interface. A process calculating a value at its boundary uses ghost cell data of width $w$ grid cells. This data is up to $m \\Delta t$ old. During this period, a diffusion front from the true, evolving solution in the neighboring subdomain propagates. For the calculation to be stable, this front must not propagate beyond the region covered by the ghost cells.\n\nLet's consider subdomain $i$. The physical width of its ghost layer is $W_i = w h_i$. The time over which data is stale is $T = m \\Delta t$. The diffusion occurs with coefficient $D_{e,i}$ (conservatively, we could consider the maximum of the two adjacent diffusion coefficients, but considering the local one provides a valid conservative bound for the subdomain-wise condition). The diffusion length $L_i$ in subdomain $i$ over this time must be less than or equal to the ghost layer width $W_i$:\n$$\nL_i \\le W_i\n$$\nUsing the characteristic diffusion length scaling:\n$$\n\\sqrt{2 d D_{e,i} (m \\Delta t)} \\le w h_i\n$$\nSquaring both sides yields the constraint on $\\Delta t$:\n$$\n2 d D_{e,i} m \\Delta t \\le (w h_i)^2\n$$\n$$\n\\Delta t \\le \\frac{w^2 h_i^2}{2 d m D_{e,i}}\n$$\nThis condition must hold for every subdomain $i$. Therefore, the interface stability bound, $\\Delta t_{\\text{interface}}$, is determined by the most restrictive case:\n$$\n\\Delta t_{\\text{interface}} = \\min_{i=1,\\dots,P} \\left( \\frac{w^2 h_i^2}{2 d m D_{e,i}} \\right)\n$$\n\n**3. Global Stability Bound**\n\nThe global maximum time step, $\\Delta t_{\\max}$, must satisfy both the interior and interface stability constraints. It is therefore the minimum of the two derived bounds:\n$$\n\\Delta t_{\\max} = \\min(\\Delta t_{\\text{interior}}, \\Delta t_{\\text{interface}})\n$$\nLet's define the sequential CFL time step for each subdomain as $\\Delta t_{\\text{CFL},i} = \\frac{h_i^2}{2 d D_{e,i}}$. The global bound can then be written as:\n$$\n\\Delta t_{\\max} = \\min \\left( \\min_{i=1,\\dots,P} \\left(\\Delta t_{\\text{CFL},i}\\right), \\min_{i=1,\\dots,P} \\left(\\frac{w^2}{m} \\Delta t_{\\text{CFL},i}\\right) \\right)\n$$\nThis can be simplified to:\n$$\n\\Delta t_{\\max} = \\min\\left(1, \\frac{w^2}{m}\\right) \\min_{i=1,\\dots,P} \\left( \\frac{h_i^2}{2 d D_{e,i}} \\right)\n$$\nThis is the derived conservative global stability bound.\n\n**4. Numerical Calculation**\n\nWe are given the following specific parameters:\n- Dimensions: $d=3$\n- Number of subdomains: $P=4$\n- Ghost layer width: $w=1$\n- Communication frequency: $m=3$\n- Subdomain properties:\n    - $h_1 = 6.0 \\times 10^{-6} \\, \\text{m}$, $D_{e,1} = 2.2 \\times 10^{-10} \\, \\text{m}^2/\\text{s}$\n    - $h_2 = 5.0 \\times 10^{-6} \\, \\text{m}$, $D_{e,2} = 2.5 \\times 10^{-10} \\, \\text{m}^2/\\text{s}$\n    - $h_3 = 4.0 \\times 10^{-6} \\, \\text{m}$, $D_{e,3} = 1.8 \\times 10^{-10} \\, \\text{m}^2/\\text{s}$\n    - $h_4 = 5.0 \\times 10^{-6} \\, \\text{m}$, $D_{e,4} = 2.4 \\times 10^{-10} \\, \\text{m}^2/\\text{s}$\n\nFirst, we evaluate the factor $\\min(1, \\frac{w^2}{m})$. With $w=1$ and $m=3$:\n$$\n\\min\\left(1, \\frac{1^2}{3}\\right) = \\min\\left(1, \\frac{1}{3}\\right) = \\frac{1}{3}\n$$\nNext, we calculate the term $\\frac{h_i^2}{2 d D_{e,i}} = \\frac{h_i^2}{6 D_{e,i}}$ for each subdomain.\n- For $i=1$: $\\frac{(6.0 \\times 10^{-6})^2}{6 \\times (2.2 \\times 10^{-10})} = \\frac{36 \\times 10^{-12}}{13.2 \\times 10^{-10}} \\approx 0.02727 \\, \\text{s}$\n- For $i=2$: $\\frac{(5.0 \\times 10^{-6})^2}{6 \\times (2.5 \\times 10^{-10})} = \\frac{25 \\times 10^{-12}}{15.0 \\times 10^{-10}} \\approx 0.01667 \\, \\text{s}$\n- For $i=3$: $\\frac{(4.0 \\times 10^{-6})^2}{6 \\times (1.8 \\times 10^{-10})} = \\frac{16 \\times 10^{-12}}{10.8 \\times 10^{-10}} \\approx 0.01481 \\, \\text{s}$\n- For $i=4$: $\\frac{(5.0 \\times 10^{-6})^2}{6 \\times (2.4 \\times 10^{-10})} = \\frac{25 \\times 10^{-12}}{14.4 \\times 10^{-10}} \\approx 0.01736 \\, \\text{s}$\n\nThe minimum of these values corresponds to subdomain $i=3$:\n$$\n\\min_{i=1,\\dots,4} \\left( \\frac{h_i^2}{6 D_{e,i}} \\right) = \\frac{(4.0 \\times 10^{-6})^2}{6 \\times (1.8 \\times 10^{-10})} = \\frac{16 \\times 10^{-12}}{10.8 \\times 10^{-10}} = \\frac{16}{1080} = \\frac{2}{135} \\, \\text{s}\n$$\nNow we can compute the final global time step bound $\\Delta t_{\\max}$:\n$$\n\\Delta t_{\\max} = \\frac{1}{3} \\times \\frac{2}{135} = \\frac{2}{405} \\, \\text{s}\n$$\nNumerically, this value is:\n$$\n\\Delta t_{\\max} = \\frac{2}{405} \\approx 0.0049382716... \\, \\text{s}\n$$\nRounding to four significant figures as required:\n$$\n\\Delta t_{\\max} \\approx 0.004938 \\, \\text{s}\n$$\nThis can be written in scientific notation as $4.938 \\times 10^{-3} \\, \\text{s}$.",
            "answer": "$$\\boxed{4.938 \\times 10^{-3}}$$"
        }
    ]
}