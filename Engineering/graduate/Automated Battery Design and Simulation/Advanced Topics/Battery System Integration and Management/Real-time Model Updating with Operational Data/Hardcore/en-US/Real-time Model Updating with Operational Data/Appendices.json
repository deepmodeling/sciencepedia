{
    "hands_on_practices": [
        {
            "introduction": "Real-time estimators for non-linear systems, such as the Extended Kalman Filter (EKF), rely on local linearization of the system dynamics at each time step. This practice provides a foundational exercise in deriving and implementing the Jacobian matrix, which is the mathematical representation of this linearization. By working through a physically-grounded thermal model of a battery, you will connect the principles of energy conservation to the calculus required for state estimation and appreciate the need for computationally efficient code in real-time applications .",
            "id": "3944867",
            "problem": "You are given a lumped one-dimensional thermal resistance-capacitance (RC) network model of a battery module consisting of $n$ nodes, where the state vector $x_k \\in \\mathbb{R}^n$ at discrete time step $k$ collects the node temperatures $x_k = [T_{1,k}, T_{2,k}, \\dots, T_{n,k}]^\\top$ in Kelvin. The operational inputs at step $k$ are the pack current $I_k$ in Ampere and the ambient temperature $T_{\\mathrm{amb},k}$ in Kelvin. Each node $i$ has a thermal capacitance $C_i$ in Joule per Kelvin, convective resistance to ambient $R_{\\mathrm{conv},i}$ in Kelvin per Watt, and is thermally connected to its immediate neighbors via conduction resistances $R_{\\mathrm{cond},i}$ in Kelvin per Watt for pairs $(i,i+1)$ for $i = 1, \\dots, n-1$. Heat generation at each node due to electrical losses is modeled as $q_i(I_k, T_{i,k}) = I_k^2 R_{\\mathrm{int},i}(T_{i,k})$, where the temperature-dependent internal resistance is $R_{\\mathrm{int},i}(T_{i,k}) = R_{0,i} \\exp(\\alpha_i (T_{i,k} - T_{\\mathrm{ref}}))$ with $R_{0,i}$ in Ohm, $\\alpha_i$ in inverse Kelvin, and reference temperature $T_{\\mathrm{ref}}$ in Kelvin.\n\nStart from the node-wise energy balance based on conservation of energy, Newton's law of cooling, and lumped thermal conduction: for each node $i$,\n$$\nC_i \\frac{d T_i}{d t} = \\sum_{j \\in \\mathcal{N}(i)} \\frac{T_j - T_i}{R_{i,j}} + \\frac{T_{\\mathrm{amb}} - T_i}{R_{\\mathrm{conv},i}} + I^2 R_{0,i}\\,\\exp\\!\\big(\\alpha_i (T_i - T_{\\mathrm{ref}})\\big),\n$$\nwhere $\\mathcal{N}(i)$ denotes the set of neighbors of node $i$ in the one-dimensional chain, $R_{i,j}$ are the symmetric conduction resistances with $R_{i,i+1} = R_{\\mathrm{cond},i}$ and $R_{i+1,i} = R_{\\mathrm{cond},i}$, and $R_{\\mathrm{conv},i}$ is the convective resistance to ambient for node $i$ (for interior nodes $R_{\\mathrm{conv},i}$ can be large to represent weak convection). Discretize this continuous-time model using the forward Euler method with time step $\\Delta t$ in seconds to obtain the discrete-time state transition function $F(x_k, u_k)$ with $u_k = (I_k, T_{\\mathrm{amb},k})$ and $x_{k+1} = F(x_k, u_k) = x_k + \\Delta t\\, f(x_k, u_k)$, where $f(x_k, u_k)$ is determined by the right-hand side divided by $C_i$ for each node.\n\nTask 1. Derive analytically the Jacobian matrix $A_k = \\frac{\\partial F}{\\partial x}\\big|_{(x_k,u_k)} \\in \\mathbb{R}^{n \\times n}$ for the above discrete-time model, expressing the entries $[A_k]_{i,j}$ in terms of the parameters $(C_i, R_{\\mathrm{cond},i}, R_{\\mathrm{conv},i}, R_{0,i}, \\alpha_i, T_{\\mathrm{ref}})$, the current state $x_k$, and the operational input $I_k$. Your derivation should start from the given energy balance and the forward Euler discretization, and proceed by taking partial derivatives component-wise without invoking any shortcut formulas.\n\nTask 2. Implement a complete, runnable program that assembles $A_k$ efficiently for real-time execution using only vectorized operations and sparse tridiagonal construction. Your implementation must avoid per-element Python loops and leverage the banded sparsity of the one-dimensional chain. For each provided test case, compute $A_k$ and return its diagonal entries $[A_k]_{i,i}$ for $i = 1,\\dots,n$.\n\nPhysical units: Temperatures must be in Kelvin, current in Ampere, resistances in Kelvin per Watt, capacitances in Joule per Kelvin, and time step in seconds. The Jacobian $A_k$ is dimensionless.\n\nAngle units are not applicable. If you need to report fractional quantities, express them as decimals.\n\nTest suite. Use the following four test cases, each specified by $(n, \\Delta t, C, R_{\\mathrm{cond}}, R_{\\mathrm{conv}}, R_0, \\alpha, T_{\\mathrm{ref}}, x_k, I_k, T_{\\mathrm{amb},k})$:\n\n- Case 1 (general happy path): $n = 4$, $\\Delta t = 0.1$, $C = [500.0, 520.0, 510.0, 505.0]$, $R_{\\mathrm{cond}} = [0.4, 0.45, 0.5]$, $R_{\\mathrm{conv}} = [2.5, 1.0\\times 10^9, 1.0\\times 10^9, 2.0]$, $R_0 = [2.0\\times 10^{-3}, 2.2\\times 10^{-3}, 2.1\\times 10^{-3}, 2.0\\times 10^{-3}]$, $\\alpha = [0.01, 0.012, 0.011, 0.01]$, $T_{\\mathrm{ref}} = 298.0$, $x_k = [300.0, 299.0, 301.0, 300.5]$, $I_k = 30.0$, $T_{\\mathrm{amb},k} = 298.0$.\n\n- Case 2 (zero-current edge case): Same as Case 1 except $I_k = 0.0$.\n\n- Case 3 (time-step edge case): Same as Case 1 except $\\Delta t = 1.0\\times 10^{-6}$.\n\n- Case 4 (nonlinearity stress): $n = 4$, $\\Delta t = 0.05$, $C = [480.0, 500.0, 495.0, 485.0]$, $R_{\\mathrm{cond}} = [0.35, 0.4, 0.38]$, $R_{\\mathrm{conv}} = [2.0, 1.0\\times 10^9, 1.0\\times 10^9, 1.8]$, $R_0 = [3.0\\times 10^{-3}, 2.8\\times 10^{-3}, 2.9\\times 10^{-3}, 3.1\\times 10^{-3}]$, $\\alpha = [0.02, 0.018, 0.019, 0.02]$, $T_{\\mathrm{ref}} = 298.0$, $x_k = [330.0, 328.0, 332.0, 331.0]$, $I_k = 100.0$, $T_{\\mathrm{amb},k} = 305.0$.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a list of the diagonal entries of $A_k$ in order $i = 1,\\dots,n$, rounded to six decimal places. For example: \"[[a11,a22,a33,a44],[...],[...],[...]]\".",
            "solution": "The user has requested the derivation and implementation of the Jacobian for a discrete-time battery thermal model.\n\n### Problem Validation\n\nI will first validate the problem statement according to the specified criteria.\n\n**Step 1: Extract Givens**\n\n*   **Model**: One-dimensional lumped thermal resistance-capacitance (RC) network with $n$ nodes.\n*   **State Vector**: $x_k = [T_{1,k}, T_{2,k}, \\dots, T_{n,k}]^\\top \\in \\mathbb{R}^n$, where $T_{i,k}$ is the temperature of node $i$ at time step $k$ in Kelvin.\n*   **Input Vector**: $u_k = (I_k, T_{\\mathrm{amb},k})$, where $I_k$ is the pack current in Ampere and $T_{\\mathrm{amb},k}$ is the ambient temperature in Kelvin.\n*   **Parameters for node $i$**:\n    *   Thermal capacitance: $C_i$ (J/K)\n    *   Convective resistance to ambient: $R_{\\mathrm{conv},i}$ (K/W)\n    *   Internal resistance at reference temp: $R_{0,i}$ (Ohm)\n    *   Temperature coefficient of resistance: $\\alpha_i$ (1/K)\n*   **Parameters for connections**:\n    *   Conduction resistance between nodes $i$ and $i+1$: $R_{\\mathrm{cond},i}$ (K/W) for $i=1, \\dots, n-1$.\n*   **Reference Temperature**: $T_{\\mathrm{ref}}$ (K).\n*   **Heat Generation Model**: $q_i(I_k, T_{i,k}) = I_k^2 R_{\\mathrm{int},i}(T_{i,k})$, where $R_{\\mathrm{int},i}(T_{i,k}) = R_{0,i} \\exp(\\alpha_i (T_{i,k} - T_{\\mathrm{ref}}))$.\n*   **Continuous-Time Energy Balance for node $i$**:\n    $$ C_i \\frac{d T_i}{d t} = \\sum_{j \\in \\mathcal{N}(i)} \\frac{T_j - T_i}{R_{i,j}} + \\frac{T_{\\mathrm{amb}} - T_i}{R_{\\mathrm{conv},i}} + I^2 R_{0,i}\\,\\exp\\!\\big(\\alpha_i (T_i - T_{\\mathrm{ref}})\\big) $$\n*   **Discretization**: Forward Euler with time step $\\Delta t$ (s), yielding $x_{k+1} = F(x_k, u_k) = x_k + \\Delta t\\, f(x_k, u_k)$.\n*   **Task 1**: Derive the Jacobian matrix $A_k = \\frac{\\partial F}{\\partial x}\\big|_{(x_k,u_k)}$.\n*   **Task 2**: Implement a program to compute the diagonal entries of $A_k$ for four given test cases.\n*   **Test Cases**: Four sets of numerical values for all parameters, state, and inputs are provided.\n*   **Output Format**: A single line string `[[d1_case1,d2_case1,...],[d1_case2,d2_case2,...],...]` where `d`'s are diagonal entries rounded to six decimal places.\n\n**Step 2: Validate Using Extracted Givens**\n\n1.  **Scientifically Grounded**: The model is based on established principles of heat transfer (conduction, convection) and electrical heating (Joule's law). The lumped-parameter RC network is a standard and physically valid simplification for thermal analysis of systems like battery packs. The temperature-dependent resistance model is a common empirical formulation. The problem is scientifically sound.\n2.  **Well-Posed**: The task is to derive and compute a Jacobian matrix from a given state-space model. This is a well-defined mathematical problem. The provided parameters and initial states in the test cases are sufficient to compute a unique result.\n3.  **Objective**: The problem is stated using precise mathematical and physical terminology. There is no subjective or ambiguous language.\n4.  **Incomplete or Contradictory Setup**: All necessary parameters, equations, and values for the test cases are provided. There are no contradictions. The use of a very large $R_{\\mathrm{conv},i}$ for interior nodes is a standard technique to model negligible convection, which is consistent.\n5.  **Unrealistic or Infeasible**: The parameter values are physically plausible for battery modules. The required computations are feasible.\n6.  **Ill-Posed or Poorly Structured**: The problem is clearly structured into an analytical derivation part and a computational implementation part. A unique solution exists for the derivation and for the numerical computation.\n7.  **Pseudo-Profound, Trivial, or Tautological**: The problem requires a multi-step derivation involving calculus and careful attention to indexing, followed by a non-trivial vectorized implementation. It is not a trivial problem.\n\n**Step 3: Verdict and Action**\n\nThe problem is determined to be **valid**. I will proceed with the solution.\n\n### Task 1: Analytical Derivation of the Jacobian\n\nThe discrete-time state transition function is given by the forward Euler method:\n$$ x_{k+1} = F(x_k, u_k) = x_k + \\Delta t\\, f(x_k, u_k) $$\nThe Jacobian matrix $A_k$ is the partial derivative of $F$ with respect to the state vector $x_k$:\n$$ A_k = \\frac{\\partial F}{\\partial x_k} = \\frac{\\partial}{\\partial x_k} \\left( x_k + \\Delta t\\, f(x_k, u_k) \\right) = I + \\Delta t \\frac{\\partial f}{\\partial x_k} $$\nwhere $I$ is the $n \\times n$ identity matrix. Let $J_f = \\frac{\\partial f}{\\partial x_k}$ be the Jacobian of the continuous-time dynamics function $f$. We need to find the entries of $J_f$, which are $[J_f]_{i,j} = \\frac{\\partial f_i}{\\partial T_{j,k}}$.\n\nFrom the energy balance equation, the $i$-th component of $f(x_k, u_k)$ is:\n$$ f_i(x_k, u_k) = \\frac{1}{C_i} \\left( \\sum_{j \\in \\mathcal{N}(i)} \\frac{T_{j,k} - T_{i,k}}{R_{i,j}} + \\frac{T_{\\mathrm{amb},k} - T_{i,k}}{R_{\\mathrm{conv},i}} + q_i(I_k, T_{i,k}) \\right) $$\nwhere $q_i(I_k, T_{i,k}) = I_k^2 R_{0,i} \\exp(\\alpha_i (T_{i,k} - T_{\\mathrm{ref}}))$.\n\nLet's compute the partial derivatives $\\frac{\\partial f_i}{\\partial T_{j,k}}$.\n\nFirst, we find the partial derivative of the heat generation term with respect to its own node temperature $T_{i,k}$:\n$$ \\frac{\\partial q_i}{\\partial T_{i,k}} = \\frac{\\partial}{\\partial T_{i,k}} \\left( I_k^2 R_{0,i} \\exp(\\alpha_i (T_{i,k} - T_{\\mathrm{ref}})) \\right) = I_k^2 R_{0,i} \\alpha_i \\exp(\\alpha_i (T_{i,k} - T_{\\mathrm{ref}})) $$\nFor $j \\neq i$, $\\frac{\\partial q_i}{\\partial T_{j,k}} = 0$.\n\nThe Jacobian $J_f$ will be tridiagonal due to the one-dimensional chain structure. We consider three cases for the rows of the Jacobian: the first node ($i=1$), an interior node ($i \\in \\{2, \\dots, n-1\\}$), and the last node ($i=n$).\n\n**Case 1: Diagonal entries, $[J_f]_{i,i} = \\frac{\\partial f_i}{\\partial T_{i,k}}$**\n\nThe terms in $f_i$ containing $T_{i,k}$ are the conduction terms, the convection term, and the heat generation term.\n*   For the first node ($i=1$): $\\mathcal{N}(1)=\\{2\\}$.\n    $$ f_1 = \\frac{1}{C_1} \\left( \\frac{T_{2,k} - T_{1,k}}{R_{\\mathrm{cond},1}} + \\frac{T_{\\mathrm{amb},k} - T_{1,k}}{R_{\\mathrm{conv},1}} + q_1 \\right) $$\n    $$ \\frac{\\partial f_1}{\\partial T_{1,k}} = \\frac{1}{C_1} \\left( -\\frac{1}{R_{\\mathrm{cond},1}} - \\frac{1}{R_{\\mathrm{conv},1}} + \\frac{\\partial q_1}{\\partial T_{1,k}} \\right) $$\n*   For an interior node ($i \\in \\{2, \\dots, n-1\\}$): $\\mathcal{N}(i)=\\{i-1, i+1\\}$.\n    $$ f_i = \\frac{1}{C_i} \\left( \\frac{T_{i-1,k} - T_{i,k}}{R_{\\mathrm{cond},i-1}} + \\frac{T_{i+1,k} - T_{i,k}}{R_{\\mathrm{cond},i}} + \\frac{T_{\\mathrm{amb},k} - T_{i,k}}{R_{\\mathrm{conv},i}} + q_i \\right) $$\n    $$ \\frac{\\partial f_i}{\\partial T_{i,k}} = \\frac{1}{C_i} \\left( -\\frac{1}{R_{\\mathrm{cond},i-1}} - \\frac{1}{R_{\\mathrm{cond},i}} - \\frac{1}{R_{\\mathrm{conv},i}} + \\frac{\\partial q_i}{\\partial T_{i,k}} \\right) $$\n*   For the last node ($i=n$): $\\mathcal{N}(n)=\\{n-1\\}$.\n    $$ f_n = \\frac{1}{C_n} \\left( \\frac{T_{n-1,k} - T_{n,k}}{R_{\\mathrm{cond},n-1}} + \\frac{T_{\\mathrm{amb},k} - T_{n,k}}{R_{\\mathrm{conv},n}} + q_n \\right) $$\n    $$ \\frac{\\partial f_n}{\\partial T_{n,k}} = \\frac{1}{C_n} \\left( -\\frac{1}{R_{\\mathrm{cond},n-1}} - \\frac{1}{R_{\\mathrm{conv},n}} + \\frac{\\partial q_n}{\\partial T_{n,k}} \\right) $$\n\n**Case 2: Off-diagonal entries, $[J_f]_{i,j} = \\frac{\\partial f_i}{\\partial T_{j,k}}$ for $i \\neq j$**\n\nA non-zero derivative occurs only if node $j$ is a neighbor of node $i$.\n*   Super-diagonal ($j = i+1$): The term involving $T_{i+1,k}$ in $f_i$ is $\\frac{T_{i+1,k}}{C_i R_{\\mathrm{cond},i}}$.\n    $$ \\frac{\\partial f_i}{\\partial T_{i+1,k}} = \\frac{1}{C_i R_{\\mathrm{cond},i}}, \\quad \\text{for } i = 1, \\dots, n-1 $$\n*   Sub-diagonal ($j = i-1$): The term involving $T_{i-1,k}$ in $f_i$ is $\\frac{T_{i-1,k}}{C_i R_{\\mathrm{cond},i-1}}$.\n    $$ \\frac{\\partial f_i}{\\partial T_{i-1,k}} = \\frac{1}{C_i R_{\\mathrm{cond},i-1}}, \\quad \\text{for } i = 2, \\dots, n $$\n*   All other off-diagonal entries are zero.\n\n**Assembling the Jacobian $A_k$**\n\nThe entries of $A_k = I + \\Delta t J_f$ are $[A_k]_{i,j} = \\delta_{i,j} + \\Delta t [J_f]_{i,j}$.\n\n*   **Diagonal entries $[A_k]_{i,i}$**:\n    *   For $i=1$:\n        $$ [A_k]_{1,1} = 1 + \\frac{\\Delta t}{C_1} \\left( -\\frac{1}{R_{\\mathrm{cond},1}} - \\frac{1}{R_{\\mathrm{conv},1}} + I_k^2 R_{0,1} \\alpha_1 e^{\\alpha_1(T_{1,k}-T_{\\mathrm{ref}})} \\right) $$\n    *   For $i \\in \\{2, \\dots, n-1\\}$:\n        $$ [A_k]_{i,i} = 1 + \\frac{\\Delta t}{C_i} \\left( -\\frac{1}{R_{\\mathrm{cond},i-1}} - \\frac{1}{R_{\\mathrm{cond},i}} - \\frac{1}{R_{\\mathrm{conv},i}} + I_k^2 R_{0,i} \\alpha_i e^{\\alpha_i(T_{i,k}-T_{\\mathrm{ref}})} \\right) $$\n    *   For $i=n$:\n        $$ [A_k]_{n,n} = 1 + \\frac{\\Delta t}{C_n} \\left( -\\frac{1}{R_{\\mathrm{cond},n-1}} - \\frac{1}{R_{\\mathrm{conv},n}} + I_k^2 R_{0,n} \\alpha_n e^{\\alpha_n(T_{n,k}-T_{\\mathrm{ref}})} \\right) $$\n\n*   **Sub-diagonal entries $[A_k]_{i,i-1}$** for $i=2, \\dots, n$:\n    $$ [A_k]_{i, i-1} = \\frac{\\Delta t}{C_i R_{\\mathrm{cond},i-1}} $$\n\n*   **Super-diagonal entries $[A_k]_{i,i+1}$** for $i=1, \\dots, n-1$:\n    $$ [A_k]_{i, i+1} = \\frac{\\Delta t}{C_i R_{\\mathrm{cond},i}} $$\n\nThis completes the analytical derivation. The implementation will focus on computing the diagonal entries $[A_k]_{i,i}$ as requested.\n\n### Task 2: Implementation\n\nThe implementation will compute the diagonal of $A_k$ using vectorized NumPy operations to satisfy the efficiency requirement. The strategy is to compute a vector for the thermal resistance terms and a vector for the heat generation derivative, combine them, and then compute the final diagonal vector according to the derived formula.",
            "answer": "```python\nimport numpy as np\n\ndef compute_jacobian_diagonal(n, dt, C, R_cond, R_conv, R0, alpha, T_ref, xk, Ik, Tambk):\n    \"\"\"\n    Computes the diagonal entries of the Jacobian matrix A_k for the battery thermal model.\n\n    The implementation uses vectorized numpy operations to achieve efficiency.\n    \"\"\"\n    # Convert all list-like inputs to numpy arrays for vectorized calculations.\n    C = np.array(C, dtype=float)\n    R_cond = np.array(R_cond, dtype=float)\n    R_conv = np.array(R_conv, dtype=float)\n    R0 = np.array(R0, dtype=float)\n    alpha = np.array(alpha, dtype=float)\n    xk = np.array(xk, dtype=float)\n\n    # 1. Calculate the derivative of the heat generation term w.r.t temperature for each node.\n    # This is a vector of [dq_1/dT_1, dq_2/dT_2, ..., dq_n/dT_n].\n    # dq_i/dT_i = I_k^2 * R_{0,i} * alpha_i * exp(alpha_i * (T_{i,k} - T_ref))\n    dq_dT = (Ik**2) * R0 * alpha * np.exp(alpha * (xk - T_ref))\n\n    # 2. Calculate the summation of inverse resistances for each node.\n    # This represents the total thermal conductance away from each node.\n    # For a generic node i, this sum is (1/R_cond,i-1 + 1/R_cond,i + 1/R_conv,i).\n    # We build this vector efficiently without loops.\n    \n    # Initialize a vector for the sum of inverse resistances (conductances).\n    sum_inv_R = np.zeros(n, dtype=float)\n    \n    # Add conduction terms. R_cond is of length n-1.\n    if n > 1:\n        inv_R_cond = 1.0 / R_cond\n        # Node 0 (i=1 in problem statement) is connected to its right neighbor.\n        sum_inv_R[0] += inv_R_cond[0]\n        # Node n-1 (i=n) is connected to its left neighbor.\n        sum_inv_R[-1] += inv_R_cond[-1]\n        # Interior nodes (1 to n-2) are connected to both left and right neighbors.\n        # sum_inv_R[i] += inv_R_cond[i-1] + inv_R_cond[i]\n        sum_inv_R[1:-1] += inv_R_cond[:-1] + inv_R_cond[1:]\n\n    # Add convection terms for all nodes.\n    sum_inv_R += 1.0 / R_conv\n\n    # 3. Calculate the diagonal of the Jacobian of the continuous dynamics function, J_f.\n    # [J_f]_{i,i} = (1/C_i) * (dq_i/dT_i - sum of inverse resistances for node i)\n    diag_Jf = (1.0 / C) * (dq_dT - sum_inv_R)\n\n    # 4. Calculate the diagonal of the final discrete-time Jacobian, A_k.\n    # [A_k]_{i,i} = 1 + dt * [J_f]_{i,i}\n    diag_A = 1.0 + dt * diag_Jf\n    \n    return diag_A\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and print results in the required format.\n    \"\"\"\n    test_cases = [\n        # Case 1 (general happy path)\n        {\n            \"n\": 4, \"dt\": 0.1, \"C\": [500.0, 520.0, 510.0, 505.0], \n            \"R_cond\": [0.4, 0.45, 0.5], \"R_conv\": [2.5, 1.0e9, 1.0e9, 2.0],\n            \"R0\": [2.0e-3, 2.2e-3, 2.1e-3, 2.0e-3], \n            \"alpha\": [0.01, 0.012, 0.011, 0.01], \"T_ref\": 298.0,\n            \"xk\": [300.0, 299.0, 301.0, 300.5], \"Ik\": 30.0, \"Tambk\": 298.0\n        },\n        # Case 2 (zero-current edge case)\n        {\n            \"n\": 4, \"dt\": 0.1, \"C\": [500.0, 520.0, 510.0, 505.0], \n            \"R_cond\": [0.4, 0.45, 0.5], \"R_conv\": [2.5, 1.0e9, 1.0e9, 2.0],\n            \"R0\": [2.0e-3, 2.2e-3, 2.1e-3, 2.0e-3], \n            \"alpha\": [0.01, 0.012, 0.011, 0.01], \"T_ref\": 298.0,\n            \"xk\": [300.0, 299.0, 301.0, 300.5], \"Ik\": 0.0, \"Tambk\": 298.0\n        },\n        # Case 3 (time-step edge case)\n        {\n            \"n\": 4, \"dt\": 1.0e-6, \"C\": [500.0, 520.0, 510.0, 505.0], \n            \"R_cond\": [0.4, 0.45, 0.5], \"R_conv\": [2.5, 1.0e9, 1.0e9, 2.0],\n            \"R0\": [2.0e-3, 2.2e-3, 2.1e-3, 2.0e-3], \n            \"alpha\": [0.01, 0.012, 0.011, 0.01], \"T_ref\": 298.0,\n            \"xk\": [300.0, 299.0, 301.0, 300.5], \"Ik\": 30.0, \"Tambk\": 298.0\n        },\n        # Case 4 (nonlinearity stress)\n        {\n            \"n\": 4, \"dt\": 0.05, \"C\": [480.0, 500.0, 495.0, 485.0], \n            \"R_cond\": [0.35, 0.4, 0.38], \"R_conv\": [2.0, 1.0e9, 1.0e9, 1.8],\n            \"R0\": [3.0e-3, 2.8e-3, 2.9e-3, 3.1e-3], \n            \"alpha\": [0.02, 0.018, 0.019, 0.02], \"T_ref\": 298.0,\n            \"xk\": [330.0, 328.0, 332.0, 331.0], \"Ik\": 100.0, \"Tambk\": 305.0\n        }\n    ]\n\n    results_for_all_cases = []\n    for case in test_cases:\n        # Calculate the diagonal of A_k for the current case\n        diag_A = compute_jacobian_diagonal(**case)\n        # Round the results to 6 decimal places as required\n        rounded_diag_A = np.round(diag_A, 6)\n        results_for_all_cases.append(rounded_diag_A.tolist())\n    \n    # Format the final output string exactly as specified in the problem statement.\n    # The format is [[d11,d12,...],[d21,d22,...]], with no spaces inside the inner lists.\n    case_strings = []\n    for result_list in results_for_all_cases:\n        # Format each float to ensure it has 6 decimal places, e.g., 1.0 -> 1.000000\n        # This matches the spirit of \"rounded to six decimal places\" for display.\n        s_list = [f\"{val:.6f}\" for val in result_list]\n        case_strings.append(f\"[{','.join(s_list)}]\")\n    \n    final_output_string = f\"[{','.join(case_strings)}]\"\n    print(final_output_string)\n\nsolve()\n```"
        },
        {
            "introduction": "Particle filters, a powerful class of Sequential Monte Carlo methods, are essential for tracking states in highly non-linear or non-Gaussian systems where traditional Kalman filters may fail. A key challenge in their practical implementation is \"sample degeneracy,\" where most computational effort is wasted on particles with negligible importance. This exercise guides you through implementing the core mechanism to combat this issue: calculating the Effective Sample Size ($ESS$) and applying low-variance resampling techniques, which are crucial skills for developing robust Bayesian estimators .",
            "id": "3944847",
            "problem": "Consider a streaming Bayesian state estimator for a lithium-ion battery in which a Sequential Importance Resampling (SIR) particle filter is used to update latent states and parameters in real time from operational current and voltage data. Let there be $N$ particles indexed by $i \\in \\{0,1,\\dots,N-1\\}$ with importance weights $w_t^{(i)}$ at discrete time $t \\in \\mathbb{N}$. The online update is driven by incremental log-likelihood contributions $\\ell_t^{(i)}$ computed from the most recent measurement residual under a physically consistent electrochemical-thermal surrogate model. Assume that, conditioned on the most recent measurement at time $t$, the unnormalized log-weights obey\n$$\n\\log \\tilde{w}_t^{(i)} = \\log w_{t-1}^{(i)} + \\ell_t^{(i)} \\, ,\n$$\nand the normalized weights are\n$$\nw_t^{(i)} = \\frac{\\exp\\left(\\log \\tilde{w}_t^{(i)}\\right)}{\\sum_{j=0}^{N-1}\\exp\\left(\\log \\tilde{w}_t^{(j)}\\right)} \\, .\n$$\nThe Effective Sample Size (ESS) at time $t$ is defined by\n$$\n\\mathrm{ESS}_t = \\frac{1}{\\sum_{i=0}^{N-1}\\left(w_t^{(i)}\\right)^2} \\, .\n$$\nYou must implement an online algorithm that, at each time $t$, updates the weights, computes $\\mathrm{ESS}_t$, and applies a resampling step if $\\mathrm{ESS}_t$ falls below a threshold. The threshold is defined as $\\tau = \\alpha N$ for a given fraction $\\alpha \\in (0,1]$. When resampling triggers, apply one of two low-variance resampling schemes that are suitable for real-time implementation:\n\n- Systematic resampling: draw a single $u \\sim \\mathrm{Uniform}(0, 1/N)$ and place $N$ points $u_k = u + k/N$ for $k \\in \\{0,1,\\dots,N-1\\}$ on the cumulative distribution function determined by $\\{w_t^{(i)}\\}$ to select ancestor indices.\n\n- Stratified resampling: draw $u_k \\sim \\mathrm{Uniform}(k/N, (k+1)/N)$ independently for $k \\in \\{0,1,\\dots,N-1\\}$ and select ancestor indices by inverting the cumulative distribution function determined by $\\{w_t^{(i)}\\}$.\n\nIn either resampling method, when resampling occurs at time $t$, you must:\n- Select an index vector of ancestors of length $N$ using the specified method and the normalized weights $\\{w_t^{(i)}\\}$.\n- Reset the particle weights to uniform, that is $w_t^{(i)} = 1/N$ for all $i$.\n- For subsequent time steps, continue the same online procedure using the updated weights.\n\nNumerical stability requirement: When computing normalized weights from log-weights, you must use a numerically stable normalization that does not overflow for large magnitude $\\ell_t^{(i)}$. For example, you may subtract $\\max_i \\log \\tilde{w}_t^{(i)}$ before exponentiation.\n\nDecision and record-keeping requirements:\n- At each time $t$, compute $\\mathrm{ESS}_t$ using the normalized weights $w_t^{(i)}$ prior to any resampling, and record it as a floating-point number rounded to $3$ decimals.\n- Determine if resampling is triggered at time $t$ by checking if $\\mathrm{ESS}_t < \\tau$. Record a Boolean indicating whether resampling was triggered at that time.\n- If resampling is triggered at time $t$, record the integer list of ancestor indices (zero-based) returned by the resampling method at that time. If multiple resamplings occur across time steps in a test case, only the most recent resampling’s ancestor index list should be recorded at the end of the test case. If no resampling occurs in a test case, the recorded ancestor index list must be the empty list.\n\nRandomness and reproducibility: Use a fixed random number generator seed $12345$ to ensure deterministic outputs for the resampling schemes.\n\nYou are given a test suite comprising four independent test cases. In all cases, the initial weights are uniform, that is $w_{-1}^{(i)} = 1/N$. For each test case, you are provided:\n- The particle count $N$.\n- The resampling fraction $\\alpha$.\n- The resampling method, either “systematic” or “stratified”.\n- A sequence of incremental log-likelihood vectors $\\ell_t \\in \\mathbb{R}^N$ for $t=0,1,\\dots,T-1$.\n\nTest Suite:\n- Test Case $1$:\n  - $N = 8$\n  - $\\alpha = 0.5$\n  - method: systematic\n  - $\\ell_0 = [\\, 0.0,\\; -0.2,\\; 0.1,\\; -0.1,\\; 0.3,\\; -0.5,\\; 0.2,\\; -0.3 \\,]$\n  - $\\ell_1 = [\\, 0.4,\\; -0.4,\\; 0.0,\\; 0.1,\\; -0.2,\\; 0.3,\\; -0.1,\\; -0.1 \\,]$\n  - $\\ell_2 = [\\, 0.0,\\; 0.0,\\; 0.0,\\; 0.6,\\; -0.6,\\; 0.2,\\; 0.1,\\; -0.3 \\,]$\n\n- Test Case $2$:\n  - $N = 10$\n  - $\\alpha = 0.5$\n  - method: systematic\n  - $\\ell_0 = [\\, 0.0,\\; 0.0,\\; 0.0,\\; 0.0,\\; 0.0,\\; 0.0,\\; 0.0,\\; 0.0,\\; 0.0,\\; 0.0 \\,]$\n  - $\\ell_1 = [\\, 0.0,\\; 0.0,\\; 0.0,\\; 0.0,\\; 0.0,\\; 0.0,\\; 0.0,\\; 0.0,\\; 0.0,\\; 0.0 \\,]$\n\n- Test Case $3$:\n  - $N = 7$\n  - $\\alpha = 0.7$\n  - method: stratified\n  - $\\ell_0 = [\\, 2.0,\\; -3.0,\\; -3.0,\\; -3.0,\\; -3.0,\\; -3.0,\\; -3.0 \\,]$\n  - $\\ell_1 = [\\, 0.1,\\; -0.1,\\; 0.2,\\; -0.2,\\; 0.0,\\; 0.0,\\; 0.0 \\,]$\n\n- Test Case $4$:\n  - $N = 6$\n  - $\\alpha = 0.75$\n  - method: systematic\n  - $\\ell_0 = [\\, 0.0,\\; 0.0,\\; 0.0,\\; 0.0,\\; 0.0,\\; 0.0 \\,]$\n  - $\\ell_1 = [\\, 0.5,\\; 0.49,\\; 0.48,\\; 0.47,\\; 0.46,\\; 0.45 \\,]$\n  - $\\ell_2 = [\\, -0.5,\\; -0.49,\\; -0.48,\\; -0.47,\\; -0.46,\\; -0.45 \\,]$\n  - $\\ell_3 = [\\, 1.0,\\; -0.2,\\; -0.2,\\; -0.2,\\; -0.2,\\; -0.2 \\,]$\n\nRequirements for what to compute and return per test case:\n- A list of $\\mathrm{ESS}_t$ values for each time step $t$, rounded to $3$ decimals.\n- A list of Booleans of the same length indicating whether resampling was triggered at each time step $t$.\n- The list of ancestor indices from the last time step in the test case at which resampling was triggered (zero-based indexing). If no resampling occurred in the test case, this list must be empty.\n\nFinal output format:\n- Your program must produce a single line of output that is a comma-separated list enclosed in square brackets. Each element corresponds to one test case and must itself be a list with three elements in the following order: the list of rounded $\\mathrm{ESS}_t$ values, the list of resampling Booleans, and the final recorded ancestor index list (possibly empty).\n- Concretely, the output must look like a Python list-of-lists, for example: `[[[ess_list_case1],[flags_case1],[ancestors_case1]],[[ess_list_case2],[flags_case2],[ancestors_case2]],[[ess_list_case3],[flags_case3],[ancestors_case3]],[[ess_list_case4],[flags_case4],[ancestors_case4]]]`.\n- No other text may be printed.\n\nAll answers are dimensionless; no physical units are required. Angles are not used. Percentages must be expressed as decimals (for example, $\\alpha = 0.5$ rather than $50$). Use zero-based indices for ancestor lists. Ensure the algorithm runs in time $\\mathcal{O}(N)$ per time step and uses only operations suitable for real-time implementation.",
            "solution": "The problem requires the implementation of a Sequential Importance Resampling (SIR) particle filter algorithm, a standard method in Bayesian state estimation. The solution involves processing a series of incremental log-likelihoods over discrete time steps to update particle weights, assess sample degeneracy via the Effective Sample Size (ESS), and perform resampling when necessary. The process must be executed for four independent test cases, with specific record-keeping at each step.\n\nThe algorithmic procedure for each test case is as follows:\n\n1.  **Initialization**:\n    For each test case, we are given the number of particles $N$, a resampling threshold factor $\\alpha$, a resampling method, and a time-series of incremental log-likelihood vectors $\\{\\ell_t\\}$. The process starts at time $t=0$.\n    - The initial particle weights at time $t=-1$ are uniform, $w_{-1}^{(i)} = 1/N$ for all particles $i \\in \\{0, 1, \\dots, N-1\\}$. Consequently, the initial log-weights are $\\log w_{-1}^{(i)} = -\\log N$.\n    - A random number generator is initialized with the specified seed, $12345$, to ensure deterministic and reproducible outputs for the stochastic resampling steps. To ensure test cases are independent, the generator is re-initialized for each case.\n    - Result containers are initialized: an empty list for $\\mathrm{ESS}_t$ values, an empty list for resampling trigger booleans, and an empty list for the final set of ancestor indices.\n\n2.  **Iterative Update Loop**:\n    The core of the algorithm is a loop that iterates through each time step $t=0, 1, \\dots, T-1$. At each step $t$:\n\n    a.  **Weight Update**: The unnormalized log-weights, $\\log \\tilde{w}_t^{(i)}$, are calculated by incorporating the new information from the incremental log-likelihoods, $\\ell_t^{(i)}$:\n        $$\n        \\log \\tilde{w}_t^{(i)} = \\log w_{t-1}^{(i)} + \\ell_t^{(i)}\n        $$\n        Here, $\\log w_{t-1}^{(i)}$ are the logarithm of the normalized weights from the previous time step. For $t=0$, these are the initial uniform log-weights.\n\n    b.  **Numerically Stable Normalization**: The normalized weights $w_t^{(i)}$ are computed from the unnormalized log-weights. To prevent numerical overflow or underflow from the exponentiation of potentially large or small log-weight values, the log-sum-exp trick is employed. Let $c = \\max_j \\log \\tilde{w}_t^{(j)}$. The normalized weights are then:\n        $$\n        w_t^{(i)} = \\frac{\\exp(\\log \\tilde{w}_t^{(i)} - c)}{\\sum_{j=0}^{N-1} \\exp(\\log \\tilde{w}_t^{(j)} - c)}\n        $$\n        The logarithm of these normalized weights, $\\log w_t^{(i)}$, is stored for the next iteration's update step, unless a resampling event occurs.\n\n    c.  **Effective Sample Size (ESS) Calculation**: The ESS is a measure of weight degeneracy. It is calculated using the normalized weights $w_t^{(i)}$:\n        $$\n        \\mathrm{ESS}_t = \\frac{1}{\\sum_{i=0}^{N-1} (w_t^{(i)})^2}\n        $$\n        A value of $\\mathrm{ESS}_t \\approx N$ indicates that all weights are nearly uniform, while $\\mathrm{ESS}_t \\approx 1$ indicates that a single particle has a weight close to $1$, and all others are near $0$. The calculated $\\mathrm{ESS}_t$ is rounded to $3$ decimal places and appended to the results list for the current test case.\n\n    d.  **Resampling Decision**: A decision is made whether to resample the particles. Resampling is triggered if the ESS falls below a predefined threshold, $\\tau = \\alpha N$:\n        $$\n        \\mathrm{Resample} \\iff \\mathrm{ESS}_t < \\tau\n        $$\n        A boolean value indicating this decision ($True$ or $False$) is appended to the corresponding results list.\n\n    e.  **Resampling and Weight Management**:\n        - **If resampling is triggered**:\n            One of two specified low-variance resampling schemes is used to select a new set of $N$ ancestor particles. The selection is based on the current normalized weights $w_t^{(i)}$.\n            - **Systematic Resampling**: A single random number $u$ is drawn from $\\mathrm{Uniform}(0, 1/N)$. This defines a grid of $N$ points $u_k = u + k/N$ for $k \\in \\{0, 1, \\dots, N-1\\}$.\n            - **Stratified Resampling**: $N$ random numbers $u_k$ are drawn independently, each from a different stratum: $u_k \\sim \\mathrm{Uniform}(k/N, (k+1)/N)$.\n            In both methods, these $N$ points $\\{u_k\\}$ are used to invert the cumulative distribution function (CDF) of the particle weights, $F(i) = \\sum_{j=0}^i w_t^{(j)}$, to find the ancestor indices. This can be implemented efficiently in $\\mathcal{O}(N)$ time using a sorted search on the CDF array. The resulting vector of ancestor indices is stored, overwriting any list from a previous resampling event within the same test case. After resampling, the particle weights are reset to be uniform for the next time step, $t+1$. Thus, the log-weights become $\\log w_t^{(i)} = -\\log N$ for all $i$.\n\n        - **If resampling is not triggered**:\n            The particles and their weights are carried forward. The log-weights for the next step, $\\log w_t^{(i)}$, are the normalized log-weights computed in step 2b.\n\n3.  **Output Aggregation**:\n    After processing all time steps for a given test case, the final results consist of three components: the list of all computed $\\mathrm{ESS}_t$ values, the list of all resampling booleans, and the list of ancestor indices from the most recent resampling event (or an empty list if no resampling occurred). These three lists are collected into a single list for the test case. The final output is an aggregation of these results from all four test cases into a single list-of-lists structure, formatted as a string according to the problem specification.",
            "answer": "```python\nimport numpy as np\n\ndef systematic_resample(weights, rng):\n    \"\"\"\n    Performs systematic resampling.\n    \"\"\"\n    N = len(weights)\n    positions = (rng.uniform(0.0, 1.0) + np.arange(N)) / N\n    cdf = np.cumsum(weights)\n    indices = np.searchsorted(cdf, positions)\n    return indices\n\ndef stratified_resample(weights, rng):\n    \"\"\"\n    Performs stratified resampling.\n    \"\"\"\n    N = len(weights)\n    positions = (rng.uniform(size=N) + np.arange(N)) / N\n    cdf = np.cumsum(weights)\n    indices = np.searchsorted(cdf, positions)\n    return indices\n\ndef solve_case(N, alpha, method, log_likelihoods_sequence, rng):\n    \"\"\"\n    Processes a single test case for the SIR particle filter.\n    \"\"\"\n    log_weights = np.log(np.full(N, 1.0 / N, dtype=np.float64))\n    \n    ess_history = []\n    resample_flags = []\n    last_ancestors = []\n    \n    threshold = alpha * N\n\n    for t, ll_t in enumerate(log_likelihoods_sequence):\n        # 1. Update unnormalized log-weights\n        log_w_tilde = log_weights + np.array(ll_t, dtype=np.float64)\n\n        # 2. Numerically stable normalization (log-sum-exp)\n        c = np.max(log_w_tilde)\n        exp_log_w_shifted = np.exp(log_w_tilde - c)\n        w_t = exp_log_w_shifted / np.sum(exp_log_w_shifted)\n\n        # 3. Compute and store ESS\n        ess_t = 1.0 / np.sum(w_t**2)\n        ess_history.append(round(ess_t, 3))\n\n        # 4. Resampling decision\n        resample_triggered = ess_t  threshold\n        resample_flags.append(resample_triggered)\n\n        # 5. Resample if needed\n        if resample_triggered:\n            if method == \"systematic\":\n                ancestors = systematic_resample(w_t, rng)\n            elif method == \"stratified\":\n                ancestors = stratified_resample(w_t, rng)\n            \n            last_ancestors = ancestors.tolist()\n            \n            # Reset weights to uniform\n            log_weights = np.log(np.full(N, 1.0 / N, dtype=np.float64))\n        else:\n            # Propagate normalized log-weights\n            log_weights = np.log(w_t)\n            # Handle -inf from weights that are numerically zero\n            log_weights[np.isneginf(log_weights)] = -1e100 # A large negative number\n\n    return [ess_history, resample_flags, last_ancestors]\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final result.\n    \"\"\"\n    test_cases = [\n        {\n            \"N\": 8, \"alpha\": 0.5, \"method\": \"systematic\",\n            \"log_likelihoods\": [\n                [0.0, -0.2, 0.1, -0.1, 0.3, -0.5, 0.2, -0.3],\n                [0.4, -0.4, 0.0, 0.1, -0.2, 0.3, -0.1, -0.1],\n                [0.0, 0.0, 0.0, 0.6, -0.6, 0.2, 0.1, -0.3]\n            ]\n        },\n        {\n            \"N\": 10, \"alpha\": 0.5, \"method\": \"systematic\",\n            \"log_likelihoods\": [\n                [0.0] * 10,\n                [0.0] * 10\n            ]\n        },\n        {\n            \"N\": 7, \"alpha\": 0.7, \"method\": \"stratified\",\n            \"log_likelihoods\": [\n                [2.0, -3.0, -3.0, -3.0, -3.0, -3.0, -3.0],\n                [0.1, -0.1, 0.2, -0.2, 0.0, 0.0, 0.0]\n            ]\n        },\n        {\n            \"N\": 6, \"alpha\": 0.75, \"method\": \"systematic\",\n            \"log_likelihoods\": [\n                [0.0] * 6,\n                [0.5, 0.49, 0.48, 0.47, 0.46, 0.45],\n                [-0.5, -0.49, -0.48, -0.47, -0.46, -0.45],\n                [1.0, -0.2, -0.2, -0.2, -0.2, -0.2]\n            ]\n        }\n    ]\n\n    all_results = []\n    for case_data in test_cases:\n        # Re-initialize RNG for each independent test case\n        rng = np.random.default_rng(12345)\n        \n        result = solve_case(\n            case_data[\"N\"],\n            case_data[\"alpha\"],\n            case_data[\"method\"],\n            case_data[\"log_likelihoods\"],\n            rng\n        )\n        all_results.append(result)\n\n    # Format the final output string to have no spaces\n    result_strings = [str(res).replace(\" \", \"\") for res in all_results]\n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The theoretical elegance of the Kalman filter equations can be challenged by the practical realities of finite-precision arithmetic on embedded hardware. This hands-on practice delves into the critical topic of numerical stability, exploring how limited floating-point precision can lead to the loss of fundamental properties of the covariance matrix, such as symmetry and positive semidefiniteness. By comparing single-precision ($\\texttt{float32}$) and double-precision ($\\texttt{float64}$) implementations of a battery state estimator, you will gain invaluable insight into diagnosing filter health and managing the trade-offs between computational speed and numerical robustness .",
            "id": "3944888",
            "problem": "You are tasked with analyzing the impact of floating-point precision on the stability of the Extended Kalman Filter (EKF) covariance update in a battery estimation setting. Consider a single-cell lithium-ion battery modeled by a $3$-state Equivalent Circuit Model (ECM) used for real-time estimation with operational data. The states are $x_k = [q_k, v_{1,k}, v_{2,k}]^\\top$ where $q_k$ is State of Charge (SOC), and $v_{1,k}$ and $v_{2,k}$ are the polarization voltages of two resistor-capacitor (RC) pairs. The time index is $k \\in \\{0,1,2,\\dots\\}$. The discrete-time state dynamics are linearized and given by\n$$\nx_{k+1} = F x_k + G i_k + w_k,\n$$\nwith process noise $w_k \\sim \\mathcal{N}(0, Q)$, and known input current $i_k$ (in $\\mathrm{A}$). For the covariance dynamics, the input matrix $G$ does not appear, and the linearized transition Jacobian is\n$$\nF = \\mathrm{diag}(1, a_1, a_2), \\quad a_j = \\exp\\left(-\\frac{\\Delta t}{\\tau_j}\\right), \\quad \\tau_j = R_j C_j,\n$$\nwhere $R_j$ (in $\\Omega$) and $C_j$ (in $\\mathrm{F}$) are the parameters of the RC pairs, and $\\Delta t$ (in $\\mathrm{s}$) is the sampling time. The voltage measurement model is\n$$\nz_k = \\mathrm{OCV}(q_k) - v_{1,k} - v_{2,k} - R_0 i_k + v_k,\n$$\nwith measurement noise $v_k \\sim \\mathcal{N}(0, R)$ and ohmic resistance $R_0$ (in $\\Omega$). Linearizing the measurement about a nominal SOC $q^\\star$ yields $z_k \\approx h_q q_k - v_{1,k} - v_{2,k} + c - R_0 i_k + v_k$ where $h_q = \\frac{d\\,\\mathrm{OCV}}{d q}(q^\\star)$ (in $\\mathrm{V}$ per unit SOC) and $c$ is a constant that does not impact the covariance. The measurement Jacobian is then\n$$\nH = \\begin{bmatrix} h_q  -1  -1 \\end{bmatrix}.\n$$\n\nThe Extended Kalman Filter (EKF) covariance recursion, using the Joseph stabilized update form, is\n$$\nP_{k+1}^- = F P_k F^\\top + Q,\n$$\n$$\nS_{k+1} = H P_{k+1}^- H^\\top + R,\n$$\n$$\nK_{k+1} = P_{k+1}^- H^\\top S_{k+1}^{-1},\n$$\n$$\nP_{k+1} = (I - K_{k+1} H) P_{k+1}^- (I - K_{k+1} H)^\\top + K_{k+1} R K_{k+1}^\\top,\n$$\nwhere $I$ is the identity matrix, $Q$ is the process noise covariance, $R$ is the measurement noise covariance, and $P_k$ is the state covariance at time index $k$. In exact arithmetic, the Joseph form preserves positive semidefiniteness and symmetry of $P_{k+1}$. In finite precision arithmetic, roundoff and conditioning effects can degrade these properties. Your goal is to compare single-precision ($\\texttt{float32}$) and double-precision ($\\texttt{float64}$) computations, and quantify numerical error impacts on the covariance updates over repeated real-time assimilation steps.\n\nImplement a program that simulates the covariance propagation and update for $N$ steps with constant current $i_k = i$ and repeated measurement updates using the above Jacobians and covariances. For each precision, compute across all steps the following metrics:\n- The maximum deviation from symmetry of the covariance, measured by the Frobenius norm $||P_k - P_k^\\top||_F$.\n- The count of steps where the symmetric part $\\frac{1}{2}(P_k + P_k^\\top)$ has a negative minimum eigenvalue (i.e., the minimum eigenvalue is less than $0$), indicating a violation of positive semidefiniteness.\nAdditionally, compute the Frobenius norm of the difference between the final-step covariances obtained with single-precision and double-precision, $||P_N^{(32)} - P_N^{(64)}||_F$, where the superscript denotes precision.\n\nUse the following test suite of parameter sets. All physical units must be respected: resistances in $\\Omega$, capacitances in $\\mathrm{F}$, time in $\\mathrm{s}$, current in $\\mathrm{A}$, and measurement noise covariance in $\\mathrm{V}^2$. The initial covariance $P_0$ and process noise covariance $Q$ are diagonal matrices with the provided diagonal entries. The measurement slope $h_q$ is in $\\mathrm{V}$ per unit SOC.\n\nTest Case $1$ (happy path, moderate conditioning):\n- $\\Delta t = 0.1$,\n- $R_0 = 0.01$, $R_1 = 0.005$, $C_1 = 2000$, $R_2 = 0.02$, $C_2 = 1500$,\n- $Q = \\mathrm{diag}(1\\times 10^{-7}, 1\\times 10^{-5}, 1\\times 10^{-5})$,\n- $R = 1\\times 10^{-3}$,\n- $P_0 = \\mathrm{diag}(1\\times 10^{-2}, 1\\times 10^{-2}, 1\\times 10^{-2})$,\n- $h_q = 0.1$,\n- $i = 5$,\n- $N = 200$.\n\nTest Case $2$ (ill-conditioned innovation covariance, very low noises):\n- $\\Delta t = 0.1$,\n- $R_0 = 0.01$, $R_1 = 0.005$, $C_1 = 2000$, $R_2 = 0.02$, $C_2 = 1500$,\n- $Q = \\mathrm{diag}(1\\times 10^{-12}, 1\\times 10^{-12}, 1\\times 10^{-12})$,\n- $R = 1\\times 10^{-8}$,\n- $P_0 = \\mathrm{diag}(1\\times 10^{-6}, 1\\times 10^{-6}, 1\\times 10^{-6})$,\n- $h_q = 0.1$,\n- $i = 5$,\n- $N = 200$.\n\nTest Case $3$ (high measurement noise):\n- $\\Delta t = 0.1$,\n- $R_0 = 0.01$, $R_1 = 0.005$, $C_1 = 2000$, $R_2 = 0.02$, $C_2 = 1500$,\n- $Q = \\mathrm{diag}(1\\times 10^{-7}, 1\\times 10^{-5}, 1\\times 10^{-5})$,\n- $R = 1.0$,\n- $P_0 = \\mathrm{diag}(1\\times 10^{-2}, 1\\times 10^{-2}, 1\\times 10^{-2})$,\n- $h_q = 0.1$,\n- $i = 5$,\n- $N = 200$.\n\nTest Case $4$ (near-integrator RC dynamics, long time constants, extended horizon):\n- $\\Delta t = 0.1$,\n- $R_0 = 0.01$, $R_1 = 0.0005$, $C_1 = 200000$, $R_2 = 0.001$, $C_2 = 150000$,\n- $Q = \\mathrm{diag}(1\\times 10^{-9}, 1\\times 10^{-9}, 1\\times 10^{-9})$,\n- $R = 1\\times 10^{-4}$,\n- $P_0 = \\mathrm{diag}(1\\times 10^{-3}, 1\\times 10^{-3}, 1\\times 10^{-3})$,\n- $h_q = 0.05$,\n- $i = 2$,\n- $N = 500$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each item corresponds to one test case and is itself a list of four values $[m_1, m_2, m_3, m_4]$ defined as:\n- $m_1$: maximum symmetry deviation $||P_k - P_k^\\top||_F$ over all steps for $\\texttt{float32}$,\n- $m_2$: maximum symmetry deviation $||P_k - P_k^\\top||_F$ over all steps for $\\texttt{float64}$,\n- $m_3$: count of steps with negative minimum eigenvalue of $\\frac{1}{2}(P_k + P_k^\\top)$ for $\\texttt{float32}$,\n- $m_4$: Frobenius norm $||P_N^{(32)} - P_N^{(64)}||_F$.\n\nThe final output format must be exactly:\n$$\n[[m_1^{(1)}, m_2^{(1)}, m_3^{(1)}, m_4^{(1)}], [m_1^{(2)}, m_2^{(2)}, m_3^{(2)}, m_4^{(2)}], [m_1^{(3)}, m_2^{(3)}, m_3^{(3)}, m_4^{(3)}], [m_1^{(4)}, m_2^{(4)}, m_3^{(4)}, m_4^{(4)}]]\n$$\nwith no extra text, where the superscript indicates the test case index.",
            "solution": "The problem is valid as it presents a well-defined, scientifically-grounded task in numerical analysis applied to battery state estimation. All necessary parameters, equations, and evaluation criteria are provided, forming a self-contained and objective problem statement. The task is to analyze the numerical stability of the Joseph stabilized form of the Kalman filter covariance update by comparing its behavior under single-precision (`float32`) and double-precision (`float64`) arithmetic.\n\nThe solution involves implementing a simulation of the covariance propagation over a specified number of steps for four distinct test cases. For each case, the simulation is run once for each floating-point precision. At each step of the simulation, from the initial covariance $P_0$ to the final covariance $P_N$, key properties of the covariance matrix $P_k$ are evaluated to quantify numerical degradation.\n\n**1. Model and Algorithm Formulation**\n\nThe simulation is based on a linearized time-invariant system, making the state transition matrix $F$ and measurement Jacobian matrix $H$ constant throughout the simulation.\n\nFirst, we construct these matrices from the provided physical parameters for a given floating-point precision `dtype`. The state vector dimensionality is $n=3$.\n-   The time constants for the two RC pairs are $\\tau_j = R_j C_j$ for $j \\in \\{1, 2\\}$.\n-   The state transition matrix $F$ is a $3 \\times 3$ diagonal matrix:\n    $$\n    F = \\mathrm{diag}\\left(1, \\exp\\left(-\\frac{\\Delta t}{\\tau_1}\\right), \\exp\\left(-\\frac{\\Delta t}{\\tau_2}\\right)\\right)\n    $$\n-   The measurement Jacobian $H$ is a $1 \\times 3$ row vector:\n    $$\n    H = \\begin{bmatrix} h_q  -1  -1 \\end{bmatrix}\n    $$\n-   The process noise covariance $Q$, measurement noise covariance $R$, and initial state covariance $P_0$ are initialized from the provided diagonal values. $Q$ and $P_0$ are $3 \\times 3$ matrices, and $R$ is treated as a $1 \\times 1$ matrix.\n-   The identity matrix $I$ is a $3 \\times 3$ matrix.\n\nAll these matrices and scalars ($F, H, Q, R, P_0, I$) are instantiated with the target `dtype` (`float32` or `float64`) to ensure all subsequent arithmetic operations are performed at the specified precision.\n\n**2. Covariance Propagation Simulation**\n\nA function is designed to execute the simulation for a given set of parameters and a specified `dtype`. The simulation starts with the initial covariance $P_0$ and iterates $N$ times to compute the sequence of posterior covariance matrices $P_1, P_2, \\ldots, P_N$. The simulation loop implements the Joseph form of the covariance update equations:\n\nFor each step $k$ from $0$ to $N-1$, to compute $P_{k+1}$ from $P_k$:\n1.  **Prediction Step**: The a priori covariance for step $k+1$ is calculated:\n    $$ P_{k+1}^- = F P_k F^\\top + Q $$\n2.  **Innovation Covariance**: The covariance of the innovation (measurement residual) is calculated. Since $H$ is a row vector, $S_{k+1}$ is a scalar (a $1 \\times 1$ matrix).\n    $$ S_{k+1} = H P_{k+1}^- H^\\top + R $$\n3.  **Kalman Gain**: The Kalman gain $K_{k+1}$ is computed. As $S_{k+1}$ is a scalar, its inverse is simply its reciprocal.\n    $$ K_{k+1} = P_{k+1}^- H^\\top S_{k+1}^{-1} $$\n4.  **Update Step (Joseph Form)**: The a posteriori covariance for step $k+1$ is computed using the numerically stable Joseph form, which is structured to better preserve the positive semidefiniteness and symmetry of the covariance matrix in finite-precision arithmetic.\n    $$ P_{k+1} = (I - K_{k+1} H) P_{k+1}^- (I - K_{k+1} H)^\\top + K_{k+1} R K_{k+1}^\\top $$\nThe resulting $P_{k+1}$ becomes the input $P_k$ for the next iteration.\n\n**3. Metric Calculation**\n\nDuring the simulation, we monitor the properties of all $N+1$ posterior covariance matrices, $P_0, P_1, \\ldots, P_N$. Four metrics are computed for each test case as follows:\n\n-   $m_1$: **Maximum Symmetry Deviation (float32)**. For each matrix $P_k$ generated in the `float32` simulation, the deviation from symmetry is computed using the Frobenius norm of the skew-symmetric part, $||P_k - P_k^\\top||_F$. Metric $m_1$ is the maximum value of this deviation observed over all steps $k=0, \\ldots, N$.\n-   $m_2$: **Maximum Symmetry Deviation (float64)**. This is analogous to $m_1$, but computed for the `float64` simulation. We expect this to be significantly smaller, near machine epsilon for double precision.\n-   $m_3$: **Non-Positive Semidefinite Count (float32)**. For each $P_k$ in the `float32` simulation, we check for positive semidefiniteness. Since numerical errors can make $P_k$ slightly non-symmetric, we first obtain its symmetric part, $P_{k, \\text{symm}} = \\frac{1}{2}(P_k + P_k^\\top)$. We then compute the eigenvalues of $P_{k, \\text{symm}}$ (using `numpy.linalg.eigvalsh` for robustness). If the minimum eigenvalue is less than zero, the matrix has lost its positive semidefinite property. Metric $m_3$ is the total count of such occurrences over all steps $k=0, \\ldots, N$.\n-   $m_4$: **Final Covariance Difference**. This metric quantifies the total divergence between the two precision runs. It is the Frobenius norm of the difference between the final covariance matrices from the single-precision and double-precision simulations, $||P_N^{(32)} - P_N^{(64)}||_F$. To compute this accurately, the `float32` matrix is cast to `float64` before subtraction.\n\nThe implementation iterates through the four provided test cases, running both `float32` and `float64` simulations for each, calculating the four metrics, and compiling the results into the specified final output format.",
            "answer": "```python\nimport numpy as np\n\ndef run_simulation(params, dtype):\n    \"\"\"\n    Simulates the EKF covariance propagation for N steps.\n\n    Args:\n        params (dict): A dictionary containing all model and simulation parameters.\n        dtype: The numpy data type to use for calculations (np.float32 or np.float64).\n\n    Returns:\n        tuple: A tuple containing:\n            - P_final (np.ndarray): The final covariance matrix P_N.\n            - max_symm_dev (float): The maximum symmetry deviation over all steps.\n            - neg_eig_count (int): The number of steps where P_k was not positive semidefinite.\n    \"\"\"\n    # Unpack parameters\n    delta_t = params['delta_t']\n    R1, C1 = params['R1'], params['C1']\n    R2, C2 = params['R2'], params['C2']\n    Q_diag = params['Q_diag']\n    R_val = params['R']\n    P0_diag = params['P0_diag']\n    h_q = params['h_q']\n    N = params['N']\n\n    # Convert all parameters and constants to the specified dtype\n    R1, C1 = dtype(R1), dtype(C1)\n    R2, C2 = dtype(R2), dtype(C2)\n    delta_t = dtype(delta_t)\n    h_q = dtype(h_q)\n\n    # Calculate State Transition Matrix F\n    tau1 = R1 * C1\n    tau2 = R2 * C2\n    a1 = np.exp(-delta_t / tau1)\n    a2 = np.exp(-delta_t / tau2)\n    F = np.diag(np.array([1.0, a1, a2], dtype=dtype))\n\n    # Measurement Jacobian H\n    H = np.array([[h_q, -1.0, -1.0]], dtype=dtype)\n    \n    # Covariance Matrices\n    Q = np.diag(np.array(Q_diag, dtype=dtype))\n    R_mat = np.array([[dtype(R_val)]], dtype=dtype)\n    P = np.diag(np.array(P0_diag, dtype=dtype))\n\n    # Identity Matrix\n    I = np.eye(3, dtype=dtype)\n    \n    symm_devs = []\n    neg_eig_counts = 0\n\n    # The simulation checks properties of P_0, P_1, ..., P_N (N+1 matrices)\n    for step in range(N + 1):\n        # --- Metric Calculation for current P ---\n        # 1. Symmetry Deviation\n        symm_dev = np.linalg.norm(P - P.T, 'fro')\n        symm_devs.append(symm_dev)\n\n        # 2. Positive Semidefiniteness Check\n        P_symm = 0.5 * (P + P.T)\n        try:\n            # Use eigvalsh for symmetric matrices for speed and stability\n            min_eig = np.min(np.linalg.eigvalsh(P_symm))\n            if min_eig  0:\n                neg_eig_counts += 1\n        except np.linalg.LinAlgError:\n            # If eigenvalue computation fails, it's a severe numerical issue\n            neg_eig_counts += 1\n        \n        # Stop after analyzing P_N; no need to compute P_{N+1}\n        if step == N:\n            break\n\n        # --- Covariance Propagation from P_k to P_{k+1} ---\n        # Prediction\n        P_minus = F @ P @ F.T + Q\n        \n        # Innovation Covariance\n        S = H @ P_minus @ H.T + R_mat\n\n        # Kalman Gain\n        S_inv = np.array([[1.0 / S[0, 0]]], dtype=dtype)\n        K = P_minus @ H.T @ S_inv\n        \n        # Update (Joseph Form)\n        I_KH = I - K @ H\n        P = (I_KH @ P_minus @ I_KH.T) + (K @ R_mat @ K.T)\n\n    max_symm_dev = np.max(symm_devs) if symm_devs else 0.0\n    \n    return P, max_symm_dev, neg_eig_counts\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        { # Test Case 1\n            'delta_t': 0.1, 'R1': 0.005, 'C1': 2000, 'R2': 0.02, 'C2': 1500,\n            'Q_diag': [1e-7, 1e-5, 1e-5], 'R': 1e-3, 'P0_diag': [1e-2, 1e-2, 1e-2],\n            'h_q': 0.1, 'N': 200\n        },\n        { # Test Case 2\n            'delta_t': 0.1, 'R1': 0.005, 'C1': 2000, 'R2': 0.02, 'C2': 1500,\n            'Q_diag': [1e-12, 1e-12, 1e-12], 'R': 1e-8, 'P0_diag': [1e-6, 1e-6, 1e-6],\n            'h_q': 0.1, 'N': 200\n        },\n        { # Test Case 3\n            'delta_t': 0.1, 'R1': 0.005, 'C1': 2000, 'R2': 0.02, 'C2': 1500,\n            'Q_diag': [1e-7, 1e-5, 1e-5], 'R': 1.0, 'P0_diag': [1e-2, 1e-2, 1e-2],\n            'h_q': 0.1, 'N': 200\n        },\n        { # Test Case 4\n            'delta_t': 0.1, 'R1': 0.0005, 'C1': 200000, 'R2': 0.001, 'C2': 150000,\n            'Q_diag': [1e-9, 1e-9, 1e-9], 'R': 1e-4, 'P0_diag': [1e-3, 1e-3, 1e-3],\n            'h_q': 0.05, 'N': 500\n        }\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        # Run with single precision (float32)\n        P_n_32, max_symm_dev_32, neg_eig_count_32 = run_simulation(case, np.float32)\n\n        # Run with double precision (float64)\n        P_n_64, max_symm_dev_64, _ = run_simulation(case, np.float64)\n\n        # Calculate metrics\n        m1 = float(max_symm_dev_32)\n        m2 = float(max_symm_dev_64)\n        m3 = int(neg_eig_count_32)\n        \n        # Ensure difference is computed in higher precision\n        diff_norm = np.linalg.norm(P_n_64 - P_n_32.astype(np.float64), 'fro')\n        m4 = float(diff_norm)\n\n        all_results.append([m1, m2, m3, m4])\n\n    # Format the final output string exactly as required\n    # str() on a list creates the desired '[...]' representation\n    result_str = f\"[{','.join(map(str, all_results))}]\"\n    print(result_str.replace(\" \", \"\"))\n\nsolve()\n```"
        }
    ]
}