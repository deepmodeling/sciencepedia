{
    "hands_on_practices": [
        {
            "introduction": "Raw tomographic data is often compromised by artifacts that must be addressed to ensure quantitative accuracy. One of the most common issues in laboratory X-ray systems is beam hardening, which arises from the polychromatic (multi-energy) nature of the source. This practice guides you through modeling this phenomenon from first principles, simulating the non-linear attenuation that causes artifacts, and implementing a polynomial correction—a critical skill for anyone working with quantitative CT data. You will see firsthand how such corrections can mitigate the \"cupping\" artifact in a uniform phantom, turning a distorted measurement into a reliable one. ",
            "id": "3891025",
            "problem": "You are given a physically grounded model of X-ray tomography for reconstructing three-dimensional electrode microstructures. The beam is polychromatic and obeys Beer–Lambert attenuation. Beam hardening arises because lower-energy photons are attenuated more strongly, leading to a non-linear mapping from material thickness to measured line integral. This non-linearity induces cupping artifacts in reconstructions of uniform calibration phantoms. Your task is to derive a polynomial beam hardening correction function by fitting measured attenuation through step wedges of known thicknesses and apply it to mitigate cupping in a uniform cylindrical phantom. You must implement the model and correction as a complete, runnable program, and produce a single-line output as specified below.\n\nFundamental base:\n- Beer–Lambert law for a monochromatic beam: for a given energy $E$ and path length $L$ through a material with attenuation coefficient $\\mu(E)$, the transmitted intensity is $I(E;L) = I_{0}(E)\\,\\exp\\!\\left(-\\mu(E)\\,L\\right)$.\n- For a polychromatic beam with incident spectrum $S(E)$ after filtration, the measured intensity is $$I(L) = \\int_{E_{\\min}}^{E_{\\max}} S(E)\\,\\exp\\!\\left(-\\mu(E)\\,L\\right)\\,\\mathrm{d}E,$$ and the incident intensity is $$I_{0} = \\int_{E_{\\min}}^{E_{\\max}} S(E)\\,\\mathrm{d}E.$$\n- The measured line integral is defined as $$m(L) = -\\ln\\!\\left(\\frac{I(L)}{I_{0}}\\right).$$\n\nCore definitions and modeling assumptions:\n- The material attenuation coefficient is modeled as $$\\mu(E) = \\alpha\\,E^{-3} + \\beta,$$ which captures photoelectric absorption ($\\propto E^{-3}$) and a near-constant Compton scatter term. The energy $E$ is expressed in kiloelectronvolts (keV), and the attenuation $\\mu(E)$ is in inverse millimeters ($\\mathrm{mm}^{-1}$), with path length $L$ in millimeters ($\\mathrm{mm}$).\n- The incident spectrum after filtration is modeled as $$S(E) = E^{p}\\,\\exp\\!\\left(-\\delta\\,E^{-3}\\,T_{\\mathrm{f}}\\right),$$ where $p$ is a non-negative real exponent parameterizing the source spectrum shape, $\\delta$ modulates energy-dependent filtration, and $T_{\\mathrm{f}}$ is the filter thickness in millimeters ($\\mathrm{mm}$). This filtration term reduces low-energy content to emulate real beam hardening mitigation.\n- A polynomial correction function $q(m)$ of degree $d$ is fit from step wedge measurements $\\{(L_{i},m(L_{i}))\\}$ to approximate the inverse mapping $L \\approx q(m)$. The coefficients are determined by least squares, minimizing $$\\sum_{i} \\left(q\\!\\left(m(L_{i})\\right) - L_{i}\\right)^{2}.$$\n- A uniform cylindrical calibration phantom has radius $R$ (in millimeters), with true monochromatic reference attenuation $\\mu_{\\mathrm{ref}}$ defined at the effective energy $$E_{\\mathrm{eff}} = \\frac{\\int_{E_{\\min}}^{E_{\\max}} E\\,S(E)\\,\\mathrm{d}E}{\\int_{E_{\\min}}^{E_{\\max}} S(E)\\,\\mathrm{d}E},\\quad \\mu_{\\mathrm{ref}} = \\mu(E_{\\mathrm{eff}}).$$\n- For a chord through the phantom at a radial position $r$ (in millimeters), the path length is $$L(r) = 2\\,\\sqrt{R^{2} - r^{2}}.$$ In a naive reconstruction that ignores beam hardening, an apparent attenuation may be estimated as $$\\mu_{\\mathrm{naive}}(r) = \\frac{m\\!\\left(L(r)\\right)}{L(r)}.$$ The polynomial beam hardening correction maps the measured line integral $m\\!\\left(L(r)\\right)$ to an estimated thickness $L_{\\mathrm{est}}(r) \\approx q\\!\\left(m\\!\\left(L(r)\\right)\\right)$, from which a corrected attenuation estimate is $$\\mu_{\\mathrm{corr}}(r) = \\mu_{\\mathrm{ref}}\\,\\frac{L_{\\mathrm{est}}(r)}{L(r)}.$$\n\nCupping metric:\n- Define the cupping magnitude at two representative radial positions, the center $r=0$ and an edge fraction $r = \\rho\\,R$ with $0 < \\rho < 1$, by the unitless decimal quantities $$C_{\\mathrm{before}} = \\frac{\\mu_{\\mathrm{naive}}(\\rho R) - \\mu_{\\mathrm{naive}}(0)}{\\mu_{\\mathrm{ref}}},\\quad C_{\\mathrm{after}} = \\frac{\\mu_{\\mathrm{corr}}(\\rho R) - \\mu_{\\mathrm{corr}}(0)}{\\mu_{\\mathrm{ref}}}.$$ These are dimensionless ratios (express answers as decimals without any percent sign).\n\nAlgorithm to implement:\n- For each test case, construct a uniform energy grid over $[E_{\\min},E_{\\max}]$ and numerically integrate using a stable quadrature (for example, the trapezoidal rule) to obtain $I_{0}$ and $I(L)$ values. Compute $m(L)$ for wedge thicknesses $\\{L_{i}\\}$ and fit a polynomial $q$ that maps $m$ to $L$ via least squares. Compute $E_{\\mathrm{eff}}$ and $\\mu_{\\mathrm{ref}}$. For the phantom, evaluate $L(0)$ and $L(\\rho R)$, compute $\\mu_{\\mathrm{naive}}$ and $\\mu_{\\mathrm{corr}}$ at these two radii, and finally compute $C_{\\mathrm{before}}$ and $C_{\\mathrm{after}}$.\n\nPhysical units:\n- Energies must be treated in kiloelectronvolts (keV).\n- Path lengths and radii must be treated in millimeters ($\\mathrm{mm}$).\n- Attenuations must be treated in inverse millimeters ($\\mathrm{mm}^{-1}$).\n- The output cupping magnitudes are dimensionless and must be expressed as decimals.\n\nTest suite:\n- Case $1$ (baseline beam hardening): $E_{\\min} = 20$ $\\mathrm{keV}$, $E_{\\max} = 80$ $\\mathrm{keV}$, $p = 2$, $\\alpha = 80$ $\\mathrm{mm}^{-1}\\,\\mathrm{keV}^{3}$, $\\beta = 0.015$ $\\mathrm{mm}^{-1}$, $\\delta = 8000$ $\\mathrm{mm}^{-1}\\,\\mathrm{keV}^{3}$, $T_{\\mathrm{f}} = 1.0$ $\\mathrm{mm}$, $R = 5.0$ $\\mathrm{mm}$, wedge thicknesses $\\{0,1,2,4,8\\}$ $\\mathrm{mm}$, polynomial degree $d=3$, edge fraction $\\rho=0.9$.\n- Case $2$ (heavy filtration, reduced beam hardening): $E_{\\min} = 20$ $\\mathrm{keV}$, $E_{\\max} = 80$ $\\mathrm{keV}$, $p = 2$, $\\alpha = 80$ $\\mathrm{mm}^{-1}\\,\\mathrm{keV}^{3}$, $\\beta = 0.015$ $\\mathrm{mm}^{-1}$, $\\delta = 20000$ $\\mathrm{mm}^{-1}\\,\\mathrm{keV}^{3}$, $T_{\\mathrm{f}} = 2.0$ $\\mathrm{mm}$, $R = 5.0$ $\\mathrm{mm}$, wedge thicknesses $\\{0,1,2,4,8\\}$ $\\mathrm{mm}$, polynomial degree $d=3$, edge fraction $\\rho=0.9$.\n- Case $3$ (minimal wedge steps, quadratic fit): $E_{\\min} = 40$ $\\mathrm{keV}$, $E_{\\max} = 80$ $\\mathrm{keV}$, $p = 2$, $\\alpha = 80$ $\\mathrm{mm}^{-1}\\,\\mathrm{keV}^{3}$, $\\beta = 0.015$ $\\mathrm{mm}^{-1}$, $\\delta = 15000$ $\\mathrm{mm}^{-1}\\,\\mathrm{keV}^{3}$, $T_{\\mathrm{f}} = 1.5$ $\\mathrm{mm}$, $R = 5.0$ $\\mathrm{mm}$, wedge thicknesses $\\{0,2,8\\}$ $\\mathrm{mm}$, polynomial degree $d=2$, edge fraction $\\rho=0.9$.\n- Case $4$ (no filtration, strong beam hardening): $E_{\\min} = 20$ $\\mathrm{keV}$, $E_{\\max} = 80$ $\\mathrm{keV}$, $p = 3$, $\\alpha = 200$ $\\mathrm{mm}^{-1}\\,\\mathrm{keV}^{3}$, $\\beta = 0.010$ $\\mathrm{mm}^{-1}$, $\\delta = 0$ $\\mathrm{mm}^{-1}\\,\\mathrm{keV}^{3}$, $T_{\\mathrm{f}} = 0.0$ $\\mathrm{mm}$, $R = 5.0$ $\\mathrm{mm}$, wedge thicknesses $\\{0,1,2,3,4,5,6,7,8,9,10\\}$ $\\mathrm{mm}$, polynomial degree $d=4$, edge fraction $\\rho=0.9$.\n\nRequired final output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case contributes a two-element list $[C_{\\mathrm{before}},C_{\\mathrm{after}}]$. The final output must be a list of these lists, for example, $$\\big[\\,[c_{1,\\mathrm{before}},c_{1,\\mathrm{after}}],[c_{2,\\mathrm{before}},c_{2,\\mathrm{after}}],\\dots\\,\\big].$$",
            "solution": "The solution requires implementing a numerical model of X-ray transmission through a material to quantify and correct for beam hardening artifacts. The process for each test case is as follows.\n\n**1. Numerical Framework for Integration**\nThe core of the problem involves computing integrals of the form $\\int_{E_{\\min}}^{E_{\\max}} f(E)\\,\\mathrm{d}E$. These integrals do not have closed-form analytical solutions due to the complexity of the integrands. They must be approximated numerically. A stable and common method is the trapezoidal rule. We establish a discrete energy grid $E_j$ spanning the range $[E_{\\min}, E_{\\max}]$ with a small step size $\\Delta E$. For a function $f(E)$, the integral is approximated as:\n$$ \\int_{E_{\\min}}^{E_{\\max}} f(E)\\,\\mathrm{d}E \\approx \\sum_{j=1}^{N-1} \\frac{f(E_j) + f(E_{j+1})}{2} \\Delta E $$\nwhere $N$ is the number of points in the energy grid. A sufficiently large $N$ (e.g., $N=4096$) will ensure accurate results.\n\n**2. Modeling the Physical System**\nThe provided physical models are implemented.\n- The energy-dependent attenuation coefficient is: $\\mu(E) = \\alpha\\,E^{-3} + \\beta$.\n- The polychromatic spectrum is: $S(E) = E^{p}\\,\\exp\\!\\left(-\\delta\\,E^{-3}\\,T_{\\mathrm{f}}\\right)$. At energies near zero, $E^{-3}$ diverges; however, the integration range $[E_{\\min}, E_{\\max}]$ with $E_{\\min}>0$ avoids this singularity.\n\nUsing the numerical integration framework, we can compute the measured intensity $I(L)$ for a given path length $L$:\n$$I(L) = \\int_{E_{\\min}}^{E_{\\max}} S(E)\\,\\exp\\!\\left(-\\left[\\alpha\\,E^{-3} + \\beta\\right]\\,L\\right)\\,\\mathrm{d}E$$\nThe incident intensity $I_0$ is simply $I(L)$ at $L=0$:\n$$I_0 = I(0) = \\int_{E_{\\min}}^{E_{\\max}} S(E)\\,\\mathrm{d}E$$\nThe measured line integral, which represents the raw data in tomography, is then:\n$$m(L) = -\\ln\\!\\left(\\frac{I(L)}{I_{0}}\\right)$$\nThis function $m(L)$ is non-linear due to the polychromatic nature of the beam, which is the source of the beam hardening artifact.\n\n**3. Beam Hardening Correction via Polynomial Fitting**\nTo correct for the non-linearity, a calibration step is performed using a step wedge with known thicknesses $\\{L_i\\}$.\nFor each thickness $L_i$, the corresponding measurement $m(L_i)$ is calculated using the model above. This generates a set of data points $\\{(m(L_i), L_i)\\}$.\nWe seek a polynomial function $q(m)$ of degree $d$,\n$$q(m) = c_d m^d + c_{d-1} m^{d-1} + \\dots + c_1 m + c_0$$\nthat provides an estimate of the true thickness $L$ from a given measurement $m$. The coefficients $\\{c_j\\}$ are determined by solving the linear least squares problem:\n$$ \\min_{\\{c_j\\}} \\sum_{i} \\left( q(m(L_i)) - L_i \\right)^2 $$\nThis is a standard procedure readily available in numerical libraries.\n\n**4. Analysis of the Cylindrical Phantom**\nWith the correction function $q(m)$ established, we analyze a uniform cylindrical phantom of radius $R$.\nFirst, the reference attenuation $\\mu_{\\mathrm{ref}}$ is calculated. This requires the effective energy $E_{\\mathrm{eff}}$ of the beam:\n$$ E_{\\mathrm{eff}} = \\frac{\\int_{E_{\\min}}^{E_{\\max}} E\\,S(E)\\,\\mathrm{d}E}{\\int_{E_{\\min}}^{E_{\\max}} S(E)\\,\\mathrm{d}E} = \\frac{\\int E\\,S(E)\\,\\mathrm{d}E}{I_0} $$\nThe reference attenuation is the monochromatic attenuation at this energy:\n$$ \\mu_{\\mathrm{ref}} = \\mu(E_{\\mathrm{eff}}) = \\alpha\\,E_{\\mathrm{eff}}^{-3} + \\beta $$\n\nNext, we evaluate the phantom at two radial positions: the center ($r_1=0$) and the edge ($r_2=\\rho R$). The path lengths are:\n$$ L_1 = L(0) = 2R $$\n$$ L_2 = L(\\rho R) = 2\\sqrt{R^2 - (\\rho R)^2} = 2R\\sqrt{1-\\rho^2} $$\nFor each path length $L_k$, we compute the corresponding measurement $m_k = m(L_k)$.\n\n**5. Cupping Metric Calculation**\nThe cupping artifact is quantified before and after correction.\n- **Before Correction**: The naive attenuation is calculated assuming a linear relationship, $\\mu = m/L$:\n  $$ \\mu_{\\mathrm{naive}}(r_k) = \\frac{m_k}{L_k} $$\n  The pre-correction cupping metric is the normalized difference in this naive attenuation:\n  $$ C_{\\mathrm{before}} = \\frac{\\mu_{\\mathrm{naive}}(r_2) - \\mu_{\\mathrm{naive}}(r_1)}{\\mu_{\\mathrm{ref}}} $$\n\n- **After Correction**: The polynomial function $q(m)$ is used to obtain a corrected estimate of the path length:\n  $$ L_{\\mathrm{est}}(r_k) = q(m_k) $$\n  The corrected attenuation is then scaled by the reference value:\n  $$ \\mu_{\\mathrm{corr}}(r_k) = \\mu_{\\mathrm{ref}} \\frac{L_{\\mathrm{est}}(r_k)}{L_k} $$\n  If the correction is perfect, $L_{\\mathrm{est}}(r_k) \\approx L_k$, and thus $\\mu_{\\mathrm{corr}}(r_k) \\approx \\mu_{\\mathrm{ref}}$ for all $r_k$, eliminating the artifact. The post-correction cupping metric is:\n  $$ C_{\\mathrm{after}} = \\frac{\\mu_{\\mathrm{corr}}(r_2) - \\mu_{\\mathrm{corr}}(r_1)}{\\mu_{\\mathrm{ref}}} $$\n\nThe implementation will execute this entire sequence for each test case provided, collating the resulting $[C_{\\mathrm{before}}, C_{\\mathrm{after}}]$ pairs into a final list.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.integrate import trapezoid\n\ndef solve():\n    \"\"\"\n    Main function to run the beam hardening simulation for all test cases.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (baseline beam hardening)\n        {'E_min': 20.0, 'E_max': 80.0, 'p': 2.0, 'alpha': 80.0, 'beta': 0.015,\n         'delta': 8000.0, 'T_f': 1.0, 'R': 5.0,\n         'wedge_thicknesses': np.array([0.0, 1.0, 2.0, 4.0, 8.0]),\n         'd': 3, 'rho': 0.9},\n        # Case 2 (heavy filtration, reduced beam hardening)\n        {'E_min': 20.0, 'E_max': 80.0, 'p': 2.0, 'alpha': 80.0, 'beta': 0.015,\n         'delta': 20000.0, 'T_f': 2.0, 'R': 5.0,\n         'wedge_thicknesses': np.array([0.0, 1.0, 2.0, 4.0, 8.0]),\n         'd': 3, 'rho': 0.9},\n        # Case 3 (minimal wedge steps, quadratic fit)\n        {'E_min': 40.0, 'E_max': 80.0, 'p': 2.0, 'alpha': 80.0, 'beta': 0.015,\n         'delta': 15000.0, 'T_f': 1.5, 'R': 5.0,\n         'wedge_thicknesses': np.array([0.0, 2.0, 8.0]),\n         'd': 2, 'rho': 0.9},\n        # Case 4 (no filtration, strong beam hardening)\n        {'E_min': 20.0, 'E_max': 80.0, 'p': 3.0, 'alpha': 200.0, 'beta': 0.010,\n         'delta': 0.0, 'T_f': 0.0, 'R': 5.0,\n         'wedge_thicknesses': np.arange(0.0, 11.0, 1.0),\n         'd': 4, 'rho': 0.9},\n    ]\n\n    results = []\n    for params in test_cases:\n        result = run_simulation(**params)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join([str(res) for res in results])}]\")\n\ndef run_simulation(E_min, E_max, p, alpha, beta, delta, T_f, R, wedge_thicknesses, d, rho):\n    \"\"\"\n    Performs the full simulation for a single set of parameters.\n    \"\"\"\n    # 1. Numerical Framework and Physical Models\n    num_energy_steps = 4096\n    energies = np.linspace(E_min, E_max, num_energy_steps, dtype=np.float64)\n\n    # Avoid division by zero at E=0, though problem statement ensures E_min > 0.\n    # Add a small epsilon to be safe, which has no effect for E_min > 0.\n    energies_safe = energies + 1e-12\n\n    # Attenuation coefficient mu(E)\n    mu_E = alpha * energies_safe**(-3) + beta\n\n    # Incident spectrum S(E)\n    # Handle the case where delta is zero to avoid multiplying by a large number and then zero.\n    if delta == 0.0 or T_f == 0.0:\n        S_E = energies**p\n    else:\n        S_E = energies**p * np.exp(-delta * energies_safe**(-3) * T_f)\n\n    # 2. Function to compute measured line integral m(L)\n    I0 = trapezoid(S_E, energies)\n    \n    def get_m(L_val):\n        if L_val == 0.0:\n            return 0.0\n        integrand = S_E * np.exp(-mu_E * L_val)\n        I_L = trapezoid(integrand, energies)\n        # Handle potential numerical issue where I_L/I0 > 1\n        ratio = max(1e-300, I_L / I0)\n        return -np.log(ratio)\n\n    # 3. Beam Hardening Correction Function (Polynomial Fitting)\n    m_values = np.array([get_m(L) for L in wedge_thicknesses])\n    \n    # Fit polynomial L = q(m)\n    coeffs = np.polyfit(m_values, wedge_thicknesses, d)\n    q_poly = np.poly1d(coeffs)\n\n    # 4. Analysis of Cylindrical Phantom\n    # Reference attenuation\n    E_eff_numerator = trapezoid(energies * S_E, energies)\n    E_eff = E_eff_numerator / I0\n    mu_ref = alpha * E_eff**(-3) + beta\n\n    # Path lengths at center (r=0) and edge (r=rho*R)\n    L_center = 2 * R\n    L_edge = 2 * np.sqrt(R**2 - (rho * R)**2)\n\n    # Corresponding measurements\n    m_center = get_m(L_center)\n    m_edge = get_m(L_edge)\n\n    # 5. Cupping Metric Calculation\n    # Before correction\n    mu_naive_center = m_center / L_center if L_center > 0 else 0.0\n    mu_naive_edge = m_edge / L_edge if L_edge > 0 else 0.0\n    C_before = (mu_naive_edge - mu_naive_center) / mu_ref\n\n    # After correction\n    L_est_center = q_poly(m_center)\n    L_est_edge = q_poly(m_edge)\n    \n    mu_corr_center = mu_ref * (L_est_center / L_center) if L_center > 0 else mu_ref\n    mu_corr_edge = mu_ref * (L_est_edge / L_edge) if L_edge > 0 else mu_ref\n    C_after = (mu_corr_edge - mu_corr_center) / mu_ref\n\n    return [C_before, C_after]\n\nsolve()\n```"
        },
        {
            "introduction": "Once projection data is acquired and corrected, the next step is to reconstruct the 3D volume. While analytical methods like Filtered Backprojection are common, iterative algorithms such as the Simultaneous Algebraic Reconstruction Technique (SART) offer superior results, especially with noisy or incomplete data. This exercise demystifies the reconstruction \"black box\" by having you implement a single SART update step, providing a concrete understanding of how the algorithm iteratively refines a voxel-based solution to match the measured projections. ",
            "id": "3891022",
            "problem": "Consider a discretized tomographic reconstruction problem arising in the analysis of battery electrode microstructures, where three-dimensional microstructure reconstruction from line-integral measurements is approximated on a voxel grid. Let there be a linear system defined by the forward model $y = A x$, where $A \\in \\mathbb{R}^{m \\times n}$ is the projection matrix whose entry $A_{ij} \\ge 0$ represents the intersection length of ray $i$ through voxel $j$, $y \\in \\mathbb{R}^{m}$ is the measured sinogram vector, and $x \\in \\mathbb{R}^{n}$ is the unknown voxel property vector (e.g., local attenuation or porosity surrogate). Assume that the measurements are modeled by the linear superposition of voxel contributions under small-contrast conditions derived from the Beer–Lambert law for X-ray tomography, and that we seek an iterative correction to $x$ that respects the geometry of ray-voxel interactions.\n\nImplement one iteration of the Simultaneous Algebraic Reconstruction Technique (SART) update for a given subset containing all rays, using a relaxation parameter $\\alpha \\in [0,1]$, row-normalization weights $r_i$ for each ray $i$ (typically $r_i = \\sum_{j=1}^{n} A_{ij}$), and column-normalization weights $c_j$ for each voxel $j$ (typically $c_j = \\sum_{i=1}^{m} A_{ij}$). The SART update to produce $x^{(1)}$ from an initial guess $x^{(0)}$ is defined component-wise, for each voxel index $j \\in \\{1, \\dots, n\\}$, by\n$$\nx^{(1)}_j = x^{(0)}_j + \\alpha \\cdot \\frac{\\sum_{i=1}^{m} A_{ij} \\cdot \\left( \\frac{y_i - \\sum_{k=1}^{n} A_{ik} x^{(0)}_k}{r_i} \\right)}{c_j},\n$$\nwith the following safeguards to ensure numerical well-posedness:\n- If $r_i = 0$, then the contribution of ray $i$ to all voxels is defined to be $0$ (i.e., skip that ray).\n- If $c_j = 0$, then the update for voxel $j$ is defined to be $0$ (i.e., $x^{(1)}_j = x^{(0)}_j$).\n\nAll quantities in this problem are dimensionless. The program must compute the updated voxel values and round each component of $x^{(1)}$ to six decimal places.\n\nYour program must implement the above SART update for the following test suite of parameter sets, each specified by $(A, y, x^{(0)}, \\alpha, r, c)$. For clarity, all arrays are given explicitly:\n\n- Test case $1$ (general case with nonzero row and column normalizations):\n  $$\n  A = \\begin{bmatrix}\n  0.8 & 0.1 & 0.0 & 0.3 \\\\\n  0.2 & 0.5 & 0.7 & 0.1 \\\\\n  0.0 & 0.4 & 0.6 & 0.0\n  \\end{bmatrix}, \\quad\n  y = \\begin{bmatrix} 1.2 \\\\ 1.8 \\\\ 0.9 \\end{bmatrix}, \\quad\n  x^{(0)} = \\begin{bmatrix} 0.5 \\\\ 0.5 \\\\ 0.5 \\\\ 0.5 \\end{bmatrix}, \\quad\n  \\alpha = 0.7,\n  $$\n  $$\n  r = \\begin{bmatrix} 1.2 \\\\ 1.5 \\\\ 1.0 \\end{bmatrix}, \\quad\n  c = \\begin{bmatrix} 1.0 \\\\ 1.0 \\\\ 1.3 \\\\ 0.4 \\end{bmatrix}.\n  $$\n\n- Test case $2$ (boundary condition with zero relaxation parameter):\n  $$\n  A = \\begin{bmatrix}\n  0.5 & 0.5 & 0.0 \\\\\n  0.0 & 0.3 & 0.7\n  \\end{bmatrix}, \\quad\n  y = \\begin{bmatrix} 0.9 \\\\ 0.7 \\end{bmatrix}, \\quad\n  x^{(0)} = \\begin{bmatrix} 0.2 \\\\ 0.4 \\\\ 0.6 \\end{bmatrix}, \\quad\n  \\alpha = 0.0,\n  $$\n  $$\n  r = \\begin{bmatrix} 1.0 \\\\ 1.0 \\end{bmatrix}, \\quad\n  c = \\begin{bmatrix} 0.5 \\\\ 0.8 \\\\ 0.7 \\end{bmatrix}.\n  $$\n\n- Test case $3$ (edge case with a ray of zero total intersection length):\n  $$\n  A = \\begin{bmatrix}\n  0.0 & 0.0 & 0.0 & 0.0 \\\\\n  0.4 & 0.0 & 0.2 & 0.0 \\\\\n  0.0 & 0.3 & 0.5 & 0.2\n  \\end{bmatrix}, \\quad\n  y = \\begin{bmatrix} 0.0 \\\\ 0.5 \\\\ 1.0 \\end{bmatrix}, \\quad\n  x^{(0)} = \\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix}, \\quad\n  \\alpha = 1.0,\n  $$\n  $$\n  r = \\begin{bmatrix} 0.0 \\\\ 0.6 \\\\ 1.0 \\end{bmatrix}, \\quad\n  c = \\begin{bmatrix} 0.4 \\\\ 0.3 \\\\ 0.7 \\\\ 0.2 \\end{bmatrix}.\n  $$\n\n- Test case $4$ (edge case with a voxel never intersected by any ray):\n  $$\n  A = \\begin{bmatrix}\n  0.2 & 0.0 & 0.3 \\\\\n  0.1 & 0.0 & 0.4\n  \\end{bmatrix}, \\quad\n  y = \\begin{bmatrix} 0.4 \\\\ 0.5 \\end{bmatrix}, \\quad\n  x^{(0)} = \\begin{bmatrix} 0.1 \\\\ 0.9 \\\\ 0.2 \\end{bmatrix}, \\quad\n  \\alpha = 0.5,\n  $$\n  $$\n  r = \\begin{bmatrix} 0.5 \\\\ 0.5 \\end{bmatrix}, \\quad\n  c = \\begin{bmatrix} 0.3 \\\\ 0.0 \\\\ 0.7 \\end{bmatrix}.\n  $$\n\n- Test case $5$ (balanced weights with multiple rays and voxels):\n  $$\n  A = \\begin{bmatrix}\n  0.3 & 0.7 \\\\\n  0.6 & 0.4 \\\\\n  0.5 & 0.5\n  \\end{bmatrix}, \\quad\n  y = \\begin{bmatrix} 1.0 \\\\ 0.8 \\\\ 0.9 \\end{bmatrix}, \\quad\n  x^{(0)} = \\begin{bmatrix} 0.3 \\\\ 0.6 \\end{bmatrix}, \\quad\n  \\alpha = 0.9,\n  $$\n  $$\n  r = \\begin{bmatrix} 1.0 \\\\ 1.0 \\\\ 1.0 \\end{bmatrix}, \\quad\n  c = \\begin{bmatrix} 1.4 \\\\ 1.6 \\end{bmatrix}.\n  $$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case result and is itself a bracket-enclosed, comma-separated list of the updated voxel values $x^{(1)}$ rounded to six decimal places, for example, $\\left[ [0.123456,0.234567], [\\dots], \\dots \\right]$. The output must have no additional spaces or text in the printed line.",
            "solution": "The problem is valid. All givens are provided and are self-consistent. The problem is scientifically grounded in the well-established field of tomographic reconstruction and poses a clear, objective, and solvable computational task. The use of the Simultaneous Algebraic Reconstruction Technique (SART) is standard practice for solving linear systems of the form $y = Ax$ that arise in such contexts.\n\nThe core of the problem is to implement a single iterative update for the unknown voxel property vector $x \\in \\mathbb{R}^{n}$, starting from an initial guess $x^{(0)}$. The update rule is given for each voxel component $j$ as:\n$$\nx^{(1)}_j = x^{(0)}_j + \\alpha \\cdot \\frac{\\sum_{i=1}^{m} A_{ij} \\cdot \\left( \\frac{y_i - \\sum_{k=1}^{n} A_{ik} x^{(0)}_k}{r_i} \\right)}{c_j}\n$$\nHere, $y \\in \\mathbb{R}^{m}$ is the measurement vector, $A \\in \\mathbb{R}^{m \\times n}$ is the projection matrix, $\\alpha$ is a relaxation parameter, and $r \\in \\mathbb{R}^{m}$ and $c \\in \\mathbb{R}^{n}$ are normalization vectors for rays and voxels, respectively.\n\nTo design an efficient algorithm, we can express this component-wise formula using vector and matrix operations. This approach is not only computationally faster, especially with libraries like NumPy, but also provides a clearer correspondence to the underlying physical and mathematical steps of the process: forward projection, residual calculation, back-projection, and update.\n\nThe algorithmic steps are as follows:\n1.  **Forward Projection**: First, we compute the \"simulated\" measurements based on the current voxel guess $x^{(0)}$. This corresponds to the inner sum $\\sum_{k=1}^{n} A_{ik} x^{(0)}_k$. In vector notation, this is the matrix-vector product $p^{(0)} = A x^{(0)}$.\n2.  **Residual Calculation**: We then find the difference between the actual measurements $y$ and the simulated measurements $p^{(0)}$. This difference, or residual, is $\\Delta y = y - p^{(0)}$.\n3.  **Weighted Residual Projection**: The residual for each ray $i$ is weighted by the inverse of the total intersection length for that ray, $1/r_i$. This term, $w_i = (y_i - p^{(0)}_i)/r_i$, represents a normalized error for each projection. We must handle the case where $r_i=0$ (a ray that intersects no voxels) by setting its contribution to $0$. Let $w$ be the vector of these weighted residuals.\n4.  **Back-Projection**: The weighted residuals are then \"back-projected\" onto the voxel grid. The term $\\sum_{i=1}^{m} A_{ij} w_i$ calculates the total contribution of all weighted ray errors to voxel $j$. This is equivalent to the matrix-vector product $b = A^T w$, where $A^T$ is the transpose of the projection matrix.\n5.  **Voxel Update Normalization**: The back-projected correction for each voxel $j$ is then normalized by the total intersection weight for that voxel, $c_j$. This gives the normalized update direction $d_j = b_j / c_j$. Again, we must handle the case where $c_j=0$ (a voxel that is never intersected by any ray) by setting its update to $0$. Let $d$ be the vector of these normalized updates.\n6.  **Final Voxel Update**: Finally, the initial voxel vector $x^{(0)}$ is updated by adding the scaled and relaxed correction term. The complete vector update is $x^{(1)} = x^{(0)} + \\alpha d$.\n\nThis vectorized sequence of operations—$p^{(0)} = A x^{(0)}$, $\\Delta y = y - p^{(0)}$, $w$ from $\\Delta y / r$, $b = A^T w$, $d$ from $b/c$, and $x^{(1)} = x^{(0)} + \\alpha d$—is a direct and robust implementation of the SART formula. The provided safeguards for division by zero are naturally incorporated by using conditional logic or masked array operations during the computation of $w$ and $d$. The final result is obtained by rounding the components of $x^{(1)}$ to six decimal places.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the SART iteration problem for a suite of test cases.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1\n        (\n            np.array([[0.8, 0.1, 0.0, 0.3], [0.2, 0.5, 0.7, 0.1], [0.0, 0.4, 0.6, 0.0]]),\n            np.array([1.2, 1.8, 0.9]),\n            np.array([0.5, 0.5, 0.5, 0.5]),\n            0.7,\n            np.array([1.2, 1.5, 1.0]),\n            np.array([1.0, 1.0, 1.3, 0.4])\n        ),\n        # Test case 2\n        (\n            np.array([[0.5, 0.5, 0.0], [0.0, 0.3, 0.7]]),\n            np.array([0.9, 0.7]),\n            np.array([0.2, 0.4, 0.6]),\n            0.0,\n            np.array([1.0, 1.0]),\n            np.array([0.5, 0.8, 0.7])\n        ),\n        # Test case 3\n        (\n            np.array([[0.0, 0.0, 0.0, 0.0], [0.4, 0.0, 0.2, 0.0], [0.0, 0.3, 0.5, 0.2]]),\n            np.array([0.0, 0.5, 1.0]),\n            np.array([0.0, 0.0, 0.0, 0.0]),\n            1.0,\n            np.array([0.0, 0.6, 1.0]),\n            np.array([0.4, 0.3, 0.7, 0.2])\n        ),\n        # Test case 4\n        (\n            np.array([[0.2, 0.0, 0.3], [0.1, 0.0, 0.4]]),\n            np.array([0.4, 0.5]),\n            np.array([0.1, 0.9, 0.2]),\n            0.5,\n            np.array([0.5, 0.5]),\n            np.array([0.3, 0.0, 0.7])\n        ),\n        # Test case 5\n        (\n            np.array([[0.3, 0.7], [0.6, 0.4], [0.5, 0.5]]),\n            np.array([1.0, 0.8, 0.9]),\n            np.array([0.3, 0.6]),\n            0.9,\n            np.array([1.0, 1.0, 1.0]),\n            np.array([1.4, 1.6])\n        )\n    ]\n    \n    formatted_results = []\n\n    for A, y, x0, alpha, r, c in test_cases:\n        # Step 1: Forward Projection\n        # p0 = A @ x0\n        p0 = A.dot(x0)\n        \n        # Step 2: Residual Calculation\n        # delta_y = y - p0\n        delta_y = y - p0\n        \n        # Step 3: Weighted Residual Projection (with safeguard for r_i = 0)\n        # w = (y - p0) / r\n        w = np.divide(delta_y, r, out=np.zeros_like(y, dtype=float), where=r!=0)\n\n        # Step 4: Back-Projection\n        # b = A^T @ w\n        b = A.T.dot(w)\n        \n        # Step 5: Voxel Update Normalization (with safeguard for c_j = 0)\n        # d = b / c\n        d = np.divide(b, c, out=np.zeros_like(x0, dtype=float), where=c!=0)\n        \n        # Step 6: Final Voxel Update\n        # x1 = x0 + alpha * d\n        x1 = x0 + alpha * d\n        \n        # Round to six decimal places as specified\n        x1_rounded = np.round(x1, 6)\n        \n        # Format the result for the specific output string requirement\n        # Ensures trailing zeros to meet 6 decimal places format\n        per_case_result = \",\".join([f\"{val:.6f}\" for val in x1_rounded])\n        formatted_results.append(f\"[{per_case_result}]\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A reconstructed 3D volume is initially just a map of arbitrary grayscale values. To unlock its full potential for scientific analysis, these values must be converted into meaningful physical properties. This practice addresses this crucial final step by demonstrating how to perform a two-point calibration, mapping the reconstructed intensities to the linear attenuation coefficient, $\\mu$. By using reference materials like air and a known standard, you will derive a linear transformation that makes the entire dataset quantitative and physically interpretable. ",
            "id": "3891003",
            "problem": "A laboratory X-ray Computed Tomography (XCT) system reconstructs three-dimensional volumes of lithium-ion battery electrodes. At a fixed tube potential, after standard pre-processing to mitigate beam-hardening and ring artifacts, the system’s reconstruction algorithm (Filtered Backprojection, FBP) produces voxel intensities that are linearly related to the local linear attenuation coefficient. The forward model for X-ray transmission is given by the Beer–Lambert law: for a ray traversing a sample, the transmitted intensity satisfies $I = I_{0} \\exp\\!\\left(-\\int \\mu(\\mathbf{r})\\, ds\\right)$, where $I_{0}$ is the incident intensity and $\\mu(\\mathbf{r})$ is the position-dependent linear attenuation coefficient. The reconstruction recovers the attenuation field up to an unknown affine transformation due to detector gains and offsets.\n\nA calibration is performed using two homogeneous regions acquired under the same effective energy: (i) air, whose attenuation is negligible at the operating energy so it can be taken as $\\,\\mu_{\\text{air}} \\approx 0\\,$, and (ii) a liquid reference material with known attenuation coefficient $\\,\\mu_{\\text{ref}}\\,$. The mean reconstructed grayscale values in the calibration regions are measured as $\\,g_{\\text{air}} = 120\\,$ and $\\,g_{\\text{ref}} = 1120\\,$. The known attenuation of the reference material at the effective beam energy is $\\,\\mu_{\\text{ref}} = 19.2\\,\\text{m}^{-1}\\,$.\n\nIn a reconstructed cathode subvolume, segmentation yields phase-wise mean grayscale values: electrolyte-filled pore $\\,g_{\\text{el}} = 520\\,$, polymer binder $\\,g_{\\text{bi}} = 1620\\,$, active material $\\,g_{\\text{am}} = 3120\\,$, and conductive carbon $\\,g_{\\text{c}} = 920\\,$. Assume the reconstruction’s intensity-to-attenuation relationship is an unknown affine map that is common across the entire dynamic range of interest, and that the calibration data apply directly (same effective spectrum, geometry, and reconstruction settings).\n\nStarting from the Beer–Lambert law and the linearity of the reconstruction, derive the mapping from reconstructed grayscale $\\,g\\,$ to linear attenuation coefficient $\\,\\mu(g)\\,$ using the air and reference material calibrations. Then, compute the attenuation coefficients for the four electrode phases listed. Express each final value in $\\text{m}^{-1}$ and round to four significant figures. Present the four phase values in the order $\\left[\\mu_{\\text{el}},\\, \\mu_{\\text{bi}},\\, \\mu_{\\text{am}},\\, \\mu_{\\text{c}}\\right]$.",
            "solution": "The problem states that the relationship between the linear attenuation coefficient $\\mu$ and the reconstructed grayscale value $g$ is an affine map. This can be expressed as a linear equation:\n$$ \\mu(g) = A \\cdot g + B $$\nwhere $A$ is the slope (scaling factor) and $B$ is the intercept (offset) of the transformation. Our goal is to determine the constants $A$ and $B$ using the two provided calibration points.\n\nThe two calibration points are $(\\mu_{\\text{air}}, g_{\\text{air}})$ and $(\\mu_{\\text{ref}}, g_{\\text{ref}})$. We can substitute these into the affine equation to form a system of two linear equations:\n1. $\\mu_{\\text{air}} = A \\cdot g_{\\text{air}} + B$\n2. $\\mu_{\\text{ref}} = A \\cdot g_{\\text{ref}} + B$\n\nSubstituting the given numerical values:\n1. $0 = A \\cdot 120 + B$\n2. $19.2 = A \\cdot 1120 + B$\n\nWe can solve this system for $A$ and $B$. First, we solve for the slope $A$ by subtracting equation (1) from equation (2):\n$$ \\mu_{\\text{ref}} - \\mu_{\\text{air}} = (A \\cdot g_{\\text{ref}} + B) - (A \\cdot g_{\\text{air}} + B) $$\n$$ \\mu_{\\text{ref}} - \\mu_{\\text{air}} = A \\cdot (g_{\\text{ref}} - g_{\\text{air}}) $$\n$$ A = \\frac{\\mu_{\\text{ref}} - \\mu_{\\text{air}}}{g_{\\text{ref}} - g_{\\text{air}}} $$\nSubstituting the numerical values for the calibration points:\n$$ A = \\frac{19.2 - 0}{1120 - 120} = \\frac{19.2}{1000} = 0.0192 $$\nThe units of $A$ are $\\text{m}^{-1}$ per grayscale unit.\n\nNext, we solve for the intercept $B$ by rearranging equation (1):\n$$ B = -A \\cdot g_{\\text{air}} $$\nSubstituting the values of $A$ and $g_{\\text{air}}$:\n$$ B = -(0.0192) \\cdot 120 = -2.304 $$\nThe units of $B$ are $\\text{m}^{-1}$.\n\nThus, the derived affine mapping from grayscale $g$ to attenuation coefficient $\\mu$ is:\n$$ \\mu(g) = 0.0192 \\cdot g - 2.304 $$\n\nNow, we use this mapping to compute the attenuation coefficients for the four specified electrode phases. All results will be reported in $\\text{m}^{-1}$ and rounded to four significant figures.\n\n1.  **Electrolyte-filled pore ($\\mu_{\\text{el}}$):** with $g_{\\text{el}} = 520$.\n    $$ \\mu_{\\text{el}} = (0.0192 \\cdot 520) - 2.304 = 9.984 - 2.304 = 7.68 $$\n    Rounding to four significant figures gives $7.680$.\n\n2.  **Polymer binder ($\\mu_{\\text{bi}}$):** with $g_{\\text{bi}} = 1620$.\n    $$ \\mu_{\\text{bi}} = (0.0192 \\cdot 1620) - 2.304 = 31.104 - 2.304 = 28.8 $$\n    Rounding to four significant figures gives $28.80$.\n\n3.  **Active material ($\\mu_{\\text{am}}$):** with $g_{\\text{am}} = 3120$.\n    $$ \\mu_{\\text{am}} = (0.0192 \\cdot 3120) - 2.304 = 59.904 - 2.304 = 57.6 $$\n    Rounding to four significant figures gives $57.60$.\n\n4.  **Conductive carbon ($\\mu_{\\text{c}}$):** with $g_{\\text{c}} = 920$.\n    $$ \\mu_{\\text{c}} = (0.0192 \\cdot 920) - 2.304 = 17.664 - 2.304 = 15.36 $$\n    This value already has four significant figures.\n\nThe calculated attenuation coefficients for the four phases, rounded to four significant figures, are: $\\mu_{\\text{el}} = 7.680\\,\\text{m}^{-1}$, $\\mu_{\\text{bi}} = 28.80\\,\\text{m}^{-1}$, $\\mu_{\\text{am}} = 57.60\\,\\text{m}^{-1}$, and $\\mu_{\\text{c}} = 15.36\\,\\text{m}^{-1}$.",
            "answer": "$$\\boxed{\\begin{pmatrix} 7.680 & 28.80 & 57.60 & 15.36 \\end{pmatrix}}$$"
        }
    ]
}