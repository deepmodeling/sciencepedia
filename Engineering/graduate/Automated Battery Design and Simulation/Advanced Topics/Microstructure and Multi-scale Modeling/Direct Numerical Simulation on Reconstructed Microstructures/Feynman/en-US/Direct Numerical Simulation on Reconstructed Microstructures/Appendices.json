{
    "hands_on_practices": [
        {
            "introduction": "The first step in performing DNS on a real microstructure is to convert a grayscale image, often from X-ray tomography, into a distinct digital representation of material phases. This process, known as segmentation, is critical as it defines the geometry for the simulation. This practice  will guide you through deriving and applying Otsu's method, an elegant and widely used algorithm that automates this process by finding an optimal threshold that maximizes the statistical variance between the resulting classes.",
            "id": "3907165",
            "problem": "A grayscale microstructure image of a porous lithium-ion battery cathode reconstructed from X-ray Computed Tomography (XCT) is to be segmented into pore and solid phases to enable Direct Numerical Simulation (DNS) of ionic transport. Segmentation is performed by a single global threshold on the $8$-bit grayscale intensity, thereby assigning all voxels with intensity $\\leq t$ to the pore phase and all voxels with intensity $> t$ to the solid phase. The observed grayscale histogram is bimodal and has nonzero counts only at the following $8$ gray levels: $(g_i, n_i)$ pairs are $(30, 400)$, $(60, 800)$, $(90, 1200)$, $(120, 600)$, $(150, 300)$, $(180, 600)$, $(210, 900)$, $(240, 450)$, where $g_i$ is the gray level and $n_i$ the number of voxels at that level.\n\nStarting only from the definitions of probability, mean, and variance of a discrete distribution, and from the variance decomposition identity that the total variance can be written as the sum of within-class and between-class contributions, derive the criterion used in Otsu’s method for choosing the optimal threshold $t$ by maximizing the inter-class (between-class) variance of the two classes induced by thresholding. Then, apply the derived criterion to the provided histogram to determine the unique threshold $t^{\\star}$ among the candidate gray levels $t \\in \\{30, 60, 90, 120, 150, 180, 210\\}$ that maximizes the inter-class variance.\n\nExpress the final threshold as an integer gray level on the $8$-bit scale. No rounding is necessary.",
            "solution": "The task is to first derive the criterion for Otsu's method of image thresholding and then apply it to a given histogram to find the optimal threshold.\n\n### Part 1: Derivation of the Otsu's Method Criterion\n\nLet the discrete gray levels be denoted by $i$, where $i \\in \\{0, 1, \\dots, L-1\\}$. For an $8$-bit image, $L=256$. Let $n_i$ be the number of voxels with gray level $i$ in the image, and let $N$ be the total number of voxels, $N = \\sum_{i=0}^{L-1} n_i$.\n\nFrom the definition of probability for a discrete distribution, the probability of a voxel having gray level $i$ is:\n$$\np_i = \\frac{n_i}{N}\n$$\nThese probabilities satisfy $\\sum_{i=0}^{L-1} p_i = 1$.\n\nFrom the definition of the mean of a discrete distribution, the total mean gray level of the image is:\n$$\n\\mu_T = \\sum_{i=0}^{L-1} i \\cdot p_i\n$$\n\nA threshold at gray level $t$ partitions the voxels into two classes:\n- Class 1 ($C_1$): Pore phase, containing voxels with gray levels $\\{0, 1, \\dots, t\\}$.\n- Class 2 ($C_2$): Solid phase, containing voxels with gray levels $\\{t+1, \\dots, L-1\\}$.\n\nThe probability of a randomly selected voxel belonging to Class 1, $\\omega_1(t)$, is the sum of the probabilities of the gray levels in $C_1$:\n$$\n\\omega_1(t) = P(C_1) = \\sum_{i=0}^{t} p_i\n$$\nSimilarly, the probability of belonging to Class 2, $\\omega_2(t)$, is:\n$$\n\\omega_2(t) = P(C_2) = \\sum_{i=t+1}^{L-1} p_i\n$$\nIt follows that $\\omega_1(t) + \\omega_2(t) = 1$.\n\nThe mean gray level for each class is calculated as a conditional expectation:\n$$\n\\mu_1(t) = \\frac{\\sum_{i=0}^{t} i \\cdot p_i}{\\omega_1(t)} \\quad \\text{and} \\quad \\mu_2(t) = \\frac{\\sum_{i=t+1}^{L-1} i \\cdot p_i}{\\omega_2(t)}\n$$\nThe total mean $\\mu_T$ can be expressed as a weighted average of the class means: $\\mu_T = \\omega_1(t)\\mu_1(t) + \\omega_2(t)\\mu_2(t)$.\n\nThe problem states that we must use the variance decomposition identity, which expresses the total variance of the image's gray level distribution, $\\sigma_T^2$, as the sum of two components: the within-class variance, $\\sigma_W^2(t)$, and the between-class variance, $\\sigma_B^2(t)$.\n$$\n\\sigma_T^2 = \\sigma_W^2(t) + \\sigma_B^2(t)\n$$\nThe total variance $\\sigma_T^2 = \\sum_{i=0}^{L-1} (i - \\mu_T)^2 p_i$ is a constant for a given image and does not depend on the threshold $t$.\n\nThe within-class variance, $\\sigma_W^2(t)$, is the weighted average of the variances of each class: $\\sigma_W^2(t) = \\omega_1(t)\\sigma_1^2(t) + \\omega_2(t)\\sigma_2^2(t)$, where $\\sigma_1^2(t)$ and $\\sigma_2^2(t)$ are the variances within $C_1$ and $C_2$, respectively. It measures the homogeneity within the classes. A good segmentation should yield homogeneous classes, so the goal is to minimize $\\sigma_W^2(t)$.\n\nThe between-class variance, $\\sigma_B^2(t)$, is defined as:\n$$\n\\sigma_B^2(t) = \\omega_1(t)(\\mu_1(t) - \\mu_T)^2 + \\omega_2(t)(\\mu_2(t) - \\mu_T)^2\n$$\nThis measures the separability of the two classes.\n\nSince $\\sigma_T^2$ is constant, minimizing the within-class variance $\\sigma_W^2(t)$ is mathematically equivalent to maximizing the between-class variance $\\sigma_B^2(t)$. Otsu's method is defined by this principle. Therefore, the criterion for selecting the optimal threshold $t^\\star$ is to maximize the between-class variance:\n$$\nt^\\star = \\arg\\max_{t} \\left\\{ \\sigma_B^2(t) \\right\\}\n$$\nFor computational purposes, the expression for $\\sigma_B^2(t)$ can be simplified. Using $\\mu_T = \\omega_1\\mu_1 + \\omega_2\\mu_2$ and $\\omega_1+\\omega_2=1$:\n$$\n\\begin{align*} \\sigma_B^2(t) &= \\omega_1(\\mu_1 - (\\omega_1\\mu_1 + \\omega_2\\mu_2))^2 + \\omega_2(\\mu_2 - (\\omega_1\\mu_1 + \\omega_2\\mu_2))^2 \\\\ &= \\omega_1((1-\\omega_1)\\mu_1 - \\omega_2\\mu_2)^2 + \\omega_2(-\\omega_1\\mu_1 + (1-\\omega_2)\\mu_2)^2 \\\\ &= \\omega_1(\\omega_2\\mu_1 - \\omega_2\\mu_2)^2 + \\omega_2(-\\omega_1\\mu_1 + \\omega_1\\mu_2)^2 \\\\ &= \\omega_1\\omega_2^2(\\mu_1 - \\mu_2)^2 + \\omega_2\\omega_1^2(\\mu_1 - \\mu_2)^2 \\\\ &= (\\omega_1\\omega_2^2 + \\omega_2\\omega_1^2)(\\mu_1 - \\mu_2)^2 \\\\ &= \\omega_1\\omega_2(\\omega_2 + \\omega_1)(\\mu_1 - \\mu_2)^2 \\\\ &= \\omega_1(t)\\omega_2(t)(\\mu_1(t) - \\mu_2(t))^2 \\end{align*}\n$$\nThis completes the derivation of the criterion.\n\n### Part 2: Application to the Given Data\n\nThe provided histogram data is:\n$(g_i, n_i) \\in \\{(30, 400), (60, 800), (90, 1200), (120, 600), (150, 300), (180, 600), (210, 900), (240, 450)\\}$.\n\nFirst, we calculate the total number of voxels, $N$:\n$$\nN = 400 + 800 + 1200 + 600 + 300 + 600 + 900 + 450 = 5250\n$$\nThe total first moment (sum of $g_i \\cdot n_i$) is:\n$$\nM_T = (30)(400) + (60)(800) + (90)(1200) + (120)(600) + (150)(300) + (180)(600) + (210)(900) + (240)(450)\n$$\n$$\nM_T = 12000 + 48000 + 108000 + 72000 + 45000 + 108000 + 189000 + 108000 = 690000\n$$\nWe will test each candidate threshold $t \\in \\{30, 60, 90, 120, 150, 180, 210\\}$. For each $t$, we compute the necessary quantities for $C_1$ (gray levels $\\le t$) and $C_2$ (gray levels $> t$). Let $N_1(t)$ and $M_1(t)$ be the number of voxels and the first moment for $C_1$, respectively. Then $\\omega_1(t) = N_1(t)/N$, $\\mu_1(t) = M_1(t)/N_1(t)$, $\\omega_2(t) = (N - N_1(t))/N$, and $\\mu_2(t) = (M_T - M_1(t))/(N-N_1(t))$.\n\nThe cumulative sums of counts ($N_1$) and moments ($M_1$) are pre-calculated for efficiency:\n- $g_i \\le 30$: $N_1=400$, $M_1=12000$\n- $g_i \\le 60$: $N_1=1200$, $M_1=60000$\n- $g_i \\le 90$: $N_1=2400$, $M_1=168000$\n- $g_i \\le 120$: $N_1=3000$, $M_1=240000$\n- $g_i \\le 150$: $N_1=3300$, $M_1=285000$\n- $g_i \\le 180$: $N_1=3900$, $M_1=393000$\n- $g_i \\le 210$: $N_1=4800$, $M_1=582000$\n\nNow, we calculate $\\sigma_B^2(t) = \\omega_1(t)\\omega_2(t)(\\mu_1(t) - \\mu_2(t))^2$ for each candidate $t$.\n\n- **For $t=30$:**\n  $N_1=400$, $M_1=12000 \\implies \\mu_1=\\frac{12000}{400}=30$.\n  $N_2=4850$, $M_2=678000 \\implies \\mu_2=\\frac{678000}{4850} \\approx 139.79$.\n  $\\sigma_B^2(30) = \\left(\\frac{400}{5250}\\right)\\left(\\frac{4850}{5250}\\right)(30-139.79)^2 \\approx 848.7$\n\n- **For $t=60$:**\n  $N_1=1200$, $M_1=60000 \\implies \\mu_1=\\frac{60000}{1200}=50$.\n  $N_2=4050$, $M_2=630000 \\implies \\mu_2=\\frac{630000}{4050} \\approx 155.56$.\n  $\\sigma_B^2(60) = \\left(\\frac{1200}{5250}\\right)\\left(\\frac{4050}{5250}\\right)(50-155.56)^2 \\approx 1965.7$\n\n- **For $t=90$:**\n  $N_1=2400$, $M_1=168000 \\implies \\mu_1=\\frac{168000}{2400}=70$.\n  $N_2=2850$, $M_2=522000 \\implies \\mu_2=\\frac{522000}{2850} \\approx 183.16$.\n  $\\sigma_B^2(90) = \\left(\\frac{2400}{5250}\\right)\\left(\\frac{2850}{5250}\\right)(70-183.16)^2 \\approx 3179.5$\n\n- **For $t=120$:**\n  $N_1=3000$, $M_1=240000 \\implies \\mu_1=\\frac{240000}{3000}=80$.\n  $N_2=2250$, $M_2=450000 \\implies \\mu_2=\\frac{450000}{2250}=200$.\n  $\\sigma_B^2(120) = \\left(\\frac{3000}{5250}\\right)\\left(\\frac{2250}{5250}\\right)(80-200)^2 = \\left(\\frac{4}{7}\\right)\\left(\\frac{3}{7}\\right)(-120)^2 = \\frac{12}{49}(14400) = \\frac{172800}{49} \\approx 3526.5$\n\n- **For $t=150$:**\n  $N_1=3300$, $M_1=285000 \\implies \\mu_1=\\frac{285000}{3300} \\approx 86.36$.\n  $N_2=1950$, $M_2=405000 \\implies \\mu_2=\\frac{405000}{1950} \\approx 207.69$.\n  $\\sigma_B^2(150) = \\left(\\frac{3300}{5250}\\right)\\left(\\frac{1950}{5250}\\right)(86.36-207.69)^2 \\approx 3435.3$\n\n- **For $t=180$:**\n  $N_1=3900$, $M_1=393000 \\implies \\mu_1=\\frac{393000}{3900} \\approx 100.77$.\n  $N_2=1350$, $M_2=297000 \\implies \\mu_2=\\frac{297000}{1350}=220$.\n  $\\sigma_B^2(180) = \\left(\\frac{3900}{5250}\\right)\\left(\\frac{1350}{5250}\\right)(100.77-220)^2 \\approx 2715.2$\n\n- **For $t=210$:**\n  $N_1=4800$, $M_1=582000 \\implies \\mu_1=\\frac{582000}{4800}=121.25$.\n  $N_2=450$, $M_2=108000 \\implies \\mu_2=\\frac{108000}{450}=240$.\n  $\\sigma_B^2(210) = \\left(\\frac{4800}{5250}\\right)\\left(\\frac{450}{5250}\\right)(121.25-240)^2 \\approx 1109.5$\n\nComparing the calculated values for the between-class variance:\n- $\\sigma_B^2(30) \\approx 848.7$\n- $\\sigma_B^2(60) \\approx 1965.7$\n- $\\sigma_B^2(90) \\approx 3179.5$\n- $\\sigma_B^2(120) \\approx 3526.5$\n- $\\sigma_B^2(150) \\approx 3435.3$\n- $\\sigma_B^2(180) \\approx 2715.2$\n- $\\sigma_B^2(210) \\approx 1109.5$\n\nThe maximum value of $\\sigma_B^2(t)$ occurs at $t=120$. Therefore, the optimal threshold is $t^\\star = 120$.",
            "answer": "$$\\boxed{120}$$"
        },
        {
            "introduction": "Mesoscale simulation methods like the Lattice Boltzmann Method (LBM) operate using algorithmic parameters, such as a relaxation time, which must be correctly mapped to the physical properties of the system being modeled. In this exercise , you will use the multiscale Chapman-Enskog expansion to formally derive the relationship between the LBM relaxation parameter and the macroscopic diffusion coefficient. This provides a rigorous link between the simulation algorithm and the continuum physics it represents, a crucial step for setting up a physically meaningful simulation.",
            "id": "3907182",
            "problem": "Consider a Direct Numerical Simulation (DNS) of ion transport in the electrolyte-filled pore phase of a reconstructed lithium-ion battery electrode microstructure. The pore space is obtained from three-dimensional imaging, and the solid phase is treated with a no-flux boundary condition. The transport of a passive scalar concentration field $\\phi(\\mathbf{x},t)$ in the pore phase is modeled by the single-relaxation-time Bhatnagar-Gross-Krook (BGK) Lattice Boltzmann Method (LBM), with discrete velocities $\\{\\mathbf{e}_i\\}$ and weights $\\{w_i\\}$ chosen to satisfy second-order isotropy. The LBM evolution equation for the scalar distribution $g_i(\\mathbf{x},t)$ is\n$$\ng_i(\\mathbf{x}+\\mathbf{e}_i \\Delta t,\\, t+\\Delta t) - g_i(\\mathbf{x},t) \\;=\\; -\\frac{\\Delta t}{\\tau_{\\text{LB}}}\\,\\Big(g_i(\\mathbf{x},t) - g_i^{\\text{eq}}(\\mathbf{x},t)\\Big),\n$$\nwith equilibrium $g_i^{\\text{eq}}(\\mathbf{x},t) = w_i\\,\\phi(\\mathbf{x},t)$, relaxation time $\\tau_{\\text{LB}}$, and time step $\\Delta t$. The weights and velocities satisfy\n$$\n\\sum_i w_i \\;=\\; 1,\\qquad \\sum_i w_i\\,\\mathbf{e}_i \\;=\\; \\mathbf{0},\\qquad \\sum_i w_i\\, e_{i\\alpha} e_{i\\beta} \\;=\\; c_s^2\\,\\delta_{\\alpha\\beta},\n$$\nwhere $c_s^2$ is the lattice sound speed squared associated with the chosen stencil and $\\delta_{\\alpha\\beta}$ is the Kronecker delta.\n\nStarting from the above evolution equation and the conservation of the zeroth moment $\\sum_i g_i = \\phi$, perform a multiscale Chapman–Enskog expansion under diffusive scaling to recover the macroscopic diffusion equation for $\\phi(\\mathbf{x},t)$ in the pore region,\n$$\n\\partial_t \\phi \\;=\\; D\\,\\nabla^2 \\phi,\n$$\nand derive a closed-form expression for the macroscopic diffusivity $D$ in terms of $\\tau_{\\text{LB}}$, $c_s^2$, and $\\Delta t$. You may assume sufficiently smooth fields, $\\mathcal{O}(\\Delta t^2)$ consistency of the Taylor expansion of the streaming operator, and that the reconstructed microstructure enters only through boundary conditions, not through the bulk constitutive relation. Express your final answer for $D$ as a single analytical expression. No numerical evaluation is required.",
            "solution": "The goal is to start from the single-relaxation-time Bhatnagar–Gross–Krook (BGK) Lattice Boltzmann Method (LBM) evolution equation for a passive scalar and, via a Chapman–Enskog multiscale expansion under diffusive scaling, recover the macroscopic diffusion equation and identify the macroscopic diffusivity $D$ in terms of $\\tau_{\\text{LB}}$, $c_s^2$, and $\\Delta t$.\n\nWe begin from the discrete-time evolution equation\n$$\ng_i(\\mathbf{x}+\\mathbf{e}_i \\Delta t,\\, t+\\Delta t) - g_i(\\mathbf{x},t) \\;=\\; -\\frac{\\Delta t}{\\tau_{\\text{LB}}}\\,\\Big(g_i(\\mathbf{x},t) - g_i^{\\text{eq}}(\\mathbf{x},t)\\Big),\n$$\nwith $g_i^{\\text{eq}}(\\mathbf{x},t) = w_i\\,\\phi(\\mathbf{x},t)$ and the zeroth moment conservation $\\sum_i g_i = \\phi$. The discrete velocity set and weights obey the standard isotropy conditions\n$$\n\\sum_i w_i \\;=\\; 1,\\qquad \\sum_i w_i\\,\\mathbf{e}_i \\;=\\; \\mathbf{0},\\qquad \\sum_i w_i\\, e_{i\\alpha} e_{i\\beta} \\;=\\; c_s^2\\,\\delta_{\\alpha\\beta}.\n$$\n\nTo connect the discrete evolution to a continuum partial differential equation, we perform a second-order Taylor expansion of the streaming operator on the left-hand side around $(\\mathbf{x},t)$, retaining terms up to $\\mathcal{O}(\\Delta t^2)$:\n$$\ng_i(\\mathbf{x}+\\mathbf{e}_i \\Delta t,\\, t+\\Delta t) \\;=\\; g_i(\\mathbf{x},t) + \\Delta t\\,\\big(\\partial_t + \\mathbf{e}_i \\cdot \\nabla\\big) g_i(\\mathbf{x},t) + \\frac{\\Delta t^2}{2}\\,\\big(\\partial_t + \\mathbf{e}_i \\cdot \\nabla\\big)^2 g_i(\\mathbf{x},t) + \\mathcal{O}(\\Delta t^3).\n$$\nSubstituting into the evolution equation yields\n$$\n\\Delta t\\,\\big(\\partial_t + \\mathbf{e}_i \\cdot \\nabla\\big) g_i + \\frac{\\Delta t^2}{2}\\,\\big(\\partial_t + \\mathbf{e}_i \\cdot \\nabla\\big)^2 g_i \\;=\\; -\\frac{\\Delta t}{\\tau_{\\text{LB}}}\\,\\Big(g_i - g_i^{\\text{eq}}\\Big).\n$$\n\nWe next perform a Chapman–Enskog multiscale expansion consistent with diffusive scaling. Introduce a small parameter $\\epsilon$ and expand the distribution function and derivatives as\n$$\ng_i \\;=\\; g_i^{(0)} + \\epsilon\\,g_i^{(1)} + \\epsilon^2\\,g_i^{(2)} + \\cdots,\\qquad \\partial_t \\;=\\; \\epsilon\\,\\partial_{t_1} + \\epsilon^2\\,\\partial_{t_2},\\qquad \\nabla \\;=\\; \\epsilon\\,\\nabla_1.\n$$\nThe equilibrium is taken at leading order, $g_i^{\\text{eq}} = w_i\\,\\phi$, with $\\phi(\\mathbf{x},t)$ varying on the slow scales. Substituting these expansions into the Taylor-expanded evolution and grouping by powers of $\\epsilon$ gives, at orders $\\epsilon^0$, $\\epsilon^1$, and $\\epsilon^2$:\n\nAt order $\\epsilon^0$:\n$$\n0 \\;=\\; -\\frac{\\Delta t}{\\tau_{\\text{LB}}}\\,\\Big(g_i^{(0)} - g_i^{\\text{eq}}\\Big) \\;\\;\\Rightarrow\\;\\; g_i^{(0)} \\;=\\; g_i^{\\text{eq}} \\;=\\; w_i\\,\\phi.\n$$\n\nAt order $\\epsilon^1$:\n$$\n\\Delta t\\,\\big(\\partial_{t_1} + \\mathbf{e}_i \\cdot \\nabla_1\\big) g_i^{(0)} \\;=\\; -\\frac{\\Delta t}{\\tau_{\\text{LB}}}\\,g_i^{(1)} \\;\\;\\Rightarrow\\;\\; g_i^{(1)} \\;=\\; -\\tau_{\\text{LB}}\\,\\big(\\partial_{t_1} + \\mathbf{e}_i \\cdot \\nabla_1\\big) g_i^{(0)}.\n$$\nUsing $g_i^{(0)} = w_i\\,\\phi$ gives\n$$\ng_i^{(1)} \\;=\\; -\\tau_{\\text{LB}}\\,\\Big(w_i\\,\\partial_{t_1}\\phi + w_i\\,\\mathbf{e}_i \\cdot \\nabla_1 \\phi\\Big).\n$$\n\nWe proceed by taking velocity moments to obtain macroscopic equations. The scalar field is the zeroth moment $\\phi = \\sum_i g_i$, and collision conserves the zeroth moment: $\\sum_i (g_i - g_i^{\\text{eq}}) = 0$, which implies $\\sum_i g_i^{(n)} = 0$ for $n \\geq 1$.\n\nZeroth moment of $\\epsilon^1$ order: summing over $i$,\n$$\n\\Delta t\\,\\partial_{t_1} \\sum_i g_i^{(0)} + \\Delta t\\,\\nabla_1 \\cdot \\sum_i \\mathbf{e}_i g_i^{(0)} \\;=\\; -\\frac{\\Delta t}{\\tau_{\\text{LB}}}\\,\\sum_i g_i^{(1)}.\n$$\nUsing $\\sum_i g_i^{(0)} = \\phi$, $\\sum_i \\mathbf{e}_i g_i^{(0)} = \\sum_i \\mathbf{e}_i w_i \\phi = \\mathbf{0}$ by symmetry, and $\\sum_i g_i^{(1)} = 0$, we obtain\n$$\n\\partial_{t_1} \\phi \\;=\\; 0.\n$$\nThus, the $\\epsilon^1$ time scale contributes no evolution: diffusion emerges at the $\\epsilon^2$ scale.\n\nZeroth moment of $\\epsilon^2$ order: We sum the $\\epsilon^2$ equation over $i$. Using the conservation properties $\\sum_i g_i^{(n)} = 0$ for $n \\ge 1$ and $\\sum_i g_i^{(0)}=\\phi$, the resulting moment balance is:\n$$\n\\Delta t\\,\\partial_{t_2}\\phi + \\Delta t\\,\\nabla_1 \\cdot \\sum_i \\mathbf{e}_i g_i^{(1)} + \\frac{\\Delta t^2}{2}\\,\\sum_i \\big(\\partial_{t_1} + \\mathbf{e}_i \\cdot \\nabla_1\\big)^2 g_i^{(0)} = 0.\n$$\nWe now evaluate the two summation terms. The first-order flux term $\\sum_i \\mathbf{e}_i g_i^{(1)}$ is evaluated using the expression for $g_i^{(1)}$ and the lattice isotropy conditions:\n$$\n\\sum_i \\mathbf{e}_i g_i^{(1)} = -\\tau_{\\text{LB}} \\sum_i \\mathbf{e}_i \\Big(w_i\\,\\partial_{t_1}\\phi + w_i\\,\\mathbf{e}_i \\cdot \\nabla_1 \\phi\\Big) = -\\tau_{\\text{LB}}\\,c_s^2\\,\\nabla_1 \\phi.\n$$\nThe second-order streaming term is evaluated using $\\partial_{t_1}\\phi=0$ and lattice isotropy:\n$$\n\\sum_i \\big(\\partial_{t_1} + \\mathbf{e}_i \\cdot \\nabla_1\\big)^2 g_i^{(0)} = \\sum_i w_i\\,(\\mathbf{e}_i \\cdot \\nabla_1)^2 \\phi = c_s^2\\,\\nabla_1^2 \\phi.\n$$\nSubstituting these into the balance equation:\n$$\n\\Delta t\\,\\partial_{t_2}\\phi - \\Delta t\\,\\tau_{\\text{LB}}\\,c_s^2\\,\\nabla_1^2 \\phi + \\frac{\\Delta t^2}{2}\\,c_s^2\\,\\nabla_1^2 \\phi = 0.\n$$\nDividing by $\\Delta t$ and rearranging for $\\partial_{t_2}\\phi$:\n$$\n\\partial_{t_2} \\phi = c_s^2\\,\\Big(\\tau_{\\text{LB}} - \\frac{\\Delta t}{2}\\Big)\\,\\nabla_1^2 \\phi.\n$$\nFinally, we reconstruct the physical-scale equation by summing the time scales. Since $\\partial_{t_1}\\phi = 0$, the macroscopic evolution is governed by $\\partial_t \\phi = \\epsilon^2 \\partial_{t_2}\\phi$ with $\\nabla = \\epsilon \\nabla_1$, yielding the diffusion equation\n$$\n\\partial_t \\phi \\;=\\; D\\,\\nabla^2 \\phi,\n$$\nwith the diffusivity\n$$\nD \\;=\\; c_s^2\\,\\Big(\\tau_{\\text{LB}} - \\frac{\\Delta t}{2}\\Big).\n$$\nThis expression is independent of the specific reconstructed microstructure in the bulk; the microstructure influences boundary conditions (e.g., no-flux at solid interfaces) but not the constitutive relation for $D$ derived here, which follows from the lattice symmetry and the BGK relaxation under diffusive scaling.",
            "answer": "$$\\boxed{c_s^2\\left(\\tau_{\\text{LB}}-\\frac{\\Delta t}{2}\\right)}$$"
        },
        {
            "introduction": "A key task in computational science is to quantify the discretization error inherent in any numerical solution and provide confidence bounds on the computed quantities. This process of verification ensures that simulation results are not artifacts of the chosen grid resolution. This practice  introduces a standardized procedure for this task, guiding you to use results from multiple grid levels to estimate the grid-independent solution and calculate the Grid Convergence Index (GCI), a reliable metric for reporting numerical uncertainty.",
            "id": "3907212",
            "problem": "You are studying Direct Numerical Simulation (DNS) of heat conduction in reconstructed battery electrode microstructures to predict the isotropic effective thermal conductivity, denoted by $k_{\\mathrm{eff}}$. Under stationary conditions and small imposed thermal gradients, Fourier’s law at the homogenized (volume-averaged) level reads $\\mathbf{q} = -k_{\\mathrm{eff}} \\nabla T$, where $\\mathbf{q}$ is the heat flux and $T$ is the temperature. In numerical practice, $k_{\\mathrm{eff}}$ is approximated on a computational grid with characteristic spacing $h$; denote the grid-approximated effective conductivity by $k_h$. For sufficiently fine grids and a consistent discretization, the discretization error is well-approximated by the leading-order algebraic model $k_h = k_{\\infty} + C h^{p}$, where $k_{\\infty}$ is the exact (grid-independent) value, $C$ is a constant independent of $h$, and $p$ is the order of accuracy. \n\nGiven three nested grids with spacings $h_1$, $h_2$, and $h_3$ such that $h_2 = r h_1$ and $h_3 = r h_2$ for a common refinement ratio $r > 1$, and their corresponding effective conductivity estimates $k_{h_1}$, $k_{h_2}$, and $k_{h_3}$, your task is to:\n- Derive, from the leading-order discretization error model, a three-solution Richardson extrapolation procedure to estimate the observed order of accuracy $p$ and the extrapolated conductivity $k_{\\infty}$.\n- Using the standard definition of the Grid Convergence Index (GCI) with safety factor $1.25$ applied to the fine–medium grid pair, compute the fine-grid GCI as a decimal fraction (not a percentage).\n- From the GCI, report symmetric uncertainty bounds for the finest-grid estimate $k_{h_1}$ as $[k_{\\mathrm{lower}}, k_{\\mathrm{upper}}]$, in watts per meter-kelvin (W m$^{-1}$ K$^{-1}$). Express all reported conductivities and bounds in W m$^{-1}$ K$^{-1}$ and round all outputs to $6$ decimal places.\n\nAssumptions:\n- Convergence is monotonic across the three grids (no odd-even oscillations).\n- The refinement ratio $r$ is the same between successive grid levels.\n- The discretization error follows the leading-order algebraic model for sufficiently fine grids.\n\nInput for each test case consists of the tuple $(r, k_{h_1}, k_{h_2}, k_{h_3})$, with all $k$ values in W m$^{-1}$ K$^{-1}$. Use the following four test cases (the test suite), chosen to probe a typical case, a non-integer refinement ratio case, a case converging from below, and a near-converged edge case:\n- Case $1$: $(r, k_{h_1}, k_{h_2}, k_{h_3}) = (\\,2\\,,\\,1.52\\,,\\,1.58\\,,\\,1.82\\,)$\n- Case $2$: $(r, k_{h_1}, k_{h_2}, k_{h_3}) = (\\,1.5\\,,\\,0.85\\,,\\,0.891856\\,,\\,0.96875\\,)$\n- Case $3$: $(r, k_{h_1}, k_{h_2}, k_{h_3}) = (\\,2\\,,\\,1.92\\,,\\,1.6324\\,,\\,0.5298\\,)$\n- Case $4$: $(r, k_{h_1}, k_{h_2}, k_{h_3}) = (\\,2\\,,\\,1.001\\,,\\,1.004\\,,\\,1.016\\,)$\n\nYour program must:\n- Implement the derivation to compute, for each test case: the observed order $p$, the extrapolated conductivity $k_{\\infty}$, the fine-grid GCI as a decimal fraction, and the symmetric uncertainty bounds $[k_{\\mathrm{lower}}, k_{\\mathrm{upper}}]$ around $k_{h_1}$ in W m$^{-1}$ K$^{-1}$.\n- Round all reported floating-point outputs to $6$ decimal places.\n- Produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case result is itself a list $[p, k_{\\infty}, \\mathrm{GCI}, k_{\\mathrm{lower}}, k_{\\mathrm{upper}}]$ with all entries as floats. For example: $[[p_1,k_{\\infty,1},\\mathrm{GCI}_1,k_{\\mathrm{lower},1},k_{\\mathrm{upper},1}],[p_2,\\dots],\\dots]$.\n\nNote: Angles are not involved, so no angle unit specification is required. Percentages must be expressed as decimal fractions (for example, use $0.05$ instead of $5$ percent).",
            "solution": "The problem is assessed to be valid as it is scientifically grounded in the principles of numerical analysis (specifically, discretization error analysis), well-posed, and objective. It provides a complete and consistent set of data and assumptions to derive a unique solution using standard, verifiable methods.\n\nThe task is to estimate the grid-independent effective thermal conductivity $k_{\\infty}$ and the observed order of accuracy $p$ from a series of three numerical simulations on nested grids. We are also asked to compute the Grid Convergence Index (GCI) and the corresponding uncertainty interval for the finest-grid solution.\n\nThe starting point is the provided leading-order algebraic error model for the numerically approximated effective thermal conductivity, $k_h$:\n$$k_h = k_{\\infty} + C h^{p}$$\nwhere $k_{\\infty}$ is the exact, grid-independent value, $h$ is the characteristic grid spacing, $p$ is the order of accuracy, and $C$ is a constant.\n\nWe are given three conductivity estimates, $k_{h_1}$, $k_{h_2}$, and $k_{h_3}$, computed on three grids with spacings $h_1$ (fine), $h_2$ (medium), and $h_3$ (coarse). The grids are related by a common refinement ratio $r > 1$ such that $h_2 = r h_1$ and $h_3 = r h_2 = r^2 h_1$.\n\nWe can write the error model for each of the three grids:\n$$k_{h_1} = k_{\\infty} + C h_1^p \\quad (1)$$\n$$k_{h_2} = k_{\\infty} + C h_2^p = k_{\\infty} + C (r h_1)^p \\quad (2)$$\n$$k_{h_3} = k_{\\infty} + C h_3^p = k_{\\infty} + C (r^2 h_1)^p \\quad (3)$$\n\nTo find the observed order of accuracy $p$, we first eliminate the unknown $k_{\\infty}$ by taking differences between successive equations.\nSubtracting $(1)$ from $(2)$:\n$$k_{h_2} - k_{h_1} = C ((r h_1)^p - h_1^p) = C h_1^p (r^p - 1) \\quad (4)$$\nSubtracting $(2)$ from $(3)$:\n$$k_{h_3} - k_{h_2} = C ((r^2 h_1)^p - (r h_1)^p) = C (r h_1)^p (r^p - 1) = C h_1^p r^p (r^p - 1) \\quad (5)$$\n\nNow, to eliminate $C$ and $h_1$, we take the ratio of equation $(5)$ to equation $(4)$:\n$$\\frac{k_{h_3} - k_{h_2}}{k_{h_2} - k_{h_1}} = \\frac{C h_1^p r^p (r^p - 1)}{C h_1^p (r^p - 1)} = r^p$$\nThis allows us to solve for $p$ by taking the natural logarithm of both sides:\n$$\\ln\\left(\\frac{k_{h_3} - k_{h_2}}{k_{h_2} - k_{h_1}}\\right) = \\ln(r^p) = p \\ln(r)$$\nThus, the observed order of accuracy is:\n$$p = \\frac{\\ln\\left(\\frac{k_{h_3} - k_{h_2}}{k_{h_2} - k_{h_1}}\\right)}{\\ln(r)}$$\n\nWith the value of $p$ determined, we can estimate the grid-independent solution $k_{\\infty}$ using Richardson extrapolation. Rearranging equation $(4)$ gives an expression for the error term on the fine grid, $C h_1^p$:\n$$C h_1^p = \\frac{k_{h_2} - k_{h_1}}{r^p - 1}$$\nSubstituting this back into equation $(1)$:\n$$k_{h_1} = k_{\\infty} + \\frac{k_{h_2} - k_{h_1}}{r^p - 1}$$\nSolving for $k_{\\infty}$:\n$$k_{\\infty} = k_{h_1} - \\frac{k_{h_2} - k_{h_1}}{r^p - 1} = \\frac{k_{h_1}(r^p - 1) - (k_{h_2} - k_{h_1})}{r^p - 1} = \\frac{k_{h_1}r^p - k_{h_1} - k_{h_2} + k_{h_1}}{r^p - 1}$$\nThis simplifies to the Richardson extrapolation formula:\n$$k_{\\infty} = \\frac{r^p k_{h_1} - k_{h_2}}{r^p - 1}$$\n\nNext, we compute the Grid Convergence Index (GCI) for the fine-grid solution, based on the fine-medium grid pair ($h_1, h_2$). The GCI provides an estimate of the numerical uncertainty. Using a factor of safety $F_s = 1.25$, the GCI is defined as a decimal fraction:\n$$\\mathrm{GCI} = F_s \\left| \\frac{\\epsilon_a}{r^p - 1} \\right|$$\nwhere $\\epsilon_a$ is the approximate relative error between the fine and medium grid solutions:\n$$\\epsilon_a = \\frac{k_{h_1} - k_{h_2}}{k_{h_1}}$$\nSubstituting $\\epsilon_a$ into the GCI formula:\n$$\\mathrm{GCI} = 1.25 \\left| \\frac{(k_{h_1} - k_{h_2})/k_{h_1}}{r^p - 1} \\right| = \\frac{1.25}{|r^p - 1|} \\left| \\frac{k_{h_1} - k_{h_2}}{k_{h_1}} \\right|$$\n\nFinally, we determine the symmetric uncertainty bounds for the finest-grid estimate $k_{h_1}$. The GCI represents the relative uncertainty in the solution $k_{h_1}$. The absolute uncertainty is $\\Delta k = \\mathrm{GCI} \\times |k_{h_1}|$. Since thermal conductivity is a positive quantity, $|k_{h_1}| = k_{h_1}$. The symmetric uncertainty interval $[k_{\\mathrm{lower}}, k_{\\mathrm{upper}}]$ is therefore:\n$$k_{\\mathrm{lower}} = k_{h_1} - \\mathrm{GCI} \\cdot k_{h_1} = k_{h_1} (1 - \\mathrm{GCI})$$\n$$k_{\\mathrm{upper}} = k_{h_1} + \\mathrm{GCI} \\cdot k_{h_1} = k_{h_1} (1 + \\mathrm{GCI})$$\n\nThese derived formulas will be implemented for each test case, with all final floating-point values rounded to $6$ decimal places.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the grid convergence analysis problem for the given test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (r, k_h1, k_h2, k_h3)\n        (2.0, 1.52, 1.58, 1.82),\n        (1.5, 0.85, 0.891856, 0.96875),\n        (2.0, 1.92, 1.6324, 0.5298),\n        (2.0, 1.001, 1.004, 1.016),\n    ]\n\n    all_results = []\n    \n    # Safety factor for GCI calculation\n    F_s = 1.25\n\n    for case in test_cases:\n        r, k_h1, k_h2, k_h3 = case\n\n        # Step 1: Calculate the observed order of accuracy 'p'\n        # p = ln((k_h3 - k_h2) / (k_h2 - k_h1)) / ln(r)\n        # Check for division by zero, although problem data avoids this.\n        diff_21 = k_h2 - k_h1\n        if diff_21 == 0:\n            # If the two finer solutions are identical, p cannot be determined from this formula.\n            # This indicates convergence or issues. For this problem, we assume diff_21 != 0.\n            # In a real scenario, one might have to handle this (e.g., assume p is the theoretical order).\n            # For this problem, we can continue as the test data is well-behaved.\n            pass\n        \n        ratio_of_diffs = (k_h3 - k_h2) / diff_21\n        # The argument to log must be positive. Monotonic convergence ensures this.\n        p = np.log(ratio_of_diffs) / np.log(r)\n\n        # Step 2: Calculate the extrapolated grid-independent solution 'k_inf'\n        # k_inf = (r^p * k_h1 - k_h2) / (r^p - 1)\n        rp = r**p\n        k_inf = (rp * k_h1 - k_h2) / (rp - 1)\n\n        # Step 3: Calculate the Grid Convergence Index (GCI) for the fine grid\n        # GCI = F_s * | (k_h1 - k_h2) / k_h1 | / (r^p - 1)\n        approx_rel_err = (k_h1 - k_h2) / k_h1\n        gci = F_s * np.abs(approx_rel_err) / (rp - 1)\n        \n        # Step 4: Calculate the symmetric uncertainty bounds for k_h1\n        # k_lower = k_h1 * (1 - GCI)\n        # k_upper = k_h1 * (1 + GCI)\n        k_lower = k_h1 * (1.0 - gci)\n        k_upper = k_h1 * (1.0 + gci)\n\n        # Append the results for the current test case, after rounding\n        all_results.append([\n            round(p, 6),\n            round(k_inf, 6),\n            round(gci, 6),\n            round(k_lower, 6),\n            round(k_upper, 6),\n        ])\n\n    # Format the final output string as a list of lists.\n    # e.g., [[p1, k_inf1, ...], [p2, k_inf2, ...]]\n    inner_list_strings = []\n    for result_list in all_results:\n        # Convert each float in the list to a string. `str()` handles `2.0` vs `2.123` correctly.\n        str_list = [str(val) for val in result_list]\n        inner_list_strings.append(f\"[{','.join(str_list)}]\")\n    \n    final_output = f\"[{','.join(inner_list_strings)}]\"\n    \n    print(final_output)\n\nsolve()\n```"
        }
    ]
}