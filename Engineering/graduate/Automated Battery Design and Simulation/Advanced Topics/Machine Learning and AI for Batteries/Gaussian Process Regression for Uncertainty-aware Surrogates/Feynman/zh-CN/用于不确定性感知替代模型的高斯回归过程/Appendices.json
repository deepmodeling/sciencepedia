{
    "hands_on_practices": [
        {
            "introduction": "高斯过程模型的表达能力很大程度上取决于其核函数中的超参数，例如长度尺度 $\\ell$。为模型选择最佳超参数的原则性方法是最大化边缘似然。本练习将引导你推导边缘似然函数相对于长度尺度的梯度，这是利用基于梯度的优化算法高效进行超参数调优的关键一步 。通过这个推导，你将掌握应用于机器学习模型的矩阵微积分基本技能，并揭开超参数“自动”调优的神秘面纱。",
            "id": "3915981",
            "problem": "在一个自动化的锂离子电池单元设计工作流程中，高保真电化学仿真会生成训练数据，用于构建一个以设计变量为函数的放电容量代理模型。令 $X \\in \\mathbb{R}^{n \\times d}$ 表示 $n$ 个仿真设计（每行 $x_{i} \\in \\mathbb{R}^{d}$ 编码了孔隙率、活性材料分数、压实压力和隔膜曲折度等特征），并令 $y \\in \\mathbb{R}^{n}$ 为相应的标准化放电容量（零均值和单位方差）。考虑一个高斯过程 (GP) 回归代理模型，其具有零均值和各向同性平方指数核 $k(x, x^{\\prime}) = \\sigma_{f}^{2} \\exp\\!\\left(-\\frac{\\|x - x^{\\prime}\\|_{2}^{2}}{2 \\ell^{2}}\\right)$，以及方差为 $\\sigma_{n}^{2}$ 的加性独立高斯噪声。协方差矩阵为 $K = K_{y} + \\sigma_{n}^{2} I$，其中 $[K_{y}]_{ij} = k(x_{i}, x_{j})$，$I$ 是单位矩阵，超参数为 $\\theta = \\{\\ell, \\sigma_{f}^{2}, \\sigma_{n}^{2}\\}$。\n\n仅从 $y \\sim \\mathcal{N}(0, K)$ 的多元高斯对数似然的定义出发，使用矩阵微积分推导其关于核长度尺度 $\\ell$ 的梯度 $\\frac{\\partial \\log p(y \\mid X, \\theta)}{\\partial \\ell}$。您的推导过程必须明确使用矩阵逆的导数和对数行列式的导数的恒等式，并且必须将所有常数保持为符号形式。然后，通过使用两两之间的欧几里得距离 $d_{ij} = \\|x_{i} - x_{j}\\|_{2}$、向量 $\\alpha = K^{-1} y$ 和矩阵 $K^{-1}$ 来表示结果，从而将该结果特化于各向同性平方指数核。\n\n在一个简短的段落中，解释在自动化电池设计中，该梯度如何在实践中为能够感知不确定性的代理模型实现基于梯度的超参数优化，包括如何对 $\\ell$ 强制施加正性约束以及如何稳定地进行所需的线性代数计算。\n\n将您的最终梯度表示为包含对索引 $i$ 和 $j$ 的双重求和的单个闭式解析表达式。不需要进行数值计算。最终表达式中不应包含物理单位。不需要进行四舍五入。",
            "solution": "该问题要求推导高斯过程 (GP) 的对数边缘似然关于核长度尺度超参数 $\\ell$ 的梯度，然后解释其在超参数优化中的应用。\n\n观测数据向量 $y \\in \\mathbb{R}^{n}$ 被建模为从一个零均值、协方差矩阵为 $K$ 的多元高斯分布中抽取，即 $y \\sim \\mathcal{N}(0, K)$。对数边缘似然，记为 $\\mathcal{L}(\\theta) = \\log p(y \\mid X, \\theta)$，由下式给出：\n$$\n\\mathcal{L}(\\theta) = \\log p(y \\mid K) = -\\frac{n}{2} \\log(2\\pi) - \\frac{1}{2} \\log|K| - \\frac{1}{2} y^T K^{-1} y\n$$\n此处，$K$ 是协方差矩阵，它依赖于超参数 $\\theta = \\{\\ell, \\sigma_f^2, \\sigma_n^2\\}$。我们的任务是求出 $\\mathcal{L}(\\theta)$ 关于长度尺度 $\\ell$ 的偏导数。\n\n首先，我们对 $\\mathcal{L}(\\theta)$ 求关于 $\\ell$ 的导数。第一项 $-\\frac{n}{2} \\log(2\\pi)$ 是一个关于 $\\ell$ 的常数，因此其导数为零。其余两项通过矩阵 $K$ 依赖于 $\\ell$。\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\ell} = -\\frac{1}{2} \\frac{\\partial}{\\partial \\ell} (\\log|K|) - \\frac{1}{2} \\frac{\\partial}{\\partial \\ell} (y^T K^{-1} y)\n$$\n我们应用矩阵导数的链式法则。对于一个矩阵的一般标量函数 $f(K)$，其关于一个标量参数 $\\lambda$ 的导数由 $\\frac{\\partial f(K)}{\\partial \\lambda} = \\text{Tr}\\left( \\frac{\\partial f(K)}{\\partial K} \\frac{\\partial K}{\\partial \\lambda} \\right)$ 给出，假设 $K$ 是对称的。\n\n对于对数行列式项，我们使用恒等式 $\\frac{\\partial \\log|K|}{\\partial K} = K^{-1}$。应用链式法则：\n$$\n\\frac{\\partial}{\\partial \\ell} (\\log|K|) = \\text{Tr}\\left( \\frac{\\partial \\log|K|}{\\partial K} \\frac{\\partial K}{\\partial \\ell} \\right) = \\text{Tr}\\left( K^{-1} \\frac{\\partial K}{\\partial \\ell} \\right)\n$$\n对于二次型项，我们首先使用矩阵逆的导数恒等式：$\\frac{\\partial K^{-1}}{\\partial \\lambda} = -K^{-1} \\frac{\\partial K}{\\partial \\lambda} K^{-1}$。应用此恒等式：\n$$\n\\frac{\\partial}{\\partial \\ell} (y^T K^{-1} y) = y^T \\left( \\frac{\\partial K^{-1}}{\\partial \\ell} \\right) y = y^T \\left( -K^{-1} \\frac{\\partial K}{\\partial \\ell} K^{-1} \\right) y\n$$\n结合这些结果，对数似然的梯度为：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\ell} = -\\frac{1}{2} \\text{Tr}\\left( K^{-1} \\frac{\\partial K}{\\partial \\ell} \\right) - \\frac{1}{2} \\left( -y^T K^{-1} \\frac{\\partial K}{\\partial \\ell} K^{-1} y \\right) = \\frac{1}{2} \\left( y^T K^{-1} \\frac{\\partial K}{\\partial \\ell} K^{-1} y - \\text{Tr}\\left( K^{-1} \\frac{\\partial K}{\\partial \\ell} \\right) \\right)\n$$\n根据问题规定，令 $\\alpha = K^{-1} y$。由于 $K$ 是对称的，$\\alpha^T = (K^{-1}y)^T = y^T (K^{-1})^T = y^T K^{-1}$。将 $\\alpha$ 代入表达式中：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\ell} = \\frac{1}{2} \\left( \\alpha^T \\frac{\\partial K}{\\partial \\ell} \\alpha - \\text{Tr}\\left( K^{-1} \\frac{\\partial K}{\\partial \\ell} \\right) \\right)\n$$\n第一项 $\\alpha^T \\frac{\\partial K}{\\partial \\ell} \\alpha$ 是一个标量，因此它等于其自身的迹。利用迹的循环性质 $\\text{Tr}(ABC) = \\text{Tr}(BCA)$，我们有 $\\text{Tr}(\\alpha^T \\frac{\\partial K}{\\partial \\ell} \\alpha) = \\text{Tr}(\\alpha \\alpha^T \\frac{\\partial K}{\\partial \\ell})$。这使我们能够提出导数项的公因子：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\ell} = \\frac{1}{2} \\text{Tr}\\left( \\alpha \\alpha^T \\frac{\\partial K}{\\partial \\ell} \\right) - \\frac{1}{2} \\text{Tr}\\left( K^{-1} \\frac{\\partial K}{\\partial \\ell} \\right) = \\frac{1}{2} \\text{Tr}\\left( (\\alpha \\alpha^T - K^{-1}) \\frac{\\partial K}{\\partial \\ell} \\right)\n$$\n接下来，我们将此结果特化于各向同性平方指数核。协方差矩阵为 $K = K_y + \\sigma_n^2 I$，其中 $[K_y]_{ij} = k(x_i, x_j)$。噪声方差 $\\sigma_n^2$ 和单位矩阵 $I$ 不依赖于 $\\ell$，所以 $\\frac{\\partial K}{\\partial \\ell} = \\frac{\\partial K_y}{\\partial \\ell}$。核函数为 $k(x, x') = \\sigma_f^2 \\exp\\left(-\\frac{\\|x - x'\\|_2^2}{2\\ell^2}\\right)$。令 $d_{ij} = \\|x_i - x_j\\|_2$。矩阵 $K$ 的第 $(i, j)$ 个元素关于 $\\ell$ 的偏导数为：\n$$\n\\frac{\\partial K_{ij}}{\\partial \\ell} = \\frac{\\partial}{\\partial \\ell} \\left( \\sigma_f^2 \\exp\\left(-\\frac{d_{ij}^2}{2\\ell^2}\\right) \\right) = \\sigma_f^2 \\exp\\left(-\\frac{d_{ij}^2}{2\\ell^2}\\right) \\cdot \\frac{\\partial}{\\partial \\ell} \\left(-\\frac{d_{ij}^2}{2\\ell^2}\\right) = \\sigma_f^2 \\exp\\left(-\\frac{d_{ij}^2}{2\\ell^2}\\right) \\cdot \\left( - \\frac{d_{ij}^2}{2} \\cdot (-2\\ell^{-3}) \\right)\n$$\n$$\n\\frac{\\partial K_{ij}}{\\partial \\ell} = \\sigma_f^2 \\exp\\left(-\\frac{d_{ij}^2}{2\\ell^2}\\right) \\frac{d_{ij}^2}{\\ell^3}\n$$\n迹项 $\\text{Tr}(AB)$ 可以计算为逐元素乘积之和，即 $\\sum_{i,j} A_{ij} B_{ji}$。由于矩阵 $(\\alpha\\alpha^T - K^{-1})$ 和 $\\frac{\\partial K}{\\partial \\ell}$ 都是对称的，这可以变为 $\\sum_{i,j} (\\alpha\\alpha^T - K^{-1})_{ij} (\\frac{\\partial K}{\\partial \\ell})_{ij}$。我们现在可以将梯度的最终表达式写成一个双重求和：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\ell} = \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n \\left( [\\alpha\\alpha^T]_{ij} - [K^{-1}]_{ij} \\right) \\frac{\\partial K_{ij}}{\\partial \\ell} = \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n \\left( \\alpha_i \\alpha_j - [K^{-1}]_{ij} \\right) \\left( \\sigma_f^2 \\exp\\left(-\\frac{d_{ij}^2}{2\\ell^2}\\right) \\frac{d_{ij}^2}{\\ell^3} \\right)\n$$\n这个表达式可以稍作整理，将常数从求和中提出，从而得到所要求的最终形式。\n\n在电池设计工作流程中，推导出的梯度对于自动化调优 GP 代理模型的超参数至关重要。其目标是最大化关于超参数 $\\theta = \\{\\ell, \\sigma_f^2, \\sigma_n^2\\}$ 的对数边缘似然，这一过程被称为第二类最大似然估计。此优化通常使用基于梯度的算法（如 L-BFGS-B）来执行，该算法需要目标函数（对数似然）关于每个超参数的解析梯度。上面推导的表达式提供了关于 $\\ell$ 的梯度分量。因为长度尺度 $\\ell$ 必须严格为正，所以对 $\\ell$ 的直接优化是受约束的。为了处理这个问题，通常通过对一个无约束变量 $\\xi \\in \\mathbb{R}$ 进行优化来重新参数化 $\\ell$，其中 $\\ell = \\exp(\\xi)$。然后通过链式法则计算关于 $\\xi$ 的梯度：$\\frac{\\partial \\mathcal{L}}{\\partial \\xi} = \\frac{\\partial \\mathcal{L}}{\\partial \\ell} \\frac{\\partial \\ell}{\\partial \\xi} = \\frac{\\partial \\mathcal{L}}{\\partial \\ell} \\exp(\\xi) = \\frac{\\partial \\mathcal{L}}{\\partial \\ell} \\ell$。从实践的角度来看，$\\alpha=K^{-1}y$ 和 $K^{-1}$ 的计算不是通过直接矩阵求逆来执行的，因为这种方法在数值上不稳定且计算成本高昂 ($O(n^3)$)。替代方法是计算 Cholesky 分解 $K = LL^T$。对于对称正定的协方差矩阵 $K$，这种分解是数值稳定的，并且通过求解两个三角系统 $Lz=y$ 和 $L^T\\alpha=z$ 可以在 $O(n^2)$ 时间内高效地求出 $\\alpha$。梯度计算所需的矩阵逆 $K^{-1}$ 也可以通过 Cholesky 因子稳定地计算出来，即 $K^{-1} = (L^T)^{-1}L^{-1}$。",
            "answer": "$$\n\\boxed{\\frac{\\sigma_f^2}{2\\ell^3} \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\left( \\alpha_i \\alpha_j - [K^{-1}]_{ij} \\right) d_{ij}^2 \\exp\\left(-\\frac{d_{ij}^2}{2\\ell^2}\\right)}\n$$"
        },
        {
            "introduction": "一个模型的有效性离不开其数值稳定性，尤其对于依赖精确不确定性量化的代理模型而言。本练习探讨了核函数长度尺度 $\\ell$ 的选择如何深刻影响协方差矩阵的条件数，进而影响不确定性预测的可靠性 。通过分析这种关系，你将对超参数、训练数据分布以及高斯过程不确定性估计的可信度之间的相互作用建立起关键的直觉，这对于构建鲁棒的代理模型至关重要。",
            "id": "3915947",
            "problem": "在锂离子电池电极的自动化设计中，考虑使用高斯过程 (GP) 回归和径向基函数 (RBF) 核来构建一个能够感知不确定性的放电容量代理模型。设训练输入为 $X=\\{x_i\\}_{i=1}^n \\subset \\mathbb{R}^d$，观测输出为 $y \\in \\mathbb{R}^n$。GP 先验的均值为零，协方差函数为 $k(x,x')=\\sigma_f^2 \\exp\\!\\big(-\\tfrac{1}{2}\\|x-x'\\|_2^2/\\ell^2\\big)$，其中 $\\sigma_f^20$ 是信号方差，$\\ell0$ 是长度尺度。观测噪声是独立同分布的高斯噪声，其方差为 $\\sigma_n^20$。用 $K \\in \\mathbb{R}^{n \\times n}$ 表示核矩阵，其元素为 $K_{ij}=k(x_i,x_j)$，并定义 $A=K+\\sigma_n^2 I$。在测试输入 $x_\\star$ 处的预测方差为 $s^2(x_\\star)=k(x_\\star,x_\\star)-k_\\star^\\top A^{-1} k_\\star$，其中 $k_\\star=[k(x_\\star,x_1),\\dots,k(x_\\star,x_n)]^\\top$。一个对称正定矩阵 $M$ 的条件数为 $\\kappa(M)=\\lambda_{\\max}(M)/\\lambda_{\\min}(M)$，其中 $\\lambda_{\\max}$ 和 $\\lambda_{\\min}$ 分别是最大和最小特征值。\n\n假设不同训练输入之间的最小成对距离为 $\\delta_{\\min}=\\min_{i \\neq j}\\|x_i-x_j\\|_20$。请从基本原理出发，分析当长度尺度 $\\ell$ 相对于 $\\delta_{\\min}$ 很小时，条件数 $\\kappa(A)$ 的行为，并解释其对靠近和远离训练输入的 $s^2(x_\\star)$ 的影响。然后，评估重复或近乎重复的设计点（即，对于某些 $i \\neq j$，有 $\\|x_i-x_j\\|_2 \\approx 0$）的存在如何改变条件数和不确定性估计的可靠性。你的推理应仅基于核心定义和已充分验证的事实，例如对称矩阵的特征值界、Gershgorin 圆盘定理和 GP 预测方差公式。\n\n选择所有正确的陈述。\n\nA. 如果 $\\ell$ 相对于 $\\delta_{\\min}$ 足够小（且没有重复点），那么 $K$ 是严格对角占优的，$A$ 接近于 $(\\sigma_f^2+\\sigma_n^2)I$，且当 $\\ell \\to 0$ 时 $\\kappa(A) \\to 1$。因此，$A$ 的数值求逆是稳定的；对于远离 $X$ 的 $x_\\star$，$s^2(x_\\star)\\approx \\sigma_f^2$，而在靠近任意 $x_i$ 处，有 $s^2(x_i)\\approx \\sigma_f^2 \\sigma_n^2/(\\sigma_f^2+\\sigma_n^2)$，当 $\\sigma_n^2 \\to 0$ 时，该值趋于 $0$。\n\nB. 当 $\\ell$ 很小时，$K$ 变得近似为秩-1，因此 $\\kappa(A)$ 必然无界增长，这会全局性地增大不确定性估计（即，$s^2(x_\\star)$ 在各处都增加）。\n\nC. 如果存在重复输入 $x_i=x_j$（或点对非常接近以至于 $\\|x_i-x_j\\|_2 \\ll \\ell$）且 $\\sigma_n^2$ 非常小，那么 $K$ 有近乎相同的行并且是近乎奇异的；$A$ 的最小特征值在 $\\sigma_n^2$ 的量级，最大特征值在 $\\sigma_f^2+\\sigma_n^2$ 的量级，因此 $\\kappa(A)\\gtrsim (\\sigma_f^2+\\sigma_n^2)/\\sigma_n^2$。这会降低 $s^2(x_\\star)$ 的数值可靠性，除非添加额外的抖动项 $\\tau^2 I$（其中 $\\tau^20$）。\n\nD. 对于小的 $\\ell$，$A$ 的条件数与输入分布无关，因为 $A=(\\sigma_f^2+\\sigma_n^2)I$ 精确成立，所以对于任何 $X$ 和任何 $\\ell$，$\\kappa(A)=1$。\n\nE. 减小 $\\ell$ 会单调地减小定义域内各处的 $s^2(x_\\star)$ 的认知不确定性部分，而无论 $\\sigma_n^2$ 和训练输入位置如何，因为更小的长度尺度总是意味着更可靠的局部插值。",
            "solution": "我们从核心定义和标准的矩阵分析事实出发，逐一分析每个选项。矩阵 $A = K + \\sigma_n^2 I$，其中 $K_{ij} = \\sigma_f^2 \\exp(-\\frac{\\|x_i-x_j\\|^2}{2\\ell^2})$。\n\n**选项 A 的分析：**\n当长度尺度 $\\ell$ 相对于训练点之间的最小距离 $\\delta_{\\min}$ 非常小时（$\\ell \\ll \\delta_{\\min}$），对于任意 $i \\neq j$，比值 $\\|x_i-x_j\\|^2 / \\ell^2$ 会非常大。这导致核函数中的指数项 $\\exp(-\\frac{\\|x_i-x_j\\|^2}{2\\ell^2})$ 趋近于 0。因此，核矩阵 $K$ 的非对角线元素 $K_{ij}$ 都将趋近于 0。对角线元素 $K_{ii} = \\sigma_f^2 \\exp(0) = \\sigma_f^2$。所以，核矩阵 $K$ 近似于一个对角矩阵 $\\sigma_f^2 I$。\n那么，矩阵 $A = K + \\sigma_n^2 I$ 近似于 $A \\approx \\sigma_f^2 I + \\sigma_n^2 I = (\\sigma_f^2 + \\sigma_n^2)I$。\n这是一个标量乘以单位矩阵，其所有特征值都等于 $\\sigma_f^2 + \\sigma_n^2$。因此，$\\lambda_{\\max}(A) = \\lambda_{\\min}(A) = \\sigma_f^2 + \\sigma_n^2$，条件数 $\\kappa(A) = \\lambda_{\\max}(A) / \\lambda_{\\min}(A) \\to 1$。这意味着矩阵求逆是数值稳定的。\n对于远离训练数据 $X$ 的测试点 $x_\\star$，$\\|x_\\star-x_i\\|$ 很大，因此 $k(x_\\star, x_i) \\to 0$。向量 $k_\\star$ 趋近于零向量，导致预测方差 $s^2(x_\\star) = k(x_\\star,x_\\star) - k_\\star^\\top A^{-1} k_\\star \\approx k(x_\\star,x_\\star) = \\sigma_f^2$。\n对于靠近某个训练数据点 $x_i$ 的测试点，例如 $x_\\star = x_i$，我们有 $k_\\star \\approx \\sigma_f^2 e_i$（$e_i$ 是第 $i$ 个标准基向量），而 $A^{-1} \\approx \\frac{1}{\\sigma_f^2+\\sigma_n^2}I$。那么 $k_\\star^\\top A^{-1} k_\\star \\approx (\\sigma_f^2 e_i^\\top) (\\frac{1}{\\sigma_f^2+\\sigma_n^2}I) (\\sigma_f^2 e_i) = \\frac{\\sigma_f^4}{\\sigma_f^2+\\sigma_n^2}$。\n因此，预测方差为 $s^2(x_i) \\approx \\sigma_f^2 - \\frac{\\sigma_f^4}{\\sigma_f^2+\\sigma_n^2} = \\frac{\\sigma_f^2(\\sigma_f^2+\\sigma_n^2) - \\sigma_f^4}{\\sigma_f^2+\\sigma_n^2} = \\frac{\\sigma_f^2 \\sigma_n^2}{\\sigma_f^2+\\sigma_n^2}$。这个量度量的是在数据点处对函数 $f$ 的认知不确定性。当观测噪声 $\\sigma_n^2 \\to 0$ 时，该不确定性趋于 0，这符合直觉。\n综上，选项 A 的所有陈述都是正确的。\n\n**选项 B 的分析：**\n当 $\\ell$ 很小时，如上所述，$K$ 趋向于对角矩阵，而不是秩-1 矩阵。秩-1 矩阵是在 $\\ell$ 很大的情况下出现的，此时所有 $K_{ij}$ 都趋于 $\\sigma_f^2$。因此，选项 B 的前提是错误的。\n\n**选项 C 的分析：**\n如果存在重复的输入点 $x_i=x_j$ (for $i \\neq j$)，那么核矩阵 $K$ 的第 $i$ 行和第 $j$ 行将完全相同。这意味着 $K$ 是奇异的，其最小特征值 $\\lambda_{\\min}(K)=0$。如果点非常接近（$\\|x_i-x_j\\|_2 \\ll \\ell$），那么第 $i$ 行和第 $j$ 行将非常相似，导致 $K$ 是近奇异的，$\\lambda_{\\min}(K) \\approx 0$。\n对于矩阵 $A=K+\\sigma_n^2 I$，根据韦尔不等式，其最小特征值 $\\lambda_{\\min}(A) \\ge \\lambda_{\\min}(K) + \\lambda_{\\min}(\\sigma_n^2 I) \\approx 0 + \\sigma_n^2$。其最大特征值 $\\lambda_{\\max}(A)$ 大约是 $\\sigma_f^2 + \\sigma_n^2$（根据格氏圆盘定理，特征值被以对角元素为中心的圆盘所包含）。\n因此，条件数 $\\kappa(A) = \\frac{\\lambda_{\\max}(A)}{\\lambda_{\\min}(A)} \\gtrsim \\frac{\\sigma_f^2+\\sigma_n^2}{\\sigma_n^2}$。如果噪声方差 $\\sigma_n^2$ 很小，这个条件数会非常大，导致矩阵 $A$ 是病态的（ill-conditioned）。这会使得 $A^{-1}$ 的计算在数值上非常不稳定，从而严重影响预测方差 $s^2(x_\\star)$ 的可靠性。为了缓解这个问题，实践中通常会向对角线添加一个额外的“抖动”（jitter）项 $\\tau^2 I$，人为地增大了 $\\lambda_{\\min}(A)$，从而改善条件数。\n因此，选项 C 的陈述是正确的。\n\n**选项 D 的分析：**\n如选项 A 和 C 的分析所示，条件数强烈依赖于 $\\ell$ 和训练数据的分布（特别是点之间的间距）。因此，该陈述是错误的。\n\n**选项 E 的分析：**\n减小 $\\ell$ 会使得模型认为数据点之间的相关性衰减得非常快。这会导致 GP 在训练点之间快速地回归到先验均值，并在远离训练点的区域产生很高的不确定性。这通常会增加而不是减少认知不确定性，与可靠插值的直觉相反。因此，该陈述是错误的。",
            "answer": "$$\\boxed{AC}$$"
        },
        {
            "introduction": "拥有一个训练有素且易于理解的高斯过程代理模型后，我们如何利用它来指导决策，例如在浩瀚的设计空间中寻找最优的电池参数？本练习将高斯过程的应用与贝叶斯优化联系起来，要求你推导并分析“改进概率”（Probability of Improvement, PI）这一经典的采集函数 。通过揭示PI这类简单采集函数的内在局限性，本练习旨在激发你对更复杂、更强大的采集函数（如“期望改进”Expected Improvement, EI）的深入思考，从而更好地平衡探索与利用。",
            "id": "3915948",
            "problem": "在自动化电池设计和仿真中，作为设计变量函数的放电容量（表示为 $f(x)$）采用高斯过程回归（Gaussian Process Regression, GPR）进行建模。在候选设计 $x$ 处，根据 GPR 预测模型，假设给定数据的 $f(x)$ 的后验分布是均值为 $\\mu(x)$、方差为 $\\sigma^{2}(x)$ 的高斯分布，即 $f(x) \\mid \\mathcal{D} \\sim \\mathcal{N}\\!\\left(\\mu(x), \\sigma^{2}(x)\\right)$。设迄今为止观测到的最佳放电容量为 $f^{\\star}$，并定义改进随机变量 $I(x)$ 为 $I(x) = \\max\\!\\left(f(x) - f^{\\star} - \\xi, 0\\right)$，其中 $\\xi \\ge 0$ 是用户指定的裕度，代表了相比当前最优值的最小改进量。仅使用 $I(x)$ 的定义、$f(x)$ 的高斯预测模型以及高斯（正态）分布的性质，推导“改进概率”（Probability of Improvement, PI）采集函数的闭式解析表达式。该函数定义为在 $x$ 处的预测分布中的一次抽样超过 $f^{\\star} + \\xi$ 的概率。然后，对于一个锂离子正极设计场景，其中 $f^{\\star} = 195\\,\\mathrm{mAh/g}$，裕度 $\\xi = 2\\,\\mathrm{mAh/g}$，一个候选点 $x$ 的 $\\mu(x) = 192\\,\\mathrm{mAh/g}$ 且 $\\sigma(x) = 10\\,\\mathrm{mAh/g}$，对 PI 进行数值评估。将数值 PI 表示为小数，并将答案四舍五入到四位有效数字。最后，构建并讨论一对科学上合理的候选点 $x_{1}$ 和 $x_{2}$，使得它们的 PI 值相等，但“期望改进”（Expected Improvement, EI）采集函数值却有显著差异，并解释为何 PI 相对于 EI 会失效，原因是 PI 对改进的幅度不敏感。您可以假设标准正态累积分布函数和概率密度函数是明确定义的，并可以根据需要使用，但未经推导不得使用任何快捷公式。",
            "solution": "该问题要求推导“改进概率”（PI）采集函数，对一个特定场景进行数值计算，并对 PI 与“期望改进”（EI）采集函数进行概念性比较和讨论。此问题具有科学依据，提法得当，且客观，因此被认为是有效的。\n\n### 第一部分：改进概率（PI）的推导\n\n改进概率 $\\text{PI}(x)$ 定义为：在候选设计点 $x$ 进行一次新观测 $f(x)$，其值比迄今为止的最佳观测值 $f^\\star$ 至少大一个裕度 $\\xi$ 的概率。用数学形式表示为：\n$$\n\\text{PI}(x) = P(f(x)  f^\\star + \\xi)\n$$\n问题指出，放电容量 $f(x)$ 由高斯过程建模，其在 $x$ 点的后验预测分布是均值为 $\\mu(x)$、方差为 $\\sigma^2(x)$ 的正态分布：\n$$\nf(x) \\mid \\mathcal{D} \\sim \\mathcal{N}(\\mu(x), \\sigma^2(x))\n$$\n为计算该概率，我们将随机变量 $f(x)$ 标准化为一个标准正态随机变量 $Z \\sim \\mathcal{N}(0, 1)$。标准化变换为：\n$$\nZ = \\frac{f(x) - \\mu(x)}{\\sigma(x)}\n$$\n我们将此变换应用于概率表达式内的不等式：\n$$\nf(x)  f^\\star + \\xi\n$$\n两边同时减去均值 $\\mu(x)$：\n$$\nf(x) - \\mu(x)  f^\\star + \\xi - \\mu(x)\n$$\n假设 $\\sigma(x)  0$（方差为零的退化情况对优化无益），我们可以用标准差 $\\sigma(x)$ 对不等式两边进行相除，而不改变不等号的方向：\n$$\n\\frac{f(x) - \\mu(x)}{\\sigma(x)}  \\frac{f^\\star + \\xi - \\mu(x)}{\\sigma(x)}\n$$\n左边是我们的标准正态变量 $Z$。因此，概率变为：\n$$\n\\text{PI}(x) = P\\left(Z  \\frac{f^\\star + \\xi - \\mu(x)}{\\sigma(x)}\\right)\n$$\n设 $\\Phi(z)$ 为标准正态分布的累积分布函数（CDF），定义为 $\\Phi(z) = P(Z \\le z)$。$Z$ 超过一个值 $z$ 的概率由 $P(Z  z) = 1 - P(Z \\le z) = 1 - \\Phi(z)$ 给出。\n使用这个性质，我们得到：\n$$\n\\text{PI}(x) = 1 - \\Phi\\left(\\frac{f^\\star + \\xi - \\mu(x)}{\\sigma(x)}\\right)\n$$\n由于标准正态分布的对称性，我们有性质 $1 - \\Phi(z) = \\Phi(-z)$。应用此性质，我们可以将表达式写得更紧凑：\n$$\n\\text{PI}(x) = \\Phi\\left(-\\left(\\frac{f^\\star + \\xi - \\mu(x)}{\\sigma(x)}\\right)\\right) = \\Phi\\left(\\frac{\\mu(x) - f^\\star - \\xi}{\\sigma(x)}\\right)\n$$\n这就是“改进概率”采集函数的闭式解析表达式。为方便起见，我们将 CDF 的参数定义为 $\\gamma(x)$：\n$$\n\\gamma(x) = \\frac{\\mu(x) - f^\\star - \\xi}{\\sigma(x)}\n$$\n那么，最终表达式为 $\\text{PI}(x) = \\Phi(\\gamma(x))$。\n\n### 第二部分：数值计算\n\n对于一个锂离子正极设计场景，我们已知以下数值：\n- 最佳观测容量：$f^\\star = 195\\,\\mathrm{mAh/g}$\n- 裕度：$\\xi = 2\\,\\mathrm{mAh/g}$\n- 候选点 $x$ 处的预测均值：$\\mu(x) = 192\\,\\mathrm{mAh/g}$\n- 候选点 $x$ 处的预测标准差：$\\sigma(x) = 10\\,\\mathrm{mAh/g}$\n\n首先，我们计算标准化变量 $\\gamma(x)$ 的值：\n$$\n\\gamma(x) = \\frac{\\mu(x) - f^\\star - \\xi}{\\sigma(x)} = \\frac{192 - 195 - 2}{10} = \\frac{-5}{10} = -0.5\n$$\n那么，改进概率为：\n$$\n\\text{PI}(x) = \\Phi(-0.5)\n$$\n使用标准正态分布表或计算库，CDF 在 $z = -0.5$ 处的值约为 $\\Phi(-0.5) \\approx 0.3085375...$。\n按要求将此值四舍五入到四位有效数字，得到：\n$$\n\\text{PI}(x) \\approx 0.3085\n$$\n\n### 第三部分：PI 与 EI 的比较\n\n为了说明 PI 的局限性，我们必须构建一个场景，其中有两个候选设计点 $x_1$ 和 $x_2$，使得 $\\text{PI}(x_1) = \\text{PI}(x_2)$，但它们的期望改进 $\\text{EI}(x_1)$ 和 $\\text{EI}(x_2)$ 却有显著差异。\n\n条件 $\\text{PI}(x_1) = \\text{PI}(x_2)$ 意味着 $\\Phi(\\gamma(x_1)) = \\Phi(\\gamma(x_2))$。由于 $\\Phi$ 是一个单调递增函数，这等价于 $\\gamma(x_1) = \\gamma(x_2)$。\n$$\n\\frac{\\mu(x_1) - f^\\star - \\xi}{\\sigma(x_1)} = \\frac{\\mu(x_2) - f^\\star - \\xi}{\\sigma(x_2)}\n$$\n“期望改进”采集函数定义为 $\\text{EI}(x) = \\mathbb{E}[\\max(f(x) - f^{\\star} - \\xi, 0)]$。其闭式表达式为：\n$$\n\\text{EI}(x) = (\\mu(x) - f^\\star - \\xi) \\Phi(\\gamma(x)) + \\sigma(x) \\phi(\\gamma(x))\n$$\n其中 $\\phi(\\cdot)$ 是标准正态概率密度函数（PDF）。将 $\\mu(x) - f^\\star - \\xi = \\gamma(x)\\sigma(x)$ 代入 EI 公式，我们得到：\n$$\n\\text{EI}(x) = (\\gamma(x)\\sigma(x)) \\Phi(\\gamma(x)) + \\sigma(x) \\phi(\\gamma(x)) = \\sigma(x) \\left[ \\gamma(x)\\Phi(\\gamma(x)) + \\phi(\\gamma(x)) \\right]\n$$\n如果我们设定 $\\gamma(x_1) = \\gamma(x_2) = \\gamma_0$，那么方括号中的项对于两个候选点来说就成了一个常数。EI 值则为：\n$\\text{EI}(x_1) = \\sigma(x_1) \\left[ \\gamma_0\\Phi(\\gamma_0) + \\phi(\\gamma_0) \\right]$\n$\\text{EI}(x_2) = \\sigma(x_2) \\left[ \\gamma_0\\Phi(\\gamma_0) + \\phi(\\gamma_0) \\right]$\nEI 值的比率就是标准差的比率：$\\frac{\\text{EI}(x_2)}{\\text{EI}(x_1)} = \\frac{\\sigma(x_2)}{\\sigma(x_1)}$。因此，如果我们选择 $\\sigma(x_1) \\neq \\sigma(x_2)$，EI 值就会不同。\n\n让我们构建一对合理的候选点，使用相同的上下文 $f^\\star = 195\\,\\mathrm{mAh/g}$ 和 $\\xi = 2\\,\\mathrm{mAh/g}$，因此改进阈值为 $f^\\star + \\xi = 197\\,\\mathrm{mAh/g}$。我们选择一个共同的值 $\\gamma_0 = 0.5$。\n\n- **候选点 $x_1$（低不确定性，利用性选择）：**\n  假设预测不确定性较低，例如 $\\sigma(x_1) = 4\\,\\mathrm{mAh/g}$。\n  为满足 $\\gamma(x_1) = 0.5$，我们需要 $\\frac{\\mu(x_1) - 197}{4} = 0.5$。\n  这给出的预测均值为 $\\mu(x_1) = 197 + 0.5 \\times 4 = 199\\,\\mathrm{mAh/g}$。\n  所以，$x_1$ 的参数为 $(\\mu_1, \\sigma_1) = (199, 4)$。\n\n- **候选点 $x_2$（高不确定性，探索性选择）：**\n  假设预测不确定性较高，例如 $\\sigma(x_2) = 12\\,\\mathrm{mAh/g}$，是 $x_1$ 的三倍。\n  为满足 $\\gamma(x_2) = 0.5$，我们需要 $\\frac{\\mu(x_2) - 197}{12} = 0.5$。\n  这给出的预测均值为 $\\mu(x_2) = 197 + 0.5 \\times 12 = 203\\,\\mathrm{mAh/g}$。\n  所以，$x_2$ 的参数为 $(\\mu_2, \\sigma_2) = (203, 12)$。\n\n对于这两个候选点，改进概率均为 $\\text{PI}(x_1) = \\text{PI}(x_2) = \\Phi(0.5) \\approx 0.6915$。从 PI 的角度来看，这两个候选点同样可取。\n\n然而，它们的期望改进值将有显著不同。方括号中的项为 $C = [0.5 \\cdot \\Phi(0.5) + \\phi(0.5)]$。\n$\\text{EI}(x_1) = 4 \\times C$\n$\\text{EI}(x_2) = 12 \\times C = 3 \\times \\text{EI}(x_1)$\n\n**讨论：**\nPI 采集函数在这种情况下会失效，因为它对潜在改进的*幅度*不敏感。它只记录发生改进的概率，将所有改进（无论是微小的还是巨大的）都同等对待。候选点 $x_1$ 提供了一个获得小幅改进的高概率（其均值仅略高于阈值，且方差很小）。候选点 $x_2$ 具有相同的改进概率，但其更高的均值和大得多的方差意味着有很大机会获得非常大的改进。$f(x_2)$ 更宽的分布意味着其对应大容量值的上尾部，与 $f(x_1)$ 相比，在远离均值的地方拥有更多的概率质量。\n\nEI 采集函数通过计算改进的期望值，正确地平衡了改进的概率与其可能的幅度。它在改进区域上对改进量（$f(x) - (f^\\star + \\xi)$）与其概率密度的乘积进行积分。因此，EI 正确地将 $x_2$ 识别为更有希望的候选点，因为获得颠覆性重大发现的潜力，超过了 $x_1$ 所提供的获得微小改进的相似概率。这使得 EI 成为一种更有效的指导搜索策略，因为它自然地平衡了“利用”（在已知的好点附近采样，如 $x_1$）和“探索”（在高不确定性区域采样，如 $x_2$，这可能导致发现新的最优点）。而 PI 因为“对幅度不敏感”，可能会陷入对局部微小改进的利用中，从而无法探索具有高潜在收益的区域。",
            "answer": "$$\\boxed{\\begin{pmatrix} \\Phi\\left(\\frac{\\mu(x) - f^{\\star} - \\xi}{\\sigma(x)}\\right)  0.3085 \\end{pmatrix}}$$"
        }
    ]
}