{
    "hands_on_practices": [
        {
            "introduction": "The foundational step in applying machine learning to materials discovery is translating a chemical composition into a fixed-length numerical feature vector that a model can process. This exercise focuses on creating these features using compositional descriptors, a widely adopted approach. You will practice calculating statistical moments, such as the weighted mean and variance, of elemental properties to generate a \"fingerprint\" for a material, a technique central to frameworks like MAGPIE . Mastering this skill allows you to build the input for any composition-based materials model and critically assess the powerful, yet simplified, \"bag-of-atoms\" assumption that underpins this method.",
            "id": "3926519",
            "problem": "In an automated battery design and simulation pipeline for layered oxide cathode materials, screening is accelerated by using compositional descriptors derived from the Materials-Agnostic Platform for Informatics and Exploration (MAGPIE). These descriptors aggregate elemental properties across a composition via statistical summaries computed with stoichiometric atomic fractions. As a context-appropriate fundamental base, you may use the following: for a compound with stoichiometric coefficients, the atomic fraction $x_{i}$ of element $i$ is its stoichiometric coefficient divided by the sum of all stoichiometric coefficients; for a property $p_{i}$ associated with element $i$, the composition-weighted mean is $\\mu_{p}=\\sum_{i}x_{i}p_{i}$ and the composition-weighted variance is $\\sigma_{p}^{2}=\\sum_{i}x_{i}\\left(p_{i}-\\mu_{p}\\right)^{2}$.\n\nConsider the layered oxide $\\mathrm{LiNi_{0.6}Mn_{0.2}Co_{0.2}O_{2}}$ with stoichiometry $\\mathrm{Li}_{1}\\mathrm{Ni}_{0.6}\\mathrm{Mn}_{0.2}\\mathrm{Co}_{0.2}\\mathrm{O}_{2}$. Use the following elemental properties:\n- Pauling electronegativity $\\chi$:\n  - $\\mathrm{Li}$: $\\chi=0.98$\n  - $\\mathrm{Ni}$: $\\chi=1.91$\n  - $\\mathrm{Mn}$: $\\chi=1.55$\n  - $\\mathrm{Co}$: $\\chi=1.88$\n  - $\\mathrm{O}$: $\\chi=3.44$\n- Covalent radius $r$ (in $\\mathrm{\\AA}$):\n  - $\\mathrm{Li}$: $r=1.28$\n  - $\\mathrm{Ni}$: $r=1.24$\n  - $\\mathrm{Mn}$: $r=1.39$\n  - $\\mathrm{Co}$: $r=1.26$\n  - $\\mathrm{O}$: $r=0.66$\n\nConstruct the MAGPIE-style descriptor components for this composition for the two properties above by computing the composition-weighted mean electronegativity $\\mu_{\\chi}$, the composition-weighted variance $\\sigma_{\\chi}^{2}$, and the composition-weighted mean covalent radius $\\mu_{r}$. From these, define a scalar screening score\n$$\nS=\\alpha\\,\\mu_{\\chi}+\\beta\\,\\sigma_{\\chi}^{2}+\\gamma\\,\\mu_{r},\n$$\nwith coefficients $\\alpha=0.90$, $\\beta=-0.25$, and $\\gamma=0.30\\,\\mathrm{\\AA}^{-1}$ chosen to yield a dimensionless score. Compute $S$ and round your final answer to four significant figures. Report $S$ as a dimensionless quantity. Additionally, explain the assumptions underlying the statistical aggregation of elemental properties in this descriptor construction, including how these assumptions interface with machine learning for accelerated materials screening.",
            "solution": "The problem statement is evaluated to be valid. It is scientifically grounded in the principles of computational materials science and machine learning for materials discovery. The compound $\\mathrm{LiNi_{0.6}Mn_{0.2}Co_{0.2}O_{2}}$ is a well-established cathode material. The use of MAGPIE-style descriptors, based on statistical aggregation of elemental properties, is a standard technique in materials informatics. The problem is well-posed, providing all necessary definitions, data, and constants to arrive at a unique, verifiable solution. The language is objective and the tasks are unambiguous.\n\nThe solution proceeds in two parts: first, the numerical computation of the screening score $S$; second, an explanation of the underlying assumptions and their context in machine learning.\n\nPart 1: Computation of the Screening Score $S$\n\nThe first step is to determine the atomic fraction, $x_i$, for each element in the compound $\\mathrm{Li}_{1}\\mathrm{Ni}_{0.6}\\mathrm{Mn}_{0.2}\\mathrm{Co}_{0.2}\\mathrm{O}_{2}$. The stoichiometric coefficients are $s_{\\mathrm{Li}}=1$, $s_{\\mathrm{Ni}}=0.6$, $s_{\\mathrm{Mn}}=0.2$, $s_{\\mathrm{Co}}=0.2$, and $s_{\\mathrm{O}}=2$.\n\nThe sum of the stoichiometric coefficients, $S_{total}$, is:\n$$\nS_{total} = s_{\\mathrm{Li}} + s_{\\mathrm{Ni}} + s_{\\mathrm{Mn}} + s_{\\mathrm{Co}} + s_{\\mathrm{O}} = 1 + 0.6 + 0.2 + 0.2 + 2 = 4\n$$\nThe atomic fraction $x_i$ for each element $i$ is calculated as $x_i = s_i / S_{total}$:\n$$\nx_{\\mathrm{Li}} = \\frac{1}{4} = 0.25\n$$\n$$\nx_{\\mathrm{Ni}} = \\frac{0.6}{4} = 0.15\n$$\n$$\nx_{\\mathrm{Mn}} = \\frac{0.2}{4} = 0.05\n$$\n$$\nx_{\\mathrm{Co}} = \\frac{0.2}{4} = 0.05\n$$\n$$\nx_{\\mathrm{O}} = \\frac{2}{4} = 0.50\n$$\nThe sum of the atomic fractions is $\\sum x_i = 0.25 + 0.15 + 0.05 + 0.05 + 0.50 = 1$, as required.\n\nNext, we compute the composition-weighted mean electronegativity, $\\mu_{\\chi}$, using the formula $\\mu_{p}=\\sum_{i}x_{i}p_{i}$ and the given Pauling electronegativity values $\\chi_i$:\n$$\n\\mu_{\\chi} = x_{\\mathrm{Li}}\\chi_{\\mathrm{Li}} + x_{\\mathrm{Ni}}\\chi_{\\mathrm{Ni}} + x_{\\mathrm{Mn}}\\chi_{\\mathrm{Mn}} + x_{\\mathrm{Co}}\\chi_{\\mathrm{Co}} + x_{\\mathrm{O}}\\chi_{\\mathrm{O}}\n$$\n$$\n\\mu_{\\chi} = (0.25)(0.98) + (0.15)(1.91) + (0.05)(1.55) + (0.05)(1.88) + (0.50)(3.44)\n$$\n$$\n\\mu_{\\chi} = 0.245 + 0.2865 + 0.0775 + 0.094 + 1.72 = 2.423\n$$\n\nThen, we compute the composition-weighted variance of electronegativity, $\\sigma_{\\chi}^{2}$, using the formula $\\sigma_{p}^{2}=\\sum_{i}x_{i}(p_{i}-\\mu_{p})^{2}$:\n$$\n\\sigma_{\\chi}^{2} = x_{\\mathrm{Li}}(\\chi_{\\mathrm{Li}}-\\mu_{\\chi})^2 + x_{\\mathrm{Ni}}(\\chi_{\\mathrm{Ni}}-\\mu_{\\chi})^2 + x_{\\mathrm{Mn}}(\\chi_{\\mathrm{Mn}}-\\mu_{\\chi})^2 + x_{\\mathrm{Co}}(\\chi_{\\mathrm{Co}}-\\mu_{\\chi})^2 + x_{\\mathrm{O}}(\\chi_{\\mathrm{O}}-\\mu_{\\chi})^2\n$$\n$$\n\\sigma_{\\chi}^{2} = (0.25)(0.98-2.423)^2 + (0.15)(1.91-2.423)^2 + (0.05)(1.55-2.423)^2 + (0.05)(1.88-2.423)^2 + (0.50)(3.44-2.423)^2\n$$\n$$\n\\sigma_{\\chi}^{2} = (0.25)(-1.443)^2 + (0.15)(-0.513)^2 + (0.05)(-0.873)^2 + (0.05)(-0.543)^2 + (0.50)(1.017)^2\n$$\n$$\n\\sigma_{\\chi}^{2} = (0.25)(2.082249) + (0.15)(0.263169) + (0.05)(0.762129) + (0.05)(0.294849) + (0.50)(1.034289)\n$$\n$$\n\\sigma_{\\chi}^{2} \\approx 0.52056 + 0.03948 + 0.03811 + 0.01474 + 0.51714 = 1.13003\n$$\n\nNext, we compute the composition-weighted mean covalent radius, $\\mu_{r}$, using the given radii $r_i$:\n$$\n\\mu_{r} = x_{\\mathrm{Li}}r_{\\mathrm{Li}} + x_{\\mathrm{Ni}}r_{\\mathrm{Ni}} + x_{\\mathrm{Mn}}r_{\\mathrm{Mn}} + x_{\\mathrm{Co}}r_{\\mathrm{Co}} + x_{\\mathrm{O}}r_{\\mathrm{O}}\n$$\n$$\n\\mu_{r} = (0.25)(1.28\\,\\mathrm{\\AA}) + (0.15)(1.24\\,\\mathrm{\\AA}) + (0.05)(1.39\\,\\mathrm{\\AA}) + (0.05)(1.26\\,\\mathrm{\\AA}) + (0.50)(0.66\\,\\mathrm{\\AA})\n$$\n$$\n\\mu_{r} = 0.32\\,\\mathrm{\\AA} + 0.186\\,\\mathrm{\\AA} + 0.0695\\,\\mathrm{\\AA} + 0.063\\,\\mathrm{\\AA} + 0.33\\,\\mathrm{\\AA} = 0.9685\\,\\mathrm{\\AA}\n$$\n\nFinally, we compute the scalar screening score $S$ using the given formula and coefficients $\\alpha=0.90$, $\\beta=-0.25$, and $\\gamma=0.30\\,\\mathrm{\\AA}^{-1}$:\n$$\nS = \\alpha\\,\\mu_{\\chi} + \\beta\\,\\sigma_{\\chi}^{2} + \\gamma\\,\\mu_{r}\n$$\n$$\nS = (0.90)(2.423) + (-0.25)(1.13003) + (0.30\\,\\mathrm{\\AA}^{-1})(0.9685\\,\\mathrm{\\AA})\n$$\n$$\nS = 2.1807 - 0.2825075 + 0.29055\n$$\n$$\nS \\approx 2.1887425\n$$\nRounding to four significant figures, we get $S \\approx 2.189$.\n\nPart 2: Assumptions and Context in Machine Learning\n\nThe construction of these descriptors rests on several key assumptions, which represent a deliberate trade-off between physical accuracy and computational efficiency.\n\n1.  **Atomistic Additivity and Homogenization**: The fundamental assumption is that a macroscopic or bulk property of a material can be approximated by a statistical aggregation of the intrinsic properties of its constituent elements. This \"bag of atoms\" approach effectively homogenizes the material, treating it as a simple mixture. It largely ignores the complex, specific details of the crystal structure, local coordination environments, bond types (ionic, covalent, metallic), oxidation states, and long-range electronic and structural interactions. For instance, the Pauling electronegativity of an element is a fixed value, but its effective electronegativity within a compound is modulated by its chemical environment.\n\n2.  **Stoichiometric Proportionality**: The use of atomic fractions as weights presumes that each atom's contribution to the aggregate property is directly proportional to its abundance in the chemical formula. This is a first-order approximation that does not account for the non-linear effects that arise from the specific arrangement and interaction of different atoms. A minority element, for example, might have a disproportionately large effect on a property if it occupies a critical site in the crystal lattice.\n\n3.  **Context-Independent Elemental Properties**: The model uses elemental properties (e.g., covalent radius) that are defined for an element in a reference state. The problem specifies covalent radius, which is most relevant for covalent bonds. However, a material like $\\mathrm{LiNi_{0.6}Mn_{0.2}Co_{0.2}O_{2}}$ has bonding with significant ionic character. Using covalent radius instead of, for example, ionic radius is a simplification. The descriptor generation framework accepts this imperfection, hypothesizing that even if the absolute values are not perfectly representative, their relative trends and weighted averages still contain predictive information.\n\nThese assumptions are crucial for the interface with machine learning for accelerated materials screening:\n\n-   **Feature Engineering**: The core task of machine learning is to learn a mapping from input features to an output target. These statistically-aggregated descriptors ($\\mu_{\\chi}$, $\\sigma_{\\chi}^{2}$, $\\mu_{r}$, etc.) serve as a fixed-length numerical feature vector that represents a chemical composition. This transformation from a chemical formula to a vector is essential for the application of standard machine learning algorithms.\n\n-   **Abstraction and Dimensionality Reduction**: Instead of describing a material by the explicit coordinates of every atom in a unit cell (a high-dimensional and variable-size representation), these descriptors provide a low-dimensional, composition-based abstraction. This simplification is what makes the screening process computationally tractable.\n\n-   **Surrogate Modeling**: The purpose of this approach is to create a fast, approximate \"surrogate model\". Instead of conducting expensive, high-fidelity quantum mechanical simulations (like Density Functional Theory) or time-consuming laboratory experiments for every one of millions of potential material compositions, one can calculate these simple descriptors nearly instantaneously. A machine learning model, trained on a dataset where both descriptors and a target property (e.g., battery voltage, ionic conductivity) are known, learns the complex, non-linear relationships between the approximate features and the true material behavior.\n\n-   **Accelerated Screening Pipeline**: The machine learning model, once trained, can predict the target property for millions of hypothetical compositions in minutes or hours. This allows researchers to rapidly screen a vast chemical space and down-select a small number of promising candidates for more rigorous, expensive validation. The success of this methodology hinges on the fact that while the underlying assumptions are physically simplistic, the resulting features retain enough of the essential chemical and physical \"signal\" for the machine learning model to identify meaningful correlations and make useful predictions. The variance term, $\\sigma_{\\chi}^{2}$, is particularly important as it captures the degree of elemental heterogeneity, a property that a simple mean would miss entirely.",
            "answer": "$$\n\\boxed{2.189}\n$$"
        },
        {
            "introduction": "With a predictive model in hand, the challenge shifts from prediction to strategic exploration. Instead of exhaustively testing every possibility, we can use the model to intelligently guide our search for the next, most informative experiment. This practice introduces Bayesian Optimization, a core methodology for accelerated screening, through its popular Upper Confidence Bound (UCB) acquisition function . You will learn how UCB formalizes the crucial trade-off between exploiting known high-performing materials (regions of high predicted performance) and exploring uncertain regions of the chemical space (regions of high model uncertainty), providing a principled way to navigate vast design spaces efficiently.",
            "id": "3926501",
            "problem": "A battery research team is using a Gaussian process (GP) surrogate to accelerate screening of candidate solid electrolyte compositions along a one-dimensional lithium fraction axis $x \\in [0,1]$. The target property is the log-scale ionic conductivity, modeled as a GP posterior predictive distribution at any candidate $x$ that is Gaussian with mean $\\mu(x)$ and standard deviation $\\sigma(x)$, computed from a squared-exponential kernel and prior training data. To select the next experiment, the team uses the Upper Confidence Bound (UCB) acquisition, defined by the upper $q$-quantile of the Gaussian predictive distribution at each $x$, where the “exploration rate” is the quantile level $q \\in (0.5,1)$ and the corresponding UCB is given by the $q$-quantile $\\mu(x) + \\kappa\\,\\sigma(x)$ with $\\kappa = \\Phi^{-1}(q)$ and $\\Phi^{-1}$ the inverse cumulative distribution function of the standard normal distribution.\n\nYou are provided with the following precomputed GP posterior predictive means and standard deviations at three candidate compositions $x_1=0.21$, $x_2=0.58$, and $x_3=0.83$. These values were obtained from the same kernel hyperparameters but under two homoskedastic observation-noise scenarios representative of different experimental setups:\n\n- Means (approximately invariant across the two noise scenarios due to dense training data): $\\mu(x_1)=2.10$, $\\mu(x_2)=2.05$, $\\mu(x_3)=1.95$.\n- Low-noise predictive standard deviations: $\\sigma_{\\mathrm{L}}(x_1)=0.06$, $\\sigma_{\\mathrm{L}}(x_2)=0.09$, $\\sigma_{\\mathrm{L}}(x_3)=0.14$.\n- High-noise predictive standard deviations: $\\sigma_{\\mathrm{H}}(x_1)=0.10$, $\\sigma_{\\mathrm{H}}(x_2)=0.13$, $\\sigma_{\\mathrm{H}}(x_3)=0.20$.\n\nAssume the team targets an exploration rate (quantile level) of $q=0.90$ for its next selection. Using the definition of the Gaussian $q$-quantile, compute the next query composition $x^{\\star}$ by maximizing the UCB under the low-noise predictive standard deviations. Then, based on the same definitions and data, analyze how the identity of $x^{\\star}$ would change as a function of $\\kappa$ (equivalently $q$) under both the low-noise and high-noise standard deviations, deriving the critical values of $\\kappa$ at which the maximizer switches between candidates. Your analysis should start from the definitions of the GP predictive distribution and the UCB quantile interpretation, and proceed by first principles.\n\nReport only the final selected composition $x^{\\star}$ for the low-noise case as a pure number in $[0,1]$, rounded to three significant figures. Do not include any units in your final numerical answer.",
            "solution": "The problem is first validated to ensure it is well-posed, scientifically grounded, and contains all necessary information.\n\n**Step 1: Extract Givens**\n- Domain for lithium fraction: $x \\in [0,1]$.\n- Model: Gaussian Process (GP) with posterior predictive distribution $N(\\mu(x), \\sigma(x)^2)$.\n- Acquisition function: Upper Confidence Bound (UCB), defined as the $q$-quantile of the predictive distribution.\n- UCB formula: $\\text{UCB}(x) = \\mu(x) + \\kappa\\,\\sigma(x)$, where $\\kappa = \\Phi^{-1}(q)$.\n- Quantile level for initial selection: $q = 0.90$.\n- Candidate compositions: $x_1=0.21$, $x_2=0.58$, $x_3=0.83$.\n- Predictive means: $\\mu(x_1)=2.10$, $\\mu(x_2)=2.05$, $\\mu(x_3)=1.95$.\n- Low-noise predictive standard deviations: $\\sigma_{\\mathrm{L}}(x_1)=0.06$, $\\sigma_{\\mathrm{L}}(x_2)=0.09$, $\\sigma_{\\mathrm{L}}(x_3)=0.14$.\n- High-noise predictive standard deviations: $\\sigma_{\\mathrm{H}}(x_1)=0.10$, $\\sigma_{\\mathrm{H}}(x_2)=0.13$, $\\sigma_{\\mathrm{H}}(x_3)=0.20$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem describes a standard Bayesian optimization loop using a GP surrogate and the UCB acquisition function, a widely accepted method in machine learning for materials science. All concepts are standard in statistics and machine learning.\n- **Well-Posed:** The problem provides a discrete set of candidates and a well-defined objective function to maximize. It supplies all necessary numerical values to perform the calculations and the analysis.\n- **Objective:** The problem is stated in precise, quantitative terms, free of ambiguity or subjective claims.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A complete solution will be provided.\n\n**Solution Derivation**\n\nThe core of the problem is to identify which candidate composition $x_i$ maximizes the Upper Confidence Bound (UCB) acquisition function, defined as:\n$$\n\\text{UCB}(x) = \\mu(x) + \\kappa\\,\\sigma(x)\n$$\nHere, $\\mu(x)$ is the posterior mean, $\\sigma(x)$ is the posterior standard deviation, and $\\kappa = \\Phi^{-1}(q)$ is a parameter controlling the trade-off between exploitation (high mean) and exploration (high uncertainty), with $\\Phi^{-1}$ being the inverse cumulative distribution function (CDF) of the standard normal distribution. The problem asks us to first find the maximizer for a specific quantile $q=0.90$ under low-noise conditions, and then to analyze how the maximizer changes as a function of $\\kappa$ for both low- and high-noise scenarios.\n\n**Part 1: Selection for the Low-Noise Case with $q=0.90$**\n\nFor an exploration rate (quantile level) of $q=0.90$, the parameter $\\kappa$ is:\n$$\n\\kappa = \\Phi^{-1}(0.90) \\approx 1.28155\n$$\nWe now compute the UCB value for each of the three candidates using the low-noise standard deviations, $\\sigma_{\\mathrm{L}}$.\n\nFor candidate $x_1=0.21$:\n$$\n\\text{UCB}_{\\mathrm{L}}(x_1) = \\mu(x_1) + \\kappa\\,\\sigma_{\\mathrm{L}}(x_1) = 2.10 + (1.28155)(0.06) \\approx 2.10 + 0.07689 = 2.17689\n$$\n\nFor candidate $x_2=0.58$:\n$$\n\\text{UCB}_{\\mathrm{L}}(x_2) = \\mu(x_2) + \\kappa\\,\\sigma_{\\mathrm{L}}(x_2) = 2.05 + (1.28155)(0.09) \\approx 2.05 + 0.11534 = 2.16534\n$$\n\nFor candidate $x_3=0.83$:\n$$\n\\text{UCB}_{\\mathrm{L}}(x_3) = \\mu(x_3) + \\kappa\\,\\sigma_{\\mathrm{L}}(x_3) = 1.95 + (1.28155)(0.14) \\approx 1.95 + 0.17942 = 2.12942\n$$\n\nComparing the three values:\n$$\n\\text{UCB}_{\\mathrm{L}}(x_1) > \\text{UCB}_{\\mathrm{L}}(x_2) > \\text{UCB}_{\\mathrm{L}}(x_3)\n$$\nThe composition that maximizes the UCB is $x_1$. Therefore, the next query composition is $x^{\\star} = x_1 = 0.21$.\n\n**Part 2: Analysis of Critical Values of $\\kappa$**\n\nThe identity of the optimal candidate $x^{\\star}$ depends on the value of $\\kappa$. For each candidate $i$, the UCB is a linear function of $\\kappa$: $\\text{UCB}_i(\\kappa) = \\mu_i + \\kappa\\sigma_i$. The optimal choice switches from candidate $i$ to candidate $j$ at a critical value of $\\kappa$ where their UCBs are equal:\n$$\n\\mu_i + \\kappa_{ij}\\sigma_i = \\mu_j + \\kappa_{ij}\\sigma_j\n$$\nSolving for the critical value $\\kappa_{ij}$ yields:\n$$\n\\kappa_{ij} = \\frac{\\mu_i - \\mu_j}{\\sigma_j - \\sigma_i}\n$$\nWe are interested in positive values of $\\kappa$ since $q \\in (0.5, 1)$ implies $\\kappa > 0$.\n\n**Analysis for the Low-Noise Case ($\\sigma_{\\mathrm{L}}$):**\n- Crossover between $x_1$ and $x_2$:\n$$\n\\kappa_{12,\\mathrm{L}} = \\frac{\\mu(x_1) - \\mu(x_2)}{\\sigma_{\\mathrm{L}}(x_2) - \\sigma_{\\mathrm{L}}(x_1)} = \\frac{2.10 - 2.05}{0.09 - 0.06} = \\frac{0.05}{0.03} = \\frac{5}{3} \\approx 1.667\n$$\n- Crossover between $x_2$ and $x_3$:\n$$\n\\kappa_{23,\\mathrm{L}} = \\frac{\\mu(x_2) - \\mu(x_3)}{\\sigma_{\\mathrm{L}}(x_3) - \\sigma_{\\mathrm{L}}(x_2)} = \\frac{2.05 - 1.95}{0.14 - 0.09} = \\frac{0.10}{0.05} = 2\n$$\n- Crossover between $x_1$ and $x_3$:\n$$\n\\kappa_{13,\\mathrm{L}} = \\frac{\\mu(x_1) - \\mu(x_3)}{\\sigma_{\\mathrm{L}}(x_3) - \\sigma_{\\mathrm{L}}(x_1)} = \\frac{2.10 - 1.95}{0.14 - 0.06} = \\frac{0.15}{0.08} = \\frac{15}{8} = 1.875\n$$\nFor $\\kappa=0$, $x_1$ is optimal since $\\mu(x_1)$ is the largest. It remains optimal until its UCB is surpassed. The first crossover involving $x_1$ is with $x_2$ at $\\kappa \\approx 1.667$. The crossover from $x_2$ to $x_3$ occurs later at $\\kappa = 2$.\nThus, for the low-noise case, the sequence of optimal candidates is:\n- $x_1$ is optimal for $\\kappa \\in [0, 5/3)$.\n- $x_2$ is optimal for $\\kappa \\in [5/3, 2)$.\n- $x_3$ is optimal for $\\kappa \\ge 2$.\nOur value $\\kappa \\approx 1.28155$ falls in the first interval, confirming $x_1$ as the correct choice.\n\n**Analysis for the High-Noise Case ($\\sigma_{\\mathrm{H}}$):**\n- Crossover between $x_1$ and $x_2$:\n$$\n\\kappa_{12,\\mathrm{H}} = \\frac{\\mu(x_1) - \\mu(x_2)}{\\sigma_{\\mathrm{H}}(x_2) - \\sigma_{\\mathrm{H}}(x_1)} = \\frac{2.10 - 2.05}{0.13 - 0.10} = \\frac{0.05}{0.03} = \\frac{5}{3} \\approx 1.667\n$$\n- Crossover between $x_2$ and $x_3$:\n$$\n\\kappa_{23,\\mathrm{H}} = \\frac{\\mu(x_2) - \\mu(x_3)}{\\sigma_{\\mathrm{H}}(x_3) - \\sigma_{\\mathrm{H}}(x_2)} = \\frac{2.05 - 1.95}{0.20 - 0.13} = \\frac{0.10}{0.07} = \\frac{10}{7} \\approx 1.429\n$$\n- Crossover between $x_1$ and $x_3$:\n$$\n\\kappa_{13,\\mathrm{H}} = \\frac{\\mu(x_1) - \\mu(x_3)}{\\sigma_{\\mathrm{H}}(x_3) - \\sigma_{\\mathrm{H}}(x_1)} = \\frac{2.10 - 1.95}{0.20 - 0.10} = \\frac{0.15}{0.10} = 1.5\n$$\nFor $\\kappa=0$, $x_1$ is optimal. The first critical value involving $x_1$ is $\\kappa_{13,\\mathrm{H}}=1.5$. At this point, $\\text{UCB}_{\\mathrm{H}}(x_1) = \\text{UCB}_{\\mathrm{H}}(x_3)$. For $\\kappa > 1.5$, $\\text{UCB}_{\\mathrm{H}}(x_3) > \\text{UCB}_{\\mathrm{H}}(x_1)$. We must check the status of $x_2$. Since $\\kappa_{23,\\mathrm{H}} \\approx 1.429 < 1.5$, for any $\\kappa > 1.429$, we have $\\text{UCB}_{\\mathrm{H}}(x_3) > \\text{UCB}_{\\mathrm{H}}(x_2)$. Therefore, at the switch point $\\kappa=1.5$, $x_3$ surpasses $x_1$ and is already greater than a declining $x_2$. Candidate $x_2$ is never optimal in the high-noise scenario.\nThus, for the high-noise case, the sequence of optimal candidates is:\n- $x_1$ is optimal for $\\kappa \\in [0, 1.5)$.\n- $x_3$ is optimal for $\\kappa \\ge 1.5$.\n\nThe problem asks only for the final selected composition $x^{\\star}$ for the low-noise case at $q=0.90$. As computed in Part 1, this is $x_1=0.21$. The problem requests the answer rounded to three significant figures.\n\nFinal Answer: $x^{\\star} = 0.210$.",
            "answer": "$$\\boxed{0.210}$$"
        },
        {
            "introduction": "The ultimate goal of materials screening is to make robust decisions that lead to the synthesis of novel, high-performance materials. This task is complicated by the presence of multiple, often conflicting, design objectives (e.g., high stability vs. high conductivity) and the inherent uncertainty in our machine learning predictions. This advanced exercise guides you through robust multi-objective optimization, a framework for decision-making under these real-world constraints . By using chance constraints to manage risk and constructing a Pareto front to visualize trade-offs, you will learn to identify a set of optimal, non-dominated candidates, moving from probabilistic predictions to actionable insights.",
            "id": "3926474",
            "problem": "You are tasked with constructing a robust Pareto set for battery electrode materials screening under prediction uncertainty in two objectives: thermodynamic stability and ionic conductivity. The design context is automated battery design and simulation using machine learning predictions for accelerated materials screening. Each candidate material is characterized by a predictive distribution for the energy above the convex hull, denoted by $\\Delta E_{\\mathrm{hull}}$ in units of millielectronvolts per atom (meV/atom), and a predictive distribution for ionic conductivity $\\sigma$ in units of millisiemens per centimeter (mS/cm).\n\nFundamental base:\n- Suppose the machine learning model outputs the following for each material $i$:\n  - A predictive normal distribution for $\\Delta E_{\\mathrm{hull}, i}$ with mean $\\mu_{E,i}$ and standard deviation $s_{E,i}$, modeled as $\\Delta E_{\\mathrm{hull}, i} \\sim \\mathcal{N}(\\mu_{E,i}, s_{E,i}^2)$.\n  - A predictive log-normal distribution for conductivity $\\sigma_i$ such that $\\ln \\sigma_i \\sim \\mathcal{N}(\\mu_{\\ell,i}, s_{\\ell,i}^2)$, where $\\mu_{\\ell,i}$ and $s_{\\ell,i}$ are the mean and standard deviation of the natural logarithm of the conductivity.\n\n- Robust feasibility is enforced via chance constraints:\n  - Stability chance constraint: $\\mathbb{P}(\\Delta E_{\\mathrm{hull}, i} \\le 0) \\ge 1-\\alpha$.\n  - Conductivity chance constraint: $\\mathbb{P}(\\sigma_i \\ge \\sigma_0) \\ge 1-\\beta$.\n  Here, $\\alpha \\in (0,1)$ and $\\beta \\in (0,1)$ are user-specified risk levels, and $\\sigma_0$ is the conductivity threshold to be met. Interpret $\\Delta E_{\\mathrm{hull}, i} \\le 0$ meV/atom as thermodynamic stability relative to the convex hull, and $\\sigma_i \\ge \\sigma_0$ mS/cm as meeting the minimum required conductivity.\n\n- Robust performance metrics for Pareto comparison under these chance constraints use conservative quantiles:\n  - For stability, use the upper $1-\\alpha$ quantile $q_{E,i}^{\\mathrm{up}}$ of $\\Delta E_{\\mathrm{hull}, i}$.\n  - For conductivity, use the lower $\\beta$ quantile $q_{\\sigma,i}^{\\mathrm{low}}$ of $\\sigma_i$.\n  These quantiles are defined using the standard normal quantile $z_p$ satisfying $\\Phi(z_p) = p$ for the cumulative distribution function $\\Phi$. For the normal variable $\\Delta E_{\\mathrm{hull}, i}$, $q_{E,i}^{\\mathrm{up}} = \\mu_{E,i} + z_{1-\\alpha} s_{E,i}$. For the log-normal $\\sigma_i$, $q_{\\sigma,i}^{\\mathrm{low}} = \\exp\\left(\\mu_{\\ell,i} + z_{\\beta} s_{\\ell,i}\\right)$. If $s_{E,i} = 0$ or $s_{\\ell,i} = 0$, interpret the distributions as deterministic so that the corresponding quantiles equal the means of the variables.\n\n- Robust Pareto set definition under feasibility:\n  - First, determine the robustly feasible set $\\mathcal{F} = \\{ i : \\mathbb{P}(\\Delta E_{\\mathrm{hull}, i} \\le 0) \\ge 1-\\alpha,\\, \\mathbb{P}(\\sigma_i \\ge \\sigma_0) \\ge 1-\\beta \\}$.\n  - Among $i \\in \\mathcal{F}$, define candidate $a$ to dominate candidate $b$ in the robust sense if $q_{E,a}^{\\mathrm{up}} \\le q_{E,b}^{\\mathrm{up}}$ and $q_{\\sigma,a}^{\\mathrm{low}} \\ge q_{\\sigma,b}^{\\mathrm{low}}$, with at least one strict inequality. The robust Pareto set $\\mathcal{P} \\subseteq \\mathcal{F}$ is the set of candidates that are not dominated by any other feasible candidate.\n\n- Selection probabilities:\n  - For a decision policy that selects uniformly at random from $\\mathcal{P}$ when $\\mathcal{P} \\neq \\emptyset$ and selects nothing when $\\mathcal{P} = \\emptyset$, the selection probability for candidate $i$ is defined as $p^{\\mathrm{sel}}_i = \\frac{1}{\\lvert \\mathcal{P} \\rvert}$ if $i \\in \\mathcal{P}$ and $p^{\\mathrm{sel}}_i = 0$ otherwise.\n\nAll physical quantities must be treated in their specified units: $\\Delta E_{\\mathrm{hull}}$ in meV/atom and $\\sigma$ in mS/cm. Angles are not part of this problem. All answers are unitless selection probabilities expressed as floating-point numbers.\n\nYour task is to implement a complete, runnable program that:\n- Evaluates robust feasibility via chance constraints for each test case.\n- Constructs the robust Pareto set using the conservative quantile metrics.\n- Computes selection probabilities per candidate as described.\n\nTest suite:\n- Case $1$ (happy path):\n  - Parameters: $\\alpha = 0.1$, $\\beta = 0.1$, $\\sigma_0 = 1.5$ mS/cm.\n  - Candidates (each tuple lists $(\\mu_{E}\\,[\\mathrm{meV/atom}],\\, s_{E}\\,[\\mathrm{meV/atom}],\\, \\mu_{\\ell},\\, s_{\\ell})$):\n    - $\\mathrm{M1}: (-1.0,\\, 0.5,\\, \\ln 1.8,\\, 0.1)$\n    - $\\mathrm{M2}: (-0.5,\\, 0.3,\\, \\ln 1.2,\\, 0.2)$\n    - $\\mathrm{M3}: (1.5,\\, 1.0,\\, \\ln 2.5,\\, 0.3)$\n    - $\\mathrm{M4}: (4.0,\\, 2.0,\\, \\ln 4.0,\\, 0.25)$\n    - $\\mathrm{M5}: (0.1,\\, 0.7,\\, \\ln 0.8,\\, 0.2)$\n- Case $2$ (strict constraints edge case):\n  - Parameters: $\\alpha = 0.01$, $\\beta = 0.05$, $\\sigma_0 = 2.0$ mS/cm.\n  - Candidates: same as Case $1$.\n- Case $3$ (deterministic-variable edge case):\n  - Parameters: $\\alpha = 0.2$, $\\beta = 0.2$, $\\sigma_0 = 1.5$ mS/cm.\n  - Candidates:\n    - $\\mathrm{F1}: (0.0,\\, 0.0,\\, \\ln 1.7,\\, 0.0)$\n    - $\\mathrm{F2}: (-0.5,\\, 0.3,\\, \\ln 2.0,\\, 0.3)$\n    - $\\mathrm{F3}: (4.0,\\, 8.0,\\, \\ln 0.5,\\, 0.4)$\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a list of floating-point selection probabilities in the same order as the candidates in that test case. For example, a valid output structure is of the form $[ [p_1, p_2], [q_1, q_2, q_3] ]$.",
            "solution": "The problem requires the construction of a robust Pareto set for screening battery electrode materials under uncertainty. The goal is to identify a set of non-dominated candidates based on two conflicting objectives: maximizing thermodynamic stability and maximizing ionic conductivity. The uncertainty in the machine learning predictions for these properties is modeled by probability distributions. The solution involves a multi-step process encompassing robust feasibility analysis, multi-objective optimization, and calculation of selection probabilities.\n\nFirst, we must formalize the conditions for a candidate material to be considered 'robustly feasible'. The problem defines feasibility using chance constraints, which set a lower bound on the probability of meeting certain performance thresholds.\n\nFor thermodynamic stability, the property of interest is the energy above the convex hull, $\\Delta E_{\\mathrm{hull}, i}$, for each material $i$. This is modeled by a normal distribution, $\\Delta E_{\\mathrm{hull}, i} \\sim \\mathcal{N}(\\mu_{E,i}, s_{E,i}^2)$. A material is considered stable if its energy is non-positive, i.e., $\\Delta E_{\\mathrm{hull}, i} \\le 0$. The stability chance constraint requires this condition to hold with a probability of at least $1-\\alpha$:\n$$\n\\mathbb{P}(\\Delta E_{\\mathrm{hull}, i} \\le 0) \\ge 1-\\alpha\n$$\nTo make this condition operational, we standardize the random variable. Let $Z \\sim \\mathcal{N}(0, 1)$ be a standard normal variable. The constraint becomes:\n$$\n\\mathbb{P}\\left(\\frac{\\Delta E_{\\mathrm{hull}, i} - \\mu_{E,i}}{s_{E,i}} \\le \\frac{0 - \\mu_{E,i}}{s_{E,i}}\\right) \\ge 1-\\alpha \\quad \\implies \\quad \\Phi\\left(\\frac{-\\mu_{E,i}}{s_{E,i}}\\right) \\ge 1-\\alpha\n$$\nwhere $\\Phi$ is the cumulative distribution function (CDF) of the standard normal distribution. Using the property $\\Phi(-x) = 1-\\Phi(x)$, this is equivalent to $1 - \\Phi\\left(\\frac{\\mu_{E,i}}{s_{E,i}}\\right) \\ge 1-\\alpha$, which simplifies to $\\Phi\\left(\\frac{\\mu_{E,i}}{s_{E,i}}\\right) \\le \\alpha$. Applying the inverse CDF, $\\Phi^{-1}$, also known as the quantile function or probit function, we get $\\frac{\\mu_{E,i}}{s_{E,i}} \\le z_{\\alpha}$, where $z_p = \\Phi^{-1}(p)$. This can be rearranged as $\\mu_{E,i} - z_{\\alpha} s_{E,i} \\le 0$. Using the identity $z_{\\alpha} = -z_{1-\\alpha}$, the condition is $\\mu_{E,i} + z_{1-\\alpha} s_{E,i} \\le 0$. This expression is precisely the upper $1-\\alpha$ quantile of $\\Delta E_{\\mathrm{hull}, i}$, denoted $q_{E,i}^{\\mathrm{up}}$. Thus, the stability feasibility condition is elegantly simplified to:\n$$\nq_{E,i}^{\\mathrm{up}} = \\mu_{E,i} + z_{1-\\alpha} s_{E,i} \\le 0\n$$\n\nFor ionic conductivity, $\\sigma_i$, the property is modeled by a log-normal distribution, such that its natural logarithm follows a normal distribution: $\\ln \\sigma_i \\sim \\mathcal{N}(\\mu_{\\ell,i}, s_{\\ell,i}^2)$. The conductivity chance constraint requires $\\sigma_i$ to be at least a threshold value $\\sigma_0$ with a probability of at least $1-\\beta$:\n$$\n\\mathbb{P}(\\sigma_i \\ge \\sigma_0) \\ge 1-\\beta\n$$\nBy taking the natural logarithm, which is a monotonic function, we can work with the normally distributed variable $\\ln \\sigma_i$:\n$$\n\\mathbb{P}(\\ln \\sigma_i \\ge \\ln \\sigma_0) \\ge 1-\\beta\n$$\nStandardizing this variable yields:\n$$\n\\mathbb{P}\\left(\\frac{\\ln \\sigma_i - \\mu_{\\ell,i}}{s_{\\ell,i}} \\ge \\frac{\\ln \\sigma_0 - \\mu_{\\ell,i}}{s_{\\ell,i}}\\right) \\ge 1-\\beta \\quad \\implies \\quad 1 - \\Phi\\left(\\frac{\\ln \\sigma_0 - \\mu_{\\ell,i}}{s_{\\ell,i}}\\right) \\ge 1-\\beta\n$$\nThis simplifies to $\\Phi\\left(\\frac{\\ln \\sigma_0 - \\mu_{\\ell,i}}{s_{\\ell,i}}\\right) \\le \\beta$. Applying the inverse CDF, we get $\\frac{\\ln \\sigma_0 - \\mu_{\\ell,i}}{s_{\\ell,i}} \\le z_{\\beta}$, which rearranges to $\\ln \\sigma_0 \\le \\mu_{\\ell,i} + z_{\\beta} s_{\\ell,i}$. Taking the exponential of both sides gives $\\sigma_0 \\le \\exp(\\mu_{\\ell,i} + z_{\\beta} s_{\\ell,i})$. The right-hand side is the definition of the lower $\\beta$ quantile of $\\sigma_i$, denoted $q_{\\sigma,i}^{\\mathrm{low}}$. So, the conductivity feasibility condition is:\n$$\nq_{\\sigma,i}^{\\mathrm{low}} = \\exp(\\mu_{\\ell,i} + z_{\\beta} s_{\\ell,i}) \\ge \\sigma_0\n$$\nA candidate material $i$ is robustly feasible if and only if it satisfies both of these quantile-based conditions. The set of all such materials is denoted $\\mathcal{F}$.\n\nThe next step is to find the robust Pareto set, $\\mathcal{P}$, from the feasible set $\\mathcal{F}$. The optimization objectives are to minimize the robust stability metric $q_{E,i}^{\\mathrm{up}}$ (a lower value is better) and maximize the robust conductivity metric $q_{\\sigma,i}^{\\mathrm{low}}$ (a higher value is better). For any two feasible candidates $a, b \\in \\mathcal{F}$, candidate $a$ is said to dominate candidate $b$ if:\n$$\n(q_{E,a}^{\\mathrm{up}} \\le q_{E,b}^{\\mathrm{up}} \\land q_{\\sigma,a}^{\\mathrm{low}} \\ge q_{\\sigma,b}^{\\mathrm{low}}) \\land (q_{E,a}^{\\mathrm{up}} < q_{E,b}^{\\mathrm{up}} \\lor q_{\\sigma,a}^{\\mathrm{low}} > q_{\\sigma,b}^{\\mathrm{low}})\n$$\nThis means $a$ is at least as good as $b$ on both objectives and strictly better on at least one. The robust Pareto set $\\mathcal{P}$ is the subset of $\\mathcal{F}$ containing all candidates that are not dominated by any other candidate in $\\mathcal{F}$. The algorithm to find $\\mathcal{P}$ is as follows: for each candidate $i \\in \\mathcal{F}$, compare it with every other candidate $j \\in \\mathcal{F}$. If any $j$ dominates $i$, then $i$ is not in the Pareto set. If no such $j$ exists after checking all other feasible candidates, then $i$ is a member of $\\mathcal{P}$.\n\nFinally, we calculate the selection probability for each candidate. This probability is defined based on a policy of selecting uniformly at random from the non-empty Pareto set. If the Pareto set $\\mathcal{P}$ is not empty ($|\\mathcal{P}| > 0$), the selection probability $p^{\\mathrm{sel}}_i$ for a candidate $i$ is:\n$$\np^{\\mathrm{sel}}_i =\n\\begin{cases}\n1/|\\mathcal{P}| & \\text{if } i \\in \\mathcal{P} \\\\\n0 & \\text{if } i \\notin \\mathcal{P}\n\\end{cases}\n$$\nIf the Pareto set is empty ($\\mathcal{P} = \\emptyset$), which occurs when the feasible set $\\mathcal{F}$ is empty, the selection probability is $0$ for all candidates.\n\nThe overall procedure for each test case is:\n1.  Read the control parameters $\\alpha$, $\\beta$, $\\sigma_0$, and the candidate material data $(\\mu_{E,i}, s_{E,i}, \\mu_{\\ell,i}, s_{\\ell,i})$.\n2.  Compute the necessary standard normal quantiles, $z_{1-\\alpha}$ and $z_{\\beta}$, using a numerical library.\n3.  For each candidate $i$:\n    a. Calculate the robust performance metrics $q_{E,i}^{\\mathrm{up}}$ and $q_{\\sigma,i}^{\\mathrm{low}}$.\n    b. Check if $q_{E,i}^{\\mathrm{up}} \\le 0$ and $q_{\\sigma,i}^{\\mathrm{low}} \\ge \\sigma_0$.\n    c. If feasible, store the candidate's index and its objective vector $(q_{E,i}^{\\mathrm{up}}, q_{\\sigma,i}^{\\mathrm{low}})$.\n4.  Construct the Pareto set $\\mathcal{P}$ from the set of feasible candidates by identifying all non-dominated members.\n5.  Calculate the selection probabilities for all initial candidates based on the size and composition of $\\mathcal{P}$.\n6.  Collect the list of probabilities for the current test case and repeat for all cases. The final output is a list of these probability lists.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Solves the robust materials screening problem for all test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"params\": {\"alpha\": 0.1, \"beta\": 0.1, \"sigma_0\": 1.5},\n            \"candidates\": [\n                # M1: mu_E, s_E, mu_l, s_l\n                (-1.0, 0.5, np.log(1.8), 0.1),\n                # M2\n                (-0.5, 0.3, np.log(1.2), 0.2),\n                # M3\n                (1.5, 1.0, np.log(2.5), 0.3),\n                # M4\n                (4.0, 2.0, np.log(4.0), 0.25),\n                # M5\n                (0.1, 0.7, np.log(0.8), 0.2)\n            ]\n        },\n        {\n            \"params\": {\"alpha\": 0.01, \"beta\": 0.05, \"sigma_0\": 2.0},\n            \"candidates\": [\n                # M1\n                (-1.0, 0.5, np.log(1.8), 0.1),\n                # M2\n                (-0.5, 0.3, np.log(1.2), 0.2),\n                # M3\n                (1.5, 1.0, np.log(2.5), 0.3),\n                # M4\n                (4.0, 2.0, np.log(4.0), 0.25),\n                # M5\n                (0.1, 0.7, np.log(0.8), 0.2)\n            ]\n        },\n        {\n            \"params\": {\"alpha\": 0.2, \"beta\": 0.2, \"sigma_0\": 1.5},\n            \"candidates\": [\n                # F1\n                (0.0, 0.0, np.log(1.7), 0.0),\n                # F2\n                (-0.5, 0.3, np.log(2.0), 0.3),\n                # F3\n                (4.0, 8.0, np.log(0.5), 0.4)\n            ]\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        params = case[\"params\"]\n        candidates_data = case[\"candidates\"]\n        \n        alpha = params[\"alpha\"]\n        beta = params[\"beta\"]\n        sigma_0 = params[\"sigma_0\"]\n        num_candidates = len(candidates_data)\n\n        # Compute standard normal quantiles\n        z_1_minus_alpha = norm.ppf(1 - alpha)\n        z_beta = norm.ppf(beta)\n        \n        feasible_candidates = []\n        for i, (mu_E, s_E, mu_l, s_l) in enumerate(candidates_data):\n            # Calculate robust performance metrics (quantiles)\n            q_E_up = mu_E + z_1_minus_alpha * s_E\n            q_sigma_low = np.exp(mu_l + z_beta * s_l)\n            \n            # Check for robust feasibility\n            is_stable = (q_E_up = 0)\n            is_conductive = (q_sigma_low >= sigma_0)\n            \n            if is_stable and is_conductive:\n                feasible_candidates.append({\n                    \"id\": i,\n                    \"q_E\": q_E_up,\n                    \"q_sigma\": q_sigma_low\n                })\n\n        # Construct the robust Pareto set\n        pareto_set_indices = []\n        if feasible_candidates:\n            for i, cand_i in enumerate(feasible_candidates):\n                is_dominated = False\n                for j, cand_j in enumerate(feasible_candidates):\n                    if i == j:\n                        continue\n                    \n                    # Check if cand_j dominates cand_i\n                    # Objectives: minimize q_E, maximize q_sigma\n                    # Dominance: q_E_j = q_E_i AND q_sigma_j >= q_sigma_i (with one strict)\n                    c1 = cand_j[\"q_E\"] = cand_i[\"q_E\"]\n                    c2 = cand_j[\"q_sigma\"] >= cand_i[\"q_sigma\"]\n                    c3 = cand_j[\"q_E\"]  cand_i[\"q_E\"] or cand_j[\"q_sigma\"] > cand_i[\"q_sigma\"]\n                    \n                    if c1 and c2 and c3:\n                        is_dominated = True\n                        break\n                \n                if not is_dominated:\n                    pareto_set_indices.append(cand_i[\"id\"])\n\n        # Calculate selection probabilities\n        probabilities = [0.0] * num_candidates\n        if pareto_set_indices:\n            prob = 1.0 / len(pareto_set_indices)\n            for idx in pareto_set_indices:\n                probabilities[idx] = prob\n        \n        results.append(probabilities)\n\n    # Final print statement in the exact required format.\n    # The default string representation of a list of lists matches the example format.\n    print(results)\n\nsolve()\n```"
        }
    ]
}