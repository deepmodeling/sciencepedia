{
    "hands_on_practices": [
        {
            "introduction": "在应用机器学习之前，我们必须将化学成分转化为模型可以理解的定长数值向量，这一过程称为特征工程。基于组成的描述符，例如在 MAGPIE 框架中开发的描述符，通过计算基本元素属性的统计聚合（例如，平均值、方差）来实现这一目标。这项练习  提供了计算这些关键特征的实践经验，并强调了在高通量筛选中至关重要的计算效率和物理近似之间的平衡。",
            "id": "3926519",
            "problem": "在用于层状氧化物正极材料的自动化电池设计和模拟流程中，通过使用源自“与材料无关的信息学和探索平台”（Materials-Agnostic Platform for Informatics and Exploration, MAGPIE）的成分描述符来加速筛选过程。这些描述符通过使用化学计量原子分数计算的统计摘要，来聚合整个成分的元素性质。作为一个与上下文相符的基本基础，您可以使用以下定义：对于一个具有化学计量系数的化合物，元素 $i$ 的原子分数 $x_{i}$ 是其化学计量系数除以所有化学计量系数的总和；对于与元素 $i$ 相关的性质 $p_{i}$，成分加权平均值为 $\\mu_{p}=\\sum_{i}x_{i}p_{i}$，成分加权方差为 $\\sigma_{p}^{2}=\\sum_{i}x_{i}\\left(p_{i}-\\mu_{p}\\right)^{2}$。\n\n考虑化学计量为 $\\mathrm{Li}_{1}\\mathrm{Ni}_{0.6}\\mathrm{Mn}_{0.2}\\mathrm{Co}_{0.2}\\mathrm{O}_{2}$ 的层状氧化物 $\\mathrm{LiNi_{0.6}Mn_{0.2}Co_{0.2}O_{2}}$。使用以下元素性质：\n- 泡林电负性 $\\chi$：\n  - $\\mathrm{Li}$: $\\chi=0.98$\n  - $\\mathrm{Ni}$: $\\chi=1.91$\n  - $\\mathrm{Mn}$: $\\chi=1.55$\n  - $\\mathrm{Co}$: $\\chi=1.88$\n  - $\\mathrm{O}$: $\\chi=3.44$\n- 共价半径 $r$ (单位 $\\mathrm{\\AA}$):\n  - $\\mathrm{Li}$: $r=1.28$\n  - $\\mathrm{Ni}$: $r=1.24$\n  - $\\mathrm{Mn}$: $r=1.39$\n  - $\\mathrm{Co}$: $r=1.26$\n  - $\\mathrm{O}$: $r=0.66$\n\n通过计算成分加权平均电负性 $\\mu_{\\chi}$、成分加权方差 $\\sigma_{\\chi}^{2}$ 和成分加权平均共价半径 $\\mu_{r}$，为该成分构建上述两种性质的 MAGPIE 风格的描述符分量。基于这些分量，定义一个标量筛选分数\n$$\nS=\\alpha\\,\\mu_{\\chi}+\\beta\\,\\sigma_{\\chi}^{2}+\\gamma\\,\\mu_{r},\n$$\n其中系数 $\\alpha=0.90$、$\\beta=-0.25$ 和 $\\gamma=0.30\\,\\mathrm{\\AA}^{-1}$ 的选择是为了得到一个无量纲的分数。计算 $S$ 的值，并将您的最终答案四舍五入到四位有效数字。将 $S$ 报告为一个无量纲的量。此外，解释在这种描述符构建中统计聚合元素性质所依据的假设，包括这些假设如何与用于加速材料筛选的机器学习相结合。",
            "solution": "经评估，问题陈述有效。它在科学上基于计算材料科学和机器学习用于材料发现的原理。化合物 $\\mathrm{LiNi_{0.6}Mn_{0.2}Co_{0.2}O_{2}}$ 是一种成熟的正极材料。使用基于元素性质统计聚合的 MAGPIE 风格描述符是材料信息学中的一种标准技术。该问题设定良好，提供了所有必要的定义、数据和常数，以得出一个唯一的、可验证的解决方案。其语言、目标和任务都是明确的。\n\n解决方案分为两部分：首先，筛选分数 $S$ 的数值计算；其次，对基本假设及其在机器学习中的背景进行解释。\n\n第 1 部分：筛选分数 $S$ 的计算\n\n第一步是确定化合物 $\\mathrm{Li}_{1}\\mathrm{Ni}_{0.6}\\mathrm{Mn}_{0.2}\\mathrm{Co}_{0.2}\\mathrm{O}_{2}$ 中各元素的原子分数 $x_i$。化学计量系数为 $s_{\\mathrm{Li}}=1$、$s_{\\mathrm{Ni}}=0.6$、$s_{\\mathrm{Mn}}=0.2$、$s_{\\mathrm{Co}}=0.2$ 和 $s_{\\mathrm{O}}=2$。\n\n化学计量系数的总和 $S_{total}$ 是：\n$$\nS_{total} = s_{\\mathrm{Li}} + s_{\\mathrm{Ni}} + s_{\\mathrm{Mn}} + s_{\\mathrm{Co}} + s_{\\mathrm{O}} = 1 + 0.6 + 0.2 + 0.2 + 2 = 4\n$$\n各元素 $i$ 的原子分数 $x_i$ 计算如下：$x_i = s_i / S_{total}$：\n$$\nx_{\\mathrm{Li}} = \\frac{1}{4} = 0.25\n$$\n$$\nx_{\\mathrm{Ni}} = \\frac{0.6}{4} = 0.15\n$$\n$$\nx_{\\mathrm{Mn}} = \\frac{0.2}{4} = 0.05\n$$\n$$\nx_{\\mathrm{Co}} = \\frac{0.2}{4} = 0.05\n$$\n$$\nx_{\\mathrm{O}} = \\frac{2}{4} = 0.50\n$$\n原子分数的总和为 $\\sum x_i = 0.25 + 0.15 + 0.05 + 0.05 + 0.50 = 1$，符合要求。\n\n接下来，我们使用公式 $\\mu_{p}=\\sum_{i}x_{i}p_{i}$ 和给定的泡林电负性值 $\\chi_i$ 来计算成分加权平均电负性 $\\mu_{\\chi}$：\n$$\n\\mu_{\\chi} = x_{\\mathrm{Li}}\\chi_{\\mathrm{Li}} + x_{\\mathrm{Ni}}\\chi_{\\mathrm{Ni}} + x_{\\mathrm{Mn}}\\chi_{\\mathrm{Mn}} + x_{\\mathrm{Co}}\\chi_{\\mathrm{Co}} + x_{\\mathrm{O}}\\chi_{\\mathrm{O}}\n$$\n$$\n\\mu_{\\chi} = (0.25)(0.98) + (0.15)(1.91) + (0.05)(1.55) + (0.05)(1.88) + (0.50)(3.44)\n$$\n$$\n\\mu_{\\chi} = 0.245 + 0.2865 + 0.0775 + 0.094 + 1.72 = 2.423\n$$\n\n然后，我们使用公式 $\\sigma_{p}^{2}=\\sum_{i}x_{i}(p_{i}-\\mu_{p})^{2}$ 来计算电负性的成分加权方差 $\\sigma_{\\chi}^{2}$：\n$$\n\\sigma_{\\chi}^{2} = x_{\\mathrm{Li}}(\\chi_{\\mathrm{Li}}-\\mu_{\\chi})^2 + x_{\\mathrm{Ni}}(\\chi_{\\mathrm{Ni}}-\\mu_{\\chi})^2 + x_{\\mathrm{Mn}}(\\chi_{\\mathrm{Mn}}-\\mu_{\\chi})^2 + x_{\\mathrm{Co}}(\\chi_{\\mathrm{Co}}-\\mu_{\\chi})^2 + x_{\\mathrm{O}}(\\chi_{\\mathrm{O}}-\\mu_{\\chi})^2\n$$\n$$\n\\sigma_{\\chi}^{2} = (0.25)(0.98-2.423)^2 + (0.15)(1.91-2.423)^2 + (0.05)(1.55-2.423)^2 + (0.05)(1.88-2.423)^2 + (0.50)(3.44-2.423)^2\n$$\n$$\n\\sigma_{\\chi}^{2} = (0.25)(-1.443)^2 + (0.15)(-0.513)^2 + (0.05)(-0.873)^2 + (0.05)(-0.543)^2 + (0.50)(1.017)^2\n$$\n$$\n\\sigma_{\\chi}^{2} = (0.25)(2.082249) + (0.15)(0.263169) + (0.05)(0.762129) + (0.05)(0.294849) + (0.50)(1.034289)\n$$\n$$\n\\sigma_{\\chi}^{2} \\approx 0.52056 + 0.03948 + 0.03811 + 0.01474 + 0.51714 = 1.13003\n$$\n\n接下来，我们使用给定的半径 $r_i$ 来计算成分加权平均共价半径 $\\mu_{r}$：\n$$\n\\mu_{r} = x_{\\mathrm{Li}}r_{\\mathrm{Li}} + x_{\\mathrm{Ni}}r_{\\mathrm{Ni}} + x_{\\mathrm{Mn}}r_{\\mathrm{Mn}} + x_{\\mathrm{Co}}r_{\\mathrm{Co}} + x_{\\mathrm{O}}r_{\\mathrm{O}}\n$$\n$$\n\\mu_{r} = (0.25)(1.28\\,\\mathrm{\\AA}) + (0.15)(1.24\\,\\mathrm{\\AA}) + (0.05)(1.39\\,\\mathrm{\\AA}) + (0.05)(1.26\\,\\mathrm{\\AA}) + (0.50)(0.66\\,\\mathrm{\\AA})\n$$\n$$\n\\mu_{r} = 0.32\\,\\mathrm{\\AA} + 0.186\\,\\mathrm{\\AA} + 0.0695\\,\\mathrm{\\AA} + 0.063\\,\\mathrm{\\AA} + 0.33\\,\\mathrm{\\AA} = 0.9685\\,\\mathrm{\\AA}\n$$\n\n最后，我们使用给定的公式和系数 $\\alpha=0.90$、$\\beta=-0.25$ 和 $\\gamma=0.30\\,\\mathrm{\\AA}^{-1}$ 来计算标量筛选分数 $S$：\n$$\nS = \\alpha\\,\\mu_{\\chi} + \\beta\\,\\sigma_{\\chi}^{2} + \\gamma\\,\\mu_{r}\n$$\n$$\nS = (0.90)(2.423) + (-0.25)(1.13003) + (0.30\\,\\mathrm{\\AA}^{-1})(0.9685\\,\\mathrm{\\AA})\n$$\n$$\nS = 2.1807 - 0.2825075 + 0.29055\n$$\n$$\nS \\approx 2.1887425\n$$\n四舍五入到四位有效数字，我们得到 $S \\approx 2.189$。\n\n第 2 部分：假设及其在机器学习中的背景\n\n这些描述符的构建基于几个关键假设，这些假设代表了在物理准确性和计算效率之间的一种刻意权衡。\n\n1.  **原子可加性和均质化**：基本假设是，材料的宏观或体性质可以通过其构成元素的内在性质的统计聚合来近似。这种“原子袋”方法有效地将材料均质化，将其视为一种简单的混合物。它在很大程度上忽略了晶体结构、局部配位环境、键合类型（离子键、共价键、金属键）、氧化态以及长程电子和结构相互作用等复杂而具体的细节。例如，一个元素的泡林电负性是一个固定值，但其在化合物中的有效电负性会受到其化学环境的调节。\n\n2.  **化学计量比例性**：使用原子分数作为权重，其前提是每个原子对总性质的贡献与其在化学式中的丰度成正比。这是一种一阶近似，没有考虑由不同原子的特定排列和相互作用产生的非线性效应。例如，如果一个少数元素占据了晶格中的关键位置，它可能会对某个性质产生不成比例的巨大影响。\n\n3.  **与环境无关的元素性质**：该模型使用的元素性质（例如共价半径）是在元素的参考状态下定义的。问题指定了共价半径，这与共价键最相关。然而，像 $\\mathrm{LiNi_{0.6}Mn_{0.2}Co_{0.2}O_{2}}$ 这样的材料具有显著的离子键特征。使用共价半径而非（例如）离子半径是一种简化。描述符生成框架接受这种不完美，并假设即使绝对值不完全具有代表性，它们的相对趋势和加权平均值仍然包含预测性信息。\n\n这些假设对于与机器学习相结合以加速材料筛选至关重要：\n\n-   **特征工程**：机器学习的核心任务是学习从输入特征到输出目标的映射。这些统计聚合的描述符（$\\mu_{\\chi}$、$\\sigma_{\\chi}^{2}$、$\\mu_{r}$ 等）作为一个定长的数值特征向量，代表一种化学成分。这种从化学式到向量的转换对于应用标准机器学习算法至关重要。\n\n-   **抽象化和降维**：这些描述符提供了一种低维的、基于成分的抽象，而不是通过晶胞中每个原子的明确坐标（一种高维且可变大小的表示）来描述材料。这种简化使得筛选过程在计算上变得可行。\n\n-   **代理建模**：这种方法的目的是创建一个快速的、近似的“代理模型”。对于数百万种潜在的材料成分，人们可以几乎瞬时地计算这些简单的描述符，而无需为每一种成分进行昂贵的、高保真的量子力学模拟（如密度泛函理论）或耗时的实验室实验。一个在已知描述符和目标性质（例如，电池电压、离子电导率）的数据集上训练的机器学习模型，会学习近似特征与真实材料行为之间复杂的非线性关系。\n\n-   **加速筛选流程**：机器学习模型一旦训练完成，就可以在几分钟或几小时内预测数百万种假设成分的目标性质。这使得研究人员能够快速筛选广阔的化学空间，并筛选出少量有希望的候选材料，以进行更严格、更昂贵的验证。这种方法的成功取决于这样一个事实：虽然其基本假设在物理上是简化的，但由此产生的特征保留了足够的化学和物理“信号”，足以让机器学习模型识别出有意义的相关性并做出有用的预测。方差项 $\\sigma_{\\chi}^{2}$ 尤其重要，因为它捕捉了元素的异质性程度，而这是一个简单的平均值会完全忽略的性质。",
            "answer": "$$\n\\boxed{2.189}\n$$"
        },
        {
            "introduction": "有了预测模型之后，我们需要一个智能策略来导航广阔的材料空间，而不是采用蛮力搜索。贝叶斯优化为此提供了一个框架，它使用诸如上限置信区间（Upper Confidence Bound, UCB）之类的采集函数来指导下一个待评估候选材料的选择。这项练习  演示了如何应用 UCB 准则，该准则战略性地平衡了利用预测性能好的材料（高均值）与探索模型不确定的材料（高方差），从而加速了发现过程。",
            "id": "3926501",
            "problem": "一个电池研究团队正在使用高斯过程 (GP) 代理模型来加速筛选一维锂分数轴 $x \\in [0,1]$ 上的候选固态电解质组分。目标属性是对数尺度离子电导率，对于任何候选组分 $x$，其被建模为一个高斯过程后验预测分布，该分布是均值为 $\\mu(x)$、标准差为 $\\sigma(x)$ 的高斯分布，由平方指数核和先验训练数据计算得出。为了选择下一个实验，该团队使用上置信界 (UCB) 采集函数，其定义为每个 $x$ 处高斯预测分布的上 $q$-分位数。其中，“探索率”是分位数水平 $q \\in (0.5,1)$，相应的 UCB 由 $q$-分位数 $\\mu(x) + \\kappa\\,\\sigma(x)$ 给出，其中 $\\kappa = \\Phi^{-1}(q)$，而 $\\Phi^{-1}$ 是标准正态分布的逆累积分布函数。\n\n现提供在三个候选组分 $x_1=0.21$、$x_2=0.58$ 和 $x_3=0.83$ 处预先计算的高斯过程后验预测均值和标准差。这些值是使用相同的核超参数，但在代表不同实验设置的两种同方差观测噪声情景下获得的：\n\n- 均值（由于密集的训练数据，在两种噪声情景下近似不变）：$\\mu(x_1)=2.10$，$\\mu(x_2)=2.05$，$\\mu(x_3)=1.95$。\n- 低噪声预测标准差：$\\sigma_{\\mathrm{L}}(x_1)=0.06$，$\\sigma_{\\mathrm{L}}(x_2)=0.09$，$\\sigma_{\\mathrm{L}}(x_3)=0.14$。\n- 高噪声预测标准差：$\\sigma_{\\mathrm{H}}(x_1)=0.10$，$\\sigma_{\\mathrm{H}}(x_2)=0.13$，$\\sigma_{\\mathrm{H}}(x_3)=0.20$。\n\n假设团队为其下一次选择设定了 $q=0.90$ 的探索率（分位数水平）。请使用高斯 $q$-分位数的定义，在低噪声预测标准差下，通过最大化 UCB 来计算下一个查询组分 $x^{\\star}$。然后，基于相同的定义和数据，分析在低噪声和高噪声标准差下，$x^{\\star}$ 的身份如何随 $\\kappa$（等价于 $q$）的变化而变化，并推导出使最大化点在候选者之间切换的 $\\kappa$ 的临界值。您的分析应从 GP 预测分布和 UCB 分位数解释的定义开始，并遵循第一性原理进行。\n\n仅报告低噪声情况下最终选定的组分 $x^{\\star}$，作为一个在 $[0,1]$ 范围内的纯数字，四舍五入到三位有效数字。最终数值答案中不要包含任何单位。",
            "solution": "首先验证问题以确保其是适定的、有科学依据的，并包含所有必要信息。\n\n**步骤 1：提取已知条件**\n- 锂分数域：$x \\in [0,1]$。\n- 模型：高斯过程 (GP)，其后验预测分布为 $N(\\mu(x), \\sigma(x)^2)$。\n- 采集函数：上置信界 (UCB)，定义为预测分布的 $q$-分位数。\n- UCB 公式：$\\text{UCB}(x) = \\mu(x) + \\kappa\\,\\sigma(x)$，其中 $\\kappa = \\Phi^{-1}(q)$。\n- 初始选择的分位数水平：$q = 0.90$。\n- 候选组分：$x_1=0.21$，$x_2=0.58$，$x_3=0.83$。\n- 预测均值：$\\mu(x_1)=2.10$，$\\mu(x_2)=2.05$，$\\mu(x_3)=1.95$。\n- 低噪声预测标准差：$\\sigma_{\\mathrm{L}}(x_1)=0.06$，$\\sigma_{\\mathrm{L}}(x_2)=0.09$，$\\sigma_{\\mathrm{L}}(x_3)=0.14$。\n- 高噪声预测标准差：$\\sigma_{\\mathrm{H}}(x_1)=0.10$，$\\sigma_{\\mathrm{H}}(x_2)=0.13$，$\\sigma_{\\mathrm{H}}(x_3)=0.20$。\n\n**步骤 2：使用提取的已知条件进行验证**\n- **有科学依据：**该问题描述了一个标准的贝叶斯优化循环，使用了 GP 代理模型和 UCB 采集函数，这是材料科学机器学习中广泛接受的方法。所有概念都是统计学和机器学习中的标准概念。\n- **适定的：**该问题提供了一组离散的候选者和一个明确定义的目标函数以进行最大化。它提供了执行计算和分析所需的所有数值。\n- **客观性：**问题以精确、定量的术语陈述，没有歧义或主观论断。\n\n**步骤 3：结论和行动**\n问题有效。将提供完整的解决方案。\n\n**求解推导**\n\n问题的核心是确定哪个候选组分 $x_i$ 能最大化上置信界 (UCB) 采集函数，其定义为：\n$$\n\\text{UCB}(x) = \\mu(x) + \\kappa\\,\\sigma(x)\n$$\n此处，$\\mu(x)$ 是后验均值，$\\sigma(x)$ 是后验标准差，而 $\\kappa = \\Phi^{-1}(q)$ 是一个控制利用（高均值）和探索（高不确定性）之间权衡的参数，其中 $\\Phi^{-1}$ 是标准正态分布的逆累积分布函数 (CDF)。问题要求我们首先在低噪声条件下找到特定分位数 $q=0.90$ 的最大化点，然后分析在低噪声和高噪声两种情景下，该最大化点如何随 $\\kappa$ 的变化而变化。\n\n**第 1 部分：在 $q=0.90$ 的低噪声情况下的选择**\n\n对于探索率（分位数水平）$q=0.90$，参数 $\\kappa$ 为：\n$$\n\\kappa = \\Phi^{-1}(0.90) \\approx 1.28155\n$$\n我们现在使用低噪声标准差 $\\sigma_{\\mathrm{L}}$ 计算三个候选者中每一个的 UCB 值。\n\n对于候选者 $x_1=0.21$：\n$$\n\\text{UCB}_{\\mathrm{L}}(x_1) = \\mu(x_1) + \\kappa\\,\\sigma_{\\mathrm{L}}(x_1) = 2.10 + (1.28155)(0.06) \\approx 2.10 + 0.07689 = 2.17689\n$$\n\n对于候选者 $x_2=0.58$：\n$$\n\\text{UCB}_{\\mathrm{L}}(x_2) = \\mu(x_2) + \\kappa\\,\\sigma_{\\mathrm{L}}(x_2) = 2.05 + (1.28155)(0.09) \\approx 2.05 + 0.11534 = 2.16534\n$$\n\n对于候选者 $x_3=0.83$：\n$$\n\\text{UCB}_{\\mathrm{L}}(x_3) = \\mu(x_3) + \\kappa\\,\\sigma_{\\mathrm{L}}(x_3) = 1.95 + (1.28155)(0.14) \\approx 1.95 + 0.17942 = 2.12942\n$$\n\n比较这三个值：\n$$\n\\text{UCB}_{\\mathrm{L}}(x_1) > \\text{UCB}_{\\mathrm{L}}(x_2) > \\text{UCB}_{\\mathrm{L}}(x_3)\n$$\n最大化 UCB 的组分是 $x_1$。因此，下一个查询组分是 $x^{\\star} = x_1 = 0.21$。\n\n**第 2 部分：$\\kappa$ 临界值的分析**\n\n最优候选者 $x^{\\star}$ 的身份取决于 $\\kappa$ 的值。对于每个候选者 $i$，UCB 是 $\\kappa$ 的线性函数：$\\text{UCB}_i(\\kappa) = \\mu_i + \\kappa\\sigma_i$。当候选者 $i$ 和 $j$ 的 UCB 相等时，最优选择会从 $i$ 切换到 $j$，此时的 $\\kappa$ 值即为临界值：\n$$\n\\mu_i + \\kappa_{ij}\\sigma_i = \\mu_j + \\kappa_{ij}\\sigma_j\n$$\n求解临界值 $\\kappa_{ij}$ 得：\n$$\n\\kappa_{ij} = \\frac{\\mu_i - \\mu_j}{\\sigma_j - \\sigma_i}\n$$\n我们只关心 $\\kappa$ 的正值，因为 $q \\in (0.5, 1)$ 意味着 $\\kappa > 0$。\n\n**低噪声情况下的分析 ($\\sigma_{\\mathrm{L}}$)：**\n- $x_1$ 和 $x_2$ 之间的交叉点：\n$$\n\\kappa_{12,\\mathrm{L}} = \\frac{\\mu(x_1) - \\mu(x_2)}{\\sigma_{\\mathrm{L}}(x_2) - \\sigma_{\\mathrm{L}}(x_1)} = \\frac{2.10 - 2.05}{0.09 - 0.06} = \\frac{0.05}{0.03} = \\frac{5}{3} \\approx 1.667\n$$\n- $x_2$ 和 $x_3$ 之间的交叉点：\n$$\n\\kappa_{23,\\mathrm{L}} = \\frac{\\mu(x_2) - \\mu(x_3)}{\\sigma_{\\mathrm{L}}(x_3) - \\sigma_{\\mathrm{L}}(x_2)} = \\frac{2.05 - 1.95}{0.14 - 0.09} = \\frac{0.10}{0.05} = 2\n$$\n- $x_1$ 和 $x_3$ 之间的交叉点：\n$$\n\\kappa_{13,\\mathrm{L}} = \\frac{\\mu(x_1) - \\mu(x_3)}{\\sigma_{\\mathrm{L}}(x_3) - \\sigma_{\\mathrm{L}}(x_1)} = \\frac{2.10 - 1.95}{0.14 - 0.06} = \\frac{0.15}{0.08} = \\frac{15}{8} = 1.875\n$$\n当 $\\kappa=0$ 时，$x_1$ 是最优的，因为 $\\mu(x_1)$ 是最大的。它将保持最优，直到其 UCB 被超越。涉及 $x_1$ 的第一个交叉点是与 $x_2$ 在 $\\kappa \\approx 1.667$ 处相交。从 $x_2$到 $x_3$ 的交叉点稍后在 $\\kappa = 2$ 处发生。\n因此，在低噪声情况下，最优候选者的序列是：\n- $x_1$ 在 $\\kappa \\in [0, 5/3)$ 时为最优。\n- $x_2$ 在 $\\kappa \\in [5/3, 2)$ 时为最优。\n- $x_3$ 在 $\\kappa \\ge 2$ 时为最优。\n我们的值 $\\kappa \\approx 1.28155$ 落在第一个区间内，这证实了 $x_1$ 是正确的选择。\n\n**高噪声情况下的分析 ($\\sigma_{\\mathrm{H}}$)：**\n- $x_1$ 和 $x_2$ 之间的交叉点：\n$$\n\\kappa_{12,\\mathrm{H}} = \\frac{\\mu(x_1) - \\mu(x_2)}{\\sigma_{\\mathrm{H}}(x_2) - \\sigma_{\\mathrm{H}}(x_1)} = \\frac{2.10 - 2.05}{0.13 - 0.10} = \\frac{0.05}{0.03} = \\frac{5}{3} \\approx 1.667\n$$\n- $x_2$ 和 $x_3$ 之间的交叉点：\n$$\n\\kappa_{23,\\mathrm{H}} = \\frac{\\mu(x_2) - \\mu(x_3)}{\\sigma_{\\mathrm{H}}(x_3) - \\sigma_{\\mathrm{H}}(x_2)} = \\frac{2.05 - 1.95}{0.20 - 0.13} = \\frac{0.10}{0.07} = \\frac{10}{7} \\approx 1.429\n$$\n- $x_1$ 和 $x_3$ 之间的交叉点：\n$$\n\\kappa_{13,\\mathrm{H}} = \\frac{\\mu(x_1) - \\mu(x_3)}{\\sigma_{\\mathrm{H}}(x_3) - \\sigma_{\\mathrm{H}}(x_1)} = \\frac{2.10 - 1.95}{0.20 - 0.10} = \\frac{0.15}{0.10} = 1.5\n$$\n当 $\\kappa=0$ 时，$x_1$ 是最优的。涉及 $x_1$ 的第一个临界值是 $\\kappa_{13,\\mathrm{H}}=1.5$。在这一点上，$\\text{UCB}_{\\mathrm{H}}(x_1) = \\text{UCB}_{\\mathrm{H}}(x_3)$。对于 $\\kappa > 1.5$，有 $\\text{UCB}_{\\mathrm{H}}(x_3) > \\text{UCB}_{\\mathrm{H}}(x_1)$。我们必须检查 $x_2$ 的状态。由于 $\\kappa_{23,\\mathrm{H}} \\approx 1.429 < 1.5$，对于任何 $\\kappa > 1.429$，我们都有 $\\text{UCB}_{\\mathrm{H}}(x_3) > \\text{UCB}_{\\mathrm{H}}(x_2)$。因此，在切换点 $\\kappa=1.5$ 处，$x_3$ 超过了 $x_1$，并且已经大于一个正在下降的 $x_2$ 的 UCB。在高噪声情景下，候选者 $x_2$ 从不是最优的。\n因此，在高噪声情况下，最优候选者的序列是：\n- $x_1$ 在 $\\kappa \\in [0, 1.5)$ 时为最优。\n- $x_3$ 在 $\\kappa \\ge 1.5$ 时为最优。\n\n问题只要求在 $q=0.90$ 的低噪声情况下最终选定的组分 $x^{\\star}$。如第 1 部分计算所示，这是 $x_1=0.21$。问题要求答案四舍五入到三位有效数字。\n\n最终答案：$x^{\\star} = 0.210$。",
            "answer": "$$\\boxed{0.210}$$"
        },
        {
            "introduction": "实用材料的发现通常需要同时优化多个相互冲突的属性，例如同时实现高稳定性和高电导率。当对这些属性的预测不确定时，鲁棒优化方法可以帮助我们识别一组“帕累托最优”的候选材料，它们代表了在特定置信水平下的最佳权衡。在这个高级练习  中，您将学习使用机会约束来构建鲁棒帕累托集，这是一种在面对机器学习模型固有的不确定性时做出可靠决策的强大技术。",
            "id": "3926474",
            "problem": "您的任务是为电池电极材料筛选构建一个鲁棒的帕累托集，该筛选在两个目标（热力学稳定性和离子电导率）的预测不确定性下进行。设计背景是使用机器学习预测进行自动化电池设计和模拟，以加速材料筛选。每个候选材料都通过以下预测分布来表征：凸包以上能量（用 $\\Delta E_{\\mathrm{hull}}$ 表示，单位为毫电子伏特每原子 meV/atom）的预测分布，以及离子电导率 $\\sigma$（单位为毫西门子每厘米 mS/cm）的预测分布。\n\n基本原理：\n- 假设机器学习模型对每种材料 $i$ 输出以下内容：\n  - $\\Delta E_{\\mathrm{hull}, i}$ 的预测正态分布，均值为 $\\mu_{E,i}$，标准差为 $s_{E,i}$，模型为 $\\Delta E_{\\mathrm{hull}, i} \\sim \\mathcal{N}(\\mu_{E,i}, s_{E,i}^2)$。\n  - 电导率 $\\sigma_i$ 的预测对数正态分布，使得 $\\ln \\sigma_i \\sim \\mathcal{N}(\\mu_{\\ell,i}, s_{\\ell,i}^2)$，其中 $\\mu_{\\ell,i}$ 和 $s_{\\ell,i}$ 分别是电导率自然对数的均值和标准差。\n\n- 鲁棒可行性通过机会约束来强制执行：\n  - 稳定性机会约束：$\\mathbb{P}(\\Delta E_{\\mathrm{hull}, i} \\le 0) \\ge 1-\\alpha$。\n  - 电导率机会约束：$\\mathbb{P}(\\sigma_i \\ge \\sigma_0) \\ge 1-\\beta$。\n  此处，$\\alpha \\in (0,1)$ 和 $\\beta \\in (0,1)$ 是用户指定的风险水平，$\\sigma_0$ 是需要满足的电导率阈值。将 $\\Delta E_{\\mathrm{hull}, i} \\le 0$ meV/atom 解释为相对于凸包的热力学稳定性，将 $\\sigma_i \\ge \\sigma_0$ mS/cm 解释为满足最低要求的电导率。\n\n- 在这些机会约束下，用于帕累托比较的鲁棒性能指标使用保守分位数：\n  - 对于稳定性，使用 $\\Delta E_{\\mathrm{hull}, i}$ 的上 $1-\\alpha$ 分位数 $q_{E,i}^{\\mathrm{up}}$。\n  - 对于电导率，使用 $\\sigma_i$ 的下 $\\beta$ 分位数 $q_{\\sigma,i}^{\\mathrm{low}}$。\n  这些分位数使用标准正态分位数 $z_p$ 定义，对于累积分布函数 $\\Phi$，满足 $\\Phi(z_p) = p$。对于正态变量 $\\Delta E_{\\mathrm{hull}, i}$，$q_{E,i}^{\\mathrm{up}} = \\mu_{E,i} + z_{1-\\alpha} s_{E,i}$。对于对数正态变量 $\\sigma_i$，$q_{\\sigma,i}^{\\mathrm{low}} = \\exp\\left(\\mu_{\\ell,i} + z_{\\beta} s_{\\ell,i}\\right)$。如果 $s_{E,i} = 0$ 或 $s_{\\ell,i} = 0$，则将分布解释为确定性的，因此相应的分位数等于变量的均值。\n\n- 可行性下的鲁棒帕累托集定义：\n  - 首先，确定鲁棒可行集 $\\mathcal{F} = \\{ i : \\mathbb{P}(\\Delta E_{\\mathrm{hull}, i} \\le 0) \\ge 1-\\alpha,\\, \\mathbb{P}(\\sigma_i \\ge \\sigma_0) \\ge 1-\\beta \\}$。\n  - 在 $i \\in \\mathcal{F}$ 中，如果 $q_{E,a}^{\\mathrm{up}} \\le q_{E,b}^{\\mathrm{up}}$ 且 $q_{\\sigma,a}^{\\mathrm{low}} \\ge q_{\\sigma,b}^{\\mathrm{low}}$，并且至少有一个不等式是严格的，则定义候选者 $a$ 在鲁棒意义上支配候选者 $b$。鲁棒帕累托集 $\\mathcal{P} \\subseteq \\mathcal{F}$ 是不受任何其他可行候选者支配的候选者集合。\n\n- 选择概率：\n  - 对于一个决策策略，当 $\\mathcal{P} \\neq \\emptyset$ 时从 $\\mathcal{P}$ 中均匀随机选择，当 $\\mathcal{P} = \\emptyset$ 时不选择任何东西，候选者 $i$ 的选择概率定义为：如果 $i \\in \\mathcal{P}$，则 $p^{\\mathrm{sel}}_i = \\frac{1}{|\\mathcal{P}|}$；否则 $p^{\\mathrm{sel}}_i = 0$。\n\n所有物理量必须以其指定单位处理：$\\Delta E_{\\mathrm{hull}}$ 以 meV/atom 为单位，$\\sigma$ 以 mS/cm 为单位。角度不属于此问题的一部分。所有答案都是无单位的选择概率，表示为浮点数。\n\n您的任务是实现一个完整、可运行的程序，该程序：\n- 对每个测试用例通过机会约束评估鲁棒可行性。\n- 使用保守分位数指标构建鲁棒帕累托集。\n- 按所述计算每个候选者的选择概率。\n\n测试套件：\n- 案例1（理想情况）：\n  - 参数：$\\alpha = 0.1$, $\\beta = 0.1$, $\\sigma_0 = 1.5$ mS/cm。\n  - 候选者（每个元组列出 $(\\mu_{E}\\,[{\\mathrm{meV/atom}}],\\, s_{E}\\,[{\\mathrm{meV/atom}}],\\, \\mu_{\\ell},\\, s_{\\ell})$）：\n    - $\\mathrm{M1}: (-1.0,\\, 0.5,\\, \\ln 1.8,\\, 0.1)$\n    - $\\mathrm{M2}: (-0.5,\\, 0.3,\\, \\ln 1.2,\\, 0.2)$\n    - $\\mathrm{M3}: (1.5,\\, 1.0,\\, \\ln 2.5,\\, 0.3)$\n    - $\\mathrm{M4}: (4.0,\\, 2.0,\\, \\ln 4.0,\\, 0.25)$\n    - $\\mathrm{M5}: (0.1,\\, 0.7,\\, \\ln 0.8,\\, 0.2)$\n- 案例2（严格约束边界情况）：\n  - 参数：$\\alpha = 0.01$, $\\beta = 0.05$, $\\sigma_0 = 2.0$ mS/cm。\n  - 候选者：与案例1相同。\n- 案例3（确定性变量边界情况）：\n  - 参数：$\\alpha = 0.2$, $\\beta = 0.2$, $\\sigma_0 = 1.5$ mS/cm。\n  - 候选者：\n    - $\\mathrm{F1}: (0.0,\\, 0.0,\\, \\ln 1.7,\\, 0.0)$\n    - $\\mathrm{F2}: (-0.5,\\, 0.3,\\, \\ln 2.0,\\, 0.3)$\n    - $\\mathrm{F3}: (4.0,\\, 8.0,\\, \\ln 0.5,\\, 0.4)$\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，其中每个元素对应一个测试用例，并且本身是浮点选择概率的列表，顺序与该测试用例中候选者的顺序相同。例如，一个有效的输出结构形式为 $[ [p_1, p_2], [q_1, q_2, q_3] ]$。",
            "solution": "该问题要求在不确定性下为电池电极材料筛选构建一个鲁棒的帕累托集。目标是基于两个相互冲突的目标——最大化热力学稳定性和最大化离子电导率——来识别一组非支配的候选材料。机器学习对这些属性预测的不确定性通过概率分布来建模。解决方案涉及一个多步骤过程，包括鲁棒可行性分析、多目标优化和选择概率的计算。\n\n首先，我们必须形式化候选材料被认为是“鲁棒可行”的条件。问题使用机会约束来定义可行性，这些约束为满足特定性能阈值的概率设定了下限。\n\n对于热力学稳定性，感兴趣的属性是每种材料 $i$ 的凸包以上能量 $\\Delta E_{\\mathrm{hull}, i}$。这由一个正态分布建模，$\\Delta E_{\\mathrm{hull}, i} \\sim \\mathcal{N}(\\mu_{E,i}, s_{E,i}^2)$。如果一种材料的能量为非正值，即 $\\Delta E_{\\mathrm{hull}, i} \\le 0$，则认为它是稳定的。稳定性机会约束要求此条件以至少 $1-\\alpha$ 的概率成立：\n$$\n\\mathbb{P}(\\Delta E_{\\mathrm{hull}, i} \\le 0) \\ge 1-\\alpha\n$$\n为了使此条件可操作，我们对随机变量进行标准化。设 $Z \\sim \\mathcal{N}(0, 1)$ 为标准正态变量。该约束变为：\n$$\n\\mathbb{P}\\left(\\frac{\\Delta E_{\\mathrm{hull}, i} - \\mu_{E,i}}{s_{E,i}} \\le \\frac{0 - \\mu_{E,i}}{s_{E,i}}\\right) \\ge 1-\\alpha \\quad \\implies \\quad \\Phi\\left(\\frac{-\\mu_{E,i}}{s_{E,i}}\\right) \\ge 1-\\alpha\n$$\n其中 $\\Phi$ 是标准正态分布的累积分布函数（CDF）。利用属性 $\\Phi(-x) = 1-\\Phi(x)$，这等价于 $1 - \\Phi\\left(\\frac{\\mu_{E,i}}{s_{E,i}}\\right) \\ge 1-\\alpha$，可简化为 $\\Phi\\left(\\frac{\\mu_{E,i}}{s_{E,i}}\\right) \\le \\alpha$。应用逆CDF $\\Phi^{-1}$（也称为分位数函数或probit函数），我们得到 $\\frac{\\mu_{E,i}}{s_{E,i}} \\le z_{\\alpha}$，其中 $z_p = \\Phi^{-1}(p)$。这可以重排为 $\\mu_{E,i} - z_{\\alpha} s_{E,i} \\le 0$。利用恒等式 $z_{\\alpha} = -z_{1-\\alpha}$，条件变为 $\\mu_{E,i} + z_{1-\\alpha} s_{E,i} \\le 0$。这个表达式正是 $\\Delta E_{\\mathrm{hull}, i}$ 的上 $1-\\alpha$ 分位数，记为 $q_{E,i}^{\\mathrm{up}}$。因此，稳定性可行性条件被优雅地简化为：\n$$\nq_{E,i}^{\\mathrm{up}} = \\mu_{E,i} + z_{1-\\alpha} s_{E,i} \\le 0\n$$\n\n对于离子电导率 $\\sigma_i$，该属性由对数正态分布建模，使其自然对数遵循正态分布：$\\ln \\sigma_i \\sim \\mathcal{N}(\\mu_{\\ell,i}, s_{\\ell,i}^2)$。电导率机会约束要求 $\\sigma_i$ 至少达到一个阈值 $\\sigma_0$，且概率至少为 $1-\\beta$：\n$$\n\\mathbb{P}(\\sigma_i \\ge \\sigma_0) \\ge 1-\\beta\n$$\n通过取自然对数（这是一个单调函数），我们可以处理正态分布的变量 $\\ln \\sigma_i$：\n$$\n\\mathbb{P}(\\ln \\sigma_i \\ge \\ln \\sigma_0) \\ge 1-\\beta\n$$\n对此变量进行标准化得到：\n$$\n\\mathbb{P}\\left(\\frac{\\ln \\sigma_i - \\mu_{\\ell,i}}{s_{\\ell,i}} \\ge \\frac{\\ln \\sigma_0 - \\mu_{\\ell,i}}{s_{\\ell,i}}\\right) \\ge 1-\\beta \\quad \\implies \\quad 1 - \\Phi\\left(\\frac{\\ln \\sigma_0 - \\mu_{\\ell,i}}{s_{\\ell,i}}\\right) \\ge 1-\\beta\n$$\n这简化为 $\\Phi\\left(\\frac{\\ln \\sigma_0 - \\mu_{\\ell,i}}{s_{\\ell,i}}\\right) \\le \\beta$。应用逆CDF，我们得到 $\\frac{\\ln \\sigma_0 - \\mu_{\\ell,i}}{s_{\\ell,i}} \\le z_{\\beta}$，重排后为 $\\ln \\sigma_0 \\le \\mu_{\\ell,i} + z_{\\beta} s_{\\ell,i}$。对两边取指数得到 $\\sigma_0 \\le \\exp(\\mu_{\\ell,i} + z_{\\beta} s_{\\ell,i})$。右侧是 $\\sigma_i$ 的下 $\\beta$ 分位数的定义，记为 $q_{\\sigma,i}^{\\mathrm{low}}$。因此，电导率可行性条件是：\n$$\nq_{\\sigma,i}^{\\mathrm{low}} = \\exp(\\mu_{\\ell,i} + z_{\\beta} s_{\\ell,i}) \\ge \\sigma_0\n$$\n一个候选材料 $i$ 是鲁棒可行的，当且仅当它同时满足这两个基于分位数的条件。所有这类材料的集合记为 $\\mathcal{F}$。\n\n下一步是从可行集 $\\mathcal{F}$ 中找到鲁棒帕累托集 $\\mathcal{P}$。优化目标是最小化鲁棒稳定性指标 $q_{E,i}^{\\mathrm{up}}$（值越低越好）和最大化鲁棒电导率指标 $q_{\\sigma,i}^{\\mathrm{low}}$（值越高越好）。对于任意两个可行候选者 $a, b \\in \\mathcal{F}$，如果满足以下条件，则称候选者 $a$ 支配候选者 $b$：\n$$\n(q_{E,a}^{\\mathrm{up}} \\le q_{E,b}^{\\mathrm{up}} \\land q_{\\sigma,a}^{\\mathrm{low}} \\ge q_{\\sigma,b}^{\\mathrm{low}}) \\land (q_{E,a}^{\\mathrm{up}}  q_{E,b}^{\\mathrm{up}} \\lor q_{\\sigma,a}^{\\mathrm{low}} > q_{\\sigma,b}^{\\mathrm{low}})\n$$\n这意味着 $a$ 在两个目标上至少与 $b$ 一样好，并且在至少一个目标上严格优于 $b$。鲁棒帕累托集 $\\mathcal{P}$ 是 $\\mathcal{F}$ 中所有不被 $\\mathcal{F}$ 中任何其他候选者支配的候选者的子集。寻找 $\\mathcal{P}$ 的算法如下：对于每个候选者 $i \\in \\mathcal{F}$，将其与每个其他候选者 $j \\in \\mathcal{F}$ 进行比较。如果有任何 $j$ 支配 $i$，那么 $i$ 就不在帕累托集中。如果在检查了所有其他可行候选者之后不存在这样的 $j$，那么 $i$ 就是 $\\mathcal{P}$ 的一个成员。\n\n最后，我们计算每个候选者的选择概率。这个概率是基于从非空帕累托集中均匀随机选择的策略来定义的。如果帕累托集 $\\mathcal{P}$ 非空（$|\\mathcal{P}| > 0$），则候选者 $i$ 的选择概率 $p^{\\mathrm{sel}}_i$ 为：\n$$\np^{\\mathrm{sel}}_i =\n\\begin{cases}\n1/|\\mathcal{P}|   \\text{if } i \\in \\mathcal{P} \\\\\n0   \\text{if } i \\notin \\mathcal{P}\n\\end{cases}\n$$\n如果帕累托集为空（$\\mathcal{P} = \\emptyset$），这发生在可行集 $\\mathcal{F}$ 为空时，则所有候选者的选择概率均为 $0$。\n\n每个测试用例的总体流程如下：\n1.  读取控制参数 $\\alpha$、$\\beta$、$\\sigma_0$ 和候选材料数据 $(\\mu_{E,i}, s_{E,i}, \\mu_{\\ell,i}, s_{\\ell,i})$。\n2.  使用数值库计算必要的标准正态分位数 $z_{1-\\alpha}$ 和 $z_{\\beta}$。\n3.  对于每个候选者 $i$：\n    a. 计算鲁棒性能指标 $q_{E,i}^{\\mathrm{up}}$ 和 $q_{\\sigma,i}^{\\mathrm{low}}$。\n    b. 检查是否满足 $q_{E,i}^{\\mathrm{up}} \\le 0$ 和 $q_{\\sigma,i}^{\\mathrm{low}} \\ge \\sigma_0$。\n    c. 如果可行，存储候选者的索引及其目标向量 $(q_{E,i}^{\\mathrm{up}}, q_{\\sigma,i}^{\\mathrm{low}})$。\n4.  通过识别所有非支配成员，从可行候选者集合中构建帕累托集 $\\mathcal{P}$。\n5.  根据 $\\mathcal{P}$ 的大小和构成，计算所有初始候选者的选择概率。\n6.  收集当前测试用例的概率列表，并对所有用例重复此过程。最终输出是这些概率列表的列表。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Solves the robust materials screening problem for all test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"params\": {\"alpha\": 0.1, \"beta\": 0.1, \"sigma_0\": 1.5},\n            \"candidates\": [\n                # M1: mu_E, s_E, mu_l, s_l\n                (-1.0, 0.5, np.log(1.8), 0.1),\n                # M2\n                (-0.5, 0.3, np.log(1.2), 0.2),\n                # M3\n                (1.5, 1.0, np.log(2.5), 0.3),\n                # M4\n                (4.0, 2.0, np.log(4.0), 0.25),\n                # M5\n                (0.1, 0.7, np.log(0.8), 0.2)\n            ]\n        },\n        {\n            \"params\": {\"alpha\": 0.01, \"beta\": 0.05, \"sigma_0\": 2.0},\n            \"candidates\": [\n                # M1\n                (-1.0, 0.5, np.log(1.8), 0.1),\n                # M2\n                (-0.5, 0.3, np.log(1.2), 0.2),\n                # M3\n                (1.5, 1.0, np.log(2.5), 0.3),\n                # M4\n                (4.0, 2.0, np.log(4.0), 0.25),\n                # M5\n                (0.1, 0.7, np.log(0.8), 0.2)\n            ]\n        },\n        {\n            \"params\": {\"alpha\": 0.2, \"beta\": 0.2, \"sigma_0\": 1.5},\n            \"candidates\": [\n                # F1\n                (0.0, 0.0, np.log(1.7), 0.0),\n                # F2\n                (-0.5, 0.3, np.log(2.0), 0.3),\n                # F3\n                (4.0, 8.0, np.log(0.5), 0.4)\n            ]\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        params = case[\"params\"]\n        candidates_data = case[\"candidates\"]\n        \n        alpha = params[\"alpha\"]\n        beta = params[\"beta\"]\n        sigma_0 = params[\"sigma_0\"]\n        num_candidates = len(candidates_data)\n\n        # Compute standard normal quantiles\n        z_1_minus_alpha = norm.ppf(1 - alpha)\n        z_beta = norm.ppf(beta)\n        \n        feasible_candidates = []\n        for i, (mu_E, s_E, mu_l, s_l) in enumerate(candidates_data):\n            # Calculate robust performance metrics (quantiles)\n            q_E_up = mu_E + z_1_minus_alpha * s_E\n            q_sigma_low = np.exp(mu_l + z_beta * s_l)\n            \n            # Check for robust feasibility\n            is_stable = (q_E_up = 0)\n            is_conductive = (q_sigma_low = sigma_0)\n            \n            if is_stable and is_conductive:\n                feasible_candidates.append({\n                    \"id\": i,\n                    \"q_E\": q_E_up,\n                    \"q_sigma\": q_sigma_low\n                })\n\n        # Construct the robust Pareto set\n        pareto_set_indices = []\n        if feasible_candidates:\n            for i, cand_i in enumerate(feasible_candidates):\n                is_dominated = False\n                for j, cand_j in enumerate(feasible_candidates):\n                    if i == j:\n                        continue\n                    \n                    # Check if cand_j dominates cand_i\n                    # Objectives: minimize q_E, maximize q_sigma\n                    # Dominance: q_E_j = q_E_i AND q_sigma_j = q_sigma_i (with one strict)\n                    c1 = cand_j[\"q_E\"] = cand_i[\"q_E\"]\n                    c2 = cand_j[\"q_sigma\"] = cand_i[\"q_sigma\"]\n                    c3 = cand_j[\"q_E\"]  cand_i[\"q_E\"] or cand_j[\"q_sigma\"]  cand_i[\"q_sigma\"]\n                    \n                    if c1 and c2 and c3:\n                        is_dominated = True\n                        break\n                \n                if not is_dominated:\n                    pareto_set_indices.append(cand_i[\"id\"])\n\n        # Calculate selection probabilities\n        probabilities = [0.0] * num_candidates\n        if pareto_set_indices:\n            prob = 1.0 / len(pareto_set_indices)\n            for idx in pareto_set_indices:\n                probabilities[idx] = prob\n        \n        results.append(probabilities)\n\n    # Final print statement in the exact required format.\n    # The default string representation of a list of lists matches the example format.\n    print(results)\n\nsolve()\n```"
        }
    ]
}