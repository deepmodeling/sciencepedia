{
    "hands_on_practices": [
        {
            "introduction": "The journey of applying machine learning to materials discovery begins with a fundamental question: how do we represent a chemical composition in a language that algorithms can understand? This exercise introduces a powerful and widely-used technique for this task, creating feature vectors from elemental properties using the Materials-Agnostic Platform for Informatics and Exploration (MAGPIE) approach. By calculating these compositional descriptors for a well-known cathode material, you will gain hands-on experience in feature engineering and critically evaluate the physical assumptions that enable this efficient representation of materials .",
            "id": "3926519",
            "problem": "In an automated battery design and simulation pipeline for layered oxide cathode materials, screening is accelerated by using compositional descriptors derived from the Materials-Agnostic Platform for Informatics and Exploration (MAGPIE). These descriptors aggregate elemental properties across a composition via statistical summaries computed with stoichiometric atomic fractions. As a context-appropriate fundamental base, you may use the following: for a compound with stoichiometric coefficients, the atomic fraction $x_{i}$ of element $i$ is its stoichiometric coefficient divided by the sum of all stoichiometric coefficients; for a property $p_{i}$ associated with element $i$, the composition-weighted mean is $\\mu_{p}=\\sum_{i}x_{i}p_{i}$ and the composition-weighted variance is $\\sigma_{p}^{2}=\\sum_{i}x_{i}\\left(p_{i}-\\mu_{p}\\right)^{2}$.\n\nConsider the layered oxide $\\text{LiNi}_{0.6}\\text{Mn}_{0.2}\\text{Co}_{0.2}\\text{O}_{2}$ with stoichiometry $\\text{Li}_{1}\\text{Ni}_{0.6}\\text{Mn}_{0.2}\\text{Co}_{0.2}\\text{O}_{2}$. Use the following elemental properties:\n- Pauling electronegativity $\\chi$:\n  - $\\text{Li}$: $\\chi=0.98$\n  - $\\text{Ni}$: $\\chi=1.91$\n  - $\\text{Mn}$: $\\chi=1.55$\n  - $\\text{Co}$: $\\chi=1.88$\n  - $\\text{O}$: $\\chi=3.44$\n- Covalent radius $r$ (in $\\mathrm{\\AA}$):\n  - $\\text{Li}$: $r=1.28$\n  - $\\text{Ni}$: $r=1.24$\n  - $\\text{Mn}$: $r=1.39$\n  - $\\text{Co}$: $r=1.26$\n  - $\\text{O}$: $r=0.66$\n\nConstruct the MAGPIE-style descriptor components for this composition for the two properties above by computing the composition-weighted mean electronegativity $\\mu_{\\chi}$, the composition-weighted variance $\\sigma_{\\chi}^{2}$, and the composition-weighted mean covalent radius $\\mu_{r}$. From these, define a scalar screening score\n$$\nS=\\alpha\\,\\mu_{\\chi}+\\beta\\,\\sigma_{\\chi}^{2}+\\gamma\\,\\mu_{r},\n$$\nwith coefficients $\\alpha=0.90$, $\\beta=-0.25$, and $\\gamma=0.30\\,\\mathrm{\\AA}^{-1}$ chosen to yield a dimensionless score. Compute $S$ and round your final answer to four significant figures. Report $S$ as a dimensionless quantity. Additionally, explain the assumptions underlying the statistical aggregation of elemental properties in this descriptor construction, including how these assumptions interface with machine learning for accelerated materials screening.",
            "solution": "The problem statement is evaluated to be valid. It is scientifically grounded in the principles of computational materials science and machine learning for materials discovery. The compound $\\text{LiNi}_{0.6}\\text{Mn}_{0.2}\\text{Co}_{0.2}\\text{O}_{2}$ is a well-established cathode material. The use of MAGPIE-style descriptors, based on statistical aggregation of elemental properties, is a standard technique in materials informatics. The problem is well-posed, providing all necessary definitions, data, and constants to arrive at a unique, verifiable solution. The language is objective and the tasks are unambiguous.\n\nThe solution proceeds in two parts: first, the numerical computation of the screening score $S$; second, an explanation of the underlying assumptions and their context in machine learning.\n\nPart 1: Computation of the Screening Score $S$\n\nThe first step is to determine the atomic fraction, $x_i$, for each element in the compound $\\text{Li}_{1}\\text{Ni}_{0.6}\\text{Mn}_{0.2}\\text{Co}_{0.2}\\text{O}_{2}$. The stoichiometric coefficients are $s_{\\mathrm{Li}}=1$, $s_{\\mathrm{Ni}}=0.6$, $s_{\\mathrm{Mn}}=0.2$, $s_{\\mathrm{Co}}=0.2$, and $s_{\\mathrm{O}}=2$.\n\nThe sum of the stoichiometric coefficients, $S_{total}$, is:\n$$\nS_{total} = s_{\\mathrm{Li}} + s_{\\mathrm{Ni}} + s_{\\mathrm{Mn}} + s_{\\mathrm{Co}} + s_{\\mathrm{O}} = 1 + 0.6 + 0.2 + 0.2 + 2 = 4\n$$\nThe atomic fraction $x_i$ for each element $i$ is calculated as $x_i = s_i / S_{total}$:\n$$\nx_{\\mathrm{Li}} = \\frac{1}{4} = 0.25\n$$\n$$\nx_{\\mathrm{Ni}} = \\frac{0.6}{4} = 0.15\n$$\n$$\nx_{\\mathrm{Mn}} = \\frac{0.2}{4} = 0.05\n$$\n$$\nx_{\\mathrm{Co}} = \\frac{0.2}{4} = 0.05\n$$\n$$\nx_{\\mathrm{O}} = \\frac{2}{4} = 0.50\n$$\nThe sum of the atomic fractions is $\\sum x_i = 0.25 + 0.15 + 0.05 + 0.05 + 0.50 = 1$, as required.\n\nNext, we compute the composition-weighted mean electronegativity, $\\mu_{\\chi}$, using the formula $\\mu_{p}=\\sum_{i}x_{i}p_{i}$ and the given Pauling electronegativity values $\\chi_i$:\n$$\n\\mu_{\\chi} = x_{\\mathrm{Li}}\\chi_{\\mathrm{Li}} + x_{\\mathrm{Ni}}\\chi_{\\mathrm{Ni}} + x_{\\mathrm{Mn}}\\chi_{\\mathrm{Mn}} + x_{\\mathrm{Co}}\\chi_{\\mathrm{Co}} + x_{\\mathrm{O}}\\chi_{\\mathrm{O}}\n$$\n$$\n\\mu_{\\chi} = (0.25)(0.98) + (0.15)(1.91) + (0.05)(1.55) + (0.05)(1.88) + (0.50)(3.44)\n$$\n$$\n\\mu_{\\chi} = 0.245 + 0.2865 + 0.0775 + 0.094 + 1.72 = 2.423\n$$\n\nThen, we compute the composition-weighted variance of electronegativity, $\\sigma_{\\chi}^{2}$, using the formula $\\sigma_{p}^{2}=\\sum_{i}x_{i}(p_{i}-\\mu_{p})^{2}$:\n$$\n\\sigma_{\\chi}^{2} = x_{\\mathrm{Li}}(\\chi_{\\mathrm{Li}}-\\mu_{\\chi})^2 + x_{\\mathrm{Ni}}(\\chi_{\\mathrm{Ni}}-\\mu_{\\chi})^2 + x_{\\mathrm{Mn}}(\\chi_{\\mathrm{Mn}}-\\mu_{\\chi})^2 + x_{\\mathrm{Co}}(\\chi_{\\mathrm{Co}}-\\mu_{\\chi})^2 + x_{\\mathrm{O}}(\\chi_{\\mathrm{O}}-\\mu_{\\chi})^2\n$$\n$$\n\\sigma_{\\chi}^{2} = (0.25)(0.98-2.423)^2 + (0.15)(1.91-2.423)^2 + (0.05)(1.55-2.423)^2 + (0.05)(1.88-2.423)^2 + (0.50)(3.44-2.423)^2\n$$\n$$\n\\sigma_{\\chi}^{2} = (0.25)(-1.443)^2 + (0.15)(-0.513)^2 + (0.05)(-0.873)^2 + (0.05)(-0.543)^2 + (0.50)(1.017)^2\n$$\n$$\n\\sigma_{\\chi}^{2} = (0.25)(2.082249) + (0.15)(0.263169) + (0.05)(0.762129) + (0.05)(0.294849) + (0.50)(1.034289)\n$$\n$$\n\\sigma_{\\chi}^{2} \\approx 0.52056 + 0.03948 + 0.03811 + 0.01474 + 0.51714 = 1.13003\n$$\n\nNext, we compute the composition-weighted mean covalent radius, $\\mu_{r}$, using the given radii $r_i$:\n$$\n\\mu_{r} = x_{\\mathrm{Li}}r_{\\mathrm{Li}} + x_{\\mathrm{Ni}}r_{\\mathrm{Ni}} + x_{\\mathrm{Mn}}r_{\\mathrm{Mn}} + x_{\\mathrm{Co}}r_{\\mathrm{Co}} + x_{\\mathrm{O}}r_{\\mathrm{O}}\n$$\n$$\n\\mu_{r} = (0.25)(1.28\\,\\mathrm{\\AA}) + (0.15)(1.24\\,\\mathrm{\\AA}) + (0.05)(1.39\\,\\mathrm{\\AA}) + (0.05)(1.26\\,\\mathrm{\\AA}) + (0.50)(0.66\\,\\mathrm{\\AA})\n$$\n$$\n\\mu_{r} = 0.32\\,\\mathrm{\\AA} + 0.186\\,\\mathrm{\\AA} + 0.0695\\,\\mathrm{\\AA} + 0.063\\,\\mathrm{\\AA} + 0.33\\,\\mathrm{\\AA} = 0.9685\\,\\mathrm{\\AA}\n$$\n\nFinally, we compute the scalar screening score $S$ using the given formula and coefficients $\\alpha=0.90$, $\\beta=-0.25$, and $\\gamma=0.30\\,\\mathrm{\\AA}^{-1}$:\n$$\nS = \\alpha\\,\\mu_{\\chi} + \\beta\\,\\sigma_{\\chi}^{2} + \\gamma\\,\\mu_{r}\n$$\n$$\nS = (0.90)(2.423) + (-0.25)(1.13003) + (0.30\\,\\mathrm{\\AA}^{-1})(0.9685\\,\\mathrm{\\AA})\n$$\n$$\nS = 2.1807 - 0.2825075 + 0.29055\n$$\n$$\nS \\approx 2.1887425\n$$\nRounding to four significant figures, we get $S \\approx 2.189$.\n\nPart 2: Assumptions and Context in Machine Learning\n\nThe construction of these descriptors rests on several key assumptions, which represent a deliberate trade-off between physical accuracy and computational efficiency.\n\n1.  **Atomistic Additivity and Homogenization**: The fundamental assumption is that a macroscopic or bulk property of a material can be approximated by a statistical aggregation of the intrinsic properties of its constituent elements. This \"bag of atoms\" approach effectively homogenizes the material, treating it as a simple mixture. It largely ignores the complex, specific details of the crystal structure, local coordination environments, bond types (ionic, covalent, metallic), oxidation states, and long-range electronic and structural interactions. For instance, the Pauling electronegativity of an element is a fixed value, but its effective electronegativity within a compound is modulated by its chemical environment.\n\n2.  **Stoichiometric Proportionality**: The use of atomic fractions as weights presumes that each atom's contribution to the aggregate property is directly proportional to its abundance in the chemical formula. This is a first-order approximation that does not account for the non-linear effects that arise from the specific arrangement and interaction of different atoms. A minority element, for example, might have a disproportionately large effect on a property if it occupies a critical site in the crystal lattice.\n\n3.  **Context-Independent Elemental Properties**: The model uses elemental properties (e.g., covalent radius) that are defined for an element in a reference state. The problem specifies covalent radius, which is most relevant for covalent bonds. However, a material like $\\text{LiNi}_{0.6}\\text{Mn}_{0.2}\\text{Co}_{0.2}\\text{O}_{2}$ has bonding with significant ionic character. Using covalent radius instead of, for example, ionic radius is a simplification. The descriptor generation framework accepts this imperfection, hypothesizing that even if the absolute values are not perfectly representative, their relative trends and weighted averages still contain predictive information.\n\nThese assumptions are crucial for the interface with machine learning for accelerated materials screening:\n\n-   **Feature Engineering**: The core task of machine learning is to learn a mapping from input features to an output target. These statistically-aggregated descriptors ($\\mu_{\\chi}$, $\\sigma_{\\chi}^{2}$, $\\mu_{r}$, etc.) serve as a fixed-length numerical feature vector that represents a chemical composition. This transformation from a chemical formula to a vector is essential for the application of standard machine learning algorithms.\n\n-   **Abstraction and Dimensionality Reduction**: Instead of describing a material by the explicit coordinates of every atom in a unit cell (a high-dimensional and variable-size representation), these descriptors provide a low-dimensional, composition-based abstraction. This simplification is what makes the screening process computationally tractable.\n\n-   **Surrogate Modeling**: The purpose of this approach is to create a fast, approximate \"surrogate model\". Instead of conducting expensive, high-fidelity quantum mechanical simulations (like Density Functional Theory) or time-consuming laboratory experiments for every one of millions of potential material compositions, one can calculate these simple descriptors nearly instantaneously. A machine learning model, trained on a dataset where both descriptors and a target property (e.g., battery voltage, ionic conductivity) are known, learns the complex, non-linear relationships between the approximate features and the true material behavior.\n\n-   **Accelerated Screening Pipeline**: The machine learning model, once trained, can predict the target property for millions of hypothetical compositions in minutes or hours. This allows researchers to rapidly screen a vast chemical space and down-select a small number of promising candidates for more rigorous, expensive validation. The success of this methodology hinges on the fact that while the underlying assumptions are physically simplistic, the resulting features retain enough of the essential chemical and physical \"signal\" for the machine learning model to identify meaningful correlations and make useful predictions. The variance term, $\\sigma_{\\chi}^{2}$, is particularly important as it captures the degree of elemental heterogeneity, a property that a simple mean would miss entirely.",
            "answer": "$$\n\\boxed{2.189}\n$$"
        },
        {
            "introduction": "A trained machine learning model is only as reliable as our ability to evaluate it, and in materials science, the \"ground truth\" data from experiments or simulations is often noisy. This label noise can artificially inflate performance metrics like the Mean Absolute Error (MAE), obscuring the model's true predictive power. This practice tackles this critical issue head-on, guiding you to derive the expected error contribution from label noise and then use that insight to construct a de-noised performance estimator for a more accurate model assessment .",
            "id": "3926506",
            "problem": "A laboratory conducting high-throughput screening for lithium-ion battery cathode candidates uses a supervised regression model to predict a latent property $y^{\\star}(x)$, such as the $0$ K formation energy per formula unit, from a descriptor vector $x$ derived from first-principles features. The model is well-specified, in the sense that for any input $x$ the prediction satisfies $f(x) = y^{\\star}(x)$. However, due to measurement or computational variability, the observed label for a candidate is corrupted by additive noise, $\\tilde{y} = y^{\\star} + \\epsilon$, with $\\epsilon \\sim \\mathcal{N}(0, \\sigma^{2})$ independent of $x$ and of $y^{\\star}$. The evaluation on a hold-out set of size $n$ uses the Mean Absolute Error (MAE), defined as $\\mathrm{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} \\left| \\tilde{y}_{i} - f(x_{i}) \\right|$. In addition, for a subset of $m$ items, two independent replicate labels are recorded, $\\tilde{y}_{i1} = y_{i}^{\\star} + \\epsilon_{i1}$ and $\\tilde{y}_{i2} = y_{i}^{\\star} + \\epsilon_{i2}$, with $\\epsilon_{i1}, \\epsilon_{i2} \\overset{\\text{iid}}{\\sim} \\mathcal{N}(0, \\sigma^{2})$. The goal is to quantify and correct the impact of label noise on the expected MAE.\n\nStarting from the definitions of expectation, absolute value, and the normal (Gaussian) distribution, and using only standard probabilistic facts, perform the following:\n\n$1.$ Compute $\\mathbb{E}\\!\\left[ \\mathrm{MAE} \\right]$ as a function of the label noise standard deviation $\\sigma$ under the well-specified model assumption $f(x) = y^{\\star}(x)$.\n\n$2.$ Using the errors-in-variables (EIV) framework and the replicate measurements, derive a closed-form de-noising estimator for the latent noise-free MAE by expressing an estimator $\\widehat{\\mathrm{MAE}}_{\\text{denoise}}$ as the observed sample MAE corrected by a consistent estimator of $\\mathbb{E}\\!\\left[ |\\epsilon| \\right]$. Express the final $\\widehat{\\mathrm{MAE}}_{\\text{denoise}}$ in terms of $n$, $m$, the observed residuals $\\left| \\tilde{y}_{i} - f(x_{i}) \\right|$, and the replicate differences $\\left| \\tilde{y}_{i1} - \\tilde{y}_{i2} \\right|$.\n\nYour final answer must be a single closed-form analytic expression or a pair of expressions presented as a single row matrix using LaTeX. No rounding is required. If any physical units are used, express them verbally in the derivation; the final boxed answer must not include units. Define all acronyms on first use, including Mean Absolute Error (MAE) and errors-in-variables (EIV).",
            "solution": "The problem statement has been validated and is deemed to be scientifically grounded, well-posed, objective, and self-contained. It presents a standard problem in statistical modeling and estimation theory, applied to a realistic context in materials science. There are no contradictions, ambiguities, or instances of pseudoscience. We may proceed with the solution.\n\nThe problem asks for two derivations related to the Mean Absolute Error (MAE) in a supervised regression setting where the labels are corrupted by additive Gaussian noise.\n\n**1. Computation of the Expected Mean Absolute Error, $\\mathbb{E}\\!\\left[ \\mathrm{MAE} \\right]$**\n\nThe Mean Absolute Error (MAE) is defined on a hold-out set of size $n$ as:\n$$\n\\mathrm{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} \\left| \\tilde{y}_{i} - f(x_{i}) \\right|\n$$\nWe want to compute its expectation, $\\mathbb{E}\\!\\left[ \\mathrm{MAE} \\right]$. By the linearity of expectation, we can write:\n$$\n\\mathbb{E}\\!\\left[ \\mathrm{MAE} \\right] = \\mathbb{E}\\!\\left[ \\frac{1}{n} \\sum_{i=1}^{n} \\left| \\tilde{y}_{i} - f(x_{i}) \\right| \\right] = \\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{E}\\!\\left[ \\left| \\tilde{y}_{i} - f(x_{i}) \\right| \\right]\n$$\nThe problem states that the observed label is $\\tilde{y}_{i} = y^{\\star}_{i} + \\epsilon_{i}$, where $\\epsilon_{i} \\sim \\mathcal{N}(0, \\sigma^{2})$. It also provides the crucial condition that the model is \"well-specified\", meaning $f(x_{i}) = y^{\\star}_{i}$ for all $i$. Substituting these into the expression inside the expectation gives:\n$$\n\\tilde{y}_{i} - f(x_{i}) = (y^{\\star}_{i} + \\epsilon_{i}) - y^{\\star}_{i} = \\epsilon_{i}\n$$\nTherefore, the expectation simplifies to:\n$$\n\\mathbb{E}\\!\\left[ \\mathrm{MAE} \\right] = \\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{E}\\!\\left[ \\left| \\epsilon_{i} \\right| \\right]\n$$\nSince each noise term $\\epsilon_{i}$ is drawn from the same distribution $\\mathcal{N}(0, \\sigma^{2})$, the expectation $\\mathbb{E}\\!\\left[ |\\epsilon_{i}| \\right]$ is constant for all $i$. Let's denote a generic random variable from this distribution as $Z \\sim \\mathcal{N}(0, \\sigma^{2})$. The expectation becomes:\n$$\n\\mathbb{E}\\!\\left[ \\mathrm{MAE} \\right] = \\frac{1}{n} \\cdot n \\cdot \\mathbb{E}\\!\\left[ |Z| \\right] = \\mathbb{E}\\!\\left[ |Z| \\right]\n$$\nWe compute $\\mathbb{E}\\!\\left[ |Z| \\right]$ using the definition of expectation for a continuous random variable. The probability density function (PDF) of $Z$ is $p(z) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{z^2}{2\\sigma^2}\\right)$.\n$$\n\\mathbb{E}\\!\\left[ |Z| \\right] = \\int_{-\\infty}^{\\infty} |z| p(z) dz = \\int_{-\\infty}^{\\infty} |z| \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{z^2}{2\\sigma^2}\\right) dz\n$$\nSince the integrand $|z|p(z)$ is an even function, we can simplify the integral:\n$$\n\\mathbb{E}\\!\\left[ |Z| \\right] = 2 \\int_{0}^{\\infty} z \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{z^2}{2\\sigma^2}\\right) dz\n$$\nTo solve this integral, we use the substitution $u = \\frac{z^2}{2\\sigma^2}$. This gives $du = \\frac{2z}{2\\sigma^2} dz = \\frac{z}{\\sigma^2} dz$, which implies $z dz = \\sigma^2 du$. The limits of integration remain $0$ to $\\infty$.\n$$\n\\mathbb{E}\\!\\left[ |Z| \\right] = \\frac{2}{\\sigma\\sqrt{2\\pi}} \\int_{0}^{\\infty} \\exp(-u) (\\sigma^2 du) = \\frac{2\\sigma^2}{\\sigma\\sqrt{2\\pi}} \\int_{0}^{\\infty} \\exp(-u) du\n$$\n$$\n\\mathbb{E}\\!\\left[ |Z| \\right] = \\frac{2\\sigma}{\\sqrt{2\\pi}} \\left[ -\\exp(-u) \\right]_{0}^{\\infty} = \\frac{2\\sigma}{\\sqrt{2\\pi}} \\left( - \\lim_{u\\to\\infty}\\exp(-u) - (-\\exp(0)) \\right) = \\frac{2\\sigma}{\\sqrt{2\\pi}} (0 - (-1)) = \\frac{2\\sigma}{\\sqrt{2\\pi}}\n$$\nSimplifying this expression gives the final result for the first part:\n$$\n\\mathbb{E}\\!\\left[ \\mathrm{MAE} \\right] = \\sigma\\sqrt{\\frac{4}{2\\pi}} = \\sigma\\sqrt{\\frac{2}{\\pi}}\n$$\n\n**2. Derivation of the De-noised MAE Estimator, $\\widehat{\\mathrm{MAE}}_{\\text{denoise}}$**\n\nThe goal is to derive a de-noised estimator for the \"latent noise-free MAE\". The latent MAE is the error calculated with the true labels $y^{\\star}_i$:\n$$\n\\mathrm{MAE}_{\\text{latent}} = \\frac{1}{n} \\sum_{i=1}^{n} \\left| y^{\\star}_i - f(x_i) \\right|\n$$\nGiven the well-specified model assumption $f(x_i) = y^{\\star}_i$, the latent MAE is exactly $0$. Our estimator should therefore have an expectation of $0$.\n\nThe problem specifies the form of the estimator: it should be the observed sample MAE corrected by a consistent estimator of the bias term, $\\mathbb{E}[|\\epsilon|]$. This approach is related to the errors-in-variables (EIV) framework, although here the error is only in the response variable.\n$$\n\\widehat{\\mathrm{MAE}}_{\\text{denoise}} = \\mathrm{MAE}_{\\text{obs}} - \\widehat{\\mathbb{E}[|\\epsilon|]}\n$$\nwhere $\\mathrm{MAE}_{\\text{obs}} = \\frac{1}{n} \\sum_{i=1}^{n} |\\tilde{y}_i - f(x_i)|$ and $\\widehat{\\mathbb{E}[|\\epsilon|]}$ is an estimator for the bias, which we found in Part 1 to be $\\sigma\\sqrt{2/\\pi}$.\n\nTo estimate this bias, we must first estimate $\\sigma$. We use the $m$ replicate measurements for this purpose. For each of these $m$ items, we have two independent noisy labels:\n$$\n\\tilde{y}_{i1} = y_{i}^{\\star} + \\epsilon_{i1} \\quad \\text{and} \\quad \\tilde{y}_{i2} = y_{i}^{\\star} + \\epsilon_{i2}\n$$\nwhere $\\epsilon_{i1}, \\epsilon_{i2} \\overset{\\text{iid}}{\\sim} \\mathcal{N}(0, \\sigma^{2})$. Consider their difference:\n$$\n\\Delta\\tilde{y}_i = \\tilde{y}_{i1} - \\tilde{y}_{i2} = (y_{i}^{\\star} + \\epsilon_{i1}) - (y_{i}^{\\star} + \\epsilon_{i2}) = \\epsilon_{i1} - \\epsilon_{i2}\n$$\nThe difference $\\Delta\\epsilon_i = \\epsilon_{i1} - \\epsilon_{i2}$ is a linear combination of independent Gaussian random variables, and is therefore also Gaussian. Its mean is $\\mathbb{E}[\\Delta\\epsilon_i] = \\mathbb E[\\epsilon_{i1}] - \\mathbb E[\\epsilon_{i2}] = 0 - 0 = 0$. Its variance is $\\mathrm{Var}(\\Delta\\epsilon_i) = \\mathrm{Var}(\\epsilon_{i1}) + \\mathrm{Var}(\\epsilon_{i2}) = \\sigma^2 + \\sigma^2 = 2\\sigma^2$, due to independence. Thus, $\\Delta\\epsilon_i \\sim \\mathcal{N}(0, 2\\sigma^2)$.\n\nWe can now compute the expectation of the absolute value of this difference, $|\\Delta\\epsilon_i|$, using the result from Part 1 with a standard deviation of $\\sqrt{2\\sigma^2} = \\sigma\\sqrt{2}$:\n$$\n\\mathbb{E}[|\\Delta\\epsilon_i|] = (\\sigma\\sqrt{2})\\sqrt{\\frac{2}{\\pi}} = \\frac{2\\sigma}{\\sqrt{\\pi}}\n$$\nWe now have two expected values, both dependent on $\\sigma$:\n$$\n\\mathbb{E}[|\\epsilon|] = \\sigma\\sqrt{\\frac{2}{\\pi}} \\quad \\text{and} \\quad \\mathbb{E}[|\\Delta\\epsilon|] = \\frac{2\\sigma}{\\sqrt{\\pi}}\n$$\nWe can establish a relationship between them that is independent of $\\sigma$:\n$$\n\\frac{\\mathbb{E}[|\\epsilon|]}{\\mathbb{E}[|\\Delta\\epsilon|]} = \\frac{\\sigma\\sqrt{2/\\pi}}{2\\sigma/\\sqrt{\\pi}} = \\frac{\\sigma\\sqrt{2}/\\sqrt{\\pi}}{2\\sigma/\\sqrt{\\pi}} = \\frac{\\sqrt{2}}{2} = \\frac{1}{\\sqrt{2}}\n$$\nThis gives us the key analytic relationship:\n$$\n\\mathbb{E}[|\\epsilon|] = \\frac{1}{\\sqrt{2}} \\mathbb{E}[|\\Delta\\epsilon|]\n$$\nTo construct an estimator for the bias $\\mathbb{E}[|\\epsilon|]$, we can construct a consistent estimator for $\\mathbb{E}[|\\Delta\\epsilon|]$ from the $m$ replicate samples. The sample mean is a consistent estimator for the population mean:\n$$\n\\widehat{\\mathbb{E}[|\\Delta\\epsilon|]} = \\frac{1}{m} \\sum_{i=1}^{m} |\\Delta\\tilde{y}_i| = \\frac{1}{m} \\sum_{i=1}^{m} |\\tilde{y}_{i1} - \\tilde{y}_{i2}|\n$$\nSubstituting this into our relationship, we obtain a consistent estimator for the bias term:\n$$\n\\widehat{\\mathbb{E}[|\\epsilon|]} = \\frac{1}{\\sqrt{2}} \\left( \\frac{1}{m} \\sum_{i=1}^{m} |\\tilde{y}_{i1} - \\tilde{y}_{i2}| \\right) = \\frac{1}{m\\sqrt{2}} \\sum_{i=1}^{m} |\\tilde{y}_{i1} - \\tilde{y}_{i2}|\n$$\nFinally, we construct the de-noised estimator $\\widehat{\\mathrm{MAE}}_{\\text{denoise}}$ by subtracting this bias estimator from the observed MAE:\n$$\n\\widehat{\\mathrm{MAE}}_{\\text{denoise}} = \\left( \\frac{1}{n} \\sum_{i=1}^{n} |\\tilde{y}_i - f(x_i)| \\right) - \\left( \\frac{1}{m\\sqrt{2}} \\sum_{i=1}^{m} |\\tilde{y}_{i1} - \\tilde{y}_{i2}| \\right)\n$$\nThe expectation of this estimator is $\\mathbb{E}[\\widehat{\\mathrm{MAE}}_{\\text{denoise}}] = \\mathbb{E}[\\mathrm{MAE}_{\\text{obs}}] - \\mathbb{E}[\\widehat{\\mathbb{E}[|\\epsilon|]}] = \\mathbb{E}[|\\epsilon|] - \\mathbb{E}[|\\epsilon|] = 0$, which is the true value of the latent MAE, as required.\n\nThe two requested results are the expression for $\\mathbb{E}[\\mathrm{MAE}]$ and the expression for $\\widehat{\\mathrm{MAE}}_{\\text{denoise}}$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\sigma \\sqrt{\\frac{2}{\\pi}} & \\frac{1}{n} \\sum_{i=1}^{n} |\\tilde{y}_i - f(x_i)| - \\frac{1}{m\\sqrt{2}} \\sum_{i=1}^{m} |\\tilde{y}_{i1} - \\tilde{y}_{i2}|\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "The ultimate goal of accelerated screening is not just to make predictions, but to make confident decisions, often by navigating trade-offs between multiple, conflicting objectives like thermodynamic stability and ionic conductivity. This advanced practice moves beyond simple point predictions to leverage the full predictive distributions from a machine learning model. You will learn to perform robust multi-objective optimization by constructing a Pareto set of candidates that satisfy performance targets with a specified level of statistical confidence, a key step in translating probabilistic forecasts into actionable discoveries .",
            "id": "3926474",
            "problem": "You are tasked with constructing a robust Pareto set for battery electrode materials screening under prediction uncertainty in two objectives: thermodynamic stability and ionic conductivity. The design context is automated battery design and simulation using machine learning predictions for accelerated materials screening. Each candidate material is characterized by a predictive distribution for the energy above the convex hull, denoted by $\\Delta E_{\\text{hull}}$ in units of millielectronvolts per atom (meV/atom), and a predictive distribution for ionic conductivity $\\sigma$ in units of millisiemens per centimeter (mS/cm).\n\nFundamental base:\n- Suppose the machine learning model outputs the following for each material $i$:\n  - A predictive normal distribution for $\\Delta E_{\\text{hull}, i}$ with mean $\\mu_{E,i}$ and standard deviation $s_{E,i}$, modeled as $\\Delta E_{\\text{hull}, i} \\sim \\mathcal{N}(\\mu_{E,i}, s_{E,i}^2)$.\n  - A predictive log-normal distribution for conductivity $\\sigma_i$ such that $\\ln \\sigma_i \\sim \\mathcal{N}(\\mu_{\\ell,i}, s_{\\ell,i}^2)$, where $\\mu_{\\ell,i}$ and $s_{\\ell,i}$ are the mean and standard deviation of the natural logarithm of the conductivity.\n\n- Robust feasibility is enforced via chance constraints:\n  - Stability chance constraint: $\\mathbb{P}(\\Delta E_{\\text{hull}, i} \\le 0) \\ge 1-\\alpha$.\n  - Conductivity chance constraint: $\\mathbb{P}(\\sigma_i \\ge \\sigma_0) \\ge 1-\\beta$.\n  Here, $\\alpha \\in (0,1)$ and $\\beta \\in (0,1)$ are user-specified risk levels, and $\\sigma_0$ is the conductivity threshold to be met. Interpret $\\Delta E_{\\text{hull}, i} \\le 0$ meV/atom as thermodynamic stability relative to the convex hull, and $\\sigma_i \\ge \\sigma_0$ mS/cm as meeting the minimum required conductivity.\n\n- Robust performance metrics for Pareto comparison under these chance constraints use conservative quantiles:\n  - For stability, use the upper $1-\\alpha$ quantile $q_{E,i}^{\\text{up}}$ of $\\Delta E_{\\text{hull}, i}$.\n  - For conductivity, use the lower $\\beta$ quantile $q_{\\sigma,i}^{\\text{low}}$ of $\\sigma_i$.\n  These quantiles are defined using the standard normal quantile $z_p$ satisfying $\\Phi(z_p) = p$ for the cumulative distribution function $\\Phi$. For the normal variable $\\Delta E_{\\text{hull}, i}$, $q_{E,i}^{\\text{up}} = \\mu_{E,i} + z_{1-\\alpha} s_{E,i}$. For the log-normal $\\sigma_i$, $q_{\\sigma,i}^{\\text{low}} = \\exp(\\mu_{\\ell,i} + z_{\\beta} s_{\\ell,i})$. If $s_{E,i} = 0$ or $s_{\\ell,i} = 0$, interpret the distributions as deterministic so that the corresponding quantiles equal the means of the variables.\n\n- Robust Pareto set definition under feasibility:\n  - First, determine the robustly feasible set $\\mathcal{F} = \\{ i : \\mathbb{P}(\\Delta E_{\\text{hull}, i} \\le 0) \\ge 1-\\alpha, \\, \\mathbb{P}(\\sigma_i \\ge \\sigma_0) \\ge 1-\\beta \\}$.\n  - Among $i \\in \\mathcal{F}$, define candidate $a$ to dominate candidate $b$ in the robust sense if $q_{E,a}^{\\text{up}} \\le q_{E,b}^{\\text{up}}$ and $q_{\\sigma,a}^{\\text{low}} \\ge q_{\\sigma,b}^{\\text{low}}$, with at least one strict inequality. The robust Pareto set $\\mathcal{P} \\subseteq \\mathcal{F}$ is the set of candidates that are not dominated by any other feasible candidate.\n\n- Selection probabilities:\n  - For a decision policy that selects uniformly at random from $\\mathcal{P}$ when $\\mathcal{P} \\neq \\emptyset$ and selects nothing when $\\mathcal{P} = \\emptyset$, the selection probability for candidate $i$ is defined as $p^{\\text{sel}}_i = \\frac{1}{|\\mathcal{P}|}$ if $i \\in \\mathcal{P}$ and $p^{\\text{sel}}_i = 0$ otherwise.\n\nAll physical quantities must be treated in their specified units: $\\Delta E_{\\text{hull}}$ in meV/atom and $\\sigma$ in mS/cm. Angles are not part of this problem. All answers are unitless selection probabilities expressed as floating-point numbers.\n\nYour task is to implement a complete, runnable program that:\n- Evaluates robust feasibility via chance constraints for each test case.\n- Constructs the robust Pareto set using the conservative quantile metrics.\n- Computes selection probabilities per candidate as described.\n\nTest suite:\n- Case 1 (happy path):\n  - Parameters: $\\alpha = 0.1$, $\\beta = 0.1$, $\\sigma_0 = 1.5$ mS/cm.\n  - Candidates (each tuple lists ($\\mu_{E}$ [meV/atom], $s_{E}$ [meV/atom], $\\mu_{\\ell}$, $s_{\\ell}$)):\n    - M1: $(-1.0, 0.5, \\ln 1.8, 0.1)$\n    - M2: $(-0.5, 0.3, \\ln 1.2, 0.2)$\n    - M3: $(1.5, 1.0, \\ln 2.5, 0.3)$\n    - M4: $(4.0, 2.0, \\ln 4.0, 0.25)$\n    - M5: $(0.1, 0.7, \\ln 0.8, 0.2)$\n- Case 2 (strict constraints edge case):\n  - Parameters: $\\alpha = 0.01$, $\\beta = 0.05$, $\\sigma_0 = 2.0$ mS/cm.\n  - Candidates: same as Case 1.\n- Case 3 (deterministic-variable edge case):\n  - Parameters: $\\alpha = 0.2$, $\\beta = 0.2$, $\\sigma_0 = 1.5$ mS/cm.\n  - Candidates:\n    - F1: $(0.0, 0.0, \\ln 1.7, 0.0)$\n    - F2: $(-0.5, 0.3, \\ln 2.0, 0.3)$\n    - F3: $(4.0, 8.0, \\ln 0.5, 0.4)$\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a list of floating-point selection probabilities in the same order as the candidates in that test case. For example, a valid output structure is of the form `[ [p_1, p_2], [q_1, q_2, q_3] ]`.",
            "solution": "The problem requires the construction of a robust Pareto set for screening battery electrode materials under uncertainty. The goal is to identify a set of non-dominated candidates based on two conflicting objectives: maximizing thermodynamic stability and maximizing ionic conductivity. The uncertainty in the machine learning predictions for these properties is modeled by probability distributions. The solution involves a multi-step process encompassing robust feasibility analysis, multi-objective optimization, and calculation of selection probabilities.\n\nFirst, we must formalize the conditions for a candidate material to be considered 'robustly feasible'. The problem defines feasibility using chance constraints, which set a lower bound on the probability of meeting certain performance thresholds.\n\nFor thermodynamic stability, the property of interest is the energy above the convex hull, $\\Delta E_{\\text{hull}, i}$, for each material $i$. This is modeled by a normal distribution, $\\Delta E_{\\text{hull}, i} \\sim \\mathcal{N}(\\mu_{E,i}, s_{E,i}^2)$. A material is considered stable if its energy is non-positive, i.e., $\\Delta E_{\\text{hull}, i} \\le 0$. The stability chance constraint requires this condition to hold with a probability of at least $1-\\alpha$:\n$$\n\\mathbb{P}(\\Delta E_{\\text{hull}, i} \\le 0) \\ge 1-\\alpha\n$$\nTo make this condition operational, we standardize the random variable. Let $Z \\sim \\mathcal{N}(0, 1)$ be a standard normal variable. The constraint becomes:\n$$\n\\mathbb{P}\\left(\\frac{\\Delta E_{\\text{hull}, i} - \\mu_{E,i}}{s_{E,i}} \\le \\frac{0 - \\mu_{E,i}}{s_{E,i}}\\right) \\ge 1-\\alpha \\quad \\implies \\quad \\Phi\\left(\\frac{-\\mu_{E,i}}{s_{E,i}}\\right) \\ge 1-\\alpha\n$$\nwhere $\\Phi$ is the cumulative distribution function (CDF) of the standard normal distribution. Using the property $\\Phi(-x) = 1-\\Phi(x)$, this is equivalent to $1 - \\Phi\\left(\\frac{\\mu_{E,i}}{s_{E,i}}\\right) \\ge 1-\\alpha$, which simplifies to $\\Phi\\left(\\frac{\\mu_{E,i}}{s_{E,i}}\\right) \\le \\alpha$. Applying the inverse CDF, $\\Phi^{-1}$, also known as the quantile function or probit function, we get $\\frac{\\mu_{E,i}}{s_{E,i}} \\le z_{\\alpha}$, where $z_p = \\Phi^{-1}(p)$. This can be rearranged as $\\mu_{E,i} - z_{\\alpha} s_{E,i} \\le 0$. Using the identity $z_{\\alpha} = -z_{1-\\alpha}$, the condition is $\\mu_{E,i} + z_{1-\\alpha} s_{E,i} \\le 0$. This expression is precisely the upper $1-\\alpha$ quantile of $\\Delta E_{\\text{hull}, i}$, denoted $q_{E,i}^{\\text{up}}$. Thus, the stability feasibility condition is elegantly simplified to:\n$$\nq_{E,i}^{\\text{up}} = \\mu_{E,i} + z_{1-\\alpha} s_{E,i} \\le 0\n$$\n\nFor ionic conductivity, $\\sigma_i$, the property is modeled by a log-normal distribution, such that its natural logarithm follows a normal distribution: $\\ln \\sigma_i \\sim \\mathcal{N}(\\mu_{\\ell,i}, s_{\\ell,i}^2)$. The conductivity chance constraint requires $\\sigma_i$ to be at least a threshold value $\\sigma_0$ with a probability of at least $1-\\beta$:\n$$\n\\mathbb{P}(\\sigma_i \\ge \\sigma_0) \\ge 1-\\beta\n$$\nBy taking the natural logarithm, which is a monotonic function, we can work with the normally distributed variable $\\ln \\sigma_i$:\n$$\n\\mathbb{P}(\\ln \\sigma_i \\ge \\ln \\sigma_0) \\ge 1-\\beta\n$$\nStandardizing this variable yields:\n$$\n\\mathbb{P}\\left(\\frac{\\ln \\sigma_i - \\mu_{\\ell,i}}{s_{\\ell,i}} \\ge \\frac{\\ln \\sigma_0 - \\mu_{\\ell,i}}{s_{\\ell,i}}\\right) \\ge 1-\\beta \\quad \\implies \\quad 1 - \\Phi\\left(\\frac{\\ln \\sigma_0 - \\mu_{\\ell,i}}{s_{\\ell,i}}\\right) \\ge 1-\\beta\n$$\nThis simplifies to $\\Phi\\left(\\frac{\\ln \\sigma_0 - \\mu_{\\ell,i}}{s_{\\ell,i}}\\right) \\le \\beta$. Applying the inverse CDF, we get $\\frac{\\ln \\sigma_0 - \\mu_{\\ell,i}}{s_{\\ell,i}} \\le z_{\\beta}$, which rearranges to $\\ln \\sigma_0 \\le \\mu_{\\ell,i} + z_{\\beta} s_{\\ell,i}$. Taking the exponential of both sides gives $\\sigma_0 \\le \\exp(\\mu_{\\ell,i} + z_{\\beta} s_{\\ell,i})$. The right-hand side is the definition of the lower $\\beta$ quantile of $\\sigma_i$, denoted $q_{\\sigma,i}^{\\text{low}}$. So, the conductivity feasibility condition is:\n$$\nq_{\\sigma,i}^{\\text{low}} = \\exp(\\mu_{\\ell,i} + z_{\\beta} s_{\\ell,i}) \\ge \\sigma_0\n$$\nA candidate material $i$ is robustly feasible if and only if it satisfies both of these quantile-based conditions. The set of all such materials is denoted $\\mathcal{F}$.\n\nThe next step is to find the robust Pareto set, $\\mathcal{P}$, from the feasible set $\\mathcal{F}$. The optimization objectives are to minimize the robust stability metric $q_{E,i}^{\\text{up}}$ (a lower value is better) and maximize the robust conductivity metric $q_{\\sigma,i}^{\\text{low}}$ (a higher value is better). For any two feasible candidates $a, b \\in \\mathcal{F}$, candidate $a$ is said to dominate candidate $b$ if:\n$$\n(q_{E,a}^{\\text{up}} \\le q_{E,b}^{\\text{up}} \\land q_{\\sigma,a}^{\\text{low}} \\ge q_{\\sigma,b}^{\\text{low}}) \\land (q_{E,a}^{\\text{up}}  q_{E,b}^{\\text{up}} \\lor q_{\\sigma,a}^{\\text{low}} > q_{\\sigma,b}^{\\text{low}})\n$$\nThis means $a$ is at least as good as $b$ on both objectives and strictly better on at least one. The robust Pareto set $\\mathcal{P}$ is the subset of $\\mathcal{F}$ containing all candidates that are not dominated by any other candidate in $\\mathcal{F}$. The algorithm to find $\\mathcal{P}$ is as follows: for each candidate $i \\in \\mathcal{F}$, compare it with every other candidate $j \\in \\mathcal{F}$. If any $j$ dominates $i$, then $i$ is not in the Pareto set. If no such $j$ exists after checking all other feasible candidates, then $i$ is a member of $\\mathcal{P}$.\n\nFinally, we calculate the selection probability for each candidate. This probability is defined based on a policy of selecting uniformly at random from the non-empty Pareto set. If the Pareto set $\\mathcal{P}$ is not empty ($|\\mathcal{P}| > 0$), the selection probability $p^{\\text{sel}}_i$ for a candidate $i$ is:\n$$\np^{\\text{sel}}_i =\n\\begin{cases}\n1/|\\mathcal{P}|  \\text{if } i \\in \\mathcal{P} \\\\\n0  \\text{if } i \\notin \\mathcal{P}\n\\end{cases}\n$$\nIf the Pareto set is empty ($\\mathcal{P} = \\emptyset$), which occurs when the feasible set $\\mathcal{F}$ is empty, the selection probability is $0$ for all candidates.\n\nThe overall procedure for each test case is:\n1.  Read the control parameters $\\alpha$, $\\beta$, $\\sigma_0$, and the candidate material data $(\\mu_{E,i}, s_{E,i}, \\mu_{\\ell,i}, s_{\\ell,i})$.\n2.  Compute the necessary standard normal quantiles, $z_{1-\\alpha}$ and $z_{\\beta}$, using a numerical library.\n3.  For each candidate $i$:\n    a. Calculate the robust performance metrics $q_{E,i}^{\\text{up}}$ and $q_{\\sigma,i}^{\\text{low}}$.\n    b. Check if $q_{E,i}^{\\text{up}} \\le 0$ and $q_{\\sigma,i}^{\\text{low}} \\ge \\sigma_0$.\n    c. If feasible, store the candidate's index and its objective vector $(q_{E,i}^{\\text{up}}, q_{\\sigma,i}^{\\text{low}})$.\n4.  Construct the Pareto set $\\mathcal{P}$ from the set of feasible candidates by identifying all non-dominated members.\n5.  Calculate the selection probabilities for all initial candidates based on the size and composition of $\\mathcal{P}$.\n6.  Collect the list of probabilities for the current test case and repeat for all cases. The final output is a list of these probability lists.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Solves the robust materials screening problem for all test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"params\": {\"alpha\": 0.1, \"beta\": 0.1, \"sigma_0\": 1.5},\n            \"candidates\": [\n                # M1: mu_E, s_E, mu_l, s_l\n                (-1.0, 0.5, np.log(1.8), 0.1),\n                # M2\n                (-0.5, 0.3, np.log(1.2), 0.2),\n                # M3\n                (1.5, 1.0, np.log(2.5), 0.3),\n                # M4\n                (4.0, 2.0, np.log(4.0), 0.25),\n                # M5\n                (0.1, 0.7, np.log(0.8), 0.2)\n            ]\n        },\n        {\n            \"params\": {\"alpha\": 0.01, \"beta\": 0.05, \"sigma_0\": 2.0},\n            \"candidates\": [\n                # M1\n                (-1.0, 0.5, np.log(1.8), 0.1),\n                # M2\n                (-0.5, 0.3, np.log(1.2), 0.2),\n                # M3\n                (1.5, 1.0, np.log(2.5), 0.3),\n                # M4\n                (4.0, 2.0, np.log(4.0), 0.25),\n                # M5\n                (0.1, 0.7, np.log(0.8), 0.2)\n            ]\n        },\n        {\n            \"params\": {\"alpha\": 0.2, \"beta\": 0.2, \"sigma_0\": 1.5},\n            \"candidates\": [\n                # F1\n                (0.0, 0.0, np.log(1.7), 0.0),\n                # F2\n                (-0.5, 0.3, np.log(2.0), 0.3),\n                # F3\n                (4.0, 8.0, np.log(0.5), 0.4)\n            ]\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        params = case[\"params\"]\n        candidates_data = case[\"candidates\"]\n        \n        alpha = params[\"alpha\"]\n        beta = params[\"beta\"]\n        sigma_0 = params[\"sigma_0\"]\n        num_candidates = len(candidates_data)\n\n        # Compute standard normal quantiles\n        z_1_minus_alpha = norm.ppf(1 - alpha)\n        z_beta = norm.ppf(beta)\n        \n        feasible_candidates = []\n        for i, (mu_E, s_E, mu_l, s_l) in enumerate(candidates_data):\n            # Calculate robust performance metrics (quantiles)\n            q_E_up = mu_E + z_1_minus_alpha * s_E\n            q_sigma_low = np.exp(mu_l + z_beta * s_l)\n            \n            # Check for robust feasibility\n            is_stable = (q_E_up = 0)\n            is_conductive = (q_sigma_low = sigma_0)\n            \n            if is_stable and is_conductive:\n                feasible_candidates.append({\n                    \"id\": i,\n                    \"q_E\": q_E_up,\n                    \"q_sigma\": q_sigma_low\n                })\n\n        # Construct the robust Pareto set\n        pareto_set_indices = []\n        if feasible_candidates:\n            for i, cand_i in enumerate(feasible_candidates):\n                is_dominated = False\n                for j, cand_j in enumerate(feasible_candidates):\n                    if i == j:\n                        continue\n                    \n                    # Check if cand_j dominates cand_i\n                    # Objectives: minimize q_E, maximize q_sigma\n                    # Dominance: q_E_j = q_E_i AND q_sigma_j = q_sigma_i (with one strict)\n                    c1 = cand_j[\"q_E\"] = cand_i[\"q_E\"]\n                    c2 = cand_j[\"q_sigma\"] = cand_i[\"q_sigma\"]\n                    c3 = cand_j[\"q_E\"]  cand_i[\"q_E\"] or cand_j[\"q_sigma\"]  cand_i[\"q_sigma\"]\n                    \n                    if c1 and c2 and c3:\n                        is_dominated = True\n                        break\n                \n                if not is_dominated:\n                    pareto_set_indices.append(cand_i[\"id\"])\n\n        # Calculate selection probabilities\n        probabilities = [0.0] * num_candidates\n        if pareto_set_indices:\n            prob = 1.0 / len(pareto_set_indices)\n            for idx in pareto_set_indices:\n                probabilities[idx] = prob\n        \n        results.append(probabilities)\n\n    # Final print statement in the exact required format.\n    # The default string representation of a list of lists matches the example format.\n    print(results)\n\nsolve()\n```"
        }
    ]
}