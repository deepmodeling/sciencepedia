{
    "hands_on_practices": [
        {
            "introduction": "High-fidelity battery simulations often produce vast amounts of data, such as the potential field across the electrolyte at thousands of points. Surrogate modeling begins by asking if we can capture the essential behavior of this field using a much simpler representation. This exercise explores Principal Component Analysis (PCA), a powerful technique for identifying the most dominant spatial patterns in simulation data and building efficient linear surrogates based on them. By working through this problem, you will derive the fundamental error limit of such approximations, linking the abstract mathematics of eigenvalues to the tangible accuracy of a reduced-order model. ",
            "id": "3926888",
            "problem": "A zero-mean electrolyte potential field in a lithium-ion cell electrolyte domain is represented after spatial discretization by a random vector $\\boldsymbol{\\phi}\\in\\mathbb{R}^{M}$ with covariance matrix $\\mathbf{C}=\\mathbb{E}[\\boldsymbol{\\phi}\\boldsymbol{\\phi}^{\\top}]$. Assume $\\mathbf{C}$ is symmetric positive semidefinite with an eigendecomposition $\\mathbf{C}=\\mathbf{U}\\,\\boldsymbol{\\Lambda}\\,\\mathbf{U}^{\\top}$, where $\\mathbf{U}=\\big[\\mathbf{u}_{1},\\dots,\\mathbf{u}_{M}\\big]$ is orthonormal and $\\boldsymbol{\\Lambda}=\\operatorname{diag}(\\lambda_{1},\\dots,\\lambda_{M})$ has eigenvalues ordered $\\lambda_{1}\\ge\\lambda_{2}\\ge\\cdots\\ge\\lambda_{M}\\ge 0$. Consider linear rank-$k$ surrogates for the electrolyte potential field obtained by orthogonal projection onto a $k$-dimensional subspace, and in particular the subspace spanned by the top $k$ eigenvectors (Principal Component Analysis (PCA)). Throughout, use the Euclidean norm and assume the discretization is scaled so that $\\|\\boldsymbol{\\phi}\\|_{2}^{2}$ has units of volt-squared.\n\nTask:\n- Starting only from the definitions of covariance, orthogonal projection, and the spectral theorem, derive an expression for the minimal achievable expected squared reconstruction error $\\mathbb{E}\\big[\\|\\boldsymbol{\\phi}-\\hat{\\boldsymbol{\\phi}}\\|_{2}^{2}\\big]$ among all rank-$k$ linear surrogates, and show that the PCA surrogate attains this minimum. Express the result in terms of the eigenvalues $\\{\\lambda_{i}\\}_{i=1}^{M}$. Then, interpret its physical meaning for electrolyte potential fields.\n- For a particular electrolyte dataset discretized to $M\\ge 5$ nodes, the top five eigenvalues (in volt-squared) are measured to be\n$$\n\\lambda_{1}=3.2\\times 10^{-4},\\quad\n\\lambda_{2}=1.6\\times 10^{-4},\\quad\n\\lambda_{3}=8.0\\times 10^{-5},\\quad\n\\lambda_{4}=4.0\\times 10^{-5},\\quad\n\\lambda_{5}=2.0\\times 10^{-5},\n$$\nwith all subsequent eigenvalues negligible for the purpose of this question. If a PCA surrogate with $k=2$ is used, compute the numerical value of the minimal expected squared reconstruction error bound implied by your derivation. Round your answer to four significant figures and express it in volt-squared.",
            "solution": "We begin from the definitions. Let $\\boldsymbol{\\phi}\\in\\mathbb{R}^{M}$ be a zero-mean random vector with covariance $\\mathbf{C}=\\mathbb{E}[\\boldsymbol{\\phi}\\boldsymbol{\\phi}^{\\top}]$. Consider any rank-$k$ linear surrogate formed by an orthogonal projector $\\mathbf{P}$ onto a $k$-dimensional subspace, so that the reconstruction is $\\hat{\\boldsymbol{\\phi}}=\\mathbf{P}\\boldsymbol{\\phi}$ with $\\mathbf{P}^{2}=\\mathbf{P}$ and $\\mathbf{P}^{\\top}=\\mathbf{P}$, and $\\operatorname{rank}(\\mathbf{P})=k$. The expected squared reconstruction error is\n$$\n\\mathbb{E}\\big[\\|\\boldsymbol{\\phi}-\\hat{\\boldsymbol{\\phi}}\\|_{2}^{2}\\big]\n=\\mathbb{E}\\big[\\|(\\mathbf{I}-\\mathbf{P})\\boldsymbol{\\phi}\\|_{2}^{2}\\big]\n=\\mathbb{E}\\big[\\boldsymbol{\\phi}^{\\top}(\\mathbf{I}-\\mathbf{P})^{\\top}(\\mathbf{I}-\\mathbf{P})\\boldsymbol{\\phi}\\big].\n$$\nSince $\\mathbf{P}$ is an orthogonal projector, $(\\mathbf{I}-\\mathbf{P})^{\\top}(\\mathbf{I}-\\mathbf{P})=\\mathbf{I}-\\mathbf{P}$. Using the cyclic property of the trace and the fact that $\\mathbb{E}[\\boldsymbol{\\phi}\\boldsymbol{\\phi}^{\\top}]=\\mathbf{C}$, we have\n$$\n\\mathbb{E}\\big[\\|\\boldsymbol{\\phi}-\\hat{\\boldsymbol{\\phi}}\\|_{2}^{2}\\big]\n=\\mathbb{E}\\big[\\operatorname{tr}\\big((\\mathbf{I}-\\mathbf{P})\\boldsymbol{\\phi}\\boldsymbol{\\phi}^{\\top}\\big)\\big]\n=\\operatorname{tr}\\big((\\mathbf{I}-\\mathbf{P})\\,\\mathbb{E}[\\boldsymbol{\\phi}\\boldsymbol{\\phi}^{\\top}]\\big)\n=\\operatorname{tr}\\big((\\mathbf{I}-\\mathbf{P})\\,\\mathbf{C}\\big).\n$$\nWrite the spectral decomposition $\\mathbf{C}=\\mathbf{U}\\boldsymbol{\\Lambda}\\mathbf{U}^{\\top}$ and define $\\tilde{\\mathbf{P}}=\\mathbf{U}^{\\top}\\mathbf{P}\\mathbf{U}$, which is also an orthogonal projector of rank $k$ (congruence preserves idempotency and rank). Then\n$$\n\\operatorname{tr}\\big((\\mathbf{I}-\\mathbf{P})\\,\\mathbf{C}\\big)\n=\\operatorname{tr}\\big((\\mathbf{I}-\\mathbf{P})\\,\\mathbf{U}\\boldsymbol{\\Lambda}\\mathbf{U}^{\\top}\\big)\n=\\operatorname{tr}\\big(\\mathbf{U}^{\\top}(\\mathbf{I}-\\mathbf{P})\\mathbf{U}\\,\\boldsymbol{\\Lambda}\\big)\n=\\operatorname{tr}\\big((\\mathbf{I}-\\tilde{\\mathbf{P}})\\,\\boldsymbol{\\Lambda}\\big).\n$$\nBecause $\\boldsymbol{\\Lambda}$ is diagonal with entries $\\lambda_{1},\\dots,\\lambda_{M}$ and $\\tilde{\\mathbf{P}}$ is a projector of rank $k$, the quantity $\\operatorname{tr}((\\mathbf{I}-\\tilde{\\mathbf{P}})\\boldsymbol{\\Lambda})$ equals the sum of those $\\lambda_{i}$ whose indices correspond to directions excluded by the projector. The minimal value over all rank-$k$ orthogonal projectors is achieved by choosing $\\tilde{\\mathbf{P}}$ to select the $k$ largest diagonal entries of $\\boldsymbol{\\Lambda}$, that is, to project onto the span of the top $k$ eigenvectors. This is a consequence of the rearrangement inequality or equivalently Ky Fan’s maximum principle, which states that for any symmetric matrix $\\mathbf{A}$ with eigenvalues ordered nonincreasingly, the maximum of $\\operatorname{tr}(\\mathbf{P}\\mathbf{A})$ over rank-$k$ orthogonal projectors $\\mathbf{P}$ is $\\sum_{i=1}^{k}\\lambda_{i}(\\mathbf{A})$. Applying this with $\\mathbf{A}=\\mathbf{C}$ gives\n$$\n\\min_{\\substack{\\mathbf{P}^{\\top}=\\mathbf{P}=\\mathbf{P}^{2}\\\\ \\operatorname{rank}(\\mathbf{P})=k}}\\operatorname{tr}\\big((\\mathbf{I}-\\mathbf{P})\\,\\mathbf{C}\\big)\n=\\operatorname{tr}(\\mathbf{C})-\\max_{\\mathbf{P}}\\operatorname{tr}(\\mathbf{P}\\mathbf{C})\n=\\Big(\\sum_{i=1}^{M}\\lambda_{i}\\Big)-\\Big(\\sum_{i=1}^{k}\\lambda_{i}\\Big)\n=\\sum_{i=k+1}^{M}\\lambda_{i}.\n$$\nTherefore, the minimal achievable expected squared reconstruction error among all rank-$k$ linear surrogates is\n$$\n\\mathbb{E}\\big[\\|\\boldsymbol{\\phi}-\\hat{\\boldsymbol{\\phi}}\\|_{2}^{2}\\big]_{\\min}=\\sum_{i=k+1}^{M}\\lambda_{i},\n$$\nand the Principal Component Analysis (PCA) surrogate that projects onto the span of the top $k$ eigenvectors attains this minimum. This quantity also serves as an error bound, since any other rank-$k$ linear reconstruction cannot have a smaller expected squared error.\n\nPhysical interpretation for electrolyte potential fields: The eigenvalues $\\{\\lambda_{i}\\}$ are the variances of the random field $\\boldsymbol{\\phi}$ along orthogonal spatial modes (eigenvectors) that capture coherent patterns of electrolyte potential fluctuation across operating conditions. The sum $\\sum_{i=k+1}^{M}\\lambda_{i}$ equals the total variance residing in the neglected modes. Thus, the bound quantifies the average unresolved squared electrolyte potential fluctuation that any rank-$k$ linear surrogate must leave out. In practical terms, it measures the mean-squared magnitude (in volt-squared) of electrolyte potential features—such as fine-scale ohmic drops and tortuosity-induced heterogeneities—not captured by the chosen $k$-dimensional representation.\n\nNow evaluate the bound for the given spectrum when using $k=2$. With\n$$\n\\lambda_{1}=3.2\\times 10^{-4},\\quad\n\\lambda_{2}=1.6\\times 10^{-4},\\quad\n\\lambda_{3}=8.0\\times 10^{-5},\\quad\n\\lambda_{4}=4.0\\times 10^{-5},\\quad\n\\lambda_{5}=2.0\\times 10^{-5},\n$$\nand all subsequent eigenvalues negligible, we have\n$$\n\\sum_{i=k+1}^{M}\\lambda_{i}=\\lambda_{3}+\\lambda_{4}+\\lambda_{5}\n=\\big(8.0\\times 10^{-5}\\big)+\\big(4.0\\times 10^{-5}\\big)+\\big(2.0\\times 10^{-5}\\big)\n=1.4\\times 10^{-4}.\n$$\nRounded to four significant figures and expressed in volt-squared, the minimal expected squared reconstruction error bound is $1.400\\times 10^{-4}$.",
            "answer": "$$\\boxed{1.400 \\times 10^{-4}}$$"
        },
        {
            "introduction": "A reliable surrogate model should not only provide accurate predictions but also quantify its own uncertainty. Gaussian Process (GP) regression is a powerful non-linear technique that naturally provides a full predictive distribution—a mean prediction and its associated variance. This exercise demonstrates how to leverage this uncertainty information, propagating it from the primary model outputs (like voltage and current) to a derived quantity of interest (power). Mastering this practice is key to building trust in surrogate models and making risk-informed decisions in battery design and control. ",
            "id": "3926913",
            "problem": "A lithium-ion cell is modeled with a multi-output Gaussian Process Regression (GPR) surrogate that predicts terminal voltage $V$ and discharge current $I$ at an operating condition $x^{\\star}$ defined by temperature, State of Charge (SOC), and current setpoint. By the definition of a Gaussian Process (GP), the finite-dimensional posterior predictive distribution at $x^{\\star}$ is multivariate normal for any set of outputs; in particular, the joint predictive distribution of $(V,I)$ at $x^{\\star}$ is Gaussian with some mean vector and covariance matrix determined by the kernel and the training data. Confidence Interval (CI) construction for a Gaussian random variable uses standard normal quantiles.\n\nAt the specific operating condition $x^{\\star}$, the trained surrogate yields the following posterior predictive summary for $(V,I)$:\n- Mean voltage $\\mu_{V} = 3.7$ and mean current $\\mu_{I} = 2.0$,\n- Variance of voltage $\\sigma_{V}^{2} = (0.05)^{2}$,\n- Variance of current $\\sigma_{I}^{2} = (0.2)^{2}$,\n- Covariance $\\sigma_{VI} = -0.006$.\n\nAssume the joint predictive distribution of $(V,I)$ is Gaussian with these parameters, and that model and observation noise have been appropriately integrated into $\\sigma_{V}^{2}$, $\\sigma_{I}^{2}$, and $\\sigma_{VI}$ by the GPR posterior. The derived quantity of interest is power $P = V I$. Starting from the Gaussian Process definition and the properties of multivariate normal moments, derive the approximate two-sided $95\\%$ CI for $P$ by:\n1. Deriving the first two moments of $P$ from the joint Gaussian predictive distribution of $(V,I)$,\n2. Approximating the distribution of $P$ by a normal distribution with these moments,\n3. Using the standard normal quantile $z_{0.975} \\approx 1.96$ to construct a symmetric two-sided confidence interval.\n\nCompute the two-sided $95\\%$ CI half-width for $P$ under this approximation and express your final numerical answer in watts (W). Round your answer to four significant figures.",
            "solution": "The joint posterior predictive distribution of terminal voltage $V$ and discharge current $I$ at operating condition $x^{\\star}$ is a bivariate Gaussian, $\\begin{pmatrix} V \\\\ I \\end{pmatrix} \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$, with mean vector $\\boldsymbol{\\mu}$ and covariance matrix $\\boldsymbol{\\Sigma}$:\n$$\n\\boldsymbol{\\mu} = \\begin{pmatrix} \\mu_V \\\\ \\mu_I \\end{pmatrix} = \\begin{pmatrix} 3.7 \\\\ 2.0 \\end{pmatrix}, \\quad\n\\boldsymbol{\\Sigma} = \\begin{pmatrix} \\sigma_V^2 & \\sigma_{VI} \\\\ \\sigma_{VI} & \\sigma_I^2 \\end{pmatrix} = \\begin{pmatrix} 0.05^2 & -0.006 \\\\ -0.006 & 0.2^2 \\end{pmatrix} = \\begin{pmatrix} 0.0025 & -0.006 \\\\ -0.006 & 0.04 \\end{pmatrix}.\n$$\nThe derived quantity of interest is the power, $P = VI$. We find an approximate $95\\%$ confidence interval (CI) for $P$ by first deriving its exact mean and variance, and then approximating the distribution of $P$ as normal.\n\n**1. Derivation of the Moments of Power ($P$)**\n\n**Mean of $P$:** The expected value of the product of two random variables is the product of their means plus their covariance.\n$$\n\\mu_P = E[P] = E[VI] = E[V]E[I] + \\text{Cov}(V, I) = \\mu_V \\mu_I + \\sigma_{VI}\n$$\nSubstituting the given values:\n$$\n\\mu_P = (3.7)(2.0) + (-0.006) = 7.4 - 0.006 = 7.394 \\text{ W}.\n$$\n\n**Variance of $P$:** The variance of the product of two correlated normal random variables is given by the formula:\n$$\n\\sigma_P^2 = \\text{Var}(VI) = \\mu_V^2 \\sigma_I^2 + \\mu_I^2 \\sigma_V^2 + 2\\mu_V \\mu_I \\sigma_{VI} + \\sigma_V^2 \\sigma_I^2 + \\sigma_{VI}^2.\n$$\nSubstituting the numerical values: $\\mu_V = 3.7$, $\\mu_I = 2.0$, $\\sigma_V^2 = 0.0025$, $\\sigma_I^2 = 0.04$, and $\\sigma_{VI} = -0.006$.\n$$\n\\sigma_P^2 = (3.7)^2(0.04) + (2.0)^2(0.0025) + 2(3.7)(2.0)(-0.006) + (0.0025)(0.04) + (-0.006)^2\n$$\n$$\n\\sigma_P^2 = (13.69)(0.04) + (4)(0.0025) - 0.0888 + 0.0001 + 0.000036\n$$\n$$\n\\sigma_P^2 = 0.5476 + 0.01 - 0.0888 + 0.0001 + 0.000036\n$$\n$$\n\\sigma_P^2 = 0.468936.\n$$\n\n**2. Normal Approximation for the Distribution of $P$**\n\nThe distribution of the product of two Gaussian random variables is not strictly Gaussian. However, for constructing an approximate confidence interval, we approximate the distribution of $P$ by a normal distribution with the moments derived above:\n$$\nP \\approx \\mathcal{N}(\\mu_P, \\sigma_P^2) \\quad \\text{where} \\quad \\mu_P = 7.394, \\quad \\sigma_P^2 = 0.468936.\n$$\n\n**3. Construction of the $95\\%$ Confidence Interval**\n\nA two-sided $95\\%$ confidence interval for a normally distributed variable is $\\mu_P \\pm z_{0.975} \\sigma_P$, where $z_{0.975} \\approx 1.96$ is the standard normal quantile. The half-width (HW) of this interval is:\n$$\nHW = z_{0.975} \\sigma_P = 1.96 \\times \\sqrt{0.468936}\n$$\n$$\nHW \\approx 1.96 \\times 0.684789 \\approx 1.3421864 \\text{ W}.\n$$\nRounding the numerical answer to four significant figures gives:\n$$\nHW \\approx 1.342 \\text{ W}.\n$$",
            "answer": "$$\n\\boxed{1.342}\n$$"
        },
        {
            "introduction": "The most advanced surrogates can integrate physical laws directly into their training process, ensuring their predictions are not just data-consistent but also physically plausible. This exercise focuses on Physics-Informed Neural Networks (PINNs) and the critical task of enforcing a global conservation law—the conservation of total lithium in a closed cell. You will analyze how this physical constraint is added to the training objective and, most importantly, quantify how it improves the model's generalization performance on a related downstream prediction task. This practice reveals the profound impact of baking physics into machine learning for scientific simulation. ",
            "id": "3926983",
            "problem": "A Physics-Informed Neural Network (PINN) is trained as a surrogate for a porous-electrode lithium-ion cell, with two network outputs: the solid-phase lithium concentration $c_{s}(\\mathbf{x}, t; \\boldsymbol{\\theta}_{s})$ defined on the solid domain $\\Omega_{s}$ (volume $V_{s}$), and the electrolyte-phase lithium concentration $c_{e}(\\mathbf{y}, t; \\boldsymbol{\\theta}_{e})$ defined on the electrolyte domain $\\Omega_{e}$ (volume $V_{e}$). Assume a closed cell with no side reactions and negligible loss of cyclable lithium. By conservation of mass, the total lithium inventory is conserved for all times $t$:\n$$\n\\int_{\\Omega_{s}} c_{s}(\\mathbf{x}, t)\\, dV + \\int_{\\Omega_{e}} c_{e}(\\mathbf{y}, t)\\, dV = M_{\\text{tot}} \\quad \\text{for all } t,\n$$\nwhere $M_{\\text{tot}}$ is a constant. The PINN is trained with a loss that contains data misfit and Partial Differential Equation (PDE) residual terms plus boundary and interface terms. You are asked to enforce the global integral conservation law in the PINN and to quantify its effect on generalization of a downstream voltage predictor.\n\nConsider a downstream linear voltage surrogate defined by\n$$\nV(t) = \\alpha\\, I_{s}(t) + \\beta\\, I_{e}(t) + \\gamma,\n$$\nwhere $I_{s}(t) = \\int_{\\Omega_{s}} c_{s}(\\mathbf{x}, t)\\, dV$ and $I_{e}(t) = \\int_{\\Omega_{e}} c_{e}(\\mathbf{y}, t)\\, dV$, with constants $\\alpha$, $\\beta$, and $\\gamma$. Suppose after training, the PINN’s integral conservation residual $r(t) \\equiv I_{s}(t) + I_{e}(t) - M_{\\text{tot}}$ satisfies $\\sup_{t} |r(t)| \\le \\varepsilon$ for some small $\\varepsilon > 0$, and the PINN’s integral prediction errors $e_{s}(t) \\equiv \\widehat{I}_{s}(t) - I_{s}(t)$ and $e_{e}(t) \\equiv \\widehat{I}_{e}(t) - I_{e}(t)$ on unseen current profiles are zero-mean, with variances $\\sigma_{s}^{2}$ and $\\sigma_{e}^{2}$, respectively. Assume independence of $e_{s}(t)$ and $e_{e}(t)$ in the unconstrained case, and that the conservation enforcement couples these errors so that $e_{e}(t) \\approx -e_{s}(t) + \\delta(t)$ with $|\\delta(t)| \\le \\varepsilon$.\n\nWhich option correctly specifies a global PINN constraint that enforces integral lithium conservation and the resulting test-time upper bound on the expected squared voltage error for the downstream predictor under the stated assumptions?\n\nA. Add a soft global constraint term to the PINN loss\n$$\n\\mathcal{L}_{\\text{cons}} = \\frac{1}{T}\\int_{0}^{T} \\left(I_{s}(t) + I_{e}(t) - M_{\\text{tot}}\\right)^{2}\\, dt,\n$$\nwhere $I_{s}(t) = \\int_{\\Omega_{s}} c_{s}(\\mathbf{x}, t)\\, dV$ and $I_{e}(t) = \\int_{\\Omega_{e}} c_{e}(\\mathbf{y}, t)\\, dV$, whose functional gradient with respect to $c_{s}$ and $c_{e}$ couples the two outputs through the scalar residual $I_{s}(t) + I_{e}(t) - M_{\\text{tot}}$. Under the given assumptions, the test-time expected squared voltage error satisfies\n$$\n\\mathbb{E}\\big[(\\Delta V(t))^{2}\\big] \\le (\\alpha - \\beta)^{2}\\,\\sigma_{s}^{2} + \\beta^{2}\\,\\varepsilon^{2},\n$$\nwhile in the unconstrained case\n$$\n\\mathbb{E}\\big[(\\Delta V(t))^{2}\\big] = \\alpha^{2}\\,\\sigma_{s}^{2} + \\beta^{2}\\,\\sigma_{e}^{2}.\n$$\n\nB. Add a local pointwise constraint\n$$\n\\mathcal{L}_{\\text{local}} = \\int_{\\Omega_{s}\\cup \\Omega_{e}} \\left(c_{s}(\\mathbf{z}, t) + c_{e}(\\mathbf{z}, t) - \\frac{M_{\\text{tot}}}{V_{s}+V_{e}}\\right)^{2}\\, dV,\n$$\nand assume the downstream expected squared voltage error bound becomes\n$$\n\\mathbb{E}\\big[(\\Delta V(t))^{2}\\big] \\le (\\alpha + \\beta)^{2}\\,\\sigma_{s}^{2}.\n$$\n\nC. Introduce a hard Lagrange multiplier $\\lambda(t)$ to enforce conservation exactly via\n$$\n\\mathcal{L}_{\\text{aug}} = \\mathcal{L}_{\\text{PDE}} + \\mathcal{L}_{\\text{BC}} + \\mathcal{L}_{\\text{data}} + \\int_{0}^{T}\\lambda(t)\\,\\big(I_{s}(t) + I_{e}(t) - M_{\\text{tot}}\\big)\\, dt,\n$$\nand claim the downstream expected squared voltage error is unchanged by the constraint:\n$$\n\\mathbb{E}\\big[(\\Delta V(t))^{2}\\big] = \\alpha^{2}\\,\\sigma_{s}^{2} + \\beta^{2}\\,\\sigma_{e}^{2}.\n$$\n\nD. Penalize conservation at endpoints only\n$$\n\\mathcal{L}_{\\text{end}} = \\big(I_{s}(0) + I_{e}(0) - M_{\\text{tot}}\\big)^{2} + \\big(I_{s}(T) + I_{e}(T) - M_{\\text{tot}}\\big)^{2},\n$$\nand conclude that the constraint yields no improvement in the downstream expected squared voltage error:\n$$\n\\mathbb{E}\\big[(\\Delta V(t))^{2}\\big] = \\alpha^{2}\\,\\sigma_{s}^{2} + \\beta^{2}\\,\\sigma_{e}^{2}.\n$$",
            "solution": "The correct choice is A. Here is the step-by-step derivation.\n\n**1. Unconstrained Voltage Error**\n\nThe error in the downstream voltage prediction is $\\Delta V(t) = \\widehat{V}(t) - V(t)$. Substituting the definitions of the predictor and the errors $e_s(t) = \\widehat{I}_s(t) - I_s(t)$ and $e_e(t) = \\widehat{I}_e(t) - I_e(t)$, we get:\n$$ \\Delta V(t) = (\\alpha \\widehat{I}_s(t) + \\beta \\widehat{I}_e(t) + \\gamma) - (\\alpha I_s(t) + \\beta I_e(t) + \\gamma) = \\alpha e_s(t) + \\beta e_e(t) $$\nIn the unconstrained case, the errors $e_s(t)$ and $e_e(t)$ are assumed to be independent and zero-mean. The expected squared error is:\n$$ \\mathbb{E}[(\\Delta V(t))^2] = \\mathbb{E}[(\\alpha e_s(t) + \\beta e_e(t))^2] = \\alpha^2 \\mathbb{E}[e_s(t)^2] + \\beta^2 \\mathbb{E}[e_e(t)^2] + 2\\alpha\\beta\\mathbb{E}[e_s(t)e_e(t)] $$\nDue to independence, $\\mathbb{E}[e_s(t)e_e(t)] = \\mathbb{E}[e_s(t)]\\mathbb{E}[e_e(t)] = 0$. Thus,\n$$ \\mathbb{E}[(\\Delta V(t))^2] = \\alpha^2\\sigma_s^2 + \\beta^2\\sigma_e^2 $$\nThis matches the expression for the unconstrained case in option A.\n\n**2. Constrained Voltage Error**\n\nThe conservation law for the true integrals is $I_s(t) + I_e(t) = M_{\\text{tot}}$. The PINN's predictions have a residual $\\widehat{r}(t) = \\widehat{I}_s(t) + \\widehat{I}_e(t) - M_{\\text{tot}}$. The sum of the prediction errors is:\n$$ e_s(t) + e_e(t) = (\\widehat{I}_s(t) - I_s(t)) + (\\widehat{I}_e(t) - I_e(t)) = (\\widehat{I}_s(t) + \\widehat{I}_e(t)) - (I_s(t) + I_e(t)) = \\widehat{r}(t) $$\nThe problem states this relationship as $e_e(t) \\approx -e_s(t) + \\delta(t)$ with $|\\delta(t)| \\le \\varepsilon$, confirming that $\\delta(t)$ is the network's conservation residual. Substituting $e_e(t) = -e_s(t) + \\delta(t)$ into the voltage error equation:\n$$ \\Delta V(t) = \\alpha e_s(t) + \\beta(-e_s(t) + \\delta(t)) = (\\alpha - \\beta)e_s(t) + \\beta\\delta(t) $$\nThe expected squared error is:\n$$ \\mathbb{E}[(\\Delta V(t))^2] = \\mathbb{E}[((\\alpha - \\beta)e_s(t) + \\beta\\delta(t))^2] = (\\alpha - \\beta)^2\\mathbb{E}[e_s(t)^2] + \\beta^2\\mathbb{E}[\\delta(t)^2] + 2\\beta(\\alpha - \\beta)\\mathbb{E}[e_s(t)\\delta(t)] $$\nAssuming the prediction error $e_s(t)$ is uncorrelated with the model's structural conservation residual $\\delta(t)$, the cross-term $\\mathbb{E}[e_s(t)\\delta(t)]$ is zero. We are given the bound $|\\delta(t)| \\le \\varepsilon$, which implies $\\mathbb{E}[\\delta(t)^2] \\le \\varepsilon^2$. This leads to the upper bound on the expected squared error:\n$$ \\mathbb{E}[(\\Delta V(t))^2] \\le (\\alpha - \\beta)^2\\sigma_s^2 + \\beta^2\\varepsilon^2 $$\nThis matches the error bound for the constrained case in option A.\n\n**3. Constraint Formulation**\n\nThe constraint proposed in option A, $\\mathcal{L}_{\\text{cons}} = \\frac{1}{T}\\int_{0}^{T} (I_s(t) + I_e(t) - M_{\\text{tot}})^2\\, dt$, is a standard and effective method for imposing a global, time-dependent conservation law as a soft penalty in the PINN loss function. This term penalizes deviations from the conservation law across the entire time domain, coupling the network outputs and encouraging the learned solution to respect the physical principle. The other options are incorrect: B enforces a physically nonsensical local constraint, C makes a false claim about the error being unchanged, and D enforces the constraint only at the endpoints, which is insufficient.",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}