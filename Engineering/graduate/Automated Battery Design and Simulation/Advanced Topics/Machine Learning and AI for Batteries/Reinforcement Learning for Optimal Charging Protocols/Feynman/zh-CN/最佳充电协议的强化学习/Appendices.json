{
    "hands_on_practices": [
        {
            "introduction": "奖励函数塑造是定义强化学习问题的关键步骤，一个设计不当的奖励函数可能导致代理出现意想不到的次优行为。本练习旨在探讨一个看似直观的奖励函数——即通过对电流平方($I^2$)进行二次惩罚来模拟和减少焦耳热——如何无意中惩罚了脉冲充电等有效策略。通过这个清晰的简化反例()，您将对奖励工程的微妙之处获得更深的直觉，并学会批判性地评估奖励函数可能引入的潜在偏差。",
            "id": "3946202",
            "problem": "一个电池充电智能体在用于自动化电池设计和模拟的强化学习（RL）框架内被建模为马尔可夫决策过程（MDP）。在每个离散时间步 $t \\in \\{0,1\\}$，该智能体选择一个归一化充电电流 $i_t \\in [0,1]$，其中物理电流为 $I_t = i_t I_{\\max}$。归一化荷电状态 $s_t \\in [0,1]$ 根据单位效率的库仑电荷转移定义演化，因此 $s_{t+1} = s_t + i_t$，传递的电荷增量为 $s_{t+1} - s_t = i_t$。电池中的焦耳热与 $I_t^2$成正比，但一个专用的热管理控制器（外在于RL智能体）会施加冷却，在每个时间步完美补偿焦耳热，从而保持一个恒定的归一化温度 $\\theta_t$；也就是说，对于所有的 $t$，无论 $i_t$ 的值如何，都有 $\\theta_{t+1} = \\theta_t$。\n\n智能体在每个时间步的塑形奖励定义为\n$$\nr_t = \\beta \\, (s_{t+1} - s_t) - \\alpha \\, i_t^{2},\n$$\n其中 $\\beta = 1$ 且 $\\alpha = 1$。回合长度为 $N = 2$（时间步 $t=0,1$），回报为 $G = \\sum_{t=0}^{N-1} r_t$。\n\n为了探究直接惩罚 $i_t^2$ 的朴素奖励塑形是否会在热安全得到完全管理的情况下，仍然使策略偏向于避免脉冲充电，请比较以下两条轨迹。这两条轨迹由于完美的热管理，传递了相同的总电荷并保持了相同的温度：\n\n- 恒流轨迹：$i_0 = \\frac{1}{2}$ 且 $i_1 = \\frac{1}{2}$。\n- 脉冲充电轨迹：$i_0 = 1$ 且 $i_1 = 0$。\n\n计算累积塑形回报之差的确切值，\n$$\nG_{\\mathrm{pulse}} - G_{\\mathrm{const}},\n$$\n结果为一个实数。请以无单位的精确值形式表达您的最终答案（回报是无量纲的）。",
            "solution": "问题要求计算两种不同充电轨迹之间的累积塑形回报之差 $G_{\\mathrm{pulse}} - G_{\\mathrm{const}}$。分析从奖励函数和总回报的定义开始。\n\n在每个时间步 $t$ 的奖励由以下表达式给出：\n$$\nr_t = \\beta \\, (s_{t+1} - s_t) - \\alpha \\, i_t^{2}\n$$\n归一化荷电状态 $s_t$ 的状态演化定义为：\n$$\ns_{t+1} = s_t + i_t\n$$\n这意味着在一个时间步内的电荷增量等于该步施加的归一化电流：\n$$\ns_{t+1} - s_t = i_t\n$$\n将此关系代入奖励函数，简化了 $r_t$ 的表达式。它变得仅依赖于该步所采取的动作 $i_t$：\n$$\nr_t = \\beta \\, i_t - \\alpha \\, i_t^{2}\n$$\n问题指定了参数 $\\beta$ 和 $\\alpha$ 的值为：\n$$\n\\beta = 1\n$$\n$$\n\\alpha = 1\n$$\n因此，本问题的特定奖励函数为：\n$$\nr_t = i_t - i_t^{2}\n$$\n总回报 $G$ 是整个回合中奖励的总和，回合长度为 $N=2$（时间步 $t=0$ 和 $t=1$）：\n$$\nG = \\sum_{t=0}^{N-1} r_t = r_0 + r_1\n$$\n代入 $r_t$ 的表达式，总回报为：\n$$\nG = (i_0 - i_0^{2}) + (i_1 - i_1^{2})\n$$\n我们现在计算两条指定轨迹各自的总回报。\n\n首先，考虑恒流轨迹，其中 $i_0 = \\frac{1}{2}$ 且 $i_1 = \\frac{1}{2}$。总回报 $G_{\\mathrm{const}}$ 计算如下：\n$$\nG_{\\mathrm{const}} = \\left(\\frac{1}{2} - \\left(\\frac{1}{2}\\right)^2\\right) + \\left(\\frac{1}{2} - \\left(\\frac{1}{2}\\right)^2\\right)\n$$\n$$\nG_{\\mathrm{const}} = \\left(\\frac{1}{2} - \\frac{1}{4}\\right) + \\left(\\frac{1}{2} - \\frac{1}{4}\\right)\n$$\n$$\nG_{\\mathrm{const}} = \\frac{1}{4} + \\frac{1}{4} = \\frac{1}{2}\n$$\n\n接下来，考虑脉冲充电轨迹，其中 $i_0 = 1$ 且 $i_1 = 0$。总回报 $G_{\\mathrm{pulse}}$ 计算如下：\n$$\nG_{\\mathrm{pulse}} = (1 - 1^2) + (0 - 0^2)\n$$\n$$\nG_{\\mathrm{pulse}} = (1 - 1) + (0 - 0)\n$$\n$$\nG_{\\mathrm{pulse}} = 0 + 0 = 0\n$$\n\n最后，我们计算脉冲充电轨迹和恒流轨迹的回报之差：\n$$\nG_{\\mathrm{pulse}} - G_{\\mathrm{const}} = 0 - \\frac{1}{2}\n$$\n$$\nG_{\\mathrm{pulse}} - G_{\\mathrm{const}} = -\\frac{1}{2}\n$$\n这个结果表明，对于给定的奖励结构，恒流轨迹产生的累积回报高于脉冲充电轨迹，尽管两者传递了相同的总电荷（$i_0 + i_1 = 1$）并且其热效应得到了完美管理。惩罚项 $-i_t^2$ 是非线性的，它对更高的瞬时电流惩罚得更严重，从而导致了这种偏好。",
            "answer": "$$\n\\boxed{-\\frac{1}{2}}\n$$"
        },
        {
            "introduction": "在现实世界的应用中，强化学习代理很少能获得环境的完整和完美状态，这种部分可观测性是部署强化学习系统时面临的核心挑战。本练习模拟了一个真实场景：由于传感器饱和，电池的真实充电状态($x_t$)在接近满电时变得无法观测，我们将此问题构建为一个部分可观测马尔可夫决策过程(POMDP)。通过实现并比较一个忽略这种不确定性的朴素代理和一个主动管理不确定性的信念感知代理()，您将理解可能发生的严重性能损失，并学习设计能够在部分可观测性下有效推理和行动的代理的基本原理。",
            "id": "3946159",
            "problem": "考虑一个针对充电末期锂离子电池的序贯最优充电控制问题，该问题被建模为部分可观测马尔可夫决策过程（POMDP）。其基础状态变量为离散时间 $t \\in \\{0,1,\\ldots,T-1\\}$ 时的荷电状态 $x_t \\in [0,1]$。智能体在每一步选择一个充电电流 $I_t \\in [0,I_{\\max}]$。物理演化和可观测性由以下源自经过充分测试的电池物理学和电路关系的要素定义：\n\n- 电荷平衡：$x_{t+1} = \\min\\left(1, x_t + \\eta \\dfrac{I_t \\Delta t}{C}\\right)$，其中 $C$ 是以库仑为单位的容量，$\\eta \\in (0,1]$ 是库仑效率，$\\Delta t$ 是以秒为单位的采样间隔。\n- 开路电压：$V_{\\mathrm{oc}}(x) = V_{\\min} + k_{\\mathrm{voc}} x$，这是一个线性化近似，捕捉了在高荷电状态区间内随荷电状态单调增加的特性。\n- 充电末期内阻增长：$R(x) = R_0 + \\dfrac{k_R}{1 - x + \\varepsilon}$，其中 $R_0 > 0$ 是基线欧姆电阻，$k_R > 0$ 捕捉了当 $x \\to 1$ 时电阻的增长，而 $\\varepsilon > 0$ 对表达式进行正则化以保持其有限，并模拟了有限的陡峭度。\n- 电流 $I_t$ 下的端电压：$V_t = V_{\\mathrm{oc}}(x_t) + R(x_t) I_t$，根据欧姆定律，将串联的电压源与内阻相结合。\n- 最大电压约束：$V_t \\le V_{\\max}$；违规代表充电末期的过压/过流应力。\n\n部分可观测性源于荷电状态估计中的传感器饱和。每一步的可观测量为\n$$\ny_t = \\min(x_t,\\, 1 - \\delta),\n$$\n其中 $\\delta \\in [0,1)$ 表示饱和偏移，使得即使真实的 $x_t$ 更大，测量值也不能超过 $1 - \\delta$。智能体在时间 $t$ 的奖励定义为\n$$\nr_t(I_t, x_t) = I_t \\Delta t - \\lambda \\cdot \\mathbb{1}\\!\\left\\{V_{\\mathrm{oc}}(x_t) + R(x_t) I_t > V_{\\max}\\right\\},\n$$\n单位为库仑 ($\\mathrm{A}\\cdot \\mathrm{s}$)。目标是最大化累积奖励 $\\sum_{t=0}^{T-1} r_t(I_t, x_t)$。\n\n令一个忽略部分可观测性的智能体将 $y_t$ 视为真实状态，并短视地选择 $I_t$ 以在满足以 $y_t$ 评估的最大电压约束的条件下最大化即时奖励。令一个信念感知智能体使用观测模型和电荷平衡来传播关于 $x_t$ 的信念，并在每一步选择 $I_t$ 以在与饱和观测和动力学一致的 $x_t$ 容许区间上的均匀信念下最大化期望即时奖励。两个智能体都从指定的初始状态 $x_0$ 开始，运行 $T$ 步。\n\n您的任务是构建一个反例，证明忽略部分可观测性会导致充电末期出现系统性过流，并量化性能损失，该损失定义为信念感知智能体与忽略部分可观测性的智能体之间的累积奖励之差（单位为 $\\mathrm{A}\\cdot \\mathrm{s}$）。请使用以下科学上真实的参数化。在整个过程中，将最终性能损失以 $\\mathrm{A}\\cdot \\mathrm{s}$ 为单位表示为十进制浮点数。不涉及角度。不得使用百分比。\n\n使用的基本依据：\n- 负载下端电压的欧姆定律。\n- 高荷电状态区间内开路电压随荷电状态单调增加。\n- 荷电状态动力学的电荷守恒。\n- 由饱和观测和单调动力学引起的隐藏状态上的均匀先验。\n\n按照上述描述实现两个智能体，并在指定的时域内进行仿真，根据物理模型更新 $x_t$。对于信念感知智能体，维持一个保守的信念区间 $[x^{\\min}_t, x^{\\max}_t]$，其中 $x^{\\max}_t = 1$ 且 $x^{\\min}_{t+1} = \\min\\!\\left(1, \\max\\!\\left(y_t, x^{\\min}_t + \\eta \\dfrac{I_t \\Delta t}{C}\\right)\\right)$。在每一步，通过在当前信念区间的均匀网格上积分来计算期望即时奖励，并选择 $I_t$ 以最大化此期望即时奖励。对于忽略部分可观测性的智能体，在每一步将 $y_t$ 视为真实状态，并选择 $I_t$ 以在满足于 $y_t$ 处评估的电压约束的条件下最大化即时奖励。\n\n测试套件。使用 $T = 5$, $\\eta = 1$ 以及以下参数集：\n\n- 案例1（带饱和的理想路径）：$x_0 = 0.98$，$\\delta = 0.04$，$V_{\\min} = 3.0\\,\\mathrm{V}$，$k_{\\mathrm{voc}} = 1.2\\,\\mathrm{V}$，$V_{\\max} = 4.2\\,\\mathrm{V}$，$R_0 = 0.05\\,\\Omega$，$k_R = 0.04\\,\\Omega$，$\\varepsilon = 0.01$，$C = 10800\\,\\mathrm{C}$，$\\Delta t = 1\\,\\mathrm{s}$，$I_{\\max} = 2.0\\,\\mathrm{A}$，$\\lambda = 0.08\\,\\mathrm{C}$。\n- 案例2（边界，完全可观测）：$x_0 = 0.96$，$\\delta = 0.0$，其余参数与案例1相同。\n- 案例3（边缘，更陡峭的电阻增长）：$x_0 = 0.99$，$\\delta = 0.02$，$V_{\\min} = 3.0\\,\\mathrm{V}$，$k_{\\mathrm{voc}} = 1.2\\,\\mathrm{V}$，$V_{\\max} = 4.2\\,\\mathrm{V}$，$R_0 = 0.05\\,\\Omega$，$k_R = 0.08\\,\\Omega$，$\\varepsilon = 0.01$，$C = 10800\\,\\mathrm{C}$，$\\Delta t = 1\\,\\mathrm{s}$，$I_{\\max} = 2.0\\,\\mathrm{A}$，$\\lambda = 0.12\\,\\mathrm{C}$。\n\n您的程序应生成单行输出，其中包含三个案例的性能损失，格式为方括号括起来的逗号分隔列表，例如，“[loss_case1,loss_case2,loss_case3]”。每个损失值必须是单位为 $\\mathrm{A}\\cdot \\mathrm{s}$ 的浮点数。",
            "solution": "用户希望解决一个最优电池充电问题，该问题被构建为部分可观测马尔可夫决策过程（POMDP）。我必须首先验证问题陈述。\n\n### 第1步：提取给定信息\n\n- **状态变量**：$x_t \\in [0,1]$，在时间 $t \\in \\{0, 1, \\ldots, T-1\\}$ 时的荷电状态。\n- **动作变量**：$I_t \\in [0, I_{\\max}]$，充电电流。\n- **状态动力学（电荷平衡）**：$x_{t+1} = \\min\\left(1, x_t + \\eta \\dfrac{I_t \\Delta t}{C}\\right)$。\n- **动力学参数**：$C$（容量，C），$\\eta$（库仑效率），$\\Delta t$（采样间隔，s）。\n- **开路电压**：$V_{\\mathrm{oc}}(x) = V_{\\min} + k_{\\mathrm{voc}} x$。\n- **内阻**：$R(x) = R_0 + \\dfrac{k_R}{1 - x + \\varepsilon}$。\n- **电化学参数**：$V_{\\min}$，$k_{\\mathrm{voc}}$，$R_0$，$k_R$，$\\varepsilon$。\n- **端电压**：$V_t = V_{\\mathrm{oc}}(x_t) + R(x_t) I_t$。\n- **约束**：$V_t \\le V_{\\max}$。\n- **观测模型（部分可观测性）**：$y_t = \\min(x_t, 1 - \\delta)$，其中 $\\delta \\in [0,1)$ 是饱和偏移。\n- **奖励函数**：$r_t(I_t, x_t) = I_t \\Delta t - \\lambda \\cdot \\mathbb{1}\\!\\left\\{V_{\\mathrm{oc}}(x_t) + R(x_t) I_t > V_{\\max}\\right\\}$，单位为库仑（$A \\cdot s$）。\n- **目标**：最大化累积奖励 $\\sum_{t=0}^{T-1} r_t(I_t, x_t)$。\n- **智能体1（忽略部分可观测性）**：将 $y_t$ 视为真实状态。短视地选择 $I_t$ 以在以 $y_t$ 评估的最大电压约束下最大化即时奖励。这意味着在 $V_{\\mathrm{oc}}(y_t) + R(y_t) I_t \\le V_{\\max}$ 的约束下最大化 $I_t \\Delta t$。\n- **智能体2（信念感知）**：传播对 $x_t$ 的信念。选择 $I_t$ 以在容许区间上的均匀信念下最大化期望即时奖励 $\\mathbb{E}[r_t(I_t, x_t)]$。\n- **智能体2的信念传播规则**：智能体维持一个信念区间 $[x^{\\min}_t, x^{\\max}_t]$。规则指定为 $x^{\\max}_t = 1$ 和 $x^{\\min}_{t+1} = \\min\\!\\left(1, \\max\\!\\left(y_t, x^{\\min}_t + \\eta \\dfrac{I_t \\Delta t}{C}\\right)\\right)$。智能体从指定的初始状态 $x_0$ 开始。这意味着 $x^{\\min}_0 = x_0$。\n- **任务**：计算性能损失，定义为（信念感知智能体的累积奖励）-（朴素智能体的累积奖励）。\n- **测试套件参数**：\n    - 通用：$T = 5$，$\\eta = 1$，$\\Delta t = 1\\,\\mathrm{s}$，$C = 10800\\,\\mathrm{C}$，$I_{\\max} = 2.0\\,\\mathrm{A}$，$V_{\\min} = 3.0\\,\\mathrm{V}$，$k_{\\mathrm{voc}} = 1.2\\,\\mathrm{V}$，$V_{\\max} = 4.2\\,\\mathrm{V}$，$\\varepsilon = 0.01$。\n    - 案例1：$x_0 = 0.98$，$\\delta = 0.04$，$R_0 = 0.05\\,\\Omega$，$k_R = 0.04\\,\\Omega$，$\\lambda = 0.08\\,\\mathrm{C}$。\n    - 案例2：$x_0 = 0.96$，$\\delta = 0.0$，$R_0 = 0.05\\,\\Omega$，$k_R = 0.04\\,\\Omega$，$\\lambda = 0.08\\,\\mathrm{C}$。\n    - 案例3：$x_0 = 0.99$, $\\delta = 0.02$, $R_0 = 0.05\\,\\Omega$, $k_R = 0.08\\,\\Omega$, $\\lambda = 0.12\\,\\mathrm{C}$。\n\n### 第2步：使用提取的给定信息进行验证\n\n- **科学依据**：该问题使用了锂离子电池的标准等效电路模型，这是一种科学有效且广泛使用的简化方法。电荷平衡、开路电压（针对小范围进行线性化）、内阻（捕捉充电末期的非线性效应）和端电压的方程都基于电化学和电路理论的基本原理。POMDP的概念是用于不确定性下决策的标准框架。该模型是可靠的。\n- **良构性**：问题是良构的。动力学、观测模型和奖励函数都有明确定义。两种智能体的策略都描述得足够详细，可以实现。为每个测试案例提供了初始状态和所有必要参数。这确保了可以为每个智能体确定唯一且有意义的仿真轨迹和由此产生的累积奖励。\n- **目标**：问题以精确的数学和算法术语陈述，没有主观或模糊的语言。\n- **完整性和一致性**：问题是自洽的。所有常数和初始条件都已提供。问题陈述中没有明显的矛盾。单位是一致的（$I \\Delta t$ 的单位是 $A \\cdot s$ 或库仑，$\\lambda$ 也以库仑为单位，使得奖励函数在量纲上一致）。\n- **真实性**：参数值据称是真实的，并与锂离子电池的典型值一致（例如，$C=10800 \\mathrm{C} = 3 \\mathrm{Ah}$，电压在 $3-4.2V$ 范围内，电流低于1C倍率）。\n\n### 第3步：结论和行动\n\n问题是**有效的**。它科学可靠、良构、客观且完整。我将继续提供完整的解决方案。\n\n### 解决方案\n解决方案要求在 $T$ 个时间步长上模拟两种不同的充电智能体，并比较它们的性能。问题的核心在于两种智能体处理真实荷电状态 $x_t$ 不确定性的方式不同。\n\n**物理模型实现**\n电池的物理行为由一组给定方程控制。我们将把这些实现为辅助函数。\n-   $V_{\\mathrm{oc}}(x_t) = V_{\\min} + k_{\\mathrm{voc}} x_t$：开路电压，作为荷电状态的线性函数。\n-   $R(x_t) = R_0 + k_R / (1 - x_t + \\varepsilon)$：内阻，当 $x_t \\to 1$ 时急剧增加。\n-   $V_t(x_t, I_t) = V_{\\mathrm{oc}}(x_t) + R(x_t) I_t$：端电压。\n-   $x_{t+1}(x_t, I_t) = \\min(1, x_t + \\eta I_t \\Delta t / C)$：荷电状态更新。\n-   $y_t(x_t) = \\min(x_t, 1 - \\delta)$：状态的饱和观测值。\n-   $r_t(x_t, I_t) = I_t \\Delta t - \\lambda \\cdot \\mathbb{1}\\{V_t(x_t, I_t) > V_{\\max}\\}$：奖励函数，激励充电（$I_t > 0$）但惩罚电压违规。\n\n**智能体1：朴素智能体**\n该智能体忽略部分可观测性。在每个时间步 $t$，它接收观测值 $y_t$ 并将其视为真实状态。其策略是选择一个尽可能大的电流 $I_t$，同时不违反*在所相信的状态 $y_t$ 下*的电压约束。它旨在满足 $V_{\\mathrm{oc}}(y_t) + R(y_t) I_t \\le V_{\\max}$。求解 $I_t$，我们得到 $I_t \\le (V_{\\max} - V_{\\mathrm{oc}}(y_t)) / R(y_t)$。因此，智能体选择的电流为：\n$$\nI_t^{\\text{naive}} = \\max\\left(0, \\min\\left(I_{\\max}, \\frac{V_{\\max} - V_{\\mathrm{oc}}(y_t)}{R(y_t)}\\right)\\right)\n$$\n采取此行动后，系统根据*真实*状态 $x_t$ 演化。奖励也根据真实状态计算。如果真实状态 $x_t$ 高于观测状态 $y_t$，智能体的错误感知可能导致电压违规，因为 $R(x_t)$ 将高于 $R(y_t)$。\n\n**智能体2：信念感知智能体**\n该智能体考虑了不确定性。它以保守区间 $[x_t^{\\min}, 1]$ 的形式维持对当前状态 $x_t$ 的信念。在 $t=0$ 时，我们初始化 $x_0^{\\min} = x_0$。对于随后的每一步，智能体选择一个电流 $I_t$ 以最大化*期望*即时奖励，其中期望是在信念区间 $[x_t^{\\min}, 1]$ 上对 $x_t$ 的均匀分布下计算的。\n$$\n\\max_{I_t \\in [0, I_{\\max}]} \\mathbb{E}_{x \\sim U[x_t^{\\min}, 1]}[r_t(I_t, x)] = \\max_{I_t \\in [0, I_{\\max}]} \\int_{x_t^{\\min}}^{1} \\frac{1}{1-x_t^{\\min}} r_t(I_t, x) \\, dx\n$$\n由于该积分很复杂，我们将通过在区间上采样点来进行数值近似。对于一组候选电流，我们计算期望奖励并选择产生最大值的电流。\n选择并应用 $I_t$ 后，真实状态演化到 $x_{t+1}$，智能体使用指定公式更新其下一步的信念：\n$$\nx_{t+1}^{\\min} = \\min\\left(1, \\max\\left(y_t, x_t^{\\min} + \\eta \\frac{I_t \\Delta t}{C}\\right)\\right)\n$$\n其中 $y_t$ 是真实状态 $x_t$ 的观测值。此更新规则结合了新的观测值（$y_t$）和系统的动力学。\n\n**仿真与比较**\n对于每个测试案例，我们将为每个智能体运行两次独立的仿真，从相同的初始状态 $x_0$ 开始，进行 $T=5$ 步。\n1.  **朴素智能体仿真**：在每一步 $t$，我们计算 $y_t$，确定 $I_t^{\\text{naive}}$，计算真实奖励 $r_t(x_t, I_t^{\\text{naive}})$，并将真实状态更新为 $x_{t+1}$。我们将奖励相加得到总累积奖励。\n2.  **信念感知智能体仿真**：在每一步 $t$，我们通过最大化在 $[x_t^{\\min}, 1]$ 上的期望奖励来确定 $I_t^{\\text{belief}}$，计算真实奖励 $r_t(x_t, I_t^{\\text{belief}})$，将真实状态更新为 $x_t^{\\text{true}}$，并将信念下界更新为 $x_{t+1}^{\\min}$。我们将奖励相加得到总和。\n\n最终的性能损失计算为 $R_{\\text{total}}^{\\text{belief}} - R_{\\text{total}}^{\\text{naive}}$。正值表示信念感知方法更优越，成功地减轻了部分可观测性的负面影响。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Validates and solves the battery charging POMDP problem.\n    \"\"\"\n\n    # --- Physical Model Functions ---\n\n    def V_oc(x, p):\n        \"\"\"Calculates open-circuit voltage.\"\"\"\n        return p['V_min'] + p['k_voc'] * x\n\n    def R(x, p):\n        \"\"\"Calculates internal resistance.\"\"\"\n        return p['R_0'] + p['k_R'] / (1.0 - x + p['epsilon'])\n\n    def V_terminal(x, I, p):\n        \"\"\"Calculates terminal voltage.\"\"\"\n        return V_oc(x, p) + R(x, p) * I\n\n    def charge_balance(x, I, p):\n        \"\"\"Updates state-of-charge.\"\"\"\n        return min(1.0, x + p['eta'] * I * p['delta_t'] / p['C'])\n\n    def observation(x, p):\n        \"\"\"Generates the saturated observation.\"\"\"\n        return min(x, 1.0 - p['delta'])\n\n    def reward(I, x, p):\n        \"\"\"Calculates the reward for a given action and true state.\"\"\"\n        voltage = V_terminal(x, I, p)\n        penalty = p['lambda'] if voltage > p['V_max'] else 0.0\n        return I * p['delta_t'] - penalty\n\n    # --- Agent Action Selection ---\n\n    def get_naive_action(y_t, p):\n        \"\"\"\n        Selects current for the naive agent, who treats y_t as the true state.\n        The agent maximizes current subject to the voltage constraint at y_t.\n        \"\"\"\n        if y_t >= 1.0 - p['epsilon']:\n            # Handle the case where denominator of R is zero or negative.\n            # This is an edge case where state is at or beyond the valid modeling limit.\n            # A safe action is to stop charging.\n            return 0.0\n            \n        R_at_y = R(y_t, p)\n        Voc_at_y = V_oc(y_t, p)\n        \n        if Voc_at_y >= p['V_max']:\n            return 0.0\n        \n        # This can happen if R_at_y is very small, but I_max will cap it.\n        I_allowed = (p['V_max'] - Voc_at_y) / R_at_y if R_at_y > 0 else p['I_max']\n        \n        return max(0.0, min(p['I_max'], I_allowed))\n\n    def get_belief_action(x_min_t, p, I_grid_size=201, x_grid_size=101):\n        \"\"\"\n        Selects current for the belief-aware agent.\n        Maximizes expected reward over the belief interval [x_min_t, 1.0].\n        \"\"\"\n        if x_min_t >= 1.0:\n            return 0.0\n\n        belief_interval_x = np.linspace(x_min_t, 1.0, x_grid_size)\n        current_options = np.linspace(0, p['I_max'], I_grid_size)\n        \n        best_I = 0.0\n        max_expected_reward = -np.inf\n\n        for I_candidate in current_options:\n            rewards_on_grid = np.array([reward(I_candidate, x, p) for x in belief_interval_x])\n            expected_reward = np.mean(rewards_on_grid)\n            \n            if expected_reward > max_expected_reward:\n                max_expected_reward = expected_reward\n                best_I = I_candidate\n                \n        return best_I\n\n    # --- Simulation Logic ---\n\n    def simulate_naive_agent(p):\n        \"\"\"Simulates the naive agent and returns total cumulative reward.\"\"\"\n        x_true = p['x0']\n        total_reward_val = 0.0\n        \n        for _ in range(p['T']):\n            y_t = observation(x_true, p)\n            I_t = get_naive_action(y_t, p)\n            r_t = reward(I_t, x_true, p)\n            total_reward_val += r_t\n            x_true = charge_balance(x_true, I_t, p)\n            \n        return total_reward_val\n\n    def simulate_belief_agent(p):\n        \"\"\"Simulates the belief-aware agent and returns total cumulative reward.\"\"\"\n        x_true = p['x0']\n        x_min = p['x0']\n        total_reward_val = 0.0\n        \n        for _ in range(p['T']):\n            I_t = get_belief_action(x_min, p)\n            r_t = reward(I_t, x_true, p)\n            total_reward_val += r_t\n            \n            # Observe the true state AFTER the action is applied and reward is calculated\n            y_t_next = observation(x_true, p)\n            x_true_next = charge_balance(x_true, I_t, p)\n            \n            # Update belief for the NEXT step\n            predicted_x_min = x_min + p['eta'] * I_t * p['delta_t'] / p['C']\n\n            x_min = min(1.0, max(y_t_next, predicted_x_min))\n            x_true = x_true_next\n\n        return total_reward_val\n\n    # --- Test Suite ---\n\n    common_params = {\n        'T': 5, 'eta': 1.0, 'delta_t': 1.0, 'C': 10800.0, 'I_max': 2.0,\n        'V_min': 3.0, 'k_voc': 1.2, 'V_max': 4.2, 'epsilon': 0.01\n    }\n\n    test_cases = [\n        {**common_params, 'x0': 0.98, 'delta': 0.04, 'R_0': 0.05, 'k_R': 0.04, 'lambda': 0.08},\n        {**common_params, 'x0': 0.96, 'delta': 0.0, 'R_0': 0.05, 'k_R': 0.04, 'lambda': 0.08},\n        {**common_params, 'x0': 0.99, 'delta': 0.02, 'R_0': 0.05, 'k_R': 0.08, 'lambda': 0.12},\n    ]\n\n    results = []\n    for params in test_cases:\n        reward_naive = simulate_naive_agent(params)\n        reward_belief = simulate_belief_agent(params)\n        performance_loss = reward_belief - reward_naive\n        results.append(performance_loss)\n\n    print(f\"[{','.join(f'{r:.6f}' for r in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "一旦强化学习代理训练完成，评估其性能并非运行一次测试那么简单；由于环境或策略的随机性，我们需要一个统计上稳健的方法来量化其期望性能及相关的不确定性。本练习介绍了非参数自举法(non-parametric bootstrap)，这是一种强大且广泛应用的计算方法，它可以在不对底层数据分布做强假设的情况下估计置信区间。通过这项实践()，您将学会实施自举法来为一个关键性能指标（电池退化）创建置信区间，并批判性地分析这些结果的解释局限性，例如模拟与现实的差距以及策略的非平稳性。",
            "id": "3946228",
            "problem": "您正在一个物理一致的自动化电池模拟器中评估一个已学习的强化学习 (RL) 充电策略。在每个评估回合中，该策略从一个受控的初始健康状态和环境开始执行一个充电协议，模拟器返回该回合中积分的退化量，表示为该回合损失的标称容量的分数。将每回合的退化表示为随机变量 $D$，以分数容量单位（无量纲）衡量。假设您在一个固定的已学习策略下运行 $n$ 个回合，并获得观测值 $d_1,\\dots,d_n$，其中 $d_i \\in [0,\\infty)$ 是分数。您希望推断每回合的期望退化 $\\mu = \\mathbb{E}[D]$，并使用对回合的非参数自举法来量化不确定性。\n\n从期望和样本均值的基本定义出发，并基于固定策略和静态环境下回合独立同分布 (IID) 的基本采样假设，设计一种通过有放回地重采样回合（非参数自举法）来为 $\\mu$ 构建一个 $(1-\\alpha)$ 中心置信区间的方法。不要对 $D$ 的分布做任何参数形式的假设。您的推导必须解释正在估计什么，为什么在插件原则下重采样回合可以作为样本均值抽样分布的合理近似，以及如何从经验自举分布构建该区间。然后，将此方法实现为一个程序，在给定一个小的回合数据集测试套件时，返回所要求的区间。最后，讨论这些区间在强化学习用于充电的背景下的解释局限性，特别是关于回合间的依赖性、策略的非平稳性以及模拟与现实的差距。所有物理量必须以分数容量单位（无量纲）表示。如果您需要表示比率或比例，请以小数形式表示，而不是百分比。\n\n使用的数学基础和约束：\n- 在固定策略和静态模拟器下，回合被建模为对 $D$ 的独立同分布抽取。目标是 $\\mu = \\mathbb{E}[D]$。\n- 估计量是样本均值 $\\bar{D}_n = \\frac{1}{n}\\sum_{i=1}^n d_i$。\n- 自举法使用经验分布，该分布在每个观测值 $d_i$ 上赋予 $\\frac{1}{n}$ 的概率质量，并有放回地重采样 $n$ 次，以形成一个均值的自举复制。重复此过程 $B$ 次，以近似 $\\bar{D}_n$ 的抽样分布。\n- 通过取 $B$ 个自举复制在水平 $\\frac{\\alpha}{2}$ 和 $1-\\frac{\\alpha}{2}$ 处的下和上经验分位数，来构建一个中心 $(1-\\alpha)$ 区间。\n\n不涉及角度单位。所有输出必须是分数容量单位（无量纲小数）。\n\n测试套件和可复现性：\n- 在每个测试案例中，对自举重采样使用带有固定种子的伪随机数生成器，以确保确定性输出。使用下面指定的种子。\n- 对于每个测试案例，使用 $B$ 次自举复制，在指定的显著性水平 $\\alpha$ 下，计算并返回 $\\mu$ 的中心百分位区间。\n\n测试案例：\n- 案例 A (理想路径，中等变异性)：$n = 20$， $B = 5000$， $\\alpha = 0.10$， 种子 $= 123$。回合退化值：[$0.0010$, $0.0012$, $0.0011$, $0.0015$, $0.0013$, $0.0014$, $0.0016$, $0.0012$, $0.0018$, $0.0017$, $0.0013$, $0.0019$, $0.0020$, $0.0015$, $0.0014$, $0.0016$, $0.0011$, $0.0012$, $0.0018$, $0.0010$]。\n- 案例 B (带有离群值的偏态)：$n = 15$， $B = 5000$， $\\alpha = 0.05$， 种子 $= 456$。回合退化值：[$0.0006$, $0.0007$, $0.0005$, $0.0006$, $0.0008$, $0.0007$, $0.0006$, $0.0005$, $0.0007$, $0.0006$, $0.0005$, $0.0007$, $0.0006$, $0.0006$, $0.0100$]。\n- 案例 C (退化方差)：$n = 10$， $B = 5000$， $\\alpha = 0.20$， 种子 $= 789$。回合退化值：[$0.0010$, $0.0010$, $0.0010$, $0.0010$, $0.0010$, $0.0010$, $0.0010$, $0.0010$, $0.0010$, $0.0010$]。\n- 案例 D (单回合边界情况)：$n = 1$， $B = 5000$， $\\alpha = 0.05$， 种子 $= 101112$。回合退化值：[$0.0015$]。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，列表中的每个元素是对应测试案例（按 A、B、C、D 的顺序）的区间下界和上界的双元素列表 [$L$, $U$]。例如：\"[[L_A,U_A],[L_B,U_B],[L_C,U_C],[L_D,U_D]]\"。每个 $L$ 和 $U$ 都必须是分数容量单位的小数。",
            "solution": "该问题要求设计并实现一种方法，使用非参数自举法为每回合电池期望退化 $\\mu$ 构建一个中心置信区间。该方法基于从一个运行固定强化学习策略的基于物理的模拟器中获得的 $n$ 个观测退化值 $d_1, d_2, \\dots, d_n$ 的样本。\n\n目标参数是每回合的真实期望退化，表示为 $\\mu = \\mathbb{E}[D]$，其中 $D$ 是代表退化的随机变量。观测数据 $\\{d_1, \\dots, d_n\\}$ 被假定为从 $D$ 的真实但未知的概率分布 $F$ 中抽取的 $n$ 个独立同分布 (IID) 的实现值样本。$\\mu$ 的一个自然且无偏的点估计量是样本均值：\n$$\n\\bar{D}_n = \\frac{1}{n}\\sum_{i=1}^n d_i\n$$\n为了量化此估计中的不确定性，我们的目标是为 $\\mu$ 构建一个 $(1-\\alpha)$ 置信区间。置信区间是根据样本数据计算出的一个区间，如果实验重复多次，该区间将在 $(1-\\alpha)$ 的情况下包含真实参数 $\\mu$。构建这样一个区间需要了解估计量 $\\bar{D}_n$ 的抽样分布，该分布描述了从真实分布 $F$ 中抽取的不同大小为 $n$ 的随机样本中，$\\bar{D}_n$ 的值将如何变化。\n\n核心困难在于真实分布 $F$ 是未知的，因此 $\\bar{D}_n$ 的确切抽样分布也是未知的。非参数自举法提供了一种计算方法，可以在不对 $F$ 的参数形式做任何假设的情况下近似该抽样分布。该程序的合理性依据在于**插件原则 (plug-in principle)**。插件原则表明，我们可以通过在 $F$ 的一个经验估计上计算相同的泛函，来估计真实分布 $F$ 的一个泛函。来自数据 $\\{d_1, \\dots, d_n\\}$ 的 $F$ 的最佳非参数估计是经验分布函数 (EDF)，表示为 $\\hat{F}_n$。该分布在每个观测数据点 $d_i$ 上赋予 $\\frac{1}{n}$ 的概率质量。\n\n自举法使用这一原则来近似 $\\bar{D}_n$ 的抽样分布。我们不是从未知的真实分布 $F$ 中抽取样本，而是从我们对它的最佳估计 $\\hat{F}_n$ 中抽取样本。从 $\\hat{F}_n$ 中抽取大小为 $n$ 的样本的过程，等同于从原始数据集 $\\{d_1, \\dots, d_n\\}$ 中有放回地抽取 $n$ 个值。\n\n构建一个 $(1-\\alpha)$ 中心百分位自举置信区间的算法如下：\n$1$. 从原始数据样本 $\\{d_1, \\dots, d_n\\}$ 中，生成大量的（$B$ 个）自举样本。每个自举样本 $\\{d_{1,j}^*, \\dots, d_{n,j}^*\\}$（其中 $j=1, \\dots, B$）都是通过从原始样本中有放回地抽取 $n$ 个数据点形成的。\n$2$. 对于 $B$ 个自举样本中的每一个，计算样本均值（统计量的自举复制）：\n$$\n\\bar{d}_j^* = \\frac{1}{n}\\sum_{i=1}^n d_{i,j}^*\n$$\n$3$. 这 $B$ 个自举均值的集合 $\\{\\bar{d}_1^*, \\dots, \\bar{d}_B^*\\}$，构成了对 $\\bar{D}_n$ 抽样分布的经验近似。\n$4$. 为了构建中心 $(1-\\alpha)$ 置信区间，我们找到这个自举分布的经验分位数。下界 $L$ 是排序后自举均值的第 $100 \\times (\\frac{\\alpha}{2})$ 百分位数，上界 $U$ 是第 $100 \\times (1-\\frac{\\alpha}{2})$ 百分位数。\n\n这种方法通过从样本本身进行重采样，有效地模拟了从总体中重采样的过程，从而提供了对均值抽样变异性的估计。\n\n**在强化学习用于充电背景下的解释局限性**\n\n尽管在统计上很强大，但自举置信区间的有效性和解释取决于几个关键假设，这些假设在实际应用中可能会受到挑战。\n\n$1$. **回合间的依赖性**：该方法从根本上假设退化值 $d_i$ 是独立同分布的。在电池充电中，这要求每个回合都从一个相同的初始状态开始。然而，如果一个充电回合的退化会延续下去并改变下一个回合的初始健康状态 (SoH)，那么这些回合就不再是独立的。回合 $i+1$ 的退化将取决于回合 $i$ 的结果。这种时间依赖性违反了 IID 假设，由此产生的置信区间很可能不准确，通常会低估长期平均退化的真实不确定性。\n\n$2$. **策略的非平稳性**：该分析是基于一个*固定*的已学习策略。如果在收集数据的 $n$ 个回合期间，强化学习智能体仍在训练或调整其策略，那么该策略就是非平稳的。这意味着抽取 $d_i$ 的分布 $F$ 在每个回合之间都在变化。这样一来，样本将是来自不同分布的混合体，计算出的均值 $\\mu$ 及其置信区间将不对应于任何单一、稳定策略的性能，从而使结果难以解释。\n\n$3$. **模拟与现实 (Sim2Real) 差距**：整个分析是使用来自模拟器的数据进行的。由此产生的置信区间量化的是*在模拟环境内*平均退化值的*统计不确定性*。它描述了由于我们只运行了有限数量的回合 ($n$)，而对该策略*在模拟器中*的真实平均性能所存在的不确定性。这个区间没有提供任何关于*系统误差*或*模型形式不确定性*的信息，这些不确定性源于模拟器的物理特性与真实世界电池行为之间的差异（即“模拟与现实差距”）。真实的现实世界退化可能系统性地高于或低于模拟值，并且现实世界的均值可能远远超出计算出的置信区间。因此，如果没有针对经验数据对模拟器进行单独的、严格的验证，该区间就不能被解释为该策略在物理电池上性能的一个可信范围。\n\n总之，自举置信区间是在具有 IID 回合的静态模拟环境范围内量化统计不确定性的强大工具。将其扩展到真实世界性能，需要仔细考虑回合间的依赖性、策略的平稳性以及模拟器的保真度。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by calculating bootstrap confidence intervals for the mean\n    degradation for several test cases.\n    \"\"\"\n\n    def calculate_bootstrap_ci(data, n_boot, alpha, seed):\n        \"\"\"\n        Calculates a (1-alpha) central confidence interval for the mean of the data\n        using the nonparametric percentile bootstrap method.\n\n        Args:\n            data (list or np.array): The observed data points.\n            n_boot (int): The number of bootstrap replications.\n            alpha (float): The significance level for the interval.\n            seed (int): The seed for the random number generator.\n\n        Returns:\n            list: A list containing the lower and upper bounds of the confidence interval.\n        \"\"\"\n        n = len(data)\n        data_arr = np.array(data)\n        \n        # Initialize a random number generator with the specified seed for reproducibility.\n        rng = np.random.default_rng(seed)\n        \n        # Handle the edge case of n=1, where bootstrap resampling is degenerate.\n        # Any resample will be the single element itself.\n        if n == 1:\n            mean_val = data_arr[0]\n            return [mean_val, mean_val]\n\n        # Generate bootstrap replicate means.\n        bootstrap_means = np.zeros(n_boot)\n        for i in range(n_boot):\n            # Draw n samples with replacement from the original data.\n            bootstrap_sample = rng.choice(data_arr, size=n, replace=True)\n            # Calculate the mean of the bootstrap sample.\n            bootstrap_means[i] = np.mean(bootstrap_sample)\n\n        # Calculate the lower and upper percentile points for the central interval.\n        lower_quantile = (alpha / 2.0) * 100.0\n        upper_quantile = (1.0 - alpha / 2.0) * 100.0\n        \n        # Determine the confidence interval from the percentiles of the bootstrap distribution.\n        lower_bound, upper_bound = np.percentile(bootstrap_means, [lower_quantile, upper_quantile])\n        \n        return [lower_bound, upper_bound]\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"id\": \"A\", \"n\": 20, \"B\": 5000, \"alpha\": 0.10, \"seed\": 123,\n            \"data\": [0.0010, 0.0012, 0.0011, 0.0015, 0.0013, 0.0014, 0.0016, 0.0012, \n                     0.0018, 0.0017, 0.0013, 0.0019, 0.0020, 0.0015, 0.0014, 0.0016, \n                     0.0011, 0.0012, 0.0018, 0.0010]\n        },\n        {\n            \"id\": \"B\", \"n\": 15, \"B\": 5000, \"alpha\": 0.05, \"seed\": 456,\n            \"data\": [0.0006, 0.0007, 0.0005, 0.0006, 0.0008, 0.0007, 0.0006, 0.0005, \n                     0.0007, 0.0006, 0.0005, 0.0007, 0.0006, 0.0006, 0.0100]\n        },\n        {\n            \"id\": \"C\", \"n\": 10, \"B\": 5000, \"alpha\": 0.20, \"seed\": 789,\n            \"data\": [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, \n                     0.0010, 0.0010]\n        },\n        {\n            \"id\": \"D\", \"n\": 1, \"B\": 5000, \"alpha\": 0.05, \"seed\": 101112,\n            \"data\": [0.0015]\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        # Calculate the confidence interval for the current test case.\n        interval = calculate_bootstrap_ci(\n            data=case[\"data\"],\n            n_boot=case[\"B\"],\n            alpha=case[\"alpha\"],\n            seed=case[\"seed\"]\n        )\n        results.append(interval)\n\n    # Convert the list of lists to the required string format.\n    # e.g., [[L_A, U_A], [L_B, U_B], ...]\n    result_str = str(results).replace(\" \", \"\")\n\n    # Final print statement in the exact required format.\n    print(result_str)\n\nsolve()\n```"
        }
    ]
}