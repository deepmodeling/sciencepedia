{
    "hands_on_practices": [
        {
            "introduction": "Modern battery testing generates high-dimensional datasets, where each cycle might be described by dozens of features. Principal Component Analysis (PCA) is an essential technique for reducing this complexity by transforming correlated features into a smaller set of uncorrelated principal components. This practice challenges you to derive, from first principles, how the variance of the data is partitioned among these components, a crucial step for understanding how to effectively summarize battery degradation signatures. ",
            "id": "3926067",
            "problem": "A dataset of $n$ lithium-ion battery cycles is described by $p$ scalar features extracted per cycle: voltage plateau duration during charge, differential capacity peak magnitude near the $\\mathrm{LiC}_6/\\mathrm{LiC}_{12}$ transition, impedance magnitude at a mid-frequency, charge time to a preset cutoff, and round-trip energy efficiency. Each feature is standardized across the dataset to zero mean and unit variance. Let the mean-centered and standardized data matrix be $X \\in \\mathbb{R}^{n \\times p}$, and let the unbiased sample covariance matrix be $\\Sigma \\in \\mathbb{R}^{p \\times p}$.\n\nDefine Principal Component Analysis (PCA) and derive, from first principles starting with the definitions of covariance and orthonormal eigen-decomposition, the expression for the fraction of total variance explained by the first $k$ principal components in terms of the eigenvalues of $\\Sigma$.\n\nThen, in the specific case $p = 5$ with eigenvalues of $\\Sigma$ given by $\\lambda_{1} = 2.1$, $\\lambda_{2} = 1.3$, $\\lambda_{3} = 0.9$, $\\lambda_{4} = 0.5$, and $\\lambda_{5} = 0.2$, compute the fraction of variance explained by the first $k = 2$ principal components. Express your final result as a decimal, and round your answer to four significant figures. Do not use a percentage sign.",
            "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\n**Step 1: Extract Givens**\n- A dataset of $n$ lithium-ion battery cycles.\n- $p$ scalar features are extracted per cycle.\n- The features are standardized to have zero mean and unit variance.\n- The mean-centered and standardized data matrix is $X \\in \\mathbb{R}^{n \\times p}$.\n- The unbiased sample covariance matrix is $\\Sigma \\in \\mathbb{R}^{p \\times p}$.\n- Task 1: Define Principal Component Analysis (PCA) and derive the expression for the fraction of total variance explained by the first $k$ principal components in terms of the eigenvalues of $\\Sigma$. The derivation must start from the definitions of covariance and orthonormal eigen-decomposition.\n- Task 2: For the specific case where $p = 5$ and the eigenvalues of $\\Sigma$ are $\\lambda_{1} = 2.1$, $\\lambda_{2} = 1.3$, $\\lambda_{3} = 0.9$, $\\lambda_{4} = 0.5$, and $\\lambda_{5} = 0.2$, compute the fraction of variance explained by the first $k = 2$ principal components.\n- The final numerical result must be expressed as a decimal rounded to four significant figures.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem uses standard, well-established principles of linear algebra and statistics (PCA, covariance, eigenvalues) in the context of a realistic application (battery data analysis). The premise is sound. A key consistency check is that for standardized data (where each feature has variance $1$), the total variance is the number of features, $p$. The total variance is also the trace of the covariance matrix, which equals the sum of its eigenvalues. Here, $p=5$, and the sum of the given eigenvalues is $\\lambda_1 + \\lambda_2 + \\lambda_3 + \\lambda_4 + \\lambda_5 = 2.1 + 1.3 + 0.9 + 0.5 + 0.2 = 5.0$. This matches $p=5$, confirming the internal consistency and scientific validity of the problem statement.\n- **Well-Posed:** The problem asks for a standard theoretical derivation and a specific calculation. It provides all necessary information for a unique solution.\n- **Objective:** The language is clear, precise, and devoid of subjective claims.\n\n**Step 3: Verdict and Action**\nThe problem is valid. The solution will be provided, starting with the derivation and followed by the specific calculation.\n\n### Part 1: Derivation\n\nPrincipal Component Analysis (PCA) is a linear transformation technique used for dimensionality reduction. It transforms a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components. The transformation is defined in such a way that the first principal component has the largest possible variance (that is, it accounts for as much of the variability in the data as possible), and each succeeding component in turn has the highest variance possible under the constraint that it is orthogonal to the preceding components.\n\nLet the mean-centered data matrix be $X \\in \\mathbb{R}^{n \\times p}$. The unbiased sample covariance matrix $\\Sigma$ is given by:\n$$\n\\Sigma = \\frac{1}{n-1} X^T X\n$$\nA principal component is a linear combination of the original features. A direction for this linear combination can be represented by a unit vector $w \\in \\mathbb{R}^p$ (where $w^T w = 1$). The scores of the $n$ data points when projected onto this direction are given by the vector $z = Xw$.\n\nThe variance of these projected scores is:\n$$\n\\text{Var}(z) = \\frac{1}{n-1} z^T z = \\frac{1}{n-1} (Xw)^T (Xw) = \\frac{1}{n-1} w^T X^T X w\n$$\nSubstituting the expression for $\\Sigma$, we get:\n$$\n\\text{Var}(z) = w^T \\left( \\frac{1}{n-1} X^T X \\right) w = w^T \\Sigma w\n$$\nThe first principal component is found by choosing the direction $w_1$ that maximizes this variance, subject to the constraint that $w_1$ is a unit vector. This is a constrained optimization problem:\n$$\n\\max_{w_1} w_1^T \\Sigma w_1 \\quad \\text{subject to} \\quad w_1^T w_1 = 1\n$$\nWe use the method of Lagrange multipliers. The Lagrangian function is:\n$$\nL(w_1, \\lambda) = w_1^T \\Sigma w_1 - \\lambda(w_1^T w_1 - 1)\n$$\nTo find the maximum, we compute the gradient with respect to $w_1$ and set it to zero. Since $\\Sigma$ is symmetric, the derivative of $w_1^T \\Sigma w_1$ with respect to $w_1$ is $2\\Sigma w_1$. The derivative of $w_1^T w_1$ is $2w_1$.\n$$\n\\frac{\\partial L}{\\partial w_1} = 2\\Sigma w_1 - 2\\lambda w_1 = 0\n$$\nThis simplifies to the eigenvalue equation:\n$$\n\\Sigma w_1 = \\lambda w_1\n$$\nThis shows that the optimal direction $w_1$ must be an eigenvector of the covariance matrix $\\Sigma$, and $\\lambda$ is its corresponding eigenvalue. To determine which eigenvector maximizes the variance, we substitute $\\Sigma w_1 = \\lambda w_1$ back into the variance expression:\n$$\n\\text{Var}(z) = w_1^T \\Sigma w_1 = w_1^T (\\lambda w_1) = \\lambda (w_1^T w_1)\n$$\nSince $w_1^T w_1 = 1$, we have:\n$$\n\\text{Var}(z) = \\lambda\n$$\nThe variance of the data projected onto an eigenvector is equal to the corresponding eigenvalue. To maximize the variance, we must choose the eigenvector $w_1$ corresponding to the largest eigenvalue, $\\lambda_1$. Thus, the first principal component is the direction of the eigenvector with the largest eigenvalue, and the variance it explains is $\\lambda_1$.\n\nSubsequent principal components are found by maximizing variance in directions orthogonal to all previous components. This procedure systematically selects the eigenvectors of $\\Sigma$ in descending order of their corresponding eigenvalues. The $j$-th principal component corresponds to the eigenvector $w_j$ with the $j$-th largest eigenvalue $\\lambda_j$, and it explains a variance of $\\lambda_j$.\n\nThe total variance of the original dataset is the sum of the variances of the individual features, which is the sum of the diagonal elements of the covariance matrix, i.e., its trace:\n$$\n\\text{Total Variance} = \\text{tr}(\\Sigma) = \\sum_{i=1}^{p} \\Sigma_{ii}\n$$\nThe covariance matrix $\\Sigma$ is a real, symmetric matrix, so it admits an orthonormal eigen-decomposition $\\Sigma = W \\Lambda W^T$, where $W$ is an orthogonal matrix ($W^T W = I$) whose columns are the eigenvectors $w_j$, and $\\Lambda$ is a diagonal matrix of the eigenvalues $\\lambda_j$. Using the cyclic property of the trace ($\\text{tr}(ABC) = \\text{tr}(BCA)$):\n$$\n\\text{tr}(\\Sigma) = \\text{tr}(W \\Lambda W^T) = \\text{tr}(W^T W \\Lambda) = \\text{tr}(I \\Lambda) = \\text{tr}(\\Lambda)\n$$\nThe trace of the diagonal matrix $\\Lambda$ is the sum of its diagonal entries, which are the eigenvalues:\n$$\n\\text{Total Variance} = \\sum_{j=1}^{p} \\lambda_j\n$$\nThe total variance of the dataset is equal to the sum of the eigenvalues of its covariance matrix.\n\nThe variance explained by the first $k$ principal components is the sum of their individual variances:\n$$\n\\text{Variance explained by first } k \\text{ PCs} = \\sum_{j=1}^{k} \\lambda_j\n$$\nTherefore, the fraction of total variance explained by the first $k$ principal components is the ratio of the variance they explain to the total variance:\n$$\n\\text{Fraction of Variance} = \\frac{\\sum_{j=1}^{k} \\lambda_j}{\\sum_{j=1}^{p} \\lambda_j}\n$$\nThis completes the derivation.\n\n### Part 2: Calculation\n\nWe are given $p = 5$ features and asked to find the fraction of variance explained by the first $k=2$ principal components. The eigenvalues are provided in descending order:\n$\\lambda_{1} = 2.1$, $\\lambda_{2} = 1.3$, $\\lambda_{3} = 0.9$, $\\lambda_{4} = 0.5$, and $\\lambda_{5} = 0.2$.\n\nThe variance explained by the first $k=2$ principal components is the sum of the first two eigenvalues:\n$$\n\\sum_{j=1}^{2} \\lambda_j = \\lambda_1 + \\lambda_2 = 2.1 + 1.3 = 3.4\n$$\nThe total variance is the sum of all $p=5$ eigenvalues:\n$$\n\\sum_{j=1}^{5} \\lambda_j = \\lambda_1 + \\lambda_2 + \\lambda_3 + \\lambda_4 + \\lambda_5 = 2.1 + 1.3 + 0.9 + 0.5 + 0.2 = 5.0\n$$\nThe fraction of variance explained by the first $k=2$ components is:\n$$\n\\text{Fraction} = \\frac{\\sum_{j=1}^{2} \\lambda_j}{\\sum_{j=1}^{5} \\lambda_j} = \\frac{3.4}{5.0} = 0.68\n$$\nThe problem requires the answer to be rounded to four significant figures.\n$$\n0.68 = 0.6800\n$$",
            "answer": "$$\\boxed{0.6800}$$"
        },
        {
            "introduction": "Predicting battery properties like cycle life is a central task, but experimental measurements are often subject to noise and occasional large errors from testing anomalies. Simply minimizing the squared error can lead to models that are overly sensitive to these outliers. This exercise introduces the Huber loss function, a robust alternative that combines the best properties of squared and absolute error, and asks you to derive its gradient to understand mathematically why it provides more stable training on imperfect real-world data. ",
            "id": "3926194",
            "problem": "In automated battery design and simulation, consider supervised regression to predict cycle life from high-fidelity simulation descriptors. Let the input descriptor vectors be $\\{x_i\\}_{i=1}^{N}$ with $x_i \\in \\mathbb{R}^{d}$, and the measured cycle lives be $\\{y_i\\}_{i=1}^{N}$, where each $y_i$ is the number of charge-discharge cycles until a specified end-of-life criterion. Assume a linear predictor $f_{\\theta}(x) = x^{\\top}\\theta$ with parameter vector $\\theta \\in \\mathbb{R}^{d}$ and residuals $r_i(\\theta) = x_i^{\\top}\\theta - y_i$. Measurement processes for $y_i$ are subject to occasional large errors due to test interruptions and sensor miscalibrations, yielding heavy-tailed residual distributions.\n\nStarting from the empirical risk minimization principle, define a robust loss that interpolates between the squared loss for small residuals and the absolute-value loss for large residuals. Using this definition as the data fidelity term, construct the empirical objective $J(\\theta)$ for the dataset $\\{(x_i, y_i)\\}_{i=1}^{N}$ and derive the gradient $\\nabla_{\\theta} J(\\theta)$. Your derivation must begin from fundamental definitions of loss-based risk and proceed by applying the chain rule for vector-valued parameters.\n\nExplain, using the structure of your derived gradient, why this loss is more robust to outliers than the pure squared loss in the presence of noisy battery life measurements. Explicitly characterize how the per-sample contribution to the gradient changes as $|r_i(\\theta)|$ grows and how this affects optimization stability in the face of extreme residuals.\n\nExpress your final answer as a single closed-form analytical expression for $\\nabla_{\\theta} J(\\theta)$ in terms of $x_i$, $y_i$, $\\theta$, and a positive threshold parameter $\\delta$, with $N$ samples. No numerical evaluation is required. Do not report intermediate results. If you introduce any auxiliary functions, define them within your derivation but ensure the final answer is written as a single expression. The final answer must be unitless and does not require rounding.",
            "solution": "The problem statement is evaluated as valid. It is scientifically grounded in standard machine learning principles, well-posed with a clear objective, and free from any listed invalidating flaws.\n\n**1. Definition of the Robust Loss Function**\n\nThe empirical risk minimization principle states that we seek a parameter vector $\\theta$ that minimizes an objective function $J(\\theta)$, defined as the average loss over a dataset.\n$$\nJ(\\theta) = \\frac{1}{N} \\sum_{i=1}^{N} L(y_i, f_{\\theta}(x_i))\n$$\nThe problem requires a robust loss function that interpolates between the squared loss ($L_2$) for small residuals and the absolute-value loss ($L_1$) for large residuals. This is characteristic of the Huber loss, which is defined using a positive threshold parameter $\\delta$. Let the residual be $r = f_{\\theta}(x) - y = x^{\\top}\\theta - y$. The Huber loss, $L_{\\delta}(r)$, is given by:\n$$\nL_{\\delta}(r) =\n\\begin{cases}\n\\frac{1}{2} r^2 & \\text{if } |r| \\le \\delta \\\\\n\\delta|r| - \\frac{1}{2}\\delta^2 & \\text{if } |r| > \\delta\n\\end{cases}\n$$\nThis function is quadratic for small residuals ($|r| \\le \\delta$), promoting efficiency, and linear for large residuals ($|r| > \\delta$), providing robustness to outliers. The term $-\\frac{1}{2}\\delta^2$ ensures that the function is continuous at the points $|r| = \\delta$. The first derivative of the loss with respect to the residual $r$ is also continuous:\n$$\n\\frac{dL_{\\delta}}{dr}(r) =\n\\begin{cases}\nr & \\text{if } |r| \\le \\delta \\\\\n\\delta \\cdot \\text{sgn}(r) & \\text{if } |r| > \\delta\n\\end{cases}\n$$\nwhere $\\text{sgn}(r)$ is the sign function.\n\n**2. Construction of the Empirical Objective and Gradient Derivation**\n\nUsing the Huber loss as the data fidelity term, the empirical objective function $J(\\theta)$ for the dataset $\\{(x_i, y_i)\\}_{i=1}^{N}$ is the average loss over all samples:\n$$\nJ(\\theta) = \\frac{1}{N} \\sum_{i=1}^{N} L_{\\delta}(r_i(\\theta))\n$$\nwhere $r_i(\\theta) = x_i^{\\top}\\theta - y_i$.\n\nTo find the gradient $\\nabla_{\\theta} J(\\theta)$, we differentiate $J(\\theta)$ with respect to the vector $\\theta$. By linearity of the gradient operator:\n$$\n\\nabla_{\\theta} J(\\theta) = \\nabla_{\\theta} \\left( \\frac{1}{N} \\sum_{i=1}^{N} L_{\\delta}(r_i(\\theta)) \\right) = \\frac{1}{N} \\sum_{i=1}^{N} \\nabla_{\\theta} L_{\\delta}(r_i(\\theta))\n$$\nWe apply the chain rule for vector-valued parameters to each term in the summation. The loss $L_{\\delta}$ is a scalar function of the scalar residual $r_i$, which in turn is a scalar function of the vector parameter $\\theta$.\n$$\n\\nabla_{\\theta} L_{\\delta}(r_i(\\theta)) = \\frac{dL_{\\delta}}{dr_i}(r_i(\\theta)) \\cdot \\nabla_{\\theta} r_i(\\theta)\n$$\nFirst, we compute the gradient of the residual $r_i(\\theta)$ with respect to $\\theta$:\n$$\nr_i(\\theta) = x_i^{\\top}\\theta - y_i\n$$\n$$\n\\nabla_{\\theta} r_i(\\theta) = \\nabla_{\\theta} (x_i^{\\top}\\theta - y_i) = x_i\n$$\nNow, substituting the derivatives back into the chain rule expression:\n$$\n\\nabla_{\\theta} L_{\\delta}(r_i(\\theta)) = \\left( \\begin{cases} r_i(\\theta) & \\text{if } |r_i(\\theta)| \\le \\delta \\\\ \\delta \\cdot \\text{sgn}(r_i(\\theta)) & \\text{if } |r_i(\\theta)| > \\delta \\end{cases} \\right) \\cdot x_i\n$$\nSubstituting $r_i(\\theta) = x_i^{\\top}\\theta - y_i$, the per-sample contribution to the gradient is:\n$$\n\\nabla_{\\theta} L_{\\delta}(r_i(\\theta)) =\n\\begin{cases}\n(x_i^{\\top}\\theta - y_i)x_i & \\text{if } |x_i^{\\top}\\theta - y_i| \\le \\delta \\\\\n\\delta \\cdot \\text{sgn}(x_i^{\\top}\\theta - y_i) \\cdot x_i & \\text{if } |x_i^{\\top}\\theta - y_i| > \\delta\n\\end{cases}\n$$\nFinally, summing over all $N$ samples and dividing by $N$ gives the complete gradient of the objective function:\n$$\n\\nabla_{\\theta} J(\\theta) = \\frac{1}{N} \\sum_{i=1}^{N}\n\\begin{cases}\n(x_i^{\\top}\\theta - y_i)x_i & \\text{if } |x_i^{\\top}\\theta - y_i| \\le \\delta \\\\\n\\delta \\cdot \\text{sgn}(x_i^{\\top}\\theta - y_i) \\cdot x_i & \\text{if } |x_i^{\\top}\\theta - y_i| > \\delta\n\\end{cases}\n$$\n\n**3. Analysis of Robustness to Outliers**\n\nThe robustness of the Huber loss is evident from the structure of its gradient, especially when compared to the gradient of the pure squared loss (Mean Squared Error, MSE). For MSE, the loss for a single sample is $\\frac{1}{2}r_i^2$, and the corresponding gradient contribution is always $r_i(\\theta)x_i = (x_i^{\\top}\\theta - y_i)x_i$.\n\n-   **For small residuals ($|r_i(\\theta)| \\le \\delta$):** The gradient contribution for the Huber loss is $(x_i^{\\top}\\theta - y_i)x_i$. This is identical to the gradient contribution for the squared loss. In this regime, where data points are considered \"inliers\" and fit the model well, the optimization behaves like standard least squares.\n\n-   **For large residuals ($|r_i(\\theta)| > \\delta$):** This is the critical region for robustness.\n    -   For the **squared loss**, the magnitude of the gradient contribution, $\\|(x_i^{\\top}\\theta - y_i)x_i\\| = |r_i(\\theta)| \\|x_i\\|$, grows *linearly and without bound* as the residual magnitude $|r_i(\\theta)|$ increases. An outlier with a very large residual (due to sensor miscalibration, etc.) will generate a disproportionately large gradient. In an optimization algorithm like gradient descent, this single outlier can dominate the gradient update step, pulling the parameter vector $\\theta$ far from a solution that is optimal for the rest of the data. This leads to instability and a final model that is heavily skewed by outliers.\n\n    -   For the **Huber loss**, the magnitude of the gradient contribution becomes constant once the residual exceeds $\\delta$. The contribution is $\\delta \\cdot \\text{sgn}(x_i^{\\top}\\theta - y_i) \\cdot x_i$, and its magnitude is $\\|\\delta \\cdot \\text{sgn}(r_i(\\theta)) \\cdot x_i\\| = \\delta \\|x_i\\|$. This value is *independent* of the residual magnitude $|r_i(\\theta)|$. This means that an outlier with an extremely large residual contributes no more to the gradient update than a point whose residual just barely exceeds $\\delta$. The gradient is effectively \"clipped\".\n\nThis clipping mechanism prevents outliers from exerting an unbounded influence on the gradient. It ensures that the optimization process is more stable and that the final parameters $\\theta$ reflect the underlying trend of the majority of the data, rather than being distorted by a few anomalous measurements. Therefore, the Huber loss is more robust than the squared loss in the presence of heavy-tailed noise and outliers, such as the noisy battery life measurements described in the problem.",
            "answer": "$$ \\boxed{ \\frac{1}{N} \\sum_{i=1}^{N} \\begin{cases} (x_i^{\\top}\\theta - y_i)x_i & \\text{if } |x_i^{\\top}\\theta - y_i| \\le \\delta \\\\ \\delta \\, \\text{sgn}(x_i^{\\top}\\theta - y_i) x_i & \\text{if } |x_i^{\\top}\\theta - y_i| > \\delta \\end{cases} } $$"
        },
        {
            "introduction": "Evaluating a model's true performance requires a carefully designed validation strategy. In battery failure prediction, this is complicated by two common issues: severe class imbalance (failures are rare) and batch-to-batch variations from manufacturing. This practice explores how stratified sampling is used to create representative cross-validation folds, ensuring that the model is tested fairly on all failure types and across different manufacturing batches, leading to a more reliable estimate of its generalization capability. ",
            "id": "3926147",
            "problem": "A battery failure prediction dataset used in Automated Battery Design and Simulation consists of cell-level measurements grouped by manufacturing batch. Each cell is labeled with one of three classes: Class $0$ (healthy), Class $1$ (early capacity fade), and Class $2$ (internal short). The dataset has $B=10$ batches and a total of $N=500$ cells. The per-batch counts are:\n- Batch $1$: Class $0$: $50$, Class $1$: $5$, Class $2$: $2$.\n- Batch $2$: Class $0$: $40$, Class $1$: $8$, Class $2$: $0$.\n- Batch $3$: Class $0$: $45$, Class $1$: $9$, Class $2$: $1$.\n- Batch $4$: Class $0$: $48$, Class $1$: $8$, Class $2$: $3$.\n- Batch $5$: Class $0$: $42$, Class $1$: $12$, Class $2$: $2$.\n- Batch $6$: Class $0$: $39$, Class $1$: $10$, Class $2$: $1$.\n- Batch $7$: Class $0$: $45$, Class $1$: $12$, Class $2$: $0$.\n- Batch $8$: Class $0$: $41$, Class $1$: $8$, Class $2$: $5$.\n- Batch $9$: Class $0$: $25$, Class $1$: $5$, Class $2$: $3$.\n- Batch $10$: Class $0$: $25$, Class $1$: $3$, Class $2$: $3$.\n\nThe totals are Class $0$: $400$, Class $1$: $80$, Class $2$: $20$, so the global class proportions are $p_0=400/500=0.8$, $p_1=80/500=0.16$, and $p_2=20/500=0.04$.\n\nYou are asked to construct $K=5$ stratified folds for Cross-Validation (CV) under the following constraints designed to ensure minimum batch representation of minority failure classes:\n- Each fold must include samples from at least $m_1=3$ distinct batches that contain Class $1$, and at least $m_2=2$ distinct batches that contain Class $2$.\n- If a batch contributes to a fold for Class $1$, allocate at least $s_{\\min,1}=2$ Class $1$ samples from that batch to the fold. If a batch contributes to a fold for Class $2$, allocate at least $s_{\\min,2}=1$ Class $2$ sample from that batch to the fold.\n- After satisfying these minimum batch-representation constraints in a fold, fill the remaining positions in the fold by stratified sampling from the remaining pool, preserving the global class proportions of the remaining samples as closely as possible.\n- Each fold has size $N/K=100$.\n\nStarting from the definition of stratified sampling and the law of total expectation, explain how stratified splitting addresses imbalanced failure classes in the context of battery datasets with batch effects, and compute the expected class proportions in each fold under the above constraints. Select the correct option for the expected class proportions per fold.\n\nA. Class $0$: $0.80$, Class $1$: $0.16$, Class $2$: $0.04$.\n\nB. Class $0$: $0.78$, Class $1$: $0.18$, Class $2$: $0.04$.\n\nC. Class $0$: $0.80$, Class $1$: $0.12$, Class $2$: $0.08$.\n\nD. Class $0$: $0.79$, Class $1$: $0.15$, Class $2$: $0.06$.",
            "solution": "The problem statement has been validated and is deemed scientifically grounded, consistent, and well-posed under a standard interpretation of its constraints. All given data, including class counts ($N_0=400$, $N_1=80$, $N_2=20$) and total sample size ($N=500$), are internally consistent. The task is twofold: first, to explain the role of stratified splitting for imbalanced datasets with batch effects, and second, to compute the expected class proportions in each of the $K=5$ cross-validation folds.\n\n**Conceptual Explanation of Stratified Splitting**\n\nIn machine learning, particularly with imbalanced datasets such as those for battery failure prediction, standard random sampling for cross-validation can be problematic. A minority class, like 'internal short' (Class $2$), might by chance be entirely absent from a test fold. This would render performance evaluation on that class impossible for that iteration of the cross-validation, leading to an unreliable and biased overall performance estimate.\n\nStratified sampling directly addresses this issue. By definition, stratification is the process of dividing a population into homogeneous subgroups (strata) before sampling. In the context of cross-validation, the strata are the classes. Stratified splitting ensures that each fold is a microcosm of the entire dataset, preserving the global class proportions as closely as possible. For instance, if Class $2$ constitutes $4\\%$ of the total data, each fold will also contain approximately $4\\%$ of Class $2$ samples. This guarantees that the model is trained and evaluated on all classes in every fold, yielding a more stable and accurate assessment of its generalization performance.\n\nThe problem introduces an additional layer of complexity: batch effects. Battery cells from different manufacturing batches can have subtle variations that affect their performance and failure modes. A model might perform well on failures from one batch but poorly on another. The constraints to include samples from a minimum number of distinct batches for the failure classes ($m_1=3$ for Class $1$, $m_2=2$ for Class $2$) are designed to combat this. This forces each test fold to contain a diversity of failure \"signatures\" from different batches, providing a more rigorous test of the model's ability to generalize not just across samples, but also across manufacturing batches.\n\n**Derivation of Expected Class Proportions**\n\nThe problem asks for the expected class proportions in each fold. This can be determined from first principles of probability and symmetry, a conclusion that can be verified by stepping through the described complex sampling procedure.\n\n**Method 1: Argument from Symmetry and Expectation**\n\nLet $N_c$ be the total number of samples of class $c$ in the dataset, where $c \\in \\{0, 1, 2\\}$. We have $N_0 = 400$, $N_1 = 80$, and $N_2 = 20$. The total number of samples is $N = \\sum_c N_c = 500$. The task is to partition these $N$ samples into $K=5$ folds.\n\nLet $N_{k,c}$ be the random variable representing the number of samples of class $c$ in a specific fold $k$. The partitioning rule means that every sample must belong to exactly one fold, so summing over all folds must yield the total number of samples of that class:\n$$ \\sum_{k=1}^{K} N_{k,c} = N_c $$\nBy the linearity of expectation, the sum of the expected values is the expectation of the sum:\n$$ \\sum_{k=1}^{K} E[N_{k,c}] = E\\left[\\sum_{k=1}^{K} N_{k,c}\\right] = E[N_c] = N_c $$\nThe last equality holds because $N_c$ is a fixed, given quantity.\n\nThe problem describes a set of rules for constructing the folds. Crucially, these rules are identical for every fold; the procedure does not distinguish between fold $1$, fold $2$, etc. Therefore, by symmetry, the expected composition of each fold must be the same. This means the expected number of samples of class $c$ is the same for all folds:\n$$ E[N_{1,c}] = E[N_{2,c}] = \\dots = E[N_{K,c}] $$\nLet's denote this common expected value as $E_c$. Substituting this into the sum:\n$$ \\sum_{k=1}^{K} E_c = K \\cdot E_c = N_c $$\nThis gives us a fundamental result for the expected number of samples of any class $c$ in any given fold $k$:\n$$ E[N_{k,c}] = \\frac{N_c}{K} $$\nThis result holds as long as the constraints specified in the problem permit a valid partition, which they do.\n\nUsing the given values:\n- Expected count for Class $0$: $E[N_{k,0}] = \\frac{400}{5} = 80$.\n- Expected count for Class $1$: $E[N_{k,1}] = \\frac{80}{5} = 16$.\n- Expected count for Class $2$: $E[N_{k,2}] = \\frac{20}{5} = 4$.\n\nThe size of each fold is $N/K = 500/5 = 100$. The expected proportions are:\n- Proportion Class $0$: $\\frac{80}{100} = 0.80$.\n- Proportion Class $1$: $\\frac{16}{100} = 0.16$.\n- Proportion Class $2$: $\\frac{4}{100} = 0.04$.\n\nThese expected proportions are identical to the global class proportions.\n\n**Method 2: Verification via the Specified Sampling Procedure**\n\nWe can verify this result by meticulously following the described two-stage procedure. This demonstrates that the complex procedure is consistent with the symmetry argument. The procedure involves a \"constrained allocation\" stage followed by a \"stratified fill-up\" stage. We assume the ambiguous phrase \"allocate at least $s_{\\min}$\" implies allocating exactly the minimum required number of samples for the constrained part, which is a standard interpretation.\n\n**Stage 1: Constrained Allocation**\nFor each of the $K=5$ folds, we must allocate samples to satisfy the minimum batch representation constraints.\n- For Class $1$: Each fold requires samples from $m_1=3$ batches, with at least $s_{\\min,1}=2$ samples from each. The number of Class $1$ samples pre-allocated to each fold is $3 \\times 2 = 6$.\n- For Class $2$: Each fold requires samples from $m_2=2$ batches, with at least $s_{\\min,2}=1$ sample from each. The number of Class $2$ samples pre-allocated to each fold is $2 \\times 1 = 2$.\n- For Class $0$: There are no such constraints, so $0$ samples are pre-allocated.\n\nThe total number of samples allocated across all $5$ folds in this stage is:\n- Class $1$: $5 \\text{ folds} \\times 6 \\text{ samples/fold} = 30$ samples.\n- Class $2$: $5 \\text{ folds} \\times 2 \\text{ samples/fold} = 10$ samples.\n- Class $0$: $5 \\text{ folds} \\times 0 \\text{ samples/fold} = 0$ samples.\n\n**Stage 2: Stratified Fill-Up**\nNow, we find the pool of remaining samples to be distributed.\n- Remaining Class $0$ samples: $400 - 0 = 400$.\n- Remaining Class $1$ samples: $80 - 30 = 50$.\n- Remaining Class $2$ samples: $20 - 10 = 10$.\n- Total remaining samples: $400 + 50 + 10 = 460$.\n\nNext, we find the number of slots remaining in each fold.\n- Each fold has a total size of $100$.\n- In Stage 1, each fold was allocated $6$ (Class $1$) $+ 2$ (Class $2$) $= 8$ samples.\n- Number of remaining slots per fold: $100 - 8 = 92$.\n- Total remaining slots across all folds: $5 \\times 92 = 460$, which matches the total remaining samples, confirming consistency.\n\nThese $92$ slots per fold are filled by stratified sampling from the remaining pool of $460$ samples. The expected number of samples of each class added to a fold is:\n- Class $0$ added: $92 \\times \\frac{400}{460} = 92 \\times \\frac{40}{46} = 2 \\times 40 = 80$.\n- Class $1$ added: $92 \\times \\frac{50}{460} = 92 \\times \\frac{5}{46} = 2 \\times 5 = 10$.\n- Class $2$ added: $92 \\times \\frac{10}{460} = 92 \\times \\frac{1}{46} = 2 \\times 1 = 2$.\n\n**Final Composition**\nThe total expected count for each class per fold is the sum from Stage 1 and Stage 2.\n- Expected Class $0$ count: $0 (\\text{Stage } 1) + 80 (\\text{Stage } 2) = 80$.\n- Expected Class $1$ count: $6 (\\text{Stage } 1) + 10 (\\text{Stage } 2) = 16$.\n- Expected Class $2$ count: $2 (\\text{Stage } 1) + 2 (\\text{Stage } 2) = 4$.\n\nThe total count per fold is $80 + 16 + 4 = 100$. The expected proportions are:\n- Proportion Class $0$: $80/100 = 0.80$.\n- Proportion Class $1$: $16/100 = 0.16$.\n- Proportion Class $2$: $4/100 = 0.04$.\n\nBoth methods yield the same result. The complex constraints, while ensuring batch diversity, are constructed in a way that perfectly preserves the principles of stratification in expectation.\n\n**Option-by-Option Analysis**\n\nA. Class $0$: $0.80$, Class $1$: $0.16$, Class $2$: $0.04$.\nThis option matches our derived expected proportions $\\{0.80, 0.16, 0.04\\}$.\n**Verdict: Correct.**\n\nB. Class $0$: $0.78$, Class $1$: $0.18$, Class $2$: $0.04$.\nThis option suggests $78$ samples of Class $0$, $18$ of Class $1$, and $4$ of Class $2$. This is inconsistent with our derivation.\n**Verdict: Incorrect.**\n\nC. Class $0$: $0.80$, Class $1$: $0.12$, Class $2$: $0.08$.\nThis option suggests $80$ samples of Class $0$, $12$ of Class $1$, and $8$ of Class $2$. This is inconsistent with our derivation.\n**Verdict: Incorrect.**\n\nD. Class $0$: $0.79$, Class $1$: $0.15$, Class $2$: $0.06$.\nThis option suggests $79$ samples of Class $0$, $15$ of Class $1$, and $6$ of Class $2$. This is inconsistent with our derivation.\n**Verdict: Incorrect.**",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}