{
    "hands_on_practices": [
        {
            "introduction": "在构建多项式代理模型时，一个基本问题是模型参数的可辨识性。当模型中的特征（多项式基函数）数量超过观测样本数时，标准的最小二乘法将无法给出唯一解。本练习通过一个假设场景，探讨了当设计矩阵的列数 $m$ 大于行数 $n$ 时出现的可辨识性问题，并要求您从数学原理出发，诊断该问题并选择最合适的修正策略 。这有助于您深入理解正则化、特征选择和实验设计在处理欠定问题时的关键作用。",
            "id": "3941886",
            "problem": "在自动化电池设计和仿真中，假设需要寻找一个代理模型，该模型将电极设计变量向量 $\\mathbf{x} \\in \\mathbb{R}^{p}$（例如，厚度、孔隙率、颗粒尺寸、粘合剂分数）映射到诸如放电容量 $C$ 的响应。该代理模型被指定为一个总次数为 $3$ 的多项式回归，构建于一个具有 $m=35$ 个基函数（包括交互项和三次项）的多项式特征映射 $\\phi(\\mathbf{x}) \\in \\mathbb{R}^{m}$ 之上，从而得到一个参数线性模型 $C \\approx \\phi(\\mathbf{x})^{\\top}\\boldsymbol{\\beta}$，其参数向量为 $\\boldsymbol{\\beta} \\in \\mathbb{R}^{m}$。通过自动化仿真收集了一个包含 $n=30$ 个电池设计的合成数据集，得到设计矩阵 $X \\in \\mathbb{R}^{n \\times m}$（其中 $n=30$，$m=35$）和响应 $\\mathbf{y} \\in \\mathbb{R}^{n}$。\n\n根据线性模型的可辨识性定义和最小二乘估计的性质，判断在此设置中参数向量 $\\boldsymbol{\\beta}$ 是否可以从 $(X,\\mathbf{y})$ 中辨识出来，并选择最合适的补救措施，以便在自动化电池设计和仿真的约束下获得一个唯一且具有物理意义的代理模型。\n\n选择唯一的最佳选项：\n\nA. 模型是不可辨识的，因为 $m>n$ 意味着 $\\operatorname{rank}(X) \\leq n  m$，所以 $\\boldsymbol{\\beta}$ 不能由 $(X,\\mathbf{y})$ 唯一确定。一种补救措施是施加正则化（例如，使用惩罚参数 $\\lambda0$ 的岭回归），或通过基于物理信息的特征选择来减少 $m$，和/或通过实验设计增加 $n$ 以实现 $\\operatorname{rank}(X)=m$。\n\nB. 模型通过包含一个截距项而变得可辨识，因为增加一个常数列可以稳定回归；因此只需将 $\\phi(\\mathbf{x})$ 扩充一个 $1$ 即可解决问题，而无需改变 $n$。\n\nC. 模型是可辨识的，因为一个 $3$ 次多项式足够灵活以捕捉电池的物理特性，所以可以无需修改直接进行普通最小二乘法。\n\nD. 如果测量噪声是高斯的，模型就是可辨识的，因为高斯噪声意味着最大似然与最小二乘法一致，所以可以无需正则化进行拟合。\n\nE. 模型不可辨识仅仅是由于多项式特征之间的多重共线性；即使在 $mn$ 的情况下，仅对基进行正交化（例如，通过 Gram–Schmidt 方法）就能确保可辨识性。",
            "solution": "问题陈述描述了一个场景，即需要为一个自动化电池设计过程学习一个代理模型。该模型是基于一组多项式特征的线性回归。我们必须首先分析模型参数唯一可辨识性的条件。\n\n线性模型由方程 $\\mathbf{y} = X\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}$ 给出，其中 $\\mathbf{y} \\in \\mathbb{R}^{n}$ 是观测响应向量，$X \\in \\mathbb{R}^{n \\times m}$ 是设计矩阵（其行是特征向量 $\\phi(\\mathbf{x}_i)^\\top$），$\\boldsymbol{\\beta} \\in \\mathbb{R}^{m}$ 是待估参数向量，$\\boldsymbol{\\epsilon} \\in \\mathbb{R}^{n}$ 是未观测误差向量。\n\n如果参数向量 $\\boldsymbol{\\beta}$ 可以从数据 $(X, \\mathbf{y})$ 中唯一确定，则认为它是可辨识的。在普通最小二乘法 (OLS) 的背景下，估计值 $\\hat{\\boldsymbol{\\beta}}$ 是使残差平方和 $||\\mathbf{y} - X\\boldsymbol{\\beta}||_2^2$ 最小化的向量。这个最小化问题的解由正规方程给出：\n$$X^{\\top}X\\boldsymbol{\\beta} = X^{\\top}\\mathbf{y}$$\n为了使 $\\boldsymbol{\\beta}$ 存在唯一解，矩阵 $X^{\\top}X$ 必须是可逆的。矩阵 $X^{\\top}X$ 是一个 $m \\times m$ 的方阵。一个方阵可逆的充要条件是它具有满秩，即 $\\operatorname{rank}(X^{\\top}X) = m$。这个条件当且仅当设计矩阵 $X$ 具有满列秩时才满足，即 $\\operatorname{rank}(X) = m$。\n\n任何矩阵的秩不能超过其行数和列数的最小值。对于设计矩阵 $X \\in \\mathbb{R}^{n \\times m}$，我们有：\n$$\\operatorname{rank}(X) \\leq \\min(n, m)$$\n在问题中，我们已知：\n- 观测数量（电池设计）：$n=30$\n- 模型参数数量（基函数）：$m=35$\n\n由于 $n  m$，设计矩阵 $X$ 的最大可能秩受行数限制：\n$$\\operatorname{rank}(X) \\leq \\min(30, 35) = 30$$\n这意味着 $\\operatorname{rank}(X) \\leq 30$，严格小于参数数量 $m=35$。因此，$X$ 不具有满列秩。因此，矩阵 $X^{\\top}X$ 是奇异的（不可逆的），正规方程 $X^{\\top}X\\boldsymbol{\\beta} = X^{\\top}\\mathbf{y}$ 没有唯一解。存在无穷多个向量 $\\boldsymbol{\\beta}$ 满足这些方程，这意味着参数向量无法使用 OLS 从数据中辨识出来。这是一个经典的“欠定”或“高维”问题，其中特征数量多于观测数量。\n\n为了在这种情况下获得唯一且稳定的解，必须修改问题。标准方法包括：\n1.  **正则化 (Regularization)：** 在最小二乘目标函数中增加一个惩罚项。例如，岭回归（Tikhonov 正则化）最小化 $||\\mathbf{y} - X\\boldsymbol{\\beta}||_2^2 + \\lambda ||\\boldsymbol{\\beta}||_2^2$，其中惩罚参数 $\\lambda  0$。解为 $\\hat{\\boldsymbol{\\beta}} = (X^{\\top}X + \\lambda I)^{-1}X^{\\top}\\mathbf{y}$。对于任何 $\\lambda  0$，矩阵 $(X^{\\top}X + \\lambda I)$ 都是可逆的，从而保证了唯一解。其他方法如 Lasso（L1 正则化）也能产生唯一解并能进行特征选择。\n2.  **特征缩减 (Feature Reduction)：** 减少参数数量 $m$，使其满足 $m \\leq n$。这可以通过基于先验知识（例如，基于物理信息的特征选择）或自动化技术从最初的 $35$ 个特征中选择一个子集来完成。\n3.  **数据增广 (Data Augmentation)：** 增加观测数量 $n$，使其满足 $n \\geq m$。这涉及到运行更多的仿真，通常由实验设计 (DoE) 方法论指导，以确保新的数据点信息量最大，并有助于确保最终的设计矩阵 $X$ 具有满列秩。\n\n现在我们评估每个选项：\n\n**A. 模型是不可辨识的，因为 $mn$ 意味着 $\\operatorname{rank}(X) \\leq n  m$，所以 $\\boldsymbol{\\beta}$ 不能由 $(X,\\mathbf{y})$ 唯一确定。一种补救措施是施加正则化（例如，使用惩罚参数 $\\lambda0$ 的岭回归），或通过基于物理信息的特征选择来减少 $m$，和/或通过实验设计增加 $n$ 以实现 $\\operatorname{rank}(X)=m$。**\n该陈述基于 $n=30$ 和 $m=35$ 之间的关系准确地诊断了问题。它正确地得出结论，$mn$ 导致 $\\operatorname{rank}(X)  m$，从而导致不可辨识性。然后，它正确地列出了针对这种情况的三种标准且适当的补救措施：正则化、减少特征数 $m$ 以及增加观测数 $n$。所提出的补救措施在数学上是合理的，在自动化设计问题的背景下也是恰当的。\n**结论：** 正确。\n\n**B. 模型通过包含一个截距项而变得可辨识，因为增加一个常数列可以稳定回归；因此只需将 $\\phi(\\mathbf{x})$ 扩充一个 $1$ 即可解决问题，而无需改变 $n$。**\n问题陈述中说有 $m=35$ 个基函数，这通常包括截距项（一个 $0$ 次多项式）。如果未包含，添加它会将参数数量增加到 $m=36$。这将使问题更加欠定（$n=30$，$m=36$），从而恶化而非解决可辨识性问题。添加一列无法使矩阵的秩超过 $n=30$。\n**结论：** 不正确。\n\n**C. 模型是可辨识的，因为一个 $3$ 次多项式足够灵活以捕捉电池的物理特性，所以可以无需修改直接进行普通最小二乘法。**\n所选函数形式（$3$ 次多项式）的物理相关性或灵活性与参数可辨识性的数学问题无关。可辨识性由设计矩阵 $X$ 的性质以及 $n$ 和 $m$ 之间的关系决定。无论模型从物理角度看多么合适，其 $m=35$ 个参数都无法仅从 $n=30$ 个观测值中使用 OLS 唯一估计出来。\n**结论：** 不正确。\n\n**D. 如果测量噪声是高斯的，模型就是可辨识的，因为高斯噪声意味着最大似然与最小二乘法一致，所以可以无需正则化进行拟合。**\n关于误差的高斯噪声假设为使用最小二乘代价函数提供了统计学上的理由，因为它等价于最大化数据的似然。然而，这个假设并不能改变底层的线性代数。如果由于 $mn$ 导致矩阵 $X^{\\top}X$ 是奇异的，那么似然函数将不会有唯一的最大值。相反，会存在一个参数向量 $\\boldsymbol{\\beta}$ 的子空间，它们都能达到相同的最大似然，因此估计量仍然不是唯一的。\n**结论：** 不正确。\n\n**E. 模型不可辨识仅仅是由于多项式特征之间的多重共线性；即使在 $mn$ 的情况下，仅对基进行正交化（例如，通过 Gram–Schmidt 方法）就能确保可辨识性。**\n当 $mn$ 时，$X$ 的 $m$ 个列向量位于一个 $n$ 维空间（或其子空间）中，因此它们保证是线性相关的。这是多重共线性的一种极端形式。通过像 Gram-Schmidt 这样的过程进行正交化可以为 $X$ 的列空间生成一个正交基，但这个空间的维度最多为 $n=30$。在 $30$ 维空间中找到 $35$ 个正交向量在数学上是不可能的。正交化并不能解决参数数量超过观测数量这个根本问题。\n**结论：** 不正确。",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "数值稳定性和多重共线性是多项式回归中普遍存在的挑战，它们会导致模型系数的估计值不稳定且方差过大。通过分析一系列诊断指标，如条件数、方差膨胀因子(VIF)和相关系数矩阵，我们可以精确地诊断出问题的根源。本练习提供了一组来自仿真流程的真实诊断数据，要求您判断模型病态是主要源于输入变量的尺度差异、基函数间的内在相关性，还是两者兼有，并据此提出一套系统的解决方案 。",
            "id": "3942011",
            "problem": "您正在一个自动化的电池设计和仿真工作流中，为一个锂离子电池的放电电阻构建一个多元多项式回归代理模型，该模型是设计和材料变量的函数。该代理模型是参数线性的，其基函数包括一个截距项以及输入变量 $x_1, x_2, x_3, x_4$ 中所有总次数最高为 $2$ 的单项式。其中，$x_1$ 是集电器厚度（单位：微米），$x_2$ 是电极孔隙率（无量纲），$x_3$ 是电解质离子电导率（单位：西门子/米），$x_4$ 是比表面积（单位：米⁻¹）。您有 $n$ 次仿真实验，其中 $n \\gg p$，$p$ 是设计矩阵 $X \\in \\mathbb{R}^{n \\times p}$ 的列数。从您的拟合流程中报告了以下观测结果：\n\n- $X^{\\top}X$ 的 $2$-范数条件数约为 $1.2 \\times 10^{16}$。\n- 在按列进行零均值中心化并将每列缩放到单位样本标准差（z-score标准化）后，新格拉姆矩阵的 $2$-范数条件数降至约 $1.8 \\times 10^{3}$。\n- z-score标准化后各列的经验相关矩阵显示 $|\\rho(x_1, x_1^2)| \\approx 0.98$，$|\\rho(x_1, x_1 x_4)| \\approx 0.97$ 和 $|\\rho(x_4, x_4^2)| \\approx 0.99$。\n- 通过将 z-score 标准化设计矩阵中的 $x_1$ 列对所有其他列进行回归计算，得到的 $x_1$ 列的方差膨胀因子约为 $220$；截距项的方差膨胀因子约为 $1$。\n- 重复一些设计点以获得重复实验（不改变唯一设计点的集合）会使 $X^{\\top}X$ 按一个整数因子缩放，但不会实质性地改变其条件数。\n\n假设数据是由一个从输入到真实响应的物理平滑映射生成的，并且数值精度是标准的双精度。基于线性回归和数值线性代数的基本原理，判断 $X^{\\top}X$ 的近奇异性主要是由多重共线性、不良缩放、两者兼有还是两者皆非引起的，并选择最合适的一组诊断和修复方法。\n\n哪个选项最合适？\n\nA) 问题仅仅是不良缩放。诊断应侧重于比较原始和 z-score 标准化后的条件数，修复方法是将输入重新缩放到可比较的量级。不需要进一步的更改。\n\nB) 不良缩放和多重共线性都存在。诊断应包括比较 z-score 标准化前后的条件数，检查标准化后各列的相关矩阵或奇异值谱，以及计算方差膨胀因子。修复方法应包括将输入映射到一个公共域（如 $[-1,1]$），进行中心化以减少多项式项之间的相关性，在标准化域上切换到正交多项式基，重新设计实验以减少列之间的对齐（例如，通过空间填充设计或带轴向点的响应面设计），以及在需要时，在求解正规方程时应用正则化或截断奇异值分解。\n\nC) 问题仅仅是在形成 $X^{\\top}X$ 时的数值舍入误差。诊断应强调用更高精度重新计算 $X^{\\top}X$。修复方法是使用扩展精度算法或对 $X^{\\top}X$ 进行 Cholesky 分解，而不改变设计或基。\n\nD) 问题是由于截距项列造成了共线性。修复方法是从基中移除截距项；不需要重新缩放或重新设计。\n\nE) 问题是样本量不足。修复方法是在现有的设计点上增加许多重复运行以降低条件数，而不修改基或输入缩放。",
            "solution": "首先验证问题陈述，以确保其科学上合理、提法恰当且客观。\n\n**第一步：提取已知信息**\n- **模型：** 多元多项式回归代理模型。\n- **响应：** 锂离子电池的放电电阻。\n- **输入：** $x_1$ (集电器厚度, $\\mu m$), $x_2$ (电极孔隙率, 无量纲), $x_3$ (电解质离子电导率, S/m), $x_4$ (比表面积, $m^{-1}$)。\n- **基：** 截距项加上 $x_1, x_2, x_3, x_4$ 中所有总次数最高为 $2$ 的单项式。这会产生 $p = 1 + 4 + \\binom{4}{2} + 4 = 1 + 4 + 6 + 4 = 15$ 个基函数（设计矩阵 $X$ 的列）。基函数为 $1$, $x_i$, $x_i^2$, 和 $x_i x_j$，其中 $i,j \\in \\{1,2,3,4\\}$ 且 $i \\leq j$。\n- **数据：** $n$ 次仿真实验，其中 $n \\gg p$。设计矩阵为 $X \\in \\mathbb{R}^{n \\times p}$。\n- **观测 1：** 格拉姆矩阵的 $2$-范数条件数 $\\kappa_2(X^{\\top}X)$ 约为 $1.2 \\times 10^{16}$。\n- **观测 2：** 对 $X$ 的列进行 z-score 标准化后，新格拉姆矩阵的条件数降至约 $1.8 \\times 10^3$。\n- **观测 3：** 对于 z-score 标准化后的列，经验相关性很高：$|\\rho(x_1, x_1^2)| \\approx 0.98$, $|\\rho(x_1, x_1 x_4)| \\approx 0.97$, 和 $|\\rho(x_4, x_4^2)| \\approx 0.99$。\n- **观测 4：** z-score 标准化设计矩阵中 $x_1$ 列的方差膨胀因子 (VIF) 约为 $220$。截距项的 VIF 约为 $1$。\n- **观测 5：** 重复设计点会使 $X^{\\top}X$ 按一个整数因子缩放，但不会实质性地改变其条件数。\n- **假设：** 数据由一个平滑的物理映射生成；使用标准双精度算法。\n\n**第二步：使用提取的已知信息进行验证**\n该问题具有科学依据，描述了工程建模中的一个常见任务。它提法恰当，提供了能够进行明确诊断的定量数据。语言客观而精确。所有提供的观测结果都与数值线性代数和回归诊断的既定原则相符。\n- 初始条件数 $\\kappa_2(X^{\\top}X) \\approx 1.2 \\times 10^{16}$ 与双精度的 $1/\\epsilon_m$（$\\epsilon_m \\approx 2.22 \\times 10^{-16}$）在同一数量级，表明存在数值奇异性。这与 $\\kappa_2(X^{\\top}X) = [\\kappa_2(X)]^2$ 一致。\n- 缩放后条件数的大幅下降是 $X$ 各列之间不良缩放的典型症状。\n- 缩放后残余的 $1.8 \\times 10^3$ 的条件数、预测变量之间的高相关性以及 $220$ 的 VIF 是严重多重共线性的典型症状。$220$ 的 VIF 意味着当该预测变量对所有其他预测变量进行回归时，其 $R^2$ 值为 $1 - 1/220 \\approx 0.995$。\n- 关于重复实验的观测在数学上是正确的：如果 $X_{new}$ 是由 $X$ 堆叠 $k$ 次形成的，那么 $X_{new}^{\\top}X_{new} = k X^{\\top}X$。条件数对于标量乘法是不变的，即 $\\kappa(cA) = \\kappa(A)$。\n因此，该问题是有效的。\n\n**推导与评估**\n目标是诊断 $X^{\\top}X$ 近奇异性的原因。我们分析所提供的证据。\n\n1.  **不良缩放：** 格拉姆矩阵的初始条件数是 $\\kappa_2(X^{\\top}X) \\approx 1.2 \\times 10^{16}$。在对设计矩阵 $X$ 的列进行 z-score 标准化（将其均值标准化为 $0$，标准差标准化为 $1$）后，条件数急剧下降到约 $1.8 \\times 10^3$。这种超过 $13$ 个数量级的显著改善是**不良缩放**是病态问题主要原因的明确证据。$X$ 的原始列，包括像 $x_1$ 和 $x_1^2$ 这样的项，很可能具有截然不同的量级和方差，导致了一个缩放不良的格拉姆矩阵。\n\n2.  **多重共线性：** 尽管通过缩放得到了显著改善，但新的条件数仍然是 $1.8 \\times 10^3$。虽然稳健的求解器可以处理这个值，但它远非理想值 $\\approx 1$，并表明缩放后矩阵的列仍然不接近正交。这指向一个遗留问题：**多重共线性**。\n    - 这一诊断由观测 3 直接证实：标准化矩阵中某些列之间的相关系数极高，例如 $|\\rho(x_1, x_1^2)| \\approx 0.98$ 和 $|\\rho(x_4, x_4^2)| \\approx 0.99$。这表明这些预测变量之间存在近乎完美的线性关系。如此高的相关性是多重共线性的一个标志，在多项式回归中很常见，尤其是在对输入变量取幂之前未将其中心化到零附近时。\n    - 观测 4 提供了进一步的证实。预测变量 $x_1$ 的方差膨胀因子 (VIF) 约为 $220$。第 $j$ 个预测变量的 VIF 是 $VIF_j = 1/(1-R_j^2)$，其中 $R_j^2$ 是将第 $j$ 个预测变量对所有其他预测变量进行回归得到的决定系数。$220$ 的 VIF 非常高（通常的经验法则是 VIF 高于 $5$ 或 $10$ 就有问题），这表明设计矩阵中的 $x_1$ 向量非常接近由其他列向量张成的子空间。\n    - 观测 5 正确地排除了仅通过重复实验来增加数据点数量就能解决问题的可能性，因为这不会改变 $X$ 列向量之间的几何关系。\n\n**结论：** 证据明确指向同时存在严重的**不良缩放**和严重的**多重共线性**。一个恰当的对策必须承认并针对这两个问题提出解决方案。\n\n**逐项分析**\n\n**A) 问题仅仅是不良缩放。诊断应侧重于比较原始和 z-score 标准化后的条件数，修复方法是将输入重新缩放到可比较的量级。不需要进一步的更改。**\n这个选项正确地识别了不良缩放，但错误地指出这是*唯一*的问题。缩放后 $1.8 \\times 10^3$ 的残余条件数、$220$ 的 VIF 以及高相关性都为多重共线性提供了强有力的证据。提议不作进一步更改是错误的。\n**结论：错误。**\n\n**B) 不良缩放和多重共线性都存在。诊断应包括比较 z-score 标准化前后的条件数，检查标准化后各列的相关矩阵或奇异值谱，以及计算方差膨胀因子。修复方法应包括将输入映射到一个公共域（如 $[-1,1]$），进行中心化以减少多项式项之间的相关性，在标准化域上切换到正交多项式基，重新设计实验以减少列之间的对齐（例如，通过空间填充设计或带轴向点的响应面设计），以及在需要时，在求解正规方程时应用正则化或截断奇异值分解。**\n这个选项正确地识别出不良缩放和多重共线性都是问题所在。它列出了一套全面且正确的诊断方法（条件数、相关性、VIF），与问题数据相符。它还提出了一整套最先进的修复方案，从根本上解决这两个问题：将输入映射到标准化域（如 $[-1,1]$）以进行缩放和初始中心化，使用正交多项式基（例如勒让德多项式）直接对抗多项式项相关性，改进实验设计以打破预测变量间的依赖关系，以及在无法改变设计时使用数值/统计补救措施，如正则化（岭回归/Lasso）或截断奇异值分解（TSVD）。这是一个完整且正确的评估和行动计划。\n**结论：正确。**\n\n**C) 问题仅仅是在形成 $X^{\\top}X$ 时的数值舍入误差。诊断应强调用更高精度重新计算 $X^{\\top}X$。修复方法是使用扩展精度算法或对 $X^{\\top}X$ 进行 Cholesky 分解，而不改变设计或基。**\n极端的病态确实会导致数值舍入误差在标准精度计算中占主导地位。然而，舍入误差是一个症状，而不是根本原因。根本原因是不良缩放和多重共线性。使用更高精度只会更准确地计算出一个不稳定的解；它无法解决由多重共线性引起的估计系数高方差的统计问题。对于这样一个病态矩阵，在标准精度下，Cholesky 分解这一标准数值方法将会失败或不准确。这个选项本末倒置，将症状当成了病因。\n**结论：错误。**\n\n**D) 问题是由于截距项列造成了共线性。修复方法是从基中移除截距项；不需要重新缩放或重新设计。**\n这在事实上是错误的。观测 4 指出截距项的 VIF 约为 $1$，表明它与其他（中心化后的）预测变量是正交的。共线性存在于非截距项的多项式项之间。移除截距项通常是错误的，并且可能会引入偏差。\n**结论：错误。**\n\n**E) 问题是样本量不足。修复方法是在现有的设计点上增加许多重复运行以降低条件数，而不修改基或输入缩放。**\n这与观测 5 明确矛盾，观测 5 正确地指出增加重复实验并不会实质性地改变条件数。问题在于设计点的选择（它们在输入空间中的位置），而不是它们的数量。陈述 $n \\gg p$ 也表明样本量不是限制因素。\n**结论：错误。**",
            "answer": "$$\\boxed{B}$$"
        },
        {
            "introduction": "代理模型不仅是静态的函数近似，更是自动化工作流程中的强大工具。主动学习便是一个典型应用，其中代理模型自身会引导对新数据点的探索，从而高效地提升模型精度。这项编程练习将之前的所有概念融会贯通，您将实现一个完整的主动学习循环，包括拟合模型、估计不确定性与梯度、并据此在复杂的设计约束下选择下一个最优的实验点 。",
            "id": "3941922",
            "problem": "您正在设计一个主动学习程序，以改进一个无量纲电池设计响应的多项式回归代理模型，同时遵守可行性约束。设计向量为 $\\mathbf{x} = (L,\\varepsilon)$，其中 $L$ 是归一化的电极厚度，$\\varepsilon$ 是孔隙率分数，两者均为无量纲。真实响应是一个未知的标量函数 $f(L,\\varepsilon)$，您的仿真仅使用它来生成初始训练数据。代理模型是一个总次数为 $2$ 的多项式回归 $\\hat{f}(L,\\varepsilon)$，通过岭正则化最小二乘法拟合。您的主动学习规则必须通过最大化一个平衡了模型不确定性和代理模型局部灵敏度的采集函数，在一个受约束的可行集内选择下一个样本点 $\\mathbf{x}^\\star$，同时严格遵守可行性。\n\n使用的基础理论：\n- 普通最小二乘（OLS）回归最小化残差平方和，从而得出求解正规方程组的系数。\n- 岭回归（Tikhonov 正则化的一种形式）通过一个非零惩罚项来增强正规方程组，在设计矩阵是病态时提高数值稳定性。\n- Gauss–Markov 定理给出了线性模型下预测方差的结构，该方差与设计矩阵引起的杠杆率成正比。\n- 多项式代理模型的梯度可以通过对基函数求导得出。\n\n您的程序必须为每个测试用例实现以下步骤：\n1. 构建一个与总次数为 $2$ 的多项式相对应的特征映射 $\\boldsymbol{\\phi}(L,\\varepsilon)$：\n   $$\\boldsymbol{\\phi}(L,\\varepsilon) = [\\,1,\\;L,\\;\\varepsilon,\\;L^2,\\;L\\varepsilon,\\;\\varepsilon^2\\,]^\\top.$$\n2. 给定训练数据 $\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^n$，通过以下方式拟合岭正则化系数 $\\boldsymbol{\\beta} \\in \\mathbb{R}^6$：\n   $$\\boldsymbol{\\beta} = \\arg\\min_{\\boldsymbol{\\beta}} \\sum_{i=1}^n \\left(y_i - \\boldsymbol{\\phi}(\\mathbf{x}_i)^\\top \\boldsymbol{\\beta}\\right)^2 + \\lambda \\|\\boldsymbol{\\beta}\\|_2^2,$$\n   其中岭参数 $\\lambda = 10^{-6}$，通过以下线性系统实现：\n   $$\\left(\\mathbf{X}^\\top \\mathbf{X} + \\lambda \\mathbf{I}\\right) \\boldsymbol{\\beta} = \\mathbf{X}^\\top \\mathbf{y},$$\n   其中 $\\mathbf{X}$ 是将 $\\boldsymbol{\\phi}(\\mathbf{x}_i)^\\top$ 按行堆叠而成，$\\mathbf{y}$ 是将 $y_i$ 堆叠而成。\n3. 估计残差方差为：\n   $$\\hat{\\sigma}^2 = \\frac{\\|\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}\\|_2^2}{\\max(n-p,1)},$$\n   其中 $p=6$ 为系数数量。使用岭调整后的逆矩阵：\n   $$\\mathbf{A}^{-1} = \\left(\\mathbf{X}^\\top \\mathbf{X} + \\lambda \\mathbf{I}\\right)^{-1}.$$\n4. 对于一个候选点 $\\mathbf{x}$，计算不确定性代理：\n   $$s^2(\\mathbf{x}) = \\hat{\\sigma}^2\\, \\boldsymbol{\\phi}(\\mathbf{x})^\\top \\mathbf{A}^{-1} \\boldsymbol{\\phi}(\\mathbf{x}), \\quad s(\\mathbf{x}) = \\sqrt{s^2(\\mathbf{x})},$$\n   并计算代理模型的梯度：\n   $$\\nabla \\hat{f}(L,\\varepsilon) = \\left[\\;\\frac{\\partial \\hat{f}}{\\partial L},\\; \\frac{\\partial \\hat{f}}{\\partial \\varepsilon}\\;\\right]^\\top,$$\n   其中\n   $$\\frac{\\partial \\hat{f}}{\\partial L} = \\beta_1 + 2\\beta_3 L + \\beta_4 \\varepsilon,\\qquad \\frac{\\partial \\hat{f}}{\\partial \\varepsilon} = \\beta_2 + \\beta_4 L + 2\\beta_5 \\varepsilon.$$\n5. 为可调标量 $\\alpha$ 定义采集函数为：\n   $$A(\\mathbf{x}) = s(\\mathbf{x})\\left(1 + \\alpha \\left\\|\\nabla \\hat{f}(\\mathbf{x})\\right\\|_2\\right).$$\n6. 施加可行性约束。可行集是箱式约束和耦合约束的交集：\n   $$\\mathcal{F} = \\left\\{(L,\\varepsilon): L_{\\min} \\le L \\le L_{\\max},\\; \\varepsilon_{\\min} \\le \\varepsilon \\le \\varepsilon_{\\max},\\; \\underline{c} \\le L\\varepsilon \\le \\overline{c}\\right\\}.$$\n7. 在箱式边界上的均匀候选网格上，按可行性进行筛选并选择：\n   $$\\mathbf{x}^\\star = \\arg\\max_{\\mathbf{x} \\in \\mathcal{F}} A(\\mathbf{x}).$$\n\n真实响应的仿真（仅用于生成初始训练输出 $y_i$）：\n$$f(L,\\varepsilon) = e^{-L}\\left(1 + 0.2\\,L\\,\\varepsilon\\right) + \\sqrt{\\varepsilon} - 0.6\\,L - 0.15\\,\\varepsilon^2 + 0.05\\,L\\,\\sin(6\\,\\varepsilon).$$\n\n测试套件：\n- 案例 1（一般情况）：\n  - 边界：$L \\in [0.8,1.4]$，$\\varepsilon \\in [0.5,0.7]$，耦合 $L\\varepsilon \\in [0.5,0.9]$。\n  - 训练输入：$(1.0,0.5)$，$(1.2,0.5)$，$(1.0,0.7)$，$(1.3,0.6)$，$(0.8,0.7)$，$(1.4,0.5)$。\n  - 采集参数：$\\alpha = 0.25$。\n  - 网格分辨率：每个维度 $51 \\times 51$。\n- 案例 2（窄耦合带）：\n  - 边界：$L \\in [0.9,1.2]$，$\\varepsilon \\in [0.6,0.8]$，耦合 $L\\varepsilon \\in [0.72,0.75]$。\n  - 训练输入：$(0.9,0.8)$，$(1.0,0.72)$，$(1.2,0.6)$，$(1.1,0.68)$。\n  - 采集参数：$\\alpha = 0.25$。\n  - 网格分辨率：$51 \\times 51$。\n- 案例 3（病态设计，需要岭回归）：\n  - 边界：$L \\in [0.9,1.1]$，$\\varepsilon \\in [0.5,0.7]$，耦合 $L\\varepsilon \\in [0.5,0.77]$。\n  - 训练输入：$(1.0,0.5)$，$(1.0,0.55)$，$(1.0,0.6)$，$(1.0,0.65)$。\n  - 采集参数：$\\alpha = 0.25$。\n  - 网格分辨率：$51 \\times 51$。\n\n您的程序必须：\n- 为每个案例实现上述过程。\n- 将每个案例选择的下一个样本点 $\\mathbf{x}^\\star$ 作为列表的列表，在单行中输出。\n- 将每个坐标四舍五入到 4 位小数。\n\n最终输出格式：\n您的程序应生成单行输出，该输出包含一个由方括号括起来的逗号分隔列表，其中每个元素是一个双元素列表 $[L^\\star,\\varepsilon^\\star]$，对应于一个案例所选择的下一个样本点（例如，`[[L_1^\\star,\\varepsilon_1^\\star],[L_2^\\star,\\varepsilon_2^\\star],[L_3^\\star,\\varepsilon_3^\\star]]`）。每个数值项必须四舍五入到 4 位小数并且是无量纲的。",
            "solution": "用户提供的问题已经过验证，被认为是科学基础扎实、适定、客观且自洽的。所概述的程序是基于代理模型的优化和主动学习中的一种标准方法。我现在将提供一个完整的、有理有据的解决方案。\n\n该过程分解为以下步骤：\n\n**步骤1：数据生成与模型定义**\n该过程从一组 $n$ 个初始训练点 $\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^n$ 开始。输入的设计向量 $\\mathbf{x}_i = (L_i, \\varepsilon_i)$ 是给定的。相应的输出 $y_i$ 是通过评估一个给定的“真实”函数 $f(L, \\varepsilon)$ 生成的：\n$$f(L,\\varepsilon) = e^{-L}\\left(1 + 0.2\\,L\\,\\varepsilon\\right) + \\sqrt{\\varepsilon} - 0.6\\,L - 0.15\\,\\varepsilon^2 + 0.05\\,L\\,\\sin(6\\,\\varepsilon)$$\n代理模型 $\\hat{f}(L,\\varepsilon)$ 是一个总次数为 2 的多项式。它在某点 $\\mathbf{x}=(L,\\varepsilon)$ 的值是基函数的线性组合，表示为 $\\hat{f}(\\mathbf{x}) = \\boldsymbol{\\phi}(\\mathbf{x})^\\top \\boldsymbol{\\beta}$。特征映射 $\\boldsymbol{\\phi}(\\mathbf{x})$ 和系数向量 $\\boldsymbol{\\beta}$ 定义如下：\n$$\\boldsymbol{\\phi}(L,\\varepsilon) = [\\,1,\\;L,\\;\\varepsilon,\\;L^2,\\;L\\varepsilon,\\;\\varepsilon^2\\,]^\\top \\in \\mathbb{R}^6$$\n$$\\boldsymbol{\\beta} = [\\,\\beta_0,\\;\\beta_1,\\;\\beta_2,\\;\\beta_3,\\;\\beta_4,\\;\\beta_5\\,]^\\top \\in \\mathbb{R}^6$$\n\n**步骤2：岭正则化回归**\n系数向量 $\\boldsymbol{\\beta}$ 是通过使用岭正则化最小二乘法将代理模型拟合到训练数据来确定的。这涉及到解决以下最小化问题：\n$$\\boldsymbol{\\beta} = \\arg\\min_{\\boldsymbol{\\beta}} \\sum_{i=1}^n \\left(y_i - \\boldsymbol{\\phi}(\\mathbf{x}_i)^\\top \\boldsymbol{\\beta}\\right)^2 + \\lambda \\|\\boldsymbol{\\beta}\\|_2^2$$\n岭参数 $\\lambda$ 给定为 $10^{-6}$。这个正则化项惩罚大的系数值，这对数值稳定性至关重要，尤其是在设计矩阵是病态的（即当训练点共线或几乎共线时）。这个最小化问题的解是通过求解称为正则化正规方程组的线性系统找到的：\n$$(\\mathbf{X}^\\top \\mathbf{X} + \\lambda \\mathbf{I}) \\boldsymbol{\\beta} = \\mathbf{X}^\\top \\mathbf{y}$$\n在这里，$\\mathbf{X}$ 是 $n \\times p$ 的设计矩阵，其中每一行是一个特征向量 $\\boldsymbol{\\phi}(\\mathbf{x}_i)^\\top$，$p=6$ 是特征的数量，$\\mathbf{y}$ 是训练输出的 $n \\times 1$ 向量，$\\mathbf{I}$ 是 $p \\times p$ 的单位矩阵。\n\n**步骤3：不确定性量化**\n对代理模型预测的不确定性进行估计。首先，从模型拟合中计算残差方差 $\\hat{\\sigma}^2$：\n$$\\hat{\\sigma}^2 = \\frac{\\|\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}\\|_2^2}{\\max(n-p,1)}$$\n分母 $\\max(n-p,1)$ 是一个稳健的自由度调整，当训练点数 $n$ 小于或等于参数数量 $p$ 时，可以避免除以零或负数。\n对于一个新的候选点 $\\mathbf{x}$，不确定性代理表示为 $s^2(\\mathbf{x})$，它是一种基于普通最小二乘设置下预测的统计方差，并为岭回归进行了调整的启发式方法：\n$$s^2(\\mathbf{x}) = \\hat{\\sigma}^2\\, \\boldsymbol{\\phi}(\\mathbf{x})^\\top \\mathbf{A}^{-1} \\boldsymbol{\\phi}(\\mathbf{x})$$\n其中 $\\mathbf{A}^{-1} = (\\mathbf{X}^\\top \\mathbf{X} + \\lambda \\mathbf{I})^{-1}$。$\\boldsymbol{\\phi}(\\mathbf{x})^\\top \\mathbf{A}^{-1} \\boldsymbol{\\phi}(\\mathbf{x})$ 项被称为点 $\\mathbf{x}$ 的杠杆率，对于远离训练数据中心的点，其值更高。那么不确定性就是 $s(\\mathbf{x}) = \\sqrt{s^2(\\mathbf{x})}$。\n\n**步骤4：通过梯度进行灵敏度分析**\n代理模型的局部灵敏度通过其梯度的范数 $\\left\\|\\nabla \\hat{f}(\\mathbf{x})\\right\\|_2$ 来量化。梯度分量是通过对 $\\hat{f}(\\mathbf{x}) = \\sum_{j=0}^{5} \\beta_j \\phi_j(\\mathbf{x})$ 关于 $L$ 和 $\\varepsilon$ 求导得出的：\n$$\\frac{\\partial \\hat{f}}{\\partial L} = \\beta_1 + 2\\beta_3 L + \\beta_4 \\varepsilon$$\n$$\\frac{\\partial \\hat{f}}{\\partial \\varepsilon} = \\beta_2 + \\beta_4 L + 2\\beta_5 \\varepsilon$$\n梯度的欧几里得范数平方则为 $\\left\\|\\nabla \\hat{f}(\\mathbf{x})\\right\\|_2^2 = (\\frac{\\partial \\hat{f}}{\\partial L})^2 + (\\frac{\\partial \\hat{f}}{\\partial \\varepsilon})^2$。\n\n**步骤5：采集函数**\n采集函数 $A(\\mathbf{x})$ 结合了不确定性和灵敏度度量，以指导寻找下一个样本点。它定义为：\n$$A(\\mathbf{x}) = s(\\mathbf{x})\\left(1 + \\alpha \\left\\|\\nabla \\hat{f}(\\mathbf{x})\\right\\|_2\\right)$$\n其中 $\\alpha=0.25$ 是一个可调参数。最大化此函数有利于选择位于模型高不确定性区域（探索）或模型响应快速变化区域（利用）的点，或两者兼备。\n\n**步骤6：通过网格搜索进行约束优化**\n最优的下一个样本点 $\\mathbf{x}^\\star$ 是在指定可行集 $\\mathcal{F}$ 内使采集函数最大化的点。可行集由箱式约束和耦合约束的交集定义：\n$$\\mathcal{F} = \\left\\{(L,\\varepsilon): L_{\\min} \\le L \\le L_{\\max},\\; \\varepsilon_{\\min} \\le \\varepsilon \\le \\varepsilon_{\\max},\\; \\underline{c} \\le L\\varepsilon \\le \\overline{c}\\right\\}$$\n优化问题是：\n$$\\mathbf{x}^\\star = \\arg\\max_{\\mathbf{x} \\in \\mathcal{F}} A(\\mathbf{x})$$\n这通过在候选点的离散网格上执行搜索来计算求解。在箱式约束 $[L_{\\min}, L_{\\max}] \\times [\\varepsilon_{\\min}, \\varepsilon_{\\max}]$ 上生成一个均匀的 $51 \\times 51$ 网格。在每个网格点上评估采集函数 $A(\\mathbf{x})$。不满足耦合约束 $ \\underline{c} \\le L\\varepsilon \\le \\overline{c}$ 的点被丢弃。在可行候选点中产生最高采集值的网格点被选为 $\\mathbf{x}^\\star$。对所提供的三个测试用例中的每一个都重复这整个过程。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements an active learning procedure to select the next sample point for improving\n    a polynomial regression surrogate model of a battery design response.\n    \"\"\"\n\n    # True response function (used only to generate initial training data)\n    def f_true(L, eps):\n        return (np.exp(-L) * (1 + 0.2 * L * eps) + np.sqrt(eps)\n                - 0.6 * L - 0.15 * eps**2 + 0.05 * L * np.sin(6 * eps))\n\n    # Feature map for total-degree-2 polynomial\n    def phi(L, eps):\n        return np.array([1, L, eps, L**2, L * eps, eps**2])\n\n    test_cases = [\n        {\n            \"name\": \"Case 1 (general)\",\n            \"L_bounds\": [0.8, 1.4],\n            \"eps_bounds\": [0.5, 0.7],\n            \"coupling_bounds\": [0.5, 0.9],\n            \"training_inputs\": [(1.0, 0.5), (1.2, 0.5), (1.0, 0.7), (1.3, 0.6), (0.8, 0.7), (1.4, 0.5)],\n            \"alpha\": 0.25,\n            \"grid_res\": 51,\n        },\n        {\n            \"name\": \"Case 2 (narrow coupling band)\",\n            \"L_bounds\": [0.9, 1.2],\n            \"eps_bounds\": [0.6, 0.8],\n            \"coupling_bounds\": [0.72, 0.75],\n            \"training_inputs\": [(0.9, 0.8), (1.0, 0.72), (1.2, 0.6), (1.1, 0.68)],\n            \"alpha\": 0.25,\n            \"grid_res\": 51,\n        },\n        {\n            \"name\": \"Case 3 (ill-conditioned design)\",\n            \"L_bounds\": [0.9, 1.1],\n            \"eps_bounds\": [0.5, 0.7],\n            \"coupling_bounds\": [0.5, 0.77],\n            \"training_inputs\": [(1.0, 0.5), (1.0, 0.55), (1.0, 0.6), (1.0, 0.65)],\n            \"alpha\": 0.25,\n            \"grid_res\": 51,\n        },\n    ]\n\n    final_results = []\n    \n    for case in test_cases:\n        # ===== Step 1: Generate training data =====\n        train_x = np.array(case['training_inputs'])\n        train_y = np.array([f_true(L, eps) for L, eps in train_x])\n        \n        # ===== Step 2: Fit ridge regression model =====\n        lambda_reg = 1e-6\n        p = 6  # Number of polynomial features\n        X = np.array([phi(L, eps) for L, eps in train_x])\n        n = X.shape[0]\n        \n        I = np.identity(p)\n        A = X.T @ X + lambda_reg * I\n        b = X.T @ train_y\n        beta = np.linalg.solve(A, b)\n\n        # ===== Step 3: Estimate residual variance =====\n        y_hat = X @ beta\n        residuals = train_y - y_hat\n        ssr = residuals.T @ residuals\n        sigma_sq = ssr / max(n - p, 1)\n\n        # Pre-compute the inverse of the regularized matrix for efficiency\n        A_inv = np.linalg.inv(A)\n\n        # ===== Step 6  7: Grid search over feasible set =====\n        L_vals = np.linspace(case['L_bounds'][0], case['L_bounds'][1], case['grid_res'])\n        eps_vals = np.linspace(case['eps_bounds'][0], case['eps_bounds'][1], case['grid_res'])\n        L_grid, Eps_grid = np.meshgrid(L_vals, eps_vals)\n\n        # Flatten grid for vectorized processing\n        L_flat = L_grid.flatten()\n        Eps_flat = Eps_grid.flatten()\n\n        # Filter by feasibility coupling constraint\n        LE_prod = L_flat * Eps_flat\n        c_min, c_max = case['coupling_bounds']\n        feasible_mask = (LE_prod = c_min)  (LE_prod = c_max)\n        \n        L_feasible = L_flat[feasible_mask]\n        Eps_feasible = Eps_flat[feasible_mask]\n\n        if L_feasible.size == 0:\n            # Handle the case where no grid points are feasible (not expected for these test cases)\n            final_results.append([np.nan, np.nan])\n            continue\n            \n        # Vectorized calculation of acquisition function over feasible points\n        \n        # 1. Feature matrix for all feasible points\n        phi_feasible = np.vstack([\n            np.ones_like(L_feasible),\n            L_feasible,\n            Eps_feasible,\n            L_feasible**2,\n            L_feasible * Eps_feasible,\n            Eps_feasible**2\n        ]).T\n        \n        # ===== Step 4: Compute uncertainty s(x) =====\n        s_sq_vals = sigma_sq * np.sum((phi_feasible @ A_inv) * phi_feasible, axis=1)\n        s_vals = np.sqrt(s_sq_vals)\n\n        # ===== Step 4 (cont.): Compute gradient norm ||grad f_hat(x)|| =====\n        grad_L = beta[1] + 2 * beta[3] * L_feasible + beta[4] * Eps_feasible\n        grad_Eps = beta[2] + beta[4] * L_feasible + 2 * beta[5] * Eps_feasible\n        grad_norm_vals = np.sqrt(grad_L**2 + grad_Eps**2)\n        \n        # ===== Step 5: Compute acquisition function A(x) =====\n        alpha = case['alpha']\n        acq_vals = s_vals * (1 + alpha * grad_norm_vals)\n\n        # Find the point that maximizes the acquisition function\n        max_idx = np.argmax(acq_vals)\n        x_star = [L_feasible[max_idx], Eps_feasible[max_idx]]\n        \n        # Round to 4 decimal places and append\n        final_results.append([round(c, 4) for c in x_star])\n\n    # Format the final output string\n    # e.g., [[val1,val2],[val3,val4]]\n    # Using str().replace() is a simple way to achieve the required no-space format.\n    print(str(final_results).replace(\" \", \"\"))\n\nsolve()\n```"
        }
    ]
}