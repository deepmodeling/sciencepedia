{
    "hands_on_practices": [
        {
            "introduction": "A virtual prototype is only as reliable as its inputs. This exercise grounds our exploration of error quantification in a fundamental task: sensor calibration. We will see how to statistically model the relationship between a sensor's measurement and the true physical quantity, and more importantly, how to propagate the uncertainty from this calibration into a physics-based model for heat generation . This practice demonstrates that correctly accounting for parameter covariance, which naturally arises during fitting, is crucial for obtaining an accurate estimate of a model's predictive uncertainty.",
            "id": "3959862",
            "problem": "A Lithium-ion pouch cell for virtual prototyping is instrumented with an embedded thermocouple whose raw readings are processed into a measured temperature $T_{\\mathrm{meas}}$ in kelvin. To validate the virtual prototype and quantify error, the thermocouple is calibrated against a Platinum Resistance Thermometer (PRT) reference measurement $T_{\\mathrm{true}}$ across a range of chamber setpoints. Assume that the PRT readings have independent Gaussian uncertainties with known standard deviations, and that $T_{\\mathrm{meas}}$ can be treated as the independent variable with negligible uncertainty for calibration.\n\nYou are given calibration data for one thermocouple:\n- Measurement pairs $(T_{\\mathrm{meas},i}, T_{\\mathrm{true},i})$ at five setpoints: $(298.0, 298.10)$, $(308.0, 308.12)$, $(318.0, 318.14)$, $(328.0, 328.16)$, $(338.0, 338.18)$, all in kelvin.\n- Each reference value $T_{\\mathrm{true},i}$ has a standard uncertainty $\\sigma_i = 0.10$ kelvin.\n\nModel the calibration as a linear relation $T_{\\mathrm{true}} = a + b\\,T_{\\mathrm{meas}}$ and estimate the parameters $a$ and $b$ using a statistically sound procedure that begins from the Gaussian error model and yields the maximum-likelihood linear fit. Then, derive the parameter covariance matrix for $(a,b)$ using first principles.\n\nIn a separate discharge experiment used for validation and error quantification of the virtual prototype, at a specific time $t^\\star$ the measured quantities are:\n- $T_{\\mathrm{meas}}(t^\\star) = 320.0$ kelvin,\n- $\\frac{d T_{\\mathrm{meas}}}{dt}(t^\\star) = 0.020$ kelvin per second.\n\nAssume a lumped energy balance for the cell-plus-fixture node based on the First Law of Thermodynamics and Newtonian convection,\n$$ q = m\\,c\\,\\frac{d T_{\\mathrm{true}}}{dt} + h\\,A\\,\\left(T_{\\mathrm{true}} - T_{\\mathrm{amb}}\\right), $$\nwhere $m = 0.050$ kilograms, $c = 1100$ joules per kilogram-kelvin, $h\\,A = 2.5$ watts per kelvin, and $T_{\\mathrm{amb}} = 298.0$ kelvin. Treat $m$, $c$, $h\\,A$, $T_{\\mathrm{amb}}$, $T_{\\mathrm{meas}}(t^\\star)$, and $\\frac{d T_{\\mathrm{meas}}}{dt}(t^\\star)$ as known exactly. Use the calibrated relation $T_{\\mathrm{true}} = a + b\\,T_{\\mathrm{meas}}$ and $\\frac{d T_{\\mathrm{true}}}{dt} = b\\,\\frac{d T_{\\mathrm{meas}}}{dt}$.\n\nPropagate the calibration parameter uncertainties to obtain the standard uncertainty of the heat generation rate $q$ at time $t^\\star$ by linear (first-order) uncertainty propagation, fully accounting for the covariance between $a$ and $b$ arising from the fit.\n\nRound your final answer for the standard uncertainty in $q$ to four significant figures. Express the final answer in watts.",
            "solution": "The user wants me to solve a multi-part physics and statistics problem.\n\nHere's the plan:\n1.  **Validate the Problem**:\n    *   Extract all given data and relationships.\n    *   Check for scientific soundness, well-posedness, completeness, and objectivity.\n    *   The problem involves linear regression (maximum likelihood estimation for Gaussian errors), uncertainty propagation, and a simple thermodynamic model. These are all standard, well-established concepts. The data points seem reasonable. The assumptions are clearly stated (e.g., $T_{\\text{meas}}$ has negligible uncertainty, PRT uncertainties are Gaussian and known). The problem seems valid.\n\n2.  **Part 1: Linear Regression (Maximum Likelihood Estimation)**\n    *   The model is $T_{\\text{true},i} = a + b\\,T_{\\text{meas},i}$.\n    *   The measurements are $T_{\\text{true},i}$ with uncertainty $\\sigma_i$.\n    *   The likelihood function for Gaussian errors is proportional to $\\exp(-\\frac{1}{2}\\chi^2)$, where $\\chi^2 = \\sum_{i=1}^{N} \\left(\\frac{T_{\\text{true},i} - (a + b\\,T_{\\text{meas},i})}{\\sigma_i}\\right)^2$.\n    *   Maximizing the likelihood is equivalent to minimizing $\\chi^2$.\n    *   This is a weighted least squares problem.\n    *   Let $x_i = T_{\\text{meas},i}$, $y_i = T_{\\text{true},i}$, and $w_i = 1/\\sigma_i^2$.\n    *   The $\\chi^2$ function to minimize is $\\chi^2(a, b) = \\sum_{i=1}^{N} w_i (y_i - a - b x_i)^2$.\n    *   To find the minimum, I need to set the partial derivatives with respect to $a$ and $b$ to zero.\n        *   $\\frac{\\partial \\chi^2}{\\partial a} = \\sum_{i=1}^{N} w_i (-2)(y_i - a - b x_i) = 0$\n        *   $\\frac{\\partial \\chi^2}{\\partial b} = \\sum_{i=1}^{N} w_i (-2x_i)(y_i - a - b x_i) = 0$\n    *   This leads to a system of two linear equations for $a$ and $b$:\n        *   $a \\sum w_i + b \\sum w_i x_i = \\sum w_i y_i$\n        *   $a \\sum w_i x_i + b \\sum w_i x_i^2 = \\sum w_i x_i y_i$\n    *   In this specific problem, all $\\sigma_i = 0.10$ K, so all weights $w_i = 1/(0.10)^2 = 100$ are the same. Let $w = w_i$.\n    *   The equations simplify:\n        *   $a N w + b w \\sum x_i = w \\sum y_i \\implies aN + b \\sum x_i = \\sum y_i$\n        *   $a w \\sum x_i + b w \\sum x_i^2 = w \\sum x_i y_i \\implies a \\sum x_i + b \\sum x_i^2 = \\sum x_i y_i$\n    *   These are the standard normal equations for unweighted linear regression, which is correct since all weights are equal.\n    *   Data:\n        *   $x_i$: $298.0, 308.0, 318.0, 328.0, 338.0$\n        *   $y_i$: $298.10, 308.12, 318.14, 328.16, 338.18$\n        *   $N=5$\n    *   Calculations needed:\n        *   $\\sum x_i = 298.0 + 308.0 + 318.0 + 328.0 + 338.0 = 1590.0$\n        *   $\\sum y_i = 298.10 + 308.12 + 318.14 + 328.16 + 338.18 = 1590.70$\n        *   $\\sum x_i^2 = 298^2 + 308^2 + 318^2 + 328^2 + 338^2 = 88804 + 94864 + 101124 + 107584 + 114244 = 506620$\n        *   $\\sum x_i y_i = (298.0 \\times 298.10) + (308.0 \\times 308.12) + (318.0 \\times 318.14) + (328.0 \\times 328.16) + (338.0 \\times 338.18)$\n            *   $88833.8$\n            *   $94901.36$\n            *   $101168.52$\n            *   $107632.48$\n            *   $114304.84$\n            *   $\\sum x_i y_i = 506841.0$\n    *   System of equations:\n        *   $5a + 1590b = 1590.70$\n        *   $1590a + 506620b = 506841.0$\n    *   Solve for $a$ and $b$. From the first equation: $a = \\frac{1590.70 - 1590b}{5} = 318.14 - 318b$.\n    *   Substitute into the second equation:\n        *   $1590(318.14 - 318b) + 506620b = 506841.0$\n        *   $505842.6 - 505620b + 506620b = 506841.0$\n        *   $1000b = 506841.0 - 505842.6 = 998.4$\n        *   $b = 0.9984$\n    *   Now find $a$:\n        *   $a = 318.14 - 318(0.9984) = 318.14 - 317.4912 = 0.6488$\n    *   So, the fit is $T_{\\text{true}} = 0.6488 + 0.9984 \\, T_{\\text{meas}}$.\n\n3.  **Part 2: Covariance Matrix**\n    *   The general formula for the covariance matrix of the parameters $(a,b)$ in a weighted linear least squares fit is $(\\mathbf{X}^T \\mathbf{W} \\mathbf{X})^{-1}$, where $\\mathbf{X}$ is the design matrix and $\\mathbf{W}$ is the weight matrix.\n    *   The model is $y = a \\cdot 1 + b \\cdot x$.\n    *   The design matrix $\\mathbf{X}$ is:\n        $$ \\mathbf{X} = \\begin{pmatrix} 1 & x_1 \\\\ 1 & x_2 \\\\ \\vdots & \\vdots \\\\ 1 & x_N \\end{pmatrix} $$\n    *   The weight matrix $\\mathbf{W}$ is a diagonal matrix with entries $w_i = 1/\\sigma_i^2$.\n        $$ \\mathbf{W} = \\text{diag}(w_1, w_2, \\dots, w_N) $$\n    *   In our case, $w_i = w = 1/(0.1)^2 = 100$ for all $i$. So $\\mathbf{W} = w \\mathbf{I}$.\n    *   $\\mathbf{X}^T \\mathbf{W} \\mathbf{X} = w \\mathbf{X}^T \\mathbf{X}$. Wait, this is not quite right.\n    *   Let's re-derive from the Hessian of $\\chi^2$. The covariance matrix is $2 \\mathbf{H}^{-1}$, where $\\mathbf{H}$ is the Hessian matrix of $\\chi^2$.\n        *   $\\frac{\\partial^2 \\chi^2}{\\partial a^2} = \\sum w_i (2) = 2 \\sum w_i$\n        *   $\\frac{\\partial^2 \\chi^2}{\\partial b^2} = \\sum w_i (2x_i^2) = 2 \\sum w_i x_i^2$\n        *   $\\frac{\\partial^2 \\chi^2}{\\partial a \\partial b} = \\sum w_i (2x_i) = 2 \\sum w_i x_i$\n    *   So the Hessian is:\n        $$ \\mathbf{H} = 2 \\begin{pmatrix} \\sum w_i & \\sum w_i x_i \\\\ \\sum w_i x_i & \\sum w_i x_i^2 \\end{pmatrix} $$\n    *   The covariance matrix is $\\mathbf{C} = 2 \\mathbf{H}^{-1}$.\n        $$ \\mathbf{C} = \\begin{pmatrix} \\sum w_i & \\sum w_i x_i \\\\ \\sum w_i x_i & \\sum w_i x_i^2 \\end{pmatrix}^{-1} $$\n    *   Let's check this. The standard formula for the covariance matrix in weighted least squares is indeed $(\\mathbf{X}^T \\mathbf{W} \\mathbf{X})^{-1}$.\n        *   $\\mathbf{X}^T = \\begin{pmatrix} 1 & 1 & \\dots & 1 \\\\ x_1 & x_2 & \\dots & x_N \\end{pmatrix}$\n        *   $\\mathbf{X}^T \\mathbf{W} = \\begin{pmatrix} w_1 & w_2 & \\dots & w_N \\\\ w_1 x_1 & w_2 x_2 & \\dots & w_N x_N \\end{pmatrix}$\n        *   $\\mathbf{X}^T \\mathbf{W} \\mathbf{X} = \\begin{pmatrix} \\sum w_i & \\sum w_i x_i \\\\ \\sum w_i x_i & \\sum w_i x_i^2 \\end{pmatrix}$\n    *   This confirms my formula for the matrix to be inverted.\n    *   Since all $w_i = w = 100$:\n        *   $\\sum w_i = N w = 5 \\times 100 = 500$\n        *   $\\sum w_i x_i = w \\sum x_i = 100 \\times 1590 = 159000$\n        *   $\\sum w_i x_i^2 = w \\sum x_i^2 = 100 \\times 506620 = 50662000$\n    *   The matrix to invert is:\n        $$ \\mathbf{M} = \\begin{pmatrix} 500 & 159000 \\\\ 159000 & 50662000 \\end{pmatrix} $$\n    *   The inverse of a $2 \\times 2$ matrix $\\begin{pmatrix} A & B \\\\ C & D \\end{pmatrix}$ is $\\frac{1}{AD-BC} \\begin{pmatrix} D & -B \\\\ -C & A \\end{pmatrix}$.\n    *   Determinant: $\\det(\\mathbf{M}) = (500)(50662000) - (159000)^2$\n        *   $500 \\times 50662000 = 25331000000$\n        *   $159000^2 = (1.59 \\times 10^5)^2 = 2.5281 \\times 10^{10} = 25281000000$\n        *   $\\det(\\mathbf{M}) = 25331000000 - 25281000000 = 50000000$\n    *   So the covariance matrix $\\mathbf{C}$ is:\n        $$ \\mathbf{C} = \\frac{1}{50000000} \\begin{pmatrix} 50662000 & -159000 \\\\ -159000 & 500 \\end{pmatrix} $$\n        $$ \\mathbf{C} = \\begin{pmatrix} \\frac{50662000}{50000000} & \\frac{-159000}{50000000} \\\\ \\frac{-159000}{50000000} & \\frac{500}{50000000} \\end{pmatrix} $$\n        $$ \\mathbf{C} = \\begin{pmatrix} 1.01324 & -0.00318 \\\\ -0.00318 & 0.00001 \\end{pmatrix} = \\begin{pmatrix} \\sigma_a^2 & \\text{cov}(a,b) \\\\ \\text{cov}(a,b) & \\sigma_b^2 \\end{pmatrix} $$\n    *   So, $\\sigma_a^2 = 1.01324 \\text{ K}^2$, $\\sigma_b^2 = 1.0 \\times 10^{-5}$, and $\\text{cov}(a,b) = -0.00318 \\text{ K}$.\n    *   Wait, units. $a$ has units of K. $b$ is dimensionless. So $\\sigma_a^2$ is K$^2$, $\\sigma_b^2$ is dimensionless, and $\\text{cov}(a,b)$ is K. My units seem correct.\n    *   Let's double-check the formula for covariance matrix. Sometimes it is multiplied by $\\chi^2_{\\text{min}}/(N-2)$. This is when the individual uncertainties $\\sigma_i$ are not known, but are assumed to be equal to some unknown $\\sigma$. We then estimate $\\sigma^2$ from the residuals. Here, the problem states the $\\sigma_i$ are known. So the factor is not needed. The formula $\\mathbf{C} = (\\mathbf{X}^T \\mathbf{W} \\mathbf{X})^{-1}$ is correct.\n\n4.  **Part 3: Uncertainty Propagation**\n    *   The quantity of interest is the heat generation rate $q$.\n    *   $q = m\\,c\\,\\frac{d T_{\\mathrm{true}}}{dt} + h\\,A\\,\\left(T_{\\mathrm{true}} - T_{\\mathrm{amb}}\\right)$\n    *   We need to express $q$ in terms of the fit parameters $a$ and $b$ and the measured quantities.\n    *   $T_{\\text{true}} = a + b\\,T_{\\text{meas}}$\n    *   $\\frac{d T_{\\text{true}}}{dt} = \\frac{d}{dt}(a + b\\,T_{\\text{meas}}) = b\\,\\frac{d T_{\\text{meas}}}{dt}$ (since $a$ and $b$ are constants).\n    *   Substitute these into the equation for $q$:\n        $$ q(a, b) = m\\,c\\,\\left(b\\,\\frac{d T_{\\mathrm{meas}}}{dt}\\right) + h\\,A\\,\\left((a + b\\,T_{\\mathrm{meas}}) - T_{\\mathrm{amb}}\\right) $$\n    *   Rearrange to group terms with $a$ and $b$:\n        $$ q(a, b) = (hA) a + \\left(mc \\frac{dT_{\\text{meas}}}{dt} + hA \\, T_{\\text{meas}}\\right) b - hA \\, T_{\\mathrm{amb}} $$\n    *   The problem states to treat all other quantities ($m, c, hA, T_{\\text{amb}}, T_{\\text{meas}}, \\frac{dT_{\\text{meas}}}{dt}$) as known exactly. The only sources of uncertainty are $a$ and $b$.\n    *   Let's define some constants based on the given values at $t^\\star$:\n        *   $T_m = T_{\\text{meas}}(t^\\star) = 320.0$ K\n        *   $\\dot{T}_m = \\frac{d T_{\\text{meas}}}{dt}(t^\\star) = 0.020$ K/s\n        *   $m = 0.050$ kg\n        *   $c = 1100$ J/(kg·K)\n        *   $hA = 2.5$ W/K\n        *   $T_{\\text{amb}} = 298.0$ K\n    *   So, $mc = 0.050 \\times 1100 = 55$ J/K.\n    *   Let's call the coefficients of $a$ and $b$:\n        *   $C_a = hA = 2.5$\n        *   $C_b = mc \\dot{T}_m + hA T_m = (55)(0.020) + (2.5)(320.0)$\n            *   $55 \\times 0.020 = 1.1$\n            *   $2.5 \\times 320.0 = 800.0$\n            *   $C_b = 1.1 + 800.0 = 801.1$\n    *   So the function for $q$ is:\n        $$ q(a, b) = C_a a + C_b b + \\text{constant} $$\n        where the constant is $-hA \\, T_{\\text{amb}}$.\n    *   The formula for linear propagation of uncertainty for a function $f(x_1, x_2, \\dots)$ is:\n        $$ \\sigma_f^2 = \\sum_{i} \\sum_{j} \\frac{\\partial f}{\\partial x_i} \\frac{\\partial f}{\\partial x_j} C_{ij} $$\n        where $C_{ij}$ is the covariance matrix of the variables $x_k$.\n    *   In our case, the variables are $a$ and $b$.\n        $$ \\sigma_q^2 = \\left(\\frac{\\partial q}{\\partial a}\\right)^2 \\sigma_a^2 + \\left(\\frac{\\partial q}{\\partial b}\\right)^2 \\sigma_b^2 + 2 \\left(\\frac{\\partial q}{\\partial a}\\right) \\left(\\frac{\\partial q}{\\partial b}\\right) \\text{cov}(a,b) $$\n    *   The partial derivatives are simply the coefficients we found:\n        *   $\\frac{\\partial q}{\\partial a} = C_a = 2.5$ W/K (units check: $q$ is W, $a$ is K)\n        *   $\\frac{\\partial q}{\\partial b} = C_b = 801.1$ W (units check: $q$ is W, $b$ is dimensionless)\n    *   Now plug in the values from the covariance matrix:\n        *   $\\sigma_a^2 = 1.01324$ K$^2$\n        *   $\\sigma_b^2 = 1.0 \\times 10^{-5}$\n        *   $\\text{cov}(a,b) = -0.00318$ K\n    *   Calculate the terms for $\\sigma_q^2$:\n        *   Term 1: $(2.5)^2 \\times (1.01324) = 6.25 \\times 1.01324 = 6.33275$\n        *   Term 2: $(801.1)^2 \\times (1.0 \\times 10^{-5}) = 641761.21 \\times 1.0 \\times 10^{-5} = 6.4176121$\n        *   Term 3: $2 \\times (2.5) \\times (801.1) \\times (-0.00318) = 5 \\times 801.1 \\times (-0.00318) = 4005.5 \\times (-0.00318) = -12.73749$\n    *   Sum the terms:\n        *   $\\sigma_q^2 = 6.33275 + 6.4176121 - 12.73749 \\approx 12.75036 - 12.73749 = 0.01287$\n    *   Let's re-calculate with higher precision.\n        *   $b = 998.4 / 1000 = 0.9984$ (exact)\n        *   $a = 0.6488$ (exact)\n        *   $\\det(\\mathbf{M}) = 5 \\times 10^7$ (exact)\n        *   $\\mathbf{C} = \\frac{1}{5 \\times 10^7} \\begin{pmatrix} 50662000 & -159000 \\\\ -159000 & 500 \\end{pmatrix} = \\begin{pmatrix} 1.01324 & -0.00318 \\\\ -0.00318 & 1 \\times 10^{-5} \\end{pmatrix}$ (all exact)\n        *   $C_a = 2.5$ (exact)\n        *   $C_b = 801.1$ (exact)\n        *   Term 1: $(2.5)^2 \\times 1.01324 = 6.25 \\times 1.01324 = 6.33275$ (exact)\n        *   Term 2: $(801.1)^2 \\times 10^{-5} = 641761.21 \\times 10^{-5} = 6.4176121$ (exact)\n        *   Term 3: $2 \\times 2.5 \\times 801.1 \\times (-0.00318) = 5 \\times 801.1 \\times (-0.00318) = 4005.5 \\times (-0.00318) = -12.73749$ (exact)\n        *   $\\sigma_q^2 = 6.33275 + 6.4176121 - 12.73749 = 12.7503621 - 12.73749 = 0.0128721$ W$^2$\n    *   The standard uncertainty is $\\sigma_q = \\sqrt{\\sigma_q^2}$.\n        *   $\\sigma_q = \\sqrt{0.0128721} \\approx 0.11345527$ W\n    *   The problem asks to round the final answer to four significant figures.\n        *   $0.1135$ W.\n\n5.  **Review and Final Write-up Structure**\n    *   Start with problem validation. It's valid. State this clearly.\n    *   Section 1: Maximum Likelihood Estimation.\n        *   Define the likelihood and chi-squared function for Gaussian errors.\n        *   Show minimization leads to weighted least squares normal equations.\n        *   Note that weights are equal, simplifying to unweighted least squares.\n        *   List the data and calculate the sums: $\\sum x_i, \\sum y_i, \\sum x_i^2, \\sum x_i y_i$.\n        *   Set up and solve the $2 \\times 2$ linear system for $a$ and $b$.\n    *   Section 2: Covariance Matrix Derivation.\n        *   Define the covariance matrix as the inverse of the Fisher information matrix, which for this problem is $(\\mathbf{X}^T \\mathbf{W} \\mathbf{X})^{-1}$.\n        *   Construct the matrix $\\mathbf{M} = \\mathbf{X}^T \\mathbf{W} \\mathbf{X}$ using the calculated sums and the constant weight $w = 1/\\sigma^2$.\n        *   Calculate the inverse $\\mathbf{M}^{-1}$ to get the covariance matrix $\\mathbf{C}$. State the components $\\sigma_a^2$, $\\sigma_b^2$, and $\\text{cov}(a,b)$.\n    *   Section 3: Uncertainty Propagation.\n        *   Write down the expression for the heat generation rate $q$.\n        *   Substitute the linear calibration model $T_{\\text{true}}(T_{\\text{meas}})$ and its time derivative into the expression for $q$.\n        *   The resulting expression for $q$ should be a function of $a$ and $b$.\n        *   State the general formula for first-order uncertainty propagation with covariance.\n        *   Calculate the partial derivatives $\\partial q / \\partial a$ and $\\partial q / \\partial b$ at the specified state ($T_{\\text{meas}}(t^\\star), \\dot{T}_{\\text{meas}}(t^\\star)$).\n        *   Plug all the numbers (derivatives and covariance matrix elements) into the uncertainty formula to calculate $\\sigma_q^2$.\n        *   Calculate $\\sigma_q = \\sqrt{\\sigma_q^2}$.\n    *   Final Answer:\n        *   State the numerical value of $\\sigma_q$ rounded to four significant figures.\n        *   Enclose in `$$\\boxed{...}$$`.\n    *   Formatting: Ensure all mathematical entities are in LaTeX. No markdown headers.\n\nLet's do a sanity check on the numbers.\n*   The data for $T_{\\text{true}}$ vs $T_{\\text{meas}}$ is very close to a line with slope 1 and intercept 0.\n    *   $y_i - x_i$: $0.10, 0.12, 0.14, 0.16, 0.18$. This is a perfectly linear trend.\n    *   Let $\\delta_i = y_i - x_i$. Then $\\delta_i = 0.10 + 0.002 (x_i - 298.0)$.\n    *   $y_i = x_i + 0.10 + 0.002 x_i - 0.002 \\times 298$.\n    *   $y_i = 1.002 x_i + (0.10 - 0.596) = 1.002 x_i - 0.496$.\n    *   My calculated fit was $b=0.9984$ and $a=0.6488$. Why the difference?\n    *   Let's re-solve the system of equations.\n        *   $5a + 1590b = 1590.70$\n        *   $1590a + 506620b = 506841.0$\n    *   Multiply first eq by $1590/5 = 318$:\n        *   $1590a + 318 \\times 1590 b = 318 \\times 1590.70$\n        *   $1590a + 505620 b = 505842.6$\n    *   Subtract this from the second equation:\n        *   $(506620 - 505620)b = 506841.0 - 505842.6$\n        *   $1000b = 998.4$\n        *   $b = 0.9984$. This is correct.\n    *   Substitute $b$ back into $5a + 1590b = 1590.70$:\n        *   $5a + 1590(0.9984) = 1590.70$\n        *   $5a + 1587.456 = 1590.70$\n        *   $5a = 1590.70 - 1587.456 = 3.244$\n        *   $a = 3.244 / 5 = 0.6488$. This is also correct.\n    *   The \"sanity check\" calculation was an approximation. The least squares method is the correct one.\n    *   $y = 0.6488 + 0.9984 x$.\n    *   Let's check the residuals for the fit.\n        *   $x=298$: $y_{fit} = 0.6488 + 0.9984 \\times 298 = 0.6488 + 297.5232 = 298.172$. Given $y=298.10$. Residual is $-0.072$.\n        *   $x=308$: $y_{fit} = 0.6488 + 0.9984 \\times 308 = 0.6488 + 307.5152 = 308.164$. Given $y=308.12$. Residual is $-0.044$.\n        *   $x=318$: $y_{fit} = 0.6488 + 0.9984 \\times 318 = 0.6488 + 317.5072 = 318.156$. Given $y=318.14$. Residual is $0.016$.\n        *   $x=328$: $y_{fit} = 0.6488 + 0.9984 \\times 328 = 0.6488 + 327.4992 = 328.148$. Given $y=328.16$. Residual is $0.012$.\n        *   $x=338$: $y_{fit} = 0.6488 + 0.9984 \\times 338 = 0.6488 + 337.4832 = 338.132$. Given $y=338.18$. Residual is $0.048$.\n    *   The residuals are all smaller than the uncertainty $\\sigma_i = 0.10$. This is reasonable. The fit seems correct.\n\nLet's re-check the uncertainty propagation part.\n*    $q(a, b) = (hA) a + \\left(mc \\dot{T}_m + hA T_m\\right) b - hA T_{\\text{amb}}$\n*    $\\sigma_q^2 = \\mathbf{J} \\mathbf{C} \\mathbf{J}^T$, where $\\mathbf{J} = \\begin{pmatrix} \\frac{\\partial q}{\\partial a} & \\frac{\\partial q}{\\partial b} \\end{pmatrix}$ and $\\mathbf{C}$ is the covariance matrix of $(a,b)$.\n*   $\\mathbf{J} = \\begin{pmatrix} C_a & C_b \\end{pmatrix} = \\begin{pmatrix} 2.5 & 801.1 \\end{pmatrix}$.\n*   $\\mathbf{C} = \\begin{pmatrix} \\sigma_a^2 & \\text{cov}(a,b) \\\\ \\text{cov}(a,b) & \\sigma_b^2 \\end{pmatrix} = \\begin{pmatrix} 1.01324 & -0.00318 \\\\ -0.00318 & 1 \\times 10^{-5} \\end{pmatrix}$.\n*   $\\sigma_q^2 = \\begin{pmatrix} 2.5 & 801.1 \\end{pmatrix} \\begin{pmatrix} 1.01324 & -0.00318 \\\\ -0.00318 & 1 \\times 10^{-5} \\end{pmatrix} \\begin{pmatrix} 2.5 \\\\ 801.1 \\end{pmatrix}$\n*   First matrix product:\n    *   $\\begin{pmatrix} 2.5 & 801.1 \\end{pmatrix} \\begin{pmatrix} 1.01324 & -0.00318 \\\\ -0.00318 & 1 \\times 10^{-5} \\end{pmatrix}$\n    *   $= \\begin{pmatrix} (2.5)(1.01324) + (801.1)(-0.00318) & (2.5)(-0.00318) + (801.1)(10^{-5}) \\end{pmatrix}$\n    *   $= \\begin{pmatrix} 2.5331 - 2.547498 & -0.00795 + 0.008011 \\end{pmatrix}$\n    *   $= \\begin{pmatrix} -0.014398 & 0.000061 \\end{pmatrix}$\n*   Second matrix product:\n    *   $\\begin{pmatrix} -0.014398 & 0.000061 \\end{pmatrix} \\begin{pmatrix} 2.5 \\\\ 801.1 \\end{pmatrix}$\n    *   $= (-0.014398)(2.5) + (0.000061)(801.1)$\n    *   $= -0.035995 + 0.0488671$\n    *   $= 0.0128721$\n*   This matches my previous calculation. The matrix formulation confirms the component-wise expansion.\n*   The large negative covariance term is crucial. Let's analyze it.\n    *   $\\sigma_a^2$ term = $6.33275$\n    *   $\\sigma_b^2$ term = $6.4176121$\n    *   covariance term = $-12.73749$\n    *   The two variance terms sum to $12.7503621$. The covariance term nearly cancels them out.\n    *   Why is the covariance negative? The estimators for the intercept and slope are often negatively correlated. If the data cloud is tilted up (larger $b$), the line's intercept with the y-axis (at $x=0$) will be lower, unless the center of the data is at $x=0$. The center of our $x$ data is $\\bar{x} = 1590/5 = 318$. Since $\\bar{x} > 0$, we expect a negative covariance. The formula for the covariance is $\\text{cov}(a,b) = - \\bar{x} \\sigma_b^2 / (\\text{some factor})$. In our case, $\\text{cov}(a,b) = -\\frac{\\sum w_i x_i}{\\det(\\mathbf{M})} = - \\frac{159000}{50000000} = -0.00318$. This is indeed negative.\n    *   The near-cancellation must be physically significant.\n    *   $q(a,b) \\approx C_a a + C_b b$.\n    *   When we evaluate $q$, we use $a$ and $b$ at a measured temperature $T_m = 320$.\n    *   This is very close to the center of the calibration data, $\\bar{x} = 318$.\n    *   Uncertainty in a fitted line $y=a+bx$ is given by $\\sigma_y^2 = \\sigma_a^2 + x^2 \\sigma_b^2 + 2x \\, \\text{cov}(a,b)$.\n    *   This uncertainty is minimal at $x = -\\text{cov}(a,b) / \\sigma_b^2$.\n    *   From our values: $-\\text{cov}(a,b) / \\sigma_b^2 = -(-0.00318) / (10^{-5}) = 318$. This is exactly $\\bar{x}$. So the uncertainty in the calibrated temperature $T_{\\text{true}}$ is minimal at the average measurement temperature of the calibration.\n    *   Our expression for $q$ is a linear combination of $a$ and $b$. Let's see if the point of minimum uncertainty has any meaning here.\n    *   The uncertainty $\\sigma_q^2$ is a quadratic function of the coefficients $C_a$ and $C_b$. It's not immediately obvious why the cancellation is so strong.\n    *   Let's check the coefficients $C_a$ and $C_b$ again.\n        *   $C_a = hA = 2.5$\n        *   $C_b = mc \\dot{T}_m + hA T_m = 1.1 + 2.5 \\times 320 = 1.1 + 800 = 801.1$\n    *   The ratio $C_b/C_a = 801.1 / 2.5 = 320.44$.\n    *   This is very close to $T_m = 320$.\n    *   Let's rewrite $q$ using $T_{\\text{true}} = a+b T_m$ and $\\dot{T}_{\\text{true}} = b \\dot{T}_m$.\n        *   $q = mc \\dot{T}_{\\text{true}} + hA(T_{\\text{true}} - T_{\\text{amb}})$\n        *   $q = mc (b \\dot{T}_m) + hA (a + b T_m - T_{\\text{amb}})$\n        *   $q = (hA)a + (mc \\dot{T}_m + hA T_m)b - hA T_{\\text{amb}}$\n    *   The derivatives are $\\partial q / \\partial a = hA$ and $\\partial q / \\partial b = mc \\dot{T}_m + hA T_m$.\n    *   The uncertainty is\n        $$ \\sigma_q^2 = (hA)^2 \\sigma_a^2 + (mc \\dot{T}_m + hA T_m)^2 \\sigma_b^2 + 2(hA)(mc \\dot{T}_m + hA T_m) \\text{cov}(a,b) $$\n    *   This can be recognized as the uncertainty of the quantity $Y = hA \\cdot a + (mc \\dot{T}_m + hA T_m) \\cdot b$.\n    *   Let's analyze the expression for $q$ in a different way.\n        *   $q = mc (b \\dot{T}_m) + hA (T_{\\text{true}} - T_{\\text{amb}})$\n        *   The uncertainty in $q$ comes from uncertainty in $b$ (affecting the derivative term) and uncertainty in $T_{\\text{true}}$ (affecting the convection term).\n        *   Let $Y_1 = b$ and $Y_2 = T_{\\text{true}} = a + b T_m$.\n        *   $q = mc \\dot{T}_m Y_1 + hA (Y_2 - T_{\\text{amb}})$.\n        *   $\\sigma_q^2 = (mc \\dot{T}_m)^2 \\sigma_{Y_1}^2 + (hA)^2 \\sigma_{Y_2}^2 + 2(mc \\dot{T}_m)(hA) \\text{cov}(Y_1, Y_2)$\n        *   We need the covariance matrix of $(Y_1, Y_2)$.\n            *   $Y_1 = b$\n            *   $Y_2 = a + T_m b$\n            *   $\\sigma_{Y_1}^2 = \\sigma_b^2$\n            *   $\\sigma_{Y_2}^2 = \\text{var}(a + T_m b) = \\sigma_a^2 + T_m^2 \\sigma_b^2 + 2 T_m \\text{cov}(a,b)$. This is the uncertainty of the fitted line at $T_m$.\n            *   $\\text{cov}(Y_1, Y_2) = \\text{cov}(b, a+T_m b) = \\text{cov}(b,a) + T_m \\text{cov}(b,b) = \\text{cov}(a,b) + T_m \\sigma_b^2$.\n    *   Let's plug in the numbers for $T_m = 320$.\n        *   $\\sigma_{Y_2}^2 = 1.01324 + (320)^2 (10^{-5}) + 2(320)(-0.00318)$\n        *   $\\sigma_{Y_2}^2 = 1.01324 + 102400 \\times 10^{-5} + 640(-0.00318)$\n        *   $\\sigma_{Y_2}^2 = 1.01324 + 1.024 - 2.0352 = 0.00204$ K$^2$. This is very small, as expected since $T_m = 320$ is close to $\\bar{x}=318$.\n        *   $\\text{cov}(Y_1, Y_2) = -0.00318 + 320 \\times (10^{-5}) = -0.00318 + 0.00320 = 0.00002$. Also very small.\n    *   Now plug this into the $\\sigma_q^2$ formula:\n        *   $(mc \\dot{T}_m)^2 \\sigma_{Y_1}^2 = (55 \\times 0.02)^2 \\sigma_b^2 = (1.1)^2 \\times 10^{-5} = 1.21 \\times 10^{-5}$. This is a tiny term.\n        *   $(hA)^2 \\sigma_{Y_2}^2 = (2.5)^2 \\times 0.00204 = 6.25 \\times 0.00204 = 0.01275$. This is the dominant term.\n        *   $2(mc \\dot{T}_m)(hA) \\text{cov}(Y_1, Y_2) = 2(1.1)(2.5)(0.00002) = 5.5 \\times 0.00002 = 0.00011$.\n    *   $\\sigma_q^2 \\approx 0.01275 + 0.00011 = 0.01286$.\n    *   This is very close to my original calculation of $0.0128721$. The small discrepancy is likely from rounding during intermediate steps of this alternative calculation. The first method was more direct and used exact intermediate values, so it is preferable.\n    *   The strong cancellation is confirmed. It comes from the fact that we are evaluating the uncertainty at a temperature $T_m$ near the center of the calibration data, which makes the uncertainty in the predicted $T_{\\text{true}}$ very small.\n    *   Let's see what happens if $T_m$ was far from the center, say $T_m = 0$.\n        *   $C_b = mc \\dot{T}_m + hA T_m = 1.1 + 0 = 1.1$.\n        *   $C_a = 2.5$.\n        *   $\\sigma_q^2 = (2.5)^2(1.01324) + (1.1)^2(10^{-5}) + 2(2.5)(1.1)(-0.00318)$\n        *   $\\sigma_q^2 = 6.33275 + 0.0000121 + 5.5(-0.00318) = 6.33276 - 0.01749 = 6.315...$. A much larger uncertainty, as expected.\n\nThe whole procedure seems sound. I will write up the solution following the plan.\n\nFinal check on the question: \"Propagate the calibration parameter uncertainties to obtain the standard uncertainty of the heat generation rate $q$ at time $t^\\star$ by linear (first-order) uncertainty propagation, fully accounting for the covariance between $a$ and $b$ arising from the fit. Round your final answer for the standard uncertainty in $q$ to four significant figures. Express the final answer in watts.\"\n\nMy result is $\\sigma_q \\approx 0.113455$ W. Rounded to four significant figures, this is $0.1135$ W. The units are correct. The method is correct. The calculation seems correct.\n\nI will structure the solution carefully, explaining each step from first principles as requested.\n\n### Problem Validation\n\nThe problem appears to be valid.\n\n**Extracted Givens:**\n- Calibration model: $T_{\\mathrm{true}} = a + b\\,T_{\\mathrm{meas}}$\n- Calibration data pairs $(T_{\\mathrm{meas},i}, T_{\\mathrm{true},i})$: $(298.0, 298.10)$, $(308.0, 308.12)$, $(318.0, 318.14)$, $(328.0, 328.16)$, $(338.0, 338.18)$, all in kelvin.\n- Number of data points: $N=5$.\n- Uncertainty in each $T_{\\mathrm{true},i}$ is Gaussian with standard deviation $\\sigma_i = 0.10$ K.\n- Uncertainty in $T_{\\mathrm{meas},i}$ is negligible.\n- Energy balance equation: $q = m\\,c\\,\\frac{d T_{\\mathrm{true}}}{dt} + h\\,A\\,\\left(T_{\\mathrm{true}} - T_{\\mathrm{amb}}\\right)$.\n- Thermodynamic parameters: $m = 0.050$ kg, $c = 1100$ J/(kg·K), $h\\,A = 2.5$ W/K, $T_{\\mathrm{amb}} = 298.0$ K.\n- State at time $t^\\star$: $T_{\\mathrm{meas}}(t^\\star) = 320.0$ K, $\\frac{d T_{\\mathrm{meas}}}{dt}(t^\\star) = 0.020$ K/s.\n- All parameters other than $a$ and $b$ are treated as known exactly.\n- Propagated uncertainty should be calculated using linear, first-order propagation.\n\n**Validation Checklist & Verdict:**\n1.  **Scientifically Grounded:** Yes. The problem uses established principles of thermodynamics (First Law, Newtonian convection) and statistics (Maximum Likelihood Estimation for Gaussian errors, linear regression, uncertainty propagation).\n2.  **Well-Posed:** Yes. The problem asks for specific, calculable quantities ($a$, $b$, their covariance matrix, and $\\sigma_q$) from a complete set of data and relations.\n3.  **Objective:** Yes. The problem is stated in precise, quantitative terms.\n4.  **Incomplete or Contradictory Setup:** No. The problem provides all necessary information and conditions.\n5.  **Unrealistic or Infeasible:** No. The physical parameters and temperature values are within a realistic range for battery testing.\n\nThe problem is valid and can be solved as stated.\n\n### Solution Derivations\n\nThe solution proceeds in three main parts:\n1.  Estimation of the calibration parameters $a$ and $b$ using maximum likelihood.\n2.  Derivation of the covariance matrix for the parameters $(a,b)$.\n3.  Propagation of the parameter uncertainties to find the uncertainty in the heat generation rate $q$.\n\n**1. Maximum Likelihood Estimation of Calibration Parameters**\n\nThe model is $T_{\\mathrm{true}} = a + b\\,T_{\\mathrm{meas}}$. We are given $N=5$ pairs of data $(x_i, y_i) = (T_{\\mathrm{meas},i}, T_{\\mathrm{true},i})$, where each $y_i$ has a Gaussian uncertainty with a known standard deviation $\\sigma_i$. The probability of observing a particular $y_i$ given the model is:\n$$ P(y_i|x_i, a, b) = \\frac{1}{\\sigma_i\\sqrt{2\\pi}} \\exp\\left(-\\frac{(y_i - (a+bx_i))^2}{2\\sigma_i^2}\\right) $$\nAssuming independent measurements, the total likelihood $\\mathcal{L}$ is the product of individual probabilities. Maximizing $\\mathcal{L}$ is equivalent to minimizing the negative log-likelihood, which simplifies to minimizing the chi-squared function, $\\chi^2$:\n$$ \\chi^2(a,b) = \\sum_{i=1}^N \\left(\\frac{y_i - a - bx_i}{\\sigma_i}\\right)^2 = \\sum_{i=1}^N w_i (y_i - a - bx_i)^2 $$\nwhere the weights are $w_i = 1/\\sigma_i^2$.\nTo find the parameters $a$ and $b$ that minimize $\\chi^2$, we set the partial derivatives to zero:\n$$ \\frac{\\partial\\chi^2}{\\partial a} = -2\\sum_{i=1}^N w_i(y_i - a - bx_i) = 0 \\implies a\\sum w_i + b\\sum w_i x_i = \\sum w_i y_i $$\n$$ \\frac{\\partial\\chi^2}{\\partial b} = -2\\sum_{i=1}^N w_i x_i(y_i - a - bx_i) = 0 \\implies a\\sum w_i x_i + b\\sum w_i x_i^2 = \\sum w_i x_i y_i $$\nIn this problem, all $\\sigma_i = 0.10$ K, so the weights $w_i = w = 1/(0.10)^2 = 100$ are constant and can be factored out. This simplifies the normal equations to the unweighted case:\n$$ aN + b\\sum x_i = \\sum y_i $$\n$$ a\\sum x_i + b\\sum x_i^2 = \\sum x_i y_i $$\nThe data are $x_i = \\{298.0, 308.0, 318.0, 328.0, 338.0\\}$ and $y_i = \\{298.10, 308.12, 318.14, 328.16, 338.18\\}$. We calculate the required sums:\n- $N=5$\n- $\\sum x_i = 1590.0$\n- $\\sum y_i = 1590.70$\n- $\\sum x_i^2 = 298.0^2 + 308.0^2 + 318.0^2 + 328.0^2 + 338.0^2 = 506620.0$\n- $\\sum x_i y_i = (298.0)(298.10) + \\dots + (338.0)(338.18) = 506841.0$\n\nThe system of equations is:\n$$ 5a + 1590b = 1590.70 $$\n$$ 1590a + 506620b = 506841.0 $$\nSolving this system yields the maximum-likelihood estimates:\n$b = 0.9984$\n$a = 0.6488$\n\n**2. Covariance Matrix of Parameters**\n\nThe covariance matrix $\\mathbf{C}$ for the parameters $(a,b)$ is given by the inverse of the Fisher information matrix. For linear least squares, this is the inverse of the matrix of second derivatives of $\\frac{1}{2}\\chi^2$, which is:\n$$ \\mathbf{C} = \\begin{pmatrix} \\sum w_i & \\sum w_i x_i \\\\ \\sum w_i x_i & \\sum w_i x_i^2 \\end{pmatrix}^{-1} $$\nWith constant weight $w=100$:\n- $\\sum w_i = Nw = 5 \\times 100 = 500$\n- $\\sum w_i x_i = w \\sum x_i = 100 \\times 1590.0 = 159000$\n- $\\sum w_i x_i^2 = w \\sum x_i^2 = 100 \\times 506620.0 = 50662000$\n\nThe matrix to be inverted is:\n$$ \\mathbf{M} = \\begin{pmatrix} 500 & 159000 \\\\ 159000 & 50662000 \\end{pmatrix} $$\nThe determinant is $\\det(\\mathbf{M}) = (500)(50662000) - (159000)^2 = 25331000000 - 25281000000 = 50000000$.\nThe inverse matrix is:\n$$ \\mathbf{C} = \\frac{1}{\\det(\\mathbf{M})} \\begin{pmatrix} 50662000 & -159000 \\\\ -159000 & 500 \\end{pmatrix} = \\frac{1}{5 \\times 10^7} \\begin{pmatrix} 5.0662 \\times 10^7 & -1.59 \\times 10^5 \\\\ -1.59 \\times 10^5 & 500 \\end{pmatrix} $$\n$$ \\mathbf{C} = \\begin{pmatrix} \\sigma_a^2 & \\mathrm{cov}(a,b) \\\\ \\mathrm{cov}(a,b) & \\sigma_b^2 \\end{pmatrix} = \\begin{pmatrix} 1.01324 & -0.00318 \\\\ -0.00318 & 1.0 \\times 10^{-5} \\end{pmatrix} $$\nThe units are $\\mathrm{K}^2$ for $\\sigma_a^2$, dimensionless for $\\sigma_b^2$, and $\\mathrm{K}$ for $\\mathrm{cov}(a,b)$.\n\n**3. Uncertainty Propagation for Heat Generation Rate**\n\nThe heat generation rate $q$ is given by:\n$$ q = m\\,c\\,\\frac{d T_{\\mathrm{true}}}{dt} + h\\,A\\,\\left(T_{\\mathrm{true}} - T_{\\mathrm{amb}}\\right) $$\nUsing the calibration model $T_{\\mathrm{true}} = a + b\\,T_{\\mathrm{meas}}$, we also have $\\frac{d T_{\\mathrm{true}}}{dt} = b\\,\\frac{d T_{\\mathrm{meas}}}{dt}$.\nSubstituting these into the equation for $q$ gives $q$ as a function of the parameters $a$ and $b$:\n$$ q(a,b) = m\\,c\\,\\left(b\\,\\frac{d T_{\\mathrm{meas}}}{dt}\\right) + h\\,A\\,\\left((a + b\\,T_{\\mathrm{meas}}) - T_{\\mathrm{amb}}\\right) $$\nRearranging to isolate $a$ and $b$:\n$$ q(a,b) = (hA) a + \\left(mc \\frac{dT_{\\mathrm{meas}}}{dt} + hA T_{\\mathrm{meas}}\\right) b - hA T_{\\mathrm{amb}} $$\nFor linear (first-order) uncertainty propagation, the variance of $q$, $\\sigma_q^2$, is given by:\n$$ \\sigma_q^2 = \\left(\\frac{\\partial q}{\\partial a}\\right)^2 \\sigma_a^2 + \\left(\\frac{\\partial q}{\\partial b}\\right)^2 \\sigma_b^2 + 2\\left(\\frac{\\partial q}{\\partial a}\\right)\\left(\\frac{\\partial q}{\\partial b}\\right) \\mathrm{cov}(a,b) $$\nThe partial derivatives are the coefficients of $a$ and $b$ in the expression for $q(a,b)$:\n$$ \\frac{\\partial q}{\\partial a} = hA $$\n$$ \\frac{\\partial q}{\\partial b} = mc \\frac{dT_{\\mathrm{meas}}}{dt} + hA T_{\\mathrm{meas}} $$\nWe evaluate these derivatives using the given values at time $t^\\star$:\n- $hA = 2.5 \\, \\text{W/K}$\n- $mc = (0.050 \\, \\text{kg})(1100 \\, \\text{J/(kg K)}) = 55 \\, \\text{J/K}$\n- $T_{\\mathrm{meas}}(t^\\star) = 320.0 \\, \\text{K}$\n- $\\frac{dT_{\\mathrm{meas}}}{dt}(t^\\star) = 0.020 \\, \\text{K/s}$\n\nSo, the derivatives are:\n$$ \\frac{\\partial q}{\\partial a} = 2.5 $$\n$$ \\frac{\\partial q}{\\partial b} = (55)(0.020) + (2.5)(320.0) = 1.1 + 800.0 = 801.1 $$\nNow we substitute these values and the elements of the covariance matrix into the variance formula:\n$$ \\sigma_q^2 = (2.5)^2(1.01324) + (801.1)^2(1.0 \\times 10^{-5}) + 2(2.5)(801.1)(-0.00318) $$\n$$ \\sigma_q^2 = (6.25)(1.01324) + (641761.21)(1.0 \\times 10^{-5}) + (5)(801.1)(-0.00318) $$\n$$ \\sigma_q^2 = 6.33275 + 6.4176121 - 12.73749 $$\n$$ \\sigma_q^2 = 0.0128721 \\, \\mathrm{W}^2 $$\nThe standard uncertainty $\\sigma_q$ is the square root of the variance:\n$$ \\sigma_q = \\sqrt{0.0128721} \\approx 0.11345527 \\, \\mathrm{W} $$\nRounding to four significant figures as requested gives $0.1135 \\, \\mathrm{W}$.",
            "answer": "$$\n\\boxed{0.1135}\n$$"
        },
        {
            "introduction": "Having established how parameter uncertainty propagates, we now address a core question: where do these parameter uncertainties come from? This practice dives into the Bayesian inverse problem, a powerful framework for calibrating a model's parameters using experimental data . By working with a common SEI aging model, you will learn how to combine prior physical knowledge (in the form of Arrhenius-informed priors) with measurement data to obtain a posterior distribution, and quantify the often-strong correlations between parameters like activation energy ($E_a$) and the pre-exponential factor ($k_0$).",
            "id": "3959883",
            "problem": "An advanced virtual prototyping study for a lithium-ion battery Solid Electrolyte Interphase (SEI) growth model is required. Consider the parabolic SEI growth kinetics where the SEI thickness $x$ evolves as $x^2(t) - x^2(0) = 2 k(T) t$, with negligible initial thickness so that $x(0) \\approx 0$. The capacity loss is proportional to the SEI thickness, modeled as $y(t, T) = c \\sqrt{2 k(T) t}$, where $y$ is capacity loss in ampere-hours (Ah), $c$ is a proportionality constant in ampere-hours per meter (Ah/m), $t$ is time in seconds (s), and $k(T)$ is the parabolic rate constant in square meters per second ($\\mathrm{m^2/s}$). The temperature dependence of $k(T)$ follows the Arrhenius law $k(T) = k_0 \\exp(-E_a/(R T))$, where $k_0$ is the pre-exponential factor in $\\mathrm{m^2/s}$, $E_a$ is the activation energy in joules per mole (J/mol), $T$ is absolute temperature in kelvin (K), and $R$ is the universal gas constant in $\\mathrm{J/(mol \\cdot K)}$. Use $R = 8.314462618 \\ \\mathrm{J/(mol \\cdot K)}$.\n\nAssume independent and identically distributed Gaussian observation noise with known standard deviation $\\sigma$ (in Ah). The goal is to calibrate the parameters $k_0$, $E_a$, and $c$ using a Bayesian inverse problem with Arrhenius-informed priors, and then quantify the posterior correlations between these parameters. Specifically, use the following priors:\n- $\\log k_0 \\sim \\mathcal{N}(\\mu_{\\log k_0}, \\sigma_{\\log k_0}^2)$,\n- $E_a \\sim \\mathcal{N}(\\mu_{E_a}, \\sigma_{E_a}^2)$,\n- $c \\sim \\mathcal{N}(\\mu_c, \\sigma_c^2)$,\n\nwhere $\\log$ denotes the natural logarithm. The hyperparameters are fixed and given by: $\\mu_{\\log k_0} = \\log(10^{-6})$, $\\sigma_{\\log k_0} = 1.5$, $\\mu_{E_a} = 60000 \\ \\mathrm{J/mol}$, $\\sigma_{E_a} = 10000 \\ \\mathrm{J/mol}$, $\\mu_c = 1000 \\ \\mathrm{Ah/m}$, and $\\sigma_c = 300 \\ \\mathrm{Ah/m}$. These priors encode Arrhenius-informed magnitudes reflecting typical SEI growth behavior. Impose parameter bounds of $E_a > 0$ and $c > 0$.\n\nFormulate the Bayesian inverse problem with the likelihood given by Gaussian noise and the prior as specified above. Use a Laplace (Gaussian) approximation around the Maximum A Posteriori (MAP) estimate to compute the approximate posterior covariance. Then transform the covariance from the parameterization $(\\log k_0, E_a, c)$ to $(k_0, E_a, c)$ using the first-order delta method (Jacobian of the transformation), and compute the corresponding pairwise correlation coefficients between $k_0$, $E_a$, and $c$.\n\nAll physical quantities must use the following units:\n- $k_0$ in $\\mathrm{m^2/s}$,\n- $E_a$ in $\\mathrm{J/mol}$,\n- $c$ in $\\mathrm{Ah/m}$,\n- $t$ in $\\mathrm{s}$,\n- $T$ in $\\mathrm{K}$,\n- $y$ in $\\mathrm{Ah}$.\n\nYour program must implement the following steps for each test case:\n1. Compute the MAP estimate of $(\\log k_0, E_a, c)$ by minimizing the negative log-posterior (sum of the negative log-likelihood and negative log-prior), enforcing the bounds $E_a > 0$ and $c > 0$.\n2. Construct the Gauss–Newton approximation to the Hessian of the negative log-posterior at the MAP, and invert it to obtain the approximate posterior covariance in $(\\log k_0, E_a, c)$ space.\n3. Transform the covariance to $(k_0, E_a, c)$ space by applying the Jacobian of $k_0 = \\exp(\\log k_0)$ and identity for $E_a$ and $c$.\n4. Report the MAP estimates for $(k_0, E_a, c)$ and the pairwise correlation coefficients $(\\rho_{k_0,E_a}, \\rho_{k_0,c}, \\rho_{E_a,c})$ computed from the transformed covariance.\n\nTest Suite:\nProvide results for the following three test cases. Each case specifies arrays of temperatures $T$ (in K), times $t$ (in s), observed capacity losses $y$ (in Ah), and the known noise standard deviation $\\sigma$ (in Ah). The arrays are aligned elementwise, i.e., each index $i$ corresponds to one observation $(T_i, t_i, y_i)$.\n\n- Case A (multi-temperature, moderate durations):\n  - $T = [\\ 298,\\ 298,\\ 298,\\ 323,\\ 323,\\ 323,\\ 348,\\ 348,\\ 348\\ ] \\ \\mathrm{K}$\n  - $t = [\\ 10^6,\\ 2 \\cdot 10^6,\\ 4 \\cdot 10^6,\\ 10^6,\\ 2 \\cdot 10^6,\\ 4 \\cdot 10^6,\\ 10^6,\\ 2 \\cdot 10^6,\\ 4 \\cdot 10^6\\ ] \\ \\mathrm{s}$\n  - $y = [\\ 0.00780,\\ 0.01110,\\ 0.01570,\\ 0.02000,\\ 0.02830,\\ 0.03990,\\ 0.04450,\\ 0.06300,\\ 0.08960\\ ] \\ \\mathrm{Ah}$\n  - $\\sigma = 0.002 \\ \\mathrm{Ah}$\n\n- Case B (broader temperature range, longer durations):\n  - $T = [\\ 273,\\ 273,\\ 273,\\ 298,\\ 298,\\ 298,\\ 323,\\ 323,\\ 323,\\ 358,\\ 358,\\ 358\\ ] \\ \\mathrm{K}$\n  - $t = [\\ 2 \\cdot 10^6,\\ 4 \\cdot 10^6,\\ 8 \\cdot 10^6,\\ 2 \\cdot 10^6,\\ 4 \\cdot 10^6,\\ 8 \\cdot 10^6,\\ 2 \\cdot 10^6,\\ 4 \\cdot 10^6,\\ 8 \\cdot 10^6,\\ 2 \\cdot 10^6,\\ 4 \\cdot 10^6,\\ 8 \\cdot 10^6\\ ] \\ \\mathrm{s}$\n  - $y = [\\ 0.00370,\\ 0.00520,\\ 0.00740,\\ 0.01110,\\ 0.01560,\\ 0.02220,\\ 0.02830,\\ 0.03990,\\ 0.05650,\\ 0.08490,\\ 0.11950,\\ 0.16850\\ ] \\ \\mathrm{Ah}$\n  - $\\sigma = 0.003 \\ \\mathrm{Ah}$\n\n- Case C (limited data, small noise):\n  - $T = [\\ 298,\\ 298,\\ 348,\\ 348\\ ] \\ \\mathrm{K}$\n  - $t = [\\ 2 \\cdot 10^6,\\ 6 \\cdot 10^6,\\ 2 \\cdot 10^6,\\ 6 \\cdot 10^6\\ ] \\ \\mathrm{s}$\n  - $y = [\\ 0.01110,\\ 0.01920,\\ 0.06300,\\ 0.10900\\ ] \\ \\mathrm{Ah}$\n  - $\\sigma = 0.001 \\ \\mathrm{Ah}$\n\nFinal Output Format:\nYour program should produce a single line of output containing a list of three lists, one per test case, where each inner list is ordered as $[\\ k_0,\\ E_a,\\ c,\\ \\rho_{k_0,E_a},\\ \\rho_{k_0,c},\\ \\rho_{E_a,c}\\ ]$. The values for $k_0$, $E_a$, and $c$ must be in $\\mathrm{m^2/s}$, $\\mathrm{J/mol}$, and $\\mathrm{Ah/m}$ respectively. The correlation coefficients must be real numbers between $-1$ and $1$. The overall output must be a single line formatted as a comma-separated list enclosed in square brackets, with inner lists also comma-separated and enclosed in square brackets (e.g., $[\\ [v_{1,1},v_{1,2},\\dots],\\ [v_{2,1},\\dots],\\ [v_{3,1},\\dots]\\ ]$).",
            "solution": "The user-provided problem has been validated and is deemed to be scientifically grounded, well-posed, objective, and complete. All necessary models, data, and constraints are provided to formulate and solve a standard Bayesian parameter estimation problem. The physical models, namely parabolic SEI growth kinetics and the Arrhenius law for temperature dependence, are well-established in the field of battery science. The statistical framework, involving a Bayesian inverse problem with Gaussian priors and likelihood, to be solved using a Laplace approximation around the Maximum A Posteriori estimate, is a rigorous and conventional approach. Therefore, a full solution is warranted.\n\n### 1. Bayesian Problem Formulation\n\nThe problem is to infer the parameters $\\boldsymbol{\\theta} = (k_0, E_a, c)^T$ of a physical model for battery capacity loss. For numerical stability and consistency with the specified priors, we define a working parameter vector $\\boldsymbol{\\phi} = (\\phi_1, \\phi_2, \\phi_3)^T = (\\log k_0, E_a, c)^T$, where $\\log$ denotes the natural logarithm.\n\n**Forward Model:** The capacity loss $y$ at time $t$ and temperature $T$ is given by\n$$\ny(t, T) = c \\sqrt{2 k(T) t}\n$$\nThe rate constant $k(T)$ follows the Arrhenius law:\n$$\nk(T) = k_0 \\exp\\left(-\\frac{E_a}{RT}\\right)\n$$\nSubstituting the expression for $k(T)$ and the reparameterization in terms of $\\boldsymbol{\\phi}$, the model prediction for a given data point $i$ with conditions $(T_i, t_i)$ is:\n$$\nm(\\boldsymbol{d}_i; \\boldsymbol{\\phi}) = \\phi_3 \\sqrt{2 t_i \\exp(\\phi_1) \\exp\\left(-\\frac{\\phi_2}{RT_i}\\right)} = \\phi_3 \\sqrt{2 t_i} \\exp\\left(\\frac{\\phi_1}{2} - \\frac{\\phi_2}{2RT_i}\\right)\n$$\nwhere $\\boldsymbol{d}_i = (T_i, t_i)$ represents the experimental conditions.\n\n**Likelihood:** The observations $y_i$ are assumed to be corrupted by independent and identically distributed Gaussian noise with a known standard deviation $\\sigma$, so $y_i \\sim \\mathcal{N}(m(\\boldsymbol{d}_i; \\boldsymbol{\\phi}), \\sigma^2)$. For a dataset of $N$ observations $\\boldsymbol{y} = (y_1, \\dots, y_N)^T$, the likelihood function is:\n$$\np(\\boldsymbol{y}|\\boldsymbol{\\phi}) = \\prod_{i=1}^{N} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - m(\\boldsymbol{d}_i; \\boldsymbol{\\phi}))^2}{2\\sigma^2}\\right)\n$$\nThe corresponding negative log-likelihood, ignoring constant terms, is:\n$$\n\\mathcal{L}_{\\text{like}}(\\boldsymbol{\\phi}) = \\frac{1}{2\\sigma^2} \\sum_{i=1}^{N} (y_i - m(\\boldsymbol{d}_i; \\boldsymbol{\\phi}))^2\n$$\n\n**Prior:** The prior distributions for the parameters in $\\boldsymbol{\\phi}$ are given as independent Gaussians:\n- $\\phi_1 = \\log k_0 \\sim \\mathcal{N}(\\mu_{\\log k_0}, \\sigma_{\\log k_0}^2)$\n- $\\phi_2 = E_a \\sim \\mathcal{N}(\\mu_{E_a}, \\sigma_{E_a}^2)$\n- $\\phi_3 = c \\sim \\mathcal{N}(\\mu_c, \\sigma_c^2)$\n\nThe joint negative log-prior, ignoring constants, is:\n$$\n\\mathcal{L}_{\\text{prior}}(\\boldsymbol{\\phi}) = \\frac{1}{2} \\left( \\frac{(\\phi_1 - \\mu_{\\log k_0})^2}{\\sigma_{\\log k_0}^2} + \\frac{(\\phi_2 - \\mu_{E_a})^2}{\\sigma_{E_a}^2} + \\frac{(\\phi_3 - \\mu_c)^2}{\\sigma_c^2} \\right)\n$$\n\n**Posterior:** According to Bayes' theorem, the posterior distribution is proportional to the product of the likelihood and the prior, $p(\\boldsymbol{\\phi}|\\boldsymbol{y}) \\propto p(\\boldsymbol{y}|\\boldsymbol{\\phi})p(\\boldsymbol{\\phi})$.\n\n### 2. Maximum A Posteriori (MAP) Estimation\n\nThe MAP estimate, $\\boldsymbol{\\phi}_{\\text{MAP}}$, is the value of $\\boldsymbol{\\phi}$ that maximizes the posterior probability, which is equivalent to minimizing the negative log-posterior function $J(\\boldsymbol{\\phi})$:\n$$\n\\boldsymbol{\\phi}_{\\text{MAP}} = \\arg\\min_{\\boldsymbol{\\phi}} J(\\boldsymbol{\\phi}) = \\arg\\min_{\\boldsymbol{\\phi}} \\left( \\mathcal{L}_{\\text{like}}(\\boldsymbol{\\phi}) + \\mathcal{L}_{\\text{prior}}(\\boldsymbol{\\phi}) \\right)\n$$\nsubject to the physical constraints $\\phi_2 = E_a > 0$ and $\\phi_3 = c > 0$. This constrained numerical optimization problem yields the most probable parameter values given the data and prior beliefs.\n\n### 3. Laplace Approximation for Posterior Covariance\n\nThe posterior distribution is approximated by a multivariate Gaussian distribution centered at the MAP estimate:\n$$\np(\\boldsymbol{\\phi}|\\boldsymbol{y}) \\approx \\mathcal{N}(\\boldsymbol{\\phi}_{\\text{MAP}}, \\boldsymbol{\\Sigma}_{\\boldsymbol{\\phi}})\n$$\nThe covariance matrix $\\boldsymbol{\\Sigma}_{\\boldsymbol{\\phi}}$ is the inverse of the Hessian of the negative log-posterior, evaluated at $\\boldsymbol{\\phi}_{\\text{MAP}}$:\n$$\n\\boldsymbol{\\Sigma}_{\\boldsymbol{\\phi}} = \\boldsymbol{H}^{-1}(\\boldsymbol{\\phi}_{\\text{MAP}}), \\quad \\text{where} \\quad \\boldsymbol{H}(\\boldsymbol{\\phi}) = \\nabla^2 J(\\boldsymbol{\\phi})\n$$\nThe problem specifies using the Gauss-Newton approximation for the Hessian. The full Hessian is $\\boldsymbol{H} = \\nabla^2 \\mathcal{L}_{\\text{like}} + \\nabla^2 \\mathcal{L}_{\\text{prior}}$.\nThe Hessian of the negative log-prior is constant and diagonal:\n$$\n\\boldsymbol{H}_{\\text{prior}} = \\nabla^2 \\mathcal{L}_{\\text{prior}} = \\text{diag}\\left(\\frac{1}{\\sigma_{\\log k_0}^2}, \\frac{1}{\\sigma_{E_a}^2}, \\frac{1}{\\sigma_c^2}\\right) = \\boldsymbol{\\Sigma}_{\\text{prior}}^{-1}\n$$\nFor the likelihood term, the Gauss-Newton approximation neglects terms involving second derivatives of the model function, resulting in:\n$$\n\\boldsymbol{H}_{\\text{like, GN}} \\approx \\frac{1}{\\sigma^2} \\sum_{i=1}^{N} \\left(\\nabla_{\\boldsymbol{\\phi}} m(\\boldsymbol{d}_i; \\boldsymbol{\\phi})\\right) \\left(\\nabla_{\\boldsymbol{\\phi}} m(\\boldsymbol{d}_i; \\boldsymbol{\\phi})\\right)^T = \\frac{1}{\\sigma^2} \\boldsymbol{J}_{\\text{model}}^T \\boldsymbol{J}_{\\text{model}}\n$$\nwhere $\\boldsymbol{J}_{\\text{model}}$ is the Jacobian matrix of the model predictions with respect to $\\boldsymbol{\\phi}$, with entries $(\\boldsymbol{J}_{\\text{model}})_{ij} = \\frac{\\partial m_i}{\\partial \\phi_j}$. The derivatives are:\n- $\\frac{\\partial m_i}{\\partial \\phi_1} = \\frac{\\partial m_i}{\\partial (\\log k_0)} = \\frac{m_i}{2}$\n- $\\frac{\\partial m_i}{\\partial \\phi_2} = \\frac{\\partial m_i}{\\partial E_a} = -\\frac{m_i}{2RT_i}$\n- $\\frac{\\partial m_i}{\\partial \\phi_3} = \\frac{\\partial m_i}{\\partial c} = \\frac{m_i}{\\phi_3}$\n\nThe total approximate Hessian is $\\boldsymbol{H}_{\\text{GN}} = \\boldsymbol{H}_{\\text{like, GN}} + \\boldsymbol{H}_{\\text{prior}}$. The posterior covariance in the $\\boldsymbol{\\phi}$-space is then $\\boldsymbol{\\Sigma}_{\\boldsymbol{\\phi}} = (\\boldsymbol{H}_{\\text{GN}}(\\boldsymbol{\\phi}_{\\text{MAP}}))^{-1}$.\n\n### 4. Covariance Transformation and Correlation\n\nWe transform the covariance matrix from the $\\boldsymbol{\\phi} = (\\log k_0, E_a, c)^T$ space to the original parameter space $\\boldsymbol{\\theta} = (k_0, E_a, c)^T$ using the first-order delta method. The transformation is $\\boldsymbol{\\theta} = g(\\boldsymbol{\\phi})$, where $k_0 = \\exp(\\phi_1)$, $E_a = \\phi_2$, and $c = \\phi_3$. The Jacobian of this transformation is:\n$$\n\\boldsymbol{J}_g = \\frac{\\partial \\boldsymbol{\\theta}}{\\partial \\boldsymbol{\\phi}^T} = \\begin{pmatrix} \\partial k_0/\\partial \\phi_1 & 0 & 0 \\\\ 0 & \\partial E_a/\\partial \\phi_2 & 0 \\\\ 0 & 0 & \\partial c/\\partial \\phi_3 \\end{pmatrix} = \\begin{pmatrix} \\exp(\\phi_1) & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} = \\begin{pmatrix} k_0 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}\n$$\nThe covariance matrix in the $\\boldsymbol{\\theta}$-space is approximated as:\n$$\n\\boldsymbol{\\Sigma}_{\\boldsymbol{\\theta}} \\approx \\boldsymbol{J}_g \\boldsymbol{\\Sigma}_{\\boldsymbol{\\phi}} \\boldsymbol{J}_g^T\n$$\nwhere $\\boldsymbol{J}_g$ is evaluated at $\\boldsymbol{\\phi}_{\\text{MAP}}$.\n\nFinally, the pairwise correlation coefficient between two parameters $\\theta_i$ and $\\theta_j$ is calculated from the transformed covariance matrix $\\boldsymbol{\\Sigma}_{\\boldsymbol{\\theta}}$:\n$$\n\\rho_{ij} = \\frac{(\\boldsymbol{\\Sigma}_{\\boldsymbol{\\theta}})_{ij}}{\\sqrt{(\\boldsymbol{\\Sigma}_{\\boldsymbol{\\theta}})_{ii} (\\boldsymbol{\\Sigma}_{\\boldsymbol{\\theta}})_{jj}}}\n$$\nThe required outputs are the MAP estimates $(k_{0, \\text{MAP}}, E_{a, \\text{MAP}}, c_{\\text{MAP}})$ and the correlation coefficients $(\\rho_{k_0,E_a}, \\rho_{k_0,c}, \\rho_{E_a,c})$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Solves the Bayesian inverse problem for SEI growth model parameters for three test cases.\n    \"\"\"\n    R = 8.314462618  # J/(mol.K)\n\n    # Prior hyperparameters\n    mu_logk0 = np.log(1e-6)\n    sigma_logk0 = 1.5\n    mu_Ea = 60000.0  # J/mol\n    sigma_Ea = 10000.0  # J/mol\n    mu_c = 1000.0  # Ah/m\n    sigma_c = 300.0  # Ah/m\n\n    priors = {\n        'means': np.array([mu_logk0, mu_Ea, mu_c]),\n        'stds': np.array([sigma_logk0, sigma_Ea, sigma_c])\n    }\n\n    # Test cases\n    test_cases = [\n        {\n            \"T\": np.array([298, 298, 298, 323, 323, 323, 348, 348, 348]),\n            \"t\": np.array([1e6, 2e6, 4e6, 1e6, 2e6, 4e6, 1e6, 2e6, 4e6]),\n            \"y\": np.array([0.00780, 0.01110, 0.01570, 0.02000, 0.02830, 0.03990, 0.04450, 0.06300, 0.08960]),\n            \"sigma\": 0.002\n        },\n        {\n            \"T\": np.array([273, 273, 273, 298, 298, 298, 323, 323, 323, 358, 358, 358]),\n            \"t\": np.array([2e6, 4e6, 8e6, 2e6, 4e6, 8e6, 2e6, 4e6, 8e6, 2e6, 4e6, 8e6]),\n            \"y\": np.array([0.00370, 0.00520, 0.00740, 0.01110, 0.01560, 0.02220, 0.02830, 0.03990, 0.05650, 0.08490, 0.11950, 0.16850]),\n            \"sigma\": 0.003\n        },\n        {\n            \"T\": np.array([298, 298, 348, 348]),\n            \"t\": np.array([2e6, 6e6, 2e6, 6e6]),\n            \"y\": np.array([0.01110, 0.01920, 0.06300, 0.10900]),\n            \"sigma\": 0.001\n        }\n    ]\n\n    results = []\n\n    for case in test_cases:\n        T_data, t_data, y_data, sigma = case[\"T\"], case[\"t\"], case[\"y\"], case[\"sigma\"]\n\n        def model(phi, T, t):\n            log_k0, Ea, c = phi\n            k_T = np.exp(log_k0) * np.exp(-Ea / (R * T))\n            return c * np.sqrt(2 * k_T * t)\n\n        def neg_log_posterior(phi, T, t, y, sigma_noise, prior_params):\n            # Log-likelihood term\n            y_model = model(phi, T, t)\n            log_likelihood_term = 0.5 * np.sum(((y - y_model) / sigma_noise)**2)\n            \n            # Log-prior term\n            prior_means = prior_params['means']\n            prior_stds = prior_params['stds']\n            log_prior_term = 0.5 * np.sum(((phi - prior_means) / prior_stds)**2)\n            \n            return log_likelihood_term + log_prior_term\n\n        # Initial guess from prior means\n        phi0 = priors['means']\n        \n        # Bounds for parameters (Ea > 0, c > 0)\n        bounds = [(None, None), (0, None), (0, None)]\n\n        # Find MAP estimate\n        opt_result = minimize(\n            neg_log_posterior,\n            phi0,\n            args=(T_data, t_data, y_data, sigma, priors),\n            method='L-BFGS-B',\n            bounds=bounds\n        )\n        phi_map = opt_result.x\n        log_k0_map, Ea_map, c_map = phi_map\n        k0_map = np.exp(log_k0_map)\n\n        # Compute posterior covariance using Laplace approximation\n        # 1. Jacobian of the model w.r.t phi\n        model_at_map = model(phi_map, T_data, t_data)\n        \n        # Derivatives of model w.r.t phi = (log_k0, Ea, c)\n        d_model_d_logk0 = model_at_map / 2.0\n        d_model_d_Ea = -model_at_map / (2.0 * R * T_data)\n        d_model_d_c = model_at_map / c_map if c_map != 0 else np.zeros_like(model_at_map)\n        \n        J_model = np.vstack([d_model_d_logk0, d_model_d_Ea, d_model_d_c]).T\n\n        # 2. Gauss-Newton Hessian of negative log-posterior\n        H_likelihood = (1 / sigma**2) * (J_model.T @ J_model)\n        H_prior = np.diag(1 / priors['stds']**2)\n        H_gn = H_likelihood + H_prior\n\n        # 3. Covariance in phi-space\n        cov_phi = np.linalg.inv(H_gn)\n\n        # 4. Transform covariance to theta-space (k0, Ea, c) via Delta method\n        J_transform = np.diag([k0_map, 1.0, 1.0])\n        cov_theta = J_transform @ cov_phi @ J_transform.T\n\n        # 5. Compute correlation coefficients\n        std_devs = np.sqrt(np.diag(cov_theta))\n        corr_matrix = cov_theta / np.outer(std_devs, std_devs)\n        \n        rho_k0_Ea = corr_matrix[0, 1]\n        rho_k0_c = corr_matrix[0, 2]\n        rho_Ea_c = corr_matrix[1, 2]\n\n        # Store results for this case\n        case_results = [k0_map, Ea_map, c_map, rho_k0_Ea, rho_k0_c, rho_Ea_c]\n        results.append(case_results)\n\n    # Format final output string\n    output_str = f\"[{','.join(f'[{\",\".join(map(str, row))}]' for row in results)}]\"\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "The final step in our hands-on sequence is to use our understanding of parametric uncertainty for a crucial validation task in engineering design: ensuring model predictions remain within a safe and physically admissible operating envelope. This exercise introduces constraint-based validation through Monte Carlo simulation, a technique to assess the reliability of a virtual prototype . You will propagate uncertainties through an electro-thermal model to quantify the probability of violating critical constraints on temperature, voltage, and state of charge, effectively transforming uncertainty analysis into a quantitative risk assessment.",
            "id": "3959907",
            "problem": "You will implement a virtual prototyping and validation workflow for a lithium-ion battery cell model under parametric uncertainty. The objective is to perform constraint-based validation by rejecting any simulation in which state variables exit physically admissible bounds, and to quantify the rejection frequency as a decimal for each provided test case. The program must propagate uncertainty through the model by sampling input parameters from specified distributions and simulating trajectories over time.\n\nThe fundamental base consists of conservation of charge and energy, and linear circuit dynamics, specified as follows:\n\n- Let the State of Charge (SOC) be denoted by $s(t)$, the cell temperature by $T(t)$ in kelvins, and a polarization state voltage by $v_{\\mathrm{rc}}(t)$ representing the Resistive-Capacitive (RC) dynamics of a single-element overpotential.\n- The battery is subject to a constant discharge current $I$ (in amperes) per simulation run, a series resistance $R$ (in ohms), nominal capacity $Q_{\\mathrm{nom}}$ (in coulombs), lumped thermal capacity $C_{\\mathrm{th}}$ (in joules per kelvin), and a lumped heat transfer coefficient-area product $hA$ (in watts per kelvin). The ambient temperature is $T_{\\mathrm{amb}}$ in kelvins.\n- The Open Circuit Voltage (OCV) function is $V_{\\mathrm{oc}}(s)$ given by a physically plausible monotonic function:\n$$\nV_{\\mathrm{oc}}(s) = 3.0 + 1.2\\,s - 0.1\\,e^{-10 s}.\n$$\n- Charge conservation yields the SOC dynamics:\n$$\n\\frac{ds}{dt} = -\\frac{I}{Q_{\\mathrm{nom}}}.\n$$\n- Energy balance yields the lumped thermal dynamics:\n$$\nC_{\\mathrm{th}} \\frac{dT}{dt} = I^2 R - hA \\left(T - T_{\\mathrm{amb}}\\right).\n$$\n- The RC polarization voltage evolves according to a linear time-invariant first-order model:\n$$\n\\frac{dv_{\\mathrm{rc}}}{dt} = -\\frac{1}{\\tau} v_{\\mathrm{rc}} + \\frac{\\alpha}{\\tau} I,\n$$\nwhere $\\tau$ is a time constant (in seconds) and $\\alpha$ is a dimensionless coupling coefficient.\n- The terminal voltage $V(t)$ is computed as:\n$$\nV(t) = V_{\\mathrm{oc}}(s(t)) - I R - v_{\\mathrm{rc}}(t).\n$$\n\nConstraint-based validation: A simulation trajectory is rejected if at any time $t$ it violates any of the following admissible bounds:\n- SOC bound: $s(t) \\in [0, 1]$ (dimensionless).\n- Temperature bound: $T(t) \\in [T_{\\min}, T_{\\max}]$ in kelvins.\n- Voltage bound: $V(t) \\in [V_{\\min}, V_{\\max}]$ in volts.\n\nUncertainty propagation: For each simulation run, sample uncertain parameters independently from the specified distributions (defined per test case below). Simulate the system over a finite horizon using time discretization and determine whether constraints are violated. Quantify the rejection frequency as the number of rejected runs divided by the total number of runs, expressed as a decimal (not a percentage). Use a fixed pseudo-random seed of $42$ for reproducibility.\n\nImplementation details:\n- Use explicit time-stepping with a fixed step $\\Delta t$ over horizon $[0, t_{\\mathrm{h}}]$.\n- Units must be strictly respected: $s$ is dimensionless, $I$ in amperes, $R$ in ohms, $Q_{\\mathrm{nom}}$ in coulombs, $T$ and $T_{\\mathrm{amb}}$ in kelvins, $C_{\\mathrm{th}}$ in joules per kelvin, $hA$ in watts per kelvin, $\\tau$ in seconds, $V$ in volts.\n- The initial conditions $s(0)$ and $T(0)$ must be sampled as specified per test case. The initial polarization state is $v_{\\mathrm{rc}}(0) = 0$.\n\nTest suite: Implement four test cases with the following parameterizations. In each case, perform $N$ independent runs and output the fraction of runs that are rejected due to constraint violations.\n\n- Case A (baseline, moderate stress):\n  - $N = 500$, $t_{\\mathrm{h}} = 600\\,\\mathrm{s}$, $\\Delta t = 0.5\\,\\mathrm{s}$, $\\tau = 20\\,\\mathrm{s}$, $\\alpha = 1$.\n  - Current $I \\sim \\mathrm{Uniform}(1, 8)$ (amperes).\n  - Resistance $R \\sim \\mathrm{Normal}(\\mu_R = 0.05, \\sigma_R = 0.01)$ (ohms), truncated below at $R_{\\min} = 0.01$.\n  - Capacity $Q_{\\mathrm{nom}} \\sim \\mathrm{Normal}(\\mu_Q = 10800, \\sigma_Q = 300)$ (coulombs), truncated below at $Q_{\\min} = 8000$.\n  - Thermal parameters: $hA \\sim \\mathrm{Normal}(\\mu_{hA} = 5.0, \\sigma_{hA} = 0.8)$ (watts per kelvin), truncated below at $hA_{\\min} = 1.0$; $C_{\\mathrm{th}} \\sim \\mathrm{Normal}(\\mu_C = 500, \\sigma_C = 50)$ (joules per kelvin), truncated below at $C_{\\min} = 300$.\n  - Initial conditions: $s(0) \\sim \\mathrm{Uniform}(0.2, 0.9)$; $T(0) \\sim \\mathrm{Normal}(\\mu_{T0} = 298, \\sigma_{T0} = 2)$ (kelvins), truncated below at $T_{\\mathrm{abs},\\min} = 273.15$.\n  - Ambient: $T_{\\mathrm{amb}} \\sim \\mathrm{Normal}(\\mu_{\\mathrm{amb}} = 298, \\sigma_{\\mathrm{amb}} = 2)$ (kelvins).\n  - Bounds: $T_{\\min} = 273.15\\,\\mathrm{K}$, $T_{\\max} = 335\\,\\mathrm{K}$, $V_{\\min} = 2.5\\,\\mathrm{V}$, $V_{\\max} = 4.2\\,\\mathrm{V}$.\n\n- Case B (high-current stress and extended horizon):\n  - $N = 500$, $t_{\\mathrm{h}} = 1200\\,\\mathrm{s}$, $\\Delta t = 0.5\\,\\mathrm{s}$, $\\tau = 20\\,\\mathrm{s}$, $\\alpha = 1$.\n  - Current $I \\sim \\mathrm{Uniform}(6, 12)$ (amperes).\n  - Resistance $R \\sim \\mathrm{Normal}(\\mu_R = 0.06, \\sigma_R = 0.012)$ (ohms), truncated below at $R_{\\min} = 0.015$.\n  - Capacity $Q_{\\mathrm{nom}} \\sim \\mathrm{Normal}(\\mu_Q = 10500, \\sigma_Q = 350)$ (coulombs), truncated below at $Q_{\\min} = 8000$.\n  - Thermal: $hA \\sim \\mathrm{Normal}(\\mu_{hA} = 4.0, \\sigma_{hA} = 0.7)$ (watts per kelvin), truncated below at $hA_{\\min} = 1.0$; $C_{\\mathrm{th}} \\sim \\mathrm{Normal}(\\mu_C = 450, \\sigma_C = 60)$ (joules per kelvin), truncated below at $C_{\\min} = 300$.\n  - Initial: $s(0) \\sim \\mathrm{Uniform}(0.3, 0.8)$; $T(0) \\sim \\mathrm{Normal}(\\mu_{T0} = 300, \\sigma_{T0} = 2.5)$ (kelvins), truncated below at $T_{\\mathrm{abs},\\min} = 273.15$.\n  - Ambient: $T_{\\mathrm{amb}} \\sim \\mathrm{Normal}(\\mu_{\\mathrm{amb}} = 300, \\sigma_{\\mathrm{amb}} = 3)$ (kelvins).\n  - Bounds: $T_{\\min} = 273.15\\,\\mathrm{K}$, $T_{\\max} = 335\\,\\mathrm{K}$, $V_{\\min} = 2.5\\,\\mathrm{V}$, $V_{\\max} = 4.2\\,\\mathrm{V}$.\n\n- Case C (low-current, conservative thermal management):\n  - $N = 400$, $t_{\\mathrm{h}} = 900\\,\\mathrm{s}$, $\\Delta t = 0.5\\,\\mathrm{s}$, $\\tau = 20\\,\\mathrm{s}$, $\\alpha = 1$.\n  - Current $I \\sim \\mathrm{Uniform}(0, 0.5)$ (amperes).\n  - Resistance $R \\sim \\mathrm{Normal}(\\mu_R = 0.045, \\sigma_R = 0.008)$ (ohms), truncated below at $R_{\\min} = 0.01$.\n  - Capacity $Q_{\\mathrm{nom}} \\sim \\mathrm{Normal}(\\mu_Q = 11200, \\sigma_Q = 250)$ (coulombs), truncated below at $Q_{\\min} = 9000$.\n  - Thermal: $hA \\sim \\mathrm{Normal}(\\mu_{hA} = 6.0, \\sigma_{hA} = 0.9)$ (watts per kelvin), truncated below at $hA_{\\min} = 1.0$; $C_{\\mathrm{th}} \\sim \\mathrm{Normal}(\\mu_C = 520, \\sigma_C = 40)$ (joules per kelvin), truncated below at $C_{\\min} = 350$.\n  - Initial: $s(0) \\sim \\mathrm{Uniform}(0.4, 0.95)$; $T(0) \\sim \\mathrm{Normal}(\\mu_{T0} = 297, \\sigma_{T0} = 1.5)$ (kelvins), truncated below at $T_{\\mathrm{abs},\\min} = 273.15$.\n  - Ambient: $T_{\\mathrm{amb}} \\sim \\mathrm{Normal}(\\mu_{\\mathrm{amb}} = 297, \\sigma_{\\mathrm{amb}} = 1.5)$ (kelvins).\n  - Bounds: $T_{\\min} = 273.15\\,\\mathrm{K}$, $T_{\\max} = 335\\,\\mathrm{K}$, $V_{\\min} = 2.5\\,\\mathrm{V}$, $V_{\\max} = 4.2\\,\\mathrm{V}$.\n\n- Case D (boundary SOC near depletion with moderate current):\n  - $N = 300$, $t_{\\mathrm{h}} = 1800\\,\\mathrm{s}$, $\\Delta t = 0.5\\,\\mathrm{s}$, $\\tau = 20\\,\\mathrm{s}$, $\\alpha = 1$.\n  - Current $I \\sim \\mathrm{Uniform}(2, 4)$ (amperes).\n  - Resistance $R \\sim \\mathrm{Normal}(\\mu_R = 0.05, \\sigma_R = 0.01)$ (ohms), truncated below at $R_{\\min} = 0.01$.\n  - Capacity $Q_{\\mathrm{nom}} \\sim \\mathrm{Normal}(\\mu_Q = 10800, \\sigma_Q = 300)$ (coulombs), truncated below at $Q_{\\min} = 9000$.\n  - Thermal: $hA \\sim \\mathrm{Normal}(\\mu_{hA} = 5.0, \\sigma_{hA} = 0.8)$ (watts per kelvin), truncated below at $hA_{\\min} = 1.0$; $C_{\\mathrm{th}} \\sim \\mathrm{Normal}(\\mu_C = 500, \\sigma_C = 50)$ (joules per kelvin), truncated below at $C_{\\min} = 300$.\n  - Initial: $s(0) \\sim \\mathrm{Uniform}(0.05, 0.15)$; $T(0) \\sim \\mathrm{Normal}(\\mu_{T0} = 298, \\sigma_{T0} = 2)$ (kelvins), truncated below at $T_{\\mathrm{abs},\\min} = 273.15$.\n  - Ambient: $T_{\\mathrm{amb}} \\sim \\mathrm{Normal}(\\mu_{\\mathrm{amb}} = 298, \\sigma_{\\mathrm{amb}} = 2)$ (kelvins).\n  - Bounds: $T_{\\min} = 273.15\\,\\mathrm{K}$, $T_{\\max} = 335\\,\\mathrm{K}$, $V_{\\min} = 2.5\\,\\mathrm{V}$, $V_{\\max} = 4.2\\,\\mathrm{V}$.\n\nNumerical requirements:\n- Express all computed rejection frequencies as decimals in $[0,1]$.\n- The final program output must be a single line containing a comma-separated list of the four rejection frequencies enclosed in square brackets, for example, $[0.12,0.34,0.00,0.67]$, with no additional text.\n\nAngle units do not appear in this problem. Ensure all temperature values are in kelvins, currents in amperes, resistances in ohms, capacities in coulombs, energy-related parameters in joules per kelvin or watts per kelvin as specified, time in seconds, and voltage in volts.",
            "solution": "The problem requires the implementation of a virtual prototyping workflow for a lithium-ion battery cell model to quantify the frequency of constraint violations under parametric uncertainty. This task is accomplished through a Monte Carlo simulation. For each of four specified test cases, a large number of simulations ($N$) are performed. In each simulation run, the model's parameters are sampled from specified probability distributions. The system's state is then evolved over time using numerical integration. The trajectory of each state variable is checked against predefined physical and operational constraints. A run is deemed \"rejected\" if any constraint is violated at any point in time. The final output for each case is the rejection frequency, calculated as the ratio of rejected runs to the total number of runs, $N$.\n\nThe model consists of a system of three coupled ordinary differential equations (ODEs) describing the evolution of the State of Charge $s(t)$, temperature $T(t)$, and a polarization voltage $v_{\\mathrm{rc}}(t)$.\n\nThe governing equations are:\n1.  **State of Charge dynamics (Charge Conservation):**\n    $$\n    \\frac{ds}{dt} = -\\frac{I}{Q_{\\mathrm{nom}}}\n    $$\n    where $I$ is the discharge current and $Q_{\\mathrm{nom}}$ is the nominal capacity.\n\n2.  **Temperature dynamics (Energy Balance):**\n    $$\n    C_{\\mathrm{th}} \\frac{dT}{dt} = I^2 R - hA \\left(T - T_{\\mathrm{amb}}\\right)\n    $$\n    where $C_{\\mathrm{th}}$ is the thermal capacity, $R$ is the internal resistance, $hA$ is the heat transfer coefficient-area product, and $T_{\\mathrm{amb}}$ is the ambient temperature. The term $I^2 R$ represents Joule heating.\n\n3.  **Polarization dynamics (RC Circuit):**\n    $$\n    \\frac{dv_{\\mathrm{rc}}}{dt} = -\\frac{1}{\\tau} v_{\\mathrm{rc}} + \\frac{\\alpha}{\\tau} I\n    $$\n    where $\\tau$ is the time constant and $\\alpha$ is a coupling coefficient for the single-element Resistive-Capacitive (RC) pair.\n\nThe terminal voltage $V(t)$ is an algebraic function of the states:\n$$\nV(t) = V_{\\mathrm{oc}}(s(t)) - I R - v_{\\mathrm{rc}}(t)\n$$\nwith the Open Circuit Voltage (OCV) given by:\n$$\nV_{\\mathrm{oc}}(s) = 3.0 + 1.2\\,s - 0.1\\,e^{-10 s}\n$$\n\nThe solution methodology involves these key steps:\n\n**1. Numerical Integration:**\nThe system of ODEs is solved numerically using the explicit Euler method with a fixed time step $\\Delta t$. For a state vector $\\mathbf{x}_k = [s_k, T_k, v_{\\mathrm{rc},k}]^T$ at time $t_k = k \\Delta t$, the state at the next time step $t_{k+1}$ is computed as:\n$$\ns_{k+1} = s_k - \\frac{I}{Q_{\\mathrm{nom}}} \\Delta t\n$$\n$$\nT_{k+1} = T_k + \\frac{\\Delta t}{C_{\\mathrm{th}}} \\left(I^2 R - hA (T_k - T_{\\mathrm{amb}})\\right)\n$$\n$$\nv_{\\mathrm{rc},k+1} = v_{\\mathrm{rc},k} \\left(1 - \\frac{\\Delta t}{\\tau}\\right) + I \\frac{\\alpha \\Delta t}{\\tau}\n$$\n\n**2. Uncertainty Propagation:**\nFor each of the $N$ runs in a given test case, the uncertain parameters ($I, R, Q_{\\mathrm{nom}}, C_{\\mathrm{th}}, hA$) and initial conditions ($s(0), T(0)$) are sampled from their respective specified distributions (Uniform or Normal, possibly truncated). A single pseudo-random number generator is initialized with a fixed seed of $42$ to ensure reproducibility of the entire sequence of random samples.\n-   For Uniform distributions $\\mathrm{Uniform}(a, b)$, samples are drawn using standard library functions.\n-   For Normal distributions $\\mathrm{Normal}(\\mu, \\sigma)$, samples are drawn similarly.\n-   For truncated Normal distributions, with a mean $\\mu$, standard deviation $\\sigma$, and a lower bound $x_{\\min}$, we use specialized functions, such as `scipy.stats.truncnorm`. The standardized lower bound for sampling is calculated as $a = (x_{\\min} - \\mu) / \\sigma$, with the upper bound $b = \\infty$.\n\n**3. Constraint Validation:**\nIn each run, at every time step $t_k$ from $t_0 = 0$ to the simulation horizon $t_{\\mathrm{h}}$, the state variables and the computed terminal voltage are checked against the following constraints:\n-   $s(t_k) \\in [0, 1]$\n-   $T(t_k) \\in [T_{\\min}, T_{\\max}]$\n-   $V(t_k) \\in [V_{\\min}, V_{\\max}]$\n\nIf any of these conditions are violated at any $t_k$, the simulation for that run is immediately terminated, and the run is counted as \"rejected\".\n\n**4. Rejection Frequency Quantification:**\nAfter completing all $N$ runs for a test case, the rejection frequency $f_{\\mathrm{reject}}$ is computed as:\n$$\nf_{\\mathrm{reject}} = \\frac{\\text{Number of rejected runs}}{N}\n$$\nThis process is repeated for all four test cases, and the resulting frequencies are collected.\n\n**Implementation Summary:**\nThe algorithm is implemented in Python using the `numpy` library for numerical operations and `scipy.stats.truncnorm` for sampling from truncated normal distributions. A main function orchestrates the simulations for the four test cases. A helper function is used to handle the sampling of truncated normal variables cleanly. The main simulation logic iterates through each of the $N$ runs. Within each run, another loop iterates through the time steps, updating the state and performing constraint checks.\nThe initial conditions for each run are $s(0)$ and $T(0)$ sampled from their distributions, and $v_{\\mathrm{rc}}(0) = 0$. The check for constraint violation is performed starting at $t=0$ to ensure the initial state itself is valid.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import truncnorm\n\ndef solve():\n    \"\"\"\n    Solves the battery virtual prototyping problem for all test cases.\n    \"\"\"\n    \n    # A single random number generator for reproducibility across all cases.\n    rng = np.random.default_rng(42)\n\n    def sample_truncated_normal(mu, sigma, lower_bound, size=1):\n        \"\"\"\n        Samples from a lower-truncated normal distribution.\n        \"\"\"\n        if sigma = 0:\n            return np.full(size, lower_bound)\n        a = (lower_bound - mu) / sigma\n        b = np.inf\n        return truncnorm.rvs(a, b, loc=mu, scale=sigma, size=size, random_state=rng)\n\n    def run_simulation(case_params, rng_instance):\n        \"\"\"\n        Runs a Monte Carlo simulation for a single test case.\n        \"\"\"\n        N = case_params['N']\n        t_h = case_params['t_h']\n        dt = case_params['dt']\n        tau = case_params['tau']\n        alpha = case_params['alpha']\n        \n        # Unpack constraint boundaries\n        T_min, T_max = case_params['bounds']['T']\n        V_min, V_max = case_params['bounds']['V']\n        s_min, s_max = 0.0, 1.0\n\n        # Unpack sampling parameters\n        p = case_params['params']\n        \n        rejected_count = 0\n        for _ in range(N):\n            # Sample parameters for this single run\n            I = rng_instance.uniform(p['I'][0], p['I'][1])\n            R = sample_truncated_normal(p['R']['mu'], p['R']['sigma'], p['R']['min'])[0]\n            Q_nom = sample_truncated_normal(p['Q_nom']['mu'], p['Q_nom']['sigma'], p['Q_nom']['min'])[0]\n            hA = sample_truncated_normal(p['hA']['mu'], p['hA']['sigma'], p['hA']['min'])[0]\n            C_th = sample_truncated_normal(p['C_th']['mu'], p['C_th']['sigma'], p['C_th']['min'])[0]\n            \n            s0 = rng_instance.uniform(p['s0'][0], p['s0'][1])\n            T0 = sample_truncated_normal(p['T0']['mu'], p['T0']['sigma'], p['T0']['min'])[0]\n            T_amb = rng_instance.normal(p['T_amb']['mu'], p['T_amb']['sigma'])\n            \n            # Initialize states\n            s = s0\n            T = T0\n            v_rc = 0.0\n\n            is_rejected = False\n            num_steps = int(t_h / dt)\n\n            for step in range(num_steps + 1):\n                # Calculate OCV and Terminal Voltage at current step\n                V_oc = 3.0 + 1.2 * s - 0.1 * np.exp(-10.0 * s)\n                V = V_oc - I * R - v_rc\n\n                # Check constraints\n                if not (s_min = s = s_max and T_min = T = T_max and V_min = V = V_max):\n                    is_rejected = True\n                    break\n\n                # If it's the last step, no need to update state for the next step.\n                if step == num_steps:\n                    break\n                \n                # Update states for the next step using explicit Euler method\n                s = s - (I / Q_nom) * dt\n                T = T + (dt / C_th) * (I**2 * R - hA * (T - T_amb))\n                v_rc = v_rc * (1.0 - dt / tau) + (alpha * dt / tau) * I\n\n            if is_rejected:\n                rejected_count += 1\n        \n        return rejected_count / N\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A\n        {\n            'N': 500, 't_h': 600.0, 'dt': 0.5, 'tau': 20.0, 'alpha': 1.0,\n            'bounds': {'T': (273.15, 335.0), 'V': (2.5, 4.2)},\n            'params': {\n                'I': (1.0, 8.0),\n                'R': {'mu': 0.05, 'sigma': 0.01, 'min': 0.01},\n                'Q_nom': {'mu': 10800, 'sigma': 300, 'min': 8000},\n                'hA': {'mu': 5.0, 'sigma': 0.8, 'min': 1.0},\n                'C_th': {'mu': 500, 'sigma': 50, 'min': 300},\n                's0': (0.2, 0.9),\n                'T0': {'mu': 298, 'sigma': 2, 'min': 273.15},\n                'T_amb': {'mu': 298, 'sigma': 2},\n            }\n        },\n        # Case B\n        {\n            'N': 500, 't_h': 1200.0, 'dt': 0.5, 'tau': 20.0, 'alpha': 1.0,\n            'bounds': {'T': (273.15, 335.0), 'V': (2.5, 4.2)},\n            'params': {\n                'I': (6.0, 12.0),\n                'R': {'mu': 0.06, 'sigma': 0.012, 'min': 0.015},\n                'Q_nom': {'mu': 10500, 'sigma': 350, 'min': 8000},\n                'hA': {'mu': 4.0, 'sigma': 0.7, 'min': 1.0},\n                'C_th': {'mu': 450, 'sigma': 60, 'min': 300},\n                's0': (0.3, 0.8),\n                'T0': {'mu': 300, 'sigma': 2.5, 'min': 273.15},\n                'T_amb': {'mu': 300, 'sigma': 3},\n            }\n        },\n        # Case C\n        {\n            'N': 400, 't_h': 900.0, 'dt': 0.5, 'tau': 20.0, 'alpha': 1.0,\n            'bounds': {'T': (273.15, 335.0), 'V': (2.5, 4.2)},\n            'params': {\n                'I': (0.0, 0.5),\n                'R': {'mu': 0.045, 'sigma': 0.008, 'min': 0.01},\n                'Q_nom': {'mu': 11200, 'sigma': 250, 'min': 9000},\n                'hA': {'mu': 6.0, 'sigma': 0.9, 'min': 1.0},\n                'C_th': {'mu': 520, 'sigma': 40, 'min': 350},\n                's0': (0.4, 0.95),\n                'T0': {'mu': 297, 'sigma': 1.5, 'min': 273.15},\n                'T_amb': {'mu': 297, 'sigma': 1.5},\n            }\n        },\n        # Case D\n        {\n            'N': 300, 't_h': 1800.0, 'dt': 0.5, 'tau': 20.0, 'alpha': 1.0,\n            'bounds': {'T': (273.15, 335.0), 'V': (2.5, 4.2)},\n            'params': {\n                'I': (2.0, 4.0),\n                'R': {'mu': 0.05, 'sigma': 0.01, 'min': 0.01},\n                'Q_nom': {'mu': 10800, 'sigma': 300, 'min': 9000},\n                'hA': {'mu': 5.0, 'sigma': 0.8, 'min': 1.0},\n                'C_th': {'mu': 500, 'sigma': 50, 'min': 300},\n                's0': (0.05, 0.15),\n                'T0': {'mu': 298, 'sigma': 2, 'min': 273.15},\n                'T_amb': {'mu': 298, 'sigma': 2},\n            }\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_simulation(case, rng)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.10f}'.rstrip('0').rstrip('.') for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}