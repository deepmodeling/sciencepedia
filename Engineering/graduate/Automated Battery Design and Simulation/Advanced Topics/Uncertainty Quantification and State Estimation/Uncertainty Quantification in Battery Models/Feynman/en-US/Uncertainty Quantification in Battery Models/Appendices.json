{
    "hands_on_practices": [
        {
            "introduction": "Understanding how variability in manufacturing or material properties affects battery performance is a central goal of uncertainty quantification. This first exercise introduces the delta method, a foundational technique for propagating uncertainty through a model. By linearizing the model around the mean values of its inputs, you will calculate the resulting variance in cell voltage, providing a direct link between parameter sensitivities and output uncertainty .",
            "id": "3959245",
            "problem": "A lithium-ion cell operated at a fixed current density is described by a reduced physics model in which the cell voltage $V$ depends on the solid-phase lithium diffusivity $D_s$ and the surface reaction rate constant $k$. In Uncertainty Quantification (UQ), treat $D_s$ and $k$ as random inputs arising from manufacturing variability, modeled as independent normal random variables $D_s \\sim \\mathcal{N}(\\mu_{D_s},\\sigma_{D_s}^{2})$ and $k \\sim \\mathcal{N}(\\mu_{k},\\sigma_{k}^{2})$. The operating-point sensitivities of the voltage to these parameters, obtained from a small-signal linearization of the reduced model built on Fickian diffusion and Butler–Volmer (BV) kinetics, are computed at $(\\mu_{D_s},\\mu_{k})$ via high-fidelity simulation. The values are\n$$\\left.\\frac{\\partial V}{\\partial D_s}\\right|_{(\\mu_{D_s},\\mu_{k})} = -3.0 \\times 10^{5}\\ \\mathrm{V\\,s/m^{2}}, \\quad \\left.\\frac{\\partial V}{\\partial k}\\right|_{(\\mu_{D_s},\\mu_{k})} = -3.0 \\times 10^{3}\\ \\mathrm{V\\,s/m}.$$\nAssume the following parameter statistics, consistent with advanced electrode materials operated under moderate current:\n$$\\mu_{D_s} = 1.0 \\times 10^{-14}\\ \\mathrm{m^{2}/s}, \\quad \\sigma_{D_s} = 2.0 \\times 10^{-15}\\ \\mathrm{m^{2}/s},$$\n$$\\mu_{k} = 1.0 \\times 10^{-10}\\ \\mathrm{m/s}, \\quad \\sigma_{k} = 2.0 \\times 10^{-11}\\ \\mathrm{m/s},$$\nand that $D_s$ and $k$ are independent. Using a first-order Taylor expansion of $V(D_s,k)$ about $(\\mu_{D_s},\\mu_k)$ and the law of variance under independence, derive and compute the first-order delta-method approximation to the voltage variance $\\mathrm{Var}[V]$. Express the final variance in $\\mathrm{V^{2}}$ and round your answer to $4$ significant figures. The final answer must be a single real number.",
            "solution": "The user wants to solve the following problem:\nA lithium-ion cell operated at a fixed current density is described by a reduced physics model in which the cell voltage $V$ depends on the solid-phase lithium diffusivity $D_s$ and the surface reaction rate constant $k$. In Uncertainty Quantification (UQ), treat $D_s$ and $k$ as random inputs arising from manufacturing variability, modeled as independent normal random variables $D_s \\sim \\mathcal{N}(\\mu_{D_s},\\sigma_{D_s}^{2})$ and $k \\sim \\mathcal{N}(\\mu_{k},\\sigma_{k}^{2})$. The operating-point sensitivities of the voltage to these parameters, obtained from a small-signal linearization of the reduced model built on Fickian diffusion and Butler–Volmer (BV) kinetics, are computed at $(\\mu_{D_s},\\mu_{k})$ via high-fidelity simulation. The values are\n$$\\left.\\frac{\\partial V}{\\partial D_s}\\right|_{(\\mu_{D_s},\\mu_{k})} = -3.0 \\times 10^{5}\\ \\mathrm{V\\,s/m^{2}}, \\quad \\left.\\frac{\\partial V}{\\partial k}\\right|_{(\\mu_{D_s},\\mu_{k})} = -3.0 \\times 10^{3}\\ \\mathrm{V\\,s/m}.$$\nAssume the following parameter statistics, consistent with advanced electrode materials operated under moderate current:\n$$\\mu_{D_s} = 1.0 \\times 10^{-14}\\ \\mathrm{m^{2}/s}, \\quad \\sigma_{D_s} = 2.0 \\times 10^{-15}\\ \\mathrm{m^{2}/s},$$\n$$\\mu_{k} = 1.0 \\times 10^{-10}\\ \\mathrm{m/s}, \\quad \\sigma_{k} = 2.0 \\times 10^{-11}\\ \\mathrm{m/s},$$\nand that $D_s$ and $k$ are independent. Using a first-order Taylor expansion of $V(D_s,k)$ about $(\\mu_{D_s},\\mu_k)$ and the law of variance under independence, derive and compute the first-order delta-method approximation to the voltage variance $\\mathrm{Var}[V]$. Express the final variance in $\\mathrm{V^{2}}$ and round your answer to $4$ significant figures. The final answer must be a single real number.\n\nThe problem is first validated to ensure it is scientifically grounded, well-posed, and all necessary data are provided and consistent. The problem describes a standard application of uncertainty propagation using the delta method in the context of battery modeling. The physical parameters ($D_s$, $k$), their statistics ($\\mu$, $\\sigma$), and their sensitivities are given with consistent physical units. The assumption of a normal distribution for positive physical quantities is a common and acceptable approximation in such analyses, especially when the mean is several standard deviations away from zero (here, $\\mu = 5\\sigma$ for both parameters). The problem is self-contained, unambiguous, and scientifically sound. Thus, the problem is deemed valid and a solution will be provided.\n\nThe objective is to compute the variance of the cell voltage, $\\mathrm{Var}[V]$, using a first-order approximation. The cell voltage $V$ is a function of two independent random variables, the solid-phase diffusivity $D_s$ and the surface reaction rate constant $k$. This task is accomplished using the delta method, which approximates the variance of a function of random variables using a Taylor series expansion of the function around the mean of the variables.\n\nLet the voltage be represented by the function $V(D_s, k)$. The first-order Taylor expansion of $V(D_s, k)$ about the mean values of the parameters, $(\\mu_{D_s}, \\mu_k)$, is given by:\n$$V(D_s, k) \\approx V(\\mu_{D_s}, \\mu_k) + \\left.\\frac{\\partial V}{\\partial D_s}\\right|_{(\\mu_{D_s}, \\mu_k)} (D_s - \\mu_{D_s}) + \\left.\\frac{\\partial V}{\\partial k}\\right|_{(\\mu_{D_s}, \\mu_k)} (k - \\mu_k)$$\nTo find the variance of $V$, we take the variance of this approximate expression:\n$$\\mathrm{Var}[V] \\approx \\mathrm{Var}\\left[V(\\mu_{D_s}, \\mu_k) + \\left.\\frac{\\partial V}{\\partial D_s}\\right|_{(\\mu_{D_s}, \\mu_k)} (D_s - \\mu_{D_s}) + \\left.\\frac{\\partial V}{\\partial k}\\right|_{(\\mu_{D_s}, \\mu_k)} (k - \\mu_k)\\right]$$\nWe use the general property of variance for a linear combination of random variables $X$ and $Y$ with constants $a, b, c$: $\\mathrm{Var}[aX + bY + c] = a^2 \\mathrm{Var}[X] + b^2 \\mathrm{Var}[Y] + 2ab \\mathrm{Cov}[X, Y]$.\nIn our expression, the term $V(\\mu_{D_s}, \\mu_k)$ is a constant, and the sensitivities $\\left.\\frac{\\partial V}{\\partial D_s}\\right|_{(\\mu_{D_s}, \\mu_k)}$ and $\\left.\\frac{\\partial V}{\\partial k}\\right|_{(\\mu_{D_s}, \\mu_k)}$ are also constants evaluated at the mean. The terms $- \\mu_{D_s}$ and $- \\mu_k$ are constant shifts. Since variance is invariant to constant shifts, the expression simplifies.\nThe problem states that $D_s$ and $k$ are independent random variables. A consequence of independence is that their covariance is zero, i.e., $\\mathrm{Cov}[D_s, k] = 0$. Applying these properties, the variance of the voltage is approximated by:\n$$\\mathrm{Var}[V] \\approx \\left(\\left.\\frac{\\partial V}{\\partial D_s}\\right|_{(\\mu_{D_s}, \\mu_k)}\\right)^2 \\mathrm{Var}[D_s] + \\left(\\left.\\frac{\\partial V}{\\partial k}\\right|_{(\\mu_{D_s}, \\mu_k)}\\right)^2 \\mathrm{Var}[k]$$\nThe variances of the input parameters are the squares of their standard deviations: $\\mathrm{Var}[D_s] = \\sigma_{D_s}^2$ and $\\mathrm{Var}[k] = \\sigma_k^2$. Substituting these into the equation gives the final formula for the first-order approximation of the voltage variance:\n$$\\mathrm{Var}[V] \\approx \\left(\\left.\\frac{\\partial V}{\\partial D_s}\\right|_{(\\mu_{D_s}, \\mu_k)}\\right)^2 \\sigma_{D_s}^2 + \\left(\\left.\\frac{\\partial V}{\\partial k}\\right|_{(\\mu_{D_s}, \\mu_k)}\\right)^2 \\sigma_k^2$$\nNow, we substitute the given numerical values into this formula.\nThe given sensitivities are:\n$$S_{D_s} = \\left.\\frac{\\partial V}{\\partial D_s}\\right|_{(\\mu_{D_s}, \\mu_k)} = -3.0 \\times 10^{5}\\ \\mathrm{V\\,s/m^{2}}$$\n$$S_k = \\left.\\frac{\\partial V}{\\partial k}\\right|_{(\\mu_{D_s}, \\mu_k)} = -3.0 \\times 10^{3}\\ \\mathrm{V\\,s/m}$$\nThe given standard deviations are:\n$$\\sigma_{D_s} = 2.0 \\times 10^{-15}\\ \\mathrm{m^{2}/s}$$\n$$\\sigma_k = 2.0 \\times 10^{-11}\\ \\mathrm{m/s}$$\nWe calculate the two terms for the variance contribution separately.\nThe contribution from the uncertainty in $D_s$ is:\n$$\\mathrm{Var}_{D_s}[V] = (S_{D_s})^2 \\sigma_{D_s}^2 = (-3.0 \\times 10^{5})^2 (2.0 \\times 10^{-15})^2$$\n$$\\mathrm{Var}_{D_s}[V] = (9.0 \\times 10^{10}) \\times (4.0 \\times 10^{-30})$$\n$$\\mathrm{Var}_{D_s}[V] = 36.0 \\times 10^{-20} = 3.6 \\times 10^{-19}\\ \\mathrm{V^2}$$\nThe contribution from the uncertainty in $k$ is:\n$$\\mathrm{Var}_k[V] = (S_k)^2 \\sigma_k^2 = (-3.0 \\times 10^{3})^2 (2.0 \\times 10^{-11})^2$$\n$$\\mathrm{Var}_k[V] = (9.0 \\times 10^{6}) \\times (4.0 \\times 10^{-22})$$\n$$\\mathrm{Var}_k[V] = 36.0 \\times 10^{-16} = 3.6 \\times 10^{-15}\\ \\mathrm{V^2}$$\nThe total variance is the sum of these two contributions:\n$$\\mathrm{Var}[V] \\approx \\mathrm{Var}_{D_s}[V] + \\mathrm{Var}_k[V]$$\n$$\\mathrm{Var}[V] \\approx 3.6 \\times 10^{-19}\\ \\mathrm{V^2} + 3.6 \\times 10^{-15}\\ \\mathrm{V^2}$$\nTo sum these numbers, we express them with the same power of $10$:\n$$\\mathrm{Var}[V] \\approx (0.00036 \\times 10^{-15})\\ \\mathrm{V^2} + (3.6 \\times 10^{-15})\\ \\mathrm{V^2}$$\n$$\\mathrm{Var}[V] \\approx (0.00036 + 3.6) \\times 10^{-15}\\ \\mathrm{V^2}$$\n$$\\mathrm{Var}[V] \\approx 3.60036 \\times 10^{-15}\\ \\mathrm{V^2}$$\nThe problem requires the final answer to be rounded to $4$ significant figures.\n$$\\mathrm{Var}[V] \\approx 3.600 \\times 10^{-15}\\ \\mathrm{V^2}$$\nThis is the first-order delta-method approximation to the voltage variance.",
            "answer": "$$\n\\boxed{3.600 \\times 10^{-15}}\n$$"
        },
        {
            "introduction": "To design robust batteries, we need to know not just that uncertainty exists, but which parameters contribute most to it. This practice moves from local sensitivity to global sensitivity analysis by deriving an estimator for Sobol' indices, which partition the output variance into contributions from each uncertain input. Working through the derivation for the Saltelli sampling scheme will give you a deep understanding of how to rigorously identify the most critical parameters in a complex model .",
            "id": "3959226",
            "problem": "Consider a physics-based lithium-ion cell model used in automated battery design, where the predicted quantity of interest is the terminal voltage at a prescribed time under constant-current discharge, denoted by a deterministic mapping $f(\\boldsymbol{x})$ from a vector of uncertain, design-relevant inputs $\\boldsymbol{x} = (x_{1}, \\dots, x_{d})$ to a real number. Assume the inputs $(x_{1}, \\dots, x_{d})$ are independent random variables with known marginal probability laws. Examples of such inputs include the solid-phase diffusion coefficient $D_{s}$, the reaction rate constant $k_{0}$, the contact resistance $R_{c}$, and parameters of the open-circuit voltage curve, but the explicit model form $f$ need not be specified in this problem. Let $X = (X_{1}, \\dots, X_{d})$ be a random vector with the same distribution as $\\boldsymbol{x}$, and let $\\mu = \\mathbb{E}[f(X)]$ and $\\sigma^{2} = \\mathbb{V}[f(X)]$.\n\nThe first-order Sobol’ sensitivity index $S_{i}$ for input $X_{i}$ is defined from the law of total variance as\n$$\nS_{i} = \\frac{\\mathbb{V}\\!\\left(\\mathbb{E}\\!\\left[f(X)\\mid X_{i}\\right]\\right)}{\\mathbb{V}\\!\\left(f(X)\\right)}.\n$$\n\nUsing only the independence of inputs, the law of total expectation and variance, and the principle of Monte Carlo (MC) estimation for expectations of independent and identically distributed (IID) samples, derive a Monte Carlo estimator for the first-order Sobol’ index $S_{i}$ based on Saltelli’s sampling scheme, described as follows. Draw two IID sample matrices\n$$\n\\mathbf{A} = \\begin{pmatrix}\n\\mathbf{a}^{(1)}\\\\\n\\vdots\\\\\n\\mathbf{a}^{(N)}\n\\end{pmatrix}\n\\in \\mathbb{R}^{N\\times d}\n\\quad \\text{and} \\quad\n\\mathbf{B} = \\begin{pmatrix}\n\\mathbf{b}^{(1)}\\\\\n\\vdots\\\\\n\\mathbf{b}^{(N)}\n\\end{pmatrix}\n\\in \\mathbb{R}^{N\\times d},\n$$\nwhere each row $\\mathbf{a}^{(k)}$ and $\\mathbf{b}^{(k)}$ is drawn IID from the joint distribution of $X$. For each $i \\in \\{1, \\dots, d\\}$, construct the hybrid matrix $\\mathbf{A}_{B}^{(i)}$ whose $k$-th row $\\mathbf{a}_{b}^{(i,k)}$ equals $\\mathbf{a}^{(k)}$ with the $i$-th component replaced by the $i$-th component of $\\mathbf{b}^{(k)}$. You have access to the model evaluations $f(\\mathbf{a}^{(k)})$, $f(\\mathbf{b}^{(k)})$, and $f(\\mathbf{a}_{b}^{(i,k)})$ for $k=1,\\dots,N$.\n\nStarting from the fundamental definition of $S_{i}$ and the properties of conditional expectation under independence, show how Saltelli’s construction yields an empirical estimator. Provide your final estimator as a single closed-form analytic expression in terms of the available model evaluations and $N$. Then, discuss the estimator’s finite-sample bias and variance properties, identifying the source of any bias and the scaling of the variance with $N$ based on first principles. No rounding is required. The final estimator has no physical units: express it as a unitless analytic expression.",
            "solution": "The problem requires the derivation of a Monte Carlo estimator for the first-order Sobol' sensitivity index, $S_{i}$, using Saltelli's sampling scheme, and a discussion of the estimator's properties.\n\nThe first-order Sobol' index for an input $X_{i}$ is defined as the fraction of the total variance of the model output $f(X)$ that can be attributed to the variance of $X_{i}$ alone:\n$$\nS_{i} = \\frac{\\mathbb{V}\\!\\left(\\mathbb{E}\\!\\left[f(X)\\mid X_{i}\\right]\\right)}{\\mathbb{V}\\!\\left(f(X)\\right)}\n$$\nLet's denote the numerator as $V_{i} = \\mathbb{V}\\!\\left(\\mathbb{E}\\!\\left[f(X)\\mid X_{i}\\right]\\right)$ and the denominator as $V = \\mathbb{V}\\!\\left(f(X)\\right)$. We need to construct estimators for both $V_{i}$ and $V$ using the provided sample matrices $\\mathbf{A}$ and $\\mathbf{B}$.\n\nLet $A$ and $B$ be two independent random vectors representing a single draw from the joint distribution of $X=(X_1, \\dots, X_d)$. The rows of the matrices $\\mathbf{A}$ and $\\mathbf{B}$, denoted $\\mathbf{a}^{(k)}$ and $\\mathbf{b}^{(k)}$ for $k=1, \\dots, N$, are $N$ independent realizations of $A$ and $B$. Let $A_{\\sim i}$ and $B_{\\sim i}$ denote the vectors $A$ and $B$ with their $i$-th component removed. Let $A_i$ and $B_i$ be the $i$-th components of $A$ and $B$. The hybrid vector $\\mathbf{a}_{b}^{(i,k)}$ is a realization of a random vector we can denote as $A_{B}^{(i)}$, which is constructed as $(A_1, \\dots, A_{i-1}, B_i, A_{i+1}, \\dots, A_d)$.\n\nFirst, we derive an alternative expression for the numerator, $V_{i}$. Using the definition of variance, $\\mathbb{V}(Y) = \\mathbb{E}[Y^2] - (\\mathbb{E}[Y])^2$, we have:\n$$\nV_{i} = \\mathbb{E}\\left[\\left(\\mathbb{E}\\left[f(X)\\mid X_{i}\\right]\\right)^2\\right] - \\left(\\mathbb{E}\\left[\\mathbb{E}\\left[f(X)\\mid X_{i}\\right]\\right]\\right)^2\n$$\nBy the law of total expectation, the second term simplifies: $\\mathbb{E}\\left[\\mathbb{E}\\left[f(X)\\mid X_{i}\\right]\\right] = \\mathbb{E}[f(X)] = \\mu$. So, the numerator is:\n$$\nV_{i} = \\mathbb{E}\\left[\\left(\\mathbb{E}\\left[f(X)\\mid X_{i}\\right]\\right)^2\\right] - \\mu^2\n$$\nThe main challenge is to find an estimable expression for the first term, $\\mathbb{E}\\left[\\left(\\mathbb{E}\\left[f(X)\\mid X_{i}\\right]\\right)^2\\right]$. Let's analyze the expectation of a product of model evaluations constructed from the Saltelli scheme. Consider the product $f(B) f(A_B^{(i)})$. The expectation is taken over the joint distribution of $A$ and $B$.\n$$\n\\mathbb{E}\\left[ f(B) f(A_B^{(i)}) \\right] = \\mathbb{E}\\left[ f(B_i, B_{\\sim i}) f(A_{\\sim i}, B_i) \\right]\n$$\nWe can compute this expectation by conditioning on $B_i$. Since $A$ and $B$ are independent, $A_{\\sim i}$ and $B_{\\sim i}$ are also independent.\n$$\n\\mathbb{E}\\left[ f(B) f(A_B^{(i)}) \\right] = \\mathbb{E}_{B_i}\\left[ \\mathbb{E}_{A_{\\sim i}, B_{\\sim i}}\\left[ f(B_i, B_{\\sim i}) f(A_{\\sim i}, B_i) \\mid B_i \\right] \\right]\n$$\nDue to the independence between $A_{\\sim i}$ and $B_{\\sim i}$, the inner expectation separates into a product of expectations:\n$$\n\\mathbb{E}_{B_i}\\left[ \\mathbb{E}_{B_{\\sim i}}\\left[ f(B_i, B_{\\sim i}) \\mid B_i \\right] \\cdot \\mathbb{E}_{A_{\\sim i}}\\left[ f(A_{\\sim i}, B_i) \\mid B_i \\right] \\right]\n$$\nThe two inner expectations are identical functions of the conditioning variable, which is $B_i$. They both correspond to $\\mathbb{E}_{X_{\\sim i}}[f(X_i, X_{\\sim i}) \\mid X_i = B_i]$, which is precisely $\\mathbb{E}[f(X) \\mid X_i = B_i]$. Therefore, the expression becomes:\n$$\n\\mathbb{E}_{B_i}\\left[ \\left(\\mathbb{E}\\left[f(X)\\mid X_i=B_i\\right]\\right)^2 \\right]\n$$\nSince $B_i$ has the same distribution as $X_i$, this is exactly $\\mathbb{E}\\left[\\left(\\mathbb{E}\\left[f(X)\\mid X_{i}\\right]\\right)^2\\right]$.\n\nWe have found that $\\mathbb{E}\\left[f(B) f(A_B^{(i)})\\right] = \\mathbb{E}\\left[\\left(\\mathbb{E}\\left[f(X)\\mid X_i\\right]\\right)^2\\right]$. Now consider the expectation of another product, $f(B) f(A)$. Due to the independence of $A$ and $B$, $\\mathbb{E}[f(B)f(A)] = \\mathbb{E}[f(B)] \\mathbb{E}[f(A)] = \\mu \\cdot \\mu = \\mu^2$.\nCombining these results, we can express the numerator $V_i$ as:\n$$\nV_{i} = \\mathbb{E}\\left[f(B) f(A_B^{(i)})\\right] - \\mathbb{E}[f(B)f(A)] = \\mathbb{E}\\left[ f(B) \\left( f(A_B^{(i)}) - f(A) \\right) \\right]\n$$\nThis form connects the theoretical quantity $V_i$ directly to the random variables generated by Saltelli's scheme. Based on the principle of Monte Carlo estimation, we can construct an unbiased estimator for $V_i$ by averaging over the $N$ available samples:\n$$\n\\widehat{V}_i = \\frac{1}{N} \\sum_{k=1}^{N} f(\\mathbf{b}^{(k)}) \\left( f(\\mathbf{a}_{b}^{(i,k)}) - f(\\mathbf{a}^{(k)}) \\right)\n$$\n\nNext, we need an estimator for the total variance, $V = \\mathbb{V}(f(X)) = \\mathbb{E}[f(X)^2] - \\mu^2$. We can use the set of model evaluations $f(\\mathbf{a}^{(k)})$ for $k=1, \\dots, N$ to estimate $V$. The standard sample variance is a choice, but for consistency in a ratio estimator, it's common to use the biased sample variance (dividing by $N$):\n$$\n\\widehat{V} = \\frac{1}{N} \\sum_{k=1}^{N} \\left(f(\\mathbf{a}^{(k)})\\right)^2 - \\left( \\frac{1}{N} \\sum_{k=1}^{N} f(\\mathbf{a}^{(k)}) \\right)^2\n$$\nThis estimator for $V$ is computed using only the model evaluations on the sample matrix $\\mathbf{A}$.\n\nFinally, the Monte Carlo estimator for the Sobol' index $\\hat{S}_i$ is the ratio of the estimator for the numerator to the estimator for the denominator:\n$$\n\\hat{S}_i = \\frac{\\widehat{V}_i}{\\widehat{V}} = \\frac{\\frac{1}{N} \\sum_{k=1}^{N} f(\\mathbf{b}^{(k)}) \\left( f(\\mathbf{a}_{b}^{(i,k)}) - f(\\mathbf{a}^{(k)}) \\right)}{\\frac{1}{N} \\sum_{k=1}^{N} \\left(f(\\mathbf{a}^{(k)})\\right)^2 - \\left( \\frac{1}{N} \\sum_{k=1}^{N} f(\\mathbf{a}^{(k)}) \\right)^2}\n$$\nThe factor of $1/N$ in the numerator can be cancelled with the leading $1/N$ in the denominator's expression:\n$$\n\\hat{S}_i = \\frac{ \\sum_{k=1}^{N} f(\\mathbf{b}^{(k)}) \\left( f(\\mathbf{a}_{b}^{(i,k)}) - f(\\mathbf{a}^{(k)}) \\right) }{ \\sum_{k=1}^{N} \\left(f(\\mathbf{a}^{(k)})\\right)^2 - \\frac{1}{N}\\left(\\sum_{k=1}^{N} f(\\mathbf{a}^{(k)})\\right)^2 }\n$$\nThis is the required closed-form analytic expression for the estimator.\n\nNow, we discuss the statistical properties of this estimator.\n\n**Finite-Sample Bias:** The estimator $\\hat{S}_i$ is biased for finite sample sizes $N$. There are two primary sources of this bias.\n1.  **Ratio Estimator Bias:** $\\hat{S}_i$ is a ratio of two random variables, $\\widehat{V}_i$ and $\\widehat{V}$. In general, for two random variables $Y$ and $Z$, $\\mathbb{E}[Y/Z] \\neq \\mathbb{E}[Y]/\\mathbb{E}[Z]$. Even though the numerator estimator $\\widehat{V}_i$ is unbiased for $V_i$ (as $\\mathbb{E}[\\widehat{V}_i] = V_i$), the expectation of the ratio will not equal the ratio of the expectations. This type of bias is inherent to ratio estimators and is typically of order $O(1/N)$.\n2.  **Denominator Bias:** The estimator chosen for the total variance, $\\widehat{V}$, is the biased sample variance. Its expectation is $\\mathbb{E}[\\widehat{V}] = \\frac{N-1}{N} \\mathbb{V}(f(X)) = (1-\\frac{1}{N})V$. This bias also contributes to the overall bias of $\\hat{S}_i$ and is of order $O(1/N)$.\nBecause the bias from both sources diminishes as $N$ increases, the estimator is asymptotically unbiased, i.e., $\\lim_{N \\to \\infty} \\mathbb{E}[\\hat{S}_i] = S_i$.\n\n**Variance:** The estimator $\\hat{S}_i$ is a Monte Carlo estimator based on a sum of $N$ independent and identically distributed terms (in both numerator and denominator). According to the Central Limit Theorem, the variance of such an estimator decreases as the sample size $N$ increases. Specifically, the variance of $\\hat{S}_i$ scales inversely with $N$:\n$$\n\\mathbb{V}(\\hat{S}_i) \\propto \\frac{1}{N}\n$$\nThis $O(1/N)$ convergence of the variance is a standard characteristic of Monte Carlo methods. It implies that to reduce the standard error of the estimate by a factor of $2$, the number of samples $N$ (and thus the total number of model evaluations, which is $N(d+2)$) must be increased by a factor of $4$. The exact constant of proportionality depends on the variances and covariances of the terms involved in the numerator and denominator.",
            "answer": "$$\n\\boxed{\\frac{\\sum_{k=1}^{N} f(\\mathbf{b}^{(k)}) \\left( f(\\mathbf{a}_{b}^{(i,k)}) - f(\\mathbf{a}^{(k)}) \\right)}{\\sum_{k=1}^{N} \\left(f(\\mathbf{a}^{(k)})\\right)^{2} - \\frac{1}{N}\\left(\\sum_{k=1}^{N} f(\\mathbf{a}^{(k)})\\right)^{2}}}\n$$"
        },
        {
            "introduction": "Effective battery models must be calibrated against real-world data, which always contains measurement noise. This final exercise shifts our focus from parameter uncertainty to data uncertainty by modeling correlated measurement errors with an Autoregressive (AR(1)) process. You will derive the exact likelihood function for a sequence of voltage measurements, a critical step that forms the basis for sophisticated parameter estimation and state-of-health tracking algorithms .",
            "id": "3959210",
            "problem": "A lithium-ion cell in an automated battery design and simulation pipeline is driven by a known current profile, denoted by the sequence $\\{u_t\\}_{t=1}^T$. A mechanistic voltage model produces the predicted terminal voltage $\\hat{v}_t = h(\\boldsymbol{\\theta}, u_t)$ at discrete times $t = 1, 2, \\dots, T$, where $\\boldsymbol{\\theta}$ is a vector of design parameters. The measured terminal voltage is modeled as $y_t = \\hat{v}_t + \\epsilon_t$, where the measurement error $\\epsilon_t$ captures correlated sensor drift. To perform Uncertainty Quantification (UQ), assume the error process is Autoregressive of order $1$ (AR(1)), namely $\\epsilon_t = \\phi \\epsilon_{t-1} + \\eta_t$ for $t \\geq 2$, with $\\eta_t \\sim \\mathcal{N}(0, \\sigma_\\eta^2)$ independent and identically distributed, and $|\\phi|  1$. Assume the process is stationary so that $\\epsilon_1 \\sim \\mathcal{N}\\!\\left(0, \\frac{\\sigma_\\eta^2}{1 - \\phi^2}\\right)$, and that $\\{\\eta_t\\}_{t=2}^T$ are mutually independent and independent of $\\epsilon_1$ and $\\boldsymbol{\\theta}$. Define the residuals $r_t = y_t - \\hat{v}_t = y_t - h(\\boldsymbol{\\theta}, u_t)$ for $t = 1, \\dots, T$. Starting from fundamental probability rules and the properties of Gaussian random variables, derive the closed-form likelihood $p(y_{1:T} \\mid \\boldsymbol{\\theta}, \\phi, \\sigma_\\eta)$ as a function of $\\{r_t\\}_{t=1}^T$, $\\phi$, and $\\sigma_\\eta$. Provide your final answer as a single analytical expression for the likelihood $p(y_{1:T} \\mid \\boldsymbol{\\theta}, \\phi, \\sigma_\\eta)$, expressed in terms of $r_t$, $\\phi$, $\\sigma_\\eta$, and $T$. You may treat $\\{u_t\\}_{t=1}^T$ as known and fixed. The final answer must be an analytic expression and not an inequality or implicit equation.",
            "solution": "We begin from the measurement model $y_t = \\hat{v}_t + \\epsilon_t$ and define the residuals $r_t = y_t - \\hat{v}_t = \\epsilon_t$. The residual sequence $\\{r_t\\}$ inherits the Autoregressive of order $1$ (AR(1)) dynamics:\n$$\nr_t = \\phi r_{t-1} + \\eta_t \\quad \\text{for} \\quad t \\geq 2,\n$$\nwith $\\eta_t \\sim \\mathcal{N}(0, \\sigma_\\eta^2)$ independent and identically distributed, and a stationary initial distribution $r_1 \\sim \\mathcal{N}\\!\\left(0, \\frac{\\sigma_\\eta^2}{1 - \\phi^2}\\right)$. The conditions $|\\phi|  1$ ensure stationarity.\n\nThe likelihood $p(y_{1:T} \\mid \\boldsymbol{\\theta}, \\phi, \\sigma_\\eta)$ is the same as the joint density $p(r_{1:T} \\mid \\phi, \\sigma_\\eta)$ because $\\hat{v}_t = h(\\boldsymbol{\\theta}, u_t)$ is deterministic given $\\boldsymbol{\\theta}$ and $\\{u_t\\}_{t=1}^T$. Using the chain rule for probabilities,\n$$\np(r_{1:T} \\mid \\phi, \\sigma_\\eta) = p(r_1 \\mid \\phi, \\sigma_\\eta)\\prod_{t=2}^T p(r_t \\mid r_{t-1}, \\phi, \\sigma_\\eta).\n$$\nFrom the AR(1) model and Gaussian properties,\n- The initial term is\n$$\np(r_1 \\mid \\phi, \\sigma_\\eta) = \\frac{1}{\\sqrt{2\\pi \\, \\sigma_1^2}} \\exp\\!\\left(-\\frac{r_1^2}{2\\sigma_1^2}\\right), \\quad \\text{with} \\quad \\sigma_1^2 = \\frac{\\sigma_\\eta^2}{1 - \\phi^2}.\n$$\n- The conditional terms are\n$$\np(r_t \\mid r_{t-1}, \\phi, \\sigma_\\eta) = \\frac{1}{\\sqrt{2\\pi \\, \\sigma_\\eta^2}} \\exp\\!\\left(-\\frac{(r_t - \\phi r_{t-1})^2}{2\\sigma_\\eta^2}\\right), \\quad t = 2, \\dots, T.\n$$\n\nMultiplying these densities,\n$$\np(r_{1:T} \\mid \\phi, \\sigma_\\eta)\n= \\left[\\frac{1}{\\sqrt{2\\pi \\, \\sigma_1^2}}\\right] \\prod_{t=2}^T \\left[\\frac{1}{\\sqrt{2\\pi \\, \\sigma_\\eta^2}}\\right]\n\\exp\\!\\left(-\\frac{r_1^2}{2\\sigma_1^2} - \\sum_{t=2}^T \\frac{(r_t - \\phi r_{t-1})^2}{2\\sigma_\\eta^2}\\right).\n$$\nSubstitute $\\sigma_1^2 = \\frac{\\sigma_\\eta^2}{1 - \\phi^2}$ to obtain the normalization constant:\n$$\n\\frac{1}{\\sqrt{2\\pi \\, \\sigma_1^2}} \\prod_{t=2}^T \\frac{1}{\\sqrt{2\\pi \\, \\sigma_\\eta^2}}\n= (2\\pi)^{-T/2} \\left(\\frac{1 - \\phi^2}{\\sigma_\\eta^2}\\right)^{1/2} \\left(\\sigma_\\eta^2\\right)^{-(T-1)/2}\n= (2\\pi)^{-T/2} (1 - \\phi^2)^{1/2} \\, \\sigma_\\eta^{-T}.\n$$\nThe exponent simplifies as\n$$\n-\\frac{r_1^2}{2\\sigma_1^2} - \\sum_{t=2}^T \\frac{(r_t - \\phi r_{t-1})^2}{2\\sigma_\\eta^2}\n= -\\frac{1}{2\\sigma_\\eta^2}\\left[(1 - \\phi^2) r_1^2 + \\sum_{t=2}^T (r_t - \\phi r_{t-1})^2 \\right].\n$$\n\nTherefore, the joint likelihood of the observations $\\{y_t\\}_{t=1}^T$ given $(\\boldsymbol{\\theta}, \\phi, \\sigma_\\eta)$, expressed in terms of the residuals $r_t = y_t - h(\\boldsymbol{\\theta}, u_t)$, is\n$$\np(y_{1:T} \\mid \\boldsymbol{\\theta}, \\phi, \\sigma_\\eta)\n= (2\\pi)^{-T/2} (1 - \\phi^2)^{1/2} \\, \\sigma_\\eta^{-T}\n\\exp\\!\\left(\n-\\frac{1}{2\\sigma_\\eta^2}\n\\left[\n(1 - \\phi^2) r_1^2 + \\sum_{t=2}^T (r_t - \\phi r_{t-1})^2\n\\right]\n\\right),\n$$\nwhich is a closed-form analytical expression constructed from fundamental probability rules and Gaussian AR(1) properties under stationarity.",
            "answer": "$$\\boxed{(2\\pi)^{-T/2}\\,(1-\\phi^{2})^{1/2}\\,\\sigma_{\\eta}^{-T}\\,\\exp\\!\\left(-\\frac{1}{2\\sigma_{\\eta}^{2}}\\left[(1-\\phi^{2})\\,r_{1}^{2}+\\sum_{t=2}^{T}\\left(r_{t}-\\phi\\,r_{t-1}\\right)^{2}\\right]\\right)}$$"
        }
    ]
}