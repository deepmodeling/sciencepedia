{
    "hands_on_practices": [
        {
            "introduction": "The power of Polynomial Chaos Expansions stems from choosing an orthogonal basis that perfectly matches the probability measure of the uncertain inputs. This practice moves beyond simple uniform or normal distributions to the flexible Beta distribution, which is invaluable for modeling parameters constrained to a finite interval, such as porosity or state-of-charge. By working through the construction of the corresponding orthonormal Jacobi polynomials, you will gain a deep, practical understanding of the Wiener-Askey scheme, which is the theoretical foundation of PCE .",
            "id": "3941378",
            "problem": "In automated battery design and simulation, uncertain microstructural parameters such as the electrolyte porosity $\\,\\varepsilon\\,$ of a porous electrode are commonly modeled probabilistically. Suppose $\\,\\varepsilon\\,$ follows a Beta distribution $\\mathrm{Beta}(\\alpha,\\beta)$ with $\\alpha0$ and $\\beta0$ on the interval $(0,1)$, so that its probability density function is proportional to $\\,w(x)\\propto x^{\\alpha-1}(1-x)^{\\beta-1}\\,$ for $\\,x\\in(0,1)$. To construct a Polynomial Chaos Expansion (PCE) basis tailored to this uncertainty for propagating uncertainty in a scalar performance metric of interest, we require an orthonormal polynomial basis $\\{\\psi_n(x)\\}_{n\\ge 0}$ on $(0,1)$ with respect to the weight $\\,w(x)\\,$.\n\nStarting from first principles of orthogonality with respect to a weight and the definition of the Beta function $\\,B(\\alpha,\\beta)\\,$ and Gamma function $\\,\\Gamma(\\cdot)\\,$, derive an orthonormal basis on $(0,1)$ by appropriately shifting and scaling the classical Jacobi polynomials so that\n$$\n\\int_{0}^{1} w(x)\\,\\psi_m(x)\\,\\psi_n(x)\\,dx \\;=\\; \\delta_{mn}.\n$$\nExpress your orthonormal polynomials in the form\n$$\n\\psi_n(x)\\;=\\;c_n\\,P_n^{(\\beta-1,\\alpha-1)}\\!\\big(2x-1\\big),\n$$\nwhere $\\,P_n^{(a,b)}(\\cdot)\\,$ denotes the Jacobi polynomial of degree $n$ with parameters $(a,b)$, and determine the normalization constants $\\,c_n\\,$ explicitly for degrees $\\,n=0,1,2\\,$.\n\nProvide your final answer as a single row vector containing the three normalization constants in the order $\\,n=0,1,2\\,$, expressed in closed form using $\\,\\Gamma(\\cdot)\\,$ and $\\,B(\\alpha,\\beta)\\,$ as needed. No numerical evaluation or rounding is required, and no units are needed.",
            "solution": "The problem is valid. It is a standard, well-posed problem in the theory of orthogonal polynomials, specifically as it applies to the construction of Polynomial Chaos Expansions for uncertainty quantification. All provided information is scientifically sound and mathematically consistent.\n\nWe are tasked with finding the normalization constants $c_n$ for $n=0,1,2$ such that the polynomials $\\psi_n(x) = c_n P_n^{(\\beta-1,\\alpha-1)}(2x-1)$ form an orthonormal set with respect to the weight function $w(x) = x^{\\alpha-1}(1-x)^{\\beta-1}$ on the interval $(0,1)$. The condition for orthonormality is\n$$\n\\int_{0}^{1} w(x)\\,\\psi_m(x)\\,\\psi_n(x)\\,dx = \\delta_{mn}\n$$\nwhere $\\delta_{mn}$ is the Kronecker delta.\n\nFor the case $m=n$, this condition becomes a normalization requirement:\n$$\n\\int_{0}^{1} w(x)\\,[\\psi_n(x)]^2\\,dx = 1\n$$\nSubstituting the given forms for $w(x)$ and $\\psi_n(x)$, we have:\n$$\n\\int_{0}^{1} x^{\\alpha-1}(1-x)^{\\beta-1} \\left[c_n P_n^{(\\beta-1,\\alpha-1)}(2x-1)\\right]^2 dx = 1\n$$\nThe constant $c_n^2$ can be factored out of the integral:\n$$\nc_n^2 \\int_{0}^{1} x^{\\alpha-1}(1-x)^{\\beta-1} \\left[P_n^{(\\beta-1,\\alpha-1)}(2x-1)\\right]^2 dx = 1\n$$\nLet's name the integral $I_n$:\n$$\nI_n = \\int_{0}^{1} x^{\\alpha-1}(1-x)^{\\beta-1} \\left[P_n^{(\\beta-1,\\alpha-1)}(2x-1)\\right]^2 dx\n$$\nFrom this, the normalization constant is given by $c_n^2 = 1/I_n$.\n\nTo evaluate $I_n$, we transform the integral to the canonical domain of Jacobi polynomials, which is the interval $[-1, 1]$. We introduce the change of variables $y = 2x-1$.\nThis implies:\n$x = \\frac{y+1}{2}$\n$1-x = 1 - \\frac{y+1}{2} = \\frac{1-y}{2}$\n$dx = \\frac{1}{2}dy$\nAs $x$ goes from $0$ to $1$, $y$ goes from $-1$ to $1$.\n\nSubstituting these into the expression for $I_n$:\n$$\nI_n = \\int_{-1}^{1} \\left(\\frac{y+1}{2}\\right)^{\\alpha-1} \\left(\\frac{1-y}{2}\\right)^{\\beta-1} \\left[P_n^{(\\beta-1,\\alpha-1)}(y)\\right]^2 \\frac{dy}{2}\n$$\nWe can separate the constants from the integrand:\n$$\nI_n = \\frac{1}{2^{\\alpha-1}} \\frac{1}{2^{\\beta-1}} \\frac{1}{2} \\int_{-1}^{1} (1-y)^{\\beta-1} (1+y)^{\\alpha-1} \\left[P_n^{(\\beta-1,\\alpha-1)}(y)\\right]^2 dy\n$$\n$$\nI_n = \\frac{1}{2^{\\alpha+\\beta-1}} \\int_{-1}^{1} (1-y)^{\\beta-1} (1+y)^{\\alpha-1} \\left[P_n^{(\\beta-1,\\alpha-1)}(y)\\right]^2 dy\n$$\nThe integral on the right-hand side is the squared norm of the Jacobi polynomial $P_n^{(a,b)}(y)$ with parameters $a = \\beta-1$ and $b = \\alpha-1$. The standard formula for this squared norm, let's call it $h_n^{(a,b)}$, is:\n$$\nh_n^{(a,b)} = \\int_{-1}^{1} (1-y)^a (1+y)^b \\left[P_n^{(a,b)}(y)\\right]^2 dy = \\frac{2^{a+b+1}}{2n+a+b+1} \\frac{\\Gamma(n+a+1)\\Gamma(n+b+1)}{n!\\Gamma(n+a+b+1)}\n$$\nFor our specific parameters $a = \\beta-1$ and $b = \\alpha-1$, we have:\n$a+b+1 = (\\beta-1) + (\\alpha-1) + 1 = \\alpha+\\beta-1$\n$n+a+1 = n+\\beta$\n$n+b+1 = n+\\alpha$\n$n+a+b+1 = n+\\alpha+\\beta-1$\n\nSubstituting these into the formula for $h_n^{(a,b)}$:\n$$\nh_n^{(\\beta-1, \\alpha-1)} = \\frac{2^{\\alpha+\\beta-1}}{2n+\\alpha+\\beta-1} \\frac{\\Gamma(n+\\beta)\\Gamma(n+\\alpha)}{n!\\Gamma(n+\\alpha+\\beta-1)}\n$$\nNow, we can express $I_n$ in terms of this squared norm:\n$$\nI_n = \\frac{1}{2^{\\alpha+\\beta-1}} h_n^{(\\beta-1, \\alpha-1)} = \\frac{1}{2^{\\alpha+\\beta-1}} \\left( \\frac{2^{\\alpha+\\beta-1}}{2n+\\alpha+\\beta-1} \\frac{\\Gamma(n+\\alpha)\\Gamma(n+\\beta)}{n!\\Gamma(n+\\alpha+\\beta-1)} \\right)\n$$\n$$\nI_n = \\frac{\\Gamma(n+\\alpha)\\Gamma(n+\\beta)}{n!(2n+\\alpha+\\beta-1)\\Gamma(n+\\alpha+\\beta-1)}\n$$\nSince $c_n^2 = 1/I_n$, we get the general formula for the squared normalization constant:\n$$\nc_n^2 = \\frac{n!(2n+\\alpha+\\beta-1)\\Gamma(n+\\alpha+\\beta-1)}{\\Gamma(n+\\alpha)\\Gamma(n+\\beta)}\n$$\nBy convention, we take the positive square root for $c_n$. We now compute $c_n$ for $n=0, 1, 2$.\n\nFor $n=0$:\n$$\nc_0^2 = \\frac{0!(\\alpha+\\beta-1)\\Gamma(\\alpha+\\beta-1)}{\\Gamma(\\alpha)\\Gamma(\\beta)}\n$$\nUsing the property $\\Gamma(z+1)=z\\Gamma(z)$, we have $(\\alpha+\\beta-1)\\Gamma(\\alpha+\\beta-1) = \\Gamma(\\alpha+\\beta)$.\n$$\nc_0^2 = \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} = \\frac{1}{B(\\alpha,\\beta)}\n$$\nThus, $c_0 = \\frac{1}{\\sqrt{B(\\alpha,\\beta)}}$.\n\nFor $n=1$:\n$$\nc_1^2 = \\frac{1!(2+\\alpha+\\beta-1)\\Gamma(1+\\alpha+\\beta-1)}{\\Gamma(1+\\alpha)\\Gamma(1+\\beta)} = \\frac{(\\alpha+\\beta+1)\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha+1)\\Gamma(\\beta+1)}\n$$\nUsing $\\Gamma(\\alpha+1)=\\alpha\\Gamma(\\alpha)$ and $\\Gamma(\\beta+1)=\\beta\\Gamma(\\beta)$:\n$$\nc_1^2 = \\frac{(\\alpha+\\beta+1)\\Gamma(\\alpha+\\beta)}{\\alpha\\Gamma(\\alpha)\\beta\\Gamma(\\beta)} = \\frac{\\alpha+\\beta+1}{\\alpha\\beta} \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} = \\frac{\\alpha+\\beta+1}{\\alpha\\beta B(\\alpha,\\beta)}\n$$\nThus, $c_1 = \\sqrt{\\frac{\\alpha+\\beta+1}{\\alpha\\beta B(\\alpha,\\beta)}}$.\n\nFor $n=2$:\n$$\nc_2^2 = \\frac{2!(4+\\alpha+\\beta-1)\\Gamma(2+\\alpha+\\beta-1)}{\\Gamma(2+\\alpha)\\Gamma(2+\\beta)} = \\frac{2(\\alpha+\\beta+3)\\Gamma(\\alpha+\\beta+1)}{\\Gamma(\\alpha+2)\\Gamma(\\beta+2)}\n$$\nUsing the Gamma function properties repeatedly:\n$\\Gamma(\\alpha+\\beta+1) = (\\alpha+\\beta)\\Gamma(\\alpha+\\beta)$\n$\\Gamma(\\alpha+2) = (\\alpha+1)\\alpha\\Gamma(\\alpha)$\n$\\Gamma(\\beta+2) = (\\beta+1)\\beta\\Gamma(\\beta)$\n$$\nc_2^2 = \\frac{2(\\alpha+\\beta+3)(\\alpha+\\beta)\\Gamma(\\alpha+\\beta)}{(\\alpha+1)\\alpha\\Gamma(\\alpha) (\\beta+1)\\beta\\Gamma(\\beta)} = \\frac{2(\\alpha+\\beta)(\\alpha+\\beta+3)}{\\alpha(\\alpha+1)\\beta(\\beta+1)} \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}\n$$\n$$\nc_2^2 = \\frac{2(\\alpha+\\beta)(\\alpha+\\beta+3)}{\\alpha(\\alpha+1)\\beta(\\beta+1)B(\\alpha,\\beta)}\n$$\nThus, $c_2 = \\sqrt{\\frac{2(\\alpha+\\beta)(\\alpha+\\beta+3)}{\\alpha(\\alpha+1)\\beta(\\beta+1)B(\\alpha,\\beta)}}$.\n\nThe three normalization constants are:\n$c_0 = \\frac{1}{\\sqrt{B(\\alpha,\\beta)}}$\n$c_1 = \\sqrt{\\frac{\\alpha+\\beta+1}{\\alpha\\beta B(\\alpha,\\beta)}}$\n$c_2 = \\sqrt{\\frac{2(\\alpha+\\beta)(\\alpha+\\beta+3)}{\\alpha(\\alpha+1)\\beta(\\beta+1)B(\\alpha,\\beta)}}$",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{1}{\\sqrt{B(\\alpha,\\beta)}}  \\sqrt{\\frac{\\alpha+\\beta+1}{\\alpha\\beta B(\\alpha,\\beta)}}  \\sqrt{\\frac{2(\\alpha+\\beta)(\\alpha+\\beta+3)}{\\alpha(\\alpha+1)\\beta(\\beta+1)B(\\alpha,\\beta)}}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "In many real-world battery design problems, running high-fidelity simulations is computationally expensive, limiting the data available for uncertainty quantification. This exercise introduces the powerful concept of sparsity—the idea that many complex models are dominated by a few key parameters and their interactions. You will explore how compressed sensing, specifically through $\\ell_1$ regularization, allows us to efficiently discover these sparse structures and build accurate PCE surrogate models even from a small number of noisy simulations .",
            "id": "3941421",
            "problem": "An automated battery design workflow uses high-fidelity electrochemical simulators to evaluate the discharge voltage at a fixed time $t_\\star$ under uncertain cell design variables $\\boldsymbol{\\xi} \\in \\mathbb{R}^d$ (for example, porosity, particle radius, and reaction-rate constant), modeled as a random vector with a known joint distribution. The voltage response $y = \\mathcal{M}(\\boldsymbol{\\xi})$ is approximated by a polynomial chaos expansion (PCE), i.e., $y \\approx \\sum_{j=1}^P c_j \\,\\psi_j(\\boldsymbol{\\xi})$, where $\\{\\psi_j\\}_{j=1}^P$ are basis polynomials orthonormal with respect to the law of $\\boldsymbol{\\xi}$. To fit the coefficients $\\mathbf{c} \\in \\mathbb{R}^P$, the workflow runs $M$ noisy simulations, producing data $\\{(\\boldsymbol{\\xi}^{(i)}, y^{(i)})\\}_{i=1}^M$ with $y^{(i)} = \\mathcal{M}(\\boldsymbol{\\xi}^{(i)}) + \\eta^{(i)}$, where $\\eta^{(i)}$ models additive simulator noise and model discrepancy. The resulting linear system is $\\mathbf{y} = \\boldsymbol{\\Phi}\\mathbf{c} + \\boldsymbol{\\eta}$, where $\\mathbf{y} \\in \\mathbb{R}^M$, $\\boldsymbol{\\Phi} \\in \\mathbb{R}^{M \\times P}$ with entries $\\Phi_{ij} = \\psi_j(\\boldsymbol{\\xi}^{(i)})$, and $\\boldsymbol{\\eta} \\in \\mathbb{R}^M$. In this setting, sparsity is expected because many high-order interaction terms are negligible in the voltage response.\n\nFrom first principles of compressed sensing and statistical estimation, justify how $\\ell_1$ regularization promotes sparsity in the PCE coefficient vector $\\mathbf{c}$, and select the one option that correctly formulates the Basis Pursuit Denoising (BPDN) problem appropriate for noisy simulator outputs, including a principled way to select the regularization parameter.\n\nChoose the single best option.\n\nA. $\\ell_1$ regularization is a convex relaxation of the $\\ell_0$ “counting” penalty and promotes sparse solutions because the $\\ell_1$ unit ball has vertices aligned with coordinate axes; when intersected with an affine residual set, solutions tend to lie on axes, setting many coefficients to zero. Under conditions such as the Restricted Isometry Property, the $\\ell_1$ solution recovers or approximates the true sparse PCE coefficients. For noisy outputs with approximately Gaussian noise of variance $\\sigma^2$, BPDN is\n$\\min_{\\mathbf{c} \\in \\mathbb{R}^P} \\|\\mathbf{c}\\|_1 \\;\\; \\text{s.t.} \\;\\; \\|\\boldsymbol{\\Phi}\\mathbf{c} - \\mathbf{y}\\|_2 \\le \\epsilon,$\nwith $\\epsilon$ chosen by the discrepancy principle, e.g., $\\epsilon \\approx \\sqrt{M}\\,\\hat{\\sigma}$ where $\\hat{\\sigma}$ estimates $\\sigma$ from replicated runs or residual analysis.\n\nB. $\\ell_1$ regularization primarily shrinks all coefficients uniformly but does not yield exact zeros; sparsity arises only if features are orthogonal. For noisy outputs, BPDN is the unconstrained Lagrangian form\n$\\min_{\\mathbf{c} \\in \\mathbb{R}^P} \\|\\boldsymbol{\\Phi}\\mathbf{c} - \\mathbf{y}\\|_2^2 + \\lambda \\|\\mathbf{c}\\|_1,$\nwith $\\lambda$ selected by minimizing the in-sample residual norm $\\|\\boldsymbol{\\Phi}\\mathbf{c} - \\mathbf{y}\\|_2$, since smaller residuals indicate better fit.\n\nC. $\\ell_1$ regularization promotes sparsity because it penalizes large coefficients more heavily than small ones, similar to ridge regression. For noisy outputs, BPDN is\n$\\min_{\\mathbf{c} \\in \\mathbb{R}^P} \\|\\mathbf{c}\\|_2 \\;\\; \\text{s.t.} \\;\\; \\|\\boldsymbol{\\Phi}\\mathbf{c} - \\mathbf{y}\\|_2 \\le \\epsilon,$\nwith $\\epsilon$ set to $\\epsilon = \\|\\mathbf{y}\\|_2 / \\sqrt{M}$ so that the residual is normalized by the sample size.\n\nD. $\\ell_1$ regularization promotes sparsity only when the number of samples $M$ exceeds the number of terms $P$ by a large margin; otherwise it behaves like an unregularized least squares estimator. For noisy outputs, BPDN is\n$\\min_{\\mathbf{c} \\in \\mathbb{R}^P} \\|\\mathbf{c}\\|_1 \\;\\; \\text{s.t.} \\;\\; \\|\\boldsymbol{\\Phi}\\mathbf{c} - \\mathbf{y}\\|_1 \\le \\epsilon,$\nwith $\\epsilon$ selected by the Bayesian Information Criterion (BIC) to penalize model complexity regardless of the noise distribution.",
            "solution": "The user wants me to validate the problem statement, and if it is valid, to solve it.\n\n### Step 1: Extract Givens\n- **System Model**: The discharge voltage $y$ at a fixed time $t_\\star$ is a function of uncertain cell design variables $\\boldsymbol{\\xi} \\in \\mathbb{R}^d$, denoted as $y = \\mathcal{M}(\\boldsymbol{\\xi})$.\n- **Uncertain Inputs**: $\\boldsymbol{\\xi}$ is a random vector with a known joint probability distribution.\n- **Surrogate Model**: The voltage response is approximated by a Polynomial Chaos Expansion (PCE): $y \\approx \\sum_{j=1}^P c_j \\,\\psi_j(\\boldsymbol{\\xi})$.\n- **PCE Basis**: The basis polynomials $\\{\\psi_j\\}_{j=1}^P$ are orthonormal with respect to the probability distribution of $\\boldsymbol{\\xi}$.\n- **Coefficient Estimation**: The coefficients $\\mathbf{c} \\in \\mathbb{R}^P$ are determined from $M$ noisy simulations.\n- **Data Set**: The simulation data is $\\{(\\boldsymbol{\\xi}^{(i)}, y^{(i)})\\}_{i=1}^M$.\n- **Noise Model**: The measurements are noisy, $y^{(i)} = \\mathcal{M}(\\boldsymbol{\\xi}^{(i)}) + \\eta^{(i)}$, where $\\eta^{(i)}$ represents additive noise and model discrepancy.\n- **Linear System**: The problem is expressed as a linear system $\\mathbf{y} = \\boldsymbol{\\Phi}\\mathbf{c} + \\boldsymbol{\\eta}$, where:\n    - $\\mathbf{y} \\in \\mathbb{R}^M$ is the vector of observed voltages.\n    - $\\boldsymbol{\\Phi} \\in \\mathbb{R}^{M \\times P}$ is the design matrix with entries $\\Phi_{ij} = \\psi_j(\\boldsymbol{\\xi}^{(i)})$.\n    - $\\mathbf{c} \\in \\mathbb{R}^P$ is the vector of unknown PCE coefficients.\n    - $\\boldsymbol{\\eta} \\in \\mathbb{R}^M$ is the vector of noise terms.\n- **Sparsity Assumption**: The true coefficient vector $\\mathbf{c}$ is expected to be sparse, as many high-order interaction terms are assumed to be negligible.\n- **Task**: Justify how $\\ell_1$ regularization promotes sparsity and select the option that correctly formulates the Basis Pursuit Denoising (BPDN) problem for this setting, including a principled method for choosing the regularization parameter.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is evaluated for validity.\n\n- **Scientifically Grounded**: The problem is well-grounded in the field of Uncertainty Quantification (UQ) and computational science. Using PCE to build surrogate models of complex physical simulators (like battery models) is a standard and widely-used technique. The formulation of coefficient estimation as a linear regression problem $\\mathbf{y} = \\boldsymbol{\\Phi}\\mathbf{c} + \\boldsymbol{\\eta}$ is correct. The assumption of sparsity in the PCE coefficient vector is common in many applications, where the output quantity of interest is dominated by a few uncertain parameters and their low-order interactions. Employing techniques from compressed sensing, such as $\\ell_1$ regularization, to exploit this sparsity is a modern and active area of research and application in UQ. All concepts are standard in applied mathematics, statistics, and engineering.\n- **Well-Posed**: The question is well-posed. It asks for a conceptual justification and the identification of a standard mathematical formulation (BPDN) within the given context. It does not ask for a numerical solution but rather an understanding of the underlying principles. The premises logically lead to the question.\n- **Objective**: The problem is stated using precise, objective, and standard technical terminology. There are no subjective or opinion-based elements.\n\nThe problem statement is found to be complete, consistent, scientifically realistic, and well-structured. It does not exhibit any of the flaws listed in the validation checklist.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. I will proceed with the solution derivation and option analysis.\n\n### Derivation from First Principles\nThe core task is to find a sparse coefficient vector $\\mathbf{c}$ that is consistent with the noisy data $\\mathbf{y} \\approx \\boldsymbol{\\Phi}\\mathbf{c}$.\n\n1.  **Sparsity and the $\\ell_0$ \"Norm\"**: The \"sparsest\" vector $\\mathbf{c}$ is the one that minimizes the number of non-zero entries. This is measured by the $\\ell_0$ quasi-norm, $\\|\\mathbf{c}\\|_0 = |\\{j : c_j \\neq 0\\}|$. The ideal optimization problem would be to find the sparsest solution that fits the data up to the noise level:\n    $$ \\min_{\\mathbf{c} \\in \\mathbb{R}^P} \\|\\mathbf{c}\\|_0 \\quad \\text{subject to} \\quad \\|\\boldsymbol{\\Phi}\\mathbf{c} - \\mathbf{y}\\|_2 \\le \\epsilon $$\n    where $\\epsilon$ is a parameter related to the magnitude of the noise vector $\\boldsymbol{\\eta}$. However, minimizing the $\\ell_0$ norm is a combinatorial problem, which is NP-hard and computationally intractable for all but the smallest problems.\n\n2.  **Convex Relaxation and the $\\ell_1$ Norm**: The central idea of compressed sensing and related methods is to relax the non-convex $\\ell_0$ minimization problem to a convex one. The tightest convex relaxation of the $\\ell_0$ quasi-norm is the $\\ell_1$ norm, defined as $\\|\\mathbf{c}\\|_1 = \\sum_{j=1}^P |c_j|$. Replacing $\\|\\mathbf{c}\\|_0$ with $\\|\\mathbf{c}\\|_1$ leads to the Basis Pursuit Denoising (BPDN) problem, which is a convex optimization problem (specifically, a quadratically constrained linear program) and thus computationally tractable.\n    $$ \\min_{\\mathbf{c} \\in \\mathbb{R}^P} \\|\\mathbf{c}\\|_1 \\quad \\text{subject to} \\quad \\|\\boldsymbol{\\Phi}\\mathbf{c} - \\mathbf{y}\\|_2 \\le \\epsilon $$\n    The crucial discovery, formalized by the theory of compressed sensing (e.g., under conditions on $\\boldsymbol{\\Phi}$ like the Restricted Isometry Property or RIP), is that for sufficiently sparse signals, the solution of this $\\ell_1$-minimization problem is often identical to the solution of the intractable $\\ell_0$-minimization problem.\n\n3.  **Why $\\ell_1$ Promotes Sparsity (Geometric Intuition)**: The objective is to find a point in the feasible set $S = \\{\\mathbf{c} : \\|\\boldsymbol{\\Phi}\\mathbf{c} - \\mathbf{y}\\|_2 \\le \\epsilon\\}$ that has the minimum $\\ell_1$ norm. We can visualize this as expanding a ball of the form $B_1(r) = \\{\\mathbf{c} : \\|\\mathbf{c}\\|_1 \\le r\\}$ until it first touches the set $S$. The unit ball for the $\\ell_1$ norm in $\\mathbb{R}^P$ is a cross-polytope, which has sharp vertices on the coordinate axes and lower-dimensional edges and faces. Because of these \"corners,\" the intersection of the expanding $\\ell_1$ ball and the smooth, convex feasible set $S$ is highly likely to occur at one of these vertices or edges. Points on these features have one or more coordinates equal to zero by definition, leading to a sparse solution vector $\\mathbf{c}$. In contrast, the $\\ell_2$ ball (a hypersphere) is perfectly round, and its first point of contact with $S$ will generically have no zero components.\n\n4.  **Principled Selection of the Regularization Parameter $\\epsilon$**: The parameter $\\epsilon$ bounds the Euclidean norm of the residual, $\\mathbf{r} = \\mathbf{y} - \\boldsymbol{\\Phi}\\mathbf{c}$. Given the model $\\mathbf{y} = \\boldsymbol{\\Phi}\\mathbf{c}_{\\text{true}} + \\boldsymbol{\\eta}$, the ideal residual is the noise vector $\\boldsymbol{\\eta}$. Therefore, a principled choice for $\\epsilon$ should correspond to a high-probability upper bound on the norm of the noise, $\\|\\boldsymbol{\\eta}\\|_2$. If the noise components $\\eta^{(i)}$ are assumed to be independent and identically distributed (i.i.d.) with mean $0$ and variance $\\sigma^2$, then the expected value of the squared norm is $\\mathbb{E}[\\|\\boldsymbol{\\eta}\\|_2^2] = \\sum_{i=1}^M \\mathbb{E}[(\\eta^{(i)})^2] = M\\sigma^2$. This implies that $\\|\\boldsymbol{\\eta}\\|_2$ should be on the order of $\\sqrt{M}\\sigma$. This approach is known as the discrepancy principle. The noise variance $\\sigma^2$ is typically unknown and must be estimated (e.g., as $\\hat{\\sigma}^2$) from the data, for instance, by running replicate simulations or from the residuals of a preliminary fit. Thus, a sound choice is $\\epsilon \\approx \\sqrt{M}\\,\\hat{\\sigma}$.\n\n### Option-by-Option Analysis\n\n**A. $\\ell_1$ regularization is a convex relaxation of the $\\ell_0$ “counting” penalty and promotes sparse solutions because the $\\ell_1$ unit ball has vertices aligned with coordinate axes; when intersected with an affine residual set, solutions tend to lie on axes, setting many coefficients to zero. Under conditions such as the Restricted Isometry Property, the $\\ell_1$ solution recovers or approximates the true sparse PCE coefficients. For noisy outputs with approximately Gaussian noise of variance $\\sigma^2$, BPDN is $\\min_{\\mathbf{c} \\in \\mathbb{R}^P} \\|\\mathbf{c}\\|_1 \\;\\; \\text{s.t.} \\;\\; \\|\\boldsymbol{\\Phi}\\mathbf{c} - \\mathbf{y}\\|_2 \\le \\epsilon,$ with $\\epsilon$ chosen by the discrepancy principle, e.g., $\\epsilon \\approx \\sqrt{M}\\,\\hat{\\sigma}$ where $\\hat{\\sigma}$ estimates $\\sigma$ from replicated runs or residual analysis.**\n\n-   **Justification for Sparsity**: The explanation is correct. It correctly identifies $\\ell_1$ as a convex relaxation of $\\ell_0$ and provides the correct geometric intuition based on the shape of the $\\ell_1$ unit ball.\n-   **Theoretical Context**: The mention of the Restricted Isometry Property (RIP) is accurate; it is one of the key theoretical conditions guaranteeing the success of $\\ell_1$ minimization.\n-   **BPDN Formulation**: The mathematical formulation $\\min \\|\\mathbf{c}\\|_1$ s.t. $\\|\\boldsymbol{\\Phi}\\mathbf{c} - \\mathbf{y}\\|_2 \\le \\epsilon$ is the canonical form of Basis Pursuit Denoising. The use of the $\\ell_2$ norm for the residual is standard and statistically motivated by assuming i.i.d. noise (often Gaussian).\n-   **Parameter Selection**: The method for choosing $\\epsilon$ is correct. It is based on the discrepancy principle, linking $\\epsilon$ to the statistical properties of the noise, with the scaling $\\epsilon \\approx \\sqrt{M}\\sigma$ being a standard result.\n-   **Verdict**: **Correct**.\n\n**B. $\\ell_1$ regularization primarily shrinks all coefficients uniformly but does not yield exact zeros; sparsity arises only if features are orthogonal. For noisy outputs, BPDN is the unconstrained Lagrangian form $\\min_{\\mathbf{c} \\in \\mathbb{R}^P} \\|\\boldsymbol{\\Phi}\\mathbf{c} - \\mathbf{y}\\|_2^2 + \\lambda \\|\\mathbf{c}\\|_1,$ with $\\lambda$ selected by minimizing the in-sample residual norm $\\|\\boldsymbol{\\Phi}\\mathbf{c} - \\mathbf{y}\\|_2$, since smaller residuals indicate better fit.**\n\n-   **Justification for Sparsity**: This statement is incorrect. It more accurately describes $\\ell_2$ (ridge) regression, which shrinks coefficients but does not produce exact zeros. A key feature of $\\ell_1$ regularization is precisely its ability to set coefficients to exactly zero (soft-thresholding), even for highly correlated features.\n-   **BPDN Formulation**: The formulation shown is the Lagrangian form, commonly known as LASSO (Least Absolute Shrinkage and Selection Operator). While mathematically equivalent to BPDN for some correspondence between $\\lambda$ and $\\epsilon$, it is not the canonical BPDN formulation.\n-   **Parameter Selection**: The method for selecting $\\lambda$ is fundamentally flawed. Minimizing the in-sample residual $\\|\\boldsymbol{\\Phi}\\mathbf{c} - \\mathbf{y}\\|_2$ would lead to choosing $\\lambda = 0$, which removes the regularization entirely and results in an unregularized (and likely overfit) solution. Proper selection of $\\lambda$ requires a criterion that balances fit and complexity, such as cross-validation.\n-   **Verdict**: **Incorrect**.\n\n**C. $\\ell_1$ regularization promotes sparsity because it penalizes large coefficients more heavily than small ones, similar to ridge regression. For noisy outputs, BPDN is $\\min_{\\mathbf{c} \\in \\mathbb{R}^P} \\|\\mathbf{c}\\|_2 \\;\\; \\text{s.t.} \\;\\; \\|\\boldsymbol{\\Phi}\\mathbf{c} - \\mathbf{y}\\|_2 \\le \\epsilon,$ with $\\epsilon$ set to $\\epsilon = \\|\\mathbf{y}\\|_2 / \\sqrt{M}$ so that the residual is normalized by the sample size.**\n\n-   **Justification for Sparsity**: The reasoning is misleading. The key difference is not that it penalizes large coefficients more, but the constant penalty on the absolute value, which creates the thresholding effect. The comparison to ridge regression ($\\ell_2$) is wrong, as ridge regression does not induce sparsity.\n-   **BPDN Formulation**: The formulation is incorrect. It minimizes the $\\ell_2$ norm of the coefficients, $\\|\\mathbf{c}\\|_2$, which corresponds to ridge regression, not BPDN. BPDN is defined by the $\\ell_1$ norm.\n-   **Parameter Selection**: The choice of $\\epsilon$ is incorrect and unprincipled. It depends on the norm of the measurement vector $\\mathbf{y}$, not the noise level. For a high signal-to-noise ratio, $\\|\\mathbf{y}\\|_2$ would be large, making $\\epsilon$ inappropriately large and allowing for overly sparse or trivial solutions (e.g., $\\mathbf{c}=0$).\n-   **Verdict**: **Incorrect**.\n\n**D. $\\ell_1$ regularization promotes sparsity only when the number of samples $M$ exceeds the number of terms $P$ by a large margin; otherwise it behaves like an unregularized least squares estimator. For noisy outputs, BPDN is $\\min_{\\mathbf{c} \\in \\mathbb{R}^P} \\|\\mathbf{c}\\|_1 \\;\\; \\text{s.t.} \\;\\; \\|\\boldsymbol{\\Phi}\\mathbf{c} - \\mathbf{y}\\|_1 \\le \\epsilon,$ with $\\epsilon$ selected by the Bayesian Information Criterion (BIC) to penalize model complexity regardless of the noise distribution.**\n\n-   **Justification for Sparsity**: This statement is the opposite of the truth. The primary utility of $\\ell_1$ regularization in this context is precisely for the underdetermined case ($M  P$), where standard least squares has no unique solution. In the overdetermined case ($M  P$), its role is better described as variable selection.\n-   **BPDN Formulation**: The formulation is incorrect. It uses an $\\ell_1$ norm for the residual, $\\|\\boldsymbol{\\Phi}\\mathbf{c} - \\mathbf{y}\\|_1$. While this is a valid robust regression formulation (Least Absolute Deviations), it is not the standard BPDN, which uses the $\\ell_2$ norm motivated by statistical properties of i.i.d. noise.\n-   **Parameter Selection**: This is a category error. BIC is a model selection criterion, typically used to compare models with different complexities (e.g., to choose the optimal regularization parameter $\\lambda$ in the LASSO form). It is not a method for directly setting the noise-level bound $\\epsilon$ in the BPDN formulation.\n-   **Verdict**: **Incorrect**.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "While PCEs are remarkably effective for smooth models, they can struggle when the quantity of interest has sharp features or discontinuities, such as those from a voltage cut-off or a safety switch. This practice explores the resulting Gibbs-like phenomenon, where the polynomial approximation exhibits persistent oscillations near the discontinuity. By analyzing this limitation, you will learn to identify when standard PCE might fail and evaluate advanced mitigation strategies that are crucial for robustly modeling real-world battery systems .",
            "id": "3941459",
            "problem": "An automated battery discharge test uses a simple Thevenin-equivalent single-resistor model with voltage $V(t;\\xi)=V_{\\mathrm{oc}}(z(t)) - I R(\\xi)$, where $V_{\\mathrm{oc}}$ is the open-circuit voltage as a function of state-of-charge $z(t)$, $I$ is a prescribed constant discharge current, and $R(\\xi)$ is the internal resistance parameterized by a scalar random input $\\xi$ that captures manufacturing variability. The test includes a hard cut-off at voltage $V_{\\mathrm{cut}}$: discharge is terminated at the stopping time $\\tau(\\xi)$ defined by the event $V(\\tau(\\xi);\\xi)=V_{\\mathrm{cut}}$, or at a fixed horizon $T$ if the cut-off is not reached earlier. Consider the quantity of interest $Y(\\xi)$ defined as the safety trip indicator up to time $T$, \n$$\nY(\\xi)=\\mathbf{1}\\{\\tau(\\xi)\\le T\\}.\n$$\nAssume the input $\\xi\\sim \\mathcal{U}(-1,1)$ with probability density $\\rho(\\xi)=\\tfrac{1}{2}$, and that $R(\\xi)$ is monotone in $\\xi$ so that the event surface $\\{\\xi: \\tau(\\xi)=T\\}$ is a single threshold $\\xi_0\\in(-1,1)$, hence \n$$\nY(\\xi)=H(\\xi-\\xi_0),\n$$\nwhere $H$ is the Heaviside step. To propagate uncertainty, one constructs a Polynomial Chaos Expansion (PCE) of $Y(\\xi)$ using the Legendre basis $\\{P_n(\\xi)\\}_{n\\ge 0}$ orthonormal on $[-1,1]$ with respect to $\\rho(\\xi)$. The truncated $L^2$ projection of degree $p$ is\n$$\nY_p(\\xi)=\\sum_{n=0}^p c_n \\,\\phi_n(\\xi), \\quad \\phi_n(\\xi)=\\sqrt{\\tfrac{2n+1}{2}}\\,P_n(\\xi), \\quad c_n=\\int_{-1}^1 Y(\\zeta)\\,\\phi_n(\\zeta)\\,\\rho(\\zeta)\\,d\\zeta.\n$$\n\nStarting from the definitions of $L^2$ projection and orthogonality of Legendre polynomials, explain why the truncation $Y_p(\\xi)$ exhibits non-vanishing oscillatory overshoots and undershoots localized near the threshold $\\xi_0$ as $p\\to\\infty$ (a Gibbs-like phenomenon), even though $Y\\in L^2([-1,1],\\rho)$. Your explanation should use only properties of orthogonal projections and kernels (for example, the Christoffel–Darboux identity) and should not assume any special Fourier-series facts. Then, based on this analysis and the battery switching mechanism, identify which of the following statements are correct about mitigation strategies and their implications in this context.\n\nA. Simply increasing the global polynomial degree $p$ in the Legendre PCE will eliminate the oscillations and recover exponential convergence of $Y_p$ to $Y$ in the $L^\\infty$ norm, even in the presence of the cut-off switching.\n\nB. Constructing a piecewise PCE by partitioning the input domain at $\\xi_0$ into two elements $[-1,\\xi_0)$ and $(\\xi_0,1]$ and fitting separate local Legendre expansions on each element can suppress Gibbs-like ringing and restore fast convergence consistent with the smoothness of $Y$ restricted to each element.\n\nC. When estimating a PCE from data via regression, augmenting the least-squares fit with a Total Variation (TV) penalty in the input domain, i.e., solving $\\min_{u\\in\\mathcal{P}_p}\\sum_{j=1}^m\\big(u(\\xi^{(j)})-Y^{(j)}\\big)^2+\\lambda\\int_{-1}^1 |u'(\\xi)|\\,d\\xi$ with $\\lambda0$, can reduce oscillations near $\\xi_0$ but may introduce bias in estimates of moments such as the mean and variance.\n\nD. Pre-smoothing the output with a Gaussian kernel, i.e., replacing $Y(\\xi)$ by $(G_\\sigma\\ast Y)(\\xi)$ with a small standard deviation $\\sigma>0$, yields a PCE whose mean and variance remain unbiased estimators of those of $Y$ while removing Gibbs oscillations.\n\nE. An affine reparameterization of the input, $\\eta=a\\xi+b$ with constants $a\\ne 0$ and $b$, always removes Gibbs-like oscillations for step-type quantities like $Y(\\xi)$, regardless of the orientation of the discontinuity surface in higher-dimensional inputs.\n\nSelect all that apply. Provide a rigorous, principle-based argument to justify the Gibbs-like phenomenon and to evaluate each option. All mathematical symbols, variables, functions, operators, and numbers must appear in LaTeX. Assume $V_{\\mathrm{oc}}$ is smooth and strictly decreasing in $z$ over the relevant range, $R(\\xi)$ is smooth and strictly increasing in $\\xi$, and $I$, $V_{\\mathrm{cut}}$, and $T$ are fixed positive constants.",
            "solution": "The problem statement poses a valid and well-posed question in the field of uncertainty quantification, grounded in a realistic (though simplified) model of battery discharge. The core of the problem lies in the mathematical properties of Polynomial Chaos Expansions (PCE) when approximating discontinuous functions, which is a standard and important topic. All assumptions are clearly stated and are physically and mathematically consistent.\n\nThe first task is to explain the Gibbs-like phenomenon for the Legendre PCE of the step function $Y(\\xi) = H(\\xi - \\xi_0)$.\n\nThe truncated PCE of degree $p$, denoted $Y_p(\\xi)$, is the orthogonal projection of the function $Y(\\xi)$ onto the subspace $\\mathcal{P}_p$ of polynomials of degree at most $p$. The projection is defined with respect to the $L^2$ inner product $\\langle f, g \\rangle = \\int_{-1}^1 f(\\xi)g(\\xi)\\rho(\\xi)d\\xi$. The projection $Y_p$ is the unique polynomial in $\\mathcal{P}_p$ that minimizes the squared error in the $L^2$ norm:\n$$\nY_p = \\arg\\min_{u \\in \\mathcal{P}_p} \\|Y - u\\|_{L^2}^2 = \\arg\\min_{u \\in \\mathcal{P}_p} \\int_{-1}^1 (Y(\\xi) - u(\\xi))^2 \\rho(\\xi) d\\xi\n$$\nThe formal expression for this projection is given by\n$$\nY_p(\\xi) = \\sum_{n=0}^p c_n \\phi_n(\\xi) = \\int_{-1}^1 Y(\\zeta) \\left( \\sum_{n=0}^p \\phi_n(\\xi) \\phi_n(\\zeta) \\right) \\rho(\\zeta) d\\zeta\n$$\nLet's define the kernel of the projection operator as $K_p(\\xi, \\zeta) = \\sum_{n=0}^p \\phi_n(\\xi) \\phi_n(\\zeta)$. The projection can then be written as a convolution-like integral:\n$$\nY_p(\\xi) = \\int_{-1}^1 Y(\\zeta) K_p(\\xi, \\zeta) \\rho(\\zeta) d\\zeta\n$$\nSince $Y(\\xi)$ is the Heaviside function $H(\\xi-\\xi_0)$, the integral becomes:\n$$\nY_p(\\xi) = \\int_{\\xi_0}^1 K_p(\\xi, \\zeta) \\rho(\\zeta) d\\zeta\n$$\nThe Christoffel-Darboux formula for the orthonormal Legendre polynomials $\\phi_n(\\xi) = \\sqrt{\\frac{2n+1}{2}}P_n(\\xi)$ provides an expression for the kernel:\n$$\nK_p(\\xi, \\zeta) = \\sqrt{\\tfrac{p+1}{2}\\tfrac{2p+3}{2}} \\frac{\\phi_{p+1}(\\xi)\\phi_p(\\zeta) - \\phi_p(\\xi)\\phi_{p+1}(\\zeta)}{\\xi-\\zeta}\n$$\nThe key to understanding the Gibbs phenomenon lies in the properties of this projection.\n1.  **Nature of Convergence**: Since $Y(\\xi)$ is square-integrable on $[-1, 1]$ (i.e., $Y \\in L^2([-1,1], \\rho)$), the theory of orthogonal expansions guarantees that the sequence of projections $Y_p$ converges to $Y$ in the $L^2$ norm. That is, $\\|Y_p - Y\\|_{L^2} \\to 0$ as $p \\to \\infty$.\n2.  **Lack of Uniform Convergence**: However, $L^2$ convergence does not imply uniform convergence (convergence in the $L^\\infty$ norm, $\\|f\\|_\\infty = \\sup_\\xi |f(\\xi)|$). For uniform convergence to hold, the limit function must be continuous, as the uniform limit of a sequence of continuous functions (the polynomials $Y_p$) must be continuous. Our target function $Y(\\xi)$ has a jump discontinuity at $\\xi = \\xi_0$. Therefore, the convergence of $Y_p$ to $Y$ cannot be uniform.\n3.  **Mechanism of Oscillation**: The failure to converge uniformly is precisely the Gibbs phenomenon. The polynomial $Y_p(\\xi)$, being infinitely smooth, cannot reproduce a sharp jump. To minimize the integrated squared error, the polynomial must rise very steeply near $\\xi_0$. A steep polynomial rise inevitably leads to an \"overshoot\" past the target value of $1$. As the degree $p$ increases, the approximation gets better in the $L^2$ sense: the oscillations are squeezed into a narrower and narrower region around the discontinuity $\\xi_0$. However, the maximum height of the overshoot does not decrease to zero. Instead, it converges to a fixed value greater than $1$ (approximately $1.09$ for a normalized jump of $1$ in the classic Fourier series case; the value is different but the principle is the same for Legendre series). This non-vanishing amplitude of the oscillation is the hallmark of the Gibbs phenomenon. The integral nature of the $L^2$ projection means it is insensitive to localized, large pointwise errors as long as the region they occupy has a measure that shrinks sufficiently fast.\n\nNow, we evaluate each of the proposed statements.\n\n**A. Simply increasing the global polynomial degree $p$ in the Legendre PCE will eliminate the oscillations and recover exponential convergence of $Y_p$ to $Y$ in the $L^\\infty$ norm, even in the presence of the cut-off switching.**\nThis statement is fundamentally incorrect. As explained above, the Gibbs phenomenon is characterized by non-vanishing oscillations for PCEs of discontinuous functions. Increasing the polynomial degree $p$ localizes the oscillations closer to the discontinuity but does not eliminate them. Therefore, convergence in the $L^\\infty$ norm (uniform convergence) is not achieved. Furthermore, exponential convergence of PCEs is typically associated with functions that are analytic within an ellipse in the complex plane containing the interval $[-1, 1]$. The function $Y(\\xi)$ is a step function, which is not even continuous, let alone analytic. The convergence rate in $L^2$ is algebraic, not exponential.\n**Verdict: Incorrect**\n\n**B. Constructing a piecewise PCE by partitioning the input domain at $\\xi_0$ into two elements $[-1,\\xi_0)$ and $(\\xi_0,1]$ and fitting separate local Legendre expansions on each element can suppress Gibbs-like ringing and restore fast convergence consistent with the smoothness of $Y$ restricted to each element.**\nThis statement is correct. The Gibbs phenomenon arises from approximating a non-smooth function with a single global basis of smooth functions. By partitioning the domain at the point of discontinuity $\\xi_0$, this strategy treats the problem as two separate approximation problems. On the subdomain $[-1, \\xi_0)$, the function $Y(\\xi)$ is identically zero. On the subdomain $(\\xi_0, 1]$, the function is identically one. Both $f(\\xi)=0$ and $f(\\xi)=1$ are constant functions, hence they are infinitely smooth ($C^\\infty$). A polynomial expansion can represent a constant exactly with a single, degree-$0$ term. Therefore, a piecewise PCE (with degree $p \\ge 0$ on each piece) will represent $Y(\\xi)$ exactly, with no error and consequently no Gibbs oscillations. This restores the convergence properties to be consistent with the smoothness of the function *within each subdomain*.\n**Verdict: Correct**\n\n**C. When estimating a PCE from data via regression, augmenting the least-squares fit with a Total Variation (TV) penalty in the input domain, i.e., solving $\\min_{u\\in\\mathcal{P}_p}\\sum_{j=1}^m\\big(u(\\xi^{(j)})-Y^{(j)}\\big)^2+\\lambda\\int_{-1}^1 |u'(\\xi)|\\,d\\xi$ with $\\lambda0$, can reduce oscillations near $\\xi_0$ but may introduce bias in estimates of moments such as the mean and variance.**\nThis statement is correct. The standard least-squares fit is a discrete analogue of the $L^2$ projection and thus exhibits Gibbs oscillations. The term $\\lambda\\int_{-1}^1 |u'(\\xi)|\\,d\\xi$ is a Total Variation (TV) regularization penalty. This penalty term discourages solutions $u(\\xi)$ with large variations, such as the rapid oscillations of the Gibbs phenomenon. Maximizing this penalty favors smoother, less oscillatory functions. Thus, including it in the minimization objective will suppress the ringing artifacts. However, this comes at a cost. The solution to the regularized problem is no longer the true projection; it is a biased approximation. The regularization systematically alters the coefficients of the polynomial away from their unbiased least-squares values. Since the moments of the random variable (e.g., mean $\\mathbb{E}[Y]$, variance $\\text{Var}[Y]$) are computed from these coefficients (e.g., $\\mathbb{E}[Y_p] = c_0 \\phi_0(0)$), using the biased coefficients from the TV-regularized fit will result in biased estimates of the moments. This is a classic example of the bias-variance trade-off in statistics and machine learning.\n**Verdict: Correct**\n\n**D. Pre-smoothing the output with a Gaussian kernel, i.e., replacing $Y(\\xi)$ by $(G_\\sigma\\ast Y)(\\xi)$ with a small standard deviation $\\sigma0$, yields a PCE whose mean and variance remain unbiased estimators of those of $Y$ while removing Gibbs oscillations.**\nThis statement is incorrect. Convolving the step function $Y(\\xi)$ with a Gaussian kernel $G_\\sigma$ produces a smooth function $Y_\\sigma(\\xi)$. A PCE of this new, smooth function $Y_\\sigma(\\xi)$ will indeed converge rapidly and without Gibbs oscillations. So the first part of the claim is correct. However, the second part is false. The PCE of $Y_\\sigma(\\xi)$ gives us approximations of the moments of $Y_\\sigma(\\xi)$, not $Y$. The process of smoothing changes the function, and therefore changes its moments. For example, the mean is $\\mathbb{E}[Y] = \\int_{-1}^1 Y(\\xi)\\rho(\\xi)d\\xi$. The mean of the smoothed function is $\\mathbb{E}[Y_\\sigma] = \\int_{-1}^1 Y_\\sigma(\\xi)\\rho(\\xi)d\\xi$. Since $Y_\\sigma \\neq Y$, there is no reason to expect $\\mathbb{E}[Y_\\sigma] = \\mathbb{E}[Y]$. The smoothing operation introduces a systematic error, or bias, into the moments. Thus, the mean and variance of the PCE of $Y_\\sigma(\\xi)$ are biased estimators of the mean and variance of $Y$.\n**Verdict: Incorrect**\n\n**E. An affine reparameterization of the input, $\\eta=a\\xi+b$ with constants $a\\ne 0$ and $b$, always removes Gibbs-like oscillations for step-type quantities like $Y(\\xi)$, regardless of the orientation of the discontinuity surface in higher-dimensional inputs.**\nThis statement is incorrect. An affine reparameterization $\\eta=a\\xi+b$ simply maps the interval $[-1, 1]$ to another interval and shifts/scales the coordinate system. The function $Y(\\xi) = H(\\xi-\\xi_0)$ becomes $\\tilde{Y}(\\eta) = H((\\eta-b)/a - \\xi_0)$, which is still a step function with a discontinuity at $\\eta_0 = a\\xi_0+b$. Approximating this new step function $\\tilde{Y}(\\eta)$ with polynomials in $\\eta$ will still produce the Gibbs phenomenon for the exact same reasons as before. An affine map does not alter the fundamental nature of the problem: approximating a discontinuous function with smooth global basis functions. The claim is patently false, and its generalization to higher dimensions is equally invalid.\n**Verdict: Incorrect**",
            "answer": "$$\\boxed{BC}$$"
        }
    ]
}