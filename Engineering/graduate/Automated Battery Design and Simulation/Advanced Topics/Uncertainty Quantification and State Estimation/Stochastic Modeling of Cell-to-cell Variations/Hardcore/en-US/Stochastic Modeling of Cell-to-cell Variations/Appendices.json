{
    "hands_on_practices": [
        {
            "introduction": "A crucial first step in managing cell-to-cell variation is to identify which physical parameters have the most significant impact on performance. Local sensitivity analysis provides a powerful framework for this, quantifying how a small change in an input parameter affects an output metric. This practice focuses on deriving such a sensitivity measure from first principles, connecting the electrolyte-filled porosity $\\varepsilon$ to the terminal voltage $V$ through the gradient $\\partial V / \\partial \\varepsilon$ . By working through a simplified porous-electrode model, you will gain a concrete understanding of how fundamental material properties drive macroscopic electrical behavior and its variability.",
            "id": "3953319",
            "problem": "Consider a one-dimensional, isothermal porous-electrode representation of a lithium-ion cell under galvanostatic operation, suitable for automated battery design and simulation and stochastic modeling of cell-to-cell variations. The focus is on the electrolyte-phase ohmic drop within the porous structure, where porosity modulates the effective electrolyte conductivity. Assume the following physically justified simplifications at a nominal low-rate operating condition: (i) solid-phase electrical conductivity is sufficiently high that the solid potential is spatially uniform in each current collector; (ii) electrolyte concentration gradients are small so that migrational and diffusional contributions to the electrolyte current are negligible compared to purely ohmic conduction; (iii) reaction current density is smoothly distributed so that the macroscopic electrolyte current density can be treated as uniform across the considered electrolyte path; and (iv) temperature is constant.\n\nLet the effective electrolyte conductivity obey the Bruggeman-type relation $\\,\\kappa_{\\mathrm{eff}}(\\varepsilon) = \\kappa_{0}\\,\\varepsilon^{b}\\,$, where $\\,\\varepsilon\\,$ is the electrolyte-filled porosity in the porous pathway, $\\,\\kappa_{0}\\,$ is the intrinsic conductivity of the electrolyte, and $\\,b\\,$ is the Bruggeman exponent. Under the assumptions above, electrolyte Ohm’s law and charge conservation imply a linear electrolyte potential drop across the effective electrolyte path of length $\\,L_{e}\\,$, yielding a terminal voltage contribution that depends on $\\,\\kappa_{\\mathrm{eff}}(\\varepsilon)\\,$. Denote the applied current by $\\,I\\,$ and the cell geometric area by $\\,A\\,$, so that the macroscopic current density is $\\,j = I/A\\,$. Treat all other contributions to the terminal voltage as independent of $\\,\\varepsilon\\,$ at the nominal point.\n\nStarting from electrolyte charge conservation and Ohm’s law in porous media, derive from first principles the gradient-based local sensitivity measure $\\,S_{\\varepsilon} = \\partial V/\\partial \\varepsilon\\,$ of the terminal voltage $\\,V\\,$ with respect to porosity $\\,\\varepsilon\\,$. Then evaluate $\\,S_{\\varepsilon}\\,$ at the nominal operating point using the parameter values $\\,I = 10\\,$ A, $\\,A = 0.01\\,$ m$^{2}$, $\\,L_{e} = 100\\,\\mu\\mathrm{m}\\,$, $\\,\\kappa_{0} = 1\\,$ S/m, $\\,b = 3/2\\,$, and $\\,\\varepsilon_{0} = 0.40\\,$.\n\nExpress the sensitivity $\\,S_{\\varepsilon}\\,$ in volts and round your final numerical answer to four significant figures. Finally, interpret the units and physical meaning of $\\,S_{\\varepsilon}\\,$ in the context of small stochastic cell-to-cell porosity variations near $\\,\\varepsilon_{0}\\,$ (e.g., a zero-mean random perturbation $\\,\\delta \\varepsilon\\,$ with variance $\\,\\sigma_{\\varepsilon}^{2}\\,$), but ensure that your final reported answer is the single requested calculation.",
            "solution": "The problem statement has been validated and is deemed scientifically grounded, well-posed, and objective. It provides a set of simplifying assumptions common in introductory-level battery modeling to pose a solvable problem in sensitivity analysis. All necessary parameters are provided.\n\nThe objective is to derive the local sensitivity measure $S_{\\varepsilon} = \\partial V/\\partial \\varepsilon$, which quantifies the change in terminal voltage $V$ with respect to a change in electrolyte-filled porosity $\\varepsilon$.\n\nThe terminal voltage $V$ of the cell is the sum of various contributions, including the equilibrium potential, activation overpotentials, and ohmic losses. The problem states that all contributions to the terminal voltage, except for the electrolyte-phase ohmic drop, are to be treated as independent of porosity $\\varepsilon$. We can thus write the terminal voltage as:\n$$V(\\varepsilon) = V_{\\text{other}} - V_{\\Omega,e}(\\varepsilon)$$\nwhere $V_{\\Omega,e}$ is the voltage loss due to ohmic resistance in the electrolyte, and $V_{\\text{other}}$ encapsulates all other voltage contributions, which are constant with respect to $\\varepsilon$.\n\nThe sensitivity $S_{\\varepsilon}$ is the partial derivative of $V$ with respect to $\\varepsilon$:\n$$S_{\\varepsilon} = \\frac{\\partial V}{\\partial \\varepsilon} = \\frac{\\partial}{\\partial \\varepsilon} \\left( V_{\\text{other}} - V_{\\Omega,e}(\\varepsilon) \\right)$$\nSince $V_{\\text{other}}$ is independent of $\\varepsilon$, its derivative is zero.\n$$S_{\\varepsilon} = - \\frac{\\partial V_{\\Omega,e}}{\\partial \\varepsilon}$$\nOur first task is to find an expression for the ohmic voltage loss, $V_{\\Omega,e}$. The problem directs us to use electrolyte charge conservation and Ohm's law. In a one-dimensional system, Ohm's law relates the electrolyte current density $j_e$ to the gradient of the electrolyte potential $\\phi_e$:\n$$j_e(x) = -\\kappa_{\\mathrm{eff}} \\frac{d\\phi_e}{dx}$$\nwhere $\\kappa_{\\mathrm{eff}}$ is the effective electrolyte conductivity.\n\nThe problem specifies to assume that the macroscopic electrolyte current density is uniform across the effective electrolyte path of length $L_e$. This means $j_e(x)$ is constant and equal to the total applied current density $j = I/A$.\n$$j_e = j = \\frac{I}{A}$$\nWith $j_e$ and $\\kappa_{\\mathrm{eff}}$ constant along the path, the potential gradient $d\\phi_e/dx$ is also constant. We can find the potential drop $\\Delta \\phi_e$ by integrating over the path length $L_e$:\n$$\\Delta \\phi_e = \\phi_e(L_e) - \\phi_e(0) = \\int_0^{L_e} \\frac{d\\phi_e}{dx} dx = \\int_0^{L_e} \\left( -\\frac{j_e}{\\kappa_{\\mathrm{eff}}} \\right) dx = -\\frac{j_e L_e}{\\kappa_{\\mathrm{eff}}}$$\nThe ohmic voltage loss $V_{\\Omega,e}$ is the magnitude of this potential drop, which is a positive quantity:\n$$V_{\\Omega,e} = |\\Delta \\phi_e| = \\frac{j_e L_e}{\\kappa_{\\mathrm{eff}}} = \\frac{I L_e}{A \\kappa_{\\mathrm{eff}}}$$\nThe problem provides the Bruggeman relation for the effective conductivity:\n$$\\kappa_{\\mathrm{eff}}(\\varepsilon) = \\kappa_{0}\\varepsilon^{b}$$\nSubstituting this into the expression for $V_{\\Omega,e}$ gives its dependence on porosity $\\varepsilon$:\n$$V_{\\Omega,e}(\\varepsilon) = \\frac{I L_e}{A (\\kappa_{0}\\varepsilon^{b})} = \\frac{I L_e}{A \\kappa_{0}} \\varepsilon^{-b}$$\nNow, we can compute the sensitivity $S_{\\varepsilon}$ by taking the negative partial derivative of $V_{\\Omega,e}$ with respect to $\\varepsilon$:\n$$S_{\\varepsilon} = - \\frac{\\partial}{\\partial \\varepsilon} \\left( \\frac{I L_e}{A \\kappa_{0}} \\varepsilon^{-b} \\right)$$\nThe term $\\frac{I L_e}{A \\kappa_{0}}$ is a constant with respect to $\\varepsilon$. Applying the power rule for differentiation:\n$$S_{\\varepsilon} = - \\frac{I L_e}{A \\kappa_{0}} \\frac{d}{d\\varepsilon}(\\varepsilon^{-b}) = - \\frac{I L_e}{A \\kappa_{0}} (-b \\varepsilon^{-b-1})$$\nThis yields the final analytical expression for the sensitivity:\n$$S_{\\varepsilon} = \\frac{b I L_e}{A \\kappa_{0} \\varepsilon^{b+1}}$$\nNext, we evaluate this expression using the given nominal parameter values:\n- $I = 10\\,$ A\n- $A = 0.01\\,$ m$^{2}$\n- $L_{e} = 100\\,\\mu\\mathrm{m} = 100 \\times 10^{-6}\\,$ m $= 1 \\times 10^{-4}\\,$ m\n- $\\kappa_{0} = 1\\,$ S/m\n- $b = 3/2 = 1.5$\n- $\\varepsilon = \\varepsilon_{0} = 0.40$\n\nLet's substitute these values into the expression for $S_{\\varepsilon}$:\n$$S_{\\varepsilon} = \\frac{(1.5) (10\\,\\text{A}) (1 \\times 10^{-4}\\,\\text{m})}{(0.01\\,\\text{m}^2) (1\\,\\text{S/m}) (0.40)^{1.5+1}}$$\n$$S_{\\varepsilon} = \\frac{1.5 \\times 10^{-3}}{0.01 \\times (0.40)^{2.5}} \\quad \\left[\\frac{\\text{A} \\cdot \\text{m}}{\\text{m}^2 \\cdot \\text{S/m}}\\right]$$\nThe units simplify as follows: $\\frac{\\text{A} \\cdot \\text{m}}{\\text{m}^2 \\cdot \\text{S/m}} = \\frac{\\text{A}}{\\text{m} \\cdot \\text{S}} = \\frac{\\text{A}}{\\text{m} \\cdot \\text{(A/V)/m}} = \\frac{\\text{A}}{\\text{A/V}} = \\text{V}$. The unit of sensitivity is Volts, as required.\n\nNow we compute the numerical value:\n$$S_{\\varepsilon} = \\frac{1.5 \\times 10^{-3}}{10^{-2} \\times (0.40)^{2.5}} = \\frac{0.15}{(0.40)^{2.5}}$$\nThe denominator is $(0.40)^{2.5} \\approx 0.10119288$.\n$$S_{\\varepsilon} \\approx \\frac{0.15}{0.10119288} \\approx 1.482316\\,\\text{V}$$\nRounding the result to four significant figures gives:\n$$S_{\\varepsilon} \\approx 1.482\\,\\text{V}$$\nThe physical interpretation of this result is that at the nominal operating point, the cell's terminal voltage is sensitive to changes in porosity. The sensitivity $S_{\\varepsilon} = \\partial V/\\partial \\varepsilon \\approx 1.482\\,$ V has units of volts (as porosity is dimensionless), representing the change in terminal voltage per unit change in porosity. The positive sign indicates that an increase in porosity $\\varepsilon$ leads to an increase in terminal voltage $V$. This is physically consistent: higher porosity increases the effective electrolyte conductivity $\\kappa_{\\mathrm{eff}}$, which reduces the ohmic voltage loss $V_{\\Omega,e}$. Since the terminal voltage is given by $V = V_{\\text{other}} - V_{\\Omega,e}$, a smaller ohmic loss results in a higher terminal voltage. In the context of stochastic modeling, this sensitivity value is a crucial parameter. For small, zero-mean porosity variations $\\delta\\varepsilon$ with variance $\\sigma_{\\varepsilon}^{2}$ across a population of cells, the resulting first-order variation in terminal voltage is $\\delta V \\approx S_{\\varepsilon} \\delta\\varepsilon$. The variance of the voltage can be approximated as $\\sigma_{V}^{2} \\approx S_{\\varepsilon}^{2} \\sigma_{\\varepsilon}^{2}$. Therefore, $S_{\\varepsilon}$ directly relates manufacturing variability in porosity to performance variability in terminal voltage.",
            "answer": "$$\\boxed{1.482}$$"
        },
        {
            "introduction": "While sensitivity analysis reveals the impact of individual parameters, real-world variations involve multiple parameters that are often statistically correlated. To predict the total performance variability, we must propagate the combined uncertainty from all these random inputs through our model. This exercise introduces the first-order Delta method, a cornerstone of uncertainty propagation that uses the sensitivities (gradients) of a model's output with respect to its inputs to approximate the output's variance. By applying this method to a Single-Particle Model with random particle radius $r$ and diffusion coefficient $D$, you will learn how to translate knowledge of input statistics, including their covariance, into a prediction of cell voltage variance .",
            "id": "3953335",
            "problem": "Consider a Single-Particle Model (SPM) of a porous electrode in a lithium-ion cell, where each active material grain is modeled as a sphere of random radius $r$ and random solid-phase diffusion coefficient $D$. The electrode is driven at time $t=0$ by a constant volumetric current density $i_{n}$ (units $\\mathrm{A}\\,\\mathrm{m}^{-3}$), and the time of interest $t_{0}$ satisfies the early-time condition $t_{0} \\ll r^{2}/D$ for the nominal values. Neglect charge-transfer and ohmic drops so that the terminal voltage is approximated by the open-circuit potential $U(c_{s}^{\\mathrm{surf}}(t))$, linearized around the initial uniform concentration $c_{0}$ as $U(c_{s}^{\\mathrm{surf}}(t)) \\approx U(c_{0}) + U'(c_{0})\\,(c_{s}^{\\mathrm{surf}}(t) - c_{0})$, where $U'(c_{0})$ is the slope of the open-circuit voltage with respect to solid concentration at $c_{0}$. The surface flux boundary condition at the particle surface is related to the applied current by Faraday’s law and the specific surface area $a_{s}$ via $J = i_{n}/(F a_{s})$, where the specific surface area for spherical particles of radius $r$ is $a_{s} = 3 \\epsilon_{s}/r$, with $\\epsilon_{s}$ the solid volume fraction and $F$ the Faraday constant.\n\nUse Fick’s second law in the solid particle and the early-time semi-infinite diffusion approximation to obtain the leading-order dependence of $c_{s}^{\\mathrm{surf}}(t_{0})$ on $(r,D)$, then linearize the terminal voltage $V(t_{0})$ with respect to small perturbations in $(r,D)$ about their means. Finally, use the first-order delta method to approximate the variance of the terminal voltage across a population of cells with independent realizations of $(r,D)$.\n\nAssume the following physically plausible parameters and statistics:\n- Volumetric current density: $i_{n} = 1.0 \\times 10^{6}\\ \\mathrm{A}\\,\\mathrm{m}^{-3}$.\n- Time of interest: $t_{0} = 1.0\\ \\mathrm{s}$.\n- Faraday constant: $F = 96485\\ \\mathrm{C}\\,\\mathrm{mol}^{-1}$.\n- Solid volume fraction: $\\epsilon_{s} = 0.50$.\n- Open-circuit slope at $c_{0}$: $U'(c_{0}) = 3.0 \\times 10^{-6}\\ \\mathrm{V}\\,(\\mathrm{mol}\\,\\mathrm{m}^{-3})^{-1}$.\n- Joint statistics of $(r,D)$ across cells: mean radius $\\mu_{r} = 5.0 \\times 10^{-6}\\ \\mathrm{m}$, standard deviation $\\sigma_{r} = 2.0 \\times 10^{-7}\\ \\mathrm{m}$; mean diffusivity $\\mu_{D} = 1.0 \\times 10^{-14}\\ \\mathrm{m}^{2}\\,\\mathrm{s}^{-1}$, standard deviation $\\sigma_{D} = 2.0 \\times 10^{-15}\\ \\mathrm{m}^{2}\\,\\mathrm{s}^{-1}$; correlation coefficient $\\rho = 0.40$.\n\nUnder these assumptions and using a first-order linearization about $(\\mu_{r},\\mu_{D})$, compute the approximate variance of the terminal voltage at time $t_{0}$, denoted $\\mathrm{Var}[V(t_{0})]$. Express the final variance in $\\mathrm{V}^{2}$ and round your answer to four significant figures.",
            "solution": "The problem statement has been validated and is deemed valid. It is scientifically grounded in established principles of electrochemical engineering and transport phenomena, specifically the Single-Particle Model for lithium-ion batteries. The problem is well-posed, providing all necessary parameters and statistical information for a unique solution. The language is objective and the setup is internally consistent and physically plausible. The \"early-time\" condition, $t_{0} \\ll r^{2}/D$, is verifiable with the given mean parameters: $(\\mu_r)^2/\\mu_D = (5.0 \\times 10^{-6}\\ \\mathrm{m})^2 / (1.0 \\times 10^{-14}\\ \\mathrm{m}^2\\,\\mathrm{s}^{-1}) = 2500\\ \\mathrm{s}$, which is much larger than $t_0 = 1.0\\ \\mathrm{s}$, justifying the use of the semi-infinite diffusion approximation.\n\nThe solution proceeds in three main steps: first, we determine the solid-phase concentration at the particle surface as a function of the random variables $r$ and $D$; second, we express the terminal voltage as a function of these variables; and third, we apply the first-order delta method to approximate the variance of this voltage.\n\nStep 1: Surface Concentration in the Early-Time Regime\n\nWe begin with Fick's second law for diffusion in the spherical particle. Under the early-time condition $t \\ll r^2/D$, the diffusion depth is small compared to the particle radius, allowing us to approximate the spherical shell near the surface as a semi-infinite planar medium. The diffusion equation is thus:\n$$\n\\frac{\\partial c_s}{\\partial t} = D \\frac{\\partial^2 c_s}{\\partial x^2}\n$$\nwhere $x$ is the spatial coordinate pointing into the particle from its surface. The initial condition is a uniform concentration, $c_s(x, t=0) = c_0$. The boundary condition at the surface ($x=0$) is a constant flux $J$ into the particle, given by:\n$$\n-D \\frac{\\partial c_s}{\\partial x}\\bigg|_{x=0} = J\n$$\nThe solution to this standard transport problem for the concentration at the surface is:\n$$\nc_s^{\\mathrm{surf}}(t) - c_0 = c_s(x=0, t) - c_0 = 2J \\sqrt{\\frac{t}{\\pi D}}\n$$\nThe molar flux $J$ is related to the volumetric current density $i_n$ through Faraday's law and the specific surface area $a_s$. The current per unit active surface area is $i_n/a_s$, so the molar flux is $J = i_n / (F a_s)$. The specific surface area for spherical particles of radius $r$ is given by $a_s = 3\\epsilon_s/r$. Substituting this into the flux expression yields:\n$$\nJ = \\frac{i_n}{F(3\\epsilon_s/r)} = \\frac{i_n r}{3 F \\epsilon_s}\n$$\nSubstituting this expression for $J$ into the surface concentration equation at time $t_0$, we find the dependence of the surface concentration on the random variables $r$ and $D$:\n$$\nc_s^{\\mathrm{surf}}(t_0) - c_0 = 2 \\left(\\frac{i_n r}{3 F \\epsilon_s}\\right) \\sqrt{\\frac{t_0}{\\pi D}} = \\left( \\frac{2 i_n \\sqrt{t_0}}{3 F \\epsilon_s \\sqrt{\\pi}} \\right) \\frac{r}{\\sqrt{D}}\n$$\n\nStep 2: Terminal Voltage as a Function of $(r,D)$\n\nThe terminal voltage $V(t)$ is approximated by the open-circuit potential $U(c_s^{\\mathrm{surf}}(t))$, which is linearized around the initial concentration $c_0$:\n$$\nV(t_0) \\approx U(c_0) + U'(c_0)(c_s^{\\mathrm{surf}}(t_0) - c_0)\n$$\nSubstituting the expression for the surface concentration change, we obtain the voltage as a function of $r$ and $D$:\n$$\nV(t_0) \\approx U(c_0) + U'(c_0) \\left( \\frac{2 i_n \\sqrt{t_0}}{3 F \\epsilon_s \\sqrt{\\pi}} \\right) r D^{-1/2}\n$$\nSince $U(c_0)$ is a constant, it does not contribute to the variance. Let us define a function $h(r,D)$ representing the variable part of the voltage:\n$$\nh(r, D) = C_0 r D^{-1/2} \\quad \\text{where} \\quad C_0 = U'(c_0) \\frac{2 i_n \\sqrt{t_0}}{3 F \\epsilon_s \\sqrt{\\pi}}\n$$\nThus, $\\mathrm{Var}[V(t_0)] \\approx \\mathrm{Var}[h(r, D)]$.\n\nStep 3: Variance Calculation using the Delta Method\n\nThe first-order delta method approximates the variance of a function $h(r,D)$ of two correlated random variables $r$ and $D$ as:\n$$\n\\mathrm{Var}[h(r,D)] \\approx \\left(\\frac{\\partial h}{\\partial r}\\right)^2 \\mathrm{Var}[r] + \\left(\\frac{\\partial h}{\\partial D}\\right)^2 \\mathrm{Var}[D] + 2\\left(\\frac{\\partial h}{\\partial r}\\right)\\left(\\frac{\\partial h}{\\partial D}\\right)\\mathrm{Cov}[r,D]\n$$\nwhere the partial derivatives are evaluated at the mean values $(\\mu_r, \\mu_D)$. The variances are $\\mathrm{Var}[r]=\\sigma_r^2$ and $\\mathrm{Var}[D]=\\sigma_D^2$, and the covariance is $\\mathrm{Cov}[r,D] = \\rho \\sigma_r \\sigma_D$.\n\nFirst, we calculate the partial derivatives of $h(r,D) = C_0 r D^{-1/2}$:\n$$\n\\frac{\\partial h}{\\partial r} = C_0 D^{-1/2} \\quad \\implies \\quad \\left.\\frac{\\partial h}{\\partial r}\\right|_{(\\mu_r, \\mu_D)} = C_0 \\mu_D^{-1/2}\n$$\n$$\n\\frac{\\partial h}{\\partial D} = C_0 r \\left(-\\frac{1}{2} D^{-3/2}\\right) \\quad \\implies \\quad \\left.\\frac{\\partial h}{\\partial D}\\right|_{(\\mu_r, \\mu_D)} = -\\frac{1}{2} C_0 \\mu_r \\mu_D^{-3/2}\n$$\nSubstituting these into the variance formula:\n$$\n\\mathrm{Var}[V(t_0)] \\approx (C_0 \\mu_D^{-1/2})^2 \\sigma_r^2 + \\left(-\\frac{1}{2} C_0 \\mu_r \\mu_D^{-3/2}\\right)^2 \\sigma_D^2 + 2(C_0 \\mu_D^{-1/2})\\left(-\\frac{1}{2} C_0 \\mu_r \\mu_D^{-3/2}\\right)\\rho\\sigma_r\\sigma_D\n$$\nSimplifying the expression:\n$$\n\\mathrm{Var}[V(t_0)] \\approx C_0^2 \\mu_D^{-1}\\sigma_r^2 + \\frac{1}{4} C_0^2 \\mu_r^2 \\mu_D^{-3}\\sigma_D^2 - C_0^2 \\mu_r \\mu_D^{-2}\\rho\\sigma_r\\sigma_D\n$$\nLet $\\mu_h = h(\\mu_r,\\mu_D) = C_0 \\mu_r \\mu_D^{-1/2}$ be the first-order approximation of the mean of $h$. Then $\\mu_h^2 = C_0^2 \\mu_r^2 \\mu_D^{-1}$. We can factor this term out:\n$$\n\\mathrm{Var}[V(t_0)] \\approx C_0^2 \\mu_r^2 \\mu_D^{-1} \\left[ \\left(\\frac{\\sigma_r}{\\mu_r}\\right)^2 + \\frac{1}{4}\\left(\\frac{\\sigma_D}{\\mu_D}\\right)^2 - \\rho\\left(\\frac{\\sigma_r}{\\mu_r}\\right)\\left(\\frac{\\sigma_D}{\\mu_D}\\right) \\right]\n$$\nThis gives the compact form:\n$$\n\\mathrm{Var}[V(t_0)] \\approx \\mu_h^2 \\left[ \\left(\\frac{\\sigma_r}{\\mu_r}\\right)^2 + \\frac{1}{4}\\left(\\frac{\\sigma_D}{\\mu_D}\\right)^2 - \\rho\\left(\\frac{\\sigma_r}{\\mu_r}\\right)\\left(\\frac{\\sigma_D}{\\mu_D}\\right) \\right]\n$$\nNow, we substitute the given numerical values:\n$i_{n} = 1.0 \\times 10^{6}\\ \\mathrm{A}\\,\\mathrm{m}^{-3}$, $t_{0} = 1.0\\ \\mathrm{s}$, $F = 96485\\ \\mathrm{C}\\,\\mathrm{mol}^{-1}$, $\\epsilon_{s} = 0.50$, $U'(c_{0}) = 3.0 \\times 10^{-6}\\ \\mathrm{V}\\,(\\mathrm{mol}\\,\\mathrm{m}^{-3})^{-1}$, $\\mu_{r} = 5.0 \\times 10^{-6}\\ \\mathrm{m}$, $\\sigma_{r} = 2.0 \\times 10^{-7}\\ \\mathrm{m}$, $\\mu_{D} = 1.0 \\times 10^{-14}\\ \\mathrm{m}^{2}\\,\\mathrm{s}^{-1}$, $\\sigma_{D} = 2.0 \\times 10^{-15}\\ \\mathrm{m}^{2}\\,\\mathrm{s}^{-1}$, and $\\rho = 0.40$.\n\nFirst, calculate the mean voltage change, $\\mu_h$:\n$$\n\\mu_h = \\left( (3.0 \\times 10^{-6}) \\frac{2 (1.0 \\times 10^6) \\sqrt{1.0}}{3(96485)(0.50)\\sqrt{\\pi}} \\right) (5.0 \\times 10^{-6}) (1.0 \\times 10^{-14})^{-1/2}\n$$\n$$\n\\mu_h \\approx \\left( \\frac{6.0}{256561.4} \\right) (5.0 \\times 10^{-6})(1.0 \\times 10^7) \\approx (2.33865 \\times 10^{-5})(50) \\approx 1.16933 \\times 10^{-3}\\ \\mathrm{V}\n$$\nSo, $\\mu_h^2 \\approx (1.16933 \\times 10^{-3})^2 \\approx 1.36733 \\times 10^{-6}\\ \\mathrm{V}^2$.\n\nNext, calculate the coefficients of variation (CV):\n$$\nCV_r = \\frac{\\sigma_r}{\\mu_r} = \\frac{2.0 \\times 10^{-7}}{5.0 \\times 10^{-6}} = 0.04\n$$\n$$\nCV_D = \\frac{\\sigma_D}{\\mu_D} = \\frac{2.0 \\times 10^{-15}}{1.0 \\times 10^{-14}} = 0.20\n$$\nNow, calculate the term in the square brackets:\n$$\n\\text{Bracket Term} = (CV_r)^2 + \\frac{1}{4}(CV_D)^2 - \\rho(CV_r)(CV_D)\n$$\n$$\n\\text{Bracket Term} = (0.04)^2 + \\frac{1}{4}(0.20)^2 - (0.40)(0.04)(0.20)\n$$\n$$\n\\text{Bracket Term} = 0.0016 + 0.25(0.04) - 0.0032 = 0.0016 + 0.01 - 0.0032 = 0.0084\n$$\nFinally, compute the variance:\n$$\n\\mathrm{Var}[V(t_0)] \\approx \\mu_h^2 \\times (\\text{Bracket Term}) \\approx (1.36733 \\times 10^{-6}) \\times (0.0084) \\approx 1.14856 \\times 10^{-8}\\ \\mathrm{V}^2\n$$\nRounding the result to four significant figures gives $1.149 \\times 10^{-8}\\ \\mathrm{V}^2$.",
            "answer": "$$\\boxed{1.149 \\times 10^{-8}}$$"
        },
        {
            "introduction": "Physics-based models, while insightful, can be complex, and the true functional relationship between cell properties and performance may be unknown. This practice explores a powerful, data-driven alternative: building a probabilistic surrogate model using Gaussian Processes (GPs). A GP is a nonparametric Bayesian method that learns complex input-output mappings directly from data, providing not only predictions but also a principled measure of uncertainty. In this exercise, you will implement a GP to model a latent microstructural feature from Electrochemical Impedance Spectroscopy (EIS) data, deriving the posterior predictive equations and using them to assess performance against a specification threshold . This hands-on coding task demonstrates a modern machine learning approach to creating \"digital twins\" for characterizing and managing cell-to-cell heterogeneity.",
            "id": "3953320",
            "problem": "You are tasked with implementing a Bayesian nonparametric model that captures stochastic cell-to-cell variations in a latent microstructural feature inferred from Electrochemical Impedance Spectroscopy (EIS) parameters. The model is a Gaussian process on a regression function from EIS parameter vectors to a scalar microstructural feature. Your goal is to start from the definition of a Gaussian process and conditioning properties of multivariate normal distributions and derive the posterior predictive distribution for a new cell. Then, write a program to compute the posterior predictive mean, posterior predictive variance, and the posterior exceedance probability of meeting a specification threshold for a set of test cases.\n\nAssume the following fundamental base:\n- A Gaussian process is a collection of random variables where any finite collection has a joint multivariate normal distribution. Use a zero mean Gaussian process prior.\n- The observation model is additive independent Gaussian noise.\n- The squared exponential (also known as radial basis function) kernel is employed.\n\nFormulation:\n- Let the EIS-derived feature vector for the $i$-th cell be $\\mathbf{x}_{i} \\in \\mathbb{R}^{d}$ and the corresponding observed scalar microstructural feature be $y_{i} \\in \\mathbb{R}$.\n- Let $f(\\cdot)$ be the latent function such that $y_{i} = f(\\mathbf{x}_{i}) + \\varepsilon_{i}$ with $\\varepsilon_{i} \\sim \\mathcal{N}(0, \\sigma_{n}^{2})$ and independent across $i$.\n- Place a Gaussian process prior $f(\\cdot) \\sim \\mathcal{GP}(0, k(\\cdot, \\cdot))$ with the squared exponential kernel\n$$\nk(\\mathbf{x}, \\mathbf{x}') = \\sigma_{f}^{2} \\exp\\left(-\\frac{1}{2 \\ell^{2}} \\lVert \\mathbf{x} - \\mathbf{x}' \\rVert^{2}\\right),\n$$\nwhere $\\sigma_{f} > 0$ is the signal standard deviation and $\\ell > 0$ is the length scale.\n- For a test cell with EIS feature vector $\\mathbf{x}_{\\star}$ and a specification threshold $\\tau$, the posterior predictive distribution of $f(\\mathbf{x}_{\\star})$ is univariate normal with some mean and variance that must be derived from the fundamental base described above. The exceedance probability is the posterior probability that $f(\\mathbf{x}_{\\star}) \\ge \\tau$.\n\nYour program must:\n- Implement the above model and compute, for each test case:\n    - The posterior predictive mean $\\mu_{\\star}$.\n    - The posterior predictive variance $s_{\\star}^{2}$.\n    - The exceedance probability $p_{\\star} = \\mathbb{P}\\left(f(\\mathbf{x}_{\\star}) \\ge \\tau \\mid \\mathcal{D}\\right)$, expressed as a decimal.\n- Use numerically stable linear algebra with a small diagonal jitter for positive definiteness if needed.\n- Round each reported numeric result to exactly $6$ decimal places.\n\nTest suite:\n- Each test case specifies the dimension $d$, hyperparameters $(\\sigma_{f}, \\ell, \\sigma_{n})$, training inputs $\\mathbf{X}_{\\text{train}} \\in \\mathbb{R}^{n \\times d}$, training outputs $\\mathbf{y}_{\\text{train}} \\in \\mathbb{R}^{n}$, a query input $\\mathbf{x}_{\\star} \\in \\mathbb{R}^{d}$, and a threshold $\\tau \\in \\mathbb{R}$.\n\nCase A (general case near interpolation):\n- $d = 2$.\n- $(\\sigma_{f}, \\ell, \\sigma_{n}) = (0.2, 1.0, 0.01)$.\n- $\\mathbf{X}_{\\text{train}} = [ (0.0, 0.0), (1.0, 0.5), (0.5, 1.5), (1.5, 1.0) ]$.\n- $\\mathbf{y}_{\\text{train}} = [ 0.10, 0.15, 0.18, 0.20 ]$.\n- $\\mathbf{x}_{\\star} = (1.0, 1.0)$.\n- $\\tau = 0.17$.\n\nCase B (near-singular training design to test numerical stability):\n- $d = 2$.\n- $(\\sigma_{f}, \\ell, \\sigma_{n}) = (0.3, 10.0, 0.0001)$.\n- $\\mathbf{X}_{\\text{train}} = [ (0.0, 0.0), (0.000001, 0.0), (0.0, 0.000001) ]$.\n- $\\mathbf{y}_{\\text{train}} = [ 0.12, 0.121, 0.119 ]$.\n- $\\mathbf{x}_{\\star} = (0.0, 0.0)$.\n- $\\tau = 0.12$.\n\nCase C (far extrapolation to test prior reversion):\n- $d = 2$.\n- $(\\sigma_{f}, \\ell, \\sigma_{n}) = (0.25, 0.3, 0.02)$.\n- $\\mathbf{X}_{\\text{train}} = [ (0.0, 0.0), (0.5, 0.0), (0.0, 0.5) ]$.\n- $\\mathbf{y}_{\\text{train}} = [ 0.11, 0.115, 0.112 ]$.\n- $\\mathbf{x}_{\\star} = (3.0, 3.0)$.\n- $\\tau = 0.11$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets and ordered as\n$$\n[\\mu_{\\star}^{(A)}, s_{\\star}^{2(A)}, p_{\\star}^{(A)}, \\mu_{\\star}^{(B)}, s_{\\star}^{2(B)}, p_{\\star}^{(B)}, \\mu_{\\star}^{(C)}, s_{\\star}^{2(C)}, p_{\\star}^{(C)}],\n$$\nwhere each entry is rounded to exactly $6$ decimal places. For example, an acceptable format is $[0.123456,0.000789,0.654321, \\dots]$ with no additional text.\n- All quantities are dimensionless; no physical units are required.",
            "solution": "The problem requires the derivation and implementation of a Gaussian Process (GP) regression model to predict a latent microstructural feature from Electrochemical Impedance Spectroscopy (EIS) parameters. We must derive the posterior predictive distribution and then compute its mean, variance, and an exceedance probability for given test cases.\n\nLet the training data be denoted by $\\mathcal{D} = \\{\\mathbf{X}, \\mathbf{y}\\}$, where $\\mathbf{X} = \\{\\mathbf{x}_1, \\dots, \\mathbf{x}_n\\}$ is the set of $n$ training input vectors in $\\mathbb{R}^d$ and $\\mathbf{y} = [y_1, \\dots, y_n]^T$ is the vector of corresponding observed scalar outputs. We are given a new test input $\\mathbf{x}_{\\star}$ and wish to predict the corresponding latent function value $f_{\\star} = f(\\mathbf{x}_{\\star})$.\n\nThe model assumptions are:\n1. A zero-mean Gaussian Process prior is placed on the latent function $f(\\cdot)$:\n$$\nf(\\cdot) \\sim \\mathcal{GP}(0, k(\\mathbf{x}, \\mathbf{x}'))\n$$\nThe kernel function is the squared exponential (or Radial Basis Function, RBF) kernel:\n$$\nk(\\mathbf{x}, \\mathbf{x}') = \\sigma_{f}^{2} \\exp\\left(-\\frac{1}{2 \\ell^{2}} \\lVert \\mathbf{x} - \\mathbf{x}' \\rVert^{2}\\right)\n$$\nwhere $\\sigma_{f} > 0$ is the signal standard deviation and $\\ell > 0$ is the length scale.\n\n2. The observations $y_i$ are related to the latent function values $f_i = f(\\mathbf{x}_i)$ through an additive, independent, and identically distributed Gaussian noise model:\n$$\ny_i = f(\\mathbf{x}_i) + \\varepsilon_i, \\quad \\text{with } \\varepsilon_i \\sim \\mathcal{N}(0, \\sigma_{n}^{2})\n$$\n\nFrom the definition of a GP, any finite collection of function values has a joint multivariate normal distribution. Let $\\mathbf{f} = [f(\\mathbf{x}_1), \\dots, f(\\mathbf{x}_n)]^T$ be the vector of latent values at the training points and $f_{\\star} = f(\\mathbf{x}_{\\star})$ be the latent value at the test point. Their joint prior distribution is:\n$$\n\\begin{pmatrix} \\mathbf{f} \\\\ f_{\\star} \\end{pmatrix} \\sim \\mathcal{N} \\left( \\mathbf{0}, \\begin{pmatrix} K(\\mathbf{X}, \\mathbf{X}) & K(\\mathbf{X}, \\mathbf{x}_{\\star}) \\\\ K(\\mathbf{x}_{\\star}, \\mathbf{X}) & k(\\mathbf{x}_{\\star}, \\mathbf{x}_{\\star}) \\end{pmatrix} \\right)\n$$\nHere, $K(\\mathbf{X}, \\mathbf{X})$ is the $n \\times n$ covariance matrix where $[K(\\mathbf{X}, \\mathbf{X})]_{ij} = k(\\mathbf{x}_i, \\mathbf{x}_j)$, $K(\\mathbf{X}, \\mathbf{x}_{\\star})$ is the $n \\times 1$ vector of covariances where $[K(\\mathbf{X}, \\mathbf{x}_{\\star})]_i = k(\\mathbf{x}_i, \\mathbf{x}_{\\star})$, and $k(\\mathbf{x}_{\\star}, \\mathbf{x}_{\\star})$ is the prior variance at the test point. For brevity, we denote these as $K$, $\\mathbf{k}_{\\star}$, and $k_{\\star\\star}$, respectively.\n\nThe vector of observations $\\mathbf{y}$ is given by $\\mathbf{y} = \\mathbf{f} + \\boldsymbol{\\varepsilon}$, where $\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma_n^2 I_n)$. Since both $\\mathbf{f}$ and $\\boldsymbol{\\varepsilon}$ are Gaussian, their sum $\\mathbf{y}$ is also Gaussian. We consider the joint distribution of the observations $\\mathbf{y}$ and the latent test value $f_{\\star}$:\n$$\n\\begin{pmatrix} \\mathbf{y} \\\\ f_{\\star} \\end{pmatrix} \\sim \\mathcal{N} \\left( \\begin{pmatrix} \\mathbb{E}[\\mathbf{f} + \\boldsymbol{\\varepsilon}] \\\\ \\mathbb{E}[f_{\\star}] \\end{pmatrix}, \\begin{pmatrix} \\text{Cov}(\\mathbf{y}, \\mathbf{y}) & \\text{Cov}(\\mathbf{y}, f_{\\star}) \\\\ \\text{Cov}(f_{\\star}, \\mathbf{y}) & \\text{Cov}(f_{\\star}, f_{\\star}) \\end{pmatrix} \\right)\n$$\nThe mean vector is zero: $\\mathbb{E}[\\mathbf{f} + \\boldsymbol{\\varepsilon}] = \\mathbb{E}[\\mathbf{f}] + \\mathbb{E}[\\boldsymbol{\\varepsilon}] = \\mathbf{0} + \\mathbf{0} = \\mathbf{0}$, and $\\mathbb{E}[f_{\\star}] = 0$.\nThe covariance matrix blocks are:\n- $\\text{Cov}(\\mathbf{y}, \\mathbf{y}) = \\text{Cov}(\\mathbf{f} + \\boldsymbol{\\varepsilon}, \\mathbf{f} + \\boldsymbol{\\varepsilon}) = \\text{Cov}(\\mathbf{f}, \\mathbf{f}) + \\text{Cov}(\\boldsymbol{\\varepsilon}, \\boldsymbol{\\varepsilon}) = K + \\sigma_n^2 I_n$, since $\\mathbf{f}$ and $\\boldsymbol{\\varepsilon}$ are independent.\n- $\\text{Cov}(\\mathbf{y}, f_{\\star}) = \\text{Cov}(\\mathbf{f} + \\boldsymbol{\\varepsilon}, f_{\\star}) = \\text{Cov}(\\mathbf{f}, f_{\\star}) = \\mathbf{k}_{\\star}$.\n- $\\text{Cov}(f_{\\star}, f_{\\star}) = k_{\\star\\star}$.\n\nThus, the joint distribution is:\n$$\n\\begin{pmatrix} \\mathbf{y} \\\\ f_{\\star} \\end{pmatrix} \\sim \\mathcal{N} \\left( \\mathbf{0}, \\begin{pmatrix} K + \\sigma_n^2 I_n & \\mathbf{k}_{\\star} \\\\ \\mathbf{k}_{\\star}^T & k_{\\star\\star} \\end{pmatrix} \\right)\n$$\n\nWe are interested in the posterior predictive distribution $p(f_{\\star} | \\mathbf{y}, \\mathbf{X}, \\mathbf{x}_{\\star})$. This is obtained by conditioning the joint multivariate normal distribution. Using the standard formula for the conditional distribution of a partitioned Gaussian, if $\\begin{pmatrix} \\mathbf{z}_1 \\\\ \\mathbf{z}_2 \\end{pmatrix} \\sim \\mathcal{N} \\left( \\begin{pmatrix} \\boldsymbol{\\mu}_1 \\\\ \\boldsymbol{\\mu}_2 \\end{pmatrix}, \\begin{pmatrix} \\Sigma_{11} & \\Sigma_{12} \\\\ \\Sigma_{21} & \\Sigma_{22} \\end{pmatrix} \\right)$, then $\\mathbf{z}_2 | \\mathbf{z}_1 \\sim \\mathcal{N}(\\boldsymbol{\\mu}', \\Sigma')$, where:\n$$\n\\boldsymbol{\\mu}' = \\boldsymbol{\\mu}_2 + \\Sigma_{21} \\Sigma_{11}^{-1} (\\mathbf{z}_1 - \\boldsymbol{\\mu}_1)\n$$\n$$\n\\Sigma' = \\Sigma_{22} - \\Sigma_{21} \\Sigma_{11}^{-1} \\Sigma_{12}\n$$\nBy identifying $\\mathbf{z}_1 \\leftrightarrow \\mathbf{y}$ and $\\mathbf{z}_2 \\leftrightarrow f_{\\star}$, we find the posterior distribution of $f_{\\star}$ is a univariate normal $f_{\\star} | \\mathcal{D} \\sim \\mathcal{N}(\\mu_{\\star}, s_{\\star}^2)$ with mean $\\mu_{\\star}$ and variance $s_{\\star}^2$ given by:\n$$\n\\mu_{\\star} = \\mathbf{k}_{\\star}^T (K + \\sigma_n^2 I_n)^{-1} \\mathbf{y}\n$$\n$$\ns_{\\star}^2 = k_{\\star\\star} - \\mathbf{k}_{\\star}^T (K + \\sigma_n^2 I_n)^{-1} \\mathbf{k}_{\\star}\n$$\nFor numerical stability, the inverse $(K + \\sigma_n^2 I_n)^{-1}$ is not computed directly. Instead, we solve the linear system $(K + \\sigma_n^2 I_n) \\boldsymbol{\\alpha} = \\mathbf{y}$ for $\\boldsymbol{\\alpha}$, from which the mean is $\\mu_{\\star} = \\mathbf{k}_{\\star}^T \\boldsymbol{\\alpha}$. Similarly, we solve $(K + \\sigma_n^2 I_n) \\mathbf{v} = \\mathbf{k}_{\\star}$ for $\\mathbf{v}$, yielding the variance as $s_{\\star}^2 = k_{\\star\\star} - \\mathbf{k}_{\\star}^T \\mathbf{v}$. A Cholesky decomposition of the matrix $K_y = K + \\sigma_n^2 I_n$ provides a robust method for solving these systems.\n\nFinally, the exceedance probability $p_{\\star}$ is the posterior probability that $f_{\\star}$ is greater than or equal to a threshold $\\tau$:\n$$\np_{\\star} = \\mathbb{P}(f_{\\star} \\ge \\tau \\mid \\mathcal{D}) = \\int_{\\tau}^{\\infty} \\mathcal{N}(f | \\mu_{\\star}, s_{\\star}^2) df\n$$\nBy standardizing the variable, let $Z = (f_{\\star} - \\mu_{\\star}) / s_{\\star}$, where $s_\\star = \\sqrt{s_\\star^2}$. Then $Z \\sim \\mathcal{N}(0, 1)$. The inequality becomes $Z \\ge (\\tau - \\mu_{\\star}) / s_{\\star}$. The probability is given by the survival function (complementary CDF) of the standard normal distribution:\n$$\np_{\\star} = 1 - \\Phi\\left(\\frac{\\tau - \\mu_{\\star}}{s_{\\star}}\\right)\n$$\nwhere $\\Phi(\\cdot)$ is the CDF of the standard normal distribution. This is computed using standard scientific libraries. The implementation will follow this derived mathematical framework.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import cholesky, cho_solve\nfrom scipy.stats import norm\nfrom scipy.spatial.distance import cdist\n\ndef compute_gp_posterior(params, X_train, y_train, x_star, tau):\n    \"\"\"\n    Computes the posterior predictive distribution for a Gaussian Process model.\n\n    Args:\n        params (tuple): A tuple of hyperparameters (sigma_f, ell, sigma_n).\n        X_train (np.ndarray): Training input data, shape (n, d).\n        y_train (np.ndarray): Training output data, shape (n,).\n        x_star (np.ndarray): Test input data, shape (d,).\n        tau (float): The specification threshold.\n\n    Returns:\n        tuple: A tuple containing the posterior mean, posterior variance,\n               and exceedance probability, each rounded to 6 decimal places.\n    \"\"\"\n    sigma_f, ell, sigma_n = params\n    n, d = X_train.shape\n    \n    # Reshape x_star to be a 2D array for cdist\n    x_star_2d = x_star.reshape(1, d)\n\n    # Squared exponential kernel function\n    def rbf_kernel(X1, X2, sf, l):\n        sqdist = cdist(X1, X2, 'sqeuclidean')\n        return sf**2 * np.exp(-0.5 / l**2 * sqdist)\n\n    # Compute kernel matrices\n    K = rbf_kernel(X_train, X_train, sigma_f, ell)\n    k_star_vec = rbf_kernel(X_train, x_star_2d, sigma_f, ell).flatten()\n    k_star_star = rbf_kernel(x_star_2d, x_star_2d, sigma_f, ell)[0, 0]\n\n    # Form the covariance matrix of the noisy observations\n    K_y = K + (sigma_n**2) * np.eye(n)\n\n    try:\n        # Use Cholesky decomposition for stable and efficient linear system solving\n        L = cholesky(K_y, lower=True)\n        \n        # Solve for alpha = (K_y)^-1 * y_train\n        alpha = cho_solve((L, True), y_train)\n        \n        # Compute posterior mean: mu_star = k_star^T * alpha\n        mu_star = k_star_vec.T @ alpha\n        \n        # Solve for v = (K_y)^-1 * k_star\n        v = cho_solve((L, True), k_star_vec)\n        \n        # Compute posterior variance: s2_star = k_star_star - k_star^T * v\n        s2_star = k_star_star - k_star_vec.T @ v\n    except np.linalg.LinAlgError:\n        # Fallback for singular matrix, although the jitter should prevent this.\n        # This case is not expected with the given problem setup.\n        Ky_inv = np.linalg.pinv(K_y)\n        mu_star = k_star_vec.T @ Ky_inv @ y_train\n        s2_star = k_star_star - k_star_vec.T @ Ky_inv @ k_star_vec\n\n    # Ensure variance is non-negative due to potential numerical precision errors\n    s2_star = max(0, s2_star)\n    s_star = np.sqrt(s2_star)\n\n    # Compute exceedance probability P(f_star >= tau)\n    # This is calculated using the survival function (1 - CDF) of the normal distribution.\n    if s_star  1e-12:  # If variance is effectively zero, the distribution is a Dirac delta\n        p_star = 1.0 if mu_star >= tau else 0.0\n    else:\n        p_star = norm.sf((tau - mu_star) / s_star)\n\n    # Round results to 6 decimal places as required\n    return round(mu_star, 6), round(s2_star, 6), round(p_star, 6)\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"d\": 2,\n            \"params\": (0.2, 1.0, 0.01),\n            \"X_train\": np.array([[0.0, 0.0], [1.0, 0.5], [0.5, 1.5], [1.5, 1.0]]),\n            \"y_train\": np.array([0.10, 0.15, 0.18, 0.20]),\n            \"x_star\": np.array([1.0, 1.0]),\n            \"tau\": 0.17\n        },\n        {\n            \"d\": 2,\n            \"params\": (0.3, 10.0, 0.0001),\n            \"X_train\": np.array([[0.0, 0.0], [0.000001, 0.0], [0.0, 0.000001]]),\n            \"y_train\": np.array([0.12, 0.121, 0.119]),\n            \"x_star\": np.array([0.0, 0.0]),\n            \"tau\": 0.12\n        },\n        {\n            \"d\": 2,\n            \"params\": (0.25, 0.3, 0.02),\n            \"X_train\": np.array([[0.0, 0.0], [0.5, 0.0], [0.0, 0.5]]),\n            \"y_train\": np.array([0.11, 0.115, 0.112]),\n            \"x_star\": np.array([3.0, 3.0]),\n            \"tau\": 0.11\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        mu_star, s2_star, p_star = compute_gp_posterior(\n            case[\"params\"],\n            case[\"X_train\"],\n            case[\"y_train\"],\n            case[\"x_star\"],\n            case[\"tau\"]\n        )\n        results.extend([f\"{mu_star:.6f}\", f\"{s2_star:.6f}\", f\"{p_star:.6f}\"])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}