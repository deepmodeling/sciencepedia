## Applications and Interdisciplinary Connections

The principles of [space-filling sampling](@entry_id:1132002), including methods such as Latin Hypercube Sampling (LHS), quasi-Monte Carlo (QMC) sequences, and maximin designs, form a cornerstone of modern computational science and engineering. While the preceding chapters have detailed the theoretical foundations and construction of these strategies, their true power is revealed through their application to complex, real-world problems. This chapter demonstrates the utility, extension, and integration of space-filling principles across a diverse range of disciplines. We will explore how these methods are adapted and applied to build predictive [surrogate models](@entry_id:145436), optimize systems under constraints, perform sensitivity analysis, and even probe the structure of neural representations in the brain. The objective is not to re-teach the core concepts, but to illustrate their indispensable role in the efficient exploration and understanding of high-dimensional parameter spaces.

### Surrogate Modeling and Design of Computer Experiments

Perhaps the most common application of [space-filling sampling](@entry_id:1132002) lies in the field of **Design and Analysis of Computer Experiments (DACE)**. Many phenomena in science and engineering are modeled by complex systems of equations (e.g., partial differential equations) that are computationally expensive to solve. A single high-fidelity simulation, such as a detailed electrochemical model of a battery or a computational fluid dynamics model of a thermal system, can take hours or days to complete. This cost prohibits direct use of the model for tasks requiring many evaluations, such as optimization, [uncertainty quantification](@entry_id:138597), or [design space exploration](@entry_id:1123590).

The solution is to construct a cheap-to-evaluate mathematical approximation of the expensive simulation, known as a **surrogate model** or **emulator**. The accuracy of this surrogate depends critically on the set of initial high-fidelity simulations used to train it. A well-chosen set of training points, known as a **[design of experiments](@entry_id:1123585)**, should provide maximal information about the global behavior of the simulation output. This is precisely the goal of [space-filling sampling](@entry_id:1132002). By distributing points evenly throughout the parameter domain, space-filling designs prevent large, unsampled gaps where the surrogate's error could be large, and they avoid wasteful clustering of points in one region.

Latin Hypercube Sampling (LHS) is a foundational technique for this purpose. An $N$-point LHS design in a $d$-dimensional [hypercube](@entry_id:273913) ensures that for each of the $d$ parameter dimensions, every one of $N$ equally probable strata is sampled exactly once. This guarantees excellent coverage in one-dimensional projections, a property that makes LHS far superior to [simple random sampling](@entry_id:754862) for building global surrogates . The **maximin distance criterion**, which seeks to select a set of points that maximizes the minimum distance between any pair, is another key principle. Often, these two ideas are combined: one generates many random LHS designs and selects the one with the best maximin properties, yielding a maximin-LHS design that possesses both good projection and space-filling characteristics .

In high-dimensional parameter spaces, the "curse of dimensionality" renders traditional grid-based [sampling strategies](@entry_id:188482) infeasible, as the number of points grows exponentially with dimension. Space-filling methods, where the number of samples $N$ is independent of the dimension $d$, are essential. In addition to LHS, **[low-discrepancy sequences](@entry_id:139452)** (a type of quasi-Monte Carlo method) such as Sobol sequences are widely used. These are deterministic sequences constructed to fill the space with even greater uniformity than random methods. This uniformity is formally measured by **discrepancy**, and the superior scaling of discrepancy for QMC sequences often leads to more accurate [surrogate models](@entry_id:145436) for a given number of samples . The effectiveness of a design can be linked to theoretical [error bounds](@entry_id:139888); for instance, for certain classes of functions, the maximum [interpolation error](@entry_id:139425) of a surrogate is related to the **fill distance** of the design—the radius of the largest empty sphere in the domain. Space-filling designs are explicitly constructed to minimize such metrics, thereby providing a pathway to more accurate and reliable surrogates .

### Incorporating Domain-Specific Knowledge: Anisotropic and Constrained Sampling

Standard space-filling methods operate on a normalized unit [hypercube](@entry_id:273913), assuming all directions are equally important and the domain is unconstrained. Real-world problems are rarely so simple. Effective application of these principles requires adapting them to the specific [geometry and physics](@entry_id:265497) of the problem at hand.

#### Anisotropic Sampling Guided by Sensitivity

In many systems, the output of interest is far more sensitive to changes in some parameters than in others. For example, in a battery model, discharge capacity might be highly sensitive to electrode porosity but relatively insensitive to the separator thickness. In such cases, it is inefficient to sample the space with uniform density in all directions. Instead, we should sample more densely in the directions of high sensitivity. This can be formalized by replacing the standard Euclidean distance with an **anisotropic metric**. A common choice is the Mahalanobis distance, $d_W(\mathbf{x},\mathbf{y}) = \sqrt{(\mathbf{x}-\mathbf{y})^{\top} W (\mathbf{x}-\mathbf{y})}$, where $W$ is a positive-definite weight matrix . The diagonal entries of $W$ can be set to reflect the relative sensitivity of each parameter. A large weight $w_i$ inflates distances along the $i$-th coordinate, meaning that to achieve the same separation in the weighted metric, points must be placed closer together in the original coordinate system. Consequently, applying a maximin criterion with this metric encourages denser sampling along more sensitive directions, efficiently focusing computational effort where it is most needed to control surrogate error  .

#### Sampling on Constrained Domains

Engineering and scientific design spaces are frequently defined by complex physical, chemical, or safety constraints, resulting in non-rectangular feasible domains. For instance, in biomedical modeling, physiological parameters must satisfy certain relationships to be viable; in battery design, material compositions must add to unity and electrode structures must be physically realizable  . A naive approach of generating a [space-filling design](@entry_id:755078) in a [bounding box](@entry_id:635282) and then rejecting infeasible points is highly inefficient and destroys the carefully constructed space-filling properties of the original design.

More advanced strategies are required. One class of methods involves generating candidate points directly within the feasible set using specialized MCMC techniques like hit-and-run sampling, and then selecting a space-filling subset using a [greedy algorithm](@entry_id:263215) . Another powerful approach is to formulate the search for a constrained [space-filling design](@entry_id:755078) as an optimization problem. Here, an objective function that encourages point separation (e.g., a pairwise repulsive energy) is minimized, while constraints are enforced using techniques like barrier or penalty functions. This allows for the direct generation of designs that are both space-filling and feasible . These methods are essential for applying experimental design principles to realistic, complex systems.

### Advanced Applications in Sequential and Adaptive Design

While space-filling designs are often used to generate a single, static set of training points (a "one-shot" design), their principles are also deeply integrated into modern sequential and adaptive sampling frameworks, where data points are collected iteratively.

#### Bayesian Optimization and Hybrid Acquisition Functions

**Bayesian optimization** is a powerful sequential design strategy for finding the global optimum of an expensive function. It uses a surrogate model, typically a Gaussian Process (GP), to guide the search. At each step, an **[acquisition function](@entry_id:168889)** is used to decide where to sample next. This function balances **exploitation** (sampling where the model predicts a good outcome) and **exploration** (sampling where the model is uncertain). A pure exploitation strategy can get stuck in local optima. To ensure robust global exploration, space-filling principles are often explicitly incorporated. One way is through a **hybrid [acquisition function](@entry_id:168889)** that is a convex combination of an exploitation-focused term, such as Expected Improvement (EI), and a purely exploratory, space-filling term, such as the distance to the nearest previously sampled point. By tuning the mixing weight, a practitioner can control the balance, ensuring the search continues to explore empty regions of the space while simultaneously refining knowledge around promising candidates .

#### Adaptive Construction of Reduced-Order Models

In the field of **Model Order Reduction (MOR)**, the goal is to create extremely fast, physics-based approximations of [large-scale systems](@entry_id:166848). Methods like the Reduced Basis (RB) method build a low-dimensional solution subspace from a set of high-fidelity "snapshot" solutions. The choice of snapshots is critical. A powerful approach is a **[greedy algorithm](@entry_id:263215)** where snapshots are chosen adaptively. This process often starts with an initial basis built from a small, [space-filling design](@entry_id:755078) (e.g., LHS or a sparse grid). Then, a cheap [error indicator](@entry_id:164891) is evaluated over a very large *training set* of candidate parameters—itself a [space-filling design](@entry_id:755078). The parameter where the estimated error is largest is chosen for the next high-fidelity simulation, and the resulting solution snapshot is used to enrich the basis. This cycle repeats until a desired accuracy is reached. This methodology, which combines space-filling designs for both initialization and adaptive refinement, is a state-of-the-art technique for efficiently building accurate [reduced-order models](@entry_id:754172) for complex parametric systems like lithium-ion [battery models](@entry_id:1121428)  .

#### Dimension Reduction with Active Subspaces

For problems with very high-dimensional inputs ($d \gg 10$), even standard space-filling methods can struggle to provide adequate coverage. The **[active subspace method](@entry_id:746243)** is a dimension-reduction technique that seeks to identify a low-dimensional subspace of the input parameters that governs most of the variation in the output. This is accomplished by analyzing the eigenvalues and eigenvectors of the matrix $C = \mathbb{E}[\nabla f(x) \nabla f(x)^\top]$, which is the average [outer product](@entry_id:201262) of the function's gradients. The eigenvectors corresponding to the largest eigenvalues span the "[active subspace](@entry_id:1120749)." If a low-dimensional [active subspace](@entry_id:1120749) exists, the function can be well-approximated by a function of just these few active variables. The sampling strategy then becomes dramatically more efficient: one generates a [space-filling design](@entry_id:755078) in the low-dimensional [active subspace](@entry_id:1120749) and maps these points back to the original high-dimensional space for simulation. This focuses the computational budget on the directions that truly matter, enabling the exploration of problems that would otherwise be intractable .

### Interdisciplinary Connections

The utility of [space-filling sampling](@entry_id:1132002) extends far beyond its origins in [computational engineering](@entry_id:178146) and statistics. Its principles are now vital in a wide array of scientific domains.

#### Uncertainty Quantification and Global Sensitivity Analysis

**Global Sensitivity Analysis (GSA)** aims to apportion the uncertainty in a model's output to the uncertainty in its various inputs. **Sobol' indices**, which are based on a functional [variance decomposition](@entry_id:272134), are the gold standard for this task. Calculating these indices requires computing [high-dimensional integrals](@entry_id:137552) over the input parameter space. For expensive models, this is infeasible. The standard workflow is to first train a GP emulator on a limited set of model runs. The accuracy of the GSA is then entirely dependent on the fidelity of this emulator. To build an emulator that is accurate across the entire input distribution—a prerequisite for computing global sensitivity indices—one must use a high-quality [space-filling design](@entry_id:755078) for the initial training runs. This synergy between [space-filling design](@entry_id:755078), surrogate modeling, and GSA is fundamental to modern [uncertainty quantification](@entry_id:138597) .

#### Experimental Design for Validation and Calibration

The principles of [space-filling design](@entry_id:755078) are not limited to *computer* experiments. They are equally valuable for designing physical experiments, especially when tests are time-consuming or expensive. For example, in validating a [battery management system](@entry_id:1121417) using **Hardware-in-the-Loop (HIL)** simulation, one must create a test matrix that comprehensively covers the operational envelope of the battery (e.g., temperature, state-of-charge, C-rate). A [space-filling design](@entry_id:755078) provides a systematic way to achieve this coverage efficiently. The design must often accommodate practical realities, such as non-linear parameter scalings (e.g., using a logarithmic scale for time to capture both [fast and slow dynamics](@entry_id:265915)) and strict time budgets, making the intelligent selection of test points crucial . Similarly, when calibrating a physical sub-model, such as the thermal contact resistance of a joint, a [space-filling design](@entry_id:755078) ensures that the limited experimental data provides information across the entire operational range of pressure and temperature, leading to a more robustly calibrated model .

#### Probing Geometric Structure in Neuroscience

A striking application of these concepts comes from [cognitive neuroscience](@entry_id:914308). **Representational Similarity Analysis (RSA)** is a popular method for characterizing how information is encoded in patterns of brain activity. It works by computing a **Representational Dissimilarity Matrix (RDM)**, which contains the pairwise dissimilarities between neural responses to a set of experimental stimuli. The RDM provides a snapshot of the "[representational geometry](@entry_id:1130876)" for the tested stimuli. A key scientific goal is to make generalizable inferences about this geometry. This goal transforms the selection of stimuli into an experimental design problem. If stimuli are chosen naively (e.g., by varying only one feature while holding others constant), any conclusions are limited to that narrow slice of the stimulus space. To make claims that generalize, neuroscientists must sample the stimulus feature space effectively. Employing space-filling designs to select stimuli with diverse, well-distributed features allows for the construction of an RDM that provides a much richer, more holistic view of the representational geometry. This enables the fitting and validation of models that can generalize to predict neural responses to novel stimuli, providing far more powerful insights into brain function .

In conclusion, [space-filling sampling](@entry_id:1132002) is a powerful and versatile methodology. It provides a rigorous foundation for the [design of experiments](@entry_id:1123585), enabling efficient learning in the face of computational expense, high dimensionality, and complex constraints. Its applications are broad and deep, underpinning cutting-edge work in engineering design, sequential optimization, uncertainty quantification, and fundamental scientific discovery.