{
    "hands_on_practices": [
        {
            "introduction": "In surrogate modeling for complex systems like batteries, the quality of the training data is paramount. A \"good\" set of simulation points should cover the parameter space evenly to capture the system's behavior accurately. We can quantify this \"evenness\" using intuitive geometric metrics. The fill distance, $h_X$, measures the largest gap in the domain, while the separation distance, $q_X$, ensures points are not too close to one another.\n\nThis foundational practice introduces a quantitative toolkit for evaluating any given experimental design . By implementing and calculating these metrics for various sample distributions—from well-behaved to clustered—you will build a strong intuition for what constitutes a high-quality, space-filling design and how these metrics reflect that quality.",
            "id": "3951685",
            "problem": "Consider a three-parameter, normalized design space for automated battery design and simulation, defined as the unit cube $[0,1]^3$, where the parameters are electrode porosity $\\phi$, electrolyte salt concentration $c$, and separator thickness $\\delta$, each normalized to lie in $[0,1]$. Let $X = \\{x_i\\}_{i=1}^N \\subset [0,1]^3$ be a finite sample set used to train a surrogate model for a high-fidelity electrochemical simulator. The space-filling quality of $X$ can be characterized by the fill distance $h_X$, the separation distance $q_X$, and the mesh ratio $\\rho_X = h_X/q_X$, under the standard Euclidean metric in $\\mathbb{R}^3$. The fill distance $h_X$ is the worst-case distance from any point in the domain to its nearest sample in $X$, the separation distance $q_X$ is half the minimum Euclidean distance between distinct samples in $X$, and the mesh ratio $\\rho_X$ is the ratio of these two quantities.\n\nYou must compute $h_X$, $q_X$, and $\\rho_X$ for several specified sample sets by approximating $h_X$ using a uniform Cartesian grid search over $[0,1]^3$ with a prescribed resolution $n_g$ points per axis (including the endpoints) to evaluate the nearest-sample distances at grid points. Use the Euclidean norm to measure distances. Distances are to be expressed in units of the normalized cube edge length (dimensionless). The angle unit is not applicable. The outputs must be rounded to $6$ decimal places.\n\nFor each test case, the input to your program is fixed and embedded in the program: the sample set $X$ and the grid resolution $n_g$. Your program should calculate a triple $[h_X,q_X,\\rho_X]$ for each test case and aggregate the results across all test cases into a single line of output in the exact format described below.\n\nUse the following test suite (each case provides a specific $X$ and $n_g$):\n\n- Case A (diverse design, akin to Latin Hypercube Sampling (LHS)): $N = 10$, $n_g = 25$, with\n  $$\n  X_A = \\Big\\{\n  (0.05,0.62,0.11),\\,\n  (0.18,0.86,0.73),\\,\n  (0.29,0.19,0.44),\\,\n  (0.41,0.48,0.95),\\,\n  (0.52,0.03,0.29),\\,\n  (0.63,0.77,0.05),\\,\n  (0.74,0.34,0.58),\\,\n  (0.80,0.10,0.88),\\,\n  (0.92,0.56,0.21),\\,\n  (0.15,0.72,0.37)\n  \\Big\\}.\n  $$\n\n- Case B (clustered near one corner, illustrating poor coverage): $N = 10$, $n_g = 25$, with\n  $$\n  X_B = \\Big\\{\n  (0.02,0.01,0.03),\\,\n  (0.05,0.04,0.02),\\,\n  (0.08,0.06,0.05),\\,\n  (0.10,0.02,0.08),\\,\n  (0.07,0.09,0.04),\\,\n  (0.12,0.11,0.07),\\,\n  (0.15,0.14,0.06),\\,\n  (0.18,0.16,0.10),\\,\n  (0.20,0.19,0.12),\\,\n  (0.22,0.18,0.15)\n  \\Big\\}.\n  $$\n\n- Case C (structured interior grid, $3\\times 3\\times 3$): $N = 27$, $n_g = 30$, with\n  $$\n  X_C = \\{(x,y,z) \\mid x \\in \\{1/6,\\, 1/2,\\, 5/6\\},\\; y \\in \\{1/6,\\, 1/2,\\, 5/6\\},\\; z \\in \\{1/6,\\, 1/2,\\, 5/6\\}\\}.\n  $$\n\n- Case D (only cube corners): $N = 8$, $n_g = 30$, with\n  $$\n  X_D = \\{(x,y,z) \\mid x \\in \\{0,\\,1\\},\\; y \\in \\{0,\\,1\\},\\; z \\in \\{0,\\,1\\}\\}.\n  $$\n\nScientific realism requirement: You must use the Euclidean distance in $\\mathbb{R}^3$; approximate $h_X$ by evaluating the nearest-sample distance at each point of a uniform Cartesian grid of resolution $n_g$ per axis over $[0,1]^3$, including endpoints; compute $q_X$ exactly from pairwise sample distances as half the minimum distance between distinct samples; then compute $\\rho_X = h_X/q_X$. No other simplifications are allowed.\n\nDesign for coverage: The test suite explores a general \"happy path\" case (Case A), a severe clustering edge case (Case B), a structured quasi-uniform interior grid (Case C), and a boundary-only case (Case D).\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list of the four per-case triples, each triple itself being a comma-separated list of three floats rounded to $6$ decimal places, with no spaces, enclosed in square brackets. For example, your output must look like\n$$\n[\\,[h_A,q_A,\\rho_A],\\,[h_B,q_B,\\rho_B],\\,[h_C,q_C,\\rho_C],\\,[h_D,q_D,\\rho_D]\\,]\n$$\nbut printed as plain text without extra spaces, using decimal floats. The values for $h_X$, $q_X$, and $\\rho_X$ are dimensionless.",
            "solution": "The problem requires the computation of three standard metrics for characterizing the space-filling properties of a sample set $X = \\{x_i\\}_{i=1}^N$ within the three-dimensional unit cube $[0,1]^3$. These metrics are the fill distance $h_X$, the separation distance $q_X$, and the mesh ratio $\\rho_X$. The computations are to be performed for four distinct sample sets, $X_A, X_B, X_C,$ and $X_D$. The underlying space is $\\mathbb{R}^3$ equipped with the standard Euclidean norm, $\\|v\\|_2 = \\sqrt{v_1^2 + v_2^2 + v_3^2}$.\n\nThe solution methodology is implemented in three sequential steps for each test case: calculation of the separation distance $q_X$, approximation of the fill distance $h_X$, and finally, computation of the mesh ratio $\\rho_X$.\n\n### 1. Calculation of the Separation Distance, $q_X$\n\nThe separation distance $q_X$ quantifies the minimum spacing between points in the sample set $X$. It is formally defined as half the minimum Euclidean distance between any two distinct points in the set:\n$$\nq_X = \\frac{1}{2} \\min_{i \\neq j} \\|x_i - x_j\\|_2\n$$\nA larger $q_X$ implies that the sample points are well-separated, avoiding clustering.\n\n**Algorithmic Approach**:\nTo compute $q_X$, we must calculate the Euclidean distance between all unique pairs of points $(x_i, x_j)$ in the set $X$ where $i \\neq j$. The total number of such pairs for a set of $N$ points is $\\binom{N}{2} = N(N-1)/2$. After computing all these pairwise distances, the minimum among them, $d_{min} = \\min_{i \\neq j} \\|x_i - x_j\\|_2$, is found. The separation distance is then $q_X = d_{min} / 2$. This computation can be efficiently performed using vectorized library functions, such as `scipy.spatial.distance.pdist`, which computes the pairwise distances for all points in a given collection.\n\n### 2. Approximation of the Fill Distance, $h_X$\n\nThe fill distance $h_X$ (also known as dispersion) measures the largest \"gap\" in the sampling. It is defined as the maximum possible distance from any point in the domain $[0,1]^3$ to its nearest sample point in $X$:\n$$\nh_X = \\sup_{p \\in [0,1]^3} \\left( \\min_{x_i \\in X} \\|p - x_i\\|_2 \\right)\n$$\nA smaller $h_X$ indicates better coverage of the design space, as no point in the domain is too far from a sample.\n\n**Algorithmic Approach**:\nCalculating the exact value of $h_X$ is a computationally difficult global optimization problem. The problem specifies an approximation method based on a discrete search over a uniform Cartesian grid.\nA grid $G$ is constructed within the domain $[0,1]^3$ with a resolution of $n_g$ points along each axis, including the endpoints. The coordinates for each axis are given by the set $\\{0, 1/(n_g-1), 2/(n_g-1), \\ldots, 1\\}$. The full grid $G$ is the Cartesian product of these coordinate sets, resulting in $n_g^3$ grid points in total.\n\nThe approximated fill distance, which we will also denote by $h_X$, is then the maximum of the nearest-sample distances evaluated at each grid point $g_k \\in G$:\n$$\nh_X \\approx \\max_{g_k \\in G} \\left( \\min_{x_i \\in X} \\|g_k - x_i\\|_2 \\right)\n$$\nThis procedure involves the following steps:\n1.  Generate the set of all $n_g^3$ grid points $G$.\n2.  For each grid point $g_k \\in G$, calculate its distance to every sample point $x_i \\in X$.\n3.  For each $g_k$, find the minimum of these distances, which is the distance to its nearest neighbor in $X$.\n4.  The fill distance $h_X$ is the maximum value among all these minimum distances.\n\nThis computation can be effectively vectorized. For example, `scipy.spatial.distance.cdist` can compute a matrix of distances between all grid points and all sample points. Subsequent `min` and `max` operations along the appropriate axes of this matrix yield the final value for $h_X$.\n\n### 3. Calculation of the Mesh Ratio, $\\rho_X$\n\nThe mesh ratio $\\rho_X$ is a dimensionless quantity that combines the fill and separation distances to provide a single measure of sample quality. It is defined as:\n$$\n\\rho_X = \\frac{h_X}{q_X}\n$$\nAn ideal set of points would have a mesh ratio close to $1$, which would correspond to the case where the radius of the largest empty sphere ($h_X$) is equal to the radius of the smallest sphere of influence around a sample point ($q_X$). Large values of $\\rho_X$ can indicate either poor coverage (large $h_X$) or significant clustering (small $q_X$).\n\n**Algorithmic Approach**:\nOnce the values for $h_X$ and $q_X$ are computed as described above, $\\rho_X$ is obtained by their direct division. If $q_X$ is zero (which happens if there are duplicate points in the sample set), the mesh ratio is undefined. However, the provided test cases consist of distinct points, so $q_X > 0$.\n\nBy applying this three-step procedure to each of the four specified test cases, we systematically derive the required metrics. The results are then rounded to $6$ decimal places and formatted as specified.",
            "answer": "```python\nimport numpy as np\nfrom scipy.spatial.distance import pdist, cdist\n\ndef solve():\n    \"\"\"\n    Computes space-filling metrics (h_X, q_X, rho_X) for four test cases\n    of sample sets in a 3D unit cube.\n    \"\"\"\n\n    def compute_metrics(X, n_g):\n        \"\"\"\n        Calculates separation distance, fill distance, and mesh ratio.\n\n        Args:\n            X (np.ndarray): A set of N sample points, shape (N, 3).\n            n_g (int): The number of grid points per axis for h_X approximation.\n\n        Returns:\n            tuple: A tuple containing (h_X, q_X, rho_X).\n        \"\"\"\n        # Ensure X is a NumPy array\n        X = np.array(X)\n        N = X.shape[0]\n\n        # 1. Compute Separation Distance (q_X)\n        # q_X is half the minimum Euclidean distance between distinct samples.\n        if N > 1:\n            # pdist computes pairwise distances between all points in X.\n            pairwise_distances = pdist(X, 'euclidean')\n            min_dist = np.min(pairwise_distances)\n            q_X = 0.5 * min_dist\n        else:\n            # Undefined for a single point, but we can set it to infinity\n            # to indicate no \"separation\" constraint.\n            # Handle as per problem context if it arises. For these tests N > 1.\n            q_X = np.inf\n\n        # 2. Approximate Fill Distance (h_X)\n        # h_X is the max distance from any point in the domain to its nearest sample.\n        # We approximate this by searching over a uniform grid.\n        \n        # Create grid points\n        axis_coords = np.linspace(0, 1, n_g)\n        grid_points = np.stack(np.meshgrid(axis_coords, axis_coords, axis_coords), axis=-1).reshape(-1, 3)\n\n        # Compute distances from each grid point to each sample point.\n        # cdist(A, B) creates a matrix where entry (i,j) is the distance from A[i] to B[j].\n        # Shape: (n_g**3, N)\n        dist_matrix = cdist(grid_points, X, 'euclidean')\n\n        # For each grid point, find the minimum distance to any sample point.\n        # Shape: (n_g**3,)\n        min_dists_from_grid = np.min(dist_matrix, axis=1)\n\n        # The fill distance h_X is the maximum of these minimum distances.\n        h_X = np.max(min_dists_from_grid)\n\n        # 3. Compute Mesh Ratio (rho_X)\n        if q_X > 0:\n            rho_X = h_X / q_X\n        else:\n            # This case (duplicate points) does not occur in the test suite.\n            rho_X = np.inf\n            \n        return h_X, q_X, rho_X\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A: Diverse design\n        {\n            \"name\": \"A\",\n            \"n_g\": 25,\n            \"X\": [\n                (0.05, 0.62, 0.11), (0.18, 0.86, 0.73), (0.29, 0.19, 0.44),\n                (0.41, 0.48, 0.95), (0.52, 0.03, 0.29), (0.63, 0.77, 0.05),\n                (0.74, 0.34, 0.58), (0.80, 0.10, 0.88), (0.92, 0.56, 0.21),\n                (0.15, 0.72, 0.37)\n            ]\n        },\n        # Case B: Clustered design\n        {\n            \"name\": \"B\",\n            \"n_g\": 25,\n            \"X\": [\n                (0.02, 0.01, 0.03), (0.05, 0.04, 0.02), (0.08, 0.06, 0.05),\n                (0.10, 0.02, 0.08), (0.07, 0.09, 0.04), (0.12, 0.11, 0.07),\n                (0.15, 0.14, 0.06), (0.18, 0.16, 0.10), (0.20, 0.19, 0.12),\n                (0.22, 0.18, 0.15)\n            ]\n        },\n        # Case C: Structured interior grid\n        {\n            \"name\": \"C\",\n            \"n_g\": 30,\n            \"X\": [(x, y, z) for x in [1/6, 1/2, 5/6] \n                              for y in [1/6, 1/2, 5/6]\n                              for z in [1/6, 1/2, 5/6]]\n        },\n        # Case D: Cube corners\n        {\n            \"name\": \"D\",\n            \"n_g\": 30,\n            \"X\": [(x, y, z) for x in [0, 1] for y in [0, 1] for z in [0, 1]]\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        h, q, rho = compute_metrics(case[\"X\"], case[\"n_g\"])\n        results.append([round(val, 6) for val in [h, q, rho]])\n\n    # Format the output string as per the requirement:\n    # [[h_A,q_A,rho_A],[h_B,q_B,rho_B],...] with 6 decimal places.\n    formatted_cases = []\n    for case_result in results:\n        # Format each number to 6 decimal places.\n        formatted_nums = [f\"{num:.6f}\" for num in case_result]\n        formatted_cases.append(f\"[{','.join(formatted_nums)}]\")\n    \n    final_output_string = f\"[{','.join(formatted_cases)}]\"\n\n    # Final print statement in the exact required format.\n    print(final_output_string)\n\nsolve()\n```"
        },
        {
            "introduction": "Beyond simple geometric measures, discrepancy theory offers a powerful framework for quantifying uniformity. The star discrepancy, $D_N^*(X)$, measures the worst-case deviation of a point set from a perfectly uniform distribution by comparing the fraction of points in test regions to the volume of those regions. Unlike the purely geometric fill distance $h_X$, star discrepancy is particularly sensitive to how points are distributed relative to the origin.\n\nThis exercise will deepen your understanding of quality metrics by contrasting the geometric intuition of fill distance with the distributional perspective of star discrepancy . Implementing both will reveal their different sensitivities, especially to point clustering near boundaries, a critical aspect to consider when choosing a metric for sampling applications.",
            "id": "3951667",
            "problem": "In automated battery design and simulation, high-dimensional parameter studies are often framed as sampling problems over a reference domain. Consider the unit hypercube domain $\\Omega = [0,1]^d$ with $d=2$, and a finite set of sample points $X = \\{x_i\\}_{i=1}^N \\subset \\Omega$. Two commonly used space-filling quality measures are the star discrepancy $D_N^*(X)$ and the fill distance $h_X(\\Omega)$. The star discrepancy $D_N^*(X)$ is defined as the supremum (over axis-aligned boxes anchored at the origin) of the absolute difference between the empirical measure induced by $X$ and the Lebesgue measure of the box. The fill distance $h_X(\\Omega)$ is defined as the supremum, over all $x \\in \\Omega$, of the distance from $x$ to its nearest point in $X$ under the Euclidean norm.\n\nYour task is to construct and analyze sets that exhibit edge clustering, and to quantify explicitly how $D_N^*(X)$ and $h_X(\\Omega)$ respond to boundary undersampling. You must implement a program that, for each specified test set, computes:\n- the exact anchored star discrepancy $D_N^*(X)$ in dimension $d=2$ by evaluating all candidate boxes with side lengths drawn from the set of unique coordinate values present in $X$ and the values $\\{0,1\\}$,\n- the fill distance $h_X(\\Omega)$ for $\\Omega = [0,1]^2$ approximated by evaluating the supremum over a uniform Cartesian grid of resolution $R \\times R$ with $R=401$ (that is, grid nodes at coordinates $\\{0, \\tfrac{1}{400}, \\tfrac{2}{400}, \\dots, 1\\}$ in each axis), using the Euclidean norm.\n\nYou must treat all coordinates and computations within $\\Omega = [0,1]^2$. Angles are not involved. No physical units are involved. All outputs must be expressed as real numbers rounded to $6$ decimal places.\n\nThe test suite consists of the following four point sets in $\\mathbb{R}^2$:\n- Test A (baseline near-uniform grid, $N=9$): $X_A = \\{(0.2,0.2),(0.2,0.5),(0.2,0.8),(0.5,0.2),(0.5,0.5),(0.5,0.8),(0.8,0.2),(0.8,0.5),(0.8,0.8)\\}$.\n- Test B (edge clustering near the origin, $N=9$): $X_B = \\{(0.05,0.05),(0.05,0.10),(0.10,0.05),(0.15,0.05),(0.05,0.15),(0.10,0.10),(0.15,0.10),(0.10,0.15),(0.15,0.15)\\}$.\n- Test C (edge clustering near the far corner $(1,1)$, $N=9$): $X_C = \\{(0.85,0.85),(0.85,0.90),(0.90,0.85),(0.95,0.85),(0.85,0.95),(0.90,0.90),(0.95,0.90),(0.90,0.95),(0.95,0.95)\\}$.\n- Test D (boundary-concentrated corners, $N=4$): $X_D = \\{(0,0),(0,1),(1,0),(1,1)\\}$.\n\nUse the following foundational definitions as the starting point:\n- For any $u = (u_1,u_2) \\in [0,1]^2$, let $[0,u) = [0,u_1) \\times [0,u_2)$. The star discrepancy is defined as\n$$\nD_N^*(X) = \\sup_{u \\in [0,1]^2} \\left| \\frac{1}{N} \\sum_{i=1}^N \\mathbf{1}\\{x_i \\in [0,u)\\} - u_1 u_2 \\right|.\n$$\n- The fill distance is defined as\n$$\nh_X(\\Omega) = \\sup_{x \\in \\Omega} \\min_{1 \\le i \\le N} \\lVert x - x_i \\rVert_2.\n$$\n\nProgram requirements:\n- For $D_N^*(X)$ in $d=2$, evaluate the supremum exactly by checking all $u$ whose coordinates are drawn from $\\{0\\} \\cup \\{x\\text{-coordinates of }X\\} \\cup \\{1\\}$ and $\\{0\\} \\cup \\{y\\text{-coordinates of }X\\} \\cup \\{1\\}$.\n- For $h_X(\\Omega)$, approximate the supremum by evaluating the nearest-neighbor distance at each node of a uniform $R \\times R$ grid with $R=401$, and taking the maximum over all nodes. Use the Euclidean norm. Report the square root of the maximum squared distance.\n- For each test set, return two floats: first $D_N^*(X)$ and then $h_X(\\Omega)$.\n- Final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered as $[D_A, h_A, D_B, h_B, D_C, h_C, D_D, h_D]$, where each value is rounded to $6$ decimal places.\n\nDesign for coverage:\n- Test A represents a near-uniform baseline in $d=2$.\n- Test B probes sensitivity to clustering near the origin boundary.\n- Test C probes sensitivity to clustering near the $(1,1)$ boundary, emphasizing anchored-box asymmetry in $D_N^*(X)$.\n- Test D includes points on the domain boundary to exercise strict inequalities in the anchored-count and to contrast boundary coverage effects on $h_X(\\Omega)$.\n\nYour program must be completely self-contained and must not require any user input or external data. It must implement the definitions above directly and produce the single specified output line. All real-valued results must be rounded to $6$ decimal places.",
            "solution": "The user has provided a well-defined computational problem in the domain of numerical analysis and sampling theory, specifically related to the evaluation of space-filling properties of point sets. The problem is scientifically grounded, self-contained, and algorithmically specified. The task is to compute two quality metrics, star discrepancy $D_N^*(X)$ and fill distance $h_X(\\Omega)$, for four distinct point sets in the unit square $\\Omega = [0,1]^2$.\n\nThe solution will be presented in two parts, corresponding to the computation of the two metrics, followed by their application to the specified test cases.\n\n### Part 1: Star Discrepancy, $D_N^*(X)$\n\nThe star discrepancy is a quantitative measure of the deviation of a point set's distribution from perfect uniformity. For a set of $N$ points $X = \\{x_i\\}_{i=1}^N$ in the $d$-dimensional unit hypercube $\\Omega = [0,1]^d$, it is defined as the largest difference between the fraction of points falling into an origin-anchored hyperrectangle and the volume of that hyperrectangle.\n\nFor the specified case of $d=2$, the definition is:\n$$\nD_N^*(X) = \\sup_{u = (u_1, u_2) \\in [0,1]^2} \\left| \\frac{1}{N} \\sum_{i=1}^N \\mathbf{1}\\{x_i \\in [0,u)\\} - u_1 u_2 \\right|\n$$\nHere, $[0,u)$ denotes the axis-aligned rectangular region $[0,u_1) \\times [0,u_2)$, and $\\mathbf{1}\\{\\cdot\\}$ is the indicator function, which equals $1$ if the condition is true and $0$ otherwise. The term $\\frac{1}{N} \\sum_{i=1}^N \\mathbf{1}\\{x_i \\in [0,u)\\}$ represents the empirical measure of the box $[0,u)$, while $u_1 u_2$ is its Lebesgue measure (volume). The strict inequality in the indicator function, $x_i \\in [0,u)$, means a point $x_i=(x_{i,1}, x_{i,2})$ is inside if and only if $0 \\le x_{i,1} < u_1$ and $0 \\le x_{i,2} < u_2$.\n\nA key theorem in discrepancy theory states that the supremum is achieved when the coordinates of the test box corner $u$ are chosen from the coordinates of the points in $X$. The problem specifies an exact algorithm based on this principle. To compute $D_N^*(X)$, we construct a finite set of candidate test boxes and find the maximum discrepancy over this set.\n\nThe algorithmic procedure is as follows:\n1.  Given a point set $X = \\{x_i\\}_{i=1}^N \\subset [0,1]^2$, with $x_i = (x_{i,1}, x_{i,2})$.\n2.  Construct the set of unique candidate coordinates for each axis. Let $C_1$ be the set of unique values in $\\{0, 1\\} \\cup \\{x_{i,1} \\mid i=1,\\dots,N\\}$, and $C_2$ be the set of unique values in $\\{0, 1\\} \\cup \\{x_{i,2} \\mid i=1,\\dots,N\\}$.\n3.  Initialize a variable, `max_discrepancy`, to $0$.\n4.  Iterate through every possible test box corner $u = (u_1, u_2)$ where $u_1 \\in C_1$ and $u_2 \\in C_2$.\n5.  For each $u$:\n    a. Calculate the box volume: $V = u_1 u_2$.\n    b. Count the number of points in $X$ that lie inside the box $[0,u)$: $C(u) = \\sum_{i=1}^N \\mathbf{1}\\{x_{i,1} < u_1 \\text{ and } x_{i,2} < u_2\\}$.\n    c. Compute the local discrepancy: $D(u) = \\left| \\frac{C(u)}{N} - V \\right|$.\n    d. Update `max_discrepancy` = $\\max(\\text{max\\_discrepancy}, D(u))$.\n6.  The final value of `max_discrepancy` is the star discrepancy $D_N^*(X)$.\n\n### Part 2: Fill Distance, $h_X(\\Omega)$\n\nThe fill distance, also known as the dispersion, measures the size of the largest \"gap\" in the sampling of the domain $\\Omega$. It is the radius of the largest empty ball (with respect to the Euclidean norm) that can be placed in $\\Omega$ without containing any sample points from $X$.\n\nThe definition is given by:\n$$\nh_X(\\Omega) = \\sup_{x \\in \\Omega} \\min_{1 \\le i \\le N} \\lVert x - x_i \\rVert_2\n$$\nHere, $\\lVert \\cdot \\rVert_2$ denotes the Euclidean distance. The term $\\min_{1 \\le i \\le N} \\lVert x - x_i \\rVert_2$ gives the distance from a point $x \\in \\Omega$ to the closest point in the set $X$. The fill distance is the supremum of these distances over all $x$ in the domain.\n\nAn exact analytical computation of the supremum is complex as it involves finding the center of the largest empty circle within the domain's boundary, a problem from computational geometry. The problem specifies an approximation method by discretizing the domain $\\Omega = [0,1]^2$ into a fine grid and evaluating the function at each grid node.\n\nThe algorithmic procedure is as follows:\n1.  Given a point set $X = \\{x_i\\}_{i=1}^N$.\n2.  Define a uniform Cartesian grid over $\\Omega = [0,1]^2$. The resolution is specified as $R \\times R$ with $R = 401$. The grid nodes are the points $g_{k,l} = (\\frac{k}{R-1}, \\frac{l}{R-1})$ for $k, l \\in \\{0, 1, \\dots, R-1\\}$.\n3.  Initialize a variable, `max_min_sq_dist`, to $0$. Using squared distances avoids costly square root operations inside the main loop.\n4.  For each grid node $g$:\n    a. For each point $x_i \\in X$, compute the squared Euclidean distance: $d^2(g, x_i) = (g_1 - x_{i,1})^2 + (g_2 - x_{i,2})^2$.\n    b. Find the minimum of these squared distances: $d^2_{min}(g, X) = \\min_{1 \\le i \\le N} d^2(g, x_i)$.\n    c. Update `max_min_sq_dist` = $\\max(\\text{max\\_min\\_sq\\_dist}, d^2_{min}(g, X))$.\n5.  The approximate fill distance is the square root of the result: $h_X(\\Omega) \\approx \\sqrt{\\text{max\\_min\\_sq\\_dist}}$.\n\nThese two algorithms will be implemented and applied to the four test sets $X_A$, $X_B$, $X_C$, and $X_D$ as specified. The results will be aggregated into a single output line.",
            "answer": "```python\nimport numpy as np\nfrom scipy.spatial.distance import cdist\n\ndef compute_star_discrepancy(points: np.ndarray) -> float:\n    \"\"\"\n    Computes the exact anchored star discrepancy for a 2D point set.\n\n    The supremum is found by testing all boxes [0, u) where u's coordinates\n    are drawn from the set of unique coordinate values in the points, plus 0 and 1.\n    \"\"\"\n    N = points.shape[0]\n    if N == 0:\n        return 1.0\n\n    # Construct the set of candidate coordinates for the test boxes\n    u1_coords = np.unique(np.concatenate(([0.0, 1.0], points[:, 0])))\n    u2_coords = np.unique(np.concatenate(([0.0, 1.0], points[:, 1])))\n\n    max_discrepancy = 0.0\n\n    for u1 in u1_coords:\n        for u2 in u2_coords:\n            # Volume of the box [0,u1) x [0,u2)\n            volume = u1 * u2\n\n            # Count points inside the box [0,u1) x [0,u2) using strict inequality\n            count = np.sum((points[:, 0] < u1) & (points[:, 1] < u2))\n            \n            # Empirical measure\n            emp_measure = count / N\n            \n            # Local discrepancy\n            discrepancy = np.abs(emp_measure - volume)\n            \n            if discrepancy > max_discrepancy:\n                max_discrepancy = discrepancy\n\n    return max_discrepancy\n\ndef compute_fill_distance(points: np.ndarray) -> float:\n    \"\"\"\n    Approximates the fill distance for a 2D point set in [0,1]^2.\n\n    The supremum of the nearest-neighbor distance is approximated by evaluating\n    it over a uniform R x R grid.\n    \"\"\"\n    N = points.shape[0]\n    if N == 0:\n        return np.sqrt(2) # Furthest point from origin (0,0) is (1,1) in [0,1]^2\n\n    R = 401\n    grid_coords = np.linspace(0.0, 1.0, R)\n    \n    # Create a grid of evaluation points\n    grid_x, grid_y = np.meshgrid(grid_coords, grid_coords)\n    grid_points = np.vstack([grid_x.ravel(), grid_y.ravel()]).T\n\n    # Compute squared Euclidean distances from each grid point to each sample point\n    # cdist(A, B, 'sqeuclidean') computes the squared Euclidean distance\n    # between each pair of rows in A and B.\n    # The result `dists_sq` is a (R*R, N) matrix.\n    dists_sq = cdist(grid_points, points, 'sqeuclidean')\n\n    # For each grid point, find the minimum squared distance to any sample point\n    min_dists_sq = np.min(dists_sq, axis=1)\n\n    # The maximum of these minimum distances is the squared fill distance (approximated)\n    max_min_sq_dist = np.max(min_dists_sq)\n\n    # The fill distance is the square root of this value\n    return np.sqrt(max_min_sq_dist)\n    \ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test cases.\n    \"\"\"\n    test_cases = {\n        \"A\": np.array([\n            (0.2, 0.2), (0.2, 0.5), (0.2, 0.8),\n            (0.5, 0.2), (0.5, 0.5), (0.5, 0.8),\n            (0.8, 0.2), (0.8, 0.5), (0.8, 0.8)\n        ]),\n        \"B\": np.array([\n            (0.05, 0.05), (0.05, 0.10), (0.10, 0.05),\n            (0.15, 0.05), (0.05, 0.15), (0.10, 0.10),\n            (0.15, 0.10), (0.10, 0.15), (0.15, 0.15)\n        ]),\n        \"C\": np.array([\n            (0.85, 0.85), (0.85, 0.90), (0.90, 0.85),\n            (0.95, 0.85), (0.85, 0.95), (0.90, 0.90),\n            (0.95, 0.90), (0.90, 0.95), (0.95, 0.95)\n        ]),\n        \"D\": np.array([\n            (0.0, 0.0), (0.0, 1.0), (1.0, 0.0), (1.0, 1.0)\n        ])\n    }\n\n    results = []\n    # Ensure correct processing order A, B, C, D\n    for key in sorted(test_cases.keys()):\n        points = test_cases[key]\n        \n        # Calculate star discrepancy\n        discrepancy = compute_star_discrepancy(points)\n        \n        # Calculate fill distance\n        fill_dist = compute_fill_distance(points)\n        \n        results.append(discrepancy)\n        results.append(fill_dist)\n\n    # Format results to 6 decimal places for final output\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "After learning to evaluate existing designs, the next logical step is to generate new ones. A powerful and intuitive approach is a sequential method that adds points one by one to fill the largest existing gap in the design space. The greedy farthest-point algorithm formalizes this by iteratively selecting a new sample point that is maximally distant from the set of existing points.\n\nThis practice provides hands-on experience in creating a high-quality space-filling design from scratch . It introduces a crucial real-world adaptation: a weighted metric space, which normalizes parameters with different physical units and scales, reflecting their relative importance in the model. Tracking the evolution of the fill ($h_X$) and separation ($q_X$) distances with each new point provides direct insight into the constructive nature of sequential design strategies.",
            "id": "3951689",
            "problem": "Consider a parameter domain for lithium-ion battery design represented as a hyperrectangle $\\Omega \\subset \\mathbb{R}^d$, with axes corresponding to physically meaningful parameters. To construct space-filling designs suitable for automated battery simulation, implement a greedy farthest-point sampling strategy in a weighted metric space, where weights encode dimension-specific characteristic scales to render the metric dimensionless and reflect parameter sensitivity. Let $X_k = \\{x_1, \\dots, x_k\\} \\subset \\Omega$ denote the current design set, and let $C \\subset \\Omega$ be a finite candidate set sampled on a grid. Define a diagonal positive-definite weight matrix $W = \\mathrm{diag}(w_1, \\dots, w_d)$ with $w_i = 1/s_i^2$, where $s_i > 0$ is the characteristic scale for the $i$-th parameter. Define the weighted distance between $x, y \\in \\Omega$ by\n$$\nd_W(x, y) = \\sqrt{(x - y)^\\top W (x - y)} = \\sqrt{\\sum_{i=1}^d w_i (x_i - y_i)^2},\n$$\nwhich is dimensionless when $s_i$ is chosen to match the unit of the $i$-th coordinate.\n\nFor a finite set $X \\subset \\Omega$, define the separation distance\n$$\nq_X = \\min_{\\substack{x, y \\in X\\\\ x \\neq y}} d_W(x, y),\n$$\nand the fill distance with respect to the candidate set $C$\n$$\nh_X = \\max_{z \\in C} \\min_{x \\in X} d_W(z, x).\n$$\nAdopt the convention $q_X = 0$ when $\\lvert X \\rvert = 1$. The greedy farthest-point algorithm iteratively augments $X_k$ by selecting\n$$\nx_{k+1} = \\arg\\max_{z \\in C \\setminus X_k} \\left( \\min_{x \\in X_k} d_W(z, x) \\right),\n$$\nstarting from an initial point chosen as the candidate closest to the center of $\\Omega$ under $d_W$. After each addition, compute $q_{X_k}$ and $h_{X_k}$. All distances are dimensionless and must be reported as floats rounded to six decimal places.\n\nYou must implement a complete program that:\n- Builds the candidate set $C$ as a full grid over $\\Omega$ with specified resolutions along each axis.\n- Initializes $X_1$ by selecting the candidate closest (under $d_W$) to the center of $\\Omega$.\n- Applies the greedy farthest-point rule to select $K$ points and, after each addition, computes $q_{X_k}$ and $h_{X_k}$ on the candidate set $C$.\n\nUse the following test suite, which covers typical and edge behaviors in battery parameter domains:\n\nTest Case A (Happy path, three-dimensional, moderate anisotropy):\n- Dimension $d = 3$ with parameters: electrode thickness $t$ in meters, porosity $\\varepsilon$ dimensionless, and electrolyte conductivity $\\kappa$ in siemens per meter.\n- Bounds $\\Omega = [60 \\times 10^{-6}, 120 \\times 10^{-6}] \\times [0.25, 0.45] \\times [0.8, 2.0]$.\n- Characteristic scales $s = [30 \\times 10^{-6}, 0.10, 0.60]$ so $W = \\mathrm{diag}(1/s_i^2)$.\n- Candidate grid resolutions $\\text{res} = [7, 5, 6]$.\n- Number of points $K = 6$.\n\nTest Case B (Boundary behavior, two-dimensional, strong anisotropy):\n- Dimension $d = 2$ with parameters: solid-phase diffusion coefficient $D_s$ in square meters per second, and exchange current density $i_0$ in amperes per square meter.\n- Bounds $\\Omega = [1 \\times 10^{-14}, 5 \\times 10^{-14}] \\times [0.5, 2.5]$.\n- Characteristic scales $s = [0.5 \\times 10^{-14}, 1.0]$ so $W = \\mathrm{diag}(1/s_i^2)$.\n- Candidate grid resolutions $\\text{res} = [8, 9]$.\n- Number of points $K = 5$.\n\nTest Case C (Edge case, one-dimensional, candidate exhaustion):\n- Dimension $d = 1$ with parameter: separator thickness $\\delta$ in meters.\n- Bounds $\\Omega = [10 \\times 10^{-6}, 12 \\times 10^{-6}]$.\n- Characteristic scales $s = [1 \\times 10^{-6}]$ so $W = \\mathrm{diag}(1/s_i^2)$.\n- Candidate grid resolution $\\text{res} = [5]$.\n- Number of points $K = 5$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case result is a list of pairs $[q_{X_k}, h_{X_k}]$ for $k = 1, \\dots, K$. For example, the output must be of the form\n$[ \\text{caseA}, \\text{caseB}, \\text{caseC} ]$\nwith each $\\text{caseX}$ equal to a list of lists of floats rounded to six decimal places. All distances are dimensionless and must be reported as floats rounded to six decimal places. Angle units are not applicable. Percentages are not applicable.",
            "solution": "The problem statement is valid. It is scientifically grounded in the field of Design of Experiments (DoE) for computational simulations, mathematically well-posed, objective, and provides a complete and consistent set of definitions and data for implementation. The problem asks for the implementation of a greedy farthest-point sampling algorithm to generate a space-filling design within a specified hyperrectangular parameter domain $\\Omega \\subset \\mathbb{R}^d$. The solution involves several integrated steps, based on the principles of weighted metric spaces and iterative optimization.\n\nFirst, we establish the geometric setting. The parameter domain $\\Omega$ is defined by its lower and upper bounds along each of its $d$ dimensions. The candidate points $C$, from which the design points are selected, form a discrete grid over $\\Omega$. This grid is constructed by taking the Cartesian product of one-dimensional, linearly spaced points along each axis, with specified resolutions. The number of points in the grid for the $i$-th dimension is $\\text{res}_i$.\n\nA crucial concept is the weighted distance $d_W(x, y)$, defined as:\n$$\nd_W(x, y) = \\sqrt{(x - y)^\\top W (x - y)} = \\sqrt{\\sum_{i=1}^d w_i (x_i - y_i)^2}\n$$\nThe weight matrix $W$ is diagonal, with entries $w_i = 1/s_i^2$, where $s_i$ is a characteristic scale for the $i$-th parameter. This metric serves two purposes: it renders the distance dimensionless by normalizing each coordinate difference $(x_i - y_i)$ by its scale $s_i$, and it allows for encoding differing sensitivities or importance across parameters. All subsequent geometric calculations are performed in this weighted space.\n\nThe algorithm proceeds as follows:\n\n1.  **Initialization**: The design set is initialized with a single point, $X_1 = \\{x_1\\}$. The point $x_1$ is chosen from the candidate set $C$ to be the one closest to the geometric center of the domain $\\Omega$ under the weighted distance $d_W$. The center $c$ of $\\Omega = \\prod_{i=1}^d [l_i, u_i]$ is the point with coordinates $c_i = (l_i + u_i)/2$. Thus, $x_1 = \\arg\\min_{z \\in C} d_W(z, c)$.\n\n2.  **Iterative Selection**: For $k = 1, \\dots, K-1$, the design set $X_k$ is augmented to $X_{k+1}$ by adding the candidate point that is farthest from the existing set $X_k$. This is the greedy farthest-point or minimax selection rule:\n    $$\n    x_{k+1} = \\arg\\max_{z \\in C \\setminus X_k} \\left( \\min_{x \\in X_k} d_W(z, x) \\right)\n    $$\n    This rule iteratively places new points in the least-sampled regions of the domain, as measured by the fill distance on the candidate set. To implement this efficiently, we maintain an array of minimum distances from each candidate point $z \\in C$ to the current design set $X_k$. When a new point $x_{k+1}$ is added, this array is updated by taking the element-wise minimum of the old distances and the distances to the new point $x_{k+1}$. The next point to add is then simply the candidate corresponding to the maximum value in this updated array of minimum distances.\n\n3.  **Metric Computation**: After each point $x_k$ is added (for $k=1, \\dots, K$), two quality metrics for the set $X_k$ are computed.\n    *   **Separation Distance ($q_{X_k}$)**: This metric measures the minimum pairwise distance between points within the design set. It is defined as:\n        $$\n        q_{X_k} = \\min_{\\substack{x, y \\in X_k \\\\ x \\neq y}} d_W(x, y)\n        $$\n        By convention, for a singleton set $X_1$, $q_{X_1}$ is set to $0$. For $k > 1$, this requires computing all $\\binom{k}{2}$ pairwise distances within $X_k$ and finding the minimum.\n    *   **Fill Distance ($h_{X_k}$)**: This metric measures how well the design set covers the candidate set $C$. It is defined as the largest distance from any candidate point to its nearest neighbor in the design set:\n        $$\n        h_{X_k} = \\max_{z \\in C} \\min_{x \\in X_k} d_W(z, x)\n        $$\n        As noted in the selection step, the value $h_{X_k}$ is precisely the maximum of the minimum-distance array used to select the subsequent point $x_{k+2}$.\n\nThe process is repeated for each of the three test cases provided. For each case, we generate a list of pairs $[q_{X_k}, h_{X_k}]$ for $k=1, \\dots, K$. All distance values are rounded to six decimal places as required. The implementation relies on `numpy` for efficient, vectorized calculations of distances and for managing the candidate and design sets.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final results.\n    \"\"\"\n    test_cases = [\n        {\n            \"name\": \"Case A\",\n            \"d\": 3,\n            \"bounds\": np.array([[60e-6, 120e-6], [0.25, 0.45], [0.8, 2.0]]),\n            \"scales\": np.array([30e-6, 0.10, 0.60]),\n            \"resolutions\": [7, 5, 6],\n            \"K\": 6,\n        },\n        {\n            \"name\": \"Case B\",\n            \"d\": 2,\n            \"bounds\": np.array([[1e-14, 5e-14], [0.5, 2.5]]),\n            \"scales\": np.array([0.5e-14, 1.0]),\n            \"resolutions\": [8, 9],\n            \"K\": 5,\n        },\n        {\n            \"name\": \"Case C\",\n            \"d\": 1,\n            \"bounds\": np.array([[10e-6, 12e-6]]),\n            \"scales\": np.array([1e-6]),\n            \"resolutions\": [5],\n            \"K\": 5,\n        },\n    ]\n\n    all_results = []\n    for case in test_cases:\n        result = run_case(case[\"bounds\"], case[\"scales\"], case[\"resolutions\"], case[\"K\"])\n        all_results.append(result)\n\n    # Format the output string as per the requirement: no spaces, 6 decimal places.\n    all_case_strings = []\n    for case_result in all_results:\n        pair_strings = [f\"[{pair[0]:.6f},{pair[1]:.6f}]\" for pair in case_result]\n        all_case_strings.append(f\"[{','.join(pair_strings)}]\")\n    \n    final_output = f\"[{','.join(all_case_strings)}]\"\n    print(final_output)\n\n\ndef run_case(bounds, scales, resolutions, K):\n    \"\"\"\n    Executes the greedy farthest-point sampling for a single test case.\n    \"\"\"\n    # 1. Setup: Weights and Candidate Set\n    weights = 1.0 / (scales**2)\n    grid_axes = [np.linspace(b[0], b[1], r) for b, r in zip(bounds, resolutions)]\n    mesh = np.meshgrid(*grid_axes, indexing='ij')\n    candidates = np.vstack([m.ravel() for m in mesh]).T\n    \n    # 2. Initialization: Find starting point x_1\n    center = np.mean(bounds, axis=1)\n    \n    # helper for weighted distance from one point p1 to many points p2\n    def weighted_dist(p1, p2_array):\n        diff = p2_array - p1\n        return np.sqrt(np.sum(weights * diff**2, axis=1))\n\n    dists_to_center = weighted_dist(center, candidates)\n    initial_idx = np.argmin(dists_to_center)\n    \n    X_indices = {initial_idx}\n    X_points = [candidates[initial_idx]]\n    \n    case_results = []\n\n    # 3. Iteratively select points and compute metrics\n    min_dists_to_X = weighted_dist(X_points[0], candidates)\n\n    for k in range(1, K + 1):\n        # Metrics for the current set X_k\n        \n        # Calculate q_Xk (Separation Distance)\n        if k == 1:\n            q_k = 0.0\n        else:\n            current_X_array = np.array(X_points)\n            pairwise_dists = []\n            for i in range(k):\n                for j in range(i + 1, k):\n                    diff = current_X_array[i] - current_X_array[j]\n                    dist = np.sqrt(np.sum(weights * diff**2))\n                    pairwise_dists.append(dist)\n            q_k = np.min(pairwise_dists)\n            \n        # Calculate h_Xk (Fill Distance)\n        # h_k is the max of the current minimum distances to X_k\n        h_k = np.max(min_dists_to_X)\n        \n        case_results.append([round(q_k, 6), round(h_k, 6)])\n        \n        # If we have collected K points, we stop.\n        if k == K:\n            break\n            \n        # Select next point (x_{k+1})\n        # The next point is the candidate farthest from the current set X_k.\n        # To strictly implement C \\ X_k, we mask already selected points.\n        temp_min_dists = np.copy(min_dists_to_X)\n        temp_min_dists[list(X_indices)] = -1.0 # Ensure selected points are not chosen again\n        next_idx = np.argmax(temp_min_dists)\n        \n        next_point = candidates[next_idx]\n        X_indices.add(next_idx)\n        X_points.append(next_point)\n        \n        # Update minimum distances for the new set X_{k+1}\n        dists_to_new_point = weighted_dist(next_point, candidates)\n        min_dists_to_X = np.minimum(min_dists_to_X, dists_to_new_point)\n        \n    return case_results\n\nsolve()\n```"
        }
    ]
}