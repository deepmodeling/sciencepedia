{
    "hands_on_practices": [
        {
            "introduction": "In battery design, we rarely have the luxury of optimizing a single metric. Improving energy density might increase cost, while designing for high power might compromise lifetime. This practice puts you in the role of a design engineer facing a classic trade-off between performance (deliverable energy) and manufacturing cost. You will construct simple but physically-grounded surrogate models and use them to map out the Pareto front—the set of all non-dominated optimal designs—providing a powerful tool for informed decision-making.",
            "id": "3954458",
            "problem": "A positive electrode in a lithium-ion cell is manufactured with thickness $L_p$ and porosity $\\epsilon_p$. Consider techno-economic optimization with two objectives: maximize performance and minimize cost. You are to compute and analyze the Pareto front over discrete design choices of $L_p$ and $\\epsilon_p$ using surrogate models derived from fundamental physical and economic considerations.\n\nStart from the following foundations:\n\n- Definition of porosity and solid volume fraction: the solid fraction is $1 - \\epsilon_p$.\n- Areal mass of active solid: $m_a = \\rho_s (1 - \\epsilon_p) L_p$, where $\\rho_s$ is the density of the solid active material.\n- Areal capacity: $Q_a = Q_s \\, m_a$, where $Q_s$ is the specific capacity of the solid.\n- Areal energy content for discharge around a mean voltage: $E_a = Q_a \\, U_{\\text{mean}}$, where $U_{\\text{mean}}$ is the mean discharge voltage. Express $E_a$ in $\\mathrm{Wh/m^2}$.\n- Bruggeman Effective Medium (BEM) relation for effective electrolyte conductivity in the porous network: $\\kappa_{\\text{eff}} = \\kappa \\, \\epsilon_p^{\\beta}$, where $\\kappa$ is bulk electrolyte conductivity and $\\beta$ is the Bruggeman exponent.\n- One-Dimensional Steady-State Ohmic Conduction (SSOC) in the pore phase: the ohmic potential drop $\\Delta \\phi$ across the electrode for a uniform current density $i$ is $\\Delta \\phi = i \\, L_p / \\kappa_{\\text{eff}}$. Constrain the allowable drop to a fraction $\\alpha$ of the mean discharge voltage $U_{\\text{mean}}$ so that $i \\le i_{\\text{lim}} = \\alpha \\, U_{\\text{mean}} \\, \\kappa_{\\text{eff}} / L_p$.\n- Deliverable energy at a specified target current density $i_{\\text{target}}$: define the deliverable fraction $f = \\min\\left(1, \\, i_{\\text{lim}} / i_{\\text{target}}\\right)$ and the deliverable areal energy $E_{\\text{del}} = f \\, E_a$ expressed in $\\mathrm{Wh/m^2}$.\n- Cost per area combining materials and processing: $C_{\\text{area}} = c_s \\, m_a + c_{p0} + c_{pL} \\, L_p \\, \\left(1 + \\gamma (1 - \\epsilon_p)\\right)$, expressed in $\\$\\!/\\mathrm{m^2}$, where $c_s$ is the material cost per $\\mathrm{kg}$, $c_{p0}$ is a fixed processing overhead per $\\mathrm{m^2}$, $c_{pL}$ is a thickness-proportional processing cost per $\\mathrm{m^3}$, and $\\gamma$ modulates added processing burden with higher solid fraction.\n\nTasks:\n\n1. Using only the foundations above, derive explicit surrogate expressions for $E_{\\text{del}}(L_p,\\epsilon_p)$ and $C_{\\text{area}}(L_p,\\epsilon_p)$ that depend on $L_p$ and $\\epsilon_p$ and the given constants.\n2. Over the discrete design grid $L_p \\in \\{60\\,\\mu\\mathrm{m},\\,100\\,\\mu\\mathrm{m},\\,140\\,\\mu\\mathrm{m},\\,180\\,\\mu\\mathrm{m},\\,220\\,\\mu\\mathrm{m}\\}$ and $\\epsilon_p \\in \\{0.30,\\,0.35,\\,0.40,\\,0.45,\\,0.50\\}$, compute $(E_{\\text{del}}, C_{\\text{area}})$ for each pair.\n3. Compute the Pareto front for maximizing $E_{\\text{del}}$ and minimizing $C_{\\text{area}}$. A design $(E_1, C_1)$ dominates $(E_2, C_2)$ if $E_1 \\ge E_2$ and $C_1 \\le C_2$ with at least one inequality strict. The Pareto front is the set of non-dominated designs.\n4. Interpret trade-offs via curvature of the Pareto front. Normalize the Pareto front coordinates to $[0,1]$ on each axis using min-max normalization: $x = (C - C_{\\min})/(C_{\\max} - C_{\\min})$ and $y = (E - E_{\\min})/(E_{\\max} - E_{\\min})$, where $(x,y)$ are the normalized cost and performance. Compute the discrete curvature at each interior point of the normalized front using the circumcircle formula for three consecutive points $(x_{i-1},y_{i-1})$, $(x_i,y_i)$, $(x_{i+1},y_{i+1})$:\n$$\n\\kappa_i = \\frac{2 A_i}{d_{i-1,i} \\, d_{i,i+1} \\, d_{i+1,i-1}},\n$$\nwhere $A_i$ is the area of the triangle formed by the three points (using the shoelace formula) and $d_{j,k}$ is the Euclidean distance between points $j$ and $k$. Identify the \"knee\" point as the Pareto design with the maximum curvature $\\kappa_i$ on the normalized front. If the Pareto front has fewer than $3$ points, define $\\kappa = 0$ and choose the knee as the point with lowest $x$ and highest $y$ by lexicographic preference on $(x,-y)$.\n5. For each parameter set in the test suite below, compute the knee design and report:\n   - $L_p$ in $\\mu\\mathrm{m}$ as an integer,\n   - $\\epsilon_p$ as a decimal fraction rounded to two decimals,\n   - the normalized curvature $\\kappa$ rounded to three decimals,\n   - $E_{\\text{del}}$ in $\\mathrm{Wh/m^2}$ rounded to one decimal,\n   - $C_{\\text{area}}$ in $\\$\\!/\\mathrm{m^2}$ rounded to two decimals.\n\nTest suite (each case gives $(\\rho_s, Q_s, U_{\\text{mean}}, \\kappa, \\beta, \\alpha, i_{\\text{target}}, c_s, c_{p0}, c_{pL}, \\gamma)$):\n\n- Case $\\mathbf{A}$ (base): ($3000\\,\\mathrm{kg/m^3}$, $170\\,\\mathrm{Ah/kg}$, $3.7\\,\\mathrm{V}$, $1.0\\,\\mathrm{S/m}$, $1.5$, $0.03$, $200\\,\\mathrm{A/m^2}$, $22\\,\\$/\\mathrm{kg}$, $1.5\\,\\$/\\mathrm{m^2}$, $80\\,\\$/\\mathrm{m^3}$, $1.2$).\n- Case $\\mathbf{B}$ (higher conductivity electrolyte): ($3000\\,\\mathrm{kg/m^3}$, $170\\,\\mathrm{Ah/kg}$, $3.7\\,\\mathrm{V}$, $1.5\\,\\mathrm{S/m}$, $1.5$, $0.03$, $200\\,\\mathrm{A/m^2}$, $22\\,\\$/\\mathrm{kg}$, $1.5\\,\\$/\\mathrm{m^2}$, $80\\,\\$/\\mathrm{m^3}$, $1.2$).\n- Case $\\mathbf{C}$ (stricter ohmic constraint and higher target current): ($3000\\,\\mathrm{kg/m^3}$, $170\\,\\mathrm{Ah/kg}$, $3.7\\,\\mathrm{V}$, $1.0\\,\\mathrm{S/m}$, $1.5$, $0.02$, $250\\,\\mathrm{A/m^2}$, $22\\,\\$/\\mathrm{kg}$, $1.5\\,\\$/\\mathrm{m^2}$, $80\\,\\$/\\mathrm{m^3}$, $1.2$).\n- Case $\\mathbf{D}$ (higher material cost): ($3000\\,\\mathrm{kg/m^3}$, $170\\,\\mathrm{Ah/kg}$, $3.7\\,\\mathrm{V}$, $1.0\\,\\mathrm{S/m}$, $1.5$, $0.03$, $200\\,\\mathrm{A/m^2}$, $35\\,\\$/\\mathrm{kg}$, $1.5\\,\\$/\\mathrm{m^2}$, $80\\,\\$/\\mathrm{m^3}$, $1.2$).\n\nAngle units do not apply. Percentages must be expressed as decimal fractions.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the list for one case in the order $\\mathbf{A}$, $\\mathbf{B}$, $\\mathbf{C}$, $\\mathbf{D}$: $[\\,[L_p,\\epsilon_p,\\kappa,E_{\\text{del}},C_{\\text{area}}],\\,[\\dots],\\,[\\dots],\\,[\\dots]\\,]$.",
            "solution": "The posed problem is a well-defined exercise in techno-economic optimization and surrogate modeling, grounded in established physical and economic principles of lithium-ion battery design. The problem statement is self-contained, with all necessary parameters, definitions, and equations provided. The units are consistent, and the given parameter values are physically realistic. The tasks are objective, quantitative, and lead to a unique, computable solution. Therefore, the problem is deemed valid.\n\nThe solution proceeds by first deriving the explicit surrogate models for the objectives, then outlining the computational procedure for identifying the Pareto-optimal designs and the preferred \"knee\" point, which represents a balanced trade-off between performance and cost.\n\n### 1. Derivation of Surrogate Models\n\nThe problem requires the derivation of explicit expressions for the deliverable areal energy, $E_{\\text{del}}(L_p, \\epsilon_p)$, and the cost per area, $C_{\\text{area}}(L_p, \\epsilon_p)$, as functions of the electrode thickness $L_p$ and porosity $\\epsilon_p$.\n\n**Areal Cost ($C_{\\text{area}}$):**\nThe cost model is given as:\n$$\nC_{\\text{area}} = c_s \\, m_a + c_{p0} + c_{pL} \\, L_p \\, \\left(1 + \\gamma (1 - \\epsilon_p)\\right)\n$$\nThe areal mass of active solid, $m_a$, is defined as:\n$$\nm_a = \\rho_s (1 - \\epsilon_p) L_p\n$$\nSubstituting the expression for $m_a$ into the cost equation yields the final surrogate model for cost, which is a function of $L_p$ and $\\epsilon_p$ and the material/process constants:\n$$\nC_{\\text{area}}(L_p, \\epsilon_p) = c_s \\rho_s (1 - \\epsilon_p) L_p + c_{p0} + c_{pL} L_p \\left(1 + \\gamma (1 - \\epsilon_p)\\right)\n$$\n\n**Deliverable Areal Energy ($E_{\\text{del}}$):**\nThe deliverable energy is defined as $E_{\\text{del}} = f \\cdot E_a$, where $E_a$ is the total areal energy and $f$ is the deliverable fraction.\n\nFirst, we express the total areal energy, $E_a$. It is the product of the areal capacity, $Q_a$, and the mean discharge voltage, $U_{\\text{mean}}$.\n$$\nE_a = Q_a \\, U_{\\text{mean}}\n$$\nThe areal capacity is the product of the specific capacity, $Q_s$, and the areal mass, $m_a$:\n$$\nQ_a = Q_s \\, m_a = Q_s \\rho_s (1 - \\epsilon_p) L_p\n$$\nCombining these gives the expression for total areal energy:\n$$\nE_a(L_p, \\epsilon_p) = Q_s \\rho_s U_{\\text{mean}} (1 - \\epsilon_p) L_p\n$$\nThis shows that total energy content scales linearly with thickness $L_p$ and solid volume fraction $(1 - \\epsilon_p)$.\n\nNext, we derive the deliverable fraction, $f$. This fraction is limited by ohmic losses in the electrolyte, modeled by One-Dimensional Steady-State Ohmic Conduction (SSOC). The limiting current density, $i_{\\text{lim}}$, is found by constraining the ohmic potential drop $\\Delta \\phi$ to a fraction $\\alpha$ of the mean voltage $U_{\\text{mean}}$:\n$$\n\\Delta \\phi = \\frac{i L_p}{\\kappa_{\\text{eff}}} \\le \\alpha U_{\\text{mean}} \\implies i \\le i_{\\text{lim}} = \\frac{\\alpha U_{\\text{mean}} \\kappa_{\\text{eff}}}{L_p}\n$$\nThe effective electrolyte conductivity, $\\kappa_{\\text{eff}}$, is given by the Bruggeman relation:\n$$\n\\kappa_{\\text{eff}} = \\kappa \\, \\epsilon_p^{\\beta}\n$$\nSubstituting this into the expression for $i_{\\text{lim}}$ gives:\n$$\ni_{\\text{lim}}(L_p, \\epsilon_p) = \\frac{\\alpha U_{\\text{mean}} \\kappa \\epsilon_p^{\\beta}}{L_p}\n$$\nThe deliverable fraction $f$ is the ratio of this limiting current to a target current density $i_{\\text{target}}$, capped at $1$:\n$$\nf = \\min\\left(1, \\frac{i_{\\text{lim}}}{i_{\\text{target}}}\\right) = \\min\\left(1, \\frac{\\alpha U_{\\text{mean}} \\kappa \\epsilon_p^{\\beta}}{L_p i_{\\text{target}}}\\right)\n$$\nFinally, the surrogate model for deliverable areal energy is the product of $f$ and $E_a$:\n$$\nE_{\\text{del}}(L_p, \\epsilon_p) = \\min\\left(1, \\frac{\\alpha U_{\\text{mean}} \\kappa \\epsilon_p^{\\beta}}{L_p i_{\\text{target}}}\\right) \\cdot \\left[ Q_s \\rho_s U_{\\text{mean}} (1 - \\epsilon_p) L_p \\right]\n$$\nThis expression captures the fundamental trade-off: thicker, denser electrodes (high $L_p$, low $\\epsilon_p$) have high theoretical energy $E_a$, but may suffer from power limitations (low $f$), reducing the actually deliverable energy $E_{\\text{del}}$.\n\n### 2. Pareto Front Computation\n\nThe optimization problem involves two objectives: maximizing deliverable energy $E_{\\text{del}}$ and minimizing areal cost $C_{\\text{area}}$. The solution is not a single point but a set of non-dominated designs, known as the Pareto front.\n\nA design point $p_1 = (E_1, C_1)$ is said to dominate another point $p_2 = (E_2, C_2)$ if $E_1 \\ge E_2$ and $C_1 \\le C_2$, with at least one inequality being strict. The Pareto front is the set of all designs that are not dominated by any other design in the set.\n\nThe computational procedure is as follows:\n1.  Define the discrete design grid of $N_L=5$ thickness values $L_p \\in \\{60, 100, 140, 180, 220\\}\\,\\mu\\mathrm{m}$ and $N_\\epsilon=5$ porosity values $\\epsilon_p \\in \\{0.30, 0.35, 0.40, 0.45, 0.50\\}$, resulting in $N=25$ total design points. Note that $L_p$ must be converted to meters for all calculations (e.g., $60\\,\\mu\\mathrm{m} = 6 \\times 10^{-5}\\,\\mathrm{m}$).\n2.  For each of the $N$ design pairs $(L_p, \\epsilon_p)$, compute the objective pair $(E_{\\text{del}}, C_{\\text{area}})$ using the derived surrogate models.\n3.  Identify the Pareto front by iterating through all design points. For each point $p_i$, check if it is dominated by any other point $p_j$. If no such $p_j$ exists, $p_i$ is a member of the Pareto front.\n4.  The identified Pareto front points are then sorted by increasing cost $C_{\\text{area}}$ for subsequent analysis.\n\n### 3. Knee Point Identification via Curvature\n\nThe \"knee\" of the Pareto front represents a point of inflection, often interpreted as the most balanced trade-off. It is identified as the point of maximum curvature on the normalized front.\n\nThe procedure is:\n1.  If the Pareto front contains fewer than $3$ points, a meaningful curvature cannot be computed. In this case, the curvature $\\kappa$ is defined as $0$, and the knee point is chosen as the design with the minimum cost, which, on a Pareto front, will also have the highest energy for that cost.\n2.  If the front has $3$ or more points, first normalize the coordinates. Let the points on the sorted Pareto front be $(C_i, E_i)$. Find $C_{\\min}, C_{\\max}, E_{\\min}, E_{\\max}$ over all points on the front. The normalized coordinates $(x_i, y_i)$ are:\n    $$\n    x_i = \\frac{C_i - C_{\\min}}{C_{\\max} - C_{\\min}}, \\quad y_i = \\frac{E_i - E_{\\min}}{E_{\\max} - E_{\\min}}\n    $$\n3.  For each interior point $i$ on the normalized front (i.e., not the first or last point), consider the triangle formed by it and its neighbors, $(x_{i-1}, y_{i-1})$, $(x_i, y_i)$, and $(x_{i+1}, y_{i+1})$.\n4.  The discrete curvature $\\kappa_i$ at point $i$ is calculated using the circumcircle formula:\n    $$\n    \\kappa_i = \\frac{2 A_i}{d_{i-1,i} \\, d_{i,i+1} \\, d_{i+1,i-1}}\n    $$\n    where $d_{j,k}$ is the Euclidean distance between normalized points_j_ and_k_, and $A_i$ is the area of the triangle formed by the three points, calculated using the shoelace formula.\n5.  The knee point is the design corresponding to the interior Pareto point with the maximum calculated curvature $\\kappa_i$.\n\n### 4. Implementation for Test Cases\n\nThe complete procedure—from calculating objectives for all $25$ designs to identifying the knee point of the Pareto front—is encapsulated in a Python script. This script iterates through each of the four specified test cases, applies the constants for that case, and computes the required output values for the knee design: $L_p$ (in $\\mu\\mathrm{m}$), $\\epsilon_p$, the normalized curvature $\\kappa$, $E_{\\text{del}}$ (in $\\mathrm{Wh/m^2}$), and $C_{\\text{area}}$ (in $\\$\\!/\\mathrm{m^2}$), with specified rounding.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the techno-economic optimization problem for multiple test cases.\n    \"\"\"\n    # 2. Define the discrete design grid\n    Lp_grid_um = np.array([60, 100, 140, 180, 220])\n    Lp_grid_m = Lp_grid_um * 1e-6\n    ep_grid = np.array([0.30, 0.35, 0.40, 0.45, 0.50])\n\n    # Test cases from the problem statement\n    test_cases = [\n        # Case A: base\n        (3000, 170, 3.7, 1.0, 1.5, 0.03, 200, 22, 1.5, 80, 1.2),\n        # Case B: higher conductivity electrolyte\n        (3000, 170, 3.7, 1.5, 1.5, 0.03, 200, 22, 1.5, 80, 1.2),\n        # Case C: stricter ohmic constraint and higher target current\n        (3000, 170, 3.7, 1.0, 1.5, 0.02, 250, 22, 1.5, 80, 1.2),\n        # Case D: higher material cost\n        (3000, 170, 3.7, 1.0, 1.5, 0.03, 200, 35, 1.5, 80, 1.2),\n    ]\n\n    final_results = []\n    \n    for params in test_cases:\n        rho_s, Q_s, U_mean, kappa, beta, alpha, i_target, c_s, c_p0, c_pL, gamma = params\n\n        # Task 2: Compute (E_del, C_area) for each pair\n        design_points = []\n        for Lp_m, Lp_um in zip(Lp_grid_m, Lp_grid_um):\n            for ep in ep_grid:\n                # Surrogate model for E_del\n                m_a = rho_s * (1 - ep) * Lp_m\n                E_a = Q_s * m_a * U_mean\n                kappa_eff = kappa * ep**beta\n                i_lim = alpha * U_mean * kappa_eff / Lp_m\n                f = min(1.0, i_lim / i_target)\n                E_del = f * E_a\n\n                # Surrogate model for C_area\n                C_area = c_s * m_a + c_p0 + c_pL * Lp_m * (1 + gamma * (1 - ep))\n                \n                design_points.append({\n                    'Lp': Lp_um,\n                    'ep': ep,\n                    'E_del': E_del,\n                    'C_area': C_area\n                })\n\n        # Task 3: Compute the Pareto front\n        pareto_front_points = []\n        for p1 in design_points:\n            is_dominated = False\n            for p2 in design_points:\n                if p1 is p2:\n                    continue\n                # p2 dominates p1 if it is better or equal in both objectives\n                # and strictly better in at least one.\n                if (p2['E_del'] = p1['E_del'] and p2['C_area'] = p1['C_area']):\n                    if (p2['E_del']  p1['E_del'] or p2['C_area']  p1['C_area']):\n                        is_dominated = True\n                        break\n            if not is_dominated:\n                pareto_front_points.append(p1)\n\n        # Sort the front by increasing cost\n        pareto_front_points.sort(key=lambda p: p['C_area'])\n\n        # Task 4: Compute the knee point\n        if len(pareto_front_points)  3:\n            # Special case: choose the point with the lowest cost\n            knee_point = pareto_front_points[0]\n            knee_curvature = 0.0\n        else:\n            # Normalize the Pareto front coordinates\n            C_vals = np.array([p['C_area'] for p in pareto_front_points])\n            E_vals = np.array([p['E_del'] for p in pareto_front_points])\n            \n            C_min, C_max = C_vals.min(), C_vals.max()\n            E_min, E_max = E_vals.min(), E_vals.max()\n            \n            C_range = C_max - C_min if (C_max  C_min) else 1.0\n            E_range = E_max - E_min if (E_max  E_min) else 1.0\n\n            normalized_front = np.array([\n                ((p['C_area'] - C_min) / C_range, (p['E_del'] - E_min) / E_range)\n                for p in pareto_front_points\n            ])\n\n            # Calculate curvature for interior points\n            curvatures = []\n            for i in range(1, len(normalized_front) - 1):\n                p_prev = normalized_front[i-1]\n                p_curr = normalized_front[i]\n                p_next = normalized_front[i+1]\n\n                d_prev_curr = np.linalg.norm(p_curr - p_prev)\n                d_curr_next = np.linalg.norm(p_next - p_curr)\n                d_next_prev = np.linalg.norm(p_prev - p_next)\n\n                # Shoelace formula for triangle area\n                area = 0.5 * abs(p_prev[0] * (p_curr[1] - p_next[1]) +\n                                   p_curr[0] * (p_next[1] - p_prev[1]) +\n                                   p_next[0] * (p_prev[1] - p_curr[1]))\n\n                denominator = d_prev_curr * d_curr_next * d_next_prev\n                if denominator  1e-9: # Avoid division by zero for collinear points\n                    curvature = 0.0\n                else:\n                    curvature = 2 * area / denominator\n                curvatures.append(curvature)\n\n            # Find point with max curvature\n            max_curvature_idx = np.argmax(curvatures)\n            knee_curvature = curvatures[max_curvature_idx]\n            # Index of knee point in pareto front is max_curvature_idx + 1\n            knee_point = pareto_front_points[max_curvature_idx + 1]\n\n        # Task 5: Report the results for the knee design\n        final_results.append([\n            int(knee_point['Lp']),\n            round(knee_point['ep'], 2),\n            round(knee_curvature, 3),\n            round(knee_point['E_del'], 1),\n            round(knee_point['C_area'], 2)\n        ])\n\n    # Format the final output string\n    result_str = ','.join([str(res) for res in final_results])\n    print(f\"[{result_str}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Optimal manufacturing doesn't happen in a vacuum; it is bound by physical, safety, and quality constraints. This exercise moves from exploring trade-offs to finding a single optimal operating point under strict limitations. You will apply the powerful Karush-Kuhn-Tucker (KKT) conditions to solve a constrained optimization problem and, crucially, interpret the resulting Lagrange multipliers as 'shadow prices,' revealing the economic value of relaxing each specific constraint.",
            "id": "3954480",
            "problem": "A cathode coating line for lithium-ion battery cell manufacturing is being optimized to minimize a surrogate levelized manufacturing cost per unit energy, denoted by $J(\\varepsilon,y)$ in dollars per kilowatt-hour. The decision variables are the electrode porosity $\\varepsilon$ (dimensionless) and the manufacturing yield $y$ (fraction of cells passing end-of-line tests, dimensionless). The surrogate cost function is defined as\n$$\nJ(\\varepsilon,y) \\;=\\; \\frac{A}{1-\\varepsilon} \\;-\\; B\\,\\varepsilon \\;+\\; C\\,(y - y_{0})^{2},\n$$\nwhere $A0$, $B0$, $C0$, and $y_{0}\\in(0,1)$ are parameters chosen to capture first-order techno-economic trade-offs: the term $\\frac{A}{1-\\varepsilon}$ penalizes loss of energy density as porosity increases, the linear term $-B\\,\\varepsilon$ reflects process and transport benefits of increasing porosity up to a limit, and the quadratic term $C\\,(y-y_{0})^{2}$ captures the cost of deviating from a nominal natural yield $y_{0}$ through additional inspection, rework, or process intensity.\n\nThe design must satisfy a maximum porosity constraint and a minimum yield constraint:\n$$\n\\varepsilon \\;\\le\\; \\varepsilon_{\\max}, \\qquad y \\;\\ge\\; y_{\\min},\n$$\ntogether with feasibility $0\\varepsilon1$ and $0y\\le 1$. Consider the parameter values\n$$\nA \\;=\\; 0.09,\\quad B \\;=\\; 0.36,\\quad C \\;=\\; 100,\\quad y_{0} \\;=\\; 0.80,\\quad \\varepsilon_{\\max} \\;=\\; 0.40,\\quad y_{\\min} \\;=\\; 0.90.\n$$\nFormulate the constrained optimization and compute the Lagrange multipliers associated with the porosity and yield constraints at the optimal solution, using the Karush-Kuhn-Tucker (KKT) conditions derived from first principles of constrained optimization. Interpret each multiplier as a shadow price, namely the marginal increase in optimal cost $J$ due to an infinitesimal tightening of its corresponding constraint.\n\nProvide your final answer as the ordered pair $\\big(\\lambda_{\\varepsilon},\\,\\mu_{y}\\big)$, where $\\lambda_{\\varepsilon}$ is the multiplier for the maximum porosity constraint and $\\mu_{y}$ is the multiplier for the minimum yield constraint. Express the numerical values in dollars per kilowatt-hour. No rounding is required; provide exact decimal values.",
            "solution": "The user wants me to solve a constrained optimization problem and find the Lagrange multipliers.\n\n### Step 1: Extract Givens\n\nThe problem statement provides the following information:\n-   **Objective Function**: A surrogate levelized manufacturing cost per unit energy, $J(\\varepsilon,y)$, in dollars per kilowatt-hour.\n    $$\n    J(\\varepsilon,y) \\;=\\; \\frac{A}{1-\\varepsilon} \\;-\\; B\\,\\varepsilon \\;+\\; C\\,(y - y_{0})^{2}\n    $$\n-   **Decision Variables**:\n    -   $\\varepsilon$: electrode porosity (dimensionless)\n    -   $y$: manufacturing yield (dimensionless)\n-   **Parameters**:\n    -   $A = 0.09$\n    -   $B = 0.36$\n    -   $C = 100$\n    -   $y_{0} = 0.80$\n-   **Constraints**:\n    -   Porosity constraint: $\\varepsilon \\le \\varepsilon_{\\max}$, where $\\varepsilon_{\\max} = 0.40$.\n    -   Yield constraint: $y \\ge y_{\\min}$, where $y_{\\min} = 0.90$.\n    -   Feasibility constraints: $0  \\varepsilon  1$ and $0  y \\le 1$.\n-   **Task**: Formulate the constrained optimization problem, compute the Lagrange multipliers $\\lambda_{\\varepsilon}$ (for the porosity constraint) and $\\mu_{y}$ (for the yield constraint) at the optimal solution using Karush-Kuhn-Tucker (KKT) conditions.\n\n### Step 2: Validate Using Extracted Givens\n\nThe problem is a standard nonlinear programming problem in engineering optimization.\n-   **Scientifically Grounded**: The objective function is a surrogate model, which is a common and valid technique in techno-economic analysis and system design. The terms are justified based on physical and economic trade-offs in battery manufacturing. There are no violations of fundamental scientific or mathematical principles.\n-   **Well-Posed**: The problem is to minimize a function over a compact, non-empty feasible set. The objective function is continuous and, as will be shown, convex on the feasible domain. The constraints are linear. This structure guarantees that a unique optimal solution exists.\n-   **Objective**: The problem is stated using precise mathematical language and objective, quantifiable parameters.\n-   **Completeness**: All necessary data ($A, B, C, y_0, \\varepsilon_{\\max}, y_{\\min}$) are provided.\n-   **Consistency**: The constraints and parameters are self-consistent. The feasible domain for $\\varepsilon$ is $(0, 0.40]$ and for $y$ is $[0.90, 1]$, which are valid subsets of their natural domains.\n\nThe problem is valid.\n\n### Step 3: Proceed to Solution\n\nWe begin by formally stating the optimization problem. We want to minimize the objective function $J(\\varepsilon, y)$ subject to the inequality constraints. It is standard practice to write all inequality constraints in the form $g_i(\\mathbf{x}) \\le 0$.\n\nThe optimization problem is:\n$$\n\\text{minimize} \\quad J(\\varepsilon, y) = \\frac{A}{1-\\varepsilon} - B\\varepsilon + C(y - y_0)^2\n$$\nsubject to:\n$$\ng_1(\\varepsilon, y) = \\varepsilon - \\varepsilon_{\\max} \\le 0 \\\\\ng_2(\\varepsilon, y) = y_{\\min} - y \\le 0\n$$\nand the implicit constraints $0  \\varepsilon  1$ and $0  y \\le 1$.\n\nThe Lagrangian function $\\mathcal{L}$ for this problem is formed by adding the objective function and the constraints multiplied by their respective non-negative Lagrange multipliers, $\\lambda_{\\varepsilon}$ and $\\mu_y$.\n$$\n\\mathcal{L}(\\varepsilon, y, \\lambda_{\\varepsilon}, \\mu_y) = J(\\varepsilon, y) + \\lambda_{\\varepsilon}g_1(\\varepsilon, y) + \\mu_y g_2(\\varepsilon, y)\n$$\n$$\n\\mathcal{L}(\\varepsilon, y, \\lambda_{\\varepsilon}, \\mu_y) = \\frac{A}{1-\\varepsilon} - B\\varepsilon + C(y - y_0)^2 + \\lambda_{\\varepsilon}(\\varepsilon - \\varepsilon_{\\max}) + \\mu_y(y_{\\min} - y)\n$$\nThe Karush-Kuhn-Tucker (KKT) conditions provide the necessary conditions for an optimal solution $(\\varepsilon^*, y^*)$. These conditions are:\n\n1.  **Stationarity**: The gradient of the Lagrangian with respect to the decision variables must be zero.\n    $$\n    \\frac{\\partial \\mathcal{L}}{\\partial \\varepsilon} = \\frac{A}{(1-\\varepsilon)^2} - B + \\lambda_{\\varepsilon} = 0\n    $$\n    $$\n    \\frac{\\partial \\mathcal{L}}{\\partial y} = 2C(y - y_0) - \\mu_y = 0\n    $$\n\n2.  **Primal Feasibility**: The solution must satisfy all constraints.\n    $$\n    \\varepsilon^* - \\varepsilon_{\\max} \\le 0\n    $$\n    $$\n    y_{\\min} - y^* \\le 0 \\quad (\\text{or } y^* \\ge y_{\\min})\n    $$\n\n3.  **Dual Feasibility**: The Lagrange multipliers must be non-negative.\n    $$\n    \\lambda_{\\varepsilon} \\ge 0\n    $$\n    $$\n    \\mu_y \\ge 0\n    $$\n\n4.  **Complementary Slackness**: The product of each multiplier and its corresponding constraint must be zero.\n    $$\n    \\lambda_{\\varepsilon}(\\varepsilon^* - \\varepsilon_{\\max}) = 0\n    $$\n    $$\n    \\mu_y(y_{\\min} - y^*) = 0\n    $$\n\nWe can solve this system by considering the separability of the objective function. The function $J(\\varepsilon, y)$ can be written as $J_{\\varepsilon}(\\varepsilon) + J_y(y)$, where $J_{\\varepsilon}(\\varepsilon) = \\frac{A}{1-\\varepsilon} - B\\varepsilon$ and $J_y(y) = C(y - y_0)^2$. We can optimize for $\\varepsilon$ and $y$ independently.\n\nFirst, let's find the unconstrained minimum for each variable.\nFor $\\varepsilon$, we set the derivative of $J_{\\varepsilon}$ to zero:\n$$\n\\frac{dJ_{\\varepsilon}}{d\\varepsilon} = \\frac{A}{(1-\\varepsilon)^2} - B = 0 \\implies (1-\\varepsilon)^2 = \\frac{A}{B} \\implies \\varepsilon = 1 - \\sqrt{\\frac{A}{B}}\n$$\nWe take the positive root for $1-\\varepsilon$ as $\\varepsilon  1$.\nSubstituting the values $A=0.09$ and $B=0.36$:\n$$\n\\varepsilon_{\\text{unc}} = 1 - \\sqrt{\\frac{0.09}{0.36}} = 1 - \\sqrt{\\frac{1}{4}} = 1 - \\frac{1}{2} = 0.5\n$$\nThe second derivative $\\frac{d^2 J_{\\varepsilon}}{d\\varepsilon^2} = \\frac{2A}{(1-\\varepsilon)^3}$ is positive for $\\varepsilon  1$, so $J_{\\varepsilon}$ is convex. The unconstrained minimum is at $\\varepsilon = 0.5$.\n\nFor $y$, we set the derivative of $J_y$ to zero:\n$$\n\\frac{dJ_y}{dy} = 2C(y - y_0) = 0 \\implies y = y_0\n$$\nSubstituting the value $y_0=0.80$:\n$$\ny_{\\text{unc}} = 0.80\n$$\nThe second derivative $\\frac{d^2 J_y}{dy^2} = 2C$ is positive ($200$), so $J_y$ is convex. The unconstrained minimum is at $y=0.80$.\n\nThe unconstrained optimal point is $(\\varepsilon, y) = (0.5, 0.8)$. We now check this point against the constraints:\n-   $\\varepsilon \\le \\varepsilon_{\\max} \\implies 0.5 \\le 0.40$, which is **false**.\n-   $y \\ge y_{\\min} \\implies 0.80 \\ge 0.90$, which is **false**.\n\nSince both unconstrained optima lie outside their respective feasible intervals, and both cost functions ($J_{\\varepsilon}$ and $J_y$) are convex, the optimal solution within the feasible region must lie at the boundary point closest to the unconstrained optimum.\n-   For $\\varepsilon$, the feasible region is $(0, \\varepsilon_{\\max}] = (0, 0.40]$. The unconstrained minimum is $\\varepsilon_{\\text{unc}} = 0.5$. The closest feasible point is $\\varepsilon^* = 0.40$.\n-   For $y$, the feasible region is $[y_{\\min}, 1] = [0.90, 1]$. The unconstrained minimum is $y_{\\text{unc}} = 0.80$. The closest feasible point is $y^* = 0.90$.\n\nThus, the optimal solution is $(\\varepsilon^*, y^*) = (0.40, 0.90)$.\nAt this point, both constraints are active:\n-   $\\varepsilon^* - \\varepsilon_{\\max} = 0.40 - 0.40 = 0$\n-   $y_{\\min} - y^* = 0.90 - 0.90 = 0$\n\nSince the constraints are active, the complementary slackness conditions do not force the multipliers to be zero. We can now solve for the multipliers using the stationarity conditions at the optimal point $(\\varepsilon^*, y^*) = (0.40, 0.90)$.\n\nFrom the first stationarity condition for $\\varepsilon$:\n$$\n\\lambda_{\\varepsilon} = B - \\frac{A}{(1-\\varepsilon^*)^2}\n$$\nSubstituting the values:\n$$\n\\lambda_{\\varepsilon} = 0.36 - \\frac{0.09}{(1-0.40)^2} = 0.36 - \\frac{0.09}{(0.60)^2} = 0.36 - \\frac{0.09}{0.36} = 0.36 - 0.25 = 0.11\n$$\nSince $\\lambda_{\\varepsilon} = 0.11  0$, the dual feasibility and complementary slackness conditions are satisfied for this constraint.\n\nFrom the second stationarity condition for $y$:\n$$\n\\mu_y = 2C(y^* - y_0)\n$$\nSubstituting the values:\n$$\n\\mu_y = 2(100)(0.90 - 0.80) = 200(0.10) = 20\n$$\nSince $\\mu_y = 20  0$, the dual feasibility and complementary slackness conditions are satisfied for this constraint.\n\nAll KKT conditions are satisfied for the solution $(\\varepsilon^*, y^*) = (0.40, 0.90)$ with multipliers $\\lambda_{\\varepsilon} = 0.11$ and $\\mu_y = 20$. As the problem is convex (minimizing a convex function over a convex set), this KKT point corresponds to the unique global minimum.\n\nThe Lagrange multipliers, interpreted as shadow prices, indicate the rate of change of the optimal objective function value with respect to a relaxation of the constraint. The problem asks for the marginal increase in cost due to a *tightening* of the constraint, which is given by the value of the multiplier itself.\n-   $\\lambda_{\\varepsilon} = 0.11$: A small decrease in $\\varepsilon_{\\max}$ (tightening) will increase the optimal cost $J^*$ at a rate of $0.11$ dollars per kWh per unit decrease in $\\varepsilon_{\\max}$.\n-   $\\mu_y = 20$: A small increase in $y_{\\min}$ (tightening) will increase the optimal cost $J^*$ at a rate of $20$ dollars per kWh per unit increase in $y_{\\min}$.\n\nThe requested final answer is the ordered pair $(\\lambda_{\\varepsilon}, \\mu_y)$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.11  20\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "The models we use for optimization are only as good as the parameters we feed them, which are often uncertain. This advanced practice simulates a cutting-edge scenario where you must design experiments to learn these unknown parameters on the fly, directly on the production line. You will implement a Bayesian experimental design policy that myopically balances the need for information gain with the practical costs of time and induced degradation, offering a glimpse into the future of smart, adaptive manufacturing.",
            "id": "3954452",
            "problem": "Consider a sequential Design of Experiments (DOE) problem for the initial formation of lithium-ion cells in manufacturing, where the goal is to adaptively select experiments to learn unknown degradation parameters while respecting production throughput. The degradation parameter vector is denoted by $\\boldsymbol{\\theta} \\in \\mathbb{R}^p$, and is modeled with a Gaussian prior $\\boldsymbol{\\theta} \\sim \\mathcal{N}(\\boldsymbol{\\theta}_0, \\boldsymbol{\\Sigma}_0)$. Each formation experiment is indexed by $e \\in \\{0,1,\\dots,m-1\\}$ and is characterized by a design vector $\\boldsymbol{x}_e \\in \\mathbb{R}^p$, a scalar measurement noise variance $\\sigma_e^2  0$, a duration $t_e$ in hours, and an expected incremental aging penalty $a_e$ measured in ampere-hour. Performing experiment $e$ produces a scalar measurement $y_e$ obeying a linear-Gaussian model $y_e = \\boldsymbol{x}_e^\\top \\boldsymbol{\\theta} + \\varepsilon_e$, where $\\varepsilon_e \\sim \\mathcal{N}(0, \\sigma_e^2)$ and is independent across experiments.\n\nYou must design an adaptive policy that selects up to $H$ experiments, allowing repetition, subject to a total time budget $T_{\\max}$ in hours, to maximize a myopic objective at each decision step that balances expected information gain about $\\boldsymbol{\\theta}$ with production throughput and aging cost. Start from the following fundamental base: Bayes’ rule for linear-Gaussian models, the definition of differential entropy for a multivariate Gaussian, and standard matrix identities for rank-$1$ updates. Do not assume or use any unprovided shortcut formulas for entropy reduction or posterior updates; instead, derive any needed expressions from the stated base. The expected information gain at each step must be quantified as the expected reduction in the differential entropy of the posterior distribution of $\\boldsymbol{\\theta}$ that would result from performing the selected experiment. The overall one-step objective at posterior covariance $\\boldsymbol{\\Sigma}$ and candidate experiment $e$ is a weighted sum of the expected information gain and explicit penalties proportional to $t_e$ and $a_e$, with strictly positive weights. After selecting an experiment, update the posterior distribution of $\\boldsymbol{\\theta}$ before the next decision. Selections are constrained by the remaining time budget; if no experiment fits the remaining time, the procedure must stop early. All time quantities must be handled in hours.\n\nImplement a program that, for each test case below, computes the chosen experiment index sequence (using zero-based indexing) produced by the described adaptive greedy policy until either $H$ selections have been made or the time budget is exhausted. Your program must output the sequences for all test cases aggregated in a single line as a comma-separated list enclosed in square brackets, where each element is the Python-style list of selected indices for that test case (for example, $[\\,[0,2],\\,[1],\\,[3,3]\\,]$). No physical quantity other than time in hours and aging penalty in ampere-hour appears in the output; the final output is purely the index sequences.\n\nTest suite:\n- Test case $1$ (general happy path):\n  - Dimension $p = 3$.\n  - Prior mean $\\boldsymbol{\\theta}_0 = [0, 0, 0]$.\n  - Prior covariance $\\boldsymbol{\\Sigma}_0 = \\mathrm{diag}([0.09, 0.04, 0.16])$.\n  - Experiments ($m = 5$):\n    - Experiment $0$: $\\boldsymbol{x}_0 = [1.0, 0.0, 0.5]$, $\\sigma_0^2 = 0.01$, $t_0 = 2.0$ hours, $a_0 = 0.020$ ampere-hour.\n    - Experiment $1$: $\\boldsymbol{x}_1 = [0.0, 1.0, 0.5]$, $\\sigma_1^2 = 0.02$, $t_1 = 1.0$ hours, $a_1 = 0.010$ ampere-hour.\n    - Experiment $2$: $\\boldsymbol{x}_2 = [0.5, 0.5, 0.5]$, $\\sigma_2^2 = 0.015$, $t_2 = 1.5$ hours, $a_2 = 0.015$ ampere-hour.\n    - Experiment $3$: $\\boldsymbol{x}_3 = [1.0, 0.0, 0.0]$, $\\sigma_3^2 = 0.005$, $t_3 = 2.5$ hours, $a_3 = 0.030$ ampere-hour.\n    - Experiment $4$: $\\boldsymbol{x}_4 = [0.0, 0.0, 1.0]$, $\\sigma_4^2 = 0.020$, $t_4 = 1.2$ hours, $a_4 = 0.012$ ampere-hour.\n  - Weights: information gain weight $\\alpha = 1.0$, time penalty weight $\\lambda = 0.1$, aging penalty weight $\\gamma = 1.0$.\n  - Time budget $T_{\\max} = 5.0$ hours, horizon $H = 4$.\n\n- Test case $2$ (boundary throughput requirement):\n  - Dimension $p = 3$.\n  - Prior mean $\\boldsymbol{\\theta}_0 = [0, 0, 0]$.\n  - Prior covariance $\\boldsymbol{\\Sigma}_0 = \\mathrm{diag}([0.09, 0.09, 0.09])$.\n  - Experiments ($m = 3$):\n    - Experiment $0$: $\\boldsymbol{x}_0 = [1.0, 1.0, 0.0]$, $\\sigma_0^2 = 0.010$, $t_0 = 3.0$ hours, $a_0 = 0.030$ ampere-hour.\n    - Experiment $1$: $\\boldsymbol{x}_1 = [0.0, 1.0, 1.0]$, $\\sigma_1^2 = 0.040$, $t_1 = 0.6$ hours, $a_1 = 0.008$ ampere-hour.\n    - Experiment $2$: $\\boldsymbol{x}_2 = [1.0, 0.0, 1.0]$, $\\sigma_2^2 = 0.020$, $t_2 = 1.0$ hours, $a_2 = 0.010$ ampere-hour.\n  - Weights: $\\alpha = 1.0$, $\\lambda = 1.0$, $\\gamma = 1.0$.\n  - Time budget $T_{\\max} = 1.2$ hours, horizon $H = 3$.\n\n- Test case $3$ (edge case: low noise long duration versus high noise short duration):\n  - Dimension $p = 2$.\n  - Prior mean $\\boldsymbol{\\theta}_0 = [0, 0]$.\n  - Prior covariance $\\boldsymbol{\\Sigma}_0 = \\mathrm{diag}([0.25, 0.01])$.\n  - Experiments ($m = 3$):\n    - Experiment $0$: $\\boldsymbol{x}_0 = [1.0, 0.0]$, $\\sigma_0^2 = 0.005$, $t_0 = 2.5$ hours, $a_0 = 0.020$ ampere-hour.\n    - Experiment $1$: $\\boldsymbol{x}_1 = [1.0, 0.0]$, $\\sigma_1^2 = 0.050$, $t_1 = 0.5$ hours, $a_1 = 0.005$ ampere-hour.\n    - Experiment $2$: $\\boldsymbol{x}_2 = [0.0, 1.0]$, $\\sigma_2^2 = 0.020$, $t_2 = 0.6$ hours, $a_2 = 0.006$ ampere-hour.\n  - Weights: $\\alpha = 1.0$, $\\lambda = 0.3$, $\\gamma = 0.5$.\n  - Time budget $T_{\\max} = 2.5$ hours, horizon $H = 3$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the Python-style list of selected experiment indices for the corresponding test case (for example, $[\\,[0,3,4],\\,[1,1],\\,[0]\\,]$). All angles are not applicable. No percentage signs are permitted anywhere in the output; any fraction must be represented as a decimal number, but the required output here contains only integer indices. Time quantities must be handled in hours in the algorithm, but the output contains only indices and has no units.",
            "solution": "The problem requires the design and implementation of an adaptive greedy policy for a sequential Design of Experiments (DOE) problem. The goal is to select a sequence of experiments to learn an unknown parameter vector $\\boldsymbol{\\theta}$ while balancing information gain against time and aging costs. The solution must be derived from fundamental principles as specified.\n\nThe problem is a classic application of Bayesian experimental design. At each step $k$, we have a posterior belief about the parameter vector $\\boldsymbol{\\theta}$, which is represented by a multivariate Gaussian distribution, $\\boldsymbol{\\theta} \\sim \\mathcal{N}(\\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)$. Initially, at step $k=0$, this is the prior $\\mathcal{N}(\\boldsymbol{\\theta}_0, \\boldsymbol{\\Sigma}_0)$. The task is to select the next experiment $e$ from a set of $m$ possible experiments to myopically maximize an objective function.\n\nFirst, we derive the necessary mathematical expressions from the fundamental base provided: Bayes' rule for linear-Gaussian models, the definition of differential entropy, and matrix identities for rank-1 updates.\n\n**1. Bayesian Posterior Update**\n\nLet the current posterior distribution be $p(\\boldsymbol{\\theta}) = \\mathcal{N}(\\boldsymbol{\\theta} | \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)$. When we perform experiment $e$, we obtain a measurement $y_e$. The likelihood of this measurement is given by the linear-Gaussian model $p(y_e|\\boldsymbol{\\theta}) = \\mathcal{N}(y_e | \\boldsymbol{x}_e^\\top \\boldsymbol{\\theta}, \\sigma_e^2)$.\n\nAccording to Bayes' rule, the new posterior distribution $p(\\boldsymbol{\\theta}|y_e)$ is proportional to the product of the likelihood and the prior (which is our current posterior):\n$$\np(\\boldsymbol{\\theta}|y_e) \\propto p(y_e|\\boldsymbol{\\theta}) p(\\boldsymbol{\\theta})\n$$\nThe logarithm of the posterior is therefore:\n$$\n\\ln p(\\boldsymbol{\\theta}|y_e) = \\ln p(y_e|\\boldsymbol{\\theta}) + \\ln p(\\boldsymbol{\\theta}) + \\text{constant}\n$$\nSubstituting the Gaussian probability density functions:\n$$\n\\ln p(\\boldsymbol{\\theta}|y_e) = -\\frac{1}{2\\sigma_e^2}(y_e - \\boldsymbol{x}_e^\\top \\boldsymbol{\\theta})^2 - \\frac{1}{2}(\\boldsymbol{\\theta} - \\boldsymbol{\\mu}_k)^\\top \\boldsymbol{\\Sigma}_k^{-1}(\\boldsymbol{\\theta} - \\boldsymbol{\\mu}_k) + \\text{constant}\n$$\nThis expression is a quadratic form in $\\boldsymbol{\\theta}$, which implies the posterior is also Gaussian, say $\\mathcal{N}(\\boldsymbol{\\theta} | \\boldsymbol{\\mu}_{k+1}, \\boldsymbol{\\Sigma}_{k+1})$. The inverse of the posterior covariance matrix, $\\boldsymbol{\\Sigma}_{k+1}^{-1}$, is the Hessian of the negative log-posterior with respect to $\\boldsymbol{\\theta}$. Expanding the quadratic terms involving $\\boldsymbol{\\theta}$:\n$$\n-\\ln p(\\boldsymbol{\\theta}|y_e) \\propto \\frac{1}{2\\sigma_e^2}(\\boldsymbol{\\theta}^\\top \\boldsymbol{x}_e \\boldsymbol{x}_e^\\top \\boldsymbol{\\theta}) + \\frac{1}{2}\\boldsymbol{\\theta}^\\top \\boldsymbol{\\Sigma}_k^{-1} \\boldsymbol{\\theta} - \\dots = \\frac{1}{2} \\boldsymbol{\\theta}^\\top \\left( \\boldsymbol{\\Sigma}_k^{-1} + \\frac{1}{\\sigma_e^2}\\boldsymbol{x}_e \\boldsymbol{x}_e^\\top \\right) \\boldsymbol{\\theta} - \\dots\n$$\nFrom this, we identify the inverse posterior covariance (or precision matrix):\n$$\n\\boldsymbol{\\Sigma}_{k+1}^{-1} = \\boldsymbol{\\Sigma}_k^{-1} + \\frac{1}{\\sigma_e^2}\\boldsymbol{x}_e \\boldsymbol{x}_e^\\top\n$$\nThis is a rank-1 update to the precision matrix. To find the posterior covariance $\\boldsymbol{\\Sigma}_{k+1}$, we invert this expression using the Sherman-Morrison-Woodbury formula for a rank-1 update, which states $(A + \\boldsymbol{u}\\boldsymbol{v}^\\top)^{-1} = A^{-1} - \\frac{A^{-1}\\boldsymbol{u}\\boldsymbol{v}^\\top A^{-1}}{1 + \\boldsymbol{v}^\\top A^{-1} \\boldsymbol{u}}$.\nLetting $A = \\boldsymbol{\\Sigma}_k^{-1}$, $\\boldsymbol{u} = \\frac{1}{\\sigma_e^2}\\boldsymbol{x}_e$, and $\\boldsymbol{v} = \\boldsymbol{x}_e$, we get:\n$$\n\\boldsymbol{\\Sigma}_{k+1} = \\left(\\boldsymbol{\\Sigma}_k^{-1} + \\frac{1}{\\sigma_e^2}\\boldsymbol{x}_e \\boldsymbol{x}_e^\\top\\right)^{-1} = \\boldsymbol{\\Sigma}_k - \\frac{\\boldsymbol{\\Sigma}_k \\left(\\frac{1}{\\sigma_e^2}\\boldsymbol{x}_e\\right) \\boldsymbol{x}_e^\\top \\boldsymbol{\\Sigma}_k}{1 + \\boldsymbol{x}_e^\\top \\boldsymbol{\\Sigma}_k \\left(\\frac{1}{\\sigma_e^2}\\boldsymbol{x}_e\\right)} = \\boldsymbol{\\Sigma}_k - \\frac{\\boldsymbol{\\Sigma}_k \\boldsymbol{x}_e \\boldsymbol{x}_e^\\top \\boldsymbol{\\Sigma}_k}{\\sigma_e^2 + \\boldsymbol{x}_e^\\top \\boldsymbol{\\Sigma}_k \\boldsymbol{x}_e}\n$$\nThis is the update rule for the posterior covariance. Notably, $\\boldsymbol{\\Sigma}_{k+1}$ depends only on $\\boldsymbol{\\Sigma}_k$ and the experiment parameters $(\\boldsymbol{x}_e, \\sigma_e^2)$, not on the specific measurement $y_e$.\n\n**2. Expected Information Gain (EIG)**\n\nThe information content of the distribution of $\\boldsymbol{\\theta}$ is quantified by its differential entropy. For a $p$-dimensional Gaussian $\\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$, the entropy is:\n$$\nh(\\boldsymbol{\\theta}) = \\frac{1}{2} \\ln \\det(2\\pi e \\boldsymbol{\\Sigma}) = \\frac{p}{2} \\ln(2\\pi e) + \\frac{1}{2} \\ln \\det(\\boldsymbol{\\Sigma})\n$$\nThe information gain from an experiment is the reduction in entropy, $IG = h(\\boldsymbol{\\theta}_{\\text{prior}}) - h(\\boldsymbol{\\theta}_{\\text{posterior}})$. Since the posterior depends on the random outcome $y_e$, we consider the *expected* information gain, $EIG = \\mathbb{E}_{y_e}[IG]$.\n$$\nEIG = h(\\boldsymbol{\\theta}_k) - \\mathbb{E}_{y_e}[h(\\boldsymbol{\\theta}_{k+1})]\n$$\nAs shown above, the posterior covariance $\\boldsymbol{\\Sigma}_{k+1}$ does not depend on $y_e$. Since entropy only depends on the covariance, $h(\\boldsymbol{\\theta}_{k+1})$ is also independent of $y_e$. Thus, the expectation is trivial: $\\mathbb{E}_{y_e}[h(\\boldsymbol{\\theta}_{k+1})] = h(\\boldsymbol{\\theta}_{k+1})$.\nThe EIG simplifies to a direct entropy difference:\n$$\nEIG = h(\\boldsymbol{\\theta}_k) - h(\\boldsymbol{\\theta}_{k+1}) = \\left(\\frac{p}{2}\\ln(2\\pi e) + \\frac{1}{2}\\ln\\det(\\boldsymbol{\\Sigma}_k)\\right) - \\left(\\frac{p}{2}\\ln(2\\pi e) + \\frac{1}{2}\\ln\\det(\\boldsymbol{\\Sigma}_{k+1})\\right) = \\frac{1}{2} \\ln\\left(\\frac{\\det(\\boldsymbol{\\Sigma}_k)}{\\det(\\boldsymbol{\\Sigma}_{k+1})}\\right)\n$$\nTo evaluate this, we need the ratio of determinants. We use the matrix determinant lemma: $\\det(A + \\boldsymbol{u}\\boldsymbol{v}^\\top) = (1 + \\boldsymbol{v}^\\top A^{-1} \\boldsymbol{u})\\det(A)$.\nUsing the precision matrix update $\\boldsymbol{\\Sigma}_{k+1}^{-1} = \\boldsymbol{\\Sigma}_k^{-1} + \\frac{1}{\\sigma_e^2}\\boldsymbol{x}_e \\boldsymbol{x}_e^\\top$:\n$$\n\\det(\\boldsymbol{\\Sigma}_{k+1}^{-1}) = \\det\\left(\\boldsymbol{\\Sigma}_k^{-1} + \\frac{1}{\\sigma_e^2}\\boldsymbol{x}_e \\boldsymbol{x}_e^\\top\\right) = \\left(1 + \\boldsymbol{x}_e^\\top (\\boldsymbol{\\Sigma}_k^{-1})^{-1} \\frac{\\boldsymbol{x}_e}{\\sigma_e^2}\\right) \\det(\\boldsymbol{\\Sigma}_k^{-1}) = \\left(1 + \\frac{\\boldsymbol{x}_e^\\top \\boldsymbol{\\Sigma}_k \\boldsymbol{x}_e}{\\sigma_e^2}\\right) \\det(\\boldsymbol{\\Sigma}_k^{-1})\n$$\nSince $\\det(A^{-1}) = 1/\\det(A)$, we have:\n$$\n\\frac{1}{\\det(\\boldsymbol{\\Sigma}_{k+1})} = \\left(1 + \\frac{\\boldsymbol{x}_e^\\top \\boldsymbol{\\Sigma}_k \\boldsymbol{x}_e}{\\sigma_e^2}\\right) \\frac{1}{\\det(\\boldsymbol{\\Sigma}_k)}\n$$\nRearranging gives the desired ratio:\n$$\n\\frac{\\det(\\boldsymbol{\\Sigma}_k)}{\\det(\\boldsymbol{\\Sigma}_{k+1})} = 1 + \\frac{\\boldsymbol{x}_e^\\top \\boldsymbol{\\Sigma}_k \\boldsymbol{x}_e}{\\sigma_e^2}\n$$\nSubstituting this into the EIG formula yields the final expression:\n$$\nEIG(e | \\boldsymbol{\\Sigma}_k) = \\frac{1}{2} \\ln\\left(1 + \\frac{\\boldsymbol{x}_e^\\top \\boldsymbol{\\Sigma}_k \\boldsymbol{x}_e}{\\sigma_e^2}\\right)\n$$\n\n**3. Myopic Objective Function and Greedy Algorithm**\n\nThe one-step objective for a candidate experiment $e$ is a weighted sum of the EIG and penalties for duration $t_e$ and aging $a_e$:\n$$\nJ(e | \\boldsymbol{\\Sigma}_k) = \\alpha \\cdot EIG(e | \\boldsymbol{\\Sigma}_k) - \\lambda t_e - \\gamma a_e\n$$\n$$\nJ(e | \\boldsymbol{\\Sigma}_k) = \\frac{\\alpha}{2} \\ln\\left(1 + \\frac{\\boldsymbol{x}_e^\\top \\boldsymbol{\\Sigma}_k \\boldsymbol{x}_e}{\\sigma_e^2}\\right) - \\lambda t_e - \\gamma a_e\n$$\nThe adaptive greedy policy proceeds as follows:\n\n1.  **Initialization**: Start with the prior covariance $\\boldsymbol{\\Sigma} = \\boldsymbol{\\Sigma}_0$, total time budget $T_{\\text{rem}} = T_{\\max}$, an empty sequence of chosen experiments $S = []$, and a selection count of $k=0$.\n2.  **Iteration**: Repeat for $k=0, 1, \\dots, H-1$:\n    a.  **Filter Candidates**: Identify the set of feasible experiments $C = \\{e \\mid t_e \\le T_{\\text{rem}}\\}$.\n    b.  **Check Termination**: If $C$ is empty, no experiment can be performed within the remaining time budget. Terminate the process.\n    c.  **Evaluate and Select**: For each experiment $e \\in C$, compute the objective $J(e | \\boldsymbol{\\Sigma})$. Select the experiment $e^*$ that maximizes this objective: $e^* = \\arg\\max_{e \\in C} J(e | \\boldsymbol{\\Sigma})$. In case of a tie, the experiment with the lower index is chosen.\n    d.  **Update State**:\n        i.   Append the chosen index to the result sequence: $S.append(e^*)$.\n        ii.  Decrement the time budget: $T_{\\text{rem}} \\leftarrow T_{\\text{rem}} - t_{e^*}$.\n        iii. Update the covariance matrix for the next step: $\\boldsymbol{\\Sigma} \\leftarrow \\boldsymbol{\\Sigma} - \\frac{\\boldsymbol{\\Sigma} \\boldsymbol{x}_{e^*} \\boldsymbol{x}_{e^*}^\\top \\boldsymbol{\\Sigma}}{\\sigma_{e^*}^2 + \\boldsymbol{x}_{e^*}^\\top \\boldsymbol{\\Sigma} \\boldsymbol{x}_{e^*}}$.\n3.  **Output**: After the loop terminates (either by reaching $H$ selections or running out of time), the final sequence $S$ is the result.\n\nThis procedure is implemented for each test case to generate the required output.",
            "answer": "```python\nimport numpy as np\n\ndef run_simulation(p, theta0, sigma0, experiments, alpha, lambda_, gamma, t_max, h):\n    \"\"\"\n    Runs the adaptive greedy experiment selection simulation for one test case.\n    \n    Args:\n        p (int): Dimension of the parameter vector.\n        theta0 (list): Prior mean vector.\n        sigma0 (list of lists): Prior covariance matrix.\n        experiments (list of dicts): List of available experiments.\n        alpha (float): Weight for information gain.\n        lambda_ (float): Weight for time penalty.\n        gamma (float): Weight for aging penalty.\n        t_max (float): Maximum total time budget.\n        h (int): Maximum number of experiments to select.\n\n    Returns:\n        list: The sequence of selected experiment indices.\n    \"\"\"\n    \n    # Initialize state\n    sigma_k = np.array(sigma0, dtype=float)\n    t_rem = float(t_max)\n    selected_indices = []\n\n    for _ in range(h):\n        candidate_experiments = []\n        for i, exp in enumerate(experiments):\n            if exp['t'] = t_rem:\n                candidate_experiments.append((i, exp))\n\n        if not candidate_experiments:\n            # No experiment fits within the remaining time budget\n            break\n\n        best_e_idx = -1\n        max_objective = -np.inf\n\n        for idx, exp in candidate_experiments:\n            x_e = np.array(exp['x'], dtype=float)\n            sigma_e_sq = exp['sigma_sq']\n            t_e = exp['t']\n            a_e = exp['a']\n            \n            # Calculate term x_e.T * Sigma_k * x_e\n            # This is a scalar\n            xt_sigma_x = x_e.T @ sigma_k @ x_e\n            \n            # Calculate Expected Information Gain (EIG)\n            eig = 0.5 * np.log(1 + xt_sigma_x / sigma_e_sq)\n            \n            # Calculate the myopic objective function J\n            objective = alpha * eig - lambda_ * t_e - gamma * a_e\n            \n            if objective  max_objective:\n                max_objective = objective\n                best_e_idx = idx\n\n        # Select the best experiment\n        best_experiment = experiments[best_e_idx]\n        selected_indices.append(best_e_idx)\n        \n        # Update state\n        t_rem -= best_experiment['t']\n        \n        # Update covariance matrix Sigma\n        x_best = np.array(best_experiment['x'], dtype=float)\n        sigma_best_sq = best_experiment['sigma_sq']\n        \n        # Denominator for the update rule\n        denominator = sigma_best_sq + x_best.T @ sigma_k @ x_best\n        \n        # Numerator for the update rule (a matrix)\n        # Using np.outer for clarity in rank-1 update\n        sigma_x = sigma_k @ x_best\n        numerator = np.outer(sigma_x, sigma_x.T)\n        \n        sigma_k -= numerator / denominator\n        \n    return selected_indices\n\ndef solve():\n    \"\"\"\n    Defines and runs all test cases, then prints the formatted result.\n    \"\"\"\n    test_cases = [\n        {\n            \"p\": 3,\n            \"theta0\": [0, 0, 0],\n            \"sigma0\": [[0.09, 0, 0], [0, 0.04, 0], [0, 0, 0.16]],\n            \"experiments\": [\n                {'x': [1.0, 0.0, 0.5], 'sigma_sq': 0.01, 't': 2.0, 'a': 0.020},\n                {'x': [0.0, 1.0, 0.5], 'sigma_sq': 0.02, 't': 1.0, 'a': 0.010},\n                {'x': [0.5, 0.5, 0.5], 'sigma_sq': 0.015, 't': 1.5, 'a': 0.015},\n                {'x': [1.0, 0.0, 0.0], 'sigma_sq': 0.005, 't': 2.5, 'a': 0.030},\n                {'x': [0.0, 0.0, 1.0], 'sigma_sq': 0.020, 't': 1.2, 'a': 0.012},\n            ],\n            \"alpha\": 1.0, \"lambda_\": 0.1, \"gamma\": 1.0,\n            \"t_max\": 5.0, \"h\": 4\n        },\n        {\n            \"p\": 3,\n            \"theta0\": [0, 0, 0],\n            \"sigma0\": [[0.09, 0, 0], [0, 0.09, 0], [0, 0, 0.09]],\n            \"experiments\": [\n                {'x': [1.0, 1.0, 0.0], 'sigma_sq': 0.010, 't': 3.0, 'a': 0.030},\n                {'x': [0.0, 1.0, 1.0], 'sigma_sq': 0.040, 't': 0.6, 'a': 0.008},\n                {'x': [1.0, 0.0, 1.0], 'sigma_sq': 0.020, 't': 1.0, 'a': 0.010},\n            ],\n            \"alpha\": 1.0, \"lambda_\": 1.0, \"gamma\": 1.0,\n            \"t_max\": 1.2, \"h\": 3\n        },\n        {\n            \"p\": 2,\n            \"theta0\": [0, 0],\n            \"sigma0\": [[0.25, 0], [0, 0.01]],\n            \"experiments\": [\n                {'x': [1.0, 0.0], 'sigma_sq': 0.005, 't': 2.5, 'a': 0.020},\n                {'x': [1.0, 0.0], 'sigma_sq': 0.050, 't': 0.5, 'a': 0.005},\n                {'x': [0.0, 1.0], 'sigma_sq': 0.020, 't': 0.6, 'a': 0.006},\n            ],\n            \"alpha\": 1.0, \"lambda_\": 0.3, \"gamma\": 0.5,\n            \"t_max\": 2.5, \"h\": 3\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        result_sequence = run_simulation(\n            case[\"p\"], case[\"theta0\"], case[\"sigma0\"], case[\"experiments\"],\n            case[\"alpha\"], case[\"lambda_\"], case[\"gamma\"], case[\"t_max\"], case[\"h\"]\n        )\n        all_results.append(result_sequence)\n\n    # Format the output string as a list of lists, with no spaces.\n    # Ex: [[0,2],[1],[3,3]]\n    string_results = [str(r).replace(\" \", \"\") for r in all_results]\n    print(f\"[{','.join(string_results)}]\")\n\nsolve()\n```"
        }
    ]
}