{
    "hands_on_practices": [
        {
            "introduction": "Multi-objective optimization is at the heart of battery design, forcing engineers to balance competing goals like energy density, cycle life, and cost. A powerful strategy for navigating these trade-offs is to combine multiple objectives into a single, aggregate objective function. This exercise challenges you to derive and apply the weighted Chebyshev scalarization, a method renowned for its ability to find all optimal trade-offs, even on complex, non-convex Pareto fronts where simpler linear weighting methods can fail.",
            "id": "3905756",
            "problem": "In automated battery design and simulation, multiple objective optimization (MOO) is used to explore trade-offs among competing performance metrics. Consider a simplified cell design vector $x = (\\ell_{s}, \\epsilon_{p})$, where $\\ell_{s}$ is the separator thickness and $\\epsilon_{p}$ is the electrode porosity. Suppose the following two dimensionless, normalized objectives are to be minimized over the feasible set $X = \\{ (\\ell_{s}, \\epsilon_{p}) \\,|\\, \\ell_{s} \\in [0.2, 1.0], \\epsilon_{p} \\in [0.3, 0.7] \\}$:\n$$\nf_{1}(x) = 0.5\\,\\ell_{s} + \\frac{0.1}{\\epsilon_{p}},\n\\qquad\nf_{2}(x) = \\frac{0.3}{\\ell_{s}} + 0.4\\,\\epsilon_{p}^{2}.\n$$\nThese proxies capture, respectively, normalized ohmic-heating propensity under a fixed high-rate pulse and normalized degradation tendency under reference cycling; lower values indicate better designs. Let the utopia point be defined component-wise by $z_{i}^{*} = \\inf_{x \\in X} f_{i}(x)$ for $i \\in \\{1,2\\}$. Let the designerâ€™s priority vector be $w = (w_{1}, w_{2})$ with $w_{1} > 0$, $w_{2} > 0$, and $w_{1} + w_{2} = 1$.\n\nStarting from the definitions of Pareto dominance and the utopia point, derive an aggregate scalar objective that encodes the worst-case weighted deviation of $f(x)$ from the utopia point across objectives, and express it in closed form. Using your derived scalarization, evaluate it for the weights $w_{1} = 0.6$, $w_{2} = 0.4$ at the candidate design $x = (\\ell_{s}, \\epsilon_{p}) = (0.6, 0.5)$. Round your final numerical result to four significant figures and express it as a dimensionless number.\n\nFinally, provide a concise, principle-based explanation of why this worst-case weighted-deviation scalarization can capture Pareto-optimal points on non-convex trade-off fronts that linear weighted-sum aggregation cannot, in the context of battery design trade-offs. Your explanation must be grounded in the geometry of objective-space level sets and the definition of Pareto optimality, without relying on empirical claims.",
            "solution": "The user wants me to solve a multi-objective optimization problem.\n\n## Problem Validation\n\n### Step 1: Extract Givens\n- **Design Vector**: $x = (\\ell_{s}, \\epsilon_{p})$, where $\\ell_{s}$ is separator thickness and $\\epsilon_{p}$ is electrode porosity.\n- **Feasible Set**: $X = \\{ (\\ell_{s}, \\epsilon_{p}) \\,|\\, \\ell_{s} \\in [0.2, 1.0], \\epsilon_{p} \\in [0.3, 0.7] \\}$.\n- **Objective Functions to be Minimized**:\n  - $f_{1}(x) = 0.5\\,\\ell_{s} + \\frac{0.1}{\\epsilon_{p}}$\n  - $f_{2}(x) = \\frac{0.3}{\\ell_{s}} + 0.4\\,\\epsilon_{p}^{2}$\n- **Utopia Point Definition**: $z_{i}^{*} = \\inf_{x \\in X} f_{i}(x)$ for $i \\in \\{1,2\\}$.\n- **Priority Vector**: $w = (w_{1}, w_{2})$ with $w_{1} > 0$, $w_{2} > 0$, and $w_{1} + w_{2} = 1$.\n- **Specific Evaluation Case**:\n  - Weights: $w_{1} = 0.6$, $w_{2} = 0.4$.\n  - Candidate design: $x = (0.6, 0.5)$.\n- **Tasks**:\n  1. Derive an aggregate scalar objective for the worst-case weighted deviation from the utopia point.\n  2. Evaluate this scalar objective for the given weights and design point, rounding to four significant figures.\n  3. Provide a principle-based explanation for why this scalarization can capture non-convex Pareto fronts.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is a mathematical optimization problem set in the context of battery design. The objective functions are simplified proxies, but their forms represent plausible trade-offs (e.g., ohmic heating versus degradation) and do not violate any scientific principles. The methods used (multi-objective optimization, Chebyshev scalarization) are standard in engineering and operations research.\n- **Well-Posed**: The feasible set $X$ is a compact (closed and bounded) subset of $\\mathbb{R}^{2}$. The objective functions $f_{1}$ and $f_{2}$ are continuous on $X$. By the Extreme Value Theorem, both functions attain their minima on $X$, ensuring the utopia point $z^{*}$ is well-defined. The tasks are specific and lead to a unique numerical and theoretical answer.\n- **Objective**: The problem is stated in precise mathematical language, free of subjective or ambiguous terminology.\n- **Completeness**: All necessary functions, domains, constants, and values are provided to complete the tasks.\n- **Consistency and Feasibility**: The constraints and definitions are internally consistent. The numerical ranges for the design variables are physically plausible for normalized parameters.\n\n### Step 3: Verdict and Action\nThe problem is valid as it is scientifically grounded, well-posed, objective, and self-contained. I will proceed with the solution.\n\n***\n\nThe solution proceeds in three parts as requested by the problem statement.\n\nFirst, we derive the aggregate scalar objective function. The problem asks for a scalarization that encodes the \"worst-case weighted deviation of $f(x)$ from the utopia point across objectives\". The deviation for objective $i$ is $f_{i}(x) - z_{i}^{*}$, where $z_{i}^{*}$ is the ideal minimum value for that objective. The weighted deviation is $w_{i}(f_{i}(x) - z_{i}^{*})$. The \"worst-case\" among these weighted deviations is their maximum. Therefore, the aggregate scalar objective function, which we denote as $S(x)$, is given by the weighted Chebyshev (or Tchebycheff) metric:\n$$\nS(x) = \\max_{i \\in \\{1,2\\}} \\{ w_{i} (f_{i}(x) - z_{i}^{*}) \\}\n$$\nThis is the required closed-form expression for the scalar objective.\n\nSecond, we evaluate this objective for the given parameters. This requires calculating the utopia point $z^{*} = (z_{1}^{*}, z_{2}^{*})$.\nThe utopia point components are the infima of the objectives over the feasible set $X$.\nFor $z_{1}^{*}$, we minimize $f_{1}(x)$:\n$$\nz_{1}^{*} = \\inf_{(\\ell_{s}, \\epsilon_{p}) \\in X} \\left( 0.5\\,\\ell_{s} + \\frac{0.1}{\\epsilon_{p}} \\right)\n$$\nThe function $f_{1}$ is a sum of two terms, each depending on a single independent variable. To minimize the sum, we minimize each term independently over its domain. The term $0.5\\,\\ell_{s}$ is minimized at the smallest possible value of $\\ell_{s}$, which is $\\ell_{s} = 0.2$. The term $\\frac{0.1}{\\epsilon_{p}}$ is minimized at the largest possible value of $\\epsilon_{p}$, which is $\\epsilon_{p} = 0.7$. Thus:\n$$\nz_{1}^{*} = 0.5(0.2) + \\frac{0.1}{0.7} = 0.1 + \\frac{1}{7} = \\frac{1}{10} + \\frac{1}{7} = \\frac{7+10}{70} = \\frac{17}{70}\n$$\nFor $z_{2}^{*}$, we minimize $f_{2}(x)$:\n$$\nz_{2}^{*} = \\inf_{(\\ell_{s}, \\epsilon_{p}) \\in X} \\left( \\frac{0.3}{\\ell_{s}} + 0.4\\,\\epsilon_{p}^{2} \\right)\n$$\nSimilarly, we minimize the terms independently. The term $\\frac{0.3}{\\ell_{s}}$ is minimized at the largest possible value of $\\ell_{s}$, which is $\\ell_{s} = 1.0$. The term $0.4\\,\\epsilon_{p}^{2}$ is minimized at the smallest possible value of $\\epsilon_{p}$, which is $\\epsilon_{p} = 0.3$. Thus:\n$$\nz_{2}^{*} = \\frac{0.3}{1.0} + 0.4(0.3)^{2} = 0.3 + 0.4(0.09) = 0.3 + 0.036 = 0.336\n$$\nThe utopia point is $z^{*} = (\\frac{17}{70}, 0.336)$.\n\nNow, we evaluate the objective functions at the candidate design point $x = (0.6, 0.5)$:\n$$\nf_{1}(0.6, 0.5) = 0.5(0.6) + \\frac{0.1}{0.5} = 0.3 + 0.2 = 0.5\n$$\n$$\nf_{2}(0.6, 0.5) = \\frac{0.3}{0.6} + 0.4(0.5)^{2} = 0.5 + 0.4(0.25) = 0.5 + 0.1 = 0.6\n$$\nWe use the given weights $w_{1} = 0.6$ and $w_{2} = 0.4$ to calculate the weighted deviations:\n$$\nw_{1} (f_{1}(x) - z_{1}^{*}) = 0.6 \\left( 0.5 - \\frac{17}{70} \\right) = 0.6 \\left( \\frac{35}{70} - \\frac{17}{70} \\right) = 0.6 \\left( \\frac{18}{70} \\right) = \\frac{10.8}{70} = \\frac{108}{700} = \\frac{27}{175}\n$$\n$$\nw_{2} (f_{2}(x) - z_{2}^{*}) = 0.4 (0.6 - 0.336) = 0.4 (0.264) = 0.1056\n$$\nNow we find the maximum of these two values to get $S(x)$:\n$$\nS(0.6, 0.5) = \\max \\left( \\frac{27}{175}, 0.1056 \\right)\n$$\nTo compare them, we convert the fraction to a decimal: $\\frac{27}{175} \\approx 0.1542857$.\nSince $0.1542857... > 0.1056$, the value of the scalar objective is $\\frac{27}{175}$.\nRounding to four significant figures, we get $0.1543$.\n\nThird, we provide the principle-based explanation.\nIn multi-objective optimization, the set of all non-dominated feasible points in the objective space forms the Pareto-optimal front. A common goal of design space exploration is to identify points on this front. The geometry of the scalarization method's level sets determines its ability to find all such points.\n\nA point $x^{*}$ is Pareto-optimal if there is no other feasible point $x$ such that $f_{i}(x) \\le f_{i}(x^{*})$ for all $i$ and $f_{j}(x) < f_{j}(x^{*})$ for at least one index $j$.\n\nThe linear weighted-sum aggregation method uses the scalar objective $S_{sum}(x) = \\sum_{i} w_{i}f_{i}(x)$. In the two-dimensional objective space $(f_{1}, f_{2})$, the level sets of this function ($S_{sum}(x) = \\text{constant}$) are straight lines with a slope of $-w_{1}/w_{2}$. Minimizing $S_{sum}$ is geometrically equivalent to moving this line until it is tangent to the feasible objective region. If the Pareto front is non-convex (i.e., it has regions that are concave as viewed from the origin), there exist points on the front that can never be found by such a tangency condition. These are known as unsupported Pareto-optimal points.\n\nThe worst-case weighted-deviation scalarization (weighted Chebyshev method) uses $S_{Cheby}(x) = \\max_{i} \\{w_{i}(f_{i}(x) - z_{i}^{*})\\}$. A level set $S_{Cheby}(x) = \\alpha$ (where $\\alpha > 0$ is a constant) corresponds to the boundary of the region defined by $w_{i}(f_{i}(x) - z_{i}^{*}) \\le \\alpha$ for all $i$, or $f_{i}(x) \\le z_{i}^{*} + \\alpha/w_{i}$. In the $(f_{1}, f_{2})$ objective space, this region is a rectangle anchored at the utopia point $z^{*}=(z_1^*, z_2^*)$ and extending to $(z_1^* + \\alpha/w_1, z_2^* + \\alpha/w_2)$. Minimizing $S_{Cheby}(x)$ is equivalent to finding the smallest such rectangle (i.e., smallest $\\alpha$) that intersects the feasible objective region. The first point of contact between this expanding rectangular level set and the feasible region is the solution. Due to the sharp, right-angled corner of the rectangle, it can \"probe\" into and make contact with points within non-convex \"dents\" of the Pareto front. By varying the weight vector $w$, the aspect ratio of the rectangle changes, allowing its corner to touch any point on the Pareto front, whether it is convex or non-convex. Therefore, the weighted Chebyshev method is guaranteed to be able to find any Pareto-optimal point, making it a more robust method for exploring design trade-offs, especially when the shape of the Pareto front is unknown, as is typical in automated battery design.",
            "answer": "$$\n\\boxed{0.1543}\n$$"
        },
        {
            "introduction": "After an optimization algorithm identifies a set of non-dominated designs, how do we objectively measure the quality of this entire set, known as the Pareto front? The Hypervolume (HV) indicator provides a comprehensive, single-value metric for this purpose, representing the volume of objective space dominated by the solution set. In this advanced practice, you will not only define the HV indicator but also derive its sensitivity to the reference point, a crucial calculation for many advanced DSE algorithms that use the HV to guide their search.",
            "id": "3905771",
            "problem": "An automated battery design platform is performing multi-objective design space exploration over three performance metrics: energy density $E$ (to be maximized, in $\\mathrm{Wh/kg}$), cycle life $L$ (to be maximized, in cycles), and cost per energy $C$ (to be minimized, in $\\$/\\mathrm{kWh}$). To convert this into a canonical maximization setting, define the third objective as $f_{3} = -C$ so that larger $f_{3}$ values correspond to lower cost.\n\nYou are given three non-dominated candidate designs with objective vectors\n$$\n\\mathbf{y}^{A} = (E^{A}, L^{A}, f_{3}^{A}) = (250, 1500, -90),\\quad \\mathbf{y}^{B} = (220, 2000, -110),\\quad \\mathbf{y}^{C} = (280, 1200, -100),\n$$\nand a strictly dominated reference point\n$$\n\\mathbf{R} = (R_{1}, R_{2}, R_{3}) = (200, 1000, -150).\n$$\n\nStarting only from:\n- the definition of Pareto dominance for maximization (a point $\\mathbf{y}$ dominates $\\mathbf{x}$ if $y_{i} \\ge x_{i}$ for all $i$ and $y_{j} > x_{j}$ for at least one $j$),\n- and the definition of the Lebesgue measure in $\\mathbb{R}^{n}$,\n\n(1) Formally define the Hypervolume (HV) indicator for the three-objective maximization problem with respect to the reference point $\\mathbf{R}$ as the three-dimensional Lebesgue measure of the region weakly dominated by the non-dominated set $\\{\\mathbf{y}^{A}, \\mathbf{y}^{B}, \\mathbf{y}^{C}\\}$ and bounded by $\\mathbf{R}$.\n\n(2) Derive, from first principles via geometric reasoning and measure-theoretic differentiation, the sensitivity of the hypervolume with respect to the reference point coordinates, i.e., the gradient $\\nabla_{\\mathbf{R}}\\mathrm{HV}(\\mathbf{R}) = \\left( \\frac{\\partial \\mathrm{HV}}{\\partial R_{1}}, \\frac{\\partial \\mathrm{HV}}{\\partial R_{2}}, \\frac{\\partial \\mathrm{HV}}{\\partial R_{3}} \\right)$ evaluated at the given $\\mathbf{R}$.\n\nExpress each component of the sensitivity in units of hypervolume per unit change of the corresponding axis as follows:\n- $\\frac{\\partial \\mathrm{HV}}{\\partial R_{1}}$ in $(\\mathrm{Wh/kg}) \\cdot \\mathrm{cycles} \\cdot (\\$/\\mathrm{kWh})$ per $\\mathrm{Wh/kg}$,\n- $\\frac{\\partial \\mathrm{HV}}{\\partial R_{2}}$ in $(\\mathrm{Wh/kg}) \\cdot \\mathrm{cycles} \\cdot (\\$/\\mathrm{kWh})$ per cycle,\n- $\\frac{\\partial \\mathrm{HV}}{\\partial R_{3}}$ in $(\\mathrm{Wh/kg}) \\cdot \\mathrm{cycles} \\cdot (\\$/\\mathrm{kWh})$ per $\\$/\\mathrm{kWh}$.\n\nReport the final sensitivity vector numerically. Do not include units in your final boxed answer. No rounding is required; present exact integers if they arise naturally from the derivation.",
            "solution": "The problem is assessed to be valid. It is scientifically grounded in the established theory of multi-objective optimization and real analysis, is well-posed with a unique and derivable solution, is stated objectively, and provides all necessary information for its resolution.\n\nThe problem asks for two tasks: ($1$) to formally define the Hypervolume (HV) indicator and ($2$) to derive its sensitivity with respect to the reference point coordinates.\n\n### Part (1): Formal Definition of the Hypervolume (HV) Indicator\n\nLet $S = \\{\\mathbf{y}^{A}, \\mathbf{y}^{B}, \\mathbf{y}^{C}\\} \\subset \\mathbb{R}^3$ be the given set of three non-dominated objective vectors. Let $\\mathbf{R} \\in \\mathbb{R}^3$ be the given reference point, which is dominated by all points in $S$. The objectives are to be maximized.\n\nA point $\\mathbf{p} \\in \\mathbb{R}^3$ is weakly dominated by a point $\\mathbf{y} \\in S$ if $y_i \\ge p_i$ for all $i \\in \\{1, 2, 3\\}$. The region weakly dominated by the set $S$ is the union of the regions weakly dominated by each point in $S$.\n\nThe hypervolume indicator, $\\mathrm{HV}(S, \\mathbf{R})$, is defined as the Lebesgue measure of the set of points in the objective space that are weakly dominated by at least one point in $S$ and that dominate the reference point $\\mathbf{R}$. Let this region be denoted by $H(S, \\mathbf{R})$. Formally, this set is defined as:\n$$\nH(S, \\mathbf{R}) = \\left\\{ \\mathbf{p} \\in \\mathbb{R}^3 \\mid \\exists \\mathbf{y} \\in S \\text{ such that } y_i \\ge p_i \\ge R_i \\text{ for } i = 1, 2, 3 \\right\\}\n$$\nThis can be expressed as the union of three hyperrectangles (cuboids), one for each point in $S$:\n$$\nH(S, \\mathbf{R}) = \\bigcup_{\\mathbf{y} \\in S} [\\![ \\mathbf{R}, \\mathbf{y} ]\\!]\n$$\nwhere $[\\![ \\mathbf{R}, \\mathbf{y} ]\\!] = [R_1, y_1] \\times [R_2, y_2] \\times [R_3, y_3]$ is the hyperrectangle defined by $\\mathbf{R}$ and $\\mathbf{y}$ as opposite corners.\n\nThe hypervolume is the $3$-dimensional Lebesgue measure, $\\lambda_3$, of this set:\n$$\n\\mathrm{HV}(S, \\mathbf{R}) = \\lambda_3(H(S, \\mathbf{R})) = \\int_{\\mathbb{R}^3} \\mathbf{1}_{H(S, \\mathbf{R})}(\\mathbf{p}) \\, d\\mathbf{p}\n$$\nwhere $\\mathbf{1}_{H(S, \\mathbf{R})}$ is the indicator function for the set $H(S, \\mathbf{R})$.\n\n### Part (2): Derivation of the Sensitivity $\\nabla_{\\mathbf{R}}\\mathrm{HV}(\\mathbf{R})$\n\nThe sensitivity of the hypervolume with respect to the reference point is the gradient vector $\\nabla_{\\mathbf{R}}\\mathrm{HV}(\\mathbf{R}) = \\left( \\frac{\\partial \\mathrm{HV}}{\\partial R_{1}}, \\frac{\\partial \\mathrm{HV}}{\\partial R_{2}}, \\frac{\\partial \\mathrm{HV}}{\\partial R_{3}} \\right)$. We can express the hypervolume as an integral with limits dependent on $\\mathbf{R}$:\n$$\n\\mathrm{HV}(S, \\mathbf{R}) = \\int_{R_1}^{\\max_{\\mathbf{y} \\in S} y_1} \\int_{R_2}^{\\max_{\\mathbf{y} \\in S} y_2} \\int_{R_3}^{\\max_{\\mathbf{y} \\in S} y_3} \\mathbf{1}_{\\bigcup_{\\mathbf{y} \\in S} [\\![ \\mathbf{y}', \\mathbf{y} ]\\!]}(\\mathbf{p}) \\, dp_3 \\, dp_2 \\, dp_1\n$$\nwhere $\\mathbf{y}'$ is a point whose coordinates are the maximum values from $S$ for each objective. A more direct representation is:\n$$\n\\mathrm{HV}(S, \\mathbf{R}) = \\int_{R_1}^{\\infty} \\int_{R_2}^{\\infty} \\int_{R_3}^{\\infty} \\mathbf{1}_{\\mathcal{U}(S)}(\\mathbf{p}) \\, dp_3 \\, dp_2 \\, dp_1\n$$\nwhere $\\mathcal{U}(S)$ is the region weakly dominated by the set $S$.\n\nUsing the Leibniz integral rule (a consequence of the Fundamental Theorem of Calculus) for differentiation under the integral sign, the partial derivative with respect to a lower limit of integration is the negative of the integrand evaluated at that limit. For the first component, $R_1$:\n$$\n\\frac{\\partial \\mathrm{HV}}{\\partial R_1} = \\frac{\\partial}{\\partial R_1} \\int_{R_1}^{\\infty} \\left( \\int_{R_2}^{\\infty} \\int_{R_3}^{\\infty} \\mathbf{1}_{\\mathcal{U}(S)}(p_1, p_2, p_3) \\, dp_3 \\, dp_2 \\right) \\, dp_1 = - \\int_{R_2}^{\\infty} \\int_{R_3}^{\\infty} \\mathbf{1}_{\\mathcal{U}(S)}(R_1, p_2, p_3) \\, dp_3 \\, dp_2\n$$\nThis integral represents the $2$-dimensional Lebesgue measure (area) of the slice of the hypervolume region at the plane $p_1 = R_1$. Let's denote this area as $A_1$. Thus, $\\frac{\\partial \\mathrm{HV}}{\\partial R_1} = -A_1$. Similarly, $\\frac{\\partial \\mathrm{HV}}{\\partial R_2} = -A_2$ and $\\frac{\\partial \\mathrm{HV}}{\\partial R_3} = -A_3$, where $A_2$ and $A_3$ are the areas of the corresponding slices at $p_2=R_2$ and $p_3=R_3$.\n\nThe given data are:\n$\\mathbf{y}^{A} = (250, 1500, -90)$, $\\mathbf{y}^{B} = (220, 2000, -110)$, $\\mathbf{y}^{C} = (280, 1200, -100)$, and $\\mathbf{R} = (200, 1000, -150)$.\nAll points in $S$ have coordinates greater than or equal to the corresponding coordinates of $\\mathbf{R}$, so for each calculation, the set of relevant points from $S$ is the entire set $S$.\n\n**Calculation of $\\frac{\\partial \\mathrm{HV}}{\\partial R_1}$:**\nWe need to calculate the area $A_1$, which is the area of the union of rectangles in the $(p_2, p_3)$-plane. The rectangles are defined by $[R_2, y_i] \\times [R_3, y_i]$ for each point $\\mathbf{y}^i$'s projection onto the $(p_2, p_3)$ plane. The projected points are $(y_2, y_3)$: $P_A = (1500, -90)$, $P_B = (2000, -110)$, $P_C = (1200, -100)$. The reference corner is $(R_2, R_3) = (1000, -150)$. We slice the region by $p_2$ coordinates: $1000, 1200, 1500, 2000$.\n- For $p_2 \\in [1000, 1200]$ (width $200$), the upper bound for $p_3$ is $\\max(-90, -110, -100) = -90$. The height is $-90 - (-150) = 60$. Area = $200 \\times 60 = 12000$.\n- For $p_2 \\in (1200, 1500]$ (width $300$), point $C$ is passed. Upper bound is $\\max(-90, -110) = -90$. Height is $-90 - (-150) = 60$. Area = $300 \\times 60 = 18000$.\n- For $p_2 \\in (1500, 2000]$ (width $500$), point $A$ is passed. Upper bound is $-110$. Height is $-110 - (-150) = 40$. Area = $500 \\times 40 = 20000$.\n$A_1 = 12000 + 18000 + 20000 = 50000$.\nThus, $\\frac{\\partial \\mathrm{HV}}{\\partial R_1} = -50000$. The units are $(\\mathrm{cycles}) \\cdot (\\$/\\mathrm{kWh})$.\n\n**Calculation of $\\frac{\\partial \\mathrm{HV}}{\\partial R_2}$:**\nWe calculate area $A_2$ from projections on the $(p_1, p_3)$-plane: $P_A = (250, -90)$, $P_B = (220, -110)$, $P_C = (280, -100)$, with reference corner $(R_1, R_3) = (200, -150)$. We slice by $p_1$ coordinates: $200, 220, 250, 280$.\n- For $p_1 \\in [200, 220]$ (width $20$), upper bound for $p_3$ is $\\max(-90, -110, -100) = -90$. Height is $-90 - (-150) = 60$. Area = $20 \\times 60 = 1200$.\n- For $p_1 \\in (220, 250]$ (width $30$), point $B$ is passed. Upper bound is $\\max(-90, -100) = -90$. Height is $-90 - (-150) = 60$. Area = $30 \\times 60 = 1800$.\n- For $p_1 \\in (250, 280]$ (width $30$), point $A$ is passed. Upper bound is $-100$. Height is $-100 - (-150) = 50$. Area = $30 \\times 50 = 1500$.\n$A_2 = 1200 + 1800 + 1500 = 4500$.\nThus, $\\frac{\\partial \\mathrm{HV}}{\\partial R_2} = -4500$. The units are $(\\mathrm{Wh/kg}) \\cdot (\\$/\\mathrm{kWh})$.\n\n**Calculation of $\\frac{\\partial \\mathrm{HV}}{\\partial R_3}$:**\nWe calculate area $A_3$ from projections on the $(p_1, p_2)$-plane: $P_A = (250, 1500)$, $P_B = (220, 2000)$, $P_C = (280, 1200)$, with reference corner $(R_1, R_2) = (200, 1000)$. We slice by $p_1$ coordinates: $200, 220, 250, 280$.\n- For $p_1 \\in [200, 220]$ (width $20$), upper bound for $p_2$ is $\\max(1500, 2000, 1200) = 2000$. Height is $2000 - 1000 = 1000$. Area = $20 \\times 1000 = 20000$.\n- For $p_1 \\in (220, 250]$ (width $30$), point $B$ is passed. Upper bound is $\\max(1500, 1200) = 1500$. Height is $1500 - 1000 = 500$. Area = $30 \\times 500 = 15000$.\n- For $p_1 \\in (250, 280]$ (width $30$), point $A$ is passed. Upper bound is $1200$. Height is $1200 - 1000 = 200$. Area = $30 \\times 200 = 6000$.\n$A_3 = 20000 + 15000 + 6000 = 41000$.\nThus, $\\frac{\\partial \\mathrm{HV}}{\\partial R_3} = -41000$. The units are $(\\mathrm{Wh/kg}) \\cdot (\\mathrm{cycles})$.\n\nThe final sensitivity vector is the gradient:\n$$\n\\nabla_{\\mathbf{R}}\\mathrm{HV}(\\mathbf{R}) = \\left( -50000, -4500, -41000 \\right).\n$$",
            "answer": "$$\n\\boxed{\\begin{pmatrix} -50000 & -4500 & -41000 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Modern Design Space Exploration is often impossible without surrogate models, which provide fast approximations of expensive simulations. The success of this approach hinges on the surrogate's accuracy and our ability to improve it efficiently. This exercise delves into the core of surrogate model validation, guiding you to derive the expressions for leave-one-out cross-validation error for a Gaussian Process model, a technique that allows for efficient model assessment and guides adaptive sampling strategies.",
            "id": "3905833",
            "problem": "In the automated design and simulation of advanced lithium-ion cells, a common design variable is electrode thickness, denoted by $t$, and a critical quantity of interest is a plating risk metric at the end of a specified fast-charge protocol, denoted by $r(t)$. Suppose that a Gaussian Process (GP) surrogate is used to model $r(t)$ over a one-dimensional design space of thickness $t \\in \\mathbb{R}$ based on $n$ simulated data pairs $\\mathcal{D} = \\{(t_i, y_i)\\}_{i=1}^n$, where $y_i$ is a noisy realization of $r(t_i)$ with additive independent Gaussian noise of variance $\\sigma_n^2$. Assume a zero-mean GP prior with covariance function $k_{\\boldsymbol{\\theta}}(t,t')$ that is strictly positive definite, and let $K \\in \\mathbb{R}^{n \\times n}$ denote the covariance matrix with entries $K_{ij} = k_{\\boldsymbol{\\theta}}(t_i, t_j)$. Define $A = K + \\sigma_n^2 I$, where $I$ is the identity matrix, and let $\\mathbf{y} = [y_1,\\dots,y_n]^{\\top}$. Assume that the GP hyperparameters $\\boldsymbol{\\theta}$ and the observation noise variance $\\sigma_n^2$ are fixed and known.\n\nStarting only from the following fundamental bases:\n- the property that any finite collection of GP values is jointly Gaussian;\n- the standard multivariate normal conditioning identities; and\n- elementary block-matrix inversion via the Schur complement,\n\ndo the following.\n\n1) Derive the leave-one-out predictive mean $\\mu_{-i}(t_i)$ and predictive variance $\\sigma_{-i}^2(t_i)$ for the $i$-th training input when the model is trained on the dataset with the $i$-th pair removed, in terms of $A^{-1}$ and $\\mathbf{y}$. Make no assumptions on the specific form of $k_{\\boldsymbol{\\theta}}(\\cdot,\\cdot)$ beyond positive definiteness.\n\n2) From your derivation, obtain a compact expression for the leave-one-out residual at $t_i$, defined as $e^{\\mathrm{loo}}_i = y_i - \\mu_{-i}(t_i)$, expressed entirely using the vector $\\boldsymbol{\\alpha} = A^{-1} \\mathbf{y}$ and the diagonal entries of $A^{-1}$.\n\n3) Define the leave-one-out root-mean-square error over the $n$ training inputs as $\\mathrm{RMSE}_{\\mathrm{loo}} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n \\left(e^{\\mathrm{loo}}_i\\right)^2}$. Provide a single closed-form analytic expression for $\\mathrm{RMSE}_{\\mathrm{loo}}$ purely in terms of $A^{-1}$ and $\\mathbf{y}$, eliminating $\\mu_{-i}(t_i)$ and $\\sigma_{-i}^2(t_i)$.\n\n4) Briefly justify, using first principles of Bayesian uncertainty quantification and design space exploration, how the magnitude of $e^{\\mathrm{loo}}_i$ and $\\sigma_{-i}(t_i)$ can be used to adaptively refine sampling near thickness regions where $r(t)$ varies rapidly with $t$. Your justification should connect the gradient magnitude $|\\partial \\mu(t)/\\partial t|$ and local predictive uncertainty to the decision of where to add new simulation points, without invoking any acquisition function beyond what can be derived from the stated bases.\n\nYour final answer must be the single closed-form analytic expression requested in item $3)$. No numerical evaluation is required, and no units are needed. If you choose to simplify notation, define it explicitly. The final answer must be a single analytic expression, not an inequality or an equation with free verbal qualifiers.",
            "solution": "The problem statement has been critically validated and is deemed to be scientifically grounded, well-posed, objective, and internally consistent. It presents a standard but rigorous exercise in the statistical theory of Gaussian Processes (GPs), with a direct and relevant application to automated engineering design. All necessary definitions and constraints are provided, and the problem is solvable using the specified fundamental bases. We may therefore proceed with a full derivation.\n\nThe problem is addressed in four parts as requested. Our analysis will rely on the properties of multivariate normal distributions and block matrix inversion. Let $A = K + \\sigma_n^2 I$, where $K$ is the $n \\times n$ covariance matrix with entries $K_{ij} = k_{\\boldsymbol{\\theta}}(t_i, t_j)$. Since the kernel $k_{\\boldsymbol{\\theta}}(\\cdot, \\cdot)$ is strictly positive definite, $K$ is a positive definite matrix. Given that the noise variance $\\sigma_n^2$ must be non-negative (and is implicitly positive for a realistic noisy system), the matrix $A$ is the sum of a positive definite matrix and a positive semi-definite matrix ($ \\sigma_n^2 I $), which makes $A$ itself strictly positive definite and therefore invertible.\n\n**1) Derivation of Leave-One-Out (LOO) Predictive Mean and Variance**\n\nThe goal is to find the predictive distribution for the latent function value $r(t_i)$ using the training data with the $i$-th point removed, denoted $\\mathcal{D}_{-i} = \\mathcal{D} \\setminus \\{(t_i, y_i)\\}$. The vector of training inputs with $t_i$ removed is $\\mathbf{t}_{-i}$, and the corresponding outputs are $\\mathbf{y}_{-i}$.\n\nAccording to the definition of a Gaussian Process, any finite collection of function values is jointly Gaussian. Considering the noisy observations $\\mathbf{y}$, their joint distribution is given by $p(\\mathbf{y}) = \\mathcal{N}(\\mathbf{y}|\\mathbf{0}, A)$. We can partition this distribution with respect to the $i$-th element, $y_i$, and the remaining elements, $\\mathbf{y}_{-i}$. Let's permute the elements for clarity, though it's not strictly necessary. The joint distribution of $(y_i, \\mathbf{y}_{-i}^\\top)^\\top$ is:\n$$\n\\begin{pmatrix} y_i \\\\ \\mathbf{y}_{-i} \\end{pmatrix} \\sim \\mathcal{N} \\left( \\begin{pmatrix} 0 \\\\ \\mathbf{0} \\end{pmatrix}, \\begin{pmatrix} a_{ii} & \\mathbf{a}_{i,-i} \\\\ \\mathbf{a}_{-i,i} & A_{-i,-i} \\end{pmatrix} \\right)\n$$\nwhere $a_{ii} = K_{ii} + \\sigma_n^2$, $\\mathbf{a}_{i,-i} = [K_{i1}, \\dots, K_{i,i-1}, K_{i,i+1}, \\dots, K_{in}]$, and $A_{-i,-i}$ is the matrix $A$ with the $i$-th row and column removed. Note that $\\mathbf{a}_{-i,i} = \\mathbf{a}_{i,-i}^\\top$. The matrix $A_{-i,-i}$ is equivalent to $K_{-i,-i} + \\sigma_n^2 I_{n-1}$, where $K_{-i,-i}$ is the covariance matrix for inputs $\\mathbf{t}_{-i}$.\n\nUsing the standard conditioning rules for a multivariate normal distribution, the conditional distribution $p(y_i|\\mathbf{y}_{-i})$ is a Gaussian with mean and variance:\n$$\n\\mathbb{E}[y_i|\\mathbf{y}_{-i}] = \\mathbf{a}_{i,-i} (A_{-i,-i})^{-1} \\mathbf{y}_{-i}\n$$\n$$\n\\mathrm{Var}[y_i|\\mathbf{y}_{-i}] = a_{ii} - \\mathbf{a}_{i,-i} (A_{-i,-i})^{-1} \\mathbf{a}_{-i,i}\n$$\nThe leave-one-out predictive mean $\\mu_{-i}(t_i)$ for the latent function value $r(t_i)$ is calculated by conditioning on the noisy data $\\mathbf{y}_{-i}$. The joint distribution of $(r(t_i), \\mathbf{y}_{-i}^\\top)^\\top$ is:\n$$\n\\begin{pmatrix} r(t_i) \\\\ \\mathbf{y}_{-i} \\end{pmatrix} \\sim \\mathcal{N} \\left( \\begin{pmatrix} 0 \\\\ \\mathbf{0} \\end{pmatrix}, \\begin{pmatrix} K_{ii} & \\mathbf{k}_{i,-i} \\\\ \\mathbf{k}_{-i,i} & K_{-i,-i} + \\sigma_n^2 I_{n-1} \\end{pmatrix} \\right)\n$$\nwhere $\\mathbf{k}_{i,-i}$ is the row vector of covariances $[k(t_i, t_j)]_{j \\neq i}$, which is identical to $\\mathbf{a}_{i,-i}$. Conditioning $r(t_i)$ on $\\mathbf{y}_{-i}$ gives the LOO predictive mean:\n$$\n\\mu_{-i}(t_i) = \\mathbb{E}[r(t_i)|\\mathbf{y}_{-i}] = \\mathbf{k}_{i,-i} (K_{-i,-i} + \\sigma_n^2 I_{n-1})^{-1} \\mathbf{y}_{-i} = \\mathbf{a}_{i,-i} (A_{-i,-i})^{-1} \\mathbf{y}_{-i}\n$$\nSo we find that $\\mu_{-i}(t_i) = \\mathbb{E}[y_i|\\mathbf{y}_{-i}]$. The LOO predictive variance of the latent function is:\n$$\n\\sigma_{-i}^2(t_i) = \\mathrm{Var}[r(t_i)|\\mathbf{y}_{-i}] = K_{ii} - \\mathbf{k}_{i,-i} (K_{-i,-i} + \\sigma_n^2 I_{n-1})^{-1} \\mathbf{k}_{-i,i}\n$$\nThe task is to express these quantities using the full inverse matrix $A^{-1}$. We use the block matrix inversion formula, which relies on the Schur complement. The Schur complement of the block $A_{-i,-i}$ in matrix $A$ is $S_i = a_{ii} - \\mathbf{a}_{i,-i} (A_{-i,-i})^{-1} \\mathbf{a}_{-i,i}$. The $(i,i)$-th element of the inverse matrix $A^{-1}$ is the inverse of this Schur complement:\n$$\n(A^{-1})_{ii} = S_i^{-1} = (a_{ii} - \\mathbf{a}_{i,-i} (A_{-i,-i})^{-1} \\mathbf{a}_{-i,i})^{-1}\n$$\nThis is precisely the inverse of the conditional variance $\\mathrm{Var}[y_i|\\mathbf{y}_{-i}]$.\n$$\n\\mathrm{Var}[y_i|\\mathbf{y}_{-i}] = \\frac{1}{(A^{-1})_{ii}}\n$$\nSubstituting the definitions $a_{ii} = K_{ii} + \\sigma_n^2$ and $\\mathbf{a}_{i,-i} = \\mathbf{k}_{i,-i}$, we get:\n$$\n\\frac{1}{(A^{-1})_{ii}} = (K_{ii} + \\sigma_n^2) - \\mathbf{k}_{i,-i} (K_{-i,-i} + \\sigma_n^2 I_{n-1})^{-1} \\mathbf{k}_{-i,i}\n$$\nBy rearranging and comparing with the expression for $\\sigma_{-i}^2(t_i)$, we find:\n$$\n\\sigma_{-i}^2(t_i) = K_{ii} - \\mathbf{k}_{i,-i} (K_{-i,-i} + \\sigma_n^2 I_{n-1})^{-1} \\mathbf{k}_{-i,i} = \\frac{1}{(A^{-1})_{ii}} - \\sigma_n^2\n$$\nThis provides the LOO predictive variance.\n\nFor the LOO predictive mean, we use another identity from partitioned matrices and Gaussian conditioning. Consider the precision matrix $\\Lambda = A^{-1}$. The conditional mean of $y_i$ given $\\mathbf{y}_{-i}$ (for a zero-mean process) is given by $\\mathbb{E}[y_i|\\mathbf{y}_{-i}] = -(\\Lambda_{ii})^{-1} \\sum_{j \\neq i} \\Lambda_{ij} y_j$. Since we have shown $\\mu_{-i}(t_i) = \\mathbb{E}[y_i|\\mathbf{y}_{-i}]$, we have:\n$$\n\\mu_{-i}(t_i) = -\\frac{1}{(A^{-1})_{ii}} \\sum_{j \\neq i} (A^{-1})_{ij} y_j\n$$\nThis expression is in terms of $A^{-1}$ and $\\mathbf{y}$. An alternative, more compact form can be found by relating it to the vector $\\boldsymbol{\\alpha} = A^{-1} \\mathbf{y}$. By definition, $\\boldsymbol{\\alpha}_i = \\sum_{j=1}^n (A^{-1})_{ij} y_j = (A^{-1})_{ii} y_i + \\sum_{j \\neq i} (A^{-1})_{ij} y_j$.\nRearranging gives $\\sum_{j \\neq i} (A^{-1})_{ij} y_j = \\boldsymbol{\\alpha}_i - (A^{-1})_{ii} y_i$. Substituting this into the expression for $\\mu_{-i}(t_i)$:\n$$\n\\mu_{-i}(t_i) = -\\frac{1}{(A^{-1})_{ii}} (\\boldsymbol{\\alpha}_i - (A^{-1})_{ii} y_i) = y_i - \\frac{\\boldsymbol{\\alpha}_i}{(A^{-1})_{ii}}\n$$\n\n**2) Expression for the Leave-One-Out Residual**\n\nThe leave-one-out residual is defined as $e^{\\mathrm{loo}}_i = y_i - \\mu_{-i}(t_i)$. Using the compact expression for $\\mu_{-i}(t_i)$ derived above:\n$$\ne^{\\mathrm{loo}}_i = y_i - \\left( y_i - \\frac{\\boldsymbol{\\alpha}_i}{(A^{-1})_{ii}} \\right) = \\frac{\\boldsymbol{\\alpha}_i}{(A^{-1})_{ii}}\n$$\nwhere $\\boldsymbol{\\alpha} = A^{-1} \\mathbf{y}$. This expresses the residual entirely in terms of the vector $\\boldsymbol{\\alpha}$ and the diagonal entries of $A^{-1}$, as required.\n\n**3) Closed-Form Expression for $\\mathrm{RMSE}_{\\mathrm{loo}}$**\n\nThe leave-one-out root-mean-square error is defined as $\\mathrm{RMSE}_{\\mathrm{loo}} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (e^{\\mathrm{loo}}_i)^2}$. We substitute the expression for $e^{\\mathrm{loo}}_i$ from part 2 into this definition:\n$$\n\\mathrm{RMSE}_{\\mathrm{loo}} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n \\left( \\frac{\\boldsymbol{\\alpha}_i}{(A^{-1})_{ii}} \\right)^2}\n$$\nTo make the dependence on $A^{-1}$ and $\\mathbf{y}$ fully explicit, we substitute $\\boldsymbol{\\alpha}_i = (A^{-1} \\mathbf{y})_i = \\sum_{j=1}^n (A^{-1})_{ij} y_j$. This yields the final closed-form expression:\n$$\n\\mathrm{RMSE}_{\\mathrm{loo}} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n \\left( \\frac{\\sum_{j=1}^n (A^{-1})_{ij} y_j}{(A^{-1})_{ii}} \\right)^2}\n$$\nThis expression depends only on the pre-computed matrix $A^{-1}$ and the observation vector $\\mathbf{y}$, and provides a computationally efficient way to estimate the LOO cross-validation error without retraining the GP model $n$ times.\n\n**4) Justification for Adaptive Sampling**\n\nThe goal of adaptive sampling in design space exploration is to add new simulation points in regions that most effectively improve the model's accuracy and reduce its uncertainty. The LOO metrics provide valuable heuristics for identifying such regions.\n\nThe magnitude of the LOO residual, $|e^{\\mathrm{loo}}_i| = |\\frac{\\boldsymbol{\\alpha}_i}{(A^{-1})_{ii}}|$, quantifies the \"surprise\" of data point $(t_i, y_i)$ with respect to the rest of the dataset. A large $|e^{\\mathrm{loo}}_i|$ signifies that the model trained on $\\mathcal{D}_{-i}$ makes a poor prediction at $t_i$. Within the GP framework, this implies that the function $r(t)$ is behaving in a way not easily interpolated from neighboring points, likely exhibiting rapid change or high curvature near $t_i$. To capture this rapid variation, the posterior mean $\\mu(t)$ must have a large gradient magnitude $|\\partial \\mu(t)/\\partial t|$ in the vicinity of $t_i$. Therefore, regions with high LOO residuals are prime candidates for adding new samples to better resolve the function's local behavior.\n\nThe LOO predictive uncertainty, represented by the standard deviation $\\sigma_{-i}(t_i) = \\sqrt{(A^{-1})_{ii}^{-1} - \\sigma_n^2}$, measures the model's epistemic (or reducible) uncertainty at location $t_i$ when that point is excluded from training. A large $\\sigma_{-i}(t_i)$ indicates that the point $t_i$ is poorly constrained by the other data points. In a one-dimensional space, this typically occurs when $t_i$ is in a sparsely sampled region, i.e., the distance to its neighbors is large. The correlation function $k(t,t')$ decays with distance, so distant points offer little information, resulting in high posterior variance. Sampling in regions of high $\\sigma_{-i}(t_i)$ directly targets areas of high model uncertainty, reducing the overall predictive variance of the surrogate and making it more reliable.\n\nIn summary, a principled adaptive sampling strategy can be devised by inspecting the LOO metrics across the training set. One would compute $|e^{\\mathrm{loo}}_i|$ and $\\sigma_{-i}(t_i)$ for all $i=1, \\dots, n$. The next simulation should be run at a new point $t_{\\text{new}}$ in the neighborhood of the existing point $t_k$ which exhibits the largest error or uncertainty (e.g., maximum $|e^{\\mathrm{loo}}_k|$). This iteratively refines the model by adding data where it is most needed, either to correct prediction errors (indicated by $|e^{\\mathrm{loo}}_i|$) or to reduce model ignorance (indicated by $\\sigma_{-i}(t_i)$). This approach uses information intrinsic to the GP's Bayesian formulation to guide the exploration process efficiently.",
            "answer": "$$\n\\boxed{\\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} \\left( \\frac{\\sum_{j=1}^{n} (A^{-1})_{ij} y_j}{(A^{-1})_{ii}} \\right)^{2}}}\n$$"
        }
    ]
}