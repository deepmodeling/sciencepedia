{
    "hands_on_practices": [
        {
            "introduction": "The simplest assumption for modeling manufacturing defects is that they are randomly and uniformly distributed, which leads to the Poisson yield model. This practice  challenges you to derive both this basic model and the more realistic Negative Binomial model, which accounts for the common phenomenon of defect clustering. By working through this exercise, you will gain a quantitative understanding of why simple models can be overly pessimistic and how incorporating spatial statistics is fundamental to accurate yield prediction, especially for large die areas.",
            "id": "4281279",
            "problem": "A fabrication line for integrated circuits operates in a regime where random fatal defects can be treated using point processes. Assume the following modeling base:\n\n- Under spatially homogeneous defects, the number of fatal defects on a die of area $A$ follows a Poisson distribution with mean $\\mu = D_{0} A$, where $D_{0}$ is the defect density in units of defects per square centimeter. The die yield is the probability that a die has zero fatal defects.\n\n- Under spatially clustered defects, the local defectivity is heterogeneous across the wafer: conditional on a local defectivity $\\Lambda$, the defects on a die are Poisson with mean $\\Lambda A$, and the heterogeneity in $\\Lambda$ across dies follows a Gamma distribution with a shape parameter $\\alpha$ that quantifies clustering. The die yield is the probability of zero fatal defects marginalized over this heterogeneity.\n\nStarting strictly from these definitions, derive closed-form expressions for the die yield under the homogeneous (Poisson) and clustered (Gamma-Poisson mixture) cases, then determine the die area $A$ (in $\\mathrm{cm}^{2}$) at which the clustered-model yield exceeds the homogeneous-model yield by a factor of $1.05$ for a process characterized by $D_{0} = 0.3$ defects per square centimeter and clustering parameter $\\alpha = 5$. Round your final numerical result for $A$ to three significant figures and express it in square centimeters. In addition, discuss the practical implications of your result for predicting manufacturing yield and for cost sensitivity to die size under clustering. Your final answer must be only the computed value of $A$.",
            "solution": "The user has provided a problem statement that requires validation before a solution is attempted.\n\n### Step 1: Extract Givens\n-   For a homogeneous defect distribution, the number of fatal defects on a die of area $A$ follows a Poisson distribution with mean $\\mu = D_{0} A$.\n-   $D_{0}$ is the defect density.\n-   The yield is the probability of zero fatal defects.\n-   For a clustered defect distribution, the number of fatal defects on a die of area $A$ follows a Poisson distribution with mean $\\Lambda A$, conditional on a local defectivity $\\Lambda$.\n-   The local defectivity $\\Lambda$ follows a Gamma distribution with a shape parameter $\\alpha$.\n-   The goal is to find the die area $A$ (in $\\mathrm{cm}^{2}$) where the clustered-model yield is $1.05$ times the homogeneous-model yield.\n-   The given parameters are $D_{0} = 0.3 \\text{ defects/cm}^2$ and $\\alpha = 5$.\n-   The final numerical result for $A$ must be rounded to three significant figures.\n-   The solution must include a discussion of practical implications.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is scientifically grounded, well-posed, and objective. It is based on standard and widely accepted statistical models for manufacturing yield in integrated circuits: the Poisson model for purely random defects and the Gamma-Poisson (Negative Binomial) model for clustered defects. The provided parameters, $D_{0} = 0.3 \\text{ defects/cm}^2$ and $\\alpha = 5$, are realistic for a semiconductor manufacturing process. The problem implicitly assumes that the mean of the Gamma distribution for the local defectivity, $E[\\Lambda]$, is equal to the global average defect density $D_{0}$. This is a standard and necessary condition for a meaningful comparison between the two models and does not render the problem invalid; rather, it is a standard convention in the field. The problem is self-contained, mathematically formalizable, and has a unique, meaningful solution.\n\n### Step 3: Verdict and Action\nThe problem is deemed valid. A full solution will be provided.\n\n### Derivation of Yield Models\n\nFirst, we derive the closed-form expressions for the die yield under both models as defined in the problem. The yield, $Y$, is the probability that a die has zero fatal defects.\n\n**1. Homogeneous (Poisson) Model**\n\nLet $N$ be the random variable representing the number of fatal defects on a die of area $A$. According to the problem, $N$ follows a Poisson distribution with mean $\\mu = D_{0} A$. The probability mass function (PMF) of a Poisson distribution is given by:\n$$\nP(N=k) = \\frac{\\mu^{k} e^{-\\mu}}{k!} = \\frac{(D_{0}A)^{k} \\exp(-D_{0}A)}{k!}\n$$\nThe yield, which we denote as $Y_{\\text{hom}}$, is the probability of having zero defects ($k=0$):\n$$\nY_{\\text{hom}}(A) = P(N=0) = \\frac{(D_{0}A)^{0} \\exp(-D_{0}A)}{0!}\n$$\nSince $x^{0}=1$ and $0!=1$, this simplifies to the well-known Poisson yield model:\n$$\nY_{\\text{hom}}(A) = \\exp(-D_{0}A)\n$$\n\n**2. Clustered (Gamma-Poisson) Model**\n\nIn this model, the defect density is not uniform. The local defect density $\\Lambda$ is itself a random variable following a Gamma distribution. The probability density function (PDF) of a Gamma distribution with shape parameter $\\alpha$ and scale parameter $\\theta$ is:\n$$\nf(\\lambda; \\alpha, \\theta) = \\frac{\\lambda^{\\alpha-1} \\exp(-\\lambda/\\theta)}{\\theta^{\\alpha} \\Gamma(\\alpha)} \\quad \\text{for } \\lambda \\ge 0\n$$\nwhere $\\Gamma(\\alpha)$ is the Gamma function. For the two models to be comparable, they must share the same overall average defect density. We set the expected value of $\\Lambda$ equal to $D_{0}$:\n$$\nE[\\Lambda] = \\alpha\\theta = D_{0}\n$$\nFrom this, we determine the scale parameter $\\theta = D_{0}/\\alpha$. Substituting this back into the PDF:\n$$\nf(\\lambda) = \\frac{\\lambda^{\\alpha-1} \\exp(-\\lambda\\alpha/D_{0})}{(D_{0}/\\alpha)^{\\alpha} \\Gamma(\\alpha)}\n$$\nConditional on a specific value $\\Lambda = \\lambda$, the number of defects $N$ follows a Poisson distribution with mean $\\lambda A$. The conditional probability of zero defects is:\n$$\nP(N=0 | \\Lambda = \\lambda) = \\exp(-\\lambda A)\n$$\nTo find the overall yield for the clustered model, $Y_{\\text{clust}}$, we must marginalize over all possible values of $\\lambda$ by integrating the conditional probability weighted by the PDF of $\\Lambda$:\n$$\nY_{\\text{clust}}(A) = \\int_{0}^{\\infty} P(N=0 | \\Lambda=\\lambda) f(\\lambda) \\,d\\lambda = \\int_{0}^{\\infty} \\exp(-\\lambda A) \\frac{\\lambda^{\\alpha-1} \\exp(-\\lambda\\alpha/D_{0})}{(D_{0}/\\alpha)^{\\alpha} \\Gamma(\\alpha)} \\,d\\lambda\n$$\nCombining the exponential terms and factoring out constants:\n$$\nY_{\\text{clust}}(A) = \\frac{1}{(D_{0}/\\alpha)^{\\alpha} \\Gamma(\\alpha)} \\int_{0}^{\\infty} \\lambda^{\\alpha-1} \\exp\\left(-\\lambda \\left(A + \\frac{\\alpha}{D_{0}}\\right)\\right) \\,d\\lambda\n$$\nThe integral is of the form of a Gamma function. Using the identity $\\int_{0}^{\\infty} x^{z-1} \\exp(-cx) \\,dx = \\frac{\\Gamma(z)}{c^{z}}$ with $z=\\alpha$ and $c = A + \\frac{\\alpha}{D_{0}}$, the integral evaluates to:\n$$\n\\int_{0}^{\\infty} \\lambda^{\\alpha-1} \\exp\\left(-\\lambda \\left(\\frac{AD_{0}+\\alpha}{D_{0}}\\right)\\right) \\,d\\lambda = \\frac{\\Gamma(\\alpha)}{\\left(\\frac{AD_{0}+\\alpha}{D_{0}}\\right)^{\\alpha}}\n$$\nSubstituting this result back into the expression for $Y_{\\text{clust}}(A)$:\n$$\nY_{\\text{clust}}(A) = \\frac{1}{(D_{0}/\\alpha)^{\\alpha} \\Gamma(\\alpha)} \\cdot \\frac{\\Gamma(\\alpha)}{\\left(\\frac{AD_{0}+\\alpha}{D_{0}}\\right)^{\\alpha}} = \\left(\\frac{\\alpha}{D_{0}}\\right)^{\\alpha} \\left(\\frac{D_{0}}{AD_{0}+\\alpha}\\right)^{\\alpha}\n$$\nThis simplifies to the Negative Binomial yield model:\n$$\nY_{\\text{clust}}(A) = \\left(\\frac{\\alpha}{AD_{0}+\\alpha}\\right)^{\\alpha} = \\left(1 + \\frac{D_{0}A}{\\alpha}\\right)^{-\\alpha}\n$$\n\n### Solving for the Die Area A\n\nThe problem requires finding the area $A$ at which the clustered-model yield exceeds the homogeneous-model yield by a factor of $1.05$. This condition is expressed as:\n$$\nY_{\\text{clust}}(A) = 1.05 \\cdot Y_{\\text{hom}}(A)\n$$\nSubstituting the derived yield formulas:\n$$\n\\left(1 + \\frac{D_{0}A}{\\alpha}\\right)^{-\\alpha} = 1.05 \\cdot \\exp(-D_{0}A)\n$$\nNow, we substitute the given numerical values: $D_{0} = 0.3$ and $\\alpha = 5$.\n$$\n\\left(1 + \\frac{0.3 A}{5}\\right)^{-5} = 1.05 \\cdot \\exp(-0.3 A)\n$$\n$$\n(1 + 0.06 A)^{-5} = 1.05 \\cdot \\exp(-0.3 A)\n$$\nThis is a transcendental equation that must be solved for $A$. To facilitate a numerical solution, we can take the natural logarithm of both sides:\n$$\n\\ln\\left((1 + 0.06 A)^{-5}\\right) = \\ln(1.05 \\cdot \\exp(-0.3 A))\n$$\n$$\n-5 \\ln(1 + 0.06 A) = \\ln(1.05) - 0.3 A\n$$\nRearranging the terms, we seek the root of the function $g(A)$:\n$$\ng(A) = 0.3 A - 5 \\ln(1 + 0.06 A) - \\ln(1.05) = 0\n$$\nWe can solve this equation numerically. Using a numerical solver (such as the Newton-Raphson method or a built-in solver function in a computational tool), we find the value of $A$. Let's perform an iterative search for the root.\nThe value of $\\ln(1.05)$ is approximately $0.048790$.\nLet's test $A = 2.4$:\n$g(2.4) = 0.3(2.4) - 5\\ln(1 + 0.06 \\times 2.4) - 0.04879 = 0.72 - 5\\ln(1.144) - 0.04879 \\approx 0.72 - 5(0.13456) - 0.04879 = 0.72 - 0.6728 - 0.04879 = -0.00159$.\nLet's test $A = 2.44$:\n$g(2.44) = 0.3(2.44) - 5\\ln(1 + 0.06 \\times 2.44) - 0.04879 = 0.732 - 5\\ln(1.1464) - 0.04879 \\approx 0.732 - 5(0.13661) - 0.04879 = 0.732 - 0.68305 - 0.04879 = 0.00016$.\nThe root is between $A=2.4$ and $A=2.44$. A more precise calculation yields $A \\approx 2.4384$.\nRounding the result to three significant figures, we get $A = 2.44 \\, \\mathrm{cm}^{2}$.\n\n### Discussion of Practical Implications\n\nThe result shows that for a die area of $A = 2.44 \\, \\mathrm{cm}^{2}$, a relatively large but common size for modern processors, the more realistic clustered defect model predicts a yield that is $5\\%$ higher than the prediction from the simplistic homogeneous Poisson model. This has two major practical implications:\n\n1.  **For predicting manufacturing yield:** The simple Poisson model is consistently pessimistic. It assumes defects are uniformly distributed, whereas in reality they tend to cluster. This clustering \"protects\" some parts of the wafer, leading to a higher number of defect-free dies than would be expected under a uniform distribution. For a given average defect density $D_{0}$, the actual yield will be higher than $\\exp(-D_{0}A)$. Accurately predicting yield is crucial for financial forecasting and process planning. Relying on the Poisson model could lead to underestimating profitability and making incorrect decisions, such as prematurely discontinuing a promising but seemingly low-yield manufacturing process. Capturing spatial statistics, such as the clustering parameter $\\alpha$, is essential for accurate yield modeling.\n\n2.  **For cost sensitivity to die size under clustering:** The yield penalty for increasing die size is significantly lower in the clustered model compared to the homogeneous model. The Poisson yield, $Y_{\\text{hom}} = \\exp(-D_{0}A)$, decays exponentially with area, making very large dies appear economically unviable. In contrast, the Negative Binomial yield, $Y_{\\text{clust}} = (1 + D_{0}A/\\alpha)^{-\\alpha}$, decays as a power law for large $A$. This slower rate of decay makes larger dies (e.g., for high-performance computing, GPUs, and FPGAs) more feasible from a cost perspective. The analysis shows that the ratio $Y_{\\text{clust}}/Y_{\\text{hom}}$ grows with $A$, meaning the economic advantage of accounting for clustering becomes more pronounced for larger and more complex chips. This understanding alters the economic trade-offs involved in chip design, potentially justifying larger die sizes than would be considered under a simple Poisson yield assumption.",
            "answer": "$$\n\\boxed{2.44}\n$$"
        },
        {
            "introduction": "After establishing theoretical models, a practical question arises: how do we determine the model parameters from real data? This exercise  places you in this scenario, tasking you with estimating die yield from wafer test results. You will not only calculate the most likely yield value using maximum likelihood estimation but also construct a confidence interval to formally quantify the uncertainty of your estimate, a critical skill for making informed process decisions based on limited data.",
            "id": "4281273",
            "problem": "A fabrication line for Integrated Circuits (ICs) within the domain of Electronic Design Automation (EDA) uses wafer-level electrical test to estimate die yield. Consider a single wafer test map reporting $N_{\\mathrm{tested}} = 100$ dice probed and $N_{\\mathrm{good}} = 96$ dice passing all parametric and functional criteria. Assume each die outcome is an independent Bernoulli trial with success probability $p$ equal to the underlying die yield on the wafer, and that the observed count $N_{\\mathrm{good}}$ is a realization of a binomial random variable with parameters $(n, p)$ where $n = N_{\\mathrm{tested}}$. Starting from the binomial model and its likelihood, (i) derive the maximum likelihood estimator for $p$ and explain its interpretation as the observed die yield, and (ii) construct a two-sided $95\\%$ confidence interval for $p$ by inverting the score test for the binomial proportion (Wilson method), explaining the statistical principle that motivates this construction and contrasting it with the exact Clopper–Pearson interval in terms of coverage and conservatism. In addition, (iii) starting from Bayes’ rule and the conjugacy of the Beta prior for a binomial likelihood, derive the posterior distribution for $p$ under a $\\mathrm{Beta}(\\alpha,\\beta)$ prior and give conditions under which a Bayesian analysis is preferable to a frequentist confidence interval in the context of manufacturing yield prediction across wafers and lots. Finally, using the Wilson score interval derived in part (ii), compute the lower bound of the two-sided $95\\%$ confidence interval for the die yield for this wafer, and report only that lower bound. Express your final numerical result as a decimal fraction and round your answer to four significant figures.",
            "solution": "The problem as stated is a well-posed and self-contained question in applied statistics, specifically within the context of integrated circuit manufacturing yield modeling. It is scientifically sound, objective, and provides all necessary information for a complete solution. Therefore, we proceed with the derivation and calculation.\n\nThe problem asks for a multi-part analysis of die yield based on wafer test data, where $N_{\\mathrm{tested}} = n = 100$ and $N_{\\mathrm{good}} = k = 96$. The underlying die yield is denoted by the parameter $p$.\n\n(i) Maximum Likelihood Estimator (MLE) for $p$\n\nThe number of good dice, $k$, is modeled as a realization of a binomial random variable $K \\sim \\mathrm{Bin}(n, p)$. The probability mass function is given by:\n$$P(K=k | n, p) = \\binom{n}{k} p^k (1-p)^{n-k}$$\nThe likelihood function $L(p|k)$ is numerically equal to this probability, but is viewed as a function of the parameter $p$ for a fixed observation $k$:\n$$L(p|k) = \\binom{n}{k} p^k (1-p)^{n-k}$$\nTo find the maximum likelihood estimator $\\hat{p}_{\\mathrm{MLE}}$, we find the value of $p$ that maximizes $L(p|k)$. It is equivalent and simpler to maximize the log-likelihood function, $\\ell(p|k) = \\ln L(p|k)$:\n$$\\ell(p|k) = \\ln\\left(\\binom{n}{k}\\right) + k \\ln(p) + (n-k) \\ln(1-p)$$\nWe differentiate $\\ell(p|k)$ with respect to $p$ and set the derivative to zero:\n$$\\frac{d\\ell}{dp} = \\frac{k}{p} - \\frac{n-k}{1-p}$$\nSetting $\\frac{d\\ell}{dp} = 0$ to find the critical point $\\hat{p}$:\n$$\\frac{k}{\\hat{p}} = \\frac{n-k}{1-\\hat{p}}$$\n$$k(1-\\hat{p}) = (n-k)\\hat{p}$$\n$$k - k\\hat{p} = n\\hat{p} - k\\hat{p}$$\n$$k = n\\hat{p}$$\nThis yields the maximum likelihood estimator for $p$:\n$$\\hat{p}_{\\mathrm{MLE}} = \\frac{k}{n}$$\nThis result has a highly intuitive interpretation: the best estimate for the true die yield $p$ is the observed proportion of good dice in the sample, which is the observed die yield. For the given data, $\\hat{p} = \\frac{96}{100} = 0.96$.\n\n(ii) Wilson Score Confidence Interval\n\nThe Wilson score interval is derived by inverting the score test. The score test statistic for a proportion is $Z = \\frac{\\hat{p} - p_0}{\\sqrt{p_0(1-p_0)/n}}$, where $p_0$ is the proportion under the null hypothesis. To construct a $(1-\\alpha)$ confidence interval, we find the set of all values of $p_0$ for which we would fail to reject the null hypothesis $H_0: p = p_0$ at significance level $\\alpha$. This corresponds to finding the values of $p$ that satisfy the inequality $|Z| \\le z_{\\alpha/2}$, where $z_{\\alpha/2}$ is the upper $\\alpha/2$ quantile of the standard normal distribution. The endpoints of the interval are found by solving the equation $|Z| = z_{\\alpha/2}$, or equivalently $Z^2 = z_{\\alpha/2}^2$:\n$$\\frac{(\\hat{p} - p)^2}{\\frac{p(1-p)}{n}} = z_{\\alpha/2}^2$$\nRearranging this gives a quadratic equation in $p$:\n$$(\\hat{p} - p)^2 = \\frac{z_{\\alpha/2}^2}{n} p(1-p)$$\n$$ \\hat{p}^2 - 2\\hat{p}p + p^2 = \\frac{z_{\\alpha/2}^2}{n}p - \\frac{z_{\\alpha/2}^2}{n}p^2 $$\n$$ \\left(1 + \\frac{z_{\\alpha/2}^2}{n}\\right)p^2 - \\left(2\\hat{p} + \\frac{z_{\\alpha/2}^2}{n}\\right)p + \\hat{p}^2 = 0 $$\nSolving this quadratic equation for $p$ using the quadratic formula yields the lower and upper bounds of the Wilson interval:\n$$ p_{\\text{lower, upper}} = \\frac{\\left(2\\hat{p} + \\frac{z_{\\alpha/2}^2}{n}\\right) \\pm \\sqrt{\\left(2\\hat{p} + \\frac{z_{\\alpha/2}^2}{n}\\right)^2 - 4\\left(1 + \\frac{z_{\\alpha/2}^2}{n}\\right)\\hat{p}^2}}{2\\left(1 + \\frac{z_{\\alpha/2}^2}{n}\\right)} $$\nA more common simplified form is:\n$$ p_{\\text{lower, upper}} = \\frac{1}{1 + \\frac{z_{\\alpha/2}^2}{n}} \\left( \\hat{p} + \\frac{z_{\\alpha/2}^2}{2n} \\pm z_{\\alpha/2} \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n} + \\frac{z_{\\alpha/2}^2}{4n^2}} \\right) $$\nThe statistical principle motivating this construction is that the score test generally has better performance regarding its actual Type I error rate compared to the Wald test, especially for proportions near $0$ or $1$ and for small sample sizes. The Wilson interval inherits this advantage, having an actual coverage probability that is, on average, closer to the nominal $1-\\alpha$ level.\n\nIn contrast, the Clopper-Pearson \"exact\" interval is constructed to guarantee that the coverage probability is *at least* $1-\\alpha$ for all values of $p$. This guarantee makes it conservative, meaning the intervals are often wider than necessary, leading to a loss of precision. The Wilson interval is not strictly conservative but provides a better balance between coverage and interval width.\n\n(iii) Bayesian Analysis with Beta-Binomial Model\n\nIn a Bayesian framework, we combine a prior belief about the parameter $p$ with the evidence from the data (the likelihood) to form an updated belief (the posterior distribution). Bayes' rule states:\n$$P(p|k) \\propto P(k|p) \\cdot P(p)$$\nwhere $P(p|k)$ is the posterior, $P(k|p)$ is the likelihood, and $P(p)$ is the prior distribution. The Beta distribution is the conjugate prior for the binomial likelihood. This means if the prior is a Beta distribution, the posterior will also be a Beta distribution. Let the prior for $p$ be a $\\mathrm{Beta}(\\alpha, \\beta)$ distribution:\n$$P(p) \\propto p^{\\alpha-1} (1-p)^{\\beta-1}$$\nThe binomial likelihood is $P(k|p) \\propto p^k (1-p)^{n-k}$.\nThe posterior distribution is therefore:\n$$P(p|k) \\propto (p^k (1-p)^{n-k}) \\cdot (p^{\\alpha-1} (1-p)^{\\beta-1})$$\n$$P(p|k) \\propto p^{k+\\alpha-1} (1-p)^{n-k+\\beta-1}$$\nThis is the kernel of a Beta distribution with updated parameters $\\alpha' = \\alpha + k$ and $\\beta' = \\beta + n - k$. Thus, the posterior distribution for $p$ is:\n$$p | k, n, \\alpha, \\beta \\sim \\mathrm{Beta}(\\alpha + N_{\\mathrm{good}}, \\beta + N_{\\mathrm{tested}} - N_{\\mathrm{good}})$$\nA Bayesian analysis is preferable to a frequentist confidence interval under several conditions:\n1.  **Availability of Prior Information:** In manufacturing, there is often substantial historical data from previously processed wafers or lots. This process knowledge can be codified into an informative prior distribution ($\\alpha, \\beta$), leading to more robust and stable yield predictions, especially when the current sample size $n$ is small.\n2.  **Small Sample Sizes:** When $N_{\\mathrm{tested}}$ is low, the frequentist MLE can be extreme (e.g., $\\hat{p}=0$ or $\\hat{p}=1$), and the associated confidence intervals can have poor properties. The Bayesian posterior, regularized by the prior, provides a more reasonable full distributional estimate for $p$.\n3.  **Interpretability:** A Bayesian credible interval has a more direct probabilistic interpretation: there is a $(1-\\alpha)$ probability that the true parameter $p$ lies within the interval, given the data and the model. A frequentist confidence interval has a more convoluted interpretation related to long-run frequencies of the interval-generating procedure.\n4.  **Decision Making:** The posterior distribution is a complete summary of uncertainty about $p$. It can be directly used in decision-theoretic frameworks to calculate expected costs or utilities for process control decisions, which is more difficult with only a confidence interval.\n\nFinal Calculation: Lower Bound of the $95\\%$ Wilson Score Interval\n\nWe are given $n = 100$, $k = 96$, and a $95\\%$ confidence level.\nThis implies $\\alpha = 0.05$, and $z_{\\alpha/2} = z_{0.025}$. The standard value for $z_{0.025}$ is approximately $1.96$. A more precise value is $1.959964$. We will use this more precise value for accuracy.\nThe sample proportion is $\\hat{p} = \\frac{k}{n} = \\frac{96}{100} = 0.96$.\n\nWe use the formula for the lower bound:\n$$p_{\\text{lower}} = \\frac{1}{1 + \\frac{z_{\\alpha/2}^2}{n}} \\left( \\hat{p} + \\frac{z_{\\alpha/2}^2}{2n} - z_{\\alpha/2} \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n} + \\frac{z_{\\alpha/2}^2}{4n^2}} \\right)$$\nFirst, we compute the necessary terms:\n$z_{\\alpha/2} \\approx 1.959964$\n$z_{\\alpha/2}^2 \\approx 3.841459$\n$n = 100$\n$\\hat{p} = 0.96$\n$1-\\hat{p} = 0.04$\n\nDenominator term: $1 + \\frac{z_{\\alpha/2}^2}{n} = 1 + \\frac{3.841459}{100} = 1.03841459$\nNumerator terms:\nCenter-shift term: $\\hat{p} + \\frac{z_{\\alpha/2}^2}{2n} = 0.96 + \\frac{3.841459}{200} = 0.96 + 0.0192073 = 0.9792073$\nSquare root term:\n$\\frac{\\hat{p}(1-\\hat{p})}{n} = \\frac{0.96 \\times 0.04}{100} = 0.000384$\n$\\frac{z_{\\alpha/2}^2}{4n^2} = \\frac{3.841459}{4 \\times 100^2} = \\frac{3.841459}{40000} \\approx 0.000096036$\nSum under square root: $0.000384 + 0.000096036 = 0.000480036$\nSquare root: $\\sqrt{0.000480036} \\approx 0.0219097$\nMinus term: $z_{\\alpha/2} \\times \\sqrt{\\dots} = 1.959964 \\times 0.0219097 \\approx 0.0429424$\nNumerator: $0.9792073 - 0.0429424 = 0.9362649$\n\nFinally, the lower bound is:\n$$p_{\\text{lower}} = \\frac{0.9362649}{1.03841459} \\approx 0.901648$$\nRounding to four significant figures, we get $0.9016$.",
            "answer": "$$\\boxed{0.9016}$$"
        },
        {
            "introduction": "The ultimate goal of yield modeling is to guide design improvements. In this final practice , you will tackle a classic design-for-yield problem: using redundancy to enhance chip robustness against manufacturing defects. You will derive a cost-benefit metric—the marginal yield gain per unit of area—and then design a greedy algorithm to determine the most area-efficient strategy for meeting a yield target. This exercise bridges the gap between probabilistic modeling and practical, cost-driven engineering optimization.",
            "id": "4281256",
            "problem": "Consider a chip-level manufacturing yield planning problem in the domain of Electronic Design Automation (EDA). A chip consists of $n$ functionally independent modules. Each module $i$ occupies area $a_i$ expressed in $\\text{cm}^2$. The defect density is uniform and equal to $D_0$ defects per $\\text{cm}^2$, and defects are assumed to cause catastrophic failure of a module if at least one defect lands in its area. Assume independence of defect events across disjoint areas.\n\nFundamental basis to use:\n- Under the classical Poisson defect model, the yield of a single module of area $a_i$ is $Y_i = \\exp(-D_0 a_i)$.\n- Under the standard series composition rule from probability theory for independent events, the chip operates correctly if each required function operates correctly. If a function is implemented with $k_i$ parallel redundant identical modules (each of area $a_i$ and independent in the defect sense), the function operates correctly if at least one of the $k_i$ modules operates correctly.\n\nYou may assume independence of failure events across module copies and across different modules. Redundancy is inserted by duplicating modules so that function $i$ is implemented by $k_i \\ge 1$ identical copies. The physical area consumption of function $i$ scales as $k_i a_i$, and the total added area due to redundancy is $\\sum_{i=1}^n (k_i - 1) a_i$, expressed in $\\text{cm}^2$.\n\nTask:\n1. Starting from the fundamental basis above, derive the expression for the success probability of a function implemented with $k_i$ redundant copies and the overall chip yield as a function of the vector $k = (k_1, \\dots, k_n)$.\n2. Derive, from first principles using probability and the Poisson model, the marginal gain in overall chip yield when increasing $k_i$ by one, and then derive the marginal yield gain per unit area decision rule for selecting the next module to duplicate. The decision rule must be expressed as a quantity to be maximized at each step of a greedy heuristic that inserts one duplicate at a time.\n3. Design and implement a greedy redundancy insertion heuristic that:\n   - Initializes with no redundancy, i.e., $k_i = 1$ for all $i$.\n   - At each step, selects the module index $i$ that maximizes the derived marginal yield gain per unit area decision rule, increases $k_i$ by one, and repeats until the chip yield meets or exceeds a given target yield $Y_\\mathrm{target}$ (expressed as a decimal, not a percentage).\n   - If multiple modules have equal decision rule values, break ties by selecting the module with smaller $a_i$, and if still tied, by the smallest index $i$.\n   - Outputs, for each test case, the final redundancy vector $[k_1, \\dots, k_n]$ as integers, the total added area in $\\text{cm}^2$ as a float, and the achieved chip yield as a decimal float.\n\nAngle units are not relevant in this problem. Physical units are strictly required for area: all area values must be handled in $\\text{cm}^2$. All yields must be decimals in the unitless range $(0,1)$.\n\nYour program should produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets (e.g., $[result1,result2,\\dots]$), where each $resultj$ is a list of the form $[k_1,\\dots,k_n,\\text{added\\_area\\_cm2},\\text{achieved\\_yield}]$.\n\nTest suite:\nProvide results for the following parameter sets $(D_0, [a_1,\\dots,a_n], Y_\\mathrm{target})$:\n- Case A: $D_0 = 0.5$, $[a_1,a_2,a_3] = [0.2, 0.05, 0.1]$, $Y_\\mathrm{target} = 0.85$.\n- Case B: $D_0 = 0.1$, $[a_1,a_2,a_3] = [0.3, 0.3, 0.4]$, $Y_\\mathrm{target} = 0.90$.\n- Case C: $D_0 = 2.0$, $[a_1,a_2,a_3,a_4,a_5] = [0.02, 0.02, 0.02, 0.02, 0.02]$, $Y_\\mathrm{target} = 0.95$.\n- Case D: $D_0 = 0.8$, $[a_1,a_2,a_3] = [0.5, 0.01, 0.01]$, $Y_\\mathrm{target} = 0.75$.\n- Case E: $D_0 = 0.3$, $[a_1,a_2] = [0.1, 0.1]$, $Y_\\mathrm{target} = 0.995$.\n\nYour program must compute the redundancy vector and outputs in the specified format for these cases, and no other input should be read.",
            "solution": "The problem statement has been analyzed and is determined to be valid. It is scientifically sound, well-posed, objective, and contains all necessary information to derive a unique solution based on the provided methodology. The problem is grounded in the established Poisson yield model and fundamental probability theory as applied to system reliability in integrated circuit manufacturing.\n\nHerein, a step-by-step derivation and algorithmic design are presented, as requested.\n\n### Task 1: Derivation of Yield Expressions\n\nThe objective is to derive the success probability for a function with redundancy and the overall chip yield.\n\nLet $D_0$ be the defect density and $a_i$ be the area of module $i$.\nThe yield of a single, non-redundant copy of module $i$, denoted as $Y_{\\text{copy},i}$, is given by the Poisson model:\n$$Y_{\\text{copy},i} = \\exp(-D_0 a_i)$$\nThe probability of failure for a single copy is the complement of its yield:\n$$F_{\\text{copy},i} = 1 - Y_{\\text{copy},i} = 1 - \\exp(-D_0 a_i)$$\nA function $i$ is implemented with $k_i$ identical, parallel, and independent copies. The function operates correctly if at least one of these $k_i$ copies is defect-free. It is more direct to first calculate the probability that the function fails. The function fails only if all $k_i$ copies fail. Due to the independence of failure events among the copies, the probability of function $i$ failing, $F_{\\text{func},i}$, is the product of the individual copy failure probabilities:\n$$F_{\\text{func},i}(k_i) = (F_{\\text{copy},i})^{k_i} = (1 - \\exp(-D_0 a_i))^{k_i}$$\nThe success probability, or yield, of function $i$, denoted as $Y_{\\text{func},i}(k_i)$, is the complement of its failure probability:\n$$Y_{\\text{func},i}(k_i) = 1 - F_{\\text{func},i}(k_i) = 1 - (1 - \\exp(-D_0 a_i))^{k_i}$$\nThe chip consists of $n$ functionally independent modules. The chip operates correctly only if all $n$ functions operate correctly. This represents a series system of functions. The total chip yield, $Y_{\\text{chip}}$, is the product of the yields of all individual functions. Given a redundancy configuration vector $k = (k_1, k_2, \\dots, k_n)$, the chip yield is:\n$$Y_{\\text{chip}}(k) = \\prod_{i=1}^n Y_{\\text{func},i}(k_i)$$\nSubstituting the expression for $Y_{\\text{func},i}(k_i)$:\n$$Y_{\\text{chip}}(k) = \\prod_{i=1}^n \\left[1 - (1 - \\exp(-D_0 a_i))^{k_i}\\right]$$\nThis completes the derivation for the first task.\n\n### Task 2: Derivation of the Marginal Gain Decision Rule\n\nThe second task is to derive the marginal gain in chip yield from adding one redundant copy to a module $j$ and to formulate a decision rule for a greedy heuristic based on yield gain per unit area.\n\nLet the current redundancy vector be $k = (k_1, \\dots, k_j, \\dots, k_n)$. The current chip yield is $Y_{\\text{chip}}(k)$. If we increase the redundancy of module $j$ from $k_j$ to $k_j+1$, the new vector is $k' = (k_1, \\dots, k_j+1, \\dots, k_n)$. The marginal gain in chip yield, $\\Delta Y_{\\text{chip}, j}$, is the difference $Y_{\\text{chip}}(k') - Y_{\\text{chip}}(k)$.\n\nThe chip yield can be written as the product of the yield of function $j$ and the yield of all other functions:\n$$Y_{\\text{chip}}(k) = Y_{\\text{func},j}(k_j) \\cdot \\prod_{i \\ne j} Y_{\\text{func},i}(k_i)$$\nThe new yield is:\n$$Y_{\\text{chip}}(k') = Y_{\\text{func},j}(k_j+1) \\cdot \\prod_{i \\ne j} Y_{\\text{func},i}(k_i)$$\nThe marginal gain is therefore:\n$$\\Delta Y_{\\text{chip}, j} = \\left[ Y_{\\text{func},j}(k_j+1) - Y_{\\text{func},j}(k_j) \\right] \\cdot \\prod_{i \\ne j} Y_{\\text{func},i}(k_i)$$\nLet's analyze the term in the square brackets representing the change in yield for function $j$:\n$$\\Delta Y_{\\text{func},j} = Y_{\\text{func},j}(k_j+1) - Y_{\\text{func},j}(k_j)$$\n$$= \\left[1 - (1 - e^{-D_0 a_j})^{k_j+1}\\right] - \\left[1 - (1 - e^{-D_0 a_j})^{k_j}\\right]$$\n$$= (1 - e^{-D_0 a_j})^{k_j} - (1 - e^{-D_0 a_j})^{k_j+1}$$\nFactoring out the common term $(1 - e^{-D_0 a_j})^{k_j}$:\n$$= (1 - e^{-D_0 a_j})^{k_j} \\left[1 - (1 - e^{-D_0 a_j})\\right]$$\n$$= (1 - e^{-D_0 a_j})^{k_j} \\cdot e^{-D_0 a_j}$$\nThe product term $\\prod_{i \\ne j} Y_{\\text{func},i}(k_i)$ can be expressed as $\\frac{Y_{\\text{chip}}(k)}{Y_{\\text{func},j}(k_j)}$. Substituting these components back into the expression for $\\Delta Y_{\\text{chip}, j}$:\n$$\\Delta Y_{\\text{chip}, j} = \\left[ (1 - e^{-D_0 a_j})^{k_j} e^{-D_0 a_j} \\right] \\cdot \\frac{Y_{\\text{chip}}(k)}{Y_{\\text{func},j}(k_j)}$$\n$$\\Delta Y_{\\text{chip}, j} = Y_{\\text{chip}}(k) \\cdot \\frac{e^{-D_0 a_j} (1 - e^{-D_0 a_j})^{k_j}}{1 - (1 - e^{-D_0 a_j})^{k_j}}$$\nThis is the marginal gain in overall chip yield.\n\nThe cost of this action is the area of the added module copy, which is $a_j$. The greedy decision rule should select the module that provides the maximum marginal yield gain per unit of added area. This metric, which we will call $M_j$, is:\n$$M_j = \\frac{\\Delta Y_{\\text{chip}, j}}{a_j} = \\frac{Y_{\\text{chip}}(k)}{a_j} \\cdot \\frac{e^{-D_0 a_j} (1 - e^{-D_0 a_j})^{k_j}}{1 - (1 - e^{-D_0 a_j})^{k_j}}$$\nAt any given step of the greedy algorithm, the current chip yield $Y_{\\text{chip}}(k)$ is a positive constant factor for all modules $j$. Since we are only interested in finding the index $j$ that maximizes $M_j$, we can simplify the decision by maximizing a metric $D_j$ where this common factor is removed:\n$$D_j = \\frac{1}{a_j} \\cdot \\frac{e^{-D_0 a_j} (1 - e^{-D_0 a_j})^{k_j}}{1 - (1 - e^{-D_0 a_j})^{k_j}}$$\nAt each step, the greedy heuristic will choose to increment $k_j$ for the module $j$ that has the largest value of $D_j$.\n\n### Task 3: Greedy Redundancy Insertion Heuristic Design\n\nThe algorithm proceeds as follows:\n\n1.  **Initialization**: Given a defect density $D_0$, a vector of module areas $a = [a_1, \\dots, a_n]$, and a target yield $Y_{\\text{target}}$.\n    -   Initialize the redundancy vector $k$ to $[1, 1, \\dots, 1]$, representing the baseline design with no redundancy.\n    -   Calculate the initial chip yield $Y_{\\text{chip}}$ using the formula derived in Task 1 with the initial $k$.\n\n2.  **Greedy Iteration**: Enter a loop that continues as long as the current $Y_{\\text{chip}}$ is less than $Y_{\\text{target}}$.\n    a.  **Evaluate Candidates**: For each module $j \\in \\{1, \\dots, n\\}$, calculate the decision metric $D_j$ using the formula from Task 2 with the current redundancy level $k_j$.\n    b.  **Select Best Module**: Identify the module index $j^*$ that maximizes the decision metric $D_j$. The selection must adhere to the specified tie-breaking rules:\n        i.  If multiple modules share the same maximum $D_j$ value, select the one with the smallest area $a_j$.\n        ii. If a tie persists (i.e., same $D_j$ and same $a_j$), select the one with the smallest index $j$.\n    c.  **Update State**: Increment the redundancy count for the selected module: $k_{j^*} \\leftarrow k_{j^*} + 1$.\n    d.  **Recalculate Yield**: Update the total chip yield $Y_{\\text{chip}}$ a using the new redundancy vector $k$.\n\n3.  **Termination**: The loop terminates when $Y_{\\text{chip}} \\ge Y_{\\text{target}}$. Since adding redundancy always increases yield, termination is guaranteed for any $Y_{\\text{target}}  1$.\n    -   Upon termination, calculate the total added area: Added Area $= \\sum_{i=1}^n (k_i - 1) a_i$.\n    -   The final outputs are the final redundancy vector $k$, the total added area, and the achieved chip yield $Y_{\\text{chip}}$.\n\nThis algorithm systematically improves the chip yield by making locally optimal choices at each step, aiming to reach the target yield with an efficient use of area.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main solver function to run the greedy redundancy insertion heuristic\n    for all specified test cases.\n    \"\"\"\n    # Test suite: (D0, [a1, ..., an], Y_target)\n    test_cases = [\n        # Case A\n        (0.5, [0.2, 0.05, 0.1], 0.85),\n        # Case B\n        (0.1, [0.3, 0.3, 0.4], 0.90),\n        # Case C\n        (2.0, [0.02, 0.02, 0.02, 0.02, 0.02], 0.95),\n        # Case D\n        (0.8, [0.5, 0.01, 0.01], 0.75),\n        # Case E\n        (0.3, [0.1, 0.1], 0.995),\n    ]\n\n    results = []\n    for D0, a_list, Y_target in test_cases:\n        areas = np.array(a_list, dtype=float)\n        n_modules = len(areas)\n        k = np.ones(n_modules, dtype=int)\n        \n        # Pre-calculate constants for each module to speed up iterations\n        # Y_copy_i = exp(-D0 * a_i)\n        # F_copy_i = 1 - Y_copy_i\n        Y_copy = np.exp(-D0 * areas)\n        F_copy = 1.0 - Y_copy\n\n        def calculate_chip_yield(current_k):\n            \"\"\"Calculates total chip yield for a given redundancy vector k.\"\"\"\n            # Y_func_i = 1 - (F_copy_i)^k_i\n            Y_func = 1.0 - np.power(F_copy, current_k)\n            # Y_chip = product of all Y_func_i\n            return np.prod(Y_func)\n\n        # Initial state\n        current_yield = calculate_chip_yield(k)\n\n        # Greedy iteration loop\n        while current_yield  Y_target:\n            decision_metrics = []\n            \n            for i in range(n_modules):\n                # Decision metric D_j = (1/a_j) * (Y_copy_j * F_copy_j^k_j) / (1 - F_copy_j^k_j)\n                # Y_func_j = 1 - F_copy_j^k_j\n                f_copy_i_k = np.power(F_copy[i], k[i])\n                y_func_i = 1.0 - f_copy_i_k\n                \n                # Handle potential numerical instability if y_func_i is near 0.\n                if y_func_i  1e-12: # This would mean chip yield is already extremely low\n                    # In this regime, even a small improvement is huge relatively.\n                    # This metric would be very large, prioritizing this module.\n                    # The formula remains valid.\n                    pass\n\n                numerator = Y_copy[i] * f_copy_i_k\n                denominator = areas[i] * y_func_i\n                \n                metric = numerator / denominator\n                \n                # Store metric with tie-breaking info: (-metric, area, index)\n                # Sorting will maximize metric, then minimize area, then minimize index.\n                decision_metrics.append((-metric, areas[i], i))\n\n            # Sort to find the best candidate according to the rules\n            decision_metrics.sort()\n            \n            # The best module is the first one in the sorted list\n            best_module_index = decision_metrics[0][2]\n            \n            # Update redundancy for the best module\n            k[best_module_index] += 1\n            \n            # Recalculate chip yield\n            current_yield = calculate_chip_yield(k)\n\n        # Calculate final results\n        initial_areas = np.array(a_list, dtype=float)\n        added_area = float(np.sum((k - 1) * initial_areas))\n        achieved_yield = float(current_yield)\n\n        # Format result for the current case: [k1, ..., kn, added_area, yield]\n        final_k_list = k.tolist()\n        result_case = final_k_list + [added_area, achieved_yield]\n        results.append(str(result_case))\n    \n    # Print the final output in the required format\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}