## 引言
随着摩尔定律的演进，[集成电路设计](@entry_id:1126551)的复杂性呈指数级增长，传统的基于启发式和人工经验的电子设计自动化（EDA）工具正面临着前所未有的挑战。为了在巨大的设计空间中找到功耗、性能和面积（PPA）的最优解，工程师们亟需更智能、更高效的设计方法。机器学习（ML）作为一种强大的数据驱动范式，正逐渐成为突破EDA瓶瓶颈、推动半导体行业创新的关键技术。然而，如何将抽象的机器学习模型与具体的[物理设计](@entry_id:1129644)问题有效结合，是当前领域面临的核心知识缺口。

本文旨在系统性地介绍机器学习在EDA中的应用，为读者构建一个从理论到实践的完整知识体系。通过学习本文，你将掌握将电路设计挑战转化为机器学习任务的核心思想，并了解最前沿的建模技术如何赋能芯片设计的关键环节。
- 在 **“原理与机制”** 一章中，我们将深入探讨如何为机器学习表征电路，介绍图神经网络等关键学习机制，并讨论[混合模型](@entry_id:266571)、可微EDA等高级建模范式，同时关注模型的[可解释性](@entry_id:637759)与可靠性。
- 在 **“应用与跨学科交叉”** 一章中，我们将沿着标准的IC设计流程，展示机器学习在逻辑综合、[物理设计](@entry_id:1129644)、时序验证等具体场景中的实际应用，并探讨[主动学习](@entry_id:157812)、[元学习](@entry_id:635305)等贯穿式方法论。
- 在 **“动手实践”** 部分，你将有机会通过具体的编码练习，亲手实现[特征提取](@entry_id:164394)、模型预测和GNN[前向传播](@entry_id:193086)等核心操作，从而巩固所学知识。

现在，让我们一同深入探索，揭示机器学习如何重塑芯片设计的未来。

## 原理与机制

本章旨在深入探讨在电子设计自动化（EDA）中应用机器学习（ML）的核心科学原理与关键技术机制。我们将从如何为机器学习模型表征复杂的电子设计问题入手，逐步深入到学习算法的核心机制、高级建模范式，并最终讨论如何确保这些模型的可靠性、可解释性及在不断演进的工艺环境中的适用性。

### 面向机器学习的EDA问题表征

将[物理设计](@entry_id:1129644)问题转化为机器学习模型可以理解和处理的格式，是成功应用ML的首要步骤。这不仅涉及数据结构的转换，更关键的是要捕捉到决定设计质量的关键物理与拓扑信息。

#### 图与超图：电路的数学抽象

[数字电路](@entry_id:268512)的连接关系天然地形成了图结构。门、宏单元（macro）或I/O端口可以被抽象为图的**顶点（vertices）**，而连接这些顶点的线网（nets）则可以被抽象为**边（edges）**。然而，由于一个线网常常连接两个以上的单元，使用**[超图](@entry_id:270943)（hypergraphs）**来进行表征更为精确。在超图 $H=(V, E)$ 中，$V$ 是代表电路单元的顶点集合，而 $E$ 是一组**超边（hyperedges）**，每条超边（即一个线网）是 $V$ 的一个子集，包含了该线网连接的所有单元。这种表征方法能够无损地捕捉多终端网络的连接性 。

这种结构可以用**[关联矩阵](@entry_id:263683)（incidence matrix）** $A$ 来精确描述，其中 $A_{v,e} = 1$ 表示单元 $v$ 属于线网 $e$。而物理布局，即**布局（placement）**，则是一个将每个顶点 $v \in V$ 映射到芯片二维平面上一个坐标 $(x_v, y_v)$ 的函数。

#### 不变性与学习目标

EDA中的许多关键性能指标，如**[半周长线长](@entry_id:1125886)（Half-Perimeter Wirelength, HPWL）**，天然具有**[置换不变性](@entry_id:753356)（permutation invariance）**。HPWL作为线长的常用代理指标，其定义为线网包围盒的半周长：$h(e) = (x_{\max}(e)-x_{\min}(e)) + (y_{\max}(e)-y_{\min}(e))$。这个值仅依赖于一个线网中所有单元的坐标集合，而与这些单元的内部标签或索引顺序无关。如果我们将电路单元重新编号，只要坐标和连接关系保持一致，总线长不会改变 。这一特性至关重要，因为它要求我们设计的[机器学习模型](@entry_id:262335)也必须具备这种对节点顺序不敏感的能力。同时，像HPWL这样的指标不仅是置换不变的，也是**平移不变的（translation invariant）**，但会对**缩放（scaling）**做出[线性响应](@entry_id:146180)，这些都是模型需要学习或内置的先验知识 。

与HPWL这类具有良好数学性质的目标相反，布局问题的[可行解](@entry_id:634783)空间 $\mathcal{F}$ 是高度**非凸的（non-convex）**。任意两个单元不能重叠的约束，表现为一系列[析取范式](@entry_id:151536)（“或”逻辑），例如，对于单元 $i$ 和 $j$，它们的位置必须满足 $(x_i + w_i \le x_j) \lor (x_j + w_j \le x_i) \lor \dots$。这种约束的并集形成了非凸区域，使得[布局优化](@entry_id:1125092)成为一个[NP难问题](@entry_id:146946) 。

#### 领域知识驱动的特征工程

除了原始的图结构和坐标，为了让模型更高效地学习，我们需要提取具有物理意义的**领域特征（domain-specific features）**。这些特征将EDA的内在逻辑和物理规律编码为模型的输入，远比通用的、与任务无关的特征（如实例名称的字符串哈希值）更为有效。

例如，在预测布线拥塞时，一些关键的领域特征包括 ：
- **线网扇出（Net Fanout）**: 定义为一个线网驱动的负载（sinks）数量，$f(n) = |S(n)|$。[扇出](@entry_id:173211)越大的线网，通常需要更复杂的布线路径，更容易导致拥塞。
- **单元引脚数（Pin Count）**: 一个单元的信号引脚数量，$p(u) = |P_{\mathrm{sig}}(u)|$，反映了局部连接的密集程度。
- **线网时序关键性（Net Criticality）**: 通过静态时序分析（STA）得到。一个线网的关键性 $\kappa(n)$ 可以定义为其所在最差[时序路径](@entry_id:898372)的负裕量（negative slack）的归一化度量。例如，$\kappa(n) = \max_{p \ni n} \max(0, -s(p)/T)$，其中 $s(p)$ 是路径 $p$ 的裕量，$T$ 是[时钟周期](@entry_id:165839)。这个特征告诉模型哪些线网对设计性能至关重要，优化器可能会为它们分配更优的布线资源。
- **局部拥塞率（Local Congestion Ratio）**: 在一个给定的布线网格单元 $g$ 和金属层 $\ell$ 上，此值定义为预估的布线需求 $d(g,\ell)$ 与可用的布线容量 $c(g,\ell)$ 之比。这是拥塞最直接的表征。
- **宏单元阻塞掩码（Macro Blockage Mask）**: 一个二元特征 $b(g,\ell)$，表示特定网格和层是否被宏单元或布线禁区所覆盖。

这些特征将电路的拓扑、时序和物理约束转化为模型可以直接利用的信号，极大地降低了学习难度。

### 电路结构上的学习机制

有了合适的表征和特征，下一步是选择能够有效处理这些[结构化数据](@entry_id:914605)的模型。[图神经网络](@entry_id:136853)（GNNs）及其相关思想为直接在电路图上进行学习提供了强大的框架。

#### 图神经网络与消息传递

GNN的核心思想是通过**消息传递（message-passing）**机制，让每个节点能够聚合其邻居节点的信息来更新自身状态。一个通用的GNN层可以被形式化为 ：
$$
h_i^{(k+1)}=\phi\Big(h_i^{(k)},\; \square_{j\in\mathcal{N}(i)}\, \psi\big(h_i^{(k)},h_j^{(k)},e_{ij}\big)\Big)
$$
其中，$h_i^{(k)}$ 是节点 $i$ 在第 $k$ 层的[特征向量](@entry_id:151813)，$\mathcal{N}(i)$ 是节点 $i$ 的邻居集合。$\psi$ 是一个**消息函数**，用于从邻居 $j$ 生成传递给 $i$ 的消息；$\square$ 是一个**聚合函数（aggregation operator）**，用于将所有来自邻居的消息汇集起来；$\phi$ 是一个**[更新函数](@entry_id:275392)**，结合节点自身旧的状态和聚合后的消息来计算新状态。

关键在于聚合函数 $\square$ 必须是**置换不变的**。由于节点的邻居集合是无序的，聚合结果不能依赖于邻居被处理的顺序。常用的置换不变[聚合算子](@entry_id:746335)包括**求和（sum）**、**均值（mean）**和**最大值（max）**。简单地将邻居消息拼接起来再输入多层感知机（MLP）是错误的，因为拼接操作本身是顺序依赖的 。

一种更强大的聚合机制是**注意力（attention）**。例如，在[图注意力网络](@entry_id:1125735)（GAT）中，聚合操作是一个加权和，其权重 $\alpha_{ij}$ 是根据节点 $i$ 和邻居 $j$ 的特征动态计算出来的。
$$
\square_{j\in\mathcal{N}(i)}\psi(\cdot)=\sum_{j\in\mathcal{N}(i)}\alpha_{ij}\,\psi\big(h_i^{(k)},h_j^{(k)},e_{ij}\big)
$$
由于权重是通过对所有邻居的得分进行softmax归一化得到的，并且最终的求和操作是置换不变的，因此整个注意力聚合机制也是置换不变的 。

#### 面向集合的学习

GNN中的[置换不变性](@entry_id:753356)原理可以被推广到更[一般性](@entry_id:161765)的“集合学习”问题上。例如，在EDA中，一个线网是由一组引脚（pins）构成的集合，我们需要预测这个线网的某些属性（如[寄生电容](@entry_id:270891)）。这个问题可以被看作是学习一个定义在集合上的函数 $f(S)$，其中 $S=\{x_i\}_{i=1}^n$ 是引脚[特征向量](@entry_id:151813)的集合。

理论研究表明，任何作用于集合的置换不变函数 $f(S)$ 都可以被分解为如下形式 ：
$$
f(S) \approx \sigma\left(\sum_{x \in S} g(x)\right)
$$
其中，$g$ 是一个应用于每个元素的变换函数（例如一个MLP），$\sum$ 是一个置换不变的求和池化操作，而 $\sigma$ 是另一个应用于聚合结果的变换函数（例如另一个MLP）。这个结构，通常被称为**Deep Sets**架构，为学习线网级别等基于集合的预测任务提供了理论依据和通用框架。它不仅保证了[置换不变性](@entry_id:753356)，还能自然地处理可变大小的输入集合（不同线网的引脚数不同），并且通过外层函数 $\sigma$ 能够建模元素间的[非线性](@entry_id:637147)相互作用 。

### 学习范式与核心应用

将EDA问题进行数学表征并选择合适的学习机制后，我们可以根据问题的性质将其归入不同的[机器学习范式](@entry_id:637731)，并应用于解决具体的设计挑战。

#### EDA任务的[机器学习范式](@entry_id:637731)分类

EDA中的任务可以大致映射到三种主要的学习范式：[监督学习](@entry_id:161081)、[无监督学习](@entry_id:160566)和[强化学习](@entry_id:141144) 。

- **监督学习（Supervised Learning）**: 当我们拥有大量带有“正确答案”（标签）的数据时，适用监督学习。例如，在**[时序收敛](@entry_id:167567)（timing closure）**中，我们可以从[静态时序分析](@entry_id:177351)（STA）工具中提取[关键路径](@entry_id:265231)的特征（$x$），并使用签核（signoff）工具的精确结果作为标签（$y$），如预测的路径延时或推荐的工程变更指令（ECO）类型。模型 $f_\theta(x) \approx y$ 被训练用来预测这些标签，从而加速迭代修复过程 。

- **[无监督学习](@entry_id:160566)（Unsupervised Learning）**: 当我们只有数据而没有标签时，可以使用[无监督学习](@entry_id:160566)来发现数据中的内在结构。例如，对大量的布线违规模式进行聚类，以识别常见的设计规则问题根源。然而，[无监督学习](@entry_id:160566)本身通常不直接提供优化决策。

- **强化学习（Reinforcement Learning）**: 对于需要做出一系列决策以达到最终目标的复杂[组合优化](@entry_id:264983)问题，强化学习（RL）提供了一个强大的框架。RL将问题建模为一个[马尔可夫决策过程](@entry_id:140981)（MDP），其中智能体（agent）在某个状态（state）下采取一个动作（action），转移到新的状态，并获得一个奖励（reward），其目标是最大化累积奖励。
    - **布局（Placement）**: 可以被建模为一个序列决策过程。状态 $s_t$ 编码了部分完成的布局（如已放置的宏单元、拥塞图），动作 $a_t$ 是选择下一个单元并将其放置在合法位置。奖励 $r_t$ 可以设计为与最终的线长、拥塞和时序等指标相关。智能体通过试错学习一个策略，以生成高质量的布局方案 。
    - **布线（Routing）**: 同样可以建模为RL问题。状态 $s_t$ 描述了当前的布线资源占用情况和部分完成的路径，动作 $a_t$ 是将线网延伸到下一个网格点。奖励函数会惩罚设计规则违规（DRC）和过长的线，并奖励成功的连接 。

#### 应用案例：基于代理模型的[设计空间探索](@entry_id:1123590)

EDA工具链通常包含大量可调参数（如综合、布局和布线中的各种开关和权重）。**[设计空间探索](@entry_id:1123590)（Design Space Exploration, DSE）** 的目标是在这个高维参数空间 $\mathcal{X}$ 中找到一组最优参数 $\mathbf{x}$，以最小化一个或多个设计指标（如功耗、性能、面积，即PPA）。评估每组参数的成本非常高，因为它需要运行完整的EDA流程。

**基于代理模型的搜索（Surrogate-assisted search）**，特别是**贝叶斯优化（Bayesian Optimization）**，是解决此类昂贵黑盒优化问题的有效方法。其核心思想是：
1.  建立一个廉价的**代理模型（surrogate model）**，通常是**高斯过程（Gaussian Process, GP）**，来近似昂贵的真实[目标函数](@entry_id:267263) $f(\mathbf{x})$。GP不仅能提供在任意点 $\mathbf{x}$ 的预测均值 $\mu(\mathbf{x})$，还能提供预测的不确定性（方差）$\sigma^2(\mathbf{x})$。
2.  使用一个**[采集函数](@entry_id:168889)（acquisition function）**来智能地选择下一个要评估的参数点。[采集函数](@entry_id:168889)会平衡**探索（exploration）**（在模型不确定的区域进行采样，可能会发现更好的解）和**利用（exploitation）**（在模型预测最优的区域进行采样）。

一个常用的采集函数是**[期望提升](@entry_id:749168)（Expected Improvement, EI）**。对于最小化问题，给定当前观测到的最佳值 $f^\star$，在点 $\mathbf{x}$ 的EI定义为期望能比 $f^\star$ 更好的量。其[闭式](@entry_id:271343)解为 ：
$$
\mathrm{EI}(\mathbf{x}) = (f^\star - \mu(\mathbf{x})) \Phi(z(\mathbf{x})) + \sigma(\mathbf{x}) \phi(z(\mathbf{x}))
$$
其中 $z(\mathbf{x}) = \frac{f^\star - \mu(\mathbf{x})}{\sigma(\mathbf{x})}$，$\Phi(\cdot)$ 和 $\phi(\cdot)$ 分别是[标准正态分布](@entry_id:184509)的[累积分布函数](@entry_id:143135)（CDF）和[概率密度函数](@entry_id:140610)（PDF）。即使某点的预测均值 $\mu(\mathbf{x})$ 比当前最优值 $f^\star$ 差，只要其不确定性 $\sigma(\mathbf{x})$ 足够大，EI值也可能很高，从而引导算法去探索这个有潜力的区域。通过迭代地使用EI选择下一个评估点并更新GP模型，[贝叶斯优化](@entry_id:175791)能用远少于穷举搜索的评估次数找到接近全局最优的解 。

### 高级建模技术

为了应对EDA问题中更深层次的挑战，研究人员开发了一系列更复杂的建模技术，旨在将物理知识更紧密地融入[机器学习模型](@entry_id:262335)，或构建端到端的优化框架。

#### [混合模型](@entry_id:266571)：融合物理定律与数据驱动方法

纯粹的数据驱动模型在面对训练数据分布之外的情况时，其泛化能力往往有限。**混合模型（Hybrid Models）**，或称**[物理信息](@entry_id:152556)机器学习（Physics-Informed Machine Learning, PIML）**，通过将已知的物理或分析模型与[机器学习模型](@entry_id:262335)相结合，旨在克服这一局限。

一种常见的[混合模型](@entry_id:266571)结构是加性模型 ：
$$
f_h(\mathbf{x}) = f_a(\mathbf{x}) + g_{\boldsymbol{\phi}}(\mathbf{x})
$$
这里，$f_a(\mathbf{x})$ 是一个基于物理定律的分析模型（例如，基于Elmore延时的 $RC$ 延时模型，其延时与 $L^2$ 成正比），它捕捉了问题的主要趋势和正确的[渐近行为](@entry_id:160836)。$g_{\boldsymbol{\phi}}(\mathbf{x})$ 是一个[机器学习模型](@entry_id:262335)（如神经网络），其任务是学习和修正分析模型未能捕捉到的**残差（residual）**，如边缘电容效应、串扰等。

这种结构有两大优势：
1.  **提升外推能力（Extrapolation）**: 当模型需要对远离训练数据范围的输入（例如更长的线）进行预测时，其行为将由物理模型 $f_a(\mathbf{x})$ 主导，这通常比纯数据驱动模型的无根据猜测更为可靠 。
2.  **提高数据效率（Data Efficiency）**: 由于物理模型已经处理了大部分的“简单”工作，[机器学习模型](@entry_id:262335) $g_{\boldsymbol{\phi}}$ 只需要学习一个更“简单”的残差函数。从[统计学习理论](@entry_id:274291)的角度看，这相当于减小了[假设空间](@entry_id:635539)的复杂度（例如，Rademacher复杂度），从而降低了对大量训练样本的需求 。

另一种实现方式是在训练目标中加入物理约束作为正则化项，强制模型输出满足诸如[基尔霍夫电流定律](@entry_id:270632)（KCL）或网络无源性等物理规律，从而在没有直接数据标签的区域也能引导模型学习物理上合理的行为 。

#### 可微EDA：端到端梯度优化

传统的EDA流程由多个离散的、非可微的模块串联而成，这使得基于梯度的端到端优化变得不可能。**可微EDA（Differentiable EDA）** 的目标是构建一个从输入（网表）到输出（PPA指标）完全可微的计算流程。

这需要将流程中的非可[微操作](@entry_id:751957)替换为它们的光滑、可微的**代理函数（surrogate functions）** ：
- **线长模型**: 标准的HPWL包含不可微的 `max` 和 `min` 操作。可以用**Log-Sum-Exp (LSE)** 函数来平滑地逼近它们。
- **布[线与](@entry_id:177118)拥塞模型**: 离散的布线路径可以被连续的**密度场（density field）**所取代。例如，可以将布线建模为[扩散过程](@entry_id:268015)，其电势场 $u$ 是一个[椭圆偏微分方程](@entry_id:178258)（PDE）的解，或者一个线性[电阻网络](@entry_id:263830)的解。根据[隐函数定理](@entry_id:147247)，这些方程的解相对于模型参数是可微的，从而允许梯度通过求解器[反向传播](@entry_id:199535)。
- **约束**: 像单元重叠这样的硬约束，可以用**可微的[障碍函数](@entry_id:168066)（barrier functions）**或软惩罚项来近似。

一旦整个流程变得可微，我们就可以直接计算最终设计目标（如总线长、拥塞）关于上游设计参数（如单元坐标）的梯度。这使得我们可以使用强大的梯度下降算法来直接优化布局，实现真正的**协同优化（Co-optimization）**，即同时考虑多个相互依赖的目标，而不是孤立地、串行地处理它们  。例如，一个协同优化的目标函数可以显式地将时序、功耗和拥塞的代理模型耦合在一起，共同指导布局决策，从而避免早期阶段的局部最优选择损害最终的全局设计质量 。

### EDA机器学习中的信任与可靠性

将机器学习模型应用于高风险的芯片设计决策，必须建立对其预测的信任。这不仅要求模型准确，还要求我们能理解其预测的不确定性，并能解释其决策过程。

#### [不确定性量化](@entry_id:138597)：知道模型何时“不知道”

[机器学习模型](@entry_id:262335)的预测不确定性可以分解为两种类型 ：
- **[偶然不确定性](@entry_id:634772)（Aleatoric Uncertainty）**: 这是数据本身固有的、不可约减的随机性。在EDA中，它源于制造过程中的物理变化（[PVT变化](@entry_id:1130319)）、[晶体管失配](@entry_id:1133337)和电路运行时的噪声。即使拥有完美的模型，这种不确定性依然存在。
- **认知不确定性（Epistemic Uncertainty）**: 这源于模型自身知识的局限，例如训练数据不足或模型结构不当。原则上，认知不确定性可以通过收集更多数据或改进模型来降低。

在贝叶斯模型中，这两种不确定性可以通过总方差定律进行分解。总预测方差等于[偶然不确定性](@entry_id:634772)（在模型参数上的期望）与认知不确定性（模型预测均值的方差）之和。随着数据量的增加，认知不确定性会趋于零，但[偶然不确定性](@entry_id:634772)将收敛到数据内在的噪声水平 。

#### 校准：确保[概率预测](@entry_id:1130184)的真实性

对于需要做风险决策的任务（如决定是否流片），模型的点预测（如预测的WNS值）是不够的，我们需要准确的**概率预测**（如WNS小于0的概率）。一个模型的概率预测被称为**良好校准的（well-calibrated）**，如果其预测的概率与事件发生的实际频率相符。例如，对于模型预测有10%违规风险的所有设计，我们应该发现其中确实有大约10%的设计最终违规了 。

在[贝叶斯决策理论](@entry_id:909090)下，最优决策依赖于对[损失函数](@entry_id:634569)和校准后验概率的精确计算。一个过自信（即低估不确定性）的模型可能会低估风险，导致做出代价高昂的错误决策。因此，校准对于任何基于概率的[风险管理](@entry_id:141282)都至关重要 。

#### 可解释性：理解模型的决策依据

为了让设计工程师信任并调试M[L模](@entry_id:1126990)型，理解模型为何做出特定预测至关重要。**[特征归因](@entry_id:926392)（feature attribution）**方法旨在量化每个输入特征对模型输出的贡献。

两种主流的归因方法是：
- **[Shapley值](@entry_id:634984)（Shapley Values）**: 源于合作博弈论，[Shapley值](@entry_id:634984)为每个特征分配一个“贡献值”，该值是其在所有可能的特征组合（联盟）中的平均边际贡献。它具有“效率”（所有特征的贡献值之和等于总预测值与基线预测值之差）、“对称性”等优良的公理化性质。[Shapley值](@entry_id:634984)是模型无关的，不要求模型可微，可以解释任何[黑盒模型](@entry_id:1121697) 。
- **[积分梯度](@entry_id:637152)（Integrated Gradients, IG）**: 这是一种基于梯度的归因方法。它通过对从一个**基线输入（baseline input）** $\mathbf{x}'$（代表一个无信息或参考状态）到当前输入 $\mathbf{x}$ 的路径上的梯度进行积分，来计算每个特征的贡献。IG方法也满足与[Shapley值](@entry_id:634984)效率公理类似的“完备性”（completeness）属性，即所有特征的归因值之和也等于 $f(\mathbf{x}) - f(\mathbf{x}')$。它要求模型是可微的，尤其适用于解释深度学习模型 。

在EDA应用中，选择一个物理上可行的设计状态作为基线至关重要，否则归因结果可能因模型在分布外区域的不可靠行为而失真 。

### 模型的生命周期与部署

最后，一个在特定工艺节点上训练好的模型，如何适应新的工艺节点，是其实用价值的关键。这就引出了**迁移学习（Transfer Learning）**和**领[域适应](@entry_id:637871)（Domain Adaptation）**的问题。

当我们将模型从源领域（如14nm工艺）迁移到目标领域（如7nm工艺）时，数据分布通常会发生变化，即 $p_{\mathcal{S}}(\mathbf{x}, y) \neq p_{\mathcal{T}}(\mathbf{x}, y)$。这种**[分布偏移](@entry_id:915633)（distribution shift）**可以细分为三种主要类型 ：
- **[协变量偏移](@entry_id:636196)（Covariate Shift）**: 输入特征的边缘分布 $p(\mathbf{x})$ 改变，但输入与输出之间的关系 $p(y|\mathbf{x})$ 保持不变。例如，从14nm到7nm，单元尺寸和布线层堆叠的变化导致了描述布局几何的特征分布改变，但这些几何特征与最终拥塞之间的物理关系可能保持相似。
- **标签偏移（Label Shift）**: 输出标签的边缘分布 $p(y)$ 改变，但给定标签下特征的[条件分布](@entry_id:138367) $p(\mathbf{x}|y)$ 保持不变。例如，由于7nm的[设计规则](@entry_id:1123586)更严格，DRC违规的热点（hotspot）标签比例增加了，但违规模式本身的视觉特征（在给定它是一个违规的情况下）可能保持不变。
- **概念偏移（Concept Shift）**: 输入与输出之间的根本关系 $p(y|\mathbf{x})$ 发生了改变。这是最根本的挑战。例如，在7nm节点，由于新的晶体管物理特性和不同的线电阻，相同的电路拓扑特征可能对应着完全不同的[时序违规](@entry_id:177649)概率。

识别出主导的偏移类型，有助于选择合适的[迁移学习](@entry_id:178540)策略——如实例重加权（针对[协变量偏移](@entry_id:636196)）、模型微调（fine-tuning）或[特征对齐](@entry_id:634064)等——来有效地将源领域知识迁移到目标领域，特别是在目标领域标记数据稀缺的情况下 。