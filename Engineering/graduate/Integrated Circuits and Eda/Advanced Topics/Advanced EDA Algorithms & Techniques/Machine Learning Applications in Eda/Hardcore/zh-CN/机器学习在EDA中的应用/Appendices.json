{
    "hands_on_practices": [
        {
            "introduction": "将复杂的电子设计问题转化为机器学习模型可以理解的格式，是应用智能算法的第一步。这个练习将指导你完成一个基本但至关重要的任务：从一个给定的单元布局和网表（netlist）中提取特征。你将计算半周长线长（HPWL），这是一个在布线和可布线性预测中广泛使用的指标，并学习如何对其进行标准化，为模型输入做准备。",
            "id": "4281028",
            "problem": "考虑在电子设计自动化 (EDA) 中，如何从一个合法的单元布局为机器学习 (ML) 构建输入特征。一组五个方向为 $N$ (无旋转或镜像) 的实例放置如下：实例 $U_1$ 位于 $(10,10)\\,\\mathrm{\\mu m}$，实例 $U_2$ 位于 $(16,13)\\,\\mathrm{\\mu m}$，实例 $U_3$ 位于 $(22,9)\\,\\mathrm{\\mu m}$，实例 $U_4$ 位于 $(15,18)\\,\\mathrm{\\mu m}$，以及实例 $U_5$ 位于 $(20,13)\\,\\mathrm{\\mu m}$。每个实例都有相对于实例原点的局部偏移量的引脚：在 $U_1$ 上，引脚 $A$ 的偏移量为 $(1,0)\\,\\mathrm{\\mu m}$，引脚 $B$ 的偏移量为 $(0,2)\\,\\mathrm{\\mu m}$；在 $U_2$ 上，引脚 $A$ 的偏移量为 $(-1,1)\\,\\mathrm{\\mu m}$，引脚 $B$ 的偏移量为 $(2,-1)\\,\\mathrm{\\mu m}$；在 $U_3$ 上，引脚 $A$ 的偏移量为 $(0,-2)\\,\\mathrm{\\mu m}$，引脚 $C$ 的偏移量为 $(3,1)\\,\\mathrm{\\mu m}$；在 $U_4$ 上，引脚 $C$ 的偏移量为 $(-2,2)\\,\\mathrm{\\mu m}$，引脚 $D$ 的偏移量为 $(1,-3)\\,\\mathrm{\\mu m}$；在 $U_5$ 上，引脚 $A$ 的偏移量为 $(0,0)\\,\\mathrm{\\mu m}$。由于方向为 $N$，引脚的绝对坐标通过实例坐标与引脚局部偏移量的矢量相加得到。\n\n网络列表如下：\n- 网络 $e_1$：引脚 $U_1/A$、$U_2/B$、$U_3/A$。\n- 网络 $e_2$：引脚 $U_1/B$、$U_4/C$。\n- 网络 $e_3$：引脚 $U_2/A$、$U_3/C$、$U_4/D$、$U_5/A$。\n- 网络 $e_4$：引脚 $U_1/A$、$U_2/A$、$U_5/A$。\n\n使用半周长线长 (HPWL) 的标准定义：对于一个引脚坐标为 $\\{(x_i,y_i)\\}_{i \\in e}$ 的网络 $e$，其 HPWL 特征 $w_e$ 是包含其所有引脚的最小轴对齐边界矩形的水平跨度与垂直跨度之和，即 $w_e$ 等于水平跨度 $\\max_{i \\in e} x_i - \\min_{i \\in e} x_i$ 加上垂直跨度 $\\max_{i \\in e} y_i - \\min_{i \\in e} y_i$。根据给定的布局和网络列表，计算 HPWL 特征 $w_{e_1}$、$w_{e_2}$、$w_{e_3}$ 和 $w_{e_4}$。然后，使用样本均值和无偏样本标准差（即，对于 $n=4$，除以分母为 $n-1$ 的样本方差的平方根）对集合 $\\{w_{e_1},w_{e_2},w_{e_3},w_{e_4}\\}$ 中的 HPWL 特征进行 $z$-score 标准化。将网络 $e_3$ 的标准化 HPWL 作为单一的封闭形式精确代数表达式提供。标准化后的 HPWL 是无单位的。不要对答案进行四舍五入。",
            "solution": "所述问题是有效的。它在科学上基于电子设计自动化 (EDA) 的原理和标准统计方法。半周长线长 (HPWL) 和 z-score 标准化的定义是清晰且标准的。所有必要的数据，包括实例布局、引脚偏移量和网络列表，均已提供，并且没有内部矛盾。该问题是适定的，并存在一个唯一的、可验证的解。\n\n解答过程遵循以下步骤：\n$1$。通过将局部引脚偏移量加到它们各自的实例坐标上，确定所有引脚的绝对坐标。\n$2$。计算四个网络中每一个的 HPWL 特征，记为 $w_{e_1}$、$w_{e_2}$、$w_{e_3}$ 和 $w_{e_4}$。\n$3$。计算生成的 HPWL 值集合 $\\{w_{e_1}, w_{e_2}, w_{e_3}, w_{e_4}\\}$ 的样本均值 $\\bar{w}$ 和无偏样本标准差 $s$。\n$4$。使用公式 $z_{e_3} = (w_{e_3} - \\bar{w}) / s$ 计算 $w_{e_3}$ 的 z-score。\n\n步骤 1：绝对引脚坐标\n所有坐标的单位均为 $\\mathrm{\\mu m}$。引脚的绝对坐标由实例坐标和引脚的局部偏移量的矢量和给出。\n\n对于网络 $e_1$ (引脚 $U_1/A, U_2/B, U_3/A$):\n- 引脚 $U_1/A$: $(10,10) + (1,0) = (11,10)$\n- 引脚 $U_2/B$: $(16,13) + (2,-1) = (18,12)$\n- 引脚 $U_3/A$: $(22,9) + (0,-2) = (22,7)$\n\n对于网络 $e_2$ (引脚 $U_1/B, U_4/C$):\n- 引脚 $U_1/B$: $(10,10) + (0,2) = (10,12)$\n- 引脚 $U_4/C$: $(15,18) + (-2,2) = (13,20)$\n\n对于网络 $e_3$ (引脚 $U_2/A, U_3/C, U_4/D, U_5/A$):\n- 引脚 $U_2/A$: $(16,13) + (-1,1) = (15,14)$\n- 引脚 $U_3/C$: $(22,9) + (3,1) = (25,10)$\n- 引脚 $U_4/D$: $(15,18) + (1,-3) = (16,15)$\n- 引脚 $U_5/A$: $(20,13) + (0,0) = (20,13)$\n\n对于网络 $e_4$ (引脚 $U_1/A, U_2/A, U_5/A$):\n- 引脚 $U_1/A$: $(11,10)$\n- 引脚 $U_2/A$: $(15,14)$\n- 引脚 $U_5/A$: $(20,13)$\n\n步骤 2：每个网络的 HPWL 计算\nHPWL, $w_e$, 定义为 $(\\max_{i \\in e} x_i - \\min_{i \\in e} x_i) + (\\max_{i \\in e} y_i - \\min_{i \\in e} y_i)$。\n\n对于网络 $e_1$，其引脚坐标为 $\\{(11,10), (18,12), (22,7)\\}$:\n- 水平跨度: $\\max(11, 18, 22) - \\min(11, 18, 22) = 22 - 11 = 11$\n- 垂直跨度: $\\max(10, 12, 7) - \\min(10, 12, 7) = 12 - 7 = 5$\n- $w_{e_1} = 11 + 5 = 16$\n\n对于网络 $e_2$，其引脚坐标为 $\\{(10,12), (13,20)\\}$:\n- 水平跨度: $\\max(10, 13) - \\min(10, 13) = 13 - 10 = 3$\n- 垂直跨度: $\\max(12, 20) - \\min(12, 20) = 20 - 12 = 8$\n- $w_{e_2} = 3 + 8 = 11$\n\n对于网络 $e_3$，其引脚坐标为 $\\{(15,14), (25,10), (16,15), (20,13)\\}$:\n- 水平跨度: $\\max(15, 25, 16, 20) - \\min(15, 25, 16, 20) = 25 - 15 = 10$\n- 垂直跨度: $\\max(14, 10, 15, 13) - \\min(14, 10, 15, 13) = 15 - 10 = 5$\n- $w_{e_3} = 10 + 5 = 15$\n\n对于网络 $e_4$，其引脚坐标为 $\\{(11,10), (15,14), (20,13)\\}$:\n- 水平跨度: $\\max(11, 15, 20) - \\min(11, 15, 20) = 20 - 11 = 9$\n- 垂直跨度: $\\max(10, 14, 13) - \\min(10, 14, 13) = 14 - 10 = 4$\n- $w_{e_4} = 9 + 4 = 13$\n\nHPWL 特征集为 $W = \\{16, 11, 15, 13\\}$。\n\n步骤 3：统计计算\n我们有 $n=4$ 个 HPWL 值的样本。\n样本均值 $\\bar{w}$ 为：\n$$ \\bar{w} = \\frac{1}{4} (16 + 11 + 15 + 13) = \\frac{55}{4} $$\n无偏样本方差 $s^2$ 的计算分母为 $n-1 = 3$：\n$$ s^2 = \\frac{1}{3} \\sum_{w \\in W} (w - \\bar{w})^2 $$\n$$ s^2 = \\frac{1}{3} \\left[ \\left(16 - \\frac{55}{4}\\right)^2 + \\left(11 - \\frac{55}{4}\\right)^2 + \\left(15 - \\frac{55}{4}\\right)^2 + \\left(13 - \\frac{55}{4}\\right)^2 \\right] $$\n$$ s^2 = \\frac{1}{3} \\left[ \\left(\\frac{9}{4}\\right)^2 + \\left(-\\frac{11}{4}\\right)^2 + \\left(\\frac{5}{4}\\right)^2 + \\left(-\\frac{3}{4}\\right)^2 \\right] $$\n$$ s^2 = \\frac{1}{3} \\left[ \\frac{81}{16} + \\frac{121}{16} + \\frac{25}{16} + \\frac{9}{16} \\right] = \\frac{1}{3} \\left( \\frac{236}{16} \\right) = \\frac{1}{3} \\left( \\frac{59}{4} \\right) = \\frac{59}{12} $$\n无偏样本标准差 $s$ 是方差的平方根：\n$$ s = \\sqrt{\\frac{59}{12}} $$\n\n步骤 4：$w_{e_3}$ 的 Z-score 标准化\n$w_{e_3}=15$ 的 z-score 为：\n$$ z_{e_3} = \\frac{w_{e_3} - \\bar{w}}{s} = \\frac{15 - \\frac{55}{4}}{\\sqrt{\\frac{59}{12}}} = \\frac{\\frac{60-55}{4}}{\\sqrt{\\frac{59}{12}}} = \\frac{\\frac{5}{4}}{\\frac{\\sqrt{59}}{\\sqrt{12}}} $$\n简化表达式：\n$$ z_{e_3} = \\frac{5}{4} \\frac{\\sqrt{12}}{\\sqrt{59}} = \\frac{5}{4} \\frac{\\sqrt{4 \\cdot 3}}{\\sqrt{59}} = \\frac{5}{4} \\frac{2\\sqrt{3}}{\\sqrt{59}} = \\frac{5\\sqrt{3}}{2\\sqrt{59}} $$\n为了提供一个具有有理分母的封闭形式表达式，我们将分子和分母同乘以 $\\sqrt{59}$：\n$$ z_{e_3} = \\frac{5\\sqrt{3}}{2\\sqrt{59}} \\cdot \\frac{\\sqrt{59}}{\\sqrt{59}} = \\frac{5\\sqrt{3 \\cdot 59}}{2 \\cdot 59} = \\frac{5\\sqrt{177}}{118} $$\n这是最终的、精确的、无单位的值。",
            "answer": "$$\\boxed{\\frac{5\\sqrt{177}}{118}}$$"
        },
        {
            "introduction": "一旦我们有了特征，下一步就是使用训练好的模型进行预测，并评估其准确性。本练习模拟了一个常见的EDA场景：使用机器学习模型快速预测时序路径的余量（slack），以减少对耗时的静态时序分析（STA）的依赖。通过将一个简单的线性模型的预测结果与STA的基准值进行比较，你将亲身体验模型评估的过程，并计算平均绝对误差（MAE）这一关键性能指标。",
            "id": "4281022",
            "problem": "一个电子设计自动化（EDA）的研究小组正在训练一个机器学习模型，用于预测路径时序裕量，以减少进行完整静态时序分析（STA）的频率。对于一个给定的设计，训练好的预测器是一个线性模型，它将一个工程化的特征向量映射到一个预测的裕量。该模型接收一个特征向量 $\\mathbf{x} \\in \\mathbb{R}^{5}$、一个学习到的权重向量 $\\mathbf{w} \\in \\mathbb{R}^{5}$ 和一个学习到的偏置 $b \\in \\mathbb{R}$，并产生一个预测的裕量。这五个特征是根据路径特性推导出的工程化无量纲指数，并且在训练和推理过程中具有一致的内部尺度；示例包括驱动强度指数、扇出负载指数、反压摆率指数、互连长度指数和高阈值单元比例指数。目标是通过将模型的预测值与参考的STA裕量进行比较，来评估该模型在三条未见过的路径上的表现。\n\n给定学习到的参数 $\\mathbf{w}$ 和 $b$：\n- $\\mathbf{w} = [\\,0.12,\\,-0.08,\\,0.50,\\,-0.02,\\,-0.40\\,]$,\n- $b = 0.05$.\n\n同时给定三个路径的特征向量及其对应的STA裕量（参考值），所有裕量均以纳秒为单位：\n- 路径 $1$: $\\mathbf{x}^{(1)} = [\\,10,\\,5,\\,0.3,\\,20,\\,0.1\\,]$, STA 裕量 $s^{\\mathrm{STA}}_{1} = 0.60$。\n- 路径 $2$: $\\mathbf{x}^{(2)} = [\\,8,\\,7,\\,0.5,\\,15,\\,0.2\\,]$, STA 裕量 $s^{\\mathrm{STA}}_{2} = 0.28$。\n- 路径 $3$: $\\mathbf{x}^{(3)} = [\\,12,\\,3,\\,0.2,\\,25,\\,0.05\\,]$, STA 裕量 $s^{\\mathrm{STA}}_{3} = 0.70$。\n\n使用该线性模型计算每条路径的预测裕量，然后计算三个预测值与三个STA裕量之间的平均绝对误差。以纳秒表示最终的平均绝对误差。将最终答案四舍五入到四位有效数字。",
            "solution": "对问题陈述进行验证。\n**第一步：提取已知条件**\n- 机器学习模型：用于预测裕量 $s^{\\mathrm{pred}}$ 的线性模型。\n- 模型形式：$s^{\\mathrm{pred}} = \\mathbf{w}^T \\mathbf{x} + b$。\n- 特征向量：$\\mathbf{x} \\in \\mathbb{R}^{5}$。\n- 权重向量：$\\mathbf{w} \\in \\mathbb{R}^{5}$。\n- 偏置：$b \\in \\mathbb{R}$。\n- 学习到的权重向量：$\\mathbf{w} = [\\,0.12,\\,-0.08,\\,0.50,\\,-0.02,\\,-0.40\\,]$。\n- 学习到的偏置：$b = 0.05$。\n- 路径 $1$ 特征向量：$\\mathbf{x}^{(1)} = [\\,10,\\,5,\\,0.3,\\,20,\\,0.1\\,]$。\n- 路径 $1$ STA 裕量：$s^{\\mathrm{STA}}_{1} = 0.60$ ns。\n- 路径 $2$ 特征向量：$\\mathbf{x}^{(2)} = [\\,8,\\,7,\\,0.5,\\,15,\\,0.2\\,]$。\n- 路径 $2$ STA 裕量：$s^{\\mathrm{STA}}_{2} = 0.28$ ns。\n- 路径 $3$ 特征向量：$\\mathbf{x}^{(3)} = [\\,12,\\,3,\\,0.2,\\,25,\\,0.05\\,]$。\n- 路径 $3$ STA 裕量：$s^{\\mathrm{STA}}_{3} = 0.70$ ns。\n- 任务：计算每条路径的预测裕量以及预测值与STA裕量之间的平均绝对误差（MAE）。\n- 最终答案要求：四舍五入到四位有效数字。\n\n**第二步：使用提取的已知条件进行验证**\n根据验证标准对问题进行评估。\n- **科学基础**：该问题基于线性回归的标准应用，这是一种基础的机器学习技术，用于预测电子设计自动化（EDA）中的时序裕量。这是该领域一种成熟且有效的方法。\n- **适定性**：该问题是适定的。它提供了一个清晰的数学模型，所有必要的参数（$\\mathbf{w}$, $b$）和输入（$\\mathbf{x}^{(i)}$）的数值，以及待计算量（平均绝对误差）的精确定义。存在唯一解。\n- **客观性**：该问题使用客观、技术性的语言表述，没有歧义或主观论断。\n- **完整性与一致性**：该问题是自洽且一致的。提供了所有必要的数据，给定信息中没有矛盾。向量的维度与指定的操作兼容。\n\n**第三步：结论与行动**\n该问题被认为是 **有效的**，因为它满足了一个可解科学问题的所有标准。现在开始求解过程。\n\n预测裕量 $s^{\\mathrm{pred}}$ 使用以下线性模型计算：\n$$ s^{\\mathrm{pred}} = \\mathbf{w}^T \\mathbf{x} + b = \\sum_{j=1}^{5} w_j x_j + b $$\n给定的参数是权重向量 $\\mathbf{w} = [\\,0.12,\\, -0.08,\\, 0.50,\\, -0.02,\\, -0.40\\,]^T$ 和偏置 $b = 0.05$。\n\n我们计算三条路径中每一条的预测裕量。\n\n对于路径 1，其特征向量为 $\\mathbf{x}^{(1)} = [\\,10,\\, 5,\\, 0.3,\\, 20,\\, 0.1\\,]^T$：\n$$ s^{\\mathrm{pred}}_{1} = \\mathbf{w}^T \\mathbf{x}^{(1)} + b $$\n$$ s^{\\mathrm{pred}}_{1} = (0.12)(10) + (-0.08)(5) + (0.50)(0.3) + (-0.02)(20) + (-0.40)(0.1) + 0.05 $$\n$$ s^{\\mathrm{pred}}_{1} = 1.20 - 0.40 + 0.15 - 0.40 - 0.04 + 0.05 $$\n$$ s^{\\mathrm{pred}}_{1} = 1.40 - 0.84 = 0.56 $$\n\n对于路径 2，其特征向量为 $\\mathbf{x}^{(2)} = [\\,8,\\, 7,\\, 0.5,\\, 15,\\, 0.2\\,]^T$：\n$$ s^{\\mathrm{pred}}_{2} = \\mathbf{w}^T \\mathbf{x}^{(2)} + b $$\n$$ s^{\\mathrm{pred}}_{2} = (0.12)(8) + (-0.08)(7) + (0.50)(0.5) + (-0.02)(15) + (-0.40)(0.2) + 0.05 $$\n$$ s^{\\mathrm{pred}}_{2} = 0.96 - 0.56 + 0.25 - 0.30 - 0.08 + 0.05 $$\n$$ s^{\\mathrm{pred}}_{2} = 1.26 - 0.94 = 0.32 $$\n\n对于路径 3，其特征向量为 $\\mathbf{x}^{(3)} = [\\,12,\\, 3,\\, 0.2,\\, 25,\\, 0.05\\,]^T$：\n$$ s^{\\mathrm{pred}}_{3} = \\mathbf{w}^T \\mathbf{x}^{(3)} + b $$\n$$ s^{\\mathrm{pred}}_{3} = (0.12)(12) + (-0.08)(3) + (0.50)(0.2) + (-0.02)(25) + (-0.40)(0.05) + 0.05 $$\n$$ s^{\\mathrm{pred}}_{3} = 1.44 - 0.24 + 0.10 - 0.50 - 0.02 + 0.05 $$\n$$ s^{\\mathrm{pred}}_{3} = 1.59 - 0.76 = 0.83 $$\n\n接下来，我们计算预测裕量（$s^{\\mathrm{pred}}_{i}$）与参考STA裕量（$s^{\\mathrm{STA}}_{i}$）之间的平均绝对误差（MAE）。对于 $N=3$ 个样本，MAE的公式为：\n$$ \\text{MAE} = \\frac{1}{N} \\sum_{i=1}^{N} |s^{\\mathrm{pred}}_{i} - s^{\\mathrm{STA}}_{i}| $$\n给定的参考STA裕量为 $s^{\\mathrm{STA}}_{1} = 0.60$，$s^{\\mathrm{STA}}_{2} = 0.28$ 和 $s^{\\mathrm{STA}}_{3} = 0.70$。\n\n首先，我们计算每条路径的绝对误差：\n- 路径 $1$ 误差：$|s^{\\mathrm{pred}}_{1} - s^{\\mathrm{STA}}_{1}| = |0.56 - 0.60| = |-0.04| = 0.04$。\n- 路径 $2$ 误差：$|s^{\\mathrm{pred}}_{2} - s^{\\mathrm{STA}}_{2}| = |0.32 - 0.28| = |0.04| = 0.04$。\n- 路径 $3$ 误差：$|s^{\\mathrm{pred}}_{3} - s^{\\mathrm{STA}}_{3}| = |0.83 - 0.70| = |0.13| = 0.13$。\n\n现在，我们计算MAE：\n$$ \\text{MAE} = \\frac{1}{3} (0.04 + 0.04 + 0.13) $$\n$$ \\text{MAE} = \\frac{1}{3} (0.21) $$\n$$ \\text{MAE} = 0.07 $$\n\n问题要求将最终答案四舍五入到四位有效数字。计算值为 $0.07$。为了用四位有效数字表示，我们添加后缀零，得到 $0.07000$。所有裕量和误差的单位都是纳秒。",
            "answer": "$$\\boxed{0.07000}$$"
        },
        {
            "introduction": "现代EDA中的许多挑战，从时序预测到可制造性分析，其底层数据结构本质上是图。图神经网络（GNN）已成为处理这类问题的最强大和最主流的模型架构。这个动手实践将带你深入GNN的核心，通过从头开始实现一个两层GNN的前向传播过程，让你掌握节点嵌入是如何通过消息传递机制进行计算和更新的。",
            "id": "4281004",
            "problem": "您的任务是实现一个两层图神经网络 (GNN)，用于在电子设计自动化 (EDA) 的背景下预测线网关键性。线网在图中表示为有向边，网络通过在有向图上执行消息传递来生成节点嵌入，然后通过边级别的读出操作为每个线网生成一个标量分数。您必须为下面指定的小型测试图计算完整的前向传播，包括聚合和更新步骤。\n\n从图论和线性代数的基础出发，使用以下定义：\n\n- 设一个有向图表示为 $G = (V, E)$，其中 $V$ 是节点集，$E \\subseteq V \\times V$ 是有向边集。对于一个节点 $i \\in V$，其入邻居集合为 $\\mathcal{N}_{\\text{in}}(i) = \\{ j \\in V \\mid (j, i) \\in E \\}$。\n- 设 $X \\in \\mathbb{R}^{|V| \\times F}$ 为节点特征矩阵，其中 $F$ 是输入特征维度。\n- 定义一个对传入消息进行操作的聚合器：可以是求和聚合器或均值聚合器。对于均值聚合器，当 $\\mathcal{N}_{\\text{in}}(i)$ 为空时，将均值定义为 $\\mathbb{R}^{H}$ 中维度合适的零向量。\n- 第一层使用仿射自更新和聚合的邻居消息生成隐藏节点嵌入 $H^{(1)} \\in \\mathbb{R}^{|V| \\times H}$，然后应用一个修正线性单元 (ReLU) 非线性激活函数。第二层类似地生成 $H^{(2)} \\in \\mathbb{R}^{|V| \\times H_{2}}$。\n- ReLU 非线性激活函数按元素定义为 $\\sigma(x) = \\max(x, 0)$，其中 $x$ 为任意实标量。\n- 对于每一层 $\\ell \\in \\{1, 2\\}$，使用一个自权重矩阵 $W_{s}^{(\\ell)}$、一个消息权重矩阵 $W_{m}^{(\\ell)}$ 和一个维度合适的偏置向量 $b^{(\\ell)}$。节点 $i$ 的更新规则计算如下：将仿射自变换应用于节点的当前表示，加上由消息权重矩阵变换后的入邻居的聚合消息，然后加上偏置向量，并应用 ReLU 非线性激活函数。\n- 对于边 $(u,v) \\in E$，其线网（边）级别的读出分数 $s_{(u,v)}$ 是通过最终的节点嵌入，使用双线性形式和线性修正计算得出的：\n$$\ns_{(u,v)} = \\left(H^{(2)}_u\\right)^{\\top} R \\, H^{(2)}_v + c^{\\top} \\left(H^{(2)}_u + H^{(2)}_v\\right) + b,\n$$\n其中 $R \\in \\mathbb{R}^{H_{2} \\times H_{2}}$ 是一个读出矩阵，$c \\in \\mathbb{R}^{H_{2}}$ 是一个读出向量，$b \\in \\mathbb{R}$ 是一个标量偏置。\n\n实现一个两层 GNN 的前向传播，该网络使用上述定义计算 $H^{(1)}$，然后是 $H^{(2)}$，最后是边级别的分数。您必须支持求和与均值两种聚合器，具体使用哪种由每个测试用例指定。\n\n您的程序应计算下面指定的每个测试用例中指定“目标线网”的输出分数。程序的最终输出必须是一行，包含一个用方括号括起来的逗号分隔列表，其中包含所有测试用例中目标线网的分数。每个分数必须四舍五入到 $6$ 位小数。\n\n测试套件和参数：\n\n- 所有用例的通用维度：\n    - 输入特征维度 $F = 2$。\n    - 第一隐藏层维度 $H = 3$。\n    - 第二隐藏层维度 $H_{2} = 2$。\n\n- 所有测试用例使用的通用权重和偏置：\n    - 第一层消息权重：\n    $$\n    W_{m}^{(1)} = \\begin{bmatrix}\n    0.2  & -0.1  & 0.0 \\\\\n    0.0  & 0.3  & 0.1\n    \\end{bmatrix}\n    $$\n    - 第一层自权重：\n    $$\n    W_{s}^{(1)} = \\begin{bmatrix}\n    -0.1  & 0.2  & 0.4 \\\\\n    0.5  & -0.3  & 0.2\n    \\end{bmatrix}\n    $$\n    - 第一层偏置：\n    $$\n    b^{(1)} = \\begin{bmatrix}\n    0.0  & 0.1  & -0.2\n    \\end{bmatrix}\n    $$\n    - 第二层消息权重：\n    $$\n    W_{m}^{(2)} = \\begin{bmatrix}\n    0.1  & -0.2 \\\\\n    0.0  & 0.3 \\\\\n    -0.1  & 0.0\n    \\end{bmatrix}\n    $$\n    - 第二层自权重：\n    $$\n    W_{s}^{(2)} = \\begin{bmatrix}\n    0.2  & 0.1 \\\\\n    0.1  & -0.1 \\\\\n    0.0  & 0.2\n    \\end{bmatrix}\n    $$\n    - 第二层偏置：\n    $$\n    b^{(2)} = \\begin{bmatrix}\n    0.05  & -0.05\n    \\end{bmatrix}\n    $$\n    - 边读出矩阵：\n    $$\n    R = \\begin{bmatrix}\n    0.3  & 0.0 \\\\\n    0.0  & -0.2\n    \\end{bmatrix}\n    $$\n    - 边读出向量：\n    $$\n    c = \\begin{bmatrix}\n    0.1  & 0.05\n    \\end{bmatrix}\n    $$\n    - 边读出偏置：\n    $$\n    b = -0.01\n    $$\n\n- 测试用例 $1$（正常路径，有向链）：\n    - 节点 $|V| = 3$，索引为 $0, 1, 2$。\n    - 边 $E = \\{ (0,1), (1,2) \\}$。\n    - 聚合器：求和 (sum)。\n    - 节点特征矩阵：\n    $$\n    X = \\begin{bmatrix}\n    0.5  & -1.0 \\\\\n    1.0  & 0.0 \\\\\n    0.5  & 0.5\n    \\end{bmatrix}\n    $$\n    - 目标线网索引：$0$（边 $(0,1)$）。\n\n- 测试用例 $2$（自环和孤立节点，均值聚合）：\n    - 节点 $|V| = 4$，索引为 $0, 1, 2, 3$。\n    - 边 $E = \\{ (0,0), (0,1), (2,1) \\}$。\n    - 聚合器：均值 (mean)。\n    - 节点特征矩阵：\n    $$\n    X = \\begin{bmatrix}\n    0.0  & 1.0 \\\\\n    -0.5  & 0.5 \\\\\n    1.0  & -1.0 \\\\\n    0.2  & 0.2\n    \\end{bmatrix}\n    $$\n    - 目标线网索引：$2$（边 $(2,1)$）。\n\n- 测试用例 $3$（用双向边建模的无向线网）：\n    - 节点 $|V| = 3$，索引为 $0, 1, 2$。\n    - 边 $E = \\{ (0,1), (1,0), (1,2), (2,1) \\}$。\n    - 聚合器：求和 (sum)。\n    - 节点特征矩阵：\n    $$\n    X = \\begin{bmatrix}\n    1.0  & 1.0 \\\\\n    0.0  & 1.0 \\\\\n    -1.0  & 0.5\n    \\end{bmatrix}\n    $$\n    - 目标线网索引：$1$（边 $(1,0)$）。\n\n- 测试用例 $4$（零特征的边界情况）：\n    - 节点 $|V| = 2$，索引为 $0, 1$。\n    - 边 $E = \\{ (0,1) \\}$。\n    - 聚合器：求和 (sum)。\n    - 节点特征矩阵：\n    $$\n    X = \\begin{bmatrix}\n    0.0  & 0.0 \\\\\n    0.0  & 0.0\n    \\end{bmatrix}\n    $$\n    - 目标线网索引：$0$（边 $(0,1)$）。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果（例如，$[r_1, r_2, r_3, r_4]$），其中 $r_k$ 是测试用例 $k$ 中目标线网的分数。每个 $r_k$ 必须四舍五入到 $6$ 位小数。不应打印任何额外文本。",
            "solution": "用户提供了一个明确定义的问题，要求计算一个两层图神经网络 (GNN) 的前向传播和一个边级别的分数。该问题经核实具有科学依据、定义良好且内部一致，包含了所有必要的参数和规范。解决方案通过分步实现所定义的数学运算来推进。\n\n设图为 $G=(V, E)$，节点特征为 $X \\in \\mathbb{R}^{|V| \\times F}$。GNN 架构由两个消息传递层和一个边级别的读出函数组成。\n\n**GNN 层更新规则**\n\n对于每一层 $\\ell \\in \\{1, 2\\}$，节点 $i \\in V$ 的隐藏表示 $H_i^{(\\ell)}$ 是基于其前一层的表示 $H_i^{(\\ell-1)}$ 和其入邻居 $\\mathcal{N}_{\\text{in}}(i) = \\{j \\in V \\mid (j,i) \\in E\\}$ 的表示来计算的。第一层的输入是节点特征矩阵，因此 $H^{(0)} = X$。\n\n节点 $i$ 在第 $\\ell$ 层的更新方程为：\n$$\nH_i^{(\\ell)} = \\sigma \\left( \\left( H_i^{(\\ell-1)} \\right)^{\\top} W_{s}^{(\\ell)} + A_i^{(\\ell)} + b^{(\\ell)} \\right)\n$$\n这种形式略显非传统。更标准的表示法假定 $H_i$ 是行向量。让我们使用行向量表示法重新陈述这些操作，这在深度学习库中是标准的，并能简化矩阵代数表示法。设 $h_i^{(\\ell-1)}$ 为来自第 $\\ell-1$ 层的节点 $i$ 的行向量。更新过程为：\n$$\nh_i^{(\\ell)} = \\sigma \\left( h_i^{(\\ell-1)} W_{s}^{(\\ell)} + A_i^{(\\ell)} + b^{(\\ell)} \\right)\n$$\n其中 $\\sigma(x) = \\max(x, 0)$ 是逐元素的 ReLU 激活函数，$W_s^{(\\ell)}$ 和 $W_m^{(\\ell)}$ 分别是第 $\\ell$ 层的自权重和消息权重矩阵，$b^{(\\ell)}$ 是偏置向量。项 $A_i^{(\\ell)}$ 表示从入邻居聚合的消息：\n$$\nA_i^{(\\ell)} = \\text{AGG}_{j \\in \\mathcal{N}_{\\text{in}}(i)} \\left( h_j^{(\\ell-1)} W_{m}^{(\\ell)} \\right)\n$$\n\n**聚合函数**\n\n问题指定了两种类型的聚合器：`sum` 和 `mean`。\n1.  **求和聚合器**：对传入的消息进行求和。\n    $$\n    A_i^{(\\ell)} = \\sum_{j \\in \\mathcal{N}_{\\text{in}}(i)} h_j^{(\\ell-1)} W_{m}^{(\\ell)}\n    $$\n2.  **均值聚合器**：对传入的消息进行平均。\n    $$\n    A_i^{(\\ell)} = \\frac{1}{|\\mathcal{N}_{\\text{in}}(i)|} \\sum_{j \\in \\mathcal{N}_{\\text{in}}(i)} h_j^{(\\ell-1)} W_{m}^{(\\ell)}\n    $$\n在这两种情况下，如果入邻居集合 $\\mathcal{N}_{\\text{in}}(i)$ 为空，则聚合后的消息 $A_i^{(\\ell)}$ 是一个维度合适（$\\ell=1$ 时为 $H$，$\\ell=2$ 时为 $H_2$）的零向量。\n\n**边读出函数**\n\n在计算出第二层的最终节点嵌入 $H^{(2)}$ 后，计算从节点 $u$到节点 $v$ 的线网（边）的关键性分数 $s_{(u,v)}$。设 $h_u^{(2)}$ 和 $h_v^{(2)}$ 分别为节点 $u$ 和 $v$ 的最终行向量嵌入。该分数由一个双线性形式和一个线性形式给出：\n$$\ns_{(u,v)} = h_u^{(2)} R \\left(h_v^{(2)}\\right)^{\\top} + \\left( h_u^{(2)} + h_v^{(2)} \\right) c^{\\top} + b\n$$\n其中 $R$ 是一个读出矩阵，$c$ 是一个读出向量（此处为与问题定义保持一致表示为行向量，需要转置），$b$ 是一个标量偏置。原始问题陈述在读出部分对嵌入使用了列向量表示法，$s_{(u,v)} = (H^{(2)}_u)^{\\top} R H^{(2)}_v + c^{\\top}(H^{(2)}_u+H^{(2)}_v)+b$。当使用 `numpy` 中的一维数组实现时，两种表述在数值上是等效的。\n\n**计算步骤**\n\n每个测试用例的前向传播按以下步骤执行：\n\n1.  **初始化**：给定图结构 $(V, E)$ 和节点特征 $X$，为每个节点 $i$ 确定其入邻居 $\\mathcal{N}_{\\text{in}}(i)$。设置 $H^{(0)} = X$。\n2.  **第 1 层计算**：对于每个节点 $i \\in V$，使用第 $\\ell=1$ 层的更新规则，结合权重 $W_s^{(1)}$、$W_m^{(1)}$、偏置 $b^{(1)}$ 和指定的聚合器，计算隐藏状态 $h_i^{(1)}$。这将生成矩阵 $H^{(1)} \\in \\mathbb{R}^{|V| \\times H}$。\n3.  **第 2 层计算**：对于每个节点 $i \\in V$，使用第 $\\ell=2$ 层的更新规则，以 $H^{(1)}$ 为输入，结合权重 $W_s^{(2)}$、$W_m^{(2)}$ 和偏置 $b^{(2)}$，计算最终嵌入 $h_i^{(2)}$。这将生成最终的嵌入矩阵 $H^{(2)} \\in \\mathbb{R}^{|V| \\times H_2}$。\n4.  **分数计算**：确定当前测试用例的目标线网 $(u,v)$。检索其最终节点嵌入 $h_u^{(2)}$ 和 $h_v^{(2)}$。使用读出公式以及参数 $R$、$c$ 和 $b$ 计算最终分数 $s_{(u,v)}$。\n5.  **最终输出**：收集所有测试用例的分数，四舍五入到 $6$ 位小数，并格式化为所需的输出字符串。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    # --- Common Dimensions ---\n    # F = 2, H = 3, H2 = 2\n\n    # --- Common Weights and Biases ---\n    W_m1 = np.array([\n        [0.2, -0.1, 0.0],\n        [0.0, 0.3, 0.1]\n    ])\n    W_s1 = np.array([\n        [-0.1, 0.2, 0.4],\n        [0.5, -0.3, 0.2]\n    ])\n    b1 = np.array([0.0, 0.1, -0.2])\n\n    W_m2 = np.array([\n        [0.1, -0.2],\n        [0.0, 0.3],\n        [-0.1, 0.0]\n    ])\n    W_s2 = np.array([\n        [0.2, 0.1],\n        [0.1, -0.1],\n        [0.0, 0.2]\n    ])\n    b2 = np.array([0.05, -0.05])\n\n    R = np.array([\n        [0.3, 0.0],\n        [0.0, -0.2]\n    ])\n    c = np.array([0.1, 0.05])\n    b_readout = -0.01\n\n    def relu(x):\n        return np.maximum(x, 0)\n\n    def compute_gnn_pass(num_nodes, edges, X, aggregator, target_net_index):\n        # --- Pre-compute in-neighbors for all nodes ---\n        in_neighbors = {i: [] for i in range(num_nodes)}\n        for u, v in edges:\n            in_neighbors[v].append(u)\n\n        # --- Layer 1 ---\n        H0 = X\n        H1 = np.zeros((num_nodes, 3)) # H = 3\n        for i in range(num_nodes):\n            # Self-transformation\n            self_term = H0[i] @ W_s1\n\n            # Message aggregation\n            if not in_neighbors[i]:\n                agg_messages = np.zeros(3)\n            else:\n                messages = np.array([H0[j] @ W_m1 for j in in_neighbors[i]])\n                if aggregator == 'sum':\n                    agg_messages = np.sum(messages, axis=0)\n                elif aggregator == 'mean':\n                    agg_messages = np.mean(messages, axis=0)\n            \n            pre_activation = self_term + agg_messages + b1\n            H1[i] = relu(pre_activation)\n        \n        # --- Layer 2 ---\n        H2 = np.zeros((num_nodes, 2)) # H2 = 2\n        for i in range(num_nodes):\n            # Self-transformation\n            self_term = H1[i] @ W_s2\n\n            # Message aggregation\n            if not in_neighbors[i]:\n                agg_messages = np.zeros(2)\n            else:\n                messages = np.array([H1[j] @ W_m2 for j in in_neighbors[i]])\n                if aggregator == 'sum':\n                    agg_messages = np.sum(messages, axis=0)\n                elif aggregator == 'mean':\n                    agg_messages = np.mean(messages, axis=0)\n\n            pre_activation = self_term + agg_messages + b2\n            H2[i] = relu(pre_activation)\n            \n        # --- Edge Readout ---\n        u, v = edges[target_net_index]\n        h_u = H2[u]\n        h_v = H2[v]\n\n        bilinear_term = h_u @ R @ h_v\n        linear_term = (h_u + h_v) @ c\n        score = bilinear_term + linear_term + b_readout\n\n        return score\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"num_nodes\": 3,\n            \"edges\": [(0, 1), (1, 2)],\n            \"aggregator\": \"sum\",\n            \"X\": np.array([[0.5, -1.0], [1.0, 0.0], [0.5, 0.5]]),\n            \"target_net_index\": 0\n        },\n        {\n            \"num_nodes\": 4,\n            \"edges\": [(0, 0), (0, 1), (2, 1)],\n            \"aggregator\": \"mean\",\n            \"X\": np.array([[0.0, 1.0], [-0.5, 0.5], [1.0, -1.0], [0.2, 0.2]]),\n            \"target_net_index\": 2\n        },\n        {\n            \"num_nodes\": 3,\n            \"edges\": [(0, 1), (1, 0), (1, 2), (2, 1)],\n            \"aggregator\": \"sum\",\n            \"X\": np.array([[1.0, 1.0], [0.0, 1.0], [-1.0, 0.5]]),\n            \"target_net_index\": 1\n        },\n        {\n            \"num_nodes\": 2,\n            \"edges\": [(0, 1)],\n            \"aggregator\": \"sum\",\n            \"X\": np.array([[0.0, 0.0], [0.0, 0.0]]),\n            \"target_net_index\": 0\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        score = compute_gnn_pass(\n            case[\"num_nodes\"],\n            case[\"edges\"],\n            case[\"X\"],\n            case[\"aggregator\"],\n            case[\"target_net_index\"]\n        )\n        results.append(f\"{score:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}