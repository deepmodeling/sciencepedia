{
    "hands_on_practices": [
        {
            "introduction": "A cornerstone of Statistical Static Timing Analysis (SSTA) is the ability to propagate the effects of process variations through circuit models. This first practice grounds the abstract concepts of SSTA in the familiar territory of interconnect delay, using the classic Elmore delay model. You will derive the statistical properties of the delay by applying a first-order Taylor expansion, a fundamental technique for linearizing non-linear dependencies on sources of variation. ",
            "id": "4301862",
            "problem": "Consider a resistive-capacitive interconnect tree driven by a source and observed at a designated sink node. Let the path from the source to the sink be denoted by the ordered edge set $\\mathcal{P}$, indexed by $k \\in \\{1,2,\\dots, m\\}$, where each edge $k$ has resistance $R_{k}$, and let the capacitive loads be associated to nodes, indexed by $j \\in \\{1,2,\\dots, n\\}$, with capacitances $C_{j}$. For each edge $k$ along $\\mathcal{P}$, define the downstream set $\\mathcal{D}(k, S)$ as the set of indices of capacitances whose charging current flows through edge $k$ on the way to the sink $S$.\n\nFor Statistical Static Timing Analysis (SSTA), suppose parameterized process variation induces multiplicative perturbations on both resistances and capacitances, modeled as\n$$\nR_{k} \\;=\\; R_{k,0}\\left(1 + \\alpha_{k} G + \\sum_{p=1}^{P_{r}} \\gamma_{k,p} X_{p}\\right), \\quad k \\in \\mathcal{P},\n$$\n$$\nC_{j} \\;=\\; C_{j,0}\\left(1 + \\beta_{j} G + \\sum_{q=1}^{P_{c}} \\delta_{j,q} Y_{q}\\right), \\quad j \\in \\{1,\\dots,n\\},\n$$\nwhere $R_{k,0} > 0$ and $C_{j,0} > 0$ are nominal values, $G \\sim \\mathcal{N}(0,\\sigma_{G}^{2})$ is a global zero-mean Gaussian source common to both resistances and capacitances, and $\\{X_{p}\\}_{p=1}^{P_{r}}$ and $\\{Y_{q}\\}_{q=1}^{P_{c}}$ are mutually independent, zero-mean Gaussian local sources with variances $\\sigma_{X_{p}}^{2}$ and $\\sigma_{Y_{q}}^{2}$, respectively. Assume $G$, all $X_{p}$, and all $Y_{q}$ are mutually independent, and that the perturbations are sufficiently small to justify a first-order linearization in the random sources (i.e., products of random sources are neglected).\n\nUsing the standard definition of Elmore delay for an interconnect resistive-capacitive tree, construct the random variable for the Elmore delay $T_{S}$ to the sink $S$ in a canonical linear form with respect to the sources $G$, $\\{X_{p}\\}$, and $\\{Y_{q}\\}$. Then, derive closed-form expressions for the mean $\\mathbb{E}[T_{S}]$ and the variance $\\mathrm{Var}(T_{S})$ in terms of the nominal parameters $\\{R_{k,0}\\}$, $\\{C_{j,0}\\}$, the path $\\mathcal{P}$, the downstream sets $\\{\\mathcal{D}(k,S)\\}$, and the sensitivity coefficients $\\{\\alpha_{k}\\}$, $\\{\\beta_{j}\\}$, $\\{\\gamma_{k,p}\\}$, and $\\{\\delta_{j,q}\\}$, as well as the source variances $\\sigma_{G}^{2}$, $\\{\\sigma_{X_{p}}^{2}\\}$, and $\\{\\sigma_{Y_{q}}^{2}\\}$.\n\nExpress the mean in seconds and the variance in square seconds. Provide your final answer as a two-entry row matrix containing $\\mathbb{E}[T_S]$ and $\\mathrm{Var}(T_S)$, respectively, in a single closed-form analytic expression.",
            "solution": "The user wants to find the mean and variance of the Elmore delay for a specified sink node in an RC interconnect tree, where the resistances and capacitances are subject to statistical variations.\n\n### Step 1: Problem Validation\n\n**1. Extract Givens:**\n- An RC interconnect tree with a source and a sink node $S$.\n- The path from source to sink is an ordered edge set $\\mathcal{P}$, with edges indexed by $k \\in \\{1, 2, \\dots, m\\}$.\n- Edge resistances $R_k$ and nodal capacitances $C_j$ for $j \\in \\{1, 2, \\dots, n\\}$.\n- The downstream set for an edge $k \\in \\mathcal{P}$ is $\\mathcal{D}(k, S)$, the set of indices of capacitances whose charging current flows through edge $k$.\n- The statistical model for resistance $R_k$ is $R_{k} = R_{k,0}(1 + \\alpha_{k} G + \\sum_{p=1}^{P_{r}} \\gamma_{k,p} X_{p})$, where $R_{k,0} > 0$ is the nominal resistance.\n- The statistical model for capacitance $C_j$ is $C_{j} = C_{j,0}(1 + \\beta_{j} G + \\sum_{q=1}^{P_{c}} \\delta_{j,q} Y_{q})$, where $C_{j,0} > 0$ is the nominal capacitance.\n- $G \\sim \\mathcal{N}(0,\\sigma_{G}^{2})$ is a global zero-mean Gaussian random source.\n- $\\{X_{p}\\}_{p=1}^{P_{r}}$ are mutually independent, zero-mean Gaussian random sources with variances $\\sigma_{X_{p}}^{2}$.\n- $\\{Y_{q}\\}_{q=1}^{P_{c}}$ are mutually independent, zero-mean Gaussian random sources with variances $\\sigma_{Y_{q}}^{2}$.\n- All random sources ($G$, $\\{X_p\\}$, $\\{Y_q\\}$) are mutually independent.\n- A first-order linearization is assumed, meaning products of random sources are neglected.\n\n**2. Validate Using Extracted Givens:**\n- **Scientifically Grounded:** The problem is a standard exercise in Statistical Static Timing Analysis (SSTA) for integrated circuits. The Elmore delay is a well-established and fundamental first-order approximation for signal delay in RC networks. The linear canonical model for process variations is a cornerstone of many SSTA methodologies. The problem is firmly rooted in electrical engineering and applied statistics.\n- **Well-Posed:** All necessary parameters, models, and assumptions (Elmore delay definition, statistical models for R and C, independence of random sources, linearization) are explicitly provided to derive a unique solution for the mean and variance.\n- **Objective:** The problem is stated in precise, formal mathematical and engineering terms, free of any subjectivity or ambiguity.\n\n**3. Verdict and Action:**\nThe problem is valid. It is scientifically sound, well-posed, and objective. I will proceed with the derivation of the solution.\n\n### Step 2: Solution Derivation\n\nThe Elmore delay $T_S$ at a sink node $S$ is defined as the sum of $RC$ products over the path from the source to the sink. Specifically, for each resistance $R_k$ on the path $\\mathcal{P}$, it is multiplied by the total capacitance downstream of it.\nThe formula for the Elmore delay $T_S$ is:\n$$\nT_{S} = \\sum_{k \\in \\mathcal{P}} R_{k} \\left( \\sum_{j \\in \\mathcal{D}(k,S)} C_{j} \\right)\n$$\nGiven the indexing $k \\in \\{1, 2, \\dots, m\\}$ for the path $\\mathcal{P}$, we can write this as:\n$$\nT_{S} = \\sum_{k=1}^{m} R_{k} \\left( \\sum_{j \\in \\mathcal{D}(k,S)} C_{j} \\right)\n$$\nThe problem specifies statistical models for $R_k$ and $C_j$. We substitute these into the Elmore delay formula:\n$$\nT_{S} = \\sum_{k=1}^{m} \\left[ R_{k,0}\\left(1 + \\alpha_{k} G + \\sum_{p=1}^{P_{r}} \\gamma_{k,p} X_{p}\\right) \\right] \\left[ \\sum_{j \\in \\mathcal{D}(k,S)} C_{j,0}\\left(1 + \\beta_{j} G + \\sum_{q=1}^{P_{c}} \\delta_{j,q} Y_{q}\\right) \\right]\n$$\nThe problem states that we must use a first-order linearization, which means we neglect terms involving products of the random sources (e.g., $G^2$, $GX_p$, $X_p Y_q$, etc.). Let's expand the expression for $T_S$ and keep only the constant and first-order terms in the random variables $G$, $\\{X_p\\}$, and $\\{Y_q\\}$.\nThe product within the summation is of the form $(A_0 + \\Delta A)(B_0 + \\Delta B)$, where $A_0$ and $B_0$ are nominal values and $\\Delta A, \\Delta B$ are linear combinations of the random sources. The first-order approximation is $A_0 B_0 + A_0 \\Delta B + B_0 \\Delta A$.\nFor each term $k$ in the summation:\n- The nominal resistance part is $R_{k,0}$.\n- The nominal downstream capacitance part is $C_{down,0}(k) = \\sum_{j \\in \\mathcal{D}(k,S)} C_{j,0}$.\n- The variation in resistance is $\\Delta R_k = R_{k,0} \\left( \\alpha_k G + \\sum_{p=1}^{P_r} \\gamma_{k,p} X_p \\right)$.\n- The variation in downstream capacitance is $\\Delta C_{down}(k) = \\sum_{j \\in \\mathcal{D}(k,S)} \\Delta C_j = \\sum_{j \\in \\mathcal{D}(k,S)} C_{j,0} \\left( \\beta_j G + \\sum_{q=1}^{P_c} \\delta_{j,q} Y_q \\right)$.\n\nApplying the first-order approximation to the delay expression:\n$$\nT_S \\approx \\sum_{k=1}^{m} \\left( R_{k,0} C_{down,0}(k) + R_{k,0} \\Delta C_{down}(k) + C_{down,0}(k) \\Delta R_k \\right)\n$$\nSubstituting the expressions for the variations:\n$$\nT_S \\approx \\sum_{k=1}^{m} R_{k,0} C_{down,0}(k) + \\sum_{k=1}^{m} \\left( R_{k,0} \\sum_{j \\in \\mathcal{D}(k,S)} C_{j,0} \\left( \\beta_j G + \\sum_{q=1}^{P_c} \\delta_{j,q} Y_q \\right) + C_{down,0}(k) R_{k,0} \\left( \\alpha_k G + \\sum_{p=1}^{P_r} \\gamma_{k,p} X_p \\right) \\right)\n$$\nLet's group the terms by the random sources to obtain the canonical linear form $T_S \\approx T_{S,0} + S_G G + \\sum_{p=1}^{P_r} S_{X_p} X_p + \\sum_{q=1}^{P_c} S_{Y_q} Y_q$.\n\nThe nominal delay, $T_{S,0}$, is the term with no random variables:\n$$\nT_{S,0} = \\sum_{k=1}^{m} R_{k,0} C_{down,0}(k) = \\sum_{k=1}^{m} R_{k,0} \\left( \\sum_{j \\in \\mathcal{D}(k,S)} C_{j,0} \\right)\n$$\nThe sensitivity coefficient for the global source $G$, $S_G$, is the coefficient of $G$:\n$$\nS_G = \\sum_{k=1}^{m} \\left( R_{k,0} \\sum_{j \\in \\mathcal{D}(k,S)} C_{j,0} \\beta_j + C_{down,0}(k) R_{k,0} \\alpha_k \\right) = \\sum_{k=1}^{m} \\left( \\alpha_k R_{k,0} \\sum_{j \\in \\mathcal{D}(k,S)} C_{j,0} + R_{k,0} \\sum_{j \\in \\mathcal{D}(k,S)} \\beta_j C_{j,0} \\right)\n$$\n$$\nS_G = \\sum_{k=1}^{m} \\sum_{j \\in \\mathcal{D}(k,S)} R_{k,0} C_{j,0} (\\alpha_k + \\beta_j)\n$$\nThe sensitivity coefficient for the local resistance source $X_p$, $S_{X_p}$, is the coefficient of $X_p$:\n$$\nS_{X_p} = \\sum_{k=1}^{m} C_{down,0}(k) R_{k,0} \\gamma_{k,p} = \\sum_{k=1}^{m} \\gamma_{k,p} R_{k,0} \\left( \\sum_{j \\in \\mathcal{D}(k,S)} C_{j,0} \\right) = \\sum_{k=1}^{m} \\sum_{j \\in \\mathcal{D}(k,S)} R_{k,0} C_{j,0} \\gamma_{k,p}\n$$\nThe sensitivity coefficient for the local capacitance source $Y_q$, $S_{Y_q}$, is the coefficient of $Y_q$:\n$$\nS_{Y_q} = \\sum_{k=1}^{m} R_{k,0} \\sum_{j \\in \\mathcal{D}(k,S)} C_{j,0} \\delta_{j,q} = \\sum_{k=1}^{m} \\sum_{j \\in \\mathcal{D}(k,S)} R_{k,0} C_{j,0} \\delta_{j,q}\n$$\n\n**Derivation of the Mean $\\mathbb{E}[T_S]$**\nThe mean of the delay is found by taking the expectation of the linearized expression for $T_S$. By the linearity of expectation:\n$$\n\\mathbb{E}[T_S] \\approx \\mathbb{E}[T_{S,0}] + S_G \\mathbb{E}[G] + \\sum_{p=1}^{P_r} S_{X_p} \\mathbb{E}[X_p] + \\sum_{q=1}^{P_c} S_{Y_q} \\mathbb{E}[Y_q]\n$$\nSince all random sources are defined to be zero-mean ($\\mathbb{E}[G]=0$, $\\mathbb{E}[X_p]=0$, $\\mathbb{E}[Y_q]=0$), the expression simplifies to:\n$$\n\\mathbb{E}[T_S] = T_{S,0} = \\sum_{k=1}^{m} R_{k,0} \\left( \\sum_{j \\in \\mathcal{D}(k,S)} C_{j,0} \\right)\n$$\n\n**Derivation of the Variance $\\mathrm{Var}(T_S)$**\nThe variance of a sum of independent random variables is the sum of their variances. Since $G$, $\\{X_p\\}$, and $\\{Y_q\\}$ are mutually independent:\n$$\n\\mathrm{Var}(T_S) \\approx \\mathrm{Var}(T_{S,0} + S_G G + \\sum_{p=1}^{P_r} S_{X_p} X_p + \\sum_{q=1}^{P_c} S_{Y_q} Y_q)\n$$\nThe nominal term $T_{S,0}$ is a constant, so its variance is zero.\n$$\n\\mathrm{Var}(T_S) = \\mathrm{Var}(S_G G) + \\sum_{p=1}^{P_r} \\mathrm{Var}(S_{X_p} X_p) + \\sum_{q=1}^{P_c} \\mathrm{Var}(S_{Y_q} Y_q)\n$$\nUsing the property $\\mathrm{Var}(aZ) = a^2 \\mathrm{Var}(Z)$:\n$$\n\\mathrm{Var}(T_S) = S_G^2 \\mathrm{Var}(G) + \\sum_{p=1}^{P_r} S_{X_p}^2 \\mathrm{Var}(X_p) + \\sum_{q=1}^{P_c} S_{Y_q}^2 \\mathrm{Var}(Y_q)\n$$\nSubstituting the given variances ($\\sigma_G^2$, $\\sigma_{X_p}^2$, $\\sigma_{Y_q}^2$) and the expressions for the sensitivity coefficients:\n$$\n\\mathrm{Var}(T_S) = \\sigma_G^2 S_G^2 + \\sum_{p=1}^{P_r} \\sigma_{X_p}^2 S_{X_p}^2 + \\sum_{q=1}^{P_c} \\sigma_{Y_q}^2 S_{Y_q}^2\n$$\n$$\n\\mathrm{Var}(T_S) = \\sigma_G^2 \\left( \\sum_{k=1}^m \\sum_{j \\in \\mathcal{D}(k,S)} R_{k,0} C_{j,0} (\\alpha_k + \\beta_j) \\right)^2 + \\sum_{p=1}^{P_r} \\sigma_{X_p}^2 \\left( \\sum_{k=1}^m \\sum_{j \\in \\mathcal{D}(k,S)} R_{k,0} C_{j,0} \\gamma_{k,p} \\right)^2 + \\sum_{q=1}^{P_c} \\sigma_{Y_q}^2 \\left( \\sum_{k=1}^m \\sum_{j \\in \\mathcal{D}(k,S)} R_{k,0} C_{j,0} \\delta_{j,q} \\right)^2\n$$\nThe problem asks for the mean in seconds and the variance in square seconds. Since $R_{k,0}$ and $C_{j,0}$ have units of resistance and capacitance respectively, their product $R_{k,0}C_{j,0}$ has units of time (e.g., Ohms $\\times$ Farads = seconds). The sensitivity coefficients $\\alpha, \\beta, \\gamma, \\delta$ and the random sources are dimensionless. Thus, the derived expression for the mean has units of time, and the variance (which involves squared sensitivities and dimensionless source variances) has units of time squared, as required.\n\nFinal expressions for mean and variance are:\n$$\n\\mathbb{E}[T_S] = \\sum_{k=1}^{m} R_{k,0} \\sum_{j \\in \\mathcal{D}(k,S)} C_{j,0}\n$$\n$$\n\\mathrm{Var}(T_S) = \\sigma_{G}^{2} \\left[ \\sum_{k=1}^{m}\\sum_{j \\in \\mathcal{D}(k,S)} R_{k,0}C_{j,0}(\\alpha_k + \\beta_j) \\right]^{2} + \\sum_{p=1}^{P_r} \\sigma_{X_{p}}^{2} \\left[ \\sum_{k=1}^{m} \\sum_{j \\in \\mathcal{D}(k,S)} R_{k,0}C_{j,0} \\gamma_{k,p} \\right]^{2} + \\sum_{q=1}^{P_c} \\sigma_{Y_{q}}^{2} \\left[ \\sum_{k=1}^{m} \\sum_{j \\in \\mathcal{D}(k,S)} R_{k,0}C_{j,0} \\delta_{j,q} \\right]^{2}\n$$",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\displaystyle\\sum_{k=1}^{m} R_{k,0} \\sum_{j \\in \\mathcal{D}(k,S)} C_{j,0}  \\sigma_{G}^{2} \\left( \\sum_{k=1}^{m}\\sum_{j \\in \\mathcal{D}(k,S)} R_{k,0}C_{j,0}(\\alpha_k + \\beta_j) \\right)^{2} + \\sum_{p=1}^{P_r} \\sigma_{X_{p}}^{2} \\left( \\sum_{k=1}^{m} \\sum_{j \\in \\mathcal{D}(k,S)} R_{k,0}C_{j,0} \\gamma_{k,p} \\right)^{2} + \\sum_{q=1}^{P_c} \\sigma_{Y_{q}}^{2} \\left( \\sum_{k=1}^{m} \\sum_{j \\in \\mathcal{D}(k,S)} R_{k,0}C_{j,0} \\delta_{j,q} \\right)^{2} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "In SSTA, no timing path exists in isolation; paths often share segments or are affected by the same global process variations, leading to statistical correlation. This exercise provides a foundational look at how this correlation is quantified directly from the underlying canonical linear models of variation. By tracking the sensitivity coefficients of shared random sources, you will derive the Pearson correlation coefficient between two arrival times, revealing the statistical linkage created by physical dependencies. ",
            "id": "4301807",
            "problem": "In Statistical Static Timing Analysis (SSTA) within Electronic Design Automation (EDA), path arrival times are modeled as affine functions of independent random sources representing process, environment, and noise variations. Consider two node arrival times, $T_{A}$ and $T_{B}$, in a synchronous digital integrated circuit. Each arrival time is modeled as an affine transformation of a common set of independent standard normal basis variables that encode global and local variation modes, plus node-specific independent noise.\n\nLet the global variation modes be $X_{1}$ and $X_{2}$ (each $X_{i} \\sim \\mathcal{N}(0,1)$), and two spatial local variation modes be $X_{3}$ (region $R$) and $X_{4}$ (region $S$), each $X_{i} \\sim \\mathcal{N}(0,1)$ and mutually independent. Additionally, node-specific independent noise terms are modeled as $Z_{A} \\sim \\mathcal{N}(0,1)$ and $Z_{B} \\sim \\mathcal{N}(0,1)$, independent of all $X_{i}$ and of each other. The arrival times are given by\n$$\nT_{A} = \\mu_{A} + a_{1} X_{1} + a_{2} X_{2} + a_{3} X_{3} + s_{A} Z_{A},\n$$\n$$\nT_{B} = \\mu_{B} + b_{1} X_{1} + b_{2} X_{2} + b_{4} X_{4} + s_{B} Z_{B},\n$$\nwhere $\\mu_{A}$ and $\\mu_{B}$ are deterministic means, and the coefficients $a_{i}$, $b_{i}$, $s_{A}$, and $s_{B}$ are deterministic sensitivity coefficients with respect to the corresponding standard normal modes. The local mode $X_{3}$ influences node $A$ only (region $R$), and $X_{4}$ influences node $B$ only (region $S$). All random variables listed are mutually independent except for the shared global modes $X_{1}$ and $X_{2}$.\n\nSuppose the sensitivity coefficients are\n$$\na_{1} = 0.8,\\quad a_{2} = 0.5,\\quad a_{3} = 0.4,\\quad s_{A} = 0.3,\n$$\n$$\nb_{1} = 0.7,\\quad b_{2} = 0.6,\\quad b_{4} = 0.5,\\quad s_{B} = 0.2.\n$$\n\nStarting from the definitions of covariance, variance, and the Pearson correlation coefficient for jointly modeled random variables, derive an expression for the correlation coefficient between $T_{A}$ and $T_{B}$ in terms of the sensitivity coefficients and the independence of the basis variables. Then compute the numerical value of the correlation coefficient between $T_{A}$ and $T_{B}$ for the given sensitivity coefficients. Express your final answer as a pure number and round your answer to four significant figures.",
            "solution": "The problem as stated is subject to a rigorous validation process.\n\n### Step 1: Extract Givens\n-   The arrival times $T_{A}$ and $T_{B}$ are modeled as affine functions of independent random sources.\n-   Global variation modes: $X_{1} \\sim \\mathcal{N}(0,1)$, $X_{2} \\sim \\mathcal{N}(0,1)$.\n-   Local variation modes: $X_{3} \\sim \\mathcal{N}(0,1)$, $X_{4} \\sim \\mathcal{N}(0,1)$.\n-   Node-specific independent noise: $Z_{A} \\sim \\mathcal{N}(0,1)$, $Z_{B} \\sim \\mathcal{N}(0,1)$.\n-   All basis random variables ($X_{1}$, $X_{2}$, $X_{3}$, $X_{4}$, $Z_{A}$, $Z_{B}$) are mutually independent.\n-   Arrival time for node A: $T_{A} = \\mu_{A} + a_{1} X_{1} + a_{2} X_{2} + a_{3} X_{3} + s_{A} Z_{A}$.\n-   Arrival time for node B: $T_{B} = \\mu_{B} + b_{1} X_{1} + b_{2} X_{2} + b_{4} X_{4} + s_{B} Z_{B}$.\n-   Coefficients for $T_A$: $a_{1} = 0.8$, $a_{2} = 0.5$, $a_{3} = 0.4$, $s_{A} = 0.3$.\n-   Coefficients for $T_B$: $b_{1} = 0.7$, $b_{2} = 0.6$, $b_{4} = 0.5$, $s_{B} = 0.2$.\n-   The task is to derive the correlation coefficient between $T_{A}$ and $T_{B}$ and then compute its numerical value, rounded to four significant figures.\n\n### Step 2: Validate Using Extracted Givens\n-   **Scientifically Grounded**: The problem is well-grounded in the theory of statistical static timing analysis (SSTA) for integrated circuits. The use of affine models based on independent Gaussian random variables to represent process variations is a standard and well-established technique in this field. The statistical concepts used (variance, covariance, correlation) are fundamental and correctly applied.\n-   **Well-Posed**: The problem is clearly defined and provides all necessary information (functional forms, distributions of all random variables, and all coefficient values) to derive a unique numerical solution.\n-   **Objective**: The problem is stated using precise mathematical language, free of ambiguity, subjectivity, or opinion-based claims.\n\n### Step 3: Verdict and Action\nThe problem is deemed valid. It is scientifically sound, well-posed, and objective. There are no contradictions, missing information, or violations of physical or mathematical principles. I will now proceed with the solution.\n\nThe Pearson correlation coefficient, $\\rho(T_{A}, T_{B})$, between two random variables $T_{A}$ and $T_{B}$ is defined as:\n$$\n\\rho(T_{A}, T_{B}) = \\frac{\\text{Cov}(T_{A}, T_{B})}{\\sqrt{\\text{Var}(T_{A}) \\text{Var}(T_{B})}}\n$$\nwhere $\\text{Var}(\\cdot)$ denotes the variance and $\\text{Cov}(\\cdot, \\cdot)$ denotes the covariance. We must compute the variance of $T_{A}$ and $T_{B}$, and their covariance.\n\nFirst, we determine the expected values of $T_{A}$ and $T_{B}$. The basis random variables $X_{i}$ and $Z_{j}$ are standard normal, so their expected value is $0$.\n$$\nE[T_{A}] = E[\\mu_{A} + a_{1} X_{1} + a_{2} X_{2} + a_{3} X_{3} + s_{A} Z_{A}]\n$$\nBy linearity of expectation:\n$$\nE[T_{A}] = \\mu_{A} + a_{1} E[X_{1}] + a_{2} E[X_{2}] + a_{3} E[X_{3}] + s_{A} E[Z_{A}] = \\mu_{A} + 0 = \\mu_{A}\n$$\nSimilarly, for $T_{B}$:\n$$\nE[T_{B}] = E[\\mu_{B} + b_{1} X_{1} + b_{2} X_{2} + b_{4} X_{4} + s_{B} Z_{B}] = \\mu_{B}\n$$\n\nNext, we compute the variance of $T_{A}$. The variance of a sum of independent random variables is the sum of their variances. Since $\\mu_A$ is a constant, it does not contribute to the variance. The variables $X_{1}$, $X_{2}$, $X_{3}$, and $Z_{A}$ are mutually independent.\n$$\n\\text{Var}(T_{A}) = \\text{Var}(a_{1} X_{1} + a_{2} X_{2} + a_{3} X_{3} + s_{A} Z_{A})\n$$\nUsing the property $\\text{Var}(cY) = c^2 \\text{Var}(Y)$ and the independence of the variables:\n$$\n\\text{Var}(T_{A}) = a_{1}^2 \\text{Var}(X_{1}) + a_{2}^2 \\text{Var}(X_{2}) + a_{3}^2 \\text{Var}(X_{3}) + s_{A}^2 \\text{Var}(Z_{A})\n$$\nSince all basis variables are standard normal, their variance is $1$.\n$$\n\\text{Var}(T_{A}) = a_{1}^2(1) + a_{2}^2(1) + a_{3}^2(1) + s_{A}^2(1) = a_{1}^2 + a_{2}^2 + a_{3}^2 + s_{A}^2\n$$\nBy the same reasoning, the variance of $T_{B}$ is:\n$$\n\\text{Var}(T_{B}) = \\text{Var}(b_{1} X_{1} + b_{2} X_{2} + b_{4} X_{4} + s_{B} Z_{B}) = b_{1}^2 \\text{Var}(X_{1}) + b_{2}^2 \\text{Var}(X_{2}) + b_{4}^2 \\text{Var}(X_{4}) + s_{B}^2 \\text{Var(Z_{B})}\n$$\n$$\n\\text{Var}(T_{B}) = b_{1}^2 + b_{2}^2 + b_{4}^2 + s_{B}^2\n$$\n\nNow, we compute the covariance of $T_{A}$ and $T_{B}$.\n$$\n\\text{Cov}(T_{A}, T_{B}) = E[(T_{A} - E[T_{A}])(T_{B} - E[T_{B}])]\n$$\n$$\n\\text{Cov}(T_{A}, T_{B}) = E[(a_{1} X_{1} + a_{2} X_{2} + a_{3} X_{3} + s_{A} Z_{A})(b_{1} X_{1} + b_{2} X_{2} + b_{4} X_{4} + s_{B} Z_{B})]\n$$\nExpanding the product and applying the linearity of expectation, we get a sum of terms like $E[c_{i}d_{j}V_{i}W_{j}]$, where $V_i$ and $W_j$ are basis random variables. Since all basis variables are independent and have zero mean, the expectation of any cross-product of different variables is zero. For example, $E[X_{1}X_{2}] = E[X_{1}]E[X_{2}] = 0 \\times 0 = 0$. The only non-zero terms arise from the products of a variable with itself. The shared variables between the expressions for $T_A$ and $T_B$ are $X_1$ and $X_2$.\n$$\n\\text{Cov}(T_{A}, T_{B}) = E[a_{1}b_{1} X_{1}^2 + a_{2}b_{2} X_{2}^2 + \\text{other cross-product terms}]\n$$\n$$\n\\text{Cov}(T_{A}, T_{B}) = a_{1}b_{1}E[X_{1}^2] + a_{2}b_{2}E[X_{2}^2]\n$$\nFor a standard normal variable $X$, $\\text{Var}(X) = E[X^2] - (E[X])^2$. With $\\text{Var}(X) = 1$ and $E[X] = 0$, we have $E[X^2] = 1$.\nTherefore, the covariance is:\n$$\n\\text{Cov}(T_{A}, T_{B}) = a_{1}b_{1} + a_{2}b_{2}\n$$\nNow we can write the general expression for the correlation coefficient:\n$$\n\\rho(T_{A}, T_{B}) = \\frac{a_{1}b_{1} + a_{2}b_{2}}{\\sqrt{(a_{1}^2 + a_{2}^2 + a_{3}^2 + s_{A}^2)(b_{1}^2 + b_{2}^2 + b_{4}^2 + s_{B}^2)}}\n$$\nWe now substitute the given numerical values for the sensitivity coefficients:\n$a_{1} = 0.8$, $a_{2} = 0.5$, $a_{3} = 0.4$, $s_{A} = 0.3$\n$b_{1} = 0.7$, $b_{2} = 0.6$, $b_{4} = 0.5$, $s_{B} = 0.2$\n\nCalculate the covariance:\n$$\n\\text{Cov}(T_{A}, T_{B}) = (0.8)(0.7) + (0.5)(0.6) = 0.56 + 0.30 = 0.86\n$$\nCalculate the variance of $T_A$:\n$$\n\\text{Var}(T_{A}) = (0.8)^2 + (0.5)^2 + (0.4)^2 + (0.3)^2 = 0.64 + 0.25 + 0.16 + 0.09 = 1.14\n$$\nCalculate the variance of $T_B$:\n$$\n\\text{Var}(T_{B}) = (0.7)^2 + (0.6)^2 + (0.5)^2 + (0.2)^2 = 0.49 + 0.36 + 0.25 + 0.04 = 1.14\n$$\nNow, compute the correlation coefficient:\n$$\n\\rho(T_{A}, T_{B}) = \\frac{0.86}{\\sqrt{(1.14)(1.14)}} = \\frac{0.86}{1.14}\n$$\n$$\n\\rho(T_{A}, T_{B}) \\approx 0.75438596...\n$$\nRounding the result to four significant figures gives $0.7544$.",
            "answer": "$$\n\\boxed{0.7544}\n$$"
        },
        {
            "introduction": "The ultimate goal of SSTA is to provide a more accurate prediction of circuit performance and yield. This final practice applies the concepts of statistical moments and correlation to the critical task of calculating setup slack, the buffer against timing failures. You will see firsthand how accounting for correlations between the data path and clock path is essential for avoiding overly pessimistic results and achieving a realistic timing signoff. ",
            "id": "4301947",
            "problem": "Consider Statistical Static Timing Analysis (SSTA) within the broader context of Electronic Design Automation (EDA) for integrated circuits. An endpoint setup slack is defined as the difference between a required clock time and the sum of the actual clock arrival time and the data arrival time. Let the slack be defined by the fundamental endpoint setup relationship\n$$\nS \\equiv \\left(T_{\\text{clk,req}} - T_{\\text{clk,arr}}\\right) - T_{\\text{data}},\n$$\nwhere $T_{\\text{clk,req}}$ is the required clock time at the capture register, $T_{\\text{clk,arr}}$ is the actual clock arrival time at the same register, and $T_{\\text{data}}$ is the data arrival time at that register.\n\nAssume that process, voltage, and temperature variations induce correlated random fluctuations such that $\\left(T_{\\text{clk,req}},\\,T_{\\text{clk,arr}},\\,T_{\\text{data}}\\right)$ are jointly Gaussian random variables. Their means and standard deviations are given by\n$$\n\\mathbb{E}[T_{\\text{clk,req}}] = 502.7\\,\\mathrm{ps},\\quad \\mathbb{E}[T_{\\text{clk,arr}}] = 121.3\\,\\mathrm{ps},\\quad \\mathbb{E}[T_{\\text{data}}] = 350.8\\,\\mathrm{ps},\n$$\n$$\n\\sigma_{T_{\\text{clk,req}}} = 18\\,\\mathrm{ps},\\quad \\sigma_{T_{\\text{clk,arr}}} = 22\\,\\mathrm{ps},\\quad \\sigma_{T_{\\text{data}}} = 35\\,\\mathrm{ps}.\n$$\nCorrelations arise from shared global variation sources and partial common-path dependencies, modeled by the correlation coefficients\n$$\n\\rho\\!\\left(T_{\\text{clk,req}},T_{\\text{clk,arr}}\\right) = 0.7,\\quad\n\\rho\\!\\left(T_{\\text{clk,req}},T_{\\text{data}}\\right) = 0.3,\\quad\n\\rho\\!\\left(T_{\\text{clk,arr}},T_{\\text{data}}\\right) = 0.5.\n$$\n\nStarting from the core definitions of slack and the properties of linear transformations of jointly Gaussian random variables, derive expressions for the mean and variance of $S$ that explicitly include covariance terms. Then, evaluate these expressions numerically using the parameters provided. Express the mean slack in $\\mathrm{ps}$ and the variance of slack in $\\mathrm{ps}^2$. Round each of the two numerical results to $4$ significant figures. Report your final answer as two values ordered as $\\left(\\text{mean},\\,\\text{variance}\\right)$.",
            "solution": "The problem is valid as it is scientifically grounded in the principles of statistical static timing analysis, is well-posed with all necessary information provided, and is formulated objectively. We can proceed with the solution.\n\nLet the random variables for the timing parameters be denoted as $X_1 = T_{\\text{clk,req}}$, $X_2 = T_{\\text{clk,arr}}$, and $X_3 = T_{\\text{data}}$. The setup slack, $S$, is defined as a linear combination of these variables:\n$$\nS = X_1 - X_2 - X_3\n$$\nThe problem provides the means, standard deviations, and correlation coefficients for these jointly Gaussian random variables.\n\nFirst, we derive the expression for the mean of the slack, $\\mathbb{E}[S]$. Due to the linearity of the expectation operator, the mean of a linear combination of random variables is the linear combination of their means.\n$$\n\\mathbb{E}[S] = \\mathbb{E}[X_1 - X_2 - X_3] = \\mathbb{E}[X_1] - \\mathbb{E}[X_2] - \\mathbb{E}[X_3]\n$$\nUsing the provided values:\n$$\n\\mathbb{E}[X_1] = 502.7\\,\\mathrm{ps} \\\\\n\\mathbb{E}[X_2] = 121.3\\,\\mathrm{ps} \\\\\n\\mathbb{E}[X_3] = 350.8\\,\\mathrm{ps}\n$$\nWe can compute the numerical value of the mean slack:\n$$\n\\mathbb{E}[S] = 502.7 - 121.3 - 350.8 = 30.6\\,\\mathrm{ps}\n$$\nRounding to $4$ significant figures, the mean slack is $30.60\\,\\mathrm{ps}$.\n\nNext, we derive the expression for the variance of the slack, $\\mathrm{Var}(S)$. For a general linear combination of three correlated random variables $Y = a_1 X_1 + a_2 X_2 + a_3 X_3$, the variance is given by:\n$$\n\\mathrm{Var}(Y) = \\sum_{i=1}^3 \\sum_{j=1}^3 a_i a_j \\mathrm{Cov}(X_i, X_j)\n$$\nExpanding this for our specific case where $a_1=1$, $a_2=-1$, and $a_3=-1$:\n$$\n\\mathrm{Var}(S) = a_1^2 \\mathrm{Var}(X_1) + a_2^2 \\mathrm{Var}(X_2) + a_3^2 \\mathrm{Var}(X_3) + 2a_1 a_2 \\mathrm{Cov}(X_1, X_2) + 2a_1 a_3 \\mathrm{Cov}(X_1, X_3) + 2a_2 a_3 \\mathrm{Cov}(X_2, X_3)\n$$\nSubstituting the coefficients:\n$$\n\\mathrm{Var}(S) = (1)^2 \\mathrm{Var}(X_1) + (-1)^2 \\mathrm{Var}(X_2) + (-1)^2 \\mathrm{Var}(X_3) + 2(1)(-1)\\mathrm{Cov}(X_1, X_2) + 2(1)(-1)\\mathrm{Cov}(X_1, X_3) + 2(-1)(-1)\\mathrm{Cov}(X_2, X_3)\n$$\nThis simplifies to the required expression explicitly including covariance terms:\n$$\n\\mathrm{Var}(S) = \\mathrm{Var}(X_1) + \\mathrm{Var}(X_2) + \\mathrm{Var}(X_3) - 2\\mathrm{Cov}(X_1, X_2) - 2\\mathrm{Cov}(X_1, X_3) + 2\\mathrm{Cov}(X_2, X_3)\n$$\nTo evaluate this expression, we use the relationship between covariance, correlation coefficient ($\\rho$), and standard deviation ($\\sigma$): $\\mathrm{Cov}(X_i, X_j) = \\rho(X_i, X_j) \\sigma_{X_i} \\sigma_{X_j}$. Also, $\\mathrm{Var}(X_i) = \\sigma_{X_i}^2$. Let $\\sigma_i = \\sigma_{X_i}$ and $\\rho_{ij} = \\rho(X_i, X_j)$. The variance expression becomes:\n$$\n\\mathrm{Var}(S) = \\sigma_1^2 + \\sigma_2^2 + \\sigma_3^2 - 2\\rho_{12}\\sigma_1\\sigma_2 - 2\\rho_{13}\\sigma_1\\sigma_3 + 2\\rho_{23}\\sigma_2\\sigma_3\n$$\nWe are given the following standard deviations and correlation coefficients:\n$$\n\\sigma_1 = \\sigma_{T_{\\text{clk,req}}} = 18\\,\\mathrm{ps},\\quad \\sigma_2 = \\sigma_{T_{\\text{clk,arr}}} = 22\\,\\mathrm{ps},\\quad \\sigma_3 = \\sigma_{T_{\\text{data}}} = 35\\,\\mathrm{ps}\n$$\n$$\n\\rho_{12} = \\rho\\!\\left(T_{\\text{clk,req}},T_{\\text{clk,arr}}\\right) = 0.7,\\quad\n\\rho_{13} = \\rho\\!\\left(T_{\\text{clk,req}},T_{\\text{data}}\\right) = 0.3,\\quad\n\\rho_{23} = \\rho\\!\\left(T_{\\text{clk,arr}},T_{\\text{data}}\\right) = 0.5\n$$\nFirst, we calculate the individual variance and covariance terms:\nThe variances are:\n$$\n\\sigma_1^2 = 18^2 = 324\\,\\mathrm{ps}^2 \\\\\n\\sigma_2^2 = 22^2 = 484\\,\\mathrm{ps}^2 \\\\\n\\sigma_3^2 = 35^2 = 1225\\,\\mathrm{ps}^2\n$$\nThe covariance-related terms are:\n$$\n2\\rho_{12}\\sigma_1\\sigma_2 = 2(0.7)(18)(22) = 554.4\\,\\mathrm{ps}^2 \\\\\n2\\rho_{13}\\sigma_1\\sigma_3 = 2(0.3)(18)(35) = 378\\,\\mathrm{ps}^2 \\\\\n2\\rho_{23}\\sigma_2\\sigma_3 = 2(0.5)(22)(35) = 770\\,\\mathrm{ps}^2\n$$\nSubstituting these values back into the variance formula for $S$:\n$$\n\\mathrm{Var}(S) = (324 + 484 + 1225) - 554.4 - 378 + 770\\,\\mathrm{ps}^2\n$$\n$$\n\\mathrm{Var}(S) = 2033 - 554.4 - 378 + 770 = 1870.6\\,\\mathrm{ps}^2\n$$\nRounding to $4$ significant figures, the variance of the slack is $1871\\,\\mathrm{ps}^2$.\n\nThe final results are a mean slack of $30.60\\,\\mathrm{ps}$ and a slack variance of $1871\\,\\mathrm{ps}^2$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n30.60  1871\n\\end{pmatrix}\n}\n$$"
        }
    ]
}