## 引言
在数据驱动的时代，传统计算架构正面临一个根本性的挑战：数据移动的成本远超计算本身。这种被称为“[冯·诺依曼瓶颈](@entry_id:1133907)”的现象，严重制约了性能和[能效](@entry_id:272127)的提升，尤其是在人工智能等数据密集型应用中。为了打破这堵“[内存墙](@entry_id:636725)”，学术界和工业界正积极探索一种颠覆性的计算范式——[存内计算](@entry_id:1122818)（In-memory Computing, IMC）。其核心思想是将计算任务从中央处理器迁移到[数据存储](@entry_id:141659)的位置，实现“计算与存储一体化”，从而最大限度地减少高昂的数据搬运开销。

本文旨在系统性地剖析[存内计算](@entry_id:1122818)架构的全貌。我们将从第一章“原理与机制”出发，深入探讨支撑IMC的物理原理，量化分析其相对于传统架构的优势，并详细拆解模拟与数字IMC的技术路径及其面临的非理想性挑战。随后，在第二章“应用与跨学科连接”中，我们将展示IMC如何在加速人工智能、应对电子设计自动化（EDA）挑战等实际场景中发挥作用，并探索其与神经形态计算等前沿领域的交叉融合。最后，第三章“动手实践”将通过具体的工程问题，帮助读者巩固对关键设计概念的理解。通过这三个层层递进的章节，读者将对存内计算这一前沿技术建立起从基础理论到系统应用的完整知识体系。

## 原理与机制

在介绍章节中，我们确立了内存计算（In-memory Computing, IMC）作为一种旨在克服传统计算架构中数据移动瓶颈的新兴范式。本章将深入探讨支撑IMC的核心科学原理和关键实现机制。我们将从其根本动机——[冯·诺依曼瓶颈](@entry_id:1133907)——出发，系统地剖析IMC如何通过将计算任务重新定位到[数据存储](@entry_id:141659)的位置来解决这一挑战。随后，我们将考察实现这一目标的各种物理途径，包括不同的计算模式、使能技术及其固有的非理想性。最后，我们将建立一个系统级的设计框架，用于管理和权衡这些复杂因素。

### 根本动机：[冯·诺依曼瓶颈](@entry_id:1133907)的量化分析

经典的**[冯·诺依曼架构](@entry_id:756577)**通过将中央处理单元（CPU）与[存储层次结构](@entry_id:755484)物理分离，奠定了现代计算的基础。在这种模型中，每次算术运算通常都需要从内存中获取操作数，并将结果[写回](@entry_id:756770)，这些数据交换都通过带宽有限且具有非零单位比特传输能耗的互连总线进行。随着半导体技术的飞速发展，处理器的计算能力（以每秒操作次数衡量）的增长速度远超[内存带宽](@entry_id:751847)的增长速度。这种日益加剧的不匹配导致了所谓的“内存墙”或**冯·诺依曼瓶颈**——一个在性能和能效两个维度上都存在的根本性限制 。

#### 性能瓶颈：吞吐率匹配的视角

我们可以通过一个简单的吞吐率模型来精确地量化性能瓶颈。一个计算系统的持续吞吐率 $T$ 受限于其最慢的服务阶段，即计算阶段的最大服务速率 $R_{\text{compute}}$ 和内存阶段的最大服务速率 $R_{\text{memory}}$ 中的较小者：

$$
T = \min(R_{\text{compute}}, R_{\text{memory}})
$$

其中，$R_{\text{compute}}$ 是处理器的峰值计算速率（例如，以每秒万亿次操作，即TOPS为单位）。而 $R_{\text{memory}}$ 则由内存系统的带宽 $B$（例如，以GB/s为单位）和每次操作所需的数据传输量（或称为“数据足迹”）$D$（以B/op为单位）共同决定：

$$
R_{\text{memory}} = \frac{B}{D}
$$

当 $R_{\text{memory}}  R_{\text{compute}}$ 时，系统被称为**内存受限**（memory-bound）。在这种情况下，尽管处理器拥有强大的计算潜力，但其大部分时间都在空闲等待数据的到来或结果的[写回](@entry_id:756770)，导致实际性能远低于其峰值性能。

考虑一个假设场景 ：一个峰值计算能力为 $1\,\mathrm{TOPS}$ 的常规处理器执行一个流式向量乘加累积任务。假设每次操作需要从片外DRAM读取两个32位操作数并[写回](@entry_id:756770)一个32位结果，且没有数据复用。那么，每次操作的数据足迹为 $D = 2 \times 4\,\mathrm{B} + 4\,\mathrm{B} = 12\,\mathrm{B/op}$。如果片外[内存带宽](@entry_id:751847)为 $B = 64\,\mathrm{GB/s}$，那么内存能支持的最大操作速率为：

$$
R_{\text{memory}} = \frac{64 \times 10^9\,\mathrm{B/s}}{12\,\mathrm{B/op}} \approx 5.33 \times 10^9\,\mathrm{ops/s} = 5.33\,\mathrm{GOPS}
$$

系统的持续吞吐率因此被限制在 $T = \min(1000\,\mathrm{GOPS}, 5.33\,\mathrm{GOPS}) = 5.33\,\mathrm{GOPS}$。这意味着，由于[内存带宽](@entry_id:751847)的限制，这台拥有1 TOPS峰值算力的处理器，其实际性能仅发挥了约 $0.5\%$。

值得强调的是，这种带宽瓶颈与现代处理器中常见的缓存未命中（cache miss）效应在概念上是不同的。缓存是一种用于缓解**平均访问延迟**的[微架构](@entry_id:751960)技术，它通过存储常用数据来减少对慢速[主存](@entry_id:751652)的访问次数。缓存未命中主要引入**延迟惩罚**，导致处理器停顿。然而，[冯·诺依曼瓶颈](@entry_id:1133907)是一个根本性的**速率匹配**问题。即使通过[乱序执行](@entry_id:753020)和预取等技术完美地隐藏了所有缓存未命中的延迟，数据仍然必须通过有限的内存总线进行传输。如果一个工作负载所需的数据传输速率超过了总线带宽，系统无论如何都将受限于[内存带宽](@entry_id:751847) 。

#### [能效](@entry_id:272127)瓶颈：数据移动的主导能耗

除了性能限制，数据移动的能耗是另一个关键挑战。在当代技术节点中，从片外DRAM中移动一个数据字的能量开销可能比在处理器内部对该数据字执行一次简单的算术运算（如乘法或加法）的能量开销高出几个数量级。

我们可以用一个简化的总能耗模型来阐明这一点 。假设一个算法的总能耗 $E_{\text{total}}$ 由计算能耗和数据移动能耗两部分组成：

$$
E_{\text{total}} = N_{\text{ops}} E_{\text{MAC}} + N_{\text{words}} E_{\text{move}}
$$

其中，$N_{\text{ops}}$ 是乘加累积（MAC）操作的总次数，$E_{\text{MAC}}$ 是单次MAC操作的能耗；$N_{\text{words}}$ 是在计算单元与内存边界之间移动的数据字总数，$E_{\text{move}}$ 是移动一个数据字的能耗。

IMC架构的主要优势在于通过在内存内部或近内存处执[行运算](@entry_id:149765)，显著减少 $N_{\text{words}}$。一个有趣的问题是：在什么条件下，减少数据移动（即减小 $N_{\text{words}}$）比通过算法优化减少计算量（即减小 $N_{\text{ops}}$）能带来更大的能量收益？我们可以通过比较等比例缩减 $N_{\text{words}}$ 和 $N_{\text{ops}}$ 对总能耗的贡献来找到答案。对 $E_{\text{total}}$ 求[微分](@entry_id:158422)，我们发现当数据移动的总能耗超过计算的总能耗时，优先优化数据移动是更有利的，即：

$$
N_{\text{words}} E_{\text{move}}  N_{\text{ops}} E_{\text{MAC}}
$$

整理上式，我们得到一个关于**计算密度**（computational density）或**[算术强度](@entry_id:746514)**（arithmetic intensity）的倒数，$N_{\text{words}}/N_{\text{ops}}$ 的不等式：

$$
\frac{N_{\text{words}}}{N_{\text{ops}}}  \frac{E_{\text{MAC}}}{E_{\text{move}}}
$$

这个不等式清晰地指出了IMC的适用领域：对于那些每次操作需要移动大量数据（即 $N_{\text{words}}/N_{\text{ops}}$ 较大）的应用，[内存计算](@entry_id:1122818)的节[能效](@entry_id:272127)果最为显著。例如，假设一次MAC操作的能耗 $E_{\text{MAC}} = 1\,\mathrm{pJ}$，而一次片外数据移动的能耗 $E_{\text{move}} = 50\,\mathrm{pJ}$，那么能耗比为 $E_{\text{MAC}}/E_{\text{move}} = 0.02$。这意味着，只要一个算法平均每次操作需要移动的数据量超过 $0.02$ 个字，其总能耗就由数据移动主导，此时IMC将成为一个极具吸[引力](@entry_id:189550)的能效优化方案 。

### [内存计算](@entry_id:1122818)范式

为了应对冯·诺依曼瓶颈，内存计算的核心思想是**将计算物理地执行在存储阵列内部或其紧邻位置**，从而最大限度地减少处理器和[主存](@entry_id:751652)之间的[数据传输](@entry_id:276754) 。这不仅仅是一种简单的优化，而是一种计算范式的转变。它通过将数据密集型的计算原语，特别是那些构成大规模[线性变换](@entry_id:149133)基础的**线性代数累积原语**（如加权求和、点积和乘加累积），从CPU的[算术逻辑单元](@entry_id:178218)（ALU）中**重新定位**到内存组织（memory fabric）中。

在物理层面，IMC利用存储单元的固有物理定律来实现计算。例如，在执行向量-[矩阵乘法](@entry_id:156035) $\mathbf{y} = \mathbf{W}\mathbf{x}$ 时，输入向量 $\mathbf{x}$ 的分量可以被编码为施加在[存储阵列](@entry_id:174803)行线（word-lines）上的电压，而权重矩阵 $\mathbf{W}$ 的元素则被编码为阵列中交叉点器件的**电导**（conductance）。根据**欧姆定律** ($I = GV$)，流过每个交叉点器件的电流即为输入电压与权电导的乘积。接着，根据**基尔霍夫电流定律**（KCL），汇集在每条列线（bit-line）上的总电流就是所有行贡献电流的线性叠加。这样，一次模拟的读操作就能在物理层面上并行地完成整个点积运算。

通过这种方式，IMC极大地降低了每次操作所需的片外[数据传输](@entry_id:276754)量 $D$。在  的例子中，IMC架构通过将权重存储在片上，并利用局部累积，将片外数据足迹从 $12\,\mathrm{B/op}$ 锐减至 $4.25\,\mathrm{B/op}$。这使得内存支持的吞吐率上限提升至：

$$
R_{\text{memory, IMC}} = \frac{64 \times 10^9\,\mathrm{B/s}}{4.25\,\mathrm{B/op}} \approx 15.06\,\mathrm{GOPS}
$$

相比于传统架构的 $5.33\,\mathrm{GOPS}$，这是一个显著的提升，清晰地展示了IMC在缓解性能瓶颈方面的巨大潜力。

### IMC的物理实现机制

实现内存计算有多种技术路径，它们在计算方式、所依赖的存储技术以及性能特征上各有不同。广义上，我们可以将它们分为模拟IMC和数字IMC两大类。

#### 模拟IMC vs. 数字IMC

**模拟[内存计算](@entry_id:1122818)（Analog IMC）**直接利用物理定律（如[欧姆定律](@entry_id:276027)和基尔霍夫定律）进行计算。如前所述，通过将输入编码为电压、权重编码为电导，[存储阵列](@entry_id:174803)的列电流自然地形成了乘[积之和](@entry_id:266697)。这个过程在本质上是并行的，一次“读”操作即可完成整个向量的点积运算。计算结果是一个模拟物理量（电流或电荷），需要通过**模数转换器（[ADC](@entry_id:200983)）**转换回数字域。这种方法的优点是极高的并行度和潜在的[能效](@entry_id:272127)，但其精度受到各种物理非理想性的制约，包括器件差异、热噪声和[ADC](@entry_id:200983)的[量化误差](@entry_id:196306) 。

**数字内存计算（Digital IMC）**则在标准的数字存储单元（如SRAM）内部或其周边执行位级（bit-wise）逻辑运算。一个多比特的乘法被分解为一系列的[位运算](@entry_id:172125)。例如，一个 $b_w$ 位权重与一个 $b_a$ 位输入的乘法可以分解为 $b_w \times b_a$ 个单位比特的乘积（通常是逻辑与或异或非操作）。在每个时钟周期内，[存储阵列](@entry_id:174803)执行一次或多次这样的位级运算，并通过**群体计数（population count）**等方式累加结果。最终，通过数字域的移位和加法操作重构出完整的多比特乘积结果。数字IMC的计算过程是离散和串行的（就比特位而言），完成一次 $b_w \times b_a$ 的乘法大约需要 $b_w \times b_a$ 个[位运算](@entry_id:172125)周期。它的主要优势在于，在没有时序错误的前提下，其算术运算是精确的，不受模拟噪声的影响。然而，其[吞吐量](@entry_id:271802)受限于位串行操作的延迟 。

#### 关键使能存储技术

IMC架构的实现与底层的存储技术密不可分。不同技术的物理特性决定了它们是否适合以及如何用于[内存计算](@entry_id:1122818) 。

*   **SRAM ([静态随机存取存储器](@entry_id:170500))**：SRAM通过交叉耦合的反相器构成的[双稳态锁存器](@entry_id:166609)存储二[进制](@entry_id:634389)信息。其固有的两个稳定平衡点使得存储中间的模拟状态变得不稳定。因此，标准SRAM本质上是数字器件，不适合直接存储多级模拟权重。然而，它被广泛用于数字IMC，其存储单元可以执行位级逻辑。

*   **DRAM (动态随机存取存储器)**：DRAM通过在电容器上[存储电荷](@entry_id:1132461)来记录信息。尽管电荷量是模拟的，但其易失性（电荷泄漏需要周期性刷新）、破坏性读出以及对[热噪声](@entry_id:139193)的敏感性（$k T / C$ 噪声）使其不适合用作存储长期、精确模拟权重的介质。不过，它可用于实现短暂的电荷域计算。

*   **RRAM (阻变存储器)** 与 **PCM (相变存储器)**：这两类新兴的非易失性存储器是模拟IMC最有前景的候选技术。它们都是两端器件，其电阻（或电导）状态可以通过施加电压或电流脉冲来改变。至关重要的是，它们的电导值可以被精确地调控到多个中间能级，从而实现**多级存储（multi-level）**。一个RRAM或PCM单元的电导 $G$ 可以直接作为神经网络的突触权重，其阵列天然地实现了基于 $I = GV$ 的向量-[矩阵乘法](@entry_id:156035)。尽管它们也面临着[器件变异性](@entry_id:1123623)、状态漂移等挑战，但其基本物理原理与模拟IMC的需求高度契合。

*   **FeFET (铁电[场效应晶体管](@entry_id:1124930))**：FeFET是一种三端器件，它在传统晶体管的栅极结构中引入了[铁电材料](@entry_id:273847)。通过施加栅极脉冲改变铁电层的极化状态，可以非易失地调制晶体管的**阈值电压 $V_T$**。由于晶体管的漏极电流是栅极电压和阈值电压的函数，可编程的 $V_T$ 就能够像电导一样编码模拟权重。通过部分极化切换，FeFET同样可以实现多级权重存储。

### 模拟IMC的挑战与非理想性

尽管模拟IMC在理论上具有极高的[能效](@entry_id:272127)和并行度，但在实际应用中，它必须克服一系列源于物理世界的非理想性。这些挑战从根本上决定了模拟IMC系统的精度和可靠性。

#### 噪声与精度限制

模拟计算的精度受到各种随机噪声源的根本限制。一个典型的模拟IMC读出通路由一个[跨阻放大器](@entry_id:275441)（TIA）将列电流转换为电压，或一个电容器在一段时间[内积](@entry_id:750660)分累积电荷。

*   **基本噪声源**：在电流域求和中，主要的噪声源是与[直流偏置](@entry_id:271748)电流相关的**散粒噪声（shot noise）**和放大器自身的[电子噪声](@entry_id:894877)。在电荷域积分中，核心噪声是与电容采样相关的**[热噪声](@entry_id:139193)（thermal noise）**，其方差由著名的 $k T / C$ 关系决定，其中 $k$ 是玻尔兹曼常数，$T$ 是绝对温度，$C$ 是积分电容。这意味着为了降低噪声（即提高[信噪比](@entry_id:271861)），必须增大电容，但这又会带来面积和功耗的开销。设计者必须在噪声、[线性范围](@entry_id:181847)、速度和功耗之间做出权衡，例如，通过计算满足噪声和信号摆幅约束的最小电容值来指导电路设计 。

*   **噪声与变异的谱系**：模拟IMC中的误差源可以根据其时间和空间相关性进行细致分类 。
    *   **静态误差**：**器件间（Device-to-Device, D2D）差异**是主要的静态误差源。由于制造工艺的微观不均匀性，即使设计上完全相同的两个存储单元，其物理特性（如电导）也会有微小的固定差异。这种误差对于一个给定的芯片是固定的“模式噪声”，不会随着时间改变，因此无法通过[重复测量](@entry_id:896842)求平均来消除。其对点积运算结果的影响方差随着参与计算的单元数量 $N$ 的增加而累积。
    *   **动态[随机误差](@entry_id:144890)**：
        *   **周期与周期（Cycle-to-Cycle, C2C）变化**：指器件在每次被编程到相同目标状态时表现出的微小随机波动。这种误差在每次操作中是独立的，因此可以通过对多次独立操作的结果进行平均来有效降低，其标准差随平均次数 $M$ 的增加而以 $1/\sqrt{M}$ 的速率减小。
        *   **[随机电报噪声](@entry_id:269610)（Random Telegraph Noise, RTN）**：源于器件内部单个或少数缺陷（陷阱）对载流子的随机俘获和释放，导致器件电导在两个或多个离散能级之间随机跳变。其影响与噪声特征时间 $\tau$ 和[信号积分](@entry_id:175426)时间 $T$ 的相对关系密切。当 $\tau \gg T$ 时，RTN在单次测量中表现为准静态偏移，类似于D2D误差；但若在远大于 $\tau$ 的时间尺度上进行多次测量，其贡献可以被平均掉。
        *   **1/f 噪声（闪烁噪声）**：是一种在低频段占据主导地位的噪声，其[功率谱密度](@entry_id:141002)与频率 $f$ 成反比。由于其[能量集中](@entry_id:203621)在低频，简单的长[时间平均](@entry_id:267915)对其抑制效果不佳。在精密模拟电路中，通常采用**调制（modulation）**或**斩波（chopping）**技术，将信号搬移到[噪声功率谱密度](@entry_id:274939)较低的高频区域进行处理，从而有效规避1/f噪声的影响。

#### 有符号权重的表示方案

另一个实际挑战是，像RRAM和PCM这样的器件，其物理电导值 $G$ 是非负的。然而，许多神经网络模型需要有符号（正负）的权重。为了在物理上实现这一点，研究人员提出了多种编码方案 。

*   **基线偏移编码（Multi-level with Baseline）**：此方案将一个有符号权重 $w$ [线性映射](@entry_id:185132)到一个以基线电导 $G_b$ 为中心的电导值 $G = G_b + \alpha w$。例如，可以将权重范围 $[-W_{\max}, +W_{\max}]$ 映射到电导范围 $[G_{\min}, G_{\max}]$。读出时，需要从信号单元的输出电流中减去一个由基准单元（其电导为 $G_b$）产生的参考电流。这种方法的代价是信号的动态范围减半，并且总噪声是信号单元和基准单元噪声功率之和。

*   **差分编码（Differential Encoding）**：每个权重由一对（两个）存储单元 $(G^+, G^-)$ 来表示，其有效值为 $w \propto (G^+ - G^-)$。要表示一个正权重，可以设置 $G^+  G^-$；反之，表示负权重则设置 $G^-  G^+$。这种方法可以利用整个电导动态范围，但代价是硬件面积加倍。总噪声同样是两个单元的噪声功率之和。

*   **位切片编码（Bit-sliced Encoding）**：将一个多比特的权重分解为其二进制位，每个（或每组）位由一个独立的物理单元来表示。在读出时，来自不同位切片的电流被外部的数字逻辑根据其对应的位权重（$2^0, 2^1, \dots$）进行加权求和。这种方法将精度问题转移到了数字域，但需要更复杂的片上或片外外围电路。

对这些方案的选择涉及到对动态范围、[信噪比](@entry_id:271861)、硬件面积和[电路复杂性](@entry_id:270718)的综合权衡。例如，差分编码相比基线偏移编码提供了更大的信号动态范围，但在相同的满幅值条件下，两者可能具有相似的总噪声功率（因为差分编码的一个单元电导可以为最小值，贡献较少噪声）。

#### [器件可靠性](@entry_id:1123620)

作为非易失性存储，用于IMC的器件还必须应对长期的可靠性问题，这些问题对推理（inference）和[片上学习](@entry_id:1129110)（on-chip learning）两种不同工作负载的影响截然不同 。

*   **耐久性（Endurance）**：指器件在性能劣化到不可接受之前所能承受的编程/擦除（写）操作的次数。对于需要频繁更新权重的**[片上学习](@entry_id:1129110)**应用，耐久性是一个极其关键的瓶颈，直接决定了模型的训练寿命。而对于**推理**应用，权重通常只在部署前编程一次，之后几乎全是读操作，因此对写耐久性的要求很低。

*   **保持力（Retention）**：指器件在无电源情况下保持其编程状态的能力。状态的丢失通常是热激活过程，其寿命随温度升高而呈指数级下降。对于**推理**应用，权重必须在产品的整个生命周期内保持稳定，因此长期的保持力至关重要。任何状态的丢失都会导致模型精度的永久性下降。相反，在**[片上学习](@entry_id:1129110)**中，由于权重会周期性地被更新，学习算法可以自然地纠正因保持力不足而在两次更新之间发生的微小状态漂移，因此对长期保持力的要求相对宽松。

*   **漂移（Drift）**：是某些材料（如PCM）中表现出的一种特殊的保持力问题，即器件的电导在编程后会随时间发生缓慢、系统性的变化（通常遵循幂律，如 $G(t) \propto t^{-\nu}$）。对于**推理**，这是一种必须在系统层面进行补偿的系统误差。对于**[片上学习](@entry_id:1129110)**，有趣的现象是，学习算法（作为一种闭环反馈系统）会自动尝试纠正漂移，但这种纠正动作本身就是“写”操作，会反过来消耗器件宝贵的耐久性。这揭示了不同可靠性机制之间复杂的相互作用。

### 系统级设计与误差预算

一个实用的IMC宏单元设计需要一个系统性的方法来管理上述所有非理想性。**误差预算（Error Budget）**就是这样一个关键的设计原则和工具 。其目标是是将系统总输出[误差控制](@entry_id:169753)在可接受的范围内，并将这个总误差限额合理地分配给各个独立的误差源。

一个典型的IMC宏单元的误差源包括：输入DAC的**[量化误差](@entry_id:196306)**、权重编程的**随机噪声**、电路的**热噪声**、输出[ADC](@entry_id:200983)的**[量化误差](@entry_id:196306)**以及互连线上的**[IR压降](@entry_id:272464)**等。基于[线性叠加原理](@entry_id:196987)，如果这些误差源是零均值且相互独立的，那么总输出误差的方差 $\sigma_e^2$ 就是各个误差源贡献到输出的方差之和：

$$
\sigma_e^2 = \sum_{i} \sigma_{i, \text{out}}^2 = \sum_{i} (\alpha_i \sigma_i)^2
$$

这里，$\sigma_i$ 是第 $i$ 个原始误差源的[均方根](@entry_id:263605)（RMS）值，而 $\alpha_i$ 是其传播到输出端的**敏感度系数**。例如，对于输入[量化误差](@entry_id:196306) $\sigma_{xq}$，其对点积输出的影响被权重向量的[L2范数](@entry_id:172687) $\|\mathbf{w}\|_2$ 放大，因此其敏感度系数 $\alpha_{xq} = \|\mathbf{w}\|_2$。对于权重编程误差 $\sigma_{wp}$，其敏感度系数则为输入向量的[L2范数](@entry_id:172687) $\|\mathbf{x}\|_2$。

误差预算的目标是确保 $\sigma_e^2 \le \sigma_{\text{tot}}^2$，其中 $\sigma_{\text{tot}}$ 是系统允许的总输出RMS误差。接下来的关键问题是如何在满足这个约束的前提下，为每个 $\sigma_i$ 分配预算。一个简单的（但通常不是最优的）方法是均分输出端方差贡献。一个更严谨的方法是求解一个**约束优化问题**：在满足总误差预算的条件下，最小化一个与实现各项精度指标相关的总“设计成本”函数。例如，如果实现更低的噪声 $\sigma_i$ 的成本与 $\sigma_i^{-2}$ 成正比，那么就可以通过[拉格朗日乘子法](@entry_id:176596)等数学工具，找到一个在成本和性能之间达到最佳平衡的最优预算分配方案 。

通过这种系统化的方法，设计者可以从全局视角权衡不同技术选择，例如，是花费更多成本来提高[ADC](@entry_id:200983)的分辨率，还是投资于具有更低编程噪声的存储器件技术，从而在满足系统整体精度要求的同时，实现最优的资源配置。