{
    "hands_on_practices": [
        {
            "introduction": "The foundational analytical placement methods model the complex interconnect of a circuit as a simpler system of interconnected springs. By minimizing the total potential energy of these springs, which is a quadratic function of the cell coordinates, we can find an optimal placement that minimizes wirelength. This exercise guides you through the process of constructing the quadratic energy function for a small circuit and solving the resulting linear system to find the optimal cell positions. ",
            "id": "4282571",
            "problem": "Consider an instance of analytical placement in Electronic Design Automation (EDA), where nets are modeled as linear springs and quadratic placement minimizes a convex energy derived from pairwise spring extensions. In one spatial dimension, let there be three movable standard cells with unknown horizontal coordinates $x_{1}$, $x_{2}$, and $x_{3}$ (in micrometers), and two fixed terminals with known coordinates. The interconnect topology is given by two nets:\n- Net $\\mathcal{N}_{1}$ connects the pins of movable cells $1$ and $2$ and a fixed terminal $f_{1}$ located at $x_{f_{1}}=0$.\n- Net $\\mathcal{N}_{2}$ connects the pins of movable cells $2$ and $3$ and a fixed terminal $f_{2}$ located at $x_{f_{2}}=12$.\n\nAssume a one-dimensional placement model along the $x$-axis. Use the following fundamental base:\n- Springs model: Each pair of pins connected within a net contributes a spring with Hookean energy proportional to the squared difference of their $x$-coordinates. Specifically, under the clique expansion model for hyperedges, each net of cardinality $k$ is replaced by a complete graph on its $k$ pins, with unit spring constant $w=1$ on each pair.\n- The total quadratic energy is the sum of spring energies over all pairs. Minimization of this convex quadratic energy subject to fixed terminal positions yields linear normal equations of the form $L \\mathbf{x} = \\mathbf{b}$, where $L$ is a symmetric positive definite matrix when the netlist includes fixed terminals that connect to the movable cells.\n\nTasks:\n1. Starting from the described springs model and the clique expansion assumption, write the total quadratic energy $Q(\\mathbf{x})$ in terms of $x_{1}$, $x_{2}$, $x_{3}$, $x_{f_{1}}$, and $x_{f_{2}}$ by summing contributions for all spring pairs induced by $\\mathcal{N}_{1}$ and $\\mathcal{N}_{2}$.\n2. Derive the linear system for the stationary point by setting the gradient of $Q(\\mathbf{x})$ with respect to $(x_{1}, x_{2}, x_{3})$ equal to zero. Explicitly assemble the coefficient matrix $L$ and the right-hand side vector $\\mathbf{b}$.\n3. Solve $L \\mathbf{x} = \\mathbf{b}$ to obtain the optimal coordinates $(x_{1}, x_{2}, x_{3})$.\n4. Briefly interpret the resulting coordinates in physical terms based on the spring model.\n\nExpress the final positions in micrometers. The final answer must be the ordered triple $(x_{1}, x_{2}, x_{3})$. No rounding is required; report exact values.",
            "solution": "The problem is a valid instance of one-dimensional analytical placement, a standard topic in Electronic Design Automation (EDA). It is scientifically grounded, well-posed, and all necessary information is provided for a unique solution. We can proceed with the derivation.\n\nThe problem involves three movable cells with coordinates $x_1, x_2, x_3$ and two fixed terminals at $x_{f_1}=0$ and $x_{f_2}=12$. The connectivity is defined by two nets: $\\mathcal{N}_1 = \\{1, 2, f_1\\}$ and $\\mathcal{N}_2 = \\{2, 3, f_2\\}$.\n\n**1. Total Quadratic Energy $Q(\\mathbf{x})$**\n\nThe energy is modeled using the clique expansion model, where each net of cardinality $k$ is decomposed into a complete graph of $\\binom{k}{2}$ edges. Each edge corresponds to a spring with energy $w(x_i-x_j)^2$, where the spring constant is given as $w=1$.\n\nFor net $\\mathcal{N}_1 = \\{1, 2, f_1\\}$, the cardinality is $k=3$. The clique expansion results in $\\binom{3}{2}=3$ pairs: $(1, 2)$, $(1, f_1)$, and $(2, f_1)$. The energy contribution from $\\mathcal{N}_1$, denoted $Q_1$, is the sum of the energies of these three springs:\n$$Q_1 = (x_1 - x_2)^2 + (x_1 - x_{f_1})^2 + (x_2 - x_{f_1})^2$$\n\nFor net $\\mathcal{N}_2 = \\{2, 3, f_2\\}$, the cardinality is also $k=3$. The clique expansion results in $\\binom{3}{2}=3$ pairs: $(2, 3)$, $(2, f_2)$, and $(3, f_2)$. The energy contribution from $\\mathcal{N}_2$, denoted $Q_2$, is:\n$$Q_2 = (x_2 - x_3)^2 + (x_2 - x_{f_2})^2 + (x_3 - x_{f_2})^2$$\n\nThe total quadratic energy $Q(\\mathbf{x})$ is the sum of the energies from all nets, $Q(\\mathbf{x}) = Q_1 + Q_2$.\n$$Q(x_1, x_2, x_3) = (x_1 - x_2)^2 + (x_1 - x_{f_1})^2 + (x_2 - x_{f_1})^2 + (x_2 - x_3)^2 + (x_2 - x_{f_2})^2 + (x_3 - x_{f_2})^2$$\n\nSubstituting the given fixed terminal positions, $x_{f_1}=0$ and $x_{f_2}=12$:\n$$Q(x_1, x_2, x_3) = (x_1 - x_2)^2 + (x_1 - 0)^2 + (x_2 - 0)^2 + (x_2 - x_3)^2 + (x_2 - 12)^2 + (x_3 - 12)^2$$\n$$Q(x_1, x_2, x_3) = (x_1 - x_2)^2 + x_1^2 + x_2^2 + (x_2 - x_3)^2 + (x_2 - 12)^2 + (x_3 - 12)^2$$\n\n**2. Derivation of the Linear System $L\\mathbf{x} = \\mathbf{b}$**\n\nTo find the optimal coordinates that minimize the energy $Q(\\mathbf{x})$, we must find the stationary point where the gradient of $Q$ with respect to the vector of movable coordinates $\\mathbf{x} = (x_1, x_2, x_3)^T$ is zero. That is, $\\nabla Q = \\left(\\frac{\\partial Q}{\\partial x_1}, \\frac{\\partial Q}{\\partial x_2}, \\frac{\\partial Q}{\\partial x_3}\\right) = (0, 0, 0)$.\n\nWe compute the partial derivatives:\n\nFor $x_1$:\n$$\\frac{\\partial Q}{\\partial x_1} = \\frac{\\partial}{\\partial x_1} \\left[ (x_1 - x_2)^2 + x_1^2 \\right] = 2(x_1 - x_2)(1) + 2x_1 = 4x_1 - 2x_2$$\n\nFor $x_2$:\n$$\\frac{\\partial Q}{\\partial x_2} = \\frac{\\partial}{\\partial x_2} \\left[ (x_1 - x_2)^2 + x_2^2 + (x_2 - x_3)^2 + (x_2 - 12)^2 \\right]$$\n$$\\frac{\\partial Q}{\\partial x_2} = 2(x_1 - x_2)(-1) + 2x_2 + 2(x_2 - x_3)(1) + 2(x_2 - 12)(1)$$\n$$\\frac{\\partial Q}{\\partial x_2} = -2x_1 + 2x_2 + 2x_2 + 2x_2 - 2x_3 + 2x_2 - 24 = -2x_1 + 8x_2 - 2x_3 - 24$$\n\nFor $x_3$:\n$$\\frac{\\partial Q}{\\partial x_3} = \\frac{\\partial}{\\partial x_3} \\left[ (x_2 - x_3)^2 + (x_3 - 12)^2 \\right] = 2(x_2 - x_3)(-1) + 2(x_3 - 12)(1)$$\n$$\\frac{\\partial Q}{\\partial x_3} = -2x_2 + 2x_3 + 2x_3 - 24 = -2x_2 + 4x_3 - 24$$\n\nSetting these derivatives to zero gives the system of linear equations:\n$$4x_1 - 2x_2 = 0$$\n$$-2x_1 + 8x_2 - 2x_3 = 24$$\n$$-2x_2 + 4x_3 = 24$$\n\nThis system is of the form $L \\mathbf{x} = \\mathbf{b}$, where $\\mathbf{x} = \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix}$. The coefficient matrix $L$ and the right-hand side vector $\\mathbf{b}$ are:\n$$L = \\begin{pmatrix} 4 & -2 & 0 \\\\ -2 & 8 & -2 \\\\ 0 & -2 & 4 \\end{pmatrix}, \\quad \\mathbf{b} = \\begin{pmatrix} 0 \\\\ 24 \\\\ 24 \\end{pmatrix}$$\n\n**3. Solving the Linear System**\n\nWe solve the system of equations:\n1. $4x_1 - 2x_2 = 0$\n2. $-2x_1 + 8x_2 - 2x_3 = 24$\n3. $-2x_2 + 4x_3 = 24$\n\nFrom equation (1), we have $4x_1 = 2x_2$, which simplifies to $x_2 = 2x_1$.\n\nFrom equation (3), we can simplify by dividing by $2$ to get $-x_2 + 2x_3 = 12$, which gives $2x_3 = x_2 + 12$.\n\nNow, substitute $x_2 = 2x_1$ into the expression for $2x_3$:\n$2x_3 = (2x_1) + 12$, which implies $x_3 = x_1 + 6$.\n\nNext, we substitute both $x_2 = 2x_1$ and $x_3 = x_1 + 6$ into equation (2):\n$$-2x_1 + 8(2x_1) - 2(x_1 + 6) = 24$$\n$$-2x_1 + 16x_1 - 2x_1 - 12 = 24$$\n$$12x_1 = 36$$\n$$x_1 = 3$$\n\nNow we find $x_2$ and $x_3$ by back-substitution:\n$$x_2 = 2x_1 = 2(3) = 6$$\n$$x_3 = x_1 + 6 = 3 + 6 = 9$$\n\nThe optimal coordinates are $(x_1, x_2, x_3) = (3, 6, 9)$.\n\n**4. Physical Interpretation**\n\nThe stationary point equations can be interpreted as force-balance equations. For a given movable cell $i$, its optimal position $x_i$ is the weighted average of the positions of all other pins (movable or fixed) connected to it via clique-decomposed springs. The force-balance equation for cell $i$ is:\n$$(\\sum_{j \\neq i, \\text{connected}} w_{ij}) x_i = \\sum_{j \\neq i, \\text{connected}} w_{ij} x_j$$\nSince all weights $w_{ij}=1$, this simplifies to $x_i$ being the average position of its neighbors in the clique graph.\n- **Cell 1**: is connected to cell $2$ and terminal $f_1$. Its degree is $2$. Thus, $2x_1 = x_2 + x_{f_1}$. Using the calculated values: $2(3) = 6+0$. This holds.\n- **Cell 2**: is connected to cells $1, 3$ and terminals $f_1, f_2$. Its degree is $4$. Thus, $4x_2 = x_1 + x_3 + x_{f_1} + x_{f_2}$. Using the calculated values: $4(6) = 3+9+0+12 = 24$. This holds.\n- **Cell 3**: is connected to cell $2$ and terminal $f_2$. Its degree is $2$. Thus, $2x_3 = x_2 + x_{f_2}$. Using the calculated values: $2(9) = 6+12 = 18$. This holds.\n\nPhysically, each movable cell settles at the \"center of gravity\" of the components to which it is connected by springs. The solution $(x_1, x_2, x_3) = (3, 6, 9)$ places the movable cells at evenly spaced intervals between the fixed terminals at $0$ and $12$. The full arrangement is $x_{f_1}=0, x_1=3, x_2=6, x_3=9, x_{f_2}=12$. This perfect arithmetic progression arises from the symmetry in the connectivity.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n3 & 6 & 9\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Shifting from continuous optimization, we now explore a discrete, partitioning-based approach to placement. Min-cut algorithms work by recursively dividing cells into partitions while minimizing the number of connections cut between them. This exercise provides hands-on practice with the seminal Fiduccia-Mattheyses (FM) algorithm, an efficient heuristic for iteratively improving a hypergraph partition to reduce the cutsize. ",
            "id": "4282597",
            "problem": "Consider a hypergraph partitioning instance arising in Electronic Design Automation (EDA). The hypergraph has vertex set $\\{v_1, v_2, v_3, v_4, v_5, v_6\\}$ and hyperedge set $\\{e_1, e_2, e_3, e_4, e_5, e_6\\}$ with integer weights $w(e_i)$ as specified below. The initial bipartition is $A = \\{v_1, v_2, v_3\\}$ and $B = \\{v_4, v_5, v_6\\}$. All vertices have unit weight. The hyperedges are:\n- $e_1 = \\{v_1, v_4\\}$ with $w(e_1) = 2$,\n- $e_2 = \\{v_1, v_2, v_4\\}$ with $w(e_2) = 1$,\n- $e_3 = \\{v_2, v_3, v_5\\}$ with $w(e_3) = 2$,\n- $e_4 = \\{v_3, v_6\\}$ with $w(e_4) = 1$,\n- $e_5 = \\{v_4, v_5, v_6\\}$ with $w(e_5) = 3$,\n- $e_6 = \\{v_2, v_6\\}$ with $w(e_6) = 1$.\n\nThe cutsize of a bipartition is defined as the sum of weights of hyperedges that have at least one pin in $A$ and at least one pin in $B$. A single pass of the Fiducciaâ€“Mattheyses (FM) algorithm is performed as follows: iteratively select an unlocked vertex with maximum gain (the gain being the change in cutsize if that vertex is moved to the opposite block), move it, and lock it; break ties for equal gains by choosing the vertex with the smallest index. Continue until all vertices are locked. For this exercise, ignore balance constraints on block sizes throughout the pass.\n\nStarting strictly from the operational definition of cutsize and set membership, compute:\n1. The initial cutsize.\n2. The initial gain for each vertex based on moving it to the opposite block.\n3. The complete sequence of moves in one FM pass, showing at each step the selected vertex, its gain, and the updated cutsize.\n4. The cumulative cutsize improvement after each move, and the maximum cumulative improvement attained by any prefix of moves in the pass.\n\nYour final answer must be the single real-valued number equal to the maximum cumulative cutsize improvement achieved by the pass. No rounding is required. Express the final answer as a pure number without units.",
            "solution": "The problem is a valid exercise in applying the Fiduccia-Mattheyses (FM) algorithm to a hypergraph partitioning instance. All definitions are clear, the data is self-consistent, and the rules provided, including the tie-breaking condition, ensure a unique, deterministic sequence of operations. We will proceed with the solution by following the four specified computational steps.\n\nThe hypergraph is defined by the vertex set $V = \\{v_1, v_2, v_3, v_4, v_5, v_6\\}$ and the hyperedge set $E = \\{e_1, e_2, e_3, e_4, e_5, e_6\\}$. The initial bipartition is $A = \\{v_1, v_2, v_3\\}$ and $B = \\{v_4, v_5, v_6\\}$.\n\nThe hyperedges and their weights $w(e_i)$ are:\n- $e_1 = \\{v_1, v_4\\}$, $w(e_1) = 2$\n- $e_2 = \\{v_1, v_2, v_4\\}$, $w(e_2) = 1$\n- $e_3 = \\{v_2, v_3, v_5\\}$, $w(e_3) = 2$\n- $e_4 = \\{v_3, v_6\\}$, $w(e_4) = 1$\n- $e_5 = \\{v_4, v_5, v_6\\}$, $w(e_5) = 3$\n- $e_6 = \\{v_2, v_6\\}$, $w(e_6) = 1$\n\n### 1. Initial Cutsize Computation\n\nA hyperedge is cut if it has at least one vertex in partition $A$ and at least one vertex in partition $B$. We examine each hyperedge with respect to the initial partition $A = \\{v_1, v_2, v_3\\}$ and $B = \\{v_4, v_5, v_6\\}$.\n- $e_1 = \\{v_1, v_4\\}$: $v_1 \\in A$ and $v_4 \\in B$. It is cut.\n- $e_2 = \\{v_1, v_2, v_4\\}$: $\\{v_1, v_2\\} \\subset A$ and $v_4 \\in B$. It is cut.\n- $e_3 = \\{v_2, v_3, v_5\\}$: $\\{v_2, v_3\\} \\subset A$ and $v_5 \\in B$. It is cut.\n- $e_4 = \\{v_3, v_6\\}$: $v_3 \\in A$ and $v_6 \\in B$. It is cut.\n- $e_5 = \\{v_4, v_5, v_6\\}$: All vertices are in $B$. It is not cut.\n- $e_6 = \\{v_2, v_6\\}$: $v_2 \\in A$ and $v_6 \\in B$. It is cut.\n\nThe initial cutsize $C_0$ is the sum of the weights of the cut hyperedges:\n$$C_0 = w(e_1) + w(e_2) + w(e_3) + w(e_4) + w(e_6) = 2 + 1 + 2 + 1 + 1 = 7$$\n\n### 2. Initial Gain Computation\n\nThe gain $G(v)$ of moving a vertex $v$ is the reduction in cutsize. For a hyperedge $e$ containing $v$, moving $v$ from its current block $S_v$ to the other block $T_v$:\n- Increases the cutsize by $w(e)$ if $e \\subseteq S_v$ before the move (contribution to gain is $-w(e)$).\n- Decreases the cutsize by $w(e)$ if $v$ is the only vertex of $e$ in $S_v$ (contribution to gain is $+w(e)$).\n- Does not change the cut status otherwise (contribution to gain is $0$).\n\nLet's compute the initial gains for all vertices.\n**For vertices in A ($v \\in \\{v_1, v_2, v_3\\}$):**\n- $G(v_1)$: $v_1$ is in $e_1, e_2$. For $e_1=\\{v_1, v_4\\}$, $|e_1 \\cap A|=1$, so moving $v_1$ un-cuts $e_1$. Gain contribution is $+w(e_1)=+2$. For $e_2=\\{v_1, v_2, v_4\\}$, $|e_2 \\cap A|=2$, so moving $v_1$ does not change the cut status. Gain contribution is $0$. Total $G(v_1) = 2$.\n- $G(v_2)$: $v_2$ is in $e_2, e_3, e_6$. For $e_2=\\{v_1, v_2, v_4\\}$, $|e_2 \\cap A|=2$, gain contribution $0$. For $e_3=\\{v_2, v_3, v_5\\}$, $|e_3 \\cap A|=2$, gain contribution $0$. For $e_6=\\{v_2, v_6\\}$, $|e_6 \\cap A|=1$, gain contribution $+w(e_6)=+1$. Total $G(v_2) = 1$.\n- $G(v_3)$: $v_3$ is in $e_3, e_4$. For $e_3=\\{v_2, v_3, v_5\\}$, $|e_3 \\cap A|=2$, gain contribution $0$. For $e_4=\\{v_3, v_6\\}$, $|e_4 \\cap A|=1$, gain contribution $+w(e_4)=+1$. Total $G(v_3) = 1$.\n\n**For vertices in B ($v \\in \\{v_4, v_5, v_6\\}$):**\n- $G(v_4)$: $v_4$ is in $e_1, e_2, e_5$. For $e_1=\\{v_1, v_4\\}$, $|e_1 \\cap B|=1$, gain contribution $+w(e_1)=+2$. For $e_2=\\{v_1, v_2, v_4\\}$, $|e_2 \\cap B|=1$, gain contribution $+w(e_2)=+1$. For $e_5=\\{v_4, v_5, v_6\\}$, $e_5 \\subseteq B$, moving $v_4$ cuts $e_5$, gain contribution $-w(e_5)=-3$. Total $G(v_4) = 2+1-3=0$.\n- $G(v_5)$: $v_5$ is in $e_3, e_5$. For $e_3=\\{v_2, v_3, v_5\\}$, $|e_3 \\cap B|=1$, gain contribution $+w(e_3)=+2$. For $e_5=\\{v_4, v_5, v_6\\}$, $e_5 \\subseteq B$, gain contribution $-w(e_5)=-3$. Total $G(v_5) = 2-3=-1$.\n- $G(v_6)$: $v_6$ is in $e_4, e_5, e_6$. For $e_4=\\{v_3, v_6\\}$, $|e_4 \\cap B|=1$, gain contribution $+w(e_4)=+1$. For $e_5=\\{v_4, v_5, v_6\\}$, $e_5 \\subseteq B$, gain contribution $-w(e_5)=-3$. For $e_6=\\{v_2, v_6\\}$, $|e_6 \\cap B|=1$, gain contribution $+w(e_6)=+1$. Total $G(v_6) = 1-3+1=-1$.\n\nInitial Gains Summary: $G(v_1)=2, G(v_2)=1, G(v_3)=1, G(v_4)=0, G(v_5)=-1, G(v_6)=-1$.\n\n### 3. Complete Sequence of Moves in One FM Pass\n\nWe iteratively select the unlocked vertex with the highest gain (smallest index for ties), move it, lock it, and update gains.\n\n**Step 1:**\n- **Selection:** Maximum gain is $G(v_1)=2$.\n- **Move:** Move $v_1$ from $A$ to $B$. Gain $g_1=2$. Lock $v_1$.\n- **State:** $A'=\\{v_2,v_3\\}$, $B'=\\{v_1,v_4,v_5,v_6\\}$. Cutsize $C_1 = C_0 - g_1 = 7-2=5$.\n- **Gain Update:** Recomputing for unlocked vertices: $G(v_2)=2, G(v_3)=1, G(v_4)=-5, G(v_5)=-1, G(v_6)=-1$.\n\n**Step 2:**\n- **Selection:** Maximum gain among unlocked vertices is $G(v_2)=2$.\n- **Move:** Move $v_2$ from $A'$ to $B'$. Gain $g_2=2$. Lock $v_2$.\n- **State:** $A''=\\{v_3\\}$, $B''=\\{v_1,v_2,v_4,v_5,v_6\\}$. Cutsize $C_2 = C_1 - g_2 = 5-2=3$.\n- **Gain Update:** Recomputing for unlocked vertices: $G(v_3)=3, G(v_4)=-6, G(v_5)=-3, G(v_6)=-3$.\n\n**Step 3:**\n- **Selection:** Maximum gain is $G(v_3)=3$.\n- **Move:** Move $v_3$ from $A''$ to $B''$. Gain $g_3=3$. Lock $v_3$.\n- **State:** $A'''=\\emptyset$, $B'''=\\{v_1,v_2,v_3,v_4,v_5,v_6\\}$. Cutsize $C_3 = C_2 - g_3 = 3-3=0$.\n- **Gain Update:** Recomputing for unlocked vertices: $G(v_4)=-6, G(v_5)=-5, G(v_6)=-5$.\n\n**Step 4:**\n- **Selection:** Maximum gain is $-5$, achieved by both $v_5$ and $v_6$. Tie-break rule selects the vertex with the smallest index, so we choose $v_5$.\n- **Move:** Move $v_5$ from $B'''$ to $A'''$. Gain $g_4=-5$. Lock $v_5$.\n- **State:** $A''''=\\{v_5\\}$, $B''''=\\{v_1,v_2,v_3,v_4,v_6\\}$. Cutsize $C_4 = C_3 - g_4 = 0-(-5)=5$.\n- **Gain Update:** Recomputing for unlocked vertices: $G(v_4)=-3, G(v_6)=-2$.\n\n**Step 5:**\n- **Selection:** Maximum gain is $G(v_6)=-2$.\n- **Move:** Move $v_6$ from $B''''$ to $A''''$. Gain $g_5=-2$. Lock $v_6$.\n- **State:** $A'''''=\\{v_5,v_6\\}$, $B'''''=\\{v_1,v_2,v_3,v_4\\}$. Cutsize $C_5 = C_4 - g_5 = 5-(-2)=7$.\n- **Gain Update:** Recomputing for the unlocked vertex: $G(v_4)=0$.\n\n**Step 6:**\n- **Selection:** Only $v_4$ is left unlocked.\n- **Move:** Move $v_4$ from $B'''''$ to $A'''''$. Gain $g_6=0$. Lock $v_4$.\n- **State:** $A''''''=\\{v_4,v_5,v_6\\}$, $B''''''=\\{v_1,v_2,v_3\\}$. Cutsize $C_6 = C_5 - g_6 = 7-0=7$.\n\n### 4. Cumulative Improvement and Maximum Improvement\n\nThe cumulative improvement after $k$ moves, denoted $\\Delta_k$, is the sum of the gains of the first $k$ moves, $\\Delta_k = \\sum_{i=1}^k g_i$. The maximum cumulative improvement is the maximum value in the sequence $\\Delta_1, \\Delta_2, \\dots, \\Delta_6$.\n\nThe sequence of moves, gains, updated cutsizes, and cumulative improvements is summarized below:\n\n| Step $k$ | Vertex Moved | Gain $g_k$ | Cutsize $C_k$ | Cumulative Improvement $\\Delta_k = \\sum_{i=1}^k g_i$ |\n|:---:|:---:|:---:|:---:|:---:|\n| $0$ | - | - | $7$ | $0$ |\n| $1$ | $v_1$ | $2$ | $5$ | $2$ |\n| $2$ | $v_2$ | $2$ | $3$ | $2+2=4$ |\n| $3$ | $v_3$ | $3$ | $0$ | $4+3=7$ |\n| $4$ | $v_5$ | $-5$ | $5$ | $7-5=2$ |\n| $5$ | $v_6$ | $-2$ | $7$ | $2-2=0$ |\n| $6$ | $v_4$ | $0$ | $7$ | $0+0=0$ |\n\nThe cumulative cutsize improvements at each step are $2, 4, 7, 2, 0, 0$.\nThe maximum value in this sequence is $7$. This is the maximum cumulative cutsize improvement achieved by any prefix of the move sequence. This improvement corresponds to the partition after $3$ moves, which has a cutsize of $0$.",
            "answer": "$$\\boxed{7}$$"
        },
        {
            "introduction": "Modern analytical placers must balance the primary goal of wirelength minimization with the practical necessity of avoiding cell overlaps. This is achieved by integrating sophisticated non-linear wirelength models, like the Log-Sum-Exp (LSE) approximation, with explicit density penalties. This advanced exercise demonstrates how a powerful optimization technique, Nesterov's accelerated gradient method, is used to take a single step toward a solution that balances these competing objectives. ",
            "id": "4282599",
            "problem": "Consider an analytical placement subproblem in Electronic Design Automation (EDA) for three standard-cell instances labeled $1$, $2$, and $3$ placed in the plane with coordinates $(x_i, y_i)$ for $i \\in \\{1,2,3\\}$. The netlist consists of two two-pin nets $N_1 = \\{1,2\\}$ and $N_2 = \\{2,3\\}$. The continuous wirelength objective is built from the log-sum-exp (LSE) approximation of the half-perimeter wirelength for each net in both $x$ and $y$ dimensions. Let the LSE smoothing parameter be $\\gamma = 0.5$. Define the LSE wirelength of a net $N$ as the sum of its $x$-component and $y$-component:\n$$\n\\mathrm{WL}_{\\mathrm{LSE}}(N) \\triangleq \\gamma \\ln\\!\\Bigg(\\sum_{i \\in N} \\exp\\!\\Big(\\frac{x_i}{\\gamma}\\Big)\\Bigg) + \\gamma \\ln\\!\\Bigg(\\sum_{i \\in N} \\exp\\!\\Big(-\\frac{x_i}{\\gamma}\\Big)\\Bigg) + \\gamma \\ln\\!\\Bigg(\\sum_{i \\in N} \\exp\\!\\Big(\\frac{y_i}{\\gamma}\\Big)\\Bigg) + \\gamma \\ln\\!\\Bigg(\\sum_{i \\in N} \\exp\\!\\Big(-\\frac{y_i}{\\gamma}\\Big)\\Bigg).\n$$\nThe total wirelength objective is $\\mathrm{WL}_{\\mathrm{LSE}}^{\\mathrm{tot}} \\triangleq \\sum_{N \\in \\{N_1, N_2\\}} \\mathrm{WL}_{\\mathrm{LSE}}(N)$.\n\nTo model spreading, use a single-bin density penalty centered at $(x_b, y_b) = (1.0, 1.0)$ with a separable isotropic Gaussian kernel of variance $\\sigma^2 = 1.0$. Let the cell areas be $a_1 = a_2 = a_3 = 1.0$ and the bin capacity be $C = 1.8$. Define the smooth density field contribution of cell $i$ as\n$$\nK_i \\triangleq \\exp\\!\\Bigg(-\\frac{(x_i - x_b)^{2} + (y_i - y_b)^{2}}{\\sigma^{2}}\\Bigg),\n$$\nand define the scalar density penalty energy\n$$\nE_{\\mathrm{dens}} \\triangleq \\frac{1}{2}\\Bigg(\\sum_{i=1}^{3} a_i K_i - C\\Bigg)^{2}.\n$$\nThe objective to be minimized is\n$$\nf(\\mathbf{x}) \\triangleq \\mathrm{WL}_{\\mathrm{LSE}}^{\\mathrm{tot}}(\\mathbf{x}) + \\eta\\, E_{\\mathrm{dens}}(\\mathbf{x}),\n$$\nwith density weight $\\eta = 0.3$ and placement vector $\\mathbf{x} \\triangleq (x_1, y_1, x_2, y_2, x_3, y_3)^{\\mathsf{T}}$.\n\nPerform one Nesterov accelerated gradient step with momentum $\\beta = 0.3$ and step size $\\alpha = 0.1$, using the standard two-sequence update\n$$\n\\mathbf{y}^{(k)} = \\mathbf{x}^{(k)} + \\beta\\big(\\mathbf{x}^{(k)} - \\mathbf{x}^{(k-1)}\\big), \\quad \\mathbf{x}^{(k+1)} = \\mathbf{y}^{(k)} - \\alpha\\, \\nabla f\\big(\\mathbf{y}^{(k)}\\big).\n$$\nThe current and previous placement vectors are\n$$\n\\mathbf{x}^{(k)} = \\big(1.2,\\, 0.8,\\, 0.0,\\, 1.5,\\, 2.0,\\, 0.2\\big)^{\\mathsf{T}}, \\quad \\mathbf{x}^{(k-1)} = \\big(1.0,\\, 1.0,\\, -0.2,\\, 1.3,\\, 1.8,\\, 0.0\\big)^{\\mathsf{T}}.\n$$\nStarting from the above definitions and fundamental calculus rules, compute $\\mathbf{y}^{(k)}$, derive the gradient $\\nabla f(\\mathbf{y}^{(k)})$ by combining the contributions from the LSE wirelength and the density penalty, and then compute $\\mathbf{x}^{(k+1)}$. Express your final answer as the ordered row vector of updated coordinates $(x_1^{(k+1)}, y_1^{(k+1)}, x_2^{(k+1)}, y_2^{(k+1)}, x_3^{(k+1)}, y_3^{(k+1)})$ and round each coordinate to four significant figures. No units are required.",
            "solution": "We proceed from first principles. The total objective is $f(\\mathbf{x}) = \\mathrm{WL}_{\\mathrm{LSE}}^{\\mathrm{tot}}(\\mathbf{x}) + \\eta\\, E_{\\mathrm{dens}}(\\mathbf{x})$ where the two terms are defined in the statement. The Nesterov step requires the extrapolated point $\\mathbf{y}^{(k)}$ and the gradient $\\nabla f(\\mathbf{y}^{(k)})$.\n\nFirst, compute the extrapolated point using the given momentum $\\beta = 0.3$:\n$$\n\\mathbf{y}^{(k)} = \\mathbf{x}^{(k)} + \\beta\\big(\\mathbf{x}^{(k)} - \\mathbf{x}^{(k-1)}\\big).\n$$\nCompute the difference $\\mathbf{x}^{(k)} - \\mathbf{x}^{(k-1)}$ coordinate-wise:\n\n$$\n\\begin{aligned}\nx_1: &\\; 1.2 - 1.0 = 0.2, \\quad y_1: \\; 0.8 - 1.0 = -0.2, \\\\\nx_2: &\\; 0.0 - (-0.2) = 0.2, \\quad y_2: \\; 1.5 - 1.3 = 0.2, \\\\\nx_3: &\\; 2.0 - 1.8 = 0.2, \\quad y_3: \\; 0.2 - 0.0 = 0.2.\n\\end{aligned}\n$$\n\nMultiplying by $\\beta = 0.3$ and adding to $\\mathbf{x}^{(k)}$ gives\n\n$$\n\\begin{aligned}\nx_1^{y} &= 1.2 + 0.3 \\cdot 0.2 = 1.26, & y_1^{y} &= 0.8 + 0.3 \\cdot (-0.2) = 0.74, \\\\\nx_2^{y} &= 0.0 + 0.3 \\cdot 0.2 = 0.06, & y_2^{y} &= 1.5 + 0.3 \\cdot 0.2 = 1.56, \\\\\nx_3^{y} &= 2.0 + 0.3 \\cdot 0.2 = 2.06, & y_3^{y} &= 0.2 + 0.3 \\cdot 0.2 = 0.26.\n\\end{aligned}\n$$\n\nWe denote the extrapolated coordinates by $(x_i^{y}, y_i^{y})$ for each $i$.\n\nNext, derive the gradient of the LSE wirelength term. For a net $N$ in the $x$-dimension, the contribution is\n$$\n\\mathrm{WL}_{x}(N) = \\gamma \\ln\\!\\Bigg(\\sum_{j \\in N} \\exp\\!\\Big(\\frac{x_j}{\\gamma}\\Big)\\Bigg) + \\gamma \\ln\\!\\Bigg(\\sum_{j \\in N} \\exp\\!\\Big(-\\frac{x_j}{\\gamma}\\Big)\\Bigg).\n$$\nBy the chain rule and the derivative of the natural logarithm and exponential, for any $i \\in N$,\n\n$$\n\\frac{\\partial \\mathrm{WL}_{x}(N)}{\\partial x_i} = \\frac{\\exp\\!\\big(x_i/\\gamma\\big)}{\\sum_{j \\in N} \\exp\\!\\big(x_j/\\gamma\\big)} - \\frac{\\exp\\!\\big(-x_i/\\gamma\\big)}{\\sum_{j \\in N} \\exp\\!\\big(-x_j/\\gamma\\big)}.\n$$\n\nAn identical expression holds in $y$ by replacing $x_i$ with $y_i$. The total LSE gradient at a point is the sum over nets containing cell $i$:\n\n$$\n\\Big(\\nabla \\mathrm{WL}_{\\mathrm{LSE}}^{\\mathrm{tot}}(\\mathbf{z})\\Big)_{x_i} = \\sum_{N: i \\in N} \\left[\\frac{\\exp\\!\\big(x_i/\\gamma\\big)}{\\sum_{j \\in N} \\exp\\!\\big(x_j/\\gamma\\big)} - \\frac{\\exp\\!\\big(-x_i/\\gamma\\big)}{\\sum_{j \\in N} \\exp\\!\\big(-x_j/\\gamma\\big)}\\right],\n$$\n\n\n$$\n\\Big(\\nabla \\mathrm{WL}_{\\mathrm{LSE}}^{\\mathrm{tot}}(\\mathbf{z})\\Big)_{y_i} = \\sum_{N: i \\in N} \\left[\\frac{\\exp\\!\\big(y_i/\\gamma\\big)}{\\sum_{j \\in N} \\exp\\!\\big(y_j/\\gamma\\big)} - \\frac{\\exp\\!\\big(-y_i/\\gamma\\big)}{\\sum_{j \\in N} \\exp\\!\\big(-y_j/\\gamma\\big)}\\right].\n$$\n\nWe evaluate these at $\\mathbf{z} = \\mathbf{y}^{(k)}$ with $\\gamma = 0.5$.\n\nFor net $N_1 = \\{1,2\\}$ in $x$ with $(x_1^{y}, x_2^{y}) = (1.26, 0.06)$, compute\n\n$$\n\\sum_{j \\in N_1} \\exp\\!\\Big(\\frac{x_j^{y}}{\\gamma}\\Big) = \\exp(2.52) + \\exp(0.12), \\quad \\sum_{j \\in N_1} \\exp\\!\\Big(-\\frac{x_j^{y}}{\\gamma}\\Big) = \\exp(-2.52) + \\exp(-0.12).\n$$\n\nNumerically, $\\exp(2.52) \\approx 12.421$, $\\exp(0.12) \\approx 1.1275$, $\\exp(-2.52) \\approx 0.08046$, and $\\exp(-0.12) \\approx 0.8870$, hence\n\n$$\nS_{x}^{+}(N_1) \\approx 13.5485, \\quad S_{x}^{-}(N_1) \\approx 0.96746.\n$$\n\nThus,\n\n$$\n\\frac{\\partial \\mathrm{WL}_{x}(N_1)}{\\partial x_1} \\approx \\frac{12.421}{13.5485} - \\frac{0.08046}{0.96746} \\approx 0.9170 - 0.08325 \\approx 0.83375,\n$$\n\n\n$$\n\\frac{\\partial \\mathrm{WL}_{x}(N_1)}{\\partial x_2} \\approx \\frac{1.1275}{13.5485} - \\frac{0.8870}{0.96746} \\approx 0.08325 - 0.91675 \\approx -0.83350.\n$$\n\nFor $N_1$ in $y$ with $(y_1^{y}, y_2^{y}) = (0.74, 1.56)$,\n\n$$\n\\sum_{j \\in N_1} \\exp\\!\\Big(\\frac{y_j^{y}}{\\gamma}\\Big) = \\exp(1.48) + \\exp(3.12) \\approx 4.396 + 22.644 = 27.040,\n$$\n\n\n$$\n\\sum_{j \\in N_1} \\exp\\!\\Big(-\\frac{y_j^{y}}{\\gamma}\\Big) = \\exp(-1.48) + \\exp(-3.12) \\approx 0.2276 + 0.04417 = 0.27177.\n$$\n\nHence,\n\n$$\n\\frac{\\partial \\mathrm{WL}_{y}(N_1)}{\\partial y_1} \\approx \\frac{4.396}{27.040} - \\frac{0.2276}{0.27177} \\approx 0.1626 - 0.8374 \\approx -0.6748,\n$$\n\n\n$$\n\\frac{\\partial \\mathrm{WL}_{y}(N_1)}{\\partial y_2} \\approx \\frac{22.644}{27.040} - \\frac{0.04417}{0.27177} \\approx 0.8374 - 0.1626 \\approx 0.6748.\n$$\n\nFor net $N_2 = \\{2,3\\}$ in $x$ with $(x_2^{y}, x_3^{y}) = (0.06, 2.06)$,\n\n$$\n\\sum_{j \\in N_2} \\exp\\!\\Big(\\frac{x_j^{y}}{\\gamma}\\Big) = \\exp(0.12) + \\exp(4.12) \\approx 1.1275 + 61.565 = 62.6925,\n$$\n\n\n$$\n\\sum_{j \\in N_2} \\exp\\!\\Big(-\\frac{x_j^{y}}{\\gamma}\\Big) = \\exp(-0.12) + \\exp(-4.12) \\approx 0.8870 + 0.01624 = 0.90324.\n$$\n\nThus,\n\n$$\n\\frac{\\partial \\mathrm{WL}_{x}(N_2)}{\\partial x_2} \\approx \\frac{1.1275}{62.6925} - \\frac{0.8870}{0.90324} \\approx 0.01798 - 0.98199 \\approx -0.96401,\n$$\n\n\n$$\n\\frac{\\partial \\mathrm{WL}_{x}(N_2)}{\\partial x_3} \\approx \\frac{61.565}{62.6925} - \\frac{0.01624}{0.90324} \\approx 0.98202 - 0.01798 \\approx 0.96404.\n$$\n\nFor $N_2$ in $y$ with $(y_2^{y}, y_3^{y}) = (1.56, 0.26)$,\n\n$$\n\\sum_{j \\in N_2} \\exp\\!\\Big(\\frac{y_j^{y}}{\\gamma}\\Big) = \\exp(3.12) + \\exp(0.52) \\approx 22.644 + 1.682 = 24.326,\n$$\n\n\n$$\n\\sum_{j \\in N_2} \\exp\\!\\Big(-\\frac{y_j^{y}}{\\gamma}\\Big) = \\exp(-3.12) + \\exp(-0.52) \\approx 0.04417 + 0.595 \\approx 0.63917.\n$$\n\nHence,\n\n$$\n\\frac{\\partial \\mathrm{WL}_{y}(N_2)}{\\partial y_2} \\approx \\frac{22.644}{24.326} - \\frac{0.04417}{0.63917} \\approx 0.9310 - 0.06910 \\approx 0.8619,\n$$\n\n\n$$\n\\frac{\\partial \\mathrm{WL}_{y}(N_2)}{\\partial y_3} \\approx \\frac{1.682}{24.326} - \\frac{0.595}{0.63917} \\approx 0.06910 - 0.9310 \\approx -0.8619.\n$$\n\nSumming the LSE gradients over nets for each cell gives\n\n$$\n\\begin{aligned}\n\\text{Cell }1: &\\quad \\big(\\nabla \\mathrm{WL}_{\\mathrm{LSE}}^{\\mathrm{tot}}\\big)_{x_1} \\approx 0.83375, \\quad \\big(\\nabla \\mathrm{WL}_{\\mathrm{LSE}}^{\\mathrm{tot}}\\big)_{y_1} \\approx -0.6748, \\\\\n\\text{Cell }2: &\\quad \\big(\\nabla \\mathrm{WL}_{\\mathrm{LSE}}^{\\mathrm{tot}}\\big)_{x_2} \\approx -0.83350 + (-0.96401) = -1.79751, \\quad \\big(\\nabla \\mathrm{WL}_{\\mathrm{LSE}}^{\\mathrm{tot}}\\big)_{y_2} \\approx 0.6748 + 0.8619 = 1.5367, \\\\\n\\text{Cell }3: &\\quad \\big(\\nabla \\mathrm{WL}_{\\mathrm{LSE}}^{\\mathrm{tot}}\\big)_{x_3} \\approx 0.96404, \\quad \\big(\\nabla \\mathrm{WL}_{\\mathrm{LSE}}^{\\mathrm{tot}}\\big)_{y_3} \\approx -0.8619.\n\\end{aligned}\n$$\n\nNow derive the density penalty gradient. With\n\n$$\nE_{\\mathrm{dens}} = \\frac{1}{2}\\Big(S - C\\Big)^{2}, \\quad S \\triangleq \\sum_{i=1}^{3} a_i K_i, \\quad K_i = \\exp\\!\\Bigg(-\\frac{(x_i - x_b)^{2} + (y_i - y_b)^{2}}{\\sigma^{2}}\\Bigg),\n$$\n\nthe chain rule yields\n\n$$\n\\frac{\\partial E_{\\mathrm{dens}}}{\\partial x_i} = (S - C)\\, a_i\\, \\frac{\\partial K_i}{\\partial x_i}, \\quad \\frac{\\partial E_{\\mathrm{dens}}}{\\partial y_i} = (S - C)\\, a_i\\, \\frac{\\partial K_i}{\\partial y_i}.\n$$\n\nSince $\\sigma^{2} = 1$ and $a_i = 1$, and\n\n$$\n\\frac{\\partial K_i}{\\partial x_i} = K_i \\cdot \\left(-2(x_i - x_b)\\right), \\quad \\frac{\\partial K_i}{\\partial y_i} = K_i \\cdot \\left(-2(y_i - y_b)\\right),\n$$\n\nwe have\n\n$$\n\\frac{\\partial E_{\\mathrm{dens}}}{\\partial x_i} = (S - C)\\, K_i \\cdot \\left(-2(x_i - x_b)\\right), \\quad \\frac{\\partial E_{\\mathrm{dens}}}{\\partial y_i} = (S - C)\\, K_i \\cdot \\left(-2(y_i - y_b)\\right).\n$$\n\nEvaluate at $\\mathbf{y}^{(k)}$ with $(x_b, y_b) = (1.0, 1.0)$. First, compute $K_i$:\n\n$$\n\\begin{aligned}\n\\text{Cell }1: &\\quad x_1^{y} - x_b = 0.26, \\; y_1^{y} - y_b = -0.26, \\; r_1^{2} = 0.26^{2} + (-0.26)^{2} = 0.1352, \\; K_1 = \\exp(-0.1352) \\approx 0.874, \\\\\n\\text{Cell }2: &\\quad x_2^{y} - x_b = -0.94, \\; y_2^{y} - y_b = 0.56, \\; r_2^{2} = 0.8836 + 0.3136 = 1.1972, \\; K_2 = \\exp(-1.1972) \\approx 0.302, \\\\\n\\text{Cell }3: &\\quad x_3^{y} - x_b = 1.06, \\; y_3^{y} - y_b = -0.74, \\; r_3^{2} = 1.1236 + 0.5476 = 1.6712, \\; K_3 = \\exp(-1.6712) \\approx 0.188.\n\\end{aligned}\n$$\n\nThen $S = K_1 + K_2 + K_3 \\approx 0.874 + 0.302 + 0.188 = 1.364$, hence $S - C \\approx 1.364 - 1.8 = -0.436$.\n\nCompute the partial derivatives:\n\n$$\n\\begin{aligned}\n\\frac{\\partial E_{\\mathrm{dens}}}{\\partial x_1} &\\approx (-0.436)\\cdot 0.874 \\cdot \\big(-2\\cdot 0.26\\big) \\approx (+0.198), &\n\\frac{\\partial E_{\\mathrm{dens}}}{\\partial y_1} &\\approx (-0.436)\\cdot 0.874 \\cdot \\big(-2\\cdot (-0.26)\\big) \\approx (-0.198), \\\\\n\\frac{\\partial E_{\\mathrm{dens}}}{\\partial x_2} &\\approx (-0.436)\\cdot 0.302 \\cdot \\big(-2\\cdot (-0.94)\\big) \\approx (-0.248), &\n\\frac{\\partial E_{\\mathrm{dens}}}{\\partial y_2} &\\approx (-0.436)\\cdot 0.302 \\cdot \\big(-2\\cdot 0.56\\big) \\approx (+0.148), \\\\\n\\frac{\\partial E_{\\mathrm{dens}}}{\\partial x_3} &\\approx (-0.436)\\cdot 0.188 \\cdot \\big(-2\\cdot 1.06\\big) \\approx (+0.174), &\n\\frac{\\partial E_{\\mathrm{dens}}}{\\partial y_3} &\\approx (-0.436)\\cdot 0.188 \\cdot \\big(-2\\cdot (-0.74)\\big) \\approx (-0.121).\n\\end{aligned}\n$$\n\nScale the density gradient by $\\eta = 0.3$ to obtain its contribution to $\\nabla f$:\n\n$$\n\\begin{aligned}\n\\big(\\eta \\nabla E_{\\mathrm{dens}}\\big)_{1} &\\approx ( +0.0594,\\; -0.0594), \\\\\n\\big(\\eta \\nabla E_{\\mathrm{dens}}\\big)_{2} &\\approx ( -0.0744,\\; +0.0444), \\\\\n\\big(\\eta \\nabla E_{\\mathrm{dens}}\\big)_{3} &\\approx ( +0.0522,\\; -0.0364).\n\\end{aligned}\n$$\n\nCombine with the LSE gradients to obtain the total gradient at $\\mathbf{y}^{(k)}$:\n\n$$\n\\begin{aligned}\n\\text{Cell }1: &\\quad \\nabla f_{x_1} \\approx 0.83375 + 0.0594 = 0.89315, \\quad \\nabla f_{y_1} \\approx -0.6748 - 0.0594 = -0.7342, \\\\\n\\text{Cell }2: &\\quad \\nabla f_{x_2} \\approx -1.79751 - 0.0744 = -1.87191, \\quad \\nabla f_{y_2} \\approx 1.5367 + 0.0444 = 1.5811, \\\\\n\\text{Cell }3: &\\quad \\nabla f_{x_3} \\approx 0.96404 + 0.0522 = 1.01624, \\quad \\nabla f_{y_3} \\approx -0.8619 - 0.0364 = -0.8983.\n\\end{aligned}\n$$\n\nFinally, apply the gradient step with step size $\\alpha = 0.1$:\n\n$$\nx_i^{(k+1)} = x_i^{y} - \\alpha\\, \\nabla f_{x_i}, \\quad y_i^{(k+1)} = y_i^{y} - \\alpha\\, \\nabla f_{y_i}.\n$$\n\nCompute the updates:\n\n$$\n\\begin{aligned}\nx_1^{(k+1)} &\\approx 1.26 - 0.1 \\cdot 0.89315 = 1.170685, & y_1^{(k+1)} &\\approx 0.74 - 0.1 \\cdot (-0.7342) = 0.81342, \\\\\nx_2^{(k+1)} &\\approx 0.06 - 0.1 \\cdot (-1.87191) = 0.247191, & y_2^{(k+1)} &\\approx 1.56 - 0.1 \\cdot 1.5811 = 1.40189, \\\\\nx_3^{(k+1)} &\\approx 2.06 - 0.1 \\cdot 1.01624 = 1.958376, & y_3^{(k+1)} &\\approx 0.26 - 0.1 \\cdot (-0.8983) = 0.34983.\n\\end{aligned}\n$$\n\nRounding each coordinate to four significant figures yields\n\n$$\nx_1^{(k+1)} \\approx 1.171, \\quad y_1^{(k+1)} \\approx 0.8134, \\quad x_2^{(k+1)} \\approx 0.2472, \\quad y_2^{(k+1)} \\approx 1.402, \\quad x_3^{(k+1)} \\approx 1.958, \\quad y_3^{(k+1)} \\approx 0.3498.\n$$\n\nWe report the ordered row vector $(x_1^{(k+1)}, y_1^{(k+1)}, x_2^{(k+1)}, y_2^{(k+1)}, x_3^{(k+1)}, y_3^{(k+1)})$ accordingly.",
            "answer": "$$\\boxed{\\begin{pmatrix}1.171 & 0.8134 & 0.2472 & 1.402 & 1.958 & 0.3498\\end{pmatrix}}$$"
        }
    ]
}