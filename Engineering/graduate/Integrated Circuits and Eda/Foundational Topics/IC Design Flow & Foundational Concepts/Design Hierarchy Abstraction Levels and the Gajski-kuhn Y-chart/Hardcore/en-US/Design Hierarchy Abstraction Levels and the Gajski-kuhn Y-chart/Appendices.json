{
    "hands_on_practices": [
        {
            "introduction": "The journey from an abstract algorithm to a concrete integrated circuit begins with scheduling. This process maps a behavioral description, often represented as a Directed Acyclic Graph (DAG), onto a timeline, respecting both data dependencies and hardware resource constraints. This exercise  introduces list scheduling, a fundamental heuristic used in High-Level Synthesis (HLS) to tackle this complex optimization problem. By working through this example, you will gain hands-on experience in converting a behavioral specification into a resource-aware execution schedule, a critical first step in bridging the behavioral and structural domains of the Y-chart.",
            "id": "4264767",
            "problem": "In the spirit of Electronic Design Automation (EDA) and within the Gajski-Kuhn Y-chart view, scheduling at the behavioral domain must respect precedence at the algorithmic level while mapping to a single structural resource and a geometric time axis. Consider a computation described at the behavior domain by a Directed Acyclic Graph (DAG) of operations $\\{O_1,\\dots,O_9\\}$, each with an integer operation latency $L_i$ measured in cycles. The structural domain provides a single fully pipelined functional unit with initiation interval $1$ cycle, which enforces a single-issue constraint: at most one operation can be initiated per integer cycle. The geometric domain is a discrete-time grid of integer cycles starting at cycle $0$.\n\nThe operation latencies are: $L_1=2$, $L_2=3$, $L_3=1$, $L_4=2$, $L_5=3$, $L_6=2$, $L_7=1$, $L_8=4$, $L_9=2$. Precedence constraints are given by edges $O_1 \\to O_3$, $O_1 \\to O_4$, $O_2 \\to O_4$, $O_3 \\to O_5$, $O_3 \\to O_6$, $O_4 \\to O_7$, $O_5 \\to O_7$, $O_6 \\to O_8$, $O_7 \\to O_9$, $O_8 \\to O_9$.\n\nYou must derive, from first principles, the earliest unconstrained start times at the behavior domain and then a list-scheduled start-time assignment at the structure/geometry domains under the single-issue resource constraint, resolving resource conflicts using a critical-path priority consistent with latency-weighted behavior-to-structure mapping.\n\nStart from core definitions: an operation $O_i$ may start at cycle $t$ only if all of its immediate predecessors have completed by $t$, where completion of $O_i$ started at $s_i$ occurs at $s_i+L_i$. First, compute the earliest unconstrained start times $e_i$ (ignoring the single-issue constraint) based solely on precedence and latencies. Then, compute a list schedule under the single-issue constraint as follows: at each integer cycle $t$, among all ready and not-yet-started operations, select a single operation to initiate; break ties and resolve conflicts by selecting the operation with the largest latency-weighted critical-path priority $W_i$, defined recursively by\n$$\nW_i=\n\\begin{cases}\nL_i,  \\text{if $O_i$ has no successors (sink)}\\\\\nL_i+\\max\\limits_{O_j \\in \\mathrm{succ}(i)} W_j,  \\text{otherwise,}\n\\end{cases}\n$$\nwhere $\\mathrm{succ}(i)$ denotes the set of immediate successors of $O_i$.\n\nProvide your final schedule as the start-time vector $s=[s_1,\\dots,s_9]$ in cycles (ordered as $(O_1,\\dots,O_9)$) together with the overall latency $T=\\max_i(s_i+L_i)$, expressed in cycles. Express all cycles as integers; no rounding is required. Present the final answer as a single row matrix containing $s_1,\\dots,s_9,T$ in that order.",
            "solution": "The solution proceeds in three stages: first, calculation of the earliest unconstrained start times (ASAP times); second, calculation of the critical-path priority weights for scheduling; and third, execution of the list scheduling algorithm to find the final start times and total latency.\n\n**1. Earliest Unconstrained Start Times ($e_i$)**\n\nThe earliest start time $e_i$ for an operation $O_i$ is determined by the completion times of all its predecessors. For an operation with no predecessors, $e_i=0$. Otherwise, $e_i = \\max_{O_j \\in \\mathrm{pred}(i)} \\{e_j + L_j\\}$. Given the latencies $L=[2, 3, 1, 2, 3, 2, 1, 4, 2]$ and the specified precedences:\n*   $e_1 = 0$\n*   $e_2 = 0$\n*   $e_3 = e_1 + L_1 = 0 + 2 = 2$\n*   $e_4 = \\max(e_1 + L_1, e_2 + L_2) = \\max(0 + 2, 0 + 3) = 3$\n*   $e_5 = e_3 + L_3 = 2 + 1 = 3$\n*   $e_6 = e_3 + L_3 = 2 + 1 = 3$\n*   $e_7 = \\max(e_4 + L_4, e_5 + L_5) = \\max(3 + 2, 3 + 3) = 6$\n*   $e_8 = e_6 + L_6 = 3 + 2 = 5$\n*   $e_9 = \\max(e_7 + L_7, e_8 + L_8) = \\max(6 + 1, 5 + 4) = 9$\n\n**2. Latency-Weighted Critical-Path Priority ($W_i$)**\n\nThe priority weight $W_i$ is the sum of an operation's latency and the maximum weight of its successors. This is calculated by traversing the DAG in reverse topological order.\n*   $W_9 = L_9 = 2$\n*   $W_8 = L_8 + W_9 = 4 + 2 = 6$\n*   $W_7 = L_7 + W_9 = 1 + 2 = 3$\n*   $W_6 = L_6 + W_8 = 2 + 6 = 8$\n*   $W_5 = L_5 + W_7 = 3 + 3 = 6$\n*   $W_4 = L_4 + W_7 = 2 + 3 = 5$\n*   $W_3 = L_3 + \\max(W_5, W_6) = 1 + \\max(6, 8) = 9$\n*   $W_2 = L_2 + W_4 = 3 + 5 = 8$\n*   $W_1 = L_1 + \\max(W_3, W_4) = 2 + \\max(9, 5) = 11$\nThe priority list, from highest to lowest $W_i$ and using lower index as a tie-breaker, is: $O_1, O_3, O_2, O_6, O_5, O_8, O_4, O_7, O_9$.\n\n**3. List Scheduling**\n\nWe iterate cycle by cycle, maintaining a list of ready operations. At each cycle, we schedule the ready operation with the highest priority.\n*   **Cycle $t=0$**: Ready operations are $\\{O_1, O_2\\}$. $O_1$ has higher priority ($W_1=11 > W_2=8$). Schedule $O_1$. $s_1=0$.\n*   **Cycle $t=1$**: Ready operation is $\\{O_2\\}$. Schedule $O_2$. $s_2=1$.\n*   **Cycle $t=2$**: $O_1$ completes (at $0+2=2$), making $O_3$ ready. Ready operation is $\\{O_3\\}$. Schedule $O_3$. $s_3=2$.\n*   **Cycle $t=3$**: $O_3$ completes (at $2+1=3$), making $\\{O_5, O_6\\}$ ready. $O_6$ has higher priority ($W_6=8 > W_5=6$). Schedule $O_6$. $s_6=3$.\n*   **Cycle $t=4$**: $O_2$ completes (at $1+3=4$), making $O_4$ ready. The ready list is $\\{O_4, O_5\\}$. $O_5$ has higher priority ($W_5=6 > W_4=5$). Schedule $O_5$. $s_5=4$.\n*   **Cycle $t=5$**: $O_6$ completes (at $3+2=5$), making $O_8$ ready. The ready list is $\\{O_4, O_8\\}$. $O_8$ has higher priority ($W_8=6 > W_4=5$, tie-broken by lower index if weights were equal, but here they are not). Schedule $O_8$. $s_8=5$.\n*   **Cycle $t=6$**: The only ready operation is $\\{O_4\\}$. Schedule $O_4$. $s_4=6$.\n*   **Cycle $t=7$**: No new operations become ready. Predecessor $O_4$ for $O_7$ does not complete until cycle $6+2=8$. This is an idle cycle.\n*   **Cycle $t=8$**: $O_4$ completes. $O_7$ is now ready. Ready operation is $\\{O_7\\}$. Schedule $O_7$. $s_7=8$.\n*   **Cycle $t=9$**: $O_7$ completes (at $8+1=9$) and $O_8$ completes (at $5+4=9$). $O_9$ becomes ready. Ready operation is $\\{O_9\\}$. Schedule $O_9$. $s_9=9$.\n\nThe final scheduled start times are $s=[0, 1, 2, 6, 4, 3, 8, 5, 9]$.\n\nThe overall latency $T$ is the completion time of the final operation, $O_9$.\n$T = s_9 + L_9 = 9 + 2 = 11$.\nThe final result is the start-time vector followed by the total latency.",
            "answer": "$$ \\boxed{ \\begin{pmatrix} 0  1  2  6  4  3  8  5  9  11 \\end{pmatrix} } $$"
        },
        {
            "introduction": "With a datapath architecture and a schedule in place, the next critical step is to design the controller that orchestrates the entire process. This exercise  challenges you to create a minimal Finite State Machine (FSM) for a common digital signal processing block, an FIR filter. You will see how the FSM's states directly correspond to the distinct phases of the filter's operation—idling, computing, and outputting—while its logic manages resource allocation and external I/O handshaking protocols. This practice provides essential insight into how control logic bridges the behavioral specification and the structural hardware implementation.",
            "id": "4264779",
            "problem": "A designer is synthesizing a controller for a $16$-tap Finite Impulse Response (FIR) filter with saturation in an Integrated Circuit (IC) using Electronic Design Automation (EDA) tools. The filter must be modeled and implemented consistently across the Gajski-Kuhn Y-chart domains: behavioral, structural, and geometric. At the behavioral level, the filter computes the saturated output $$y[n] = S\\!\\left(\\sum_{k=0}^{15} h[k]\\,x[n-k]\\right),$$ where $x[n]$ is the input sample stream, $h[k]$ are fixed coefficients, and $S(\\cdot)$ is a saturation function that clamps to a representable range determined by a fixed word width. The structural choice is a single multiply-accumulate datapath that processes one tap per clock cycle, a circular buffer for the $16$ most recent input samples, and a combinational saturator. The controller must be a Mealy Finite State Machine (FSM), where outputs are functions of the current state and inputs.\n\nThe system uses synchronous two-phase ready/valid handshakes on both input and output: for input, the upstream asserts $x_{\\text{valid}}$, the filter asserts $x_{\\text{ready}}$, and a sample is captured on a rising clock edge when both $x_{\\text{valid}}=1$ and $x_{\\text{ready}}=1$. For output, the filter asserts $y_{\\text{valid}}$ when a result is available, the downstream asserts $y_{\\text{ready}}$, and the result is transferred on a rising clock edge when both $y_{\\text{valid}}=1$ and $y_{\\text{ready}}=1$. The filter must not drop inputs; if a new input sample arrives while the filter is busy, it must deassert $x_{\\text{ready}}$ to backpressure the upstream until it can accept the next sample. The saturation is combinational and can be produced in the same clock cycle as the final accumulation.\n\nStarting from formal definitions of a Mealy Finite State Machine and the stated handshake semantics, and respecting the separation of concerns implied by the Gajski-Kuhn Y-chart (behavioral algorithm mapped to a structural datapath with a controller in the same clocked geometric domain), derive the minimum number of controller states required to correctly orchestrate:\n- input acceptance under the two-phase handshake,\n- sequential processing of all $16$ taps with a single multiply-accumulate resource,\n- saturated output production and delivery under the two-phase handshake,\n- and proper stalling of input while busy.\n\nAssume one tap is processed per clock cycle and that the circular buffer update for the newly accepted input sample completes within the same clock cycle as acceptance. Express your final answer as a single integer giving the minimum number of controller states required. No rounding is required.",
            "solution": "To determine the minimum number of states for the Finite State Machine (FSM) controller, we must identify the distinct operational phases of the FIR filter, each defined by a unique set of control actions and external signaling requirements. The primary outputs of the controller are the handshake signals $x_{\\text{ready}}$ and $y_{\\text{valid}}$. A minimal FSM requires a distinct state for each phase that necessitates a unique output vector, according to the principles of FSM state minimization.\n\nThe filter's operation can be divided into three sequential phases:\n\n1.  **IDLE Phase**: The filter is waiting to accept a new input sample. In this phase, it must signal its readiness to the upstream source by asserting $x_{\\text{ready}}$. Simultaneously, no valid output is available, so $y_{\\text{valid}}$ must be deasserted.\n    *   Required output vector `(x_ready, y_valid)`: `(1, 0)`.\n    *   The FSM remains in this state until a new sample arrives (indicated by $x_{\\text{valid}}=1$). When a transaction occurs (both $x_{\\text{valid}}=1$ and $x_{\\text{ready}}=1$), the FSM transitions to the next phase.\n\n2.  **COMPUTE Phase**: After accepting a new sample, the filter computes the 16-tap sum. The datapath uses a single multiply-accumulate (MAC) unit, processing one tap per clock cycle. This phase lasts for 16 clock cycles. During this entire period, the filter is busy and cannot accept new input samples. The final result is also not yet available. Therefore, the FSM must deassert both $x_{\\text{ready}}$ (to backpressure the input) and $y_{\\text{valid}}$.\n    *   Required output vector `(x_ready, y_valid)`: `(0, 0)`.\n    *   A minimal FSM uses a single state for this entire phase, typically in conjunction with a counter (external to the FSM state logic) to track the 16 cycles. After the 16th cycle, the computation is complete, and the FSM transitions to the output phase.\n\n3.  **OUTPUT Phase**: Once the 16-cycle computation is finished, the saturated result is ready for output. The FSM must signal this by asserting $y_{\\text{valid}}$. The filter cannot start a new computation until the current result is transferred, so it is still not ready for a new input, meaning $x_{\\text{ready}}$ remains deasserted.\n    *   Required output vector `(x_ready, y_valid)`: `(0, 1)`.\n    *   The FSM stays in this state until the downstream sink accepts the data (indicated by $y_{\\text{ready}}=1$). Upon a successful output transaction, the controller's task for this sample is complete, and it transitions back to the IDLE phase.\n\nWe have identified three operational phases, each requiring a distinct and constant output vector for the handshake signals:\n*   Phase 1 (IDLE): `(1, 0)`\n*   Phase 2 (COMPUTE): `(0, 0)`\n*   Phase 3 (OUTPUT): `(0, 1)`\n\nSince the three required output vectors are unique, any two phases are distinguishable by their immediate outputs. Therefore, a minimal FSM must have at least three states, one corresponding to each of these phases. An FSM with three states—let's call them $S_{\\text{IDLE}}$, $S_{\\text{COMPUTE}}$, and $S_{\\text{OUTPUT}}$—is sufficient to implement the required logic.\n\nThus, the minimum number of controller states required is 3.",
            "answer": "$$\\boxed{3}$$"
        },
        {
            "introduction": "A design specified with logic gates must ultimately be translated into a transistor-level netlist for physical implementation. This refinement process, a key step down the structural axis of the Y-chart, often involves significant trade-offs between custom design and hierarchical composition. This practice  provides a quantitative exploration of these trade-offs by comparing a direct, custom implementation of a multi-input logic gate against a hierarchical one built from a library of simpler cells. By calculating the resulting transistor counts, you will gain a concrete appreciation for how structural choices at the logic level directly influence the complexity and cost of the final circuit implementation.",
            "id": "4264793",
            "problem": "An advanced design team is refining a gate-level netlist within the framework of the Gajski-Kuhn Y-chart, moving between abstraction levels and domains. Consider a node in the netlist that is a $k$-input logical NAND, where $k \\geq 2$. Two refinement paths are considered from the structural, logic-level description down to the structural, circuit-level description in Complementary Metal-Oxide-Semiconductor (CMOS) technology.\n\nPath A: Direct refinement to a single static Complementary Metal-Oxide-Semiconductor (CMOS) gate. The transistor-level schematic is constructed using the complementary pull-down network and pull-up network method: the pull-down network is realized with series-parallel networks of n-channel metal-oxide-semiconductor field-effect transistors (NMOS) corresponding to the conditions under which the output must be logic low, and the pull-up network is the dual network of p-channel metal-oxide-semiconductor field-effect transistors (PMOS) corresponding to the logical complement of those conditions. Assume minimum, series-parallel static CMOS without transmission gates, without factoring, and without any duplication or redundancy. Ignore well taps, body ties, routing devices, and any upsizing for performance; count only functional transistors.\n\nPath B: Hierarchical refinement constrained by a standard-cell composition rule at the logic level before circuit mapping: only $2$-input NAND cells are available at the gate level. The $k$ inputs must be combined by a binary tree whose internal nodes compute logical AND, except that the root produces logical NAND so that the overall function is the $k$-input NAND. Logical AND at each non-root internal node must be constructed using only $2$-input NAND gates and inverters. An inverter must be realized using a $2$-input NAND with its two inputs tied together. After this logic-level decomposition, each $2$-input NAND is refined to its static CMOS implementation as in Path A. As above, ignore well taps, body ties, routing devices, and upsizing, and count only functional transistors.\n\nStart from fundamental Boolean algebra and the standard static CMOS complementary pull-up/pull-down construction rule, and derive closed-form expressions, as functions of $k$, for the total number of transistors obtained by Path A and by Path B. Finally, define $D(k)$ to be the difference between the Path B transistor count and the Path A transistor count, and provide $D(k)$ as a single simplified symbolic expression in terms of $k$. No numerical substitution is required. Your final answer must be the single expression for $D(k)$.",
            "solution": "The solution requires deriving the transistor counts for two different implementation paths of a $k$-input NAND gate and then finding their difference.\n\n**1. Transistor Count for Path A: $T_A(k)$**\n\nIn Path A, a $k$-input NAND gate is implemented as a single, static CMOS logic gate. The Boolean function is $Y = \\overline{I_1 \\cdot I_2 \\cdot \\ldots \\cdot I_k}$.\n- The Pull-Down Network (PDN) implements the logic for a '0' output, which occurs when all inputs are '1' ($I_1 \\cdot I_2 \\cdot \\ldots \\cdot I_k$). In CMOS, this AND logic is realized with $k$ NMOS transistors connected in series.\n- The Pull-Up Network (PUN) is the dual of the PDN. A series connection of NMOS transistors corresponds to a parallel connection of PMOS transistors. The PUN therefore consists of $k$ PMOS transistors in parallel.\n\nThe total number of functional transistors is the sum of transistors in the PDN and the PUN.\n$$T_A(k) = (\\text{NMOS count}) + (\\text{PMOS count}) = k + k = 2k$$\n\n**2. Transistor Count for Path B: $T_B(k)$**\n\nIn Path B, the $k$-input NAND function is decomposed into a binary tree of 2-input gates.\n- A binary tree with $k$ inputs (leaves) has $k-1$ internal nodes (gates).\n- The structure consists of one root 2-input NAND gate and $k-2$ internal 2-input AND gates.\n\nFirst, we determine the cost of each component in terms of primitive 2-input NAND cells:\n- A 2-input NAND gate is one primitive cell.\n- An inverter is implemented as a 2-input NAND gate with tied inputs, costing one primitive cell.\n- A 2-input AND gate is constructed as a NAND followed by an inverter ($A \\cdot B = \\overline{\\overline{A \\cdot B}}$). This requires one cell for the NAND and one cell for the inverter, for a total of 2 primitive 2-input NAND cells.\n\nNow, we calculate the total number of primitive cells for the entire $k$-input structure:\n- Number of 2-input AND gates: $k-2$\n- Number of 2-input NAND gates (the root): $1$\n- Total primitive cells = $(k-2) \\times (\\text{cells per AND}) + 1 \\times (\\text{cells per NAND})$\n- Total primitive cells = $(k-2) \\times 2 + 1 \\times 1 = 2k - 4 + 1 = 2k - 3$\n\nNext, we find the transistor count for one primitive 2-input NAND cell. Following the same logic as in Path A for $k=2$, a 2-input NAND gate has 2 NMOS in series and 2 PMOS in parallel, totaling $2+2=4$ transistors.\n\nFinally, the total transistor count for Path B is the total number of cells multiplied by the transistors per cell.\n$$T_B(k) = (2k-3) \\times 4 = 8k - 12$$\n\n**3. Calculation of the Difference $D(k)$**\n\nThe difference $D(k)$ is the transistor count of Path B minus the transistor count of Path A.\n$$D(k) = T_B(k) - T_A(k)$$\nSubstituting the derived expressions:\n$$D(k) = (8k - 12) - (2k)$$\n$$D(k) = 6k - 12$$\nThis expression is valid for $k \\geq 2$. For the base case $k=2$, $D(2)=0$, correctly reflecting that both paths result in a single 2-input NAND gate.",
            "answer": "$$\\boxed{6k-12}$$"
        }
    ]
}