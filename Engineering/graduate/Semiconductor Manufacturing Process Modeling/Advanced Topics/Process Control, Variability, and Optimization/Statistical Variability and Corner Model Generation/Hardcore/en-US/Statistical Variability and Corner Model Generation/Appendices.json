{
    "hands_on_practices": [
        {
            "introduction": "Accurate statistical models are built upon high-quality data. This exercise  addresses the foundational challenge of separating true process variability from the noise introduced by metrology tools. By deriving an unbiased estimator for process variance, you will practice a critical skill in statistical process control, ensuring that your corner models are based on actual manufacturing fluctuations, not measurement artifacts.",
            "id": "4167543",
            "problem": "A semiconductor fabrication line uses in-line metrology to monitor a critical dimension. For each wafer indexed by $i \\in \\{1, \\dots, N\\}$, the metrology tool performs $r$ repeated measurements indexed by $j \\in \\{1, \\dots, r\\}$. Let the measured value be modeled as\n$$\ny_{ij} = \\mu + w_i + m_{ij},\n$$\nwhere $w_i$ is the wafer-level process deviation, and $m_{ij}$ is the metrology measurement error. Assume $\\{w_i\\}$ are independent and identically distributed with $\\mathbb{E}[w_i] = 0$ and $\\operatorname{Var}(w_i) = \\sigma_{W}^{2}$, and $\\{m_{ij}\\}$ are independent and identically distributed with $\\mathbb{E}[m_{ij}] = 0$ and $\\operatorname{Var}(m_{ij}) = \\sigma_{M}^{2}$, independent of $\\{w_i\\}$. The metrology measurement error variance $\\sigma_{M}^{2}$ is known from a Gauge Repeatability and Reproducibility (R&R) study.\n\nAn engineer estimates the wafer-level variance component using the sample variance of the wafer means, defined by\n$$\n\\bar{y}_{i} = \\frac{1}{r}\\sum_{j=1}^{r} y_{ij}, \\quad \\bar{y}_{\\cdot\\cdot} = \\frac{1}{N}\\sum_{i=1}^{N} \\bar{y}_{i}, \\quad s^{2} = \\frac{1}{N-1}\\sum_{i=1}^{N} \\left(\\bar{y}_{i} - \\bar{y}_{\\cdot\\cdot}\\right)^{2}.\n$$\nIf the engineer ignores metrology measurement error and treats $s^{2}$ as an estimator of $\\sigma_{W}^{2}$, then a bias is introduced into the variance component estimate used in statistical corner model generation.\n\nStarting only from the independence assumptions and the basic properties of expectation and variance for sums and averages of random variables, compute the bias of the naive estimator $s^{2}$ for $\\sigma_{W}^{2}$ when metrology measurement error is ignored. Then, derive a corrected unbiased estimator for the wafer-level variance component $\\sigma_{W}^{2}$ that uses the known metrology variance $\\sigma_{M}^{2}$ from the Gauge R&R model and the replication count $r$.\n\nExpress your final answer as a two-entry row matrix containing:\n- the bias of $s^{2}$ as an estimator of $\\sigma_{W}^{2}$, and\n- the corrected estimator for $\\sigma_{W}^{2}$ in terms of $s^{2}$, $\\sigma_{M}^{2}$, and $r$.\n\nNo numerical rounding is required, and no physical units should be included in the final boxed expression.",
            "solution": "The problem statement is first subjected to validation.\n\n### Step 1: Extract Givens\n- The model for the measured value is $y_{ij} = \\mu + w_i + m_{ij}$, where $i \\in \\{1, \\dots, N\\}$ is the wafer index and $j \\in \\{1, \\dots, r\\}$ is the repetition index.\n- $w_i$ is the wafer-level process deviation. The set $\\{w_i\\}$ consists of independent and identically distributed (i.i.d.) random variables with expectation $\\mathbb{E}[w_i] = 0$ and variance $\\operatorname{Var}(w_i) = \\sigma_{W}^{2}$.\n- $m_{ij}$ is the metrology measurement error. The set $\\{m_{ij}\\}$ consists of i.i.d. random variables with expectation $\\mathbb{E}[m_{ij}] = 0$ and variance $\\operatorname{Var}(m_{ij}) = \\sigma_{M}^{2}$.\n- The sets of random variables $\\{w_i\\}$ and $\\{m_{ij}\\}$ are independent of each other.\n- The metrology measurement error variance $\\sigma_{M}^{2}$ is a known constant.\n- The wafer mean is defined as $\\bar{y}_{i} = \\frac{1}{r}\\sum_{j=1}^{r} y_{ij}$.\n- The grand mean is defined as $\\bar{y}_{\\cdot\\cdot} = \\frac{1}{N}\\sum_{i=1}^{N} \\bar{y}_{i}$.\n- The sample variance of the wafer means is defined as $s^{2} = \\frac{1}{N-1}\\sum_{i=1}^{N} \\left(\\bar{y}_{i} - \\bar{y}_{\\cdot\\cdot}\\right)^{2}$.\n- The task is to compute the bias of $s^2$ as an estimator for $\\sigma_W^2$ and to derive a corrected, unbiased estimator for $\\sigma_W^2$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is evaluated against the validation checklist.\n- **Scientifically Grounded**: The problem describes a one-way random effects model, a standard and fundamental construct in statistics, particularly in the context of Analysis of Variance (ANOVA) and statistical process control (SPC). This model is widely used in manufacturing and engineering to partition sources of variation. The use of a Gauge R&R study to determine measurement system variance ($\\sigma_M^2$) is standard industrial practice. The problem is firmly rooted in established statistical methodology.\n- **Well-Posed**: The problem is clearly stated with all necessary assumptions (i.i.d. variables, independence, known variance component $\\sigma_M^2$) and definitions. The goals—to compute a bias and derive an unbiased estimator—are specific mathematical tasks for which a unique solution can be derived from the given information.\n- **Objective**: The problem is expressed in precise, objective mathematical language. All terms are standard statistical definitions. There is no subjective or ambiguous phrasing.\n- **Completeness and Consistency**: The problem is self-contained and internally consistent. All necessary components of the statistical model are defined, and no contradictions are present.\n- **Realism**: The scenario is highly realistic. Characterizing and separating process variance from measurement variance is a critical and common task in semiconductor manufacturing for process monitoring and control.\n- **Other Flaws**: The problem is not trivial, as it requires a careful application of the properties of expectation and variance. It is not metaphorical, ill-posed, or unverifiable.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. It is a well-defined, scientifically sound problem in applied statistics. I will now proceed with the solution.\n\nThe objective is to find the bias of $s^{2}$ as an estimator of $\\sigma_W^2$ and then to construct a corrected unbiased estimator. The bias of an estimator $\\hat{\\theta}$ for a parameter $\\theta$ is defined as $\\operatorname{Bias}(\\hat{\\theta}) = \\mathbb{E}[\\hat{\\theta}] - \\theta$. In this case, we need to compute $\\operatorname{Bias}(s^2) = \\mathbb{E}[s^2] - \\sigma_W^2$. The core of the task is to find the expectation of $s^2$.\n\nThe statistic $s^2$ is the sample variance of the wafer means $\\{\\bar{y}_1, \\bar{y}_2, \\dots, \\bar{y}_N\\}$. A fundamental property of the sample variance is that if it is calculated from a set of i.i.d. random variables, its expectation is equal to the population variance of those variables. Let us first analyze the properties of the random variables $\\bar{y}_i$.\n\nThe wafer mean $\\bar{y}_i$ is given by:\n$$\n\\bar{y}_{i} = \\frac{1}{r}\\sum_{j=1}^{r} y_{ij} = \\frac{1}{r}\\sum_{j=1}^{r} (\\mu + w_i + m_{ij})\n$$\nBy distributing the summation and the factor $\\frac{1}{r}$:\n$$\n\\bar{y}_{i} = \\frac{1}{r}(r\\mu + rw_i + \\sum_{j=1}^{r} m_{ij}) = \\mu + w_i + \\frac{1}{r}\\sum_{j=1}^{r} m_{ij}\n$$\nLet's denote the mean of the measurement errors for wafer $i$ as $\\bar{m}_i = \\frac{1}{r}\\sum_{j=1}^{r} m_{ij}$. Then the model for the wafer mean is:\n$$\n\\bar{y}_i = \\mu + w_i + \\bar{m}_i\n$$\nThe variables $\\{w_i\\}$ are i.i.d. across $i$. The variables $\\{m_{ij}\\}$ are i.i.d. for all $i$ and $j$. This implies that the mean errors $\\{\\bar{m}_i\\}$ are also i.i.d. across $i$. Since $\\{w_i\\}$ and $\\{m_{ij}\\}$ are independent, it follows that $\\{w_i\\}$ and $\\{\\bar{m}_i\\}$ are independent. Therefore, the wafer means $\\{\\bar{y}_i\\}$ constitute a set of i.i.d. random variables.\n\nBecause the $\\bar{y}_i$ are i.i.d., the expectation of their sample variance $s^2$ is equal to their population variance, $\\operatorname{Var}(\\bar{y}_i)$:\n$$\n\\mathbb{E}[s^2] = \\operatorname{Var}(\\bar{y}_i)\n$$\nWe now compute $\\operatorname{Var}(\\bar{y}_i)$:\n$$\n\\operatorname{Var}(\\bar{y}_i) = \\operatorname{Var}(\\mu + w_i + \\bar{m}_i)\n$$\nSince $\\mu$ is a constant, its variance is $0$. Due to the independence of $w_i$ and $\\bar{m}_i$, the variance of their sum is the sum of their variances:\n$$\n\\operatorname{Var}(\\bar{y}_i) = \\operatorname{Var}(w_i) + \\operatorname{Var}(\\bar{m}_i)\n$$\nWe are given $\\operatorname{Var}(w_i) = \\sigma_{W}^{2}$. We need to compute $\\operatorname{Var}(\\bar{m}_i)$:\n$$\n\\operatorname{Var}(\\bar{m}_i) = \\operatorname{Var}\\left(\\frac{1}{r}\\sum_{j=1}^{r} m_{ij}\\right)\n$$\nUsing the properties of variance, the constant $\\frac{1}{r}$ is factored out as $\\frac{1}{r^2}$:\n$$\n\\operatorname{Var}(\\bar{m}_i) = \\frac{1}{r^2} \\operatorname{Var}\\left(\\sum_{j=1}^{r} m_{ij}\\right)\n$$\nSince the $\\{m_{ij}\\}$ are i.i.d. for a given $i$ across $j=1, \\dots, r$, the variance of their sum is the sum of their variances:\n$$\n\\operatorname{Var}(\\bar{m}_i) = \\frac{1}{r^2} \\sum_{j=1}^{r} \\operatorname{Var}(m_{ij}) = \\frac{1}{r^2} \\sum_{j=1}^{r} \\sigma_{M}^{2} = \\frac{1}{r^2} (r \\sigma_{M}^{2}) = \\frac{\\sigma_{M}^{2}}{r}\n$$\nSubstituting these results back into the expression for $\\operatorname{Var}(\\bar{y}_i)$:\n$$\n\\operatorname{Var}(\\bar{y}_i) = \\sigma_{W}^{2} + \\frac{\\sigma_{M}^{2}}{r}\n$$\nThus, the expectation of the naive estimator $s^2$ is:\n$$\n\\mathbb{E}[s^2] = \\sigma_{W}^{2} + \\frac{\\sigma_{M}^{2}}{r}\n$$\nThe bias of $s^2$ as an estimator for $\\sigma_W^2$ is:\n$$\n\\operatorname{Bias}(s^2) = \\mathbb{E}[s^2] - \\sigma_{W}^{2} = \\left(\\sigma_{W}^{2} + \\frac{\\sigma_{M}^{2}}{r}\\right) - \\sigma_{W}^{2} = \\frac{\\sigma_{M}^{2}}{r}\n$$\nThis is the first part of the required answer. The naive estimator $s^2$ is positively biased, with the bias being the variance of the mean measurement error.\n\nFor the second part, we need to derive a corrected, unbiased estimator for $\\sigma_{W}^{2}$. Let this corrected estimator be denoted by $\\hat{\\sigma}_{W, \\text{corr}}^{2}$. We require that $\\mathbb{E}[\\hat{\\sigma}_{W, \\text{corr}}^{2}] = \\sigma_{W}^{2}$.\nFrom the expectation of $s^2$ we have:\n$$\n\\mathbb{E}[s^2] = \\sigma_{W}^{2} + \\frac{\\sigma_{M}^{2}}{r}\n$$\nSince $\\sigma_{M}^{2}$ and $r$ are known constants, we can rearrange this equation to isolate $\\sigma_{W}^{2}$:\n$$\n\\sigma_{W}^{2} = \\mathbb{E}[s^2] - \\frac{\\sigma_{M}^{2}}{r}\n$$\nUsing the linearity of the expectation operator, we can write:\n$$\n\\sigma_{W}^{2} = \\mathbb{E}\\left[s^2 - \\frac{\\sigma_{M}^{2}}{r}\\right]\n$$\nThis equation shows that the statistic formed by subtracting the bias term $\\frac{\\sigma_{M}^{2}}{r}$ from $s^2$ has an expectation equal to $\\sigma_{W}^{2}$. Therefore, the corrected unbiased estimator for $\\sigma_W^2$ is:\n$$\n\\hat{\\sigma}_{W, \\text{corr}}^{2} = s^2 - \\frac{\\sigma_{M}^{2}}{r}\n$$\nThis is the second part of the required answer. This common technique is known as the method of moments for estimating variance components.\n\nThe final answer requires a two-entry row matrix containing the bias of $s^2$ and the corrected estimator for $\\sigma_W^2$.\n1. Bias of $s^2$: $\\frac{\\sigma_{M}^{2}}{r}$\n2. Corrected estimator: $s^2 - \\frac{\\sigma_{M}^{2}}{r}$",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{\\sigma_{M}^{2}}{r} & s^{2} - \\frac{\\sigma_{M}^{2}}{r}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "With reliable process data, the next step is to select an appropriate statistical distribution. Assuming a simple Gaussian model can be misleading, as real-world process variations are often asymmetric. This practice  introduces a powerful Bayesian method to compare competing models, helping you justify the choice of a more complex distribution that better captures the tails, which is essential for defining realistic and reliable process corners.",
            "id": "4167562",
            "problem": "A lithography-etch module generates in-line Critical Dimension (CD) measurements across multiple wafers to support Process Design Kit (PDK) corner model generation in semiconductor manufacturing. Suppose a sample of $n = 500$ independent CD measurements is considered for statistical modeling of across-lot variability. Two competing distribution models are assessed: a Gaussian model with parameters $\\mu$ and $\\sigma$, and a skew-normal model (Azzalini parametrization) with location $\\xi$, scale $\\omega$, and shape $\\alpha$. The goal is to decide which model better explains the observed CD variability using Bayesian model comparison.\n\nAssume the following:\n- The Gaussian model has $k_{\\mathrm{G}} = 2$ free parameters $(\\mu, \\sigma)$, and the skew-normal model has $k_{\\mathrm{SN}} = 3$ free parameters $(\\xi, \\omega, \\alpha)$.\n- The models have equal prior probabilities and diffuse parameter priors appropriate for large-sample asymptotics.\n- The maximum log-likelihoods computed from the data are $\\ell_{\\mathrm{G}} = -1420.0$ for the Gaussian model and $\\ell_{\\mathrm{SN}} = -1413.8$ for the skew-normal model.\n\nStarting from the definition of the Bayes factor as the ratio of marginal likelihoods, and using a valid large-sample asymptotic approximation justified under regularity conditions and diffuse priors, compute the Bayes factor in favor of the skew-normal model over the Gaussian model. Round your final numerical answer to three significant figures.",
            "solution": "The problem is first validated by a systematic analysis of its premises and requirements.\n\n### Step 1: Extract Givens\n- Sample size of independent Critical Dimension (CD) measurements: $n = 500$.\n- Model 1 (Gaussian): $k_{\\mathrm{G}} = 2$ free parameters $(\\mu, \\sigma)$.\n- Model 2 (Skew-Normal): $k_{\\mathrm{SN}} = 3$ free parameters $(\\xi, \\omega, \\alpha)$.\n- Prior probabilities of models: Equal.\n- Parameter priors: Diffuse, appropriate for large-sample asymptotics.\n- Maximum log-likelihood for the Gaussian model: $\\ell_{\\mathrm{G}} = -1420.0$.\n- Maximum log-likelihood for the skew-normal model: $\\ell_{\\mathrm{SN}} = -1413.8$.\n- Objective: Compute the Bayes factor in favor of the skew-normal model over the Gaussian model, $B_{\\mathrm{SN,G}}$.\n- Requirement: Round the final numerical answer to three significant figures.\n- Requirement: Provide a brief interpretation of the result's implications.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded (Critical)**: The problem is grounded in standard principles of Bayesian statistics and their application to semiconductor process modeling. The Gaussian and skew-normal distributions are well-defined statistical models. Bayesian model comparison via Bayes factors is a standard methodology. The use of a large-sample approximation (like the Bayesian Information Criterion, BIC) is a valid and common technique. The context of CD variability and corner model generation is a scientifically and industrially relevant application. The values provided are numerically plausible. The problem is scientifically sound.\n- **Well-Posed**: The problem provides all necessary information to compute the requested quantity. The quantities $n$, $k_{\\mathrm{G}}$, $k_{\\mathrm{SN}}$, $\\ell_{\\mathrm{G}}$, and $\\ell_{\\mathrm{SN}}$ are sufficient to calculate the Bayes factor under the specified large-sample approximation. The question is unambiguous and directs toward a unique solution.\n- **Objective (Critical)**: The problem statement is expressed in precise, technical language, free from subjectivity or opinion.\n\nThe problem does not exhibit any flaws such as scientific unsoundness, incompleteness, contradiction, or ambiguity.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be developed.\n\n### Solution Derivation\nThe objective is to compute the Bayes factor, $B_{\\mathrm{SN,G}}$, which compares the evidence for the skew-normal model ($M_{\\mathrm{SN}}$) against the Gaussian model ($M_{\\mathrm{G}}$). The Bayes factor is defined as the ratio of their marginal likelihoods:\n$$B_{\\mathrm{SN,G}} = \\frac{p(\\text{data} | M_{\\mathrm{SN}})}{p(\\text{data} | M_{\\mathrm{G}})}$$\nThe problem states that we should use a large-sample asymptotic approximation. Under regularity conditions and with diffuse parameter priors, the log-marginal likelihood for a model $M$ with $k$ parameters, given a sample of size $n$ and a maximum log-likelihood $\\ell_{\\max}$, can be approximated using the Schwarz criterion (or BIC approximation):\n$$\\ln(p(\\text{data} | M)) \\approx \\ell_{\\max} - \\frac{k}{2} \\ln(n)$$\nThe logarithm of the Bayes factor, $\\ln(B_{\\mathrm{SN,G}})$, is the difference between the log-marginal likelihoods of the two models:\n$$\\ln(B_{\\mathrm{SN,G}}) = \\ln(p(\\text{data} | M_{\\mathrm{SN}})) - \\ln(p(\\text{data} | M_{\\mathrm{G}}))$$\nApplying the approximation to each term, we get:\n$$\\ln(B_{\\mathrm{SN,G}}) \\approx \\left(\\ell_{\\mathrm{SN}} - \\frac{k_{\\mathrm{SN}}}{2} \\ln(n)\\right) - \\left(\\ell_{\\mathrm{G}} - \\frac{k_{\\mathrm{G}}}{2} \\ln(n)\\right)$$\nThis expression can be rearranged to separate the contribution from the likelihood fit and the penalty for model complexity:\n$$\\ln(B_{\\mathrm{SN,G}}) \\approx (\\ell_{\\mathrm{SN}} - \\ell_{\\mathrm{G}}) - \\frac{1}{2}(k_{\\mathrm{SN}} - k_{\\mathrm{G}})\\ln(n)$$\nWe are given the following values:\n- Sample size: $n = 500$\n- Maximum log-likelihoods: $\\ell_{\\mathrm{SN}} = -1413.8$ and $\\ell_{\\mathrm{G}} = -1420.0$\n- Number of parameters: $k_{\\mathrm{SN}} = 3$ and $k_{\\mathrm{G}} = 2$\n\nFirst, we calculate the difference in maximum log-likelihoods:\n$$\\ell_{\\mathrm{SN}} - \\ell_{\\mathrm{G}} = -1413.8 - (-1420.0) = 6.2$$\nNext, we calculate the difference in the number of parameters:\n$$k_{\\mathrm{SN}} - k_{\\mathrm{G}} = 3 - 2 = 1$$\nNow, we substitute these values into the approximation for the log-Bayes factor:\n$$\\ln(B_{\\mathrm{SN,G}}) \\approx 6.2 - \\frac{1}{2}(1)\\ln(500)$$\nWe compute the natural logarithm of the sample size:\n$$\\ln(500) \\approx 6.214608$$\nSubstituting this value back into the expression:\n$$\\ln(B_{\\mathrm{SN,G}}) \\approx 6.2 - \\frac{1}{2}(6.214608) = 6.2 - 3.107304 = 3.092696$$\nTo find the Bayes factor $B_{\\mathrm{SN,G}}$, we exponentiate the result:\n$$B_{\\mathrm{SN,G}} = \\exp(\\ln(B_{\\mathrm{SN,G}})) \\approx \\exp(3.092696) \\approx 22.0365$$\nThe problem requires rounding the final answer to three significant figures.\n$$B_{\\mathrm{SN,G}} \\approx 22.0$$\n\n### Interpretation\nA Bayes factor of $B_{\\mathrm{SN,G}} \\approx 22.0$ indicates that the observed CD data are approximately $22$ times more likely under the skew-normal model than under the Gaussian model. According to standard interpretation scales (e.g., Kass and Raftery), a Bayes factor of this magnitude provides \"strong\" evidence in favor of the skew-normal model.\n\nThe primary implication for corner model generation pertains to the distributional tails. A Gaussian model is symmetric, implying that positive and negative deviations from the mean (e.g., at $\\pm 3\\sigma$, defining \"slow\" and \"fast\" corners) are equally probable. The strong evidence for the skew-normal model suggests the CD distribution is significantly asymmetric. This means one tail of the distribution is heavier than the other. For instance, if the skewness is positive, devices with unusually large CDs (\"slow\" devices) would be more common than predicted by a Gaussian model, while devices with unusually small CDs (\"fast\" devices) would be less common.\n\nTherefore, for an accurate Process Design Kit (PDK), the corner selection policy must abandon the assumption of symmetry. Instead of defining corners based on symmetric quantiles of a fitted Gaussian distribution, the policy should use the quantiles of the better-fitting skew-normal model. This ensures that the \"worst-case\" scenarios used for circuit timing and power analysis are based on a more realistic representation of process variability, leading to more robust and reliable chip designs.",
            "answer": "$$\\boxed{22.0}$$"
        },
        {
            "introduction": "This final practice  applies these statistical insights to the ultimate goal: generating practical design corners. In a high-dimensional parameter space, Principal Component Analysis (PCA) provides an elegant method to identify the dominant modes of variation. This exercise will guide you through creating a reduced, physically meaningful set of corners, a sophisticated technique used for efficient and effective circuit simulation and design-for-manufacturability.",
            "id": "4167545",
            "problem": "A semiconductor process integration team wishes to generate reduced-order corners for device-level evaluation from a statistically characterized set of process parameters. Let the physical parameter vector be $p = (L, t_{\\mathrm{ox}}, V_{\\mathrm{th}})$, where $L$ is gate length, $t_{\\mathrm{ox}}$ is gate oxide thickness, and $V_{\\mathrm{th}}$ is threshold voltage. To preserve physical constraints, the team uses a bijective pre-processing map to unconstrained variables $x = (x_{1}, x_{2}, x_{3})$ defined by\n$$\nx_{1} = \\ln\\!\\left(\\frac{L}{L_{0}}\\right), \\quad x_{2} = \\ln\\!\\left(\\frac{t_{\\mathrm{ox}}}{t_{0}}\\right), \\quad x_{3} = \\ln\\!\\left(\\frac{V_{\\mathrm{th}} - V_{\\min}}{V_{\\max} - V_{\\mathrm{th}}}\\right),\n$$\nwith $L_{0} = 20\\,\\mathrm{nm}$, $t_{0} = 1.4\\,\\mathrm{nm}$, $V_{\\min} = 0.35\\,\\mathrm{V}$, and $V_{\\max} = 0.55\\,\\mathrm{V}$. The transformed variables are modeled as a zero-mean multivariate normal random vector $x \\sim \\mathcal{N}(\\mu,\\Sigma)$ with $\\mu = (0,0,0)$ and covariance\n$$\n\\Sigma = \\begin{pmatrix}\n0.0004 & 0.0003 & 0 \\\\\n0.0003 & 0.0016 & 0 \\\\\n0 & 0 & 0.0075\n\\end{pmatrix}.\n$$\nThe team applies Principal Component Analysis (PCA) to $x$ and retains the $r=2$ principal components corresponding to the two largest eigenvalues of $\\Sigma$. Define a fast-process corner by taking the reduced-order principal scores $s = (s_{1}, s_{2})$ equal to negative three times the standard deviation along each retained principal component (i.e., $s_{i} = -3\\sqrt{\\lambda_{i}}$ for each retained eigenvalue $\\lambda_{i}$), where the sign is chosen to jointly decrease $L$, $t_{\\mathrm{ox}}$, and $V_{\\mathrm{th}}$.\n\nUsing this reduced-order model, reconstruct the unconstrained variables via\n$$\nx \\approx \\mu + U_{r}\\,s,\n$$\nwhere $U_{r}$ is the $3\\times 2$ matrix whose columns are the retained unit-norm eigenvectors of $\\Sigma$, and then invert the pre-processing map to recover the physical parameters. Compute the reconstructed gate length $L$ for this fast-process corner. Express your final answer in nanometers and round to four significant figures.",
            "solution": "The user wants me to solve a statistical modeling problem in semiconductor manufacturing.\n\n### Step 1: Extract Givens\n- Physical parameter vector: $p = (L, t_{\\mathrm{ox}}, V_{\\mathrm{th}})$\n- Pre-processing map to unconstrained variables $x = (x_{1}, x_{2}, x_{3})$:\n  $$ x_{1} = \\ln\\!\\left(\\frac{L}{L_{0}}\\right), \\quad x_{2} = \\ln\\!\\left(\\frac{t_{\\mathrm{ox}}}{t_{0}}\\right), \\quad x_{3} = \\ln\\!\\left(\\frac{V_{\\mathrm{th}} - V_{\\min}}{V_{\\max} - V_{\\mathrm{th}}}\\right) $$\n- Constants:\n  $$ L_{0} = 20\\,\\mathrm{nm}, \\quad t_{0} = 1.4\\,\\mathrm{nm}, \\quad V_{\\min} = 0.35\\,\\mathrm{V}, \\quad V_{\\max} = 0.55\\,\\mathrm{V} $$\n- Statistical model for $x$: $x \\sim \\mathcal{N}(\\mu, \\Sigma)$\n  - Mean vector: $\\mu = (0, 0, 0)^T$\n  - Covariance matrix:\n    $$ \\Sigma = \\begin{pmatrix} 0.0004 & 0.0003 & 0 \\\\ 0.0003 & 0.0016 & 0 \\\\ 0 & 0 & 0.0075 \\end{pmatrix} $$\n- Procedure:\n  1. Apply Principal Component Analysis (PCA) to $x$.\n  2. Retain $r=2$ principal components corresponding to the two largest eigenvalues of $\\Sigma$.\n  3. Define a fast-process corner with principal scores $s = (s_1, s_2)^T$ where $s_i = -3\\sqrt{\\lambda_i}$ for each retained eigenvalue $\\lambda_i$.\n  4. The sign of the eigenvectors is chosen to jointly decrease $L$, $t_{\\mathrm{ox}}$, and $V_{\\mathrm{th}}$.\n  5. Reconstruct the unconstrained variables via $x \\approx \\mu + U_{r}\\,s$, where $U_r$ is the $3 \\times 2$ matrix of retained unit-norm eigenvectors.\n- Objective: Compute the reconstructed gate length $L$ for this corner, expressed in nanometers and rounded to four significant figures.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientifically Grounded**: The problem utilizes established statistical methods (PCA, multivariate normal distribution) and transformations (log-normal, logit-normal) that are standard in the field of semiconductor process modeling. The concept of generating process corners is a fundamental practice in integrated circuit design and manufacturing. The formulation is scientifically sound.\n2.  **Well-Posed**: The problem is clearly defined with all necessary data and a step-by-step procedure for calculation. The covariance matrix $\\Sigma$ is symmetric and can be shown to be positive definite, which is a requirement for a non-degenerate multivariate normal distribution. The problem leads to a unique, meaningful solution.\n3.  **Objective**: The problem is stated in precise, technical language, free from any subjectivity or opinion.\n4.  **Completeness and Consistency**: The problem is self-contained. The condition that \"$L$, $t_{\\mathrm{ox}}$, and $V_{\\mathrm{th}}$\" are jointly decreased provides a clear and verifiable constraint for choosing the signs of the eigenvectors, ensuring consistency.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. It is scientifically grounded, well-posed, and internally consistent. I will now proceed with the solution.\n\nThe goal is to compute the gate length $L$ for the specified fast-process corner. The procedure involves performing a principal component analysis on the covariance matrix $\\Sigma$, constructing a reduced-order model, and then using it to find the corner parameters.\n\nFirst, we find the eigenvalues and eigenvectors of the covariance matrix $\\Sigma$.\n$$ \\Sigma = \\begin{pmatrix} 0.0004 & 0.0003 & 0 \\\\ 0.0003 & 0.0016 & 0 \\\\ 0 & 0 & 0.0075 \\end{pmatrix} $$\nThe matrix is block-diagonal. One eigenvalue is immediately apparent: $\\lambda = 0.0075$ with a corresponding eigenvector in the direction $(0,0,1)^T$. The other two eigenvalues are the eigenvalues of the top-left $2 \\times 2$ submatrix:\n$$ A = \\begin{pmatrix} 0.0004 & 0.0003 \\\\ 0.0003 & 0.0016 \\end{pmatrix} = 10^{-4} \\begin{pmatrix} 4 & 3 \\\\ 3 & 16 \\end{pmatrix} $$\nLet's find the eigenvalues of the scaled matrix $A' = \\begin{pmatrix} 4 & 3 \\\\ 3 & 16 \\end{pmatrix}$. The characteristic equation is $\\det(A' - \\lambda' I) = 0$.\n$$ (4 - \\lambda')(16 - \\lambda') - 3 \\cdot 3 = 0 $$\n$$ \\lambda'^2 - 20\\lambda' + 64 - 9 = 0 $$\n$$ \\lambda'^2 - 20\\lambda' + 55 = 0 $$\nUsing the quadratic formula:\n$$ \\lambda' = \\frac{20 \\pm \\sqrt{(-20)^2 - 4(1)(55)}}{2} = \\frac{20 \\pm \\sqrt{400 - 220}}{2} = \\frac{20 \\pm \\sqrt{180}}{2} = \\frac{20 \\pm 6\\sqrt{5}}{2} = 10 \\pm 3\\sqrt{5} $$\nThe eigenvalues of $A$ are therefore $\\lambda'_A = (10 + 3\\sqrt{5}) \\times 10^{-4}$ and $\\lambda''_A = (10 - 3\\sqrt{5}) \\times 10^{-4}$.\n\nThe three eigenvalues of $\\Sigma$ in descending order are:\n$\\lambda_1 = 0.0075$\n$\\lambda_2 = (10 + 3\\sqrt{5}) \\times 10^{-4} \\approx 0.001671$\n$\\lambda_3 = (10 - 3\\sqrt{5}) \\times 10^{-4} \\approx 0.000329$\n\nWe retain the two largest eigenvalues, $\\lambda_1$ and $\\lambda_2$.\n\nNext, we find the corresponding unit-norm eigenvectors.\nFor $\\lambda_1 = 0.0075$: The eigenvector is orthogonal to the first two standard basis vectors, so it must be in the direction of $(0, 0, 1)^T$. The unit-norm eigenvector is $v_1 = (0, 0, 1)^T$.\n\nFor $\\lambda_2 = (10 + 3\\sqrt{5}) \\times 10^{-4}$: We find the eigenvector for $\\lambda'_2 = 10 + 3\\sqrt{5}$ from the submatrix $A'$. Let the eigenvector be $(a, b)^T$.\n$$ (4 - (10 + 3\\sqrt{5}))a + 3b = 0 \\implies (-6 - 3\\sqrt{5})a + 3b = 0 \\implies b = (2 + \\sqrt{5})a $$\nAn unnormalized eigenvector is $(1, 2+\\sqrt{5})^T$. To normalize it, we divide by its norm:\n$$ \\|(1, 2+\\sqrt{5})\\| = \\sqrt{1^2 + (2+\\sqrt{5})^2} = \\sqrt{1 + 4 + 4\\sqrt{5} + 5} = \\sqrt{10 + 4\\sqrt{5}} $$\nSo, the unit-norm eigenvector for the submatrix is $\\frac{1}{\\sqrt{10+4\\sqrt{5}}}(1, 2+\\sqrt{5})^T$.\nThe corresponding unit-norm eigenvector for $\\Sigma$ is $v_2 = \\left( \\frac{1}{\\sqrt{10+4\\sqrt{5}}}, \\frac{2+\\sqrt{5}}{\\sqrt{10+4\\sqrt{5}}}, 0 \\right)^T$.\n\nThe problem states that the sign of the eigenvectors must be chosen to ensure that $L$, $t_{\\mathrm{ox}}$, and $V_{\\mathrm{th}}$ decrease. This corresponds to negative values for $x_1$, $x_2$, and $x_3$. The reconstructed vector is $x = \\mu + U_r s = U_r s$ since $\\mu=0$. The scores are $s_1 = -3\\sqrt{\\lambda_1}$ and $s_2 = -3\\sqrt{\\lambda_2}$, which are both negative. Let the chosen eigenvectors be $u_1$ and $u_2$.\n$x = s_1 u_1 + s_2 u_2$.\nFor the first principal component, $s_1 u_1$, to contribute to a decrease in $V_{\\mathrm{th}}$ (i.e., cause $x_3 < 0$), the third component of $u_1$ must be positive since $s_1 < 0$. We choose $u_1 = v_1 = (0, 0, 1)^T$.\nFor the second principal component, $s_2 u_2$, to contribute to a decrease in $L$ and $t_{\\mathrm{ox}}$ (i.e., cause $x_1 < 0$ and $x_2 < 0$), the first and second components of $u_2$ must be positive since $s_2 < 0$. Our calculated $v_2$ has positive components $v_{2,1} = \\frac{1}{\\sqrt{10+4\\sqrt{5}}} > 0$ and $v_{2,2} = \\frac{2+\\sqrt{5}}{\\sqrt{10+4\\sqrt{5}}} > 0$. We choose $u_2 = v_2$.\n\nThe matrix of retained eigenvectors is $U_r = [u_1, u_2]$.\n$$ U_r = \\begin{pmatrix} 0 & \\frac{1}{\\sqrt{10+4\\sqrt{5}}} \\\\ 0 & \\frac{2+\\sqrt{5}}{\\sqrt{10+4\\sqrt{5}}} \\\\ 1 & 0 \\end{pmatrix} $$\nThe vector of principal scores is $s = (s_1, s_2)^T$.\nThe reconstructed vector $x$ is:\n$$ x = \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} = U_r s = \\begin{pmatrix} 0 & u_{2,1} \\\\ 0 & u_{2,2} \\\\ 1 & 0 \\end{pmatrix} \\begin{pmatrix} s_1 \\\\ s_2 \\end{pmatrix} = \\begin{pmatrix} u_{2,1} s_2 \\\\ u_{2,2} s_2 \\\\ s_1 \\end{pmatrix} $$\nWe need to calculate $L$, so we focus on $x_1$.\n$$ x_1 = u_{2,1} s_2 = \\frac{1}{\\sqrt{10+4\\sqrt{5}}} \\left(-3\\sqrt{\\lambda_2}\\right) = \\frac{-3\\sqrt{(10 + 3\\sqrt{5}) \\times 10^{-4}}}{\\sqrt{10+4\\sqrt{5}}} $$\n$$ x_1 = -3 \\times 10^{-2} \\sqrt{\\frac{10 + 3\\sqrt{5}}{10 + 4\\sqrt{5}}} $$\nTo simplify the fraction under the square root, we rationalize the denominator:\n$$ \\frac{10 + 3\\sqrt{5}}{10 + 4\\sqrt{5}} = \\frac{10 + 3\\sqrt{5}}{10 + 4\\sqrt{5}} \\cdot \\frac{10 - 4\\sqrt{5}}{10 - 4\\sqrt{5}} = \\frac{100 - 40\\sqrt{5} + 30\\sqrt{5} - 12(5)}{10^2 - (4\\sqrt{5})^2} = \\frac{40 - 10\\sqrt{5}}{100 - 80} = \\frac{40 - 10\\sqrt{5}}{20} = 2 - \\frac{\\sqrt{5}}{2} $$\nSo, the expression for $x_1$ simplifies to:\n$$ x_1 = -0.03 \\sqrt{2 - \\frac{\\sqrt{5}}{2}} $$\nNow, we compute the numerical value:\n$$ x_1 \\approx -0.03 \\sqrt{2 - \\frac{2.236068}{2}} = -0.03 \\sqrt{2 - 1.118034} = -0.03 \\sqrt{0.881966} \\approx -0.03 \\times 0.93913045 \\approx -0.02817391 $$\nFinally, we invert the pre-processing map to find $L$.\n$$ x_1 = \\ln\\left(\\frac{L}{L_0}\\right) \\implies L = L_0 \\exp(x_1) $$\nGiven $L_0 = 20\\,\\mathrm{nm}$:\n$$ L = 20 \\exp(-0.02817391) \\approx 20 \\times 0.9722215 = 19.44443\\,\\mathrm{nm} $$\nRounding the result to four significant figures gives $L = 19.44\\,\\mathrm{nm}$.",
            "answer": "$$\n\\boxed{19.44}\n$$"
        }
    ]
}