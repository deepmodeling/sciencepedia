{
    "hands_on_practices": [
        {
            "introduction": "The cornerstone of many CMP process models is the empirical relationship known as Preston's equation, which states that the material removal rate ($RR$) is directly proportional to the product of downforce pressure ($P$) and relative velocity ($V$). This exercise provides a hands-on guide to validating this fundamental model using hypothetical experimental data. By applying Ordinary Least Squares (OLS) regression, you will not only estimate the crucial Preston coefficient ($k$) but also learn how to statistically test for deviations from this linear behavior, a critical skill for process characterization and model refinement .",
            "id": "4114526",
            "problem": "You are asked to model the removal rate in Chemical-Mechanical Planarization (CMP) from first principles consistent with Semiconductor Manufacturing Process Modeling. Begin from the empirical Preston regime, which asserts that material removal in CMP scales with the frictional contact work rate at the pad–wafer interface. Treat the measured removal rate $RR$ in $\\mathrm{nm/min}$ as a linear response to the scalar regressor given by the product of applied downforce $P$ in $\\mathrm{kPa}$ and relative velocity $V$ in $\\mathrm{m/s}$, and then consider the possibility that deviations from the linear regime occur at elevated $P$ or $V$. Assume additive measurement noise that is independent and identically distributed, and use Ordinary Least Squares (OLS), which is defined by minimizing the sum of squared residuals, to estimate model parameters and quantify uncertainty with confidence intervals formed under a standard normal-error linear model.\n\nTask specification:\n- For each dataset, construct a design with a single regressor $x_i$ equal to the product $P_i V_i$, and fit a one-parameter linear model for $RR_i$ using OLS without an intercept. Compute the parameter estimate and its $95$ percent confidence interval using the Student’s $t$ distribution.\n- For each dataset, augment the model with quadratic terms in $P$ and $V$ as additional regressors to test for deviations from linearity at elevated $P$ or $V$. Fit the three-parameter model (no intercept) using OLS. Compute $95$ percent confidence intervals for each parameter. Then perform two-sided significance tests for the quadratic parameters using the Student’s $t$ distribution at level $0.05$ to decide whether deviations from linearity are present.\n- Units: express the linear coefficient for the single-regressor model in $(\\mathrm{nm/min})/(\\mathrm{kPa\\cdot m/s})$. In the augmented model, express the coefficient of the product regressor in $(\\mathrm{nm/min})/(\\mathrm{kPa\\cdot m/s})$, the coefficient of the quadratic pressure regressor in $(\\mathrm{nm/min})/(\\mathrm{kPa}^2)$, and the coefficient of the quadratic velocity regressor in $(\\mathrm{nm/min})/(\\mathrm{(m/s)}^2)$. Round all reported numerical outputs (parameter estimates and confidence interval endpoints) to $4$ decimal places. For the deviation decision, output an integer where $1$ indicates that at least one quadratic term is statistically significant at level $0.05$, and $0$ otherwise.\n\nDatasets (each consists of matched arrays of $P$, $V$, and $RR$, all of equal length):\n- Dataset A (baseline regime):\n    - $P=\\{20,30,40,50,60,25,35,45,55,65,28,52\\}\\,\\mathrm{kPa}$\n    - $V=\\{0.5,0.6,0.7,0.8,0.9,0.55,0.65,0.75,0.85,0.95,0.62,0.88\\}\\,\\mathrm{m/s}$\n    - $RR=\\{8.4,13.64,22.14,30.5,42.62,10.525,18.145,26.025,36.765,47.665,13.5408,35.8928\\}\\,\\mathrm{nm/min}$\n- Dataset B (pressure-enhanced regime):\n    - $P=\\{80,90,100,110,120,95,105,115,125,85,130,100\\}\\,\\mathrm{kPa}$\n    - $V=\\{0.7,0.7,0.7,0.7,0.7,0.8,0.8,0.8,0.8,0.75,0.75,0.9\\}\\,\\mathrm{m/s}$\n    - $RR=\\{47.8,56.75,63.3,72.85,79.9,67.0375,74.9375,84.8375,93.2375,55.5625,93.0,78.0\\}\\,\\mathrm{nm/min}$\n- Dataset C (velocity-enhanced regime):\n    - $P=\\{40,50,60,70,80,55,65,75,85,45,68,72\\}\\,\\mathrm{kPa}$\n    - $V=\\{0.6,0.7,0.8,0.9,1.0,1.1,1.2,1.0,0.9,1.2,0.95,0.85\\}\\,\\mathrm{m/s}$\n    - $RR=\\{17.8,24.9,34.6,44.5,57.0,45.475,57.7,54.05,53.375,42.8,46.4025,43.3925\\}\\,\\mathrm{nm/min}$\n\nRequired computations for each dataset:\n- Fit the single-regressor OLS model and report the coefficient estimate and its $95$ percent confidence interval.\n- Fit the augmented three-regressor OLS model (with regressors $x_1=P V$, $x_2=P^2$, $x_3=V^2$) and report the three coefficient estimates and their $95$ percent confidence intervals.\n- Using two-sided $t$-tests at level $0.05$, determine whether any quadratic term is significant and output the deviation flag as $1$ if significant and $0$ otherwise.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a list of lists. For each dataset, output a list with the following $13$ values in order:\n    1. The single-regressor coefficient estimate,\n    2. Its lower $95$ percent confidence bound,\n    3. Its upper $95$ percent confidence bound,\n    4. The augmented-model coefficient for $P V$,\n    5. The augmented-model coefficient for $P^2$,\n    6. The augmented-model coefficient for $V^2$,\n    7. The lower $95$ percent confidence bound for the augmented-model $P V$ coefficient,\n    8. The upper $95$ percent confidence bound for the augmented-model $P V$ coefficient,\n    9. The lower $95$ percent confidence bound for the augmented-model $P^2$ coefficient,\n    10. The upper $95$ percent confidence bound for the augmented-model $P^2$ coefficient,\n    11. The lower $95$ percent confidence bound for the augmented-model $V^2$ coefficient,\n    12. The upper $95$ percent confidence bound for the augmented-model $V^2$ coefficient,\n    13. The deviation flag ($0$ or $1$).\n- The final printed line must be a single Python-style list string with three inner lists, one per dataset, for example: $\\left[ [\\dots],[\\dots],[\\dots] \\right]$.",
            "solution": "The problem requires us to model the material removal rate ($RR$) in a Chemical–Mechanical Planarization (CMP) process using linear regression. We are given three datasets, each containing measurements of applied downforce ($P$), relative velocity ($V$), and the resulting removal rate ($RR$). The analysis is performed in two stages for each dataset.\n\nFirst, we fit a simple linear model based on Preston's equation, which posits that the removal rate is proportional to the mechanical work rate, represented by the product of pressure and velocity. This gives a single-regressor model with no intercept.\nSecond, we fit an augmented model that includes quadratic terms in pressure ($P^2$) and velocity ($V^2$) to test for non-linear behavior, which can occur at higher process parameters. This is a multiple linear regression model with three regressors and no intercept.\n\nFor both models, the parameters are estimated using the method of Ordinary Least Squares (OLS). We will also compute $95\\%$ confidence intervals for the estimated parameters and perform hypothesis tests on the coefficients of the quadratic terms to check for statistically significant deviations from the simple Preston model.\n\nThe general framework for Ordinary Least Squares (OLS) regression for a model of the form $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}$, where $\\mathbf{y}$ is an $n \\times 1$ vector of observations, $\\mathbf{X}$ is an $n \\times p$ design matrix of rank $p$, $\\boldsymbol{\\beta}$ is a $p \\times 1$ vector of unknown parameters, and $\\boldsymbol{\\epsilon}$ is an $n \\times 1$ vector of unobserved random errors, is to find the parameter vector $\\hat{\\boldsymbol{\\beta}}$ that minimizes the sum of squared residuals ($SSR$).\n$$SSR = \\sum_{i=1}^{n} e_i^2 = \\mathbf{e}^T\\mathbf{e} = (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^T (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})$$\nThe OLS estimator for $\\boldsymbol{\\beta}$ is found by solving $\\frac{\\partial(SSR)}{\\partial \\boldsymbol{\\beta}} = 0$, which yields the normal equations $\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta} = \\mathbf{X}^T\\mathbf{y}$. The solution is:\n$$\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}$$\n\nAssuming the errors $\\epsilon_i$ are independent and identically distributed with a normal distribution $N(0, \\sigma^2)$, the estimator $\\hat{\\boldsymbol{\\beta}}$ is also normally distributed. The unknown error variance $\\sigma^2$ is estimated by the sample variance:\n$$\\hat{\\sigma}^2 = \\frac{\\mathbf{e}^T\\mathbf{e}}{n-p} = \\frac{(\\mathbf{y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}})^T(\\mathbf{y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}})}{df}$$\nwhere $df = n-p$ are the degrees of freedom.\n\nThe covariance matrix of the parameter estimates is given by:\n$$Cov(\\hat{\\boldsymbol{\\beta}}) = \\hat{\\sigma}^2 (\\mathbf{X}^T\\mathbf{X})^{-1}$$\nThe standard error for a specific parameter estimate $\\hat{\\beta}_j$ is the square root of the $j$-th diagonal element of this covariance matrix, $se(\\hat{\\beta}_j) = \\sqrt{Cov(\\hat{\\boldsymbol{\\beta}})_{jj}}$.\n\nA $(1-\\alpha) \\times 100\\%$ confidence interval for a parameter $\\beta_j$ is constructed using the Student's $t$-distribution:\n$$\\hat{\\beta}_j \\pm t_{1-\\alpha/2, df} \\cdot se(\\hat{\\beta}_j)$$\nwhere $t_{1-\\alpha/2, df}$ is the critical value from the $t$-distribution with $df$ degrees of freedom for a cumulative probability of $1-\\alpha/2$. For this problem, we use a $95\\%$ confidence level, so $\\alpha=0.05$.\n\nA two-sided significance test for the null hypothesis $H_0: \\beta_j = 0$ is performed by computing the t-statistic $t_{stat} = \\hat{\\beta}_j / se(\\hat{\\beta}_j)$. The null hypothesis is rejected at significance level $\\alpha$ if $|t_{stat}| > t_{1-\\alpha/2, df}$, which is equivalent to the condition that the $(1-\\alpha)$ confidence interval for $\\beta_j$ does not contain $0$.\n\n**Model 1: Single-Regressor (Preston) Model**\n\nThe model is $RR_i = \\beta (P_i V_i) + \\epsilon_i$.\nHere, the response is $y_i = RR_i$ and the single regressor is $x_i = P_i V_i$.\nThe number of parameters is $p=1$. The degrees of freedom are $df = n-1 = 12-1=11$.\nThe design matrix $\\mathbf{X}$ is an $n \\times 1$ column vector of the $x_i$ values.\nThe OLS estimate $\\hat{\\beta}$ simplifies from the matrix form to:\n$$\\hat{\\beta} = \\frac{\\sum_{i=1}^{n} x_i y_i}{\\sum_{i=1}^{n} x_i^2}$$\nThe variance of the estimate is $Var(\\hat{\\beta}) = \\hat{\\sigma}^2 / (\\sum x_i^2)$, and its standard error is the square root of this value. The $95\\%$ confidence interval is then computed using the critical value $t_{0.975, 11}$.\n\n**Model 2: Augmented Three-Regressor Model**\n\nThe model is $RR_i = \\beta_1 (P_i V_i) + \\beta_2 (P_i^2) + \\beta_3 (V_i^2) + \\epsilon_i$.\nThe response is $y_i = RR_i$. The regressors are $x_{i1} = P_i V_i$, $x_{i2} = P_i^2$, and $x_{i3} = V_i^2$.\nThe number of parameters is $p=3$. The degrees of freedom are $df = n-3 = 12-3=9$.\nThe design matrix $\\mathbf{X}$ is an $n \\times 3$ matrix where the columns are the vectors of $x_{i1}$, $x_{i2}$, and $x_{i3}$.\nThe parameter vector estimate $\\hat{\\boldsymbol{\\beta}} = [\\hat{\\beta}_1, \\hat{\\beta}_2, \\hat{\\beta}_3]^T$ is computed using the full matrix formula $\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}$.\nConfidence intervals for each $\\hat{\\beta}_j$ are calculated using their respective standard errors (from the diagonal of the covariance matrix) and the critical value $t_{0.975, 9}$.\n\n**Deviation Decision**\nThe deviation flag is set to $1$ if the null hypothesis $H_0: \\beta_2 = 0$ or $H_0: \\beta_3 = 0$ is rejected at the $\\alpha=0.05$ significance level. This means the flag is $1$ if the $95\\%$ confidence interval for $\\beta_2$ does not contain $0$, or if the $95\\%$ confidence interval for $\\beta_3$ does not contain $0$. Otherwise, the flag is $0$.\n\nThe following steps are applied consistently to each of the three provided datasets.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import t\n\ndef solve():\n    \"\"\"\n    Main function to process all datasets and print the final results.\n    \"\"\"\n\n    # Define the datasets from the problem statement.\n    test_cases = [\n        # Dataset A (baseline regime)\n        (\n            [20, 30, 40, 50, 60, 25, 35, 45, 55, 65, 28, 52], # P (kPa)\n            [0.5, 0.6, 0.7, 0.8, 0.9, 0.55, 0.65, 0.75, 0.85, 0.95, 0.62, 0.88], # V (m/s)\n            [8.4, 13.64, 22.14, 30.5, 42.62, 10.525, 18.145, 26.025, 36.765, 47.665, 13.5408, 35.8928] # RR (nm/min)\n        ),\n        # Dataset B (pressure-enhanced regime)\n        (\n            [80, 90, 100, 110, 120, 95, 105, 115, 125, 85, 130, 100], # P (kPa)\n            [0.7, 0.7, 0.7, 0.7, 0.7, 0.8, 0.8, 0.8, 0.8, 0.75, 0.75, 0.9], # V (m/s)\n            [47.8, 56.75, 63.3, 72.85, 79.9, 67.0375, 74.9375, 84.8375, 93.2375, 55.5625, 93.0, 78.0] # RR (nm/min)\n        ),\n        # Dataset C (velocity-enhanced regime)\n        (\n            [40, 50, 60, 70, 80, 55, 65, 75, 85, 45, 68, 72], # P (kPa)\n            [0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.0, 0.9, 1.2, 0.95, 0.85], # V (m/s)\n            [17.8, 24.9, 34.6, 44.5, 57.0, 45.475, 57.7, 54.05, 53.375, 42.8, 46.4025, 43.3925] # RR (nm/min)\n        )\n    ]\n\n    all_results = []\n    for case in test_cases:\n        P_data, V_data, RR_data = case\n        result_for_case = process_dataset(P_data, V_data, RR_data)\n        all_results.append(result_for_case)\n\n    # Final print statement in the exact required format.\n    print(all_results)\n\ndef process_dataset(P, V, RR):\n    \"\"\"\n    Performs OLS regression for both models on a single dataset.\n\n    Args:\n        P (list): Pressure data in kPa.\n        V (list): Velocity data in m/s.\n        RR (list): Removal Rate data in nm/min.\n\n    Returns:\n        list: A list of 13 computed values, rounded to 4 decimal places where appropriate.\n    \"\"\"\n    P_arr, V_arr, RR_arr = np.array(P), np.array(V), np.array(RR)\n    n = len(P_arr)\n    y = RR_arr.reshape(-1, 1)\n\n    # --- Model 1: RR = beta * (P*V) ---\n    p1 = 1\n    df1 = n - p1\n    x1_reg = (P_arr * V_arr).reshape(-1, 1)\n\n    # Estimate beta\n    beta1_hat = np.linalg.inv(x1_reg.T @ x1_reg) @ x1_reg.T @ y\n    beta1_hat = beta1_hat[0, 0]\n\n    # Confidence interval for beta\n    residuals1 = y - x1_reg * beta1_hat\n    rss1 = (residuals1**2).sum()\n    sigma_sq_hat1 = rss1 / df1\n    var_beta1_hat = sigma_sq_hat1 * np.linalg.inv(x1_reg.T @ x1_reg)[0, 0]\n    se_beta1_hat = np.sqrt(var_beta1_hat)\n    t_crit1 = t.ppf(0.975, df1)\n    ci1_margin = t_crit1 * se_beta1_hat\n    ci1_lower = beta1_hat - ci1_margin\n    ci1_upper = beta1_hat + ci1_margin\n\n    # --- Model 2: RR = b1*(PV) + b2*(P^2) + b3*(V^2) ---\n    p2 = 3\n    df2 = n - p2\n    x_pv = P_arr * V_arr\n    x_p2 = P_arr**2\n    x_v2 = V_arr**2\n    X2 = np.vstack([x_pv, x_p2, x_v2]).T\n\n    # Estimate betas\n    beta2_hat_vec = np.linalg.inv(X2.T @ X2) @ X2.T @ y\n    b1_hat, b2_hat, b3_hat = beta2_hat_vec.flatten()\n\n    # Confidence intervals for betas\n    residuals2 = y - X2 @ beta2_hat_vec\n    rss2 = (residuals2**2).sum()\n    sigma_sq_hat2 = rss2 / df2\n    cov_beta2_hat = sigma_sq_hat2 * np.linalg.inv(X2.T @ X2)\n    se_beta2_hat = np.sqrt(np.diag(cov_beta2_hat))\n    t_crit2 = t.ppf(0.975, df2)\n    ci2_margins = t_crit2 * se_beta2_hat\n\n    ci2_b1_lower, ci2_b1_upper = b1_hat - ci2_margins[0], b1_hat + ci2_margins[0]\n    ci2_b2_lower, ci2_b2_upper = b2_hat - ci2_margins[1], b2_hat + ci2_margins[1]\n    ci2_b3_lower, ci2_b3_upper = b3_hat - ci2_margins[2], b3_hat + ci2_margins[2]\n\n    # --- Deviation Flag ---\n    # Significant if 95% CI does not contain 0\n    b2_significant = not (ci2_b2_lower  0 and ci2_b2_upper > 0)\n    b3_significant = not (ci2_b3_lower  0 and ci2_b3_upper > 0)\n    deviation_flag = 1 if b2_significant or b3_significant else 0\n\n    # --- Assemble and Round Results ---\n    results = [\n        beta1_hat, ci1_lower, ci1_upper,\n        b1_hat, b2_hat, b3_hat,\n        ci2_b1_lower, ci2_b1_upper,\n        ci2_b2_lower, ci2_b2_upper,\n        ci2_b3_lower, ci2_b3_upper,\n        deviation_flag\n    ]\n    \n    # Round float values to 4 decimal places, leave integers as is.\n    rounded_results = [round(r, 4) if isinstance(r, (float, np.floating)) else r for r in results]\n    \n    return rounded_results\n\nif __name__ == '__main__':\n    solve()\n\n```"
        },
        {
            "introduction": "While Preston's equation provides a global understanding of removal rates, real-world CMP performance is highly dependent on the local features of the integrated circuit layout. This practice guides you through building a one-dimensional simulation to explore how pattern density—the local concentration of features—modulates the removal rate. You will implement a weighted-density model, where the basic Preston's law is modified by a function of the local density, to predict the evolution of wafer topography and understand the origins of pattern-dependent erosion .",
            "id": "4114506",
            "problem": "You are asked to build a one-dimensional simulation of Chemical-Mechanical Planarization (CMP) topography evolution under a weighted density model. The fundamental base must start from Preston’s equation, which states that the local removal rate is proportional to the product of applied pressure and relative velocity. Specifically, use the following base laws and definitions:\n\n- Preston’s equation: the local removal rate satisfies $RR(x) \\propto P \\cdot V$.\n- Conservation of thickness: the film thickness $h(x,t)$ decreases over time according to the local removal rate, so that $\\dfrac{\\partial h(x,t)}{\\partial t} = -RR(x)$.\n- Weighted density definition: the effective local pattern density $\\rho(x)$ is defined as the convolution of a binary pattern indicator $p(x)$ and a normalized Gaussian kernel $G_{\\sigma}(x)$ over the domain. The Gaussian kernel must be normalized so that $\\int G_{\\sigma}(x)\\,dx = 1$; in the discrete periodic setting, its sum must be $1$.\n- The weighted density model for CMP removal rate is $RR(x) = k \\, P \\, V \\, g(\\rho(x))$, where $k$ is the Preston coefficient, $P$ is the applied pressure, $V$ is the relative velocity, and $g$ is a specified density-to-removal modifier.\n\nYou must simulate the evolution of $h(x,t)$ on a one-dimensional, periodic domain of $N$ grid points representing a line across a die. The binary pattern indicator $p(x)$ encodes stripes of features with a specified duty cycle. The domain consists of two halves:\n\n- The left half is “dense” with duty cycle $D_{\\mathrm{dense}}$.\n- The right half is “sparse” with duty cycle $D_{\\mathrm{sparse}}$.\n\nWithin each half, construct a periodic stripe pattern of period $\\Lambda$ grid points, where $p(x)=1$ for the first $D\\cdot\\Lambda$ points of each period and $p(x)=0$ for the remaining points in that period, with $D$ equal to $D_{\\mathrm{dense}}$ in the left half and $D_{\\mathrm{sparse}}$ in the right half.\n\nDefine the effective density by a circular (periodic) convolution\n$$\n\\rho(x) = (p * G_{\\sigma})(x),\n$$\nwhere $G_{\\sigma}(x)$ is a discrete circular Gaussian kernel of bandwidth $\\sigma$ grid points and unit sum. Use the specific removal modifier function\n$$\ng(\\rho) = 1 - \\alpha \\, \\rho^{\\beta},\n$$\nwith parameters $\\alpha$ and $\\beta$ given per test case. The initial film thickness is uniform, $h(x,0) = H_0$.\n\nEvolve the thickness with an explicit scheme consistent with the base laws:\n$$\nh(x,t + \\Delta t) = h(x,t) - RR(x)\\,\\Delta t,\n$$\nfor $t$ from $0$ to $T$ in steps of $\\Delta t$. Compute the erosion $E(x) = H_0 - h(x,T)$ in $\\mathrm{nm}$.\n\nTo compare dense and sparse regions, compute the average erosion over interior windows to avoid boundary effects:\n- Dense window: indices $i$ with $i \\in [\\lfloor 0.1 N \\rfloor, \\lfloor 0.4 N \\rfloor]$.\n- Sparse window: indices $i$ with $i \\in [\\lfloor 0.6 N \\rfloor, \\lfloor 0.9 N \\rfloor]$.\n\nFor each test case, compare the predicted average erosion in the dense and sparse windows to provided measured values and report whether each comparison is within a specified absolute tolerance. Specifically, define the boolean results:\n- Dense comparison: $\\left| \\overline{E}_{\\mathrm{dense}} - E_{\\mathrm{dense,meas}} \\right| \\le \\tau$.\n- Sparse comparison: $\\left| \\overline{E}_{\\mathrm{sparse}} - E_{\\mathrm{sparse,meas}} \\right| \\le \\tau$.\n\nAll physical quantities must use explicit units:\n- Report erosion in $\\mathrm{nm}$.\n- Use $P$ in $\\mathrm{kPa}$.\n- Use $V$ in $\\mathrm{m/s}$.\n- Use $k$ in $\\mathrm{nm}/(\\mathrm{kPa}\\cdot\\mathrm{m})$.\n- Use time in $\\mathrm{s}$.\n\nDomain and discretization:\n- Use a periodic domain with $N$ grid points with $N = 1000$ (dimensionless grid index).\n- Use a stripe period $\\Lambda$ in grid points (dimensionless).\n- Use Gaussian bandwidth $\\sigma$ in grid points (dimensionless).\n- Use time step $\\Delta t$ in $\\mathrm{s}$ and total polish time $T$ in $\\mathrm{s}$.\n\nOutput specification:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case contributes a two-element list of booleans in the order $[\\text{dense},\\text{sparse}]$. For example: $[[\\text{True},\\text{False}],[\\text{True},\\text{True}]]$.\n\nTest suite:\nUse $N = 1000$ and the following parameter sets. Each case is a tuple $(k, P, V, \\alpha, \\beta, \\sigma, \\Lambda, D_{\\mathrm{dense}}, D_{\\mathrm{sparse}}, H_0, T, \\Delta t, \\tau, E_{\\mathrm{dense,meas}}, E_{\\mathrm{sparse,meas}})$:\n\n- Case $1$ (happy path): (0.25 nm/(kPa·m), 10 kPa, 0.5 m/s, 0.6, 1.0, 5, 20, 0.7, 0.2, 1000 nm, 120 s, 0.5 s, 5 nm, 90 nm, 130 nm).\n\n- Case $2$ (boundary; zero pressure): (0.25 nm/(kPa·m), 0 kPa, 0.5 m/s, 0.6, 1.0, 5, 20, 0.7, 0.2, 1000 nm, 120 s, 0.5 s, 0.5 nm, 0 nm, 0 nm).\n\n- Case $3$ (strong density sensitivity): (0.25 nm/(kPa·m), 10 kPa, 0.5 m/s, 0.9, 2.0, 5, 20, 0.7, 0.2, 1000 nm, 120 s, 0.5 s, 5 nm, 85 nm, 145 nm).\n\n- Case $4$ (wide weighting; near-uniform effective density): (0.25 nm/(kPa·m), 10 kPa, 0.5 m/s, 0.6, 1.0, 200, 20, 0.7, 0.2, 1000 nm, 120 s, 0.5 s, 3 nm, 110 nm, 110 nm).\n\nYour task:\n- Implement the simulation and compute the boolean comparison results for each case exactly as specified.\n- Express all erosion values in $\\mathrm{nm}$.\n- Produce the final output as a single line: a list of four two-element lists $[[b_{1,d},b_{1,s}],[b_{2,d},b_{2,s}],[b_{3,d},b_{3,s}],[b_{4,d},b_{4,s}]]$ where each $b$ is either $\\mathrm{True}$ or $\\mathrm{False}$.",
            "solution": "The user provides a problem that requires the implementation of a one-dimensional simulation for Chemical–Mechanical Planarization (CMP) topography evolution. The solution must be based on a weighted density model derived from Preston's equation.\n\n### Problem Validation\nThe provided problem statement has been rigorously validated and is determined to be **valid**. It is scientifically grounded in established CMP modeling principles, well-posed with a complete and consistent set of definitions, equations, and parameters, and is expressed in objective, formalizable language. All units are consistent, and the numerical parameters and test cases are physically realistic. The problem requires a step-by-step implementation of a well-defined simulation, and contains no ambiguities or contradictions that would prevent a unique solution.\n\n### Principle-Based Solution Design\n\nThe simulation of the film thickness evolution $h(x,t)$ is developed following the specified physical model. The core of the model is the local material removal rate, $RR(x)$, which is assumed to be constant over the polishing time $T$.\n\n**1. Removal Rate Formulation**\n\nThe local removal rate is given by the weighted density model:\n$$\nRR(x) = k \\, P \\, V \\, g(\\rho(x))\n$$\nwhere $k$ is the Preston coefficient, $P$ is the applied pressure, $V$ is the relative polishing velocity, and $g(\\rho(x))$ is a function that modifies the removal rate based on the effective local pattern density $\\rho(x)$. The specific modifier function is:\n$$\ng(\\rho) = 1 - \\alpha \\, \\rho^{\\beta}\n$$\nThe parameters $k, P, V, \\alpha,$ and $\\beta$ are provided for each test case.\n\nSince $RR(x)$ is independent of time $t$ and thickness $h$, the evolution of the film thickness given by the differential equation $\\frac{\\partial h}{\\partial t} = -RR(x)$ with initial condition $h(x,0) = H_0$ can be integrated directly over the total polish time $T$:\n$$\nh(x,T) = h(x,0) - \\int_0^T RR(x) \\, dt = H_0 - RR(x) \\cdot T\n$$\nThe total erosion, defined as the amount of material removed, is therefore:\n$$\nE(x) = H_0 - h(x,T) = RR(x) \\cdot T\n$$\nThe explicit time-stepping scheme provided in the problem, $h(x,t + \\Delta t) = h(x,t) - RR(x)\\,\\Delta t$, is consistent with this direct integration, as summing the removals over all time steps from $t=0$ to $T$ results in a total removal of $RR(x) \\cdot T$.\n\n**2. Domain and Pattern Definition**\n\nThe simulation domain is a one-dimensional periodic grid of $N=1000$ points. This grid represents a line scan across a semiconductor die. The domain is partitioned into two halves:\n-   **Dense Region**: The left half (indices $i \\in [0, N/2 - 1]$) with a higher feature duty cycle $D_{\\mathrm{dense}}$.\n-   **Sparse Region**: The right half (indices $i \\in [N/2, N - 1]$) with a lower feature duty cycle $D_{\\mathrm{sparse}}$.\n\nA binary pattern indicator array, $p(x)$, represents the presence ($1$) or absence ($0$) of features. Within each half, $p(x)$ is constructed by repeating a basic periodic pattern of length $\\Lambda$. For a given duty cycle $D$ and period $\\Lambda$, the basic pattern consists of $\\lfloor D \\cdot \\Lambda \\rfloor$ points with value $1$, followed by $(\\Lambda - \\lfloor D \\cdot \\Lambda \\rfloor)$ points with value $0$. This tile is repeated to fill the corresponding half of the domain.\n\n**3. Effective Pattern Density Calculation**\n\nThe key physical concept is that the local removal rate depends not on the point-wise pattern $p(x)$, but on a spatially-averaged or \"effective\" pattern density, $\\rho(x)$. This models physical effects such as pad bending and slurry transport that have a characteristic length scale. This effective density is calculated by a circular convolution of the binary pattern $p(x)$ with a normalized discrete Gaussian kernel $G_{\\sigma}(x)$:\n$$\n\\rho(x) = (p * G_{\\sigma})(x)\n$$\nThe Gaussian kernel $G_{\\sigma}(x)$ is centered at $x=0$ on the periodic domain of size $N$, has a bandwidth (standard deviation) of $\\sigma$, and is normalized such that its elements sum to $1$. For a discrete grid point $i \\in [0, N-1]$, the kernel is defined as:\n$$\nG_{\\sigma}(i) = C \\cdot \\exp\\left(-\\frac{d(i,0)^2}{2\\sigma^2}\\right)\n$$\nwhere $d(i,0) = \\min(i, N-i)$ is the periodic distance from the origin, and $C$ is a normalization constant ensuring $\\sum_{i=0}^{N-1} G_{\\sigma}(i) = 1$. The circular convolution is efficiently computed using the Fast Fourier Transform (FFT) via the convolution theorem:\n$$\n\\rho = \\mathcal{F}^{-1}\\{\\mathcal{F}\\{p\\} \\cdot \\mathcal{F}\\{G_{\\sigma}\\}\\}\n$$\nwhere $\\mathcal{F}$ and $\\mathcal{F}^{-1}$ denote the forward and inverse FFT, respectively.\n\n**4. Analysis and Validation**\n\nAfter computing the erosion array $E(x)$, the simulation results are compared to measured data. To avoid edge effects near the dense/sparse boundary and the periodic wrap-around point, the average erosion is calculated over specified interior windows:\n-   $\\overline{E}_{\\mathrm{dense}} = \\mathrm{mean}(E(i))$ for $i \\in [\\lfloor 0.1 N \\rfloor, \\lfloor 0.4 N \\rfloor]$\n-   $\\overline{E}_{\\mathrm{sparse}} = \\mathrm{mean}(E(i))$ for $i \\in [\\lfloor 0.6 N \\rfloor, \\lfloor 0.9 N \\rfloor]$\n\nThese computed average erosion values are then compared against the provided measured values, $E_{\\mathrm{dense,meas}}$ and $E_{\\mathrm{sparse,meas}}$. A match is declared if the absolute difference is within a given tolerance $\\tau$:\n-   Dense comparison: $|\\overline{E}_{\\mathrm{dense}} - E_{\\mathrm{dense,meas}}| \\le \\tau$\n-   Sparse comparison: $|\\overline{E}_{\\mathrm{sparse}} - E_{\\mathrm{sparse,meas}}| \\le \\tau$\n\nThe final output for each test case is a list of two boolean values representing the outcomes of these two comparisons.\n\n**Implementation Summary**\n\nThe algorithm proceeds as follows for each test case:\n1.  Unpack all input parameters.\n2.  Construct the $N$-point binary pattern array $p(x)$ based on $D_{\\mathrm{dense}}$, $D_{\\mathrm{sparse}}$, and $\\Lambda$.\n3.  Construct the normalized discrete circular Gaussian kernel $G_{\\sigma}(x)$ based on $\\sigma$ and $N$.\n4.  Compute the effective density array $\\rho(x)$ via FFT-based circular convolution.\n5.  Calculate the local removal rate array $RR(x)$.\n6.  Calculate the total erosion array $E(x) = RR(x) \\cdot T$.\n7.  Compute the average erosion over the dense and sparse windows.\n8.  Perform the boolean comparisons with the measured data and tolerance $\\tau$.\n9.  Format and store the resulting `[boolean, boolean]` pair.\nAfter processing all cases, the collected results are formatted into a single string as specified.",
            "answer": "```python\nimport numpy as np\n\ndef run_simulation(params):\n    \"\"\"\n    Runs a single 1D CMP simulation for a given set of parameters.\n\n    Args:\n        params (tuple): A tuple containing all parameters for the simulation case.\n          The order is (k, P, V, alpha, beta, sigma, Lambda, D_dense, D_sparse,\n                         H0, T, dt, tau, E_dense_meas, E_sparse_meas).\n\n    Returns:\n        list: A list of two booleans [dense_comparison, sparse_comparison].\n    \"\"\"\n    (k, P, V, alpha, beta, sigma, Lambda, D_dense, D_sparse,\n     H0, T, dt, tau, E_dense_meas, E_sparse_meas) = params\n\n    N = 1000  # Number of grid points\n\n    # Step 1: Construct the binary pattern indicator p(x)\n    p = np.zeros(N)\n    N_half = N // 2\n\n    # Dense half (left)\n    if Lambda  0:\n        w_dense = int(D_dense * Lambda)\n        tile_dense = np.zeros(Lambda)\n        tile_dense[:w_dense] = 1\n        num_tiles_dense = N_half // Lambda\n        if N_half % Lambda != 0:\n            # This case is not expected based on problem description (500 is multiple of 20)\n            # but handle it for robustness.\n            raise ValueError(\"Size of half-domain must be a multiple of Lambda.\")\n        p_dense = np.tile(tile_dense, num_tiles_dense)\n        p[:N_half] = p_dense\n\n    # Sparse half (right)\n    if Lambda  0:\n        w_sparse = int(D_sparse * Lambda)\n        tile_sparse = np.zeros(Lambda)\n        tile_sparse[:w_sparse] = 1\n        num_tiles_sparse = N_half // Lambda\n        if N_half % Lambda != 0:\n            raise ValueError(\"Size of half-domain must be a multiple of Lambda.\")\n        p_sparse = np.tile(tile_sparse, num_tiles_sparse)\n        p[N_half:] = p_sparse\n\n    # Step 2: Construct the discrete circular Gaussian kernel G_sigma(x)\n    indices = np.arange(N)\n    distances = np.minimum(indices, N - indices)\n    \n    # Handle sigma=0 case to avoid division by zero, although not in test cases.\n    # It would correspond to a delta function.\n    if sigma  0:\n        G_sigma = np.exp(-distances**2 / (2 * sigma**2))\n    else:\n        G_sigma = np.zeros(N)\n        G_sigma[0] = 1.0\n\n    G_sigma /= np.sum(G_sigma)\n\n    # Step 3: Compute the effective density rho(x) via circular convolution\n    p_fft = np.fft.fft(p)\n    G_fft = np.fft.fft(G_sigma)\n    rho = np.real(np.fft.ifft(p_fft * G_fft))\n    \n    # Step 4: Calculate the removal rate RR(x)\n    g_rho = 1 - alpha * rho**beta\n    RR = k * P * V * g_rho\n    \n    # Step 5: Calculate the final erosion E(x)\n    # E(x) = H0 - h(x, T) = H0 - (H0 - RR(x) * T) = RR(x) * T\n    erosion = RR * T\n\n    # Step 6: Compute average erosion in dense and sparse windows\n    dense_win_start = int(0.1 * N)\n    dense_win_end = int(0.4 * N)\n    sparse_win_start = int(0.6 * N)\n    sparse_win_end = int(0.9 * N)\n    \n    # Python slicing is exclusive on the right, but the spec is inclusive.\n    avg_E_dense = np.mean(erosion[dense_win_start : dense_win_end + 1])\n    avg_E_sparse = np.mean(erosion[sparse_win_start : sparse_win_end + 1])\n\n    # Step 7: Perform boolean comparisons\n    is_dense_match = abs(avg_E_dense - E_dense_meas) = tau\n    is_sparse_match = abs(avg_E_sparse - E_sparse_meas) = tau\n\n    # Ensure python bool type instead of numpy.bool_\n    return [bool(is_dense_match), bool(is_sparse_match)]\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (k, P, V, alpha, beta, sigma, Lambda, D_dense, D_sparse, H0, T, dt, tau, E_dense_meas, E_sparse_meas)\n    test_cases = [\n        # (k, P, V, alpha, beta, sigma, Lambda, D_dense, D_sparse, H0, T, dt, tau, E_dense_meas, E_sparse_meas)\n        (0.25, 10, 0.5, 0.6, 1.0, 5, 20, 0.7, 0.2, 1000, 120, 0.5, 5, 90, 130), # Case 1\n        (0.25, 0, 0.5, 0.6, 1.0, 5, 20, 0.7, 0.2, 1000, 120, 0.5, 0.5, 0, 0),    # Case 2\n        (0.25, 10, 0.5, 0.9, 2.0, 5, 20, 0.7, 0.2, 1000, 120, 0.5, 5, 85, 145), # Case 3\n        (0.25, 10, 0.5, 0.6, 1.0, 200, 20, 0.7, 0.2, 1000, 120, 0.5, 3, 110, 110),# Case 4\n    ]\n\n    results = []\n    for case in test_cases:\n        case_result = run_simulation(case)\n        # Format each two-element list into its string representation '[True,False]'\n        # to correctly build the final output string.\n        results.append(str(case_result).replace(\" \", \"\"))\n    \n    # Final print statement in the exact required format.\n    # The map(str,...) is now redundant as we manually stringified, but kept for structure.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Effective process control in manufacturing must account for inherent variability and measurement uncertainty. This practice moves from deterministic prediction to stochastic optimization, tackling the critical challenge of setting an optimal barrier layer removal time in CMP. You will develop a process policy that balances the competing goals of minimizing unwanted material erosion and limiting the risk of incomplete polishing (under-polish), given that pre-process measurements of film thickness are inherently noisy. This exercise demonstrates how to use fundamental probability theory to design a robust, risk-aware process window .",
            "id": "4114508",
            "problem": "Consider a simplified but scientifically sound model of Chemical-Mechanical Planarization (CMP) for barrier layer removal in semiconductor manufacturing. The goal is to design a time-based endpoint policy that minimizes expected erosion while satisfying a probabilistic risk constraint on under-polish. Assume the following fundamentals as the modeling base:\n\n- Chemical-Mechanical Planarization (CMP) removes material at a constant removal rate when process conditions are fixed. If the true barrier thickness is $T_{0}$ in $\\mathrm{nm}$ and the barrier-layer removal rate is $R_{b}$ in $\\mathrm{nm/s}$, then the ideal clear time (the time needed to just remove the barrier) is $t_{\\mathrm{clear}} = T_{0} / R_{b}$ in $\\mathrm{s}$.\n- A pre-polish metrology estimates the barrier thickness by a single-shot noisy measurement $T_{0}^{\\mathrm{hat}} = T_{0} + \\varepsilon$, where $\\varepsilon$ is an independent, zero-mean Gaussian random variable with standard deviation $\\sigma_{m}$ in $\\mathrm{nm}$, i.e., $\\varepsilon \\sim \\mathcal{N}(0,\\sigma_{m}^{2})$.\n- The removal policy is time-based: choose a polishing time $t = T_{0}^{\\mathrm{hat}}/R_{b} + m$, where $m \\ge 0$ is an over-polish margin in $\\mathrm{s}$ to be designed.\n- If $t \\lt t_{\\mathrm{clear}}$, the barrier is not fully removed (under-polish). If $t \\ge t_{\\mathrm{clear}}$, the barrier is cleared, and the additional time $(t - t_{\\mathrm{clear}})$ is over-polish time. Erosion is modeled as proportional to the over-polish time with a constant erosion rate $R_{e}$ in $\\mathrm{nm/s}$, so the erosion thickness for one trial is $E = R_{e} \\max\\{t - t_{\\mathrm{clear}}, 0\\}$ in $\\mathrm{nm}$.\n\nYou are given that across locations on the wafer, the true barrier thickness $T_{0}$ is normally distributed as $T_{0} \\sim \\mathcal{N}(\\mu_{T}, \\sigma_{T}^{2})$ in $\\mathrm{nm}$, independently of the measurement noise $\\varepsilon$. You do not observe $T_{0}$ directly before polishing; only $T_{0}^{\\mathrm{hat}}$ is observed once. The decision policy must be a fixed margin $m$ applied to all sites per the rule above.\n\nYour task is to determine the optimal over-polish margin $m^{\\star}$ in $\\mathrm{s}$ that minimizes the expected erosion thickness $\\mathbb{E}[E]$ (in $\\mathrm{nm}$) subject to a probabilistic endpoint risk constraint that the probability of under-polish does not exceed a specified tolerance $\\alpha \\in (0,1)$:\n$$\n\\mathbb{P}\\left(t \\lt t_{\\mathrm{clear}}\\right) \\le \\alpha.\n$$\nProvide a derivation grounded in the stated fundamentals and probability theory to:\n1) Characterize the distribution of the random variable $\\Delta = t - t_{\\mathrm{clear}}$.\n2) Express the under-polish probability $\\mathbb{P}(\\Delta \\lt 0)$ in terms of $m$, $R_{b}$, and $\\sigma_{m}$.\n3) Determine the smallest $m$ that satisfies the risk constraint.\n4) Compute the resulting minimum expected erosion $\\mathbb{E}[E]$ under that choice.\n\nYour program must, for each test case below, compute and output the pair $[m^{\\star}, \\mathbb{E}[E]_{\\mathrm{min}}]$ where $m^{\\star}$ is in $\\mathrm{s}$ and $\\mathbb{E}[E]_{\\mathrm{min}}$ is in $\\mathrm{nm}$. Each floating-point output must be rounded to six decimal places.\n\nInputs for each test case are the tuple $(\\mu_{T}, \\sigma_{T}, \\sigma_{m}, R_{b}, R_{e}, \\alpha)$ with units $(\\mathrm{nm}, \\mathrm{nm}, \\mathrm{nm}, \\mathrm{nm/s}, \\mathrm{nm/s}, \\text{unitless})$. Note that all physical rates and thicknesses are scientifically plausible, and the model assumes constant rates during the relevant phases.\n\nTest suite (use exactly these five cases in this order):\n- Case $1$: $(\\mu_{T}, \\sigma_{T}, \\sigma_{m}, R_{b}, R_{e}, \\alpha) = (30, 3, 5, 1.0, 0.2, 0.05)$\n- Case $2$: $(\\mu_{T}, \\sigma_{T}, \\sigma_{m}, R_{b}, R_{e}, \\alpha) = (15, 1, 2, 1.2, 0.1, 0.5)$\n- Case $3$: $(\\mu_{T}, \\sigma_{T}, \\sigma_{m}, R_{b}, R_{e}, \\alpha) = (40, 5, 0.1, 1.5, 0.05, 0.01)$\n- Case $4$: $(\\mu_{T}, \\sigma_{T}, \\sigma_{m}, R_{b}, R_{e}, \\alpha) = (25, 4, 10, 0.8, 0.25, 0.1)$\n- Case $5$: $(\\mu_{T}, \\sigma_{T}, \\sigma_{m}, R_{b}, R_{e}, \\alpha) = (50, 8, 7, 0.5, 0.2, 10^{-6})$\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list of five items, each item being the two-element list $[m^{\\star}, \\mathbb{E}[E]_{\\mathrm{min}}]$ with six decimal places, all enclosed in one top-level pair of square brackets. For example, an output with two cases would look like $[[m_{1},e_{1}],[m_{2},e_{2}]]$. For this problem, the output must be a single line of the form $[[m^{\\star}_{1},e_{1}],[m^{\\star}_{2},e_{2}],[m^{\\star}_{3},e_{3}],[m^{\\star}_{4},e_{4}],[m^{\\star}_{5},e_{5}]]$ with no spaces.",
            "solution": "The problem as stated is scientifically sound, well-posed, and contains all necessary information for a unique solution. It is a standard problem in statistical process control applied to semiconductor manufacturing. We proceed with the solution.\n\nThe objective is to find the optimal over-polish margin, $m^{\\star}$, that minimizes the expected erosion, $\\mathbb{E}[E]$, subject to a constraint on the probability of under-polish. The solution requires a four-step derivation as outlined in the problem statement.\n\n**1. Characterize the distribution of the random variable $\\Delta = t - t_{\\mathrm{clear}}$**\n\nThe total polishing time, $t$, is defined by the policy:\n$$t = \\frac{T_{0}^{\\mathrm{hat}}}{R_{b}} + m$$\nwhere $T_{0}^{\\mathrm{hat}}$ is the noisy measurement of the true barrier thickness $T_{0}$, $R_{b}$ is the barrier removal rate, and $m$ is the over-polish margin. The measurement is given by $T_{0}^{\\mathrm{hat}} = T_{0} + \\varepsilon$, where the measurement noise $\\varepsilon$ is a random variable distributed as $\\varepsilon \\sim \\mathcal{N}(0, \\sigma_{m}^{2})$.\n\nThe ideal clear time, $t_{\\mathrm{clear}}$, is the time required to exactly remove the true barrier thickness $T_{0}$:\n$$t_{\\mathrm{clear}} = \\frac{T_{0}}{R_{b}}$$\nThe random variable $\\Delta$ represents the difference between the applied polish time and the ideal clear time. Substituting the definitions of $t$ and $t_{\\mathrm{clear}}$:\n$$\\Delta = t - t_{\\mathrm{clear}} = \\left(\\frac{T_{0}^{\\mathrm{hat}}}{R_{b}} + m\\right) - \\frac{T_{0}}{R_{b}}$$\nSubstituting $T_{0}^{\\mathrm{hat}} = T_{0} + \\varepsilon$:\n$$\\Delta = \\left(\\frac{T_{0} + \\varepsilon}{R_{b}} + m\\right) - \\frac{T_{0}}{R_{b}} = \\frac{T_{0}}{R_{b}} + \\frac{\\varepsilon}{R_{b}} + m - \\frac{T_{0}}{R_{b}}$$\nThe terms involving the true thickness $T_{0}$ cancel, yielding a simplified expression for $\\Delta$:\n$$\\Delta = m + \\frac{\\varepsilon}{R_{b}}$$\nThis is a crucial result: the distribution of the time difference $\\Delta$ depends only on the over-polish margin $m$ and the distribution of the measurement noise $\\varepsilon$, not on the distribution of the true thickness $T_{0}$. The provided parameters $\\mu_{T}$ and $\\sigma_{T}$ are therefore not required for this part of the analysis.\n\nSince $\\varepsilon \\sim \\mathcal{N}(0, \\sigma_{m}^{2})$, $\\Delta$ is a linear transformation of a Gaussian random variable and is therefore also Gaussian. Its mean $\\mu_{\\Delta}$ and variance $\\sigma_{\\Delta}^{2}$ are:\n$$\\mu_{\\Delta} = \\mathbb{E}[\\Delta] = \\mathbb{E}\\left[m + \\frac{\\varepsilon}{R_{b}}\\right] = m + \\frac{\\mathbb{E}[\\varepsilon]}{R_{b}} = m + \\frac{0}{R_{b}} = m$$\n$$\\sigma_{\\Delta}^{2} = \\mathrm{Var}(\\Delta) = \\mathrm{Var}\\left(m + \\frac{\\varepsilon}{R_{b}}\\right) = \\frac{1}{R_{b}^{2}}\\mathrm{Var}(\\varepsilon) = \\frac{\\sigma_{m}^{2}}{R_{b}^{2}}$$\nThus, the distribution of $\\Delta$ is:\n$$\\Delta \\sim \\mathcal{N}\\left(m, \\left(\\frac{\\sigma_{m}}{R_{b}}\\right)^{2}\\right)$$\n\n**2. Express the under-polish probability $\\mathbb{P}(\\Delta  0)$**\n\nUnder-polish occurs if the applied polish time $t$ is less than the ideal clear time $t_{\\mathrm{clear}}$, which is equivalent to the condition $\\Delta  0$. We can compute this probability using the distribution of $\\Delta$ derived above. Let $\\Phi(z)$ be the cumulative distribution function (CDF) of the standard normal distribution $\\mathcal{N}(0,1)$.\n$$\\mathbb{P}(\\Delta  0) = \\mathbb{P}\\left(\\frac{\\Delta - \\mu_{\\Delta}}{\\sigma_{\\Delta}}  \\frac{0 - m}{\\sigma_{m}/R_{b}}\\right)$$\n$$\\mathbb{P}(\\Delta  0) = \\Phi\\left(-\\frac{m R_{b}}{\\sigma_{m}}\\right)$$\n\n**3. Determine the smallest $m$ that satisfies the risk constraint**\n\nThe problem specifies a risk constraint on the probability of under-polish:\n$$\\mathbb{P}(t  t_{\\mathrm{clear}}) \\le \\alpha \\implies \\mathbb{P}(\\Delta  0) \\le \\alpha$$\nSubstituting the expression for the probability:\n$$\\Phi\\left(-\\frac{m R_{b}}{\\sigma_{m}}\\right) \\le \\alpha$$\nSince the CDF $\\Phi$ is monotonically increasing, we can apply its inverse, the quantile function $\\Phi^{-1}$, to both sides:\n$$-\\frac{m R_{b}}{\\sigma_{m}} \\le \\Phi^{-1}(\\alpha)$$\nSolving for $m$ (and noting that $R_{b}  0, \\sigma_{m}  0$):\n$$m \\ge -\\frac{\\sigma_{m}}{R_{b}}\\Phi^{-1}(\\alpha)$$\nUsing the symmetry property of the standard normal distribution, $\\Phi^{-1}(\\alpha) = -\\Phi^{-1}(1-\\alpha)$, we get:\n$$m \\ge \\frac{\\sigma_{m}}{R_{b}}\\Phi^{-1}(1-\\alpha)$$\nThe objective is to minimize expected erosion, $\\mathbb{E}[E]$. As we will see, $\\mathbb{E}[E]$ is a monotonically increasing function of $m$. Therefore, to minimize $\\mathbb{E}[E]$ while satisfying the constraint, we must choose the smallest possible value for $m$. This occurs when the inequality becomes an equality. The optimal margin $m^{\\star}$ is thus:\n$$m^{\\star} = \\frac{\\sigma_{m}}{R_{b}}\\Phi^{-1}(1-\\alpha)$$\nSince $\\alpha \\in (0,1)$, for typical risk tolerances ($\\alpha  0.5$), we have $1-\\alpha  0.5$ and $\\Phi^{-1}(1-\\alpha)  0$, ensuring $m^{\\star} \\ge 0$ as required.\n\n**4. Compute the minimum expected erosion $\\mathbb{E}[E]$**\n\nThe erosion thickness $E$ is given by $E = R_{e} \\max\\{t - t_{\\mathrm{clear}}, 0\\}$, which can be written as:\n$$E = R_{e} \\max\\{\\Delta, 0\\}$$\nThe expected erosion is $\\mathbb{E}[E] = R_{e} \\mathbb{E}[\\max\\{\\Delta, 0\\}]$. This requires calculating the expectation of a rectified Gaussian random variable. For a general Gaussian random variable $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$, this expectation is a standard result:\n$$\\mathbb{E}[\\max\\{X, 0\\}] = \\mu \\Phi\\left(\\frac{\\mu}{\\sigma}\\right) + \\sigma \\phi\\left(\\frac{\\mu}{\\sigma}\\right)$$\nwhere $\\phi(z) = (2\\pi)^{-1/2} e^{-z^2/2}$ is the probability density function (PDF) of the standard normal distribution.\n\nIn our case, the random variable is $\\Delta \\sim \\mathcal{N}(\\mu_{\\Delta}, \\sigma_{\\Delta}^{2})$, with $\\mu_{\\Delta} = m^{\\star}$ and $\\sigma_{\\Delta} = \\sigma_{m}/R_{b}$. Let's define $z_{\\alpha} = \\Phi^{-1}(1-\\alpha)$.\nThe mean of $\\Delta$ under the optimal policy is $\\mu_{\\Delta} = m^{\\star} = \\sigma_{\\Delta} z_{\\alpha}$.\nThe ratio $\\mu_{\\Delta}/\\sigma_{\\Delta}$ is therefore simply $z_{\\alpha}$.\nSubstituting these into the formula for the expectation:\n$$\\mathbb{E}[\\max\\{\\Delta, 0\\}] = \\mu_{\\Delta} \\Phi(z_{\\alpha}) + \\sigma_{\\Delta} \\phi(z_{\\alpha})$$\nBy definition of the quantile function, $\\Phi(z_{\\alpha}) = \\Phi(\\Phi^{-1}(1-\\alpha)) = 1-\\alpha$.\n$$\\mathbb{E}[\\max\\{\\Delta, 0\\}] = m^{\\star}(1-\\alpha) + \\frac{\\sigma_{m}}{R_{b}}\\phi(z_{\\alpha})$$\nFinally, the minimum expected erosion, $\\mathbb{E}[E]_{\\mathrm{min}}$, is:\n$$\\mathbb{E}[E]_{\\mathrm{min}} = R_{e} \\left[ m^{\\star}(1-\\alpha) + \\frac{\\sigma_{m}}{R_{b}}\\phi(\\Phi^{-1}(1-\\alpha)) \\right]$$\nThese formulae for $m^{\\star}$ and $\\mathbb{E}[E]_{\\mathrm{min}}$ can be directly implemented to solve for the given test cases. Notice that the final expressions do not depend on $\\mu_{T}$ or $\\sigma_{T}$, as their influence on the polish time calculation cancels out when considering the over-polish duration $\\Delta$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Computes the optimal over-polish margin and resulting minimum expected erosion\n    for a series of CMP process scenarios.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (mu_T, sigma_T, sigma_m, R_b, R_e, alpha)\n    test_cases = [\n        (30.0, 3.0, 5.0, 1.0, 0.2, 0.05),\n        (15.0, 1.0, 2.0, 1.2, 0.1, 0.5),\n        (40.0, 5.0, 0.1, 1.5, 0.05, 0.01),\n        (25.0, 4.0, 10.0, 0.8, 0.25, 0.1),\n        (50.0, 8.0, 7.0, 0.5, 0.2, 1e-6)\n    ]\n\n    results = []\n    for case in test_cases:\n        # Unpack parameters for the current test case.\n        # mu_T and sigma_T are not required for the calculation as shown in the derivation.\n        _mu_T, _sigma_T, sigma_m, R_b, R_e, alpha = case\n\n        # 1. Calculate the (1-alpha) quantile of the standard normal distribution.\n        # This value is often denoted as z_alpha.\n        # z_alpha = Phi^{-1}(1 - alpha)\n        z_alpha = norm.ppf(1 - alpha)\n\n        # 2. Calculate the optimal over-polish margin m_star.\n        # m_star = (sigma_m / R_b) * z_alpha\n        m_star = (sigma_m / R_b) * z_alpha\n\n        # 3. Calculate the value of the standard normal PDF at z_alpha.\n        # phi_z_alpha = phi(z_alpha)\n        phi_z_alpha = norm.pdf(z_alpha)\n\n        # 4. Compute the minimum expected erosion E_E_min.\n        # E_E_min = R_e * [m_star * (1-alpha) + (sigma_m / R_b) * phi(z_alpha)]\n        term1 = m_star * (1 - alpha)\n        term2 = (sigma_m / R_b) * phi_z_alpha\n        E_E_min = R_e * (term1 + term2)\n        \n        # Round the results to six decimal places as required.\n        m_star_rounded = round(m_star, 6)\n        E_E_min_rounded = round(E_E_min, 6)\n        \n        results.append(f\"[{m_star_rounded:.6f},{E_E_min_rounded:.6f}]\")\n\n    # Final print statement in the exact required format.\n    # e.g., [[m*1,e1],[m*2,e2],...]\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}