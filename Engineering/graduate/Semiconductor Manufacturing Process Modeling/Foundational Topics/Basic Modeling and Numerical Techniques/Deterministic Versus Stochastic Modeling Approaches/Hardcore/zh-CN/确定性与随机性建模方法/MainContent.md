## 引言
在追求极致精度和可靠性的[半导体制造](@entry_id:187383)业中，数学模型是连接工艺配方与最终产品性能的关键桥梁。然而，在构建这些模型时，工程师和科学家面临一个根本性的抉择：是采用确定性方法，将所有变化归因于可控因素的变化；还是拥抱随机性方法，承认并量化系统中固有的、不可消除的波动？这一选择不仅是技术偏好问题，更直接决定了我们对工艺变异的理解深度、良率预测的准确性以及过程控制策略的有效性。错误地忽略随机性可能导致对风险的严重低估和控制方案的失效，而过度复杂的随机模型又可能难以辨识和应用。

本文旨在系统性地解决这一核心问题，为读者提供一个清晰的框架来理解、选择和应用[确定性与随机性](@entry_id:636235)建模方法。我们将通过三个循序渐进的章节，带领您深入这一领域。在“原理与机制”一章中，我们将剖析两种方法的基本哲学差异，探讨半导体工艺中变异的物理来源，并学习如何从随机的微观事件中推导出宏观的确定性规律。接下来的“应用与跨学科联系”一章将通过从原子层沉积到工厂调度的丰富案例，展示这些理论在不同尺度和领域中的实际威力。最后，“动手实践”部分将提供具体的计算和分析问题，帮助您将理论知识转化为解决实际问题的能力。

让我们首先进入第一章，从最基本的原理出发，探索[确定性与随机性](@entry_id:636235)这两种看待世界的不同视角。

## 原理与机制

在[半导体制造](@entry_id:187383)的复杂世界中，对工艺的精确建模是实现高产量和高可靠性的基石。一个核心的建模决策在于选择确定性方法还是随机性方法。本章将深入探讨这两种方法的原理与机制，阐明它们的基本区别，分析半导体工艺中变异的物理来源，并展示如何根据具体问题选择和应用合适的模型。我们将从基本定义出发，逐步过渡到高级建模技术和实际应用中的[模型选择](@entry_id:155601)策略。

### 基本区别：[确定性与随机性](@entry_id:636235)观点

工艺建模的根本目标是建立一个数学关系，将可控的输入（如工艺配方）和可测量的输出（如关键尺寸或薄膜厚度）联系起来。选择确定性模型还是随机性模型，反映了我们对系统内在可预测性的不同假设。

#### 确定性理想

一个**确定性模型 (deterministic model)** 假设，对于给定的输入和一组固定的模型参数，输出是唯一确定的。其数学形式可以抽象地表示为 $y = f(\mathbf{x}, \boldsymbol{\theta})$，其中 $\mathbf{x}$ 是输入向量，$\boldsymbol{\theta}$ 是参数向量，而 $y$ 是输出。在这种观点下，任何观测到的变化都被归因于输入 $\mathbf{x}$ 的变化。如果能够精确地重复输入条件，那么输出也应该是完全可重复的。例如，一个理想的[化学反应速率](@entry_id:147315)定律，只要温度、压力和反应物浓度完全相同，[反应速率](@entry_id:185114)就应该是一个确定的值。确定性模型的核心是因果关系的唯一性和可预测性。

#### 随机性现实

与此相对，**随机性模型 (stochastic model)** 承认系统内在的、不可消除的随机性。在这种模型中，即使输入 $\mathbf{x}$ 和参数 $\boldsymbol{\theta}$ 完全固定，输出 $Y$ 仍然是一个[随机变量](@entry_id:195330)，遵循某个概率分布。这意味着，在完全相同的设定下重复实验，我们会得到一系列不同的输出值，这些值构成了一个统计总体。随机性模型的目标不是预测一个单一的输出值，而是描述输出值的概率分布特征，如期望、方差和相关性。例如，[光刻胶](@entry_id:159022)中的光子吸收过程，即使入射[光强度](@entry_id:177094)恒定，到达特定微小区域的光子数在每次曝光中也会随机波动。这种随机性源于量子物理的内在属性。

#### 统一框架：[偶然不确定性与认知不确定性](@entry_id:1120923)

为了更深刻地理解这两种建模方法的适用场景，引入**[偶然不确定性](@entry_id:634772) (aleatory uncertainty)** 和**认知不确定性 (epistemic uncertainty)** 的概念至关重要。

- **[偶然不确定性](@entry_id:634772)** 是系统固有的、不可简化的随机性。它源于物理过程内在的随机波动，即使我们拥有关于系统的完美知识，这种不确定性依然存在。例如，等离子体中离子的热运动、测量设备中的[电子噪声](@entry_id:894877)、或[材料微观结构](@entry_id:198422)的不均匀性都属于[偶然不确定性](@entry_id:634772)。随机性模型的主要目标就是量化这种[偶然不确定性](@entry_id:634772)。

- **认知不确定性** 是由于我们对系统缺乏完整知识而产生的不确定性。这包括对模型结构是否正确的疑虑，以及对模型参数 $\boldsymbol{\theta}$ 真值的无知。与[偶然不确定性](@entry_id:634772)不同，认知不确定性可以通过收集更多数据、进行更精确的实验或改进模型来降低。例如，通过多次刻蚀实验，我们可以更精确地估计刻蚀速率模型中的反应活化能参数，从而减少关于该参数的认知不确定性。

在实践中，一个完整的模型通常需要同时处理这两种不确定性。例如，在一个贝叶斯框架中，我们可以用一个概率分布来描述模型参数 $\boldsymbol{\theta}$ 的认知不确定性（即后验分布），同时模型本身包含一个随机项来描述[偶然不确定性](@entry_id:634772)（如工艺噪声和测量噪声）。通过精心设计的重复实验，我们甚至可以经验性地将这两种不确定性分离开来。

### [半导体制造](@entry_id:187383)中的变异来源与表现

半导体工艺中的变异来源多种多样，正确地将其归类为确定性趋势或随机性波动，是建立有效模型的第一步。

#### 系统性与确定性变异

某些变异来源尽管随时间或空间变化，但其演化规律是可预测的，因此适合用确定性模型来描述。

- **设备漂移 (Equipment Drift)**：随着设备长时间运行，其部件会发生磨损、腔室壁会沉积副产物，导致工艺性能发生缓慢、单向的变化。这种漂移通常在多次生产运行之间持续存在，并且其趋势（例如，单调递增或递减）在一定程度上是可预测的。因此，设备状况可以被建模为一个确定性的[状态变量](@entry_id:138790) $\theta(t)$，其演化遵循一个动态方程，例如 $\dot{\theta}(t) = -\alpha \theta(t) + b$。这里，我们对参数 $\alpha$ 和 $b$ 的不确定性是认知性的，但状态 $\theta(t)$ 本身的演化被假定为确定性的。

- **晶圆内空间梯度 (Spatial Gradients)**：由于气体流动、等离子体分布或温度场的不均匀性，晶圆上的工艺结果（如刻蚀速率）常常表现出系统性的空间分布模式，例如中心快、边缘慢。这种模式可以在所有晶圆上重复出现，因此可以被建模为一个关于空间坐标 $\mathbf{s}$ 的确定性函数，例如坐标的多项式函数 $\mathbf{x}(\mathbf{s})^\top \boldsymbol{\beta}$。

#### 内在随机性变异（[偶然不确定性](@entry_id:634772)）

许多变异来源本质上是随机的，无法通过简单的确定性函数来预测。

- **材料[异质性](@entry_id:275678) (Material Heterogeneity)**：对于单片晶圆，其微观结构是固定的。但从整个生产过程的角度看，我们无法预知下一片晶圆具体的微观属性分布。晶圆与晶圆之间的这种不可预测的差异，使得材料[异质性](@entry_id:275678)成为一个随机因素。在建模时，它通常被处理为一个**随机场 (random field)** $H(\mathbf{r})$，其[空间相关性](@entry_id:203497)结构描述了晶圆上属性变化的典型模式。

- **环境波动 (Environmental Fluctuations)**：尽管无尘室对温度、湿度和振动有严格的控制，但这种控制并非完美。未被完全调节的微[小波](@entry_id:636492)动，其具体的时间轨迹在每次运行中都不同，因此是随机的。这些波动通常表现出时间上的相关性（例如，温度变化是一个连续的过程），适合用具有时间自相关性的[随机过程](@entry_id:268487)来建模。

- **计量噪声 (Metrology Noise)**：任何测量仪器在重复测量同一个稳定物体时，读数都会有微小的变化。这种变化是典型的随机测量误差。最简单的模型是将其视为一个零均值、与其它变量独立的加性**[白噪声](@entry_id:145248) (white noise)** $\eta(t, \mathbf{r})$。

- **离散事件随机性 (Discrete Event Stochasticity)**：许多半导体工艺依赖于离散粒子的行为，例如离子的注入、光子的吸收。由于这些事件在时间和空间上的到达是随机的，它们引入了所谓的**[散粒噪声](@entry_id:140025) (shot noise)**。
    - 在[离子注入](@entry_id:160493)中，离子的到达可以被建模为一个**更新过程 (renewal process)**，其中离子到达的间隔时间是[独立同分布](@entry_id:169067)的[随机变量](@entry_id:195330)。
    - 在[光刻](@entry_id:158096)工艺中，光子到达[光刻胶](@entry_id:159022)表面的过程通常遵循**泊松过程 (Poisson process)**，这意味着在给定区域和时间内的光子数量是一个泊松[随机变量](@entry_id:195330)。 

### 从随机基础到确定性行为的涌现

尽管微观层面充满了随机性，但宏观尺度上的许多工艺行为却能被确定性模型很好地描述。这种从随机到确定的过渡，是统计物理中**大数定律 (Law of Large Numbers)** 的一个体现。其核心思想是，当大量独立的随机事件被平均时，相对波动会减小，系统的行为会趋向于其[期望值](@entry_id:150961)。

#### 大数定律与平均效应

考虑一个[随机过程](@entry_id:268487)，其事件发生的数量或强度随时间或空间累积。虽然瞬时的值是随机的，但长期或大范围的平均行为会变得非常稳定和可预测。

- **案例：[离子注入](@entry_id:160493)通量**：在[离子注入](@entry_id:160493)中，到达晶圆的总离子数 $N(t)$ 是一个[随机过程](@entry_id:268487)。其[期望值](@entry_id:150961)由平均[到达率](@entry_id:271803) $1/\mu$ 决定，即 $\mathbb{E}[N(t)] \approx t/\mu$。其方差则由[到达间隔时间](@entry_id:271977)的均值 $\mu$ 和方差 $\sigma^2$ 共同决定，渐近表达式为 $\operatorname{Var}[N(t)] \approx \frac{\sigma^2 t}{\mu^3}$。衡量相对波动的指标是相对均方波动 $R(t) = \frac{\operatorname{Var}[N(t)]}{(\mathbb{E}[N(t)])^2}$。我们可以推导出 $R(t) \approx \frac{\sigma^2}{\mu t}$。 这个结果表明，随着时间 $t$ 的增长，相对波动 $R(t)$ 趋向于零。这意味着，对于足够长的注入时间，随机的离子[到达过程](@entry_id:263434)可以被一个确定性的平均通量 $1/\mu$ 非常精确地近似。

- **案例：光刻曝光剂量**：在[光刻](@entry_id:158096)中，到达单位面积的光子数 $N$ 服从[泊松分布](@entry_id:147769)，其均值为 $\lambda$。$\lambda$ 代表曝光剂量。物理[化学[反应速](@entry_id:147315)率](@entry_id:185114) $R$ 依赖于实际接收到的光子数，是一个[随机变量](@entry_id:195330)。然而，一个物理上合理的模型应该依赖于光子密度，而不是绝对数量。因此，[速率函数](@entry_id:154177)应表示为归一化光子数 $N/\lambda$ 的函数，即 $R(N/\lambda)$。根据[大数定律](@entry_id:140915)，当剂量 $\lambda \to \infty$ 时，归一化的光子数 $N/\lambda$ 在概率上收敛于 $1$。根据**[连续映射定理](@entry_id:269346) (Continuous Mapping Theorem)**，如果[速率函数](@entry_id:154177) $R(\cdot)$ 是连续的，那么随机速率 $R(N/\lambda)$ 将在概率上收敛于确定性速率 $R(1)$。 这解释了为什么在高剂量下，我们可以忽略[光子散粒噪声](@entry_id:1129630)，使用一个确定性的速率模型。

#### [泊松成品率模型](@entry_id:1129894)：一个经典案例

[半导体制造](@entry_id:187383)中的成品率（yield）建模是随机性思想应用的一个经典例子。假设芯片上的致命缺陷是随机、独立且均匀地分布在晶圆表面上，其平均密度为 $D_0$。一个芯片的面积为 $A$，那么该芯片上的平均缺陷数是 $\lambda = D_0 A$。

一个纯粹的确定性观点可能会得出错误的结论：“如果平均缺陷数 $D_0 A  1$，那么成品率为 $100\%$；如果 $D_0 A \ge 1$，则成品率为 $0\%$。” 这种观点忽略了缺陷分布的随机性。

一个更真实的随机性模型是，将芯片划分为 $n$ 个极小的子区域。每个子区域有缺陷的概率很小，约为 $p_n = D_0 A / n$。整个芯片无缺陷的概率，即成品率 $Y$，是所有子区域都无缺陷的概率。由于各子区域独立，所以 $Y \approx (1 - p_n)^n = (1 - D_0 A / n)^n$。在极限情况下，当 $n \to \infty$ 时，我们得到经典的**[泊松成品率模型](@entry_id:1129894) (Poisson yield model)**：
$$
Y = \exp(-D_0 A)
$$
这个模型正确地刻画了成品率随芯片面积和缺陷密度的指数衰减关系，它本身是一个确定性公式，但其推导过程完全基于随机性假设。这完美地展示了如何从底层的随机事件（缺陷的随机放置）推导出宏观的、确定性的预测规律。

### 随机性的建模与分解

当随机性不可忽略时，我们需要更复杂的模型来描述和分解其不同来源。

#### [复合过程](@entry_id:1130720)：随机性的级联

在许多工艺中，一个[随机过程](@entry_id:268487)的输出会成为下一个[随机过程](@entry_id:268487)的输入，形成随机性的级联。例如，在[化学放大光刻胶](@entry_id:192110)中，光子吸收过程（第一阶段随机性）产生光酸（PAG），而每个光酸的产生过程本身也具有随机性（第二阶段随机性）。

假设到达[光刻胶](@entry_id:159022)体积的吸收光子数 $N_{\mathrm{abs}}$ 是一个泊松[随机变量](@entry_id:195330)，均值为 $\lambda_{\mathrm{abs}}$。每个被吸收的光子产生的PAG分子数 $X_i$ 也是一个[随机变量](@entry_id:195330)（例如，[泊松分布](@entry_id:147769)），均值为[量子产率](@entry_id:148822) $\eta$。那么，生成的总PAG分子数 $S = \sum_{i=1}^{N_{\mathrm{abs}}} X_i$ 是一个**复合[随机变量](@entry_id:195330) (compound random variable)**。

要计算其总方差，我们可以使用**[全方差公式](@entry_id:177482) (Law of Total Variance)**：
$$
\operatorname{Var}(S) = \mathbb{E}[\operatorname{Var}(S|N_{\mathrm{abs}})] + \operatorname{Var}(\mathbb{E}[S|N_{\mathrm{abs}}])
$$
- 第一项 $\mathbb{E}[\operatorname{Var}(S|N_{\mathrm{abs}})]$ 代表由化学随机性（每个光子产生PAG数量的波动）贡献的方差，在所有可能的光子数上取平均。
- 第二项 $\operatorname{Var}(\mathbb{E}[S|N_{\mathrm{abs}}])$ 代表由初始[光子散粒噪声](@entry_id:1129630)（$N_{\mathrm{abs}}$ 的波动）传播而来的方差。

通过计算，可以得到 $\operatorname{Var}(S) = (\eta + \eta^2)\lambda_{\mathrm{abs}}$。这清楚地表明，总噪声是两个来源之和：一个与 $\eta$ 成正比（[化学噪声](@entry_id:196777)），一个与 $\eta^2$ 成正比（传播的光子噪声）。忽略任何一个来源都会低估系统的总变异。

#### 分层模型：结构化变异的分解

当变异表现出多层次结构时，例如晶圆内和晶圆间的变异，**[线性混合效应模型](@entry_id:917842) (linear mixed-effects models)** 提供了一个强大的框架来分解它们。

假设我们测量了来自 $W$ 个晶圆的数据，每个晶圆上有多个测量点。观测到的响应 $y_{wi}$ (晶圆 $w$ 上的第 $i$ 个点) 可以被分解为：
$$
y_{wi} = \mathbf{x}(\mathbf{s}_{wi})^\top \boldsymbol{\beta} + a_w + \delta_w(\mathbf{s}_{wi}) + \varepsilon_{wi}
$$
- $\mathbf{x}(\mathbf{s}_{wi})^\top \boldsymbol{\beta}$ 是**固定效应 (fixed effect)**，代表所有晶圆共享的、确定性的空间趋势（如前述的空间梯度）。
- $a_w$ 是晶圆层面的**[随机效应](@entry_id:915431) (random effect)**，代表晶圆与晶圆之间不可预测的平均偏移。通常假设 $a_w \sim \mathcal{N}(0, \sigma_a^2)$ 且[相互独立](@entry_id:273670)。
- $\delta_w(\mathbf{s}_{wi})$ 是晶圆内的空间[随机效应](@entry_id:915431)，代表每个晶圆上独特的、具有[空间相关性](@entry_id:203497)的随机场。
- $\varepsilon_{wi}$ 是纯粹的、不相关的测量误差。

这种分层模型能够同时捕捉确定性趋势和多源的、结构化的随机变异。为了使模型参数能够被唯一地从数据中估计出来（即**可识别性 (identifiability)**），需要满足一些条件，例如：有足够多的晶圆来估计晶圆间方差 $\sigma_a^2$，每个晶圆上有足够多的测量点来估计[空间相关性](@entry_id:203497)结构。

#### 忽略随机性的后果：有偏推断

在建模时，错误地将一个[随机变量](@entry_id:195330)当作确定性量来处理，可能会导致系统性的推断偏差。一个典型的例子是[回归分析](@entry_id:165476)中的**变量误差 (Errors-in-Variables, EIV)** 问题。

假设我们想建立真实输入 $x$ 和输出 $y$ 之间的线性关系 $y = \beta_0 + \beta_1 x + q$，其中 $q$ 是工艺噪声。然而，在实际测量中，我们只能观测到带有测量误差的输入 $w = x + r$。如果我们忽略这个测量误差，直接用[普通最小二乘法](@entry_id:137121)（OLS）回归 $y$ 对 $w$ 进行拟合，得到的斜率估计 $\hat{\beta}_1$ 将是有偏的。

可以证明，在样本量很大时，$\hat{\beta}_1$ 的[期望值](@entry_id:150961)会收敛到：
$$
\lim_{n \to \infty} \mathbb{E}[\hat{\beta}_{1}] = \beta_1 \frac{\sigma_X^2}{\sigma_X^2 + \sigma_R^2}
$$
其中 $\sigma_X^2$ 是真实输入 $x$ 的方差，$\sigma_R^2$ 是测量误差 $r$ 的方差。其偏差为：
$$
\text{Bias} = -\beta_1 \frac{\sigma_R^2}{\sigma_X^2 + \sigma_R^2}
$$
这个偏差总是使得估计的斜率的绝对值小于真实斜率的绝对值，这种现象被称为**[衰减偏误](@entry_id:912170) (attenuation bias)**。有趣的是，响应变量 $y$ 中的随机性 $q$ 并不会导致斜率估计产生偏差，它只会增加拟合的残差方差。这个例子深刻地说明了，在建模时审慎处理所有变量的不确定性是多么重要。

### 实践中的[模型选择](@entry_id:155601)：平衡复杂性与拟合度

在确定性模型和随机性模型之间，或者在不同复杂度的随机模型之间，我们如何做出选择？一个过于简单的模型可能无法捕捉重要的系统行为（[欠拟合](@entry_id:634904)），而一个过于复杂的模型可能会拟[合数](@entry_id:263553)据中的噪声，导致预测能力下降（过拟合）。**信息准则 (information criteria)** 为我们提供了一种在[模型拟合](@entry_id:265652)度和复杂性之间进行权衡的系统性方法。

#### 信息准则：AIC 与 BIC

最常用的信息准则是**赤池信息准则 (Akaike Information Criterion, AIC)** 和**贝叶斯信息准则 (Bayesian Information Criterion, BIC)**。它们的通用形式是：
$$
\text{IC} = -2 \ln(\hat{L}) + \text{penalty}(k, n)
$$
其中 $\hat{L}$ 是模型的最大似然值，反映了模型对数据的拟合优度；$k$ 是模型中自由参数的数量，代表模型的复杂性；$n$ 是样本量。惩罚项 $\text{penalty}(k, n)$ 随着模型复杂度的增加而增加。
- AIC 的惩罚项是 $2k$。
- BIC 的惩罚项是 $k \ln(n)$。

当比较一系列候选模型时，我们倾向于选择信息准则值（AIC 或 BIC）最小的模型。由于 $\ln(n)$ 在 $n \ge 8$ 时大于 $2$，BIC 对模型复杂度的惩罚通常比 AIC 更重，因此倾向于选择更简洁的模型。

#### 应用：工具漂移建模

考虑一个工具校准偏移量的[时间序列数据](@entry_id:262935)。我们面临两种可能的解释：一种是偏移量随时间发生确定性的线性漂移，外加一些测量噪声；另一种是偏移量遵循一个[随机游走过程](@entry_id:171699)，即每次的变化都是随机的。
- **确定性线性漂移模型**：$x_t = x_0 + \alpha t + \varepsilon_t$。这是一个有3个参数（$x_0, \alpha, \sigma^2$）的模型。
- **[随机游走模型](@entry_id:180803)**：$x_t = x_{t-1} + \epsilon_t$。这是一个只有1个参数（$\sigma^2$）的模型。

通过对同一组数据分别拟合这两个模型，并计算它们的AI[C值](@entry_id:272975)，我们可以客观地判断哪种模型提供了更好的解释。如果线性趋势明显，确定性模型将具有更好的拟合度（更高的 $\hat{L}$），足以抵消其更多的参数惩罚。反之，如果数据更像无规则的随机波动，更简洁的[随机游走模型](@entry_id:180803)将胜出。

#### 应用：环境波动建模

类似地，对于无尘室中的温度波动数据，我们可能想知道这些波动是由HVAC系统引入的确定性周期性模式，还是由更复杂的、[自相关](@entry_id:138991)的随机噪声主导。
- **确定性周期模型**：可以将温度建模为一个正弦函数，外加[白噪声](@entry_id:145248)。$y_t = \mu + A \sin(2\pi f t + \phi) + \eta_t$。
- **随机[噪声模型](@entry_id:752540)**：可以将其建模为一个[自回归移动平均](@entry_id:143076)（ARMA）过程，例如ARMA(1,1)：$y_t = c + \phi y_{t-1} + \epsilon_t + \theta \epsilon_{t-1}$。

通过拟合这两个模型并比较它们的BIC值，我们可以做出决策。如果数据中存在强烈的、稳定的周期性信号，正弦模型的拟合度会非常高，其BIC值可能会更低。如果数据表现为随机但持续的波动（即今天的温度与昨天的相关），[ARMA模型](@entry_id:139294)可能提供更简洁且同样有效的描述。

总之，[确定性与随机性](@entry_id:636235)建模并非相互排斥，而是相辅相成的两种观点。理解它们的基本原理、识别工艺中变异的来源、并掌握在它们之间进行选择的量化工具，是任何高级工艺建模工程师必备的核心技能。