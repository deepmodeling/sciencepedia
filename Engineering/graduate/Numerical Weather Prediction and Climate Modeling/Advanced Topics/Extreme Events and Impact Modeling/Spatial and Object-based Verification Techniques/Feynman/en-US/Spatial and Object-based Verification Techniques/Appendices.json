{
    "hands_on_practices": [
        {
            "introduction": "The first step in any object-based verification (OBV) scheme is to translate raw, gridded model output into a set of discrete, meaningful objects. This exercise  provides a complete, hands-on walkthrough of this fundamental process. You will start with gridded intensity fields, apply a threshold to identify regions of interest, use a connectivity algorithm to define distinct objects, and then compute a set of physically relevant attributes—such as area, eccentricity, and maximum intensity—that form the basis for quantitatively comparing forecast and observed phenomena.",
            "id": "4090773",
            "problem": "You are given a synthetic setup for evaluating spatial and object-based verification in Numerical Weather Prediction (NWP) and climate modeling. Specifically, you will compute attributes for binary-thresholded connected objects in two-dimensional gridded fields and then perform optimal matching between forecast and observed objects using a rigorously defined cost function and the Linear Sum Assignment (LSA) criterion.\n\nFundamental base:\n- Object-based verification (OBV) operates on thresholded binary fields defined from gridded intensities and partitions the field into connected components under a specified neighborhood rule. Attributes such as area, shape descriptors, and intensity statistics are computed per object.\n- For a two-dimensional (2D) gridded field, object boundaries arise from a threshold $T$ applied to intensities $I_{r,c}$ on a discrete grid indexed by row $r$ and column $c$.\n- Connectivity is defined via $8$-connectivity: two grid cells are neighbors if their row indices differ by at most $1$ and their column indices differ by at most $1$, including diagonal adjacency.\n\nDefinitions to be used:\n- Thresholding: For each grid cell $(r,c)$ with intensity $I_{r,c}$, define a binary mask $M_{r,c} = 1$ if $I_{r,c} \\ge T$ and $M_{r,c} = 0$ otherwise.\n- Connected object: A maximal set of cells with $M_{r,c} = 1$ that are connected under $8$-connectivity.\n- Area: The area of an object is the count of its grid cells. Express area as an integer count of grid cells.\n- Maximum intensity: The maximum of $I_{r,c}$ over the cells in the object. Express this as a real number.\n- Eccentricity: For an object with $n$ cells at coordinates $(x_i,y_i)$ where $x_i$ is the column index and $y_i$ is the row index, compute the centroid $(\\bar{x},\\bar{y})$ with\n$$\n\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n} x_i, \\quad \\bar{y} = \\frac{1}{n}\\sum_{i=1}^{n} y_i.\n$$\nDefine the second central moment (covariance) matrix\n$$\n\\mathbf{C} = \\begin{pmatrix}\n\\frac{1}{n}\\sum_{i=1}^{n}(x_i-\\bar{x})^2 & \\frac{1}{n}\\sum_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})\\\\\n\\frac{1}{n}\\sum_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y}) & \\frac{1}{n}\\sum_{i=1}^{n}(y_i-\\bar{y})^2\n\\end{pmatrix}.\n$$\nLet $\\lambda_{\\max}$ and $\\lambda_{\\min}$ be the eigenvalues of $\\mathbf{C}$ with $\\lambda_{\\max} \\ge \\lambda_{\\min} \\ge 0$. Define the object eccentricity\n$$\ne = \\sqrt{1 - \\frac{\\lambda_{\\min}}{\\lambda_{\\max}}}\n$$\nfor $\\lambda_{\\max} > 0$, and set $e = 0$ if $\\lambda_{\\max} = 0$. Express eccentricity as a real number in $[0,1]$.\n\nPairwise attribute-difference normalization:\n- Let forecast object attributes be $(A_f, e_f, M_f)$ and observed object attributes be $(A_o, e_o, M_o)$. Define normalized attribute differences\n$$\nd_A = \\frac{|A_f - A_o|}{\\max(A_f, A_o)}, \\quad d_E = |e_f - e_o|, \\quad d_M = \\frac{|M_f - M_o|}{\\max(M_f, M_o)}.\n$$\nAll differences are dimensionless and bounded in $[0,1]$.\n\nMatching cost function:\n- Given weights $(w_A, w_E, w_M)$, define the pairwise cost\n$$\nc = w_A d_A + w_E d_E + w_M d_M.\n$$\n- For a rectangular forecast-versus-observed pairing with $N_f$ forecast objects and $N_o$ observed objects, the optimal assignment minimizes the sum of selected pair costs over a one-to-one matching of size $\\min(N_f, N_o)$, plus a penalty $p_u$ for each unmatched forecast object:\n$$\nJ = \\sum_{(i,j)\\in \\mathcal{M}} c_{ij} + p_u \\cdot (N_f - |\\mathcal{M}|),\n$$\nwhere $\\mathcal{M}$ is the set of matched index pairs and $|\\mathcal{M}| = \\min(N_f, N_o)$.\n\nData specification:\n- Grid dimension is $8 \\times 8$; indices $r$ and $c$ run from $0$ to $7$.\n- Forecast field intensities $I^{(f)}_{r,c}$: all unspecified cells have intensity $0$. The nonzero intensities are at the following coordinates, with each pair listed as $((r,c), I)$:\n    - $((1,1), 2.0)$, $((1,2), 1.6)$, $((2,1), 1.4)$, $((2,2), 1.2)$,\n    - $((3,4), 1.1)$, $((3,5), 1.4)$, $((3,6), 1.6)$, $((4,4), 1.0)$, $((4,5), 1.2)$, $((4,6), 1.3)$,\n    - $((5,1), 0.95)$, $((5,2), 1.0)$, $((6,1), 1.2)$, $((6,2), 1.3)$, $((7,1), 2.2)$, $((7,2), 2.0)$.\n- Observed field intensities $I^{(o)}_{r,c}$: all unspecified cells have intensity $0$. The nonzero intensities are at the following coordinates:\n    - $((3,2), 1.0)$, $((3,3), 1.3)$, $((3,4), 1.5)$, $((4,2), 0.95)$, $((4,3), 1.1)$, $((4,4), 1.2)$,\n    - $((4,6), 1.1)$, $((4,7), 1.2)$, $((5,6), 1.4)$, $((5,7), 1.6)$, $((6,6), 1.7)$, $((6,7), 1.8)$.\n- Threshold is fixed at $T = 1.0$ for all test cases.\n\nConnectivity rule:\n- Use $8$-connectivity to identify connected components in the thresholded binary masks.\n\nTest suite:\nFor each case, compute forecast and observed objects, derive $(A,e,M)$ for each object, form the pairwise cost matrix using the given weights, solve the optimal assignment, and compute the total objective $J$ including unmatched penalty. Return $J$ rounded to $4$ decimal places as a float.\n- Case $1$: $(w_A, w_E, w_M) = (1.0, 1.0, 1.0)$, $p_u = 0.25$.\n- Case $2$: $(w_A, w_E, w_M) = (0.5, 2.0, 1.0)$, $p_u = 0.25$.\n- Case $3$: $(w_A, w_E, w_M) = (1.5, 0.0, 1.0)$, $p_u = 0.40$.\n- Case $4$: $(w_A, w_E, w_M) = (0.2, 1.0, 3.0)$, $p_u = 0.10$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results for Cases $1$ through $4$ as a comma-separated list enclosed in square brackets, with each float rounded to $4$ decimal places (for example, $[j_1,j_2,j_3,j_4]$).",
            "solution": "The problem is assessed to be valid. It is scientifically grounded in the principles of numerical weather prediction verification, algorithmically well-posed, internally consistent, and contains all necessary information to proceed to a unique solution.\n\nThe solution is developed through a sequence of four main steps:\n1.  Object Identification: Isolate connected objects in the forecast and observed fields based on the given intensity threshold.\n2.  Attribute Calculation: Compute the specified attributes (Area, Maximum Intensity, Eccentricity) for each identified object.\n3.  Cost Matrix Construction: For each test case, calculate the pairwise cost between every forecast object and every observed object using the case-specific weights.\n4.  Optimal Assignment and Total Cost: Solve the linear sum assignment problem to find the optimal matching and compute the total objective function $J$, including penalties for unmatched forecast objects.\n\nAll mathematical entities are typeset in LaTeX as per the requirements.\n\n**Step 1: Object Identification**\n\nFirst, the $8 \\times 8$ forecast ($I^{(f)}$) and observed ($I^{(o)}$) intensity grids are populated. A binary mask is then generated for each grid by identifying cells where the intensity $I_{r,c}$ is greater than or equal to the threshold $T=1.0$.\n\n$M_{r,c} = 1 \\text{ if } I_{r,c} \\ge T, \\text{ and } M_{r,c} = 0 \\text{ otherwise.}$\n\nUsing an $8$-connectivity connected components labeling algorithm (e.g., Breadth-First Search on the binary masks), the objects are identified.\n\nFor the **forecast field**, the following cells are above the threshold:\n- $\\{(1,1), (1,2), (2,1), (2,2)\\}$\n- $\\{(3,4), (3,5), (3,6), (4,4), (4,5), (4,6)\\}$\n- $\\{(5,2), (6,1), (6,2), (7,1), (7,2)\\}$\n\nThis process yields $N_f = 3$ distinct forecast objects:\n- $F_1$: A $2 \\times 2$ block of cells at the top-left.\n- $F_2$: A set of $6$ cells in the middle of the grid.\n- $F_3$: A group of $5$ cells in the bottom-left.\n\nFor the **observed field**, the following cells are above the threshold:\n- $\\{(3,2), (3,3), (3,4), (4,3), (4,4)\\}$\n- $\\{(4,6), (4,7), (5,6), (5,7), (6,6), (6,7)\\}$\n\nThis process yields $N_o = 2$ distinct observed objects:\n- $O_1$: A group of $5$ cells.\n- $O_2$: A $3 \\times 2$ block of $6$ cells.\n\n**Step 2: Attribute Calculation**\n\nFor each of the $N_f=3$ forecast objects and $N_o=2$ observed objects, we compute a set of three attributes: Area ($A$), Maximum Intensity ($M$), and Eccentricity ($e$). The coordinates $(x,y)$ are defined as (column index, row index).\n\n**Example Calculation for Forecast Object $F_2$:**\n- **Cells**: $\\{(3,4), (3,5), (3,6), (4,4), (4,5), (4,6)\\}$\n- **Area ($A_{F2}$)**: The object consists of $n=6$ cells, so $A_{F2} = 6$.\n- **Maximum Intensity ($M_{F2}$)**: The intensities are $\\{1.1, 1.4, 1.6, 1.0, 1.2, 1.3\\}$. The maximum is $M_{F2} = 1.6$.\n- **Eccentricity ($e_{F2}$)**:\n    - Coordinates $(x_i, y_i)$: $\\{(4,3), (5,3), (6,3), (4,4), (5,4), (6,4)\\}$.\n    - Centroid: $(\\bar{x}, \\bar{y}) = (\\frac{1}{6}\\sum x_i, \\frac{1}{6}\\sum y_i) = (5.0, 3.5)$.\n    - Covariance Matrix $\\mathbf{C}$:\n    $$ \\mathbf{C} = \\begin{pmatrix} \\frac{1}{n}\\sum(x_i-\\bar{x})^2 & \\frac{1}{n}\\sum(x_i-\\bar{x})(y_i-\\bar{y})\\\\ \\frac{1}{n}\\sum(x_i-\\bar{x})(y_i-\\bar{y}) & \\frac{1}{n}\\sum(y_i-\\bar{y})^2 \\end{pmatrix} = \\begin{pmatrix} 0.666... & 0.0 \\\\ 0.0 & 0.25 \\end{pmatrix} $$\n    - Eigenvalues: The eigenvalues of this diagonal matrix are $\\lambda_{\\max} = 2/3$ and $\\lambda_{\\min} = 0.25$.\n    - Eccentricity: $e_{F2} = \\sqrt{1 - \\frac{\\lambda_{\\min}}{\\lambda_{\\max}}} = \\sqrt{1 - \\frac{0.25}{2/3}} = \\sqrt{1 - 0.375} = \\sqrt{0.625} \\approx 0.79057$.\n    \nFollowing this procedure for all objects yields the following attributes:\n\n| Object | Area ($A$) | Max Intensity ($M$) | Eccentricity ($e$) |\n|:------:|:----------:|:-------------------:|:------------------:|\n| $F_1$  |     $4$    |        $2.0$        |     $0.0$          |\n| $F_2$  |     $6$    |        $1.6$        |  $\\approx 0.79057$ |\n| $F_3$  |     $5$    |        $2.2$        |  $\\approx 0.81650$ |\n| $O_1$  |     $5$    |        $1.5$        |  $\\approx 0.81650$ |\n| $O_2$  |     $6$    |        $1.8$        |  $\\approx 0.79057$ |\n\n**Step 3: Cost Matrix and Optimal Matching**\n\nFor each test case, we construct a $3 \\times 2$ cost matrix where entry $c_{ij}$ is the cost of matching forecast object $F_i$ with observed object $O_j$. The cost is $c_{ij} = w_A d_A + w_E d_E + w_M d_M$.\n\n**Example for Case 1**: $(w_A, w_E, w_M) = (1.0, 1.0, 1.0)$.\nLet's compute the cost $c_{22}$ for matching $F_2$ with $O_2$.\n- Attributes: $F_2(A=6, M=1.6, e\\approx0.79057)$ and $O_2(A=6, M=1.8, e\\approx0.79057)$.\n- $d_A = \\frac{|6-6|}{\\max(6,6)} = 0$.\n- $d_E = |0.79057 - 0.79057| = 0$.\n- $d_M = \\frac{|1.6-1.8|}{\\max(1.6, 1.8)} = \\frac{0.2}{1.8} \\approx 0.11111$.\n- $c_{22} = 1.0 \\cdot (0) + 1.0 \\cdot (0) + 1.0 \\cdot (0.11111) = 0.11111$.\n\nFilling the entire cost matrix for Case 1:\n$$ \\mathbf{C}_{\\text{cost}} = \\begin{pmatrix}\n1.2665 & 1.2239 \\\\\n0.2551 & 0.1111 \\\\\n0.3182 & 0.3744\n\\end{pmatrix} $$\nWe apply the linear sum assignment algorithm to this matrix. Since $N_f > N_o$, the algorithm finds the best matching for the $N_o=2$ observed objects. The optimal assignment for Case $1$ is to match $(F_2, O_2)$ and $(F_3, O_1)$, leaving $F_1$ unmatched.\n\n**Step 4: Total Objective Calculation**\n\nThe total objective function is $J = \\sum_{(i,j)\\in \\mathcal{M}} c_{ij} + p_u \\cdot (N_f - |\\mathcal{M}|)$, where $\\mathcal{M}$ is the set of optimal-cost pairs.\n\n**Example for Case 1**:\n- Weights: $(w_A, w_E, w_M) = (1.0, 1.0, 1.0)$.\n- Penalty: $p_u = 0.25$.\n- Optimal pairs: $(F_2, O_2)$ with cost $c_{22} \\approx 0.1111$ and $(F_3, O_1)$ with cost $c_{31} \\approx 0.3182$.\n- Sum of matched costs: $0.11111 + 0.31818 \\approx 0.4293$.\n- Unmatched forecast objects: $N_f - |\\mathcal{M}| = 3 - 2 = 1$.\n- Penalty cost: $p_u \\cdot 1 = 0.25$.\n- Total objective: $J_1 = 0.4293 + 0.25 = 0.6793$.\n\nThis entire process is repeated for the four test cases, each with different weights and penalties, to produce the final vector of results.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import linear_sum_assignment\n\ndef solve():\n    \"\"\"\n    Computes an object-based verification metric for synthetic forecast and observation fields.\n    \n    The process involves:\n    1. Identifying connected objects in both fields via thresholding.\n    2. Calculating area, max intensity, and eccentricity for each object.\n    3. For each test case, building a pairwise cost matrix based on attribute differences.\n    4. Solving the linear sum assignment problem to find the optimal matching.\n    5. Calculating the total objective J, including a penalty for unmatched forecast objects.\n    \"\"\"\n\n    # --- Data Specification ---\n    GRID_DIM = 8\n    THRESHOLD = 1.0\n    \n    forecast_intensities_spec = {\n        ((1, 1), 2.0), ((1, 2), 1.6), ((2, 1), 1.4), ((2, 2), 1.2),\n        ((3, 4), 1.1), ((3, 5), 1.4), ((3, 6), 1.6), ((4, 4), 1.0), ((4, 5), 1.2), ((4, 6), 1.3),\n        ((5, 1), 0.95), ((5, 2), 1.0), ((6, 1), 1.2), ((6, 2), 1.3), ((7, 1), 2.2), ((7, 2), 2.0)\n    }\n    \n    observed_intensities_spec = {\n        ((3, 2), 1.0), ((3, 3), 1.3), ((3, 4), 1.5), ((4, 2), 0.95), ((4, 3), 1.1), ((4, 4), 1.2),\n        ((4, 6), 1.1), ((4, 7), 1.2), ((5, 6), 1.4), ((5, 7), 1.6), ((6, 6), 1.7), ((6, 7), 1.8)\n    }\n\n    # --- Test Suite ---\n    test_cases = [\n        # (w_A, w_E, w_M), p_u\n        ((1.0, 1.0, 1.0), 0.25),\n        ((0.5, 2.0, 1.0), 0.25),\n        ((1.5, 0.0, 1.0), 0.40),\n        ((0.2, 1.0, 3.0), 0.10),\n    ]\n\n    def create_grid(spec, dim):\n        grid = np.zeros((dim, dim), dtype=float)\n        for (r, c), intensity in spec:\n            grid[r, c] = intensity\n        return grid\n\n    def find_objects(intensity_grid, threshold):\n        mask = intensity_grid >= threshold\n        labels = np.zeros_like(mask, dtype=int)\n        current_label = 0\n        rows, cols = mask.shape\n        \n        for r in range(rows):\n            for c in range(cols):\n                if mask[r, c] and labels[r, c] == 0:\n                    current_label += 1\n                    q = [(r, c)]\n                    labels[r, c] = current_label\n                    head = 0\n                    while head  len(q):\n                        row, col = q[head]\n                        head += 1\n                        \n                        for dr in [-1, 0, 1]:\n                            for dc in [-1, 0, 1]:\n                                if dr == 0 and dc == 0:\n                                    continue\n                                nr, nc = row + dr, col + dc\n                                \n                                if 0 = nr  rows and 0 = nc  cols and \\\n                                   mask[nr, nc] and labels[nr, nc] == 0:\n                                    labels[nr, nc] = current_label\n                                    q.append((nr, nc))\n                                    \n        return labels, current_label\n\n    def calculate_attributes(labeled_grid, intensity_grid, num_objects):\n        attributes = []\n        for label in range(1, num_objects + 1):\n            # Coordinates are (row, col)\n            coords_r, coords_c = np.where(labeled_grid == label)\n            \n            # Area\n            area = len(coords_r)\n            \n            # Max Intensity\n            max_intensity = np.max(intensity_grid[coords_r, coords_c])\n            \n            # Eccentricity\n            if area = 1:\n                eccentricity = 0.0\n            else:\n                # Per problem spec, x is col, y is row\n                x_coords = coords_c.astype(float)\n                y_coords = coords_r.astype(float)\n                \n                # Covariance matrix with 1/n normalization (bias=True)\n                # np.cov expects variables as rows, observations as columns\n                cov_matrix = np.cov(np.vstack((x_coords, y_coords)), bias=True)\n\n                if cov_matrix.ndim  2:\n                    # Occurs if e.g. all x or all y are identical\n                    lambda_max = cov_matrix.item()\n                    lambda_min = 0.0\n                else: \n                    eigenvalues = np.linalg.eigvalsh(cov_matrix)\n                    lambda_min, lambda_max = eigenvalues[0], eigenvalues[1]\n\n                if lambda_max > 0:\n                    # Clip argument to sqrt to avoid small negative values from float precision\n                    eccentricity = np.sqrt(max(0, 1 - lambda_min / lambda_max))\n                else:\n                    eccentricity = 0.0\n\n            attributes.append({'A': area, 'e': eccentricity, 'M': max_intensity})\n        return attributes\n\n    # --- Step 1  2: Identify objects and calculate base attributes (once) ---\n    I_f = create_grid(forecast_intensities_spec, GRID_DIM)\n    I_o = create_grid(observed_intensities_spec, GRID_DIM)\n\n    labels_f, n_f = find_objects(I_f, THRESHOLD)\n    labels_o, n_o = find_objects(I_o, THRESHOLD)\n\n    attrs_f = calculate_attributes(labels_f, I_f, n_f)\n    attrs_o = calculate_attributes(labels_o, I_o, n_o)\n\n    results = []\n    # --- Loop through test cases ---\n    for (w_A, w_E, w_M), p_u in test_cases:\n        \n        # --- Step 3: Cost Matrix Construction ---\n        cost_matrix = np.zeros((n_f, n_o))\n        for i in range(n_f):\n            for j in range(n_o):\n                f_obj = attrs_f[i]\n                o_obj = attrs_o[j]\n                \n                d_A = abs(f_obj['A'] - o_obj['A']) / max(f_obj['A'], o_obj['A'])\n                d_E = abs(f_obj['e'] - o_obj['e'])\n                d_M = abs(f_obj['M'] - o_obj['M']) / max(f_obj['M'], o_obj['M'])\n                \n                cost = w_A * d_A + w_E * d_E + w_M * d_M\n                cost_matrix[i, j] = cost\n\n        # --- Step 4: Optimal Assignment and Total Cost ---\n        row_ind, col_ind = linear_sum_assignment(cost_matrix)\n        \n        matched_cost_sum = cost_matrix[row_ind, col_ind].sum()\n        \n        num_matched = len(row_ind)\n        num_unmatched_f = n_f - num_matched\n        \n        unmatched_penalty = p_u * num_unmatched_f\n        \n        total_objective_J = matched_cost_sum + unmatched_penalty\n        results.append(round(total_objective_J, 4))\n        \n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Once forecast and observed objects are identified, we need robust metrics to diagnose the sources of forecast error. The Structure-Amplitude-Location (SAL) method is a powerful diagnostic tool that deconstructs error into three intuitive components, answering whether the forecast was wrong in its shape, its magnitude, or its position. This practice  uses a clear, analytical scenario with idealized Gaussian-shaped objects to help you build a quantitative intuition for how the $S$, $A$, and $L$ components are calculated and how they isolate different types of forecast deficiencies.",
            "id": "4090731",
            "problem": "Consider continuous, two-dimensional precipitation intensity fields defined on a circular domain $\\Omega = \\{\\mathbf{x} \\in \\mathbb{R}^{2}:\\ \\|\\mathbf{x}\\| \\leq R\\}$ of radius $R$, with the forecast field given by $F(\\mathbf{x}) = H_{f} \\exp\\!\\big(-\\|\\mathbf{x} - \\mathbf{x}_{f}\\|^{2}/(2\\sigma_{f}^{2})\\big)$ and the observed field given by $O(\\mathbf{x}) = H_{o} \\exp\\!\\big(-\\|\\mathbf{x} - \\mathbf{x}_{o}\\|^{2}/(2\\sigma_{o}^{2})\\big)$. The centers are colocated at the origin, that is, $\\mathbf{x}_{f} = \\mathbf{x}_{o} = \\mathbf{0}$. Assume isotropy and that the domain radius $R$ is sufficiently large relative to $\\sigma_{f}$ and $\\sigma_{o}$ so that truncation of Gaussian tails at $r=R$ is negligible to four significant figures.\n\nUsing the Structure–Amplitude–Location (SAL) verification triplet, compute the components $S$, $A$, and $L$ for the parameter choices\n- $H_{o} = 20$ in $\\mathrm{mm\\,h^{-1}}$, $\\sigma_{o} = 15$ in $\\mathrm{km}$,\n- $H_{f} = 12$ in $\\mathrm{mm\\,h^{-1}}$, $\\sigma_{f} = 30$ in $\\mathrm{km}$,\n- $R = 200$ in $\\mathrm{km}$.\n\nUse the following definitions:\n- Amplitude component: the domain means $\\overline{F} = \\frac{1}{\\pi R^{2}} \\int_{\\Omega} F(\\mathbf{x})\\, d\\mathbf{x}$ and $\\overline{O} = \\frac{1}{\\pi R^{2}} \\int_{\\Omega} O(\\mathbf{x})\\, d\\mathbf{x}$, and\n$$A = \\frac{2\\big(\\overline{F} - \\overline{O}\\big)}{\\overline{F} + \\overline{O}}.$$\n- Location component: $L = L_{1} + L_{2}$ with $L_{1} = \\frac{\\|\\mathbf{x}_{f} - \\mathbf{x}_{o}\\|}{2R}$ and the dispersion-based term\n$$r(F) = \\frac{\\int_{\\Omega} \\|\\mathbf{x} - \\mathbf{x}_{f}\\|\\, F(\\mathbf{x})\\, d\\mathbf{x}}{\\int_{\\Omega} F(\\mathbf{x})\\, d\\mathbf{x}}, \\quad r(O) = \\frac{\\int_{\\Omega} \\|\\mathbf{x} - \\mathbf{x}_{o}\\|\\, O(\\mathbf{x})\\, d\\mathbf{x}}{\\int_{\\Omega} O(\\mathbf{x})\\, d\\mathbf{x}}, \\quad L_{2} = \\frac{|r(F) - r(O)|}{2R}.$$\n- Structure component: define the high-intensity core of each field at half-maximum,\n$$C(F) = \\left\\{\\mathbf{x} \\in \\Omega:\\ F(\\mathbf{x}) \\geq \\frac{H_{f}}{2}\\right\\}, \\quad C(O) = \\left\\{\\mathbf{x} \\in \\Omega:\\ O(\\mathbf{x}) \\geq \\frac{H_{o}}{2}\\right\\},$$\nand the normalized core-area fractions\n$$s(F) = \\frac{\\mathrm{area}\\big(C(F)\\big)}{\\pi R^{2}}, \\quad s(O) = \\frac{\\mathrm{area}\\big(C(O)\\big)}{\\pi R^{2}}, \\quad S = \\frac{2\\big(s(F) - s(O)\\big)}{s(F) + s(O)}.$$\n\nCompute $S$, $A$, and $L$ under the large-domain approximation stated above, and provide the final SAL triplet as dimensionless numbers. Round each component to four significant figures. Express the final answer as a single row vector using the LaTeX $\\mathrm{pmatrix}$ environment.",
            "solution": "The user has provided a problem that passes all validation criteria. It is scientifically grounded in the field of meteorological forecast verification, mathematically well-posed, objective, and contains all necessary information for a unique solution. I will therefore proceed with a full derivation of the Structure-Amplitude-Location (SAL) components.\n\nThe problem requires the computation of the SAL triplet $(S, A, L)$ for two-dimensional Gaussian precipitation fields. The forecast field is $F(\\mathbf{x}) = H_{f} \\exp(-\\|\\mathbf{x} - \\mathbf{x}_{f}\\|^{2}/(2\\sigma_{f}^{2}))$ and the observed field is $O(\\mathbf{x}) = H_{o} \\exp(-\\|\\mathbf{x} - \\mathbf{x}_{o}\\|^{2}/(2\\sigma_{o}^{2}))$. The centers are colocated at the origin, so $\\mathbf{x}_{f} = \\mathbf{x}_{o} = \\mathbf{0}$. The fields are defined on a circular domain $\\Omega$ of radius $R$. The large-domain approximation states that the contribution from the fields outside $\\Omega$ is negligible, allowing us to approximate the integrals over $\\Omega$ by integrals over the entire plane $\\mathbb{R}^{2}$.\n\nThe given parameters are:\n$H_{o} = 20$, $\\sigma_{o} = 15$\n$H_{f} = 12$, $\\sigma_{f} = 30$\n$R = 200$\nAll units are consistent (mm/h for intensity, km for length).\n\nWe will compute each component of the SAL triplet in turn.\n\n### 1. Amplitude Component ($A$)\n\nThe amplitude component $A$ is defined as\n$$A = \\frac{2\\big(\\overline{F} - \\overline{O}\\big)}{\\overline{F} + \\overline{O}}$$\nwhere $\\overline{F}$ and $\\overline{O}$ are the domain-averaged precipitation intensities.\n$\\overline{F} = \\frac{1}{\\pi R^{2}} \\int_{\\Omega} F(\\mathbf{x})\\, d\\mathbf{x}$ and $\\overline{O} = \\frac{1}{\\pi R^{2}} \\int_{\\Omega} O(\\mathbf{x})\\, d\\mathbf{x}$.\n\nLet's first compute the total precipitation volume, which is the integral of the intensity field over the domain. For the forecast field, with $\\mathbf{x}_{f} = \\mathbf{0}$:\n$$V_{f} = \\int_{\\Omega} F(\\mathbf{x})\\, d\\mathbf{x} = \\int_{\\Omega} H_{f} \\exp\\left(\\frac{-\\|\\mathbf{x}\\|^{2}}{2\\sigma_{f}^{2}}\\right)\\, d\\mathbf{x}$$\nUsing the large-domain approximation, we can extend the integral over the entire plane $\\mathbb{R}^{2}$. We switch to polar coordinates $(r, \\theta)$, where $\\|\\mathbf{x}\\| = r$ and $d\\mathbf{x} = r\\,dr\\,d\\theta$.\n$$V_{f} \\approx \\int_{0}^{2\\pi} \\int_{0}^{\\infty} H_{f} \\exp\\left(\\frac{-r^{2}}{2\\sigma_{f}^{2}}\\right) r\\,dr\\,d\\theta$$\nThe integral separates into an angular and a radial part:\n$$V_{f} = 2\\pi H_{f} \\int_{0}^{\\infty} r \\exp\\left(\\frac{-r^{2}}{2\\sigma_{f}^{2}}\\right) dr$$\nLet $u = r^{2}/(2\\sigma_{f}^{2})$, so $du = r/\\sigma_{f}^{2} \\,dr$, which gives $r\\,dr = \\sigma_{f}^{2}\\,du$.\n$$V_{f} = 2\\pi H_{f} \\int_{0}^{\\infty} \\exp(-u) \\sigma_{f}^{2}\\,du = 2\\pi H_{f} \\sigma_{f}^{2} [-\\exp(-u)]_{0}^{\\infty} = 2\\pi H_{f} \\sigma_{f}^{2}$$\nBy exact analogy, the total observed volume is $V_{o} = 2\\pi H_{o} \\sigma_{o}^{2}$.\n\nNow we can compute the domain-averaged intensities:\n$$\\overline{F} = \\frac{V_{f}}{\\pi R^{2}} = \\frac{2\\pi H_{f} \\sigma_{f}^{2}}{\\pi R^{2}} = \\frac{2 H_{f} \\sigma_{f}^{2}}{R^{2}}$$\n$$\\overline{O} = \\frac{V_{o}}{\\pi R^{2}} = \\frac{2\\pi H_{o} \\sigma_{o}^{2}}{\\pi R^{2}} = \\frac{2 H_{o} \\sigma_{o}^{2}}{R^{2}}$$\nSubstituting the given values:\n$\\overline{F} = \\frac{2(12)(30)^{2}}{200^{2}} = \\frac{24 \\cdot 900}{40000} = \\frac{21600}{40000} = 0.54$.\n$\\overline{O} = \\frac{2(20)(15)^{2}}{200^{2}} = \\frac{40 \\cdot 225}{40000} = \\frac{9000}{40000} = 0.225$.\n\nFinally, we compute $A$:\n$$A = \\frac{2(0.54 - 0.225)}{0.54 + 0.225} = \\frac{2(0.315)}{0.765} = \\frac{0.63}{0.765} \\approx 0.823529...$$\nRounding to four significant figures, $A = 0.8235$.\n\n### 2. Location Component ($L$)\n\nThe location component is $L = L_{1} + L_{2}$.\nThe term $L_{1}$ is related to the displacement of the centers of mass:\n$$L_{1} = \\frac{\\|\\mathbf{x}_{f} - \\mathbf{x}_{o}\\|}{2R}$$\nSince the centers are given as colocated, $\\mathbf{x}_{f} = \\mathbf{x}_{o} = \\mathbf{0}$, we have $L_{1} = 0$.\n\nThe term $L_{2}$ is related to the difference in the average distance of precipitation from the respective centers:\n$$L_{2} = \\frac{|r(F) - r(O)|}{2R}$$\nwhere $r(F) = \\frac{\\int_{\\Omega} \\|\\mathbf{x} - \\mathbf{x}_{f}\\|\\, F(\\mathbf{x})\\, d\\mathbf{x}}{\\int_{\\Omega} F(\\mathbf{x})\\, d\\mathbf{x}}$.\nWith $\\mathbf{x}_{f} = \\mathbf{0}$, the denominator is $V_{f} = 2\\pi H_{f} \\sigma_{f}^{2}$. The numerator is:\n$$N_{f} = \\int_{\\Omega} \\|\\mathbf{x}\\| F(\\mathbf{x})\\, d\\mathbf{x} = \\int_{\\Omega} \\|\\mathbf{x}\\| H_{f} \\exp\\left(\\frac{-\\|\\mathbf{x}\\|^{2}}{2\\sigma_{f}^{2}}\\right)\\, d\\mathbf{x}$$\nAgain, using polar coordinates and the large-domain approximation:\n$$N_{f} \\approx \\int_{0}^{2\\pi} \\int_{0}^{\\infty} r H_{f} \\exp\\left(\\frac{-r^{2}}{2\\sigma_{f}^{2}}\\right) r\\,dr\\,d\\theta = 2\\pi H_{f} \\int_{0}^{\\infty} r^{2} \\exp\\left(\\frac{-r^{2}}{2\\sigma_{f}^{2}}\\right) dr$$\nThis is a standard Gaussian integral of the form $\\int_{0}^{\\infty} x^{2} \\exp(-ax^{2})dx = \\frac{\\sqrt{\\pi}}{4a^{3/2}}$. Here, $a=1/(2\\sigma_{f}^{2})$.\n$$\\int_{0}^{\\infty} r^{2} \\exp\\left(\\frac{-r^{2}}{2\\sigma_{f}^{2}}\\right) dr = \\frac{\\sqrt{\\pi}}{4(1/(2\\sigma_{f}^{2}))^{3/2}} = \\frac{\\sqrt{\\pi}}{4/(2\\sigma_{f}^{2}\\sqrt{2\\sigma_{f}^{2}})} = \\frac{\\sqrt{\\pi} \\cdot 2\\sqrt{2} \\sigma_{f}^{3}}{4} = \\frac{\\sqrt{2\\pi}}{2}\\sigma_{f}^{3}$$\nSo, the numerator is $N_{f} = 2\\pi H_{f} \\left(\\frac{\\sqrt{2\\pi}}{2}\\sigma_{f}^{3}\\right) = \\pi\\sqrt{2\\pi}H_{f}\\sigma_{f}^{3}$.\nThen, $r(F)$ is the ratio of the numerator to the denominator:\n$$r(F) = \\frac{N_{f}}{V_{f}} = \\frac{\\pi\\sqrt{2\\pi}H_{f}\\sigma_{f}^{3}}{2\\pi H_{f}\\sigma_{f}^{2}} = \\frac{\\sqrt{2\\pi}}{2}\\sigma_{f}$$\nSimilarly, for the observed field, $r(O) = \\frac{\\sqrt{2\\pi}}{2}\\sigma_{o}$.\n\nNow we substitute the values:\n$r(F) = \\frac{\\sqrt{2\\pi}}{2}(30) = 15\\sqrt{2\\pi}$ km.\n$r(O) = \\frac{\\sqrt{2\\pi}}{2}(15) = 7.5\\sqrt{2\\pi}$ km.\n\n$L_{2} = \\frac{|15\\sqrt{2\\pi} - 7.5\\sqrt{2\\pi}|}{2(200)} = \\frac{7.5\\sqrt{2\\pi}}{400}$.\n$L_{2} \\approx \\frac{7.5(2.506628)}{400} \\approx \\frac{18.79971}{400} \\approx 0.046999...$\nThus, $L = L_{1} + L_{2} = 0 + L_{2} \\approx 0.046999...$.\nRounding to four significant figures, $L = 0.04700$.\n\n### 3. Structure Component ($S$)\n\nThe structure component $S$ is defined as\n$$S = \\frac{2\\big(s(F) - s(O)\\big)}{s(F) + s(O)}$$\nwhere $s(F)$ and $s(O)$ are normalized core-area fractions. The core $C(F)$ is defined as the region where the intensity is at least half its maximum value:\n$C(F) = \\{\\mathbf{x} \\in \\Omega:\\ F(\\mathbf{x}) \\geq H_{f}/2\\}$.\n$$H_{f} \\exp\\left(\\frac{-\\|\\mathbf{x}\\|^{2}}{2\\sigma_{f}^{2}}\\right) \\geq \\frac{H_{f}}{2} \\implies \\exp\\left(\\frac{-\\|\\mathbf{x}\\|^{2}}{2\\sigma_{f}^{2}}\\right) \\geq \\frac{1}{2}$$\nTaking the natural logarithm of both sides:\n$$\\frac{-\\|\\mathbf{x}\\|^{2}}{2\\sigma_{f}^{2}} \\geq \\ln\\left(\\frac{1}{2}\\right) = -\\ln(2)$$\n$$\\|\\mathbf{x}\\|^{2} \\leq 2\\sigma_{f}^{2}\\ln(2) \\implies \\|\\mathbf{x}\\| \\leq \\sigma_{f}\\sqrt{2\\ln(2)}$$\nThis describes a circular disk centered at the origin. The radius of this core is $R_{f} = \\sigma_{f}\\sqrt{2\\ln(2)}$. The area of the core is $\\mathrm{area}(C(F)) = \\pi R_{f}^{2} = 2\\pi\\sigma_{f}^{2}\\ln(2)$.\nThe normalized core-area fraction is:\n$$s(F) = \\frac{\\mathrm{area}(C(F))}{\\pi R^{2}} = \\frac{2\\pi\\sigma_{f}^{2}\\ln(2)}{\\pi R^{2}} = \\frac{2\\sigma_{f}^{2}\\ln(2)}{R^{2}}$$\nSimilarly, $s(O) = \\frac{2\\sigma_{o}^{2}\\ln(2)}{R^{2}}$.\n\nNow we compute $S$:\n$$S = \\frac{2\\left(\\frac{2\\sigma_{f}^{2}\\ln(2)}{R^{2}} - \\frac{2\\sigma_{o}^{2}\\ln(2)}{R^{2}}\\right)}{\\frac{2\\sigma_{f}^{2}\\ln(2)}{R^{2}} + \\frac{2\\sigma_{o}^{2}\\ln(2)}{R^{2}}}$$\nThe common term $2\\ln(2)/R^{2}$ cancels from the numerator and denominator:\n$$S = \\frac{2(\\sigma_{f}^{2} - \\sigma_{o}^{2})}{\\sigma_{f}^{2} + \\sigma_{o}^{2}}$$\nSubstituting the given values for $\\sigma_{f}$ and $\\sigma_{o}$:\n$\\sigma_{f}^{2} = 30^{2} = 900$.\n$\\sigma_{o}^{2} = 15^{2} = 225$.\n$$S = \\frac{2(900 - 225)}{900 + 225} = \\frac{2(675)}{1125} = \\frac{1350}{1125} = 1.2$$\nTo four significant figures, $S = 1.200$.\n\n### Summary\n\nThe computed components of the SAL triplet are:\n- Structure $S = 1.200$\n- Amplitude $A = 0.8235$\n- Location $L = 0.04700$\n\nThe final SAL triplet, ordered as $(S, A, L)$, is $(1.200, 0.8235, 0.04700)$.",
            "answer": "$$\\boxed{\\begin{pmatrix} 1.200  0.8235  0.04700 \\end{pmatrix}}$$"
        },
        {
            "introduction": "Beyond comparing discrete objects, spatial verification often employs neighborhood, or \"fuzzy,\" methods that relax the requirement of an exact location match. The Fractions Skill Score (FSS) is a premier example, measuring skill by comparing event fractions within a neighborhood of a given physical scale. This practice  tackles a critical and practical challenge in model evaluation: understanding how the FSS is sensitive to the underlying grid resolution of the forecast model, a crucial consideration when comparing predictions from different numerical weather prediction systems.",
            "id": "4090719",
            "problem": "You are tasked with implementing a numerical sensitivity analysis of Fractions Skill Score (FSS) to changes in grid spacing in the context of numerical weather prediction and climate modeling, using spatial and object-based verification techniques. The Fractions Skill Score (FSS) must be defined from first principles using neighborhood fractions computed at a fixed physical scale, and the analysis must isolate the influence of grid spacing on the smoothing operator while keeping physical scales unchanged.\n\nBegin from the following fundamental base:\n- Represent forecast and observation event occurrence as binary indicator fields on a two-dimensional domain, where the value equals $1$ inside the event region and equals $0$ outside.\n- Compute neighborhood fractions by convolving the indicator fields with a discrete top-hat (circular) kernel whose radius corresponds to a fixed physical length scale. The neighborhood fraction at a grid cell is defined as the fraction of grid cells within the kernel that belong to the event, normalized by the kernel's total count.\n- Use the fixed physical neighborhood radius across all grid spacings, ensuring that the kernel size in grid cells changes consistently with the grid spacing.\n- Construct the Fractions Skill Score (FSS) between the forecast and observation fraction fields as a unitless skill metric that equals $1$ for perfect match, is bounded above by $1$, and decreases monotonically with increasing squared differences between the fraction fields, normalized by the sum of the self-squared fraction fields. Handle any corner cases of zero denominators in a scientifically reasonable manner.\n\nScientific setup:\n- Consider a square physical domain of size $L_x = 100$ km by $L_y = 100$ km.\n- The observation event is a filled disk of radius $R_o = 12$ km centered at $(x_o,y_o)$.\n- The forecast event is a filled disk of radius $R_f = 12$ km centered at $(x_f,y_f)$.\n- The fixed physical neighborhood (smoothing) radius is $R_s = 10$ km.\n\nDiscretization rules:\n- For a chosen grid spacing $d_x$ (in km), discretize the domain into a uniform grid whose cell centers tile $[0, L_x)$ by $[0, L_y)$.\n- The indicator field for a disk of center $(x_c,y_c)$ and radius $R$ equals $1$ at grid cells whose Euclidean distance from $(x_c,y_c)$ is less than or equal to $R$ and equals $0$ otherwise.\n- The discrete circular top-hat kernel has radius $r_{\\text{cells}} = \\max(1,\\mathrm{round}(R_s/d_x))$ grid cells. The kernel weights equal $1$ at offsets whose Euclidean distance in grid cells is less than or equal to $r_{\\text{cells}}$ and equal $0$ otherwise. Use zero boundary padding, and normalize neighborhood fractions by the constant kernel sum.\n- Compute the Fractions Skill Score (FSS) between the forecast and observation fraction fields over all grid cells in the domain.\n\nUnits:\n- Distances, grid spacing, and radii must be treated in kilometers. The final answer must be unitless and expressed as floats rounded to exactly six decimal places.\n\nTest suite:\nCompute the FSS for the following four test cases, each specified by $(d_x,(x_o,y_o),(x_f,y_f))$, with $L_x = L_y = 100$ km, $R_o = R_f = 12$ km, and $R_s = 10$ km fixed:\n1. $d_x = 1$ km, $(x_o,y_o) = (50,50)$ km, $(x_f,y_f) = (55,50)$ km.\n2. $d_x = 2$ km, $(x_o,y_o) = (50,50)$ km, $(x_f,y_f) = (55,50)$ km.\n3. $d_x = 5$ km, $(x_o,y_o) = (50,50)$ km, $(x_f,y_f) = (55,50)$ km.\n4. $d_x = 2$ km, $(x_o,y_o) = (12,50)$ km, $(x_f,y_f) = (18,50)$ km.\n\nFinal output format:\n- Your program should produce a single line of output containing the four FSS values for the above test cases rounded to six decimal places, as a comma-separated list enclosed in square brackets, for example, \"[$f_1,f_2,f_3,f_4$]\".",
            "solution": "The problem requires the implementation of a numerical sensitivity analysis for the Fractions Skill Score (FSS) with respect to changes in grid spacing. The analysis is to be performed on a synthetic dataset representing forecast and observed events as circular regions on a two-dimensional domain. The solution must be constructed from first principles as outlined.\n\nThe methodological approach involves several distinct steps: first, the discretization of the continuous physical domain and the event features onto a grid; second, the transformation of binary event fields into continuous fraction fields via a smoothing operation; and third, the computation of the FSS from these fraction fields.\n\n**1. Domain and Field Discretization**\n\nThe physical domain is a square of size $L_x \\times L_y$, where $L_x = L_y = 100$ km. This domain is discretized into a uniform grid of $N_x \\times N_y$ cells, where the number of cells in each dimension is determined by the grid spacing, $d_x$. Specifically, $N_x = \\text{int}(L_x / d_x)$ and $N_y = \\text{int}(L_y / d_x)$. The cell centers $(x_i, y_j)$ are defined to tile the interval $[0, L_x) \\times [0, L_y)$, which we implement by setting their coordinates as:\n$$ x_i = i \\cdot d_x + \\frac{d_x}{2}, \\quad \\text{for } i \\in \\{0, 1, \\dots, N_x-1\\} $$\n$$ y_j = j \\cdot d_x + \\frac{d_x}{2}, \\quad \\text{for } j \\in \\{0, 1, \\dots, N_y-1\\} $$\n\nThe observation and forecast events are filled disks of radii $R_o = 12$ km and $R_f = 12$ km, centered at $(x_o, y_o)$ and $(x_f, y_f)$ respectively. These continuous shapes are rasterized onto the grid to create binary indicator fields, $I_o$ and $I_f$. A grid cell at $(x_i, y_j)$ is assigned a value of $1$ if its center falls within or on the boundary of the disk, and $0$ otherwise. This is expressed by the condition:\n$$ I(i,j) = \\begin{cases} 1  \\text{if } \\sqrt{(x_i-x_c)^2 + (y_j-y_c)^2} \\le R \\\\ 0  \\text{otherwise} \\end{cases} $$\nwhere $(x_c, y_c)$ and $R$ are the center and radius of the respective disk.\n\n**2. Neighborhood Smoothing and Fraction Fields**\n\nThe binary indicator fields, which represent the presence or absence of an event, are converted into continuous fields of neighborhood fractions. This is achieved by convolving each indicator field with a smoothing kernel. The problem specifies a circular top-hat kernel with a fixed physical radius $R_s = 10$ km. The key aspect of this sensitivity analysis is that as the grid spacing $d_x$ changes, the size of this kernel in grid cells must adapt to maintain the fixed physical scale. The kernel's radius in grid cells, $r_{\\text{cells}}$, is calculated for each $d_x$:\n$$ r_{\\text{cells}} = \\max(1, \\text{round}(R_s / d_x)) $$\nThe kernel, $K$, is a two-dimensional matrix where elements are $1$ if they are within a Euclidean distance of $r_{\\text{cells}}$ from the kernel's center, and $0$ otherwise.\n\nThe neighborhood fraction fields, $F_o$ and $F_f$, are computed by performing a 2D convolution ($*$) of the indicator fields with the kernel $K$, and normalizing by the sum of the kernel weights. This operation, for any given cell $(i,j)$, effectively calculates the fraction of the neighborhood (defined by the kernel's shape and size) that contains the event. The convolution uses zero-padding at the boundaries.\n$$ F(i,j) = \\frac{(I * K)(i,j)}{\\sum_{k,l} K(k,l)} = \\frac{\\sum_{k,l} I(i-k, j-l) K(k,l)}{\\sum_{k,l} K(k,l)} $$\nThe denominator, $\\sum_{k,l} K(k,l)$, is the total number of grid cells within the discrete circular kernel.\n\n**3. Fractions Skill Score (FSS) Calculation**\n\nThe FSS is a metric that quantifies the skill of the forecast fraction field $F_f$ relative to the observation fraction field $F_o$. It is defined based on the Mean Squared Error (MSE) between the two fields, normalized by a reference MSE. The MSE is:\n$$ \\text{MSE} = \\frac{1}{N_x N_y} \\sum_{i=0}^{N_x-1} \\sum_{j=0}^{N_y-1} (F_f(i,j) - F_o(i,j))^2 $$\nThe reference MSE, $MSE_{ref}$, is the sum of the MSEs of the forecast and observation fields against a zero field, representing the worst possible case of no overlap. It is defined as:\n$$ \\text{MSE}_{\\text{ref}} = \\frac{1}{N_x N_y} \\left( \\sum_{i,j} F_f(i,j)^2 + \\sum_{i,j} F_o(i,j)^2 \\right) $$\nThe FSS is then given by:\n$$ \\text{FSS} = 1 - \\frac{\\text{MSE}}{\\text{MSE}_{\\text{ref}}} $$\nSubstituting the definitions and simplifying by canceling the $1/(N_x N_y)$ term, the formula becomes:\n$$ \\text{FSS} = 1 - \\frac{\\sum_{i,j} (F_f(i,j) - F_o(i,j))^2}{\\sum_{i,j} F_f(i,j)^2 + \\sum_{i,j} F_o(i,j)^2} $$\nThis formulation ensures FSS is $1$ for a perfect forecast ($F_f = F_o$), and decreases as the difference between the fields increases. A special case arises if the denominator is zero. This occurs only if both $F_f$ and $F_o$ are zero everywhere, meaning no events were observed or forecast. In this scenario, the numerator is also zero. This constitutes a perfect forecast of a non-event, so the FSS is correctly defined to be $1$.\n\nThe algorithm is implemented by applying these steps sequentially for each test case provided in the problem statement.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.signal import convolve2d\n\ndef calculate_fss(d_x, center_o, center_f, L_x, L_y, R_o, R_f, R_s):\n    \"\"\"\n    Computes the Fractions Skill Score (FSS) for a given set of parameters.\n\n    Args:\n        d_x (float): Grid spacing in km.\n        center_o (tuple): Center coordinates (x, y) of the observation disk in km.\n        center_f (tuple): Center coordinates (x, y) of the forecast disk in km.\n        L_x (float): Domain width in km.\n        L_y (float): Domain height in km.\n        R_o (float): Radius of the observation disk in km.\n        R_f (float): Radius of the forecast disk in km.\n        R_s (float): Physical radius of the smoothing kernel in km.\n\n    Returns:\n        float: The calculated Fractions Skill Score.\n    \"\"\"\n    # 1. Grid setup\n    # Determine the number of grid points\n    N_x = int(L_x / d_x)\n    N_y = int(L_y / d_x)\n    \n    # Create grid cell center coordinates\n    x_coords = np.arange(N_x) * d_x + d_x / 2.0\n    y_coords = np.arange(N_y) * d_x + d_x / 2.0\n    xx, yy = np.meshgrid(x_coords, y_coords)\n\n    # 2. Generate Binary Indicator Fields\n    # Observation field\n    x_o, y_o = center_o\n    dist_sq_o = (xx - x_o)**2 + (yy - y_o)**2\n    indicator_o = (dist_sq_o = R_o**2).astype(float)\n\n    # Forecast field\n    x_f, y_f = center_f\n    dist_sq_f = (xx - x_f)**2 + (yy - y_f)**2\n    indicator_f = (dist_sq_f = R_f**2).astype(float)\n\n    # 3. Define and Apply Smoothing Kernel\n    # Calculate kernel radius in grid cells from fixed physical radius\n    r_cells = int(max(1, np.round(R_s / d_x)))\n    \n    # Create the circular top-hat kernel\n    ky, kx = np.mgrid[-r_cells:r_cells+1, -r_cells:r_cells+1]\n    kernel_mask = (kx**2 + ky**2 = r_cells**2).astype(float)\n    kernel_sum = np.sum(kernel_mask)\n\n    # 4. Compute Neighborhood Fraction Fields\n    # Use 2D convolution to apply the smoothing kernel\n    if kernel_sum == 0:\n        # This case is unlikely with r_cells >= 1 but is included for robustness\n        fraction_field_o = np.zeros_like(indicator_o)\n        fraction_field_f = np.zeros_like(indicator_f)\n    else:\n        # Pad with zeros at the boundary as specified\n        fraction_field_o = convolve2d(indicator_o, kernel_mask, mode='same', boundary='fill', fillvalue=0) / kernel_sum\n        fraction_field_f = convolve2d(indicator_f, kernel_mask, mode='same', boundary='fill', fillvalue=0) / kernel_sum\n\n    # 5. Compute the Fractions Skill Score (FSS)\n    # Sum of squared differences between the fraction fields\n    numerator = np.sum((fraction_field_o - fraction_field_f)**2)\n    # Sum of self-squared fraction fields for reference\n    denominator = np.sum(fraction_field_o**2) + np.sum(fraction_field_f**2)\n\n    if denominator == 0:\n        # If both fields are zero, it's a perfect forecast of a non-event. FSS is 1.\n        return 1.0\n    \n    fss = 1.0 - numerator / denominator\n    return fss\n    \ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Format: (d_x, (x_o, y_o), (x_f, y_f))\n    test_cases = [\n        (1.0, (50.0, 50.0), (55.0, 50.0)),\n        (2.0, (50.0, 50.0), (55.0, 50.0)),\n        (5.0, (50.0, 50.0), (55.0, 50.0)),\n        (2.0, (12.0, 50.0), (18.0, 50.0)),\n    ]\n\n    # Fixed scientific parameters\n    L_x = 100.0  # km\n    L_y = 100.0  # km\n    R_o = 12.0   # km\n    R_f = 12.0   # km\n    R_s = 10.0   # km\n\n    results = []\n    for case in test_cases:\n        d_x, center_o, center_f = case\n        fss = calculate_fss(d_x, center_o, center_f, L_x, L_y, R_o, R_f, R_s)\n        results.append(fss)\n\n    # Final print statement in the exact required format.\n    # Results are rounded to six decimal places.\n    print(f\"[{','.join([f'{r:.6f}' for r in results])}]\")\n\nsolve()\n```"
        }
    ]
}