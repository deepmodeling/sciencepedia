{
    "hands_on_practices": [
        {
            "introduction": "A foundational step in statistical downscaling is constructing a model that accurately reflects the underlying physical processes. This exercise demonstrates the power of feature engineering by comparing a baseline model using only large-scale atmospheric fields with an augmented model that includes explicit orographic predictors. By quantifying the improvement in predictive accuracy, you will gain hands-on experience in how incorporating domain-specific knowledge leads to more skillful and physically meaningful downscaling models. ",
            "id": "4093954",
            "problem": "Construct a program that performs statistical downscaling of daily precipitation for a mountainous region using large-scale geopotential height and moisture with explicit orographic predictors, and then quantifies bias reduction achieved by including such orographic predictors. The target domain is Numerical Weather Prediction (NWP) and climate modeling, and the method must be rooted in first principles and well-tested statistical formulations.\n\nFundamental base and rationale:\n- Precipitation formation is supported by the interplay of vertical motion and moisture availability. Under the quasi-geostrophic framework, synoptic-scale ascent is associated with lower geopotential heights and positive vorticity advection; thus lower geopotential height often proxies enhanced ascent. Moisture availability is captured by specific humidity. Orographic enhancement is driven by uplift of moist air over terrain and depends on slope magnitude and alignment of the flow relative to terrain aspect. These principles motivate the predictor set combining large-scale fields and explicit orographic terms.\n- Ordinary Least Squares (OLS) under an additive Gaussian error model provides a foundational estimator for linear statistical downscaling. Given a design matrix $X$ and observations $y$, the OLS estimator $\\hat{\\beta}$ is given by\n$$\n\\hat{\\beta} = (X^{\\top} X)^{-1} X^{\\top} y,\n$$\nwhen $(X^{\\top} X)$ is invertible, or equivalently via the Moore–Penrose pseudoinverse when necessary. Predictions are then $X \\hat{\\beta}$, here truncated to non-negative to respect the physical constraint that precipitation is never negative:\n$$\n\\hat{y}^{+} = \\max(0, X \\hat{\\beta}),\n$$\napplied element-wise.\n\nModels to estimate and compare:\n- Baseline model using only large-scale predictors:\n$$\ny = \\beta_0 + \\beta_H H + \\beta_q q + \\varepsilon,\n$$\nwhere $H$ denotes geopotential height, and $q$ denotes specific humidity.\n- Orographic-augmented model:\n$$\ny = \\gamma_0 + \\gamma_H H + \\gamma_q q + \\gamma_z z + \\gamma_s s + \\gamma_u u + \\gamma_{su} (s \\cdot u) + \\varepsilon,\n$$\nwhere $z$ is elevation, $s$ is slope magnitude, $u$ is a dimensionless upslope alignment factor, and $(s \\cdot u)$ is an explicit interaction term that captures the multiplicative effect of slope magnitude and upslope flow alignment.\n\nUnits and angle specification:\n- Geopotential height $H$ is provided in meters.\n- Specific humidity $q$ is provided in kilograms per kilogram.\n- Elevation $z$ is provided in meters.\n- Slope magnitude $s$ is provided in degrees; angles must be treated in degrees and not converted to radians for this task.\n- Precipitation $y$ is provided and must be predicted in millimeters per day.\n\nRequired computations:\n1. Estimate $\\hat{\\beta}$ for the baseline model and $\\hat{\\gamma}$ for the orographic-augmented model using the provided calibration dataset via the OLS estimator.\n2. Produce downscaled predictions for a test suite using both models and truncate them to non-negative values, i.e., apply $\\max(0,\\cdot)$ element-wise.\n3. Compute the bias for each test case as model prediction minus observed precipitation. Then, compute the fractional bias reduction for each test case:\n$$\nr_i = \\frac{|\\text{bias}_{\\text{baseline},i}| - |\\text{bias}_{\\text{orographic},i}|}{|\\text{bias}_{\\text{baseline},i}|},\n$$\nwith the convention that if $|\\text{bias}_{\\text{baseline},i}| = 0$, then $r_i = 0$ to avoid division by zero.\n4. Report the fractional bias reductions rounded to four decimal places.\n\nCalibration dataset (each sample is a station-day with varying orography and large-scale conditions). For each sample $i$ provide $(H_i,q_i,z_i,s_i,u_i,y_i)$:\n- Sample $1$: $(H_1 = 5600, q_1 = 0.004, z_1 = 800, s_1 = 10, u_1 = 0.2, y_1 = 11.2)$\n- Sample $2$: $(H_2 = 5550, q_2 = 0.006, z_2 = 1500, s_2 = 25, u_2 = 0.7, y_2 = 19.0)$\n- Sample $3$: $(H_3 = 5450, q_3 = 0.008, z_3 = 2200, s_3 = 30, u_3 = 0.9, y_3 = 26.6)$\n- Sample $4$: $(H_4 = 5700, q_4 = 0.003, z_4 = 1200, s_4 = 15, u_4 = 0.4, y_4 = 8.9)$\n- Sample $5$: $(H_5 = 5300, q_5 = 0.010, z_5 = 1800, s_5 = 20, u_5 = 0.6, y_5 = 29.2)$\n- Sample $6$: $(H_6 = 5400, q_6 = 0.007, z_6 = 1000, s_6 = 12, u_6 = 0.3, y_6 = 20.22)$\n- Sample $7$: $(H_7 = 5580, q_7 = 0.005, z_7 = 900, s_7 = 8, u_7 = 0.1, y_7 = 12.96)$\n- Sample $8$: $(H_8 = 5490, q_8 = 0.0065, z_8 = 2000, s_8 = 28, u_8 = 0.8, y_8 = 22.43)$\n- Sample $9$: $(H_9 = 5350, q_9 = 0.009, z_9 = 2500, s_9 = 35, u_9 = 1.0, y_9 = 32.0)$\n- Sample $10$: $(H_{10} = 5620, q_{10} = 0.0045, z_{10} = 1600, s_{10} = 22, u_{10} = 0.5, y_{10} = 14.15)$\n- Sample $11$: $(H_{11} = 5500, q_{11} = 0.0068, z_{11} = 1400, s_{11} = 18, u_{11} = 0.55, y_{11} = 19.58)$\n- Sample $12$: $(H_{12} = 5380, q_{12} = 0.0075, z_{12} = 2100, s_{12} = 32, u_{12} = 0.85, y_{12} = 27.19)$\n\nTest suite:\n- Test case $A$ (general \"happy path\"): $(H_A = 5450, q_A = 0.007, z_A = 1700, s_A = 24, u_A = 0.7)$, with observed precipitation $(y_A = 22.56)$ in $\\mathrm{mm/day}$.\n- Test case $B$ (boundary dryness and no upslope alignment): $(H_B = 5700, q_B = 0.002, z_B = 2100, s_B = 30, u_B = 0.0)$, with observed precipitation $(y_B = 7.1)$ in $\\mathrm{mm/day}$.\n- Test case $C$ (edge case of strong orographic enhancement): $(H_C = 5350, q_C = 0.0095, z_C = 2400, s_C = 34, u_C = 0.95)$, with observed precipitation $(y_C = 32.11)$ in $\\mathrm{mm/day}$.\n\nAlgorithmic requirements:\n- Use Ordinary Least Squares (OLS) to fit both models from the calibration dataset and produce predictions for the test suite.\n- Apply non-negativity truncation to predictions: for each predicted value $\\hat{y}$, compute $\\hat{y}^{+} = \\max(0,\\hat{y})$.\n- Compute fractional bias reductions $r_A$, $r_B$, and $r_C$ as defined above, rounding each to four decimal places.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered as $[r_A,r_B,r_C]$, for example, $[0.1234,0.5678,0.9012]$.",
            "solution": "The problem requires the construction and comparison of two statistical models for downscaling daily precipitation. The first is a baseline model using large-scale atmospheric predictors, and the second is an augmented model that includes local orographic predictors. The comparison is to be performed by quantifying the fractional reduction in prediction bias achieved by the more complex model. The entire procedure will be based on the principles of Ordinary Least Squares (OLS) regression.\n\nFirst, we define the two linear models as stipulated.\n\nThe baseline model relates precipitation $y$ to large-scale geopotential height $H$ and specific humidity $q$:\n$$\ny = \\beta_0 + \\beta_H H + \\beta_q q + \\varepsilon\n$$\nHere, $\\beta_0$ is the intercept, $\\beta_H$ and $\\beta_q$ are the coefficients for the respective predictors, and $\\varepsilon$ is the error term, assumed to be Gaussian.\n\nThe orographic-augmented model adds predictors related to local terrain: elevation $z$, slope magnitude $s$, an upslope alignment factor $u$, and an interaction term between slope and alignment $s \\cdot u$:\n$$\ny = \\gamma_0 + \\gamma_H H + \\gamma_q q + \\gamma_z z + \\gamma_s s + \\gamma_u u + \\gamma_{su} (s \\cdot u) + \\varepsilon\n$$\nThe coefficients are denoted by $\\gamma_j$ for this model. Both models can be expressed in the general matrix form of a linear model, $Y = X\\beta + \\varepsilon$, where $Y$ is the vector of observed precipitation values, $X$ is the design matrix containing the predictor values (with a leading column of ones for the intercept), and $\\beta$ is the vector of coefficients to be estimated.\n\nThe estimation of the coefficient vectors, $\\hat{\\beta}$ and $\\hat{\\gamma}$, is performed using Ordinary Least Squares (OLS). The OLS estimator provides the coefficient values that minimize the sum of the squared differences between observed and predicted values. The solution is given by the normal equations:\n$$\n\\hat{\\beta} = (X^{\\top} X)^{-1} X^{\\top} Y\n$$\nThis solution is valid when the matrix $X^{\\top} X$ is invertible. In cases of multicollinearity or other numerical issues, the Moore-Penrose pseudoinverse provides a robust alternative. We will use a numerical linear algebra solver that implicitly handles this, equivalent to using the pseudoinverse.\n\nThe first step is to construct the design matrices and the response vector from the provided calibration dataset, which consists of $N=12$ samples. The response vector $Y$ is:\n$$\nY = [11.2, 19.0, 26.6, 8.9, 29.2, 20.22, 12.96, 22.43, 32.0, 14.15, 19.58, 27.19]^{\\top}\n$$\nFor the baseline model, the design matrix $X_{\\text{base}}$ is a $12 \\times 3$ matrix, with columns corresponding to the intercept, $H$, and $q$:\n$$\nX_{\\text{base}} = \\begin{pmatrix}\n1  5600  0.004 \\\\\n1  5550  0.006 \\\\\n\\vdots  \\vdots  \\vdots \\\\\n1  5380  0.0075\n\\end{pmatrix}\n$$\nFor the orographic-augmented model, the design matrix $X_{\\text{oro}}$ is a $12 \\times 7$ matrix, with columns corresponding to the intercept, $H$, $q$, $z$, $s$, $u$, and the interaction term $s \\cdot u$:\n$$\nX_{\\text{oro}} = \\begin{pmatrix}\n1  5600  0.004  800  10  0.2  2.0 \\\\\n1  5550  0.006  1500  25  0.7  17.5 \\\\\n\\vdots  \\vdots  \\vdots  \\vdots  \\vdots  \\vdots  \\vdots \\\\\n1  5380  0.0075  2100  32  0.85  27.2\n\\end{pmatrix}\n$$\nThe coefficient vectors $\\hat{\\beta}$ and $\\hat{\\gamma}$ are then computed by solving the OLS problem for each model using the calibration data.\n\nNext, we use the fitted models to make predictions for the three test cases (A, B, C). For each test case $i$, we construct the corresponding predictor vectors, $x_{\\text{base},i}$ (a $1 \\times 3$ row vector) and $x_{\\text{oro},i}$ (a $1 \\times 7$ row vector).\n\nThe initial predictions are calculated as the dot product of the predictor vector with the corresponding estimated coefficient vector:\n$$\n\\hat{y}_{\\text{base}, i} = x_{\\text{base}, i} \\hat{\\beta}\n$$\n$$\n\\hat{y}_{\\text{oro}, i} = x_{\\text{oro}, i} \\hat{\\gamma}\n$$\nSince precipitation cannot be negative, these predictions are truncated at zero:\n$$\n\\hat{y}^{+}_{\\text{base}, i} = \\max(0, \\hat{y}_{\\text{base}, i})\n$$\n$$\n\\hat{y}^{+}_{\\text{oro}, i} = \\max(0, \\hat{y}_{\\text{oro}, i})\n$$\nThe final step is to quantify the performance improvement. We first calculate the prediction bias for each model and test case, defined as the predicted value minus the observed value ($y_{\\text{obs},i}$):\n$$\n\\text{bias}_{\\text{base}, i} = \\hat{y}^{+}_{\\text{base}, i} - y_{\\text{obs}, i}\n$$\n$$\n\\text{bias}_{\\text{oro}, i} = \\hat{y}^{+}_{\\text{oro}, i} - y_{\\text{obs}, i}\n$$\nThe fractional bias reduction, $r_i$, is then computed using the formula:\n$$\nr_i = \\frac{|\\text{bias}_{\\text{base},i}| - |\\text{bias}_{\\text{oro},i}|}{|\\text{bias}_{\\text{base},i}|}\n$$\nA special case is handled where if the denominator $|\\text{bias}_{\\text{base},i}|$ is zero, $r_i$ is defined as $0$. The calculated values $r_A$, $r_B$, and $r_C$ are then rounded to four decimal places for the final output. This entire procedure is implemented in the following program.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Performs statistical downscaling of precipitation using two linear models,\n    and quantifies the bias reduction from including orographic predictors.\n    \"\"\"\n    # Define the calibration dataset from the problem statement.\n    # Each entry: (H, q, z, s, u, y)\n    calibration_data = [\n        (5600, 0.004, 800, 10, 0.2, 11.2),\n        (5550, 0.006, 1500, 25, 0.7, 19.0),\n        (5450, 0.008, 2200, 30, 0.9, 26.6),\n        (5700, 0.003, 1200, 15, 0.4, 8.9),\n        (5300, 0.010, 1800, 20, 0.6, 29.2),\n        (5400, 0.007, 1000, 12, 0.3, 20.22),\n        (5580, 0.005, 900, 8, 0.1, 12.96),\n        (5490, 0.0065, 2000, 28, 0.8, 22.43),\n        (5350, 0.009, 2500, 35, 1.0, 32.0),\n        (5620, 0.0045, 1600, 22, 0.5, 14.15),\n        (5500, 0.0068, 1400, 18, 0.55, 19.58),\n        (5380, 0.0075, 2100, 32, 0.85, 27.19),\n    ]\n\n    # Define the test suite.\n    # Each entry: (H, q, z, s, u, y_obs)\n    test_cases = [\n        (5450, 0.007, 1700, 24, 0.7, 22.56),   # Case A\n        (5700, 0.002, 2100, 30, 0.0, 7.1),     # Case B\n        (5350, 0.0095, 2400, 34, 0.95, 32.11)  # Case C\n    ]\n\n    # Prepare calibration data for OLS\n    calib_array = np.array(calibration_data)\n    y_calib = calib_array[:, 5]\n\n    # Construct design matrix for the baseline model\n    # Columns: intercept, H, q\n    X_base_calib = np.vstack([\n        np.ones(len(calib_array)),\n        calib_array[:, 0],\n        calib_array[:, 1]\n    ]).T\n\n    # Construct design matrix for the orographic-augmented model\n    # Columns: intercept, H, q, z, s, u, s*u\n    s_u_interaction = calib_array[:, 3] * calib_array[:, 4]\n    X_oro_calib = np.vstack([\n        np.ones(len(calib_array)),\n        calib_array[:, 0], # H\n        calib_array[:, 1], # q\n        calib_array[:, 2], # z\n        calib_array[:, 3], # s\n        calib_array[:, 4], # u\n        s_u_interaction\n    ]).T\n\n    # Fit the models using OLS (via numpy.linalg.lstsq)\n    # This is numerically robust and equivalent to using the Moore-Penrose pseudoinverse.\n    beta_hat, _, _, _ = np.linalg.lstsq(X_base_calib, y_calib, rcond=None)\n    gamma_hat, _, _, _ = np.linalg.lstsq(X_oro_calib, y_calib, rcond=None)\n    \n    results = []\n    \n    # Process each test case\n    for case in test_cases:\n        H, q, z, s, u, y_obs = case\n\n        # --- Baseline Model Prediction ---\n        x_base_test = np.array([1, H, q])\n        y_pred_base = x_base_test @ beta_hat\n        y_pred_base_truncated = max(0, y_pred_base)\n        bias_base = y_pred_base_truncated - y_obs\n\n        # --- Orographic-Augmented Model Prediction ---\n        s_u_interact_test = s * u\n        x_oro_test = np.array([1, H, q, z, s, u, s_u_interact_test])\n        y_pred_oro = x_oro_test @ gamma_hat\n        y_pred_oro_truncated = max(0, y_pred_oro)\n        bias_oro = y_pred_oro_truncated - y_obs\n\n        # --- Fractional Bias Reduction ---\n        abs_bias_base = abs(bias_base)\n        abs_bias_oro = abs(bias_oro)\n        \n        if abs_bias_base == 0:\n            r = 0.0\n        else:\n            r = (abs_bias_base - abs_bias_oro) / abs_bias_base\n            \n        results.append(round(r, 4))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "While selecting appropriate predictors is crucial, ensuring the statistical validity of the model is equally important, especially when dealing with time series data common in climate science. This practice delves into the critical issue of temporal autocorrelation in model residuals, a common violation of standard regression assumptions. You will learn to diagnose this issue using statistical tests and apply the Prais–Winsten procedure to obtain more efficient and reliable coefficient estimates, a key skill for robust time series modeling. ",
            "id": "4094057",
            "problem": "Consider a statistical downscaling setting where a station temperature time series is modeled as a linear response to large-scale predictors with autoregressive noise. Let the station temperature be denoted by $T_t$ in degrees Celsius, large-scale geopotential height by $Z_t$ in meters, and large-scale specific humidity by $H_t$ in kilograms per kilogram. Assume the linear model\n$$\nT_t \\;=\\; \\beta_0 \\;+\\; \\beta_1 Z_t \\;+\\; \\beta_2 H_t \\;+\\; \\varepsilon_t,\n$$\nwith autoregressive errors of order one (AutoRegressive of order one (AR(1))) given by\n$$\n\\varepsilon_t \\;=\\; \\phi\\,\\varepsilon_{t-1} \\;+\\; u_t,\n$$\nwhere $\\phi$ is the lag-$1$ autoregressive coefficient and $u_t$ is white noise with zero mean and finite variance. Angles appearing in any trigonometric functions must be treated in radians.\n\nYou must implement an Ordinary Least Squares (OLS) fit of the linear model to obtain $\\hat{\\beta}$, test the OLS residuals for lag-$1$ autocorrelation, and then adjust the regression using Feasible Generalized Least Squares (FGLS) under AR(1) errors by applying the Prais–Winsten transformation with an estimated $\\hat{\\phi}$. For autocorrelation testing, compute the Durbin–Watson statistic and the Ljung–Box test statistic at lag $1$, using the chi-square distribution with $1$ degree of freedom to obtain a $p$-value. Declare autocorrelation detected if the $p$-value is less than $0.05$.\n\nTo ensure reproducibility without external randomness, generate $u_t$ deterministically by a linear congruential generator (LCG) for a pseudo-random uniform sequence and rescale it to have a specified target standard deviation. Use the recurrence\n$$\ns_t \\;=\\; (a\\,s_{t-1} + c)\\;\\bmod\\; m,\\quad x_t \\;=\\; \\frac{s_t}{m},\\quad v_t \\;=\\; x_t - \\frac{1}{2},\\quad u_t \\;=\\; \\frac{\\sigma}{\\sqrt{1/12}}\\; v_t,\n$$\nwith $a=16807$, $c=0$, $m=2147483647$, and specified $s_0$ (the seed). The units are as follows: temperature $T_t$ in degrees Celsius, geopotential height $Z_t$ in meters, and specific humidity $H_t$ in kilograms per kilogram. Express all final regression coefficients $\\beta_0$ in degrees Celsius, $\\beta_1$ in degrees Celsius per meter, and $\\beta_2$ in degrees Celsius per kilogram per kilogram. The Durbin–Watson statistic and Ljung–Box $p$-value are unitless.\n\nConstruct the time series using\n$$\nZ_t \\;=\\; z_0 \\;+\\; A_Z \\sin\\!\\left(\\frac{2\\pi t}{12}\\right),\n$$\n$$\nH_t \\;=\\; h_0 \\;+\\; A_H \\cos\\!\\left(\\frac{2\\pi t}{12} + \\theta\\right),\n$$\nwith $t=1,2,\\dots,N$, and the AR(1) recursion $\\varepsilon_t = \\phi\\,\\varepsilon_{t-1} + u_t$ with $\\varepsilon_0 = 0$.\n\nFor each test case, compute:\n- The OLS coefficients $\\hat{\\beta}_0$, $\\hat{\\beta}_1$, $\\hat{\\beta}_2$.\n- The Durbin–Watson statistic $D$ computed from the OLS residuals $r_t$ as\n$$\nD \\;=\\; \\frac{\\sum_{t=2}^{N} (r_t - r_{t-1})^2}{\\sum_{t=1}^{N} r_t^2}.\n$$\n- The Ljung–Box lag-$1$ $p$-value using\n$$\nr_1 \\;=\\; \\frac{\\sum_{t=2}^{N} r_t r_{t-1}}{\\sum_{t=1}^{N} r_t^2},\\quad\nQ \\;=\\; \\frac{N(N+2)}{N-1}\\, r_1^2,\\quad p \\;=\\; 1 - F_{\\chi^2,1}(Q),\n$$\nwhere $F_{\\chi^2,1}$ is the cumulative distribution function of the chi-square distribution with $1$ degree of freedom.\n- The autoregressive parameter estimate\n$$\n\\hat{\\phi} \\;=\\; \\frac{\\sum_{t=2}^{N} r_t r_{t-1}}{\\sum_{t=2}^{N} r_{t-1}^2},\n$$\nclipped to the interval $[-0.99,0.99]$ for numerical stability.\n- The Prais–Winsten FGLS coefficients $\\tilde{\\beta}_0$, $\\tilde{\\beta}_1$, $\\tilde{\\beta}_2$ obtained by transforming the design matrix and response via\n$$\nY_t^* =\n\\begin{cases}\n\\sqrt{1-\\hat{\\phi}^2} Y_1,  t=1 \\\\\nY_t - \\hat{\\phi} Y_{t-1},  t=2,\\dots,N\n\\end{cases}\n\\qquad\nX_t^* =\n\\begin{cases}\n\\sqrt{1-\\hat{\\phi}^2} X_1,  t=1 \\\\\nX_t - \\hat{\\phi} X_{t-1},  t=2,\\dots,N\n\\end{cases}\n$$\nand regressing $Y^*$ on $X^*$ by OLS, where $X_t = [1,\\;Z_t,\\;H_t]$ includes an intercept.\n\nUse the following test suite of parameter sets, each specified by $(N,\\;\\phi,\\;\\beta_0,\\;\\beta_1,\\;\\beta_2,\\;z_0,\\;A_Z,\\;h_0,\\;A_H,\\;\\theta,\\;\\sigma,\\;s_0)$:\n\n- Test case $1$ (general case with moderate autocorrelation): $(N=\\;120,\\;\\phi=\\;0.6,\\;\\beta_0=\\;5.0,\\;\\beta_1=\\;-0.004,\\;\\beta_2=\\;8.0,\\;z_0=\\;1500,\\;A_Z=\\;100,\\;h_0=\\;0.006,\\;A_H=\\;0.002,\\;\\theta=\\;0.7,\\;\\sigma=\\;0.5,\\;s_0=\\;13579)$.\n- Test case $2$ (boundary case with no autocorrelation): $(N=\\;100,\\;\\phi=\\;0.0,\\;\\beta_0=\\;2.0,\\;\\beta_1=\\;0.003,\\;\\beta_2=\\;-4.0,\\;z_0=\\;3000,\\;A_Z=\\;50,\\;h_0=\\;0.012,\\;A_H=\\;0.001,\\;\\theta=\\;1.1,\\;\\sigma=\\;0.4,\\;s_0=\\;24680)$.\n- Test case $3$ (edge case with near-unit-root autocorrelation): $(N=\\;200,\\;\\phi=\\;0.95,\\;\\beta_0=\\;0.0,\\;\\beta_1=\\;0.001,\\;\\beta_2=\\;2.0,\\;z_0=\\;500,\\;A_Z=\\;200,\\;h_0=\\;0.010,\\;A_H=\\;0.003,\\;\\theta=\\;0.3,\\;\\sigma=\\;0.2,\\;s_0=\\;98765)$.\n\nYour program must:\n- Construct $Z_t$, $H_t$, generate $u_t$ via the LCG and $\\varepsilon_t$ via the AR(1) recursion, and then $T_t$ from the specified linear model for each test case.\n- Perform OLS to obtain $\\hat{\\beta}$, compute the Durbin–Watson statistic $D$, compute the Ljung–Box $p$-value, estimate $\\hat{\\phi}$, and perform Prais–Winsten FGLS to obtain $\\tilde{\\beta}$.\n- Produce as output, for each test case in order, the flattened sequence\n$$\n\\left[\\hat{\\beta}_0,\\;\\hat{\\beta}_1,\\;\\hat{\\beta}_2,\\;D,\\;p,\\;\\hat{\\phi},\\;\\tilde{\\beta}_0,\\;\\tilde{\\beta}_1,\\;\\tilde{\\beta}_2\\right],\n$$\nwith all floats rounded to six decimal places. Aggregate the results for all three test cases into a single list by concatenation.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[result1,result2,\\dots]$), where all floats are rounded to six decimal places and booleans, if any, would appear as $True$ or $False$. Angles must be in radians, and all physical quantities must respect the units stated above.",
            "solution": "The problem requires the implementation of a statistical analysis pipeline for a time series model with autoregressive errors. The process involves data generation, model estimation via Ordinary Least Squares (OLS), diagnostic testing for autocorrelation, and model correction using Feasible Generalized Least Squares (FGLS) via the Prais-Winsten transformation. The entire procedure is deterministic to ensure reproducibility.\n\n**1. Data Generation**\n\nThe first step is to construct the time series dataset for each test case. The model is given by:\n$$\nT_t = \\beta_0 + \\beta_1 Z_t + \\beta_2 H_t + \\varepsilon_t\n$$\nwhere $T_t$ is the response variable (temperature) and $Z_t$ and $H_t$ are predictors (geopotential height and specific humidity). The predictors are generated as deterministic sinusoidal functions of time $t=1, 2, \\dots, N$:\n$$\nZ_t = z_0 + A_Z \\sin\\left(\\frac{2\\pi t}{12}\\right)\n$$\n$$\nH_t = h_0 + A_H \\cos\\left(\\frac{2\\pi t}{12} + \\theta\\right)\n$$\nThe error term, $\\varepsilon_t$, follows a first-order autoregressive (AR(1)) process, which introduces serial correlation:\n$$\n\\varepsilon_t = \\phi\\,\\varepsilon_{t-1} + u_t\n$$\nThe initial condition is $\\varepsilon_0 = 0$. The term $u_t$ represents white noise, which is generated deterministically using a Linear Congruential Generator (LCG) to ensure reproducible results. The LCG sequence $s_t$ is defined by $s_t = (a s_{t-1} + c) \\pmod m$ with given parameters $a = 16807$, $c = 0$, $m = 2147483647$, and a specific seed $s_0$. This sequence is then transformed into a sequence $u_t$ with zero mean and a specified standard deviation $\\sigma$. The transformation is:\n$$\nu_t = \\frac{\\sigma}{\\sqrt{1/12}} \\left( \\frac{s_t}{m} - \\frac{1}{2} \\right)\n$$\nThis scaling is correct because a uniform distribution on $[0,1]$ has a variance of $1/12$, and subtracting $1/2$ centers the mean at $0$ without changing the variance.\n\n**2. Ordinary Least Squares (OLS) Estimation**\n\nWe first fit the model using OLS, which assumes the errors $\\varepsilon_t$ are uncorrelated. The model is expressed in matrix form as $Y = X\\beta + \\varepsilon$, where $Y$ is the vector of temperatures $T_t$, and $X$ is the design matrix with columns for the intercept, $Z_t$, and $H_t$. The OLS estimator for $\\beta = [\\beta_0, \\beta_1, \\beta_2]^T$ is:\n$$\n\\hat{\\beta} = (X^T X)^{-1} X^T Y\n$$\nThis provides the initial estimates $(\\hat{\\beta}_0, \\hat{\\beta}_1, \\hat{\\beta}_2)$.\n\n**3. Autocorrelation Diagnostics**\n\nSince the data was generated with AR(1) errors (when $\\phi \\neq 0$), the OLS assumption of uncorrelated errors is violated. This can lead to inefficient coefficient estimates and biased standard errors. We test for this using the OLS residuals, $r_t = T_t - (\\hat{\\beta}_0 + \\hat{\\beta}_1 Z_t + \\hat{\\beta}_2 H_t)$.\n\n- **Durbin-Watson Statistic ($D$)**: This statistic tests for first-order autocorrelation. It is calculated as:\n  $$\n  D = \\frac{\\sum_{t=2}^{N} (r_t - r_{t-1})^2}{\\sum_{t=1}^{N} r_t^2}\n  $$\n  A value of $D \\approx 2$ suggests no autocorrelation, while values approaching $0$ indicate positive autocorrelation, and values approaching $4$ indicate negative autocorrelation.\n\n- **Ljung-Box Test**: This is a more general test. For lag $1$, the test statistic $Q$ is computed from the first-order sample autocorrelation of the residuals, $r_1$:\n  $$\n  r_1 = \\frac{\\sum_{t=2}^{N} r_t r_{t-1}}{\\sum_{t=1}^{N} r_t^2}, \\quad Q = \\frac{N(N+2)}{N-1} r_1^2\n  $$\n  Under the null hypothesis of no autocorrelation, $Q$ follows a chi-square distribution with $1$ degree of freedom ($\\chi^2_1$). The $p$-value is the probability of observing a test statistic as extreme as or more extreme than $Q$, calculated as $p = 1 - F_{\\chi^2,1}(Q)$, where $F$ is the cumulative distribution function (CDF).\n\n**4. Feasible Generalized Least Squares (FGLS) Estimation**\n\nIf autocorrelation is detected, we can apply FGLS to obtain more efficient estimates. The Prais-Winsten method is a FGLS procedure for AR(1) errors.\n\n- **Estimate Autocorrelation Coefficient ($\\hat{\\phi}$)**: First, the AR(1) parameter $\\phi$ is estimated from the OLS residuals by regressing $r_t$ on $r_{t-1}$:\n  $$\n  \\hat{\\phi} = \\frac{\\sum_{t=2}^{N} r_t r_{t-1}}{\\sum_{t=2}^{N} r_{t-1}^2}\n  $$\n  To ensure the stability of the next step, this estimate is clipped to the interval $[-0.99, 0.99]$.\n\n- **Prais-Winsten Transformation**: The original data is transformed to remove the autocorrelation. The transformation quasi-differences the data:\n  $$\n  Y_t^* = Y_t - \\hat{\\phi} Y_{t-1}, \\quad X_t^* = X_t - \\hat{\\phi} X_{t-1} \\quad \\text{for } t=2, \\dots, N\n  $$\n  The first observation ($t=1$) is handled specially to retain its information, weighted by $\\sqrt{1-\\hat{\\phi}^2}$:\n  $$\n  Y_1^* = \\sqrt{1-\\hat{\\phi}^2} Y_1, \\quad X_1^* = \\sqrt{1-\\hat{\\phi}^2} X_1\n  $$\n  The transformed model $Y^* = X^*\\beta + u^*$ now has an error term $u^*$ that is approximately white noise.\n\n- **OLS on Transformed Data**: Finally, OLS is applied to the transformed variables to obtain the FGLS estimator, $\\tilde{\\beta}$:\n  $$\n  \\tilde{\\beta} = ((X^*)^T X^*)^{-1} (X^*)^T Y^*\n  $$\nThis provides the corrected FGLS coefficients $(\\tilde{\\beta}_0, \\tilde{\\beta}_1, \\tilde{\\beta}_2)$, which are generally more efficient than the OLS estimates in the presence of serial correlation. The implementation will execute this entire sequence for each of the three test cases provided.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import chi2\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases.\n    Generates time series data with AR(1) errors, performs OLS and FGLS,\n    and computes autocorrelation diagnostics.\n    \"\"\"\n    test_cases = [\n        # (N, phi, beta0, beta1, beta2, z0, AZ, h0, AH, theta, sigma, s0)\n        (120, 0.6, 5.0, -0.004, 8.0, 1500, 100, 0.006, 0.002, 0.7, 0.5, 13579),\n        (100, 0.0, 2.0, 0.003, -4.0, 3000, 50, 0.012, 0.001, 1.1, 0.4, 24680),\n        (200, 0.95, 0.0, 0.001, 2.0, 500, 200, 0.010, 0.003, 0.3, 0.2, 98765)\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        N, phi, beta0, beta1, beta2, z0, AZ, h0, AH, theta, sigma, s0 = case\n        \n        # 1. Data Generation\n        t = np.arange(1, N + 1)\n        \n        # Generate predictors\n        Z_t = z0 + AZ * np.sin(2 * np.pi * t / 12)\n        H_t = h0 + AH * np.cos(2 * np.pi * t / 12 + theta)\n        \n        # Generate white noise u_t using LCG\n        s = np.zeros(N, dtype=np.int64)\n        s_prev = s0\n        a, c, m = 16807, 0, 2147483647\n        for i in range(N):\n            s_curr = (a * s_prev + c) % m\n            s[i] = s_curr\n            s_prev = s_curr\n        \n        x_t = s / m\n        v_t = x_t - 0.5\n        u_t = (sigma / np.sqrt(1/12)) * v_t\n        \n        # Generate AR(1) errors epsilon_t\n        eps_t = np.zeros(N)\n        eps_t[0] = phi * 0 + u_t[0]  # eps_0 = 0\n        for i in range(1, N):\n            eps_t[i] = phi * eps_t[i-1] + u_t[i]\n            \n        # Generate response variable T_t\n        T_t = beta0 + beta1 * Z_t + beta2 * H_t + eps_t\n        \n        # 2. Ordinary Least Squares (OLS)\n        Y = T_t\n        X = np.c_[np.ones(N), Z_t, H_t]\n        \n        try:\n            beta_hat = np.linalg.inv(X.T @ X) @ X.T @ Y\n        except np.linalg.LinAlgError:\n            # Fallback to pseudo-inverse for near-singular matrices\n            beta_hat = np.linalg.pinv(X.T @ X) @ X.T @ Y\n\n        b0_hat, b1_hat, b2_hat = beta_hat\n        \n        # 3. Autocorrelation Diagnostics\n        residuals = Y - X @ beta_hat\n        \n        # Durbin-Watson statistic\n        dw_num = np.sum((residuals[1:] - residuals[:-1])**2)\n        dw_den = np.sum(residuals**2)\n        D = dw_num / dw_den\n        \n        # Ljung-Box lag-1 test\n        r1_num = np.sum(residuals[1:] * residuals[:-1])\n        r1_den = np.sum(residuals**2)\n        r1 = r1_num / r1_den\n        Q = N * (N + 2) / (N - 1) * r1**2\n        p_value = 1.0 - chi2.cdf(Q, df=1)\n        \n        # 4. Feasible Generalized Least Squares (FGLS)\n        # Estimate phi from OLS residuals\n        phi_hat_num = np.sum(residuals[1:] * residuals[:-1])\n        phi_hat_den = np.sum(residuals[:-1]**2)\n        phi_hat_raw = phi_hat_num / phi_hat_den\n        phi_hat = np.clip(phi_hat_raw, -0.99, 0.99)\n        \n        # Prais-Winsten transformation\n        Y_star = np.zeros_like(Y)\n        X_star = np.zeros_like(X)\n        \n        pw_factor = np.sqrt(1 - phi_hat**2)\n        \n        Y_star[0] = pw_factor * Y[0]\n        X_star[0, :] = pw_factor * X[0, :]\n        \n        Y_star[1:] = Y[1:] - phi_hat * Y[:-1]\n        X_star[1:, :] = X[1:, :] - phi_hat * X[:-1, :]\n        \n        # OLS on transformed data\n        try:\n            beta_tilde = np.linalg.inv(X_star.T @ X_star) @ X_star.T @ Y_star\n        except np.linalg.LinAlgError:\n            beta_tilde = np.linalg.pinv(X_star.T @ X_star) @ X_star.T @ Y_star\n\n        b0_tilde, b1_tilde, b2_tilde = beta_tilde\n        \n        # 5. Collect and format results\n        # The problem asks for the clipped phi_hat to be reported\n        case_results = [\n            b0_hat, b1_hat, b2_hat,\n            D, p_value, phi_hat,\n            b0_tilde, b1_tilde, b2_tilde\n        ]\n        \n        all_results.extend([round(v, 6) for v in case_results])\n\n    print(str(all_results).replace(\" \", \"\"))\n\nsolve()\n```"
        },
        {
            "introduction": "Modern statistical downscaling often employs complex, non-linear models that can be difficult to interpret, acting as \"black boxes.\" This exercise introduces powerful techniques from the field of explainable AI—Partial Dependence plots and Shapley values—to peer inside such models. By applying these methods, you will learn to quantify the contribution of each predictor and diagnose whether the model has learned physically plausible relationships, a vital step for building trust in scientific machine learning applications. ",
            "id": "4093998",
            "problem": "You will analyze a synthetic, physically consistent statistical downscaling model used in numerical weather prediction and climate modeling. The goal is to interpret the model’s predictions using Partial Dependence (PD) and Shapley values, and cross-check the results with physical expectations about precipitation formation.\n\nThe model maps large-scale predictors to local daily precipitation intensity in millimeters per day (mm/day). Predictors are:\n- Specific humidity at $850$ hPa, denoted by $q$, measured in $\\mathrm{kg/kg}$.\n- Upward vertical motion proxy, denoted by $a$, defined as $a = \\max(0, -\\omega)$ where $\\omega$ is vertical pressure velocity at $700$ hPa in $\\mathrm{Pa/s}$ (so $a$ is in $\\mathrm{Pa/s}$ and is non-negative).\n- Orographic slope magnitude, denoted by $s$, dimensionless with range $[0,1]$.\n\nThe downscaling model, a non-linear parametric surrogate for a machine learning model, is defined as\n$$\nf(q,a,s) = \\mathrm{softplus}\\left(\\beta_0 + \\beta_q \\cdot (1000\\,q) + \\beta_a \\cdot (10\\,a) + \\beta_s \\cdot s + \\beta_{\\mathrm{int}} \\cdot (1000\\,q)\\cdot(10\\,a)\\right),\n$$\nwhere $\\mathrm{softplus}(z) = \\ln(1 + e^z)$ and the coefficients are fixed as\n$$\n\\beta_0 = -2.0,\\quad \\beta_q = 0.25,\\quad \\beta_a = 1.2,\\quad \\beta_s = 0.5,\\quad \\beta_{\\mathrm{int}} = 0.03.\n$$\nThe model output $f(q,a,s)$ must be interpreted as daily precipitation intensity expressed in $\\mathrm{mm/day}$.\n\nAssume the joint distribution of $(q,a,s)$ represents a climatologically plausible background state and is used for expectations:\n- $q \\sim \\mathcal{N}(\\mu_q, \\sigma_q^2)$ truncated to the interval $[q_{\\min}, q_{\\max}]$ with $\\mu_q = 0.012\\,\\mathrm{kg/kg}$, $\\sigma_q = 0.003\\,\\mathrm{kg/kg}$, $q_{\\min} = 0.004\\,\\mathrm{kg/kg}$, and $q_{\\max} = 0.020\\,\\mathrm{kg/kg}$.\n- $a \\sim \\mathcal{N}(\\mu_a, \\sigma_a^2)$ truncated to the interval $[a_{\\min}, a_{\\max}]$ with $\\mu_a = 0.05\\,\\mathrm{Pa/s}$, $\\sigma_a = 0.03\\,\\mathrm{Pa/s}$, $a_{\\min} = 0.0\\,\\mathrm{Pa/s}$, and $a_{\\max} = 0.3\\,\\mathrm{Pa/s}$.\n- $s \\sim \\mathrm{Uniform}(0,1)$.\n\nYou must implement the following interpretation functions based on first principles:\n1. Partial Dependence (PD): Partial Dependence (PD) is the expected prediction as one feature is set to a fixed value and others are marginalized over their background distribution. For a feature index $i$ with value $x_i$, define\n$$\n\\mathrm{PD}_i(x_i) = \\mathbb{E}\\left[f\\left(X_1, X_2, X_3\\right) \\mid X_i = x_i\\right],\n$$\nwhere the expectation is with respect to the background distribution of the other features $(X_j)_{j\\neq i}$.\n2. Shapley values: Use Shapley values from cooperative game theory to quantify each feature’s contribution at a particular instance $x = (q,a,s)$. Let the value function for a subset $S \\subseteq \\{1,2,3\\}$ be\n$$\nv(S;x) = \\mathbb{E}\\left[f\\left(x_S, X_{-S}\\right)\\right],\n$$\nwhere $x_S$ denotes fixing the features in $S$ to the instance’s values and $X_{-S}$ denotes sampling the remaining features from the background distribution. The baseline is $v(\\emptyset;x) = \\mathbb{E}[f(X)]$. For $n=3$ features, the Shapley value for feature $i$ is\n$$\n\\phi_i(x) = \\sum_{S \\subseteq N \\setminus \\{i\\}} \\frac{|S|!(n - |S| - 1)!}{n!}\\left[v(S \\cup \\{i\\};x) - v(S;x)\\right],\n$$\nwhere $N=\\{1,2,3\\}$. Use the interventional formulation (features outside $S$ are independently sampled from the background distribution).\n\nAll expectations must be computed numerically using Monte Carlo sampling from the specified background distribution. All quantities of $f$ must be expressed in $\\mathrm{mm/day}$, and comparisons must be made on these units. No angles are involved.\n\nTest Suite and Required Output:\n- Define and evaluate the following cases:\n  1. Monotonicity of $\\mathrm{PD}_q$: evaluate $\\mathrm{PD}_q(q_{\\mathrm{low}})$ and $\\mathrm{PD}_q(q_{\\mathrm{high}})$ at $q_{\\mathrm{low}}=0.006\\,\\mathrm{kg/kg}$ and $q_{\\mathrm{high}}=0.016\\,\\mathrm{kg/kg}$, and return a boolean indicating whether $\\mathrm{PD}_q(q_{\\mathrm{high}})  \\mathrm{PD}_q(q_{\\mathrm{low}})$.\n  2. Monotonicity of $\\mathrm{PD}_a$: evaluate $\\mathrm{PD}_a(a_{\\mathrm{low}})$ and $\\mathrm{PD}_a(a_{\\mathrm{high}})$ at $a_{\\mathrm{low}}=0.02\\,\\mathrm{Pa/s}$ and $a_{\\mathrm{high}}=0.20\\,\\mathrm{Pa/s}$, and return a boolean indicating whether $\\mathrm{PD}_a(a_{\\mathrm{high}})  \\mathrm{PD}_a(a_{\\mathrm{low}})$.\n  3. Shapley efficiency at $x_1=(q,a,s)=(0.016\\,\\mathrm{kg/kg},\\,0.20\\,\\mathrm{Pa/s},\\,0.5)$: compute $\\phi_i(x_1)$ for all features $i$, and check whether\n     $$\n     \\left|\\left(f(x_1) - \\mathbb{E}[f(X)]\\right) - \\sum_{i=1}^3 \\phi_i(x_1)\\right|  \\tau,\n     $$\n     with tolerance $\\tau = 0.05\\,\\mathrm{mm/day}$. Return a boolean.\n  4. Sign consistency of Shapley values at $x_1$: return two booleans indicating $\\phi_q(x_1)  0$ and $\\phi_a(x_1)  0$.\n  5. Positive synergy between $q$ and $a$: compute\n     $$\n     \\Delta_{\\mathrm{syn}} = \\mathbb{E}_s\\left[f(q_{\\mathrm{high}}, a_{\\mathrm{high}}, s) - f(q_{\\mathrm{high}}, a_{\\mathrm{low}}, s) - f(q_{\\mathrm{low}}, a_{\\mathrm{high}}, s) + f(q_{\\mathrm{low}}, a_{\\mathrm{low}}, s)\\right],\n     $$\n     using $q_{\\mathrm{low}}, q_{\\mathrm{high}}, a_{\\mathrm{low}}, a_{\\mathrm{high}}$ as defined above, and return a boolean indicating $\\Delta_{\\mathrm{syn}}  0$. Expectations are over $s \\sim \\mathrm{Uniform}(0,1)$.\n  6. Dryness edge case prediction: compute $f(0.004\\,\\mathrm{kg/kg},\\,0.00\\,\\mathrm{Pa/s},\\,0.2)$ and return the resulting precipitation intensity as a float in $\\mathrm{mm/day}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the exact order:\n$[$case1, case2, case3, case4\\_q, case4\\_a, case5, case6$]$.\nThe first five entries must be booleans, and the last entry must be a float in $\\mathrm{mm/day}$ rounded by standard floating-point representation.",
            "solution": "This problem requires interpreting a non-linear statistical downscaling model using techniques from explainable AI. The core of the task is to numerically estimate expectations via Monte Carlo integration to compute Partial Dependence (PD) and Shapley values.\n\n**1. Model and Background Distribution**\nFirst, we implement the given downscaling model $f(q, a, s)$. A key detail is using a numerically stable version of the $\\mathrm{softplus}(z) = \\ln(1+e^z)$ function, which can be prone to overflow for large $z$. This is typically handled by specialized library functions or by computing it as $z$ for large $z$.\n\nTo compute the necessary expectations, we must sample from the background joint distribution of the predictors $(q, a, s)$. This is done by generating a large number of independent samples for each predictor according to its specified distribution:\n-   $q$ and $a$ are sampled from truncated normal distributions.\n-   $s$ is sampled from a standard uniform distribution.\nThis large set of background samples (e.g., $N=500,000$) will serve as the basis for all Monte Carlo estimations.\n\n**2. Partial Dependence (PD) Calculation**\nTo calculate the partial dependence $\\mathrm{PD}_i(x_i)$, we take the set of background samples, replace the entire column for predictor $i$ with the fixed value $x_i$, and then compute the average of the model's output $f$ over this modified dataset. This process is repeated for the \"low\" and \"high\" values specified for predictors $q$ and $a$ to check for monotonicity, which verifies if the model has learned a physically sensible relationship (e.g., more moisture leads to more precipitation, on average).\n\n**3. Shapley Value Calculation**\nShapley values require a more complex set of expectations. For a single prediction instance $x=(q_1, a_1, s_1)$, we need to compute the \"value\" of every possible coalition (subset) of features, $v(S; x)$. The value function $v(S;x)$ is defined as the expected model output when features in coalition $S$ are fixed to their values in $x$, and all other features are averaged over their background distribution. This is computed using the same Monte Carlo approach as for PD. For $n=3$ features, we must compute $2^3=8$ such expectations:\n-   $v(\\emptyset) = \\mathbb{E}[f(q,a,s)]$ (the baseline or average prediction)\n-   $v(\\{q\\}) = \\mathbb{E}[f(q_1,a,s)]$\n-   ...and so on for all subsets, up to $v(\\{q,a,s\\}) = f(q_1,a_1,s_1)$.\n\nWith these value functions, the Shapley value for each feature can be calculated using the provided combinatorial formula. The results are then used to check two properties:\n-   **Efficiency**: The sum of the Shapley values for all features must equal the difference between the specific prediction $f(x)$ and the baseline average prediction $\\mathbb{E}[f]$. This is a fundamental property that serves as a sanity check on the calculations.\n-   **Sign Consistency**: We check if the contributions of moisture ($q$) and ascent ($a$) are positive, as expected from physical principles.\n\n**4. Synergy and Edge Case Evaluation**\n-   The synergy term $\\Delta_{\\mathrm{syn}}$ is a discrete version of a mixed partial derivative, which quantifies the interaction effect between $q$ and $a$. A positive value indicates that the effect of increasing ascent is greater when moisture is also high, which is physically correct and is captured by the positive interaction coefficient $\\beta_{\\mathrm{int}}$ in the model. This expectation is computed by averaging over samples of the remaining feature, $s$.\n-   The final case is a direct evaluation of the model function $f$ at a specific point corresponding to dry conditions with no vertical motion, testing the model's behavior at an edge of its input space.\n\nThe entire procedure is implemented in the provided Python code, which follows these steps to generate the required boolean and floating-point results.",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import truncnorm\n\ndef solve():\n    \"\"\"\n    Solves the statistical downscaling interpretation problem.\n    \"\"\"\n    # Use a fixed seed for reproducibility of Monte Carlo sampling.\n    np.random.seed(42)\n\n    # --- 1. Define Model and Parameters ---\n\n    # Model coefficients\n    BETA = {\n        '0': -2.0,\n        'q': 0.25,\n        'a': 1.2,\n        's': 0.5,\n        'int': 0.03,\n    }\n\n    def model_f(q, a, s):\n        \"\"\"\n        Computes the precipitation from the downscaling model.\n        Inputs q, a, s can be scalars or numpy arrays.\n        \"\"\"\n        # Scaled predictors\n        q_scaled = 1000.0 * q\n        a_scaled = 10.0 * a\n\n        # Linear combination in the argument of softplus\n        z = (BETA['0'] +\n             BETA['q'] * q_scaled +\n             BETA['a'] * a_scaled +\n             BETA['s'] * s +\n             BETA['int'] * q_scaled * a_scaled)\n\n        # Numerically stable softplus function: log(1 + exp(z))\n        return np.logaddexp(0, z)\n\n    # --- 2. Background Distribution and Sampling ---\n\n    # Number of Monte Carlo samples\n    N_SAMPLES = 500000\n\n    # Parameters for q distribution (truncated normal)\n    q_params = {'mu': 0.012, 'sigma': 0.003, 'min': 0.004, 'max': 0.020}\n    q_a_bound = (q_params['min'] - q_params['mu']) / q_params['sigma']\n    q_b_bound = (q_params['max'] - q_params['mu']) / q_params['sigma']\n    q_dist = truncnorm(a=q_a_bound, b=q_b_bound, loc=q_params['mu'], scale=q_params['sigma'])\n\n    # Parameters for a distribution (truncated normal)\n    a_params = {'mu': 0.05, 'sigma': 0.03, 'min': 0.0, 'max': 0.3}\n    a_a_bound = (a_params['min'] - a_params['mu']) / a_params['sigma']\n    a_b_bound = (a_params['max'] - a_params['mu']) / a_params['sigma']\n    a_dist = truncnorm(a=a_a_bound, b=a_b_bound, loc=a_params['mu'], scale=a_params['sigma'])\n\n    # Generate a single background sample set for all expectations\n    q_samples = q_dist.rvs(N_SAMPLES)\n    a_samples = a_dist.rvs(N_SAMPLES)\n    s_samples = np.random.uniform(0.0, 1.0, N_SAMPLES)\n\n    # --- 3. Expectation Helper Function ---\n\n    def compute_expectation(fixed_vars={}):\n        \"\"\"\n        Computes the expected value of the model f using Monte Carlo,\n        fixing a subset of variables.\n        \"\"\"\n        q_eval = fixed_vars.get('q', q_samples)\n        a_eval = fixed_vars.get('a', a_samples)\n        s_eval = fixed_vars.get('s', s_samples)\n        return np.mean(model_f(q_eval, a_eval, s_eval))\n\n    # --- 4. Evaluate Test Cases ---\n\n    results = []\n\n    # Case 1: Monotonicity of PD_q\n    q_low, q_high = 0.006, 0.016\n    pd_q_low = compute_expectation({'q': q_low})\n    pd_q_high = compute_expectation({'q': q_high})\n    results.append(pd_q_high > pd_q_low)\n\n    # Case 2: Monotonicity of PD_a\n    a_low, a_high = 0.02, 0.20\n    pd_a_low = compute_expectation({'a': a_low})\n    pd_a_high = compute_expectation({'a': a_high})\n    results.append(pd_a_high > pd_a_low)\n\n    # Case 3  4: Shapley Values and Efficiency\n    x1 = {'q': 0.016, 'a': 0.20, 's': 0.5}\n    tau = 0.05\n\n    # Compute all necessary value functions for Shapley calculation\n    v_empty = compute_expectation({}) # E[f(Q,A,S)]\n    v_q = compute_expectation({'q': x1['q']}) # E[f(q,A,S)]\n    v_a = compute_expectation({'a': x1['a']}) # E[f(Q,a,S)]\n    v_s = compute_expectation({'s': x1['s']}) # E[f(Q,A,s)]\n    v_qa = compute_expectation({'q': x1['q'], 'a': x1['a']}) # E[f(q,a,S)]\n    v_qs = compute_expectation({'q': x1['q'], 's': x1['s']}) # E[f(q,A,s)]\n    v_as = compute_expectation({'a': x1['a'], 's': x1['s']}) # E[f(Q,a,s)]\n    v_qas = model_f(x1['q'], x1['a'], x1['s']) # f(q,a,s)\n\n    # Calculate Shapley values using the n=3 formula\n    phi_q = (1/3)*(v_q - v_empty) + (1/6)*(v_qa - v_a) + (1/6)*(v_qs - v_s)\n    phi_a = (1/3)*(v_a - v_empty) + (1/6)*(v_qa - v_q) + (1/6)*(v_as - v_s)\n    phi_s = (1/3)*(v_s - v_empty) + (1/6)*(v_qs - v_q) + (1/6)*(v_as - v_a)\n    \n    # Add second-order interaction terms\n    phi_q += (1/3) * (v_qas - v_as)\n    phi_a += (1/3) * (v_qas - v_qs)\n    phi_s += (1/3) * (v_qas - v_qa)\n\n    # Case 3: Shapley efficiency check\n    shapley_sum = phi_q + phi_a + phi_s\n    model_payout = v_qas - v_empty\n    results.append(np.abs(model_payout - shapley_sum)  tau)\n\n    # Case 4: Sign consistency of Shapley values\n    results.append(phi_q > 0)\n    results.append(phi_a > 0)\n\n    # Case 5: Positive synergy between q and a\n    # Re-using background samples for s for this expectation\n    f_hh = model_f(q_high, a_high, s_samples)\n    f_hl = model_f(q_high, a_low, s_samples)\n    f_lh = model_f(q_low, a_high, s_samples)\n    f_ll = model_f(q_low, a_low, s_samples)\n    delta_syn = np.mean(f_hh - f_hl - f_lh + f_ll)\n    results.append(delta_syn > 0)\n    \n    # Case 6: Dryness edge case prediction\n    q_dry, a_dry, s_dry = 0.004, 0.00, 0.2\n    precip_dry = model_f(q_dry, a_dry, s_dry)\n    results.append(precip_dry)\n\n    # --- 5. Print Final Result ---\n    # The output format is a list of 7 items: 6 booleans and 1 float.\n    # [case1, case2, case3, case4_q, case4_a, case5, case6]\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        }
    ]
}