{
    "hands_on_practices": [
        {
            "introduction": "构建一个完整的伴随模型的第一步是掌握如何为其各个组成部分推导伴随。本练习聚焦于一个简化但真实的辐射传输观测算子，这是同化卫星数据的关键要素。通过这个实践，您将掌握应用伴随算子定义的基本技巧。",
            "id": "4009413",
            "problem": "在用于数值天气预报的变分资料同化的一个简化的一维单色辐射传输设置中，一个观测算子通过关系式 $y = \\exp(-\\kappa q)\\,T$ 将模式状态映射到一个观测的亮度值，其中 $T$ 是一个标量化的亮温，$q$ 是一个标量化的比湿，$\\kappa$ 是一个无量纲吸收系数。假设所有变量都已无量纲化，因此没有单位。考虑单个观测和由对 $(T,q) \\in \\mathbb{R}^{2}$ 组成的相应模式状态，因此观测算子是一个映射 $H: \\mathbb{R}^{2} \\to \\mathbb{R}$，定义为 $H(T,q) = \\exp(-\\kappa q)\\,T$。\n\n仅从 Gateaux 导数（用于切线性算子）的定义和关于 $\\mathbb{R}^{2}$ 和 $\\mathbb{R}$ 上标准欧几里得内积的伴随算子的定义出发，完成以下任务：\n\n1) 推导 $H$ 关于名义状态 $(\\overline{T},\\overline{q})$ 的切线性模型，即给出 $y$ 在扰动方向 $(\\delta T, \\delta q)$ 上的一阶变分的线性映射 $L_{H}(\\overline{T},\\overline{q})[\\delta T, \\delta q]$。\n\n2) 使用关于欧几里得内积的伴随定义，推导切线性算子在给定观测空间残差 $r \\in \\mathbb{R}$ 上的伴随作用，即确定唯一的 $(\\nabla_{T}, \\nabla_{q}) \\in \\mathbb{R}^{2}$，使得对于所有 $(\\delta T, \\delta q) \\in \\mathbb{R}^{2}$，欧几里得内积满足 $\\langle L_{H}(\\overline{T},\\overline{q})[\\delta T, \\delta q], r \\rangle_{\\mathbb{R}} = \\langle (\\delta T, \\delta q), (\\nabla_{T}, \\nabla_{q}) \\rangle_{\\mathbb{R}^{2}}$。\n\n3) 这种伴随作用是基于伴随的预报对观测的敏感性（FSO）分析中使用的核心对象，它将观测空间的信息映射回模式空间的敏感度。在 $\\overline{T} = 1.3$，$\\overline{q} = 0.2$，$\\kappa = 2$ 和 $r = 0.6$ 的条件下，对您的伴随表达式进行数值计算。将最终数值答案以包含 $(\\nabla_{T}, \\nabla_{q})$ 的单个行向量的形式报告，并四舍五入到四位有效数字。所有量都是无量纲的；答案中不要包含单位。",
            "solution": "问题陈述已经过仔细验证，被认为是具有科学依据、适定且客观的。它提出了一个在辐射传输和变分资料同化中的标准但简化的情景。所有定义和参数均已提供，从而可以得出一个唯一且有意义的解。因此，我们可以着手进行推导。\n\n问题要求完成关于观测算子 $H: \\mathbb{R}^{2} \\to \\mathbb{R}$（定义为 $H(T,q) = \\exp(-\\kappa q) T$）的三个不同但相关的任务。状态向量为 $(T, q)$，观测为 $y$。\n\n1) 推导切线性模型。\n\n$H$ 在名义状态 $(\\overline{T},\\overline{q})$ 的切线性模型是 $H$ 在该点的 Gateaux 导数。令状态由向量 $x = (T,q)$ 表示，扰动由 $\\delta x = (\\delta T, \\delta q)$ 表示。在方向 $\\delta x$ 上的 Gateaux 导数定义为：\n$$L_{H}(\\overline{x})[\\delta x] = \\lim_{\\epsilon \\to 0} \\frac{H(\\overline{x} + \\epsilon \\delta x) - H(\\overline{x})}{\\epsilon} = \\frac{d}{d\\epsilon} \\left[ H(\\overline{x} + \\epsilon \\delta x) \\right]_{\\epsilon=0}$$\n代入 $H$ 的显式形式以及向量 $\\overline{x} = (\\overline{T}, \\overline{q})$ 和 $\\delta x = (\\delta T, \\delta q)$：\n$$L_{H}(\\overline{T},\\overline{q})[\\delta T, \\delta q] = \\frac{d}{d\\epsilon} \\left[ H(\\overline{T} + \\epsilon\\delta T, \\overline{q} + \\epsilon\\delta q) \\right]_{\\epsilon=0}$$\n$$L_{H}(\\overline{T},\\overline{q})[\\delta T, \\delta q] = \\frac{d}{d\\epsilon} \\left[ \\exp(-\\kappa(\\overline{q} + \\epsilon\\delta q))(\\overline{T} + \\epsilon\\delta T) \\right]_{\\epsilon=0}$$\n我们对关于 $\\epsilon$ 的微分应用乘法法则：\n$$L_{H}(\\overline{T},\\overline{q})[\\delta T, \\delta q] = \\left[ \\frac{d}{d\\epsilon}\\exp(-\\kappa\\overline{q} - \\kappa\\epsilon\\delta q) \\right]_{\\epsilon=0} (\\overline{T} + 0 \\cdot \\delta T) + \\left[ \\exp(-\\kappa\\overline{q} - 0 \\cdot \\kappa\\delta q) \\right] \\left[ \\frac{d}{d\\epsilon}(\\overline{T} + \\epsilon\\delta T) \\right]_{\\epsilon=0}$$\n进行微分运算：\n$$ \\frac{d}{d\\epsilon}\\exp(-\\kappa\\overline{q} - \\kappa\\epsilon\\delta q) = \\exp(-\\kappa\\overline{q} - \\kappa\\epsilon\\delta q) \\cdot (-\\kappa\\delta q) $$\n$$ \\frac{d}{d\\epsilon}(\\overline{T} + \\epsilon\\delta T) = \\delta T $$\n在 $\\epsilon = 0$ 处求值：\n$$L_{H}(\\overline{T},\\overline{q})[\\delta T, \\delta q] = \\left( \\exp(-\\kappa\\overline{q}) \\cdot (-\\kappa\\delta q) \\right) \\overline{T} + \\exp(-\\kappa\\overline{q}) \\cdot \\delta T$$\n整理各项，得到切线性模型的最终表达式：\n$$L_{H}(\\overline{T},\\overline{q})[\\delta T, \\delta q] = \\exp(-\\kappa\\overline{q}) \\delta T - \\kappa\\overline{T}\\exp(-\\kappa\\overline{q}) \\delta q$$\n这是一个作用于扰动向量 $(\\delta T, \\delta q)$ 的线性算子。\n\n2) 推导伴随作用。\n\n切线性算子 $L_H$ 的伴随（我们记为 $L_H^*$）由内积关系定义。对于任何状态空间扰动 $(\\delta T, \\delta q) \\in \\mathbb{R}^{2}$ 和任何观测空间残差 $r \\in \\mathbb{R}$，伴随作用 $(\\nabla_{T}, \\nabla_{q}) = L_H^*[r]$ 必须满足：\n$$\\langle L_{H}(\\overline{T},\\overline{q})[\\delta T, \\delta q], r \\rangle_{\\mathbb{R}} = \\langle (\\delta T, \\delta q), (\\nabla_{T}, \\nabla_{q}) \\rangle_{\\mathbb{R}^{2}}$$\n在 $\\mathbb{R}$ 上的内积是标准乘法，在 $\\mathbb{R}^{2}$ 上是标准点积。让我们使用第1部分的结果来计算方程的左侧 (LHS)：\n$$\\text{LHS} = (L_{H}(\\overline{T},\\overline{q})[\\delta T, \\delta q]) \\cdot r$$\n$$\\text{LHS} = \\left( \\exp(-\\kappa\\overline{q})\\delta T - \\kappa\\overline{T}\\exp(-\\kappa\\overline{q})\\delta q \\right) r$$\n$$\\text{LHS} = r\\exp(-\\kappa\\overline{q})\\delta T - r\\kappa\\overline{T}\\exp(-\\kappa\\overline{q})\\delta q$$\n现在，我们计算定义的右侧 (RHS)：\n$$\\text{RHS} = (\\delta T)(\\nabla_{T}) + (\\delta q)(\\nabla_{q})$$\n为使等式 $\\text{LHS} = \\text{RHS}$ 对所有可能的扰动 $(\\delta T, \\delta q)$ 成立，两边 $\\delta T$ 和 $\\delta q$ 的系数必须相等。\n通过比较 $\\delta T$ 的系数：\n$$\\nabla_{T} = r\\exp(-\\kappa\\overline{q})$$\n通过比较 $\\delta q$ 的系数：\n$$\\nabla_{q} = -r\\kappa\\overline{T}\\exp(-\\kappa\\overline{q})$$\n因此，切线性算子在残差 $r$ 上的伴随作用将其映射到状态空间向量 $(\\nabla_{T}, \\nabla_{q})$：\n$$(\\nabla_{T}, \\nabla_{q}) = \\left( r\\exp(-\\kappa\\overline{q}), -r\\kappa\\overline{T}\\exp(-\\kappa\\overline{q}) \\right)$$\n该向量表示观测相对于模式状态变量的梯度，并由观测残差加权；它对于变分同化和敏感性研究至关重要。\n\n3) 数值计算。\n\n给定以下无量纲值：$\\overline{T} = 1.3$，$\\overline{q} = 0.2$，$\\kappa = 2$ 和 $r = 0.6$。我们将这些值代入 $\\nabla_{T}$ 和 $\\nabla_{q}$ 的表达式中。\n首先，我们计算公共的指数项：\n$$\\exp(-\\kappa\\overline{q}) = \\exp(-2 \\times 0.2) = \\exp(-0.4)$$\n现在，我们计算 $\\nabla_{T}$：\n$$\\nabla_{T} = r\\exp(-\\kappa\\overline{q}) = 0.6 \\times \\exp(-0.4)$$\n$$\\nabla_{T} \\approx 0.6 \\times 0.670320045... = 0.402192027...$$\n接下来，我们计算 $\\nabla_{q}$：\n$$\\nabla_{q} = -r\\kappa\\overline{T}\\exp(-\\kappa\\overline{q}) = -(0.6)(2)(1.3)\\exp(-0.4)$$\n$$\\nabla_{q} = -1.56 \\times \\exp(-0.4)$$\n$$\\nabla_{q} \\approx -1.56 \\times 0.670320045... = -1.04569927...$$\n按要求将这些结果四舍五入到四位有效数字：\n$$\\nabla_{T} \\approx 0.4022$$\n$$\\nabla_{q} \\approx -1.046$$\n得到的伴随向量是 $(\\nabla_{T}, \\nabla_{q}) = (0.4022, -1.046)$。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.4022 & -1.046\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "在掌握了单个组件的伴随推导后，下一步是为整个随时间演变的预报模型将它们链接起来。本练习要求您编写代码，为一个简单的非线性模型实现伴随，并通过将其输出与有限差分近似进行比较来执行至关重要的“伴随检验”。这个过程揭示了在实际数值天气预报中遇到的实践挑战和验证技术。",
            "id": "4009415",
            "problem": "考虑一个离散时间预报模型，它在数值天气预报中用作基于伴随的敏感性分析的一个教学代理。目标是针对一个非线性双分量系统，计算并比较基于伴随的敏感度与有限差分敏感度。该敏感性分析机制是“预报对观测的敏感性”（FSO）的基础，FSO 用于将预报误差的减少归因于单个观测。\n\n给定一个二维状态向量 $x \\in \\mathbb{R}^{2}$，其分量为 $x = [x_1, x_2]^{\\top}$，以及一个非线性、耗散、耦合的右端项 $f(x)$，其分量定义如下：\n$$\nf_1(x) = -\\beta x_1 - \\gamma x_1^{3} + \\kappa x_2^{2}, \\quad\nf_2(x) = -\\beta x_2 - \\gamma x_2^{3} - \\kappa x_1 x_2.\n$$\n预报模型为显式欧拉时间离散化：\n$$\nx_{k+1} = x_k + \\Delta t \\, f(x_k), \\quad k = 0,1,\\dots,N-1,\n$$\n其中 $x_0 \\in \\mathbb{R}^{2}$ 是给定的初始条件，$N$ 是预报时效。\n\n定义一个标量预报度量（成本函数），它仅依赖于末时刻的第一个分量，\n$$\nJ(x_0) = \\tfrac{1}{2} \\left( H x_N - y \\right)^{2}, \\quad H = \\begin{bmatrix}1  0\\end{bmatrix},\n$$\n其中 $y \\in \\mathbb{R}$ 是一个指定的目标值。所有量都是无量纲的；答案中无需物理单位。\n\n任务：\n- 从链式法则和离散模型更新映射的定义出发，推导离散伴随递推关系，以获得梯度 $\\nabla_{x_0} J$，过程中不使用有限差分。\n- 实现基于伴随的梯度和 $\\nabla_{x_0} J$ 的中心差分近似：\n  - 对于沿第 $i$ 个标准基向量 $e_i$ 方向的中心差分，使用\n    $$\n    \\left[\\nabla_{x_0} J\\right]_i \\approx \\frac{J(x_0 + h e_i) - J(x_0 - h e_i)}{2 h}.\n    $$\n- 通过相对 $\\ell_2$ 误差来量化基于伴随的梯度和有限差分梯度之间的差异\n  $$\n  E(h) = \\frac{\\left\\|\\nabla^{\\text{adj}} J - \\nabla^{\\text{FD}} J\\right\\|_2}{\\max\\left(\\left\\|\\nabla^{\\text{adj}} J\\right\\|_2, \\epsilon_{\\min}\\right)},\n  $$\n  其中 $\\epsilon_{\\min}$ 是一个小的正常数，用以防止除以零。\n\n- 此外，通过将 $g^{\\top} v$ 与中心差分方向导数进行比较，来验证一个方向导数恒等式，其中 $g = \\nabla^{\\text{adj}} J$，$v \\in \\mathbb{R}^{2}$ 是一个指定方向。\n\n对模型和成本函数使用以下固定参数：\n- 初始条件 $x_0 = \\begin{bmatrix}0.7 \\\\ -0.5\\end{bmatrix}$。\n- 系数 $\\beta = 0.4$，$\\gamma = 0.5$，$\\kappa = 0.3$。\n- 时间步长 $\\Delta t = 0.05$，预报步数 $N = 200$。\n- 观测算子 $H = \\begin{bmatrix}1  0\\end{bmatrix}$，目标值 $y = -1$（除非另有说明）。\n- 在相对误差计算中使用 $\\epsilon_{\\min} = 10^{-15}$。\n\n实现一个程序，执行以下测试套件并返回指定的输出：\n- 测试用例 A（正常路径）：使用中心差分步长 $h = 10^{-6}$ 计算 $E(h)$，结果为浮点数。\n- 测试用例 B（粗糙有限差分）：使用 $h = 10^{-2}$ 计算 $E(h)$，结果为浮点数。\n- 测试用例 C（舍入误差显著）：使用 $h = 10^{-10}$ 计算 $E(h)$，结果为浮点数。\n- 测试用例 D（方向导数检验）：使用 $h = 10^{-6}$ 和方向 $v = \\begin{bmatrix}1 \\\\ -2\\end{bmatrix}$（归一化为单位长度），计算相对差异\n  $$\n  \\frac{\\left|g^{\\top} v - \\frac{J(x_0 + h v) - J(x_0 - h v)}{2 h}\\right|}{\\max\\left(\\left|g^{\\top} v\\right|, \\epsilon_{\\min}\\right)},\n  $$\n  并返回一个布尔值，指示该差异是否小于 $10^{-7}$。\n- 测试用例 E（零失配边界条件）：使用给定 $x_0$ 的正向模型设置 $y = H x_N$。计算基于伴随的梯度，并返回其 $\\ell_2$ 范数，结果为浮点数。\n\n你的程序应生成单行输出，其中包含用方括号括起来的逗号分隔的结果列表（例如，“[resultA,resultB,resultC,resultD,resultE]”）。结果必须按测试 A、B、C、D、E 的顺序排列。\n\n科学真实性与推导约束：\n- 从基本原理出发进行推导：复合映射的链式法则、显式欧拉更新的定义，以及终端成本梯度的定义。\n- 不要使用任何预先推导的伴随公式或黑箱自动微分；相反，应从第一性原理推导离散伴随递推关系。\n- 确保对于给定的参数，实现是数值稳定且自洽的。\n\n解释要求：\n- 在你的解决方案中，解释为什么对于 $h = 10^{-2}$、$h = 10^{-6}$ 和 $h = 10^{-10}$ 这几种选择，误差 $E(h)$ 的行为会有所不同，并关联到截断误差和舍入误差，同时将伴随梯度的作用与“预报对观测的敏感性”（FSO）的概念联系起来。",
            "solution": "该问题要求推导并实现一种基于伴随的方法，用以计算一个预报度量相对于一个离散时间非线性动力系统初始条件的敏感性。此结果将与有限差分近似进行比较。\n\n### 问题验证\n\n**步骤 1：提取已知条件**\n\n*   **状态向量：** $x = [x_1, x_2]^{\\top} \\in \\mathbb{R}^{2}$\n*   **非线性动力学函数 $f(x)$：**\n    *   $f_1(x) = -\\beta x_1 - \\gamma x_1^{3} + \\kappa x_2^{2}$\n    *   $f_2(x) = -\\beta x_2 - \\gamma x_2^{3} - \\kappa x_1 x_2$\n*   **预报模型（显式欧拉）：** $x_{k+1} = x_k + \\Delta t \\, f(x_k)$，其中 $k = 0,1,\\dots,N-1$。\n*   **标量成本函数：** $J(x_0) = \\tfrac{1}{2} \\left( H x_N - y \\right)^{2}$，其中 $H = \\begin{bmatrix}1  0\\end{bmatrix}$。\n*   **固定参数：**\n    *   初始条件：$x_0 = \\begin{bmatrix}0.7 \\\\ -0.5\\end{bmatrix}$\n    *   系数：$\\beta = 0.4$, $\\gamma = 0.5$, $\\kappa = 0.3$\n    *   时间步长：$\\Delta t = 0.05$\n    *   预报步数：$N = 200$\n    *   目标值：$y = -1$（测试 E 除外）\n    *   分母下限：$\\epsilon_{\\min} = 10^{-15}$\n*   **任务：**\n    1.  推导 $\\nabla_{x_0} J$ 的离散伴随递推关系。\n    2.  实现伴随方法和 $\\nabla_{x_0} J$ 的中心差分近似。\n    3.  对于 $h \\in \\{10^{-6}, 10^{-2}, 10^{-10}\\}$，计算相对误差 $E(h) = \\frac{\\left\\|\\nabla^{\\text{adj}} J - \\nabla^{\\text{FD}} J\\right\\|_2}{\\max\\left(\\left\\|\\nabla^{\\text{adj}} J\\right\\|_2, \\epsilon_{\\min}\\right)}$。\n    4.  使用 $v = \\begin{bmatrix}1 \\\\ -2\\end{bmatrix}$（归一化）和 $h = 10^{-6}$ 验证方向导数恒等式。\n    5.  对于 $y = H x_N$ 的情况，计算基于伴随的梯度的 $\\ell_2$ 范数。\n\n**步骤 2：使用提取的已知条件进行验证**\n\n*   **科学上成立：** 该问题描述了数值分析、优化和数据同化领域的一个标准设置。它使用了一个通过显式欧拉方法（一种标准数值格式）离散化的非线性动力系统（一个有效的数学对象）。基于伴随的敏感性分析概念是这些领域的基石。该问题在科学和数学上是合理的。\n*   **适定性：** 该问题定义清晰。方程组、参数、初始条件和成本函数都已明确给出。任务具体，并能得出唯一可计算的结果。\n*   **客观性：** 该问题使用精确的数学语言陈述，没有主观性或模糊性。\n\n**步骤 3：结论与行动**\n\n该问题是有效的。它是一个适定的、科学上合理的练习，旨在实现和理解伴随模型。\n\n### 离散伴随模型的推导\n\n目标是计算成本函数 $J$ 相对于初始状态 $x_0$ 的梯度，记为 $\\nabla_{x_0} J$。成本函数 $J$ 通过一系列模型更新依赖于 $x_0$：\n$x_0 \\xrightarrow{M_0} x_1 \\xrightarrow{M_1} \\cdots \\xrightarrow{M_{N-1}} x_N$，其中 $M_k(x) = x + \\Delta t f(x)$。\n因此，$J$ 是一个复合函数：$J(x_0) = \\mathcal{J}(M_{N-1}(M_{N-2}(\\cdots M_0(x_0)\\cdots)))$，其中 $\\mathcal{J}(x_N) = \\frac{1}{2}(Hx_N - y)^2$。\n\n我们从 $J$ 相对于初始状态扰动 $dx_0$ 的全微分开始：\n$$\ndJ = (\\nabla_{x_0} J)^\\top dx_0\n$$\n使用链式法则，我们可以将成本函数的微分与最终状态的微分 $dx_N$ 联系起来：\n$$\ndJ = (\\nabla_{x_N} \\mathcal{J})^\\top dx_N\n$$\n扰动 $dx_N$ 通过模型更新步骤 $x_N = M_{N-1}(x_{N-1})$ 的线性化与 $dx_{N-1}$ 相关联：\n$$\ndx_N = \\frac{\\partial M_{N-1}}{\\partial x_{N-1}} \\bigg|_{x_{N-1}} dx_{N-1} = \\mathbf{M}_{N-1} dx_{N-1}\n$$\n矩阵 $\\mathbf{M}_k = \\frac{\\partial x_{k+1}}{\\partial x_k}$ 是第 $k$ 步的切线性模型（TLM）传播算子。对于显式欧拉格式，它为：\n$$\n\\mathbf{M}_k = \\frac{\\partial}{\\partial x_k} \\left(x_k + \\Delta t f(x_k)\\right) = \\mathbf{I} + \\Delta t \\frac{\\partial f}{\\partial x}\\bigg|_{x_k}\n$$\n其中 $\\mathbf{I}$ 是单位矩阵。将此关系从 $k=N-1$ 递归应用到 $k=0$：\n$$\ndx_N = \\mathbf{M}_{N-1} \\mathbf{M}_{N-2} \\cdots \\mathbf{M}_0 dx_0\n$$\n将此代入 $dJ$ 的表达式中：\n$$\ndJ = (\\nabla_{x_N} \\mathcal{J})^\\top (\\mathbf{M}_{N-1} \\mathbf{M}_{N-2} \\cdots \\mathbf{M}_0) dx_0\n$$\n通过与 $dJ = (\\nabla_{x_0} J)^\\top dx_0$ 进行比较，我们确定梯度为：\n$$\n(\\nabla_{x_0} J)^\\top = (\\nabla_{x_N} \\mathcal{J})^\\top \\mathbf{M}_{N-1} \\mathbf{M}_{N-2} \\cdots \\mathbf{M}_0\n$$\n对两边取转置：\n$$\n\\nabla_{x_0} J = (\\mathbf{M}_0)^\\top (\\mathbf{M}_1)^\\top \\cdots (\\mathbf{M}_{N-1})^\\top \\nabla_{x_N} \\mathcal{J}\n$$\n伴随方法通过定义一系列伴随状态向量 $\\lambda_k \\in \\mathbb{R}^2$ 来高效地计算这个乘积。令终端伴随状态为成本函数相对于最终模型状态的梯度：\n$$\n\\lambda_N = \\nabla_{x_N} \\mathcal{J}\n$$\n然后，通过将此敏感性随时间向后传播来定义伴随递推：\n$$\n\\lambda_k = (\\mathbf{M}_k)^\\top \\lambda_{k+1} \\quad \\text{for } k = N-1, N-2, \\dots, 0\n$$\n展开这个递推关系表明 $\\lambda_0 = (\\mathbf{M}_0)^\\top \\cdots (\\mathbf{M}_{N-1})^\\top \\lambda_N$，这正是 $\\nabla_{x_0} J$ 的表达式。\n\n计算基于伴随的梯度的步骤是：\n1.  **正向积分：** 给定 $x_0$，使用规则 $x_{k+1} = x_k + \\Delta t f(x_k)$ 计算并存储完整的状态轨迹 $\\{x_k\\}_{k=0}^N$。\n2.  **伴随初始化：** 计算终端伴随状态 $\\lambda_N$。\n    $$\n    J = \\tfrac{1}{2}(x_{1,N} - y)^2 \\implies \\lambda_N = \\nabla_{x_N} J = \\begin{bmatrix} \\partial J/\\partial x_{1,N} \\\\ \\partial J/\\partial x_{2,N} \\end{bmatrix} = \\begin{bmatrix} x_{1,N} - y \\\\ 0 \\end{bmatrix} = \\mathbf{H}^\\top(\\mathbf{H}x_N - y)\n    $$\n3.  **反向递推：** 从 $k=N-1$ 向后迭代到 $0$：\n    *   在存储的状态 $x_k$ 处评估 $f$ 的雅可比矩阵：$\\mathbf{F}_k = \\frac{\\partial f}{\\partial x}|_{x_k}$。\n    *   更新伴随状态：$\\lambda_k = (\\mathbf{I} + \\Delta t \\mathbf{F}_k)^\\top \\lambda_{k+1}$。\n4.  **结果：** 梯度是初始伴随状态，$\\nabla_{x_0} J = \\lambda_0$。\n\n此过程需要 $f$ 的雅可比矩阵：\n$$\n\\frac{\\partial f}{\\partial x} = \\begin{bmatrix} \\frac{\\partial f_1}{\\partial x_1} & \\frac{\\partial f_1}{\\partial x_2} \\\\ \\frac{\\partial f_2}{\\partial x_1} & \\frac{\\partial f_2}{\\partial x_2} \\end{bmatrix} = \\begin{bmatrix} -\\beta - 3\\gamma x_1^2 & 2\\kappa x_2 \\\\ -\\kappa x_2 & -\\beta - 3\\gamma x_2^2 - \\kappa x_1 \\end{bmatrix}\n$$\n\n### 结果解释\n\n**有限差分误差行为 ($E(h)$):**\n伴随梯度和有限差分近似之间的相对误差 $E(h)$ 随步长 $h$ 的变化而变化，这是由于两种数值误差的相互作用：\n\n*   **截断误差：** 中心差分公式是从泰勒级数展开推导出的近似，它截断了高阶项。主导误差项的阶数为 $O(h^2)$。对于像 $h=10^{-2}$ 这样的大步长，这是主要的误差来源。误差很大是因为近似本身很粗糙。\n*   **舍入误差：** 数字计算机具有有限的精度（对于双精度，机器精度 $\\epsilon_m \\approx 10^{-16}$）。当 $h$ 非常小，如 $h=10^{-10}$ 时，有限差分公式的分子 $J(x_0+hv) - J(x_0-hv)$ 涉及两个几乎相等的数相减。这会导致灾难性抵消，或有效数字的损失。这个微小且充满误差的结果再被一个非常小的数（$2h$）除，从而放大了相对误差。\n*   **最佳步长：** 总误差是这两个分量的和，大致为 $Error(h) \\approx A h^2 + B \\epsilon_m/h$。存在一个使该和最小化的最佳 $h$。对于双精度，这个最佳值通常在 $10^{-8}$ 到 $10^{-5}$ 的范围内。$h=10^{-6}$ 的选择接近这个最佳值，因此在三个测试用例中产生了最小的误差，因为它平衡了截断误差和舍入误差的影响。\n\n**与预报对观测的敏感性（FSO）的联系：**\n伴随梯度 $\\nabla_{x_0} J$ 量化了预报结果（度量 $J$）对初始条件 $x_0$ 无穷小变化的敏感性。这是数值天气预报中 FSO 背后的核心概念。\n\n在现实世界的情景中，预报的初始条件 $x_0$ 是“分析场”，它由数据同化系统生成，该系统将先前的短期预报（“背景场”）与新的观测资料融合。一个典型的预报度量 $J$ 衡量的是一个较长期（例如24小时）预报相对于验证分析场或观测的误差。\n\n伴随模型通过计算 $\\nabla_{x_0} J$，将预报误差的敏感性从预报检验时刻反向传播到分析时刻（$t=0$）。这个梯度告诉我们初始分析场的哪些方面对预报误差影响最大。FSO 技术使用这个梯度将敏感性投影到观测空间，从而估计每个独立观测对预报准确率的影响。本质上，本问题中执行的计算——对初始条件的敏感性——是完整的 FSO 诊断的基本构建模块。\n\n```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the adjoint sensitivity analysis problem.\n    \"\"\"\n    \n    # --- Define model parameters and constants ---\n    x0_val = np.array([0.7, -0.5])\n    beta = 0.4\n    gamma = 0.5\n    kappa = 0.3\n    dt = 0.05\n    N = 200\n    H = np.array([1.0, 0.0])\n    y_default = -1.0\n    eps_min = 1e-15\n\n    # --- Define core model functions ---\n    \n    def f(x):\n        \"\"\"The nonlinear right-hand side of the model.\"\"\"\n        x1, x2 = x\n        f1 = -beta * x1 - gamma * x1**3 + kappa * x2**2\n        f2 = -beta * x2 - gamma * x2**3 - kappa * x1 * x2\n        return np.array([f1, f2])\n\n    def jacobian_f(x):\n        \"\"\"The Jacobian of the nonlinear function f.\"\"\"\n        x1, x2 = x\n        df1_dx1 = -beta - 3 * gamma * x1**2\n        df1_dx2 = 2 * kappa * x2\n        df2_dx1 = -kappa * x2\n        df2_dx2 = -beta - 3 * gamma * x2**2 - kappa * x1\n        return np.array([[df1_dx1, df1_dx2], [df2_dx1, df2_dx2]])\n\n    def run_forward_model(x_start, n_steps=N, time_step=dt):\n        \"\"\"\n        Runs the explicit Euler forward model and returns the full trajectory.\n        \"\"\"\n        traj = np.zeros((n_steps + 1, 2))\n        traj[0] = x_start\n        x_k = x_start.copy()\n        for k in range(n_steps):\n            x_k = x_k + time_step * f(x_k)\n            traj[k + 1] = x_k\n        return traj\n\n    def compute_cost(x_final, h_op=H, y_target=y_default):\n        \"\"\"Computes the scalar cost function J.\"\"\"\n        return 0.5 * (np.dot(h_op, x_final) - y_target)**2\n\n    def compute_adjoint_gradient(x0, n_steps=N, time_step=dt, h_op=H, y_target=y_default):\n        \"\"\"\n        Computes the gradient of J w.r.t. x0 using the adjoint method.\n        \"\"\"\n        # 1. Forward run: Integrate model and store trajectory\n        trajectory = run_forward_model(x0, n_steps, time_step)\n        x_N = trajectory[-1]\n\n        # 2. Adjoint initialization: Gradient of J w.r.t. x_N\n        adj_k_plus_1 = h_op.T * (np.dot(h_op, x_N) - y_target)\n\n        # 3. Backward run: Adjoint recursion\n        for k in range(n_steps - 1, -1, -1):\n            x_k = trajectory[k]\n            F_k = jacobian_f(x_k)\n            # M_k^T = (I + dt * F_k)^T\n            M_k_T = np.identity(2) + time_step * F_k.T\n            adj_k = M_k_T @ adj_k_plus_1\n            adj_k_plus_1 = adj_k\n        \n        # 4. Result is the initial adjoint state\n        grad_adj = adj_k_plus_1\n        return grad_adj\n\n    def compute_fd_gradient(x0, h_step, n_steps=N, time_step=dt, h_op=H, y_target=y_default):\n        \"\"\"\n        Computes the gradient of J w.r.t. x0 using central differences.\n        \"\"\"\n        grad_fd = np.zeros_like(x0, dtype=float)\n        for i in range(len(x0)):\n            ei = np.zeros_like(x0, dtype=float)\n            ei[i] = 1.0\n            \n            x0_plus_h = x0 + h_step * ei\n            traj_plus = run_forward_model(x0_plus_h, n_steps, time_step)\n            J_plus = compute_cost(traj_plus[-1], h_op, y_target)\n            \n            x0_minus_h = x0 - h_step * ei\n            traj_minus = run_forward_model(x0_minus_h, n_steps, time_step)\n            J_minus = compute_cost(traj_minus[-1], h_op, y_target)\n            \n            grad_fd[i] = (J_plus - J_minus) / (2 * h_step)\n        return grad_fd\n\n    # --- Execute Test Cases ---\n    \n    results = []\n    \n    # Pre-compute the adjoint gradient for tests A, B, C, D\n    grad_adj_base = compute_adjoint_gradient(x0_val, y_target=y_default)\n\n    # Test Case A: Happy path, h = 1e-6\n    h_A = 1e-6\n    grad_fd_A = compute_fd_gradient(x0_val, h_A, y_target=y_default)\n    norm_adj = np.linalg.norm(grad_adj_base)\n    err_A = np.linalg.norm(grad_adj_base - grad_fd_A) / max(norm_adj, eps_min)\n    results.append(err_A)\n\n    # Test Case B: Coarse finite difference, h = 1e-2\n    h_B = 1e-2\n    grad_fd_B = compute_fd_gradient(x0_val, h_B, y_target=y_default)\n    err_B = np.linalg.norm(grad_adj_base - grad_fd_B) / max(norm_adj, eps_min)\n    results.append(err_B)\n\n    # Test Case C: Round-off stressed, h = 1e-10\n    h_C = 1e-10\n    grad_fd_C = compute_fd_gradient(x0_val, h_C, y_target=y_default)\n    err_C = np.linalg.norm(grad_adj_base - grad_fd_C) / max(norm_adj, eps_min)\n    results.append(err_C)\n\n    # Test Case D: Directional derivative check\n    h_D = 1e-6\n    v = np.array([1.0, -2.0])\n    v_norm = v / np.linalg.norm(v)\n\n    # Adjoint-based directional derivative g^T * v\n    dir_deriv_adj = np.dot(grad_adj_base.T, v_norm)\n\n    # FD-based directional derivative\n    x0_plus_hv = x0_val + h_D * v_norm\n    x0_minus_hv = x0_val - h_D * v_norm\n    traj_plus_v = run_forward_model(x0_plus_hv)\n    traj_minus_v = run_forward_model(x0_minus_hv)\n    J_plus_v = compute_cost(traj_plus_v[-1], y_target=y_default)\n    J_minus_v = compute_cost(traj_minus_v[-1], y_target=y_default)\n    dir_deriv_fd = (J_plus_v - J_minus_v) / (2 * h_D)\n\n    discrepancy_D = abs(dir_deriv_adj - dir_deriv_fd) / max(abs(dir_deriv_adj), eps_min)\n    result_D = discrepancy_D  1e-7\n    results.append(result_D)\n\n    # Test Case E: Zero-mismatch boundary condition\n    # First, find the \"true\" final state to set y\n    trajectory_E = run_forward_model(x0_val)\n    y_E = np.dot(H, trajectory_E[-1])\n    \n    # Now compute the adjoint gradient with this y\n    grad_adj_E = compute_adjoint_gradient(x0_val, y_target=y_E)\n    norm_E = np.linalg.norm(grad_adj_E)\n    results.append(norm_E)\n    \n    return results\n\nresults = solve()\nformatted_results = []\nfor res in results:\n    if isinstance(res, bool):\n        formatted_results.append(str(res).lower())\n    else:\n        formatted_results.append(f\"{res:.12e}\")\n        \noutput_str = f\"[{','.join(formatted_results)}]\"\n# The problem asks the program to return the specified output.\n# The code above calculates it.\n# The result is: [2.527339897103e-08,9.752002353386e-03,1.603306006764e-01,true,0.000000000000e+00]\n```",
            "answer": "```\n[2.527339897103e-08,9.752002353386e-03,1.603306006764e-01,true,0.000000000000e+00]\n```"
        },
        {
            "introduction": "计算敏感性的最终目的是为了获得物理洞察并辅助决策。本练习将展示“对观测的预报敏感性”（FSO）的强大功能，它将通过伴随方法计算出的敏感性与一个具体的度量——预报误差方差的预期减少量——联系起来。这说明了伴随方法如何为评估观测系统中不同观测的价值提供定量依据。",
            "id": "4009367",
            "problem": "考虑一个在数值天气预报和气候模拟中典型的线性、无偏、高斯数据同化设定。设预报状态为 $x \\in \\mathbb{R}^{n}$，其背景（先验）误差协方差为 $P^{f} \\in \\mathbb{R}^{n \\times n}$。设一个标量预报指标为 $J = c^{\\top} x_{f}$，其中 $c \\in \\mathbb{R}^{n}$ 是 $J$ 相对于预报状态的梯度，通过伴随模式计算得到。假设在分析时刻增加一个标量观测，其观测算子为 $H = e_{i}^{\\top}$（即，它测量第 $i$ 个状态分量），观测误差 $\\varepsilon$ 为零均值且与背景误差无关，观测误差方差为 $R = \\sigma_{o}^{2}$。假设背景误差协方差在观测位置是局部对角的，使得 $P^{f}_{ii} = \\sigma_{b}^{2}$ 且涉及索引 $i$ 的非对角元素可以忽略不计。将在观测位置的预报对观测的敏感性（FSO）表示为 $g = c_{i}$，即基于伴随的预报梯度相对于第 $i$ 个状态变量的分量。\n\n仅使用这些假设和线性高斯估计的基本原理，推导由于最优地同化此观测而导致的 $J$ 的预报误差方差的期望减少量。也就是说，推导 $\\Delta \\mathrm{Var}(J) = \\mathrm{Var}_{f}(J) - \\mathrm{Var}_{a}(J)$ 关于 $g$、$\\sigma_{b}^{2}$ 和 $\\sigma_{o}^{2}$ 的闭式表达式。\n\n请用一个包含 $g$、$\\sigma_{b}^{2}$ 和 $\\sigma_{o}^{2}$ 的单一闭式解析表达式来表示你的最终答案。不需要进行数值取整，最终表达式中也不应报告单位。",
            "solution": "此问题是有效的，因为它在科学上基于线性高斯估计理论的原理，特别是在应用于数值天气预报的数据同化背景下。它是一个适定问题，具有明确的目标和一套完整且一致的假设，从而可以推导出唯一的解析解。\n\n设系统的真实状态表示为 $x_t \\in \\mathbb{R}^{n}$。预报误差为 $\\delta x_f = x_f - x_t$，其中 $x_f$ 是预报状态。标量预报指标由 $J = c^{\\top} x_f$ 给出。该指标的误差为 $\\delta J_f = c^{\\top} x_f - c^{\\top} x_t = c^{\\top} (x_f - x_t) = c^{\\top} \\delta x_f$。\n\n$J$ 的预报误差方差，记为 $\\mathrm{Var}_f(J)$，是 $J$ 中误差平方的期望值。\n$$\n\\mathrm{Var}_f(J) = E[(\\delta J_f)^2] = E[(c^{\\top} \\delta x_f)(c^{\\top} \\delta x_f)^{\\top}] = E[c^{\\top} \\delta x_f \\delta x_f^{\\top} c]\n$$\n根据定义，背景（预报）误差协方差矩阵为 $P^f = E[\\delta x_f \\delta x_f^{\\top}]$。因此，$J$ 的预报误差方差为：\n$$\n\\mathrm{Var}_f(J) = c^{\\top} P^f c\n$$\n同化一个新的观测后，我们得到一个分析状态 $x_a$ 和一个相应的分析误差协方差矩阵 $P^a$。$J$ 的分析误差方差则为：\n$$\n\\mathrm{Var}_a(J) = c^{\\top} P^a c\n$$\n在一个最优线性估计框架（等同于卡尔曼滤波器的一个步骤）中，分析误差协方差 $P^a$ 与预报误差协方差 $P^f$ 通过以下公式相关联：\n$$\nP^a = (I - KH)P^f\n$$\n其中 $I \\in \\mathbb{R}^{n \\times n}$ 是单位矩阵，$H$ 是观测算子，$K$ 是最优卡尔曼增益。卡尔曼增益 $K$ 由以下公式给出：\n$$\nK = P^f H^{\\top} (H P^f H^{\\top} + R)^{-1}\n$$\n这里，$R$ 是观测误差协方差矩阵。\n\n目标是求出由于同化导致的 $J$ 的预报误差方差的减少量，即 $\\Delta \\mathrm{Var}(J) = \\mathrm{Var}_f(J) - \\mathrm{Var}_a(J)$。\n$$\n\\Delta \\mathrm{Var}(J) = c^{\\top} P^f c - c^{\\top} P^a c = c^{\\top} P^f c - c^{\\top} (I - KH)P^f c\n$$\n展开第二项得到：\n$$\n\\Delta \\mathrm{Var}(J) = c^{\\top} P^f c - (c^{\\top} P^f c - c^{\\top} KH P^f c) = c^{\\top} K H P^f c\n$$\n现在，我们代入问题陈述中提供的具体形式和值。\n观测是一个单一标量，所以其误差方差是一个标量 $R = \\sigma_o^2$。\n观测算子是 $H = e_i^{\\top}$，这是一个在第 $i$ 个位置为1，其余位置为0的行向量。\n\n首先，我们简化 $K$ 表达式中的项 $(H P^f H^{\\top} + R)^{-1}$。\n$H P^f H^{\\top}$ 项是一个标量：\n$$\nH P^f H^{\\top} = e_i^{\\top} P^f e_i\n$$\n这个表达式提取了 $P^f$ 的第 $i$ 行和第 $i$ 列的元素，即 $P^f_{ii}$。问题陈述中指出 $P^f_{ii} = \\sigma_b^2$。因此，$H P^f H^{\\top} = \\sigma_b^2$。\n逆项变为：\n$$\n(H P^f H^{\\top} + R)^{-1} = (\\sigma_b^2 + \\sigma_o^2)^{-1}\n$$\n这是一个标量逆。现在我们可以写出卡尔曼增益 $K$：\n$$\nK = P^f H^{\\top} (\\sigma_b^2 + \\sigma_o^2)^{-1} = P^f e_i (\\sigma_b^2 + \\sigma_o^2)^{-1}\n$$\n这里，$P^f e_i$ 是矩阵 $P^f$ 的第 $i$ 列，我们可以将其表示为 $P^f_{:,i}$。所以，$K = \\frac{1}{\\sigma_b^2 + \\sigma_o^2} P^f_{:,i}$。$K$ 是一个大小为 $n \\times 1$ 的列向量。\n\n现在，将 $K$ 和 $H$ 的表达式代入 $\\Delta \\mathrm{Var}(J)$ 的方程中：\n$$\n\\Delta \\mathrm{Var}(J) = c^{\\top} \\left( \\frac{1}{\\sigma_b^2 + \\sigma_o^2} P^f_{:,i} \\right) (e_i^{\\top}) (P^f c)\n$$\n我们可以重新排列标量和向量的乘积。注意 $e_i^{\\top} (P^f c)$ 是一个标量，等于向量 $P^f c$ 的第 $i$ 个分量，记为 $(P^f c)_i$。项 $c^{\\top} P^f_{:,i}$ 也是一个标量，表示向量 $c$ 和 $P^f$ 的第 $i$ 列的内积。\n由于 $P^f$ 是一个协方差矩阵，它是对称的（$P^f = (P^f)^{\\top}$）。因此：\n$$\nc^{\\top} P^f_{:,i} = c^{\\top} (P^f e_i) = (e_i^{\\top} (P^f)^{\\top} c)^{\\top} = (e_i^{\\top} P^f c)^{\\top}\n$$\n由于 $e_i^{\\top} P^f c$ 是一个标量，其转置是它自身。所以，$c^{\\top} P^f_{:,i} = e_i^{\\top} P^f c = (P^f c)_i$。\n我们定义 $S_i = (P^f c)_i$。方差减少量的表达式变为：\n$$\n\\Delta \\mathrm{Var}(J) = \\frac{1}{\\sigma_b^2 + \\sigma_o^2} S_i \\cdot S_i = \\frac{S_i^2}{\\sigma_b^2 + \\sigma_o^2}\n$$\n现在我们必须使用最后一个假设来计算 $S_i$。$S_i$ 是向量 $P^f c$ 的第 $i$ 个分量：\n$$\nS_i = (P^f c)_i = \\sum_{j=1}^{n} P^f_{ij} c_j\n$$\n问题陈述中指出背景误差协方差在观测位置是局部对角的，因此涉及索引 $i$ 的非对角元素可以忽略不计。这意味着我们可以做近似 $P^f_{ij} \\approx 0$ 对于所有 $j \\neq i$。\n在这个近似下，$S_i$ 的求和简化为单个项：\n$$\nS_i \\approx P^f_{ii} c_i\n$$\n我们已知 $P^f_{ii} = \\sigma_b^2$ 并且伴随梯度 $c$ 的第 $i$ 个分量是预报对观测的敏感性（FSO），即 $g = c_i$。\n将这些代入 $S_i$ 的表达式中：\n$$\nS_i \\approx \\sigma_b^2 g\n$$\n最后，将此结果代入 $\\Delta \\mathrm{Var}(J)$ 的方程中：\n$$\n\\Delta \\mathrm{Var}(J) = \\frac{(\\sigma_b^2 g)^2}{\\sigma_b^2 + \\sigma_o^2} = \\frac{\\sigma_b^4 g^2}{\\sigma_b^2 + \\sigma_o^2}\n$$\n这就是由于同化单个观测而导致的指标 $J$ 的预报误差方差期望减少量的闭式表达式。",
            "answer": "$$\n\\boxed{\\frac{\\sigma_{b}^{4} g^{2}}{\\sigma_{b}^{2} + \\sigma_{o}^{2}}}\n$$"
        }
    ]
}