{
    "hands_on_practices": [
        {
            "introduction": "To understand the limits of predictability, we must first explore the origin of sensitive dependence on initial conditions. This practice delves into the fundamental mechanism of error growth by performing a linear stability analysis on the Lorenz '96 model, a system widely used to study atmospheric dynamics. By deriving the Jacobian matrix and analyzing its eigenvalues for a simplified equilibrium state, you will analytically determine the growth rates of small perturbations and identify the modes of fastest growth, providing a foundational look at how chaos is initiated in a dynamical system .",
            "id": "4077566",
            "problem": "Consider an idealized chaotic multiscale system used in Numerical Weather Prediction (NWP), the Lorenz '96 model, posed on a periodic domain with $N \\geq 3$ variables. The state vector is $\\boldsymbol{x} = (x_{1}, x_{2}, \\dots, x_{N})$ with indices taken modulo $N$ so that $x_{i+N} = x_{i}$. The model dynamics are given by the system of ordinary differential equations\n$$\n\\dot{x}_{i} = (x_{i+1} - x_{i-2}) x_{i-1} - x_{i} + F, \\quad i = 1, 2, \\dots, N,\n$$\nwhere $F$ is a constant external forcing.\n\nStarting from the standard tangent-linear framework for nonlinear autonomous systems $\\dot{\\boldsymbol{x}} = \\boldsymbol{f}(\\boldsymbol{x})$, in which local error growth is governed by the Jacobian $J(\\boldsymbol{x}) = \\frac{\\partial \\boldsymbol{f}}{\\partial \\boldsymbol{x}}$ through $\\dot{\\boldsymbol{\\delta x}} = J(\\boldsymbol{x}) \\boldsymbol{\\delta x}$, proceed as follows:\n\n1. Derive the general Jacobian matrix entries $J_{i,j}(\\boldsymbol{x}) = \\frac{\\partial \\dot{x}_{i}}{\\partial x_{j}}$ for the Lorenz '96 model on the periodic domain.\n2. Show that there exists a homogeneous equilibrium $x_{i} = X$ for all $i$ and determine $X$ in terms of $F$.\n3. Specialize the Jacobian to this homogeneous equilibrium and exploit periodicity to diagonalize the linear operator using discrete Fourier modes. Obtain the eigenvalues $\\lambda(q)$ of the Jacobian as functions of the wavenumber $q \\in [0, 2\\pi)$.\n4. In the continuum (large-$N$) limit, determine the maximum instantaneous linear growth rate of perturbations, defined as $\\max_{q \\in [0, 2\\pi)} \\Re[\\lambda(q)]$, and express it in closed form explicitly in terms of $F$.\n\nYour final answer must be a single closed-form analytical expression. No rounding is required, and no units are to be reported in the final expression.",
            "solution": "The problem statement has been validated and is deemed to be scientifically grounded, well-posed, and objective. It presents a standard analysis of the Lorenz '96 model, a canonical system in the study of chaotic dynamics and numerical weather prediction. The problem is free of any scientific, logical, or structural flaws. We may therefore proceed with a full solution.\n\nThe solution will be structured according to the four parts requested in the problem statement.\n\n**Part 1: Derivation of the Jacobian Matrix**\n\nThe dynamics of the system are given by the set of ordinary differential equations:\n$$\n\\dot{x}_{i} = f_i(\\boldsymbol{x}) = (x_{i+1} - x_{i-2}) x_{i-1} - x_{i} + F, \\quad i = 1, 2, \\dots, N\n$$\nThe Jacobian matrix entries $J_{i,j}(\\boldsymbol{x})$ are defined as $J_{i,j} = \\frac{\\partial f_i}{\\partial x_j}$. The function $f_i$ depends explicitly on the variables $x_{i-2}$, $x_{i-1}$, $x_i$, and $x_{i+1}$. Consequently, the partial derivatives will be non-zero only when $j$ corresponds to one of these indices. All indices are taken modulo $N$.\n\nWe compute the partial derivatives:\n-   For $j=i$:\n    $$\n    \\frac{\\partial f_i}{\\partial x_i} = \\frac{\\partial}{\\partial x_i} \\left[ (x_{i+1} - x_{i-2}) x_{i-1} - x_{i} + F \\right] = -1\n    $$\n-   For $j=i-1$:\n    $$\n    \\frac{\\partial f_i}{\\partial x_{i-1}} = \\frac{\\partial}{\\partial x_{i-1}} \\left[ (x_{i+1} - x_{i-2}) x_{i-1} - x_{i} + F \\right] = x_{i+1} - x_{i-2}\n    $$\n-   For $j=i+1$:\n    $$\n    \\frac{\\partial f_i}{\\partial x_{i+1}} = \\frac{\\partial}{\\partial x_{i+1}} \\left[ (x_{i+1} - x_{i-2}) x_{i-1} - x_{i} + F \\right] = x_{i-1}\n    $$\n-   For $j=i-2$:\n    $$\n    \\frac{\\partial f_i}{\\partial x_{i-2}} = \\frac{\\partial}{\\partial x_{i-2}} \\left[ (x_{i+1} - x_{i-2}) x_{i-1} - x_{i} + F \\right] = -x_{i-1}\n    $$\nFor all other values of $j$, the derivative is zero. Thus, the non-zero entries of the Jacobian matrix $J(\\boldsymbol{x})$ are:\n$$\nJ_{i,i} = -1 \\\\\nJ_{i,i-1} = x_{i+1} - x_{i-2} \\\\\nJ_{i,i+1} = x_{i-1} \\\\\nJ_{i,i-2} = -x_{i-1}\n$$\n\n**Part 2: Homogeneous Equilibrium State**\n\nAn equilibrium state $\\boldsymbol{x}_{\\text{eq}}$ is a point where the system's state does not change over time, i.e., $\\dot{\\boldsymbol{x}} = \\boldsymbol{0}$. For a homogeneous equilibrium, we impose the condition $x_i = X$ for all $i=1, \\dots, N$, where $X$ is a constant.\nSubstituting this into the governing equation for any $i$:\n$$\n\\dot{x}_i = (X - X) X - X + F = 0\n$$\n$$\n0 \\cdot X - X + F = 0\n$$\n$$\n-X + F = 0\n$$\nThis yields $X = F$. Therefore, a homogeneous equilibrium state exists and is given by $\\boldsymbol{x}_{\\text{eq}} = (F, F, \\dots, F)$.\n\n**Part 3: Jacobian Eigenvalues at Equilibrium**\n\nWe now evaluate the general Jacobian entries at the homogeneous equilibrium state $\\boldsymbol{x}_{\\text{eq}}$, where $x_k = F$ for all $k$. Let the resulting specialized Jacobian be denoted $J_{\\text{eq}}$.\n$$\nJ_{\\text{eq},i,i} = -1 \\\\\nJ_{\\text{eq},i,i-1} = F - F = 0 \\\\\nJ_{\\text{eq},i,i+1} = F \\\\\nJ_{\\text{eq},i,i-2} = -F\n$$\nThe tangent-linear model $\\dot{\\boldsymbol{\\delta x}} = J_{\\text{eq}} \\boldsymbol{\\delta x}$ for a small perturbation $\\boldsymbol{\\delta x}$ around the equilibrium becomes:\n$$\n\\dot{\\delta x}_i = -F\\delta x_{i-2} - \\delta x_i + F\\delta x_{i+1}\n$$\nThis is a system of linear homogeneous ordinary differential equations with constant coefficients. Due to the periodic boundary conditions and the structure of the couplings, the matrix $J_{\\text{eq}}$ is a circulant matrix. The eigenvectors of a circulant matrix are the discrete Fourier modes. We seek solutions of the form $\\delta x_j(t) = v_j e^{\\lambda t}$, which leads to the eigenvalue problem $J_{\\text{eq}} \\boldsymbol{v} = \\lambda \\boldsymbol{v}$, or component-wise:\n$$\n\\lambda v_i = -F v_{i-2} - v_i + F v_{i+1}\n$$\nWe propose an eigenvector with components $v_j = e^{\\mathrm{i} q j}$ for some wavenumber $q$. The periodicity condition $v_{j+N} = v_j$ implies $e^{\\mathrm{i} q (j+N)} = e^{\\mathrm{i} q j}$, which requires $e^{\\mathrm{i} q N} = 1$. The discrete wavenumbers are therefore $q_m = \\frac{2\\pi m}{N}$ for $m = 0, 1, \\dots, N-1$. For the large-$N$ continuum limit, we treat $q$ as a continuous variable in the interval $[0, 2\\pi)$.\n\nSubstituting the ansatz $v_j = e^{\\mathrm{i} q j}$ into the eigenvalue equation:\n$$\n\\lambda e^{\\mathrm{i} q i} = -F e^{\\mathrm{i} q (i-2)} - e^{\\mathrm{i} q i} + F e^{\\mathrm{i} q (i+1)}\n$$\nDividing by the non-zero common factor $e^{\\mathrm{i} q i}$:\n$$\n\\lambda(q) = -F e^{-2\\mathrm{i} q} - 1 + F e^{\\mathrm{i} q}\n$$\nThis equation gives the eigenvalues $\\lambda$ as a function of the wavenumber $q$. Using Euler's formula, $e^{\\mathrm{i}\\theta} = \\cos(\\theta) + \\mathrm{i}\\sin(\\theta)$, we can separate the real and imaginary parts:\n$$\n\\lambda(q) = -1 + F(e^{\\mathrm{i} q} - e^{-2\\mathrm{i} q}) = -1 + F(\\cos(q) + \\mathrm{i}\\sin(q) - (\\cos(2q) - \\mathrm{i}\\sin(2q)))\n$$\n$$\n\\lambda(q) = [-1 + F(\\cos(q) - \\cos(2q))] + \\mathrm{i}[F(\\sin(q) + \\sin(2q))]\n$$\n\n**Part 4: Maximum Instantaneous Linear Growth Rate**\n\nThe instantaneous linear growth rate of a perturbation mode is given by the real part of its corresponding eigenvalue, $\\Re[\\lambda(q)]$. We wish to find the maximum of this quantity over all possible modes, i.e., $\\max_{q \\in [0, 2\\pi)} \\Re[\\lambda(q)]$.\nLet $g(q) = \\Re[\\lambda(q)] = -1 + F(\\cos(q) - \\cos(2q))$. To find the maximum value of $g(q)$, we compute its derivative with respect to $q$ and set it to zero.\n$$\ng'(q) = \\frac{d}{dq} g(q) = F(-\\sin(q) + 2\\sin(2q))\n$$\nSetting $g'(q) = 0$ (and assuming $F \\neq 0$), we have:\n$$\n-\\sin(q) + 2\\sin(2q) = 0\n$$\nUsing the double-angle identity $\\sin(2q) = 2\\sin(q)\\cos(q)$:\n$$\n-\\sin(q) + 4\\sin(q)\\cos(q) = 0\n$$\n$$\n\\sin(q) (4\\cos(q) - 1) = 0\n$$\nThis equation yields two sets of critical points in the interval $q \\in [0, 2\\pi)$:\n1. $\\sin(q) = 0$, which gives $q=0$ and $q=\\pi$.\n2. $4\\cos(q) - 1 = 0$, which gives $\\cos(q) = \\frac{1}{4}$.\n\nWe evaluate $g(q)$ at these points.\nFor Case 1 ($q=0$ and $q=\\pi$):\n- If $q=0$, $g(0) = -1 + F(\\cos(0) - \\cos(0)) = -1$.\n- If $q=\\pi$, $g(\\pi) = -1 + F(\\cos(\\pi) - \\cos(2\\pi)) = -1 + F(-1 - 1) = -1 - 2F$.\n\nFor Case 2 ($\\cos(q)=\\frac{1}{4}$):\nWe use the identity $\\cos(2q) = 2\\cos^2(q) - 1$ to find $\\cos(2q)$.\n$$\n\\cos(2q) = 2\\left(\\frac{1}{4}\\right)^2 - 1 = 2\\left(\\frac{1}{16}\\right) - 1 = \\frac{1}{8} - 1 = -\\frac{7}{8}\n$$\nNow we evaluate $g(q)$ for these values of $q$:\n$$\ng(q) = -1 + F\\left(\\cos(q) - \\cos(2q)\\right) = -1 + F\\left(\\frac{1}{4} - \\left(-\\frac{7}{8}\\right)\\right)\n$$\n$$\ng(q) = -1 + F\\left(\\frac{2}{8} + \\frac{7}{8}\\right) = -1 + \\frac{9}{8}F\n$$\nTo determine which of these critical points corresponds to a maximum, we examine the second derivative, $g''(q)=F(-\\cos(q) + 4\\cos(2q))$.\nFor Case 1:\n- At $q=0$, $g''(0) = F(-1+4)=3F$. If $F>0$, this is a local minimum.\n- At $q=\\pi$, $g''(\\pi) = F(-(-1)+4(1))=5F$. If $F>0$, this is a local minimum.\nFor Case 2:\n- When $\\cos(q)=\\frac{1}{4}$, $g''(q) = F(-\\frac{1}{4} + 4(-\\frac{7}{8}))=F(-\\frac{1}{4}-\\frac{7}{2}) = -\\frac{15}{4}F$. If $F>0$, this is a local maximum.\n\nComparing the values at the critical points, $-1$, $-1-2F$, and $\\frac{9}{8}F - 1$, it is clear that for any positive forcing $F$ (which is necessary for chaotic behavior), the maximum value is $\\frac{9}{8}F - 1$.\nThe maximum instantaneous linear growth rate is thus $\\frac{9}{8}F - 1$.",
            "answer": "$$\n\\boxed{\\frac{9}{8}F - 1}\n$$"
        },
        {
            "introduction": "While stability analysis of a fixed point reveals the potential for chaos, real-world error growth occurs along complex, evolving trajectories. This exercise provides direct, hands-on experience in quantifying this growth by calculating the Finite-Time Lyapunov Exponent (FTLE) for the classic Lorenz '63 model. You will numerically integrate the full nonlinear system alongside its corresponding variational equations to see how an initial, tiny perturbation is amplified over time, a core skill in diagnosing predictability in any chaotic system .",
            "id": "4077615",
            "problem": "Consider a continuous-time dynamical system defined by an ordinary differential equation (ODE) $\\,\\dot{\\boldsymbol{x}} = \\boldsymbol{f}(\\boldsymbol{x})\\,$, where $\\boldsymbol{x}(t) \\in \\mathbb{R}^3$. The finite-time Lyapunov exponent (FTLE) over a finite time $T>0$ along a trajectory $\\boldsymbol{x}(t)$ is defined for a perturbation $\\delta \\boldsymbol{x}(t)$ that evolves according to the variational (tangent linear) equations as\n$$\n\\lambda_T = \\frac{1}{T}\\,\\ln\\left(\\frac{\\|\\delta \\boldsymbol{x}(T)\\|}{\\|\\delta \\boldsymbol{x}(0)\\|}\\right).\n$$\nThe perturbation $\\delta \\boldsymbol{x}(t)$ satisfies the linearized dynamics\n$$\n\\frac{d}{dt}\\,\\delta \\boldsymbol{x}(t) = \\mathbf{J}(\\boldsymbol{x}(t))\\,\\delta \\boldsymbol{x}(t),\n$$\nwhere $\\mathbf{J}(\\boldsymbol{x}) = \\frac{\\partial \\boldsymbol{f}}{\\partial \\boldsymbol{x}}$ is the Jacobian matrix of $\\boldsymbol{f}$ evaluated along the reference trajectory $\\boldsymbol{x}(t)$. Let $\\mathbf{\\Phi}(t)$ denote the fundamental matrix solution of the variational equations satisfying $\\dot{\\mathbf{\\Phi}}(t)=\\mathbf{J}(\\boldsymbol{x}(t))\\,\\mathbf{\\Phi}(t)$ and $\\mathbf{\\Phi}(0)=\\mathbf{I}$. Then $\\delta \\boldsymbol{x}(T) = \\mathbf{\\Phi}(T)\\,\\delta \\boldsymbol{x}(0)$.\n\nIn this problem, the base flow is the Lorenz '63 model:\n- $\\,\\dot{x} = \\sigma(y-x)$,\n- $\\,\\dot{y} = x(\\rho - z) - y$,\n- $\\,\\dot{z} = xy - \\beta z$,\nwith the standard parameters $\\sigma = 10$, $\\rho = 28$, and $\\beta = 8/3$. The initial condition for the base trajectory is $\\boldsymbol{x}(0)=(1,1,1)$, and the time horizon is $T=5$ unless otherwise specified. The Jacobian $\\mathbf{J}(\\boldsymbol{x})$ for Lorenz '63 is\n$$\n\\mathbf{J}(x,y,z) =\n\\begin{bmatrix}\n-\\sigma & \\sigma & 0 \\\\\n\\rho - z & -1 & -x \\\\\ny & x & -\\beta\n\\end{bmatrix}.\n$$\n\nYour task is to compute the finite-time Lyapunov exponent $\\lambda_T$ using the variational equations by numerically integrating the augmented system for $(\\boldsymbol{x}(t), \\mathbf{\\Phi}(t))$ and then applying the definition of $\\lambda_T$. The norm $\\|\\cdot\\|$ will be one of the following:\n- The $p=2$ Euclidean norm: $\\|\\boldsymbol{v}\\|_2 = \\sqrt{v_1^2 + v_2^2 + v_3^2}$.\n- The $p=1$ Manhattan norm: $\\|\\boldsymbol{v}\\|_1 = |v_1| + |v_2| + |v_3|$.\n- The $p=\\infty$ maximum norm: $\\|\\boldsymbol{v}\\|_\\infty = \\max(|v_1|,|v_2|,|v_3|)$.\n\nUse the following test suite of cases, each specifying the perturbation vector $\\delta \\boldsymbol{x}(0)$, the horizon $T$, and the norm type:\n1. Case A (happy path): $\\delta \\boldsymbol{x}(0) = [1\\times 10^{-6},\\,0,\\,0]$, $T = 5$, norm $p=2$.\n2. Case B (scaling invariance check): $\\delta \\boldsymbol{x}(0) = [1\\times 10^{-8},\\,0,\\,0]$, $T = 5$, norm $p=2$.\n3. Case C (norm sensitivity): $\\delta \\boldsymbol{x}(0) = [0,\\,1\\times 10^{-6},\\,0]$, $T = 5$, norm $p=1$.\n4. Case D (norm sensitivity): $\\delta \\boldsymbol{x}(0) = [0,\\,0,\\,1\\times 10^{-6}]$, $T = 5$, norm $p=\\infty$.\n5. Case E (short-time behavior): $\\delta \\boldsymbol{x}(0) = [1\\times 10^{-6},\\,1\\times 10^{-6},\\,1\\times 10^{-6}]$, $T = 0.5$, norm $p=2$.\n\nRequirements and constraints:\n- Start the base trajectory at $\\boldsymbol{x}(0)=(1,1,1)$ for all cases.\n- Integrate the augmented system for $(\\boldsymbol{x}(t), \\mathbf{\\Phi}(t))$ with $\\mathbf{\\Phi}(0)=\\mathbf{I}$ and then compute $\\delta \\boldsymbol{x}(T)=\\mathbf{\\Phi}(T)\\delta \\boldsymbol{x}(0)$ to evaluate $\\lambda_T$ using the specified norm for each case.\n- Ensure $T>0$ in all computations.\n- All outputs are dimensionless. Express the final numeric results as floats rounded to six decimal places.\n- Your program should produce a single line of output containing the results for Cases A–E, in order, as a comma-separated list enclosed in square brackets (e.g., \"[resultA,resultB,resultC,resultD,resultE]\").\n\nDeliverables:\n- A complete, runnable program that carries out the numerical integration of the Lorenz '63 system and its variational equations for the specified cases and returns the requested FTLE values, adhering to the final output format specified above.",
            "solution": "The problem requires the computation of the finite-time Lyapunov exponent (FTLE) for the Lorenz '63 system for several test cases. The FTLE, denoted $\\lambda_T$, quantifies the average exponential rate of divergence of infinitesimally close trajectories over a finite time interval $T > 0$. It is defined as:\n$$\n\\lambda_T = \\frac{1}{T}\\,\\ln\\left(\\frac{\\|\\delta \\boldsymbol{x}(T)\\|}{\\|\\delta \\boldsymbol{x}(0)\\|}\\right)\n$$\nwhere $\\delta \\boldsymbol{x}(t)$ is an infinitesimal perturbation vector whose evolution is governed by the linearized dynamics of the system.\n\nThe core of the methodology is to solve an augmented system of ordinary differential equations (ODEs). This system combines the original nonlinear Lorenz '63 equations for the state vector $\\boldsymbol{x}(t) = [x(t), y(t), z(t)]^T \\in \\mathbb{R}^3$ with the variational equations that govern the evolution of the fundamental matrix solution $\\mathbf{\\Phi}(t) \\in \\mathbb{R}^{3 \\times 3}$. The fundamental matrix maps initial perturbations to final perturbations via $\\delta\\boldsymbol{x}(T) = \\mathbf{\\Phi}(T)\\delta\\boldsymbol{x}(0)$.\n\nThe state of the augmented system is a $12$-dimensional vector $\\boldsymbol{Y}(t)$, constructed by concatenating the Lorenz state vector $\\boldsymbol{x}(t)$ and the vectorized fundamental matrix $\\mathbf{\\Phi}(t)$:\n$$\n\\boldsymbol{Y}(t) = [\\boldsymbol{x}(t)^T, (\\text{vec}(\\mathbf{\\Phi}(t)))^T]^T \\in \\mathbb{R}^{12}\n$$\nThe governing ODE for this augmented system is $\\dot{\\boldsymbol{Y}}(t) = \\boldsymbol{F}(\\boldsymbol{Y}(t))$, which breaks down into two parts:\n$1$. The Lorenz '63 equations:\n$$\n\\dot{\\boldsymbol{x}} = \\boldsymbol{f}(\\boldsymbol{x}) =\n\\begin{pmatrix}\n\\sigma(y-x) \\\\\nx(\\rho - z) - y \\\\\nxy - \\beta z\n\\end{pmatrix}\n$$\nwith parameters $\\sigma = 10$, $\\rho = 28$, and $\\beta = 8/3$.\n$2$. The variational equation for the fundamental matrix:\n$$\n\\dot{\\mathbf{\\Phi}}(t) = \\mathbf{J}(\\boldsymbol{x}(t))\\,\\mathbf{\\Phi}(t)\n$$\nwhere $\\mathbf{J}(\\boldsymbol{x})$ is the Jacobian matrix of $\\boldsymbol{f}(\\boldsymbol{x})$, evaluated along the trajectory $\\boldsymbol{x}(t)$:\n$$\n\\mathbf{J}(x,y,z) =\n\\begin{bmatrix}\n-\\sigma & \\sigma & 0 \\\\\n\\rho - z & -1 & -x \\\\\ny & x & -\\beta\n\\end{bmatrix}\n$$\nThe initial conditions for this augmented system are $\\boldsymbol{x}(0) = (1,1,1)$ and $\\mathbf{\\Phi}(0) = \\mathbf{I}$, where $\\mathbf{I}$ is the $3 \\times 3$ identity matrix.\n\nThe computational procedure for each test case is as follows:\n$1$. Construct the initial state vector $\\boldsymbol{Y}(0)$ for the $12$-dimensional augmented system.\n$2$. Numerically integrate the system of $12$ ODEs from $t=0$ to the specified final time $T$, using a high-precision solver such as `scipy.integrate.solve_ivp`.\n$3$. From the final state $\\boldsymbol{Y}(T)$, extract the $3 \\times 3$ fundamental matrix $\\mathbf{\\Phi}(T)$.\n$4$. For the given initial perturbation $\\delta\\boldsymbol{x}(0)$, compute the final perturbation $\\delta\\boldsymbol{x}(T) = \\mathbf{\\Phi}(T)\\delta\\boldsymbol{x}(0)$.\n$5$. Calculate the norm of the initial perturbation, $\\|\\delta\\boldsymbol{x}(0)\\|$, and the final perturbation, $\\|\\delta\\boldsymbol{x}(T)\\|$, using the norm ($p=1$, $p=2$, or $p=\\infty$) specified for the case.\n$6$. Calculate the FTLE $\\lambda_T$ using its definition.\n\nIt is a key property of FTLEs derived from linear variational equations that the result is independent of the initial perturbation's magnitude, only its direction. This is because the norm is homogeneous, i.e., $\\|c\\boldsymbol{v}\\| = |c|\\|\\boldsymbol{v}\\|$, and the term $|c|$ cancels out in the ratio within the logarithm. Therefore, Case A and Case B are expected to yield identical results, providing a validation of the numerical implementation. The other cases test sensitivity to the initial perturbation direction and the choice of norm.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.integrate import solve_ivp\n\ndef solve():\n    \"\"\"\n    Computes the finite-time Lyapunov exponent (FTLE) for the Lorenz '63 system\n    for a suite of test cases.\n    \"\"\"\n\n    def lorenz_augmented_rhs(t, state, sigma, rho, beta):\n        \"\"\"\n        Computes the time derivative of the augmented state vector, which includes\n        the Lorenz '63 state and the fundamental matrix.\n        \n        Args:\n            t (float): Current time (unused, as the system is autonomous).\n            state (np.ndarray): 12-element array [x, y, z, phi_11, ..., phi_33].\n            sigma (float): Lorenz parameter sigma.\n            rho (float): Lorenz parameter rho.\n            beta (float): Lorenz parameter beta.\n\n        Returns:\n            np.ndarray: The 12-element time derivative of the state vector.\n        \"\"\"\n        x, y, z = state[:3]\n        phi = state[3:].reshape((3, 3))\n\n        # Lorenz '63 equations\n        dxdt = sigma * (y - x)\n        dydt = x * (rho - z) - y\n        dzdt = x * y - beta * z\n        \n        lorenz_derivs = np.array([dxdt, dydt, dzdt])\n\n        # Jacobian matrix\n        J = np.array([\n            [-sigma, sigma, 0],\n            [rho - z, -1, -x],\n            [y, x, -beta]\n        ])\n\n        # Variational equations for the fundamental matrix\n        dphi_dt = J @ phi\n\n        # Concatenate derivatives into a single state derivative vector\n        d_state_dt = np.concatenate((lorenz_derivs, dphi_dt.flatten()))\n        \n        return d_state_dt\n\n    # Lorenz '63 parameters\n    sigma = 10.0\n    rho = 28.0\n    beta = 8.0 / 3.0\n\n    # Initial condition for the base trajectory\n    x0 = np.array([1.0, 1.0, 1.0])\n\n    # Define the test cases from the problem statement\n    test_cases = [\n        # Case A: happy path\n        {'delta_x0': np.array([1e-6, 0.0, 0.0]), 'T': 5.0, 'norm_p': 2},\n        # Case B: scaling invariance check\n        {'delta_x0': np.array([1e-8, 0.0, 0.0]), 'T': 5.0, 'norm_p': 2},\n        # Case C: norm sensitivity\n        {'delta_x0': np.array([0.0, 1e-6, 0.0]), 'T': 5.0, 'norm_p': 1},\n        # Case D: norm sensitivity\n        {'delta_x0': np.array([0.0, 0.0, 1e-6]), 'T': 5.0, 'norm_p': np.inf},\n        # Case E: short-time behavior\n        {'delta_x0': np.array([1e-6, 1e-6, 1e-6]), 'T': 0.5, 'norm_p': 2},\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        T = case['T']\n        delta_x0 = case['delta_x0']\n        norm_p = case['norm_p']\n\n        # Initial conditions for the augmented system\n        phi0_flat = np.identity(3).flatten()\n        y0 = np.concatenate((x0, phi0_flat))\n\n        # Integrate the augmented ODE system.\n        # Higher precision is used for better accuracy in chaotic systems.\n        sol = solve_ivp(\n            lorenz_augmented_rhs,\n            [0, T],\n            y0,\n            t_eval=[T],\n            args=(sigma, rho, beta),\n            method='RK45',\n            rtol=1e-9,  # Relative tolerance\n            atol=1e-12 # Absolute tolerance\n        )\n        \n        # Extract the final state and the fundamental matrix\n        y_T = sol.y[:, 0]\n        Phi_T = y_T[3:].reshape((3, 3))\n\n        # Compute the final perturbation vector\n        delta_x_T = Phi_T @ delta_x0\n\n        # Calculate initial and final norms\n        norm_initial = np.linalg.norm(delta_x0, ord=norm_p)\n        norm_final = np.linalg.norm(delta_x_T, ord=norm_p)\n\n        # Calculate FTLE, ensuring T > 0 and norm_initial > 0 to avoid division by zero\n        if T > 0 and norm_initial > 0:\n            ftle = (1.0 / T) * np.log(norm_final / norm_initial)\n        else:\n            # This case should not be reached based on the problem description\n            ftle = 0.0\n\n        results.append(f\"{ftle:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "In a high-dimensional system like the atmosphere, not all degrees of freedom are equally active; the dynamics are often confined to a much lower-dimensional chaotic attractor. This capstone practice bridges the statistical perspective of ensemble forecasting with the geometric view of dynamical systems theory to characterize this effective dimensionality. You will implement algorithms to compute both the participation ratio from ensemble statistics and the Kaplan-Yorke dimension from the Lyapunov spectrum, comparing these two powerful diagnostics to gain a deeper understanding of a chaotic system's complexity and its implications for predictability .",
            "id": "4077608",
            "problem": "Consider a discrete-time, finite-dimensional dynamical system representing the evolution of an atmosphere–ocean state vector. An ensemble of trajectories is used to quantify variability and forecast uncertainty. Let the ensemble anomalies be collected in a matrix whose sample covariance is symmetric positive semi-definite. Principal Component Analysis (PCA) provides an orthogonal basis in which the covariance is diagonal, with eigenvalues representing variances along principal components. The participation ratio is an effective-dimension concept that quantifies how variance is distributed across principal components. Separately, chaotic dynamics are characterized by Lyapunov exponents, and the Kaplan–Yorke dimension estimates the fractal dimension of the attractor from these exponents. Your task is to derive and implement the effective degrees of freedom from the participation ratio of principal components and relate it to the Kaplan–Yorke dimension computed from a Lyapunov spectrum.\n\nStarting only from fundamental definitions, do the following:\n- Begin from the definition of sample covariance for ensemble anomalies, the orthogonal diagonalization of a symmetric positive semi-definite matrix, and the notion of variance fractions across principal components. Use these to derive the participation ratio-based effective dimension as a function of principal component variances. Then implement an algorithm that computes this effective dimension from a given covariance matrix by first extracting its principal component variances.\n- Begin from the definition of Lyapunov exponents as long-time average exponential rates of divergence or convergence of nearby trajectories, ordered from largest to smallest. Use the standard construction that accumulates ordered exponents to identify where total expansion changes sign, and derive the Kaplan–Yorke dimension as a function of the ordered exponents. Then implement an algorithm that computes it from a given list of exponents. Clarify and implement the boundary cases where the largest exponent is negative, where all partial sums remain nonnegative up to the last exponent, and where the next exponent at the threshold is zero.\n\nInterpret both dimensions in terms of predictability limits and active degrees of freedom in numerical weather prediction and climate modeling. You must provide numerical outputs for a specified test suite, and you must implement the algorithms in a single program that outputs the required results. No external input is allowed.\n\nAll answers in this problem are dimensionless quantities. No physical units or angle units are required except where a rotation is explicitly specified; in that case, the angle is given in radians.\n\nDefinitions you must start from:\n- Let ensemble anomalies be column vectors in $\\mathbb{R}^n$. The sample covariance matrix is defined by $$\\mathbf{C}=\\frac{1}{m-1}\\sum_{k=1}^{m} \\mathbf{x}_k' \\mathbf{x}_k'^{\\top},$$ where $m$ is the ensemble size and $\\mathbf{x}_k'$ are anomalies.\n- Since $\\mathbf{C}$ is symmetric positive semi-definite, there exists an orthogonal matrix $\\mathbf{Q}$ such that $$\\mathbf{Q}^{\\top}\\mathbf{C}\\mathbf{Q}=\\operatorname{diag}(\\lambda_1,\\ldots,\\lambda_n),$$ with $\\lambda_i \\ge 0$ the principal component variances.\n- Lyapunov exponents $\\{\\ell_i\\}_{i=1}^n$ are defined as long-time average exponential rates and are ordered $\\ell_1 \\ge \\ell_2 \\ge \\cdots \\ge \\ell_n$.\n\nTest suite to evaluate your implementation:\n- Case $1$ (anisotropic covariance, moderately dissipative spectrum):\n  - Covariance eigenvalues: $[4.0,\\,1.0,\\,0.25,\\,0.25,\\,0.01]$.\n  - Lyapunov exponents: $[0.4,\\,0.05,\\,-0.02,\\,-0.3,\\,-0.5]$.\n- Case $2$ (isotropic covariance, boundary with a zero exponent):\n  - Covariance eigenvalues: $[2.0,\\,2.0,\\,2.0,\\,2.0,\\,2.0]$.\n  - Lyapunov exponents: $[0.2,\\,0.0,\\,-0.2,\\,-0.2,\\,-0.2]$.\n- Case $3$ (one dominant principal component, strongly damped remainder):\n  - Covariance eigenvalues: $[100.0,\\,0.001,\\,0.001,\\,0.001,\\,0.001]$.\n  - Lyapunov exponents: $[0.5,\\,-0.8,\\,-1.0,\\,-1.0,\\,-2.0]$.\n- Case $4$ (rank-deficient covariance with rotation, damped spectrum):\n  - Covariance eigenvalues: $[3.0,\\,2.0,\\,0.0,\\,0.0,\\,0.0]$ and an orthogonal rotation by angle $\\theta=\\pi/6$ radians in the first two coordinates, i.e., $$\\mathbf{Q}=\\begin{bmatrix}\\cos\\theta & -\\sin\\theta & 0 & 0 & 0\\\\ \\sin\\theta & \\cos\\theta & 0 & 0 & 0\\\\ 0 & 0 & 1 & 0 & 0\\\\ 0 & 0 & 0 & 1 & 0\\\\ 0 & 0 & 0 & 0 & 1\\end{bmatrix},\\quad \\mathbf{C}=\\mathbf{Q}\\,\\operatorname{diag}(3.0,\\,2.0,\\,0.0,\\,0.0,\\,0.0)\\,\\mathbf{Q}^{\\top}.$$\n  - Lyapunov exponents: $[0.3,\\,-0.4,\\,-0.5,\\,-1.0,\\,-2.0]$.\n- Case $5$ (mixed small variances, all exponents negative):\n  - Covariance eigenvalues: $[1.0,\\,0.5,\\,0.5,\\,0.2,\\,0.1]$.\n  - Lyapunov exponents: $[-0.05,\\,-0.1,\\,-0.2,\\,-0.3,\\,-0.4]$.\n\nRequired outputs:\n- For each case, compute the participation-ratio effective dimension $D_{\\mathrm{PR}}$ from the covariance and the Kaplan–Yorke dimension $D_{\\mathrm{KY}}$ from the Lyapunov spectrum. Also compute the difference $\\Delta=D_{\\mathrm{PR}}-D_{\\mathrm{KY}}$.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each case is represented as a list of three floats $[D_{\\mathrm{PR}},\\,D_{\\mathrm{KY}},\\,\\Delta]$ rounded to $6$ decimal places. For example, an output could look like $[[1.234000,2.345000,-1.111000],[\\dots],\\dots]$.\n\nDesign for coverage:\n- Case $1$ is a general \"happy path\" with mixed variances and a spectrum with a few weakly negative exponents.\n- Case $2$ tests the boundary condition where a partial sum of exponents is exactly zero and the covariance is isotropic.\n- Case $3$ tests extreme anisotropy in covariance with a single dominant mode and a spectrum with strong damping after the first exponent.\n- Case $4$ tests a non-diagonal covariance built via a rotation, ensuring eigenvalue extraction is invariant under orthogonal transformations.\n- Case $5$ tests the edge case where all Lyapunov exponents are negative, implying $D_{\\mathrm{KY}}=0$.\n\nYour solution must demonstrate the derivations and provide the algorithms used for the computations. The final program must implement these algorithms and output the results in the exact format specified.",
            "solution": "This problem requires the derivation and implementation of two distinct measures of effective dimensionality for a dynamical system: the Participation Ratio-based effective dimension, $D_{\\mathrm{PR}}$, derived from statistical analysis of an ensemble, and the Kaplan-Yorke dimension, $D_{\\mathrm{KY}}$, derived from the system's Lyapunov exponents. We will first derive the requisite formulae from fundamental principles, then discuss their interpretation, and finally outline the algorithms for their computation.\n\n### Part 1: Participation Ratio Effective Dimension ($D_{\\mathrm{PR}}$)\n\nThe Participation Ratio is a measure used to estimate the effective number of basis states that contribute to a given state. In our context, we apply this concept to the distribution of variance across the principal components of an ensemble of system states.\n\nLet the ensemble anomalies be represented by a set of vectors, from which a sample covariance matrix $\\mathbf{C}$ is computed. As stated, $\\mathbf{C}$ is a symmetric positive semi-definite matrix of size $n \\times n$. By the spectral theorem, it can be diagonalized by an orthogonal matrix $\\mathbf{Q}$:\n$$\n\\mathbf{Q}^{\\top}\\mathbf{C}\\mathbf{Q} = \\mathbf{\\Lambda} = \\operatorname{diag}(\\lambda_1, \\lambda_2, \\ldots, \\lambda_n)\n$$\nwhere $\\lambda_i \\ge 0$ are the eigenvalues of $\\mathbf{C}$. These eigenvalues, also known as principal component variances, quantify the amount of variance of the ensemble data along each principal component directions (the columns of $\\mathbf{Q}$).\n\nThe total variance of the ensemble is the sum of the variances along all directions, which is given by the trace of the covariance matrix. The trace is invariant under a change of basis, so:\n$$\n\\operatorname{Tr}(\\mathbf{C}) = \\sum_{i=1}^n \\lambda_i\n$$\nThe fraction of the total variance captured by the $i$-th principal component is:\n$$\np_i = \\frac{\\lambda_i}{\\sum_{j=1}^n \\lambda_j}\n$$\nThe set of fractions $\\{p_i\\}_{i=1}^n$ forms a discrete probability distribution, since $p_i \\ge 0$ and $\\sum_{i=1}^n p_i = 1$. This distribution describes how the system's variability is partitioned among the principal components.\n\nThe participation ratio, $D_{\\mathrm{PR}}$, quantifies the effective number of principal components contributing to the total variance. It is defined as the inverse of the sum of squared fractions, a measure related to the inverse Simpson index or Herfindahl-Hirschman index in other fields.\n$$\nD_{\\mathrm{PR}} = \\frac{1}{\\sum_{i=1}^n p_i^2}\n$$\nSubstituting the expression for $p_i$, we derive the formula for $D_{\\mathrm{PR}}$ in terms of the eigenvalues $\\lambda_i$:\n$$\nD_{\\mathrm{PR}} = \\frac{1}{\\sum_{i=1}^n \\left( \\frac{\\lambda_i}{\\sum_{j=1}^n \\lambda_j} \\right)^2} = \\frac{(\\sum_{j=1}^n \\lambda_j)^2}{\\sum_{i=1}^n \\lambda_i^2}\n$$\nThis is the formula to be implemented. Its value ranges from $1$ (if all variance is in a single PC, i.e., $\\lambda_k > 0$ for only one $k$) to $n$ (if variance is distributed equally among all PCs, i.e., all $\\lambda_i$ are equal and non-zero). If all $\\lambda_i = 0$, there is no variance, and the effective dimension is $0$.\n\n**Interpretation**: $D_{\\mathrm{PR}}$ measures the effective dimension of the subspace that captures the system's short-term variability, as represented by the ensemble. In numerical weather prediction, this relates to the number of \"growing modes\" or patterns that are most important for forecast uncertainty at the time of the forecast. A low $D_{\\mathrm{PR}}$ indicates that the uncertainty is concentrated in a few well-defined spatial patterns.\n\n**Algorithm for $D_{\\mathrm{PR}}$**:\n1.  Obtain the principal component variances (eigenvalues) $\\lambda_1, \\ldots, \\lambda_n$ of the covariance matrix $\\mathbf{C}$. In this problem, these are either given directly or are a trivial consequence of the eigendecomposition of $\\mathbf{C}$ as in Case 4.\n2.  Calculate the sum of the eigenvalues, $S_1 = \\sum_{i=1}^n \\lambda_i$.\n3.  If $S_1 = 0$, return $D_{\\mathrm{PR}} = 0$.\n4.  Calculate the sum of the squared eigenvalues, $S_2 = \\sum_{i=1}^n \\lambda_i^2$.\n5.  Compute $D_{\\mathrm{PR}} = S_1^2 / S_2$.\n\n### Part 2: Kaplan-Yorke Dimension ($D_{\\mathrm{KY}}$)\n\nThe Kaplan-Yorke dimension is a conjecture-based estimate for the fractal dimension of a chaotic attractor. It is derived from the system's Lyapunov exponents.\n\nLet the Lyapunov exponents be ordered from largest to smallest: $\\ell_1 \\ge \\ell_2 \\ge \\cdots \\ge \\ell_n$. These exponents quantify the average exponential rate of divergence or convergence of infinitesimally close trajectories along $n$ orthogonal directions in phase space.\n- A positive exponent ($\\ell_i > 0$) indicates an expanding (unstable) direction.\n- A zero exponent ($\\ell_i = 0$) corresponds to a neutral direction, typically along the flow of the trajectory.\n- A negative exponent ($\\ell_i < 0$) indicates a contracting (stable) direction.\n\nThe sum of the first $k$ exponents, $\\sum_{i=1}^k \\ell_i$, determines the rate of change of a $k$-dimensional volume element in phase space. The Kaplan-Yorke conjecture posits that the dimension of the attractor is the value $D$ for which an infinitesimal $D$-dimensional volume element neither grows nor shrinks on average.\n\nTo find this dimension, we first find the largest integer $j$ such that a $j$-dimensional volume element is non-contracting, i.e., the sum of the first $j$ exponents is non-negative.\n$$\n\\sum_{i=1}^j \\ell_i \\ge 0 \\quad \\text{and} \\quad \\sum_{i=1}^{j+1} \\ell_i < 0\n$$\nThis implies that the attractor has at least $j$ dimensions, but less than $j+1$. The Kaplan-Yorke dimension $D_{\\mathrm{KY}}$ is found by linear interpolation, adding a fractional part to $j$ that balances the positive sum $\\sum_{i=1}^j \\ell_i$ with the next, negative exponent $\\ell_{j+1}$.\n$$\nD_{\\mathrm{KY}} = j + \\frac{\\sum_{i=1}^j \\ell_i}{|\\ell_{j+1}|} = j + \\frac{\\sum_{i=1}^j \\ell_i}{-\\ell_{j+1}}\n$$\nThe second form is valid because $\\ell_{j+1}$ must be negative for the sum to cross zero.\n\nWe must consider the boundary cases:\n1.  **All exponents negative**: If $\\ell_1 < 0$, all trajectories converge to a stable fixed point. The attractor dimension is $0$. Our formula handles this: if $\\ell_1 < 0$, we define $j=0$ (an empty sum is $0$), and $D_{\\mathrm{KY}} = 0 + 0/|\\ell_1| = 0$.\n2.  **Sum of all exponents is non-negative**: If $\\sum_{i=1}^n \\ell_i \\ge 0$, the system is not dissipative enough to have a finite-dimensional attractor. The \"attractor\" fills the entire phase space. In this case, $D_{\\mathrm{KY}} = n$.\n3.  **Partial sum is zero**: If $\\sum_{i=1}^j \\ell_i = 0$ and $\\ell_{j+1} < 0$, the formula gives $D_{\\mathrm{KY}} = j + 0/|\\ell_{j+1}| = j$. This is a well-defined boundary.\n\n**Interpretation**: $D_{\\mathrm{KY}}$ estimates the dimension of the geometric object (the attractor) on which the system's long-term evolution is confined. It represents the number of active, dynamically relevant degrees of freedom in the system's long-term behavior. For predictability, a state can only be known to within the attractor's geometry, and its dimension quantifies this fundamental limit.\n\n**Algorithm for $D_{\\mathrm{KY}}$**:\n1.  Obtain the ordered Lyapunov exponents $\\ell_1, \\ldots, \\ell_n$.\n2.  If the list is empty or $\\ell_1 < 0$, return $D_{\\mathrm{KY}} = 0.0$.\n3.  Initialize a partial sum $S = 0.0$.\n4.  Iterate with index $j$ from $0$ to $n-1$:\n    a. Let $S_{\\text{next}} = S + \\ell_{j}$.\n    b. If $S_{\\text{next}} < 0$, then the crossover point is found. The integer part is $j$. The dimension is $D_{\\mathrm{KY}} = j + S / |\\ell_{j}|$. Return this value.\n    c. Update $S = S_{\\text{next}}$.\n5.  If the loop completes, it means all partial sums were non-negative. The dimension is the full dimension of the space, so return $D_{\\mathrm{KY}} = n$.\n\nThe difference $\\Delta = D_{\\mathrm{PR}} - D_{\\mathrm{KY}}$ compares the dimensionality of short-term uncertainty structure with that of the long-term attractor, providing insight into how well ensemble-based variability samples the underlying chaotic dynamics.",
            "answer": "```python\nimport numpy as np\n\ndef compute_d_pr(eigenvalues):\n    \"\"\"\n    Computes the Participation Ratio-based effective dimension.\n\n    Args:\n        eigenvalues (list or np.ndarray): A list of the eigenvalues (principal\n                                          component variances) of the covariance matrix.\n\n    Returns:\n        float: The effective dimension D_PR.\n    \"\"\"\n    lambdas = np.array(eigenvalues, dtype=float)\n    \n    # Filter out any non-positive eigenvalues, which don't contribute to variance\n    lambdas = lambdas[lambdas > 1e-12] # Use a small tolerance for zero\n    \n    if lambdas.size == 0:\n        return 0.0\n\n    sum_lambda = np.sum(lambdas)\n    sum_lambda_sq = np.sum(lambdas**2)\n\n    if sum_lambda_sq == 0.0:\n        # This case implies all positive eigenvalues are numerically zero,\n        # covered by the size==0 check, but as a safeguard.\n        return 0.0\n\n    d_pr = sum_lambda**2 / sum_lambda_sq\n    return d_pr\n\ndef compute_d_ky(lyap_exponents):\n    \"\"\"\n    Computes the Kaplan-Yorke dimension.\n\n    Args:\n        lyap_exponents (list or np.ndarray): A list of Lyapunov exponents,\n                                             ordered from largest to smallest.\n\n    Returns:\n        float: The Kaplan-Yorke dimension D_KY.\n    \"\"\"\n    exponents = np.array(lyap_exponents, dtype=float)\n    n = len(exponents)\n\n    if n == 0 or exponents[0]  0:\n        return 0.0\n\n    partial_sum = 0.0\n    for j, exp in enumerate(exponents):\n        if partial_sum + exp  0:\n            # The sum becomes negative when adding the j-th exponent (0-indexed).\n            # The integer part of the dimension is j.\n            # The fractional part is partial_sum / |exp|.\n            if abs(exp)  1e-12:\n                # This case should not be reached if partial_sum + exp  0\n                # and partial_sum = 0, unless exp is negative. If it is\n                # numerically zero, we can consider the dimension to be the\n                # integer part.\n                return float(j)\n            return j + partial_sum / abs(exp)\n        partial_sum += exp\n    \n    # If the loop completes, the sum of all exponents is non-negative.\n    # The dimension is the full phase space dimension.\n    return float(n)\n\n\ndef solve():\n    \"\"\"\n    Solves the problem by calculating D_PR, D_KY, and their difference\n    for a suite of test cases.\n    \"\"\"\n    test_cases = [\n        # Case 1: anisotropic covariance, moderately dissipative spectrum\n        {\n            \"cov_eigenvalues\": [4.0, 1.0, 0.25, 0.25, 0.01],\n            \"lyap_exponents\": [0.4, 0.05, -0.02, -0.3, -0.5]\n        },\n        # Case 2: isotropic covariance, boundary with a zero exponent\n        {\n            \"cov_eigenvalues\": [2.0, 2.0, 2.0, 2.0, 2.0],\n            \"lyap_exponents\": [0.2, 0.0, -0.2, -0.2, -0.2]\n        },\n        # Case 3: one dominant principal component, strongly damped remainder\n        {\n            \"cov_eigenvalues\": [100.0, 0.001, 0.001, 0.001, 0.001],\n            \"lyap_exponents\": [0.5, -0.8, -1.0, -1.0, -2.0]\n        },\n        # Case 4: rank-deficient covariance. Eigenvalues are given by the\n        # diagonal matrix in the SVD-like decomposition.\n        {\n            \"cov_eigenvalues\": [3.0, 2.0, 0.0, 0.0, 0.0],\n            \"lyap_exponents\": [0.3, -0.4, -0.5, -1.0, -2.0]\n        },\n        # Case 5: mixed small variances, all exponents negative\n        {\n            \"cov_eigenvalues\": [1.0, 0.5, 0.5, 0.2, 0.1],\n            \"lyap_exponents\": [-0.05, -0.1, -0.2, -0.3, -0.4]\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        d_pr = compute_d_pr(case[\"cov_eigenvalues\"])\n        d_ky = compute_d_ky(case[\"lyap_exponents\"])\n        delta = d_pr - d_ky\n        results.append([d_pr, d_ky, delta])\n\n    # Format the output string exactly as specified.\n    case_strings = []\n    for res in results:\n        d_pr_str = f\"{res[0]:.6f}\"\n        d_ky_str = f\"{res[1]:.6f}\"\n        delta_str = f\"{res[2]:.6f}\"\n        case_strings.append(f\"[{d_pr_str},{d_ky_str},{delta_str}]\")\n    \n    final_output = f\"[{','.join(case_strings)}]\"\n    print(final_output)\n\nsolve()\n```"
        }
    ]
}