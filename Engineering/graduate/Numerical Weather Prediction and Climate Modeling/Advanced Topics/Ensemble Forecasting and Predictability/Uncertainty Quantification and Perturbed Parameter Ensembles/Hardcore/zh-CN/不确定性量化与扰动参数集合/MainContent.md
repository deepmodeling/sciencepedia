## 引言
在模拟地球系统等复杂[非线性](@entry_id:637147)现象时，不确定性是[科学建模](@entry_id:171987)中一个无法回避的核心挑战。无论是源于我们对物理过程不完整的理解，还是模型本身的简化，这些不确定性都直接影响着预测和预估的可靠性。因此，如何系统地量化、归因并最终削减这些不确定性，已成为推动计算科学向前发展的关键问题。本文旨在为研究生及研究人员提供一个关于不确定性量化（UQ）的全面指南，并以[扰动参数集合](@entry_id:1129539)（PPE）作为核心工具进行深入探讨。

为实现这一目标，本文将分为三个紧密相连的部分。首先，在“原理与机制”一章中，我们将奠定理论基础，详细拆解不确定性的不同类型，并介绍贝叶斯推断、方差分解等用于分析不确定性的核心统计框架。接着，在“应用与跨学科联系”一章中，我们将展示这些理论和方法如何在数值天气预报、气候建模以及其他科学领域中发挥关键作用，并探讨其如何与数据同化等技术相结合以削减不确定性。最后，通过“动手实践”部分，读者将有机会通过具体的计算练习，将所学理论应用于解决实际问题，加深对核心概念的理解。通过这一结构化的学习路径，本文期望读者能够掌握不确定性量化的精髓，并将其有效地应用于各自的研究领域。

## 原理与机制

在对地球系统等复杂[非线性系统](@entry_id:168347)进行建模时，不确定性是不可避免的。这些不确定性源于我们对系统物理过程的不完全理解、观测数据的局限性以及模型本身的简化。不确定性量化（UQ）提供了一个严谨的框架，用于识别、表征和减少这些不确定性。[扰动参数集合](@entry_id:1129539)（Perturbed Parameter Ensembles, PPEs）是实施[不确定性量化](@entry_id:138597)的核心工具之一。本章将深入探讨支持不确定性量化和[扰动参数集合](@entry_id:1129539)的关键原理与机制。

### 不确定性的基本概念

在着手[量化不确定性](@entry_id:272064)之前，对其进行分类至关重要，因为不同类型的不确定性需要不同的处理方法。最基本的分类是将不确定性分为[偶然不确定性](@entry_id:634772)（aleatoric uncertainty）和认知不确定性（epistemic uncertainty）。

#### [偶然不确定性与认知不确定性](@entry_id:1120923)

**[偶然不确定性](@entry_id:634772)**，源于系统固有的、内在的随机性或变率。这种不确定性也被称为统计不确定性或不可约减不确定性。即使我们拥有一个完美的模型和完全精确的参数，[偶然不确定性](@entry_id:634772)依然存在。在数值天气预报（NWP）和气候模型中，其来源包括：
- **内在变率**：气候系统是混沌的，即使在固定的外部强迫下，由于初始条件的微小差异，系统也会展现出内部的、不可预测的年际到年代际变率。在气候预测的语境下，这种由初始状态$x_0$差异引起的集合离散度通常被视为[偶然不确定性](@entry_id:634772)，因为它代表了系统自身的可[变性](@entry_id:165583)。
- **[随机过程](@entry_id:268487)**：模型通常包含随机分量，以表示未解析的次网格过程。例如，一个随机的次网格倾向项$\eta(t)$可以用来模拟[大气湍流](@entry_id:200206)或对流过程中的随机波动。即使模型参数$\theta$和结构$M$完全固定，$\eta(t)$仍会为预报带来不确定性。

[偶然不确定性](@entry_id:634772)无法通过收集更多数据或改进模型来消除，但我们可以通过概率分布来对其进行表征和量化 。

**认知不确定性**，源于我们知识的缺乏。它也被称为系统不确定性或可约减不确定性。原则上，通过更多的观测、更优的理论或更精细的模型，认知不确定性可以被减小。其主要来源包括：
- **参数不确定性**：模型中许多参数的值并非基于第一性原理，而是通过观测数据估计或专家判断确定的。例如，云微物理方案中的[自动转化](@entry_id:1121257)和碰并系数，或对流方案中的夹卷率，都存在不确定性。
- **结构不确定性**：我们可能不确定描述某个物理过程的最优数学方程或函数形式。例如，一个模型可能包含多个可选的对流[参数化](@entry_id:265163)方案，每个方案都基于不同的物理假设。
- **初始条件不确定性**：在天气预报中，我们对大气在初始时刻的真实状态$x_0$的了解是不完整的，这是一种认知不确定性。

通过更有信息量的观测数据$D$来约束参数，例如使参数的[后验分布](@entry_id:145605)$p(\theta|D,M)$变得更集中，或者通过改进模型$M$的物理过程表示以减少其结构性偏差，认知不确定性是可以被减小的  。

#### 结构不确定性与参数不确定性

认知不确定性可以进一步细分为**参数不确定性**和**结构不确定性**。这种区分对于设计[不确定性量化](@entry_id:138597)实验至关重要。

**参数不确定性** 指的是在一个固定的模型结构$M$内部，其参数$\theta$的值不确定。例如，考虑一个阈值松弛形式的对流[参数化](@entry_id:265163)方案$T(q; \gamma, q_s) = \gamma \max(0, q - q_s)$，其中$q$是比湿。在这个固定的函数形式下，松弛率$\gamma$和阈值$q_s$的不确定性就是[参数不确定性](@entry_id:264387) 。

**结构不确定性** 指的是模型本身数学形式或结构$M$的不确定性。这源于我们对复杂过程（如云和对流）的简化表示存在多种可能。例如，除了上述的阈值松弛方案$T^{(B)}$，我们还可以选择一个幂律形式的方案$T^{(A)}(q; \alpha, \beta) = \alpha q^\beta$。这两个方案具有根本不同的数学行为：$T^{(A)}$对于任何正值的$q$都是正的，而$T^{(B)}$在$q$低于阈值$q_s$时为零。在这两个方案之间的选择，就构成了结构不确定性。任何一个方案内部的参数扰动（例如，在$T^{(A)}$中改变$\alpha$和$\beta$）都无法完全再现另一个方案的特征行为（例如$T^{(B)}$的阈值特性）。因此，结构不确定性反映了更深层次的知识缺失 。

### 用于量化不确定性的集合方法

[集合预报](@entry_id:1124525)是探索和量化上述不确定性对模型输出影响的主要工具。通过运行一组模型模拟，每组成员使用略微不同的输入，我们可以得到一个预测结果的分布，从而量化其不确定性。

- **初始条件集合（Initial Condition Ensembles）**：这种集合通过对初始状态$u_0$进行微小扰动来生成，而模型参数$\theta$保持不变。其主要目的是量化由初始条件误差和系统内部[混沌动力学](@entry_id:142566)（即[偶然不确定性](@entry_id:634772)）引起的预报离散度。这在短期和中期天气预报中至关重要 。

- **[扰动参数集合](@entry_id:1129539)（Perturbed Parameter Ensembles, PPEs）**：这种集合通过从一个代表其不确定性的概率分布$p(\theta)$中采样参数$\theta$来生成，而初始条件$u_0$通常保持不变或从[平衡态](@entry_id:270364)开始。PPE 的主要目标是量化由参数不确定性（一种认知不确定性）引起的模型输出的不确定性。集合的离散度直接反映了我们对模型物理过程[参数化](@entry_id:265163)的知识局限性 。

- **多模型集合（Multi-Model Ensembles, MMEs）**：这种集合通过组合来自不同模型结构$M$的输出来构建，每个模型可能具有不同的物理方案或甚至不同的[动力核心](@entry_id:1124042)。MME 是探索结构不确定性（另一种认知不确定性）的主要工具 。

在实践中，这些方法可以结合使用，例如一个同时扰动初始条件、参数和模型物理随机性的“超集合”（super-ensemble），以更全面地表示总的不确定性。

### 参数估计的统计基础

为了减少认知不确定性，我们需要利用观测数据来约束模型参数。贝叶斯推断为此提供了一个强大的形式化框架。

#### 用于从观测中学习的贝叶斯框架

贝叶斯推断的核心是将参数$\theta$视为一个[随机变量](@entry_id:195330)，并使用数据$y$来更新我们关于$\theta$的知识。这个过程涉及三个关键要素 ：

1.  **先验分布 (Prior Distribution) $p(\theta)$**：这代表了在观测数据$y$之前，我们对参数$\theta$不确定性的初始信念。先验可以基于专家知识（[主观先验](@entry_id:174420)）或基于信息论原则（[客观先验](@entry_id:167984)）。

2.  **[似然函数](@entry_id:921601) (Likelihood Function) $p(y|\theta)$**：这描述了在给定一组特定参数$\theta$的情况下，观测到数据$y$的概率。它将模型与数据联系起来。例如，对于一个加性高斯误差模型$y = H(M(\theta)) + \varepsilon$，其中$\varepsilon \sim \mathcal{N}(0, R)$，[似然函数](@entry_id:921601)为：
    $$
    p(y | \theta) \propto \exp\left(-\frac{1}{2}\left(y - H(M(\theta))\right)^\top R^{-1}\left(y - H(M(\theta))\right)\right)
    $$
    这里$H$是[观测算子](@entry_id:752875)，$M(\theta)$是模型预测，$R$是观测误差协方差矩阵。

3.  **后验分布 (Posterior Distribution) $p(\theta|y)$**：这是在考虑了观测数据$y$之后，我们对参数$\theta$不确定性的更新信念。它通过**[贝叶斯定理](@entry_id:897366)**计算得出：
    $$
    p(\theta | y) = \frac{p(y | \theta) p(\theta)}{p(y)} \propto p(y | \theta) p(\theta)
    $$
    [后验分布](@entry_id:145605)是[先验信息](@entry_id:753750)和数据信息（通过[似然函数](@entry_id:921601)体现）的结合。

在获得后验分布$p(\theta|y)$后，我们可以通过对所有可能的$\theta$值进行积分（[边缘化](@entry_id:264637)），来对未来某个观测$\tilde{y}$做出预测，并量化其不确定性。这就是**[后验预测分布](@entry_id:167931) (Posterior Predictive Distribution)**：
$$
p(\tilde{y} | y) = \int p(\tilde{y} | \theta) p(\theta | y) d\theta
$$
这个过程充分考虑了参数估计后仍然存在的认知不确定性。随着[信息量](@entry_id:272315)丰富的观测数据不断增加，[后验分布](@entry_id:145605)通常会变得越来越集中，并且受[先验分布](@entry_id:141376)的影响越来越小，最终由[似然函数](@entry_id:921601)主导。这反映了认知不确定性被数据约减的过程 。

#### 方差分解定律

[方差分解](@entry_id:912477)定律（Law of Total Variance）为我们提供了一个精确分解模型输出总不确定性的数学工具。假设模型输出$X$是一个[随机变量](@entry_id:195330)，其不确定性同时来自参数$\theta$和其他来源（如初始条件$I$）。总方差$\mathrm{Var}(X)$可以分解为：
$$
\mathrm{Var}(X) = \mathbb{E}[\mathrm{Var}(X | \theta)] + \mathrm{Var}(\mathbb{E}[X | \theta])
$$
这个恒等式具有深刻的物理解释 ：
-   **第一项 $\mathbb{E}[\mathrm{Var}(X|\theta)]$**：被称为“方差的期望”。$\mathrm{Var}(X|\theta)$是在参数$\theta$固定时$X$的方差，它代表了由初始条件不确定性和模型内部随机性引起的预报[离散度](@entry_id:168823)。对所有可能的$\theta$取期望，$\mathbb{E}[\mathrm{Var}(X|\theta)]$就代表了由非参数来源引起的平均不确定性。
-   **第二项 $\mathrm{Var}(\mathbb{E}[X|\theta])$**：被称为“期望的方差”。$\mathbb{E}[X|\theta]$是在参数$\theta$固定时的预报均值。$\mathrm{Var}(\mathbb{E}[X|\theta])$则衡量了这个预报均值如何随着参数$\theta$的变化而变化。因此，这一项直接量化了由参数不确定性$\theta$贡献的总方差部分。

这个分解是普遍成立的，不要求参数$\theta$与其他不确定性来源（如初始条件$I$）相互独立。它清晰地揭示了总不确定性的两个主要构成部分，为不确定性归因提供了坚实的理论基础。

#### 费雪信息矩阵

在实际进行昂贵的[贝叶斯推断](@entry_id:146958)之前，评估观测系统对参数的潜在约束能力是很有价值的。**费雪信息矩阵 (Fisher Information Matrix, FIM)** $I(\theta)$ 正是为此而设计的工具。它量化了观测数据$y$中包含的关于参数$\theta$的[信息量](@entry_id:272315)。$I(\theta)$有两种等价定义：
$$
I(\theta) = \mathbb{E}_{y | \theta}\left[ (\nabla_{\theta} \log p(y | \theta)) (\nabla_{\theta} \log p(y | \theta))^{\top} \right] = - \mathbb{E}_{y | \theta}\left[ \nabla_{\theta}^2 \log p(y | \theta) \right]
$$
第一种定义表明，[似然函数](@entry_id:921601)对参数越敏感（即得分函数$\nabla_\theta \log p$的方差越大），信息量就越大。第二种定义表明，负对数似然函数在参数$\theta$处的曲率越大，信息量也越大。

FIM 的重要性体现在**[克拉默-拉奥下界](@entry_id:154412) (Cramér-Rao Lower Bound, CRLB)** 中，它为任何[无偏估计量](@entry_id:756290)$\hat{\theta}$的协方差矩阵$\operatorname{Cov}(\hat{\theta})$设定了一个理论下限：
$$
\operatorname{Cov}(\hat{\theta}) \succeq I(\theta)^{-1}
$$
这里的$\succeq$表示矩阵半正定序。这个不等式意味着，FIM 的[逆矩阵](@entry_id:140380)给出了参数估计可能达到的最佳精度。一个“大”的 FIM（在矩阵意义上）意味着参数可以被精确估计。

对于前面提到的加性高斯误差模型，FIM 具有一个非常直观的形式 ：
$$
I(\theta) = \sum_{i=1}^N J_i(\theta)^{\top} R_i^{-1} J_i(\theta)
$$
其中$J_i(\theta) = \partial h_i(\theta)/\partial \theta$是模型输出对参数的[雅可比矩阵](@entry_id:178326)（即敏感性）。这个形式清晰地表明，信息量来源于模型对参数的敏感性$J_i(\theta)$，并由观测误差的倒数$R_i^{-1}$加权。

### [扰动参数集合](@entry_id:1129539)的分析与解释

生成 PPE 只是第一步，更关键的是如何从集合中提取科学洞见。

#### 敏感性分析：识别关键参数

[敏感性分析](@entry_id:147555)旨在确定哪些参数对模型输出的影响最大。

-   **局部[敏感性分析](@entry_id:147555) (Local Sensitivity Analysis)**：这通过[计算模型](@entry_id:637456)输出$y$在某个特定参数点$\theta^*$处对参数$\theta_i$的[偏导数](@entry_id:146280)$\partial y/\partial \theta_i$来实现。这种方法计算成本相对较低（尤其是在有伴随模型的情况下），能够快速诊断模型响应的方向（正或负）和局部刚度。然而，它的局限性在于其“局部”性：它无法捕捉参数在其整个不确定性范围内的非线性响应或参数间的相互作用 。

-   **全局敏感性分析 (Global Sensitivity Analysis)**：这是一种更全面的方法，它在整个[参数空间](@entry_id:178581)上量化参数不确定性对输出不确定性的贡献。基于方差的方法，如 **Sobol 指数**，尤其强大。
    -   **一阶 Sobol 指数 $S_i$** 定义为由参数$\theta_i$单独引起的输出方差占总方差的比例：
        $$
        S_i = \frac{\operatorname{Var}_{\theta_i}\!(\mathbb{E}[y \mid \theta_i])}{\operatorname{Var}(y)}
        $$
    -   **二阶 Sobol 指数 $S_{ij}$** 量化了参数$\theta_i$和$\theta_j$之间的纯[交互效应](@entry_id:164533)对输出方差的贡献，它剔除了各自的一阶效应：
        $$
        S_{ij} = \frac{\operatorname{Var}_{\theta_i,\theta_j}\!(\mathbb{E}[y \mid \theta_i,\theta_j])}{\operatorname{Var}(y)} - S_i - S_j
        $$
    Sobol 指数能够揭示参数的主效应、交互效应和模型的高度[非线性](@entry_id:637147)，这是局部敏感性分析无法做到的。因此，局部和[全局敏感性分析](@entry_id:171355)提供了互补的视角，共同帮助我们诊断[不确定性的来源](@entry_id:164809)并确定参数校准的优先次序 。

#### 参数可识别性与“潦草性”

在多[参数模型](@entry_id:170911)中，一个常见的问题是参数之间的“补偿效应”，即同时改变多个参数可能对模型输出产生非常微小的影响。这导致参数难以被唯一确定，即使它们对模型输出具有非零的敏感性。这种现象被称为**参数潦草性 (parameter sloppiness)**。

FIM 的[特征值分解](@entry_id:272091)是诊断“潦草性”的有力工具。FIM 的特征值反映了负[对数似然函数](@entry_id:168593)在不同方向上的曲率。
-   大的特征值对应“刚性”(stiff) 方向，即沿该方向微调参数组合会使[似然函数](@entry_id:921601)急剧下降，表明该参数组合能被数据很好地约束。
-   小的特征值对应“潦草”(sloppy) 方向。其对应的[特征向量](@entry_id:151813)指出了一个参数组合，沿此方向改变参数对模型输出的影响很小。

如果 FIM 的条件数（[最大特征值](@entry_id:1127078)$\lambda_{\max}$与[最小特征值](@entry_id:177333)$\lambda_{\min}$之比）非常大，则模型是“潦草的”。与[最小特征值](@entry_id:177333)$\lambda_{\min}$相关联的[特征向量](@entry_id:151813)$v_{\min}$精确地定义了这个最难被约束的参数组合方向。识别这些“潦草”方向对于理解模型的内在结构、参数补偿机制以及设计更有效的观测试验至关重要 。

### 计算机制与先进技术

实施上述分析，尤其是对于计算昂贵的[地球系统模型](@entry_id:1124096)，需要高效的计算策略。

#### 高效的集合设计：[拉丁超立方抽样](@entry_id:751167)

为了让 PPE 在有限的样本量下尽可能好地探索高维[参数空间](@entry_id:178581)，[采样策略](@entry_id:188482)至关重要。与简单的[独立同分布](@entry_id:169067)蒙特卡洛（MC）采样相比，**[拉丁超立方抽样](@entry_id:751167) (Latin Hypercube Sampling, LHS)** 是一种更高效的[方差缩减技术](@entry_id:141433)。

LHS 的构造确保在每个参数维度上，样本都均匀地分布在其概率分布的各个分层中。具体来说，对于$n$个样本和$d$个参数，每个参数的范围被划分为$n$个等概率的区间，并且每个区间内恰好只落入一个样本点。这种沿每个坐标轴的强制分层，使得样本在低维投影上具有极佳的均匀性 。

LHS 的主要优势在于，它在样本之间引入了负相关性。对于坐标单调的模型响应函数$f(X)$，这种负相关性可以传递到模型输出上，使得任意两个不同样本输出之间的协方差$\mathrm{Cov}(f(X^{(i)}), f(X^{(j)}))$为非正数。这直接导致样本均值[估计量的方差](@entry_id:167223)减小：
$$
\mathrm{Var}(\hat{\mu}_{LHS}) \le \mathrm{Var}(\hat{\mu}_{MC})
$$
这意味着，要达到相同的估计精度，LHS 所需的[样本量](@entry_id:910360)（即模型运行次数）通常远少于传统 MC 采样，从而极大地节省了计算资源 。

#### 代理建模：[高斯过程模拟器](@entry_id:749754)

当单个模型运行的成本极高，以至于连一个中等规模的 LHS 集合也难以承受时，**代理模型 (surrogate model)** 或 **模拟器 (emulator)** 便成为不可或缺的工具。其思想是用一个计算上非常廉价的统计模型来近似昂贵的物理模型。

**[高斯过程](@entry_id:182192) (Gaussian Process, GP)** 模拟器是一种特别强大和流行的代理模型。GP 是一个非参数的贝叶斯模型，它直接对函数进行建模。一个 GP 由一个**先验均函数$m(\theta)$** 和一个**[协方差函数](@entry_id:265031)（或核函数）$k(\theta, \theta')$** 完全定义。[核函数](@entry_id:145324)编码了关于[函数平滑](@entry_id:201048)性的先验知识，例如，常用的[自动相关性确定](@entry_id:746592)（ARD）[平方指数核](@entry_id:191141)：
$$
k(\theta,\theta') = \sigma_f^2 \exp\left(-\frac{1}{2}\sum_{j=1}^{p} \frac{(\theta_j - \theta'_j)^2}{\ell_j^2}\right)
$$
它假设输入参数$\theta$和$\theta'$越接近，其输出$y(\theta)$和$y(\theta')$的相关性就越强，并且能够为每个参数维度$j$学习一个不同的长度尺度$\ell_j$。

工作流程如下：
1.  在一个精心设计的小规模样本点集（例如，一个小的 LHS 设计）上运行昂贵的物理模型，获得训练数据$\{\theta_i, y_i\}$。
2.  使用这些训练数据来“训练”GP。这实质上是应用[贝叶斯定理](@entry_id:897366)来更新 GP 的先验，得到一个后验 GP。
3.  这个经过训练的 GP 模拟器可以对任何新的参数点$\theta_*$给出近乎瞬时的预测。关键是，GP 的预测是概率性的，它不仅提供一个预测均值$\mu(\theta_*)$，还提供一个预测方差$s^2(\theta_*)$，后者量化了模拟器在该点的不确定性。

GP 模拟器通过以极低的计算成本替代昂贵的模型评估，使得原本不可行的分析成为可能，例如进行密集的全局敏感性分析或在[马尔可夫链蒙特卡洛](@entry_id:138779)（MCMC）算法中进行[参数推断](@entry_id:753157) 。模拟器本身的不确定性$s^2(\theta_*)$也可以在整个不确定性量化链条中得到传递和考虑。