{
    "hands_on_practices": [
        {
            "introduction": "Before interpreting a rank histogram, it is essential to understand the fundamental statistical properties of an ideal, perfectly reliable forecast system. This first exercise  guides you through a foundational derivation of the expected number of counts in each rank bin and the inherent variability (standard deviation) of these counts. Mastering this calculation provides the statistical baseline needed to judge the performance of any real-world ensemble forecast.",
            "id": "4081430",
            "problem": "Consider an ensemble forecasting system in Numerical Weather Prediction (NWP) designed for probabilistic verification of a scalar quantity at a single location and lead time. An ensemble of size $m$ is used to construct the ensemble rank histogram for $N$ independent verification times. Under the reliability assumption, the verifying observation is exchangeable with the ensemble members so that the observation’s rank among the $m$ deterministic ensemble members is uniformly distributed across the $m+1$ possible ranks.\n\nAssume the following base:\n- The ensemble size is $m=20$, so there are $m+1$ rank bins.\n- The $N$ verification times are independent, with $N=400$.\n- Under reliability, the rank of each verification falls into each bin with equal probability.\n\nUsing only fundamental probability definitions (indicator variables, expectations, and variances for Bernoulli trials) and the construction of the multinomial model from independent trials with categorical outcomes, derive the expected count per rank bin and the standard deviation of the count in a single bin. Then evaluate these quantities for the specified $m$ and $N$.\n\nExpress the final numerical values as a pair in a single row matrix, ordered as “expected count per rank bin” and “standard deviation of the count in a single bin.” Round your answer to four significant figures. No units are required.",
            "solution": "The problem asks for the derivation and evaluation of the expected count and the standard deviation of the count in a single rank bin for a reliable ensemble forecast system.\n\nFirst, we must validate the problem statement.\n\n### Step 1: Extract Givens\n- Ensemble size: $m$\n- Number of independent verification times: $N$\n- Under reliability, the observation's rank is uniformly distributed across $m+1$ possible ranks.\n- Specific values: $m=20$, $N=400$.\n- Task: Derive the expected count per rank bin and the standard deviation of the count in a single bin using fundamental probability definitions (indicator variables, expectations, variances) and the multinomial model context. Evaluate for the given $m$ and $N$.\n- Output format: A pair of numerical values (expected count, standard deviation) rounded to four significant figures.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, as rank histograms and their statistical properties are a cornerstone of ensemble forecast verification in meteorology. The assumptions, such as reliability leading to a uniform rank distribution, and the statistical model (independent trials leading to a multinomial distribution) are standard in the field. The problem is well-posed, providing all necessary parameters ($m$, $N$) and assumptions for a unique solution. The language is objective and precise. The specified values ($m=20$, $N=400$) are realistic. Therefore, the problem is deemed valid.\n\n### Step 3: Verdict and Action\nThe problem is valid. We proceed with the solution.\n\n### Derivation\n\nLet $K$ be the number of possible ranks, or bins. Since the observation is ranked among the $m$ ensemble members, there are $m+1$ possible ranks (from being smaller than all members, to being larger than all members).\n$$K = m+1$$\nThe problem states that for a reliable ensemble, the probability of the verifying observation falling into any one of these $K$ bins is uniform. Let $p_j$ be the probability of the observation falling into bin $j$, for $j \\in \\{1, 2, ..., K\\}$.\n$$p_j = p = \\frac{1}{K} = \\frac{1}{m+1}$$\nThis holds for all bins $j$.\n\nWe are considering $N$ independent verification times. The count of observations falling into a specific bin $j$, which we denote by the random variable $C_j$, is the result of $N$ independent trials.\n\nTo derive the expectation and variance of $C_j$, we use the method of indicator variables as requested. For each verification time $i \\in \\{1, 2, ..., N\\}$, let us define an indicator random variable $I_{i,j}$ for a specific, arbitrary bin $j$:\n$$\nI_{i,j} =\n\\begin{cases}\n1  \\text{if observation } i \\text{ falls into bin } j \\\\\n0  \\text{otherwise}\n\\end{cases}\n$$\nEach $I_{i,j}$ follows a Bernoulli distribution with a \"success\" probability of $p = \\frac{1}{m+1}$. The expectation of a Bernoulli random variable is its success probability:\n$$\\mathbb{E}[I_{i,j}] = 1 \\cdot P(I_{i,j}=1) + 0 \\cdot P(I_{i,j}=0) = p = \\frac{1}{m+1}$$\nThe variance of a Bernoulli random variable is given by $p(1-p)$:\n$$\\text{Var}(I_{i,j}) = \\mathbb{E}[I_{i,j}^2] - (\\mathbb{E}[I_{i,j}])^2$$\nFor an indicator variable, $I_{i,j}^2 = I_{i,j}$, so $\\mathbb{E}[I_{i,j}^2] = \\mathbb{E}[I_{i,j}] = p$.\n$$\\text{Var}(I_{i,j}) = p - p^2 = p(1-p) = \\frac{1}{m+1}\\left(1 - \\frac{1}{m+1}\\right)$$\n\nThe total count $C_j$ in bin $j$ over all $N$ verification times is the sum of these independent indicator variables:\n$$C_j = \\sum_{i=1}^{N} I_{i,j}$$\nThis structure, a sum of $N$ independent and identically distributed Bernoulli trials, defines a Binomial distribution $C_j \\sim B(N, p)$. This is consistent with the requested context of a multinomial model, as the marginal distribution of the count in any single category of a multinomial distribution is a binomial distribution.\n\nThe expected count in bin $j$, $\\mathbb{E}[C_j]$, can be found using the linearity of expectation:\n$$\\mathbb{E}[C_j] = \\mathbb{E}\\left[\\sum_{i=1}^{N} I_{i,j}\\right] = \\sum_{i=1}^{N} \\mathbb{E}[I_{i,j}]$$\nSince $\\mathbb{E}[I_{i,j}] = p$ for all $i$:\n$$\\mathbb{E}[C_j] = \\sum_{i=1}^{N} p = Np = \\frac{N}{m+1}$$\nThis is the general expression for the expected count per rank bin.\n\nThe variance of the count in bin $j$, $\\text{Var}(C_j)$, is found by noting that the verification times are independent, which means the indicator variables $I_{i,j}$ for different $i$ are independent. For a sum of independent random variables, the variance of the sum is the sum of the variances:\n$$\\text{Var}(C_j) = \\text{Var}\\left(\\sum_{i=1}^{N} I_{i,j}\\right) = \\sum_{i=1}^{N} \\text{Var}(I_{i,j})$$\nSince $\\text{Var}(I_{i,j}) = p(1-p)$ for all $i$:\n$$\\text{Var}(C_j) = \\sum_{i=1}^{N} p(1-p) = Np(1-p)$$\nSubstituting $p = \\frac{1}{m+1}$:\n$$\\text{Var}(C_j) = N \\left(\\frac{1}{m+1}\\right) \\left(1 - \\frac{1}{m+1}\\right) = N \\frac{1}{m+1} \\frac{m+1-1}{m+1} = \\frac{Nm}{(m+1)^2}$$\nThe standard deviation of the count in a single bin, $\\sigma_{C_j}$, is the square root of the variance:\n$$\\sigma_{C_j} = \\sqrt{\\text{Var}(C_j)} = \\sqrt{\\frac{Nm}{(m+1)^2}}$$\n\nNow, we evaluate these quantities for the given values $m=20$ and $N=400$.\nThe number of bins is $K = m+1 = 20+1 = 21$.\n\nThe expected count per rank bin is:\n$$\\mathbb{E}[C_j] = \\frac{N}{m+1} = \\frac{400}{21} \\approx 19.047619...$$\nRounding to four significant figures, we get $19.05$.\n\nThe standard deviation of the count in a single bin is:\n$$\\sigma_{C_j} = \\sqrt{\\frac{Nm}{(m+1)^2}} = \\sqrt{\\frac{400 \\cdot 20}{(20+1)^2}} = \\sqrt{\\frac{8000}{21^2}} = \\sqrt{\\frac{8000}{441}} \\approx \\sqrt{18.140589...} \\approx 4.259176...$$\nRounding to four significant figures, we get $4.259$.\n\nThe final answer is the pair of these two values.",
            "answer": "$$\\boxed{\\begin{pmatrix} 19.05  4.259 \\end{pmatrix}}$$"
        },
        {
            "introduction": "While a flat, uniform rank histogram is the hallmark of a reliable ensemble, this can sometimes be a deceptive signal. This next practice  presents a carefully constructed thought experiment to reveal a common pitfall: how a forecast system with significant, but compensating, biases under different conditions (e.g., weather regimes) can produce a perfectly flat overall histogram. This exercise underscores the critical importance of conditional verification and warns against the dangers of over-interpreting aggregated results.",
            "id": "4081443",
            "problem": "In ensemble-based Numerical Weather Prediction (NWP) and climate modeling, the reliability of a forecast ensemble can be assessed using the rank histogram of the verifying observation’s rank among the ensemble members. For an exchangeable ensemble of size $m$, under reliability the rank $R$ of the observation among the $m$ ordered ensemble members is uniformly distributed on $\\{1,2,\\ldots,m+1\\}$, both unconditionally and conditionally on any covariate $C$ that may affect the joint distribution of forecasts and observations. Consider a binary regime indicator $C \\in \\{0,1\\}$ that captures two distinct meteorological regimes (for example, synoptically stable versus convective). Assume an exchangeable ensemble of size $m=2$ and define the conditional rank distributions as follows:\n- In regime $C=0$ (systematically underdispersed condition), the observation tends to fall outside the ensemble spread. The conditional rank probabilities are\n$$\n\\mathbb{P}(R=1 \\mid C=0)=\\mathbb{P}(R=3 \\mid C=0)=\\frac{9}{20}, \\quad \\mathbb{P}(R=2 \\mid C=0)=\\frac{1}{10}.\n$$\n- In regime $C=1$ (systematically overdispersed condition), the observation tends to lie between ensemble members. The conditional rank probabilities are\n$$\n\\mathbb{P}(R=1 \\mid C=1)=\\mathbb{P}(R=3 \\mid C=1)=\\frac{1}{5}, \\quad \\mathbb{P}(R=2 \\mid C=1)=\\frac{3}{5}.\n$$\nLet the regime frequency be $\\mathbb{P}(C=0)=p$ and $\\mathbb{P}(C=1)=1-p$. Using only the definition of rank histograms and the law of total probability, determine the value of $p$ for which the unconditional rank histogram is exactly flat, that is,\n$$\n\\mathbb{P}(R=r)=\\frac{1}{3} \\quad \\text{for each } r \\in \\{1,2,3\\}.\n$$\nExpress your answer as an exact fraction.",
            "solution": "The problem requires us to determine the value of a parameter $p$, representing the frequency of a meteorological regime $C=0$, such that the unconditional rank histogram of an ensemble forecast is flat. The ensemble size is given as $m=2$, which means the rank $R$ of the verifying observation can take values in the set $\\{1, 2, \\ldots, m+1\\}$, i.e., $R \\in \\{1, 2, 3\\}$.\n\nA flat or uniform unconditional rank histogram implies that the probability of the observation falling into any of the $m+1 = 3$ possible ranks is equal. This condition is formally stated as:\n$$\n\\mathbb{P}(R=r) = \\frac{1}{m+1} = \\frac{1}{3} \\quad \\text{for each } r \\in \\{1, 2, 3\\}.\n$$\nThe problem provides the conditional probabilities of the rank $R$ given the meteorological regime $C \\in \\{0, 1\\}$. The regimes occur with probabilities $\\mathbb{P}(C=0)=p$ and $\\mathbb{P}(C=1)=1-p$.\n\nTo find the unconditional probability $\\mathbb{P}(R=r)$, we must use the law of total probability, which allows us to compute the unconditional probability by summing over the conditional probabilities weighted by the probability of each condition. For the two regimes, this law is expressed as:\n$$\n\\mathbb{P}(R=r) = \\mathbb{P}(R=r \\mid C=0)\\mathbb{P}(C=0) + \\mathbb{P}(R=r \\mid C=1)\\mathbb{P}(C=1).\n$$\nSubstituting the given regime frequencies, we get:\n$$\n\\mathbb{P}(R=r) = \\mathbb{P}(R=r \\mid C=0) \\cdot p + \\mathbb{P}(R=r \\mid C=1) \\cdot (1-p).\n$$\nWe can establish an equation for $p$ by enforcing the condition $\\mathbb{P}(R=r) = \\frac{1}{3}$ for any one of the ranks $r \\in \\{1, 2, 3\\}$. Because the sum of probabilities $\\sum_{r=1}^3 \\mathbb{P}(R=r)$ must equal $1$, satisfying the condition for any two ranks automatically satisfies it for the third. Therefore, we only need to solve the equation for a single rank. Let us choose $r=1$.\n\nThe given conditional probabilities for $r=1$ are:\n$$\n\\mathbb{P}(R=1 \\mid C=0) = \\frac{9}{20}\n$$\n$$\n\\mathbb{P}(R=1 \\mid C=1) = \\frac{1}{5}\n$$\nSetting the unconditional probability $\\mathbb{P}(R=1)$ to be $\\frac{1}{3}$, we obtain the equation:\n$$\n\\frac{1}{3} = \\left(\\frac{9}{20}\\right) p + \\left(\\frac{1}{5}\\right)(1-p).\n$$\nWe now solve this linear equation for $p$. First, distribute the term $(1-p)$:\n$$\n\\frac{1}{3} = \\frac{9}{20} p + \\frac{1}{5} - \\frac{1}{5} p.\n$$\nNext, we gather all terms involving $p$ on one side of the equation and constant terms on the other:\n$$\n\\frac{1}{3} - \\frac{1}{5} = \\frac{9}{20} p - \\frac{1}{5} p.\n$$\nTo combine the terms, we find common denominators. For the left side, the common denominator is $15$. For the right side, it is $20$.\n$$\n\\frac{5}{15} - \\frac{3}{15} = \\left(\\frac{9}{20} - \\frac{4}{20}\\right) p.\n$$\nSimplifying both sides gives:\n$$\n\\frac{2}{15} = \\left(\\frac{5}{20}\\right) p.\n$$\nThe fraction on the right side simplifies to $\\frac{5}{20} = \\frac{1}{4}$.\n$$\n\\frac{2}{15} = \\frac{1}{4} p.\n$$\nFinally, to isolate $p$, we multiply both sides by $4$:\n$$\np = 4 \\times \\frac{2}{15} = \\frac{8}{15}.\n$$\nThis gives the required value of $p$. To ensure consistency, we can verify that this value of $p$ also yields $\\mathbb{P}(R=2) = \\frac{1}{3}$. We have $p = \\frac{8}{15}$ and $1-p = 1 - \\frac{8}{15} = \\frac{7}{15}$. The conditional probabilities for $r=2$ are:\n$$\n\\mathbb{P}(R=2 \\mid C=0)=\\frac{1}{10}, \\quad \\mathbb{P}(R=2 \\mid C=1)=\\frac{3}{5}.\n$$\nThe unconditional probability is:\n$$\n\\mathbb{P}(R=2) = \\left(\\frac{1}{10}\\right) \\left(\\frac{8}{15}\\right) + \\left(\\frac{3}{5}\\right) \\left(\\frac{7}{15}\\right) = \\frac{8}{150} + \\frac{21}{75}.\n$$\nUsing a common denominator of $150$:\n$$\n\\mathbb{P}(R=2) = \\frac{8}{150} + \\frac{21 \\times 2}{75 \\times 2} = \\frac{8}{150} + \\frac{42}{150} = \\frac{50}{150} = \\frac{1}{3}.\n$$\nThe condition holds for $r=2$. The conditional probabilities for $r=3$ are identical to those for $r=1$, so the same value of $p$ will satisfy $\\mathbb{P}(R=3) = \\frac{1}{3}$. Thus, the solution is correct. The value of $p$ for which the unconditional rank histogram is flat is $\\frac{8}{15}$.",
            "answer": "$$\n\\boxed{\\frac{8}{15}}\n$$"
        },
        {
            "introduction": "Visual inspection of a rank histogram is useful, but often we need to move beyond subjective assessment to formally test for reliability. This practice  provides the tools for such an objective analysis by guiding you through the derivation and application of the Pearson's chi-square goodness-of-fit test. This is a cornerstone of statistical verification, allowing you to quantify whether observed deviations from the ideal uniform distribution are statistically significant or merely the result of sampling variability.",
            "id": "4081467",
            "problem": "In ensemble-based numerical weather prediction, reliability of probabilistic forecasts is often assessed using the rank histogram of verifying observations relative to an $m$-member ensemble. For each verification time, the observation is ranked among the $m$ ordered ensemble members using mid-ranks that create $k=m+1$ mutually exclusive rank bins. Under a reliable ensemble and independent cases, the rank is uniformly distributed over the $k$ bins.\n\nConsider $n$ independent forecast–observation pairs that produce rank counts $\\{O_{r}\\}_{r=1}^{k}$ across the $k$ bins. Assume the null hypothesis of reliability, which implies a multinomial sampling model with equal bin probabilities $p_{r}=1/k$.\n\nUsing only fundamental definitions of the multinomial model and the multivariate Central Limit Theorem, derive a large-sample goodness-of-fit test for the uniformity of the rank distribution by:\n1. Constructing a quadratic form based on the asymptotic covariance of the vector of counts about its expectation, and\n2. Simplifying it to a scalar statistic that depends additively on binwise discrepancies between observed counts and their expectations.\n\nShow that the null asymptotic distribution of your test statistic is a chi-square distribution with $k-1$ degrees of freedom, and explain the origin of the loss of one degree of freedom.\n\nThen, for an ensemble with $m=10$ members observed over $n=220$ independent cases, the resulting rank histogram has $k=m+1=11$ bins with observed counts\n$\\{O_{r}\\}_{r=1}^{11}=\\{14,\\,19,\\,20,\\,26,\\,25,\\,22,\\,21,\\,24,\\,18,\\,17,\\,14\\}$.\nCompute the value of your test statistic under the reliability null, using the expected counts $E_{r}$ implied by the null model. Express your final numeric result as a pure number with no units, rounded to four significant figures.",
            "solution": "The problem asks for the derivation of a large-sample goodness-of-fit test for the uniformity of a rank histogram, its asymptotic distribution, and its application to a specific dataset. The validation of the problem statement confirms its scientific and mathematical soundness, completeness, and clarity. We may therefore proceed with the solution.\n\nLet the number of ensemble members be $m$. The rank of the observation is categorized into $k = m+1$ bins. We are given $n$ independent forecast-observation pairs, resulting in a vector of observed counts in each bin, $\\mathbf{O} = (O_1, O_2, \\ldots, O_k)^T$, where $\\sum_{r=1}^{k} O_r = n$.\n\nThe null hypothesis, $H_0$, states that the ensemble is reliable, which implies that the probability of an observation falling into any rank bin is equal. Thus, the probability for any bin $r$ is $p_r = 1/k$. Under $H_0$, the vector of counts $\\mathbf{O}$ follows a multinomial distribution, $\\mathbf{O} \\sim \\text{Multinomial}(n; p_1, p_2, \\ldots, p_k)$, with $p_r = 1/k$ for all $r$.\n\nThe expected count in each bin is $E_r = E[O_r] = n p_r = n/k$. The vector of expected counts is $\\mathbf{E} = (E_1, E_2, \\ldots, E_k)^T$.\n\nThe elements of the $k \\times k$ covariance matrix $\\boldsymbol{\\Sigma}$ of the vector $\\mathbf{O}$ are given by:\n$$\n\\text{Var}(O_r) = \\Sigma_{rr} = n p_r (1 - p_r) = n \\frac{1}{k} \\left(1 - \\frac{1}{k}\\right) = \\frac{n(k-1)}{k^2}\n$$\n$$\n\\text{Cov}(O_r, O_s) = \\Sigma_{rs} = -n p_r p_s = -n \\frac{1}{k} \\frac{1}{k} = -\\frac{n}{k^2} \\quad \\text{for } r \\neq s\n$$\n\nDue to the constraint $\\sum_{r=1}^k O_r = n$, the components of the vector $\\mathbf{O}$ are not linearly independent. This implies that the covariance matrix $\\boldsymbol{\\Sigma}$ is singular, with $\\text{rank}(\\boldsymbol{\\Sigma}) = k-1$.\n\nAccording to the multivariate Central Limit Theorem, for large $n$, the distribution of $\\mathbf{O}$ approaches a multivariate normal distribution with mean $\\mathbf{E}$ and covariance matrix $\\boldsymbol{\\Sigma}$. That is, the vector of deviations $\\mathbf{d} = \\mathbf{O} - \\mathbf{E}$ is asymptotically distributed as $N(\\mathbf{0}, \\boldsymbol{\\Sigma})$.\n\nTo construct the required quadratic form, we need an inverse of the covariance matrix. Since $\\boldsymbol{\\Sigma}$ is singular, we cannot use a standard inverse. A common approach is to consider a reduced vector of the first $k-1$ counts, $\\mathbf{O}^* = (O_1, O_2, \\ldots, O_{k-1})^T$. This vector is asymptotically distributed as a $(k-1)$-variate normal distribution with mean $\\mathbf{E}^* = (E_1, \\ldots, E_{k-1})^T$ and a non-singular $(k-1) \\times (k-1)$ covariance matrix $\\boldsymbol{\\Sigma}^*$, which is the upper-left submatrix of $\\boldsymbol{\\Sigma}$.\n\nThe test statistic $Q$ can then be constructed as a quadratic form:\n$$\nQ = (\\mathbf{O}^* - \\mathbf{E}^*)^T (\\boldsymbol{\\Sigma}^*)^{-1} (\\mathbf{O}^* - \\mathbf{E}^*)\n$$\nBy construction, this statistic will asymptotically follow a chi-square distribution with $k-1$ degrees of freedom.\n\nThe matrix $\\boldsymbol{\\Sigma}^*$ can be written as $\\boldsymbol{\\Sigma}^* = a\\mathbf{I}_{k-1} + b\\mathbf{J}_{k-1}$, where $\\mathbf{I}_{k-1}$ is the identity matrix and $\\mathbf{J}_{k-1}$ is the matrix of all ones. We have $\\Sigma^*_{rr} = a+b = \\frac{n(k-1)}{k^2}$ and $\\Sigma^*_{rs} = b = -\\frac{n}{k^2}$. This gives $a = \\frac{n(k-1)}{k^2} - (-\\frac{n}{k^2}) = \\frac{nk}{k^2} = \\frac{n}{k}$. The inverse is known to be $(\\boldsymbol{\\Sigma}^*)^{-1} = \\frac{1}{a}\\mathbf{I}_{k-1} - \\frac{b}{a(a+(k-1)b)}\\mathbf{J}_{k-1}$.\nSubstituting $a = n/k$ and $b = -n/k^2$:\n$$\na + (k-1)b = \\frac{n}{k} + (k-1)\\left(-\\frac{n}{k^2}\\right) = \\frac{nk - n(k-1)}{k^2} = \\frac{n}{k^2}\n$$\nSo, the inverse is:\n$$\n(\\boldsymbol{\\Sigma}^*)^{-1} = \\frac{k}{n}\\mathbf{I}_{k-1} - \\frac{-n/k^2}{(n/k)(n/k^2)}\\mathbf{J}_{k-1} = \\frac{k}{n}\\mathbf{I}_{k-1} + \\frac{k}{n}\\mathbf{J}_{k-1} = \\frac{k}{n}(\\mathbf{I}_{k-1} + \\mathbf{J}_{k-1})\n$$\nLet $\\mathbf{d}^* = \\mathbf{O}^* - \\mathbf{E}^* = (O_1-E_1, \\ldots, O_{k-1}-E_{k-1})^T$. The quadratic form is:\n$$\nQ = (\\mathbf{d}^*)^T \\left[\\frac{k}{n}(\\mathbf{I}_{k-1} + \\mathbf{J}_{k-1})\\right] \\mathbf{d}^* = \\frac{k}{n} \\left( (\\mathbf{d}^*)^T \\mathbf{d}^* + (\\mathbf{d}^*)^T \\mathbf{J}_{k-1} \\mathbf{d}^* \\right)\n$$\nThis simplifies to:\n$$\nQ = \\frac{k}{n} \\left( \\sum_{r=1}^{k-1} (O_r - E_r)^2 + \\left(\\sum_{r=1}^{k-1} (O_r - E_r)\\right)^2 \\right)\n$$\nFrom the constraint $\\sum_{r=1}^{k} O_r = n$ and $\\sum_{r=1}^{k} E_r = n$, we have $\\sum_{r=1}^{k} (O_r - E_r) = 0$.\nThis implies $\\sum_{r=1}^{k-1} (O_r - E_r) = -(O_k - E_k)$. Substituting this into the expression for $Q$:\n$$\nQ = \\frac{k}{n} \\left( \\sum_{r=1}^{k-1} (O_r - E_r)^2 + (-(O_k - E_k))^2 \\right) = \\frac{k}{n} \\sum_{r=1}^{k} (O_r - E_r)^2\n$$\nSince $E_r = n/k$, we can write this as:\n$$\nQ = \\sum_{r=1}^{k} \\frac{(O_r - E_r)^2}{n/k} = \\sum_{r=1}^{k} \\frac{(O_r - E_r)^2}{E_r}\n$$\nThis is the well-known Pearson's chi-square statistic. This derivation fulfills the requirement to construct a quadratic form and simplify it to a scalar statistic based on binwise discrepancies.\n\nThe null asymptotic distribution of this statistic is $\\chi^2_{k-1}$ (a chi-square distribution with $k-1$ degrees of freedom). The loss of one degree of freedom from the $k$ bins arises from the single linear constraint imposed on the counts: $\\sum_{r=1}^k O_r = n$. This constraint means that if the values of any $k-1$ counts are known, the $k$-th count is completely determined. Therefore, there are only $k-1$ freely varying quantities or \"degrees of freedom\" in the system. Mathematically, the vector of deviations $\\mathbf{d} = \\mathbf{O} - \\mathbf{E}$ is constrained to lie in a $(k-1)$-dimensional subspace of $\\mathbb{R}^k$, and the rank of the covariance matrix $\\boldsymbol{\\Sigma}$ is $k-1$, which determines the degrees of freedom of the resulting chi-square distribution.\n\nNow, we apply this test to the given data:\n- Number of ensemble members $m = 10$.\n- Number of bins $k = m+1 = 11$.\n- Number of cases $n = 220$.\n- Observed counts $\\{O_r\\}_{r=1}^{11} = \\{14, 19, 20, 26, 25, 22, 21, 24, 18, 17, 14\\}$.\n\nUnder the null hypothesis of reliability, the expected count for each bin is:\n$$\nE_r = \\frac{n}{k} = \\frac{220}{11} = 20\n$$\nThe test statistic is calculated as:\n$$\n\\chi^2 = \\sum_{r=1}^{11} \\frac{(O_r - E_r)^2}{E_r} = \\sum_{r=1}^{11} \\frac{(O_r - 20)^2}{20}\n$$\nLet's compute the terms for each bin:\n$$\n\\chi^2 = \\frac{(14-20)^2}{20} + \\frac{(19-20)^2}{20} + \\frac{(20-20)^2}{20} + \\frac{(26-20)^2}{20} + \\frac{(25-20)^2}{20} + \\frac{(22-20)^2}{20} + \\frac{(21-20)^2}{20} + \\frac{(24-20)^2}{20} + \\frac{(18-20)^2}{20} + \\frac{(17-20)^2}{20} + \\frac{(14-20)^2}{20}\n$$\n$$\n\\chi^2 = \\frac{(-6)^2}{20} + \\frac{(-1)^2}{20} + \\frac{0^2}{20} + \\frac{6^2}{20} + \\frac{5^2}{20} + \\frac{2^2}{20} + \\frac{1^2}{20} + \\frac{4^2}{20} + \\frac{(-2)^2}{20} + \\frac{(-3)^2}{20} + \\frac{(-6)^2}{20}\n$$\n$$\n\\chi^2 = \\frac{36}{20} + \\frac{1}{20} + \\frac{0}{20} + \\frac{36}{20} + \\frac{25}{20} + \\frac{4}{20} + \\frac{1}{20} + \\frac{16}{20} + \\frac{4}{20} + \\frac{9}{20} + \\frac{36}{20}\n$$\n$$\n\\chi^2 = \\frac{36+1+0+36+25+4+1+16+4+9+36}{20} = \\frac{168}{20} = 8.4\n$$\nThe problem requires the result to be rounded to four significant figures.\n$$\n\\chi^2 = 8.400\n$$",
            "answer": "$$\n\\boxed{8.400}\n$$"
        }
    ]
}