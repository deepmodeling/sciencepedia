{
    "hands_on_practices": [
        {
            "introduction": "A central challenge in subseasonal-to-seasonal (S2S) prediction is to identify and quantify the sources of predictability that extend beyond the typical weather forecast horizon. Slow-varying components of the Earth system, such as sea surface temperatures (SSTs) and soil moisture (SM), are known to be primary drivers of this extended-range skill. This exercise introduces the powerful methodology of controlled numerical experimentation, where the influence of one climate component is isolated by fixing others to their climatological average, allowing us to attribute forecast skill to specific physical processes . By working through a simplified linear model, you will derive and compute how the anomaly correlation coefficient (ACC) depends on the signal strength from these boundary conditions relative to intrinsic atmospheric noise.",
            "id": "4096547",
            "problem": "Consider a simplified weekly mean near-surface temperature anomaly model appropriate for Subseasonal-to-Seasonal (S2S) prediction in Numerical Weather Prediction (NWP) and climate modeling. The objective is to isolate the impact of Soil Moisture (SM) versus Sea Surface Temperature (SST) on weekly temperature predictability by designing controlled experiments with fixed climatological boundary fields. Let the weekly mean $2$-meter temperature anomaly be denoted by $T$, soil moisture anomaly by $W$, and sea surface temperature anomaly by $S$. Assume that, at weekly lead $1$, the temperature anomaly responds linearly to contemporaneous land and ocean anomalies and to fast atmospheric noise, with controlled initial conditions satisfying $T_0 = 0$ (climatology). The response is described by\n$$\nT_1 = b_w W_0 + b_s S_0 + \\eta_0,\n$$\nwhere $b_w$ and $b_s$ are linear sensitivity coefficients (units of temperature anomaly per unit anomaly of $W$ and $S$, respectively), $W_0$ and $S_0$ are zero-mean random initial anomalies with variances $\\sigma_w^2$ and $\\sigma_s^2$, and $\\eta_0$ is zero-mean fast atmospheric noise with variance $\\sigma_\\eta^2$, independent of $W_0$ and $S_0$. All variances are strictly nonnegative real numbers and $\\sigma_\\eta^2 > 0$ for scientific realism.\n\nDesign two controlled experiments:\n$1.$ Soil-only experiment: fix the SST boundary field to climatology, i.e., set $S_0 = 0$, while allowing $W_0$ to vary according to its distribution. Forecast the weekly mean temperature anomaly using the linear model and controlled initial conditions.\n$2.$ Ocean-only experiment: fix the SM boundary field to climatology, i.e., set $W_0 = 0$, while allowing $S_0$ to vary according to its distribution. Forecast the weekly mean temperature anomaly using the linear model and controlled initial conditions.\n\nDefine the Anomaly Correlation Coefficient (ACC) between the forecast anomaly and the verifying truth anomaly for the weekly mean, which is the Pearson correlation between anomalies computed as the covariance divided by the product of standard deviations. In both experiments, treat the forecast as the deterministic model response to the retained anomaly (SM or SST) under $T_0 = 0$, and treat the verifying truth as the model response including stochastic noise. The ACC is dimensionless.\n\nImplement a program that, given sets of parameters $(b_w, \\sigma_w, b_s, \\sigma_s, \\sigma_\\eta)$, computes for each test case:\n$1.$ The ACC under the soil-only experiment, denoted $\\mathrm{ACC}_{\\text{soil}}$.\n$2.$ The ACC under the ocean-only experiment, denoted $\\mathrm{ACC}_{\\text{ocean}}$.\n\nFor scientific consistency, assume $W_0 \\sim \\mathcal{N}(0,\\sigma_w^2)$, $S_0 \\sim \\mathcal{N}(0,\\sigma_s^2)$, and $\\eta_0 \\sim \\mathcal{N}(0,\\sigma_\\eta^2)$, all mutually independent, and use the standard correlation definition. Although $b_w$ and $b_s$ may implicitly carry physical units via the linear sensitivities, the outputs $\\mathrm{ACC}_{\\text{soil}}$ and $\\mathrm{ACC}_{\\text{ocean}}$ are dimensionless floats. No angles or percentages are involved in the outputs.\n\nUse the following test suite of parameter sets to evaluate the design:\n$1.$ $(b_w, \\sigma_w, b_s, \\sigma_s, \\sigma_\\eta) = (0.6, 1.2, 0.4, 1.0, 1.0)$.\n$2.$ $(b_w, \\sigma_w, b_s, \\sigma_s, \\sigma_\\eta) = (0.0, 1.0, 0.6, 1.2, 1.0)$.\n$3.$ $(b_w, \\sigma_w, b_s, \\sigma_s, \\sigma_\\eta) = (0.5, 1.0, 0.5, 1.0, 3.0)$.\n$4.$ $(b_w, \\sigma_w, b_s, \\sigma_s, \\sigma_\\eta) = (0.2, 1.0, 1.0, 1.5, 0.8)$.\n$5.$ $(b_w, \\sigma_w, b_s, \\sigma_s, \\sigma_\\eta) = (0.5, 1.0, 0.5, 1.0, 1.5)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each test case, output two floats in the order $\\mathrm{ACC}_{\\text{soil}}, \\mathrm{ACC}_{\\text{ocean}}$, rounded to three decimal places, concatenated across all test cases so that the final output is of the form $[\\mathrm{ACC}_{\\text{soil},1},\\mathrm{ACC}_{\\text{ocean},1},\\mathrm{ACC}_{\\text{soil},2},\\mathrm{ACC}_{\\text{ocean},2},\\dots]$. All outputs are dimensionless floats.",
            "solution": "The guiding principle is to express weekly mean temperature anomaly predictability as a correlation that arises from linear deterministic signal components and stochastic weather noise, with controlled isolation of Soil Moisture (SM) and Sea Surface Temperature (SST) influences via fixed climatological boundary fields. We begin with a linear response model rooted in the surface energy balance linearization and the well-tested behavior of weekly anomalies, where land and ocean anomalies exert a first-order linear influence on near-surface temperature:\n$$\nT_1 = b_w W_0 + b_s S_0 + \\eta_0,\n$$\nwith $T_0 = 0$ enforced to eliminate atmospheric persistence confounding and to implement controlled initial conditions that focus solely on land and ocean boundary anomaly impacts. Here, $W_0$ and $S_0$ denote zero-mean random initial anomalies with variances $\\sigma_w^2$ and $\\sigma_s^2$, respectively, and $\\eta_0$ denotes zero-mean fast atmospheric noise with variance $\\sigma_\\eta^2$. Assume mutual independence among $W_0$, $S_0$, and $\\eta_0$, which is a standard and tractable idealization for deriving predictability metrics.\n\nWe isolate SM and SST impacts by performing two experiments that fix the other boundary field to climatology (zero anomaly):\n$1.$ Soil-only: set $S_0 = 0$. The verifying truth is\n$$\nT_1^{(\\text{soil})} = b_w W_0 + \\eta_0,\n$$\nand the deterministic forecast based on the controlled initial condition and the linear model is\n$$\nF^{(\\text{soil})} = b_w W_0.\n$$\n$2.$ Ocean-only: set $W_0 = 0$. The verifying truth is\n$$\nT_1^{(\\text{ocean})} = b_s S_0 + \\eta_0,\n$$\nand the deterministic forecast is\n$$\nF^{(\\text{ocean})} = b_s S_0.\n$$\n\nTo quantify predictability, we use the Anomaly Correlation Coefficient (ACC), defined as the Pearson correlation between forecast and verifying truth anomalies. For any pair of random variables $X$ and $Y$ with finite variances, the correlation is\n$$\n\\rho(X,Y) = \\frac{\\mathrm{Cov}(X,Y)}{\\sqrt{\\mathrm{Var}(X)\\,\\mathrm{Var}(Y)}}.\n$$\nThis is a well-tested statistical formula appropriate for evaluating forecast skill as correlation.\n\nWe now compute $\\mathrm{ACC}_{\\text{soil}}$. Under the independence assumptions and zero means:\n$1.$ Compute $\\mathrm{Cov}(F^{(\\text{soil})}, T_1^{(\\text{soil})})$. Since $F^{(\\text{soil})} = b_w W_0$ and $T_1^{(\\text{soil})} = b_w W_0 + \\eta_0$, and $\\eta_0$ is independent of $W_0$ and has zero mean, we have\n$$\n\\mathrm{Cov}(F^{(\\text{soil})}, T_1^{(\\text{soil})}) = \\mathrm{Cov}(b_w W_0, b_w W_0 + \\eta_0) = b_w^2 \\mathrm{Var}(W_0) = b_w^2 \\sigma_w^2.\n$$\n$2.$ Compute $\\mathrm{Var}(F^{(\\text{soil})})$. Because $F^{(\\text{soil})} = b_w W_0$ with $W_0$ zero mean, we obtain\n$$\n\\mathrm{Var}(F^{(\\text{soil})}) = b_w^2 \\mathrm{Var}(W_0) = b_w^2 \\sigma_w^2.\n$$\n$3.$ Compute $\\mathrm{Var}(T_1^{(\\text{soil})})$. Using independence,\n$$\n\\mathrm{Var}(T_1^{(\\text{soil})}) = \\mathrm{Var}(b_w W_0 + \\eta_0) = b_w^2 \\sigma_w^2 + \\sigma_\\eta^2.\n$$\n$4.$ Combine these to get\n$$\n\\mathrm{ACC}_{\\text{soil}} = \\frac{b_w^2 \\sigma_w^2}{\\sqrt{(b_w^2 \\sigma_w^2)\\,(b_w^2 \\sigma_w^2 + \\sigma_\\eta^2)}} = \\sqrt{\\frac{b_w^2 \\sigma_w^2}{b_w^2 \\sigma_w^2 + \\sigma_\\eta^2}}.\n$$\n\nBy symmetry, the ocean-only case yields\n$1.$ $\\mathrm{Cov}(F^{(\\text{ocean})}, T_1^{(\\text{ocean})}) = b_s^2 \\sigma_s^2$,\n$2.$ $\\mathrm{Var}(F^{(\\text{ocean})}) = b_s^2 \\sigma_s^2$,\n$3.$ $\\mathrm{Var}(T_1^{(\\text{ocean})}) = b_s^2 \\sigma_s^2 + \\sigma_\\eta^2$,\nand hence\n$$\n\\mathrm{ACC}_{\\text{ocean}} = \\sqrt{\\frac{b_s^2 \\sigma_s^2}{b_s^2 \\sigma_s^2 + \\sigma_\\eta^2}}.\n$$\n\nThese expressions are dimensionless and reflect the isolation achieved by fixing boundary anomalies to climatology and using controlled initial conditions. They quantify the impact of SM and SST on weekly temperature predictability by directly comparing the signal-to-noise ratio embedded in the forecast-versus-truth correlation for each controlled experiment.\n\nAlgorithmic design for the program:\n$1.$ Encode the test suite as a list of parameter tuples $(b_w, \\sigma_w, b_s, \\sigma_s, \\sigma_\\eta)$.\n$2.$ For each tuple, compute $\\mathrm{ACC}_{\\text{soil}}$ using\n$$\n\\mathrm{ACC}_{\\text{soil}} = \\sqrt{\\frac{b_w^2 \\sigma_w^2}{b_w^2 \\sigma_w^2 + \\sigma_\\eta^2}}.\n$$\n$3.$ Compute $\\mathrm{ACC}_{\\text{ocean}}$ using\n$$\n\\mathrm{ACC}_{\\text{ocean}} = \\sqrt{\\frac{b_s^2 \\sigma_s^2}{b_s^2 \\sigma_s^2 + \\sigma_\\eta^2}}.\n$$\n$4.$ Round each ACC to $3$ decimal places and append them to a single flat list in the order $\\mathrm{ACC}_{\\text{soil}}, \\mathrm{ACC}_{\\text{ocean}}$ for each test case.\n$5.$ Print the final list on a single line, with comma-separated entries enclosed in square brackets, as specified.\n\nScientific realism and edge cases:\n$1.$ If $b_w = 0$ then $\\mathrm{ACC}_{\\text{soil}} = 0$ because the forecast has no signal; similarly, if $b_s = 0$ then $\\mathrm{ACC}_{\\text{ocean}} = 0$.\n$2.$ Larger $\\sigma_\\eta^2$ reduces both ACCs, reflecting greater weather noise that reduces weekly predictability.\n$3.$ Larger $b_w^2 \\sigma_w^2$ or $b_s^2 \\sigma_s^2$ increases ACCs, reflecting stronger land or ocean signals and/or larger initial anomaly variance contributing to forecastable signal.\n\nThe provided test suite covers a general case, a boundary case with no soil coupling, a noise-dominated case, a strong ocean coupling case, and a balanced case with equal sensitivities, ensuring comprehensive verification of the logic under distinct regimes. No physical units are required in the outputs, as ACCs are dimensionless by construction, and all calculations are confined to lead-$1$ weekly predictability under controlled boundary conditions.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef acc_linear(b: float, sigma_source: float, sigma_eta: float) -> float:\n    \"\"\"\n    Compute the Anomaly Correlation Coefficient (ACC) for a linear forecast:\n        F = b * X\n        T = b * X + eta\n    where X ~ N(0, sigma_source^2), eta ~ N(0, sigma_eta^2), independent.\n\n    ACC = sqrt( (b^2 * sigma_source^2) / (b^2 * sigma_source^2 + sigma_eta^2) )\n\n    Parameters\n    ----------\n    b : float\n        Linear sensitivity coefficient.\n    sigma_source : float\n        Standard deviation of the source anomaly (soil moisture or SST).\n    sigma_eta : float\n        Standard deviation of the atmospheric noise.\n\n    Returns\n    -------\n    float\n        Dimensionless ACC in [0, 1].\n    \"\"\"\n    signal_var = (b ** 2) * (sigma_source ** 2)\n    total_var = signal_var + (sigma_eta ** 2)\n    # Guard against numerical issues; total_var > 0 by construction in test suite.\n    if total_var <= 0.0:\n        return 0.0\n    return float(np.sqrt(signal_var / total_var))\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each tuple: (b_w, sigma_w, b_s, sigma_s, sigma_eta)\n    test_cases = [\n        (0.6, 1.2, 0.4, 1.0, 1.0),\n        (0.0, 1.0, 0.6, 1.2, 1.0),\n        (0.5, 1.0, 0.5, 1.0, 3.0),\n        (0.2, 1.0, 1.0, 1.5, 0.8),\n        (0.5, 1.0, 0.5, 1.0, 1.5),\n    ]\n\n    results = []\n    for (b_w, sigma_w, b_s, sigma_s, sigma_eta) in test_cases:\n        acc_soil = acc_linear(b_w, sigma_w, sigma_eta)\n        acc_ocean = acc_linear(b_s, sigma_s, sigma_eta)\n        # Round to three decimal places as specified\n        results.append(round(acc_soil, 3))\n        results.append(round(acc_ocean, 3))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Due to the chaotic nature of the atmosphere and the long lead times involved, S2S predictions are fundamentally probabilistic, providing a distribution of likely outcomes rather than a single deterministic value. Evaluating such forecasts requires metrics that go beyond simple right-or-wrong assessments. This practice focuses on the Brier score, a cornerstone for verifying probabilistic forecasts of binary events, such as the probability of experiencing above-average temperatures . You will implement the Brier score and its decomposition into reliability, resolution, and uncertainty, providing a deep diagnostic tool to understand not just *if* a forecast is skillful, but *why*.",
            "id": "4096511",
            "problem": "Consider probabilistic verification for Subseasonal-to-Seasonal (S2S) prediction within numerical weather prediction and climate modeling. Let a binary event represent occurrence of the upper-tercile category for a climate variable at a given lead time and location, with forecast probabilities and verifying outcomes collected over a sample of $N$ independent forecast-observation pairs. The forecast probabilities are denoted by $p_i \\in [0,1]$ and the outcomes by $o_i \\in \\{0,1\\}$ for $i=1,\\dots,N$. The Brier score (BS) for this binary event is defined as the mean squared error between forecast probabilities and outcomes, and its decomposition into reliability, resolution, and uncertainty is based on conditioning by forecast categories and the law of total variance. Reliability quantifies miscalibration across forecast categories, resolution quantifies the degree to which the forecast partitions the sample into subsets with differing event frequencies, and uncertainty depends only on the climatological event frequency.\n\nStarting from the definitions of probability, expectation, variance, and conditioning, as well as the definition of the Brier score as a mean squared error for binary events, derive a bin-wise decomposition of the Brier score by partitioning the forecasts into fixed probability bins. In this bin-wise approach, for a set of $K$ bins with edges $0=e_1 < e_2 < \\dots < e_{K+1}=1$, define the $k$-th bin as the set of indices for which $e_k \\le p_i < e_{k+1}$ for $k=1,\\dots,K-1$, and $e_K \\le p_i \\le e_{K+1}$ for the last bin. Let $n_k$ denote the number of points in bin $k$, let $\\bar{p}_k$ denote the mean forecast probability within bin $k$, and let $\\bar{o}_k$ denote the mean observed frequency within bin $k$. Let $\\bar{o}$ denote the overall climatological event frequency across the entire sample. Using these bin-wise statistics, compute:\n- the Brier score $BS$,\n- the reliability component,\n- the resolution component,\n- and the uncertainty component.\n\nYou must implement the bin-wise decomposition using the fixed bins specified below. Bins with $n_k=0$ must contribute zero to the reliability and resolution sums. All quantities are dimensionless and must be expressed as decimals. Angles do not appear in this problem.\n\nTest Suite:\nFor each test case, the program shall ingest the provided arrays of forecast probabilities and verifying binary outcomes, and use the fixed bin edges $E=[0,0.2,0.4,0.6,0.8,1.0]$ (with the last bin including $p=1.0$) to compute the requested quantities.\n\n- Test case 1 (perfect deterministic, correct forecasts):\n  - $N=12$\n  - $p=[0,0,0,0,0,0,1,1,1,0,0,1]$\n  - $o=[0,0,0,0,0,0,1,1,1,0,0,1]$\n\n- Test case 2 (uninformative climatology forecasts):\n  - $N=10$\n  - $p=[0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3]$\n  - $o=[1,0,0,0,1,0,1,0,0,0]$\n\n- Test case 3 (strongly miscalibrated, two-category forecasts):\n  - $N=12$\n  - $p=[0.8,0.8,0.8,0.8,0.8,0.8,0.2,0.2,0.2,0.2,0.2,0.2]$\n  - $o=[0,0,0,0,0,0,1,0,1,0,1,0]$\n\n- Test case 4 (mixed probabilities with extremes and mid-values):\n  - $N=8$\n  - $p=[0.0,0.0,1.0,1.0,0.5,0.5,0.1,0.9]$\n  - $o=[0,0,1,1,0,1,0,1]$\n\n- Test case 5 (single forecast-observation pair; edge case):\n  - $N=1$\n  - $p=[0.7]$\n  - $o=[1]$\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case yields a list of four decimals in the order $[BS,\\text{Reliability},\\text{Resolution},\\text{Uncertainty}]$. Each decimal must be rounded to six places. The overall output should be a list of these per-test-case lists; for example, a syntactically valid output would resemble $[[0.123456,0.010000,0.050000,0.163456],[\\dots],\\dots]$.",
            "solution": "We begin from the foundational definition of the Brier score for binary events. Given forecasts $p_i \\in [0,1]$ and outcomes $o_i \\in \\{0,1\\}$ for $i=1,\\dots,N$, the Brier score is the mean squared error\n$$\nBS \\equiv \\frac{1}{N} \\sum_{i=1}^{N} (p_i - o_i)^2 = \\mathbb{E}\\left[ (p - o)^2 \\right],\n$$\nwhere $\\mathbb{E}[\\cdot]$ denotes the sample mean.\n\nTo decompose $BS$ into reliability, resolution, and uncertainty, we invoke conditional expectation on forecast categories and the law of total variance, tailored to binary outcomes. Define forecast categories by partitioning the unit interval into fixed bins with edges $0=e_1 < e_2 < \\dots < e_{K+1}=1$. A forecast with value $p$ is assigned to bin $k$ if $e_k \\le p < e_{k+1}$ for $k=1,\\dots,K-1$, and $e_{K} \\le p \\le e_{K+1}$ for the last bin. Let the random variable $C$ denote the bin index of a forecast. Let $n_k$ be the number of forecasts in bin $k$, and define the bin weights $w_k \\equiv n_k/N$. Define the within-bin averages\n$$\n\\bar{p}_k \\equiv \\frac{1}{n_k} \\sum_{i \\in \\mathcal{I}_k} p_i, \\quad \\bar{o}_k \\equiv \\frac{1}{n_k} \\sum_{i \\in \\mathcal{I}_k} o_i, \\quad \\text{for } n_k>0,\n$$\nwhere $\\mathcal{I}_k$ are the indices belonging to bin $k$. Define the overall climatology\n$$\n\\bar{o} \\equiv \\frac{1}{N} \\sum_{i=1}^{N} o_i.\n$$\n\nThe bin-wise decomposition proceeds by conditioning on $C$ and expanding the square inside the expectation. For a given bin $k$, we write\n$$\n\\mathbb{E}\\left[ (p - o)^2 \\mid C=k \\right] = \\mathbb{E}\\left[ \\left( (p - \\bar{p}_k) + (\\bar{p}_k - \\bar{o}_k) + (\\bar{o}_k - o) \\right)^2 \\mid C=k \\right].\n$$\nExpanding the square and taking conditional expectations yields cross terms that vanish due to zero-mean properties of centered variables within each bin, leaving components that can be grouped into calibration (difference between bin-mean forecast and bin-mean outcome), stratification (difference between bin-mean outcome and climatology), and outcome variability (binary uncertainty relative to climatology). Aggregating over bins with weights $w_k$ gives\n$$\nBS = \\sum_{k=1}^{K} w_k (\\bar{p}_k - \\bar{o}_k)^2 - \\sum_{k=1}^{K} w_k (\\bar{o}_k - \\bar{o})^2 + \\bar{o}(1 - \\bar{o}) + \\Delta_{\\text{bin}},\n$$\nwhere $\\Delta_{\\text{bin}}$ collects terms associated with within-bin variation of $p$ around $\\bar{p}_k$ when bins include heterogeneous forecast values. In the special case where categories correspond to unique forecast probabilities (i.e., all $p_i$ in a category equal a common value), $\\Delta_{\\text{bin}}=0$ exactly and the decomposition identity\n$$\nBS = \\text{Reliability} - \\text{Resolution} + \\text{Uncertainty}\n$$\nholds exactly with\n$$\n\\text{Reliability} \\equiv \\sum_{k=1}^{K} w_k (\\bar{p}_k - \\bar{o}_k)^2, \\quad\n\\text{Resolution} \\equiv \\sum_{k=1}^{K} w_k (\\bar{o}_k - \\bar{o})^2, \\quad\n\\text{Uncertainty} \\equiv \\bar{o}(1 - \\bar{o}).\n$$\nWhen fixed bins group heterogeneous forecast values, this decomposition remains a standard and scientifically sound approximation, and the equality holds up to $\\Delta_{\\text{bin}}$ which tends to be small for sufficiently fine bins or weak within-bin variability.\n\nAlgorithmic design:\n1. Accept arrays $p_1,\\dots,p_N$ and $o_1,\\dots,o_N$ for a test case. Compute\n   $$\n   BS = \\frac{1}{N} \\sum_{i=1}^{N} (p_i - o_i)^2, \\quad \\bar{o} = \\frac{1}{N} \\sum_{i=1}^{N} o_i, \\quad \\text{Uncertainty} = \\bar{o}(1-\\bar{o}).\n   $$\n2. With fixed bin edges $E=[0,0.2,0.4,0.6,0.8,1.0]$, assign each forecast $p_i$ to a bin index $k$ using $e_k \\le p_i < e_{k+1}$ for $k=1,\\dots,K-1$ and $e_K \\le p_i \\le e_{K+1}$ for the last bin.\n3. For each bin $k$ with $n_k>0$, compute $w_k = n_k/N$, $\\bar{p}_k$, and $\\bar{o}_k$ as simple averages over the indices in the bin. If $n_k=0$, skip the bin (it contributes zero to the sums).\n4. Compute reliability and resolution by\n   $\n   \\text{Reliability} = \\sum_{k: n_k>0} w_k (\\bar{p}_k - \\bar{o}_k)^2\n   $\n   and\n   $\n   \\text{Resolution} = \\sum_{k: n_k>0} w_k (\\bar{o}_k - \\bar{o})^2.\n   $\n5. Return the tuple $[BS,\\text{Reliability},\\text{Resolution},\\text{Uncertainty}]$ for the test case.\n6. Repeat for each test case in the suite, and print a single line containing a list of these tuples with each decimal rounded to six places.\n\nScientific realism and edge handling:\n- The event is a tercile category occurrence represented as a binary variable, a standard construct in S2S verification.\n- Fixed bins across $[0,1]$ are widely used for bin-wise reliability diagrams and decompositions; bins with zero count contribute zero.\n- The single-observation edge case ($N=1$) is handled naturally by the definitions; reliability reduces to $(p - o)^2$, resolution is zero, and uncertainty is $\\bar{o}(1-\\bar{o})=0$ when $\\bar{o} \\in \\{0,1\\}$.\n- Deterministic perfect forecasts ($p_i \\in \\{0,1\\}$ and $p_i=o_i$) yield $BS=0$, reliability $=0$, resolution $=\\text{Uncertainty}$, consistent with the decomposition.\n\nThe final output is a single list of five per-test-case lists $[BS,\\text{Reliability},\\text{Resolution},\\text{Uncertainty}]$, each rounded to six decimal places, printed on one line as specified.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef brier_decomposition(p, o, edges):\n    \"\"\"\n    Compute Brier score and its bin-wise decomposition into reliability,\n    resolution, and uncertainty components using fixed bin edges.\n\n    Parameters:\n        p (list or np.ndarray): Forecast probabilities in [0,1].\n        o (list or np.ndarray): Binary outcomes in {0,1}.\n        edges (list or np.ndarray): Bin edges, ascending, covering [0,1],\n                                    last bin inclusive of right edge.\n\n    Returns:\n        tuple: (BS, REL, RES, UNC) as floats.\n    \"\"\"\n    p = np.asarray(p, dtype=float)\n    o = np.asarray(o, dtype=float)\n    N = p.size\n    if N == 0:\n        raise ValueError(\"Empty input arrays are not allowed.\")\n\n    # Core quantities\n    BS = float(np.mean((p - o) ** 2))\n    o_bar = float(np.mean(o))\n    UNC = o_bar * (1.0 - o_bar)\n\n    # Bin-wise stats\n    edges = np.asarray(edges, dtype=float)\n    K = edges.size - 1\n    REL = 0.0\n    RES = 0.0\n\n    for k in range(K):\n        left = edges[k]\n        right = edges[k + 1]\n        if k < K - 1:\n            idx = np.where((p >= left) & (p < right))[0]\n        else:\n            # Last bin includes right edge\n            idx = np.where((p >= left) & (p <= right))[0]\n        n_k = idx.size\n        if n_k == 0:\n            continue\n        w_k = n_k / N\n        p_k = float(np.mean(p[idx]))\n        o_k = float(np.mean(o[idx]))\n        REL += w_k * (p_k - o_k) ** 2\n        RES += w_k * (o_k - o_bar) ** 2\n\n    return BS, REL, RES, UNC\n\ndef solve():\n    # Define fixed bin edges as specified: last bin includes 1.0\n    edges = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1: perfect deterministic, correct forecasts\n        ( [0,0,0,0,0,0,1,1,1,0,0,1],\n          [0,0,0,0,0,0,1,1,1,0,0,1] ),\n        # Test case 2: uninformative climatology forecasts\n        ( [0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3],\n          [1,0,0,0,1,0,1,0,0,0] ),\n        # Test case 3: strongly miscalibrated, two-category forecasts\n        ( [0.8,0.8,0.8,0.8,0.8,0.8,0.2,0.2,0.2,0.2,0.2,0.2],\n          [0,0,0,0,0,0,1,0,1,0,1,0] ),\n        # Test case 4: mixed probabilities with extremes and mid-values\n        ( [0.0,0.0,1.0,1.0,0.5,0.5,0.1,0.9],\n          [0,0,1,1,0,1,0,1] ),\n        # Test case 5: single forecast-observation pair; edge case\n        ( [0.7],\n          [1] ),\n    ]\n\n    results = []\n    for p, o in test_cases:\n        BS, REL, RES, UNC = brier_decomposition(p, o, edges)\n        results.append([BS, REL, RES, UNC])\n\n    # Format each float to six decimal places and print the nested list\n    formatted_cases = []\n    for case in results:\n        formatted_values = [f\"{v:.6f}\" for v in case]\n        formatted_cases.append(\"[\" + \",\".join(formatted_values) + \"]\")\n    print(\"[\" + \",\".join(formatted_cases) + \"]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Raw output from numerical weather and climate models often exhibits systematic errors, or biases, such as being consistently too cold or having an incorrect range of variability. Statistical post-processing is a critical and ubiquitous step in operational forecasting to correct these deficiencies and enhance the value of the forecasts. This hands-on problem guides you through the implementation of a fundamental bias correction technique, mean-variance calibration, which adjusts the forecast distribution to better match the observed climatology . You will apply this correction and use standard verification metrics like the Root Mean Square Error (RMSE) and Anomaly Correlation (AC) to rigorously quantify the improvement.",
            "id": "4096533",
            "problem": "A weekly subseasonal-to-seasonal forecast system produces a time series of forecast temperatures for a fixed calendar week across multiple past years (the hindcast period) and a separate time series for a current validation period. Let the hindcast forecasts be a list of real numbers in degrees Celsius and let the hindcast observations be a list of real numbers in degrees Celsius for the same calendar week and years. The validation period consists of a list of real-valued weekly forecasts and a list of real-valued weekly observations for the same calendar week in subsequent years. You are asked to design and implement a program that performs a mean-variance calibration derived from first principles and then evaluates its impact using two metrics: anomaly correlation and root mean square error.\n\nStart from the following fundamental bases:\n- The laws of statistics governing linear transformations and moments: if a random variable $X$ has mean $\\mu_X$ and standard deviation $\\sigma_X$ (population definition), and $Y = aX + b$, then the mean of $Y$ is $a \\mu_X + b$ and the standard deviation of $Y$ is $|a| \\sigma_X$.\n- The population mean and population standard deviation for a finite list $\\{x_i\\}_{i=1}^N$ are defined as $\\mu = \\frac{1}{N} \\sum_{i=1}^N x_i$ and $\\sigma = \\sqrt{\\frac{1}{N} \\sum_{i=1}^N (x_i - \\mu)^2}$, respectively.\n- The Root Mean Square Error (RMSE) between two finite lists of equal length $\\{p_i\\}_{i=1}^N$ and $\\{q_i\\}_{i=1}^N$ is defined as $\\mathrm{RMSE} = \\sqrt{\\frac{1}{N} \\sum_{i=1}^N (p_i - q_i)^2}$.\n- The anomaly of a value relative to a climatological baseline is its difference from a reference mean. For weekly anomaly verification, use the hindcast observed mean for that calendar week as the climatological baseline.\n- The Pearson correlation coefficient between two finite lists $\\{x_i\\}_{i=1}^N$ and $\\{y_i\\}_{i=1}^N$ is defined as $\\rho = \\frac{\\sum_{i=1}^N (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^N (x_i - \\bar{x})^2} \\sqrt{\\sum_{i=1}^N (y_i - \\bar{y})^2}}$, where $\\bar{x}$ and $\\bar{y}$ are sample means across the $N$ values being correlated. In this problem, apply this to anomalies computed relative to the hindcast observed mean. If there are fewer than $2$ valid pairs or if either anomaly series has zero variance, define the anomaly correlation to be $0$.\n\nYour program must:\n- Estimate the hindcast forecast mean $\\mu_f$ and hindcast forecast standard deviation $\\sigma_f$ using the population definitions from the hindcast forecast list.\n- Estimate the hindcast observed mean $\\mu_o$ and hindcast observed standard deviation $\\sigma_o$ using the population definitions from the hindcast observation list.\n- Derive a linear transformation of the validation forecasts that matches the hindcast observed mean and standard deviation (i.e., yields a corrected forecast series with mean $\\mu_o$ and standard deviation $\\sigma_o$). Implement the resulting correction and use it to produce a corrected validation forecast series. If $\\sigma_f = 0$ (or effectively zero within numerical tolerance), define the corrected forecast to equal $\\mu_o$ for all validation times.\n- Compute the anomaly correlation and RMSE before correction (using the raw validation forecasts) and after correction (using the corrected validation forecasts). For anomalies, subtract $\\mu_o$ from both forecasts and observations. Express RMSE in degrees Celsius. If any validation forecast-observation pair contains a non-finite value, omit that pair from all metric calculations. If there are zero valid pairs, define both metrics as $0$.\n\nUse the following test suite, where all temperatures are in degrees Celsius:\n- Case $1$ (general happy path):\n    - Hindcast forecasts: $[10,12,14,16]$\n    - Hindcast observations: $[11,12,15,17]$\n    - Validation forecasts: $[13,15,17]$\n    - Validation observations: $[12,16,18]$\n- Case $2$ (boundary condition $\\sigma_f = 0$):\n    - Hindcast forecasts: $[14,14,14,14]$\n    - Hindcast observations: $[13,14,15,16]$\n    - Validation forecasts: $[14,14,14]$\n    - Validation observations: $[13,16,15]$\n- Case $3$ (boundary condition $N = 1$ in validation):\n    - Hindcast forecasts: $[8,9,10,11]$\n    - Hindcast observations: $[7,9,11,13]$\n    - Validation forecasts: $[10]$\n    - Validation observations: $[12]$\n- Case $4$ (negative covariance scenario):\n    - Hindcast forecasts: $[20,18,16,14,12]$\n    - Hindcast observations: $[15,16,17,18,19]$\n    - Validation forecasts: $[13,17,15,19]$\n    - Validation observations: $[19,15,17,13]$\n\nFinal output specification:\n- Your program should produce a single line of output containing a comma-separated list of results for the four cases, enclosed in square brackets. Each caseâ€™s result must be a list of four floats in the order $[\\text{AC}_{\\text{raw}}, \\text{RMSE}_{\\text{raw}}, \\text{AC}_{\\text{corr}}, \\text{RMSE}_{\\text{corr}}]$, where $\\text{AC}$ is the anomaly correlation (unitless) and $\\text{RMSE}$ is in degrees Celsius. For example, the format is $[[x_{1,1},x_{1,2},x_{1,3},x_{1,4}],[x_{2,1},x_{2,2},x_{2,3},x_{2,4}],[x_{3,1},x_{3,2},x_{3,3},x_{3,4}],[x_{4,1},x_{4,2},x_{4,3},x_{4,4}]]$ with no extra whitespace or text.",
            "solution": "The core of this problem is to derive and apply a linear statistical correction to a forecast time series. The correction, known as mean-variance calibration, aims to adjust the raw forecast distribution to match the observed climatological distribution from a long-term hindcast period. Let a raw forecast value be denoted by $f_{raw}$. We seek a linear transformation to a corrected forecast, $f_{corr}$, of the form:\n$$\nf_{corr} = a \\cdot f_{raw} + b\n$$\nwhere $a$ and $b$ are the calibration parameters. We determine these parameters by enforcing that the mean and standard deviation of the corrected forecast series match those of the hindcast observation series.\n\nLet $\\mu_f$ and $\\sigma_f$ be the population mean and standard deviation of the hindcast forecasts, and let $\\mu_o$ and $\\sigma_o$ be the corresponding statistics for the hindcast observations. Using the properties of linear transformations of random variables, we set up two equations:\n1.  **Matching the mean**: The expected value of the corrected forecast must equal the observed mean.\n    $$ \\mathbb{E}[f_{corr}] = a \\cdot \\mathbb{E}[f_{raw}] + b \\implies \\mu_o = a \\mu_f + b $$\n2.  **Matching the standard deviation**: The standard deviation of the corrected forecast must equal the observed standard deviation. We assume a positive scaling factor $a$ to preserve the sign of anomalies.\n    $$ \\mathrm{StdDev}[f_{corr}] = |a| \\cdot \\mathrm{StdDev}[f_{raw}] \\implies \\sigma_o = a \\sigma_f $$\n\nFrom the second equation, we can solve for the scaling factor $a$, provided that the forecast has non-zero variance ($\\sigma_f > 0$):\n$$\na = \\frac{\\sigma_o}{\\sigma_f}\n$$\nIf $\\sigma_f = 0$, the forecast is constant, and this formula is undefined. The problem statement specifies that in this case, the corrected forecast should be a constant series equal to $\\mu_o$.\n\nNext, we substitute the expression for $a$ back into the first equation to solve for the offset $b$:\n$$\nb = \\mu_o - a \\mu_f = \\mu_o - \\left(\\frac{\\sigma_o}{\\sigma_f}\\right) \\mu_f\n$$\nCombining these results, the full linear transformation for a raw validation forecast $f_{raw,val}$ is:\n$$\nf_{corr,val} = \\left(\\frac{\\sigma_o}{\\sigma_f}\\right) f_{raw,val} + \\left(\\mu_o - \\frac{\\sigma_o}{\\sigma_f} \\mu_f\\right)\n$$\nThis equation can be rearranged into a more intuitive form:\n$$\nf_{corr,val} = \\mu_o + \\frac{\\sigma_o}{\\sigma_f} (f_{raw,val} - \\mu_f)\n$$\nThis form clearly shows the process: take the raw forecast's deviation from its own climatological mean ($f_{raw,val} - \\mu_f$), scale this anomaly by the ratio of observed-to-forecast standard deviations, and then add this adjusted anomaly to the observed climatological mean $\\mu_o$.\n\nThe algorithmic steps are:\n1.  Calculate $\\mu_f, \\sigma_f, \\mu_o, \\sigma_o$ from the hindcast data using population statistics.\n2.  For each validation forecast, apply the transformation above. Handle the $\\sigma_f = 0$ case separately.\n3.  Compute the Anomaly Correlation (AC) and Root Mean Square Error (RMSE) for both the raw and corrected validation forecasts against the validation observations. Anomalies are calculated relative to the observed climatology, $\\mu_o$.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to process test cases and print results.\n    \"\"\"\n\n    def process_case(h_forecasts, h_observations, v_forecasts, v_observations):\n        \"\"\"\n        Processes a single test case for forecast calibration and verification.\n        \n        Args:\n            h_forecasts (list): Hindcast forecast values.\n            h_observations (list): Hindcast observation values.\n            v_forecasts (list): Validation forecast values.\n            v_observations (list): Validation observation values.\n\n        Returns:\n            list: A list of four floats [AC_raw, RMSE_raw, AC_corr, RMSE_corr].\n        \"\"\"\n        # Convert all inputs to numpy arrays for vectorized operations\n        h_forecasts_np = np.array(h_forecasts, dtype=float)\n        h_observations_np = np.array(h_observations, dtype=float)\n        v_forecasts_np = np.array(v_forecasts, dtype=float)\n        v_observations_np = np.array(v_observations, dtype=float)\n\n        # Filter validation data for finite pairs\n        valid_mask = np.isfinite(v_forecasts_np) & np.isfinite(v_observations_np)\n        valid_v_forecasts = v_forecasts_np[valid_mask]\n        valid_v_observations = v_observations_np[valid_mask]\n\n        # Handle edge case: zero valid pairs\n        if valid_v_forecasts.size == 0:\n            return [0.0, 0.0, 0.0, 0.0]\n\n        # --- Step 1: Calculate Hindcast Statistics ---\n        # Population mean and standard deviation for hindcast period\n        mu_f = np.mean(h_forecasts_np)\n        sigma_f = np.std(h_forecasts_np) # np.std computes population std dev by default\n        mu_o = np.mean(h_observations_np)\n        sigma_o = np.std(h_observations_np)\n\n        # The climatological baseline for anomalies is the hindcast observed mean\n        climatology = mu_o\n\n        # --- Helper function to compute metrics ---\n        def calculate_metrics(forecasts, observations):\n            \"\"\"Computes Anomaly Correlation and RMSE.\"\"\"\n            # RMSE\n            rmse = np.sqrt(np.mean((forecasts - observations)**2))\n            \n            # Anomaly Correlation (AC)\n            # Conditions for AC to be 0\n            if forecasts.size < 2:\n                ac = 0.0\n            else:\n                anom_f = forecasts - climatology\n                anom_o = observations - climatology\n                \n                # Check for zero variance in anomaly series\n                if np.isclose(np.var(anom_f), 0) or np.isclose(np.var(anom_o), 0):\n                    ac = 0.0\n                else:\n                    # np.corrcoef returns a 2x2 matrix\n                    ac = np.corrcoef(anom_f, anom_o)[0, 1]\n            return ac, rmse\n\n        # --- Step 2: Calculate Raw Metrics ---\n        ac_raw, rmse_raw = calculate_metrics(valid_v_forecasts, valid_v_observations)\n\n        # --- Step 3: Apply Correction to Validation Forecasts ---\n        # Check for the special case of zero forecast standard deviation\n        if np.isclose(sigma_f, 0):\n            # Corrected forecast is a constant series of the observed mean\n            corrected_v_forecasts = np.full_like(valid_v_forecasts, mu_o)\n        else:\n            # Apply the mean-variance calibration formula\n            # f_corr = mu_o + (sigma_o / sigma_f) * (f_raw - mu_f)\n            corrected_v_forecasts = mu_o + (sigma_o / sigma_f) * (valid_v_forecasts - mu_f)\n\n        # --- Step 4: Calculate Corrected Metrics ---\n        ac_corr, rmse_corr = calculate_metrics(corrected_v_forecasts, valid_v_observations)\n        \n        return [ac_raw, rmse_raw, ac_corr, rmse_corr]\n\n    # Define the test cases from the problem statement\n    test_cases = [\n        {\n            \"h_forecasts\": [10, 12, 14, 16],\n            \"h_observations\": [11, 12, 15, 17],\n            \"v_forecasts\": [13, 15, 17],\n            \"v_observations\": [12, 16, 18],\n        },\n        {\n            \"h_forecasts\": [14, 14, 14, 14],\n            \"h_observations\": [13, 14, 15, 16],\n            \"v_forecasts\": [14, 14, 14],\n            \"v_observations\": [13, 16, 15],\n        },\n        {\n            \"h_forecasts\": [8, 9, 10, 11],\n            \"h_observations\": [7, 9, 11, 13],\n            \"v_forecasts\": [10],\n            \"v_observations\": [12],\n        },\n        {\n            \"h_forecasts\": [20, 18, 16, 14, 12],\n            \"h_observations\": [15, 16, 17, 18, 19],\n            \"v_forecasts\": [13, 17, 15, 19],\n            \"v_observations\": [19, 15, 17, 13],\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        result = process_case(**case)\n        results.append(result)\n        \n    # Format the final output string according to the spec (no spaces)\n    case_strings = []\n    for res_list in results:\n        # Format each inner list: [float1,float2,...]\n        inner_str = f\"[{','.join(f'{x:.10g}' for x in res_list)}]\"\n        case_strings.append(inner_str)\n    \n    # Join the case strings and enclose in brackets: [[...],[...]]\n    final_output = f\"[{','.join(case_strings)}]\"\n    \n    print(final_output)\n\nsolve()\n```"
        }
    ]
}