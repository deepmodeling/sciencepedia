{
    "hands_on_practices": [
        {
            "introduction": "We begin with the Brier score, a cornerstone for verifying binary event forecasts. This first exercise explores its fundamental properties under hypothetical conditions to reveal its behavior. By deriving the expected score for a theoretically perfect forecast, first under perfect verification and then with simulated observation error, you will gain a crucial insight into how the verification process itself can influence evaluation results .",
            "id": "4079336",
            "problem": "Consider a binary event verification setting in numerical weather prediction and climate modeling, where the event indicator is denoted by $y \\in \\{0,1\\}$ and a forecast issues a probability $p \\in [0,1]$. The Brier score for a single binary forecast-observation pair is defined as $S(p,y) = (p - y)^{2}$. Suppose there exists a deterministic oracle that has access to the true event and issues $p = y$ for every case.\n\nNow consider two verification scenarios:\n- Perfect verification: the evaluation uses the true indicator $y$.\n- Observation error: the verification uses a corrupted indicator $\\tilde{y}$ such that, conditional on $y$, the observed $\\tilde{y}$ equals $y$ with probability $1 - \\epsilon$ and equals $1 - y$ with probability $\\epsilon$, where $0 \\leq \\epsilon \\leq 1$. Assume the misclassification is independent of the forecast and of the data-generating mechanism for $y$.\n\nUnder these assumptions, derive from first principles the expected Brier score in each scenario for the deterministic oracle $p = y$. In the observation error scenario, the expectation must be taken with respect to the joint distribution induced by the true $y$ and the error process for $\\tilde{y}$, and the final answer must be expressed purely as a function of $\\epsilon$ (i.e., it must not depend on the distribution of $y$).\n\nProvide the final answer as a two-entry row matrix containing:\n- the expected Brier score under perfect verification, and\n- the expected Brier score under the observation error model described above.\n\nNo numerical approximation is required. If you choose to simplify, express any final expression in exact symbolic form. The final answer must be unitless and expressed exactly.",
            "solution": "The problem asks for the expected Brier score for a deterministic oracle under two distinct verification scenarios: one with perfect observations and one with corrupted observations. We will address each scenario separately.\n\nThe Brier score for a single forecast-observation pair is given by $S(p,y) = (p - y)^{2}$, where $p \\in [0,1]$ is the forecast probability and $y \\in \\{0,1\\}$ is the binary event indicator.\nThe deterministic oracle is defined as having perfect knowledge of the true event, thus issuing the forecast $p = y$.\n\n**Scenario 1: Perfect Verification**\nIn this scenario, the evaluation uses the true event indicator, $y$. The forecast from the oracle is $p = y$. We substitute this into the Brier score definition:\n$$\nS(p,y) = S(y,y) = (y - y)^{2}\n$$\nFor any realization of the event, whether $y=0$ or $y=1$, the score is:\n$$\nS(y,y) = (0)^{2} = 0\n$$\nSince the score is identically $0$ for every possible outcome of the event $y$, its expected value is also $0$. Let $E[\\cdot]$ denote the expectation operator over the distribution of $y$.\n$$\nE[S(p,y)] = E[(y-y)^{2}] = E[0] = 0\n$$\nTherefore, the expected Brier score for the perfect oracle under perfect verification is $0$. This result is independent of the underlying climatological probability of the event $y$.\n\n**Scenario 2: Observation Error**\nIn this scenario, the evaluation uses a corrupted event indicator, $\\tilde{y}$. The oracle's forecast remains based on the true event, so $p=y$. The Brier score is now calculated as $S(p, \\tilde{y}) = (p - \\tilde{y})^{2}$. Substituting $p=y$, the score becomes:\n$$\nS(y, \\tilde{y}) = (y - \\tilde{y})^{2}\n$$\nWe are asked to compute the expected value of this score, $E[(y - \\tilde{y})^{2}]$, where the expectation is taken over the joint distribution of the true event $y$ and the corrupted observation $\\tilde{y}$. We can compute this using the law of total expectation, conditioning on the true event $y$:\n$$\nE[(y - \\tilde{y})^{2}] = E_{y} \\left[ E_{\\tilde{y}|y} \\left[ (y - \\tilde{y})^{2} | y \\right] \\right]\n$$\nLet's first compute the inner expectation, which is the expected score conditional on a specific realization of the true event $y$. The distribution of the corrupted observation $\\tilde{y}$ is given as conditional on $y$:\n$P(\\tilde{y} = y | y) = 1 - \\epsilon$\n$P(\\tilde{y} = 1 - y | y) = \\epsilon$\n\nThe term $(y - \\tilde{y})^{2}$ can take on two possible values depending on the value of $\\tilde{y}$:\n1. If $\\tilde{y} = y$ (no error), then $(y - \\tilde{y})^{2} = (y-y)^{2} = 0$. This occurs with probability $1 - \\epsilon$.\n2. If $\\tilde{y} = 1 - y$ (error), then $(y - \\tilde{y})^{2} = (y - (1-y))^{2} = (2y - 1)^{2}$. This occurs with probability $\\epsilon$.\n\nThe inner conditional expectation is therefore:\n$$\nE_{\\tilde{y}|y} \\left[ (y - \\tilde{y})^{2} | y \\right] = 0 \\cdot P(\\tilde{y}=y|y) + (2y - 1)^{2} \\cdot P(\\tilde{y}=1-y|y)\n$$\n$$\nE_{\\tilde{y}|y} \\left[ (y - \\tilde{y})^{2} | y \\right] = 0 \\cdot (1-\\epsilon) + (2y - 1)^{2} \\cdot \\epsilon = (2y - 1)^{2} \\epsilon\n$$\nNow, we must compute the outer expectation over the distribution of the true event $y$:\n$$\nE[(y - \\tilde{y})^{2}] = E_{y} \\left[ (2y - 1)^{2} \\epsilon \\right]\n$$\nSince $\\epsilon$ is a constant, we can pull it out of the expectation:\n$$\nE[(y - \\tilde{y})^{2}] = \\epsilon \\cdot E_{y} \\left[ (2y - 1)^{2} \\right]\n$$\nThe random variable $y$ can only take values in $\\{0, 1\\}$. Let's evaluate the term $(2y-1)^{2}$ for each case:\n- If $y=1$, then $(2(1)-1)^{2} = (1)^{2} = 1$.\n- If $y=0$, then $(2(0)-1)^{2} = (-1)^{2} = 1$.\nIn both possible states for the true event $y$, the quantity $(2y-1)^{2}$ is identically equal to $1$. Therefore, its expectation is also $1$, regardless of the probability distribution of $y$ (i.e., regardless of $P(y=1)$):\n$$\nE_{y} \\left[ (2y - 1)^{2} \\right] = 1\n$$\nSubstituting this back into our expression for the total expected score:\n$$\nE[(y - \\tilde{y})^{2}] = \\epsilon \\cdot 1 = \\epsilon\n$$\nThe expected Brier score for the perfect oracle under the specified observation error model is $\\epsilon$. This result depends only on the error probability $\\epsilon$ and not on the climatology of the event $y$, as required by the problem statement.\n\nTo summarize, the two required values are:\n1. Expected Brier score under perfect verification: $0$.\n2. Expected Brier score under observation error: $\\epsilon$.\n\nThese are presented as a two-entry row matrix.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0 & \\epsilon\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "While the Brier score is designed for binary events, the Continuous Ranked Probability Score (CRPS) is a powerful tool for evaluating probabilistic forecasts of continuous variables like temperature. This practice challenges you to bridge theory and application by deriving an efficient computational formula for the CRPS of an ensemble forecast. This exercise mirrors a common and essential task in operational weather prediction, demonstrating how to move from an abstract integral definition to a practical, implementable algorithm .",
            "id": "4079332",
            "problem": "A numerical weather prediction system issues an equally weighted ensemble forecast of daily $2$-meter air temperature (Kelvin) for a single location, with $m$ ensemble members $\\{x_{i}\\}_{i=1}^{m}$ and a verifying observation $y$. The task is to evaluate the quality of this probabilistic forecast using the Continuous Ranked Probability Score (CRPS), defined for a forecast cumulative distribution function $F$ and observation $y$ by the integral\n$$\n\\mathrm{CRPS}(F,y) = \\int_{-\\infty}^{\\infty} \\left(F(z) - \\mathbb{1}\\{z \\ge y\\}\\right)^{2} \\, dz,\n$$\nwhere $\\mathbb{1}\\{\\cdot\\}$ denotes the indicator function. For an equally weighted ensemble, the forecast cumulative distribution function is the empirical distribution\n$$\nF(z) = \\frac{1}{m} \\sum_{i=1}^{m} \\mathbb{1}\\{x_{i} \\le z\\}.\n$$\nStarting only from these definitions, derive a closed-form algebraic expression for $\\mathrm{CRPS}(F,y)$ in terms of the sorted ensemble $\\{x_{(i)}\\}_{i=1}^{m}$, the observation $y$, and cumulative sums that can be computed after sorting. Your expression must make clear how to achieve an algorithm with computational complexity $\\mathcal{O}(m \\log m)$ by sorting $\\{x_{i}\\}$ and using cumulative sums to evaluate the needed quantities.\n\nThen, apply your derived expression to the following ensemble (Kelvin) and observation (Kelvin), given in unsorted order:\n$$\n\\{x_{i}\\}_{i=1}^{8} = \\{300, 295, 303, 289, 298, 301, 296, 292\\}, \\quad y = 297.\n$$\nCompute the CRPS in Kelvin and round your final numerical answer to four significant figures. Express the final score in Kelvin. For context, note that other probability scoring rules such as the Brier Score (BS) and the Receiver Operating Characteristic (ROC) curve target different aspects of forecast quality, but this problem focuses on the CRPS.",
            "solution": "The problem requires the derivation of a closed-form expression for the Continuous Ranked Probability Score (CRPS) for an equally weighted ensemble forecast, starting from its integral definition. The CRPS for a forecast cumulative distribution function (CDF) $F$ and a scalar observation $y$ is given by\n$$\n\\mathrm{CRPS}(F,y) = \\int_{-\\infty}^{\\infty} \\left(F(z) - \\mathbb{1}\\{z \\ge y\\}\\right)^{2} \\, dz\n$$\nwhere $\\mathbb{1}\\{\\cdot\\}$ is the indicator function. The ensemble-based forecast CDF is the empirical distribution of the $m$ ensemble members $\\{x_i\\}_{i=1}^m$:\n$$\nF(z) = \\frac{1}{m} \\sum_{i=1}^{m} \\mathbb{1}\\{x_{i} \\le z\\}\n$$\nThe indicator function $\\mathbb{1}\\{z \\ge y\\}$ is equivalent to the Heaviside step function $H(z-y)$. We can split the integral at $z=y$:\n$$\n\\mathrm{CRPS}(F,y) = \\int_{-\\infty}^{y} \\left(F(z) - 0\\right)^{2} \\, dz + \\int_{y}^{\\infty} \\left(F(z) - 1\\right)^{2} \\, dz = \\int_{-\\infty}^{y} F(z)^2 \\, dz + \\int_{y}^{\\infty} \\left(1 - F(z)\\right)^2 \\, dz\n$$\nTo evaluate these integrals, we must first sort the ensemble members in non-decreasing order, denoted $\\{x_{(i)}\\}_{i=1}^{m}$, where $x_{(1)} \\le x_{(2)} \\le \\dots \\le x_{(m)}$. The empirical CDF $F(z)$ is a step function that is piecewise constant. Its value increases by $1/m$ at each $x_{(i)}$. Specifically, for $z$ in an interval $[x_{(i)}, x_{(i+1)})$, the number of ensemble members less than or equal to $z$ is $i$, so $F(z) = i/m$. For convenience, we define $x_{(0)} = -\\infty$ and $x_{(m+1)} = \\infty$.\nLet $k$ be the number of ensemble members less than or equal to the observation $y$. We can find $k$ such that $x_{(k)} \\le y < x_{(k+1)}$ (with $k=0$ if $y < x_{(1)}$ and $k=m$ if $y \\ge x_{(m)}$).\n\nLet's evaluate the first integral, $I_1 = \\int_{-\\infty}^{y} F(z)^2 \\, dz$. We partition the integration domain $(-\\infty, y]$ using the sorted ensemble members up to $x_{(k)}$.\n\\begin{align*}\nI_1 &= \\sum_{i=0}^{k-1} \\int_{x_{(i)}}^{x_{(i+1)}} F(z)^2 \\, dz + \\int_{x_{(k)}}^{y} F(z)^2 \\, dz \\\\\n&= \\sum_{i=1}^{k-1} \\int_{x_{(i)}}^{x_{(i+1)}} \\left(\\frac{i}{m}\\right)^2 \\, dz + \\int_{x_{(k)}}^{y} \\left(\\frac{k}{m}\\right)^2 \\, dz \\quad (\\text{since } F(z)=0 \\text{ for } z<x_{(1)}) \\\\\n&= \\frac{1}{m^2} \\left[ \\sum_{i=1}^{k-1} i^2 (x_{(i+1)} - x_{(i)}) + k^2 (y - x_{(k)}) \\right]\n\\end{align*}\nThe sum can be rearranged via summation by parts:\n\\begin{align*}\n\\sum_{i=1}^{k-1} i^2 (x_{(i+1)} - x_{(i)}) &= \\sum_{i=1}^{k-1} i^2 x_{(i+1)} - \\sum_{i=1}^{k-1} i^2 x_{(i)} \\\\\n&= \\sum_{j=2}^{k} (j-1)^2 x_{(j)} - \\sum_{i=1}^{k-1} i^2 x_{(i)} \\\\\n&= -x_{(1)} + \\sum_{i=2}^{k-1} \\left((i-1)^2 - i^2\\right) x_{(i)} + (k-1)^2 x_{(k)} \\\\\n&= -x_{(1)} + \\sum_{i=2}^{k-1} (1-2i) x_{(i)} + (k^2-2k+1) x_{(k)}\n\\end{align*}\nSubstituting this back into the expression for $I_1$:\n$$\nm^2 I_1 = -x_{(1)} + \\sum_{i=2}^{k-1} (1-2i) x_{(i)} + (k^2-2k+1) x_{(k)} + k^2 y - k^2 x_{(k)} = k^2 y + \\sum_{i=1}^{k} (1-2i) x_{(i)}\n$$\nSo, $I_1 = \\frac{1}{m^2} \\left[ k^2 y + \\sum_{i=1}^{k} (1-2i) x_{(i)} \\right]$.\n\nNow we evaluate the second integral, $I_2 = \\int_{y}^{\\infty} (1-F(z))^2 \\, dz$.\n\\begin{align*}\nI_2 &= \\int_{y}^{x_{(k+1)}} (1 - F(z))^2 \\, dz + \\sum_{i=k+1}^{m-1} \\int_{x_{(i)}}^{x_{(i+1)}} (1 - F(z))^2 \\, dz \\\\\n&= \\int_{y}^{x_{(k+1)}} \\left(1-\\frac{k}{m}\\right)^2 \\, dz + \\sum_{i=k+1}^{m-1} \\int_{x_{(i)}}^{x_{(i+1)}} \\left(1-\\frac{i}{m}\\right)^2 \\, dz \\quad (\\text{since } 1-F(z)=0 \\text{ for } z \\ge x_{(m)}) \\\\\n&= \\frac{1}{m^2} \\left[ (m-k)^2 (x_{(k+1)} - y) + \\sum_{i=k+1}^{m-1} (m-i)^2 (x_{(i+1)} - x_{(i)}) \\right]\n\\end{align*}\nUsing a similar summation by parts for the sum term, we find:\n$$\nm^2 I_2 = (m-k)^2 (x_{(k+1)} - y) - (m-k-1)^2 x_{(k+1)} + \\sum_{i=k+2}^{m-1} ((m-i+1)^2 - (m-i)^2) x_{(i)} + x_{(m)}\n$$\nThe coefficient becomes $2(m-i)+1$. This yields:\n$$\nm^2 I_2 = -(m-k)^2 y + (2(m-k)-1)x_{(k+1)} + \\sum_{i=k+2}^{m} (2(m-i)+1)x_{(i)} = -(m-k)^2 y + \\sum_{i=k+1}^{m} (2(m-i)+1) x_{(i)}\n$$\nSo, $I_2 = \\frac{1}{m^2} \\left[ -(m-k)^2 y + \\sum_{i=k+1}^{m} (2(m-i)+1) x_{(i)} \\right]$.\n\nCombining $I_1$ and $I_2$:\n\\begin{align*}\n\\mathrm{CRPS} &= I_1 + I_2 \\\\\n&= \\frac{1}{m^2} \\left[ (k^2 - (m-k)^2) y + \\sum_{i=1}^{k} (1-2i) x_{(i)} + \\sum_{i=k+1}^{m} (2m-2i+1) x_{(i)} \\right] \\\\\n&= \\frac{m(2k-m)}{m^2} y + \\frac{1}{m^2} \\left[ \\sum_{i=1}^{k} (1-2i) x_{(i)} + \\sum_{i=k+1}^{m} (2m-2i+1) x_{(i)} \\right]\n\\end{align*}\nThis expression can be computed in $\\mathcal{O}(m \\log m)$ due to sorting, followed by $\\mathcal{O}(m)$ for the sums. To make the use of cumulative sums explicit, we can simplify the summation term. Let $S_j = \\sum_{i=1}^j x_{(i)}$ and $W_j = \\sum_{i=1}^j i x_{(i)}$.\nThe summation term is $\\sum_{i=1}^m A_i x_{(i)}$ where $A_i = (1-2i)$ if $i \\le k$ and $A_i = (2m-2i+1)$ if $i > k$.\nThis can be rewritten as:\n$$\n\\sum_{i=1}^m (2m-2i+1)x_{(i)} - \\sum_{i=1}^k ( (2m-2i+1) - (1-2i) ) x_{(i)} \\\\\n= (2m+1)S_m - 2W_m - \\sum_{i=1}^k (2m) x_{(i)} \\\\\n= (2m+1)S_m - 2W_m - 2m S_k\n$$\nThus, the final expression for the CRPS is:\n$$\n\\mathrm{CRPS}(F,y) = \\frac{2k-m}{m} y + \\frac{1}{m^2} \\left[ (2m+1)\\sum_{i=1}^m x_{(i)} - 2\\sum_{i=1}^m i x_{(i)} - 2m\\sum_{i=1}^k x_{(i)} \\right]\n$$\nThis form shows that after sorting the ensemble members ($\\mathcal{O}(m \\log m)$), one can precompute the cumulative sum of members $S_m$, the weighted sum of members $W_m$, and the partial cumulative sums $S_k$ ($\\mathcal{O}(m)$). For any given observation $y$, finding $k$ takes $\\mathcal{O}(\\log m)$ via binary search, and the CRPS is then computed in $\\mathcal{O}(1)$.\n\nNow, we apply this formula to the given data.\nEnsemble: $\\{x_{i}\\}_{i=1}^{8} = \\{300, 295, 303, 289, 298, 301, 296, 292\\}$.\nObservation: $y = 297 \\, \\mathrm{K}$.\nNumber of members: $m=8$.\n\n1.  Sort the ensemble:\n    $\\{x_{(i)}\\}_{i=1}^{8} = \\{289, 292, 295, 296, 298, 300, 301, 303\\}$.\n\n2.  Determine $k$, the number of members $\\le y$:\n    $y=297$. The members $289, 292, 295, 296$ are less than or equal to $297$.\n    So, $k=4$.\n\n3.  Calculate the required sums:\n    $S_k = S_4 = \\sum_{i=1}^4 x_{(i)} = 289 + 292 + 295 + 296 = 1172$.\n    $S_m = S_8 = \\sum_{i=1}^8 x_{(i)} = 1172 + (298 + 300 + 301 + 303) = 1172 + 1202 = 2374$.\n    $W_m = W_8 = \\sum_{i=1}^8 i x_{(i)} = 1(289) + 2(292) + 3(295) + 4(296) + 5(298) + 6(300) + 7(301) + 8(303)$.\n    $W_8 = 289 + 584 + 885 + 1184 + 1490 + 1800 + 2107 + 2424 = 10763$.\n\n4.  Compute the CRPS.\n    We have $m=8$, $k=4$, $y=297$.\n    The first term is $\\frac{2k-m}{m} y = \\frac{2(4)-8}{8} (297) = \\frac{0}{8} (297) = 0$.\n    The second term requires the values: $m^2 = 64$, $2m+1 = 17$, $2m=16$.\n    $$\n    \\mathrm{CRPS} = 0 + \\frac{1}{64} \\left[ 17 \\cdot S_8 - 2 \\cdot W_8 - 16 \\cdot S_4 \\right]\n    $$\n    Substituting the computed sums:\n    \\begin{align*}\n    \\mathrm{CRPS} &= \\frac{1}{64} \\left[ 17(2374) - 2(10763) - 16(1172) \\right] \\\\\n    &= \\frac{1}{64} \\left[ 40358 - 21526 - 18752 \\right] \\\\\n    &= \\frac{1}{64} \\left[ 40358 - 40278 \\right] \\\\\n    &= \\frac{80}{64} = \\frac{5}{4} = 1.25\n    \\end{align*}\nThe CRPS is $1.25 \\, \\mathrm{K}$. Rounding to four significant figures gives $1.250 \\, \\mathrm{K}$.",
            "answer": "$$\\boxed{1.250}$$"
        },
        {
            "introduction": "Properly chosen scoring rules are not just for post-hoc evaluation; they form the objective functions we minimize to create better, more calibrated forecasts. This final exercise demonstrates this principle by framing the Brier score as a loss function for a parametric statistical model. You will derive the necessary calculus for a second-order optimization scheme, providing a direct link between scoring rules and the machine learning techniques used for forecast post-processing and calibration .",
            "id": "4079349",
            "problem": "A probabilistic classifier is used in Numerical Weather Prediction (NWP) to forecast the occurrence of a binary event (e.g., whether 24-hour accumulated precipitation exceeds a fixed threshold at a location). For a dataset of size $N$, let $\\{(x_{i}, y_{i})\\}_{i=1}^{N}$ denote feature vectors $x_{i} \\in \\mathbb{R}^{d}$ (derived from numerical weather prediction model outputs) and binary outcomes $y_{i} \\in \\{0,1\\}$. The model outputs a parametric probability forecast $p_{\\theta}(x_{i}) \\in (0,1)$, where $\\theta \\in \\mathbb{R}^{d}$, and $p_{\\theta}(\\cdot)$ is twice continuously differentiable in $\\theta$.\n\nDefine the sample-average Brier score\n$$\nJ(\\theta) \\equiv \\frac{1}{N} \\sum_{i=1}^{N} \\big(p_{\\theta}(x_{i}) - y_{i}\\big)^{2}.\n$$\n\nStarting from core definitions in probability scoring rules and calculus (chain rule and product rule), and without invoking any pre-derived gradient or Hessian identities:\n\n1) Derive the gradient $\\nabla_{\\theta} J(\\theta)$ and Hessian $\\nabla_{\\theta}^{2} J(\\theta)$ in terms of $\\nabla_{\\theta} p_{\\theta}(x_{i})$ and $\\nabla_{\\theta}^{2} p_{\\theta}(x_{i})$.\n\n2) Specialize your results to the logistic calibration model $p_{\\theta}(x) = \\sigma(\\theta^{\\top} x)$ with $\\sigma(u) = \\frac{1}{1 + \\exp(-u)}$, and express $\\nabla_{\\theta} J(\\theta)$ and $\\nabla_{\\theta}^{2} J(\\theta)$ explicitly in terms of $\\{x_{i}, y_{i}\\}$ and $\\theta$.\n\n3) Propose a second-order optimization scheme suitable for calibration, based on a damped Newton step using a Tikhonov regularization parameter $\\gamma > 0$, and write the single-iteration update formula $\\theta_{t+1}$ in terms of the gradient and Hessian you derived.\n\nExpress your final answer as a single closed-form analytic expression collecting the expressions for the gradient, the Hessian, and the damped Newton update in one line using a row matrix. No numerical evaluation is required. Do not include any units. No rounding is required.",
            "solution": "The objective function is the sample-average Brier score, given by\n$$\nJ(\\theta) = \\frac{1}{N} \\sum_{i=1}^{N} \\left( p_{\\theta}(x_i) - y_i \\right)^2\n$$\nFor notational simplicity, let $p_i(\\theta) = p_{\\theta}(x_i)$. The objective function is a sum of individual loss terms, $J(\\theta) = \\frac{1}{N} \\sum_{i=1}^{N} L_i(\\theta)$, where $L_i(\\theta) = (p_i(\\theta) - y_i)^2$. Due to the linearity of the differentiation operator, we can compute the gradient and Hessian by summing the contributions from each term.\n\n**1) General Gradient and Hessian**\n\nFirst, we compute the gradient vector $\\nabla_{\\theta} J(\\theta)$. The $k$-th component of the gradient is the partial derivative with respect to $\\theta_k$:\n$$\n\\frac{\\partial J(\\theta)}{\\partial \\theta_k} = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{\\partial}{\\partial \\theta_k} \\left( p_i(\\theta) - y_i \\right)^2\n$$\nApplying the chain rule gives:\n$$\n\\frac{\\partial J(\\theta)}{\\partial \\theta_k} = \\frac{1}{N} \\sum_{i=1}^{N} 2 \\left( p_i(\\theta) - y_i \\right) \\frac{\\partial p_i(\\theta)}{\\partial \\theta_k}\n$$\nAssembling these components into a vector yields the gradient:\n$$\n\\nabla_{\\theta} J(\\theta) = \\frac{2}{N} \\sum_{i=1}^{N} \\left( p_i(\\theta) - y_i \\right) \\nabla_{\\theta} p_i(\\theta)\n$$\n\nNext, we compute the Hessian matrix $\\nabla_{\\theta}^2 J(\\theta)$. The entry at row $j$ and column $k$ of the Hessian is the second partial derivative, $\\frac{\\partial^2 J(\\theta)}{\\partial \\theta_j \\partial \\theta_k}$. We differentiate the expression for $\\frac{\\partial J(\\theta)}{\\partial \\theta_k}$ with respect to $\\theta_j$:\n$$\n\\frac{\\partial^2 J(\\theta)}{\\partial \\theta_j \\partial \\theta_k} = \\frac{\\partial}{\\partial \\theta_j} \\left[ \\frac{2}{N} \\sum_{i=1}^{N} \\left( p_i(\\theta) - y_i \\right) \\frac{\\partial p_i(\\theta)}{\\partial \\theta_k} \\right]\n$$\nApplying the product rule to the terms inside the summation:\n$$\n\\frac{\\partial}{\\partial \\theta_j} \\left[ \\left( p_i(\\theta) - y_i \\right) \\frac{\\partial p_i(\\theta)}{\\partial \\theta_k} \\right] = \\left( \\frac{\\partial p_i(\\theta)}{\\partial \\theta_j} \\right) \\left( \\frac{\\partial p_i(\\theta)}{\\partial \\theta_k} \\right) + \\left( p_i(\\theta) - y_i \\right) \\left( \\frac{\\partial^2 p_i(\\theta)}{\\partial \\theta_j \\partial \\theta_k} \\right)\n$$\nIn matrix notation, the first term $\\frac{\\partial p_i(\\theta)}{\\partial \\theta_j} \\frac{\\partial p_i(\\theta)}{\\partial \\theta_k}$ corresponds to the $(j,k)$-th element of the outer product $(\\nabla_{\\theta} p_i(\\theta))(\\nabla_{\\theta} p_i(\\theta))^{\\top}$. The second term is the $(j,k)$-th element of the matrix $( p_i(\\theta) - y_i ) \\nabla_{\\theta}^2 p_i(\\theta)$. Summing over all data points and multiplying by the constant factor, the Hessian matrix is:\n$$\n\\nabla_{\\theta}^2 J(\\theta) = \\frac{2}{N} \\sum_{i=1}^{N} \\left[ (\\nabla_{\\theta} p_i(\\theta)) (\\nabla_{\\theta} p_i(\\theta))^{\\top} + (p_i(\\theta) - y_i) \\nabla_{\\theta}^2 p_i(\\theta) \\right]\n$$\n\n**2) Specialization to the Logistic Model**\n\nFor this part, we specialize to $p_i(\\theta) = p_{\\theta}(x_i) = \\sigma(\\theta^{\\top} x_i)$, with $\\sigma(u) = (1 + \\exp(-u))^{-1}$. Let $u_i(\\theta) = \\theta^{\\top} x_i$. First, we need the derivatives of the sigmoid function $\\sigma(u)$:\n$$\n\\sigma'(u) = \\frac{d\\sigma}{du} = \\frac{\\exp(-u)}{(1+\\exp(-u))^2} = \\frac{1}{1+\\exp(-u)} \\frac{\\exp(-u)}{1+\\exp(-u)} = \\sigma(u)(1-\\sigma(u))\n$$\nAnd the second derivative:\n$$\n\\sigma''(u) = \\frac{d}{du} \\left( \\sigma(u) - \\sigma(u)^2 \\right) = \\sigma'(u) - 2\\sigma(u)\\sigma'(u) = \\sigma'(u)(1-2\\sigma(u)) = \\sigma(u)(1-\\sigma(u))(1-2\\sigma(u))\n$$\nNow we find the derivatives of $p_i(\\theta)$ with respect to $\\theta$ using the multivariate chain rule. Note that $\\nabla_{\\theta} u_i(\\theta) = x_i$ and $\\nabla_{\\theta}^2 u_i(\\theta) = \\mathbf{0}$ (the zero matrix) since $u_i$ is linear in $\\theta$.\n$$\n\\nabla_{\\theta} p_i(\\theta) = \\sigma'(u_i) \\nabla_{\\theta} u_i(\\theta) = \\sigma(u_i)(1-\\sigma(u_i)) x_i = p_i(\\theta)(1 - p_i(\\theta)) x_i\n$$\nFor the Hessian of $p_i(\\theta)$:\n$$\n\\nabla_{\\theta}^2 p_i(\\theta) = \\nabla_{\\theta} \\left( \\sigma'(u_i) x_i^{\\top} \\right)^{\\top} = \\nabla_{\\theta} \\left( \\sigma'(u_i) x_i \\right) = x_i (\\nabla_{\\theta} \\sigma'(u_i))^{\\top} = x_i \\left( \\sigma''(u_i) (\\nabla_{\\theta} u_i)^{\\top} \\right) = \\sigma''(u_i) x_i x_i^{\\top}\n$$\nSubstituting the expression for $\\sigma''(u_i)$:\n$$\n\\nabla_{\\theta}^2 p_i(\\theta) = p_i(\\theta)(1-p_i(\\theta))(1-2p_i(\\theta)) x_i x_i^{\\top}\n$$\nNow, we substitute these specialized derivatives into the general expressions for the gradient and Hessian of $J(\\theta)$. For clarity, we will let $p_{i,\\theta}$ denote $p_{\\theta}(x_i) = \\sigma(\\theta^{\\top}x_i)$.\n\nThe gradient becomes:\n$$\n\\nabla_{\\theta} J(\\theta) = \\frac{2}{N} \\sum_{i=1}^{N} (p_{i,\\theta} - y_i) \\left( p_{i,\\theta}(1 - p_{i,\\theta}) x_i \\right) = \\frac{2}{N} \\sum_{i=1}^{N} (p_{i,\\theta} - y_i) p_{i,\\theta}(1 - p_{i,\\theta}) x_i\n$$\nThe Hessian becomes:\n\\begin{align*}\n\\nabla_{\\theta}^2 J(\\theta) &= \\frac{2}{N} \\sum_{i=1}^{N} \\left[ \\left( p_{i,\\theta}(1 - p_{i,\\theta}) x_i \\right) \\left( p_{i,\\theta}(1 - p_{i,\\theta}) x_i \\right)^{\\top} + (p_{i,\\theta} - y_i) \\left( p_{i,\\theta}(1 - p_{i,\\theta})(1 - 2p_{i,\\theta}) x_i x_i^{\\top} \\right) \\right] \\\\\n&= \\frac{2}{N} \\sum_{i=1}^{N} \\left[ (p_{i,\\theta}(1 - p_{i,\\theta}))^2 + (p_{i,\\theta} - y_i) p_{i,\\theta}(1 - p_{i,\\theta})(1 - 2p_{i,\\theta}) \\right] x_i x_i^{\\top}\n\\end{align*}\n\n**3) Damped Newton Optimization Scheme**\n\nThe goal is to find $\\theta$ that minimizes $J(\\theta)$. A second-order optimization method, Newton's method, uses the update rule $\\theta_{t+1} = \\theta_t - (\\nabla_{\\theta}^2 J(\\theta_t))^{-1} \\nabla_{\\theta} J(\\theta_t)$. To ensure that the Hessian matrix is positive definite and invertible, and to control the step size, Tikhonov regularization is employed. This involves adding a positive multiple of the identity matrix, $\\gamma I$ with $\\gamma > 0$, to the Hessian. This is also known as a Levenberg-Marquardt type method. The update step $\\Delta\\theta_t = \\theta_{t+1} - \\theta_t$ is found by solving the linear system:\n$$\n(\\nabla_{\\theta}^2 J(\\theta_t) + \\gamma I) \\Delta\\theta_t = - \\nabla_{\\theta} J(\\theta_t)\n$$\nSolving for $\\Delta\\theta_t$ and updating $\\theta_t$ gives the single-iteration update formula:\n$$\n\\theta_{t+1} = \\theta_t - \\left(\\nabla_{\\theta}^2 J(\\theta_t) + \\gamma I\\right)^{-1} \\nabla_{\\theta} J(\\theta_t)\n$$\nThis formula uses the gradient and Hessian derived in the previous parts, evaluated at the current iterate $\\theta_t$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\nabla_{\\theta} J(\\theta) = \\frac{2}{N} \\sum_{i=1}^{N} (p_{i,\\theta} - y_i) p_{i,\\theta}(1-p_{i,\\theta}) x_i & \\nabla_{\\theta}^2 J(\\theta) = \\frac{2}{N} \\sum_{i=1}^{N} \\left[ (p_{i,\\theta}(1 - p_{i,\\theta}))^2 + (p_{i,\\theta} - y_i) p_{i,\\theta}(1 - p_{i,\\theta})(1 - 2p_{i,\\theta}) \\right] x_i x_i^{\\top} & \\theta_{t+1} = \\theta_t - \\left(\\nabla_{\\theta}^2 J(\\theta_t) + \\gamma I\\right)^{-1} \\nabla_{\\theta} J(\\theta_t)\n\\end{pmatrix}\n}\n$$"
        }
    ]
}