{
    "hands_on_practices": [
        {
            "introduction": "The core of data-driven weather prediction lies in optimally combining information from a physical model with real-world observations. This first practice takes you back to the foundational principles of this process. By deriving the posterior distribution for a simple scalar system from scratch, you will build a concrete understanding of how Bayesian inference allows us to weight a background forecast (the prior) and an observation (the likelihood) according to their respective uncertainties to produce an improved estimate, known as the analysis . This is the mathematical cornerstone of classical data assimilation techniques like Optimal Interpolation and the Kalman filter.",
            "id": "4030133",
            "problem": "Consider a one-dimensional data assimilation step within a Numerical Weather Prediction (NWP) system for a single grid-point atmospheric scalar state, denoted by $x$. The background (prior) information about $x$ is modeled as a Gaussian random variable $x_b \\sim \\mathcal{N}(\\mu_b,\\sigma_b^2)$, where $x_b$ represents the background forecast, $\\mu_b$ is its mean, and $\\sigma_b^2$ is its error variance. A single collocated observation $y$ of the same state is available through a linear measurement model $y = x + \\epsilon$, where the observation error $\\epsilon$ is Gaussian with $\\epsilon \\sim \\mathcal{N}(0,\\sigma_o^2)$ and independent of the background error. Assume all distributions and parameters are correctly specified, and that $x$ is the latent true state to be inferred.\n\nStarting only from Bayesâ€™ theorem and the definitions of the Gaussian prior and Gaussian likelihood, derive the posterior distribution $p(x \\mid y)$, explicitly obtaining closed-form expressions for the posterior mean $\\mu_a$ and the posterior variance $\\sigma_a^2$. Express the posterior mean as a convex combination of the background mean and the observation, $\\mu_a = w_b \\mu_b + w_o y$, and provide the analytic expressions for the weights $w_b$ and $w_o$ in terms of $\\sigma_b^2$ and $\\sigma_o^2$. State the final answers for $\\mu_a$, $\\sigma_a^2$, $w_b$, and $w_o$ as explicit functions of $\\mu_b$, $y$, $\\sigma_b^2$, and $\\sigma_o^2$. No numerical evaluation is required, and no rounding is needed. The final answer must be given as a single row vector containing $\\mu_a$, $\\sigma_a^2$, $w_b$, and $w_o$ in that order.",
            "solution": "The problem presents a standard Bayesian inference task in the context of data assimilation, often referred to as optimal interpolation in a scalar case. The problem is scientifically grounded, mathematically well-posed, completely specified, and objective. It is a valid problem.\n\nWe are asked to derive the posterior distribution $p(x | y)$ for the state $x$ given an observation $y$. The derivation begins with Bayes' theorem:\n$$\np(x \\mid y) = \\frac{p(y \\mid x) p(x)}{p(y)}\n$$\nThe term $p(y)$ is a normalization constant that does not depend on $x$. Therefore, we can work with proportionality:\n$$\np(x \\mid y) \\propto p(y \\mid x) p(x)\n$$\n\nFirst, we define the prior distribution, $p(x)$. The problem states that the background information about $x$ is modeled as a Gaussian random variable with mean $\\mu_b$ and variance $\\sigma_b^2$. Thus, the prior distribution for the true state $x$ is:\n$$\np(x) = \\mathcal{N}(x \\mid \\mu_b, \\sigma_b^2) = \\frac{1}{\\sqrt{2\\pi\\sigma_b^2}} \\exp\\left( -\\frac{(x - \\mu_b)^2}{2\\sigma_b^2} \\right)\n$$\n\nNext, we define the likelihood function, $p(y \\mid x)$. The observation model is given by $y = x + \\epsilon$, where the observation error $\\epsilon$ is a zero-mean Gaussian random variable with variance $\\sigma_o^2$, i.e., $\\epsilon \\sim \\mathcal{N}(0, \\sigma_o^2)$. This implies that given a true state $x$, the observation $y$ is a Gaussian random variable with mean $x$ and variance $\\sigma_o^2$. The likelihood function is therefore:\n$$\np(y \\mid x) = \\mathcal{N}(y \\mid x, \\sigma_o^2) = \\frac{1}{\\sqrt{2\\pi\\sigma_o^2}} \\exp\\left( -\\frac{(y - x)^2}{2\\sigma_o^2} \\right)\n$$\n\nNow, we substitute these expressions into the proportionality relation for the posterior:\n$$\np(x \\mid y) \\propto \\left[ \\frac{1}{\\sqrt{2\\pi\\sigma_o^2}} \\exp\\left( -\\frac{(y - x)^2}{2\\sigma_o^2} \\right) \\right] \\left[ \\frac{1}{\\sqrt{2\\pi\\sigma_b^2}} \\exp\\left( -\\frac{(x - \\mu_b)^2}{2\\sigma_b^2} \\right) \\right]\n$$\nIgnoring the constant multiplicative terms, the posterior is proportional to the exponential terms:\n$$\np(x \\mid y) \\propto \\exp\\left( -\\frac{(y - x)^2}{2\\sigma_o^2} - \\frac{(x - \\mu_b)^2}{2\\sigma_b^2} \\right)\n$$\nThe product of two Gaussian distributions (or their PDFs) results in another Gaussian distribution. We know the posterior $p(x \\mid y)$ will be a Gaussian, which we denote as $\\mathcal{N}(x \\mid \\mu_a, \\sigma_a^2)$, having the form:\n$$\np(x \\mid y) \\propto \\exp\\left( -\\frac{(x - \\mu_a)^2}{2\\sigma_a^2} \\right)\n$$\nTo find the posterior mean $\\mu_a$ and variance $\\sigma_a^2$, we must manipulate the exponent of our derived posterior to match the canonical Gaussian form. This is achieved by the method of completing the square with respect to $x$. Let's analyze the exponent, which we denote as $E(x)$:\n$$\nE(x) = -\\frac{(y - x)^2}{2\\sigma_o^2} - \\frac{(x - \\mu_b)^2}{2\\sigma_b^2}\n$$\nWe expand the quadratic terms inside the parentheses:\n$$\nE(x) = -\\frac{1}{2}\\left( \\frac{y^2 - 2yx + x^2}{\\sigma_o^2} + \\frac{x^2 - 2x\\mu_b + \\mu_b^2}{\\sigma_b^2} \\right)\n$$\nWe collect terms based on powers of $x$:\n$$\nE(x) = -\\frac{1}{2}\\left[ x^2 \\left(\\frac{1}{\\sigma_o^2} + \\frac{1}{\\sigma_b^2}\\right) - 2x \\left(\\frac{y}{\\sigma_o^2} + \\frac{\\mu_b}{\\sigma_b^2}\\right) + \\left(\\frac{y^2}{\\sigma_o^2} + \\frac{\\mu_b^2}{\\sigma_b^2}\\right) \\right]\n$$\nThe general form of the exponent for a Gaussian $\\mathcal{N}(x \\mid \\mu_a, \\sigma_a^2)$ is:\n$$\n-\\frac{(x - \\mu_a)^2}{2\\sigma_a^2} = -\\frac{1}{2\\sigma_a^2}(x^2 - 2x\\mu_a + \\mu_a^2) = -\\frac{1}{2}\\left[ x^2\\left(\\frac{1}{\\sigma_a^2}\\right) - 2x\\left(\\frac{\\mu_a}{\\sigma_a^2}\\right) + \\frac{\\mu_a^2}{\\sigma_a^2} \\right]\n$$\nBy comparing the coefficients of the $x^2$ term in $E(x)$ with the general form, we can identify the inverse of the posterior variance, $\\sigma_a^2$:\n$$\n\\frac{1}{\\sigma_a^2} = \\frac{1}{\\sigma_o^2} + \\frac{1}{\\sigma_b^2} = \\frac{\\sigma_b^2 + \\sigma_o^2}{\\sigma_o^2 \\sigma_b^2}\n$$\nSolving for $\\sigma_a^2$ gives the posterior variance:\n$$\n\\sigma_a^2 = \\left( \\frac{\\sigma_b^2 + \\sigma_o^2}{\\sigma_o^2 \\sigma_b^2} \\right)^{-1} = \\frac{\\sigma_o^2 \\sigma_b^2}{\\sigma_b^2 + \\sigma_o^2}\n$$\nNext, by comparing the coefficients of the linear $x$ term, we find:\n$$\n\\frac{\\mu_a}{\\sigma_a^2} = \\frac{y}{\\sigma_o^2} + \\frac{\\mu_b}{\\sigma_b^2}\n$$\nSolving for the posterior mean $\\mu_a$:\n$$\n\\mu_a = \\sigma_a^2 \\left( \\frac{y}{\\sigma_o^2} + \\frac{\\mu_b}{\\sigma_b^2} \\right)\n$$\nSubstituting our expression for $\\sigma_a^2$:\n$$\n\\mu_a = \\left( \\frac{\\sigma_o^2 \\sigma_b^2}{\\sigma_b^2 + \\sigma_o^2} \\right) \\left( \\frac{y\\sigma_b^2 + \\mu_b\\sigma_o^2}{\\sigma_o^2 \\sigma_b^2} \\right)\n$$\nThe term $(\\sigma_o^2 \\sigma_b^2)$ cancels out, yielding:\n$$\n\\mu_a = \\frac{y\\sigma_b^2 + \\mu_b\\sigma_o^2}{\\sigma_b^2 + \\sigma_o^2}\n$$\nThe problem requires expressing the posterior mean as a convex combination $\\mu_a = w_b \\mu_b + w_o y$. We can rearrange our expression for $\\mu_a$ to match this form:\n$$\n\\mu_a = \\left( \\frac{\\sigma_o^2}{\\sigma_b^2 + \\sigma_o^2} \\right) \\mu_b + \\left( \\frac{\\sigma_b^2}{\\sigma_b^2 + \\sigma_o^2} \\right) y\n$$\nBy direct comparison with $\\mu_a = w_b \\mu_b + w_o y$, we identify the weights $w_b$ and $w_o$:\n$$\nw_b = \\frac{\\sigma_o^2}{\\sigma_b^2 + \\sigma_o^2}\n$$\n$$\nw_o = \\frac{\\sigma_b^2}{\\sigma_b^2 + \\sigma_o^2}\n$$\nThe sum of the weights is $w_b + w_o = \\frac{\\sigma_o^2 + \\sigma_b^2}{\\sigma_b^2 + \\sigma_o^2} = 1$. Since variances are non-negative, the weights are also non-negative, confirming this is a proper convex combination.\n\nThe final required expressions are:\n1.  Posterior mean $\\mu_a = \\frac{\\mu_b \\sigma_o^2 + y \\sigma_b^2}{\\sigma_b^2 + \\sigma_o^2}$\n2.  Posterior variance $\\sigma_a^2 = \\frac{\\sigma_b^2 \\sigma_o^2}{\\sigma_b^2 + \\sigma_o^2}$\n3.  Weight on background mean $w_b = \\frac{\\sigma_o^2}{\\sigma_b^2 + \\sigma_o^2}$\n4.  Weight on observation $w_o = \\frac{\\sigma_b^2}{\\sigma_b^2 + \\sigma_o^2}$\n\nThese are the required analytical forms.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{\\mu_b \\sigma_o^2 + y \\sigma_b^2}{\\sigma_b^2 + \\sigma_o^2}  \\frac{\\sigma_b^2 \\sigma_o^2}{\\sigma_b^2 + \\sigma_o^2}  \\frac{\\sigma_o^2}{\\sigma_b^2 + \\sigma_o^2}  \\frac{\\sigma_b^2}{\\sigma_b^2 + \\sigma_o^2}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Building on the single-step Bayesian update, this practice moves from static theory to dynamic simulation. You will implement a Kalman Filter, which recursively applies the principles of the previous exercise to track a system's state over time . This hands-on coding problem is designed to reveal the practical consequences of our modeling assumptions, particularly the critical role of the model error covariance, $Q$. By observing how the filter behaves when its internal assumptions about model error do not match reality, you will gain insight into a central challenge in operational data assimilation: diagnosing and tuning the filter for robust performance.",
            "id": "4030151",
            "problem": "Consider a one-dimensional linear Gaussian state-space model used as a simplified building block for Data Assimilation (DA) in Numerical Weather Prediction (NWP). The physical quantities are abstracted to dimensionless scalars. The true state evolves according to a stable or near-stable autoregressive dynamics, and observations are direct noisy measurements of the state. You will implement an analysis cycle using the Kalman Filter (KF) with a potentially mis-specified model error covariance. Your task is to compute the analysis increments at each cycle and to quantify drift in the forecast error over time when the model error covariance is mis-specified.\n\nFundamental base to use:\n- Linear state-space model, Gaussian additive errors, and Bayesian filtering in the linear-Gaussian case.\n- The state evolves as $x_{k+1} = a\\,x_k + w_k$, where $w_k$ is zero-mean Gaussian model error with variance $Q_{\\text{true}}$.\n- The observation at cycle $k$ is $y_k = x_k + v_k$, where $v_k$ is zero-mean Gaussian observation error with variance $R$.\n- The Kalman Filter (KF) analysis cycle consists of prediction (forecast) and update (analysis) steps derived from the linear-Gaussian Bayesian framework and the Gauss-Markov theorem, without relying on shortcut formulas.\n\nDefinitions and quantities to compute:\n- Forecast mean $x_f(k)$ and forecast variance $P_f(k)$ obtained by propagating the previous analysis through the model with $(a)$ and the assumed model error variance $Q_{\\text{assumed}}$.\n- Analysis mean $x_a(k)$ and analysis variance $P_a(k)$ obtained by combining the forecast with the current observation $y_k$ under the linear-Gaussian update derived from first principles.\n- Analysis increment at cycle $k$: $\\Delta(k) = x_a(k) - x_f(k)$.\n- Forecast error at cycle $k$: $e_f(k) = x_f(k) - x_{\\text{true}}(k)$.\n- Drift metric: the least-squares linear trend (slope) $s$ of the sequence $\\{e_f(k)\\}_{k=1}^N$ versus the cycle index $k$, where $N$ is the total number of cycles. That is, fit $e_f(k) \\approx s\\,k + b$ in the least-squares sense and report $s$.\n\nAll quantities are dimensionless. Angles are not used. Percentages are not used. You must express the drift metric $s$ and the mean absolute analysis increment $\\overline{|\\Delta|}$ as decimal floats.\n\nInitialization:\n- The initial true state is $x_{\\text{true}}(0) = x_0$.\n- The initial analysis mean is $x_a(0) = x_0$.\n- The initial analysis variance is $P_a(0) = P_0$.\n\nSimulation and analysis cycle per time step $k = 1,\\dots,N$:\n- Advance the true state using $x_{\\text{true}}(k) = a\\,x_{\\text{true}}(k-1) + w_k$, where $w_k$ is drawn from a Gaussian distribution with variance $Q_{\\text{true}}$ and zero mean.\n- Generate the observation $y_k = x_{\\text{true}}(k) + v_k$, where $v_k$ is drawn from a Gaussian distribution with variance $R$ and zero mean.\n- Compute the forecast mean $x_f(k)$ and forecast variance $P_f(k)$ using the analysis from step $k-1$ and the assumed model error variance $Q_{\\text{assumed}}$.\n- Compute the analysis mean $x_a(k)$ and analysis variance $P_a(k)$ using the linear-Gaussian Bayesian update with the observation $y_k$. Do not rely on shortcut formulas; derive the update from first principles (innovation and least-squares weighting implied by the Gauss-Markov theorem).\n- Compute the analysis increment $\\Delta(k) = x_a(k) - x_f(k)$ and the forecast error $e_f(k) = x_f(k) - x_{\\text{true}}(k)$.\n\nOutputs to report for each test case:\n- The mean absolute analysis increment $\\overline{|\\Delta|} = \\frac{1}{N}\\sum_{k=1}^{N} |\\Delta(k)|$.\n- The drift metric $s$ (slope of the least-squares fit of $e_f(k)$ versus $k$), expressed in units of state per cycle.\n\nProvide the following test suite with fixed random seeds for reproducibility:\n- Test case $1$ (happy path, correctly specified $Q$): $a = 0.95$, $Q_{\\text{true}} = 0.04$, $Q_{\\text{assumed}} = 0.04$, $R = 0.09$, $x_0 = 1.0$, $P_0 = 0.1$, $N = 60$, $\\text{seed} = 123$.\n- Test case $2$ (under-dispersed forecast model): $a = 0.95$, $Q_{\\text{true}} = 0.04$, $Q_{\\text{assumed}} = 0.0$, $R = 0.09$, $x_0 = 1.0$, $P_0 = 0.1$, $N = 60$, $\\text{seed} = 123$.\n- Test case $3$ (over-dispersed forecast model): $a = 0.95$, $Q_{\\text{true}} = 0.04$, $Q_{\\text{assumed}} = 0.2$, $R = 0.09$, $x_0 = 1.0$, $P_0 = 0.1$, $N = 60$, $\\text{seed} = 123$.\n- Test case $4$ (near-unstable dynamics): $a = 1.05$, $Q_{\\text{true}} = 0.04$, $Q_{\\text{assumed}} = 0.04$, $R = 0.09$, $x_0 = 1.0$, $P_0 = 0.1$, $N = 60$, $\\text{seed} = 456$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets, where each test case result is a two-element list $[\\overline{|\\Delta|}, s]$.\n- Each float must be rounded to $6$ decimals.\n- For example, the output should look like $[[0.123456,-0.000789],[\\dots,\\dots],[\\dots,\\dots],[\\dots,\\dots]]$.",
            "solution": "The problem requires the implementation of a one-dimensional Kalman Filter (KF) to perform a data assimilation analysis cycle. We must simulate a true state, generate noisy observations, and then apply the KF to estimate the state. A key aspect is to analyze the filter's performance when the model error covariance used by the filter, $Q_{\\text{assumed}}$, differs from the true value, $Q_{\\text{true}}$. The required outputs are the mean absolute analysis increment and a drift metric for the forecast error.\n\nThe solution is developed by first principles, as specified. The Kalman filter is the optimal linear estimator for a linear-Gaussian state-space system, and its equations can be derived directly from Bayesian principles.\n\n**1. State-Space Model**\n\nThe system is defined by a linear state-space model:\n- **State Equation (Process Model):** The true state $x_{\\text{true}}(k)$ at cycle $k$ evolves according to a first-order autoregressive process with additive Gaussian noise:\n$$ x_{\\text{true}}(k) = a \\, x_{\\text{true}}(k-1) + w_{k-1} $$\nwhere $a$ is the dynamics coefficient and $w_{k-1} \\sim \\mathcal{N}(0, Q_{\\text{true}})$ is the process noise with true variance $Q_{\\text{true}}$.\n\n- **Observation Equation (Measurement Model):** The observation $y_k$ at cycle $k$ is a direct, noisy measurement of the true state:\n$$ y_k = x_{\\text{true}}(k) + v_k $$\nwhere $v_k \\sim \\mathcal{N}(0, R)$ is the measurement noise with variance $R$.\n\n**2. Kalman Filter Algorithm**\n\nThe Kalman filter recursively computes an estimate of the state by executing a two-step cycle: prediction (forecast) and update (analysis).\n\n**2.1. Prediction (Forecast) Step**\n\nThe prediction step projects the state estimate and its uncertainty forward in time. Starting from the analysis (posterior) estimate of the state at cycle $k-1$, which we denote by its mean $x_a(k-1)$ and variance $P_a(k-1)$, we compute the forecast (prior) estimate for cycle $k$.\n- **Forecast Mean:** The expected value of the state at cycle $k$ is found by applying the deterministic part of the process model:\n$$ x_f(k) = E[a \\, x_a(k-1) + w_{k-1}] = a \\, E[x_a(k-1)] = a \\, x_a(k-1) $$\n- **Forecast Variance:** The variance of the forecast is the sum of the propagated analysis variance and the assumed model error variance. The filter uses $Q_{\\text{assumed}}$, which may not be equal to $Q_{\\text{true}}$.\n$$ P_f(k) = \\text{Var}[a \\, x_a(k-1) + w_{k-1}] = a^2 \\, \\text{Var}[x_a(k-1)] + \\text{Var}[w_{k-1}] = a^2 P_a(k-1) + Q_{\\text{assumed}} $$\n\n**2.2. Update (Analysis) Step from First Principles**\n\nThe update step refines the forecast estimate by incorporating the new observation $y_k$. This is a Bayesian update where the forecast serves as the prior and the observation provides the likelihood.\n\n- **Prior:** The forecast provides a prior probability distribution for the state $x_k$, which is Gaussian: $p(x_k) = \\mathcal{N}(x_k | x_f(k), P_f(k))$. Its probability density function (PDF) is proportional to:\n$$ p(x_k) \\propto \\exp\\left(-\\frac{1}{2} \\frac{(x_k - x_f(k))^2}{P_f(k)}\\right) $$\n\n- **Likelihood:** The observation model $y_k = x_k + v_k$ gives the likelihood of observing $y_k$ given a state $x_k$: $p(y_k|x_k) = \\mathcal{N}(y_k | x_k, R)$. The PDF is proportional to:\n$$ p(y_k|x_k) \\propto \\exp\\left(-\\frac{1}{2} \\frac{(y_k - x_k)^2}{R}\\right) $$\n\n- **Posterior:** According to Bayes' theorem, the posterior distribution of the state, $p(x_k|y_k)$, is proportional to the product of the prior and the likelihood. This posterior is the analysis distribution, $\\mathcal{N}(x_k | x_a(k), P_a(k))$.\n$$ p(x_k|y_k) \\propto p(y_k|x_k) p(x_k) \\propto \\exp\\left(-\\frac{1}{2} \\left[ \\frac{(x_k - x_f(k))^2}{P_f(k)} + \\frac{(x_k - y_k)^2}{R} \\right]\\right) $$\nThe product of two Gaussian PDFs is another Gaussian PDF (up to a normalization constant). To find its mean $x_a(k)$ and variance $P_a(k)$, we complete the square for the term in the exponent with respect to $x_k$. The term quadratic in $x_k$ is $x_k^2 \\left(\\frac{1}{P_f(k)} + \\frac{1}{R}\\right)$, which implies that the inverse posterior variance (precision) is the sum of the prior precisions:\n$$ \\frac{1}{P_a(k)} = \\frac{1}{P_f(k)} + \\frac{1}{R} \\implies P_a(k) = \\frac{P_f(k) R}{P_f(k) + R} $$\nThe term linear in $x_k$ is $-2x_k \\left(\\frac{x_f(k)}{P_f(k)} + \\frac{y_k}{R}\\right)$. By comparing this with the canonical form of a Gaussian exponent, we find the posterior mean $x_a(k)$:\n$$ \\frac{x_a(k)}{P_a(k)} = \\frac{x_f(k)}{P_f(k)} + \\frac{y_k}{R} \\implies x_a(k) = P_a(k) \\left(\\frac{x_f(k)}{P_f(k)} + \\frac{y_k}{R}\\right) $$\nSubstituting the expression for $P_a(k)$ and simplifying gives the weighted average of the forecast and the observation:\n$$ x_a(k) = \\frac{R x_f(k) + P_f(k) y_k}{P_f(k) + R} $$\nThis equation can be rearranged into the more common \"innovations\" form:\n$$ x_a(k) = x_f(k) + \\frac{P_f(k)}{P_f(k) + R}(y_k - x_f(k)) $$\nHere, $K_k = \\frac{P_f(k)}{P_f(k) + R}$ is the Kalman gain, and $(y_k - x_f(k))$ is the innovation or measurement residual. This derivation from first principles legitimizes the use of these standard KF update equations.\n\n**3. Simulation and Diagnostics**\n\nThe simulation proceeds for $k = 1, \\dots, N$ cycles, starting with initial conditions $x_{\\text{true}}(0) = x_0$, $x_a(0) = x_0$, and $P_a(0) = P_0$. In each cycle, we:\n1.  Generate the true state $x_{\\text{true}}(k)$ and observation $y_k$ using the given random seed.\n2.  Execute the KF Prediction step to get $x_f(k)$ and $P_f(k)$.\n3.  Execute the KF Update step to get $x_a(k)$ and $P_a(k)$.\n4.  Compute the required diagnostic quantities:\n    - **Analysis Increment:** $\\Delta(k) = x_a(k) - x_f(k)$. This represents the correction applied to the forecast based on the new observation. The mean absolute increment is calculated as $\\overline{|\\Delta|} = \\frac{1}{N}\\sum_{k=1}^{N} |\\Delta(k)|$.\n    - **Forecast Error:** $e_f(k) = x_f(k) - x_{\\text{true}}(k)$. This measures how far the forecast is from the (unknowable in reality) true state.\n    - **Drift Metric ($s$):** This metric quantifies any systematic drift in the forecast error over time. It is computed as the slope of a linear least-squares regression line fitted to the time series of forecast errors $\\{e_f(k)\\}_{k=1}^N$ against the cycle index $k$. A non-zero slope indicates filter divergence or sub-optimal, biased performance. The model for the fit is $e_f(k) \\approx s \\cdot k + b$.\n\n**4. Implementation**\n\nThe final implementation will follow this logic for each test case provided. A random number generator is seeded for reproducibility. The outputs $\\overline{|\\Delta|}$ and $s$ are calculated and formatted as specified. Mis-specification of $Q$ ($Q_{\\text{assumed}} \\neq Q_{\\text{true}}$) tests the robustness of the filter. If $Q_{\\text{assumed}}$ is too small (under-dispersed), the filter becomes overconfident and may ignore observations, leading to error drift ($s \\neq 0$). If $Q_{\\text{assumed}}$ is too large (over-dispersed), the filter is overly conservative and relies too heavily on noisy observations, resulting in sub-optimal but generally stable performance.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_kalman_filter_simulation(a, Q_true, Q_assumed, R, x0, P0, N, seed):\n    \"\"\"\n    Performs a data assimilation simulation using a 1D Kalman Filter.\n\n    This function simulates a true state, generates observations, runs the Kalman\n    Filter, and computes diagnostic metrics.\n\n    Args:\n        a (float): State transition scalar.\n        Q_true (float): True variance of the model error.\n        Q_assumed (float): Assumed variance of the model error for the filter.\n        R (float): Variance of the observation error.\n        x0 (float): Initial true state and analysis mean.\n        P0 (float): Initial analysis variance.\n        N (int): Total number of simulation cycles.\n        seed (int): Seed for the random number generator for reproducibility.\n\n    Returns:\n        list: A two-element list containing the mean absolute analysis\n              increment and the forecast error drift metric (slope), both\n              rounded to 6 decimal places.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n\n    # Initialize state variables and histories\n    x_true = x0\n    x_a = x0\n    P_a = P0\n\n    forecast_errors = []\n    analysis_increments = []\n\n    # Simulation loop for cycles k = 1, ..., N\n    for _ in range(1, N + 1):\n        # 1. Advance the true state\n        w_k = rng.normal(loc=0.0, scale=np.sqrt(Q_true))\n        x_true = a * x_true + w_k\n\n        # 2. Generate the observation from the new true state\n        v_k = rng.normal(loc=0.0, scale=np.sqrt(R))\n        y_k = x_true + v_k\n\n        # 3. Kalman Filter: Prediction (Forecast) Step\n        # Propagate state estimate and its variance using previous analysis\n        x_f = a * x_a\n        P_f = a * P_a * a + Q_assumed\n\n        # 4. Kalman Filter: Update (Analysis) Step\n        # Derivation: The analysis is the Bayesian combination of the forecast prior\n        # and observation likelihood. The resulting mean is a weighted average.\n        # This leads to the standard Kalman update equations.\n        innovation = y_k - x_f\n        # Innovation covariance S_k = H P_f H' + R, where H=1 in our 1D case.\n        S_k = P_f + R\n        # Kalman Gain K_k = P_f H' inv(S_k), where H=1, H'=1.\n        K_k = P_f / S_k\n        \n        # Update analysis (posterior) mean and variance\n        x_a = x_f + K_k * innovation\n        P_a = (1.0 - K_k) * P_f\n\n        # 5. Compute and store diagnostic quantities\n        delta_k = x_a - x_f\n        analysis_increments.append(delta_k)\n\n        e_f_k = x_f - x_true\n        forecast_errors.append(e_f_k)\n\n    # Post-processing to calculate final metrics\n    # a. Mean absolute analysis increment\n    mean_abs_delta = np.mean(np.abs(np.array(analysis_increments)))\n\n    # b. Drift metric 's' (slope of least-squares fit of forecast error vs. time)\n    k_values = np.arange(1, N + 1)\n    # np.polyfit returns coefficients [slope, intercept] for a degree 1 polynomial\n    poly_coeffs = np.polyfit(k_values, forecast_errors, 1)\n    s = poly_coeffs[0]\n\n    return [round(mean_abs_delta, 6), round(s, 6)]\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (a, Q_true, Q_assumed, R, x0, P0, N, seed)\n        (0.95, 0.04, 0.04, 0.09, 1.0, 0.1, 60, 123),  # Test case 1\n        (0.95, 0.04, 0.0, 0.09, 1.0, 0.1, 60, 123),   # Test case 2\n        (0.95, 0.04, 0.2, 0.09, 1.0, 0.1, 60, 123),  # Test case 3\n        (1.05, 0.04, 0.04, 0.09, 1.0, 0.1, 60, 456),  # Test case 4\n    ]\n\n    results = []\n    for case in test_cases:\n        a, Q_true, Q_assumed, R, x0, P0, N, seed = case\n        result = run_kalman_filter_simulation(a, Q_true, Q_assumed, R, x0, P0, N, seed)\n        results.append(result)\n\n    # Format the final output string exactly as specified.\n    # The default str() for a list includes spaces, so we build the string manually.\n    result_strings = [f\"[{res[0]},{res[1]}]\" for res in results]\n    final_output = f\"[{','.join(result_strings)}]\"\n    \n    # Final print statement in the exact required format.\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "While classical methods like the Kalman Filter provide a strong theoretical foundation, modern data-driven weather prediction increasingly leverages complex, non-linear models such as Convolutional Neural Networks (CNNs). Though these models can learn intricate patterns directly from data, their \"black box\" nature poses a challenge to scientific trust and verification. This practice introduces a key technique from Explainable AI (XAI): the use of saliency maps to interpret a model's behavior . By computing the gradient of the model's prediction with respect to its inputs, you will visualize which features the model \"pays attention to,\" allowing you to assess whether it has learned physically meaningful relationships or is relying on spurious correlations.",
            "id": "4030093",
            "problem": "You are given a simplified, scientifically plausible Convolutional Neural Network (CNN) architecture for scalar rainfall prediction from gridded atmospheric input fields in the context of numerical weather prediction and climate modeling. The CNN uses cross-correlation with \"same\" padding, Rectified Linear Unit (ReLU) activation, and global average pooling to produce a single scalar prediction. Your task is to compute saliency maps for this CNN by evaluating the gradient of the predicted rainfall with respect to the input fields, and to summarize the physically meaningful concentration of saliency within regions of high moisture and positive convergence.\n\nFundamental base and definitions:\n- Convolutional Neural Network (CNN) is defined as follows. Let the input be $x \\in \\mathbb{R}^{C \\times H \\times W}$, with $C$ channels and spatial dimensions $H \\times W$. Here, $x^{(1)}$ is column-integrated water vapor in $\\mathrm{kg}\\,\\mathrm{m}^{-2}$, $x^{(2)}$ is near-surface temperature anomaly in $\\mathrm{K}$, and $x^{(3)}$ is horizontal convergence in $\\mathrm{s}^{-1}$. You may assume $C = 3$, $H = W = 16$.\n- The CNN has $F$ filters (here $F = 2$). The pre-activation map for filter $f$ at location $(i,j)$ is\n$$\nz^{(f)}_{i,j} = \\sum_{c=1}^{C} \\sum_{u=1}^{k} \\sum_{v=1}^{k} K^{(f)}_{c,u,v}\\,x^{(c)}_{i+u',j+v'} + b^{(f)},\n$$\nwhere $K^{(f)} \\in \\mathbb{R}^{C \\times k \\times k}$ is the filter kernel, $b^{(f)} \\in \\mathbb{R}$ is a bias, $k=3$, indices $(u',v')$ implement \"same\" padding as in cross-correlation (no kernel rotation in the forward pass), and padding ensures the sum is well-defined for all $(i,j)$.\n- The activation is $a^{(f)}_{i,j} = \\max(0, z^{(f)}_{i,j})$, and the global average pooled feature is\n$$\ng^{(f)} = \\frac{1}{H W} \\sum_{i=1}^{H} \\sum_{j=1}^{W} a^{(f)}_{i,j}.\n$$\n- The scalar rainfall prediction is\n$$\n\\hat{y} = \\sum_{f=1}^{F} w_f\\, g^{(f)} + b_{\\mathrm{out}},\n$$\nexpressed in $\\mathrm{mm}\\,\\mathrm{h}^{-1}$.\n\nSaliency and physical interpretation:\n- Define the saliency map for channel $c$ as\n$$\nS^{(c)}_{i,j} = \\frac{\\partial \\hat{y}}{\\partial x^{(c)}_{i,j}}.\n$$\n- Define the physically meaningful mask\n$$\nM_{i,j} = \\begin{cases}\n1  \\text{if } x^{(1)}_{i,j}  q_{\\mathrm{th}} \\text{ and } x^{(3)}_{i,j}  0, \\\\\n0  \\text{otherwise},\n\\end{cases}\n$$\nwith threshold $q_{\\mathrm{th}} = 25$ (in $\\mathrm{kg}\\,\\mathrm{m}^{-2}$).\n- Define the saliency concentration fraction\n$$\n\\phi = \\frac{\\sum_{i=1}^{H} \\sum_{j=1}^{W} \\sum_{c=1}^{C} \\left| S^{(c)}_{i,j} \\right|\\, M_{i,j}}{\\sum_{i=1}^{H} \\sum_{j=1}^{W} \\sum_{c=1}^{C} \\left| S^{(c)}_{i,j} \\right|},\n$$\nwhich measures the fraction of total saliency magnitude that lies in high-moisture, convergent regions ($\\phi \\in [0,1]$, dimensionless).\n\nNetwork specification (fixed and known):\n- Filter $f=1$ (\"rain-support\"):\n  - $K^{(1)}_{q} = 0.02 \\times \\mathbf{1}_{3 \\times 3}$,\n  - $K^{(1)}_{T'} = 0.01 \\times \\mathbf{1}_{3 \\times 3}$,\n  - $K^{(1)}_{c} = 500 \\times \\mathbf{1}_{3 \\times 3}$,\n  - $b^{(1)} = 0$,\n  - $w_1 = 20$.\n- Filter $f=2$ (\"front detector\"):\n  - $K^{(2)}_{q} = \\mathbf{0}_{3 \\times 3}$,\n  - $K^{(2)}_{T'} = 0.5 \\times \\begin{bmatrix} -1  0  1 \\\\ -1  0  1 \\\\ -1  0  1 \\end{bmatrix}$,\n  - $K^{(2)}_{c} = \\mathbf{0}_{3 \\times 3}$,\n  - $b^{(2)} = 0$,\n  - $w_2 = 5$.\n- Output bias: $b_{\\mathrm{out}} = 0$.\n\nTest suite:\nWork on a spatial grid of size $H=W=16$ with indices $i,j \\in \\{0,1,\\dots,15\\}$ and construct the following $4$ test cases for $(x^{(1)}, x^{(2)}, x^{(3)})$:\n- Case $1$ (happy path, central moist-convergent plume):\n  - $x^{(1)}_{i,j} = 10 + 30 \\, G(i,j; c_x=8, c_y=8, \\sigma=2)$,\n  - $x^{(2)}_{i,j} = 2 \\, G(i,j; c_x=8, c_y=8, \\sigma=2)$,\n  - $x^{(3)}_{i,j} = 10^{-3} \\, G(i,j; c_x=8, c_y=8, \\sigma=2)$.\n- Case $2$ (boundary condition, corner plume):\n  - $x^{(1)}_{i,j} = 10 + 30 \\, G(i,j; c_x=3, c_y=3, \\sigma=2)$,\n  - $x^{(2)}_{i,j} = 2 \\, G(i,j; c_x=3, c_y=3, \\sigma=2)$,\n  - $x^{(3)}_{i,j} = 10^{-3} \\, G(i,j; c_x=3, c_y=3, \\sigma=2)$.\n- Case $3$ (edge case, uniform fields):\n  - $x^{(1)}_{i,j} = 20$,\n  - $x^{(2)}_{i,j} = 0$,\n  - $x^{(3)}_{i,j} = 0$.\n- Case $4$ (counter-physical convergence, ring moisture and cold anomaly):\n  - $x^{(1)}_{i,j} = 10 + 35 \\left[ G(i,j; c_x=8, c_y=8, \\sigma=3) - 0.7\\, G(i,j; c_x=8, c_y=8, \\sigma=1.5) \\right]$ clipped below at $0$,\n  - $x^{(2)}_{i,j} = -2 \\, G(i,j; c_x=8, c_y=8, \\sigma=2.5)$,\n  - $x^{(3)}_{i,j} = -10^{-3} \\, G(i,j; c_x=8, c_y=8, \\sigma=2)$.\n- Here the Gaussian is defined by\n$$\nG(i,j; c_x, c_y, \\sigma) = \\exp\\left( -\\frac{(i-c_x)^2 + (j-c_y)^2}{2\\sigma^2} \\right).\n$$\n\nTasks:\n1. Implement the specified CNN forward map to obtain $\\hat{y}$ for each case.\n2. Compute the saliency map $S^{(c)}_{i,j} = \\partial \\hat{y} / \\partial x^{(c)}_{i,j}$ exactly by applying the chain rule and the definition of cross-correlation. Use the fact that the derivative of ReLU is $1$ for positive pre-activation and $0$ otherwise, and that the derivative of global average pooling is a constant scaling by $1/(H W)$. For the backward mapping through cross-correlation, use a correlation of the upstream gradient with the kernel rotated by $180^\\circ$ per channel.\n3. Compute the mask $M_{i,j}$ using $q_{\\mathrm{th}} = 25$ (in $\\mathrm{kg}\\,\\mathrm{m}^{-2}$), and the concentration fraction $\\phi$ as defined.\n4. For each case, produce two quantities:\n   - The predicted rainfall $\\hat{y}$, expressed in $\\mathrm{mm}\\,\\mathrm{h}^{-1}$ and rounded to three decimal places.\n   - The dimensionless saliency concentration fraction $\\phi$, rounded to three decimal places.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order $[\\hat{y}_1, \\phi_1, \\hat{y}_2, \\phi_2, \\hat{y}_3, \\phi_3, \\hat{y}_4, \\phi_4]$, where subscript indices correspond to the numbered test cases above. Each number must be formatted with exactly three decimal places.\n\nEnsure scientific realism: the CNN and fields must be implemented as specified, and all computations must be deterministic and self-contained without external data. Angles are not used in this problem. Units used are $\\mathrm{kg}\\,\\mathrm{m}^{-2}$, $\\mathrm{K}$, $\\mathrm{s}^{-1}$ for inputs and $\\mathrm{mm}\\,\\mathrm{h}^{-1}$ for outputs.",
            "solution": "The user-provided problem is a well-defined computational exercise in the domain of scientific machine learning, specifically for interpreting a Convolutional Neural Network (CNN) used in a simplified weather prediction context. The problem is scientifically grounded, mathematically consistent, and all necessary parameters and definitions are provided for a unique solution to be determined. We will therefore proceed with a full solution.\n\nThe solution involves three main computational steps:\n1.  A **forward pass** through the CNN to compute the scalar rainfall prediction $\\hat{y}$ for each input atmospheric state.\n2.  A **backward pass** using the chain rule to compute the gradient of the prediction $\\hat{y}$ with respect to each input variable, yielding the saliency maps $S^{(c)}$.\n3.  An **analysis step** to compute the saliency concentration fraction $\\phi$ based on a physically motivated mask $M$.\n\nWe will now detail the mathematical formulation for each step.\n\n### 1. Forward Propagation\n\nThe forward pass computes the predicted rainfall $\\hat{y}$ from the input tensor $x \\in \\mathbb{R}^{C \\times H \\times W}$, where $C=3$, $H=16$, $W=16$. The input channels are $x^{(1)}$ (water vapor), $x^{(2)}$ (temperature anomaly), and $x^{(3)}$ (convergence).\n\nFirst, for each of the $F=2$ filters, we compute a pre-activation map $z^{(f)}$. This is achieved by performing a multi-channel 2D cross-correlation of the input $x$ with the filter kernel $K^{(f)} \\in \\mathbb{R}^{C \\times k \\times k}$ ($k=3$), followed by the addition of a bias term $b^{(f)}$. The operation uses \"same\" padding, meaning the input is padded with zeros such that the output map has the same spatial dimensions $H \\times W$ as the input maps.\n\nThe pre-activation for filter $f$ at grid point $(i,j)$ is:\n$$\nz^{(f)}_{i,j} = b^{(f)} + \\sum_{c=1}^{C} (K^{(f)}_c \\star x^{(c)})_{i,j}\n$$\nwhere $\\star$ denotes 2D cross-correlation and $K^{(f)}_c$ is the kernel slice for channel $c$.\n\nSecond, a Rectified Linear Unit (ReLU) activation function is applied element-wise to the pre-activation maps. This introduces non-linearity into the model.\n$$\na^{(f)}_{i,j} = \\max(0, z^{(f)}_{i,j})\n$$\n\nThird, each activation map $a^{(f)}$ is reduced to a single scalar feature $g^{(f)}$ via global average pooling.\n$$\ng^{(f)} = \\frac{1}{H W} \\sum_{i=1}^{H} \\sum_{j=1}^{W} a^{(f)}_{i,j}\n$$\n\nFinally, the scalar rainfall prediction $\\hat{y}$ (in $\\mathrm{mm}\\,\\mathrm{h}^{-1}$) is computed as a weighted sum of the features $g^{(f)}$, plus an output bias $b_{\\mathrm{out}}$.\n$$\n\\hat{y} = b_{\\mathrm{out}} + \\sum_{f=1}^{F} w_f g^{(f)}\n$$\nAll network parameters ($K^{(f)}$, $b^{(f)}$, $w_f$, $b_{\\mathrm{out}}$) are provided in the problem statement.\n\n### 2. Backward Propagation for Saliency Maps\n\nThe saliency map for an input channel $c$, $S^{(c)}$, is defined as the partial derivative of the output prediction $\\hat{y}$ with respect to the input map $x^{(c)}$. We compute this using the chain rule, propagating gradients backward from the output to the input.\n\n1.  **Gradient at the output layer**: The derivative of $\\hat{y}$ with respect to the pooled features $g^{(f)}$ is simply the output weight $w_f$.\n    $$\n    \\frac{\\partial \\hat{y}}{\\partial g^{(f)}} = w_f\n    $$\n\n2.  **Gradient at the pooling layer**: The derivative of $g^{(f)}$ with respect to a single element of the activation map $a^{(f)}_{i,j}$ is constant due to the averaging operation.\n    $$\n    \\frac{\\partial g^{(f)}}{\\partial a^{(f)}_{i,j}} = \\frac{1}{H W}\n    $$\n    By the chain rule, the gradient of $\\hat{y}$ with respect to $a^{(f)}_{i,j}$ is a constant map:\n    $$\n    \\frac{\\partial \\hat{y}}{\\partial a^{(f)}_{i,j}} = \\frac{\\partial \\hat{y}}{\\partial g^{(f)}} \\frac{\\partial g^{(f)}}{\\partial a^{(f)}_{i,j}} = \\frac{w_f}{H W}\n    $$\n\n3.  **Gradient at the activation layer**: The derivative of the ReLU activation $a^{(f)}_{i,j}$ with respect to its input $z^{(f)}_{i,j}$ is $1$ if $z^{(f)}_{i,j}  0$ and $0$ otherwise. We denote this using the Heaviside step function $H(\\cdot)$.\n    $$\n    \\frac{\\partial a^{(f)}_{i,j}}{\\partial z^{(f)}_{i,j}} = H(z^{(f)}_{i,j}) = \\begin{cases} 1  \\text{if } z^{(f)}_{i,j}  0 \\\\ 0  \\text{if } z^{(f)}_{i,j} \\le 0 \\end{cases}\n    $$\n    The upstream gradient for the pre-activation map is therefore:\n    $$\n    \\frac{\\partial \\hat{y}}{\\partial z^{(f)}_{i,j}} = \\frac{\\partial \\hat{y}}{\\partial a^{(f)}_{i,j}} \\frac{\\partial a^{(f)}_{i,j}}{\\partial z^{(f)}_{i,j}} = \\frac{w_f}{HW} H(z^{(f)}_{i,j})\n    $$\n    Let this upstream gradient map be denoted $\\delta z^{(f)}$.\n\n4.  **Gradient at the cross-correlation layer**: The final step is to find the gradient with respect to the input $x^{(c)}$. The pre-activation $z^{(f)}$ is a sum of cross-correlations over the input channels. The derivative of a cross-correlation operation with respect to its input is a convolution operation with the original kernel.\n    $$\n    S^{(c)} = \\frac{\\partial \\hat{y}}{\\partial x^{(c)}} = \\sum_{f=1}^{F} \\frac{\\partial z^{(f)}}{\\partial x^{(c)}} \\frac{\\partial \\hat{y}}{\\partial z^{(f)}} = \\sum_{f=1}^{F} (\\delta z^{(f)} \\circledast K^{(f)}_c)\n    $$\n    where $\\circledast$ denotes 2D convolution and $K^{(f)}_c$ is the kernel for filter $f$ and channel $c$. This operation is equivalent to cross-correlating the upstream gradient map $\\delta z^{(f)}$ with the 180-degree-rotated kernel $K^{(f)}_{c, \\text{rot180}}$. The saliency map for each channel $c$ is the sum of contributions from all filters $f$.\n\n### 3. Saliency Concentration Fraction\n\nThe fraction $\\phi$ quantifies how much of the model's sensitivity (saliency) is focused on regions that are physically conducive to rainfall.\n\nFirst, a binary mask $M \\in \\{0,1\\}^{H \\times W}$ is created. It identifies grid points with both high atmospheric moisture and positive convergence, which are prerequisites for many types of precipitation.\n$$\nM_{i,j} = \\begin{cases} 1  \\text{if } x^{(1)}_{i,j}  q_{\\mathrm{th}} \\text{ and } x^{(3)}_{i,j}  0 \\\\ 0  \\text{otherwise} \\end{cases}\n$$\nwhere the moisture threshold is $q_{\\mathrm{th}} = 25 \\, \\mathrm{kg}\\,\\mathrm{m}^{-2}$.\n\nThe saliency concentration fraction $\\phi$ is then calculated as the ratio of the total absolute saliency within the masked region to the total absolute saliency over the entire domain.\n$$\n\\phi = \\frac{\\sum_{c=1}^{C} \\sum_{i=1}^{H} \\sum_{j=1}^{W} |S^{(c)}_{i,j}| M_{i,j}}{\\sum_{c=1}^{C} \\sum_{i=1}^{H} \\sum_{j=1}^{W} |S^{(c)}_{i,j}|}\n$$\nA value of $\\phi$ close to $1$ indicates that the model primarily uses information from physically relevant regions to make its prediction, suggesting that it has learned a physically meaningful relationship. A value close to $0$ suggests the opposite. If the total saliency is zero, $\\phi$ is defined as $0$.\n\nThe implementation will follow these steps for each of the four test cases provided.",
            "answer": "```python\nimport numpy as np\nfrom scipy import signal\n\ndef solve():\n    \"\"\"\n    Main function to solve the CNN saliency problem for all test cases.\n    \"\"\"\n    \n    # -------------------\n    # Problem Configuration\n    # -------------------\n    H, W = 16, 16\n    C, F = 3, 2\n    k = 3\n    q_th = 25.0\n\n    # Network parameters\n    kernels = np.zeros((F, C, k, k))\n    # Filter f=1 (index 0)\n    kernels[0, 0, :, :] = 0.02 * np.ones((k, k))  # K_q\n    kernels[0, 1, :, :] = 0.01 * np.ones((k, k))  # K_T'\n    kernels[0, 2, :, :] = 500.0 * np.ones((k, k)) # K_c\n    # Filter f=2 (index 1)\n    kernels[1, 0, :, :] = 0.0 * np.zeros((k, k)) # K_q\n    kernels[1, 1, :, :] = 0.5 * np.array([[-1, 0, 1], [-1, 0, 1], [-1, 0, 1]]) # K_T'\n    kernels[1, 2, :, :] = 0.0 * np.zeros((k, k)) # K_c\n\n    biases_conv = np.array([0.0, 0.0])\n    weights_out = np.array([20.0, 5.0])\n    bias_out = 0.0\n\n    # -------------------\n    # Helper Functions\n    # -------------------\n    def gaussian(i_grid, j_grid, cx, cy, sigma):\n        return np.exp(-((i_grid - cx)**2 + (j_grid - cy)**2) / (2 * sigma**2))\n\n    # Grid for generating fields\n    i_grid, j_grid = np.meshgrid(np.arange(H), np.arange(W), indexing='ij')\n\n    # Test Case Definitions\n    test_cases = [\n        # Case 1: Central plume\n        (\n            10 + 30 * gaussian(i_grid, j_grid, 8, 8, 2),\n            2 * gaussian(i_grid, j_grid, 8, 8, 2),\n            1e-3 * gaussian(i_grid, j_grid, 8, 8, 2)\n        ),\n        # Case 2: Corner plume\n        (\n            10 + 30 * gaussian(i_grid, j_grid, 3, 3, 2),\n            2 * gaussian(i_grid, j_grid, 3, 3, 2),\n            1e-3 * gaussian(i_grid, j_grid, 3, 3, 2)\n        ),\n        # Case 3: Uniform fields\n        (\n            20 * np.ones((H, W)),\n            0 * np.ones((H, W)),\n            0 * np.ones((H, W))\n        ),\n        # Case 4: Ring moisture, central cold anomaly, divergence\n        (\n            np.maximum(0, 10 + 35 * (gaussian(i_grid, j_grid, 8, 8, 3) - 0.7 * gaussian(i_grid, j_grid, 8, 8, 1.5))),\n            -2 * gaussian(i_grid, j_grid, 8, 8, 2.5),\n            -1e-3 * gaussian(i_grid, j_grid, 8, 8, 2)\n        )\n    ]\n    \n    results = []\n    \n    for case_data in test_cases:\n        x = np.array(case_data)  # Shape (C, H, W)\n\n        # -------------------\n        # 1. Forward Pass\n        # -------------------\n        z_maps = np.zeros((F, H, W))\n        a_maps = np.zeros((F, H, W))\n        g_features = np.zeros(F)\n\n        for f in range(F):\n            z_f = np.full((H, W), biases_conv[f])\n            for c in range(C):\n                z_f += signal.correlate2d(x[c, :, :], kernels[f, c, :, :], mode='same', boundary='fill', fillvalue=0)\n            z_maps[f, :, :] = z_f\n            a_maps[f, :, :] = np.maximum(0, z_f)\n            g_features[f] = np.mean(a_maps[f, :, :])\n\n        y_hat = np.sum(weights_out * g_features) + bias_out\n\n        # -------------------\n        # 2. Backward Pass (Saliency)\n        # -------------------\n        saliency_maps = np.zeros((C, H, W))\n        \n        for f in range(F):\n            # Upstream gradient from output layer, through pooling\n            d_y_hat_d_g_f = weights_out[f]\n            d_g_f_d_a_f = 1.0 / (H * W)\n            d_y_hat_d_a_f = d_y_hat_d_g_f * d_g_f_d_a_f\n            \n            # Upstream gradient through ReLU\n            d_a_f_d_z_f = (z_maps[f, :, :] > 0).astype(float)\n            delta_z_f = d_y_hat_d_a_f * d_a_f_d_z_f\n\n            # Gradient w.r.t input x\n            for c in range(C):\n                # Gradient of cross-correlation is convolution with original kernel\n                saliency_maps[c, :, :] += signal.convolve2d(delta_z_f, kernels[f, c, :, :], mode='same', boundary='fill', fillvalue=0)\n        \n        # -------------------\n        # 3. Saliency Concentration Fraction (phi)\n        # -------------------\n        mask = ((x[0, :, :] > q_th)  (x[2, :, :] > 0)).astype(float)\n        \n        saliency_abs_total = np.sum(np.abs(saliency_maps))\n        \n        numerator = np.sum(np.abs(saliency_maps) * mask[np.newaxis, :, :])\n        \n        phi = 0.0\n        if saliency_abs_total > 1e-12: # Avoid division by zero\n            phi = numerator / saliency_abs_total\n            \n        # Store results\n        results.append(y_hat)\n        results.append(phi)\n        \n    # Format and print the final output\n    formatted_results = [f\"{r:.3f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}