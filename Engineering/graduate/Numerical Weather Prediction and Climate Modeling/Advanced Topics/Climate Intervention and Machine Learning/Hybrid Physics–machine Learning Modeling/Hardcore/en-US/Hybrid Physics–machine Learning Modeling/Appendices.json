{
    "hands_on_practices": [
        {
            "introduction": "A powerful method for integrating physical knowledge into machine learning is to modify the training objective itself. This practice guides you in constructing a composite loss function that simultaneously penalizes supervised prediction errors and violations of a core physical principle. You will work with a single-column atmospheric model and enforce the conservation of moist static energy, demonstrating a cornerstone technique for applying \"soft\" physical constraints during model training .",
            "id": "4052750",
            "problem": "You are designing a hybrid physics–Machine Learning (ML) parameterization for a Single Column Model (SCM) used in Numerical Weather Prediction (NWP) and climate simulations. The ML component predicts layerwise tendencies of temperature and specific humidity, and must obey the First Law of Thermodynamics for moist air through closure of the moist static energy budget. Your task is to derive, implement, and evaluate a composite loss function that penalizes both supervised errors and violations of moist static energy closure.\n\nBegin from the following fundamental base. Consider the First Law of Thermodynamics applied to a fixed-height Eulerian control volume for moist air with constant acceleration due to gravity. Define moist static energy as $$s = c_p T + g z + L_v q,$$ where $T$ is temperature, $z$ is geometric height, $q$ is specific humidity, $c_p$ is specific heat capacity of air at constant pressure, $g$ is gravitational acceleration, and $L_v$ is latent heat of vaporization. In a fixed-height layer where $z$ does not change with time, the local moist static energy tendency reduces to $$\\frac{\\partial s}{\\partial t} = c_p \\frac{\\partial T}{\\partial t} + L_v \\frac{\\partial q}{\\partial t}.$$ In the absence of advection when focusing on the subgrid and radiative processes learned by the ML model, closure of the local budget requires the diabatic source per unit mass, denoted $H$ with units of $\\mathrm{W\\,kg^{-1}}$, to satisfy $$c_p \\frac{\\partial T}{\\partial t} + L_v \\frac{\\partial q}{\\partial t} = H.$$\n\nYou will construct a composite loss with two parts: a supervised data loss and a physics closure regularization. The supervised data loss compares the ML-predicted tendencies $\\widehat{\\partial T/\\partial t}$ and $\\widehat{\\partial q/\\partial t}$ to observed target tendencies $y_T$ and $y_q$ at each layer. The physics loss penalizes non-closure of the moist static energy budget using the residual $$r = c_p \\widehat{\\frac{\\partial T}{\\partial t}} + L_v \\widehat{\\frac{\\partial q}{\\partial t}} - H.$$ The residual must be mass-weighted across layers to represent a column-integrated penalty. For a column discretized into layers indexed by $i$, with layer density $\\rho_i$ and thickness $\\Delta z_i$, the mass per unit area is $w_i = \\rho_i \\Delta z_i$ and the column-mean squared residual should be weighted by $w_i$.\n\nDefine the following composite loss that your program must compute for each test case:\n- The supervised data loss is\n$$L_{\\text{data}} = \\frac{1}{N} \\sum_{i=1}^{N} \\left[ \\left( \\frac{\\widehat{T}_i - y_{T,i}}{S_T} \\right)^2 + \\beta \\left( \\frac{\\widehat{q}_i - y_{q,i}}{S_q} \\right)^2 \\right],$$\nwhere $N$ is the number of layers, $\\widehat{T}_i = \\widehat{\\partial T/\\partial t}$ at layer $i$ in $\\mathrm{K\\,s^{-1}}$, $\\widehat{q}_i = \\widehat{\\partial q/\\partial t}$ at layer $i$ in $\\mathrm{s^{-1}}$ (since $q$ is dimensionless in $\\mathrm{kg\\,kg^{-1}}$), $y_{T,i}$ and $y_{q,i}$ are the corresponding observed targets, $S_T$ and $S_q$ are scaling parameters to nondimensionalize the terms, and $\\beta$ is a relative weighting parameter.\n- The physics closure loss is\n$$L_{\\text{phys}} = \\frac{\\sum_{i=1}^{N} w_i \\, r_i^2}{\\left(\\sum_{i=1}^{N} w_i\\right) \\, H_{\\text{scale}}^2},$$\nwhere $r_i = c_p \\widehat{T}_i + L_v \\widehat{q}_i - H_i$, $w_i = \\rho_i \\Delta z_i$, and $H_{\\text{scale}}$ is a fixed scaling constant used for nondimensionalization.\n- The total loss is\n$$L_{\\text{total}} = L_{\\text{data}} + \\lambda \\, L_{\\text{phys}},$$\nwhere $\\lambda$ is a nonnegative tuning parameter.\n\nUse the following constants (expressed in SI units) in all computations:\n- $c_p = 1004 \\ \\mathrm{J\\,kg^{-1}\\,K^{-1}}$,\n- $L_v = 2.5 \\times 10^6 \\ \\mathrm{J\\,kg^{-1}}$,\n- $g = 9.81 \\ \\mathrm{m\\,s^{-2}}$ (note that $g$ will not be directly used since $z$ is fixed),\n- $S_T = 1.0 \\times 10^{-5} \\ \\mathrm{K\\,s^{-1}}$,\n- $S_q = 1.0 \\times 10^{-8} \\ \\mathrm{s^{-1}}$,\n- $\\beta = 1$,\n- $H_{\\text{scale}} = 5.0 \\times 10^{-2} \\ \\mathrm{W\\,kg^{-1}}$,\n- $\\lambda = 10$.\n\nImplement a program that computes $L_{\\text{total}}$ for each of the following test cases. All arrays are listed in the order of increasing layer index $i$, and each entry is a layer value. Units must be respected exactly as specified.\n\nTest Case A (happy path, exact closure but nonzero supervised error):\n- Number of layers $N = 3$.\n- Layer thicknesses $\\Delta z = [1000, 1000, 1000] \\ \\mathrm{m}$.\n- Layer densities $\\rho = [1.2, 0.9, 0.7] \\ \\mathrm{kg\\,m^{-3}}$.\n- ML-predicted temperature tendencies $\\widehat{T} = [1.0 \\times 10^{-5}, 1.2 \\times 10^{-5}, 0.8 \\times 10^{-5}] \\ \\mathrm{K\\,s^{-1}}$.\n- ML-predicted specific humidity tendencies $\\widehat{q} = [1.0 \\times 10^{-8}, 0.5 \\times 10^{-8}, 0.0 \\times 10^{-8}] \\ \\mathrm{s^{-1}}$.\n- Known diabatic sources $H$ constructed to satisfy exact closure layerwise:\n  - $H = [0.03504, 0.024548, 0.008032] \\ \\mathrm{W\\,kg^{-1}}$.\n- Observed target tendencies $y_T = [1.1 \\times 10^{-5}, 1.1 \\times 10^{-5}, 0.9 \\times 10^{-5}] \\ \\mathrm{K\\,s^{-1}}$, $y_q = [0.8 \\times 10^{-8}, 0.6 \\times 10^{-8}, 0.1 \\times 10^{-8}] \\ \\mathrm{s^{-1}}$.\n\nTest Case B (strong physics violation and supervised discrepancies):\n- Number of layers $N = 3$.\n- Layer thicknesses $\\Delta z = [800, 1200, 1500] \\ \\mathrm{m}$.\n- Layer densities $\\rho = [1.0, 0.8, 0.6] \\ \\mathrm{kg\\,m^{-3}}$.\n- ML-predicted temperature tendencies $\\widehat{T} = [2.0 \\times 10^{-5}, -0.5 \\times 10^{-5}, 0.0 \\times 10^{-5}] \\ \\mathrm{K\\,s^{-1}}$.\n- ML-predicted specific humidity tendencies $\\widehat{q} = [2.0 \\times 10^{-8}, -1.0 \\times 10^{-8}, 0.5 \\times 10^{-8}] \\ \\mathrm{s^{-1}}$.\n- Known diabatic sources $H = [0.005, 0.0, -0.002] \\ \\mathrm{W\\,kg^{-1}}$.\n- Observed target tendencies $y_T = [1.5 \\times 10^{-5}, -0.4 \\times 10^{-5}, 0.2 \\times 10^{-5}] \\ \\mathrm{K\\,s^{-1}}$, $y_q = [1.5 \\times 10^{-8}, -1.2 \\times 10^{-8}, 0.4 \\times 10^{-8}] \\ \\mathrm{s^{-1}}$.\n\nTest Case C (edge case: single dry layer, trivial closure and supervision):\n- Number of layers $N = 1$.\n- Layer thicknesses $\\Delta z = [500] \\ \\mathrm{m}$.\n- Layer densities $\\rho = [1.1] \\ \\mathrm{kg\\,m^{-3}}$.\n- ML-predicted temperature tendencies $\\widehat{T} = [0.0] \\ \\mathrm{K\\,s^{-1}}$.\n- ML-predicted specific humidity tendencies $\\widehat{q} = [0.0] \\ \\mathrm{s^{-1}}$.\n- Known diabatic sources $H = [0.0] \\ \\mathrm{W\\,kg^{-1}}$.\n- Observed target tendencies $y_T = [0.0] \\ \\mathrm{K\\,s^{-1}}$, $y_q = [0.0] \\ \\mathrm{s^{-1}}$.\n\nTest Case D (boundary case: perfect supervision and exact closure):\n- Number of layers $N = 2$.\n- Layer thicknesses $\\Delta z = [1000, 1000] \\ \\mathrm{m}$.\n- Layer densities $\\rho = [1.1, 0.9] \\ \\mathrm{kg\\,m^{-3}}$.\n- ML-predicted temperature tendencies $\\widehat{T} = [1.0 \\times 10^{-5}, -1.0 \\times 10^{-5}] \\ \\mathrm{K\\,s^{-1}}$.\n- ML-predicted specific humidity tendencies $\\widehat{q} = [1.0 \\times 10^{-8}, 2.0 \\times 10^{-8}] \\ \\mathrm{s^{-1}}$.\n- Known diabatic sources $H = [0.03504, 0.03996] \\ \\mathrm{W\\,kg^{-1}}$.\n- Observed target tendencies $y_T = [1.0 \\times 10^{-5}, -1.0 \\times 10^{-5}] \\ \\mathrm{K\\,s^{-1}}$, $y_q = [1.0 \\times 10^{-8}, 2.0 \\times 10^{-8}] \\ \\mathrm{s^{-1}}$.\n\nYour program must compute $L_{\\text{total}}$ for each test case and produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $[ \\ell_A, \\ell_B, \\ell_C, \\ell_D ]$ where each $\\ell$ is a floating-point value. No other output is permitted. All calculations must be performed in SI units, and the output should be printed in the exact single-line format described here.",
            "solution": "The problem is scientifically and mathematically valid. It is well-posed, self-contained, and grounded in the principles of atmospheric thermodynamics and physics-informed machine learning. All constants, variables, and functional forms are explicitly defined, and the provided test cases contain all necessary data for a unique solution. We will proceed with the calculation.\n\nThe objective is to compute a total loss function, $L_{\\text{total}}$, for four distinct test cases. This loss function is a composite of a supervised data loss component, $L_{\\text{data}}$, and a physics-based closure loss component, $L_{\\text{phys}}$. The total loss is defined as a weighted sum:\n$$L_{\\text{total}} = L_{\\text{data}} + \\lambda \\, L_{\\text{phys}}$$\n\nThe supervised data loss, $L_{\\text{data}}$, quantifies the mean squared error between the ML-predicted tendencies and the observed target tendencies, normalized by scaling factors. For a column with $N$ layers, it is given by:\n$$L_{\\text{data}} = \\frac{1}{N} \\sum_{i=1}^{N} \\left[ \\left( \\frac{\\widehat{T}_i - y_{T,i}}{S_T} \\right)^2 + \\beta \\left( \\frac{\\widehat{q}_i - y_{q,i}}{S_q} \\right)^2 \\right]$$\nwhere $\\widehat{T}_i$ and $\\widehat{q}_i$ are the predicted tendencies for temperature and specific humidity at layer $i$, $y_{T,i}$ and $y_{q,i}$ are the corresponding observed targets, $S_T$ and $S_q$ are normalization scales, and $\\beta$ is a relative weighting factor.\n\nThe physics closure loss, $L_{\\text{phys}}$, penalizes violations of the moist static energy budget. It is formulated as a mass-weighted, normalized mean squared residual of the energy conservation equation:\n$$L_{\\text{phys}} = \\frac{\\sum_{i=1}^{N} w_i \\, r_i^2}{\\left(\\sum_{i=1}^{N} w_i\\right) \\, H_{\\text{scale}}^2}$$\nThe physical residual, $r_i$, for each layer is $r_i = c_p \\widehat{T}_i + L_v \\widehat{q}_i - H_i$, where $H_i$ is the known diabatic source. The mass weight for each layer is $w_i = \\rho_i \\Delta z_i$, where $\\rho_i$ is the layer density and $\\Delta z_i$ is the layer thickness. $H_{\\text{scale}}$ is a constant for nondimensionalization.\n\nThe calculations will use the following SI unit constants as specified:\n- Specific heat capacity of air, $c_p = 1004 \\ \\mathrm{J\\,kg^{-1}\\,K^{-1}}$\n- Latent heat of vaporization, $L_v = 2.5 \\times 10^6 \\ \\mathrm{J\\,kg^{-1}}$\n- Temperature tendency scale, $S_T = 1.0 \\times 10^{-5} \\ \\mathrm{K\\,s^{-1}}$\n- Specific humidity tendency scale, $S_q = 1.0 \\times 10^{-8} \\ \\mathrm{s^{-1}}$\n- Data loss humidity weight, $\\beta = 1$\n- Physics loss scale, $H_{\\text{scale}} = 5.0 \\times 10^{-2} \\ \\mathrm{W\\,kg^{-1}}$\n- Physics loss weight, $\\lambda = 10$\n\nWe now compute $L_{\\text{total}}$ for each test case.\n\n**Test Case A**\nThe model has $N=3$ layers. The problem states that the diabatic sources $H_i$ are constructed for exact closure, implying the physical residual $r_i=0$ for all $i$.\nFirst, we verify this property for layer $i=1$:\n$r_1 = c_p \\widehat{T}_1 + L_v \\widehat{q}_1 - H_1 = (1004)(1.0 \\times 10^{-5}) + (2.5 \\times 10^6)(1.0 \\times 10^{-8}) - 0.03504 = 0.01004 + 0.025 - 0.03504 = 0$.\nThe residuals for layers $i=2$ and $i=3$ are also $0$ by construction. Consequently, $L_{\\text{phys}} = 0$.\n\nNext, we compute $L_{\\text{data}}$:\nThe layer-wise normalized squared errors for temperature are:\n$\\left( \\frac{(1.0 - 1.1) \\times 10^{-5}}{1.0 \\times 10^{-5}} \\right)^2 = (-0.1)^2 = 0.01$\n$\\left( \\frac{(1.2 - 1.1) \\times 10^{-5}}{1.0 \\times 10^{-5}} \\right)^2 = (0.1)^2 = 0.01$\n$\\left( \\frac{(0.8 - 0.9) \\times 10^{-5}}{1.0 \\times 10^{-5}} \\right)^2 = (-0.1)^2 = 0.01$\nThe layer-wise normalized squared errors for specific humidity are:\n$\\left( \\frac{(1.0 - 0.8) \\times 10^{-8}}{1.0 \\times 10^{-8}} \\right)^2 = (0.2)^2 = 0.04$\n$\\left( \\frac{(0.5 - 0.6) \\times 10^{-8}}{1.0 \\times 10^{-8}} \\right)^2 = (-0.1)^2 = 0.01$\n$\\left( \\frac{(0.0 - 0.1) \\times 10^{-8}}{1.0 \\times 10^{-8}} \\right)^2 = (-0.1)^2 = 0.01$\nWith $\\beta=1$, the sum of the layer-wise loss terms is $(0.01 + 0.04) + (0.01 + 0.01) + (0.01 + 0.01) = 0.05 + 0.02 + 0.02 = 0.09$.\n$L_{\\text{data}} = \\frac{1}{3} \\times 0.09 = 0.03$.\n$L_{\\text{total, A}} = L_{\\text{data}} + \\lambda L_{\\text{phys}} = 0.03 + (10)(0) = 0.03$.\n\n**Test Case B**\nThe model has $N=3$ layers.\nFirst, we compute $L_{\\text{data}}$:\nThe layer-wise normalized squared errors are:\n$i=1: \\left(\\frac{(2.0 - 1.5) \\times 10^{-5}}{1.0 \\times 10^{-5}}\\right)^2 + 1 \\cdot \\left(\\frac{(2.0 - 1.5) \\times 10^{-8}}{1.0 \\times 10^{-8}}\\right)^2 = (0.5)^2 + (0.5)^2 = 0.25 + 0.25 = 0.5$.\n$i=2: \\left(\\frac{(-0.5 - (-0.4)) \\times 10^{-5}}{1.0 \\times 10^{-5}}\\right)^2 + 1 \\cdot \\left(\\frac{(-1.0 - (-1.2)) \\times 10^{-8}}{1.0 \\times 10^{-8}}\\right)^2 = (-0.1)^2 + (0.2)^2 = 0.01 + 0.04 = 0.05$.\n$i=3: \\left(\\frac{(0.0 - 0.2) \\times 10^{-5}}{1.0 \\times 10^{-5}}\\right)^2 + 1 \\cdot \\left(\\frac{(0.5 - 0.4) \\times 10^{-8}}{1.0 \\times 10^{-8}}\\right)^2 = (-0.2)^2 + (0.1)^2 = 0.04 + 0.01 = 0.05$.\n$L_{\\text{data}} = \\frac{1}{3} (0.5 + 0.05 + 0.05) = \\frac{0.6}{3} = 0.2$.\n\nNext, we compute $L_{\\text{phys}}$:\nLayer mass weights $w_i = \\rho_i \\Delta z_i$:\n$w_1 = (1.0)(800) = 800 \\ \\mathrm{kg\\,m^{-2}}$.\n$w_2 = (0.8)(1200) = 960 \\ \\mathrm{kg\\,m^{-2}}$.\n$w_3 = (0.6)(1500) = 900 \\ \\mathrm{kg\\,m^{-2}}$.\nTotal mass $\\sum w_i = 800 + 960 + 900 = 2660 \\ \\mathrm{kg\\,m^{-2}}$.\nPhysical residuals $r_i = c_p \\widehat{T}_i + L_v \\widehat{q}_i - H_i$:\n$r_1 = (1004)(2.0 \\times 10^{-5}) + (2.5 \\times 10^6)(2.0 \\times 10^{-8}) - 0.005 = 0.02008 + 0.05 - 0.005 = 0.06508$.\n$r_2 = (1004)(-0.5 \\times 10^{-5}) + (2.5 \\times 10^6)(-1.0 \\times 10^{-8}) - 0.0 = -0.00502 - 0.025 = -0.03002$.\n$r_3 = (1004)(0.0) + (2.5 \\times 10^6)(0.5 \\times 10^{-8}) - (-0.002) = 0 + 0.0125 + 0.002 = 0.0145$.\nNumerator of $L_{\\text{phys}}$: $\\sum w_i r_i^2 = 800(0.06508)^2 + 960(-0.03002)^2 + 900(0.0145)^2 \\approx 3.38833 + 0.86515 + 0.189225 = 4.4427$.\nDenominator of $L_{\\text{phys}}$: $(\\sum w_i) H_{\\text{scale}}^2 = (2660)(5.0 \\times 10^{-2})^2 = 2660 \\times 0.0025 = 6.65$.\n$L_{\\text{phys}} \\approx 4.4427 / 6.65 \\approx 0.6680756$.\n$L_{\\text{total, B}} = L_{\\text{data}} + \\lambda L_{\\text{phys}} = 0.2 + 10(0.6680756) = 0.2 + 6.680756 = 6.880756$.\n\n**Test Case C**\nThis is a single-layer case ($N=1$) where all predicted tendencies, target tendencies, and diabatic sources are zero.\n$L_{\\text{data}}$: The differences $\\widehat{T}_1 - y_{T,1}$ and $\\widehat{q}_1 - y_{q,1}$ are both $0$. Thus, $L_{\\text{data}} = 0$.\n$L_{\\text{phys}}$: The residual $r_1 = c_p(0) + L_v(0) - 0 = 0$. Thus, $L_{\\text{phys}} = 0$.\n$L_{\\text{total, C}} = L_{\\text{data}} + \\lambda L_{\\text{phys}} = 0 + (10)(0) = 0$.\n\n**Test Case D**\nThis is a case with $N=2$ layers, perfect supervision ($\\widehat{T}_i = y_{T,i}$, $\\widehat{q}_i = y_{q,i}$), and exact physical closure.\n$L_{\\text{data}}$: Since the predicted and target tendencies are identical for all layers, the errors are all $0$. Thus, $L_{\\text{data}} = 0$.\n$L_{\\text{phys}}$: By construction, the closure is exact, meaning $r_i=0$ for all layers. We can verify for layer $i=2$:\n$r_2 = (1004)(-1.0 \\times 10^{-5}) + (2.5 \\times 10^6)(2.0 \\times 10^{-8}) - 0.03996 = -0.01004 + 0.05 - 0.03996 = 0$.\nSince all residuals are $0$, $L_{\\text{phys}} = 0$.\n$L_{\\text{total, D}} = L_{\\text{data}} + \\lambda L_{\\text{phys}} = 0 + (10)(0) = 0$.\n\nSummary of results:\n- Test Case A: $L_{\\text{total}} = 0.03$\n- Test Case B: $L_{\\text{total}} \\approx 6.880756$\n- Test Case C: $L_{\\text{total}} = 0.0$\n- Test Case D: $L_{\\text{total}} = 0.0$\nThese values will be computed programmatically to machine precision.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the total loss for a hybrid physics-ML model for given test cases.\n    \"\"\"\n    # Define physical and model constants\n    c_p = 1004.0         # J kg^-1 K^-1\n    L_v = 2.5e6          # J kg^-1\n    S_T = 1.0e-5         # K s^-1\n    S_q = 1.0e-8         # s^-1\n    beta = 1.0           # dimensionless\n    H_scale = 5.0e-2     # W kg^-1\n    lambda_ = 10.0       # dimensionless\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test Case A\n        {\n            \"N\": 3,\n            \"delta_z\": np.array([1000, 1000, 1000]),\n            \"rho\": np.array([1.2, 0.9, 0.7]),\n            \"T_hat\": np.array([1.0e-5, 1.2e-5, 0.8e-5]),\n            \"q_hat\": np.array([1.0e-8, 0.5e-8, 0.0e-8]),\n            \"H\": np.array([0.03504, 0.024548, 0.008032]),\n            \"y_T\": np.array([1.1e-5, 1.1e-5, 0.9e-5]),\n            \"y_q\": np.array([0.8e-8, 0.6e-8, 0.1e-8]),\n        },\n        # Test Case B\n        {\n            \"N\": 3,\n            \"delta_z\": np.array([800, 1200, 1500]),\n            \"rho\": np.array([1.0, 0.8, 0.6]),\n            \"T_hat\": np.array([2.0e-5, -0.5e-5, 0.0e-5]),\n            \"q_hat\": np.array([2.0e-8, -1.0e-8, 0.5e-8]),\n            \"H\": np.array([0.005, 0.0, -0.002]),\n            \"y_T\": np.array([1.5e-5, -0.4e-5, 0.2e-5]),\n            \"y_q\": np.array([1.5e-8, -1.2e-8, 0.4e-8]),\n        },\n        # Test Case C\n        {\n            \"N\": 1,\n            \"delta_z\": np.array([500]),\n            \"rho\": np.array([1.1]),\n            \"T_hat\": np.array([0.0]),\n            \"q_hat\": np.array([0.0]),\n            \"H\": np.array([0.0]),\n            \"y_T\": np.array([0.0]),\n            \"y_q\": np.array([0.0]),\n        },\n        # Test Case D\n        {\n            \"N\": 2,\n            \"delta_z\": np.array([1000, 1000]),\n            \"rho\": np.array([1.1, 0.9]),\n            \"T_hat\": np.array([1.0e-5, -1.0e-5]),\n            \"q_hat\": np.array([1.0e-8, 2.0e-8]),\n            \"H\": np.array([0.03504, 0.03996]),\n            \"y_T\": np.array([1.0e-5, -1.0e-5]),\n            \"y_q\": np.array([1.0e-8, 2.0e-8]),\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        N = case[\"N\"]\n        T_hat = case[\"T_hat\"]\n        q_hat = case[\"q_hat\"]\n        y_T = case[\"y_T\"]\n        y_q = case[\"y_q\"]\n        delta_z = case[\"delta_z\"]\n        rho = case[\"rho\"]\n        H = case[\"H\"]\n\n        # 1. Compute L_data\n        term_T = ((T_hat - y_T) / S_T)**2\n        term_q = ((q_hat - y_q) / S_q)**2\n        L_data = (1.0 / N) * np.sum(term_T + beta * term_q)\n\n        # 2. Compute L_phys\n        # Physical residual for each layer\n        r = c_p * T_hat + L_v * q_hat - H\n        \n        # Mass weight for each layer\n        w = rho * delta_z\n        \n        # Mass-weighted squared residual sum (numerator)\n        numerator = np.sum(w * r**2)\n        \n        # Total mass and scaling (denominator)\n        denominator = np.sum(w) * (H_scale**2)\n        \n        # Handle case of zero denominator to avoid division by zero\n        if denominator == 0:\n            if numerator == 0:\n                L_phys = 0.0\n            else:\n                # This case implies non-zero residual with zero mass,\n                # which is physically ill-defined, but for numerical\n                # stability we can treat it as infinite penalty.\n                L_phys = np.inf\n        else:\n            L_phys = numerator / denominator\n\n        # 3. Compute L_total\n        L_total = L_data + lambda_ * L_phys\n        results.append(L_total)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.7f}' for r in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Beyond penalizing violations, we can design model architectures that inherently respect physical laws, a form of \"hard\" constraint enforcement. This exercise focuses on a turbulence closure model, where you will implement a parametrization that guarantees non-negative kinetic energy dissipation by construction. You will achieve this by ensuring a learned eddy-viscosity tensor is always positive semidefinite (PSD), a sophisticated technique that makes unphysical behavior impossible by design .",
            "id": "4052738",
            "problem": "Consider resolved dynamics governed by the incompressible, filtered Navier–Stokes equations with an eddy-viscosity closure for subgrid stresses in two spatial dimensions. Let the resolved velocity be denoted by $\\mathbf{u}(\\mathbf{x},t)$, the resolved pressure by $p(\\mathbf{x},t)$, and the resolved, symmetric strain-rate tensor be defined by $S = (\\nabla \\mathbf{u} + \\nabla \\mathbf{u}^{\\top})/2$ with components $S_{ij}$ for $i,j \\in \\{x,y\\}$. The resolved kinetic energy density is $K = (1/2)\\,\\mathbf{u}\\cdot\\mathbf{u}$. A hybrid physics–Machine Learning (ML) eddy-viscosity closure is to be constructed that guarantees nonnegative energy dissipation. The closure acts linearly on the space of symmetric rank-$2$ tensors. To represent this linear action, adopt a Voigt-like vectorization of $S$ as $s \\in \\mathbb{R}^{3}$ given by $s = [S_{xx}, \\sqrt{2}\\,S_{xy}, S_{yy}]^{\\top}$ so that the Frobenius inner product $S:S$ equals $s^{\\top}s$.\n\nThe eddy-viscosity operator is represented as a symmetric matrix $A(\\mathbf{x}) \\in \\mathbb{R}^{3\\times 3}$ acting on $s$. The objective is to design $A(\\mathbf{x})$ so that it is Positive Semidefinite (PSD), thereby ensuring nonnegative kinetic energy dissipation.\n\nPart I (derivation): Starting from the incompressible momentum equations and the definition of resolved kinetic energy density $K$, derive the expression for the pointwise dissipation density induced by a linear, symmetric eddy-viscosity operator $A$ acting on $s$, and prove that if $A$ is PSD then the dissipation density is nonnegative. Your derivation must begin from fundamental laws, specifically the filtered incompressible Navier–Stokes momentum equation and the definition of $K$, and proceed through the kinetic energy budget. Clearly state any assumptions on boundary conditions needed to eliminate boundary fluxes. Conclude with a statement showing why the PSD property of $A$ implies nonnegative dissipation density.\n\nPart II (parametrization): Propose a parametrization of $A$ that guarantees it is PSD for any unconstrained ML outputs. Use a lower-triangular matrix $L \\in \\mathbb{R}^{3\\times 3}$ with diagonals constrained to be nonnegative via a smooth nonlinearity, and set\n$$\nA = L\\,L^{\\top} + \\nu_{0}\\,I,\n$$\nwhere $I$ is the $3\\times 3$ identity and $\\nu_{0}\\ge 0$ is a scalar, baseline eddy viscosity from physics. Specify how to construct $L$ from an unconstrained vector $z \\in \\mathbb{R}^{6}$ and how to construct $\\nu_{0}$ from features $x \\in \\mathbb{R}^{p}$ so that $A$ is PSD. Your parametrization must include:\n- A definition of $L$ in terms of the entries of $z$ with diagonals enforced nonnegative using a smooth function such as the SoftPlus nonlinearity defined by $\\mathrm{softplus}(a)=\\log(1+\\exp(a))$.\n- A definition of $\\nu_{0}$ in terms of a linear map of $x$ passed through $\\mathrm{softplus}$.\n- A small positive constant $\\varepsilon$ added to diagonal entries to ensure numerical stability.\n\nPart III (implementation): Implement a program that constructs $A$ for several test cases and verifies nonnegative dissipation using the derived expression from Part I. Use the following specification for the ML and physics components:\n- Dimension is two, so $s\\in\\mathbb{R}^{3}$ and $A\\in\\mathbb{R}^{3\\times 3}$.\n- Given features $x\\in\\mathbb{R}^{5}$, form $z = W\\,x + b \\in \\mathbb{R}^{6}$, where\n$$\nW = \\begin{bmatrix}\n0.5 & -0.3 & 0.1 & 0.0 & 0.2 \\\\\n-0.4 & 0.6 & -0.2 & 0.1 & -0.1 \\\\\n0.3 & -0.5 & 0.2 & -0.1 & 0.0 \\\\\n0.2 & 0.1 & -0.3 & 0.5 & -0.4 \\\\\n-0.1 & 0.4 & 0.2 & -0.2 & 0.3 \\\\\n0.6 & -0.2 & 0.0 & 0.1 & -0.3\n\\end{bmatrix},\\quad\nb = \\begin{bmatrix}-1.0 \\\\ -1.5 \\\\ -2.0 \\\\ 0.0 \\\\ 0.5 \\\\ -0.3\\end{bmatrix}.\n$$\n- Construct $L$ from $z$ as\n$$\nL = \\begin{bmatrix}\nd_{1} & 0 & 0 \\\\\n\\ell_{21} & d_{2} & 0 \\\\\n\\ell_{31} & \\ell_{32} & d_{3}\n\\end{bmatrix},\n\\quad\n\\text{with}\\quad\n\\begin{aligned}\nd_{1} &= \\mathrm{softplus}(z_{1}) + \\varepsilon,\\\\\nd_{2} &= \\mathrm{softplus}(z_{2}) + \\varepsilon,\\\\\nd_{3} &= \\mathrm{softplus}(z_{3}) + \\varepsilon,\\\\\n\\ell_{21} &= z_{4},\\ \\ell_{31} = z_{5},\\ \\ell_{32} = z_{6}.\n\\end{aligned}\n$$\n- Define the baseline scalar eddy viscosity as\n$$\n\\nu_{0} = \\mathrm{softplus}(\\gamma^{\\top} x + \\gamma_{0}),\n\\quad\n\\gamma = \\begin{bmatrix}0.2\\\\ -0.1\\\\ 0.4\\\\ 0.0\\\\ 0.3\\end{bmatrix},\\quad\n\\gamma_{0} = -0.2.\n$$\n- Use $\\varepsilon = 10^{-9}$.\n\nGiven a symmetric strain-rate matrix\n$$\nS = \\begin{bmatrix} S_{xx} & S_{xy} \\\\ S_{xy} & S_{yy} \\end{bmatrix},\n$$\nform $s = [S_{xx}, \\sqrt{2}\\,S_{xy}, S_{yy}]^{\\top}$ and compute the dissipation density according to your Part I derivation.\n\nUse the following test suite of feature vectors $x^{(k)}$ and strain matrices $S^{(k)}$:\n- Case $1$: $x^{(1)} = [0.8, 0.1, 0.5, 0.0, 0.2]^{\\top}$, $S^{(1)} = \\begin{bmatrix}0.01 & 0.02 \\\\ 0.02 & -0.01\\end{bmatrix}$.\n- Case $2$: $x^{(2)} = [0.0, 0.0, 0.0, 0.0, 0.0]^{\\top}$, $S^{(2)} = \\begin{bmatrix}0.0 & 0.0 \\\\ 0.0 & 0.0\\end{bmatrix}$.\n- Case $3$: $x^{(3)} = [2.0, -1.0, 3.0, 0.5, -0.5]^{\\top}$, $S^{(3)} = \\begin{bmatrix}0.5 & -0.8 \\\\ -0.8 & 0.3\\end{bmatrix}$.\n- Case $4$: $x^{(4)} = [-10.0, -10.0, -10.0, -10.0, -10.0]^{\\top}$, $S^{(4)} = \\begin{bmatrix}0.1 & -0.05 \\\\ -0.05 & 0.2\\end{bmatrix}$.\n\nVerification and numerical tolerance: For each case, compute the dissipation density and check nonnegativity with a numerical tolerance of $\\delta = 10^{-12}$, interpreting values greater than or equal to $-\\delta$ as nonnegative. Additionally, compute the smallest eigenvalue of $A$ and check that it is nonnegative within the same tolerance.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order\n$$\n[B_{1}, B_{2}, B_{3}, B_{4}],\n$$\nwhere $B_{k}$ is a boolean indicating whether both the dissipation density and the smallest eigenvalue of $A$ are nonnegative (within tolerance) for case $k$. No physical units are required because the quantities are expressed in consistent, dimensionless form.",
            "solution": "The problem requires the derivation of an energy dissipation condition for a hybrid physics-ML model, the formulation of a specific parametrization that satisfies this condition, and the implementation of this model for verification against test cases. The solution is presented in three parts as requested.\n\n### Part I: Derivation of Dissipation Density\n\nThe analysis begins with the filtered, incompressible Navier-Stokes equations in two spatial dimensions ($i,j \\in \\{x,y\\}$). The momentum equation for the resolved velocity field $\\mathbf{u}(\\mathbf{x},t)$ can be written as:\n$$\n\\frac{\\partial u_i}{\\partial t} + u_j \\frac{\\partial u_i}{\\partial x_j} = - \\frac{\\partial p}{\\partial x_i} - \\frac{\\partial \\tau_{ij}}{\\partial x_j}\n$$\nHere, $p$ is the resolved pressure (normalized by a constant density $\\rho=1$), and $\\tau_{ij}$ is the subgrid-scale (SGS) stress tensor, which represents the effects of unresolved motions on the resolved flow. Molecular viscosity has been neglected, as is common in high-Reynolds-number turbulence modeling where the SGS effects are dominant. The flow is incompressible, satisfying $\\nabla \\cdot \\mathbf{u} = 0$, or $\\partial u_k / \\partial x_k = 0$.\n\nThe resolved kinetic energy density is defined as $K = (1/2) \\mathbf{u}\\cdot\\mathbf{u} = (1/2) u_i u_i$. To derive its evolution equation, we take the time derivative of $K$ and substitute the momentum equation:\n$$\n\\frac{\\partial K}{\\partial t} = u_i \\frac{\\partial u_i}{\\partial t} = u_i \\left( - u_j \\frac{\\partial u_i}{\\partial x_j} - \\frac{\\partial p}{\\partial x_i} - \\frac{\\partial \\tau_{ij}}{\\partial x_j} \\right)\n$$\nWe analyze each term on the right-hand side:\n$1$. **Advection Term:** Using the product rule and the incompressibility condition, this term represents the transport of kinetic energy:\n$$\n- u_i u_j \\frac{\\partial u_i}{\\partial x_j} = - u_j \\frac{\\partial}{\\partial x_j} \\left(\\frac{1}{2} u_i u_i\\right) = - \\mathbf{u} \\cdot \\nabla K = -\\nabla \\cdot (K\\mathbf{u})\n$$\n$2$. **Pressure Term:** This term represents the transport of energy due to pressure forces:\n$$\n- u_i \\frac{\\partial p}{\\partial x_i} = - \\frac{\\partial (u_i p)}{\\partial x_i} + p \\frac{\\partial u_i}{\\partial x_i} = -\\nabla \\cdot (p \\mathbf{u}) \\quad (\\text{since } \\nabla \\cdot \\mathbf{u} = 0)\n$$\n$3$. **SGS Stress Term:** This term describes the exchange of energy between resolved and unresolved scales:\n$$\n- u_i \\frac{\\partial \\tau_{ij}}{\\partial x_j} = - \\frac{\\partial (u_i \\tau_{ij})}{\\partial x_j} + \\tau_{ij} \\frac{\\partial u_i}{\\partial x_j}\n$$\nThe second part, $\\tau_{ij} (\\partial u_i / \\partial x_j)$, is the rate of work done by the SGS stresses on the resolved velocity field. Since the SGS tensor $\\tau_{ij}$ is symmetric, its product with the velocity gradient tensor can be simplified by decomposing the gradient into its symmetric and skew-symmetric parts:\n$$\n\\tau_{ij} \\frac{\\partial u_i}{\\partial x_j} = \\tau_{ij} \\left( \\frac{1}{2}\\left(\\frac{\\partial u_i}{\\partial x_j} + \\frac{\\partial u_j}{\\partial x_i}\\right) \\right) + \\tau_{ij} \\left( \\frac{1}{2}\\left(\\frac{\\partial u_i}{\\partial x_j} - \\frac{\\partial u_j}{\\partial x_i}\\right) \\right)\n$$\nThe second term on the right vanishes because it is the inner product of a symmetric tensor ($\\boldsymbol{\\tau}$) and a skew-symmetric tensor. The first term involves the symmetric strain-rate tensor $S_{ij} = (1/2)(\\partial u_i / \\partial x_j + \\partial u_j / \\partial x_i)$. Thus, the work rate is $\\boldsymbol{\\tau}:S$.\n\nCombining all terms, the pointwise kinetic energy budget is:\n$$\n\\frac{\\partial K}{\\partial t} = -\\nabla \\cdot (K\\mathbf{u}) - \\nabla \\cdot (p\\mathbf{u}) - \\nabla \\cdot (\\mathbf{u} \\cdot \\boldsymbol{\\tau}) + \\boldsymbol{\\tau}:S\n$$\nThe final term, $\\boldsymbol{\\tau}:S$, represents production (if positive) or removal (if negative) of kinetic energy from the resolved scales. The pointwise dissipation density, $\\mathcal{D}$, is defined as the rate of energy transfer from the resolved scales to the subgrid scales, which corresponds to a loss term in the energy budget. Therefore, $\\mathcal{D} = -\\boldsymbol{\\tau}:S$. For the model to be physically dissipative, we require $\\mathcal{D} \\ge 0$.\n\nThe problem states that the eddy-viscosity closure is a linear operator acting on the space of symmetric rank-$2$ tensors, represented by a matrix $A$ in Voigt notation. A general linear anisotropic eddy-viscosity model, extending the scalar model $\\boldsymbol{\\tau}=-2\\nu_t S$, is formulated as:\n$$\nt_v = -2 A s\n$$\nwhere $t_v$ and $s$ are the Voigt vectorizations of $\\boldsymbol{\\tau}$ and $S$, respectively. The problem specifies the Voigt map such that the Frobenius inner product $S:S$ equals $s^\\top s$. This property extends to the inner product of two different tensors, meaning $\\boldsymbol{\\tau}:S = t_v^\\top s$.\n\nSubstituting the model into the dissipation density expression:\n$$\n\\mathcal{D} = -\\boldsymbol{\\tau}:S = -t_v^\\top s = -(-2As)^\\top s = 2(As)^\\top s = 2 s^\\top A^\\top s\n$$\nSince the problem specifies that $A$ is a symmetric matrix ($A^\\top = A$), the final expression for the pointwise dissipation density is:\n$$\n\\mathcal{D} = 2 s^\\top A s\n$$\nTo guarantee nonnegative dissipation ($\\mathcal{D} \\ge 0$) for any possible state of the flow (i.e., for any symmetric strain-rate tensor $S$ and its corresponding vector $s$), the condition $s^\\top A s \\ge 0$ must hold for all $s \\in \\mathbb{R}^3$. This is the definition of a Positive Semidefinite (PSD) matrix. Therefore, if the matrix $A$ is PSD, the closure model guarantees nonnegative energy dissipation.\n\nFor the total kinetic energy budget over a domain $\\Omega$, one would integrate the pointwise equation. The divergence terms can be converted to surface integrals via the divergence theorem. To isolate the total dissipation, one must assume boundary conditions (e.g., periodic boundaries) for which these surface integrals vanish.\n\n### Part II: PSD-Guaranteed Parametrization\n\nThe objective is to parametrize the symmetric matrix $A \\in \\mathbb{R}^{3\\times 3}$ such that it is guaranteed to be PSD, regardless of the unconstrained outputs of a machine learning model. A robust method to construct a PSD matrix is through Cholesky-like decomposition. We propose the form:\n$$\nA = L L^\\top + \\nu_0 I\n$$\nwhere $L \\in \\mathbb{R}^{3\\times 3}$ is a lower-triangular matrix, $I$ is the $3\\times 3$ identity matrix, and $\\nu_0 \\ge 0$ is a scalar baseline viscosity.\n\nThis construction guarantees that $A$ is PSD. To see this, consider the quadratic form $s^\\top A s$ for any vector $s \\in \\mathbb{R}^3$:\n$$\ns^\\top A s = s^\\top (L L^\\top + \\nu_0 I) s = s^\\top L L^\\top s + \\nu_0 s^\\top I s = (L^\\top s)^\\top (L^\\top s) + \\nu_0 s^\\top s = \\|L^\\top s\\|_2^2 + \\nu_0 \\|s\\|_2^2\n$$\nSince the squared Euclidean norm of any vector is non-negative and we enforce $\\nu_0 \\ge 0$, both terms on the right are non-negative. Therefore, $s^\\top A s \\ge 0$, which proves that $A$ is PSD.\n\nThe specific parametrization from the unconstrained ML output vector $z \\in \\mathbb{R}^6$ and feature vector $x \\in \\mathbb{R}^p$ is as follows:\n$1$. The lower-triangular matrix $L$ is constructed from $z$:\n$$\nL = \\begin{bmatrix} d_1 & 0 & 0 \\\\ \\ell_{21} & d_2 & 0 \\\\ \\ell_{31} & \\ell_{32} & d_3 \\end{bmatrix}\n$$\nThe off-diagonal elements are unconstrained and can be taken directly from $z$: $\\ell_{21} = z_4$, $\\ell_{31} = z_5$, $\\ell_{32} = z_6$.\nThe diagonal elements must be non-negative. This is enforced using the SoftPlus function, $\\mathrm{softplus}(a) = \\log(1+\\exp(a))$, which maps any real number to a positive real number. A small constant $\\varepsilon > 0$ is added for numerical stability, ensuring the diagonal entries are strictly positive.\n$$\n\\begin{aligned}\nd_1 &= \\mathrm{softplus}(z_1) + \\varepsilon \\\\\nd_2 &= \\mathrm{softplus}(z_2) + \\varepsilon \\\\\nd_3 &= \\mathrm{softplus}(z_3) + \\varepsilon\n\\end{aligned}\n$$\n$2$. The baseline scalar eddy viscosity $\\nu_0$ must also be non-negative. It is constructed from the feature vector $x$ via a linear transformation followed by the SoftPlus function:\n$$\n\\nu_0 = \\mathrm{softplus}(\\gamma^\\top x + \\gamma_0)\n$$\nwhere $\\gamma$ is a weight vector and $\\gamma_0$ is a bias. This ensures $\\nu_0 > 0$.\n\nThis parametrization guarantees that $A$ is not only PSD but strictly Positive Definite (PD), as $\\|s\\|_2^2 > 0$ for any $s \\ne 0$, and $\\nu_0 > 0$.\n\n### Part III: Implementation Logic\n\nThe implementation will follow the parametrization from Part II and the dissipation formula from Part I to verify the model on the provided test cases. For each case, defined by a feature vector $x^{(k)}$ and a strain-rate matrix $S^{(k)}$, the following steps are performed:\n$1$. **Compute ML outputs:** The unconstrained vector $z$ is computed from the features $x$ using the given affine transformation: $z = Wx + b$.\n$2$. **Construct L:** The lower-triangular matrix $L$ is assembled using the elements of $z$ as specified in Part II. The diagonal elements are computed using the SoftPlus function with the addition of $\\varepsilon = 10^{-9}$.\n$3$. **Compute $\\nu_0$:** The baseline viscosity $\\nu_0$ is computed from $x$ using the specified linear model and the SoftPlus function.\n$4$. **Construct A:** The PSD matrix $A$ is built as $A = L L^\\top + \\nu_0 I$.\n$5$. **Compute eigenvalues of A:** The eigenvalues of the symmetric matrix $A$ are calculated. The smallest eigenvalue, $\\lambda_{\\min}$, is identified. According to the theory, $\\lambda_{\\min}$ must be non-negative. We check if $\\lambda_{\\min} \\ge -\\delta$ with tolerance $\\delta = 10^{-12}$.\n$6$. **Compute dissipation:** The strain-rate matrix $S$ is converted to its Voigt vector form $s = [S_{xx}, \\sqrt{2}S_{xy}, S_{yy}]^\\top$. The dissipation density is then computed as $\\mathcal{D} = 2 s^\\top A s$.\n$7$. **Verify dissipation:** The computed dissipation $\\mathcal{D}$ must be non-negative. We check if $\\mathcal{D} \\ge -\\delta$.\n$8$. **Determine result:** The boolean result for the case, $\\text{B}_k$, is `True` if and only if both the minimum eigenvalue and the dissipation density are non-negative within the specified tolerance.\nThese steps are repeated for all four test cases.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by constructing a PSD eddy-viscosity matrix A\n    and verifying non-negative dissipation for several test cases.\n    \"\"\"\n\n    # --- Problem Constants and Definitions ---\n\n    # Part III: ML model parameters\n    W = np.array([\n        [0.5, -0.3, 0.1, 0.0, 0.2],\n        [-0.4, 0.6, -0.2, 0.1, -0.1],\n        [0.3, -0.5, 0.2, -0.1, 0.0],\n        [0.2, 0.1, -0.3, 0.5, -0.4],\n        [-0.1, 0.4, 0.2, -0.2, 0.3],\n        [0.6, -0.2, 0.0, 0.1, -0.3]\n    ])\n    b = np.array([-1.0, -1.5, -2.0, 0.0, 0.5, -0.3])\n\n    gamma = np.array([0.2, -0.1, 0.4, 0.0, 0.3])\n    gamma_0 = -0.2\n\n    epsilon = 1e-9\n    delta = 1e-12\n\n    # Part II: Smooth nonlinearity\n    def softplus(a):\n        \"\"\"Computes the softplus function log(1 + exp(a)).\"\"\"\n        # Clip 'a' to avoid overflow in exp(a) for large positive values.\n        # For large a, softplus(a) approaches a.\n        # For large negative a, softplus(a) approaches 0.\n        clipped_a = np.clip(a, -np.inf, 50)\n        return np.log(1.0 + np.exp(clipped_a))\n\n    # --- Test Cases ---\n    test_cases = [\n        {\n            \"x\": np.array([0.8, 0.1, 0.5, 0.0, 0.2]),\n            \"S\": np.array([[0.01, 0.02], [0.02, -0.01]])\n        },\n        {\n            \"x\": np.array([0.0, 0.0, 0.0, 0.0, 0.0]),\n            \"S\": np.array([[0.0, 0.0], [0.0, 0.0]])\n        },\n        {\n            \"x\": np.array([2.0, -1.0, 3.0, 0.5, -0.5]),\n            \"S\": np.array([[0.5, -0.8], [-0.8, 0.3]])\n        },\n        {\n            \"x\": np.array([-10.0, -10.0, -10.0, -10.0, -10.0]),\n            \"S\": np.array([[0.1, -0.05], [-0.05, 0.2]])\n        }\n    ]\n\n    results = []\n\n    for case in test_cases:\n        x = case[\"x\"]\n        S = case[\"S\"]\n\n        # 1. Compute ML outputs (z and nu_0)\n        z = W @ x + b\n        nu_0 = softplus(gamma.T @ x + gamma_0)\n\n        # 2. Construct the lower-triangular matrix L\n        L = np.zeros((3, 3))\n        # Diagonals with softplus and epsilon\n        L[0, 0] = softplus(z[0]) + epsilon\n        L[1, 1] = softplus(z[1]) + epsilon\n        L[2, 2] = softplus(z[2]) + epsilon\n        # Off-diagonals\n        L[1, 0] = z[3]  # l_21\n        L[2, 0] = z[4]  # l_31\n        L[2, 1] = z[5]  # l_32\n\n        # 3. Construct the PSD matrix A\n        A = L @ L.T + nu_0 * np.eye(3)\n\n        # 4. Check if A is PSD by computing its smallest eigenvalue\n        # Use eigvalsh for symmetric matrices for efficiency and numerical stability\n        eigenvalues = np.linalg.eigvalsh(A)\n        min_eigenvalue = np.min(eigenvalues)\n        is_psd = min_eigenvalue >= -delta\n\n        # 5. Form strain-rate vector s\n        S_xx, S_xy, S_yy = S[0, 0], S[0, 1], S[1, 1]\n        s = np.array([S_xx, np.sqrt(2) * S_xy, S_yy])\n\n        # 6. Compute dissipation density D = 2 * s^T * A * s\n        dissipation = 2 * s.T @ A @ s\n        is_dissipation_nonnegative = dissipation >= -delta\n        \n        # 7. Final check for the case\n        results.append(is_psd and is_dissipation_nonnegative)\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "A key motivation for hybrid modeling is accelerating complex physical simulations, but this speedup often comes at the cost of some accuracy. This practice tackles the crucial task of evaluation, where you will quantify the performance of an ML surrogate for a radiative transfer model. By calculating metrics for computational speedup, flux error, and an overall trade-off score, you will learn how to make principled assessments of a hybrid model's practical value .",
            "id": "4052754",
            "problem": "Consider a hybrid physics–machine learning modeling scenario for radiative transfer in numerical weather prediction and climate modeling. A Line-By-Line (LBL) solver computes spectrally resolved longwave upward flux at the top of the atmosphere by numerically integrating the Schwarzschild equation,\n$$\n\\frac{dI_{\\nu}}{ds} = -\\kappa_{\\nu} I_{\\nu} + \\kappa_{\\nu} B_{\\nu}(T),\n$$\nwhere $I_{\\nu}$ is the monochromatic intensity, $s$ is path length, $\\kappa_{\\nu}$ is the absorption coefficient, and $B_{\\nu}(T)$ is the Planck function at temperature $T$. The spectrally integrated upward flux $F$ is obtained by angular and spectral integration,\n$$\nF = \\int_{0}^{\\infty} \\int_{4\\pi} I_{\\nu} \\cos \\theta \\, d\\Omega \\, d\\nu,\n$$\nwhich in practice is approximated by a finite set of spectral bands with quadrature weights. A machine learning surrogate approximates the band-integrated fluxes from the atmospheric state. To quantitatively evaluate the reduction in computational cost and the accuracy trade-off, define the following metrics based on a band decomposition with weights:\n1. A quadrature-based approximation of the spectrally integrated flux uses bands indexed by $j = 1,2,\\dots,N$ with band fluxes $F_j$ and nonnegative weights $w_j$ satisfying $\\sum_{j=1}^{N} w_j = 1$, which approximate the spectral integral under a weighting related to the Planck function.\n2. The weighted $L^1$ relative flux error between the machine learning surrogate (denoted by $F^{\\mathrm{ML}}_j$) and the Line-By-Line baseline (denoted by $F^{\\mathrm{LBL}}_j$) is defined as\n$$\n\\epsilon = \\frac{\\sum_{j=1}^{N} w_j \\left| F^{\\mathrm{ML}}_j - F^{\\mathrm{LBL}}_j \\right|}{\\delta + \\sum_{j=1}^{N} w_j \\left| F^{\\mathrm{LBL}}_j \\right|},\n$$\nwhere $\\delta$ is a small regularization constant to avoid division by zero. Use $\\delta = 10^{-12}$ in units of watts per square meter to ensure numerical robustness. The error $\\epsilon$ is non-dimensional and must be reported as a decimal (not a percentage).\n3. The computational speedup is defined as\n$$\nS = \\frac{t_{\\mathrm{LBL}}}{t_{\\mathrm{ML}}},\n$$\nwhere $t_{\\mathrm{LBL}}$ and $t_{\\mathrm{ML}}$ are the wall-clock runtimes of the Line-By-Line solver and the machine learning surrogate, respectively, both expressed in seconds.\n4. A simple accuracy–cost trade-off score is defined as\n$$\nT = \\frac{S}{1 + \\epsilon},\n$$\nwhich increases with speedup and decreases with error, prioritizing accurate, faster surrogates.\n\nYour task is to implement a program that, for each test case listed below, computes the triple $(S, \\epsilon, T)$ using the definitions above. All fluxes must be treated in watts per square meter, all times in seconds, and angles do not appear explicitly in this setup. The program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered by concatenating $(S, \\epsilon, T)$ for each test case in the given order.\n\nTest Suite:\n- Case $1$ (typical clear-sky longwave):\n  - Number of bands: $N = 5$.\n  - Weights: $w = [0.25, 0.20, 0.18, 0.22, 0.15]$.\n  - LBL band fluxes: $F^{\\mathrm{LBL}} = [50, 60, 55, 65, 30]$ (in watts per square meter).\n  - ML band fluxes: $F^{\\mathrm{ML}} = [49, 61, 54, 66, 31]$ (in watts per square meter).\n  - Runtimes: $t_{\\mathrm{LBL}} = 2.8$ (in seconds), $t_{\\mathrm{ML}} = 0.07$ (in seconds).\n- Case $2$ (surrogate faster but less accurate):\n  - Number of bands: $N = 5$.\n  - Weights: $w = [0.25, 0.20, 0.18, 0.22, 0.15]$.\n  - LBL band fluxes: $F^{\\mathrm{LBL}} = [52, 58, 57, 63, 30]$ (in watts per square meter).\n  - ML band fluxes: $F^{\\mathrm{ML}} = [57, 53, 62, 58, 35]$ (in watts per square meter).\n  - Runtimes: $t_{\\mathrm{LBL}} = 2.8$ (in seconds), $t_{\\mathrm{ML}} = 0.02$ (in seconds).\n- Case $3$ (perfect surrogate accuracy):\n  - Number of bands: $N = 5$.\n  - Weights: $w = [0.25, 0.20, 0.18, 0.22, 0.15]$.\n  - LBL band fluxes: $F^{\\mathrm{LBL}} = [48, 62, 56, 64, 30]$ (in watts per square meter).\n  - ML band fluxes: $F^{\\mathrm{ML}} = [48, 62, 56, 64, 30]$ (in watts per square meter).\n  - Runtimes: $t_{\\mathrm{LBL}} = 3.0$ (in seconds), $t_{\\mathrm{ML}} = 0.06$ (in seconds).\n\nAnswer specification:\n- Compute $(S, \\epsilon, T)$ for each case using the formulas above with $\\delta = 10^{-12}$ (in watts per square meter).\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order $[S_1, \\epsilon_1, T_1, S_2, \\epsilon_2, T_2, S_3, \\epsilon_3, T_3]$, where subscript indexes the test case number.",
            "solution": "The problem statement is scientifically grounded, well-posed, and provides all necessary information for a unique solution. The context is a standard application of machine learning surrogates in computational physics, specifically for atmospheric radiative transfer. The defined metrics for speedup, error, and trade-off are coherent and mathematically sound. No inconsistencies or ambiguities were found. The problem is therefore deemed valid and we may proceed to the solution.\n\nThe objective is to compute three performance metrics for a machine learning (ML) surrogate of a Line-By-Line (LBL) radiative transfer model. These metrics are the computational speedup ($S$), a weighted relative flux error ($\\epsilon$), and an accuracy-cost trade-off score ($T$). We are provided with three distinct test cases, each with specific data for runtimes and band-integrated fluxes. The calculations will be performed for each case sequentially.\n\nThe foundational formulas are:\n1. Computational Speedup, $S$: This metric quantifies the acceleration achieved by the ML surrogate relative to the LBL model. It is the ratio of their respective runtimes, $t_{\\mathrm{LBL}}$ and $t_{\\mathrm{ML}}$.\n$$\nS = \\frac{t_{\\mathrm{LBL}}}{t_{\\mathrm{ML}}}\n$$\n\n2. Weighted $L^1$ Relative Flux Error, $\\epsilon$: This metric measures the accuracy of the ML surrogate. It is the weighted sum of absolute differences in band fluxes, normalized by the weighted sum of the LBL reference fluxes. A small constant $\\delta = 10^{-12}$ ensures the denominator is non-zero.\n$$\n\\epsilon = \\frac{\\sum_{j=1}^{N} w_j \\left| F^{\\mathrm{ML}}_j - F^{\\mathrm{LBL}}_j \\right|}{\\delta + \\sum_{j=1}^{N} w_j \\left| F^{\\mathrm{LBL}}_j \\right|}\n$$\n\n3. Accuracy–Cost Trade-off Score, $T$: This score balances the speedup against the error. A higher score indicates a more desirable surrogate model.\n$$\nT = \\frac{S}{1 + \\epsilon}\n$$\nWe will now apply these formulas to each test case.\n\n**Case 1: Typical clear-sky longwave**\nThe provided data are:\n- Weights: $w = [0.25, 0.20, 0.18, 0.22, 0.15]$\n- LBL band fluxes: $F^{\\mathrm{LBL}} = [50, 60, 55, 65, 30]$ W/m$^2$\n- ML band fluxes: $F^{\\mathrm{ML}} = [49, 61, 54, 66, 31]$ W/m$^2$\n- Runtimes: $t_{\\mathrm{LBL}} = 2.8$ s, $t_{\\mathrm{ML}} = 0.07$ s\n\nFirst, we compute the speedup $S_1$:\n$$\nS_1 = \\frac{2.8}{0.07} = 40.0\n$$\nNext, we compute the error $\\epsilon_1$. The numerator is the weighted sum of absolute flux differences:\n$$\n\\sum_{j=1}^{5} w_j \\left| F^{\\mathrm{ML}}_j - F^{\\mathrm{LBL}}_j \\right| = 0.25|49-50| + 0.20|61-60| + 0.18|54-55| + 0.22|66-65| + 0.15|31-30|\n$$\n$$\n= 0.25(1) + 0.20(1) + 0.18(1) + 0.22(1) + 0.15(1) = 1.0\n$$\nThe denominator is the regularized weighted sum of LBL fluxes (which are all positive, so $|F_j^{\\mathrm{LBL}}| = F_j^{\\mathrm{LBL}}$):\n$$\n\\delta + \\sum_{j=1}^{5} w_j F^{\\mathrm{LBL}}_j = 10^{-12} + (0.25 \\cdot 50 + 0.20 \\cdot 60 + 0.18 \\cdot 55 + 0.22 \\cdot 65 + 0.15 \\cdot 30)\n$$\n$$\n= 10^{-12} + (12.5 + 12.0 + 9.9 + 14.3 + 4.5) = 10^{-12} + 53.2\n$$\nThe error $\\epsilon_1$ is the ratio:\n$$\n\\epsilon_1 = \\frac{1.0}{53.2 + 10^{-12}} \\approx 0.018796992481203006\n$$\nFinally, we compute the trade-off score $T_1$:\n$$\nT_1 = \\frac{S_1}{1 + \\epsilon_1} = \\frac{40.0}{1 + 0.018796992481203006} \\approx 39.26182329304142\n$$\n\n**Case 2: Surrogate faster but less accurate**\nThe provided data are:\n- $w = [0.25, 0.20, 0.18, 0.22, 0.15]$\n- $F^{\\mathrm{LBL}} = [52, 58, 57, 63, 30]$ W/m$^2$\n- $F^{\\mathrm{ML}} = [57, 53, 62, 58, 35]$ W/m$^2$\n- $t_{\\mathrm{LBL}} = 2.8$ s, $t_{\\mathrm{ML}} = 0.02$ s\n\nThe speedup $S_2$ is:\n$$\nS_2 = \\frac{2.8}{0.02} = 140.0\n$$\nThe numerator for $\\epsilon_2$:\n$$\n\\sum w_j |F^{\\mathrm{ML}}_j - F^{\\mathrm{LBL}}_j| = 0.25|57-52| + 0.20|53-58| + 0.18|62-57| + 0.22|58-63| + 0.15|35-30|\n$$\n$$\n= 0.25(5) + 0.20(5) + 0.18(5) + 0.22(5) + 0.15(5) = 5 \\sum w_j = 5(1) = 5.0\n$$\nThe denominator for $\\epsilon_2$:\n$$\n\\delta + \\sum w_j F^{\\mathrm{LBL}}_j = 10^{-12} + (0.25 \\cdot 52 + 0.20 \\cdot 58 + 0.18 \\cdot 57 + 0.22 \\cdot 63 + 0.15 \\cdot 30)\n$$\n$$\n= 10^{-12} + (13.0 + 11.6 + 10.26 + 13.86 + 4.5) = 10^{-12} + 53.22\n$$\nThe error $\\epsilon_2$ is:\n$$\n\\epsilon_2 = \\frac{5.0}{53.22 + 10^{-12}} \\approx 0.09394964300075159\n$$\nThe trade-off score $T_2$ is:\n$$\nT_2 = \\frac{S_2}{1 + \\epsilon_2} = \\frac{140.0}{1 + 0.09394964300075159} \\approx 127.97746031746032\n$$\n\n**Case 3: Perfect surrogate accuracy**\nThe provided data are:\n- $w = [0.25, 0.20, 0.18, 0.22, 0.15]$\n- $F^{\\mathrm{LBL}} = [48, 62, 56, 64, 30]$ W/m$^2$\n- $F^{\\mathrm{ML}} = [48, 62, 56, 64, 30]$ W/m$^2$\n- $t_{\\mathrm{LBL}} = 3.0$ s, $t_{\\mathrm{ML}} = 0.06$ s\n\nThe speedup $S_3$ is:\n$$\nS_3 = \\frac{3.0}{0.06} = 50.0\n$$\nFor the error $\\epsilon_3$, we observe that $F^{\\mathrm{ML}}_j = F^{\\mathrm{LBL}}_j$ for all bands $j$. Thus, the absolute difference $|F^{\\mathrm{ML}}_j - F^{\\mathrm{LBL}}_j|$ is $0$ for all $j$. The numerator of $\\epsilon_3$ is therefore $0$.\n$$\n\\sum w_j |F^{\\mathrm{ML}}_j - F^{\\mathrm{LBL}}_j| = 0\n$$\nThe denominator is non-zero, so the error $\\epsilon_3$ is exactly $0$:\n$$\n\\epsilon_3 = 0.0\n$$\nThe trade-off score $T_3$ is:\n$$\nT_3 = \\frac{S_3}{1 + \\epsilon_3} = \\frac{50.0}{1 + 0} = 50.0\n$$\nThese computed triples, $(S_1, \\epsilon_1, T_1)$, $(S_2, \\epsilon_2, T_2)$, and $(S_3, \\epsilon_3, T_3)$, will be concatenated to form the final output.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes performance metrics for a machine learning surrogate model\n    based on three test cases.\n    \"\"\"\n    # The regularization constant delta is defined as 10^-12 W/m^2.\n    delta = 1e-12\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"w\": np.array([0.25, 0.20, 0.18, 0.22, 0.15]),\n            \"f_lbl\": np.array([50, 60, 55, 65, 30]),\n            \"f_ml\": np.array([49, 61, 54, 66, 31]),\n            \"t_lbl\": 2.8,\n            \"t_ml\": 0.07,\n        },\n        {\n            \"w\": np.array([0.25, 0.20, 0.18, 0.22, 0.15]),\n            \"f_lbl\": np.array([52, 58, 57, 63, 30]),\n            \"f_ml\": np.array([57, 53, 62, 58, 35]),\n            \"t_lbl\": 2.8,\n            \"t_ml\": 0.02,\n        },\n        {\n            \"w\": np.array([0.25, 0.20, 0.18, 0.22, 0.15]),\n            \"f_lbl\": np.array([48, 62, 56, 64, 30]),\n            \"f_ml\": np.array([48, 62, 56, 64, 30]),\n            \"t_lbl\": 3.0,\n            \"t_ml\": 0.06,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        # Extract data for the current case.\n        w = case[\"w\"]\n        f_lbl = case[\"f_lbl\"]\n        f_ml = case[\"f_ml\"]\n        t_lbl = case[\"t_lbl\"]\n        t_ml = case[\"t_ml\"]\n\n        # 1. Compute computational speedup (S).\n        # S = t_LBL / t_ML\n        S = t_lbl / t_ml\n\n        # 2. Compute weighted L1 relative flux error (epsilon).\n        # epsilon = (sum(w * |F_ML - F_LBL|)) / (delta + sum(w * |F_LBL|))\n        # Note: Since fluxes are physical quantities (power), they are non-negative,\n        # so abs(F_LBL) is F_LBL.\n        numerator_eps = np.sum(w * np.abs(f_ml - f_lbl))\n        denominator_eps = delta + np.sum(w * np.abs(f_lbl))\n        epsilon = numerator_eps / denominator_eps\n\n        # 3. Compute accuracy-cost trade-off score (T).\n        # T = S / (1 + epsilon)\n        T = S / (1.0 + epsilon)\n\n        # Append the calculated triple (S, epsilon, T) to the results list.\n        results.extend([S, epsilon, T])\n\n    # Final print statement must produce a single line with comma-separated\n    # values enclosed in square brackets.\n    # The map(str, ...) function ensures all numbers are converted to strings\n    # for the join operation.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}