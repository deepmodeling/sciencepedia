{
    "hands_on_practices": [
        {
            "introduction": "在将深度学习模型应用于气候科学时，理解其内部工作机制是建立信任的第一步。此练习通过计算一个用于降水预报的卷积-循环混合神经网络的“时空感受野”，来实践一种基础性的模型解释方法。通过这个计算，您将学会如何从结构上确定模型的某个预测结果最多可能受到哪些时空范围内的输入数据的影响，这为后续更复杂的归因分析提供了物理和机制上的约束 。",
            "id": "4040909",
            "problem": "考虑一个在数值天气预报 (NWP) 和气候建模中的降水预报系统，该系统使用一个卷积神经网络 (CNN) 后接一个长短期记忆 (LSTM) 网络。卷积神经网络 (CNN) 和长短期记忆 (LSTM) 是机器学习中的标准架构；在这里，它们被用来处理时空降水场，以生成未来一个时间步的预报。在气候科学领域可解释性人工智能的更广阔背景下，时空感受野量化了哪些输入网格单元和过去的时间步理论上可以影响一个给定的预报输出，从而为解释模型归因和显著性图提供了机制基础。\n\n给定以下架构和假设：\n\n- 输入是在规则网格上的 $T$ 个小时降水场序列。在每个时间步，一个空间 CNN 处理该场，以生成由步幅决定的更粗分辨率下的特征。\n- 空间 CNN 由 $4$ 个二维卷积层组成，这些层具有方形核和各向同性的步幅，按顺序应用于每个输入帧：\n  - 层 $\\mathrm{S1}$：核大小 $k_{1} = 3$，步幅 $s_{1} = 1$。\n  - 层 $\\mathrm{S2}$：核大小 $k_{2} = 5$，步幅 $s_{2} = 2$。\n  - 层 $\\mathrm{S3}$：核大小 $k_{3} = 3$，步幅 $s_{3} = 2$。\n  - 层 $\\mathrm{S4}$：核大小 $k_{4} = 3$，步幅 $s_{4} = 1$。\n  - 没有使用空洞卷积，并使用标准零填充来保持对齐；当 $s_{\\ell} = 1$ 时，空间分辨率得以保留；当 $s_{\\ell} = 2$ 时，该层的分辨率在每个空间轴上被下采样 $2$ 倍。\n- 在 $\\mathrm{S4}$ 生成的最终特征图的每个空间位置上，一个 $1 \\times 1$ 的卷积将特征映射为一个标量，从而在每个位置上产生一个随时间变化的潜序列。\n- 时间模块是一个一维时间卷积，后跟一个 LSTM：\n  - 时间卷积 $\\mathrm{T1}$：核大小 $k_{t} = 5$，步幅 $s_{t} = 2$，在每个空间位置上沿时间维度独立应用，生成一个下采样的潜特征序列。\n  - LSTM 仅使用 $\\mathrm{T1}$ 的最近 $W = 4$ 个输出（一个固定的滑动窗口），并在 $\\mathrm{S4}$ 特征图的相应空间位置上发出下一个时间步的预报。\n- 关注 $\\mathrm{S4}$ 特征图中心空间位置的预报，以及相对于最近输入的前向一步时间。\n\n使用离散卷积和采样理论的第一性原理，将此 CNN-LSTM 架构的时空感受野定义为：在所述假设下，理论上可以影响所选预报输出的、跨越空间和过去时间步的输入网格单元集合。然后，根据各层的核大小和步幅，以闭合形式推导出沿两个空间轴（输入网格单元的宽度和高度）和时间轴（输入时间步数）的有效感受野大小，并计算它们对于上述架构的数值。\n\n以行矩阵的形式提供最终答案，列出有效感受野的空间宽度、空间高度和时间长度，单位为无量纲计数（网格单元和时间步）。无需四舍五入；报告精确的整数计数。以 $\\left[ \\text{宽度} \\;\\; \\text{高度} \\;\\; \\text{时间} \\right]$ 的形式表示最终答案。",
            "solution": "我们首先为给定架构形式化地定义时空感受野。对于在特定输出空间位置和前向一步时间生成的预报，感受野是其值能通过网络操作影响输出的输入空间网格单元和过去时间索引的集合。在没有跳跃连接的纯前馈卷积网络中，这个集合由核的支撑集以及各层位置采样和映射所用的步幅决定。在时间维度上，一个带步幅的一维卷积后接一个在固定窗口上的循环计算，意味着一个有限的时间支撑集，该支撑集由卷积的核、步幅以及 LSTM 窗口长度决定。我们分别推导空间和时间分量，然后将它们组合起来。\n\n空间感受野推导（每帧）：\n为清晰起见，我们采用标准的一维推导方法，然后通过各向同性将其扩展到二维。考虑一个离散卷积栈，其核大小为 $k_{\\ell}$，步幅为 $s_{\\ell}$，其中 $\\ell \\in \\{1,2,3,4\\}$ 索引从 $\\mathrm{S1}$ 到 $\\mathrm{S4}$ 的各层。定义跳跃 $J_{\\ell}$ 为第 $\\ell$ 层相邻感受野中心在输入坐标中的间距，定义感受野大小 $R_{\\ell}$ 为沿一个轴影响第 $\\ell$ 层单个单元的输入网格单元数量。基本条件是 $J_{0} = 1$ 和 $R_{0} = 1$，对应于输入像素自身的感受野和单位采样。\n\n对于一个核大小为 $k_{\\ell}$、步幅为 $s_{\\ell}$ 的卷积层，映射规则为\n$$\nJ_{\\ell} = J_{\\ell-1} \\, s_{\\ell},\n$$\n$$\nR_{\\ell} = R_{\\ell-1} + (k_{\\ell} - 1) \\, J_{\\ell-1}.\n$$\n这些规则源于以下事实：步幅 $s_{\\ell}$ 将感受野中心的间距相对于前一层扩大了 $s_{\\ell}$ 倍，而大小为 $k_{\\ell}$ 的核在前一层中覆盖了中心之外 $(k_{\\ell}-1)$ 个大小为 $J_{\\ell-1}$ 的步长。\n\n将这些递推关系应用于给定的空间层：\n- 层 $\\mathrm{S1}$ 的参数为 $k_{1} = 3$, $s_{1} = 1$。因此\n$$\nJ_{1} = J_{0} \\, s_{1} = 1 \\cdot 1 = 1, \\quad R_{1} = R_{0} + (k_{1}-1) J_{0} = 1 + (3-1)\\cdot 1 = 3.\n$$\n- 层 $\\mathrm{S2}$ 的参数为 $k_{2} = 5$, $s_{2} = 2$。因此\n$$\nJ_{2} = J_{1} \\, s_{2} = 1 \\cdot 2 = 2, \\quad R_{2} = R_{1} + (k_{2}-1) J_{1} = 3 + (5-1)\\cdot 1 = 7.\n$$\n- 层 $\\mathrm{S3}$ 的参数为 $k_{3} = 3$, $s_{3} = 2$。因此\n$$\nJ_{3} = J_{2} \\, s_{3} = 2 \\cdot 2 = 4, \\quad R_{3} = R_{2} + (k_{3}-1) J_{2} = 7 + (3-1)\\cdot 2 = 11.\n$$\n- 层 $\\mathrm{S4}$ 的参数为 $k_{4} = 3$, $s_{4} = 1$。因此\n$$\nJ_{4} = J_{3} \\, s_{4} = 4 \\cdot 1 = 4, \\quad R_{4} = R_{3} + (k_{4}-1) J_{3} = 11 + (3-1)\\cdot 4 = 19.\n$$\n\n最后的 $1 \\times 1$ 卷积不扩大感受野（$k=1$, $s=1$），因此它保持 $R_{4}$ 不变。因此，沿单个空间轴，感受野大小为 $R_{4} = 19$。由于核和步幅是各向同性和方形的，感受野在两个轴上是相同的，因此对于所考虑的 $\\mathrm{S4}$ 输出位置的单个单元，其空间感受野为 $19 \\times 19$ 个输入网格单元。\n\n时间感受野推导：\n对于时间维度，一维卷积 $\\mathrm{T1}$ 的核大小为 $k_{t} = 5$，步幅为 $s_{t} = 2$。$\\mathrm{T1}$ 的单个输出取决于 $k_{t}$ 个连续的输入时间步，根据填充约定居中；$\\mathrm{T1}$ 的相邻输出之间相隔 $s_{t}$ 个输入步长。LSTM 仅使用 $\\mathrm{T1}$ 的最近 $W = 4$ 个输出来生成预报。最近 $W$ 个 $\\mathrm{T1}$ 输出的支撑集的并集覆盖了一个连续的输入时间步块，其长度为\n$$\nR_{t} = k_{t} + (W - 1) \\, s_{t},\n$$\n因为在没有空洞卷积和固定步幅的情况下，每个后续的 $\\mathrm{T1}$ 输出都将覆盖范围在前一个输出的支撑集之外扩展了 $s_{t}$ 个新的输入步长。代入给定值可得\n$$\nR_{t} = 5 + (4 - 1) \\cdot 2 = 11.\n$$\n\n时空感受野总结：\n在所述架构和假设下，对于所选空间位置和前向一步时间的预报，其有效感受野在每个空间轴上跨越 $19$ 个输入网格单元，在时间轴上跨越 $11$ 个输入时间步。用可解释性人工智能的术语来说，这个理论支撑集定义了可以对所选预报的归因分数或影响函数做出贡献的最大时空邻域，从而为可解释性分析提供了原则性约束。\n\n因此，所要求的列出空间宽度、空间高度和时间长度的行矩阵是 $\\left[ 19 \\;\\; 19 \\;\\; 11 \\right]$。",
            "answer": "$$\\boxed{\\begin{pmatrix}19 & 19 & 11\\end{pmatrix}}$$"
        },
        {
            "introduction": "一个理想的归因方法应确保所有特征的重要性之和等于模型的总输出变化，这被称为“完备性”公理。本练习将指导您从多元微积分的基本定理出发，推导出一个满足完备性的归因方法，并以一个具体的降水预报模型为例进行数值验证。完成此练习有助于您深刻理解如积分梯度 (Integrated Gradients) 等先进可解释性方法背后的数学原理，并掌握评估归因方法严谨性的核心标准 。",
            "id": "4040883",
            "problem": "您正在研究一种用于数值天气预报和气候模拟领域中可微降水预测器的、基于归因的解释方法。考虑一个预测器 $f:\\mathbb{R}^d \\to \\mathbb{R}$、一个输入 $\\mathbf{x} \\in \\mathbb{R}^d$ 以及一个代表长期平均归一化特征的气候学基线 $\\mathbf{x}' \\in \\mathbb{R}^d$。您的任务是在此设定下，为特征归因提出并验证一个完整性属性。整个任务必须从第一性原理出发，基于链式法则、微积分基本定理以及多变量微积分中关于路径积分的成熟理论。\n\n请提出一个数学上精确的完整性属性：一组特征归因 $\\{\\phi_i\\}_{i=1}^d$ 相对于基线 $\\mathbf{x}'$ 是完整的，当且仅当\n$$\n\\sum_{i=1}^d \\phi_i \\;=\\; f(\\mathbf{x}) - f(\\mathbf{x}') \\, .\n$$\n请使用从 $\\mathbf{x}'$ 到 $\\mathbf{x}$ 的直线路径，推导出一个通过构造即能满足此属性的归因方案。然后实现该方案，并数值验证其完整性残差很小。\n\n将降水预测器 $f$ 定义如下。设特征按顺序排列为 $\\mathbf{x} = (m,u,s,t,o,c)$，分别代表归一化水汽柱含量 $m$、归一化上升速度振幅 $u$、归一化垂直风切变 $s$、归一化温度异常 $t$、归一化地形抬升因子 $o$ 和归一化对流有效位能 $c$。设\n$$\nz(\\mathbf{x}) \\;=\\; a_0 + a_1 m + a_2 u + a_3 s + a_4 t + a_5 o + a_6 c + a_7 m c + a_8 u o - a_9 s t \\, ,\n$$\n系数为\n$$\na_0=-1.2,\\; a_1=2.0,\\; a_2=1.5,\\; a_3=-0.7,\\; a_4=0.3,\\; a_5=1.2,\\; a_6=1.8,\\; a_7=2.5,\\; a_8=1.0,\\; a_9=0.6 \\, .\n$$\n使用 softplus 连接函数：\n$$\n\\mathrm{softplus}(z) \\;=\\; \\log\\!\\big(1+e^{z}\\big) \\, ,\n$$\n并定义\n$$\nf(\\mathbf{x}) \\;=\\; \\mathrm{softplus}\\!\\left(z(\\mathbf{x})\\right) \\, .\n$$\n所有输入都是无量纲的，并且位于单位超立方体内，因此答案中无需物理单位。\n\n使用直线路径 $\\gamma(\\alpha) = \\mathbf{x}' + \\alpha(\\mathbf{x}-\\mathbf{x}')$，其中 $\\alpha \\in [0,1]$。通过偏导数的路径积分来定义特征 $i$ 的归因：\n$$\n\\phi_i(\\mathbf{x};\\mathbf{x}') \\;=\\; (x_i - x_i') \\int_{0}^{1} \\frac{\\partial f\\big(\\gamma(\\alpha)\\big)}{\\partial x_i} \\, d\\alpha \\, .\n$$\n使用包含 $M$ 个等长子区间的中点黎曼和来数值近似该积分：\n$$\n\\phi_i(\\mathbf{x};\\mathbf{x}') \\;\\approx\\; (x_i - x_i') \\cdot \\frac{1}{M}\\sum_{k=1}^{M} \\left.\\frac{\\partial f(\\mathbf{x})}{\\partial x_i}\\right|_{\\mathbf{x}=\\gamma\\!\\left(\\frac{k-\\tfrac{1}{2}}{M}\\right)} \\, .\n$$\n对于下方的每个测试用例，计算绝对完整性残差\n$$\n\\varepsilon \\;=\\; \\left| \\sum_{i=1}^{6} \\phi_i \\;-\\; \\big(f(\\mathbf{x}) - f(\\mathbf{x}')\\big) \\right|.\n$$\n\n使用以下基线和测试套件。基线为\n$$\n\\mathbf{x}' \\;=\\; (0.6,\\, 0.5,\\, 0.4,\\, 0.5,\\, 0.3,\\, 0.4) \\, .\n$$\n测试用例是元组 $(\\mathbf{x}, M)$：\n- 用例 A: $\\mathbf{x}_1 = (0.85,\\, 0.7,\\, 0.2,\\, 0.55,\\, 0.8,\\, 0.9)$，$M=200$。\n- 用例 B: $\\mathbf{x}_2 = (0.6,\\, 0.5,\\, 0.4,\\, 0.5,\\, 0.3,\\, 0.4)$，$M=50$。\n- 用例 C: $\\mathbf{x}_3 = (0.1,\\, 0.2,\\, 0.9,\\, 0.3,\\, 0.1,\\, 0.05)$，$M=500$。\n- 用例 D: $\\mathbf{x}_4 = (0.95,\\, 0.9,\\, 0.1,\\, 0.6,\\, 0.95,\\, 0.95)$，$M=50$。\n- 用例 E: $\\mathbf{x}_5 = (0.6,\\, 0.52,\\, 0.39,\\, 0.49,\\, 0.31,\\, 0.41)$，$M=10$。\n- 用例 F: $\\mathbf{x}_6 = (0.9,\\, 0.1,\\, 0.9,\\, 0.9,\\, 0.0,\\, 0.9)$，$M=20$。\n\n您的程序必须：\n- 通过链式法则实现 $f$ 及其梯度，使用 $\\sigma(z) = 1/(1+e^{-z})$ 作为 $\\frac{d}{dz}\\mathrm{softplus}(z)=\\sigma(z)$。\n- 对每个测试用例，使用指定数量的 $M$ 通过中点黎曼和计算 $\\phi_i$。\n- 返回这六个用例的绝对完整性残差 $\\varepsilon$ 列表。\n\n最终输出格式：您的程序应生成单行输出，其中包含六个残差值，格式为方括号内以逗号分隔的列表，每个数字四舍五入到十位小数（例如，$[0.0000000000,0.1234567890,\\dots]$）。不应打印任何其他文本。",
            "solution": "用户提供了一个在气候模拟的可解释性人工智能领域中，陈述清晰且具有科学依据的问题。任务是首先为一种特定特征归因方法的完整性属性提供理论证明，然后针对给定的模型和一组测试用例对该属性进行数值验证。\n\n### 完整性属性的理论推导\n\n目标是证明所提出的归因方案通过构造即能满足完整性属性。该属性指出，每个特征的归因值 $\\{\\phi_i\\}_{i=1}^d$ 之和必须等于预测器在输入 $\\mathbf{x}$ 和基线 $\\mathbf{x}'$ 之间输出的总变化量：\n$$\n\\sum_{i=1}^d \\phi_i = f(\\mathbf{x}) - f(\\mathbf{x}')\n$$\n此推导依赖于微积分基本定理和多变量链式法则，并将其应用于一条指定的路径。\n\n设 $f: \\mathbb{R}^d \\to \\mathbb{R}$ 是一个连续可微函数。问题定义了一条从基线 $\\mathbf{x}'$ 到输入 $\\mathbf{x}$ 的直线路径 $\\gamma(\\alpha)$，由参数 $\\alpha \\in [0, 1]$ 参数化：\n$$\n\\gamma(\\alpha) = \\mathbf{x}' + \\alpha(\\mathbf{x} - \\mathbf{x}')\n$$\n观察可知 $\\gamma(0) = \\mathbf{x}'$ 且 $\\gamma(1) = \\mathbf{x}$。\n\n我们定义一个新的单变量函数 $g(\\alpha) = f(\\gamma(\\alpha))$，它表示预测器 $f$ 沿该路径的值。根据微积分基本定理，$g$ 从 $\\alpha=0$ 到 $\\alpha=1$ 的总变化量是其导数的积分：\n$$\nf(\\mathbf{x}) - f(\\mathbf{x}') = g(1) - g(0) = \\int_0^1 \\frac{dg}{d\\alpha} \\, d\\alpha\n$$\n为了计算导数 $\\frac{dg}{d\\alpha}$，我们应用多变量链式法则。函数 $g(\\alpha)$ 是一个复合函数 $f(\\gamma_1(\\alpha), \\gamma_2(\\alpha), \\dots, \\gamma_d(\\alpha))$，其中 $\\gamma_i(\\alpha) = x_i' + \\alpha(x_i - x_i')$ 是路径向量的第 $i$ 个分量。每个分量相对于 $\\alpha$ 的导数是：\n$$\n\\frac{d\\gamma_i}{d\\alpha} = x_i - x_i'\n$$\n链式法则给出：\n$$\n\\frac{dg}{d\\alpha} = \\sum_{i=1}^d \\frac{\\partial f}{\\partial x_i}\\bigg|_{\\mathbf{x}=\\gamma(\\alpha)} \\cdot \\frac{d\\gamma_i}{d\\alpha} = \\sum_{i=1}^d \\frac{\\partial f}{\\partial x_i}\\bigg|_{\\mathbf{x}=\\gamma(\\alpha)} \\cdot (x_i - x_i')\n$$\n将此导数表达式代入积分，我们得到：\n$$\nf(\\mathbf{x}) - f(\\mathbf{x}') = \\int_0^1 \\left( \\sum_{i=1}^d \\frac{\\partial f}{\\partial x_i}\\bigg|_{\\mathbf{x}=\\gamma(\\alpha)} \\cdot (x_i - x_i') \\right) \\, d\\alpha\n$$\n由于求和是有限的，我们可以交换积分和求和的顺序。项 $(x_i - x_i')$ 相对于 $\\alpha$ 是一个常数，可以移到积分符号之外：\n$$\nf(\\mathbf{x}) - f(\\mathbf{x}') = \\sum_{i=1}^d (x_i - x_i') \\int_0^1 \\frac{\\partial f}{\\partial x_i}\\bigg|_{\\mathbf{x}=\\gamma(\\alpha)} \\, d\\alpha\n$$\n右侧的表达式正是问题陈述中定义的归因值 $\\phi_i$ 之和：\n$$\n\\phi_i(\\mathbf{x};\\mathbf{x}') = (x_i - x_i') \\int_{0}^{1} \\frac{\\partial f\\big(\\gamma(\\alpha)\\big)}{\\partial x_i} \\, d\\alpha\n$$\n因此，我们已经正式证明了 $\\sum_{i=1}^d \\phi_i = f(\\mathbf{x}) - f(\\mathbf{x}')$。这种被称为积分梯度 (Integrated Gradients) 的特定归因方法是依构造即完整的。\n\n### 数值验证过程\n\n任务的第二部分是数值验证此属性。$\\phi_i$ 定义中的积分使用包含 $M$ 个等长子区间的中点黎曼和进行近似。绝对完整性残差 $\\varepsilon$ 用于衡量此数值近似的误差。\n\n$$\n\\varepsilon = \\left| \\sum_{i=1}^{6} \\phi_i^{\\text{approx}} - \\big(f(\\mathbf{x}) - f(\\mathbf{x}')\\big) \\right|\n$$\n其中 $\\phi_i^{\\text{approx}}$ 是特征 $i$ 的数值计算归因值。\n\n预测器函数 $f(\\mathbf{x})$ 定义为 $f(\\mathbf{x}) = \\mathrm{softplus}(z(\\mathbf{x}))$，其中 $\\mathbf{x} = (x_1, \\dots, x_6) = (m, u, s, t, o, c)$。中间函数 $z(\\mathbf{x})$ 是：\n$$\nz(\\mathbf{x}) = a_0 + a_1 x_1 + a_2 x_2 + a_3 x_3 + a_4 x_4 + a_5 x_5 + a_6 x_6 + a_7 x_1 x_6 + a_8 x_2 x_5 - a_9 x_3 x_4\n$$\n为了计算归因值，我们首先需要 $f$ 的梯度。使用链式法则，$\\frac{\\partial f}{\\partial x_i} = \\frac{d\\mathrm{softplus}(z)}{dz} \\cdot \\frac{\\partial z}{\\partial x_i}$。问题中给出 $\\frac{d\\mathrm{softplus}(z)}{dz} = \\sigma(z) = 1/(1+e^{-z})$。$z(\\mathbf{x})$ 的偏导数是：\n$$\n\\frac{\\partial z}{\\partial x_1} = a_1 + a_7 x_6, \\quad \\frac{\\partial z}{\\partial x_2} = a_2 + a_8 x_5, \\quad \\frac{\\partial z}{\\partial x_3} = a_3 - a_9 x_4 $$\n$$\n\\frac{\\partial z}{\\partial x_4} = a_4 - a_9 x_3, \\quad \\frac{\\partial z}{\\partial x_5} = a_5 + a_8 x_2, \\quad \\frac{\\partial z}{\\partial x_6} = a_6 + a_7 x_1\n$$\n$\\phi_i$ 的数值近似则为：\n$$\n\\phi_i^{\\text{approx}} = (x_i - x_i') \\cdot \\frac{1}{M}\\sum_{k=1}^{M} \\left. \\left( \\sigma(z(\\mathbf{x})) \\frac{\\partial z}{\\partial x_i} \\right) \\right|_{\\mathbf{x}=\\gamma(\\alpha_k)} \\quad \\text{其中 } \\alpha_k = \\frac{k - 1/2}{M}\n$$\n实现中将计算这些近似归因值的总和 $\\sum_{i=1}^6 \\phi_i^{\\text{approx}}$，并将其与精确差值 $f(\\mathbf{x}) - f(\\mathbf{x}')$ 进行比较，以求得每个测试用例的残差 $\\varepsilon$。为了提高效率，采用了矢量化实现，同时评估积分路径上的所有点。残差 $\\varepsilon$ 预计会很小，并随着 $M$ 的增加而减小，这反映了数值积分方案的收敛性。对于 $\\mathbf{x} = \\mathbf{x}'$ 的特殊情况，路径长度为零，因此归因总和和总变化量都恰好为 $0$，从而得到残差为 $0$。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of numerically verifying the completeness property\n    for an attribution method in a climate science context.\n    \"\"\"\n\n    # Define coefficients and baseline from the problem statement\n    a = {\n        'a0': -1.2, 'a1': 2.0, 'a2': 1.5, 'a3': -0.7, 'a4': 0.3, \n        'a5': 1.2, 'a6': 1.8, 'a7': 2.5, 'a8': 1.0, 'a9': 0.6\n    }\n    x_prime = np.array([0.6, 0.5, 0.4, 0.5, 0.3, 0.4])\n\n    # Define the test cases from the problem statement.\n    # Each case is a tuple (x_vector, M_steps).\n    test_cases = [\n        (np.array([0.85, 0.7, 0.2, 0.55, 0.8, 0.9]), 200),  # Case A\n        (np.array([0.6, 0.5, 0.4, 0.5, 0.3, 0.4]), 50),     # Case B\n        (np.array([0.1, 0.2, 0.9, 0.3, 0.1, 0.05]), 500),  # Case C\n        (np.array([0.95, 0.9, 0.1, 0.6, 0.95, 0.95]), 50),   # Case D\n        (np.array([0.6, 0.52, 0.39, 0.49, 0.31, 0.41]), 10), # Case E\n        (np.array([0.9, 0.1, 0.9, 0.9, 0.0, 0.9]), 20),     # Case F\n    ]\n\n    def z_func(x_vec):\n        \"\"\"Computes the intermediate function z(x), vectorized.\"\"\"\n        # x_vec can be a 1D array (single point) or 2D array (multiple points)\n        single_dim = x_vec.ndim == 1\n        if single_dim:\n            x_vec = x_vec[np.newaxis, :]\n\n        m, u, s, t, o, c = x_vec[:, 0], x_vec[:, 1], x_vec[:, 2], x_vec[:, 3], x_vec[:, 4], x_vec[:, 5]\n        \n        val = (a['a0'] + a['a1'] * m + a['a2'] * u + a['a3'] * s + a['a4'] * t + \n               a['a5'] * o + a['a6'] * c + a['a7'] * m * c + \n               a['a8'] * u * o - a['a9'] * s * t)\n        \n        return val[0] if single_dim else val\n\n    def softplus(z_val):\n        \"\"\"Numerically stable softplus function, vectorized.\"\"\"\n        # For large z, softplus(z) approx z. This avoids overflow in np.exp(z).\n        return np.where(z_val > 30, z_val, np.log1p(np.exp(z_val)))\n\n    def f_func(x_vec):\n        \"\"\"Computes the predictor function f(x), vectorized.\"\"\"\n        return softplus(z_func(x_vec))\n\n    def sigma(z_val):\n        \"\"\"Numerically stable sigmoid function, vectorized.\"\"\"\n        # For z >= 0, use 1/(1+e^-z)\n        # For z  0, use e^z/(1+e^z) to avoid overflow\n        return np.where(\n            z_val >= 0,\n            1.0 / (1.0 + np.exp(-z_val)),\n            np.exp(z_val) / (1.0 + np.exp(z_val))\n        )\n\n    def grad_z(x_vec):\n        \"\"\"Computes the gradient of z(x), vectorized.\"\"\"\n        single_dim = x_vec.ndim == 1\n        if single_dim:\n            x_vec = x_vec[np.newaxis, :]\n\n        n_pts = x_vec.shape[0]\n        grad = np.zeros((n_pts, 6))\n        \n        m, u, s, t, o, c = x_vec[:, 0], x_vec[:, 1], x_vec[:, 2], x_vec[:, 3], x_vec[:, 4], x_vec[:, 5]\n\n        grad[:, 0] = a['a1'] + a['a7'] * c  # d/dm\n        grad[:, 1] = a['a2'] + a['a8'] * o  # d/du\n        grad[:, 2] = a['a3'] - a['a9'] * t  # d/ds\n        grad[:, 3] = a['a4'] - a['a9'] * s  # d/dt\n        grad[:, 4] = a['a5'] + a['a8'] * u  # d/do\n        grad[:, 5] = a['a6'] + a['a7'] * m  # d/dc\n\n        return grad[0] if single_dim else grad\n\n    def grad_f(x_vec):\n        \"\"\"Computes the gradient of f(x), vectorized.\"\"\"\n        z_val = z_func(x_vec)\n        sigma_val = sigma(z_val)\n        grad_z_val = grad_z(x_vec)\n        \n        # Broadcasting sigma_val for element-wise multiplication\n        if x_vec.ndim > 1:\n            sigma_val = sigma_val[:, np.newaxis]\n            \n        return sigma_val * grad_z_val\n\n    def calculate_residual(x_vec, x_prime_vec, M):\n        \"\"\"\n        Calculates the absolute completeness residual for a given case.\n        \"\"\"\n        # Handle the trivial case where x equals the baseline\n        if np.array_equal(x_vec, x_prime_vec):\n            return 0.0\n\n        delta_x = x_vec - x_prime_vec\n        \n        # Generate midpoints for the Riemann sum\n        alphas = (np.arange(1, M + 1) - 0.5) / M\n        \n        # Create all points along the path in a vectorized manner\n        # path_points will be an (M, 6) array\n        path_points = x_prime_vec + alphas[:, np.newaxis] * delta_x\n        \n        # Compute gradients at all path points\n        all_grads = grad_f(path_points)\n        \n        # Average the gradients over the path\n        avg_grads = np.mean(all_grads, axis=0)\n        \n        # The sum of attributions is the dot product of delta_x and avg_grads\n        total_attribution = np.dot(delta_x, avg_grads)\n        \n        # The exact change in the function output\n        total_change = f_func(x_vec) - f_func(x_prime_vec)\n        \n        # The absolute completeness residual\n        residual = np.abs(total_attribution - total_change)\n        \n        return residual\n\n    results = []\n    for x, M in test_cases:\n        residual = calculate_residual(x, x_prime, M)\n        results.append(f\"{residual:.10f}\")\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在复杂的气候系统中，相关性不等于因果性，这为可解释AI带来了巨大挑战。本练习通过一个模拟的结构因果模型 (Structural Causal Model)，让您亲手构建一个因“混杂偏倚”而导致朴素归因方法失效的场景。您将通过编程实践“后门调整”这一因果推断核心技术，来消除混杂因素的干扰，从而区分出虚假的统计关联和真实的物理驱动因素，这是将可解释AI从“模型在看什么”提升到“物理世界的驱动力是什么”的关键一步 。",
            "id": "4040934",
            "problem": "考虑一个合成气候模拟情景，其中海表温度（$\\mathrm{SST}$）、500 hPa 对流层中层位势高度（$\\mathrm{Z500}$）的标准化异常以及一个目标标量 $y$（例如，区域降水异常）之间的关系由一个线性结构因果模型（SCM）控制。令 $u$ 表示一个未观测到的季节-行星尺度状态，它是 $\\mathrm{SST}$ 和 $\\mathrm{Z500}$ 的共同原因。数据生成过程由以下方程指定：\n$$\nu \\sim \\mathcal{N}(0,1),\n$$\n$$\n\\mathrm{SST} = a\\,u + \\varepsilon_s,\\quad \\varepsilon_s \\sim \\mathcal{N}(0,\\sigma_s^2),\n$$\n$$\n\\mathrm{Z500} = b\\,u + \\varepsilon_z,\\quad \\varepsilon_z \\sim \\mathcal{N}(0,\\sigma_z^2),\n$$\n$$\ny = d\\,\\mathrm{Z500} + \\varepsilon_y,\\quad \\varepsilon_y \\sim \\mathcal{N}(0,\\sigma_y^2).\n$$\n所有变量都是标准化异常，因此是无量纲的。$\\mathrm{SST}$ 对 $y$ 没有直接的因果效应；$\\mathrm{SST}$ 与 $y$ 之间的任何统计关联都纯粹是由 $u$ 的混淆作用以及由此产生的 $\\mathrm{SST}$ 与 $\\mathrm{Z500}$ 之间的相关性引起的。在此设置中，线性预测模型的基于梯度的归因是预测相对于输入的偏导数，其值等于模型对该输入的系数。\n\n您必须构建一个完整、可运行的程序，该程序：\n- 为下面的每个测试用例，使用固定的随机种子从 SCM 生成合成数据集，每个用例独立生成。\n- 使用岭回归（带 $\\ell_2$ 惩罚的平方损失）训练两个线性预测器。岭回归系数向量 $\\beta$ 必须计算为 $\\beta = \\left(X^\\top X + \\lambda I\\right)^{-1} X^\\top y$，其中 $X$ 是列经过均值中心化后的特征矩阵，$y$ 是经过均值中心化的。\n- 通过训练一个仅使用 $\\mathrm{SST}$ 作为 $y$ 预测变量的岭回归模型，计算一个“朴素”的基于梯度的归因，从而得出一个单一系数作为 $\\mathrm{SST}$ 的基于梯度的归因。\n- 通过训练一个使用调整集 $\\{\\mathrm{SST},\\mathrm{Z500},u\\}$ 的岭回归模型，计算一个“调整后”的因果归因，从而得出 $\\mathrm{SST}$ 和 $\\mathrm{Z500}$ 的系数。在上述 SCM 中，$\\{u\\}$ 阻断了混淆 $\\mathrm{SST}$ 和 $y$ 的后门路径，而包含 $\\mathrm{Z500}$ 则可以识别出 $y$ 的直接物理驱动因素。\n- 为每个测试用例按顺序报告以下列表：朴素 $\\mathrm{SST}$ 系数、调整后 $\\mathrm{SST}$ 系数、调整后 $\\mathrm{Z500}$ 系数、一个指示调整后 $\\mathrm{SST}$ 归因的绝对值是否小于或等于朴素归因绝对值的布尔值，以及一个指示调整后 $\\mathrm{Z500}$ 系数是否在真实因果效应 $d$ 的指定容差范围内的布尔值。\n\n推理的基本依据：\n- 线性结构因果模型 (SCM) 的定义和性质以及后门调整的概念。\n- 岭回归的定义及其闭式解。\n- 线性模型的基于梯度的归因等于模型对每个输入的系数。\n- 由共同原因引起的混淆会导致虚假关联，从而使朴素归因产生偏差，而在一个有效的调整集上进行条件化可以恢复因果解释这一概念。\n\n测试套件：\n- 案例 A（强混淆，充分采样）：$N=10000$，$a=1.5$，$b=1.0$，$d=2.0$，$\\sigma_s=0.3$，$\\sigma_z=0.3$，$\\sigma_y=0.5$，$\\lambda=1.0$，$d$ 接近度的容差 $=0.15$，种子 $=42$。\n- 案例 B（无混淆，充分采样）：$N=10000$，$a=0.0$，$b=1.0$，$d=2.0$，$\\sigma_s=0.3$，$\\sigma_z=0.3$，$\\sigma_y=0.5$，$\\lambda=1.0$，容差 $=0.15$，种子 $=43$。\n- 案例 C（强混淆，小样本且有噪声）：$N=100$，$a=1.5$，$b=1.0$，$d=2.0$，$\\sigma_s=1.0$，$\\sigma_z=1.0$，$\\sigma_y=1.0$，$\\lambda=2.0$，容差 $=0.5$，种子 $=44$。\n\n要求的最终输出格式：\n您的程序应生成单行输出，其中包含三个测试用例的结果，格式为逗号分隔的列表的列表，严格按照 A、B、C 的顺序，其中每个内部列表为：\n$[\\text{naive\\_sst},\\text{adjusted\\_sst},\\text{adjusted\\_z500},\\text{bias\\_reduced},\\text{z500\\_close}]$\n例如：\n$[[x_A,y_A,z_A,b_A,c_A],[x_B,y_B,z_B,b_B,c_B],[x_C,y_C,z_C,b_C,c_C]]$\n所有报告的量都是无量纲的；布尔值必须是 $\\texttt{True}$ 或 $\\texttt{False}$。",
            "solution": "用户提供的问题是有效的。它在科学上基于因果推断和线性回归的原理，定义明确，包含所有必要的参数和清晰的目标，并且没有任何指定的无效标准。因此，我们可以进行完整的解答。\n\n该问题要求我们模拟一个由线性结构因果模型（SCM）描述的合成气候情景，并分析朴素统计归因与基于因果的归因之间的差异。SCM 将数据生成过程定义如下：\n$$\nu \\sim \\mathcal{N}(0,1)\n$$\n$$\n\\mathrm{SST} = a\\,u + \\varepsilon_s,\\quad \\varepsilon_s \\sim \\mathcal{N}(0,\\sigma_s^2)\n$$\n$$\n\\mathrm{Z500} = b\\,u + \\varepsilon_z,\\quad \\varepsilon_z \\sim \\mathcal{N}(0,\\sigma_z^2)\n$$\n$$\ny = d\\,\\mathrm{Z500} + \\varepsilon_y,\\quad \\varepsilon_y \\sim \\mathcal{N}(0,\\sigma_y^2)\n$$\n在此模型中，$u$ 是海表温度（$\\mathrm{SST}$）和对流层中层位势高度（$\\mathrm{Z500}$）的一个未观测到的共同原因（混淆因子）。目标变量 $y$ 仅由 $\\mathrm{Z500}$ 直接引起。从 $\\mathrm{SST}$到 $y$ 没有直接的因果链。然而，由于后门路径 $\\mathrm{SST} \\leftarrow u \\rightarrow \\mathrm{Z500} \\rightarrow y$ 的存在，统计模型可能会发现 $\\mathrm{SST}$ 和 $y$ 之间的虚假相关。\n\n我们的任务是通过比较两个用岭回归训练的线性模型来量化这种效应。线性模型的基于梯度的归因就是与每个输入变量相关联的系数。\n\n**步骤 1：数据生成**\n对于每个测试用例，我们将根据 SCM 生成一个大小为 $N$ 的数据集。我们将为每个用例使用一个专用的随机数生成器，并用指定的种子进行初始化，以确保可复现性。过程如下：\n1. 从标准正态分布 $\\mathcal{N}(0,1)$ 中生成 $N$ 个潜变量 $u$ 的样本。\n2. 从各自的正态分布 $\\mathcal{N}(0, \\sigma_s^2)$、$\\mathcal{N}(0, \\sigma_z^2)$ 和 $\\mathcal{N}(0, \\sigma_y^2)$ 中为每个噪声项 $\\varepsilon_s$、$\\varepsilon_z$ 和 $\\varepsilon_y$ 生成 $N$ 个样本。\n3. 使用 SCM 中提供的线性方程计算可观测变量 $\\mathrm{SST}$、$\\mathrm{Z500}$ 和 $y$。\n\n**步骤 2：岭回归实现**\n我们将实现一个函数来执行岭回归。给定一个特征矩阵 $X$ 和一个目标向量 $y$，岭回归系数向量 $\\beta$ 使用闭式解计算得出：\n$$\n\\beta = \\left(X_c^\\top X_c + \\lambda I\\right)^{-1} X_c^\\top y_c\n$$\n其中 $X_c$ 是列经过均值中心化后的特征矩阵，$y_c$ 是经过均值中心化的目标向量，$\\lambda$ 是正则化参数，$I$ 是相应维度的单位矩阵。均值中心化确保我们估计的是一个穿过数据均值的模型的斜率系数，从而有效地隐式处理了截距项。\n\n**步骤 3：朴素归因模型**\n第一个模型是“朴素”模型，它尝试仅使用 $\\mathrm{SST}$ 来预测 $y$。回归模型为 $y \\sim \\mathrm{SST}$。\n特征矩阵 $X_{\\text{naive}}$ 的维度为 $(N, 1)$，包含 $\\mathrm{SST}$ 数据。在均值中心化后，我们应用岭回归公式。得到的系数 $\\beta_{\\text{naive,SST}}$ 代表了 $y$ 对 $\\mathrm{SST}$ 的朴素基于梯度的归因。当混淆很强时（即 $a \\neq 0$），我们预计这个系数不为零，从而虚假地表明 $\\mathrm{SST}$ 和 $y$ 之间存在关系。这是因为 $\\mathrm{SST}$ 充当了真实因果驱动因素 $u$ 的代理变量。\n\n**步骤 4：调整后的因果归因模型**\n第二个模型是“调整后”的模型，旨在提供更好的因果估计。问题指出调整集为 $\\{\\mathrm{SST}, \\mathrm{Z500}, u\\}$。回归模型为 $y \\sim \\mathrm{SST} + \\mathrm{Z500} + u$。\n特征矩阵 $X_{\\text{adjusted}}$ 的维度为 $(N, 3)$，其列对应 $\\mathrm{SST}$、$\\mathrm{Z500}$ 和（实践中未观测到的）混淆因子 $u$。通过在回归中包含 $u$，我们明确地控制了共同原因，阻断了 $\\mathrm{SST}$ 和 $y$ 之间的后门路径。这就是后门调整的原则。该模型中 $\\mathrm{SST}$ 的系数 $\\beta_{\\text{adj,SST}}$ 应该是 $\\mathrm{SST}$ 对 $y$ 的真实直接因果效应的无偏估计。根据 SCM，该效应为 $0$。因此，我们预计 $\\beta_{\\text{adj,SST}}$ 接近于 $0$。$\\mathrm{Z500}$ 的系数 $\\beta_{\\text{adj,Z500}}$ 应该是 $\\mathrm{Z500}$ 对 $y$ 的真实直接因果效应的无偏估计，即参数 $d$。\n\n**步骤 5：报告结果**\n对于每个测试用例，我们计算并报告五个值：\n1. `naive_sst`：来自朴素模型的系数 $\\beta_{\\text{naive,SST}}$。\n2. `adjusted_sst`：来自调整后模型的系数 $\\beta_{\\text{adj,SST}}$。\n3. `adjusted_z500`：来自调整后模型的系数 $\\beta_{\\text{adj,Z500}}$。\n4. `bias_reduced`：一个布尔值，如果 $|\\beta_{\\text{adj,SST}}| \\le |\\beta_{\\text{naive,SST}}|$ 则为 `True`。这测试了对混淆因子进行调整是否减小了对 $\\mathrm{SST}$ 的（虚假）归因的幅度。我们预计在存在混淆的情况下，该值为 `True`。\n5. `z500_close`：一个布尔值，如果 $|\\beta_{\\text{adj,Z500}} - d| \\le \\text{tolerance}$ 则为 `True`。这测试了调整后的模型是否在给定的数值容差内正确地恢复了真实的因果参数 $d$。该估计的准确性将取决于样本大小 $N$ 和噪声水平。\n\n此过程将应用于所有三个测试用例，以展示因果调整如何帮助纠正由混淆导致的带偏见的归因，这是在气候科学等复杂系统中构建可信的 XAI 方法的一个关键考虑因素。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by generating data from the SCM, training naive and\n    adjusted ridge regression models, and reporting the resulting coefficients\n    and comparisons.\n    \"\"\"\n    test_cases = [\n        # Case A (strong confounding, well-sampled)\n        {'N': 10000, 'a': 1.5, 'b': 1.0, 'd': 2.0, 'sigma_s': 0.3, 'sigma_z': 0.3,\n         'sigma_y': 0.5, 'lambda_val': 1.0, 'tolerance': 0.15, 'seed': 42},\n        # Case B (no confounding, well-sampled)\n        {'N': 10000, 'a': 0.0, 'b': 1.0, 'd': 2.0, 'sigma_s': 0.3, 'sigma_z': 0.3,\n         'sigma_y': 0.5, 'lambda_val': 1.0, 'tolerance': 0.15, 'seed': 43},\n        # Case C (strong confounding, small sample and noisy)\n        {'N': 100, 'a': 1.5, 'b': 1.0, 'd': 2.0, 'sigma_s': 1.0, 'sigma_z': 1.0,\n         'sigma_y': 1.0, 'lambda_val': 2.0, 'tolerance': 0.5, 'seed': 44},\n    ]\n\n    results = []\n\n    def run_ridge_regression(X: np.ndarray, y: np.ndarray, lambda_val: float) -> np.ndarray:\n        \"\"\"\n        Computes ridge regression coefficients using the closed-form solution.\n        Assumes X and y are not yet mean-centered.\n        \"\"\"\n        # Mean-center the data\n        X_c = X - X.mean(axis=0)\n        y_c = y - y.mean()\n\n        n_samples, n_features = X_c.shape\n        \n        # Identity matrix for regularization term\n        I = np.identity(n_features)\n        \n        # Closed-form solution: beta = (X'X + lambda*I)^-1 * X'y\n        try:\n            A = X_c.T @ X_c + lambda_val * I\n            B = X_c.T @ y_c\n            beta = np.linalg.inv(A) @ B\n        except np.linalg.LinAlgError:\n            # In case of singularity, although unlikely with lambda > 0\n            beta = np.zeros(n_features)\n            \n        return beta\n\n    for case in test_cases:\n        N = case['N']\n        a = case['a']\n        b = case['b']\n        d = case['d']\n        sigma_s = case['sigma_s']\n        sigma_z = case['sigma_z']\n        sigma_y = case['sigma_y']\n        lambda_val = case['lambda_val']\n        tolerance = case['tolerance']\n        seed = case['seed']\n        \n        # 1. Generate synthetic data from the SCM\n        rng = np.random.default_rng(seed)\n        \n        u = rng.normal(loc=0.0, scale=1.0, size=N)\n        eps_s = rng.normal(loc=0.0, scale=sigma_s, size=N)\n        eps_z = rng.normal(loc=0.0, scale=sigma_z, size=N)\n        eps_y = rng.normal(loc=0.0, scale=sigma_y, size=N)\n        \n        sst = a * u + eps_s\n        z500 = b * u + eps_y\n        y = d * z500 + eps_y\n\n        # 2. Compute naive attribution (y ~ SST)\n        X_naive = sst.reshape(-1, 1)\n        beta_naive = run_ridge_regression(X_naive, y, lambda_val)\n        naive_sst = beta_naive[0]\n\n        # 3. Compute adjusted attribution (y ~ SST + Z500 + u)\n        X_adjusted = np.vstack([sst, z500, u]).T\n        beta_adjusted = run_ridge_regression(X_adjusted, y, lambda_val)\n        adjusted_sst = beta_adjusted[0]\n        adjusted_z500 = beta_adjusted[1]\n        \n        # 4. Perform comparisons\n        bias_reduced = abs(adjusted_sst) = abs(naive_sst)\n        z500_close = abs(adjusted_z500 - d) = tolerance\n        \n        # 5. Store results for the case\n        case_results = [\n            naive_sst, \n            adjusted_sst, \n            adjusted_z500,\n            bool(bias_reduced),  # Ensure Python bool not numpy.bool_\n            bool(z500_close)\n        ]\n        results.append(case_results)\n\n    # Format the final output string\n    # E.g., [[x_A,y_A,z_A,b_A,c_A],[x_B,y_B,z_B,b_B,c_B],[x_C,y_C,z_C,b_C,c_C]]\n    # Using a list comprehension to build the string representation of each inner list\n    result_str = '[' + ','.join(['[' + ','.join(map(str, res)) + ']' for res in results]) + ']'\n    print(result_str)\n\nsolve()\n```"
        }
    ]
}