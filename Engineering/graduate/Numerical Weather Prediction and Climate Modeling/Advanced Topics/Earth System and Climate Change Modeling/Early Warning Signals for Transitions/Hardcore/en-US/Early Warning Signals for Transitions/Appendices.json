{
    "hands_on_practices": [
        {
            "introduction": "The foundational concept behind many early warning signals is 'critical slowing down,' the tendency for a system to recover more slowly from perturbations as it approaches a tipping point. This practice connects the statistical signature of slowing down, an increase in the lag-1 autocorrelation ($\\rho_1$), to its physical meaning: a longer recovery time ($\\tau$). By calculating the change in $\\tau$ based on a hypothetical trend in $\\rho_1$ , you will develop a concrete understanding of how these abstract indicators translate into tangible changes in system dynamics.",
            "id": "4033204",
            "problem": "A climate subsystem near a stable equilibrium, as represented in Numerical Weather Prediction (NWP) and climate modeling, can be locally approximated by linear relaxation dynamics with stochastic forcing, which in continuous time is modeled as an Ornstein–Uhlenbeck (OU) process. Under this approximation, the e-folding recovery time $\\tau$ characterizes the rate at which perturbations decay back to equilibrium. For a time series sampled discretely at interval $\\Delta t$, the corresponding lag-1 autocorrelation coefficient $\\rho_1$ of the sampled series is related to the continuous-time decay by the identity $\\rho_1 = \\exp(-\\Delta t/\\tau)$, implying $\\tau = -\\Delta t/\\ln \\rho_1$. In early warning signal analysis for transitions, an increasing $\\rho_1$ indicates critical slowing down and a growing $\\tau$.\n\nConsider a daily-resolved climate index time series ($\\Delta t = 1$ day) whose estimated lag-1 autocorrelation coefficient $\\rho_1(t)$ increases approximately linearly over time due to slow external forcing: $\\rho_1(t) \\approx \\rho_{1,0} + s t$, where $t$ is in years, $\\rho_{1,0} = 0.95$, and the trend $s = 0.004\\,\\text{year}^{-1}$ persists uniformly over $T = 10$ years. Assume $0  \\rho_1(t)  1$ throughout the interval, consistent with a stationary Autoregressive (AR) model of order one ($\\text{AR}(1)$).\n\nUsing the relationship $\\tau = -\\Delta t/\\ln \\rho_1$, compute the expected change in e-folding recovery time $\\Delta \\tau = \\tau(T) - \\tau(0)$ over the $T = 10$ years. Express your final answer in days and round your answer to four significant figures.",
            "solution": "The e-folding recovery time $τ$ is given as a function of the lag-1 autocorrelation coefficient $ρ_1$:\n$$ \\tau(\\rho_1) = -\\frac{\\Delta t}{\\ln \\rho_1} $$\nThe lag-1 autocorrelation coefficient $ρ_1$ is itself a function of time $t$:\n$$ \\rho_1(t) = \\rho_{1,0} + s t $$\nTherefore, the recovery time $τ$ as a function of time $t$ can be written as:\n$$ \\tau(t) = -\\frac{\\Delta t}{\\ln(\\rho_{1,0} + s t)} $$\nThe goal is to compute the change in recovery time, $Δτ$, over the interval from $t=0$ to $t=T$.\n$$ \\Delta\\tau = \\tau(T) - \\tau(0) $$\n\nFirst, we calculate the recovery time at the initial time, $t=0$.\nThe autocorrelation at $t=0$ is:\n$$ \\rho_1(0) = \\rho_{1,0} + s(0) = \\rho_{1,0} = 0.95 $$\nThe corresponding recovery time is:\n$$ \\tau(0) = -\\frac{\\Delta t}{\\ln(\\rho_1(0))} = -\\frac{\\Delta t}{\\ln(0.95)} $$\n\nNext, we calculate the recovery time at the final time, $t=T=10$ years.\nThe autocorrelation at $t=10$ is:\n$$ \\rho_1(10) = \\rho_{1,0} + sT = 0.95 + (0.004 \\,\\text{year}^{-1})(10 \\,\\text{years}) = 0.95 + 0.04 = 0.99 $$\nThe corresponding recovery time is:\n$$ \\tau(10) = -\\frac{\\Delta t}{\\ln(\\rho_1(10))} = -\\frac{\\Delta t}{\\ln(0.99)} $$\n\nNow, we compute the change $Δτ$:\n$$ \\Delta\\tau = \\tau(10) - \\tau(0) = \\left(-\\frac{\\Delta t}{\\ln(0.99)}\\right) - \\left(-\\frac{\\Delta t}{\\ln(0.95)}\\right) $$\n$$ \\Delta\\tau = \\Delta t \\left( \\frac{1}{\\ln(0.95)} - \\frac{1}{\\ln(0.99)} \\right) $$\nWe are given that the sampling interval is $Δt = 1$ day. Substituting this value:\n$$ \\Delta\\tau = (1 \\text{ day}) \\left( \\frac{1}{\\ln(0.95)} - \\frac{1}{\\ln(0.99)} \\right) $$\nTo obtain the numerical value, we use the natural logarithms of the arguments:\n$$ \\ln(0.95) \\approx -0.05129329438 $$\n$$ \\ln(0.99) \\approx -0.01005033585 $$\nLet's compute the individual recovery times first, as this illustrates the physical meaning of \"slowing down\".\n$$ \\tau(0) = -\\frac{1}{\\ln(0.95)} \\approx -\\frac{1}{-0.05129329} \\approx 19.49576 \\text{ days} $$\n$$ \\tau(10) = -\\frac{1}{\\ln(0.99)} \\approx -\\frac{1}{-0.01005034} \\approx 99.49916 \\text{ days} $$\nThe change $Δτ$ is the difference between these two values:\n$$ \\Delta\\tau = \\tau(10) - \\tau(0) \\approx 99.49916 - 19.49576 \\approx 80.0034 \\text{ days} $$\nThe problem requires the final answer to be rounded to four significant figures. The value $80.0034$ has more than four significant figures. The first four are $8$, $0$, $0$, and $0$. The fifth digit is $3$, which is less than $5$, so we round down.\n$$ \\Delta\\tau \\approx 80.00 \\text{ days} $$\nThe expected change in e-folding recovery time over the $10$-year period is $80.00$ days.",
            "answer": "$$\n\\boxed{80.00}\n$$"
        },
        {
            "introduction": "In real-world climate data, internal variability is often superimposed on a deterministic trend driven by external forcing. This exercise  explores a crucial pitfall: how failing to remove such a trend can lead to a spurious early warning signal, creating the illusion of critical slowing down where none exists. By deriving the analytical bias in the estimated autocorrelation, you will see precisely why detrending is an indispensable first step in any robust analysis of early warning signals.",
            "id": "4033195",
            "problem": "Consider a forced climate time series used in numerical weather prediction and climate modeling in which the observable, such as global-mean surface temperature anomaly, is the sum of a deterministic forced response and internal variability. Let the observable be modeled as $y_{t} = \\mu_{0} + \\beta t + x_{t}$ for $t = 1, 2, \\dots, N$, where $\\mu_{0}$ is a constant, $\\beta$ is a constant linear trend due to external forcing, and $x_{t}$ is a zero-mean stationary first-order autoregressive process $\\mathrm{AR}(1)$ satisfying $x_{t} = \\phi x_{t-1} + \\varepsilon_{t}$ with $|\\phi|  1$ and $\\varepsilon_{t}$ independent and identically distributed Gaussian noise with variance $\\sigma_{\\varepsilon}^{2}$. Denote the stationary variance of $x_{t}$ by $\\sigma_{x}^{2}$.\n\nEarly warning signals (EWS) for transitions often monitor changes in the lag-$1$ autocorrelation, here denoted $\\rho_{1}$. In operational practice, a common estimator of $\\rho_{1}$ is computed from the mean-removed but not detrended series:\n$$\n\\hat{\\rho}_{1} \\equiv \\frac{\\sum_{t=2}^{N} \\left(y_{t} - \\bar{y}\\right)\\left(y_{t-1} - \\bar{y}\\right)}{\\sum_{t=1}^{N} \\left(y_{t} - \\bar{y}\\right)^{2}},\n$$\nwhere $\\bar{y} \\equiv \\frac{1}{N}\\sum_{t=1}^{N} y_{t}$ is the sample mean. Assume $x_{t}$ is independent of the deterministic trend component and that $N \\geq 4$.\n\nStarting from the definitions above and standard properties of the $\\mathrm{AR}(1)$ process and time-averages of deterministic sequences, derive a closed-form analytic expression for the large-sample expected bias of $\\hat{\\rho}_{1}$ when the linear trend is neglected, defined as $\\mathrm{Bias} \\equiv \\mathbb{E}[\\hat{\\rho}_{1}] - \\phi$, in terms of $N$, $\\beta$, $\\phi$, and $\\sigma_{x}^{2}$. In your derivation, use the leading-order large-$N$ approximations $\\mathbb{E}\\!\\left[\\sum_{t=1}^{N} \\left(x_{t} - \\bar{x}\\right)^{2}\\right] \\approx N \\sigma_{x}^{2}$ and $\\mathbb{E}\\!\\left[\\sum_{t=2}^{N} \\left(x_{t} - \\bar{x}\\right)\\left(x_{t-1} - \\bar{x}\\right)\\right] \\approx (N-1)\\phi \\sigma_{x}^{2}$, where $\\bar{x} \\equiv \\frac{1}{N}\\sum_{t=1}^{N} x_{t}$. Also, use exact closed-form sums for the linear deterministic component.\n\nExplain briefly, based on your derivation, why detrending is essential for EWS detection in forced climate systems. Express the final bias as a single closed-form analytic expression. No rounding is required, and no units are needed in your final answer.",
            "solution": "The observable is $y_{t} = m_{t} + x_{t}$ with deterministic mean $m_{t} \\equiv \\mu_{0} + \\beta t$ and stochastic variability $x_{t}$. The estimator of lag-$1$ autocorrelation is\n$$\n\\hat{\\rho}_{1} = \\frac{\\sum_{t=2}^{N} \\left[(m_{t}-\\bar{m}) + (x_{t}-\\bar{x})\\right]\\left[(m_{t-1}-\\bar{m}) + (x_{t-1}-\\bar{x})\\right]}{\\sum_{t=1}^{N} \\left[(m_{t}-\\bar{m}) + (x_{t}-\\bar{x})\\right]^{2}},\n$$\nwhere $\\bar{m} \\equiv \\frac{1}{N}\\sum_{t=1}^{N} m_{t}$ and $\\bar{x} \\equiv \\frac{1}{N}\\sum_{t=1}^{N} x_{t}$. Because $m_{t}$ is deterministic and independent of $x_{t}$, the expectation over the stochastic component eliminates cross terms of the form $(m_{t}-\\bar{m})(x_{t-1}-\\bar{x})$ and $(x_{t}-\\bar{x})(m_{t-1}-\\bar{m})$, yielding\n$$\n\\mathbb{E}\\!\\left[\\sum_{t=2}^{N} (y_{t}-\\bar{y})(y_{t-1}-\\bar{y})\\right] = \\sum_{t=2}^{N} (m_{t}-\\bar{m})(m_{t-1}-\\bar{m}) + \\mathbb{E}\\!\\left[\\sum_{t=2}^{N} (x_{t}-\\bar{x})(x_{t-1}-\\bar{x})\\right],\n$$\nand\n$$\n\\mathbb{E}\\!\\left[\\sum_{t=1}^{N} (y_{t}-\\bar{y})^{2}\\right] = \\sum_{t=1}^{N} (m_{t}-\\bar{m})^{2} + \\mathbb{E}\\!\\left[\\sum_{t=1}^{N} (x_{t}-\\bar{x})^{2}\\right].\n$$\n\nWe compute the deterministic sums exactly. The sample mean of the linear sequence $m_{t} = \\mu_{0} + \\beta t$ is $\\bar{m} = \\mu_{0} + \\beta \\frac{N+1}{2}$. Define the centered deterministic sequence\n$$\nd_{t} \\equiv m_{t} - \\bar{m} = \\beta\\left(t - \\frac{N+1}{2}\\right).\n$$\nThen\n$$\n\\sum_{t=1}^{N} d_{t}^{2} = \\beta^{2} \\sum_{t=1}^{N} \\left(t - \\frac{N+1}{2}\\right)^{2} = \\beta^{2}\\,\\frac{N\\left(N^{2}-1\\right)}{12},\n$$\na standard result for the sum of squares about the mean of the integers $1,\\dots,N$.\n\nFor the lag-$1$ deterministic covariance sum, note that\n$$\nd_{t}\\,d_{t-1} = \\beta^{2}\\left(t-\\frac{N+1}{2}\\right)\\left[(t-1)-\\frac{N+1}{2}\\right] = \\beta^{2}\\left[\\left(t-\\frac{N+1}{2}\\right)^{2} - \\left(t-\\frac{N+1}{2}\\right)\\right].\n$$\nTherefore,\n\\begin{align*}\n\\sum_{t=2}^{N} d_{t}\\,d_{t-1}\n= \\beta^{2}\\left[\\sum_{t=2}^{N} \\left(t-\\frac{N+1}{2}\\right)^{2} - \\sum_{t=2}^{N} \\left(t-\\frac{N+1}{2}\\right)\\right] \\\\\n= \\beta^{2}\\left[\\sum_{t=1}^{N} \\left(t-\\frac{N+1}{2}\\right)^{2} - \\left(1-\\frac{N+1}{2}\\right)^{2} - \\left(\\sum_{t=1}^{N} \\left(t-\\frac{N+1}{2}\\right) - \\left(1-\\frac{N+1}{2}\\right)\\right)\\right].\n\\end{align*}\nSince $\\sum_{t=1}^{N} \\left(t-\\frac{N+1}{2}\\right) = 0$, the second bracket simplifies to $+\\left(1-\\frac{N+1}{2}\\right)$. Using $\\left(1-\\frac{N+1}{2}\\right) = -\\frac{N-1}{2}$ and $\\left(1-\\frac{N+1}{2}\\right)^{2} = \\frac{(N-1)^{2}}{4}$, we obtain\n\\begin{align*}\n\\sum_{t=2}^{N} d_{t}\\,d_{t-1}\n= \\beta^{2}\\left[\\frac{N\\left(N^{2}-1\\right)}{12} - \\frac{(N-1)^{2}}{4} - \\frac{N-1}{2}\\right] \\\\\n= \\beta^{2}\\left[\\frac{N\\left(N^{2}-1\\right)}{12} - \\frac{N^{2}-1}{4}\\right] \\\\\n= \\beta^{2}\\,\\frac{(N^{2}-1)(N-3)}{12}.\n\\end{align*}\n\nNext, we use the large-$N$ expectations for the stochastic sums. For an $\\mathrm{AR}(1)$ process with lag-$1$ autocorrelation $\\phi$ and stationary variance $\\sigma_{x}^{2}$, the leading-order approximations are\n$$\n\\mathbb{E}\\!\\left[\\sum_{t=1}^{N} (x_{t}-\\bar{x})^{2}\\right] \\approx N\\,\\sigma_{x}^{2}, \\qquad \\mathbb{E}\\!\\left[\\sum_{t=2}^{N} (x_{t}-\\bar{x})(x_{t-1}-\\bar{x})\\right] \\approx (N-1)\\,\\phi\\,\\sigma_{x}^{2}.\n$$\n\nCombining deterministic and stochastic contributions, the expected numerator and denominator of $\\hat{\\rho}_{1}$ are\n\\begin{align*}\n\\mathbb{E}[\\text{num}] = \\beta^{2}\\,\\frac{(N^{2}-1)(N-3)}{12} + (N-1)\\,\\phi\\,\\sigma_{x}^{2}, \\\\\n\\mathbb{E}[\\text{den}] = \\beta^{2}\\,\\frac{N\\left(N^{2}-1\\right)}{12} + N\\,\\sigma_{x}^{2}.\n\\end{align*}\nThus,\n$$\n\\mathbb{E}[\\hat{\\rho}_{1}] \\approx \\frac{\\beta^{2}\\,\\frac{(N^{2}-1)(N-3)}{12} + (N-1)\\,\\phi\\,\\sigma_{x}^{2}}{\\beta^{2}\\,\\frac{N\\left(N^{2}-1\\right)}{12} + N\\,\\sigma_{x}^{2}}.\n$$\nThe bias relative to the true lag-$1$ autocorrelation $\\phi$ is\n\\begin{align*}\n\\mathrm{Bias} \\equiv \\mathbb{E}[\\hat{\\rho}_{1}] - \\phi\n= \\frac{\\beta^{2}\\,\\frac{(N^{2}-1)(N-3)}{12} + (N-1)\\,\\phi\\,\\sigma_{x}^{2}}{\\beta^{2}\\,\\frac{N\\left(N^{2}-1\\right)}{12} + N\\,\\sigma_{x}^{2}} - \\phi \\\\\n= \\frac{\\beta^{2}\\,\\frac{(N^{2}-1)(N-3)}{12} - \\phi\\,\\beta^{2}\\,\\frac{N\\left(N^{2}-1\\right)}{12} + (N-1)\\,\\phi\\,\\sigma_{x}^{2} - \\phi\\,N\\,\\sigma_{x}^{2}}{\\beta^{2}\\,\\frac{N\\left(N^{2}-1\\right)}{12} + N\\,\\sigma_{x}^{2}} \\\\\n= \\frac{\\beta^{2}\\,\\frac{(N^{2}-1)}{12}\\left[(N-3) - \\phi N\\right] - \\phi\\,\\sigma_{x}^{2}}{N\\left(\\beta^{2}\\,\\frac{(N^{2}-1)}{12} + \\sigma_{x}^{2}\\right)}.\n\\end{align*}\n\nThis expression makes clear why detrending is essential for early warning signals detection in forced climate systems. As $|\\beta|$ increases or $N$ grows, the deterministic trend contributes strongly to both the numerator and the denominator. In the limit where the deterministic trend dominates the variance (i.e., $\\beta^{2}\\,\\frac{(N^{2}-1)}{12} \\gg \\sigma_{x}^{2}$), the expected estimator approaches the deterministic ratio $\\frac{(N-3)}{N}$, which is close to $1$ for large $N$, falsely signaling critical slowing down even if the true $\\phi$ is modest. Detrending removes the deterministic contribution, restoring an unbiased estimate of $\\rho_{1}$ attributable to internal variability alone and preventing spurious EWS detection driven by external forcing rather than proximity to a dynamical transition.",
            "answer": "$$\\boxed{\\frac{\\beta^{2}\\,\\frac{(N^{2}-1)}{12}\\left[(N-3) - \\phi N\\right] - \\phi\\,\\sigma_{x}^{2}}{N\\left(\\beta^{2}\\,\\frac{(N^{2}-1)}{12} + \\sigma_{x}^{2}\\right)}}$$"
        },
        {
            "introduction": "Climate and weather phenomena rarely operate in isolation; they are typically part of larger, coupled systems. This practice  extends the principles of early warning signals from a single time series to a multivariate framework using a Vector Autoregressive (VAR) model. You will learn to assess system stability by analyzing the eigenvalues of the companion matrix, understanding that its spectral radius approaching unity is the multivariate signature of critical slowing down.",
            "id": "4033239",
            "problem": "Consider a simplified two-index subsystem within numerical weather prediction (NWP) and climate modeling that captures monthly anomalies of a zonal-mean jet index and a sea-ice edge latitude index. Let the anomaly vector be $x_t \\in \\mathbb{R}^2$ at discrete time $t \\in \\mathbb{Z}$, modeled as a second-order vector autoregression (VAR(2)):\n$$\nx_t = A_1 x_{t-1} + A_2 x_{t-2} + \\varepsilon_t,\n$$\nwhere $\\varepsilon_t$ is zero-mean white noise with stationary covariance, and $A_1, A_2 \\in \\mathbb{R}^{2 \\times 2}$ are constant coefficient matrices that represent linear memory of the indices. Early warning signals (EWS) for transitions in this linear discrete-time system are connected to the approach of the state-transition eigenvalues toward the unit circle, equivalently a spectral radius approaching unity. The state-transition over one step can be written via the $4 \\times 4$ companion matrix\n$$\nC = \\begin{pmatrix}\nA_1  A_2 \\\\\nI_2  0_2\n\\end{pmatrix},\n$$\nwhere $I_2$ is the $2 \\times 2$ identity and $0_2$ is the $2 \\times 2$ zero matrix. Assume the coefficient structure\n$$\nA_1 = \\alpha I_2 + \\delta B, \\quad A_2 = \\beta I_2,\n$$\nwith $B = \\begin{pmatrix} 1  0 \\\\ 0  -1 \\end{pmatrix}$, and the parameter values $\\alpha = 0.80$, $\\delta = 0.15$, and $\\beta = 0.02$. Define the early warning proximity indicator as\n$$\n\\Delta = 1 - \\rho(C),\n$$\nwhere $\\rho(C)$ is the spectral radius of $C$, namely the largest absolute value of the eigenvalues of $C$. Using only the fundamental definitions of linear discrete-time stability and the structure given above, derive the eigenvalues of $C$ and compute $\\Delta$. Round your answer to four significant figures. Express your final answer as a dimensionless quantity.",
            "solution": "The goal is to compute $\\Delta = 1 - \\rho(C)$, which requires finding the eigenvalues of the companion matrix $C$. The eigenvalue equation for $C$ is $C \\mathbf{v} = \\lambda \\mathbf{v}$, where $\\mathbf{v}$ is a $4 \\times 1$ eigenvector and $\\lambda$ is the corresponding eigenvalue. We can write the eigenvector as $\\mathbf{v} = \\begin{pmatrix} v \\\\ w \\end{pmatrix}$, where $v, w \\in \\mathbb{R}^2$. The eigenvalue equation expands to a system of two equations:\n$$\nA_1 v + A_2 w = \\lambda v\n$$\n$$\nI_2 v + 0_2 w = \\lambda w \\implies v = \\lambda w\n$$\nAssuming $\\lambda \\neq 0$ (if $\\lambda=0$, then $v=0$, implying $A_2 w = 0$. Since $A_2 = \\beta I_2$ with $\\beta=0.02 \\neq 0$, $A_2$ is invertible, so $w=0$, giving the trivial solution. Thus $\\lambda \\neq 0$), we can substitute $w = \\lambda^{-1}v$ into the first equation:\n$$\nA_1 v + A_2 (\\lambda^{-1} v) = \\lambda v\n$$\nMultiplying by $\\lambda$ gives:\n$$\n\\lambda A_1 v + A_2 v = \\lambda^2 v\n$$\nRearranging the terms, we obtain a quadratic eigenvalue problem:\n$$\n(\\lambda^2 I_2 - \\lambda A_1 - A_2) v = 0\n$$\nFor a non-trivial eigenvector $v$ to exist, the matrix $(\\lambda^2 I_2 - \\lambda A_1 - A_2)$ must be singular. Therefore, its determinant must be zero:\n$$\n\\det(\\lambda^2 I_2 - \\lambda A_1 - A_2) = 0\n$$\nThis equation is the characteristic equation for the eigenvalues $\\lambda$ of $C$.\n\nNext, we substitute the specific structures of $A_1$ and $A_2$:\n$A_1 = \\alpha I_2 + \\delta B = \\alpha \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} + \\delta \\begin{pmatrix} 1  0 \\\\ 0  -1 \\end{pmatrix} = \\begin{pmatrix} \\alpha+\\delta  0 \\\\ 0  \\alpha-\\delta \\end{pmatrix}$\n$A_2 = \\beta I_2 = \\begin{pmatrix} \\beta  0 \\\\ 0  \\beta \\end{pmatrix}$\nBoth $A_1$ and $A_2$ are diagonal matrices. The matrix in the characteristic equation is then:\n$$\n\\lambda^2 I_2 - \\lambda A_1 - A_2 = \\begin{pmatrix} \\lambda^2 - \\lambda(\\alpha+\\delta) - \\beta  0 \\\\ 0  \\lambda^2 - \\lambda(\\alpha-\\delta) - \\beta \\end{pmatrix}\n$$\nThe determinant of this diagonal matrix is the product of its diagonal entries:\n$$\n\\det(\\dots) = (\\lambda^2 - \\lambda(\\alpha+\\delta) - \\beta)(\\lambda^2 - \\lambda(\\alpha-\\delta) - \\beta) = 0\n$$\nThis single characteristic equation decouples into two independent quadratic equations:\n1. $\\lambda^2 - (\\alpha+\\delta)\\lambda - \\beta = 0$\n2. $\\lambda^2 - (\\alpha-\\delta)\\lambda - \\beta = 0$\n\nNow, we substitute the given parameter values: $\\alpha = 0.80$, $\\delta = 0.15$, $\\beta = 0.02$.\n$\\alpha+\\delta = 0.80 + 0.15 = 0.95$\n$\\alpha-\\delta = 0.80 - 0.15 = 0.65$\n\nThe two equations become:\n1. $\\lambda^2 - 0.95\\lambda - 0.02 = 0$\n2. $\\lambda^2 - 0.65\\lambda - 0.02 = 0$\n\nThe four eigenvalues of $C$ are the roots of these two equations. We use the quadratic formula $\\lambda = \\frac{-b \\pm \\sqrt{b^2-4ac}}{2a}$ for each equation.\n\nFor the first equation:\n$\\lambda = \\frac{0.95 \\pm \\sqrt{(-0.95)^2 - 4(1)(-0.02)}}{2} = \\frac{0.95 \\pm \\sqrt{0.9025 + 0.08}}{2} = \\frac{0.95 \\pm \\sqrt{0.9825}}{2}$\nThe two eigenvalues are $\\lambda_1 = \\frac{0.95 + \\sqrt{0.9825}}{2}$ and $\\lambda_2 = \\frac{0.95 - \\sqrt{0.9825}}{2}$.\n\nFor the second equation:\n$\\lambda = \\frac{0.65 \\pm \\sqrt{(-0.65)^2 - 4(1)(-0.02)}}{2} = \\frac{0.65 \\pm \\sqrt{0.4225 + 0.08}}{2} = \\frac{0.65 \\pm \\sqrt{0.5025}}{2}$\nThe other two eigenvalues are $\\lambda_3 = \\frac{0.65 + \\sqrt{0.5025}}{2}$ and $\\lambda_4 = \\frac{0.65 - \\sqrt{0.5025}}{2}$.\n\nThe spectral radius $\\rho(C)$ is the maximum of the absolute values of these four eigenvalues: $\\rho(C) = \\max(|\\lambda_1|, |\\lambda_2|, |\\lambda_3|, |\\lambda_4|)$.\nFor an equation of the form $\\lambda^2 - a\\lambda - b = 0$ with $a, b  0$, the roots are real, one positive and one negative. The absolute value of the positive root is larger.\nThe largest-magnitude root from the first equation is $\\lambda_1$.\nThe largest-magnitude root from the second equation is $\\lambda_3$.\nWe must compare $\\lambda_1$ and $\\lambda_3$.\nLet $f(a) = \\frac{a + \\sqrt{a^2+4\\beta}}{2}$. Since $a  0$ and $\\beta  0$, $f(a)$ is an increasing function of $a$.\nSince $\\alpha+\\delta = 0.95  \\alpha-\\delta = 0.65$, we have $\\lambda_1  \\lambda_3$.\nThus, the spectral radius is given by the largest root of the first equation.\n$$\n\\rho(C) = \\lambda_1 = \\frac{0.95 + \\sqrt{0.9825}}{2}\n$$\nNow we compute the numerical value:\n$\\sqrt{0.9825} \\approx 0.99121138$\n$$\n\\rho(C) \\approx \\frac{0.95 + 0.99121138}{2} \\approx \\frac{1.94121138}{2} \\approx 0.97060569\n$$\nFinally, we compute the early warning proximity indicator $\\Delta$:\n$$\n\\Delta = 1 - \\rho(C) \\approx 1 - 0.97060569 = 0.02939431\n$$\nThe problem requires rounding the final answer to four significant figures. The first significant figure is $2$, the second is $9$, the third is $3$, and the fourth is $9$. The following digit is $4$, so we round down.\n$$\n\\Delta \\approx 0.02939\n$$",
            "answer": "$$\\boxed{0.02939}$$"
        }
    ]
}