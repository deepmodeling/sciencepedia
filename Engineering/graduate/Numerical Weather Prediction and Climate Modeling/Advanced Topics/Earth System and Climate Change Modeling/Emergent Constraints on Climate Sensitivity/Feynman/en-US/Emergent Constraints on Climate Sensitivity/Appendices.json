{
    "hands_on_practices": [
        {
            "introduction": "Before exploring advanced statistical methods, it is essential to build intuition from the underlying physics. This exercise uses a conceptual two-box energy balance model to formally distinguish between the Transient Climate Response (TCR) and the Equilibrium Climate Sensitivity (ECS). By deriving these quantities from first principles, you will develop a concrete understanding of how ocean heat uptake delays warming, which is a key process that many emergent constraints aim to capture.",
            "id": "4035774",
            "problem": "Consider a globally averaged two-box energy balance representation of the climate system, with an upper box representing the atmosphere–mixed-layer ocean and a lower box representing the deep ocean. The evolution of globally averaged temperature anomalies in the upper box, $T_{u}(t)$, and deep ocean, $T_{d}(t)$, is governed by conservation of energy:\n$$\nC_{u}\\,\\frac{d T_{u}}{dt} \\;=\\; F(t) \\;-\\; \\lambda\\,T_{u}(t) \\;-\\; \\gamma\\,\\big(T_{u}(t) - T_{d}(t)\\big),\n$$\n$$\nC_{d}\\,\\frac{d T_{d}}{dt} \\;=\\; \\gamma\\,\\big(T_{u}(t) - T_{d}(t)\\big),\n$$\nwhere $C_{u}$ and $C_{d}$ are effective heat capacities of the upper and deep ocean boxes, $F(t)$ is the externally imposed radiative forcing, $\\lambda$ is the net climate feedback parameter (positive for a stabilizing feedback), and $\\gamma$ is the effective heat exchange coefficient between the boxes. Define the top-of-atmosphere net downward energy imbalance as $N(t) \\equiv F(t) - \\lambda\\,T_{u}(t)$, and the ocean heat uptake efficiency as $\\kappa$, an emergent property of the transient response such that, over multi-decadal timescales under increasing radiative forcing, $N(t)$ is approximately proportional to $T_{u}(t)$, i.e., $N(t) \\approx \\kappa\\,T_{u}(t)$.\n\nAssume an idealized scenario of exponentially increasing atmospheric carbon dioxide concentration at $1\\%$ per year such that radiative forcing increases monotonically in time until carbon dioxide doubling. Let $F_{2\\times\\mathrm{CO_{2}}}$ denote the radiative forcing at doubling, and let the equilibrium climate sensitivity be defined by $\\mathrm{ECS} \\equiv T_{\\mathrm{eq}}$ satisfying the equilibrium balance $0 = F_{2\\times\\mathrm{CO_{2}}} - \\lambda\\,T_{\\mathrm{eq}}$. Let the transient climate response be defined by $\\mathrm{TCR} \\equiv T_{u}(t_{2})$ at the time $t_{2}$ of carbon dioxide doubling under the transient scenario above.\n\nUsing only the governing balance equations, the definition of $N(t)$, and the emergent proportionality $N(t) \\approx \\kappa\\,T_{u}(t)$ that holds during the transient, derive an expression for $\\mathrm{TCR}$ in terms of $F_{2\\times\\mathrm{CO_{2}}}$, $\\lambda$, and $\\kappa$, and also an expression for $\\mathrm{ECS}$ in terms of $F_{2\\times\\mathrm{CO_{2}}}$ and $\\lambda$. Then, with the hypothetical but scientifically plausible parameter values $F_{2\\times\\mathrm{CO_{2}}} = 3.70\\,\\mathrm{W\\,m^{-2}}$, $\\lambda = 1.30\\,\\mathrm{W\\,m^{-2}\\,K^{-1}}$, and $\\kappa = 0.70\\,\\mathrm{W\\,m^{-2}\\,K^{-1}}$, compute the dimensionless ratio\n$$\nR \\equiv \\frac{\\mathrm{TCR}}{\\mathrm{ECS}}\n$$\nas a quantitative measure of timescale effects.\n\nRound your final answer for $R$ to four significant figures. Express the final answer as a pure number with no units.",
            "solution": "The problem statement has been critically validated and found to be scientifically grounded, well-posed, objective, and self-contained. The provided two-box energy balance model is a standard conceptual tool in climate dynamics, and the definitions of Equilibrium Climate Sensitivity (ECS), Transient Climate Response (TCR), and ocean heat uptake efficiency ($\\kappa$) are correct. The given parameter values are physically plausible. The problem is therefore deemed valid and a full solution follows.\n\nThe problem requires the derivation of expressions for the Equilibrium Climate Sensitivity ($\\mathrm{ECS}$) and the Transient Climate Response ($\\mathrm{TCR}$), followed by the calculation of their ratio, $R$.\n\nFirst, we derive the expression for $\\mathrm{ECS}$. The $\\mathrm{ECS}$ is defined as the equilibrium temperature anomaly, $T_{\\mathrm{eq}}$, in response to a sustained radiative forcing $F_{2\\times\\mathrm{CO_{2}}}$. At equilibrium, all time derivatives are zero, $\\frac{dT_u}{dt} = 0$ and $\\frac{dT_d}{dt} = 0$. The second governing equation, $C_{d}\\,\\frac{d T_{d}}{dt} = \\gamma\\,\\big(T_{u}(t) - T_{d}(t)\\big)$, implies that at equilibrium, $T_{u} - T_{d} = 0$, so $T_u = T_d = T_{\\mathrm{eq}}$. Substituting these conditions into the first governing equation yields:\n$$\nC_{u} \\cdot 0 = F_{2\\times\\mathrm{CO_{2}}} - \\lambda\\,T_{\\mathrm{eq}} - \\gamma\\,\\big(T_{\\mathrm{eq}} - T_{\\mathrm{eq}}\\big)\n$$\n$$\n0 = F_{2\\times\\mathrm{CO_{2}}} - \\lambda\\,T_{\\mathrm{eq}}\n$$\nThis matches the equilibrium balance provided in the problem statement. The $\\mathrm{ECS}$ is defined as $\\mathrm{ECS} \\equiv T_{\\mathrm{eq}}$. Solving for $T_{\\mathrm{eq}}$ gives the expression for $\\mathrm{ECS}$:\n$$\n\\mathrm{ECS} = \\frac{F_{2\\times\\mathrm{CO_{2}}}}{\\lambda}\n$$\n\nNext, we derive the expression for $\\mathrm{TCR}$. The $\\mathrm{TCR}$ is defined as the upper-box temperature anomaly at the time of $\\mathrm{CO_2}$ doubling, $\\mathrm{TCR} \\equiv T_{u}(t_{2})$. At this specific time, the radiative forcing is $F(t_2) = F_{2\\times\\mathrm{CO_{2}}}$. The problem states that during the transient warming scenario, the top-of-atmosphere net downward energy imbalance, $N(t)$, is approximately proportional to the upper-box temperature anomaly, $T_u(t)$, according to the relation $N(t) \\approx \\kappa\\,T_{u}(t)$. The problem also provides the physical definition of the energy imbalance: $N(t) \\equiv F(t) - \\lambda\\,T_{u}(t)$.\n\nBy equating these two expressions for $N(t)$ at the time of doubling, $t_2$, we can establish a relationship for the transient state. We will treat the approximation as an equality as instructed by the problem setup (\"using ... the emergent proportionality\").\n$$\n\\kappa\\,T_{u}(t_{2}) = F(t_{2}) - \\lambda\\,T_{u}(t_{2})\n$$\nSubstituting the definitions $\\mathrm{TCR} \\equiv T_{u}(t_{2})$ and $F(t_{2}) = F_{2\\times\\mathrm{CO_{2}}}$ into this equation:\n$$\n\\kappa\\,\\mathrm{TCR} = F_{2\\times\\mathrm{CO_{2}}} - \\lambda\\,\\mathrm{TCR}\n$$\nWe can now rearrange this algebraic equation to solve for $\\mathrm{TCR}$:\n$$\n\\kappa\\,\\mathrm{TCR} + \\lambda\\,\\mathrm{TCR} = F_{2\\times\\mathrm{CO_{2}}}\n$$\n$$\n(\\lambda + \\kappa)\\,\\mathrm{TCR} = F_{2\\times\\mathrm{CO_{2}}}\n$$\nThis gives the expression for $\\mathrm{TCR}$ in terms of the specified parameters:\n$$\n\\mathrm{TCR} = \\frac{F_{2\\times\\mathrm{CO_{2}}}}{\\lambda + \\kappa}\n$$\n\nFinally, we are asked to compute the dimensionless ratio $R \\equiv \\frac{\\mathrm{TCR}}{\\mathrm{ECS}}$. Using the expressions derived above:\n$$\nR = \\frac{\\mathrm{TCR}}{\\mathrm{ECS}} = \\frac{\\left( \\frac{F_{2\\times\\mathrm{CO_{2}}}}{\\lambda + \\kappa} \\right)}{\\left( \\frac{F_{2\\times\\mathrm{CO_{2}}}}{\\lambda} \\right)}\n$$\nThe term $F_{2\\times\\mathrm{CO_{2}}}$ cancels from the numerator and the denominator, yielding a simplified symbolic expression for the ratio $R$:\n$$\nR = \\frac{\\lambda}{\\lambda + \\kappa}\n$$\nThis expression shows that the ratio of the transient to the equilibrium response is determined by the ratio of the climate feedback parameter to the sum of the feedback parameter and the ocean heat uptake efficiency. Since $\\kappa > 0$, it follows that $R < 1$, which is physically expected, as the ocean heat uptake delays surface warming.\n\nNow, we substitute the given numerical values to compute the value of $R$:\n$\\lambda = 1.30\\,\\mathrm{W\\,m^{-2}\\,K^{-1}}$\n$\\kappa = 0.70\\,\\mathrm{W\\,m^{-2}\\,K^{-1}}$\n\n$$\nR = \\frac{1.30}{1.30 + 0.70} = \\frac{1.30}{2.00} = 0.65\n$$\nThe problem requires the final answer to be rounded to four significant figures. Therefore, we express the result as:\n$$\nR = 0.6500\n$$",
            "answer": "$$\\boxed{0.6500}$$"
        },
        {
            "introduction": "With a physical basis established, we now formalize the core statistical task of an emergent constraint: using an observable quantity to reduce uncertainty in a projected one. This problem guides you through the derivation of the updated (posterior) mean and variance of Equilibrium Climate Sensitivity (ECS) after incorporating an observation of a correlated predictor, crucially accounting for measurement error. Mastering this derivation  illuminates how an observation, its uncertainty, and the inter-model relationship quantitatively combine to refine our projections.",
            "id": "4035788",
            "problem": "Consider an emergent constraint framework in climate modeling where the Equilibrium Climate Sensitivity (ECS), denoted by $Y$, and a candidate predictor of variability or feedback strength, denoted by the latent true predictor $X^{\\ast}$, are jointly distributed across an ensemble of models. Assume the following:\n\n- The pair $(X^{\\ast}, Y)$ is jointly Gaussian with mean vector $\\left(\\mu_{X}, \\mu_{Y}\\right)$ and covariance matrix\n$$\n\\begin{pmatrix}\n\\sigma_{X}^{2} & r\\,\\sigma_{X}\\sigma_{Y} \\\\\nr\\,\\sigma_{X}\\sigma_{Y} & \\sigma_{Y}^{2}\n\\end{{pmatrix}},\n$$\nwhere $r$ is the ensemble correlation between $X^{\\ast}$ and $Y$, $\\sigma_{X}^{2}$ is the ensemble variance of $X^{\\ast}$, and $\\sigma_{Y}^{2}$ is the ensemble variance of $Y$.\n\n- The observational estimate of the predictor, $X_{o}$, is given by a linear measurement model with additive error,\n$$\nX_{o} \\;=\\; X^{\\ast} \\,+\\, \\eta,\n$$\nwhere $\\eta \\sim \\mathcal{N}(0, \\sigma_{\\eta}^{2})$ is Gaussian measurement error, independent of both $X^{\\ast}$ and $Y$.\n\nAssume $\\mu_{X}$, $\\mu_{Y}$, $\\sigma_{X}^{2}$, $\\sigma_{Y}^{2}$, $r$, and $\\sigma_{\\eta}^{2}$ are known constants characterizing the model ensemble and the observing system, and $X_{o}$ is a single observed value.\n\nStarting from the definitions of covariance, correlation, and the properties of jointly Gaussian random variables under linear transformations, derive the posterior mean $\\mathbb{E}[Y \\mid X_{o}]$ and posterior variance $\\operatorname{Var}(Y \\mid X_{o})$ as analytic functions of $r$, $\\sigma_{X}^{2}$, $\\sigma_{Y}^{2}$, $\\mu_{X}$, $\\mu_{Y}$, $\\sigma_{\\eta}^{2}$, and $X_{o}$. Express the posterior mean in kelvin and the posterior variance in kelvin squared. The final answer must be a single closed-form expression listing the posterior mean and variance together as a row matrix.",
            "solution": "The problem is subjected to validation.\n\n### Step 1: Extract Givens\n- The latent variables $(X^{\\ast}, Y)$ are jointly Gaussian.\n- Mean vector: $(\\mu_{X}, \\mu_{Y})$.\n- Covariance matrix:\n$$\n\\begin{pmatrix}\n\\sigma_{X}^{2} & r\\,\\sigma_{X}\\sigma_{Y} \\\\\nr\\,\\sigma_{X}\\sigma_{Y} & \\sigma_{Y}^{2}\n\\end{pmatrix}\n$$\n- $Y$ is the Equilibrium Climate Sensitivity (ECS).\n- $X^{\\ast}$ is a latent true predictor.\n- $r$ is the ensemble correlation between $X^{\\ast}$ and $Y$.\n- $\\sigma_{X}^{2}$ is the ensemble variance of $X^{\\ast}$.\n- $\\sigma_{Y}^{2}$ is the ensemble variance of $Y$.\n- The observational estimate of the predictor is $X_{o}$.\n- Measurement model: $X_{o} = X^{\\ast} + \\eta$.\n- Measurement error $\\eta \\sim \\mathcal{N}(0, \\sigma_{\\eta}^{2})$.\n- The error $\\eta$ is independent of both $X^{\\ast}$ and $Y$.\n- $\\mu_{X}$, $\\mu_{Y}$, $\\sigma_{X}^{2}$, $\\sigma_{Y}^{2}$, $r$, and $\\sigma_{\\eta}^{2}$ are known constants.\n- $X_{o}$ is a single observed value.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is a standard exercise in multivariate statistics, specifically concerning the properties of conditional Gaussian distributions.\n- **Scientifically Grounded:** The framework described is a simplified but conceptually sound representation of an \"emergent constraint\" analysis used in climate science. The use of a linear Gaussian model is a common and valid statistical approach for such problems.\n- **Well-Posed:** The problem is well-posed. Given that all underlying random variables ($X^{\\ast}$, $Y$, $\\eta$) are Gaussian, their linear combinations and joint distributions are also Gaussian. The conditional distribution of a subset of jointly Gaussian variables, given the others, is well-defined and has a unique Gaussian form. All necessary parameters and relationships are specified.\n- **Objective:** The problem is stated in precise mathematical language without any ambiguity or subjectivity.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. A full analytical solution will be provided.\n\nThe objective is to derive the posterior mean $\\mathbb{E}[Y \\mid X_{o}]$ and posterior variance $\\operatorname{Var}(Y \\mid X_{o})$. Since $Y$ is a component of a jointly Gaussian vector $(X^{\\ast}, Y)$, and $X_{o}$ is a linear function of $X^{\\ast}$ and another independent Gaussian variable $\\eta$, the pair $(Y, X_{o})$ is also jointly Gaussian. The conditional distribution of $Y$ given $X_{o}$ will therefore be Gaussian, and our task is to find the mean and variance of this conditional distribution.\n\nLet's first determine the parameters of the joint distribution of $(Y, X_{o})$. This is a bivariate normal distribution characterized by its mean vector and covariance matrix.\n\nThe mean vector is $\\boldsymbol{\\mu} = (\\mathbb{E}[Y], \\mathbb{E}[X_{o}])^{T}$.\nThe components are:\n1.  $\\mathbb{E}[Y] = \\mu_{Y}$, which is given.\n2.  $\\mathbb{E}[X_{o}] = \\mathbb{E}[X^{\\ast} + \\eta]$. By linearity of expectation, this is $\\mathbb{E}[X^{\\ast}] + \\mathbb{E}[\\eta]$. We are given $\\mathbb{E}[X^{\\ast}] = \\mu_{X}$ and $\\mathbb{E}[\\eta] = 0$. Therefore, $\\mathbb{E}[X_{o}] = \\mu_{X}$.\n\nSo, the mean vector is $\\begin{pmatrix} \\mu_{Y} \\\\ \\mu_{X} \\end{pmatrix}$.\n\nThe covariance matrix is $\\mathbf{\\Sigma} = \\begin{pmatrix} \\operatorname{Var}(Y) & \\operatorname{Cov}(Y, X_{o}) \\\\ \\operatorname{Cov}(Y, X_{o}) & \\operatorname{Var}(X_{o}) \\end{pmatrix}$.\nLet's compute its components:\n1.  $\\operatorname{Var}(Y) = \\sigma_{Y}^{2}$, which is given.\n2.  $\\operatorname{Var}(X_{o}) = \\operatorname{Var}(X^{\\ast} + \\eta)$. Since $X^{\\ast}$ and $\\eta$ are independent, the variance of their sum is the sum of their variances:\n    $$\n    \\operatorname{Var}(X_{o}) = \\operatorname{Var}(X^{\\ast}) + \\operatorname{Var}(\\eta) = \\sigma_{X}^{2} + \\sigma_{\\eta}^{2}.\n    $$\n3.  $\\operatorname{Cov}(Y, X_{o}) = \\operatorname{Cov}(Y, X^{\\ast} + \\eta)$. By bilinearity of covariance:\n    $$\n    \\operatorname{Cov}(Y, X_{o}) = \\operatorname{Cov}(Y, X^{\\ast}) + \\operatorname{Cov}(Y, \\eta).\n    $$\n    We are given from the problem setup that $\\eta$ is independent of both $X^{\\ast}$ and $Y$. Therefore, $\\operatorname{Cov}(Y, \\eta) = 0$. This leaves:\n    $$\n    \\operatorname{Cov}(Y, X_{o}) = \\operatorname{Cov}(Y, X^{\\ast}).\n    $$\n    The covariance between $X^{\\ast}$ and $Y$ is given in the covariance matrix as $r\\sigma_{X}\\sigma_{Y}$. Thus, $\\operatorname{Cov}(Y, X_{o}) = r\\sigma_{X}\\sigma_{Y}$.\n\nThe joint distribution of $(Y, X_{o})$ is therefore a bivariate normal distribution with mean vector $\\begin{pmatrix} \\mu_{Y} \\\\ \\mu_{X} \\end{pmatrix}$ and covariance matrix:\n$$\n\\mathbf{\\Sigma} = \\begin{pmatrix}\n\\sigma_{Y}^{2} & r\\sigma_{X}\\sigma_{Y} \\\\\nr\\sigma_{X}\\sigma_{Y} & \\sigma_{X}^{2} + \\sigma_{\\eta}^{2}\n\\end{pmatrix}.\n$$\n\nNow we use the standard formulae for the conditional distribution of jointly Gaussian variables. If $(V_{1}, V_{2})$ are jointly Gaussian with mean $(\\mu_{1}, \\mu_{2})$ and covariance matrix $\\begin{pmatrix} \\Sigma_{11} & \\Sigma_{12} \\\\ \\Sigma_{21} & \\Sigma_{22} \\end{pmatrix}$, then the conditional distribution of $V_{1}$ given $V_{2} = v_{2}$ is Gaussian with:\n-   Conditional Mean: $\\mathbb{E}[V_{1} \\mid V_{2}=v_{2}] = \\mu_{1} + \\Sigma_{12}\\Sigma_{22}^{-1}(v_{2} - \\mu_{2})$\n-   Conditional Variance: $\\operatorname{Var}(V_{1} \\mid V_{2}=v_{2}) = \\Sigma_{11} - \\Sigma_{12}\\Sigma_{22}^{-1}\\Sigma_{21}$\n\nWe make the following substitutions:\n$V_{1} \\to Y$, $V_{2} \\to X_{o}$\n$\\mu_{1} \\to \\mu_{Y}$, $\\mu_{2} \\to \\mu_{X}$\n$v_{2} \\to X_{o}$ (the observed value)\n$\\Sigma_{11} \\to \\sigma_{Y}^{2}$\n$\\Sigma_{12} \\to r\\sigma_{X}\\sigma_{Y}$\n$\\Sigma_{21} \\to r\\sigma_{X}\\sigma_{Y}$\n$\\Sigma_{22} \\to \\sigma_{X}^{2} + \\sigma_{\\eta}^{2}$\n\nThe posterior mean $\\mathbb{E}[Y \\mid X_{o}]$ is:\n$$\n\\mathbb{E}[Y \\mid X_{o}] = \\mu_{Y} + (r\\sigma_{X}\\sigma_{Y})(\\sigma_{X}^{2} + \\sigma_{\\eta}^{2})^{-1}(X_{o} - \\mu_{X})\n$$\n$$\n\\mathbb{E}[Y \\mid X_{o}] = \\mu_{Y} + \\frac{r\\sigma_{X}\\sigma_{Y}}{\\sigma_{X}^{2} + \\sigma_{\\eta}^{2}}(X_{o} - \\mu_{X}).\n$$\nPhysically, the units of $\\mu_Y$ and $\\sigma_Y$ are kelvin, so this expression for the posterior mean correctly evaluates to units of kelvin.\n\nThe posterior variance $\\operatorname{Var}(Y \\mid X_{o})$ is:\n$$\n\\operatorname{Var}(Y \\mid X_{o}) = \\sigma_{Y}^{2} - (r\\sigma_{X}\\sigma_{Y})(\\sigma_{X}^{2} + \\sigma_{\\eta}^{2})^{-1}(r\\sigma_{X}\\sigma_{Y})\n$$\n$$\n\\operatorname{Var}(Y \\mid X_{o}) = \\sigma_{Y}^{2} - \\frac{(r\\sigma_{X}\\sigma_{Y})^{2}}{\\sigma_{X}^{2} + \\sigma_{\\eta}^{2}} = \\sigma_{Y}^{2} - \\frac{r^{2}\\sigma_{X}^{2}\\sigma_{Y}^{2}}{\\sigma_{X}^{2} + \\sigma_{\\eta}^{2}}\n$$\nFactoring out $\\sigma_{Y}^{2}$:\n$$\n\\operatorname{Var}(Y \\mid X_{o}) = \\sigma_{Y}^{2} \\left( 1 - \\frac{r^{2}\\sigma_{X}^{2}}{\\sigma_{X}^{2} + \\sigma_{\\eta}^{2}} \\right).\n$$\nThis expression represents the reduction in variance of $Y$ after observing $X_o$. The term $\\frac{r^{2}\\sigma_{X}^{2}}{\\sigma_{X}^{2} + \\sigma_{\\eta}^{2}}$ is the squared correlation between $Y$ and $X_o$, so the reduction in variance is proportional to this squared correlation, as expected. Given that $\\sigma_Y^2$ has units of kelvin squared, this expression for the posterior variance correctly evaluates to units of kelvin squared.\n\nThe two required quantities are the posterior mean and posterior variance. They are presented together as a row matrix in the final answer.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\mu_{Y} + \\frac{r\\sigma_{X}\\sigma_{Y}}{\\sigma_{X}^{2} + \\sigma_{\\eta}^{2}}(X_{o} - \\mu_{X}) & \\sigma_{Y}^{2} \\left( 1 - \\frac{r^{2}\\sigma_{X}^{2}}{\\sigma_{X}^{2} + \\sigma_{\\eta}^{2}} \\right) \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "A strong correlation in a model ensemble does not guarantee a physically meaningful constraint; it could arise from statistical chance, especially when multiple predictors or model forms are tested. This exercise challenges you to think critically about the statistical validation of an emergent constraint by designing a \"negative control\" test. By evaluating different strategies to estimate the false-positive rate of a discovery pipeline , you will learn to assess the statistical robustness of published claims and appreciate the methodological rigor required in this field.",
            "id": "4035768",
            "problem": "Consider an ensemble of $M$ global climate models (GCMs), with $M = 28$. For each model $i \\in \\{1,\\dots,M\\}$, let $E_i$ denote equilibrium climate sensitivity (ECS), defined as the equilibrium increase in global mean surface temperature in response to a doubling of atmospheric carbon dioxide, and let $X_i$ denote a present-day diagnostic observable derived from shortwave cloud metrics in subtropical regions that has been proposed as a predictor for $E_i$ in an emergent constraint framework. Suppose the emergent constraint pipeline under consideration selects among $K$ candidate functional forms $f_k(\\cdot)$ applied to $X_i$ (such as standardized transforms, linear and logarithmic forms), fits a linear regression across models for each candidate, and retains the form with the smallest $p$-value for the slope. The pipeline uses a nominal significance threshold $\\alpha = 0.05$ to declare a statistically significant emergent constraint and then maps the retained cross-model relationship to an inferred constraint on equilibrium climate sensitivity. You wish to perform a negative control test to empirically estimate the false-positive rate of this discovery pipeline by introducing a pseudo-predictor that is unlinked to feedback physics and repeating the entire selection and testing procedure $R$ times, with $R = 2000$. The goal is to obtain an unbiased estimate of the Type I error rate under the null hypothesis that no physically meaningful link exists between the pseudo-predictor and $E_i$, while respecting the statistical properties of the models, including inter-model dependence.\n\nFrom first principles of statistical inference, under a null hypothesis $H_0$ in which the regression slope is zero, and assuming the test is correctly calibrated, the $p$-value is uniformly distributed on $\\left[0,1\\right]$ when model errors satisfy the test assumptions. In emergent constraint discovery, multiple testing across $K$ candidate forms can inflate the false-positive rate relative to $\\alpha$ if the minimum $p$-value is used without correction. Inter-model dependence (e.g., shared code ancestry and structural similarity) reduces the effective degrees of freedom relative to $M$, making naive cross-model regression $p$-values potentially anti-conservative. A scientifically sound negative control should satisfy two requirements: the pseudo-predictor must be statistically independent of $E_i$ and must mimic the distributional properties of $X_i$ across models sufficiently to exercise the pipeline as implemented, including its model dependence handling and selection steps.\n\nWhich of the following negative control designs, and corresponding false-positive estimation procedure, most appropriately delivers an unbiased estimate of the discovery pipeline’s false-positive rate?\n\nA. For each replicate $r \\in \\{1,\\dots,R\\}$, generate a pseudo-predictor $Z_i^{(r)}$ by independently drawing across models from a Gaussian distribution $\\mathcal{N}(\\mu_X,\\sigma_X^2)$ that matches the across-model mean $\\mu_X$ and variance $\\sigma_X^2$ of $X_i$ but is independent of $E_i$. For each $r$, run the full pipeline: evaluate all $K$ candidate forms $f_k(Z_i^{(r)})$, select the form with the minimum regression $p$-value for the slope in a cross-model weighted least squares fit that uses genealogy-aware weights to account for inter-model dependence, and record whether the minimum $p$-value is below $\\alpha$. Estimate the false-positive rate as the fraction of replicates with discovery, $\\hat{\\mathrm{FPR}} = \\frac{1}{R}\\sum_{r=1}^{R} \\mathbf{1}\\left\\{\\min_{k} p_k^{(r)} < \\alpha\\right\\}$.\n\nB. Use the observed global mean surface temperature trend over the period $1979$–$2010$ as the pseudo-predictor for each model, arguing that present-day forced warming is only weakly related to long-term climate sensitivity. Fit a single linear regression of $E_i$ on this pseudo-predictor across models, without any weighting for inter-model dependence, and compute the $p$-value once. Declare the false-positive rate to be equal to this $p$-value because, under $H_0$, $p$ represents the probability of a false discovery.\n\nC. Construct negative controls by permuting the ECS labels $\\{E_i\\}$ across models uniformly at random to obtain $E_{\\pi(i)}^{(r)}$ for replicate $r$, fit a single linear regression of $E_{\\pi(i)}^{(r)}$ on the original $X_i$ without re-running the candidate-form selection or accounting for inter-model dependence, and report the nominal false-positive rate $\\alpha$ as the expected rate of false discoveries, asserting that permutation guarantees $p$-values uniformly distributed on $\\left[0,1\\right]$.\n\nD. Create pseudo-predictors by shuffling the indices of $X_i$ across models to obtain $Z_i^{(r)} = X_{\\pi(i)}$, then for each replicate $r$ construct $100$ additional pseudo-predictors by random linear combinations of $Z_i^{(r)}$, select the single smallest $p$-value among these, and define the false-positive rate as this minimum $p$-value because it quantifies the most extreme null-case evidence against $H_0$.\n\nSelect the option that correctly specifies a negative control procedure using a pseudo-predictor unlinked to feedback physics and a valid method to assess the false-positive rate for the entire emergent constraint discovery pipeline.",
            "solution": "The user requires a critical validation of the problem statement, followed by a detailed solution and evaluation of the provided options, all adhering to strict formatting and rigor standards.\n\n### Step 1: Extract Givens\n\n-   **Ensemble**: $M = 28$ global climate models (GCMs).\n-   **Variables per model $i$**:\n    -   $E_i$: Equilibrium Climate Sensitivity (ECS).\n    -   $X_i$: A present-day diagnostic observable from shortwave cloud metrics, a potential predictor for $E_i$.\n-   **Emergent Constraint Pipeline**:\n    1.  A set of $K$ candidate functional forms, $f_k(\\cdot)$, are applied to the predictor $X_i$.\n    2.  For each form $f_k(X_i)$, a linear regression of $E_i$ on $f_k(X_i)$ is performed across the $M$ models.\n    3.  The functional form yielding the minimum $p$-value for the regression slope is selected.\n    4.  A \"discovery\" is declared if this minimum $p$-value is less than the nominal significance threshold, $\\alpha = 0.05$.\n-   **Negative Control Test Objective**:\n    -   To empirically estimate the false-positive rate (Type I error rate) of the entire discovery pipeline.\n    -   The test uses pseudo-predictors that are, by construction, unlinked to the underlying feedback physics of $E_i$.\n    -   The procedure is repeated $R = 2000$ times.\n    -   The procedure must respect the statistical properties of the GCM ensemble, including inter-model dependence.\n-   **Key Statistical Context**:\n    -   The null hypothesis, $H_0$, is that the regression slope is zero.\n    -   Multiple testing across $K$ forms inflates the false-positive rate above $\\alpha$.\n    -   Inter-model dependence reduces the effective degrees of freedom, which can make uncorrected $p$-values anti-conservative (i.e., smaller than they should be).\n    -   A valid negative control must use a pseudo-predictor that is (1) statistically independent of $E_i$ and (2) mimics the distributional properties of $X_i$ to ensure the pipeline is tested realistically.\n-   **Question**: Identify the most appropriate negative control design and false-positive estimation procedure from the given options.\n\n### Step 2: Validate Using Extracted Givens\n\n-   **Scientific Groundedness**: The problem is firmly grounded in the field of climate science and statistics. Emergent constraints, Equilibrium Climate Sensitivity (ECS), the use of GCM ensembles (like those in CMIP), accounting for inter-model dependence, and concern over false-positive rates due to multiple testing are all current and critical topics in the scientific literature. The setup is scientifically realistic and does not contain pseudoscience.\n-   **Well-Posedness**: The problem is well-posed. It asks for the evaluation of several proposed statistical procedures against a set of clearly defined criteria for estimating the false-positive rate of a specific data analysis pipeline. A single best option can be determined based on established principles of statistical inference and Monte Carlo simulation.\n-   **Objectivity**: The problem is stated in precise, objective, and technical language, free from subjective or biased phrasing.\n-   **Completeness and Consistency**: The problem statement is self-contained. It provides a complete description of the data analysis pipeline to be tested and the goals of the negative control procedure. The challenges mentioned (multiple testing, inter-model dependence) are consistent with the overall goal and are crucial for evaluating the options.\n-   **Realism and Feasibility**: The described procedure is a standard computational statistics exercise (a parametric or non-parametric bootstrap/simulation) that is entirely feasible. The numerical values ($M=28$, $R=2000$, $\\alpha=0.05$) are typical for such studies.\n-   **Other Flaws**: The problem is not trivial, ill-posed, or pseudo-profound. It addresses a substantive methodological challenge in a complex scientific domain.\n\n### Step 3: Verdict and Action\n\nThe problem statement is **valid**. It is a well-formulated question about statistical methodology in climate science. I will now proceed with the derivation and evaluation of options.\n\n### Derivation of the Correct Approach\n\nThe objective is to estimate the false-positive rate (FPR) of a multi-step discovery pipeline. The FPR, or Type I error rate, is the probability of rejecting the null hypothesis ($H_0$) when $H_0$ is true. Here, $H_0$ is the hypothesis that there is no true physical relationship between the predictor variable and ECS. The pipeline's complexity arises from two main sources:\n1.  **Selection Bias/Multiple Testing**: The pipeline does not test a single, pre-specified relationship. Instead, it tries $K$ different relationships and selects the one that looks most significant (i.e., has the minimum $p$-value). This \"cherry-picking\" process inherently inflates the probability of finding a small $p$-value by chance, causing the true FPR to be higher than the nominal threshold $\\alpha$.\n2.  **Inter-model Dependence**: The data points $\\{ (X_i, E_i) \\}_{i=1}^M$ are not independent draws from a population. GCMs often share code, parameterizations, and structural components, leading to correlated errors in the regression. This violates the independence assumption of standard ordinary least squares regression and can lead to incorrectly estimated $p$-values if not properly handled.\n\nA valid negative control test must therefore accomplish the following:\n1.  **Simulate the Null Hypothesis**: It must generate data for which $H_0$ is true by construction. This means creating pairs of (pseudo-predictor, response) variables that have no statistical relationship.\n2.  **Replicate the Entire Pipeline**: The generated null data must be passed through the *exact* same pipeline that is used for the real data. This includes applying all $K$ functional forms, performing the regressions (with any weighting for dependence), finding the minimum $p$-value, and comparing it to $\\alpha$. Any simplification of the pipeline will result in an estimate of the FPR for the wrong procedure.\n3.  **Estimate the Rate via Monte Carlo Simulation**: The process must be repeated a large number of times ($R$). The FPR is then estimated as the fraction of these repetitions that result in a false discovery (i.e., $\\min_{k} p_k < \\alpha$).\n    $$ \\hat{\\mathrm{FPR}} = \\frac{\\text{Number of replicates with } \\min_{k} p_k < \\alpha}{R} $$\n\nWith these principles in mind, we evaluate the options.\n\n### Option-by-Option Analysis\n\n**A. For each replicate $r \\in \\{1,\\dots,R\\}$, generate a pseudo-predictor $Z_i^{(r)}$ by independently drawing across models from a Gaussian distribution $\\mathcal{N}(\\mu_X,\\sigma_X^2)$ that matches the across-model mean $\\mu_X$ and variance $\\sigma_X^2$ of $X_i$ but is independent of $E_i$. For each $r$, run the full pipeline: evaluate all $K$ candidate forms $f_k(Z_i^{(r)})$, select the form with the minimum regression $p$-value for the slope in a cross-model weighted least squares fit that uses genealogy-aware weights to account for inter-model dependence, and record whether the minimum $p$-value is below $\\alpha$. Estimate the false-positive rate as the fraction of replicates with discovery, $\\hat{\\mathrm{FPR}} = \\frac{1}{R}\\sum_{r=1}^{R} \\mathbf{1}\\left\\{\\min_{k} p_k^{(r)} < \\alpha\\right\\}$.**\n\n-   **Simulating the Null**: This procedure generates a pseudo-predictor $Z^{(r)}$ that is, by construction, statistically independent of the ECS values $E_i$. This correctly establishes the null hypothesis. Generating from a Gaussian distribution that matches the first two moments is a standard, acceptable way to create a null predictor, although non-parametric methods like permutation of $X_i$ could also be used.\n-   **Replicating the Pipeline**: This option correctly specifies that the *full pipeline* must be run for each replicate. It explicitly mentions evaluating all $K$ forms, selecting the minimum $p$-value, and using the specified weighted least squares method. This is crucial for capturing the FPR inflation from both multiple testing and the effects of the dependence-correction scheme.\n-   **Estimating the Rate**: The formula $\\hat{\\mathrm{FPR}} = \\frac{1}{R}\\sum_{r=1}^{R} \\mathbf{1}\\left\\{\\min_{k} p_k^{(r)} < \\alpha\\right\\}$ is the correct mathematical definition of a Monte Carlo estimate for the false-positive rate of this specific discovery pipeline.\n\n**Verdict**: **Correct**. This option describes a statistically sound procedure for empirically estimating the false-positive rate of the described pipeline, adhering to all the necessary principles.\n\n**B. Use the observed global mean surface temperature trend over the period $1979$–$2010$ as the pseudo-predictor for each model, arguing that present-day forced warming is only weakly related to long-term climate sensitivity. Fit a single linear regression of $E_i$ on this pseudo-predictor across models, without any weighting for inter-model dependence, and compute the $p$-value once. Declare the false-positive rate to be equal to this $p$-value because, under $H_0$, $p$ represents the probability of a false discovery.**\n\n-   **Simulating the Null**: The null hypothesis is not guaranteed to be true. The argument that the physical link is \"weak\" is an assertion, not a constructive guarantee of independence required for a true negative control. There might be a real, albeit weak, relationship.\n-   **Replicating the Pipeline**: This fails completely. It tests only a *single* regression, ignoring the critical pipeline step of selecting from $K$ candidates, which is a primary reason for needing the control in the first place. It also ignores the specified procedure for handling inter-model dependence.\n-   **Estimating the Rate**: This option displays a fundamental misunderstanding of statistical terms. A $p$-value is a statistic calculated from a single sample; a false-positive rate is a long-run property of a testing procedure. They are not the same. Equating the two is a severe conceptual error.\n\n**Verdict**: **Incorrect**. This procedure is flawed in its premise, its execution, and its interpretation of the result.\n\n**C. Construct negative controls by permuting the ECS labels $\\{E_i\\}$ across models uniformly at random to obtain $E_{\\pi(i)}^{(r)}$ for replicate $r$, fit a single linear regression of $E_{\\pi(i)}^{(r)}$ on the original $X_i$ without re-running the candidate-form selection or accounting for inter-model dependence, and report the nominal false-positive rate $\\alpha$ as the expected rate of false discoveries, asserting that permutation guarantees $p$-values uniformly distributed on $\\left[0,1\\right]$.**\n\n-   **Simulating the Null**: Permuting the response variable ($E_i$) relative to the predictor ($X_i$) is a valid non-parametric method for breaking the association and simulating the null hypothesis.\n-   **Replicating the Pipeline**: This is a critical failure. Like option B, it ignores the pipeline's multiple testing aspect by fitting only a \"single linear regression\" and not re-running the selection from $K$ forms. It also explicitly omits the component for handling inter-model dependence. Thus, it does not test the pipeline in question.\n-   **Estimating the Rate**: It incorrectly reports the *nominal* rate $\\alpha$ as the answer. The entire purpose of the exercise is to determine if the *actual* rate differs from $\\alpha$. Furthermore, while permutation under the null (with exchangeable errors) produces a uniform $p$-value distribution for a *single* test, it does not do so for the *minimum* of several tests. The distribution of the minimum of $K$ $p$-values is not uniform on $[0,1]$.\n\n**Verdict**: **Incorrect**. While it starts with a valid technique for generating a null instance (permutation), it fails to apply it to the actual pipeline and misinterprets the expected outcome.\n\n**D. Create pseudo-predictors by shuffling the indices of $X_i$ across models to obtain $Z_i^{(r)} = X_{\\pi(i)}$, then for each replicate $r$ construct $100$ additional pseudo-predictors by random linear combinations of $Z_i^{(r)}$, select the single smallest $p$-value among these, and define the false-positive rate as this minimum $p$-value because it quantifies the most extreme null-case evidence against $H_0$.**\n\n-   **Simulating the Null**: Shuffling the predictor indices ($X_{\\pi(i)}$) is a valid way to simulate the null hypothesis, equivalent to permuting the response in option C. However, the subsequent step of creating \"100 additional pseudo-predictors by random linear combinations\" is an arbitrary and statistically unjustifiable procedure.\n-   **Replicating the Pipeline**: This option invents a new, different pipeline involving 100+ random predictors. It does not test the pipeline described in the problem, which involves $K$ *functional forms* of a *single* predictor.\n-   **Estimating the Rate**: Like option B, it fundamentally confuses a single $p$-value with a false-positive rate. The FPR is a rate estimated over many trials, not a single minimum $p$-value from one trial.\n\n**Verdict**: **Incorrect**. This option describes a confusing and statistically invalid procedure that does not address the question asked.",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}