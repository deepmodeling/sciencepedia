{
    "hands_on_practices": [
        {
            "introduction": "To effectively improve forecasts, we must quantify the value of each observation. Forecast Sensitivity to Observation Impact (FSOI) is a powerful diagnostic for this purpose, but it relies on linear assumptions. This exercise provides a foundational understanding by asking you to derive the FSOI analytically in an idealized linear model and compare it to the \"true\" impact from an Observing System Experiment (OSE), revealing the conditions under which these two measures are equivalent .",
            "id": "4071087",
            "problem": "Consider a one-dimensional periodic domain $x \\in [0,L]$ with $L>0$ and a perfect linear advection model governed by $\\frac{\\partial u}{\\partial t} + c \\frac{\\partial u}{\\partial x} = 0$, where $c>0$ is constant. Assume the true state $u^{\\mathrm{true}}(x,t)$ is dominated by a single spatial Fourier mode at wavenumber $k = \\frac{2\\pi}{L}$, so that the forecast and analysis can be represented in terms of a single scalar amplitude $a(t)$ multiplying a fixed spatial pattern $\\phi(x) = \\cos\\!\\left(\\frac{2\\pi x}{L}\\right)$. The advection model preserves the amplitude of this mode, so $a(t)$ is constant in time under the forecast model.\n\nSuppose a single instantaneous point observation of the state is made at location $x_0 = \\frac{L}{3}$ and time $t_0>0$. The observation model is linear, $y = h\\,a(t_0) + \\varepsilon$, where $h = \\phi(x_0)$ and $\\varepsilon$ is zero-mean Gaussian noise with variance $R>0$. The background (prior) amplitude error at the analysis time $t_0$ is zero mean with variance $\\sigma_b^2>0$, and the data assimilation uses an optimal linear update consistent with a Gaussian background and observation errors.\n\nDefine the forecast verification metric at time $T>t_0$ by the spatially integrated squared error norm,\n$$\nJ_f = \\frac{1}{2} \\int_{0}^{L} \\left(u(x,T) - u^{\\mathrm{true}}(x,T)\\right)^{2} \\,\\mathrm{d}x,\n$$\nand note that for the single-mode representation and perfect advection, $u(x,T) - u^{\\mathrm{true}}(x,T) = e_a\\,\\phi(x - cT)$ with $e_a$ the scalar amplitude error at $t_0$, implying $J_f = \\frac{1}{2}\\,\\alpha\\, e_a^{2}$ where $\\alpha = \\int_{0}^{L} \\phi(x)^{2}\\,\\mathrm{d}x$.\n\nLet Forecast Sensitivity to Observation Impact (FSOI) be defined here as the expected reduction in the forecast error norm due to assimilating the single observation, that is the difference between the expected value of $J_f$ computed from the background variance and the expected value of $J_f$ computed from the analysis variance. Let an Observing System Experiment (OSE) skill difference be defined as the expected difference in $J_f$ between two assimilation configurations: one that includes the observation and one that excludes it.\n\nUsing the context above and starting from fundamental properties of linear advection and optimal linear Gaussian estimation, do the following:\n\n- Derive an analytical expression for the FSOI in terms of $\\sigma_b^2$, $R$, and $h$.\n- Derive the OSE-based expected skill difference in terms of the same quantities.\n- Compute the ratio of the FSOI to the OSE-based expected skill difference for the specific parameter values $L = 1000$, $x_0 = \\frac{L}{3}$, $k = \\frac{2\\pi}{L}$, $c = 10$, $\\sigma_b^2 = 4$, and $R = 1$. Use $\\phi(x) = \\cos\\!\\left(\\frac{2\\pi x}{L}\\right)$.\n\nExpress the final answer as a dimensionless decimal. No rounding instruction is necessary for this problem.",
            "solution": "The objective is to derive expressions for the Forecast Sensitivity to Observation Impact (FSOI) and the Observing System Experiment (OSE) skill difference based on the provided definitions, and then to compute their ratio. The problem is set in the context of a simplified one-dimensional linear advection model with a single Fourier mode representation.\n\nFirst, we analyze the data assimilation step, which is a scalar optimal linear estimation problem. The state is described by a single amplitude, $a$. The background estimate of this amplitude, $a_b$, has an associated error, $e_b = a_b - a_{\\mathrm{true}}$, with zero mean, $E[e_b] = 0$, and variance $\\sigma_b^2 = E[e_b^2]$.\n\nAn observation $y$ is made at time $t_0$, with the linear observation model $y = h a_{\\mathrm{true}} + \\varepsilon$. The observation operator for the amplitude is $h = \\phi(x_0)$, where $\\phi(x) = \\cos\\left(\\frac{2\\pi x}{L}\\right)$ and the observation location is $x_0 = \\frac{L}{3}$. The observation error $\\varepsilon$ is a zero-mean Gaussian random variable with variance $R$. We are given that the background and observation errors are uncorrelated, i.e., $E[e_b \\varepsilon] = 0$.\n\nThe analysis amplitude, $a_a$, is obtained by updating the background with the observation:\n$$\na_a = a_b + K (y - h a_b)\n$$\nwhere $K$ is the Kalman gain. The analysis error is $e_a = a_a - a_{\\mathrm{true}}$. Substituting the expressions for $a_a$ and $y$:\n$$\ne_a = \\left[a_b + K (h a_{\\mathrm{true}} + \\varepsilon - h a_b)\\right] - a_{\\mathrm{true}}\n$$\n$$\ne_a = (a_b - a_{\\mathrm{true}}) - K h (a_b - a_{\\mathrm{true}}) + K \\varepsilon\n$$\n$$\ne_a = (1 - K h) e_b + K \\varepsilon\n$$\nThe analysis error variance, $\\sigma_a^2 = E[e_a^2]$, is then:\n$$\n\\sigma_a^2 = E\\left[((1 - K h) e_b + K \\varepsilon)^2\\right] = E\\left[(1 - K h)^2 e_b^2 + 2 K (1 - K h) e_b \\varepsilon + K^2 \\varepsilon^2\\right]\n$$\nDue to the uncorrelation of $e_b$ and $\\varepsilon$, $E[e_b \\varepsilon] = 0$, so the expression simplifies to:\n$$\n\\sigma_a^2 = (1 - K h)^2 E[e_b^2] + K^2 E[\\varepsilon^2] = (1 - K h)^2 \\sigma_b^2 + K^2 R\n$$\nTo find the optimal gain $K$ that minimizes $\\sigma_a^2$, we differentiate with respect to $K$ and set the result to zero:\n$$\n\\frac{\\mathrm{d}\\sigma_a^2}{\\mathrm{d}K} = 2(1 - K h)(-h)\\sigma_b^2 + 2 K R = -2h\\sigma_b^2 + 2K h^2 \\sigma_b^2 + 2KR = 0\n$$\n$$\nK(h^2 \\sigma_b^2 + R) = h \\sigma_b^2\n$$\nThis gives the optimal Kalman gain:\n$$\nK = \\frac{h \\sigma_b^2}{h^2 \\sigma_b^2 + R}\n$$\nSubstituting this optimal gain back into the expression for $\\sigma_a^2$:\n$$\n\\sigma_a^2 = \\left(1 - \\frac{h^2 \\sigma_b^2}{h^2 \\sigma_b^2 + R}\\right)^2 \\sigma_b^2 + \\left(\\frac{h \\sigma_b^2}{h^2 \\sigma_b^2 + R}\\right)^2 R\n$$\n$$\n\\sigma_a^2 = \\left(\\frac{R}{h^2 \\sigma_b^2 + R}\\right)^2 \\sigma_b^2 + \\frac{h^2 (\\sigma_b^2)^2 R}{(h^2 \\sigma_b^2 + R)^2}\n$$\n$$\n\\sigma_a^2 = \\frac{R^2 \\sigma_b^2 + h^2 (\\sigma_b^2)^2 R}{(h^2 \\sigma_b^2 + R)^2} = \\frac{R \\sigma_b^2 (R + h^2 \\sigma_b^2)}{(h^2 \\sigma_b^2 + R)^2} = \\frac{R \\sigma_b^2}{h^2 \\sigma_b^2 + R}\n$$\nThis is the variance of the analysis error amplitude.\n\nNext, we relate this to the forecast verification metric $J_f$. The problem states $J_f = \\frac{1}{2} \\alpha e^2$, where $e$ is the amplitude error at the analysis time $t_0$, and $\\alpha = \\int_0^L \\phi(x)^2 \\,\\mathrm{d}x$. We compute $\\alpha$:\n$$\n\\alpha = \\int_0^L \\cos^2\\left(\\frac{2\\pi x}{L}\\right) \\,\\mathrm{d}x = \\int_0^L \\frac{1}{2}\\left(1 + \\cos\\left(\\frac{4\\pi x}{L}\\right)\\right) \\,\\mathrm{d}x\n$$\n$$\n\\alpha = \\frac{1}{2}\\left[x + \\frac{L}{4\\pi}\\sin\\left(\\frac{4\\pi x}{L}\\right)\\right]_0^L = \\frac{1}{2}\\left(L + 0 - 0\\right) = \\frac{L}{2}\n$$\nThe expected value of $J_f$ is $E[J_f] = E[\\frac{1}{2} \\alpha e^2] = \\frac{1}{2} \\alpha E[e^2]$. For a forecast initialized from a state with error variance $\\sigma^2$, the expected metric is $E[J_f] = \\frac{1}{2} \\alpha \\sigma^2$.\n\nThe expected value of $J_f$ for a forecast initialized from the background state is therefore based on the background error variance $\\sigma_b^2$:\n$$\nE[J_f(\\text{background})] = \\frac{1}{2} \\alpha \\sigma_b^2\n$$\nThe expected value of $J_f$ for a forecast initialized from the analysis state is based on the analysis error variance $\\sigma_a^2$:\n$$\nE[J_f(\\text{analysis})] = \\frac{1}{2} \\alpha \\sigma_a^2 = \\frac{1}{2} \\alpha \\frac{R \\sigma_b^2}{h^2 \\sigma_b^2 + R}\n$$\n\nNow we derive the expression for FSOI. According to the problem's definition, FSOI is the difference between the expected value of $J_f$ computed from the background variance and that computed from the analysis variance.\n$$\n\\text{FSOI} = E[J_f(\\text{background})] - E[J_f(\\text{analysis})] = \\frac{1}{2} \\alpha \\sigma_b^2 - \\frac{1}{2} \\alpha \\sigma_a^2\n$$\n$$\n\\text{FSOI} = \\frac{1}{2} \\alpha (\\sigma_b^2 - \\sigma_a^2)\n$$\nSubstituting the expression for $\\sigma_a^2$:\n$$\n\\text{FSOI} = \\frac{1}{2} \\alpha \\left(\\sigma_b^2 - \\frac{R \\sigma_b^2}{h^2 \\sigma_b^2 + R}\\right) = \\frac{1}{2} \\alpha \\sigma_b^2 \\left(1 - \\frac{R}{h^2 \\sigma_b^2 + R}\\right)\n$$\n$$\n\\text{FSOI} = \\frac{1}{2} \\alpha \\sigma_b^2 \\left(\\frac{h^2 \\sigma_b^2 + R - R}{h^2 \\sigma_b^2 + R}\\right) = \\frac{1}{2} \\alpha \\frac{h^2 (\\sigma_b^2)^2}{h^2 \\sigma_b^2 + R}\n$$\nThis is the analytical expression for FSOI.\n\nNext, we derive the OSE-based expected skill difference. This is defined as the expected difference in $J_f$ between a configuration that excludes the observation and one that includes it. For a single realization, the error in the \"exclude\" run is $e_b$ and the error in the \"include\" run is $e_a$. The skill difference for this realization is $J_f(\\text{exclude}) - J_f(\\text{include}) = \\frac{1}{2}\\alpha e_b^2 - \\frac{1}{2}\\alpha e_a^2$. The OSE skill difference is the expectation of this quantity over all realizations:\n$$\n\\text{OSE} = E\\left[\\frac{1}{2}\\alpha e_b^2 - \\frac{1}{2}\\alpha e_a^2\\right]\n$$\nBy linearity of expectation:\n$$\n\\text{OSE} = \\frac{1}{2}\\alpha E[e_b^2] - \\frac{1}{2}\\alpha E[e_a^2] = \\frac{1}{2}\\alpha (\\sigma_b^2 - \\sigma_a^2)\n$$\nThis expression is identical to the one derived for FSOI. Therefore:\n$$\n\\text{OSE} = \\frac{1}{2} \\alpha \\frac{h^2 (\\sigma_b^2)^2}{h^2 \\sigma_b^2 + R}\n$$\nThe problem's specific definitions for FSOI and OSE skill difference render them mathematically equivalent in this idealized context.\n\nFinally, we compute the ratio of FSOI to the OSE-based skill difference. Since the analytical expressions for both quantities are identical, their ratio is necessarily $1$.\n$$\n\\frac{\\text{FSOI}}{\\text{OSE}} = \\frac{\\frac{1}{2} \\alpha \\frac{h^2 (\\sigma_b^2)^2}{h^2 \\sigma_b^2 + R}}{\\frac{1}{2} \\alpha \\frac{h^2 (\\sigma_b^2)^2}{h^2 \\sigma_b^2 + R}} = 1\n$$\nThis result is independent of the specific parameter values given in the problem ($L = 1000$, $x_0 = \\frac{L}{3}$, $\\sigma_b^2 = 4$, $R = 1$, etc.), provided that the denominators are non-zero, which is guaranteed as $h^2 \\sigma_b^2 \\ge 0$ and $R>0$. The ratio is a constant value of $1$.",
            "answer": "$$\\boxed{1}$$"
        },
        {
            "introduction": "Real-world atmospheric dynamics are profoundly nonlinear, a reality that can challenge the predictions of linear impact assessment tools. This practice explores the limits of the linear approximation by examining a simple but strongly nonlinear forecast model. You will see firsthand how the impact of an observation can deviate from linear predictions, even changing from beneficial to detrimental depending on its value, a critical lesson for interpreting sensitivity studies in complex systems .",
            "id": "4071058",
            "problem": "Consider a scalar forecast–analysis system designed to study forecast sensitivity to observation impact (FSOI) and adaptive observing under strong nonlinearity. Let the scalar state be $x \\in \\mathbb{R}$ with background $x_b = 0$. A single scalar observation $y \\in \\mathbb{R}$ observes the state through a linear observation operator $H(x) = x$ with observation-error variance $\\sigma_o^2 = 1$. The background-error variance is $\\sigma_b^2 = 1$, so that the incremental three-dimensional variational (3D-Var) analysis update computed at $x_b$ by the standard linearized Gaussian formula is\n$$\n\\delta x_a \\equiv x_a - x_b = K \\,\\varepsilon,\n$$\nwhere the innovation is $\\varepsilon \\equiv y - H(x_b)$ and the analysis gain is $K \\equiv \\frac{\\sigma_b^2}{\\sigma_b^2 + \\sigma_o^2}$.\n\nDefine the scalar nonlinear forecast mapping by $M(x) = x - x^3$, which represents a simple saturating growth process. The scalar forecast-verification quantity is $z = 1$. Define the forecast metric\n$$\nJ_f(x) \\equiv \\frac{1}{2}\\,\\big(M(x) - z\\big)^2,\n$$\nwhich is a standard squared-error-type target functional. The FSOI linear estimate at $x_b$ is based on the first-order variation $\\delta J_f \\approx \\nabla J_f(x_b) \\,\\delta x_a$ computed with the tangent-linear/adjoint chain.\n\nTasks:\n1. Starting from the definitions above and the multivariate Taylor expansion specialized to the scalar case, derive the third-order expansion of the true forecast-metric change $\\Delta J_f(\\varepsilon) \\equiv J_f(x_a) - J_f(x_b)$ in powers of the innovation $\\varepsilon$, expressed as\n$$\n\\Delta J_f(\\varepsilon) = A\\,\\varepsilon + B\\,\\varepsilon^2 + C\\,\\varepsilon^3 + \\mathcal{O}(\\varepsilon^4),\n$$\nand determine $A$, $B$, and $C$ in terms of $K$, $z$, and the derivatives of $M$ at $x_b$.\n2. Using the specific values $\\sigma_b^2 = \\sigma_o^2 = 1$ (hence $K=\\tfrac{1}{2}$), $x_b=0$, $M(x)=x - x^3$, and $z=1$, compute the explicit cubic approximation $\\Delta J_f(\\varepsilon)$ and show analytically that linear superposition fails, that is, $\\Delta J_f(\\varepsilon_1 + \\varepsilon_2) \\neq \\Delta J_f(\\varepsilon_1) + \\Delta J_f(\\varepsilon_2)$ in general.\n3. With the same parameter values as in Task 2, determine the smallest positive innovation amplitude $\\varepsilon^{\\star} > 0$ at which the sign of the forecast-metric change switches, that is, the smallest positive root of $\\Delta J_f(\\varepsilon) = 0$ beyond the trivial root at $\\varepsilon=0$. Give your final answer for $\\varepsilon^{\\star}$ as a single exact closed-form expression. Because an exact expression is requested, no rounding is needed. The innovation $\\varepsilon$ is dimensionless, so express your final result as a dimensionless number.",
            "solution": "The problem statement is evaluated to be valid. It is scientifically grounded in the principles of data assimilation and forecast sensitivity, using a standard, albeit simplified, nonlinear model. The given parameters and definitions are self-contained, consistent, and well-posed, allowing for a unique and meaningful solution. The tasks are objective and mathematically rigorous.\n\nWe begin by addressing the three tasks in sequence.\n\n### Task 1: Derivation of the Taylor expansion\nThe true forecast-metric change is defined as $\\Delta J_f(\\varepsilon) \\equiv J_f(x_a) - J_f(x_b)$. The analysis state $x_a$ is given by the update formula $x_a = x_b + \\delta x_a = x_b + K\\varepsilon$.\nSubstituting this into the definition of $\\Delta J_f(\\varepsilon)$ gives:\n$$\n\\Delta J_f(\\varepsilon) = J_f(x_b + K\\varepsilon) - J_f(x_b)\n$$\nWe seek a third-order Taylor series expansion of this expression in powers of the innovation $\\varepsilon$ around $\\varepsilon = 0$. Let $g(\\varepsilon) = J_f(x_b + K\\varepsilon)$. The Taylor expansion of $\\Delta J_f(\\varepsilon) = g(\\varepsilon) - g(0)$ around $\\varepsilon=0$ is:\n$$\n\\Delta J_f(\\varepsilon) = g'(0)\\varepsilon + \\frac{g''(0)}{2!}\\varepsilon^2 + \\frac{g'''(0)}{3!}\\varepsilon^3 + \\mathcal{O}(\\varepsilon^4)\n$$\nThe coefficients are $A = g'(0)$, $B = \\frac{1}{2}g''(0)$, and $C = \\frac{1}{6}g'''(0)$. We compute the derivatives of $g(\\varepsilon)$ with respect to $\\varepsilon$ using the chain rule:\n$$\ng'(\\varepsilon) = \\frac{d}{d\\varepsilon} J_f(x_b + K\\varepsilon) = J_f'(x_b + K\\varepsilon) \\cdot K\n$$\n$$\ng''(\\varepsilon) = \\frac{d}{d\\varepsilon} [K J_f'(x_b + K\\varepsilon)] = J_f''(x_b + K\\varepsilon) \\cdot K^2\n$$\n$$\ng'''(\\varepsilon) = \\frac{d}{d\\varepsilon} [K^2 J_f''(x_b + K\\varepsilon)] = J_f'''(x_b + K\\varepsilon) \\cdot K^3\n$$\nEvaluating these at $\\varepsilon = 0$ (which corresponds to evaluating the derivatives of $J_f$ at $x=x_b$):\n$$\ng'(0) = K J_f'(x_b)\n$$\n$$\ng''(0) = K^2 J_f''(x_b)\n$$\n$$\ng'''(0) = K^3 J_f'''(x_b)\n$$\nNext, we find the first three derivatives of the forecast metric $J_f(x) = \\frac{1}{2}(M(x) - z)^2$ with respect to $x$:\n$$\nJ_f'(x) = \\frac{dJ_f}{dx} = (M(x) - z) M'(x)\n$$\n$$\nJ_f''(x) = \\frac{d^2J_f}{dx^2} = [M'(x)]^2 + (M(x) - z) M''(x)\n$$\n$$\nJ_f'''(x) = \\frac{d^3J_f}{dx^3} = 2M'(x)M''(x) + M''(x)M'(x) + (M(x) - z)M'''(x) = 3M'(x)M''(x) + (M(x) - z)M'''(x)\n$$\nNow, we can express the coefficients $A$, $B$, and $C$ in terms of $K$, $z$, and the derivatives of $M(x)$ evaluated at $x_b$:\n$$\nA = g'(0) = K J_f'(x_b) = K (M(x_b) - z) M'(x_b)\n$$\n$$\nB = \\frac{1}{2}g''(0) = \\frac{1}{2} K^2 J_f''(x_b) = \\frac{1}{2} K^2 \\left( [M'(x_b)]^2 + (M(x_b) - z) M''(x_b) \\right)\n$$\n$$\nC = \\frac{1}{6}g'''(0) = \\frac{1}{6} K^3 J_f'''(x_b) = \\frac{1}{6} K^3 \\left( 3M'(x_b)M''(x_b) + (M(x_b) - z)M'''(x_b) \\right)\n$$\nThis completes the derivation for Task 1.\n\n### Task 2: Specific values and failure of superposition\nWe are given the specific values: $\\sigma_b^2 = 1$, $\\sigma_o^2 = 1$, $x_b = 0$, $M(x) = x - x^3$, and $z = 1$.\nFirst, we compute the Kalman gain $K$:\n$$\nK = \\frac{\\sigma_b^2}{\\sigma_b^2 + \\sigma_o^2} = \\frac{1}{1 + 1} = \\frac{1}{2}\n$$\nNext, we compute the derivatives of $M(x)$ and evaluate them at $x_b = 0$:\n$M(x) = x - x^3 \\implies M(0) = 0$\n$M'(x) = 1 - 3x^2 \\implies M'(0) = 1$\n$M''(x) = -6x \\implies M''(0) = 0$\n$M'''(x) = -6 \\implies M'''(0) = -6$\nNow we substitute these values into the expressions for the coefficients $A$, $B$, and $C$:\n$$\nA = K (M(0) - z) M'(0) = \\frac{1}{2} (0 - 1) (1) = -\\frac{1}{2}\n$$\n$$\nB = \\frac{1}{2} K^2 \\left( [M'(0)]^2 + (M(0) - z) M''(0) \\right) = \\frac{1}{2} \\left(\\frac{1}{2}\\right)^2 \\left( [1]^2 + (0-1)(0) \\right) = \\frac{1}{2} \\cdot \\frac{1}{4} \\cdot (1) = \\frac{1}{8}\n$$\n$$\nC = \\frac{1}{6} K^3 \\left( 3 M'(0) M''(0) + (M(0) - z) M'''(0) \\right) = \\frac{1}{6} \\left(\\frac{1}{2}\\right)^3 \\left( 3(1)(0) + (0-1)(-6) \\right) = \\frac{1}{6} \\cdot \\frac{1}{8} \\cdot (6) = \\frac{6}{48} = \\frac{1}{8}\n$$\nThe explicit cubic approximation for $\\Delta J_f(\\varepsilon)$ is therefore:\n$$\n\\Delta J_f(\\varepsilon) = -\\frac{1}{2}\\varepsilon + \\frac{1}{8}\\varepsilon^2 + \\frac{1}{8}\\varepsilon^3\n$$\nTo show that linear superposition fails, we check if $\\Delta J_f(\\varepsilon_1 + \\varepsilon_2) = \\Delta J_f(\\varepsilon_1) + \\Delta J_f(\\varepsilon_2)$.\nLeft side:\n$$\n\\Delta J_f(\\varepsilon_1 + \\varepsilon_2) = -\\frac{1}{2}(\\varepsilon_1 + \\varepsilon_2) + \\frac{1}{8}(\\varepsilon_1 + \\varepsilon_2)^2 + \\frac{1}{8}(\\varepsilon_1 + \\varepsilon_2)^3\n$$\n$$\n\\Delta J_f(\\varepsilon_1 + \\varepsilon_2) = -\\frac{1}{2}(\\varepsilon_1 + \\varepsilon_2) + \\frac{1}{8}(\\varepsilon_1^2 + 2\\varepsilon_1\\varepsilon_2 + \\varepsilon_2^2) + \\frac{1}{8}(\\varepsilon_1^3 + 3\\varepsilon_1^2\\varepsilon_2 + 3\\varepsilon_1\\varepsilon_2^2 + \\varepsilon_2^3)\n$$\nRight side:\n$$\n\\Delta J_f(\\varepsilon_1) + \\Delta J_f(\\varepsilon_2) = \\left(-\\frac{1}{2}\\varepsilon_1 + \\frac{1}{8}\\varepsilon_1^2 + \\frac{1}{8}\\varepsilon_1^3\\right) + \\left(-\\frac{1}{2}\\varepsilon_2 + \\frac{1}{8}\\varepsilon_2^2 + \\frac{1}{8}\\varepsilon_2^3\\right)\n$$\nThe left side contains cross-product terms such as $\\frac{2}{8}\\varepsilon_1\\varepsilon_2 = \\frac{1}{4}\\varepsilon_1\\varepsilon_2$, which are absent from the right side. Since the coefficients $B$ and $C$ are non-zero, the function $\\Delta J_f(\\varepsilon)$ contains nonlinear terms ($\\varepsilon^2$, $\\varepsilon^3$). Therefore, in general, $\\Delta J_f(\\varepsilon_1 + \\varepsilon_2) \\neq \\Delta J_f(\\varepsilon_1) + \\Delta J_f(\\varepsilon_2)$, and linear superposition fails.\n\n### Task 3: Smallest positive root of $\\Delta J_f(\\varepsilon) = 0$\nUsing the cubic approximation derived in Task 2, we need to find the smallest positive root $\\varepsilon^{\\star} > 0$ of the equation $\\Delta J_f(\\varepsilon) = 0$.\n$$\n-\\frac{1}{2}\\varepsilon + \\frac{1}{8}\\varepsilon^2 + \\frac{1}{8}\\varepsilon^3 = 0\n$$\nTo simplify, we multiply the entire equation by $8$:\n$$\n\\varepsilon^3 + \\varepsilon^2 - 4\\varepsilon = 0\n$$\nWe can factor out a term in $\\varepsilon$:\n$$\n\\varepsilon(\\varepsilon^2 + \\varepsilon - 4) = 0\n$$\nThis equation has three roots. One is the trivial root $\\varepsilon = 0$. The other two roots are solutions to the quadratic equation:\n$$\n\\varepsilon^2 + \\varepsilon - 4 = 0\n$$\nWe solve this using the quadratic formula $\\varepsilon = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$, with $a=1$, $b=1$, and $c=-4$:\n$$\n\\varepsilon = \\frac{-1 \\pm \\sqrt{1^2 - 4(1)(-4)}}{2(1)} = \\frac{-1 \\pm \\sqrt{1 + 16}}{2} = \\frac{-1 \\pm \\sqrt{17}}{2}\n$$\nThe two roots are $\\varepsilon_1 = \\frac{-1 - \\sqrt{17}}{2}$ and $\\varepsilon_2 = \\frac{-1 + \\sqrt{17}}{2}$.\nSince $\\sqrt{17} > \\sqrt{1} = 1$, the first root $\\varepsilon_1$ is negative. The second root $\\varepsilon_2$ is positive.\nTherefore, the smallest positive innovation amplitude $\\varepsilon^{\\star}$ for which $\\Delta J_f(\\varepsilon) = 0$ is:\n$$\n\\varepsilon^{\\star} = \\frac{-1 + \\sqrt{17}}{2}\n$$\nThis is the required exact closed-form expression.",
            "answer": "$$\\boxed{\\frac{\\sqrt{17}-1}{2}}$$"
        },
        {
            "introduction": "Beyond assessing the impact of existing observations, a key goal is to strategically deploy new ones—the essence of adaptive observing. This task reframes the challenge as a constrained optimization problem, a common approach in operational settings. By treating observation selection as a classic knapsack problem, you will learn how to maximize forecast skill improvement under a fixed budget, directly connecting data assimilation theory to practical decision-making .",
            "id": "4071080",
            "problem": "Adaptive observing in Numerical Weather Prediction (NWP) seeks to select informative measurements subject to logistical constraints to reduce forecast uncertainty. Consider a targeted design for a single nondimensional scalar forecast error mode $x$ to be reduced via Data Assimilation (DA). The prior is Gaussian with $x \\sim \\mathcal{N}(0, P_b)$, where $P_b > 0$ is known. There are $m$ candidate observations, indexed by $i \\in \\{1,\\dots,m\\}$, each measuring $y_i = h_i x + \\epsilon_i$, where $h_i \\in \\mathbb{R}$ is the known linearized observation operator coefficient and $\\epsilon_i$ is a zero-mean Gaussian measurement error with variance $r_i > 0$. Assume $\\epsilon_i$ are mutually independent across $i$. Each candidate observation $i$ carries a deployment cost $c_i > 0$, and the total budget is $B > 0$. Define binary decision variables $x_i \\in \\{0,1\\}$ indicating whether observation $i$ is selected.\n\nStarting from the linear-Gaussian DA model and the definition that the A-optimal criterion minimizes the trace of the posterior error covariance, derive a mixed-integer optimization formulation whose objective is the A-optimal trace of the posterior covariance of $x$ as a function of the selection vector $(x_1,\\dots,x_m)$, and whose constraints enforce the budget and binary decisions. Clearly state the mathematical optimization problem in terms of the decision variables, objective, and constraints.\n\nThen, for the specific numerical instance with $m=5$, prior variance $P_b = 1$, budget $B = 12$, and parameters\n- costs: $c_1 = 6$, $c_2 = 5$, $c_3 = 5$, $c_4 = 7$, $c_5 = 2$,\n- observation operator coefficients: $h_1 = 0.8$, $h_2 = 0.5$, $h_3 = 1.2$, $h_4 = 0.3$, $h_5 = 0.7$,\n- error variances: $r_1 = 0.09$, $r_2 = 0.04$, $r_3 = 0.36$, $r_4 = 0.01$, $r_5 = 0.49$,\n\ndetermine the optimal selection and compute the minimal posterior variance (which, for a scalar, equals the trace of the posterior covariance) achieved by this selection. The scalar $x$ is nondimensional; express the final answer as a nondimensional real number. Round your final answer to four significant figures.",
            "solution": "The problem is posed within the linear-Gaussian Data Assimilation (DA) framework. For a scalar state $x$ with prior $x \\sim \\mathcal{N}(0, P_b)$, and measurements $y_i = h_i x + \\epsilon_i$ with independent $\\epsilon_i \\sim \\mathcal{N}(0, r_i)$, the posterior distribution of $x$ given a selected subset of observations remains Gaussian. A core well-tested fact in linear-Gaussian inference is that information (precision) adds: the posterior precision equals the prior precision plus the sum of the information contributed by each measurement.\n\nWe formalize the selection via binary variables $x_i \\in \\{0,1\\}$. If observation $i$ is selected ($x_i = 1$), its likelihood contributes a term proportional to $h_i^2 / r_i$ to the Fisher information about $x$. To see this, we can write the negative log-posterior (up to an additive constant) given selected observations $\\{i : x_i = 1\\}$:\n\n$$\n\\frac{x^2}{2 P_b} + \\sum_{i=1}^{m} x_i \\frac{\\left(y_i - h_i x\\right)^2}{2 r_i}.\n$$\n\nExpanding the sum and collecting terms in $x$, the quadratic term in $x$ yields the posterior precision:\n\n$$\nP_a^{-1} = P_b^{-1} + \\sum_{i=1}^{m} x_i \\frac{h_i^2}{r_i}.\n$$\n\nThus, the posterior variance is\n\n$$\nP_a = \\left(P_b^{-1} + \\sum_{i=1}^{m} x_i \\frac{h_i^2}{r_i}\\right)^{-1}.\n$$\n\nFor a scalar state, the A-optimal criterion, defined as minimizing the trace of the posterior covariance, reduces to minimizing $P_a$ itself. Therefore, the mixed-integer optimization problem under an A-optimal objective is\n\n$$\n\\min_{x_1,\\dots,x_m} \\;\\; \\left(P_b^{-1} + \\sum_{i=1}^{m} x_i \\frac{h_i^2}{r_i}\\right)^{-1}\n$$\n\nsubject to\n\n$$\n\\sum_{i=1}^{m} c_i x_i \\leq B, \\quad x_i \\in \\{0,1\\} \\quad \\text{for all } i \\in \\{1,\\dots,m\\}.\n$$\n\nBecause the function $f(s) = \\left(P_b^{-1} + s\\right)^{-1}$ is strictly decreasing in $s$ for $s \\ge 0$, minimizing $P_a$ is equivalent to maximizing the summed information contribution\n\n$$\n\\sum_{i=1}^{m} x_i \\frac{h_i^2}{r_i}\n$$\n\nunder the same constraints. This reveals that, in the scalar case, the A-optimal design reduces to a 0–1 knapsack problem with item values $w_i = \\frac{h_i^2}{r_i}$ and weights $c_i$.\n\nWe now solve the provided numerical instance. Compute the information contributions $w_i = \\frac{h_i^2}{r_i}$:\n- For $i=1$: $h_1 = 0.8$, $r_1 = 0.09$, so $w_1 = \\frac{(0.8)^2}{0.09} = \\frac{0.64}{0.09} = 7.111\\overline{1}$, i.e., $w_1 = 7.\\overline{1}$.\n- For $i=2$: $h_2 = 0.5$, $r_2 = 0.04$, so $w_2 = \\frac{(0.5)^2}{0.04} = \\frac{0.25}{0.04} = 6.25$.\n- For $i=3$: $h_3 = 1.2$, $r_3 = 0.36$, so $w_3 = \\frac{(1.2)^2}{0.36} = \\frac{1.44}{0.36} = 4$.\n- For $i=4$: $h_4 = 0.3$, $r_4 = 0.01$, so $w_4 = \\frac{(0.3)^2}{0.01} = \\frac{0.09}{0.01} = 9$.\n- For $i=5$: $h_5 = 0.7$, $r_5 = 0.49$, so $w_5 = \\frac{(0.7)^2}{0.49} = \\frac{0.49}{0.49} = 1$.\n\nThe costs are $c_1 = 6$, $c_2 = 5$, $c_3 = 5$, $c_4 = 7$, $c_5 = 2$, with budget $B = 12$. We seek to maximize $\\sum_{i=1}^{5} x_i w_i$ subject to $\\sum_{i=1}^{5} c_i x_i \\le 12$ and $x_i \\in \\{0,1\\}$.\n\nWe enumerate feasible high-value combinations under the budget:\n- Pick $i=4$ and $i=2$: total cost $7 + 5 = 12$ (feasible), total value $9 + 6.25 = 15.25$.\n- Pick $i=4$ and $i=3$: total cost $7 + 5 = 12$ (feasible), total value $9 + 4 = 13$.\n- Pick $i=1$ and $i=2$: total cost $6 + 5 = 11$ (feasible), total value $7.111\\overline{1} + 6.25 = 13.361\\overline{1}$.\n- Pick $i=2$, $i=3$, and $i=5$: total cost $5 + 5 + 2 = 12$ (feasible), total value $6.25 + 4 + 1 = 11.25$.\n- Other feasible combinations, such as $i=4$ and $i=5$ (cost $9$, value $10$), $i=1$ and $i=3$ (cost $11$, value $11.111\\overline{1}$), or any addition of $i=5$ to $i=1$ and $i=2$ or $i=1$ and $i=3$ would exceed the budget ($13$ or higher).\n\nThe maximal value under the budget is therefore achieved by selecting $i=4$ and $i=2$, with summed information $s^{\\star} = 15.25$. With $P_b = 1$, the optimal posterior variance is\n\n$$\nP_a^{\\star} = \\left(P_b^{-1} + s^{\\star}\\right)^{-1} = \\left(1 + 15.25\\right)^{-1} = \\frac{1}{16.25} = 0.0615384615\\ldots\n$$\n\nRounded to four significant figures, the minimal posterior variance is $0.06154$ (nondimensional).",
            "answer": "$$\\boxed{0.06154}$$"
        }
    ]
}