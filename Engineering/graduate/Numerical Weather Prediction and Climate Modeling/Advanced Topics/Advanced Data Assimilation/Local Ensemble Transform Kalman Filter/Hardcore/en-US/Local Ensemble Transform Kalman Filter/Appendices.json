{
    "hands_on_practices": [
        {
            "introduction": "Having established the issue of spurious long-range correlations, we now turn to the solution: covariance localization. The LETKF addresses this by ensuring that an observation only influences the model state within a defined local neighborhood. This is accomplished by applying a tapering function that smoothly reduces an observation's influence to zero at a specified cutoff distance. This hands-on exercise  makes this abstract concept practical, guiding you through the derivation and application of a widely used localization taper.",
            "id": "4060590",
            "problem": "In data assimilation for Numerical Weather Prediction and climate modeling, the Local Ensemble Transform Kalman Filter (LETKF) restricts the impact of observations to a geographically local neighborhood around each model grid point and applies covariance localization through a taper that smoothly reduces observation influence to zero at a finite cutoff distance. Consider a single two-dimensional horizontal grid point at the origin, with coordinates $(0,0)$ measured in kilometers. A set of six observation stations are located at the following Cartesian coordinates (in kilometers) relative to the grid point:\n$(30,40)$, $(15,20)$, $(3,4)$, $(35,0)$, $(0,0)$, $(60,0)$.\nLet the localization radius be $r=50$ kilometers.\n\nAdopt the following foundations:\n- The horizontal distance $d$ from the grid point to a station at $(x,y)$ is given by the Euclidean norm $d=\\sqrt{x^{2}+y^{2}}$.\n- A localization taper weight $w(d)$ is dimensionless, depends only on distance $d$, and must satisfy smoothness and boundary constraints appropriate for covariance localization in LETKF: $w(0)=1$, $w(r)=0$, and $w'(0)=0$, $w'(r)=0$, with $w(d)=0$ for all $d \\ge r$.\n- Among functions consistent with these constraints, use the simplest trigonometric form that is twice continuously differentiable and compactly supported on $[0,r]$.\n\nTask:\n- Determine which observations are included in the local analysis domain, defined as those with $d \\le r$.\n- Derive the corresponding taper $w(d)$ from the stated constraints and compute the weight for each station in the order listed above. For any observation with $d>r$, set its weight to $0$.\n\nExpress the final answer as a single row vector of the six weights in the order of the given stations, rounded to five significant figures. The weights are dimensionless numbers.",
            "solution": "The problem statement is evaluated to be scientifically grounded, well-posed, objective, and complete. It describes a standard procedure in data assimilation, specifically the application of a covariance localization taper in the context of a Local Ensemble Transform Kalman Filter (LETKF). The provided data and constraints are sufficient and consistent for deriving a unique solution. Therefore, the problem is valid.\n\nThe task is to determine the localization taper weights for a set of six observation stations relative to a grid point at the origin $(0,0)$. This requires two main steps: first, deriving the analytical form of the taper weight function $w(d)$ from the given constraints, and second, applying this function to each observation after calculating its distance $d$ from the origin.\n\nFirst, we derive the taper function $w(d)$. The problem specifies that $w(d)$ must be the simplest trigonometric form that is twice continuously differentiable, defined on the interval $[0, r]$ where $r$ is the localization radius, and satisfies the following four boundary conditions:\n1. $w(0) = 1$ (full weight at the grid point)\n2. $w(r) = 0$ (zero weight at the cutoff radius)\n3. $w'(0) = 0$ (zero slope at the grid point for smoothness)\n4. $w'(r) = 0$ (zero slope at the cutoff radius for smoothness)\n\nThe function is also defined to be $w(d) = 0$ for all $d \\ge r$. The requirement for a trigonometric form suggests a function involving sines and/or cosines. A simple candidate form is $w(d) = A\\cos(kd) + B\\sin(kd) + C$.\n\nLet's apply the boundary conditions. The condition $w'(0) = 0$ is a strong constraint. The derivative is $w'(d) = -Ak\\sin(kd) + Bk\\cos(kd)$. At $d=0$, we have $w'(0) = Bk\\cos(0) = Bk$. For this to be zero, either $B=0$ or $k=0$. If $k=0$, the function is constant, which cannot satisfy the other conditions. Thus, we must have $B=0$. This simplifies our candidate function to $w(d) = A\\cos(kd) + C$, which is an even function, consistent with the expected symmetry around the origin.\n\nNow we apply the remaining three conditions to $w(d) = A\\cos(kd) + C$:\n- $w(0) = 1 \\implies A\\cos(0) + C = A + C = 1$.\n- $w'(r) = 0 \\implies -Ak\\sin(kr) = 0$. Since $A \\ne 0$ and $k \\ne 0$ for a non-trivial solution, we must have $\\sin(kr) = 0$. This implies $kr = n\\pi$ for some integer $n$. For the simplest non-constant function, we choose $n=1$, which gives $k = \\frac{\\pi}{r}$.\n- $w(r) = 0 \\implies A\\cos(kr) + C = 0$. Substituting $k=\\frac{\\pi}{r}$, we get $A\\cos(\\pi) + C = -A + C = 0$.\n\nWe now have a system of two linear equations for $A$ and $C$:\n$$ A + C = 1 $$\n$$ -A + C = 0 \\implies A = C $$\nSubstituting $A=C$ into the first equation gives $2A=1$, so $A = \\frac{1}{2}$ and $C = \\frac{1}{2}$.\n\nThe resulting localization function for $d \\in [0, r]$ is:\n$$ w(d) = \\frac{1}{2}\\cos\\left(\\frac{\\pi d}{r}\\right) + \\frac{1}{2} = \\frac{1}{2}\\left(1 + \\cos\\left(\\frac{\\pi d}{r}\\right)\\right) $$\nThis function is a form of a raised-cosine or Hanning window. It is infinitely differentiable on the interval $(0, r)$, thus satisfying the \"twice continuously differentiable\" requirement on its domain of support. It is a standard and simple choice for a localization taper.\n\nGiven the localization radius $r=50$ km, the specific taper function is:\n$$ w(d) = \\frac{1}{2}\\left(1 + \\cos\\left(\\frac{\\pi d}{50}\\right)\\right) \\quad \\text{for } 0 \\le d \\le 50 $$\nAnd $w(d)=0$ for $d > 50$.\n\nNext, we calculate the distance $d$ and the corresponding weight $w(d)$ for each of the six stations.\n\n1. Station 1 at $(30, 40)$:\n$d_1 = \\sqrt{30^2 + 40^2} = \\sqrt{900 + 1600} = \\sqrt{2500} = 50$ km.\nSince $d_1 = r = 50$, this observation is at the edge of the local domain. The weight is $w_1 = w(50) = 0$, as per the boundary condition $w(r)=0$.\n$w_1 = 0.00000$.\n\n2. Station 2 at $(15, 20)$:\n$d_2 = \\sqrt{15^2 + 20^2} = \\sqrt{225 + 400} = \\sqrt{625} = 25$ km.\nSince $d_2 \\le r$, the observation is included.\n$w_2 = w(25) = \\frac{1}{2}\\left(1 + \\cos\\left(\\frac{25\\pi}{50}\\right)\\right) = \\frac{1}{2}\\left(1 + \\cos\\left(\\frac{\\pi}{2}\\right)\\right) = \\frac{1}{2}(1+0) = 0.5$.\n$w_2 = 0.50000$.\n\n3. Station 3 at $(3, 4)$:\n$d_3 = \\sqrt{3^2 + 4^2} = \\sqrt{9 + 16} = \\sqrt{25} = 5$ km.\nSince $d_3 \\le r$, the observation is included.\n$w_3 = w(5) = \\frac{1}{2}\\left(1 + \\cos\\left(\\frac{5\\pi}{50}\\right)\\right) = \\frac{1}{2}\\left(1 + \\cos\\left(\\frac{\\pi}{10}\\right)\\right)$.\nNumerically, $\\cos(\\frac{\\pi}{10}) \\approx 0.9510565163$.\n$w_3 \\approx \\frac{1}{2}(1 + 0.9510565163) = 0.975528258$.\nRounded to five significant figures, $w_3 = 0.97553$.\n\n4. Station 4 at $(35, 0)$:\n$d_4 = \\sqrt{35^2 + 0^2} = 35$ km.\nSince $d_4 \\le r$, the observation is included.\n$w_4 = w(35) = \\frac{1}{2}\\left(1 + \\cos\\left(\\frac{35\\pi}{50}\\right)\\right) = \\frac{1}{2}\\left(1 + \\cos\\left(\\frac{7\\pi}{10}\\right)\\right)$.\nNumerically, $\\cos(\\frac{7\\pi}{10}) \\approx -0.5877852523$.\n$w_4 \\approx \\frac{1}{2}(1 - 0.5877852523) = 0.20610737385$.\nRounded to five significant figures, $w_4 = 0.20611$.\n\n5. Station 5 at $(0, 0)$:\n$d_5 = \\sqrt{0^2 + 0^2} = 0$ km.\nThis observation is at the grid point itself.\n$w_5 = w(0) = 1$, as per the boundary condition.\n$w_5 = 1.0000$.\n\n6. Station 6 at $(60, 0)$:\n$d_6 = \\sqrt{60^2 + 0^2} = 60$ km.\nSince $d_6 > r$, this observation is outside the local domain.\nPer the problem definition, $w(d)=0$ for $d \\ge r$.\n$w_6 = 0.00000$.\n\nThe final answer is the row vector of these six weights in the specified order.",
            "answer": "$$ \\boxed{ \\begin{pmatrix} 0.00000 & 0.50000 & 0.97553 & 0.20611 & 1.0000 & 0.00000 \\end{pmatrix} } $$"
        },
        {
            "introduction": "At the heart of the LETKF is an elegant and computationally efficient update mechanism performed in the low-dimensional space spanned by the ensemble members. Instead of directly calculating and storing the enormous state error covariance matrix, the algorithm computes a small transformation matrix, $T$, that directly updates the ensemble anomalies. This exercise  delves into this core \"Ensemble Transform,\" walking you through the derivation of $T$ from first principles and revealing the mathematical engine that makes the LETKF a powerful tool for high-dimensional systems.",
            "id": "4060622",
            "problem": "Consider a single local analysis volume in Numerical Weather Prediction (NWP) where the Ensemble Transform Kalman Filter (ETKF) is applied. The Ensemble Transform Kalman Filter (ETKF) expresses the Kalman analysis update in the ensemble subspace using ensemble forecast anomalies, and constructs a linear transform that maps forecast anomalies to analysis anomalies while ensuring the updated ensemble covariance is consistent with the Kalman analysis error covariance. The ensemble anomalies must have zero column mean, and the transform must preserve this property. The observation error covariance is assumed symmetric positive definite, and the observation operator is linear in the neighborhood of the forecast mean.\n\nYou are given a small, fully specified numeric example:\n\n- State dimension is $n = 2$, ensemble size is $N = 3$, and the forecast ensemble anomalies are scaled by the factor $1/\\sqrt{N-1}$ so that the sample covariance equals $X'^{f} (X'^{f})^{\\top}$. The anomalies are\n$$\nX'^{f} \\;=\\; \\frac{1}{\\sqrt{2}}\n\\begin{pmatrix}\n2 & -1 & -1 \\\\\n0 & 1 & -1\n\\end{pmatrix}.\n$$\n- The observation operator is the linear map $H : \\mathbb{R}^{2} \\to \\mathbb{R}$ that extracts the first state component, i.e., $H = \\begin{pmatrix} 1 & 0 \\end{pmatrix}$. The corresponding forecast observation anomalies are therefore\n$$\nY'^{f} \\;=\\; H X'^{f} \\;=\\; \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 2 & -1 & -1 \\end{pmatrix}.\n$$\n- The observation error covariance is $R = 1$ (a scalar), and the innovation is $d = 1.2$ (a scalar).\n\nTasks:\n\n1. Starting from the standard Kalman filtering objective in observation space and the ensemble representation of forecast error covariance, derive the ETKF analysis weights $w^{a}$ in the ensemble space and compute their explicit numeric values for the given $X'^{f}$, $Y'^{f}$, $R$, and $d$. In deriving the weights, respect the zero-column-mean constraint of the anomalies by decomposing the ensemble space into the mean-direction and its orthogonal complement.\n\n2. Construct the ETKF anomaly transform matrix $T$ in the ensemble space that maps $X'^{f}$ to the analysis anomalies $X'^{a} = X'^{f} T$. Use an orthonormal basis that separates the mean-direction vector and its orthogonal complement. Apply $T$ to obtain $X'^{a}$ and verify both of the following properties:\n   - The columns of $X'^{a}$ have zero sum, i.e., $X'^{a} \\mathbf{1} = \\mathbf{0}$ with $\\mathbf{1} = (1,1,1)^{\\top}$,\n   - The rank is preserved, i.e., $\\mathrm{rank}(X'^{a}) = \\mathrm{rank}(X'^{f})$.\n\n3. Compute the determinant of the ETKF anomaly transform matrix $T$ from your construction in Task 2. Provide this determinant as your final answer. Express the determinant as an exact value (no rounding required). The final answer must be a single real-valued number.\n\nYour work must be self-contained and begin from fundamental Kalman filtering principles (the gain minimizes the posterior mean-square error) and the ensemble covariance representation (sample covariance from scaled anomalies), without assuming any specialized ETKF formulas a priori.",
            "solution": "The problem requires a derivation and calculation of key quantities in the Ensemble Transform Kalman Filter (ETKF) from first principles. We are given the scaled forecast ensemble anomalies $X'^{f}$, the observation operator $H$, the observation error covariance $R$, and the innovation $d$.\n\nThe state dimension is $n=2$, the ensemble size is $N=3$. The scaled forecast anomalies are given by\n$$\nX'^{f} \\;=\\; \\frac{1}{\\sqrt{N-1}}\n\\begin{pmatrix}\n2 & -1 & -1 \\\\\n0 & 1 & -1\n\\end{pmatrix}\n\\;=\\; \\frac{1}{\\sqrt{2}}\n\\begin{pmatrix}\n2 & -1 & -1 \\\\\n0 & 1 & -1\n\\end{pmatrix}.\n$$\nThe ensemble forecast error covariance matrix is approximated by the sample covariance $P^f = X'^{f} (X'^{f})^{\\top}$. The observation operator is $H = \\begin{pmatrix} 1 & 0 \\end{pmatrix}$. The forecast observation anomalies are $Y'^{f} = H X'^{f}$, and the observation error covariance is $R=1$. The innovation is $d=1.2$.\n\n### Task 1: Derivation and computation of analysis weights $w^a$\n\nThe standard Kalman filter analysis update for the mean state is\n$$\n\\bar{x}^a = \\bar{x}^f + K d,\n$$\nwhere $d = y^o - H\\bar{x}^f$ is the innovation and $K$ is the Kalman gain. The Kalman gain is defined to minimize the analysis error variance and is given by\n$$\nK = P^f H^{\\top} (H P^f H^{\\top} + R)^{-1}.\n$$\nSubstituting the ensemble estimate for the forecast error covariance, $P^f = X'^{f} (X'^{f})^{\\top}$, we can express the gain in terms of the ensemble anomalies.\nThe term $P^f H^{\\top}$ becomes:\n$$\nP^f H^{\\top} = X'^{f} (X'^{f})^{\\top} H^{\\top} = X'^{f} (H X'^{f})^{\\top} = X'^{f} (Y'^{f})^{\\top}.\n$$\nThe term $H P^f H^{\\top}$ becomes:\n$$\nH P^f H^{\\top} = H (X'^{f} (X'^{f})^{\\top}) H^{\\top} = (H X'^{f})(H X'^{f})^{\\top} = Y'^{f} (Y'^{f})^{\\top}.\n$$\nThus, the Kalman gain in the ensemble subspace is\n$$\nK = X'^{f} (Y'^{f})^{\\top} (Y'^{f} (Y'^{f})^{\\top} + R)^{-1}.\n$$\nThe update to the ensemble mean, $\\Delta\\bar{x} = \\bar{x}^a - \\bar{x}^f = K d$, is then\n$$\n\\Delta\\bar{x} = X'^{f} \\left[ (Y'^{f})^{\\top} (Y'^{f} (Y'^{f})^{\\top} + R)^{-1} d \\right].\n$$\nThe ETKF framework expresses this update as a linear combination of the forecast anomaly columns, $\\Delta\\bar{x} = X'^{f} w^a$, where $w^a$ is the vector of analysis weights in the ensemble space. By comparing the two expressions for $\\Delta\\bar{x}$, we identify the analysis weights as\n$$\nw^a = (Y'^{f})^{\\top} (Y'^{f} (Y'^{f})^{\\top} + R)^{-1} d.\n$$\nNow, we compute the numerical values for the given data.\nThe forecast observation anomalies are\n$$\nY'^{f} = H X'^{f} = \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 2 & -1 & -1 \\\\ 0 & 1 & -1 \\end{pmatrix} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 2 & -1 & -1 \\end{pmatrix}.\n$$\nThe term $Y'^{f} (Y'^{f})^{\\top}$ is a scalar in this case, as there is one observation:\n$$\nY'^{f} (Y'^{f})^{\\top} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 2 & -1 & -1 \\end{pmatrix} \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 2 \\\\ -1 \\\\ -1 \\end{pmatrix} = \\frac{1}{2} (2^2 + (-1)^2 + (-1)^2) = \\frac{1}{2}(4+1+1) = 3.\n$$\nGiven $R=1$ and $d=1.2$, the inverse term is $(Y'^{f} (Y'^{f})^{\\top} + R)^{-1} = (3 + 1)^{-1} = \\frac{1}{4}$.\nThe transpose of the forecast observation anomalies is $(Y'^{f})^{\\top} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 2 \\\\ -1 \\\\ -1 \\end{pmatrix}$.\nSubstituting these values into the expression for $w^a$:\n$$\nw^a = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 2 \\\\ -1 \\\\ -1 \\end{pmatrix} \\left(\\frac{1}{4}\\right) (1.2) = \\frac{0.3}{\\sqrt{2}} \\begin{pmatrix} 2 \\\\ -1 \\\\ -1 \\end{pmatrix} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 0.6 \\\\ -0.3 \\\\ -0.3 \\end{pmatrix}.\n$$\n\n### Task 2: Construction of the anomaly transform matrix $T$\n\nThe analysis error covariance matrix $P^a$ is given by the Kalman update formula\n$$\nP^a = (I - KH)P^f.\n$$\nSubstituting the ensemble expressions for $K$ and $P^f$:\n$$\nP^a = P^f - K H P^f = X'^{f} (X'^{f})^{\\top} - \\left( X'^{f} (Y'^{f})^{\\top} (Y'^{f} (Y'^{f})^{\\top} + R)^{-1} \\right) H \\left( X'^{f} (X'^{f})^{\\top} \\right).\n$$\nUsing $Y'^{f} = H X'^{f}$, this simplifies to\n$$\nP^a = X'^{f} (X'^{f})^{\\top} - X'^{f} (Y'^{f})^{\\top} (Y'^{f} (Y'^{f})^{\\top} + R)^{-1} Y'^{f} (X'^{f})^{\\top}.\n$$\nFactoring out $X'^{f}$ and $(X'^{f})^{\\top}$:\n$$\nP^a = X'^{f} \\left[ I_N - (Y'^{f})^{\\top} (Y'^{f} (Y'^{f})^{\\top} + R)^{-1} Y'^{f} \\right] (X'^{f})^{\\top}.\n$$\nwhere $I_N$ is the $N \\times N$ identity matrix.\nThe ETKF defines the analysis anomalies as a linear transformation of the forecast anomalies, $X'^{a} = X'^{f} T$. The analysis covariance is then $P^a = X'^{a}(X'^{a})^{\\top} = X'^{f} T T^{\\top} (X'^{f})^{\\top}$. Comparing this with the expression for $P^a$ above, we identify\n$$\nT T^{\\top} = I_N - (Y'^{f})^{\\top} (Y'^{f} (Y'^{f})^{\\top} + R)^{-1} Y'^{f}.\n$$\nThe ETKF chooses $T$ to be the symmetric square root of this matrix, $T = (T T^{\\top})^{1/2}$. To find this square root, we perform a spectral decomposition of $T T^{\\top}$. Let $C = T T^{\\top}$.\n$$\nC = I_3 - (Y'^{f})^{\\top} (3+1)^{-1} Y'^{f} = I_3 - \\frac{1}{4} (Y'^{f})^{\\top} Y'^{f}.\n$$\n$$\n(Y'^{f})^{\\top} Y'^{f} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 2 \\\\ -1 \\\\ -1 \\end{pmatrix} \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 2 & -1 & -1 \\end{pmatrix} = \\frac{1}{2} \\begin{pmatrix} 4 & -2 & -2 \\\\ -2 & 1 & 1 \\\\ -2 & 1 & 1 \\end{pmatrix}.\n$$\nSo, $C = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} - \\frac{1}{8} \\begin{pmatrix} 4 & -2 & -2 \\\\ -2 & 1 & 1 \\\\ -2 & 1 & 1 \\end{pmatrix} = \\begin{pmatrix} 1-1/2 & 1/4 & 1/4 \\\\ 1/4 & 1-1/8 & -1/8 \\\\ 1/4 & -1/8 & 1-1/8 \\end{pmatrix} = \\begin{pmatrix} 1/2 & 1/4 & 1/4 \\\\ 1/4 & 7/8 & -1/8 \\\\ 1/4 & -1/8 & 7/8 \\end{pmatrix}$.\n\nTo find $T = C^{1/2}$, we find the eigenvalues and eigenvectors of $C$. The matrix $(Y'^{f})^{\\top} Y'^{f}$ has rank $1$. Its non-zero eigenvalue corresponds to the eigenvector $(Y'^{f})^{\\top}$. The eigenvalue is $Y'^{f}(Y'^{f})^{\\top} = 3$. The other two eigenvalues are $0$. The eigenvectors for the eigenvalue $0$ span the null space of $Y'^{f}$, i.e., vectors $v$ such that $Y'^f v = 0$. One such vector is the mean direction vector $\\mathbf{1} = (1,1,1)^{\\top}$, since the anomalies sum to zero. Another vector orthogonal to both $(Y'^{f})^{\\top} \\propto (2,-1,-1)^{\\top}$ and $\\mathbf{1}$ is $(0,1,-1)^{\\top}$.\n\nThe eigenvalues of $C = I_3 - \\frac{1}{4} (Y'^{f})^{\\top} Y'^{f}$ are $1 - \\frac{1}{4}\\lambda$, where $\\lambda$ are the eigenvalues of $(Y'^{f})^{\\top} Y'^{f}$.\n- Eigenvector $\\propto (2,-1,-1)^{\\top}$: eigenvalue is $1 - \\frac{1}{4}(3) = 1/4$.\n- Eigenvector $\\propto (1,1,1)^{\\top}$: eigenvalue is $1 - \\frac{1}{4}(0) = 1$.\n- Eigenvector $\\propto (0,1,-1)^{\\top}$: eigenvalue is $1 - \\frac{1}{4}(0) = 1$.\n\nThe orthonormal eigenvectors are:\n$u_1 = \\frac{1}{\\sqrt{6}}(2,-1,-1)^{\\top}$ (eigenvalue $\\lambda_1 = 1/4$),\n$u_2 = \\frac{1}{\\sqrt{3}}(1,1,1)^{\\top}$ (eigenvalue $\\lambda_2 = 1$),\n$u_3 = \\frac{1}{\\sqrt{2}}(0,1,-1)^{\\top}$ (eigenvalue $\\lambda_3 = 1$).\nLet $V$ be the matrix of these eigenvectors: $V = \\begin{pmatrix} 2/\\sqrt{6} & 1/\\sqrt{3} & 0 \\\\ -1/\\sqrt{6} & 1/\\sqrt{3} & 1/\\sqrt{2} \\\\ -1/\\sqrt{6} & 1/\\sqrt{3} & -1/\\sqrt{2} \\end{pmatrix}$.\nThe diagonal matrix of eigenvalues of $C$ is $\\Lambda_C = \\mathrm{diag}(1/4, 1, 1)$.\nThe matrix $T$ has the same eigenvectors, but its eigenvalues are the square roots of $\\Lambda_C$.\n$\\Lambda_T = \\mathrm{diag}(\\sqrt{1/4}, \\sqrt{1}, \\sqrt{1}) = \\mathrm{diag}(1/2, 1, 1)$.\n$T = V \\Lambda_T V^{\\top}$.\n$V\\Lambda_T = \\begin{pmatrix} 1/\\sqrt{6} & 1/\\sqrt{3} & 0 \\\\ -1/(2\\sqrt{6}) & 1/\\sqrt{3} & 1/\\sqrt{2} \\\\ -1/(2\\sqrt{6}) & 1/\\sqrt{3} & -1/\\sqrt{2} \\end{pmatrix}$.\n$T = (V\\Lambda_T) V^\\top = \\begin{pmatrix} 1/\\sqrt{6} & 1/\\sqrt{3} & 0 \\\\ -1/(2\\sqrt{6}) & 1/\\sqrt{3} & 1/\\sqrt{2} \\\\ -1/(2\\sqrt{6}) & 1/\\sqrt{3} & -1/\\sqrt{2} \\end{pmatrix} \\begin{pmatrix} 2/\\sqrt{6} & -1/\\sqrt{6} & -1/\\sqrt{6} \\\\ 1/\\sqrt{3} & 1/\\sqrt{3} & 1/\\sqrt{3} \\\\ 0 & 1/\\sqrt{2} & -1/\\sqrt{2} \\end{pmatrix}$.\n$T_{11} = \\frac{2}{6} + \\frac{1}{3} = \\frac{1}{3}+\\frac{1}{3} = \\frac{2}{3}$.\n$T_{12} = -\\frac{1}{6} + \\frac{1}{3} = \\frac{1}{6}$. $T_{13} = -\\frac{1}{6} + \\frac{1}{3} = \\frac{1}{6}$.\n$T_{21} = -\\frac{2}{12} + \\frac{1}{3} = -\\frac{1}{6}+\\frac{1}{3} = \\frac{1}{6}$.\n$T_{22} = \\frac{1}{12} + \\frac{1}{3} + \\frac{1}{2} = \\frac{1+4+6}{12} = \\frac{11}{12}$.\n$T_{23} = \\frac{1}{12} + \\frac{1}{3} - \\frac{1}{2} = \\frac{1+4-6}{12} = -\\frac{1}{12}$.\nSince $T$ is symmetric, $T_{ij}=T_{ji}$.\n$T = \\begin{pmatrix} 2/3 & 1/6 & 1/6 \\\\ 1/6 & 11/12 & -1/12 \\\\ 1/6 & -1/12 & 11/12 \\end{pmatrix}$.\n\nNow, compute $X'^{a} = X'^{f} T$:\n$$\nX'^{a} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 2 & -1 & -1 \\\\ 0 & 1 & -1 \\end{pmatrix} \\begin{pmatrix} 2/3 & 1/6 & 1/6 \\\\ 1/6 & 11/12 & -1/12 \\\\ 1/6 & -1/12 & 11/12 \\end{pmatrix}.\n$$\nThe first row of the product is $(\\frac{4}{3}-\\frac{1}{6}-\\frac{1}{6}, \\frac{2}{6}-\\frac{11}{12}+\\frac{1}{12}, \\frac{2}{6}+\\frac{1}{12}-\\frac{11}{12}) = (\\frac{4}{3}-\\frac{1}{3}, \\frac{4-11+1}{12}, \\frac{4+1-11}{12}) = (1, -\\frac{6}{12}, -\\frac{6}{12}) = (1, -1/2, -1/2)$.\nThe second row is $(0+\\frac{1}{6}-\\frac{1}{6}, 0+\\frac{11}{12}+\\frac{1}{12}, 0-\\frac{1}{12}-\\frac{11}{12}) = (0, 1, -1)$.\nSo, $X'^{a} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 & -1/2 & -1/2 \\\\ 0 & 1 & -1 \\end{pmatrix}$.\n\nVerification of properties:\n1. Zero column sum: $X'^{a}\\mathbf{1} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 - 1/2 - 1/2 \\\\ 0 + 1 - 1 \\end{pmatrix} = \\mathbf{0}$. This property holds.\n2. Rank preservation: The rank of a matrix product $\\mathrm{rank}(AB)$ is less than or equal to the minimum of $\\mathrm{rank}(A)$ and $\\mathrm{rank}(B)$. The rank is preserved if the transformation matrix $T$ is full-rank. The eigenvalues of $T$ are $1/2$, $1$, and $1$. Since none are zero, $T$ is invertible and has full rank ($3$). Therefore, $\\mathrm{rank}(X'^{a}) = \\mathrm{rank}(X'^{f}T) = \\mathrm{rank}(X'^{f})$. The matrix $X'^{f}$ has two linearly independent rows, so its rank is $2$. The matrix $X'^{a}$ also clearly has two linearly independent rows, so its rank is also $2$. The rank is preserved.\n\n### Task 3: Determinant of the transform matrix $T$\n\nThe determinant of a matrix is the product of its eigenvalues. We found the eigenvalues of the transform matrix $T$ during its construction in Task 2. The eigenvalues of $T$ are the square roots of the eigenvalues of $C=TT^\\top$. The eigenvalues of $C$ were found to be $1/4$, $1$, and $1$.\nThe eigenvalues of $T$ are therefore $\\sqrt{1/4} = 1/2$, $\\sqrt{1} = 1$, and $\\sqrt{1} = 1$.\nThe determinant of $T$ is the product of these eigenvalues:\n$$\n\\det(T) = \\frac{1}{2} \\times 1 \\times 1 = \\frac{1}{2}.\n$$\nThis can be confirmed by direct computation from the matrix $T$ derived earlier:\n$\\det(T) = \\det \\begin{pmatrix} 2/3 & 1/6 & 1/6 \\\\ 1/6 & 11/12 & -1/12 \\\\ 1/6 & -1/12 & 11/12 \\end{pmatrix} = \\frac{2}{3}(\\frac{121}{144}-\\frac{1}{144}) - \\frac{1}{6}(\\frac{11}{72}+\\frac{1}{72}) + \\frac{1}{6}(-\\frac{1}{72}-\\frac{11}{72}) = \\frac{2}{3}(\\frac{120}{144}) - \\frac{1}{6}(\\frac{12}{72}) + \\frac{1}{6}(-\\frac{12}{72}) = \\frac{2}{3}(\\frac{5}{6}) - \\frac{1}{6}(\\frac{1}{6}) - \\frac{1}{6}(\\frac{1}{6}) = \\frac{10}{18} - \\frac{1}{36} - \\frac{1}{36} = \\frac{20-1-1}{36} = \\frac{18}{36} = \\frac{1}{2}$.\nThe computed determinant is $1/2$.",
            "answer": "$$\\boxed{\\frac{1}{2}}$$"
        }
    ]
}