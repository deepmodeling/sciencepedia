{
    "hands_on_practices": [
        {
            "introduction": "Before optimizing a complex geophysical model, we must first understand its intrinsic performance characteristics. A fundamental concept in this analysis is arithmetic intensity, which measures the ratio of floating-point operations to memory traffic. This exercise guides you through a first-principles derivation of arithmetic intensity for a common computational kernel, reinforcing how this metric is independent of problem size but inherent to the algorithm's structure. ",
            "id": "4051457",
            "problem": "A three-dimensional geophysical fluid dynamics kernel updates a scalar field on a uniform Cartesian grid using a seven-point finite-difference stencil at each interior grid location. Assume a streaming execution on a cache-coherent shared-memory node without temporal blocking, so that data movement from main memory dominates and each update is independent in terms of counted traffic. Let the grid have $N_x$ points in the $x$-direction, $N_y$ points in the $y$-direction, and $N_z$ points in the $z$-direction, with $N \\equiv N_x N_y N_z$ interior updates per time step.\n\nYou are told that, per grid point, the kernel performs $n_f$ floating-point operations and causes $n_b$ bytes to be transferred to or from main memory (including all loads and stores needed for the update under the given execution assumptions). Starting from the definition of arithmetic intensity and from first principles of counting work and data movement, derive the arithmetic intensity of this stencil kernel as a closed-form analytic expression in terms of $n_f$ and $n_b$.\n\nExpress the final arithmetic intensity in floating-point operations per byte (flops per byte). No rounding is required.",
            "solution": "The problem requires the derivation of the arithmetic intensity of a stencil kernel based on first principles.\n\nThe arithmetic intensity, denoted by $I$, is formally defined as the ratio of the total number of floating-point operations performed to the total volume of data moved between the processor and main memory. The units are typically floating-point operations per byte.\n\n$$\nI = \\frac{\\text{Total Floating-Point Operations}}{\\text{Total Data Movement (Bytes)}}\n$$\n\nWe are given a computational kernel that operates on a three-dimensional Cartesian grid. The total number of interior grid points where an update is performed is given as $N = N_x N_y N_z$.\n\nFirst, let us determine the total number of floating-point operations, which we will denote as $W$. The problem states that the kernel performs $n_f$ floating-point operations for the update at each single grid point. Since there are $N$ such independent updates in total per time step, the total computational work is the product of the work per point and the number of points.\n\n$$\nW = N \\times n_f\n$$\n\nNext, we determine the total data movement, which we will denote as $Q$. The problem specifies a streaming execution model where data movement from main memory is the dominant factor, and cache effects are not considered for the purpose of counting byte traffic (i.e., each update is treated as independent in terms of data loads and stores from main memory). It is given that $n_b$ bytes are transferred to or from main memory for the update at each single grid point. As there are $N$ such updates, the total data volume transferred is the product of the bytes per point and the number of points.\n\n$$\nQ = N \\times n_b\n$$\n\nNow, we can substitute these expressions for total work $W$ and total data movement $Q$ into the definition of arithmetic intensity $I$.\n\n$$\nI = \\frac{W}{Q} = \\frac{N \\times n_f}{N \\times n_b}\n$$\n\nThe term representing the total number of grid points, $N$, appears in both the numerator and the denominator. We can simplify the expression by canceling this term. This demonstrates that for this model, the arithmetic intensity is an intrinsic property of the per-point computation and its associated memory access pattern, independent of the overall grid size.\n\n$$\nI = \\frac{n_f}{n_b}\n$$\n\nThis is the final closed-form analytic expression for the arithmetic intensity of the kernel. It is expressed in terms of the given parameters $n_f$ (flops per point) and $n_b$ (bytes per point), and its units are indeed floating-point operations per byte, as required. The contextual information regarding the specific stencil type (seven-point) and the grid dimensions ($N_x$, $N_y$, $N_z$) is implicitly accounted for within the provided parameters $n_f$ and $n_b$, and thus does not explicitly appear in the final expression.",
            "answer": "$$\n\\boxed{\\frac{n_f}{n_b}}\n$$"
        },
        {
            "introduction": "When a simulation domain is distributed across multiple compute nodes, minimizing communication becomes paramount for achieving good scalability. This problem tackles the classic surface-to-volume optimization challenge inherent in domain decomposition. By working through this exercise, you will derive the ideal subdomain shape that minimizes data exchange for a given memory capacity, a foundational skill in designing efficient parallel models. ",
            "id": "4051428",
            "problem": "A structured-grid three-dimensional geophysical fluid model for Numerical Weather Prediction (NWP) is implemented on a distributed-memory High-Performance Computing (HPC) system using domain decomposition. Each compute node owns exactly one axis-aligned rectangular block of interior grid cells of dimensions $n_x \\times n_y \\times n_z$, augmented with a cell-based halo of width $h$ on all six faces to support stencil computations and Message Passing Interface (MPI) exchanges. The per-node memory capacity is $M$, and each grid cell holds $q$ prognostic or diagnostic arrays, each stored with $b$ bytes, for an effective per-cell memory footprint $\\beta = q b$. Assume the dominant inter-node communication each time step consists of exchanging halo slabs of thickness $h$ across the six faces.\n\nStarting from first principles of geometric optimization and resource constraints, and using only the definitions provided above, derive the interior block dimensions $(n_x,n_y,n_z)$ that minimize the total halo communication volume per time step, subject to exactly saturating the per-node memory capacity. Express the final optimal interior block dimensions as an analytic expression in terms of $M$, $h$, $b$, and $q$ only. Assume $M$ is sufficiently large that the optimal block dimensions are nonnegative. No rounding is required. Express the final answer as the number of cells along each dimension.",
            "solution": "The goal is to find the interior grid dimensions $(n_x, n_y, n_z)$ that minimize communication volume while fully utilizing the available memory $M$. This is a classic surface-to-volume optimization problem.\n\n**1. Formulate the Memory Constraint**\nThe total memory used on a node is determined by the total number of cells it stores (interior plus halo) multiplied by the memory per cell ($\\beta = qb$). The block of cells stored in memory has dimensions $(n_x + 2h) \\times (n_y + 2h) \\times (n_z + 2h)$. The memory constraint is thus:\n$$ (n_x + 2h)(n_y + 2h)(n_z + 2h) qb = M $$\n\n**2. Formulate the Communication Volume**\nCommunication consists of exchanging halo data. The volume of data sent is proportional to the surface area of the interior block, as slabs of thickness $h$ are exchanged across each of the six faces. The total number of cells communicated is $N_{\\text{comm}} = 2(n_x n_y h + n_x n_z h + n_y n_z h)$. To minimize communication volume, we must minimize the interior block's surface area function $S(n_x, n_y, n_z) = n_x n_y + n_x n_z + n_y n_z$.\n\n**3. Solve the Constrained Optimization Problem**\nWe need to minimize $S(n_x, n_y, n_z)$ subject to the memory constraint. It is a well-known geometric principle that for a given volume, the shape that minimizes surface area is a cube. This principle extends to our problem: the optimal shape for the interior block that minimizes its surface area for a fixed total memory volume is a cube. Therefore, the optimal solution must have:\n$$ n_x = n_y = n_z = n $$\n\n**4. Determine the Optimal Dimension**\nSubstitute the cubic shape condition $n_x = n_y = n_z = n$ into the memory constraint equation:\n$$ (n + 2h)(n + 2h)(n + 2h) qb = M $$\n$$ (n + 2h)^3 qb = M $$\nNow, we solve for $n$:\n$$ (n + 2h)^3 = \\frac{M}{qb} $$\n$$ n + 2h = \\left(\\frac{M}{qb}\\right)^{1/3} $$\n$$ n = \\left(\\frac{M}{qb}\\right)^{1/3} - 2h $$\nThis single value represents the optimal length for each of the interior dimensions $(n_x, n_y, n_z)$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\left(\\frac{M}{q b}\\right)^{1/3} - 2h  \\left(\\frac{M}{q b}\\right)^{1/3} - 2h  \\left(\\frac{M}{q b}\\right)^{1/3} - 2h\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Implementing the communication patterns derived from domain decomposition requires careful handling of parallel programming primitives to avoid subtle but catastrophic bugs. This hands-on coding exercise challenges you to design and simulate a deadlock-free halo exchange protocol using non-blocking communication, a critical real-world problem in Message Passing Interface (MPI) programming. Successfully completing this task demonstrates a mastery of the coordination required for robust parallel algorithms. ",
            "id": "4051447",
            "problem": "Consider a $3$-dimensional domain decomposition for a geophysical model on a Cartesian processor grid of size $N_x \\times N_y \\times N_z$, where each processor holds a subdomain and exchanges halo data with its six face-neighbor processors. In High-Performance Computing (HPC), halo exchanges are often implemented with nonblocking primitives in the Message Passing Interface (MPI). In a rendezvous protocol for large messages, a send operation can only complete after the matching receive is posted on the destination. Assume a simplified, abstracted semantics for nonblocking communication:\n\n- Each processor can post nonblocking receive operations, denoted as $P_r$, which do not consume injection capacity.\n- Each processor can post nonblocking send operations, denoted as $P_s$, up to an injection capacity of $C$ outstanding send messages. Injection capacity $C$ is measured as a count of messages, not bytes.\n- Each posted send of volume $V$ bytes behaves as follows:\n  - If $V \\le V_\\mathrm{RZ}$ (eager regime), it can complete without a prior matching receive, provided injection capacity allows $P_s$ to be posted.\n  - If $V  V_\\mathrm{RZ}$ (rendezvous regime), it can complete only after the destination has posted the matching $P_r$.\n- A wait operation, denoted as $W$, blocks until all previously posted operations in its scope have completed.\n- The grid is nonperiodic; boundary processors have fewer than six neighbors.\n\nDeadlock in this abstract model is defined via a wait-for graph: if there exists a cycle of operations where each processor is waiting for another processor to perform an action that cannot occur due to posted operations and capacity constraints, then the system is deadlocked.\n\nTask: Construct a deadlock-free nonblocking halo exchange for $3$-dimensional decomposition by ordering $P_r$, $P_s$, and $W$ operations correctly. Your algorithm must:\n\n1. Respect rendezvous semantics when $V  V_\\mathrm{RZ}$.\n2. Respect per-processor injection capacity $C$ messages.\n3. Avoid deadlock by ensuring the wait-for graph is acyclic.\n4. Complete halo exchanges along the $x$-, $y$-, and $z$-axes.\n\nYou must implement a simulation of this abstracted protocol in a program. The simulation should model processors at integer coordinates $(i,j,k)$ for $0 \\le i lt; N_x$, $0 \\le j lt; N_y$, $0 \\le k lt; N_z$, with unit-distance neighbors in the six axis-aligned directions where they exist. Each halo exchange along an axis consists of at most two directional messages per processor (one to the negative direction neighbor and one to the positive direction neighbor), constrained by boundary conditions.\n\nDesign an ordering strategy that achieves deadlock freedom and termination for rendezvous messages by phasing operations along each axis and using a checkerboard parity to serialize conflicting sends. Specifically, you must decide, for each axis, the sequence in which processors post $P_r$, post $P_s$, and invoke $W$, taking into account that $C$ may be as small as $1$.\n\nSimulation rules to implement:\n\n- The program must internally model the posting of $P_r$ and $P_s$ operations, their matching, and completion under the rendezvous constraint when $V  V_\\mathrm{RZ}$.\n- Posting a $P_s$ increases a processorâ€™s outstanding send count by $1$; completion of that send decreases it by $1$.\n- A $P_r$ can be posted at any time and does not consume capacity.\n- A send in the rendezvous regime completes only if the destination has posted the matching $P_r$.\n- A wait $W$ for an axis must ensure all posted operations for that axis complete before proceeding to the next axis.\n- If, during a $W$, no operations can complete and no further posting is possible due to capacity or ordering constraints, the simulation declares deadlock for that test case.\n\nYour program should implement your ordering strategy and report whether the halo exchange completes without deadlock for a test suite of cases. Use $V_\\mathrm{RZ} = 512$ bytes. All message volumes $V$ in the test suite are in bytes.\n\nTest Suite (each case is a tuple $(N_x,N_y,N_z,V,C)$):\n\n- Case $1$: $(2,2,2,4096,1)$, rendezvous regime, balanced grid; expected to complete.\n- Case $2$: $(4,1,1,4096,1)$, rendezvous regime, $x$-line decomposition; expected to complete.\n- Case $3$: $(3,3,1,8192,1)$, rendezvous regime, slab decomposition; expected to complete.\n- Case $4$: $(3,3,3,8192,1)$, rendezvous regime, cubic grid; expected to complete.\n- Case $5$: $(2,2,2,8192,0)$, rendezvous regime with zero injection capacity; expected to deadlock.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, \"[$r_1$,$r_2$,$r_3$,$r_4$,$r_5$]\" where each $r_i$ is a boolean indicating whether the halo exchange completes ($\\mathrm{True}$) or deadlocks ($\\mathrm{False}$) for case $i$.",
            "solution": "The primary challenge is to avoid deadlock, which arises from circular \"wait-for\" dependencies where processors wait for each other to post a receive ($P_r$) for a rendezvous send ($P_s$) they have already posted. A limited injection capacity $C$ exacerbates this risk.\n\nThe standard and most robust solution, which is implemented in the accompanying code, is to break these cyclical dependencies by structuring the communication in phases.\n\n1.  **Dimensional Phasing**: The full 3D halo exchange is decomposed into three independent, sequential exchanges, one for each axis ($x$, $y$, and $z$). A global synchronization (a wait operation, $W$) is placed between each axis exchange. This ensures that all dependencies are confined to a single axis at a time.\n\n2.  **Parity-Based Role Phasing (Checkerboard Pattern)**: Within a single axis exchange, deadlocks between adjacent processors (e.g., processor $i$ and $i+1$ both sending to each other) are prevented by assigning roles based on processor coordinate parity. The exchange along an axis is split into two sub-phases:\n    *   **Sub-phase 1**: Processors with an even coordinate index along the axis act as \"senders,\" and processors with an odd coordinate index act as \"receivers.\" The even-indexed processors post sends ($P_s$) to their odd-indexed neighbors, while odd-indexed processors post receives ($P_r$) from their even-indexed neighbors. Because sends are only directed from even to odd processors, the wait-for graph is a directed acyclic graph (DAG), and no deadlock can occur.\n    *   **Sub-phase 2**: The roles are reversed. Odd-indexed processors become \"senders,\" and even-indexed processors become \"receivers.\" This completes the bidirectional halo exchange for the axis and is also deadlock-free for the same reason.\n\nThis two-phase, parity-based approach guarantees deadlock freedom even with a minimal injection capacity of $C=1$. The provided simulation code models this strategy, along with the logic for handling outstanding operations and detecting deadlock, to verify its correctness against the test suite.",
            "answer": "```python\nimport numpy as np\nfrom collections import namedtuple\n\n# Define simple data structures for communication operations\nSendOperation = namedtuple('SendOperation', ['dest_coords'])\nRecvOperation = namedtuple('RecvOperation', ['source_coords'])\n\nclass Processor:\n    \"\"\"Models a single processor in the Cartesian grid.\"\"\"\n    \n    def __init__(self, coords, capacity):\n        self.coords = coords\n        self.capacity = capacity\n        self.reset_phase_state()\n\n    def reset_phase_state(self):\n        \"\"\"Resets the state for a new communication phase.\"\"\"\n        self.pending_sends = []\n        self.outstanding_sends = []\n        self.posted_receives = []\n\n    def post_send(self, dest_coords):\n        \"\"\"Posts a send operation, respecting injection capacity.\"\"\"\n        op = SendOperation(dest_coords=dest_coords)\n        if len(self.outstanding_sends)  self.capacity:\n            self.outstanding_sends.append(op)\n        else:\n            self.pending_sends.append(op)\n            \n    def post_receive(self, source_coords):\n        \"\"\"Posts a receive operation.\"\"\"\n        op = RecvOperation(source_coords=source_coords)\n        self.posted_receives.append(op)\n\ndef simulate_wait_phase(processors, is_rendezvous):\n    \"\"\"\n    Simulates the network until all posted operations complete for a phase.\n    Returns True on success, False on deadlock.\n    \"\"\"\n    while True:\n        made_progress = False\n\n        # --- Part 1: Try to complete outstanding sends ---\n        # We iterate over a snapshot of all sends to allow modification of lists during the loop\n        sends_to_check = []\n        for p in processors.values():\n            for send_op in p.outstanding_sends:\n                sends_to_check.append((p, send_op))\n        \n        for sender_proc, send_op in sends_to_check:\n            # Check if this send is still outstanding (might have been completed already in this loop)\n            if send_op not in sender_proc.outstanding_sends:\n                continue\n\n            dest_proc = processors[send_op.dest_coords]\n            \n            matching_recv = next((recv for recv in dest_proc.posted_receives if recv.source_coords == sender_proc.coords), None)\n            \n            can_complete = (not is_rendezvous) or (is_rendezvous and matching_recv is not None)\n            \n            if can_complete:\n                sender_proc.outstanding_sends.remove(send_op)\n                if matching_recv:\n                    dest_proc.posted_receives.remove(matching_recv)\n                made_progress = True\n\n        # --- Part 2: Try to post pending sends ---\n        for p in processors.values():\n            while p.pending_sends and len(p.outstanding_sends)  p.capacity:\n                op_to_post = p.pending_sends.pop(0)\n                p.outstanding_sends.append(op_to_post)\n                made_progress = True\n\n        # --- Part 3: Check for completion or deadlock ---\n        all_done = all(not p.pending_sends and not p.outstanding_sends for p in processors.values())\n        \n        if all_done:\n            # All posted and pending sends for this phase are complete.\n            # We must also check that all receives were matched.\n            if any(p.posted_receives for p in processors.values()):\n                # This indicates an asymmetry or error in the algorithm setup\n                return False \n            return True # Phase successful\n\n        if not made_progress:\n            return False # Deadlock: work remains but no progress was made.\n\ndef run_simulation(Nx, Ny, Nz, V, C):\n    \"\"\"\n    Runs the simulation for a single test case.\n    Returns True if the halo exchange completes, False if it deadlocks.\n    \"\"\"\n    V_RZ = 512\n    is_rendezvous = V > V_RZ\n    \n    processors = {\n        (i, j, k): Processor(coords=(i, j, k), capacity=C)\n        for i in range(Nx) for j in range(Ny) for k in range(Nz)\n    }\n\n    dimensions = [(0, Nx), (1, Ny), (2, Nz)]\n\n    for dim_idx, N_dim in dimensions:\n        if N_dim = 1:\n            continue\n\n        # --- Phase 1: Even-indexed processors send, Odd-indexed receive ---\n        for p in processors.values():\n            p.reset_phase_state()\n\n        # Odd processors post receives from their even neighbors\n        for coords, proc in processors.items():\n            if coords[dim_idx] % 2 != 0:\n                if coords[dim_idx] > 0: # has negative neighbor\n                    neg_coords = list(coords); neg_coords[dim_idx] -= 1\n                    proc.post_receive(source_coords=tuple(neg_coords))\n                if coords[dim_idx]  N_dim - 1: # has positive neighbor\n                    pos_coords = list(coords); pos_coords[dim_idx] += 1\n                    proc.post_receive(source_coords=tuple(pos_coords))\n\n        # Even processors post sends to their odd neighbors\n        for coords, proc in processors.items():\n            if coords[dim_idx] % 2 == 0:\n                if coords[dim_idx] > 0: # has negative neighbor\n                    neg_coords = list(coords); neg_coords[dim_idx] -= 1\n                    proc.post_send(dest_coords=tuple(neg_coords))\n                if coords[dim_idx]  N_dim - 1: # has positive neighbor\n                    pos_coords = list(coords); pos_coords[dim_idx] += 1\n                    proc.post_send(dest_coords=tuple(pos_coords))\n\n        if not simulate_wait_phase(processors, is_rendezvous):\n            return False\n\n        # --- Phase 2: Odd-indexed processors send, Even-indexed receive ---\n        for p in processors.values():\n            p.reset_phase_state()\n\n        # Even processors post receives from their odd neighbors\n        for coords, proc in processors.items():\n            if coords[dim_idx] % 2 == 0:\n                if coords[dim_idx] > 0:\n                    neg_coords = list(coords); neg_coords[dim_idx] -= 1\n                    proc.post_receive(source_coords=tuple(neg_coords))\n                if coords[dim_idx]  N_dim - 1:\n                    pos_coords = list(coords); pos_coords[dim_idx] += 1\n                    proc.post_receive(source_coords=tuple(pos_coords))\n\n        # Odd processors post sends to their even neighbors\n        for coords, proc in processors.items():\n            if coords[dim_idx] % 2 != 0:\n                if coords[dim_idx] > 0:\n                    neg_coords = list(coords); neg_coords[dim_idx] -= 1\n                    proc.post_send(dest_coords=tuple(neg_coords))\n                if coords[dim_idx]  N_dim - 1:\n                    pos_coords = list(coords); pos_coords[dim_idx] += 1\n                    proc.post_send(dest_coords=tuple(pos_coords))\n\n        if not simulate_wait_phase(processors, is_rendezvous):\n            return False\n\n    return True\n\ndef solve():\n    test_cases = [\n        (2, 2, 2, 4096, 1),\n        (4, 1, 1, 4096, 1),\n        (3, 3, 1, 8192, 1),\n        (3, 3, 3, 8192, 1),\n        (2, 2, 2, 8192, 0),\n    ]\n\n    results = []\n    for case in test_cases:\n        Nx, Ny, Nz, V, C = case\n        result = run_simulation(Nx, Ny, Nz, V, C)\n        results.append(result)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}