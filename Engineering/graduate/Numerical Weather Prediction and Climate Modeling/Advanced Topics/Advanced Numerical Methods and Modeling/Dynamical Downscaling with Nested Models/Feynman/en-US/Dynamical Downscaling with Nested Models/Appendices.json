{
    "hands_on_practices": [
        {
            "introduction": "A primary challenge in dynamical downscaling is managing the artificial lateral boundaries of the nested, limited-area model (LAM). Without proper handling, meteorological waves can reflect off these boundaries, generating spurious noise that contaminates the model solution. This practice  delves into the buffer zone, a common and effective solution where the LAM's prognostic variables are relaxed, or \"nudged,\" toward the driving data from the parent model. By deriving the attenuation of a perturbation as it passes through this zone, you will gain a quantitative understanding of how this technique maintains the stability and physical realism of the nested simulation.",
            "id": "4032871",
            "problem": "A one-way nested Limited-Area Model (LAM) of lateral width $600\\ \\mathrm{km}$ employs a lateral buffer zone of uniform width $W=60\\ \\mathrm{km}$, within which Newtonian relaxation (also called Davies-type relaxation) is applied uniformly in time with a constant relaxation timescale $\\tau=600\\ \\mathrm{s}$. Consider a synoptic-scale perturbation of passive scalar amplitude $a$ that enters the buffer from the outer boundary and is advected steadily inward with constant phase speed $U=15\\ \\mathrm{m}\\ \\mathrm{s}^{-1}$ normal to the boundary. Assume the perturbation evolves within the buffer by linear advection and linear Newtonian relaxation only, with no sources, sinks, or nonlinearity, and that $a$ is sufficiently small to justify linear superposition.\n\nStarting from first principles of linear advection and Newtonian relaxation along a Lagrangian trajectory, derive the expression for the attenuation factor $A$, defined as the ratio of the perturbation amplitude at the inner edge of the buffer to its amplitude at the outer boundary. Then evaluate $A$ for the given parameters. Express the final answer as a dimensionless number rounded to three significant figures.",
            "solution": "The problem concerns the amplitude evolution of a passively advected perturbation subject to Newtonian relaxation in a buffer zone. The foundational elements are:\n\n1. Linear advection of a materially conserved quantity implies that along a Lagrangian trajectory, advection alone does not change the amplitude $a$; material changes arise from non-advective processes.\n2. Newtonian relaxation of a perturbation $a$ with timescale $\\tau$ imposes a linear damping tendency proportional to $-a/\\tau$.\n\nLet $a(t)$ denote the perturbation amplitude following the parcel as it traverses the buffer. Under the assumptions stated, the amplitude evolution along the Lagrangian trajectory within the buffer is governed by the ordinary differential equation\n$$\n\\frac{\\mathrm{d}a}{\\mathrm{d}t} \\;=\\; -\\frac{1}{\\tau}\\,a.\n$$\nThis equation follows from the fact that advection transports $a$ without changing it along a trajectory, while the only local process acting on $a$ is Newtonian relaxation with rate $1/\\tau$.\n\nSolving this linear ordinary differential equation by separation of variables yields\n$$\n\\frac{\\mathrm{d}a}{a} \\;=\\; -\\frac{\\mathrm{d}t}{\\tau}\n\\quad\\Rightarrow\\quad\n\\ln a(t) - \\ln a(0) \\;=\\; -\\frac{t}{\\tau}\n\\quad\\Rightarrow\\quad\na(t) \\;=\\; a(0)\\,\\exp\\!\\left(-\\frac{t}{\\tau}\\right).\n$$\nThe perturbation enters the buffer at time $t=0$ at the outer boundary, and exits the buffer at time $t=t_{\\mathrm{buf}}$ at the inner edge. The travel time across the buffer of width $W$ at speed $U$ is the advection time\n$$\nt_{\\mathrm{buf}} \\;=\\; \\frac{W}{U}.\n$$\nTherefore, the attenuation factor $A$, defined as the ratio of the amplitude at the inner edge to that at the outer boundary, is\n$$\nA \\;\\equiv\\; \\frac{a(t_{\\mathrm{buf}})}{a(0)} \\;=\\; \\exp\\!\\left(-\\frac{t_{\\mathrm{buf}}}{\\tau}\\right) \\;=\\; \\exp\\!\\left(-\\frac{W}{U\\,\\tau}\\right).\n$$\n\nFor the given values $W=60\\ \\mathrm{km}=60{,}000\\ \\mathrm{m}$, $U=15\\ \\mathrm{m}\\ \\mathrm{s}^{-1}$, and $\\tau=600\\ \\mathrm{s}$, we compute\n$$\nt_{\\mathrm{buf}} \\;=\\; \\frac{60{,}000\\ \\mathrm{m}}{15\\ \\mathrm{m}\\ \\mathrm{s}^{-1}} \\;=\\; 4{,}000\\ \\mathrm{s},\n$$\nand thus\n$$\n\\frac{t_{\\mathrm{buf}}}{\\tau} \\;=\\; \\frac{4{,}000\\ \\mathrm{s}}{600\\ \\mathrm{s}} \\;=\\; \\frac{20}{3} \\;\\approx\\; 6.666\\overline{6}.\n$$\nHence,\n$$\nA \\;=\\; \\exp\\!\\left(-\\frac{20}{3}\\right) \\;\\approx\\; 1.2726338\\times 10^{-3}.\n$$\n\nRounded to three significant figures, the attenuation factor is $1.27\\times 10^{-3}$. Interpreting this value, the perturbation experiences approximately $6.67$ e-foldings of decay across the buffer, which strongly damps incoming synoptic disturbances within the buffer transit time, indicating that the chosen relaxation timescale is adequate for $U=15\\ \\mathrm{m}\\ \\mathrm{s}^{-1}$ synoptic motions. However, as required, the requested final answer is the numerical value of $A$.",
            "answer": "$$\\boxed{1.27 \\times 10^{-3}}$$"
        },
        {
            "introduction": "To resolve finer-scale atmospheric features, a child model uses a much smaller grid spacing than its parent model. This spatial refinement necessitates a correspondingly smaller time step to satisfy the Courant–Friedrichs–Lewy (CFL) condition for numerical stability. This exercise  explores the critical concept of temporal subcycling, where the child model executes multiple time steps for each single time step of the parent model. You will analyze the synchronization between the two models, a fundamental aspect of ensuring that the lateral boundary conditions are passed consistently from the parent to the child.",
            "id": "4032922",
            "problem": "Consider a one-way nested regional model embedded within a coarser parent model for numerical weather prediction. The parent model advances prognostic fields on a uniform time grid with time step $\\Delta t_p$, and the nested child model advances on a finer time grid with time step $\\Delta t_c$. The child receives Lateral Boundary Conditions (LBC) from the parent, which are updated at the parent model time levels and held piecewise-constant between successive parent updates. Assume a refinement ratio $r$ (defined by the spatial grid ratio) and that temporal subcycling is used so that the child executes multiple substeps within each parent step.\n\nStarting from the Courant–Friedrichs–Lewy (CFL) condition for a linear advection scheme and the definition of discrete time stepping on nested grids, derive the child subcycling schedule within a single parent time interval $[t_n, t_{n+1}]$ in terms of the parent time levels $t_n = t_0 + n \\,\\Delta t_p$, the child substep index $k \\in \\{1,\\dots,r\\}$, and the time steps $\\Delta t_p$ and $\\Delta t_c$. Specialize your result to the case $r = 3$, $\\Delta t_p = 60$ s, and a child requirement $\\Delta t_c = 20$ s. Then, to verify the temporal consistency of LBC updates with the child subcycling, define the LBC consistency residual\n$$\n\\delta \\equiv \\Delta t_p - r\\,\\Delta t_c,\n$$\nwhich measures the mismatch between the parent update interval and the cumulative duration of one full set of $r$ child substeps. Compute $\\delta$ for the given values and interpret its value in terms of whether child substeps land exactly on parent update times or would require temporal interpolation of LBCs.\n\nExpress your final answer as the value of $\\delta$ in seconds. No derivations are required in the final answer. No rounding is required; report an exact value.",
            "solution": "The problem statement is valid. It is scientifically grounded in the principles of numerical weather prediction and climate modeling, specifically concerning the technique of one-way grid nesting. The concepts of parent and child models, time step refinement, the Courant–Friedrichs–Lewy (CFL) condition, and Lateral Boundary Condition (LBC) updates are standard and well-defined. The problem is self-contained, objective, and well-posed, admitting a unique and meaningful solution.\n\nWe begin by establishing the context. In nested grid modeling, a high-resolution \"child\" model is embedded within a lower-resolution \"parent\" model. The parent model provides the initial state and time-dependent LBCs for the child model. To maintain numerical stability on the finer spatial grid of the child model (with grid spacing $\\Delta x_c$), a smaller time step ($\\Delta t_c$) is typically required compared to the parent model's time step ($\\Delta t_p$). This is dictated by the CFL condition, which for a simple one-dimensional linear advection equation with wave speed $u$ is given by:\n$$\nC = \\frac{u \\Delta t}{\\Delta x} \\le C_{\\text{max}}\n$$\nwhere $C$ is the Courant number and $C_{\\text{max}}$ is a constant typically on the order of $1$ for explicit numerical schemes. If a spatial refinement ratio $r$ is used, such that $\\Delta x_c = \\Delta x_p / r$, then to maintain a similar Courant number on both grids, the time steps must be related by approximately $\\Delta t_c \\approx \\Delta t_p / r$. This practice is known as temporal subcycling, where the child model executes $r$ time steps for every single time step of the parent model.\n\nThe problem asks for the derivation of the child subcycling schedule. Let the parent model advance on a time grid defined by discrete time levels $t_n = t_0 + n \\Delta t_p$ for integer $n \\ge 0$. Consider a single parent time interval from $t_n$ to $t_{n+1} = t_n + \\Delta t_p$. The child model starts its integration for this interval at time $t_n$, using LBCs provided by the parent model valid at time $t_n$. These LBCs are held piecewise-constant until the next parent update time, $t_{n+1}$.\n\nThe child model performs $r$ substeps, each of duration $\\Delta t_c$. Let $\\tau_{n,k}$ denote the time at the *end* of the $k$-th child substep within the $n$-th parent interval, where the substep index is $k \\in \\{1, 2, \\dots, r\\}$.\nThe first substep starts at $t_n$ and ends at:\n$$\n\\tau_{n,1} = t_n + \\Delta t_c\n$$\nThe second substep starts at $\\tau_{n,1}$ and ends at:\n$$\n\\tau_{n,2} = \\tau_{n,1} + \\Delta t_c = (t_n + \\Delta t_c) + \\Delta t_c = t_n + 2 \\Delta t_c\n$$\nGeneralizing this, the time at the end of the $k$-th substep is given by:\n$$\n\\tau_{n,k} = t_n + k \\Delta t_c\n$$\nSubstituting the expression for the parent time level $t_n = t_0 + n \\Delta t_p$, we obtain the general formula for the child model's discrete time levels:\n$$\n\\tau_{n,k} = t_0 + n \\Delta t_p + k \\Delta t_c, \\quad \\text{for } n \\ge 0 \\text{ and } k \\in \\{1, 2, \\dots, r\\}\n$$\nThis is the child subcycling schedule.\n\nFor the specific case given, we have a refinement ratio $r = 3$, a parent time step $\\Delta t_p = 60$ s, and a child time step $\\Delta t_c = 20$ s. The child model will perform $3$ substeps of $20$ s each for every $60$ s parent step. For a parent interval starting at $t_n$, the child model times will be:\n-   End of 1st substep: $\\tau_{n,1} = t_n + 1 \\times 20 \\text{ s}$\n-   End of 2nd substep: $\\tau_{n,2} = t_n + 2 \\times 20 \\text{ s} = t_n + 40 \\text{ s}$\n-   End of 3rd substep: $\\tau_{n,3} = t_n + 3 \\times 20 \\text{ s} = t_n + 60 \\text{ s}$\nThe parent's next time level is $t_{n+1} = t_n + \\Delta t_p = t_n + 60$ s. We observe that $\\tau_{n,3} = t_{n+1}$, indicating perfect synchronization.\n\nNext, we compute and interpret the LBC consistency residual, $\\delta$. This quantity is defined as:\n$$\n\\delta \\equiv \\Delta t_p - r \\Delta t_c\n$$\nThis residual measures the temporal mismatch between the duration of one parent time step, $\\Delta t_p$, and the total accumulated time from one full cycle of $r$ child substeps, which is $r \\Delta t_c$.\n\nUsing the given values $r=3$, $\\Delta t_p = 60$ s, and $\\Delta t_c = 20$ s, we compute $\\delta$:\n$$\n\\delta = 60 \\text{ s} - (3)(20 \\text{ s}) = 60 \\text{ s} - 60 \\text{ s} = 0 \\text{ s}\n$$\n\nThe interpretation of this result is as follows:\nA value of $\\delta = 0$ indicates perfect temporal synchronization between the parent and child models. The final substep of the child model (the $r$-th substep) concludes at the exact moment the parent model completes its time step and provides new LBCs for the next interval. In this scenario, the LBCs, which are updated at times $t_n$, $t_{n+1}$, etc., and held piecewise-constant, can be applied to the child model without any temporal ambiguity or the need for interpolation. The child model receives the boundary data for time $t_{n+1}$ precisely when it is ready to begin its next cycle of substeps.\n\nIf $\\delta$ were non-zero, it would signify a temporal desynchronization. For instance, if $\\delta > 0$, the child model would finish its $r$ substeps before the parent model provides the new LBCs. If $\\delta  0$, the parent model's update time would arrive before the child model has completed its full set of $r$ substeps. In either of these asynchronous cases ($\\delta \\neq 0$), a simple piecewise-constant application of LBCs becomes problematic. To maintain accuracy, one would typically need to perform temporal interpolation (e.g., linear interpolation) of the LBCs from the parent model's output times ($t_n, t_{n+1}$) to the specific times required by the child model's boundary. The fact that $\\delta=0$ confirms the ideal and simplest coupling strategy is being used.\n\nThe final answer required is the value of $\\delta$ in seconds.\n$$\n\\delta = 0\n$$",
            "answer": "$$\\boxed{0}$$"
        },
        {
            "introduction": "The ultimate goal of dynamical downscaling is to generate more accurate and detailed regional forecasts than are available from a coarse global model. But does a higher-resolution model always produce a better forecast? This practice  addresses this crucial question of \"added value\" by introducing modern verification techniques based on quantile scoring. By implementing and applying these metrics to hypothetical forecast data, you will learn how to rigorously quantify whether the computationally expensive child model provides a meaningful improvement in skill, particularly for challenging high-impact events like extreme precipitation.",
            "id": "4032939",
            "problem": "Consider a nested dynamical downscaling setup within Numerical Weather Prediction (NWP), where a $12\\,\\mathrm{km}$ \"parent\" model provides lateral boundary conditions and a $3\\,\\mathrm{km}$ \"child\" model refines the solution over a smaller domain. Verification is performed against observations aggregated to a $5\\,\\mathrm{km}$ scale for an event characterized by precipitation extremes. The objective is to quantify the difference in extremal quantile skill between the parent and child and interpret the added value of the child. Work entirely in mathematical terms using the following definitions and test data.\n\nDefinitions:\n- Let $\\tau \\in (0,1)$ denote the quantile level. For extremes, take $\\tau$ close to $1$ (e.g., $\\tau=0.98$).\n- For a set of $n$ verification points with observed precipitation intensities $y_i$ in $\\mathrm{mm}/\\mathrm{h}$ and a model’s predicted $\\tau$-quantile $q_i$, define the Quantile Score (QS), also known as the pinball loss, as\n$$\n\\mathrm{QS}_\\tau(y,q) \\equiv \\frac{1}{n}\\sum_{i=1}^{n} \\left(\\tau - \\mathbb{I}\\{y_i  q_i\\}\\right)\\,(y_i - q_i),\n$$\nwhere $\\mathbb{I}\\{\\cdot\\}$ is the indicator function.\n- Given a common reference quantile predictor $q_i^{\\mathrm{ref}}$ (e.g., a climatological $\\tau$-quantile) with $\\mathrm{QS}_\\tau(y,q^{\\mathrm{ref}}) > 0$, define the Quantile Score Skill (QSS) for a model with quantile prediction $q$ as\n$$\n\\mathrm{QSS}_\\tau(y,q; q^{\\mathrm{ref}}) \\equiv 1 - \\frac{\\mathrm{QS}_\\tau(y,q)}{\\mathrm{QS}_\\tau(y,q^{\\mathrm{ref}})}.\n$$\nHigher values of $\\mathrm{QSS}_\\tau$ indicate better skill relative to the reference.\n- Define the skill difference\n$$\n\\Delta \\mathrm{QSS}_\\tau \\equiv \\mathrm{QSS}_\\tau(y,q^{\\mathrm{child}}; q^{\\mathrm{ref}}) - \\mathrm{QSS}_\\tau(y,q^{\\mathrm{parent}}; q^{\\mathrm{ref}}),\n$$\nwhere $q^{\\mathrm{parent}}$ and $q^{\\mathrm{child}}$ are the parent and child model $\\tau$-quantile predictions, respectively. A positive $\\Delta \\mathrm{QSS}_\\tau$ indicates that the child model has higher skill than the parent relative to the same reference.\n- For interpretation of added value, also consider the fractional improvement of the child over the parent in terms of the Quantile Score:\n$$\n\\mathrm{AV}_\\tau \\equiv \\frac{\\mathrm{QS}_\\tau(y,q^{\\mathrm{parent}}) - \\mathrm{QS}_\\tau(y,q^{\\mathrm{child}})}{\\mathrm{QS}_\\tau(y,q^{\\mathrm{parent}})}.\n$$\nA positive $\\mathrm{AV}_\\tau$ indicates added value from the child model.\n\nUse the following test suite with $\\tau = 0.98$ and precipitation intensities in $\\mathrm{mm}/\\mathrm{h}$:\n\n- Test Case $1$ (general extreme event with mixed dry and heavy points):\n  - Observations $y$: $[0, 12, 35, 0.5, 60, 80]$.\n  - Parent $\\tau$-quantile predictions $q^{\\mathrm{parent}}$: $[5, 20, 40, 5, 50, 70]$.\n  - Child $\\tau$-quantile predictions $q^{\\mathrm{child}}$: $[3, 25, 45, 2, 55, 85]$.\n  - Reference $\\tau$-quantile predictions $q^{\\mathrm{ref}}$: $[10, 10, 10, 10, 10, 10]$.\n\n- Test Case $2$ (boundary condition with identical parent and child predictions):\n  - Observations $y$: $[10, 10, 10, 10]$.\n  - Parent $\\tau$-quantile predictions $q^{\\mathrm{parent}}$: $[12, 12, 12, 12]$.\n  - Child $\\tau$-quantile predictions $q^{\\mathrm{child}}$: $[12, 12, 12, 12]$.\n  - Reference $\\tau$-quantile predictions $q^{\\mathrm{ref}}$: $[15, 15, 15, 15]$.\n\n- Test Case $3$ (edge case combining dry points and extreme heavy precipitation):\n  - Observations $y$: $[0, 0, 0, 40, 100]$.\n  - Parent $\\tau$-quantile predictions $q^{\\mathrm{parent}}$: $[5, 5, 5, 50, 90]$.\n  - Child $\\tau$-quantile predictions $q^{\\mathrm{child}}$: $[1, 1, 1, 60, 110]$.\n  - Reference $\\tau$-quantile predictions $q^{\\mathrm{ref}}$: $[8, 8, 8, 40, 100]$.\n\nRequirements:\n- Implement the above definitions to compute $\\Delta \\mathrm{QSS}_\\tau$ for each test case.\n- All precipitation values are in $\\mathrm{mm}/\\mathrm{h}$; the outputs are dimensionless skill differences.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result_1,result_2,result_3]$), where each $result_i$ is the value of $\\Delta \\mathrm{QSS}_\\tau$ for Test Case $i$, rounded to six decimal places.",
            "solution": "We start from the definition of a $\\tau$-quantile estimator in probabilistic forecasting, which is grounded in minimizing the expected pinball loss. For a given $\\tau \\in (0,1)$ and a forecast $\\tau$-quantile $q$, the pinball loss for a single observation $y$ is defined by the function\n$$\n\\ell_\\tau(y,q) = \\left(\\tau - \\mathbb{I}\\{y  q\\}\\right)\\,(y - q).\n$$\nThis loss is nonnegative because when $y \\ge q$ the term is $\\tau (y - q) \\ge 0$, and when $y  q$ it becomes $(1-\\tau)(q - y) \\ge 0$. Aggregating over $n$ verification points yields the Quantile Score (QS),\n$$\n\\mathrm{QS}_\\tau(y,q) = \\frac{1}{n}\\sum_{i=1}^{n} \\ell_\\tau(y_i,q_i) = \\frac{1}{n}\\sum_{i=1}^{n}\\left(\\tau - \\mathbb{I}\\{y_i  q_i\\}\\right)\\,(y_i - q_i).\n$$\nLower values of $\\mathrm{QS}_\\tau$ are better because they indicate smaller pinball loss.\n\nTo obtain a skill measure relative to a fixed reference forecast $q^{\\mathrm{ref}}$, we define the Quantile Score Skill (QSS) as\n$$\n\\mathrm{QSS}_\\tau(y,q; q^{\\mathrm{ref}}) = 1 - \\frac{\\mathrm{QS}_\\tau(y,q)}{\\mathrm{QS}_\\tau(y,q^{\\mathrm{ref}})}.\n$$\nThis construction is analogous to conventional skill scores: if $\\mathrm{QS}_\\tau(y,q)$ is much smaller than $\\mathrm{QS}_\\tau(y,q^{\\mathrm{ref}})$, then $\\mathrm{QSS}_\\tau$ approaches $1$; if the model is worse than the reference, then $\\mathrm{QSS}_\\tau$ becomes negative.\n\nThe difference in skill between the child and parent models is\n$$\n\\Delta \\mathrm{QSS}_\\tau = \\mathrm{QSS}_\\tau(y,q^{\\mathrm{child}}; q^{\\mathrm{ref}}) - \\mathrm{QSS}_\\tau(y,q^{\\mathrm{parent}}; q^{\\mathrm{ref}})\n= \\left(1 - \\frac{\\mathrm{QS}_\\tau(y,q^{\\mathrm{child}})}{\\mathrm{QS}_\\tau(y,q^{\\mathrm{ref}})}\\right) - \\left(1 - \\frac{\\mathrm{QS}_\\tau(y,q^{\\mathrm{parent}})}{\\mathrm{QS}_\\tau(y,q^{\\mathrm{ref}})}\\right),\n$$\nwhich simplifies to\n$$\n\\Delta \\mathrm{QSS}_\\tau = \\frac{\\mathrm{QS}_\\tau(y,q^{\\mathrm{parent}}) - \\mathrm{QS}_\\tau(y,q^{\\mathrm{child}})}{\\mathrm{QS}_\\tau(y,q^{\\mathrm{ref}})}.\n$$\nThis shows that the sign and magnitude of $\\Delta \\mathrm{QSS}_\\tau$ depend on the difference in pinball losses between parent and child, scaled by the reference loss. A positive $\\Delta \\mathrm{QSS}_\\tau$ means the child has lower pinball loss than the parent, hence higher skill relative to the same reference.\n\nFor interpretation of added value, we also define\n$$\n\\mathrm{AV}_\\tau = \\frac{\\mathrm{QS}_\\tau(y,q^{\\mathrm{parent}}) - \\mathrm{QS}_\\tau(y,q^{\\mathrm{child}})}{\\mathrm{QS}_\\tau(y,q^{\\mathrm{parent}})}.\n$$\nThis is a fractional improvement metric: $\\mathrm{AV}_\\tau > 0$ indicates the child reduces loss relative to the parent, and its magnitude indicates the fraction of the parent’s loss that is eliminated.\n\nAlgorithmic steps:\n1. Fix $\\tau = 0.98$ for all test cases.\n2. For each test case, read arrays $y$, $q^{\\mathrm{parent}}$, $q^{\\mathrm{child}}$, and $q^{\\mathrm{ref}}$.\n3. Compute $\\mathrm{QS}_\\tau(y,q)$ for $q \\in \\{q^{\\mathrm{parent}}, q^{\\mathrm{child}}, q^{\\mathrm{ref}}\\}$ via the formula for $\\ell_\\tau(y_i,q_i)$ and averaging over all points.\n4. Compute $\\mathrm{QSS}_\\tau$ for parent and child using the reference.\n5. Compute $\\Delta \\mathrm{QSS}_\\tau$ as the difference of the child and parent skills.\n6. Round each $\\Delta \\mathrm{QSS}_\\tau$ to six decimal places and output the three results in a single list as specified.\n\nAnalytical verification for Test Case $1$ (to confirm magnitudes):\n- Using $\\tau = 0.98$, compute the per-point pinball losses and average them to obtain $\\mathrm{QS}_\\tau(y,q^{\\mathrm{parent}}) \\approx 3.341667$, $\\mathrm{QS}_\\tau(y,q^{\\mathrm{child}}) \\approx 0.925000$, and $\\mathrm{QS}_\\tau(y,q^{\\mathrm{ref}}) \\approx 24.075000$.\n- Then $\\mathrm{QSS}_\\tau(y,q^{\\mathrm{parent}}; q^{\\mathrm{ref}}) \\approx 1 - 3.341667/24.075 \\approx 0.8613$, $\\mathrm{QSS}_\\tau(y,q^{\\mathrm{child}}; q^{\\mathrm{ref}}) \\approx 1 - 0.925/24.075 \\approx 0.9616$, giving $\\Delta \\mathrm{QSS}_\\tau \\approx 0.1003 > 0$, indicating higher skill for the child and positive added value.\n\nFor Test Case $2$, since $q^{\\mathrm{child}} = q^{\\mathrm{parent}}$, both skill scores are equal and $\\Delta \\mathrm{QSS}_\\tau = 0$, meaning no added value.\n\nFor Test Case $3$, the child substantially reduces pinball loss relative to the parent, while the reference is unusually close to observations for heavy points, yielding a small $\\mathrm{QS}_\\tau(y,q^{\\mathrm{ref}})$ and thus a large positive $\\Delta \\mathrm{QSS}_\\tau$. This corresponds to strong added value from the child in representing extremes.\n\nThe program implements these steps to produce the required outputs.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef quantile_score(y: np.ndarray, q: np.ndarray, tau: float) - float:\n    \"\"\"\n    Compute the Quantile Score (pinball loss) for quantile level tau.\n    QS_tau(y,q) = mean( (tau - I[y  q]) * (y - q) )\n    \"\"\"\n    indicator = (y  q).astype(float)\n    loss = (tau - indicator) * (y - q)\n    return float(np.mean(loss))\n\ndef compute_delta_qss(y: np.ndarray, q_parent: np.ndarray, q_child: np.ndarray,\n                      q_ref: np.ndarray, tau: float) - float:\n    \"\"\"\n    Compute ΔQSS_tau = QSS_child - QSS_parent using a common reference.\n    \"\"\"\n    qs_parent = quantile_score(y, q_parent, tau)\n    qs_child = quantile_score(y, q_child, tau)\n    qs_ref = quantile_score(y, q_ref, tau)\n    # To avoid division by zero (should not occur with provided test cases),\n    # add a minimal epsilon safeguard if qs_ref is extremely small.\n    eps = 1e-12\n    denom = qs_ref if qs_ref  eps else eps\n    qss_parent = 1.0 - (qs_parent / denom)\n    qss_child = 1.0 - (qs_child / denom)\n    delta_qss = qss_child - qss_parent\n    return delta_qss\n\ndef solve():\n    tau = 0.98\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test Case 1\n        (\n            np.array([0.0, 12.0, 35.0, 0.5, 60.0, 80.0]),\n            np.array([5.0, 20.0, 40.0, 5.0, 50.0, 70.0]),\n            np.array([3.0, 25.0, 45.0, 2.0, 55.0, 85.0]),\n            np.array([10.0, 10.0, 10.0, 10.0, 10.0, 10.0]),\n        ),\n        # Test Case 2\n        (\n            np.array([10.0, 10.0, 10.0, 10.0]),\n            np.array([12.0, 12.0, 12.0, 12.0]),\n            np.array([12.0, 12.0, 12.0, 12.0]),\n            np.array([15.0, 15.0, 15.0, 15.0]),\n        ),\n        # Test Case 3\n        (\n            np.array([0.0, 0.0, 0.0, 40.0, 100.0]),\n            np.array([5.0, 5.0, 5.0, 50.0, 90.0]),\n            np.array([1.0, 1.0, 1.0, 60.0, 110.0]),\n            np.array([8.0, 8.0, 8.0, 40.0, 100.0]),\n        ),\n    ]\n\n    results = []\n    for (y, q_parent, q_child, q_ref) in test_cases:\n        delta_qss = compute_delta_qss(y, q_parent, q_child, q_ref, tau)\n        # Round to six decimal places as required\n        results.append(round(delta_qss, 6))\n\n    # Final print statement in the exact required format (no spaces).\n    print(\"[\" + \",\".join(f\"{r:.6f}\" for r in results) + \"]\")\n\nsolve()\n```"
        }
    ]
}