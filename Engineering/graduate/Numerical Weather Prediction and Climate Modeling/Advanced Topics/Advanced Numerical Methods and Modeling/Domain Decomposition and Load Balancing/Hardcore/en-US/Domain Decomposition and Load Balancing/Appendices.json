{
    "hands_on_practices": [
        {
            "introduction": "The foundation of parallelizing a grid-based climate or weather model is domain decomposition. This exercise guides you through the fundamental mechanics of partitioning a two-dimensional grid onto a Cartesian processor topology. By working through the integer arithmetic of block distribution, you will discover how even a perfectly uniform computational domain can lead to minor load imbalances, a crucial first step in understanding the challenges of parallel performance. ",
            "id": "4032181",
            "problem": "A global atmospheric model for numerical weather prediction is discretized on a uniform latitudeâ€“longitude grid with horizontal dimensions $N_x$ by $N_y$, where $N_x$ is the number of longitudes and $N_y$ is the number of latitudes. The model advances prognostic variables using a finite-volume scheme with uniform computational cost per horizontal grid cell, so the total computational cost is proportional to the number of grid cells assigned to a rank.\n\nTo parallelize the model, a two-dimensional Cartesian Message Passing Interface (MPI) domain decomposition is used, partitioning the horizontal grid into $p_x$ blocks along the $x$-direction and $p_y$ blocks along the $y$-direction, with one MPI rank per subdomain. You are given $N_x = 1536$, $N_y = 768$, and a total of $p = 128$ MPI ranks. For architectural symmetry reasons, the project manager requires $p_x = p_y$ in the horizontal decomposition. The partitioning must adhere to the following widely used block distribution rule to preserve load balance under uniform per-cell cost:\n- Along each dimension $\\alpha \\in \\{x,y\\}$, let $q_{\\alpha} = \\lfloor N_{\\alpha} / p_{\\alpha} \\rfloor$ and $r_{\\alpha} = N_{\\alpha} \\bmod p_{\\alpha}$. The first $r_{\\alpha}$ blocks along dimension $\\alpha$ receive $q_{\\alpha} + 1$ cells, and the remaining $p_{\\alpha} - r_{\\alpha}$ blocks receive $q_{\\alpha}$ cells.\n\nStarting from these definitions and constraints, determine:\n1. The largest feasible integers $p_x$ and $p_y$ such that $p_x = p_y$ and $p_x p_y \\le p$.\n2. The base block sizes $q_x$ and $q_y$, the larger sizes $q_x + 1$ and $q_y + 1$, and the remainders $r_x$ and $r_y$.\n3. The counts of ranks in each of the four local size categories: $(q_x + 1) \\times (q_y + 1)$, $(q_x + 1) \\times q_y$, $q_x \\times (q_y + 1)$, and $q_x \\times q_y$.\n4. The number of unused MPI ranks (idle ranks) resulting from the $p_x = p_y$ constraint.\n5. Under the assumption of uniform per-cell cost, the ratio of the maximum local workload to the minimum local workload across ranks.\n\nExpress your final answer as a row matrix using the LaTeX `pmatrix` environment with the entries in the following order: $p_x$, $p_y$, $q_x$, $q_x + 1$, $q_y$, $q_y + 1$, $r_x r_y$, $r_x (p_y - r_y)$, $(p_x - r_x) r_y$, $(p_x - r_x)(p_y - r_y)$, $p - p_x p_y$, and the workload ratio $\\frac{(q_x + 1)(q_y + 1)}{q_x q_y}$. No rounding is required, and the entries are pure numbers without units.",
            "solution": "The problem statement is evaluated to be valid. It is scientifically grounded in the principles of parallel computing and numerical methods, specifically domain decomposition for grid-based models. The problem is well-posed, providing all necessary data and constraints ($N_x = 1536$, $N_y = 768$, $p = 128$, $p_x = p_y$) to arrive at a unique, meaningful solution. The definitions and objectives are stated clearly and free of ambiguity or contradiction.\n\nWe will proceed with the solution by addressing each of the five parts of the problem in sequence.\n\n1. Determination of $p_x$ and $p_y$\n\nThe problem requires a two-dimensional Cartesian decomposition of the grid into $p_x \\times p_y$ subdomains. We are given the constraints $p_x = p_y$ and that the total number of utilized MPI ranks, $p_x p_y$, must not exceed the total available ranks, $p = 128$.\nSubstituting the first constraint into the second gives:\n$$p_x \\cdot p_x \\le p$$\n$$p_x^2 \\le 128$$\nWe seek the largest integer $p_x$ that satisfies this inequality. We take the square root of both sides:\n$$p_x \\le \\sqrt{128} = \\sqrt{64 \\times 2} = 8\\sqrt{2}$$\nUsing the approximation $\\sqrt{2} \\approx 1.414$, we find $p_x \\le 8 \\times 1.414 = 11.312$. The largest integer satisfying this condition is $p_x = 11$.\nDue to the constraint $p_x = p_y$, we have $p_y = 11$.\nThe total number of ranks used is $p_x p_y = 11 \\times 11 = 121$, which is indeed less than or equal to the available $128$ ranks.\n\n2. Calculation of Block Sizes and Remainders\n\nThe problem defines a block distribution rule based on integer division. For each dimension $\\alpha \\in \\{x,y\\}$, the base block size is $q_{\\alpha} = \\lfloor N_{\\alpha} / p_{\\alpha} \\rfloor$ and the remainder is $r_{\\alpha} = N_{\\alpha} \\bmod p_{\\alpha}$.\n\nFor the $x$-direction (longitude):\nGiven $N_x = 1536$ and $p_x = 11$.\n$$q_x = \\lfloor \\frac{1536}{11} \\rfloor = \\lfloor 139.636... \\rfloor = 139$$\n$$r_x = 1536 \\bmod 11$$\nTo find the remainder, we use the division algorithm: $1536 = 11 \\times 139 + r_x$.\n$11 \\times 139 = 1529$.\n$1536 = 1529 + 7$, so $r_x = 7$.\nThe block sizes in the $x$-direction are the base size $q_x = 139$ and the larger size $q_x + 1 = 140$.\n\nFor the $y$-direction (latitude):\nGiven $N_y = 768$ and $p_y = 11$.\n$$q_y = \\lfloor \\frac{768}{11} \\rfloor = \\lfloor 69.818... \\rfloor = 69$$\n$$r_y = 768 \\bmod 11$$\nUsing the division algorithm: $768 = 11 \\times 69 + r_y$.\n$11 \\times 69 = 759$.\n$768 = 759 + 9$, so $r_y = 9$.\nThe block sizes in the $y$-direction are the base size $q_y = 69$ and the larger size $q_y + 1 = 70$.\n\n3. Tallying Ranks by Local Size Category\n\nThe total $p_x \\times p_y$ domain is partitioned into four groups based on local grid size.\n- In the $x$-direction, the first $r_x = 7$ blocks have size $q_x+1$, and the remaining $p_x - r_x = 11 - 7 = 4$ blocks have size $q_x$.\n- In the $y$-direction, the first $r_y = 9$ blocks have size $q_y+1$, and the remaining $p_y - r_y = 11 - 9 = 2$ blocks have size $q_y$.\n\nThe number of ranks in each of the four size categories is:\n- Subdomains of size $(q_x + 1) \\times (q_y + 1)$: These correspond to the intersection of the first $r_x$ columns and first $r_y$ rows.\n  Number of ranks = $r_x \\times r_y = 7 \\times 9 = 63$.\n- Subdomains of size $(q_x + 1) \\times q_y$: These correspond to the first $r_x$ columns and the remaining $p_y - r_y$ rows.\n  Number of ranks = $r_x \\times (p_y - r_y) = 7 \\times 2 = 14$.\n- Subdomains of size $q_x \\times (q_y + 1)$: These correspond to the remaining $p_x - r_x$ columns and the first $r_y$ rows.\n  Number of ranks = $(p_x - r_x) \\times r_y = 4 \\times 9 = 36$.\n- Subdomains of size $q_x \\times q_y$: These correspond to the remaining $p_x - r_x$ columns and the remaining $p_y - r_y$ rows.\n  Number of ranks = $(p_x - r_x) \\times (p_y - r_y) = 4 \\times 2 = 8$.\nThe sum of ranks is $63 + 14 + 36 + 8 = 121$, which matches $p_x p_y$.\n\n4. Determination of Unused Ranks\n\nThe number of unused (idle) MPI ranks is the difference between the total available ranks and the total ranks utilized in the decomposition.\nNumber of idle ranks = $p - p_x p_y = 128 - 121 = 7$.\n\n5. Calculation of the Workload Imbalance Ratio\n\nAssuming uniform computational cost per grid cell, the workload of a rank is proportional to the number of grid cells in its subdomain. The workload imbalance is the ratio of the maximum to the minimum workload.\nThe four possible subdomain sizes (workloads) are:\n- $W_{max} = (q_x + 1) \\times (q_y + 1) = (139 + 1) \\times (69 + 1) = 140 \\times 70 = 9800$.\n- $W_2 = (q_x + 1) \\times q_y = 140 \\times 69 = 9660$.\n- $W_3 = q_x \\times (q_y + 1) = 139 \\times 70 = 9730$.\n- $W_{min} = q_x \\times q_y = 139 \\times 69 = 9591$.\nThe maximum workload is $9800$ and the minimum is $9591$. The problem asks for the ratio of the maximum to the minimum, which is $\\frac{W_{max}}{W_{min}}$.\n$$\\text{Ratio} = \\frac{(q_x + 1)(q_y + 1)}{q_x q_y} = \\frac{9800}{9591}$$\nThis fraction is irreducible, as the prime factorization of the numerator is $9800 = 2^3 \\times 5^2 \\times 7^2$ and the denominator is $9591 = 3 \\times 23 \\times 139$. There are no common prime factors.\n\nThe final results are assembled according to the specified order.\n- $p_x = 11$\n- $p_y = 11$\n- $q_x = 139$\n- $q_x+1 = 140$\n- $q_y = 69$\n- $q_y+1 = 70$\n- $r_x r_y = 63$\n- $r_x(p_y-r_y) = 14$\n- $(p_x-r_x)r_y = 36$\n- $(p_x-r_x)(p_y-r_y) = 8$\n- $p - p_x p_y = 7$\n- Workload ratio = $\\frac{9800}{9591}$",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n11 & 11 & 139 & 140 & 69 & 70 & 63 & 14 & 36 & 8 & 7 & \\frac{9800}{9591}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Achieving high parallel speedup requires more than just dividing the computational work; it demands a careful analysis of the communication overheads introduced by the decomposition. This advanced practice challenges you to synthesize Amdahl's Law with a standard latency-bandwidth model for communication costs in a three-dimensional domain. By deriving and calculating the communication-aware speedup, you will gain a deeper appreciation for the factors that limit strong scaling in real-world high-performance computing systems. ",
            "id": "4032167",
            "problem": "A global climate model updates prognostic variables on a three-dimensional grid using explicit time-stepping. The model is implemented on a High-Performance Computing (HPC) system using the Message Passing Interface (MPI) and domain decomposition. The spatial domain is a cube of $N \\times N \\times N$ grid cells with $N = 2048$, decomposed into $p$ equal subdomains arranged as a $q \\times q \\times q$ Cartesian partition, where $q^{3} = p$ and $q \\in \\mathbb{N}$. Each subdomain is a cube and exchanges halo data of width $h$ grid cells across all $6$ faces every time step, for $n_{v}$ prognostic variables stored in $b$-byte floating-point precision. The network parameters are a per-message latency $\\alpha$ and inverse bandwidth $\\beta$. Assume no overlap of computation and communication.\n\nThe single-processor baseline time per time step (including physics and dynamics) is $T_{1} = 10$ s. A fraction $f = 0.98$ of $T_{1}$ is ideally parallelizable and the remaining fraction $1 - f$ is strictly serial. In a strong-scaling regime, the ideal per-time-step compute time on $p$ processors is obtained by dividing the parallelizable work evenly, while the serial fraction is unchanged. Communication consists of nearest-neighbor exchanges of halo data each time step. For a cube of side length $N/q$ grid cells, the message size per face is\n$$\nm(p) = h \\, n_{v} \\, b \\, \\left(\\frac{N}{q}\\right)^{2},\n$$\nand the per-processor communication time per time step is\n$$\nT_{\\mathrm{comm}}(p) = 6 \\left[ \\alpha + \\beta \\, m(p) \\right].\n$$\nUse $p = 512$, $h = 4$, $n_{v} = 50$, $b = 8$ bytes, $\\alpha = 2 \\times 10^{-5}$ s, and $\\beta = 5 \\times 10^{-11}$ s/byte.\n\nStarting from the definition of speedup as $S(p) = T_{1}/T_{p}$ and the decomposition of $T_{p}$ into parallelizable, strictly serial, and communication components, first derive the ideal speedup formula in the absence of communication from first principles. Then incorporate $T_{\\mathrm{comm}}(p)$ to express the communication-aware speedup $S(p)$ and the effective parallel fraction $f_{\\mathrm{eff}}(p)$ that would reproduce the same $T_{p}$ under an Amdahl-like partitioning. Explain qualitatively, using the geometry of domain decomposition, how the scaling of $T_{\\mathrm{comm}}(p)$ with $p$ affects $f_{\\mathrm{eff}}(p)$ as $p$ increases.\n\nFinally, compute the communication-aware speedup $S(p)$ for the given parameters at $p = 512$. Round your final speedup to four significant figures. Express your final answer as a dimensionless number.",
            "solution": "The problem is first validated to ensure it is self-contained, scientifically grounded, and well-posed before a solution is attempted.\n\n### Step 1: Extract Givens\n-   Domain grid dimensions: $N \\times N \\times N$ with $N = 2048$.\n-   Number of processors (subdomains): $p$.\n-   Subdomain partition: $q \\times q \\times q$, where $q^{3} = p$ and $q \\in \\mathbb{N}$.\n-   Halo width: $h = 4$ grid cells.\n-   Number of prognostic variables: $n_{v} = 50$.\n-   Floating-point precision: $b = 8$ bytes.\n-   Network latency: $\\alpha = 2 \\times 10^{-5}$ s.\n-   Network inverse bandwidth: $\\beta = 5 \\times 10^{-11}$ s/byte.\n-   Single-processor time per step: $T_{1} = 10$ s.\n-   Ideally parallelizable fraction of $T_{1}$: $f = 0.98$.\n-   Strictly serial fraction of $T_{1}$: $1 - f = 0.02$.\n-   Message size per face: $m(p) = h \\, n_{v} \\, b \\, \\left(\\frac{N}{q}\\right)^{2}$.\n-   Per-processor communication time per step: $T_{\\mathrm{comm}}(p) = 6 \\left[ \\alpha + \\beta \\, m(p) \\right]$.\n-   Specific number of processors for calculation: $p = 512$.\n-   Assumption: No overlap of computation and communication.\n\n### Step 2: Validate Using Extracted Givens\n-   **Scientifically Grounded:** The problem uses standard, well-established concepts from high-performance computing and performance modeling, including domain decomposition, halo exchange, Amdahl's Law, and the latency-bandwidth model for communication costs. These are fundamental to the analysis of parallel algorithms.\n-   **Well-Posed:** The problem provides all necessary parameters and equations to derive the requested formulas and compute the final numerical value. The relationship $p=q^3$ is consistent with the given value $p=512$, as $q=\\sqrt[3]{512}=8$, which is a natural number.\n-   **Objective:** The problem is stated in precise, quantitative, and objective terms.\n-   **Consistency Check:** All parameters are given with consistent units (or are dimensionless). The formulas provided for message size and communication time are standard representations for this type of problem.\n\n### Step 3: Verdict and Action\nThe problem is valid. A full solution will be provided.\n\n### Derivation of Ideal Speedup (Amdahl's Law)\nThe total time on a single processor is $T_{1}$. This time is composed of a fraction $f$ that is perfectly parallelizable and a fraction $1-f$ that is strictly serial.\n$$T_{1} = f T_{1} + (1-f) T_{1}$$\nWhen the workload is distributed across $p$ processors in a strong-scaling regime, the parallelizable portion of the work is divided among the processors, while the serial portion remains unchanged. In the ideal case (no communication overhead), the time on $p$ processors, $T_{p, \\text{ideal}}$, is:\n$$T_{p, \\text{ideal}} = \\frac{f T_{1}}{p} + (1-f) T_{1}$$\nSpeedup, $S(p)$, is defined as the ratio of the single-processor time to the multi-processor time. The ideal speedup is therefore:\n$$S_{\\text{ideal}}(p) = \\frac{T_{1}}{T_{p, \\text{ideal}}} = \\frac{T_{1}}{\\frac{f T_{1}}{p} + (1-f) T_{1}}$$\nDividing the numerator and denominator by $T_{1}$ yields the standard expression for Amdahl's Law:\n$$S_{\\text{ideal}}(p) = \\frac{1}{\\frac{f}{p} + (1-f)}$$\n\n### Communication-Aware Speedup and Effective Parallel Fraction\nThe assumption of no overlap between computation and communication means the communication time $T_{\\mathrm{comm}}(p)$ is added to the computation time. The total time on $p$ processors, $T_{p}$, is:\n$$T_{p} = T_{p, \\text{ideal}} + T_{\\mathrm{comm}}(p) = \\frac{f T_{1}}{p} + (1-f) T_{1} + T_{\\mathrm{comm}}(p)$$\nThe communication-aware speedup, $S(p)$, is:\n$$S(p) = \\frac{T_{1}}{T_{p}} = \\frac{T_{1}}{\\frac{f T_{1}}{p} + (1-f) T_{1} + T_{\\mathrm{comm}}(p)}$$\nDividing the numerator and denominator by $T_{1}$, we get the expression for $S(p)$:\n$$S(p) = \\frac{1}{\\frac{f}{p} + (1-f) + \\frac{T_{\\mathrm{comm}}(p)}{T_{1}}}$$\nThe effective parallel fraction, $f_{\\mathrm{eff}}(p)$, is defined such that the observed time $T_{p}$ can be described by a simple Amdahl's Law model using $f_{\\mathrm{eff}}(p)$:\n$$T_{p} = \\frac{f_{\\mathrm{eff}}(p) T_{1}}{p} + (1 - f_{\\mathrm{eff}}(p)) T_{1}$$\nWe equate the two expressions for $T_{p}$ and solve for $f_{\\mathrm{eff}}(p)$:\n$$\\frac{f T_{1}}{p} + (1-f) T_{1} + T_{\\mathrm{comm}}(p) = \\frac{f_{\\mathrm{eff}}(p) T_{1}}{p} + (1 - f_{\\mathrm{eff}}(p)) T_{1}$$\nDividing by $T_{1}$:\n$$\\frac{f}{p} + 1 - f + \\frac{T_{\\mathrm{comm}}(p)}{T_{1}} = \\frac{f_{\\mathrm{eff}}(p)}{p} + 1 - f_{\\mathrm{eff}}(p)$$\nRearranging to isolate terms with $f_{\\mathrm{eff}}(p)$:\n$$\\frac{f}{p} - f + \\frac{T_{\\mathrm{comm}}(p)}{T_{1}} = f_{\\mathrm{eff}}(p) \\left(\\frac{1}{p} - 1\\right) = f_{\\mathrm{eff}}(p) \\left(\\frac{1-p}{p}\\right)$$\nSolving for $f_{\\mathrm{eff}}(p)$:\n$$f_{\\mathrm{eff}}(p) = \\frac{p}{1-p} \\left( \\frac{f}{p} - f + \\frac{T_{\\mathrm{comm}}(p)}{T_{1}} \\right) = \\frac{p}{1-p} \\left( f\\left(\\frac{1-p}{p}\\right) + \\frac{T_{\\mathrm{comm}}(p)}{T_{1}} \\right)$$\n$$f_{\\mathrm{eff}}(p) = f - \\frac{p}{p-1} \\frac{T_{\\mathrm{comm}}(p)}{T_{1}}$$\n\n### Qualitative Analysis of Scaling\nTo understand how $T_{\\mathrm{comm}}(p)$ affects $f_{\\mathrm{eff}}(p)$, we first analyze the scaling of $T_{\\mathrm{comm}}(p)$. With $q=p^{1/3}$, the message size is $m(p) = h n_v b (N/p^{1/3})^2 = h n_v b N^2 p^{-2/3}$. The communication time is:\n$$T_{\\mathrm{comm}}(p) = 6\\alpha + 6\\beta h n_v b N^2 p^{-2/3}$$\nAs $p$ increases, the subdomain size decreases, leading to smaller surface areas and thus smaller messages. Consequently, the bandwidth-dependent part of $T_{\\mathrm{comm}}(p)$ decreases, and $T_{\\mathrm{comm}}(p)$ as a whole decreases, approaching a limit of $6\\alpha$.\n\nThe effective parallel fraction is $f_{\\mathrm{eff}}(p) = f - \\frac{p}{p-1} \\frac{T_{\\mathrm{comm}}(p)}{T_{1}}$. The term being subtracted, $H(p) = \\frac{p}{p-1} \\frac{T_{\\mathrm{comm}}(p)}{T_{1}}$, represents the performance penalty from communication, mapped into the Amdahl formalism. As $p$ increases, the factor $\\frac{p}{p-1}$ approaches $1$. The term $T_{\\mathrm{comm}}(p)$ decreases as $p^{-2/3}$ (ignoring the constant latency part for scaling behavior). The penalty term $H(p)$ is a product of two decreasing functions of $p$ (for $p>1$), and can be shown to be a decreasing function itself. Therefore, as $p$ increases, the penalty $H(p)$ decreases, and $f_{\\mathrm{eff}}(p)$ increases towards a limit of $f - 6\\alpha/T_{1}$.\n\nThis may seem counter-intuitive, as communication overhead is known to reduce parallel efficiency, especially at high processor counts. The resolution lies in recognizing that $f_{\\mathrm{eff}}(p)$ is an algebraic construct to fit a complex performance model into a simpler one. The increase in $f_{\\mathrm{eff}}(p)$ does not imply better overall efficiency. The parallel efficiency $E(p) = S(p)/p$ is given by $E(p) = T_{1}/(pT_{p}) = T_{1}/(fT_{1} + p(1-f)T_{1} + pT_{\\mathrm{comm}}(p))$. The term $pT_{\\mathrm{comm}}(p) = 6\\alpha p + 6\\beta h n_v b N^2 p^{1/3}$ grows with $p$. This causes the denominator of $E(p)$ to grow faster than linearly in $p$, leading to a sharp decline in efficiency, which is the expected behavior. The communication overhead, which scales geometrically as a surface-to-volume ratio ($p^{1/3}$), increasingly dominates the scaling behavior and degrades performance.\n\n### Calculation of Speedup for $p=512$\nWe are given the parameters: $N = 2048$, $p = 512$, $h = 4$, $n_{v} = 50$, $b = 8$, $\\alpha = 2 \\times 10^{-5}$ s, $\\beta = 5 \\times 10^{-11}$ s/byte, $T_{1} = 10$ s, and $f = 0.98$.\n\n1.  Calculate $q$:\n    $$q = p^{1/3} = (512)^{1/3} = 8$$\n\n2.  Calculate the message size $m(p)$:\n    $$m(512) = h \\, n_{v} \\, b \\, \\left(\\frac{N}{q}\\right)^{2} = 4 \\times 50 \\times 8 \\times \\left(\\frac{2048}{8}\\right)^{2}$$\n    $$m(512) = 1600 \\times (256)^{2} = 1600 \\times 65536 = 104,857,600 \\, \\text{bytes}$$\n\n3.  Calculate the communication time $T_{\\mathrm{comm}}(p)$:\n    $$T_{\\mathrm{comm}}(512) = 6 \\left[ \\alpha + \\beta \\, m(512) \\right]$$\n    $$T_{\\mathrm{comm}}(512) = 6 \\left[ 2 \\times 10^{-5} + (5 \\times 10^{-11}) \\times 104,857,600 \\right]$$\n    $$T_{\\mathrm{comm}}(512) = 6 \\left[ 2 \\times 10^{-5} + 0.00524288 \\right]$$\n    $$T_{\\mathrm{comm}}(512) = 6 \\left[ 0.00526288 \\right] = 0.03157728 \\, \\text{s}$$\n\n4.  Calculate the total time on $p=512$ processors, $T_{512}$:\n    $$T_{512} = \\frac{f T_{1}}{p} + (1-f) T_{1} + T_{\\mathrm{comm}}(512)$$\n    $$T_{512} = \\frac{0.98 \\times 10}{512} + (1-0.98) \\times 10 + 0.03157728$$\n    $$T_{512} = \\frac{9.8}{512} + 0.2 + 0.03157728$$\n    $$T_{512} = 0.019140625 + 0.2 + 0.03157728 = 0.250717905 \\, \\text{s}$$\n\n5.  Calculate the communication-aware speedup $S(p)$:\n    $$S(512) = \\frac{T_{1}}{T_{512}} = \\frac{10}{0.250717905} \\approx 39.88551$$\n\nRounding to four significant figures, the speedup is $39.89$.",
            "answer": "$$\n\\boxed{39.89}\n$$"
        },
        {
            "introduction": "While theoretical models often assume uniform computational cost, real-world models exhibit significant load imbalance due to factors like complex orography or spatially varying physical parameterizations. This exercise moves from theory to practice, presenting you with a realistic scenario of heterogeneous per-processor timings. You will learn to quantify the direct performance penalty of this imbalance by calculating the fraction of processor time wasted at synchronization barriers, providing a concrete measure of parallel inefficiency. ",
            "id": "4032234",
            "problem": "A hydrostatic primitive-equation core for Numerical Weather Prediction (NWP) is executed in a Single Program, Multiple Data (SPMD) model over Message Passing Interface (MPI) ranks. The horizontal domain is decomposed into six subdomains whose per-timestep wall-clock times reflect realistic heterogeneity due to polar filtering and orography-triggered physics. Denote by $t_i$ the wall-clock time per timestep of rank $i$, and let the timestep end with a halo exchange that imposes barrier synchronization semantics. Measured per-timestep times are\n$$t_1 = 17.8~\\mathrm{ms},\\quad t_2 = 18.2~\\mathrm{ms},\\quad t_3 = 19.5~\\mathrm{ms},\\quad t_4 = 18.0~\\mathrm{ms},\\quad t_5 = 23.7~\\mathrm{ms},\\quad t_6 = 18.9~\\mathrm{ms}.$$\nStarting from first principles of barrier synchronization and parallel execution time in SPMD, define a load-imbalance factor that compares the slowest rank to the average rank. Then, derive a closed-form analytic expression relating this imbalance to the fraction of processor cycles that are wasted at the synchronization barrier each timestep. Using the given data, compute that fraction.\n\nExpress the final fraction as a decimal with no units and round your answer to four significant figures.",
            "solution": "The problem is valid as it is scientifically grounded in the principles of parallel computing, well-posed with sufficient data, and objectively stated.\n\nThe problem asks for a derivation of the fraction of wasted processor cycles due to load imbalance in a Single Program, Multiple Data (SPMD) parallel execution model with barrier synchronization. We are given the per-timestep computational wall-clock times for $N=6$ Message Passing Interface (MPI) ranks.\n\nLet $N$ be the number of MPI ranks. In this problem, $N=6$.\nLet $t_i$ be the wall-clock time required for rank $i$ to complete its computational tasks within a single timestep, for $i \\in \\{1, 2, \\dots, N\\}$.\n\nThe execution model employs barrier synchronization at the end of each timestep. This means that all ranks must wait at the barrier until the last (slowest) rank has finished its computation. Therefore, the total wall-clock time for one complete timestep, denoted $T_{\\text{step}}$, is determined by the maximum of the individual rank times:\n$$T_{\\text{step}} = \\max_{i=1, \\dots, N} \\{t_i\\}$$\n\nDuring this time $T_{\\text{step}}$, a total of $N$ processors are allocated to the task. The total available processor time (or total processor cycles allocated) for one timestep is the sum of the time each processor is reserved, which is $N$ multiplied by $T_{\\text{step}}$.\n$$T_{\\text{total\\_alloc}} = N \\cdot T_{\\text{step}} = N \\cdot \\max_{i=1, \\dots, N} \\{t_i\\}$$\n\nThe amount of time spent performing useful computations across all ranks is the sum of the individual computational times:\n$$T_{\\text{useful}} = \\sum_{i=1}^{N} t_i$$\n\nThe time that is wasted is spent by the faster ranks idling at the synchronization barrier. This wasted time is the difference between the total allocated processor time and the useful computational time:\n$$T_{\\text{wasted}} = T_{\\text{total\\_alloc}} - T_{\\text{useful}} = N \\cdot \\max_{i=1, \\dots, N} \\{t_i\\} - \\sum_{i=1}^{N} t_i$$\n\nThe fraction of processor cycles that are wasted, denoted $f_{\\text{waste}}$, is the ratio of the wasted time to the total allocated time:\n$$f_{\\text{waste}} = \\frac{T_{\\text{wasted}}}{T_{\\text{total\\_alloc}}} = \\frac{N \\cdot \\max_{i=1, \\dots, N} \\{t_i\\} - \\sum_{i=1}^{N} t_i}{N \\cdot \\max_{i=1, \\dots, N} \\{t_i\\}} = 1 - \\frac{\\sum_{i=1}^{N} t_i}{N \\cdot \\max_{i=1, \\dots, N} \\{t_i\\}}$$\n\nThe problem asks to first define a load-imbalance factor, $\\mathcal{L}$, that compares the slowest rank to the average rank. Let $t_{\\text{slowest}}$ be the time of the slowest rank and $t_{\\text{avg}}$ be the average time across all ranks.\n$$t_{\\text{slowest}} = \\max_{i=1, \\dots, N} \\{t_i\\}$$\n$$t_{\\text{avg}} = \\frac{1}{N} \\sum_{i=1}^{N} t_i$$\nA natural definition for the load-imbalance factor is the ratio of these two quantities:\n$$\\mathcal{L} = \\frac{t_{\\text{slowest}}}{t_{\\text{avg}}} = \\frac{\\max_{i=1, \\dots, N} \\{t_i\\}}{\\frac{1}{N} \\sum_{i=1}^{N} t_i}$$\n\nNow, we derive the relationship between $f_{\\text{waste}}$ and $\\mathcal{L}$. From the definition of $t_{\\text{avg}}$, we have $\\sum_{i=1}^{N} t_i = N \\cdot t_{\\text{avg}}$. Substituting this into the expression for $f_{\\text{waste}}$:\n$$f_{\\text{waste}} = 1 - \\frac{N \\cdot t_{\\text{avg}}}{N \\cdot \\max_{i=1, \\dots, N} \\{t_i\\}} = 1 - \\frac{t_{\\text{avg}}}{\\max_{i=1, \\dots, N} \\{t_i\\}}$$\nRecognizing that the term $\\frac{t_{\\text{avg}}}{\\max_{i=1, \\dots, N} \\{t_i\\}}$ is the reciprocal of the load-imbalance factor $\\mathcal{L}$, we arrive at the closed-form analytic expression:\n$$f_{\\text{waste}} = 1 - \\frac{1}{\\mathcal{L}}$$\n\nFinally, we compute the numerical value of $f_{\\text{waste}}$ using the provided data:\n$$t_1 = 17.8~\\mathrm{ms}, \\quad t_2 = 18.2~\\mathrm{ms}, \\quad t_3 = 19.5~\\mathrm{ms}, \\quad t_4 = 18.0~\\mathrm{ms}, \\quad t_5 = 23.7~\\mathrm{ms}, \\quad t_6 = 18.9~\\mathrm{ms}$$\nThe number of ranks is $N=6$.\n\nFirst, find the time of the slowest rank:\n$$t_{\\text{slowest}} = \\max\\{17.8, 18.2, 19.5, 18.0, 23.7, 18.9\\} = 23.7~\\mathrm{ms}$$\n\nNext, compute the sum of the times:\n$$\\sum_{i=1}^{6} t_i = 17.8 + 18.2 + 19.5 + 18.0 + 23.7 + 18.9 = 116.1~\\mathrm{ms}$$\n\nNow, we can compute the fraction of wasted cycles directly. The units of milliseconds will cancel.\n$$f_{\\text{waste}} = 1 - \\frac{\\sum_{i=1}^{6} t_i}{6 \\cdot t_{\\text{slowest}}} = 1 - \\frac{116.1}{6 \\cdot 23.7}$$\n$$f_{\\text{waste}} = 1 - \\frac{116.1}{142.2}$$\n$$f_{\\text{waste}} \\approx 1 - 0.816455696...$$\n$$f_{\\text{waste}} \\approx 0.183544303...$$\nRounding the result to four significant figures gives $0.1835$.",
            "answer": "$$\\boxed{0.1835}$$"
        }
    ]
}