{
    "hands_on_practices": [
        {
            "introduction": "In shared-memory programming, one of the most fundamental and dangerous pitfalls is the data race. This exercise  provides a hands-on exploration of this concept within the familiar context of updating a prognostic field from multiple, concurrent contributions. By implementing and comparing a naive (racy) scatter-add with correct atomic and reduction-based approaches, you will gain a concrete understanding of how race conditions lead to incorrect results and master the essential techniques to ensure correctness in your parallel code.",
            "id": "4074054",
            "problem": "Consider a shared-memory parallel update of a one-dimensional prognostic field in numerical weather prediction. Let $q_i^n$ denote the cell-mean value of a prognostic variable at discrete time level $n$ for cell index $i \\in \\{0,1,\\dots,N-1\\}$. Suppose there are $M$ independent contributions (for example, flux divergences computed on faces, subgrid tendencies, or physics parameterization increments) that must be accumulated into the field to form $q^{n+1}$. Each contribution $k \\in \\{0,1,\\dots,M-1\\}$ is characterized by a destination cell index $d_k \\in \\{0,1,\\dots,N-1\\}$ and an increment value $\\Delta_k \\in \\mathbb{R}$, and the serial mathematical specification of the update is\n$$\nq_i^{n+1} \\;=\\; q_i^n \\;+\\; \\sum_{k:\\, d_k = i} \\Delta_k \\quad \\text{for all } i \\in \\{0,1,\\dots,N-1\\}.\n$$\nDefinition task. Precisely define a data race for this setting using standard shared-memory concurrency terminology. Start from the fundamental definition: in a shared-memory program with a happens-before relation, a data race occurs when there exist two or more dynamic operations that access the same memory location, at least one is a write, and the accesses are not ordered by the happens-before relation. Then specialize this definition to the read-modify-write accumulations required to realize $q_i^{n+1}$ from $q_i^n$ and $\\{(d_k,\\Delta_k)\\}_{k=0}^{M-1}$.\n\nConstruction task. Construct a minimal parallelization thought-experiment that exhibits a race in this accumulation: model the parallel update of $q[d_k] \\mathrel{+}= \\Delta_k$ for many $k$ as concurrent read-modify-write operations on the same memory location $q[i]$. Explain why, in the absence of synchronization, concurrent operations $q[i] \\leftarrow q[i] + \\Delta_k$ and $q[i] \\leftarrow q[i] + \\Delta_{k'}$ with $d_k = d_{k'} = i$ can lose increments due to write-write conflicts or lost updates. Then show two resolution strategies grounded in first principles:\n- Atomic scatter-add semantics: make each update $q[d_k] \\mathrel{+}= \\Delta_k$ an atomic read-modify-write so that every increment is applied exactly once.\n- Reduction by grouping: compute, for each $i$, the grouped sum $S_i = \\sum_{k:\\, d_k = i} \\Delta_k$ and then perform the single write $q[i] \\leftarrow q[i] + S_i$, which is equivalent to a deterministic reduction.\n\nProgramming task. Write a complete, runnable program that, for a given collection of test cases, computes three arrays for each case:\n- A naive vectorized scatter-add that models racy, non-atomic overlapping writes by performing $q[d_k] \\mathrel{+}= \\Delta_k$ with advanced indexing semantics that do not resolve overlaps deterministically as a sum.\n- An atomic-like scatter-add that guarantees that repeated indices $d_k$ lead to true accumulation, modeling correct atomic additions.\n- A grouped reduction that computes $S_i$ for all $i$ and then updates $q[i]$ exactly once.\n\nFor each test case, compute two scalar diagnostics: the maximum absolute difference between the naive result and the atomic-like result, and the maximum absolute difference between the grouped-reduction result and the atomic-like result. These diagnostics must be floating-point numbers.\n\nTest suite. Use the following five cases; in each, assume $q^n$ is identically zero to isolate accumulation effects, and all arrays are one-dimensional:\n- Case $1$ (no collisions, happy path): $N = 5$, $M = 4$, $d = [0,1,2,3]$, $\\Delta = [1.0,1.0,1.0,1.0]$.\n- Case $2$ (complete collision on one cell): $N = 3$, $M = 3$, $d = [1,1,1]$, $\\Delta = [0.1,0.2,0.3]$.\n- Case $3$ (boundary indices and canceling increments): $N = 4$, $M = 3$, $d = [3,3,0]$, $\\Delta = [-2.0,2.0,5.0]$.\n- Case $4$ (empty contributions edge case): $N = 10$, $M = 0$, $d = []$, $\\Delta = []$.\n- Case $5$ (randomized stress test with fixed seed): $N = 100$, $M = 1000$, draw $d_k$ independently and uniformly from $\\{0,1,\\dots,99\\}$ and $\\Delta_k$ independently from a standard normal distribution $\\mathcal{N}(0,1)$ using a fixed pseudorandom seed so the result is deterministic.\n\nFinal output format. Your program should produce a single line of output containing the results as a JSON-like list of length $5$, where the $j$-th element is a two-element list $[x_j,y_j]$. Here $x_j$ is the maximum absolute difference between the naive result and the atomic-like result for case $j$, and $y_j$ is the maximum absolute difference between the grouped-reduction result and the atomic-like result for case $j$. The line must contain no extra commentary, for example:\n```\n[[0.0,0.0],[0.3,0.0],[2.0,0.0],[0.0,0.0],[...,0.0]]\n```\nUnits. There are no physical units or angles in this problem; all outputs are real numbers. Express all numbers as plain decimals. The grouped-reduction and atomic-like results should agree to within round-off so $y_j$ is expected to be exactly $0.0$ in these cases, while $x_j$ will be $0.0$ only when there are no index collisions. The program must not require any input. It must be self-contained and deterministic.",
            "solution": "The problem posed is valid. It is scientifically grounded in the principles of parallel computing, well-posed with a clear objective and sufficient data, objective in its language, and internally consistent. It addresses a fundamental and non-trivial challenge in implementing numerical algorithms on shared-memory architectures. We may therefore proceed with a solution.\n\nThe problem consists of three tasks: defining a data race in the context of a parallel scatter-add operation, constructing a thought experiment to illustrate the race and its resolution, and implementing a program to demonstrate the different numerical outcomes of racy and correct implementations.\n\n**Definition Task: Data Race in Parallel Accumulation**\n\nThe fundamental definition of a data race is as follows: in a shared-memory program with a defined happens-before relation, a data race occurs when there exist two or more concurrent operations that access the same memory location, where at least one of the operations is a write, and the accesses are not ordered by the happens-before relation.\n\nWe specialize this definition for the numerical update problem. The serial mathematical specification is given by\n$$\nq_i^{n+1} \\;=\\; q_i^n \\;+\\; \\sum_{k:\\, d_k = i} \\Delta_k \\quad \\text{for all } i \\in \\{0,1,\\dots,N-1\\}.\n$$\nA parallel implementation might assign different threads to compute and apply the increments $\\{\\Delta_k\\}_{k=0}^{M-1}$. A common parallel strategy is to have each thread iterate over a subset of the $M$ contributions. For each contribution $k$ it is assigned, a thread performs the update $q_{d_k} \\mathrel{+}= \\Delta_k$.\n\nThe shared memory locations are the elements of the array storing the prognostic field, which we denote as $q$. Specifically, the memory accessed is the location corresponding to $q_{i}$ for $i \\in \\{0, \\dots, N-1\\}$. The operation $q_{d_k} \\mathrel{+}= \\Delta_k$ is not a single, instantaneous machine instruction. It is a composite **read-modify-write (RMW)** sequence:\n1.  **Read**: A temporary register is loaded with the current value of $q_{d_k}$. Let this be $v_{\\text{old}} \\leftarrow q_{d_k}$.\n2.  **Modify**: The increment is added to the value in the register. Let this be $v_{\\text{new}} \\leftarrow v_{\\text{old}} + \\Delta_k$.\n3.  **Write**: The new value from the register is written back to the memory location of $q_{d_k}$. Let this be $q_{d_k} \\leftarrow v_{\\text{new}}$.\n\nNow, consider two concurrent threads, Thread A and Thread B, assigned contributions $k_A$ and $k_B$ respectively, such that the destination indices are the same: $d_{k_A} = d_{k_B} = i$.\n- Thread A intends to perform the RMW sequence for $\\Delta_{k_A}$ on memory location $q_i$.\n- Thread B intends to perform the RMW sequence for $\\Delta_{k_B}$ on memory location $q_i$.\n\nSince both threads access the same memory location $q_i$, and both operations involve a write, a data race occurs if the execution of Thread A's operations and Thread B's operations are not ordered by a happens-before relation. In a typical unsynchronized parallel execution, no such ordering is guaranteed. The individual read, modify, and write steps of the two threads can be interleaved in an unpredictable manner, leading to incorrect results. This formalizes the data race for this specific accumulation problem.\n\n**Construction Task: Race Condition and Resolution Strategies**\n\nWe construct a minimal thought experiment. Let the initial value at a cell be $q_i^n$. Two concurrent threads, A and B, are tasked with adding increments $\\Delta_A$ and $\\Delta_B$ to this cell, respectively. The correct final value should be $q_i^{n+1} = q_i^n + \\Delta_A + \\Delta_B$. In the absence of synchronization, the following interleaving of the RMW sequences can occur:\n\n1.  Thread A reads $q_i^n$ into its private register. (Register A contains $q_i^n$)\n2.  The scheduler preempts Thread A and runs Thread B.\n3.  Thread B reads $q_i^n$ into its private register. (Register B contains $q_i^n$)\n4.  Thread B computes its update: $q_i^n + \\Delta_B$. It writes this result back to memory. Memory $q_i$ now holds $q_i^n + \\Delta_B$.\n5.  The scheduler resumes Thread A.\n6.  Thread A, unaware of Thread B's activity, computes its update based on the *stale value* it read in step $1$: $q_i^n + \\Delta_A$. It writes this result back to memory. Memory $q_i$ now holds $q_i^n + \\Delta_A$.\n\nThe final result is $q_i^n + \\Delta_A$, meaning the increment $\\Delta_B$ from Thread B has been lost. This phenomenon is known as a **lost update**, a direct consequence of the data race. The write from Thread A overwrites the write from Thread B. The final state depends on the arbitrary timing of thread execution, a hallmark of a race condition.\n\nTwo principal strategies can resolve this race condition:\n\n1.  **Atomic Scatter-Add:** This strategy makes the entire read-modify-write sequence an **atomic operation**. An atomic operation is guaranteed by the hardware and/or operating system to execute indivisibly, without interruption from other threads accessing the same memory location. When Thread A begins its atomic update on $q_i$, the memory location is effectively \"locked,\" preventing Thread B from reading it until Thread A's write is complete. This enforces a serial ordering on the conflicting accesses. If Thread A executes its atomic `+=` first, Thread B will then read the updated value $q_i^n + \\Delta_A$ and correctly compute the final result $(q_i^n + \\Delta_A) + \\Delta_B$. Every increment $\\Delta_k$ is guaranteed to be applied exactly once, realizing the mathematically correct sum. This is often called a \"scatter-add\" since increments are scattered to their destinations and added.\n\n2.  **Reduction by Grouping:** This strategy reframes the problem to avoid multiple threads writing to the same destination cell. It is a two-phase process:\n    -   **Phase 1 (Reduction):** First, the increments are grouped by their destination index $i$. For each $i \\in \\{0, \\dots, N-1\\}$, a partial sum $S_i = \\sum_{k: d_k = i} \\Delta_k$ is computed. This reduction can itself be parallelized. For instance, a temporary array for the sums $S$ can be created, and contributions can be accumulated into it using atomic operations. Alternatively, the set of increments can be partitioned among threads, where each thread computes sums for its partition into a private temporary array, followed by a final reduction of the private arrays into the global $S$ array.\n    -   **Phase 2 (Update):** Once the array of sums $S$ is complete, the final field $q^{n+1}$ is computed by performing the update $q_i^{n+1} = q_i^n + S_i$ for all $i$. This second phase is embarrassingly parallel, as each update is independent (one write per destination $i$), so no race conditions can occur on the $q$ array. This approach is deterministic and mathematically equivalent to the serial specification and the atomic scatter-add.\n\nThe programming task below will implement a naive, racy approach, the atomic-like scatter-add, and the grouped reduction to demonstrate these concepts numerically.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport json\n\ndef solve():\n    \"\"\"\n    Solves the scatter-add problem for a suite of test cases, comparing\n    naive, atomic-like, and grouped reduction methods.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases_spec = [\n        # Case 1: No collisions\n        {'N': 5, 'M': 4, 'd': [0, 1, 2, 3], 'delta': [1.0, 1.0, 1.0, 1.0]},\n        # Case 2: Complete collision on one cell\n        {'N': 3, 'M': 3, 'd': [1, 1, 1], 'delta': [0.1, 0.2, 0.3]},\n        # Case 3: Boundary indices and canceling increments\n        {'N': 4, 'M': 3, 'd': [3, 3, 0], 'delta': [-2.0, 2.0, 5.0]},\n        # Case 4: Empty contributions\n        {'N': 10, 'M': 0, 'd': [], 'delta': []},\n        # Case 5: Randomized stress test\n        {'N': 100, 'M': 1000, 'random_seed': 42}\n    ]\n\n    results = []\n\n    for case in test_cases_spec:\n        N = case['N']\n        M = case['M']\n\n        if 'random_seed' in case:\n            # Generate random data for the stress test case\n            rng = np.random.default_rng(seed=case['random_seed'])\n            d = rng.integers(0, N, size=M)\n            delta = rng.normal(size=M)\n        else:\n            # Use predefined data\n            d = np.array(case['d'], dtype=int)\n            delta = np.array(case['delta'], dtype=float)\n\n        # Initial field q^n is all zeros\n        q_n = np.zeros(N, dtype=float)\n\n        # 1. Naive scatter-add (racy behavior)\n        # In NumPy, a[indices] += b is a \"buffered\" operation. If indices\n        # contains duplicates, the operation is performed for each element, and\n        # for a given index, the result of the last corresponding operation\n        # is written. This models a \"lost update\" race condition.\n        q_naive = q_n.copy()\n        if M > 0:\n            q_naive[d] += delta\n\n        # 2. Atomic-like scatter-add (correct accumulation)\n        # numpy.add.at performs an unbuffered in-place operation, which\n        # correctly accumulates values for repeated indices. This models\n        # an atomic scatter-add.\n        q_atomic = q_n.copy()\n        if M > 0:\n            np.add.at(q_atomic, d, delta)\n        \n        # 3. Grouped reduction (correct accumulation via pre-computation)\n        # This method first computes the total sum S_i for each cell i and\n        # then performs a single update per cell.\n        q_grouped = q_n.copy()\n        if M > 0:\n            # Phase 1: Compute the reduction S_i = sum_{k: d_k=i} Delta_k\n            S = np.zeros(N, dtype=float)\n            np.add.at(S, d, delta)\n            # Phase 2: Apply the single, non-conflicting update\n            q_grouped += S\n        \n        # Compute the required diagnostics\n        # max_abs_diff(naive, atomic)\n        diff_naive_atomic = np.max(np.abs(q_naive - q_atomic)) if N > 0 else 0.0\n        \n        # max_abs_diff(grouped, atomic)\n        # This should be zero up to floating point precision.\n        diff_grouped_atomic = np.max(np.abs(q_grouped - q_atomic)) if N > 0 else 0.0\n\n        results.append([float(diff_naive_atomic), float(diff_grouped_atomic)])\n\n    # Final print statement in the exact required format.\n    # The use of json.dumps ensures correct formatting without extra spaces.\n    print(json.dumps(results).replace(\" \", \"\"))\n\nsolve()\n```"
        },
        {
            "introduction": "Efficient data communication is the cornerstone of high-performance distributed-memory models, and this practice  tackles a critical aspect of this: exchanging ghost zone data that is not contiguous in memory. You will design and verify a Message Passing Interface (MPI) derived datatype that describes the strided memory layout of vertical columns in a numerical model. This exercise demonstrates how to eliminate the need for manual data packing and unpacking, a common performance bottleneck, resulting in cleaner and more efficient code.",
            "id": "4074064",
            "problem": "Consider an atmospheric state variable stored on a three-dimensional grid with indices $(i,j,k)$, where $i$ indexes the zonal direction, $j$ indexes the meridional direction, and $k$ indexes the vertical levels. Assume a row-major (C-style) memory layout in which the fastest-varying index is $k$, then $j$, then $i$. Let the grid dimensions be $N_i$, $N_j$, and $N_k$, respectively, and let each element be an $8$-byte IEEE $754$ double-precision number. For any $(i,j,k)$, the linear memory offset (in units of elements) is defined by the widely used mapping\n$$\n\\mathrm{offset}(i,j,k) = i\\cdot(N_j N_k) + j\\cdot N_k + k.\n$$\nIn a domain decomposition where processes are partitioned along the $j$-dimension, each process must exchange ghost zones in $j$ with its neighbors. A ghost zone of width $G$ consists of all vertical columns across $k$ at the boundary faces in $j$. For a given face (either low $j$ or high $j$), the data to send is the set of all $(i,j,k)$ for $i\\in\\{0,\\dots,N_i-1\\}$, $j\\in J_\\mathrm{face}$ (where $J_\\mathrm{face}$ is either $\\{0,\\dots,G-1\\}$ for the low face or $\\{N_j-G,\\dots,N_j-1\\}$ for the high face), and $k\\in\\{0,\\dots,N_k-1\\}$.\n\nYou must design a Message Passing Interface (MPI) derived datatype that describes this send pattern without application-level packing, using only the fundamental base described above. Specifically, construct a vector type whose parameters are integer counts in units of the old elementary datatype (double precision elements). Then, prove from the memory mapping that this derived datatype is packing-free for sending vertical columns across $k$, and quantify the bytes transferred per single vertical column as a function of $N_k$.\n\nMathematically and algorithmically, your tasks are:\n- Derive the correct vector parameters $(\\mathrm{count}, \\mathrm{blocklength}, \\mathrm{stride})$ from the mapping and the ghost width $G$ for both low-$j$ and high-$j$ faces, ensuring the description applies generally to any $N_i$, $N_j$, $N_k$, and $G$ with $G \\le N_j$.\n- Prove that this derived datatype captures contiguous runs along $k$ inside each block and uses a fixed stride to step across $i$, eliminating the need to pack into an intermediate buffer, according to the mapping.\n- Compute the bytes per column and express this in bytes, as an integer.\n\nYour program must implement the following, entirely in pure computation without using MPI libraries:\n- Given $(N_i,N_j,N_k,G,\\mathrm{side})$, where $\\mathrm{side} \\in \\{\\text{\"low\"}, \\text{\"high\"}\\}$ indicates the face, compute:\n  1. The bytes per vertical column $B$ in bytes as an integer.\n  2. The MPI vector parameters $(\\mathrm{count}, \\mathrm{blocklength}, \\mathrm{stride})$ that would describe the send buffer for the ghost zone on that face, measured in elements.\n  3. A boolean asserting packing-free correctness by enumerating linear offsets predicted by the mapping for the ghost zone and comparing them to the offsets generated by the vector description; these must match exactly for the datatype to be considered packing-free in the sense of not requiring application-level packing.\n- For angles, none are used; no angle units are needed.\n- Express bytes strictly in bytes. The final answers are integers, and the boolean is either true or false.\n\nUse the following test suite to ensure coverage:\n- Case A (general case): $N_i=4$, $N_j=5$, $N_k=10$, $G=1$, $\\mathrm{side}=\\text{\"low\"}$.\n- Case B (wider ghost, high face): $N_i=1$, $N_j=8$, $N_k=16$, $G=2$, $\\mathrm{side}=\\text{\"high\"}$.\n- Case C (degenerate $j$): $N_i=3$, $N_j=1$, $N_k=7$, $G=1$, $\\mathrm{side}=\\text{\"low\"}$.\n- Case D (degenerate $k$): $N_i=2$, $N_j=3$, $N_k=1$, $G=1$, $\\mathrm{side}=\\text{\"high\"}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each entry corresponds to a test case and is itself a list in the format $[B,\\mathrm{count},\\mathrm{blocklength},\\mathrm{stride},\\mathrm{packing\\_free}]$. For example, the output format is $[[b_1,c_1,bl_1,s_1,p_1],[b_2,c_2,bl_2,s_2,p_2],...]$ without spaces, where $b_i$ is an integer (bytes), $c_i$, $bl_i$, $s_i$ are integers (elements), and $p_i$ is a boolean.",
            "solution": "The problem requires the design and verification of a Message Passing Interface (MPI) derived datatype to describe a ghost zone exchange in a three-dimensional grid, without resorting to manual data packing. The solution must be derived from first principles based on the provided memory layout.\n\nFirst, we analyze the memory mapping and the structure of the data to be exchanged. The three-dimensional grid has dimensions $N_i, N_j, N_k$, and the memory offset for an element at index $(i,j,k)$ is given by the row-major formula:\n$$\n\\mathrm{offset}(i,j,k) = i \\cdot (N_j N_k) + j \\cdot N_k + k\n$$\nThis formula indicates that the index $k$ varies the fastest, meaning elements along the $k$-axis (a vertical column) for a fixed $(i,j)$ are contiguous in memory. A block of $N_k$ such elements is followed by the block for the next $j$ index, and a block of $N_j N_k$ elements (a full $j-k$ plane) is followed by the block for the next $i$ index.\n\nThe data to be sent for a ghost zone of width $G$ consists of all points $(i,j,k)$ for $i \\in \\{0, \\dots, N_i-1\\}$, $k \\in \\{0, \\dots, N_k-1\\}$, and $j$ within a specific range $J_{\\mathrm{face}}$ near the boundary of a domain partition along the $j$-axis. For a \"low\" face, $J_{\\mathrm{face}} = \\{0, \\dots, G-1\\}$; for a \"high\" face, $J_{\\mathrm{face}} = \\{N_j-G, \\dots, N_j-1\\}$.\n\nWe will use an MPI vector datatype, `MPI_Type_vector`, which is defined by three integer parameters:\n1.  $\\mathrm{count}$: The number of blocks in the datatype.\n2.  $\\mathrm{blocklength}$: The number of elements in each block.\n3.  $\\mathrm{stride}$: The displacement in memory (in units of elements) from the start of one block to the start of the next.\n\nOur goal is to find these parameters to describe the ghost zone data pattern. Let's analyze the structure of this data in memory.\nConsider the data for a fixed zonal index $i$. The indices to be sent are $(i,j,k)$ for all $j \\in J_{\\mathrm{face}}$ and all $k \\in \\{0, \\dots, N_k-1\\}$. Let's examine their offsets.\n$$\n\\mathrm{offset}(i,j,k) = i \\cdot N_j N_k + (j \\cdot N_k + k)\n$$\nFor a fixed $i$, the term $i \\cdot N_j N_k$ is a constant base offset. The memory layout of the ghost data within this $i$-slice is determined by the term $j \\cdot N_k + k$.\n\nFor the low-$j$ face, $j \\in \\{0, \\dots, G-1\\}$. The term $j \\cdot N_k + k$ generates offsets from $0 \\cdot N_k + 0 = 0$ up to $(G-1) \\cdot N_k + (N_k-1) = G \\cdot N_k - 1$. This corresponds to a single contiguous block of $G \\cdot N_k$ elements. The starting absolute offset for this block is $\\mathrm{offset}(i,0,0) = i \\cdot N_j N_k$.\n\nFor the high-$j$ face, $j \\in \\{N_j-G, \\dots, N_j-1\\}$. Let's define a relative index $j' = j - (N_j-G)$, so $j' \\in \\{0, \\dots, G-1\\}$. The offset term becomes $(j' + N_j - G) \\cdot N_k + k$. The starting element corresponds to $j=N_j-G, k=0$, which gives a relative offset of $(N_j-G)N_k$. The final element corresponds to $j=N_j-1, k=N_k-1$, giving a relative offset of $(N_j-1)N_k + (N_k-1) = N_j N_k - 1$. The elements for a fixed $i$ again form a single contiguous block of memory, spanning from absolute offset $\\mathrm{offset}(i, N_j-G, 0)$ to $\\mathrm{offset}(i, N_j-1, N_k-1)$. The size of this block is again $G \\cdot N_k$ elements.\n\nIn both cases, for any fixed $i$, the ghost zone data forms a single contiguous block of $G \\cdot N_k$ elements. This immediately gives us the `blocklength` parameter for our vector type:\n$$\n\\mathrm{blocklength} = G \\cdot N_k\n$$\n\nNext, we must consider how these blocks for different $i$ are arranged in memory. The data is required for all $i$ from $0$ to $N_i-1$. This implies there are $N_i$ such blocks. This gives the `count` parameter:\n$$\n\\mathrm{count} = N_i\n$$\n\nFinally, we need the `stride`: the displacement between the start of a block for index $i$ and the start of the block for index $i+1$.\nFor the low face, the starting offset of the block for index $i$ is $\\mathrm{offset}(i,0,0) = i \\cdot N_j N_k$. The starting offset for the block for index $i+1$ is $\\mathrm{offset}(i+1,0,0) = (i+1) \\cdot N_j N_k$. The stride is the difference:\n$$\n\\mathrm{stride}_{\\mathrm{low}} = \\mathrm{offset}(i+1,0,0) - \\mathrm{offset}(i,0,0) = ((i+1) - i) \\cdot N_j N_k = N_j N_k\n$$\nFor the high face, the starting offset of the block for index $i$ is $\\mathrm{offset}(i,N_j-G,0) = i \\cdot N_j N_k + (N_j-G)N_k$. The starting offset for index $i+1$ is $\\mathrm{offset}(i+1,N_j-G,0) = (i+1) \\cdot N_j N_k + (N_j-G)N_k$. The stride is again the difference:\n$$\n\\mathrm{stride}_{\\mathrm{high}} = \\mathrm{offset}(i+1,N_j-G,0) - \\mathrm{offset}(i,N_j-G,0) = N_j N_k\n$$\nThe stride is identical for both faces and is independent of $G$. Thus, the `stride` parameter is:\n$$\n\\mathrm{stride} = N_j N_k\n$$\n\nCombining these results, the MPI vector datatype that describes the ghost zone data is defined by the parameters $(\\mathrm{count}, \\mathrm{blocklength}, \\mathrm{stride}) = (N_i, G \\cdot N_k, N_j \\cdot N_k)$. The starting memory address for the MPI communication call would be the address of element $(0,0,0)$ for the low face, and the address of element $(0, N_j-G, 0)$ for the high face.\n\nThe proof that this datatype description is packing-free follows directly. We have shown that the ghost zone data is structured as $N_i$ segments, one for each $i$-plane. Each segment is a contiguous memory block of size $G \\cdot N_k$ elements. These segments are separated by a constant memory stride of $N_j N_k$ elements. This is precisely the memory access pattern described by an `MPI_Type_vector` with the parameters we derived. The MPI library can therefore directly read from (for a send) or write to (for a receive) these disjoint memory locations without an intermediate application-level buffer where the data is manually gathered or scattered. This fulfills the definition of a packing-free data transfer.\n\nLastly, we must compute the bytes transferred per single vertical column. A vertical column is defined by fixed $(i,j)$ with $k$ varying over its full range. Thus, a vertical column contains $N_k$ elements. Since each element is an $8$-byte double-precision number, the total bytes $B$ per column is:\n$$\nB = 8 \\cdot N_k\n$$\n\nThe provided test cases will be evaluated using these derived formulas. The correctness of the vector parameters will be algorithmically verified by generating the list of memory offsets described by the datatype and comparing it to the list of offsets generated by explicitly enumerating all $(i,j,k)$ indices in the ghost zone. A perfect match confirms the packing-free correctness of the datatype.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes MPI vector datatype parameters for ghost zone exchange and verifies correctness.\n    \"\"\"\n    # Test suite as defined in the problem statement.\n    test_cases = [\n        (4, 5, 10, 1, \"low\"),   # Case A\n        (1, 8, 16, 2, \"high\"),  # Case B\n        (3, 1, 7, 1, \"low\"),   # Case C\n        (2, 3, 1, 1, \"high\")   # Case D\n    ]\n\n    results = []\n    for case in test_cases:\n        Ni, Nj, Nk, G, side = case\n\n        # 1. Compute bytes per vertical column (B)\n        bytes_per_element = 8\n        B = bytes_per_element * Nk\n\n        # 2. Compute MPI vector parameters (count, blocklength, stride)\n        # These are derived from the memory mapping and data structure.\n        # count: number of i-slices\n        # blocklength: contiguous block size for a fixed i (G vertical columns)\n        # stride: distance between the start of blocks in consecutive i-slices\n        count = Ni\n        blocklength = G * Nk\n        stride = Nj * Nk\n\n        # 3. Verify packing-free correctness\n        \n        # Helper for linear offset calculation\n        def get_offset(i, j, k, _nj, _nk):\n            return i * (_nj * _nk) + j * _nk + k\n\n        # Generate the list of true offsets by iterating through the specified indices\n        true_offsets = []\n        if side == \"low\":\n            j_indices = range(G)\n        else:  # side == \"high\"\n            j_indices = range(Nj - G, Nj)\n        \n        for i in range(Ni):\n            for j in j_indices:\n                for k in range(Nk):\n                    true_offsets.append(get_offset(i, j, k, Nj, Nk))\n        \n        # Generate the list of offsets using the derived vector datatype parameters\n        vector_offsets = []\n        \n        # The start of the MPI transfer is the first element of the ghost zone\n        if side == \"low\":\n            start_offset = 0 # offset(0,0,0)\n        else: # side == \"high\"\n            start_offset = get_offset(0, Nj - G, 0, Nj, Nk)\n\n        for c in range(count): # Iterate through blocks (i-slices)\n            block_start = start_offset + c * stride\n            for b in range(blocklength): # Iterate through elements within a block\n                vector_offsets.append(block_start + b)\n        \n        # The datatype is correct if the generated offset lists match exactly.\n        packing_free = (true_offsets == vector_offsets)\n\n        results.append([B, count, blocklength, stride, packing_free])\n\n    # Format the final output string exactly as required.\n    results_str = []\n    for res in results:\n        b, c, bl, s, p = res\n        p_str = str(p).lower()\n        res_str = f\"[{b},{c},{bl},{s},{p_str}]\"\n        results_str.append(res_str)\n    \n    print(f\"[{','.join(results_str)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "While parallel speedup is a primary goal, ensuring scientific reproducibility is equally important, a challenge highlighted in this advanced practice . You will confront the subtle problem of non-associative floating-point arithmetic, which can cause run-to-run variations in a global reduction like summing mass or energy. You will not only implement a strategy to enforce a deterministic summation order but also conduct a principled performance analysis to quantify the overhead this introduces, learning to balance the need for reproducibility against computational cost.",
            "id": "4074085",
            "problem": "You are given a scenario common in numerical weather prediction and climate modeling where the global mass of a discretized atmosphere or ocean must be summed across a distributed-memory decomposition and within shared-memory worker threads. In floating-point arithmetic as defined by Institute of Electrical and Electronics Engineers (IEEE) $754$, addition is not associative, so an unordered parallel reduction can lead to run-to-run variability. Your task is to construct and analyze a deterministic reduction strategy that fixes an order across ranks and threads, then quantify its overhead relative to an unordered reduction under a principled cost model.\n\nFundamental base:\n- Floating-point addition is non-associative due to rounding, meaning that for real numbers $a$, $b$, and $c$, generally $(a + b) + c \\neq a + (b + c)$ in floating-point arithmetic because each operation may incur rounding.\n- A reproducible reduction can be forced by imposing a total order on the operands and executing additions in that fixed order.\n- In distributed-memory programming, such as with Message Passing Interface (MPI), and shared-memory threading, such as with Open Multi-Processing (OpenMP), parallel reductions typically use tree-like combining structures that minimize critical-path depth but do not guarantee a fixed cross-process ordering unless explicitly enforced.\n\nDeterministic strategy to implement:\n- Within each thread, sort local contributions by descending absolute value using a stable comparison-based algorithm, then sum in that order.\n- Within each rank, sum thread partials strictly sequentially in ascending thread identifier order.\n- Across ranks, sum rank partials strictly sequentially in ascending rank identifier order.\nThis defines a global lexicographic order on all contributions by the tuple $(r, t, i)$ where $r$ is the rank identifier, $t$ is the thread identifier, and $i$ is the index within the sorted thread-local sequence (sorted by descending absolute value), thereby ensuring a fixed order of additions.\n\nUnordered baseline to compare against:\n- Within each thread, sum in the given input order (no sorting).\n- Within each rank, combine thread partials using a balanced binary tree reduction.\n- Across ranks, combine rank partials using a balanced binary tree reduction.\n\nCost model and quantities to compute:\n- Let $R$ denote the number of ranks and $T$ denote the number of threads per rank. Let $n_{r,t}$ denote the number of contributions in thread $t$ of rank $r$, with total $N = \\sum_{r=0}^{R-1}\\sum_{t=0}^{T-1} n_{r,t}$.\n- Define the unordered work cost as the number of additions, which is $N - 1$ for summing $N$ numbers.\n- Define the deterministic work cost as the number of additions plus the number of comparisons performed by a stable mergesort within each thread when sorting by descending absolute value. The comparisons are empirically counted by the sort procedure.\n- Define the unordered span (critical-path depth) as $\\left\\lceil \\log_2 T \\right\\rceil + \\left\\lceil \\log_2 R \\right\\rceil$, which measures the number of tree-reduction rounds assuming balanced trees for combining $T$ thread partials within each rank and then $R$ rank partials globally.\n- Define the deterministic span as $\\left\\lceil \\log_2 n^* \\right\\rceil + (T - 1) + (R - 1)$ where $n^* = \\max_{r,t} n_{r,t}$ is the largest thread-local sequence length, modeling the depth of mergesort within the slowest thread plus fully sequential combining across threads and ranks.\n\nFor each test case, compute two floats:\n- The work overhead factor $F_{\\text{work}} = \\dfrac{\\text{adds}_{\\text{det}} + \\text{comps}_{\\text{det}}}{\\text{adds}_{\\text{unord}}} = 1 + \\dfrac{\\text{comps}_{\\text{det}}}{N - 1}$ for $N \\geq 2$.\n- The span overhead factor $F_{\\text{span}} = \\dfrac{\\left\\lceil \\log_2 n^* \\right\\rceil + (T - 1) + (R - 1)}{\\left\\lceil \\log_2 T \\right\\rceil + \\left\\lceil \\log_2 R \\right\\rceil}$.\n\nYour program must implement both the deterministic and unordered reductions, count comparisons precisely via a stable mergesort used for the deterministic sort by descending absolute value, and compute the two overhead factors defined above. The actual floating-point sums must be performed, but the outputs to be reported are only the overhead factors and must be expressed as decimal floats rounded to six places.\n\nTest suite:\n- Case $1$ (general mixed sizes): $R = 3$, $T = 4$, with contributions (per rank, then per thread within rank):\n  - Rank $0$: thread $0$: $[10^6, 10^{-2}, 3.0]$, thread $1$: $[4\\times10^5, -3\\times10^5, 10^{-4}, 2.5]$, thread $2$: $[7\\times10^3, 2\\times10^3, 5\\times10^3]$, thread $3$: $[1.2345\\times10^2, -1.2344\\times10^2]$.\n  - Rank $1$: thread $0$: $[9\\times10^6, 10^{-3}]$, thread $1$: $[5\\times10^5, 5\\times10^5, -10^5]$, thread $2$: $[3.14, 2.71, -6.28]$, thread $3$: $[8\\times10^4]$.\n  - Rank $2$: thread $0$: $[10^3, 10^3, 10^3, -10^3]$, thread $1$: $[10^{-6}, 10^6]$, thread $2$: $[2.0, -2.0, 0.5]$, thread $3$: $[7\\times10^7, -6\\times10^7, 10^7]$.\n- Case $2$ (single rank, many threads, includes an empty thread): $R = 1$, $T = 8$, with contributions:\n  - Rank $0$: thread $0$: $[10^5, 2\\times10^5, 3\\times10^5]$, thread $1$: $[10^{-4}, -10^{-4}]$, thread $2$: $[10.0, 20.0, 30.0, -60.0, 0.1]$, thread $3$: $[3\\times10^8, -2\\times10^8, 10^8]$, thread $4$: $[1.0]$, thread $5$: $[5\\times10^2, 2.5\\times10^2]$, thread $6$: $[]$, thread $7$: $[7.77, -0.77, 0.77]$.\n- Case $3$ (many ranks, single thread per rank): $R = 4$, $T = 1$, with contributions:\n  - Rank $0$: thread $0$: $[10^9, -10^9, 123.45]$, rank $1$: thread $0$: $[2\\times10^9, 2\\times10^{-9}, -10^9]$, rank $2$: thread $0$: $[3\\times10^2, -10^2, -2\\times10^2, 10^{-2}]$, rank $3$: thread $0$: $[4.0, 5.0, -9.0]$.\n- Case $4$ (uneven, includes empty threads across ranks): $R = 2$, $T = 5$, with contributions:\n  - Rank $0$: threads: $[10^7, 2\\times10^7]$, $[10^{-3}, 2\\times10^{-3}, 3\\times10^{-3}]$, $[]$, $[4\\times10^{-5}]$, $[5\\times10^3, -5\\times10^3]$.\n  - Rank $1$: threads: $[6\\times10^1, 7\\times10^1, 8\\times10^1]$, $[9\\times10^2]$, $[10^{-8}, -10^{-8}, 3\\times10^{-8}]$, $[2.2]$, $[]$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each result must be a two-element list $[F_{\\text{work}}, F_{\\text{span}}]$, both expressed as decimal floats rounded to six places, in the order of the test cases described above. For example: $[[a,b],[c,d],[e,f],[g,h]]$ where $a$, $b$, $c$, $d$, $e$, $f$, $g$, $h$ are decimal floats.",
            "solution": "The problem statement has been critically validated and is deemed sound. It is scientifically grounded in the principles of floating-point arithmetic and parallel computing, well-posed with clear definitions and constraints, and presents a non-trivial, verifiable algorithmic analysis task. All data and formulas required for a unique solution are provided, and the test cases are well-behaved, avoiding degenerate conditions.\n\nThe solution proceeds by first implementing the two reduction strategies—deterministic and unordered—as specified. The core of the analysis lies in quantifying the computational overheads introduced by the deterministic approach to guarantee reproducibility. This involves calculating two metrics: the work overhead factor, $F_{\\text{work}}$, and the span overhead factor, $F_{\\text{span}}$.\n\nA custom, stable mergesort algorithm is implemented to sort local contributions and, critically, to count the exact number of comparisons performed. This count, $\\text{comps}_{\\text{det}}$, is central to the work overhead calculation. The summation of floating-point numbers is performed according to each strategy's rules to fully adhere to the problem description, although the summed values themselves are not part of the final output.\n\nThe methodology for each test case is as follows:\n$1$. Parse the input data to determine the number of ranks $R$, the number of threads per rank $T$, and the list of contributions for each thread.\n$2$. Compute primary quantities from the input data:\n- The total number of contributions, $N = \\sum_{r=0}^{R-1}\\sum_{t=0}^{T-1} n_{r,t}$, where $n_{r,t}$ is the number of values for thread $t$ in rank $r$.\n- The maximum number of contributions in any single thread, $n^* = \\max_{r,t} n_{r,t}$.\n$3$. Calculate the total number of comparisons for the deterministic strategy, $\\text{comps}_{\\text{det}}$. This is done by iterating through every thread's local data, applying the implemented comparison-counting stable mergesort, and summing the comparison counts. The mergesort is configured to sort by descending absolute value. In our implementation, a recursive mergesort is defined. The merge step, which combines two sorted sub-arrays, is where comparisons are counted. For two elements $a$ and $b$ being considered, a comparison is counted. To ensure stability, if $|a| = |b|$, the element from the sub-array that appeared first in the original array (the \"left\" sub-array in a typical implementation) is chosen first. This is achieved by using a non-strict inequality, $|a| \\ge |b|$, when deciding which element to append to the merged list.\n$4$. Calculate the work overhead factor, $F_{\\text{work}}$. The work for the unordered strategy is the number of additions, $\\text{adds}_{\\text{unord}} = N-1$. The work for the deterministic strategy includes both additions, $\\text{adds}_{\\text{det}} = N-1$, and the comparisons from sorting, $\\text{comps}_{\\text{det}}$. The formula is given as:\n$$F_{\\text{work}} = \\frac{\\text{adds}_{\\text{det}} + \\text{comps}_{\\text{det}}}{\\text{adds}_{\\text{unord}}} = \\frac{(N - 1) + \\text{comps}_{\\text{det}}}{N - 1} = 1 + \\frac{\\text{comps}_{\\text{det}}}{N - 1}$$\nThis formula is valid for $N \\ge 2$, which holds for all test cases.\n\n$5$. Calculate the span overhead factor, $F_{\\text{span}}$. The span, or critical path length, is modeled differently for each strategy.\n- For the unordered strategy, the span is the depth of the two-level tree reduction: $\\text{span}_{\\text{unord}} = \\lceil \\log_2 T \\rceil + \\lceil \\log_2 R \\rceil$.\n- For the deterministic strategy, the span is the sum of the depth of the local sort (modeled as a parallel sort like mergesort, with depth $\\lceil \\log_2 n^* \\rceil$) and the depths of the sequential summations across threads ($T-1$) and ranks ($R-1$): $\\text{span}_{\\text{det}} = \\lceil \\log_2 n^* \\rceil + (T - 1) + (R - 1)$.\nThe overhead factor is the ratio of these two spans:\n$$F_{\\text{span}} = \\frac{\\text{span}_{\\text{det}}}{\\text{span}_{\\text{unord}}} = \\frac{\\lceil \\log_2 n^* \\rceil + T + R - 2}{\\lceil \\log_2 T \\rceil + \\lceil \\log_2 R \\rceil}$$\nThe term $\\lceil \\log_2 k \\rceil$ is calculated for integer $k \\ge 1$. For $k=1$, $\\lceil \\log_2 1 \\rceil = 0$. For $k>1$, this represents the depth of a balanced binary tree on $k$ leaves. The provided test cases ensure that $R$ and $T$ are not simultaneously $1$, so the denominator is always non-zero.\n\nThese calculations are performed for each of the $4$ test cases, and the resulting pairs of $(F_{\\text{work}}, F_{\\text{span}})$ are formatted to $6$ decimal places as required.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport math\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test cases.\n    It calculates work and span overhead for a deterministic parallel reduction\n    compared to a non-deterministic one.\n    \"\"\"\n    test_cases = [\n        # Case 1\n        [\n            [[10e6, 10e-2, 3.0], [4e5, -3e5, 10e-4, 2.5], [7e3, 2e3, 5e3], [1.2345e2, -1.2344e2]],\n            [[9e6, 10e-3], [5e5, 5e5, -1e5], [3.14, 2.71, -6.28], [8e4]],\n            [[1e3, 1e3, 1e3, -1e3], [10e-6, 10e6], [2.0, -2.0, 0.5], [7e7, -6e7, 1e7]]\n        ],\n        # Case 2\n        [\n            [[1e5, 2e5, 3e5], [1e-4, -1e-4], [10.0, 20.0, 30.0, -60.0, 0.1], [3e8, -2e8, 1e8], [1.0], [5e2, 2.5e2], [], [7.77, -0.77, 0.77]]\n        ],\n        # Case 3\n        [\n            [[1e9, -1e9, 123.45]],\n            [[2e9, 2e-9, -1e9]],\n            [[3e2, -1e2, -2e2, 1e-2]],\n            [[4.0, 5.0, -9.0]]\n        ],\n        # Case 4\n        [\n            [[1e7, 2e7], [1e-3, 2e-3, 3e-3], [], [4e-5], [5e3, -5e3]],\n            [[6e1, 7e1, 8e1], [9e2], [1e-8, -1e-8, 3e-8], [2.2], []]\n        ],\n    ]\n\n    def mergesort_with_comp_count(arr):\n        \"\"\"\n        A stable mergesort that sorts by descending absolute value and counts comparisons.\n        \"\"\"\n        comparisons = 0\n\n        def merge(left, right):\n            nonlocal comparisons\n            merged = []\n            i, j = 0, 0\n            while i  len(left) and j  len(right):\n                comparisons += 1\n                # Use >= for stability and descending order by absolute value\n                if abs(left[i]) >= abs(right[j]):\n                    merged.append(left[i])\n                    i += 1\n                else:\n                    merged.append(right[j])\n                    j += 1\n            merged.extend(left[i:])\n            merged.extend(right[j:])\n            return merged\n\n        def sort(sub_arr):\n            if len(sub_arr) = 1:\n                return sub_arr\n            mid = len(sub_arr) // 2\n            left = sort(sub_arr[:mid])\n            right = sort(sub_arr[mid:])\n            return merge(left, right)\n\n        # Create a copy to avoid modifying the original list\n        sorted_arr = sort(list(arr))\n        return sorted_arr, comparisons\n\n    def tree_reduce(items):\n        \"\"\"\n        Simulates a balanced binary tree reduction.\n        \"\"\"\n        if not items:\n            return 0.0\n        \n        queue = list(items)\n        while len(queue) > 1:\n            next_level = []\n            for i in range(0, len(queue), 2):\n                if i + 1  len(queue):\n                    next_level.append(queue[i] + queue[i+1])\n                else:\n                    next_level.append(queue[i])\n            queue = next_level\n        return queue[0]\n\n    def ceil_log2(n):\n        \"\"\"\n        Computes ceil(log2(n)) for n >= 1.\n        \"\"\"\n        if n = 0: return -float('inf') # Should not happen in this problem\n        if n == 1: return 0\n        return math.ceil(math.log2(n))\n\n    def calculate_overheads(contributions):\n        \"\"\"\n        Calculates F_work and F_span for a given test case.\n        \"\"\"\n        R = len(contributions)\n        T = len(contributions[0])\n\n        total_contributions = 0\n        max_thread_len = 0\n        total_comparisons = 0\n        \n        # --- Perform Reductions and Gather Metrics ---\n        # Deterministic\n        rank_partials_det = []\n        for r in range(R):\n            thread_partials_det = []\n            for t in range(T):\n                thread_data = contributions[r][t]\n                n_rt = len(thread_data)\n                total_contributions += n_rt\n                if n_rt > max_thread_len:\n                    max_thread_len = n_rt\n                \n                # Sort and count comparisons\n                sorted_data, comps = mergesort_with_comp_count(thread_data)\n                total_comparisons += comps\n                \n                # Sum within thread\n                thread_sum = sum(sorted_data)\n                thread_partials_det.append(thread_sum)\n\n            # Sum across threads (sequentially)\n            rank_sum_det = sum(thread_partials_det)\n            rank_partials_det.append(rank_sum_det)\n        \n        # Sum across ranks (sequentially)\n        total_sum_det = sum(rank_partials_det)\n\n        # Unordered (for completeness, sum is not used for factors)\n        rank_partials_unord = []\n        for r in range(R):\n            thread_partials_unord = [sum(thread_data) for thread_data in contributions[r]]\n            rank_sum_unord = tree_reduce(thread_partials_unord)\n            rank_partials_unord.append(rank_sum_unord)\n\n        # Sum across ranks (tree)\n        total_sum_unord = tree_reduce(rank_partials_unord)\n\n        # --- Calculate Overhead Factors ---\n        N = total_contributions\n        n_star = max_thread_len\n        \n        # Work Overhead\n        if N  2:\n            f_work = 1.0\n        else:\n            f_work = 1.0 + total_comparisons / (N - 1)\n\n        # Span Overhead\n        span_det = ceil_log2(n_star) + (T - 1) + (R - 1)\n        span_unord_denom = ceil_log2(T) + ceil_log2(R)\n        \n        if span_unord_denom == 0:\n            # This case (R=1, T=1) is not in test suite, but handle defensively\n            f_span = float('inf') if span_det > 0 else 1.0 \n        else:\n            f_span = span_det / span_unord_denom\n\n        return [f_work, f_span]\n\n    results = []\n    for case in test_cases:\n        overheads = calculate_overheads(case)\n        results.append(overheads)\n\n    formatted_pairs = [f\"[{w:.6f},{s:.6f}]\" for w, s in results]\n    final_output = f\"[{','.join(formatted_pairs)}]\"\n    print(final_output)\n\nsolve()\n```"
        }
    ]
}