{
    "hands_on_practices": [
        {
            "introduction": "The Extended Kalman Filter (EKF) hinges on the accurate linearization of nonlinear dynamics. This first exercise provides fundamental practice in this crucial step by focusing on the state transition Jacobian matrix, $A_k$. You will work with a simplified barotropic vorticity model, a classic tool in geophysical fluid dynamics, to derive its analytical Jacobian and verify your result against a numerical approximation, a core skill for developing and debugging any EKF implementation .",
            "id": "4057052",
            "problem": "Consider a simplified, nondimensional barotropic vorticity model on a periodic square domain. Let the state vector be the discrete relative vorticity field $\\boldsymbol{\\zeta}_k \\in \\mathbb{R}^{N}$ at discrete time index $k$, where $N = n_x n_y$, $n_x$ is the number of grid points in the $x$-direction, and $n_y$ is the number of grid points in the $y$-direction. The continuous barotropic vorticity equation on a beta-plane without planetary vorticity gradient and without friction or forcing reduces to\n$$\n\\frac{\\partial \\zeta}{\\partial t} = - J(\\psi, \\zeta),\n$$\nwhere $\\zeta = \\nabla^2 \\psi$, $\\psi$ is the streamfunction, and $J(\\psi, \\zeta) = \\frac{\\partial \\psi}{\\partial x} \\frac{\\partial \\zeta}{\\partial y} - \\frac{\\partial \\psi}{\\partial y} \\frac{\\partial \\zeta}{\\partial x}$ is the Jacobian (advective nonlinearity). Under periodic boundary conditions and a uniform grid, define discrete central finite difference operators $D_x$ and $D_y$ that approximate $\\partial/\\partial x$ and $\\partial/\\partial y$, respectively, and a discrete inverse-Laplacian operator $P$ such that $\\boldsymbol{\\psi} = P \\boldsymbol{\\zeta}$ is the solution to the discrete Poisson equation $\\nabla^2 \\psi = \\zeta$ with zero-mean constraint enforced spectrally. Let the discrete velocity components be\n$$\n\\boldsymbol{u} = - D_y \\boldsymbol{\\psi}, \\quad \\boldsymbol{v} = D_x \\boldsymbol{\\psi},\n$$\nand the discrete spatial derivatives of vorticity be\n$$\n\\boldsymbol{\\zeta}_x = D_x \\boldsymbol{\\zeta}, \\quad \\boldsymbol{\\zeta}_y = D_y \\boldsymbol{\\zeta}.\n$$\nThen the discrete right-hand side is\n$$\n\\mathbf{F}(\\boldsymbol{\\zeta}) = \\boldsymbol{u} \\circ \\boldsymbol{\\zeta}_x + \\boldsymbol{v} \\circ \\boldsymbol{\\zeta}_y,\n$$\nwhere $\\circ$ denotes elementwise multiplication. Consider a single forward Euler time step,\n$$\n\\boldsymbol{\\zeta}_{k+1} = \\boldsymbol{\\zeta}_k + \\Delta t \\, \\mathbf{F}(\\boldsymbol{\\zeta}_k),\n$$\nwhose state transition Jacobian for the Extended Kalman Filter (EKF) is\n$$\nA_k = I + \\Delta t \\, J_{\\mathbf{F}}(\\boldsymbol{\\zeta}_k),\n$$\nwhere $I$ is the identity matrix and $J_{\\mathbf{F}}(\\boldsymbol{\\zeta}_k)$ is the Jacobian of $\\mathbf{F}$ evaluated at $\\boldsymbol{\\zeta}_k$.\n\nYour task is to:\n- Construct $D_x$ and $D_y$ as central finite difference matrices with periodic boundary conditions on a uniform grid over the domain of length $L_x$ in $x$ and $L_y$ in $y$. Use spacing $d_x = L_x / n_x$ and $d_y = L_y / n_y$.\n- Implement $P$ spectrally using the discrete Fourier transform so that $\\widehat{\\psi}(\\boldsymbol{k}) = - \\widehat{\\zeta}(\\boldsymbol{k}) / \\|\\boldsymbol{k}\\|^2$ for nonzero wavenumbers and $\\widehat{\\psi}(\\boldsymbol{0}) = 0$, where $\\boldsymbol{k}$ collects the wavenumbers in $x$ and $y$.\n- Derive and implement the analytical Jacobian $J_{\\mathbf{F}}(\\boldsymbol{\\zeta})$ as a linear operator acting on perturbations $\\delta \\boldsymbol{\\zeta}$,\n$$\n\\delta \\mathbf{F} = \\left[ \\operatorname{diag}(\\boldsymbol{\\zeta}_x) \\left( - D_y P \\right) + \\operatorname{diag}(\\boldsymbol{u}) D_x + \\operatorname{diag}(\\boldsymbol{\\zeta}_y) \\left( D_x P \\right) + \\operatorname{diag}(\\boldsymbol{v}) D_y \\right] \\delta \\boldsymbol{\\zeta},\n$$\nso that\n$$\nJ_{\\mathbf{F}}(\\boldsymbol{\\zeta}) = \\operatorname{diag}(\\boldsymbol{\\zeta}_x) \\left( - D_y P \\right) + \\operatorname{diag}(\\boldsymbol{u}) D_x + \\operatorname{diag}(\\boldsymbol{\\zeta}_y) \\left( D_x P \\right) + \\operatorname{diag}(\\boldsymbol{v}) D_y.\n$$\n- Compute $A_k$ analytically via $A_k^{(\\text{ana})} = I + \\Delta t \\, J_{\\mathbf{F}}(\\boldsymbol{\\zeta}_k)$.\n- Compute $A_k$ numerically via finite differences for the Jacobian using a symmetric perturbation of magnitude $\\varepsilon$ to each component of $\\boldsymbol{\\zeta}_k$, i.e.,\n$$\n\\left[J_{\\mathbf{F}}(\\boldsymbol{\\zeta}_k)\\right]_{:,i} \\approx \\frac{\\mathbf{F}(\\boldsymbol{\\zeta}_k + \\varepsilon \\mathbf{e}_i) - \\mathbf{F}(\\boldsymbol{\\zeta}_k - \\varepsilon \\mathbf{e}_i)}{2 \\varepsilon},\n$$\nand then $A_k^{(\\text{num})} = I + \\Delta t \\, J_{\\mathbf{F}}^{(\\text{num})}(\\boldsymbol{\\zeta}_k)$.\n\nCompare $A_k^{(\\text{ana})}$ and $A_k^{(\\text{num})}$ by computing the Frobenius norm of their difference,\n$$\n\\| A_k^{(\\text{num})} - A_k^{(\\text{ana})} \\|_F.\n$$\n\nTest Suite:\n- Case $1$: $n_x = 4$, $n_y = 4$, $L_x = 2\\pi$, $L_y = 2\\pi$, $\\Delta t = 0.01$, $\\boldsymbol{\\zeta}_k$ is the zero field, $\\varepsilon = 10^{-6}$.\n- Case $2$: $n_x = 4$, $n_y = 4$, $L_x = 2\\pi$, $L_y = 2\\pi$, $\\Delta t = 0.01$, $\\boldsymbol{\\zeta}_k$ is a random field with zero mean generated from a fixed seed and scaled to amplitude $0.1$, $\\varepsilon = 10^{-5}$.\n- Case $3$: $n_x = 5$, $n_y = 5$, $L_x = 2\\pi$, $L_y = 2\\pi$, $\\Delta t = 0.05$, $\\boldsymbol{\\zeta}_k(i,j) = \\sin(k_x x_j) \\sin(k_y y_i)$ with $k_x = 1$, $k_y = 2$, amplitude $1.0$, $\\varepsilon = 10^{-6}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each float rounded to $8$ decimal places, in the order of the test cases: $[\\|A_1^{(\\text{num})} - A_1^{(\\text{ana})}\\|_F,\\|A_2^{(\\text{num})} - A_2^{(\\text{ana})}\\|_F,\\|A_3^{(\\text{num})} - A_3^{(\\text{ana})}\\|_F]$.",
            "solution": "The user has provided a problem that requires the verification of an analytical Jacobian matrix for a discrete dynamical system against a numerically computed one. The system models the barotropic vorticity equation on a periodic domain. The task involves constructing several discrete operators, implementing the system's dynamics, and then computing and comparing the Jacobians.\n\n### Step 1: Extract Givens\n\n-   **State Vector**: Discrete relative vorticity $\\boldsymbol{\\zeta}_k \\in \\mathbb{R}^{N}$ at time $k$, where $N = n_x n_y$.\n-   **Governing Equation (Continuous)**: $\\frac{\\partial \\zeta}{\\partial t} = - J(\\psi, \\zeta)$, with $\\zeta = \\nabla^2 \\psi$ and $J(\\psi, \\zeta) = \\frac{\\partial \\psi}{\\partial x} \\frac{\\partial \\zeta}{\\partial y} - \\frac{\\partial \\psi}{\\partial y} \\frac{\\partial \\zeta}{\\partial x}$.\n-   **Domain**: Periodic square (or rectangular) domain of size $L_x \\times L_y$.\n-   **Grid**: Uniform with $n_x \\times n_y$ points. Spacing is $d_x = L_x / n_x$ and $d_y = L_y / n_y$.\n-   **Discrete Operators**:\n    -   $D_x, D_y$: Central finite difference matrices for $\\partial/\\partial x, \\partial/\\partial y$ with periodic boundaries.\n    -   $P$: Spectral inverse-Laplacian operator, $\\boldsymbol{\\psi} = P \\boldsymbol{\\zeta}$, solving $\\nabla^2 \\psi = \\zeta$. In Fourier space, $\\widehat{\\psi}(\\boldsymbol{k}) = - \\widehat{\\zeta}(\\boldsymbol{k}) / \\|\\boldsymbol{k}\\|^2$ for $\\boldsymbol{k} \\neq \\boldsymbol{0}$ and $\\widehat{\\psi}(\\boldsymbol{0}) = 0$.\n-   **Discrete Velocity and Vorticity Gradients**:\n    -   $\\boldsymbol{u} = - D_y \\boldsymbol{\\psi}$, $\\boldsymbol{v} = D_x \\boldsymbol{\\psi}$.\n    -   $\\boldsymbol{\\zeta}_x = D_x \\boldsymbol{\\zeta}$, $\\boldsymbol{\\zeta}_y = D_y \\boldsymbol{\\zeta}$.\n-   **Discrete Dynamics Function**: $\\mathbf{F}(\\boldsymbol{\\zeta}) = \\boldsymbol{u} \\circ \\boldsymbol{\\zeta}_x + \\boldsymbol{v} \\circ \\boldsymbol{\\zeta}_y$, where $\\circ$ is elementwise multiplication.\n-   **Time-Stepping Scheme**: Forward Euler, $\\boldsymbol{\\zeta}_{k+1} = \\boldsymbol{\\zeta}_k + \\Delta t \\, \\mathbf{F}(\\boldsymbol{\\zeta}_k)$.\n-   **State Transition Jacobian**: $A_k = I + \\Delta t \\, J_{\\mathbf{F}}(\\boldsymbol{\\zeta}_k)$.\n-   **Analytical Jacobian of F**: $J_{\\mathbf{F}}(\\boldsymbol{\\zeta}) = \\operatorname{diag}(\\boldsymbol{\\zeta}_x) \\left( - D_y P \\right) + \\operatorname{diag}(\\boldsymbol{u}) D_x + \\operatorname{diag}(\\boldsymbol{\\zeta}_y) \\left( D_x P \\right) + \\operatorname{diag}(\\boldsymbol{v}) D_y$.\n-   **Numerical Jacobian of F**: The $i$-th column is given by $\\frac{\\mathbf{F}(\\boldsymbol{\\zeta}_k + \\varepsilon \\mathbf{e}_i) - \\mathbf{F}(\\boldsymbol{\\zeta}_k - \\varepsilon \\mathbf{e}_i)}{2 \\varepsilon}$.\n-   **Comparison Metric**: Frobenius norm of the difference, $\\| A_k^{(\\text{num})} - A_k^{(\\text{ana})} \\|_F$.\n-   **Test Cases**:\n    1.  $n_x = 4, n_y = 4, L_x = 2\\pi, L_y = 2\\pi, \\Delta t = 0.01, \\boldsymbol{\\zeta}_k = \\mathbf{0}, \\varepsilon = 10^{-6}$.\n    2.  $n_x = 4, n_y = 4, L_x = 2\\pi, L_y = 2\\pi, \\Delta t = 0.01$, $\\boldsymbol{\\zeta}_k$ is a random field with zero mean and amplitude $0.1$ (from a fixed seed), $\\varepsilon = 10^{-5}$.\n    3.  $n_x = 5, n_y = 5, L_x = 2\\pi, L_y = 2\\pi, \\Delta t = 0.05, \\boldsymbol{\\zeta}_k(i,j) = \\sin(k_x x_j) \\sin(k_y y_i)$ with $k_x = 1, k_y = 2$, amplitude $1.0$, $\\varepsilon = 10^{-6}$.\n\n### Step 2: Validate Using Extracted Givens\n\n-   **Scientifically Grounded**: The problem is set in the context of geophysical fluid dynamics and data assimilation (EKF), which are established scientific fields. The model is a standard simplification of the shallow water equations. The discrete form of the advection term $\\mathbf{F}$ has a sign discrepancy with the standard barotropic vorticity equation ($u\\zeta_x - v\\zeta_y$), as it is given as $u\\zeta_x + v\\zeta_y$. However, the problem is self-contained: it defines a function $\\mathbf{F}$ and asks for its Jacobian. The task is a mathematical and numerical analysis exercise, not a test of physical fidelity. The setup is internally consistent and mathematically rigorous.\n-   **Well-Posed**: The task is to compute a scalar value (the Frobenius norm of a matrix difference). All necessary functions, parameters, and methods are explicitly defined. A unique, stable, and meaningful solution exists for each test case.\n-   **Objective**: The problem is specified using precise mathematical language and quantitative data. There are no subjective or ambiguous statements.\n\nThe problem is free of the invalidity flaws. It is scientifically grounded within the context of numerical methods, well-posed, and objective.\n\n### Step 3: Verdict and Action\n\nThe problem statement is **valid**. A solution will be provided.\n\n### Principle-Based Design of the Solution\n\nThe core of the problem is to implement several linear operators and a nonlinear function, and then to compute and compare two different representations of the Jacobian of this function. The solution will be structured to systematically build each component.\n\n1.  **Discretization and Indexing**: We will represent the 2D vorticity field $\\zeta(y,x)$ as a 2D `numpy` array of shape $(n_y, n_x)$. The first index will correspond to the $y$-direction (rows) and the second to the $x$-direction (columns). When a 1D state vector $\\boldsymbol{\\zeta}$ is needed, this 2D array will be flattened in row-major ('C') order. All linear operators ($D_x, D_y, P$) will be constructed as dense $N \\times N$ matrices, where $N=n_x n_y$, consistent with this flattening scheme.\n\n2.  **Finite Difference Operators ($D_x, D_y$)**: These matrices represent central differences on a periodic grid. For an $N$-point grid, the $k$-th row of the matrix will have non-zero entries corresponding to the neighbors of the $k$-th grid point. For our 2D grid flattened to 1D, the neighbors of point $(i,j)$ (with flat index $k=i \\cdot n_x + j$) are $(i, j \\pm 1)$ for $D_x$ and $(i \\pm 1, j)$ for $D_y$. The periodic boundary conditions are handled using modulo arithmetic on the indices.\n\n3.  **Spectral Inverse-Laplacian Operator ($P$)**: This operator is most naturally implemented as a function that transforms its input vector to Fourier space.\n    -   The input vector $\\boldsymbol{\\zeta}$ is reshaped into a 2D field.\n    -   A 2D Fast Fourier Transform (FFT) is applied to get $\\widehat{\\boldsymbol{\\zeta}}$.\n    -   A grid of squared wavenumbers, $\\|\\boldsymbol{k}\\|^2 = k_x^2 + k_y^2$, is constructed.\n    -   $\\widehat{\\boldsymbol{\\psi}}$ is computed by dividing $\\widehat{\\boldsymbol{\\zeta}}$ by $-\\|\\boldsymbol{k}\\|^2$ for all non-zero wavenumbers. The zero-wavenumber component of $\\widehat{\\boldsymbol{\\psi}}$ is set to zero, enforcing a zero mean on the streamfunction.\n    -   An inverse 2D FFT yields the 2D streamfunction field $\\boldsymbol{\\psi}$, which is then flattened.\n    -   To obtain the matrix form $P_{\\text{mat}}$, this function is applied to each standard basis vector $\\mathbf{e}_i$, with the result forming the $i$-th column of $P_{\\text{mat}}$.\n\n4.  **Nonlinear Dynamics Function ($\\mathbf{F}$)**: This function takes $\\boldsymbol{\\zeta}$ and computes $\\mathbf{F}(\\boldsymbol{\\zeta}) = (\\boldsymbol{-D_y P \\zeta}) \\circ (\\boldsymbol{D_x \\zeta}) + (\\boldsymbol{D_x P \\zeta}) \\circ (\\boldsymbol{D_y \\zeta})$. This involves applying the pre-computed operator matrices to $\\boldsymbol{\\zeta}$ and performing element-wise vector products.\n\n5.  **Analytical Jacobian ($J_{\\mathbf{F}}^{(\\text{ana})}$)**: The matrix is constructed according to the formula provided in the problem. This requires the state-dependent vectors $\\boldsymbol{u}, \\boldsymbol{v}, \\boldsymbol{\\zeta}_x, \\boldsymbol{\\zeta}_y$, which are computed at the given state $\\boldsymbol{\\zeta}_k$. These vectors are then formed into diagonal matrices and multiplied with the appropriate operator matrices ($D_x, D_y, P_{\\text{mat}}$ and their products).\n\n6.  **Numerical Jacobian ($J_{\\mathbf{F}}^{(\\text{num})}$)**: This is computed column-by-column using the central finite difference formula. For each column $i$, the function $\\mathbf{F}$ is evaluated at two perturbed states, $\\boldsymbol{\\zeta}_k \\pm \\varepsilon \\mathbf{e}_i$, and the difference is scaled. This is a direct implementation of the numerical differentiation definition.\n\n7.  **Comparison**: Finally, the state transition matrices $A_k = I + \\Delta t J_{\\mathbf{F}}$ are formed for both the analytical and numerical Jacobians. The Frobenius norm of their difference, $\\|A_k^{(\\text{num})} - A_k^{(\\text{ana})}\\|_F$, is computed, which quantifies the discrepancy. For the case where $\\boldsymbol{\\zeta}_k = \\mathbf{0}$, the function $\\mathbf{F}$ is a homogeneous quadratic polynomial, leading to $J_{\\mathbf{F}}(\\mathbf{0}) = \\mathbf{0}$. The numerical finite difference of a quadratic function at zero also yields zero, so we expect the norm of the difference to be exactly zero. For non-zero states, the norm should be small, consistent with the truncation error of the numerical differentiation, which is of order $O(\\varepsilon^2)$.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main solver function that iterates through test cases and prints the results.\n    \"\"\"\n\n    def compute_norm_diff(nx, ny, Lx, Ly, dt, zeta_k, eps):\n        \"\"\"\n        Computes the Frobenius norm of the difference between numerical and\n        analytical state transition Jacobians for a given test case.\n        \"\"\"\n        N = nx * ny\n        dx = Lx / nx\n        dy = Ly / ny\n\n        # 1. Construct discrete finite difference operators Dx and Dy\n        Dx = np.zeros((N, N))\n        Dy = np.zeros((N, N))\n        for i in range(ny):\n            for j in range(nx):\n                k = i * nx + j\n                # Dx (periodic)\n                k_xp1 = i * nx + (j + 1) % nx\n                k_xm1 = i * nx + (j - 1 + nx) % nx\n                Dx[k, k_xp1] = 1.0 / (2.0 * dx)\n                Dx[k, k_xm1] = -1.0 / (2.0 * dx)\n                # Dy (periodic)\n                k_yp1 = ((i + 1) % ny) * nx + j\n                k_ym1 = ((i - 1 + ny) % ny) * nx + j\n                Dy[k, k_yp1] = 1.0 / (2.0 * dy)\n                Dy[k, k_ym1] = -1.0 / (2.0 * dy)\n\n        # 2. Define the spectral inverse-Laplacian operator P as a function\n        kx_vec = 2.0 * np.pi * np.fft.fftfreq(nx, d=dx)\n        ky_vec = 2.0 * np.pi * np.fft.fftfreq(ny, d=dy)\n        kx_grid, ky_grid = np.meshgrid(kx_vec, ky_vec, indexing='xy')\n        ksq = kx_grid**2 + ky_grid**2\n        \n        def P_op(zeta_flat):\n            zeta_2d = zeta_flat.reshape(ny, nx)\n            zeta_hat = np.fft.fft2(zeta_2d)\n            psi_hat = np.zeros_like(zeta_hat, dtype=complex)\n            \n            nonzero_idx = ksq != 0\n            # Broadcasted division\n            psi_hat[nonzero_idx] = -zeta_hat[nonzero_idx] / ksq[nonzero_idx]\n\n            psi_2d = np.real(np.fft.ifft2(psi_hat))\n            return psi_2d.flatten()\n\n        # 3. Construct the matrix form of P\n        P_mat = np.zeros((N, N))\n        I_N = np.identity(N)\n        for j in range(N):\n            P_mat[:, j] = P_op(I_N[:, j])\n            \n        # 4. Define the nonlinear dynamics function F\n        def F_func(zeta_flat):\n            psi_flat = P_op(zeta_flat)\n            u_flat = -Dy @ psi_flat\n            v_flat = Dx @ psi_flat\n            zetax_flat = Dx @ zeta_flat\n            zetay_flat = Dy @ zeta_flat\n            return u_flat * zetax_flat + v_flat * zetay_flat\n\n        # 5. Compute the analytical Jacobian J_ana\n        psi_k = P_op(zeta_k)\n        u_k = -Dy @ psi_k\n        v_k = Dx @ psi_k\n        zetax_k = Dx @ zeta_k\n        zetay_k = Dy @ zeta_k\n\n        term1 = np.diag(zetax_k) @ (-Dy @ P_mat)\n        term2 = np.diag(u_k) @ Dx\n        term3 = np.diag(zetay_k) @ (Dx @ P_mat)\n        term4 = np.diag(v_k) @ Dy\n        J_ana = term1 + term2 + term3 + term4\n\n        # 6. Compute the numerical Jacobian J_num\n        J_num = np.zeros((N, N))\n        for j in range(N):\n            ej = I_N[:, j]\n            zeta_plus = zeta_k + eps * ej\n            zeta_minus = zeta_k - eps * ej\n            F_plus = F_func(zeta_plus)\n            F_minus = F_func(zeta_minus)\n            J_num[:, j] = (F_plus - F_minus) / (2.0 * eps)\n            \n        # 7. Compute state transition matrices and their difference norm\n        A_ana = I_N + dt * J_ana\n        A_num = I_N + dt * J_num\n        norm_diff = np.linalg.norm(A_num - A_ana, 'fro')\n        \n        return norm_diff\n\n    # Define test cases\n    test_cases_params = [\n        {'nx': 4, 'ny': 4, 'Lx': 2*np.pi, 'Ly': 2*np.pi, 'dt': 0.01, 'eps': 1e-6, 'case_id': 1},\n        {'nx': 4, 'ny': 4, 'Lx': 2*np.pi, 'Ly': 2*np.pi, 'dt': 0.01, 'eps': 1e-5, 'case_id': 2},\n        {'nx': 5, 'ny': 5, 'Lx': 2*np.pi, 'Ly': 2*np.pi, 'dt': 0.05, 'eps': 1e-6, 'case_id': 3},\n    ]\n\n    results = []\n    for params in test_cases_params:\n        nx, ny = params['nx'], params['ny']\n        Lx, Ly = params['Lx'], params['Ly']\n        dx, dy = Lx / nx, Ly / ny\n\n        if params['case_id'] == 1:\n            zeta_2d = np.zeros((ny, nx))\n        elif params['case_id'] == 2:\n            rng = np.random.default_rng(seed=0)\n            zeta_2d = rng.random((ny, nx))\n            zeta_2d -= np.mean(zeta_2d)\n            max_abs = np.max(np.abs(zeta_2d))\n            if max_abs > 0:\n                zeta_2d = 0.1 * zeta_2d / max_abs\n        elif params['case_id'] == 3:\n            kx, ky = 1.0, 2.0\n            x_coords = np.arange(nx) * dx\n            y_coords = np.arange(ny) * dy\n            yy_grid, xx_grid = np.meshgrid(y_coords, x_coords, indexing='ij')\n            zeta_2d = np.sin(kx * xx_grid) * np.sin(ky * yy_grid)\n        \n        zeta_k = zeta_2d.flatten()\n        \n        norm_val = compute_norm_diff(params['nx'], params['ny'], params['Lx'], params['Ly'], \n                                     params['dt'], zeta_k, params['eps'])\n        results.append(norm_val)\n\n    # Format and print the final result\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A correctly configured Kalman filter has specific, testable statistical properties. This practice explores one of the most powerful diagnostic tools in data assimilation: the analysis of innovation statistics. You will first derive the theoretical mean and covariance of the innovation vector—the difference between observations and forecasts—and then use these properties to construct a chi-square ($\\chi^2$) test to check for consistency, applying it to a practical example .",
            "id": "4057079",
            "problem": "Consider a data assimilation cycle within Numerical Weather Prediction (NWP) using a Kalman filter (KF) or Extended Kalman filter (EKF), where a forecast state $x^{f} \\in \\mathbb{R}^{n}$ is mapped to the observation space by an observation operator $\\mathcal{H}(\\cdot)$. The innovation is defined as $d = y - \\mathcal{H}(x^{f})$, where $y \\in \\mathbb{R}^{m}$ is the observation vector. Assume the following foundational modeling assumptions that are standard in geophysical data assimilation:\n- The forecast (background) error $e^{f} = x^{t} - x^{f}$, where $x^{t}$ is the true state, is unbiased with $\\mathbb{E}[e^{f}] = 0$, has covariance $\\mathcal{P}^{f}$, and is Gaussian.\n- The observation error $e^{o} = y - \\mathcal{H}(x^{t})$ is unbiased with $\\mathbb{E}[e^{o}] = 0$, has covariance $R$, is Gaussian, and is independent of $e^{f}$.\n- For the EKF, the observation operator is differentiable and linearized at $x^{f}$, so $\\mathcal{H}(x^{t}) \\approx \\mathcal{H}(x^{f}) + H (x^{t} - x^{f})$, where $H$ is the Jacobian evaluated at $x^{f}$.\n\nUnder these assumptions, derive from first principles the expected value and covariance of the innovation $d$. Then, propose a test based on the chi-square distribution to detect inconsistency of $(\\mathcal{P}^{f}, R)$ with the observed innovation statistics.\n\nNow apply your derivation to the following scientifically plausible configuration involving $m = 2$ observations (a near-surface temperature and a horizontal wind component) and $n = 3$ forecast state variables:\n- Forecast state $x^{f} = \\begin{pmatrix} 290 \\\\ 12 \\\\ 0.8 \\end{pmatrix}$.\n- Linearized observation operator $H = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}$ so that $H x^{f} = \\begin{pmatrix} 290 \\\\ 0.8 \\end{pmatrix}$.\n- Observations $y = \\begin{pmatrix} 289.5 \\\\ 1.1 \\end{pmatrix}$.\n- Forecast error covariance\n$$\n\\mathcal{P}^{f} = \\begin{pmatrix}\n0.25 & 0.05 & 0.03 \\\\\n0.05 & 1.00 & 0.02 \\\\\n0.03 & 0.02 & 0.16\n\\end{pmatrix}.\n$$\n- Observation error covariance\n$$\nR = \\begin{pmatrix}\n0.09 & 0.012 \\\\\n0.012 & 0.04\n\\end{pmatrix}.\n$$\n\nUsing your derived expressions, compute the chi-square test statistic for the realized innovation in this cycle. The test statistic you should compute is the quadratic form constructed from the innovation and its covariance under the null hypothesis of consistency. Express your final numerical answer as a single real number. Round your answer to four significant figures. The test statistic is dimensionless, so no physical units are required in the final answer.",
            "solution": "The problem asks for a derivation of the expected value and covariance of the innovation vector in a data assimilation context, the formulation of a chi-square test for statistical consistency, and the application of these results to a specific numerical example. The problem is well-posed and scientifically grounded in the principles of state estimation and data assimilation.\n\nFirst, we derive the statistical properties of the innovation, $d$. The innovation is defined as the difference between the observation vector $y \\in \\mathbb{R}^{m}$ and the forecast state projected into the observation space, $d = y - \\mathcal{H}(x^{f})$.\n\nThe observation vector $y$ is related to the true state $x^t$ through the observation error $e^o$ as $y = \\mathcal{H}(x^t) + e^o$. The true state $x^t$ is related to the forecast state $x^f$ through the forecast error $e^f$ as $x^t = x^f + e^f$.\n\nFor the Extended Kalman Filter (EKF), the observation operator $\\mathcal{H}$ is linearized around the forecast state $x^f$:\n$$\n\\mathcal{H}(x^t) = \\mathcal{H}(x^f + e^f) \\approx \\mathcal{H}(x^f) + H e^f\n$$\nwhere $H$ is the Jacobian of $\\mathcal{H}$ evaluated at $x^f$.\n\nSubstituting these relationships into the definition of the innovation $d$:\n$$\nd = y - \\mathcal{H}(x^f) = (\\mathcal{H}(x^t) + e^o) - \\mathcal{H}(x^f)\n$$\nUsing the linearization, we get:\n$$\nd \\approx (\\mathcal{H}(x^f) + H e^f + e^o) - \\mathcal{H}(x^f) = H e^f + e^o\n$$\nThis expression relates the innovation to the underlying model and observation errors.\n\nNow, we compute the expected value of the innovation, $\\mathbb{E}[d]$. Using the linearity of the expectation operator:\n$$\n\\mathbb{E}[d] \\approx \\mathbb{E}[H e^f + e^o] = \\mathbb{E}[H e^f] + \\mathbb{E}[e^o] = H \\mathbb{E}[e^f] + \\mathbb{E}[e^o]\n$$\nThe problem states that both the forecast error and the observation error are unbiased, meaning $\\mathbb{E}[e^f] = 0$ and $\\mathbb{E}[e^o] = 0$. Therefore, the expected value of the innovation is:\n$$\n\\mathbb{E}[d] \\approx H(0) + 0 = 0\n$$\nUnder the given assumptions, the innovation has an expected value of zero.\n\nNext, we derive the covariance of the innovation, which we denote as $S$. The covariance is defined as $S = \\text{Cov}(d) = \\mathbb{E}[(d - \\mathbb{E}[d])(d - \\mathbb{E}[d])^T]$. Since $\\mathbb{E}[d] \\approx 0$, this simplifies to $S \\approx \\mathbb{E}[d d^T]$.\n$$\nS \\approx \\mathbb{E}[(H e^f + e^o)(H e^f + e^o)^T]\n$$\nExpanding the term inside the expectation:\n$$\nS \\approx \\mathbb{E}[ (H e^f + e^o) ((e^f)^T H^T + (e^o)^T) ] = \\mathbb{E}[ H e^f (e^f)^T H^T + H e^f (e^o)^T + e^o (e^f)^T H^T + e^o (e^o)^T ]\n$$\nBy the linearity of expectation:\n$$\nS \\approx \\mathbb{E}[H e^f (e^f)^T H^T] + \\mathbb{E}[H e^f (e^o)^T] + \\mathbb{E}[e^o (e^f)^T H^T] + \\mathbb{E}[e^o (e^o)^T]\n$$\nWe can factor out the constant matrix $H$:\n$$\nS \\approx H \\mathbb{E}[e^f (e^f)^T] H^T + H \\mathbb{E}[e^f (e^o)^T] + \\mathbb{E}[e^o (e^f)^T] H^T + \\mathbb{E}[e^o (e^o)^T]\n$$\nThe problem provides the covariance matrices $\\mathcal{P}^f = \\mathbb{E}[e^f (e^f)^T]$ and $R = \\mathbb{E}[e^o (e^o)^T]$. It also states that the forecast and observation errors are independent. This implies that their cross-covariance is zero: $\\mathbb{E}[e^f (e^o)^T] = \\mathbb{E}[e^f]\\mathbb{E}[(e^o)^T] = 0 \\cdot 0^T = 0$. The term $\\mathbb{E}[e^o (e^f)^T]$ is the transpose of this and is also a zero matrix.\nSubstituting these into the expression for $S$:\n$$\nS \\approx H \\mathcal{P}^f H^T + R\n$$\nThis is the theoretical covariance of the innovation vector.\n\nTo propose a test for consistency, we use the fact that $e^f$ and $e^o$ are stated to be Gaussian. Since $d \\approx H e^f + e^o$ is a linear combination of Gaussian random vectors, $d$ itself is approximately Gaussian, with mean $0$ and covariance $S$. That is, $d \\sim \\mathcal{N}(0, S)$. For a random vector $z \\in \\mathbb{R}^m$ following a multivariate normal distribution $\\mathcal{N}(0, \\Sigma)$, the quadratic form $z^T \\Sigma^{-1} z$ follows a chi-square distribution with $m$ degrees of freedom, denoted $\\chi^2_m$.\nTherefore, we can construct the test statistic $\\chi^2 = d^T S^{-1} d$. Under the null hypothesis that the specified covariances $\\mathcal{P}^f$ and $R$ are correct, this statistic should follow a $\\chi^2_m$ distribution, where $m$ is the dimension of the observation space. An observed $\\chi^2$ value that is improbably large (e.g., in the upper tail of the $\\chi^2_m$ distribution) suggests that the underlying assumptions about $\\mathcal{P}^f$ and $R$ are inconsistent with the observed innovation.\n\nNow, we apply these results to the given numerical case.\nThe observation vector is $y = \\begin{pmatrix} 289.5 \\\\ 1.1 \\end{pmatrix}$ and the mapped forecast is $H x^f = \\begin{pmatrix} 290 \\\\ 0.8 \\end{pmatrix}$.\nThe realized innovation $d$ is:\n$$\nd = y - Hx^f = \\begin{pmatrix} 289.5 - 290 \\\\ 1.1 - 0.8 \\end{pmatrix} = \\begin{pmatrix} -0.5 \\\\ 0.3 \\end{pmatrix}\n$$\nNext, we compute the theoretical innovation covariance $S = H \\mathcal{P}^f H^T + R$.\nGiven $H = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}$ and $\\mathcal{P}^{f} = \\begin{pmatrix} 0.25 & 0.05 & 0.03 \\\\ 0.05 & 1.00 & 0.02 \\\\ 0.03 & 0.02 & 0.16 \\end{pmatrix}$.\n$$\nH \\mathcal{P}^f = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} 0.25 & 0.05 & 0.03 \\\\ 0.05 & 1.00 & 0.02 \\\\ 0.03 & 0.02 & 0.16 \\end{pmatrix} = \\begin{pmatrix} 0.25 & 0.05 & 0.03 \\\\ 0.03 & 0.02 & 0.16 \\end{pmatrix}\n$$\n$$\nH \\mathcal{P}^f H^T = \\begin{pmatrix} 0.25 & 0.05 & 0.03 \\\\ 0.03 & 0.02 & 0.16 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 0.25 & 0.03 \\\\ 0.03 & 0.16 \\end{pmatrix}\n$$\nNow we add the observation error covariance $R = \\begin{pmatrix} 0.09 & 0.012 \\\\ 0.012 & 0.04 \\end{pmatrix}$:\n$$\nS = H \\mathcal{P}^f H^T + R = \\begin{pmatrix} 0.25 & 0.03 \\\\ 0.03 & 0.16 \\end{pmatrix} + \\begin{pmatrix} 0.09 & 0.012 \\\\ 0.012 & 0.04 \\end{pmatrix} = \\begin{pmatrix} 0.34 & 0.042 \\\\ 0.042 & 0.20 \\end{pmatrix}\n$$\nTo compute the $\\chi^2$ statistic, we need the inverse of $S$. For a $2 \\times 2$ matrix $\\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}$, the inverse is $\\frac{1}{ad-bc} \\begin{pmatrix} d & -b \\\\ -c & a \\end{pmatrix}$.\nThe determinant of $S$ is:\n$$\n\\det(S) = (0.34)(0.20) - (0.042)^2 = 0.068 - 0.001764 = 0.066236\n$$\nThe inverse $S^{-1}$ is:\n$$\nS^{-1} = \\frac{1}{0.066236} \\begin{pmatrix} 0.20 & -0.042 \\\\ -0.042 & 0.34 \\end{pmatrix}\n$$\nFinally, we compute the test statistic $\\chi^2 = d^T S^{-1} d$:\n$$\n\\chi^2 = \\begin{pmatrix} -0.5 & 0.3 \\end{pmatrix} \\frac{1}{0.066236} \\begin{pmatrix} 0.20 & -0.042 \\\\ -0.042 & 0.34 \\end{pmatrix} \\begin{pmatrix} -0.5 \\\\ 0.3 \\end{pmatrix}\n$$\nThe quadratic form in the numerator is:\n$$\n\\begin{pmatrix} -0.5 & 0.3 \\end{pmatrix} \\begin{pmatrix} 0.20 & -0.042 \\\\ -0.042 & 0.34 \\end{pmatrix} \\begin{pmatrix} -0.5 \\\\ 0.3 \\end{pmatrix} = \\begin{pmatrix} -0.5 & 0.3 \\end{pmatrix} \\begin{pmatrix} (0.20)(-0.5) + (-0.042)(0.3) \\\\ (-0.042)(-0.5) + (0.34)(0.3) \\end{pmatrix}\n$$\n$$\n= \\begin{pmatrix} -0.5 & 0.3 \\end{pmatrix} \\begin{pmatrix} -0.1 - 0.0126 \\\\ 0.021 + 0.102 \\end{pmatrix} = \\begin{pmatrix} -0.5 & 0.3 \\end{pmatrix} \\begin{pmatrix} -0.1126 \\\\ 0.123 \\end{pmatrix}\n$$\n$$\n= (-0.5)(-0.1126) + (0.3)(0.123) = 0.0563 + 0.0369 = 0.0932\n$$\nDividing by the determinant gives the $\\chi^2$ value:\n$$\n\\chi^2 = \\frac{0.0932}{0.066236} \\approx 1.4069925...\n$$\nRounding to four significant figures, the test statistic is $1.407$. This value would be compared to the $\\chi^2_2$ distribution to assess consistency.",
            "answer": "$$\\boxed{1.407}$$"
        },
        {
            "introduction": "This final practice integrates the concepts of linearization and analysis into a complete, end-to-end implementation of an Extended Kalman Filter. You will apply the EKF to the Lorenz-96 model, a widely used \"toy model\" that captures key features of atmospheric chaos, using nonlinear observations. This comprehensive exercise will solidify your understanding of the full forecast-analysis cycle, including the use of numerical Jacobians and robust covariance updating techniques .",
            "id": "4057058",
            "problem": "Consider a discrete-time data assimilation problem for the Lorenz–$96$ (L$96$) model, using the Extended Kalman Filter (EKF). The Lorenz–$96$ model has a state vector $\\mathbf{x} \\in \\mathbb{R}^{K}$ with components $x_{i}$ for $i \\in \\{0,\\dots,K-1\\}$ satisfying the cyclic indexing rule $x_{i+K} = x_{i}$. The continuous-time dynamics are defined by the ordinary differential equation\n$$\n\\frac{dx_{i}}{dt} = \\left(x_{i+1} - x_{i-2}\\right) x_{i-1} - x_{i} + F,\n$$\nwhere $F$ is a constant forcing and $K$ is the system dimension. Assume the time-discrete forecast model is obtained by numerically integrating this ordinary differential equation for one step of length $dt$ using a fourth-order Runge–Kutta method, thereby defining a discrete mapping $\\mathbf{M} : \\mathbb{R}^{K} \\to \\mathbb{R}^{K}$ such that $\\mathbf{x}_{k+1} = \\mathbf{M}(\\mathbf{x}_{k})$. Observations are obtained via a nonlinear observation operator $\\mathbf{h} : \\mathbb{R}^{K} \\to \\mathbb{R}^{m}$ that maps the state vector into observed space for a given index set of observed components. Assume additive Gaussian process and observation noise with covariances $\\mathbf{Q}$ and $\\mathbf{R}$, respectively.\n\nYou must implement an Extended Kalman Filter analysis at one assimilation time, starting from a prior (background) mean $\\mathbf{x}_{b}$ and covariance $\\mathbf{P}_{b}$, performing a single forecast step via $\\mathbf{M}$ to obtain the forecast mean $\\mathbf{x}_{f}$, and then assimilating a given observation vector $\\mathbf{y}$ to produce an analysis mean $\\mathbf{x}_{a}$ and analysis covariance $\\mathbf{P}_{a}$. In the EKF, linearizations must be computed by numerical Jacobians using central finite differences. Specifically:\n- The state transition Jacobian $\\mathbf{F} = \\left.\\frac{\\partial \\mathbf{M}}{\\partial \\mathbf{x}}\\right|_{\\mathbf{x}_{b}}$ must be approximated numerically by perturbing each component of $\\mathbf{x}_{b}$ by a small magnitude $\\varepsilon$ and using a central difference scheme.\n- The observation Jacobian $\\mathbf{H} = \\left.\\frac{\\partial \\mathbf{h}}{\\partial \\mathbf{x}}\\right|_{\\mathbf{x}_{f}}$ must likewise be approximated numerically by central finite differences.\n\nIn this problem, you must:\n1. Implement the Lorenz–$96$ right-hand side and a fourth-order Runge–Kutta one-step integrator to define $\\mathbf{M}$.\n2. Implement numerical Jacobians for both $\\mathbf{M}$ and $\\mathbf{h}$ using central finite differences with a small perturbation magnitude $\\varepsilon$.\n3. Propagate the prior covariance through the forecast step using the linearized dynamics to obtain the forecast covariance.\n4. Perform the EKF analysis update with a numerically computed observation Jacobian and the Joseph-stabilized covariance update.\n5. Evaluate the posterior error covariance by computing three scalar diagnostics: the trace of $\\mathbf{P}_{a}$, the largest eigenvalue of $\\mathbf{P}_{a}$, and the Frobenius norm of $\\mathbf{P}_{a}$.\n\nBase your derivation and implementation on the following fundamental principles:\n- The Lorenz–$96$ dynamical system definition and numerical time integration as a well-tested approximation to the continuous-time dynamics,\n- Gaussian uncertainty propagation under linearization of nonlinear mappings,\n- Bayes’ rule for combining prior information with likelihoods, and its Gaussian specialization leading to Kalman-type updates.\n\nUse the following test suite of parameter values. Indices are counted from $0$.\n\nTest case $1$ (happy path, partial nonlinear sinusoidal observations, moderate noise):\n- Dimension $K = 5$, forcing $F = 8$, time step $dt = 0.05$,\n- Prior mean $\\mathbf{x}_{b} = [\\,1.5,\\,1.2,\\,1.3,\\,1.4,\\,1.6\\,]$,\n- Prior covariance $\\mathbf{P}_{b} = \\operatorname{diag}([\\,0.25,\\,0.25,\\,0.25,\\,0.25,\\,0.25\\,])$,\n- Process noise covariance $\\mathbf{Q} = \\operatorname{diag}([\\,0.01,\\,0.01,\\,0.01,\\,0.01,\\,0.01\\,])$,\n- Observation operator $\\mathbf{h}$ defined on observed indices $[\\,0,\\,2,\\,4\\,]$ as element-wise $\\sin(\\cdot)$ on those components, so $m = 3$,\n- Observation noise covariance $\\mathbf{R} = \\operatorname{diag}([\\,0.0025,\\,0.0025,\\,0.0025\\,])$,\n- True state $\\mathbf{x}_{\\text{true}} = [\\,1.6,\\,1.4,\\,1.5,\\,1.7,\\,1.8\\,]$, and observations $\\mathbf{y} = \\mathbf{h}(\\mathbf{x}_{\\text{true}})$.\n\nTest case $2$ (boundary case, quadratic observations on all components, zero observation noise, zero process noise):\n- Dimension $K = 5$, forcing $F = 8$, time step $dt = 0.05$,\n- Prior mean $\\mathbf{x}_{b} = [\\,1.0,\\,0.0,\\,0.5,\\,-0.5,\\,1.0\\,]$,\n- Prior covariance $\\mathbf{P}_{b} = \\operatorname{diag}([\\,0.01,\\,0.01,\\,0.01,\\,0.01,\\,0.01\\,])$,\n- Process noise covariance $\\mathbf{Q} = \\operatorname{diag}([\\,0,\\,0,\\,0,\\,0,\\,0\\,])$,\n- Observation operator $\\mathbf{h}$ defined on observed indices $[\\,0,\\,1,\\,2,\\,3,\\,4\\,]$ as element-wise $(\\cdot)^{2}$ on those components, so $m = 5$,\n- Observation noise covariance $\\mathbf{R} = \\operatorname{diag}([\\,0,\\,0,\\,0,\\,0,\\,0\\,])$,\n- True state $\\mathbf{x}_{\\text{true}} = [\\,1.0,\\,0.5,\\,-0.2,\\,0.8,\\,-1.1\\,]$, and observations $\\mathbf{y} = \\mathbf{h}(\\mathbf{x}_{\\text{true}})$.\n\nTest case $3$ (edge case, higher dimension, sparse single-component sinusoidal observation, relatively large observation noise, anisotropic prior covariance):\n- Dimension $K = 7$, forcing $F = 8$, time step $dt = 0.05$,\n- Prior mean $\\mathbf{x}_{b} = [\\,0.5,\\,-0.3,\\,1.0,\\,0.2,\\,-0.7,\\,0.8,\\,1.2\\,]$,\n- Prior covariance $\\mathbf{P}_{b} = \\operatorname{diag}([\\,0.04,\\,0.09,\\,0.16,\\,0.01,\\,0.25,\\,0.09,\\,0.04\\,])$,\n- Process noise covariance $\\mathbf{Q} = \\operatorname{diag}([\\,0.02,\\,0.02,\\,0.02,\\,0.02,\\,0.02,\\,0.02,\\,0.02\\,])$,\n- Observation operator $\\mathbf{h}$ defined on observed indices $[\\,3\\,]$ as element-wise $\\sin(\\cdot)$ on that single component, so $m = 1$,\n- Observation noise covariance $\\mathbf{R} = \\operatorname{diag}([\\,0.25\\,])$,\n- True state $\\mathbf{x}_{\\text{true}} = [\\,0.7,\\,-0.1,\\,0.9,\\,0.3,\\,-0.5,\\,1.1,\\,1.4\\,]$, and observation $\\mathbf{y} = \\mathbf{h}(\\mathbf{x}_{\\text{true}})$.\n\nYour program must:\n- Use a central finite difference step $\\varepsilon = 10^{-6}$ for all numerical Jacobians,\n- Use the Joseph-stabilized covariance update to compute $\\mathbf{P}_{a}$,\n- Symmetrize the final $\\mathbf{P}_{a}$ as $\\tfrac{1}{2}\\left(\\mathbf{P}_{a} + \\mathbf{P}_{a}^{\\top}\\right)$ before computing diagnostics.\n\nThe required final output format is a single line containing a list of lists with three floats per test case, ordered as $[\\,\\operatorname{tr}(\\mathbf{P}_{a}),\\,\\lambda_{\\max}(\\mathbf{P}_{a}),\\,\\|\\mathbf{P}_{a}\\|_{F}\\,]$, aggregated for all test cases, in the exact string form\n$$\n[\\,[\\text{t}_{1},\\text{l}_{1},\\text{n}_{1}],\\,[\\text{t}_{2},\\text{l}_{2},\\text{n}_{2}],\\,[\\text{t}_{3},\\text{l}_{3},\\text{n}_{3}]\\,],\n$$\nwith no additional text. All quantities are unitless real numbers. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[\\,\\text{result}_{1},\\text{result}_{2},\\text{result}_{3}\\,]$).",
            "solution": "The problem requires the implementation of a single Extended Kalman Filter (EKF) forecast-analysis cycle for the Lorenz–$96$ dynamical system. The validation of the problem statement indicates that it is scientifically grounded, well-posed, objective, and contains all necessary information to proceed. The problem is a standard application of data assimilation techniques to a well-known chaotic model. We will now derive the solution by constructing the necessary components of the EKF algorithm.\n\nThe EKF is an adaptation of the Kalman filter for nonlinear systems, which operates in two steps: a forecast step to propagate the state and its uncertainty forward in time, and an analysis step to update the forecasted state with new observations.\n\n### 1. Forecast Step\n\nThe forecast step advances the prior (or background) state estimate $\\mathbf{x}_{b} \\in \\mathbb{R}^{K}$ and its associated error covariance matrix $\\mathbf{P}_{b} \\in \\mathbb{R}^{K \\times K}$ to the time of the next observation.\n\n**State Forecast:**\nThe prior mean state $\\mathbf{x}_{b}$ is propagated through the nonlinear forecast model $\\mathbf{M}$ to obtain the forecast mean state $\\mathbf{x}_{f}$:\n$$\n\\mathbf{x}_{f} = \\mathbf{M}(\\mathbf{x}_{b})\n$$\nThe model $\\mathbf{M}$ is defined as a single time step of length $dt$ using a fourth-order Runge–Kutta (RK4) method to integrate the Lorenz–$96$ ordinary differential equations:\n$$\n\\frac{dx_{i}}{dt} = f_i(\\mathbf{x}) = (x_{i+1} - x_{i-2}) x_{i-1} - x_{i} + F\n$$\nfor $i=0, \\dots, K-1$, with cyclic boundary conditions (e.g., $x_{-1} = x_{K-1}$, $x_{-2} = x_{K-2}$). The RK4 scheme for a general system $\\frac{d\\mathbf{x}}{dt} = f(\\mathbf{x})$ is:\n$$\n\\begin{align*}\n\\mathbf{k}_{1} &= f(\\mathbf{x}(t)) \\\\\n\\mathbf{k}_{2} &= f(\\mathbf{x}(t) + \\frac{dt}{2}\\mathbf{k}_{1}) \\\\\n\\mathbf{k}_{3} &= f(\\mathbf{x}(t) + \\frac{dt}{2}\\mathbf{k}_{2}) \\\\\n\\mathbf{k}_{4} &= f(\\mathbf{x}(t) + dt\\mathbf{k}_{3}) \\\\\n\\mathbf{x}(t+dt) &= \\mathbf{x}(t) + \\frac{dt}{6}(\\mathbf{k}_{1} + 2\\mathbf{k}_{2} + 2\\mathbf{k}_{3} + \\mathbf{k}_{4})\n\\end{align*}\n$$\nApplying this to the Lorenz–$96$ dynamics at state $\\mathbf{x}_b$ yields $\\mathbf{x}_f$.\n\n**Covariance Forecast:**\nThe prior covariance $\\mathbf{P}_{b}$ is propagated by linearizing the model dynamics around the prior mean $\\mathbf{x}_{b}$. The forecast covariance $\\mathbf{P}_{f}$ is given by:\n$$\n\\mathbf{P}_{f} = \\mathbf{F} \\mathbf{P}_{b} \\mathbf{F}^{\\top} + \\mathbf{Q}\n$$\nwhere $\\mathbf{Q}$ is the process noise covariance and $\\mathbf{F}$ is the tangent linear model, or the Jacobian of $\\mathbf{M}$ evaluated at $\\mathbf{x}_{b}$:\n$$\n\\mathbf{F} = \\left.\\frac{\\partial \\mathbf{M}}{\\partial \\mathbf{x}}\\right|_{\\mathbf{x}_{b}}\n$$\nThis Jacobian is computed numerically using a central finite difference scheme. For each column $j$ of $\\mathbf{F}$, we perturb the $j$-th component of $\\mathbf{x}_{b}$:\n$$\n\\mathbf{F}_{:, j} = \\frac{\\mathbf{M}(\\mathbf{x}_{b} + \\varepsilon \\mathbf{e}_{j}) - \\mathbf{M}(\\mathbf{x}_{b} - \\varepsilon \\mathbf{e}_{j})}{2\\varepsilon}\n$$\nwhere $\\mathbf{e}_{j}$ is the $j$-th standard basis vector and $\\varepsilon$ is a small perturbation, given as $\\varepsilon = 10^{-6}$.\n\n### 2. Analysis Step\n\nThe analysis step combines the forecast state $\\mathbf{x}_{f}$ and its covariance $\\mathbf{P}_{f}$ with an observation vector $\\mathbf{y} \\in \\mathbb{R}^{m}$ to produce an improved analysis (posterior) state $\\mathbf{x}_{a}$ and covariance $\\mathbf{P}_{a}$.\n\n**Innovation:**\nThe innovation or observation residual, $\\mathbf{d}$, is the difference between the actual observation $\\mathbf{y}$ and the observation predicted from the forecast state:\n$$\n\\mathbf{d} = \\mathbf{y} - \\mathbf{h}(\\mathbf{x}_{f})\n$$\nwhere $\\mathbf{h}: \\mathbb{R}^{K} \\to \\mathbb{R}^{m}$ is the (possibly nonlinear) observation operator. The observations are given by $\\mathbf{y} = \\mathbf{h}(\\mathbf{x}_{\\text{true}})$.\n\n**Observation Jacobian:**\nThe update requires the linearization of the observation operator, $\\mathbf{H}$, which is its Jacobian evaluated at the forecast state $\\mathbf{x}_{f}$:\n$$\n\\mathbf{H} = \\left.\\frac{\\partial \\mathbf{h}}{\\partial \\mathbf{x}}\\right|_{\\mathbf{x}_{f}}\n$$\nAs with $\\mathbf{F}$, this $m \\times K$ matrix is computed numerically using central finite differences:\n$$\n\\mathbf{H}_{:, j} = \\frac{\\mathbf{h}(\\mathbf{x}_{f} + \\varepsilon \\mathbf{e}_{j}) - \\mathbf{h}(\\mathbf{x}_{f} - \\varepsilon \\mathbf{e}_{j})}{2\\varepsilon}\n$$\n\n**Kalman Gain and State/Covariance Update:**\nThe analysis is a weighted average of the forecast and the observation, with the weights determined by their respective uncertainties. The innovation covariance is:\n$$\n\\mathbf{S} = \\mathbf{H} \\mathbf{P}_{f} \\mathbf{H}^{\\top} + \\mathbf{R}\n$$\nwhere $\\mathbf{R}$ is the observation error covariance. The Kalman gain $\\mathbf{K}$, which determines the weight given to the innovation, is:\n$$\n\\mathbf{K} = \\mathbf{P}_{f} \\mathbf{H}^{\\top} \\mathbf{S}^{-1}\n$$\nNumerically, it is more stable to compute $\\mathbf{K}$ by solving the linear system $\\mathbf{S}\\mathbf{K}^{\\top} = \\mathbf{H}\\mathbf{P}_{f}$ for $\\mathbf{K}^{\\top}$ and then transposing the result.\n\nThe analysis mean state $\\mathbf{x}_{a}$ and covariance $\\mathbf{P}_{a}$ are then updated:\n$$\n\\mathbf{x}_{a} = \\mathbf{x}_{f} + \\mathbf{K} \\mathbf{d}\n$$\nFor the covariance update, we use the Joseph-stabilized form, which is numerically more robust and guaranteed to produce a symmetric positive semi-definite matrix, even in the presence of floating-point errors:\n$$\n\\mathbf{P}_{a} = (\\mathbf{I} - \\mathbf{K}\\mathbf{H}) \\mathbf{P}_{f} (\\mathbf{I} - \\mathbf{K}\\mathbf{H})^{\\top} + \\mathbf{K}\\mathbf{R}\\mathbf{K}^{\\top}\n$$\nwhere $\\mathbf{I}$ is the identity matrix.\n\n### 3. Final Diagnostics\n\nAfter computing the analysis covariance $\\mathbf{P}_{a}$, it is first explicitly symmetrized to remove any numerical asymmetry:\n$$\n\\mathbf{P}_{a} \\leftarrow \\frac{1}{2}(\\mathbf{P}_{a} + \\mathbf{P}_{a}^{\\top})\n$$\nFinally, three scalar diagnostics are computed to summarize the posterior error covariance:\n1.  **Trace:** $\\operatorname{tr}(\\mathbf{P}_{a}) = \\sum_{i} (\\mathbf{P}_{a})_{ii}$. This represents the sum of the posterior error variances.\n2.  **Largest Eigenvalue:** $\\lambda_{\\max}(\\mathbf{P}_{a})$. This measures the magnitude of the largest principal component of the error distribution.\n3.  **Frobenius Norm:** $\\|\\mathbf{P}_{a}\\|_{F} = \\sqrt{\\sum_{i,j} |(\\mathbf{P}_{a})_{ij}|^2}$. This provides an overall measure of the size of the covariance matrix.\n\nThe implementation will follow these steps for each of the three test cases provided.",
            "answer": "```python\nimport numpy as np\nimport scipy.linalg\n\ndef solve():\n    \"\"\"\n    Implements and runs the Extended Kalman Filter for the Lorenz-96 model\n    for three specified test cases.\n    \"\"\"\n\n    test_cases = [\n        # Test case 1\n        {\n            \"K\": 5, \"F\": 8.0, \"dt\": 0.05,\n            \"xb\": np.array([1.5, 1.2, 1.3, 1.4, 1.6]),\n            \"Pb\": np.diag([0.25, 0.25, 0.25, 0.25, 0.25]),\n            \"Q\": np.diag([0.01, 0.01, 0.01, 0.01, 0.01]),\n            \"obs_indices\": np.array([0, 2, 4]),\n            \"obs_op\": np.sin,\n            \"R\": np.diag([0.0025, 0.0025, 0.0025]),\n            \"xtrue\": np.array([1.6, 1.4, 1.5, 1.7, 1.8]),\n        },\n        # Test case 2\n        {\n            \"K\": 5, \"F\": 8.0, \"dt\": 0.05,\n            \"xb\": np.array([1.0, 0.0, 0.5, -0.5, 1.0]),\n            \"Pb\": np.diag([0.01, 0.01, 0.01, 0.01, 0.01]),\n            \"Q\": np.diag([0.0, 0.0, 0.0, 0.0, 0.0]),\n            \"obs_indices\": np.array([0, 1, 2, 3, 4]),\n            \"obs_op\": lambda x: x**2,\n            \"R\": np.diag([0.0, 0.0, 0.0, 0.0, 0.0]),\n            \"xtrue\": np.array([1.0, 0.5, -0.2, 0.8, -1.1]),\n        },\n        # Test case 3\n        {\n            \"K\": 7, \"F\": 8.0, \"dt\": 0.05,\n            \"xb\": np.array([0.5, -0.3, 1.0, 0.2, -0.7, 0.8, 1.2]),\n            \"Pb\": np.diag([0.04, 0.09, 0.16, 0.01, 0.25, 0.09, 0.04]),\n            \"Q\": np.diag([0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02]),\n            \"obs_indices\": np.array([3]),\n            \"obs_op\": np.sin,\n            \"R\": np.diag([0.25]),\n            \"xtrue\": np.array([0.7, -0.1, 0.9, 0.3, -0.5, 1.1, 1.4]),\n        },\n    ]\n\n    epsilon = 1e-6\n    results = []\n\n    def lorenz96_rhs(x, F):\n        \"\"\"Lorenz-96 RHS with cyclic boundary conditions.\"\"\"\n        K = x.shape[0]\n        dxdt = np.zeros(K)\n        # Using np.roll for efficient cyclic indexing\n        x_p1 = np.roll(x, -1)\n        x_m1 = np.roll(x, 1)\n        x_m2 = np.roll(x, 2)\n        dxdt = (x_p1 - x_m2) * x_m1 - x + F\n        return dxdt\n\n    def rk4_step(x, func_rhs, dt, F):\n        \"\"\"Fourth-order Runge-Kutta integrator for one step.\"\"\"\n        k1 = func_rhs(x, F)\n        k2 = func_rhs(x + 0.5 * dt * k1, F)\n        k3 = func_rhs(x + 0.5 * dt * k2, F)\n        k4 = func_rhs(x + dt * k3, F)\n        return x + (dt / 6.0) * (k1 + 2.0 * k2 + 2.0 * k3 + k4)\n\n    def compute_jacobian(func, x, epsilon):\n        \"\"\"Computes Jacobian of func at x using central finite differences.\"\"\"\n        K = x.shape[0]\n        y_ref = func(x)\n        m = y_ref.shape[0]\n        J = np.zeros((m, K))\n        x_pert = np.copy(x)\n        for j in range(K):\n            # Perturb x_pert in place, which is slightly more efficient\n            original_val = x_pert[j]\n            x_pert[j] = original_val + epsilon\n            y_plus = func(x_pert)\n            x_pert[j] = original_val - epsilon\n            y_minus = func(x_pert)\n            J[:, j] = (y_plus - y_minus) / (2.0 * epsilon)\n            # Restore original value\n            x_pert[j] = original_val\n        return J\n\n    for case in test_cases:\n        K = case[\"K\"]\n        F = case[\"F\"]\n        dt = case[\"dt\"]\n        xb = case[\"xb\"]\n        Pb = case[\"Pb\"]\n        Q = case[\"Q\"]\n        obs_indices = case[\"obs_indices\"]\n        obs_op = case[\"obs_op\"]\n        R = case[\"R\"]\n        xtrue = case[\"xtrue\"]\n        \n        # Define the specific forecast model M and observation operator h\n        M = lambda x: rk4_step(x, lorenz96_rhs, dt, F)\n        h = lambda x: obs_op(x[obs_indices])\n        \n        # Generate observation y\n        y = h(xtrue)\n        \n        # --- FORECAST STEP ---\n        # State forecast\n        xf = M(xb)\n        \n        # Covariance forecast\n        F_jac = compute_jacobian(M, xb, epsilon)\n        Pf = F_jac @ Pb @ F_jac.T + Q\n        \n        # --- ANALYSIS STEP ---\n        # Innovation\n        d = y - h(xf)\n        \n        # Observation Jacobian\n        H_jac = compute_jacobian(h, xf, epsilon)\n        \n        # Innovation covariance\n        S = H_jac @ Pf @ H_jac.T + R\n        \n        # Kalman Gain (using numerically stable linear solve)\n        # Solve S K.T = H Pf\n        KT = scipy.linalg.solve(S, H_jac @ Pf, assume_a='sym')\n        K_gain = KT.T\n        \n        # State analysis\n        # xa = xf + K_gain @ d  # Not required for the final output\n\n        # Covariance analysis (Joseph form)\n        I = np.identity(K)\n        term1 = I - K_gain @ H_jac\n        Pa = term1 @ Pf @ term1.T + K_gain @ R @ K_gain.T\n        \n        # --- POST-PROCESSING & DIAGNOSTICS ---\n        # Symmetrize Pa to correct for numerical inaccuracies\n        Pa_sym = 0.5 * (Pa + Pa.T)\n        \n        # Calculate diagnostics\n        tr_Pa = np.trace(Pa_sym)\n        # Use eigvalsh for symmetric matrices\n        eigvals = np.linalg.eigvalsh(Pa_sym)\n        lmax_Pa = np.max(eigvals)\n        fnorm_Pa = np.linalg.norm(Pa_sym, 'fro')\n        \n        results.append([tr_Pa, lmax_Pa, fnorm_Pa])\n\n    # Format the final output string exactly as specified.\n    # str(results) produces spaces, which need to be removed.\n    output_str = str(results).replace(\" \", \"\")\n    print(output_str)\n\nsolve()\n```"
        }
    ]
}