{
    "hands_on_practices": [
        {
            "introduction": "本练习旨在通过一个基础的“纸笔”计算，巩固三维变分（3D-Var）数据同化的核心思想。您将为一个低维系统推导并求解最优分析状态，此过程将帮助您深入理解代价函数的构建，以及如何通过最小化代价函数来融合背景场和观测信息。这个练习是掌握变分同化数学基础的关键一步。",
            "id": "3864622",
            "problem": "考虑一个双分量环境状态向量 $x \\in \\mathbb{R}^{2}$，它表示从线性设置下的单个卫星观测中要分析的空间聚合量。假设一个源于高斯误差统计和线性观测算子的三维变分（3D-Var）数据同化框架，其中分析场 $x^{a}$ 通过最小化由背景和观测失配构建的二次代价函数得到。设背景误差协方差为 $B=\\begin{bmatrix}1  0 \\\\ 0  4\\end{bmatrix}$，观测算子为 $H=\\begin{bmatrix}1  1\\end{bmatrix}$，观测误差协方差为 $R=1$，背景场（也称先验）为 $x_{b}=\\begin{bmatrix}0 \\\\ 0\\end{bmatrix}$，观测为 $y=3$。\n\n从在线性高斯假设下结合了背景和观测失配的 3D-Var 代价函数的定义出发，推导表征其最小值的一阶最优性条件（正规方程）。然后，使用给定的数值，计算分析增量 $\\delta x^{a}=x^{a}-x_{b}$ 和分析场 $x^{a}$。将您的最终结果表示为 $\\big[\\delta x^{a}_{1},\\,\\delta x^{a}_{2},\\,x^{a}_{1},\\,x^{a}_{2}\\big]$ 顺序的单行矩阵。\n\n无需单位。提供精确值；无需四舍五入。",
            "solution": "首先对问题陈述进行严格的验证过程。\n\n### 第一步：提取已知条件\n-   状态向量：$x \\in \\mathbb{R}^{2}$\n-   背景误差协方差：$B=\\begin{bmatrix}1  0 \\\\ 0  4\\end{bmatrix}$\n-   观测算子：$H=\\begin{bmatrix}1  1\\end{bmatrix}$\n-   观测误差协方差：$R=1$\n-   背景场：$x_{b}=\\begin{bmatrix}0 \\\\ 0\\end{bmatrix}$\n-   观测：$y=3$\n-   任务：推导正规方程，然后计算分析增量 $\\delta x^{a}=x^{a}-x_{b}$ 和分析场 $x^{a}$。\n-   最终输出格式：单行矩阵 $\\big[\\delta x^{a}_{1},\\,\\delta x^{a}_{2},\\,x^{a}_{1},\\,x^{a}_{2}\\big]$。\n\n### 第二步：使用提取的已知条件进行验证\n根据验证标准对问题进行评估。\n-   **科学性：** 该问题描述了一个标准的三维变分（3D-Var）数据同化场景。二次代价函数、背景/观测误差协方差以及线性观测算子的使用是这种在遥感和环境建模中已确立的科学方法的典型要素。该框架是在线性高斯假设下从贝叶斯定理推导出来的。该问题在科学上是合理的。\n-   **良态性：** 该问题是求解一个二次代价函数的最小值。背景误差协方差矩阵 $B$ 的特征值为 $1$ 和 $4$，均为正数，因此 $B$ 是正定的，从而可逆。观测误差协方差 $R=1$ 是一个正标量。代价函数的Hessian矩阵 $(B^{-1} + H^T R^{-1} H)$ 将被证明是正定的，这保证了唯一、稳定最小值的存在。所有必要信息都已提供，且无矛盾之处。该问题是良态的。\n-   **客观性：** 该问题使用精确的数学语言和定义（$x_b, B, H, R, y$）进行陈述。没有主观或模糊的术语。该问题是客观的。\n-   **其他缺陷：** 该问题并非微不足道，因为它需要推导和矩阵代数。对于教科书中的例子来说，它不是隐喻性的、不完整的或不切实际的。\n\n### 第三步：结论和行动\n问题有效。将提供解答。\n\n3D-Var 分析场 $x^a$ 是使代价函数 $J(x)$ 最小化的状态向量 $x$。该代价函数衡量了与背景场和观测的失配，并由它们各自的误差协方差加权。对于线性高斯系统，其表达式为：\n$$J(x) = \\frac{1}{2}(x - x_b)^T B^{-1} (x - x_b) + \\frac{1}{2}(y - Hx)^T R^{-1} (y - Hx)$$\n为求最小值，我们计算 $J(x)$ 关于 $x$ 的梯度，并将其设为零。梯度 $\\nabla_x J(x)$ 为：\n$$\\nabla_x J(x) = B^{-1}(x - x_b) - H^T R^{-1} (y - Hx)$$\n将 $x=x^a$ 处的梯度设为零，得到一阶最优性条件，也称为正规方程：\n$$B^{-1}(x^a - x_b) - H^T R^{-1} (y - Hx^a) = 0$$\n这就是所要求的正规方程的推导。\n\n我们被要求求解定义为 $\\delta x^a = x^a - x_b$ 的分析增量。我们将 $x^a = x_b + \\delta x^a$ 代入正规方程：\n$$B^{-1}(\\delta x^a) - H^T R^{-1} (y - H(x_b + \\delta x^a)) = 0$$\n$$B^{-1}(\\delta x^a) - H^T R^{-1} (y - Hx_b - H\\delta x^a) = 0$$\n重新整理各项以求解 $\\delta x^a$：\n$$B^{-1}(\\delta x^a) + H^T R^{-1} H\\delta x^a = H^T R^{-1} (y - Hx_b)$$\n$$(B^{-1} + H^T R^{-1} H) \\delta x^a = H^T R^{-1} (y - Hx_b)$$\n这个方程可以用来求解分析增量 $\\delta x^a$。\n\n现在，我们代入给定的数值：\n-   $x_b = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$\n-   $B = \\begin{bmatrix} 1  0 \\\\ 0  4 \\end{bmatrix} \\implies B^{-1} = \\begin{bmatrix} 1  0 \\\\ 0  \\frac{1}{4} \\end{bmatrix}$\n-   $H = \\begin{bmatrix} 1  1 \\end{bmatrix} \\implies H^T = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$\n-   $R = 1 \\implies R^{-1} = 1$\n-   $y = 3$\n\n首先，我们计算方程中求解 $\\delta x^a$ 的各个部分。\n项 $(y - Hx_b)$ 是新息：\n$$y - Hx_b = 3 - \\begin{bmatrix} 1  1 \\end{bmatrix} \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} = 3 - 0 = 3$$\n方程的右边是：\n$$H^T R^{-1} (y - Hx_b) = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} (1) (3) = \\begin{bmatrix} 3 \\\\ 3 \\end{bmatrix}$$\n方程左边的矩阵是代价函数的Hessian矩阵：\n$$B^{-1} + H^T R^{-1} H = \\begin{bmatrix} 1  0 \\\\ 0  \\frac{1}{4} \\end{bmatrix} + \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} (1) \\begin{bmatrix} 1  1 \\end{bmatrix}$$\n$$= \\begin{bmatrix} 1  0 \\\\ 0  \\frac{1}{4} \\end{bmatrix} + \\begin{bmatrix} 1  1 \\\\ 1  1 \\end{bmatrix} = \\begin{bmatrix} 2  1 \\\\ 1  1 + \\frac{1}{4} \\end{bmatrix} = \\begin{bmatrix} 2  1 \\\\ 1  \\frac{5}{4} \\end{bmatrix}$$\n为了求解 $\\delta x^a$，我们需要求这个矩阵的逆。其行列式为：\n$$\\det\\left(\\begin{bmatrix} 2  1 \\\\ 1  \\frac{5}{4} \\end{bmatrix}\\right) = (2)\\left(\\frac{5}{4}\\right) - (1)(1) = \\frac{5}{2} - 1 = \\frac{3}{2}$$\n逆矩阵是：\n$$\\begin{bmatrix} 2  1 \\\\ 1  \\frac{5}{4} \\end{bmatrix}^{-1} = \\frac{1}{\\frac{3}{2}} \\begin{bmatrix} \\frac{5}{4}  -1 \\\\ -1  2 \\end{bmatrix} = \\frac{2}{3} \\begin{bmatrix} \\frac{5}{4}  -1 \\\\ -1  2 \\end{bmatrix} = \\begin{bmatrix} \\frac{10}{12}  -\\frac{2}{3} \\\\ -\\frac{2}{3}  \\frac{4}{3} \\end{bmatrix} = \\begin{bmatrix} \\frac{5}{6}  -\\frac{2}{3} \\\\ -\\frac{2}{3}  \\frac{4}{3} \\end{bmatrix}$$\n现在我们可以求解 $\\delta x^a$：\n$$\\delta x^a = \\begin{bmatrix} \\frac{5}{6}  -\\frac{2}{3} \\\\ -\\frac{2}{3}  \\frac{4}{3} \\end{bmatrix} \\begin{bmatrix} 3 \\\\ 3 \\end{bmatrix} = \\begin{bmatrix} \\frac{5}{6}(3) - \\frac{2}{3}(3) \\\\ -\\frac{2}{3}(3) + \\frac{4}{3}(3) \\end{bmatrix} = \\begin{bmatrix} \\frac{5}{2} - 2 \\\\ -2 + 4 \\end{bmatrix} = \\begin{bmatrix} \\frac{1}{2} \\\\ 2 \\end{bmatrix}$$\n因此，分析增量的分量是 $\\delta x^a_1 = \\frac{1}{2}$ 和 $\\delta x^a_2 = 2$。\n\n最后，我们计算分析场 $x^a$：\n$$x^a = x_b + \\delta x^a = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} + \\begin{bmatrix} \\frac{1}{2} \\\\ 2 \\end{bmatrix} = \\begin{bmatrix} \\frac{1}{2} \\\\ 2 \\end{bmatrix}$$\n分析场的分量是 $x^a_1 = \\frac{1}{2}$ 和 $x^a_2 = 2$。\n\n最终结果要求以 $\\big[\\delta x^{a}_{1},\\,\\delta x^{a}_{2},\\,x^{a}_{1},\\,x^{a}_{2}\\big]$ 的顺序表示为单行矩阵。这给出：\n$$\\begin{pmatrix} \\frac{1}{2}  2  \\frac{1}{2}  2 \\end{pmatrix}$$",
            "answer": "$$\\boxed{\\begin{pmatrix} \\frac{1}{2}  2  \\frac{1}{2}  2 \\end{pmatrix}}$$"
        },
        {
            "introduction": "在掌握了静态的 3D-Var 框架后，我们将把变分原理扩展到时间维度，探索强约束四维变分（4D-Var）的基本机制。此练习通过一个简单的一维线性模型，演示如何通过优化系统的*初始状态*，使其在整个时间窗口内的演变轨迹能最佳地拟合所有可用的观测。这个过程揭示了 4D-Var 利用动力学模型连接不同时间点观测的核心优势。",
            "id": "3864672",
            "problem": "考虑一个简化的环境模型中的标量线性离散时间预报变量，其演变遵循完美模型（强约束）动力学 $x_{k+1} = 0.5\\, x_{k}$，其中 $k \\in \\{0,1,2\\}$。给定初始状态的背景（先验）估计为 $x_{b} = 0$，背景误差协方差为 $B = 1$。在每个时间 $k \\in \\{0,1,2\\}$，都有一个对状态的直接观测，其观测算子为 $H = 1$，观测误差协方差为 $R_{k} = 1$。观测值为 $y_{0} = 1$，$y_{1} = 0.5$ 和 $y_{2} = 0.25$。\n\n仅使用强约束情况下四维变分数据同化（4D-Var）代价函数的基本定义——即对偏离背景的二次惩罚项和对时间窗内模型预测的观测不符的二次惩罚项之和——将初始状态 $x_{0}$ 的估计问题表述为仅关于 $x_{0}$ 的最小化问题。从该定义推导出一阶最优性条件，并解析求解强约束四维变分数据同化（4D-Var）分析场 $x_{0}^{a}$。\n\n以单一最简分数的形式给出 $x_{0}^{a}$ 的精确值。不要进行近似或四舍五入。",
            "solution": "该问题要求通过最小化强约束四维变分数据同化（4D-Var）代价函数来确定最优初始状态，记为分析场 $x_{0}^{a}$。我们将首先根据其基本定义构建该代价函数，然后推导出一阶最优性条件，最后求解 $x_{0}^{a}$。\n\n对于初始状态 $x_0$，强约束4D-Var代价函数 $J$ 的基本定义是两个惩罚项之和：一个是对初始状态偏离先验（背景）估计进行惩罚的背景项，另一个是对时间窗内模型预测与观测之间的不符进行惩罚的观测项。对于离散时间系统，这表示为：\n$$J(x_{0}) = \\frac{1}{2} (x_{0} - x_{b})^T B^{-1} (x_{0} - x_{b}) + \\frac{1}{2} \\sum_{k=0}^{N} (H_{k} M_{k}(x_{0}) - y_{k})^T R_{k}^{-1} (H_{k} M_{k}(x_{0}) - y_{k})$$\n其中 $x_0$ 是待估计的初始状态向量，$x_b$ 是背景状态向量，$B$ 是背景误差协方差矩阵，$N$ 是同化窗口内的时间步数，$y_k$ 是时间 $k$ 的观测向量，$H_k$ 是将模型状态空间映射到时间 $k$ 的观测空间的观测算子，$R_k$ 是时间 $k$ 的观测误差协方差矩阵，而 $M_k(x_0)$ 表示初始状态 $x_0$ 通过预报模型演变到时间 $k$ 的状态。\n\n该问题提供了一个标量系统，因此所有矩阵和向量都变为标量。给定值为：\n- 背景状态：$x_{b} = 0$\n- 背景误差协方差：$B = 1$，所以 $B^{-1} = 1$\n- 观测算子：$H = 1$（直接观测），对所有 $k$ 成立\n- 观测误差协方差：$R_{k} = 1$，所以 $R_{k}^{-1} = 1$，对所有 $k$ 成立\n- 观测值：$y_{0} = 1$，$y_{1} = 0.5$，$y_{2} = 0.25$\n- 同化窗口：$k \\in \\{0,1,2\\}$，所以 $N=2$\n\n标量代价函数为：\n$$J(x_{0}) = \\frac{1}{2} B^{-1} (x_{0} - x_{b})^2 + \\frac{1}{2} \\sum_{k=0}^{2} R_{k}^{-1} (H x_{k} - y_{k})^2$$\n代入给定的协方差、背景状态和观测算子的数值：\n$$J(x_{0}) = \\frac{1}{2} (1) (x_{0} - 0)^2 + \\frac{1}{2} \\sum_{k=0}^{2} (1) ((1) x_{k} - y_{k})^2$$\n$$J(x_{0}) = \\frac{1}{2} x_{0}^2 + \\frac{1}{2} \\sum_{k=0}^{2} (x_{k} - y_{k})^2$$\n模型动力学由 $x_{k+1} = 0.5 x_{k}$ 给出。这是一个线性模型。我们可以用初始状态 $x_0$ 来表示任意时间 $k$ 的状态 $x_k$：\n- 对于 $k=0$：$x_0 = (0.5)^0 x_0 = 1 \\cdot x_0$\n- 对于 $k=1$：$x_1 = 0.5 x_0$\n- 对于 $k=2$：$x_2 = 0.5 x_1 = 0.5 (0.5 x_0) = (0.5)^2 x_0 = 0.25 x_0$\n一般地，$x_k = M_k(x_0) = (0.5)^k x_0$。\n\n现在，我们将 $x_k$ 的这些表达式和给定的 $y_k$ 值代入代价函数，使其仅用 $x_0$ 表示：\n$$J(x_{0}) = \\frac{1}{2} x_{0}^2 + \\frac{1}{2} \\left[ (x_{0} - y_{0})^2 + (x_{1} - y_{1})^2 + (x_{2} - y_{2})^2 \\right]$$\n$$J(x_{0}) = \\frac{1}{2} x_{0}^2 + \\frac{1}{2} \\left[ (x_{0} - 1)^2 + (0.5 x_{0} - 0.5)^2 + (0.25 x_{0} - 0.25)^2 \\right]$$\n我们注意到，从 $y_0=1$ 开始，观测值遵循模型动力学：$y_1 = 0.5 = 0.5 y_0$ 且 $y_2=0.25=(0.5)^2 y_0$。因此，$y_k = (0.5)^k y_0 = (0.5)^k$。\n这使得观测不符项可以简化为：\n$$x_k - y_k = (0.5)^k x_0 - (0.5)^k = (0.5)^k (x_0 - 1)$$\n代价函数中的求和项变为：\n$$\\sum_{k=0}^{2} (x_k - y_k)^2 = \\sum_{k=0}^{2} \\left[ (0.5)^k (x_0 - 1) \\right]^2 = \\sum_{k=0}^{2} (0.5)^{2k} (x_0 - 1)^2 = (x_0 - 1)^2 \\sum_{k=0}^{2} (0.5)^{2k}$$\n我们来计算这个和：\n$$\\sum_{k=0}^{2} (0.5)^{2k} = (0.5)^0 + (0.5)^2 + (0.5)^4 = 1 + \\left(\\frac{1}{2}\\right)^2 + \\left(\\frac{1}{2}\\right)^4 = 1 + \\frac{1}{4} + \\frac{1}{16} = \\frac{16}{16} + \\frac{4}{16} + \\frac{1}{16} = \\frac{21}{16}$$\n将此结果代回代价函数的表达式：\n$$J(x_{0}) = \\frac{1}{2} x_{0}^2 + \\frac{1}{2} \\left( \\frac{21}{16} \\right) (x_{0} - 1)^2$$\n最优初始状态 $x_{0}^{a}$ 是使该代价函数最小化的 $x_0$ 的值。由于 $J(x_0)$ 是一个二次凸函数，其最小值可以通过将其关于 $x_0$ 的一阶导数设为零来找到。这是一阶最优性条件：\n$$\\frac{dJ}{dx_{0}}\\bigg|_{x_{0}=x_{0}^{a}} = 0$$\n我们来计算导数：\n$$\\frac{dJ}{dx_{0}} = \\frac{d}{dx_{0}} \\left[ \\frac{1}{2} x_{0}^2 + \\frac{21}{32} (x_{0} - 1)^2 \\right]$$\n$$\\frac{dJ}{dx_{0}} = \\frac{1}{2} (2 x_{0}) + \\frac{21}{32} (2 (x_{0} - 1)) = x_{0} + \\frac{21}{16} (x_{0} - 1)$$\n将导数设为零以求得 $x_{0}^{a}$：\n$$x_{0}^{a} + \\frac{21}{16} (x_{0}^{a} - 1) = 0$$\n现在，我们求解这个关于 $x_{0}^{a}$ 的线性方程：\n$$x_{0}^{a} + \\frac{21}{16} x_{0}^{a} - \\frac{21}{16} = 0$$\n$$x_{0}^{a} \\left( 1 + \\frac{21}{16} \\right) = \\frac{21}{16}$$\n$$x_{0}^{a} \\left( \\frac{16}{16} + \\frac{21}{16} \\right) = \\frac{21}{16}$$\n$$x_{0}^{a} \\left( \\frac{37}{16} \\right) = \\frac{21}{16}$$\n$$x_{0}^{a} = \\frac{21/16}{37/16} = \\frac{21}{37}$$\n初始状态的分析场为 $x_{0}^{a} = \\frac{21}{37}$。这是一个最简分数，因为 37 是一个质数，并且不是 21 的因子。",
            "answer": "$$\\boxed{\\frac{21}{37}}$$"
        },
        {
            "introduction": "理论计算之后，本练习将引导您进入更具实践性的计算分析，旨在通过编码实验来探究 3D-Var 系统对关键参数的敏感性。您将系统地调整背景误差协方差矩阵 $B$ 的大小，并量化其对分析结果的影响。通过这个过程，您将直观地理解系统是如何在信任背景信息和信任观测信息之间进行权衡的，这是数据同化中的一个核心概念。",
            "id": "4108358",
            "problem": "考虑线性三维变分资料同化 (3D-Var)，它是四维变分资料同化 (4D-Var) 在仅限于单个分析时间时的特例。在 3D-Var 中，状态向量表示为 $\\boldsymbol{x} \\in \\mathbb{R}^n$，背景场状态为 $\\boldsymbol{x}_b \\in \\mathbb{R}^n$，观测向量为 $\\boldsymbol{y} \\in \\mathbb{R}^m$，线性观测算子为 $\\boldsymbol{H} \\in \\mathbb{R}^{m \\times n}$。背景误差协方差为 $\\boldsymbol{B} \\in \\mathbb{R}^{n \\times n}$，观测误差协方差为 $\\boldsymbol{R} \\in \\mathbb{R}^{m \\times m}$，且这两个协方差矩阵都是对称正定的。3D-Var 分析是在高斯线性假设下，通过最小化标准二次目标函数得到的。本问题中所有量均为无量纲，无需物理单位。\n\n你的任务是评估分析场对于背景误差协方差 $\\boldsymbol{B}$ 乘以标量因子 $s \\in \\{0.5, 2.0\\}$（相对于基准情况 $s = 1.0$）的敏感性，并量化由此在两个指标上引起的变化：\n- 后验方差度量，定义为分析误差协方差的迹 $\\operatorname{tr}(\\boldsymbol{A})$，其中 $\\boldsymbol{A}$ 是由最小化过程隐含的分析误差协方差矩阵。\n- 分析增量的大小，定义为增量向量的欧几里得范数 $\\lVert \\boldsymbol{x}_a - \\boldsymbol{x}_b \\rVert_2$，其中 $\\boldsymbol{x}_a$ 是使目标函数最小化的分析状态。\n\n从基本原理出发：包含背景项和观测项的 3D-Var 高斯线性公式，以及严格凸二次型的最小值点是通过将其梯度设为零得到的。在未从这些原理推导出任何快捷公式之前，不要假设或使用它们。\n\n实现一个程序，对于下述的每个测试用例，计算上述两个指标。这些计算需针对应用于 $\\boldsymbol{B}$ 的三个缩放因子 $s \\in \\{1.0, 0.5, 2.0\\}$ 进行，即使用 $\\boldsymbol{B}_s = s \\boldsymbol{B}$，同时保持 $\\boldsymbol{R}$、$\\boldsymbol{H}$、$\\boldsymbol{x}_b$ 和 $\\boldsymbol{y}$ 不变。\n\n测试套件：\n- 案例 1 (理想路径，具有精确观测的标量状态)：\n  - $n = 1$, $m = 1$,\n  - $\\boldsymbol{B} = [1.0]$,\n  - $\\boldsymbol{R} = [0.25]$,\n  - $\\boldsymbol{H} = [1.0]$,\n  - $\\boldsymbol{x}_b = [0.0]$,\n  - $\\boldsymbol{y} = [2.0]$,\n  - 评估 $s \\in \\{1.0, 0.5, 2.0\\}$。\n\n- 案例 2 (边缘情况，对第一个分量有单一、带噪声的观测，且背景误差相关的二维状态)：\n  - $n = 2$, $m = 1$,\n  - $\\boldsymbol{B} = \\begin{bmatrix}1.0  0.3 \\\\ 0.3  1.5\\end{bmatrix}$,\n  - $\\boldsymbol{R} = [4.0]$,\n  - $\\boldsymbol{H} = \\begin{bmatrix}1.0  0.0\\end{bmatrix}$,\n  - $\\boldsymbol{x}_b = \\begin{bmatrix}0.0 \\\\ 0.0\\end{bmatrix}$,\n  - $\\boldsymbol{y} = [1.0]$,\n  - 评估 $s \\in \\{1.0, 0.5, 2.0\\}$。\n\n对于六次评估中的每一次（两个案例乘以三个缩放因子），计算：\n1. 后验方差度量 $\\operatorname{tr}(\\boldsymbol{A})$，作为一个浮点数。\n2. 增量大小 $\\lVert \\boldsymbol{x}_a - \\boldsymbol{x}_b \\rVert_2_，作为一个浮点数。\n\n最终输出格式：\n- 将每个浮点数四舍五入到六位小数。\n- 你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。列表中的每个元素都是一次评估的结果对 $[\\operatorname{tr}(\\boldsymbol{A}), \\lVert \\boldsymbol{x}_a - \\boldsymbol{x}_b \\rVert_2]$，顺序如下：\n  - 案例 1, $s=1.0$；案例 1, $s=0.5$；案例 1, $s=2.0$；案例 2, $s=1.0$；案例 2, $s=0.5$；案例 2, $s=2.0$。\n- 具体来说，程序必须打印出以下形式的单行内容：\n  - $[[v_1,i_1],[v_2,i_2],[v_3,i_3],[v_4,i_4],[v_5,i_5],[v_6,i_6]]$\n  其中每个 $v_k$ 和 $i_k$ 都四舍五入到六位小数，并表示为十进制数（不带百分号）。",
            "solution": "经评估，此问题是有效的。它在科学上基于变分资料同化的原理，是适定的，代价函数的凸性保证了其解的唯一性，并且其表述是客观的。为两个测试用例提供了所有必要的数据和条件，没有矛盾或歧义。\n\n该任务是分析 3D-Var 分析对背景误差协方差矩阵的标量扰动的敏感性。这需要从第一性原理推导出分析状态和分析误差协方差。\n\n待最小化的 3D-Var 代价函数 $J(\\boldsymbol{x})$ 在高斯误差假设下代表了后验概率密度函数的负对数。它由两个二次项组成：一个背景项，用于度量状态向量 $\\boldsymbol{x} \\in \\mathbb{R}^n$ 与背景场状态 $\\boldsymbol{x}_b \\in \\mathbb{R}^n$ 之间的偏离；以及一个观测项，用于度量投影状态 $\\boldsymbol{H}\\boldsymbol{x}$ 与观测值 $\\boldsymbol{y} \\in \\mathbb{R}^m$ 之间的偏离。\n\n$$\nJ(\\boldsymbol{x}) = \\frac{1}{2}(\\boldsymbol{x} - \\boldsymbol{x}_b)^T \\boldsymbol{B}^{-1} (\\boldsymbol{x} - \\boldsymbol{x}_b) + \\frac{1}{2}(\\boldsymbol{y} - \\boldsymbol{H}\\boldsymbol{x})^T \\boldsymbol{R}^{-1} (\\boldsymbol{y} - \\boldsymbol{H}\\boldsymbol{x})\n$$\n\n此处，$\\boldsymbol{B} \\in \\mathbb{R}^{n \\times n}$ 是背景误差协方差矩阵，$\\boldsymbol{R} \\in \\mathbb{R}^{m \\times m}$ 是观测误差协方差矩阵。两者均被设定为对称正定，这确保了 $J(\\boldsymbol{x})$ 是一个严格凸函数，并有唯一的最小值。\n\n为找到使 $J(\\boldsymbol{x})$ 最小化的分析状态 $\\boldsymbol{x}_a$，我们必须找到代价函数关于 $\\boldsymbol{x}$ 的梯度为零的点。根据标准矩阵微积分法则，梯度为：\n\n$$\n\\nabla_{\\boldsymbol{x}} J(\\boldsymbol{x}) = \\boldsymbol{B}^{-1}(\\boldsymbol{x} - \\boldsymbol{x}_b) + \\boldsymbol{H}^T \\boldsymbol{R}^{-1}(\\boldsymbol{H}\\boldsymbol{x} - \\boldsymbol{y})\n$$\n\n在 $\\boldsymbol{x} = \\boldsymbol{x}_a$ 处将梯度设为零：\n\n$$\n\\boldsymbol{B}^{-1}(\\boldsymbol{x}_a - \\boldsymbol{x}_b) + \\boldsymbol{H}^T \\boldsymbol{R}^{-1}(\\boldsymbol{H}\\boldsymbol{x}_a - \\boldsymbol{y}) = 0\n$$\n\n重排各项以求解 $\\boldsymbol{x}_a$：\n\n$$\n\\boldsymbol{B}^{-1}\\boldsymbol{x}_a - \\boldsymbol{B}^{-1}\\boldsymbol{x}_b + \\boldsymbol{H}^T \\boldsymbol{R}^{-1}\\boldsymbol{H}\\boldsymbol{x}_a - \\boldsymbol{H}^T \\boldsymbol{R}^{-1}\\boldsymbol{y} = 0\n$$\n$$\n(\\boldsymbol{B}^{-1} + \\boldsymbol{H}^T \\boldsymbol{R}^{-1}\\boldsymbol{H})\\boldsymbol{x}_a = \\boldsymbol{B}^{-1}\\boldsymbol{x}_b + \\boldsymbol{H}^T \\boldsymbol{R}^{-1}\\boldsymbol{y}\n$$\n\n因此，分析状态为：\n$$\n\\boldsymbol{x}_a = (\\boldsymbol{B}^{-1} + \\boldsymbol{H}^T \\boldsymbol{R}^{-1}\\boldsymbol{H})^{-1} (\\boldsymbol{B}^{-1}\\boldsymbol{x}_b + \\boldsymbol{H}^T \\boldsymbol{R}^{-1}\\boldsymbol{y})\n$$\n\n分析误差协方差矩阵 $\\boldsymbol{A}$ 是代价函数在最小值点处 Hessian 矩阵的逆。Hessian 矩阵是二阶导数矩阵：\n\n$$\n\\boldsymbol{A} = (\\nabla^2_{\\boldsymbol{x}} J(\\boldsymbol{x}))^{-1} = (\\boldsymbol{B}^{-1} + \\boldsymbol{H}^T \\boldsymbol{R}^{-1}\\boldsymbol{H})^{-1}\n$$\n\n虽然这些形式是正确的，但它们涉及到对 $\\boldsymbol{B}$ 和 $\\boldsymbol{R}$ 求逆，对于大的状态和观测空间而言，这在计算上可能非常昂贵。可以推导出一种替代的、更实用的公式。让我们通过重排梯度方程来表示分析增量 $\\delta\\boldsymbol{x} = \\boldsymbol{x}_a - \\boldsymbol{x}_b$：\n\n$$\n(\\boldsymbol{B}^{-1} + \\boldsymbol{H}^T \\boldsymbol{R}^{-1}\\boldsymbol{H})(\\boldsymbol{x}_a - \\boldsymbol{x}_b) = \\boldsymbol{H}^T \\boldsymbol{R}^{-1}(\\boldsymbol{y} - \\boldsymbol{H}\\boldsymbol{x}_b)\n$$\n$$\n\\boldsymbol{x}_a - \\boldsymbol{x}_b = (\\boldsymbol{B}^{-1} + \\boldsymbol{H}^T \\boldsymbol{R}^{-1}\\boldsymbol{H})^{-1} \\boldsymbol{H}^T \\boldsymbol{R}^{-1}(\\boldsymbol{y} - \\boldsymbol{H}\\boldsymbol{x}_b)\n$$\n\n使用 Woodbury 矩阵恒等式，我们可以写出 $(\\boldsymbol{B}^{-1} + \\boldsymbol{H}^T \\boldsymbol{R}^{-1}\\boldsymbol{H})^{-1} = \\boldsymbol{B} - \\boldsymbol{B}\\boldsymbol{H}^T(\\boldsymbol{H}\\boldsymbol{B}\\boldsymbol{H}^T + \\boldsymbol{R})^{-1}\\boldsymbol{H}\\boldsymbol{B}$。这就是 $\\boldsymbol{A}$ 的表达式。\n将此代入增量方程很繁琐。一个更直接的方法是定义 Kalman 增益矩阵 $\\boldsymbol{K}$：\n\n$$\n\\boldsymbol{K} = \\boldsymbol{B}\\boldsymbol{H}^T(\\boldsymbol{H}\\boldsymbol{B}\\boldsymbol{H}^T + \\boldsymbol{R})^{-1}\n$$\n\n通过这个定义，分析增量和分析误差协方差可以高效地表示为：\n\n1.  分析增量：$\\boldsymbol{x}_a - \\boldsymbol{x}_b = \\boldsymbol{K}(\\boldsymbol{y} - \\boldsymbol{H}\\boldsymbol{x}_b)$\n2.  分析误差协方差：$\\boldsymbol{A} = (\\boldsymbol{I} - \\boldsymbol{K}\\boldsymbol{H})\\boldsymbol{B}$\n\n这种公式避免了对大矩阵 $\\boldsymbol{B}$ 求逆，而是需要对尺寸为 $m \\times m$ 的小矩阵 $(\\boldsymbol{H}\\boldsymbol{B}\\boldsymbol{H}^T + \\boldsymbol{R})$ 求逆。\n\n问题要求评估对 $\\boldsymbol{B}$ 乘以因子 $s \\in \\{1.0, 0.5, 2.0\\}$ 进行缩放的敏感性。我们定义 $\\boldsymbol{B}_s = s\\boldsymbol{B}$。每次评估的算法如下：\n\n令 $\\boldsymbol{B}_s = s\\boldsymbol{B}$。\n1.  计算缩放后问题的 Kalman 增益矩阵：\n    $$\n    \\boldsymbol{K}_s = \\boldsymbol{B}_s \\boldsymbol{H}^T (\\boldsymbol{H} \\boldsymbol{B}_s \\boldsymbol{H}^T + \\boldsymbol{R})^{-1}\n    $$\n2.  计算分析增量向量：\n    $$\n    \\delta\\boldsymbol{x}_s = \\boldsymbol{x}_{a,s} - \\boldsymbol{x}_b = \\boldsymbol{K}_s(\\boldsymbol{y} - \\boldsymbol{H}\\boldsymbol{x}_b)\n    $$\n3.  计算第一个指标，即分析增量的大小：\n    $$\n    \\text{指标 2: } \\lVert \\delta\\boldsymbol{x}_s \\rVert_2\n    $$\n4.  计算缩放后问题的分析误差协方差矩阵：\n    $$\n    \\boldsymbol{A}_s = (\\boldsymbol{I} - \\boldsymbol{K}_s\\boldsymbol{H})\\boldsymbol{B}_s\n    $$\n5.  计算第二个指标，即后验方差度量：\n    $$\n    \\text{指标 1: } \\operatorname{tr}(\\boldsymbol{A}_s)\n    $$\n\n对每个测试用例和每个 $s$ 值执行这五个步骤。然后将结果整理并按照问题规范进行格式化。\n\n对于案例 1，标量情况 ($n=1, m=1$): $\\boldsymbol{B}=[1.0]$, $\\boldsymbol{R}=[0.25]$, $\\boldsymbol{H}=[1.0]$, $\\boldsymbol{x}_b=[0.0]$, $\\boldsymbol{y}=[2.0]$。\n当 $s=1.0$ 时：$\\boldsymbol{B}_s=[1.0]$。增益为 $K_s = 1.0 \\times (1.0 \\times 1.0 \\times 1.0 + 0.25)^{-1} = 0.8$。增量为 $0.8 \\times (2.0 - 0.0) = 1.6$。范数为 $1.6$。分析协方差为 $A_s = (1 - 0.8 \\times 1.0) \\times 1.0 = 0.2$。迹为 $0.2$。\n\n对于案例 2，二维状态 ($n=2, m=1$): $\\boldsymbol{B} = \\begin{bmatrix}1.0  0.3 \\\\ 0.3  1.5\\end{bmatrix}$, $\\boldsymbol{R} = [4.0]$, $\\boldsymbol{H} = \\begin{bmatrix}1.0  0.0\\end{bmatrix}$, $\\boldsymbol{x}_b = \\begin{bmatrix}0.0 \\\\ 0.0\\end{bmatrix}$, $\\boldsymbol{y} = [1.0]$。\n当 $s=1.0$ 时：$\\boldsymbol{B}_s = \\boldsymbol{B}$。新息为 $\\boldsymbol{y} - \\boldsymbol{H}\\boldsymbol{x}_b = [1.0]$。项 $\\boldsymbol{H}\\boldsymbol{B}_s\\boldsymbol{H}^T + \\boldsymbol{R}$ 变为 $[1.0] + [4.0] = [5.0]$。增益矩阵为 $\\boldsymbol{K}_s = \\begin{bmatrix}1.0 \\\\ 0.3\\end{bmatrix} [5.0]^{-1} = \\begin{bmatrix}0.2 \\\\ 0.06\\end{bmatrix}$。增量为 $\\begin{bmatrix}0.2 \\\\ 0.06\\end{bmatrix} \\times 1.0 = \\begin{bmatrix}0.2 \\\\ 0.06\\end{bmatrix}$。范数为 $\\sqrt{0.2^2 + 0.06^2} \\approx 0.208806$。分析协方差为 $\\boldsymbol{A}_s = (\\boldsymbol{I} - \\boldsymbol{K}_s\\boldsymbol{H})\\boldsymbol{B}_s = \\begin{bmatrix}0.8  0.24 \\\\ 0.24  1.482\\end{bmatrix}$。迹为 $0.8 + 1.482 = 2.282$。\n\n剩下的计算对其他 $s$ 值遵循相同的步骤。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the 3D-Var sensitivity analysis problem.\n\n    The function computes two metrics for two different test cases, each evaluated\n    with three different scaling factors for the background-error covariance matrix.\n    The metrics are:\n    1. The trace of the analysis-error covariance matrix.\n    2. The Euclidean norm of the analysis increment.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"n\": 1,\n            \"B\": np.array([[1.0]]),\n            \"R\": np.array([[0.25]]),\n            \"H\": np.array([[1.0]]),\n            \"xb\": np.array([0.0]),\n            \"y\": np.array([2.0]),\n        },\n        {\n            \"n\": 2,\n            \"B\": np.array([[1.0, 0.3], [0.3, 1.5]]),\n            \"R\": np.array([[4.0]]),\n            \"H\": np.array([[1.0, 0.0]]),\n            \"xb\": np.array([0.0, 0.0]),\n            \"y\": np.array([1.0]),\n        }\n    ]\n\n    scales = [1.0, 0.5, 2.0]\n    \n    all_results = []\n\n    for case in test_cases:\n        n = case[\"n\"]\n        B = case[\"B\"]\n        R = case[\"R\"]\n        H = case[\"H\"]\n        xb = case[\"xb\"]\n        y = case[\"y\"]\n\n        for s in scales:\n            # Scale the background-error covariance matrix B\n            Bs = s * B\n\n            # Compute the innovation vector (d = y - H*xb)\n            innovation = y - H @ xb\n\n            # Compute the Kalman gain matrix K_s\n            # K_s = B_s * H^T * (H * B_s * H^T + R)^-1\n            HBH_T = H @ Bs @ H.T\n            inv_term = np.linalg.inv(HBH_T + R)\n            Ks = Bs @ H.T @ inv_term\n\n            # 1. Compute the analysis increment and its Euclidean norm\n            # inc = K_s * (y - H*xb)\n            increment = Ks @ innovation\n            increment_norm = np.linalg.norm(increment)\n\n            # 2. Compute the analysis-error covariance A_s and its trace\n            # A_s = (I - K_s * H) * B_s\n            identity_n = np.eye(n)\n            As = (identity_n - Ks @ H) @ Bs\n            trace_A = np.trace(As)\n\n            # Append the pair of results for this evaluation\n            all_results.append([trace_A, increment_norm])\n\n    # Format the final output string as specified in the problem statement.\n    # e.g., [[v1,i1],[v2,i2],...] with 6 decimal places.\n    formatted_results = [f\"[{res[0]:.6f},{res[1]:.6f}]\" for res in all_results]\n    final_output = f\"[{','.join(formatted_results)}]\"\n    \n    print(final_output)\n\nsolve()\n```"
        }
    ]
}