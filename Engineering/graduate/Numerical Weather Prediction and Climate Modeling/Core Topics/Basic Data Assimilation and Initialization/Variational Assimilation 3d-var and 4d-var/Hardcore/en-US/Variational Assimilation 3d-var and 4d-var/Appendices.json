{
    "hands_on_practices": [
        {
            "introduction": "To truly grasp the mechanics of three-dimensional variational (3D-Var) assimilation, we begin with a foundational calculation. This exercise provides a concrete, low-dimensional scenario to demonstrate how 3D-Var blends information from a prior (background) estimate and a new observation. By manually deriving and computing the analysis state, you will gain direct insight into how the background and observation error covariances, $B$, and $R$, weight each piece of information to produce an optimal posterior estimate. ",
            "id": "3864622",
            "problem": "Consider a two-component environmental state vector $x \\in \\mathbb{R}^{2}$ representing spatially aggregated quantities to be analyzed from a single satellite-derived observation in a linear setting. Assume a three-dimensional variational (3D-Var) data assimilation framework derived from Gaussian error statistics and a linear observation operator, where the analysis $x^{a}$ minimizes the quadratic cost function built from the background and observation misfits. Let the background error covariance be $B=\\begin{bmatrix}1  0 \\\\ 0  4\\end{bmatrix}$, the observation operator be $H=\\begin{bmatrix}1  1\\end{bmatrix}$, the observation error covariance be $R=1$, the background (also called prior) be $x_{b}=\\begin{bmatrix}0 \\\\ 0\\end{bmatrix}$, and the observation be $y=3$.\n\nStarting from the definition of the 3D-Var cost function that combines the background and observation misfits under linear-Gaussian assumptions, derive the first-order optimality condition (normal equation) that characterizes the minimum. Then, using the given numerical values, compute the analysis increment $\\delta x^{a}=x^{a}-x_{b}$ and the analysis state $x^{a}$. Express your final result as a single row matrix in the order $\\big[\\delta x^{a}_{1},\\,\\delta x^{a}_{2},\\,x^{a}_{1},\\,x^{a}_{2}\\big]$.\n\nNo units are required. Provide the exact values; no rounding is necessary.",
            "solution": "The problem statement is first subjected to a rigorous validation process.\n\n### Step 1: Extract Givens\n-   State vector: $x \\in \\mathbb{R}^{2}$\n-   Background error covariance: $B=\\begin{bmatrix}1  0 \\\\ 0  4\\end{bmatrix}$\n-   Observation operator: $H=\\begin{bmatrix}1  1\\end{bmatrix}$\n-   Observation error covariance: $R=1$\n-   Background state: $x_{b}=\\begin{bmatrix}0 \\\\ 0\\end{bmatrix}$\n-   Observation: $y=3$\n-   Task: Derive the normal equation, then compute the analysis increment $\\delta x^{a}=x^{a}-x_{b}$ and the analysis state $x^{a}$.\n-   Final Output Format: A single row matrix $\\big[\\delta x^{a}_{1},\\,\\delta x^{a}_{2},\\,x^{a}_{1},\\,x^{a}_{2}\\big]$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed against the validation criteria.\n-   **Scientifically Grounded:** The problem describes a standard three-dimensional variational (3D-Var) data assimilation scenario. The use of a quadratic cost function, background/observation error covariances, and a linear observation operator are canonical elements of this established scientific method in remote sensing and environmental modeling. The framework is derived from Bayes' theorem under linear-Gaussian assumptions. The problem is scientifically sound.\n-   **Well-Posed:** The problem is to find the minimum of a quadratic cost function. The background error covariance matrix $B$ has eigenvalues $1$ and $4$, which are both positive, so $B$ is positive definite and thus invertible. The observation error covariance $R=1$ is a positive scalar. The Hessian of the cost function, $(B^{-1} + H^T R^{-1} H)$, will be shown to be positive definite, which guarantees the existence of a unique, stable minimum. All necessary information is provided, and there are no contradictions. The problem is well-posed.\n-   **Objective:** The problem is stated using precise mathematical language and definitions ($x_b, B, H, R, y$). There are no subjective or ambiguous terms. The problem is objective.\n-   **Other Flaws:** The problem is not trivial, as it requires derivation and matrix algebra. It is not metaphorical, incomplete, or unrealistic for a textbook example.\n\n### Step 3: Verdict and Action\nThe problem is valid. A solution will be provided.\n\nThe 3D-Var analysis state $x^a$ is the state vector $x$ that minimizes the cost function $J(x)$. The cost function measures the misfit to the background state and the observations, weighted by their respective error covariances. For a linear-Gaussian system, it is given by:\n$$J(x) = \\frac{1}{2}(x - x_b)^T B^{-1} (x - x_b) + \\frac{1}{2}(y - Hx)^T R^{-1} (y - Hx)$$\nTo find the minimum, we compute the gradient of $J(x)$ with respect to $x$ and set it to zero. The gradient $\\nabla_x J(x)$ is:\n$$\\nabla_x J(x) = B^{-1}(x - x_b) - H^T R^{-1} (y - Hx)$$\nSetting the gradient to zero at $x=x^a$ gives the first-order optimality condition, also known as the normal equation:\n$$B^{-1}(x^a - x_b) - H^T R^{-1} (y - Hx^a) = 0$$\nThis is the requested derivation of the normal equation.\n\nWe are asked to find the analysis increment, defined as $\\delta x^a = x^a - x_b$. We substitute $x^a = x_b + \\delta x^a$ into the normal equation:\n$$B^{-1}(\\delta x^a) - H^T R^{-1} (y - H(x_b + \\delta x^a)) = 0$$\n$$B^{-1}(\\delta x^a) - H^T R^{-1} (y - Hx_b - H\\delta x^a) = 0$$\nRearranging the terms to solve for $\\delta x^a$:\n$$B^{-1}(\\delta x^a) + H^T R^{-1} H\\delta x^a = H^T R^{-1} (y - Hx_b)$$\n$$(B^{-1} + H^T R^{-1} H) \\delta x^a = H^T R^{-1} (y - Hx_b)$$\nThis equation can be solved for the analysis increment $\\delta x^a$.\n\nNow, we substitute the given numerical values:\n-   $x_b = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$\n-   $B = \\begin{bmatrix} 1  0 \\\\ 0  4 \\end{bmatrix} \\implies B^{-1} = \\begin{bmatrix} 1  0 \\\\ 0  \\frac{1}{4} \\end{bmatrix}$\n-   $H = \\begin{bmatrix} 1  1 \\end{bmatrix} \\implies H^T = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$\n-   $R = 1 \\implies R^{-1} = 1$\n-   $y = 3$\n\nFirst, let's compute the components of the equation for $\\delta x^a$.\nThe term $(y - Hx_b)$ is the innovation:\n$$y - Hx_b = 3 - \\begin{bmatrix} 1  1 \\end{bmatrix} \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} = 3 - 0 = 3$$\nThe right-hand side of the equation is:\n$$H^T R^{-1} (y - Hx_b) = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} (1) (3) = \\begin{bmatrix} 3 \\\\ 3 \\end{bmatrix}$$\nThe matrix on the left-hand side is the Hessian of the cost function:\n$$B^{-1} + H^T R^{-1} H = \\begin{bmatrix} 1  0 \\\\ 0  \\frac{1}{4} \\end{bmatrix} + \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} (1) \\begin{bmatrix} 1  1 \\end{bmatrix}$$\n$$= \\begin{bmatrix} 1  0 \\\\ 0  \\frac{1}{4} \\end{bmatrix} + \\begin{bmatrix} 1  1 \\\\ 1  1 \\end{bmatrix} = \\begin{bmatrix} 2  1 \\\\ 1  1 + \\frac{1}{4} \\end{bmatrix} = \\begin{bmatrix} 2  1 \\\\ 1  \\frac{5}{4} \\end{bmatrix}$$\nTo solve for $\\delta x^a$, we need to invert this matrix. The determinant is:\n$$\\det\\left(\\begin{bmatrix} 2  1 \\\\ 1  \\frac{5}{4} \\end{bmatrix}\\right) = (2)\\left(\\frac{5}{4}\\right) - (1)(1) = \\frac{5}{2} - 1 = \\frac{3}{2}$$\nThe inverse is:\n$$\\begin{bmatrix} 2  1 \\\\ 1  \\frac{5}{4} \\end{bmatrix}^{-1} = \\frac{1}{\\frac{3}{2}} \\begin{bmatrix} \\frac{5}{4}  -1 \\\\ -1  2 \\end{bmatrix} = \\frac{2}{3} \\begin{bmatrix} \\frac{5}{4}  -1 \\\\ -1  2 \\end{bmatrix} = \\begin{bmatrix} \\frac{10}{12}  -\\frac{2}{3} \\\\ -\\frac{2}{3}  \\frac{4}{3} \\end{bmatrix} = \\begin{bmatrix} \\frac{5}{6}  -\\frac{2}{3} \\\\ -\\frac{2}{3}  \\frac{4}{3} \\end{bmatrix}$$\nNow we can solve for $\\delta x^a$:\n$$\\delta x^a = \\begin{bmatrix} \\frac{5}{6}  -\\frac{2}{3} \\\\ -\\frac{2}{3}  \\frac{4}{3} \\end{bmatrix} \\begin{bmatrix} 3 \\\\ 3 \\end{bmatrix} = \\begin{bmatrix} \\frac{5}{6}(3) - \\frac{2}{3}(3) \\\\ -\\frac{2}{3}(3) + \\frac{4}{3}(3) \\end{bmatrix} = \\begin{bmatrix} \\frac{5}{2} - 2 \\\\ -2 + 4 \\end{bmatrix} = \\begin{bmatrix} \\frac{1}{2} \\\\ 2 \\end{bmatrix}$$\nSo, the components of the analysis increment are $\\delta x^a_1 = \\frac{1}{2}$ and $\\delta x^a_2 = 2$.\n\nFinally, we compute the analysis state $x^a$:\n$$x^a = x_b + \\delta x^a = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} + \\begin{bmatrix} \\frac{1}{2} \\\\ 2 \\end{bmatrix} = \\begin{bmatrix} \\frac{1}{2} \\\\ 2 \\end{bmatrix}$$\nThe components of the analysis state are $x^a_1 = \\frac{1}{2}$ and $x^a_2 = 2$.\n\nThe final result is requested as a single row matrix in the order $\\big[\\delta x^{a}_{1},\\,\\delta x^{a}_{2},\\,x^{a}_{1},\\,x^{a}_{2}\\big]$. This gives:\n$$\\begin{pmatrix} \\frac{1}{2}  2  \\frac{1}{2}  2 \\end{pmatrix}$$",
            "answer": "$$\\boxed{\\begin{pmatrix} \\frac{1}{2}  2  \\frac{1}{2}  2 \\end{pmatrix}}$$"
        },
        {
            "introduction": "Building on the static 3D-Var framework, we now introduce the dimension of time with four-dimensional variational (4D-Var) assimilation. This practice explores the core concept of strong-constraint 4D-Var, where a model's dynamics are assumed to be perfect over an assimilation window. The challenge shifts from correcting the state at a single moment to finding the optimal initial state $x_0$ that, when evolved by the model, provides the best fit to a series of observations distributed in time. ",
            "id": "3864672",
            "problem": "Consider a scalar linear discrete-time prognostic variable in a simplified environmental model, evolving according to the perfect-model (strong-constraint) dynamics $x_{k+1} = 0.5\\, x_{k}$ for $k \\in \\{0,1,2\\}$. You are given a background (prior) estimate of the initial state $x_{b} = 0$ with background error covariance $B = 1$. At each time $k \\in \\{0,1,2\\}$, a direct observation of the state is available with observation operator $H = 1$ and observation error covariance $R_{k} = 1$. The observed values are $y_{0} = 1$, $y_{1} = 0.5$, and $y_{2} = 0.25$.\n\nUsing only the foundational definition of the four-dimensional variational data assimilation (4D-Var) cost function for the strong-constraint case—namely, a quadratic penalty on the deviation from the background and a sum of quadratic penalties on the model-predicted observation misfits over the time window—formulate the estimation of the initial state $x_{0}$ as a minimization problem in terms of $x_{0}$ alone. Derive the first-order optimality condition from this definition and solve analytically for the strong-constraint four-dimensional variational data assimilation (4D-Var) analysis $x_{0}^{a}$.\n\nProvide the exact value of $x_{0}^{a}$ as a single simplified fraction. Do not approximate or round.",
            "solution": "The problem requires the determination of the optimal initial state, denoted as the analysis $x_{0}^{a}$, by minimizing the strong-constraint four-dimensional variational data assimilation (4D-Var) cost function. We will first formulate this cost function based on its foundational definition, then derive the first-order optimality condition, and finally solve for $x_{0}^{a}$.\n\nThe foundational definition of the strong-constraint 4D-Var cost function $J$ for an initial state $x_0$ is the sum of two penalty terms: a background term penalizing the deviation of the initial state from a prior (background) estimate, and an observation term penalizing the misfit between model predictions and observations over a time window. For a discrete-time system, this is expressed as:\n$$J(x_{0}) = \\frac{1}{2} (x_{0} - x_{b})^T B^{-1} (x_{0} - x_{b}) + \\frac{1}{2} \\sum_{k=0}^{N} (H_{k} M_{k}(x_{0}) - y_{k})^T R_{k}^{-1} (H_{k} M_{k}(x_{0}) - y_{k})$$\nwhere $x_0$ is the initial state vector to be estimated, $x_b$ is the background state vector, $B$ is the background error covariance matrix, $N$ is the number of time steps in the assimilation window, $y_k$ is the observation vector at time $k$, $H_k$ is the observation operator that maps the model state space to the observation space at time $k$, $R_k$ is the observation error covariance matrix at time $k$, and $M_k(x_0)$ represents the evolution of the initial state $x_0$ to time $k$ by the prognostic model.\n\nThe problem provides a scalar system, so all matrices and vectors become scalars. The given values are:\n- Background state: $x_{b} = 0$\n- Background error covariance: $B = 1$, so $B^{-1} = 1$\n- Observation operator: $H = 1$ (direct observation) for all $k$\n- Observation error covariance: $R_{k} = 1$, so $R_{k}^{-1} = 1$ for all $k$\n- Observations: $y_{0} = 1$, $y_{1} = 0.5$, $y_{2} = 0.25$\n- Assimilation window: $k \\in \\{0,1,2\\}$, so $N=2$\n\nThe scalar cost function is:\n$$J(x_{0}) = \\frac{1}{2} B^{-1} (x_{0} - x_{b})^2 + \\frac{1}{2} \\sum_{k=0}^{2} R_{k}^{-1} (H x_{k} - y_{k})^2$$\nSubstituting the given numeric values for the covariances, background state, and observation operator:\n$$J(x_{0}) = \\frac{1}{2} (1) (x_{0} - 0)^2 + \\frac{1}{2} \\sum_{k=0}^{2} (1) ((1) x_{k} - y_{k})^2$$\n$$J(x_{0}) = \\frac{1}{2} x_{0}^2 + \\frac{1}{2} \\sum_{k=0}^{2} (x_{k} - y_{k})^2$$\nThe model dynamics are given by $x_{k+1} = 0.5 x_{k}$. This is a linear model. We can express the state $x_k$ at any time $k$ in terms of the initial state $x_0$:\n- For $k=0$: $x_0 = (0.5)^0 x_0 = 1 \\cdot x_0$\n- For $k=1$: $x_1 = 0.5 x_0$\n- For $k=2$: $x_2 = 0.5 x_1 = 0.5 (0.5 x_0) = (0.5)^2 x_0 = 0.25 x_0$\nIn general, $x_k = M_k(x_0) = (0.5)^k x_0$.\n\nNow, we substitute these expressions for $x_k$ and the given values for $y_k$ into the cost function, to express it solely in terms of $x_0$:\n$$J(x_{0}) = \\frac{1}{2} x_{0}^2 + \\frac{1}{2} \\left[ (x_{0} - y_{0})^2 + (x_{1} - y_{1})^2 + (x_{2} - y_{2})^2 \\right]$$\n$$J(x_{0}) = \\frac{1}{2} x_{0}^2 + \\frac{1}{2} \\left[ (x_{0} - 1)^2 + (0.5 x_{0} - 0.5)^2 + (0.25 x_{0} - 0.25)^2 \\right]$$\nWe notice that the observations follow the model dynamics starting from $y_0=1$: $y_1 = 0.5 = 0.5 y_0$ and $y_2=0.25=(0.5)^2 y_0$. Therefore, $y_k = (0.5)^k y_0 = (0.5)^k$.\nThis allows for a simplification of the observation misfit terms:\n$$x_k - y_k = (0.5)^k x_0 - (0.5)^k = (0.5)^k (x_0 - 1)$$\nThe sum in the cost function becomes:\n$$\\sum_{k=0}^{2} (x_k - y_k)^2 = \\sum_{k=0}^{2} \\left[ (0.5)^k (x_0 - 1) \\right]^2 = \\sum_{k=0}^{2} (0.5)^{2k} (x_0 - 1)^2 = (x_0 - 1)^2 \\sum_{k=0}^{2} (0.5)^{2k}$$\nLet's compute the sum:\n$$\\sum_{k=0}^{2} (0.5)^{2k} = (0.5)^0 + (0.5)^2 + (0.5)^4 = 1 + \\left(\\frac{1}{2}\\right)^2 + \\left(\\frac{1}{2}\\right)^4 = 1 + \\frac{1}{4} + \\frac{1}{16} = \\frac{16}{16} + \\frac{4}{16} + \\frac{1}{16} = \\frac{21}{16}$$\nSubstituting this back into the cost function expression:\n$$J(x_{0}) = \\frac{1}{2} x_{0}^2 + \\frac{1}{2} \\left( \\frac{21}{16} \\right) (x_{0} - 1)^2$$\nThe optimal initial state $x_{0}^{a}$ is the value of $x_0$ that minimizes this cost function. Since $J(x_0)$ is a quadratic and convex function, its minimum can be found by setting its first derivative with respect to $x_0$ to zero. This is the first-order optimality condition:\n$$\\frac{dJ}{dx_{0}}\\bigg|_{x_{0}=x_{0}^{a}} = 0$$\nLet's compute the derivative:\n$$\\frac{dJ}{dx_{0}} = \\frac{d}{dx_{0}} \\left[ \\frac{1}{2} x_{0}^2 + \\frac{21}{32} (x_{0} - 1)^2 \\right]$$\n$$\\frac{dJ}{dx_{0}} = \\frac{1}{2} (2 x_{0}) + \\frac{21}{32} (2 (x_{0} - 1)) = x_{0} + \\frac{21}{16} (x_{0} - 1)$$\nSetting the derivative to zero to find $x_{0}^{a}$:\n$$x_{0}^{a} + \\frac{21}{16} (x_{0}^{a} - 1) = 0$$\nNow, we solve this linear equation for $x_{0}^{a}$:\n$$x_{0}^{a} + \\frac{21}{16} x_{0}^{a} - \\frac{21}{16} = 0$$\n$$x_{0}^{a} \\left( 1 + \\frac{21}{16} \\right) = \\frac{21}{16}$$\n$$x_{0}^{a} \\left( \\frac{16}{16} + \\frac{21}{16} \\right) = \\frac{21}{16}$$\n$$x_{0}^{a} \\left( \\frac{37}{16} \\right) = \\frac{21}{16}$$\n$$x_{0}^{a} = \\frac{21/16}{37/16} = \\frac{21}{37}$$\nThe analysis of the initial state is $x_{0}^{a} = \\frac{21}{37}$. This is a simplified fraction as $37$ is a prime number and is not a factor of $21$.",
            "answer": "$$\\boxed{\\frac{21}{37}}$$"
        },
        {
            "introduction": "Moving beyond single analytical solutions, this practice delves into the practical behavior and sensitivity of the 3D-Var system. The background error covariance matrix, $B$, is a critical component that encodes the uncertainty and error structures of the model forecast. This computational exercise tasks you with systematically exploring how scaling $B$ impacts the final analysis, providing crucial intuition on how data assimilation systems are 'tuned' and how they balance the trust between the background and incoming observations. ",
            "id": "4108358",
            "problem": "Consider linear Three-Dimensional Variational data assimilation (3D-Var), a special case of Four-Dimensional Variational data assimilation (4D-Var) when restricted to a single analysis time. In 3D-Var, the state vector is denoted by $\\boldsymbol{x} \\in \\mathbb{R}^n$, the background state by $\\boldsymbol{x}_b \\in \\mathbb{R}^n$, the observation vector by $\\boldsymbol{y} \\in \\mathbb{R}^m$, and the linear observation operator by $\\boldsymbol{H} \\in \\mathbb{R}^{m \\times n}$. The background-error covariance is $\\boldsymbol{B} \\in \\mathbb{R}^{n \\times n}$ and the observation-error covariance is $\\boldsymbol{R} \\in \\mathbb{R}^{m \\times m}$, with both covariances symmetric and positive definite. The 3D-Var analysis is obtained by minimizing the standard quadratic objective under the Gaussian-linear assumptions. All quantities in this problem are dimensionless, and no physical units are required.\n\nYour task is to assess the sensitivity of the analysis to scaling the background-error covariance $\\boldsymbol{B}$ by a scalar factor $s \\in \\{0.5, 2.0\\}$ relative to the baseline case $s = 1.0$, and to quantify the induced changes in two metrics:\n- The posterior variance measure defined as the trace of the analysis-error covariance, $\\operatorname{tr}(\\boldsymbol{A})$, where $\\boldsymbol{A}$ is the analysis-error covariance matrix implied by the minimization.\n- The magnitude of the analysis increment, defined as the Euclidean norm of the increment vector $\\lVert \\boldsymbol{x}_a - \\boldsymbol{x}_b \\rVert_2$, where $\\boldsymbol{x}_a$ is the analysis state that minimizes the objective.\n\nStart from the fundamental base: the Gaussian-linear formulation of 3D-Var with the background and observation terms, and the fact that the minimizer of a strictly convex quadratic is obtained by setting the gradient to zero. Do not assume or use any shortcut formulas without deriving them from these principles.\n\nImplement a program that, for each test case described below, computes the two metrics above for the three scaling factors $s \\in \\{1.0, 0.5, 2.0\\}$ applied to $\\boldsymbol{B}$, i.e., use $\\boldsymbol{B}_s = s \\boldsymbol{B}$ while keeping $\\boldsymbol{R}$, $\\boldsymbol{H}$, $\\boldsymbol{x}_b$, and $\\boldsymbol{y}$ unchanged.\n\nTest Suite:\n- Case 1 (happy path, scalar state with accurate observation):\n  - $n = 1$, $m = 1$,\n  - $\\boldsymbol{B} = [1.0]$,\n  - $\\boldsymbol{R} = [0.25]$,\n  - $\\boldsymbol{H} = [1.0]$,\n  - $\\boldsymbol{x}_b = [0.0]$,\n  - $\\boldsymbol{y} = [2.0]$,\n  - evaluate $s \\in \\{1.0, 0.5, 2.0\\}$.\n\n- Case 2 (edge case, two-dimensional state with a single, noisy observation of the first component and correlated background errors):\n  - $n = 2$, $m = 1$,\n  - $\\boldsymbol{B} = \\begin{bmatrix}1.0  0.3 \\\\ 0.3  1.5\\end{bmatrix}$,\n  - $\\boldsymbol{R} = [4.0]$,\n  - $\\boldsymbol{H} = \\begin{bmatrix}1.0  0.0\\end{bmatrix}$,\n  - $\\boldsymbol{x}_b = \\begin{bmatrix}0.0 \\\\ 0.0\\end{bmatrix}$,\n  - $\\boldsymbol{y} = [1.0]$,\n  - evaluate $s \\in \\{1.0, 0.5, 2.0\\}$.\n\nFor each of the six evaluations (two cases times three scaling factors), compute:\n1. The posterior variance measure $\\operatorname{tr}(\\boldsymbol{A})$ as a single float.\n2. The increment magnitude $\\lVert \\boldsymbol{x}_a - \\boldsymbol{x}_b \\rVert_2$ as a single float.\n\nFinal Output Format:\n- Round each float to six decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the pair $[\\operatorname{tr}(\\boldsymbol{A}), \\lVert \\boldsymbol{x}_a - \\boldsymbol{x}_b \\rVert_2]$ for one evaluation, in the order:\n  - Case 1 with $s=1.0$, Case 1 with $s=0.5$, Case 1 with $s=2.0$, Case 2 with $s=1.0$, Case 2 with $s=0.5$, Case 2 with $s=2.0$.\n- Concretely, the program must print a single line of the form:\n  - $[[v_1,i_1],[v_2,i_2],[v_3,i_3],[v_4,i_4],[v_5,i_5],[v_6,i_6]]$\n  with each $v_k$ and $i_k$ rounded to six decimal places and expressed as decimal numbers (no percentage signs).",
            "solution": "The problem is assessed to be valid. It is scientifically grounded in the principles of variational data assimilation, well-posed with a unique solution guaranteed by the convexity of the cost function, and objective in its formulation. All necessary data and conditions for the two test cases are provided, and there are no contradictions or ambiguities.\n\nThe task is to analyze the sensitivity of a 3D-Var analysis to a scalar perturbation of the background-error covariance matrix. This requires deriving the analysis state and the analysis-error covariance from first principles.\n\nThe 3D-Var cost function, $J(\\boldsymbol{x})$, which is to be minimized, represents the negative logarithm of the posterior probability density function under Gaussian error assumptions. It is composed of two quadratic terms: a background term measuring the departure of the state vector $\\boldsymbol{x} \\in \\mathbb{R}^n$ from the background state $\\boldsymbol{x}_b \\in \\mathbb{R}^n$, and an observation term measuring the departure of the projected state $\\boldsymbol{H}\\boldsymbol{x}$ from the observations $\\boldsymbol{y} \\in \\mathbb{R}^m$.\n\n$$\nJ(\\boldsymbol{x}) = \\frac{1}{2}(\\boldsymbol{x} - \\boldsymbol{x}_b)^T \\boldsymbol{B}^{-1} (\\boldsymbol{x} - \\boldsymbol{x}_b) + \\frac{1}{2}(\\boldsymbol{y} - \\boldsymbol{H}\\boldsymbol{x})^T \\boldsymbol{R}^{-1} (\\boldsymbol{y} - \\boldsymbol{H}\\boldsymbol{x})\n$$\n\nHere, $\\boldsymbol{B} \\in \\mathbb{R}^{n \\times n}$ is the background-error covariance matrix and $\\boldsymbol{R} \\in \\mathbb{R}^{m \\times m}$ is the observation-error covariance matrix. Both are given as symmetric and positive definite, which ensures that $J(\\boldsymbol{x})$ is a strictly convex function and has a unique minimum.\n\nTo find the analysis state $\\boldsymbol{x}_a$ that minimizes $J(\\boldsymbol{x})$, we must find the point where the gradient of the cost function with respect to $\\boldsymbol{x}$ is zero. The gradient, using standard rules of matrix calculus, is:\n\n$$\n\\nabla_{\\boldsymbol{x}} J(\\boldsymbol{x}) = \\boldsymbol{B}^{-1}(\\boldsymbol{x} - \\boldsymbol{x}_b) + \\boldsymbol{H}^T \\boldsymbol{R}^{-1}(\\boldsymbol{H}\\boldsymbol{x} - \\boldsymbol{y})\n$$\n\nSetting the gradient to zero at $\\boldsymbol{x} = \\boldsymbol{x}_a$:\n\n$$\n\\boldsymbol{B}^{-1}(\\boldsymbol{x}_a - \\boldsymbol{x}_b) + \\boldsymbol{H}^T \\boldsymbol{R}^{-1}(\\boldsymbol{H}\\boldsymbol{x}_a - \\boldsymbol{y}) = 0\n$$\n\nRearranging the terms to solve for $\\boldsymbol{x}_a$:\n\n$$\n\\boldsymbol{B}^{-1}\\boldsymbol{x}_a - \\boldsymbol{B}^{-1}\\boldsymbol{x}_b + \\boldsymbol{H}^T \\boldsymbol{R}^{-1}\\boldsymbol{H}\\boldsymbol{x}_a - \\boldsymbol{H}^T \\boldsymbol{R}^{-1}\\boldsymbol{y} = 0\n$$\n$$\n(\\boldsymbol{B}^{-1} + \\boldsymbol{H}^T \\boldsymbol{R}^{-1}\\boldsymbol{H})\\boldsymbol{x}_a = \\boldsymbol{B}^{-1}\\boldsymbol{x}_b + \\boldsymbol{H}^T \\boldsymbol{R}^{-1}\\boldsymbol{y}\n$$\n\nThe analysis state is therefore:\n$$\n\\boldsymbol{x}_a = (\\boldsymbol{B}^{-1} + \\boldsymbol{H}^T \\boldsymbol{R}^{-1}\\boldsymbol{H})^{-1} (\\boldsymbol{B}^{-1}\\boldsymbol{x}_b + \\boldsymbol{H}^T \\boldsymbol{R}^{-1}\\boldsymbol{y})\n$$\n\nThe analysis-error covariance matrix, $\\boldsymbol{A}$, is the inverse of the Hessian of the cost function evaluated at the minimum. The Hessian is the matrix of second derivatives:\n\n$$\n\\boldsymbol{A} = (\\nabla^2_{\\boldsymbol{x}} J(\\boldsymbol{x}))^{-1} = (\\boldsymbol{B}^{-1} + \\boldsymbol{H}^T \\boldsymbol{R}^{-1}\\boldsymbol{H})^{-1}\n$$\n\nWhile these forms are correct, they involve inverting $\\boldsymbol{B}$ and $\\boldsymbol{R}$, which can be computationally expensive for large state and observation spaces. An alternative, more practical formulation can be derived. Let us express the analysis increment, $\\delta\\boldsymbol{x} = \\boldsymbol{x}_a - \\boldsymbol{x}_b$, by rearranging the gradient equation:\n\n$$\n(\\boldsymbol{B}^{-1} + \\boldsymbol{H}^T \\boldsymbol{R}^{-1}\\boldsymbol{H})(\\boldsymbol{x}_a - \\boldsymbol{x}_b) = \\boldsymbol{H}^T \\boldsymbol{R}^{-1}(\\boldsymbol{y} - \\boldsymbol{H}\\boldsymbol{x}_b)\n$$\n$$\n\\boldsymbol{x}_a - \\boldsymbol{x}_b = (\\boldsymbol{B}^{-1} + \\boldsymbol{H}^T \\boldsymbol{R}^{-1}\\boldsymbol{H})^{-1} \\boldsymbol{H}^T \\boldsymbol{R}^{-1}(\\boldsymbol{y} - \\boldsymbol{H}\\boldsymbol{x}_b)\n$$\n\nUsing the Woodbury matrix identity, we can write $(\\boldsymbol{B}^{-1} + \\boldsymbol{H}^T \\boldsymbol{R}^{-1}\\boldsymbol{H})^{-1} = \\boldsymbol{B} - \\boldsymbol{B}\\boldsymbol{H}^T(\\boldsymbol{H}\\boldsymbol{B}\\boldsymbol{H}^T + \\boldsymbol{R})^{-1}\\boldsymbol{H}\\boldsymbol{B}$. This is the expression for $\\boldsymbol{A}$.\nSubstituting this into the increment equation is cumbersome. A more direct route is to define the Kalman gain matrix $\\boldsymbol{K}$:\n\n$$\n\\boldsymbol{K} = \\boldsymbol{B}\\boldsymbol{H}^T(\\boldsymbol{H}\\boldsymbol{B}\\boldsymbol{H}^T + \\boldsymbol{R})^{-1}\n$$\n\nWith this definition, the analysis increment and analysis-error covariance can be expressed efficiently as:\n\n1.  Analysis Increment: $\\boldsymbol{x}_a - \\boldsymbol{x}_b = \\boldsymbol{K}(\\boldsymbol{y} - \\boldsymbol{H}\\boldsymbol{x}_b)$\n2.  Analysis-Error Covariance: $\\boldsymbol{A} = (\\boldsymbol{I} - \\boldsymbol{K}\\boldsymbol{H})\\boldsymbol{B}$\n\nThis formulation avoids inverting the large matrix $\\boldsymbol{B}$ and instead requires inverting the smaller matrix $(\\boldsymbol{H}\\boldsymbol{B}\\boldsymbol{H}^T + \\boldsymbol{R})$ of size $m \\times m$.\n\nThe problem requires evaluating the sensitivity to scaling $\\boldsymbol{B}$ by a factor $s \\in \\{1.0, 0.5, 2.0\\}$. We define $\\boldsymbol{B}_s = s\\boldsymbol{B}$. The algorithm for each evaluation is as follows:\n\nLet $\\boldsymbol{B}_s = s\\boldsymbol{B}$.\n1.  Compute the Kalman gain matrix for the scaled problem:\n    $$\n    \\boldsymbol{K}_s = \\boldsymbol{B}_s \\boldsymbol{H}^T (\\boldsymbol{H} \\boldsymbol{B}_s \\boldsymbol{H}^T + \\boldsymbol{R})^{-1}\n    $$\n2.  Compute the analysis increment vector:\n    $$\n    \\delta\\boldsymbol{x}_s = \\boldsymbol{x}_{a,s} - \\boldsymbol{x}_b = \\boldsymbol{K}_s(\\boldsymbol{y} - \\boldsymbol{H}\\boldsymbol{x}_b)\n    $$\n3.  Compute the first metric, the magnitude of the analysis increment:\n    $$\n    \\text{Metric 2: } \\lVert \\delta\\boldsymbol{x}_s \\rVert_2\n    $$\n4.  Compute the analysis-error covariance matrix for the scaled problem:\n    $$\n    \\boldsymbol{A}_s = (\\boldsymbol{I} - \\boldsymbol{K}_s\\boldsymbol{H})\\boldsymbol{B}_s\n    $$\n5.  Compute the second metric, the posterior variance measure:\n    $$\n    \\text{Metric 1: } \\operatorname{tr}(\\boldsymbol{A}_s)\n    $$\n\nThese five steps are executed for each test case and for each value of $s$. The results are then collated and formatted as per the problem specification.\n\nFor Case 1, with scalar quantities ($n=1, m=1$): $\\boldsymbol{B}=[1.0]$, $\\boldsymbol{R}=[0.25]$, $\\boldsymbol{H}=[1.0]$, $\\boldsymbol{x}_b=[0.0]$, $\\boldsymbol{y}=[2.0]$.\nFor $s=1.0$: $\\boldsymbol{B}_s=[1.0]$. The gain is $K_s = 1.0 \\times (1.0 \\times 1.0 \\times 1.0 + 0.25)^{-1} = 0.8$. The increment is $0.8 \\times (2.0 - 0.0) = 1.6$. The norm is $1.6$. The analysis covariance is $A_s = (1 - 0.8 \\times 1.0) \\times 1.0 = 0.2$. The trace is $0.2$.\n\nFor Case 2, with a 2-dimensional state ($n=2, m=1$): $\\boldsymbol{B} = \\begin{bmatrix}1.0  0.3 \\\\ 0.3  1.5\\end{bmatrix}$, $\\boldsymbol{R} = [4.0]$, $\\boldsymbol{H} = \\begin{bmatrix}1.0  0.0\\end{bmatrix}$, $\\boldsymbol{x}_b = \\begin{bmatrix}0.0 \\\\ 0.0\\end{bmatrix}$, $\\boldsymbol{y} = [1.0]$.\nFor $s=1.0$: $\\boldsymbol{B}_s = \\boldsymbol{B}$. The innovation is $\\boldsymbol{y} - \\boldsymbol{H}\\boldsymbol{x}_b = [1.0]$. The term $\\boldsymbol{H}\\boldsymbol{B}_s\\boldsymbol{H}^T + \\boldsymbol{R}$ becomes $[1.0] + [4.0] = [5.0]$. The gain matrix is $\\boldsymbol{K}_s = \\begin{bmatrix}1.0 \\\\ 0.3\\end{bmatrix} [5.0]^{-1} = \\begin{bmatrix}0.2 \\\\ 0.06\\end{bmatrix}$. The increment is $\\begin{bmatrix}0.2 \\\\ 0.06\\end{bmatrix} \\times 1.0 = \\begin{bmatrix}0.2 \\\\ 0.06\\end{bmatrix}$. The norm is $\\sqrt{0.2^2 + 0.06^2} \\approx 0.208806$. The analysis covariance is $\\boldsymbol{A}_s = (\\boldsymbol{I} - \\boldsymbol{K}_s\\boldsymbol{H})\\boldsymbol{B}_s = \\begin{bmatrix}0.8  0.24 \\\\ 0.24  1.482\\end{bmatrix}$. The trace is $0.8 + 1.482 = 2.282$.\n\nThe remaining calculations follow the same procedure for the other values of $s$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the 3D-Var sensitivity analysis problem.\n\n    The function computes two metrics for two different test cases, each evaluated\n    with three different scaling factors for the background-error covariance matrix.\n    The metrics are:\n    1. The trace of the analysis-error covariance matrix.\n    2. The Euclidean norm of the analysis increment.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"n\": 1,\n            \"B\": np.array([[1.0]]),\n            \"R\": np.array([[0.25]]),\n            \"H\": np.array([[1.0]]),\n            \"xb\": np.array([0.0]),\n            \"y\": np.array([2.0]),\n        },\n        {\n            \"n\": 2,\n            \"B\": np.array([[1.0, 0.3], [0.3, 1.5]]),\n            \"R\": np.array([[4.0]]),\n            \"H\": np.array([[1.0, 0.0]]),\n            \"xb\": np.array([0.0, 0.0]),\n            \"y\": np.array([1.0]),\n        }\n    ]\n\n    scales = [1.0, 0.5, 2.0]\n    \n    all_results = []\n\n    for case in test_cases:\n        n = case[\"n\"]\n        B = case[\"B\"]\n        R = case[\"R\"]\n        H = case[\"H\"]\n        xb = case[\"xb\"]\n        y = case[\"y\"]\n\n        for s in scales:\n            # Scale the background-error covariance matrix B\n            Bs = s * B\n\n            # Compute the innovation vector (d = y - H*xb)\n            innovation = y - H @ xb\n\n            # Compute the Kalman gain matrix K_s\n            # K_s = B_s * H^T * (H * B_s * H^T + R)^-1\n            HBH_T = H @ Bs @ H.T\n            inv_term = np.linalg.inv(HBH_T + R)\n            Ks = Bs @ H.T @ inv_term\n\n            # 1. Compute the analysis increment and its Euclidean norm\n            # inc = K_s * (y - H*xb)\n            increment = Ks @ innovation\n            increment_norm = np.linalg.norm(increment)\n\n            # 2. Compute the analysis-error covariance A_s and its trace\n            # A_s = (I - K_s * H) * B_s\n            identity_n = np.eye(n)\n            As = (identity_n - Ks @ H) @ Bs\n            trace_A = np.trace(As)\n\n            # Append the pair of results for this evaluation\n            all_results.append([trace_A, increment_norm])\n\n    # Format the final output string as specified in the problem statement.\n    # e.g., [[v1,i1],[v2,i2],...] with 6 decimal places.\n    formatted_results = [f\"[{res[0]:.6f},{res[1]:.6f}]\" for res in all_results]\n    final_output = f\"[{','.join(formatted_results)}]\"\n    \n    print(final_output)\n\nsolve()\n```"
        }
    ]
}