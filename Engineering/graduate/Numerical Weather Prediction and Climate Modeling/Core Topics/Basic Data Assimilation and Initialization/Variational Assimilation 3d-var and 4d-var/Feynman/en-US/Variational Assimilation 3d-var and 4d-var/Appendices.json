{
    "hands_on_practices": [
        {
            "introduction": "The balance between belief in our background forecast and new observations lies at the heart of data assimilation. This balance is controlled by the specified background-error covariance ($B$) and observation-error covariance ($R$) matrices. This exercise provides a hands-on method to explore this fundamental concept by systematically scaling the $B$ matrix and quantifying the resulting changes in the analysis increment and the posterior analysis error, offering crucial insights into the system's sensitivity. ",
            "id": "4108358",
            "problem": "Consider linear Three-Dimensional Variational data assimilation (3D-Var), a special case of Four-Dimensional Variational data assimilation (4D-Var) when restricted to a single analysis time. In 3D-Var, the state vector is denoted by $\\boldsymbol{x} \\in \\mathbb{R}^n$, the background state by $\\boldsymbol{x}_b \\in \\mathbb{R}^n$, the observation vector by $\\boldsymbol{y} \\in \\mathbb{R}^m$, and the linear observation operator by $\\boldsymbol{H} \\in \\mathbb{R}^{m \\times n}$. The background-error covariance is $\\boldsymbol{B} \\in \\mathbb{R}^{n \\times n}$ and the observation-error covariance is $\\boldsymbol{R} \\in \\mathbb{R}^{m \\times m}$, with both covariances symmetric and positive definite. The 3D-Var analysis is obtained by minimizing the standard quadratic objective under the Gaussian-linear assumptions. All quantities in this problem are dimensionless, and no physical units are required.\n\nYour task is to assess the sensitivity of the analysis to scaling the background-error covariance $\\boldsymbol{B}$ by a scalar factor $s \\in \\{0.5, 2.0\\}$ relative to the baseline case $s = 1.0$, and to quantify the induced changes in two metrics:\n- The posterior variance measure defined as the trace of the analysis-error covariance, $\\operatorname{tr}(\\boldsymbol{A})$, where $\\boldsymbol{A}$ is the analysis-error covariance matrix implied by the minimization.\n- The magnitude of the analysis increment, defined as the Euclidean norm of the increment vector $\\lVert \\boldsymbol{x}_a - \\boldsymbol{x}_b \\rVert_2$, where $\\boldsymbol{x}_a$ is the analysis state that minimizes the objective.\n\nStart from the fundamental base: the Gaussian-linear formulation of 3D-Var with the background and observation terms, and the fact that the minimizer of a strictly convex quadratic is obtained by setting the gradient to zero. Do not assume or use any shortcut formulas without deriving them from these principles.\n\nImplement a program that, for each test case described below, computes the two metrics above for the three scaling factors $s \\in \\{1.0, 0.5, 2.0\\}$ applied to $\\boldsymbol{B}$, i.e., use $\\boldsymbol{B}_s = s \\boldsymbol{B}$ while keeping $\\boldsymbol{R}$, $\\boldsymbol{H}$, $\\boldsymbol{x}_b$, and $\\boldsymbol{y}$ unchanged.\n\nTest Suite:\n- Case 1 (happy path, scalar state with accurate observation):\n  - $n = 1$, $m = 1$,\n  - $\\boldsymbol{B} = [1.0]$,\n  - $\\boldsymbol{R} = [0.25]$,\n  - $\\boldsymbol{H} = [1.0]$,\n  - $\\boldsymbol{x}_b = [0.0]$,\n  - $\\boldsymbol{y} = [2.0]$,\n  - evaluate $s \\in \\{1.0, 0.5, 2.0\\}$.\n\n- Case 2 (edge case, two-dimensional state with a single, noisy observation of the first component and correlated background errors):\n  - $n = 2$, $m = 1$,\n  - $\\boldsymbol{B} = \\begin{bmatrix}1.0  0.3 \\\\ 0.3  1.5\\end{bmatrix}$,\n  - $\\boldsymbol{R} = [4.0]$,\n  - $\\boldsymbol{H} = \\begin{bmatrix}1.0  0.0\\end{bmatrix}$,\n  - $\\boldsymbol{x}_b = \\begin{bmatrix}0.0 \\\\ 0.0\\end{bmatrix}$,\n  - $\\boldsymbol{y} = [1.0]$,\n  - evaluate $s \\in \\{1.0, 0.5, 2.0\\}$.\n\nFor each of the six evaluations (two cases times three scaling factors), compute:\n1. The posterior variance measure $\\operatorname{tr}(\\boldsymbol{A})$ as a single float.\n2. The increment magnitude $\\lVert \\boldsymbol{x}_a - \\boldsymbol{x}_b \\rVert_2$ as a single float.\n\nFinal Output Format:\n- Round each float to six decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the pair $[\\operatorname{tr}(\\boldsymbol{A}), \\lVert \\boldsymbol{x}_a - \\boldsymbol{x}_b \\rVert_2]$ for one evaluation, in the order:\n  - Case 1 with $s=1.0$, Case 1 with $s=0.5$, Case 1 with $s=2.0$, Case 2 with $s=1.0$, Case 2 with $s=0.5$, Case 2 with $s=2.0$.\n- Concretely, the program must print a single line of the form:\n  - $[[v_1,i_1],[v_2,i_2],[v_3,i_3],[v_4,i_4],[v_5,i_5],[v_6,i_6]]$\n  with each $v_k$ and $i_k$ rounded to six decimal places and expressed as decimal numbers (no percentage signs).",
            "solution": "The problem is assessed to be valid. It is scientifically grounded in the principles of variational data assimilation, well-posed with a unique solution guaranteed by the convexity of the cost function, and objective in its formulation. All necessary data and conditions for the two test cases are provided, and there are no contradictions or ambiguities.\n\nThe task is to analyze the sensitivity of a 3D-Var analysis to a scalar perturbation of the background-error covariance matrix. This requires deriving the analysis state and the analysis-error covariance from first principles.\n\nThe 3D-Var cost function, $J(\\boldsymbol{x})$, which is to be minimized, represents the negative logarithm of the posterior probability density function under Gaussian error assumptions. It is composed of two quadratic terms: a background term measuring the departure of the state vector $\\boldsymbol{x} \\in \\mathbb{R}^n$ from the background state $\\boldsymbol{x}_b \\in \\mathbb{R}^n$, and an observation term measuring the departure of the projected state $\\boldsymbol{H}\\boldsymbol{x}$ from the observations $\\boldsymbol{y} \\in \\mathbb{R}^m$.\n\n$$\nJ(\\boldsymbol{x}) = \\frac{1}{2}(\\boldsymbol{x} - \\boldsymbol{x}_b)^T \\boldsymbol{B}^{-1} (\\boldsymbol{x} - \\boldsymbol{x}_b) + \\frac{1}{2}(\\boldsymbol{y} - \\boldsymbol{H}\\boldsymbol{x})^T \\boldsymbol{R}^{-1} (\\boldsymbol{y} - \\boldsymbol{H}\\boldsymbol{x})\n$$\n\nHere, $\\boldsymbol{B} \\in \\mathbb{R}^{n \\times n}$ is the background-error covariance matrix and $\\boldsymbol{R} \\in \\mathbb{R}^{m \\times m}$ is the observation-error covariance matrix. Both are given as symmetric and positive definite, which ensures that $J(\\boldsymbol{x})$ is a strictly convex function and has a unique minimum.\n\nTo find the analysis state $\\boldsymbol{x}_a$ that minimizes $J(\\boldsymbol{x})$, we must find the point where the gradient of the cost function with respect to $\\boldsymbol{x}$ is zero. The gradient, using standard rules of matrix calculus, is:\n\n$$\n\\nabla_{\\boldsymbol{x}} J(\\boldsymbol{x}) = \\boldsymbol{B}^{-1}(\\boldsymbol{x} - \\boldsymbol{x}_b) + \\boldsymbol{H}^T \\boldsymbol{R}^{-1}(\\boldsymbol{H}\\boldsymbol{x} - \\boldsymbol{y})\n$$\n\nSetting the gradient to zero at $\\boldsymbol{x} = \\boldsymbol{x}_a$:\n\n$$\n\\boldsymbol{B}^{-1}(\\boldsymbol{x}_a - \\boldsymbol{x}_b) + \\boldsymbol{H}^T \\boldsymbol{R}^{-1}(\\boldsymbol{H}\\boldsymbol{x}_a - \\boldsymbol{y}) = 0\n$$\n\nRearranging the terms to solve for $\\boldsymbol{x}_a$:\n\n$$\n\\boldsymbol{B}^{-1}\\boldsymbol{x}_a - \\boldsymbol{B}^{-1}\\boldsymbol{x}_b + \\boldsymbol{H}^T \\boldsymbol{R}^{-1}\\boldsymbol{H}\\boldsymbol{x}_a - \\boldsymbol{H}^T \\boldsymbol{R}^{-1}\\boldsymbol{y} = 0\n$$\n$$\n(\\boldsymbol{B}^{-1} + \\boldsymbol{H}^T \\boldsymbol{R}^{-1}\\boldsymbol{H})\\boldsymbol{x}_a = \\boldsymbol{B}^{-1}\\boldsymbol{x}_b + \\boldsymbol{H}^T \\boldsymbol{R}^{-1}\\boldsymbol{y}\n$$\n\nThe analysis state is therefore:\n$$\n\\boldsymbol{x}_a = (\\boldsymbol{B}^{-1} + \\boldsymbol{H}^T \\boldsymbol{R}^{-1}\\boldsymbol{H})^{-1} (\\boldsymbol{B}^{-1}\\boldsymbol{x}_b + \\boldsymbol{H}^T \\boldsymbol{R}^{-1}\\boldsymbol{y})\n$$\n\nThe analysis-error covariance matrix, $\\boldsymbol{A}$, is the inverse of the Hessian of the cost function evaluated at the minimum. The Hessian is the matrix of second derivatives:\n\n$$\n\\boldsymbol{A} = (\\nabla^2_{\\boldsymbol{x}} J(\\boldsymbol{x}))^{-1} = (\\boldsymbol{B}^{-1} + \\boldsymbol{H}^T \\boldsymbol{R}^{-1}\\boldsymbol{H})^{-1}\n$$\n\nWhile these forms are correct, they involve inverting $\\boldsymbol{B}$ and $\\boldsymbol{R}$, which can be computationally expensive for large state and observation spaces. An alternative, more practical formulation can be derived. Let us express the analysis increment, $\\delta\\boldsymbol{x} = \\boldsymbol{x}_a - \\boldsymbol{x}_b$, by rearranging the gradient equation:\n\n$$\n(\\boldsymbol{B}^{-1} + \\boldsymbol{H}^T \\boldsymbol{R}^{-1}\\boldsymbol{H})(\\boldsymbol{x}_a - \\boldsymbol{x}_b) = \\boldsymbol{H}^T \\boldsymbol{R}^{-1}(\\boldsymbol{y} - \\boldsymbol{H}\\boldsymbol{x}_b)\n$$\n$$\n\\boldsymbol{x}_a - \\boldsymbol{x}_b = (\\boldsymbol{B}^{-1} + \\boldsymbol{H}^T \\boldsymbol{R}^{-1}\\boldsymbol{H})^{-1} \\boldsymbol{H}^T \\boldsymbol{R}^{-1}(\\boldsymbol{y} - \\boldsymbol{H}\\boldsymbol{x}_b)\n$$\n\nUsing the Woodbury matrix identity, we can write $(\\boldsymbol{B}^{-1} + \\boldsymbol{H}^T \\boldsymbol{R}^{-1}\\boldsymbol{H})^{-1} = \\boldsymbol{B} - \\boldsymbol{B}\\boldsymbol{H}^T(\\boldsymbol{H}\\boldsymbol{B}\\boldsymbol{H}^T + \\boldsymbol{R})^{-1}\\boldsymbol{H}\\boldsymbol{B}$. This is the expression for $\\boldsymbol{A}$.\nSubstituting this into the increment equation is cumbersome. A more direct route is to define the Kalman gain matrix $\\boldsymbol{K}$:\n\n$$\n\\boldsymbol{K} = \\boldsymbol{B}\\boldsymbol{H}^T(\\boldsymbol{H}\\boldsymbol{B}\\boldsymbol{H}^T + \\boldsymbol{R})^{-1}\n$$\n\nWith this definition, the analysis increment and analysis-error covariance can be expressed efficiently as:\n\n1.  Analysis Increment: $\\boldsymbol{x}_a - \\boldsymbol{x}_b = \\boldsymbol{K}(\\boldsymbol{y} - \\boldsymbol{H}\\boldsymbol{x}_b)$\n2.  Analysis-Error Covariance: $\\boldsymbol{A} = (\\boldsymbol{I} - \\boldsymbol{K}\\boldsymbol{H})\\boldsymbol{B}$\n\nThis formulation avoids inverting the large matrix $\\boldsymbol{B}$ and instead requires inverting the smaller matrix $(\\boldsymbol{H}\\boldsymbol{B}\\boldsymbol{H}^T + \\boldsymbol{R})$ of size $m \\times m$.\n\nThe problem requires evaluating the sensitivity to scaling $\\boldsymbol{B}$ by a factor $s \\in \\{1.0, 0.5, 2.0\\}$. We define $\\boldsymbol{B}_s = s\\boldsymbol{B}$. The algorithm for each evaluation is as follows:\n\nLet $\\boldsymbol{B}_s = s\\boldsymbol{B}$.\n1.  Compute the Kalman gain matrix for the scaled problem:\n    $$\n    \\boldsymbol{K}_s = \\boldsymbol{B}_s \\boldsymbol{H}^T (\\boldsymbol{H} \\boldsymbol{B}_s \\boldsymbol{H}^T + \\boldsymbol{R})^{-1}\n    $$\n2.  Compute the analysis increment vector:\n    $$\n    \\delta\\boldsymbol{x}_s = \\boldsymbol{x}_{a,s} - \\boldsymbol{x}_b = \\boldsymbol{K}_s(\\boldsymbol{y} - \\boldsymbol{H}\\boldsymbol{x}_b)\n    $$\n3.  Compute the first metric, the magnitude of the analysis increment:\n    $$\n    \\text{Metric 2: } \\lVert \\delta\\boldsymbol{x}_s \\rVert_2\n    $$\n4.  Compute the analysis-error covariance matrix for the scaled problem:\n    $$\n    \\boldsymbol{A}_s = (\\boldsymbol{I} - \\boldsymbol{K}_s\\boldsymbol{H})\\boldsymbol{B}_s\n    $$\n5.  Compute the second metric, the posterior variance measure:\n    $$\n    \\text{Metric 1: } \\operatorname{tr}(\\boldsymbol{A}_s)\n    $$\n\nThese five steps are executed for each test case and for each value of $s$. The results are then collated and formatted as per the problem specification.\n\nFor Case 1, with scalar quantities ($n=1, m=1$): $\\boldsymbol{B}=[1.0]$, $\\boldsymbol{R}=[0.25]$, $\\boldsymbol{H}=[1.0]$, $\\boldsymbol{x}_b=[0.0]$, $\\boldsymbol{y}=[2.0]$.\nFor $s=1.0$: $\\boldsymbol{B}_s=[1.0]$. The gain is $K_s = 1.0 \\times (1.0 \\times 1.0 \\times 1.0 + 0.25)^{-1} = 0.8$. The increment is $0.8 \\times (2.0 - 0.0) = 1.6$. The norm is $1.6$. The analysis covariance is $A_s = (1 - 0.8 \\times 1.0) \\times 1.0 = 0.2$. The trace is $0.2$.\n\nFor Case 2, with a 2-dimensional state ($n=2, m=1$): $\\boldsymbol{B} = \\begin{bmatrix}1.0  0.3 \\\\ 0.3  1.5\\end{bmatrix}$, $\\boldsymbol{R} = [4.0]$, $\\boldsymbol{H} = \\begin{bmatrix}1.0  0.0\\end{bmatrix}$, $\\boldsymbol{x}_b = \\begin{bmatrix}0.0 \\\\ 0.0\\end{bmatrix}$, $\\boldsymbol{y} = [1.0]$.\nFor $s=1.0$: $\\boldsymbol{B}_s = \\boldsymbol{B}$. The innovation is $\\boldsymbol{y} - \\boldsymbol{H}\\boldsymbol{x}_b = [1.0]$. The term $\\boldsymbol{H}\\boldsymbol{B}_s\\boldsymbol{H}^T + \\boldsymbol{R}$ becomes $[1.0] + [4.0] = [5.0]$. The gain matrix is $\\boldsymbol{K}_s = \\begin{bmatrix}1.0 \\\\ 0.3\\end{bmatrix} [5.0]^{-1} = \\begin{bmatrix}0.2 \\\\ 0.06\\end{bmatrix}$. The increment is $\\begin{bmatrix}0.2 \\\\ 0.06\\end{bmatrix} \\times 1.0 = \\begin{bmatrix}0.2 \\\\ 0.06\\end{bmatrix}$. The norm is $\\sqrt{0.2^2 + 0.06^2} \\approx 0.208806$. The analysis covariance is $\\boldsymbol{A}_s = (\\boldsymbol{I} - \\boldsymbol{K}_s\\boldsymbol{H})\\boldsymbol{B}_s = \\begin{bmatrix}0.8  0.24 \\\\ 0.24  1.482\\end{bmatrix}$. The trace is $0.8 + 1.482 = 2.282$.\n\nThe remaining calculations follow the same procedure for the other values of $s$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the 3D-Var sensitivity analysis problem.\n\n    The function computes two metrics for two different test cases, each evaluated\n    with three different scaling factors for the background-error covariance matrix.\n    The metrics are:\n    1. The trace of the analysis-error covariance matrix.\n    2. The Euclidean norm of the analysis increment.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"n\": 1,\n            \"B\": np.array([[1.0]]),\n            \"R\": np.array([[0.25]]),\n            \"H\": np.array([[1.0]]),\n            \"xb\": np.array([0.0]),\n            \"y\": np.array([2.0]),\n        },\n        {\n            \"n\": 2,\n            \"B\": np.array([[1.0, 0.3], [0.3, 1.5]]),\n            \"R\": np.array([[4.0]]),\n            \"H\": np.array([[1.0, 0.0]]),\n            \"xb\": np.array([0.0, 0.0]),\n            \"y\": np.array([1.0]),\n        }\n    ]\n\n    scales = [1.0, 0.5, 2.0]\n    \n    all_results = []\n\n    for case in test_cases:\n        n = case[\"n\"]\n        B = case[\"B\"]\n        R = case[\"R\"]\n        H = case[\"H\"]\n        xb = case[\"xb\"]\n        y = case[\"y\"]\n\n        for s in scales:\n            # Scale the background-error covariance matrix B\n            Bs = s * B\n\n            # Compute the innovation vector (d = y - H*xb)\n            innovation = y - H @ xb\n\n            # Compute the Kalman gain matrix K_s\n            # K_s = B_s * H^T * (H * B_s * H^T + R)^-1\n            HBH_T = H @ Bs @ H.T\n            inv_term = np.linalg.inv(HBH_T + R)\n            Ks = Bs @ H.T @ inv_term\n\n            # 1. Compute the analysis increment and its Euclidean norm\n            # inc = K_s * (y - H*xb)\n            increment = Ks @ innovation\n            increment_norm = np.linalg.norm(increment)\n\n            # 2. Compute the analysis-error covariance A_s and its trace\n            # A_s = (I - K_s * H) * B_s\n            identity_n = np.eye(n)\n            As = (identity_n - Ks @ H) @ Bs\n            trace_A = np.trace(As)\n\n            # Append the pair of results for this evaluation\n            all_results.append([trace_A, increment_norm])\n\n    # Format the final output string as specified in the problem statement.\n    # e.g., [[v1,i1],[v2,i2],...] with 6 decimal places.\n    formatted_results = [f\"[{res[0]:.6f},{res[1]:.6f}]\" for res in all_results]\n    final_output = f\"[{','.join(formatted_results)}]\"\n    \n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "Real-world systems are rarely linear, and the nonlinearity of observation operators poses a significant challenge for variational assimilation, as the cost function becomes non-quadratic and difficult to minimize. This exercise demonstrates the widely-used incremental method, which iteratively approximates the solution by solving a sequence of quadratic problems. By comparing a one-loop analysis to a more refined two-loop analysis, you will directly quantify the benefits of re-linearizing the problem and gain an appreciation for handling nonlinearity in practice. ",
            "id": "4108403",
            "problem": "Consider the standard Bayesian formulation of three-dimensional variational assimilation (3D-Var) and four-dimensional variational assimilation (4D-Var), where Gaussian error assumptions lead to a quadratic penalization of departures from prior and observations. In 3D-Var, the analysis state is obtained by minimizing a scalar-valued cost function that measures the misfit to prior information and to observations under a possibly nonlinear observation operator. Let the state vector be $x \\in \\mathbb{R}^n$, the prior (background) state be $x_b \\in \\mathbb{R}^n$ with covariance $B \\in \\mathbb{R}^{n \\times n}$, and the observation vector be $y \\in \\mathbb{R}^m$ with covariance $R \\in \\mathbb{R}^{m \\times m}$. The observation operator $h:\\mathbb{R}^n \\to \\mathbb{R}^m$ is generally nonlinear. The 3D-Var objective function is\n$$\nJ(x) = \\frac{1}{2}(x - x_b)^\\top B^{-1}(x - x_b) + \\frac{1}{2}\\big(h(x) - y\\big)^\\top R^{-1}\\big(h(x) - y\\big).\n$$\nIn the incremental approach, the nonlinear observation operator is linearized about a reference state $x_0$ using the first-order Taylor expansion $h(x_0 + \\delta x) \\approx h(x_0) + H(x_0)\\delta x$, where $H(x_0) \\in \\mathbb{R}^{m \\times n}$ is the Jacobian matrix of $h$ evaluated at $x_0$. This yields a quadratic incremental cost whose minimizer produces an update $\\delta x$ for the outer-loop reference state. The number of outer loops controls the extent to which the linearization point is updated and relinearized, thereby affecting the analysis in nonlinear settings. The purpose of this problem is to quantify the impact of the linearization error on the final analysis by comparing one and two outer loops in a controlled nonlinear toy problem.\n\nYou are to implement, analyze, and compare one versus two outer-loop incremental 3D-Var analyses for the following toy problem:\n\n- Dimension and structure: Let $n=m=5$. Let the true state be fixed as $x_{\\text{true}} = [1.5,-1.0,0.5,-0.8,1.2]^\\top$.\n- Nonlinear observation operator: For a scalar parameter $\\gamma \\ge 0$, define the component-wise nonlinear operator\n$$\nh(x)_i = x_i + \\gamma x_i^3, \\quad i = 1,\\dots,n,\n$$\nand its Jacobian at $x$ as the diagonal matrix $H(x) = \\operatorname{diag}\\big(1 + 3\\gamma x_i^2\\big)$.\n- Covariances: Let $B = \\operatorname{diag}(\\sigma_b^2,\\dots,\\sigma_b^2)$ and $R = \\operatorname{diag}(\\sigma_r^2,\\dots,\\sigma_r^2)$.\n- Background state: Let $x_b = x_{\\text{true}} + \\alpha \\cdot p$, where $p = [0.7,-0.7,0.3,-0.3,0.5]^\\top$ and $\\alpha \\ge 0$ scales the background error.\n- Observations: Let $y = h(x_{\\text{true}}) + \\beta \\cdot q$, where $q = [0.1,-0.05,0.03,-0.02,0.0]^\\top$ and $\\beta \\ge 0$ scales a fixed deterministic observation error vector. No random numbers may be used.\n\nDefine the incremental 3D-Var update at a linearization point $x_0$ by minimizing the quadratic approximation\n$$\n\\tilde{J}(\\delta x; x_0) = \\frac{1}{2}\\delta x^\\top B^{-1}\\delta x + \\frac{1}{2}\\big(H(x_0)\\delta x - d(x_0)\\big)^\\top R^{-1}\\big(H(x_0)\\delta x - d(x_0)\\big),\n$$\nwhere the innovation at the linearization point is $d(x_0) = y - h(x_0)$. The first-order optimality conditions yield the normal equations\n$$\n\\big(B^{-1} + H(x_0)^\\top R^{-1} H(x_0)\\big)\\delta x = H(x_0)^\\top R^{-1} d(x_0).\n$$\nGiven the diagonal structure, this system decouples across components and admits a closed-form solution for each component $i$:\n$$\n\\delta x_i = \\frac{H_{ii}(x_0) \\, d_i(x_0)/\\sigma_r^2}{1/\\sigma_b^2 + H_{ii}^2(x_0)/\\sigma_r^2}.\n$$\nThe analysis with one outer loop is $x^{(1)} = x_b + \\delta x(x_b)$ obtained by linearizing at $x_0 = x_b$. The analysis with two outer loops is $x^{(2)} = x^{(1)} + \\delta x(x^{(1)})$, where the second update reuses the formula but linearizes at $x_0 = x^{(1)}$.\n\nTo quantify the impact of the linearization error, compute the following metrics for each test case:\n- The Euclidean norm of the analysis error for one loop, $\\|x^{(1)} - x_{\\text{true}}\\|_2$.\n- The Euclidean norm of the analysis error for two loops, $\\|x^{(2)} - x_{\\text{true}}\\|_2$.\n- The improvement in the Euclidean norm due to a second outer loop, $\\|x^{(1)} - x_{\\text{true}}\\|_2 - \\|x^{(2)} - x_{\\text{true}}\\|_2$.\n- A linearization error indicator for one loop defined by the Euclidean norm of the first-order Taylor remainder at $x^{(1)}$ with respect to $x_0 = x_b$,\n$$\nE_{\\text{lin}}^{(1)} = \\big\\| h(x^{(1)}) - \\big(h(x_b) + H(x_b)(x^{(1)} - x_b)\\big) \\big\\|_2.\n$$\n- The Euclidean norm of the final innovation for two loops, $\\|y - h(x^{(2)})\\|_2$.\n\nImplement a program that computes these metrics for the following four test cases, which together form a basic test suite probing different regimes of nonlinearity and background/observation error scales:\n1. $\\gamma = 0.8$, $\\alpha = 1.0$, $\\beta = 1.0$, $\\sigma_b = 0.5$, $\\sigma_r = 0.2$.\n2. $\\gamma = 0.0$, $\\alpha = 1.0$, $\\beta = 1.0$, $\\sigma_b = 0.5$, $\\sigma_r = 0.2$.\n3. $\\gamma = 2.0$, $\\alpha = 2.0$, $\\beta = 0.5$, $\\sigma_b = 0.8$, $\\sigma_r = 0.1$.\n4. $\\gamma = 1.5$, $\\alpha = 0.3$, $\\beta = 0.0$, $\\sigma_b = 0.4$, $\\sigma_r = 0.05$.\n\nThere are no physical units in this problem; all quantities are dimensionless real numbers. Angles are not involved. Percentages must not be used. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each test case, output a list of five floating-point numbers in the order specified above. Aggregate the four lists into a single outer list, so the final output format must be\n$$\n\\big[[m_{11},m_{12},m_{13},m_{14},m_{15}], [m_{21},m_{22},m_{23},m_{24},m_{25}], [m_{31},m_{32},m_{33},m_{34},m_{35}], [m_{41},m_{42},m_{43},m_{44},m_{45}]\\big],\n$$\nwhere $m_{ij}$ denotes the $j$-th metric of the $i$-th test case. The program must be self-contained and must not read any external input nor use any randomness.",
            "solution": "The problem requires the implementation and comparison of one-loop and two-loop incremental 3D-Var analyses for a specified nonlinear toy problem. The analysis is performed by sequentially solving for an increment that minimizes a quadratic cost function, which is a linearization of the full nonlinear cost function.\n\nThe core of the problem lies in the incremental 3D-Var update. At a given linearization point $x_0$, the analysis increment $\\delta x$ is found by minimizing the quadratic cost function:\n$$\n\\tilde{J}(\\delta x; x_0) = \\frac{1}{2}\\delta x^\\top B^{-1}\\delta x + \\frac{1}{2}\\big(H(x_0)\\delta x - d(x_0)\\big)^\\top R^{-1}\\big(H(x_0)\\delta x - d(x_0)\\big)\n$$\nwhere $d(x_0) = y - h(x_0)$ is the innovation vector. The minimizer $\\delta x$ is the solution to the normal equations:\n$$\n\\big(B^{-1} + H(x_0)^\\top R^{-1} H(x_0)\\big)\\delta x = H(x_0)^\\top R^{-1} d(x_0)\n$$\nGiven that the covariance matrices $B = \\sigma_b^2 I$ and $R = \\sigma_r^2 I$ are diagonal, and the Jacobian $H(x_0)$ is also diagonal, this system of linear equations decouples. For each component $i \\in \\{1, \\dots, n\\}$, the equation is:\n$$\n\\left(\\frac{1}{\\sigma_b^2} + \\frac{H_{ii}(x_0)^2}{\\sigma_r^2}\\right) \\delta x_i = \\frac{H_{ii}(x_0) d_i(x_0)}{\\sigma_r^2}\n$$\nThis gives the closed-form solution for each component of the increment vector:\n$$\n\\delta x_i = \\frac{H_{ii}(x_0) \\, d_i(x_0)/\\sigma_r^2}{1/\\sigma_b^2 + H_{ii}^2(x_0)/\\sigma_r^2}\n$$\nwhere $H_{ii}(x_0) = 1 + 3\\gamma (x_0)_i^2$ and $d_i(x_0) = y_i - h((x_0)_i)$.\n\nThe procedure for each test case, defined by parameters $(\\gamma, \\alpha, \\beta, \\sigma_b, \\sigma_r)$, is as follows:\n\n**1. Initialization**\nFirst, we establish the fixed and parametrically defined quantities for a given test case.\n-   The true state is $x_{\\text{true}} = [1.5, -1.0, 0.5, -0.8, 1.2]^\\top$.\n-   The background state is computed as $x_b = x_{\\text{true}} + \\alpha \\cdot p$, using $p = [0.7, -0.7, 0.3, -0.3, 0.5]^\\top$.\n-   The observation operator is $h(x)_i = x_i + \\gamma x_i^3$.\n-   The observations are generated as $y = h(x_{\\text{true}}) + \\beta \\cdot q$, using $q = [0.1, -0.05, 0.03, -0.02, 0.0]^\\top$.\n\n**2. One-Loop Analysis ($x^{(1)}$)**\nThe first outer loop uses the background state $x_b$ as the linearization point.\n-   Set linearization point: $x_0 = x_b$.\n-   Compute the innovation vector: $d(x_b) = y - h(x_b)$.\n-   Compute the diagonal Jacobian elements: $H_{ii}(x_b) = 1 + 3\\gamma (x_b)_i^2$.\n-   Calculate the first increment vector, $\\delta x^{(1)} = \\delta x(x_b)$, using the component-wise formula above.\n-   The one-loop analysis is then $x^{(1)} = x_b + \\delta x^{(1)}$.\n\n**3. Two-Loop Analysis ($x^{(2)}$)**\nThe second outer loop updates the linearization point to the result of the first loop, $x^{(1)}$.\n-   Set new linearization point: $x_0' = x^{(1)}$.\n-   Compute the new innovation vector: $d(x^{(1)}) = y - h(x^{(1)})$.\n-   Compute the new diagonal Jacobian elements: $H_{ii}(x^{(1)}) = 1 + 3\\gamma (x^{(1)})_i^2$.\n-   Calculate the second increment vector, $\\delta x^{(2)} = \\delta x(x^{(1)})$, using the same component-wise formula but with the updated inputs.\n-   The two-loop analysis is $x^{(2)} = x^{(1)} + \\delta x^{(2)}$.\n\n**4. Metric Computation**\nWith the analysis states $x^{(1)}$ and $x^{(2)}$ computed, we calculate the five required metrics. These metrics quantify the accuracy of the analyses and the impact of the nonlinearity.\n-   $m_1 = \\|x^{(1)} - x_{\\text{true}}\\|_2$: Euclidean norm of the one-loop analysis error.\n-   $m_2 = \\|x^{(2)} - x_{\\text{true}}\\|_2$: Euclidean norm of the two-loop analysis error.\n-   $m_3 = m_1 - m_2$: Improvement in analysis error norm from one to two loops. A positive value indicates that the second loop improved the analysis.\n-   $m_4 = E_{\\text{lin}}^{(1)} = \\big\\| h(x^{(1)}) - \\big(h(x_b) + H(x_b)(x^{(1)} - x_b)\\big) \\big\\|_2$: This metric measures the error in the first-order Taylor approximation of $h(x)$ around $x_b$ when evaluated at $x^{(1)}$. It is an indicator of how much the nonlinearity affects the first analysis step.\n-   $m_5 = \\|y - h(x^{(2)})\\|_2$: The Euclidean norm of the final innovation, or observation-minus-forecast departure, after the second loop. This measures how well the final analysis fits the observations in observation space.\n\nThis entire procedure is deterministic and will be implemented computationally. A key insight for implementation is that since all operations are component-wise, they can be efficiently handled using vectorized computations in a library like NumPy. This avoids explicit loops over the vector components. For each of the four test cases, this sequence of calculations will be performed to generate a list of the five metrics. The final result is an aggregation of these lists.\nFor the special case where $\\gamma = 0$, the observation operator $h(x) = x$ is linear. Consequently, its Jacobian $H(x) = I$ is the identity matrix, independent of the linearization point. The incremental cost function $\\tilde{J}$ becomes identical to the full cost function $J$, which is quadratic. A single minimization step is sufficient to find the global minimum. Therefore, the second increment $\\delta x^{(2)}$ will be zero, leading to $x^{(2)} = x^{(1)}$ and an improvement metric $m_3 = 0$. The linearization error $m_4$ will also be zero, as a linear function is perfectly represented by its first-order Taylor series.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the incremental 3D-Var problem for the given test cases.\n    \"\"\"\n    \n    # Define the fixed vectors and test cases from the problem statement.\n    x_true = np.array([1.5, -1.0, 0.5, -0.8, 1.2])\n    p = np.array([0.7, -0.7, 0.3, -0.3, 0.5])\n    q = np.array([0.1, -0.05, 0.03, -0.02, 0.0])\n\n    test_cases = [\n        # (gamma, alpha, beta, sigma_b, sigma_r)\n        (0.8, 1.0, 1.0, 0.5, 0.2), # Case 1\n        (0.0, 1.0, 1.0, 0.5, 0.2), # Case 2\n        (2.0, 2.0, 0.5, 0.8, 0.1), # Case 3\n        (1.5, 0.3, 0.0, 0.4, 0.05), # Case 4\n    ]\n\n    all_results = []\n\n    # Define the component-wise nonlinear observation operator and its Jacobian\n    def h(x, gamma):\n        return x + gamma * x**3\n\n    def H_diag(x, gamma):\n        return 1.0 + 3.0 * gamma * x**2\n\n    def calculate_increment(x0, y, gamma, sb, sr):\n        \"\"\"\n        Calculates the analysis increment delta_x at a linearization point x0.\n        \"\"\"\n        d = y - h(x0, gamma)\n        H_ii = H_diag(x0, gamma)\n        \n        var_b = sb**2\n        var_r = sr**2\n        \n        numerator = (H_ii * d) / var_r\n        denominator = 1.0 / var_b + (H_ii**2) / var_r\n        \n        # Handle potential division by zero if denominator is zero, though unlikely here\n        # given the problem constraints (sb > 0, sr > 0).\n        delta_x = numerator / denominator\n        return delta_x\n\n    for case in test_cases:\n        gamma, alpha, beta, sigma_b, sigma_r = case\n\n        # --- 1. Initialization ---\n        xb = x_true + alpha * p\n        y = h(x_true, gamma) + beta * q\n        \n        # --- 2. One-Loop Analysis (x^{(1)}) ---\n        # Linearization point is xb\n        dx1 = calculate_increment(xb, y, gamma, sigma_b, sigma_r)\n        x1 = xb + dx1\n\n        # --- 3. Two-Loop Analysis (x^{(2)}) ---\n        # Linearization point is x1\n        dx2 = calculate_increment(x1, y, gamma, sigma_b, sigma_r)\n        x2 = x1 + dx2\n        \n        # --- 4. Metric Computation ---\n        # Metric 1: ||x^(1) - x_true||_2\n        m1 = np.linalg.norm(x1 - x_true)\n\n        # Metric 2: ||x^(2) - x_true||_2\n        m2 = np.linalg.norm(x2 - x_true)\n\n        # Metric 3: Improvement ||x^(1) - x_true||_2 - ||x^(2) - x_true||_2\n        m3 = m1 - m2\n        \n        # Metric 4: Linearization error E_lin^(1)\n        # E_lin^(1) = || h(x^(1)) - (h(x_b) + H(x_b)(x^(1) - x_b)) ||_2\n        # Note that x^(1) - x_b is just dx1\n        linear_approx_at_x1 = h(xb, gamma) + H_diag(xb, gamma) * dx1\n        m4 = np.linalg.norm(h(x1, gamma) - linear_approx_at_x1)\n\n        # Metric 5: Final innovation norm ||y - h(x^(2))||_2\n        m5 = np.linalg.norm(y - h(x2, gamma))\n        \n        case_results = [m1, m2, m3, m4, m5]\n        all_results.append(case_results)\n\n    # Format the final output string as specified.\n    # The output string should be a list of lists, e.g., [[m11,...,m15],[m21,...,m25],...]\n    result_str = \"[\" + \", \".join([str(res) for res in all_results]) + \"]\"\n\n    # Printing must be exactly in the required format.\n    print(result_str.replace(\" \", \"\"))\n\n\nsolve()\n```"
        },
        {
            "introduction": "The core of any variational assimilation system is a numerical optimizer that relies on the gradient of the cost function to find the best analysis. A subtle error in the implementation of this gradient can lead to silent failures and scientifically invalid results. This practice introduces the \"gradient check,\" an indispensable verification technique where the analytically derived gradient is compared against a numerical finite-difference approximation, ensuring your implementation is correct and robust. ",
            "id": "4108421",
            "problem": "Consider the Bayesian Maximum A Posteriori (MAP) formulation of data assimilation used in three-dimensional variational assimilation (3D-Var) and four-dimensional variational assimilation (4D-Var). Assume Gaussian background and observation errors, linear observation operators, and linear time-stepping models. The negative log-posterior defines a cost functional whose analytical gradient can be derived from first principles of multivariate calculus and linear algebra. Your task is to derive and implement the analytical gradients for both 3D-Var and 4D-Var, and then verify their correctness via finite-difference directional derivative tests over a range of perturbation magnitudes.\n\nDefinitions and assumptions (to be used as the fundamental base for your derivations and implementation):\n- The state vector at a single analysis time is $x \\in \\mathbb{R}^{n}$ and the background (prior) state is $x_{b} \\in \\mathbb{R}^{n}$.\n- The background error covariance is a symmetric positive definite matrix $B \\in \\mathbb{R}^{n \\times n}$.\n- The observation operator at analysis time is $H \\in \\mathbb{R}^{m \\times n}$, and the observation vector is $y \\in \\mathbb{R}^{m}$. The observation error covariance is a symmetric positive definite matrix $R \\in \\mathbb{R}^{m \\times m}$.\n- For four-dimensional variational assimilation, the initial state is $x_{0} \\in \\mathbb{R}^{n}$, the linear model propagator from time $0$ to time $k$ is $M_{0 \\rightarrow k} \\in \\mathbb{R}^{n \\times n}$, the observation operator at time $k$ is $H_{k} \\in \\mathbb{R}^{m_{k} \\times n}$ with observation vector $y_{k} \\in \\mathbb{R}^{m_{k}}$, and the observation error covariance is $R_{k} \\in \\mathbb{R}^{m_{k} \\times m_{k}}$.\n\nTasks:\n1. Using the MAP framework, write the 3D-Var cost functional $J_{3\\mathrm{D}}(x)$ and the 4D-Var cost functional $J_{4\\mathrm{D}}(x_{0})$ in terms of $x$, $x_{b}$, $B$, $H$, $y$, $R$, and, for 4D-Var, $M_{0 \\rightarrow k}$, $H_{k}$, $y_{k}$, and $R_{k}$. Then, from first principles, derive expressions for their analytical gradients with respect to $x$ (for 3D-Var) and $x_{0}$ (for 4D-Var). Do not use or assume any formula not derived from these definitions.\n2. Implement functions to evaluate $J_{3\\mathrm{D}}(x)$ and its gradient, and $J_{4\\mathrm{D}}(x_{0})$ and its gradient, using the matrices and vectors provided in the test suite below. All linear solves involving covariance matrices must avoid explicit matrix inversion; use linear system solves.\n3. Verify gradient correctness using finite-difference directional derivative tests. For a given direction $v \\in \\mathbb{R}^{n}$ and perturbation magnitude $\\epsilon  0$, define the scalar function $\\phi(\\epsilon) = J(z + \\epsilon v)$, where $z$ is the base point ($x$ for 3D-Var or $x_{0}$ for 4D-Var). Compute the central-difference approximation to the directional derivative,\n   $$ D_{\\mathrm{FD}}(\\epsilon) = \\frac{\\phi(\\epsilon) - \\phi(-\\epsilon)}{2 \\epsilon}, $$\n   and compare it to the analytical directional derivative $D_{\\mathrm{AN}} = v^{\\top} \\nabla J(z)$. For numerical robustness, report the relative mismatch\n   $$ E(\\epsilon) = \\frac{\\left| D_{\\mathrm{FD}}(\\epsilon) - D_{\\mathrm{AN}} \\right|}{\\max\\left(1, \\left| D_{\\mathrm{AN}} \\right| \\right)}. $$\n   For each test case, evaluate $E(\\epsilon)$ over a sweep of $\\epsilon$ values and return the maximum value over the sweep.\n4. Your program must output a single list of floating-point values, one per test case, where each value is the maximum relative mismatch over the specified $\\epsilon$ sweep.\n\nTest suite (all matrices and vectors are fully specified):\n- Test case A (3D-Var):\n  - State dimension $n = 3$, observation dimension $m = 2$.\n  - $x_{b} = \\begin{bmatrix} 1.0 \\\\ -1.0 \\\\ 0.5 \\end{bmatrix}$.\n  - $B = \\begin{bmatrix} 0.5  0.1  0.0 \\\\ 0.1  0.3  0.05 \\\\ 0.0  0.05  0.2 \\end{bmatrix}$.\n  - $H = \\begin{bmatrix} 1.0  0.5  0.0 \\\\ 0.0  1.0  -0.2 \\end{bmatrix}$.\n  - $R = \\begin{bmatrix} 0.2  0.05 \\\\ 0.05  0.1 \\end{bmatrix}$.\n  - $y = \\begin{bmatrix} 1.2 \\\\ -0.3 \\end{bmatrix}$.\n  - Base point $x = \\begin{bmatrix} 0.8 \\\\ -0.7 \\\\ 0.3 \\end{bmatrix}$, direction $v = \\begin{bmatrix} 0.3 \\\\ -0.4 \\\\ 0.5 \\end{bmatrix}$.\n\n- Test case B (3D-Var):\n  - State and observation dimensions $n = m = 5$.\n  - $x_{b} = \\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 0.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix}$.\n  - $B = \\mathrm{diag}\\!\\left( 10^{-2}, 10^{-1}, 1, 10, 100 \\right)$.\n  - $H = I_{5}$.\n  - $R = \\mathrm{diag}\\!\\left( 0.5, 0.05, 0.2, 2.0, 5.0 \\right)$.\n  - $y = \\begin{bmatrix} 0.1 \\\\ -0.2 \\\\ 0.3 \\\\ -0.4 \\\\ 0.5 \\end{bmatrix}$.\n  - Base point $x = \\begin{bmatrix} 0.05 \\\\ -0.1 \\\\ 0.25 \\\\ -0.35 \\\\ 0.6 \\end{bmatrix}$, direction $v = \\begin{bmatrix} 1.0 \\\\ 1.0 \\\\ 1.0 \\\\ 1.0 \\\\ 1.0 \\end{bmatrix}$.\n\n- Test case C (4D-Var):\n  - State dimension $n = 3$, time steps $K = 3$, with $M_{0 \\rightarrow k} = A^{k}$ where\n    $$ A = \\begin{bmatrix} 1.0  0.1  0.0 \\\\ 0.0  1.0  0.1 \\\\ 0.0  0.0  1.0 \\end{bmatrix}. $$\n  - $x_{b} = \\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix}$, $B = \\mathrm{diag}\\!\\left( 0.3, 0.3, 0.3 \\right)$.\n  - For all $k \\in \\{1,2,3\\}$, $H_{k} = \\begin{bmatrix} 1.0  0.0  0.0 \\\\ 0.0  1.0  0.3 \\end{bmatrix}$ and $R_{k} = \\begin{bmatrix} 0.05  0.01 \\\\ 0.01  0.08 \\end{bmatrix}$.\n  - Observations: $y_{1} = \\begin{bmatrix} 0.95 \\\\ -0.45 \\end{bmatrix}$, $y_{2} = \\begin{bmatrix} 1.05 \\\\ -0.55 \\end{bmatrix}$, $y_{3} = \\begin{bmatrix} 1.00 \\\\ -0.50 \\end{bmatrix}$.\n  - Base point $x_{0} = \\begin{bmatrix} 0.8 \\\\ -0.6 \\\\ 0.2 \\end{bmatrix}$, direction $v = \\begin{bmatrix} 0.2 \\\\ -0.1 \\\\ 0.3 \\end{bmatrix}$.\n\n- Test case D (4D-Var):\n  - State dimension $n = 4$, time steps $K = 2$, with $M_{0 \\rightarrow k} = A^{k}$ where\n    $$ A = \\begin{bmatrix} 1.0  0.05  0.0  0.0 \\\\ 0.0  1.0  0.05  0.0 \\\\ 0.0  0.0  1.0  0.05 \\\\ 0.0  0.0  0.0  1.0 \\end{bmatrix}. $$\n  - $x_{b} = \\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix}$, $B = \\mathrm{diag}\\!\\left( 0.5, 0.2, 0.1, 0.05 \\right)$.\n  - $H_{1} = H_{2} = I_{4}$, $R_{1} = \\mathrm{diag}\\!\\left( 0.1, 0.2, 0.3, 0.4 \\right)$, $R_{2} = \\mathrm{diag}\\!\\left( 0.2, 0.1, 0.4, 0.3 \\right)$.\n  - Observations: $y_{1} = \\begin{bmatrix} 0.1 \\\\ -0.1 \\\\ 0.2 \\\\ -0.2 \\end{bmatrix}$, $y_{2} = \\begin{bmatrix} 0.15 \\\\ -0.05 \\\\ 0.25 \\\\ -0.15 \\end{bmatrix}$.\n  - Base point $x_{0} = \\begin{bmatrix} 0.05 \\\\ -0.08 \\\\ 0.12 \\\\ -0.05 \\end{bmatrix}$, direction $v = \\begin{bmatrix} 0.5 \\\\ -0.3 \\\\ 0.2 \\\\ 0.1 \\end{bmatrix}$.\n\nFinite-difference sweep:\n- For each test case, evaluate the relative mismatch $E(\\epsilon)$ for the set of perturbations\n  $$ \\epsilon \\in \\left\\{ 10^{-1}, 3 \\times 10^{-2}, 10^{-2}, 3 \\times 10^{-3}, 10^{-3}, 3 \\times 10^{-4}, 10^{-4}, 3 \\times 10^{-5}, 10^{-5} \\right\\}. $$\n- For each test case, return the maximum of $E(\\epsilon)$ over this set.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order [A, B, C, D], where each entry is the maximum relative mismatch for the corresponding test case. For example, it should look like\n  $$ \\texttt{[result\\_A,result\\_B,result\\_C,result\\_D]} $$\nwith each result written as a standard floating-point number. No physical units are involved. All angles, if any, should be interpreted in radians. Percentages, if any, must be represented as decimals, but this test suite does not require percentages.",
            "solution": "The problem requires the derivation and implementation of the cost functions and their gradients for three-dimensional (3D-Var) and four-dimensional (4D-Var) variational data assimilation, followed by numerical verification of the gradients. The framework is based on Bayesian Maximum A Posteriori (MAP) estimation, assuming Gaussian error distributions and linear operators.\n\n### Part 1: 3D-Var Derivation\n\nIn 3D-Var, we seek the most probable state $x \\in \\mathbb{R}^{n}$ at a single analysis time, given a background (prior) estimate $x_b \\in \\mathbb{R}^{n}$ and a set of observations $y \\in \\mathbb{R}^{m}$. According to Bayes' theorem, the posterior probability density function (PDF) $P(x|y)$ is proportional to the product of the likelihood $P(y|x)$ and the prior $P(x)$:\n$$\nP(x|y) \\propto P(y|x) P(x)\n$$\nWe assume the prior knowledge about the state is described by a Gaussian PDF centered on the background state $x_b$ with a covariance matrix $B \\in \\mathbb{R}^{n \\times n}$. $B$ is symmetric and positive definite.\n$$\nP(x) \\propto \\exp\\left(-\\frac{1}{2}(x - x_b)^\\top B^{-1} (x - x_b)\\right)\n$$\nSimilarly, we assume the observation errors are Gaussian with zero mean and a symmetric positive definite covariance matrix $R \\in \\mathbb{R}^{m \\times m}$. The observations $y$ are related to the true state $x$ by a linear observation operator $H \\in \\mathbb{R}^{m \\times n}$, such that $y = Hx + \\epsilon_o$, where $\\epsilon_o \\sim \\mathcal{N}(0, R)$. The likelihood of observing $y$ given a state $x$ is:\n$$\nP(y|x) \\propto \\exp\\left(-\\frac{1}{2}(y - Hx)^\\top R^{-1} (y - Hx)\\right)\n$$\nMaximizing the posterior probability $P(x|y)$ is equivalent to minimizing its negative logarithm. This defines the 3D-Var cost function $J_{3\\mathrm{D}}(x)$, which, omitting constant terms, is:\n$$\nJ_{3\\mathrm{D}}(x) = \\frac{1}{2}(x - x_b)^\\top B^{-1} (x - x_b) + \\frac{1}{2}(y - Hx)^\\top R^{-1} (y - Hx)\n$$\nThis function consists of two quadratic penalty terms: the background term, which measures the squared Mahalanobis distance of the analysis $x$ from the background $x_b$, and the observation term, which measures the squared Mahalanobis distance of the model-projected observations $Hx$ from the actual observations $y$.\n\nTo find the minimum of $J_{3\\mathrm{D}}(x)$, we need its gradient with respect to $x$, denoted $\\nabla J_{3\\mathrm{D}}(x)$. We derive this using standard rules of multivariate calculus. Let the background term be $J_b(x)$ and the observation term be $J_o(x)$.\nThe gradient of the background term $J_b(x) = \\frac{1}{2}(x - x_b)^\\top B^{-1} (x - x_b)$ is:\n$$\n\\nabla_x J_b(x) = B^{-1} (x - x_b)\n$$\nThis follows from the rule $\\nabla_z (\\frac{1}{2} z^\\top A z) = Az$ for symmetric $A$, applied with $z = x - x_b$.\n\nThe gradient of the observation term $J_o(x) = \\frac{1}{2}(y - Hx)^\\top R^{-1} (y - Hx)$ is found using the chain rule. Let $d(x) = y - Hx$. Then $J_o = \\frac{1}{2}d^\\top R^{-1} d$. The gradient is $(\\frac{\\partial d}{\\partial x})^\\top \\nabla_d J_o$.\nThe Jacobian of $d(x)$ is $\\frac{\\partial d}{\\partial x} = -H$. The gradient of $J_o$ with respect to $d$ is $\\nabla_d J_o = R^{-1}d$. Thus:\n$$\n\\nabla_x J_o(x) = (-H)^\\top (R^{-1} (y - Hx)) = -H^\\top R^{-1} (y - Hx) = H^\\top R^{-1} (Hx - y)\n$$\nCombining the gradients of the two terms, the full gradient of the 3D-Var cost function is:\n$$\n\\nabla J_{3\\mathrm{D}}(x) = B^{-1}(x - x_b) + H^\\top R^{-1} (Hx - y)\n$$\n\n### Part 2: 4D-Var Derivation\n\nIn 4D-Var, the control variable is the initial state of the system, $x_0 \\in \\mathbb{R}^{n}$, at the beginning of an assimilation window (time $t=0$). The state at any future time $k$ is obtained by propagating the initial state forward with a linear model, $x_k = M_{0 \\rightarrow k} x_0$, where $M_{0 \\rightarrow k} \\in \\mathbb{R}^{n \\times n}$ is the linear propagator from time $0$ to $k$. Observations $y_k \\in \\mathbb{R}^{m_k}$ are available at multiple times $k=1, \\dots, K$.\n\nThe cost function $J_{4\\mathrm{D}}(x_0)$ is analogous to the 3D-Var case. The background term penalizes deviation of the initial state $x_0$ from the background $x_b$. The observation term is a sum of penalties for misfits at all observation times.\n$$\nJ_{4\\mathrm{D}}(x_0) = \\frac{1}{2}(x_0 - x_b)^\\top B^{-1} (x_0 - x_b) + \\frac{1}{2} \\sum_{k=1}^K (y_k - H_k x_k)^\\top R_k^{-1} (y_k - H_k x_k)\n$$\nSubstituting $x_k = M_{0 \\rightarrow k} x_0$, we obtain the cost function in terms of the control variable $x_0$:\n$$\nJ_{4\\mathrm{D}}(x_0) = \\frac{1}{2}(x_0 - x_b)^\\top B^{-1} (x_0 - x_b) + \\frac{1}{2} \\sum_{k=1}^K (y_k - H_k M_{0 \\rightarrow k} x_0)^\\top R_k^{-1} (y_k - H_k M_{0 \\rightarrow k} x_0)\n$$\nTo derive the gradient $\\nabla J_{4\\mathrm{D}}(x_0)$, we differentiate with respect to $x_0$. The gradient of the background term is identical in form to the 3D-Var case:\n$$\n\\nabla_{x_0} J_b(x_0) = B^{-1} (x_0 - x_b)\n$$\nThe observation term is a sum, so its gradient is the sum of the gradients of each term. For a single time $k$, let the operator be $\\mathcal{H}_k = H_k M_{0 \\rightarrow k}$. The term is $\\frac{1}{2}(y_k - \\mathcal{H}_k x_0)^\\top R_k^{-1} (y_k - \\mathcal{H}_k x_0)$. Using the same logic as for the 3D-Var observation term gradient:\n$$\n\\nabla_{x_0} J_{o,k}(x_0) = \\mathcal{H}_k^\\top R_k^{-1} (\\mathcal{H}_k x_0 - y_k) = (H_k M_{0 \\rightarrow k})^\\top R_k^{-1} (H_k M_{0 \\rightarrow k} x_0 - y_k)\n$$\nThis can be written as $M_{0 \\rightarrow k}^\\top H_k^\\top R_k^{-1} (H_k M_{0 \\rightarrow k} x_0 - y_k)$. The matrix $M_{0 \\rightarrow k}^\\top$ is the adjoint of the forward model propagator.\nThe full 4D-Var gradient is the sum over all time steps plus the background gradient:\n$$\n\\nabla J_{4\\mathrm{D}}(x_0) = B^{-1}(x_0 - x_b) + \\sum_{k=1}^K M_{0 \\rightarrow k}^\\top H_k^\\top R_k^{-1} (H_k M_{0 \\rightarrow k} x_0 - y_k)\n$$\n\n### Part 3: Numerical Verification\n\nThe correctness of the derived analytical gradients is verified using a finite-difference directional derivative test. For a scalar function $J(z)$, its directional derivative at point $z$ in the direction $v$ is given by $(\\nabla J(z))^\\top v$. We define a scalar function $\\phi(\\epsilon) = J(z + \\epsilon v)$. The analytical directional derivative is then $D_{\\mathrm{AN}} = \\phi'(0) = (\\nabla J(z))^\\top v$.\nThis can be approximated using a second-order accurate central finite difference:\n$$\nD_{\\mathrm{FD}}(\\epsilon) = \\frac{\\phi(\\epsilon) - \\phi(-\\epsilon)}{2 \\epsilon} = \\frac{J(z + \\epsilon v) - J(z - \\epsilon v)}{2 \\epsilon}\n$$\nTaylor expansion shows that $D_{\\mathrm{FD}}(\\epsilon) = D_{\\mathrm{AN}} + \\mathcal{O}(\\epsilon^2)$. The relative mismatch $E(\\epsilon) = \\frac{|D_{\\mathrm{FD}}(\\epsilon) - D_{\\mathrm{AN}}|}{\\max(1, |D_{\\mathrm{AN}}|)}$ should therefore decrease quadratically as $\\epsilon \\rightarrow 0$, until round-off errors dominate at very small $\\epsilon$. We evaluate the maximum mismatch over a sweep of $\\epsilon$ values to confirm the correctness of the analytical gradient implementation.",
            "answer": "```python\nimport numpy as np\n\ndef cost_3d(x, xb, B, y, H, R):\n    \"\"\"Computes the 3D-Var cost function.\"\"\"\n    dx = x - xb\n    try:\n        B_inv_dx = np.linalg.solve(B, dx)\n    except np.linalg.LinAlgError:\n        return np.inf\n    Jb = 0.5 * dx.T @ B_inv_dx\n\n    innovation = y - H @ x\n    try:\n        R_inv_innovation = np.linalg.solve(R, innovation)\n    except np.linalg.LinAlgError:\n        return np.inf\n    Jo = 0.5 * innovation.T @ R_inv_innovation\n    \n    return Jb + Jo\n\ndef grad_3d(x, xb, B, y, H, R):\n    \"\"\"Computes the gradient of the 3D-Var cost function.\"\"\"\n    B_inv_dx = np.linalg.solve(B, x - xb)\n    \n    innovation = H @ x - y\n    R_inv_innovation = np.linalg.solve(R, innovation)\n    grad_Jo = H.T @ R_inv_innovation\n    \n    return B_inv_dx + grad_Jo\n\ndef cost_4d(x0, xb, B, yk_list, Hk_list, Rk_list, Mk_list):\n    \"\"\"Computes the 4D-Var cost function.\"\"\"\n    dx0 = x0 - xb\n    try:\n        B_inv_dx0 = np.linalg.solve(B, dx0)\n    except np.linalg.LinAlgError:\n        return np.inf\n    Jb = 0.5 * dx0.T @ B_inv_dx0\n    \n    Jo = 0.0\n    for k in range(len(yk_list)):\n        Mk = Mk_list[k]\n        Hk = Hk_list[k]\n        Rk = Rk_list[k]\n        yk = yk_list[k]\n        \n        xk = Mk @ x0\n        innovation = yk - Hk @ xk\n        try:\n            Rk_inv_innovation = np.linalg.solve(Rk, innovation)\n        except np.linalg.LinAlgError:\n            return np.inf\n        Jo += 0.5 * innovation.T @ Rk_inv_innovation\n        \n    return Jb + Jo\n\ndef grad_4d(x0, xb, B, yk_list, Hk_list, Rk_list, Mk_list):\n    \"\"\"Computes the gradient of the 4D-Var cost function.\"\"\"\n    grad_Jb = np.linalg.solve(B, x0 - xb)\n    \n    grad_Jo = np.zeros_like(x0, dtype=float)\n    for k in range(len(yk_list)):\n        Mk = Mk_list[k]\n        Hk = Hk_list[k]\n        Rk = Rk_list[k]\n        yk = yk_list[k]\n        \n        xk = Mk @ x0\n        innovation = Hk @ xk - yk\n        Rk_inv_innovation = np.linalg.solve(Rk, innovation)\n        grad_Jo += Mk.T @ Hk.T @ Rk_inv_innovation\n        \n    return grad_Jb + grad_Jo\n\ndef verify_and_get_max_error(cost_func, grad_func, z, v, eps_sweep, *args):\n    \"\"\"\n    Performs a finite-difference directional derivative test and returns the max error.\n    \"\"\"\n    grad_z = grad_func(z, *args)\n    D_an = v.T @ grad_z\n    \n    mismatches = []\n    for eps in eps_sweep:\n        phi_plus = cost_func(z + eps * v, *args)\n        phi_minus = cost_func(z - eps * v, *args)\n        D_fd = (phi_plus - phi_minus) / (2.0 * eps)\n        \n        error = np.abs(D_fd - D_an) / np.maximum(1.0, np.abs(D_an))\n        mismatches.append(error)\n        \n    return np.max(mismatches)\n\ndef solve():\n    \"\"\"\n    Defines test cases, runs gradient verification, and prints results.\n    \"\"\"\n    eps_sweep = [1e-1, 3e-2, 1e-2, 3e-3, 1e-3, 3e-4, 1e-4, 3e-5, 1e-5]\n\n    test_cases = [\n        {\n            \"name\": \"A\",\n            \"type\": \"3D-Var\",\n            \"xb\": np.array([1.0, -1.0, 0.5]),\n            \"B\": np.array([[0.5, 0.1, 0.0], [0.1, 0.3, 0.05], [0.0, 0.05, 0.2]]),\n            \"H\": np.array([[1.0, 0.5, 0.0], [0.0, 1.0, -0.2]]),\n            \"R\": np.array([[0.2, 0.05], [0.05, 0.1]]),\n            \"y\": np.array([1.2, -0.3]),\n            \"x\": np.array([0.8, -0.7, 0.3]),\n            \"v\": np.array([0.3, -0.4, 0.5]),\n        },\n        {\n            \"name\": \"B\",\n            \"type\": \"3D-Var\",\n            \"xb\": np.zeros(5),\n            \"B\": np.diag([1e-2, 1e-1, 1, 10, 100]),\n            \"H\": np.identity(5),\n            \"R\": np.diag([0.5, 0.05, 0.2, 2.0, 5.0]),\n            \"y\": np.array([0.1, -0.2, 0.3, -0.4, 0.5]),\n            \"x\": np.array([0.05, -0.1, 0.25, -0.35, 0.6]),\n            \"v\": np.ones(5),\n        },\n        {\n            \"name\": \"C\",\n            \"type\": \"4D-Var\",\n            \"K\": 3,\n            \"A\": np.array([[1.0, 0.1, 0.0], [0.0, 1.0, 0.1], [0.0, 0.0, 1.0]]),\n            \"xb\": np.zeros(3),\n            \"B\": np.diag([0.3, 0.3, 0.3]),\n            \"Hk_val\": np.array([[1.0, 0.0, 0.0], [0.0, 1.0, 0.3]]),\n            \"Rk_val\": np.array([[0.05, 0.01], [0.01, 0.08]]),\n            \"yk_list\": [np.array([0.95, -0.45]), np.array([1.05, -0.55]), np.array([1.00, -0.50])],\n            \"x0\": np.array([0.8, -0.6, 0.2]),\n            \"v\": np.array([0.2, -0.1, 0.3]),\n        },\n        {\n            \"name\": \"D\",\n            \"type\": \"4D-Var\",\n            \"K\": 2,\n            \"A\": np.array([[1.0, 0.05, 0.0, 0.0], [0.0, 1.0, 0.05, 0.0], [0.0, 0.0, 1.0, 0.05], [0.0, 0.0, 0.0, 1.0]]),\n            \"xb\": np.zeros(4),\n            \"B\": np.diag([0.5, 0.2, 0.1, 0.05]),\n            \"Hk_list\": [np.identity(4), np.identity(4)],\n            \"Rk_list\": [np.diag([0.1, 0.2, 0.3, 0.4]), np.diag([0.2, 0.1, 0.4, 0.3])],\n            \"yk_list\": [np.array([0.1, -0.1, 0.2, -0.2]), np.array([0.15, -0.05, 0.25, -0.15])],\n            \"x0\": np.array([0.05, -0.08, 0.12, -0.05]),\n            \"v\": np.array([0.5, -0.3, 0.2, 0.1]),\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        if case[\"type\"] == \"3D-Var\":\n            args = (case[\"xb\"], case[\"B\"], case[\"y\"], case[\"H\"], case[\"R\"])\n            max_error = verify_and_get_max_error(cost_3d, grad_3d, case[\"x\"], case[\"v\"], eps_sweep, *args)\n            results.append(max_error)\n        \n        elif case[\"type\"] == \"4D-Var\":\n            Mk_list = [np.linalg.matrix_power(case[\"A\"], k) for k in range(1, case[\"K\"] + 1)]\n            \n            if \"Hk_val\" in case: # Case C where H_k is constant\n                Hk_list = [case[\"Hk_val\"]] * case[\"K\"]\n            else: # Case D where H_k is provided as a list\n                Hk_list = case[\"Hk_list\"]\n                \n            if \"Rk_val\" in case: # Case C where R_k is constant\n                Rk_list = [case[\"Rk_val\"]] * case[\"K\"]\n            else: # Case D where R_k is provided as a list\n                Rk_list = case[\"Rk_list\"]\n            \n            args = (case[\"xb\"], case[\"B\"], case[\"yk_list\"], Hk_list, Rk_list, Mk_list)\n            max_error = verify_and_get_max_error(cost_4d, grad_4d, case[\"x0\"], case[\"v\"], eps_sweep, *args)\n            results.append(max_error)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}