{
    "hands_on_practices": [
        {
            "introduction": "The background error covariance matrix, $B$, is a cornerstone of data assimilation, encapsulating our knowledge of model error structure. This practice moves beyond simple statistical assumptions to demonstrate how physical laws, such as geostrophic balance, can be used to build dynamically consistent, multivariate covariance models. By deriving the wind error cross-covariances from a streamfunction error field, you will gain hands-on experience with the techniques used to create sophisticated covariance matrices in operational weather prediction .",
            "id": "4091344",
            "problem": "Consider a two-dimensional, horizontally homogeneous and isotropic error field used in Numerical Weather Prediction (NWP) data assimilation. Let the geostrophic streamfunction error be denoted by $\\psi(\\boldsymbol{r})$, where $\\boldsymbol{r} = (x,y)$ is the horizontal position vector. Assume zero-mean errors and the following physically motivated relations for balanced wind components: $u(\\boldsymbol{r}) = -\\partial \\psi(\\boldsymbol{r}) / \\partial y$ and $v(\\boldsymbol{r}) = \\partial \\psi(\\boldsymbol{r}) / \\partial x$, where $u$ and $v$ are the zonal and meridional wind errors, respectively. The background error covariance (BEC) function of the streamfunction is specified as a homogeneous isotropic Gaussian,\n$$\nC_{\\psi}(r) = \\sigma_{\\psi}^{2} \\exp\\!\\left(-\\frac{r^{2}}{L^{2}}\\right),\n$$\nwhere $r = |\\boldsymbol{r}|$, $\\sigma_{\\psi}^{2}$ is the streamfunction error variance, and $L$ is the horizontal correlation length scale.\n\nAccording to the definition of a covariance, the physically structured background error covariance matrix $\\boldsymbol{B}$ for the wind components is induced by the linear differential mapping from $\\psi$ to $(u,v)$. Define the cross-covariance entry $B_{uv}(\\boldsymbol{r})$ as the covariance between $u$ at the origin and $v$ at the location $\\boldsymbol{r}$, that is $B_{uv}(\\boldsymbol{r}) = \\mathbb{E}\\!\\left[ u(\\boldsymbol{0}) \\, v(\\boldsymbol{r}) \\right]$.\n\nStarting from the fundamental definitions of covariance for homogeneous fields and the specified windâ€“streamfunction relations, derive a closed-form analytic expression for $B_{uv}(\\boldsymbol{r})$ in terms of $r$, the polar angle $\\theta$ of $\\boldsymbol{r}$ with respect to the $x$-axis, $\\sigma_{\\psi}^{2}$, and $L$. Express your final answer as a single analytic expression. Do not attach units to your answer.",
            "solution": "The problem is valid as it is scientifically grounded in the principles of geophysical fluid dynamics and statistical data assimilation, is well-posed with all necessary information provided, and is formulated objectively using standard mathematical and physical terminology.\n\nWe are tasked with deriving the cross-covariance $B_{uv}(\\boldsymbol{r})$ between the zonal wind error $u$ at the origin $\\boldsymbol{0}$ and the meridional wind error $v$ at a position $\\boldsymbol{r} = (x,y)$. The definition is given as:\n$$\nB_{uv}(\\boldsymbol{r}) = \\mathbb{E}\\!\\left[ u(\\boldsymbol{0}) \\, v(\\boldsymbol{r}) \\right]\n$$\nThe errors are specified to be zero-mean. The wind components $u$ and $v$ are related to the geostrophic streamfunction error $\\psi$ by the following differential operators:\n$$\nu(\\boldsymbol{r}) = -\\frac{\\partial \\psi(\\boldsymbol{r})}{\\partial y}\n\\quad \\text{and} \\quad\nv(\\boldsymbol{r}) = \\frac{\\partial \\psi(\\boldsymbol{r})}{\\partial x}\n$$\nTo evaluate the covariance, we must introduce two distinct points in space for the two variables. Let the location for $u$ be $\\boldsymbol{r}' = (x', y')$ and the location for $v$ be $\\boldsymbol{r} = (x, y)$. The expression for the covariance then involves derivatives at these two points. We will set $\\boldsymbol{r}'=\\boldsymbol{0}$ at the end of the calculation.\n$$\nB_{uv}(\\boldsymbol{r}) = \\mathbb{E}\\!\\left[ \\left(-\\left.\\frac{\\partial \\psi(\\boldsymbol{r}')}{\\partial y'}\\right|_{\\boldsymbol{r}'=\\boldsymbol{0}}\\right) \\left(\\frac{\\partial \\psi(\\boldsymbol{r})}{\\partial x}\\right) \\right]\n$$\nSince the random field $\\psi$ has a zero mean, and differentiation is a linear operation, we can commute the expectation operator $\\mathbb{E}$ with the partial derivatives:\n$$\nB_{uv}(\\boldsymbol{r}) = -\\frac{\\partial}{\\partial y'} \\frac{\\partial}{\\partial x} \\mathbb{E}\\!\\left[ \\psi(\\boldsymbol{r}') \\psi(\\boldsymbol{r}) \\right] \\bigg|_{\\boldsymbol{r}'=\\boldsymbol{0}}\n$$\nThe term $\\mathbb{E}\\!\\left[ \\psi(\\boldsymbol{r}') \\psi(\\boldsymbol{r}) \\right]$ is the definition of the two-point covariance of the streamfunction field. Since the field is assumed to be homogeneous, this covariance depends only on the separation vector $\\boldsymbol{\\delta} = \\boldsymbol{r} - \\boldsymbol{r}'$.\n$$\n\\mathbb{E}\\!\\left[ \\psi(\\boldsymbol{r}') \\psi(\\boldsymbol{r}) \\right] = C_{\\psi}(\\boldsymbol{r}-\\boldsymbol{r}')\n$$\nSubstituting this into our expression for $B_{uv}(\\boldsymbol{r})$:\n$$\nB_{uv}(\\boldsymbol{r}) = -\\frac{\\partial}{\\partial y'} \\frac{\\partial}{\\partial x} C_{\\psi}(\\boldsymbol{r}-\\boldsymbol{r}') \\bigg|_{\\boldsymbol{r}'=\\boldsymbol{0}}\n$$\nLet's analyze the derivatives with respect to the components of $\\boldsymbol{r}=(x,y)$ and $\\boldsymbol{r}'=(x',y')$. Let $\\boldsymbol{\\delta} = (\\delta_x, \\delta_y) = (x-x', y-y')$. Using the chain rule for partial differentiation:\n$$\n\\frac{\\partial}{\\partial x} C_{\\psi}(x-x', y-y') = \\frac{\\partial C_{\\psi}}{\\partial \\delta_x} \\frac{\\partial \\delta_x}{\\partial x} = \\frac{\\partial C_{\\psi}}{\\partial \\delta_x}\n$$\nAnd for the derivative with respect to $y'$:\n$$\n\\frac{\\partial}{\\partial y'} \\left( \\frac{\\partial C_{\\psi}}{\\partial \\delta_x} \\right) = \\frac{\\partial^2 C_{\\psi}}{\\partial \\delta_y \\partial \\delta_x} \\frac{\\partial \\delta_y}{\\partial y'} = -\\frac{\\partial^2 C_{\\psi}}{\\partial \\delta_y \\partial \\delta_x}\n$$\nSubstituting this back into the expression for $B_{uv}(\\boldsymbol{r})$ gives:\n$$\nB_{uv}(\\boldsymbol{r}) = -\\left( -\\frac{\\partial^2 C_{\\psi}(\\boldsymbol{r}-\\boldsymbol{r}')}{\\partial \\delta_y \\partial \\delta_x} \\right) \\bigg|_{\\boldsymbol{r}'=\\boldsymbol{0}}\n$$\nWhen we evaluate this at $\\boldsymbol{r}'=\\boldsymbol{0}$, the separation vector $\\boldsymbol{\\delta}$ becomes $\\boldsymbol{r}$, and the derivatives $\\partial/\\partial \\delta_x$ and $\\partial/\\partial \\delta_y$ become $\\partial/\\partial x$ and $\\partial/\\partial y$, respectively.\n$$\nB_{uv}(\\boldsymbol{r}) = \\frac{\\partial^2 C_{\\psi}(\\boldsymbol{r})}{\\partial x \\partial y}\n$$\nNow, we must compute this mixed partial derivative for the given streamfunction covariance function:\n$$\nC_{\\psi}(r) = \\sigma_{\\psi}^{2} \\exp\\!\\left(-\\frac{r^{2}}{L^{2}}\\right)\n$$\nIn Cartesian coordinates, $r^2 = x^2 + y^2$, so the function is:\n$$\nC_{\\psi}(x,y) = \\sigma_{\\psi}^{2} \\exp\\!\\left(-\\frac{x^2+y^2}{L^2}\\right)\n$$\nFirst, we differentiate with respect to $y$:\n$$\n\\frac{\\partial C_{\\psi}}{\\partial y} = \\sigma_{\\psi}^{2} \\frac{\\partial}{\\partial y} \\exp\\!\\left(-\\frac{x^2+y^2}{L^2}\\right) = \\sigma_{\\psi}^{2} \\exp\\!\\left(-\\frac{x^2+y^2}{L^2}\\right) \\cdot \\left(-\\frac{2y}{L^2}\\right)\n$$\nNext, we differentiate the result with respect to $x$:\n$$\n\\frac{\\partial^2 C_{\\psi}}{\\partial x \\partial y} = \\frac{\\partial}{\\partial x} \\left[ -\\frac{2y\\sigma_{\\psi}^{2}}{L^2} \\exp\\!\\left(-\\frac{x^2+y^2}{L^2}\\right) \\right]\n$$\nTreating the terms involving $y$ as constants with respect to differentiation by $x$:\n$$\n\\frac{\\partial^2 C_{\\psi}}{\\partial x \\partial y} = -\\frac{2y\\sigma_{\\psi}^{2}}{L^2} \\cdot \\frac{\\partial}{\\partial x} \\exp\\!\\left(-\\frac{x^2+y^2}{L^2}\\right) = -\\frac{2y\\sigma_{\\psi}^{2}}{L^2} \\cdot \\exp\\!\\left(-\\frac{x^2+y^2}{L^2}\\right) \\cdot \\left(-\\frac{2x}{L^2}\\right)\n$$\nSimplifying the expression, we get:\n$$\nB_{uv}(x,y) = \\frac{\\partial^2 C_{\\psi}}{\\partial x \\partial y} = \\frac{4xy\\sigma_{\\psi}^{2}}{L^4} \\exp\\!\\left(-\\frac{x^2+y^2}{L^2}\\right)\n$$\nThe problem requires the answer in terms of polar coordinates $(r, \\theta)$, where $\\boldsymbol{r}$ has magnitude $r$ and polar angle $\\theta$ with respect to the $x$-axis. The coordinate transformation is:\n$$\nx = r \\cos(\\theta)\n\\quad \\text{and} \\quad\ny = r \\sin(\\theta)\n$$\nFrom this, we have:\n$$\nx^2+y^2 = r^2\n$$\n$$\nxy = r^2 \\cos(\\theta) \\sin(\\theta)\n$$\nUsing the trigonometric double-angle identity $2\\sin(\\theta)\\cos(\\theta) = \\sin(2\\theta)$, we can write:\n$$\nxy = \\frac{1}{2} r^2 \\sin(2\\theta)\n$$\nSubstituting these polar coordinate expressions into our result for $B_{uv}(x,y)$:\n$$\nB_{uv}(r, \\theta) = \\frac{4\\sigma_{\\psi}^{2}}{L^4} \\left(\\frac{1}{2} r^2 \\sin(2\\theta)\\right) \\exp\\!\\left(-\\frac{r^2}{L^2}\\right)\n$$\nFinally, simplifying the numerical factor gives the closed-form analytic expression for the cross-covariance:\n$$\nB_{uv}(r, \\theta) = \\frac{2\\sigma_{\\psi}^{2} r^2}{L^4} \\sin(2\\theta) \\exp\\!\\left(-\\frac{r^2}{L^2}\\right)\n$$",
            "answer": "$$\\boxed{\\frac{2\\sigma_{\\psi}^{2} r^{2}}{L^{4}} \\sin(2\\theta) \\exp\\!\\left(-\\frac{r^{2}}{L^{2}}\\right)}$$"
        },
        {
            "introduction": "The observation error covariance matrix, $R$, quantifies the uncertainty in our measurements, but its specification is more nuanced than simply considering instrument noise. This exercise guides you through constructing an $R$ matrix by combining distinct and physically meaningful sources of error, including instrument noise, representativeness error, and systematically correlated errors. This process reveals why observation errors can be correlated and how to model these structures from first principles .",
            "id": "4091334",
            "problem": "Consider a two-channel satellite observation used in Numerical Weather Prediction (NWP). Each channel reports a brightness temperature. The total observation error arises from three scientifically distinct and statistically independent sources: instrument noise, representativeness error due to unresolved state variability, and a common-mode pre-processing correction uncertainty. Let the observation error covariance matrix be denoted by $R$.\n\nAssume the following physically realistic scenario:\n- The observation operator for representativeness mapping is linear with respect to a single unresolved scalar state increment $\\delta x$ (e.g., a tropospheric-layer temperature fluctuation). The two channel sensitivities are the weights $w_{1}$ and $w_{2}$, so the representativeness contribution to the observation error is proportional to $\\begin{pmatrix} w_{1} \\\\ w_{2} \\end{pmatrix} \\delta x$.\n- The unresolved scalar increment $\\delta x$ is zero-mean with standard deviation $\\sigma_{u}$.\n- Instrument noise in the two channels is zero-mean, uncorrelated between channels, with standard deviations $\\sigma_{n,1}$ and $\\sigma_{n,2}$ respectively.\n- The pre-processing correction error is modeled as a zero-mean scalar applied identically to both channels, with standard deviation $\\sigma_{c}$, independent of the other two error sources.\n\nYou are given the following numerical values:\n- $w_{1} = 0.6$, $w_{2} = 0.4$,\n- $\\sigma_{u} = 0.7 \\ \\text{K}$,\n- $\\sigma_{n,1} = 0.45 \\ \\text{K}$, $\\sigma_{n,2} = 0.35 \\ \\text{K}$,\n- $\\sigma_{c} = 0.2 \\ \\text{K}$.\n\nStarting from the definition of covariance for random vectors and the properties of covariance under linear transformations and addition of independent random components, derive the $2 \\times 2$ observation error covariance matrix $R$ implied by the three sources above and compute its determinant. Round your final numerical result to four significant figures. Express the final value in $\\text{K}^{4}$.",
            "solution": "The problem statement is deemed valid as it is scientifically grounded in the principles of data assimilation for numerical weather prediction, well-posed with sufficient information for a unique solution, and objective in its formulation.\n\nThe total observation error vector for the two-channel system, denoted by $\\boldsymbol{\\epsilon}$, is the sum of three statistically independent error sources:\n$1$. Instrument noise, $\\boldsymbol{\\epsilon}_{\\text{inst}}$.\n$2$. Representativeness error, $\\boldsymbol{\\epsilon}_{\\text{rep}}$.\n$3$. Common-mode pre-processing correction error, $\\boldsymbol{\\epsilon}_{\\text{corr}}$.\n\nThe total error vector is thus:\n$$\n\\boldsymbol{\\epsilon} = \\boldsymbol{\\epsilon}_{\\text{inst}} + \\boldsymbol{\\epsilon}_{\\text{rep}} + \\boldsymbol{\\epsilon}_{\\text{corr}}\n$$\nThe total observation error covariance matrix, $R$, is defined as the expectation of the outer product of the total error vector with itself, $R = E[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T]$. Since all error sources are specified as zero-mean and statistically independent, the covariance of their sum is the sum of their individual covariance matrices:\n$$\nR = \\text{cov}(\\boldsymbol{\\epsilon}) = \\text{cov}(\\boldsymbol{\\epsilon}_{\\text{inst}}) + \\text{cov}(\\boldsymbol{\\epsilon}_{\\text{rep}}) + \\text{cov}(\\boldsymbol{\\epsilon}_{\\text{corr}}) = R_{\\text{inst}} + R_{\\text{rep}} + R_{\\text{corr}}\n$$\nWe shall derive each component matrix, $R_{\\text{inst}}$, $R_{\\text{rep}}$, and $R_{\\text{corr}}$, separately.\n\nFirst, we consider the instrument noise, $\\boldsymbol{\\epsilon}_{\\text{inst}}$. It is given that the instrument noises for the two channels are zero-mean, uncorrelated, and have standard deviations $\\sigma_{n,1}$ and $\\sigma_{n,2}$. The error vector is $\\boldsymbol{\\epsilon}_{\\text{inst}} = \\begin{pmatrix} \\epsilon_{n,1} \\\\ \\epsilon_{n,2} \\end{pmatrix}$. The covariance matrix $R_{\\text{inst}} = E[\\boldsymbol{\\epsilon}_{\\text{inst}}\\boldsymbol{\\epsilon}_{\\text{inst}}^T]$ is therefore diagonal, with the variances on the diagonal:\n$$\nR_{\\text{inst}} = \\begin{pmatrix} \\text{var}(\\epsilon_{n,1}) & \\text{cov}(\\epsilon_{n,1}, \\epsilon_{n,2}) \\\\ \\text{cov}(\\epsilon_{n,2}, \\epsilon_{n,1}) & \\text{var}(\\epsilon_{n,2}) \\end{pmatrix} = \\begin{pmatrix} \\sigma_{n,1}^2 & 0 \\\\ 0 & \\sigma_{n,2}^2 \\end{pmatrix}\n$$\n\nSecond, we analyze the representativeness error, $\\boldsymbol{\\epsilon}_{\\text{rep}}$. This error arises from a single unresolved scalar state increment, $\\delta x$, which is zero-mean with standard deviation $\\sigma_u$. The error affects the two channels according to the weight vector $\\mathbf{w} = \\begin{pmatrix} w_{1} \\\\ w_{2} \\end{pmatrix}$. Thus, the error vector is $\\boldsymbol{\\epsilon}_{\\text{rep}} = \\mathbf{w} \\delta x$. The covariance matrix $R_{\\text{rep}}$ is given by:\n$$\nR_{\\text{rep}} = E[\\boldsymbol{\\epsilon}_{\\text{rep}}\\boldsymbol{\\epsilon}_{\\text{rep}}^T] = E[(\\mathbf{w} \\delta x)(\\mathbf{w} \\delta x)^T] = E[\\mathbf{w} (\\delta x)^2 \\mathbf{w}^T] = \\mathbf{w} E[(\\delta x)^2] \\mathbf{w}^T\n$$\nSince $\\delta x$ is zero-mean, its expected square $E[(\\delta x)^2]$ is its variance, $\\sigma_u^2$. Therefore:\n$$\nR_{\\text{rep}} = \\sigma_u^2 \\mathbf{w} \\mathbf{w}^T = \\sigma_u^2 \\begin{pmatrix} w_{1} \\\\ w_{2} \\end{pmatrix} \\begin{pmatrix} w_{1} & w_{2} \\end{pmatrix} = \\sigma_u^2 \\begin{pmatrix} w_{1}^2 & w_{1}w_{2} \\\\ w_{1}w_{2} & w_{2}^2 \\end{pmatrix}\n$$\n\nThird, we examine the common-mode pre-processing error, $\\boldsymbol{\\epsilon}_{\\text{corr}}$. This is a scalar error, $\\epsilon_c$, with zero mean and standard deviation $\\sigma_c$, applied identically to both channels. The error vector is $\\boldsymbol{\\epsilon}_{\\text{corr}} = \\begin{pmatrix} \\epsilon_c \\\\ \\epsilon_c \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\epsilon_c$. Letting $\\mathbf{1} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$, the structure is analogous to the representativeness error:\n$$\nR_{\\text{corr}} = E[\\boldsymbol{\\epsilon}_{\\text{corr}}\\boldsymbol{\\epsilon}_{\\text{corr}}^T] = E[(\\mathbf{1} \\epsilon_c)(\\mathbf{1} \\epsilon_c)^T] = \\mathbf{1} E[\\epsilon_c^2] \\mathbf{1}^T = \\sigma_c^2 \\mathbf{1} \\mathbf{1}^T = \\sigma_c^2 \\begin{pmatrix} 1 & 1 \\\\ 1 & 1 \\end{pmatrix}\n$$\n\nNow, we combine these three matrices to form the total observation error covariance matrix $R$:\n$$\nR = R_{\\text{inst}} + R_{\\text{rep}} + R_{\\text{corr}} = \\begin{pmatrix} \\sigma_{n,1}^2 & 0 \\\\ 0 & \\sigma_{n,2}^2 \\end{pmatrix} + \\sigma_u^2 \\begin{pmatrix} w_{1}^2 & w_{1}w_{2} \\\\ w_{1}w_{2} & w_{2}^2 \\end{pmatrix} + \\sigma_c^2 \\begin{pmatrix} 1 & 1 \\\\ 1 & 1 \\end{pmatrix}\n$$\nSumming the matrices element-wise gives:\n$$\nR = \\begin{pmatrix} \\sigma_{n,1}^2 + \\sigma_u^2 w_{1}^2 + \\sigma_c^2 & \\sigma_u^2 w_{1}w_{2} + \\sigma_c^2 \\\\ \\sigma_u^2 w_{1}w_{2} + \\sigma_c^2 & \\sigma_{n,2}^2 + \\sigma_u^2 w_{2}^2 + \\sigma_c^2 \\end{pmatrix}\n$$\nWe are given the following numerical values: $w_{1} = 0.6$, $w_{2} = 0.4$, $\\sigma_{u} = 0.7 \\ \\text{K}$, $\\sigma_{n,1} = 0.45 \\ \\text{K}$, $\\sigma_{n,2} = 0.35 \\ \\text{K}$, and $\\sigma_{c} = 0.2 \\ \\text{K}$.\nWe first compute the variances (all in units of $\\text{K}^2$):\n$\\sigma_{n,1}^2 = (0.45)^2 = 0.2025$\n$\\sigma_{n,2}^2 = (0.35)^2 = 0.1225$\n$\\sigma_{u}^2 = (0.7)^2 = 0.49$\n$\\sigma_{c}^2 = (0.2)^2 = 0.04$\n\nNow we compute the elements of the matrix $R$:\nThe top-left element, $R_{11}$:\n$$\nR_{11} = \\sigma_{n,1}^2 + \\sigma_u^2 w_{1}^2 + \\sigma_c^2 = 0.2025 + 0.49 \\times (0.6)^2 + 0.04 = 0.2025 + 0.49 \\times 0.36 + 0.04 = 0.2025 + 0.1764 + 0.04 = 0.4189\n$$\nThe bottom-right element, $R_{22}$:\n$$\nR_{22} = \\sigma_{n,2}^2 + \\sigma_u^2 w_{2}^2 + \\sigma_c^2 = 0.1225 + 0.49 \\times (0.4)^2 + 0.04 = 0.1225 + 0.49 \\times 0.16 + 0.04 = 0.1225 + 0.0784 + 0.04 = 0.2409\n$$\nThe off-diagonal elements, $R_{12} = R_{21}$:\n$$\nR_{12} = \\sigma_u^2 w_{1}w_{2} + \\sigma_c^2 = 0.49 \\times 0.6 \\times 0.4 + 0.04 = 0.49 \\times 0.24 + 0.04 = 0.1176 + 0.04 = 0.1576\n$$\nThus, the numerical observation error covariance matrix is:\n$$\nR = \\begin{pmatrix} 0.4189 & 0.1576 \\\\ 0.1576 & 0.2409 \\end{pmatrix} \\ \\text{K}^2\n$$\nThe determinant of a $2 \\times 2$ matrix $\\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}$ is $ad-bc$. For the matrix $R$, the determinant is:\n$$\n\\det(R) = R_{11}R_{22} - R_{12}R_{21} = (0.4189)(0.2409) - (0.1576)^2\n$$\nWe compute the numerical values:\n$$\n(0.4189)(0.2409) = 0.10091301\n$$\n$$\n(0.1576)^2 = 0.02483776\n$$\n$$\n\\det(R) = 0.10091301 - 0.02483776 = 0.07607525\n$$\nThe problem requires the result to be rounded to four significant figures. The first non-zero digit is the first significant figure. The first four significant figures are $7, 6, 0, 7$. The fifth significant digit is $5$, so we round up the fourth digit.\n$$\n\\det(R) \\approx 0.07608\n$$\nThe units of the determinant are $(\\text{K}^2)^2 = \\text{K}^4$, as specified.",
            "answer": "$$\n\\boxed{0.07608}\n$$"
        },
        {
            "introduction": "Once specified, the $B$ and $R$ matrices dictate the behavior and properties of the data assimilation system. This exercise connects the specification of these matrices to the practical challenge of solving the analysis problem by examining the conditioning of the variational cost function. By calculating the Hessian and its condition number in a simplified setting, you will see how the relative uncertainties and correlation structures encoded in $B$ and $R$ directly impact the convergence and stability of the optimization algorithms used in 3D-Var .",
            "id": "4091350",
            "problem": "In a simplified two-variable Three-Dimensional Variational (3D-Var) data assimilation system for numerical weather prediction and climate modeling, consider the state vector of temperature anomalies at two adjacent grid points, denoted by $x \\in \\mathbb{R}^{2}$. The background error covariance matrix $B \\in \\mathbb{R}^{2 \\times 2}$ is specified from a stationary, homogeneous correlation structure with correlation coefficient $\\rho$ between the two points and background standard deviation $\\sigma_{b}$ at each point, so that\n$$\nB = \\sigma_{b}^{2} \\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix}.\n$$\nA single scalar observation measures the arithmetic mean of the two anomalies with independent, zero-mean observational error of variance $r$, that is $y = \\tfrac{1}{2}(x_{1} + x_{2}) + \\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0, r)$. The corresponding linear observation operator is $H \\in \\mathbb{R}^{1 \\times 2}$ with row vector $h = \\begin{pmatrix} \\tfrac{1}{2} & \\tfrac{1}{2} \\end{pmatrix}$, and the observation error covariance matrix is $R = r \\in \\mathbb{R}^{1 \\times 1}$. The 3D-Var cost function is\n$$\nJ(x) = \\tfrac{1}{2}(x - x_{b})^{\\top} B^{-1} (x - x_{b}) + \\tfrac{1}{2}(y - H x)^{\\top} R^{-1} (y - H x),\n$$\nand the control variable is defined by $x = x_{b} + B^{1/2} v$, where $B^{1/2}$ is the unique symmetric square root of $B$.\n\nUsing only the definitions of covariance matrices, their symmetry and positive-definiteness, and standard linear algebraic properties of quadratic forms, derive the Gauss-Newton Hessian of $J$ with respect to the control variable $v$, and use it to compute its spectral condition number, defined as the ratio of its largest to its smallest eigenvalue. Take the following parameter values: $\\sigma_{b} = 1.2 \\text{ K}$, $\\rho = 0.8$, and $r = (0.3 \\text{ K})^{2}$.\n\nGive your final answer as a single dimensionless number, rounded to four significant figures.",
            "solution": "The problem is validated as scientifically grounded, well-posed, and objective. It is a standard problem in data assimilation theory, with all necessary parameters and definitions provided.\n\nThe objective is to compute the spectral condition number of the Gauss-Newton Hessian of the cost function $J$ with respect to the control variable $v$. The cost function is given by:\n$$\nJ(x) = \\tfrac{1}{2}(x - x_{b})^{\\top} B^{-1} (x - x_{b}) + \\tfrac{1}{2}(y - H x)^{\\top} R^{-1} (y - H x)\n$$\nThe control variable $v$ is related to the state vector $x$ via the transformation $x = x_{b} + B^{1/2} v$, where $B^{1/2}$ is the symmetric square root of the background error covariance matrix $B$.\n\nFirst, we express the cost function $J$ in terms of the control variable $v$.\nThe first term of $J(x)$ becomes:\n$$\n\\tfrac{1}{2}(x - x_{b})^{\\top} B^{-1} (x - x_{b}) = \\tfrac{1}{2}(B^{1/2} v)^{\\top} B^{-1} (B^{1/2} v)\n$$\nSince $B^{1/2}$ is symmetric, $(B^{1/2})^{\\top} = B^{1/2}$. Also, $B^{-1} = (B^{1/2}B^{1/2})^{-1} = (B^{1/2})^{-1}(B^{1/2})^{-1}$.\n$$\n= \\tfrac{1}{2} v^{\\top} B^{1/2} (B^{1/2})^{-1} (B^{1/2})^{-1} B^{1/2} v = \\tfrac{1}{2} v^{\\top} I v = \\tfrac{1}{2} v^{\\top} v\n$$\nThe second term of $J(x)$ involves the term $y - Hx$:\n$$\ny - Hx = y - H(x_b + B^{1/2}v) = (y - Hx_b) - H B^{1/2} v\n$$\nLet $d = y - Hx_b$ be the innovation vector (a scalar in this case), and let the transformed observation operator be $\\tilde{H} = H B^{1/2}$. The second term becomes:\n$$\n\\tfrac{1}{2}(d - \\tilde{H}v)^{\\top} R^{-1} (d - \\tilde{H}v)\n$$\nThus, the cost function in terms of $v$ is:\n$$\nJ(v) = \\tfrac{1}{2} v^{\\top} v + \\tfrac{1}{2}(d - \\tilde{H}v)^{\\top} R^{-1} (d - \\tilde{H}v)\n$$\nThis is a quadratic function of $v$. For a quadratic cost function, the Gauss-Newton Hessian is identical to the exact Hessian. We find the Hessian by differentiating $J(v)$ with respect to $v$ twice.\nExpanding the second term:\n$$\nJ(v) = \\tfrac{1}{2} v^{\\top} v + \\tfrac{1}{2}(d^{\\top}R^{-1}d - 2d^{\\top}R^{-1}\\tilde{H}v + v^{\\top}\\tilde{H}^{\\top}R^{-1}\\tilde{H}v)\n$$\nThe gradient with respect to $v$ is:\n$$\n\\nabla_v J(v) = v - (\\tilde{H}^{\\top}R^{-1}d) + \\tilde{H}^{\\top}R^{-1}\\tilde{H}v\n$$\nThe Hessian is the derivative of the gradient with respect to $v^{\\top}$:\n$$\n\\mathcal{H}_v = \\frac{\\partial(\\nabla_v J(v))}{\\partial v^{\\top}} = I + \\tilde{H}^{\\top}R^{-1}\\tilde{H}\n$$\nSubstituting $\\tilde{H} = H B^{1/2}$ and noting $(H B^{1/2})^{\\top} = (B^{1/2})^{\\top} H^{\\top} = B^{1/2} H^{\\top}$ as $B^{1/2}$ is symmetric:\n$$\n\\mathcal{H}_v = I + B^{1/2} H^{\\top} R^{-1} H B^{1/2}\n$$\nNow we must construct this matrix using the given parameters. The parameters are $\\sigma_b = 1.2$, $\\rho=0.8$, and $R=r=(0.3)^2=0.09$. So $R^{-1} = 1/r = 1/0.09 = 100/9$.\nThe background error covariance matrix is $B = \\sigma_{b}^{2} \\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix}$.\nThe observation operator is $H = \\begin{pmatrix} 1/2 & 1/2 \\end{pmatrix}$.\n\nLet's analyze the term $M = B^{1/2} H^{\\top} R^{-1} H B^{1/2}$.\n$H^{\\top}R^{-1}H = \\begin{pmatrix} 1/2 \\\\ 1/2 \\end{pmatrix} (1/r) \\begin{pmatrix} 1/2 & 1/2 \\end{pmatrix} = \\frac{1}{r} \\begin{pmatrix} 1/4 & 1/4 \\\\ 1/4 & 1/4 \\end{pmatrix} = \\frac{1}{4r} \\begin{pmatrix} 1 & 1 \\\\ 1 & 1 \\end{pmatrix}$.\nThe matrix $\\begin{pmatrix} 1 & 1 \\\\ 1 & 1 \\end{pmatrix}$ can be written in terms of its normalized eigenvectors. The eigenvectors of $\\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix}$ are $u_1 = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$ and $u_2 = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$.\nNote that $\\begin{pmatrix} 1 & 1 \\\\ 1 & 1 \\end{pmatrix} = 2 u_1 u_1^{\\top}$.\nSo, $M = B^{1/2} \\left( \\frac{2}{4r} u_1 u_1^{\\top} \\right) B^{1/2} = \\frac{1}{2r} B^{1/2} u_1 u_1^{\\top} B^{1/2}$.\nThe vector $u_1$ is an eigenvector of the matrix $\\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix}$ with eigenvalue $1+\\rho$.\nTherefore, $u_1$ is also an eigenvector of $B = \\sigma_b^2 \\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix}$ with eigenvalue $\\sigma_b^2(1+\\rho)$, and of $B^{1/2}$ with eigenvalue $\\sqrt{\\sigma_b^2(1+\\rho)} = \\sigma_b\\sqrt{1+\\rho}$.\nSo, $B^{1/2} u_1 = \\sigma_b\\sqrt{1+\\rho} \\ u_1$.\nThe term $u_1^{\\top}B^{1/2}$ is the transpose: $(B^{1/2}u_1)^{\\top} = (\\sigma_b\\sqrt{1+\\rho} \\ u_1)^{\\top} = \\sigma_b\\sqrt{1+\\rho} \\ u_1^{\\top}$.\nSubstituting these back into the expression for $M$:\n$$\nM = \\frac{1}{2r} (B^{1/2} u_1) (u_1^{\\top} B^{1/2}) = \\frac{1}{2r} (\\sigma_b\\sqrt{1+\\rho} \\ u_1) (\\sigma_b\\sqrt{1+\\rho} \\ u_1^{\\top}) = \\frac{\\sigma_b^2(1+\\rho)}{2r} u_1 u_1^{\\top}\n$$\nThe Hessian is $\\mathcal{H}_v = I + M = I + \\frac{\\sigma_b^2(1+\\rho)}{2r} u_1 u_1^{\\top}$.\nTo find the eigenvalues of $\\mathcal{H}_v$, we can test its action on the eigenvectors $u_1$ and $u_2$.\nFor $u_1$:\n$\\mathcal{H}_v u_1 = (I + \\frac{\\sigma_b^2(1+\\rho)}{2r} u_1 u_1^{\\top})u_1 = Iu_1 + \\frac{\\sigma_b^2(1+\\rho)}{2r} u_1 (u_1^{\\top}u_1)$.\nSince $u_1$ is normalized, $u_1^{\\top}u_1=1$.\n$\\mathcal{H}_v u_1 = u_1 + \\frac{\\sigma_b^2(1+\\rho)}{2r} u_1 = \\left(1 + \\frac{\\sigma_b^2(1+\\rho)}{2r}\\right)u_1$.\nSo, the first eigenvalue is $\\mu_1 = 1 + \\frac{\\sigma_b^2(1+\\rho)}{2r}$.\n\nFor $u_2$:\n$\\mathcal{H}_v u_2 = (I + \\frac{\\sigma_b^2(1+\\rho)}{2r} u_1 u_1^{\\top})u_2 = Iu_2 + \\frac{\\sigma_b^2(1+\\rho)}{2r} u_1 (u_1^{\\top}u_2)$.\nSince $u_1$ and $u_2$ are orthogonal, $u_1^{\\top}u_2=0$.\n$\\mathcal{H}_v u_2 = u_2 + 0 = 1 \\cdot u_2$.\nSo, the second eigenvalue is $\\mu_2 = 1$.\n\nNow, we substitute the numerical values: $\\sigma_b = 1.2$, $\\rho = 0.8$, $r = 0.09$.\n$\\sigma_b^2 = 1.44$.\n$$\n\\mu_1 = 1 + \\frac{1.44(1+0.8)}{2(0.09)} = 1 + \\frac{1.44 \\times 1.8}{0.18} = 1 + 1.44 \\times 10 = 1 + 14.4 = 15.4\n$$\nThe eigenvalues are $\\mu_{\\max} = 15.4$ and $\\mu_{\\min} = 1$.\nThe spectral condition number $\\kappa$ is the ratio of the largest to the smallest eigenvalue:\n$$\n\\kappa(\\mathcal{H}_v) = \\frac{\\mu_{\\max}}{\\mu_{\\min}} = \\frac{15.4}{1} = 15.4\n$$\nThe problem asks for the answer to be rounded to four significant figures. This gives $15.40$.",
            "answer": "$$\\boxed{15.40}$$"
        }
    ]
}