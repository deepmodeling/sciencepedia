{
    "hands_on_practices": [
        {
            "introduction": "Iterative minimization algorithms form the backbone of variational data assimilation. At each step, we must answer two fundamental questions: which direction to move in, and how far to travel. This first exercise focuses on the second question by implementing the Armijo backtracking line search. By working through this classic procedure, you will gain hands-on experience in ensuring that each step provides a sufficient decrease in the cost function, a critical element for guaranteeing convergence toward the optimal atmospheric state. ",
            "id": "4063601",
            "problem": "In three-dimensional variational assimilation (3D-Var) for Numerical Weather Prediction (NWP), the analysis state vector $\\mathbf{x} \\in \\mathbb{R}^{3}$ minimizes the quadratic cost function\n$$\nJ(\\mathbf{x}) = \\frac{1}{2}(\\mathbf{x} - \\mathbf{x}_b)^{\\top} \\mathbf{B}^{-1} (\\mathbf{x} - \\mathbf{x}_b) + \\frac{1}{2}(\\mathbf{H}\\mathbf{x} - \\mathbf{y})^{\\top} \\mathbf{R}^{-1} (\\mathbf{H}\\mathbf{x} - \\mathbf{y}),\n$$\nwhere $\\mathbf{B}$ is the background error covariance, $\\mathbf{R}$ is the observation error covariance, and $\\mathbf{H}$ is the linear observation operator. The gradient of $J$ with respect to $\\mathbf{x}$ is given by fundamental calculus for quadratic forms and linear mappings.\n\nConsider a specific assimilation instance with dimension three, where the matrices and vectors are\n$$\n\\mathbf{H} = \\mathbf{I}_{3}, \\quad \\mathbf{B}^{-1} = \\mathrm{diag}(1,\\,2,\\,3), \\quad \\mathbf{R}^{-1} = \\mathrm{diag}(4,\\,1,\\,2),\n$$\n$$\n\\mathbf{x}_b = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\quad \\mathbf{y} = \\begin{pmatrix} 1 \\\\ -2 \\\\ 3 \\end{pmatrix}, \\quad \\mathbf{x}_k = \\begin{pmatrix} 1 \\\\ -1 \\\\ 2 \\end{pmatrix}.\n$$\nLet the search direction be steepest descent, $\\mathbf{p}_k = -\\nabla J(\\mathbf{x}_k)$, and perform an Armijo backtracking line search starting with initial step length $\\alpha_0 = 1$, reduction factor $\\beta = \\frac{1}{2}$, and sufficient decrease parameter $c = 0.2$. The Armijo sufficient decrease condition to be enforced is\n$$\nJ(\\mathbf{x}_k + \\alpha\\,\\mathbf{p}_k) \\leq J(\\mathbf{x}_k) + c\\,\\alpha\\,\\nabla J(\\mathbf{x}_k)^{\\top} \\mathbf{p}_k.\n$$\n\nStarting from $\\alpha = \\alpha_0$, backtrack by setting $\\alpha \\leftarrow \\beta \\alpha$ until the above inequality is satisfied. Compute the accepted step length $\\alpha_k$ that satisfies the Armijo condition. Express your final answer as a real number. No rounding is required.",
            "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and objective. The problem statement describes a standard minimization problem in 3D variational data assimilation (3D-Var) using a quadratic cost function. All matrices, vectors, and algorithmic parameters are explicitly provided. The matrices $\\mathbf{B}^{-1}$ and $\\mathbf{R}^{-1}$ are diagonal with positive entries, ensuring they represent valid inverse covariance matrices (i.e., they are positive definite). The optimization task involves applying the well-defined Armijo backtracking line search with a steepest descent direction. The setup is complete, consistent, and scientifically sound. The problem is deemed valid.\n\nThe cost function is given by\n$$\nJ(\\mathbf{x}) = \\frac{1}{2}(\\mathbf{x} - \\mathbf{x}_b)^{\\top} \\mathbf{B}^{-1} (\\mathbf{x} - \\mathbf{x}_b) + \\frac{1}{2}(\\mathbf{H}\\mathbf{x} - \\mathbf{y})^{\\top} \\mathbf{R}^{-1} (\\mathbf{H}\\mathbf{x} - \\mathbf{y})\n$$\nThe gradient of the cost function with respect to $\\mathbf{x}$ is\n$$\n\\nabla J(\\mathbf{x}) = \\mathbf{B}^{-1}(\\mathbf{x} - \\mathbf{x}_b) + \\mathbf{H}^{\\top}\\mathbf{R}^{-1}(\\mathbf{H}\\mathbf{x} - \\mathbf{y})\n$$\nWe are given the following:\n$$\n\\mathbf{H} = \\mathbf{I}_{3}, \\quad \\mathbf{B}^{-1} = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 2 & 0 \\\\ 0 & 0 & 3 \\end{pmatrix}, \\quad \\mathbf{R}^{-1} = \\begin{pmatrix} 4 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 2 \\end{pmatrix}\n$$\n$$\n\\mathbf{x}_b = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\quad \\mathbf{y} = \\begin{pmatrix} 1 \\\\ -2 \\\\ 3 \\end{pmatrix}, \\quad \\mathbf{x}_k = \\begin{pmatrix} 1 \\\\ -1 \\\\ 2 \\end{pmatrix}\n$$\nSubstituting $\\mathbf{x}_b = \\mathbf{0}$ and $\\mathbf{H} = \\mathbf{I}_{3}$ into the gradient expression simplifies it to:\n$$\n\\nabla J(\\mathbf{x}) = \\mathbf{B}^{-1}\\mathbf{x} + \\mathbf{R}^{-1}(\\mathbf{x} - \\mathbf{y}) = (\\mathbf{B}^{-1} + \\mathbf{R}^{-1})\\mathbf{x} - \\mathbf{R}^{-1}\\mathbf{y}\n$$\nFirst, we compute the required matrices:\n$$\n\\mathbf{B}^{-1} + \\mathbf{R}^{-1} = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 2 & 0 \\\\ 0 & 0 & 3 \\end{pmatrix} + \\begin{pmatrix} 4 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 2 \\end{pmatrix} = \\begin{pmatrix} 5 & 0 & 0 \\\\ 0 & 3 & 0 \\\\ 0 & 0 & 5 \\end{pmatrix}\n$$\n$$\n\\mathbf{R}^{-1}\\mathbf{y} = \\begin{pmatrix} 4 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -2 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} 4 \\\\ -2 \\\\ 6 \\end{pmatrix}\n$$\nThus, the gradient is:\n$$\n\\nabla J(\\mathbf{x}) = \\begin{pmatrix} 5 & 0 & 0 \\\\ 0 & 3 & 0 \\\\ 0 & 0 & 5 \\end{pmatrix} \\mathbf{x} - \\begin{pmatrix} 4 \\\\ -2 \\\\ 6 \\end{pmatrix} = \\begin{pmatrix} 5x_1 - 4 \\\\ 3x_2 + 2 \\\\ 5x_3 - 6 \\end{pmatrix}\n$$\nNow, we evaluate the gradient at the current iterate $\\mathbf{x}_k = \\begin{pmatrix} 1 \\\\ -1 \\\\ 2 \\end{pmatrix}$:\n$$\n\\nabla J(\\mathbf{x}_k) = \\begin{pmatrix} 5(1) - 4 \\\\ 3(-1) + 2 \\\\ 5(2) - 6 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -1 \\\\ 4 \\end{pmatrix}\n$$\nThe search direction is the steepest descent direction, $\\mathbf{p}_k = -\\nabla J(\\mathbf{x}_k)$:\n$$\n\\mathbf{p}_k = - \\begin{pmatrix} 1 \\\\ -1 \\\\ 4 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ 1 \\\\ -4 \\end{pmatrix}\n$$\nThe Armijo sufficient decrease condition is $J(\\mathbf{x}_k + \\alpha\\,\\mathbf{p}_k) \\leq J(\\mathbf{x}_k) + c\\,\\alpha\\,\\nabla J(\\mathbf{x}_k)^{\\top} \\mathbf{p}_k$.\nWe need to compute the terms on the right-hand side. First, $J(\\mathbf{x}_k)$:\n$$\n\\mathbf{x}_k - \\mathbf{x}_b = \\begin{pmatrix} 1 \\\\ -1 \\\\ 2 \\end{pmatrix}, \\quad \\mathbf{H}\\mathbf{x}_k - \\mathbf{y} = \\mathbf{I}_3 \\mathbf{x}_k - \\mathbf{y} = \\begin{pmatrix} 1 \\\\ -1 \\\\ 2 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ -2 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\\\ -1 \\end{pmatrix}\n$$\n$$\nJ(\\mathbf{x}_k) = \\frac{1}{2} \\begin{pmatrix} 1 & -1 & 2 \\end{pmatrix} \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 2 & 0 \\\\ 0 & 0 & 3 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\\\ 2 \\end{pmatrix} + \\frac{1}{2} \\begin{pmatrix} 0 & 1 & -1 \\end{pmatrix} \\begin{pmatrix} 4 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 2 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\\\ -1 \\end{pmatrix}\n$$\n$$\nJ(\\mathbf{x}_k) = \\frac{1}{2} (1(1)^2 + 2(-1)^2 + 3(2)^2) + \\frac{1}{2} (4(0)^2 + 1(1)^2 + 2(-1)^2)\n$$\n$$\nJ(\\mathbf{x}_k) = \\frac{1}{2} (1 + 2 + 12) + \\frac{1}{2} (0 + 1 + 2) = \\frac{15}{2} + \\frac{3}{2} = \\frac{18}{2} = 9\n$$\nNext, we compute the directional derivative term $\\nabla J(\\mathbf{x}_k)^{\\top} \\mathbf{p}_k$:\n$$\n\\nabla J(\\mathbf{x}_k)^{\\top} \\mathbf{p}_k = \\begin{pmatrix} 1 & -1 & 4 \\end{pmatrix} \\begin{pmatrix} -1 \\\\ 1 \\\\ -4 \\end{pmatrix} = (1)(-1) + (-1)(1) + (4)(-4) = -1 - 1 - 16 = -18\n$$\nWith $c = 0.2$, the Armijo condition becomes:\n$$\nJ(\\mathbf{x}_k + \\alpha\\,\\mathbf{p}_k) \\leq 9 + (0.2)\\alpha(-18) \\implies J(\\mathbf{x}_k + \\alpha\\,\\mathbf{p}_k) \\leq 9 - 3.6\\alpha\n$$\nTo evaluate the left-hand side, we define the new point $\\mathbf{x}_{new}(\\alpha) = \\mathbf{x}_k + \\alpha\\,\\mathbf{p}_k$:\n$$\n\\mathbf{x}_{new}(\\alpha) = \\begin{pmatrix} 1 \\\\ -1 \\\\ 2 \\end{pmatrix} + \\alpha \\begin{pmatrix} -1 \\\\ 1 \\\\ -4 \\end{pmatrix} = \\begin{pmatrix} 1-\\alpha \\\\ \\alpha-1 \\\\ 2-4\\alpha \\end{pmatrix}\n$$\nLet's define a function $\\phi(\\alpha) = J(\\mathbf{x}_{new}(\\alpha))$. Since $J$ is a quadratic function of $\\mathbf{x}$, and $\\mathbf{x}_{new}$ is a linear function of $\\alpha$, $\\phi(\\alpha)$ must be a quadratic function of $\\alpha$. We can write $\\phi(\\alpha) = A\\alpha^2 + B\\alpha + C$. We know that $\\phi(0) = J(\\mathbf{x}_k) = 9$, so $C=9$. We also know that $\\phi'(0) = \\nabla J(\\mathbf{x}_k)^{\\top}\\mathbf{p}_k = -18$, so $B=-18$. The expression for $\\phi(\\alpha)$ is thus $\\phi(\\alpha) = A\\alpha^2 - 18\\alpha + 9$. The hessian of $J$ is $\\nabla^2 J(\\mathbf{x}) = \\mathbf{B}^{-1} + \\mathbf{H}^{\\top}\\mathbf{R}^{-1}\\mathbf{H} = \\mathbf{B}^{-1} + \\mathbf{R}^{-1}$. The coefficient $A$ is given by $A = \\frac{1}{2}\\mathbf{p}_k^{\\top}(\\mathbf{B}^{-1}+\\mathbf{R}^{-1})\\mathbf{p}_k$.\n$$\nA = \\frac{1}{2}\\begin{pmatrix} -1 & 1 & -4 \\end{pmatrix} \\begin{pmatrix} 5 & 0 & 0 \\\\ 0 & 3 & 0 \\\\ 0 & 0 & 5 \\end{pmatrix} \\begin{pmatrix} -1 \\\\ 1 \\\\ -4 \\end{pmatrix} = \\frac{1}{2}\\begin{pmatrix} -5 & 3 & -20 \\end{pmatrix} \\begin{pmatrix} -1 \\\\ 1 \\\\ -4 \\end{pmatrix}\n$$\n$$\nA = \\frac{1}{2}((-5)(-1) + (3)(1) + (-20)(-4)) = \\frac{1}{2}(5 + 3 + 80) = \\frac{88}{2} = 44\n$$\nSo, $J(\\mathbf{x}_k + \\alpha\\,\\mathbf{p}_k) = 44\\alpha^2 - 18\\alpha + 9$.\nThe Armijo inequality is:\n$$\n44\\alpha^2 - 18\\alpha + 9 \\leq 9 - 3.6\\alpha\n$$\n$$\n44\\alpha^2 - 18\\alpha \\leq -3.6\\alpha\n$$\n$$\n44\\alpha^2 - 14.4\\alpha \\leq 0\n$$\nSince we are seeking $\\alpha > 0$, we can divide by $\\alpha$:\n$$\n44\\alpha - 14.4 \\leq 0 \\implies 44\\alpha \\leq 14.4 \\implies \\alpha \\leq \\frac{14.4}{44} = \\frac{144}{440} = \\frac{72}{220} = \\frac{36}{110} = \\frac{18}{55}\n$$\nThe backtracking line search starts with $\\alpha = \\alpha_0 = 1$ and reduces it by a factor of $\\beta = \\frac{1}{2}$ until the condition $\\alpha \\leq \\frac{18}{55}$ is met.\n1.  Try $\\alpha = 1$: Is $1 \\leq \\frac{18}{55}$? No, since $1 = \\frac{55}{55}$. Condition is not met. Backtrack.\n2.  Try $\\alpha = \\frac{1}{2}$: Is $\\frac{1}{2} \\leq \\frac{18}{55}$? This is equivalent to $55 \\leq 36$, which is false. Condition is not met. Backtrack.\n3.  Try $\\alpha = \\frac{1}{4}$: Is $\\frac{1}{4} \\leq \\frac{18}{55}$? This is equivalent to $55 \\leq 72$, which is true. Condition is met.\n\nThe accepted step length $\\alpha_k$ is the first value in the sequence $1, \\frac{1}{2}, \\frac{1}{4}, \\dots$ that satisfies the condition. This value is $\\frac{1}{4}$.",
            "answer": "$$\\boxed{\\frac{1}{4}}$$"
        },
        {
            "introduction": "While the steepest descent direction is intuitive, its performance can be painfully slow on the complex cost functions found in weather prediction. The Limited-memory BFGS (L-BFGS) method offers a powerful alternative by building an approximation of the inverse Hessian using information from recent iterations. This practice drills down into the core computational engine of L-BFGS: the famous two-loop recursion, which calculates the search direction efficiently without ever forming or storing large matrices. ",
            "id": "4063702",
            "problem": "In strong-constraint variational data assimilation for numerical weather prediction, one minimizes a twice-differentiable cost function $J(x)$ whose gradient $\\nabla J(x)$ is available but whose Hessian $\\nabla^{2}J(x)$ is not formed explicitly. A common strategy is to use quasi-Newton methods that build an approximation to the inverse Hessian and produce a descent direction from first-order information. Consider the Limited-memory Broyden–Fletcher–Goldfarb–Shanno (L-BFGS) method applied to a three-dimensional control vector $x \\in \\mathbb{R}^{3}$ representing bias-corrected analysis increments for three aggregated state components. At the current iterate $x_{k}$, the gradient is\n$$\ng_{k} \\equiv \\nabla J(x_{k}) = \\begin{pmatrix} 3 \\\\ -1 \\\\ 2 \\end{pmatrix}.\n$$\nFrom the two most recent iterations, you have the curvature (secant) pairs\n$$\ns_{1} = \\begin{pmatrix} 1 \\\\ 0 \\\\ -1 \\end{pmatrix}, \\quad y_{1} = \\begin{pmatrix} 2 \\\\ 0 \\\\ -1 \\end{pmatrix}, \\qquad\ns_{2} = \\begin{pmatrix} 0 \\\\ 2 \\\\ 1 \\end{pmatrix}, \\quad y_{2} = \\begin{pmatrix} 0 \\\\ 3 \\\\ 2 \\end{pmatrix},\n$$\nordered from older $(i=1)$ to newer $(i=2)$, and satisfying the curvature conditions $y_{i}^{\\top} s_{i} > 0$. Using the standard two-loop recursion for L-BFGS with initial inverse-Hessian scaling chosen as a scalar multiple of the identity, $H_{0} = \\gamma I$, where $\\gamma$ is set by the newest pair as\n$$\n\\gamma = \\frac{s_{2}^{\\top} y_{2}}{y_{2}^{\\top} y_{2}},\n$$\ncompute the L-BFGS search direction\n$$\np_{k} = - H_{k} g_{k},\n$$\nwhere $H_{k}$ is the L-BFGS inverse-Hessian approximation defined implicitly by the two-loop recursion applied to $g_{k}$ and the given $(s_{i}, y_{i})$.\n\nProvide the exact rational vector for $p_{k}$ in row form. Do not round.",
            "solution": "The user requests the computation of the Limited-memory Broyden–Fletcher–Goldfarb–Shanno (L-BFGS) search direction $p_k$ for a given gradient and history of updates. The computation is performed using the standard two-loop recursion algorithm.\n\nThe problem provides the following givens:\nThe control vector is in $\\mathbb{R}^{3}$.\nThe gradient at the current iterate $x_k$ is $g_k = \\nabla J(x_k) = \\begin{pmatrix} 3 \\\\ -1 \\\\ 2 \\end{pmatrix}$.\nThe two most recent update pairs (memory $m=2$), ordered from older to newer, are:\n$s_{1} = \\begin{pmatrix} 1 \\\\ 0 \\\\ -1 \\end{pmatrix}$, $y_{1} = \\begin{pmatrix} 2 \\\\ 0 \\\\ -1 \\end{pmatrix}$\n$s_{2} = \\begin{pmatrix} 0 \\\\ 2 \\\\ 1 \\end{pmatrix}$, $y_{2} = \\begin{pmatrix} 0 \\\\ 3 \\\\ 2 \\end{pmatrix}$\n\nThe search direction is given by $p_k = -H_k g_k$, where $H_k$ is the L-BFGS approximation of the inverse Hessian. The product $H_k g_k$ is computed implicitly using the two-loop recursion.\n\nFirst, we compute the scalar values $\\rho_i = \\frac{1}{y_i^\\top s_i}$ for $i=1, 2$.\nFor $i=1$:\n$$y_1^\\top s_1 = \\begin{pmatrix} 2 & 0 & -1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ -1 \\end{pmatrix} = (2)(1) + (0)(0) + (-1)(-1) = 3$$\n$$\\rho_1 = \\frac{1}{3}$$\nFor $i=2$:\n$$y_2^\\top s_2 = \\begin{pmatrix} 0 & 3 & 2 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 2 \\\\ 1 \\end{pmatrix} = (0)(0) + (3)(2) + (2)(1) = 8$$\n$$\\rho_2 = \\frac{1}{8}$$\nThe curvature conditions $y_i^\\top s_i > 0$ are satisfied.\n\nNext, we compute the scaling factor $\\gamma$ for the initial inverse-Hessian approximation $H_0 = \\gamma I$. The formula is given as $\\gamma = \\frac{s_2^\\top y_2}{y_2^\\top y_2}$.\nWe have $s_2^\\top y_2 = y_2^\\top s_2 = 8$.\nThe denominator is:\n$$y_2^\\top y_2 = \\begin{pmatrix} 0 & 3 & 2 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 3 \\\\ 2 \\end{pmatrix} = (0)^{2} + (3)^{2} + (2)^{2} = 9 + 4 = 13$$\nThus, the scaling factor is:\n$$\\gamma = \\frac{8}{13}$$\n\nNow, we apply the L-BFGS two-loop recursion.\n\n**Loop 1 (backward pass):**\nThis loop computes a vector $q$ and scalars $\\alpha_i$ for $i=m, \\dots, 1$. Here $m=2$.\nInitialize $q = g_k = \\begin{pmatrix} 3 \\\\ -1 \\\\ 2 \\end{pmatrix}$.\n\nFor $i=2$:\n$$\\alpha_2 = \\rho_2 s_2^\\top q = \\frac{1}{8} \\begin{pmatrix} 0 & 2 & 1 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ -1 \\\\ 2 \\end{pmatrix} = \\frac{1}{8} (0 - 2 + 2) = 0$$\n$$q \\leftarrow q - \\alpha_2 y_2 = \\begin{pmatrix} 3 \\\\ -1 \\\\ 2 \\end{pmatrix} - 0 \\cdot \\begin{pmatrix} 0 \\\\ 3 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ -1 \\\\ 2 \\end{pmatrix}$$\n\nFor $i=1$:\n$$\\alpha_1 = \\rho_1 s_1^\\top q = \\frac{1}{3} \\begin{pmatrix} 1 & 0 & -1 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ -1 \\\\ 2 \\end{pmatrix} = \\frac{1}{3} (3 - 2) = \\frac{1}{3}$$\n$$q \\leftarrow q - \\alpha_1 y_1 = \\begin{pmatrix} 3 \\\\ -1 \\\\ 2 \\end{pmatrix} - \\frac{1}{3} \\begin{pmatrix} 2 \\\\ 0 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 3 - \\frac{2}{3} \\\\ -1 \\\\ 2 + \\frac{1}{3} \\end{pmatrix} = \\begin{pmatrix} \\frac{7}{3} \\\\ -1 \\\\ \\frac{7}{3} \\end{pmatrix}$$\n\n**Initial Approximation Scaling:**\nThe result of the first loop, $q$, is now scaled by the initial inverse Hessian approximation $H_0 = \\gamma I$. Let the resulting vector be $r$.\n$$r = H_0 q = \\gamma q = \\frac{8}{13} \\begin{pmatrix} \\frac{7}{3} \\\\ -1 \\\\ \\frac{7}{3} \\end{pmatrix} = \\begin{pmatrix} \\frac{56}{39} \\\\ -\\frac{8}{13} \\\\ \\frac{56}{39} \\end{pmatrix}$$\n\n**Loop 2 (forward pass):**\nThis loop starts with the vector $r$ from the previous step and updates it for $i=1, \\dots, m$.\n\nFor $i=1$:\nWe compute a scalar $\\beta$ and update $r$.\n$$\\beta_1 = \\rho_1 y_1^\\top r = \\frac{1}{3} \\begin{pmatrix} 2 & 0 & -1 \\end{pmatrix} \\begin{pmatrix} \\frac{56}{39} \\\\ -\\frac{8}{13} \\\\ \\frac{56}{39} \\end{pmatrix} = \\frac{1}{3} \\left( 2 \\cdot \\frac{56}{39} - \\frac{56}{39} \\right) = \\frac{1}{3} \\cdot \\frac{56}{39} = \\frac{56}{117}$$\nNow update $r$ using $r \\leftarrow r + s_1(\\alpha_1 - \\beta_1)$. We already have $\\alpha_1 = \\frac{1}{3}$.\n$$\\alpha_1 - \\beta_1 = \\frac{1}{3} - \\frac{56}{117} = \\frac{39}{117} - \\frac{56}{117} = -\\frac{17}{117}$$\n$$r \\leftarrow \\begin{pmatrix} \\frac{56}{39} \\\\ -\\frac{8}{13} \\\\ \\frac{56}{39} \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ 0 \\\\ -1 \\end{pmatrix} \\left( -\\frac{17}{117} \\right) = \\begin{pmatrix} \\frac{56}{39} - \\frac{17}{117} \\\\ -\\frac{8}{13} \\\\ \\frac{56}{39} + \\frac{17}{117} \\end{pmatrix} = \\begin{pmatrix} \\frac{168 - 17}{117} \\\\ -\\frac{72}{117} \\\\ \\frac{168 + 17}{117} \\end{pmatrix} = \\begin{pmatrix} \\frac{151}{117} \\\\ -\\frac{72}{117} \\\\ \\frac{185}{117} \\end{pmatrix}$$\n\nFor $i=2$:\nWe compute $\\beta_2$ and update $r$ again. We have $\\alpha_2=0$.\n$$\\beta_2 = \\rho_2 y_2^\\top r = \\frac{1}{8} \\begin{pmatrix} 0 & 3 & 2 \\end{pmatrix} \\begin{pmatrix} \\frac{151}{117} \\\\ -\\frac{72}{117} \\\\ \\frac{185}{117} \\end{pmatrix} = \\frac{1}{8 \\cdot 117} \\left( 3(-72) + 2(185) \\right) = \\frac{-216 + 370}{936} = \\frac{154}{936}$$\nSimplifying the fraction gives $\\beta_2 = \\frac{77}{468}$.\nNow update $r$ using $r \\leftarrow r + s_2(\\alpha_2 - \\beta_2)$.\n$$\\alpha_2 - \\beta_2 = 0 - \\frac{77}{468} = -\\frac{77}{468}$$\n$$r \\leftarrow \\begin{pmatrix} \\frac{151}{117} \\\\ -\\frac{72}{117} \\\\ \\frac{185}{117} \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 2 \\\\ 1 \\end{pmatrix} \\left( -\\frac{77}{468} \\right) = \\begin{pmatrix} \\frac{151}{117} \\\\ -\\frac{72}{117} - \\frac{154}{468} \\\\ \\frac{185}{117} - \\frac{77}{468} \\end{pmatrix}$$\nWe use the common denominator $468 = 4 \\cdot 117$.\n$$r_1 = \\frac{4 \\cdot 151}{468} = \\frac{604}{468} = \\frac{151}{117}$$\n$$r_2 = -\\frac{4 \\cdot 72}{468} - \\frac{154}{468} = \\frac{-288 - 154}{468} = \\frac{-442}{468} = -\\frac{221}{234} = -\\frac{17 \\cdot 13}{18 \\cdot 13} = -\\frac{17}{18}$$\n$$r_3 = \\frac{4 \\cdot 185}{468} - \\frac{77}{468} = \\frac{740 - 77}{468} = \\frac{663}{468} = \\frac{221}{156} = \\frac{17 \\cdot 13}{12 \\cdot 13} = \\frac{17}{12}$$\nSo the final vector $r$ at the end of the recursion is $r = \\begin{pmatrix} \\frac{151}{117} \\\\ -\\frac{17}{18} \\\\ \\frac{17}{12} \\end{pmatrix}$.\n\nThe L-BFGS search direction is $p_k = -r$.\n$$p_k = - \\begin{pmatrix} \\frac{151}{117} \\\\ -\\frac{17}{18} \\\\ \\frac{17}{12} \\end{pmatrix} = \\begin{pmatrix} -\\frac{151}{117} \\\\ \\frac{17}{18} \\\\ -\\frac{17}{12} \\end{pmatrix}$$\nThe problem asks for the answer as a row vector.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n-\\frac{151}{117} & \\frac{17}{18} & -\\frac{17}{12}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "The efficiency of minimization algorithms is deeply tied to the geometry of the cost function. When physical constraints, such as geostrophic balance, create long, narrow valleys in the function landscape, convergence can stall. Preconditioning is a technique used to rescale the problem, making the cost function's geometry more uniform and easier for algorithms to navigate. This exercise demonstrates how to build a physically-motivated preconditioner and analyze its effectiveness by comparing the eigenvalues of the original and preconditioned Hessian matrices. ",
            "id": "4063610",
            "problem": "A local, two-variable variational analysis step in numerical weather prediction is posed for a single grid point in midlatitude, where geostrophic balance couples the meridional wind and the zonal geopotential gradient. Let the state vector be $x = (v, \\phi)^{\\mathsf{T}}$, where $v$ is the meridional wind component and $\\phi$ is the nondimensional zonal geopotential gradient. Assume all variables have been nondimensionalized using standard midlatitude scales so that the Hessian is dimensionless. Consider the canonical three-dimensional variational (3D-Var) cost function\n$$\nJ(x) = \\frac{1}{2} (x - x_b)^{\\mathsf{T}} B^{-1} (x - x_b) + \\frac{1}{2} (H_{\\mathrm{obs}} x - y)^{\\mathsf{T}} R^{-1} (H_{\\mathrm{obs}} x - y) + \\frac{1}{2} \\beta \\left(f v - \\phi\\right)^{2},\n$$\nwhere $x_b$ is the background state, $B$ is the background-error covariance, $H_{\\mathrm{obs}}$ is the linear observation operator, $R$ is the observation-error covariance, $f$ is the nondimensional Coriolis parameter, and $\\beta>0$ is a balance penalty weight. For this local analysis, take $H_{\\mathrm{obs}}$ to be the identity on both components, and the inverse covariances to be diagonal:\n$$\nB^{-1} = \\begin{pmatrix} \\alpha_v & 0 \\\\ 0 & \\alpha_{\\phi} \\end{pmatrix}, \\quad R^{-1} = \\begin{pmatrix} \\rho_v & 0 \\\\ 0 & \\rho_{\\phi} \\end{pmatrix}.\n$$\nThe geostrophic balance penalty couples $v$ and $\\phi$ through the transform $w = T x$ with\n$$\nT = \\begin{pmatrix} f & -1 \\\\ 1 & 0 \\end{pmatrix}, \\quad \\text{so that } w_1 = f v - \\phi, \\; w_2 = v,\n$$\nand the penalty applies to $w_1$ only. With the specific parameter values $f = 1$, $\\alpha_v = 2$, $\\alpha_{\\phi} = 3$, $\\rho_v = 5$, $\\rho_{\\phi} = 7$, and $\\beta = 8$, do the following:\n\n1. Derive the Hessian $H$ of $J(x)$ with respect to $x$ and compute its two eigenvalues.\n2. Define a balance-aware symmetric positive definite preconditioner $M$ by transforming to the balanced coordinates and using a diagonal approximation there:\n$$\nM = T^{\\mathsf{T}} \\begin{pmatrix} d_1 & 0 \\\\ 0 & d_2 \\end{pmatrix} T,\n$$\nwith $d_1 = \\beta$ and $d_2 = \\alpha_v + \\rho_v$. Compute the two eigenvalues of the preconditioned operator $M^{-1} H$.\n\nExpress all eigenvalues as exact analytic numbers with no rounding and treat them as dimensionless quantities. Your final answer should list the four eigenvalues (first the two of $H$, then the two of $M^{-1} H$) as a single row matrix.",
            "solution": "The problem asks for the computation of the eigenvalues of the Hessian matrix of a variational cost function, and the eigenvalues of a preconditioned version of that Hessian. The process is divided into two parts.\n\n### Part 1: Hessian and its Eigenvalues\n\nThe cost function is given by\n$$\nJ(x) = \\frac{1}{2} (x - x_b)^{\\mathsf{T}} B^{-1} (x - x_b) + \\frac{1}{2} (H_{\\mathrm{obs}} x - y)^{\\mathsf{T}} R^{-1} (H_{\\mathrm{obs}} x - y) + \\frac{1}{2} \\beta \\left(f v - \\phi\\right)^{2}\n$$\nwhere the state vector is $x = (v, \\phi)^{\\mathsf{T}}$. Since $J(x)$ is a quadratic function of $x$, its Hessian, $H = \\nabla^2 J(x)$, is a constant matrix. The Hessian is the sum of the Hessians of the three terms in $J(x)$.\n\n1.  The background term is $J_b(x) = \\frac{1}{2} (x - x_b)^{\\mathsf{T}} B^{-1} (x - x_b)$. Its Hessian is $\\nabla^2 J_b = B^{-1}$.\n2.  The observation term is $J_o(x) = \\frac{1}{2} (H_{\\mathrm{obs}} x - y)^{\\mathsf{T}} R^{-1} (H_{\\mathrm{obs}} x - y)$. Since $H_{\\mathrm{obs}}$ is a linear operator, the Hessian is $\\nabla^2 J_o = H_{\\mathrm{obs}}^{\\mathsf{T}} R^{-1} H_{\\mathrm{obs}}$.\n3.  The balance penalty term is $J_c(x) = \\frac{1}{2} \\beta (f v - \\phi)^2$. This can be written as $J_c(x) = \\frac{1}{2} \\beta (K x)^{\\mathsf{T}} (K x) = \\frac{1}{2} \\beta x^{\\mathsf{T}} K^{\\mathsf{T}} K x$, where $K = \\begin{pmatrix} f & -1 \\end{pmatrix}$. The Hessian is $\\nabla^2 J_c = \\beta K^{\\mathsf{T}} K$.\n\nThe total Hessian $H$ is the sum:\n$$\nH = B^{-1} + H_{\\mathrm{obs}}^{\\mathsf{T}} R^{-1} H_{\\mathrm{obs}} + \\beta K^{\\mathsf{T}} K\n$$\nWe are given $H_{\\mathrm{obs}} = I$, the identity matrix. Thus, $H_{\\mathrm{obs}}^{\\mathsf{T}} R^{-1} H_{\\mathrm{obs}} = R^{-1}$. The expression for the Hessian simplifies to\n$$\nH = B^{-1} + R^{-1} + \\beta K^{\\mathsf{T}} K\n$$\nNow, we substitute the given values and matrices. The parameter values are $f = 1$, $\\alpha_v = 2$, $\\alpha_{\\phi} = 3$, $\\rho_v = 5$, $\\rho_{\\phi} = 7$, and $\\beta = 8$.\nThe inverse covariance matrices are:\n$$\nB^{-1} = \\begin{pmatrix} \\alpha_v & 0 \\\\ 0 & \\alpha_{\\phi} \\end{pmatrix} = \\begin{pmatrix} 2 & 0 \\\\ 0 & 3 \\end{pmatrix}\n$$\n$$\nR^{-1} = \\begin{pmatrix} \\rho_v & 0 \\\\ 0 & \\rho_{\\phi} \\end{pmatrix} = \\begin{pmatrix} 5 & 0 \\\\ 0 & 7 \\end{pmatrix}\n$$\nThe penalty matrix term is:\n$$\nK = \\begin{pmatrix} f & -1 \\end{pmatrix} = \\begin{pmatrix} 1 & -1 \\end{pmatrix}\n$$\n$$\n\\beta K^{\\mathsf{T}} K = 8 \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} \\begin{pmatrix} 1 & -1 \\end{pmatrix} = 8 \\begin{pmatrix} 1 & -1 \\\\ -1 & 1 \\end{pmatrix} = \\begin{pmatrix} 8 & -8 \\\\ -8 & 8 \\end{pmatrix}\n$$\nSumming the components to find $H$:\n$$\nH = \\begin{pmatrix} 2 & 0 \\\\ 0 & 3 \\end{pmatrix} + \\begin{pmatrix} 5 & 0 \\\\ 0 & 7 \\end{pmatrix} + \\begin{pmatrix} 8 & -8 \\\\ -8 & 8 \\end{pmatrix} = \\begin{pmatrix} 2+5+8 & -8 \\\\ -8 & 3+7+8 \\end{pmatrix} = \\begin{pmatrix} 15 & -8 \\\\ -8 & 18 \\end{pmatrix}\n$$\nTo find the eigenvalues of $H$, we solve the characteristic equation $\\det(H - \\lambda I) = 0$:\n$$\n\\det \\begin{pmatrix} 15-\\lambda & -8 \\\\ -8 & 18-\\lambda \\end{pmatrix} = (15 - \\lambda)(18 - \\lambda) - (-8)(-8) = 0\n$$\n$$\n270 - 15\\lambda - 18\\lambda + \\lambda^2 - 64 = 0\n$$\n$$\n\\lambda^2 - 33\\lambda + 206 = 0\n$$\nUsing the quadratic formula $\\lambda = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$:\n$$\n\\lambda = \\frac{33 \\pm \\sqrt{(-33)^2 - 4(1)(206)}}{2} = \\frac{33 \\pm \\sqrt{1089 - 824}}{2} = \\frac{33 \\pm \\sqrt{265}}{2}\n$$\nThe number $265$ is square-free ($265 = 5 \\times 53$). Thus, the two eigenvalues of $H$ are:\n$$\n\\lambda_{H,1} = \\frac{33 - \\sqrt{265}}{2}, \\quad \\lambda_{H,2} = \\frac{33 + \\sqrt{265}}{2}\n$$\n\n### Part 2: Preconditioned Operator and its Eigenvalues\n\nThe preconditioner $M$ is defined as $M = T^{\\mathsf{T}} D T$, with\n$$\nT = \\begin{pmatrix} f & -1 \\\\ 1 & 0 \\end{pmatrix} = \\begin{pmatrix} 1 & -1 \\\\ 1 & 0 \\end{pmatrix}\n$$\nand a diagonal matrix $D$ with entries $d_1 = \\beta = 8$ and $d_2 = \\alpha_v + \\rho_v = 2 + 5 = 7$.\n$$\nD = \\begin{pmatrix} d_1 & 0 \\\\ 0 & d_2 \\end{pmatrix} = \\begin{pmatrix} 8 & 0 \\\\ 0 & 7 \\end{pmatrix}\n$$\nThe transpose of $T$ is $T^{\\mathsf{T}} = \\begin{pmatrix} 1 & 1 \\\\ -1 & 0 \\end{pmatrix}$.\nNow we compute $M$:\n$$\nM = T^{\\mathsf{T}} D T = \\begin{pmatrix} 1 & 1 \\\\ -1 & 0 \\end{pmatrix} \\begin{pmatrix} 8 & 0 \\\\ 0 & 7 \\end{pmatrix} \\begin{pmatrix} 1 & -1 \\\\ 1 & 0 \\end{pmatrix}\n$$\n$$\nM = \\begin{pmatrix} 1 & 1 \\\\ -1 & 0 \\end{pmatrix} \\begin{pmatrix} 8(1)+0(1) & 8(-1)+0(0) \\\\ 0(1)+7(1) & 0(-1)+7(0) \\end{pmatrix} = \\begin{pmatrix} 1 & 1 \\\\ -1 & 0 \\end{pmatrix} \\begin{pmatrix} 8 & -8 \\\\ 7 & 0 \\end{pmatrix}\n$$\n$$\nM = \\begin{pmatrix} 1(8)+1(7) & 1(-8)+1(0) \\\\ -1(8)+0(7) & -1(-8)+0(0) \\end{pmatrix} = \\begin{pmatrix} 15 & -8 \\\\ -8 & 8 \\end{pmatrix}\n$$\nNext, we find the inverse of $M$:\n$$\n\\det(M) = (15)(8) - (-8)(-8) = 120 - 64 = 56\n$$\n$$\nM^{-1} = \\frac{1}{56} \\begin{pmatrix} 8 & 8 \\\\ 8 & 15 \\end{pmatrix}\n$$\nNow we compute the preconditioned operator $P = M^{-1} H$:\n$$\nP = \\frac{1}{56} \\begin{pmatrix} 8 & 8 \\\\ 8 & 15 \\end{pmatrix} \\begin{pmatrix} 15 & -8 \\\\ -8 & 18 \\end{pmatrix}\n$$\n$$\nP = \\frac{1}{56} \\begin{pmatrix} 8(15)+8(-8) & 8(-8)+8(18) \\\\ 8(15)+15(-8) & 8(-8)+15(18) \\end{pmatrix}\n$$\n$$\nP = \\frac{1}{56} \\begin{pmatrix} 120-64 & -64+144 \\\\ 120-120 & -64+270 \\end{pmatrix} = \\frac{1}{56} \\begin{pmatrix} 56 & 80 \\\\ 0 & 206 \\end{pmatrix}\n$$\n$$\nP = \\begin{pmatrix} \\frac{56}{56} & \\frac{80}{56} \\\\ 0 & \\frac{206}{56} \\end{pmatrix} = \\begin{pmatrix} 1 & \\frac{10}{7} \\\\ 0 & \\frac{103}{28} \\end{pmatrix}\n$$\nThe eigenvalues of an upper triangular matrix are its diagonal entries. Therefore, the eigenvalues of $P = M^{-1}H$ are:\n$$\n\\lambda_{P,1} = 1, \\quad \\lambda_{P,2} = \\frac{103}{28}\n$$\nThe problem requests the four eigenvalues as a single row matrix, listing the two for $H$ first, followed by the two for $M^{-1}H$.\n\nThe four eigenvalues are $\\frac{33 - \\sqrt{265}}{2}$, $\\frac{33 + \\sqrt{265}}{2}$, $1$, and $\\frac{103}{28}$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{33 - \\sqrt{265}}{2} & \\frac{33 + \\sqrt{265}}{2} & 1 & \\frac{103}{28} \\end{pmatrix}}\n$$"
        }
    ]
}