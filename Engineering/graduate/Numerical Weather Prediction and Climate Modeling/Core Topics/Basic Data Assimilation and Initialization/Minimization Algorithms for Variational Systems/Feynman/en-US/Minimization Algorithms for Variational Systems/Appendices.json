{
    "hands_on_practices": [
        {
            "introduction": "In large-scale variational data assimilation, explicitly forming and inverting the Hessian matrix is computationally prohibitive. Quasi-Newton methods like the Limited-memory Broyden–Fletcher–Goldfarb–Shanno (L-BFGS) algorithm provide an elegant solution by building an approximation of the inverse Hessian using only recent gradient and state information. This exercise takes you through the core mechanical step of L-BFGS, the two-loop recursion, to compute a search direction, offering a hands-on understanding of how this powerful algorithm works in practice. ",
            "id": "4063702",
            "problem": "In strong-constraint variational data assimilation for numerical weather prediction, one minimizes a twice-differentiable cost function $J(x)$ whose gradient $\\nabla J(x)$ is available but whose Hessian $\\nabla^{2}J(x)$ is not formed explicitly. A common strategy is to use quasi-Newton methods that build an approximation to the inverse Hessian and produce a descent direction from first-order information. Consider the Limited-memory Broyden–Fletcher–Goldfarb–Shanno (L-BFGS) method applied to a three-dimensional control vector $x \\in \\mathbb{R}^{3}$ representing bias-corrected analysis increments for three aggregated state components. At the current iterate $x_{k}$, the gradient is\n$$\ng_{k} \\equiv \\nabla J(x_{k}) = \\begin{pmatrix} 3 \\\\ -1 \\\\ 2 \\end{pmatrix}.\n$$\nFrom the two most recent iterations, you have the curvature (secant) pairs\n$$\ns_{1} = \\begin{pmatrix} 1 \\\\ 0 \\\\ -1 \\end{pmatrix}, \\quad y_{1} = \\begin{pmatrix} 2 \\\\ 0 \\\\ -1 \\end{pmatrix}, \\qquad\ns_{2} = \\begin{pmatrix} 0 \\\\ 2 \\\\ 1 \\end{pmatrix}, \\quad y_{2} = \\begin{pmatrix} 0 \\\\ 3 \\\\ 2 \\end{pmatrix},\n$$\nordered from older ($i=1$) to newer ($i=2$), and satisfying the curvature conditions $y_{i}^{\\top} s_{i} > 0$. Using the standard two-loop recursion for L-BFGS with initial inverse-Hessian scaling chosen as a scalar multiple of the identity, $H_{0} = \\gamma I$, where $\\gamma$ is set by the newest pair as\n$$\n\\gamma = \\frac{s_{2}^{\\top} y_{2}}{y_{2}^{\\top} y_{2}},\n$$\ncompute the L-BFGS search direction\n$$\np_{k} = - H_{k} g_{k},\n$$\nwhere $H_{k}$ is the L-BFGS inverse-Hessian approximation defined implicitly by the two-loop recursion applied to $g_{k}$ and the given $(s_{i}, y_{i})$.\n\nProvide the exact rational vector for $p_{k}$ in row form. Do not round.",
            "solution": "The L-BFGS search direction $p_k$ is computed for a given gradient and history of updates using the standard two-loop recursion algorithm.\n\nThe problem provides the following:\nThe control vector is in $\\mathbb{R}^{3}$.\nThe gradient at the current iterate $x_k$ is $g_k = \\nabla J(x_k) = \\begin{pmatrix} 3 \\\\ -1 \\\\ 2 \\end{pmatrix}$.\nThe two most recent update pairs (memory $m=2$), ordered from older to newer, are:\n$s_{1} = \\begin{pmatrix} 1 \\\\ 0 \\\\ -1 \\end{pmatrix}$, $y_{1} = \\begin{pmatrix} 2 \\\\ 0 \\\\ -1 \\end{pmatrix}$\n$s_{2} = \\begin{pmatrix} 0 \\\\ 2 \\\\ 1 \\end{pmatrix}$, $y_{2} = \\begin{pmatrix} 0 \\\\ 3 \\\\ 2 \\end{pmatrix}$\n\nThe search direction is given by $p_k = -H_k g_k$, where $H_k$ is the L-BFGS approximation of the inverse Hessian. The product $H_k g_k$ is computed implicitly using the two-loop recursion.\n\nFirst, we compute the scalar values $\\rho_i = \\frac{1}{y_i^\\top s_i}$ for $i=1, 2$.\nFor $i=1$:\n$$y_1^\\top s_1 = \\begin{pmatrix} 2 & 0 & -1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ -1 \\end{pmatrix} = (2)(1) + (0)(0) + (-1)(-1) = 3$$\n$$\\rho_1 = \\frac{1}{3}$$\nFor $i=2$:\n$$y_2^\\top s_2 = \\begin{pmatrix} 0 & 3 & 2 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 2 \\\\ 1 \\end{pmatrix} = (0)(0) + (3)(2) + (2)(1) = 8$$\n$$\\rho_2 = \\frac{1}{8}$$\nThe curvature conditions $y_i^\\top s_i > 0$ are satisfied.\n\nNext, we compute the scaling factor $\\gamma$ for the initial inverse-Hessian approximation $H_0 = \\gamma I$. The formula is given as $\\gamma = \\frac{s_2^\\top y_2}{y_2^\\top y_2}$.\nWe have $s_2^\\top y_2 = y_2^\\top s_2 = 8$.\nThe denominator is:\n$$y_2^\\top y_2 = \\begin{pmatrix} 0 & 3 & 2 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 3 \\\\ 2 \\end{pmatrix} = (0)^{2} + (3)^{2} + (2)^{2} = 9 + 4 = 13$$\nThus, the scaling factor is:\n$$\\gamma = \\frac{8}{13}$$\n\nNow, we apply the L-BFGS two-loop recursion.\n\n**Loop 1 (backward pass):**\nThis loop computes a vector $q$ and scalars $\\alpha_i$ for $i=m, \\dots, 1$. Here $m=2$.\nInitialize $q = g_k = \\begin{pmatrix} 3 \\\\ -1 \\\\ 2 \\end{pmatrix}$.\n\nFor $i=2$:\n$$\\alpha_2 = \\rho_2 s_2^\\top q = \\frac{1}{8} \\begin{pmatrix} 0 & 2 & 1 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ -1 \\\\ 2 \\end{pmatrix} = \\frac{1}{8} (0 - 2 + 2) = 0$$\n$$q \\leftarrow q - \\alpha_2 y_2 = \\begin{pmatrix} 3 \\\\ -1 \\\\ 2 \\end{pmatrix} - 0 \\cdot \\begin{pmatrix} 0 \\\\ 3 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ -1 \\\\ 2 \\end{pmatrix}$$\n\nFor $i=1$:\n$$\\alpha_1 = \\rho_1 s_1^\\top q = \\frac{1}{3} \\begin{pmatrix} 1 & 0 & -1 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ -1 \\\\ 2 \\end{pmatrix} = \\frac{1}{3} (3 - 2) = \\frac{1}{3}$$\n$$q \\leftarrow q - \\alpha_1 y_1 = \\begin{pmatrix} 3 \\\\ -1 \\\\ 2 \\end{pmatrix} - \\frac{1}{3} \\begin{pmatrix} 2 \\\\ 0 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 3 - \\frac{2}{3} \\\\ -1 \\\\ 2 + \\frac{1}{3} \\end{pmatrix} = \\begin{pmatrix} \\frac{7}{3} \\\\ -1 \\\\ \\frac{7}{3} \\end{pmatrix}$$\n\n**Initial Approximation Scaling:**\nThe result of the first loop, $q$, is now scaled by the initial inverse Hessian approximation $H_0 = \\gamma I$. Let the resulting vector be $r$.\n$$r = H_0 q = \\gamma q = \\frac{8}{13} \\begin{pmatrix} \\frac{7}{3} \\\\ -1 \\\\ \\frac{7}{3} \\end{pmatrix} = \\begin{pmatrix} \\frac{56}{39} \\\\ -\\frac{8}{13} \\\\ \\frac{56}{39} \\end{pmatrix}$$\n\n**Loop 2 (forward pass):**\nThis loop starts with the vector $r$ from the previous step and updates it for $i=1, \\dots, m$.\n\nFor $i=1$:\nWe compute a scalar $\\beta_1$ and update $r$.\n$$\\beta_1 = \\rho_1 y_1^\\top r = \\frac{1}{3} \\begin{pmatrix} 2 & 0 & -1 \\end{pmatrix} \\begin{pmatrix} \\frac{56}{39} \\\\ -\\frac{8}{13} \\\\ \\frac{56}{39} \\end{pmatrix} = \\frac{1}{3} \\left( 2 \\cdot \\frac{56}{39} - \\frac{56}{39} \\right) = \\frac{1}{3} \\cdot \\frac{56}{39} = \\frac{56}{117}$$\nNow update $r$ using $r \\leftarrow r + s_1(\\alpha_1 - \\beta_1)$. We already have $\\alpha_1 = \\frac{1}{3}$.\n$$\\alpha_1 - \\beta_1 = \\frac{1}{3} - \\frac{56}{117} = \\frac{39}{117} - \\frac{56}{117} = -\\frac{17}{117}$$\n$$r \\leftarrow \\begin{pmatrix} \\frac{56}{39} \\\\ -\\frac{8}{13} \\\\ \\frac{56}{39} \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ 0 \\\\ -1 \\end{pmatrix} \\left( -\\frac{17}{117} \\right) = \\begin{pmatrix} \\frac{56}{39} - \\frac{17}{117} \\\\ -\\frac{8}{13} \\\\ \\frac{56}{39} + \\frac{17}{117} \\end{pmatrix} = \\begin{pmatrix} \\frac{168 - 17}{117} \\\\ -\\frac{72}{117} \\\\ \\frac{168 + 17}{117} \\end{pmatrix} = \\begin{pmatrix} \\frac{151}{117} \\\\ -\\frac{72}{117} \\\\ \\frac{185}{117} \\end{pmatrix}$$\n\nFor $i=2$:\nWe compute $\\beta_2$ and update $r$ again. We have $\\alpha_2=0$.\n$$\\beta_2 = \\rho_2 y_2^\\top r = \\frac{1}{8} \\begin{pmatrix} 0 & 3 & 2 \\end{pmatrix} \\begin{pmatrix} \\frac{151}{117} \\\\ -\\frac{72}{117} \\\\ \\frac{185}{117} \\end{pmatrix} = \\frac{1}{8 \\cdot 117} \\left( 3(-72) + 2(185) \\right) = \\frac{-216 + 370}{936} = \\frac{154}{936} = \\frac{77}{468}$$\nNow update $r$ using $r \\leftarrow r + s_2(\\alpha_2 - \\beta_2)$.\n$$\\alpha_2 - \\beta_2 = 0 - \\frac{77}{468} = -\\frac{77}{468}$$\n$$r \\leftarrow \\begin{pmatrix} \\frac{151}{117} \\\\ -\\frac{72}{117} \\\\ \\frac{185}{117} \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 2 \\\\ 1 \\end{pmatrix} \\left( -\\frac{77}{468} \\right) = \\begin{pmatrix} \\frac{151}{117} \\\\ -\\frac{72}{117} - \\frac{154}{468} \\\\ \\frac{185}{117} - \\frac{77}{468} \\end{pmatrix}$$\nWe use the common denominator $468 = 4 \\cdot 117$.\n$$r_1 = \\frac{4 \\cdot 151}{468} = \\frac{604}{468} = \\frac{151}{117}$$\n$$r_2 = -\\frac{4 \\cdot 72}{468} - \\frac{154}{468} = \\frac{-288 - 154}{468} = \\frac{-442}{468} = -\\frac{221}{234} = -\\frac{17}{18}$$\n$$r_3 = \\frac{4 \\cdot 185}{468} - \\frac{77}{468} = \\frac{740 - 77}{468} = \\frac{663}{468} = \\frac{221}{156} = \\frac{17}{12}$$\nSo the final vector $r$ at the end of the recursion is $r = \\begin{pmatrix} \\frac{151}{117} \\\\ -\\frac{17}{18} \\\\ \\frac{17}{12} \\end{pmatrix}$.\n\nThe L-BFGS search direction is $p_k = -r$.\n$$p_k = - \\begin{pmatrix} \\frac{151}{117} \\\\ -\\frac{17}{18} \\\\ \\frac{17}{12} \\end{pmatrix} = \\begin{pmatrix} -\\frac{151}{117} \\\\ \\frac{17}{18} \\\\ -\\frac{17}{12} \\end{pmatrix}$$\nThe problem asks for the answer as a row vector.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n-\\frac{151}{117} & \\frac{17}{18} & -\\frac{17}{12}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "An effective search direction is only part of the story; the speed of convergence for minimization algorithms is heavily influenced by the conditioning of the problem's Hessian matrix. In geophysical applications, strong physical constraints like geostrophic balance often lead to ill-conditioned Hessians, slowing down convergence. This practice explores how a balance-aware preconditioner, designed to respect the underlying physics, can transform the problem into one that is much easier to solve, a concept we will quantify by comparing the eigenvalues of the original and preconditioned systems. ",
            "id": "4063610",
            "problem": "A local, two-variable variational analysis step in numerical weather prediction is posed for a single grid point in midlatitude, where geostrophic balance couples the meridional wind and the zonal geopotential gradient. Let the state vector be $x = (v, \\phi)^{\\mathsf{T}}$, where $v$ is the meridional wind component and $\\phi$ is the nondimensional zonal geopotential gradient. Assume all variables have been nondimensionalized using standard midlatitude scales so that the Hessian is dimensionless. Consider the canonical three-dimensional variational (3D-Var) cost function\n$$\nJ(x) = \\frac{1}{2} (x - x_b)^{\\mathsf{T}} B^{-1} (x - x_b) + \\frac{1}{2} (H_{\\mathrm{obs}} x - y)^{\\mathsf{T}} R^{-1} (H_{\\mathrm{obs}} x - y) + \\frac{1}{2} \\beta \\left(f v - \\phi\\right)^{2},\n$$\nwhere $x_b$ is the background state, $B$ is the background-error covariance, $H_{\\mathrm{obs}}$ is the linear observation operator, $R$ is the observation-error covariance, $f$ is the nondimensional Coriolis parameter, and $\\beta>0$ is a balance penalty weight. For this local analysis, take $H_{\\mathrm{obs}}$ to be the identity on both components, and the inverse covariances to be diagonal:\n$$\nB^{-1} = \\begin{pmatrix} \\alpha_v & 0 \\\\ 0 & \\alpha_{\\phi} \\end{pmatrix}, \\quad R^{-1} = \\begin{pmatrix} \\rho_v & 0 \\\\ 0 & \\rho_{\\phi} \\end{pmatrix}.\n$$\nThe geostrophic balance penalty couples $v$ and $\\phi$ through the transform $w = T x$ with\n$$\nT = \\begin{pmatrix} f & -1 \\\\ 1 & 0 \\end{pmatrix}, \\quad \\text{so that } w_1 = f v - \\phi, \\; w_2 = v,\n$$\nand the penalty applies to $w_1$ only. With the specific parameter values $f = 1$, $\\alpha_v = 2$, $\\alpha_{\\phi} = 3$, $\\rho_v = 5$, $\\rho_{\\phi} = 7$, and $\\beta = 8$, do the following:\n\n1. Derive the Hessian $H$ of $J(x)$ with respect to $x$ and compute its two eigenvalues.\n2. Define a balance-aware symmetric positive definite preconditioner $M$ by transforming to the balanced coordinates and using a diagonal approximation there:\n$$\nM = T^{\\mathsf{T}} \\begin{pmatrix} d_1 & 0 \\\\ 0 & d_2 \\end{pmatrix} T,\n$$\nwith $d_1 = \\beta$ and $d_2 = \\alpha_v + \\rho_v$. Compute the two eigenvalues of the preconditioned operator $M^{-1} H$.\n\nExpress all eigenvalues as exact analytic numbers with no rounding and treat them as dimensionless quantities. Your final answer should list the four eigenvalues (first the two of $H$, then the two of $M^{-1} H$) as a single row matrix.",
            "solution": "The problem asks for the computation of the eigenvalues of the Hessian matrix of a variational cost function, and the eigenvalues of a preconditioned version of that Hessian. The process is divided into two parts.\n\n### Part 1: Hessian and its Eigenvalues\n\nThe cost function is given by\n$$\nJ(x) = \\frac{1}{2} (x - x_b)^{\\mathsf{T}} B^{-1} (x - x_b) + \\frac{1}{2} (H_{\\mathrm{obs}} x - y)^{\\mathsf{T}} R^{-1} (H_{\\mathrm{obs}} x - y) + \\frac{1}{2} \\beta \\left(f v - \\phi\\right)^{2}\n$$\nwhere the state vector is $x = (v, \\phi)^{\\mathsf{T}}$. Since $J(x)$ is a quadratic function of $x$, its Hessian, $H = \\nabla^2 J(x)$, is a constant matrix. The Hessian is the sum of the Hessians of the three terms in $J(x)$.\n\n1.  The background term is $J_b(x) = \\frac{1}{2} (x - x_b)^{\\mathsf{T}} B^{-1} (x - x_b)$. Its Hessian is $\\nabla^2 J_b = B^{-1}$.\n2.  The observation term is $J_o(x) = \\frac{1}{2} (H_{\\mathrm{obs}} x - y)^{\\mathsf{T}} R^{-1} (H_{\\mathrm{obs}} x - y)$. Since $H_{\\mathrm{obs}}$ is a linear operator, the Hessian is $\\nabla^2 J_o = H_{\\mathrm{obs}}^{\\mathsf{T}} R^{-1} H_{\\mathrm{obs}}$.\n3.  The balance penalty term is $J_c(x) = \\frac{1}{2} \\beta (f v - \\phi)^2$. This can be written as $J_c(x) = \\frac{1}{2} \\beta (K x)^{\\mathsf{T}} (K x) = \\frac{1}{2} \\beta x^{\\mathsf{T}} K^{\\mathsf{T}} K x$, where $K = \\begin{pmatrix} f & -1 \\end{pmatrix}$. The Hessian is $\\nabla^2 J_c = \\beta K^{\\mathsf{T}} K$.\n\nThe total Hessian $H$ is the sum:\n$$\nH = B^{-1} + H_{\\mathrm{obs}}^{\\mathsf{T}} R^{-1} H_{\\mathrm{obs}} + \\beta K^{\\mathsf{T}} K\n$$\nWe are given $H_{\\mathrm{obs}} = I$, the identity matrix. Thus, $H_{\\mathrm{obs}}^{\\mathsf{T}} R^{-1} H_{\\mathrm{obs}} = R^{-1}$. The expression for the Hessian simplifies to\n$$\nH = B^{-1} + R^{-1} + \\beta K^{\\mathsf{T}} K\n$$\nNow, we substitute the given values and matrices. The parameter values are $f = 1$, $\\alpha_v = 2$, $\\alpha_{\\phi} = 3$, $\\rho_v = 5$, $\\rho_{\\phi} = 7$, and $\\beta = 8$.\nThe inverse covariance matrices are:\n$$\nB^{-1} = \\begin{pmatrix} \\alpha_v & 0 \\\\ 0 & \\alpha_{\\phi} \\end{pmatrix} = \\begin{pmatrix} 2 & 0 \\\\ 0 & 3 \\end{pmatrix}\n$$\n$$\nR^{-1} = \\begin{pmatrix} \\rho_v & 0 \\\\ 0 & \\rho_{\\phi} \\end{pmatrix} = \\begin{pmatrix} 5 & 0 \\\\ 0 & 7 \\end{pmatrix}\n$$\nThe penalty matrix term is:\n$$\nK = \\begin{pmatrix} f & -1 \\end{pmatrix} = \\begin{pmatrix} 1 & -1 \\end{pmatrix}\n$$\n$$\n\\beta K^{\\mathsf{T}} K = 8 \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} \\begin{pmatrix} 1 & -1 \\end{pmatrix} = 8 \\begin{pmatrix} 1 & -1 \\\\ -1 & 1 \\end{pmatrix} = \\begin{pmatrix} 8 & -8 \\\\ -8 & 8 \\end{pmatrix}\n$$\nSumming the components to find $H$:\n$$\nH = \\begin{pmatrix} 2 & 0 \\\\ 0 & 3 \\end{pmatrix} + \\begin{pmatrix} 5 & 0 \\\\ 0 & 7 \\end{pmatrix} + \\begin{pmatrix} 8 & -8 \\\\ -8 & 8 \\end{pmatrix} = \\begin{pmatrix} 2+5+8 & -8 \\\\ -8 & 3+7+8 \\end{pmatrix} = \\begin{pmatrix} 15 & -8 \\\\ -8 & 18 \\end{pmatrix}\n$$\nTo find the eigenvalues of $H$, we solve the characteristic equation $\\det(H - \\lambda I) = 0$:\n$$\n\\det \\begin{pmatrix} 15-\\lambda & -8 \\\\ -8 & 18-\\lambda \\end{pmatrix} = (15 - \\lambda)(18 - \\lambda) - (-8)(-8) = 0\n$$\n$$\n270 - 15\\lambda - 18\\lambda + \\lambda^2 - 64 = 0\n$$\n$$\n\\lambda^2 - 33\\lambda + 206 = 0\n$$\nUsing the quadratic formula $\\lambda = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$:\n$$\n\\lambda = \\frac{33 \\pm \\sqrt{(-33)^2 - 4(1)(206)}}{2} = \\frac{33 \\pm \\sqrt{1089 - 824}}{2} = \\frac{33 \\pm \\sqrt{265}}{2}\n$$\nThe number $265$ is square-free ($265 = 5 \\times 53$). Thus, the two eigenvalues of $H$ are:\n$$\n\\lambda_{H,1} = \\frac{33 - \\sqrt{265}}{2}, \\quad \\lambda_{H,2} = \\frac{33 + \\sqrt{265}}{2}\n$$\n\n### Part 2: Preconditioned Operator and its Eigenvalues\n\nThe preconditioner $M$ is defined as $M = T^{\\mathsf{T}} D T$, with\n$$\nT = \\begin{pmatrix} f & -1 \\\\ 1 & 0 \\end{pmatrix} = \\begin{pmatrix} 1 & -1 \\\\ 1 & 0 \\end{pmatrix}\n$$\nand a diagonal matrix $D$ with entries $d_1 = \\beta = 8$ and $d_2 = \\alpha_v + \\rho_v = 2 + 5 = 7$.\n$$\nD = \\begin{pmatrix} d_1 & 0 \\\\ 0 & d_2 \\end{pmatrix} = \\begin{pmatrix} 8 & 0 \\\\ 0 & 7 \\end{pmatrix}\n$$\nThe transpose of $T$ is $T^{\\mathsf{T}} = \\begin{pmatrix} 1 & 1 \\\\ -1 & 0 \\end{pmatrix}$.\nNow we compute $M$:\n$$\nM = T^{\\mathsf{T}} D T = \\begin{pmatrix} 1 & 1 \\\\ -1 & 0 \\end{pmatrix} \\begin{pmatrix} 8 & 0 \\\\ 0 & 7 \\end{pmatrix} \\begin{pmatrix} 1 & -1 \\\\ 1 & 0 \\end{pmatrix}\n$$\n$$\nM = \\begin{pmatrix} 1 & 1 \\\\ -1 & 0 \\end{pmatrix} \\begin{pmatrix} 8(1)+0(1) & 8(-1)+0(0) \\\\ 0(1)+7(1) & 0(-1)+7(0) \\end{pmatrix} = \\begin{pmatrix} 1 & 1 \\\\ -1 & 0 \\end{pmatrix} \\begin{pmatrix} 8 & -8 \\\\ 7 & 0 \\end{pmatrix}\n$$\n$$\nM = \\begin{pmatrix} 1(8)+1(7) & 1(-8)+1(0) \\\\ -1(8)+0(7) & -1(-8)+0(0) \\end{pmatrix} = \\begin{pmatrix} 15 & -8 \\\\ -8 & 8 \\end{pmatrix}\n$$\nNext, we find the inverse of $M$:\n$$\n\\det(M) = (15)(8) - (-8)(-8) = 120 - 64 = 56\n$$\n$$\nM^{-1} = \\frac{1}{56} \\begin{pmatrix} 8 & 8 \\\\ 8 & 15 \\end{pmatrix}\n$$\nNow we compute the preconditioned operator $P = M^{-1} H$:\n$$\nP = \\frac{1}{56} \\begin{pmatrix} 8 & 8 \\\\ 8 & 15 \\end{pmatrix} \\begin{pmatrix} 15 & -8 \\\\ -8 & 18 \\end{pmatrix}\n$$\n$$\nP = \\frac{1}{56} \\begin{pmatrix} 8(15)+8(-8) & 8(-8)+8(18) \\\\ 8(15)+15(-8) & 8(-8)+15(18) \\end{pmatrix}\n$$\n$$\nP = \\frac{1}{56} \\begin{pmatrix} 120-64 & -64+144 \\\\ 120-120 & -64+270 \\end{pmatrix} = \\frac{1}{56} \\begin{pmatrix} 56 & 80 \\\\ 0 & 206 \\end{pmatrix}\n$$\n$$\nP = \\begin{pmatrix} \\frac{56}{56} & \\frac{80}{56} \\\\ 0 & \\frac{206}{56} \\end{pmatrix} = \\begin{pmatrix} 1 & \\frac{10}{7} \\\\ 0 & \\frac{103}{28} \\end{pmatrix}\n$$\nThe eigenvalues of an upper triangular matrix are its diagonal entries. Therefore, the eigenvalues of $P = M^{-1}H$ are:\n$$\n\\lambda_{P,1} = 1, \\quad \\lambda_{P,2} = \\frac{103}{28}\n$$\nThe problem requests the four eigenvalues as a single row matrix, listing the two for $H$ first, followed by the two for $M^{-1}H$.\n\nThe four eigenvalues are $\\frac{33 - \\sqrt{265}}{2}$, $\\frac{33 + \\sqrt{265}}{2}$, $1$, and $\\frac{103}{28}$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{33 - \\sqrt{265}}{2} & \\frac{33 + \\sqrt{265}}{2} & 1 & \\frac{103}{28} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "The standard variational cost function assumes that both background and observation errors follow a Gaussian distribution, which implies a quadratic penalty for misfits. However, real-world observations are often contaminated by gross errors, or outliers, which can disproportionately corrupt the analysis under a quadratic penalty. This exercise introduces the concept of robust estimation by employing a Huber penalty, which combines the desirable properties of a quadratic penalty for small errors and a linear penalty for large errors, effectively reducing the influence of outliers. You will compute and compare the analysis produced with both penalty functions to directly observe the practical benefit of this robust approach. ",
            "id": "4063756",
            "problem": "In a one-dimensional variational data assimilation problem for numerical weather prediction, consider estimating a single gridpoint lower-tropospheric temperature state $x$ (in Kelvin) by minimizing a convex objective composed of a background term and an observation misfit term. The background (prior) state is $x_{b} = 290$ with background error standard deviation $\\sigma_{b} = 1$. There are $2$ independent observations with an identity observation operator $H(x) = x$: a near-consistent observation $y_{1} = 291$ with error standard deviation $\\sigma_{1} = 1$, and an outlier observation $y_{2} = 305$ with error standard deviation $\\sigma_{2} = 1$. Define the cost function\n$$\nJ(x) \\;=\\; \\frac{1}{2}\\,\\frac{(x - x_{b})^{2}}{\\sigma_{b}^{2}} \\;+\\; \\sum_{i=1}^{2} \\rho\\!\\left(\\frac{y_{i} - x}{\\sigma_{i}}\\right),\n$$\nwhere $\\rho$ is an observation misfit penalty applied to the standardized residuals $(y_{i} - x)/\\sigma_{i}$. Consider two choices for $\\rho$:\n- The purely quadratic penalty $\\rho_{\\mathrm{Q}}(r) = \\frac{1}{2}\\,r^{2}$.\n- The Huber penalty with threshold $\\kappa = 1.5$,\n$$\n\\rho_{\\mathrm{H}}(r) \\;=\\;\n\\begin{cases}\n\\frac{1}{2}\\,r^{2}, & \\text{if } |r| \\le \\kappa,\\\\[4pt]\n\\kappa\\,|r| - \\frac{1}{2}\\,\\kappa^{2}, & \\text{if } |r| > \\kappa.\n\\end{cases}\n$$\nUsing only these definitions and standard calculus for convex minimization, compute the unique minimizer $x_{\\mathrm{H}}$ of $J(x)$ when $\\rho = \\rho_{\\mathrm{H}}$ and the unique minimizer $x_{\\mathrm{Q}}$ when $\\rho = \\rho_{\\mathrm{Q}}$. Let the analysis increment be defined as the difference between the analysis and the background, $x - x_{b}$. Report the exact value of the difference between the Huber and quadratic analysis increments,\n$$\n\\bigl(x_{\\mathrm{H}} - x_{b}\\bigr) - \\bigl(x_{\\mathrm{Q}} - x_{b}\\bigr),\n$$\nin Kelvin. Express your final answer as an exact value in Kelvin (do not round).",
            "solution": "The problem requires computing the difference between the analysis increments obtained from a variational minimization problem using two different observation-misfit penalty functions: a standard quadratic penalty and a robust Huber penalty.\n\n**Part 1: Minimization with the Quadratic Penalty ($\\rho_{\\mathrm{Q}}$)**\n\nThe cost function using the quadratic penalty $\\rho_{\\mathrm{Q}}(r) = \\frac{1}{2}r^{2}$ is denoted by $J_{\\mathrm{Q}}(x)$.\n$$\nJ_{\\mathrm{Q}}(x) = \\frac{1}{2}\\frac{(x - x_{b})^{2}}{\\sigma_{b}^{2}} + \\sum_{i=1}^{2} \\frac{1}{2}\\left(\\frac{y_{i} - x}{\\sigma_{i}}\\right)^{2}\n$$\nSubstituting the given values $\\sigma_b = 1$, $\\sigma_1 = 1$, and $\\sigma_2 = 1$:\n$$\nJ_{\\mathrm{Q}}(x) = \\frac{1}{2}(x - x_{b})^{2} + \\frac{1}{2}(y_{1} - x)^{2} + \\frac{1}{2}(y_{2} - x)^{2}\n$$\nTo find the minimizer $x_{\\mathrm{Q}}$, we compute the derivative of $J_{\\mathrm{Q}}(x)$ with respect to $x$ and set it to zero.\n$$\n\\frac{dJ_{\\mathrm{Q}}}{dx} = (x - x_{b}) - (y_{1} - x) - (y_{2} - x) = 3x - x_{b} - y_{1} - y_{2}\n$$\nSetting the derivative to zero gives the optimal state $x_{\\mathrm{Q}}$:\n$$\n3x_{\\mathrm{Q}} - x_{b} - y_{1} - y_{2} = 0 \\implies x_{\\mathrm{Q}} = \\frac{x_{b} + y_{1} + y_{2}}{3}\n$$\nSubstituting the numerical values $x_{b} = 290$, $y_{1} = 291$, and $y_{2} = 305$:\n$$\nx_{\\mathrm{Q}} = \\frac{290 + 291 + 305}{3} = \\frac{886}{3}\n$$\nThe analysis increment for the quadratic case is:\n$$\nx_{\\mathrm{Q}} - x_{b} = \\frac{886}{3} - 290 = \\frac{886}{3} - \\frac{870}{3} = \\frac{16}{3}\n$$\n\n**Part 2: Minimization with the Huber Penalty ($\\rho_{\\mathrm{H}}$)**\n\nThe cost function using the Huber penalty is denoted by $J_{\\mathrm{H}}(x)$. With $\\sigma_i = 1$, the standardized residual is simply $r_i = y_i - x$.\n$$\nJ_{\\mathrm{H}}(x) = \\frac{1}{2}(x - x_b)^2 + \\rho_{\\mathrm{H}}(y_1 - x) + \\rho_{\\mathrm{H}}(y_2 - x)\n$$\nwhere\n$$\n\\rho_{\\mathrm{H}}(r) =\n\\begin{cases}\n\\frac{1}{2}r^2 & \\text{if } |r| \\le \\kappa \\\\\n\\kappa|r| - \\frac{1}{2}\\kappa^2 & \\text{if } |r| > \\kappa\n\\end{cases}\n$$\nTo find the minimizer $x_{\\mathrm{H}}$, we differentiate $J_{\\mathrm{H}}(x)$. The derivative of $\\rho_{\\mathrm{H}}(r)$ is the influence function $\\psi_{\\mathrm{H}}(r)$:\n$$\n\\psi_{\\mathrm{H}}(r) = \\frac{d\\rho_{\\mathrm{H}}}{dr} =\n\\begin{cases}\nr & \\text{if } |r| \\le \\kappa \\\\\n\\kappa\\,\\text{sgn}(r) & \\text{if } |r| > \\kappa\n\\end{cases}\n$$\nUsing the chain rule, $\\frac{d}{dx}\\rho_{\\mathrm{H}}(y_i - x) = \\psi_{\\mathrm{H}}(y_i - x) \\cdot (-1) = -\\psi_{\\mathrm{H}}(y_i - x)$. The derivative of the cost function is:\n$$\n\\frac{dJ_{\\mathrm{H}}}{dx} = (x - x_b) - \\psi_{\\mathrm{H}}(y_1 - x) - \\psi_{\\mathrm{H}}(y_2 - x)\n$$\nThe optimality condition at the minimum $x = x_{\\mathrm{H}}$ is:\n$$\nx_{\\mathrm{H}} - x_b - \\psi_{\\mathrm{H}}(y_1 - x_{\\mathrm{H}}) - \\psi_{\\mathrm{H}}(y_2 - x_{\\mathrm{H}}) = 0\n$$\nThis is a nonlinear equation for $x_{\\mathrm{H}}$. We must determine which regime of the Huber function applies to each observation. The data points are $x_b = 290$, $y_1 = 291$, and $y_2 = 305$. The observation $y_2$ is a significant outlier compared to the cluster formed by $x_b$ and $y_1$. The purpose of the Huber penalty is to reduce the influence of such outliers. Thus, it is reasonable to hypothesize that the residual for $y_1$ will be small (in the quadratic regime) and the residual for $y_2$ will be large (in the linear regime).\nHypothesis:\n1. $|y_1 - x_{\\mathrm{H}}| \\le \\kappa = 1.5$\n2. $|y_2 - x_{\\mathrm{H}}| > \\kappa = 1.5$\n\nUnder this hypothesis, $\\psi_{\\mathrm{H}}(y_1 - x_{\\mathrm{H}}) = y_1 - x_{\\mathrm{H}}$. For the second term, we expect $x_{\\mathrm{H}}$ to be close to $290-291$, so $y_2 - x_{\\mathrm{H}} = 305 - x_{\\mathrm{H}}$ will be positive. Thus, $\\text{sgn}(y_2 - x_{\\mathrm{H}}) = 1$ and $\\psi_{\\mathrm{H}}(y_2 - x_{\\mathrm{H}}) = \\kappa$.\n\nThe optimality condition becomes:\n$$\nx_{\\mathrm{H}} - x_b - (y_1 - x_{\\mathrm{H}}) - \\kappa = 0\n$$\nSolving for $x_{\\mathrm{H}}$:\n$$\nx_{\\mathrm{H}} - x_b - y_1 + x_{\\mathrm{H}} - \\kappa = 0 \\\\\n2x_{\\mathrm{H}} = x_b + y_1 + \\kappa \\\\\nx_{\\mathrm{H}} = \\frac{x_b + y_1 + \\kappa}{2}\n$$\nSubstituting numerical values $x_b = 290$, $y_1 = 291$, $\\kappa = 1.5$:\n$$\nx_{\\mathrm{H}} = \\frac{290 + 291 + 1.5}{2} = \\frac{582.5}{2} = 291.25\n$$\nNow, we must verify our hypothesis using this value of $x_{\\mathrm{H}}$:\n1. Check $|y_1 - x_{\\mathrm{H}}| \\le 1.5$: $|291 - 291.25| = |-0.25| = 0.25$. Since $0.25 \\le 1.5$, this part of the hypothesis is valid.\n2. Check $|y_2 - x_{\\mathrm{H}}| > 1.5$: $|305 - 291.25| = |13.75| = 13.75$. Since $13.75 > 1.5$, this part of the hypothesis is also valid.\n\nThe hypothesis is consistent, so $x_{\\mathrm{H}} = 291.25$ is the correct unique minimizer.\nThe analysis increment for the Huber case is:\n$$\nx_{\\mathrm{H}} - x_b = 291.25 - 290 = 1.25 = \\frac{5}{4}\n$$\n\n**Part 3: Final Calculation**\n\nThe required quantity is the difference between the Huber and quadratic analysis increments:\n$$\n(x_{\\mathrm{H}} - x_{b}) - (x_{\\mathrm{Q}} - x_{b}) = \\frac{5}{4} - \\frac{16}{3}\n$$\nTo subtract the fractions, we find a common denominator, which is $12$:\n$$\n\\frac{5 \\times 3}{4 \\times 3} - \\frac{16 \\times 4}{3 \\times 4} = \\frac{15}{12} - \\frac{64}{12} = \\frac{15 - 64}{12} = -\\frac{49}{12}\n$$\nThe difference is negative, which indicates that the robust Huber penalty correctly down-weights the influence of the outlier $y_2$, resulting in a smaller adjustment to the background state compared to the standard quadratic penalty.",
            "answer": "$$\\boxed{-\\frac{49}{12}}$$"
        }
    ]
}