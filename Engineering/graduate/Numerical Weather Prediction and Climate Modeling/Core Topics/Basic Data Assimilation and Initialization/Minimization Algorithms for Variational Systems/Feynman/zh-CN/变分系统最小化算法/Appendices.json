{
    "hands_on_practices": [
        {
            "introduction": "在基于梯度的优化算法中，确定了下降方向后，下一步的关键是决定沿着该方向走多远，即步长。步长太小会导致收敛缓慢，步长太大则可能越过最优点，甚至导致目标函数值增加。此练习  将通过一个具体的 3D-Var 问题，让您亲手实践 Armijo 回溯线搜索，这是一种确保每一步迭代都能获得“充分下降”的经典方法，是保证算法稳定收敛的重要组成部分。",
            "id": "4063601",
            "problem": "在用于数值天气预报 (NWP) 的三维变分同化 (3D-Var) 中，分析状态向量 $\\mathbf{x} \\in \\mathbb{R}^{3}$ 最小化二次代价函数\n$$\nJ(\\mathbf{x}) = \\frac{1}{2}(\\mathbf{x} - \\mathbf{x}_b)^{\\top} \\mathbf{B}^{-1} (\\mathbf{x} - \\mathbf{x}_b) + \\frac{1}{2}(\\mathbf{H}\\mathbf{x} - \\mathbf{y})^{\\top} \\mathbf{R}^{-1} (\\mathbf{H}\\mathbf{x} - \\mathbf{y}),\n$$\n其中 $\\mathbf{B}$ 是背景误差协方差，$\\mathbf{R}$ 是观测误差协方差，$\\mathbf{H}$ 是线性观测算子。$J$ 关于 $\\mathbf{x}$ 的梯度由二次型和线性映射的基本微积分给出。\n\n考虑一个维度为三的特定同化实例，其中矩阵和向量为\n$$\n\\mathbf{H} = \\mathbf{I}_{3}, \\quad \\mathbf{B}^{-1} = \\mathrm{diag}(1,\\,2,\\,3), \\quad \\mathbf{R}^{-1} = \\mathrm{diag}(4,\\,1,\\,2),\n$$\n$$\n\\mathbf{x}_b = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\quad \\mathbf{y} = \\begin{pmatrix} 1 \\\\ -2 \\\\ 3 \\end{pmatrix}, \\quad \\mathbf{x}_k = \\begin{pmatrix} 1 \\\\ -1 \\\\ 2 \\end{pmatrix}.\n$$\n设搜索方向为最速下降方向，$\\mathbf{p}_k = -\\nabla J(\\mathbf{x}_k)$，并执行 Armijo 回溯线搜索，初始步长为 $\\alpha_0 = 1$，缩减因子为 $\\beta = \\frac{1}{2}$，充分下降参数为 $c = 0.2$。需要满足的 Armijo 充分下降条件是\n$$\nJ(\\mathbf{x}_k + \\alpha\\,\\mathbf{p}_k) \\leq J(\\mathbf{x}_k) + c\\,\\alpha\\,\\nabla J(\\mathbf{x}_k)^{\\top} \\mathbf{p}_k.\n$$\n\n从 $\\alpha = \\alpha_0$ 开始，通过设置 $\\alpha \\leftarrow \\beta \\alpha$ 进行回溯，直到满足上述不等式。计算满足 Armijo 条件的可接受步长 $\\alpha_k$。将您的最终答案表示为一个实数。无需四舍五入。",
            "solution": "首先验证问题以确保其在科学上是合理的、适定的和客观的。问题陈述描述了一个在三维变分数据同化 (3D-Var) 中使用二次代价函数的标准最小化问题。所有的矩阵、向量和算法参数都已明确给出。矩阵 $\\mathbf{B}^{-1}$ 和 $\\mathbf{R}^{-1}$ 是对角元为正的对角矩阵，确保了它们是有效的逆协方差矩阵（即，它们是正定的）。该优化任务涉及应用定义良好的 Armijo 回溯线搜索和最速下降方向。整个设置是完整的、一致的并且在科学上是合理的。该问题被认为是有效的。\n\n代价函数由下式给出\n$$\nJ(\\mathbf{x}) = \\frac{1}{2}(\\mathbf{x} - \\mathbf{x}_b)^{\\top} \\mathbf{B}^{-1} (\\mathbf{x} - \\mathbf{x}_b) + \\frac{1}{2}(\\mathbf{H}\\mathbf{x} - \\mathbf{y})^{\\top} \\mathbf{R}^{-1} (\\mathbf{H}\\mathbf{x} - \\mathbf{y})\n$$\n代价函数关于 $\\mathbf{x}$ 的梯度是\n$$\n\\nabla J(\\mathbf{x}) = \\mathbf{B}^{-1}(\\mathbf{x} - \\mathbf{x}_b) + \\mathbf{H}^{\\top}\\mathbf{R}^{-1}(\\mathbf{H}\\mathbf{x} - \\mathbf{y})\n$$\n我们已知以下信息：\n$$\n\\mathbf{H} = \\mathbf{I}_{3}, \\quad \\mathbf{B}^{-1} = \\begin{pmatrix} 1  0  0 \\\\ 0  2  0 \\\\ 0  0  3 \\end{pmatrix}, \\quad \\mathbf{R}^{-1} = \\begin{pmatrix} 4  0  0 \\\\ 0  1  0 \\\\ 0  0  2 \\end{pmatrix}\n$$\n$$\n\\mathbf{x}_b = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\quad \\mathbf{y} = \\begin{pmatrix} 1 \\\\ -2 \\\\ 3 \\end{pmatrix}, \\quad \\mathbf{x}_k = \\begin{pmatrix} 1 \\\\ -1 \\\\ 2 \\end{pmatrix}\n$$\n将 $\\mathbf{x}_b = \\mathbf{0}$ 和 $\\mathbf{H} = \\mathbf{I}_{3}$ 代入梯度表达式，可将其简化为：\n$$\n\\nabla J(\\mathbf{x}) = \\mathbf{B}^{-1}\\mathbf{x} + \\mathbf{R}^{-1}(\\mathbf{x} - \\mathbf{y}) = (\\mathbf{B}^{-1} + \\mathbf{R}^{-1})\\mathbf{x} - \\mathbf{R}^{-1}\\mathbf{y}\n$$\n首先，我们计算所需的矩阵：\n$$\n\\mathbf{B}^{-1} + \\mathbf{R}^{-1} = \\begin{pmatrix} 1  0  0 \\\\ 0  2  0 \\\\ 0  0  3 \\end{pmatrix} + \\begin{pmatrix} 4  0  0 \\\\ 0  1  0 \\\\ 0  0  2 \\end{pmatrix} = \\begin{pmatrix} 5  0  0 \\\\ 0  3  0 \\\\ 0  0  5 \\end{pmatrix}\n$$\n$$\n\\mathbf{R}^{-1}\\mathbf{y} = \\begin{pmatrix} 4  0  0 \\\\ 0  1  0 \\\\ 0  0  2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -2 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} 4 \\\\ -2 \\\\ 6 \\end{pmatrix}\n$$\n因此，梯度为：\n$$\n\\nabla J(\\mathbf{x}) = \\begin{pmatrix} 5  0  0 \\\\ 0  3  0 \\\\ 0  0  5 \\end{pmatrix} \\mathbf{x} - \\begin{pmatrix} 4 \\\\ -2 \\\\ 6 \\end{pmatrix} = \\begin{pmatrix} 5x_1 - 4 \\\\ 3x_2 + 2 \\\\ 5x_3 - 6 \\end{pmatrix}\n$$\n现在，我们在当前迭代点 $\\mathbf{x}_k = \\begin{pmatrix} 1 \\\\ -1 \\\\ 2 \\end{pmatrix}$ 处计算梯度：\n$$\n\\nabla J(\\mathbf{x}_k) = \\begin{pmatrix} 5(1) - 4 \\\\ 3(-1) + 2 \\\\ 5(2) - 6 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -1 \\\\ 4 \\end{pmatrix}\n$$\n搜索方向为最速下降方向，$\\mathbf{p}_k = -\\nabla J(\\mathbf{x}_k)$：\n$$\n\\mathbf{p}_k = - \\begin{pmatrix} 1 \\\\ -1 \\\\ 4 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ 1 \\\\ -4 \\end{pmatrix}\n$$\nArmijo 充分下降条件是 $J(\\mathbf{x}_k + \\alpha\\,\\mathbf{p}_k) \\leq J(\\mathbf{x}_k) + c\\,\\alpha\\,\\nabla J(\\mathbf{x}_k)^{\\top} \\mathbf{p}_k$。\n我们需要计算右侧的各项。首先是 $J(\\mathbf{x}_k)$：\n$$\n\\mathbf{x}_k - \\mathbf{x}_b = \\begin{pmatrix} 1 \\\\ -1 \\\\ 2 \\end{pmatrix}, \\quad \\mathbf{H}\\mathbf{x}_k - \\mathbf{y} = \\mathbf{I}_3 \\mathbf{x}_k - \\mathbf{y} = \\begin{pmatrix} 1 \\\\ -1 \\\\ 2 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ -2 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\\\ -1 \\end{pmatrix}\n$$\n$$\nJ(\\mathbf{x}_k) = \\frac{1}{2} \\begin{pmatrix} 1  -1  2 \\end{pmatrix} \\begin{pmatrix} 1  0  0 \\\\ 0  2  0 \\\\ 0  0  3 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\\\ 2 \\end{pmatrix} + \\frac{1}{2} \\begin{pmatrix} 0  1  -1 \\end{pmatrix} \\begin{pmatrix} 4  0  0 \\\\ 0  1  0 \\\\ 0  0  2 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\\\ -1 \\end{pmatrix}\n$$\n$$\nJ(\\mathbf{x}_k) = \\frac{1}{2} (1(1)^2 + 2(-1)^2 + 3(2)^2) + \\frac{1}{2} (4(0)^2 + 1(1)^2 + 2(-1)^2)\n$$\n$$\nJ(\\mathbf{x}_k) = \\frac{1}{2} (1 + 2 + 12) + \\frac{1}{2} (0 + 1 + 2) = \\frac{15}{2} + \\frac{3}{2} = \\frac{18}{2} = 9\n$$\n接下来，我们计算方向导数项 $\\nabla J(\\mathbf{x}_k)^{\\top} \\mathbf{p}_k$：\n$$\n\\nabla J(\\mathbf{x}_k)^{\\top} \\mathbf{p}_k = \\begin{pmatrix} 1  -1  4 \\end{pmatrix} \\begin{pmatrix} -1 \\\\ 1 \\\\ -4 \\end{pmatrix} = (1)(-1) + (-1)(1) + (4)(-4) = -1 - 1 - 16 = -18\n$$\n当 $c = 0.2$ 时，Armijo 条件变为：\n$$\nJ(\\mathbf{x}_k + \\alpha\\,\\mathbf{p}_k) \\leq 9 + (0.2)\\alpha(-18) \\implies J(\\mathbf{x}_k + \\alpha\\,\\mathbf{p}_k) \\leq 9 - 3.6\\alpha\n$$\n为了计算左侧，我们定义新点 $\\mathbf{x}_{new}(\\alpha) = \\mathbf{x}_k + \\alpha\\,\\mathbf{p}_k$：\n$$\n\\mathbf{x}_{new}(\\alpha) = \\begin{pmatrix} 1 \\\\ -1 \\\\ 2 \\end{pmatrix} + \\alpha \\begin{pmatrix} -1 \\\\ 1 \\\\ -4 \\end{pmatrix} = \\begin{pmatrix} 1-\\alpha \\\\ \\alpha-1 \\\\ 2-4\\alpha \\end{pmatrix}\n$$\n让我们定义一个函数 $\\phi(\\alpha) = J(\\mathbf{x}_{new}(\\alpha))$。由于 $J$ 是 $\\mathbf{x}$ 的二次函数，而 $\\mathbf{x}_{new}$ 是 $\\alpha$ 的线性函数，因此 $\\phi(\\alpha)$ 必定是 $\\alpha$ 的二次函数。我们可以写出 $\\phi(\\alpha) = A\\alpha^2 + B\\alpha + C$。我们知道 $\\phi(0) = J(\\mathbf{x}_k) = 9$，所以 $C=9$。我们也知道 $\\phi'(0) = \\nabla J(\\mathbf{x}_k)^{\\top}\\mathbf{p}_k = -18$，所以 $B=-18$。因此 $\\phi(\\alpha)$ 的表达式是 $\\phi(\\alpha) = A\\alpha^2 - 18\\alpha + 9$。$J$ 的黑塞矩阵是 $\\nabla^2 J(\\mathbf{x}) = \\mathbf{B}^{-1} + \\mathbf{H}^{\\top}\\mathbf{R}^{-1}\\mathbf{H} = \\mathbf{B}^{-1} + \\mathbf{R}^{-1}$。系数 $A$ 由 $A = \\frac{1}{2}\\mathbf{p}_k^{\\top}(\\mathbf{B}^{-1}+\\mathbf{R}^{-1})\\mathbf{p}_k$ 给出。\n$$\nA = \\frac{1}{2}\\begin{pmatrix} -1  1  -4 \\end{pmatrix} \\begin{pmatrix} 5  0  0 \\\\ 0  3  0 \\\\ 0  0  5 \\end{pmatrix} \\begin{pmatrix} -1 \\\\ 1 \\\\ -4 \\end{pmatrix} = \\frac{1}{2}\\begin{pmatrix} -5  3  -20 \\end{pmatrix} \\begin{pmatrix} -1 \\\\ 1 \\\\ -4 \\end{pmatrix}\n$$\n$$\nA = \\frac{1}{2}((-5)(-1) + (3)(1) + (-20)(-4)) = \\frac{1}{2}(5 + 3 + 80) = \\frac{88}{2} = 44\n$$\n所以，$J(\\mathbf{x}_k + \\alpha\\,\\mathbf{p}_k) = 44\\alpha^2 - 18\\alpha + 9$。\nArmijo 不等式为：\n$$\n44\\alpha^2 - 18\\alpha + 9 \\leq 9 - 3.6\\alpha\n$$\n$$\n44\\alpha^2 - 18\\alpha \\leq -3.6\\alpha\n$$\n$$\n44\\alpha^2 - 14.4\\alpha \\leq 0\n$$\n由于我们寻找 $\\alpha > 0$，我们可以用 $\\alpha$ 除以两边：\n$$\n44\\alpha - 14.4 \\leq 0 \\implies 44\\alpha \\leq 14.4 \\implies \\alpha \\leq \\frac{14.4}{44} = \\frac{144}{440} = \\frac{72}{220} = \\frac{36}{110} = \\frac{18}{55}\n$$\n回溯线搜索从 $\\alpha = \\alpha_0 = 1$ 开始，并以 $\\beta = \\frac{1}{2}$ 的因子进行缩减，直到满足条件 $\\alpha \\leq \\frac{18}{55}$。\n1.  尝试 $\\alpha = 1$：$1 \\leq \\frac{18}{55}$ 是否成立？不成立，因为 $1 = \\frac{55}{55}$。条件不满足。回溯。\n2.  尝试 $\\alpha = \\frac{1}{2}$：$\\frac{1}{2} \\leq \\frac{18}{55}$ 是否成立？这等价于 $55 \\leq 36$，这是错误的。条件不满足。回溯。\n3.  尝试 $\\alpha = \\frac{1}{4}$：$\\frac{1}{4} \\leq \\frac{18}{55}$ 是否成立？这等价于 $55 \\leq 72$，这是正确的。条件满足。\n\n可接受的步长 $\\alpha_k$ 是序列 $1, \\frac{1}{2}, \\frac{1}{4}, \\dots$ 中第一个满足条件的值。这个值是 $\\frac{1}{4}$。",
            "answer": "$$\\boxed{\\frac{1}{4}}$$"
        },
        {
            "introduction": "虽然最速下降法方向简单直观，但更复杂的算法如拟牛顿法可以通过利用曲率信息来构造更好的搜索方向，从而显著加快收敛速度。在处理大规模变分同化问题时，有限内存的 BFGS (L-BFGS) 算法因其无需存储和计算完整的海森矩阵的逆而备受青睐。此练习  旨在揭示 L-BFGS 算法的核心机制——双循环递归，您将学习如何利用历史梯度信息来高效地计算出一个高质量的搜索方向。",
            "id": "4063702",
            "problem": "在用于数值天气预报的强约束变分资料同化中，需要最小化一个二阶可微的代价函数 $J(x)$，其梯度 $\\nabla J(x)$ 是可用的，但其Hessian矩阵 $\\nabla^{2}J(x)$ 不会显式地构建。一种常见的策略是使用拟牛顿法，该方法利用一阶信息构建Hessian逆矩阵的近似，并生成一个下降方向。考虑将内存受限的Broyden–Fletcher–Goldfarb–Shanno (L-BFGS) 方法应用于一个三维控制向量 $x \\in \\mathbb{R}^{3}$，该向量代表了三个聚合状态分量的偏差订正后的分析增量。在当前迭代点 $x_{k}$，梯度为\n$$\ng_{k} \\equiv \\nabla J(x_{k}) = \\begin{pmatrix} 3 \\\\ -1 \\\\ 2 \\end{pmatrix}.\n$$\n从最近的两次迭代中，我们得到曲率（割线）对\n$$\ns_{1} = \\begin{pmatrix} 1 \\\\ 0 \\\\ -1 \\end{pmatrix}, \\quad y_{1} = \\begin{pmatrix} 2 \\\\ 0 \\\\ -1 \\end{pmatrix}, \\qquad\ns_{2} = \\begin{pmatrix} 0 \\\\ 2 \\\\ 1 \\end{pmatrix}, \\quad y_{2} = \\begin{pmatrix} 0 \\\\ 3 \\\\ 2 \\end{pmatrix},\n$$\n按从旧 $(i=1)$ 到新 $(i=2)$ 的顺序排列，并满足曲率条件 $y_{i}^{\\top} s_{i} > 0$。使用L-BFGS的标准双循环递归，初始Hessian逆矩阵的缩放被选为单位矩阵的标量倍数，$H_{0} = \\gamma I$，其中 $\\gamma$ 由最新的曲率对设置为\n$$\n\\gamma = \\frac{s_{2}^{\\top} y_{2}}{y_{2}^{\\top} y_{2}},\n$$\n计算L-BFGS搜索方向\n$$\np_{k} = - H_{k} g_{k},\n$$\n其中 $H_{k}$ 是L-BFGS的Hessian逆矩阵近似，它由应用于 $g_{k}$ 和给定的 $(s_{i}, y_{i})$ 的双循环递归隐式定义。\n\n以行向量形式给出 $p_{k}$ 的精确有理数向量。不要四舍五入。",
            "solution": "用户要求根据给定的梯度和更新历史，计算内存受限的Broyden–Fletcher–Goldfarb–Shanno (L-BFGS) 搜索方向 $p_k$。该计算使用标准的双循环递归算法执行。\n\n问题给出以下已知条件：\n控制向量属于 $\\mathbb{R}^{3}$。\n在当前迭代点 $x_k$ 的梯度是 $g_k = \\nabla J(x_k) = \\begin{pmatrix} 3 \\\\ -1 \\\\ 2 \\end{pmatrix}$。\n最近的两个更新对（内存 $m=2$），按从旧到新的顺序排列，为：\n$s_{1} = \\begin{pmatrix} 1 \\\\ 0 \\\\ -1 \\end{pmatrix}$, $y_{1} = \\begin{pmatrix} 2 \\\\ 0 \\\\ -1 \\end{pmatrix}$\n$s_{2} = \\begin{pmatrix} 0 \\\\ 2 \\\\ 1 \\end{pmatrix}$, $y_{2} = \\begin{pmatrix} 0 \\\\ 3 \\\\ 2 \\end{pmatrix}$\n\n搜索方向由 $p_k = -H_k g_k$ 给出，其中 $H_k$ 是Hessian逆矩阵的L-BFGS近似。乘积 $H_k g_k$ 使用双循环递归隐式计算。\n\n首先，我们计算标量值 $\\rho_i = \\frac{1}{y_i^\\top s_i}$，其中 $i=1, 2$。\n对于 $i=1$:\n$$y_1^\\top s_1 = \\begin{pmatrix} 2  0  -1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ -1 \\end{pmatrix} = (2)(1) + (0)(0) + (-1)(-1) = 3$$\n$$\\rho_1 = \\frac{1}{3}$$\n对于 $i=2$:\n$$y_2^\\top s_2 = \\begin{pmatrix} 0  3  2 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 2 \\\\ 1 \\end{pmatrix} = (0)(0) + (3)(2) + (2)(1) = 8$$\n$$\\rho_2 = \\frac{1}{8}$$\n曲率条件 $y_i^\\top s_i > 0$ 得到满足。\n\n接下来，我们计算初始Hessian逆矩阵近似 $H_0 = \\gamma I$ 的缩放因子 $\\gamma$。其公式为 $\\gamma = \\frac{s_2^\\top y_2}{y_2^\\top y_2}$。\n我们有 $s_2^\\top y_2 = y_2^\\top s_2 = 8$。\n分母是：\n$$y_2^\\top y_2 = \\begin{pmatrix} 0  3  2 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 3 \\\\ 2 \\end{pmatrix} = (0)^{2} + (3)^{2} + (2)^{2} = 9 + 4 = 13$$\n因此，缩放因子为：\n$$\\gamma = \\frac{8}{13}$$\n\n现在，我们应用L-BFGS双循环递归。\n\n**循环1（后向传递）：**\n这个循环计算向量 $q$ 和标量 $\\alpha_i$，$i=m, \\dots, 1$。这里 $m=2$。\n初始化 $q = g_k = \\begin{pmatrix} 3 \\\\ -1 \\\\ 2 \\end{pmatrix}$。\n\n对于 $i=2$:\n$$\\alpha_2 = \\rho_2 s_2^\\top q = \\frac{1}{8} \\begin{pmatrix} 0  2  1 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ -1 \\\\ 2 \\end{pmatrix} = \\frac{1}{8} (0 - 2 + 2) = 0$$\n$$q \\leftarrow q - \\alpha_2 y_2 = \\begin{pmatrix} 3 \\\\ -1 \\\\ 2 \\end{pmatrix} - 0 \\cdot \\begin{pmatrix} 0 \\\\ 3 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ -1 \\\\ 2 \\end{pmatrix}$$\n\n对于 $i=1$:\n$$\\alpha_1 = \\rho_1 s_1^\\top q = \\frac{1}{3} \\begin{pmatrix} 1  0  -1 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ -1 \\\\ 2 \\end{pmatrix} = \\frac{1}{3} (3 - 2) = \\frac{1}{3}$$\n$$q \\leftarrow q - \\alpha_1 y_1 = \\begin{pmatrix} 3 \\\\ -1 \\\\ 2 \\end{pmatrix} - \\frac{1}{3} \\begin{pmatrix} 2 \\\\ 0 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 3 - \\frac{2}{3} \\\\ -1 \\\\ 2 + \\frac{1}{3} \\end{pmatrix} = \\begin{pmatrix} \\frac{7}{3} \\\\ -1 \\\\ \\frac{7}{3} \\end{pmatrix}$$\n\n**初始近似缩放：**\n第一个循环的结果 $q$ 现在通过初始Hessian逆矩阵近似 $H_0 = \\gamma I$ 进行缩放。设结果向量为 $r$。\n$$r = H_0 q = \\gamma q = \\frac{8}{13} \\begin{pmatrix} \\frac{7}{3} \\\\ -1 \\\\ \\frac{7}{3} \\end{pmatrix} = \\begin{pmatrix} \\frac{56}{39} \\\\ -\\frac{8}{13} \\\\ \\frac{56}{39} \\end{pmatrix}$$\n\n**循环2（前向传递）：**\n这个循环从上一步的向量 $r$ 开始，并对 $i=1, \\dots, m$ 进行更新。\n\n对于 $i=1$:\n我们计算一个标量 $\\beta_1$ 并更新 $r$。\n$$\\beta_1 = \\rho_1 y_1^\\top r = \\frac{1}{3} \\begin{pmatrix} 2  0  -1 \\end{pmatrix} \\begin{pmatrix} \\frac{56}{39} \\\\ -\\frac{8}{13} \\\\ \\frac{56}{39} \\end{pmatrix} = \\frac{1}{3} \\left( 2 \\cdot \\frac{56}{39} - \\frac{56}{39} \\right) = \\frac{1}{3} \\cdot \\frac{56}{39} = \\frac{56}{117}$$\n现在使用 $r \\leftarrow r + s_1(\\alpha_1 - \\beta_1)$ 更新 $r$。我们已知 $\\alpha_1 = \\frac{1}{3}$。\n$$\\alpha_1 - \\beta_1 = \\frac{1}{3} - \\frac{56}{117} = \\frac{39}{117} - \\frac{56}{117} = -\\frac{17}{117}$$\n$$r \\leftarrow \\begin{pmatrix} \\frac{56}{39} \\\\ -\\frac{8}{13} \\\\ \\frac{56}{39} \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ 0 \\\\ -1 \\end{pmatrix} \\left( -\\frac{17}{117} \\right) = \\begin{pmatrix} \\frac{56}{39} - \\frac{17}{117} \\\\ -\\frac{8}{13} \\\\ \\frac{56}{39} + \\frac{17}{117} \\end{pmatrix} = \\begin{pmatrix} \\frac{168 - 17}{117} \\\\ -\\frac{72}{117} \\\\ \\frac{168 + 17}{117} \\end{pmatrix} = \\begin{pmatrix} \\frac{151}{117} \\\\ -\\frac{72}{117} \\\\ \\frac{185}{117} \\end{pmatrix}$$\n\n对于 $i=2$:\n我们计算 $\\beta_2$ 并再次更新 $r$。我们已知 $\\alpha_2=0$。\n$$\\beta_2 = \\rho_2 y_2^\\top r = \\frac{1}{8} \\begin{pmatrix} 0  3  2 \\end{pmatrix} \\begin{pmatrix} \\frac{151}{117} \\\\ -\\frac{72}{117} \\\\ \\frac{185}{117} \\end{pmatrix} = \\frac{1}{8 \\cdot 117} \\left( 3(-72) + 2(185) \\right) = \\frac{-216 + 370}{936} = \\frac{154}{936}$$\n化简分数得到 $\\beta_2 = \\frac{77}{468}$。\n现在使用 $r \\leftarrow r + s_2(\\alpha_2 - \\beta_2)$ 更新 $r$。\n$$\\alpha_2 - \\beta_2 = 0 - \\frac{77}{468} = -\\frac{77}{468}$$\n$$r \\leftarrow \\begin{pmatrix} \\frac{151}{117} \\\\ -\\frac{72}{117} \\\\ \\frac{185}{117} \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 2 \\\\ 1 \\end{pmatrix} \\left( -\\frac{77}{468} \\right) = \\begin{pmatrix} \\frac{151}{117} \\\\ -\\frac{72}{117} - \\frac{154}{468} \\\\ \\frac{185}{117} - \\frac{77}{468} \\end{pmatrix}$$\n我们使用公分母 $468 = 4 \\cdot 117$。\n$$r_1 = \\frac{4 \\cdot 151}{468} = \\frac{604}{468} = \\frac{151}{117}$$\n$$r_2 = -\\frac{4 \\cdot 72}{468} - \\frac{154}{468} = \\frac{-288 - 154}{468} = \\frac{-442}{468} = -\\frac{221}{234} = -\\frac{17 \\cdot 13}{18 \\cdot 13} = -\\frac{17}{18}$$\n$$r_3 = \\frac{4 \\cdot 185}{468} - \\frac{77}{468} = \\frac{740 - 77}{468} = \\frac{663}{468} = \\frac{221}{156} = \\frac{17 \\cdot 13}{12 \\cdot 13} = \\frac{17}{12}$$\n因此，递归结束时的最终向量 $r$ 是 $r = \\begin{pmatrix} \\frac{151}{117} \\\\ -\\frac{17}{18} \\\\ \\frac{17}{12} \\end{pmatrix}$。\n\nL-BFGS搜索方向是 $p_k = -r$。\n$$p_k = - \\begin{pmatrix} \\frac{151}{117} \\\\ -\\frac{17}{18} \\\\ \\frac{17}{12} \\end{pmatrix} = \\begin{pmatrix} -\\frac{151}{117} \\\\ \\frac{17}{18} \\\\ -\\frac{17}{12} \\end{pmatrix}$$\n问题要求以行向量形式给出答案。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n-\\frac{151}{117} & \\frac{17}{18} & -\\frac{17}{12}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "变分系统设计的优劣不仅取决于优化算法的效率，也取决于代价函数本身的构造。真实世界的观测数据不可避免地会包含一些与背景场或其余观测严重不符的“野点”（outliers），如果使用传统的二次惩罚项，这些野点会对分析结果产生不成比例的巨大影响。此练习  通过一个简化的理想实验，对比了二次惩罚和鲁棒的 Huber 惩罚，旨在清晰地展示后者如何通过降低野点权重来获得更可靠、更稳健的分析结果。",
            "id": "4063756",
            "problem": "在一个用于数值天气预报的一维变分资料同化问题中，考虑通过最小化一个由背景项和观测不符项组成的凸目标函数来估计单个网格点对流层低层的温度状态 $x$（单位：开尔文）。背景（先验）状态为 $x_{b} = 290$，背景误差标准差为 $\\sigma_{b} = 1$。存在 $2$ 个独立的观测，其观测算子为单位算子 $H(x) = x$：一个近乎一致的观测 $y_{1} = 291$，其误差标准差为 $\\sigma_{1} = 1$；以及一个离群观测 $y_{2} = 305$，其误差标准差为 $\\sigma_{2} = 1$。定义成本函数\n$$\nJ(x) \\;=\\; \\frac{1}{2}\\,\\frac{(x - x_{b})^{2}}{\\sigma_{b}^{2}} \\;+\\; \\sum_{i=1}^{2} \\rho\\!\\left(\\frac{y_{i} - x}{\\sigma_{i}}\\right),\n$$\n其中 $\\rho$ 是应用于标准化残差 $(y_{i} - x)/\\sigma_{i}$ 的观测不符惩罚。考虑 $\\rho$ 的两种选择：\n- 纯二次惩罚 $\\rho_{\\mathrm{Q}}(r) = \\frac{1}{2}\\,r^{2}$。\n- Huber 惩罚，阈值为 $\\kappa = 1.5$，\n$$\n\\rho_{\\mathrm{H}}(r) \\;=\\;\n\\begin{cases}\n\\frac{1}{2}\\,r^{2}, & \\text{若 } |r| \\le \\kappa, \\\\[4pt]\n\\kappa\\,|r| - \\frac{1}{2}\\,\\kappa^{2}, & \\text{若 } |r| > \\kappa.\n\\end{cases}\n$$\n仅使用这些定义和凸最小化的标准微积分方法，计算当 $\\rho = \\rho_{\\mathrm{H}}$ 时 $J(x)$ 的唯一极小值点 $x_{\\mathrm{H}}$ 以及当 $\\rho = \\rho_{\\mathrm{Q}}$ 时 $J(x)$ 的唯一极小值点 $x_{\\mathrm{Q}}$。定义分析增量为分析与背景之差，即 $x - x_{b}$。报告 Huber 分析增量与二次分析增量之差的精确值，\n$$\n\\bigl(x_{\\mathrm{H}} - x_{b}\\bigr) - \\bigl(x_{\\mathrm{Q}} - x_{b}\\bigr),\n$$\n单位为开尔文。最终答案请表示为开尔文的精确值（不要四舍五入）。",
            "solution": "用户希望计算一个变分最小化问题在使用两种不同的观测不符惩罚函数（标准二次惩罚和稳健的 Huber 惩罚）时所得到的分析增量之差。\n\n首先，验证问题。\n问题陈述提供了所有必要信息：状态变量 $x$、背景状态 $x_b = 290$、背景误差标准差 $\\sigma_b = 1$、两个观测 $y_1 = 291$ 和 $y_2 = 305$、它们对应的误差标准差 $\\sigma_1 = 1$ 和 $\\sigma_2 = 1$，以及总成本函数 $J(x)$ 和两种惩罚函数 $\\rho_{\\mathrm{Q}}(r)$、$\\rho_{\\mathrm{H}}(r)$ 的明确数学形式，包括 Huber 阈值 $\\kappa = 1.5$。该问题是一维变分资料同化中的一个标准、良态练习。两个成本函数都是一个严格凸函数与多个凸函数之和，因此它们是严格凸的，并拥有唯一的极小值点。该问题具有科学依据，数学上一致且客观。因此，该问题被认为是有效的，可以推导出解决方案。\n\n需要计算的量是 $(x_{\\mathrm{H}} - x_{b}) - (x_{\\mathrm{Q}} - x_{b})$，其中 $x_{\\mathrm{Q}}$ 和 $x_{\\mathrm{H}}$ 分别是使用二次惩罚和 Huber 惩罚时成本函数 $J(x)$ 的唯一极小值点。\n\n**第一部分：使用二次惩罚 ($\\rho_{\\mathrm{Q}}$) 的最小化**\n\n使用二次惩罚 $\\rho_{\\mathrm{Q}}(r) = \\frac{1}{2}r^{2}$ 的成本函数记为 $J_{\\mathrm{Q}}(x)$。\n$$\nJ_{\\mathrm{Q}}(x) = \\frac{1}{2}\\frac{(x - x_{b})^{2}}{\\sigma_{b}^{2}} + \\sum_{i=1}^{2} \\frac{1}{2}\\left(\\frac{y_{i} - x}{\\sigma_{i}}\\right)^{2}\n$$\n代入给定值 $\\sigma_b = 1$、$\\sigma_1 = 1$ 和 $\\sigma_2 = 1$：\n$$\nJ_{\\mathrm{Q}}(x) = \\frac{1}{2}(x - x_{b})^{2} + \\frac{1}{2}(y_{1} - x)^{2} + \\frac{1}{2}(y_{2} - x)^{2}\n$$\n为了找到极小值点 $x_{\\mathrm{Q}}$，我们计算 $J_{\\mathrm{Q}}(x)$ 关于 $x$ 的导数并将其设为零。\n$$\n\\frac{dJ_{\\mathrm{Q}}}{dx} = (x - x_{b}) - (y_{1} - x) - (y_{2} - x) = 3x - x_{b} - y_{1} - y_{2}\n$$\n将导数设为零，得到最优状态 $x_{\\mathrm{Q}}$：\n$$\n3x_{\\mathrm{Q}} - x_{b} - y_{1} - y_{2} = 0 \\implies x_{\\mathrm{Q}} = \\frac{x_{b} + y_{1} + y_{2}}{3}\n$$\n代入数值 $x_{b} = 290$、$y_{1} = 291$ 和 $y_{2} = 305$：\n$$\nx_{\\mathrm{Q}} = \\frac{290 + 291 + 305}{3} = \\frac{886}{3}\n$$\n二次惩罚情况下的分析增量为：\n$$\nx_{\\mathrm{Q}} - x_{b} = \\frac{886}{3} - 290 = \\frac{886}{3} - \\frac{870}{3} = \\frac{16}{3}\n$$\n\n**第二部分：使用 Huber 惩罚 ($\\rho_{\\mathrm{H}}$) 的最小化**\n\n使用 Huber 惩罚的成本函数记为 $J_{\\mathrm{H}}(x)$。由于 $\\sigma_i = 1$，标准化残差就是 $r_i = y_i - x$。\n$$\nJ_{\\mathrm{H}}(x) = \\frac{1}{2}(x - x_b)^2 + \\rho_{\\mathrm{H}}(y_1 - x) + \\rho_{\\mathrm{H}}(y_2 - x)\n$$\n其中\n$$\n\\rho_{\\mathrm{H}}(r) =\n\\begin{cases}\n\\frac{1}{2}r^2 & \\text{若 } |r| \\le \\kappa \\\\\n\\kappa|r| - \\frac{1}{2}\\kappa^2 & \\text{若 } |r| > \\kappa\n\\end{cases}\n$$\n为了找到极小值点 $x_{\\mathrm{H}}$，我们对 $J_{\\mathrm{H}}(x)$ 求导。$\\rho_{\\mathrm{H}}(r)$ 的导数是影响函数 $\\psi_{\\mathrm{H}}(r)$：\n$$\n\\psi_{\\mathrm{H}}(r) = \\frac{d\\rho_{\\mathrm{H}}}{dr} =\n\\begin{cases}\nr & \\text{若 } |r| \\le \\kappa \\\\\n\\kappa\\,\\text{sgn}(r) & \\text{若 } |r| > \\kappa\n\\end{cases}\n$$\n使用链式法则，$\\frac{d}{dx}\\rho_{\\mathrm{H}}(y_i - x) = \\psi_{\\mathrm{H}}(y_i - x) \\cdot (-1) = -\\psi_{\\mathrm{H}}(y_i - x)$。成本函数的导数为：\n$$\n\\frac{dJ_{\\mathrm{H}}}{dx} = (x - x_b) - \\psi_{\\mathrm{H}}(y_1 - x) - \\psi_{\\mathrm{H}}(y_2 - x)\n$$\n在极小值点 $x = x_{\\mathrm{H}}$ 处的最优性条件是：\n$$\nx_{\\mathrm{H}} - x_b - \\psi_{\\mathrm{H}}(y_1 - x_{\\mathrm{H}}) - \\psi_{\\mathrm{H}}(y_2 - x_{\\mathrm{H}}) = 0\n$$\n这是一个关于 $x_{\\mathrm{H}}$ 的非线性方程。我们必须确定 Huber 函数的哪个区间适用于每个观测。数据点为 $x_b = 290$、$y_1 = 291$ 和 $y_2 = 305$。与由 $x_b$ 和 $y_1$ 形成的簇相比，观测 $y_2$ 是一个显著的离群值。Huber 惩罚的目的是减少这类离群值的影响。因此，可以合理地假设，$y_1$ 的残差会很小（在二次区间内），而 $y_2$ 的残差会很大（在线性区间内）。\n假设：\n1. $|y_1 - x_{\\mathrm{H}}| \\le \\kappa = 1.5$\n2. $|y_2 - x_{\\mathrm{H}}| > \\kappa = 1.5$\n\n在此假设下，$\\psi_{\\mathrm{H}}(y_1 - x_{\\mathrm{H}}) = y_1 - x_{\\mathrm{H}}$。对于第二项，我们预期 $x_{\\mathrm{H}}$ 接近 $290-291$，所以 $y_2 - x_{\\mathrm{H}} = 305 - x_{\\mathrm{H}}$ 将为正。因此，$\\text{sgn}(y_2 - x_{\\mathrm{H}}) = 1$ 并且 $\\psi_{\\mathrm{H}}(y_2 - x_{\\mathrm{H}}) = \\kappa$。\n\n最优性条件变为：\n$$\nx_{\\mathrm{H}} - x_b - (y_1 - x_{\\mathrm{H}}) - \\kappa = 0\n$$\n求解 $x_{\\mathrm{H}}$：\n$$\nx_{\\mathrm{H}} - x_b - y_1 + x_{\\mathrm{H}} - \\kappa = 0 \\\\\n2x_{\\mathrm{H}} = x_b + y_1 + \\kappa \\\\\nx_{\\mathrm{H}} = \\frac{x_b + y_1 + \\kappa}{2}\n$$\n代入数值 $x_b = 290$、$y_1 = 291$、$\\kappa = 1.5$：\n$$\nx_{\\mathrm{H}} = \\frac{290 + 291 + 1.5}{2} = \\frac{582.5}{2} = 291.25\n$$\n现在，我们必须用这个 $x_{\\mathrm{H}}$ 的值来验证我们的假设：\n1. 检查 $|y_1 - x_{\\mathrm{H}}| \\le 1.5$：$|291 - 291.25| = |-0.25| = 0.25$。由于 $0.25 \\le 1.5$，这部分假设是有效的。\n2. 检查 $|y_2 - x_{\\mathrm{H}}| > 1.5$：$|305 - 291.25| = |13.75| = 13.75$。由于 $13.75 > 1.5$，这部分假设也是有效的。\n\n该假设是一致的，因此 $x_{\\mathrm{H}} = 291.25$ 是正确的唯一极小值点。\nHuber 惩罚情况下的分析增量是：\n$$\nx_{\\mathrm{H}} - x_b = 291.25 - 290 = 1.25 = \\frac{5}{4}\n$$\n\n**第三部分：最终计算**\n\n所需的量是 Huber 分析增量与二次分析增量之差：\n$$\n(x_{\\mathrm{H}} - x_{b}) - (x_{\\mathrm{Q}} - x_{b}) = \\frac{5}{4} - \\frac{16}{3}\n$$\n为了进行分数相减，我们找到公分母，即 $12$：\n$$\n\\frac{5 \\times 3}{4 \\times 3} - \\frac{16 \\times 4}{3 \\times 4} = \\frac{15}{12} - \\frac{64}{12} = \\frac{15 - 64}{12} = -\\frac{49}{12}\n$$\n差值为负，这表明稳健的 Huber 惩罚正确地降低了离群值 $y_2$ 的影响权重，与标准二次惩罚相比，导致对背景状态的调整更小。",
            "answer": "$$\\boxed{-\\frac{49}{12}}$$"
        }
    ]
}