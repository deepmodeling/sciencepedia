{
    "hands_on_practices": [
        {
            "introduction": "The simplest form of statistical post-processing is correcting systematic biases using a linear model. This practice explores Model Output Statistics (MOS) by fitting a linear regression of the form $y \\approx a + b f$ between forecasts ($f$) and observations ($y$) using the method of least squares. Beyond just applying the correction, this exercise emphasizes the critical step of model diagnostics by analyzing the structure of residuals to detect unmodeled relationships, a fundamental skill in any statistical modeling task .",
            "id": "4076566",
            "problem": "A forecast-to-observation bias correction in Numerical Weather Prediction (NWP) post-processing is often modeled as a linear mapping of the form $y \\approx a + b f$, where $f$ denotes the raw model forecast and $y$ denotes the verifying observation. In Model Output Statistics (MOS), the parameters $a$ and $b$ are estimated by least squares. Starting from the principle of minimizing the sum of squared errors, derive the least squares estimators in terms of sample statistics and then evaluate them for the dataset below. After computing the linear fit, examine the residual structure to diagnose nonlinearity by quantifying the linear association between residuals $r_i$ and the squared centered forecast $s_i = (f_i - \\bar{f})^{2}$.\n\nUse the following dataset of $n = 10$ paired values $\\{(f_i, y_i)\\}_{i=1}^{10}$ for $2$-meter temperature, where $f_i$ are forecasts in Kelvin and $y_i$ are verifying observations in Kelvin:\n$\\{(f_i, y_i)\\}_{i=1}^{10} = \\{(275, 274), (280, 277.5), (285, 282), (290, 287.5), (295, 294), (275, 274), (280, 277.5), (285, 282), (290, 287.5), (295, 294)\\}$.\n\nYour tasks are:\n1. Starting from the least squares criterion and core definitions of sample mean, sample variance, and sample covariance, derive the estimators $\\hat{a}$ and $\\hat{b}$ in terms of sample statistics and compute their numerical values for the given dataset. Express $\\hat{a}$ in Kelvin and $\\hat{b}$ as dimensionless.\n2. Compute the Pearson correlation coefficient $D$ between the residuals $r_i = y_i - (\\hat{a} + \\hat{b} f_i)$ and the squared centered forecast $s_i = (f_i - \\bar{f})^{2}$. Interpret the magnitude and sign of $D$ in the context of diagnosing nonlinearity in the bias correction.\n\nIf a numerical answer requires rounding, round to four significant figures. Express $\\hat{a}$ in Kelvin, and $\\hat{b}$ and $D$ as dimensionless quantities. Provide the final numerical values for $\\hat{a}$, $\\hat{b}$, and $D$.",
            "solution": "The problem asks for the derivation and calculation of linear regression parameters for a given dataset of meteorological forecasts and observations, followed by a residual analysis to diagnose nonlinearity. The process will be conducted in two parts as requested.\n\n### Part 1: Derivation and Calculation of Least Squares Estimators\n\nThe linear model is $y \\approx a + b f$. The parameters $a$ and $b$ are estimated by minimizing the Sum of Squared Errors (SSE), $S(a, b)$.\n\n**1. Derivation of Estimators**\n\nThe SSE is defined as:\n$$S(a,b) = \\sum_{i=1}^{n} (y_i - (a + b f_i))^2$$\nTo find the values of $a$ and $b$ that minimize $S$, we take the partial derivatives of $S$ with respect to $a$ and $b$ and set them to zero. The resulting estimators are denoted $\\hat{a}$ and $\\hat{b}$.\n\nThe first partial derivative with respect to $a$ is:\n$$\\frac{\\partial S}{\\partial a} = \\sum_{i=1}^{n} 2(y_i - a - b f_i)(-1) = -2 \\sum_{i=1}^{n} (y_i - a - b f_i)$$\nSetting $\\frac{\\partial S}{\\partial a} = 0$:\n$$\\sum_{i=1}^{n} (y_i - \\hat{a} - \\hat{b} f_i) = 0$$\n$$\\sum y_i - n\\hat{a} - \\hat{b}\\sum f_i = 0$$\nDividing by the sample size $n$ and using the definitions of the sample means $\\bar{y} = \\frac{1}{n}\\sum y_i$ and $\\bar{f} = \\frac{1}{n}\\sum f_i$:\n$$\\bar{y} - \\hat{a} - \\hat{b}\\bar{f} = 0$$\nThis gives the estimator for the intercept, $\\hat{a}$, in terms of the estimator for the slope, $\\hat{b}$:\n$$\\hat{a} = \\bar{y} - \\hat{b}\\bar{f}$$\n\nThe second partial derivative with respect to $b$ is:\n$$\\frac{\\partial S}{\\partial b} = \\sum_{i=1}^{n} 2(y_i - a - b f_i)(-f_i) = -2 \\sum_{i=1}^{n} f_i(y_i - a - b f_i)$$\nSetting $\\frac{\\partial S}{\\partial b} = 0$:\n$$\\sum_{i=1}^{n} f_i(y_i - \\hat{a} - \\hat{b} f_i) = 0$$\nSubstitute the expression for $\\hat{a}$:\n$$\\sum_{i=1}^{n} f_i(y_i - (\\bar{y} - \\hat{b}\\bar{f}) - \\hat{b} f_i) = 0$$\n$$\\sum_{i=1}^{n} f_i((y_i - \\bar{y}) - \\hat{b}(f_i - \\bar{f})) = 0$$\n$$\\sum_{i=1}^{n} f_i(y_i - \\bar{y}) - \\hat{b}\\sum_{i=1}^{n} f_i(f_i - \\bar{f}) = 0$$\nSolving for $\\hat{b}$:\n$$\\hat{b} = \\frac{\\sum f_i(y_i - \\bar{y})}{\\sum f_i(f_i - \\bar{f})}$$\nThe numerator and denominator can be simplified to a more standard form expressed with centered variables:\n$\\sum (f_i - \\bar{f})(y_i - \\bar{y}) = \\sum (f_iy_i - f_i\\bar{y} - \\bar{f}y_i + \\bar{f}\\bar{y}) = \\sum f_iy_i - \\bar{y}\\sum f_i - \\bar{f}\\sum y_i + n\\bar{f}\\bar{y} = \\sum f_iy_i - n\\bar{f}\\bar{y} - n\\bar{f}\\bar{y} + n\\bar{f}\\bar{y} = \\sum f_iy_i - n\\bar{f}\\bar{y}$.\nAlso, $\\sum f_i(y_i - \\bar{y}) = \\sum f_iy_i - \\bar{y}\\sum f_i = \\sum f_iy_i - n\\bar{f}\\bar{y}$.\nSo, $\\sum_i (f_i - \\bar{f})(y_i - \\bar{y}) = \\sum_i f_i(y_i - \\bar{y})$. Similarly, $\\sum_i (f_i - \\bar{f})^2 = \\sum_i f_i(f_i - \\bar{f})$.\nThis gives the standard expression for $\\hat{b}$:\n$$\\hat{b} = \\frac{\\sum_{i=1}^{n}(f_i - \\bar{f})(y_i - \\bar{y})}{\\sum_{i=1}^{n}(f_i - \\bar{f})^2}$$\nThis expresses $\\hat{b}$ in terms of the sample covariance of $f$ and $y$ (proportional to the numerator) and the sample variance of $f$ (proportional to the denominator). Specifically, if $s_{fy} = \\frac{1}{n-1}\\sum(f_i - \\bar{f})(y_i - \\bar{y})$ and $s_f^2 = \\frac{1}{n-1}\\sum(f_i - \\bar{f})^2$, then $\\hat{b} = \\frac{s_{fy}}{s_f^2}$. This completes the derivation.\n\n**2. Calculation for the Given Dataset**\n\nThe dataset is $\\{(275, 274), (280, 277.5), (285, 282), (290, 287.5), (295, 294)\\}$, with each of the $5$ pairs repeated, for a total of $n=10$ data points.\n\nFirst, we compute the sample means $\\bar{f}$ and $\\bar{y}$:\n$$\\sum_{i=1}^{10} f_i = 2 \\times (275 + 280 + 285 + 290 + 295) = 2 \\times 1425 = 2850$$\n$$\\bar{f} = \\frac{2850}{10} = 285 \\, \\text{K}$$\n$$\\sum_{i=1}^{10} y_i = 2 \\times (274 + 277.5 + 282 + 287.5 + 294) = 2 \\times 1415 = 2830$$\n$$\\bar{y} = \\frac{2830}{10} = 283 \\, \\text{K}$$\n\nNext, we calculate the sums of squares and cross-products needed for $\\hat{b}$.\nThe centered forecast values $(f_i - \\bar{f})$ for the unique points are:\n$275 - 285 = -10$, $280 - 285 = -5$, $285 - 285 = 0$, $290 - 285 = 5$, $295 - 285 = 10$.\nThe centered observation values $(y_i - \\bar{y})$ for the unique points are:\n$274 - 283 = -9$, $277.5 - 283 = -5.5$, $282 - 283 = -1$, $287.5 - 283 = 4.5$, $294 - 283 = 11$.\n\nThe denominator for $\\hat{b}$:\n$$\\sum_{i=1}^{10}(f_i - \\bar{f})^2 = 2 \\times [(-10)^2 + (-5)^2 + 0^2 + 5^2 + 10^2] = 2 \\times [100 + 25 + 0 + 25 + 100] = 2 \\times 250 = 500$$\n\nThe numerator for $\\hat{b}$:\n$$\\sum_{i=1}^{10}(f_i - \\bar{f})(y_i - \\bar{y}) = 2 \\times [(-10)(-9) + (-5)(-5.5) + (0)(-1) + (5)(4.5) + (10)(11)]$$\n$$= 2 \\times [90 + 27.5 + 0 + 22.5 + 110] = 2 \\times 250 = 500$$\n\nNow, we compute $\\hat{b}$:\n$$\\hat{b} = \\frac{500}{500} = 1$$\nThis is a dimensionless quantity. To four significant figures, $\\hat{b} = 1.000$.\n\nFinally, we compute $\\hat{a}$:\n$$\\hat{a} = \\bar{y} - \\hat{b}\\bar{f} = 283 - (1)(285) = -2$$\nThe units of $\\hat{a}$ are Kelvin. To four significant figures, $\\hat{a} = -2.000 \\, \\text{K}$.\nThe fitted linear model is $\\hat{y} = -2 + 1 \\cdot f$.\n\n### Part 2: Residual Analysis for Nonlinearity\n\nWe now compute the Pearson correlation coefficient, $D$, between the residuals $r_i = y_i - (\\hat{a} + \\hat{b} f_i)$ and the squared centered forecast $s_i = (f_i - \\bar{f})^2$.\n\n**1. Calculation of $r_i$ and $s_i$**\n\nThe residuals $r_i$ for the unique pairs are:\nFor $(275, 274)$: $r_i = 274 - (-2 + 275) = 274 - 273 = 1$.\nFor $(280, 277.5)$: $r_i = 277.5 - (-2 + 280) = 277.5 - 278 = -0.5$.\nFor $(285, 282)$: $r_i = 282 - (-2 + 285) = 282 - 283 = -1$.\nFor $(290, 287.5)$: $r_i = 287.5 - (-2 + 290) = 287.5 - 288 = -0.5$.\nFor $(295, 294)$: $r_i = 294 - (-2 + 295) = 294 - 293 = 1$.\nThe set of all $10$ residuals is $\\{1, -0.5, -1, -0.5, 1, 1, -0.5, -1, -0.5, 1\\}$.\nThe mean of the residuals is $\\bar{r} = \\frac{1}{10} \\sum r_i = \\frac{2 \\times (1 - 0.5 - 1 - 0.5 + 1)}{10} = 0$.\n\nThe values of $s_i = (f_i - \\bar{f})^2$ for the unique pairs are:\n$s_1 = (-10)^2 = 100$.\n$s_2 = (-5)^2 = 25$.\n$s_3 = 0^2 = 0$.\n$s_4 = 5^2 = 25$.\n$s_5 = 10^2 = 100$.\nThe set of all $10$ values is $\\{100, 25, 0, 25, 100, 100, 25, 0, 25, 100\\}$.\nThe mean is $\\bar{s} = \\frac{1}{10}\\sum s_i = \\frac{500}{10} = 50$.\n\n**2. Calculation of Correlation Coefficient $D$**\n\nThe Pearson correlation coefficient $D = \\text{corr}(r, s)$ is given by:\n$$D = \\frac{\\sum_{i=1}^{n} (r_i - \\bar{r})(s_i - \\bar{s})}{\\sqrt{\\sum_{i=1}^{n} (r_i - \\bar{r})^2 \\sum_{i=1}^{n} (s_i - \\bar{s})^2}}$$\nSince $\\bar{r}=0$, the formula simplifies to:\n$$D = \\frac{\\sum_{i=1}^{n} r_i(s_i - \\bar{s})}{\\sqrt{\\sum_{i=1}^{n} r_i^2 \\sum_{i=1}^{n} (s_i - \\bar{s})^2}}$$\n\nWe compute the terms:\nNumerator: $\\sum r_i(s_i - \\bar{s}) = 2 \\times [1(100-50) + (-0.5)(25-50) + (-1)(0-50) + (-0.5)(25-50) + 1(100-50)]$.\n$$= 2 \\times [1(50) + (-0.5)(-25) + (-1)(-50) + (-0.5)(-25) + 1(50)]$$\n$$= 2 \\times [50 + 12.5 + 50 + 12.5 + 50] = 2 \\times 175 = 350$$\n\nDenominator term 1: Sum of squared residuals.\n$$\\sum r_i^2 = 2 \\times [1^2 + (-0.5)^2 + (-1)^2 + (-0.5)^2 + 1^2] = 2 \\times [1 + 0.25 + 1 + 0.25 + 1] = 2 \\times 3.5 = 7$$\n\nDenominator term 2: Sum of centered squared $s_i$.\n$$\\sum (s_i - \\bar{s})^2 = 2 \\times [(100-50)^2 + (25-50)^2 + (0-50)^2 + (25-50)^2 + (100-50)^2]$$\n$$= 2 \\times [50^2 + (-25)^2 + (-50)^2 + (-25)^2 + 50^2]$$\n$$= 2 \\times [2500 + 625 + 2500 + 625 + 2500] = 2 \\times 8750 = 17500$$\n\nNow we compute $D$:\n$$D = \\frac{350}{\\sqrt{7 \\times 17500}} = \\frac{350}{\\sqrt{122500}} = \\frac{350}{350} = 1$$\nTo four significant figures, $D = 1.000$.\n\n**3. Interpretation of $D$**\n\nThe coefficient $D$ measures the linear association between the residuals of the linear model, $r_i$, and the squared centered forecast, $s_i = (f_i - \\bar{f})^2$. A non-zero value of $D$ indicates a systematic pattern in the residuals that is related to a quadratic function of the predictor, which is a strong sign of nonlinearity and model misspecification.\n\nIn this case, $D=1.000$. This value represents a perfect positive linear correlation. It indicates that the residuals from the linear fit $y_i = \\hat{a} + \\hat{b} f_i$ are not random noise but instead follow a deterministic relationship with the forecast values. Specifically, the residuals are an exact positive linear function of $(f_i - \\bar{f})^2$.\n\nThe positive sign of $D$ implies that the true relationship between $y$ and $f$ is convex (U-shaped). The linear model systematically underestimates the observed values $y_i$ for forecasts $f_i$ that are far from the mean forecast $\\bar{f}$ (where $(f_i - \\bar{f})^2$ is large, residuals are positive), and overestimates $y_i$ for forecasts near the mean (where $(f_i - \\bar{f})^2$ is small, residuals are negative). This is confirmed by our calculated residuals: $r_i$ is $-1$ at $f_i=\\bar{f}=285$ and is $+1$ at the extreme forecast values of $275$ and $295$.\n\nTherefore, the result $D=1$ is an unambiguous diagnosis of unmodeled quadratic nonlinearity, suggesting that a model of the form $y \\approx \\beta_0 + \\beta_1 f + \\beta_2 f^2$ would provide a more accurate fit to the data.\n\nFinal numerical values rounded to four significant figures are:\n$\\hat{a} = -2.000 \\, \\text{K}$\n$\\hat{b} = 1.000$\n$D = 1.000$",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n-2.000 & 1.000 & 1.000\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "While linear models correct for mean bias, they may not capture errors across the entire forecast distribution. This exercise introduces quantile mapping, a powerful technique that corrects the full distribution of a forecast variable by aligning its quantiles with those of the observed climatology. By exploring this method in an analytical setting under nonstationary conditions, you will gain a deep understanding of a major challenge in bias correction: how violations of the stationarity assumption can lead to incorrect adjustments, particularly for extreme events .",
            "id": "4076588",
            "problem": "Consider a post-processing task in numerical weather prediction and climate modeling, focusing on bias correction using quantile mapping under potential nonstationarity. Assume a parametric setting in which both model outputs and observations are well approximated by Gaussian distributions. Let the calibration period be characterized by a model variable with distribution $X_{\\mathrm{m,cal}} \\sim \\mathcal{N}(\\mu_{\\mathrm{m}}, \\sigma_{\\mathrm{m}})$ and an observed variable with distribution $Y_{\\mathrm{o,cal}} \\sim \\mathcal{N}(\\mu_{\\mathrm{o}}, \\sigma_{\\mathrm{o}})$. Let the future period be characterized by a model variable $X_{\\mathrm{m,fut}} \\sim \\mathcal{N}(\\mu_{\\mathrm{mf}}, \\sigma_{\\mathrm{mf}})$ and the true observed variable $Y_{\\mathrm{o,fut}} \\sim \\mathcal{N}(\\mu_{\\mathrm{of}}, \\sigma_{\\mathrm{of}})$. All variables are dimensionless normalized anomalies, meaning no physical units are required.\n\nQuantile mapping is defined as follows: given a continuous and strictly increasing cumulative distribution function $F$, the quantile function $F^{-1}$ is its functional inverse. The quantile mapping correction for a future model realization $x$ uses calibration-period distributions and is given by $y_{\\mathrm{corr}} = F_{\\mathrm{o,cal}}^{-1}(F_{\\mathrm{m,cal}}(x))$. Under nonstationarity, the variance structure may change between the calibration and future periods, which can lead to over-correction or under-correction at distribution tails.\n\nYour task is to implement an analytic quantile-mapping correction under the Gaussian assumption and compute tail-quantile errors for specified test cases. Specifically:\n\n- Derive the corrected future distribution parameters for $y_{\\mathrm{corr}}$ under the Gaussian assumption, starting from the definition of quantile mapping and properties of the Gaussian cumulative distribution function.\n- For each test case, compute the error at two tail quantiles, $q_{\\mathrm{hi}}$ and $q_{\\mathrm{lo}}$, defined as\n$$\\Delta_{\\mathrm{hi}} = Q_{\\mathrm{corr}}(q_{\\mathrm{hi}}) - Q_{\\mathrm{true}}(q_{\\mathrm{hi}}), \\quad \\Delta_{\\mathrm{lo}} = Q_{\\mathrm{corr}}(q_{\\mathrm{lo}}) - Q_{\\mathrm{true}}(q_{\\mathrm{lo}}),$$\nwhere $Q_{\\mathrm{corr}}(q)$ is the quantile of the corrected distribution at level $q$, and $Q_{\\mathrm{true}}(q)$ is the quantile of the true future observed distribution at level $q$. Express $q_{\\mathrm{hi}}$ and $q_{\\mathrm{lo}}$ as decimals in $[0,1]$.\n\nInterpretation guidelines:\n- If $\\Delta_{\\mathrm{hi}}$ is positive, the correction overstates the upper tail (over-correction), and if negative, it understates the upper tail (under-correction).\n- If $\\Delta_{\\mathrm{lo}}$ is negative, the correction overstates the lower tail (over-correction), and if positive, it understates the lower tail (under-correction).\n\nUse the following test suite of parameter values, covering a baseline stationary case, various nonstationary variance changes, extreme-tail quantiles, and a mean-shift interaction case:\n\n- Test case $1$ (stationary baseline):\n  - Calibration: $\\mu_{\\mathrm{m}} = 0$, $\\sigma_{\\mathrm{m}} = 2$, $\\mu_{\\mathrm{o}} = 1$, $\\sigma_{\\mathrm{o}} = 3$.\n  - Future: $\\mu_{\\mathrm{mf}} = 0$, $\\sigma_{\\mathrm{mf}} = 2$, $\\mu_{\\mathrm{of}} = 1$, $\\sigma_{\\mathrm{of}} = 3$.\n  - Quantiles: $q_{\\mathrm{hi}} = 0.99$, $q_{\\mathrm{lo}} = 0.01$.\n\n- Test case $2$ (variance increase, moderate mismatch):\n  - Calibration: $\\mu_{\\mathrm{m}} = 0$, $\\sigma_{\\mathrm{m}} = 2$, $\\mu_{\\mathrm{o}} = 1$, $\\sigma_{\\mathrm{o}} = 3$.\n  - Future: $\\mu_{\\mathrm{mf}} = 0$, $\\sigma_{\\mathrm{mf}} = 3$, $\\mu_{\\mathrm{of}} = 1$, $\\sigma_{\\mathrm{of}} = 4$.\n  - Quantiles: $q_{\\mathrm{hi}} = 0.99$, $q_{\\mathrm{lo}} = 0.01$.\n\n- Test case $3$ (variance decrease in the model, increase in truth):\n  - Calibration: $\\mu_{\\mathrm{m}} = 0$, $\\sigma_{\\mathrm{m}} = 2$, $\\mu_{\\mathrm{o}} = 1$, $\\sigma_{\\mathrm{o}} = 3$.\n  - Future: $\\mu_{\\mathrm{mf}} = 0$, $\\sigma_{\\mathrm{mf}} = 1.5$, $\\mu_{\\mathrm{of}} = 1$, $\\sigma_{\\mathrm{of}} = 4$.\n  - Quantiles: $q_{\\mathrm{hi}} = 0.99$, $q_{\\mathrm{lo}} = 0.01$.\n\n- Test case $4$ (extreme upper and lower tails):\n  - Calibration: $\\mu_{\\mathrm{m}} = 0$, $\\sigma_{\\mathrm{m}} = 2$, $\\mu_{\\mathrm{o}} = 1$, $\\sigma_{\\mathrm{o}} = 3$.\n  - Future: $\\mu_{\\mathrm{mf}} = 0$, $\\sigma_{\\mathrm{mf}} = 5$, $\\mu_{\\mathrm{of}} = 1$, $\\sigma_{\\mathrm{of}} = 3$.\n  - Quantiles: $q_{\\mathrm{hi}} = 0.999$, $q_{\\mathrm{lo}} = 0.001$.\n\n- Test case $5$ (mean shift interaction and variance change):\n  - Calibration: $\\mu_{\\mathrm{m}} = 0$, $\\sigma_{\\mathrm{m}} = 2$, $\\mu_{\\mathrm{o}} = 1$, $\\sigma_{\\mathrm{o}} = 3$.\n  - Future: $\\mu_{\\mathrm{mf}} = 1$, $\\sigma_{\\mathrm{mf}} = 2$, $\\mu_{\\mathrm{of}} = 2.2$, $\\sigma_{\\mathrm{of}} = 3$.\n  - Quantiles: $q_{\\mathrm{hi}} = 0.99$, $q_{\\mathrm{lo}} = 0.01$.\n\nYour program must compute, for each test case, the pair of floats $[\\Delta_{\\mathrm{hi}}, \\Delta_{\\mathrm{lo}}]$ as defined above, and aggregate all test case results into a single line. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case result is a list of floats. For example, the output should look like $[[a,b],[c,d],\\dots]$. The values must be computed exactly according to the definitions above; they may be printed in standard floating-point notation. No external input is permitted, and no physical units apply.",
            "solution": "The user-provided problem has been assessed and is valid. It is scientifically grounded, well-posed, and objective. It presents a standard, formalizable problem in statistical post-processing for climate and weather models, with all necessary parameters and definitions provided.\n\nThe core of the problem is to derive the analytical form of a quantile-mapped distribution under Gaussian assumptions and then to evaluate the error of this correction method under nonstationary conditions.\n\nFirst, we derive the parameters of the corrected future distribution. The quantile mapping correction is defined as:\n$$y_{\\mathrm{corr}} = F_{\\mathrm{o,cal}}^{-1}(F_{\\mathrm{m,cal}}(x))$$\nwhere $x$ is a value from the future model output distribution, $X_{\\mathrm{m,fut}} \\sim \\mathcal{N}(\\mu_{\\mathrm{mf}}, \\sigma_{\\mathrm{mf}})$. The cumulative distribution functions (CDFs) and quantile functions (inverse CDFs) are based on the calibration period distributions: $X_{\\mathrm{m,cal}} \\sim \\mathcal{N}(\\mu_{\\mathrm{m}}, \\sigma_{\\mathrm{m}})$ and $Y_{\\mathrm{o,cal}} \\sim \\mathcal{N}(\\mu_{\\mathrm{o}}, \\sigma_{\\mathrm{o}})$.\n\nLet $\\Phi(z)$ be the CDF of the standard normal distribution $\\mathcal{N}(0, 1)$, and $\\Phi^{-1}(q)$ be its inverse (the quantile function). For a general Gaussian distribution $\\mathcal{N}(\\mu, \\sigma)$, the CDF is $F(x) = \\Phi\\left(\\frac{x - \\mu}{\\sigma}\\right)$, and the quantile function is $F^{-1}(q) = \\mu + \\sigma \\Phi^{-1}(q)$.\n\nApplying these definitions to our calibration distributions:\n$$F_{\\mathrm{m,cal}}(x) = \\Phi\\left(\\frac{x - \\mu_{\\mathrm{m}}}{\\sigma_{\\mathrm{m}}}\\right)$$\n$$F_{\\mathrm{o,cal}}^{-1}(q) = \\mu_{\\mathrm{o}} + \\sigma_{\\mathrm{o}} \\Phi^{-1}(q)$$\n\nSubstituting $F_{\\mathrm{m,cal}}(x)$ into $F_{\\mathrm{o,cal}}^{-1}(q)$ yields the transformation for any given model output $x$:\n$$y_{\\mathrm{corr}}(x) = F_{\\mathrm{o,cal}}^{-1}\\left( \\Phi\\left(\\frac{x - \\mu_{\\mathrm{m}}}{\\sigma_{\\mathrm{m}}}\\right) \\right) = \\mu_{\\mathrm{o}} + \\sigma_{\\mathrm{o}} \\Phi^{-1}\\left( \\Phi\\left(\\frac{x - \\mu_{\\mathrm{m}}}{\\sigma_{\\mathrm{m}}}\\right) \\right)$$\nSince $\\Phi^{-1}(\\Phi(z)) = z$, the expression simplifies to a linear transformation of $x$:\n$$y_{\\mathrm{corr}}(x) = \\mu_{\\mathrm{o}} + \\sigma_{\\mathrm{o}} \\left(\\frac{x - \\mu_{\\mathrm{m}}}{\\sigma_{\\mathrm{m}}}\\right) = \\left(\\mu_{\\mathrm{o}} - \\frac{\\sigma_{\\mathrm{o}}}{\\sigma_{\\mathrm{m}}}\\mu_{\\mathrm{m}}\\right) + \\left(\\frac{\\sigma_{\\mathrm{o}}}{\\sigma_{\\mathrm{m}}}\\right) x$$\nWe now apply this transformation to the random variable $X_{\\mathrm{m,fut}} \\sim \\mathcal{N}(\\mu_{\\mathrm{mf}}, \\sigma_{\\mathrm{mf}})$ to find the distribution of the corrected variable, $Y_{\\mathrm{corr}}$. Since $Y_{\\mathrm{corr}}$ is a linear transformation of a Gaussian variable, it is also Gaussian, $Y_{\\mathrm{corr}} \\sim \\mathcal{N}(\\mu_{\\mathrm{corr}}, \\sigma_{\\mathrm{corr}})$.\n\nThe mean of the corrected distribution, $\\mu_{\\mathrm{corr}}$, is:\n$$\\mu_{\\mathrm{corr}} = E[Y_{\\mathrm{corr}}] = E\\left[ \\mu_{\\mathrm{o}} - \\frac{\\sigma_{\\mathrm{o}}}{\\sigma_{\\mathrm{m}}}\\mu_{\\mathrm{m}} + \\frac{\\sigma_{\\mathrm{o}}}{\\sigma_{\\mathrm{m}}} X_{\\mathrm{m,fut}} \\right] = \\mu_{\\mathrm{o}} - \\frac{\\sigma_{\\mathrm{o}}}{\\sigma_{\\mathrm{m}}}\\mu_{\\mathrm{m}} + \\frac{\\sigma_{\\mathrm{o}}}{\\sigma_{\\mathrm{m}}} E[X_{\\mathrm{m,fut}}]$$\n$$\\mu_{\\mathrm{corr}} = \\mu_{\\mathrm{o}} + \\frac{\\sigma_{\\mathrm{o}}}{\\sigma_{\\mathrm{m}}}(\\mu_{\\mathrm{mf}} - \\mu_{\\mathrm{m}})$$\nThe variance of the corrected distribution, $\\sigma_{\\mathrm{corr}}^2$, is:\n$$\\sigma_{\\mathrm{corr}}^2 = \\mathrm{Var}(Y_{\\mathrm{corr}}) = \\mathrm{Var}\\left( \\mu_{\\mathrm{o}} - \\frac{\\sigma_{\\mathrm{o}}}{\\sigma_{\\mathrm{m}}}\\mu_{\\mathrm{m}} + \\frac{\\sigma_{\\mathrm{o}}}{\\sigma_{\\mathrm{m}}} X_{\\mathrm{m,fut}} \\right) = \\mathrm{Var}\\left( \\frac{\\sigma_{\\mathrm{o}}}{\\sigma_{\\mathrm{m}}} X_{\\mathrm{m,fut}} \\right) = \\left(\\frac{\\sigma_{\\mathrm{o}}}{\\sigma_{\\mathrm{m}}}\\right)^2 \\mathrm{Var}(X_{\\mathrm{m,fut}})$$\n$$\\sigma_{\\mathrm{corr}}^2 = \\left(\\frac{\\sigma_{\\mathrm{o}}}{\\sigma_{\\mathrm{m}}}\\right)^2 \\sigma_{\\mathrm{mf}}^2$$\nThus, the standard deviation of the corrected distribution is:\n$$\\sigma_{\\mathrm{corr}} = \\frac{\\sigma_{\\mathrm{o}}}{\\sigma_{\\mathrm{m}}} \\sigma_{\\mathrm{mf}}$$\n\nNext, we compute the tail-quantile errors, $\\Delta_{\\mathrm{hi}}$ and $\\Delta_{\\mathrm{lo}}$.\nThe quantile of the corrected distribution at level $q$ is $Q_{\\mathrm{corr}}(q) = \\mu_{\\mathrm{corr}} + \\sigma_{\\mathrm{corr}} \\Phi^{-1}(q)$.\nThe quantile of the true future observed distribution, $Y_{\\mathrm{o,fut}} \\sim \\mathcal{N}(\\mu_{\\mathrm{of}}, \\sigma_{\\mathrm{of}})$, is $Q_{\\mathrm{true}}(q) = \\mu_{\\mathrm{of}} + \\sigma_{\\mathrm{of}} \\Phi^{-1}(q)$.\n\nThe errors are defined as:\n$$\\Delta_{\\mathrm{hi}} = Q_{\\mathrm{corr}}(q_{\\mathrm{hi}}) - Q_{\\mathrm{true}}(q_{\\mathrm{hi}}) = (\\mu_{\\mathrm{corr}} + \\sigma_{\\mathrm{corr}} \\Phi^{-1}(q_{\\mathrm{hi}})) - (\\mu_{\\mathrm{of}} + \\sigma_{\\mathrm{of}} \\Phi^{-1}(q_{\\mathrm{hi}}))$$\n$$\\Delta_{\\mathrm{hi}} = (\\mu_{\\mathrm{corr}} - \\mu_{\\mathrm{of}}) + (\\sigma_{\\mathrm{corr}} - \\sigma_{\\mathrm{of}}) \\Phi^{-1}(q_{\\mathrm{hi}})$$\n\n$$\\Delta_{\\mathrm{lo}} = Q_{\\mathrm{corr}}(q_{\\mathrm{lo}}) - Q_{\\mathrm{true}}(q_{\\mathrm{lo}}) = (\\mu_{\\mathrm{corr}} + \\sigma_{\\mathrm{corr}} \\Phi^{-1}(q_{\\mathrm{lo}})) - (\\mu_{\\mathrm{of}} + \\sigma_{\\mathrm{of}} \\Phi^{-1}(q_{\\mathrm{lo}}))$$\n$$\\Delta_{\\mathrm{lo}} = (\\mu_{\\mathrm{corr}} - \\mu_{\\mathrm{of}}) + (\\sigma_{\\mathrm{corr}} - \\sigma_{\\mathrm{of}}) \\Phi^{-1}(q_{\\mathrm{lo}})$$\nLetting $z_q = \\Phi^{-1}(q)$, the formulas become:\n$$\\Delta_{\\mathrm{hi}} = (\\mu_{\\mathrm{corr}} - \\mu_{\\mathrm{of}}) + (\\sigma_{\\mathrm{corr}} - \\sigma_{\\mathrm{of}}) z_{q_{\\mathrm{hi}}}$$\n$$\\Delta_{\\mathrm{lo}} = (\\mu_{\\mathrm{corr}} - \\mu_{\\mathrm{of}}) + (\\sigma_{\\mathrm{corr}} - \\sigma_{\\mathrm{of}}) z_{q_{\\mathrm{lo}}}$$\n\nThe computational procedure for each test case is as follows:\n1.  Given the parameters for calibration ($\\mu_{\\mathrm{m}}$, $\\sigma_{\\mathrm{m}}$, $\\mu_{\\mathrm{o}}$, $\\sigma_{\\mathrm{o}}$) and future ($\\mu_{\\mathrm{mf}}$, $\\sigma_{\\mathrm{mf}}$, $\\mu_{\\mathrm{of}}$, $\\sigma_{\\mathrm{of}}$) periods, first compute the parameters of the corrected Normal distribution, $\\mu_{\\mathrm{corr}}$ and $\\sigma_{\\mathrm{corr}}$.\n2.  Given the quantile levels $q_{\\mathrm{hi}}$ and $q_{\\mathrm{lo}}$, compute the corresponding standard normal quantiles, $z_{q_{\\mathrm{hi}}}$ and $z_{q_{\\mathrm{lo}}}$, using the inverse CDF $\\Phi^{-1}$.\n3.  Substitute these values into the derived expressions for $\\Delta_{\\mathrm{hi}}$ and $\\Delta_{\\mathrm{lo}}$ to find the quantile errors.\nThis process will now be implemented for each provided test case.",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Computes quantile mapping errors for several test cases under Gaussian assumptions.\n    \"\"\"\n    # Each test case is a tuple of parameters:\n    # (mu_m, sigma_m, mu_o, sigma_o, mu_mf, sigma_mf, mu_of, sigma_of, q_hi, q_lo)\n    test_cases = [\n        # Test case 1 (stationary baseline)\n        (0.0, 2.0, 1.0, 3.0, 0.0, 2.0, 1.0, 3.0, 0.99, 0.01),\n        # Test case 2 (variance increase, moderate mismatch)\n        (0.0, 2.0, 1.0, 3.0, 0.0, 3.0, 1.0, 4.0, 0.99, 0.01),\n        # Test case 3 (variance decrease in model, increase in truth)\n        (0.0, 2.0, 1.0, 3.0, 0.0, 1.5, 1.0, 4.0, 0.99, 0.01),\n        # Test case 4 (extreme upper and lower tails)\n        (0.0, 2.0, 1.0, 3.0, 0.0, 5.0, 1.0, 3.0, 0.999, 0.001),\n        # Test case 5 (mean shift interaction and variance change)\n        (0.0, 2.0, 1.0, 3.0, 1.0, 2.0, 2.2, 3.0, 0.99, 0.01)\n    ]\n\n    results = []\n    for case in test_cases:\n        mu_m, sigma_m, mu_o, sigma_o, mu_mf, sigma_mf, mu_of, sigma_of, q_hi, q_lo = case\n\n        # Step 1: Calculate parameters of the corrected distribution.\n        # This is based on the linear transformation y_corr = a + b * x where\n        # b = sigma_o / sigma_m and a = mu_o - b * mu_m.\n        # The mean is E[a + b * X_mf] = a + b * E[X_mf] = a + b * mu_mf.\n        # The std dev is sqrt(Var(a + b * X_mf)) = sqrt(b^2 * Var(X_mf)) = b * sigma_mf.\n        \n        mu_corr = mu_o + (sigma_o / sigma_m) * (mu_mf - mu_m)\n        sigma_corr = sigma_o * (sigma_mf / sigma_m)\n\n        # Step 2: Get standard normal quantiles (z-scores) for q_hi and q_lo.\n        z_hi = norm.ppf(q_hi)\n        z_lo = norm.ppf(q_lo)\n\n        # Step 3: Compute the quantile errors.\n        # Error = Q_corr(q) - Q_true(q)\n        #       = (mu_corr + sigma_corr * z_q) - (mu_of + sigma_of * z_q)\n        #       = (mu_corr - mu_of) + (sigma_corr - sigma_of) * z_q\n        \n        mu_diff = mu_corr - mu_of\n        sigma_diff = sigma_corr - sigma_of\n        \n        delta_hi = mu_diff + sigma_diff * z_hi\n        delta_lo = mu_diff + sigma_diff * z_lo\n        \n        results.append([delta_hi, delta_lo])\n\n    # Format the final output as a single line: [[a,b],[c,d],...]\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "Statistical post-processing models are not inherently aware of the physical laws governing the climate system, and purely statistical adjustments can sometimes yield unphysical results. This problem grounds statistical correction in physical reality by requiring the validation of post-processed surface energy fluxes against the fundamental law of conservation of energy, $R_n = H + L + G + S$. Completing this exercise will train you to implement critical sanity checks, ensuring that bias-corrected outputs remain plausible and do not violate inviolable principles, thereby preventing the generation of scientifically unsound data .",
            "id": "4076541",
            "problem": "Consider a post-processing and bias correction validation problem in Numerical Weather Prediction (NWP) and climate modeling. After calibration, adjusted surface energy flux components must remain physically plausible and must satisfy the conservation of energy encoded by the Surface Energy Balance (SEB). The SEB at the land-atmosphere interface is given by the fundamental conservation law\n$$\nR_n = H + L + G + S,\n$$\nwhere $R_n$ is net radiation, $H$ is sensible heat flux, $L$ is latent heat flux, $G$ is ground heat flux, and $S$ is energy storage or residual, each expressed in watts per square meter ($\\text{W m}^{-2}$). This equality must hold at each time step within a specified tolerance.\n\nSuppose raw model outputs provide time series of $R_n$, $H$, $L$, $G$, and $S$ for a single grid cell. Post-processing applies affine corrections to each component:\n$$\nX^{\\mathrm{corr}} = s_X \\left( X^{\\mathrm{raw}} + b_X \\right),\n$$\nwhere $X \\in \\{R_n, H, L, G, S\\}$, $s_X$ is a dimensionless multiplicative scaling, and $b_X$ is an additive bias measured in watts per square meter ($\\text{W m}^{-2}$). By conservation of energy, physically consistent corrections must satisfy, at each time step $t$,\n$$\n\\epsilon_t \\equiv R_n^{\\mathrm{corr}}(t) - \\left[H^{\\mathrm{corr}}(t) + L^{\\mathrm{corr}}(t) + G^{\\mathrm{corr}}(t) + S^{\\mathrm{corr}}(t)\\right],\n$$\nwith $|\\epsilon_t| \\le \\tau$, where $\\tau$ is a nonnegative tolerance parameter in watts per square meter ($\\text{W m}^{-2}$). Additionally, physically plausible flux magnitudes must satisfy an absolute bound $|X^{\\mathrm{corr}}(t)| \\le M$ for all components $X$ and time steps $t$. Use $M = 1500$ $\\text{W m}^{-2}$ unless a test case specifies otherwise.\n\nYour task is to implement a program that, for each provided test case, computes the maximum absolute energy budget residual\n$$\n\\max_t |\\epsilon_t|\n$$\nand determines a boolean violation flag that is true if and only if any of the following holds:\n- There exists a time step $t$ such that $|\\epsilon_t| > \\tau$.\n- There exists a component $X$ and time step $t$ such that $|X^{\\mathrm{corr}}(t)| > M$.\n\nThe outputs must adhere to the following specifications:\n- The maximum absolute residual must be reported as a float in watts per square meter ($\\text{W m}^{-2}$), rounded to exactly $3$ decimal places.\n- The violation flag must be a boolean.\n- For each test case, the output must be a list of the form $[\\text{max\\_abs\\_residual}, \\text{violation\\_flag}]$.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[[0.123,\\mathrm{True}],[0.000,\\mathrm{False}]]$).\n\nUse the following test suite, which includes a range of realistic scenarios:\n\n- Test Case $1$ (happy path, conservative corrections, physically plausible magnitudes): \n  - Raw fluxes ($\\text{W m}^{-2}$):\n    - $R_n$: $[400, 300, 100, 50]$\n    - $H$: $[150, 120, 50, 20]$\n    - $L$: $[200, 150, 40, 20]$\n    - $G$: $[30, 20, 5, 5]$\n    - $S$: $[20, 10, 5, 5]$\n  - Corrections:\n    - $s_{R_n} = 1.05$, $b_{R_n} = 2.0$\n    - $s_H = 1.05$, $b_H = 0.5$\n    - $s_L = 1.05$, $b_L = 0.5$\n    - $s_G = 1.05$, $b_G = 0.5$\n    - $s_S = 1.05$, $b_S = 0.5$\n  - Tolerance: $\\tau = 0.5$ $\\text{W m}^{-2}$, Bound: $M = 1500$ $\\text{W m}^{-2}$.\n\n- Test Case $2$ (non-conservative additive biases, small closure violation):\n  - Raw fluxes ($\\text{W m}^{-2}$) identical to Test Case $1$.\n  - Corrections:\n    - $s_{R_n} = 1.0$, $b_{R_n} = 1.0$\n    - $s_H = 1.0$, $b_H = 0.1$\n    - $s_L = 1.0$, $b_L = 0.2$\n    - $s_G = 1.0$, $b_G = 0.3$\n    - $s_S = 1.0$, $b_S = 0.3$\n  - Tolerance: $\\tau = 0.05$ $\\text{W m}^{-2}$, Bound: $M = 1500$ $\\text{W m}^{-2}$.\n\n- Test Case $3$ (nighttime conditions with negative latent heat due to dew formation, conservative corrections):\n  - Raw fluxes ($\\text{W m}^{-2}$):\n    - $R_n$: $[-50, -20]$\n    - $H$: $[-20, -10]$\n    - $L$: $[-25, -5]$\n    - $G$: $[0, -3]$\n    - $S$: $[-5, -2]$\n  - Corrections:\n    - $s_{R_n} = 1.0$, $b_{R_n} = -1.0$\n    - $s_H = 1.0$, $b_H = -0.25$\n    - $s_L = 1.0$, $b_L = -0.25$\n    - $s_G = 1.0$, $b_G = 0.0$\n    - $s_S = 1.0$, $b_S = -0.5$\n  - Tolerance: $\\tau = 0.5$ $\\text{W m}^{-2}$, Bound: $M = 1500$ $\\text{W m}^{-2}$.\n\n- Test Case $4$ (physically implausible magnitudes after scaling, closure holds but bound violation):\n  - Raw fluxes ($\\text{W m}^{-2}$):\n    - $R_n$: $[1000]$\n    - $H$: $[600]$\n    - $L$: $[350]$\n    - $G$: $[30]$\n    - $S$: $[20]$\n  - Corrections:\n    - $s_{R_n} = 2.8$, $b_{R_n} = 0.0$\n    - $s_H = 2.8$, $b_H = 0.0$\n    - $s_L = 2.8$, $b_L = 0.0$\n    - $s_G = 2.8$, $b_G = 0.0$\n    - $s_S = 2.8$, $b_S = 0.0$\n  - Tolerance: $\\tau = 1.0$ $\\text{W m}^{-2}$, Bound: $M = 1500$ $\\text{W m}^{-2}$.\n\n- Test Case $5$ (boundary case with zero raw fluxes, residual equals tolerance):\n  - Raw fluxes ($\\text{W m}^{-2}$):\n    - $R_n$: $[0, 0, 0]$\n    - $H$: $[0, 0, 0]$\n    - $L$: $[0, 0, 0]$\n    - $G$: $[0, 0, 0]$\n    - $S$: $[0, 0, 0]$\n  - Corrections:\n    - $s_{R_n} = 1.0$, $b_{R_n} = 0.2$\n    - $s_H = 1.0$, $b_H = 0.05$\n    - $s_L = 1.0$, $b_L = 0.05$\n    - $s_G = 1.0$, $b_G = 0.0$\n    - $s_S = 1.0$, $b_S = 0.0$\n  - Tolerance: $\\tau = 0.1$ $\\text{W m}^{-2}$, Bound: $M = 1500$ $\\text{W m}^{-2}$.\n\nImplement the program to:\n- Apply the specified corrections to each flux component time series.\n- Compute the residual time series $\\epsilon_t$.\n- Compute $\\max_t |\\epsilon_t|$ and round to exactly $3$ decimal places in watts per square meter ($\\text{W m}^{-2}$).\n- Compute the violation flag based on the above rules.\n- Print a single line containing the list of results for the $5$ test cases, in the exact format $[[\\text{float},\\text{bool}],\\ldots]$.",
            "solution": "The problem statement has been validated and is deemed to be scientifically grounded, well-posed, and objective. It presents a clear and formalizable task rooted in the fundamental physical principle of surface energy conservation, which is a cornerstone of climatology and numerical weather prediction. The problem is self-contained, providing all necessary data and constraints for a unique and verifiable solution.\n\nThe task is to validate post-processed time series of surface energy fluxes against two primary criteria: the conservation of energy and the physical plausibility of flux magnitudes. The algorithmic procedure to accomplish this for each test case is as follows:\n\n1.  **Data Ingestion and Representation**: The raw time series for each flux component $X \\in \\{R_n, H, L, G, S\\}$ are represented as one-dimensional numerical arrays, denoted as $X^{\\mathrm{raw}}$. The correction parameters $s_X$ and $b_X$, and the validation thresholds $\\tau$ and $M$, are stored as scalar values.\n\n2.  **Application of Affine Correction**: For each flux component $X$, a corrected time series $X^{\\mathrm{corr}}$ is computed by applying the affine transformation to each element of the raw time series. The transformation is given by the equation:\n    $$\n    X^{\\mathrm{corr}}(t) = s_X \\left( X^{\\mathrm{raw}}(t) + b_X \\right)\n    $$\n    This operation is performed for all time steps $t$ for each of the five flux components.\n\n3.  **Physical Magnitude Validation**: The first validation check is for physical plausibility. A violation occurs if the absolute magnitude of any corrected flux component at any time step exceeds a predefined bound $M$. Mathematically, a violation is flagged if the following condition is met:\n    $$\n    \\exists X \\in \\{R_n, H, L, G, S\\}, \\exists t \\quad \\text{such that} \\quad |X^{\\mathrm{corr}}(t)| > M\n    $$\n    To implement this, we find the maximum absolute value across all corrected flux components and all time steps. If this maximum exceeds $M$, a `magnitude_violation` flag is set to true.\n\n4.  **Energy Closure Validation**: The second validation check ensures that the corrected fluxes adhere to the principle of energy conservation, as expressed by the Surface Energy Balance (SEB) equation, within a given tolerance $\\tau$. The energy budget residual, $\\epsilon_t$, is calculated for each time step $t$:\n    $$\n    \\epsilon_t = R_n^{\\mathrm{corr}}(t) - \\left( H^{\\mathrm{corr}}(t) + L^{\\mathrm{corr}}(t) + G^{\\mathrm{corr}}(t) + S^{\\mathrm{corr}}(t) \\right)\n    $$\n    This produces a time series of residuals, $\\epsilon$.\n\n5.  **Residual Analysis**: From the residual time series $\\epsilon$, the primary metric for judging the energy closure is the maximum absolute residual over the entire period:\n    $$\n    \\max_t |\\epsilon_t|\n    $$\n    This value is calculated and will be reported as the first part of the output, rounded to exactly $3$ decimal places.\n\n6.  **Closure Violation Check**: The maximum absolute residual is compared against the tolerance $\\tau$. A closure violation occurs if this residual exceeds the tolerance. A `closure_violation` flag is set to true if:\n    $$\n    \\max_t |\\epsilon_t| > \\tau\n    $$\n\n7.  **Final Violation Determination**: The overall violation flag for the test case is determined by the logical OR of the two individual violation flags. The `violation_flag` is true if either the magnitude constraint or the energy closure constraint is violated:\n    $$\n    \\text{violation\\_flag} = (\\text{magnitude\\_violation}) \\lor (\\text{closure\\_violation})\n    $$\n\n8.  **Output Formulation**: The final result for a single test case is a two-element list containing the calculated maximum absolute residual (as a floating-point number) and the final boolean violation flag. The complete program will return a list of these results for all specified test cases. The implementation will utilize the `numpy` library for efficient, vectorized operations on the flux time series data.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the surface energy balance validation problem for a suite of test cases.\n    \"\"\"\n    test_cases = [\n        # Test Case 1 (happy path, conservative corrections)\n        {\n            \"raw_fluxes\": {\n                \"Rn\": np.array([400, 300, 100, 50]),\n                \"H\": np.array([150, 120, 50, 20]),\n                \"L\": np.array([200, 150, 40, 20]),\n                \"G\": np.array([30, 20, 5, 5]),\n                \"S\": np.array([20, 10, 5, 5]),\n            },\n            \"corrections\": {\n                \"s\": {\"Rn\": 1.05, \"H\": 1.05, \"L\": 1.05, \"G\": 1.05, \"S\": 1.05},\n                \"b\": {\"Rn\": 2.0, \"H\": 0.5, \"L\": 0.5, \"G\": 0.5, \"S\": 0.5},\n            },\n            \"params\": {\"tau\": 0.5, \"M\": 1500.0},\n        },\n        # Test Case 2 (non-conservative additive biases)\n        {\n            \"raw_fluxes\": {\n                \"Rn\": np.array([400, 300, 100, 50]),\n                \"H\": np.array([150, 120, 50, 20]),\n                \"L\": np.array([200, 150, 40, 20]),\n                \"G\": np.array([30, 20, 5, 5]),\n                \"S\": np.array([20, 10, 5, 5]),\n            },\n            \"corrections\": {\n                \"s\": {\"Rn\": 1.0, \"H\": 1.0, \"L\": 1.0, \"G\": 1.0, \"S\": 1.0},\n                \"b\": {\"Rn\": 1.0, \"H\": 0.1, \"L\": 0.2, \"G\": 0.3, \"S\": 0.3},\n            },\n            \"params\": {\"tau\": 0.05, \"M\": 1500.0},\n        },\n        # Test Case 3 (nighttime conditions, conservative corrections)\n        {\n            \"raw_fluxes\": {\n                \"Rn\": np.array([-50, -20]),\n                \"H\": np.array([-20, -10]),\n                \"L\": np.array([-25, -5]),\n                \"G\": np.array([0, -3]),\n                \"S\": np.array([-5, -2]),\n            },\n            \"corrections\": {\n                \"s\": {\"Rn\": 1.0, \"H\": 1.0, \"L\": 1.0, \"G\": 1.0, \"S\": 1.0},\n                \"b\": {\"Rn\": -1.0, \"H\": -0.25, \"L\": -0.25, \"G\": 0.0, \"S\": -0.5},\n            },\n            \"params\": {\"tau\": 0.5, \"M\": 1500.0},\n        },\n        # Test Case 4 (implausible magnitudes, bound violation)\n        {\n            \"raw_fluxes\": {\n                \"Rn\": np.array([1000]),\n                \"H\": np.array([600]),\n                \"L\": np.array([350]),\n                \"G\": np.array([30]),\n                \"S\": np.array([20]),\n            },\n            \"corrections\": {\n                \"s\": {\"Rn\": 2.8, \"H\": 2.8, \"L\": 2.8, \"G\": 2.8, \"S\": 2.8},\n                \"b\": {\"Rn\": 0.0, \"H\": 0.0, \"L\": 0.0, \"G\": 0.0, \"S\": 0.0},\n            },\n            \"params\": {\"tau\": 1.0, \"M\": 1500.0},\n        },\n        # Test Case 5 (boundary case, residual equals tolerance)\n        {\n            \"raw_fluxes\": {\n                \"Rn\": np.array([0, 0, 0]),\n                \"H\": np.array([0, 0, 0]),\n                \"L\": np.array([0, 0, 0]),\n                \"G\": np.array([0, 0, 0]),\n                \"S\": np.array([0, 0, 0]),\n            },\n            \"corrections\": {\n                \"s\": {\"Rn\": 1.0, \"H\": 1.0, \"L\": 1.0, \"G\": 1.0, \"S\": 1.0},\n                \"b\": {\"Rn\": 0.2, \"H\": 0.05, \"L\": 0.05, \"G\": 0.0, \"S\": 0.0},\n            },\n            \"params\": {\"tau\": 0.1, \"M\": 1500.0},\n        },\n    ]\n\n    results = []\n    flux_components = [\"Rn\", \"H\", \"L\", \"G\", \"S\"]\n    \n    for case in test_cases:\n        raw_fluxes = case[\"raw_fluxes\"]\n        corrections = case[\"corrections\"]\n        params = case[\"params\"]\n        tau = params[\"tau\"]\n        M = params[\"M\"]\n\n        # Apply affine corrections\n        corr_fluxes = {}\n        for comp in flux_components:\n            s_X = corrections[\"s\"][comp]\n            b_X = corrections[\"b\"][comp]\n            X_raw = raw_fluxes[comp]\n            corr_fluxes[comp] = s_X * (X_raw + b_X)\n\n        # Check for physical magnitude violation\n        max_abs_magnitude = 0.0\n        for comp in flux_components:\n            component_max_abs = np.max(np.abs(corr_fluxes[comp]))\n            if component_max_abs > max_abs_magnitude:\n                max_abs_magnitude = component_max_abs\n        \n        magnitude_violation = max_abs_magnitude > M\n\n        # Calculate energy budget residual\n        outgoing_flux_sum = (corr_fluxes[\"H\"] + corr_fluxes[\"L\"] + \n                             corr_fluxes[\"G\"] + corr_fluxes[\"S\"])\n        epsilon = corr_fluxes[\"Rn\"] - outgoing_flux_sum\n        \n        # Calculate max absolute residual\n        max_abs_residual = np.max(np.abs(epsilon))\n\n        # Check for energy closure violation\n        closure_violation = max_abs_residual > tau\n\n        # Determine final violation flag\n        violation_flag = magnitude_violation or closure_violation\n\n        results.append([max_abs_residual, violation_flag])\n\n    # Format the final output string exactly as specified\n    result_strings = []\n    for res in results:\n        max_res_val, flag_val = res\n        # Format float to 3 decimal places and boolean to string 'True'/'False'\n        res_str = f\"[{max_res_val:.3f},{str(flag_val)}]\"\n        result_strings.append(res_str)\n    \n    final_output = f\"[{','.join(result_strings)}]\"\n    print(final_output)\n\nsolve()\n\n```"
        }
    ]
}