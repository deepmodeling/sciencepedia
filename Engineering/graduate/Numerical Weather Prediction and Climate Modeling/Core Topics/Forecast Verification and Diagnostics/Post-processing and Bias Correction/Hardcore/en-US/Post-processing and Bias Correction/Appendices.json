{
    "hands_on_practices": [
        {
            "introduction": "The foundation of many post-processing systems lies in correcting systematic model errors using statistical relationships. This first practice explores the most fundamental of these methods: linear regression, a cornerstone of Model Output Statistics (MOS). You will derive the classic least-squares estimators from first principles and apply them to a hypothetical dataset, but the core of the exercise lies in critically evaluating the model's performance . By analyzing the structure of the residuals, you will learn to diagnose whether the simple linear assumption is sufficient or if unmodeled nonlinear relationships remain, a crucial skill for any robust modeling workflow.",
            "id": "4076566",
            "problem": "A forecast-to-observation bias correction in Numerical Weather Prediction (NWP) post-processing is often modeled as a linear mapping of the form $y \\approx a + b f$, where $f$ denotes the raw model forecast and $y$ denotes the verifying observation. In Model Output Statistics (MOS), the parameters $a$ and $b$ are estimated by least squares. Starting from the principle of minimizing the sum of squared errors, derive the least squares estimators in terms of sample statistics and then evaluate them for the dataset below. After computing the linear fit, examine the residual structure to diagnose nonlinearity by quantifying the linear association between residuals $r_i$ and the squared centered forecast $s_i = (f_i - \\bar{f})^{2}$.\n\nUse the following dataset of $n = 10$ paired values $\\{(f_i, y_i)\\}_{i=1}^{10}$ for $2$-meter temperature, where $f_i$ are forecasts in Kelvin and $y_i$ are verifying observations in Kelvin:\n$\\{(f_i, y_i)\\}_{i=1}^{10} = \\{(275, 274), (280, 277.5), (285, 282), (290, 287.5), (295, 294), (275, 274), (280, 277.5), (285, 282), (290, 287.5), (295, 294)\\}$.\n\nYour tasks are:\n1. Starting from the least squares criterion and core definitions of sample mean, sample variance, and sample covariance, derive the estimators $\\hat{a}$ and $\\hat{b}$ in terms of sample statistics and compute their numerical values for the given dataset. Express $\\hat{a}$ in Kelvin and $\\hat{b}$ as dimensionless.\n2. Compute the Pearson correlation coefficient $D$ between the residuals $r_i = y_i - (\\hat{a} + \\hat{b} f_i)$ and the squared centered forecast $s_i = (f_i - \\bar{f})^{2}$. Interpret the magnitude and sign of $D$ in the context of diagnosing nonlinearity in the bias correction.\n\nIf a numerical answer requires rounding, round to four significant figures. Express $\\hat{a}$ in Kelvin, and $\\hat{b}$ and $D$ as dimensionless quantities. Provide the final numerical values for $\\hat{a}$, $\\hat{b}$, and $D$.",
            "solution": "The problem asks for the derivation and calculation of linear regression parameters for a given dataset of meteorological forecasts and observations, followed by a residual analysis to diagnose nonlinearity. The process will be conducted in two parts as requested.\n\n### Part 1: Derivation and Calculation of Least Squares Estimators\n\nThe linear model is $y \\approx a + b f$. The parameters $a$ and $b$ are estimated by minimizing the Sum of Squared Errors (SSE), $S(a, b)$.\n\n**1. Derivation of Estimators**\n\nThe SSE is defined as:\n$$S(a,b) = \\sum_{i=1}^{n} (y_i - (a + b f_i))^2$$\nTo find the values of $a$ and $b$ that minimize $S$, we take the partial derivatives of $S$ with respect to $a$ and $b$ and set them to zero. The resulting estimators are denoted $\\hat{a}$ and $\\hat{b}$.\n\nThe first partial derivative with respect to $a$ is:\n$$\\frac{\\partial S}{\\partial a} = \\sum_{i=1}^{n} 2(y_i - a - b f_i)(-1) = -2 \\sum_{i=1}^{n} (y_i - a - b f_i)$$\nSetting $\\frac{\\partial S}{\\partial a} = 0$:\n$$\\sum_{i=1}^{n} (y_i - \\hat{a} - \\hat{b} f_i) = 0$$\n$$\\sum y_i - n\\hat{a} - \\hat{b}\\sum f_i = 0$$\nDividing by the sample size $n$ and using the definitions of the sample means $\\bar{y} = \\frac{1}{n}\\sum y_i$ and $\\bar{f} = \\frac{1}{n}\\sum f_i$:\n$$\\bar{y} - \\hat{a} - \\hat{b}\\bar{f} = 0$$\nThis gives the estimator for the intercept, $\\hat{a}$, in terms of the estimator for the slope, $\\hat{b}$:\n$$\\hat{a} = \\bar{y} - \\hat{b}\\bar{f}$$\n\nThe second partial derivative with respect to $b$ is:\n$$\\frac{\\partial S}{\\partial b} = \\sum_{i=1}^{n} 2(y_i - a - b f_i)(-f_i) = -2 \\sum_{i=1}^{n} f_i(y_i - a - b f_i)$$\nSetting $\\frac{\\partial S}{\\partial b} = 0$:\n$$\\sum_{i=1}^{n} f_i(y_i - \\hat{a} - \\hat{b} f_i) = 0$$\nSubstitute the expression for $\\hat{a}$:\n$$\\sum_{i=1}^{n} f_i(y_i - (\\bar{y} - \\hat{b}\\bar{f}) - \\hat{b} f_i) = 0$$\n$$\\sum_{i=1}^{n} f_i((y_i - \\bar{y}) - \\hat{b}(f_i - \\bar{f})) = 0$$\n$$\\sum_{i=1}^{n} f_i(y_i - \\bar{y}) - \\hat{b}\\sum_{i=1}^{n} f_i(f_i - \\bar{f}) = 0$$\nSolving for $\\hat{b}$:\n$$\\hat{b} = \\frac{\\sum f_i(y_i - \\bar{y})}{\\sum f_i(f_i - \\bar{f})}$$\nThe numerator and denominator can be simplified to a more standard form expressed with centered variables:\n$\\sum (f_i - \\bar{f})(y_i - \\bar{y}) = \\sum (f_iy_i - f_i\\bar{y} - \\bar{f}y_i + \\bar{f}\\bar{y}) = \\sum f_iy_i - \\bar{y}\\sum f_i - \\bar{f}\\sum y_i + n\\bar{f}\\bar{y} = \\sum f_iy_i - n\\bar{f}\\bar{y} - n\\bar{f}\\bar{y} + n\\bar{f}\\bar{y} = \\sum f_iy_i - n\\bar{f}\\bar{y}$.\nAlso, $\\sum f_i(y_i - \\bar{y}) = \\sum f_iy_i - \\bar{y}\\sum f_i = \\sum f_iy_i - n\\bar{f}\\bar{y}$.\nSo, $\\sum_i (f_i - \\bar{f})(y_i - \\bar{y}) = \\sum_i f_i(y_i - \\bar{y})$. Similarly, $\\sum_i (f_i - \\bar{f})^2 = \\sum_i f_i(f_i - \\bar{f})$.\nThis gives the standard expression for $\\hat{b}$:\n$$\\hat{b} = \\frac{\\sum_{i=1}^{n}(f_i - \\bar{f})(y_i - \\bar{y})}{\\sum_{i=1}^{n}(f_i - \\bar{f})^2}$$\nThis expresses $\\hat{b}$ in terms of the sample covariance of $f$ and $y$ (proportional to the numerator) and the sample variance of $f$ (proportional to the denominator). Specifically, if $s_{fy} = \\frac{1}{n-1}\\sum(f_i - \\bar{f})(y_i - \\bar{y})$ and $s_f^2 = \\frac{1}{n-1}\\sum(f_i - \\bar{f})^2$, then $\\hat{b} = \\frac{s_{fy}}{s_f^2}$. This completes the derivation.\n\n**2. Calculation for the Given Dataset**\n\nThe dataset is $\\{(275, 274), (280, 277.5), (285, 282), (290, 287.5), (295, 294)\\}$, with each of the $5$ pairs repeated, for a total of $n=10$ data points.\n\nFirst, we compute the sample means $\\bar{f}$ and $\\bar{y}$:\n$$\\sum_{i=1}^{10} f_i = 2 \\times (275 + 280 + 285 + 290 + 295) = 2 \\times 1425 = 2850$$\n$$\\bar{f} = \\frac{2850}{10} = 285 \\, \\text{K}$$\n$$\\sum_{i=1}^{10} y_i = 2 \\times (274 + 277.5 + 282 + 287.5 + 294) = 2 \\times 1415 = 2830$$\n$$\\bar{y} = \\frac{2830}{10} = 283 \\, \\text{K}$$\n\nNext, we calculate the sums of squares and cross-products needed for $\\hat{b}$.\nThe centered forecast values $(f_i - \\bar{f})$ for the unique points are:\n$275 - 285 = -10$, $280 - 285 = -5$, $285 - 285 = 0$, $290 - 285 = 5$, $295 - 285 = 10$.\nThe centered observation values $(y_i - \\bar{y})$ for the unique points are:\n$274 - 283 = -9$, $277.5 - 283 = -5.5$, $282 - 283 = -1$, $287.5 - 283 = 4.5$, $294 - 283 = 11$.\n\nThe denominator for $\\hat{b}$:\n$$\\sum_{i=1}^{10}(f_i - \\bar{f})^2 = 2 \\times [(-10)^2 + (-5)^2 + 0^2 + 5^2 + 10^2] = 2 \\times [100 + 25 + 0 + 25 + 100] = 2 \\times 250 = 500$$\n\nThe numerator for $\\hat{b}$:\n$$\\sum_{i=1}^{10}(f_i - \\bar{f})(y_i - \\bar{y}) = 2 \\times [(-10)(-9) + (-5)(-5.5) + (0)(-1) + (5)(4.5) + (10)(11)]$$\n$$= 2 \\times [90 + 27.5 + 0 + 22.5 + 110] = 2 \\times 250 = 500$$\n\nNow, we compute $\\hat{b}$:\n$$\\hat{b} = \\frac{500}{500} = 1$$\nThis is a dimensionless quantity. To four significant figures, $\\hat{b} = 1.000$.\n\nFinally, we compute $\\hat{a}$:\n$$\\hat{a} = \\bar{y} - \\hat{b}\\bar{f} = 283 - (1)(285) = -2$$\nThe units of $\\hat{a}$ are Kelvin. To four significant figures, $\\hat{a} = -2.000 \\, \\text{K}$.\nThe fitted linear model is $\\hat{y} = -2 + 1 \\cdot f$.\n\n### Part 2: Residual Analysis for Nonlinearity\n\nWe now compute the Pearson correlation coefficient, $D$, between the residuals $r_i = y_i - (\\hat{a} + \\hat{b} f_i)$ and the squared centered forecast $s_i = (f_i - \\bar{f})^2$.\n\n**1. Calculation of $r_i$ and $s_i$**\n\nThe residuals $r_i$ for the unique pairs are:\nFor $(275, 274)$: $r_i = 274 - (-2 + 275) = 274 - 273 = 1$.\nFor $(280, 277.5)$: $r_i = 277.5 - (-2 + 280) = 277.5 - 278 = -0.5$.\nFor $(285, 282)$: $r_i = 282 - (-2 + 285) = 282 - 283 = -1$.\nFor $(290, 287.5)$: $r_i = 287.5 - (-2 + 290) = 287.5 - 288 = -0.5$.\nFor $(295, 294)$: $r_i = 294 - (-2 + 295) = 294 - 293 = 1$.\nThe set of all $10$ residuals is $\\{1, -0.5, -1, -0.5, 1, 1, -0.5, -1, -0.5, 1\\}$.\nThe mean of the residuals is $\\bar{r} = \\frac{1}{10} \\sum r_i = \\frac{2 \\times (1 - 0.5 - 1 - 0.5 + 1)}{10} = 0$.\n\nThe values of $s_i = (f_i - \\bar{f})^2$ for the unique pairs are:\n$s_1 = (-10)^2 = 100$.\n$s_2 = (-5)^2 = 25$.\n$s_3 = 0^2 = 0$.\n$s_4 = 5^2 = 25$.\n$s_5 = 10^2 = 100$.\nThe set of all $10$ values is $\\{100, 25, 0, 25, 100, 100, 25, 0, 25, 100\\}$.\nThe mean is $\\bar{s} = \\frac{1}{10}\\sum s_i = \\frac{500}{10} = 50$.\n\n**2. Calculation of Correlation Coefficient $D$**\n\nThe Pearson correlation coefficient $D = \\text{corr}(r, s)$ is given by:\n$$D = \\frac{\\sum_{i=1}^{n} (r_i - \\bar{r})(s_i - \\bar{s})}{\\sqrt{\\sum_{i=1}^{n} (r_i - \\bar{r})^2 \\sum_{i=1}^{n} (s_i - \\bar{s})^2}}$$\nSince $\\bar{r}=0$, the formula simplifies to:\n$$D = \\frac{\\sum_{i=1}^{n} r_i(s_i - \\bar{s})}{\\sqrt{\\sum_{i=1}^{n} r_i^2 \\sum_{i=1}^{n} (s_i - \\bar{s})^2}}$$\n\nWe compute the terms:\nNumerator: $\\sum r_i(s_i - \\bar{s}) = 2 \\times [1(100-50) + (-0.5)(25-50) + (-1)(0-50) + (-0.5)(25-50) + 1(100-50)]$.\n$$= 2 \\times [1(50) + (-0.5)(-25) + (-1)(-50) + (-0.5)(-25) + 1(50)]$$\n$$= 2 \\times [50 + 12.5 + 50 + 12.5 + 50] = 2 \\times 175 = 350$$\n\nDenominator term 1: Sum of squared residuals.\n$$\\sum r_i^2 = 2 \\times [1^2 + (-0.5)^2 + (-1)^2 + (-0.5)^2 + 1^2] = 2 \\times [1 + 0.25 + 1 + 0.25 + 1] = 2 \\times 3.5 = 7$$\n\nDenominator term 2: Sum of centered squared $s_i$.\n$$\\sum (s_i - \\bar{s})^2 = 2 \\times [(100-50)^2 + (25-50)^2 + (0-50)^2 + (25-50)^2 + (100-50)^2]$$\n$$= 2 \\times [50^2 + (-25)^2 + (-50)^2 + (-25)^2 + 50^2]$$\n$$= 2 \\times [2500 + 625 + 2500 + 625 + 2500] = 2 \\times 8750 = 17500$$\n\nNow we compute $D$:\n$$D = \\frac{350}{\\sqrt{7 \\times 17500}} = \\frac{350}{\\sqrt{122500}} = \\frac{350}{350} = 1$$\nTo four significant figures, $D = 1.000$.\n\n**3. Interpretation of $D$**\n\nThe coefficient $D$ measures the linear association between the residuals of the linear model, $r_i$, and the squared centered forecast, $s_i = (f_i - \\bar{f})^2$. A non-zero value of $D$ indicates a systematic pattern in the residuals that is related to a quadratic function of the predictor, which is a strong sign of nonlinearity and model misspecification.\n\nIn this case, $D=1.000$. This value represents a perfect positive linear correlation. It indicates that the residuals from the linear fit $y_i = \\hat{a} + \\hat{b} f_i$ are not random noise but instead follow a deterministic relationship with the forecast values. Specifically, the residuals are an exact positive linear function of $(f_i - \\bar{f})^2$.\n\nThe positive sign of $D$ implies that the true relationship between $y$ and $f$ is convex (U-shaped). The linear model systematically underestimates the observed values $y_i$ for forecasts $f_i$ that are far from the mean forecast $\\bar{f}$ (where $(f_i - \\bar{f})^2$ is large, residuals are positive), and overestimates $y_i$ for forecasts near the mean (where $(f_i - \\bar{f})^2$ is small, residuals are negative). This is confirmed by our calculated residuals: $r_i$ is $-1$ at $f_i=\\bar{f}=285$ and is $+1$ at the extreme forecast values of $275$ and $295$.\n\nTherefore, the result $D=1$ is an unambiguous diagnosis of unmodeled quadratic nonlinearity, suggesting that a model of the form $y \\approx \\beta_0 + \\beta_1 f + \\beta_2 f^2$ would provide a more accurate fit to the data.\n\nFinal numerical values rounded to four significant figures are:\n$\\hat{a} = -2.000 \\, \\text{K}$\n$\\hat{b} = 1.000$\n$D = 1.000$",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n-2.000  1.000  1.000\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "While linear models correct for mean bias, they often fail to capture errors across the entire probability distribution or preserve projected trends in a changing climate. This practice introduces Quantile Delta Mapping (QDM), a sophisticated technique designed to address these shortcomings . By working directly with empirical distribution functions, QDM corrects the model's full climatology while explicitly preserving the change signal (the \"delta\") between a model's historical and future periods. This exercise provides invaluable hands-on experience in implementing a state-of-the-art correction method essential for developing credible climate change projections.",
            "id": "4076616",
            "problem": "You are given the task of implementing Quantile Delta Mapping (QDM) for daily near-surface air temperature, with month-specific distributions, to perform bias correction that preserves changes in quantiles from a modelâ€™s historical baseline to its future projection. Quantile Delta Mapping (QDM) is a post-processing approach used in numerical weather prediction and climate modeling to correct bias while preserving projected changes. Consider an additive climate variable: daily air temperature measured in degrees Celsius.\n\nBegin from the following foundational definitions:\n- For any random variable with cumulative distribution function (CDF) denoted by $F$, the quantile function $Q$ is the inverse mapping $Q(p) = F^{-1}(p)$ for $p \\in [0,1]$.\n- Given a finite sample $\\{x_i\\}_{i=1}^n$, the empirical CDF $F_{\\mathrm{emp}}$ and empirical quantile function $Q_{\\mathrm{emp}}$ are defined as piecewise-linear interpolants constructed from the sorted sample and an associated set of plotting positions that lie in $(0,1)$.\n\nYou must:\n1. Implement month-specific empirical CDFs $F_o$ for the observed baseline and $F_m$ for both the model historical baseline and model future, computed from data using interpolation such that, for a sorted sample $\\{y_{(k)}\\}$ with $n$ elements, the plotting positions are $q_k = (k - 0.5)/n$ for $k=1,\\dots,n$.\n2. For each day $t$ in a given month, compute the quantile rank $p_t = F_{m,\\mathrm{future},\\mathrm{month}}(x_{m,\\mathrm{future}}(t))$, then compute the corresponding baseline quantile $Q_{m,\\mathrm{hist},\\mathrm{month}}(p_t)$ and observed baseline quantile $Q_{o,\\mathrm{month}}(p_t)$ using empirical quantile functions. Define the QDM-corrected future value for an additive variable by applying the quantile-preserving additive change at the same quantile rank:\n   - Do not assume any shortcut formula; instead, derive the mapping from the empirical CDF and quantile definitions such that additive changes at quantile $p_t$ between the model future and baseline are preserved when mapping to the observed baseline distribution.\n3. Quantify how the corrected time series preserves projected shifts in mean and variance by computing, for each month:\n   - The model-projected change in mean $\\Delta_{\\mu,\\mathrm{model}} = \\mu(x_{m,\\mathrm{future}}) - \\mu(x_{m,\\mathrm{hist}})$.\n   - The model-projected change in variance $\\Delta_{\\sigma^2,\\mathrm{model}} = \\sigma^2(x_{m,\\mathrm{future}}) - \\sigma^2(x_{m,\\mathrm{hist}})$.\n   - The corrected change in mean $\\Delta_{\\mu,\\mathrm{corr}} = \\mu(x_{\\mathrm{corr},\\mathrm{future}}) - \\mu(x_{o,\\mathrm{baseline}})$.\n   - The corrected change in variance $\\Delta_{\\sigma^2,\\mathrm{corr}} = \\sigma^2(x_{\\mathrm{corr},\\mathrm{future}}) - \\sigma^2(x_{o,\\mathrm{baseline}})$.\n   Use the unbiased sample variance with $n-1$ in the denominator. Then compute absolute preservation errors $e_\\mu = |\\Delta_{\\mu,\\mathrm{corr}} - \\Delta_{\\mu,\\mathrm{model}}|$ and $e_{\\sigma^2} = |\\Delta_{\\sigma^2,\\mathrm{corr}} - \\Delta_{\\sigma^2,\\mathrm{model}}|$.\n\nPhysical units: All temperatures are in degrees Celsius, and all variances are in degrees Celsius squared. Express $e_\\mu$ in degrees Celsius and $e_{\\sigma^2}$ in degrees Celsius squared. Round each reported error to three decimal places.\n\nAngle units: None.\n\nPercentages: None shall be used.\n\nYour program must implement the above using empirical CDFs and quantile functions with linear interpolation. Ensure scientific realism by using month-specific distributions, and additive QDM appropriate for temperature.\n\nTest suite:\n- Use exactly two months: January and July. For each test case, generate $30$ daily values per month for observed baseline, model historical baseline, and model future using independent Normal distributions as specified. Use a fixed random seed of $42$ for reproducibility.\n- For each case, the program must compute the per-month preservation errors in mean and variance and return them in the specified output format.\n\nCase 1 (happy path: warming with variance increase):\n- January:\n  - Observed baseline: $\\mathcal{N}(\\mu=-5, \\sigma=4)$\n  - Model historical baseline: $\\mathcal{N}(\\mu=-3, \\sigma=5)$\n  - Model future: $\\mathcal{N}(\\mu=-1, \\sigma=5.5)$\n- July:\n  - Observed baseline: $\\mathcal{N}(\\mu=25, \\sigma=3)$\n  - Model historical baseline: $\\mathcal{N}(\\mu=27, \\sigma=2)$\n  - Model future: $\\mathcal{N}(\\mu=30, \\sigma=3)$\n\nCase 2 (boundary case: no projected change):\n- January:\n  - Observed baseline: $\\mathcal{N}(\\mu=-6, \\sigma=3.5)$\n  - Model historical baseline: $\\mathcal{N}(\\mu=-4, \\sigma=4)$\n  - Model future: $\\mathcal{N}(\\mu=-4, \\sigma=4)$\n- July:\n  - Observed baseline: $\\mathcal{N}(\\mu=24, \\sigma=3)$\n  - Model historical baseline: $\\mathcal{N}(\\mu=26, \\sigma=2.5)$\n  - Model future: $\\mathcal{N}(\\mu=26, \\sigma=2.5)$\n\nCase 3 (edge case: variance decrease, mixed mean shifts):\n- January:\n  - Observed baseline: $\\mathcal{N}(\\mu=-2, \\sigma=3)$\n  - Model historical baseline: $\\mathcal{N}(\\mu=-1, \\sigma=3.5)$\n  - Model future: $\\mathcal{N}(\\mu=-3, \\sigma=2.8)$\n- July:\n  - Observed baseline: $\\mathcal{N}(\\mu=26, \\sigma=2.5)$\n  - Model historical baseline: $\\mathcal{N}(\\mu=28, \\sigma=3)$\n  - Model future: $\\mathcal{N}(\\mu=29, \\sigma=2)$\n\nOutput specification:\n- For each test case, produce a list of four floats, rounded to three decimals, in the order $[e_{\\mu,\\mathrm{Jan}}, e_{\\sigma^2,\\mathrm{Jan}}, e_{\\mu,\\mathrm{Jul}}, e_{\\sigma^2,\\mathrm{Jul}}]$.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with the three case lists inside. For example: \"[[c1_jan_mean_err,c1_jan_var_err,c1_jul_mean_err,c1_jul_var_err],[c2_...],[c3_...]]\".",
            "solution": "The user-provided problem statement is assessed to be **valid**. It is scientifically grounded in the established principles of climate model post-processing, mathematically well-posed, and objective. All necessary data, definitions, and constraints are provided to construct a unique and verifiable solution.\n\nThe task is to implement and evaluate the Quantile Delta Mapping (QDM) bias correction method for daily air temperature, an additive variable. The evaluation centers on QDM's ability to preserve the model-projected changes in the mean and variance of the distribution from a historical baseline to a future period. The implementation must be based on empirical distributions derived from finite data samples.\n\n**1. Theoretical Foundation: Empirical Distributions**\n\nFor any finite data sample $\\{x_i\\}_{i=1}^n$, we can construct an empirical cumulative distribution function (eCDF), denoted $F_{\\mathrm{emp}}$, and its inverse, the empirical quantile function (eQF), denoted $Q_{\\mathrm{emp}}$. These functions are fundamental to quantile-based correction methods.\n\nThe procedure begins by sorting the sample to get the order statistics $\\{x_{(k)}\\}_{k=1}^n$. Each ordered value $x_{(k)}$ is associated with a non-exceedance probability, or plotting position, $q_k$. As specified, we use the formula:\n$$q_k = \\frac{k - 0.5}{n} \\quad \\text{for } k = 1, \\dots, n$$\nThis establishes a set of points $(x_{(k)}, q_k)$ for the eCDF and $(q_k, x_{(k)})$ for the eQF. The functions $F_{\\mathrm{emp}}(x)$ and $Q_{\\mathrm{emp}}(p)$ are then defined as piecewise-linear interpolants over these points. This approach allows mapping any data value to its corresponding quantile rank and vice-versa.\n\n**2. Derivation of the Quantile Delta Mapping (QDM) Algorithm**\n\nQDM corrects a model's future projection by adjusting it based on the observed climatology, while critically preserving the trend or \"delta\" signal projected by the model. For an additive variable like temperature, this delta is an additive change.\n\nLet the time series for a specific month be:\n- $x_{o}$: Observed baseline data\n- $x_{m,\\mathrm{hist}}$: Model historical baseline data\n- $x_{m,\\mathrm{future}}$: Model future projection data\n\nLet their corresponding empirical distribution functions be $F_o$, $F_{m,\\mathrm{hist}}$, $F_{m,\\mathrm{future}}$ and quantile functions be $Q_o$, $Q_{m,\\mathrm{hist}}$, $Q_{m,\\mathrm{future}}$.\n\nThe QDM correction for a single future value, $x_t \\equiv x_{m,\\mathrm{future}}(t)$, proceeds as follows:\n\n1.  **Find the quantile rank**: The first step is to determine the quantile rank of the future model value $x_t$ within its own monthly distribution.\n    $$p_t = F_{m,\\mathrm{future}}(x_t)$$\n\n2.  **Determine the projected change**: The change signal projected by the model is the difference between its future and historical states. For the specific quantile $p_t$, the change is defined as the difference between the future value $x_t$ (which has quantile $p_t$) and the historical model value at that same quantile, $Q_{m,\\mathrm{hist}}(p_t)$. This defines the projected anomaly for this specific data point:\n    $$\\delta_t = x_{m,\\mathrm{future}}(t) - Q_{m,\\mathrm{hist}}(p_t)$$\n    Note that due to the nature of linear interpolation, $x_{m,\\mathrm{future}}(t)$ is equivalent to $Q_{m,\\mathrm{future}}(p_t) = Q_{m,\\mathrm{future}}(F_{m,\\mathrm{future}}(x_t))$.\n\n3.  **Apply the change to the observed baseline**: The core idea of QDM is to apply this model-projected anomaly $\\delta_t$ to the observed climatology. The value from the observed baseline distribution at the same quantile $p_t$ is $Q_o(p_t)$. The corrected future value, $x_{\\mathrm{corr},\\mathrm{future}}(t)$, is this observed baseline value plus the projected anomaly:\n    $$x_{\\mathrm{corr},\\mathrm{future}}(t) = Q_o(p_t) + \\delta_t$$\n\nSubstituting the expression for $\\delta_t$ yields the final QDM formula for an additive variable:\n$$x_{\\mathrm{corr},\\mathrm{future}}(t) = Q_o(p_t) + \\left( x_{m,\\mathrm{future}}(t) - Q_{m,\\mathrm{hist}}(p_t) \\right)$$\nwhere $p_t = F_{m,\\mathrm{future}}(x_{m,\\mathrm{future}}(t))$. This equation shows that the corrected value is the sum of the bias-corrected value using standard quantile mapping, $Q_o(p_t)$, and the model-projected anomaly relative to the historical model quantile, $x_{m,\\mathrm{future}}(t) - Q_{m,\\mathrm{hist}}(p_t)$.\n\n**3. Implementation and Evaluation Strategy**\n\nThe algorithm is implemented in Python using the `numpy` library. The piecewise-linear eCDFs and eQFs are implemented efficiently using `numpy.interp`. This function takes a set of desired points and interpolates their values based on the sample data (x-coordinates) and their corresponding plotting positions (y-coordinates), or vice versa for the eQF. The computations are vectorized to process the entire time series at once, enhancing performance.\n\nTo evaluate how well QDM preserves projected changes, we compute the following quantities for each month:\n\n1.  **Model-Projected Changes**:\n    -   Change in mean: $\\Delta_{\\mu,\\mathrm{model}} = \\mu(x_{m,\\mathrm{future}}) - \\mu(x_{m,\\mathrm{hist}})$\n    -   Change in variance: $\\Delta_{\\sigma^2,\\mathrm{model}} = \\sigma^2(x_{m,\\mathrm{future}}) - \\sigma^2(x_{m,\\mathrm{hist}})$\n\n2.  **Corrected Changes**: The change in the corrected series relative to the observed baseline.\n    -   Change in mean: $\\Delta_{\\mu,\\mathrm{corr}} = \\mu(x_{\\mathrm{corr},\\mathrm{future}}) - \\mu(x_{o})$\n    -   Change in variance: $\\Delta_{\\sigma^2,\\mathrm{corr}} = \\sigma^2(x_{\\mathrm{corr},\\mathrm{future}}) - \\sigma^2(x_{o})$\n\nAll variances are computed as the unbiased sample variance with $n-1$ degrees of freedom, corresponding to `ddof=1` in `numpy.var`.\n\n3.  **Preservation Errors**: The absolute difference between the corrected changes and the model-projected changes.\n    -   Mean preservation error: $e_\\mu = |\\Delta_{\\mu,\\mathrm{corr}} - \\Delta_{\\mu,\\mathrm{model}}|$\n    -   Variance preservation error: $e_{\\sigma^2} = |\\Delta_{\\sigma^2,\\mathrm{corr}} - \\Delta_{\\sigma^2,\\mathrm{model}}|$\n\nThe program iterates through the specified test cases, generates synthetic data for January and July using a fixed random seed for reproducibility, applies the QDM algorithm, computes the preservation errors, and formats the output as required.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and evaluates Quantile Delta Mapping (QDM) for daily air temperature.\n    \"\"\"\n    \n    # Test cases defined as a list of tuples. Each tuple contains two dictionaries, one for Jan and one for Jul.\n    # Each dictionary specifies the mean (mu) and standard deviation (sigma) for the Normal distributions.\n    test_cases = [\n        # Case 1: Warming with variance increase\n        (\n            {\"month\": \"Jan\", \"obs\": {\"mu\": -5, \"sigma\": 4}, \"hist\": {\"mu\": -3, \"sigma\": 5}, \"future\": {\"mu\": -1, \"sigma\": 5.5}},\n            {\"month\": \"Jul\", \"obs\": {\"mu\": 25, \"sigma\": 3}, \"hist\": {\"mu\": 27, \"sigma\": 2}, \"future\": {\"mu\": 30, \"sigma\": 3}},\n        ),\n        # Case 2: No projected change\n        (\n            {\"month\": \"Jan\", \"obs\": {\"mu\": -6, \"sigma\": 3.5}, \"hist\": {\"mu\": -4, \"sigma\": 4}, \"future\": {\"mu\": -4, \"sigma\": 4}},\n            {\"month\": \"Jul\", \"obs\": {\"mu\": 24, \"sigma\": 3}, \"hist\": {\"mu\": 26, \"sigma\": 2.5}, \"future\": {\"mu\": 26, \"sigma\": 2.5}},\n        ),\n        # Case 3: Variance decrease, mixed mean shifts\n        (\n            {\"month\": \"Jan\", \"obs\": {\"mu\": -2, \"sigma\": 3}, \"hist\": {\"mu\": -1, \"sigma\": 3.5}, \"future\": {\"mu\": -3, \"sigma\": 2.8}},\n            {\"month\": \"Jul\", \"obs\": {\"mu\": 26, \"sigma\": 2.5}, \"hist\": {\"mu\": 28, \"sigma\": 3}, \"future\": {\"mu\": 29, \"sigma\": 2}},\n        ),\n    ]\n\n    # Global parameters\n    rng = np.random.default_rng(42)\n    n_days = 30\n    all_case_results_str = []\n\n    def qdm_correct(obs_base, model_hist, model_future):\n        \"\"\"\n        Applies additive Quantile Delta Mapping.\n        \"\"\"\n        n_obs = len(obs_base)\n        n_hist = len(model_hist)\n        n_future = len(model_future)\n\n        # Sort data to build empirical distributions\n        obs_base_sorted = np.sort(obs_base)\n        model_hist_sorted = np.sort(model_hist)\n        model_future_sorted = np.sort(model_future)\n\n        # Create plotting positions (probabilities) for each dataset\n        p_obs = (np.arange(1, n_obs + 1) - 0.5) / n_obs\n        p_hist = (np.arange(1, n_hist + 1) - 0.5) / n_hist\n        p_future = (np.arange(1, n_future + 1) - 0.5) / n_future\n\n        # Step 1: Find quantile ranks of future values in their own distribution (F_m_future)\n        quantiles_fut = np.interp(model_future, model_future_sorted, p_future, left=p_future[0], right=p_future[-1])\n\n        # Step 2: Find corresponding values in historical model and observed distributions (Q_m_hist and Q_o)\n        model_hist_q = np.interp(quantiles_fut, p_hist, model_hist_sorted, left=model_hist_sorted[0], right=model_hist_sorted[-1])\n        obs_base_q = np.interp(quantiles_fut, p_obs, obs_base_sorted, left=obs_base_sorted[0], right=obs_base_sorted[-1])\n\n        # Step 3: Compute corrected values using the additive QDM formula\n        # x_corr = Q_o(p) + (x_m_future - Q_m_hist(p))\n        corr_future = obs_base_q + (model_future - model_hist_q)\n\n        return corr_future\n\n    def calculate_errors(obs_base, model_hist, model_future, corr_future):\n        \"\"\"\n        Calculates the preservation errors in mean and variance.\n        \"\"\"\n        # Calculate means\n        mu_m_future = np.mean(model_future)\n        mu_m_hist = np.mean(model_hist)\n        mu_o_base = np.mean(obs_base)\n        mu_corr_future = np.mean(corr_future)\n\n        # Calculate unbiased sample variances (ddof=1)\n        var_m_future = np.var(model_future, ddof=1)\n        var_m_hist = np.var(model_hist, ddof=1)\n        var_o_base = np.var(obs_base, ddof=1)\n        var_corr_future = np.var(corr_future, ddof=1)\n\n        # Calculate model-projected and corrected changes (deltas)\n        delta_mu_model = mu_m_future - mu_m_hist\n        delta_var_model = var_m_future - var_m_hist\n        delta_mu_corr = mu_corr_future - mu_o_base\n        delta_var_corr = var_corr_future - var_o_base\n\n        # Calculate absolute preservation errors\n        e_mu = abs(delta_mu_corr - delta_mu_model)\n        e_var = abs(delta_var_corr - delta_var_model)\n\n        return e_mu, e_var\n\n    for case in test_cases:\n        case_results_list = []\n        for month_params in case:\n            # Generate synthetic data for the month\n            obs_data = rng.normal(loc=month_params['obs']['mu'], scale=month_params['obs']['sigma'], size=n_days)\n            hist_data = rng.normal(loc=month_params['hist']['mu'], scale=month_params['hist']['sigma'], size=n_days)\n            future_data = rng.normal(loc=month_params['future']['mu'], scale=month_params['future']['sigma'], size=n_days)\n\n            # Apply QDM to get the corrected future time series\n            corrected_data = qdm_correct(obs_data, hist_data, future_data)\n\n            # Calculate the preservation errors\n            e_mu, e_var = calculate_errors(obs_data, hist_data, future_data, corrected_data)\n\n            # Append rounded results to the list for the current case\n            case_results_list.append(round(e_mu, 3))\n            case_results_list.append(round(e_var, 3))\n        \n        # Format the list of results for this case into the required string format\n        case_str = f\"[{','.join(map(str, case_results_list))}]\"\n        all_case_results_str.append(case_str)\n\n    # Print the final result in the exact specified format\n    print(f\"[{','.join(all_case_results_str)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A statistically perfect correction is useless if it violates the fundamental laws of physics. This final practice serves as a critical bridge between the statistical and physical domains of modeling, focusing on the principle of energy conservation . You will apply a series of post-processing adjustments to surface energy flux components and then validate whether the corrected outputs still close the surface energy budget. This exercise instills the vital discipline of ensuring that post-processed data remains physically plausible, a non-negotiable requirement for any application in numerical weather prediction or climate science.",
            "id": "4076541",
            "problem": "Consider a post-processing and bias correction validation problem in Numerical Weather Prediction (NWP) and climate modeling. After calibration, adjusted surface energy flux components must remain physically plausible and must satisfy the conservation of energy encoded by the Surface Energy Balance (SEB). The SEB at the land-atmosphere interface is given by the fundamental conservation law\n$$\nR_n = H + L + G + S,\n$$\nwhere $R_n$ is net radiation, $H$ is sensible heat flux, $L$ is latent heat flux, $G$ is ground heat flux, and $S$ is energy storage or residual, each expressed in watts per square meter (W m$^{-2}$). This equality must hold at each time step within a specified tolerance.\n\nSuppose raw model outputs provide time series of $R_n$, $H$, $L$, $G$, and $S$ for a single grid cell. Post-processing applies affine corrections to each component:\n$$\nX^{\\mathrm{corr}} = s_X \\left( X^{\\mathrm{raw}} + b_X \\right),\n$$\nwhere $X \\in \\{R_n, H, L, G, S\\}$, $s_X$ is a dimensionless multiplicative scaling, and $b_X$ is an additive bias measured in watts per square meter (W m$^{-2}$). By conservation of energy, physically consistent corrections must satisfy, at each time step $t$,\n$$\n\\epsilon_t \\equiv R_n^{\\mathrm{corr}}(t) - \\left[H^{\\mathrm{corr}}(t) + L^{\\mathrm{corr}}(t) + G^{\\mathrm{corr}}(t) + S^{\\mathrm{corr}}(t)\\right],\n$$\nwith $|\\epsilon_t| \\le \\tau$, where $\\tau$ is a nonnegative tolerance parameter in watts per square meter (W m$^{-2}$). Additionally, physically plausible flux magnitudes must satisfy an absolute bound $|X^{\\mathrm{corr}}(t)| \\le M$ for all components $X$ and time steps $t$. Use $M = 1500$ W m$^{-2}$ unless a test case specifies otherwise.\n\nYour task is to implement a program that, for each provided test case, computes the maximum absolute energy budget residual\n$$\n\\max_t |\\epsilon_t|\n$$\nand determines a boolean violation flag that is true if and only if any of the following holds:\n- There exists a time step $t$ such that $|\\epsilon_t|  \\tau$.\n- There exists a component $X$ and time step $t$ such that $|X^{\\mathrm{corr}}(t)|  M$.\n\nThe outputs must adhere to the following specifications:\n- The maximum absolute residual must be reported as a float in watts per square meter (W m$^{-2}$), rounded to exactly $3$ decimal places.\n- The violation flag must be a boolean.\n- For each test case, the output must be a list of the form $[\\text{max\\_abs\\_residual}, \\text{violation\\_flag}]$.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[[0.123,\\mathrm{True}],[0.000,\\mathrm{False}]]$).\n\nUse the following test suite, which includes a range of realistic scenarios:\n\n- Test Case $1$ (happy path, conservative corrections, physically plausible magnitudes): \n  - Raw fluxes (W m$^{-2}$):\n    - $R_n$: $[400, 300, 100, 50]$\n    - $H$: $[150, 120, 50, 20]$\n    - $L$: $[200, 150, 40, 20]$\n    - $G$: $[30, 20, 5, 5]$\n    - $S$: $[20, 10, 5, 5]$\n  - Corrections:\n    - $s_{R_n} = 1.05$, $b_{R_n} = 2.0$\n    - $s_H = 1.05$, $b_H = 0.5$\n    - $s_L = 1.05$, $b_L = 0.5$\n    - $s_G = 1.05$, $b_G = 0.5$\n    - $s_S = 1.05$, $b_S = 0.5$\n  - Tolerance: $\\tau = 0.5$ W m$^{-2}$, Bound: $M = 1500$ W m$^{-2}$.\n\n- Test Case $2$ (non-conservative additive biases, small closure violation):\n  - Raw fluxes (W m$^{-2}$) identical to Test Case $1$.\n  - Corrections:\n    - $s_{R_n} = 1.0$, $b_{R_n} = 1.0$\n    - $s_H = 1.0$, $b_H = 0.1$\n    - $s_L = 1.0$, $b_L = 0.2$\n    - $s_G = 1.0$, $b_G = 0.3$\n    - $s_S = 1.0$, $b_S = 0.3$\n  - Tolerance: $\\tau = 0.05$ W m$^{-2}$, Bound: $M = 1500$ W m$^{-2}$.\n\n- Test Case $3$ (nighttime conditions with negative latent heat due to dew formation, conservative corrections):\n  - Raw fluxes (W m$^{-2}$):\n    - $R_n$: $[-50, -20]$\n    - $H$: $[-20, -10]$\n    - $L$: $[-25, -5]$\n    - $G$: $[0, -3]$\n    - $S$: $[-5, -2]$\n  - Corrections:\n    - $s_{R_n} = 1.0$, $b_{R_n} = -1.0$\n    - $s_H = 1.0$, $b_H = -0.25$\n    - $s_L = 1.0$, $b_L = -0.25$\n    - $s_G = 1.0$, $b_G = 0.0$\n    - $s_S = 1.0$, $b_S = -0.5$\n  - Tolerance: $\\tau = 0.5$ W m$^{-2}$, Bound: $M = 1500$ W m$^{-2}$.\n\n- Test Case $4$ (physically implausible magnitudes after scaling, closure holds but bound violation):\n  - Raw fluxes (W m$^{-2}$):\n    - $R_n$: $[1000]$\n    - $H$: $[600]$\n    - $L$: $[350]$\n    - $G$: $[30]$\n    - $S$: $[20]$\n  - Corrections:\n    - $s_{R_n} = 2.8$, $b_{R_n} = 0.0$\n    - $s_H = 2.8$, $b_H = 0.0$\n    - $s_L = 2.8$, $b_L = 0.0$\n    - $s_G = 2.8$, $b_G = 0.0$\n    - $s_S = 2.8$, $b_S = 0.0$\n  - Tolerance: $\\tau = 1.0$ W m$^{-2}$, Bound: $M = 1500$ W m$^{-2}$.\n\n- Test Case $5$ (boundary case with zero raw fluxes, residual equals tolerance):\n  - Raw fluxes (W m$^{-2}$):\n    - $R_n$: $[0, 0, 0]$\n    - $H$: $[0, 0, 0]$\n    - $L$: $[0, 0, 0]$\n    - $G$: $[0, 0, 0]$\n    - $S$: $[0, 0, 0]$\n  - Corrections:\n    - $s_{R_n} = 1.0$, $b_{R_n} = 0.2$\n    - $s_H = 1.0$, $b_H = 0.05$\n    - $s_L = 1.0$, $b_L = 0.05$\n    - $s_G = 1.0$, $b_G = 0.0$\n    - $S_S = 1.0$, $b_S = 0.0$\n  - Tolerance: $\\tau = 0.1$ W m$^{-2}$, Bound: $M = 1500$ W m$^{-2}$.\n\nImplement the program to:\n- Apply the specified corrections to each flux component time series.\n- Compute the residual time series $\\epsilon_t$.\n- Compute $\\max_t |\\epsilon_t|$ and round to exactly $3$ decimal places in watts per square meter (W m$^{-2}$).\n- Compute the violation flag based on the above rules.\n- Print a single line containing the list of results for the $5$ test cases, in the exact format $[[\\text{float},\\text{bool}],\\ldots]$.",
            "solution": "The problem statement has been validated and is deemed to be scientifically grounded, well-posed, and objective. It presents a clear and formalizable task rooted in the fundamental physical principle of surface energy conservation, which is a cornerstone of climatology and numerical weather prediction. The problem is self-contained, providing all necessary data and constraints for a unique and verifiable solution.\n\nThe task is to validate post-processed time series of surface energy fluxes against two primary criteria: the conservation of energy and the physical plausibility of flux magnitudes. The algorithmic procedure to accomplish this for each test case is as follows:\n\n1.  **Data Ingestion and Representation**: The raw time series for each flux component $X \\in \\{R_n, H, L, G, S\\}$ are represented as one-dimensional numerical arrays, denoted as $X^{\\mathrm{raw}}$. The correction parameters $s_X$ and $b_X$, and the validation thresholds $\\tau$ and $M$, are stored as scalar values.\n\n2.  **Application of Affine Correction**: For each flux component $X$, a corrected time series $X^{\\mathrm{corr}}$ is computed by applying the affine transformation to each element of the raw time series. The transformation is given by the equation:\n    $$\n    X^{\\mathrm{corr}}(t) = s_X \\left( X^{\\mathrm{raw}}(t) + b_X \\right)\n    $$\n    This operation is performed for all time steps $t$ for each of the five flux components.\n\n3.  **Physical Magnitude Validation**: The first validation check is for physical plausibility. A violation occurs if the absolute magnitude of any corrected flux component at any time step exceeds a predefined bound $M$. Mathematically, a violation is flagged if the following condition is met:\n    $$\n    \\exists X \\in \\{R_n, H, L, G, S\\}, \\exists t \\quad \\text{such that} \\quad |X^{\\mathrm{corr}}(t)|  M\n    $$\n    To implement this, we find the maximum absolute value across all corrected flux components and all time steps. If this maximum exceeds $M$, a `magnitude_violation` flag is set to true.\n\n4.  **Energy Closure Validation**: The second validation check ensures that the corrected fluxes adhere to the principle of energy conservation, as expressed by the Surface Energy Balance (SEB) equation, within a given tolerance $\\tau$. The energy budget residual, $\\epsilon_t$, is calculated for each time step $t$:\n    $$\n    \\epsilon_t = R_n^{\\mathrm{corr}}(t) - \\left( H^{\\mathrm{corr}}(t) + L^{\\mathrm{corr}}(t) + G^{\\mathrm{corr}}(t) + S^{\\mathrm{corr}}(t) \\right)\n    $$\n    This produces a time series of residuals, $\\epsilon$.\n\n5.  **Residual Analysis**: From the residual time series $\\epsilon$, the primary metric for judging the energy closure is the maximum absolute residual over the entire period:\n    $$\n    \\max_t |\\epsilon_t|\n    $$\n    This value is calculated and will be reported as the first part of the output, rounded to exactly $3$ decimal places.\n\n6.  **Closure Violation Check**: The maximum absolute residual is compared against the tolerance $\\tau$. A closure violation occurs if this residual exceeds the tolerance. A `closure_violation` flag is set to true if:\n    $$\n    \\max_t |\\epsilon_t|  \\tau\n    $$\n\n7.  **Final Violation Determination**: The overall violation flag for the test case is determined by the logical OR of the two individual violation flags. The `violation_flag` is true if either the magnitude constraint or the energy closure constraint is violated:\n    $$\n    \\text{violation\\_flag} = (\\text{magnitude\\_violation}) \\lor (\\text{closure\\_violation})\n    $$\n\n8.  **Output Formulation**: The final result for a single test case is a two-element list containing the calculated maximum absolute residual (as a floating-point number) and the final boolean violation flag. The complete program will return a list of these results for all specified test cases. The implementation will utilize the `numpy` library for efficient, vectorized operations on the flux time series data.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the surface energy balance validation problem for a suite of test cases.\n    \"\"\"\n    test_cases = [\n        # Test Case 1 (happy path, conservative corrections)\n        {\n            \"raw_fluxes\": {\n                \"Rn\": np.array([400, 300, 100, 50]),\n                \"H\": np.array([150, 120, 50, 20]),\n                \"L\": np.array([200, 150, 40, 20]),\n                \"G\": np.array([30, 20, 5, 5]),\n                \"S\": np.array([20, 10, 5, 5]),\n            },\n            \"corrections\": {\n                \"s\": {\"Rn\": 1.05, \"H\": 1.05, \"L\": 1.05, \"G\": 1.05, \"S\": 1.05},\n                \"b\": {\"Rn\": 2.0, \"H\": 0.5, \"L\": 0.5, \"G\": 0.5, \"S\": 0.5},\n            },\n            \"params\": {\"tau\": 0.5, \"M\": 1500.0},\n        },\n        # Test Case 2 (non-conservative additive biases)\n        {\n            \"raw_fluxes\": {\n                \"Rn\": np.array([400, 300, 100, 50]),\n                \"H\": np.array([150, 120, 50, 20]),\n                \"L\": np.array([200, 150, 40, 20]),\n                \"G\": np.array([30, 20, 5, 5]),\n                \"S\": np.array([20, 10, 5, 5]),\n            },\n            \"corrections\": {\n                \"s\": {\"Rn\": 1.0, \"H\": 1.0, \"L\": 1.0, \"G\": 1.0, \"S\": 1.0},\n                \"b\": {\"Rn\": 1.0, \"H\": 0.1, \"L\": 0.2, \"G\": 0.3, \"S\": 0.3},\n            },\n            \"params\": {\"tau\": 0.05, \"M\": 1500.0},\n        },\n        # Test Case 3 (nighttime conditions, conservative corrections)\n        {\n            \"raw_fluxes\": {\n                \"Rn\": np.array([-50, -20]),\n                \"H\": np.array([-20, -10]),\n                \"L\": np.array([-25, -5]),\n                \"G\": np.array([0, -3]),\n                \"S\": np.array([-5, -2]),\n            },\n            \"corrections\": {\n                \"s\": {\"Rn\": 1.0, \"H\": 1.0, \"L\": 1.0, \"G\": 1.0, \"S\": 1.0},\n                \"b\": {\"Rn\": -1.0, \"H\": -0.25, \"L\": -0.25, \"G\": 0.0, \"S\": -0.5},\n            },\n            \"params\": {\"tau\": 0.5, \"M\": 1500.0},\n        },\n        # Test Case 4 (implausible magnitudes, bound violation)\n        {\n            \"raw_fluxes\": {\n                \"Rn\": np.array([1000]),\n                \"H\": np.array([600]),\n                \"L\": np.array([350]),\n                \"G\": np.array([30]),\n                \"S\": np.array([20]),\n            },\n            \"corrections\": {\n                \"s\": {\"Rn\": 2.8, \"H\": 2.8, \"L\": 2.8, \"G\": 2.8, \"S\": 2.8},\n                \"b\": {\"Rn\": 0.0, \"H\": 0.0, \"L\": 0.0, \"G\": 0.0, \"S\": 0.0},\n            },\n            \"params\": {\"tau\": 1.0, \"M\": 1500.0},\n        },\n        # Test Case 5 (boundary case, residual equals tolerance)\n        {\n            \"raw_fluxes\": {\n                \"Rn\": np.array([0, 0, 0]),\n                \"H\": np.array([0, 0, 0]),\n                \"L\": np.array([0, 0, 0]),\n                \"G\": np.array([0, 0, 0]),\n                \"S\": np.array([0, 0, 0]),\n            },\n            \"corrections\": {\n                \"s\": {\"Rn\": 1.0, \"H\": 1.0, \"L\": 1.0, \"G\": 1.0, \"S\": 1.0},\n                \"b\": {\"Rn\": 0.2, \"H\": 0.05, \"L\": 0.05, \"G\": 0.0, \"S\": 0.0},\n            },\n            \"params\": {\"tau\": 0.1, \"M\": 1500.0},\n        },\n    ]\n\n    results = []\n    flux_components = [\"Rn\", \"H\", \"L\", \"G\", \"S\"]\n    \n    for case in test_cases:\n        raw_fluxes = case[\"raw_fluxes\"]\n        corrections = case[\"corrections\"]\n        params = case[\"params\"]\n        tau = params[\"tau\"]\n        M = params[\"M\"]\n\n        # Apply affine corrections\n        corr_fluxes = {}\n        for comp in flux_components:\n            s_X = corrections[\"s\"][comp]\n            b_X = corrections[\"b\"][comp]\n            X_raw = raw_fluxes[comp]\n            corr_fluxes[comp] = s_X * (X_raw + b_X)\n\n        # Check for physical magnitude violation\n        max_abs_magnitude = 0.0\n        for comp in flux_components:\n            component_max_abs = np.max(np.abs(corr_fluxes[comp]))\n            if component_max_abs > max_abs_magnitude:\n                max_abs_magnitude = component_max_abs\n        \n        magnitude_violation = max_abs_magnitude > M\n\n        # Calculate energy budget residual\n        outgoing_flux_sum = (corr_fluxes[\"H\"] + corr_fluxes[\"L\"] + \n                             corr_fluxes[\"G\"] + corr_fluxes[\"S\"])\n        epsilon = corr_fluxes[\"Rn\"] - outgoing_flux_sum\n        \n        # Calculate max absolute residual\n        max_abs_residual = np.max(np.abs(epsilon))\n\n        # Check for energy closure violation\n        closure_violation = max_abs_residual > tau\n\n        # Determine final violation flag\n        violation_flag = magnitude_violation or closure_violation\n\n        results.append([max_abs_residual, violation_flag])\n\n    # Format the final output string exactly as specified\n    result_strings = []\n    for res in results:\n        max_res_val, flag_val = res\n        # Format float to 3 decimal places and boolean to string 'True'/'False'\n        res_str = f\"[{max_res_val:.3f},{str(flag_val)}]\"\n        result_strings.append(res_str)\n    \n    final_output = f\"[{','.join(result_strings)}]\"\n    print(final_output)\n\nsolve()\n\n```"
        }
    ]
}