{
    "hands_on_practices": [
        {
            "introduction": "The ergodic hypothesis provides the crucial link between the dynamics of a single system over time and the average behavior of a large collection of systems. To grasp this principle, we begin with a foundational, exactly solvable model: the classical harmonic oscillator. This exercise will guide you through the explicit calculation of both a long-time average and a microcanonical ensemble average, demonstrating their equivalence on a constant-energy surface and revealing why the system is nonetheless non-ergodic on the full phase space .",
            "id": "3807113",
            "problem": "Consider a coarse-grained vibrational mode of an atom in a crystalline solid modeled as a one-dimensional classical harmonic oscillator with mass $m$ and Hooke stiffness $k>0$. The Hamiltonian is $H(x,p)=\\frac{p^{2}}{2m}+\\frac{1}{2}k x^{2}$, and the angular frequency is $\\omega=\\sqrt{\\frac{k}{m}}$. In multiscale materials simulation, time averages from Molecular Dynamics (MD) are often substituted for ensemble averages under the ergodic hypothesis. Let the observable be $A(x,p)=x^{2}$.\n\nStarting from Newton’s second law and Hamiltonian mechanics, without invoking shortcut formulas, do the following:\n\n1. For a trajectory with fixed total energy $E>0$, derive the long-time average of $A$ along the trajectory,\n$$\\overline{A}_{t}=\\lim_{T\\to\\infty}\\frac{1}{T}\\int_{0}^{T}x(t)^{2}\\,dt,$$\nexpressed symbolically in terms of $E$, $k$, $m$, and $\\omega$ where appropriate.\n\n2. Using Liouville measure on phase space and the microcanonical ensemble at energy $E$, derive the ensemble average of $A$ restricted to the energy surface $H(x,p)=E$,\n$$\\langle A\\rangle_{\\text{mc}}(E)=\\frac{\\int_{\\Gamma_{E}}A(x,p)\\,\\mathrm{d}\\mu_{E}(x,p)}{\\int_{\\Gamma_{E}}\\mathrm{d}\\mu_{E}(x,p)},$$\nwhere $\\Gamma_{E}=\\{(x,p)\\in\\mathbb{R}^{2}:H(x,p)=E\\}$ and $\\mathrm{d}\\mu_{E}$ is the invariant measure induced by the Liouville measure on $\\Gamma_{E}$.\n\n3. Explain, using the definitions of ergodicity and invariants of motion, why the harmonic oscillator flow is non-ergodic with respect to Lebesgue measure on the full phase space $\\mathbb{R}^{2}$, and discuss the implication for replacing time averages by ensemble averages when modeling vibrational observables in multiscale materials simulation.\n\nProvide your final answer as the common symbolic value of the two averages in its simplest closed form. You do not need to report units. No rounding is required.",
            "solution": "The user-provided problem is valid as it is scientifically grounded, well-posed, and objective. It represents a standard problem in classical and statistical mechanics.\n\nThe problem is addressed in three parts as requested.\n\n**Part 1: Derivation of the Time Average**\n\nThe motion of the one-dimensional harmonic oscillator is governed by Newton's second law, which for a spring-mass system is $m\\ddot{x} = -kx$. The equation of motion is:\n$$\n\\ddot{x} + \\frac{k}{m}x = 0\n$$\nWith the definition of angular frequency $\\omega = \\sqrt{\\frac{k}{m}}$, this becomes $\\ddot{x} + \\omega^{2}x = 0$. The general solution for the position $x(t)$ as a function of time $t$ is:\n$$\nx(t) = C\\cos(\\omega t + \\phi)\n$$\nwhere $C$ is the amplitude and $\\phi$ is the phase constant, determined by the initial conditions. The corresponding momentum $p(t) = m\\dot{x}(t)$ is:\n$$\np(t) = -m\\omega C\\sin(\\omega t + \\phi)\n$$\nThe total energy $E$ of the oscillator is constant for a given trajectory. We can express $E$ using the Hamiltonian $H(x,p)$:\n$$\nE = H(x(t), p(t)) = \\frac{p(t)^{2}}{2m} + \\frac{1}{2}kx(t)^{2}\n$$\nSubstituting the expressions for $x(t)$ and $p(t)$:\n$$\nE = \\frac{(-m\\omega C\\sin(\\omega t + \\phi))^{2}}{2m} + \\frac{1}{2}k(C\\cos(\\omega t + \\phi))^{2}\n$$\n$$\nE = \\frac{m^{2}\\omega^{2}C^{2}}{2m}\\sin^{2}(\\omega t + \\phi) + \\frac{1}{2}kC^{2}\\cos^{2}(\\omega t + \\phi)\n$$\nUsing the relation $k = m\\omega^{2}$, we simplify the expression:\n$$\nE = \\frac{1}{2}kC^{2}\\sin^{2}(\\omega t + \\phi) + \\frac{1}{2}kC^{2}\\cos^{2}(\\omega t + \\phi) = \\frac{1}{2}kC^{2}(\\sin^{2}(\\omega t + \\phi) + \\cos^{2}(\\omega t + \\phi))\n$$\n$$\nE = \\frac{1}{2}kC^{2}\n$$\nThis allows us to express the square of the amplitude, $C^{2}$, in terms of the total energy $E$ and the stiffness $k$:\n$$\nC^{2} = \\frac{2E}{k}\n$$\nThe observable is $A = x^{2}$, so we are interested in the long-time average of $x(t)^{2}$.\n$$\n\\overline{A}_{t} = \\lim_{T\\to\\infty}\\frac{1}{T}\\int_{0}^{T}x(t)^{2}\\,dt = \\lim_{T\\to\\infty}\\frac{1}{T}\\int_{0}^{T} C^{2}\\cos^{2}(\\omega t + \\phi)\\,dt\n$$\nSince the integrand is a periodic function with period $\\tau = \\frac{2\\pi}{\\omega}$, the long-time average is equal to the average over one period:\n$$\n\\overline{A}_{t} = \\frac{1}{\\tau}\\int_{0}^{\\tau} C^{2}\\cos^{2}(\\omega t + \\phi)\\,dt\n$$\nUsing the trigonometric identity $\\cos^{2}\\theta = \\frac{1}{2}(1 + \\cos(2\\theta))$:\n$$\n\\overline{A}_{t} = \\frac{C^{2}}{\\tau}\\int_{0}^{\\tau} \\frac{1}{2}(1 + \\cos(2(\\omega t + \\phi)))\\,dt\n$$\n$$\n\\overline{A}_{t} = \\frac{C^{2}}{2\\tau}\\left[\\int_{0}^{\\tau}dt + \\int_{0}^{\\tau}\\cos(2\\omega t + 2\\phi)\\,dt\\right]\n$$\nThe first integral is $\\tau$. The second integral is over two full periods of the cosine function, which evaluates to zero:\n$$\n\\int_{0}^{\\tau}\\cos(2\\omega t + 2\\phi)\\,dt = \\left[\\frac{\\sin(2\\omega t + 2\\phi)}{2\\omega}\\right]_{0}^{\\tau} = \\frac{\\sin(2\\omega\\tau + 2\\phi) - \\sin(2\\phi)}{2\\omega} = \\frac{\\sin(4\\pi + 2\\phi) - \\sin(2\\phi)}{2\\omega} = 0\n$$\nTherefore, the time average is:\n$$\n\\overline{A}_{t} = \\frac{C^{2}}{2\\tau}[\\tau + 0] = \\frac{C^{2}}{2}\n$$\nSubstituting the expression for $C^{2}$:\n$$\n\\overline{A}_{t} = \\frac{1}{2}\\left(\\frac{2E}{k}\\right) = \\frac{E}{k}\n$$\n\n**Part 2: Derivation of the Microcanonical Ensemble Average**\n\nThe microcanonical ensemble average of an observable $A$ at a fixed energy $E$ is given by the ratio of phase space integrals over the constant energy surface $\\Gamma_{E}$. A convenient way to compute this is by using the Dirac delta function to constrain the integration to the surface $H(x,p)=E$:\n$$\n\\langle A\\rangle_{\\text{mc}}(E)=\\frac{\\int\\int A(x,p)\\delta(H(x,p)-E)\\,\\mathrm{d}x\\,\\mathrm{d}p}{\\int\\int \\delta(H(x,p)-E)\\,\\mathrm{d}x\\,\\mathrm{d}p}\n$$\nLet's first compute the denominator, which is the density of states $\\Omega(E)$:\n$$\n\\Omega(E) = \\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty} \\delta\\left(\\frac{p^{2}}{2m}+\\frac{1}{2}k x^{2} - E\\right)\\,\\mathrm{d}p\\,\\mathrm{d}x\n$$\nWe integrate over $p$ first. For a fixed $x$, the argument of the delta function has roots at $p = \\pm\\sqrt{2m(E - \\frac{1}{2}kx^{2})}$. These roots are real only if $E - \\frac{1}{2}kx^{2} \\ge 0$, which implies $|x| \\le \\sqrt{2E/k}$.\nUsing the property $\\int g(y)\\delta(f(y))dy = \\sum_{i} \\frac{g(y_i)}{|f'(y_i)|}$ where $y_i$ are the roots of $f(y)$, we have $f(p) = \\frac{p^2}{2m} + \\frac{1}{2}kx^2 - E$ and $f'(p) = \\frac{p}{m}$.\nThe integral over $p$ becomes:\n$$\n\\int_{-\\infty}^{\\infty} \\delta\\left(\\frac{p^{2}}{2m}+\\frac{1}{2}k x^{2} - E\\right)\\,\\mathrm{d}p = \\frac{1}{|p/m|}\\Bigg|_{p=\\sqrt{...}} + \\frac{1}{|-p/m|}\\Bigg|_{p=\\sqrt{...}} = \\frac{2m}{p_0} = \\frac{2m}{\\sqrt{2m(E-\\frac{1}{2}kx^2)}} = \\frac{\\sqrt{2m}}{\\sqrt{E-\\frac{1}{2}kx^2}}\n$$\nNow, we integrate this result over the allowed range of $x$:\n$$\n\\Omega(E) = \\int_{-\\sqrt{2E/k}}^{\\sqrt{2E/k}} \\frac{\\sqrt{2m}}{\\sqrt{E-\\frac{1}{2}kx^2}}\\,\\mathrm{d}x\n$$\nLet's perform a substitution $x = \\sqrt{\\frac{2E}{k}}\\sin\\theta$, so $\\mathrm{d}x = \\sqrt{\\frac{2E}{k}}\\cos\\theta\\,\\mathrm{d}\\theta$. The limits of integration for $\\theta$ become $-\\pi/2$ to $\\pi/2$.\n$$\n\\Omega(E) = \\int_{-\\pi/2}^{\\pi/2} \\frac{\\sqrt{2m}}{\\sqrt{E - \\frac{1}{2}k\\left(\\frac{2E}{k}\\sin^{2}\\theta\\right)}} \\left(\\sqrt{\\frac{2E}{k}}\\cos\\theta\\right)\\,\\mathrm{d}\\theta\n$$\n$$\n\\Omega(E) = \\int_{-\\pi/2}^{\\pi/2} \\frac{\\sqrt{2m}}{\\sqrt{E(1-\\sin^{2}\\theta)}} \\sqrt{\\frac{2E}{k}}\\cos\\theta\\,\\mathrm{d}\\theta = \\int_{-\\pi/2}^{\\pi/2} \\frac{\\sqrt{2m}}{\\sqrt{E}\\cos\\theta} \\sqrt{\\frac{2E}{k}}\\cos\\theta\\,\\mathrm{d}\\theta\n$$\n$$\n\\Omega(E) = \\int_{-\\pi/2}^{\\pi/2} \\sqrt{\\frac{4m}{k}} \\,\\mathrm{d}\\theta = 2\\sqrt{\\frac{m}{k}} [\\theta]_{-\\pi/2}^{\\pi/2} = 2\\sqrt{\\frac{m}{k}}\\pi = \\frac{2\\pi}{\\omega}\n$$\nNext, we compute the numerator for $A=x^{2}$:\n$$\nZ_{A}(E) = \\int\\int x^{2}\\delta(H(x,p)-E)\\,\\mathrm{d}x\\,\\mathrm{d}p = \\int_{-\\sqrt{2E/k}}^{\\sqrt{2E/k}} x^{2} \\left(\\frac{\\sqrt{2m}}{\\sqrt{E-\\frac{1}{2}kx^2}}\\right) \\,\\mathrm{d}x\n$$\nUsing the same substitution $x = \\sqrt{\\frac{2E}{k}}\\sin\\theta$:\n$$\nZ_{A}(E) = \\int_{-\\pi/2}^{\\pi/2} \\left(\\frac{2E}{k}\\sin^{2}\\theta\\right) \\left(\\frac{\\sqrt{2m}}{\\sqrt{E}\\cos\\theta}\\right) \\left(\\sqrt{\\frac{2E}{k}}\\cos\\theta\\right)\\,\\mathrm{d}\\theta\n$$\n$$\nZ_{A}(E) = \\frac{2E}{k} \\sqrt{\\frac{4m}{k}} \\int_{-\\pi/2}^{\\pi/2} \\sin^{2}\\theta\\,\\mathrm{d}\\theta = \\frac{4E}{k\\omega} \\int_{-\\pi/2}^{\\pi/2} \\frac{1-\\cos(2\\theta)}{2}\\,\\mathrm{d}\\theta\n$$\n$$\nZ_{A}(E) = \\frac{2E}{k\\omega} \\left[\\theta - \\frac{\\sin(2\\theta)}{2}\\right]_{-\\pi/2}^{\\pi/2} = \\frac{2E}{k\\omega} \\left(\\frac{\\pi}{2} - (-\\frac{\\pi}{2})\\right) = \\frac{2\\pi E}{k\\omega}\n$$\nThe ensemble average is the ratio of the numerator to the denominator:\n$$\n\\langle A\\rangle_{\\text{mc}}(E) = \\langle x^{2}\\rangle_{\\text{mc}}(E) = \\frac{Z_{A}(E)}{\\Omega(E)} = \\frac{2\\pi E / (k\\omega)}{2\\pi / \\omega} = \\frac{E}{k}\n$$\nThe time average and the microcanonical ensemble average are identical, as expected for an observable in a system that is ergodic on its energy surface.\n\n**Part 3: Non-Ergodicity on Full Phase Space and Implications**\n\nA dynamical system is ergodic on a manifold if and only if the only conserved quantities (invariants of motion) are constant functions on that manifold. The trajectories of an ergodic system are dense in the manifold.\n\nFor the harmonic oscillator, the full phase space is the $2$-dimensional plane $\\mathbb{R}^{2}$. The Hamiltonian $H(x,p) = \\frac{p^{2}}{2m} + \\frac{1}{2}kx^{2}$ is an invariant of motion, as $\\{H,H\\} = 0$. For any initial condition $(x_0, p_0)$, the system evolves along a trajectory where the energy is constant: $H(x,p) = E = H(x_0,p_0)$. These trajectories are ellipses in phase space centered at the origin.\n\nThe function $H(x,p)$ is not constant on the full phase space $\\mathbb{R}^{2}$; it can take any non-negative value. Since there exists a non-trivial invariant of motion, the system is **non-ergodic** with respect to the Lebesgue measure on $\\mathbb{R}^{2}$. A trajectory starting with energy $E_1$ is forever confined to the one-dimensional ellipse defined by $H(x,p)=E_1$. It will never visit regions of phase space with a different energy, $E_2 \\neq E_1$. Consequently, no single trajectory is dense in the two-dimensional phase space. The phase space $\\mathbb{R}^2$ decomposes into a continuous family of disjoint invariant sets, which are the constant-energy ellipses.\n\n**Implication for Multiscale Materials Simulation:**\nThe ergodic hypothesis posits that for a chaotic system, the long-time average of an observable along a single trajectory is equal to the average of that an observable over the statistical ensemble. This principle is what justifies using a single, long Molecular Dynamics (MD) run to compute macroscopic properties, instead of averaging over a vast number of different initial conditions.\n\nThe non-ergodicity of the harmonic oscillator on the full phase space $\\mathbb{R}^2$ highlights a critical subtlety: the choice of ensemble is paramount. A time average is taken along a trajectory with a fixed energy $E$. To be equivalent, the ensemble average must be performed over the correct ensemble, which is the microcanonical (NVE) ensemble restricted to the constant-energy surface $\\Gamma_E$ that contains the trajectory.\n\nThe harmonic oscillator flow, while non-ergodic on $\\mathbb{R}^2$, *is* ergodic on each individual energy surface $\\Gamma_E$ (for $E > 0$). The trajectory is the entire ellipse, so it is trivially dense in itself. This is why our calculations in Part 1 and Part 2 yielded the same result: $\\overline{A}_{t} = \\langle A\\rangle_{\\text{mc}}(E)$.\n\nThe implication for modeling vibrational modes in solids is that replacing a time average from an NVE simulation with an ensemble average is valid, provided the ensemble is the microcanonical one at the same energy $E$. One cannot, for instance, equate a time average from a single trajectory to a canonical (NVT) ensemble average, as the latter involves averaging over all energies, weighted by a Boltzmann factor. The non-ergodicity on the larger phase space forces a correspondence between the dynamical constraints (e.g., constant $E$) and the statistical constraints of the chosen ensemble.",
            "answer": "$$\\boxed{\\frac{E}{k}}$$"
        },
        {
            "introduction": "Moving from a single oscillator to a macroscopic material, why is it valid to use a single, long simulation to represent the entire thermodynamic ensemble? The justification lies in the statistical behavior of large systems, a concept this practice explores through the lens of energy fluctuations. By deriving the relationship between energy variance and heat capacity, and examining its scaling with the number of particles $N$, you will uncover the principle of self-averaging . This demonstrates why, for large $N$, nearly all accessible states are \"typical\" and fluctuations around the mean become negligible, making the time average a reliable estimator of the ensemble average.",
            "id": "3807172",
            "problem": "In atomistic and coarse-grained multiscale materials simulation, ensemble averages are often approximated by time averages computed over long Molecular Dynamics (MD) trajectories, an approach justified by ergodicity and the ergodic hypothesis when fluctuations are self-averaging. Consider a classical canonical ensemble at fixed volume of $N$ identical, non-interacting, monatomic point particles in $3$-dimensional space at absolute temperature $T$ with Boltzmann constant $k_B$. Let the energy be denoted by $E$, and let $Z(\\beta)$ be the canonical partition function with $\\beta = 1/(k_B T)$. Starting only from the canonical probability $P(E_i) \\propto \\exp(-\\beta E_i)$, the definition of the partition function $Z(\\beta)$, and the ensemble average $\\langle \\cdot \\rangle$, first derive from first principles an expression for the energy variance $\\mathrm{Var}(E)$ in terms of $T$ and the constant-volume heat capacity $C_V = \\left(\\partial \\langle E \\rangle/\\partial T\\right)_V$. Then specialize to the classical monatomic ideal gas, where equipartition holds, and determine the closed-form expression for the dimensionless relative variance $\\mathrm{Var}(E)/\\langle E \\rangle^2$ as a function of $N$. Your final answer must be a single simplified symbolic expression in terms of $N$ only. Do not evaluate numerically and do not include units. No rounding is required. Explain clearly how the $N$-scaling connects to self-averaging and typicality in the context of multiscale materials simulation.",
            "solution": "The problem is found to be scientifically grounded, well-posed, and objective. It presents a standard derivation in statistical mechanics and its conceptual implications, which are central to the justification of molecular dynamics simulations. We may therefore proceed with the solution.\n\nThe problem requires the derivation of the energy variance, $\\mathrm{Var}(E)$, for a system in the canonical ensemble, and then the specialization of this result to a classical monatomic ideal gas to find the relative variance. We begin from the fundamental principles of the canonical ensemble.\n\nThe probability $P(E_i)$ of a system being in a microstate $i$ with energy $E_i$ at a fixed volume $V$, particle number $N$, and temperature $T$ is given by the Boltzmann distribution:\n$$P(E_i) = \\frac{\\exp(-\\beta E_i)}{Z(\\beta)}$$\nwhere $\\beta = \\frac{1}{k_B T}$ is the inverse temperature, with $k_B$ being the Boltzmann constant. The canonical partition function, $Z(\\beta)$, serves as the normalization constant and is defined as the sum over all possible microstates $i$:\n$$Z(\\beta) = \\sum_i \\exp(-\\beta E_i)$$\n\nThe ensemble average of any observable $A$ is given by $\\langle A \\rangle = \\sum_i A_i P(E_i)$. The ensemble average of the energy, $\\langle E \\rangle$, is therefore:\n$$\\langle E \\rangle = \\sum_i E_i P(E_i) = \\frac{1}{Z(\\beta)} \\sum_i E_i \\exp(-\\beta E_i)$$\nWe can express the sum in the numerator as a derivative of the partition function with respect to $\\beta$:\n$$\\sum_i E_i \\exp(-\\beta E_i) = -\\frac{\\partial}{\\partial \\beta} \\sum_i \\exp(-\\beta E_i) = -\\frac{\\partial Z(\\beta)}{\\partial \\beta}$$\nSubstituting this into the expression for $\\langle E \\rangle$ yields a compact formula:\n$$\\langle E \\rangle = -\\frac{1}{Z(\\beta)} \\frac{\\partial Z(\\beta)}{\\partial \\beta} = -\\frac{\\partial \\ln(Z(\\beta))}{\\partial \\beta}$$\n\nNext, we derive an expression for the energy variance, $\\mathrm{Var}(E) = \\langle E^2 \\rangle - \\langle E \\rangle^2$. First, we express the mean squared energy, $\\langle E^2 \\rangle$, in a similar fashion:\n$$\\langle E^2 \\rangle = \\sum_i E_i^2 P(E_i) = \\frac{1}{Z(\\beta)} \\sum_i E_i^2 \\exp(-\\beta E_i)$$\nThe sum in the numerator can be related to the second derivative of $Z(\\beta)$:\n$$\\sum_i E_i^2 \\exp(-\\beta E_i) = \\frac{\\partial^2}{\\partial \\beta^2} \\sum_i \\exp(-\\beta E_i) = \\frac{\\partial^2 Z(\\beta)}{\\partial \\beta^2}$$\nThus, $\\langle E^2 \\rangle = \\frac{1}{Z(\\beta)} \\frac{\\partial^2 Z(\\beta)}{\\partial \\beta^2}$. While we could substitute this into the variance formula, a more direct path is to differentiate $\\langle E \\rangle$ with respect to $\\beta$:\n$$\\frac{\\partial \\langle E \\rangle}{\\partial \\beta} = \\frac{\\partial}{\\partial \\beta} \\left( -\\frac{\\partial \\ln(Z(\\beta))}{\\partial \\beta} \\right) = -\\frac{\\partial^2 \\ln(Z(\\beta))}{\\partial \\beta^2}$$\nUsing the quotient rule for the second derivative of a logarithm, $\\frac{d^2}{dx^2} \\ln(f) = \\frac{f f'' - (f')^2}{f^2}$, we get:\n$$\\frac{\\partial \\langle E \\rangle}{\\partial \\beta} = -\\left( \\frac{Z \\frac{\\partial^2 Z}{\\partial \\beta^2} - \\left(\\frac{\\partial Z}{\\partial \\beta}\\right)^2}{Z^2} \\right) = \\frac{1}{Z^2} \\left(\\frac{\\partial Z}{\\partial \\beta}\\right)^2 - \\frac{1}{Z} \\frac{\\partial^2 Z}{\\partial \\beta^2}$$\nRecognizing that $\\langle E \\rangle = -\\frac{1}{Z} \\frac{\\partial Z}{\\partial \\beta}$ and $\\langle E^2 \\rangle = \\frac{1}{Z} \\frac{\\partial^2 Z}{\\partial \\beta^2}$, we can substitute these back:\n$$\\frac{\\partial \\langle E \\rangle}{\\partial \\beta} = \\langle E \\rangle^2 - \\langle E^2 \\rangle = -(\\langle E^2 \\rangle - \\langle E \\rangle^2) = -\\mathrm{Var}(E)$$\nThis gives the fundamental result for energy fluctuations: $\\mathrm{Var}(E) = -\\frac{\\partial \\langle E \\rangle}{\\partial \\beta}$.\n\nThe problem requires the variance in terms of temperature $T$ and the constant-volume heat capacity $C_V$. We perform a change of variables from $\\beta$ to $T$ using the chain rule. Given $\\beta = (k_B T)^{-1}$, we have $\\frac{d\\beta}{dT} = -\\frac{1}{k_B T^2}$. Therefore:\n$$\\frac{\\partial \\langle E \\rangle}{\\partial T} = \\frac{\\partial \\langle E \\rangle}{\\partial \\beta} \\frac{d\\beta}{dT} = \\left(-\\mathrm{Var}(E)\\right) \\left(-\\frac{1}{k_B T^2}\\right) = \\frac{\\mathrm{Var}(E)}{k_B T^2}$$\nBy definition, the constant-volume heat capacity is $C_V = \\left(\\frac{\\partial \\langle E \\rangle}{\\partial T}\\right)_V$. Equating the two expressions for the derivative, we find the relationship between energy variance and heat capacity:\n$$\\mathrm{Var}(E) = k_B T^2 C_V$$\nThis completes the first part of the derivation.\n\nNow, we specialize this result to a classical monatomic ideal gas of $N$ particles in $3$ dimensions. For this system, the energy is purely kinetic. According to the equipartition theorem, each quadratic degree of freedom contributes an average energy of $\\frac{1}{2} k_B T$. Each of the $N$ monatomic particles has $3$ translational degrees of freedom, for a total of $3N$ quadratic degrees of freedom in the system's Hamiltonian. Thus, the average energy is:\n$$\\langle E \\rangle = \\frac{3}{2} N k_B T$$\nThe constant-volume heat capacity is then found by differentiating $\\langle E \\rangle$ with respect to $T$:\n$$C_V = \\left(\\frac{\\partial \\langle E \\rangle}{\\partial T}\\right)_V = \\frac{\\partial}{\\partial T}\\left(\\frac{3}{2} N k_B T\\right) = \\frac{3}{2} N k_B$$\n\nWe can now find the energy variance for this specific system:\n$$\\mathrm{Var}(E) = k_B T^2 C_V = k_B T^2 \\left(\\frac{3}{2} N k_B\\right) = \\frac{3}{2} N (k_B T)^2$$\nThe problem asks for the dimensionless relative variance, $\\frac{\\mathrm{Var}(E)}{\\langle E \\rangle^2}$. We compute this ratio:\n$$\\frac{\\mathrm{Var}(E)}{\\langle E \\rangle^2} = \\frac{\\frac{3}{2} N (k_B T)^2}{\\left(\\frac{3}{2} N k_B T\\right)^2} = \\frac{\\frac{3}{2} N (k_B T)^2}{\\frac{9}{4} N^2 (k_B T)^2} = \\frac{3/2}{9/4} \\frac{N}{N^2} = \\left(\\frac{3}{2}\\right)\\left(\\frac{4}{9}\\right)\\frac{1}{N} = \\frac{2}{3N}$$\n\nThe final result, $\\frac{\\mathrm{Var}(E)}{\\langle E \\rangle^2} = \\frac{2}{3N}$, has profound implications for statistical mechanics and its application in multiscale simulations. The relative standard deviation of the energy, $\\frac{\\sigma_E}{\\langle E \\rangle} = \\sqrt{\\frac{\\mathrm{Var}(E)}{\\langle E \\rangle^2}}$, scales as $N^{-1/2}$:\n$$\\frac{\\sigma_E}{\\langle E \\rangle} = \\sqrt{\\frac{2}{3N}} \\propto \\frac{1}{\\sqrt{N}}$$\nThis scaling demonstrates the principle of **self-averaging**. As the number of particles $N$ in the system approaches the thermodynamic limit ($N \\to \\infty$), the relative magnitude of energy fluctuations becomes infinitesimally small. For a macroscopic system, the energy is practically constant and equal to the ensemble average $\\langle E \\rangle$. This observation also leads to the concept of **typicality**. It implies that in a large system, an overwhelming majority of the accessible microstates have an energy that is indistinguishable from the average energy $\\langle E \\rangle$. Any randomly chosen microstate is almost certain to be a \"typical\" one.\n\nIn the context of multiscale materials simulation, particularly Molecular Dynamics (MD), this result provides a crucial justification. An MD simulation tracks the trajectory of a single system of $N$ particles through phase space. The ergodic hypothesis postulates that for a sufficiently long time, this time average of an observable is equivalent to the ensemble average. The self-averaging property ensures that, for a system with large $N$ (as is typical in simulations), the system's instantaneous energy $E(t)$ will exhibit only small fluctuations around the ensemble mean $\\langle E \\rangle$. Consequently, the time average converges efficiently to a stable value that accurately represents the thermodynamic average. The dominance of typical states means that a single simulation trajectory, provided it is long enough to explore the relevant regions of phase space, is sufficient to sample the macroscopic properties of the system, validating the entire approach of using MD to compute ensemble-averaged quantities like energy, pressure, and heat capacity.",
            "answer": "$$\\boxed{\\frac{2}{3N}}$$"
        },
        {
            "introduction": "Theoretical ergodicity is a necessary but not sufficient condition for a successful simulation; the numerical methods themselves must also preserve the system's statistical properties. This final practice moves from analytical theory to computational reality, investigating how numerical integrators can introduce artifacts that break ergodicity. You will implement a Multiple Time Step (MTS) algorithm and diagnose how choices in its parameters can induce parametric resonance, leading to unphysical energy drift and incorrect sampling of the system's configurational space . This exercise provides critical hands-on experience in identifying and mitigating numerical pathologies that threaten the validity of molecular dynamics simulations.",
            "id": "3807115",
            "problem": "Consider a two-degree-of-freedom linear Hamiltonian system with positions $x_f$ and $x_s$ and conjugate momenta $p_f$ and $p_s$. The fast mode $x_f$ is governed by a stiff spring, and the slow mode $x_s$ is governed by a soft spring. The modes are coupled by a linear spring. Assume unit masses $m_f = 1$ and $m_s = 1$ in reduced units. The potential energy is\n$$\nV(x_f,x_s) = \\frac{1}{2} k_f x_f^2 + \\frac{1}{2} k_s x_s^2 + \\frac{1}{2} k_c (x_f - x_s)^2,\n$$\nwith $k_f > 0$, $k_s > 0$, and $k_c \\ge 0$. The total energy is\n$$\nE(x_f,x_s,p_f,p_s) = \\frac{1}{2} \\frac{p_f^2}{m_f} + \\frac{1}{2} \\frac{p_s^2}{m_s} + V(x_f,x_s).\n$$\nThe exact time evolution is determined by Hamilton’s equations, equivalently Newton’s second law, given forces $F_f = -\\partial V/\\partial x_f$ and $F_s = -\\partial V/\\partial x_s$.\n\nYou will simulate this system using a two-level Multiple Time Step (MTS) scheme, specifically the Reference System Propagator Algorithm (RESPA), where the force is split into a fast component and a slow component:\n- Define the fast potential $V_{\\text{fast}}(x_f,x_s) = \\frac{1}{2} k_f x_f^2$, and the slow potential $V_{\\text{slow}}(x_f,x_s) = \\frac{1}{2} k_s x_s^2 + \\frac{1}{2} k_c (x_f - x_s)^2$.\n- Then $F_{\\text{fast}}(x_f,x_s) = -\\nabla V_{\\text{fast}}$ and $F_{\\text{slow}}(x_f,x_s) = -\\nabla V_{\\text{slow}}$.\n\nA single two-level RESPA outer step of size $\\Delta t$ with $n$ inner fast substeps proceeds as:\n1. Half slow kick: $p \\leftarrow p + \\frac{\\Delta t}{2} F_{\\text{slow}}(q)$.\n2. For $i = 1,2,\\dots,n$:\n   - Half fast kick: $p \\leftarrow p + \\frac{\\Delta t}{2n} F_{\\text{fast}}(q)$.\n   - Drift: $q \\leftarrow q + \\frac{\\Delta t}{n} M^{-1} p$, where $M = \\operatorname{diag}(m_f,m_s)$.\n   - Half fast kick: $p \\leftarrow p + \\frac{\\Delta t}{2n} F_{\\text{fast}}(q)$.\n3. Half slow kick: $p \\leftarrow p + \\frac{\\Delta t}{2} F_{\\text{slow}}(q)$.\n\nTo assess ergodicity and detect resonances induced by the MTS splitting and step sizes, define these metrics:\n- Energy drift rate: Fit a straight line to total energy $E(t)$ sampled at outer step boundaries over the simulation window $[0,T]$; let the slope be $s$. Compute the normalized slope $s_{\\text{rel}} = s / \\bar{E}$, where $\\bar{E}$ is the mean energy over the window. A positive $s_{\\text{rel}}$ significantly larger than zero indicates parametric resonance and systematic energy growth.\n- Mode energy partition: Let $V_{\\text{stiff}}(t) = \\frac{1}{2} k_f x_f(t)^2$ and $V_{\\text{tot}}(t) = V(x_f(t),x_s(t))$. Define the time-averaged ratio $R = \\langle V_{\\text{stiff}} / V_{\\text{tot}} \\rangle_t$ over $[0,T]$.\n\nTo evaluate non-ergodicity due to the integrator, compare the Multiple Time Step (MTS) trajectory’s $R$ against a high-fidelity baseline trajectory obtained using a single-level velocity Verlet integrator with a sufficiently small step $\\delta t_{\\text{ref}}$ on the full force $F = F_{\\text{fast}} + F_{\\text{slow}}$. A significant discrepancy in $R$ suggests the MTS scheme fails to reproduce correct time-averaged statistics, indicative of non-ergodicity.\n\nImplement the following procedure for each test case:\n1. Initialize $x_f(0) = 0.5$, $x_s(0) = -0.3$, $p_f(0) = 0$, $p_s(0) = 0$.\n2. Simulate the baseline (single-level velocity Verlet) for time $T = 50$ with step $\\delta t_{\\text{ref}} = \\min\\left(\\frac{1}{50\\sqrt{k_f}}, \\frac{\\Delta t}{50}\\right)$, using full forces $F = F_{\\text{fast}} + F_{\\text{slow}}$, and compute $R_{\\text{ref}}$.\n3. Simulate the MTS RESPA scheme for time $T = 50$ with outer step $\\Delta t$ and $n$ inner substeps, compute the energy time series $E(t)$ at outer boundaries, estimate $s_{\\text{rel}}$ via linear regression, and compute $R_{\\text{mts}}$.\n4. Declare resonance detection as a boolean $b_{\\text{res}}$ where $b_{\\text{res}}$ is $1$ if $s_{\\text{rel}} > 10^{-3}$ and $0$ otherwise.\n5. Declare non-ergodicity detection as a boolean $b_{\\text{ne}}$ where $b_{\\text{ne}}$ is $1$ if $\\left|R_{\\text{mts}} - R_{\\text{ref}}\\right| > 0.05$ and $0$ otherwise.\n6. Suggest a mitigation strategy encoded as an integer $m$ based on the following mapping:\n   - $m = 0$: no action needed.\n   - $m = 1$: reduce the outer slow step size $\\Delta t$ such that $\\Delta t  \\frac{1}{10} \\frac{2\\pi}{\\sqrt{k_f}}$.\n   - $m = 2$: move the coupling term (the $\\frac{1}{2} k_c (x_f - x_s)^2$ part) into the fast force split.\n   - $m = 3$: add a weak stochastic thermostat (for example, a small Langevin friction) targeting the fast mode to disrupt resonant energy pumping.\n   Use the following decision rule: if $b_{\\text{res}} = 1$ and $k_c > 0$ and $\\Delta t \\sqrt{k_f} > \\frac{\\pi}{2}$ then $m = 1$; else if $b_{\\text{res}} = 1$ and $k_c > 0$ then $m = 2$; else if $b_{\\text{ne}} = 1$ then $m = 2$; else $m = 0$. If both $b_{\\text{res}} = 1$ and $b_{\\text{ne}} = 1$, set $m = 3$.\n\nUse the following test suite of parameters $(k_f,k_s,k_c,\\Delta t,n)$ in reduced units:\n- Case $1$: $(1000, 1, 10, 0.02, 10)$.\n- Case $2$: $(1000, 1, 10, 0.095, 5)$.\n- Case $3$: $(1000, 1, 0, 0.095, 5)$.\n- Case $4$: $(1000, 1, 200, 0.06, 5)$.\n- Case $5$: $(10, 10, 1, 0.3, 5)$.\n\nFor each case, your program must output a list $[b_{\\text{res}}, b_{\\text{ne}}, m]$. Your program should produce a single line of output containing the results for all five test cases as a comma-separated list enclosed in square brackets, for example $[r_1,r_2,r_3,r_4,r_5]$, where each $r_i$ is the list $[b_{\\text{res}},b_{\\text{ne}},m]$ as defined above. Angles are not used in this problem, and there are no physical units in the outputs; all simulations use reduced units.",
            "solution": "The user has provided a problem statement that is scientifically grounded, well-posed, and objective. It describes a computational experiment to investigate numerical artifacts in a multiscale molecular dynamics simulation. The system is a standard two-degree-of-freedom coupled harmonic oscillator, a canonical model for studying such phenomena. The numerical methods, specifically the velocity Verlet and the Reference System Propagator Algorithm (RESPA), are standard techniques in the field. The analysis metrics—energy drift rate and time-averaged potential energy partitioning—are well-defined and appropriate for detecting the specified artifacts (parametric resonance and non-ergodic sampling). All parameters, initial conditions, and decision logic are explicitly provided, rendering the problem unambiguous and solvable. Therefore, the problem is deemed valid.\n\nThe solution proceeds as follows: First, we formalize the equations of motion and the force splitting. Second, we detail the implementation of the two numerical integrators: a single-step velocity Verlet for a high-fidelity baseline and a two-level RESPA for the multiple time-step (MTS) simulation. Third, we describe the procedure to compute the diagnostic metrics and apply the specified decision rules for each test case.\n\nThe Hamiltonian of the system is given by:\n$$\nH(x_f, x_s, p_f, p_s) = \\frac{p_f^2}{2m_f} + \\frac{p_s^2}{2m_s} + \\frac{1}{2} k_f x_f^2 + \\frac{1}{2} k_s x_s^2 + \\frac{1}{2} k_c (x_f - x_s)^2\n$$\nWith unit masses $m_f = m_s = 1$, the kinetic energy is $T(p_f, p_s) = \\frac{1}{2}(p_f^2 + p_s^2)$. The potential energy is $V(x_f, x_s)$. The forces on the fast and slow degrees of freedom are derived from the potential:\n$$\nF_f = -\\frac{\\partial V}{\\partial x_f} = -k_f x_f - k_c(x_f - x_s)\n$$\n$$\nF_s = -\\frac{\\partial V}{\\partial x_s} = -k_s x_s + k_c(x_f - x_s)\n$$\nIn vector form, let $q = (x_f, x_s)$ and $F = (F_f, F_s)$. Newton's equations are $\\ddot{q} = F$.\n\nThe RESPA method relies on splitting the force $F$ into a rapidly varying component $F_{\\text{fast}}$ and a slowly varying component $F_{\\text{slow}}$. The provided split is:\n$$\nF_{\\text{fast}}(q) = \\left( -k_f x_f, 0 \\right)\n$$\n$$\nF_{\\text{slow}}(q) = \\left( -k_c(x_f - x_s), -k_s x_s + k_c(x_f - x_s) \\right)\n$$\nsuch that $F(q) = F_{\\text{fast}}(q) + F_{\\text{slow}}(q)$.\n\nWe will implement two simulation procedures for each test case $(k_f, k_s, k_c, \\Delta t, n)$.\n\n**1. Baseline Simulation (Single-Step Velocity Verlet)**\nThis simulation provides a high-fidelity reference trajectory. The integrator uses a small time step $\\delta t_{\\text{ref}} = \\min\\left(\\frac{1}{50\\sqrt{k_f}}, \\frac{\\Delta t}{50}\\right)$ and the full force $F$. The velocity Verlet algorithm proceeds as:\n1. Half kick: $p(t + \\frac{\\delta t_{\\text{ref}}}{2}) = p(t) + \\frac{\\delta t_{\\text{ref}}}{2} F(q(t))$.\n2. Drift: $q(t + \\delta t_{\\text{ref}}) = q(t) + \\delta t_{\\text{ref}} M^{-1} p(t + \\frac{\\delta t_{\\text{ref}}}{2})$.\n3. Compute new force: $F(q(t+\\delta t_{\\text{ref}}))$.\n4. Half kick: $p(t + \\delta t_{\\text{ref}}) = p(t + \\frac{\\delta t_{\\text{ref}}}{2}) + \\frac{\\delta t_{\\text{ref}}}{2} F(q(t + \\delta t_{\\text{ref}}))$.\n\nThe simulation runs from $t=0$ to $t=T=50$. At each step, we calculate the stiff potential energy $V_{\\text{stiff}} = \\frac{1}{2} k_f x_f^2$ and the total potential energy $V_{\\text{tot}} = V(x_f, x_s)$. The reference ratio $R_{\\text{ref}}$ is the time average of the ratio $V_{\\text{stiff}}/V_{\\text{tot}}$ over all steps of the simulation.\n\n**2. MTS Simulation (RESPA)**\nThis simulation uses the RESPA integrator with a large outer time step $\\Delta t$ and $n$ inner substeps. One full step from $t$ to $t+\\Delta t$ is:\n1. Propagate momenta with the slow force for half a step: $p \\leftarrow p + \\frac{\\Delta t}{2} F_{\\text{slow}}(q)$.\n2. Perform $n$ inner loop steps. For each inner step $i = 1, \\dots, n$:\n   a. Propagate momenta with the fast force for half an inner step: $p \\leftarrow p + \\frac{\\Delta t}{2n} F_{\\text{fast}}(q)$.\n   b. Propagate positions for a full inner step: $q \\leftarrow q + \\frac{\\Delta t}{n} M^{-1} p$.\n   c. Propagate momenta with the fast force for the second half of the inner step: $p \\leftarrow p + \\frac{\\Delta t}{2n} F_{\\text{fast}}(q)$.\n3. Propagate momenta with the slow force for the final half step: $p \\leftarrow p + \\frac{\\Delta t}{2} F_{\\text{slow}}(q)$.\n\nThe simulation runs from $t=0$ to $t=T=50$. At the end of each outer step (at times $t_k = k \\Delta t$), we record the total energy $E(t_k)$ and the quantities $V_{\\text{stiff}}(t_k)$ and $V_{\\text{tot}}(t_k)$.\n\n**3. Analysis and Diagnostics**\nAfter the MTS simulation, we perform the following analysis:\n- **Energy Drift Rate ($s_{\\text{rel}}$)**: We perform a linear regression on the collected energy data $\\{ (t_k, E(t_k)) \\}$ to find the slope $s$. The normalized slope is $s_{\\text{rel}} = s / \\bar{E}$, where $\\bar{E}$ is the average energy over the trajectory. Resonance is detected ($b_{\\text{res}}=1$) if $s_{\\text{rel}} > 10^{-3}$. Parametric resonance occurs when the outer time step $\\Delta t$ couples with a high frequency of the system (here, $\\omega_f = \\sqrt{k_f}$), leading to systematic energy injection. This is often observed when $n_f \\Delta t \\approx T_f$ for some small integer $n_f$, where $T_f = 2\\pi/\\omega_f$ is the period of the fast motion. The condition $\\Delta t \\sqrt{k_f} > \\pi/2$ checks for this.\n- **Mode Energy Partition ($R_{\\text{mts}}$)**: We compute the time-averaged ratio $R_{\\text{mts}} = \\langle V_{\\text{stiff}} / V_{\\text{tot}} \\rangle_t$ using the values sampled at the outer step boundaries. A significant deviation from the reference value, $|R_{\\text{mts}} - R_{\\text{ref}}| > 0.05$, indicates that the MTS integrator is not sampling the phase space correctly, a sign of non-ergodicity ($b_{\\text{ne}}=1$). This can happen if the integrator artificially traps the system in a region of phase space or alters the energy flow between modes.\n- **Mitigation Strategy ($m$)**: Based on the values of $b_{\\text{res}}$ and $b_{\\text{ne}}$, a mitigation strategy is suggested using the provided decision logic. The strategies correspond to common fixes for integrator artifacts: reducing the step size to move away from the resonance condition ($m=1$), improving the force split to make $F_{\\text{slow}}$ smoother ($m=2$), or introducing dissipation to break the coherent energy pumping ($m=3$).\n\nThe Python code below implements this entire procedure for the five given test cases.",
            "answer": "```python\nimport numpy as np\nfrom scipy import stats\n\ndef solve():\n    \"\"\"\n    Solves the multiscale simulation problem by evaluating RESPA integrator artifacts.\n    \"\"\"\n    test_cases = [\n        # (k_f, k_s, k_c, Delta_t, n)\n        (1000, 1, 10, 0.02, 10),\n        (1000, 1, 10, 0.095, 5),\n        (1000, 1, 0, 0.095, 5),\n        (1000, 1, 200, 0.06, 5),\n        (10, 10, 1, 0.3, 5),\n    ]\n\n    results = []\n\n    # System parameters\n    masses = np.array([1.0, 1.0])\n    q0 = np.array([0.5, -0.3])\n    p0 = np.array([0.0, 0.0])\n    T_sim = 50.0\n\n    # Helper functions for forces and energies\n    def get_full_force(q, kf, ks, kc):\n        f = np.zeros(2)\n        f[0] = -kf * q[0] - kc * (q[0] - q[1])\n        f[1] = -ks * q[1] + kc * (q[0] - q[1])\n        return f\n\n    def get_fast_force(q, kf, ks, kc):\n        f = np.zeros(2)\n        f[0] = -kf * q[0]\n        return f\n\n    def get_slow_force(q, kf, ks, kc):\n        f = np.zeros(2)\n        f[0] = -kc * (q[0] - q[1])\n        f[1] = -ks * q[1] + kc * (q[0] - q[1])\n        return f\n    \n    def get_potential_energy(q, kf, ks, kc):\n        return 0.5 * kf * q[0]**2 + 0.5 * ks * q[1]**2 + 0.5 * kc * (q[0] - q[1])**2\n\n    def get_kinetic_energy(p):\n        return 0.5 * np.sum(p**2 / masses)\n\n    def get_total_energy(q, p, kf, ks, kc):\n        return get_potential_energy(q, kf, ks, kc) + get_kinetic_energy(p)\n        \n    def get_v_stiff(q, kf):\n        return 0.5 * kf * q[0]**2\n\n    for case in test_cases:\n        kf, ks, kc, delta_t, n = case\n\n        # --- Baseline Simulation (Single-Step Velocity Verlet) ---\n        dt_ref = min(1.0 / (50.0 * np.sqrt(kf)), delta_t / 50.0)\n        num_steps_ref = int(T_sim / dt_ref)\n        \n        q_ref = q0.copy()\n        p_ref = p0.copy()\n        \n        v_ratios_ref = []\n\n        # Initial force calculation\n        force_ref = get_full_force(q_ref, kf, ks, kc)\n        for _ in range(num_steps_ref):\n            # Verlet step\n            p_ref += 0.5 * dt_ref * force_ref\n            q_ref += dt_ref * p_ref / masses\n            force_ref = get_full_force(q_ref, kf, ks, kc)\n            p_ref += 0.5 * dt_ref * force_ref\n\n            v_stiff = get_v_stiff(q_ref, kf)\n            v_total = get_potential_energy(q_ref, kf, ks, kc)\n            if v_total  1e-9: # Avoid division by zero\n                v_ratios_ref.append(v_stiff / v_total)\n\n        R_ref = np.mean(v_ratios_ref) if v_ratios_ref else 0.0\n\n        # --- MTS Simulation (RESPA) ---\n        num_outer_steps = int(T_sim / delta_t)\n        \n        q_mts = q0.copy()\n        p_mts = p0.copy()\n\n        energies_mts = []\n        v_ratios_mts = []\n        times_mts = []\n        \n        for i in range(num_outer_steps):\n            # Record state at step beginning\n            times_mts.append(i * delta_t)\n            energies_mts.append(get_total_energy(q_mts, p_mts, kf, ks, kc))\n\n            v_stiff = get_v_stiff(q_mts, kf)\n            v_total = get_potential_energy(q_mts, kf, ks, kc)\n            if v_total  1e-9:\n                v_ratios_mts.append(v_stiff / v_total)\n\n            # RESPA step\n            # 1. Half slow kick\n            f_slow = get_slow_force(q_mts, kf, ks, kc)\n            p_mts += 0.5 * delta_t * f_slow\n            \n            # 2. Inner loop\n            dt_inner = delta_t / n\n            for _ in range(n):\n                f_fast = get_fast_force(q_mts, kf, ks, kc)\n                p_mts += 0.5 * dt_inner * f_fast\n                q_mts += dt_inner * p_mts / masses\n                f_fast = get_fast_force(q_mts, kf, ks, kc)\n                p_mts += 0.5 * dt_inner * f_fast\n\n            # 3. Half slow kick\n            f_slow = get_slow_force(q_mts, kf, ks, kc)\n            p_mts += 0.5 * delta_t * f_slow\n\n        R_mts = np.mean(v_ratios_mts) if v_ratios_mts else 0.0\n\n        # --- Analysis ---\n        # Energy drift\n        if len(energies_mts)  1:\n            slope, _, _, _, _ = stats.linregress(times_mts, energies_mts)\n            mean_energy = np.mean(energies_mts)\n            s_rel = slope / mean_energy if abs(mean_energy)  1e-9 else 0.0\n        else:\n            s_rel = 0.0\n        \n        b_res = 1 if s_rel  1e-3 else 0\n        \n        # Non-ergodicity\n        b_ne = 1 if abs(R_mts - R_ref)  0.05 else 0\n\n        # Mitigation strategy\n        m = 0\n        if b_res == 1 and b_ne == 1:\n            m = 3\n        elif b_res == 1 and kc  0 and delta_t * np.sqrt(kf)  np.pi / 2:\n            m = 1\n        elif b_res == 1 and kc  0:\n            m = 2\n        elif b_ne == 1:\n            m = 2\n        else:\n            m = 0\n            \n        results.append([b_res, b_ne, m])\n    \n    # Format output without spaces in sublists\n    result_strings = []\n    for res in results:\n        result_strings.append(f\"[{res[0]},{res[1]},{res[2]}]\")\n    \n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n```"
        }
    ]
}