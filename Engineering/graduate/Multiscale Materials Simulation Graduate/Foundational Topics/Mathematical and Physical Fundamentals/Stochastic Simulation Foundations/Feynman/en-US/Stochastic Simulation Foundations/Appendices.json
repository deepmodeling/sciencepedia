{
    "hands_on_practices": [
        {
            "introduction": "How do we simulate systems where rare, discrete events drive the evolution over time? This practice takes you back to first principles to derive the celebrated Gillespie algorithm. By leveraging the fundamental properties of independent Poisson processes, you will construct a rigorous method for determining both the time to the next event and its identity, which forms the cornerstone of kinetic Monte Carlo and other stochastic simulation methods.",
            "id": "3841418",
            "problem": "Consider a mesoscopic model of a multiscale materials system in which rare events on a lattice (such as adsorption, desorption, diffusion hops, or bond rearrangements) are represented as reaction channels indexed by $i \\in \\{1, \\dots, M\\}$. Assume that, at a given state of the system, each channel $i$ is an independent Poisson process with constant rate (also called propensity) $r_i > 0$. The evolution is modeled as a continuous-time Markov jump process. You are tasked with deriving a sampling scheme for the next-event time increment and identifying which channel fires next under the foundations of stochastic simulation.\n\nStarting only from the following fundamental bases:\n- For a homogeneous Poisson process with rate $r>0$, the waiting time to the next event is exponentially distributed with probability density function $f(t) = r \\exp(-r t)$ for $t \\geq 0$, and the process is memoryless, meaning $P(T > s + t \\,|\\, T > s) = P(T > t)$ for all $s, t \\geq 0$.\n- Independent Poisson processes superpose to give a Poisson process with rate equal to the sum of the rates.\n- The inverse transformation method: if $U$ is uniformly distributed on $(0,1)$ and $F$ is a continuous cumulative distribution function, then $F^{-1}(U)$ is a sample from the distribution with cumulative distribution function $F$.\n\nUsing only these bases and fundamental probability rules, derive:\n1. The closed-form expression for the waiting time increment $\\Delta t$ in terms of a single sample $u$ drawn from the uniform distribution on $(0,1)$ and the rates $\\{r_i\\}_{i=1}^{M}$.\n2. The closed-form expression for the probability that the next event is channel $k \\in \\{1, \\dots, M\\}$, expressed in terms of the rates $\\{r_i\\}_{i=1}^{M}$.\n\nYour derivation must invoke the memoryless property of exponential waiting times and the independence of channels. Express your final results in closed form. No numerical evaluation is required, and no intermediate shortcut formulas are permitted in the problem statement. Provide the final expressions without units.",
            "solution": "The problem statement is evaluated as scientifically sound, well-posed, objective, and self-contained, thus meeting all criteria for a valid problem. It requests the derivation of the core components of the Gillespie Stochastic Simulation Algorithm (SSA) from first principles, a fundamental task in the study of stochastic processes in materials science and related fields.\n\nLet the set of $M$ reaction channels be indexed by $i \\in \\{1, \\dots, M\\}$. Each channel $i$ is modeled as an independent homogeneous Poisson process with a constant rate (propensity) $r_i > 0$. Let $T_i$ be the random variable representing the waiting time until the next event occurs in channel $i$.\n\nAccording to the first provided basis, the process is memoryless, and the waiting time for a single channel $i$ follows an exponential distribution. The probability density function (PDF) for $T_i$ is $p_i(t) = r_i \\exp(-r_i t)$ for $t \\ge 0$. The memoryless property is crucial as it justifies modeling the time to the next event with this distribution starting from any arbitrary state of the system, without regard to its history. The cumulative distribution function (CDF) for $T_i$ is $F_i(t) = P(T_i \\le t) = \\int_{0}^{t} r_i \\exp(-r_i \\tau) d\\tau = 1 - \\exp(-r_i t)$. Consequently, the survival function is $S_i(t) = P(T_i > t) = 1 - F_i(t) = \\exp(-r_i t)$.\n\n**1. Derivation of the waiting time increment $\\Delta t$**\n\nThe next event in the overall system is the a priori unknown event that occurs first among all possible channels. Therefore, the waiting time until the next system event, denoted by $\\Delta t$, is the minimum of the individual waiting times of all channels:\n$$\n\\Delta t = \\min(T_1, T_2, \\dots, T_M)\n$$\nTo find the distribution of $\\Delta t$, we first derive its survival function, $S_{\\Delta t}(t) = P(\\Delta t > t)$. The condition $\\Delta t > t$ is met if and only if all individual waiting times $T_i$ are greater than $t$.\n$$\nP(\\Delta t > t) = P(\\min(T_1, \\dots, T_M) > t) = P(T_1 > t, T_2 > t, \\dots, T_M > t)\n$$\nSince the channels are stated to be independent processes, the joint probability is the product of the individual probabilities:\n$$\nP(\\Delta t > t) = \\prod_{i=1}^{M} P(T_i > t)\n$$\nSubstituting the survival function for each exponential distribution, $P(T_i > t) = \\exp(-r_i t)$:\n$$\nP(\\Delta t > t) = \\prod_{i=1}^{M} \\exp(-r_i t) = \\exp\\left(-\\sum_{i=1}^{M} r_i t\\right)\n$$\nLet the total rate be $R = \\sum_{i=1}^{M} r_i$. The survival function for $\\Delta t$ is $S_{\\Delta t}(t) = \\exp(-R t)$. This result demonstrates the second provided basis: the superposition of independent Poisson processes yields a new Poisson process with a rate equal to the sum of the individual rates. The random variable $\\Delta t$ is thus exponentially distributed with rate $R$.\n\nThe CDF of $\\Delta t$ is $F_{\\Delta t}(t) = 1 - S_{\\Delta t}(t) = 1 - \\exp(-R t)$.\n\nTo generate a sample of $\\Delta t$, we use the inverse transformation method, as specified in the third basis. Let $u$ be a random number drawn from the uniform distribution on $(0,1)$, denoted $U \\sim (0,1)$. We set $F_{\\Delta t}(\\Delta t) = u$:\n$$\nu = 1 - \\exp(-R \\Delta t)\n$$\nWe now solve for $\\Delta t$:\n$$\n\\exp(-R \\Delta t) = 1 - u\n$$\n$$\n-R \\Delta t = \\ln(1 - u)\n$$\n$$\n\\Delta t = -\\frac{1}{R} \\ln(1 - u)\n$$\nSince $u$ is a random variable uniformly distributed on $(0,1)$, the variable $u' = 1-u$ is also uniformly distributed on $(0,1)$. Therefore, we can simplify the expression by replacing $1-u$ with a new uniform random variate, which we can also call $u$ without loss of generality.\n$$\n\\Delta t = -\\frac{1}{R} \\ln(u) = -\\frac{\\ln(u)}{\\sum_{i=1}^{M} r_i}\n$$\nThis is the closed-form expression for the waiting time increment.\n\n**2. Derivation of the probability of channel $k$ firing next**\n\nWe wish to find the probability, $P_k$, that a specific channel $k \\in \\{1, \\dots, M\\}$ is the one that fires next. This is the probability that its waiting time, $T_k$, is the minimum among all waiting times:\n$$\nP_k = P(T_k = \\min(T_1, \\dots, T_M)) = P(T_k \\le T_i \\text{ for all } i \\in \\{1, \\dots, M\\})\n$$\nTo calculate this, we can condition on the value of $T_k$ and integrate over all possible values of its waiting time, $t$. The overall probability is the integral of the joint probability density function over the region where $t_k \\le t_i$ for all $i$. A more direct way is to use the law of total probability:\n$$\nP_k = \\int_{0}^{\\infty} P(T_k \\le T_i \\text{ for all } i \\ne k \\mid T_k = t) \\cdot p_k(t) dt\n$$\nIf $T_k = t$, the condition becomes $t \\le T_i$ for all $i \\ne k$. Due to the independence of the channels, the conditional probability is simply the probability of the event itself:\n$$\nP(T_i \\ge t \\text{ for all } i \\ne k \\mid T_k = t) = P(T_i \\ge t \\text{ for all } i \\ne k) = \\prod_{i \\ne k} P(T_i \\ge t)\n$$\nUsing the survival function $P(T_i \\ge t) = \\exp(-r_i t)$, this product becomes:\n$$\n\\prod_{i \\ne k} \\exp(-r_i t) = \\exp\\left(-t \\sum_{i \\ne k} r_i\\right)\n$$\nNow, we substitute this back into the integral, along with the PDF for $T_k$, $p_k(t) = r_k \\exp(-r_k t)$:\n$$\nP_k = \\int_{0}^{\\infty} \\left[ \\exp\\left(-t \\sum_{i \\ne k} r_i\\right) \\right] \\cdot [r_k \\exp(-r_k t)] dt\n$$\nCombining the exponential terms:\n$$\nP_k = \\int_{0}^{\\infty} r_k \\exp\\left(-t \\left(\\sum_{i \\ne k} r_i + r_k\\right)\\right) dt\n$$\nThe sum in the exponent is the total rate $R = \\sum_{i=1}^{M} r_i$.\n$$\nP_k = r_k \\int_{0}^{\\infty} \\exp(-R t) dt\n$$\nEvaluating the definite integral:\n$$\nP_k = r_k \\left[ -\\frac{1}{R} \\exp(-R t) \\right]_{t=0}^{t=\\infty} = r_k \\left( -\\frac{1}{R} \\lim_{t \\to \\infty}\\exp(-R t) - \\left(-\\frac{1}{R} \\exp(0)\\right) \\right)\n$$\nSince $R = \\sum r_i > 0$, the limit is $0$.\n$$\nP_k = r_k \\left( 0 - \\left(-\\frac{1}{R}\\right) \\right) = \\frac{r_k}{R}\n$$\nThus, the probability that channel $k$ is the next to fire is its rate divided by the sum of all rates:\n$$\nP_k = \\frac{r_k}{\\sum_{i=1}^{M} r_i}\n$$\nThis is the second required closed-form expression.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} -\\frac{\\ln(u)}{\\sum_{i=1}^{M} r_i} & \\frac{r_k}{\\sum_{i=1}^{M} r_i} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Generating a long simulation trajectory is just the first step; extracting statistically meaningful averages is the true goal. This exercise confronts a critical challenge in data analysis: temporal correlations, which can lead to a drastic underestimation of statistical error. You will derive the concepts of the integrated autocorrelation time and the effective sample size, providing a robust tool to quantify the true statistical uncertainty in your measurements from correlated time series.",
            "id": "3841383",
            "problem": "A molecular-scale observable $A$ (for example, the shear stress) is recorded from a long Molecular Dynamics (MD) trajectory of a nanocrystalline metal at equally spaced times $t_{k}=k\\,\\Delta t$ with $\\Delta t=1\\,\\mathrm{fs}$, yielding $n=2\\times 10^{5}$ samples. Assume the process $\\{A_{k}\\}$ is strictly stationary with finite variance $\\sigma^{2}=\\gamma(0)$ and normalized autocorrelation function $\\rho(k)=\\gamma(k)/\\gamma(0)$, where $\\gamma(k)$ is the autocovariance function. The autocorrelation is modeled as a physically motivated two-timescale mixture,\n$$\n\\rho(k)=a\\,\\exp\\!\\left(-\\frac{k\\,\\Delta t}{\\tau_{1}}\\right)+\\left(1-a\\right)\\,\\exp\\!\\left(-\\frac{k\\,\\Delta t}{\\tau_{2}}\\right),\n$$\nwith parameters $a=0.4$, $\\tau_{1}=3\\,\\mathrm{fs}$, and $\\tau_{2}=20\\,\\mathrm{fs}$.\n\nStarting from first principles, namely the definition of the variance of the sample mean in terms of the autocovariance,\n$$\n\\operatorname{Var}\\!\\left(\\bar{A}\\right)=\\operatorname{Var}\\!\\left(\\frac{1}{n}\\sum_{k=1}^{n}A_{k}\\right)=\\frac{1}{n^{2}}\\sum_{i=1}^{n}\\sum_{j=1}^{n}\\gamma(|i-j|),\n$$\nderive a large-$n$ expression for the integrated autocorrelation time $\\tau_{\\mathrm{int}}$ (dimensionless, in units of sampling steps) in terms of $\\rho(k)$, and from it derive a formula for the effective sample size $n_{\\mathrm{eff}}$ defined by equating $\\operatorname{Var}(\\bar{A})$ to that of $n_{\\mathrm{eff}}$ independent samples. Then, using the given $\\rho(k)$, compute the numerical value of $n_{\\mathrm{eff}}$ for the specified $n$ and parameters.\n\nFinally, briefly explain how the computed $n_{\\mathrm{eff}}$ modifies the half-width of a normal-approximation confidence interval for the mean of $A$ relative to the case of $n$ independent samples, making explicit the scaling with $n_{\\mathrm{eff}}$.\n\nRound your final numerical answer for $n_{\\mathrm{eff}}$ to four significant figures. No units are required for $n_{\\mathrm{eff}}$.",
            "solution": "The problem is assessed to be valid as it is scientifically grounded in the statistical analysis of time series from molecular simulations, is well-posed with all necessary information provided, and is expressed in objective, formal language. We may therefore proceed with a solution.\n\nThe starting point is the definition of the variance of the sample mean $\\bar{A}$ of a stationary time series $\\{A_k\\}$:\n$$\n\\operatorname{Var}(\\bar{A}) = \\operatorname{Var}\\left(\\frac{1}{n}\\sum_{k=1}^{n}A_{k}\\right) = \\frac{1}{n^2}\\operatorname{Cov}\\left(\\sum_{i=1}^{n}A_{i}, \\sum_{j=1}^{n}A_{j}\\right) = \\frac{1}{n^2}\\sum_{i=1}^{n}\\sum_{j=1}^{n}\\operatorname{Cov}(A_i, A_j).\n$$\nFor a stationary process, the autocovariance function $\\gamma(k) = \\operatorname{Cov}(A_i, A_{i+k})$ depends only on the time lag $k$. Thus, $\\operatorname{Cov}(A_i, A_j) = \\gamma(|i-j|)$. The given expression is recovered:\n$$\n\\operatorname{Var}(\\bar{A}) = \\frac{1}{n^2}\\sum_{i=1}^{n}\\sum_{j=1}^{n}\\gamma(|i-j|).\n$$\nTo simplify the double summation, we group terms with the same lag $k = |i-j|$. For a given lag $k \\in \\{1, 2, \\dots, n-1\\}$, there are $2(n-k)$ pairs $(i, j)$ in the sum such that $|i-j|=k$. For $k=0$, there are $n$ pairs $(i,i)$. The sum can be rewritten as:\n$$\n\\sum_{i=1}^{n}\\sum_{j=1}^{n}\\gamma(|i-j|) = n\\gamma(0) + \\sum_{k=1}^{n-1} 2(n-k)\\gamma(k).\n$$\nSubstituting this back into the expression for $\\operatorname{Var}(\\bar{A})$ and using the definitions $\\sigma^2 = \\gamma(0)$ and $\\rho(k) = \\gamma(k)/\\gamma(0)$:\n$$\n\\operatorname{Var}(\\bar{A}) = \\frac{1}{n^2} \\left[ n\\sigma^2 + \\sum_{k=1}^{n-1} 2(n-k)\\sigma^2\\rho(k) \\right] = \\frac{\\sigma^2}{n} \\left[ 1 + \\frac{2}{n}\\sum_{k=1}^{n-1}(n-k)\\rho(k) \\right].\n$$\n$$\n\\operatorname{Var}(\\bar{A}) = \\frac{\\sigma^2}{n} \\left[ 1 + 2\\sum_{k=1}^{n-1}\\left(1-\\frac{k}{n}\\right)\\rho(k) \\right].\n$$\nFor a large number of samples $n$, and an autocorrelation function $\\rho(k)$ that decays sufficiently fast, the term $k/n$ is negligible for the values of $k$ where $\\rho(k)$ is significantly different from zero. Also, the upper limit of the sum can be extended from $n-1$ to $\\infty$ with negligible error. This yields the large-$n$ approximation:\n$$\n\\operatorname{Var}(\\bar{A}) \\approx \\frac{\\sigma^2}{n} \\left[ 1 + 2\\sum_{k=1}^{\\infty}\\rho(k) \\right].\n$$\nThe problem asks us to derive an expression for the dimensionless integrated autocorrelation time, $\\tau_{\\mathrm{int}}$, in units of sampling steps. A standard definition in this context is:\n$$\n\\tau_{\\mathrm{int}} \\equiv \\sum_{k=1}^{\\infty}\\rho(k).\n$$\nWith this definition, the variance becomes:\n$$\n\\operatorname{Var}(\\bar{A}) \\approx \\frac{\\sigma^2}{n} (1 + 2\\tau_{\\mathrm{int}}).\n$$\nThe effective sample size, $n_{\\mathrm{eff}}$, is defined by equating this variance to the variance of the mean of $n_{\\mathrm{eff}}$ independent and identically distributed samples, which is $\\sigma^2/n_{\\mathrm{eff}}$.\n$$\n\\frac{\\sigma^2}{n_{\\mathrm{eff}}} = \\frac{\\sigma^2}{n} (1 + 2\\tau_{\\mathrm{int}}).\n$$\nSolving for $n_{\\mathrm{eff}}$ gives the desired formula:\n$$\nn_{\\mathrm{eff}} = \\frac{n}{1 + 2\\tau_{\\mathrm{int}}}.\n$$\nNow, we compute the numerical value of $n_{\\mathrm{eff}}$. First, we must calculate $\\tau_{\\mathrm{int}}$ using the given model for $\\rho(k)$:\n$$\n\\rho(k) = a\\,\\exp\\left(-\\frac{k\\,\\Delta t}{\\tau_1}\\right) + (1-a)\\,\\exp\\left(-\\frac{k\\,\\Delta t}{\\tau_2}\\right).\n$$\nWe compute the sum $\\tau_{\\mathrm{int}} = \\sum_{k=1}^{\\infty}\\rho(k)$:\n$$\n\\tau_{\\mathrm{int}} = \\sum_{k=1}^{\\infty} \\left[ a\\,\\exp\\left(-\\frac{k\\,\\Delta t}{\\tau_1}\\right) + (1-a)\\,\\exp\\left(-\\frac{k\\,\\Delta t}{\\tau_2}\\right) \\right].\n$$\nThis is the sum of two geometric series. Let $r_1 = \\exp(-\\Delta t/\\tau_1)$ and $r_2 = \\exp(-\\Delta t/\\tau_2)$. The sum of a geometric series $\\sum_{k=1}^{\\infty} q^k$ is $\\frac{q}{1-q}$ for $|q|<1$.\n$$\n\\tau_{\\mathrm{int}} = a \\left(\\sum_{k=1}^{\\infty} r_1^k\\right) + (1-a) \\left(\\sum_{k=1}^{\\infty} r_2^k\\right) = a \\frac{r_1}{1-r_1} + (1-a) \\frac{r_2}{1-r_2}.\n$$\nThis can also be written as:\n$$\n\\tau_{\\mathrm{int}} = a \\frac{\\exp(-\\Delta t/\\tau_1)}{1-\\exp(-\\Delta t/\\tau_1)} + (1-a) \\frac{\\exp(-\\Delta t/\\tau_2)}{1-\\exp(-\\Delta t/\\tau_2)} = a \\frac{1}{\\exp(\\Delta t/\\tau_1)-1} + (1-a) \\frac{1}{\\exp(\\Delta t/\\tau_2)-1}.\n$$\nWe substitute the given parameter values: $a=0.4$, $\\tau_1=3\\,\\mathrm{fs}$, $\\tau_2=20\\,\\mathrm{fs}$, and $\\Delta t=1\\,\\mathrm{fs}$.\n$$\n\\frac{\\Delta t}{\\tau_1} = \\frac{1}{3}, \\quad \\frac{\\Delta t}{\\tau_2} = \\frac{1}{20} = 0.05.\n$$\n$$\n\\tau_{\\mathrm{int}} = 0.4 \\left( \\frac{1}{\\exp(1/3)-1} \\right) + 0.6 \\left( \\frac{1}{\\exp(0.05)-1} \\right).\n$$\nNow we compute the values:\n$$\n\\exp(1/3) \\approx 1.3956124 \\\\\n\\exp(0.05) \\approx 1.0512711 \\\\\n$$\n$$\n\\tau_{\\mathrm{int}} \\approx 0.4 \\left( \\frac{1}{1.3956124 - 1} \\right) + 0.6 \\left( \\frac{1}{1.0512711 - 1} \\right) \\\\\n\\tau_{\\mathrm{int}} \\approx 0.4 \\left( \\frac{1}{0.3956124} \\right) + 0.6 \\left( \\frac{1}{0.0512711} \\right) \\\\\n\\tau_{\\mathrm{int}} \\approx 0.4 (2.527723) + 0.6 (19.504221) \\\\\n\\tau_{\\mathrm{int}} \\approx 1.011089 + 11.702533 \\approx 12.713622.\n$$\nNow we compute $n_{\\mathrm{eff}}$ using $n = 2 \\times 10^5$:\n$$\nn_{\\mathrm{eff}} = \\frac{n}{1 + 2\\tau_{\\mathrm{int}}} \\approx \\frac{2 \\times 10^5}{1 + 2(12.713622)} = \\frac{2 \\times 10^5}{1 + 25.427244} = \\frac{2 \\times 10^5}{26.427244} \\approx 7567.9228.\n$$\nRounding to four significant figures, we get $n_{\\mathrm{eff}} = 7568$.\n\nFinally, we explain the impact on the confidence interval for the mean of $A$. Under the Central Limit Theorem for correlated processes, the sampling distribution of the mean $\\bar{A}$ is approximately normal for large $n$, with mean $\\mu=E[A]$ and variance $\\operatorname{Var}(\\bar{A})$. The standard error of the mean is $SE(\\bar{A}) = \\sqrt{\\operatorname{Var}(\\bar{A})}$.\nFor correlated samples, we have $SE(\\bar{A}) = \\sqrt{\\sigma^2/n_{\\mathrm{eff}}} = \\sigma/\\sqrt{n_{\\mathrm{eff}}}$.\nIf one were to incorrectly assume the $n$ samples are independent, the standard error would be calculated as $SE_{\\mathrm{naive}}(\\bar{A}) = \\sigma/\\sqrt{n}$.\nThe half-width $W$ of a normal-approximation confidence interval is proportional to the standard error, e.g., $W = z_{\\alpha/2} SE(\\bar{A})$.\nThe presence of positive correlation ($\\tau_{\\mathrm{int}} > 0$) leads to $n_{\\mathrm{eff}} < n$. This means the true standard error is larger than the naive estimate. The scaling of the half-width is with $n_{\\mathrm{eff}}^{-1/2}$.\nThe ratio of the correct half-width to the naive half-width is:\n$$\n\\frac{W_{\\mathrm{correct}}}{W_{\\mathrm{naive}}} = \\frac{z_{\\alpha/2} \\sigma / \\sqrt{n_{\\mathrm{eff}}}}{z_{\\alpha/2} \\sigma / \\sqrt{n}} = \\sqrt{\\frac{n}{n_{\\mathrm{eff}}}} = \\sqrt{1 + 2\\tau_{\\mathrm{int}}}.\n$$\nIn this problem, this factor is $\\sqrt{26.427...} \\approx 5.14$. This means that ignoring the temporal correlations would lead to underestimating the statistical error and the width of the confidence interval by a factor of more than $5$. The half-width scales as $n_{\\mathrm{eff}}^{-1/2}$ rather than the $n^{-1/2}$ scaling expected for independent data.",
            "answer": "$$\\boxed{7568}$$"
        },
        {
            "introduction": "When simulating continuous-time stochastic differential equations (SDEs), we must replace them with discrete-time approximations, which invariably introduce numerical errors. This problem explores the nature of these errors for the widely used Euler-Maruyama scheme applied to the Ornstein-Uhlenbeck process. By explicitly calculating the discretization bias and estimator variance, you will not only understand the limitations of the method but also learn a powerful extrapolation technique to systematically improve the accuracy of your results.",
            "id": "3841416",
            "problem": "Consider the overdamped Langevin stochastic differential equation (SDE) for a one-dimensional harmonic well (the Ornstein–Uhlenbeck (OU) process)\n$$\n\\mathrm{d}X_{t} \\,=\\, -\\lambda X_{t}\\,\\mathrm{d}t \\,+\\, \\sqrt{2D}\\,\\mathrm{d}W_{t},\n$$\nwith friction parameter $\\lambda>0$, diffusion coefficient $D>0$, and $W_{t}$ a standard Wiener process. Its unique invariant distribution is Gaussian with zero mean and variance $D/\\lambda$. You discretize this SDE by the Euler–Maruyama (EM) method with timestep $\\Delta t>0$,\n$$\nX_{n+1} \\,=\\, X_{n} \\,-\\, \\lambda X_{n}\\,\\Delta t \\,+\\, \\sqrt{2D\\,\\Delta t}\\,\\xi_{n},\n$$\nwhere the $\\xi_{n}$ are independent and identically distributed standard normal random variables. Assume that the Markov chain is started in its invariant distribution and is run for a total physical time $T>0$, producing $N = T/\\Delta t$ samples. Consider the time-averaged estimator of the quadratic observable $\\varphi(x)=x^{2}$,\n$$\n\\widehat{A}_{T}(\\Delta t) \\,=\\, \\frac{1}{N}\\sum_{n=1}^{N} X_{n}^{2}.\n$$\n\nUsing only foundational definitions from stochastic processes, Markov chains, and Gaussian calculus (no pre-cited weak error formulas), do the following:\n\na) Derive the discretization bias of the time average in the infinite-time limit, defined by\n$$\nb(\\Delta t) \\,:=\\, \\lim_{T\\to\\infty} \\mathbb{E}\\!\\left[\\widehat{A}_{T}(\\Delta t)\\right] \\,-\\, \\mathbb{E}_{\\pi}\\!\\left[X^{2}\\right],\n$$\nwhere $\\pi$ denotes the invariant distribution of the continuous-time SDE. Express $b(\\Delta t)$ in closed form as a function of $\\Delta t$, $\\lambda$, and $D$.\n\nb) Derive the leading-order large-$T$ variance of the time average at fixed $\\Delta t>0$,\n$$\n\\mathrm{Var}\\!\\left[\\widehat{A}_{T}(\\Delta t)\\right],\n$$\nas an explicit function of $\\Delta t$, $\\lambda$, $D$, and $T$, by relating it to the integrated autocorrelation of the stationary EM Markov chain for the observable $X_{n}^{2}$.\n\nc) Propose a first-order extrapolation strategy that removes the leading $\\mathcal{O}(\\Delta t)$ bias as $\\Delta t\\to 0$ using two independent simulations of equal total physical time $T$ with timesteps $\\Delta t$ and $\\Delta t/2$. Write the resulting extrapolated estimator explicitly as a linear combination of the two time averages $\\widehat{A}_{T}(\\Delta t)$ and $\\widehat{A}_{T}(\\Delta t/2)$.\n\nYour final answer must consist of the three closed-form expressions from parts a), b), and c). No units are required. No rounding is required.",
            "solution": "The provided problem statement is self-contained, scientifically grounded in the theory of stochastic differential equations and numerical analysis, and posed in an objective mathematical language. It is therefore deemed a valid problem. We proceed with the solution in three parts as requested.\n\nFor part (a), we are asked to find the discretization bias of the time-averaged estimator in the infinite-time limit. The problem statement specifies that the Markov chain is started in its invariant distribution, which implies the sequence of states $\\{X_n\\}$ is stationary. The expectation of the time-averaged estimator is therefore\n$$\n\\mathbb{E}\\!\\left[\\widehat{A}_{T}(\\Delta t)\\right] = \\mathbb{E}\\!\\left[\\frac{1}{N}\\sum_{n=1}^{N} X_{n}^{2}\\right] = \\frac{1}{N}\\sum_{n=1}^{N} \\mathbb{E}[X_{n}^{2}].\n$$\nDue to stationarity, $\\mathbb{E}[X_{n}^{2}]$ is independent of $n$ and is equal to the second moment of the invariant distribution of the discrete Euler–Maruyama (EM) process, which we denote by $V_{\\Delta t} = \\mathbb{E}_{\\pi_{\\Delta t}}[X^2]$. Here, $\\pi_{\\Delta t}$ is the invariant measure of the EM Markov chain. Thus,\n$$\n\\lim_{T\\to\\infty} \\mathbb{E}\\!\\left[\\widehat{A}_{T}(\\Delta t)\\right] = V_{\\Delta t}.\n$$\nThe bias is defined as $b(\\Delta t) = V_{\\Delta t} - \\mathbb{E}_{\\pi}[X^2]$. The exact invariant distribution $\\pi$ of the continuous Ornstein–Uhlenbeck (OU) process is Gaussian with mean zero and variance $\\mathbb{E}_{\\pi}[X^2] = D/\\lambda$. We must now find $V_{\\Delta t}$.\nThe EM update rule is given by\n$$\nX_{n+1} = (1 - \\lambda \\Delta t) X_n + \\sqrt{2D\\Delta t}\\,\\xi_n,\n$$\nwhere $\\xi_n \\sim N(0, 1)$ are i.i.d. standard normal random variables. In the stationary state, the variance is constant, so $\\mathbb{E}[X_{n+1}^2] = \\mathbb{E}[X_n^2] = V_{\\Delta t}$. Squaring the update rule and taking the expectation gives\n$$\n\\mathbb{E}[X_{n+1}^2] = \\mathbb{E}\\!\\left[ \\left( (1 - \\lambda \\Delta t) X_n + \\sqrt{2D\\Delta t}\\,\\xi_n \\right)^2 \\right].\n$$\nExpanding the square, we obtain\n$$\nV_{\\Delta t} = \\mathbb{E}\\!\\left[ (1 - \\lambda \\Delta t)^2 X_n^2 + 2(1 - \\lambda \\Delta t)\\sqrt{2D\\Delta t} X_n \\xi_n + 2D\\Delta t\\,\\xi_n^2 \\right].\n$$\nBy linearity of expectation and noting that $X_n$ is independent of the future noise $\\xi_n$, the cross-term vanishes: $\\mathbb{E}[X_n \\xi_n] = \\mathbb{E}[X_n]\\mathbb{E}[\\xi_n] = 0$, since the stationary distribution must have zero mean. Using $\\mathbb{E}[X_n^2] = V_{\\Delta t}$ and $\\mathbb{E}[\\xi_n^2] = 1$, we get\n$$\nV_{\\Delta t} = (1 - \\lambda \\Delta t)^2 V_{\\Delta t} + 2D\\Delta t.\n$$\nWe solve for $V_{\\Delta t}$ (assuming $2 - \\lambda\\Delta t \\neq 0$, which is the stability condition):\n$$\nV_{\\Delta t} \\left( 1 - (1 - \\lambda \\Delta t)^2 \\right) = 2D\\Delta t\n$$\n$$\nV_{\\Delta t} \\left( 1 - (1 - 2\\lambda\\Delta t + \\lambda^2\\Delta t^2) \\right) = 2D\\Delta t\n$$\n$$\nV_{\\Delta t} \\left( 2\\lambda\\Delta t - \\lambda^2\\Delta t^2 \\right) = 2D\\Delta t.\n$$\nDividing by $\\Delta t$ (for $\\Delta t > 0$),\n$$\nV_{\\Delta t} (2\\lambda - \\lambda^2\\Delta t) = 2D \\implies V_{\\Delta t} = \\frac{2D}{2\\lambda - \\lambda^2\\Delta t} = \\frac{D}{\\lambda(1 - \\frac{\\lambda \\Delta t}{2})}.\n$$\nThe bias $b(\\Delta t)$ is then\n$$\nb(\\Delta t) = V_{\\Delta t} - \\frac{D}{\\lambda} = \\frac{D}{\\lambda(1 - \\frac{\\lambda \\Delta t}{2})} - \\frac{D}{\\lambda} = \\frac{D}{\\lambda}\\left(\\frac{1 - (1 - \\frac{\\lambda \\Delta t}{2})}{1 - \\frac{\\lambda \\Delta t}{2}}\\right) = \\frac{D}{\\lambda}\\left(\\frac{\\frac{\\lambda \\Delta t}{2}}{1 - \\frac{\\lambda \\Delta t}{2}}\\right) = \\frac{D\\Delta t}{2 - \\lambda\\Delta t}.\n$$\n\nFor part (b), we derive the large-$T$ variance of $\\widehat{A}_{T}(\\Delta t)$. For a stationary process, the variance of the time average $\\frac{1}{N}\\sum_{n=1}^N \\varphi_n$ is given by $\\frac{1}{N^2}\\sum_{n,m}\\mathrm{Cov}(\\varphi_n, \\varphi_m)$. For large $N=T/\\Delta t$, this is approximately\n$$\n\\mathrm{Var}\\!\\left[\\widehat{A}_{T}(\\Delta t)\\right] \\approx \\frac{1}{N} \\sum_{k=-\\infty}^{\\infty} c_k = \\frac{\\Delta t}{T} \\sum_{k=-\\infty}^{\\infty} c_k,\n$$\nwhere $c_k = \\mathrm{Cov}(X_0^2, X_k^2)$ is the autocovariance function of the observable $X^2$ in the stationary discrete process. Since the process $\\{X_n\\}$ is a zero-mean stationary Gaussian process, we can use Isserlis' theorem to compute $c_k$:\n$$\nc_k = \\mathbb{E}[X_0^2 X_k^2] - (\\mathbb{E}[X_0^2])^2 = \\left( \\mathbb{E}[X_0^2]\\mathbb{E}[X_k^2] + 2(\\mathbb{E}[X_0 X_k])^2 \\right) - (\\mathbb{E}[X_0^2])^2 = 2(\\mathbb{E}[X_0 X_k])^2.\n$$\nWe need the time-correlation function $C(k) = \\mathbb{E}[X_0 X_k]$. Let $\\alpha = 1 - \\lambda \\Delta t$. By iterating the EM scheme $k$ times, we have $X_k = \\alpha^k X_0 + \\sum_{j=0}^{k-1} \\alpha^j \\sqrt{2D\\Delta t} \\xi_{k-1-j}$. Since $X_0$ is uncorrelated with noises $\\xi_m$ for $m \\geq 0$, we have\n$$\nC(k) = \\mathbb{E}[X_0 X_k] = \\mathbb{E}[X_0(\\alpha^k X_0)] = \\alpha^k \\mathbb{E}[X_0^2] = \\alpha^k V_{\\Delta t}.\n$$\nTherefore, $c_k = 2(C(k))^2 = 2 V_{\\Delta t}^2 \\alpha^{2k}$ for $k \\ge 0$. By symmetry, $c_k = 2 V_{\\Delta t}^2 \\alpha^{2|k|}$ for any integer $k$. The sum is\n$$\n\\sum_{k=-\\infty}^{\\infty} c_k = \\sum_{k=-\\infty}^{\\infty} 2V_{\\Delta t}^2 \\alpha^{2|k|} = 2V_{\\Delta t}^2 \\left( 1 + 2\\sum_{k=1}^{\\infty} (\\alpha^2)^k \\right).\n$$\nThe geometric series sum is $\\sum_{k=1}^{\\infty} r^k = r/(1-r)$ for $|r|<1$. Here $r=\\alpha^2 < 1$.\n$$\n\\sum_{k=-\\infty}^{\\infty} c_k = 2V_{\\Delta t}^2 \\left( 1 + \\frac{2\\alpha^2}{1-\\alpha^2} \\right) = 2V_{\\Delta t}^2 \\frac{1+\\alpha^2}{1-\\alpha^2}.\n$$\nSubstituting $\\alpha = 1 - \\lambda \\Delta t$:\n$1 - \\alpha^2 = 1 - (1-\\lambda\\Delta t)^2 = 2\\lambda\\Delta t - \\lambda^2\\Delta t^2 = \\lambda\\Delta t(2 - \\lambda\\Delta t)$.\n$1 + \\alpha^2 = 1 + (1-\\lambda\\Delta t)^2 = 2 - 2\\lambda\\Delta t + \\lambda^2\\Delta t^2$.\nThe variance is then\n$$\n\\mathrm{Var}\\!\\left[\\widehat{A}_{T}(\\Delta t)\\right] \\approx \\frac{\\Delta t}{T} \\left( 2V_{\\Delta t}^2 \\frac{2 - 2\\lambda\\Delta t + \\lambda^2\\Delta t^2}{\\lambda\\Delta t(2 - \\lambda\\Delta t)} \\right) = \\frac{2V_{\\Delta t}^2}{T} \\frac{2 - 2\\lambda\\Delta t + \\lambda^2\\Delta t^2}{\\lambda(2 - \\lambda\\Delta t)}.\n$$\nSubstituting $V_{\\Delta t} = \\frac{2D}{\\lambda(2 - \\lambda\\Delta t)}$, we get\n$$\n\\mathrm{Var}\\!\\left[\\widehat{A}_{T}(\\Delta t)\\right] \\approx \\frac{2}{T} \\left(\\frac{4D^2}{\\lambda^2(2 - \\lambda\\Delta t)^2}\\right) \\frac{2 - 2\\lambda\\Delta t + \\lambda^2\\Delta t^2}{\\lambda(2 - \\lambda\\Delta t)} = \\frac{8D^2}{\\lambda^3 T} \\frac{2 - 2\\lambda\\Delta t + \\lambda^2\\Delta t^2}{(2 - \\lambda\\Delta t)^3}.\n$$\n\nFor part (c), we propose a first-order extrapolation, a form of Richardson extrapolation. The expected value of the estimator, $A(\\Delta t) = \\mathbb{E}[\\widehat{A}_{T}(\\Delta t)]$, has a bias that can be expanded in powers of $\\Delta t$. From part (a):\n$$\nA(\\Delta t) = V_{\\Delta t} = \\frac{D/\\lambda}{1 - \\lambda\\Delta t/2} = \\frac{D}{\\lambda} \\left(1 + \\frac{\\lambda\\Delta t}{2} + \\left(\\frac{\\lambda\\Delta t}{2}\\right)^2 + \\dots \\right) = A_\\infty + C_1 \\Delta t + C_2 \\Delta t^2 + \\dots,\n$$\nwhere $A_\\infty = D/\\lambda$ and $C_1 = D/2$. The error is first-order in $\\Delta t$. We want to find a linear combination $\\widehat{A}_{\\text{ext}} = c_1 \\widehat{A}_{T}(\\Delta t) + c_2 \\widehat{A}_{T}(\\Delta t/2)$ whose expectation has a reduced bias. The expected value is\n$$\n\\mathbb{E}[\\widehat{A}_{\\text{ext}}] = c_1 A(\\Delta t) + c_2 A(\\Delta t/2) \\approx c_1(A_\\infty + C_1\\Delta t) + c_2(A_\\infty + C_1 \\frac{\\Delta t}{2}).\n$$\nTo obtain an estimator for $A_\\infty$ and cancel the leading error term, we set up the system:\n1. $c_1 + c_2 = 1$ (to recover $A_\\infty$)\n2. $c_1 \\Delta t + c_2 \\frac{\\Delta t}{2} = 0 \\implies c_1 + \\frac{c_2}{2} = 0$ (to cancel the $\\mathcal{O}(\\Delta t)$ term)\nSolving this system gives $c_2 = 2$ and $c_1 = -1$. The resulting extrapolated estimator is\n$$\n\\widehat{A}_{\\text{ext}} = 2\\widehat{A}_{T}(\\Delta t/2) - \\widehat{A}_{T}(\\Delta t).\n$$\nThe bias of this new estimator is of order $\\mathcal{O}(\\Delta t^2)$, effectively removing the leading-order error.",
            "answer": "$$\n\\boxed{\\pmatrix{ \\frac{D \\Delta t}{2 - \\lambda\\Delta t} & \\frac{8 D^2}{\\lambda^3 T} \\frac{2 - 2\\lambda\\Delta t + \\lambda^2\\Delta t^2}{(2 - \\lambda\\Delta t)^3} & 2\\widehat{A}_{T}(\\Delta t/2) - \\widehat{A}_{T}(\\Delta t) }}\n$$"
        }
    ]
}