{
    "hands_on_practices": [
        {
            "introduction": "掌握加权直方图分析方法（WHAM）始于对其核心迭代算法的扎实理解。本练习专注于实现耦合了无偏置概率分布和窗口自由能的自洽方程。通过在一个具有不同数据特征的简单“玩具”系统上进行实践 ，您将亲身体验WHAM核心的不动点迭代过程，并观察数据质量和偏置势等因素如何影响收敛性。",
            "id": "3832602",
            "problem": "考虑使用加权直方图分析方法 (WHAM) 来组合来自多个窗口的有偏直方图，以估计离散反应坐标上的潜在无偏概率质量函数。加权直方图分析方法 (WHAM) 是一种用于无偏分布的最大似然估计量，它校正在独立模拟窗口中施加的偏置势，并求解一个耦合了无偏分布和窗口归一化自由能的自洽不动点问题。在本问题中，有两个窗口，每个窗口有三个离散的 Bin。能量以玻尔兹曼常数乘以温度 $k_{\\mathrm{B}} T$ 为单位报告，因此逆温度是无量纲的。统计无效性被忽略（设为 $1$），且 Bin 宽度被视为常数并被吸收到归一化中。\n\n从 $j \\in \\{1,2,3\\}$ 上的均匀初始分布 $p^{(0)}(x_j)$ 开始，实现 WHAM 方程所隐含的不动点迭代，以更新无偏分布 $p^{(t)}(x_j)$ 和窗口自由能 $f_k^{(t)}$（其中 $k \\in \\{1,2\\}$）。在每一步对 $p^{(t)}(x_j)$ 进行归一化，以确保 $\\sum_{j=1}^{3} p^{(t)}(x_j) = 1$。当最大绝对变化 $\\max_j \\left| p^{(t)}(x_j) - p^{(t-1)}(x_j) \\right|  10^{-6}$ 时停止。对于以下每个测试用例，量化达到容差所需的迭代次数。您的程序应以所有 $j$ 的 $p^{(0)}(x_j) = 1/3$ 开始，使用提供的直方图计数和偏置势，并将逆温度值视为给定值。所有计算都应以纯数学术语进行，不报告任何物理单位。\n\n测试套件：\n- 案例 A（均衡计数，对称偏置，等温）：\n  - 窗口 $1$ 计数 $H^{(1)} = [60,120,60]$，窗口 $2$ 计数 $H^{(2)} = [40,80,40]$。\n  - 窗口 $1$ 偏置势 $U^{(1)} = [0.0,0.5,1.0]$，窗口 $2$ 偏置势 $U^{(2)} = [1.0,0.5,0.0]$。\n  - 逆温度 $\\beta^{(1)} = 1.0$, $\\beta^{(2)} = 1.0$。\n- 案例 B（稀疏计数，非对称覆盖，等温）：\n  - 窗口 $1$ 计数 $H^{(1)} = [1,0,0]$，窗口 $2$ 计数 $H^{(2)} = [0,1,1]$。\n  - 窗口 $1$ 偏置势 $U^{(1)} = [0.0,1.0,2.0]$，窗口 $2$ 偏置势 $U^{(2)} = [2.0,1.0,0.0]$。\n  - 逆温度 $\\beta^{(1)} = 1.0$, $\\beta^{(2)} = 1.0$。\n- 案例 C（不等温，轻度变化的偏置）：\n  - 窗口 $1$ 计数 $H^{(1)} = [100,50,25]$，窗口 $2$ 计数 $H^{(2)} = [10,20,40]$。\n  - 窗口 $1$ 偏置势 $U^{(1)} = [0.2,0.2,0.2]$，窗口 $2$ 偏置势 $U^{(2)} = [0.0,0.5,1.0]$。\n  - 逆温度 $\\beta^{(1)} = 1.0$, $\\beta^{(2)} = 0.5$。\n- 案例 D（极端偏置与强极化计数，等温）：\n  - 窗口 $1$ 计数 $H^{(1)} = [200,10,1]$，窗口 $2$ 计数 $H^{(2)} = [1,10,200]$。\n  - 窗口 $1$ 偏置势 $U^{(1)} = [0.0,5.0,10.0]$，窗口 $2$ 偏置势 $U^{(2)} = [10.0,5.0,0.0]$。\n  - 逆温度 $\\beta^{(1)} = 1.0$, $\\beta^{(2)} = 1.0$。\n\n算法要求：\n- 对于 $j \\in \\{1,2,3\\}$，初始化 $p^{(0)}(x_j) = 1/3$。\n- 在每次迭代 $t$ 中，使用窗口 $k$ 的归一化条件更新窗口自由能 $f_k^{(t)}$，然后通过将两个窗口的直方图与窗口偏置和 $f_k^{(t)}$ 进行适当的重加权相结合来更新无偏分布 $p^{(t)}(x_j)$，接着进行归一化以使 $\\sum_{j=1}^{3} p^{(t)}(x_j) = 1$。\n- 使用停止准则 $\\max_j \\left| p^{(t)}(x_j) - p^{(t-1)}(x_j) \\right|  10^{-6}$，并报告达到容差所执行的总迭代次数。\n\n最终输出格式：\n您的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔的整数列表（例如 `[7,42,13,5]`），其中整数按顺序对应案例 A、B、C 和 D 的迭代次数。",
            "solution": "该问题已经过验证，并被确定为一个有效、适定的科学问题。所有必要的数据和条件都已提供，任务符合统计力学和计算科学的既定原则。\n\n该问题要求实现加权直方图分析方法 (WHAM)，以找到一个具有由 $M=3$ 个 Bin 定义的离散反应坐标的系统的无偏概率质量函数 $p(x_j)$。数据源自 $K=2$ 个窗口中的模拟，每个窗口都受偏置势 $U^{(k)}(x_j)$ 的影响。目标是针对四个不同的测试用例迭代求解 WHAM 方程，并报告收敛所需的迭代次数。\n\nWHAM 方程为无偏概率 $p(x_j)$ 和与每个模拟窗口 $k$ 相关的无量纲自由能 $f_k$ 构成了一组自洽关系。对于一个有 $K$ 个窗口和 $M$ 个 Bin 的系统，这些方程是：\n$$\np(x_j) = \\frac{\\sum_{k=1}^{K} H^{(k)}(x_j)}{\\sum_{k=1}^{K} N_k e^{\\beta^{(k)} f_k} e^{-\\beta^{(k)} U^{(k)}(x_j)}}\n\\quad \\quad (1)\n$$\n$$\ne^{-\\beta^{(k)} f_k} = \\sum_{j=1}^{M} p(x_j) e^{-\\beta^{(k)} U^{(k)}(x_j)}\n\\quad \\quad (2)\n$$\n此处，$H^{(k)}(x_j)$ 是在窗口 $k$ 的 Bin $j$ 中观测到的计数，$N_k = \\sum_{j=1}^{M} H^{(k)}(x_j)$ 是来自窗口 $k$ 的总样本数，$U^{(k)}(x_j)$ 是施加于窗口 $k$ 的 Bin $j$ 的偏置势，而 $\\beta^{(k)}$ 是相应的逆温度（以无量纲量给出）。概率分布必须满足归一化条件 $\\sum_{j=1}^{M} p(x_j) = 1$。注意，方程 $(1)$ 的解释必须使得最终得到的 $p(x_j)$ 是归一化的。\n\n这些耦合的非线性方程可以使用不动点迭代方案求解。算法流程如下：\n\n1.  **初始化**：从概率分布的初始猜测 $p^{(0)}(x_j)$ 开始。问题指定了一个均匀分布：\n    $$\n    p^{(0)}(x_j) = \\frac{1}{M}\n    $$\n    对于 $j \\in \\{1, 2, ..., M\\}$。设置迭代计数器 $t=0$ 和容差 $\\epsilon=10^{-6}$。\n\n2.  **迭代**：对于 $t = 1, 2, 3, \\dots$，执行以下步骤：\n    a.  **更新自由能**：使用前一次迭代的概率分布 $p^{(t-1)}(x_j)$ 计算每个窗口 $k$ 的自由能 $f_k^{(t)}$。对方程 $(2)$ 进行重排可得：\n        $$\n        f_k^{(t)} = -\\frac{1}{\\beta^{(k)}} \\ln\\left( \\sum_{j=1}^{M} p^{(t-1)}(x_j) e^{-\\beta^{(k)} U^{(k)}(x_j)} \\right)\n        $$\n        为了数值稳定性，通常最好计算量 $C_k^{(t)} = e^{\\beta^{(k)} f_k^{(t)}}$：\n        $$\n        C_k^{(t)} = \\left( \\sum_{j=1}^{M} p^{(t-1)}(x_j) e^{-\\beta^{(k)} U^{(k)}(x_j)} \\right)^{-1}\n        $$\n\n    b.  **更新概率**：使用更新后的自由能项 $C_k^{(t)}$，根据方程 $(1)$ 计算一个新的、未归一化的概率分布 $p_{\\text{un}}^{(t)}(x_j)$：\n        $$\n        p_{\\text{un}}^{(t)}(x_j) = \\frac{\\sum_{k=1}^{K} H^{(k)}(x_j)}{\\sum_{k=1}^{K} N_k C_k^{(t)} e^{-\\beta^{(k)} U^{(k)}(x_j)}}\n        $$\n        分子是所有窗口中 Bin $j$ 的总计数。分母是结合了所有窗口信息的重加权因子。\n\n    c.  **归一化**：通过对 $p_{\\text{un}}^{(t)}(x_j)$ 进行归一化来获得更新后的概率分布 $p^{(t)}(x_j)$：\n        $$\n        p^{(t)}(x_j) = \\frac{p_{\\text{un}}^{(t)}(x_j)}{\\sum_{j'=1}^{M} p_{\\text{un}}^{(t)}(x_{j'})}\n        $$\n\n    d.  **收敛性检查**：当当前和前一次概率分布之间的最大绝对差小于指定的容差 $\\epsilon$ 时，迭代终止：\n        $$\n        \\max_j \\left| p^{(t)}(x_j) - p^{(t-1)}(x_j) \\right|  \\epsilon\n        $$\n        如果满足条件，则过程停止。否则，增加 $t$ 并返回到步骤 2a。\n\n最终输出是为满足所提供的四个测试用例中每个用例的收敛准则所需的迭代次数。该实现将利用 `numpy` 进行高效的向量化计算。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_wham_iteration(H, U, beta, tol=1e-6):\n    \"\"\"\n    Performs WHAM fixed-point iteration to find the unbiased probability distribution.\n\n    Args:\n        H (np.ndarray): A (num_windows, num_bins) array of histogram counts.\n        U (np.ndarray): A (num_windows, num_bins) array of bias potentials.\n        beta (np.ndarray): A (num_windows,) array of inverse temperatures.\n        tol (float): The convergence tolerance.\n\n    Returns:\n        int: The number of iterations required to reach convergence.\n    \"\"\"\n    num_windows, num_bins = H.shape\n    \n    # Total number of samples in each window\n    N = np.sum(H, axis=1)\n    \n    # Total histogram counts across all windows for each bin\n    total_H = np.sum(H, axis=0)\n    \n    # Initial guess for the probability distribution (uniform)\n    p = np.full(num_bins, 1.0 / num_bins, dtype=np.float64)\n    \n    iterations = 0\n    while True:\n        iterations += 1\n        p_old = p.copy()\n        \n        # Pre-compute exponential terms for efficiency\n        # exp(-beta_k * U_kj) for all k, j\n        exp_neg_beta_U = np.exp(-beta[:, np.newaxis] * U)\n        \n        # Step 2a: Update free energies. We compute C_k = exp(beta_k * f_k).\n        # C_k = 1 / sum_j(p_j * exp(-beta_k * U_kj))\n        sum_val = np.sum(p_old[np.newaxis, :] * exp_neg_beta_U, axis=1)\n        # Avoid division by zero, although p_old and exp are always positive\n        # for this problem's setup.\n        C = 1.0 / sum_val\n        \n        # Step 2b: Update unnormalized probabilities p_un(x_j)\n        # Denominator: sum_k(N_k * C_k * exp(-beta_k * U_kj))\n        denominator = np.sum(N[:, np.newaxis] * C[:, np.newaxis] * exp_neg_beta_U, axis=0)\n        \n        p_unnormalized = np.zeros(num_bins, dtype=np.float64)\n        # Avoid division by zero if a bin has no counts at all\n        non_zero_H_mask = total_H > 0\n        non_zero_denom_mask = denominator > 0\n        valid_mask = non_zero_H_mask  non_zero_denom_mask\n        \n        p_unnormalized[valid_mask] = total_H[valid_mask] / denominator[valid_mask]\n\n        # Step 2c: Normalize probabilities\n        p_sum = np.sum(p_unnormalized)\n        if p_sum > 0:\n            p = p_unnormalized / p_sum\n        else:\n            # This would happen if total_H is all zeros, not the case for these problems.\n            # We can stop if no meaningful update is possible.\n            break\n\n        # Step 2d: Check for convergence\n        error = np.max(np.abs(p - p_old))\n        if error  tol:\n            break\n            \n    return iterations\n\ndef solve():\n    \"\"\"\n    Solves the WHAM iteration problem for all specified test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A: balanced counts, symmetric biases, equal temperature\n        {\n            \"H\": np.array([[60, 120, 60], [40, 80, 40]], dtype=np.float64),\n            \"U\": np.array([[0.0, 0.5, 1.0], [1.0, 0.5, 0.0]], dtype=np.float64),\n            \"beta\": np.array([1.0, 1.0], dtype=np.float64),\n        },\n        # Case B: scarce counts, asymmetric coverage, equal temperature\n        {\n            \"H\": np.array([[1, 0, 0], [0, 1, 1]], dtype=np.float64),\n            \"U\": np.array([[0.0, 1.0, 2.0], [2.0, 1.0, 0.0]], dtype=np.float64),\n            \"beta\": np.array([1.0, 1.0], dtype=np.float64),\n        },\n        # Case C: unequal temperatures, mildly varying biases\n        {\n            \"H\": np.array([[100, 50, 25], [10, 20, 40]], dtype=np.float64),\n            \"U\": np.array([[0.2, 0.2, 0.2], [0.0, 0.5, 1.0]], dtype=np.float64),\n            \"beta\": np.array([1.0, 0.5], dtype=np.float64),\n        },\n        # Case D: extreme biases with strongly polarized counts, equal temperature\n        {\n            \"H\": np.array([[200, 10, 1], [1, 10, 200]], dtype=np.float64),\n            \"U\": np.array([[0.0, 5.0, 10.0], [10.0, 5.0, 0.0]], dtype=np.float64),\n            \"beta\": np.array([1.0, 1.0], dtype=np.float64),\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        iterations = run_wham_iteration(case[\"H\"], case[\"U\"], case[\"beta\"])\n        results.append(iterations)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "一次成功的WHAM分析不仅需要正确的代码实现，还涉及对分析参数做出明智的选择。本实践旨在探索与直方图箱体宽度（WHAM中的一个关键超参数）相关的基本“偏差-方差”权衡。通过生成合成数据并分析均方积分误差（Mean Integrated Squared Error, MISE），您将学会如何选择一个最优的箱体宽度，以平衡系统性的箱体平均误差与统计噪声，这是生成可靠自由能曲线（Potential of Mean Force, PMF）的关键技能。",
            "id": "2465743",
            "problem": "您将实现一个关于加权直方图分析方法 (WHAM) 的数值研究，以从第一性原理出发，量化重建的平均力势 (PMF) 中系统性偏差和统计方差之间随直方图 bin 宽度 $\\,\\Delta x\\,$ 变化的权衡关系。您的实现必须遵循以下数学模型和任务。\n\n背景与核心定义：\n- 在一维情况下，沿反应坐标 $\\,x\\,$ 的无偏平衡概率密度为 $\\,p(x) \\propto \\exp\\!\\left(-\\beta U(x)\\right)\\,$，其中 $\\,U(x)\\,$ 是势能，$\\,\\beta = 1/(k_\\mathrm{B} T)\\,$。在本问题中，使用约化单位，令 $\\,\\beta = 1\\,$，因此能量单位为 $\\,k_\\mathrm{B}T\\,$。\n- 平均力势 (PMF) 定义为（相差一个可加常数）$\\,F(x) = -\\ln p(x)\\,$。当 $\\,\\beta = 1\\,$ 且 $\\,p(x) \\propto \\exp(-U(x))\\,$ 时，真实的 PMF 等于 $\\,U(x)\\,$ 加上一个常数。\n- 伞形采样在 $\\,K\\,$ 个偏置势 $\\,U_k^\\text{tot}(x) = U(x) + w_k(x)\\,$ 下测量数据，其中 $\\,w_k(x)\\,$ 是一个已知的偏置势，此处为谐波势：$\\,w_k(x) = \\tfrac{1}{2}\\kappa\\,(x - x_k)^2\\,$，$\\,\\kappa > 0\\,$ 为弹性常数，$\\,x_k\\,$ 为窗口中心。\n- 直方图估计器将域划分为宽度为 $\\,\\Delta x\\,$ 的 bin，并使用 bin 计数。加权直方图分析方法 (WHAM) 结合来自多个偏置窗口的直方图，通过求解一个关于密度和窗口自由能偏移量的自洽系统，来估计无偏密度，进而得到 PMF。\n\n您的任务：\n1) 从第一性原理生成合成数据。设真实势为对称双势阱\n$$\nU(x) \\;=\\; \\alpha \\,\\bigl(x^2 - c^2\\bigr)^2,\n$$\n其中 $\\,\\alpha > 0\\,$ 且 $\\,c > 0\\,$，并考虑域 $\\,x \\in [-L, L]\\,$。对于每个具有谐波偏置 $\\,w_k(x) = \\tfrac{1}{2}\\kappa\\,(x-x_k)^2\\,$ 和指定总样本量 $\\,N_k\\,$ 的伞形窗口 $\\,k \\in \\{1,\\dots,K\\}\\,$，通过以下方式生成合成直方图计数：\n- 通过在 $\\,[-L,L]\\,$ 上的足够精细的网格上进行数值积分，从连续密度 $\\,p_k(x) \\propto \\exp\\!\\bigl(-U(x) - w_k(x)\\bigr)\\,$ 计算出精确的偏置 bin 概率。\n- 对于窗口 $\\,k\\,$，从总数为 $\\,N_k\\,$ 且 bin 概率已计算出的多项分布中抽取每个 bin 的计数。这通过直接反映偏置下的 Boltzmann 分布来确保科学真实性，而不依赖于捷径或封闭形式的采样。\n\n2) WHAM 重建。对于给定的 $\\,\\Delta x\\,$，实现 WHAM 来估计 bin 中心的无偏密度。您的实现必须使用不动点迭代，强制密度进行适当的归一化，并确定每个窗口的自由能偏移量。PMF 估计值为 $\\,\\widehat{F}(x_b) = -\\ln \\widehat{p}(x_b)\\,$（相差一个可加常数）；为便于数值比较，选择常数使得 $\\,\\min_b \\widehat{F}(x_b) = 0\\,$，同样，将真实 PMF $\\,F_\\text{true}(x)=U(x)\\,$ 在相同的 bin 中心上求值时，也使其最小值 $\\,\\min_b F_\\text{true}(x_b) = 0\\,$。\n\n3) 作为 $\\,\\Delta x\\,$ 函数的偏差-方差分析。对于每个候选的 $\\,\\Delta x\\,$，对 $\\,R\\,$ 个独立副本重复合成数据集生成和 WHAM 重建过程，以凭经验估计：\n- 逐点平均 PMF $\\,\\overline{F}_{\\Delta x}(x_b)\\,$ 和方差 $\\,\\operatorname{Var}_{\\Delta x}\\!\\bigl[F(x_b)\\bigr]\\,$（跨副本）。\n- 积分平方偏差\n$$\nB^2(\\Delta x) \\;=\\; \\sum_b \\bigl(\\,\\overline{F}_{\\Delta x}(x_b) - F_\\text{true}(x_b)\\,\\bigr)^2 \\,\\Delta x,\n$$\n和积分方差\n$$\nV(\\Delta x) \\;=\\; \\sum_b \\operatorname{Var}_{\\Delta x}\\!\\bigl[F(x_b)\\bigr] \\,\\Delta x.\n$$\n- 均方积分误差 (MISE) $\\,\\mathrm{MISE}(\\Delta x) = B^2(\\Delta x) + V(\\Delta x)\\,$，它反映了基本的权衡关系：增加 $\\,\\Delta x\\,$ 会增加 bin 平均偏差，但会减少统计方差；减小 $\\,\\Delta x\\,$ 会减少偏差，但由于每个 bin 的计数较少而增加方差。\n\n测试套件：\n在约化单位中，采用以下固定的物理和数值参数：\n- 双势阱：$\\,\\alpha = 2\\,$, $\\,c = 1\\,$, 域 $\\,[-L,L] = [-2,2]\\,$。\n- 伞形窗口数量：$\\,K = 7\\,$，中心位于 $\\,x_k \\in \\{-1.5,-1.0,-0.5,0.0,0.5,1.0,1.5\\}\\,$，弹性常数 $\\,\\kappa = 20\\,$。\n- 用于偏差-方差估计的独立副本数量：$\\,R = 40\\,$。\n- 对所有情况，假设每个窗口的计数相等 $\\,N_k = N\\,$。\n\n定义三个测试用例，通过改变 $\\,N\\,$ 和候选的 bin 宽度来探索方差主导、平衡和偏差主导的区域：\n- 用例 $\\,1\\,$ (中等采样): $\\,N = 2000\\,$，候选 $\\,\\Delta x \\in \\{0.02,\\,0.05,\\,0.10,\\,0.20\\}\\,$。\n- 用例 $\\,2\\,$ (低采样): $\\,N = 500\\,$，候选 $\\,\\Delta x \\in \\{0.02,\\,0.05,\\,0.10,\\,0.20\\}\\,$。\n- 用例 $\\,3\\,$ (高采样): $\\,N = 10000\\,$，候选 $\\,\\Delta x \\in \\{0.02,\\,0.05,\\,0.10,\\,0.20\\}\\,$。\n\n您的程序必须执行以下操作：\n- 对于每个测试用例和每个候选的 $\\,\\Delta x\\,$，执行上述基于副本的偏差-方差分析，并计算 $\\,\\mathrm{MISE}(\\Delta x)\\,$。\n- 对于每个测试用例，选择使 $\\,\\mathrm{MISE}(\\Delta x)\\,$ 最小化的 $\\,\\Delta x\\,$。如果存在平局，选择最小化者中最小的 $\\,\\Delta x\\,$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含三个测试用例所选的最优 bin 宽度，格式为用方括号括起来的逗号分隔列表，例如 `[0.05,0.10,0.05]`。不应打印任何额外文本。\n\n重要说明：\n- 所有能量单位均为 $\\,k_\\mathrm{B}T\\,$，$\\,x\\,$ 是无量纲的。\n- 不涉及角度。\n- 随机性必须在内部可复现地处理；固定一个随机种子。\n- 实现必须是自包含的，并且不得读取或写入任何文件，也不需要用户输入。",
            "solution": "该问题要求对加权直方图分析方法 (WHAM) 关于直方图 bin 宽度 $\\Delta x$ 的偏差-方差权衡进行数值研究。该问题在科学上是有效的、适定的，并且所有必要的参数都已提供。它代表了计算化学和统计力学中的一项标准任务，基于已建立的原理。我将着手提供一个完整的解决方案。\n\n该解决方案分三个主要阶段实现：合成数据生成、使用 WHAM 进行 PMF 重建，以及对多个独立副本进行偏差-方差分析。\n\n### 1. 合成数据生成\n\n为确保科学上真实的分析，合成数据必须直接从底层的 Boltzmann 分布生成。真实的、无偏的势是一个对称双势阱势，由 $U(x) = \\alpha(x^2 - c^2)^2$ 给出，其中 $\\alpha=2$ 且 $c=1$。域为 $x \\in [-2, 2]$。所有计算均在 $\\beta = (k_B T)^{-1} = 1$ 的约化单位中执行。\n\n在伞形采样中，系统在 $K$ 个不同的偏置势 $w_k(x)$ 下进行模拟，以增强对高能区域的采样。此处使用谐波偏置 $w_k(x) = \\frac{1}{2}\\kappa(x - x_k)^2$，其中 $\\kappa=20$，$K=7$ 个窗口的中心位于 $x_k \\in \\{-1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5\\}$。\n\n对于每个窗口 $k$，总势为 $U_k^\\text{tot}(x) = U(x) + w_k(x)$，相应的平衡概率密度为 $p_k(x) \\propto \\exp(-U_k^\\text{tot}(x))$。为了为给定的 bin 宽度 $\\Delta x$ 生成直方图计数，我们首先将域 $[-L, L]$ 划分为离散的 bin。对于窗口 $k$，观察到样本在 bin $b$ 中（从 $x_b^{\\text{start}}$到$x_b^{\\text{end}}$）的确切概率由归一化密度的积分给出：\n$$\nP_{k,b} = \\frac{\\int_{x_b^{\\text{start}}}^{x_b^{\\text{end}}} \\exp\\left(-U_k^\\text{tot}(x)\\right) dx}{\\int_{-L}^{L} \\exp\\left(-U_k^\\text{tot}(x)\\right) dx}\n$$\n这些积分通过在精细网格上进行数值积分来计算。使用高分辨率网格（其间距远小于任何 $\\Delta x$）通过离散求和（梯形法则）来近似积分。\n\n有了每个窗口 $k$ 的 bin 概率向量 $\\{P_{k,b}\\}$，合成直方图计数 $\\{N_{k,b}\\}$ 从具有 $N_k$ 次总试验（样本）的多项分布中抽取。该过程精确地模拟了分子模拟中采样的统计性质。\n\n### 2. WHAM 重建\n\nWHAM 提供了一种方法，可以结合来自多个偏置模拟的数据，以计算无偏概率分布 $p(x)$ 的最优估计，从而得到平均力势 (PMF) $F(x) = -\\ln p(x)$。该方法求解一组关于每个 bin 中心 $x_b$ 处的无偏概率 $p_b$ 和每个模拟窗口的无量纲自由能 $f_k$ 的自洽方程。在 $\\beta=1$ 时，WHAM 方程为：\n$$\np_b = \\frac{\\sum_{k=1}^K N_{k,b}}{\\sum_{k=1}^K N_k \\exp(f_k - w_k(x_b))}\n$$\n$$\n\\exp(-f_k) = \\sum_b p_b \\exp(-w_k(x_b))\n$$\n此处，$p_b$ 是与真实密度成正比的未归一化概率。这些方程通过不动点迭代求解：\n1. 初始化所有自由能 $f_k = 0$。\n2. 使用当前的 $f_k$ 重复计算概率 $p_b$。\n3. 使用新的 $p_b$ 计算更新后的自由能 $f_k^{\\text{new}}$。\n4. 为防止漂移，施加一个约束，例如固定一个自由能（例如，$f_1=0$）。\n5. 迭代持续进行，直到自由能收敛到指定的容差。\n\n收敛后，最终的概率 $p_b$ 用于计算给定数据集的 PMF 估计值：$\\widehat{F}(x_b) = -\\ln p_b$。为了便于比较，估计的 PMF 通过一个可加常数进行移位，使其最小值为零。真实的 PMF，$F_{\\text{true}}(x_b) = U(x_b)$，同样被归一化，使其在同一组 bin 中心上的最小值为零。\n\n当某个 bin $b$ 在所有窗口中的计数均为零时，即 $\\sum_k N_{k,b} = 0$，会出现一个关键问题。在这种情况下，$p_b=0$，估计的 PMF $\\widehat{F}(x_b)$ 为无穷大。这表示该 bin 的估计器发生了灾难性故障。\n\n### 3. 偏差-方差分析\n\n问题的核心是量化系统误差（偏差）和统计误差（方差）之间作为 bin 宽度 $\\Delta x$ 函数的权衡关系。对于每个候选的 $\\Delta x$，我们执行 $R=40$ 次独立的数据生成和 WHAM 重建过程副本。\n\n在 $R$ 个副本中，我们计算：\n- 平均估计 PMF：$\\overline{F}_{\\Delta x}(x_b) = \\frac{1}{R} \\sum_{r=1}^R \\widehat{F}_r(x_b)$。\n- PMF 的样本方差：$\\operatorname{Var}_{\\Delta x}[F(x_b)] = \\frac{1}{R-1} \\sum_{r=1}^R (\\widehat{F}_r(x_b) - \\overline{F}_{\\Delta x}(x_b))^2$。\n\n然后将它们在域上积分，得到积分平方偏差 $B^2(\\Delta x)$ 和积分方差 $V(\\Delta x)$：\n$$\nB^2(\\Delta x) = \\sum_b \\left(\\overline{F}_{\\Delta x}(x_b) - F_\\text{true}(x_b)\\right)^2 \\Delta x\n$$\n$$\nV(\\Delta x) = \\sum_b \\operatorname{Var}_{\\Delta x}[F(x_b)] \\Delta x\n$$\n均方积分误差 (MISE) 是其和：$\\mathrm{MISE}(\\Delta x) = B^2(\\Delta x) + V(\\Delta x)$。\n\n$\\Delta x$ 的选择决定了这种权衡：\n- **小 $\\Delta x$**：偏差低，因为 bin 平均误差最小。然而，由于每个 bin 的样本数较少，统计方差很高。这增加了空 bin 的概率，从而导致无穷大的 PMF 估计。如果任何副本对任何 bin 产生无穷大的 PMF，则该 bin 的平均 PMF 也将是无穷大，导致无穷大的 MISE。\n- **大 $\\Delta x$**：方差低，因为更多的样本被汇集到每个 bin 中，减少了空 bin 的机会。然而，由于在更宽的区域内对势进行平均，偏差会增加。\n\n我们的实现通过为任何在 $R$ 个副本中导致一个或多个空 bin 的 $\\Delta x$ 分配无穷大的 MISE 来处理无穷大的 PMF 值。这正确地惩罚了对于给定样本量而言过小的 bin 宽度。最优的 $\\Delta x$ 是在产生有限 MISE 的选项中使 MISE 最小的那个。最终算法遍历每个测试用例的候选 bin 宽度，计算 MISE，并根据指定标准选择最优的 $\\Delta x$。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the WHAM bias-variance analysis for all test cases.\n    \"\"\"\n    \n    # --- Fixed Physical and Numerical Parameters ---\n    ALPHA = 2.0\n    C = 1.0\n    L = 2.0\n    K_UMBRELLA = 7\n    X_CENTERS = np.array([-1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5])\n    KAPPA = 20.0\n    R_REPLICATES = 40\n    \n    # --- WHAM Solver Parameters ---\n    WHAM_MAX_ITER = 10000\n    WHAM_TOL = 1e-8\n    \n    # --- Data Generation Parameters ---\n    FINE_GRID_FACTOR = 100\n    RANDOM_SEED = 1234\n    \n    RNG = np.random.default_rng(RANDOM_SEED)\n\n    # --- Test Cases ---\n    test_cases = [\n        # Case 1 (moderate sampling)\n        (2000, [0.02, 0.05, 0.10, 0.20]),\n        # Case 2 (low sampling)\n        (500, [0.02, 0.05, 0.10, 0.20]),\n        # Case 3 (high sampling)\n        (10000, [0.02, 0.05, 0.10, 0.20]),\n    ]\n    \n    # --- Helper Functions ---\n\n    def true_potential(x, alpha, c):\n        return alpha * (x**2 - c**2)**2\n\n    def bias_potential(x, x_k, kappa):\n        return 0.5 * kappa * (x - x_k)**2\n\n    def generate_synthetic_data(n_samples, delta_x):\n        \"\"\"\n        Generates one set of synthetic histogram data for all K windows.\n        \"\"\"\n        bins = np.arange(-L, L + delta_x, delta_x)\n        n_bins = len(bins) - 1\n        \n        fine_x = np.linspace(-L, L, n_bins * FINE_GRID_FACTOR)\n        fine_dx = fine_x[1] - fine_x[0]\n\n        U_fine = true_potential(fine_x, ALPHA, C)\n        \n        hist_counts = np.zeros((K_UMBRELLA, n_bins), dtype=np.int32)\n        \n        for k in range(K_UMBRELLA):\n            w_k_fine = bias_potential(fine_x, X_CENTERS[k], KAPPA)\n            U_tot_k_fine = U_fine + w_k_fine\n            \n            # Numerically integrate to get bin probabilities\n            unnorm_p_density = np.exp(-U_tot_k_fine)\n            Z_k = np.sum(unnorm_p_density) * fine_dx\n            \n            p_k_bins = np.zeros(n_bins)\n            for b in range(n_bins):\n                bin_mask = (fine_x >= bins[b])  (fine_x  bins[b+1])\n                p_k_bins[b] = np.sum(unnorm_p_density[bin_mask]) * fine_dx / Z_k\n            \n            # Ensure probabilities sum to 1 due to potential floating point errors\n            p_k_bins /= np.sum(p_k_bins)\n            \n            # Generate counts from multinomial distribution\n            hist_counts[k, :] = RNG.multinomial(n_samples, p_k_bins)\n            \n        return hist_counts\n\n    def run_wham(hist_counts, delta_x, n_samples_vec):\n        \"\"\"\n        Implements the WHAM fixed-point iteration to find the PMF.\n        \"\"\"\n        bins = np.arange(-L, L + delta_x, delta_x)\n        bin_centers = (bins[:-1] + bins[1:]) / 2.0\n        n_bins = len(bin_centers)\n\n        # Pre-compute biases at bin centers\n        w_kb = np.zeros((K_UMBRELLA, n_bins))\n        for k in range(K_UMBRELLA):\n            w_kb[k, :] = bias_potential(bin_centers, X_CENTERS[k], KAPPA)\n\n        f_k = np.zeros(K_UMBRELLA)\n        \n        for _ in range(WHAM_MAX_ITER):\n            f_k_old = f_k.copy()\n            \n            # Equation for probabilities p_b\n            numer = np.sum(hist_counts, axis=0) # N_b\n            log_denom_terms = f_k[:, np.newaxis] - w_kb\n            max_log = np.max(log_denom_terms, axis=0)\n            denom_arg = n_samples_vec[:, np.newaxis] * np.exp(log_denom_terms - max_log)\n            denom = np.exp(max_log) * np.sum(denom_arg, axis=0)\n\n            # p_b are unnormalized probabilities\n            p_b = np.zeros_like(numer, dtype=float)\n            non_zero_denom = denom > 1e-15 # Avoid division by zero\n            p_b[non_zero_denom] = numer[non_zero_denom] / denom[non_zero_denom]\n\n            # Equation for free energies f_k\n            exp_neg_w_kb = np.exp(-w_kb)\n            f_k_new_arg = np.sum(p_b[np.newaxis, :] * exp_neg_w_kb, axis=1)\n\n            # Avoid log(0) for windows with no overlap with data\n            f_k = -np.log(f_k_new_arg, where=f_k_new_arg > 0, out=np.full_like(f_k_new_arg, np.inf))\n            \n            # Shift to set f_1 = 0\n            f_k -= f_k[0]\n            \n            if np.linalg.norm(f_k - f_k_old)  WHAM_TOL:\n                break\n        \n        # Final PMF calculation\n        pmf = -np.log(p_b, where=p_b > 0, out=np.full_like(p_b, np.inf))\n        \n        if not np.all(np.isfinite(pmf)):\n             return pmf # Return with inf values\n        \n        pmf -= np.min(pmf)\n        return pmf\n\n    def analyze_bias_variance(n_samples, delta_x):\n        \"\"\"\n        Performs the full bias-variance analysis for a given N and delta_x.\n        \"\"\"\n        bins = np.arange(-L, L + delta_x, delta_x)\n        bin_centers = (bins[:-1] + bins[1:]) / 2.0\n        n_bins = len(bin_centers)\n        \n        # True PMF on bin centers, normalized\n        F_true = true_potential(bin_centers, ALPHA, C)\n        F_true -= np.min(F_true)\n        \n        all_pmfs = np.zeros((R_REPLICATES, n_bins))\n        n_samples_vec = np.full(K_UMBRELLA, n_samples)\n        \n        for r in range(R_REPLICATES):\n            hist_counts = generate_synthetic_data(n_samples, delta_x)\n            pmf_estimate = run_wham(hist_counts, delta_x, n_samples_vec)\n            all_pmfs[r, :] = pmf_estimate\n\n        # If any replicate resulted in an empty bin, MISE is infinite\n        if np.isinf(all_pmfs).any():\n            return np.inf\n            \n        # Calculate mean and variance of PMF estimates\n        mean_pmf = np.mean(all_pmfs, axis=0)\n        var_pmf = np.var(all_pmfs, axis=0, ddof=1) # Sample variance\n        \n        # Integrated squared bias\n        bias_sq = np.sum((mean_pmf - F_true)**2) * delta_x\n        \n        # Integrated variance\n        variance = np.sum(var_pmf) * delta_x\n        \n        return bias_sq + variance\n\n    # --- Main Loop ---\n    \n    optimal_dx_results = []\n    \n    for n_samples, dx_candidates in test_cases:\n        mises = []\n        for dx in dx_candidates:\n            mise = analyze_bias_variance(n_samples, dx)\n            mises.append(mise)\n        \n        mises_array = np.array(mises)\n        \n        # Find the minimum finite MISE\n        min_mise = np.min(mises_array[np.isfinite(mises_array)]) if np.any(np.isfinite(mises_array)) else np.inf\n\n        if not np.isfinite(min_mise):\n             # This case should not happen with the given parameters\n             # If all MISE are inf, pick the largest dx as it's most robust\n            optimal_dx = dx_candidates[-1]\n        else:\n            # Find all indices matching the minimum MISE\n            best_indices = np.where(mises_array == min_mise)[0]\n            # Tie-breaking rule: choose smallest dx\n            optimal_dx = dx_candidates[best_indices[0]]\n\n        optimal_dx_results.append(optimal_dx)\n\n    # Format the final output\n    print(f\"[{','.join(f'{x:.2f}' for x in optimal_dx_results)}]\")\n\n\nsolve()\n```"
        },
        {
            "introduction": "伞形采样和WHAM的效率高度依赖于模拟窗口的策略性放置。这项高级练习要求您基于最小化自由能曲线（PMF）不确定性的原则，设计一种用于优化实验设计的算法。您将基于有效样本量的概念，推导并实现一个贪心算法 ，以预测在何处进行新的模拟将最为有效，从而完成从WHAM的使用者到高效模拟方案设计者的转变。",
            "id": "2465751",
            "problem": "给定一个一维反应坐标 $x \\in [x_{\\min}, x_{\\max}]$（单位为纳米）和一个平均力势 (PMF) 的粗略初始估计 $\\widehat{F}(x)$（单位为千焦/摩尔）。该系统先前已使用带谐波偏置势的伞形采样进行了抽样，您需要设计并实现一个有理论依据的算法，以确定在何处放置新的伞形窗口，从而在使用加权直方图分析方法 (WHAM) 进行分析时，能够最快地减少 PMF 的不确定性。\n\n您的算法必须从第一性原理推导得出，从玻尔兹曼分布、重要性采样的定义及其有效样本量出发。您不能假定任何 WHAM 的专用公式；相反，必须仅使用以下基本要素来构建算法：\n- 玻尔兹曼分布：沿 $x$ 的无偏置概率密度满足 $p(x) \\propto \\exp(-\\beta F(x))$，其中 $\\beta = 1/(k_{\\mathrm{B}} T)$。这里 $k_{\\mathrm{B}}$ 是玻尔兹曼常数（单位为每摩尔），$T$ 是绝对温度。\n- 在一个中心为 $c$、弹簧常数为 $k$（单位为能量/平方纳米）的谐波伞形下，偏置势为 $U(x) = \\tfrac{1}{2} k (x - c)^2$，有偏置的采样密度与 $p(x)\\exp(-\\beta U(x))$ 成正比。\n- 对于带有权重 $w$ 的重要性采样，加权平均的有效样本量可以使用标准的重要性采样恒等式 $N_{\\mathrm{eff}} = \\dfrac{(\\sum w)^2}{\\sum w^2}$ 来估计，这是一个经过充分检验的统计结果。\n\n问题目标与约束：\n- PMF 的不确定性在无偏置密度 $p(x)$ 最不确定的地方最大。请仅使用每个离散箱的有效样本量，沿坐标构建一个不确定性代理指标，并定义一个标量目标来汇总沿坐标的这种不确定性。您的目标函数必须是关于有效样本量的单调递减函数在所有箱上的总和（例如，与 $1/\\sqrt{N_{\\mathrm{eff}}}$ 成正比的表达式是可以接受的），并且必须包含一个小的正正则化项 $\\varepsilon$ 以避免除以零。\n- 给定当前的伞形窗口集合及其样本数量，使用粗略的 PMF $\\widehat{F}(x)$ 来预测每个窗口在各个箱中的样本分布。然后，估计所有当前窗口组合贡献的每个箱的有效样本量。您可以通过在箱中心处求值来近似箱内平均量。\n- 为选择新的伞形窗口，请使用贪心策略：一次放置一个新窗口，通过评估一组离散的候选中心，并选择当其预期贡献被加入时，能最大程度减少总体不确定性目标的那个中心。重复此过程，直到放置了所需数量的新窗口。在任何一对窗口中心（已有的或新选择的）之间强制执行最小间距约束 $\\Delta_{\\min}$。\n- 所有积分必须通过在均匀网格上的黎曼和来近似。所有概率必须进行正确的归一化。\n\n数值与物理设置：\n- 单位：\n  - $x$ 单位为纳米。\n  - $F$ 和 $U$ 单位为千焦/摩尔。\n  - $k_{\\mathrm{B}} = 8.31446261815324 \\times 10^{-3}$ 千焦/摩尔/开尔文。\n  - $T$ 单位为开尔文。\n- 本问题不使用角度。\n- 除上述选择所隐含的单位换算外，没有其他量纲转换。\n\n离散化与候选点：\n- 将 $[x_{\\min}, x_{\\max}]$ 离散化为 $M$ 个均匀间隔的点 $x_j$，间距为常数 $\\Delta x$；使用箱中心作为新窗口中心的候选点。\n- 对于每个候选中心 $z$，通过模拟一个具有指定弹簧常数和样本数量的新谐波伞形来预测其对有效样本量的影响，并将其贡献添加到当前窗口的现有贡献中。\n\n您的程序必须实现上述算法，并将其应用于以下测试套件。在每种情况下，根据提供的公式显式构造 $\\widehat{F}(x)$，使用指定的现有窗口，并在最小间距约束下选择所需数量的新窗口。\n\n测试套件：\n- 所有案例的通用设置：\n  - $x_{\\min} = 0$ 纳米，$x_{\\max} = 1$ 纳米。\n  - 网格点数：$M = 201$。\n  - 温度：$T = 300$ 开尔文。\n  - 有效样本量的正则化项：$\\varepsilon = 10^{-12}$（无量纲，仅为防止除零而添加到任何平方根或分母内部）。\n  - 最小中心间距：$\\Delta_{\\min} = 0.05$ 纳米。\n  - 所有箱内量均在箱中心处评估。\n\n- 案例1 (单势阱 PMF，典型的内部不确定性)：\n  - PMF：$\\widehat{F}(x) = \\tfrac{1}{2} a (x - 0.5)^2$，其中 $a = 100$ 千焦/摩尔/平方纳米。\n  - 现有窗口：两个窗口，中心 $c = [0.2, 0.8]$ 纳米，弹簧常数 $k = [1000, 1000]$ 千焦/摩尔/平方纳米，样本数量 $N = [1000, 1000]$。\n  - 待放置的新窗口：一次一个，总共选择 $m_{\\text{new}} = 2$ 个窗口，每个窗口的 $k_{\\text{new}} = 1000$ 千焦/摩尔/平方纳米，样本数量 $N_{\\text{new}} = 1000$。\n\n- 案例2 (带中心势垒的双势阱 PMF，势垒附近重叠不佳)：\n  - PMF：$\\min\\{50(x-0.3)^2,\\,50(x-0.7)^2\\} + 10 \\exp\\!\\big(-\\tfrac{1}{2}\\big(\\tfrac{x-0.5}{0.05}\\big)^2\\big)$，所有能量单位为千焦/摩尔，$x$ 单位为纳米。\n  - 现有窗口：两个窗口，中心 $c = [0.25, 0.35]$ 纳米，弹簧常数 $k = [1500, 1500]$ 千焦/摩尔/平方纳米，样本数量 $N = [1500, 1500]$。\n  - 待放置的新窗口：$m_{\\text{new}} = 1$ 个窗口，其 $k_{\\text{new}} = 1500$ 千焦/摩尔/平方纳米，样本数量 $N_{\\text{new}} = 1500$。\n\n- 案例3 (无现有数据，均匀 PMF，初始设计的边界情况)：\n  - PMF：对所有 $x$，$\\widehat{F}(x) = 0$。\n  - 现有窗口：无。\n  - 待放置的新窗口：$m_{\\text{new}} = 1$ 个窗口，其 $k_{\\text{new}} = 500$ 千焦/摩尔/平方纳米，样本数量 $N_{\\text{new}} = 2000$。\n\n输出要求：\n- 对于每个案例，输出所选新窗口中心的列表（单位为纳米），保留三位小数。\n- 最终程序输出必须是单行，包含一个由逗号分隔的三个案例结果的列表，并用一对单独的方括号括起来。每个案例结果本身必须是一个浮点数列表。例如：`[[c1_1,c1_2],[c2_1],[c3_1]]`，不含空格。",
            "solution": "该问题要求制定并实现一个贪心算法，以确定新的伞形采样窗口的最佳放置位置。目标是最小化平均力势 (PMF) 的全局不确定性，该不确定性通过加权直方图分析方法 (WHAM) 进行估计。推导必须基于统计力学和重要性采样的第一性原理。\n\n首先，我们定义系统和概率。一维反应坐标 $x$ 被离散化为 $M$ 个箱，由其中心 $x_j$ 表示（$j=1, \\dots, M$）。系统处于箱 $j$ 中的无偏置概率由玻尔兹曼分布给出，$p(x_j) \\propto \\exp(-\\beta F(x_j))$，其中 $\\beta = 1/(k_{\\mathrm{B}}T)$ 是逆温度，$F(x)$ 是 PMF。对于我们的预测算法，我们使用提供的估计值 $\\widehat{F}(x)$。因此，箱 $j$ 中的未归一化概率为 $p^{\\text{un}}_j = \\exp(-\\beta \\widehat{F}(x_j))$。\n\n一次伞形采样模拟 $i$ 的特征是其中心 $c_i$、谐波弹簧常数 $k_i$ 和收集的样本数 $N_i$。这引入了一个偏置势 $U_i(x) = \\frac{1}{2} k_i (x - c_i)^2$。在模拟 $i$ 期间，在箱 $j$ 中观察到系统的概率被此偏置所改变：\n$$\nq_{ij} \\propto p(x_j) \\exp(-\\beta U_i(x_j))\n$$\n在所有箱上进行归一化后，概率变为：\n$$\nq_{ij} = \\frac{p^{\\text{un}}_j \\exp(-\\beta U_{ij})}{\\sum_{k=1}^M p^{\\text{un}}_k \\exp(-\\beta U_{ik})} = \\frac{p^{\\text{un}}_j \\exp(-\\beta U_{ij})}{Z_i}\n$$\n其中 $Z_i$ 是有偏置模拟 $i$ 的配分函数。从模拟 $i$ 中落入箱 $j$ 的预期样本数为 $n_{ij} = N_i q_{ij}$。\n\n为了合并所有模拟的数据，我们使用重要性采样。来自模拟 $i$ 在位置 $x_j$ 的一个样本必须被重新加权，以对无偏置系综做出贡献。相应的重要性权重 $w_{ij}$ 是目标概率与采样概率之比：\n$$\nw_{ij} = \\frac{p(x_j)}{q_{ij}(x_j)} \\propto \\frac{\\exp(-\\beta \\widehat{F}(x_j))}{\\exp(-\\beta (\\widehat{F}(x_j) + U_{ij}))} = \\exp(\\beta U_{ij})\n$$\n比例常数与有效样本量的计算无关，因为它会被抵消。\n\n问题指出，PMF 的不确定性在样本统计量最差的地方最高。我们使用每个箱 $j$ 的有效样本量 $N_{\\mathrm{eff}}$ 来作为其代理指标。给定来自所有 $L$ 次模拟的、在箱 $j$ 中的样本集合，其中每次模拟 $i$ 有 $n_{ij}$ 个权重为 $w_{ij}$ 的样本，则箱 $j$ 中的总有效样本量由标准公式给出：\n$$\nN_{\\mathrm{eff}, j} = \\frac{\\left( \\sum_{i=1}^L n_{ij} w_{ij} \\right)^2}{\\sum_{i=1}^L n_{ij} w_{ij}^2}\n$$\n箱 $j$ 中的不确定性被认为与 $\\sqrt{N_{\\mathrm{eff}, j}}$ 成反比。我们的全局目标函数 $J$（我们旨在最小化它）是这些不确定性在所有箱上的总和，并带有一个小的正则化项 $\\varepsilon$ 以确保数值稳定性：\n$$\nJ = \\sum_{j=1}^M \\frac{1}{\\sqrt{N_{\\mathrm{eff}, j} + \\varepsilon}}\n$$\n为了在计算 $N_{\\mathrm{eff}, j}$ 时确保数值稳定性，我们重新构造了求和项。分子求和项中的项是 $n_{ij} w_{ij} = (N_i p^{\\text{un}}_j/Z_i)$。分母求和项中的项是 $n_{ij} w_{ij}^2 = (N_i p^{\\text{un}}_j/Z_i) \\exp(\\beta U_{ij})$。这导致：\n$$\nN_{\\mathrm{eff}, j} = p^{\\text{un}}_j \\frac{\\left(\\sum_i N_i/Z_i\\right)^2}{\\sum_i (N_i/Z_i) \\exp(\\beta U_{ij})}\n$$\n项 $\\exp(\\beta U_{ij})$ 可能非常大，导致溢出。为了稳健地计算分母中的和，我们使用 log-sum-exp 方法。对于每个箱 $j$，我们计算和中每一项的对数 $\\log(N_i) - \\log(Z_i) + \\beta U_{ij}$，然后使用一个数值稳定的 `logaddexp` 函数来找到和的对数，再通过指数运算恢复出和本身。\n\n选择 $m_{\\text{new}}$ 个新窗口中心的算法是贪心算法。我们从现有窗口及其参数的集合开始。\n1. 用任何现有的窗口中心来初始化当前窗口中心的集合。\n2. 对于要放置的 $m_{\\text{new}}$ 个窗口中的每一个：\n    a. 生成所有有效候选中心的列表。一个候选点（网格点 $x_j$）是有效的，如果它到当前集合中每个中心的距离至少为 $\\Delta_{\\min}$。\n    b. 对于每个有效的候选中心 $z$，假设性地将一个中心为 $z$ 且具有指定参数（$k_{\\text{new}}, N_{\\text{new}}$）的新窗口添加到当前集合中。\n    c. 为这个假设的完整窗口集合计算目标函数 $J$。\n    d. 选择导致 $J$ 值最小的候选中心 $z^*$。\n    e. 将这个新窗口（以 $z^*$ 为中心）永久添加到当前窗口集合中，以用于下一次迭代。\n3. 最终选择的中心列表 $\\{z^*_1, \\dots, z^*_{m_{\\text{new}}}\\}$ 就是结果。\n\n这个过程确保了每个新窗口都被放置在预计能够为 PMF 总体不确定性提供最大边际减少量的位置。",
            "answer": "```python\nimport numpy as np\n\n# Physical constants and settings\nKB_J_MOL_K = 8.31446261815324\nKB = KB_J_MOL_K / 1000.0  # In kJ/mol/K\n\ndef calculate_objective(centers, ks, Ns, F_hat, x_grid, beta, epsilon):\n    \"\"\"\n    Calculates the aggregate uncertainty objective J based on a stable N_eff formulation.\n    \"\"\"\n    num_bins = len(x_grid)\n    num_windows = len(centers)\n\n    if num_windows == 0:\n        # If there are no windows, N_eff is 0 for all bins.\n        return num_bins / np.sqrt(epsilon)\n\n    # Calculate unnormalized unbiased probabilities stably\n    log_p_un = -beta * F_hat\n\n    # For each window i, calculate U_i(j) and its partition function Z_i\n    U_matrix = np.zeros((num_windows, num_bins), dtype=np.float64)\n    log_Z_values = np.zeros(num_windows, dtype=np.float64)\n    for i in range(num_windows):\n        U_matrix[i, :] = 0.5 * ks[i] * (x_grid - centers[i])**2\n        log_integrand_i = log_p_un - beta * U_matrix[i, :]\n        log_Z_values[i] = np.logaddexp.reduce(log_integrand_i)\n        \n    log_N_values = np.log(np.array(Ns, dtype=np.float64))\n    log_Ni_minus_log_Zi = log_N_values - log_Z_values\n\n    # Calculate S1_j = p_un_j * sum_i(N_i/Z_i)\n    # The sum part is constant over j.\n    C = np.sum(np.exp(log_Ni_minus_log_Zi))\n    S1_j = np.exp(log_p_un) * C\n\n    # Calculate S2_j = p_un_j * sum_i( (N_i/Z_i) * exp(beta * U_ij) )\n    # The term in the sum is exp(log(N_i) - log(Z_i) + beta * U_ij)\n    log_terms_for_S2 = log_Ni_minus_log_Zi[:, np.newaxis] + beta * U_matrix\n    \n    # logsumexp over windows i for each bin j\n    log_sum_S2_terms = np.logaddexp.reduce(log_terms_for_S2, axis=0)\n    S2_j = np.exp(log_p_un + log_sum_S2_terms)\n\n    # Calculate N_eff_j = S1_j^2 / S2_j\n    n_eff = np.zeros(num_bins, dtype=np.float64)\n    non_zero_S2 = S2_j > 1e-300 # Avoid division by zero\n    n_eff[non_zero_S2] = (S1_j[non_zero_S2]**2) / S2_j[non_zero_S2]\n    \n    # Calculate final objective J\n    objective = np.sum(1.0 / np.sqrt(n_eff + epsilon))\n    return objective\n\ndef solve_case(params):\n    \"\"\"\n    Solves a single test case using the greedy window placement algorithm.\n    \"\"\"\n    x_grid = np.linspace(params['x_min'], params['x_max'], params['M'])\n    beta = 1.0 / (KB * params['T'])\n\n    # Define and calculate the PMF estimate on the grid\n    if params['case_id'] == 1:\n        a = 100\n        F_hat = 0.5 * a * (x_grid - 0.5)**2\n    elif params['case_id'] == 2:\n        well1 = 50 * (x_grid - 0.3)**2\n        well2 = 50 * (x_grid - 0.7)**2\n        barrier = 10 * np.exp(-0.5 * ((x_grid - 0.5) / 0.05)**2)\n        F_hat = np.minimum(well1, well2) + barrier\n    else: # Case 3\n        F_hat = np.zeros_like(x_grid)\n\n    current_centers = list(params['existing_c'])\n    current_ks = list(params['existing_k'])\n    current_Ns = list(params['existing_N'])\n    \n    newly_selected_centers = []\n    \n    for _ in range(params['m_new']):\n        best_objective = np.inf\n        best_center = None\n        \n        # Filter candidate centers based on minimum separation\n        valid_candidates = []\n        for cand_c in x_grid:\n            if all(np.abs(cand_c - c) >= params['delta_min'] for c in current_centers):\n                valid_candidates.append(cand_c)\n\n        if not valid_candidates:\n            break\n\n        for cand_center in valid_candidates:\n            temp_centers = current_centers + [cand_center]\n            temp_ks = current_ks + [params['k_new']]\n            temp_Ns = current_Ns + [params['N_new']]\n            \n            objective = calculate_objective(\n                temp_centers, temp_ks, temp_Ns,\n                F_hat, x_grid, beta, params['epsilon']\n            )\n            \n            if objective  best_objective:\n                best_objective = objective\n                best_center = cand_center\n        \n        if best_center is not None:\n            current_centers.append(best_center)\n            current_ks.append(params['k_new'])\n            current_Ns.append(params['N_new'])\n            newly_selected_centers.append(best_center)\n            \n    return [round(c, 3) for c in newly_selected_centers]\n\n\ndef solve():\n    \"\"\"\n    Defines test cases and orchestrates their solution.\n    \"\"\"\n    common_settings = {\n        'x_min': 0.0,\n        'x_max': 1.0,\n        'M': 201,\n        'T': 300.0,\n        'epsilon': 1e-12,\n        'delta_min': 0.05,\n    }\n\n    test_cases = [\n        {\n            'case_id': 1,\n            **common_settings,\n            'existing_c': [0.2, 0.8],\n            'existing_k': [1000.0, 1000.0],\n            'existing_N': [1000, 1000],\n            'm_new': 2,\n            'k_new': 1000.0,\n            'N_new': 1000,\n        },\n        {\n            'case_id': 2,\n            **common_settings,\n            'existing_c': [0.25, 0.35],\n            'existing_k': [1500.0, 1500.0],\n            'existing_N': [1500, 1500],\n            'm_new': 1,\n            'k_new': 1500.0,\n            'N_new': 1500,\n        },\n        {\n            'case_id': 3,\n            **common_settings,\n            'existing_c': [],\n            'existing_k': [],\n            'existing_N': [],\n            'm_new': 1,\n            'k_new': 500.0,\n            'N_new': 2000,\n        },\n    ]\n\n    results = [solve_case(case) for case in test_cases]\n    \n    # Format the output string exactly as specified.\n    # repr() adds spaces, so we remove them.\n    formatted_results = [repr(res).replace(' ', '') for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}