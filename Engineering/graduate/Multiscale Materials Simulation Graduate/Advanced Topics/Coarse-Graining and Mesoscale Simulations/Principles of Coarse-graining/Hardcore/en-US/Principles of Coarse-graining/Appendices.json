{
    "hands_on_practices": [
        {
            "introduction": "The first step in any coarse-graining procedure is to define the mapping from the high-resolution atomistic representation to the simplified coarse-grained one. For rigid molecules, this transformation is often achieved by identifying a center of mass for position and an orientation quaternion for rotation. This exercise  provides direct, hands-on practice in computing these coarse-grained variables from a given set of atomic coordinates, making the abstract concept of a mapping operator concrete and calculable through fundamental techniques like mass-weighted averaging and optimal rotational alignment.",
            "id": "3835541",
            "problem": "Consider a rigid coarse-grained mapping for a trimer molecule in three dimensions that reduces the all-atom coordinates to a center-of-mass and a unit orientation quaternion. Let the molecule consist of three atoms with equal masses $m_{1} = m_{2} = m_{3}$ and a fixed reference geometry in a body-fixed frame given by the positions\n$$\n\\mathbf{s}_{1} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\quad\n\\mathbf{s}_{2} = \\begin{pmatrix} -\\tfrac{1}{2} \\\\ \\tfrac{\\sqrt{3}}{2} \\\\ 0 \\end{pmatrix}, \\quad\n\\mathbf{s}_{3} = \\begin{pmatrix} -\\tfrac{1}{2} \\\\ -\\tfrac{\\sqrt{3}}{2} \\\\ 0 \\end{pmatrix},\n$$\nwhich satisfy $\\sum_{i=1}^{3} m_{i} \\mathbf{s}_{i} = \\mathbf{0}$. In the laboratory frame, an observed configuration is provided by atomic coordinates\n$$\n\\mathbf{r}_{1} = \\begin{pmatrix} 1 \\\\ 3 \\\\ 3 \\end{pmatrix}, \\quad\n\\mathbf{r}_{2} = \\begin{pmatrix} 1 - \\tfrac{\\sqrt{3}}{2} \\\\ \\tfrac{3}{2} \\\\ 3 \\end{pmatrix}, \\quad\n\\mathbf{r}_{3} = \\begin{pmatrix} 1 + \\tfrac{\\sqrt{3}}{2} \\\\ \\tfrac{3}{2} \\\\ 3 \\end{pmatrix}.\n$$\nAll coordinates are in nanometers, and masses are equal and finite.\n\nTask:\n1. Starting from first principles (rigid-body kinematics, mass-weighted definitions, and the orthogonal Procrustes alignment in three dimensions), formulate explicitly the nonlinear coarse-graining map that sends $\\{\\mathbf{r}_{i}\\}_{i=1}^{3}$ to the center-of-mass $\\mathbf{R}$ and the orientation unit quaternion $\\mathbf{q} = (q_{0}, q_{1}, q_{2}, q_{3})$ (scalar-first convention), defined as the unique maximizer (up to the known double cover $\\mathbf{q} \\sim -\\mathbf{q}$) of a mass-weighted least-squares alignment of $\\{\\mathbf{s}_{i}\\}_{i=1}^{3}$ to the centered laboratory coordinates $\\{\\mathbf{r}_{i} - \\mathbf{R}\\}_{i=1}^{3}$. Your formulation must include the normalization constraint on $\\mathbf{q}$ and a clear sign convention to fix the representative of the quaternion equivalence class.\n2. Using your explicit mapping, compute the coarse variables for the provided data. Adopt the convention that $q_{0} > 0$. Express the center-of-mass in nanometers and the quaternion components as dimensionless numbers. If you introduce any approximations to radicals, round your final numerical values to four significant figures; otherwise, present exact values.\n\nReport your final answer as a single row matrix containing the seven coarse variables in the order $(R_{x}, R_{y}, R_{z}, q_{0}, q_{1}, q_{2}, q_{3})$.",
            "solution": "The problem is evaluated as scientifically grounded, well-posed, and objective. It represents a standard procedure in multiscale materials simulation for determining the coarse-grained coordinates (center-of-mass and orientation) of a rigid molecule from its atomic coordinates. The problem provides all necessary data and definitions for a unique solution. Therefore, I will proceed with a full derivation.\n\nThe coarse-graining map transforms the set of all-atom coordinates $\\{\\mathbf{r}_{i}\\}_{i=1}^{3}$ into a center-of-mass vector $\\mathbf{R}$ and an orientation unit quaternion $\\mathbf{q}$.\n\n**1. Formulation of the Coarse-Graining Map**\n\n**Center of Mass ($\\mathbf{R}$):**\nThe center of mass $\\mathbf{R}$ is the mass-weighted average of the atomic positions $\\mathbf{r}_{i}$:\n$$\n\\mathbf{R} = \\frac{\\sum_{i=1}^{3} m_{i} \\mathbf{r}_{i}}{\\sum_{i=1}^{3} m_{i}}\n$$\nGiven that the masses are equal, $m_{1} = m_{2} = m_{3} = m$, the expression simplifies to the arithmetic mean of the position vectors:\n$$\n\\mathbf{R} = \\frac{m(\\mathbf{r}_{1} + \\mathbf{r}_{2} + \\mathbf{r}_{3})}{3m} = \\frac{1}{3} (\\mathbf{r}_{1} + \\mathbf{r}_{2} + \\mathbf{r}_{3})\n$$\n\n**Orientation Quaternion ($\\mathbf{q}$):**\nThe orientation is determined by finding the rotation that best aligns the centered laboratory coordinates, $\\mathbf{r}'_{i} = \\mathbf{r}_{i} - \\mathbf{R}$, with the reference body-fixed coordinates, $\\mathbf{s}_{i}$. This is achieved by solving the orthogonal Procrustes problem, which involves minimizing the mass-weighted sum of squared deviations:\n$$\nL(\\mathbf{Q}) = \\sum_{i=1}^{3} m_{i} \\|\\mathbf{r}'_{i} - \\mathbf{Q} \\mathbf{s}_{i}\\|^{2}\n$$\nwhere $\\mathbf{Q}$ is a $3 \\times 3$ proper rotation matrix ($\\mathbf{Q}^{T}\\mathbf{Q} = \\mathbf{I}$, $\\det(\\mathbf{Q})=1$). Since all masses $m_i$ are equal and positive, minimizing $L(\\mathbf{Q})$ is equivalent to minimizing $\\frac{L(\\mathbf{Q})}{m} = \\sum_{i=1}^{3} \\|\\mathbf{r}'_{i} - \\mathbf{Q} \\mathbf{s}_{i}\\|^{2}$.\n\nExpanding the squared norm, we find that minimizing this sum is equivalent to maximizing the term $\\text{Tr}(\\mathbf{Q} \\mathbf{C})$, where $\\mathbf{C}$ is the correlation matrix given by:\n$$\n\\mathbf{C} = \\sum_{i=1}^{3} \\mathbf{s}_{i} (\\mathbf{r}'_{i})^{T}\n$$\nThe solution to this maximization problem is found via the Singular Value Decomposition (SVD) of $\\mathbf{C}$. Let the SVD of $\\mathbf{C}$ be $\\mathbf{C} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^{T}$, where $\\mathbf{U}$ and $\\mathbf{V}$ are orthogonal matrices and $\\mathbf{\\Sigma}$ is a diagonal matrix of singular values. The optimal rotation matrix $\\mathbf{Q}$ is given by:\n$$\n\\mathbf{Q} = \\mathbf{V} \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & \\det(\\mathbf{V}\\mathbf{U}^{T}) \\end{pmatrix} \\mathbf{U}^{T}\n$$\nThis formulation ensures that $\\mathbf{Q}$ is a proper rotation, correcting for any potential reflection.\n\nOnce the optimal rotation matrix $\\mathbf{Q}$ is found, it is converted to a unit quaternion $\\mathbf{q} = (q_{0}, q_{1}, q_{2}, q_{3})$ using the scalar-first convention. The components are given by:\n$$\n\\begin{aligned}\nq_{0} &= \\frac{1}{2} \\sqrt{1 + \\text{Tr}(\\mathbf{Q})} \\\\\nq_{1} &= \\frac{Q_{32} - Q_{23}}{4q_{0}} \\\\\nq_{2} &= \\frac{Q_{13} - Q_{31}}{4q_{0}} \\\\\nq_{3} &= \\frac{Q_{21} - Q_{12}}{4q_{0}}\n\\end{aligned}\n$$\nwhere $Q_{ij}$ are the elements of $\\mathbf{Q}$. The problem specifies the convention $q_{0} > 0$, which is satisfied by taking the positive root for $q_0$. This choice fixes the quaternion representative from the equivalence class $(\\mathbf{q} \\sim -\\mathbf{q})$.\n\n**2. Application to the Provided Data**\n\n**Calculate Center of Mass $\\mathbf{R}$:**\nUsing the given laboratory coordinates:\n$$\n\\mathbf{r}_{1} = \\begin{pmatrix} 1 \\\\ 3 \\\\ 3 \\end{pmatrix}, \\quad\n\\mathbf{r}_{2} = \\begin{pmatrix} 1 - \\frac{\\sqrt{3}}{2} \\\\ \\frac{3}{2} \\\\ 3 \\end{pmatrix}, \\quad\n\\mathbf{r}_{3} = \\begin{pmatrix} 1 + \\frac{\\sqrt{3}}{2} \\\\ \\frac{3}{2} \\\\ 3 \\end{pmatrix}\n$$\n$$\n\\mathbf{R} = \\frac{1}{3} \\left( \\begin{pmatrix} 1 \\\\ 3 \\\\ 3 \\end{pmatrix} + \\begin{pmatrix} 1 - \\frac{\\sqrt{3}}{2} \\\\ \\frac{3}{2} \\\\ 3 \\end{pmatrix} + \\begin{pmatrix} 1 + \\frac{\\sqrt{3}}{2} \\\\ \\frac{3}{2} \\\\ 3 \\end{pmatrix} \\right) = \\frac{1}{3} \\begin{pmatrix} 3 \\\\ 6 \\\\ 9 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix}\n$$\nThe center of mass is $\\mathbf{R} = (1, 2, 3)$.\n\n**Calculate Centered Coordinates $\\mathbf{r}'_i = \\mathbf{r}_i - \\mathbf{R}$:**\n$$\n\\mathbf{r}'_{1} = \\begin{pmatrix} 1 \\\\ 3 \\\\ 3 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}\n$$\n$$\n\\mathbf{r}'_{2} = \\begin{pmatrix} 1 - \\frac{\\sqrt{3}}{2} \\\\ \\frac{3}{2} \\\\ 3 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} -\\frac{\\sqrt{3}}{2} \\\\ -\\frac{1}{2} \\\\ 0 \\end{pmatrix}\n$$\n$$\n\\mathbf{r}'_{3} = \\begin{pmatrix} 1 + \\frac{\\sqrt{3}}{2} \\\\ \\frac{3}{2} \\\\ 3 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} \\frac{\\sqrt{3}}{2} \\\\ -\\frac{1}{2} \\\\ 0 \\end{pmatrix}\n$$\n\n**Calculate the Correlation Matrix $\\mathbf{C}$:**\nThe reference coordinates are:\n$$\n\\mathbf{s}_{1} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\quad\n\\mathbf{s}_{2} = \\begin{pmatrix} -\\frac{1}{2} \\\\ \\frac{\\sqrt{3}}{2} \\\\ 0 \\end{pmatrix}, \\quad\n\\mathbf{s}_{3} = \\begin{pmatrix} -\\frac{1}{2} \\\\ -\\frac{\\sqrt{3}}{2} \\\\ 0 \\end{pmatrix}\n$$\nThe correlation matrix $\\mathbf{C} = \\sum_{i=1}^{3} \\mathbf{s}_{i} (\\mathbf{r}'_{i})^{T}$ is calculated as:\n$$\n\\mathbf{C} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} \\begin{pmatrix} 0 & 1 & 0 \\end{pmatrix} + \\begin{pmatrix} -\\frac{1}{2} \\\\ \\frac{\\sqrt{3}}{2} \\\\ 0 \\end{pmatrix} \\begin{pmatrix} -\\frac{\\sqrt{3}}{2} & -\\frac{1}{2} & 0 \\end{pmatrix} + \\begin{pmatrix} -\\frac{1}{2} \\\\ -\\frac{\\sqrt{3}}{2} \\\\ 0 \\end{pmatrix} \\begin{pmatrix} \\frac{\\sqrt{3}}{2} & -\\frac{1}{2} & 0 \\end{pmatrix}\n$$\n$$\n\\mathbf{C} = \\begin{pmatrix} 0 & 1 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix} + \\begin{pmatrix} \\frac{\\sqrt{3}}{4} & \\frac{1}{4} & 0 \\\\ -\\frac{3}{4} & -\\frac{\\sqrt{3}}{4} & 0 \\\\ 0 & 0 & 0 \\end{pmatrix} + \\begin{pmatrix} -\\frac{\\sqrt{3}}{4} & \\frac{1}{4} & 0 \\\\ -\\frac{3}{4} & \\frac{\\sqrt{3}}{4} & 0 \\\\ 0 & 0 & 0 \\end{pmatrix}\n= \\begin{pmatrix} 0 & \\frac{3}{2} & 0 \\\\ -\\frac{3}{2} & 0 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix}\n$$\n\n**Perform SVD on $\\mathbf{C}$ and Find $\\mathbf{Q}$:**\nLet the SVD of $\\mathbf{C}$ be $\\mathbf{C} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^{T}$.\nThe matrices are:\n$$\n\\mathbf{U} = \\begin{pmatrix} 0 & 1 & 0 \\\\ -1 & 0 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}, \\quad\n\\mathbf{\\Sigma} = \\begin{pmatrix} \\frac{3}{2} & 0 & 0 \\\\ 0 & \\frac{3}{2} & 0 \\\\ 0 & 0 & 0 \\end{pmatrix}, \\quad\n\\mathbf{V} = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} = \\mathbf{I}\n$$\nWe compute $\\mathbf{Q}$ starting with $\\mathbf{V}\\mathbf{U}^{T}$:\n$$\n\\mathbf{V}\\mathbf{U}^{T} = \\mathbf{I} \\begin{pmatrix} 0 & -1 & 0 \\\\ 1 & 0 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 0 & -1 & 0 \\\\ 1 & 0 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}\n$$\nThe determinant is $\\det(\\mathbf{V}\\mathbf{U}^{T}) = 1$. Thus, no correction is needed.\n$$\n\\mathbf{Q} = \\mathbf{V}\\mathbf{U}^{T} = \\begin{pmatrix} 0 & -1 & 0 \\\\ 1 & 0 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}\n$$\nThis rotation matrix represents a rotation of $+\\frac{\\pi}{2}$ radians ($90^\\circ$) about the z-axis.\n\n**Convert $\\mathbf{Q}$ to Quaternion $\\mathbf{q}$:**\nThe trace of $\\mathbf{Q}$ is $\\text{Tr}(\\mathbf{Q}) = 0 + 0 + 1 = 1$.\nNow we compute the quaternion components:\n$$\nq_{0} = \\frac{1}{2} \\sqrt{1 + 1} = \\frac{\\sqrt{2}}{2}\n$$\n$$\nq_{1} = \\frac{Q_{32} - Q_{23}}{4q_{0}} = \\frac{0 - 0}{4(\\frac{\\sqrt{2}}{2})} = 0\n$$\n$$\nq_{2} = \\frac{Q_{13} - Q_{31}}{4q_{0}} = \\frac{0 - 0}{4(\\frac{\\sqrt{2}}{2})} = 0\n$$\n$$\nq_{3} = \\frac{Q_{21} - Q_{12}}{4q_{0}} = \\frac{1 - (-1)}{4(\\frac{\\sqrt{2}}{2})} = \\frac{2}{2\\sqrt{2}} = \\frac{1}{\\sqrt{2}} = \\frac{\\sqrt{2}}{2}\n$$\nThe resulting quaternion is $\\mathbf{q} = (\\frac{\\sqrt{2}}{2}, 0, 0, \\frac{\\sqrt{2}}{2})$. The component $q_0$ is positive, satisfying the specified convention. The values are exact.\n\nThe final coarse-grained variables are the components of the center-of-mass vector $\\mathbf{R}$ and the orientation quaternion $\\mathbf{q}$.\n$\\mathbf{R} = (1, 2, 3)$ nm.\n$\\mathbf{q} = (\\frac{\\sqrt{2}}{2}, 0, 0, \\frac{\\sqrt{2}}{2})$.\nThe final answer vector is $(R_x, R_y, R_z, q_0, q_1, q_2, q_3)$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 1 & 2 & 3 & \\frac{\\sqrt{2}}{2} & 0 & 0 & \\frac{\\sqrt{2}}{2} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Once coarse-grained sites are defined, the next critical challenge is to determine the effective potential that governs their interactions. The force-matching method provides a systematic approach by parameterizing a potential whose forces best reproduce the true underlying forces from a detailed atomistic simulation. This practice exercise  guides you through the implementation of a force-matching solver, which recasts the problem as a linear least-squares fit, and demonstrates how to handle common numerical scenarios like underdetermined or rank-deficient systems.",
            "id": "3835558",
            "problem": "You are tasked with implementing a least-squares force-matching procedure to estimate coarse-grained potential parameters from atomistic force data in a one-dimensional setting. Begin from the following foundational base: the coarse-grained potential is represented as a linear combination of basis functions, and the coarse-grained force is the negative gradient of that potential. Specifically, let the coarse-grained potential be $U(r) = \\sum_{k=1}^{K} c_k \\,\\phi_k(r)$, where $c_k$ are unknown scalar parameters and $\\{\\phi_k(r)\\}$ are known basis functions of the coordinate $r$. The coarse-grained force is defined by $F_{\\mathrm{CG}}(r;\\mathbf{c}) = -\\frac{dU}{dr}(r) = -\\sum_{k=1}^{K} c_k \\,\\frac{d\\phi_k}{dr}(r)$. Given atomistic force samples $\\{F_{\\mathrm{atom}}(r_i)\\}_{i=1}^{N}$ at coordinates $\\{r_i\\}_{i=1}^{N}$, the force-matching objective is to find $\\mathbf{c} \\in \\mathbb{R}^K$ that minimizes the residual norm $\\left\\| \\mathbf{y} - \\mathbf{A}\\mathbf{c}\\right\\|_2^2$, where the design matrix $\\mathbf{A} \\in \\mathbb{R}^{N \\times K}$ has entries $A_{ik} = -\\frac{d\\phi_k}{dr}(r_i)$ and $\\mathbf{y} \\in \\mathbb{R}^N$ has entries $y_i = F_{\\mathrm{atom}}(r_i)$.\n\nYour program must:\n- Construct $\\mathbf{A}$ from the provided basis functions and coordinates for each test case.\n- Build $\\mathbf{y}$ from the provided atomistic forces for each test case. In all test cases below, $\\mathbf{y}$ is generated deterministically from a known parameter vector $\\mathbf{c}^\\star$ via $\\mathbf{y} = \\mathbf{A}\\mathbf{c}^\\star$, ensuring a consistent evaluation of the least-squares estimator.\n- Compute the least-squares solution $\\widehat{\\mathbf{c}}$ that minimizes $\\left\\| \\mathbf{y} - \\mathbf{A}\\mathbf{c}\\right\\|_2^2$. In cases where $\\mathbf{A}$ is rank-deficient or the system is underdetermined, use the Moore–Penrose pseudoinverse to obtain the minimum-norm least-squares solution.\n- Report the estimated parameter vectors $\\widehat{\\mathbf{c}}$ for each test case.\n\nAll quantities in this problem are dimensionless. Express numerical results as decimal floats rounded to $6$ decimal places.\n\nTest Suite:\nUse the following four test cases, each describing $\\{\\phi_k(r)\\}$, the coordinates $\\{r_i\\}$, the number of samples $N$, the number of basis functions $K$, and the ground-truth parameter vector $\\mathbf{c}^\\star$.\n\n- Test Case $1$ (tall, full column rank):\n  - Basis functions: $\\phi_1(r) = r^2$, $\\phi_2(r) = \\sin(r)$, $\\phi_3(r) = e^{-r}$.\n  - Coordinates: $r = [0.0, 0.5, 1.0, 1.5, 2.0]$ with $N=5$.\n  - Parameters: $\\mathbf{c}^\\star = [0.8, -1.2, 0.5]$ with $K=3$.\n  - Atomistic forces: $F_{\\mathrm{atom}}(r_i) = F_{\\mathrm{CG}}(r_i;\\mathbf{c}^\\star)$ for each $i$.\n- Test Case $2$ (square, exactly determined):\n  - Basis functions: $\\phi_1(r) = r^2$, $\\phi_2(r) = \\sin(r)$, $\\phi_3(r) = e^{-r}$.\n  - Coordinates: $r = [0.3, 1.0, 1.7]$ with $N=3$.\n  - Parameters: $\\mathbf{c}^\\star = [-0.4, 2.0, -0.1]$ with $K=3$.\n  - Atomistic forces: $F_{\\mathrm{atom}}(r_i) = F_{\\mathrm{CG}}(r_i;\\mathbf{c}^\\star)$ for each $i$.\n- Test Case $3$ (tall, rank-deficient due to duplicate basis functions):\n  - Basis functions: $\\phi_1(r) = r$, $\\phi_2(r) = r$, $\\phi_3(r) = r^3$.\n  - Coordinates: $r = [0.2, 0.6, 1.1, 1.5]$ with $N=4$.\n  - Parameters: $\\mathbf{c}^\\star = [1.0, -1.0, 0.5]$ with $K=3$.\n  - Atomistic forces: $F_{\\mathrm{atom}}(r_i) = F_{\\mathrm{CG}}(r_i;\\mathbf{c}^\\star)$ for each $i$.\n- Test Case $4$ (underdetermined, minimum-norm solution):\n  - Basis functions: $\\phi_1(r) = r$, $\\phi_2(r) = r^2$, $\\phi_3(r) = \\sin(r)$, $\\phi_4(r) = e^{-r}$.\n  - Coordinates: $r = [0.2, 1.0]$ with $N=2$.\n  - Parameters: $\\mathbf{c}^\\star = [1.0, 0.5, -0.3, 0.2]$ with $K=4$.\n  - Atomistic forces: $F_{\\mathrm{atom}}(r_i) = F_{\\mathrm{CG}}(r_i;\\mathbf{c}^\\star)$ for each $i$.\n\nImplementation details:\n- For each basis function set, compute the design matrix entries via $A_{ik} = -\\frac{d\\phi_k}{dr}(r_i)$.\n- Form $\\mathbf{y}$ as $\\mathbf{A}\\mathbf{c}^\\star$.\n- Compute $\\widehat{\\mathbf{c}}$ using a numerically stable least-squares solver that yields the minimum-norm solution when nonuniqueness arises.\n\nFinal Output Format:\nYour program should produce a single line of output containing the list of estimated coefficient vectors across the four test cases, as a comma-separated list of lists enclosed in square brackets. Each inner list must contain the $K$ coefficients rounded to $6$ decimal places in standard decimal notation. For example, the output must look like $[[c_{1,1},c_{1,2},\\dots],[c_{2,1},\\dots],\\dots]$.",
            "solution": "The problem requires the implementation of a least-squares force-matching procedure to determine the parameters $\\mathbf{c}$ of a coarse-grained potential. The potential is given as a linear combination of basis functions, $U(r) = \\sum_{k=1}^{K} c_k \\phi_k(r)$. The corresponding coarse-grained force is $F_{\\mathrm{CG}}(r;\\mathbf{c}) = -\\frac{dU}{dr}(r) = -\\sum_{k=1}^{K} c_k \\frac{d\\phi_k}{dr}(r)$.\n\nThe problem is framed as a linear least-squares problem. Given a set of $N$ atomistic force samples $\\{F_{\\mathrm{atom}}(r_i)\\}$ at corresponding coordinates $\\{r_i\\}$, we aim to find the coefficient vector $\\mathbf{c} = [c_1, c_2, \\dots, c_K]^T$ that best reproduces these forces. This is accomplished by minimizing the squared Euclidean norm of the residual vector, $\\left\\| \\mathbf{y} - \\mathbf{A}\\mathbf{c}\\right\\|_2^2$.\n\nThe components of this standard linear model are defined as:\n1.  The design matrix $\\mathbf{A} \\in \\mathbb{R}^{N \\times K}$, whose entries are given by the negative derivatives of the basis functions evaluated at the sample coordinates: $A_{ik} = -\\frac{d\\phi_k}{dr}(r_i)$. Each column of $\\mathbf{A}$ represents the contribution of a single basis function to the force across all sample points.\n2.  The observation vector $\\mathbf{y} \\in \\mathbb{R}^N$, which contains the atomistic force samples: $y_i = F_{\\mathrm{atom}}(r_i)$. For the purposes of this problem, these forces are generated deterministically from a known ground-truth parameter vector $\\mathbf{c}^\\star$ such that $\\mathbf{y} = \\mathbf{A}\\mathbf{c}^\\star$. This implies that a perfect solution with zero residual exists.\n\nThe task is to compute the least-squares solution $\\widehat{\\mathbf{c}}$. The nature of this solution depends on the properties of the matrix $\\mathbf{A}$:\n- If $\\mathbf{A}$ has full column rank (i.e., $\\mathrm{rank}(\\mathbf{A}) = K$), the system has a unique least-squares solution. This occurs in Test Cases $1$ and $2$. Since $\\mathbf{y}$ is in the column space of $\\mathbf{A}$ (by construction), the solution $\\widehat{\\mathbf{c}}$ will be exactly equal to $\\mathbf{c}^\\star$, up to numerical precision.\n- If $\\mathbf{A}$ is rank-deficient ($\\mathrm{rank}(\\mathbf{A}) < K$) or the system is underdetermined ($N < K$), there are infinitely many solutions that perfectly minimize the residual. This occurs in Test Cases $3$ and $4$. In such scenarios, the problem specifies finding the unique solution that also has the minimum Euclidean norm, $\\|\\mathbf{c}\\|_2$. This is known as the minimum-norm least-squares solution.\n\nThe Moore-Penrose pseudoinverse, denoted $\\mathbf{A}^+$, provides a general solver for $\\widehat{\\mathbf{c}} = \\mathbf{A}^+\\mathbf{y}$ that yields the minimum-norm least-squares solution in all cases. The `numpy.linalg.lstsq` function is a numerically stable implementation for finding this solution.\n\nThe algorithmic procedure for each test case is as follows:\n1.  Define the set of coordinates $\\{r_i\\}_{i=1}^N$ and the ground-truth parameters $\\mathbf{c}^\\star$.\n2.  Analytically compute the derivatives $\\frac{d\\phi_k}{dr}$ for each basis function $\\phi_k(r)$.\n3.  Construct the $N \\times K$ design matrix $\\mathbf{A}$ by evaluating $A_{ik} = -\\frac{d\\phi_k}{dr}(r_i)$.\n4.  Construct the observation vector $\\mathbf{y}$ by computing the matrix-vector product $\\mathbf{y} = \\mathbf{A}\\mathbf{c}^\\star$.\n5.  Solve for the estimated parameters $\\widehat{\\mathbf{c}}$ using `numpy.linalg.lstsq(A, y)`.\n6.  Store the resulting vector $\\widehat{\\mathbf{c}}$ and repeat for all test cases.\n\nThe specific basis functions and their negative derivatives for each case are:\n- **Test Cases 1 & 2**: Basis functions $\\{\\phi_k(r)\\}$ are $\\{r^2, \\sin(r), e^{-r}\\}$. The columns of $\\mathbf{A}$ are constructed from $\\{-2r, -\\cos(r), e^{-r}\\}$.\n- **Test Case 3**: Basis functions are $\\{r, r, r^3\\}$. The columns of $\\mathbf{A}$ are from $\\{-1, -1, -3r^2\\}$. The first two columns are identical, making $\\mathbf{A}$ rank-deficient. The parameters $c_1$ and $c_2$ are unidentifiable individually; only their sum can be determined. The minimum-norm solution will distribute the contribution appropriately.\n- **Test Case 4**: Basis functions are $\\{r, r^2, \\sin(r), e^{-r}\\}$. The columns of $\\mathbf{A}$ are from $\\{-1, -2r, -\\cos(r), e^{-r}\\}$. With $N=2$ samples and $K=4$ parameters, the system is underdetermined. The solution $\\widehat{\\mathbf{c}}$ will be the orthogonal projection of $\\mathbf{c}^\\star$ onto the row space of $\\mathbf{A}$.\n\nThe final results are collected and formatted into a list of lists, with each numerical value rounded to $6$ decimal places.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements a least-squares force-matching procedure for four test cases\n    to estimate coarse-grained potential parameters.\n    \"\"\"\n    # Define the test cases, including coordinates, true parameters, and\n    # the basis functions for the force (negative derivative of potential basis).\n    test_cases = [\n        {\n            \"r\": np.array([0.0, 0.5, 1.0, 1.5, 2.0]),\n            \"c_star\": np.array([0.8, -1.2, 0.5]),\n            \"force_basis\": [\n                lambda r_vals: -2 * r_vals,\n                lambda r_vals: -np.cos(r_vals),\n                lambda r_vals: np.exp(-r_vals),\n            ],\n        },\n        {\n            \"r\": np.array([0.3, 1.0, 1.7]),\n            \"c_star\": np.array([-0.4, 2.0, -0.1]),\n            \"force_basis\": [\n                lambda r_vals: -2 * r_vals,\n                lambda r_vals: -np.cos(r_vals),\n                lambda r_vals: np.exp(-r_vals),\n            ],\n        },\n        {\n            \"r\": np.array([0.2, 0.6, 1.1, 1.5]),\n            \"c_star\": np.array([1.0, -1.0, 0.5]),\n            \"force_basis\": [\n                lambda r_vals: -1.0 * np.ones_like(r_vals),\n                lambda r_vals: -1.0 * np.ones_like(r_vals),\n                lambda r_vals: -3 * r_vals**2,\n            ],\n        },\n        {\n            \"r\": np.array([0.2, 1.0]),\n            \"c_star\": np.array([1.0, 0.5, -0.3, 0.2]),\n            \"force_basis\": [\n                lambda r_vals: -1.0 * np.ones_like(r_vals),\n                lambda r_vals: -2 * r_vals,\n                lambda r_vals: -np.cos(r_vals),\n                lambda r_vals: np.exp(-r_vals),\n            ],\n        },\n    ]\n\n    all_c_hats = []\n    for case in test_cases:\n        r = case[\"r\"]\n        c_star = case[\"c_star\"]\n        force_basis = case[\"force_basis\"]\n\n        N = len(r)\n        K = len(force_basis)\n\n        # Construct the design matrix A\n        A = np.zeros((N, K))\n        for k, basis_func in enumerate(force_basis):\n            A[:, k] = basis_func(r)\n\n        # Construct the observation vector y from the ground truth parameters\n        y = A @ c_star\n\n        # Compute the least-squares solution. For rank-deficient or\n        # underdetermined systems, lstsq returns the minimum-norm solution.\n        c_hat, _, _, _ = np.linalg.lstsq(A, y, rcond=None)\n        all_c_hats.append(c_hat)\n\n    # Format the results into a string representation of a list of lists.\n    formatted_results = []\n    for c_hat in all_c_hats:\n        # Round each coefficient to 6 decimal places and format as a string\n        rounded_c_str = [f\"{x:.6f}\" for x in c_hat]\n        # Create the string for the inner list, e.g., \"[c1,c2,c3]\"\n        formatted_results.append(f\"[{','.join(rounded_c_str)}]\")\n\n    # Create the final output string for the list of lists, e.g., \"[[...],[...]]\"\n    final_output_str = f\"[{','.join(formatted_results)}]\"\n\n    # Final print statement in the exact required format.\n    print(final_output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "Coarse-graining simplifies not only the structure but also the dynamics of a system. However, integrating out fast degrees of freedom can introduce \"memory\" into the equations of motion for the slow variables, a concept formalized by the Generalized Langevin Equation. The validity of simpler, memoryless (Markovian) models depends critically on the separation of timescales between the fast, eliminated motions and the slow, resolved ones. This exercise  allows you to analytically quantify the error introduced by making a Markovian approximation, providing deep insight into when this simplification is justified and when memory effects are too important to ignore.",
            "id": "3835538",
            "problem": "A single coarse variable $q(t)$, obtained by projecting a high-dimensional atomistic dynamics onto a scalar collective coordinate, obeys the Generalized Langevin Equation (GLE). According to the Mori–Zwanzig projection formalism and the Fluctuation–Dissipation Theorem (FDT), the reduced dynamics can be written as\n$$\nm \\ddot{q}(t) + \\int_{0}^{t} K(t-s)\\,\\dot{q}(s)\\,\\mathrm{d}s + k\\,q(t) = F_{\\mathrm{R}}(t),\n$$\nwhere $m$ is the effective mass, $k$ is the coarse-grained harmonic stiffness, $K(t)$ is the memory kernel determined by unresolved degrees of freedom, and $F_{\\mathrm{R}}(t)$ is a stationary random force consistent with the FDT. The memory kernel width is set by the decorrelation time of the orthogonal dynamics; denote this width by the fast timescale $\\tau_{f} > 0$. The slow relaxation timescale of the coarse mode in the Markovian overdamped limit is $\\tau_{s} = \\gamma/k$, where $\\gamma$ is the friction coefficient defined as the integral of the memory kernel.\n\nConsider the high-friction overdamped regime $m \\to 0$, and focus on the deterministic relaxation of the mean by setting $F_{\\mathrm{R}}(t) = 0$. Assume a scientifically standard parametrization of the memory kernel with weak scale separation through a single-exponential form\n$$\nK(t) = \\frac{\\gamma}{\\tau_{f}} \\exp\\!\\Big(-\\frac{t}{\\tau_{f}}\\Big),\n$$\nwhich has integral $\\int_{0}^{\\infty} K(t)\\,\\mathrm{d}t = \\gamma$ and width $\\tau_{f}$. The Markovian closure with a Dirac delta function kernel replaces $K(t)$ by $K_{\\delta}(t) = \\gamma\\,\\delta(t)$.\n\nStarting from the above foundations, determine the slow relaxation rates predicted by the non-Markovian single-exponential kernel and by the Markovian $\\delta$-kernel closure, respectively. Then, compute an analytic expression for the relative error $E$ of the Markovian prediction with respect to the non-Markovian prediction, defined as\n$$\nE \\equiv \\frac{|\\lambda_{\\delta} - \\lambda_{\\mathrm{NM}}|}{|\\lambda_{\\mathrm{NM}}|},\n$$\nwhere $\\lambda_{\\delta}$ is the relaxation rate under the Markovian $\\delta$-kernel closure and $\\lambda_{\\mathrm{NM}}$ is the relaxation rate under the non-Markovian single-exponential kernel. Your final answer must be a single closed-form expression for $E$ in terms of $\\tau_{f}$ and $\\tau_{s}$. No numerical evaluation is required. State units or angle units only if relevant; here they are not required. Interpret physically when a Markovian closure with $\\delta$-kernel friction fails by relating $E$ to a prescribed tolerance, but do not include any inequalities in your final answer.",
            "solution": "The user has provided a scientifically valid and well-posed problem statement from the field of coarse-grained modeling. All necessary definitions and equations are provided, and the problem is free of contradictions or ambiguities. I will therefore proceed with a full solution.\n\nThe problem requires us to determine the slow relaxation rates for a coarse-grained variable $q(t)$ under two different models for the memory kernel $K(t)$ in the overdamped Generalized Langevin Equation (GLE). The GLE for the deterministic relaxation of the mean, in the overdamped limit ($m \\to 0$), is given by:\n$$\n\\int_{0}^{t} K(t-s)\\,\\dot{q}(s)\\,\\mathrm{d}s + k\\,q(t) = 0\n$$\nWe will first analyze the Markovian case and then the non-Markovian case, and finally compute the relative error between their predicted relaxation rates.\n\n**1. Markovian Relaxation Rate ($\\lambda_{\\delta}$)**\n\nThe Markovian approximation replaces the memory kernel $K(t)$ with a Dirac delta function, $K_{\\delta}(t) = \\gamma\\,\\delta(t)$, where $\\gamma = \\int_{0}^{\\infty} K(t')\\,\\mathrm{d}t'$ is the total friction. Substituting this into the GLE:\n$$\n\\int_{0}^{t} \\gamma\\,\\delta(t-s)\\,\\dot{q}(s)\\,\\mathrm{d}s + k\\,q(t) = 0\n$$\nUsing the sifting property of the Dirac delta function, the integral evaluates to $\\gamma\\,\\dot{q}(t)$:\n$$\n\\gamma\\,\\dot{q}(t) + k\\,q(t) = 0\n$$\nThis is a first-order linear ordinary differential equation. We seek a solution of the form $q(t) = q(0)\\,\\exp(-\\lambda t)$, which describes exponential relaxation. Substituting this ansatz into the equation gives:\n$$\n\\gamma\\,(-\\lambda\\,q(t)) + k\\,q(t) = 0\n$$\nFor a non-trivial solution ($q(t) \\neq 0$), the rate $\\lambda$ must satisfy:\n$$\n-\\gamma\\,\\lambda + k = 0\n$$\nSolving for the relaxation rate, which we denote as $\\lambda_{\\delta}$, we find:\n$$\n\\lambda_{\\delta} = \\frac{k}{\\gamma}\n$$\nThe problem defines the slow relaxation timescale as $\\tau_s = \\gamma/k$. Therefore, the Markovian relaxation rate is:\n$$\n\\lambda_{\\delta} = \\frac{1}{\\tau_s}\n$$\n\n**2. Non-Markovian Relaxation Rate ($\\lambda_{\\mathrm{NM}}$)**\n\nFor the non-Markovian case, the memory kernel is given by the single-exponential form:\n$$\nK(t) = \\frac{\\gamma}{\\tau_{f}} \\exp\\left(-\\frac{t}{\\tau_{f}}\\right)\n$$\nwhere $\\tau_f$ is the memory decorrelation time. The governing integro-differential equation is:\n$$\n\\int_{0}^{t} \\frac{\\gamma}{\\tau_{f}} \\exp\\left(-\\frac{t-s}{\\tau_{f}}\\right)\\,\\dot{q}(s)\\,\\mathrm{d}s + k\\,q(t) = 0\n$$\nThis is a Volterra integral equation of the convolution type. A standard method to solve this is to convert it into a system of ordinary differential equations. Let us define an auxiliary variable $z(t)$ as the friction term:\n$$\nz(t) \\equiv \\int_{0}^{t} \\frac{\\gamma}{\\tau_{f}} \\exp\\left(-\\frac{t-s}{\\tau_{f}}\\right)\\,\\dot{q}(s)\\,\\mathrm{d}s\n$$\nThe GLE then becomes $z(t) + k\\,q(t) = 0$, which implies $z(t) = -k\\,q(t)$. Differentiating this with respect to time gives $\\dot{z}(t) = -k\\,\\dot{q}(t)$.\n\nNow, we differentiate the definition of $z(t)$ using the Leibniz integral rule:\n$$\n\\dot{z}(t) = \\frac{\\mathrm{d}}{\\mathrm{d}t} \\int_{0}^{t} K(t-s)\\,\\dot{q}(s)\\,\\mathrm{d}s = K(0)\\,\\dot{q}(t) + \\int_{0}^{t} \\frac{\\partial K(t-s)}{\\partial t}\\,\\dot{q}(s)\\,\\mathrm{d}s\n$$\nFor our kernel, $K(0) = \\gamma/\\tau_f$ and $\\frac{\\partial K(t-s)}{\\partial t} = -\\frac{1}{\\tau_f} K(t-s)$. Substituting these in:\n$$\n\\dot{z}(t) = \\frac{\\gamma}{\\tau_f}\\,\\dot{q}(t) - \\frac{1}{\\tau_f} \\int_{0}^{t} K(t-s)\\,\\dot{q}(s)\\,\\mathrm{d}s = \\frac{\\gamma}{\\tau_f}\\,\\dot{q}(t) - \\frac{1}{\\tau_f} z(t)\n$$\nWe now have a system of two equations:\n1. $\\dot{z}(t) = -k\\,\\dot{q}(t)$\n2. $\\dot{z}(t) = \\frac{\\gamma}{\\tau_f}\\,\\dot{q}(t) - \\frac{1}{\\tau_f} z(t)$\n\nEquating the expressions for $\\dot{z}(t)$ and substituting $z(t) = -k\\,q(t)$:\n$$\n-k\\,\\dot{q}(t) = \\frac{\\gamma}{\\tau_f}\\,\\dot{q}(t) - \\frac{1}{\\tau_f}(-k\\,q(t))\n$$\nRearranging the terms to form a single ODE for $q(t)$:\n$$\n\\left(k + \\frac{\\gamma}{\\tau_f}\\right)\\dot{q}(t) + \\frac{k}{\\tau_f}\\,q(t) = 0\n$$\nMultiplying by $\\tau_f$:\n$$\n(k\\,\\tau_f + \\gamma)\\dot{q}(t) + k\\,q(t) = 0\n$$\nThis is again a first-order linear ODE. The relaxation rate $\\lambda_{\\mathrm{NM}}$ is found from the characteristic equation:\n$$\n(k\\,\\tau_f + \\gamma)(-\\lambda_{\\mathrm{NM}}) + k = 0\n$$\nSolving for $\\lambda_{\\mathrm{NM}}$:\n$$\n\\lambda_{\\mathrm{NM}} = \\frac{k}{\\gamma + k\\,\\tau_f}\n$$\n\n**3. Relative Error Calculation**\n\nThe relative error $E$ of the Markovian prediction with respect to the non-Markovian one is defined as:\n$$\nE \\equiv \\frac{|\\lambda_{\\delta} - \\lambda_{\\mathrm{NM}}|}{|\\lambda_{\\mathrm{NM}}|}\n$$\nThe rates $\\lambda_{\\delta}$ and $\\lambda_{\\mathrm{NM}}$ are both positive, as $k, \\gamma, \\tau_f > 0$. Also, $\\gamma + k\\tau_f > \\gamma$, so $\\lambda_{\\mathrm{NM}} < \\lambda_\\delta$. The numerator is positive, so we can drop the outer absolute value.\n$$\nE = \\frac{\\lambda_{\\delta} - \\lambda_{\\mathrm{NM}}}{\\lambda_{\\mathrm{NM}}} = \\frac{\\lambda_{\\delta}}{\\lambda_{\\mathrm{NM}}} - 1\n$$\nSubstituting the expressions for the rates:\n$$\nE = \\frac{k/\\gamma}{k/(\\gamma + k\\,\\tau_f)} - 1 = \\frac{\\gamma + k\\,\\tau_f}{\\gamma} - 1 = \\left(1 + \\frac{k\\,\\tau_f}{\\gamma}\\right) - 1\n$$\nThis simplifies to:\n$$\nE = \\frac{k\\,\\tau_f}{\\gamma}\n$$\nFinally, we express this result in terms of the given timescales $\\tau_f$ and $\\tau_s = \\gamma/k$. We note that $k/\\gamma = 1/\\tau_s$.\n$$\nE = \\left(\\frac{k}{\\gamma}\\right) \\tau_f = \\frac{\\tau_f}{\\tau_s}\n$$\n\n**Physical Interpretation**\nThe relative error $E = \\tau_f / \\tau_s$ is the ratio of the fast memory timescale $\\tau_f$ to the slow coarse-grained relaxation timescale $\\tau_s$. The Markovian approximation assumes an infinite separation of timescales, i.e., that the bath decorrelates instantaneously compared to the evolution of the slow variable ($\\tau_f \\to 0$). Our result quantifies the error incurred when this assumption is not perfectly met. If a certain relative error tolerance, say $\\epsilon$, is prescribed for the relaxation rate, the Markovian closure is considered to fail when $E > \\epsilon$, which translates to $\\tau_f / \\tau_s > \\epsilon$. This failure occurs when the memory persistence time $\\tau_f$ is a non-negligible fraction of the system's own relaxation time $\\tau_s$, violating the timescale separation principle.",
            "answer": "$$\\boxed{\\frac{\\tau_f}{\\tau_s}}$$"
        }
    ]
}