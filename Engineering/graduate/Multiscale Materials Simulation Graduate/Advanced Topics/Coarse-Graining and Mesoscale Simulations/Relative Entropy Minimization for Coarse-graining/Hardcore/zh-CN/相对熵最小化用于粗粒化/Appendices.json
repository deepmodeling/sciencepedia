{
    "hands_on_practices": [
        {
            "introduction": "基于梯度的优化方法是解决相对熵最小化问题的核心工具。因此，能否从第一性原理出发，推导并计算Kullback-Leibler散度的梯度与Hessian矩阵，是掌握该方法的关键技能。本练习  将针对一个通用的离散粗粒化模型，引导你推导这些基本量，并揭示Hessian矩阵与粗粒化特征协方差之间深刻而优美的联系。",
            "id": "3838736",
            "problem": "考虑一种材料的粗粒化模型，其中微观构型已被映射到由 $i \\in \\{1,\\dots,N\\}$ 索引的有限离散粗粒状态集。每个粗粒状态 $i$ 由一个特征向量 $\\boldsymbol{\\phi}_i \\in \\mathbb{R}^d$ 描述。定义一个参数化的粗粒化势能为 $U_{\\boldsymbol{\\theta}}(i) = \\boldsymbol{\\theta} \\cdot \\boldsymbol{\\phi}_i$，其中 $\\boldsymbol{\\theta} \\in \\mathbb{R}^d$ 是待优化的参数。在逆温度 $\\beta = 1/(k_{\\mathrm{B}} T)$ 下，粗粒状态上的模型分布是玻尔兹曼分布 $p_{\\boldsymbol{\\theta}}(i) = \\exp(-\\beta U_{\\boldsymbol{\\theta}}(i)) / Z(\\boldsymbol{\\theta})$，其配分函数为 $Z(\\boldsymbol{\\theta}) = \\sum_{j=1}^N \\exp(-\\beta U_{\\boldsymbol{\\theta}}(j))$。这些状态上的目标分布 $q(i)$ 是通过原子模拟的统计稳健映射得到的，在此视为给定。优化目标是相对熵最小化 (REM)，即最小化 Kullback-Leibler 散度 $D_{\\mathrm{KL}}(q \\| p_{\\boldsymbol{\\theta}})$。\n\n从玻尔兹曼分布、配分函数和 Kullback-Leibler 散度的基本定义出发，\n- $p_{\\boldsymbol{\\theta}}(i) = \\exp(-\\beta U_{\\boldsymbol{\\theta}}(i)) / Z(\\boldsymbol{\\theta})$,\n- $Z(\\boldsymbol{\\theta}) = \\sum_{j=1}^N \\exp(-\\beta U_{\\boldsymbol{\\theta}}(j))$,\n- $D_{\\mathrm{KL}}(q \\| p_{\\boldsymbol{\\theta}}) = \\sum_{i=1}^N q(i) \\log\\left( \\frac{q(i)}{p_{\\boldsymbol{\\theta}}(i)} \\right)$,\n从第一性原理推导梯度 $\\nabla_{\\boldsymbol{\\theta}} D_{\\mathrm{KL}}(q \\| p_{\\boldsymbol{\\theta}})$ 和 Hessian 矩阵 $\\nabla^2_{\\boldsymbol{\\theta}} D_{\\mathrm{KL}}(q \\| p_{\\boldsymbol{\\theta}})$ 的表达式，用关于 $q$ 和 $p_{\\boldsymbol{\\theta}}$ 的期望值表示。推导过程必须仅依赖于这些定义以及微分和对数的标准性质；不要假设或引用任何快捷公式。\n\n然后，实现一个程序，使用推导出的公式为下面的每个测试用例计算 $D_{\\mathrm{KL}}(q \\| p_{\\boldsymbol{\\theta}})$ 的梯度和 Hessian 矩阵。通过适当的指数归一化，对大的 $\\beta$ 值使用数值稳定的方法计算 $p_{\\boldsymbol{\\theta}}$。输出中无需报告物理单位；所有量都是无量纲的。\n\n测试套件：\n- 测试用例 1 (一般情况)：\n  - $N = 5$, $d = 3$,\n  - $\\Phi = \\begin{bmatrix}\n  0.1  0.5  -0.3 \\\\\n  0.0  -0.2  0.8 \\\\\n  1.2  0.7  0.4 \\\\\n  -0.6  0.9  -1.1 \\\\\n  0.3  -0.4  0.2\n  \\end{bmatrix}$,\n  - $\\boldsymbol{q} = \\begin{bmatrix}0.05 \\\\ 0.15 \\\\ 0.40 \\\\ 0.30 \\\\ 0.10 \\end{bmatrix}$,\n  - $\\beta = 2.0$,\n  - $\\boldsymbol{\\theta} = \\begin{bmatrix}0.7 \\\\ -0.3 \\\\ 0.5 \\end{bmatrix}$.\n\n- 测试用例 2 (边界情况，共线特征导致秩亏协方差)：\n  - $N = 4$, $d = 2$,\n  - $\\Phi = \\begin{bmatrix}\n  1.0  2.0 \\\\\n  2.0  4.0 \\\\\n  3.0  6.0 \\\\\n  4.0  8.0\n  \\end{bmatrix}$,\n  - $\\boldsymbol{q} = \\begin{bmatrix}0.25 \\\\ 0.25 \\\\ 0.25 \\\\ 0.25 \\end{bmatrix}$,\n  - $\\beta = 1.0$,\n  - $\\boldsymbol{\\theta} = \\begin{bmatrix}0.0 \\\\ 0.0 \\end{bmatrix}$.\n\n- 测试用例 3 (边缘情况，大的 $\\beta$ 值导致 $p_{\\boldsymbol{\\theta}}$ 分布集中)：\n  - $N = 6$, $d = 2$,\n  - $\\Phi = \\begin{bmatrix}\n  -1.0  0.5 \\\\\n  0.2  -0.1 \\\\\n  0.0  0.0 \\\\\n  0.8  1.2 \\\\\n  -0.5  2.0 \\\\\n  1.5  -1.0\n  \\end{bmatrix}$,\n  - $\\boldsymbol{q} = \\begin{bmatrix}0.10 \\\\ 0.20 \\\\ 0.10 \\\\ 0.10 \\\\ 0.30 \\\\ 0.20 \\end{bmatrix}$,\n  - $\\beta = 50.0$,\n  - $\\boldsymbol{\\theta} = \\begin{bmatrix}0.05 \\\\ -0.02 \\end{bmatrix}$.\n\n您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果。顶层列表的每个元素对应一个测试用例，并且必须是一个浮点数列表。对于一个给定的特征维度为 $d$ 的测试用例，该列表必须按以下顺序包含 $d + d^2$ 个数字：\n- 梯度 $\\nabla_{\\boldsymbol{\\theta}} D_{\\mathrm{KL}}(q \\| p_{\\boldsymbol{\\theta}})$ 的 $d$ 个分量，\n- 后接按行主序展开的 Hessian 矩阵 $\\nabla^2_{\\boldsymbol{\\theta}} D_{\\mathrm{KL}}(q \\| p_{\\boldsymbol{\\theta}})$ 的 $d^2$ 个分量。\n\n例如，如果一个测试用例的 $d=2$，则每个用例的列表必须有 $2 + 4 = 6$ 个数字：$[\\text{grad}_1, \\text{grad}_2, \\text{H}_{11}, \\text{H}_{12}, \\text{H}_{21}, \\text{H}_{22}]$。打印的输出必须是包含上述三个测试用例的此类列表的列表，并按给定顺序排列。",
            "solution": "我们从粗粒化统计模型的定义元素开始。粗粒化势能为 $U_{\\boldsymbol{\\theta}}(i) = \\boldsymbol{\\theta} \\cdot \\boldsymbol{\\phi}_i$，其中 $\\boldsymbol{\\theta} \\in \\mathbb{R}^d$ 且 $\\boldsymbol{\\phi}_i \\in \\mathbb{R}^d$。在逆温度 $\\beta$ 下，离散集 $\\{1,\\dots,N\\}$ 上的玻尔兹曼分布为\n$$\np_{\\boldsymbol{\\theta}}(i) = \\frac{\\exp\\left(-\\beta U_{\\boldsymbol{\\theta}}(i)\\right)}{Z(\\boldsymbol{\\theta})}\n= \\frac{\\exp\\left(-\\beta \\boldsymbol{\\theta} \\cdot \\boldsymbol{\\phi}_i\\right)}{\\sum_{j=1}^N \\exp\\left(-\\beta \\boldsymbol{\\theta} \\cdot \\boldsymbol{\\phi}_j\\right)},\n$$\n其配分函数为\n$$\nZ(\\boldsymbol{\\theta}) = \\sum_{j=1}^N \\exp\\left(-\\beta \\boldsymbol{\\theta} \\cdot \\boldsymbol{\\phi}_j\\right).\n$$\n从目标分布 $q$ 到模型的 Kullback-Leibler 散度（相对熵）为\n$$\nD_{\\mathrm{KL}}(q \\| p_{\\boldsymbol{\\theta}}) = \\sum_{i=1}^N q(i) \\log\\left(\\frac{q(i)}{p_{\\boldsymbol{\\theta}}(i)}\\right).\n$$\n\n我们使用 $p_{\\boldsymbol{\\theta}}$ 的定义展开 $D_{\\mathrm{KL}}(q \\| p_{\\boldsymbol{\\theta}})$。首先，注意到\n$$\n\\log p_{\\boldsymbol{\\theta}}(i) = -\\beta \\boldsymbol{\\theta} \\cdot \\boldsymbol{\\phi}_i - \\log Z(\\boldsymbol{\\theta}).\n$$\n因此，\n\\begin{align*}\nD_{\\mathrm{KL}}(q \\| p_{\\boldsymbol{\\theta}})\n= \\sum_{i=1}^N q(i) \\log q(i) - \\sum_{i=1}^N q(i) \\log p_{\\boldsymbol{\\theta}}(i) \\\\\n= \\sum_{i=1}^N q(i) \\log q(i) - \\sum_{i=1}^N q(i)\\left(-\\beta \\boldsymbol{\\theta} \\cdot \\boldsymbol{\\phi}_i - \\log Z(\\boldsymbol{\\theta})\\right) \\\\\n= \\underbrace{\\sum_{i=1}^N q(i) \\log q(i)}_{\\text{与 } \\boldsymbol{\\theta} \\text{ 无关的常数}} + \\beta \\sum_{i=1}^N q(i)\\,\\boldsymbol{\\theta} \\cdot \\boldsymbol{\\phi}_i + \\bigg(\\sum_{i=1}^N q(i)\\bigg)\\log Z(\\boldsymbol{\\theta}).\n\\end{align*}\n因为 $\\sum_{i=1}^N q(i) = 1$，这可以简化为\n$$\nD_{\\mathrm{KL}}(q \\| p_{\\boldsymbol{\\theta}}) = \\text{const} + \\beta\\, \\boldsymbol{\\theta} \\cdot \\mathbb{E}_{q}[\\boldsymbol{\\phi}] + \\log Z(\\boldsymbol{\\theta}),\n$$\n其中 $\\mathbb{E}_{q}[\\boldsymbol{\\phi}] = \\sum_{i=1}^N q(i)\\,\\boldsymbol{\\phi}_i$。\n\n我们现在计算关于 $\\boldsymbol{\\theta}$ 的梯度。常数项的导数为零，线性项的导数则简单地是\n$$\n\\nabla_{\\boldsymbol{\\theta}} \\left( \\beta\\, \\boldsymbol{\\theta} \\cdot \\mathbb{E}_{q}[\\boldsymbol{\\phi}] \\right)\n= \\beta\\, \\mathbb{E}_{q}[\\boldsymbol{\\phi}].\n$$\n对于配分函数项，我们使用 $\\log Z(\\boldsymbol{\\theta})$ 梯度的标准恒等式：\n\\begin{align*}\n\\nabla_{\\boldsymbol{\\theta}} \\log Z(\\boldsymbol{\\theta})\n= \\frac{1}{Z(\\boldsymbol{\\theta})} \\nabla_{\\boldsymbol{\\theta}} Z(\\boldsymbol{\\theta})\n= \\frac{1}{Z(\\boldsymbol{\\theta})} \\sum_{j=1}^N \\nabla_{\\boldsymbol{\\theta}} \\exp\\left(-\\beta \\boldsymbol{\\theta} \\cdot \\boldsymbol{\\phi}_j\\right) \\\\\n= \\frac{1}{Z(\\boldsymbol{\\theta})} \\sum_{j=1}^N \\left(-\\beta \\boldsymbol{\\phi}_j\\right)\\exp\\left(-\\beta \\boldsymbol{\\theta} \\cdot \\boldsymbol{\\phi}_j\\right)\n= -\\beta \\sum_{j=1}^N p_{\\boldsymbol{\\theta}}(j)\\,\\boldsymbol{\\phi}_j \\\\\n= -\\beta\\, \\mathbb{E}_{p_{\\boldsymbol{\\theta}}}[\\boldsymbol{\\phi}].\n\\end{align*}\n结合这些结果，得到梯度\n$$\n\\nabla_{\\boldsymbol{\\theta}} D_{\\mathrm{KL}}(q \\| p_{\\boldsymbol{\\theta}})\n= \\beta\\, \\mathbb{E}_{q}[\\boldsymbol{\\phi}] - \\beta\\, \\mathbb{E}_{p_{\\boldsymbol{\\theta}}}[\\boldsymbol{\\phi}]\n= \\beta\\left(\\mathbb{E}_{q}[\\boldsymbol{\\phi}] - \\mathbb{E}_{p_{\\boldsymbol{\\theta}}}[\\boldsymbol{\\phi}]\\right).\n$$\n\n对于 Hessian 矩阵，我们对梯度进行微分。由于 $q$ 是固定的，项 $\\beta\\, \\mathbb{E}_{q}[\\boldsymbol{\\phi}]$ 在 $\\boldsymbol{\\theta}$ 上是常数，因此其导数为零。我们必须对 $- \\beta\\, \\mathbb{E}_{p_{\\boldsymbol{\\theta}}}[\\boldsymbol{\\phi}]$ 求关于 $\\boldsymbol{\\theta}$ 的导数。让我们逐分量计算二阶导数矩阵。用 $\\phi_{i,k}$ 表示 $\\boldsymbol{\\phi}_i$ 的第 $k$ 个分量。$\\mathbb{E}_{p_{\\boldsymbol{\\theta}}}[\\boldsymbol{\\phi}]$ 的第 $k$ 个分量是 $\\sum_{i=1}^N p_{\\boldsymbol{\\theta}}(i) \\phi_{i,k}$。其关于 $\\theta_\\ell$ 的导数为\n\\begin{align*}\n\\frac{\\partial}{\\partial \\theta_\\ell} \\left( \\sum_{i=1}^N p_{\\boldsymbol{\\theta}}(i) \\phi_{i,k} \\right)\n= \\sum_{i=1}^N \\phi_{i,k} \\frac{\\partial p_{\\boldsymbol{\\theta}}(i)}{\\partial \\theta_\\ell}.\n\\end{align*}\n根据定义，\n$$\np_{\\boldsymbol{\\theta}}(i) = \\frac{\\exp(-\\beta \\boldsymbol{\\theta} \\cdot \\boldsymbol{\\phi}_i)}{Z(\\boldsymbol{\\theta})},\n$$\n我们有\n\\begin{align*}\n\\frac{\\partial p_{\\boldsymbol{\\theta}}(i)}{\\partial \\theta_\\ell}\n= \\frac{-\\beta \\phi_{i,\\ell}\\exp(-\\beta \\boldsymbol{\\theta} \\cdot \\boldsymbol{\\phi}_i)}{Z(\\boldsymbol{\\theta})}\n- \\frac{\\exp(-\\beta \\boldsymbol{\\theta} \\cdot \\boldsymbol{\\phi}_i)}{Z(\\boldsymbol{\\theta})^2}\n\\frac{\\partial Z(\\boldsymbol{\\theta})}{\\partial \\theta_\\ell} \\\\\n= -\\beta \\phi_{i,\\ell} p_{\\boldsymbol{\\theta}}(i)\n- p_{\\boldsymbol{\\theta}}(i)\\left( -\\beta \\sum_{j=1}^N p_{\\boldsymbol{\\theta}}(j)\\phi_{j,\\ell} \\right) \\\\\n= -\\beta p_{\\boldsymbol{\\theta}}(i)\\left( \\phi_{i,\\ell} - \\sum_{j=1}^N p_{\\boldsymbol{\\theta}}(j)\\phi_{j,\\ell} \\right).\n\\end{align*}\n因此，\n\\begin{align*}\n\\frac{\\partial}{\\partial \\theta_\\ell} \\left( \\sum_{i=1}^N p_{\\boldsymbol{\\theta}}(i) \\phi_{i,k} \\right)\n= \\sum_{i=1}^N \\phi_{i,k} \\left[ -\\beta p_{\\boldsymbol{\\theta}}(i)\\left( \\phi_{i,\\ell} - \\sum_{j=1}^N p_{\\boldsymbol{\\theta}}(j)\\phi_{j,\\ell} \\right) \\right] \\\\\n= -\\beta \\left[ \\sum_{i=1}^N p_{\\boldsymbol{\\theta}}(i) \\phi_{i,k} \\phi_{i,\\ell} - \\left( \\sum_{i=1}^N p_{\\boldsymbol{\\theta}}(i) \\phi_{i,k} \\right)\\left( \\sum_{j=1}^N p_{\\boldsymbol{\\theta}}(j)\\phi_{j,\\ell} \\right) \\right].\n\\end{align*}\n这是 $\\beta$ 乘以 $\\boldsymbol{\\phi}$ 在 $p_{\\boldsymbol{\\theta}}$ 分布下协方差的 $(k,\\ell)$ 分量的负值。因此，Hessian 矩阵是\n$$\n\\nabla^2_{\\boldsymbol{\\theta}} D_{\\mathrm{KL}}(q \\| p_{\\boldsymbol{\\theta}})\n= \\beta^2\\, \\mathrm{Cov}_{p_{\\boldsymbol{\\theta}}}(\\boldsymbol{\\phi}),\n$$\n其中\n$$\n\\mathrm{Cov}_{p_{\\boldsymbol{\\theta}}}(\\boldsymbol{\\phi}) = \\mathbb{E}_{p_{\\boldsymbol{\\theta}}}[\\boldsymbol{\\phi}\\boldsymbol{\\phi}^\\top] - \\mathbb{E}_{p_{\\boldsymbol{\\theta}}}[\\boldsymbol{\\phi}]\\, \\mathbb{E}_{p_{\\boldsymbol{\\theta}}}[\\boldsymbol{\\phi}]^\\top.\n$$\n\n计算的算法设计：\n- 使用数值稳定的类 softmax 变换计算模型概率 $p_{\\boldsymbol{\\theta}}(i)$。具体来说，令 $s_i = -\\beta\\, \\boldsymbol{\\theta} \\cdot \\boldsymbol{\\phi}_i$。从所有 $s_i$ 中减去 $\\max_i s_i$ 以防止溢出，然后取指数得到未归一化的权重，再除以它们的和进行归一化。\n- 计算 $\\mathbb{E}_{q}[\\boldsymbol{\\phi}] = \\sum_{i=1}^N q(i)\\,\\boldsymbol{\\phi}_i$ 和 $\\mathbb{E}_{p_{\\boldsymbol{\\theta}}}[\\boldsymbol{\\phi}] = \\sum_{i=1}^N p_{\\boldsymbol{\\theta}}(i)\\,\\boldsymbol{\\phi}_i$。\n- 构建梯度 $\\boldsymbol{g} = \\beta (\\mathbb{E}_{q}[\\boldsymbol{\\phi}] - \\mathbb{E}_{p_{\\boldsymbol{\\theta}}}[\\boldsymbol{\\phi}])$。\n- 计算二阶矩 $\\mathbb{E}_{p_{\\boldsymbol{\\theta}}}[\\boldsymbol{\\phi}\\boldsymbol{\\phi}^\\top] = \\sum_{i=1}^N p_{\\boldsymbol{\\theta}}(i)\\,\\boldsymbol{\\phi}_i \\boldsymbol{\\phi}_i^\\top$ 和协方差 $\\mathrm{Cov}_{p_{\\boldsymbol{\\theta}}}(\\boldsymbol{\\phi})$，然后计算 Hessian 矩阵 $\\boldsymbol{H} = \\beta^2 \\mathrm{Cov}_{p_{\\boldsymbol{\\theta}}}(\\boldsymbol{\\phi})$。\n- 将 Hessian 矩阵按行主序展开，并将其连接在梯度之后，形成长度为 $d + d^2$ 的每个用例的输出列表。\n\n数值稳定性考虑：\n- 对于大的 $\\beta$ 值，未归一化的权重 $\\exp(s_i)$ 可能非常小或非常大。通过将 $s_i$ 减去它们的最大值，可以确保最大的指数变为 $\\exp(0) = 1$，而其他指数都 $\\le 1$，从而防止溢出并保持比例。\n- 在特征完全共线的边界情况下，协方差（以及 Hessian 矩阵）是秩亏的；这是预期的，并代表目标函数中曲率为零的方向。\n\n该实现直接遵循这些推导出的表达式，并为每个测试用例生成所需格式的梯度和 Hessian 矩阵分量。由于在这个离散统计设置中所有量本质上都是无量纲的，因此不需要物理单位。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef stable_boltzmann_probs(beta: float, theta: np.ndarray, Phi: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute p_theta(i) proportional to exp(-beta * theta dot phi_i) in a numerically stable way.\n    Phi: shape (N, d)\n    theta: shape (d,)\n    Returns p: shape (N,)\n    \"\"\"\n    # s_i = -beta * (theta · phi_i)\n    s = -beta * (Phi @ theta)\n    # Shift by max for numerical stability\n    s_shift = s - np.max(s)\n    w = np.exp(s_shift)\n    Z = np.sum(w)\n    # Normalize to get probabilities\n    p = w / Z\n    return p\n\ndef grad_hess_rem(beta: float, theta: np.ndarray, Phi: np.ndarray, q: np.ndarray):\n    \"\"\"\n    Compute gradient and Hessian of D_KL(q || p_theta) for discrete states with features Phi.\n    beta: scalar\n    theta: (d,)\n    Phi: (N, d)\n    q: (N,)\n    Returns:\n      grad: (d,)\n      hess: (d, d)\n    \"\"\"\n    # Model probabilities\n    p = stable_boltzmann_probs(beta, theta, Phi)\n    # Expectations under q and p\n    # E_q[phi] and E_p[phi]\n    E_q_phi = Phi.T @ q  # shape (d,)\n    E_p_phi = Phi.T @ p  # shape (d,)\n    # Gradient\n    grad = beta * (E_q_phi - E_p_phi)\n    # Second moment under p: sum_i p_i * phi_i phi_i^T\n    # Compute weighted outer products efficiently\n    # Shape (d, d)\n    # Use broadcasting: (N, d) * (N, 1) -> (N, d), then Phi.T @ (weighted Phi) to obtain sum_i p_i phi_i phi_i^T\n    weighted_Phi = Phi * p[:, None]\n    E_p_phi_phiT = Phi.T @ weighted_Phi\n    # Covariance under p\n    cov_p = E_p_phi_phiT - np.outer(E_p_phi, E_p_phi)\n    # Hessian\n    hess = (beta ** 2) * cov_p\n    return grad, hess\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case: (beta, theta, Phi, q)\n    test_cases = []\n\n    # Test Case 1\n    beta1 = 2.0\n    theta1 = np.array([0.7, -0.3, 0.5], dtype=float)\n    Phi1 = np.array([\n        [0.1, 0.5, -0.3],\n        [0.0, -0.2, 0.8],\n        [1.2, 0.7, 0.4],\n        [-0.6, 0.9, -1.1],\n        [0.3, -0.4, 0.2]\n    ], dtype=float)\n    q1 = np.array([0.05, 0.15, 0.40, 0.30, 0.10], dtype=float)\n\n    test_cases.append((beta1, theta1, Phi1, q1))\n\n    # Test Case 2 (collinear features, rank-deficient covariance)\n    beta2 = 1.0\n    theta2 = np.array([0.0, 0.0], dtype=float)\n    Phi2 = np.array([\n        [1.0, 2.0],\n        [2.0, 4.0],\n        [3.0, 6.0],\n        [4.0, 8.0]\n    ], dtype=float)\n    q2 = np.array([0.25, 0.25, 0.25, 0.25], dtype=float)\n\n    test_cases.append((beta2, theta2, Phi2, q2))\n\n    # Test Case 3 (large beta)\n    beta3 = 50.0\n    theta3 = np.array([0.05, -0.02], dtype=float)\n    Phi3 = np.array([\n        [-1.0,  0.5],\n        [ 0.2, -0.1],\n        [ 0.0,  0.0],\n        [ 0.8,  1.2],\n        [-0.5,  2.0],\n        [ 1.5, -1.0]\n    ], dtype=float)\n    q3 = np.array([0.10, 0.20, 0.10, 0.10, 0.30, 0.20], dtype=float)\n\n    test_cases.append((beta3, theta3, Phi3, q3))\n\n    results = []\n    for beta, theta, Phi, q in test_cases:\n        grad, hess = grad_hess_rem(beta, theta, Phi, q)\n        # Flatten result: gradient components followed by Hessian flattened row-major.\n        per_case = grad.tolist() + hess.flatten(order='C').tolist()\n        results.append(per_case)\n\n    # Final print statement in the exact required format.\n    # Print a single top-level list of per-case lists.\n    print(f\"[{','.join('[' + ','.join(map(str, case)) + ']' for case in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "掌握了目标函数及其导数这一核心工具后，我们便可以将其应用于一个完整的粗粒化问题中。本练习  将带领你经历一个典型的粗粒化流程：从一个连续的一维系统（双阱势中的粒子）出发，定义粗粒化映射，计算目标概率分布，并最终利用正则化的相对熵最小化方法，为一个离散模型找到最优参数。这个实践将理论与数值计算紧密结合，展示了如何从头开始构建并求解一个粗粒化模型。",
            "id": "3802808",
            "problem": "考虑一个一维粒子，其位置 $x \\in \\mathbb{R}$ 在一个由双势阱给出的势能面中演化。粗粒化 (CG) 映射将连续位置 $x$ 划分到有限数量的离散区间中。目标是构建一个离散的 CG 模型，并通过最小化正则化的 Kullback-Leibler 散度 (KLD) 来确定其最优对数概率。您编写的程序必须从第一性原理和经过充分测试的定义出发，实现以下步骤。\n\n1. 基本的基准分布和目标分布。设双势阱为 $U(x) = a\\,(x^2 - b^2)^2$，其中 $a > 0$ 和 $b > 0$ 是参数。在逆温度 $\\beta > 0$ 下的热平衡分布是玻尔兹曼分布\n$$\np(x) = \\frac{1}{Z}\\,\\exp\\left(-\\beta\\,U(x)\\right),\n$$\n其配分函数为\n$$\nZ = \\int_{x_{\\min}}^{x_{\\max}} \\exp\\left(-\\beta\\,U(x)\\right)\\,\\mathrm{d}x,\n$$\n其中 $[x_{\\min}, x_{\\max}]$ 是一个有限定义域，已知对于所考虑的参数，该域基本上包含了 $p(x)$ 的所有概率质量。\n\n2. 到离散区间的粗粒化映射。将区间 $[x_{\\min}, x_{\\max}]$ 划分成 $K$ 个等宽的区间，其边界为 $x_0 = x_{\\min},  x_1,  \\cdots,  x_K = x_{\\max}$。CG 变量 $i \\in \\{1,\\dots,K\\}$ 表示区间的索引。对于区间 $i$，从细粒度到粗粒度的边际概率是\n$$\nP_i = \\int_{x_{i-1}}^{x_{i}} p(x)\\,\\mathrm{d}x,\n$$\n这定义了一个离散的概率质量函数 $\\mathbf{P} = (P_1,\\dots,P_K)$，满足 $\\sum_{i=1}^{K} P_i = 1$ 和 $P_i \\ge 0$。\n\n3. 离散模型参数化。通过无约束的对数概率 $\\mathbf{l} = (l_1,\\dots,l_K)$，根据 softmax 关系对 CG 模型分布 $\\mathbf{Q} = (Q_1,\\dots,Q_K)$ 进行参数化\n$$\nQ_i(\\mathbf{l}) = \\frac{\\exp(l_i)}{\\sum_{j=1}^{K} \\exp(l_j)}.\n$$\n这确保了 $Q_i \\ge 0$ 和 $\\sum_{i=1}^K Q_i = 1$。\n\n4. 带正则化的相对熵最小化目标。定义从细粒度 CG 边际分布 $\\mathbf{P}$ 到模型 $\\mathbf{Q}(\\mathbf{l})$ 的 Kullback-Leibler 散度 (KLD) 为\n$$\nD_{\\mathrm{KL}}(\\mathbf{P}\\,\\|\\,\\mathbf{Q}(\\mathbf{l})) = \\sum_{i=1}^{K} P_i \\log\\left(\\frac{P_i}{Q_i(\\mathbf{l})}\\right).\n$$\n为防止过拟合并打破 $\\mathbf{l}$ 的规范不变性，引入一个相对于参考对数概率向量 $\\mathbf{l}^{\\mathrm{ref}}$ 的二次正则化，其强度为 $\\lambda > 0$。完整的优化目标是\n$$\n\\mathcal{F}(\\mathbf{l}) = D_{\\mathrm{KL}}(\\mathbf{P}\\,\\|\\,\\mathbf{Q}(\\mathbf{l})) + \\lambda \\sum_{i=1}^{K} \\left(l_i - l_i^{\\mathrm{ref}}\\right)^2.\n$$\n您必须在 $\\mathbf{l} \\in \\mathbb{R}^K$ 上最小化 $\\mathcal{F}(\\mathbf{l})$，以计算最优对数概率 $\\mathbf{l}^\\star$。\n\n5. 数值计算要求。\n- 通过在每个区间上对 $\\exp(-\\beta U(x))$ 进行数值积分，并在 $[x_{\\min}, x_{\\max}]$ 上计算配分函数 $Z$ 来确保正确归一化，从而计算区间概率 $\\mathbf{P}$。使用足够精确的数值求积方法。\n- 使用稳健的、基于梯度的凸优化程序来最小化 $\\mathcal{F}(\\mathbf{l})$。您必须明确构建目标函数 $\\mathcal{F}(\\mathbf{l})$ 及其关于 $\\mathbf{l}$ 的梯度，并在优化器中使用它们。\n\n6. 测试套件。实现您的程序，以计算以下测试用例的最优对数概率 $\\mathbf{l}^\\star$，在指定的定义域上使用等宽区间和给定的参考对数概率。对于每个案例，将最优对数概率向量 $\\mathbf{l}^\\star$ 输出为十进制浮点数列表。输出不需要物理单位。\n- 案例 A（理想情况）：$a = 1.0$, $b = 1.0$, $\\beta = 5.0$, $x_{\\min} = -2.5$, $x_{\\max} = 2.5$, $K = 5$, $\\lambda = 0.1$, $\\mathbf{l}^{\\mathrm{ref}} = (0,0,0,0,0)$。\n- 案例 B（接近均匀的边际分布，正则化非常弱）：$a = 1.0$, $b = 1.0$, $\\beta = 0.1$, $x_{\\min} = -3.0$, $x_{\\max} = 3.0$, $K = 4$, $\\lambda = 10^{-6}$, $\\mathbf{l}^{\\mathrm{ref}} = (0,0,0,0)$。\n- 案例 C（强正则化与有偏的参考）：$a = 1.0$, $b = 1.0$, $\\beta = 2.0$, $x_{\\min} = -2.0$, $x_{\\max} = 2.0$, $K = 6$, $\\lambda = 10.0$, $\\mathbf{l}^{\\mathrm{ref}} = (-1,0,1,2,1,0)$。\n- 案例 D（低温，尖锐双峰）：$a = 1.0$, $b = 1.0$, $\\beta = 50.0$, $x_{\\min} = -2.0$, $x_{\\max} = 2.0$, $K = 8$, $\\lambda = 0.01$, $\\mathbf{l}^{\\mathrm{ref}} = (0,0,0,0,0,0,0,0)$。\n\n7. 最终输出格式。您的程序应生成单行输出，其中包含上述四个案例的结果列表，格式如下：一个由方括号括起来的、逗号分隔的浮点数列表的列表，每个内部列表对应一个案例的 $\\mathbf{l}^\\star$，并且不包含任何附加文本。例如：\n$$\n\\text{print } [[l^\\star_{A,1},\\dots,l^\\star_{A,K_A}],[l^\\star_{B,1},\\dots,l^\\star_{B,K_B}],\\dots].\n$$\n所有角度（如有）必须以弧度为单位；然而，此问题中没有出现角度。输出必须是十进制浮点数。输出的任何地方都不允许出现百分号。数值积分和优化必须以足够的精度执行，以产生稳定的结果。",
            "solution": "该问题要求通过最小化正则化的相对熵（Kullback-Leibler 散度）来确定粗粒化 (CG) 模型的最优对数概率。该过程涉及多个步骤，从定义底层的连续系统到为离散模型参数执行数值优化。\n\n### 步骤1：目标概率密度与粗粒化\n\n该系统是在双势阱 $U(x)$ 中的一个一维粒子，其表达式为：\n$$\nU(x) = a(x^2 - b^2)^2\n$$\n其中 $a > 0$ 且 $b > 0$。在逆温度为 $\\beta > 0$ 的热平衡状态下，粒子位置 $x$ 在指定定义域 $[x_{\\min}, x_{\\max}]$ 上遵循玻尔兹曼分布：\n$$\np(x) = \\frac{1}{Z} \\exp(-\\beta U(x))\n$$\n归一化常数，即配分函数，是未归一化的玻尔兹曼因子在该定义域上的积分：\n$$\nZ = \\int_{x_{\\min}}^{x_{\\max}} \\exp(-\\beta U(x)) \\, \\mathrm{d}x\n$$\n粗粒化过程将连续定义域 $[x_{\\min}, x_{\\max}]$ 映射为 $K$ 个离散区间的集合。该区间由 $K+1$ 个等距点 $x_0, x_1, \\dots, x_K$ 划分，其中 $x_0 = x_{\\min}$ 且 $x_K = x_{\\max}$。每个区间的宽度为 $\\Delta x = (x_{\\max} - x_{\\min}) / K$，第 $i$ 个区间对应于 $[x_{i-1}, x_i]$。\n\nCG模型的目标概率质量函数，记为 $\\mathbf{P} = (P_1, \\dots, P_K)$，是通过将细粒度概率密度 $p(x)$ 在每个区间上积分得到的：\n$$\nP_i = \\int_{x_{i-1}}^{x_i} p(x) \\, \\mathrm{d}x = \\frac{1}{Z} \\int_{x_{i-1}}^{x_i} \\exp(-\\beta U(x)) \\, \\mathrm{d}x\n$$\n这些积分必须通过数值方法计算。一个稳健的自适应求积方法，例如 `scipy.integrate.quad` 提供的，适合此任务。计算过程首先计算 $Z$，然后计算每个区间 $i$ 的积分并除以 $Z$。\n\n### 步骤2：粗粒化模型与目标函数\n\nCG 模型分布 $\\mathbf{Q} = (Q_1, \\dots, Q_K)$ 由一个无约束的对数概率向量 $\\mathbf{l} = (l_1, \\dots, l_K)$ 参数化。softmax 函数确保 $\\mathbf{Q}$ 是一个有效的概率分布（即 $Q_i \\ge 0$ 且 $\\sum_i Q_i = 1$）：\n$$\nQ_i(\\mathbf{l}) = \\frac{\\exp(l_i)}{\\sum_{j=1}^{K} \\exp(l_j)}\n$$\n如果 $l_j$ 的值很大或很小，$\\sum_{j=1}^{K} \\exp(l_j)$ 这一项在数值上可能不稳定。它的对数是 log-sum-exp 函数，$\\text{logsumexp}(\\mathbf{l}) = \\log(\\sum_j \\exp(l_j))$，可以以一种稳定的方式进行计算。利用这一点，我们可以写出 $\\log Q_i(\\mathbf{l}) = l_i - \\text{logsumexp}(\\mathbf{l})$。\n\n目标是找到最优参数 $\\mathbf{l}^\\star$，使模型分布 $\\mathbf{Q}(\\mathbf{l})$ 尽可能接近目标分布 $\\mathbf{P}$。这种“接近度”由 Kullback-Leibler 散度 (KLD) 衡量：\n$$\nD_{\\mathrm{KL}}(\\mathbf{P} \\,\\|\\, \\mathbf{Q}(\\mathbf{l})) = \\sum_{i=1}^{K} P_i \\log\\left(\\frac{P_i}{Q_i(\\mathbf{l})}\\right) = \\sum_{i=1}^{K} P_i (\\log P_i - \\log Q_i(\\mathbf{l}))\n$$\n问题指定在目标函数中加入一个二次正则化项，该项惩罚 $\\mathbf{l}$ 与参考向量 $\\mathbf{l}^{\\mathrm{ref}}$ 的偏离。待最小化的完整目标函数为：\n$$\n\\mathcal{F}(\\mathbf{l}) = D_{\\mathrm{KL}}(\\mathbf{P} \\,\\|\\, \\mathbf{Q}(\\mathbf{l})) + \\lambda \\sum_{i=1}^{K} (l_i - l_i^{\\mathrm{ref}})^2\n$$\n其中 $\\lambda > 0$ 是正则化强度。当 $\\lambda > 0$ 时，该目标函数 $\\mathcal{F}(\\mathbf{l})$ 是严格凸的，保证存在唯一的最小化子 $\\mathbf{l}^\\star$。\n\n### 步骤3：基于梯度的优化\n\n为了有效地最小化 $\\mathcal{F}(\\mathbf{l})$，我们使用基于梯度的优化算法，例如 L-BFGS-B。这需要计算 $\\mathcal{F}(\\mathbf{l})$ 相对于 $\\mathbf{l}$ 的每个分量 $l_k$ 的梯度。\n\n让我们推导梯度 $\\nabla_{\\mathbf{l}} \\mathcal{F}(\\mathbf{l})$。我们将 $\\mathcal{F}(\\mathbf{l})$ 对分量 $l_k$ 求导：\n$$\n\\frac{\\partial \\mathcal{F}}{\\partial l_k} = \\frac{\\partial}{\\partial l_k} \\left( \\sum_{i=1}^{K} P_i \\log P_i - \\sum_{i=1}^{K} P_i \\log Q_i(\\mathbf{l}) \\right) + \\frac{\\partial}{\\partial l_k} \\left( \\lambda \\sum_{i=1}^{K} (l_i - l_i^{\\mathrm{ref}})^2 \\right)\n$$\n$\\sum P_i \\log P_i$ 项相对于 $\\mathbf{l}$ 是常数。正则化项的导数很简单：\n$$\n\\frac{\\partial}{\\partial l_k} \\left( \\lambda \\sum_{i=1}^{K} (l_i - l_i^{\\mathrm{ref}})^2 \\right) = 2\\lambda (l_k - l_k^{\\mathrm{ref}})\n$$\n现在我们关注涉及 $\\mathbf{Q}(\\mathbf{l})$ 的 KLD 项。回顾 $\\log Q_i(\\mathbf{l}) = l_i - \\text{logsumexp}(\\mathbf{l})$，其对目标的贡献为：\n$$\n-\\sum_{i=1}^{K} P_i (l_i - \\text{logsumexp}(\\mathbf{l})) = -\\sum_{i=1}^{K} P_i l_i + \\left(\\sum_{i=1}^{K} P_i\\right) \\text{logsumexp}(\\mathbf{l}) = -\\sum_{i=1}^{K} P_i l_i + \\text{logsumexp}(\\mathbf{l})\n$$\n这里我们使用了 $\\sum_i P_i = 1$ 的事实。此表达式关于 $l_k$ 的导数为：\n$$\n\\frac{\\partial}{\\partial l_k} \\left( -\\sum_{i=1}^{K} P_i l_i + \\text{logsumexp}(\\mathbf{l}) \\right) = -P_k + \\frac{\\partial}{\\partial l_k} \\log\\left(\\sum_{j=1}^{K} \\exp(l_j)\\right)\n$$\nlog-sum-exp 函数关于 $l_k$ 的导数已知为 $Q_k(\\mathbf{l})$：\n$$\n\\frac{\\partial}{\\partial l_k} \\log\\left(\\sum_{j=1}^{K} \\exp(l_j)\\right) = \\frac{1}{\\sum_{j} \\exp(l_j)} \\cdot \\exp(l_k) = Q_k(\\mathbf{l})\n$$\n因此，KLD 项的导数是 $Q_k(\\mathbf{l}) - P_k$。综合所有部分，梯度的第 $k$ 个分量是：\n$$\n\\frac{\\partial \\mathcal{F}}{\\partial l_k} = Q_k(\\mathbf{l}) - P_k + 2\\lambda (l_k - l_k^{\\mathrm{ref}})\n$$\n用向量表示，完整的梯度是：\n$$\n\\nabla_{\\mathbf{l}} \\mathcal{F}(\\mathbf{l}) = \\mathbf{Q}(\\mathbf{l}) - \\mathbf{P} + 2\\lambda (\\mathbf{l} - \\mathbf{l}^{\\mathrm{ref}})\n$$\n\n### 步骤4：算法实现\n\n针对每个测试用例的总体算法如下：\n1.  定义势能 $U(x)$、被积函数 $f(x) = \\exp(-\\beta U(x))$ 和定义域 $[x_{\\min}, x_{\\max}]$。\n2.  使用数值求积计算配分函数 $Z = \\int_{x_{\\min}}^{x_{\\max}} f(x) \\, \\mathrm{d}x$。\n3.  在 $[x_{\\min}, x_{\\max}]$ 上建立 $K$ 个区间的边界 $x_0, \\dots, x_K$。\n4.  通过在每个区间 $[x_{i-1}, x_i]$ 上对 $f(x)$ 进行数值积分并除以 $Z$，计算目标概率向量 $\\mathbf{P}$。\n5.  实现一个函数，该函数以 $\\mathbf{l}$ 为参数，返回目标值 $\\mathcal{F}(\\mathbf{l})$ 及其梯度 $\\nabla_{\\mathbf{l}} \\mathcal{F}(\\mathbf{l})$。此函数应使用上面推导的表达式和数值稳定的函数（如 `logsumexp`）。\n6.  使用数值优化器（如 `scipy.optimize.minimize` 的 `L-BFGS-B` 方法）找到最小化 $\\mathcal{F}(\\mathbf{l})$ 的向量 $\\mathbf{l}^\\star$。将步骤5中的函数以及一个初始猜测（例如 $\\mathbf{l}_0 = \\mathbf{l}^{\\mathrm{ref}}$）提供给优化器。\n7.  优化的结果 $\\mathbf{l}^\\star$ 是给定测试用例的解。对所有提供的测试用例重复此过程。",
            "answer": "```python\nimport numpy as np\nfrom scipy.integrate import quad\nfrom scipy.optimize import minimize\nfrom scipy.special import logsumexp\n\ndef solve():\n    \"\"\"\n    Computes the optimal log-probabilities for a coarse-grained model by minimizing\n    a regularized Kullback-Leibler divergence.\n    \"\"\"\n    test_cases = [\n        # Case A\n        {'a': 1.0, 'b': 1.0, 'beta': 5.0, 'xmin': -2.5, 'xmax': 2.5,\n         'K': 5, 'lam': 0.1, 'l_ref': [0.0, 0.0, 0.0, 0.0, 0.0]},\n        # Case B\n        {'a': 1.0, 'b': 1.0, 'beta': 0.1, 'xmin': -3.0, 'xmax': 3.0,\n         'K': 4, 'lam': 1e-6, 'l_ref': [0.0, 0.0, 0.0, 0.0]},\n        # Case C\n        {'a': 1.0, 'b': 1.0, 'beta': 2.0, 'xmin': -2.0, 'xmax': 2.0,\n         'K': 6, 'lam': 10.0, 'l_ref': [-1.0, 0.0, 1.0, 2.0, 1.0, 0.0]},\n        # Case D\n        {'a': 1.0, 'b': 1.0, 'beta': 50.0, 'xmin': -2.0, 'xmax': 2.0,\n         'K': 8, 'lam': 0.01, 'l_ref': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]},\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        a = case['a']\n        b = case['b']\n        beta = case['beta']\n        xmin = case['xmin']\n        xmax = case['xmax']\n        K = case['K']\n        lam = case['lam']\n        l_ref = np.array(case['l_ref'])\n\n        # 1. Define potential and integrand\n        def U(x):\n            return a * (x**2 - b**2)**2\n\n        def integrand(x):\n            return np.exp(-beta * U(x))\n\n        # 2. Compute partition function Z\n        Z, _ = quad(integrand, xmin, xmax, epsabs=1e-12, epsrel=1e-12)\n        if Z == 0:\n            # This case should not happen with the given parameters\n            raise ValueError(\"Partition function Z is zero.\")\n            \n        # 3. Compute target marginal probabilities P\n        bin_edges = np.linspace(xmin, xmax, K + 1)\n        P = np.zeros(K)\n        for i in range(K):\n            bin_integral, _ = quad(integrand, bin_edges[i], bin_edges[i+1], epsabs=1e-12, epsrel=1e-12)\n            P[i] = bin_integral / Z\n        \n        # Ensure P is a valid probability distribution\n        P[P  0] = 0\n        P /= np.sum(P)\n\n        # 4. Define objective function and its gradient\n        # Handle the P_i * log(P_i) term, where the result is 0 if P_i = 0\n        P_log_P = np.zeros_like(P)\n        non_zero_mask = P > 0\n        P_log_P[non_zero_mask] = P[non_zero_mask] * np.log(P[non_zero_mask])\n        dkl_const_term = np.sum(P_log_P)\n\n        def objective_and_grad(l, P_vec, lam_val, l_ref_vec, dkl_const):\n            \"\"\"\n            Computes the objective function and its gradient.\n            F(l) = D_KL(P||Q(l)) + lambda * ||l - l_ref||^2\n            \"\"\"\n            # Compute Q(l) using logsumexp for stability\n            lse = logsumexp(l)\n            log_Q = l - lse\n            Q = np.exp(log_Q)\n\n            # Objective Function F(l)\n            # D_KL = sum(P * (logP - logQ))\n            dkl = dkl_const - np.sum(P_vec * log_Q)\n            reg = lam_val * np.sum((l - l_ref_vec)**2)\n            obj_val = dkl + reg\n\n            # Gradient of F(l)\n            # grad(F) = Q - P + 2*lambda*(l - l_ref)\n            grad_vec = Q - P_vec + 2 * lam_val * (l - l_ref_vec)\n            \n            return obj_val, grad_vec\n\n        # 5. Perform the optimization\n        l_initial = np.copy(l_ref)\n        \n        result = minimize(\n            fun=objective_and_grad,\n            x0=l_initial,\n            args=(P, lam, l_ref, dkl_const_term),\n            method='L-BFGS-B',\n            jac=True,  # function returns both objective and gradient\n            options={'gtol': 1e-9, 'disp': False}\n        )\n\n        l_star = result.x\n        all_results.append(l_star.tolist())\n\n    # 6. Format the final output string\n    # E.g., [[-4.7,-0.7,0.3,-0.7,-4.7],[-0.1,0.0,0.0,-0.1]]\n    output_parts = []\n    for res_list in all_results:\n        # Format each list as '[val1,val2,...]'\n        formatted_list = f'[{\",\".join(map(str, res_list))}]'\n        output_parts.append(formatted_list)\n    \n    final_output = f\"[{','.join(output_parts)}]\"\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "相对熵最小化（REM）是参数化粗粒化模型的几种主要思想之一。为了更深刻地理解其独特属性，将其与力匹配（Force Matching, FM）等其他流行方法进行比较是非常有益的。本分析练习  构建了一个巧妙的场景，在该场景下，通过REM和FM推导出的最优参数有所不同。这个对比鲜明地揭示了每种方法背后所固有的基本假设和偏好，有助于我们根据具体问题选择最合适的粗粒化策略。",
            "id": "3456643",
            "problem": "考虑一个一维粗粒化变量 $q$，其已被无量纲化，使得 $q$ 的原子级参考平衡分布是标准正态玻尔兹曼分布 $p_{\\mathrm{ref}}(q) = (2\\pi)^{-1/2} \\exp(-q^{2}/2)$，对应于参考势 $U_{\\mathrm{ref}}(q) = q^{2}/2$。您需要寻找一个具有非线性四次势 $U_{\\theta}(q) = \\theta\\, q^{4}$ 的自顶向下粗粒化模型。\n\n两种常见的自顶向下参数估计准则是相对熵最小化 (REM) 和力匹配 (FM)。在 REM 中，需要最小化从参考玻尔兹曼分布到模型玻尔兹曼分布 $p_{\\theta}(q) \\propto \\exp(-U_{\\theta}(q))$ 的 Kullback–Leibler 散度。在 FM 中，需要最小化模型力与从参考动力学中采样的带噪声的瞬时力之间的均方差。假设瞬时力数据服从随机模型\n$$\n\\mathcal{F}(q) \\equiv -\\frac{\\partial U_{\\mathrm{ref}}}{\\partial q}(q) + s\\, q^{3} + \\varepsilon = -q + s\\, q^{3} + \\varepsilon,\n$$\n其中 $s$ 是一个固定的无量纲振幅，用于捕捉由被消除的自由度引起的状态依赖（异方差）噪声，而 $\\varepsilon$ 是一个零均值残差，满足 $\\mathbb{E}[\\varepsilon \\mid q] = 0$，且独立于 $q$ 并具有有限方差。FM 的目标是在参考分布下的均方误差，\n$$\nJ(\\theta) = \\mathbb{E}_{p_{\\mathrm{ref}}}\\!\\left[\\left(-\\frac{\\partial U_{\\theta}}{\\partial q}(q) - \\mathcal{F}(q)\\right)^{2}\\right].\n$$\n\n仅从上述定义和高斯分布 $p_{\\mathrm{ref}}(q)$ 的标准性质出发，完成以下任务：\n\n1. 推导 $\\theta$ 的 REM 一阶最优性条件，并精确求解以获得 $\\theta_{\\mathrm{REM}}$。\n2. 在给定的带噪声的力模型下，推导 $\\theta$ 的 FM 正规方程，并精确求解以获得作为 $s$ 函数的 $\\theta_{\\mathrm{FM}}$。\n\n您必须明确地计算所有需要的积分，并从第一性原理出发论证每一步。将您的最终答案以单行矩阵的形式给出，其中包含两个优化后的参数值 $\\theta_{\\mathrm{REM}}$ 和 $\\theta_{\\mathrm{FM}}$ 的精确解析形式。不需要进行数值舍入，也不需要单位，因为所有量根据构造都是无量纲的。",
            "solution": "#### 第 1 部分：相对熵最小化 (REM)\n\n目标是找到参数 $\\theta_{\\mathrm{REM}}$，以最小化从参考分布 $p_{\\mathrm{ref}}$ 到模型分布 $p_{\\theta}$ 的相对熵（或 Kullback-Leibler 散度）。相对熵定义为：\n$$\nS_{\\mathrm{KL}}(p_{\\mathrm{ref}} || p_{\\theta}) = \\int_{-\\infty}^{\\infty} p_{\\mathrm{ref}}(q) \\ln\\left(\\frac{p_{\\mathrm{ref}}(q)}{p_{\\theta}(q)}\\right) dq\n$$\n我们可以展开对数项：\n$$\nS_{\\mathrm{KL}}(p_{\\mathrm{ref}} || p_{\\theta}) = \\int_{-\\infty}^{\\infty} p_{\\mathrm{ref}}(q) \\ln(p_{\\mathrm{ref}}(q)) dq - \\int_{-\\infty}^{\\infty} p_{\\mathrm{ref}}(q) \\ln(p_{\\theta}(q)) dq\n$$\n第一项 $\\int p_{\\mathrm{ref}}(q) \\ln(p_{\\mathrm{ref}}(q)) dq$ 是参考分布的负熵，它是一个相对于模型参数 $\\theta$ 的常数。因此，最小化 $S_{\\mathrm{KL}}$ 等价于最大化第二项 $\\int p_{\\mathrm{ref}}(q) \\ln(p_{\\theta}(q)) dq$。\n\n模型分布为 $p_{\\theta}(q) = \\frac{1}{Z_{\\theta}} \\exp(-U_{\\theta}(q))$，其中 $Z_{\\theta} = \\int_{-\\infty}^{\\infty} \\exp(-U_{\\theta}(q')) dq'$ 是配分函数。因此，$\\ln(p_{\\theta}(q)) = -U_{\\theta}(q) - \\ln(Z_{\\theta})$。\n最大化 $\\int p_{\\mathrm{ref}}(q) \\ln(p_{\\theta}(q)) dq$ 等价于最小化其负值：\n$$\nL(\\theta) = -\\int_{-\\infty}^{\\infty} p_{\\mathrm{ref}}(q) \\ln(p_{\\theta}(q)) dq = \\int_{-\\infty}^{\\infty} p_{\\mathrm{ref}}(q) [U_{\\theta}(q) + \\ln(Z_{\\theta})] dq\n$$\n$$\nL(\\theta) = \\mathbb{E}_{p_{\\mathrm{ref}}}[U_{\\theta}(q)] + \\ln(Z_{\\theta})\n$$\n代入 $U_{\\theta}(q) = \\theta q^{4}$：\n$$\nL(\\theta) = \\mathbb{E}_{p_{\\mathrm{ref}}}[\\theta q^{4}] + \\ln\\left(\\int_{-\\infty}^{\\infty} \\exp(-\\theta (q')^{4}) dq'\\right) = \\theta \\langle q^{4} \\rangle_{p_{\\mathrm{ref}}} + \\ln(Z_{\\theta})\n$$\n通过将 $L(\\theta)$ 对 $\\theta$ 的导数设为零，可以找到一阶最优性条件：\n$$\n\\frac{dL}{d\\theta} = \\langle q^{4} \\rangle_{p_{\\mathrm{ref}}} + \\frac{1}{Z_{\\theta}} \\frac{dZ_{\\theta}}{d\\theta} = 0\n$$\n我们计算配分函数的导数：\n$$\n\\frac{dZ_{\\theta}}{d\\theta} = \\frac{d}{d\\theta} \\int_{-\\infty}^{\\infty} \\exp(-\\theta q^{4}) dq = \\int_{-\\infty}^{\\infty} (-q^{4}) \\exp(-\\theta q^{4}) dq\n$$\n除以 $Z_{\\theta}$ 得到 $-q^{4}$ 在模型分布 $p_{\\theta}$ 下的期望值：\n$$\n\\frac{1}{Z_{\\theta}} \\frac{dZ_{\\theta}}{d\\theta} = -\\frac{\\int_{-\\infty}^{\\infty} q^{4} \\exp(-\\theta q^{4}) dq}{\\int_{-\\infty}^{\\infty} \\exp(-\\theta q^{4}) dq} = -\\langle q^{4} \\rangle_{p_{\\theta}}\n$$\n因此，最优性条件简化为矩匹配条件：\n$$\n\\langle q^{4} \\rangle_{p_{\\mathrm{ref}}} - \\langle q^{4} \\rangle_{p_{\\theta}} = 0 \\implies \\langle q^{4} \\rangle_{p_{\\mathrm{ref}}} = \\langle q^{4} \\rangle_{p_{\\theta}}\n$$\n我们现在计算这两个期望值。\n对于参考分布 $p_{\\mathrm{ref}}(q)$，即标准正态分布 $N(0, 1)$，其 n 阶矩 $\\langle q^n \\rangle$ 可以通过高斯随机变量 $X \\sim N(0, \\sigma^2)$ 的性质 $\\mathbb{E}[X^{2k}] = (2k-1)!! \\sigma^{2k}$ 来找到。这里，$\\sigma=1$，我们需要四阶矩（$k=2$）：\n$$\n\\langle q^{4} \\rangle_{p_{\\mathrm{ref}}} = (2 \\cdot 2 - 1)!! = 3!! = 3 \\times 1 = 3\n$$\n对于模型分布 $p_{\\theta}(q)$，期望值为：\n$$\n\\langle q^{4} \\rangle_{p_{\\theta}} = \\frac{\\int_{-\\infty}^{\\infty} q^{4} \\exp(-\\theta q^{4}) dq}{\\int_{-\\infty}^{\\infty} \\exp(-\\theta q^{4}) dq}\n$$\n这些积分可以使用伽马函数积分公式 $\\int_{0}^{\\infty} x^{n-1} e^{-ax^k} dx = \\frac{1}{k} a^{-n/k} \\Gamma(n/k)$ 求解。由于被积函数是偶函数，我们可以写成 $\\int_{-\\infty}^{\\infty} (\\cdot) dq = 2 \\int_{0}^{\\infty} (\\cdot) dq$。\n分子：$2 \\int_{0}^{\\infty} q^{4} \\exp(-\\theta q^{4}) dq$。这里，$n-1=4 \\Rightarrow n=5$，$k=4$，$a=\\theta$。\n$$\n\\int_{-\\infty}^{\\infty} q^{4} \\exp(-\\theta q^{4}) dq = 2 \\left( \\frac{1}{4} \\theta^{-5/4} \\Gamma(5/4) \\right) = \\frac{1}{2} \\theta^{-5/4} \\Gamma(5/4)\n$$\n分母：$2 \\int_{0}^{\\infty} q^{0} \\exp(-\\theta q^{4}) dq$。这里，$n-1=0 \\Rightarrow n=1$，$k=4$，$a=\\theta$。\n$$\n\\int_{-\\infty}^{\\infty} \\exp(-\\theta q^{4}) dq = 2 \\left( \\frac{1}{4} \\theta^{-1/4} \\Gamma(1/4) \\right) = \\frac{1}{2} \\theta^{-1/4} \\Gamma(1/4)\n$$\n期望值是这两个结果的比值：\n$$\n\\langle q^{4} \\rangle_{p_{\\theta}} = \\frac{\\frac{1}{2} \\theta^{-5/4} \\Gamma(5/4)}{\\frac{1}{2} \\theta^{-1/4} \\Gamma(1/4)} = \\theta^{-1} \\frac{\\Gamma(5/4)}{\\Gamma(1/4)}\n$$\n使用性质 $\\Gamma(z+1)=z\\Gamma(z)$，我们有 $\\Gamma(5/4) = \\Gamma(1/4 + 1) = \\frac{1}{4}\\Gamma(1/4)$。\n$$\n\\langle q^{4} \\rangle_{p_{\\theta}} = \\theta^{-1} \\frac{\\frac{1}{4}\\Gamma(1/4)}{\\Gamma(1/4)} = \\frac{1}{4\\theta}\n$$\n将这些期望值代入最优性条件：\n$$\n3 = \\frac{1}{4\\theta_{\\mathrm{REM}}} \\implies \\theta_{\\mathrm{REM}} = \\frac{1}{12}\n$$\n\n#### 第 2 部分：力匹配 (FM)\n\nFM 的目标是找到 $\\theta_{\\mathrm{FM}}$，以最小化模型力与瞬时力之间的均方误差，该误差在参考分布上进行平均：\n$$\nJ(\\theta) = \\mathbb{E}_{p_{\\mathrm{ref}}}\\!\\left[\\left(-\\frac{\\partial U_{\\theta}}{\\partial q}(q) - \\mathcal{F}(q)\\right)^{2}\\right]\n$$\n模型力为 $F_{\\theta}(q) = -\\frac{\\partial U_{\\theta}}{\\partial q}(q) = -\\frac{d}{dq}(\\theta q^{4}) = -4\\theta q^{3}$。\n瞬时力为 $\\mathcal{F}(q) = -q + s\\, q^{3} + \\varepsilon$。\n将这些代入目标函数：\n$$\nJ(\\theta) = \\mathbb{E}_{p_{\\mathrm{ref}}}\\!\\left[\\left(-4\\theta q^{3} - (-q + s q^{3} + \\varepsilon)\\right)^{2}\\right] = \\mathbb{E}_{p_{\\mathrm{ref}}}\\!\\left[\\left(q - (4\\theta + s) q^{3} - \\varepsilon\\right)^{2}\\right]\n$$\n为了最小化 $J(\\theta)$，我们将其关于 $\\theta$ 的导数设为零（正规方程）：\n$$\n\\frac{dJ}{d\\theta} = \\frac{d}{d\\theta} \\mathbb{E}_{p_{\\mathrm{ref}}}\\!\\left[\\left(q - (4\\theta + s) q^{3} - \\varepsilon\\right)^{2}\\right] = 0\n$$\n交换期望和微分的顺序：\n$$\n\\frac{dJ}{d\\theta} = \\mathbb{E}_{p_{\\mathrm{ref}}}\\!\\left[ \\frac{d}{d\\theta} \\left(q - (4\\theta + s) q^{3} - \\varepsilon\\right)^{2} \\right]\n$$\n使用链式法则：\n$$\n\\frac{dJ}{d\\theta} = \\mathbb{E}_{p_{\\mathrm{ref}}}\\!\\left[ 2 \\left(q - (4\\theta + s) q^{3} - \\varepsilon\\right) \\cdot (-4 q^{3}) \\right] = 0\n$$\n忽略常数因子 $-8$，正规方程为：\n$$\n\\mathbb{E}_{p_{\\mathrm{ref}}}\\!\\left[ \\left(q - (4\\theta + s) q^{3} - \\varepsilon\\right) q^{3} \\right] = 0\n$$\n利用期望的线性性质：\n$$\n\\mathbb{E}_{p_{\\mathrm{ref}}}\\![q^{4}] - (4\\theta + s) \\mathbb{E}_{p_{\\mathrm{ref}}}\\![q^{6}] - \\mathbb{E}_{p_{\\mathrm{ref}}}\\![\\varepsilon q^{3}] = 0\n$$\n我们计算涉及 $\\varepsilon$ 的期望值。利用全期望定律和给定条件 $\\mathbb{E}[\\varepsilon \\mid q] = 0$：\n$$\n\\mathbb{E}_{p_{\\mathrm{ref}}}\\![\\varepsilon q^{3}] = \\mathbb{E}_{p_{\\mathrm{ref}}}\\![\\mathbb{E}[\\varepsilon q^{3} \\mid q]] = \\mathbb{E}_{p_{\\mathrm{ref}}}\\![q^{3} \\mathbb{E}[\\varepsilon \\mid q]] = \\mathbb{E}_{p_{\\mathrm{ref}}}\\![q^{3} \\cdot 0] = 0\n$$\n正规方程简化为：\n$$\n\\langle q^{4} \\rangle_{p_{\\mathrm{ref}}} - (4\\theta_{\\mathrm{FM}} + s) \\langle q^{6} \\rangle_{p_{\\mathrm{ref}}} = 0\n$$\n我们已经找到了 $\\langle q^{4} \\rangle_{p_{\\mathrm{ref}}} = 3$。我们需要计算六阶矩，$\\langle q^{6} \\rangle_{p_{\\mathrm{ref}}}$。对于标准正态分布，$k=3$：\n$$\n\\langle q^{6} \\rangle_{p_{\\mathrm{ref}}} = (2 \\cdot 3 - 1)!! = 5!! = 5 \\times 3 \\times 1 = 15\n$$\n将矩值代入方程：\n$$\n3 - (4\\theta_{\\mathrm{FM}} + s) \\cdot 15 = 0\n$$\n求解 $\\theta_{\\mathrm{FM}}$：\n$$\n3 = 15(4\\theta_{\\mathrm{FM}} + s)\n$$\n$$\n\\frac{3}{15} = 4\\theta_{\\mathrm{FM}} + s\n$$\n$$\n\\frac{1}{5} = 4\\theta_{\\mathrm{FM}} + s\n$$\n$$\n4\\theta_{\\mathrm{FM}} = \\frac{1}{5} - s\n$$\n$$\n\\theta_{\\mathrm{FM}} = \\frac{1}{4} \\left(\\frac{1}{5} - s\\right) = \\frac{1 - 5s}{20}\n$$\n两个优化后的参数值是 $\\theta_{\\mathrm{REM}} = \\frac{1}{12}$ 和 $\\theta_{\\mathrm{FM}} = \\frac{1-5s}{20}$。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{1}{12}  \\frac{1 - 5s}{20}\n\\end{pmatrix}\n}\n$$"
        }
    ]
}