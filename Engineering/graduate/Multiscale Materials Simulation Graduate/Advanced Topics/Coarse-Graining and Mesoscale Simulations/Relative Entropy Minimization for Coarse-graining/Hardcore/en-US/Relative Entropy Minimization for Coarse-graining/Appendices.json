{
    "hands_on_practices": [
        {
            "introduction": "To begin our hands-on exploration, we first build a theoretical foundation with an analytical exercise. This problem investigates how the mathematical procedure of coarse-graining, which involves integrating out degrees of freedom, gives rise to entropic contributions in the effective potential. By working through the derivation from first principles, you will see how relative entropy minimization can precisely recover the parameters of a coarse-grained model that correctly accounts for the underlying fine-grained physics, including the crucial role of the Jacobian in coordinate transformations .",
            "id": "3838743",
            "problem": "Consider a single particle in a $d$-dimensional Euclidean space $\\mathbb{R}^{d}$ with position vector $\\mathbf{r} \\in \\mathbb{R}^{d}$ and fine-grained canonical equilibrium probability density function (PDF) $P(\\mathbf{r}) \\propto \\exp(-\\beta U(\\mathbf{r}))$, where $\\beta = 1/(k_{B}T)$ is the inverse thermal energy with $k_{B}$ the Boltzmann constant and $T$ the absolute temperature, and $U(\\mathbf{r}) = \\frac{\\kappa}{2}\\|\\mathbf{r}\\|^{2}$ for a fixed stiffness $\\kappa > 0$. Define a coarse-grained mapping operator $M:\\mathbb{R}^{d} \\to \\mathbb{R}_{+}$ by $\\xi = M(\\mathbf{r}) = \\|\\mathbf{r}\\|$, i.e., the radial coordinate. The coarse-grained reference PDF $p_{\\text{ref}}(\\xi)$ is obtained from the fine-grained $P(\\mathbf{r})$ by integrating out angular degrees of freedom. The transformation from Cartesian coordinates to hyperspherical coordinates induces a Jacobian factor $J(\\xi)$.\n\nYou will construct a one-dimensional coarse-grained model on $\\xi$ with flat measure $\\mathrm{d}\\xi$ using a parameterized coarse-grained potential $U_{\\text{cg}}(\\xi; k) = \\frac{k}{2}\\xi^{2} - (d-1)k_{B}T \\ln \\xi$, where $k > 0$ is an unknown stiffness parameter to be determined. The model PDF is $p_{\\theta}(\\xi) \\propto \\exp\\big(-\\beta U_{\\text{cg}}(\\xi; k)\\big)$.\n\nStarting from the Boltzmann distribution and the definition of the Kullback–Leibler divergence (KLD), also called relative entropy, \n$$D_{\\text{KL}}(p_{\\text{ref}} \\| p_{\\theta}) = \\int_{0}^{\\infty} p_{\\text{ref}}(\\xi)\\,\\ln\\left(\\frac{p_{\\text{ref}}(\\xi)}{p_{\\theta}(\\xi)}\\right)\\,\\mathrm{d}\\xi,$$\nperform the following tasks from first principles:\n- Derive the Jacobian factor $J(\\xi)$ associated with the mapping $M(\\mathbf{r}) = \\|\\mathbf{r}\\|$ and express $p_{\\text{ref}}(\\xi)$ in terms of $J(\\xi)$, $\\beta$, and $\\kappa$.\n- Express $p_{\\theta}(\\xi)$ in terms of $\\beta$, $k$, and $\\xi$.\n- Write $D_{\\text{KL}}(p_{\\text{ref}} \\| p_{\\theta})$ explicitly.\n- Determine the stiffness parameter $k$ that minimizes $D_{\\text{KL}}(p_{\\text{ref}} \\| p_{\\theta})$.\n\nProvide the final result as a single closed-form analytic expression for the minimizer $k$ in terms of the given quantities. No rounding is required.",
            "solution": "The problem is first validated to be scientifically sound, well-posed, and objective. It is a standard problem in statistical mechanics and coarse-graining theory, utilizing established principles such as the Boltzmann distribution and relative entropy minimization. All required information is provided, and the problem is free of contradictions or ambiguities.\n\nThe problem asks for a four-part derivation to determine the optimal stiffness parameter $k$ for a coarse-grained model. We will address each part systematically.\n\n**Part 1: Derivation of the Jacobian factor $J(\\xi)$ and the reference PDF $p_{\\text{ref}}(\\xi)$**\n\nThe fine-grained system is described by the canonical equilibrium probability density function (PDF) $P(\\mathbf{r})$ for a particle with position vector $\\mathbf{r} \\in \\mathbb{R}^{d}$. This PDF is given by the Boltzmann distribution:\n$$P(\\mathbf{r}) = C_{FG} \\exp(-\\beta U(\\mathbf{r}))$$\nwhere $C_{FG}$ is a normalization constant, $\\beta = 1/(k_{B}T)$ is the inverse thermal energy, and the potential is a $d$-dimensional harmonic well, $U(\\mathbf{r}) = \\frac{\\kappa}{2}\\|\\mathbf{r}\\|^{2}$.\n\nThe coarse-grained variable is the radial coordinate $\\xi = \\|\\mathbf{r}\\|$. To find the PDF for $\\xi$, we must change from Cartesian coordinates $\\mathbf{r}$ to hyperspherical coordinates. The volume element in hyperspherical coordinates is $\\mathrm{d}^d\\mathbf{r} = \\xi^{d-1} \\mathrm{d}\\xi \\mathrm{d}\\Omega_{d-1}$, where $\\mathrm{d}\\Omega_{d-1}$ is the differential solid angle in $d$ dimensions.\n\nThe probability of finding the particle in an infinitesimal volume element $\\mathrm{d}^d\\mathbf{r}$ is $P(\\mathbf{r})\\mathrm{d}^d\\mathbf{r}$. Expressed in hyperspherical coordinates, this becomes:\n$$P(\\xi) \\xi^{d-1} \\mathrm{d}\\xi \\mathrm{d}\\Omega_{d-1} = C_{FG} \\exp\\left(-\\frac{\\beta\\kappa}{2}\\xi^2\\right) \\xi^{d-1} \\mathrm{d}\\xi \\mathrm{d}\\Omega_{d-1}$$\nThe reference coarse-grained PDF, $p_{\\text{ref}}(\\xi)$, is obtained by integrating over all angular degrees of freedom:\n$$p_{\\text{ref}}(\\xi)\\mathrm{d}\\xi = \\left( \\int_{\\Omega_{d-1}} C_{FG} \\exp\\left(-\\frac{\\beta\\kappa}{2}\\xi^2\\right) \\xi^{d-1} \\mathrm{d}\\Omega_{d-1} \\right) \\mathrm{d}\\xi$$\nThe integrand is independent of the angular variables. The integral of $\\mathrm{d}\\Omega_{d-1}$ over the entire solid angle is the surface area of a unit $(d-1)$-sphere, denoted $S_{d} = \\frac{2\\pi^{d/2}}{\\Gamma(d/2)}$, where $\\Gamma$ is the Gamma function.\n$$p_{\\text{ref}}(\\xi)\\mathrm{d}\\xi = C_{FG} S_{d} \\xi^{d-1} \\exp\\left(-\\frac{\\beta\\kappa}{2}\\xi^2\\right) \\mathrm{d}\\xi$$\nThus, the reference PDF is:\n$$p_{\\text{ref}}(\\xi) = C_{\\text{ref}} \\xi^{d-1} \\exp\\left(-\\frac{\\beta\\kappa}{2}\\xi^2\\right)$$\nwhere $C_{\\text{ref}} = C_{FG} S_{d}$ is the new normalization constant for the coarse-grained distribution.\n\nThe problem identifies a \"Jacobian factor\" $J(\\xi)$ associated with the milking. This factor represents the geometric contribution to the measure arising from the dimensionality reduction. From the derivation above, the probability density for $\\xi$ is proportional to the fine-grained density evaluated at radius $\\xi$ multiplied by the surface area of the hypersphere of radius $\\xi$, which is proportional to $\\xi^{d-1}$. Therefore, the functional form of the Jacobian factor is $J(\\xi) \\propto \\xi^{d-1}$.\n\n**Part 2: Expression for the model PDF $p_{\\theta}(\\xi)$**\n\nThe coarse-grained model is defined by the potential $U_{\\text{cg}}(\\xi; k) = \\frac{k}{2}\\xi^{2} - (d-1)k_{B}T \\ln \\xi$. The corresponding model PDF, $p_{\\theta}(\\xi)$, is proportional to $\\exp(-\\beta U_{\\text{cg}}(\\xi; k))$:\n$$p_{\\theta}(\\xi) \\propto \\exp\\left(-\\beta \\left[ \\frac{k}{2}\\xi^{2} - (d-1)k_{B}T \\ln \\xi \\right]\\right)$$\nUsing the identity $\\beta k_B T = 1$, we can simplify the exponent:\n$$-\\beta U_{\\text{cg}}(\\xi; k) = -\\frac{\\beta k}{2}\\xi^{2} + \\beta(d-1)k_{B}T \\ln \\xi = -\\frac{\\beta k}{2}\\xi^{2} + (d-1) \\ln \\xi$$\nSubstituting this back into the expression for $p_{\\theta}(\\xi)$:\n$$p_{\\theta}(\\xi) \\propto \\exp\\left(-\\frac{\\beta k}{2}\\xi^{2} + (d-1) \\ln \\xi\\right) = \\exp\\left(-\\frac{\\beta k}{2}\\xi^{2}\\right) \\exp\\left(\\ln \\xi^{d-1}\\right)$$\nThis simplifies to:\n$$p_{\\theta}(\\xi) = C_{\\theta}(k) \\xi^{d-1} \\exp\\left(-\\frac{\\beta k}{2}\\xi^2\\right)$$\nwhere $C_{\\theta}(k)$ is the normalization constant for the model PDF, which depends on the parameter $k$.\n\n**Part 3: Explicit expression for $D_{\\text{KL}}(p_{\\text{ref}} \\| p_{\\theta})$**\n\nThe Kullback-Leibler divergence is defined as:\n$$D_{\\text{KL}}(p_{\\text{ref}} \\| p_{\\theta}) = \\int_{0}^{\\infty} p_{\\text{ref}}(\\xi)\\,\\ln\\left(\\frac{p_{\\text{ref}}(\\xi)}{p_{\\theta}(\\xi)}\\right)\\,\\mathrm{d}\\xi$$\nLet's first evaluate the logarithm term:\n$$\\frac{p_{\\text{ref}}(\\xi)}{p_{\\theta}(\\xi)} = \\frac{C_{\\text{ref}} \\xi^{d-1} \\exp(-\\frac{\\beta\\kappa}{2}\\xi^2)}{C_{\\theta}(k) \\xi^{d-1} \\exp(-\\frac{\\beta k}{2}\\xi^2)} = \\frac{C_{\\text{ref}}}{C_{\\theta}(k)} \\exp\\left(\\frac{\\beta}{2}(k-\\kappa)\\xi^2\\right)$$\n$$\\ln\\left(\\frac{p_{\\text{ref}}(\\xi)}{p_{\\theta}(\\xi)}\\right) = \\ln\\left(\\frac{C_{\\text{ref}}}{C_{\\theta}(k)}\\right) + \\frac{\\beta}{2}(k-\\kappa)\\xi^2$$\nSubstituting this into the KLD expression:\n$$D_{\\text{KL}}(k) = \\int_{0}^{\\infty} p_{\\text{ref}}(\\xi) \\left[ \\ln\\left(\\frac{C_{\\text{ref}}}{C_{\\theta}(k)}\\right) + \\frac{\\beta}{2}(k-\\kappa)\\xi^2 \\right] \\mathrm{d}\\xi$$\n$$D_{\\text{KL}}(k) = \\ln\\left(\\frac{C_{\\text{ref}}}{C_{\\theta}(k)}\\right) \\int_{0}^{\\infty} p_{\\text{ref}}(\\xi) \\mathrm{d}\\xi + \\frac{\\beta}{2}(k-\\kappa) \\int_{0}^{\\infty} \\xi^2 p_{\\text{ref}}(\\xi) \\mathrm{d}\\xi$$\nThe first integral is $1$ due to normalization of $p_{\\text{ref}}$. The second integral is the expectation value $\\langle \\xi^2 \\rangle_{p_{\\text{ref}}}$.\n$$D_{\\text{KL}}(k) = \\ln\\left(\\frac{C_{\\text{ref}}}{C_{\\theta}(k)}\\right) + \\frac{\\beta}{2}(k-\\kappa) \\langle \\xi^2 \\rangle_{p_{\\text{ref}}}$$\nTo proceed, we need the ratio of normalization constants and $\\langle \\xi^2 \\rangle_{p_{\\text{ref}}}$. We use the definite integral formula $\\int_{0}^{\\infty} x^n \\exp(-ax^2) \\mathrm{d}x = \\frac{\\Gamma((n+1)/2)}{2a^{(n+1)/2}}$.\nThe normalization constants are found from $\\int_0^\\infty p(\\xi) d\\xi = 1$:\n$$C_{\\text{ref}}^{-1} = \\int_{0}^{\\infty} \\xi^{d-1} \\exp(-\\frac{\\beta\\kappa}{2}\\xi^2)\\mathrm{d}\\xi = \\frac{\\Gamma(d/2)}{2(\\beta\\kappa/2)^{d/2}}$$\n$$C_{\\theta}(k)^{-1} = \\int_{0}^{\\infty} \\xi^{d-1} \\exp(-\\frac{\\beta k}{2}\\xi^2)\\mathrm{d}\\xi = \\frac{\\Gamma(d/2)}{2(\\beta k/2)^{d/2}}$$\nThe ratio is $\\frac{C_{\\text{ref}}}{C_{\\theta}(k)} = \\left(\\frac{\\kappa}{k}\\right)^{d/2}$, so $\\ln\\left(\\frac{C_{\\text{ref}}}{C_{\\theta}(k)}\\right) = \\frac{d}{2}\\ln\\left(\\frac{\\kappa}{k}\\right)$.\n\nNext, we calculate the expectation value:\n$$\\langle \\xi^2 \\rangle_{p_{\\text{ref}}} = C_{\\text{ref}} \\int_{0}^{\\infty} \\xi^{d+1} \\exp(-\\frac{\\beta\\kappa}{2}\\xi^2) \\mathrm{d}\\xi$$\nThe integral is $\\frac{\\Gamma((d+2)/2)}{2(\\beta\\kappa/2)^{(d+2)/2}} = \\frac{(d/2)\\Gamma(d/2)}{2(\\beta\\kappa/2)^{d/2+1}}$.\n$$\\langle \\xi^2 \\rangle_{p_{\\text{ref}}} = \\frac{2(\\beta\\kappa/2)^{d/2}}{\\Gamma(d/2)} \\cdot \\frac{(d/2)\\Gamma(d/2)}{2(\\beta\\kappa/2)^{d/2}(\\beta\\kappa/2)} = \\frac{d/2}{\\beta\\kappa/2} = \\frac{d}{\\beta\\kappa}$$\nThis result is consistent with the equipartition theorem. Substituting these into the expression for $D_{\\text{KL}}$:\n$$D_{\\text{KL}}(k) = \\frac{d}{2}\\ln\\left(\\frac{\\kappa}{k}\\right) + \\frac{\\beta}{2}(k-\\kappa) \\frac{d}{\\beta\\kappa} = \\frac{d}{2}\\ln\\left(\\frac{\\kappa}{k}\\right) + \\frac{d(k-\\kappa)}{2\\kappa}$$\nThis is the explicit expression for the KLD as a function of $k$:\n$$D_{\\text{KL}}(k) = \\frac{d}{2}(\\ln\\kappa - \\ln k) + \\frac{d}{2\\kappa}k - \\frac{d}{2}$$\n\n**Part 4: Determination of the optimal parameter $k$**\n\nTo find the value of $k$ that minimizes $D_{\\text{KL}}(k)$, we differentiate with respect to $k$ and set the result to zero:\n$$\\frac{\\mathrm{d}}{\\mathrm{d}k} D_{\\text{KL}}(k) = \\frac{\\mathrm{d}}{\\mathrm{d}k} \\left[ \\frac{d}{2}(\\ln\\kappa - \\ln k) + \\frac{d}{2\\kappa}k - \\frac{d}{2} \\right]$$\n$$\\frac{\\mathrm{d}}{\\mathrm{d}k} D_{\\text{KL}}(k) = \\frac{d}{2}\\left(-\\frac{1}{k}\\right) + \\frac{d}{2\\kappa} = \\frac{d}{2}\\left(\\frac{1}{\\kappa} - \\frac{1}{k}\\right)$$\nSetting the derivative to zero (for $d>0$):\n$$\\frac{1}{\\kappa} - \\frac{1}{k} = 0 \\implies \\frac{1}{k} = \\frac{1}{\\kappa} \\implies k = \\kappa$$\nTo confirm this is a minimum, we check the second derivative:\n$$\\frac{\\mathrm{d}^2}{\\mathrm{d}k^2} D_{\\text{KL}}(k) = \\frac{\\mathrm{d}}{\\mathrm{d}k} \\left[ \\frac{d}{2}\\left(\\frac{1}{\\kappa} - \\frac{1}{k}\\right) \\right] = \\frac{d}{2k^2}$$\nSince $d>0$ and $k>0$, the second derivative is strictly positive, which confirms that $k=\\kappa$ is a global minimum. This result is expected, as the KL divergence is minimized (to a value of $0$) when the model distribution $p_{\\theta}(\\xi)$ becomes identical to the reference distribution $p_{\\text{ref}}(\\xi)$, which occurs when $k=\\kappa$.\n\nThe stiffness parameter that minimizes the Kullback-Leibler divergence is $k=\\kappa$.",
            "answer": "$$\\boxed{\\kappa}$$"
        },
        {
            "introduction": "Moving from pure theory to numerical practice, this exercise challenges you to implement a complete coarse-graining workflow for a non-trivial system. You will start with a continuous one-dimensional system, discretize it into bins, and then use numerical optimization to find the best parameters for a coarse-grained model. This hands-on coding problem will solidify your understanding of how to apply relative entropy minimization in a practical setting, where target probabilities are derived from data and optimal parameters are found using gradient-based algorithms .",
            "id": "3802808",
            "problem": "Consider a one-dimensional particle with position $x \\in \\mathbb{R}$ evolving in a potential energy landscape given by a double-well potential. The coarse-graining (CG) map bins continuous positions $x$ into a finite number of discrete bins. The aim is to construct a discrete CG model and determine its optimal log-probabilities by minimizing a regularized Kullback-Leibler divergence (KLD). The program you write must implement the following steps, starting from first principles and well-tested definitions.\n\n1. Fundamental base and target distribution. Let the double-well potential be $U(x) = a\\,(x^2 - b^2)^2$, where $a > 0$ and $b > 0$ are parameters. The thermal equilibrium distribution at inverse temperature $\\beta > 0$ is the Boltzmann distribution\n$$\np(x) = \\frac{1}{Z}\\,\\exp\\left(-\\beta\\,U(x)\\right),\n$$\nwith partition function\n$$\nZ = \\int_{x_{\\min}}^{x_{\\max}} \\exp\\left(-\\beta\\,U(x)\\right)\\,\\mathrm{d}x,\n$$\nwhere $[x_{\\min}, x_{\\max}]$ is a finite domain known to contain essentially all the probability mass of $p(x)$ for the parameters under consideration.\n\n2. Coarse-graining map to discrete bins. Partition the interval $[x_{\\min}, x_{\\max}]$ into $K$ equal-width bins with edges $x_0 = x_{\\min}  x_1  \\cdots  x_K = x_{\\max}$. The CG variable $i \\in \\{1,\\dots,K\\}$ denotes the bin index. The fine-to-coarse marginal probability for bin $i$ is\n$$\nP_i = \\int_{x_{i-1}}^{x_{i}} p(x)\\,\\mathrm{d}x,\n$$\nwhich defines a discrete probability mass function $\\mathbf{P} = (P_1,\\dots,P_K)$ satisfying $\\sum_{i=1}^{K} P_i = 1$ and $P_i \\ge 0$.\n\n3. Discrete model parameterization. Parameterize the CG model distribution $\\mathbf{Q} = (Q_1,\\dots,Q_K)$ via unconstrained log-probabilities $\\mathbf{l} = (l_1,\\dots,l_K)$ according to the softmax relation\n$$\nQ_i(\\mathbf{l}) = \\frac{\\exp(l_i)}{\\sum_{j=1}^{K} \\exp(l_j)}.\n$$\nThis ensures $Q_i \\ge 0$ and $\\sum_{i=1}^K Q_i = 1$.\n\n4. Objective for relative entropy minimization with regularization. Define the Kullback-Leibler divergence (KLD) from the fine-grained CG marginal $\\mathbf{P}$ to the model $\\mathbf{Q}(\\mathbf{l})$ as\n$$\nD_{\\mathrm{KL}}(\\mathbf{P}\\,\\|\\,\\mathbf{Q}(\\mathbf{l})) = \\sum_{i=1}^{K} P_i \\log\\left(\\frac{P_i}{Q_i(\\mathbf{l})}\\right).\n$$\nTo prevent overfitting and break the gauge invariance of $\\mathbf{l}$, introduce a quadratic regularization relative to a reference log-probability vector $\\mathbf{l}^{\\mathrm{ref}}$, with strength $\\lambda > 0$. The full objective is\n$$\n\\mathcal{F}(\\mathbf{l}) = D_{\\mathrm{KL}}(\\mathbf{P}\\,\\|\\,\\mathbf{Q}(\\mathbf{l})) + \\lambda \\sum_{i=1}^{K} \\left(l_i - l_i^{\\mathrm{ref}}\\right)^2.\n$$\nYou must minimize $\\mathcal{F}(\\mathbf{l})$ over $\\mathbf{l} \\in \\mathbb{R}^K$ to compute the optimal log-probabilities $\\mathbf{l}^\\star$.\n\n5. Numerical computation requirements. \n- Compute the bin probabilities $\\mathbf{P}$ by numerical integration of $\\exp(-\\beta U(x))$ over each bin and by computing the partition function $Z$ over $[x_{\\min}, x_{\\max}]$ to ensure proper normalization. Use sufficiently accurate numerical quadrature.\n- Minimize $\\mathcal{F}(\\mathbf{l})$ using a robust, gradient-based convex optimization procedure. You must explicitly construct both the objective $\\mathcal{F}(\\mathbf{l})$ and its gradient with respect to $\\mathbf{l}$ and use them in the optimizer.\n\n6. Test suite. Implement your program to compute $\\mathbf{l}^\\star$ for the following test cases, with equal-width bins over the specified domains and reference log-probabilities as given. For each case, output the optimal log-probability vector $\\mathbf{l}^\\star$ as a list of decimal floats. No physical units are required for the outputs.\n- Case A (happy path): $a = 1.0$, $b = 1.0$, $\\beta = 5.0$, $x_{\\min} = -2.5$, $x_{\\max} = 2.5$, $K = 5$, $\\lambda = 0.1$, $\\mathbf{l}^{\\mathrm{ref}} = (0,0,0,0,0)$.\n- Case B (near-uniform marginal, very weak regularization): $a = 1.0$, $b = 1.0$, $\\beta = 0.1$, $x_{\\min} = -3.0$, $x_{\\max} = 3.0$, $K = 4$, $\\lambda = 10^{-6}$, $\\mathbf{l}^{\\mathrm{ref}} = (0,0,0,0)$.\n- Case C (strong regularization with a skewed reference): $a = 1.0$, $b = 1.0$, $\\beta = 2.0$, $x_{\\min} = -2.0$, $x_{\\max} = 2.0$, $K = 6$, $\\lambda = 10.0$, $\\mathbf{l}^{\\mathrm{ref}} = (-1,0,1,2,1,0)$.\n- Case D (low temperature, sharply bimodal): $a = 1.0$, $b = 1.0$, $\\beta = 50.0$, $x_{\\min} = -2.0$, $x_{\\max} = 2.0$, $K = 8$, $\\lambda = 0.01$, $\\mathbf{l}^{\\mathrm{ref}} = (0,0,0,0,0,0,0,0)$.\n\n7. Final output format. Your program should produce a single line of output containing the list of results for the above four cases, in the following format: a comma-separated list of lists of floats, enclosed in a single pair of square brackets, with each inner list corresponding to $\\mathbf{l}^\\star$ for one case, and with no additional text. For example:\n$$\n\\text{print } [[l^\\star_{A,1},\\dots,l^\\star_{A,K_A}],[l^\\star_{B,1},\\dots,l^\\star_{B,K_B}],\\dots].\n$$\nAll angles, if any, must be in radians; however, no angles appear in this problem. The outputs must be decimal floats. No percentage signs are permitted anywhere in the output. The numerical integrals and optimizations must be performed with sufficient precision to produce stable results.",
            "solution": "The problem requires the determination of optimal log-probabilities for a coarse-grained (CG) model by minimizing a regularized relative entropy (Kullback-Leibler divergence). The procedure involves several steps, from defining the underlying continuous system to performing a numerical optimization for the discrete model parameters.\n\n### Step 1: Target Probability Density and Coarse-Graining\n\nThe system is a one-dimensional particle in a double-well potential $U(x)$, given by:\n$$\nU(x) = a(x^2 - b^2)^2\n$$\nwhere $a > 0$ and $b > 0$. At thermal equilibrium with inverse temperature $\\beta > 0$, the particle's position $x$ follows the Boltzmann distribution over a specified domain $[x_{\\min}, x_{\\max}]$:\n$$\np(x) = \\frac{1}{Z} \\exp(-\\beta U(x))\n$$\nThe normalization constant, known as the partition function, is the integral of the unnormalized Boltzmann factor over this domain:\n$$\nZ = \\int_{x_{\\min}}^{x_{\\max}} \\exp(-\\beta U(x)) \\, \\mathrm{d}x\n$$\nThe coarse-graining procedure maps the continuous domain $[x_{\\min}, x_{\\max}]$ into a set of $K$ discrete bins. The interval is partitioned by $K+1$ equally spaced points $x_0, x_1, \\dots, x_K$ where $x_0 = x_{\\min}$ and $x_K = x_{\\max}$. The width of each bin is $\\Delta x = (x_{\\max} - x_{\\min}) / K$, and the $i$-th bin corresponds to the interval $[x_{i-1}, x_i]$.\n\nThe target probability mass function for the CG model, denoted by $\\mathbf{P} = (P_1, \\dots, P_K)$, is derived by integrating the fine-grained probability density $p(x)$ over each bin:\n$$\nP_i = \\int_{x_{i-1}}^{x_i} p(x) \\, \\mathrm{d}x = \\frac{1}{Z} \\int_{x_{i-1}}^{x_i} \\exp(-\\beta U(x)) \\, \\mathrm{d}x\n$$\nThese integrals must be computed numerically. A robust adaptive quadrature method, such as the one provided by `scipy.integrate.quad`, is appropriate for this task. The calculation proceeds by first computing $Z$ and then computing the integral for each bin $i$ and dividing by $Z$.\n\n### Step 2: The Coarse-Grained Model and Objective Function\n\nThe CG model distribution, $\\mathbf{Q} = (Q_1, \\dots, Q_K)$, is parameterized by a vector of unconstrained log-probabilities $\\mathbf{l} = (l_1, \\dots, l_K)$. The softmax function ensures that $\\mathbf{Q}$ is a valid probability distribution (i.e., $Q_i \\ge 0$ and $\\sum_i Q_i = 1$):\n$$\nQ_i(\\mathbf{l}) = \\frac{\\exp(l_i)}{\\sum_{j=1}^{K} \\exp(l_j)}\n$$\nThe term $\\sum_{j=1}^{K} \\exp(l_j)$ can be numerically unstable if the values of $l_j$ are large or small. Its logarithm is the log-sum-exp function, $\\text{logsumexp}(\\mathbf{l}) = \\log(\\sum_j \\exp(l_j))$, which can be computed in a stable manner. Using this, we can write $\\log Q_i(\\mathbf{l}) = l_i - \\text{logsumexp}(\\mathbf{l})$.\n\nThe goal is to find the optimal parameters $\\mathbf{l}^\\star$ that make the model distribution $\\mathbf{Q}(\\mathbf{l})$ as close as possible to the target distribution $\\mathbf{P}$. This \"closeness\" is measured by the Kullback-Leibler divergence (KLD):\n$$\nD_{\\mathrm{KL}}(\\mathbf{P} \\,\\|\\, \\mathbf{Q}(\\mathbf{l})) = \\sum_{i=1}^{K} P_i \\log\\left(\\frac{P_i}{Q_i(\\mathbf{l})}\\right) = \\sum_{i=1}^{K} P_i (\\log P_i - \\log Q_i(\\mathbf{l}))\n$$\nThe problem specifies adding a quadratic regularization term to the objective function, which penalizes deviations of $\\mathbf{l}$ from a reference vector $\\mathbf{l}^{\\mathrm{ref}}$. The complete objective function to be minimized is:\n$$\n\\mathcal{F}(\\mathbf{l}) = D_{\\mathrm{KL}}(\\mathbf{P} \\,\\|\\, \\mathbf{Q}(\\mathbf{l})) + \\lambda \\sum_{i=1}^{K} (l_i - l_i^{\\mathrm{ref}})^2\n$$\nwhere $\\lambda > 0$ is the regularization strength. This objective function $\\mathcal{F}(\\mathbf{l})$ is strictly convex for $\\lambda > 0$, guaranteeing a unique minimizer $\\mathbf{l}^\\star$.\n\n### Step 3: Gradient-Based Optimization\n\nTo minimize $\\mathcal{F}(\\mathbf{l})$ efficiently, we use a gradient-based optimization algorithm, such as L-BFGS-B. This requires the gradient of $\\mathcal{F}(\\mathbf{l})$ with respect to each component $l_k$ of $\\mathbf{l}$.\n\nLet's derive the gradient $\\nabla_{\\mathbf{l}} \\mathcal{F}(\\mathbf{l})$. We differentiate $\\mathcal{F}(\\mathbf{l})$ with respect to a component $l_k$:\n$$\n\\frac{\\partial \\mathcal{F}}{\\partial l_k} = \\frac{\\partial}{\\partial l_k} \\left( \\sum_{i=1}^{K} P_i \\log P_i - \\sum_{i=1}^{K} P_i \\log Q_i(\\mathbf{l}) \\right) + \\frac{\\partial}{\\partial l_k} \\left( \\lambda \\sum_{i=1}^{K} (l_i - l_i^{\\mathrm{ref}})^2 \\right)\n$$\nThe term $\\sum P_i \\log P_i$ is constant with respect to $\\mathbf{l}$. The derivative of the regularization term is straightforward:\n$$\n\\frac{\\partial}{\\partial l_k} \\left( \\lambda \\sum_{i=1}^{K} (l_i - l_i^{\\mathrm{ref}})^2 \\right) = 2\\lambda (l_k - l_k^{\\mathrm{ref}})\n$$\nNow we focus on the KLD term involving $\\mathbf{Q}(\\mathbf{l})$. Recalling $\\log Q_i(\\mathbf{l}) = l_i - \\text{logsumexp}(\\mathbf{l})$, its contribution to the objective is:\n$$\n-\\sum_{i=1}^{K} P_i (l_i - \\text{logsumexp}(\\mathbf{l})) = -\\sum_{i=1}^{K} P_i l_i + \\left(\\sum_{i=1}^{K} P_i\\right) \\text{logsumexp}(\\mathbf{l}) = -\\sum_{i=1}^{K} P_i l_i + \\text{logsumexp}(\\mathbf{l})\n$$\nwhere we have used the fact that $\\sum_i P_i = 1$. The derivative of this expression with respect to $l_k$ is:\n$$\n\\frac{\\partial}{\\partial l_k} \\left( -\\sum_{i=1}^{K} P_i l_i + \\text{logsumexp}(\\mathbf{l}) \\right) = -P_k + \\frac{\\partial}{\\partial l_k} \\log\\left(\\sum_{j=1}^{K} \\exp(l_j)\\right)\n$$\nThe derivative of the log-sum-exp function with respect to $l_k$ is known to be $Q_k(\\mathbf{l})$:\n$$\n\\frac{\\partial}{\\partial l_k} \\log\\left(\\sum_{j=1}^{K} \\exp(l_j)\\right) = \\frac{1}{\\sum_{j} \\exp(l_j)} \\cdot \\exp(l_k) = Q_k(\\mathbf{l})\n$$\nThus, the derivative of the KLD term is $Q_k(\\mathbf{l}) - P_k$. Combining all parts, the $k$-th component of the gradient is:\n$$\n\\frac{\\partial \\mathcal{F}}{\\partial l_k} = Q_k(\\mathbf{l}) - P_k + 2\\lambda (l_k - l_k^{\\mathrm{ref}})\n$$\nIn vector notation, the full gradient is:\n$$\n\\nabla_{\\mathbf{l}} \\mathcal{F}(\\mathbf{l}) = \\mathbf{Q}(\\mathbf{l}) - \\mathbf{P} + 2\\lambda (\\mathbf{l} - \\mathbf{l}^{\\mathrm{ref}})\n$$\n\n### Step 4: Algorithmic Implementation\n\nThe overall algorithm for each test case is as follows:\n1.  Define the potential $U(x)$, integrand $f(x) = \\exp(-\\beta U(x))$, and domain $[x_{\\min}, x_{\\max}]$.\n2.  Compute the partition function $Z = \\int_{x_{\\min}}^{x_{\\max}} f(x) \\, \\mathrm{d}x$ using numerical quadrature.\n3.  Establish the $K$ bin edges $x_0, \\dots, x_K$ over $[x_{\\min}, x_{\\max}]$.\n4.  Compute the target probability vector $\\mathbf{P}$ by numerically integrating $f(x)$ over each bin $[x_{i-1}, x_i]$ and dividing by $Z$.\n5.  Implement a function that takes $\\mathbf{l}$ as an argument and returns both the objective value $\\mathcal{F}(\\mathbf{l})$ and its gradient $\\nabla_{\\mathbf{l}} \\mathcal{F}(\\mathbf{l})$, using the expressions derived above and numerically stable functions like `logsumexp`.\n6.  Use a numerical optimizer, such as `scipy.optimize.minimize` with the `L-BFGS-B` method, to find the vector $\\mathbf{l}^\\star$ that minimizes $\\mathcal{F}(\\mathbf{l})$. Provide the function from step $5$ along with an initial guess (e.g., $\\mathbf{l}_0 = \\mathbf{l}^{\\mathrm{ref}}$) to the optimizer.\n7.  The result of the optimization, $\\mathbf{l}^\\star$, is the solution for the given test case. This procedure is repeated for all provided test cases.",
            "answer": "```python\nimport numpy as np\nfrom scipy.integrate import quad\nfrom scipy.optimize import minimize\nfrom scipy.special import logsumexp\n\ndef solve():\n    \"\"\"\n    Computes the optimal log-probabilities for a coarse-grained model by minimizing\n    a regularized Kullback-Leibler divergence.\n    \"\"\"\n    test_cases = [\n        # Case A\n        {'a': 1.0, 'b': 1.0, 'beta': 5.0, 'xmin': -2.5, 'xmax': 2.5,\n         'K': 5, 'lam': 0.1, 'l_ref': [0.0, 0.0, 0.0, 0.0, 0.0]},\n        # Case B\n        {'a': 1.0, 'b': 1.0, 'beta': 0.1, 'xmin': -3.0, 'xmax': 3.0,\n         'K': 4, 'lam': 1e-6, 'l_ref': [0.0, 0.0, 0.0, 0.0]},\n        # Case C\n        {'a': 1.0, 'b': 1.0, 'beta': 2.0, 'xmin': -2.0, 'xmax': 2.0,\n         'K': 6, 'lam': 10.0, 'l_ref': [-1.0, 0.0, 1.0, 2.0, 1.0, 0.0]},\n        # Case D\n        {'a': 1.0, 'b': 1.0, 'beta': 50.0, 'xmin': -2.0, 'xmax': 2.0,\n         'K': 8, 'lam': 0.01, 'l_ref': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]},\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        a = case['a']\n        b = case['b']\n        beta = case['beta']\n        xmin = case['xmin']\n        xmax = case['xmax']\n        K = case['K']\n        lam = case['lam']\n        l_ref = np.array(case['l_ref'])\n\n        # 1. Define potential and integrand\n        def U(x):\n            return a * (x**2 - b**2)**2\n\n        def integrand(x):\n            return np.exp(-beta * U(x))\n\n        # 2. Compute partition function Z\n        Z, _ = quad(integrand, xmin, xmax, epsabs=1e-12, epsrel=1e-12)\n        if Z == 0:\n            # This case should not happen with the given parameters\n            raise ValueError(\"Partition function Z is zero.\")\n            \n        # 3. Compute target marginal probabilities P\n        bin_edges = np.linspace(xmin, xmax, K + 1)\n        P = np.zeros(K)\n        for i in range(K):\n            bin_integral, _ = quad(integrand, bin_edges[i], bin_edges[i+1], epsabs=1e-12, epsrel=1e-12)\n            P[i] = bin_integral / Z\n        \n        # Ensure P is a valid probability distribution\n        P[P  0] = 0\n        P /= np.sum(P)\n\n        # 4. Define objective function and its gradient\n        # Handle the P_i * log(P_i) term, where the result is 0 if P_i = 0\n        P_log_P = np.zeros_like(P)\n        non_zero_mask = P > 0\n        P_log_P[non_zero_mask] = P[non_zero_mask] * np.log(P[non_zero_mask])\n        dkl_const_term = np.sum(P_log_P)\n\n        def objective_and_grad(l, P_vec, lam_val, l_ref_vec, dkl_const):\n            \"\"\"\n            Computes the objective function and its gradient.\n            F(l) = D_KL(P||Q(l)) + lambda * ||l - l_ref||^2\n            \"\"\"\n            # Compute Q(l) using logsumexp for stability\n            lse = logsumexp(l)\n            log_Q = l - lse\n            Q = np.exp(log_Q)\n\n            # Objective Function F(l)\n            # D_KL = sum(P * (logP - logQ))\n            dkl = dkl_const - np.sum(P_vec * log_Q)\n            reg = lam_val * np.sum((l - l_ref_vec)**2)\n            obj_val = dkl + reg\n\n            # Gradient of F(l)\n            # grad(F) = Q - P + 2*lambda*(l - l_ref)\n            grad_vec = Q - P_vec + 2 * lam_val * (l - l_ref_vec)\n            \n            return obj_val, grad_vec\n\n        # 5. Perform the optimization\n        l_initial = np.copy(l_ref)\n        \n        result = minimize(\n            fun=objective_and_grad,\n            x0=l_initial,\n            args=(P, lam, l_ref, dkl_const_term),\n            method='L-BFGS-B',\n            jac=True,  # function returns both objective and gradient\n            options={'gtol': 1e-9, 'disp': False}\n        )\n\n        l_star = result.x\n        all_results.append(l_star.tolist())\n\n    # 6. Format the final output string\n    # E.g., [[-4.7,-0.7,0.3,-0.7,-4.7],[-0.1,0.0,0.0,-0.1]]\n    output_parts = []\n    for res_list in all_results:\n        # Format each list as '[val1,val2,...]'\n        formatted_list = f'[{\",\".join(map(str, res_list))}]'\n        output_parts.append(formatted_list)\n    \n    final_output = f\"[{','.join(output_parts)}]\"\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "Relative Entropy Minimization (REM) is a powerful method, but it is one of several philosophies for parameterizing coarse-grained models. This final practice problem provides a direct comparison between REM and another widely used technique, Force Matching (FM). By analytically solving for the optimal model parameter using both methods in a specific scenario, this exercise reveals their fundamental differences and demonstrates how their distinct underlying assumptions—matching equilibrium distributions versus matching average forces—can lead to significantly different results .",
            "id": "3456643",
            "problem": "Consider a one-dimensional coarse variable $q$ that has been nondimensionalized so that the atomistic reference equilibrium distribution for $q$ is the standard normal Boltzmann distribution $p_{\\mathrm{ref}}(q) = (2\\pi)^{-1/2} \\exp(-q^{2}/2)$, corresponding to the reference potential $U_{\\mathrm{ref}}(q) = q^{2}/2$. You seek a top-down coarse-grained model with a nonlinear quartic potential $U_{\\theta}(q) = \\theta\\, q^{4}$.\n\nTwo common top-down parameter estimation criteria are Relative Entropy Minimization (REM) and Force Matching (FM). In REM, one minimizes the Kullback–Leibler divergence from the reference Boltzmann distribution to the model Boltzmann distribution $p_{\\theta}(q) \\propto \\exp(-U_{\\theta}(q))$. In FM, one minimizes the mean squared discrepancy between the model force and noisy instantaneous forces sampled from the reference dynamics. Assume the instantaneous force data obey the stochastic model\n$$\n\\mathcal{F}(q) \\equiv -\\frac{\\partial U_{\\mathrm{ref}}}{\\partial q}(q) + s\\, q^{3} + \\varepsilon = -q + s\\, q^{3} + \\varepsilon,\n$$\nwhere $s$ is a fixed dimensionless amplitude capturing state-dependent (heteroscedastic) noise arising from eliminated degrees of freedom, and $\\varepsilon$ is a zero-mean residual with $\\mathbb{E}[\\varepsilon \\mid q] = 0$ that is independent of $q$ and has finite variance. The FM objective is the mean squared error under the reference distribution,\n$$\nJ(\\theta) = \\mathbb{E}_{p_{\\mathrm{ref}}}\\!\\left[\\left(-\\frac{\\partial U_{\\theta}}{\\partial q}(q) - \\mathcal{F}(q)\\right)^{2}\\right].\n$$\n\nStarting only from the definitions above and the standard properties of the Gaussian distribution $p_{\\mathrm{ref}}(q)$, do the following:\n\n1. Derive the REM first-order optimality condition for $\\theta$ and solve it exactly to obtain $\\theta_{\\mathrm{REM}}$.\n2. Derive the FM normal equation for $\\theta$ under the stated noisy force model and solve it exactly to obtain $\\theta_{\\mathrm{FM}}$ as a function of $s$.\n\nYou must carry out any needed integrals explicitly and justify each step from first principles. Provide your final answer as a single row matrix containing the two optimized parameter values $\\theta_{\\mathrm{REM}}$ and $\\theta_{\\mathrm{FM}}$ in exact analytical form. No numerical rounding is required, and no units are needed because all quantities are dimensionless by construction.",
            "solution": "The user wants to find the optimal parameter $\\theta$ for a coarse-grained potential $U_{\\theta}(q) = \\theta q^4$ using two different methods: Relative Entropy Minimization (REM) and Force Matching (FM). The reference system is described by a standard normal Boltzmann distribution.\n\n### Step 1: Problem Validation\n\n**1. Extract Givens:**\n-   Reference equilibrium distribution: $p_{\\mathrm{ref}}(q) = (2\\pi)^{-1/2} \\exp(-q^{2}/2)$, which is the standard normal distribution $N(0, 1)$.\n-   Reference potential: $U_{\\mathrm{ref}}(q) = q^{2}/2$. The distribution is given by $p_{\\mathrm{ref}}(q) \\propto \\exp(-U_{\\mathrm{ref}}(q))$.\n-   Model coarse-grained potential: $U_{\\theta}(q) = \\theta\\, q^{4}$.\n-   Model Boltzmann distribution: $p_{\\theta}(q) \\propto \\exp(-U_{\\theta}(q))$.\n-   REM criterion: Minimize the Kullback-Leibler (KL) divergence from $p_{\\mathrm{ref}}$ to $p_{\\theta}$, which is $S_{\\mathrm{KL}}(p_{\\mathrm{ref}} || p_{\\theta})$.\n-   Instantaneous force model for FM: $\\mathcal{F}(q) = -q + s\\, q^{3} + \\varepsilon$, where $s$ is a constant, and $\\varepsilon$ is a random variable such that its conditional expectation given $q$ is zero, $\\mathbb{E}[\\varepsilon \\mid q] = 0$.\n-   FM objective function: $J(\\theta) = \\mathbb{E}_{p_{\\mathrm{ref}}}\\!\\left[\\left(-\\frac{\\partial U_{\\theta}}{\\partial q}(q) - \\mathcal{F}(q)\\right)^{2}\\right]$.\n\n**2. Validate Using Extracted Givens:**\n-   **Scientifically Grounded:** The problem is set within the established theoretical framework of statistical mechanics and coarse-graining for molecular dynamics. Relative entropy minimization and force matching are standard, well-documented techniques. The potentials and distributions used are common models.\n-   **Well-Posed:** Both optimization problems (REM and FM) are well-defined. They involve minimizing convex or quadratic functions of the parameter $\\theta$ (or a function of $\\theta$), which leads to unique solutions. The problem is self-contained.\n-   **Objective:** The problem is stated using precise mathematical language and definitions, free of ambiguity or subjectivity.\n\n**3. Verdict:**\nThe problem is valid as it is scientifically sound, well-posed, and objective.\n\n### Step 2: Derivations and Solution\n\n#### Part 1: Relative Entropy Minimization (REM)\n\nThe objective is to find the parameter $\\theta_{\\mathrm{REM}}$ that minimizes the relative entropy (or Kullback-Leibler divergence) from the reference distribution $p_{\\mathrm{ref}}$ to the model distribution $p_{\\theta}$. The relative entropy is defined as:\n$$\nS_{\\mathrm{KL}}(p_{\\mathrm{ref}} || p_{\\theta}) = \\int_{-\\infty}^{\\infty} p_{\\mathrm{ref}}(q) \\ln\\left(\\frac{p_{\\mathrm{ref}}(q)}{p_{\\theta}(q)}\\right) dq\n$$\nWe can expand the logarithm:\n$$\nS_{\\mathrm{KL}}(p_{\\mathrm{ref}} || p_{\\theta}) = \\int_{-\\infty}^{\\infty} p_{\\mathrm{ref}}(q) \\ln(p_{\\mathrm{ref}}(q)) dq - \\int_{-\\infty}^{\\infty} p_{\\mathrm{ref}}(q) \\ln(p_{\\theta}(q)) dq\n$$\nThe first term, $\\int p_{\\mathrm{ref}}(q) \\ln(p_{\\mathrm{ref}}(q)) dq$, is the negative entropy of the reference distribution, which is a constant with respect to the model parameter $\\theta$. Therefore, minimizing $S_{\\mathrm{KL}}$ is equivalent to maximizing the second term, $\\int p_{\\mathrm{ref}}(q) \\ln(p_{\\theta}(q)) dq$.\n\nThe model distribution is $p_{\\theta}(q) = \\frac{1}{Z_{\\theta}} \\exp(-U_{\\theta}(q))$, where $Z_{\\theta} = \\int_{-\\infty}^{\\infty} \\exp(-U_{\\theta}(q')) dq'$ is the partition function. Thus, $\\ln(p_{\\theta}(q)) = -U_{\\theta}(q) - \\ln(Z_{\\theta})$.\nMaximizing $\\int p_{\\mathrm{ref}}(q) \\ln(p_{\\theta}(q)) dq$ is equivalent to minimizing its negative:\n$$\nL(\\theta) = -\\int_{-\\infty}^{\\infty} p_{\\mathrm{ref}}(q) \\ln(p_{\\theta}(q)) dq = \\int_{-\\infty}^{\\infty} p_{\\mathrm{ref}}(q) [U_{\\theta}(q) + \\ln(Z_{\\theta})] dq\n$$\n$$\nL(\\theta) = \\mathbb{E}_{p_{\\mathrm{ref}}}[U_{\\theta}(q)] + \\ln(Z_{\\theta})\n$$\nSubstituting $U_{\\theta}(q) = \\theta q^{4}$:\n$$\nL(\\theta) = \\mathbb{E}_{p_{\\mathrm{ref}}}[\\theta q^{4}] + \\ln\\left(\\int_{-\\infty}^{\\infty} \\exp(-\\theta (q')^{4}) dq'\\right) = \\theta \\langle q^{4} \\rangle_{p_{\\mathrm{ref}}} + \\ln(Z_{\\theta})\n$$\nThe first-order optimality condition is found by setting the derivative of $L(\\theta)$ with respect to $\\theta$ to zero:\n$$\n\\frac{dL}{d\\theta} = \\langle q^{4} \\rangle_{p_{\\mathrm{ref}}} + \\frac{1}{Z_{\\theta}} \\frac{dZ_{\\theta}}{d\\theta} = 0\n$$\nWe compute the derivative of the partition function:\n$$\n\\frac{dZ_{\\theta}}{d\\theta} = \\frac{d}{d\\theta} \\int_{-\\infty}^{\\infty} \\exp(-\\theta q^{4}) dq = \\int_{-\\infty}^{\\infty} (-q^{4}) \\exp(-\\theta q^{4}) dq\n$$\nDividing by $Z_{\\theta}$ gives the expectation of $-q^{4}$ with respect to the model distribution $p_{\\theta}$:\n$$\n\\frac{1}{Z_{\\theta}} \\frac{dZ_{\\theta}}{d\\theta} = -\\frac{\\int_{-\\infty}^{\\infty} q^{4} \\exp(-\\theta q^{4}) dq}{\\int_{-\\infty}^{\\infty} \\exp(-\\theta q^{4}) dq} = -\\langle q^{4} \\rangle_{p_{\\theta}}\n$$\nThus, the optimality condition simplifies to a moment-matching condition:\n$$\n\\langle q^{4} \\rangle_{p_{\\mathrm{ref}}} - \\langle q^{4} \\rangle_{p_{\\theta}} = 0 \\implies \\langle q^{4} \\rangle_{p_{\\mathrm{ref}}} = \\langle q^{4} \\rangle_{p_{\\theta}}\n$$\nWe now compute both expectations.\nFor the reference distribution $p_{\\mathrm{ref}}(q)$, which is a standard normal distribution $N(0, 1)$, the $n$-th moment $\\langle q^n \\rangle$ can be found using the property that for a Gaussian random variable $X \\sim N(0, \\sigma^2)$, $\\mathbb{E}[X^{2k}] = (2k-1)!! \\sigma^{2k}$. Here, $\\sigma=1$ and we need the $4$-th moment ($k=2$):\n$$\n\\langle q^{4} \\rangle_{p_{\\mathrm{ref}}} = (2 \\cdot 2 - 1)!! = 3!! = 3 \\times 1 = 3\n$$\nFor the model distribution $p_{\\theta}(q)$, the expectation is:\n$$\n\\langle q^{4} \\rangle_{p_{\\theta}} = \\frac{\\int_{-\\infty}^{\\infty} q^{4} \\exp(-\\theta q^{4}) dq}{\\int_{-\\infty}^{\\infty} \\exp(-\\theta q^{4}) dq}\n$$\nThese integrals can be solved using the Gamma function integral formula, $\\int_{0}^{\\infty} x^{n-1} e^{-ax^k} dx = \\frac{1}{k} a^{-n/k} \\Gamma(n/k)$. Since the integrands are even, we can write $\\int_{-\\infty}^{\\infty} (\\cdot) dq = 2 \\int_{0}^{\\infty} (\\cdot) dq$.\nNumerator: $2 \\int_{0}^{\\infty} q^{4} \\exp(-\\theta q^{4}) dq$. Here, $n-1=4 \\Rightarrow n=5$, $k=4$, $a=\\theta$.\n$$\n\\int_{-\\infty}^{\\infty} q^{4} \\exp(-\\theta q^{4}) dq = 2 \\left( \\frac{1}{4} \\theta^{-5/4} \\Gamma(5/4) \\right) = \\frac{1}{2} \\theta^{-5/4} \\Gamma(5/4)\n$$\nDenominator: $2 \\int_{0}^{\\infty} q^{0} \\exp(-\\theta q^{4}) dq$. Here, $n-1=0 \\Rightarrow n=1$, $k=4$, $a=\\theta$.\n$$\n\\int_{-\\infty}^{\\infty} \\exp(-\\theta q^{4}) dq = 2 \\left( \\frac{1}{4} \\theta^{-1/4} \\Gamma(1/4) \\right) = \\frac{1}{2} \\theta^{-1/4} \\Gamma(1/4)\n$$\nThe expectation is the ratio of these two results:\n$$\n\\langle q^{4} \\rangle_{p_{\\theta}} = \\frac{\\frac{1}{2} \\theta^{-5/4} \\Gamma(5/4)}{\\frac{1}{2} \\theta^{-1/4} \\Gamma(1/4)} = \\theta^{-1} \\frac{\\Gamma(5/4)}{\\Gamma(1/4)}\n$$\nUsing the property $\\Gamma(z+1)=z\\Gamma(z)$, we have $\\Gamma(5/4) = \\Gamma(1/4 + 1) = \\frac{1}{4}\\Gamma(1/4)$.\n$$\n\\langle q^{4} \\rangle_{p_{\\theta}} = \\theta^{-1} \\frac{\\frac{1}{4}\\Gamma(1/4)}{\\Gamma(1/4)} = \\frac{1}{4\\theta}\n$$\nSubstituting these expectations into the optimality condition:\n$$\n3 = \\frac{1}{4\\theta_{\\mathrm{REM}}} \\implies \\theta_{\\mathrm{REM}} = \\frac{1}{12}\n$$\n\n#### Part 2: Force Matching (FM)\n\nThe FM objective is to find $\\theta_{\\mathrm{FM}}$ that minimizes the mean squared error between the model force and the instantaneous force, averaged over the reference distribution:\n$$\nJ(\\theta) = \\mathbb{E}_{p_{\\mathrm{ref}}}\\!\\left[\\left(-\\frac{\\partial U_{\\theta}}{\\partial q}(q) - \\mathcal{F}(q)\\right)^{2}\\right]\n$$\nThe model force is $F_{\\theta}(q) = -\\frac{\\partial U_{\\theta}}{\\partial q}(q) = -\\frac{d}{dq}(\\theta q^{4}) = -4\\theta q^{3}$.\nThe instantaneous force is $\\mathcal{F}(q) = -q + s\\, q^{3} + \\varepsilon$.\nSubstituting these into the objective function:\n$$\nJ(\\theta) = \\mathbb{E}_{p_{\\mathrm{ref}}}\\!\\left[\\left(-4\\theta q^{3} - (-q + s q^{3} + \\varepsilon)\\right)^{2}\\right] = \\mathbb{E}_{p_{\\mathrm{ref}}}\\!\\left[\\left(q - (4\\theta + s) q^{3} - \\varepsilon\\right)^{2}\\right]\n$$\nTo minimize $J(\\theta)$, we set its derivative with respect to $\\theta$ to zero (the normal equation):\n$$\n\\frac{dJ}{d\\theta} = \\frac{d}{d\\theta} \\mathbb{E}_{p_{\\mathrm{ref}}}\\!\\left[\\left(q - (4\\theta + s) q^{3} - \\varepsilon\\right)^{2}\\right] = 0\n$$\nInterchanging expectation and differentiation:\n$$\n\\frac{dJ}{d\\theta} = \\mathbb{E}_{p_{\\mathrm{ref}}}\\!\\left[ \\frac{d}{d\\theta} \\left(q - (4\\theta + s) q^{3} - \\varepsilon\\right)^{2} \\right]\n$$\nUsing the chain rule:\n$$\n\\frac{dJ}{d\\theta} = \\mathbb{E}_{p_{\\mathrm{ref}}}\\!\\left[ 2 \\left(q - (4\\theta + s) q^{3} - \\varepsilon\\right) \\cdot (-4 q^{3}) \\right] = 0\n$$\nIgnoring the constant factor of $-8$, the normal equation is:\n$$\n\\mathbb{E}_{p_{\\mathrm{ref}}}\\!\\left[ \\left(q - (4\\theta + s) q^{3} - \\varepsilon\\right) q^{3} \\right] = 0\n$$\nUsing the linearity of expectation:\n$$\n\\mathbb{E}_{p_{\\mathrm{ref}}}\\![q^{4}] - (4\\theta + s) \\mathbb{E}_{p_{\\mathrm{ref}}}\\![q^{6}] - \\mathbb{E}_{p_{\\mathrm{ref}}}\\![\\varepsilon q^{3}] = 0\n$$\nWe evaluate the expectation involving $\\varepsilon$. Using the law of total expectation and the given condition $\\mathbb{E}[\\varepsilon \\mid q] = 0$:\n$$\n\\mathbb{E}_{p_{\\mathrm{ref}}}\\![\\varepsilon q^{3}] = \\mathbb{E}_{p_{\\mathrm{ref}}}\\![\\mathbb{E}[\\varepsilon q^{3} \\mid q]] = \\mathbb{E}_{p_{\\mathrm{ref}}}\\![q^{3} \\mathbb{E}[\\varepsilon \\mid q]] = \\mathbb{E}_{p_{\\mathrm{ref}}}\\![q^{3} \\cdot 0] = 0\n$$\nThe normal equation simplifies to:\n$$\n\\langle q^{4} \\rangle_{p_{\\mathrm{ref}}} - (4\\theta_{\\mathrm{FM}} + s) \\langle q^{6} \\rangle_{p_{\\mathrm{ref}}} = 0\n$$\nWe already found $\\langle q^{4} \\rangle_{p_{\\mathrm{ref}}} = 3$. We need to compute the sixth moment, $\\langle q^{6} \\rangle_{p_{\\mathrm{ref}}}$. For a standard normal, with $k=3$:\n$$\n\\langle q^{6} \\rangle_{p_{\\mathrm{ref}}} = (2 \\cdot 3 - 1)!! = 5!! = 5 \\times 3 \\times 1 = 15\n$$\nSubstituting the moment values into the equation:\n$$\n3 - (4\\theta_{\\mathrm{FM}} + s) \\cdot 15 = 0\n$$\nSolving for $\\theta_{\\mathrm{FM}}$:\n$$\n3 = 15(4\\theta_{\\mathrm{FM}} + s)\n$$\n$$\n\\frac{3}{15} = 4\\theta_{\\mathrm{FM}} + s\n$$\n$$\n\\frac{1}{5} = 4\\theta_{\\mathrm{FM}} + s\n$$\n$$\n4\\theta_{\\mathrm{FM}} = \\frac{1}{5} - s\n$$\n$$\n\\theta_{\\mathrm{FM}} = \\frac{1}{4} \\left(\\frac{1}{5} - s\\right) = \\frac{1 - 5s}{20}\n$$\nThe two optimized parameter values are $\\theta_{\\mathrm{REM}} = \\frac{1}{12}$ and $\\theta_{\\mathrm{FM}} = \\frac{1-5s}{20}$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{1}{12}  \\frac{1 - 5s}{20}\n\\end{pmatrix}\n}\n$$"
        }
    ]
}