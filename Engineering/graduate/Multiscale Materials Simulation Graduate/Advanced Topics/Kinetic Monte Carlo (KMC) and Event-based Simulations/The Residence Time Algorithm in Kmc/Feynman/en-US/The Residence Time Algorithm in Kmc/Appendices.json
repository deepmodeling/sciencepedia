{
    "hands_on_practices": [
        {
            "introduction": "Before implementing an algorithm, it is essential to grasp its mathematical underpinnings. This first practice guides you through a foundational derivation of the residence time distribution, which is the statistical heart of the Kinetic Monte Carlo method. By starting from the assumption of independent Poisson processes, you will derive the exponential form of the time-step distribution and its key statistical moments, solidifying your understanding of why the Residence Time Algorithm works as it does .",
            "id": "3851130",
            "problem": "Consider a kinetic Monte Carlo (KMC) simulation of a crystalline surface in which three independent elementary processes can occur at any instant: a vacancy hop, an adatom detachment, and a near-neighbor exchange. Assume each process is modeled as an independent Poisson process with constant rates $r_{1}$, $r_{2}$, and $r_{3}$, respectively, and that the simulation uses the residence time algorithm to advance the clock by the time $\\Delta t$ to the next event. The physical assumption underlying the residence time algorithm is that the next event time is the first arrival among these independent Poisson processes, and the process is Markovian.\n\nStarting from the independence of the events and the definition of the survival function $S(t)$ as the probability that no event has occurred by time $t$, derive the probability density function $f_{\\Delta t}(t)$ of $\\Delta t$ in terms of the total rate $R_{\\text{tot}}=\\sum_{i=1}^{3} r_{i}$. Then, using this $f_{\\Delta t}(t)$, compute the expectation $\\mathbb{E}[\\Delta t]$ and the variance $\\mathrm{Var}[\\Delta t]$ as functions of $R_{\\text{tot}}$.\n\nFinally, instantiate the rates as $r_{1}=3\\,\\mathrm{s}^{-1}$, $r_{2}=5\\,\\mathrm{s}^{-1}$, and $r_{3}=2\\,\\mathrm{s}^{-1}$, and evaluate the numerical values of $\\mathbb{E}[\\Delta t]$ in seconds and $\\mathrm{Var}[\\Delta t]$ in seconds squared. Provide exact numerical values; no rounding is required. Report your final pair of numerical values in the order $\\mathbb{E}[\\Delta t]$, $\\mathrm{Var}[\\Delta t]$.",
            "solution": "The problem statement will first be validated against the required criteria.\n\n### Step 1: Extract Givens\n- Three independent elementary processes: vacancy hop, adatom detachment, near-neighbor exchange.\n- Each process is modeled as an independent Poisson process with constant rates $r_{1}$, $r_{2}$, and $r_{3}$, respectively.\n- The simulation uses the residence time algorithm to advance the clock by time $\\Delta t$.\n- Physical assumption: $\\Delta t$ is the first arrival time among the independent Poisson processes. The process is Markovian.\n- Starting point: The survival function $S(t)$ is the probability that no event has occurred by time $t$.\n- Total rate is defined as $R_{\\text{tot}}=\\sum_{i=1}^{3} r_{i}$.\n- Objective 1: Derive the probability density function $f_{\\Delta t}(t)$ of $\\Delta t$ in terms of $R_{\\text{tot}}$.\n- Objective 2: Compute the expectation $\\mathbb{E}[\\Delta t]$ and the variance $\\mathrm{Var}[\\Delta t]$ as functions of $R_{\\text{tot}}$.\n- Objective 3: Instantiate rates as $r_{1}=3\\,\\mathrm{s}^{-1}$, $r_{2}=5\\,\\mathrm{s}^{-1}$, and $r_{3}=2\\,\\mathrm{s}^{-1}$.\n- Objective 4: Evaluate the numerical values of $\\mathbb{E}[\\Delta t]$ and $\\mathrm{Var}[\\Delta t]$ and report them in the specified order.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is subjected to scrutiny based on the established criteria.\n\n- **Scientifically Grounded (Critical)**: The problem is firmly based on the theory of stochastic processes, specifically Poisson processes and exponential distributions. The residence time algorithm (also known as the BKL algorithm or standard KMC) is a fundamental and widely used method in computational materials science and chemistry. The assumptions (independent Poisson processes, Markovian behavior) are standard for this type of model. The problem is scientifically sound.\n- **Well-Posed**: The problem provides all necessary information to derive the requested quantities. The relationships between rates, probabilities, and time evolution are clearly defined within the framework of probability theory. A unique and meaningful solution exists.\n- **Objective (Critical)**: The language is precise, quantitative, and free of any subjective or ambiguous terminology. The tasks are clearly stated mathematical objectives.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. It is a standard, well-posed problem in statistical mechanics and simulation theory. The solution process will now proceed.\n\n### Solution Derivation\n\nLet $T_i$ be the waiting time for the $i$-th process, where $i \\in \\{1, 2, 3\\}$. Since each process is a Poisson process with rate $r_i$, the waiting time $T_i$ is a random variable following an exponential distribution with parameter $r_i$. The probability density function (PDF) for $T_i$ is $f_{T_i}(t) = r_i \\exp(-r_i t)$ for $t \\ge 0$, and the cumulative distribution function (CDF) is $F_{T_i}(t) = P(T_i \\le t) = 1 - \\exp(-r_i t)$.\n\nThe survival function for the $i$-th process, $S_i(t)$, is the probability that process $i$ has not occurred by time $t$. This is given by:\n$$S_i(t) = P(T_i  t) = 1 - F_{T_i}(t) = \\exp(-r_i t)$$\n\nThe time to the next event in the KMC simulation, $\\Delta t$, is the minimum of the waiting times for all possible independent processes.\n$$\\Delta t = \\min(T_1, T_2, T_3)$$\n\nThe survival function for $\\Delta t$, denoted $S(t)$ as per the problem statement, is the probability that the time to the next event is greater than $t$. This is equivalent to the probability that none of the three events have occurred by time $t$.\n$$S(t) = P(\\Delta t  t) = P(\\min(T_1, T_2, T_3)  t)$$\n\nFor the minimum of several random variables to be greater than $t$, each individual random variable must be greater than $t$.\n$$S(t) = P(T_1  t \\text{ and } T_2  t \\text{ and } T_3  t)$$\n\nSince the processes are independent, the joint probability is the product of the individual probabilities:\n$$S(t) = P(T_1  t) P(T_2  t) P(T_3  t) = S_1(t) S_2(t) S_3(t)$$\n\nSubstituting the expressions for the individual survival functions:\n$$S(t) = \\exp(-r_1 t) \\exp(-r_2 t) \\exp(-r_3 t) = \\exp(-(r_1 + r_2 + r_3)t)$$\n\nUsing the definition of the total rate, $R_{\\text{tot}} = r_1 + r_2 + r_3$, the survival function becomes:\n$$S(t) = \\exp(-R_{\\text{tot}} t)$$\n\nThe probability density function $f_{\\Delta t}(t)$ is obtained by taking the negative derivative of the survival function with respect to $t$.\n$$f_{\\Delta t}(t) = -\\frac{d}{dt}S(t) = -\\frac{d}{dt}\\left(\\exp(-R_{\\text{tot}} t)\\right) = -(-R_{\\text{tot}} \\exp(-R_{\\text{tot}} t))$$\n$$f_{\\Delta t}(t) = R_{\\text{tot}} \\exp(-R_{\\text{tot}} t) \\quad \\text{for } t \\ge 0$$\nThis is the PDF of an exponential distribution with rate parameter $R_{\\text{tot}}$.\n\nNext, we compute the expectation $\\mathbb{E}[\\Delta t]$ and variance $\\mathrm{Var}[\\Delta t]$.\nThe expectation value is calculated by definition:\n$$\\mathbb{E}[\\Delta t] = \\int_{0}^{\\infty} t f_{\\Delta t}(t) dt = \\int_{0}^{\\infty} t (R_{\\text{tot}} \\exp(-R_{\\text{tot}} t)) dt$$\nWe use integration by parts, $\\int u \\, dv = uv - \\int v \\, du$, with $u = t$ and $dv = R_{\\text{tot}} \\exp(-R_{\\text{tot}} t) dt$. This gives $du = dt$ and $v = -\\exp(-R_{\\text{tot}} t)$.\n$$\\mathbb{E}[\\Delta t] = \\left[ -t \\exp(-R_{\\text{tot}} t) \\right]_{0}^{\\infty} - \\int_{0}^{\\infty} (-\\exp(-R_{\\text{tot}} t)) dt$$\nThe first term evaluates to $0$, since $\\lim_{t\\to\\infty} t \\exp(-R_{\\text{tot}} t) = 0$ for $R_{\\text{tot}}  0$.\n$$\\mathbb{E}[\\Delta t] = \\int_{0}^{\\infty} \\exp(-R_{\\text{tot}} t) dt = \\left[ -\\frac{1}{R_{\\text{tot}}} \\exp(-R_{\\text{tot}} t) \\right]_{0}^{\\infty} = 0 - \\left(-\\frac{1}{R_{\\text{tot}}}\\right) = \\frac{1}{R_{\\text{tot}}}$$\n\nTo find the variance, we first compute the second moment, $\\mathbb{E}[(\\Delta t)^2]$.\n$$\\mathbb{E}[(\\Delta t)^2] = \\int_{0}^{\\infty} t^2 f_{\\Delta t}(t) dt = \\int_{0}^{\\infty} t^2 (R_{\\text{tot}} \\exp(-R_{\\text{tot}} t)) dt$$\nAgain, we use integration by parts with $u = t^2$ and $dv = R_{\\text{tot}} \\exp(-R_{\\text{tot}} t) dt$. This gives $du = 2t \\, dt$ and $v = -\\exp(-R_{\\text{tot}} t)$.\n$$\\mathbb{E}[(\\Delta t)^2] = \\left[ -t^2 \\exp(-R_{\\text{tot}} t) \\right]_{0}^{\\infty} - \\int_{0}^{\\infty} (-\\exp(-R_{\\text{tot}} t))(2t) dt$$\nThe first term is $0$.\n$$\\mathbb{E}[(\\Delta t)^2] = 2 \\int_{0}^{\\infty} t \\exp(-R_{\\text{tot}} t) dt$$\nWe can recognize the integral as being related to the expectation calculation. From $\\mathbb{E}[\\Delta t] = R_{\\text{tot}} \\int_{0}^{\\infty} t \\exp(-R_{\\text{tot}} t) dt = \\frac{1}{R_{\\text{tot}}}$, we know that $\\int_{0}^{\\infty} t \\exp(-R_{\\text{tot}} t) dt = \\frac{1}{R_{\\text{tot}}^2}$.\n$$\\mathbb{E}[(\\Delta t)^2] = 2 \\left( \\frac{1}{R_{\\text{tot}}^2} \\right) = \\frac{2}{R_{\\text{tot}}^2}$$\n\nThe variance is given by $\\mathrm{Var}[\\Delta t] = \\mathbb{E}[(\\Delta t)^2] - (\\mathbb{E}[\\Delta t])^2$.\n$$\\mathrm{Var}[\\Delta t] = \\frac{2}{R_{\\text{tot}}^2} - \\left(\\frac{1}{R_{\\text{tot}}}\\right)^2 = \\frac{1}{R_{\\text{tot}}^2}$$\n\nFinally, we substitute the given numerical values for the rates:\n$r_{1}=3\\,\\mathrm{s}^{-1}$, $r_{2}=5\\,\\mathrm{s}^{-1}$, and $r_{3}=2\\,\\mathrm{s}^{-1}$.\nThe total rate $R_{\\text{tot}}$ is:\n$$R_{\\text{tot}} = r_1 + r_2 + r_3 = 3\\,\\mathrm{s}^{-1} + 5\\,\\mathrm{s}^{-1} + 2\\,\\mathrm{s}^{-1} = 10\\,\\mathrm{s}^{-1}$$\n\nNow we can evaluate the expectation and variance:\n$$\\mathbb{E}[\\Delta t] = \\frac{1}{R_{\\text{tot}}} = \\frac{1}{10\\,\\mathrm{s}^{-1}} = 0.1\\,\\mathrm{s}$$\n$$\\mathrm{Var}[\\Delta t] = \\frac{1}{R_{\\text{tot}}^2} = \\frac{1}{(10\\,\\mathrm{s}^{-1})^2} = \\frac{1}{100\\,\\mathrm{s}^{-2}} = 0.01\\,\\mathrm{s}^2$$\n\nThe requested numerical values are $0.1$ for the expectation and $0.01$ for the variance.",
            "answer": "$$\\boxed{\\begin{pmatrix} 0.1  0.01 \\end{pmatrix}}$$"
        },
        {
            "introduction": "This exercise serves as a bridge from abstract theory to concrete application, challenging you to model a realistic physical system. You will first derive an analytical expression for the mean-squared displacement of a particle undergoing diffusion coupled with internal state changes, and then implement a KMC simulation using the Residence Time Algorithm to validate your theoretical result. This practice not only hones your implementation skills but also demonstrates the power of KMC in accurately capturing complex kinetic pathways that are verifiable against rigorous theory .",
            "id": "3851178",
            "problem": "Consider a one-dimensional lattice with spacing $a$ (in meters). A single point particle moves on this lattice via symmetric nearest-neighbor jumps and possesses an internal two-channel state $S(t) \\in \\{A,B\\}$. The jump rates and channel switching rates depend on the current channel. Specifically, when $S(t)=A$, the particle attempts jumps to the left and right, each at rate $r_A/2$ (in $\\mathrm{s}^{-1}$), so the total jump rate in channel $A$ is $r_A$; and it switches from channel $A$ to channel $B$ at rate $k_{AB}$ (in $\\mathrm{s}^{-1}$). Similarly, when $S(t)=B$, the left and right jump rates are each $r_B/2$, so the total jump rate in channel $B$ is $r_B$, and the channel switching from $B$ to $A$ occurs at rate $k_{BA}$ (in $\\mathrm{s}^{-1}$). Let the initial probability of starting in channel $A$ be $p_A(0)$, with $p_B(0)=1-p_A(0)$, and the initial position be $x(0)=0$.\n\nYou will (i) solve analytically the mean-squared displacement $\\mathbb{E}[x(t)^2]$ in $\\mathrm{m}^2$ for this coupled jump-switch process, starting from first principles of continuous-time Markov chains and Poisson jump processes, and (ii) implement a Kinetic Monte Carlo (KMC) simulation using the Residence Time Algorithm (RTA) to generate trajectories and empirically estimate the mean-squared displacement at a specified final time $t_{\\mathrm{end}}$ (in seconds). Your KMC should treat events from the current state as follows: from channel $A$, the event set is $\\{ \\text{left jump at rate } r_A/2, \\text{right jump at rate } r_A/2, \\text{switch to } B \\text{ at rate } k_{AB} \\}$ with the total rate $R_A = r_A + k_{AB}$; from channel $B$, the event set is $\\{ \\text{left jump at rate } r_B/2, \\text{right jump at rate } r_B/2, \\text{switch to } A \\text{ at rate } k_{BA} \\}$ with the total rate $R_B = r_B + k_{BA}$. At each KMC step from a state $S$, draw the residence time $\\Delta t$ from an exponential distribution with rate $R_S$, i.e., $\\Delta t = -\\ln u / R_S$ with $u \\in (0,1)$ uniform, and choose the event by drawing another uniform variate and selecting proportional to the event rates.\n\nStarting from $S(0)$ drawn according to $p_A(0)$ and $p_B(0)$, the position $x(t)$ changes by $\\pm a$ only when a jump event occurs. You must stop each KMC trajectory at $t_{\\mathrm{end}}$ by discarding any event whose scheduled time exceeds $t_{\\mathrm{end}}$ (i.e., the position is piecewise constant between events).\n\nDerive an analytic expression for $\\mathbb{E}[x(t)^2]$ in terms of $a$, $r_A$, $r_B$, $k_{AB}$, $k_{BA}$, $p_A(0)$, and $t$, using only the following fundamental base:\n- The continuous-time two-state Markov chain for channel occupancy with generator determined by $k_{AB}$ and $k_{BA}$, and its solution for the state probabilities $p_A(t)$ and $p_B(t)$.\n- The property that, conditioned on the channel occupancy process, symmetric jumps occur as independent Poisson processes with rates $r_A$ and $r_B$ in channels $A$ and $B$, respectively.\n- The law of total expectation for the number of jump events and the variance of sums of independent, zero-mean increments.\n\nDo not use any shortcut formulas not derived from these principles. Express all mathematical entities using LaTeX.\n\nYou will validate the KMC-RTA implementation against the analytic result by computing the absolute error $|\\mathbb{E}_{\\mathrm{KMC}}[x(t_{\\mathrm{end}})^2] - \\mathbb{E}_{\\mathrm{analytic}}[x(t_{\\mathrm{end}})^2]|$ in $\\mathrm{m}^2$ for each parameter set in the test suite below.\n\nPhysical units and output requirements:\n- Use $a$ in meters, all rates in $\\mathrm{s}^{-1}$, and time in seconds.\n- Report the final mean-squared displacement in $\\mathrm{m}^2$.\n- Your program must run $N=5000$ independent trajectories per test case with a fixed random seed $12345$ to estimate $\\mathbb{E}_{\\mathrm{KMC}}[x(t_{\\mathrm{end}})^2]$.\n- The final output must be a single line containing a comma-separated list enclosed in square brackets of the absolute errors for the five test cases, in $\\mathrm{m}^2$, in the order listed. For example: \"[e1,e2,e3,e4,e5]\".\n\nTest suite (five cases that collectively probe a general case, equal-channel rates, no-switching boundaries from both initial states, and fast-switching limit):\n1. General case: $a=2\\times 10^{-10}$, $r_A=3\\times 10^{6}$, $r_B=1\\times 10^{6}$, $k_{AB}=8\\times 10^{5}$, $k_{BA}=4\\times 10^{5}$, $p_A(0)=1.0$, $t_{\\mathrm{end}}=5\\times 10^{-6}$.\n2. Equal channel jump rates: $a=5\\times 10^{-10}$, $r_A=2\\times 10^{6}$, $r_B=2\\times 10^{6}$, $k_{AB}=1\\times 10^{6}$, $k_{BA}=3\\times 10^{6}$, $p_A(0)=0.25$, $t_{\\mathrm{end}}=4\\times 10^{-6}$.\n3. No switching, start in $A$: $a=3\\times 10^{-10}$, $r_A=1.5\\times 10^{6}$, $r_B=0.5\\times 10^{6}$, $k_{AB}=0$, $k_{BA}=0$, $p_A(0)=1.0$, $t_{\\mathrm{end}}=6\\times 10^{-6}$.\n4. No switching, start in $B$: $a=1\\times 10^{-10}$, $r_A=1.5\\times 10^{6}$, $r_B=0.5\\times 10^{6}$, $k_{AB}=0$, $k_{BA}=0$, $p_A(0)=0.0$, $t_{\\mathrm{end}}=6\\times 10^{-6}$.\n5. Fast switching: $a=2.5\\times 10^{-10}$, $r_A=1.0\\times 10^{7}$, $r_B=1.0\\times 10^{6}$, $k_{AB}=5.0\\times 10^{7}$, $k_{BA}=5.0\\times 10^{7}$, $p_A(0)=0.5$, $t_{\\mathrm{end}}=2\\times 10^{-6}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,result4,result5]\").",
            "solution": "The problem is deemed valid. It is a scientifically grounded, well-posed, and objective problem in the domain of statistical physics and computational materials simulation. The setup describes a continuous-time Markov process on a discrete lattice, a standard model for phenomena like atomic diffusion. The task requires deriving the Mean-Squared Displacement (MSD) from first principles and validating it against a Kinetic Monte Carlo (KMC) simulation using the Residence Time Algorithm (RTA). Both the analytical and numerical methods are standard and appropriate for the problem. The parameters and conditions are physically realistic, complete, and consistent.\n\nThe analytical derivation for the mean-squared displacement, $\\mathbb{E}[x(t)^2]$, proceeds from first principles as specified. The particle's position at time $t$ is given by $x(t) = a \\sum_{i=1}^{N_j(t)} \\sigma_i$, where $a$ is the lattice spacing, $N_j(t)$ is the total number of jumps up to time $t$, and $\\sigma_i \\in \\{-1, +1\\}$ is the direction of the $i$-th jump. Since the jumps are symmetric nearest-neighbor jumps, the probability of jumping left is equal to the probability of jumping right. Therefore, the expectation of the jump direction is $\\mathbb{E}[\\sigma_i] = 0$. The jump directions are independent random variables, so $\\mathbb{E}[\\sigma_i \\sigma_j] = \\mathbb{E}[\\sigma_i]\\mathbb{E}[\\sigma_j] = 0$ for $i \\neq j$. Also, $\\sigma_i^2 = (\\pm 1)^2 = 1$. The mean-squared displacement is then:\n$$ \\mathbb{E}[x(t)^2] = \\mathbb{E}\\left[\\left(a \\sum_{i=1}^{N_j(t)} \\sigma_i\\right)^2\\right] = a^2 \\mathbb{E}\\left[\\sum_{i,j=1}^{N_j(t)} \\sigma_i \\sigma_j\\right] = a^2 \\mathbb{E}\\left[\\sum_{i=1}^{N_j(t)} \\sigma_i^2 + \\sum_{i \\neq j} \\sigma_i \\sigma_j\\right] $$\nUsing the law of total expectation, conditioning on $N_j(t)$, the expectation of the cross terms is zero. This simplifies the expression to:\n$$ \\mathbb{E}[x(t)^2] = a^2 \\mathbb{E}\\left[\\sum_{i=1}^{N_j(t)} 1\\right] = a^2 \\mathbb{E}[N_j(t)] $$\nThis result relies on the principle of the variance of a sum of independent, zero-mean increments. The problem is thus reduced to finding the expected total number of jumps, $\\mathbb{E}[N_j(t)]$.\n\nThe jump process is a doubly stochastic process, or a Cox process, where the rate itself is a stochastic process $r(S(t'))$ dependent on the channel state $S(t') \\in \\{A, B\\}$. The expected number of jumps is found by integrating the expected instantaneous jump rate over the time interval $[0, t]$, an application of the law of total expectation:\n$$ \\mathbb{E}[N_j(t)] = \\mathbb{E}\\left[\\int_0^t r(S(t')) dt'\\right] = \\int_0^t \\mathbb{E}[r(S(t'))] dt' $$\nThe expected rate at time $t'$ is $\\mathbb{E}[r(S(t'))] = r_A p_A(t') + r_B p_B(t')$, where $p_A(t')$ and $p_B(t')$ are the probabilities of being in channel $A$ and $B$, respectively.\n\nThe channel occupancy follows a two-state continuous-time Markov chain with rates $k_{AB}$ and $k_{BA}$. The evolution of the probabilities is governed by the master equation:\n$$ \\frac{dp_A(t)}{dt} = -k_{AB} p_A(t) + k_{BA} p_B(t) $$\nUsing the constraint $p_A(t) + p_B(t) = 1$, we get a first-order linear ordinary differential equation for $p_A(t)$:\n$$ \\frac{dp_A(t)}{dt} + (k_{AB} + k_{BA}) p_A(t) = k_{BA} $$\nLet $k = k_{AB} + k_{BA}$. For the case $k \\neq 0$, the solution with initial condition $p_A(0)$ is:\n$$ p_A(t) = p_A(0)e^{-kt} + \\frac{k_{BA}}{k}(1 - e^{-kt}) $$\nThis can be rewritten in terms of the steady-state probability $p_A^{\\infty} = \\lim_{t \\to \\infty} p_A(t) = k_{BA}/k$ as $p_A(t) = (p_A(0) - p_A^{\\infty})e^{-kt} + p_A^{\\infty}$.\n\nWe now compute $\\mathbb{E}[N_j(t)]$ by integrating the expected rate:\n$$ \\mathbb{E}[N_j(t)] = \\int_0^t [r_A p_A(t') + r_B (1 - p_A(t'))] dt' = \\int_0^t [(r_A - r_B)p_A(t') + r_B] dt' $$\n$$ \\int_0^t p_A(t') dt' = \\int_0^t \\left[(p_A(0) - p_A^{\\infty})e^{-kt'} + p_A^{\\infty}\\right] dt' = \\frac{p_A(0) - p_A^{\\infty}}{k}(1-e^{-kt}) + p_A^{\\infty}t $$\nSubstituting this integral back, we get:\n$$ \\mathbb{E}[N_j(t)] = (r_A - r_B) \\left[ \\frac{p_A(0) - p_A^{\\infty}}{k}(1-e^{-kt}) + p_A^{\\infty}t \\right] + r_B t $$\n$$ \\mathbb{E}[N_j(t)] = (r_A p_A^{\\infty} + r_B p_B^{\\infty})t + \\frac{r_A - r_B}{k}(p_A(0) - p_A^{\\infty})(1 - e^{-kt}) $$\nwhere $p_B^{\\infty} = 1 - p_A^{\\infty} = k_{AB}/k$. The final expression for the MSD for $k = k_{AB} + k_{BA} \\neq 0$ is:\n$$ \\mathbb{E}[x(t)^2] = a^2 \\left\\{ (r_A p_A^{\\infty} + r_B p_B^{\\infty})t + \\frac{r_A - r_B}{k}(p_A(0) - p_A^{\\infty})(1 - e^{-kt}) \\right\\} $$\n\nFor the special case where $k = k_{AB} + k_{BA} = 0$, which implies $k_{AB}=0$ and $k_{BA}=0$, there is no switching. The channel probability is constant: $p_A(t) = p_A(0)$. The expected number of jumps simplifies to:\n$$ \\mathbb{E}[N_j(t)] = \\int_0^t (r_A p_A(0) + r_B p_B(0)) dt' = (r_A p_A(0) + r_B(1-p_A(0)))t $$\nThus, for $k=0$:\n$$ \\mathbb{E}[x(t)^2] = a^2 (r_A p_A(0) + r_B p_B(0))t $$\n\nThe KMC simulation uses the Residence Time Algorithm (RTA), which is a direct implementation of the underlying continuous-time Markov process. The state of the system is the tuple $(x, S)$, position and channel. At each step:\n1.  From the current channel state $S \\in \\{A, B\\}$, the total exit rate $R_S$ is computed. For $S=A$, $R_A = r_A + k_{AB}$. For $S=B$, $R_B = r_B + k_{BA}$.\n2.  A residence time $\\Delta t$ in the current state is drawn from an exponential distribution with rate $R_S$: $\\Delta t = -\\ln(u_1)/R_S$, where $u_1$ is a uniform random number in $(0,1)$.\n3.  The simulation time is advanced by $\\Delta t$. If the new time exceeds the specified final time $t_{\\mathrm{end}}$, the trajectory terminates, and the position is recorded as its value at the beginning of this rejected step.\n4.  If the time step is accepted, an event is chosen based on its relative rate. A second uniform random number $u_2 \\in (0,1)$ is drawn. For state $A$, if $u_2  (r_A/2)/R_A$, a left jump occurs ($x \\to x-a$). If $(r_A/2)/R_A \\le u_2  r_A/R_A$, a right jump occurs ($x \\to x+a$). Otherwise, a channel switch occurs ($S \\to B$). A similar logic applies to state $B$.\n5.  The system state $(x, S)$ is updated, and the process repeats.\nThe final estimate $\\mathbb{E}_{\\mathrm{KMC}}[x(t_{\\mathrm{end}})^2]$ is the average of $x(t_{\\mathrm{end}})^2$ over a large number $N$ of such independent simulation trajectories.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by calculating the analytical MSD and comparing it\n    to a KMC simulation for a set of test cases.\n    \"\"\"\n\n    test_cases = [\n        # 1. General case\n        {'a': 2e-10, 'r_A': 3e6, 'r_B': 1e6, 'k_AB': 8e5, 'k_BA': 4e5, 'p_A0': 1.0, 't_end': 5e-6},\n        # 2. Equal channel jump rates\n        {'a': 5e-10, 'r_A': 2e6, 'r_B': 2e6, 'k_AB': 1e6, 'k_BA': 3e6, 'p_A0': 0.25, 't_end': 4e-6},\n        # 3. No switching, start in A\n        {'a': 3e-10, 'r_A': 1.5e6, 'r_B': 0.5e6, 'k_AB': 0.0, 'k_BA': 0.0, 'p_A0': 1.0, 't_end': 6e-6},\n        # 4. No switching, start in B\n        {'a': 1e-10, 'r_A': 1.5e6, 'r_B': 0.5e6, 'k_AB': 0.0, 'k_BA': 0.0, 'p_A0': 0.0, 't_end': 6e-6},\n        # 5. Fast switching\n        {'a': 2.5e-10, 'r_A': 1.0e7, 'r_B': 1.0e6, 'k_AB': 5.0e7, 'k_BA': 5.0e7, 'p_A0': 0.5, 't_end': 2e-6},\n    ]\n\n    N = 5000  # Number of trajectories\n    seed = 12345\n    \n    absolute_errors = []\n\n    for params in test_cases:\n        analytic_result = calculate_analytic_msd(**params)\n        kmc_result = run_kmc_simulation(N=N, seed=seed, **params)\n        error = abs(kmc_result - analytic_result)\n        absolute_errors.append(error)\n\n    print(f\"[{','.join(f'{err:.6e}' for err in absolute_errors)}]\")\n\ndef calculate_analytic_msd(a, r_A, r_B, k_AB, k_BA, p_A0, t_end):\n    \"\"\"\n    Calculates the analytical mean-squared displacement.\n    \"\"\"\n    k = k_AB + k_BA\n    \n    if k == 0:\n        p_B0 = 1.0 - p_A0\n        expected_jumps = (r_A * p_A0 + r_B * p_B0) * t_end\n    else:\n        p_A_inf = k_BA / k\n        p_B_inf = k_AB / k\n        \n        avg_rate_inf = r_A * p_A_inf + r_B * p_B_inf\n        term1 = avg_rate_inf * t_end\n        \n        prefactor = (r_A - r_B) / k\n        p_A_diff = p_A0 - p_A_inf\n        exp_term = 1.0 - np.exp(-k * t_end)\n        \n        term2 = prefactor * p_A_diff * exp_term\n        \n        expected_jumps = term1 + term2\n        \n    msd = a**2 * expected_jumps\n    return msd\n\ndef run_kmc_simulation(N, seed, a, r_A, r_B, k_AB, k_BA, p_A0, t_end):\n    \"\"\"\n    Runs the KMC simulation to estimate the mean-squared displacement.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    squared_displacements = np.zeros(N)\n\n    # Pre-calculate rates for each channel\n    R_A = r_A + k_AB\n    R_B = r_B + k_BA\n\n    # Probabilities for event selection in channel A\n    if R_A  0:\n        p_A_jump_left = (r_A / 2) / R_A\n        p_A_jump_right = r_A / R_A\n    \n    # Probabilities for event selection in channel B\n    if R_B  0:\n        p_B_jump_left = (r_B / 2) / R_B\n        p_B_jump_right = r_B / R_B\n\n    for i in range(N):\n        time = 0.0\n        position = 0.0\n        \n        # Initialize channel state\n        if rng.random()  p_A0:\n            channel = 'A'\n        else:\n            channel = 'B'\n\n        while time  t_end:\n            if channel == 'A':\n                R_S = R_A\n            else: # channel == 'B'\n                R_S = R_B\n\n            if R_S == 0:\n                break # Trapped state, no more events can occur\n\n            # Draw time step from exponential distribution\n            u1 = rng.random()\n            dt = -np.log(u1) / R_S\n            \n            if time + dt  t_end:\n                break # Event occurs after t_end, so discard it and stop\n\n            time += dt\n            \n            # Select and execute event\n            u2 = rng.random()\n            if channel == 'A':\n                if u2  p_A_jump_left:\n                    position -= a\n                elif u2  p_A_jump_right:\n                    position += a\n                else:\n                    channel = 'B'\n            else: # channel == 'B'\n                if u2  p_B_jump_left:\n                    position -= a\n                elif u2  p_B_jump_right:\n                    position += a\n                else:\n                    channel = 'A'\n        \n        squared_displacements[i] = position**2\n        \n    return np.mean(squared_displacements)\n\nsolve()\n```"
        },
        {
            "introduction": "A KMC simulation is only as reliable as the physical model—the event catalog—it is based on. This final practice delves into the critical, advanced topic of model validation. You will develop and implement a robust statistical diagnostic to test whether a given event catalog is complete or if there are missing processes influencing the system's dynamics. Mastering this technique is crucial for ensuring the fidelity and predictive power of your simulations .",
            "id": "3851111",
            "problem": "Consider a fixed configuration in Kinetic Monte Carlo (KMC), also known as a Continuous-Time Markov Chain (CTMC). In the residence time algorithm, the system waits a random time until the next event occurs. At a fixed configuration, the event catalog lists possible transitions with their rates. The total hazard at the configuration is the sum of the cataloged rates. When the catalog is complete and the rates are time-invariant, the residence time distribution must be exponential due to the Poissonian nature of independent event occurrences.\n\nStarting from the fundamental base that independent event occurrences with constant hazard yield exponentially distributed waiting times, and the memoryless property characterizes exponential distributions, derive a diagnostic that decides whether an event catalog is incomplete at a fixed configuration by testing whether observed residence times deviate from exponential statistics implied by the catalog.\n\nYour diagnostic must be based on the following elements:\n- Let the cataloged event rates at the configuration be $\\{r_k\\}_{k=1}^m$, with physical units $\\mathrm{s}^{-1}$.\n- Let $\\lambda_{\\text{catalog}} = \\sum_{k=1}^m r_k$ be the total cataloged hazard rate, with units $\\mathrm{s}^{-1}$.\n- Let the observed residence times at the configuration be $\\{t_i\\}_{i=1}^n$, measured in seconds.\n- Under a complete catalog with constant rates, the theoretical survival function is $S(t) = \\exp(-\\lambda_{\\text{catalog}} t)$ and the cumulative distribution function is $F(t) = 1 - \\exp(-\\lambda_{\\text{catalog}} t)$.\n- Use the probability integral transform to map each observed time to $u_i = F(t_i) = 1 - \\exp(-\\lambda_{\\text{catalog}} t_i)$. Under the null hypothesis of a complete catalog with constant rates, $\\{u_i\\}$ should be independent and identically distributed according to the Uniform distribution on $[0,1]$.\n- Implement a one-sample Kolmogorov–Smirnov (KS) test for Uniform$(0,1)$ on $\\{u_i\\}$ at a given significance level $\\alpha$ (expressed as a decimal in $[0,1]$). If the resulting $p$-value is less than $\\alpha$, declare the catalog incomplete for that configuration; otherwise, declare it not incomplete.\n\nYour program must implement the above diagnostic and apply it to the following test suite. The program must use a pseudo-random number generator with the specified seed to synthesize the observed residence times. All residence times are measured in seconds; all rates are in $\\mathrm{s}^{-1}$; the significance level $\\alpha$ is dimensionless.\n\nTest suite (each case provides $(\\text{seed}, \\{r_k\\}, n, \\text{generation rule}, \\alpha)$):\n- Case A (happy path, complete catalog, moderate sample size): $(\\; \\text{seed}=1, \\{r_k\\}=[1.0, 2.0], n=200, \\text{generate } t_i \\sim \\text{Exponential}(\\lambda=3.0), \\alpha=0.05 \\;)$.\n- Case B (incomplete catalog, strong deviation): $(\\; \\text{seed}=2, \\{r_k\\}=[0.5, 0.5], n=200, \\text{generate } t_i \\sim \\text{Exponential}(\\lambda=2.0), \\alpha=0.05 \\;)$.\n- Case C (boundary condition, very small sample size): $(\\; \\text{seed}=3, \\{r_k\\}=[0.1], n=5, \\text{generate } t_i \\sim \\text{Exponential}(\\lambda=0.1), \\alpha=0.05 \\;)$.\n- Case D (edge case, non-exponential due to rate heterogeneity): $(\\; \\text{seed}=4, \\{r_k\\}=[2.0], n=200, \\text{generate } t_i \\text{ as a mixture: with probability } 0.5 \\text{ use } \\text{Exponential}(\\lambda=1.0) \\text{ and with probability } 0.5 \\text{ use } \\text{Exponential}(\\lambda=3.0), \\alpha=0.05 \\;)$.\n\nHere $\\text{Exponential}(\\lambda)$ denotes the exponential distribution with probability density function $f(t) = \\lambda \\exp(-\\lambda t)$ for $t \\ge 0$, with $\\lambda$ in $\\mathrm{s}^{-1}$ and $t$ in seconds.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result_A,result_B,result_C,result_D]\"), where each entry is a boolean equal to True if the catalog is diagnosed as incomplete for that case, and False otherwise. No other output is permitted.",
            "solution": "The problem requires the formulation and implementation of a statistical diagnostic to assess the completeness of an event catalog in a Kinetic Monte Carlo (KMC) simulation at a fixed system configuration. The diagnostic is predicated on the fundamental principles of Continuous-Time Markov Chains (CTMCs), which form the theoretical basis for KMC.\n\nA core tenet of this theory is that if a system is in a specific state, and there are multiple independent, possible transitions (events) out of that state, each occurring with a constant hazard rate, then the time the system spends in the current state—the residence time—is a random variable following an exponential distribution. The rate parameter of this exponential distribution, $\\lambda$, is the sum of the individual hazard rates of all possible transitions. This is a direct consequence of the process being a Poisson process.\n\nThe diagnostic procedure leverages this principle to test a null hypothesis.\n\n**1. Formulation of the Null Hypothesis**\n\nLet the catalog of known events at a fixed configuration be associated with a set of constant rates $\\{r_k\\}_{k=1}^m$. The total cataloged rate, or hazard, is $\\lambda_{\\text{catalog}} = \\sum_{k=1}^m r_k$. The units of the rates are inverse time, e.g., $\\mathrm{s}^{-1}$.\n\nThe null hypothesis, $H_0$, posits that this catalog is complete. If $H_0$ is true, the true total rate of all events, $\\lambda_{\\text{true}}$, is equal to the cataloged rate, $\\lambda_{\\text{catalog}}$. Consequently, the observed residence times, $\\{t_i\\}_{i=1}^n$, constitute a sample drawn from an exponential distribution with rate parameter $\\lambda_{\\text{catalog}}$. The probability density function (PDF) is $f(t) = \\lambda_{\\text{catalog}} \\exp(-\\lambda_{\\text{catalog}} t)$ for $t \\ge 0$, and the cumulative distribution function (CDF) is $F(t) = 1 - \\exp(-\\lambda_{\\text{catalog}} t)$.\n\nThe alternative hypothesis, $H_1$, is that the catalog is incomplete. This could mean there are unknown events, such that $\\lambda_{\\text{true}}  \\lambda_{\\text{catalog}}$, or that the underlying process is not a simple Poisson process (e.g., rates are not truly constant), causing the residence time distribution to deviate from the exponential form predicted by the catalog.\n\n**2. The Probability Integral Transform (PIT)**\n\nThe Probability Integral Transform is a foundational result in statistics stating that if a continuous random variable $T$ has a CDF of $F_T(t)$, then the transformed random variable $U = F_T(T)$ follows a standard uniform distribution on the interval $[0, 1]$.\n\nWe apply this transform to the observed residence times $\\{t_i\\}_{i=1}^n$ using the CDF derived from the null hypothesis, $F(t; \\lambda_{\\text{catalog}})$. This yields a new set of values $\\{u_i\\}_{i=1}^n$:\n$$\nu_i = F(t_i; \\lambda_{\\text{catalog}}) = 1 - \\exp(-\\lambda_{\\text{catalog}} t_i)\n$$\nIf the null hypothesis $H_0$ is true, the set $\\{u_i\\}$ will be a sample of independent and identically distributed random variables from the $\\text{Uniform}(0,1)$ distribution. If $H_0$ is false, the distribution of $\\{u_i\\}$ will systematically deviate from uniformity. For example, if $\\lambda_{\\text{true}}  \\lambda_{\\text{catalog}}$, the observed times $t_i$ will tend to be smaller than what is expected under $H_0$. Applying the transform $F(t; \\lambda_{\\text{catalog}})$ will map these smaller times to values that are skewed towards the lower end of the $[0, 1]$ interval, breaking the uniformity.\n\n**3. The Kolmogorov-Smirnov (KS) Test**\n\nTo test whether the sample $\\{u_i\\}$ is drawn from a $\\text{Uniform}(0,1)$ distribution, we employ the one-sample Kolmogorov-Smirnov (KS) test. This non-parametric test compares the empirical cumulative distribution function (ECDF) of the sample, $\\hat{F}_n(u)$, with the theoretical CDF of the hypothesized distribution. For $\\text{Uniform}(0,1)$, the theoretical CDF is simply $G(u) = u$ for $u \\in [0, 1]$.\n\nThe KS test statistic, $D_n$, is the maximum absolute difference between the ECDF and the theoretical CDF over all possible values:\n$$\nD_n = \\sup_{u} |\\hat{F}_n(u) - G(u)|\n$$\nwhere $\\hat{F}_n(u) = \\frac{1}{n} \\sum_{i=1}^n \\mathbf{1}_{u_i \\le u}$ is the ECDF of the sample $\\{u_i\\}$. The KS test provides a $p$-value, which is the probability of observing a $D_n$ statistic at least this large, assuming $H_0$ is true.\n\n**4. Decision Rule**\n\nA pre-determined significance level, $\\alpha$, is used as a threshold for the decision. A typical value for $\\alpha$ is $0.05$.\n- If $p\\text{-value}  \\alpha$: We reject the null hypothesis $H_0$. The observed data is statistically inconsistent with the hypothesis that the catalog is complete. We conclude the catalog is incomplete.\n- If $p\\text{-value} \\ge \\alpha$: We fail to reject the null hypothesis $H_0$. There is not enough statistical evidence to claim the catalog is incomplete.\n\n**Application to Test Cases:**\n\n- **Case A (Happy Path):** The cataloged rate is $\\lambda_{\\text{catalog}} = 1.0 + 2.0 = 3.0 \\;\\mathrm{s}^{-1}$. The data is generated with $\\lambda_{\\text{true}} = 3.0 \\;\\mathrm{s}^{-1}$. Since $\\lambda_{\\text{true}} = \\lambda_{\\text{catalog}}$, the null hypothesis is true. With a sufficient sample size of $n=200$, the $p$-value is expected to be greater than $\\alpha=0.05$, leading to the conclusion that the catalog is not incomplete.\n\n- **Case B (Incomplete Catalog):** The cataloged rate is $\\lambda_{\\text{catalog}} = 0.5 + 0.5 = 1.0 \\;\\mathrm{s}^{-1}$. The data is generated with $\\lambda_{\\text{true}} = 2.0 \\;\\mathrm{s}^{-1}$. Here, $\\lambda_{\\text{true}}  \\lambda_{\\text{catalog}}$, so the catalog is genuinely incomplete. The observed residence times will be systematically shorter than predicted by the catalog. The transformed values $\\{u_i\\}$ will not be uniform, and the KS test is expected to yield a $p$-value less than $\\alpha=0.05$, correctly flagging the catalog as incomplete.\n\n- **Case C (Small Sample Size):** The cataloged rate is $\\lambda_{\\text{catalog}} = 0.1 \\;\\mathrm{s}^{-1}$, and the data is generated with $\\lambda_{\\text{true}} = 0.1 \\;\\mathrm{s}^{-1}$. The null hypothesis is true. However, the sample size is very small ($n=5$). While we expect to fail to reject $H_0$, it is important to note that statistical tests have low power for small samples, making it difficult to detect even real deviations from the null hypothesis.\n\n- **Case D (Non-Exponential Data):** The cataloged rate is $\\lambda_{\\text{catalog}} = 2.0 \\;\\mathrm{s}^{-1}$. The data is generated from a mixture of two exponential distributions. This means the underlying data-generating process is not a simple Poisson process with a single rate. The residence time distribution is not exponential with rate $\\lambda=2.0$. The null hypothesis is false. The KS test is sensitive to differences in distribution shape, so it is expected to detect this mismatch, produce a small $p$-value ($ 0.05$), and correctly identify the model specified by the catalog as being inconsistent with the data.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import kstest\n\ndef solve():\n    \"\"\"\n    Implements a statistical diagnostic to validate KMC event catalogs\n    and applies it to a suite of test cases.\n    \"\"\"\n    # Test suite format: \n    # (case_id, seed, catalog_rates_rk, sample_size_n, generation_spec, alpha)\n    # generation_spec defines how the \"observed\" residence times are created.\n    # ('exp', lambda_true) - Exponential(lambda_true)\n    # ('mix', p1, lambda1, p2, lambda2) - p1*Exp(lambda1) + p2*Exp(lambda2)\n    test_cases = [\n        ('A', 1, [1.0, 2.0], 200, ('exp', 3.0), 0.05),\n        ('B', 2, [0.5, 0.5], 200, ('exp', 2.0), 0.05),\n        ('C', 3, [0.1], 5, ('exp', 0.1), 0.05),\n        ('D', 4, [2.0], 200, ('mix', 0.5, 1.0, 0.5, 3.0), 0.05),\n    ]\n\n    results = []\n    for _, seed, r_k, n, gen_spec, alpha in test_cases:\n        # Step 1: Calculate the total cataloged hazard rate\n        lambda_catalog = np.sum(r_k)\n        \n        # Initialize a pseudo-random number generator with the specified seed\n        rng = np.random.default_rng(seed)\n        \n        # Step 2: Generate the synthetic \"observed\" residence times\n        t_obs = None\n        gen_type = gen_spec[0]\n        \n        if gen_type == 'exp':\n            # Generate from a single exponential distribution.\n            # numpy.random.exponential uses scale = 1/lambda.\n            lambda_true = gen_spec[1]\n            scale_true = 1.0 / lambda_true\n            t_obs = rng.exponential(scale=scale_true, size=n)\n            \n        elif gen_type == 'mix':\n            # Generate from a mixture of two exponential distributions.\n            p1, lambda1, _, lambda2 = gen_spec[1:]\n            scale1 = 1.0 / lambda1\n            scale2 = 1.0 / lambda2\n            \n            # Generate choices based on probability p1\n            choices = rng.uniform(size=n)  p1\n            \n            # Generate samples from both distributions\n            samples1 = rng.exponential(scale=scale1, size=n)\n            samples2 = rng.exponential(scale=scale2, size=n)\n            \n            # Combine them based on the choices\n            t_obs = np.where(choices, samples1, samples2)\n            \n        # Step 3: Apply the Probability Integral Transform (PIT)\n        # Under the null hypothesis H0, the times t_i follow Exp(lambda_catalog).\n        # The CDF is F(t) = 1 - exp(-lambda_catalog * t).\n        # The transformed variable u_i = F(t_i) should be Uniform(0,1).\n        u_transformed = 1.0 - np.exp(-lambda_catalog * t_obs)\n        \n        # Step 4: Perform a one-sample Kolmogorov-Smirnov test for uniformity\n        # The `scipy.stats.kstest` function can test against a named distribution.\n        # 'uniform' corresponds to the standard Uniform(0,1) distribution.\n        ks_result = kstest(u_transformed, 'uniform')\n        p_value = ks_result.pvalue\n        \n        # Step 5: Make a decision based on the significance level alpha\n        # If p-value  alpha, we reject H0 and declare the catalog incomplete.\n        is_incomplete = p_value  alpha\n        results.append(is_incomplete)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}