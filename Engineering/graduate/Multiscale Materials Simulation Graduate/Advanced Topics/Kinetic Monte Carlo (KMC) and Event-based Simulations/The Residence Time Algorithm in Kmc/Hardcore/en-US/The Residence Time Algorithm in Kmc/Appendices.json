{
    "hands_on_practices": [
        {
            "introduction": "The Residence Time Algorithm is built upon a solid foundation of stochastic process theory. This first practice exercise takes you back to these fundamentals, guiding you to derive the probability distribution for the simulation time step, $\\Delta t$, directly from the assumption of independent Poisson processes . By calculating its expectation and variance, you will gain a core understanding of how the algorithm advances time and why the total event rate is a crucial parameter.",
            "id": "3851130",
            "problem": "Consider a kinetic Monte Carlo (KMC) simulation of a crystalline surface in which three independent elementary processes can occur at any instant: a vacancy hop, an adatom detachment, and a near-neighbor exchange. Assume each process is modeled as an independent Poisson process with constant rates $r_{1}$, $r_{2}$, and $r_{3}$, respectively, and that the simulation uses the residence time algorithm to advance the clock by the time $\\Delta t$ to the next event. The physical assumption underlying the residence time algorithm is that the next event time is the first arrival among these independent Poisson processes, and the process is Markovian.\n\nStarting from the independence of the events and the definition of the survival function $S(t)$ as the probability that no event has occurred by time $t$, derive the probability density function $f_{\\Delta t}(t)$ of $\\Delta t$ in terms of the total rate $R_{\\text{tot}}=\\sum_{i=1}^{3} r_{i}$. Then, using this $f_{\\Delta t}(t)$, compute the expectation $\\mathbb{E}[\\Delta t]$ and the variance $\\mathrm{Var}[\\Delta t]$ as functions of $R_{\\text{tot}}$.\n\nFinally, instantiate the rates as $r_{1}=3\\,\\mathrm{s}^{-1}$, $r_{2}=5\\,\\mathrm{s}^{-1}$, and $r_{3}=2\\,\\mathrm{s}^{-1}$, and evaluate the numerical values of $\\mathbb{E}[\\Delta t]$ in seconds and $\\mathrm{Var}[\\Delta t]$ in seconds squared. Provide exact numerical values; no rounding is required. Report your final pair of numerical values in the order $\\mathbb{E}[\\Delta t]$, $\\mathrm{Var}[\\Delta t]$.",
            "solution": "The problem statement will first be validated against the required criteria.\n\n### Step 1: Extract Givens\n- Three independent elementary processes: vacancy hop, adatom detachment, near-neighbor exchange.\n- Each process is modeled as an independent Poisson process with constant rates $r_{1}$, $r_{2}$, and $r_{3}$, respectively.\n- The simulation uses the residence time algorithm to advance the clock by time $\\Delta t$.\n- Physical assumption: $\\Delta t$ is the first arrival time among the independent Poisson processes. The process is Markovian.\n- Starting point: The survival function $S(t)$ is the probability that no event has occurred by time $t$.\n- Total rate is defined as $R_{\\text{tot}}=\\sum_{i=1}^{3} r_{i}$.\n- Objective 1: Derive the probability density function $f_{\\Delta t}(t)$ of $\\Delta t$ in terms of $R_{\\text{tot}}$.\n- Objective 2: Compute the expectation $\\mathbb{E}[\\Delta t]$ and the variance $\\mathrm{Var}[\\Delta t]$ as functions of $R_{\\text{tot}}$.\n- Objective 3: Instantiate rates as $r_{1}=3\\,\\mathrm{s}^{-1}$, $r_{2}=5\\,\\mathrm{s}^{-1}$, and $r_{3}=2\\,\\mathrm{s}^{-1}$.\n- Objective 4: Evaluate the numerical values of $\\mathbb{E}[\\Delta t]$ and $\\mathrm{Var}[\\Delta t]$ and report them in the specified order.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is subjected to scrutiny based on the established criteria.\n\n- **Scientifically Grounded (Critical)**: The problem is firmly based on the theory of stochastic processes, specifically Poisson processes and exponential distributions. The residence time algorithm (also known as the BKL algorithm or standard KMC) is a fundamental and widely used method in computational materials science and chemistry. The assumptions (independent Poisson processes, Markovian behavior) are standard for this type of model. The problem is scientifically sound.\n- **Well-Posed**: The problem provides all necessary information to derive the requested quantities. The relationships between rates, probabilities, and time evolution are clearly defined within the framework of probability theory. A unique and meaningful solution exists.\n- **Objective (Critical)**: The language is precise, quantitative, and free of any subjective or ambiguous terminology. The tasks are clearly stated mathematical objectives.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. It is a standard, well-posed problem in statistical mechanics and simulation theory. The solution process will now proceed.\n\n### Solution Derivation\n\nLet $T_i$ be the waiting time for the $i$-th process, where $i \\in \\{1, 2, 3\\}$. Since each process is a Poisson process with rate $r_i$, the waiting time $T_i$ is a random variable following an exponential distribution with parameter $r_i$. The probability density function (PDF) for $T_i$ is $f_{T_i}(t) = r_i \\exp(-r_i t)$ for $t \\ge 0$, and the cumulative distribution function (CDF) is $F_{T_i}(t) = P(T_i \\le t) = 1 - \\exp(-r_i t)$.\n\nThe survival function for the $i$-th process, $S_i(t)$, is the probability that process $i$ has not occurred by time $t$. This is given by:\n$$S_i(t) = P(T_i  t) = 1 - F_{T_i}(t) = \\exp(-r_i t)$$\n\nThe time to the next event in the KMC simulation, $\\Delta t$, is the minimum of the waiting times for all possible independent processes.\n$$\\Delta t = \\min(T_1, T_2, T_3)$$\n\nThe survival function for $\\Delta t$, denoted $S(t)$ as per the problem statement, is the probability that the time to the next event is greater than $t$. This is equivalent to the probability that none of the three events have occurred by time $t$.\n$$S(t) = P(\\Delta t  t) = P(\\min(T_1, T_2, T_3)  t)$$\n\nFor the minimum of several random variables to be greater than $t$, each individual random variable must be greater than $t$.\n$$S(t) = P(T_1  t \\text{ and } T_2  t \\text{ and } T_3  t)$$\n\nSince the processes are independent, the joint probability is the product of the individual probabilities:\n$$S(t) = P(T_1  t) P(T_2  t) P(T_3  t) = S_1(t) S_2(t) S_3(t)$$\n\nSubstituting the expressions for the individual survival functions:\n$$S(t) = \\exp(-r_1 t) \\exp(-r_2 t) \\exp(-r_3 t) = \\exp(-(r_1 + r_2 + r_3)t)$$\n\nUsing the definition of the total rate, $R_{\\text{tot}} = r_1 + r_2 + r_3$, the survival function becomes:\n$$S(t) = \\exp(-R_{\\text{tot}} t)$$\n\nThe probability density function $f_{\\Delta t}(t)$ is obtained by taking the negative derivative of the survival function with respect to $t$.\n$$f_{\\Delta t}(t) = -\\frac{d}{dt}S(t) = -\\frac{d}{dt}\\left(\\exp(-R_{\\text{tot}} t)\\right) = -(-R_{\\text{tot}} \\exp(-R_{\\text{tot}} t))$$\n$$f_{\\Delta t}(t) = R_{\\text{tot}} \\exp(-R_{\\text{tot}} t) \\quad \\text{for } t \\ge 0$$\nThis is the PDF of an exponential distribution with rate parameter $R_{\\text{tot}}$.\n\nNext, we compute the expectation $\\mathbb{E}[\\Delta t]$ and variance $\\mathrm{Var}[\\Delta t]$.\nThe expectation value is calculated by definition:\n$$\\mathbb{E}[\\Delta t] = \\int_{0}^{\\infty} t f_{\\Delta t}(t) dt = \\int_{0}^{\\infty} t (R_{\\text{tot}} \\exp(-R_{\\text{tot}} t)) dt$$\nWe use integration by parts, $\\int u \\, dv = uv - \\int v \\, du$, with $u = t$ and $dv = R_{\\text{tot}} \\exp(-R_{\\text{tot}} t) dt$. This gives $du = dt$ and $v = -\\exp(-R_{\\text{tot}} t)$.\n$$\\mathbb{E}[\\Delta t] = \\left[ -t \\exp(-R_{\\text{tot}} t) \\right]_{0}^{\\infty} - \\int_{0}^{\\infty} (-\\exp(-R_{\\text{tot}} t)) dt$$\nThe first term evaluates to $0$, since $\\lim_{t\\to\\infty} t \\exp(-R_{\\text{tot}} t) = 0$ for $R_{\\text{tot}}  0$.\n$$\\mathbb{E}[\\Delta t] = \\int_{0}^{\\infty} \\exp(-R_{\\text{tot}} t) dt = \\left[ -\\frac{1}{R_{\\text{tot}}} \\exp(-R_{\\text{tot}} t) \\right]_{0}^{\\infty} = 0 - \\left(-\\frac{1}{R_{\\text{tot}}}\\right) = \\frac{1}{R_{\\text{tot}}}$$\n\nTo find the variance, we first compute the second moment, $\\mathbb{E}[(\\Delta t)^2]$.\n$$\\mathbb{E}[(\\Delta t)^2] = \\int_{0}^{\\infty} t^2 f_{\\Delta t}(t) dt = \\int_{0}^{\\infty} t^2 (R_{\\text{tot}} \\exp(-R_{\\text{tot}} t)) dt$$\nAgain, we use integration by parts with $u = t^2$ and $dv = R_{\\text{tot}} \\exp(-R_{\\text{tot}} t) dt$. This gives $du = 2t \\, dt$ and $v = -\\exp(-R_{\\text{tot}} t)$.\n$$\\mathbb{E}[(\\Delta t)^2] = \\left[ -t^2 \\exp(-R_{\\text{tot}} t) \\right]_{0}^{\\infty} - \\int_{0}^{\\infty} (-\\exp(-R_{\\text{tot}} t))(2t) dt$$\nThe first term is $0$.\n$$\\mathbb{E}[(\\Delta t)^2] = 2 \\int_{0}^{\\infty} t \\exp(-R_{\\text{tot}} t) dt$$\nWe can recognize the integral as being related to the expectation calculation. From $\\mathbb{E}[\\Delta t] = R_{\\text{tot}} \\int_{0}^{\\infty} t \\exp(-R_{\\text{tot}} t) dt = \\frac{1}{R_{\\text{tot}}}$, we know that $\\int_{0}^{\\infty} t \\exp(-R_{\\text{tot}} t) dt = \\frac{1}{R_{\\text{tot}}^2}$.\n$$\\mathbb{E}[(\\Delta t)^2] = 2 \\left( \\frac{1}{R_{\\text{tot}}^2} \\right) = \\frac{2}{R_{\\text{tot}}^2}$$\n\nThe variance is given by $\\mathrm{Var}[\\Delta t] = \\mathbb{E}[(\\Delta t)^2] - (\\mathbb{E}[\\Delta t])^2$.\n$$\\mathrm{Var}[\\Delta t] = \\frac{2}{R_{\\text{tot}}^2} - \\left(\\frac{1}{R_{\\text{tot}}}\\right)^2 = \\frac{1}{R_{\\text{tot}}^2}$$\n\nFinally, we substitute the given numerical values for the rates:\n$r_{1}=3\\,\\mathrm{s}^{-1}$, $r_{2}=5\\,\\mathrm{s}^{-1}$, and $r_{3}=2\\,\\mathrm{s}^{-1}$.\nThe total rate $R_{\\text{tot}}$ is:\n$$R_{\\text{tot}} = r_1 + r_2 + r_3 = 3\\,\\mathrm{s}^{-1} + 5\\,\\mathrm{s}^{-1} + 2\\,\\mathrm{s}^{-1} = 10\\,\\mathrm{s}^{-1}$$\n\nNow we can evaluate the expectation and variance:\n$$\\mathbb{E}[\\Delta t] = \\frac{1}{R_{\\text{tot}}} = \\frac{1}{10\\,\\mathrm{s}^{-1}} = 0.1\\,\\mathrm{s}$$\n$$\\mathrm{Var}[\\Delta t] = \\frac{1}{R_{\\text{tot}}^2} = \\frac{1}{(10\\,\\mathrm{s}^{-1})^2} = \\frac{1}{100\\,\\mathrm{s}^{-2}} = 0.01\\,\\mathrm{s}^2$$\n\nThe requested numerical values are $0.1$ for the expectation and $0.01$ for the variance.",
            "answer": "$$\\boxed{\\begin{pmatrix} 0.1  0.01 \\end{pmatrix}}$$"
        },
        {
            "introduction": "Having established the theoretical exponential distribution of residence times, we now turn to a critical task in computational modeling: validation. This practice demonstrates how to transform theoretical knowledge into a powerful diagnostic tool for verifying if a KMC simulation's event catalog is complete . You will implement a statistical test to determine if simulated residence times conform to the expected behavior, a crucial skill for debugging and ensuring the physical accuracy of your models.",
            "id": "3851111",
            "problem": "Consider a fixed configuration in Kinetic Monte Carlo (KMC), also known as a Continuous-Time Markov Chain (CTMC). In the residence time algorithm, the system waits a random time until the next event occurs. At a fixed configuration, the event catalog lists possible transitions with their rates. The total hazard at the configuration is the sum of the cataloged rates. When the catalog is complete and the rates are time-invariant, the residence time distribution must be exponential due to the Poissonian nature of independent event occurrences.\n\nStarting from the fundamental base that independent event occurrences with constant hazard yield exponentially distributed waiting times, and the memoryless property characterizes exponential distributions, derive a diagnostic that decides whether an event catalog is incomplete at a fixed configuration by testing whether observed residence times deviate from exponential statistics implied by the catalog.\n\nYour diagnostic must be based on the following elements:\n- Let the cataloged event rates at the configuration be $\\{r_k\\}_{k=1}^m$, with physical units $\\mathrm{s}^{-1}$.\n- Let $\\lambda_{\\text{catalog}} = \\sum_{k=1}^m r_k$ be the total cataloged hazard rate, with units $\\mathrm{s}^{-1}$.\n- Let the observed residence times at the configuration be $\\{t_i\\}_{i=1}^n$, measured in seconds.\n- Under a complete catalog with constant rates, the theoretical survival function is $S(t) = \\exp(-\\lambda_{\\text{catalog}} t)$ and the cumulative distribution function is $F(t) = 1 - \\exp(-\\lambda_{\\text{catalog}} t)$.\n- Use the probability integral transform to map each observed time to $u_i = F(t_i) = 1 - \\exp(-\\lambda_{\\text{catalog}} t_i)$. Under the null hypothesis of a complete catalog with constant rates, $\\{u_i\\}$ should be independent and identically distributed according to the Uniform distribution on $[0,1]$.\n- Implement a one-sample Kolmogorov–Smirnov (KS) test for Uniform$(0,1)$ on $\\{u_i\\}$ at a given significance level $\\alpha$ (expressed as a decimal in $[0,1]$). If the resulting $p$-value is less than $\\alpha$, declare the catalog incomplete for that configuration; otherwise, declare it not incomplete.\n\nYour program must implement the above diagnostic and apply it to the following test suite. The program must use a pseudo-random number generator with the specified seed to synthesize the observed residence times. All residence times are measured in seconds; all rates are in $\\mathrm{s}^{-1}$; the significance level $\\alpha$ is dimensionless.\n\nTest suite (each case provides $(\\text{seed}, \\{r_k\\}, n, \\text{generation rule}, \\alpha)$):\n- Case A (happy path, complete catalog, moderate sample size): $(\\; \\text{seed}=1, \\{r_k\\}=[1.0, 2.0], n=200, \\text{generate } t_i \\sim \\text{Exponential}(\\lambda=3.0), \\alpha=0.05 \\;)$.\n- Case B (incomplete catalog, strong deviation): $(\\; \\text{seed}=2, \\{r_k\\}=[0.5, 0.5], n=200, \\text{generate } t_i \\sim \\text{Exponential}(\\lambda=2.0), \\alpha=0.05 \\;)$.\n- Case C (boundary condition, very small sample size): $(\\; \\text{seed}=3, \\{r_k\\}=[0.1], n=5, \\text{generate } t_i \\sim \\text{Exponential}(\\lambda=0.1), \\alpha=0.05 \\;)$.\n- Case D (edge case, non-exponential due to rate heterogeneity): $(\\; \\text{seed}=4, \\{r_k\\}=[2.0], n=200, \\text{generate } t_i \\text{ as a mixture: with probability } 0.5 \\text{ use } \\text{Exponential}(\\lambda=1.0) \\text{ and with probability } 0.5 \\text{ use } \\text{Exponential}(\\lambda=3.0), \\alpha=0.05 \\;)$.\n\nHere $\\text{Exponential}(\\lambda)$ denotes the exponential distribution with probability density function $f(t) = \\lambda \\exp(-\\lambda t)$ for $t \\ge 0$, with $\\lambda$ in $\\mathrm{s}^{-1}$ and $t$ in seconds.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result_A,result_B,result_C,result_D]\"), where each entry is a boolean equal to True if the catalog is diagnosed as incomplete for that case, and False otherwise. No other output is permitted.",
            "solution": "The problem requires the formulation and implementation of a statistical diagnostic to assess the completeness of an event catalog in a Kinetic Monte Carlo (KMC) simulation at a fixed system configuration. The diagnostic is predicated on the fundamental principles of Continuous-Time Markov Chains (CTMCs), which form the theoretical basis for KMC.\n\nA core tenet of this theory is that if a system is in a specific state, and there are multiple independent, possible transitions (events) out of that state, each occurring with a constant hazard rate, then the time the system spends in the current state—the residence time—is a random variable following an exponential distribution. The rate parameter of this exponential distribution, $\\lambda$, is the sum of the individual hazard rates of all possible transitions. This is a direct consequence of the process being a Poisson process.\n\nThe diagnostic procedure leverages this principle to test a null hypothesis.\n\n**1. Formulation of the Null Hypothesis**\n\nLet the catalog of known events at a fixed configuration be associated with a set of constant rates $\\{r_k\\}_{k=1}^m$. The total cataloged rate, or hazard, is $\\lambda_{\\text{catalog}} = \\sum_{k=1}^m r_k$. The units of the rates are inverse time, e.g., $\\mathrm{s}^{-1}$.\n\nThe null hypothesis, $H_0$, posits that this catalog is complete. If $H_0$ is true, the true total rate of all events, $\\lambda_{\\text{true}}$, is equal to the cataloged rate, $\\lambda_{\\text{catalog}}$. Consequently, the observed residence times, $\\{t_i\\}_{i=1}^n$, constitute a sample drawn from an exponential distribution with rate parameter $\\lambda_{\\text{catalog}}$. The probability density function (PDF) is $f(t) = \\lambda_{\\text{catalog}} \\exp(-\\lambda_{\\text{catalog}} t)$ for $t \\ge 0$, and the cumulative distribution function (CDF) is $F(t) = 1 - \\exp(-\\lambda_{\\text{catalog}} t)$.\n\nThe alternative hypothesis, $H_1$, is that the catalog is incomplete. This could mean there are unknown events, such that $\\lambda_{\\text{true}}  \\lambda_{\\text{catalog}}$, or that the underlying process is not a simple Poisson process (e.g., rates are not truly constant), causing the residence time distribution to deviate from the exponential form predicted by the catalog.\n\n**2. The Probability Integral Transform (PIT)**\n\nThe Probability Integral Transform is a foundational result in statistics stating that if a continuous random variable $T$ has a CDF of $F_T(t)$, then the transformed random variable $U = F_T(T)$ follows a standard uniform distribution on the interval $[0, 1]$.\n\nWe apply this transform to the observed residence times $\\{t_i\\}_{i=1}^n$ using the CDF derived from the null hypothesis, $F(t; \\lambda_{\\text{catalog}})$. This yields a new set of values $\\{u_i\\}_{i=1}^n$:\n$$\nu_i = F(t_i; \\lambda_{\\text{catalog}}) = 1 - \\exp(-\\lambda_{\\text{catalog}} t_i)\n$$\nIf the null hypothesis $H_0$ is true, the set $\\{u_i\\}$ will be a sample of independent and identically distributed random variables from the $\\text{Uniform}(0,1)$ distribution. If $H_0$ is false, the distribution of $\\{u_i\\}$ will systematically deviate from uniformity. For example, if $\\lambda_{\\text{true}}  \\lambda_{\\text{catalog}}$, the observed times $t_i$ will tend to be smaller than what is expected under $H_0$. Applying the transform $F(t; \\lambda_{\\text{catalog}})$ will map these smaller times to values that are skewed towards the lower end of the $[0, 1]$ interval, breaking the uniformity.\n\n**3. The Kolmogorov-Smirnov (KS) Test**\n\nTo test whether the sample $\\{u_i\\}$ is drawn from a $\\text{Uniform}(0,1)$ distribution, we employ the one-sample Kolmogorov-Smirnov (KS) test. This non-parametric test compares the empirical cumulative distribution function (ECDF) of the sample, $\\hat{F}_n(u)$, with the theoretical CDF of the hypothesized distribution. For $\\text{Uniform}(0,1)$, the theoretical CDF is simply $G(u) = u$ for $u \\in [0, 1]$.\n\nThe KS test statistic, $D_n$, is the maximum absolute difference between the ECDF and the theoretical CDF over all possible values:\n$$\nD_n = \\sup_{u} |\\hat{F}_n(u) - G(u)|\n$$\nwhere $\\hat{F}_n(u) = \\frac{1}{n} \\sum_{i=1}^n \\mathbf{1}_{u_i \\le u}$ is the ECDF of the sample $\\{u_i\\}$. The KS test provides a $p$-value, which is the probability of observing a $D_n$ statistic at least this large, assuming $H_0$ is true.\n\n**4. Decision Rule**\n\nA pre-determined significance level, $\\alpha$, is used as a threshold for the decision. A typical value for $\\alpha$ is $0.05$.\n- If $p\\text{-value}  \\alpha$: We reject the null hypothesis $H_0$. The observed data is statistically inconsistent with the hypothesis that the catalog is complete. We conclude the catalog is incomplete.\n- If $p\\text{-value} \\ge \\alpha$: We fail to reject the null hypothesis $H_0$. There is not enough statistical evidence to claim the catalog is incomplete.\n\n**Application to Test Cases:**\n\n- **Case A (Happy Path):** The cataloged rate is $\\lambda_{\\text{catalog}} = 1.0 + 2.0 = 3.0 \\;\\mathrm{s}^{-1}$. The data is generated with $\\lambda_{\\text{true}} = 3.0 \\;\\mathrm{s}^{-1}$. Since $\\lambda_{\\text{true}} = \\lambda_{\\text{catalog}}$, the null hypothesis is true. With a sufficient sample size of $n=200$, the $p$-value is expected to be greater than $\\alpha=0.05$, leading to the conclusion that the catalog is not incomplete.\n\n- **Case B (Incomplete Catalog):** The cataloged rate is $\\lambda_{\\text{catalog}} = 0.5 + 0.5 = 1.0 \\;\\mathrm{s}^{-1}$. The data is generated with $\\lambda_{\\text{true}} = 2.0 \\;\\mathrm{s}^{-1}$. Here, $\\lambda_{\\text{true}}  \\lambda_{\\text{catalog}}$, so the catalog is genuinely incomplete. The observed residence times will be systematically shorter than predicted by the catalog. The transformed values $\\{u_i\\}$ will not be uniform, and the KS test is expected to yield a $p$-value less than $\\alpha=0.05$, correctly flagging the catalog as incomplete.\n\n- **Case C (Small Sample Size):** The cataloged rate is $\\lambda_{\\text{catalog}} = 0.1 \\;\\mathrm{s}^{-1}$, and the data is generated with $\\lambda_{\\text{true}} = 0.1 \\;\\mathrm{s}^{-1}$. The null hypothesis is true. However, the sample size is very small ($n=5$). While we expect to fail to reject $H_0$, it is important to note that statistical tests have low power for small samples, making it difficult to detect even real deviations from the null hypothesis.\n\n- **Case D (Non-Exponential Data):** The cataloged rate is $\\lambda_{\\text{catalog}} = 2.0 \\;\\mathrm{s}^{-1}$. The data is generated from a mixture of two exponential distributions. This means the underlying data-generating process is not a simple Poisson process with a single rate. The residence time distribution is not exponential with rate $\\lambda=2.0$. The null hypothesis is false. The KS test is sensitive to differences in distribution shape, so it is expected to detect this mismatch, produce a small $p$-value ($ 0.05$), and correctly identify the model specified by the catalog as being inconsistent with the data.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import kstest\n\ndef solve():\n    \"\"\"\n    Implements a statistical diagnostic to validate KMC event catalogs\n    and applies it to a suite of test cases.\n    \"\"\"\n    # Test suite format: \n    # (case_id, seed, catalog_rates_rk, sample_size_n, generation_spec, alpha)\n    # generation_spec defines how the \"observed\" residence times are created.\n    # ('exp', lambda_true) - Exponential(lambda_true)\n    # ('mix', p1, lambda1, p2, lambda2) - p1*Exp(lambda1) + p2*Exp(lambda2)\n    test_cases = [\n        ('A', 1, [1.0, 2.0], 200, ('exp', 3.0), 0.05),\n        ('B', 2, [0.5, 0.5], 200, ('exp', 2.0), 0.05),\n        ('C', 3, [0.1], 5, ('exp', 0.1), 0.05),\n        ('D', 4, [2.0], 200, ('mix', 0.5, 1.0, 0.5, 3.0), 0.05),\n    ]\n\n    results = []\n    for _, seed, r_k, n, gen_spec, alpha in test_cases:\n        # Step 1: Calculate the total cataloged hazard rate\n        lambda_catalog = np.sum(r_k)\n        \n        # Initialize a pseudo-random number generator with the specified seed\n        rng = np.random.default_rng(seed)\n        \n        # Step 2: Generate the synthetic \"observed\" residence times\n        t_obs = None\n        gen_type = gen_spec[0]\n        \n        if gen_type == 'exp':\n            # Generate from a single exponential distribution.\n            # numpy.random.exponential uses scale = 1/lambda.\n            lambda_true = gen_spec[1]\n            scale_true = 1.0 / lambda_true\n            t_obs = rng.exponential(scale=scale_true, size=n)\n            \n        elif gen_type == 'mix':\n            # Generate from a mixture of two exponential distributions.\n            p1, lambda1, _, lambda2 = gen_spec[1:]\n            scale1 = 1.0 / lambda1\n            scale2 = 1.0 / lambda2\n            \n            # Generate choices based on probability p1\n            choices = rng.uniform(size=n)  p1\n            \n            # Generate samples from both distributions\n            samples1 = rng.exponential(scale=scale1, size=n)\n            samples2 = rng.exponential(scale=scale2, size=n)\n            \n            # Combine them based on the choices\n            t_obs = np.where(choices, samples1, samples2)\n            \n        # Step 3: Apply the Probability Integral Transform (PIT)\n        # Under the null hypothesis H0, the times t_i follow Exp(lambda_catalog).\n        # The CDF is F(t) = 1 - exp(-lambda_catalog * t).\n        # The transformed variable u_i = F(t_i) should be Uniform(0,1).\n        u_transformed = 1.0 - np.exp(-lambda_catalog * t_obs)\n        \n        # Step 4: Perform a one-sample Kolmogorov-Smirnov test for uniformity\n        # The `scipy.stats.kstest` function can test against a named distribution.\n        # 'uniform' corresponds to the standard Uniform(0,1) distribution.\n        ks_result = kstest(u_transformed, 'uniform')\n        p_value = ks_result.pvalue\n        \n        # Step 5: Make a decision based on the significance level alpha\n        # If p-value  alpha, we reject H0 and declare the catalog incomplete.\n        is_incomplete = p_value  alpha\n        results.append(is_incomplete)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "While correctness is paramount, the efficiency of a KMC simulation is often critical for tackling realistic problems with a vast number of possible events. This final exercise shifts focus to computational performance by analyzing the event selection step, which can become a bottleneck . You will compare the complexity of a naive linear search with a sophisticated tree-based method, deriving the update rules that make such advanced data structures efficient for dynamic systems.",
            "id": "3851175",
            "problem": "Consider a Kinetic Monte Carlo (KMC) simulation using the Residence Time Algorithm (RTA), where there are $M$ possible events indexed by $i \\in \\{1,2,\\dots,M\\}$ with nonnegative rates $\\{r_{i}\\}$ and total rate $R = \\sum_{i=1}^{M} r_{i}$. In the RTA, an event is selected with probability $r_{i}/R$ and the waiting time is sampled from an exponential distribution with mean $1/R$. Event selection is implemented by transforming a uniform random variable $u \\in (0,1)$ into a threshold $uR$ on the cumulative distribution and then identifying the smallest index $e$ such that $\\sum_{i=1}^{e} r_{i} \\geq uR$. A naive linear scan over the rates performs this selection in $\\mathcal{O}(M)$ time.\n\nTo accelerate selection, suppose the rates $\\{r_{i}\\}$ are stored in a Fenwick tree, also known as a Binary Indexed Tree (BIT). For a $1$-indexed array, the tree maintains an auxiliary array $\\{t_{j}\\}_{j=1}^{M}$ defined by $t_{j} = \\sum_{k=j - \\mathrm{lowbit}(j) + 1}^{j} r_{k}$, where $\\mathrm{lowbit}(j)$ denotes the value of the least significant set bit in the binary representation of $j$. This data structure supports prefix-sum queries and binary probing for selection in $\\mathcal{O}(\\log M)$ time.\n\nAssume that at each KMC step, exactly one local rate $r_{i}$ changes by an increment $\\Delta$, while all other rates remain fixed. Starting from the defining property of the Fenwick tree and without invoking any pre-derived update formulas, derive the mathematical rule that updates the affected entries $\\{t_{j}\\}$ due to the change of $r_{i}$ by $\\Delta$, expressed in terms of indicator functions and $\\mathrm{lowbit}(\\cdot)$.\n\nHaving established the tree-based selection complexity and the naive linear scan complexity for event selection, define the speedup factor $S(M)$ as the ratio of the asymptotic work required for selection by naive linear scan versus tree-based selection, under the assumption that elementary arithmetic and comparison operations all have unit cost and that both methods are implemented optimally within their respective paradigms. Express $S(M)$ as a closed-form function of $M$. Your final answer must be this single expression for $S(M)$, with no units and no rounding.",
            "solution": "The problem is valid as it is scientifically grounded in the principles of computational materials science and algorithm analysis, is well-posed, objective, and self-contained. We proceed with the solution in two parts as requested.\n\nFirst, we derive the mathematical rule for updating the Fenwick tree's auxiliary array $\\{t_j\\}$ when a single rate $r_i$ is changed by an increment $\\Delta$. The new rate is $r_i' = r_i + \\Delta$. For all other indices $k \\neq i$, the rates remain unchanged, $r_k' = r_k$.\n\nThe Fenwick tree structure is defined by the auxiliary array $\\{t_j\\}_{j=1}^{M}$ where each element $t_j$ is given by the sum:\n$$t_j = \\sum_{k=j - \\mathrm{lowbit}(j) + 1}^{j} r_k$$\nHere, $\\mathrm{lowbit}(j)$ is the value of the least significant set bit in the binary representation of the index $j$.\n\nLet $t_j^{\\text{old}}$ be the value of the entry before the update and $t_j^{\\text{new}}$ be the value after the update. By definition, the new value is:\n$$t_j^{\\text{new}} = \\sum_{k=j - \\mathrm{lowbit}(j) + 1}^{j} r_k'$$\nWe can separate the sum over the new rates $\\{r_k'\\}$ into terms involving the old rates $\\{r_k\\}$ and the change $\\Delta$. The rates are related by $r_k' = r_k$ for $k \\neq i$ and $r_i' = r_i + \\Delta$. This relationship can be expressed for any index $k$ using the Kronecker delta $\\delta_{ik}$ as $r_k' = r_k + \\Delta \\cdot \\delta_{ik}$. Substituting this into the expression for $t_j^{\\text{new}}$:\n$$t_j^{\\text{new}} = \\sum_{k=j - \\mathrm{lowbit}(j) + 1}^{j} (r_k + \\Delta \\cdot \\delta_{ik})$$\nDistributing the summation, we get:\n$$t_j^{\\text{new}} = \\left( \\sum_{k=j - \\mathrm{lowbit}(j) + 1}^{j} r_k \\right) + \\left( \\sum_{k=j - \\mathrm{lowbit}(j) + 1}^{j} \\Delta \\cdot \\delta_{ik} \\right)$$\nThe first term is, by definition, $t_j^{\\text{old}}$. The second term simplifies because the Kronecker delta $\\delta_{ik}$ is $1$ only when $k=i$ and $0$ otherwise. The sum therefore contributes a value of $\\Delta$ if and only if the index $i$ falls within the summation range, i.e., $j - \\mathrm{lowbit}(j) + 1 \\leq i \\leq j$.\n\nTo express this condition formally, we use the indicator function, $\\mathbb{I}(\\cdot)$, which is $1$ if its argument is true and $0$ otherwise. The update rule for $t_j$ becomes:\n$$t_j^{\\text{new}} = t_j^{\\text{old}} + \\Delta \\cdot \\mathbb{I}(j - \\mathrm{lowbit}(j) + 1 \\leq i \\leq j)$$\nThis is the mathematical rule that governs the change for every entry $t_j$ in the auxiliary array due to a change $\\Delta$ in the single rate $r_i$. While an efficient algorithm would only visit the $\\mathcal{O}(\\log M)$ entries where the indicator function is non-zero, this equation provides the fundamental rule derived directly from the definition of the Fenwick tree.\n\nSecond, we determine the speedup factor $S(M)$. The problem defines $S(M)$ as the ratio of the asymptotic work for event selection using a naive linear scan versus a tree-based method.\nLet $W_{\\text{naive}}(M)$ be the work required for the naive linear scan and $W_{\\text{tree}}(M)$ be the work for the Fenwick tree-based selection.\n\nAccording to the problem statement, the naive linear scan performs selection in $\\mathcal{O}(M)$ time. Therefore, the asymptotic work is proportional to $M$:\n$$W_{\\text{naive}}(M) \\propto M$$\nThe Fenwick tree supports event selection in $\\mathcal{O}(\\log M)$ time. This is achieved by a search on the tree structure that takes a number of steps proportional to the number of bits in the index $M$. The asymptotic work is therefore proportional to $\\log M$:\n$$W_{\\text{tree}}(M) \\propto \\log M$$\nThe speedup factor $S(M)$ is the ratio of these asymptotic work functions:\n$$S(M) = \\frac{W_{\\text{naive}}(M)}{W_{\\text{tree}}(M)} = \\frac{c_1 M}{c_2 \\log M}$$\nwhere $c_1$ and $c_2$ are constants of proportionality. The problem asks for a closed-form function for $S(M)$, which, in the context of comparing asymptotic complexities, typically means examining the ratio of the leading terms and treating the implementation-dependent constants as a single factor, often taken as unity for a high-level comparison. The phrase \"implemented optimally within their respective paradigms\" under the \"unit cost\" assumption directs us to focus on the dominant functional dependence on $M$.\n\nThe base of the logarithm for the Fenwick tree complexity is $2$, since the data structure and its associated algorithms (including the $\\mathcal{O}(\\log M)$ search) are fundamentally based on the binary representation of indices. Thus, we use $\\log_2 M$.\n\nBy taking the ratio of the complexity functions themselves, we define the speedup factor as:\n$$S(M) = \\frac{M}{\\log_2 M}$$\nThis function captures the asymptotic performance gain of using the tree-based data structure over a simple linear scan for the event selection step in the KMC simulation.",
            "answer": "$$\\boxed{\\frac{M}{\\log_{2}(M)}}$$"
        }
    ]
}