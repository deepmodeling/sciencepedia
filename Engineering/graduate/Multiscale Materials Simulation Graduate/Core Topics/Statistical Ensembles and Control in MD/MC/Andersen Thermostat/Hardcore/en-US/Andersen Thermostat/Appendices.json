{
    "hands_on_practices": [
        {
            "introduction": "The Andersen thermostat's ability to maintain a constant temperature relies on its underlying stochastic collision mechanism. This first practice invites you to explore the statistical heart of the thermostat by deriving the probability distribution for the total number of collisions in a system. By modeling the process from first principles, you will gain a foundational understanding of the Poisson statistics that govern the thermostat's interaction with the simulated particles .",
            "id": "4110000",
            "problem": "Consider a Molecular Dynamics (MD) simulation of a complex fluid in a single cubic cell containing $N$ indistinguishable particles coupled to a heat bath via the Andersen thermostat. In this thermostat, each particle undergoes stochastic velocity reassignment events (“collisions”), modeled as occurring at random times such that the waiting-time distribution for each particle is memoryless and stationary with a constant rate parameter $\\nu$ (events per unit time). Assume collisions for different particles and across time are independent.\n\nTo connect the continuous-time stochastic description to a standard discrete-time MD integrator, suppose the dynamics are advanced in fixed steps of duration $\\Delta t$, and in each step each particle either collides once or not at all, with a per-step collision probability $p(\\Delta t)$ chosen to be consistent with the stated memoryless property. Under these assumptions, and in the limit $\\Delta t \\to 0$ holding the observation time fixed, derive from first principles the probability mass function of the total number of collision events $K$ observed in the entire cell over a unit time interval of length $1$, expressed in terms of the integer count $k$, the rate parameter $\\nu$, and the particle count $N$.\n\nYour final answer must be a single closed-form analytic expression for the probability $\\mathbb{P}(K = k)$ as a function of $k$, $\\nu$, and $N$. No numerical rounding is required. Do not include any units in your final expression.",
            "solution": "The problem requires deriving the probability mass function (PMF) for the total number of collisions, $K$, in a system of $N$ particles over a time interval of length $1$. The derivation starts from a discrete-time model and takes the limit as the time step $\\Delta t \\to 0$.\n\nFirst, we formalize the stochastic process for a single particle. A waiting-time distribution that is memoryless and stationary with a constant rate parameter $\\nu$ defines a Poisson process. The probability of observing zero events in a time interval of duration $\\tau$ is $\\exp(-\\nu \\tau)$.\n\nIn the discrete-time model with time steps of duration $\\Delta t$, a particle collides with probability $p(\\Delta t)$. For this discrete model to be consistent with the continuous-time Poisson process, the probability of a particle *not* colliding in a time step $\\Delta t$ must equal the probability of zero events in that duration for the continuous process.\n$$1 - p(\\Delta t) = \\exp(-\\nu \\Delta t)$$\nThus, the per-step, per-particle collision probability is:\n$$p(\\Delta t) = 1 - \\exp(-\\nu \\Delta t)$$\n\nNext, we consider the entire system over a unit time interval $[0, 1]$. We divide this interval into $M$ small time steps, each of duration $\\Delta t = 1/M$. As we take the limit $\\Delta t \\to 0$, we have $M \\to \\infty$.\n\nIn each time step, there are $N$ particles, and each has an independent chance to collide. The total number of collisions $K$ can be modeled as the sum of successes in a series of independent Bernoulli trials. The total number of trials is the number of particles times the number of time steps:\n$$n_{\\text{trials}} = N \\times M$$\nThe probability of success for any single trial (one particle colliding in one time step) is:\n$$p_{\\text{success}} = p(\\Delta t) = p\\left(\\frac{1}{M}\\right) = 1 - \\exp\\left(-\\frac{\\nu}{M}\\right)$$\nThe total number of collisions $K$ follows a Binomial distribution, $K \\sim \\text{Binomial}(n_{\\text{trials}}, p_{\\text{success}})$. The PMF is:\n$$\\mathbb{P}(K = k) = \\binom{NM}{k} \\left(1 - \\exp\\left(-\\frac{\\nu}{M}\\right)\\right)^k \\left(\\exp\\left(-\\frac{\\nu}{M}\\right)\\right)^{NM - k}$$\n\nWe now take the limit as $\\Delta t \\to 0$, which corresponds to $M \\to \\infty$. We use the Poisson limit theorem, which states that a Binomial distribution $\\text{Binomial}(n, p)$ converges to a Poisson distribution with parameter $\\Lambda = np$ if $n \\to \\infty$, $p \\to 0$, and the product $np$ converges to a finite value $\\Lambda$.\n\n1.  $n_{\\text{trials}} = NM \\to \\infty$ as $M \\to \\infty$.\n2.  $p_{\\text{success}} = 1 - \\exp(-\\nu/M) \\to 0$ as $M \\to \\infty$.\n3.  The product limit is:\n$$\\Lambda = \\lim_{M \\to \\infty} n_{\\text{trials}} \\cdot p_{\\text{success}} = \\lim_{M \\to \\infty} (NM) \\left(1 - \\exp\\left(-\\frac{\\nu}{M}\\right)\\right)$$\nUsing the Taylor expansion $\\exp(-x) \\approx 1-x$ for small $x$, we let $x = \\nu/M$:\n$$\\Lambda = \\lim_{M \\to \\infty} (NM) \\left(1 - \\left(1 - \\frac{\\nu}{M} + O\\left(\\frac{1}{M^2}\\right)\\right)\\right) = \\lim_{M \\to \\infty} (NM) \\left(\\frac{\\nu}{M}\\right) = N\\nu$$\nThe mean number of collisions is $\\Lambda = N\\nu$.\n\nSince all conditions for the Poisson limit are met, the distribution of $K$ converges to a Poisson distribution with mean $\\Lambda = N\\nu$. The PMF for a Poisson distribution is:\n$$\\mathbb{P}(K = k) = \\frac{\\Lambda^k \\exp(-\\Lambda)}{k!}$$\nSubstituting $\\Lambda = N\\nu$, we obtain the final expression:\n$$\\mathbb{P}(K = k) = \\frac{(N\\nu)^k \\exp(-N\\nu)}{k!}$$\nThis result is physically intuitive: the total collision process for the entire cell is a superposition of $N$ independent Poisson processes, each with rate $\\nu$. The superposition of independent Poisson processes is itself a Poisson process whose rate is the sum of the individual rates, which is $N\\nu$.",
            "answer": "$$\n\\boxed{\\frac{(N\\nu)^k \\exp(-N\\nu)}{k!}}\n$$"
        },
        {
            "introduction": "After establishing the statistical nature of the collision process, a crucial next step is to verify that it achieves its intended physical goal: sampling the canonical ensemble. This exercise applies the Andersen thermostat to a simple, analytically tractable model system—a one-dimensional harmonic chain. By calculating the stationary velocity covariance matrix, you will directly confirm that the thermostat correctly enforces the equipartition theorem, a cornerstone of statistical mechanics .",
            "id": "4109976",
            "problem": "Consider a one-dimensional periodic harmonic chain of $N \\ge 3$ identical particles of mass $m$ with positions $x_i(t)$ and velocities $v_i(t)$, where $i \\in \\{1,\\dots,N\\}$ and indices are taken modulo $N$. The interaction potential is $U(x) = \\frac{\\kappa}{2} \\sum_{i=1}^{N} \\left(x_{i+1} - x_i\\right)^{2}$ with spring constant $\\kappa  0$. Between stochastic collision events, the dynamics follow Newton’s second law. Each particle is coupled to an Andersen thermostat: independently for each $i$, collision events occur according to a homogeneous Poisson counting process of rate $\\nu  0$, and at each collision time the velocity $v_i$ is instantaneously replaced by an independent draw from the one-dimensional Maxwell–Boltzmann distribution at absolute temperature $T$, that is a Gaussian law with mean $0$ and variance $k_B T/m$, where $k_B$ is the Boltzmann constant.\n\n1. Starting from Newton’s second law and the definition of the Andersen thermostat, write the piecewise-deterministic linear stochastic differential equations for $(x_i(t), v_i(t))_{i=1}^{N}$ using a representation with independent Poisson counting processes. Your equations must clearly separate the deterministic harmonic forces and the stochastic jump terms that model velocity resampling. You may use marked Poisson counting processes to represent the random post-collision velocities.\n\n2. Assume the process is ergodic and reaches a stationary state. Using first principles of equilibrium statistical mechanics and the defining properties of the Andersen thermostat, determine the stationary velocity covariance matrix $C_v \\in \\mathbb{R}^{N \\times N}$ with entries $[C_v]_{ij} = \\lim_{t \\to \\infty} \\mathbb{E}[v_i(t) v_j(t)]$. Provide a closed-form analytic expression.\n\nAs your final answer, report the scalar $s$ such that $C_v = s I_N$, expressed symbolically in terms of $k_B$, $T$, and $m$. Do not include units in your final boxed answer. The final answer must be a single expression.",
            "solution": "### Part 1: Stochastic Differential Equations of Motion\n\nThe dynamics of the system are piecewise-deterministic. Between collision events, the particles evolve according to the deterministic laws of motion. At discrete random times, stochastic collision events instantaneously change the velocity of a particle.\n\nFirst, we derive the deterministic part of the motion. The force on particle $i$ is given by $F_i = -\\frac{\\partial U}{\\partial x_i}$. The potential energy is $U(x) = \\frac{\\kappa}{2} \\sum_{j=1}^{N} (x_{j+1} - x_j)^2$. The terms in the sum that depend on $x_i$ are for $j=i$ and $j=i-1$:\n$$\nU(x_i) = \\frac{\\kappa}{2} (x_{i+1} - x_i)^2 + \\frac{\\kappa}{2} (x_i - x_{i-1})^2 + \\text{terms independent of } x_i\n$$\nThe force is then:\n$$\nF_i = -\\frac{\\partial}{\\partial x_i} \\left[ \\frac{\\kappa}{2} (x_{i+1} - x_i)^2 + \\frac{\\kappa}{2} (x_i - x_{i-1})^2 \\right] = -\\left[ \\kappa(x_{i+1}-x_i)(-1) + \\kappa(x_i-x_{i-1})(1) \\right]\n$$\n$$\nF_i = \\kappa(x_{i+1} - x_i) - \\kappa(x_i - x_{i-1}) = \\kappa(x_{i+1} - 2x_i + x_{i-1})\n$$\nBy Newton's second law, $m\\dot{v}_i = F_i$. The deterministic evolution is thus described by:\n$$\n\\dot{x}_i(t) = v_i(t)\n$$\n$$\n\\dot{v}_i(t) = \\frac{\\kappa}{m} (x_{i+1}(t) - 2x_i(t) + x_{i-1}(t))\n$$\n\nNext, we incorporate the stochastic part due to the Andersen thermostat. For each particle $i$, we introduce an independent homogeneous Poisson counting process $N_i(t)$ with rate $\\nu$. An increment $dN_i(t)$ is $1$ if a collision event for particle $i$ occurs in the time interval $[t, t+dt)$, and $0$ otherwise. At such an event, the velocity $v_i(t)$ is replaced by a new velocity $U_i$ drawn from the Maxwell-Boltzmann distribution, i.e., a Gaussian distribution $\\mathcal{N}(0, k_B T/m)$. The change in velocity $v_i$ at the time of the event is $\\Delta v_i = U_i - v_i(t^-)$, where $v_i(t^-)$ is the velocity just before the jump.\n\nCombining the deterministic and stochastic parts, the SDEs for the state vector $(x_i(t), v_i(t))_{i=1}^N$ are:\n$$\ndx_i(t) = v_i(t) dt\n$$\n$$\ndv_i(t) = \\frac{\\kappa}{m} (x_{i+1}(t) - 2x_i(t) + x_{i-1}(t)) dt + (U_i - v_i(t^-)) dN_i(t)\n$$\nThese equations hold for each $i \\in \\{1, \\dots, N\\}$. The $N$ Poisson processes $\\{N_i(t)\\}_{i=1}^N$ are mutually independent, and the random variables $\\{U_i\\}$ are drawn independently for each event from the specified Gaussian distribution.\n\n### Part 2: Stationary Velocity Covariance Matrix\n\nThe problem posits that the system is ergodic and reaches a stationary state. The Andersen thermostat is designed to make the system sample phase space according to the canonical (NVT) ensemble. Therefore, the stationary probability distribution of the system's microstates $(x, v)$ is the Boltzmann distribution:\n$$\n\\rho(x, v) = \\frac{1}{Z} \\exp\\left(-\\frac{H(x, v)}{k_B T}\\right)\n$$\nwhere $Z$ is the partition function and $H(x, v)$ is the system's Hamiltonian:\n$$\nH(x, v) = K(v) + U(x) = \\sum_{i=1}^N \\frac{1}{2} m v_i^2 + \\frac{\\kappa}{2} \\sum_{i=1}^N (x_{i+1} - x_i)^2\n$$\nThe Hamiltonian is separable into a sum of a kinetic energy term $K(v)$ and a potential energy term $U(x)$. Consequently, the stationary probability distribution factors into a product of a velocity-dependent distribution and a position-dependent distribution:\n$$\n\\rho(x, v) = \\rho_v(v) \\rho_x(x)\n$$\nwhere\n$$\n\\rho_v(v) \\propto \\exp\\left(-\\frac{K(v)}{k_B T}\\right) = \\exp\\left(-\\frac{1}{k_B T} \\sum_{i=1}^N \\frac{1}{2} m v_i^2\\right) = \\prod_{i=1}^N \\exp\\left(-\\frac{m v_i^2}{2 k_B T}\\right)\n$$\nThe factorization of $\\rho_v(v)$ shows that in the stationary state, the velocities of the particles are statistically independent.\n\nThe distribution for a single velocity component $v_i$ is a Gaussian distribution:\n$$\n\\rho(v_i) \\propto \\exp\\left(-\\frac{v_i^2}{2 (k_B T/m)}\\right)\n$$\nThis is a normal distribution with mean $\\mathbb{E}[v_i] = 0$ and variance $\\text{Var}(v_i) = \\mathbb{E}[v_i^2] - (\\mathbb{E}[v_i])^2 = k_B T/m$.\n\nWe now compute the entries of the stationary velocity covariance matrix $C_v$, where $[C_v]_{ij} = \\mathbb{E}[v_i v_j]$.\n\nFor the diagonal entries ($i=j$):\n$$\n[C_v]_{ii} = \\mathbb{E}[v_i^2] = \\text{Var}(v_i) + (\\mathbb{E}[v_i])^2 = \\frac{k_B T}{m} + 0^2 = \\frac{k_B T}{m}\n$$\nFor the off-diagonal entries ($i \\neq j$):\nSince the velocities $v_i$ and $v_j$ are statistically independent in the stationary state, the expectation of their product is the product of their expectations:\n$$\n[C_v]_{ij} = \\mathbb{E}[v_i v_j] = \\mathbb{E}[v_i] \\mathbb{E}[v_j] = 0 \\times 0 = 0\n$$\nCombining these results, the velocity covariance matrix is diagonal:\n$$\nC_v = \\begin{pmatrix} \\frac{k_B T}{m}  0  \\cdots  0 \\\\ 0  \\frac{k_B T}{m}  \\cdots  0 \\\\ \\vdots  \\vdots  \\ddots  \\vdots \\\\ 0  0  \\cdots  \\frac{k_B T}{m} \\end{pmatrix} = \\frac{k_B T}{m} I_N\n$$\nwhere $I_N$ is the $N \\times N$ identity matrix. This matches the form $C_v = s I_N$, so the scalar $s$ is:\n$$\ns = \\frac{k_B T}{m}\n$$\nThis result is independent of the spring constant $\\kappa$ and the collision rate $\\nu$, which affect the position correlations and the rate of relaxation to equilibrium, respectively, but not the equilibrium kinetic energy distribution itself. This is a direct consequence of the equipartition theorem.",
            "answer": "$$\n\\boxed{\\frac{k_B T}{m}}\n$$"
        },
        {
            "introduction": "Moving from theory to practical application, this final hands-on exercise addresses a crucial question for any simulation practitioner: how to choose the collision frequency $\\nu$. This parameter controls the strength of the coupling to the heat bath, creating a trade-off between rapid thermal equilibration and preserving the system's natural dynamics. By developing and optimizing a quantitative error metric, you will derive an optimal working range for $\\nu$, transforming an intuitive balancing act into a rigorous calculation .",
            "id": "4110018",
            "problem": "Consider a simple fluid whose true equilibrium velocity autocorrelation function (VACF) is well-approximated at intermediate times by a single exponential decay. Let the true VACF be modeled as $C_{\\mathrm{true}}(t) = \\frac{k_{\\mathrm{B}} T}{m} \\exp(-\\gamma t)$, where $k_{\\mathrm{B}}$ is Boltzmann's constant, $T$ is the temperature, $m$ is the particle mass, and $\\gamma$ (in $\\mathrm{s}^{-1}$) is an effective decay rate associated with intermolecular interactions. Under the Andersen thermostat, each particle's velocity is instantaneously reassigned according to the Maxwell-Boltzmann distribution at random times governed by a Poisson process with rate $\\nu$ (in $\\mathrm{s}^{-1}$). Between collisions, the dynamics are otherwise governed by Newton's laws under the same interactions that produce the true VACF. In this setting, the Andersen thermostat modifies the persistence of velocity memory by superimposing an independent memoryless collision process. The combined effect yields an Andersen-thermostatted VACF of the form $C_{\\mathrm{A}}(t;\\nu) = \\frac{k_{\\mathrm{B}} T}{m} \\exp\\big(-( \\gamma + \\nu ) t\\big)$.\n\nA practitioner aims to choose the collision frequency $\\nu$ to balance two competing aims:\n\n- Rapid decorrelation: Shorter correlation time improves sampling efficiency in canonical ensemble simulations of complex fluids.\n- Dynamical fidelity: The Andersen thermostat should not excessively distort the true VACF.\n\nTo formalize this balance, define the normalized integrated squared error between the thermostatted and true VACFs as\n$$E_{\\mathrm{norm}}(\\nu) = \\gamma \\int_{0}^{\\infty} \\left( \\frac{C_{\\mathrm{A}}(t;\\nu)}{C_{\\mathrm{true}}(0)} - \\frac{C_{\\mathrm{true}}(t)}{C_{\\mathrm{true}}(0)} \\right)^2 \\, dt,$$\nwhich is dimensionless. Also define the thermostatted correlation time as\n$$\\tau_{\\mathrm{A}}(\\nu) = \\int_{0}^{\\infty} \\frac{C_{\\mathrm{A}}(t;\\nu)}{C_{\\mathrm{A}}(0)} \\, dt = \\frac{1}{\\gamma + \\nu},$$\nin $\\mathrm{s}$, and the true correlation time $\\tau_{\\mathrm{true}} = 1/\\gamma$.\n\nGiven the tolerances $\\tau_{\\max}$ (in $\\mathrm{s}$) for decorrelation time and $\\varepsilon$ (dimensionless) for error (with the requirement $0 \\le \\varepsilon \\le \\frac{1}{2}$ to ensure a meaningful bound), define the feasible range of $\\nu$ as those values satisfying both constraints:\n- $\\tau_{\\mathrm{A}}(\\nu) \\le \\tau_{\\max}$,\n- $E_{\\mathrm{norm}}(\\nu) \\le \\varepsilon$.\n\nStarting only from Newton's laws, the exponential VACF form for the true fluid, and the Poisson collision model of the Andersen thermostat, derive the closed-form expression of $E_{\\mathrm{norm}}(\\nu)$ and show how to solve the inequality $E_{\\mathrm{norm}}(\\nu) \\le \\varepsilon$ for $\\nu$. Then, implement an algorithm that:\n\n- Computes the lower bound $\\nu_{\\min}$ imposed by decorrelation, and the upper bound $\\nu_{\\max}$ imposed by error tolerance, using analytic expressions. If $\\varepsilon \\ge \\frac{1}{2}$, the error constraint imposes no finite upper bound, which should be treated as an unbounded upper limit.\n- Returns the intersection range $[\\nu_{\\min}, \\nu_{\\max}]$ if feasible, otherwise returns the empty list if no $\\nu$ satisfies both constraints.\n\nYour program should evaluate the following test suite. All rates $\\gamma$ and $\\nu$ are in $\\mathrm{s}^{-1}$, all times $\\tau_{\\max}$ are in $\\mathrm{s}$, and all tolerances $\\varepsilon$ are dimensionless:\n\n- Test case $1$: $\\gamma = 1$, $\\tau_{\\max} = 1$, $\\varepsilon = 0.1$.\n- Test case $2$: $\\gamma = 1$, $\\tau_{\\max} = 0.2$, $\\varepsilon = 0.1$.\n- Test case $3$: $\\gamma = 0.01$, $\\tau_{\\max} = 5$, $\\varepsilon = 0.49$.\n- Test case $4$: $\\gamma = 2$, $\\tau_{\\max} = 0.05$, $\\varepsilon = 0.6$.\n- Test case $5$: $\\gamma = 5$, $\\tau_{\\max} = 0.1$, $\\varepsilon = 0.3$.\n\nYour program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each entry either a two-element list $[\\nu_{\\min}, \\nu_{\\max}]$ in $\\mathrm{s}^{-1}$ rounded to six decimal places, or an empty list if infeasible. If the upper bound is unbounded, represent it using the special floating-point value $+\\infty$ and print it as $\\mathrm{inf}$. For example, the output should look like $[[\\nu_{\\min,1},\\nu_{\\max,1}],[\\nu_{\\min,2},\\nu_{\\max,2}],\\ldots]$, with each numeric bound in $\\mathrm{s}^{-1}$ rounded to six decimal places.",
            "solution": "The objective is to find the feasible range for the collision frequency $\\nu$ by satisfying two constraints. First, we must derive a closed-form expression for the normalized integrated squared error, $E_{\\mathrm{norm}}(\\nu)$.\n\nThe definition is:\n$$E_{\\mathrm{norm}}(\\nu) = \\gamma \\int_{0}^{\\infty} \\left( \\frac{C_{\\mathrm{A}}(t;\\nu)}{C_{\\mathrm{true}}(0)} - \\frac{C_{\\mathrm{true}}(t)}{C_{\\mathrm{true}}(0)} \\right)^2 \\, dt$$\nThe normalized VACFs are $\\frac{C_{\\mathrm{true}}(t)}{C_{\\mathrm{true}}(0)} = \\exp(-\\gamma t)$ and $\\frac{C_{\\mathrm{A}}(t;\\nu)}{C_{\\mathrm{true}}(0)} = \\exp(-(\\gamma + \\nu) t)$.\nSubstituting these into the integral:\n$$E_{\\mathrm{norm}}(\\nu) = \\gamma \\int_{0}^{\\infty} \\left( \\exp(-(\\gamma + \\nu) t) - \\exp(-\\gamma t) \\right)^2 \\, dt$$\nExpanding the integrand gives:\n$$ \\left( \\dots \\right)^2 = \\exp(-2(\\gamma + \\nu) t) - 2\\exp(-(2\\gamma + \\nu) t) + \\exp(-2\\gamma t) $$\nIntegrating each term from $t=0$ to $t=\\infty$ using $\\int_{0}^{\\infty} e^{-at} dt = 1/a$:\n$$ \\int_0^\\infty \\dots dt = \\frac{1}{2(\\gamma + \\nu)} - \\frac{2}{2\\gamma + \\nu} + \\frac{1}{2\\gamma} $$\nMultiplying by $\\gamma$ and simplifying the expression:\n$$ E_{\\mathrm{norm}}(\\nu) = \\gamma \\left( \\frac{1}{2(\\gamma + \\nu)} - \\frac{2}{2\\gamma + \\nu} + \\frac{1}{2\\gamma} \\right) = \\frac{1}{2} \\left( \\frac{\\gamma}{\\gamma+\\nu} - \\frac{4\\gamma}{2\\gamma+\\nu} + 1 \\right) $$\nFinding a common denominator, $2(\\gamma+\\nu)(2\\gamma+\\nu)$:\n$$ E_{\\mathrm{norm}}(\\nu) = \\frac{\\gamma(2\\gamma+\\nu) - 4\\gamma(\\gamma+\\nu) + (\\gamma+\\nu)(2\\gamma+\\nu)}{2(\\gamma+\\nu)(2\\gamma+\\nu)} $$\n$$ = \\frac{(2\\gamma^2+\\gamma\\nu) - (4\\gamma^2+4\\gamma\\nu) + (2\\gamma^2+3\\gamma\\nu+\\nu^2)}{2(\\gamma+\\nu)(2\\gamma+\\nu)} = \\frac{\\nu^2}{2(\\gamma+\\nu)(2\\gamma+\\nu)} $$\nNow we solve the two inequalities for $\\nu \\ge 0$.\n\n**1. Decorrelation Time Constraint:**\nThe requirement is $\\tau_{\\mathrm{A}}(\\nu) \\le \\tau_{\\max}$.\n$$ \\frac{1}{\\gamma + \\nu} \\le \\tau_{\\max} \\implies 1 \\le \\tau_{\\max}(\\gamma + \\nu) \\implies \\nu \\ge \\frac{1}{\\tau_{\\max}} - \\gamma $$\nSince $\\nu$ must be non-negative, the lower bound is $\\nu_{\\min} = \\max\\left(0, \\frac{1}{\\tau_{\\max}} - \\gamma\\right)$.\n\n**2. Error Tolerance Constraint:**\nThe requirement is $E_{\\mathrm{norm}}(\\nu) \\le \\varepsilon$.\n$$ \\frac{\\nu^2}{2(\\gamma+\\nu)(2\\gamma+\\nu)} \\le \\varepsilon $$\nThis rearranges to the quadratic inequality:\n$$ (2\\varepsilon - 1)\\nu^2 + 6\\varepsilon\\gamma\\nu + 4\\varepsilon\\gamma^2 \\ge 0 $$\nLet $g(\\nu) = (2\\varepsilon - 1)\\nu^2 + 6\\varepsilon\\gamma\\nu + 4\\varepsilon\\gamma^2$.\n- **Case A: $\\varepsilon \\ge 1/2$**. The coefficient of $\\nu^2$ is non-negative. All coefficients of the quadratic are non-negative, so $g(\\nu) \\ge 0$ for all $\\nu \\ge 0$. The error constraint imposes no upper bound, thus $\\nu_{\\max} = \\infty$.\n- **Case B: $0 \\le \\varepsilon  1/2$**. The coefficient of $\\nu^2$ is negative, so the parabola $g(\\nu)$ opens downwards. The inequality $g(\\nu) \\ge 0$ holds for values of $\\nu$ between the two roots of $g(\\nu)=0$. The roots are given by the quadratic formula:\n$$ \\nu = \\gamma \\frac{-3\\varepsilon \\pm \\sqrt{9\\varepsilon^2 - 4(2\\varepsilon-1)\\varepsilon}}{2\\varepsilon-1} = \\gamma \\frac{-3\\varepsilon \\pm \\sqrt{\\varepsilon^2 + 4\\varepsilon}}{2\\varepsilon-1} = \\gamma \\frac{3\\varepsilon \\mp \\sqrt{\\varepsilon(\\varepsilon + 4)}}{1 - 2\\varepsilon} $$\nOne root is negative and the other is positive. Since we require $\\nu \\ge 0$, the valid range is from $0$ up to the positive root. This positive root defines the upper bound:\n$$ \\nu_{\\max} = \\gamma \\frac{3\\varepsilon + \\sqrt{\\varepsilon(\\varepsilon + 4)}}{1 - 2\\varepsilon} $$\n\n**Final Algorithm:**\nGiven $\\gamma, \\tau_{\\max}, \\varepsilon$:\n1. Compute $\\nu_{\\min} = \\max(0, 1/\\tau_{\\max} - \\gamma)$.\n2. Compute $\\nu_{\\max}$ based on $\\varepsilon$. If $\\varepsilon \\ge 0.5$, $\\nu_{\\max} = \\infty$. Otherwise, use the formula derived above.\n3. The feasible range is $[\\nu_{\\min}, \\nu_{\\max}]$. If $\\nu_{\\min} > \\nu_{\\max}$, the range is empty.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves for the feasible range of the Andersen thermostat collision frequency\n    based on decorrelation time and dynamical error constraints.\n    \"\"\"\n    # Test cases: (gamma, tau_max, epsilon)\n    # gamma in s^-1, tau_max in s, epsilon is dimensionless.\n    test_cases = [\n        (1.0, 1.0, 0.1),\n        (1.0, 0.2, 0.1),\n        (0.01, 5.0, 0.49),\n        (2.0, 0.05, 0.6),\n        (5.0, 0.1, 0.3),\n    ]\n\n    results = []\n    \n    for gamma, tau_max, epsilon in test_cases:\n        # Step 1: Compute the lower bound nu_min from the decorrelation time constraint.\n        # tau_A(nu) = 1/(gamma + nu) = tau_max  ==  nu = 1/tau_max - gamma.\n        # Since nu must be non-negative, nu_min = max(0, 1/tau_max - gamma).\n        nu_min = max(0.0, 1.0 / tau_max - gamma)\n\n        # Step 2: Compute the upper bound nu_max from the error tolerance constraint.\n        # E_norm(nu) = nu^2 / (2 * (gamma + nu) * (2*gamma + nu)) = epsilon.\n        # This leads to a quadratic inequality: (2*epsilon - 1)*nu^2 + 6*epsilon*gamma*nu + 4*epsilon*gamma^2 = 0.\n        \n        if epsilon = 0.5:\n            # For epsilon = 0.5, the error constraint is satisfied for all nu = 0.\n            # The parabola opens upwards (or is a line for epsilon=0.5) and is non-negative for nu = 0.\n            # Thus, the upper bound is effectively infinite.\n            nu_max = np.inf\n        else:\n            # For 0 = epsilon  0.5, the parabola opens downwards.\n            # The inequality holds between the roots. The operative range for nu = 0\n            # is [0, nu_upper_root].\n            # nu_max = gamma * (3*eps + sqrt(eps*(eps+4))) / (1 - 2*eps)\n            numerator = 3.0 * epsilon + np.sqrt(epsilon * (epsilon + 4.0))\n            denominator = 1.0 - 2.0 * epsilon\n            nu_max = gamma * numerator / denominator\n\n        # Step 3: Check for feasibility. The feasible range is the intersection\n        # [nu_min, Infinity) and [0, nu_max], which is [nu_min, nu_max].\n        # The range is non-empty if and only if nu_min = nu_max.\n        if nu_min  nu_max:\n            results.append(\"[]\")\n        else:\n            # Format the valid range.\n            formatted_min = f\"{nu_min:.6f}\"\n            if np.isinf(nu_max):\n                formatted_max = \"inf\"\n            else:\n                formatted_max = f\"{nu_max:.6f}\"\n            results.append(f\"[{formatted_min},{formatted_max}]\")\n            \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n\n```"
        }
    ]
}