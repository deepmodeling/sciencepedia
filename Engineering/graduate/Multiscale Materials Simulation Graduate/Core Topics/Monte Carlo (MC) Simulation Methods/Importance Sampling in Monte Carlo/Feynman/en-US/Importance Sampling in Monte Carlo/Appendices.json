{
    "hands_on_practices": [
        {
            "introduction": "To build a solid understanding of importance sampling, we begin with a foundational exercise. This practice involves applying the technique to a simple but important integral, demonstrating how a well-chosen proposal distribution can dramatically reduce the variance of a Monte Carlo estimate. By analytically deriving the optimal parameter for a proposal distribution, you will gain direct insight into the core principle of variance reduction that motivates the use of importance sampling .",
            "id": "3253345",
            "problem": "Consider the task of numerically evaluating the improper integral $I=\\int_{0}^{\\infty} \\exp(-x)\\,dx$ using Monte Carlo (MC) integration with Importance Sampling (IS). Let the proposal density be the exponential family $q(x;\\lambda)=\\lambda \\exp(-\\lambda x)$ for $x \\ge 0$ and $\\lambda > 0$. You will construct the single-sample IS estimator and analyze its variance as a function of the parameter $\\lambda$. Starting only from the definitions of MC integration and IS (namely, that IS replaces integration with expectation under a proposal density and uses the weight $w(x)=\\frac{f(x)}{q(x;\\lambda)}$), perform the following:\n\n1. Derive the single-sample IS estimator $\\widehat{I}$ for $I$ based on a draw $X\\sim q(\\cdot;\\lambda)$ and prove that it is unbiased for any $\\lambda>0$.\n2. Derive the variance $\\operatorname{Var}_{q(\\cdot;\\lambda)}[\\widehat{I}]$ as a function of $\\lambda$ and determine precisely for which $\\lambda$ this variance is finite.\n3. Find the value of $\\lambda$ that minimizes $\\operatorname{Var}_{q(\\cdot;\\lambda)}[\\widehat{I}]$ over all $\\lambda>0$.\n\nExpress your final answer as the single optimal value of $\\lambda$ that minimizes the variance. No rounding is required.",
            "solution": "The problem as stated is well-posed, self-contained, and scientifically sound. It is a standard problem in the field of numerical methods, specifically Monte Carlo integration theory. We may proceed with the solution.\n\nThe integral to be estimated is $I = \\int_{0}^{\\infty} \\exp(-x)\\,dx$. The integrand is $f(x) = \\exp(-x)$. The true value of this integral is $I = [-\\exp(-x)]_{0}^{\\infty} = 0 - (-1) = 1$.\nThe proposal probability density function is given by the exponential distribution $q(x;\\lambda) = \\lambda \\exp(-\\lambda x)$ for $x \\ge 0$ and parameter $\\lambda > 0$.\n\nAccording to the principle of Importance Sampling (IS), the integral $I$ can be rewritten as an expectation with respect to the proposal distribution $q(x;\\lambda)$:\n$$I = \\int_{0}^{\\infty} f(x) \\,dx = \\int_{0}^{\\infty} \\frac{f(x)}{q(x;\\lambda)} q(x;\\lambda) \\,dx = \\mathbb{E}_{X \\sim q(\\cdot;\\lambda)}[W(X)]$$\nwhere $W(x) = \\frac{f(x)}{q(x;\\lambda)}$ is the importance weight.\n\n1. Derivation of the single-sample IS estimator and proof of unbiasedness.\n\nThe single-sample Importance Sampling estimator, $\\widehat{I}$, is constructed from one sample $X$ drawn from the proposal distribution $q(x;\\lambda)$. The estimator is simply the importance weight evaluated at that sample:\n$$\\widehat{I} = W(X) = \\frac{f(X)}{q(X;\\lambda)}$$\nSubstituting the given functions for $f(x)$ and $q(x;\\lambda)$:\n$$W(x) = \\frac{\\exp(-x)}{\\lambda \\exp(-\\lambda x)} = \\frac{1}{\\lambda} \\exp(-x + \\lambda x) = \\frac{1}{\\lambda} \\exp(x(\\lambda-1))$$\nThus, the single-sample estimator is $\\widehat{I} = \\frac{1}{\\lambda} \\exp(X(\\lambda-1))$, where $X \\sim q(\\cdot;\\lambda)$.\n\nTo prove that this estimator is unbiased for any $\\lambda > 0$, we must show that its expected value is equal to the true value of the integral, $I$. The expectation is taken with respect to the proposal distribution $q(x;\\lambda)$.\n$$\\mathbb{E}_{q}[\\widehat{I}] = \\int_{0}^{\\infty} \\widehat{I}(x) q(x;\\lambda) \\,dx$$\n$$\\mathbb{E}_{q}[\\widehat{I}] = \\int_{0}^{\\infty} \\left( \\frac{1}{\\lambda} \\exp(x(\\lambda-1)) \\right) (\\lambda \\exp(-\\lambda x)) \\,dx$$\n$$\\mathbb{E}_{q}[\\widehat{I}] = \\int_{0}^{\\infty} \\exp(x(\\lambda-1) - \\lambda x) \\,dx$$\n$$\\mathbb{E}_{q}[\\widehat{I}] = \\int_{0}^{\\infty} \\exp(\\lambda x - x - \\lambda x) \\,dx$$\n$$\\mathbb{E}_{q}[\\widehat{I}] = \\int_{0}^{\\infty} \\exp(-x) \\,dx = I$$\nSince we have already calculated $I=1$, we have $\\mathbb{E}_{q}[\\widehat{I}] = 1$. This holds for any value of $\\lambda > 0$ for which the derivation is valid (which is all $\\lambda > 0$). Thus, the estimator $\\widehat{I}$ is unbiased.\n\n2. Derivation of the variance and its domain of finiteness.\n\nThe variance of the estimator $\\widehat{I}$ is given by $\\operatorname{Var}_{q}[\\widehat{I}] = \\mathbb{E}_{q}[\\widehat{I}^2] - (\\mathbb{E}_{q}[\\widehat{I}])^2$.\nSince the estimator is unbiased with $\\mathbb{E}_{q}[\\widehat{I}]=1$, the variance simplifies to:\n$$\\operatorname{Var}_{q}[\\widehat{I}] = \\mathbb{E}_{q}[\\widehat{I}^2] - 1^2 = \\mathbb{E}_{q}[\\widehat{I}^2] - 1$$\nWe first compute the second moment, $\\mathbb{E}_{q}[\\widehat{I}^2]$:\n$$\\mathbb{E}_{q}[\\widehat{I}^2] = \\int_{0}^{\\infty} \\widehat{I}(x)^2 q(x;\\lambda) \\,dx = \\int_{0}^{\\infty} \\left(\\frac{f(x)}{q(x;\\lambda)}\\right)^2 q(x;\\lambda) \\,dx = \\int_{0}^{\\infty} \\frac{f(x)^2}{q(x;\\lambda)} \\,dx$$\nSubstituting the expressions for $f(x)$ and $q(x;\\lambda)$:\n$$f(x)^2 = (\\exp(-x))^2 = \\exp(-2x)$$\n$$\\mathbb{E}_{q}[\\widehat{I}^2] = \\int_{0}^{\\infty} \\frac{\\exp(-2x)}{\\lambda \\exp(-\\lambda x)} \\,dx = \\frac{1}{\\lambda} \\int_{0}^{\\infty} \\exp(-2x+\\lambda x) \\,dx$$\n$$\\mathbb{E}_{q}[\\widehat{I}^2] = \\frac{1}{\\lambda} \\int_{0}^{\\infty} \\exp(-x(2-\\lambda)) \\,dx$$\nThis integral is of the form $\\int_0^\\infty \\exp(-ax) \\,dx$, which converges if and only if the coefficient $a$ is strictly positive. In our case, $a = 2-\\lambda$. Therefore, for the integral to converge and the variance to be finite, we must have $2-\\lambda > 0$, which implies $\\lambda  2$.\nGiven the problem's constraint that $\\lambda > 0$, the variance is finite precisely for $\\lambda \\in (0, 2)$.\n\nFor $\\lambda \\in (0, 2)$, we can evaluate the integral:\n$$\\int_{0}^{\\infty} \\exp(-x(2-\\lambda)) \\,dx = \\left[ \\frac{\\exp(-x(2-\\lambda))}{-(2-\\lambda)} \\right]_0^\\infty = 0 - \\frac{1}{-(2-\\lambda)} = \\frac{1}{2-\\lambda}$$\nThus, the second moment is:\n$$\\mathbb{E}_{q}[\\widehat{I}^2] = \\frac{1}{\\lambda} \\cdot \\frac{1}{2-\\lambda} = \\frac{1}{\\lambda(2-\\lambda)}$$\nThe variance as a function of $\\lambda$ is:\n$$\\operatorname{Var}_{q(\\cdot;\\lambda)}[\\widehat{I}] = V(\\lambda) = \\frac{1}{\\lambda(2-\\lambda)} - 1 \\quad \\text{for } \\lambda \\in (0, 2)$$\n\n3. Finding the optimal value of $\\lambda$.\n\nWe seek to find the value of $\\lambda \\in (0, 2)$ that minimizes the variance $V(\\lambda)$.\n$$V(\\lambda) = \\frac{1}{2\\lambda - \\lambda^2} - 1$$\nMinimizing $V(\\lambda)$ is equivalent to minimizing the term $\\frac{1}{2\\lambda - \\lambda^2}$, since the $-1$ is a constant offset. Minimizing this fraction is, in turn, equivalent to maximizing its denominator, $g(\\lambda) = 2\\lambda - \\lambda^2$, over the interval $\\lambda \\in (0, 2)$.\n\nTo find the maximum of $g(\\lambda)$, we calculate its derivative with respect to $\\lambda$ and set it to zero:\n$$\\frac{dg}{d\\lambda} = \\frac{d}{d\\lambda}(2\\lambda - \\lambda^2) = 2 - 2\\lambda$$\nSetting the derivative to zero:\n$$2 - 2\\lambda = 0 \\implies 2\\lambda = 2 \\implies \\lambda = 1$$\nThis critical point $\\lambda=1$ lies within the domain $(0, 2)$. To confirm it is a maximum, we examine the second derivative:\n$$\\frac{d^2g}{d\\lambda^2} = -2$$\nSince the second derivative is negative, the function $g(\\lambda)$ has a local maximum at $\\lambda=1$. As $g(\\lambda)$ is a downward-opening parabola, this is its global maximum.\nTherefore, the variance $V(\\lambda)$ is minimized at $\\lambda=1$.\n\nThis result is conceptually sound. The ideal proposal distribution for IS is one that is proportional to the integrand, $f(x)$. The normalized version of our integrand is $p(x) = \\frac{f(x)}{I} = \\frac{\\exp(-x)}{1} = \\exp(-x)$, which is an exponential distribution with parameter $1$. By choosing $\\lambda=1$, our proposal distribution $q(x;1) = 1 \\cdot \\exp(-1 \\cdot x)$ becomes identical to this ideal choice. In this case, the importance weight becomes $W(x) = \\frac{\\exp(-x)}{\\exp(-x)} = 1$ for all $x$, and the estimator $\\widehat{I}$ is always $1$. The variance of a constant is zero, which is the minimum possible variance.\nThe value of $\\lambda$ that minimizes the variance is $1$.",
            "answer": "$$\\boxed{1}$$"
        },
        {
            "introduction": "In many practical applications, especially in materials simulations, the proposal distribution is an approximation that can lead to a few samples having disproportionately large weights. This concentration of weights can render an estimate unreliable, even with a large number of total samples. This exercise introduces the concept of the Effective Sample Size ($N_{\\mathrm{eff}}$), a critical diagnostic tool used to quantify the quality of an importance sampling estimate and determine if corrective actions, such as resampling, are necessary to stabilize the results .",
            "id": "3816828",
            "problem": "You are estimating a fine-scale observable in a multiscale materials simulation where atomistic states $\\mathbf{x}$ are distributed according to a Boltzmann target density $p(\\mathbf{x}) \\propto \\exp\\!\\left(-U_{\\mathrm{f}}(\\mathbf{x})/k_{B}T\\right)$ associated with a high-fidelity potential $U_{\\mathrm{f}}(\\mathbf{x})$. To reduce variance, you sample from a coarse-grained proposal density $q(\\mathbf{x}) \\propto \\exp\\!\\left(-U_{\\mathrm{c}}(\\mathbf{x})/k_{B}T\\right)$, and construct an Importance Sampling (IS) estimator using weights $w(\\mathbf{x}) = p(\\mathbf{x})/q(\\mathbf{x}) = \\exp\\!\\left(-\\big(U_{\\mathrm{f}}(\\mathbf{x})-U_{\\mathrm{c}}(\\mathbf{x})\\big)/k_{B}T\\right)$. In this setting, materials configurations exhibiting localized defects or high-strain microstructures can produce a heavy-tailed distribution of $w(\\mathbf{x})$ due to large negative values of $\\big(U_{\\mathrm{f}}(\\mathbf{x})-U_{\\mathrm{c}}(\\mathbf{x})\\big)$.\n\nSuppose you collect $N$ independent samples $\\{\\mathbf{x}_{i}\\}_{i=1}^{N}$ from $q(\\mathbf{x})$ with corresponding synthetic weights\n$$\nw_{1}=0.012,\\quad\nw_{2}=0.019,\\quad\nw_{3}=0.011,\\quad\nw_{4}=0.024,\\quad\nw_{5}=0.031,\\quad\nw_{6}=0.022,\\quad\nw_{7}=0.018,\\quad\nw_{8}=0.015,\\quad\nw_{9}=0.028,\\quad\nw_{10}=2.5,\\quad\nw_{11}=6.0,\\quad\nw_{12}=120.0,\n$$\nintended to mimic heavy-tailed behavior from rare-but-dominant fine-scale contributions. Treat the estimator as self-normalized with normalized weights $\\tilde{w}_{i} = w_{i}/\\sum_{j=1}^{N} w_{j}$.\n\nStarting from foundational definitions of Monte Carlo (MC) and Importance Sampling (IS), and using a principled notion of weight concentration that equates the second moment of the normalized weights to that of a uniform weighting over an effective number of samples, derive from first principles a mathematically consistent expression for the Effective Sample Size (ESS), denoted $N_{\\mathrm{eff}}$, in terms of $\\{w_{i}\\}_{i=1}^{N}$. Then, compute $N_{\\mathrm{eff}}$ for the given weights and, using the Sequential Monte Carlo (SMC) resampling criterion “resample when $N_{\\mathrm{eff}}  \\tau$ with $\\tau = 0.5N$,” diagnose whether resampling is needed to stabilize the IS estimates.\n\nRound your $N_{\\mathrm{eff}}$ to four significant figures. Provide your final numerical answer for $N_{\\mathrm{eff}}$ only. No physical units are required.",
            "solution": "The problem requires the derivation of an expression for the Effective Sample Size ($N_{\\mathrm{eff}}$) in the context of self-normalized Importance Sampling (IS), followed by its calculation for a given set of synthetic weights, and a diagnosis of the need for resampling.\n\nFirst, we establish the foundational principles of Importance Sampling. The goal is to estimate the expectation of an observable $A(\\mathbf{x})$ with respect to a target probability density $p(\\mathbf{x})$, defined as $\\langle A \\rangle_{p} = \\int A(\\mathbf{x}) p(\\mathbf{x}) d\\mathbf{x}$. Direct sampling from $p(\\mathbf{x})$ may be difficult. Instead, we draw $N$ independent samples $\\{\\mathbf{x}_i\\}_{i=1}^N$ from a proposal density $q(\\mathbf{x})$. The expectation can be rewritten as:\n$$\n\\langle A \\rangle_{p} = \\int A(\\mathbf{x}) \\frac{p(\\mathbf{x})}{q(\\mathbf{x})} q(\\mathbf{x}) d\\mathbf{x} = \\int A(\\mathbf{x}) w(\\mathbf{x}) q(\\mathbf{x}) d\\mathbf{x} = \\mathbb{E}_{q}[A(\\mathbf{x})w(\\mathbf{x})]\n$$\nwhere $w(\\mathbf{x}) = p(\\mathbf{x})/q(\\mathbf{x})$ is the importance weight. The standard Monte Carlo estimator for this expectation is $\\frac{1}{N} \\sum_{i=1}^N w(\\mathbf{x}_i) A(\\mathbf{x}_i)$.\n\nIn many practical applications, such as the one described involving Boltzmann distributions, the normalization constants of $p(\\mathbf{x})$ and $q(\\mathbf{x})$ are unknown. We only have access to unnormalized densities $p(\\mathbf{x}) \\propto \\bar{p}(\\mathbf{x})$ and $q(\\mathbf{x}) \\propto \\bar{q}(\\mathbf{x})$. The unnormalized weights are $w_i = \\bar{p}(\\mathbf{x}_i) / \\bar{q}(\\mathbf{x}_i)$. In this case, the self-normalized IS estimator is used:\n$$\n\\langle A \\rangle_{p} \\approx \\frac{\\sum_{i=1}^N w_i A(\\mathbf{x}_i)}{\\sum_{j=1}^N w_j} = \\sum_{i=1}^N \\tilde{w}_i A(\\mathbf{x}_i)\n$$\nwhere $\\tilde{w}_i = w_i / \\sum_{j=1}^N w_j$ are the normalized weights, which satisfy $\\sum_{i=1}^N \\tilde{w}_i = 1$.\n\nThe variance of this estimator is highly dependent on the variance of the weights. If a few weights are much larger than the others, the sum is dominated by a few terms, effectively reducing the number of samples contributing to the estimate. The Effective Sample Size ($N_{\\mathrm{eff}}$) is a heuristic measure to quantify this degradation.\n\nWe are tasked to derive an expression for $N_{\\mathrm{eff}}$ based on the principle of equating the second moment of the normalized weights to that of a uniform weighting over an effective number of samples.\n\nLet the set of actual normalized weights be $\\{\\tilde{w}_i\\}_{i=1}^N$. The second moment of this distribution of weights is given by:\n$$\nM_2 = \\sum_{i=1}^N \\tilde{w}_i^2\n$$\nNow, consider an ideal sample of size $N_{\\mathrm{eff}}$ where all weights are uniform. To satisfy the normalization condition, each of these ideal weights must be $\\tilde{w}_{\\mathrm{ideal}} = 1/N_{\\mathrm{eff}}$. The second moment of this ideal, uniform weight distribution is:\n$$\nM_{2, \\mathrm{ideal}} = \\sum_{j=1}^{N_{\\mathrm{eff}}} \\left(\\frac{1}{N_{\\mathrm{eff}}}\\right)^2 = N_{\\mathrm{eff}} \\cdot \\frac{1}{N_{\\mathrm{eff}}^2} = \\frac{1}{N_{\\mathrm{eff}}}\n$$\nEquating the two second moments as per the problem's instruction, $M_2 = M_{2, \\mathrm{ideal}}$, we get:\n$$\n\\sum_{i=1}^N \\tilde{w}_i^2 = \\frac{1}{N_{\\mathrm{eff}}}\n$$\nSolving for $N_{\\mathrm{eff}}$ yields the desired expression from first principles:\n$$\nN_{\\mathrm{eff}} = \\frac{1}{\\sum_{i=1}^N \\tilde{w}_i^2}\n$$\nThis expression can be rewritten in terms of the unnormalized weights $\\{w_i\\}_{i=1}^N$, which is often more convenient for computation. Substituting $\\tilde{w}_i = w_i / \\sum_{j=1}^N w_j$:\n$$\nN_{\\mathrm{eff}} = \\frac{1}{\\sum_{i=1}^N \\left(\\frac{w_i}{\\sum_{j=1}^N w_j}\\right)^2} = \\frac{1}{\\frac{\\sum_{i=1}^N w_i^2}{\\left(\\sum_{j=1}^N w_j\\right)^2}} = \\frac{\\left(\\sum_{i=1}^N w_i\\right)^2}{\\sum_{i=1}^N w_i^2}\n$$\nThis is the mathematically consistent expression for the Effective Sample Size derived from the specified principle.\n\nNext, we compute $N_{\\mathrm{eff}}$ for the $N=12$ given synthetic weights:\n$w_1=0.012$, $w_2=0.019$, $w_3=0.011$, $w_4=0.024$, $w_5=0.031$, $w_6=0.022$, $w_7=0.018$, $w_8=0.015$, $w_9=0.028$, $w_{10}=2.5$, $w_{11}=6.0$, $w_{12}=120.0$.\n\nWe require two quantities: the sum of the weights, $S_1 = \\sum_{i=1}^{12} w_i$, and the sum of the squares of the weights, $S_2 = \\sum_{i=1}^{12} w_i^2$.\n\nCalculation of $S_1$:\n$$\nS_1 = 0.012+0.019+0.011+0.024+0.031+0.022+0.018+0.015+0.028+2.5+6.0+120.0 = 128.68\n$$\nCalculation of $S_2$:\n\\begin{align*}\nS_2 = (0.012)^2 + (0.019)^2 + (0.011)^2 + (0.024)^2 + (0.031)^2 + (0.022)^2 + (0.018)^2 + (0.015)^2 + (0.028)^2 \\\\\n   \\quad + (2.5)^2 + (6.0)^2 + (120.0)^2 \\\\\n   = 0.000144 + 0.000361 + 0.000121 + 0.000576 + 0.000961 + 0.000484 + 0.000324 + 0.000225 + 0.000784 \\\\\n   \\quad + 6.25 + 36.0 + 14400.0 \\\\\n   = 0.00398 + 6.25 + 36.0 + 14400.0 = 14442.25398\n\\end{align*}\nNow, we use the derived formula for $N_{\\mathrm{eff}}$:\n$$\nN_{\\mathrm{eff}} = \\frac{S_1^2}{S_2} = \\frac{(128.68)^2}{14442.25398} = \\frac{16558.6224}{14442.25398} \\approx 1.1465319...\n$$\nRounding to four significant figures as requested, we obtain:\n$$\nN_{\\mathrm{eff}} \\approx 1.147\n$$\nFinally, we diagnose the need for resampling using the criterion \"resample when $N_{\\mathrm{eff}}  \\tau$ with $\\tau = 0.5N$\".\nThe total number of samples is $N = 12$. The resampling threshold is:\n$$\n\\tau = 0.5 \\times N = 0.5 \\times 12 = 6\n$$\nWe compare our calculated $N_{\\mathrm{eff}}$ to this threshold:\n$$\n1.147  6\n$$\nThe condition $N_{\\mathrm{eff}}  \\tau$ is satisfied. Therefore, a resampling step is necessary to mitigate the high variance and stabilize the importance sampling estimates. The low value of $N_{\\mathrm{eff}}$ correctly reflects the extreme concentration of weight in a single sample ($w_{12}$), which renders the remaining $11$ samples virtually irrelevant.",
            "answer": "$$\n\\boxed{1.147}\n$$"
        },
        {
            "introduction": "We conclude with an advanced application that bridges importance sampling theory with the simulation of dynamic processes, a central task in computational materials science. This practice focuses on reweighting entire reactive trajectories, which is essential for studying rare events like atomic transitions or chemical reactions. By deriving the importance weight for a discretized path generated by a guided stochastic process, you will learn how to apply importance sampling to enhance the sampling of specific, physically significant pathways in a complex system .",
            "id": "3816803",
            "problem": "Consider a one-dimensional reaction coordinate $r$ describing a collective transition pathway of a surface atom in a nanoparticle. We approximate stochastic dynamics along $r$ using overdamped Langevin dynamics (an It\\^{o} stochastic differential equation) with mobility $\\mu$ and diffusion coefficient $D$, under a double-well potential $U(r)$ that stabilizes two metastable basins. The aim is to construct an Importance Sampling (IS) scheme within Monte Carlo (MC) path sampling that proposes trajectories with an added drift toward a target reaction coordinate $r_{\\star}$ while preserving exactness via IS weights derived from first principles.\n\nFundamental base and assumptions:\n- The overdamped Langevin dynamics for $r$ is given by $dr_t = a(r_t)\\,dt + \\sqrt{2D}\\,dW_t$, where $W_t$ is a Wiener process (standard Brownian motion).\n- We discretize time with the Euler–Maruyama scheme: for time step $\\Delta t$, the dynamics induce the transition $r_{n+1} = r_n + a(r_n)\\,\\Delta t + \\sqrt{2D\\,\\Delta t}\\,\\xi_n$, where the increment $\\xi_n$ is a standard normal random variable with mean $0$ and variance $1$, independent across steps.\n- The double-well potential is $U(r) = \\alpha\\,(r^2 - 1)^2$, with $\\alpha  0$. The deterministic drift is $a(r) = \\mu\\,F(r)$, where the force $F(r)$ is the negative gradient of $U(r)$, i.e., $F(r) = -\\frac{dU}{dr}(r)$.\n- The proposal process used for path sampling adds a guiding drift toward $r_{\\star}$: $b(r) = a(r) + c\\,(r_{\\star} - r)$, with $c \\ge 0$ the guiding strength. We assume the proposal uses the same diffusion coefficient $D$ and time step $\\Delta t$ as the target dynamics.\n- The Einstein relation under non-dimensionalization sets $D = \\mu$ when the thermal energy has been scaled to unity, i.e., $k_{\\text{B}}T = 1$. We adopt this setting.\n\nTask:\n1. Starting only from the Euler–Maruyama discretization, the Gaussian nature of each step’s transition, and the definitions above, derive the explicit IS weight $w$ for a full path $r_0, r_1, \\dots, r_N$ as the product over discrete-time step likelihood ratios between the target dynamics with drift $a(\\cdot)$ and the proposal dynamics with drift $b(\\cdot)$. Express your derivation fully and clearly in terms of $r_n$, $\\Delta r_n := r_{n+1} - r_n$, $a(r_n)$, $b(r_n)$, $\\Delta t$, and $D$.\n2. Implement a program that, given the parameters and a specific path $(r_n)_{n=0}^N$, computes the IS weight $w$ for that path using your derived expression.\n\nScientific realism and parameterization:\n- Use $U(r) = \\alpha\\,(r^2 - 1)^2$ so that $\\frac{dU}{dr}(r) = 4\\,\\alpha\\,r\\,(r^2 - 1)$, $F(r) = -\\frac{dU}{dr}(r)$, and $a(r) = \\mu\\,F(r)$.\n- Use $D = \\mu$.\n- Use the same $\\Delta t$ for target and proposal.\n\nTest suite:\nCompute the IS weights for the following three test cases. All quantities are dimensionless.\n\n- Case $1$ (happy path, guided transition across the barrier):\n  - $\\mu = 0.5$, $\\alpha = 1.0$, $D = \\mu$, $\\Delta t = 0.01$, $r_{\\star} = 1.0$, $c = 1.0$,\n  - Path states $[\\, -0.9,-0.8,-0.75,-0.6,-0.4,-0.2,0.0,0.25,0.6,0.85,1.05 \\,]$.\n\n- Case $2$ (boundary condition where proposal equals target so weight should be exactly $1$):\n  - $\\mu = 0.5$, $\\alpha = 1.0$, $D = \\mu$, $\\Delta t = 0.01$, $r_{\\star} = 1.0$, $c = 0.0$,\n  - Path states $[\\, -0.95,-0.98,-1.0,-0.99,-0.98,-0.97,-0.96,-0.96,-0.95,-0.95,-0.95 \\,]$.\n\n- Case $3$ (edge case with a constant path under strong guiding drift):\n  - $\\mu = 0.5$, $\\alpha = 1.0$, $D = \\mu$, $\\Delta t = 0.01$, $r_{\\star} = 1.0$, $c = 2.0$,\n  - Path states $[\\, -0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5,-0.5 \\,]$.\n\nOutput format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[w_1,w_2,w_3]$), where $w_i$ are the computed IS weights for Case $i$, expressed as floating-point numbers.",
            "solution": "The problem asks for the derivation of the Importance Sampling (IS) weight for a discretized path evolving under overdamped Langevin dynamics and for the implementation of a program to compute this weight for given test cases.\n\n### Step 1: Validation of the Problem Statement\nFirst, the problem statement is validated against the required criteria.\n\n**Givens Extracted:**\n- **SDE:** $dr_t = a(r_t)\\,dt + \\sqrt{2D}\\,dW_t$\n- **Discretization:** $r_{n+1} = r_n + a(r_n)\\,\\Delta t + \\sqrt{2D\\,\\Delta t}\\,\\xi_n$, where $\\xi_n \\sim \\mathcal{N}(0, 1)$\n- **Potential:** $U(r) = \\alpha\\,(r^2 - 1)^2$\n- **Target Drift:** $a(r) = \\mu\\,F(r) = -\\mu\\,\\frac{dU}{dr}(r) = -4\\,\\alpha\\,\\mu\\,r\\,(r^2-1)$\n- **Proposal Drift:** $b(r) = a(r) + c\\,(r_{\\star} - r)$, with $c \\ge 0$\n- **Constraint:** $D = \\mu$\n- **Task:** Derive the IS weight $w$ for a path $(r_n)_{n=0}^N$ and implement a program to compute it for three test cases.\n\n**Validation Verdict:**\nThe problem is scientifically grounded in the fields of statistical mechanics and computational physics, specifically concerning stochastic processes and Monte Carlo methods. The use of Langevin dynamics, a double-well potential, and Importance Sampling with a guiding potential are standard and well-established techniques. The problem is well-posed, with all necessary parameters, equations, and boundary conditions clearly defined. There are no contradictions, ambiguities, or factual inaccuracies. The phrasing is objective and formal. The problem requires a non-trivial derivation followed by a direct implementation, making it a valid and substantive task.\n\n### Step 2: Derivation of the Importance Sampling Weight\n\nThe Importance Sampling (IS) weight $w$ for a path is the ratio of the probability of observing that path under the target dynamics to the probability of observing it under the proposal dynamics. Let the path be denoted by the sequence of states $R = (r_0, r_1, \\dots, r_N)$. The path probability, conditioned on the starting point $r_0$, is given by:\n$$\nP(R) = P(r_N, r_{N-1}, \\dots, r_1 | r_0)\n$$\nSince the dynamics described by the Euler-Maruyama scheme are Markovian, the probability of the entire path is the product of the single-step transition probabilities:\n$$\nP(R) = \\prod_{n=0}^{N-1} P(r_{n+1} | r_n)\n$$\nThe IS weight $w$ is therefore:\n$$\nw = \\frac{P_{\\text{target}}(R)}{P_{\\text{proposal}}(R)} = \\frac{\\prod_{n=0}^{N-1} P_{\\text{target}}(r_{n+1} | r_n)}{\\prod_{n=0}^{N-1} P_{\\text{proposal}}(r_{n+1} | r_n)} = \\prod_{n=0}^{N-1} \\frac{P_{\\text{target}}(r_{n+1} | r_n)}{P_{\\text{proposal}}(r_{n+1} | r_n)} = \\prod_{n=0}^{N-1} w_n\n$$\nwhere $w_n$ is the weight for a single time step from $n$ to $n+1$. Our task reduces to finding the expression for $w_n$.\n\nLet's first determine the single-step transition probability, $P(r_{n+1}|r_n)$. The Euler-Maruyama discretization for a generic drift $f(r)$ is:\n$$\nr_{n+1} = r_n + f(r_n)\\,\\Delta t + \\sqrt{2D\\,\\Delta t}\\,\\xi_n\n$$\nwhere $\\xi_n$ is a random variable drawn from a standard normal distribution, $\\xi_n \\sim \\mathcal{N}(0,1)$. This equation can be rearranged to show that $r_{n+1}$ is a Gaussian random variable:\n$$\nr_{n+1} \\sim \\mathcal{N}(\\mu_{r_{n+1}}, \\sigma^2_{r_{n+1}})\n$$\nThe mean is $\\mu_{r_{n+1}} = r_n + f(r_n)\\,\\Delta t$ and the variance is $\\sigma^2_{r_{n+1}} = (\\sqrt{2D\\,\\Delta t})^2 \\cdot \\text{Var}(\\xi_n) = 2D\\,\\Delta t$. The probability density for this transition is given by the Gaussian PDF:\n$$\nP(r_{n+1} | r_n) = \\frac{1}{\\sqrt{2\\pi \\cdot (2D\\,\\Delta t)}} \\exp\\left( -\\frac{(r_{n+1} - (r_n + f(r_n)\\,\\Delta t))^2}{2 \\cdot (2D\\,\\Delta t)} \\right)\n$$\nLet $\\Delta r_n = r_{n+1} - r_n$. The density can be written as:\n$$\nP(r_{n+1} | r_n) = \\frac{1}{\\sqrt{4\\pi D\\Delta t}} \\exp\\left( -\\frac{(\\Delta r_n - f(r_n)\\,\\Delta t)^2}{4D\\,\\Delta t} \\right)\n$$\nNow, we apply this to our specific target and proposal dynamics.\nFor the **target dynamics**, the drift is $f(r) = a(r)$. The probability is:\n$$\nP_{\\text{target}}(r_{n+1} | r_n) = \\frac{1}{\\sqrt{4\\pi D\\Delta t}} \\exp\\left( -\\frac{(\\Delta r_n - a(r_n)\\,\\Delta t)^2}{4D\\,\\Delta t} \\right)\n$$\nFor the **proposal dynamics**, the drift is $f(r) = b(r)$. The probability is:\n$$\nP_{\\text{proposal}}(r_{n+1} | r_n) = \\frac{1}{\\sqrt{4\\pi D\\Delta t}} \\exp\\left( -\\frac{(\\Delta r_n - b(r_n)\\,\\Delta t)^2}{4D\\,\\Delta t} \\right)\n$$\nThe single-step weight $w_n$ is the ratio of these two probabilities. The normalization constant $\\frac{1}{\\sqrt{4\\pi D\\Delta t}}$ cancels out.\n$$\nw_n = \\frac{P_{\\text{target}}(r_{n+1} | r_n)}{P_{\\text{proposal}}(r_{n+1} | r_n)} = \\frac{\\exp\\left( -\\frac{(\\Delta r_n - a(r_n)\\,\\Delta t)^2}{4D\\,\\Delta t} \\right)}{\\exp\\left( -\\frac{(\\Delta r_n - b(r_n)\\,\\Delta t)^2}{4D\\,\\Delta t} \\right)}\n$$\n$$\nw_n = \\exp\\left( \\frac{(\\Delta r_n - b(r_n)\\,\\Delta t)^2 - (\\Delta r_n - a(r_n)\\,\\Delta t)^2}{4D\\,\\Delta t} \\right)\n$$\nFor numerical stability, it is better to work with the logarithm of the weight, $\\ln(w)$. For the full path, $\\ln(w) = \\sum_{n=0}^{N-1} \\ln(w_n)$. For a single step:\n$$\n\\ln(w_n) = \\frac{(\\Delta r_n - b(r_n)\\,\\Delta t)^2 - (\\Delta r_n - a(r_n)\\,\\Delta t)^2}{4D\\,\\Delta t}\n$$\nLet's expand the numerator:\nLet $a_n = a(r_n)$ and $b_n = b(r_n)$.\nNumerator $= (\\Delta r_n^2 - 2\\Delta r_n b_n \\Delta t + b_n^2 \\Delta t^2) - (\\Delta r_n^2 - 2\\Delta r_n a_n \\Delta t + a_n^2 \\Delta t^2)$\n$= -2\\Delta r_n b_n \\Delta t + 2\\Delta r_n a_n \\Delta t + b_n^2 \\Delta t^2 - a_n^2 \\Delta t^2$\n$= 2\\Delta r_n (a_n - b_n) \\Delta t + (b_n^2 - a_n^2) \\Delta t^2$\nNow, we substitute this back into the expression for $\\ln(w_n)$:\n$$\n\\ln(w_n) = \\frac{2\\Delta r_n (a_n - b_n) \\Delta t + (b_n^2 - a_n^2) \\Delta t^2}{4D\\,\\Delta t}\n$$\nDividing by $\\Delta t$ simplifies the expression:\n$$\n\\ln(w_n) = \\frac{2\\Delta r_n (a_n - b_n) + (b_n^2 - a_n^2) \\Delta t}{4D}\n$$\nUsing the difference of squares, $b_n^2 - a_n^2 = (b_n - a_n)(b_n + a_n)$, we can factor out the term $(a_n - b_n)$:\n$$\n\\ln(w_n) = \\frac{2\\Delta r_n (a_n - b_n) - (a_n - b_n)(a_n + b_n) \\Delta t}{4D}\n$$\n$$\n\\ln(w_n) = \\frac{a_n - b_n}{4D} \\left( 2\\Delta r_n - (a_n + b_n)\\Delta t \\right)\n$$\nThis is the final, explicit expression for the logarithm of the single-step IS weight. The total path weight $w$ is then computed by summing these logarithmic contributions over all steps and exponentiating the result:\n$$\nw = \\exp\\left( \\sum_{n=0}^{N-1} \\ln(w_n) \\right) = \\exp\\left( \\sum_{n=0}^{N-1} \\frac{a(r_n) - b(r_n)}{4D} \\left( 2(r_{n+1}-r_n) - (a(r_n) + b(r_n))\\Delta t \\right) \\right)\n$$\nThis formula will be implemented in the program.\n\nThe required drifts are:\n- Target drift: $a(r) = -4\\alpha\\mu r(r^2-1)$\n- Proposal drift: $b(r) = a(r) + c(r_{\\star}-r)$\nThe implementation will compute these values at each step $r_n$ of the given path to calculate the final weight $w$.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the Importance Sampling (IS) weights for given paths and parameters.\n    The derivation is based on the ratio of path probabilities under target and proposal dynamics,\n    discretized using the Euler-Maruyama scheme.\n    \"\"\"\n    \n    test_cases = [\n        {\n            \"params\": {\"mu\": 0.5, \"alpha\": 1.0, \"dt\": 0.01, \"r_star\": 1.0, \"c\": 1.0},\n            \"path\": [-0.9, -0.8, -0.75, -0.6, -0.4, -0.2, 0.0, 0.25, 0.6, 0.85, 1.05]\n        },\n        {\n            \"params\": {\"mu\": 0.5, \"alpha\": 1.0, \"dt\": 0.01, \"r_star\": 1.0, \"c\": 0.0},\n            \"path\": [-0.95, -0.98, -1.0, -0.99, -0.98, -0.97, -0.96, -0.96, -0.95, -0.95, -0.95]\n        },\n        {\n            \"params\": {\"mu\": 0.5, \"alpha\": 1.0, \"dt\": 0.01, \"r_star\": 1.0, \"c\": 2.0},\n            \"path\": [-0.5] * 11\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        p = case[\"params\"]\n        mu, alpha, dt, r_star, c = p[\"mu\"], p[\"alpha\"], p[\"dt\"], p[\"r_star\"], p[\"c\"]\n        \n        # Per the problem statement, the diffusion coefficient D is set by the Einstein relation\n        # under non-dimensionalization, where D = mu.\n        D = mu\n        \n        path = np.array(case[\"path\"], dtype=np.float64)\n        \n        # Define the target drift function a(r)\n        def a_drift(r, alpha_val, mu_val):\n            return -4.0 * alpha_val * mu_val * r * (r**2 - 1.0)\n            \n        log_w_total = 0.0\n\n        # Sum the log-weights over all N-1 steps in the path\n        for i in range(len(path) - 1):\n            r_n = path[i]\n            r_n_plus_1 = path[i+1]\n            \n            delta_r_n = r_n_plus_1 - r_n\n            \n            a_n = a_drift(r_n, alpha, mu)\n            # The proposal drift b(r) = a(r) + c*(r_star - r)\n            b_n = a_n + c * (r_star - r_n)\n            \n            # If c = 0, then a_n = b_n, and log(w_n) is exactly 0. This avoids potential\n            # floating-point inaccuracies from the formula.\n            if c == 0.0:\n                log_w_n = 0.0\n            else:\n                # This is the derived formula for the log of the single-step weight:\n                # ln(w_n) = (a_n - b_n) / (4*D) * (2*delta_r_n - (a_n + b_n)*dt)\n                a_minus_b = a_n - b_n\n                a_plus_b = a_n + b_n\n                \n                log_w_n = (a_minus_b / (4.0 * D)) * (2.0 * delta_r_n - a_plus_b * dt)\n            \n            log_w_total += log_w_n\n        \n        # The total weight is the exponential of the sum of log-weights\n        weight = np.exp(log_w_total)\n        results.append(weight)\n\n    # Format the output as a comma-separated list in brackets\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}