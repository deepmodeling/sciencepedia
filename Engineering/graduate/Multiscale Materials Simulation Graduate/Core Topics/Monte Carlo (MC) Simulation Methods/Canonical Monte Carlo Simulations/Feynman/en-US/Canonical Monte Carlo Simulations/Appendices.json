{
    "hands_on_practices": [
        {
            "introduction": "A cornerstone of statistical mechanics is the fluctuation-dissipation theorem, which connects the response of a system to an external perturbation with its spontaneous equilibrium fluctuations. This exercise provides a hands-on application of this principle by guiding you to calculate the configurational heat capacity, $C_V^{\\mathrm{conf}}$, from the variance of the potential energy sampled during a Canonical Monte Carlo simulation . Mastering this connection is crucial for understanding how macroscopic thermodynamic properties emerge from the microscopic dynamics of a system.",
            "id": "3795411",
            "problem": "Consider a small three-dimensional Lennard–Jones (LJ) fluid undergoing Canonical ensemble (constant Number, Volume, Temperature) Monte Carlo simulation, often abbreviated as the constant Number, Volume, Temperature (NVT) ensemble in Monte Carlo (MC) sampling. The LJ fluid is described in reduced units with LJ energy scale $\\epsilon$, length scale $\\sigma$, and Boltzmann constant $k_{\\mathrm{B}}$, such that $\\epsilon = 1$, $\\sigma = 1$, and $k_{\\mathrm{B}} = 1$. The configurational contribution to the constant-volume heat capacity, denoted as $C_V^{\\mathrm{conf}}$, is defined only from potential energy fluctuations and excludes the kinetic part. Your task is to work from first principles of the canonical ensemble to derive the estimator for $C_V^{\\mathrm{conf}}$ and then, using provided synthetic sample potential energy time series, compute $C_V^{\\mathrm{conf}}$ at two temperatures and compare the results in light of known phase behavior of the LJ fluid.\n\nStarting point and derivation requirement:\n- Begin from the canonical ensemble definition with inverse temperature $\\beta = 1/(k_{\\mathrm{B}} T)$, partition function $Z(\\beta)$, and the ensemble average of the Hamiltonian $H$. Use only fundamental definitions of the canonical ensemble and standard thermodynamic identities to derive an explicit expression for $C_V$ in terms of equilibrium energy fluctuations. Then justify the separation of kinetic and potential contributions for a classical system and show how to specialize to $C_V^{\\mathrm{conf}}$ in terms of potential energy fluctuations. The derivation must demonstrate what $C_V^{\\mathrm{conf}}$ is, why it is valid, and how it follows from the canonical ensemble, without employing pre-given shortcut formulas.\n\nComputational requirement:\n- You are provided with three synthetic test cases representing sampled potential energy time series from small-system NVT MC runs. Each test case specifies the temperature $T$, the length $M$ of the time series, and parameters of a deterministic pseudo-random generator to construct the series as a Gaussian process with a specified mean and standard deviation, which serves as a controlled proxy for equilibrium sampling. For each case $j$, generate a sequence $\\{U_i^{(j)}\\}_{i=1}^{M_j}$ of potential energies using a normal distribution with the given mean and standard deviation and the provided seed. Interpret these in LJ reduced units with $k_{\\mathrm{B}} = 1$. From the generated time series, compute $C_V^{\\mathrm{conf}}$ using an unbiased estimator for the variance of the potential energy. Do not assume independence beyond using the unbiased sample variance; no correlation correction is required.\n\nPhysical units and numerical output:\n- All quantities are in reduced, dimensionless LJ units. Report $C_V^{\\mathrm{conf}}$ in units of $k_{\\mathrm{B}}$ (dimensionless in this reduced unit system). Express the final results rounded to four decimal places.\n- In addition to the heat capacities for the first two temperatures, report a logical comparison indicating whether the configurational heat capacity at the lower temperature exceeds that at the higher temperature (consistent with the expectation that configurational heat capacity is larger in more structured, dense liquid-like states than in dilute gas-like states). Also compute $C_V^{\\mathrm{conf}}$ for the third, dilute high-temperature case, which acts as an edge case where configurational fluctuations are small.\n\nTest suite:\n- Case $1$: Temperature $T_1 = 0.7$, series length $M_1 = 400$, generator seed $s_1 = 123$, normal mean $\\mu_1 = -170.0$, normal standard deviation $\\sigma_1 = 5.0$.\n- Case $2$: Temperature $T_2 = 2.0$, series length $M_2 = 400$, generator seed $s_2 = 456$, normal mean $\\mu_2 = -100.0$, normal standard deviation $\\sigma_2 = 3.0$.\n- Case $3$ (edge case, dilute and high temperature): Temperature $T_3 = 3.0$, series length $M_3 = 400$, generator seed $s_3 = 789$, normal mean $\\mu_3 = -2.0$, normal standard deviation $\\sigma_3 = 0.5$.\n\nRequired algorithmic steps:\n- For each case $j \\in \\{1,2,3\\}$:\n  1. Generate $M_j$ samples $U_i^{(j)}$ using a normal distribution with parameters $(\\mu_j, \\sigma_j)$ and seed $s_j$.\n  2. Compute the unbiased sample variance $s_j^2$ of the potential energy time series using a numerically stable method suitable for streaming data.\n  3. Compute $C_{V,j}^{\\mathrm{conf}}$ from the variance and temperature.\n- After computing $C_{V,1}^{\\mathrm{conf}}$ and $C_{V,2}^{\\mathrm{conf}}$, compute the boolean $b$ indicating whether $C_{V,1}^{\\mathrm{conf}}  C_{V,2}^{\\mathrm{conf}}$.\n\nFinal output specification:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain four entries in the following order: $[C_{V,1}^{\\mathrm{conf}}, C_{V,2}^{\\mathrm{conf}}, b, C_{V,3}^{\\mathrm{conf}}]$, where the three heat capacities are rounded to four decimal places and $b$ is a boolean.\n\nScientific realism and interpretation:\n- The lower-temperature case $T_1$ corresponds to a dense, liquid-like LJ state where configurational fluctuations and thus $C_V^{\\mathrm{conf}}$ are expected to be larger.\n- The higher-temperature case $T_2$ represents a supercritical or gas-like state at higher thermal energy, where configurational fluctuations are reduced and $C_V^{\\mathrm{conf}}$ is expected to be smaller than in Case $1$.\n- The third case $T_3$ models a dilute, high-temperature regime where interactions are weak and $C_V^{\\mathrm{conf}}$ is expected to be close to zero.\n\nYour program must implement this logic exactly and produce the final single-line output in the required format: for example, $[x_1,x_2,\\text{True},x_3]$ where $x_1$, $x_2$, and $x_3$ are floats rounded to four decimals.",
            "solution": "The problem is assessed to be valid as it is scientifically grounded in statistical mechanics, well-posed, objective, and contains all necessary information for a unique, verifiable solution. It presents a standard exercise in computational statistical physics.\n\nThe primary task is to derive the estimator for the configurational contribution to the constant-volume heat capacity, denoted $C_V^{\\mathrm{conf}}$, from the first principles of the canonical ensemble. Subsequently, this estimator is to be applied to synthetic potential energy data to compute $C_V^{\\mathrm{conf}}$ for a Lennard-Jones fluid at three different state points.\n\nFirst, we begin the derivation. In the canonical ensemble, a system of $N$ particles in a volume $V$ at a fixed temperature $T$ is described by the canonical partition function $Z$. For a classical system, $Z$ is an integral over all possible positions $\\mathbf{r}^N$ and momenta $\\mathbf{p}^N$ of the particles:\n$$\nZ(N, V, T) = \\frac{1}{N! h^{3N}} \\int d\\mathbf{r}^N d\\mathbf{p}^N e^{-\\beta H(\\mathbf{r}^N, \\mathbf{p}^N)}\n$$\nHere, $\\beta = 1/(k_{\\mathrm{B}} T)$ is the inverse temperature, with $k_{\\mathrm{B}}$ being the Boltzmann constant. $H(\\mathbf{r}^N, \\mathbf{p}^N)$ is the classical Hamiltonian of the system, and the factor $1/(N! h^{3N})$ ensures correct counting of states and dimensional consistency. The Hamiltonian is the sum of the kinetic energy $K(\\mathbf{p}^N)$ and the potential energy $U(\\mathbf{r}^N)$:\n$$\nH(\\mathbf{r}^N, \\mathbf{p}^N) = K(\\mathbf{p}^N) + U(\\mathbf{r}^N) = \\sum_{i=1}^{N} \\frac{|\\mathbf{p}_i|^2}{2m} + U(\\mathbf{r}^N)\n$$\nBecause the Hamiltonian is separable into a momentum-dependent part and a position-dependent part, the partition function can be factored:\n$$\nZ = \\left( \\frac{1}{h^{3N}} \\int d\\mathbf{p}^N e^{-\\beta K(\\mathbf{p}^N)} \\right) \\left( \\frac{1}{N!} \\int d\\mathbf{r}^N e^{-\\beta U(\\mathbf{r}^N)} \\right)\n$$\nThis separates $Z$ into a kinetic (or ideal gas) component and a configurational component. The configurational integral, $Q$, is defined as:\n$$\nQ(N, V, T) = \\int_V d\\mathbf{r}^N e^{-\\beta U(\\mathbf{r}^N)}\n$$\nThe connection between thermodynamics and statistical mechanics is established through the Helmholtz free energy $A = -k_{\\mathrm{B}} T \\ln Z = -1/\\beta \\ln Z$. The average internal energy $\\langle E \\rangle$ can be derived from $Z$:\n$$\n\\langle E \\rangle = \\langle H \\rangle = -\\frac{\\partial \\ln Z}{\\partial \\beta}\n$$\nThe constant-volume heat capacity, $C_V$, is defined as the partial derivative of the average internal energy with respect to temperature at constant volume and number of particles:\n$$\nC_V = \\left( \\frac{\\partial \\langle E \\rangle}{\\partial T} \\right)_{N,V}\n$$\nTo express this in terms of fluctuations, we first change the derivative from $T$ to $\\beta$. Using the chain rule, $\\frac{\\partial}{\\partial T} = \\frac{d\\beta}{dT} \\frac{\\partial}{\\partial \\beta} = -\\frac{1}{k_{\\mathrm{B}} T^2} \\frac{\\partial}{\\partial \\beta} = -k_{\\mathrm{B}} \\beta^2 \\frac{\\partial}{\\partial \\beta}$. Applying this to the definition of $C_V$:\n$$\nC_V = -k_{\\mathrm{B}} \\beta^2 \\frac{\\partial \\langle E \\rangle}{\\partial \\beta} = -k_{\\mathrm{B}} \\beta^2 \\frac{\\partial}{\\partial \\beta} \\left( -\\frac{\\partial \\ln Z}{\\partial \\beta} \\right) = k_{\\mathrm{B}} \\beta^2 \\frac{\\partial^2 \\ln Z}{\\partial \\beta^2}\n$$\nThe second derivative of $\\ln Z$ can be shown to be equal to the variance of the total energy $E = H$:\n$$\n\\frac{\\partial^2 \\ln Z}{\\partial \\beta^2} = \\frac{\\partial}{\\partial \\beta} \\left( \\frac{1}{Z} \\frac{\\partial Z}{\\partial \\beta} \\right) = \\frac{1}{Z}\\frac{\\partial^2 Z}{\\partial \\beta^2} - \\frac{1}{Z^2}\\left(\\frac{\\partial Z}{\\partial \\beta}\\right)^2 = \\langle E^2 \\rangle - \\langle E \\rangle^2 = \\sigma_E^2\n$$\nThis leads to the celebrated fluctuation-dissipation formula for heat capacity:\n$$\nC_V = k_{\\mathrm{B}} \\beta^2 (\\langle E^2 \\rangle - \\langle E \\rangle^2) = \\frac{\\langle E^2 \\rangle - \\langle E \\rangle^2}{k_{\\mathrm{B}} T^2}\n$$\nThe total heat capacity $C_V$ can be split into kinetic and configurational contributions. Since $\\langle E \\rangle = \\langle K \\rangle + \\langle U \\rangle$, we have:\n$$\nC_V = \\left( \\frac{\\partial \\langle K \\rangle}{\\partial T} \\right)_{N,V} + \\left( \\frac{\\partial \\langle U \\rangle}{\\partial T} \\right)_{N,V} = C_V^{\\mathrm{kin}} + C_V^{\\mathrm{conf}}\n$$\nFor a classical system with $3N$ quadratic kinetic degrees of freedom, the equipartition theorem states that $\\langle K \\rangle = \\frac{3N}{2} k_{\\mathrm{B}} T$. Thus, the kinetic contribution to the heat capacity is a constant:\n$$\nC_V^{\\mathrm{kin}} = \\frac{\\partial}{\\partial T} \\left( \\frac{3N}{2} k_{\\mathrm{B}} T \\right) = \\frac{3N}{2} k_{\\mathrm{B}}\n$$\nThe configurational contribution, $C_V^{\\mathrm{conf}}$, arises from the temperature dependence of the average potential energy:\n$$\nC_V^{\\mathrm{conf}} = \\left( \\frac{\\partial \\langle U \\rangle}{\\partial T} \\right)_{N,V}\n$$\nFollowing an identical derivation as for the total $C_V$, but starting from the average potential energy $\\langle U \\rangle = -\\frac{\\partial \\ln Q}{\\partial \\beta}$, we find:\n$$\nC_V^{\\mathrm{conf}} = k_{\\mathrm{B}} \\beta^2 (\\langle U^2 \\rangle - \\langle U \\rangle^2) = \\frac{\\langle U^2 \\rangle - \\langle U \\rangle^2}{k_{\\mathrm{B}} T^2}\n$$\nThis derivation is valid because the statistical average of the product of a purely momentum-dependent function like $K$ and a purely position-dependent function like $U$ factors into the product of their individual averages, i.e., $\\langle KU \\rangle = \\langle K \\rangle \\langle U \\rangle$. Therefore, the covariance between $K$ and $U$ is zero, and the total energy variance is the sum of the kinetic and potential energy variances: $\\sigma_E^2 = \\sigma_K^2 + \\sigma_U^2$. This validates the separation of $C_V$ into independent kinetic and configurational parts.\n\nFor the computational portion of this problem, we use the derived expression for $C_V^{\\mathrm{conf}}$. The problem specifies reduced units where $k_{\\mathrm{B}} = 1$, so the formula simplifies to:\n$$\nC_V^{\\mathrm{conf}} = \\frac{\\langle U^2 \\rangle - \\langle U \\rangle^2}{T^2} = \\frac{\\text{Var}(U)}{T^2}\n$$\nIn a simulation, the true ensemble average variance, $\\text{Var}(U)$, is estimated from a finite time series of potential energy samples $\\{U_i\\}_{i=1}^M$. The problem requires using an unbiased estimator for the variance, which is the sample variance $s^2$:\n$$\ns^2 = \\frac{1}{M-1} \\sum_{i=1}^M (U_i - \\bar{U})^2\n$$\nwhere $\\bar{U} = \\frac{1}{M} \\sum_{i=1}^M U_i$ is the sample mean. The numerical stability concern mentioned in the prompt is addressed by using high-precision arithmetic inherent in modern numerical libraries like NumPy, whose standard variance function is robust for this problem's scope.\n\nFor each of the three test cases, the procedure is as follows:\n$1$. A sequence of $M_j$ potential energy values is generated using a pseudo-random number generator, drawing from a normal distribution with specified mean $\\mu_j$, standard deviation $\\sigma_j$, and seed $s_j$.\n$2$. The unbiased sample variance, $s_j^2$, of this sequence is computed.\n$3$. The configurational heat capacity is calculated using the formula $C_{V,j}^{\\mathrm{conf}} = s_j^2 / T_j^2$.\n$4$. After computing $C_{V,1}^{\\mathrm{conf}}$ and $C_{V,2}^{\\mathrm{conf}}$, a boolean comparison $b = (C_{V,1}^{\\mathrm{conf}}  C_{V,2}^{\\mathrm{conf}})$ is performed to check if the heat capacity is larger at the lower temperature, as is physically expected for the transition from a dense liquid to a supercritical fluid. The third case serves as a check in a dilute, high-temperature regime where configurational contributions should be minimal.\nThe final output consists of these computed values, rounded as specified.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the configurational heat capacity from synthetic data and performs a comparison.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (T, M, seed, mean, std_dev)\n        (0.7, 400, 123, -170.0, 5.0), # Case 1: Dense liquid-like\n        (2.0, 400, 456, -100.0, 3.0), # Case 2: Supercritical/gas-like\n        (3.0, 400, 789, -2.0, 0.5),   # Case 3: Dilute, high-T gas\n    ]\n\n    cv_results = []\n    \n    # Process each test case\n    for case in test_cases:\n        T, M, seed, mu, sigma = case\n        \n        # 1. Generate M samples using a normal distribution with the given parameters.\n        # Set the seed for reproducibility.\n        np.random.seed(seed)\n        # Generate the potential energy time series.\n        potential_energies = np.random.normal(loc=mu, scale=sigma, size=M)\n        \n        # 2. Compute the unbiased sample variance of the potential energy time series.\n        # The parameter ddof=1 (delta degrees of freedom) ensures the denominator\n        # is N-1, providing an unbiased estimator.\n        # This is a numerically stable method for the data in this problem.\n        potential_energy_variance = np.var(potential_energies, ddof=1)\n        \n        # 3. Compute the configurational heat capacity.\n        # In reduced units, k_B = 1.\n        # C_V_conf = ( Var(U) ) / ( k_B * T^2 ) = Var(U) / T^2\n        cv_conf = potential_energy_variance / (T**2)\n        \n        cv_results.append(cv_conf)\n\n    # Extract results for clarity\n    cv_conf_1 = cv_results[0]\n    cv_conf_2 = cv_results[1]\n    cv_conf_3 = cv_results[2]\n\n    # 4. Compute the boolean 'b' indicating if C_V_conf at T1  C_V_conf at T2.\n    # This is expected behavior as the liquid state has larger configurational\n    # fluctuations than the gas-like state.\n    comparison_bool = cv_conf_1  cv_conf_2\n\n    # Format the final output as a comma-separated list in brackets,\n    # with heat capacity values rounded to four decimal places.\n    # The boolean value is automatically converted to 'True' or 'False'.\n    output_list = [\n        f\"{cv_conf_1:.4f}\",\n        f\"{cv_conf_2:.4f}\",\n        str(comparison_bool),\n        f\"{cv_conf_3:.4f}\"\n    ]\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(output_list)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A critical aspect of analyzing simulation data is the correct estimation of statistical uncertainty. Because Monte Carlo methods generate a correlated time series of configurations, a naive application of the standard error formula for independent samples will severely underestimate the true error. This practice introduces block averaging, a robust and widely used technique to account for these time correlations and produce reliable error estimates for calculated observables .",
            "id": "3795397",
            "problem": "You are tasked with designing and implementing a practical estimator for the potential energy per particle and its statistical uncertainty for samples generated by Canonical Number–Volume–Temperature (NVT) Monte Carlo (MC) simulations. The estimator must rely on block averaging to rigorously account for time correlations in the sampled data. Your program must be a complete, runnable implementation that computes the estimator on a fixed test suite and outputs the results in a single line, in the exact format specified below.\n\nStart from a valid foundational base appropriate for Canonical Number–Volume–Temperature (NVT) ensembles and Monte Carlo (MC) sampling:\n- The canonical ensemble assigns a probability density to microstates proportional to the Boltzmann factor $e^{-\\beta U(\\mathbf{x})}$, where $U(\\mathbf{x})$ is the potential energy and $\\beta$ is the inverse temperature. Ensemble averages of observables are expectations with respect to this density.\n- An ergodic Markov chain MC sampler produces a stationary time series of observable values whose long-time average converges to the ensemble-average expectation under broad regularity conditions.\n- Autocorrelation in the time series increases the uncertainty of the sample mean relative to independent sampling. Block averaging is a principled approach to mitigate autocorrelation by coarse-graining the correlated samples into averages over non-overlapping blocks.\n\nYour tasks are:\n1. Using the foundational base above, devise and implement an estimator for the ensemble-average potential energy per particle and a corresponding estimator for its statistical uncertainty that is valid for correlated MC samples via block averaging. The estimator must be constructed from first principles of stationary ergodic processes, and not rely on unmotivated shortcut formulas.\n2. Implement a program that applies this estimator to three synthetic MC trajectories of the potential energy per particle generated by a stationary first-order autoregressive recursion that emulates realistic time correlations. Each trajectory $u_t$ is generated in dimensionless reduced energy units of Lennard-Jones (that is, in units of the Lennard-Jones energy scale $\\varepsilon$, so the results are dimensionless multiples of $\\varepsilon$) by\n$$\nu_t = \\mu + \\phi \\left(u_{t-1} - \\mu\\right) + \\sigma \\,\\xi_t,\n$$\nwhere $\\xi_t \\sim \\mathcal{N}(0,1)$ are independent standard normal random variables, $\\mu$ is the mean level, $\\phi$ controls correlation strength, and $\\sigma$ sets the innovation amplitude. For each trajectory, the initial value $u_0$ is set to $u_0 = \\mu + \\sigma \\,\\xi_0$ with the same noise distribution. You must generate these trajectories using the specified parameters and seeds exactly as given in the test suite below.\n\nEstimator construction and selection rule:\n- Compute a point estimate of the ensemble-average potential energy per particle as the time average over the full trajectory.\n- For block averaging, consider non-overlapping blocks of length $b$ and compute block means. Use the variability among block means to construct a consistent estimator of the statistical uncertainty (standard error) of the overall sample mean for each block size $b$.\n- To select a block size, use the deterministic rule: choose the largest block size $b$ from the provided list that yields at least $4$ full blocks (that is, if $N$ is the trajectory length, select $b$ such that $\\left\\lfloor N/b \\right\\rfloor \\ge 4$). If no such $b$ exists, choose the largest $b$ that yields at least $2$ full blocks. This rule ensures adequate coarse-graining while preserving sufficient block count for a variance estimate.\n\nUnits and formatting requirements:\n- All energies are in reduced Lennard-Jones units, that is, multiples of $\\varepsilon$, and are therefore dimensionless. Report the potential energy per particle and its uncertainty in the same units.\n- No angles are involved.\n- There are no percentages in the output; any threshold or ratio must be handled as a pure decimal number (for example, $0.02$).\n- Round all reported numerical results to $6$ decimal places.\n\nTest suite:\nGenerate three trajectories using the recursion above with the following parameters and seeds. For each case, use the same list of block sizes $b \\in \\{1, 2, 5, 10, 20, 50, 100, 200, 500, 1000, 2000\\}$.\n\n- Case $1$ (general happy path, strong correlation, long trajectory):\n  - Length $N = 50000$,\n  - Mean $\\mu = 1.25$,\n  - Correlation parameter $\\phi = 0.95$,\n  - Innovation amplitude $\\sigma = 0.5$,\n  - Random seed $1234$.\n- Case $2$ (boundary condition, strong correlation, short trajectory):\n  - Length $N = 1500$,\n  - Mean $\\mu = -0.25$,\n  - Correlation parameter $\\phi = 0.9$,\n  - Innovation amplitude $\\sigma = 0.3$,\n  - Random seed $5678$.\n- Case $3$ (edge case, nearly independent samples):\n  - Length $N = 20000$,\n  - Mean $\\mu = 0.0$,\n  - Correlation parameter $\\phi = 0.0$,\n  - Innovation amplitude $\\sigma = 1.0$,\n  - Random seed $42$.\n\nRequired final output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case result must itself be a two-element list containing the estimated mean potential energy per particle and the estimated standard error using the selected block size, both rounded to $6$ decimal places. For example,\n$[[\\hat{\\mu}_1,\\mathrm{SE}_1],[\\hat{\\mu}_2,\\mathrm{SE}_2],[\\hat{\\mu}_3,\\mathrm{SE}_3]]$.",
            "solution": "The objective is to devise and implement an estimator for the ensemble-average potential energy per particle, $\\langle u \\rangle$, and its statistical uncertainty from a time-correlated data series $\\{u_t\\}_{t=0}^{N-1}$ generated by a Monte Carlo simulation. The method of block averaging is employed to account for the autocorrelation inherent in such data.\n\nFirst, we establish the estimator for the mean potential energy per particle. For a stationary and ergodic process, the law of large numbers guarantees that the time average converges to the ensemble average. Therefore, the sample mean is a consistent and unbiased estimator for $\\langle u \\rangle$:\n$$\n\\hat{\\mu} = \\frac{1}{N} \\sum_{t=0}^{N-1} u_t\n$$\nwhere $N$ is the total number of samples in the trajectory. This estimator will be used as the point estimate for the potential energy per particle.\n\nNext, we address the estimation of the statistical uncertainty, or standard error, of $\\hat{\\mu}$. For a sequence of independent and identically distributed (i.i.d.) random variables, the variance of the sample mean is $\\mathrm{Var}(\\hat{\\mu}) = \\sigma_u^2/N$, where $\\sigma_u^2$ is the variance of the underlying distribution. However, data from an MC simulation constitute a Markov chain and are typically correlated in time. The variance of the sample mean for a stationary, correlated time series is given by:\n$$\n\\mathrm{Var}(\\hat{\\mu}) = \\frac{1}{N^2} \\sum_{i=0}^{N-1} \\sum_{j=0}^{N-1} \\mathrm{Cov}(u_i, u_j) = \\frac{\\sigma_u^2}{N} \\left( 1 + 2 \\sum_{k=1}^{N-1} \\left(1 - \\frac{k}{N}\\right) \\rho(k) \\right)\n$$\nwhere $\\rho(k)$ is the autocorrelation function at lag $k$. For positively correlated data ($\\rho(k)0$), which is common in physical simulations, the naive i.i.d. formula drastically underestimates the true variance.\n\nBlock averaging is a robust technique to estimate $\\mathrm{Var}(\\hat{\\mu})$ without needing to explicitly compute the full autocorrelation function. The core principle is to coarse-grain the time series into blocks that are long enough to be approximately statistically independent.\n\nThe procedure is as follows:\n1.  The full data series $\\{u_t\\}_{t=0}^{N-1}$ is partitioned into $N_b = \\lfloor N/b \\rfloor$ non-overlapping blocks, each of length $b$. Any remaining $N \\pmod b$ data points at the end of the series are discarded for this part of the analysis.\n\n2.  For each block $k \\in \\{1, 2, \\dots, N_b\\}$, a block average is computed:\n    $$\n    \\bar{u}_k = \\frac{1}{b} \\sum_{i=0}^{b-1} u_{(k-1)b + i}\n    $$\n\n3.  The central hypothesis of block averaging is that if the block length $b$ is sufficiently large, specifically, much larger than the integrated autocorrelation time of the process, the block averages $\\{\\bar{u}_k\\}$ will be approximately uncorrelated. We can then treat them as a new set of $N_b$ i.i.d. samples drawn from a distribution with mean $\\langle u \\rangle$ and some variance $\\sigma_{\\bar{u}}^2$.\n\n4.  Under this assumption of independence, we can estimate the variance of the population of block averages, $\\sigma_{\\bar{u}}^2$, using the standard formula for the unbiased sample variance:\n    $$\n    \\hat{\\sigma}_{\\bar{u}}^2 = \\frac{1}{N_b - 1} \\sum_{k=1}^{N_b} (\\bar{u}_k - \\hat{\\mu}_B)^2\n    $$\n    where $\\hat{\\mu}_B = \\frac{1}{N_b} \\sum_{k=1}^{N_b} \\bar{u}_k$ is the mean of the block averages.\n\n5.  The overall sample mean $\\hat{\\mu}$ is approximately the mean of these block means. By the Central Limit Theorem, the variance of the mean of $N_b$ i.i.d. samples is the variance of a single sample divided by $N_b$. Therefore, the estimated variance of our overall mean estimator $\\hat{\\mu}$ is:\n    $$\n    \\widehat{\\mathrm{Var}}(\\hat{\\mu}) \\approx \\frac{\\hat{\\sigma}_{\\bar{u}}^2}{N_b} = \\frac{1}{N_b(N_b - 1)} \\sum_{k=1}^{N_b} (\\bar{u}_k - \\hat{\\mu}_B)^2\n    $$\n\n6.  The statistical uncertainty, or standard error of the mean ($\\mathrm{SE}(\\hat{\\mu})$), is the square root of this estimated variance:\n    $$\n    \\mathrm{SE}(\\hat{\\mu}) = \\sqrt{\\widehat{\\mathrm{Var}}(\\hat{\\mu})} = \\sqrt{\\frac{\\hat{\\sigma}_{\\bar{u}}^2}{N_b}}\n    $$\nThis provides a statistically sound estimator for the uncertainty that properly accounts for time correlations, provided the chosen block size $b$ is adequate.\n\nThe implementation will generate synthetic trajectories using the specified first-order autoregressive model, $u_t = \\mu + \\phi(u_{t-1} - \\mu) + \\sigma\\xi_t$, which mimics the correlated nature of MC data. The block size $b$ is chosen according to the deterministic rule provided: select the largest block size from the candidate list that yields at least $4$ full blocks. If none do, select the largest size that yields at least $2$ blocks. This rule balances the need for a large $b$ to ensure decorrelation of block averages against the need for a sufficient number of blocks $N_b$ to obtain a reliable estimate of the variance $\\hat{\\sigma}_{\\bar{u}}^2$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the potential energy per particle and its statistical uncertainty\n    for synthetic Monte Carlo trajectories using block averaging.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: general happy path, strong correlation, long trajectory\n        {'N': 50000, 'mu': 1.25, 'phi': 0.95, 'sigma': 0.5, 'seed': 1234},\n        # Case 2: boundary condition, strong correlation, short trajectory\n        {'N': 1500, 'mu': -0.25, 'phi': 0.9, 'sigma': 0.3, 'seed': 5678},\n        # Case 3: edge case, nearly independent samples\n        {'N': 20000, 'mu': 0.0, 'phi': 0.0, 'sigma': 1.0, 'seed': 42},\n    ]\n\n    block_sizes_candidate = [1, 2, 5, 10, 20, 50, 100, 200, 500, 1000, 2000]\n    \n    results = []\n\n    for case in test_cases:\n        N = case['N']\n        mu = case['mu']\n        phi = case['phi']\n        sigma = case['sigma']\n        seed = case['seed']\n        \n        # 1. Generate the synthetic trajectory using the AR(1) model.\n        rng = np.random.default_rng(seed)\n        trajectory = np.zeros(N)\n        \n        # Generate noise terms\n        xi = rng.standard_normal(N)\n        \n        # Initial value u_0\n        trajectory[0] = mu + sigma * xi[0]\n        \n        # Generate the rest of the trajectory\n        for t in range(1, N):\n            trajectory[t] = mu + phi * (trajectory[t-1] - mu) + sigma * xi[t]\n            \n        # 2. Compute the point estimate for the mean potential energy.\n        mean_potential_energy = np.mean(trajectory)\n        \n        # 3. Select the appropriate block size based on the specified rule.\n        selected_b = -1\n        # First-pass rule: find largest b yielding at least 4 blocks.\n        for b in reversed(block_sizes_candidate):\n            if N // b = 4:\n                selected_b = b\n                break\n        \n        # Second-pass rule if first pass fails: find largest b yielding at least 2 blocks.\n        if selected_b == -1:\n            for b in reversed(block_sizes_candidate):\n                if N // b = 2:\n                    selected_b = b\n                    break\n        \n        if selected_b == -1:\n            # This case should not be reached with the given test suite.\n            raise ValueError(\"Could not find a suitable block size for the given N.\")\n\n        # 4. Perform block averaging to compute the standard error.\n        num_blocks = N // selected_b\n        \n        # Truncate trajectory to an integer number of blocks.\n        blocked_trajectory = trajectory[:num_blocks * selected_b]\n        \n        # Reshape into blocks.\n        blocks = blocked_trajectory.reshape((num_blocks, selected_b))\n        \n        # Compute the mean of each block.\n        block_means = np.mean(blocks, axis=1)\n        \n        # 5. Calculate the standard error of the overall mean.\n        # Variance of the block means (using N_b-1 in the denominator).\n        variance_of_block_means = np.var(block_means, ddof=1)\n        \n        # Variance of the overall mean is Var(block_means) / num_blocks.\n        variance_of_mean = variance_of_block_means / num_blocks\n        \n        # Standard error is the square root of the variance of the mean.\n        standard_error = np.sqrt(variance_of_mean)\n        \n        # 6. Format and store the results.\n        rounded_mean = round(mean_potential_energy, 6)\n        rounded_se = round(standard_error, 6)\n        \n        results.append([rounded_mean, rounded_se])\n\n    # Final print statement in the exact required format (no spaces).\n    print(str(results).replace(\" \", \"\"))\n\nsolve()\n```"
        },
        {
            "introduction": "Canonical Monte Carlo simulations can become trapped in local energy minima, failing to adequately sample the configuration space for systems with complex energy landscapes. Parallel Tempering, or Replica Exchange Monte Carlo, is a powerful enhanced sampling method designed to overcome this challenge by coupling simulations at different temperatures. This exercise focuses on a key practical aspect of this technique: designing an efficient temperature schedule to ensure adequate exchange between replicas and accelerate convergence .",
            "id": "3795407",
            "problem": "You are to implement a complete, runnable program that designs a temperature schedule for Parallel Tempering (Replica Exchange) in the Canonical ensemble and computes expected swap acceptance rates for a dense Lennard–Jones (LJ) fluid from energy histograms at adjacent temperatures. Canonical Monte Carlo in the Number of particles–Volume–Temperature (NVT) ensemble assumes the energy of microstates is distributed according to the Boltzmann weight. Parallel Tempering augments canonical Monte Carlo by exchanging configurations between replicas at different temperatures to improve sampling. Begin from the following fundamental base: the canonical probability density over energy $E$ at temperature $T$ is $P(E;T) = Z(T)^{-1} g(E) \\exp(-\\beta E)$, where $Z(T)$ is the partition function, $g(E)$ is the degeneracy, and $\\beta = 1/(k_{\\mathrm{B}} T)$ with Boltzmann constant $k_{\\mathrm{B}}$. The Metropolis acceptance probability for a replica swap between temperatures $T_a$ and $T_b$ with current energies $E_a$ and $E_b$ is $\\alpha(E_a,E_b; \\beta_a,\\beta_b) = \\min\\left(1, \\exp\\left[(\\beta_a - \\beta_b)(E_b - E_a)\\right]\\right)$. The expected swap acceptance rate between two adjacent replicas is the canonical expectation of $\\alpha$ over the joint energy distribution at $T_a$ and $T_b$. Use discrete histograms of total energy to approximate this expectation.\n\nConstruct the expected acceptance between two temperatures $T_a$ and $T_b$ using a discrete approximation in which you are provided arrays of histogram bin centers $\\{E_i\\}$ and normalized bin probabilities $\\{p_i\\}$ for $T_a$, and similarly $\\{F_j\\}$ and $\\{q_j\\}$ for $T_b$. Assume independence between energies of the two replicas conditioned on their temperatures. The expected acceptance must be computed as the double sum\n$$\n\\mathbb{E}[\\alpha] \\approx \\sum_{i} \\sum_{j} p_i \\, q_j \\, \\min\\left(1, \\exp\\left[(\\beta_a - \\beta_b)(F_j - E_i)\\right]\\right).\n$$\nUse reduced Lennard–Jones units with $k_{\\mathrm{B}} = 1$ so that temperature and energy are dimensionless.\n\nDesign an algorithm that, given a fine candidate set of temperatures $\\{T_k\\}$ and corresponding energy histograms for each $T_k$, selects a subset schedule $\\{T_{s(\\ell)}\\}$ from $T_{\\min}$ to $T_{\\max}$ with the goal that each adjacent pair in the chosen schedule has expected swap acceptance within a target band $[p_{\\mathrm{low}}, p_{\\mathrm{high}}]$. Your selection should greedily minimize the number of replicas by choosing, at each step, the furthest next temperature whose expected acceptance with the current temperature is at least $p_{\\mathrm{low}}$, breaking ties by choosing the next temperature whose expected acceptance is closest to a target $p_{\\mathrm{tgt}} \\in [p_{\\mathrm{low}}, p_{\\mathrm{high}}]$. If no candidate temperature beyond the current one achieves acceptance at least $p_{\\mathrm{low}}$, you must still select the immediate next temperature, and flag that the lower bound was not achieved for this step. After constructing the schedule, also report whether the upper bound was satisfied for all neighbors. All energies and temperatures are in reduced Lennard–Jones units.\n\nTo evaluate your implementation, use the following test suite, in which total energy histograms are generated by Gaussians consistent with the central limit theorem for a dense Lennard–Jones fluid: the total energy $E$ is modeled as normal with mean $N \\mu(T)$ and variance $N \\sigma^2(T)$, where $N$ is the number of particles, and $\\mu(T)$ and $\\sigma^2(T)$ are per-particle mean and variance functions. For each temperature $T$, construct a histogram with $n_{\\mathrm{bins}}$ equally spaced bin centers spanning $\\pm n_\\sigma$ standard deviations about the mean, and convert the normal probability density function to discrete bin probabilities by multiplying by the bin width and normalizing. Use $k_{\\mathrm{B}} = 1$.\n\nTest case $1$ (happy path):\n- Number of particles $N = 64$, candidate temperatures $\\{T_k\\} = [0.70, 0.78, 0.86, 0.94, 1.02]$, per-particle mean $\\mu(T) = a_0 + a_1 T + a_2/T$ with $a_0 = -6.0$, $a_1 = 0.1$, $a_2 = -0.5$, per-particle variance $\\sigma^2(T) = c_0 T + c_1$ with $c_0 = 0.05$, $c_1 = 0.02$, histogram parameters $n_{\\mathrm{bins}} = 201$, $n_\\sigma = 4.0$, target band $[p_{\\mathrm{low}}, p_{\\mathrm{high}}] = [0.20, 0.40]$, target $p_{\\mathrm{tgt}} = 0.30$.\n\nTest case $2$ (boundary condition with narrow histograms):\n- Number of particles $N = 64$, candidate temperatures $\\{T_k\\} = [0.70, 0.72, 0.74, 0.76, 0.78]$, per-particle mean $\\mu(T) = a_0 + a_1 T + a_2/T$ with $a_0 = -6.0$, $a_1 = 0.1$, $a_2 = -0.5$, per-particle variance $\\sigma^2(T) = c_0 T$ with $c_0 = 0.01$, histogram parameters $n_{\\mathrm{bins}} = 201$, $n_\\sigma = 4.0$, target band $[p_{\\mathrm{low}}, p_{\\mathrm{high}}] = [0.15, 0.50]$, target $p_{\\mathrm{tgt}} = 0.30$.\n\nTest case $3$ (edge case with only two temperatures):\n- Number of particles $N = 16$, candidate temperatures $\\{T_k\\} = [0.70, 1.00]$, per-particle mean $\\mu(T) = a_0 + a_1 T + a_2/T$ with $a_0 = -6.0$, $a_1 = 0.1$, $a_2 = -0.5$, per-particle variance $\\sigma^2(T) = c_0 T + c_1$ with $c_0 = 0.05$, $c_1 = 0.02$, histogram parameters $n_{\\mathrm{bins}} = 201$, $n_\\sigma = 4.0$, target band $[p_{\\mathrm{low}}, p_{\\mathrm{high}}] = [0.20, 0.60]$, target $p_{\\mathrm{tgt}} = 0.40$.\n\nYour program must:\n- For each test case, generate total energy histograms for all candidate temperatures using the specified Gaussian model with the provided parameters.\n- Compute the expected swap acceptance rates between adjacent temperatures in the selected schedule using the discrete double sum approximation of the canonical expectation derived from the Metropolis criterion.\n- Construct the schedule according to the greedy rule described above, starting at the smallest temperature and ending at the largest temperature.\n- Produce a single line of output containing the results from all test cases as a comma-separated list enclosed in square brackets. Each test case’s result must be a list containing: the selected temperatures list, the list of expected acceptance rates for each adjacent pair in the selected schedule, a boolean indicating whether all acceptances achieved the lower bound, and a boolean indicating whether all acceptances satisfied the upper bound. For example, the output must be of the form $[ [ \\ldots ], [ \\ldots ], [ \\ldots ] ]$ with each inner list containing $[ \\text{temperature\\_list}, \\text{acceptance\\_list}, \\text{lower\\_bound\\_achieved}, \\text{upper\\_bound\\_achieved} ]$.\n\nAll energies and temperatures are in reduced Lennard–Jones units (dimensionless). No angles appear. There are no percentages in the output; use decimal numbers only.",
            "solution": "The problem requires the design and implementation of an algorithm to construct an optimal temperature schedule for a Parallel Tempering (or Replica Exchange) Monte Carlo simulation. The optimization goal is to minimize the number of replicas needed to span a given temperature range, from a minimum temperature $T_{\\min}$ to a maximum $T_{\\max}$, while maintaining the expected swap acceptance rate between adjacent replicas within a specified target band. The problem is well-posed and grounded in the principles of statistical mechanics and computational simulation.\n\nFirst, we establish the theoretical background. In the canonical ($NVT$) ensemble, the probability of a system at temperature $T$ having an energy $E$ is given by the Boltzmann distribution:\n$$\nP(E;T) = \\frac{1}{Z(T)} g(E) \\exp(-\\beta E)\n$$\nwhere $g(E)$ is the density of states (or degeneracy), $Z(T)$ is the canonical partition function, and $\\beta = 1/(k_{\\mathrm{B}} T)$ is the inverse temperature. For this problem, we use reduced Lennard-Jones units where the Boltzmann constant $k_{\\mathrm{B}}$ is set to $1$, so $\\beta = 1/T$.\n\nParallel Tempering enhances sampling by running multiple simulations (replicas) of the same system in parallel, each at a different temperature from a schedule $\\{T_0, T_1, \\ldots, T_{M-1}\\}$. Periodically, configurations are proposed to be swapped between replicas at adjacent temperatures, say $T_a$ and $T_b$. If replica $a$ has energy $E_a$ and replica $b$ has energy $E_b$, the swap is accepted with a Metropolis probability:\n$$\n\\alpha(E_a, E_b; \\beta_a, \\beta_b) = \\min\\left(1, \\exp\\left[(\\beta_a - \\beta_b)(E_b - E_a)\\right]\\right)\n$$\nThis maintains the detailed balance condition for the extended ensemble comprising all replicas.\n\nFor efficient sampling, the acceptance rate of these swaps should be reasonably high, but not so high that configurations do not travel far in temperature space. A common target is an acceptance rate between approximately $0.2$ and $0.4$. The expected acceptance rate, $\\mathbb{E}[\\alpha]$, is the canonical average of $\\alpha$ over the joint energy distribution $P(E_a; T_a)P(E_b; T_b)$, assuming the energies of the two replicas are independent. The problem provides a discrete approximation for this expectation, based on energy histograms obtained from simulations at each temperature:\n$$\n\\mathbb{E}[\\alpha(T_a, T_b)] \\approx \\sum_{i} \\sum_{j} p_i \\, q_j \\, \\min\\left(1, \\exp\\left[(\\beta_a - \\beta_b)(F_j - E_i)\\right]\\right)\n$$\nHere, $\\{E_i\\}$ and $\\{p_i\\}$ are the energy bin centers and normalized probabilities for the histogram at temperature $T_a$, and $\\{F_j\\}$ and $\\{q_j\\}$ are the corresponding quantities for temperature $T_b$.\n\nThe algorithmic solution involves three main stages:\n\n1.  **Energy Histogram Generation**:\n    For each candidate temperature $T_k$ provided in a test case, we must first generate a discrete energy histogram. The problem specifies that the total energy $E$ of the $N$-particle system is modeled as a Gaussian random variable. The mean and variance of this distribution are given by:\n    *   Mean: $\\bar{E}(T) = N \\cdot \\mu(T)$\n    *   Variance: $\\text{Var}(E, T) = N \\cdot \\sigma^2(T)$\n    *   Standard Deviation: $S(T) = \\sqrt{\\text{Var}(E, T)}$\n    The functions $\\mu(T)$ and $\\sigma^2(T)$ are provided for each test case.\n    A histogram with $n_{\\mathrm{bins}}$ bins is constructed over an energy range centered at the mean $\\bar{E}(T)$ and spanning $\\pm n_\\sigma$ standard deviations, i.e., $[\\bar{E}(T) - n_\\sigma S(T), \\bar{E}(T) + n_\\sigma S(T)]$. The bin centers $\\{E_i\\}$ are equally spaced within this range. The probability $p_i$ associated with each bin center $E_i$ is determined by evaluating the normal probability density function (PDF), $f(E; \\bar{E}, \\text{Var})$, at $E_i$, multiplying by the bin width $\\Delta E$, and finally normalizing the resulting probabilities so their sum is $1$.\n\n2.  **Acceptance Rate Calculation**:\n    A function is implemented to calculate $\\mathbb{E}[\\alpha(T_a, T_b)]$ using the discrete formula above. This function takes as input two energy histograms (_i.e._, the bin centers and probabilities for $T_a$ and $T_b$). The calculation is a double summation over the bins of the two histograms. This can be efficiently implemented using vectorized operations. Let $\\mathbf{p}$ be the probability vector for the histogram at $T_a$ and $\\mathbf{q}$ for $T_b$. We form a matrix of energy differences $\\Delta E_{ij} = F_j - E_i$ and a corresponding matrix of swap probabilities $\\alpha_{ij} = \\min(1, \\exp((\\beta_a - \\beta_b)\\Delta E_{ij}))$. The expected acceptance rate is then the sum of the element-wise product of the outer product of the probability vectors, $\\mathbf{p} \\otimes \\mathbf{q}$, and the acceptance matrix $\\mathbf{\\alpha}$: $\\mathbb{E}[\\alpha] = \\sum_{i,j} p_i q_j \\alpha_{ij}$.\n\n3.  **Greedy Temperature Schedule Selection**:\n    The core of the problem is a greedy algorithm to select an optimal subset of temperatures from a candidate set $\\{T_k\\}$. The schedule must start at $T_{\\min}$ (the first temperature in the set) and end at $T_{\\max}$ (the last). The goal is to minimize the number of replicas, which is achieved by taking the largest possible temperature steps, subject to the constraint that the expected acceptance rate for the step is at least $p_{\\mathrm{low}}$. The algorithm proceeds as follows:\n    a. Initialize the schedule with $T_{\\min}$. Let the index of the current temperature in the candidate list be $i_{\\text{current}}$, initially $0$.\n    b. While the current temperature is not $T_{\\max}$ (i.e., $i_{\\text{current}}  \\text{len}(\\{T_k\\}) - 1$):\n        i. Identify all possible next temperatures $T_j$ with indices $j  i_{\\text{current}}$.\n        ii. For each $T_j$, calculate the expected acceptance rate $\\mathbb{E}[\\alpha(T_{i_{\\text{current}}}, T_j)]$.\n        iii. Collect all indices $j$ for which the acceptance rate is greater than or equal to $p_{\\mathrm{low}}$ into a set of valid indices, $J_{\\text{valid}}$.\n        iv. If $J_{\\text{valid}}$ is empty, no potential next step satisfies the minimum acceptance criterion. In this case, to ensure progress, we must take the smallest possible step by selecting the immediately following temperature in the candidate list, $T_{i_{\\text{current}}+1}$. This step is flagged as failing to meet the lower bound $p_{\\mathrm{low}}$.\n        v. If $J_{\\text{valid}}$ is not empty, we select the next temperature that maximizes the step size. This corresponds to choosing the temperature with the largest index from $J_{\\text{valid}}$, i.e., $i_{\\text{next}} = \\max(J_{\\text{valid}})$. This choice directly follows the instruction to \"greedily minimize the number of replicas by choosing... the furthest next temperature\". The tie-breaking rule involving $p_{\\text{tgt}}$ is not invoked as the choice of the maximum index is unique.\n        vi. Add the chosen temperature $T_{i_{\\text{next}}}$ to the schedule, record the corresponding acceptance rate, and update $i_{\\text{current}} = i_{\\text{next}}$.\n    c. After the schedule is fully constructed, the list of recorded acceptance rates is checked to determine if all steps satisfied the upper bound, $p_{\\mathrm{high}}$.\n\nThis procedure is applied to each test case, and the results—the selected temperature schedule, the list of adjacent acceptance rates, and two booleans indicating if the lower and upper bounds were respected for all steps—are formatted as required.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Main function to run the test cases and print the results.\n    \"\"\"\n    test_cases = [\n        # Test case 1 (happy path)\n        {\n            \"N\": 64,\n            \"T_k\": [0.70, 0.78, 0.86, 0.94, 1.02],\n            \"mu_params\": {\"a0\": -6.0, \"a1\": 0.1, \"a2\": -0.5},\n            \"sigma_sq_params\": {\"c0\": 0.05, \"c1\": 0.02},\n            \"n_bins\": 201,\n            \"n_sigma\": 4.0,\n            \"p_low\": 0.20,\n            \"p_high\": 0.40,\n            \"p_tgt\": 0.30\n        },\n        # Test case 2 (boundary condition with narrow histograms)\n        {\n            \"N\": 64,\n            \"T_k\": [0.70, 0.72, 0.74, 0.76, 0.78],\n            \"mu_params\": {\"a0\": -6.0, \"a1\": 0.1, \"a2\": -0.5},\n            \"sigma_sq_params\": {\"c0\": 0.01, \"c1\": 0.0},\n            \"n_bins\": 201,\n            \"n_sigma\": 4.0,\n            \"p_low\": 0.15,\n            \"p_high\": 0.50,\n            \"p_tgt\": 0.30\n        },\n        # Test case 3 (edge case with only two temperatures)\n        {\n            \"N\": 16,\n            \"T_k\": [0.70, 1.00],\n            \"mu_params\": {\"a0\": -6.0, \"a1\": 0.1, \"a2\": -0.5},\n            \"sigma_sq_params\": {\"c0\": 0.05, \"c1\": 0.02},\n            \"n_bins\": 201,\n            \"n_sigma\": 4.0,\n            \"p_low\": 0.20,\n            \"p_high\": 0.60,\n            \"p_tgt\": 0.40\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = process_case(case)\n        # The output format requires Python's default string representation of lists\n        results.append(str(result).replace(\" \", \"\"))\n\n\n    print(f\"[{','.join(results)}]\")\n\ndef get_energy_histogram(T, N, mu_params, sigma_sq_params, n_bins, n_sigma):\n    \"\"\"\n    Generates a discrete energy histogram based on a Gaussian model.\n    \"\"\"\n    # Per-particle mean and variance functions\n    mu_T = mu_params[\"a0\"] + mu_params[\"a1\"] * T + mu_params[\"a2\"] / T\n    sigma_sq_T = sigma_sq_params.get(\"c0\", 0.0) * T + sigma_sq_params.get(\"c1\", 0.0)\n\n    # Total energy mean and variance\n    mean_E = N * mu_T\n    var_E = N * sigma_sq_T\n    std_E = np.sqrt(var_E)\n\n    # Define histogram range\n    E_min = mean_E - n_sigma * std_E\n    E_max = mean_E + n_sigma * std_E\n    \n    # Create bin centers\n    bin_centers = np.linspace(E_min, E_max, n_bins)\n    bin_width = (E_max - E_min) / (n_bins - 1) if n_bins  1 else 0\n\n    # Calculate probabilities\n    if std_E  0:\n        probabilities = norm.pdf(bin_centers, loc=mean_E, scale=std_E) * bin_width\n    else: # Delta function case if variance is zero\n        probabilities = np.zeros(n_bins)\n        center_idx = np.argmin(np.abs(bin_centers - mean_E))\n        probabilities[center_idx] = 1.0\n    \n    # Normalize probabilities\n    prob_sum = np.sum(probabilities)\n    if prob_sum  0:\n        probabilities /= prob_sum\n        \n    return bin_centers, probabilities\n\ndef calculate_acceptance_rate(hist_a, T_a, hist_b, T_b):\n    \"\"\"\n    Computes the expected swap acceptance rate between two temperatures.\n    \"\"\"\n    E_i, p_i = hist_a\n    F_j, q_j = hist_b\n    \n    beta_a = 1.0 / T_a\n    beta_b = 1.0 / T_b\n    delta_beta = beta_a - beta_b\n\n    # Outer product to create a matrix of energy differences F_j - E_i\n    delta_E_matrix = F_j[None, :] - E_i[:, None]\n    \n    # Calculate acceptance probability for each pair of energy states\n    arg = delta_beta * delta_E_matrix\n    acceptance_matrix = np.minimum(1.0, np.exp(arg))\n\n    # Calculate expected value by summing over all state pairs weighted by their probabilities\n    # p_i[:, None] * q_j[None, :] gives the joint probability matrix\n    expected_acceptance = np.sum(p_i[:, None] * q_j[None, :] * acceptance_matrix)\n    \n    return expected_acceptance\n\ndef process_case(case):\n    \"\"\"\n    Processes a single test case to generate the temperature schedule.\n    \"\"\"\n    T_k = np.array(case[\"T_k\"])\n    n_temps = len(T_k)\n\n    # Pre-compute all histograms\n    histograms = {T: get_energy_histogram(T, case[\"N\"], case[\"mu_params\"], case[\"sigma_sq_params\"], case[\"n_bins\"], case[\"n_sigma\"]) for T in T_k}\n\n    schedule = [T_k[0]]\n    acceptances = []\n    low_bound_met_flags = []\n    \n    current_idx = 0\n    while current_idx  n_temps - 1:\n        current_T = T_k[current_idx]\n        \n        valid_next_indices = []\n        for next_idx in range(current_idx + 1, n_temps):\n            next_T = T_k[next_idx]\n            acc_rate = calculate_acceptance_rate(histograms[current_T], current_T, histograms[next_T], next_T)\n            if acc_rate = case[\"p_low\"]:\n                valid_next_indices.append(next_idx)\n\n        if not valid_next_indices:\n            # If no valid step, take the smallest possible step\n            next_step_idx = current_idx + 1\n            low_bound_met_flags.append(False)\n        else:\n            # Greedily choose the \"furthest\" valid temperature (largest index)\n            next_step_idx = max(valid_next_indices)\n            low_bound_met_flags.append(True)\n            \n        next_T = T_k[next_step_idx]\n        final_acc_rate = calculate_acceptance_rate(histograms[current_T], current_T, histograms[next_T], next_T)\n        \n        schedule.append(next_T)\n        acceptances.append(final_acc_rate)\n        \n        current_idx = next_step_idx\n\n    all_low_ok = all(low_bound_met_flags)\n    all_high_ok = all(acc = case[\"p_high\"] for acc in acceptances) if acceptances else True\n    \n    return [schedule, acceptances, all_low_ok, all_high_ok]\n\n\nsolve()\n```"
        }
    ]
}