{
    "hands_on_practices": [
        {
            "introduction": "To build our intuition, we begin with the most fundamental case: a scalar, discrete-time linear system. This exercise guides you through a first-principles derivation of the minimum control energy required to steer the system to a target state, using the method of Lagrange multipliers. By analyzing the behavior of the control energy for an unstable system as the time horizon grows, you will uncover a key and somewhat counter-intuitive result in optimal control .",
            "id": "4270331",
            "problem": "Consider a discrete-time scalar Linear Time-Invariant (LTI) system with dynamics given by $x_{k+1} = a x_{k} + b u_{k}$, where $a \\in \\mathbb{R}$ with $|a| \\neq 1$, $b \\in \\mathbb{R} \\setminus \\{0\\}$, and $u_{k} \\in \\mathbb{R}$ is the control input. Suppose the initial condition is $x_{0} \\in \\mathbb{R}$ and the target terminal state at a finite horizon $N \\in \\mathbb{N}$ is $x_{N} = 0$. Among all input sequences $\\{u_{k}\\}_{k=0}^{N-1}$ that achieve $x_{N} = 0$, define the finite-horizon control energy as $E = \\sum_{k=0}^{N-1} u_{k}^{2}$.\n\nUsing only foundational principles for discrete-time linear systems, optimization with linear equality constraints, and properties of geometric series (no pre-stated control energy formulas), derive the exact minimal energy $E^{\\ast}(N)$ required to drive the system from $x_{0}$ to $x_{N} = 0$ in exactly $N$ steps. Then, analytically investigate the effect of $|a| > 1$ on $E^{\\ast}(N)$ as the horizon $N$ increases by computing the limit $E_{\\infty} = \\lim_{N \\to \\infty} E^{\\ast}(N)$ under the assumption $|a| > 1$.\n\nYour final answer must consist of two closed-form expressions presented together: first $E^{\\ast}(N)$ as an explicit function of $a$, $b$, $x_{0}$, and $N$; second the limit $E_{\\infty}$ for the case $|a| > 1$. No numerical rounding is required, and no physical units need to be specified. Express both results in exact form.",
            "solution": "The problem as stated is scientifically grounded, well-posed, objective, and self-contained. It is a fundamental problem in optimal control theory for linear systems and is therefore valid. We shall proceed with a full derivation.\n\nThe dynamics of the discrete-time scalar LTI system are given by:\n$$x_{k+1} = a x_{k} + b u_{k}$$\nwhere $x_k \\in \\mathbb{R}$ is the state, $u_k \\in \\mathbb{R}$ is the control input, and the parameters $a, b \\in \\mathbb{R}$ satisfy $|a| \\neq 1$ and $b \\neq 0$. The initial state is $x_0$ and the horizon is a finite integer $N$.\n\nFirst, we determine the state at time $N$, denoted $x_N$, by iteratively applying the state-update equation:\n$x_1 = a x_0 + b u_0$\n$x_2 = a x_1 + b u_1 = a(a x_0 + b u_0) + b u_1 = a^2 x_0 + a b u_0 + b u_1$\n$x_3 = a x_2 + b u_2 = a(a^2 x_0 + a b u_0 + b u_1) + b u_2 = a^3 x_0 + a^2 b u_0 + a b u_1 + b u_2$\n\nBy induction, the state at time $N$ is expressed as a function of the initial state $x_0$ and the sequence of control inputs $\\{u_k\\}_{k=0}^{N-1}$:\n$$x_N = a^N x_0 + \\sum_{k=0}^{N-1} a^{N-1-k} b u_k$$\n\nThe problem requires driving the system to the target state $x_N = 0$. This imposes a linear constraint on the control inputs:\n$$a^N x_0 + \\sum_{k=0}^{N-1} a^{N-1-k} b u_k = 0$$\nwhich can be rewritten as:\n$$\\sum_{k=0}^{N-1} \\left( a^{N-1-k} b \\right) u_k = -a^N x_0$$\n\nThe objective is to find the input sequence $\\{u_k\\}_{k=0}^{N-1}$ that satisfies this constraint while minimizing the control energy, defined as:\n$$E = \\sum_{k=0}^{N-1} u_k^2$$\n\nThis is a constrained optimization problem. We can solve it using the method of Lagrange multipliers. We define the Lagrangian function $\\mathcal{L}$ as:\n$$\\mathcal{L}(\\{u_k\\}, \\lambda) = \\sum_{k=0}^{N-1} u_k^2 + \\lambda \\left( \\sum_{k=0}^{N-1} a^{N-1-k} b u_k + a^N x_0 \\right)$$\nwhere $\\lambda$ is the Lagrange multiplier.\n\nTo find the minimum, we set the partial derivatives of $\\mathcal{L}$ with respect to each control input $u_j$ (for $j = 0, 1, \\dots, N-1$) to zero:\n$$\\frac{\\partial \\mathcal{L}}{\\partial u_j} = 2u_j + \\lambda \\left( a^{N-1-j} b \\right) = 0$$\nSolving for $u_j$, we find the form of the optimal control inputs, denoted $u_j^*$:\n$$u_j^* = -\\frac{\\lambda b}{2} a^{N-1-j}$$\n\nNext, we determine the value of $\\lambda$ by substituting this expression for $u_j^*$ back into the linear constraint equation:\n$$\\sum_{j=0}^{N-1} a^{N-1-j} b \\left( -\\frac{\\lambda b}{2} a^{N-1-j} \\right) = -a^N x_0$$\n$$-\\frac{\\lambda b^2}{2} \\sum_{j=0}^{N-1} a^{2(N-1-j)} = -a^N x_0$$\nLet us evaluate the summation. By changing the index of summation to $m = N-1-j$, as $j$ goes from $0$ to $N-1$, $m$ goes from $N-1$ to $0$. The sum is a geometric series:\n$$\\sum_{j=0}^{N-1} a^{2(N-1-j)} = \\sum_{m=0}^{N-1} (a^2)^m$$\nSince the problem states $|a| \\neq 1$, it follows that $a^2 \\neq 1$. Thus, we can apply the formula for the sum of a finite geometric series:\n$$\\sum_{m=0}^{N-1} (a^2)^m = \\frac{(a^2)^N - 1}{a^2 - 1} = \\frac{a^{2N} - 1}{a^2 - 1}$$\nSubstituting this back into our equation for $\\lambda$:\n$$\\frac{\\lambda b^2}{2} \\left( \\frac{a^{2N} - 1}{a^2 - 1} \\right) = a^N x_0$$\nSolving for $\\lambda$:\n$$\\lambda = \\frac{2 a^N x_0}{b^2} \\left( \\frac{a^2 - 1}{a^{2N} - 1} \\right)$$\n\nNow we can calculate the minimal control energy, $E^*(N)$, by substituting the optimal inputs $u_j^*$ into the energy-defining equation:\n$$E^*(N) = \\sum_{j=0}^{N-1} (u_j^*)^2 = \\sum_{j=0}^{N-1} \\left( -\\frac{\\lambda b}{2} a^{N-1-j} \\right)^2 = \\left( \\frac{\\lambda b}{2} \\right)^2 \\sum_{j=0}^{N-1} a^{2(N-1-j)}$$\nWe have already computed the sum, so:\n$$E^*(N) = \\left( \\frac{\\lambda b}{2} \\right)^2 \\left( \\frac{a^{2N} - 1}{a^2 - 1} \\right)$$\nSubstitute the expression for $\\lambda$:\n$$\\frac{\\lambda b}{2} = \\frac{b}{2} \\left[ \\frac{2 a^N x_0}{b^2} \\left( \\frac{a^2 - 1}{a^{2N} - 1} \\right) \\right] = \\frac{a^N x_0}{b} \\left( \\frac{a^2 - 1}{a^{2N} - 1} \\right)$$\nTherefore, the minimal energy is:\n$$E^*(N) = \\left[ \\frac{a^N x_0}{b} \\left( \\frac{a^2 - 1}{a^{2N} - 1} \\right) \\right]^2 \\left( \\frac{a^{2N} - 1}{a^2 - 1} \\right)$$\n$$E^*(N) = \\frac{a^{2N} x_0^2}{b^2} \\frac{(a^2 - 1)^2}{(a^{2N} - 1)^2} \\frac{a^{2N} - 1}{a^2 - 1}$$\nSimplifying this expression yields the first result:\n$$E^*(N) = \\frac{a^{2N} x_0^2 (a^2 - 1)}{b^2 (a^{2N} - 1)}$$\n\nThe second part of the problem is to compute the limit $E_\\infty = \\lim_{N \\to \\infty} E^*(N)$ for the case $|a| > 1$. To facilitate the limit calculation, we can rewrite $E^*(N)$ by dividing the numerator and denominator by $a^{2N}$:\n$$E^*(N) = \\frac{x_0^2 (a^2 - 1)}{b^2 (1 - a^{-2N})}$$\nNow, we take the limit as $N \\to \\infty$. Given the condition $|a| > 1$, it follows that $a^2 > 1$. Consequently, the term $a^{-2N} = (a^2)^{-N}$ approaches zero:\n$$\\lim_{N \\to \\infty} a^{-2N} = \\lim_{N \\to \\infty} \\frac{1}{(a^2)^N} = 0$$\nApplying this limit to the expression for $E^*(N)$:\n$$E_\\infty = \\lim_{N \\to \\infty} E^*(N) = \\lim_{N \\to \\infty} \\frac{x_0^2 (a^2 - 1)}{b^2 (1 - a^{-2N})} = \\frac{x_0^2 (a^2 - 1)}{b^2 (1 - 0)}$$\nThis gives the second result:\n$$E_\\infty = \\frac{x_0^2 (a^2 - 1)}{b^2}$$\nThis finite limit indicates the energy required to \"nudge\" an unstable system's trajectory onto a path that naturally converges to the origin over an infinite horizon.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{a^{2N} x_0^2 (a^2 - 1)}{b^2 (a^{2N} - 1)} & \\frac{x_0^2 (a^2 - 1)}{b^2}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "In many real-world applications, our goal is not to control the entire state of a network, but rather a specific measurement or output. This practice extends our analysis to continuous-time systems and introduces the crucial concept of output controllability. You will derive the minimum energy needed to reach a target in the output subspace and see how the standard controllability Gramian is adapted to create a reduced target metric, providing a powerful tool for focused control objectives .",
            "id": "4270307",
            "problem": "A networked system with $3$ nodes is modeled as a Linear Time-Invariant (LTI) system with state dynamics $\\dot{x}(t) = A x(t) + B u(t)$, initial condition $x(0) = 0$, and output $y(t) = C x(t)$. The control signal $u(t)$ is to be chosen to minimize the quadratic control effort $J = \\int_{0}^{T} u(t)^{\\top} u(t) \\, dt$ over a finite horizon $[0, T]$, subject to reaching a specified output target $y(T) = y_T$ in the output subspace defined by $y = C x$. Assume the matrices\n$$\nA = \\begin{pmatrix} -1 & 0 & 0 \\\\ 0 & -2 & 0 \\\\ 0 & 0 & -3 \\end{pmatrix}, \\quad B = I_{3}, \\quad C = \\begin{pmatrix} 2 & -1 & 1 \\end{pmatrix},\n$$\nwhere $I_{3}$ is the $3 \\times 3$ identity matrix. \n\nStarting from first principles, derive the expression for the minimum control energy required to satisfy the terminal output constraint $y(T) = y_T$, and show that the relevant reduced target metric in the output space is given by $C W_{c}(T) C^{\\top}$, where the finite-horizon controllability Gramian $W_{c}(T)$ is defined by \n$$\nW_{c}(T) = \\int_{0}^{T} \\exp(A t) B B^{\\top} \\exp(A^{\\top} t) \\, dt.\n$$\nCompute $C W_{c}(T) C^{\\top}$ explicitly for the given $A$, $B$, and $C$, and express the minimum control energy as a closed-form analytic expression in terms of $T$ and $y_T$. No rounding is required. Express the final answer without units.",
            "solution": "The problem as stated constitutes a well-posed, scientifically grounded problem in the field of optimal control for linear systems. All necessary data and definitions are provided, and there are no internal contradictions or violations of physical or mathematical principles. I will therefore proceed with a formal solution.\n\nThe problem is to find the minimum control energy, defined by the cost functional $J = \\int_{0}^{T} u(t)^{\\top} u(t) \\, dt$, required to drive a linear time-invariant (LTI) system from a specified initial state to a specified terminal output.\n\nThe system dynamics are given by:\n$$\n\\dot{x}(t) = A x(t) + B u(t)\n$$\nwith initial condition $x(0) = 0$ and output $y(t) = C x(t)$. The goal is to find a control input $u(t)$ on the interval $[0, T]$ that minimizes $J$ while satisfying the terminal constraint $y(T) = y_T$.\n\nFirst, we solve the state equation. The solution to the LTI system equation is given by the variation of constants formula:\n$$\nx(t) = \\exp(At) x(0) + \\int_{0}^{t} \\exp(A(t - \\tau)) B u(\\tau) \\, d\\tau\n$$\nApplying the initial condition $x(0) = 0$, the state at the final time $T$ is:\n$$\nx(T) = \\int_{0}^{T} \\exp(A(T - \\tau)) B u(\\tau) \\, d\\tau\n$$\nThe output constraint is $y(T) = C x(T) = y_T$. Substituting the expression for $x(T)$, we obtain the constraint on the control input $u(t)$:\n$$\ny_T = C \\int_{0}^{T} \\exp(A(T - \\tau)) B u(\\tau) \\, d\\tau\n$$\nThis equation can be viewed as a linear operator equation $y_T = \\mathcal{L}(u)$, where the operator $\\mathcal{L}$ maps a function $u \\in L_2([0, T], \\mathbb{R}^3)$ to a vector $y_T \\in \\mathbb{R}$. The problem is to find a function $u(t)$ that satisfies this constraint and has the minimum $L_2$ norm, where $\\|u\\|_{L_2}^2 = \\int_{0}^{T} u(\\tau)^{\\top} u(\\tau) \\, d\\tau = J$.\n\nThis is a standard minimum-norm problem in a Hilbert space. The solution is given by $u_{opt} = \\mathcal{L}^*(\\mathcal{L}\\mathcal{L}^*)^{-1}y_T$, where $\\mathcal{L}^*$ is the adjoint of the operator $\\mathcal{L}$. We determine the adjoint $\\mathcal{L}^*$ from its defining property: $\\langle v, \\mathcal{L}(u) \\rangle_{\\mathbb{R}} = \\langle \\mathcal{L}^*(v), u \\rangle_{L_2}$ for any $v \\in \\mathbb{R}$ and $u \\in L_2([0, T], \\mathbb{R}^3)$.\n\nThe left-hand side is:\n$$\n\\langle v, \\mathcal{L}(u) \\rangle = v^{\\top} y_T = v^{\\top} C \\int_{0}^{T} \\exp(A(T - \\tau)) B u(\\tau) \\, d\\tau = \\int_{0}^{T} \\left( v^{\\top} C \\exp(A(T - \\tau)) B \\right) u(\\tau) \\, d\\tau\n$$\nThe right-hand side is:\n$$\n\\langle \\mathcal{L}^*(v), u \\rangle = \\int_{0}^{T} (\\mathcal{L}^*(v))(\\tau)^{\\top} u(\\tau) \\, d\\tau\n$$\nBy comparing the integrands, we identify the action of the adjoint operator:\n$$\n(\\mathcal{L}^*(v))(\\tau)^{\\top} = v^{\\top} C \\exp(A(T - \\tau)) B\n$$\nTaking the transpose of both sides, we get:\n$$\n(\\mathcal{L}^*(v))(\\tau) = B^{\\top} \\exp(A^{\\top}(T - \\tau)) C^{\\top} v\n$$\nNext, we compute the composite operator $\\mathcal{L}\\mathcal{L}^*$, which maps $\\mathbb{R}$ to $\\mathbb{R}$:\n$$\n\\mathcal{L}\\mathcal{L}^*(v) = \\mathcal{L}((\\mathcal{L}^*(v))(\\cdot)) = C \\int_{0}^{T} \\exp(A(T - \\tau)) B \\left( B^{\\top} \\exp(A^{\\top}(T - \\tau)) C^{\\top} v \\right) \\, d\\tau\n$$\n$$\n\\mathcal{L}\\mathcal{L}^*(v) = \\left( C \\left[ \\int_{0}^{T} \\exp(A(T - \\tau)) B B^{\\top} \\exp(A^{\\top}(T - \\tau)) \\, d\\tau \\right] C^{\\top} \\right) v\n$$\nWe perform a change of variable in the integral, letting $t = T - \\tau$. Then $dt = -d\\tau$. The integration limits change from $\\tau=0 \\to t=T$ and $\\tau=T \\to t=0$.\n$$\n\\int_{T}^{0} \\exp(At) B B^{\\top} \\exp(A^{\\top} t) \\, (-dt) = \\int_{0}^{T} \\exp(At) B B^{\\top} \\exp(A^{\\top} t) \\, dt\n$$\nThis integral is precisely the definition of the finite-horizon controllability Gramian, $W_c(T)$. Therefore:\n$$\n\\mathcal{L}\\mathcal{L}^* = C W_c(T) C^{\\top}\n$$\nThe optimal control input is then:\n$$\nu_{opt}(\\tau) = \\mathcal{L}^* ((\\mathcal{L}\\mathcal{L}^*)^{-1} y_T) = B^{\\top} \\exp(A^{\\top}(T - \\tau)) C^{\\top} (C W_c(T) C^{\\top})^{-1} y_T\n$$\nThe minimum control energy $J_{min}$ is the squared norm of $u_{opt}$:\n$$\nJ_{min} = \\langle u_{opt}, u_{opt} \\rangle_{L_2} = \\langle \\mathcal{L}^* ((\\mathcal{L}\\mathcal{L}^*)^{-1} y_T), \\mathcal{L}^* ((\\mathcal{L}\\mathcal{L}^*)^{-1} y_T) \\rangle_{L_2}\n$$\nUsing the property of the adjoint operator, this becomes:\n$$\nJ_{min} = \\langle (\\mathcal{L}\\mathcal{L}^*)^{-1} y_T, \\mathcal{L}\\mathcal{L}^* ((\\mathcal{L}\\mathcal{L}^*)^{-1} y_T) \\rangle_{\\mathbb{R}} = \\langle (\\mathcal{L}\\mathcal{L}^*)^{-1} y_T, y_T \\rangle_{\\mathbb{R}}\n$$\nSince $y_T$ is a scalar (a $1 \\times 1$ vector), its transpose is itself, and the inner product is standard multiplication. Substituting $\\mathcal{L}\\mathcal{L}^* = C W_c(T) C^{\\top}$, we arrive at the expression for the minimum energy:\n$$\nJ_{min} = y_T^{\\top} (C W_c(T) C^{\\top})^{-1} y_T\n$$\nThis derivation shows that the matrix $C W_c(T) C^{\\top}$ is the critical metric that determines the control energy required to reach a target in the output space.\n\nNow, we compute this metric for the given system matrices:\n$$\nA = \\begin{pmatrix} -1 & 0 & 0 \\\\ 0 & -2 & 0 \\\\ 0 & 0 & -3 \\end{pmatrix}, \\quad B = I_{3}, \\quad C = \\begin{pmatrix} 2 & -1 & 1 \\end{pmatrix}\n$$\nSince $A$ is diagonal, its exponential is straightforward:\n$$\n\\exp(At) = \\begin{pmatrix} \\exp(-t) & 0 & 0 \\\\ 0 & \\exp(-2t) & 0 \\\\ 0 & 0 & \\exp(-3t) \\end{pmatrix}\n$$\nAlso, $A$ is symmetric, so $A^{\\top} = A$. And $B = I_3$, so $B B^{\\top} = I_3 I_3^{\\top} = I_3$.\nThe controllability Gramian is:\n$$\nW_{c}(T) = \\int_{0}^{T} \\exp(At) I_3 \\exp(At) \\, dt = \\int_{0}^{T} \\exp(2At) \\, dt\n$$\nThe matrix $\\exp(2At)$ is:\n$$\n\\exp(2At) = \\begin{pmatrix} \\exp(-2t) & 0 & 0 \\\\ 0 & \\exp(-4t) & 0 \\\\ 0 & 0 & \\exp(-6t) \\end{pmatrix}\n$$\nWe integrate this matrix element-wise from $0$ to $T$:\n$$\n(W_{c}(T))_{11} = \\int_{0}^{T} \\exp(-2t) dt = \\left[-\\frac{1}{2}\\exp(-2t)\\right]_{0}^{T} = \\frac{1}{2}(1 - \\exp(-2T))\n$$\n$$\n(W_{c}(T))_{22} = \\int_{0}^{T} \\exp(-4t) dt = \\left[-\\frac{1}{4}\\exp(-4t)\\right]_{0}^{T} = \\frac{1}{4}(1 - \\exp(-4T))\n$$\n$$\n(W_{c}(T))_{33} = \\int_{0}^{T} \\exp(-6t) dt = \\left[-\\frac{1}{6}\\exp(-6t)\\right]_{0}^{T} = \\frac{1}{6}(1 - \\exp(-6T))\n$$\nThe off-diagonal elements are zero. Thus, $W_c(T)$ is a diagonal matrix:\n$$\nW_{c}(T) = \\begin{pmatrix} \\frac{1}{2}(1 - \\exp(-2T)) & 0 & 0 \\\\ 0 & \\frac{1}{4}(1 - \\exp(-4T)) & 0 \\\\ 0 & 0 & \\frac{1}{6}(1 - \\exp(-6T)) \\end{pmatrix}\n$$\nNow we compute the scalar quantity $C W_c(T) C^{\\top}$:\n$$\nC W_c(T) C^{\\top} = \\begin{pmatrix} 2 & -1 & 1 \\end{pmatrix} W_c(T) \\begin{pmatrix} 2 \\\\ -1 \\\\ 1 \\end{pmatrix}\n$$\nSince $W_c(T)$ is diagonal, this is $\\sum_{i=1}^{3} C_{i}^2 (W_c(T))_{ii}$:\n$$\nC W_c(T) C^{\\top} = (2)^2 \\left( \\frac{1}{2}(1 - \\exp(-2T)) \\right) + (-1)^2 \\left( \\frac{1}{4}(1 - \\exp(-4T)) \\right) + (1)^2 \\left( \\frac{1}{6}(1 - \\exp(-6T)) \\right)\n$$\n$$\nC W_c(T) C^{\\top} = 4 \\cdot \\frac{1}{2}(1 - \\exp(-2T)) + 1 \\cdot \\frac{1}{4}(1 - \\exp(-4T)) + 1 \\cdot \\frac{1}{6}(1 - \\exp(-6T))\n$$\n$$\nC W_c(T) C^{\\top} = 2(1 - \\exp(-2T)) + \\frac{1}{4}(1 - \\exp(-4T)) + \\frac{1}{6}(1 - \\exp(-6T))\n$$\n$$\nC W_c(T) C^{\\top} = \\left(2 + \\frac{1}{4} + \\frac{1}{6}\\right) - 2\\exp(-2T) - \\frac{1}{4}\\exp(-4T) - \\frac{1}{6}\\exp(-6T)\n$$\nThe constant term is $2 + \\frac{1}{4} + \\frac{1}{6} = \\frac{24}{12} + \\frac{3}{12} + \\frac{2}{12} = \\frac{29}{12}$.\nSo, the explicit expression for the metric is:\n$$\nC W_c(T) C^{\\top} = \\frac{29}{12} - 2\\exp(-2T) - \\frac{1}{4}\\exp(-4T) - \\frac{1}{6}\\exp(-6T)\n$$\nFinally, the minimum control energy is $J_{min} = y_T^2 (C W_c(T) C^{\\top})^{-1}$. Since $y_T$ is a scalar:\n$$\nJ_{min} = \\frac{y_T^2}{\\frac{29}{12} - 2\\exp(-2T) - \\frac{1}{4}\\exp(-4T) - \\frac{1}{6}\\exp(-6T)}\n$$\nTo simplify the expression, we can multiply the numerator and denominator by $12$:\n$$\nJ_{min} = \\frac{12 y_T^2}{29 - 24\\exp(-2T) - 3\\exp(-4T) - 2\\exp(-6T)}\n$$\nThis is the final closed-form analytic expression for the minimum control energy.",
            "answer": "$$\n\\boxed{\\frac{12 y_T^2}{29 - 24\\exp(-2T) - 3\\exp(-4T) - 2\\exp(-6T)}}\n$$"
        },
        {
            "introduction": "While linear models provide an essential foundation, most real-world networks exhibit nonlinear dynamics. This final practice confronts this complexity by comparing a linear control approximation to a true nonlinear optimal control solution. You will apply Pontryagin's Minimum Principle to set up the necessary conditions for optimality and implement a numerical shooting method to compute the exact control energy, offering a hands-on look at the limitations of linearization and the power of computational optimal control .",
            "id": "4270303",
            "problem": "Consider the scalar controlled dynamical system with a cubic nonlinearity given by the ordinary differential equation (ODE) $ \\dot{x}(t) = a x(t) + \\alpha x(t)^3 + b u(t) $, where $ x(t) \\in \\mathbb{R} $ is the state, $ u(t) \\in \\mathbb{R} $ is the control input, $ a \\in \\mathbb{R} $ is the linear drift coefficient, $ \\alpha \\in \\mathbb{R} $ is the cubic nonlinearity coefficient, and $ b \\in \\mathbb{R} $ is the input gain. The objective is to drive the system from an initial state $ x(0) = x_0 $ to a terminal state $ x(T) = x_f $ over a fixed time horizon $ T > 0 $ while minimizing the control energy defined by $ J = \\int_0^T u(t)^2 \\, dt $.\n\nYou will compare two notions of minimal energy:\n- The minimal energy computed by linearization: linearize the system by neglecting the cubic term, yielding the linear time-invariant (LTI) system $ \\dot{x}(t) = a x(t) + b u(t) $, and compute the minimal energy required to drive the state from $ x_0 $ to $ x_f $ over the interval $ [0, T] $.\n- The exact nonlinear optimal control energy: compute the minimal energy for the nonlinear system $ \\dot{x}(t) = a x(t) + \\alpha x(t)^3 + b u(t) $ using first principles from optimal control.\n\nBase your derivations and algorithm on fundamental principles:\n- For the LTI case, use the solution representation of linear systems and the concept of the controllability Gramian to determine the minimal energy subject to endpoint constraints.\n- For the nonlinear case, use Pontryagin's Minimum Principle (PMP), constructing the Hamiltonian $ H(x,u,p) $ and deriving the optimality conditions. Pontryagin's Minimum Principle (PMP) states that for minimal $ J = \\int_0^T u(t)^2 \\, dt $ subject to the system dynamics and fixed endpoints, the optimal control minimizes the Hamiltonian $ H(x,u,p) = u^2 + p \\left(a x + \\alpha x^3 + b u\\right) $ pointwise in time, where $ p(t) $ is the costate. The necessary conditions include $ \\dot{x}(t) = \\frac{\\partial H}{\\partial p} $, $ \\dot{p}(t) = -\\frac{\\partial H}{\\partial x} $, and $ \\frac{\\partial H}{\\partial u} = 0 $.\n\nYour program must implement the following tasks:\n1. For the linearized system $ \\dot{x}(t) = a x(t) + b u(t) $, compute the exact minimal energy $ J_{\\text{lin}} $ needed to drive the system from $ x_0 $ to $ x_f $ in time $ T $.\n2. For the nonlinear system $ \\dot{x}(t) = a x(t) + \\alpha x(t)^3 + b u(t) $, compute the exact minimal energy $ J_{\\text{nl}} $ via Pontryagin's Minimum Principle and a numerical two-point boundary value approach (for a scalar system, this reduces to a shooting method on the costate initial value).\n3. Quantify the approximation error of the linearization by reporting the relative error $ \\varepsilon = \\left|J_{\\text{lin}} - J_{\\text{nl}}\\right| / J_{\\text{nl}} $.\n\nAll computations are purely mathematical; no physical units are involved. Angles are not used.\n\nTest Suite:\nUse the following parameter sets $ (a, b, \\alpha, x_0, x_f, T) $:\n- Case 1 (general, stable linear drift, moderate nonlinearity): $ a = -0.2 $, $ b = 1.0 $, $ \\alpha = 0.1 $, $ x_0 = 0.2 $, $ x_f = -0.2 $, $ T = 2.0 $.\n- Case 2 (boundary case $ a = 0 $): $ a = 0.0 $, $ b = 1.5 $, $ \\alpha = 0.05 $, $ x_0 = 0.0 $, $ x_f = 0.1 $, $ T = 1.0 $.\n- Case 3 (unstable linear drift with stabilizing cubic nonlinearity): $ a = 0.1 $, $ b = 2.0 $, $ \\alpha = -0.2 $, $ x_0 = 0.1 $, $ x_f = 0.0 $, $ T = 1.5 $.\n- Case 4 (longer horizon, stronger nonlinearity): $ a = -0.05 $, $ b = 1.0 $, $ \\alpha = 0.2 $, $ x_0 = -0.3 $, $ x_f = 0.3 $, $ T = 3.0 $.\n- Case 5 (near-linear regime): $ a = 0.0 $, $ b = 1.0 $, $ \\alpha = 10^{-3} $, $ x_0 = 0.5 $, $ x_f = -0.5 $, $ T = 1.0 $.\n\nOutput Specification:\nYour program should produce a single line of output containing the results for all five cases as a comma-separated list enclosed in square brackets, where each element is itself a list $ [J_{\\text{lin}}, J_{\\text{nl}}, \\varepsilon] $ of three real numbers in decimal notation. For example, the output format must be like $ [[J_{\\text{lin},1},J_{\\text{nl},1},\\varepsilon_1],[J_{\\text{lin},2},J_{\\text{nl},2},\\varepsilon_2],\\dots] $.\n\nYour implementation must be self-contained and require no user input.",
            "solution": "The problem is one of optimal control, where the objective is to find a control input $u(t)$ that steers a dynamical system from an initial state to a final state over a fixed time horizon while minimizing a cost functional, in this case, the control energy $J = \\int_0^T u(t)^2 \\, dt$. We are asked to compare the minimum energy for a nonlinear system with that of its linearization.\n\nThe problem is valid as it is scientifically grounded in optimal control theory, mathematically well-posed, and all definitions and parameters are provided, allowing for a unique and meaningful solution to be computed.\n\nWe will first derive the analytical solution for the minimal energy of the linearized system. Subsequently, we will apply Pontryagin's Minimum Principle (PMP) to the nonlinear system to derive the necessary optimality conditions, which form a two-point boundary value problem (TPBVP). This TPBVP will be solved numerically using a shooting method to find the exact minimal energy for the nonlinear system.\n\n### Part 1: Minimal Energy for the Linearized System ($J_{\\text{lin}}$)\n\nThe given nonlinear system is\n$$\n\\dot{x}(t) = a x(t) + \\alpha x(t)^3 + b u(t)\n$$\nBy neglecting the cubic term ($\\alpha x(t)^3$), we obtain the linearized linear time-invariant (LTI) system:\n$$\n\\dot{x}(t) = a x(t) + b u(t)\n$$\nwith boundary conditions $x(0) = x_0$ and $x(T) = x_f$. The goal is to minimize the control energy $J = \\int_0^T u(t)^2 \\, dt$.\n\nThe solution to the LTI system at time $t=T$ is given by the variation of constants formula:\n$$\nx(T) = x_f = e^{aT} x_0 + \\int_0^T e^{a(T-\\tau)} b u(\\tau) \\, d\\tau\n$$\nRearranging gives the constraint on the control input:\n$$\nx_f - e^{aT} x_0 = \\int_0^T e^{a(T-\\tau)} b u(\\tau) \\, d\\tau\n$$\nThe problem is to find the function $u(t)$ with the minimum $L_2$ norm that satisfies this integral constraint. The standard result from control theory gives the minimal energy in terms of the controllability Gramian. For this scalar system, the Gramian is a scalar:\n$$\nW(T) = \\int_0^T \\left(e^{a\\tau} b\\right) \\left(e^{a\\tau} b\\right)^T \\, d\\tau = b^2 \\int_0^T e^{2a\\tau} \\, d\\tau\n$$\nWe evaluate the integral based on the value of $a$:\n1.  If $a \\neq 0$:\n    $$\n    W(T) = b^2 \\left[ \\frac{e^{2a\\tau}}{2a} \\right]_0^T = b^2 \\frac{e^{2aT} - 1}{2a}\n    $$\n2.  If $a = 0$:\n    $$\n    W(T) = b^2 \\int_0^T 1 \\, d\\tau = b^2 T\n    $$\nThe minimum control energy for the LTI system, $J_{\\text{lin}}$, is given by the formula:\n$$\nJ_{\\text{lin}} = \\left(x_f - e^{aT}x_0\\right)^T W(T)^{-1} \\left(x_f - e^{aT}x_0\\right) = \\frac{\\left(x_f - e^{aT}x_0\\right)^2}{W(T)}\n$$\nAssuming the system is controllable (i.e., $b \\neq 0$ and $T > 0$), $W(T)$ is positive and the energy is well-defined.\n\n### Part 2: Minimal Energy for the Nonlinear System ($J_{\\text{nl}}$)\n\nWe now consider the original nonlinear system:\n$$\n\\dot{x}(t) = a x(t) + \\alpha x(t)^3 + b u(t)\n$$\nwith $x(0) = x_0$, $x(T) = x_f$, and cost $J = \\int_0^T u(t)^2 \\, dt$. We apply Pontryagin's Minimum Principle (PMP). First, we form the Hamiltonian $H(x, u, p)$, where $p(t)$ is the costate variable:\n$$\nH(x, u, p) = u(t)^2 + p(t) \\left(a x(t) + \\alpha x(t)^3 + b u(t)\\right)\n$$\nThe necessary conditions for optimality are:\n1.  **State Equation:** $\\dot{x}(t) = \\frac{\\partial H}{\\partial p} = a x(t) + \\alpha x(t)^3 + b u(t)$\n2.  **Costate Equation:** $\\dot{p}(t) = -\\frac{\\partial H}{\\partial x} = -p(t) \\left(a + 3\\alpha x(t)^2\\right)$\n3.  **Optimality Condition:** $\\frac{\\partial H}{\\partial u} = 2u(t) + p(t)b = 0$\n\nFrom the optimality condition, we find the expression for the optimal control $u^*(t)$ in terms of the costate $p(t)$:\n$$\nu^*(t) = -\\frac{b}{2} p(t)\n$$\nSubstituting this optimal control into the state equation yields a system of two coupled first-order ordinary differential equations (ODEs) for the state $x(t)$ and costate $p(t)$:\n$$\n\\begin{cases}\n\\dot{x}(t) = a x(t) + \\alpha x(t)^3 - \\frac{b^2}{2} p(t) \\\\\n\\dot{p}(t) = -p(t) \\left(a + 3\\alpha x(t)^2\\right)\n\\end{cases}\n$$\nThese ODEs are subject to the split boundary conditions $x(0) = x_0$ and $x(T) = x_f$. This constitutes a two-point boundary value problem (TPBVP).\n\nTo solve this TPBVP, we employ a numerical shooting method. We treat the unknown initial costate $p(0) = p_0$ as a parameter to be found. For a given guess of $p_0$, we can solve the system of ODEs as an initial value problem (IVP) with initial conditions $(x(0), p(0)) = (x_0, p_0)$. Integrating from $t=0$ to $t=T$ yields a terminal state $x(T; p_0)$. The correct value of $p_0$ is the one that satisfies the terminal constraint, i.e., $x(T; p_0) = x_f$. We can define an error function $E(p_0) = x(T; p_0) - x_f$ and use a numerical root-finding algorithm to find $p_0^*$ such that $E(p_0^*) = 0$.\n\nOnce the optimal initial costate $p_0^*$ is found, the system is solved one last time as an IVP to obtain the full optimal state and costate trajectories, $x^*(t)$ and $p^*(t)$. The minimum control energy for the nonlinear system, $J_{\\text{nl}}$, is then computed by integrating the square of the optimal control:\n$$\nJ_{\\text{nl}} = \\int_0^T \\left(u^*(t)\\right)^2 \\, dt = \\int_0^T \\left(-\\frac{b}{2} p^*(t)\\right)^2 \\, dt = \\frac{b^2}{4} \\int_0^T \\left(p^*(t)\\right)^2 \\, dt\n$$\nThis final integral is computed numerically using the obtained costate trajectory.\n\n### Part 3: Relative Error Calculation\n\nThe relative error $\\varepsilon$ of the linearization approach is quantified as specified:\n$$\n\\varepsilon = \\frac{\\left|J_{\\text{lin}} - J_{\\text{nl}}\\right|}{J_{\\text{nl}}}\n$$\nThis metric indicates how well the energy computed from the simplified linear model approximates the true optimal energy for the nonlinear system. A small value of $\\varepsilon$ suggests that linearization is a reasonable approximation for the given parameters and state transition, which is expected when the nonlinearity coefficient $\\alpha$ is small or the state trajectory $x(t)$ remains close to the origin.",
            "answer": "```python\nimport numpy as np\nfrom scipy.integrate import solve_ivp, simpson\nfrom scipy.optimize import root_scalar\n\ndef solve():\n    \"\"\"\n    Solves the optimal control problem for five test cases, calculating\n    linearized energy, nonlinear energy, and the relative error.\n    \"\"\"\n    test_cases = [\n        # (a, b, alpha, x0, xf, T)\n        (-0.2, 1.0, 0.1, 0.2, -0.2, 2.0),\n        (0.0, 1.5, 0.05, 0.0, 0.1, 1.0),\n        (0.1, 2.0, -0.2, 0.1, 0.0, 1.5),\n        (-0.05, 1.0, 0.2, -0.3, 0.3, 3.0),\n        (0.0, 1.0, 1e-3, 0.5, -0.5, 1.0),\n    ]\n\n    results = []\n    for case in test_cases:\n        a, b, alpha, x0, xf, T = case\n\n        # Task 1: Compute minimal energy for the linearized system (J_lin)\n        if np.abs(a) < 1e-9:  # Handle the a=0 case numerically\n            gramian = b**2 * T\n        else:\n            gramian = b**2 * (np.exp(2 * a * T) - 1) / (2 * a)\n        \n        if np.abs(gramian) < 1e-12:\n            # This case corresponds to an uncontrollable system (b=0 or T=0),\n            # which is not in the test suite.\n            j_lin = np.inf\n        else:\n            numerator = (xf - x0 * np.exp(a * T))**2\n            j_lin = numerator / gramian\n\n        # Task 2: Compute minimal energy for the nonlinear system (J_nl)\n        \n        # Define the coupled ODE system for state (x) and costate (p)\n        def ode_system(t, y, a, b, alpha):\n            x, p = y\n            x_dot = a * x + alpha * x**3 - (b**2 / 2.0) * p\n            p_dot = -(a + 3.0 * alpha * x**2) * p\n            return [x_dot, p_dot]\n\n        # Define the objective function for the shooting method.\n        # Its root is the p0 that makes x(T) = xf.\n        def objective(p0, a, b, alpha, x0, xf, T):\n            y0 = [x0, p0]\n            # Integrate the ODEs from t=0 to t=T\n            sol = solve_ivp(\n                ode_system, [0, T], y0, args=(a, b, alpha),\n                rtol=1e-9, atol=1e-9\n            )\n            # Return the difference between the final state and the target state\n            x_T = sol.y[0, -1]\n            return x_T - xf\n\n        # Use a root-finding algorithm to find the optimal initial costate p0.\n        # We use brentq, which requires a bracket [p_min, p_max] where\n        # objective(p_min) and objective(p_max) have opposite signs.\n        # Heuristically, a wide bracket should work.\n        p0_opt = 0.0\n        try:\n            bracket = [-100.0, 100.0]\n            if np.sign(objective(bracket[0], a, b, alpha, x0, xf, T)) == \\\n               np.sign(objective(bracket[1], a, b, alpha, x0, xf, T)):\n                raise ValueError(\"The function values at the bracket endpoints have the same sign.\")\n            \n            res = root_scalar(\n                objective, args=(a, b, alpha, x0, xf, T),\n                bracket=bracket, method='brentq', xtol=1e-12, rtol=1e-12\n            )\n            p0_opt = res.root\n        except ValueError:\n            # Fallback if brentq bracket fails: use a secant method with an initial\n            # guess from the linear system's optimal control.\n            p0_lin_est = -2.0 * (1.0/gramian) * (xf - np.exp(a*T) * x0) * np.exp(a*T)\n            res = root_scalar(\n                objective, args=(a, b, alpha, x0, xf, T),\n                x0=p0_lin_est, x1=p0_lin_est + 0.1, method='secant', xtol=1e-12, rtol=1e-12\n            )\n            if not res.converged:\n                raise RuntimeError(f\"Shooting method failed to converge for case {case}\")\n            p0_opt = res.root\n\n        # With the optimal p0_opt, solve the IVP one last time for the full trajectory\n        y0_opt = [x0, p0_opt]\n        sol_opt = solve_ivp(\n            ode_system, [0, T], y0_opt, args=(a, b, alpha),\n            dense_output=True, rtol=1e-9, atol=1e-9\n        )\n\n        # Compute the nonlinear control energy J_nl by integrating (b/2 * p(t))^2\n        t_eval = np.linspace(0, T, 1000)\n        p_traj = sol_opt.sol(t_eval)[1]\n        integrand = (b / 2.0 * p_traj)**2\n        j_nl = simpson(integrand, t_eval)\n\n        # Task 3: Quantify the relative error\n        if np.abs(j_nl) < 1e-12:\n            epsilon = np.inf if np.abs(j_lin) > 1e-12 else 0.0\n        else:\n            epsilon = np.abs(j_lin - j_nl) / j_nl\n        \n        results.append([j_lin, j_nl, epsilon])\n\n    # Format the final output string as specified\n    output_str = \"[\" + \",\".join(map(str, results)) + \"]\"\n    print(output_str)\n\nsolve()\n```"
        }
    ]
}