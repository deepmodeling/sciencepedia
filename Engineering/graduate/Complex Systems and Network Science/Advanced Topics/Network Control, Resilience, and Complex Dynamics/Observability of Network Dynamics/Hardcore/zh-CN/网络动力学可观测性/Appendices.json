{
    "hands_on_practices": [
        {
            "introduction": "本练习将探讨结构可观测性，这是一个与动力学参数无关的基本网络属性。通过构建与系统图相对应的二分图，并找到其最大匹配，我们可以从图论的角度确定保证系统可观测性所需的最少传感器数量。这个练习将帮助你掌握一种强大的、用于传感器布局设计的图形化方法。()",
            "id": "4293873",
            "problem": "考虑一个线性时不变（LTI）网络化动态系统，其状态向量为 $x \\in \\mathbb{R}^{6}$，动态特性为 $\\dot{x} = A x$，其中系统矩阵 $A$ 的结构受限于一个顶点集为 $\\{1,2,3,4,5,6\\}$ 的有向无环图（DAG）。该图具有有向边 $1 \\to 2$、$1 \\to 3$、$2 \\to 4$、$3 \\to 4$、$4 \\to 5$ 和 $4 \\to 6$。专用传感器定义为直接测量单个状态分量的输出，即 $y = C x$，其中 $C$ 的每个非零行选择 $x$ 的一个坐标。\n\n从结构能观性的基本定义以及与转置矩阵 $A^{\\top}$ 相关的系统二分图的图论构造出发，解释为什么最大匹配中的未匹配顶点决定了使系统实现结构能观性所需的专用传感器的数量。然后，构建由给定DAG导出的二分图，并使用最大匹配来计算确保系统结构能观性所需的最小专用传感器数量。请以单个整数形式提供您的最终答案。",
            "solution": "该问题要求解释结构能观性与二分图匹配之间的关系，然后计算给定网络化系统所需的最小专用传感器数量。\n\n首先，我们阐述其理论基础。对于能观性分析，由 $\\dot{x} = Ax + Bu$ 和 $y=Cx$ 描述的线性时不变（LTI）系统由矩阵对 $(A, C)$ 定义。当且仅当能观性矩阵 $\\mathcal{O}$ 具有满列秩时，系统是能观的，其中 $n$ 是状态空间的维度。能观性矩阵由下式给出：\n$$\n\\mathcal{O} = \\begin{pmatrix} C \\\\ CA \\\\ CA^2 \\\\ \\vdots \\\\ CA^{n-1} \\end{pmatrix}\n$$\n一个等价的能观性条件是 Popov-Belevitch-Hautus (PBH) 测试，该测试指出，当且仅当矩阵\n$$\n\\begin{pmatrix} A - \\lambda I \\\\ C \\end{pmatrix}\n$$\n对于所有复数 $\\lambda \\in \\mathbb{C}$，其秩为 $n$。\n\n对于一个*结构化*系统，矩阵 $A$ 和 $C$ 的元素要么是固定的零，要么是独立的自由参数。结构能观性要求系统对于这些自由参数的几乎所有数值都是能观的。这等价于结构化矩阵 $\\begin{pmatrix} A \\\\ C \\end{pmatrix}$ 的*泛型秩*等于 $n$ 的条件。\n\n一个结构化矩阵的泛型秩由其关联二分图中的最大匹配的大小决定。设系统有 $n$ 个状态和 $p$ 个输出。结构化矩阵 $M = \\begin{pmatrix} A \\\\ C \\end{pmatrix}$ 的维度是 $(n+p) \\times n$。我们可以构造一个二分图 $G_M = (V_{in}, V_{out}, E)$，其中输入顶点集 $V_{in} = \\{x_1, \\dots, x_n\\}$ 对应于 $M$ 的列，输出顶点集 $V_{out} = \\{\\dot{x}_1, \\dots, \\dot{x}_n, y_1, \\dots, y_p\\}$ 对应于 $M$ 的行。如果对应的元素 $M_{ij}$ 非零，则存在一条从顶点 $x_j \\in V_{in}$ 到顶点 $v_i \\in V_{out}$ 的边。$M$ 的泛型秩等于 $G_M$ 中最大匹配的大小，记为 $\\mu(G_M)$。因此，当且仅当 $\\mu(G_M) = n$ 时，系统 $(A, C)$ 是结构能观的。这意味着在最大匹配中，每个输入顶点 $x_j$ 都必须与某个输出顶点匹配。\n\n问题要求从没有传感器的系统开始，该系统由矩阵 $A$ 描述。我们可以为动态矩阵 $A$ 构造一个二分图 $G_A = (V_x, V_{\\dot{x}}, E_A)$，其中 $V_x = \\{x_1, \\dots, x_n\\}$ 且 $V_{\\dot{x}} = \\{\\dot{x}_1, \\dots, \\dot{x}_n\\}$。如果 $A_{ij}$ 是一个自由参数（非零），则存在一条边 $(x_j, \\dot{x}_i)$。设此图中的最大匹配大小为 $\\mu(G_A)$。根据定义，$\\mu(G_A) \\le n$。如果 $\\mu(G_A)  n$，则该系统在没有传感器的情况下不是结构能观的。在最大匹配中，$V_x$ 中未匹配的顶点数量为 $n - \\mu(G_A)$。\n\n为了实现结构能观性，我们必须添加传感器。在状态 $x_k$ 上的专用传感器对应于一个输出 $y_k = x_k$。这会在输出矩阵 $C$ 中引入一个新行，该行在第 $k$ 列有一个唯一的非零元素。在二分图框架中，这增加了一个新的输出顶点 $y_k$ 和一条新的边 $(x_k, y_k)$。\n\n核心任务是添加最少数量的此类传感器边，以将匹配大小增加到 $n$。设 $M$ 是图 $G_A$ 中大小为 $\\mu(G_A)$ 的一个最大匹配。存在一个集合 $U \\subset V_x$，包含 $n - \\mu(G_A)$ 个未被 $M$ 匹配的顶点。如果我们在一个状态 $x_k$（其中 $x_k \\in U$）上放置一个专用传感器，我们就会引入一个新的输出顶点 $y_k$ 和一条新的边 $(x_k, y_k)$。由于 $x_k$ 是未匹配的，而 $y_k$ 是一个不属于 $M$ 中任何边的新顶点，因此边 $(x_k, y_k)$ 可以被添加到匹配中，形成一个新的大小为 $\\mu(G_A) + 1$ 的匹配 $M' = M \\cup \\{(x_k, y_k)\\}$。通过在 $n - \\mu(G_A)$ 个未匹配的输入顶点上各放置一个专用传感器，我们可以将匹配大小增加到 $\\mu(G_A) + (n-\\mu(G_A)) = n$。由于每个传感器最多只能将匹配大小增加一，因此这是所需传感器的最小数量。\n因此，所需专用传感器的最小数量为 $N_{sensors} = n - \\mu(G_A)$。注意，问题提到了转置矩阵 $A^{\\top}$。$A^{\\top}$ 的二分图与 $A$ 的图是同构的，因此最大匹配的大小是相同的。\n\n现在我们将此方法应用于给定的问题。\n系统有 $n=6$ 个状态。动态特性受限于一个具有边 $1 \\to 2$、$1 \\to 3$、$2 \\to 4$、$3 \\to 4$、$4 \\to 5$ 和 $4 \\to 6$ 的有向无环图（DAG）。DAG中的一条边 $j \\to i$ 意味着系统矩阵 $A$ 的元素 $A_{ij}$ 是非零的。术语DAG意味着没有有向环，这也包括自环，因此对角线元素 $A_{ii}$ 均为零。结构化矩阵 $A$ 为：\n$$\nA = \\begin{pmatrix}\n0  0  0  0  0  0 \\\\\n*  0  0  0  0  0 \\\\\n*  0  0  0  0  0 \\\\\n0  *  *  0  0  0 \\\\\n0  0  0  *  0  0 \\\\\n0  0  0  *  0  0\n\\end{pmatrix}\n$$\n其中 $*$ 代表一个非零元素。\n\n我们构造相关的二分图 $G_A = (V_x, V_{\\dot{x}}, E_A)$，其中 $V_x=\\{x_1, \\dots, x_6\\}$ 且 $V_{\\dot{x}}=\\{\\dot{x}_1, \\dots, \\dot{x}_6\\}$。边 $(x_j, \\dot{x}_i)$ 对应于非零元素 $A_{ij}$：\n$E_A = \\{(x_1, \\dot{x}_2), (x_1, \\dot{x}_3), (x_2, \\dot{x}_4), (x_3, \\dot{x}_4), (x_4, \\dot{x}_5), (x_4, \\dot{x}_6)\\}$。\n\n我们的目标是找到最大匹配的大小，即 $\\mu(G_A)$。我们检查该图：\n输入顶点 $x_5$ 和 $x_6$ 没有出边，因此它们不能成为任何匹配的一部分。这直接意味着它们将是 $V_x$ 中未匹配的顶点。\n输入顶点 $x_2$ 和 $x_3$ 都只有一个可能的边目标：输出顶点 $\\dot{x}_4$。在任何匹配中，一个顶点最多只能使用一次。因此，边 $(x_2, \\dot{x}_4)$ 和 $(x_3, \\dot{x}_4)$ 中只有一个可以存在于匹配中。这意味着输入顶点 $\\{x_2, x_3\\}$ 中至少有一个必须保持未匹配。\n\n基于此分析，我们在 $V_x$ 中至少有三个未匹配的顶点：两个来自 $\\{x_5, x_6\\}$，以及至少一个来自 $\\{x_2, x_3\\}$。\n未匹配顶点的总数至少为 $3$。这意味着最大匹配的大小最多为 $n - 3 = 6 - 3 = 3$。\n\n让我们构造一个大小为 $3$ 的匹配来确认这是最大尺寸。\n考虑边集 $M = \\{(x_1, \\dot{x}_2), (x_2, \\dot{x}_4), (x_4, \\dot{x}_5)\\}$。\n涉及的顶点是来自 $V_x$ 的 $\\{x_1, x_2, x_4\\}$ 和来自 $V_{\\dot{x}}$ 的 $\\{\\dot{x}_2, \\dot{x}_4, \\dot{x}_5\\}$。没有顶点被使用超过一次，所以这是一个有效的匹配。其大小为 $3$。\n另一个可能的最大匹配是 $M' = \\{(x_1, \\dot{x}_3), (x_3, \\dot{x}_4), (x_4, \\dot{x}_6)\\}$，其大小也为 $3$。\n既然我们找到了一个大小为 $3$ 的匹配，并且我们已经论证了不存在大小为 $4$ 或更大的匹配，那么最大匹配的大小就是 $\\mu(G_A) = 3$。\n\n实现结构能观性所需的最小专用传感器数量由以下公式给出：\n$$\nN_{sensors} = n - \\mu(G_A)\n$$\n代入数值 $n=6$ 和 $\\mu(G_A)=3$：\n$$\nN_{sensors} = 6 - 3 = 3\n$$\n因此，需要 $3$ 个专用传感器。例如，在状态 $x_3$、$x_5$ 和 $x_6$ 上放置传感器将允许一个大小为 $6$ 的匹配：$\\{(x_1, \\dot{x}_2), (x_2, \\dot{x}_4), (x_4, \\dot{x}_5), (x_3, y_3), (x_5, y_5), (x_6, y_6)\\}$。",
            "answer": "$$\\boxed{3}$$"
        },
        {
            "introduction": "在网络科学中，扩散和共识过程是研究信息或影响如何在节点间传播的基本模型。本练习将具体分析一个由图拉普拉斯矩阵描述的线性扩散系统，并揭示网络拓扑结构如何直接影响可观测性。你将通过分析一个不连通图，亲手计算出由于部分子系统与测量节点解耦而产生的不可观测子空间的维度。()",
            "id": "4293879",
            "problem": "考虑一个在具有两个不连通分量的无向、无权网络上的线性扩散过程。设状态向量为 $x(t) \\in \\mathbb{R}^{5}$，其动力学由线性时不变（LTI）系统 $\\dot{x}(t) = A x(t)$ 决定，其中 $A = -L$，$L$ 是图拉普拉斯算子。第一个分量是一个2节点路径图，其拉普拉斯算子为\n$$\nL_{1} = \\begin{pmatrix}\n1  -1 \\\\\n-1  1\n\\end{pmatrix},\n$$\n第二个分量是一个3节点完全图（三角形），其拉普拉斯算子为\n$$\nL_{2} = \\begin{pmatrix}\n2  -1  -1 \\\\\n-1  2  -1 \\\\\n-1  -1  2\n\\end{pmatrix}.\n$$\n总拉普拉斯算子是块对角的，\n$$\nL = \\begin{pmatrix}\nL_{1}  0 \\\\\n0  L_{2}\n\\end{pmatrix},\n$$\n因此 $A = -L$ 也是块对角的，其块为 $A_{1} = -L_{1}$ 和 $A_{2} = -L_{2}$。假设输出是一个仅测量2节点分量中第一个节点状态的单个传感器，即，\n$$\ny(t) = C x(t), \\quad C = \\begin{pmatrix}1  0  0  0  0\\end{pmatrix}.\n$$\n从图上扩散和线性系统能观性的定义出发，并且不使用任何预先指定的快捷公式，确定系统对 $(A, C)$ 的不可观测子空间的维数。通过分离每个分量对测量输出的影响，解释拉普拉斯算子的块对角结构如何决定这个维数。将你的最终答案以单个整数形式给出。无需四舍五入。",
            "solution": "线性时不变系统对 $(A, C)$ 的不可观测子空间是指对于所有 $t \\geq 0$ 都能产生零输出 $y(t) = 0$ 的初始状态 $x(0)$ 的集合。一个等价的刻画是，不可观测子空间是能观性矩阵 $\\mathcal{O}$ 的零空间。我们所求的就是这个子空间的维数。\n\n状态空间的维数为 $n=5$。系统对 $(A, C)$ 的能观性矩阵 $\\mathcal{O}$ 构造如下：\n$$\n\\mathcal{O} = \\begin{pmatrix} C \\\\ CA \\\\ CA^2 \\\\ CA^3 \\\\ CA^4 \\end{pmatrix}\n$$\n不可观测子空间的维数由 $n - \\text{rank}(\\mathcal{O})$ 给出。\n\n问题的结构，即块对角系统矩阵 $A$ 和相应的分块输出矩阵 $C$，使得我们可以进行更具洞察力的分析。让我们根据网络分量对状态向量 $x(t)$ 进行分块：\n$$\nx(t) = \\begin{pmatrix} x_1(t) \\\\ x_2(t) \\end{pmatrix}\n$$\n其中 $x_1(t) \\in \\mathbb{R}^2$ 对应于2节点路径图的状态，$x_2(t) \\in \\mathbb{R}^3$ 对应于3节点完全图的状态。\n\n系统动力学是解耦的：\n$$\n\\dot{x}_1(t) = A_1 x_1(t) \\quad \\text{其中} \\quad A_1 = -L_1 = \\begin{pmatrix} -1  1 \\\\ 1  -1 \\end{pmatrix}\n$$\n$$\n\\dot{x}_2(t) = A_2 x_2(t) \\quad \\text{其中} \\quad A_2 = -L_2 = \\begin{pmatrix} -2  1  1 \\\\ 1  -2  1 \\\\ 1  1  -2 \\end{pmatrix}\n$$\n输出方程也可以用这个分块来表示。设 $C = \\begin{pmatrix} C_1  C_2 \\end{pmatrix}$，其中 $C_1 = \\begin{pmatrix} 1  0 \\end{pmatrix}$ 且 $C_2 = \\begin{pmatrix} 0  0  0 \\end{pmatrix}$。输出为：\n$$\ny(t) = C x(t) = \\begin{pmatrix} C_1  C_2 \\end{pmatrix} \\begin{pmatrix} x_1(t) \\\\ x_2(t) \\end{pmatrix} = C_1 x_1(t) + C_2 x_2(t) = C_1 x_1(t)\n$$\n输出 $y(t)$ 只依赖于第一个分量的状态 $x_1(t)$。第二个分量的状态 $x_2(t)$ 对测量没有直接或间接的影响。\n\n让我们正式分析不可观测状态 $x(0) = \\begin{pmatrix} x_1(0) \\\\ x_2(0) \\end{pmatrix}$。如果对于所有 $t \\ge 0$，输出 $y(t) = C e^{At} x(0)$ 恒等于零，则该状态是不可观测的。\n由于 $A$ 的块结构，矩阵指数 $e^{At}$ 也是块对角的：\n$$\ne^{At} = \\begin{pmatrix} e^{A_1 t}  0 \\\\ 0  e^{A_2 t} \\end{pmatrix}\n$$\n不可观测的条件是：\n$$\ny(t) = \\begin{pmatrix} C_1  C_2 \\end{pmatrix} \\begin{pmatrix} e^{A_1 t}  0 \\\\ 0  e^{A_2 t} \\end{pmatrix} \\begin{pmatrix} x_1(0) \\\\ x_2(0) \\end{pmatrix} = C_1 e^{A_1 t} x_1(0) + C_2 e^{A_2 t} x_2(0) = 0\n$$\n由于 $C_2$ 是零矩阵，上式简化为：\n$$\nC_1 e^{A_1 t} x_1(0) = 0 \\quad \\text{对于所有 } t \\ge 0\n$$\n这恰好是 $x_1(0)$ 成为子系统 $(A_1, C_1)$ 的不可观测状态的条件。该条件对 $x_2(0)$ 没有施加任何约束。第二个分量的任何初始状态 $x_2(0) \\in \\mathbb{R}^3$，当与第一个分量的一个不可观测状态 $x_1(0)$ 配对时，将导致整个系统的一个不可观测状态。\n\n因此，完整系统 $\\mathcal{U}$ 的不可观测子空间是第一个分量的不可观测子空间 $\\mathcal{U}_1$ 与第二个分量的整个状态空间 $\\mathbb{R}^3$ 的直和：\n$$\n\\mathcal{U} = \\mathcal{U}_1 \\oplus \\mathbb{R}^3\n$$\n其维数为 $\\text{dim}(\\mathcal{U}) = \\text{dim}(\\mathcal{U}_1) + \\text{dim}(\\mathbb{R}^3) = \\text{dim}(\\mathcal{U}_1) + 3$。\n\n我们现在通过分析子系统 $(A_1, C_1)$ 来确定 $\\text{dim}(\\mathcal{U}_1)$。状态维数为 $n_1=2$。其能观性矩阵为：\n$$\n\\mathcal{O}_1 = \\begin{pmatrix} C_1 \\\\ C_1 A_1 \\end{pmatrix}\n$$\n我们有 $C_1 = \\begin{pmatrix} 1  0 \\end{pmatrix}$ 和 $A_1 = \\begin{pmatrix} -1  1 \\\\ 1  -1 \\end{pmatrix}$。\n第二行为：\n$$\nC_1 A_1 = \\begin{pmatrix} 1  0 \\end{pmatrix} \\begin{pmatrix} -1  1 \\\\ 1  -1 \\end{pmatrix} = \\begin{pmatrix} -1  1 \\end{pmatrix}\n$$\n所以，第一个子系统的能观性矩阵为：\n$$\n\\mathcal{O}_1 = \\begin{pmatrix} 1  0 \\\\ -1  1 \\end{pmatrix}\n$$\n$\\mathcal{O}_1$ 的秩可以通过计算其行列式得到：$\\det(\\mathcal{O}_1) = (1)(1) - (0)(-1) = 1 \\ne 0$。由于行列式非零，该矩阵是满秩的，$\\text{rank}(\\mathcal{O}_1) = 2$。\n第一个分量的不可观测子空间的维数为：\n$$\n\\text{dim}(\\mathcal{U}_1) = n_1 - \\text{rank}(\\mathcal{O}_1) = 2 - 2 = 0\n$$\n这意味着第一个子系统是完全能观的，其不可观测子空间是零子空间 $\\{\\mathbf{0}\\}$。\n\n将这个结果代回到总不可观测子空间维数的表达式中：\n$$\n\\text{dim}(\\mathcal{U}) = \\text{dim}(\\mathcal{U}_1) + 3 = 0 + 3 = 3\n$$\n\n这个结果可以通过为 $5 \\times 5$ 系统构造完整的能观性矩阵 $\\mathcal{O}$ 来验证。$\\mathcal{O}$ 的行是 $CA^k$，$k=0, \\dots, 4$。\n由于 $A = \\begin{pmatrix} A_1  0 \\\\ 0  A_2 \\end{pmatrix}$ 且 $C = \\begin{pmatrix} C_1  0 \\end{pmatrix}$，我们有 $CA^k = \\begin{pmatrix} C_1 A_1^k  0 \\end{pmatrix}$。\n因此 $\\mathcal{O}$ 的行是：\n$C = \\begin{pmatrix} 1  0  0  0  0 \\end{pmatrix}$\n$CA = \\begin{pmatrix} -1  1  0  0  0 \\end{pmatrix}$\n$A_1^2 = \\begin{pmatrix} -1  1 \\\\ 1  -1 \\end{pmatrix}^2 = \\begin{pmatrix} 2  -2 \\\\ -2  2 \\end{pmatrix} = -2 A_1$。因此，$CA^2 = -2(CA) = \\begin{pmatrix} 2  -2  0  0  0 \\end{pmatrix}$。所有后续的行 $CA^k$ 都将是 $CA$ 的标量倍数，因此是线性相关的。\n能观性矩阵为：\n$$\n\\mathcal{O} = \\begin{pmatrix} 1  0  0  0  0 \\\\ -1  1  0  0  0 \\\\ 2  -2  0  0  0 \\\\ -4  4  0  0  0 \\\\ 8  -8  0  0  0 \\end{pmatrix}\n$$\n前两行是线性无关的。所有其他行都是第二行的标量倍数。因此，$\\mathcal{O}$ 的秩为 $2$。\n不可观测子空间的维数为 $n - \\text{rank}(\\mathcal{O}) = 5 - 2 = 3$。该子空间由形式为 $(0, 0, v_3, v_4, v_5)^T$ 的向量组成，这精确地对应于未被测量的第二个分量的状态空间。\n不可观测子空间的维数是 $3$。",
            "answer": "$$\\boxed{3}$$"
        },
        {
            "introduction": "理论概念的最终检验在于其在实际数据分析中的应用。本练习将带你进入数据驱动动力学建模的前沿，你将设计一个计算实验，使用子空间辨识方法（与动态模态分解/DMD密切相关）从部分输出测量中恢复网络动力学的关键特征（特征值）。通过亲手实现算法并完成一个关键的理论证明，你将深刻理解为何可观测性是成功从数据中学习动力学模型的根本前提。()",
            "id": "4293880",
            "problem": "考虑一个离散时间、线性、时不变、带部分测量的网络化动态系统。其状态动力学由 $x_{t+1} = A x_t$ 给出，其中 $A \\in \\mathbb{R}^{n \\times n}$，输出为 $y_t = C x_t$，其中 $C \\in \\mathbb{R}^{p \\times n}$。您将设计一个数据驱动的实验，利用输出 $y_t$ 来估计未知矩阵 $A$ 的主导特征值（按模长排序），然后通过构造和证明，论证从数据中一致性地估计这些特征值需要矩阵对 $(A,C)$ 是可观测的。该实验必须遵循一种与 Koopman 算子视角和动态模态分解 (DMD) 兼容的、基于原理的子空间辨识方法，具体步骤如下：构造输出的块汉克尔矩阵，执行低秩分解以逼近一个最小实现，并利用堆叠输出的时移不变性来恢复一个 $A$ 的估计器。您的解决方案不得假设可以访问状态 $x_t$ 或任何输入；只有输出 $y_t$ 可用。该算法应估计一个类 A 算子，其特征值在可观测子空间上逼近真实矩阵 $A$ 的特征值。\n\n从以下基本依据出发：离散时间线性系统的定义、$(A,C)$ 的 Kalman 可观测性概念、线性系统上的 Koopman 算子与状态转移算子重合的观点，以及在可观测性条件下有限输出序列编码了状态演化的事实。避免引入任何过度简化推导的快捷公式。\n\n将主导特征值定义为矩阵 $A$ 中绝对值最大的特征值。给定一个容差参数 $\\tau$，如果 $A$ 的 $k$ 个模长最大的特征值可以与您的子空间方法估计出的特征值一一匹配，且绝对差低于 $\\tau$，则宣布该测试用例成功。如果估计的模型阶数小于 $k$，或者前 $k$ 个真实特征值中任何一个未能以低于 $\\tau$ 的容差匹配，则宣布失败。\n\n您必须实现一个程序，对每个测试用例执行以下操作：\n- 根据指定的初始条件 $x_0$，通过 $x_{t+1} = A x_t$ 和 $y_t = C x_t$ 生成长度为 $T$ 的输出 $y_t$。\n- 使用指定的窗口长度 $L$ 构建一个过去的输出块汉克尔矩阵。\n- 使用过去的块汉克尔矩阵，执行与最小实现一致的低秩分解。\n- 利用堆叠输出的时移不变性，估计一个类 A 算子并提取其特征值。\n- 将估计的特征值与真实矩阵 $A$ 的 $k$ 个主导特征值进行比较，使用容差 $\\tau$，得出一个布尔型的成功值。\n\n您还必须包含一个证明，论证数据驱动地一致性恢复 $A$ 的主导特征值需要 $(A,C)$ 在 Kalman 意义下是可观测的，并将此要求与线性系统的 Koopman/动态模态分解视角联系起来。\n\n使用以下具有固定参数的测试套件。每个测试用例由 $(A,C,x_0,T,L,k,\\tau,\\sigma)$ 指定，其中 $\\sigma$ 是附加输出噪声的标准差， $k$ 是要测试的主导特征值的数量。所有数值均为无量纲。\n\n- 测试用例 1（可观测，部分测量，无噪声）：\n  - $n = 3$, $p = 2$,\n  - $\n    A = \\begin{bmatrix}\n    0.92  0.10  0.00 \\\\\n    0.00  0.85  0.07 \\\\\n    0.00  0.00  0.70\n    \\end{bmatrix},\\quad\n    C = \\begin{bmatrix}\n    1.00  0.00  1.00 \\\\\n    0.00  1.00  0.50\n    \\end{bmatrix}\n  $,\n  - $x_0 = \\begin{bmatrix} 1.00 \\\\ -0.50 \\\\ 0.70 \\end{bmatrix}$,\n  - $T = 200$, $L = 3$, $k = 2$, $\\tau = 10^{-2}$, $\\sigma = 0.00$.\n\n- 测试用例 2（不可观测，单输出，无噪声）：\n  - $n = 3$, $p = 1$,\n  - $\n    A = \\begin{bmatrix}\n    0.92  0.10  0.00 \\\\\n    0.00  0.85  0.07 \\\\\n    0.00  0.00  0.70\n    \\end{bmatrix},\\quad\n    C = \\begin{bmatrix}\n    0.00  1.00  0.00\n    \\end{bmatrix}\n  $,\n  - $x_0 = \\begin{bmatrix} 0.90 \\\\ 1.20 \\\\ -0.60 \\end{bmatrix}$,\n  - $T = 200$, $L = 3$, $k = 2$, $\\tau = 10^{-2}$, $\\sigma = 0.00$.\n\n- 测试用例 3（可观测，部分测量，小噪声）：\n  - $n = 4$, $p = 2$,\n  - $\n    A = \\begin{bmatrix}\n    0.95  0.02  0.00  0.00 \\\\\n    0.00  0.90  0.01  0.00 \\\\\n    0.00  0.00  0.80  0.03 \\\\\n    0.00  0.00  0.00  0.60\n    \\end{bmatrix},\\quad\n    C = \\begin{bmatrix}\n    1.00  0.00  1.00  0.00 \\\\\n    0.00  1.00  0.00  1.00\n    \\end{bmatrix}\n  $,\n  - $x_0 = \\begin{bmatrix} 1.00 \\\\ -0.25 \\\\ 0.50 \\\\ 0.10 \\end{bmatrix}$,\n  - $T = 400$, $L = 4$, $k = 2$, $\\tau = 10^{-2}$, $\\sigma = 10^{-4}$.\n\n- 测试用例 4（可观测，重复的主导特征值，无噪声）：\n  - $n = 3$, $p = 2$,\n  - $\n    A = \\begin{bmatrix}\n    0.90  0.02  0.00 \\\\\n    0.00  0.90  0.00 \\\\\n    0.00  0.00  0.50\n    \\end{bmatrix},\\quad\n    C = \\begin{bmatrix}\n    1.00  0.00  0.00 \\\\\n    0.00  1.00  1.00\n    \\end{bmatrix}\n  $,\n  - $x_0 = \\begin{bmatrix} 1.00 \\\\ 0.10 \\\\ -0.30 \\end{bmatrix}$,\n  - $T = 300$, $L = 4$, $k = 2$, $\\tau = 10^{-2}$, $\\sigma = 0.00$.\n\n您的程序必须：\n- 仅使用指定的矩阵和参数生成 $t = 0,1,\\dots,T-1$ 的输出 $y_t$。\n- 使用窗口长度 $L$ 构建过去的输出块汉克尔矩阵。\n- 仅使用输出，通过一种与上述描述一致的子空间方法来估计一个类 A 算子。\n- 计算其特征值，并检验是否在容差 $\\tau$ 内恢复了真实矩阵 $A$ 的 $k$ 个主导特征值。\n- 生成一行输出，其中包含一个用方括号括起来的逗号分隔列表（例如 `[r_1,r_2,r_3,r_4]`），其中每个 $r_i$ 是一个布尔值，表示测试用例 $i$ 的成功（$\\mathrm{True}$）或失败（$\\mathrm{False}$）。\n\n不涉及物理单位或角度；所有量都是无量纲的。算法应在指定的测试用例中对小的测量噪声具有鲁棒性，并通过对主导特征值的恢复返回失败来处理 $(A,C)$ 不可观测的情况。",
            "solution": "该问题要求一个包含两部分的回答：一个关于特征值估计需要可观测性的形式化证明，以及一个执行此估计的子空间辨识方法的算法实现。\n\n### 第 1 部分：可观测性必要性的证明\n\n我们首先建立理论基础。该系统是一个离散时间、线性时不变 (LTI) 系统，描述如下：\n$$\nx_{t+1} = A x_t\n$$\n$$\ny_t = C x_t\n$$\n其中 $x_t \\in \\mathbb{R}^{n}$ 是状态，$y_t \\in \\mathbb{R}^{p}$ 是输出，$A \\in \\mathbb{R}^{n \\times n}$ 是状态转移矩阵，$C \\in \\mathbb{R}^{p \\times n}$ 是输出矩阵。核心任务是仅使用输出序列 $\\{y_0, y_1, y_2, \\dots, y_{T-1}\\}$ 来估计 $A$ 的主导特征值。\n\n所提出的子空间辨识方法依赖于从输出数据构建块汉克尔矩阵。我们定义一个在长度为 $L$ 的窗口上堆叠的输出向量：\n$$\nY_t = \\begin{bmatrix} y_t \\\\ y_{t+1} \\\\ \\vdots \\\\ y_{t+L-1} \\end{bmatrix} \\in \\mathbb{R}^{pL}\n$$\n使用系统方程，我们可以将此堆叠向量与状态 $x_t$ 关联起来：\n$$\nY_t = \\begin{bmatrix} C x_t \\\\ C x_{t+1} \\\\ \\vdots \\\\ C x_{t+L-1} \\end{bmatrix} = \\begin{bmatrix} C \\\\ CA \\\\ \\vdots \\\\ CA^{L-1} \\end{bmatrix} x_t\n$$\n我们将阶数为 $L$ 的扩展可观测性矩阵定义为 $\\mathcal{O}_L = \\begin{bmatrix} C^T  (CA)^T  \\dots  (CA^{L-1})^T \\end{bmatrix}^T \\in \\mathbb{R}^{pL \\times n}$。那么，该关系可以紧凑地写为：\n$$\nY_t = \\mathcal{O}_L x_t\n$$\n数据驱动的实验使用这些堆叠向量的序列来形成一个块汉克尔数据矩阵。我们定义两个这样的矩阵，一个代表“过去”，一个代表“未来”：\n$$\n\\mathcal{H}_p = \\begin{bmatrix} Y_0  Y_1  \\dots  Y_{N-1} \\end{bmatrix} = \\begin{bmatrix} y_0  y_1  \\dots  y_{N-1} \\\\ y_1  y_2  \\dots  y_N \\\\ \\vdots  \\vdots  \\ddots  \\vdots \\\\ y_{L-1}  y_L  \\dots  y_{L+N-2} \\end{bmatrix}\n$$\n$$\n\\mathcal{H}_f = \\begin{bmatrix} Y_1  Y_2  \\dots  Y_N \\end{bmatrix} = \\begin{bmatrix} y_1  y_2  \\dots  y_N \\\\ y_2  y_3  \\dots  y_{N+1} \\\\ \\vdots  \\vdots  \\ddots  \\vdots \\\\ y_L  y_{L+1}  \\dots  y_{L+N-1} \\end{bmatrix}\n$$\n其中 $N$ 是列数，受总数据长度 $T$ 的限制。使用关系 $Y_t = \\mathcal{O}_L x_t$，我们可以分解这些汉克尔矩阵：\n$$\n\\mathcal{H}_p = \\mathcal{O}_L \\begin{bmatrix} x_0  x_1  \\dots  x_{N-1} \\end{bmatrix} = \\mathcal{O}_L X_p\n$$\n$$\n\\mathcal{H}_f = \\mathcal{O}_L \\begin{bmatrix} x_1  x_2  \\dots  x_N \\end{bmatrix} = \\mathcal{O}_L X_f\n$$\n动力学 $x_{t+1} = A x_t$ 意味着 $X_f = A X_p$。将此代入 $\\mathcal{H}_f$ 的方程中得到：\n$$\n\\mathcal{H}_f = \\mathcal{O}_L A X_p\n$$\n子空间方法的关键洞见是找到一个将 $\\mathcal{H}_p$ 映射到 $\\mathcal{H}_f$ 的算子。算法可用的所有信息都包含在 $\\mathcal{H}_p$ 和 $\\mathcal{H}_f$ 中。这些矩阵的列空间是扩展可观测性矩阵值域 $\\mathrm{range}(\\mathcal{O}_L)$ 的一个子集。这个空间的维度至关重要。假设状态序列 $X_p$ 足够丰富（即行满秩为 $n$），则数据矩阵 $\\mathcal{H}_p$ 的秩由下式给出：\n$$\n\\mathrm{rank}(\\mathcal{H}_p) = \\mathrm{rank}(\\mathcal{O}_L X_p) = \\mathrm{rank}(\\mathcal{O}_L)\n$$\nKalman 可观测性条件指出，如果标准可观测性矩阵 $\\mathcal{O} = \\mathcal{O}_n = \\begin{bmatrix} C^T  (CA)^T  \\dots  (CA^{n-1})^T \\end{bmatrix}^T$ 是列满秩的，即 $\\mathrm{rank}(\\mathcal{O}) = n$，则矩阵对 $(A, C)$ 是可观测的。根据 Cayley-Hamilton 定理，对于 $k \\ge n$ 的任何 $A^k$ 次幂，都是 $A$ 的低次幂的线性组合。这意味着对于任何 $L \\ge n$，$\\mathrm{rank}(\\mathcal{O}_L) = \\mathrm{rank}(\\mathcal{O})$。\n\n我们来考虑两种情况：\n情况 1：系统 $(A, C)$ 是可观测的。\n在这种情况下，$\\mathrm{rank}(\\mathcal{O}_L) = n$（对于 $L \\ge n$）。矩阵 $\\mathcal{O}_L$ 存在左逆 $\\mathcal{O}_L^\\dagger = (\\mathcal{O}_L^T \\mathcal{O}_L)^{-1}\\mathcal{O}_L^T$。从 $\\mathcal{H}_p = \\mathcal{O}_L X_p$ 出发，原则上可以恢复状态序列：$X_p = \\mathcal{O}_L^\\dagger \\mathcal{H}_p$。将此代入 $\\mathcal{H}_f$ 的表达式中：\n$$\n\\mathcal{H}_f = (\\mathcal{O}_L A \\mathcal{O}_L^\\dagger) \\mathcal{H}_p\n$$\n算子 $\\mathcal{K} = \\mathcal{O}_L A \\mathcal{O}_L^\\dagger$ 是堆叠输出空间中动力学的一种表示。像动态模态分解 (DMD) 这样的子空间辨识算法，会找到该算子低秩版本的近似。具体来说，它通过对 $\\mathcal{H}_p = U S V^T$ 进行 SVD 来计算一个降阶模型算子 $\\hat{A}_r$。对于一个可观测系统，有效秩为 $r=n$。估计的算子是 $\\hat{A}_n = U_n^T \\mathcal{H}_f V_n S_n^{-1}$，这是一个 $n \\times n$ 矩阵。可以证明，该算子通过一个相似变换与 $A$ 相关，即 $\\hat{A}_n \\sim A$。因此，$\\hat{A}_n$ 的特征值与 $A$ 的特征值相同。数据驱动的估计是成功的。\n\n情况 2：系统 $(A, C)$ 是不可观测的。\n在这种情况下，$\\mathrm{rank}(\\mathcal{O})  n$。可观测性矩阵有一个非平凡的零空间，$\\mathrm{ker}(\\mathcal{O}) \\neq \\{0\\}$。这就是不可观测子空间。设 $v$ 是此子空间中的任意向量。根据定义，$\\mathcal{O}v = 0$，这意味着对于所有 $i \\ge 0$，$CA^i v = 0$。\n现在，考虑一个初始状态 $x_0$，它在不可观测子空间中有一个分量。我们可以将 $x_0$ 分解为 $x_0 = x_{obs} + x_{unobs}$，其中 $x_{obs}$ 在 $\\mathcal{O}^T$ 的值域中（可观测子空间），而 $x_{unobs} \\in \\mathrm{ker}(\\mathcal{O})$。在时间 $t$ 的状态是 $x_t = A^t x_0 = A^t x_{obs} + A^t x_{unobs}$。输出是：\n$$\ny_t = C x_t = C A^t x_{obs} + C A^t x_{unobs}\n$$\n由于不可观测子空间是 A-不变的，所以 $A^t x_{unobs}$ 也在此不可观测子空间中。因此，$C(A^t x_{unobs}) = 0$。输出变为：\n$$\ny_t = C A^t x_{obs}\n$$\n输出序列 $\\{y_t\\}$ 完全独立于初始状态的不可观测分量 $x_{unobs}$ 及其后续演化。数据矩阵 $\\mathcal{H}_p$ 和 $\\mathcal{H}_f$ 不包含任何关于不可观测子空间内动力学的信息。数据矩阵的秩将是 $r = \\mathrm{rank}(\\mathcal{O}_L)  n$。因此，子空间辨识算法将辨识出一个维度为 $r  n$ 的降阶模型。算子 $\\hat{A}_r$ 将是一个 $r \\times r$ 矩阵，其特征值只能逼近 $A$ 在可观测子空间上投影的动力学所对应的特征值。任何与位于不可观测子空间内的特征向量或广义特征向量相关的 $A$ 的特征值对输出都是“不可见”的，无法被恢复。\n\n这直接对应于 Koopman 算子视角。该线性系统的 Koopman 算子与 $A$ 具有相同的谱。像 DMD 这样的数据驱动方法试图从数据中近似 Koopman 算子的谱特性。数据由一组“可观测函数”的测量值组成。在这个问题中，可观测函数是由 $C$ 的行定义的线性投影。如果系统是不可观测的，那么这些可观测函数及其在 Koopman 算子下的时间演化所张成的空间并不能覆盖整个状态空间。由此得到的 Koopman 算子的近似被限制在可观测子空间上，其谱将只包含系统的可观测特征值。\n\n因此，我们得出结论，矩阵对 $(A, C)$ 的可观测性是从输出数据中一致性估计 $A$ 的所有特征值的必要条件。如果一个主导特征值对应于一个不可观测的模态，它就无法被估计出来。\n\n### 第 2 部分：算法设计与实现\n\n该算法实现了证明中所述的子空间辨识过程。\n\n1.  **数据生成**：对于每个测试用例，通过从 $x_0$ 开始模拟 $x_{t+1} = Ax_t$，计算 $y_t = Cx_t$，并在指定时添加标准差为 $\\sigma$ 的高斯噪声，来生成 $t \\in [0, T-1]$ 的输出时间序列 $y_t$。\n\n2.  **块汉克尔矩阵构建**：从输出时间序列 $\\{y_t\\}_{t=0}^{T-1}$ 中，构建两个块汉克尔矩阵 $\\mathcal{H}_p$ 和 $\\mathcal{H}_f$，每个矩阵的块窗口大小为 $L$。每个矩阵的列数为 $N = T-L$。\n    $$\n    \\mathcal{H}_p = \\begin{bmatrix} y_0  y_1  \\dots  y_{T-L-1} \\\\ \\vdots  \\vdots  \\ddots  \\vdots \\\\ y_{L-1}  y_{L}  \\dots  y_{T-2} \\end{bmatrix}, \\quad\n    \\mathcal{H}_f = \\begin{bmatrix} y_1  y_2  \\dots  y_{T-L} \\\\ \\vdots  \\vdots  \\ddots  \\vdots \\\\ y_{L}  y_{L+1}  \\dots  y_{T-1} \\end{bmatrix}\n    $$\n    这些矩阵的维度为 $(p \\cdot L) \\times (T-L)$。\n\n3.  **通过 SVD 进行低秩分解**：对“过去”的数据矩阵 $\\mathcal{H}_p$ 执行奇异值分解 (SVD)：\n    $$\n    \\mathcal{H}_p = U S V^T\n    $$\n    底层可观测系统的秩 $r$ 由对角矩阵 $S$ 中显著奇异值的数量决定。对于无噪声情况，这是非零奇异值的数量。对于有噪声的情况，我们在真实底层系统的秩处截断，这对应于在奇异值图中找到“拐点”。在此实现中，使用一个实际的阈值来确定 $r$。设截断后的矩阵为 $U_r \\in \\mathbb{R}^{pL \\times r}$、 $S_r \\in \\mathbb{R}^{r \\times r}$ 和 $V_r \\in \\mathbb{R}^{N \\times r}$。\n\n4.  **估计降阶算子**：低维系统动力学由一个 $r \\times r$ 矩阵 $\\hat{A}_r$ 捕获，该矩阵逼近了 $A$ 在可观测子空间上的投影。这使用标准的 DMD 公式计算：\n    $$\n    \\hat{A}_r = U_r^T \\mathcal{H}_f V_r S_r^{-1}\n    $$\n\n5.  **特征值比较**：\n    -   计算 $\\hat{A}_r$ 的特征值，设此集合为 $\\{\\hat{\\lambda}_j\\}_{j=1}^r$。\n    -   计算真实矩阵 $A$ 的特征值，设此集合为 $\\{\\lambda_i\\}_{i=1}^n$。\n    -   按模长降序对真实特征值进行排序，并选择前 $k$ 个：$\\{\\lambda_1^*, \\dots, \\lambda_k^*\\}$。\n    -   如果在 $k$ 个主导真实特征值和 $k$ 个估计特征值之间可以找到一一匹配，使得每对的绝对差小于容差 $\\tau$，则测试被宣布为成功。这通过在绝对差的代价矩阵上使用匈牙利算法（`linear_sum_assignment`）来执行。如果估计的模型阶数 $r$ 小于 $k$，或者找不到这样的匹配，则测试失败。",
            "answer": "```python\nimport numpy as np\nfrom scipy.optimize import linear_sum_assignment\n\ndef solve():\n    \"\"\"\n    Main solver function to run all test cases.\n    \"\"\"\n    test_cases = [\n        # Test Case 1: observable, partial measurements, noise-free\n        {\n            \"A\": np.array([[0.92, 0.10, 0.00], [0.00, 0.85, 0.07], [0.00, 0.00, 0.70]]),\n            \"C\": np.array([[1.00, 0.00, 1.00], [0.00, 1.00, 0.50]]),\n            \"x0\": np.array([1.00, -0.50, 0.70]),\n            \"T\": 200, \"L\": 3, \"k\": 2, \"tau\": 1e-2, \"sigma\": 0.00\n        },\n        # Test Case 2: unobservable, single-output, noise-free\n        {\n            \"A\": np.array([[0.92, 0.10, 0.00], [0.00, 0.85, 0.07], [0.00, 0.00, 0.70]]),\n            \"C\": np.array([[0.00, 1.00, 0.00]]),\n            \"x0\": np.array([0.90, 1.20, -0.60]),\n            \"T\": 200, \"L\": 3, \"k\": 2, \"tau\": 1e-2, \"sigma\": 0.00\n        },\n        # Test Case 3: observable, partial measurements, small noise\n        {\n            \"A\": np.array([[0.95, 0.02, 0.00, 0.00], [0.00, 0.90, 0.01, 0.00], [0.00, 0.00, 0.80, 0.03], [0.00, 0.00, 0.00, 0.60]]),\n            \"C\": np.array([[1.00, 0.00, 1.00, 0.00], [0.00, 1.00, 0.00, 1.00]]),\n            \"x0\": np.array([1.00, -0.25, 0.50, 0.10]),\n            \"T\": 400, \"L\": 4, \"k\": 2, \"tau\": 1e-2, \"sigma\": 1e-4\n        },\n        # Test Case 4: observable, repeated leading eigenvalue, noise-free\n        {\n            \"A\": np.array([[0.90, 0.02, 0.00], [0.00, 0.90, 0.00], [0.00, 0.00, 0.50]]),\n            \"C\": np.array([[1.00, 0.00, 0.00], [0.00, 1.00, 1.00]]),\n            \"x0\": np.array([1.00, 0.10, -0.30]),\n            \"T\": 300, \"L\": 4, \"k\": 2, \"tau\": 1e-2, \"sigma\": 0.00\n        }\n    ]\n\n    results = []\n    for params in test_cases:\n        result = run_test_case(**params)\n        results.append(result)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef run_test_case(A, C, x0, T, L, k, tau, sigma):\n    \"\"\"\n    Executes the subspace identification experiment for a single test case.\n    \"\"\"\n    n, p = A.shape[0], C.shape[0]\n\n    # 1. Generate output data\n    x_data = np.zeros((n, T))\n    y_data = np.zeros((p, T))\n    x_data[:, 0] = x0\n    y_data[:, 0] = C @ x0\n\n    # Initialize a random number generator for reproducibility\n    rng = np.random.default_rng(seed=42)\n\n    for t in range(T - 1):\n        x_data[:, t+1] = A @ x_data[:, t]\n        y_data[:, t+1] = C @ x_data[:, t+1]\n\n    if sigma > 0.0:\n        noise = rng.normal(0, sigma, size=y_data.shape)\n        y_data += noise\n\n    # 2. Construct block-Hankel matrices\n    num_cols_hankel = T - L\n    H_p = np.zeros((p * L, num_cols_hankel))\n    H_f = np.zeros((p * L, num_cols_hankel))\n\n    for j in range(num_cols_hankel):\n        H_p[:, j] = y_data[:, j:j+L].flatten()\n        H_f[:, j] = y_data[:, j+1:j+L+1].flatten()\n\n    # 3. Low-rank factorization via SVD\n    try:\n        U, s, Vh = np.linalg.svd(H_p, full_matrices=False)\n    except np.linalg.LinAlgError:\n        return False # SVD failed, cannot proceed\n\n    # Determine model order r\n    # For noise-free cases, this is the number of singular values > tolerance\n    # For noisy cases, this threshold needs to be chosen carefully.\n    # A relative threshold is used for robustness.\n    rank_tol = 1e-8 if sigma == 0 else s[0] * 1e-3\n    r = np.sum(s > rank_tol)\n\n    # Truncate according to rank r\n    U_r = U[:, :r]\n    S_r = np.diag(s[:r])\n    V_r = Vh[:r, :].T\n\n    # 4. Estimate reduced-order operator\n    # A_hat = U_r^T @ H_f @ V_r @ S_r^{-1}\n    S_r_inv = np.linalg.inv(S_r)\n    A_hat_r = U_r.T @ H_f @ V_r @ S_r_inv\n\n    # 5. Eigenvalue comparison\n    # Get ground-truth leading eigenvalues\n    true_eigs = np.linalg.eigvals(A)\n    sorted_true_eigs = sorted(true_eigs, key=np.abs, reverse=True)\n    top_k_true_eigs = sorted_true_eigs[:k]\n\n    # Check for failure condition: model order less than k\n    if r  k:\n        return False\n\n    # Get estimated eigenvalues\n    est_eigs = np.linalg.eigvals(A_hat_r)\n\n    # Find best one-to-one matching using the Hungarian algorithm\n    cost_matrix = np.abs(np.array([top_k_true_eigs]).T - np.array([est_eigs]))\n    row_ind, col_ind = linear_sum_assignment(cost_matrix)\n    \n    # Check if all k leading eigenvalues are matched within tolerance\n    min_total_cost = cost_matrix[row_ind, col_ind].sum()\n    if len(row_ind)  k:\n        # Not enough estimated eigenvalues to match all k true ones\n        return False\n        \n    num_matched = 0\n    for i in range(len(row_ind)):\n        true_eig_idx = row_ind[i]\n        est_eig_idx = col_ind[i]\n        if cost_matrix[true_eig_idx, est_eig_idx]  tau:\n            num_matched += 1\n\n    return num_matched == k\n\nif __name__ == '__main__':\n    solve()\n\n```"
        }
    ]
}