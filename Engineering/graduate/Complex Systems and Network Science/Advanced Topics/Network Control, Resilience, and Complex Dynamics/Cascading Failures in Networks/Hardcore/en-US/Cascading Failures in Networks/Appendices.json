{
    "hands_on_practices": [
        {
            "introduction": "Understanding cascading failures often begins with concrete, physically-motivated systems like electrical power grids. This first practice dives into the fundamentals of cascade dynamics using the linearized Direct Current (DC) load flow model, a standard approximation in power systems engineering. By implementing a simulation where the most overloaded transmission line is iteratively removed, you will gain hands-on experience with the core feedback loop of failure and load redistribution that defines a cascade .",
            "id": "4266617",
            "problem": "Consider a network of $N$ nodes and $E$ undirected lines, each line connecting a pair of nodes. The network is modeled using the linearized Direct Current (DC) load flow approximation. Let the net power injection vector be $P \\in \\mathbb{R}^N$, where $P_i$ is positive for generation and negative for load at node $i$, and let each line $e$ have a positive susceptance $b_e$ and a thermal capacity limit $C_e  0$. The DC model posits that the active power flow $f_e$ on line $e$ is proportional to the difference of voltage phase angles across its endpoints, and that nodal power balance is enforced by Kirchhoff's Current Law. You will compute flows, detect overloads, remove overloaded lines by a specific rule, and simulate the resulting cascade until a stable state is reached.\n\nFundamental base for modeling:\n- Kirchhoff's Current Law: The algebraic sum of flows incident to node $i$ equals the net injection $P_i$.\n- Linearized DC load flow relation: For any line $e = (u,v)$, the active power flow satisfies $f_e = b_e \\left(\\theta_u - \\theta_v\\right)$, where $\\theta_i$ is the voltage phase angle at node $i$.\n- Incidence-based formulation: With an oriented incidence matrix $A \\in \\mathbb{R}^{E \\times N}$, defined by $A_{e,u} = +1$ and $A_{e,v} = -1$ for a line $e=(u,v)$ oriented from node $u$ to node $v$ (and zeros elsewhere), Kirchhoff's Current Law becomes $A^\\top f = P$, and the linearized DC relation becomes $f = \\operatorname{diag}(b) \\, A \\, \\theta$, leading to the weighted Laplacian system $A^\\top \\operatorname{diag}(b) A \\, \\theta = P$ on each connected component, after fixing a reference phase angle. The weighted Laplacian is singular unless one reference (slack) angle per component is fixed.\n\nCascade procedure to implement:\n1. Compute line flows $f_e$ by solving $A^\\top \\operatorname{diag}(b) A \\, \\theta = P$ on each connected component, with a slack bus chosen as the node with the smallest index in that component and its angle fixed to zero. When a component has only one node, there are no flows in that component.\n2. Define the utilization ratio $r_e = \\left|f_e\\right| / C_e$ for each active line $e$.\n3. If there exists any line with $r_e  1$, remove exactly one line: the line with the largest $r_e$. If multiple lines tie for the largest $r_e$, remove the one with the smallest line index. Recompute connected components, and repeat from step $1$.\n4. Terminate when all remaining lines satisfy $r_e \\le 1$, or no lines remain. The final state is considered stable.\n\nHandling islanding:\n- The removal of lines may disconnect the network into multiple connected components (islands). In each island, define a slack bus as the node with the smallest index within that island. The slack absorbs any aggregate power imbalance in that island by adjusting its net injection implicitly via the fixed reference angle, which yields a solvable reduced Laplacian system for the other angles. This approach is purely mathematical and does not constrain slack capacity.\n\nYour program must implement this cascade simulation exactly and return, for each test case, the triple $[k, R, M]$ where:\n- $k$ is the integer number of lines removed until the final stable state,\n- $R$ is the final maximum utilization ratio across all remaining lines, defined as $R = \\max_e r_e$ with $r_e = \\left|f_e\\right| / C_e$, expressed as a real number rounded to six decimal places,\n- $M$ is the integer number of connected components (including isolated nodes) in the final network.\n\nUse the following test suite of parameter values. In all cases, nodes are labeled by integers starting at $0$, each line is specified by its $(u,v)$ endpoints, susceptance $b_e$, and capacity $C_e$, and the orientation for $A$ is from the first endpoint $u$ to the second endpoint $v$.\n\nTest case $1$ (ring with a chord):\n- $N = 4$\n- Lines: $e_0 = (0,1,b_0=5,C_0=8)$, $e_1 = (1,2,b_1=5,C_1=8)$, $e_2 = (2,3,b_2=5,C_2=8)$, $e_3 = (3,0,b_3=5,C_3=8)$, $e_4 = (1,3,b_4=5,C_4=6)$\n- Net injections: $P = [3,-1,-2,0]$\n\nTest case $2$ (star with one tight limit):\n- $N = 5$\n- Lines: $e_0 = (0,1,b_0=4,C_0=4)$, $e_1 = (0,2,b_1=4,C_1=4)$, $e_2 = (0,3,b_2=4,C_2=4)$, $e_3 = (0,4,b_3=4,C_3=0.5)$\n- Net injections: $P = [4,-1,-1,-1,-1]$\n\nTest case $3$ (triangle, generous limits):\n- $N = 3$\n- Lines: $e_0 = (0,1,b_0=10,C_0=100)$, $e_1 = (1,2,b_1=10,C_1=100)$, $e_2 = (0,2,b_2=10,C_2=100)$\n- Net injections: $P = [1,-0.5,-0.5]$\n\nTest case $4$ (chain at exact limit):\n- $N = 3$\n- Lines: $e_0 = (0,1,b_0=10,C_0=5)$, $e_1 = (1,2,b_1=10,C_1=5)$\n- Net injections: $P = [5,0,-5]$\n\nTest case $5$ (chain with tie-breaking on overload):\n- $N = 4$\n- Lines: $e_0 = (0,1,b_0=10,C_0=3)$, $e_1 = (1,2,b_1=10,C_1=2)$, $e_2 = (2,3,b_2=10,C_2=100)$\n- Net injections: $P = [6,-2,-2,-2]$\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes a sub-list $[k,R,M]$. For example, the output must look like $[[k_1,R_1,M_1],[k_2,R_2,M_2],\\dots]$ with $R_i$ rounded to six decimal places.",
            "solution": "The problem requires the simulation of a cascading failure in an electrical power network, modeled using the linearized Direct Current (DC) load flow approximation. The simulation proceeds in discrete steps, where at each step, power flows are calculated, and the most overloaded transmission line is removed, potentially leading to network fragmentation (islanding) and further redistributions of flow until a stable state is reached.\n\nThe core of the simulation rests upon solving the DC power flow equations for each connected component of the network. The governing equation is a linear system derived from Kirchhoff's Current Law and an Ohm's Law-like relationship for active power.\n\nLet the network have $N$ nodes and $E$ lines. We define an oriented incidence matrix $A \\in \\mathbb{R}^{E \\times N}$, where for a line $e$ oriented from node $u$ to node $v$, $A_{e,u} = 1$ and $A_{e,v} = -1$, with all other entries in that row being $0$. Let $\\theta \\in \\mathbb{R}^N$ be the vector of nodal voltage phase angles, $P \\in \\mathbb{R}^N$ be the vector of net power injections, $b \\in \\mathbb{R}^E$ be the vector of line susceptances, and $f \\in \\mathbb{R}^E$ be the vector of power flows on the lines.\n\nThe model is defined by two fundamental relationships:\n1.  The flow $f_e$ on a line $e=(u,v)$ with susceptance $b_e$ is given by $f_e = b_e(\\theta_u - \\theta_v)$. In matrix form, this is $f = \\mathrm{diag}(b) A \\theta$.\n2.  The sum of power flows entering or leaving any node must balance the net power injection at that node. This is Kirchhoff's Current Law (KCL), expressed as $A^\\top f = P$.\n\nSubstituting the first equation into the second yields the system equation for the phase angles:\n$$\nA^\\top \\left( \\mathrm{diag}(b) A \\theta \\right) = P\n$$\n$$\n(A^\\top \\mathrm{diag}(b) A) \\theta = P\n$$\nThis is often written as $L \\theta = P$, where $L = A^\\top \\mathrm{diag}(b) A$ is the weighted Laplacian matrix of the network. The Laplacian matrix for any graph is singular, reflecting the physical reality that only phase angle *differences* determine power flow, not their absolute values. To obtain a unique solution, we must establish a reference.\n\nThe problem specifies a procedure to handle this singularity and the potential fragmentation of the network:\n1.  **Identify Connected Components**: The network is partitioned into its connected components using a graph traversal algorithm such as Breadth-First Search (BFS) or Depth-First Search (DFS). Each component is treated as an independent sub-problem.\n\n2.  **Solve for Each Component**: For each connected component $\\mathcal{C}$ with node set $\\mathcal{V}_\\mathcal{C}$ and line set $\\mathcal{E}_\\mathcal{C}$:\n    - If $|\\mathcal{V}_\\mathcal{C}| \\le 1$, there are no lines within the component, so no flows are calculated.\n    - If $|\\mathcal{V}_\\mathcal{C}|  1$, a reference or \"slack\" node must be chosen. The problem mandates selecting the node with the smallest index within $\\mathcal{V}_\\mathcal{C}$ as the slack node, $s$. Its phase angle is fixed to $\\theta_s = 0$.\n    - Let $\\mathcal{V'}_\\mathcal{C} = \\mathcal{V}_\\mathcal{C} \\setminus \\{s\\}$ be the set of non-slack nodes in the component. We construct a reduced linear system $L' \\theta' = P'$ to solve for the angles $\\theta'$ of the non-slack nodes.\n    - $L'$ is the submatrix of the component's Laplacian $L_\\mathcal{C}$ obtained by removing the row and column corresponding to the slack node $s$. This reduced matrix $L'$ is invertible for a connected component.\n    - $P'$ is the subvector of the power injection vector $P$ corresponding to the non-slack nodes $\\mathcal{V'}_\\mathcal{C}$. The power imbalance in the component, $\\sum_{i \\in \\mathcal{V}_\\mathcal{C}} P_i$, is implicitly absorbed by the slack node.\n    - The non-slack angles are found by solving the linear system: $\\theta' = (L')^{-1} P'$.\n    - With all nodal angles $\\theta$ for the component determined (including $\\theta_s=0$), the flow $f_e$ on each line $e=(u,v)$ within that component is calculated: $f_e = b_e(\\theta_u - \\theta_v)$.\n\nThe overall simulation proceeds as a loop:\n\n**Step I: Initialization**\n- The simulation starts with the full set of lines. The number of removed lines, $k$, is initialized to $0$.\n\n**Step II: Iterative Cascade**\nThe simulation enters a loop that continues until no lines are overloaded.\n- **a) Flow Calculation**: For the current network topology, the procedure described above (component identification and per-component linear system solution) is executed to compute all line flows $f_e$.\n- **b) Overload Assessment**: For each line $e$ with capacity $C_e$, the utilization ratio is calculated: $r_e = |f_e| / C_e$. The maximum ratio, $r_{\\max} = \\max_e r_e$, is identified. If no lines remain, $r_{\\max}$ is taken to be $0$.\n- **c) Stability Check**: If $r_{\\max} \\le 1$, the network is stable. The simulation terminates.\n- **d) Line Removal**: If $r_{\\max}  1$, the network is unstable. A single line must be removed. The line $e^*$ to be removed is the one with the maximum utilization ratio $r_{e^*} = r_{\\max}$. If there is a tie, the line with the smallest original index is chosen. The line $e^*$ is removed from the network, $k$ is incremented by $1$, and the loop repeats from Step II-a.\n\n**Step III: Final Output**\nUpon termination, the final state is characterized by:\n- $k$: The total number of lines removed during the cascade.\n- $R$: The final maximum utilization ratio, $r_{\\max}$, rounded to six decimal places.\n- $M$: The number of connected components in the final, stable network graph.\n\nThis comprehensive algorithm deterministically simulates the cascading failure process as specified. Its implementation requires robust graph algorithms for component finding and numerical linear algebra routines to solve the matrix equations.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef find_components(num_nodes, active_lines_indices, lines_def):\n    \"\"\"Finds connected components in the graph using BFS.\"\"\"\n    if num_nodes == 0:\n        return 0, []\n        \n    adj = [[] for _ in range(num_nodes)]\n    for line_idx in active_lines_indices:\n        u, v, _, _, _ = lines_def[line_idx]\n        adj[u].append(v)\n        adj[v].append(u)\n\n    visited = [False] * num_nodes\n    components = []\n    for i in range(num_nodes):\n        if not visited[i]:\n            component_nodes = set()\n            q = [i]\n            visited[i] = True\n            head = 0\n            while head  len(q):\n                u = q[head]\n                head += 1\n                component_nodes.add(u)\n                for v in adj[u]:\n                    if not visited[v]:\n                        visited[v] = True\n                        q.append(v)\n            components.append(sorted(list(component_nodes)))\n    return len(components), components\n\ndef solve_cascade(num_nodes, lines, power_injections):\n    \"\"\"\n    Simulates the cascading failure process.\n    \"\"\"\n    lines_def = [(u, v, b, c, idx) for idx, (u, v, b, c) in enumerate(lines)]\n    active_lines_indices = set(range(len(lines_def)))\n    \n    removed_lines_count = 0\n\n    while True:\n        if not active_lines_indices:\n            num_components, _ = find_components(num_nodes, active_lines_indices, lines_def)\n            return removed_lines_count, 0.0, num_components\n\n        num_components, components = find_components(num_nodes, active_lines_indices, lines_def)\n\n        flows = {}  # Using dict to store flows by line index\n\n        for comp_nodes in components:\n            if len(comp_nodes) = 1:\n                continue\n\n            # Map component-local indices to global node indices\n            node_map = {node_idx: i for i, node_idx in enumerate(comp_nodes)}\n            \n            comp_lines = []\n            for line_idx in active_lines_indices:\n                u, v, _, _, _ = lines_def[line_idx]\n                if u in comp_nodes and v in comp_nodes:\n                    comp_lines.append(line_idx)\n            \n            if not comp_lines:\n                continue\n\n            comp_size = len(comp_nodes)\n            laplacian = np.zeros((comp_size, comp_size))\n\n            # Build weighted Laplacian for the component\n            for line_idx in comp_lines:\n                u, v, b, _, _ = lines_def[line_idx]\n                u_comp, v_comp = node_map[u], node_map[v]\n                laplacian[u_comp, u_comp] += b\n                laplacian[v_comp, v_comp] += b\n                laplacian[u_comp, v_comp] -= b\n                laplacian[v_comp, u_comp] -= b\n            \n            # Select slack bus (smallest index in component)\n            slack_node_global = comp_nodes[0]\n            slack_node_comp = node_map[slack_node_global]\n            \n            non_slack_indices_comp = [i for i in range(comp_size) if i != slack_node_comp]\n            non_slack_indices_global = [node for node in comp_nodes if node != slack_node_global]\n            \n            # Reduced system\n            reduced_laplacian = laplacian[np.ix_(non_slack_indices_comp, non_slack_indices_comp)]\n            reduced_p = np.array([power_injections[i] for i in non_slack_indices_global])\n\n            # Solve for non-slack angles\n            try:\n                non_slack_thetas = np.linalg.solve(reduced_laplacian, reduced_p)\n            except np.linalg.LinAlgError:\n                # This can happen if a component is just a line, making L' singular.\n                # It's an issue with how the problem is defined, but we must handle it.\n                # A better approach would be pseudo-inverse, but we stick to the problem.\n                # For safety, use pinv if solve fails.\n                non_slack_thetas = np.linalg.pinv(reduced_laplacian) @ reduced_p\n\n            # Reconstruct full theta vector for the component\n            thetas_comp = np.zeros(comp_size)\n            thetas_comp[non_slack_indices_comp] = non_slack_thetas\n            \n            thetas_global = {comp_nodes[i]: thetas_comp[i] for i in range(comp_size)}\n\n            # Calculate flows for lines in this component\n            for line_idx in comp_lines:\n                u, v, b, _, _ = lines_def[line_idx]\n                flow = b * (thetas_global[u] - thetas_global[v])\n                flows[line_idx] = flow\n\n        # Assess overload\n        max_ratio = -1.0\n        line_to_remove = -1\n\n        ratios = []\n        for line_idx in sorted(list(active_lines_indices)):\n            u, v, b, c, _ = lines_def[line_idx]\n            flow_val = flows.get(line_idx, 0.0)\n            ratio = abs(flow_val) / c if c  0 else float('inf')\n            ratios.append((ratio, line_idx))\n\n        if not ratios:\n            # This case is handled at the loop start, but as a safeguard\n            max_ratio_val = 0.0\n        else:\n            # Sort by ratio (desc), then line index (asc)\n            ratios.sort(key=lambda x: (-x[0], x[1]))\n            max_ratio_val = ratios[0][0]\n            line_to_remove = ratios[0][1]\n\n        if max_ratio_val = 1.0:\n            final_max_ratio = max_ratio_val if ratios else 0.0\n            return removed_lines_count, final_max_ratio, num_components\n        else:\n            active_lines_indices.remove(line_to_remove)\n            removed_lines_count += 1\n\ndef solve():\n    test_cases = [\n        {\n            \"N\": 4,\n            \"lines\": [(0, 1, 5, 8), (1, 2, 5, 8), (2, 3, 5, 8), (3, 0, 5, 8), (1, 3, 5, 6)],\n            \"P\": [3, -1, -2, 0]\n        },\n        {\n            \"N\": 5,\n            \"lines\": [(0, 1, 4, 4), (0, 2, 4, 4), (0, 3, 4, 4), (0, 4, 4, 0.5)],\n            \"P\": [4, -1, -1, -1, -1]\n        },\n        {\n            \"N\": 3,\n            \"lines\": [(0, 1, 10, 100), (1, 2, 10, 100), (0, 2, 10, 100)],\n            \"P\": [1, -0.5, -0.5]\n        },\n        {\n            \"N\": 3,\n            \"lines\": [(0, 1, 10, 5), (1, 2, 10, 5)],\n            \"P\": [5, 0, -5]\n        },\n        {\n            \"N\": 4,\n            \"lines\": [(0, 1, 10, 3), (1, 2, 10, 2), (2, 3, 10, 100)],\n            \"P\": [6, -2, -2, -2]\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        k, R, M = solve_cascade(case[\"N\"], case[\"lines\"], case[\"P\"])\n        # Format the result with R rounded to 6 decimal places\n        results.append(f\"[{k},{R:.6f},{M}]\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "Beyond physically-grounded flows, cascading failures can be driven by purely topological properties of a network. In this exercise, we define a node's load by its betweenness centrality, which quantifies its importance in routing traffic along shortest paths. Simulating a cascade on a scale-free network under this model will allow you to explore the famous 'robust-yet-fragile' property, where networks that are resilient to random failures can be exceptionally vulnerable to targeted attacks .",
            "id": "4266621",
            "problem": "Consider a simple undirected network model with $N$ nodes, where the degree distribution is heavy-tailed and approximates a power law $P(k) \\propto k^{-\\gamma}$. We study cascading failures induced by load redistribution when flows are routed along shortest paths. The governing principles are: shortest path routing between all pairs of nodes, node load defined by betweenness centrality, a fixed capacity per node derived from the initial load and a nonnegative tolerance, and iterative failure when current load exceeds fixed capacity.\n\nDefinitions and fundamental base:\n1. Let the network be an undirected simple graph $G = (V,E)$ with $|V| = N$ nodes and adjacency relation $A_{ij} \\in \\{0,1\\}$ indicating the presence of a link between node $i$ and node $j$. Let the degree of node $i$ be $k_i = \\sum_{j} A_{ij}$.\n2. Assume flows are routed along shortest paths. For each distinct unordered pair of nodes $\\{s,t\\}$, consider all shortest paths from $s$ to $t$. The betweenness centrality $B(v)$ of node $v$ is defined as\n$$\nB(v) = \\sum_{\\substack{s,t \\in V \\\\ s \\neq t \\neq v}} \\frac{\\sigma_{st}(v)}{\\sigma_{st}},\n$$\nwhere $\\sigma_{st}$ is the number of shortest paths between $s$ and $t$, and $\\sigma_{st}(v)$ is the number of those shortest paths that pass through node $v$. This definition is for undirected graphs without edge weights.\n3. The initial load $L_i^{(0)}$ on node $i$ is defined by its betweenness centrality computed on the intact network $G$, namely $L_i^{(0)} = B(i)$.\n4. Each node $i$ is assigned a fixed capacity $C_i$ proportional to its initial load, with a nonnegative tolerance parameter $\\alpha$, by\n$$\nC_i = (1+\\alpha) \\, L_i^{(0)}.\n$$\n5. A cascade is triggered by removing a single node that represents a typical high-impact attack. Here, remove the node $v^\\star$ with the largest initial load $L_{v^\\star}^{(0)}$ (breaking ties by highest degree and then lowest index). After removal, recompute loads $L_i^{(t)}$ on the residual graph at each cascade iteration $t$ via the same betweenness centrality definition restricted to the remaining nodes. In each iteration, any remaining node $i$ whose current load $L_i^{(t)}$ exceeds its fixed capacity $C_i$ fails and is removed from the network. The cascade stops when no remaining node is overloaded.\n6. The cascade size $S(\\alpha)$ is defined as the fraction of nodes removed at termination (including the initially attacked node), expressed as a decimal in $[0,1]$.\n\nGoal:\nStarting from these principles, implement a program that:\n- Generates a scale-free network with $P(k) \\propto k^{-\\gamma}$ using an expected-degree construction and samples degrees $k_i$ from a truncated power-law between $k_{\\min}$ and $k_{\\max}$. Use the Chung–Lu expected-degree model to place edges independently with probability $p_{ij} = \\min\\left(1, \\frac{k_i k_j}{\\sum_{\\ell} k_{\\ell}} \\right)$ for $i \\neq j$, yielding an undirected simple graph. The model must avoid self-loops and multi-edges.\n- Computes node betweenness centrality using the Brandes algorithm for unweighted undirected graphs to obtain loads.\n- Simulates the cascading failure process under the fixed-capacity rule $C_i = (1+\\alpha)L_i^{(0)}$ for each specified tolerance $\\alpha$.\n- Returns the cascade size $S(\\alpha)$ for each $\\alpha$ tested and identifies robust-yet-fragile regimes as follows: label a parameter set as robust-yet-fragile if at least $\\lceil 0.6 m \\rceil$ of the tested tolerance values (where $m$ is the number of $\\alpha$ values in that set) yield cascade sizes $\\leq 0.1$ while the maximum cascade size across the tested $\\alpha$ values is $\\geq 0.5$.\n\nAll computations are purely combinatorial and unitless, since loads and capacities are defined via path counts. Angles are not involved. Return all cascade sizes as decimals. No percentages are permitted.\n\nTest suite:\nYour program must use the following test suite of parameter sets, each specified as a tuple $(N,\\gamma,\\text{seed},\\{ \\alpha \\text{ values} \\})$. The random seed must be set to ensure deterministic results.\n\n- Case A (happy path, heavy tail with moderate $N$): $(N=\\;60,\\;\\gamma=\\;2.5,\\;\\text{seed}=\\;12345,\\;\\{\\alpha=\\;0.0,\\;0.1,\\;0.2,\\;0.5\\})$.\n- Case B (lighter tail, same $N$): $(N=\\;60,\\;\\gamma=\\;3.5,\\;\\text{seed}=\\;54321,\\;\\{\\alpha=\\;0.0,\\;0.1,\\;0.2,\\;0.5\\})$.\n- Case C (heavier tail, smaller $N$, wider tolerance sweep): $(N=\\;40,\\;\\gamma=\\;2.2,\\;\\text{seed}=\\;10101,\\;\\{\\alpha=\\;0.0,\\;0.05,\\;0.1,\\;0.2,\\;0.3,\\;1.0\\})$.\n- Case D (boundary connectivity stress, moderate $N$, wide tail): $(N=\\;50,\\;\\gamma=\\;2.8,\\;\\text{seed}=\\;20202,\\;\\{\\alpha=\\;0.0,\\;0.02,\\;0.05,\\;0.1,\\;0.2\\})$.\n\nFor all cases, use $k_{\\min}=\\;2$ and $k_{\\max}=\\;\\left\\lfloor \\frac{N}{2} \\right\\rfloor$. Sampling must use the inverse cumulative distribution function for the continuous power law truncated to $[k_{\\min},k_{\\max}]$, then round to the nearest integer within bounds, ensuring $\\sum_i k_i$ is even by adjusting one randomly chosen degree by $\\pm 1$ within bounds if necessary.\n\nFinal output format:\nYour program should produce a single line of output containing a list with one element per test case. Each test case element is itself a list containing the cascade sizes for the specified $\\alpha$ values in order, followed by a boolean robust-yet-fragile flag. The entire output must contain no whitespace characters. For example, a line with two cases would look like\n$$\n[ [ s_{A,1}, s_{A,2}, \\dots, r_A ], [ s_{B,1}, s_{B,2}, \\dots, r_B ] ]\n$$\nbut with no spaces. Consequently, the required final output format is\n$$\n[[S_A(\\alpha_1),S_A(\\alpha_2),\\dots,\\text{flag}_A],[S_B(\\alpha_1),S_B(\\alpha_2),\\dots,\\text{flag}_B],[S_C(\\alpha_1),\\dots,\\text{flag}_C],[S_D(\\alpha_1),\\dots,\\text{flag}_D]],\n$$\nwhere each $S(\\alpha_j)$ is a decimal and each flag is a boolean.\n\nYour program must not read any input and must generate the test suite internally as specified. The output must be exactly one line, matching the format above, with no additional text.",
            "solution": "The user has provided a detailed problem statement regarding the simulation of cascading failures in scale-free networks. My first obligation is to validate this problem for scientific soundness, completeness, and clarity before proceeding.\n\n### Problem Validation\n\nI will now conduct a systematic validation of the provided problem statement.\n\n#### Step 1: Extract Givens\n\nThe problem provides the following data, definitions, and procedures:\n- **Network Structure**: An undirected simple graph $G=(V,E)$ with $N$ nodes. The degree distribution is specified to be heavy-tailed, approximating $P(k) \\propto k^{-\\gamma}$.\n- **Network Generation**:\n    - A scale-free network is to be generated using an expected-degree construction.\n    - Node degrees $k_i$ are sampled from a truncated power-law distribution over $[k_{\\min}, k_{\\max}]$. The sampling method is specified as inverse transform sampling of the continuous distribution, followed by rounding to the nearest integer and clamping to the bounds.\n    - An explicit requirement is to ensure the sum of degrees $\\sum_i k_i$ is even by adjusting a randomly selected degree.\n    - The Chung–Lu model is to be used for edge generation, with edge probability $p_{ij} = \\min\\left(1, \\frac{k_i k_j}{\\sum_{\\ell} k_{\\ell}} \\right)$ for $i \\neq j$.\n- **Load and Capacity**:\n    - The load on a node is defined by its betweenness centrality, $L_i = B(i)$, calculated for unweighted, undirected graphs: $B(v) = \\sum_{s \\neq t \\neq v} \\frac{\\sigma_{st}(v)}{\\sigma_{st}}$ over distinct unordered pairs $\\{s,t\\}$. The Brandes algorithm is specified for this calculation.\n    - Node capacity is defined as $C_i = (1+\\alpha) L_i^{(0)}$, where $L_i^{(0)}$ is the initial load on the intact network and $\\alpha$ is a non-negative tolerance parameter.\n- **Cascade Dynamics**:\n    - A cascade is initiated by removing the node $v^\\star$ with the highest initial load $L^{(0)}$. Ties are broken first by highest degree, then by lowest node index.\n    - The failure process is iterative. In each step, loads are recomputed on the remaining network. Any node $i$ whose current load $L_i^{(t)}$ exceeds its fixed capacity $C_i$ is removed.\n    - The cascade halts when an iteration occurs with no new node failures.\n- **Metrics**:\n    - The primary metric is the cascade size $S(\\alpha)$, defined as the total fraction of nodes removed (including the initial one).\n    - A \"robust-yet-fragile\" (RYF) regime is defined for a parameter set if two conditions are met: (1) for at least $\\lceil 0.6 m \\rceil$ of the $m$ tested $\\alpha$ values, the cascade size is $S(\\alpha) \\leq 0.1$; and (2) the maximum cascade size over all tested $\\alpha$ values is $\\geq 0.5$.\n- **Test Suite**:\n    - Four specific test cases are provided, each with parameters $(N, \\gamma, \\text{seed}, \\{\\alpha \\text{ values}\\})$.\n    - Fixed parameters for all cases are $k_{\\min}=2$ and $k_{\\max}=\\lfloor N/2 \\rfloor$.\n    - The use of a random seed for each case is mandated for deterministic reproducibility.\n- **Output Format**: A single-line string representing a list of lists, with no whitespace, containing the cascade sizes and the RYF boolean flag for each test case.\n\n#### Step 2: Validate Using Extracted Givens\n\n- **Scientifically Grounded**: The problem is firmly rooted in the field of complex systems and network science. The model described is a canonical model for studying load-based cascading failures, closely following seminal works in the literature (e.g., Motter  Lai, 2002). All components—power-law networks, the Chung-Lu model, betweenness centrality as load, and the iterative failure mechanism—are standard and well-accepted concepts. The problem does not violate any scientific or mathematical principles.\n- **Well-Posed**: The problem is exceptionally well-posed. The provision of specific random seeds for each test case ensures that the stochastic network generation process is fully deterministic and reproducible. The initial attack is deterministic due to a clear tie-breaking rule. The cascade dynamics are also deterministic. Consequently, a unique, stable, and meaningful solution exists for each test case.\n- **Objective**: The language is precise, quantitative, and free of subjectivity. All terms are either standard in the field or explicitly defined.\n- **Completeness and Consistency**: The problem statement is self-contained. It provides all necessary parameters, algorithms (Chung-Lu, Brandes), and definitions. The degree sampling and adjustment procedures are described in sufficient detail to be implemented unambiguously. There are no internal contradictions.\n- **Feasibility**: The network sizes ($N \\leq 60$) are small enough that the specified calculations, particularly the repeated application of the Brandes algorithm ($O(N \\cdot M)$ where $M$ is the number of edges), are computationally feasible within a reasonable time frame.\n\n#### Step 3: Verdict and Action\n\nThe problem statement is **VALID**. It is a well-structured, scientifically sound, and computationally tractable problem that tests fundamental concepts in computational network science. I will now proceed with the implementation of the solution.\n\n### Solution Design\n\nThe solution will be implemented in Python, adhering strictly to the specified libraries (`numpy`, standard library). The overall structure will be a single function `solve()` that encapsulates all logic.\n\n1.  **Main Loop**: The `solve()` function will iterate through the four test cases provided. For each case, it will call a dedicated function to perform the full simulation.\n\n2.  **Network Generation**: A helper function, `_generate_network`, will be created. It will take $N, \\gamma, k_{\\min}, k_{\\max}$, and a `numpy.random.Generator` instance as input.\n    - **Degree Sequence**: It will first generate a target degree sequence. This involves using the inverse transform sampling method for a continuous truncated power-law distribution with exponent $-\\gamma$. The formula for the inverse CDF is $x(u) = \\left[ u(k_{\\max}^{1-\\gamma} - k_{\\min}^{1-\\gamma}) + k_{\\min}^{1-\\gamma} \\right]^{1/(1-\\gamma)}$, where $u$ is a uniform random variate. The resulting continuous samples will be rounded to the nearest integer and clamped within $[k_{\\min}, k_{\\max}]$. The sum of degrees will be checked, and if odd, a single degree will be adjusted by $\\pm 1$ to make the sum even, ensuring graph realizability.\n    - **Edge Generation**: The Chung-Lu model will be used to populate an adjacency matrix. For each pair of nodes $(i, j)$ with $ij$, an edge will be created with probability $p_{ij} = \\min\\left(1, \\frac{k_i k_j}{\\sum_{\\ell} k_{\\ell}}\\right)$. All random numbers will be drawn from the provided generator instance to ensure reproducibility.\n\n3.  **Betweenness Centrality**: A function, `_calculate_betweenness`, will implement the Brandes algorithm from scratch for unweighted graphs. It will take an adjacency list and a set of active nodes as input.\n    - For each active node `s` as a source, it will perform a Breadth-First Search (BFS) to compute the number of shortest paths $\\sigma_{st}$ and predecessors for all other active nodes `t`.\n    - It will then iterate backward through the nodes in order of decreasing distance from `s` to accumulate path dependencies ($\\delta_s(\\cdot)$).\n    - The final betweenness values will be the sum of these dependencies, divided by $2$ to account for unordered pairs as specified by the problem's definition.\n\n4.  **Cascade Simulation**: The core of the logic will reside in a function that orchestrates the simulation for a single test case.\n    - First, it will convert the generated adjacency matrix to an adjacency list for efficient graph traversals.\n    - It will compute the initial loads $L_i^{(0)}$ on the full, intact network using `_calculate_betweenness`.\n    - It will identify the target node $v^\\star$ for the initial attack based on the specified criteria: maximum load, then maximum degree, then minimum index.\n    - It will then loop through each specified $\\alpha$ value. For each $\\alpha$:\n        - It calculates the fixed capacities $C_i = (1+\\alpha)L_i^{(0)}$.\n        - It initiates the cascade by removing $v^\\star$.\n        - It enters a `while` loop that continues as long as failures occur. In each iteration of this loop, it re-calculates the loads on the current network of active nodes and removes any nodes where the load exceeds capacity.\n        - Once the cascade terminates, it computes the cascade size $S(\\alpha)$.\n    - After testing all $\\alpha$ values, it will evaluate the \"robust-yet-fragile\" condition based on the collected cascade sizes.\n\n5.  **Output Formatting**: Finally, the `solve()` function will aggregate the results from all test cases. It will construct the final output string manually to ensure it is a single line with no whitespace, with boolean `True`/`False` converted to lowercase `true`/`false` as is common in data interchange formats.\n\nThis structured, principle-based design ensures that every detail of the complex problem statement is addressed correctly and efficiently.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom collections import deque\n\ndef solve():\n    \"\"\"\n    Main function to solve the cascading failure problem.\n    It encapsulates all logic, including network generation, cascade simulation,\n    and result formatting, as per the problem specification.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (60, 2.5, 12345, [0.0, 0.1, 0.2, 0.5]),\n        (60, 3.5, 54321, [0.0, 0.1, 0.2, 0.5]),\n        (40, 2.2, 10101, [0.0, 0.05, 0.1, 0.2, 0.3, 1.0]),\n        (50, 2.8, 20202, [0.0, 0.02, 0.05, 0.1, 0.2]),\n    ]\n    \n    def _generate_degrees(N, gamma, k_min, k_max, rng):\n        \"\"\"Generates a degree sequence from a truncated power-law distribution.\"\"\"\n        # Using Inverse Transform Sampling for the continuous power-law\n        u = rng.random(N)\n        if gamma == 1.0: # Should not happen with given gammas, but for completeness\n            exp_term = np.exp(u * (np.log(k_max) - np.log(k_min)))\n            continuous_degrees = k_min * exp_term\n        else:\n            g = 1.0 - gamma\n            k_min_g = k_min**g\n            k_max_g = k_max**g\n            continuous_degrees = (u * (k_max_g - k_min_g) + k_min_g)**(1.0/g)\n\n        degrees = np.round(continuous_degrees).astype(int)\n        degrees = np.clip(degrees, k_min, k_max)\n\n        # Ensure sum of degrees is even for graph realizability\n        if np.sum(degrees) % 2 != 0:\n            indices = np.arange(N)\n            rng.shuffle(indices)\n            for i in indices:\n                if degrees[i] + 1 = k_max:\n                    degrees[i] += 1\n                    break\n                elif degrees[i] - 1 = k_min:\n                    degrees[i] -= 1\n                    break\n        return degrees\n\n    def _generate_network(N, gamma, k_min, k_max, rng):\n        \"\"\"Generates a scale-free network using the Chung-Lu model.\"\"\"\n        degrees = _generate_degrees(N, gamma, k_min, k_max, rng)\n        sum_k = np.sum(degrees)\n        adj_matrix = np.zeros((N, N), dtype=int)\n\n        if sum_k == 0:\n            return adj_matrix, degrees\n\n        for i in range(N):\n            for j in range(i + 1, N):\n                p_ij = min(1.0, (degrees[i] * degrees[j]) / sum_k)\n                if rng.random()  p_ij:\n                    adj_matrix[i, j] = 1\n                    adj_matrix[j, i] = 1\n        \n        # Recalculate degrees from the generated graph, as Chung-Lu model only matches expected degrees.\n        actual_degrees = np.sum(adj_matrix, axis=1)\n        return adj_matrix, actual_degrees\n\n    def _calculate_betweenness(adj_list, active_nodes):\n        \"\"\"Calculates betweenness centrality using Brandes' algorithm for unweighted graphs.\"\"\"\n        nodes_list = list(active_nodes)\n        betweenness = {node: 0.0 for node in nodes_list}\n\n        for s in nodes_list:\n            S = []\n            P = {v: [] for v in nodes_list}\n            sigma = {v: 0.0 for v in nodes_list}; sigma[s] = 1.0\n            d = {v: -1 for v in nodes_list}; d[s] = 0\n            \n            Q = deque([s])\n\n            while Q:\n                v = Q.popleft()\n                S.append(v)\n                for w in adj_list.get(v, []):\n                    if w not in active_nodes:\n                        continue\n                    if d[w]  0:\n                        Q.append(w)\n                        d[w] = d[v] + 1\n                    if d[w] == d[v] + 1:\n                        sigma[w] += sigma[v]\n                        P[w].append(v)\n            \n            delta = {v: 0.0 for v in nodes_list}\n            while S:\n                w = S.pop()\n                for v in P[w]:\n                    if sigma[w] != 0:\n                        delta[v] += (sigma[v] / sigma[w]) * (1.0 + delta[w])\n                if w != s:\n                    betweenness[w] += delta[w]\n        \n        # For undirected graphs, divide by 2\n        for node in betweenness:\n            betweenness[node] /= 2.0\n            \n        return betweenness\n\n    def _run_single_case(case_params):\n        \"\"\"Runs the entire simulation for a single test case.\"\"\"\n        N, gamma, seed, alphas = case_params\n        k_min = 2\n        k_max = N // 2\n        rng = np.random.default_rng(seed)\n\n        adj_matrix, initial_degrees = _generate_network(N, gamma, k_min, k_max, rng)\n        adj_list = {i: list(np.where(adj_matrix[i] == 1)[0]) for i in range(N)}\n        \n        full_node_set = set(range(N))\n        initial_loads = _calculate_betweenness(adj_list, full_node_set)\n\n        # Find the node v* to remove\n        if not initial_loads: # Handle case of empty/edgeless graph\n            v_star = 0\n        else:\n            max_load = -1.0\n            candidates = []\n            for i in range(N):\n                load_i = initial_loads.get(i, 0.0)\n                if load_i  max_load:\n                    max_load = load_i\n                    candidates = [i]\n                elif load_i == max_load:\n                    candidates.append(i)\n            \n            if len(candidates)  1:\n                max_deg = -1\n                deg_candidates = []\n                for i in candidates:\n                    if initial_degrees[i]  max_deg:\n                        max_deg = initial_degrees[i]\n                        deg_candidates = [i]\n                    elif initial_degrees[i] == max_deg:\n                        deg_candidates.append(i)\n                candidates = deg_candidates\n\n            v_star = min(candidates)\n\n        cascade_sizes = []\n        for alpha in alphas:\n            capacities = {i: (1 + alpha) * initial_loads.get(i, 0.0) for i in range(N)}\n            \n            removed_nodes = {v_star}\n            active_nodes = full_node_set - removed_nodes\n\n            while True:\n                if not active_nodes: break\n                \n                current_loads = _calculate_betweenness(adj_list, active_nodes)\n                newly_failed = set()\n                for i in active_nodes:\n                    load_i = current_loads.get(i, 0.0)\n                    # Node fails if load exceeds capacity. Capacity can be 0.\n                    if load_i  capacities[i]:\n                        newly_failed.add(i)\n                \n                if not newly_failed:\n                    break\n                \n                active_nodes -= newly_failed\n                removed_nodes.update(newly_failed)\n\n            S_alpha = len(removed_nodes) / N\n            cascade_sizes.append(S_alpha)\n\n        # Evaluate robust-yet-fragile condition\n        m = len(alphas)\n        num_robust = sum(1 for s in cascade_sizes if s = 0.1)\n        max_cascade = max(cascade_sizes) if cascade_sizes else 0.0\n        is_ryf = (num_robust = np.ceil(0.6 * m)) and (max_cascade = 0.5)\n\n        return cascade_sizes + [is_ryf]\n\n    results = []\n    for case in test_cases:\n        results.append(_run_single_case(case))\n\n    # Final print statement in the exact required format.\n    case_strings = []\n    for res_list in results:\n        str_items = []\n        for item in res_list:\n            if isinstance(item, bool):\n                str_items.append(str(item).lower())\n            else:\n                # Format to a reasonable number of decimal places for consistency\n                str_items.append(f\"{item:.10f}\".rstrip('0').rstrip('.') if '.' in f\"{item:.10f}\" else f\"{item:.0f}\")\n        case_strings.append(f\"[{','.join(str_items)}]\")\n    \n    print(f\"[{','.join(case_strings)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "After learning to simulate cascades, a critical next step is to identify a system's worst-case vulnerabilities. This practice frames the problem as one of combinatorial optimization: finding the specific set of initial failures that triggers the largest possible cascade. By searching for this optimal attack strategy, you will confront the computational complexity of the problem and discover why greedy approaches fail, a consequence of the cascade size function lacking a key property known as submodularity .",
            "id": "4266579",
            "problem": "You are given a finite, weighted, undirected network represented by a graph $G = (V, E)$ with $|V| = n$, a nonnegative base-load vector $\\ell \\in \\mathbb{R}_{\\ge 0}^{n}$, a nonnegative capacity vector $c \\in \\mathbb{R}_{\\ge 0}^{n}$, and a nonnegative, symmetric adjacency-weight matrix $W \\in \\mathbb{R}_{\\ge 0}^{n \\times n}$, where $W_{ij} = W_{ji}$ for all $i,j \\in V$, and $W_{ii} = 0$ for all $i \\in V$. For any node $i \\in V$, define its set of neighbors as $N(i) = \\{ j \\in V : W_{ij}  0 \\}$. Consider a discrete-time load-redistribution cascade with the following mechanics:\n\n- Loads evolve in rounds and are held in a vector $L \\in \\mathbb{R}_{\\ge 0}^{n}$; initially $L = \\ell$.\n- A targeted removal strategy is a subset $S \\subseteq V$ with $|S| = k$. At round $t = 0$, all nodes in $S$ are failed (removed), and each such node’s current load is redistributed to its surviving neighbors proportionally to the edge weights.\n- More precisely, when a node $i$ fails at the start of a round, compute the set of surviving neighbors $N_{\\text{surv}}(i) = \\{ j \\in N(i) : j \\text{ is not failed at the start of this round} \\}$. Let $\\sigma(i) = \\sum_{j \\in N_{\\text{surv}}(i)} W_{ij}$. If $\\sigma(i)  0$, each surviving neighbor $j \\in N_{\\text{surv}}(i)$ receives an added load of $L_i \\cdot \\frac{W_{ij}}{\\sigma(i)}$ in that round; if $\\sigma(i) = 0$, the load $L_i$ is dropped (lost).\n- After redistributions for all nodes failing in the current round are computed and applied to $L$, those failing nodes are removed permanently and do not receive further load. A surviving node $j$ fails in the next round whenever $L_j  c_j$. This synchronous update repeats until no further failures occur.\n- Define the cascade size function $f(S)$ as the total number of nodes that eventually fail (including those in $S$), starting from the initial removal set $S$.\n\nThe objective is to construct a worst-case targeted removal strategy $S^{\\star}$ of a given cardinality $k$ that maximizes the cascade size $f(S)$ under the stated load-redistribution rule, and to justify the optimality of your construction using either submodularity arguments or by providing counterexamples where submodularity fails and explaining why exhaustive search is required for exact optimality in such cases.\n\nYour program must implement the above cascade model and, for each test case below, compute the exact worst-case removal set $S^{\\star}$ and the corresponding maximum cascade size $f(S^{\\star})$ by searching all subsets $S \\subseteq V$ of size $k$. Ties in $f(S)$ should be broken by choosing the lexicographically smallest set $S$ when node indices are sorted in ascending order. All node indices are integers starting from $0$.\n\nAll answers are purely mathematical and unitless. Angles and physical units do not appear in this problem.\n\nTest Suite Specification:\n\nProvide results for the following test cases. In each case, the network is specified by listing nonzero symmetric edges and their weights, along with base loads $\\ell$, capacities $c$, and removal budget $k$.\n\n- Test Case $1$ (happy path star cascade):\n  - Nodes: $n = 5$, labeled $0, 1, 2, 3, 4$.\n  - Nonzero symmetric edges and weights: $(0,1)$ with weight $1$, $(0,2)$ with weight $1$, $(0,3)$ with weight $1$, $(0,4)$ with weight $1$.\n  - Base loads: $\\ell = [0.80, 0.31, 0.31, 0.31, 0.31]$.\n  - Capacities: $c = [1.20, 0.50, 0.50, 0.50, 0.50]$.\n  - Budget: $k = 1$.\n\n- Test Case $2$ (boundary, no removals):\n  - Nodes: $n = 4$, labeled $0, 1, 2, 3$.\n  - Nonzero symmetric edges and weights: $(0,1)$ with weight $1$, $(1,2)$ with weight $1$, $(2,3)$ with weight $1$.\n  - Base loads: $\\ell = [0.20, 0.20, 0.20, 0.20]$.\n  - Capacities: $c = [0.50, 0.50, 0.50, 0.50]$.\n  - Budget: $k = 0$.\n\n- Test Case $3$ (edge-case synergy and submodularity counterexample):\n  - Nodes: $n = 5$, labeled $0, 1, 2, 3, 4$.\n  - Nonzero symmetric edges and weights: $(0,2)$ with weight $1$, $(1,2)$ with weight $1$, $(2,3)$ with weight $1$, $(2,4)$ with weight $1$.\n  - Base loads: $\\ell = [0.50, 0.50, 0.60, 0.40, 0.40]$.\n  - Capacities: $c = [0.60, 0.60, 1.20, 0.50, 0.50]$.\n  - Budget: $k = 2$.\n\nRequired Final Output Format:\n\nYour program should produce a single line of output containing the results for the three test cases as a comma-separated list enclosed in square brackets. Each result must be a two-element list containing the maximum cascade size (an integer) and the corresponding lexicographically smallest maximizing set $S^{\\star}$ (a list of integers). For example, the output should look like $[[f_1, S_1],[f_2, S_2],[f_3, S_3]]$ where $f_i$ is the maximum cascade size for test case $i$ and $S_i$ is the maximizing set.\n\nYour program must not read any input and must not print any additional text beyond the single required output line.",
            "solution": "The provided problem statement is a valid, well-posed problem in the domain of network science and complex systems. It concerns the maximization of a cascade size function over a network by selecting an optimal initial set of failed nodes.\n\nThe problem is scientifically grounded, being a formal representation of load-based cascading failures, a phenomenon studied in contexts such as electrical power grids and financial networks. The dynamics are deterministic and defined with mathematical precision, ensuring that the cascade size function $f(S)$ is well-defined for any initial set of failures $S$. Given the finite number of nodes $n$ and a fixed removal budget $k$, an optimal set $S^{\\star}$ that maximizes $f(S)$ is guaranteed to exist. The problem is complete, providing all necessary parameters and rules, and contains no internal contradictions or ambiguities. The request to find an exact solution by exhaustive search is computationally feasible for the small network sizes specified in the test cases.\n\nWe will proceed by first formalizing the cascade model, then discussing the optimization approach and its justification, and finally outlining the algorithm for implementation.\n\n**1. The Cascade Model**\n\nThe state of the network at any discrete time step $t \\ge 0$ can be described by the load vector $L^{(t)} \\in \\mathbb{R}_{\\ge 0}^{n}$ and the set of failed nodes, let's call it $\\mathcal{F}^{(t)} \\subseteq V$.\n\n*   **Initialization ($t=0$):**\n    A set of nodes $S \\subseteq V$ with $|S|=k$ is selected for initial removal. The set of all failed nodes is initialized as $\\mathcal{F}^{(0)} = S$. The initial load vector is the base-load vector, $L = \\ell$. However, the problem specifies that the nodes in $S$ are the first to fail and redistribute their load. We can model this as a sequence of rounds. Let $\\mathcal{F}_{\\text{total}}$ be the set of all nodes that have failed up to the current point, initialized to $S$. Let $F_0 = S$ be the set of nodes failing in round $0$.\n\n*   **Cascade Rounds ($t=0, 1, 2, \\dots$):**\n    The cascade proceeds in synchronous rounds. In each round $t$, a set of nodes $F_t$ fails.\n\n    1.  If $F_t = \\emptyset$, the cascade has terminated. The process stops.\n    2.  An intermediate load increment vector, $\\Delta L^{(t)} \\in \\mathbb{R}_{\\ge 0}^{n}$, is initialized to all zeros.\n    3.  For each node $i \\in F_t$ that is failing in the current round, its load $L_i$ must be redistributed. The set of its neighbors that have not yet failed is $N_{\\text{surv}}(i) = N(i) \\setminus \\mathcal{F}_{\\text{total}}$.\n    4.  The sum of weights to these surviving neighbors is $\\sigma(i) = \\sum_{j \\in N_{\\text{surv}}(i)} W_{ij}$.\n    5.  If $\\sigma(i)  0$, each surviving neighbor $j \\in N_{\\text{surv}}(i)$ receives an additional load. The increment for node $j$ due to the failure of $i$ is $L_i \\cdot \\frac{W_{ij}}{\\sigma(i)}$. This value is added to $\\Delta L_j^{(t)}$. If $\\sigma(i)=0$, the load $L_i$ is lost.\n    6.  After computing the load increments from all nodes in $F_t$, the loads of the surviving nodes are updated synchronously: $L_j \\leftarrow L_j + \\Delta L_j^{(t)}$ for all $j \\notin \\mathcal{F}_{\\text{total}}$. Note that the loads of failing nodes themselves are not updated further.\n    7.  The set of all failed nodes is updated: $\\mathcal{F}_{\\text{total}} \\leftarrow \\mathcal{F}_{\\text{total}} \\cup F_t$.\n    8.  A new set of nodes, $F_{t+1}$, is identified to fail in the next round. These are the nodes that are not yet failed but whose new load exceeds their capacity:\n        $$F_{t+1} = \\{ j \\in V \\setminus \\mathcal{F}_{\\text{total}} : L_j  c_j \\}$$\n    9.  The process repeats for round $t+1$ with the set $F_{t+1}$.\n\nThe total size of the cascade for an initial set $S$ is given by the function $f(S) = |\\mathcal{F}_{\\text{final}}|$, where $\\mathcal{F}_{\\text{final}}$ is the set $\\mathcal{F}_{\\text{total}}$ when the process terminates.\n\n**2. Optimization and Justification**\n\nThe objective is to find a set $S^{\\star}$ that solves the following optimization problem:\n$$ S^{\\star} = \\underset{S \\subseteq V, |S|=k}{\\text{argmax}} \\; f(S) $$\nwith ties broken by selecting the lexicographically smallest set $S$.\n\nThis is a combinatorial optimization problem. For many such problems, if the objective function $f(S)$ is submodular, a simple greedy algorithm can provide a solution with a performance guarantee. A set function $f$ is submodular if it exhibits a \"diminishing returns\" property. Formally, for any sets $A \\subseteq B \\subseteq V$ and any element $v \\in V \\setminus B$, it must hold that:\n$$ f(A \\cup \\{v\\}) - f(A) \\ge f(B \\cup \\{v\\}) - f(B) $$\nHowever, cascade size functions in threshold models are often not submodular. The problem statement correctly alludes to this possibility. We can use Test Case 3 to demonstrate the failure of submodularity.\n\nLet $f$ be the cascade size function for Test Case 3.\n-   Let $A = \\{0\\}$ and $B = \\emptyset$. Note that $B \\subset A$. Let's evaluate the marginal gain of adding node $\\{1\\}$.\n-   $f(\\emptyset) = 0$, as no nodes are initially overloaded.\n-   $f(\\{0\\})$: Node $0$ fails. Its load $L_0=0.5$ is transferred to its only neighbor, node $2$. Node $2$'s load becomes $L_2' = 0.6 + 0.5 = 1.1$. This is less than its capacity $c_2 = 1.2$. The cascade stops. Thus, $f(\\{0\\}) = 1$.\n-   $f(\\{1\\})$: By symmetry with node $0$, node $1$'s failure also does not trigger a cascade. Thus, $f(\\{1\\}) = 1$.\n-   $f(\\{0, 1\\})$: Nodes $0$ and $1$ fail simultaneously. Both redistribute their loads ($L_0=0.5, L_1=0.5$) to their common neighbor, node $2$. Node $2$'s load becomes $L_2' = 0.6 + 0.5 + 0.5 = 1.6$. This exceeds its capacity $c_2 = 1.2$, so node $2$ fails in the next round. When node $2$ fails, its new load of $1.6$ is redistributed to its surviving neighbors $\\{3, 4\\}$, which receive $0.8$ each. Their loads become $L_3' = 0.4+0.8=1.2  c_3=0.5$ and $L_4' = 0.4+0.8=1.2  c_4=0.5$. They both fail. The entire network collapses. Thus, $f(\\{0, 1\\}) = 5$.\n\nNow, let's check the submodularity inequality with $A = \\{0\\}$ and $B = \\emptyset$ (so $B \\subseteq A$) and element $v = 1$. The inequality requires $f(B \\cup \\{v\\}) - f(B) \\ge f(A \\cup \\{v\\}) - f(A)$.\n-   Marginal gain of adding node $1$ to the empty set $B$: $f(\\{1\\}) - f(\\emptyset) = 1 - 0 = 1$.\n-   Marginal gain of adding node $1$ to the set $A=\\{0\\}$: $f(\\{0,1\\}) - f(\\{0\\}) = 5 - 1 = 4$.\n\nWe have $1  4$, which means $f(B \\cup \\{v\\}) - f(B)  f(A \\cup \\{v\\}) - f(A)$. This violates the submodularity condition. The function exhibits synergy, or increasing returns, where the combined effect of failing two nodes is greater than the sum of their individual effects. Due to this lack of submodularity, greedy algorithms are not guaranteed to find the optimal solution. Therefore, an exhaustive search of all $\\binom{n}{k}$ possible initial sets is necessary to guarantee optimality, as required by the problem statement.\n\n**3. Algorithmic Approach**\n\nThe algorithm will consist of a main loop that orchestrates the search and a core function that simulates the cascade for a given initial set.\n\n1.  **Main Routine:** For each test case $(n, W, \\ell, c, k)$:\n    a. Initialize `max_cascade_size = -1` and `optimal_set = []`.\n    b. Generate all unique subsets $S$ of nodes of size $k$. The `itertools.combinations` function is suitable as it generates these subsets in lexicographical order.\n    c. For each subset $S$:\n        i. Call a simulation function, `simulate_cascade(S, n, W, l, c)`, which returns the final cascade size $f(S)$.\n        ii. If the returned size is greater than `max_cascade_size`, update `max_cascade_size` to this new size and set `optimal_set` to the current $S$. Because we are iterating through sets in lexicographical order, the first set that achieves the maximum possible size will be the correct one according to the tie-breaking rule.\n    d. Store the final (`max_cascade_size`, `optimal_set`) pair for the test case.\n\n2.  **`simulate_cascade(S, n, W, l, c)` Function:**\n    a. Initialize `loads = copy(l)`, `total_failed = set(S)`, and `newly_failed = set(S)`.\n    b. Enter a `while` loop that continues as long as `newly_failed` is not empty.\n    c. Inside the loop, let `failing_this_round = newly_failed`. Reset `newly_failed` to an empty set.\n    d. Initialize `load_increments = zeros(n)`.\n    e. For each node $i$ in `failing_this_round`:\n        i. Determine the set of surviving neighbors $N_{\\text{surv}}(i) = \\{ j \\mid W_{ij}0 \\text{ and } j \\notin \\text{total\\_failed} \\}$.\n        ii. Calculate $\\sigma(i) = \\sum_{j \\in N_{\\text{surv}}(i)} W_{ij}$.\n        iii. If $\\sigma(i)  0$, for each $j \\in N_{\\text{surv}}(i)$, add $loads[i] \\cdot \\frac{W_{ij}}{\\sigma(i)}$ to `load_increments[j]`.\n    f. After iterating through all nodes in `failing_this_round`, update the main `loads` vector: `loads += load_increments`.\n    g. Update the set of failed nodes: `total_failed.update(failing_this_round)`.\n    h. Iterate through all nodes $j \\in \\{0, \\dots, n-1\\}$. If $j \\notin \\text{total\\_failed}$ and $loads[j]  c[j]$, add $j$ to the `newly_failed` set.\n    i. The loop continues.\n    j. Once the loop terminates, return `len(total_failed)`.\n\nThis design faithfully implements the specified model and performs the required exhaustive search to find the exact optimal solution for each test case.",
            "answer": "```python\nimport numpy as np\nimport itertools\n\ndef solve():\n    \"\"\"\n    Solves the cascading failure problem for a set of predefined test cases.\n    It finds the initial removal set of size k that maximizes the total cascade size.\n    \"\"\"\n\n    test_cases = [\n        # Test Case 1 (happy path star cascade)\n        {\n            \"n\": 5,\n            \"edges\": [(0, 1, 1), (0, 2, 1), (0, 3, 1), (0, 4, 1)],\n            \"l\": [0.80, 0.31, 0.31, 0.31, 0.31],\n            \"c\": [1.20, 0.50, 0.50, 0.50, 0.50],\n            \"k\": 1,\n        },\n        # Test Case 2 (boundary, no removals)\n        {\n            \"n\": 4,\n            \"edges\": [(0, 1, 1), (1, 2, 1), (2, 3, 1)],\n            \"l\": [0.20, 0.20, 0.20, 0.20],\n            \"c\": [0.50, 0.50, 0.50, 0.50],\n            \"k\": 0,\n        },\n        # Test Case 3 (edge-case synergy and submodularity counterexample)\n        {\n            \"n\": 5,\n            \"edges\": [(0, 2, 1), (1, 2, 1), (2, 3, 1), (2, 4, 1)],\n            \"l\": [0.50, 0.50, 0.60, 0.40, 0.40],\n            \"c\": [0.60, 0.60, 1.20, 0.50, 0.50],\n            \"k\": 2,\n        },\n    ]\n\n    final_results = []\n\n    for case in test_cases:\n        n = case[\"n\"]\n        edges = case[\"edges\"]\n        l_vec = np.array(case[\"l\"], dtype=float)\n        c_vec = np.array(case[\"c\"], dtype=float)\n        k = case[\"k\"]\n\n        # Build the weighted adjacency matrix W\n        W = np.zeros((n, n), dtype=float)\n        for i, j, w in edges:\n            W[i, j] = w\n            W[j, i] = w\n\n        max_cascade_size = -1\n        best_s = []\n\n        # Generate all subsets of nodes of size k.\n        # itertools.combinations generates them in lexicographical order.\n        initial_sets = itertools.combinations(range(n), k)\n        \n        for s_tuple in initial_sets:\n            initial_failures = set(s_tuple)\n            \n            # Run the cascade simulation for the current set S\n            loads = np.copy(l_vec)\n            total_failed = set(initial_failures)\n            newly_failed = set(initial_failures)\n\n            while newly_failed:\n                failing_this_round = set(newly_failed)\n                newly_failed = set()\n                \n                # Update total failed set before calculating redistributions\n                # from this round's failures. This is to ensure a node failing\n                # in the same round doesn't receive load from another.\n                current_round_total_failed = total_failed.union(failing_this_round)\n                \n                load_increments = np.zeros(n)\n\n                for i in failing_this_round:\n                    surviving_neighbors = []\n                    weight_sum_surv = 0.0\n                    \n                    # Find surviving neighbors and sum of weights\n                    for j in range(n):\n                        if W[i, j]  0 and j not in current_round_total_failed:\n                            surviving_neighbors.append(j)\n                            weight_sum_surv += W[i, j]\n\n                    # Redistribute load\n                    if weight_sum_surv  0:\n                        for j in surviving_neighbors:\n                            load_increments[j] += loads[i] * (W[i, j] / weight_sum_surv)\n                \n                # Synchronous update of loads\n                loads += load_increments\n                \n                # Update total failed set after redistributions are calculated\n                total_failed.update(failing_this_round)\n\n                # Identify nodes failing in the next round\n                for j in range(n):\n                    if j not in total_failed and loads[j]  c_vec[j]:\n                        newly_failed.add(j)\n\n            current_cascade_size = len(total_failed)\n            \n            # Update best result found so far.\n            # Due to lexicographical order of combinations, the first set\n            # that achieves the max size will be the lexicographically smallest.\n            if current_cascade_size  max_cascade_size:\n                max_cascade_size = current_cascade_size\n                best_s = list(s_tuple)\n        \n        # Handle k=0 case which results in no loops\n        if k == 0 and max_cascade_size == -1:\n             max_cascade_size = 0\n             best_s = []\n\n        final_results.append([max_cascade_size, best_s])\n    \n    # Format the final output string exactly as specified.\n    result_str = \",\".join([f\"[{size},{s}]\" for size, s in final_results])\n    print(f\"[{result_str}]\".replace(\" \", \"\"))\n\n\nsolve()\n```"
        }
    ]
}