{
    "hands_on_practices": [
        {
            "introduction": "To begin our hands-on exploration, we will simulate a cascading failure in a setting inspired by an electrical power grid. This exercise requires you to implement the linearized Direct Current (DC) load flow model, a fundamental tool in power systems engineering, to understand how the failure of a single transmission line can lead to a sequence of subsequent overloads and outages. By coding this simulation, you will gain practical experience with the core mechanics of load redistribution and the iterative nature of cascades. ",
            "id": "4266617",
            "problem": "Consider a network of $N$ nodes and $E$ undirected lines, each line connecting a pair of nodes. The network is modeled using the linearized Direct Current (DC) load flow approximation. Let the net power injection vector be $P \\in \\mathbb{R}^N$, where $P_i$ is positive for generation and negative for load at node $i$, and let each line $e$ have a positive susceptance $b_e$ and a thermal capacity limit $C_e  0$. The DC model posits that the active power flow $f_e$ on line $e$ is proportional to the difference of voltage phase angles across its endpoints, and that nodal power balance is enforced by Kirchhoff's Current Law. You will compute flows, detect overloads, remove overloaded lines by a specific rule, and simulate the resulting cascade until a stable state is reached.\n\nFundamental base for modeling:\n- Kirchhoff's Current Law: The algebraic sum of flows incident to node $i$ equals the net injection $P_i$.\n- Linearized DC load flow relation: For any line $e = (u,v)$, the active power flow satisfies $f_e = b_e \\left(\\theta_u - \\theta_v\\right)$, where $\\theta_i$ is the voltage phase angle at node $i$.\n- Incidence-based formulation: With an oriented incidence matrix $A \\in \\mathbb{R}^{E \\times N}$, defined by $A_{e,u} = +1$ and $A_{e,v} = -1$ for a line $e=(u,v)$ oriented from node $u$ to node $v$ (and zeros elsewhere), Kirchhoff's Current Law becomes $A^\\top f = P$, and the linearized DC relation becomes $f = \\operatorname{diag}(b) \\, A \\, \\theta$, leading to the weighted Laplacian system $A^\\top \\operatorname{diag}(b) A \\, \\theta = P$ on each connected component, after fixing a reference phase angle. The weighted Laplacian is singular unless one reference (slack) angle per component is fixed.\n\nCascade procedure to implement:\n1. Compute line flows $f_e$ by solving $A^\\top \\operatorname{diag}(b) A \\, \\theta = P$ on each connected component, with a slack bus chosen as the node with the smallest index in that component and its angle fixed to zero. When a component has only one node, there are no flows in that component.\n2. Define the utilization ratio $r_e = \\left|f_e\\right| / C_e$ for each active line $e$.\n3. If there exists any line with $r_e  1$, remove exactly one line: the line with the largest $r_e$. If multiple lines tie for the largest $r_e$, remove the one with the smallest line index. Recompute connected components, and repeat from step $1$.\n4. Terminate when all remaining lines satisfy $r_e \\le 1$, or no lines remain. The final state is considered stable.\n\nHandling islanding:\n- The removal of lines may disconnect the network into multiple connected components (islands). In each island, define a slack bus as the node with the smallest index within that island. The slack absorbs any aggregate power imbalance in that island by adjusting its net injection implicitly via the fixed reference angle, which yields a solvable reduced Laplacian system for the other angles. This approach is purely mathematical and does not constrain slack capacity.\n\nYour program must implement this cascade simulation exactly and return, for each test case, the triple $[k, R, M]$ where:\n- $k$ is the integer number of lines removed until the final stable state,\n- $R$ is the final maximum utilization ratio across all remaining lines, defined as $R = \\max_e r_e$ with $r_e = \\left|f_e\\right| / C_e$, expressed as a real number rounded to six decimal places,\n- $M$ is the integer number of connected components (including isolated nodes) in the final network.\n\nUse the following test suite of parameter values. In all cases, nodes are labeled by integers starting at $0$, each line is specified by its $(u,v)$ endpoints, susceptance $b_e$, and capacity $C_e$, and the orientation for $A$ is from the first endpoint $u$ to the second endpoint $v$.\n\nTest case $1$ (ring with a chord):\n- $N = 4$\n- Lines: $e_0 = (0,1,b_0=5,C_0=8)$, $e_1 = (1,2,b_1=5,C_1=8)$, $e_2 = (2,3,b_2=5,C_2=8)$, $e_3 = (3,0,b_3=5,C_3=8)$, $e_4 = (1,3,b_4=5,C_4=6)$\n- Net injections: $P = [3,-1,-2,0]$\n\nTest case $2$ (star with one tight limit):\n- $N = 5$\n- Lines: $e_0 = (0,1,b_0=4,C_0=4)$, $e_1 = (0,2,b_1=4,C_1=4)$, $e_2 = (0,3,b_2=4,C_2=4)$, $e_3 = (0,4,b_3=4,C_3=0.5)$\n- Net injections: $P = [4,-1,-1,-1,-1]$\n\nTest case $3$ (triangle, generous limits):\n- $N = 3$\n- Lines: $e_0 = (0,1,b_0=10,C_0=100)$, $e_1 = (1,2,b_1=10,C_1=100)$, $e_2 = (0,2,b_2=10,C_2=100)$\n- Net injections: $P = [1,-0.5,-0.5]$\n\nTest case $4$ (chain at exact limit):\n- $N = 3$\n- Lines: $e_0 = (0,1,b_0=10,C_0=5)$, $e_1 = (1,2,b_1=10,C_1=5)$\n- Net injections: $P = [5,0,-5]$\n\nTest case $5$ (chain with tie-breaking on overload):\n- $N = 4$\n- Lines: $e_0 = (0,1,b_0=10,C_0=3)$, $e_1 = (1,2,b_1=10,C_1=2)$, $e_2 = (2,3,b_2=10,C_2=100)$\n- Net injections: $P = [6,-2,-2,-2]$\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes a sub-list $[k,R,M]$. For example, the output must look like $[[k_1,R_1,M_1],[k_2,R_2,M_2],\\dots]$ with $R_i$ rounded to six decimal places.",
            "solution": "The problem requires the simulation of a cascading failure in an electrical power network, modeled using the linearized Direct Current (DC) load flow approximation. The simulation proceeds in discrete steps, where at each step, power flows are calculated, and the most overloaded transmission line is removed, potentially leading to network fragmentation (islanding) and further redistributions of flow until a stable state is reached.\n\nThe core of the simulation rests upon solving the DC power flow equations for each connected component of the network. The governing equation is a linear system derived from Kirchhoff's Current Law and an Ohm's Law-like relationship for active power.\n\nLet the network have $N$ nodes and $E$ lines. We define an oriented incidence matrix $A \\in \\mathbb{R}^{E \\times N}$, where for a line $e$ oriented from node $u$ to node $v$, $A_{e,u} = 1$ and $A_{e,v} = -1$, with all other entries in that row being $0$. Let $\\theta \\in \\mathbb{R}^N$ be the vector of nodal voltage phase angles, $P \\in \\mathbb{R}^N$ be the vector of net power injections, $b \\in \\mathbb{R}^E$ be the vector of line susceptances, and $f \\in \\mathbb{R}^E$ be the vector of power flows on the lines.\n\nThe model is defined by two fundamental relationships:\n1.  The flow $f_e$ on a line $e=(u,v)$ with susceptance $b_e$ is given by $f_e = b_e(\\theta_u - \\theta_v)$. In matrix form, this is $f = \\mathrm{diag}(b) A \\theta$.\n2.  The sum of power flows entering or leaving any node must balance the net power injection at that node. This is Kirchhoff's Current Law (KCL), expressed as $A^\\top f = P$.\n\nSubstituting the first equation into the second yields the system equation for the phase angles:\n$$\nA^\\top \\left( \\mathrm{diag}(b) A \\theta \\right) = P\n$$\n$$\n(A^\\top \\mathrm{diag}(b) A) \\theta = P\n$$\nThis is often written as $L \\theta = P$, where $L = A^\\top \\mathrm{diag}(b) A$ is the weighted Laplacian matrix of the network. The Laplacian matrix for any graph is singular, reflecting the physical reality that only phase angle *differences* determine power flow, not their absolute values. To obtain a unique solution, we must establish a reference.\n\nThe problem specifies a procedure to handle this singularity and the potential fragmentation of the network:\n1.  **Identify Connected Components**: The network is partitioned into its connected components using a graph traversal algorithm such as Breadth-First Search (BFS) or Depth-First Search (DFS). Each component is treated as an independent sub-problem.\n\n2.  **Solve for Each Component**: For each connected component $\\mathcal{C}$ with node set $\\mathcal{V}_\\mathcal{C}$ and line set $\\mathcal{E}_\\mathcal{C}$:\n    - If $|\\mathcal{V}_\\mathcal{C}| \\le 1$, there are no lines within the component, so no flows are calculated.\n    - If $|\\mathcal{V}_\\mathcal{C}|  1$, a reference or \"slack\" node must be chosen. The problem mandates selecting the node with the smallest index within $\\mathcal{V}_\\mathcal{C}$ as the slack node, $s$. Its phase angle is fixed to $\\theta_s = 0$.\n    - Let $\\mathcal{V'}_\\mathcal{C} = \\mathcal{V}_\\mathcal{C} \\setminus \\{s\\}$ be the set of non-slack nodes in the component. We construct a reduced linear system $L' \\theta' = P'$ to solve for the angles $\\theta'$ of the non-slack nodes.\n    - $L'$ is the submatrix of the component's Laplacian $L_\\mathcal{C}$ obtained by removing the row and column corresponding to the slack node $s$. This reduced matrix $L'$ is invertible for a connected component.\n    - $P'$ is the subvector of the power injection vector $P$ corresponding to the non-slack nodes $\\mathcal{V'}_\\mathcal{C}$. The power imbalance in the component, $\\sum_{i \\in \\mathcal{V}_\\mathcal{C}} P_i$, is implicitly absorbed by the slack node.\n    - The non-slack angles are found by solving the linear system: $\\theta' = (L')^{-1} P'$.\n    - With all nodal angles $\\theta$ for the component determined (including $\\theta_s=0$), the flow $f_e$ on each line $e=(u,v)$ within that component is calculated: $f_e = b_e(\\theta_u - \\theta_v)$.\n\nThe overall simulation proceeds as a loop:\n\n**Step I: Initialization**\n- The simulation starts with the full set of lines. The number of removed lines, $k$, is initialized to $0$.\n\n**Step II: Iterative Cascade**\nThe simulation enters a loop that continues until no lines are overloaded.\n- **a) Flow Calculation**: For the current network topology, the procedure described above (component identification and per-component linear system solution) is executed to compute all line flows $f_e$.\n- **b) Overload Assessment**: For each line $e$ with capacity $C_e$, the utilization ratio is calculated: $r_e = |f_e| / C_e$. The maximum ratio, $r_{\\max} = \\max_e r_e$, is identified. If no lines remain, $r_{\\max}$ is taken to be $0$.\n- **c) Stability Check**: If $r_{\\max} \\le 1$, the network is stable. The simulation terminates.\n- **d) Line Removal**: If $r_{\\max}  1$, the network is unstable. A single line must be removed. The line $e^*$ to be removed is the one with the maximum utilization ratio $r_{e^*} = r_{\\max}$. If there is a tie, the line with the smallest original index is chosen. The line $e^*$ is removed from the network, $k$ is incremented by $1$, and the loop repeats from Step II-a.\n\n**Step III: Final Output**\nUpon termination, the final state is characterized by:\n- $k$: The total number of lines removed during the cascade.\n- $R$: The final maximum utilization ratio, $r_{\\max}$, rounded to six decimal places.\n- $M$: The number of connected components in the final, stable network graph.\n\nThis comprehensive algorithm deterministically simulates the cascading failure process as specified. Its implementation requires robust graph algorithms for component finding and numerical linear algebra routines to solve the matrix equations.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef find_components(num_nodes, active_lines_indices, lines_def):\n    \"\"\"Finds connected components in the graph using BFS.\"\"\"\n    if num_nodes == 0:\n        return 0, []\n        \n    adj = [[] for _ in range(num_nodes)]\n    for line_idx in active_lines_indices:\n        u, v, _, _, _ = lines_def[line_idx]\n        adj[u].append(v)\n        adj[v].append(u)\n\n    visited = [False] * num_nodes\n    components = []\n    for i in range(num_nodes):\n        if not visited[i]:\n            component_nodes = set()\n            q = [i]\n            visited[i] = True\n            head = 0\n            while head  len(q):\n                u = q[head]\n                head += 1\n                component_nodes.add(u)\n                for v in adj[u]:\n                    if not visited[v]:\n                        visited[v] = True\n                        q.append(v)\n            components.append(sorted(list(component_nodes)))\n    return len(components), components\n\ndef solve_cascade(num_nodes, lines, power_injections):\n    \"\"\"\n    Simulates the cascading failure process.\n    \"\"\"\n    lines_def = [(u, v, b, c, idx) for idx, (u, v, b, c) in enumerate(lines)]\n    active_lines_indices = set(range(len(lines_def)))\n    \n    removed_lines_count = 0\n\n    while True:\n        if not active_lines_indices:\n            num_components, _ = find_components(num_nodes, active_lines_indices, lines_def)\n            return removed_lines_count, 0.0, num_components\n\n        num_components, components = find_components(num_nodes, active_lines_indices, lines_def)\n\n        flows = {}  # Using dict to store flows by line index\n\n        for comp_nodes in components:\n            if len(comp_nodes) = 1:\n                continue\n\n            # Map component-local indices to global node indices\n            node_map = {node_idx: i for i, node_idx in enumerate(comp_nodes)}\n            \n            comp_lines = []\n            for line_idx in active_lines_indices:\n                u, v, _, _, _ = lines_def[line_idx]\n                if u in comp_nodes and v in comp_nodes:\n                    comp_lines.append(line_idx)\n            \n            if not comp_lines:\n                continue\n\n            comp_size = len(comp_nodes)\n            laplacian = np.zeros((comp_size, comp_size))\n\n            # Build weighted Laplacian for the component\n            for line_idx in comp_lines:\n                u, v, b, _, _ = lines_def[line_idx]\n                u_comp, v_comp = node_map[u], node_map[v]\n                laplacian[u_comp, u_comp] += b\n                laplacian[v_comp, v_comp] += b\n                laplacian[u_comp, v_comp] -= b\n                laplacian[v_comp, u_comp] -= b\n            \n            # Select slack bus (smallest index in component)\n            slack_node_global = comp_nodes[0]\n            slack_node_comp = node_map[slack_node_global]\n            \n            non_slack_indices_comp = [i for i in range(comp_size) if i != slack_node_comp]\n            non_slack_indices_global = [node for node in comp_nodes if node != slack_node_global]\n            \n            # Reduced system\n            reduced_laplacian = laplacian[np.ix_(non_slack_indices_comp, non_slack_indices_comp)]\n            reduced_p = np.array([power_injections[i] for i in non_slack_indices_global])\n\n            # Solve for non-slack angles\n            try:\n                non_slack_thetas = np.linalg.solve(reduced_laplacian, reduced_p)\n            except np.linalg.LinAlgError:\n                # This can happen if a component is just a line, making L' singular.\n                # It's an issue with how the problem is defined, but we must handle it.\n                # A better approach would be pseudo-inverse, but we stick to the problem.\n                # For safety, use pinv if solve fails.\n                non_slack_thetas = np.linalg.pinv(reduced_laplacian) @ reduced_p\n\n            # Reconstruct full theta vector for the component\n            thetas_comp = np.zeros(comp_size)\n            thetas_comp[non_slack_indices_comp] = non_slack_thetas\n            \n            thetas_global = {comp_nodes[i]: thetas_comp[i] for i in range(comp_size)}\n\n            # Calculate flows for lines in this component\n            for line_idx in comp_lines:\n                u, v, b, _, _ = lines_def[line_idx]\n                flow = b * (thetas_global[u] - thetas_global[v])\n                flows[line_idx] = flow\n\n        # Assess overload\n        max_ratio = -1.0\n        line_to_remove = -1\n\n        ratios = []\n        for line_idx in sorted(list(active_lines_indices)):\n            u, v, b, c, _ = lines_def[line_idx]\n            flow_val = flows.get(line_idx, 0.0)\n            ratio = abs(flow_val) / c if c  0 else float('inf')\n            ratios.append((ratio, line_idx))\n\n        if not ratios:\n            # This case is handled at the loop start, but as a safeguard\n            max_ratio_val = 0.0\n        else:\n            # Sort by ratio (desc), then line index (asc)\n            ratios.sort(key=lambda x: (-x[0], x[1]))\n            max_ratio_val = ratios[0][0]\n            line_to_remove = ratios[0][1]\n\n        if max_ratio_val = 1.0:\n            final_max_ratio = max_ratio_val if ratios else 0.0\n            return removed_lines_count, final_max_ratio, num_components\n        else:\n            active_lines_indices.remove(line_to_remove)\n            removed_lines_count += 1\n\ndef solve():\n    test_cases = [\n        {\n            \"N\": 4,\n            \"lines\": [(0, 1, 5, 8), (1, 2, 5, 8), (2, 3, 5, 8), (3, 0, 5, 8), (1, 3, 5, 6)],\n            \"P\": [3, -1, -2, 0]\n        },\n        {\n            \"N\": 5,\n            \"lines\": [(0, 1, 4, 4), (0, 2, 4, 4), (0, 3, 4, 4), (0, 4, 4, 0.5)],\n            \"P\": [4, -1, -1, -1, -1]\n        },\n        {\n            \"N\": 3,\n            \"lines\": [(0, 1, 10, 100), (1, 2, 10, 100), (0, 2, 10, 100)],\n            \"P\": [1, -0.5, -0.5]\n        },\n        {\n            \"N\": 3,\n            \"lines\": [(0, 1, 10, 5), (1, 2, 10, 5)],\n            \"P\": [5, 0, -5]\n        },\n        {\n            \"N\": 4,\n            \"lines\": [(0, 1, 10, 3), (1, 2, 10, 2), (2, 3, 10, 100)],\n            \"P\": [6, -2, -2, -2]\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        k, R, M = solve_cascade(case[\"N\"], case[\"lines\"], case[\"P\"])\n        # Format the result with R rounded to 6 decimal places\n        results.append(f\"[{k},{R:.6f},{M}]\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "Moving from a specific engineering model to a more general network science framework, this practice explores cascades on scale-free networks, which are characteristic of many real-world systems. You will define a node's load based on its betweenness centrality—a measure of its importance in routing traffic—and simulate how failures propagate when nodes have limited capacity. This exercise provides direct insight into the famous \"robust-yet-fragile\" nature of scale-free networks, where they are resilient to random failures but vulnerable to targeted attacks on high-load hubs. ",
            "id": "4266621",
            "problem": "Consider a simple undirected network model with $N$ nodes, where the degree distribution is heavy-tailed and approximates a power law $P(k) \\propto k^{-\\gamma}$. We study cascading failures induced by load redistribution when flows are routed along shortest paths. The governing principles are: shortest path routing between all pairs of nodes, node load defined by betweenness centrality, a fixed capacity per node derived from the initial load and a nonnegative tolerance, and iterative failure when current load exceeds fixed capacity.\n\nDefinitions and fundamental base:\n1. Let the network be an undirected simple graph $G = (V,E)$ with $|V| = N$ nodes and adjacency relation $A_{ij} \\in \\{0,1\\}$ indicating the presence of a link between node $i$ and node $j$. Let the degree of node $i$ be $k_i = \\sum_{j} A_{ij}$.\n2. Assume flows are routed along shortest paths. For each distinct unordered pair of nodes $\\{s,t\\}$, consider all shortest paths from $s$ to $t$. The betweenness centrality $B(v)$ of node $v$ is defined as\n$$\nB(v) = \\sum_{\\substack{s,t \\in V \\\\ s \\neq t \\neq v}} \\frac{\\sigma_{st}(v)}{\\sigma_{st}},\n$$\nwhere $\\sigma_{st}$ is the number of shortest paths between $s$ and $t$, and $\\sigma_{st}(v)$ is the number of those shortest paths that pass through node $v$. This definition is for undirected graphs without edge weights.\n3. The initial load $L_i^{(0)}$ on node $i$ is defined by its betweenness centrality computed on the intact network $G$, namely $L_i^{(0)} = B(i)$.\n4. Each node $i$ is assigned a fixed capacity $C_i$ proportional to its initial load, with a nonnegative tolerance parameter $\\alpha$, by\n$$\nC_i = (1+\\alpha) \\, L_i^{(0)}.\n$$\n5. A cascade is triggered by removing a single node that represents a typical high-impact attack. Here, remove the node $v^\\star$ with the largest initial load $L_{v^\\star}^{(0)}$ (breaking ties by highest degree and then lowest index). After removal, recompute loads $L_i^{(t)}$ on the residual graph at each cascade iteration $t$ via the same betweenness centrality definition restricted to the remaining nodes. In each iteration, any remaining node $i$ whose current load $L_i^{(t)}$ exceeds its fixed capacity $C_i$ fails and is removed from the network. The cascade stops when no remaining node is overloaded.\n6. The cascade size $S(\\alpha)$ is defined as the fraction of nodes removed at termination (including the initially attacked node), expressed as a decimal in $[0,1]$.\n\nGoal:\nStarting from these principles, implement a program that:\n- Generates a scale-free network with $P(k) \\propto k^{-\\gamma}$ using an expected-degree construction and samples degrees $k_i$ from a truncated power-law between $k_{\\min}$ and $k_{\\max}$. Use the Chung–Lu expected-degree model to place edges independently with probability $p_{ij} = \\min\\left(1, \\frac{k_i k_j}{\\sum_{\\ell} k_{\\ell}} \\right)$ for $i \\neq j$, yielding an undirected simple graph. The model must avoid self-loops and multi-edges.\n- Computes node betweenness centrality using the Brandes algorithm for unweighted undirected graphs to obtain loads.\n- Simulates the cascading failure process under the fixed-capacity rule $C_i = (1+\\alpha)L_i^{(0)}$ for each specified tolerance $\\alpha$.\n- Returns the cascade size $S(\\alpha)$ for each $\\alpha$ tested and identifies robust-yet-fragile regimes as follows: label a parameter set as robust-yet-fragile if at least $\\lceil 0.6 m \\rceil$ of the tested tolerance values (where $m$ is the number of $\\alpha$ values in that set) yield cascade sizes $\\leq 0.1$ while the maximum cascade size across the tested $\\alpha$ values is $\\geq 0.5$.\n\nAll computations are purely combinatorial and unitless, since loads and capacities are defined via path counts. Angles are not involved. Return all cascade sizes as decimals. No percentages are permitted.\n\nTest suite:\nYour program must use the following test suite of parameter sets, each specified as a tuple $(N,\\gamma,\\text{seed},\\{ \\alpha \\text{ values} \\})$. The random seed must be set to ensure deterministic results.\n\n- Case A (happy path, heavy tail with moderate $N$): $(N=\\;60,\\;\\gamma=\\;2.5,\\;\\text{seed}=\\;12345,\\;\\{\\alpha=\\;0.0,\\;0.1,\\;0.2,\\;0.5\\})$.\n- Case B (lighter tail, same $N$): $(N=\\;60,\\;\\gamma=\\;3.5,\\;\\text{seed}=\\;54321,\\;\\{\\alpha=\\;0.0,\\;0.1,\\;0.2,\\;0.5\\})$.\n- Case C (heavier tail, smaller $N$, wider tolerance sweep): $(N=\\;40,\\;\\gamma=\\;2.2,\\;\\text{seed}=\\;10101,\\;\\{\\alpha=\\;0.0,\\;0.05,\\;0.1,\\;0.2,\\;0.3,\\;1.0\\})$.\n- Case D (boundary connectivity stress, moderate $N$, wide tail): $(N=\\;50,\\;\\gamma=\\;2.8,\\;\\text{seed}=\\;20202,\\;\\{\\alpha=\\;0.0,\\;0.02,\\;0.05,\\;0.1,\\;0.2\\})$.\n\nFor all cases, use $k_{\\min}=\\;2$ and $k_{\\max}=\\;\\left\\lfloor \\frac{N}{2} \\right\\rfloor$. Sampling must use the inverse cumulative distribution function for the continuous power law truncated to $[k_{\\min},k_{\\max}]$, then round to the nearest integer within bounds, ensuring $\\sum_i k_i$ is even by adjusting one randomly chosen degree by $\\pm 1$ within bounds if necessary.\n\nFinal output format:\nYour program should produce a single line of output containing a list with one element per test case. Each test case element is itself a list containing the cascade sizes for the specified $\\alpha$ values in order, followed by a boolean robust-yet-fragile flag. The entire output must contain no whitespace characters. For example, a line with two cases would look like\n$$\n[ [ s_{A,1}, s_{A,2}, \\dots, r_A ], [ s_{B,1}, s_{B,2}, \\dots, r_B ] ]\n$$\nbut with no spaces. Consequently, the required final output format is\n$$\n[[S_A(\\alpha_1),S_A(\\alpha_2),\\dots,\\text{flag}_A],[S_B(\\alpha_1),S_B(\\alpha_2),\\dots,\\text{flag}_B],[S_C(\\alpha_1),\\dots,\\text{flag}_C],[S_D(\\alpha_1),\\dots,\\text{flag}_D]],\n$$\nwhere each $S(\\alpha_j)$ is a decimal and each flag is a boolean.\n\nYour program must not read any input and must generate the test suite internally as specified. The output must be exactly one line, matching the format above, with no additional text.",
            "solution": "This problem requires implementing a multi-stage simulation of a load-based cascading failure on a stochastically generated scale-free network. The solution can be broken down into three main parts: network generation, centrality calculation, and the cascade simulation itself.\n\n**1. Network Generation**\n\nFirst, a scale-free network must be created for each test case. This is a two-step process, both of which must be seeded for reproducibility.\n\n*   **Degree Sequence Generation**: A sequence of desired degrees for the $N$ nodes is generated by sampling from a truncated power-law distribution. The problem specifies using the inverse transform sampling method on the continuous version of the distribution $P(k) \\propto k^{-\\gamma}$ between $k_{\\min}$ and $k_{\\max}$. For a uniform random variate $u \\in [0,1]$, the corresponding degree sample $k$ is given by $k(u) = \\left[ u(k_{\\max}^{1-\\gamma} - k_{\\min}^{1-\\gamma}) + k_{\\min}^{1-\\gamma} \\right]^{1/(1-\\gamma)}$. The results are rounded to the nearest integer and clamped within the $[k_{\\min}, k_{\\max}]$ bounds. Crucially, the sum of degrees must be even to form a valid graph. If the sum is odd, a single degree is adjusted by $\\pm 1$ (while respecting the bounds) to correct the parity.\n\n*   **Edge Generation (Chung–Lu Model)**: Given the target degree sequence $\\{k_i\\}$, the Chung–Lu model is used to place edges. For every pair of distinct nodes $(i, j)$, an edge is created with probability $p_{ij} = \\min\\left(1, \\frac{k_i k_j}{\\sum_{\\ell} k_{\\ell}}\\right)$. This process generates a simple, undirected graph whose expected degrees match the generated sequence. The actual degrees of the resulting graph should be used for subsequent tie-breaking.\n\n**2. Load, Capacity, and Attack Initialization**\n\nOnce the network is generated, the initial conditions for the cascade are established.\n\n*   **Initial Load Calculation**: The initial load on each node, $L_i^{(0)}$, is defined as its betweenness centrality on the intact network. This requires implementing the Brandes algorithm for unweighted graphs, which efficiently calculates centrality by performing a Breadth-First Search (BFS) from each node and accumulating dependencies.\n\n*   **Initial Attack Target**: The cascade is triggered by removing a single node $v^\\star$. This node is chosen deterministically: it is the node with the highest initial load $L^{(0)}$. Ties are broken by selecting the node with the highest degree, and any further ties are broken by selecting the node with the lowest index.\n\n**3. Cascade Simulation**\n\nFor each tolerance parameter $\\alpha$ in a test case, a full cascade simulation is run.\n\n*   **Capacity Calculation**: The fixed capacity of each node is calculated once at the start of the simulation for a given $\\alpha$: $C_i = (1+\\alpha)L_i^{(0)}$. This capacity does not change during the cascade.\n\n*   **Iterative Failure**: The simulation proceeds in rounds, starting with the removal of $v^\\star$.\n    1.  In each round, the set of active (non-failed) nodes is determined.\n    2.  The betweenness centrality (load) of every active node is recalculated on the subgraph induced by the active nodes.\n    3.  Each active node $i$'s current load is compared to its fixed capacity $C_i$. Any node where load exceeds capacity is marked as failed.\n    4.  If no new nodes fail in a round, the cascade terminates. Otherwise, the newly failed nodes are removed, and the process repeats from step 1.\n\n**4. Metrics and Output**\n\nAfter the cascade stabilizes, the final cascade size $S(\\alpha)$ is computed as the total number of failed nodes (including $v^\\star$) divided by the total number of nodes $N$. After running the simulation for all $\\alpha$ values in a test case, the \"robust-yet-fragile\" (RYF) boolean flag is determined based on the specified conditions. The final output is a formatted list containing the sequence of cascade sizes and the RYF flag for each test case.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom collections import deque\n\ndef solve():\n    \"\"\"\n    Main function to solve the cascading failure problem.\n    It encapsulates all logic, including network generation, cascade simulation,\n    and result formatting, as per the problem specification.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (60, 2.5, 12345, [0.0, 0.1, 0.2, 0.5]),\n        (60, 3.5, 54321, [0.0, 0.1, 0.2, 0.5]),\n        (40, 2.2, 10101, [0.0, 0.05, 0.1, 0.2, 0.3, 1.0]),\n        (50, 2.8, 20202, [0.0, 0.02, 0.05, 0.1, 0.2]),\n    ]\n    \n    def _generate_degrees(N, gamma, k_min, k_max, rng):\n        \"\"\"Generates a degree sequence from a truncated power-law distribution.\"\"\"\n        # Using Inverse Transform Sampling for the continuous power-law\n        u = rng.random(N)\n        if gamma == 1.0: # Should not happen with given gammas, but for completeness\n            exp_term = np.exp(u * (np.log(k_max) - np.log(k_min)))\n            continuous_degrees = k_min * exp_term\n        else:\n            g = 1.0 - gamma\n            k_min_g = k_min**g\n            k_max_g = k_max**g\n            continuous_degrees = (u * (k_max_g - k_min_g) + k_min_g)**(1.0/g)\n\n        degrees = np.round(continuous_degrees).astype(int)\n        degrees = np.clip(degrees, k_min, k_max)\n\n        # Ensure sum of degrees is even for graph realizability\n        if np.sum(degrees) % 2 != 0:\n            indices = np.arange(N)\n            rng.shuffle(indices)\n            for i in indices:\n                if degrees[i] + 1 = k_max:\n                    degrees[i] += 1\n                    break\n                elif degrees[i] - 1 = k_min:\n                    degrees[i] -= 1\n                    break\n        return degrees\n\n    def _generate_network(N, gamma, k_min, k_max, rng):\n        \"\"\"Generates a scale-free network using the Chung-Lu model.\"\"\"\n        degrees = _generate_degrees(N, gamma, k_min, k_max, rng)\n        sum_k = np.sum(degrees)\n        adj_matrix = np.zeros((N, N), dtype=int)\n\n        if sum_k == 0:\n            return adj_matrix, degrees\n\n        for i in range(N):\n            for j in range(i + 1, N):\n                p_ij = min(1.0, (degrees[i] * degrees[j]) / sum_k)\n                if rng.random()  p_ij:\n                    adj_matrix[i, j] = 1\n                    adj_matrix[j, i] = 1\n        \n        # Recalculate degrees from the generated graph, as Chung-Lu model only matches expected degrees.\n        actual_degrees = np.sum(adj_matrix, axis=1)\n        return adj_matrix, actual_degrees\n\n    def _calculate_betweenness(adj_list, active_nodes):\n        \"\"\"Calculates betweenness centrality using Brandes' algorithm for unweighted graphs.\"\"\"\n        nodes_list = list(active_nodes)\n        betweenness = {node: 0.0 for node in nodes_list}\n\n        for s in nodes_list:\n            S = []\n            P = {v: [] for v in nodes_list}\n            sigma = {v: 0.0 for v in nodes_list}; sigma[s] = 1.0\n            d = {v: -1 for v in nodes_list}; d[s] = 0\n            \n            Q = deque([s])\n\n            while Q:\n                v = Q.popleft()\n                S.append(v)\n                for w in adj_list.get(v, []):\n                    if w not in active_nodes:\n                        continue\n                    if d[w]  0:\n                        Q.append(w)\n                        d[w] = d[v] + 1\n                    if d[w] == d[v] + 1:\n                        sigma[w] += sigma[v]\n                        P[w].append(v)\n            \n            delta = {v: 0.0 for v in nodes_list}\n            while S:\n                w = S.pop()\n                for v in P[w]:\n                    if sigma[w] != 0:\n                        delta[v] += (sigma[v] / sigma[w]) * (1.0 + delta[w])\n                if w != s:\n                    betweenness[w] += delta[w]\n        \n        # For undirected graphs, divide by 2\n        for node in betweenness:\n            betweenness[node] /= 2.0\n            \n        return betweenness\n\n    def _run_single_case(case_params):\n        \"\"\"Runs the entire simulation for a single test case.\"\"\"\n        N, gamma, seed, alphas = case_params\n        k_min = 2\n        k_max = N // 2\n        rng = np.random.default_rng(seed)\n\n        adj_matrix, initial_degrees = _generate_network(N, gamma, k_min, k_max, rng)\n        adj_list = {i: list(np.where(adj_matrix[i] == 1)[0]) for i in range(N)}\n        \n        full_node_set = set(range(N))\n        initial_loads = _calculate_betweenness(adj_list, full_node_set)\n\n        # Find the node v* to remove\n        if not initial_loads: # Handle case of empty/edgeless graph\n            v_star = 0\n        else:\n            max_load = -1.0\n            candidates = []\n            for i in range(N):\n                load_i = initial_loads.get(i, 0.0)\n                if load_i  max_load:\n                    max_load = load_i\n                    candidates = [i]\n                elif load_i == max_load:\n                    candidates.append(i)\n            \n            if len(candidates)  1:\n                max_deg = -1\n                deg_candidates = []\n                for i in candidates:\n                    if initial_degrees[i]  max_deg:\n                        max_deg = initial_degrees[i]\n                        deg_candidates = [i]\n                    elif initial_degrees[i] == max_deg:\n                        deg_candidates.append(i)\n                candidates = deg_candidates\n\n            v_star = min(candidates)\n\n        cascade_sizes = []\n        for alpha in alphas:\n            capacities = {i: (1 + alpha) * initial_loads.get(i, 0.0) for i in range(N)}\n            \n            removed_nodes = {v_star}\n            active_nodes = full_node_set - removed_nodes\n\n            while True:\n                if not active_nodes: break\n                \n                current_loads = _calculate_betweenness(adj_list, active_nodes)\n                newly_failed = set()\n                for i in active_nodes:\n                    load_i = current_loads.get(i, 0.0)\n                    # Node fails if load exceeds capacity. Capacity can be 0.\n                    if load_i  capacities[i]:\n                        newly_failed.add(i)\n                \n                if not newly_failed:\n                    break\n                \n                active_nodes -= newly_failed\n                removed_nodes.update(newly_failed)\n\n            S_alpha = len(removed_nodes) / N\n            cascade_sizes.append(S_alpha)\n\n        # Evaluate robust-yet-fragile condition\n        m = len(alphas)\n        num_robust = sum(1 for s in cascade_sizes if s = 0.1)\n        max_cascade = max(cascade_sizes) if cascade_sizes else 0.0\n        is_ryf = (num_robust = np.ceil(0.6 * m)) and (max_cascade = 0.5)\n\n        return cascade_sizes + [is_ryf]\n\n    results = []\n    for case in test_cases:\n        results.append(_run_single_case(case))\n\n    # Final print statement in the exact required format.\n    case_strings = []\n    for res_list in results:\n        str_items = []\n        for item in res_list:\n            if isinstance(item, bool):\n                str_items.append(str(item).lower())\n            else:\n                # Format to a reasonable number of decimal places for consistency\n                str_items.append(f\"{item:.10f}\".rstrip('0').rstrip('.') if '.' in f\"{item:.10f}\" else f\"{item:.0f}\")\n        case_strings.append(f\"[{','.join(str_items)}]\")\n    \n    print(f\"[{','.join(case_strings)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Our final practice shifts the perspective from simply observing a cascade to strategically causing one. Here, your objective is to identify the worst-case initial attack on a network by finding the set of nodes whose removal triggers the largest possible cascade. This introduces you to the challenges of vulnerability analysis and combinatorial optimization, forcing you to grapple with the fact that such problems are often computationally hard and that simple greedy strategies may not work, a concept related to the mathematical property of submodularity. ",
            "id": "4266579",
            "problem": "You are given a finite, weighted, undirected network represented by a graph $G = (V, E)$ with $|V| = n$, a nonnegative base-load vector $\\ell \\in \\mathbb{R}_{\\ge 0}^{n}$, a nonnegative capacity vector $c \\in \\mathbb{R}_{\\ge 0}^{n}$, and a nonnegative, symmetric adjacency-weight matrix $W \\in \\mathbb{R}_{\\ge 0}^{n \\times n}$, where $W_{ij} = W_{ji}$ for all $i,j \\in V$, and $W_{ii} = 0$ for all $i \\in V$. For any node $i \\in V$, define its set of neighbors as $N(i) = \\{ j \\in V : W_{ij}  0 \\}$. Consider a discrete-time load-redistribution cascade with the following mechanics:\n\n- Loads evolve in rounds and are held in a vector $L \\in \\mathbb{R}_{\\ge 0}^{n}$; initially $L = \\ell$.\n- A targeted removal strategy is a subset $S \\subseteq V$ with $|S| = k$. At round $t = 0$, all nodes in $S$ are failed (removed), and each such node’s current load is redistributed to its surviving neighbors proportionally to the edge weights.\n- More precisely, when a node $i$ fails at the start of a round, compute the set of surviving neighbors $N_{\\text{surv}}(i) = \\{ j \\in N(i) : j \\text{ is not failed at the start of this round} \\}$. Let $\\sigma(i) = \\sum_{j \\in N_{\\text{surv}}(i)} W_{ij}$. If $\\sigma(i)  0$, each surviving neighbor $j \\in N_{\\text{surv}}(i)$ receives an added load of $L_i \\cdot \\frac{W_{ij}}{\\sigma(i)}$ in that round; if $\\sigma(i) = 0$, the load $L_i$ is dropped (lost).\n- After redistributions for all nodes failing in the current round are computed and applied to $L$, those failing nodes are removed permanently and do not receive further load. A surviving node $j$ fails in the next round whenever $L_j  c_j$. This synchronous update repeats until no further failures occur.\n- Define the cascade size function $f(S)$ as the total number of nodes that eventually fail (including those in $S$), starting from the initial removal set $S$.\n\nThe objective is to construct a worst-case targeted removal strategy $S^{\\star}$ of a given cardinality $k$ that maximizes the cascade size $f(S)$ under the stated load-redistribution rule, and to justify the optimality of your construction using either submodularity arguments or by providing counterexamples where submodularity fails and explaining why exhaustive search is required for exact optimality in such cases.\n\nYour program must implement the above cascade model and, for each test case below, compute the exact worst-case removal set $S^{\\star}$ and the corresponding maximum cascade size $f(S^{\\star})$ by searching all subsets $S \\subseteq V$ of size $k$. Ties in $f(S)$ should be broken by choosing the lexicographically smallest set $S$ when node indices are sorted in ascending order. All node indices are integers starting from $0$.\n\nAll answers are purely mathematical and unitless. Angles and physical units do not appear in this problem.\n\nTest Suite Specification:\n\nProvide results for the following test cases. In each case, the network is specified by listing nonzero symmetric edges and their weights, along with base loads $\\ell$, capacities $c$, and removal budget $k$.\n\n- Test Case $1$ (happy path star cascade):\n  - Nodes: $n = 5$, labeled $0, 1, 2, 3, 4$.\n  - Nonzero symmetric edges and weights: $(0,1)$ with weight $1$, $(0,2)$ with weight $1$, $(0,3)$ with weight $1$, $(0,4)$ with weight $1$.\n  - Base loads: $\\ell = [0.80, 0.31, 0.31, 0.31, 0.31]$.\n  - Capacities: $c = [1.20, 0.50, 0.50, 0.50, 0.50]$.\n  - Budget: $k = 1$.\n\n- Test Case $2$ (boundary, no removals):\n  - Nodes: $n = 4$, labeled $0, 1, 2, 3$.\n  - Nonzero symmetric edges and weights: $(0,1)$ with weight $1$, $(1,2)$ with weight $1$, $(2,3)$ with weight $1$.\n  - Base loads: $\\ell = [0.20, 0.20, 0.20, 0.20]$.\n  - Capacities: $c = [0.50, 0.50, 0.50, 0.50]$.\n  - Budget: $k = 0$.\n\n- Test Case $3$ (edge-case synergy and submodularity counterexample):\n  - Nodes: $n = 5$, labeled $0, 1, 2, 3, 4$.\n  - Nonzero symmetric edges and weights: $(0,2)$ with weight $1$, $(1,2)$ with weight $1$, $(2,3)$ with weight $1$, $(2,4)$ with weight $1$.\n  - Base loads: $\\ell = [0.50, 0.50, 0.60, 0.40, 0.40]$.\n  - Capacities: $c = [0.60, 0.60, 1.20, 0.50, 0.50]$.\n  - Budget: $k = 2$.\n\nRequired Final Output Format:\n\nYour program should produce a single line of output containing the results for the three test cases as a comma-separated list enclosed in square brackets. Each result must be a two-element list containing the maximum cascade size (an integer) and the corresponding lexicographically smallest maximizing set $S^{\\star}$ (a list of integers). For example, the output should look like $[[f_1, S_1],[f_2, S_2],[f_3, S_3]]$ where $f_i$ is the maximum cascade size for test case $i$ and $S_i$ is the maximizing set.\n\nYour program must not read any input and must not print any additional text beyond the single required output line.",
            "solution": "The provided problem statement is a valid, well-posed problem in the domain of network science and complex systems. It concerns the maximization of a cascade size function over a network by selecting an optimal initial set of failed nodes.\n\nThe problem is scientifically grounded, being a formal representation of load-based cascading failures, a phenomenon studied in contexts such as electrical power grids and financial networks. The dynamics are deterministic and defined with mathematical precision, ensuring that the cascade size function $f(S)$ is well-defined for any initial set of failures $S$. Given the finite number of nodes $n$ and a fixed removal budget $k$, an optimal set $S^{\\star}$ that maximizes $f(S)$ is guaranteed to exist. The problem is complete, providing all necessary parameters and rules, and contains no internal contradictions or ambiguities. The request to find an exact solution by exhaustive search is computationally feasible for the small network sizes specified in the test cases.\n\nWe will proceed by first formalizing the cascade model, then discussing the optimization approach and its justification, and finally outlining the algorithm for implementation.\n\n**1. The Cascade Model**\n\nThe state of the network at any discrete time step $t \\ge 0$ can be described by the load vector $L^{(t)} \\in \\mathbb{R}_{\\ge 0}^{n}$ and the set of failed nodes, let's call it $\\mathcal{F}^{(t)} \\subseteq V$.\n\n*   **Initialization ($t=0$):**\n    A set of nodes $S \\subseteq V$ with $|S|=k$ is selected for initial removal. The set of all failed nodes is initialized as $\\mathcal{F}^{(0)} = S$. The initial load vector is the base-load vector, $L = \\ell$. However, the problem specifies that the nodes in $S$ are the first to fail and redistribute their load. We can model this as a sequence of rounds. Let $\\mathcal{F}_{\\text{total}}$ be the set of all nodes that have failed up to the current point, initialized to $S$. Let $F_0 = S$ be the set of nodes failing in round $0$.\n\n*   **Cascade Rounds ($t=0, 1, 2, \\dots$):**\n    The cascade proceeds in synchronous rounds. In each round $t$, a set of nodes $F_t$ fails.\n\n    1.  If $F_t = \\emptyset$, the cascade has terminated. The process stops.\n    2.  An intermediate load increment vector, $\\Delta L^{(t)} \\in \\mathbb{R}_{\\ge 0}^{n}$, is initialized to all zeros.\n    3.  For each node $i \\in F_t$ that is failing in the current round, its load $L_i$ must be redistributed. The set of its neighbors that have not yet failed is $N_{\\text{surv}}(i) = N(i) \\setminus \\mathcal{F}_{\\text{total}}$.\n    4.  The sum of weights to these surviving neighbors is $\\sigma(i) = \\sum_{j \\in N_{\\text{surv}}(i)} W_{ij}$.\n    5.  If $\\sigma(i)  0$, each surviving neighbor $j \\in N_{\\text{surv}}(i)$ receives an additional load. The increment for node $j$ due to the failure of $i$ is $L_i \\cdot \\frac{W_{ij}}{\\sigma(i)}$. This value is added to $\\Delta L_j^{(t)}$. If $\\sigma(i)=0$, the load $L_i$ is lost.\n    6.  After computing the load increments from all nodes in $F_t$, the loads of the surviving nodes are updated synchronously: $L_j \\leftarrow L_j + \\Delta L_j^{(t)}$ for all $j \\notin \\mathcal{F}_{\\text{total}}$. Note that the loads of failing nodes themselves are not updated further.\n    7.  The set of all failed nodes is updated: $\\mathcal{F}_{\\text{total}} \\leftarrow \\mathcal{F}_{\\text{total}} \\cup F_t$.\n    8.  A new set of nodes, $F_{t+1}$, is identified to fail in the next round. These are the nodes that are not yet failed but whose new load exceeds their capacity:\n        $$F_{t+1} = \\{ j \\in V \\setminus \\mathcal{F}_{\\text{total}} : L_j  c_j \\}$$\n    9.  The process repeats for round $t+1$ with the set $F_{t+1}$.\n\nThe total size of the cascade for an initial set $S$ is given by the function $f(S) = |\\mathcal{F}_{\\text{final}}|$, where $\\mathcal{F}_{\\text{final}}$ is the set $\\mathcal{F}_{\\text{total}}$ when the process terminates.\n\n**2. Optimization and Justification**\n\nThe objective is to find a set $S^{\\star}$ that solves the following optimization problem:\n$$ S^{\\star} = \\underset{S \\subseteq V, |S|=k}{\\text{argmax}} \\; f(S) $$\nwith ties broken by selecting the lexicographically smallest set $S$.\n\nThis is a combinatorial optimization problem. For many such problems, if the objective function $f(S)$ is submodular, a simple greedy algorithm can provide a solution with a performance guarantee. A set function $f$ is submodular if it exhibits a \"diminishing returns\" property. Formally, for any sets $A \\subseteq B \\subseteq V$ and any element $v \\in V \\setminus B$, it must hold that:\n$$ f(A \\cup \\{v\\}) - f(A) \\ge f(B \\cup \\{v\\}) - f(B) $$\nHowever, cascade size functions in threshold models are often not submodular. The problem statement correctly alludes to this possibility. We can use Test Case 3 to demonstrate the failure of submodularity.\n\nLet $f$ be the cascade size function for Test Case 3.\n-   Let $A = \\{0\\}$ and $B = \\emptyset$. Note that $B \\subset A$. Let's evaluate the marginal gain of adding node $\\{1\\}$.\n-   $f(\\emptyset) = 0$, as no nodes are initially overloaded.\n-   $f(\\{0\\})$: Node $0$ fails. Its load $L_0=0.5$ is transferred to its only neighbor, node $2$. Node $2$'s load becomes $L_2' = 0.6 + 0.5 = 1.1$. This is less than its capacity $c_2 = 1.2$. The cascade stops. Thus, $f(\\{0\\}) = 1$.\n-   $f(\\{1\\})$: By symmetry with node $0$, node $1$'s failure also does not trigger a cascade. Thus, $f(\\{1\\}) = 1$.\n-   $f(\\{0, 1\\})$: Nodes $0$ and $1$ fail simultaneously. Both redistribute their loads ($L_0=0.5, L_1=0.5$) to their common neighbor, node $2$. Node $2$'s load becomes $L_2' = 0.6 + 0.5 + 0.5 = 1.6$. This exceeds its capacity $c_2 = 1.2$, so node $2$ fails in the next round. When node $2$ fails, its new load of $1.6$ is redistributed to its surviving neighbors $\\{3, 4\\}$, which receive $0.8$ each. Their loads become $L_3' = 0.4+0.8=1.2  c_3=0.5$ and $L_4' = 0.4+0.8=1.2  c_4=0.5$. They both fail. The entire network collapses. Thus, $f(\\{0, 1\\}) = 5$.\n\nNow, let's check the submodularity inequality with $A = \\{0\\}$ and $B = \\emptyset$ (so $B \\subseteq A$) and element $v = 1$. The inequality requires $f(B \\cup \\{v\\}) - f(B) \\ge f(A \\cup \\{v\\}) - f(A)$.\n-   Marginal gain of adding node $1$ to the empty set $B$: $f(\\{1\\}) - f(\\emptyset) = 1 - 0 = 1$.\n-   Marginal gain of adding node $1$ to the set $A=\\{0\\}$: $f(\\{0,1\\}) - f(\\{0\\}) = 5 - 1 = 4$.\n\nWe have $1  4$, which means $f(B \\cup \\{v\\}) - f(B)  f(A \\cup \\{v\\}) - f(A)$. This violates the submodularity condition. The function exhibits synergy, or increasing returns, where the combined effect of failing two nodes is greater than the sum of their individual effects. Due to this lack of submodularity, greedy algorithms are not guaranteed to find the optimal solution. Therefore, an exhaustive search of all $\\binom{n}{k}$ possible initial sets is necessary to guarantee optimality, as required by the problem statement.\n\n**3. Algorithmic Approach**\n\nThe algorithm will consist of a main loop that orchestrates the search and a core function that simulates the cascade for a given initial set.\n\n1.  **Main Routine:** For each test case $(n, W, \\ell, c, k)$:\n    a. Initialize `max_cascade_size = -1` and `optimal_set = []`.\n    b. Generate all unique subsets $S$ of nodes of size $k$. The `itertools.combinations` function is suitable as it generates these subsets in lexicographical order.\n    c. For each subset $S$:\n        i. Call a simulation function, `simulate_cascade(S, n, W, l, c)`, which returns the final cascade size $f(S)$.\n        ii. If the returned size is greater than `max_cascade_size`, update `max_cascade_size` to this new size and set `optimal_set` to the current $S$. Because we are iterating through sets in lexicographical order, the first set that achieves the maximum possible size will be the correct one according to the tie-breaking rule.\n    d. Store the final (`max_cascade_size`, `optimal_set`) pair for the test case.\n\n2.  **`simulate_cascade(S, n, W, l, c)` Function:**\n    a. Initialize `loads = copy(l)`, `total_failed = set(S)`, and `newly_failed = set(S)`.\n    b. Enter a `while` loop that continues as long as `newly_failed` is not empty.\n    c. Inside the loop, let `failing_this_round = newly_failed`. Reset `newly_failed` to an empty set.\n    d. Initialize `load_increments = zeros(n)`.\n    e. For each node $i$ in `failing_this_round`:\n        i. Determine the set of surviving neighbors $N_{\\text{surv}}(i) = \\{ j \\mid W_{ij}0 \\text{ and } j \\notin \\text{total\\_failed} \\}$.\n        ii. Calculate $\\sigma(i) = \\sum_{j \\in N_{\\text{surv}}(i)} W_{ij}$.\n        iii. If $\\sigma(i)  0$, for each $j \\in N_{\\text{surv}}(i)$, add $loads[i] \\cdot \\frac{W_{ij}}{\\sigma(i)}$ to `load_increments[j]`.\n    f. After iterating through all nodes in `failing_this_round`, update the main `loads` vector: `loads += load_increments`.\n    g. Update the set of failed nodes: `total_failed.update(failing_this_round)`.\n    h. Iterate through all nodes $j \\in \\{0, \\dots, n-1\\}$. If $j \\notin \\text{total\\_failed}$ and $loads[j]  c[j]$, add $j$ to the `newly_failed` set.\n    i. The loop continues.\n    j. Once the loop terminates, return `len(total_failed)`.\n\nThis design faithfully implements the specified model and performs the required exhaustive search to find the exact optimal solution for each test case.",
            "answer": "```python\nimport numpy as np\nimport itertools\n\ndef solve():\n    \"\"\"\n    Solves the cascading failure problem for a set of predefined test cases.\n    It finds the initial removal set of size k that maximizes the total cascade size.\n    \"\"\"\n\n    test_cases = [\n        # Test Case 1 (happy path star cascade)\n        {\n            \"n\": 5,\n            \"edges\": [(0, 1, 1), (0, 2, 1), (0, 3, 1), (0, 4, 1)],\n            \"l\": [0.80, 0.31, 0.31, 0.31, 0.31],\n            \"c\": [1.20, 0.50, 0.50, 0.50, 0.50],\n            \"k\": 1,\n        },\n        # Test Case 2 (boundary, no removals)\n        {\n            \"n\": 4,\n            \"edges\": [(0, 1, 1), (1, 2, 1), (2, 3, 1)],\n            \"l\": [0.20, 0.20, 0.20, 0.20],\n            \"c\": [0.50, 0.50, 0.50, 0.50],\n            \"k\": 0,\n        },\n        # Test Case 3 (edge-case synergy and submodularity counterexample)\n        {\n            \"n\": 5,\n            \"edges\": [(0, 2, 1), (1, 2, 1), (2, 3, 1), (2, 4, 1)],\n            \"l\": [0.50, 0.50, 0.60, 0.40, 0.40],\n            \"c\": [0.60, 0.60, 1.20, 0.50, 0.50],\n            \"k\": 2,\n        },\n    ]\n\n    final_results = []\n\n    for case in test_cases:\n        n = case[\"n\"]\n        edges = case[\"edges\"]\n        l_vec = np.array(case[\"l\"], dtype=float)\n        c_vec = np.array(case[\"c\"], dtype=float)\n        k = case[\"k\"]\n\n        # Build the weighted adjacency matrix W\n        W = np.zeros((n, n), dtype=float)\n        for i, j, w in edges:\n            W[i, j] = w\n            W[j, i] = w\n\n        max_cascade_size = -1\n        best_s = []\n\n        # Generate all subsets of nodes of size k.\n        # itertools.combinations generates them in lexicographical order.\n        initial_sets = itertools.combinations(range(n), k)\n        \n        for s_tuple in initial_sets:\n            initial_failures = set(s_tuple)\n            \n            # Run the cascade simulation for the current set S\n            loads = np.copy(l_vec)\n            total_failed = set(initial_failures)\n            newly_failed = set(initial_failures)\n\n            while newly_failed:\n                failing_this_round = set(newly_failed)\n                newly_failed = set()\n                \n                # Update total failed set before calculating redistributions\n                # from this round's failures. This is to ensure a node failing\n                # in the same round doesn't receive load from another.\n                current_round_total_failed = total_failed.union(failing_this_round)\n                \n                load_increments = np.zeros(n)\n\n                for i in failing_this_round:\n                    surviving_neighbors = []\n                    weight_sum_surv = 0.0\n                    \n                    # Find surviving neighbors and sum of weights\n                    for j in range(n):\n                        if W[i, j]  0 and j not in current_round_total_failed:\n                            surviving_neighbors.append(j)\n                            weight_sum_surv += W[i, j]\n\n                    # Redistribute load\n                    if weight_sum_surv  0:\n                        for j in surviving_neighbors:\n                            load_increments[j] += loads[i] * (W[i, j] / weight_sum_surv)\n                \n                # Synchronous update of loads\n                loads += load_increments\n                \n                # Update total failed set after redistributions are calculated\n                total_failed.update(failing_this_round)\n\n                # Identify nodes failing in the next round\n                for j in range(n):\n                    if j not in total_failed and loads[j]  c_vec[j]:\n                        newly_failed.add(j)\n\n            current_cascade_size = len(total_failed)\n            \n            # Update best result found so far.\n            # Due to lexicographical order of combinations, the first set\n            # that achieves the max size will be the lexicographically smallest.\n            if current_cascade_size  max_cascade_size:\n                max_cascade_size = current_cascade_size\n                best_s = list(s_tuple)\n        \n        # Handle k=0 case which results in no loops\n        if k == 0 and max_cascade_size == -1:\n             max_cascade_size = 0\n             best_s = []\n\n        final_results.append([max_cascade_size, best_s])\n    \n    # Format the final output string exactly as specified.\n    result_str = \",\".join([f\"[{size},{s}]\" for size, s in final_results])\n    print(f\"[{result_str}]\".replace(\" \", \"\"))\n\n\nsolve()\n```"
        }
    ]
}