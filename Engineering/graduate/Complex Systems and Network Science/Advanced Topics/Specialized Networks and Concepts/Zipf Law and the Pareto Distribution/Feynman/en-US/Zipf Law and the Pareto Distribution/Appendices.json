{
    "hands_on_practices": [
        {
            "introduction": "To robustly analyze systems described by Pareto distributions, we must first master the statistical tools for parameter estimation. This exercise guides you through the derivation of the Maximum Likelihood Estimator (MLE) for the Pareto exponent, $\\alpha$, directly from first principles. By working through this foundational problem , you will not only derive the estimator itself but also its asymptotic variance, providing deep insight into its reliability and performance.",
            "id": "4312982",
            "problem": "Consider a rank-size context in complex systems where large-event sizes are hypothesized to follow a heavy-tailed law consistent with Zipf’s law for ranks. Assume that observed event sizes $\\{x_i\\}_{i=1}^{n}$ are independent and identically distributed (i.i.d.) from a Type I Pareto distribution with scale parameter $x_{\\min}  0$ and tail index $\\alpha  0$, supported on $x \\ge x_{\\min}$. The probability density function is\n$$\nf(x \\mid \\alpha, x_{\\min}) = \\alpha \\, x_{\\min}^{\\alpha} \\, x^{-(\\alpha+1)}, \\quad x \\ge x_{\\min}.\n$$\nStarting from the fundamental definitions of likelihood and maximum likelihood estimation, and assuming standard regularity conditions that justify differentiating under the summation and the use of large-sample normality of the maximum likelihood estimator (MLE), perform the following:\n\n- Derive the maximum likelihood estimator $\\hat{\\alpha}$ for $\\alpha$ based on $\\{x_i\\}_{i=1}^{n}$.\n- Using first principles for asymptotic inference (for example, Fisher information and the Cramér–Rao framework or an equivalent route grounded in the transformation $y_i = \\ln(x_i/x_{\\min})$), derive the large-sample variance of $\\hat{\\alpha}$.\n\nExpress your final answer as a single closed-form analytic expression containing both the estimator and its asymptotic variance, with no units. Do not provide any numerical approximation or rounding; give exact symbolic expressions only.",
            "solution": "The problem statement is subjected to validation prior to any attempt at a solution.\n\n### Step 1: Extract Givens\n- A set of observed event sizes $\\{x_i\\}_{i=1}^{n}$ are independent and identically distributed (i.i.d.).\n- The distribution is a Type I Pareto distribution.\n- The scale parameter is $x_{\\min}  0$.\n- The tail index is $\\alpha  0$.\n- The support is $x \\ge x_{\\min}$.\n- The probability density function (PDF) is $f(x \\mid \\alpha, x_{\\min}) = \\alpha \\, x_{\\min}^{\\alpha} \\, x^{-(\\alpha+1)}$.\n- Standard regularity conditions for Maximum Likelihood Estimation (MLE) are assumed to hold.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientifically Grounded**: The problem is well-grounded in the established fields of statistical inference and complex systems. The Pareto distribution is a standard model for heavy-tailed phenomena, and Maximum Likelihood Estimation is a fundamental method of parameter estimation. The connection to Zipf's law is conceptually appropriate.\n2.  **Well-Posed**: The problem is well-posed. It requests the derivation of a specific estimator ($\\hat{\\alpha}$) and its asymptotic variance under clearly defined statistical assumptions. The parameter $x_{\\min}$ is given as part of the distribution's definition, implying it is known, which makes the estimation of $\\alpha$ a well-defined task.\n3.  **Objective**: The problem is stated using precise and objective mathematical language, free from ambiguity or subjective claims.\n4.  **Completeness and Consistency**: The problem provides all necessary information to derive the requested quantities. The PDF is explicitly given, and the assumptions (i.i.d. data, regularity conditions) are stated. There are no internal contradictions.\n\n### Step 3: Verdict and Action\nThe problem is valid as it is scientifically sound, well-posed, objective, and complete. I will proceed with the derivation.\n\nThe solution is derived in two parts as requested: first, the maximum likelihood estimator $\\hat{\\alpha}$, and second, its large-sample variance.\n\n**Part 1: Derivation of the Maximum Likelihood Estimator $\\hat{\\alpha}$**\n\nThe likelihood function, $L(\\alpha)$, for a set of $n$ i.i.d. observations $\\{x_i\\}_{i=1}^{n}$ from the Pareto distribution is the product of the individual probability densities:\n$$\nL(\\alpha \\mid \\{x_i\\}) = \\prod_{i=1}^{n} f(x_i \\mid \\alpha, x_{\\min}) = \\prod_{i=1}^{n} \\left( \\alpha \\, x_{\\min}^{\\alpha} \\, x_i^{-(\\alpha+1)} \\right)\n$$\nThis can be written as:\n$$\nL(\\alpha) = \\alpha^n \\, x_{\\min}^{n\\alpha} \\left( \\prod_{i=1}^{n} x_i \\right)^{-(\\alpha+1)}\n$$\nTo find the maximum of $L(\\alpha)$, it is more convenient to work with the log-likelihood function, $\\ell(\\alpha) = \\ln L(\\alpha)$:\n$$\n\\ell(\\alpha) = \\ln\\left( \\alpha^n \\, x_{\\min}^{n\\alpha} \\left( \\prod_{i=1}^{n} x_i \\right)^{-(\\alpha+1)} \\right)\n$$\nUsing the properties of logarithms, this simplifies to:\n$$\n\\ell(\\alpha) = n \\ln(\\alpha) + n\\alpha \\ln(x_{\\min}) - (\\alpha+1) \\sum_{i=1}^{n} \\ln(x_i)\n$$\nTo find the value of $\\alpha$ that maximizes $\\ell(\\alpha)$, we compute the first derivative with respect to $\\alpha$ and set it to zero.\n$$\n\\frac{d\\ell}{d\\alpha} = \\frac{n}{\\alpha} + n \\ln(x_{\\min}) - \\sum_{i=1}^{n} \\ln(x_i)\n$$\nSetting the derivative to zero, $\\frac{d\\ell}{d\\alpha} = 0$, gives the equation for the MLE $\\hat{\\alpha}$:\n$$\n\\frac{n}{\\hat{\\alpha}} + n \\ln(x_{\\min}) - \\sum_{i=1}^{n} \\ln(x_i) = 0\n$$\nRearranging the terms to solve for $\\hat{\\alpha}$:\n$$\n\\frac{n}{\\hat{\\alpha}} = \\sum_{i=1}^{n} \\ln(x_i) - n \\ln(x_{\\min})\n$$\n$$\n\\frac{n}{\\hat{\\alpha}} = \\sum_{i=1}^{n} (\\ln(x_i) - \\ln(x_{\\min})) = \\sum_{i=1}^{n} \\ln\\left(\\frac{x_i}{x_{\\min}}\\right)\n$$\nInverting the expression gives the maximum likelihood estimator for $\\alpha$:\n$$\n\\hat{\\alpha} = n \\left( \\sum_{i=1}^{n} \\ln\\left(\\frac{x_i}{x_{\\min}}\\right) \\right)^{-1}\n$$\n\n**Part 2: Derivation of the Large-Sample Variance of $\\hat{\\alpha}$**\n\nThe asymptotic variance of an MLE can be found from the Fisher information. The Fisher information for a single observation, $I(\\alpha)$, is the negative of the expected value of the second derivative of the log-likelihood of a single observation.\n\nThe log-likelihood for a single observation $x$ is:\n$$\n\\ell_1(\\alpha) = \\ln f(x \\mid \\alpha, x_{\\min}) = \\ln(\\alpha) + \\alpha \\ln(x_{\\min}) - (\\alpha+1) \\ln(x)\n$$\nThe first derivative with respect to $\\alpha$ is:\n$$\n\\frac{d\\ell_1}{d\\alpha} = \\frac{1}{\\alpha} + \\ln(x_{\\min}) - \\ln(x)\n$$\nThe second derivative with respect to $\\alpha$ is:\n$$\n\\frac{d^2\\ell_1}{d\\alpha^2} = -\\frac{1}{\\alpha^2}\n$$\nThe Fisher information for one observation, $I(\\alpha)$, is:\n$$\nI(\\alpha) = -E\\left[\\frac{d^2\\ell_1}{d\\alpha^2}\\right]\n$$\nSince the second derivative, $-\\frac{1}{\\alpha^2}$, is a constant with respect to the random variable $x$, its expectation is simply the constant itself.\n$$\nI(\\alpha) = - \\left(-\\frac{1}{\\alpha^2}\\right) = \\frac{1}{\\alpha^2}\n$$\nFor a sample of $n$ i.i.d. observations, the total Fisher information $I_n(\\alpha)$ is $n$ times the information for a single observation:\n$$\nI_n(\\alpha) = n I(\\alpha) = \\frac{n}{\\alpha^2}\n$$\nUnder the assumed regularity conditions, the large-sample (asymptotic) variance of the MLE $\\hat{\\alpha}$ is the inverse of the total Fisher information. This is also the Cramér-Rao lower bound for the variance of any unbiased estimator of $\\alpha$.\n$$\n\\text{Var}_{\\infty}(\\hat{\\alpha}) = [I_n(\\alpha)]^{-1} = \\left(\\frac{n}{\\alpha^2}\\right)^{-1} = \\frac{\\alpha^2}{n}\n$$\nThis expression gives the theoretical asymptotic variance of the estimator $\\hat{\\alpha}$ as a function of the true parameter $\\alpha$ and the sample size $n$.\n\nThe two results are:\n1.  The estimator: $\\hat{\\alpha} = n \\left( \\sum_{i=1}^{n} \\ln\\left(\\frac{x_i}{x_{\\min}}\\right) \\right)^{-1}$\n2.  The asymptotic variance: $\\text{Var}_{\\infty}(\\hat{\\alpha}) = \\frac{\\alpha^2}{n}$",
            "answer": "$$\n\\boxed{\\begin{pmatrix} n \\left( \\sum_{i=1}^{n} \\ln\\left(\\frac{x_i}{x_{\\min}}\\right) \\right)^{-1}  \\frac{\\alpha^2}{n} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Theoretical derivations are essential, but the ultimate test of an estimator is its performance on data. This practice transitions from theory to application, requiring you to implement the Pareto MLE and use it to analyze datasets representing stylized income distributions . A key focus is on quantifying uncertainty by calculating confidence intervals, a crucial step for moving beyond a simple point estimate to a more complete statistical inference.",
            "id": "4313019",
            "problem": "Consider a stylized model of the upper tail of an income distribution in a complex system with interacting agents, where incomes above a fixed threshold are modeled as independent and identically distributed (i.i.d.) draws from a Pareto Type I tail. Let the threshold be denoted by $x_{\\min}  0$. Conditioned on $x \\geq x_{\\min}$, assume the data are i.i.d. with density $f_{\\alpha}(x)$, where $f_{\\alpha}(x)$ is the Pareto Type I tail density with shape parameter $\\alpha  0$ and support $x \\in [x_{\\min}, \\infty)$. Starting from the joint likelihood of the i.i.d. sample and the definition of the Pareto Type I density, derive the threshold Maximum Likelihood Estimator (Maximum Likelihood Estimator (MLE)) $\\hat{\\alpha}$ for $\\alpha$ under the assumption that $x_{\\min}$ is known. Next, derive the asymptotic normality of $\\hat{\\alpha}$ from the Fisher information and obtain a two-sided confidence interval at a prescribed confidence level $c \\in (0,1)$ using the asymptotic normal approximation.\n\nYour program must implement the estimator and its confidence interval computation using only observations that are at or above the threshold, i.e., include only those $x_i$ satisfying $x_i \\geq x_{\\min}$. Use the natural logarithm $\\ln(\\cdot)$ everywhere in your derivation and computation.\n\nTest Suite. For each case below, compute the estimate and a two-sided asymptotic confidence interval:\n- Case $1$ (happy path): incomes $[\\,\\$120,\\ \\ \\$150,\\ \\ \\$200,\\ \\ \\$330,\\ \\ \\$500,\\ \\ \\$800,\\ \\ \\$1300,\\ \\ \\$2100\\,]$, threshold $x_{\\min} = \\$100$, confidence level $c = 0.95$ (expressed as a decimal).\n- Case $2$ (boundary near-threshold): incomes $[\\,\\$101,\\ \\ \\$102,\\ \\ \\$103,\\ \\ \\$104,\\ \\ \\$105,\\ \\ \\$110\\,]$, threshold $x_{\\min} = \\$100$, confidence level $c = 0.95$ (expressed as a decimal).\n- Case $3$ (small sample size): incomes $[\\,\\$53,\\ \\ \\$57,\\ \\ \\$61\\,]$, threshold $x_{\\min} = \\$50$, confidence level $c = 0.90$ (expressed as a decimal).\n- Case $4$ (scale invariance check): incomes $[\\,\\$1200,\\ \\ \\$1500,\\ \\ \\$2000,\\ \\ \\$3300,\\ \\ \\$5000,\\ \\ \\$8000,\\ \\ \\$13000,\\ \\ \\$21000\\,]$ obtained by scaling Case $1$ incomes by a factor $10$, threshold $x_{\\min} = \\$1000$ obtained by scaling Case $1$ threshold by a factor $10$, confidence level $c = 0.95$ (expressed as a decimal). In this case, verify whether the estimate and the confidence interval are invariant under scaling by $10$ within absolute tolerance $10^{-12}$ when compared to Case $1$.\n\nOutput specification. Your program should produce a single line of output containing the aggregated results of the test suite as a comma-separated list enclosed in square brackets. The first three elements must each be a three-element list $[\\,\\hat{\\alpha},\\ \\mathrm{CI}_{\\mathrm{low}},\\ \\mathrm{CI}_{\\mathrm{high}}\\,]$ of floats for Cases $1$–$3$ in that order. The fourth element must be a boolean for Case $4$ indicating whether scale invariance holds within absolute tolerance $10^{-12}$. For example, the final output format must be $[[a_1,l_1,h_1],[a_2,l_2,h_2],[a_3,l_3,h_3],\\mathrm{True}]$ with no spaces.\n\nScientific realism and derivation base. The derivation must begin from the Pareto Type I tail model and the i.i.d. joint likelihood, proceed to the Maximum Likelihood Estimator using calculus, and then use Fisher information to establish asymptotic normality. You must avoid shortcut formulas in the problem statement. No physical units are required for the final numerical outputs because the Pareto exponent $\\alpha$ is dimensionless; the monetary values above are for contextual realism. All angles, if any appear, must be in radians, but none are required here. The answers must be floats and booleans as specified above, and the program must be self-contained with no external input.",
            "solution": "The user-provided problem is subjected to a validation process before a solution is attempted.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n-   **Model**: The problem describes a model for the upper tail of an income distribution, where incomes $x$ above a threshold $x_{\\min}  0$ are treated as independent and identically distributed (i.i.d.) random variables.\n-   **Distribution**: Conditional on $x \\geq x_{\\min}$, the data are drawn from a Pareto Type I distribution with a probability density function (PDF) $f_{\\alpha}(x)$.\n-   **Parameters**: The distribution has a shape parameter $\\alpha  0$ (unknown) and a scale parameter $x_{\\min}$ (known). The support is $x \\in [x_{\\min}, \\infty)$.\n-   **Task**:\n    1.  Derive the Maximum Likelihood Estimator (MLE) $\\hat{\\alpha}$ for the shape parameter $\\alpha$.\n    2.  Derive the asymptotic normality of $\\hat{\\alpha}$ using the Fisher information.\n    3.  Derive a two-sided confidence interval for $\\alpha$ at a given confidence level $c \\in (0,1)$.\n-   **Computational Constraints**: Use the natural logarithm $\\ln(\\cdot)$ throughout. The estimation is based only on observations $x_i$ such that $x_i \\geq x_{\\min}$.\n-   **Test Suite**: The problem provides four specific test cases to be implemented and solved.\n    -   Case 1: `incomes` = $[\\,120, 150, 200, 330, 500, 800, 1300, 2100\\,]$, $x_{\\min} = 100$, $c = 0.95$.\n    -   Case 2: `incomes` = $[\\,101, 102, 103, 104, 105, 110\\,]$, $x_{\\min} = 100$, $c = 0.95$.\n    -   Case 3: `incomes` = $[\\,53, 57, 61\\,]$, $x_{\\min} = 50$, $c = 0.90$.\n    -   Case 4: A scale invariance check using data and threshold from Case 1, both scaled by a factor of $10$. Compare results to Case 1 within an absolute tolerance of $10^{-12}$.\n-   **Output Specification**: A single-line comma-separated list: `[[a1,l1,h1],[a2,l2,h2],[a3,l3,h3],boolean]`.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is evaluated against the validation criteria:\n-   **Scientifically Grounded**: The problem is well-grounded in statistical theory. The Pareto distribution is a canonical model for heavy-tailed phenomena, and Maximum Likelihood Estimation is a fundamental method of parameter estimation. The use of Fisher information and asymptotic normality are standard techniques in inferential statistics. The problem represents a classic exercise in this field.\n-   **Well-Posed**: The problem is clearly specified. It asks for the derivation of a standard estimator and its properties, and then its application to concrete data. A unique, stable solution exists for the given test cases.\n-   **Objective**: The language is precise, mathematical, and free of any subjective or opinion-based content.\n-   **Complete and Consistent**: All necessary information (the model, its parameters, data, and objectives) is provided. There are no contradictions.\n-   **Realistic and Feasible**: The model is a well-known stylized fact in economics. The provided data are realistic, and the required calculations are standard and computationally feasible. The derivation is a textbook procedure.\n\nThe problem does not suffer from any of the specified invalidating flaws.\n\n**Step 3: Verdict and Action**\nThe problem is **valid**. A full, reasoned solution will be provided.\n\n### Derivations and Solution\n\nLet the set of observations be $\\{x_1, x_2, \\ldots, x_N\\}$. We are interested in the subset of these observations for which $x_i \\geq x_{\\min}$. Let this filtered sample be denoted as $\\{X_1, X_2, \\ldots, X_n\\}$, where $n$ is the count of such observations. These $n$ data points are assumed to be i.i.d. draws from a Pareto Type I distribution, whose probability density function (PDF) is:\n$$ f(x; \\alpha, x_{\\min}) = \\frac{\\alpha x_{\\min}^{\\alpha}}{x^{\\alpha+1}}, \\quad \\text{for } x \\geq x_{\\min} $$\nThe joint likelihood function $L(\\alpha)$ for the sample is the product of the individual PDFs:\n$$ L(\\alpha) = \\prod_{i=1}^{n} f(X_i; \\alpha, x_{\\min}) = \\prod_{i=1}^{n} \\frac{\\alpha x_{\\min}^{\\alpha}}{X_i^{\\alpha+1}} = \\frac{\\alpha^n x_{\\min}^{n\\alpha}}{\\left(\\prod_{i=1}^{n} X_i\\right)^{\\alpha+1}} $$\nTo find the Maximum Likelihood Estimator (MLE), we maximize the log-likelihood function, $\\ell(\\alpha) = \\ln L(\\alpha)$:\n$$ \\ell(\\alpha) = \\ln\\left(\\alpha^n x_{\\min}^{n\\alpha}\\right) - \\ln\\left(\\left(\\prod_{i=1}^{n} X_i\\right)^{\\alpha+1}\\right) $$\n$$ \\ell(\\alpha) = n \\ln(\\alpha) + n\\alpha \\ln(x_{\\min}) - (\\alpha+1) \\sum_{i=1}^{n} \\ln(X_i) $$\nTo find the maximum, we compute the derivative of $\\ell(\\alpha)$ with respect to $\\alpha$ and set it to zero.\n$$ \\frac{d\\ell(\\alpha)}{d\\alpha} = \\frac{n}{\\alpha} + n \\ln(x_{\\min}) - \\sum_{i=1}^{n} \\ln(X_i) $$\nSetting the derivative to zero for the MLE $\\hat{\\alpha}$:\n$$ \\frac{n}{\\hat{\\alpha}} = \\sum_{i=1}^{n} \\ln(X_i) - n \\ln(x_{\\min}) = \\sum_{i=1}^{n} \\left(\\ln(X_i) - \\ln(x_{\\min})\\right) = \\sum_{i=1}^{n} \\ln\\left(\\frac{X_i}{x_{\\min}}\\right) $$\nSolving for $\\hat{\\alpha}$ yields the estimator:\n$$ \\hat{\\alpha} = \\frac{n}{\\sum_{i=1}^{n} \\ln\\left(\\frac{X_i}{x_{\\min}}\\right)} $$\nTo verify this is a maximum, we check the second derivative:\n$$ \\frac{d^2\\ell(\\alpha)}{d\\alpha^2} = -\\frac{n}{\\alpha^2}  0 $$\nSince the second derivative is strictly negative for $n0$, the log-likelihood function is concave, and $\\hat{\\alpha}$ is indeed the unique Maximum Likelihood Estimator.\n\nNext, we establish the asymptotic normality of $\\hat{\\alpha}$. Under standard regularity conditions, an MLE is asymptotically normal with a variance given by the inverse of the Fisher information. The Fisher information for a single observation, $I(\\alpha)$, is:\n$$ I(\\alpha) = -E\\left[\\frac{d^2 \\ln f(x; \\alpha, x_{\\min})}{d\\alpha^2}\\right] $$\nWe first find the second derivative of the log-PDF for a single observation $x$:\n$$ \\ln f(x; \\alpha, x_{\\min}) = \\ln(\\alpha) + \\alpha \\ln(x_{\\min}) - (\\alpha+1)\\ln(x) $$\n$$ \\frac{d}{d\\alpha} \\ln f = \\frac{1}{\\alpha} + \\ln(x_{\\min}) - \\ln(x) $$\n$$ \\frac{d^2}{d\\alpha^2} \\ln f = -\\frac{1}{\\alpha^2} $$\nBecause this second derivative is a constant with respect to $x$, its expectation is the constant itself.\n$$ E\\left[-\\frac{1}{\\alpha^2}\\right] = -\\frac{1}{\\alpha^2} $$\nTherefore, the Fisher information for a single observation is:\n$$ I(\\alpha) = - \\left(-\\frac{1}{\\alpha^2}\\right) = \\frac{1}{\\alpha^2} $$\nFor an i.i.d. sample of size $n$, the total Fisher information is $I_n(\\alpha) = n I(\\alpha) = \\frac{n}{\\alpha^2}$. The asymptotic variance of $\\hat{\\alpha}$ is the inverse of $I_n(\\alpha)$:\n$$ \\text{Var}(\\hat{\\alpha}) \\approx [I_n(\\alpha)]^{-1} = \\frac{\\alpha^2}{n} $$\nAs $n \\to \\infty$, the distribution of $\\hat{\\alpha}$ approaches a normal distribution:\n$$ \\hat{\\alpha} \\xrightarrow{d} \\mathcal{N}\\left(\\alpha, \\frac{\\alpha^2}{n}\\right) $$\nFor constructing a confidence interval, we use the estimated standard error, $\\widehat{\\text{SE}}(\\hat{\\alpha})$, by substituting $\\hat{\\alpha}$ for the unknown $\\alpha$:\n$$ \\widehat{\\text{SE}}(\\hat{\\alpha}) = \\frac{\\hat{\\alpha}}{\\sqrt{n}} $$\nA two-sided confidence interval at confidence level $c$ is constructed using the standard normal distribution. Let $z_{(1+c)/2}$ be the quantile of the standard normal distribution such that $P(Z \\le z_{(1+c)/2}) = (1+c)/2$. The interval is given by:\n$$ \\hat{\\alpha} \\pm z_{(1+c)/2} \\cdot \\widehat{\\text{SE}}(\\hat{\\alpha}) $$\nThis results in the confidence interval $[\\mathrm{CI}_{\\mathrm{low}}, \\mathrm{CI}_{\\mathrm{high}}]$:\n$$ \\left[ \\hat{\\alpha} - z_{(1+c)/2} \\frac{\\hat{\\alpha}}{\\sqrt{n}}, \\quad \\hat{\\alpha} + z_{(1+c)/2} \\frac{\\hat{\\alpha}}{\\sqrt{n}} \\right] $$\nor more compactly:\n$$ \\hat{\\alpha} \\left(1 \\pm \\frac{z_{(1+c)/2}}{\\sqrt{n}}\\right) $$\nThese derived formulas are implemented in the provided Python code to solve the test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Main function to execute the test suite and print the results.\n    \"\"\"\n\n    def calculate_pareto_mle_ci(data, x_min, c):\n        \"\"\"\n        Calculates the MLE and confidence interval for the Pareto alpha parameter.\n\n        Args:\n            data (list or np.ndarray): The income observations.\n            x_min (float): The known threshold for the Pareto tail.\n            c (float): The confidence level for the interval.\n\n        Returns:\n            list: A list containing [alpha_hat, ci_low, ci_high].\n                  Returns None if no data points are above the threshold.\n        \"\"\"\n        # Filter data to include only observations at or above the threshold\n        observations = np.array(data)\n        valid_observations = observations[observations = x_min]\n\n        n = len(valid_observations)\n        if n == 0:\n            # This case is not expected based on the problem's test suite,\n            # but it is a necessary check for robustness.\n            return [np.nan, np.nan, np.nan]\n        \n        # Check for the edge case where all observations are exactly at the threshold.\n        # This would lead to a sum of logs equal to zero and division by zero.\n        log_ratios = np.log(valid_observations / x_min)\n        sum_log_ratios = np.sum(log_ratios)\n        if np.isclose(sum_log_ratios, 0.0):\n             return [np.inf, np.inf, np.inf]\n\n        # Calculate the Maximum Likelihood Estimator (MLE) for alpha\n        # Formula: alpha_hat = n / sum(ln(x_i / x_min))\n        alpha_hat = n / sum_log_ratios\n\n        # Calculate the two-sided confidence interval\n        # Get the critical value from the standard normal distribution\n        # For a confidence level c, we need the quantile for (1+c)/2\n        z_score = norm.ppf((1 + c) / 2)\n\n        # Estimated standard error of alpha_hat\n        # SE_hat = alpha_hat / sqrt(n)\n        se_hat = alpha_hat / np.sqrt(n)\n\n        # Calculate the confidence interval bounds\n        ci_low = alpha_hat - z_score * se_hat\n        ci_high = alpha_hat + z_score * se_hat\n\n        return [alpha_hat, ci_low, ci_high]\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"incomes\": [120, 150, 200, 330, 500, 800, 1300, 2100],\n            \"x_min\": 100.0,\n            \"c\": 0.95\n        },\n        {\n            \"incomes\": [101, 102, 103, 104, 105, 110],\n            \"x_min\": 100.0,\n            \"c\": 0.95\n        },\n        {\n            \"incomes\": [53, 57, 61],\n            \"x_min\": 50.0,\n            \"c\": 0.90\n        }\n    ]\n\n    results = []\n    \n    # Process Cases 1-3\n    for case in test_cases:\n        result = calculate_pareto_mle_ci(case[\"incomes\"], case[\"x_min\"], case[\"c\"])\n        results.append(result)\n\n    # Process Case 4 (Scale Invariance Check)\n    case1_data = test_cases[0]\n    case4_data = {\n        \"incomes\": [x * 10 for x in case1_data[\"incomes\"]],\n        \"x_min\": case1_data[\"x_min\"] * 10,\n        \"c\": case1_data[\"c\"]\n    }\n\n    # The result for Case 1 is already in results[0]\n    result_case1 = np.array(results[0])\n    result_case4 = np.array(calculate_pareto_mle_ci(case4_data[\"incomes\"], case4_data[\"x_min\"], case4_data[\"c\"]))\n\n    # Check for scale invariance within the specified absolute tolerance\n    tolerance = 1e-12\n    is_scale_invariant = np.allclose(result_case1, result_case4, atol=tolerance, rtol=0)\n    \n    # The problem states output format `[...,True]` with no spaces.\n    # The default conversion of list to string in python has spaces.\n    # Building the string manually is the most robust way.\n    res_str_1 = f\"[{results[0][0]},{results[0][1]},{results[0][2]}]\"\n    res_str_2 = f\"[{results[1][0]},{results[1][1]},{results[1][2]}]\"\n    res_str_3 = f\"[{results[2][0]},{results[2][1]},{results[2][2]}]\"\n    res_str_4 = str(is_scale_invariant)\n    \n    final_string = f\"[{res_str_1},{res_str_2},{res_str_3},{res_str_4}]\"\n\n    print(final_string)\n\n\nsolve()\n```"
        },
        {
            "introduction": "In empirical work, power-law behavior rarely describes an entire dataset, but rather its tail. This creates a critical challenge: where does the tail begin? This exercise addresses the crucial problem of selecting the lower bound $x_{\\min}$, introducing a principled, goodness-of-fit-based method to make this choice . Understanding this procedure illuminates the fundamental bias-variance trade-off that is central to effective model fitting in complex systems.",
            "id": "4312956",
            "problem": "You observe independent and identically distributed samples $\\{X_i\\}_{i=1}^n$ from a heavy-tailed process hypothesized to follow a Pareto tail above an unknown lower cutoff $x_{\\min}$. You wish to select $x_{\\min}$ in a principled way by comparing the empirical tail with the fitted Pareto model and to understand the implications for estimator error. Using only the following fundamentals as a starting point: (i) the definition of a Pareto tail as a power-law distribution restricted to $x \\ge x_{\\min}$, (ii) the concept of maximum likelihood estimation restricted to the tail subset $\\{X_i \\ge x_{\\min}\\}$, (iii) the definition of the empirical cumulative distribution function computed on the tail subset, and (iv) the Kolmogorov–Smirnov statistic as the supremum norm between two cumulative distribution functions, select the option that most accurately describes a valid procedure to choose $x_{\\min}$ by minimizing the Kolmogorov–Smirnov distance between the empirical and fitted Pareto tails, and correctly characterizes the bias–variance trade-off associated with increasing or decreasing $x_{\\min}$.\n\nA. For each candidate cutoff $x_{\\min}$ taken from distinct sample values, restrict attention to observations with $X_i \\ge x_{\\min}$, estimate the Pareto tail parameter by maximum likelihood on that restricted sample, form the model cumulative distribution on $x \\ge x_{\\min}$, compute the Kolmogorov–Smirnov distance between this model and the empirical cumulative distribution of the restricted sample, and select the $x_{\\min}$ that minimizes this distance. Increasing $x_{\\min}$ generally decreases model misspecification bias at the cost of increasing variance due to fewer tail observations.\n\nB. Use all observations without truncation to fit a power law by linear regression on the log–log rank–size plot, then select $x_{\\min}$ as the smallest value beyond which the regression residuals first fall below a fixed threshold; raising $x_{\\min}$ reduces variance because the fitted slope is estimated from a region that more closely follows a straight line.\n\nC. Set $x_{\\min}$ equal to the minimum observed value to avoid selection bias and fit the Pareto model using all observations; any truncation would increase bias and should be avoided, since including more data always reduces both bias and variance.\n\nD. For each candidate $x_{\\min}$, estimate the Pareto parameter by matching the sample mean of the restricted data to the theoretical mean of a Pareto tail, then select the $x_{\\min}$ that minimizes the average absolute error between the empirical and theoretical probability density functions on $x \\ge x_{\\min}$; increasing $x_{\\min}$ increases both bias and variance because it discards data and distorts the fitted mean.\n\nE. For each candidate $x_{\\min}$, estimate the Pareto parameter by maximum likelihood on the restricted sample with $X_i \\ge x_{\\min}$, but compute the Kolmogorov–Smirnov statistic against the model on the full data, including points with $X_i  x_{\\min}$, and choose the $x_{\\min}$ that maximizes this statistic; raising $x_{\\min}$ reduces both bias and variance because the fit ignores more outliers and the discrepancy measure becomes larger, indicating a better fit.",
            "solution": "The user has requested a rigorous validation of the problem statement, followed by a detailed derivation of the solution and evaluation of the provided options.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- Data: Independent and identically distributed samples $\\{X_i\\}_{i=1}^n$.\n- Hypothesis: The data-generating process has a heavy tail that follows a Pareto distribution for values $x \\ge x_{\\min}$, where $x_{\\min}$ is an unknown lower cutoff.\n- Objective: To select $x_{\\min}$ by minimizing the Kolmogorov–Smirnov (KS) distance between the empirical and fitted Pareto tails, and to understand the associated bias–variance trade-off.\n- Methodological Constraints (Fundamentals):\n    - (i) A Pareto tail is a power-law distribution restricted to $x \\ge x_{\\min}$.\n    - (ii) Use maximum likelihood estimation (MLE) on the tail subset $\\{X_i \\ge x_{\\min}\\}$.\n    - (iii) Use the empirical cumulative distribution function (ECDF) on the tail subset.\n    - (iv) The Kolmogorov–Smirnov statistic is the supremum norm between two cumulative distribution functions (CDFs).\n\n**Step 2: Validate Using Extracted Givens**\n1.  **Scientific or Factual Unsoundness**: The problem is scientifically and mathematically sound. The analysis of Pareto tails, the use of MLE for parameter estimation, and the application of the KS statistic for goodness-of-fit are standard, well-established methods in statistics and the study of complex systems. The formulation described is a widely-used procedure, notably detailed by Clauset, Shalizi, and Newman in their 2009 paper \"Power-law distributions in empirical data.\"\n2.  **Non-Formalizable or Irrelevant**: The problem is formalizable within the framework of statistical estimation and model selection. It is directly relevant to the topic of Zipf's law and Pareto distributions in complex systems and network science.\n3.  **Incomplete or Contradictory Setup**: The problem is self-contained. It provides all the necessary conceptual building blocks (MLE, KS statistic, ECDF) to define the required procedure without needing external information. It asks for a description of the procedure, not a numerical result, so no data is needed.\n4.  **Unrealistic or Infeasible**: The scenario is highly realistic. Estimating the onset of power-law behavior is a common and critical task in fields like finance, physics, and biology.\n5.  **Ill-Posed or Poorly Structured**: The problem is well-posed. It asks for the identification of a specific, established procedure and the correct characterization of its statistical properties.\n6.  **Pseudo-Profound, Trivial, or Tautological**: The problem addresses a non-trivial issue. The bias–variance trade-off in selecting $x_{\\min}$ is a subtle but crucial concept for robustly fitting power-law models.\n7.  **Outside Scientific Verifiability**: The concepts are all standard and verifiable within mathematical statistics.\n\n**Step 3: Verdict and Action**\n- **Verdict**: The problem statement is **valid**.\n- **Action**: Proceed with the solution.\n\n### Derivation of the Correct Procedure\n\nThe goal is to find the optimal lower cutoff, $\\hat{x}_{\\min}$, for a Pareto tail. The procedure synthesizes the four given fundamentals.\n\n1.  **Define the Model (Fundamental i)**: For a given cutoff $x_{\\min}$, the data in the tail are assumed to follow a Pareto Type I distribution. The probability density function (PDF) for $x \\ge x_{\\min}$ is $p(x | \\alpha, x_{\\min}) = \\alpha x_{\\min}^{\\alpha} x^{-(\\alpha+1)}$, and the cumulative distribution function (CDF) is $F(x | \\alpha, x_{\\min}) = P(X \\le x) = 1 - (\\frac{x_{\\min}}{x})^{\\alpha}$.\n\n2.  **Loop Through Candidate Cutoffs**: The optimal $x_{\\min}$ must be one of the values present in the data, as any choice between data points is equivalent to choosing the larger of the two. Thus, we create a set of candidate cutoffs from the unique values in the observed samples $\\{X_i\\}$.\n\n3.  **For Each Candidate $x_{\\min}$**:\n    a. **Restrict the Data**: Create a tail subset of the data, $D_{\\text{tail}} = \\{X_i \\mid X_i \\ge x_{\\min}\\}$. Let the number of points in this subset be $n_{\\text{tail}}$.\n    b. **Estimate the Parameter (Fundamental ii)**: Use Maximum Likelihood Estimation on $D_{\\text{tail}}$ to find the tail exponent $\\hat{\\alpha}$. The MLE for the Pareto exponent $\\alpha$ (as derived in the first practice problem) is:\n    $$ \\hat{\\alpha} = n_{\\text{tail}} \\left[ \\sum_{X_i \\in D_{\\text{tail}}} \\ln\\left(\\frac{X_i}{x_{\\min}}\\right) \\right]^{-1} $$\n    c. **Construct CDFs (Fundamentals iii and iv)**:\n        i.  **Model CDF**: Using the estimated $\\hat{\\alpha}$, the fitted model CDF for the tail is $F_{\\text{model}}(x) = 1 - (\\frac{x_{\\min}}{x})^{\\hat{\\alpha}}$ for $x \\ge x_{\\min}$.\n        ii. **Empirical CDF**: The ECDF for the tail data $D_{\\text{tail}}$ is $S_{\\text{tail}}(x) = \\frac{1}{n_{\\text{tail}}} \\sum_{X_i \\in D_{\\text{tail}}} I(X_i \\le x)$, where $I(\\cdot)$ is the indicator function. This ECDF is also defined for $x \\ge x_{\\min}$.\n    d. **Calculate the KS Statistic (Fundamental iv)**: Compute the distance between the model and the data for this choice of $x_{\\min}$. The KS statistic is the maximum absolute difference between the two CDFs:\n    $$ D(x_{\\min}) = \\sup_{x \\ge x_{\\min}} |F_{\\text{model}}(x) - S_{\\text{tail}}(x)| $$\n\n4.  **Select the Optimal Cutoff**: The best-fit cutoff, $\\hat{x}_{\\min}$, is the one that minimizes the KS distance, making the data in the tail \"look most like\" a Pareto distribution:\n    $$ \\hat{x}_{\\min} = \\arg\\min_{x_{\\min}} D(x_{\\min}) $$\n\n### Analysis of the Bias–Variance Trade-off\n\nThe choice of $x_{\\min}$ involves a fundamental trade-off:\n-   **Bias**: If $x_{\\min}$ is chosen too low (i.e., smaller than the true onset of the power law), the data subset $D_{\\text{tail}}$ will be contaminated with non-Pareto-distributed points. Fitting a Pareto model to this mixed data will result in a biased estimate $\\hat{\\alpha}$. This is a form of model misspecification bias. Increasing $x_{\\min}$ pushes the cutoff deeper into the tail, where the power-law hypothesis is more likely to be accurate, thus **decreasing** this bias.\n-   **Variance**: The variance of the estimator $\\hat{\\alpha}$ is inversely related to the number of data points used, $n_{\\text{tail}}$. The asymptotic variance of the MLE is approximately $\\hat{\\alpha}^2 / n_{\\text{tail}}$. As $x_{\\min}$ increases, $n_{\\text{tail}}$ necessarily decreases (or stays the same). A smaller sample size leads to greater statistical fluctuation, thereby **increasing** the variance of the estimate $\\hat{\\alpha}$.\n\nThe ideal $x_{\\min}$ strikes a balance, being large enough to minimize bias but small enough to retain sufficient data for a low-variance estimate. The KS minimization procedure is a heuristic designed to find this balance point.\n\n### Option-by-Option Analysis\n\n**A. For each candidate cutoff $x_{\\min}$ taken from distinct sample values, restrict attention to observations with $X_i \\ge x_{\\min}$, estimate the Pareto tail parameter by maximum likelihood on that restricted sample, form the model cumulative distribution on $x \\ge x_{\\min}$, compute the Kolmogorov–Smirnov distance between this model and the empirical cumulative distribution of the restricted sample, and select the $x_{\\min}$ that minimizes this distance. Increasing $x_{\\min}$ generally decreases model misspecification bias at the cost of increasing variance due to fewer tail observations.**\n\n- **Procedure**: This precisely matches the step-by-step procedure derived from the given fundamentals. It correctly specifies using MLE on the restricted sample, comparing the resulting model CDF to the ECDF of the *same* restricted sample, and minimizing the KS distance.\n- **Trade-off**: The characterization of the bias–variance trade-off is correct. Increasing $x_{\\min}$ reduces model misspecification bias but increases estimator variance due to a smaller sample size.\n- **Verdict**: **Correct**.\n\n**B. Use all observations without truncation to fit a power law by linear regression on the log–log rank–size plot, then select $x_{\\min}$ as the smallest value beyond which the regression residuals first fall below a fixed threshold; raising $x_{\\min}$ reduces variance because the fitted slope is estimated from a region that more closely follows a straight line.**\n\n- **Procedure**: This describes a completely different method (log-log regression) that violates the constraint to use MLE (Fundamental ii) and the KS statistic (Fundamental iv). It also incorrectly fits a single model to all data.\n- **Trade-off**: The reasoning is flawed. Fitting to a more linear region reduces *bias*, not variance. Reducing the number of points for the fit will increase the variance of the slope estimate.\n- **Verdict**: **Incorrect**.\n\n**C. Set $x_{\\min}$ equal to the minimum observed value to avoid selection bias and fit the Pareto model using all observations; any truncation would increase bias and should be avoided, since including more data always reduces both bias and variance.**\n\n- **Procedure**: This contradicts the problem's premise that the power law may only hold in the tail. It forces the model on the entire dataset, which is incorrect if the hypothesis is true.\n- **Trade-off**: The reasoning is fundamentally incorrect. Including data from a different underlying distribution (the non-tail part) *increases* model bias. The statement that more data always reduces both bias and variance is false; it generally reduces variance but can increase bias if the new data does not follow the model.\n- **Verdict**: **Incorrect**.\n\n**D. For each candidate $x_{\\min}$, estimate the Pareto parameter by matching the sample mean of the restricted data to the theoretical mean of a Pareto tail, then select the $x_{\\min}$ that minimizes the average absolute error between the empirical and theoretical probability density functions on $x \\ge x_{\\min}$; increasing $x_{\\min}$ increases both bias and variance because it discards data and distorts the fitted mean.**\n\n- **Procedure**: This method violates two fundamentals. It uses the method of moments, not MLE (Fundamental ii). It uses the $L_1$ distance between PDFs, not the KS distance ($L_\\infty$ on CDFs) (Fundamental iv).\n- **Trade-off**: The description of the trade-off is incorrect. As established, increasing $x_{\\min}$ typically decreases bias while increasing variance.\n- **Verdict**: **Incorrect**.\n\n**E. For each candidate $x_{\\min}$, estimate the Pareto parameter by maximum likelihood on the restricted sample with $X_i \\ge x_{\\min}$, but compute the Kolmogorov–Smirnov statistic against the model on the full data, including points with $X_i  x_{\\min}$, and choose the $x_{\\min}$ that maximizes this statistic; raising $x_{\\min}$ reduces both bias and variance because the fit ignores more outliers and the discrepancy measure becomes larger, indicating a better fit.**\n\n- **Procedure**: The procedure is nonsensical. The fitted model is only defined for $x \\ge x_{\\min}$ and cannot be compared to the ECDF of the full dataset. Furthermore, one must *minimize*, not maximize, a distance metric to find the best fit.\n- **Trade-off**: The reasoning is entirely flawed. Increasing $x_{\\min}$ increases, not reduces, variance. Maximizing a discrepancy measure indicates a worse, not a better, fit.\n- **Verdict**: **Incorrect**.",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}