{
    "hands_on_practices": [
        {
            "introduction": "The branching process serves as the canonical model for understanding how a network's activity can be poised at a critical state, balanced between dying out and runaway excitation. This simple yet powerful framework models the propagation of activity from one generation to the next. In this exercise , you will connect abstract theory to concrete computation by determining the system's effective branching parameter, which governs whether the dynamics are subcritical, critical, or supercritical.",
            "id": "4308671",
            "problem": "Consider a multi-type Galton–Watson branching process defined on a directed, weighted network that represents a coarse-grained connectome. Let the network be represented by a nonnegative weighted adjacency matrix $W\\in\\mathbb{R}_{\\ge 0}^{n\\times n}$, where $W_{ij}$ quantifies the tract strength from node $i$ to node $j$. Let an excitability parameter $\\alpha\\in\\mathbb{R}_{\\ge 0}$ scale how tract strengths translate into expected offspring counts. Define the mean offspring matrix $M\\in\\mathbb{R}_{\\ge 0}^{n\\times n}$ by $M_{ij}=\\alpha W_{ij}$. At discrete time $t\\in\\mathbb{N}$, the number of active units of type $j$ produced by a single active unit of type $i$ is modeled as a random variable with a Poisson distribution whose mean is $M_{ij}$, independently across $i$, $j$, and across trials. Let $\\mathbf{X}_t\\in\\mathbb{N}^n$ denote the random vector of active units at time $t$.\n\nThe effective branching parameter $b_{\\mathrm{eff}}$ is the dominant asymptotic per-generation multiplication factor of the expected total activity, formally defined as the unique positive scalar that satisfies the following growth-rate property: for large $t$, the expected total activity $\\|\\mathbb{E}[\\mathbf{X}_t]\\|_1$ scales proportionally to $b_{\\mathrm{eff}}^t$ up to multiplicative constants. Under widely used assumptions in complex systems and network science, this $b_{\\mathrm{eff}}$ can be computed directly from the mean offspring matrix $M$.\n\nYour task is to implement this branching-process model and compute $b_{\\mathrm{eff}}$ from $M$ for each test case specified below. You must not assume any special symmetries beyond nonnegativity, and you must treat all matrices as directed and weighted. All computations are dimensionless, so no physical units apply.\n\nImplementation requirements:\n- Construct $M$ from the given $W$ and $\\alpha$ using $M=\\alpha W$.\n- Compute $b_{\\mathrm{eff}}$ for each test case directly from $M$ using a principled method grounded in first principles of multi-type branching processes.\n- For scientific realism, implement a simulator of the branching process that can generate sample cascades across generations from an initial active state; this simulator is to be used internally for validation but does not affect the final required output format.\n\nTest suite:\n- Case $1$: $W^{(1)}=\\begin{bmatrix}0 & 0.5 & 0 \\\\ 0.5 & 0 & 0.5 \\\\ 0 & 0.5 & 0\\end{bmatrix}$, $\\alpha^{(1)}=1.0$.\n- Case $2$: $W^{(2)}=W^{(1)}$, $\\alpha^{(2)}=1.4142135623730951$.\n- Case $3$: $W^{(3)}=\\begin{bmatrix}0 & 2.0 & 0 \\\\ 0 & 0 & 1.0 \\\\ 0 & 0 & 0\\end{bmatrix}$, $\\alpha^{(3)}=0.5$.\n- Case $4$: $W^{(4)}=\\begin{bmatrix}0.8 & 0.1 & 0 & 0 \\\\ 0.1 & 0.2 & 0 & 0 \\\\ 0 & 0 & 0 & 0.3 \\\\ 0 & 0 & 0.4 & 0\\end{bmatrix}$, $\\alpha^{(4)}=1.2$.\n- Case $5$: $W^{(5)}=\\begin{bmatrix}0 & 0 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0\\end{bmatrix}$, $\\alpha^{(5)}=10.0$.\n\nAnswer specification:\n- For each case, compute $b_{\\mathrm{eff}}$ from $M$ and return it as a floating-point number rounded to six decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of cases $1$ through $5$ (for example, $[r_1,r_2,r_3,r_4,r_5]$). No additional text should be printed.",
            "solution": "The problem is subjected to validation before a solution is attempted.\n\n### Step 1: Extract Givens\n-   **Network**: A nonnegative weighted adjacency matrix $W \\in \\mathbb{R}_{\\ge 0}^{n \\times n}$, where $W_{ij}$ is the tract strength from node $i$ to node $j$.\n-   **Excitability**: A parameter $\\alpha \\in \\mathbb{R}_{\\ge 0}$.\n-   **Mean Offspring Matrix**: $M \\in \\mathbb{R}_{\\ge 0}^{n \\times n}$ defined by $M_{ij} = \\alpha W_{ij}$.\n-   **Branching Process**: The number of active units of type $j$ produced by a single unit of type $i$ at time $t$ is a Poisson-distributed random variable with mean $M_{ij}$.\n-   **State Vector**: $\\mathbf{X}_t \\in \\mathbb{N}^n$ is the random vector of active units at time $t$.\n-   **Effective Branching Parameter**: $b_{\\mathrm{eff}}$ is a scalar such that for large $t$, the expected total activity satisfies $\\|\\mathbb{E}[\\mathbf{X}_t]\\|_1 \\propto b_{\\mathrm{eff}}^t$.\n-   **Task**: Compute $b_{\\mathrm{eff}}$ for five given test cases.\n-   **Test Cases**:\n    1.  $W^{(1)}=\\begin{bmatrix}0 & 0.5 & 0 \\\\ 0.5 & 0 & 0.5 \\\\ 0 & 0.5 & 0\\end{bmatrix}$, $\\alpha^{(1)}=1.0$.\n    2.  $W^{(2)}=W^{(1)}$, $\\alpha^{(2)}=1.4142135623730951$.\n    3.  $W^{(3)}=\\begin{bmatrix}0 & 2.0 & 0 \\\\ 0 & 0 & 1.0 \\\\ 0 & 0 & 0\\end{bmatrix}$, $\\alpha^{(3)}=0.5$.\n    4.  $W^{(4)}=\\begin{bmatrix}0.8 & 0.1 & 0 & 0 \\\\ 0.1 & 0.2 & 0 & 0 \\\\ 0 & 0 & 0 & 0.3 \\\\ 0 & 0 & 0.4 & 0\\end{bmatrix}$, $\\alpha^{(4)}=1.2$.\n    5.  $W^{(5)}=\\begin{bmatrix}0 & 0 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0\\end{bmatrix}$, $\\alpha^{(5)}=10.0$.\n\n### Step 2: Validate Using Extracted Givens\n-   **Scientifically Grounded**: The problem is based on the standard theory of multi-type Galton-Watson branching processes, a cornerstone of applied probability and a common model in fields like epidemiology, population dynamics, and network science (including neuroscience, via the critical brain hypothesis). The formulation is correct and uses established principles.\n-   **Well-Posed**: The problem is well-posed. The quantity to be computed, $b_{\\mathrm{eff}}$, is defined as the asymptotic growth rate of the expected activity. In the theory of multi-type branching processes, this rate is uniquely determined by the mean offspring matrix $M$. All necessary data ($W, \\alpha$) are provided for each case.\n-   **Objective**: The problem is stated using precise mathematical language, free of ambiguity or subjective claims.\n\n### Step 3: Verdict and Action\nThe problem is valid as it is scientifically sound, self-contained, and well-posed. A solution will be provided.\n\n### Principle-Based Solution\n\nThe core of the problem is to determine the effective branching parameter $b_{\\mathrm{eff}}$ from the mean offspring matrix $M$. This requires establishing the relationship between the dynamics of the expected population size and the spectral properties of $M$.\n\nLet $\\mathbf{X}_t = [X_t^{(1)}, X_t^{(2)}, \\dots, X_t^{(n)}]^T$ be the column vector representing the number of active units of each type at generation $t$. Let $\\mathbf{x}_t = \\mathbb{E}[\\mathbf{X}_t]$ be the vector of expected counts.\n\nThe expected number of offspring of type $j$ produced by a single parent of type $i$ is given by $M_{ij}$. By the linearity of expectation, the total expected number of offspring of type $j$ at generation $t+1$, denoted $x_{t+1}^{(j)}$, is the sum of the expected contributions from all types at generation $t$:\n$$\nx_{t+1}^{(j)} = \\mathbb{E}[X_{t+1}^{(j)}] = \\sum_{i=1}^n \\mathbb{E}[\\text{offspring of type } j \\text{ from } X_t^{(i)} \\text{ parents of type } i]\n$$\nGiven that the expected number of offspring of type $j$ from a single parent of type $i$ is $M_{ij}$, and there are $X_t^{(i)}$ such parents, the expected number of offspring of type $j$ from the population of type $i$ is $x_t^{(i)} M_{ij}$. Taking the expectation over the distribution of $X_t^{(i)}$ is already done in $x_t^{(i)}$. Thus, we have:\n$$\nx_{t+1}^{(j)} = \\sum_{i=1}^n x_t^{(i)} M_{ij}\n$$\nThis equation defines the $j$-th component of a matrix-vector product. If we represent the expected counts as a row vector $\\mathbf{x}_t^T = [x_t^{(1)}, \\dots, x_t^{(n)}]$, the dynamics are given by $\\mathbf{x}_{t+1}^T = \\mathbf{x}_t^T M$.\nAlternatively, and more conventionally in linear algebra, using column vectors:\n$$\n\\mathbf{x}_{t+1} = M^T \\mathbf{x}_t\n$$\nIterating this relationship from an initial state $\\mathbf{x}_0$ gives:\n$$\n\\mathbf{x}_t = (M^T)^t \\mathbf{x}_0\n$$\nThe long-term asymptotic behavior of $\\mathbf{x}_t$ is governed by the powers of the matrix $M^T$. For a non-negative matrix $M^T$, the Perron-Frobenius theorem dictates that its spectral radius, $\\rho(M^T)$, is a non-negative real eigenvalue, and for large $t$, the norm of $\\mathbf{x}_t$ scales as:\n$$\n\\|\\mathbf{x}_t\\| \\propto (\\rho(M^T))^t\n$$\nThe problem defines $b_{\\mathrm{eff}}$ as the base of this exponential growth, i.e., $\\|\\mathbb{E}[\\mathbf{X}_t]\\|_1 \\propto b_{\\mathrm{eff}}^t$. Therefore, we can identify $b_{\\mathrm{eff}}$ with the spectral radius of $M^T$:\n$$\nb_{\\mathrm{eff}} = \\rho(M^T)\n$$\nA fundamental property of matrices is that a matrix and its transpose have the same set of eigenvalues, and thus the same spectral radius: $\\rho(M^T) = \\rho(M)$. The spectral radius $\\rho(M)$ is defined as the maximum absolute value of the eigenvalues of $M$.\n$$\nb_{\\mathrm{eff}} = \\rho(M) = \\max_i |\\lambda_i|\n$$\nwhere $\\lambda_i$ are the eigenvalues of $M$.\n\nThe computational procedure for each test case is as follows:\n1.  Construct the mean offspring matrix $M = \\alpha W$.\n2.  Compute the eigenvalues of $M$.\n3.  Find the maximum of the absolute values of these eigenvalues. This value is $b_{\\mathrm{eff}}$.\n\n**Case 1:**\nGiven $W^{(1)}=\\begin{bmatrix}0 & 0.5 & 0 \\\\ 0.5 & 0 & 0.5 \\\\ 0 & 0.5 & 0\\end{bmatrix}$ and $\\alpha^{(1)}=1.0$.\n$M^{(1)} = 1.0 \\cdot W^{(1)} = \\begin{bmatrix}0 & 0.5 & 0 \\\\ 0.5 & 0 & 0.5 \\\\ 0 & 0.5 & 0\\end{bmatrix}$.\nThe characteristic equation is $\\det(M^{(1)} - \\lambda I) = -\\lambda(\\lambda^2 - 0.5) = 0$.\nThe eigenvalues are $\\lambda_1=0$, $\\lambda_2=\\sqrt{0.5}$, $\\lambda_3=-\\sqrt{0.5}$.\nThe absolute values are $|0|=0$, $|\\sqrt{0.5}| \\approx 0.707107$, $|-\\sqrt{0.5}| \\approx 0.707107$.\n$b_{\\mathrm{eff}}^{(1)} = \\rho(M^{(1)}) = \\sqrt{0.5} \\approx 0.707107$.\n\n**Case 2:**\nGiven $W^{(2)}=W^{(1)}$ and $\\alpha^{(2)}=\\sqrt{2} \\approx 1.41421356$.\n$M^{(2)} = \\sqrt{2} \\cdot W^{(1)}$. The eigenvalues of $c A$ are $c$ times the eigenvalues of $A$.\nThe eigenvalues of $M^{(2)}$ are $\\sqrt{2} \\cdot 0 = 0$, $\\sqrt{2} \\cdot \\sqrt{0.5} = 1$, and $\\sqrt{2} \\cdot (-\\sqrt{0.5}) = -1$.\nThe absolute values are $|0|=0$, $|1|=1$, $|-1|=1$.\n$b_{\\mathrm{eff}}^{(2)} = \\rho(M^{(2)}) = 1.0$.\n\n**Case 3:**\nGiven $W^{(3)}=\\begin{bmatrix}0 & 2.0 & 0 \\\\ 0 & 0 & 1.0 \\\\ 0 & 0 & 0\\end{bmatrix}$ and $\\alpha^{(3)}=0.5$.\n$M^{(3)} = 0.5 \\cdot W^{(3)} = \\begin{bmatrix}0 & 1.0 & 0 \\\\ 0 & 0 & 0.5 \\\\ 0 & 0 & 0\\end{bmatrix}$.\nThis is an upper triangular matrix. Its eigenvalues are its diagonal entries.\nThe eigenvalues are $\\lambda_1=0$, $\\lambda_2=0$, $\\lambda_3=0$.\n$b_{\\mathrm{eff}}^{(3)} = \\rho(M^{(3)}) = 0.0$.\n\n**Case 4:**\nGiven $W^{(4)}=\\begin{bmatrix}0.8 & 0.1 & 0 & 0 \\\\ 0.1 & 0.2 & 0 & 0 \\\\ 0 & 0 & 0 & 0.3 \\\\ 0 & 0 & 0.4 & 0\\end{bmatrix}$ and $\\alpha^{(4)}=1.2$.\n$M^{(4)} = 1.2 \\cdot W^{(4)} = \\begin{bmatrix}0.96 & 0.12 & 0 & 0 \\\\ 0.12 & 0.24 & 0 & 0 \\\\ 0 & 0 & 0 & 0.36 \\\\ 0 & 0 & 0.48 & 0\\end{bmatrix}$.\nThis is a block diagonal matrix. Its eigenvalues are the union of the eigenvalues of its diagonal blocks.\nBlock $A = \\begin{bmatrix}0.96 & 0.12 \\\\ 0.12 & 0.24\\end{bmatrix}$: Eigenvalues are $\\approx 0.979473$ and $\\approx 0.220527$.\nBlock $B = \\begin{bmatrix}0 & 0.36 \\\\ 0.48 & 0\\end{bmatrix}$: Eigenvalues are $\\pm\\sqrt{0.36 \\cdot 0.48} = \\pm\\sqrt{0.1728} \\approx \\pm 0.415692$.\nThe set of eigenvalues of $M^{(4)}$ is approximately $\\{0.979473, 0.220527, 0.415692, -0.415692\\}$.\nThe maximum absolute value is $\\approx 0.979473$.\n$b_{\\mathrm{eff}}^{(4)} = \\rho(M^{(4)}) \\approx 0.979473$.\n\n**Case 5:**\nGiven $W^{(5)}=\\begin{bmatrix}0 & 0 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0\\end{bmatrix}$ and $\\alpha^{(5)}=10.0$.\n$M^{(5)} = 10.0 \\cdot W^{(5)} = \\begin{bmatrix}0 & 0 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0\\end{bmatrix}$.\nThis is the zero matrix. Its only eigenvalue is $0$.\n$b_{\\mathrm{eff}}^{(5)} = \\rho(M^{(5)}) = 0.0$.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the effective branching parameter for a multi-type Galton-Watson\n    process defined on a network for a series of test cases.\n    \"\"\"\n\n    def _simulate_branching_process(M, x0, T, seed=None):\n        \"\"\"\n        Internal validation function to simulate the branching process.\n        This function is included to fulfill the implementation requirements but\n        is not called to produce the final output.\n\n        Args:\n            M (np.ndarray): Mean offspring matrix.\n            x0 (np.ndarray): Initial state vector (number of active units).\n            T (int): Number of time steps (generations) to simulate.\n            seed (int, optional): Seed for the random number generator.\n\n        Returns:\n            list: A list of state vectors, representing the history of active units.\n        \"\"\"\n        if seed is not None:\n            np.random.seed(seed)\n        \n        n = M.shape[0]\n        x_current = np.array(x0, dtype=int)\n        history = [x_current.copy()]\n\n        for _ in range(T):\n            x_next = np.zeros(n, dtype=int)\n            for i in range(n):\n                if x_current[i] > 0:\n                    # The sum of k independent Poisson(lambda) variables is\n                    # a Poisson(k * lambda) variable.\n                    # Here, we have x_current[i] parents of type i.\n                    for j in range(n):\n                        if M[i, j] > 0:\n                            lam = x_current[i] * M[i, j]\n                            offspring_j_from_i = np.random.poisson(lam=lam)\n                            x_next[j] += offspring_j_from_i\n            x_current = x_next\n            history.append(x_current.copy())\n        \n        return history\n\n    def compute_beff(W, alpha):\n        \"\"\"\n        Computes the effective branching parameter b_eff.\n\n        b_eff is the spectral radius of the mean offspring matrix M.\n        \n        Args:\n            W (np.ndarray): The weighted adjacency matrix.\n            alpha (float): The excitability parameter.\n\n        Returns:\n            float: The effective branching parameter b_eff.\n        \"\"\"\n        M = alpha * np.array(W, dtype=float)\n        eigenvalues = np.linalg.eigvals(M)\n        b_eff = np.max(np.abs(eigenvalues))\n        return b_eff\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1\n        (\n            np.array([[0.0, 0.5, 0.0], [0.5, 0.0, 0.5], [0.0, 0.5, 0.0]]),\n            1.0\n        ),\n        # Case 2\n        (\n            np.array([[0.0, 0.5, 0.0], [0.5, 0.0, 0.5], [0.0, 0.5, 0.0]]),\n            1.4142135623730951  # sqrt(2)\n        ),\n        # Case 3\n        (\n            np.array([[0.0, 2.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 0.0]]),\n            0.5\n        ),\n        # Case 4\n        (\n            np.array([[0.8, 0.1, 0.0, 0.0], [0.1, 0.2, 0.0, 0.0], [0.0, 0.0, 0.0, 0.3], [0.0, 0.0, 0.4, 0.0]]),\n            1.2\n        ),\n        # Case 5\n        (\n            np.array([[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]]),\n            10.0\n        ),\n    ]\n\n    results = []\n    for W, alpha in test_cases:\n        result = compute_beff(W, alpha)\n        results.append(result)\n    \n    # Format the final list of results rounded to six decimal places.\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    \n    # Print the results in the specified single-line format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A key signature of a system at a critical point is the appearance of power-law distributions for event sizes and durations, described by a set of critical exponents. These exponents, such as $\\tau$ and $\\alpha$, are not arbitrary but are predicted to be constrained by scaling relations, reflecting the underlying universality of the phenomenon. This practice  provides an opportunity to derive one such relation and apply it to a hypothetical dataset, a common task when assessing whether empirical findings are consistent with the critical brain hypothesis.",
            "id": "4308702",
            "problem": "A high-density microelectrode array records neuronal avalanches in an awake mammal under spontaneous activity. Avalanche size $S$ is defined as the total number of above-threshold events within a spatiotemporally contiguous cluster, and avalanche duration $T$ is the number of consecutive time bins occupied by that cluster. The critical brain hypothesis posits that neuronal dynamics are poised near a continuous phase transition, implying scale-invariant avalanche statistics. The following are empirically observed from a single, stationary recording epoch:\n- The marginal size distribution scales as $p(S) \\sim S^{-\\tau}$ with estimate $\\hat{\\tau} = 1.52$ and standard error $\\sigma_{\\tau} = 0.05$.\n- The marginal duration distribution scales as $p(T) \\sim T^{-\\alpha}$ with estimate $\\hat{\\alpha} = 1.90$ and standard error $\\sigma_{\\alpha} = 0.06$.\n- The conditional mean size at fixed duration scales as $\\langle S \\mid T \\rangle \\sim T^{\\gamma_{\\mathrm{emp}}}$ with empirical slope $\\hat{\\gamma}_{\\mathrm{emp}} = 1.68$ and standard error $\\sigma_{\\mathrm{emp}} = 0.08$.\n\nAssume the following foundational base:\n- Scale invariance is captured by a finite-size scaling hypothesis for the joint statistics consistent with homogeneous functions.\n- The conditional distribution $p(S \\mid T)$ exhibits the scaling collapse implied by $\\langle S \\mid T \\rangle \\sim T^{\\gamma}$.\n- Reported uncertainties are one-standard-error Gaussian approximations, and $\\tau$ and $\\alpha$ estimates are independent of each other and of $\\gamma_{\\mathrm{emp}}$.\n\nTasks:\n1. Starting from a scale-invariant joint distribution consistent with the finite-size scaling hypothesis and the assumption that the conditional mean avalanche size at fixed duration scales as $\\langle S \\mid T \\rangle \\sim T^{\\gamma}$, derive the scaling relation linking $\\gamma$ to the marginal exponents $\\tau$ and $\\alpha$.\n2. Using first-order Gaussian error propagation under independence, compute the theoretical prediction $\\gamma_{\\mathrm{th}}$ and its standard error $\\sigma_{\\gamma}$ from the estimates $\\hat{\\tau}$ and $\\hat{\\alpha}$ with their standard errors $\\sigma_{\\tau}$ and $\\sigma_{\\alpha}$.\n3. Define the standardized discrepancy\n$$\nz \\equiv \\frac{\\hat{\\gamma}_{\\mathrm{emp}} - \\gamma_{\\mathrm{th}}}{\\sqrt{\\sigma_{\\gamma}^{2} + \\sigma_{\\mathrm{emp}}^{2}}}.\n$$\nCompute $z$.\n\nReport as your final answer only the value of $z$, rounded to three significant figures. The answer is dimensionless; do not include units.",
            "solution": "The problem requires the derivation of a scaling relation, the calculation of a theoretical prediction with its uncertainty, and finally the computation of a standardized discrepancy. I will address these tasks sequentially.\n\nFirst, the validation of the problem statement is successful. The givens are:\n- Avalanche size distribution: $p(S) \\sim S^{-\\tau}$ with $\\hat{\\tau} = 1.52$, $\\sigma_{\\tau} = 0.05$.\n- Avalanche duration distribution: $p(T) \\sim T^{-\\alpha}$ with $\\hat{\\alpha} = 1.90$, $\\sigma_{\\alpha} = 0.06$.\n- Conditional mean size: $\\langle S \\mid T \\rangle \\sim T^{\\gamma_{\\mathrm{emp}}}$ with $\\hat{\\gamma}_{\\mathrm{emp}} = 1.68$, $\\sigma_{\\mathrm{emp}} = 0.08$.\n- Assumptions: scale-invariant joint statistics, scaling collapse of $p(S \\mid T)$, Gaussian errors, and independence of estimates.\n\nThe problem is scientifically grounded in the theory of critical phenomena applied to neuroscience, is well-posed, objective, and internally consistent. It is a valid problem.\n\n**Task 1: Derivation of the scaling relation**\n\nThe critical brain hypothesis posits scale invariance, which can be formalized using a scaling ansatz for the joint probability distribution of avalanche size $S$ and duration $T$, $p(S, T)$. A form consistent with the problem's stated assumptions is that of a generalized homogeneous function:\n$$p(S, T) \\propto T^{-a} \\mathcal{F}\\left(\\frac{S}{T^{\\gamma}}\\right)$$\nwhere $a$ and $\\gamma$ are scaling exponents and $\\mathcal{F}$ is a scaling function. The exponent $\\gamma$ is chosen to match the scaling of the conditional mean size, $\\langle S \\mid T \\rangle \\sim T^{\\gamma}$, which we verify below.\n\nTo derive the relation between $\\gamma$, $\\tau$, and $\\alpha$, we must recover the marginal distributions $p(T)$ and $p(S)$ by integrating the joint distribution.\n\nThe marginal distribution for duration $T$ is:\n$$p(T) = \\int_{0}^{\\infty} p(S, T) dS \\propto \\int_{0}^{\\infty} T^{-a} \\mathcal{F}\\left(\\frac{S}{T^{\\gamma}}\\right) dS$$\nLet us perform a change of variables $u = S/T^{\\gamma}$, which implies $S = uT^{\\gamma}$ and $dS = T^{\\gamma}du$.\n$$p(T) \\propto T^{-a} \\int_{0}^{\\infty} \\mathcal{F}(u) (T^{\\gamma}du) = T^{\\gamma-a} \\int_{0}^{\\infty} \\mathcal{F}(u)du$$\nThe integral over $u$ is a constant, so $p(T) \\propto T^{\\gamma-a}$. By comparing this to the given empirical scaling $p(T) \\sim T^{-\\alpha}$, we find:\n$$-\\alpha = \\gamma - a \\implies a = \\gamma + \\alpha$$\n\nThe marginal distribution for size $S$ is:\n$$p(S) = \\int_{0}^{\\infty} p(S, T) dT \\propto \\int_{0}^{\\infty} T^{-a} \\mathcal{F}\\left(\\frac{S}{T^{\\gamma}}\\right) dT$$\nLet us perform a change of variables $v = S/T^{\\gamma}$, which implies $T = (S/v)^{1/\\gamma}$ and $dT = -\\frac{1}{\\gamma} S^{1/\\gamma} v^{-1/\\gamma - 1} dv$.\n$$p(S) \\propto \\int_{\\infty}^{0} \\left(\\left(\\frac{S}{v}\\right)^{1/\\gamma}\\right)^{-a} \\mathcal{F}(v) \\left(-\\frac{1}{\\gamma} S^{1/\\gamma} v^{-1/\\gamma - 1} dv\\right)$$\n$$p(S) \\propto S^{-a/\\gamma} S^{1/\\gamma} \\int_{0}^{\\infty} v^{a/\\gamma} \\mathcal{F}(v) v^{-1/\\gamma - 1} dv$$\n$$p(S) \\propto S^{(1-a)/\\gamma} \\int_{0}^{\\infty} v^{\\frac{a-1}{\\gamma} - 1} \\mathcal{F}(v) dv$$\nThe integral is a constant, so $p(S) \\propto S^{(1-a)/\\gamma}$. By comparing this to the given empirical scaling $p(S) \\sim S^{-\\tau}$, we find:\n$$-\\tau = \\frac{1-a}{\\gamma} \\implies a = 1 + \\gamma\\tau$$\n\nWe now have two expressions for the exponent $a$. Equating them yields the desired scaling relation:\n$$\\gamma + \\alpha = 1 + \\gamma\\tau$$\n$$\\gamma\\tau - \\gamma = \\alpha - 1$$\n$$\\gamma(\\tau - 1) = \\alpha - 1$$\n$$\\gamma = \\frac{\\alpha - 1}{\\tau - 1}$$\nThis completes the first task. This is a well-known scaling relation in the physics of critical phenomena.\n\n**Task 2: Computation of $\\gamma_{\\mathrm{th}}$ and $\\sigma_{\\gamma}$**\n\nThe theoretical prediction, $\\gamma_{\\mathrm{th}}$, is obtained by substituting the estimated values $\\hat{\\tau}$ and $\\hat{\\alpha}$ into the derived relation:\n$$\\gamma_{\\mathrm{th}} = \\frac{\\hat{\\alpha} - 1}{\\hat{\\tau} - 1} = \\frac{1.90 - 1}{1.52 - 1} = \\frac{0.90}{0.52} = \\frac{45}{26}$$\n\nTo find the standard error $\\sigma_{\\gamma}$, we use first-order Gaussian error propagation. For a function $\\gamma(\\alpha, \\tau)$, the variance $\\sigma_{\\gamma}^2$ is given by:\n$$\\sigma_{\\gamma}^2 \\approx \\left(\\frac{\\partial \\gamma}{\\partial \\alpha}\\right)^2 \\sigma_{\\alpha}^2 + \\left(\\frac{\\partial \\gamma}{\\partial \\tau}\\right)^2 \\sigma_{\\tau}^2$$\nThe covariance term is zero due to the assumption of independence. First, we compute the partial derivatives:\n$$\\frac{\\partial \\gamma}{\\partial \\alpha} = \\frac{\\partial}{\\partial \\alpha} \\left(\\frac{\\alpha - 1}{\\tau - 1}\\right) = \\frac{1}{\\tau - 1}$$\n$$\\frac{\\partial \\gamma}{\\partial \\tau} = \\frac{\\partial}{\\partial \\tau} \\left((\\alpha - 1)(\\tau - 1)^{-1}\\right) = -(\\alpha - 1)(\\tau - 1)^{-2} = -\\frac{\\alpha - 1}{(\\tau - 1)^2}$$\nNext, we evaluate these derivatives at the mean values $(\\hat{\\alpha}, \\hat{\\tau})$:\n$$\\frac{\\partial \\gamma}{\\partial \\alpha}\\bigg|_{(\\hat{\\alpha}, \\hat{\\tau})} = \\frac{1}{1.52 - 1} = \\frac{1}{0.52}$$\n$$\\frac{\\partial \\gamma}{\\partial \\tau}\\bigg|_{(\\hat{\\alpha}, \\hat{\\tau})} = -\\frac{1.90 - 1}{(1.52 - 1)^2} = -\\frac{0.90}{(0.52)^2}$$\nNow, we substitute these into the variance formula along with the given standard errors $\\sigma_{\\alpha} = 0.06$ and $\\sigma_{\\tau} = 0.05$:\n$$\\sigma_{\\gamma}^2 \\approx \\left(\\frac{1}{0.52}\\right)^2 (0.06)^2 + \\left(-\\frac{0.90}{(0.52)^2}\\right)^2 (0.05)^2$$\n$$\\sigma_{\\gamma}^2 \\approx \\frac{1}{0.2704} (0.0036) + \\frac{0.81}{0.07311616} (0.0025)$$\nTo maintain precision, we can rearrange the expression:\n$$\\sigma_{\\gamma}^2 = \\frac{1}{(\\hat{\\tau}-1)^2} \\left[ \\sigma_{\\alpha}^2 + \\left(\\frac{\\hat{\\alpha}-1}{\\hat{\\tau}-1}\\right)^2 \\sigma_{\\tau}^2 \\right] = \\frac{1}{(\\hat{\\tau}-1)^2} \\left[ \\sigma_{\\alpha}^2 + \\gamma_{\\mathrm{th}}^2 \\sigma_{\\tau}^2 \\right]$$\n$$\\sigma_{\\gamma}^2 \\approx \\frac{1}{(0.52)^2} \\left[ (0.06)^2 + \\left(\\frac{45}{26}\\right)^2 (0.05)^2 \\right]$$\n$$\\sigma_{\\gamma}^2 \\approx \\frac{1}{0.2704} \\left[ 0.0036 + \\left(\\frac{2025}{676}\\right) (0.0025) \\right]$$\n$$\\sigma_{\\gamma}^2 \\approx \\frac{1}{0.2704} \\left[ 0.0036 + (2.9955621...)(0.0025) \\right]$$\n$$\\sigma_{\\gamma}^2 \\approx \\frac{1}{0.2704} \\left[ 0.0036 + 0.007488905... \\right] = \\frac{0.011088905...}{0.2704} \\approx 0.041009265$$\n\n**Task 3: Computation of the standardized discrepancy $z$**\n\nThe standardized discrepancy $z$ is defined as:\n$$z = \\frac{\\hat{\\gamma}_{\\mathrm{emp}} - \\gamma_{\\mathrm{th}}}{\\sqrt{\\sigma_{\\gamma}^{2} + \\sigma_{\\mathrm{emp}}^{2}}}$$\nWe have the following values:\n- $\\hat{\\gamma}_{\\mathrm{emp}} = 1.68$\n- $\\gamma_{\\mathrm{th}} = \\frac{45}{26} \\approx 1.730769...$\n- $\\sigma_{\\gamma}^{2} \\approx 0.041009265$\n- $\\sigma_{\\mathrm{emp}} = 0.08 \\implies \\sigma_{\\mathrm{emp}}^{2} = (0.08)^2 = 0.0064$\n\nFirst, compute the numerator:\n$$\\hat{\\gamma}_{\\mathrm{emp}} - \\gamma_{\\mathrm{th}} = 1.68 - \\frac{45}{26} = \\frac{1.68 \\times 26 - 45}{26} = \\frac{43.68 - 45}{26} = -\\frac{1.32}{26} \\approx -0.050769...$$\nNext, compute the sum of variances in the denominator:\n$$\\sigma_{\\gamma}^{2} + \\sigma_{\\mathrm{emp}}^{2} \\approx 0.041009265 + 0.0064 = 0.047409265$$\nThe denominator is the square root of this sum:\n$$\\sqrt{\\sigma_{\\gamma}^{2} + \\sigma_{\\mathrm{emp}}^{2}} \\approx \\sqrt{0.047409265} \\approx 0.2177366...$$\nFinally, we compute $z$:\n$$z = \\frac{-0.050769...}{0.2177366...} \\approx -0.233163...$$\nRounding to three significant figures, the value is $-0.233$.",
            "answer": "$$\n\\boxed{-0.233}\n$$"
        },
        {
            "introduction": "The observation of a power law in neural data is a necessary but not sufficient condition for claiming criticality. A crucial part of the scientific process is to rigorously exclude alternative explanations and artifacts that can mimic these signatures. This exercise  tackles one of the most significant confounds: nonstationarity, demonstrating how aggregating data from simple, non-critical processes can misleadingly produce heavy tails and challenging you to identify a robust analysis pipeline to account for it.",
            "id": "4308659",
            "problem": "A central empirical signature appealed to by the critical brain hypothesis is approximately scale-free neuronal avalanches, often summarized by heavy-tailed distributions of avalanche sizes or durations across multielectrode recordings of Local Field Potential (LFP) or spikes. However, neural recordings frequently exhibit slow drifts, state changes, and nonstationary rate modulations that can spuriously produce heavy tails by mixing statistics across heterogeneous epochs. Consider a recording in which avalanche durations, denoted by $D$, are computed via standard binning of threshold-crossing events. Suppose that conditional on a slowly varying instantaneous rate $\\lambda(t)$, the within-epoch avalanche duration distribution is memoryless and light-tailed (for example, exponentially distributed), while $\\lambda(t)$ itself varies slowly over time due to arousal fluctuations, yielding an effective rate random variable $\\Lambda$ with density $g(\\lambda)$ when aggregated across the recording. You are told that within short epochs where $\\lambda(t)$ is approximately constant, the process is subcritical and does not produce scale-free avalanches, consistent with light-tailed $D$ conditional on $\\lambda$.\n\nTask 1 (derivation): Starting from the definition of a mixture distribution and the survival function $S(x) = \\mathbb{P}(D > x)$, derive the unconditional survival function $S(x)$ of $D$ aggregated over the recording as a mixture over $\\Lambda$. Show that if $g(\\lambda)$ has substantial mass near $\\lambda = 0$ and decays sufficiently slowly, the aggregated survival can exhibit asymptotically heavy-tailed behavior even though the conditional distribution of $D$ given $\\lambda$ is light-tailed. As a concrete case, assume $\\Lambda$ is Gamma distributed with shape $k > 0$ and scale $\\theta > 0$, and $D \\mid \\Lambda = \\lambda$ is Exponential with rate $\\lambda$. Derive the exact expression for $S(x)$ and its asymptotic scaling for large $x$.\n\nTask 2 (analysis-design): Based on your derivation and standard facts from point process theory and time series analysis (for example, the time-rescaling theorem for point processes and change-point segmentation), select the pipeline that best mitigates nonstationarity-induced heavy-tail artifacts while preserving genuine scale-free structure if present. Which option correctly identifies (i) how nonstationarity can spuriously produce heavy tails from rate mixtures and (ii) a scientifically sound detrending and segmentation pipeline to minimize this confound before evaluating avalanche statistics?\n\nChoose one:\n\nA. Recognize that aggregating light-tailed within-epoch durations $D \\mid \\lambda$ across slowly varying $\\lambda$ is a mixture whose survival $S(x)$ can asymptotically behave like a power law when $g(\\lambda)$ places mass near $\\lambda = 0$. Implement a pipeline that: estimates the instantaneous conditional intensity $\\lambda(t)$ via a point-process model or kernel estimator; applies the time-rescaling theorem to transform event times by $u_i = \\int_{t_{i-1}}^{t_i} \\lambda(s)\\, ds$ so that, under correct $\\lambda(t)$, inter-event times are Exponential with unit rate; performs Bayesian online change-point detection to segment the rescaled series into approximately stationary epochs; detrends slow amplitude baselines in LFP via robust local regression; fits avalanche distributions within segments by maximum likelihood with goodness-of-fit tests; and validates using inhomogeneous Poisson surrogates that preserve $\\lambda(t)$ but destroy correlations, to assess whether apparent heavy tails persist after detrending and segmentation.\n\nB. Accept apparent scale-free behavior when a log-log plot of the pooled avalanche distribution yields linearity with high coefficient of determination (for example, $R^2 > 0.95$). Detrend by global $z$-scoring of the entire recording and a single high-pass filter. Choose a large bin size to average over rate fluctuations, compute a single pooled distribution, and apply a Kolmogorov-Smirnov goodness-of-fit test without segmentation.\n\nC. Prevent extreme values from driving apparent heavy tails by clipping avalanches above the $95\\%$ quantile and fitting a truncated power law to the entire recording. Detrend by subtracting the global mean rate and verify stationarity by the absence of a linear trend in the pooled count time series.\n\nD. Divide the recording into equal-length windows of fixed duration without testing for stationarity, normalize avalanche sizes in each window by the window’s mean size, recombine the normalized avalanches into one dataset, and fit a single power law. Validate by shuffling event times uniformly across the recording to create a stationary null model and comparing exponents.\n\nYour answer should be the single best choice from A–D that satisfies both parts of Task 2 and follows from your derivation in Task 1.",
            "solution": "The problem statement is valid. It addresses a well-established and critical confound in the analysis of heavy-tailed distributions in neuroscience, namely the effect of nonstationarity. The premise—that mixing light-tailed distributions can spuriously generate heavy-tailed ones—is mathematically sound and observationally relevant to neural recordings. The problem is well-posed, providing a clear theoretical task and a subsequent methodological design question based on the derived insight.\n\n### Task 1: Derivation\n\nThe analysis begins with the law of total probability applied to the survival function $S(x) = \\mathbb{P}(D > x)$. The unconditional survival function is the expectation of the conditional survival function over the distribution of the rate parameter $\\Lambda$.\nLet $S_{D|\\Lambda}(x|\\lambda) = \\mathbb{P}(D > x \\mid \\Lambda = \\lambda)$ be the conditional survival function of the avalanche duration $D$ given a constant rate $\\lambda$. Let $g(\\lambda)$ be the probability density function of the rate random variable $\\Lambda$. The unconditional survival function $S(x)$ is given by the mixture:\n$$S(x) = \\mathbb{E}_{\\Lambda}[\\mathbb{P}(D > x \\mid \\Lambda)] = \\int_{0}^{\\infty} \\mathbb{P}(D > x \\mid \\Lambda = \\lambda) g(\\lambda) d\\lambda = \\int_{0}^{\\infty} S_{D|\\Lambda}(x|\\lambda) g(\\lambda) d\\lambda$$\n\nThis formula shows that the observed aggregate distribution is a weighted average of the within-epoch distributions, where the weights are determined by the distribution of the rate parameter, $g(\\lambda)$. If epochs with very slow dynamics (small $\\lambda$) occur, their contribution to the mixture will be light-tailed distributions that decay very slowly (e.g., $e^{-\\lambda x}$ for small $\\lambda$). Even if these epochs are rare, their slow decay can dominate the tail of the integral for large $x$, causing the aggregate distribution $S(x)$ to decay much slower than any single component, potentially like a power law. This demonstrates how a mixture of light-tailed distributions can yield a heavy-tailed distribution.\n\nNow, we perform the derivation for the concrete case specified.\nThe conditional distribution is Exponential with rate $\\lambda$:\n$D \\mid \\Lambda = \\lambda \\sim \\text{Exponential}(\\lambda)$.\nThe conditional survival function is therefore $S_{D|\\Lambda}(x|\\lambda) = e^{-\\lambda x}$ for $x \\ge 0$.\n\nThe mixing distribution for the rate $\\Lambda$ is a Gamma distribution with shape $k > 0$ and scale $\\theta > 0$:\n$\\Lambda \\sim \\text{Gamma}(k, \\theta)$.\nThe probability density function is $g(\\lambda) = \\frac{1}{\\Gamma(k)\\theta^k} \\lambda^{k-1} e^{-\\lambda/\\theta}$ for $\\lambda > 0$.\n\nSubstituting these into the mixture equation:\n$$S(x) = \\int_{0}^{\\infty} (e^{-\\lambda x}) \\left( \\frac{1}{\\Gamma(k)\\theta^k} \\lambda^{k-1} e^{-\\lambda/\\theta} \\right) d\\lambda$$\nWe can combine the exponential terms and factor out the constants:\n$$S(x) = \\frac{1}{\\Gamma(k)\\theta^k} \\int_{0}^{\\infty} \\lambda^{k-1} e^{-\\lambda x - \\lambda/\\theta} d\\lambda = \\frac{1}{\\Gamma(k)\\theta^k} \\int_{0}^{\\infty} \\lambda^{k-1} e^{-\\lambda(x + 1/\\theta)} d\\lambda$$\nThe integral is in the form of the kernel of a Gamma distribution. We can solve it by recognizing this form. The integral $\\int_0^\\infty t^{a-1}e^{-bt}dt = \\frac{\\Gamma(a)}{b^a}$.\nIn our case, the exponent is $a = k$ and the rate is $b = x + 1/\\theta$.\nTherefore, the integral evaluates to:\n$$\\int_{0}^{\\infty} \\lambda^{k-1} e^{-\\lambda(x + 1/\\theta)} d\\lambda = \\frac{\\Gamma(k)}{(x + 1/\\theta)^k}$$\nSubstituting this result back into the expression for $S(x)$:\n$$S(x) = \\frac{1}{\\Gamma(k)\\theta^k} \\cdot \\frac{\\Gamma(k)}{(x + 1/\\theta)^k} = \\frac{1}{\\theta^k (x + 1/\\theta)^k} = \\left( \\frac{1}{\\theta(x + 1/\\theta)} \\right)^k$$\nSimplifying the expression in the parenthesis gives the exact survival function:\n$$S(x) = \\left( \\frac{1}{1 + \\theta x} \\right)^k$$\nThis is the survival function of a Lomax distribution (also known as a Pareto Type II distribution), which is a canonical heavy-tailed distribution.\n\nFor its asymptotic scaling, we consider the limit as $x \\to \\infty$. In this limit, the term '$1$' in the denominator becomes negligible compared to $\\theta x$:\n$$S(x) \\approx \\left( \\frac{1}{\\theta x} \\right)^k = (\\theta^{-k}) x^{-k}$$\nThus, for large $x$, the survival function exhibits power-law scaling, $S(x) \\propto x^{-k}$. This confirms that a mixture of light-tailed exponential distributions, with rates governed by a Gamma distribution, produces a heavy-tailed distribution.\n\n### Task 2: Analysis of Methodological Pipelines\n\nThe derivation in Task 1 formally establishes that nonstationarity in the form of a fluctuating rate parameter can create a spurious heavy-tailed distribution from underlying light-tailed processes. A scientifically rigorous pipeline must therefore be designed to diagnose and mitigate this confound before claiming evidence for intrinsic scale-free dynamics.\n\n**A. Evaluation of Option A:**\nThis option correctly identifies the spurious mechanism as a mixture effect, directly reflecting the insight from Task 1. The proposed pipeline is a state-of-the-art, multi-step procedure for robustly handling nonstationarity.\n(1) **Estimate $\\lambda(t)$:** This is the correct first step, aimed at explicitly modeling the source of nonstationarity. Point-process models or kernel smoothing are standard and appropriate methods.\n(2) **Time-Rescaling Theorem:** This is a principled method to functionally \"remove\" the effect of the modeled rate $\\lambda(t)$ and to test the goodness-of-fit of the rate model. It transforms a nonstationary event series into a stationary one if the rate is captured correctly.\n(3) **Change-Point Detection:** This adds another layer of rigor by segmenting the data into statistically homogeneous epochs, guarding against other forms of nonstationarity that might remain.\n(4) **LFP Detrending:** This addresses raw signal artifacts, which is good practice.\n(5) **Within-Segment Analysis:** This is the correct procedure—statistical properties should be estimated only within epochs demonstrated to be stationary.\n(6) **Inhomogeneous Poisson Surrogates:** This is a crucial and sophisticated validation step. These surrogates preserve the estimated nonstationary rate $\\lambda(t)$ but destroy any finer-timescale temporal correlations. If the heavy tails are still present in the data after correcting for $\\lambda(t)$ but absent in the surrogates, it provides strong evidence that the tails are due to intrinsic correlations (the phenomenon of interest) rather than just rate modulation.\nThis pipeline is comprehensive, methodologically sound, and directly motivated by the nature of the confound.\n**Verdict: Correct.**\n\n**B. Evaluation of Option B:**\nThis option represents a naive approach.\n(1) Relying on a high $R^2$ on a log-log plot is a notoriously unreliable method for identifying power laws and completely ignores the confound of nonstationarity.\n(2) Global $z$-scoring and high-pass filtering are weak and often inappropriate tools for detrending complex, time-varying rate modulations.\n(3) Using a large bin size averages out data and can obscure or distort true dynamics.\n(4) Pooling all data into a single distribution is precisely the operation that Task 1 showed to be the source of the artifact.\nThis pipeline fails to address the core problem and is likely to lead to a spurious conclusion.\n**Verdict: Incorrect.**\n\n**C. Evaluation of Option C:**\nThis option is methodologically flawed and unscientific.\n(1) Clipping data at an arbitrary quantile (the $95\\%$ quantile) constitutes data tampering. It prevents the very phenomenon of interest—the tail of the distribution—from being studied. One cannot test a hypothesis about heavy tails by first removing them.\n(2) Detrending by subtracting the global mean is insufficient for time-varying rates.\n(3) Checking for stationarity by only looking for a linear trend is an inadequate test; stationarity requires that all statistical moments be time-invariant, not just the mean.\n**Verdict: Incorrect.**\n\n**D. Evaluation of Option D:**\nThis option attempts to account for nonstationarity but uses ad-hoc and less powerful methods than option A.\n(1) Dividing the recording into arbitrary fixed-length windows is unlikely to align with the true underlying statistical epochs of the data.\n(2) Normalizing avalanche sizes by the window's mean can distort the shape of the distribution in non-trivial ways and lacks a firm statistical foundation for preserving power-law properties.\n(3) Shuffling event times uniformly across the recording creates a stationary Poisson process. This null model is too simple because it destroys both the nonstationary rate and any intrinsic correlations. Therefore, it cannot be used to disentangle which of these two factors was responsible for the heavy tails observed in the original data. Option A's use of inhomogeneous surrogates is far superior as it isolates the role of correlations.\n**Verdict: Incorrect.**\n\nIn summary, Option A describes the only pipeline that is both aware of the artifact demonstrated in Task 1 and employs a rigorous, principled, and state-of-the-art statistical methodology to mitigate it.",
            "answer": "$$\n\\boxed{A}\n$$"
        }
    ]
}