{
    "hands_on_practices": [
        {
            "introduction": "A primary challenge in non-invasive electrophysiology (EEG/MEG) is that we measure signals outside the head and must infer the activity of sources within the brain. This 'inverse problem' is inherently imperfect, leading to spatial leakage where the estimated activity of one source is contaminated by signals from its neighbors. This practice problem invites you to mathematically derive how this leakage artifact can create spurious functional connections, demonstrating how two truly uncorrelated brain regions can appear to be functionally connected simply due to the limitations of our measurement tools. ",
            "id": "4277754",
            "problem": "Consider two cortical sources measured with Electroencephalography (EEG) or Magnetoencephalography (MEG). Let the true source time series be $x_{1}(t)$ and $x_{2}(t)$, modeled as zero-mean, stationary Gaussian processes with variances $\\sigma_{1}^{2}$ and $\\sigma_{2}^{2}$, and with zero cross-covariance at zero lag, that is, $\\mathrm{Cov}(x_{1}(t), x_{2}(t)) = 0$. The sensors measure $y(t) = G x(t) + \\epsilon(t)$, where $x(t) = \\begin{pmatrix} x_{1}(t) \\\\ x_{2}(t) \\end{pmatrix}$, $G$ is a leadfield matrix, and $\\epsilon(t)$ is zero-mean additive sensor noise, modeled as a stationary Gaussian process independent of $x(t)$.\n\nSuppose a linear inverse (source reconstruction) operator $W$ is applied, yielding estimated sources $\\hat{x}(t) = W y(t) = C x(t) + \\eta(t)$, where $C = W G$ is the resolution matrix and $\\eta(t) = W \\epsilon(t)$ is the noise mapped into source space. Assume a symmetric leakage structure\n$$\nC = \\begin{pmatrix}\n1 & \\ell \\\\\n\\ell & 1\n\\end{pmatrix},\n$$\nwhere $\\ell$ (with $|\\ell|  1$) quantifies spatial leakage between the two reconstructed sources. Assume the noise components at the estimated sources are independent and identically distributed, $\\eta_{1}(t)$ and $\\eta_{2}(t)$ are zero-mean Gaussian processes with variance $\\sigma_{\\eta}^{2}$, independent of $x_{1}(t)$ and $x_{2}(t)$.\n\nDefine the functional connectivity between the reconstructed sources as the zero-lag Pearson correlation coefficient $r$ between $\\hat{x}_{1}(t)$ and $\\hat{x}_{2}(t)$. Starting from the core definitions of covariance and correlation for stationary Gaussian processes and the linear generative model above, derive an analytic expression for the expected zero-lag Pearson correlation $r_{\\mathrm{leak}}$ as a function of $\\ell$, $\\sigma_{1}^{2}$, $\\sigma_{2}^{2}$, and $\\sigma_{\\eta}^{2}$. Briefly justify, in your derivation, how this nonzero $r_{\\mathrm{leak}}$ constitutes a spurious functional edge induced purely by spatial leakage in the absence of true interaction. You may assume that orthogonalization or ideal beamforming corresponds to setting $\\ell = 0$, but do not use this assumption in the requested calculation.\n\nExpress your final answer as a single closed-form analytic expression for $r_{\\mathrm{leak}}$. No rounding is required, and no physical units apply.",
            "solution": "The problem is first validated against the required criteria.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- True source time series: $x_{1}(t)$ and $x_{2}(t)$.\n- Source model: Zero-mean, stationary Gaussian processes.\n- Source variances: $\\mathrm{Var}(x_{1}(t)) = \\sigma_{1}^{2}$, $\\mathrm{Var}(x_{2}(t)) = \\sigma_{2}^{2}$.\n- Source cross-covariance: $\\mathrm{Cov}(x_{1}(t), x_{2}(t)) = 0$.\n- Measurement model: $y(t) = G x(t) + \\epsilon(t)$, where $x(t) = \\begin{pmatrix} x_{1}(t) \\\\ x_{2}(t) \\end{pmatrix}$.\n- Sensor noise: $\\epsilon(t)$ is a zero-mean, stationary Gaussian process, independent of $x(t)$.\n- Estimated sources: $\\hat{x}(t) = W y(t) = C x(t) + \\eta(t)$.\n- Resolution matrix: $C = W G = \\begin{pmatrix} 1  \\ell \\\\ \\ell  1 \\end{pmatrix}$, with $|\\ell|  1$.\n- Mapped noise: $\\eta(t) = W \\epsilon(t)$, with components $\\eta_{1}(t)$ and $\\eta_{2}(t)$ being independent and identically distributed (i.i.d.) zero-mean Gaussian processes with variance $\\sigma_{\\eta}^{2}$.\n- Noise independence: $\\eta(t)$ is independent of $x(t)$.\n- Definition of functional connectivity: The zero-lag Pearson correlation coefficient $r$ between $\\hat{x}_{1}(t)$ and $\\hat{x}_{2}(t)$.\n- Objective: Derive the expression for this correlation, denoted $r_{\\mathrm{leak}}$, as a function of $\\ell$, $\\sigma_{1}^{2}$, $\\sigma_{2}^{2}$, and $\\sigma_{\\eta}^{2}$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded**: The problem is well-grounded in the principles of signal processing and statistical analysis as applied to neuroimaging data (EEG/MEG). The linear model for source mixing (leakage) is a standard and widely used framework for studying artifacts in functional connectivity analysis.\n- **Well-Posed**: The problem is well-posed. It provides a complete set of mathematical definitions and assumptions required to derive a unique analytical expression for the requested quantity.\n- **Objective**: The problem is stated in precise, objective, and mathematical language.\n- **Flaw Checklist**: The problem does not violate any of the invalidity criteria. It is scientifically sound, formalizable, complete, realistic within its modeling framework, and non-trivial.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A complete solution will be provided.\n\n### Derivation of the Spurious Correlation\n\nThe quantity to be derived is the zero-lag Pearson correlation coefficient between the reconstructed source time series $\\hat{x}_{1}(t)$ and $\\hat{x}_{2}(t)$, which we denote as $r_{\\mathrm{leak}}$. For any two zero-mean stationary random processes $A(t)$ and $B(t)$, the correlation coefficient is defined as:\n$$\n\\mathrm{Corr}(A(t), B(t)) = \\frac{\\mathrm{Cov}(A(t), B(t))}{\\sqrt{\\mathrm{Var}(A(t)) \\mathrm{Var}(B(t))}}\n$$\nIn our case, $A(t) = \\hat{x}_{1}(t)$ and $B(t) = \\hat{x}_{2}(t)$. We need to calculate the covariance term in the numerator and the two variance terms in the denominator.\n\nFirst, we express the estimated sources $\\hat{x}_{1}(t)$ and $\\hat{x}_{2}(t)$ in terms of the true sources and noise. From the model $\\hat{x}(t) = C x(t) + \\eta(t)$, we have:\n$$\n\\begin{pmatrix} \\hat{x}_{1}(t) \\\\ \\hat{x}_{2}(t) \\end{pmatrix} = \\begin{pmatrix} 1  \\ell \\\\ \\ell  1 \\end{pmatrix} \\begin{pmatrix} x_{1}(t) \\\\ x_{2}(t) \\end{pmatrix} + \\begin{pmatrix} \\eta_{1}(t) \\\\ \\eta_{2}(t) \\end{pmatrix}\n$$\nThis gives the individual component equations:\n$$\n\\hat{x}_{1}(t) = x_{1}(t) + \\ell x_{2}(t) + \\eta_{1}(t)\n$$\n$$\n\\hat{x}_{2}(t) = \\ell x_{1}(t) + x_{2}(t) + \\eta_{2}(t)\n$$\nSince all underlying processes ($x_1$, $x_2$, $\\eta_1$, $\\eta_2$) are given as zero-mean, the estimated sources $\\hat{x}_{1}(t)$ and $\\hat{x}_{2}(t)$ are also zero-mean processes.\n\nNext, we calculate the variances, $\\mathrm{Var}(\\hat{x}_{1}(t))$ and $\\mathrm{Var}(\\hat{x}_{2}(t))$. The variance of a sum of uncorrelated random variables is the sum of their variances. Based on the problem statement:\n- $x_{1}(t)$ and $x_{2}(t)$ are uncorrelated: $\\mathrm{Cov}(x_{1}, x_{2}) = 0$.\n- $\\eta(t)$ is independent of $x(t)$, which implies $\\eta_{1}(t)$ and $\\eta_{2}(t)$ are uncorrelated with both $x_{1}(t)$ and $x_{2}(t)$.\n- $\\eta_{1}(t)$ and $\\eta_{2}(t)$ are independent of each other.\n\nFor $\\mathrm{Var}(\\hat{x}_{1}(t))$:\n$$\n\\mathrm{Var}(\\hat{x}_{1}(t)) = \\mathrm{Var}(x_{1}(t) + \\ell x_{2}(t) + \\eta_{1}(t))\n$$\n$$\n\\mathrm{Var}(\\hat{x}_{1}(t)) = \\mathrm{Var}(x_{1}(t)) + \\mathrm{Var}(\\ell x_{2}(t)) + \\mathrm{Var}(\\eta_{1}(t))\n$$\nUsing the property $\\mathrm{Var}(aX) = a^{2}\\mathrm{Var}(X)$ and the given variances:\n$$\n\\mathrm{Var}(\\hat{x}_{1}(t)) = \\sigma_{1}^{2} + \\ell^{2} \\sigma_{2}^{2} + \\sigma_{\\eta}^{2}\n$$\nSimilarly, for $\\mathrm{Var}(\\hat{x}_{2}(t))$:\n$$\n\\mathrm{Var}(\\hat{x}_{2}(t)) = \\mathrm{Var}(\\ell x_{1}(t) + x_{2}(t) + \\eta_{2}(t))\n$$\n$$\n\\mathrm{Var}(\\hat{x}_{2}(t)) = \\mathrm{Var}(\\ell x_{1}(t)) + \\mathrm{Var}(x_{2}(t)) + \\mathrm{Var}(\\eta_{2}(t))\n$$\n$$\n\\mathrm{Var}(\\hat{x}_{2}(t)) = \\ell^{2} \\sigma_{1}^{2} + \\sigma_{2}^{2} + \\sigma_{\\eta}^{2}\n$$\n\nNow, we calculate the covariance, $\\mathrm{Cov}(\\hat{x}_{1}(t), \\hat{x}_{2}(t))$. Using the bilinearity property of covariance:\n$$\n\\mathrm{Cov}(\\hat{x}_{1}(t), \\hat{x}_{2}(t)) = \\mathrm{Cov}(x_{1}(t) + \\ell x_{2}(t) + \\eta_{1}(t), \\ell x_{1}(t) + x_{2}(t) + \\eta_{2}(t))\n$$\nWe expand this into a sum of covariance terms:\n$$\n\\mathrm{Cov}(\\hat{x}_{1}, \\hat{x}_{2}) = \\mathrm{Cov}(x_{1}, \\ell x_{1}) + \\mathrm{Cov}(x_{1}, x_{2}) + \\mathrm{Cov}(x_{1}, \\eta_{2}) \\\\\n+ \\mathrm{Cov}(\\ell x_{2}, \\ell x_{1}) + \\mathrm{Cov}(\\ell x_{2}, x_{2}) + \\mathrm{Cov}(\\ell x_{2}, \\eta_{2}) \\\\\n+ \\mathrm{Cov}(\\eta_{1}, \\ell x_{1}) + \\mathrm{Cov}(\\eta_{1}, x_{2}) + \\mathrm{Cov}(\\eta_{1}, \\eta_{2})\n$$\nWe evaluate each term based on the given independence and covariance properties:\n- $\\mathrm{Cov}(x_{1}, \\ell x_{1}) = \\ell \\mathrm{Cov}(x_{1}, x_{1}) = \\ell \\mathrm{Var}(x_{1}) = \\ell \\sigma_{1}^{2}$\n- $\\mathrm{Cov}(x_{1}, x_{2}) = 0$ (given)\n- $\\mathrm{Cov}(x_{1}, \\eta_{2}) = 0$ ($x$ and $\\eta$ are independent)\n- $\\mathrm{Cov}(\\ell x_{2}, \\ell x_{1}) = \\ell^{2} \\mathrm{Cov}(x_{2}, x_{1}) = 0$\n- $\\mathrm{Cov}(\\ell x_{2}, x_{2}) = \\ell \\mathrm{Cov}(x_{2}, x_{2}) = \\ell \\mathrm{Var}(x_{2}) = \\ell \\sigma_{2}^{2}$\n- $\\mathrm{Cov}(\\ell x_{2}, \\eta_{2}) = 0$ ($x$ and $\\eta$ are independent)\n- $\\mathrm{Cov}(\\eta_{1}, \\ell x_{1}) = 0$ ($x$ and $\\eta$ are independent)\n- $\\mathrm{Cov}(\\eta_{1}, x_{2}) = 0$ ($x$ and $\\eta$ are independent)\n- $\\mathrm{Cov}(\\eta_{1}, \\eta_{2}) = 0$ ($\\eta_{1}$ and $\\eta_{2}$ are independent)\n\nSumming the non-zero terms, the covariance is:\n$$\n\\mathrm{Cov}(\\hat{x}_{1}(t), \\hat{x}_{2}(t)) = \\ell \\sigma_{1}^{2} + \\ell \\sigma_{2}^{2} = \\ell(\\sigma_{1}^{2} + \\sigma_{2}^{2})\n$$\n\nFinally, we assemble the expression for the correlation coefficient $r_{\\mathrm{leak}}$:\n$$\nr_{\\mathrm{leak}} = \\frac{\\mathrm{Cov}(\\hat{x}_{1}(t), \\hat{x}_{2}(t))}{\\sqrt{\\mathrm{Var}(\\hat{x}_{1}(t)) \\mathrm{Var}(\\hat{x}_{2}(t))}}\n$$\n$$\nr_{\\mathrm{leak}} = \\frac{\\ell(\\sigma_{1}^{2} + \\sigma_{2}^{2})}{\\sqrt{(\\sigma_{1}^{2} + \\ell^{2} \\sigma_{2}^{2} + \\sigma_{\\eta}^{2})(\\ell^{2} \\sigma_{1}^{2} + \\sigma_{2}^{2} + \\sigma_{\\eta}^{2})}}\n$$\n\nThis nonzero correlation $r_{\\mathrm{leak}}$ constitutes a spurious functional edge because it is an artifact of the analysis method, not a reflection of true underlying physiology. The problem begins with the explicit premise that the true cortical sources are uncorrelated, $\\mathrm{Cov}(x_1(t), x_2(t)) = 0$, meaning there is no genuine zero-lag interaction between them. However, the calculation shows that the correlation between the *reconstructed* sources is non-zero whenever the leakage parameter $\\ell$ is non-zero. This leakage, an inherent property of the inverse operator $W$, causes the signal from source $x_1(t)$ to \"leak\" into the reconstruction of source $x_2(t)$, and vice versa. As a result, the reconstructed signal $\\hat{x}_{1}(t)$ contains a component of $x_2(t)$, and $\\hat{x}_{2}(t)$ contains a component of $x_1(t)$. This artificial mixing induces a statistical correlation between them, which is directly proportional to the leakage parameter $\\ell$ and the power of the true sources. Therefore, any functional connectivity graph that includes this edge would be reporting a false positive—a spurious connection that exists only because of the imperfect spatial resolution of the source reconstruction technique.",
            "answer": "$$\n\\boxed{\\frac{\\ell(\\sigma_{1}^{2} + \\sigma_{2}^{2})}{\\sqrt{(\\sigma_{1}^{2} + \\ell^{2} \\sigma_{2}^{2} + \\sigma_{\\eta}^{2})(\\ell^{2} \\sigma_{1}^{2} + \\sigma_{2}^{2} + \\sigma_{\\eta}^{2})}}}\n$$"
        },
        {
            "introduction": "After computing a matrix of pairwise correlations, a common next step is to apply a threshold to create a binarized graph of the strongest connections. However, the choice of thresholding strategy can dramatically alter the network's topology and the validity of scientific conclusions. This exercise challenges you to critically evaluate common heuristic approaches against statistically-principled methods that control for false positives, a crucial skill for constructing brain networks that are both meaningful and reproducible. ",
            "id": "4277710",
            "problem": "A research group is constructing subject-level functional brain networks from resting-state functional magnetic resonance imaging (fMRI) time series. For each subject, they parcellate the brain into $N$ regions and compute the Pearson correlation $r_{ij}$ between every pair of regional time series $(i,j)$, resulting in a weighted, symmetric matrix. They must produce binarized graphs for network comparison, but want to control the rate of spurious edges (false positives) introduced by noise. Subjects differ in time series length and autocorrelation: subject $s$ has $T_s$ time points, and each regional time series exhibits approximately first-order autoregressive structure with coefficient $\\phi_{i,s}$, which may vary by region and subject.\n\nThe group is debating thresholding strategies:\n\n- Proportional thresholding: for each subject, retain the top fraction $d$ of edge weights (i.e., enforce a fixed network density $d$).\n- Absolute thresholding: for each subject, retain edges with $|r_{ij}| > \\tau$ for a fixed $\\tau$.\n- Sparsity constraints: for each subject, retain the top $k$ edges (or enforce degree constraints) sufficient to achieve a connected giant component.\n\nThey also consider statistical testing using known properties of sample correlations. For two stationary Gaussian processes with zero true correlation, the Fisher $z$-transform $z = \\tfrac{1}{2}\\ln\\frac{1+r}{1-r}$ of the sample correlation is approximately distributed as a normal random variable with mean $0$ and variance $1/(N_{\\mathrm{eff}}-3)$, where $N_{\\mathrm{eff}}$ is the effective sample size that accounts for autocorrelation. For a pair of autoregressive of order one processes with autoregressive parameters $\\phi_{x}$ and $\\phi_{y}$ recorded over $T$ time points, a well-tested approximation gives $N_{\\mathrm{eff}} \\approx T \\frac{1-\\phi_{x}\\phi_{y}}{1+\\phi_{x}\\phi_{y}}$. Consequently, for a given subject $s$ and edge $(i,j)$, the two-sided $p$-value under the null hypothesis $H_0:\\rho_{ij}=0$ can be computed from $z_{ij} = \\tanh^{-1}(r_{ij})$ using the variance $1/(N_{\\mathrm{eff},ij,s}-3)$, where $N_{\\mathrm{eff},ij,s}$ depends on $(\\phi_{i,s},\\phi_{j,s},T_s)$. Alternatively, one may estimate null distributions per edge by surrogate data that preserve autocorrelation (e.g., phase-randomized surrogates), yielding empirical $p$-values.\n\nLet there be $m = N(N-1)/2$ undirected edges per subject, and suppose the true network is sparse with a subject-specific nonnull fraction $\\pi_{1,s} \\in (0,1)$ of edges having nonzero correlation due to neural coupling, and a null fraction $1-\\pi_{1,s}$ of edges having zero true correlation. The group wants to compare topological properties across subjects and cohorts, but prioritizes valid control of false positives at the subject level. They seek strategies that, for each subject, maintain a pre-specified error rate when selecting edges, recognizing that $T_s$ and $\\{\\phi_{i,s}\\}$ vary across subjects.\n\nFrom first principles, use the distributional properties of the Fisher $z$-transform under $H_0$ and the definitions of multiple-testing error rates to evaluate the following candidate procedures. Assume that the dependence among edge-wise test statistics is nonnegative and primarily arises from shared nodes.\n\nA. Proportional thresholding at a fixed density $d$ across all subjects; justify false positive control by the fact that each subject contributes exactly $dm$ suprathreshold edges, so the number of spurious edges cannot inflate.\n\nB. Absolute thresholding with a single fixed correlation cutoff $\\tau$ applied identically to all subjects; choose $\\tau$ so that, for a representative effective sample size $N_{\\mathrm{eff}}^{\\ast}$ typical of the cohort, the nominal per-edge false positive rate under $H_0$ is at most $\\alpha$.\n\nC. For each subject, compute an edge-wise $p$-value by comparing the observed $z_{ij}$ to a subject- and edge-specific null distribution estimated from autocorrelation-preserving surrogate time series, then apply the Benjamini–Hochberg False Discovery Rate (FDR) procedure at level $q$ across the $m$ edges for that subject, accepting a variable number of suprathreshold edges.\n\nD. Enforce a sparsity constraint by selecting the top $k$ edges per subject, with $k$ chosen just large enough so that the largest connected component includes at least a fraction $\\gamma$ of nodes, arguing that most false positives live among weak edges and thus are excluded.\n\nE. For each subject, compute two-sided $p$-values for all $m$ edges using Fisher $z$-scores with variance $1/(N_{\\mathrm{eff},ij,s}-3)$, and then apply a Bonferroni correction to control the Family-Wise Error Rate (FWER) at level $\\alpha$ across the $m$ edges; equivalently, use a subject-specific absolute cutoff $\\tau_s$ implied by the Bonferroni-adjusted critical $z$.\n\nWhich options correctly achieve valid control of false positives at the subject level across subjects with differing $T_s$ and autocorrelation, under the stated assumptions? Select all that apply and justify your choice based on the definitions of FDR and FWER, the null distribution of Fisher $z$, and the implications of fixed-density and sparsity constraints for the false discovery proportion.",
            "solution": "This problem is scientifically valid, well-posed, and central to the practice of network neuroscience. It correctly sets up a realistic scenario and asks for an evaluation of common procedures against statistical first principles. A solution will be provided by analyzing each option.\n\nThe core principle for valid false positive control in this context is that the statistical significance of a sample correlation $r_{ij}$ depends on the effective number of samples, $N_{\\mathrm{eff},ij,s}$, which itself is a function of the time series length $T_s$ and the autocorrelation properties $(\\phi_{i,s}, \\phi_{j,s})$. Any method that ignores this subject- and edge-specific variability cannot provide uniform and valid control of false positives across subjects.\n\n**Analysis of Each Option:**\n\n**A. Proportional thresholding (fixed density):** This method is **incorrect**. It ensures that all subjects' graphs have the same number of edges, which can be useful for comparing certain topological metrics. However, it provides no statistical control over false positives. A subject with noisier data (lower $N_{\\mathrm{eff}}$) will have a much higher proportion of spurious edges among their top $d$ fraction of connections compared to a subject with cleaner data. The number of false positives is not controlled, only the total number of declared edges is fixed.\n\n**B. Absolute thresholding (fixed correlation cutoff):** This method is **incorrect**. A fixed correlation value $\\tau$ has a different statistical meaning for each subject and edge. For a subject with a long, low-autocorrelation time series (high $N_{\\mathrm{eff}}$), a correlation of $\\tau=0.2$ might be highly significant. For a subject with a short, highly autocorrelated time series (low $N_{\\mathrm{eff}}$), the same correlation of $\\tau=0.2$ could easily arise by chance. Applying a single $\\tau$ across the cohort would result in a much higher false positive rate for subjects with poorer data quality, failing the goal of valid subject-level error control.\n\n**C. FDR control with surrogate-based p-values:** This method is **correct**. It is a statistically principled, non-parametric, and adaptive approach.\n1.  **Valid p-values:** For each subject, generating surrogate time series that preserve the specific autocorrelation structure provides an empirical null distribution for correlation values. Comparing the observed correlation to this subject-specific null distribution yields a valid $p$-value for each edge, properly accounting for $T_s$ and $\\{\\phi_{i,s}\\}$.\n2.  **Valid Error Control:** The Benjamini-Hochberg (BH) procedure, when applied to valid $p$-values, is proven to control the False Discovery Rate (FDR) — the expected proportion of false positives among the selected edges. This method correctly controls a meaningful error rate at the level of each individual subject.\n\n**D. Sparsity constraints (top k edges for giant component):** This method is **incorrect**. Like proportional thresholding, it is a heuristic based on rank-ordering and graph topology, not statistical inference. While it may be true that many false positives are weak, this provides no guarantee of statistical error control. The number of edges $k$ needed to form a giant component is a property of the graph's structure, not the statistical evidence for its edges. This method does not control FWER or FDR.\n\n**E. FWER control with Fisher z-transform:** This method is **correct**. It is a statistically principled, parametric approach.\n1.  **Valid p-values:** For each subject and edge, using the Fisher $z$-transform along with the specific effective sample size $N_{\\mathrm{eff},ij,s}$ generates a valid parametric $p$-value under the null hypothesis of zero correlation. This correctly accounts for the varying data quality across subjects and edges.\n2.  **Valid Error Control:** The Bonferroni correction is a classic method for controlling the Family-Wise Error Rate (FWER) — the probability of making one or more false positive discoveries across all $m$ tests for a given subject. By setting a significance threshold of $\\alpha/m$ for each edge test, it guarantees that the FWER for that subject is controlled at level $\\alpha$. This procedure is statistically valid, although it is often criticized for being overly conservative, especially for large $m$.\n\nIn conclusion, only the methods that generate subject-specific (and ideally, edge-specific) p-values and then apply a standard multiple testing correction procedure (like BH-FDR or Bonferroni-FWER) are statistically valid for controlling false positives at the subject level. Options C and E both satisfy these criteria.",
            "answer": "$$\\boxed{CE}$$"
        },
        {
            "introduction": "The presence of an edge in a functional network signifies a statistical dependency, but it does not, by itself, reveal the underlying circuit mechanism. A strong correlation between two brain regions could arise from a direct anatomical connection, or it could be induced by a third region that provides a common input to both. This hands-on problem asks you to dissect such a scenario, deriving the spurious correlation induced by a common driver and designing a statistical test to distinguish this confounded link from a true, direct interaction. ",
            "id": "4277727",
            "problem": "Consider the distinction between structural brain networks (physical axonal connections) and functional brain networks (statistical dependencies among neural signals). Suppose an experiment records three zero-mean discrete-time stochastic processes: a putative common driver $S_t$, and two regional signals $X_t$ and $Y_t$. The common driver $S_t$ is first-order autoregressive, that is, $S_t$ satisfies $S_t = \\phi S_{t-1} + \\eta_t$ with $|\\phi|  1$, where $\\eta_t$ are independent and identically distributed Gaussian innovations with variance $\\sigma_\\eta^2$. Let $X_t$ and $Y_t$ be generated by\n$$\nX_t = a\\, S_{t-\\ell_x} + u_t,\n\\qquad\nY_t = b\\, S_{t-\\ell_y} + c\\, X_{t-\\tau} + v_t,\n$$\nwhere $a$, $b$, $c$ are real constants, $\\ell_x$, $\\ell_y$, $\\tau$ are nonnegative integers, and $u_t$, $v_t$ are independent Gaussian white noise processes with variances $\\sigma_u^2$ and $\\sigma_v^2$, respectively. All noise terms are independent of $S_t$. You do not know whether the direct coupling parameter $c$ is zero. You observe a positive sample cross-correlation between $X_{t-\\tau}$ and $Y_t$ at lag $\\tau$.\n\nPart 1. Using only the linearity of covariance, the definition of the autoregressive model, and stationarity of $S_t$, explain mechanistically how time-lagged correlations between $X_{t-\\tau}$ and $Y_t$ can arise even when $c = 0$ due to the shared input $S_t$. Derive the theoretical cross-covariance $\\operatorname{Cov}(X_{t-\\tau}, Y_t)$ in terms of $a$, $b$, $\\phi$, $\\sigma_\\eta^2$, $\\ell_x$, $\\ell_y$, and $\\tau$ under the null hypothesis $c = 0$.\n\nPart 2. You are tasked with designing a statistical test that differentiates true interactions ($c \\neq 0$) from common-driver confounds using partial correlation or residualization. Assume $N$ samples are available and Gaussian assumptions hold. Which procedure below correctly targets the null hypothesis $H_0: c = 0$ by removing the confounding pathways from $S_t$ while preserving a test for the direct interaction from $X_{t-\\tau}$ to $Y_t$?\n\nA. Declare a direct interaction if the sample cross-correlation between $X_{t-\\tau}$ and $Y_t$ is significantly greater than zero, using a threshold derived from the asymptotic variance of a correlation coefficient under independence.\n\nB. Form the control vector $Z_t$ consisting of lags of $S_t$ that enter $X_{t-\\tau}$ and $Y_t$, for example $Z_t = \\big(S_{t-\\ell_y}, S_{t-\\tau-\\ell_x}, S_{t-\\ell_y-1}, \\dots, S_{t-\\tau-\\ell_x-1}, \\dots\\big)$ up to a finite order that captures the autocorrelation of $S_t$, then test $H_0: c = 0$ by either (i) computing the partial correlation $\\rho_{X_{t-\\tau}, Y_t \\cdot Z_t}$ and using its $t$-statistic with $N - q - 2$ degrees of freedom, where $q$ is the dimension of $Z_t$, or (ii) fitting the ordinary least squares (OLS) regression $Y_t = \\beta_0 + \\beta_X X_{t-\\tau} + \\beta^\\top Z_t + \\varepsilon_t$ and testing $H_0: \\beta_X = 0$; both are equivalent under linear Gaussian assumptions.\n\nC. Compute the partial correlation between $X_{t-\\tau}$ and $Y_t$ controlling only for the instantaneous driver $S_t$, that is, $\\rho_{X_{t-\\tau}, Y_t \\cdot S_t}$, and use a $t$-test with $N - 3$ degrees of freedom.\n\nD. Residualize $Y_t$ by first regressing out $X_{t-\\tau}$ to obtain $Y_t^{\\perp X} = Y_t - \\hat{\\gamma} X_{t-\\tau}$, then test whether $Y_t^{\\perp X}$ is uncorrelated with any lags of $S_t$ using a joint significance test; if the residual is unrelated to $S_t$, declare a direct interaction from $X_{t-\\tau}$ to $Y_t$.",
            "solution": "The problem statement is scientifically sound, well-posed, and unambiguous. It presents a classic scenario in time series analysis for distinguishing direct from indirect (confounded) interactions, a fundamental topic in network neuroscience. The models are standard, and all necessary parameters and assumptions are clearly defined. We can therefore proceed with the derivation and analysis.\n\n**Part 1. Spurious Correlation and Cross-Covariance Derivation**\n\nThe problem asks for a mechanistic explanation of how correlations between $X_{t-\\tau}$ and $Y_t$ can arise even when the direct coupling constant $c$ is zero, and to derive the theoretical cross-covariance.\n\n**Mechanistic Explanation:**\nUnder the null hypothesis $H_0: c=0$, the signals $X_t$ and $Y_t$ are generated as:\n$$\nX_t = a\\, S_{t-\\ell_x} + u_t\n$$\n$$\nY_t = b\\, S_{t-\\ell_y} + v_t\n$$\nBoth $X_t$ and $Y_t$ are influenced by the same underlying process, $S_t$, which is referred to as a \"common driver\". The process $S_t$ is autoregressive, meaning its value at any time $t$ is correlated with its past values. Specifically, $S_t = \\phi S_{t-1} + \\eta_t$. This property is known as autocorrelation.\n\nWhen we examine the relationship between $X_{t-\\tau}$ and $Y_t$, we are comparing the signal $X$ at time $t-\\tau$ with the signal $Y$ at time $t$. Their expressions are:\n$$\nX_{t-\\tau} = a\\, S_{t-\\tau-\\ell_x} + u_{t-\\tau}\n$$\n$$\nY_t = b\\, S_{t-\\ell_y} + v_t\n$$\nThe correlation between $X_{t-\\tau}$ and $Y_t$ arises because they share a common cause, the process $S_t$. The signal $X_{t-\\tau}$ is influenced by $S_t$ at time $t-\\tau-\\ell_x$, while $Y_t$ is influenced by $S_t$ at time $t-\\ell_y$. Due to the autocorrelation of $S_t$, the values $S_{t-\\tau-\\ell_x}$ and $S_{t-\\ell_y}$ are themselves correlated. This correlation, propagated through the constants $a$ and $b$, induces a statistical dependency (correlation) between $X_{t-\\tau}$ and $Y_t$, even in the complete absence of a direct causal link from $X$ to $Y$ (i.e., when $c=0$). This phenomenon is a classic example of confounding.\n\n**Derivation of Cross-Covariance:**\nThe cross-covariance between $X_{t-\\tau}$ and $Y_t$ is defined as $\\operatorname{Cov}(X_{t-\\tau}, Y_t) = \\operatorname{E}[X_{t-\\tau} Y_t] - \\operatorname{E}[X_{t-\\tau}]\\operatorname{E}[Y_t]$. Since all processes are given as zero-mean, this simplifies to $\\operatorname{Cov}(X_{t-\\tau}, Y_t) = \\operatorname{E}[X_{t-\\tau} Y_t]$.\n\nUnder the null hypothesis $c=0$, we have:\n$$\n\\operatorname{Cov}(X_{t-\\tau}, Y_t) = \\operatorname{E}[(a\\, S_{t-\\tau-\\ell_x} + u_{t-\\tau})(b\\, S_{t-\\ell_y} + v_t)]\n$$\nExpanding the product gives:\n$$\n\\operatorname{Cov}(X_{t-\\tau}, Y_t) = \\operatorname{E}[ab\\, S_{t-\\tau-\\ell_x} S_{t-\\ell_y} + a\\, S_{t-\\tau-\\ell_x} v_t + b\\, S_{t-\\ell_y} u_{t-\\tau} + u_{t-\\tau} v_t]\n$$\nBy linearity of expectation, we can separate the terms:\n$$\n\\operatorname{Cov}(X_{t-\\tau}, Y_t) = ab\\, \\operatorname{E}[S_{t-\\tau-\\ell_x} S_{t-\\ell_y}] + a\\, \\operatorname{E}[S_{t-\\tau-\\ell_x} v_t] + b\\, \\operatorname{E}[S_{t-\\ell_y} u_{t-\\tau}] + \\operatorname{E}[u_{t-\\tau} v_t]\n$$\nAccording to the problem statement, all noise terms ($u_t, v_t, \\eta_t$) are mutually independent and independent of $S_t$. Since the processes are zero-mean, the expectation of products of independent variables is zero.\n- $\\operatorname{E}[S_{t-\\tau-\\ell_x} v_t] = \\operatorname{E}[S_{t-\\tau-\\ell_x}] \\operatorname{E}[v_t] = 0 \\cdot 0 = 0$.\n- $\\operatorname{E}[S_{t-\\ell_y} u_{t-\\tau}] = \\operatorname{E}[S_{t-\\ell_y}] \\operatorname{E}[u_{t-\\tau}] = 0 \\cdot 0 = 0$.\n- $\\operatorname{E}[u_{t-\\tau} v_t] = \\operatorname{E}[u_{t-\\tau}] \\operatorname{E}[v_t] = 0 \\cdot 0 = 0$ (as $u_t$ and $v_t$ are independent processes).\n\nThis leaves only the first term:\n$$\n\\operatorname{Cov}(X_{t-\\tau}, Y_t) = ab\\, \\operatorname{E}[S_{t-\\tau-\\ell_x} S_{t-\\ell_y}]\n$$\nThe term $\\operatorname{E}[S_{t-\\tau-\\ell_x} S_{t-\\ell_y}]$ is the autocovariance of the stationary process $S_t$, denoted $\\gamma_S(k)$, where the lag $k$ is the difference in the time indices: $k = (t-\\ell_y) - (t-\\tau-\\ell_x) = \\tau + \\ell_x - \\ell_y$.\nSo,\n$$\n\\operatorname{Cov}(X_{t-\\tau}, Y_t) = ab\\, \\gamma_S(\\tau + \\ell_x - \\ell_y)\n$$\nFor the stationary AR(1) process $S_t = \\phi S_{t-1} + \\eta_t$ with $|\\phi|1$, the variance is $\\gamma_S(0) = \\operatorname{Var}(S_t) = \\frac{\\sigma_\\eta^2}{1-\\phi^2}$. The autocovariance function is $\\gamma_S(k) = \\phi^{|k|} \\gamma_S(0)$. Substituting this in, we get the final expression:\n$$\n\\operatorname{Cov}(X_{t-\\tau}, Y_t) = ab\\, \\frac{\\sigma_\\eta^2}{1-\\phi^2} \\phi^{|\\tau + \\ell_x - \\ell_y|}\n$$\nThis quantity is generally non-zero if $a \\neq 0$, $b \\neq 0$, and $\\phi \\neq 0$, which explicitly shows how a non-zero cross-covariance arises from the common driver even when $c=0$.\n\n**Part 2. Design of the Statistical Test**\n\nThe task is to find a procedure that correctly tests the null hypothesis $H_0: c=0$. This requires isolating the direct influence of $X_{t-\\tau}$ on $Y_t$ from the indirect influence mediated by the common driver $S_t$. The correct approach is to statistically control for the confounding variables.\n\nThe full model is $Y_t = b\\, S_{t-\\ell_y} + c\\, X_{t-\\tau} + v_t$. The variable $X_{t-\\tau}$ is itself dependent on $S_{t-\\tau-\\ell_x}$. The variables that are common causes to both $Y_t$ and the predictor of interest $X_{t-\\tau}$ are the various lags of $S_t$. To test for the unique contribution of $X_{t-\\tau}$ to $Y_t$, one must account for the explanatory power of the $S_t$ process.\n\n**Analysis of Options:**\n\n**A.** Declaring an interaction based on the simple sample cross-correlation between $X_{t-\\tau}$ and $Y_t$ is precisely the flawed approach that fails to distinguish direct from confounded effects. As derived in Part 1, this correlation can be significant even when $c=0$. This method tests for *functional connectivity* (any statistical association) rather than *effective connectivity* (direct causal influence).\n**Verdict: Incorrect.**\n\n**B.** This option proposes forming a control vector $Z_t$ from the lags of the common driver $S_t$ and then testing for the relationship between $X_{t-\\tau}$ and $Y_t$ *after* controlling for $Z_t$. This can be done by calculating the partial correlation $\\rho_{X_{t-\\tau}, Y_t \\cdot Z_t}$ or by testing the significance of the coefficient $\\beta_X$ in the multiple regression model $Y_t = \\beta_0 + \\beta_X X_{t-\\tau} + \\beta^\\top Z_t + \\varepsilon_t$. This is the standard, correct procedure for dealing with confounders in a linear system. By including the relevant dynamics of $S_t$ in the model, we can isolate the additional explanatory power provided by $X_{t-\\tau}$, which, under this model, corresponds to the direct path parameterized by $c$. The equivalence of the partial correlation test and the t-test for a regression coefficient is a standard result for linear Gaussian models. The description of $Z_t$ correctly identifies that it should contain the lags of $S_t$ responsible for the confounding.\n**Verdict: Correct.**\n\n**C.** This option proposes to control only for the instantaneous driver $S_t$. The confounding effect, however, is mediated by specific lags $S_{t-\\ell_y}$ and $S_{t-\\tau-\\ell_x}$. Controlling for $S_t$ is arbitrary and generally incorrect; it does not properly remove the influence of the specific lags that cause the confounding. While $S_t$ is correlated with all its lags, controlling for it will typically only partially, not fully, remove the confounding effect, leading to an invalid test.\n**Verdict: Incorrect.**\n\n**D.** This option describes an incorrect logical sequence. It first regresses $Y_t$ on $X_{t-\\tau}$ to obtain $Y_t^{\\perp X} = Y_t - \\hat{\\gamma} X_{t-\\tau}$, then tests if the residual is correlated with $S_t$. If one performs the regression $Y_t = \\hat{\\gamma} X_{t-\\tau} + Y_t^{\\perp X}$, the estimated coefficient $\\hat{\\gamma}$ will be biased due to the omission of the confounder $S_t$. Finding that the residual $Y_t^{\\perp X}$ is correlated with $S_t$ merely confirms that the initial simple regression model was misspecified because it omitted a relevant variable ($S_t$). This procedure does not provide a valid test for the null hypothesis $H_0: c=0$. The correct logic is to test for the influence of $X_{t-\\tau}$ *after* controlling for $S_t$, not the other way around.\n**Verdict: Incorrect.**",
            "answer": "$$\\boxed{B}$$"
        }
    ]
}