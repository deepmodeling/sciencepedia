{
    "hands_on_practices": [
        {
            "introduction": "The security of a Proof-of-Work blockchain against double-spending is probabilistic, not absolute. This exercise delves into the foundational security model first described by Satoshi Nakamoto, quantifying the probability that an attacker can reverse a confirmed transaction. By modeling the block discovery race using principles from probability theory, you will derive the exact risk of a double-spend, providing a rigorous basis for understanding transaction finality in PoW systems .",
            "id": "4264554",
            "problem": "Consider a Proof-of-Work (PoW) blockchain where block discovery is modeled as a sequence of independent trials, each resulting in either an honest block or an adversarial block. Let the adversary control a fraction $\\alpha \\in (0, \\tfrac{1}{2})$ of the total hash power, and the honest network control a fraction $1-\\alpha$. A transaction is considered accepted after it receives $z \\in \\mathbb{N}$ confirmations, meaning $z$ honest blocks have been appended after the block containing the transaction. The merchant accepts the transaction at the stopping time when the honest chain has advanced by $z$ blocks after the transaction. The adversary mines privately from the moment the transaction is broadcast, aiming to later replace the honest chain segment containing the transaction.\n\nModel the sequence of block discoveries as independent Bernoulli trials with probability $p = 1-\\alpha$ of an honest block and $q = \\alpha$ of an adversarial block. Let $K$ be the number of adversarial blocks discovered before the $z$-th honest block is discovered. The random variable $K$ follows the negative binomial distribution with parameters $(z, q)$ in the sense that\n$$\n\\mathbb{P}[K = k] = \\binom{z+k-1}{k} p^{z} q^{k}, \\quad k \\in \\mathbb{Z}_{\\ge 0}.\n$$\nAfter the $z$-th honest block, if the adversary is behind by $d = z - k > 0$ blocks, the probability that the adversary eventually catches up (and overtakes) under a biased random walk with drift favoring the honest network is given by the classical gambler's ruin hitting probability\n$$\n\\left(\\frac{q}{p}\\right)^{d}.\n$$\nIf $d \\le 0$, the adversary is already caught up or ahead, and the eventual reversal occurs with probability $1$.\n\nStarting from these fundamental modeling assumptions and definitions, derive the exact probability $P(z,\\alpha)$ that a transaction accepted with $z$ confirmations is eventually reversed by the adversary. Express your final result in closed form using the tail of the negative binomial distribution and the regularized incomplete beta function $I_{x}(a,b) = \\dfrac{B_{x}(a,b)}{B(a,b)}$, where $B_{x}(a,b) = \\int_{0}^{x} t^{a-1}(1-t)^{b-1}\\,dt$ and $B(a,b) = \\int_{0}^{1} t^{a-1}(1-t)^{b-1}\\,dt$. Your final answer must be a single analytic expression in terms of $z$ and $\\alpha$. No numerical evaluation or rounding is required.",
            "solution": "The user wants me to find the probability that a transaction is eventually reversed.\n\n### Step 1: Extract Givens\n-   Adversary hash power fraction: $\\alpha \\in (0, \\frac{1}{2})$.\n-   Honest network hash power fraction: $1-\\alpha$.\n-   Confirmations required: $z \\in \\mathbb{N}$.\n-   Probability of an honest block (success): $p = 1-\\alpha$.\n-   Probability of an adversarial block (failure): $q = \\alpha$.\n-   $K$ is the number of adversarial blocks found before the $z$-th honest block is found.\n-   The probability mass function of $K$ is given as $\\mathbb{P}[K=k] = \\binom{z+k-1}{k} p^z q^k$ for $k \\in \\mathbb{Z}_{\\ge 0}$.\n-   The chain deficit for the adversary is $d = z-k$.\n-   The probability of reversal given $K=k$ is:\n    -   $\\left(\\frac{q}{p}\\right)^{z-k}$ if $z-k > 0$ (i.e., $k < z$).\n    -   $1$ if $z-k \\le 0$ (i.e., $k \\ge z$).\n-   The goal is to find the total probability of reversal, $P(z, \\alpha)$.\n-   The final answer must be expressed in terms of $z$, $\\alpha$, and the regularized incomplete beta function $I_x(a,b)$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement provides a well-defined probabilistic model for analyzing blockchain security, specifically the double-spend attack scenario. This model is a standard and respected approach in the academic literature on cryptocurrencies, originating from Satoshi Nakamoto's whitepaper and later formalized by researchers like Meni Rosenfeld.\n\n-   **Scientifically Grounded:** The model is based on fundamental principles of probability theory (Bernoulli trials, negative binomial distribution, gambler's ruin problem). The assumption $\\alpha < 1/2$ is the standard security assumption for Proof-of-Work blockchains, ensuring that the honest chain is expected to grow faster than any adversarial chain. The problem is scientifically sound.\n-   **Well-Posed:** The problem provides all necessary definitions, variables, and conditional probabilities to calculate the desired total probability. The objective is clear, and the setup is structured to admit a unique solution.\n-   **Objective:** The problem is stated using precise mathematical language, free from any subjective or ambiguous terminology.\n\nAll criteria for validity are met. The problem is valid.\n\n### Step 3: Verdict and Action\nThe problem is valid. I will proceed with the solution.\n\n### Derivation of the Reversal Probability\n\nLet $E$ be the event that the transaction is eventually reversed. We seek to calculate $P(z, \\alpha) = \\mathbb{P}[E]$. We can compute this by applying the law of total probability, conditioning on the number of adversarial blocks $K$ found up to the point of $z$ confirmations.\n\n$$\nP(z, \\alpha) = \\sum_{k=0}^{\\infty} \\mathbb{P}[E | K=k] \\mathbb{P}[K=k]\n$$\n\nThe problem provides the conditional probability $\\mathbb{P}[E | K=k]$ based on whether the adversary is behind ($k < z$) or not ($k \\ge z$). We can split the summation into two parts accordingly:\n$$\nP(z, \\alpha) = \\sum_{k=0}^{z-1} \\mathbb{P}[E | K=k] \\mathbb{P}[K=k] + \\sum_{k=z}^{\\infty} \\mathbb{P}[E | K=k] \\mathbb{P}[K=k]\n$$\nSubstituting the given conditional probabilities:\n$$\nP(z, \\alpha) = \\sum_{k=0}^{z-1} \\left(\\frac{q}{p}\\right)^{z-k} \\mathbb{P}[K=k] + \\sum_{k=z}^{\\infty} (1) \\cdot \\mathbb{P}[K=k]\n$$\nLet's analyze each sum separately.\n\nThe second sum is simply the tail probability of the negative binomial distribution for $K$:\n$$\n\\text{Sum 2} = \\sum_{k=z}^{\\infty} \\mathbb{P}[K=k] = \\mathbb{P}[K \\ge z]\n$$\nThe cumulative distribution function (CDF) of a negative binomial random variable for the number of failures $K$ before $z$ successes (with success probability $p$) is related to the regularized incomplete beta function by $\\mathbb{P}[K \\le k] = I_p(z, k+1)$. Therefore, the tail probability is:\n$$\n\\mathbb{P}[K \\ge z] = 1 - \\mathbb{P}[K \\le z-1] = 1 - I_p(z, (z-1)+1) = 1 - I_p(z, z)\n$$\n\nNow, let's analyze the first sum. We substitute the PMF of $K$, $\\mathbb{P}[K=k] = \\binom{z+k-1}{k} p^z q^k$:\n$$\n\\text{Sum 1} = \\sum_{k=0}^{z-1} \\left(\\frac{q}{p}\\right)^{z-k} \\binom{z+k-1}{k} p^z q^k\n$$\nLet's simplify the term inside the summation:\n$$\n\\left(\\frac{q}{p}\\right)^{z-k} p^z q^k = \\frac{q^{z-k}}{p^{z-k}} p^z q^k = p^{z - (z-k)} q^{k + (z-k)} = p^k q^z\n$$\nSo the first sum becomes:\n$$\n\\text{Sum 1} = \\sum_{k=0}^{z-1} \\binom{z+k-1}{k} p^k q^z = q^z \\sum_{k=0}^{z-1} \\binom{z+k-1}{k} p^k\n$$\nTo evaluate this finite sum, we use the identity for the negative binomial series: $\\sum_{k=0}^{\\infty} \\binom{n+k-1}{k} x^k = (1-x)^{-n}$. For our sum, with $n=z$ and $x=p$, the full infinite series would be $\\sum_{k=0}^{\\infty} \\binom{z+k-1}{k} p^k = (1-p)^{-z} = q^{-z}$.\n\nWe can express the finite sum as the infinite sum minus its tail:\n$$\n\\sum_{k=0}^{z-1} \\binom{z+k-1}{k} p^k = \\left(\\sum_{k=0}^{\\infty} \\binom{z+k-1}{k} p^k\\right) - \\left(\\sum_{k=z}^{\\infty} \\binom{z+k-1}{k} p^k\\right) = q^{-z} - \\sum_{k=z}^{\\infty} \\binom{z+k-1}{k} p^k\n$$\nSubstituting this back into the expression for Sum 1:\n$$\n\\text{Sum 1} = q^z \\left(q^{-z} - \\sum_{k=z}^{\\infty} \\binom{z+k-1}{k} p^k\\right) = 1 - q^z \\sum_{k=z}^{\\infty} \\binom{z+k-1}{k} p^k\n$$\nThe term $q^z \\sum_{k=z}^{\\infty} \\binom{z+k-1}{k} p^k$ can be interpreted as the tail probability of a different negative binomial distribution. Let $K'$ be a random variable representing the number of failures before $z$ successes, where the success probability is $q$ and failure probability is $p=1-q$. The PMF of $K'$ is $\\mathbb{P}[K'=k] = \\binom{z+k-1}{k} q^z p^k$.\nThus, the term is $\\sum_{k=z}^{\\infty} \\mathbb{P}[K'=k] = \\mathbb{P}[K' \\ge z]$. Using the same CDF-to-beta-function identity, we get:\n$$\n\\mathbb{P}[K' \\ge z] = 1 - \\mathbb{P}[K' \\le z-1] = 1 - I_q(z, z)\n$$\nTherefore, the first sum evaluates to:\n$$\n\\text{Sum 1} = 1 - \\mathbb{P}[K' \\ge z] = 1 - (1 - I_q(z,z)) = I_q(z,z)\n$$\n\nFinally, we combine the two sums to get the total probability $P(z, \\alpha)$:\n$$\nP(z, \\alpha) = \\text{Sum 1} + \\text{Sum 2} = I_q(z,z) + (1 - I_p(z,z))\n$$\nWe now use the fundamental identity of the regularized incomplete beta function: $I_x(a,b) + I_{1-x}(b,a) = 1$.\nIn our case, with $x=q$, $1-x=p$, and $a=b=z$, the identity becomes $I_q(z,z) + I_p(z,z) = 1$.\nThis implies that $1 - I_p(z,z) = I_q(z,z)$.\nSubstituting this into our expression for $P(z, \\alpha)$:\n$$\nP(z, \\alpha) = I_q(z,z) + I_q(z,z) = 2 I_q(z,z)\n$$\nSince $q = \\alpha$, the final expression for the probability of eventual reversal is:\n$$\nP(z, \\alpha) = 2 I_{\\alpha}(z,z)\n$$",
            "answer": "$$\n\\boxed{2 I_{\\alpha}(z,z)}\n$$"
        },
        {
            "introduction": "A blockchain's consensus mechanism is supported by a peer-to-peer network responsible for disseminating information. The topology of this network critically impacts the system's performance, security, and resilience against attacks. This practice provides a hands-on opportunity to apply fundamental tools from complex network science to analyze the structure and robustness of a P2P network graph, helping you quantify its properties and vulnerabilities .",
            "id": "4264612",
            "problem": "You are given undirected samples of peer-to-peer (P2P) connections in blockchain and cryptocurrency networks. Model each sample as a simple undirected graph with a fixed number of nodes and an edge list of unordered pairs. Using first principles of complex networks, you must compute three quantities for each sample: the empirical degree distribution over all nodes, the degree assortativity coefficient, and a resilience profile under targeted node removal. Work in purely mathematical terms and implement a single program that produces the specified outputs for the provided test suite.\n\nFundamental base and definitions to use and derive from:\n- A simple undirected graph is defined by a node set of size $N$ and an edge set $E \\subset \\{\\{u,v\\} \\mid u,v \\in \\{0,1,\\dots,N-1\\}, u \\neq v\\}$, with no parallel edges and no self-loops.\n- The degree of a node $i$, denoted $k_i$, is the number of neighbors of $i$ in the graph, derived from the adjacency relationship defined by $E$.\n- The empirical degree distribution is the probability mass function on $\\{0,1,\\dots,k_{\\max}\\}$ where $k_{\\max} = \\max_{i} k_i$, with probability $p_k$ equal to the fraction of nodes having degree exactly $k$; i.e., $p_k = \\frac{1}{N} |\\{i \\mid k_i = k\\}|$.\n- Degree assortativity is defined as the Pearson correlation coefficient between the degrees at the two ends of edges, computed across all edge pairs. Let $m = |E|$ be the number of edges. Let the mean degree across edge-ends be $\\mu = \\frac{1}{2m} \\sum_{\\{u,v\\}\\in E} (k_u + k_v)$. Let the covariance across paired edge-ends be $\\mathrm{cov} = \\frac{1}{m} \\sum_{\\{u,v\\}\\in E} (k_u - \\mu)(k_v - \\mu)$ and the edge-end variance be $\\sigma^2 = \\frac{1}{2m} \\sum_{\\{u,v\\}\\in E} [(k_u - \\mu)^2 + (k_v - \\mu)^2]$. The assortativity coefficient is $r = \\frac{\\mathrm{cov}}{\\sigma^2}$, with the convention that if $m=0$ or $\\sigma^2=0$ then $r=0$.\n- Resilience under targeted attack is measured by sequentially removing high-degree nodes and quantifying the size of the largest connected component among the remaining nodes. The removal strategy is adaptive: at each removal step select the node with the highest current degree; if there is a tie, select the node with the smallest index. After removing $K$ nodes, compute $S_{\\max}$, the number of remaining nodes in the largest connected component, and report the resilience ratio $R = \\frac{S_{\\max}}{N}$.\n\nTasks to implement for each test case:\n1. Compute the empirical degree distribution $[p_0, p_1, \\dots, p_{k_{\\max}}]$ over all $N$ nodes, including nodes of degree $0$.\n2. Compute the degree assortativity coefficient $r$ using the Pearson correlation defined above.\n3. Compute the resilience ratios $R$ for a list of removal fractions $P = [p^{(1)}, p^{(2)}, \\dots, p^{(L)}]$, where each $p^{(l)}$ is a decimal fraction of nodes to remove. For each fraction $p^{(l)}$, remove $K^{(l)} = \\left\\lfloor p^{(l)} N \\right\\rfloor$ nodes adaptively as specified, starting from the original graph for each $p^{(l)}$ independently, and report the corresponding $R^{(l)}$.\n\nAll quantities are unitless probabilities or ratios. Express all fractions as decimals. Angles are not used.\n\nTest suite:\n- Case $1$ (happy path, clustered P2P structure):\n    - $N = 10$\n    - $E = \\{(0,1),(1,2),(2,0),(2,3),(3,4),(4,5),(5,3),(6,7),(7,8),(8,6),(8,9),(0,9)\\}$\n    - $P = [0.0, 0.1, 0.25, 0.5]$\n- Case $2$ (boundary condition, star topology):\n    - $N = 8$\n    - $E = \\{(0,1),(0,2),(0,3),(0,4),(0,5),(0,6),(0,7)\\}$\n    - $P = [0.0, 0.1, 0.25, 0.5]$\n- Case $3$ (edge case, disjoint components and isolated nodes):\n    - $N = 9$\n    - $E = \\{(0,1),(1,2),(3,4),(4,5),(5,3)\\}$\n    - $P = [0.0, 0.1, 0.25, 0.5]$\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each case, output a list of the form $[\\text{degree\\_distribution}, r, \\text{resilience\\_vector}]$, where $\\text{degree\\_distribution}$ is a list of decimals $[p_0,\\dots,p_{k_{\\max}}]$, $r$ is a decimal, and $\\text{resilience\\_vector}$ is a list of decimals corresponding to $P$. The overall output is thus $[\\text{case}_1,\\text{case}_2,\\text{case}_3]$ in a single line, for example $[[[p_0,\\dots],r,[R^{(1)},\\dots]],[\\dots],[\\dots]]$.",
            "solution": "The problem requires the computation of three network-theoretic quantities for given graph samples: the empirical degree distribution, the degree assortativity coefficient, and a resilience profile under a targeted node removal strategy. The solution is structured by implementing algorithms derived directly from the mathematical definitions provided.\n\nAn undirected graph is represented using an adjacency list, which is a mapping from each node index $i \\in \\{0, 1, \\dots, N-1\\}$ to a set of its neighbors. This structure is efficient for calculating node degrees and for graph traversal algorithms.\n\n**1. Empirical Degree Distribution**\n\nFirst, the degree $k_i$ for each node $i$ is calculated. The degree of a node is the size of its neighbor set in the adjacency list.\nLet the list of degrees for all $N$ nodes be $[k_0, k_1, \\dots, k_{N-1}]$. The maximum degree in the graph is $k_{\\max} = \\max_{i} k_i$.\nThe empirical degree distribution is the probability mass function $\\{p_k\\}$ where $p_k$ is the fraction of nodes with degree $k$. This is computed by first creating a frequency count of each degree value from $0$ to $k_{\\max}$. For each $k \\in \\{0, 1, \\dots, k_{\\max}\\}$, we count the number of nodes $N_k = |\\{i \\mid k_i = k\\}|$. The probability $p_k$ is then given by the formula:\n$$p_k = \\frac{N_k}{N}$$\nThe final output for this part is a list $[p_0, p_1, \\dots, p_{k_{\\max}}]$.\n\n**2. Degree Assortativity Coefficient**\n\nThe degree assortativity coefficient, $r$, is computed based on the provided Pearson correlation-like formula. Let the set of edges be $E$ with size $m = |E|$. If $m=0$, $r$ is defined as $0$. Otherwise, we proceed as follows:\nFirst, we use the node degrees $k_i$ computed previously.\nThe mean degree across all edge-ends, $\\mu$, is calculated. There are $2m$ such edge-ends.\n$$\\mu = \\frac{1}{2m} \\sum_{\\{u,v\\}\\in E} (k_u + k_v)$$\nNext, the edge-end variance, $\\sigma^2$, is the variance of the degrees at all edge-ends.\n$$\\sigma^2 = \\frac{1}{2m} \\sum_{\\{u,v\\}\\in E} \\left[ (k_u - \\mu)^2 + (k_v - \\mu)^2 \\right]$$\nIf $\\sigma^2=0$, $r$ is defined as $0$. This occurs, for example, in a regular graph where all nodes have the same degree.\nThen, the covariance of degrees across edges, $\\mathrm{cov}$, is calculated:\n$$\\mathrm{cov} = \\frac{1}{m} \\sum_{\\{u,v\\}\\in E} (k_u - \\mu)(k_v - \\mu)$$\nFinally, the assortativity coefficient $r$ is the ratio of the covariance to the variance:\n$$r = \\frac{\\mathrm{cov}}{\\sigma^2}$$\n\n**3. Resilience Profile**\n\nThe resilience of the network is assessed by simulating a targeted attack. For each specified removal fraction $p^{(l)}$ from the list $P$, the following procedure is executed independently, starting each time from the original graph.\n\nFirst, the number of nodes to remove is calculated as $K^{(l)} = \\lfloor p^{(l)} N \\rfloor$.\nThe removal process is adaptive and iterative over $K^{(l)}$ steps. In each step:\n1. The current degrees of all nodes remaining in the graph are computed. The degree of a node is the number of its neighbors that are also still in the graph.\n2. The node with the highest current degree is identified. If multiple nodes share the same highest degree, the one with the smallest node index is selected for removal. This tie-breaking rule ensures the process is deterministic.\n3. The selected node is removed from the graph.\n\nAfter $K^{(l)}$ nodes have been removed, the structure of the remaining graph is analyzed. We must find the size of the largest connected component, $S_{\\max}$. This is achieved using a graph traversal algorithm, such as Breadth-First Search (BFS) or Depth-First Search (DFS). The algorithm is initiated for each unvisited node in the remaining graph, counting the nodes in its component. The maximum size found across all components is $S_{\\max}$. If no nodes remain, $S_{\\max} = 0$.\n\nThe resilience ratio $R^{(l)}$ is then calculated as the fraction of the original nodes that are in the largest component:\n$$R^{(l)} = \\frac{S_{\\max}}{N}$$\nThis process is repeated for all fractions in $P$, yielding a list of resilience ratios $[R^{(1)}, R^{(2)}, \\dots, R^{(L)}]$.\n\nThe combination of these three computed quantities—$[p_0, \\dots, p_{k_{\\max}}]$, $r$, and $[R^{(1)}, \\dots, R^{(L)}]$—forms the complete result for a single test case.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print the results.\n    \"\"\"\n    test_cases = [\n        {\n            \"N\": 10,\n            \"E\": [(0,1),(1,2),(2,0),(2,3),(3,4),(4,5),(5,3),(6,7),(7,8),(8,6),(8,9),(0,9)],\n            \"P\": [0.0, 0.1, 0.25, 0.5]\n        },\n        {\n            \"N\": 8,\n            \"E\": [(0,1),(0,2),(0,3),(0,4),(0,5),(0,6),(0,7)],\n            \"P\": [0.0, 0.1, 0.25, 0.5]\n        },\n        {\n            \"N\": 9,\n            \"E\": [(0,1),(1,2),(3,4),(4,5),(5,3)],\n            \"P\": [0.0, 0.1, 0.25, 0.5]\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        result = process_case(case[\"N\"], case[\"E\"], case[\"P\"])\n        all_results.append(result)\n\n    # Format the final output string as per specifications.\n    results_str = []\n    for res_case in all_results:\n        dist_str = f\"[{','.join(map(str, res_case[0]))}]\"\n        r_str = str(res_case[1])\n        resil_str = f\"[{','.join(map(str, res_case[2]))}]\"\n        results_str.append(f\"[{dist_str},{r_str},{resil_str}]\")\n    \n    final_output = f\"[{','.join(results_str)}]\"\n    print(final_output)\n\ndef process_case(N, E, P):\n    \"\"\"\n    Processes a single test case to compute degree distribution, assortativity, and resilience.\n    \"\"\"\n    if N == 0:\n        dist = [1.0] if N == 0 else [] # Special case for N=0\n        r = 0.0\n        resilience = [0.0] * len(P)\n        return [dist, r, resilience]\n\n    # Build adjacency list\n    adj = {i: set() for i in range(N)}\n    for u, v in E:\n        adj[u].add(v)\n        adj[v].add(u)\n\n    # 1. Degree Distribution\n    degrees = np.array([len(adj[i]) for i in range(N)])\n    if len(degrees) == 0:\n        k_max = -1\n    else:\n        k_max = np.max(degrees) if N > 0 else -1\n        \n    counts = np.bincount(degrees, minlength=k_max + 1 if k_max >= 0 else 0)\n    p_k = (counts / N).tolist() if N > 0 else []\n\n    # 2. Degree Assortativity\n    m = len(E)\n    if m == 0:\n        r = 0.0\n    else:\n        mu_numerator = sum(degrees[u] + degrees[v] for u, v in E)\n        mu = mu_numerator / (2 * m)\n\n        sigma_sq_numerator = sum((degrees[u] - mu)**2 + (degrees[v] - mu)**2 for u, v in E)\n        sigma_sq = sigma_sq_numerator / (2 * m)\n\n        if sigma_sq == 0:\n            r = 0.0\n        else:\n            cov_numerator = sum((degrees[u] - mu) * (degrees[v] - mu) for u, v in E)\n            cov = cov_numerator / m\n            r = cov / sigma_sq\n    \n    # 3. Resilience Profile\n    resilience_ratios = []\n    for p_frac in P:\n        K = int(p_frac * N)\n        \n        active_nodes = set(range(N))\n        \n        # Adaptive node removal\n        for _ in range(K):\n            if not active_nodes:\n                break\n                \n            max_deg = -1\n            nodes_to_remove_candidates = []\n            \n            # Calculate current degrees of active nodes\n            current_degrees = {}\n            for node in active_nodes:\n                current_deg = sum(1 for neighbor in adj[node] if neighbor in active_nodes)\n                current_degrees[node] = current_deg\n                if current_deg > max_deg:\n                    max_deg = current_deg\n            \n            # Find all nodes with max degree\n            for node in active_nodes:\n                if current_degrees[node] == max_deg:\n                    nodes_to_remove_candidates.append(node)\n            \n            # Tie-break with smallest index\n            node_to_remove = min(nodes_to_remove_candidates)\n            active_nodes.remove(node_to_remove)\n\n        # Find largest connected component size (S_max)\n        s_max = 0\n        visited = set()\n        for i in active_nodes:\n            if i not in visited:\n                component_size = 0\n                q = [i]\n                visited.add(i)\n                \n                head = 0\n                while head  len(q):\n                    u = q[head]\n                    head += 1\n                    component_size += 1\n                    for v in adj[u]:\n                        if v in active_nodes and v not in visited:\n                            visited.add(v)\n                            q.append(v)\n                \n                if component_size > s_max:\n                    s_max = component_size\n        \n        resilience_ratio = s_max / N if N > 0 else 0\n        resilience_ratios.append(resilience_ratio)\n\n    return [p_k, r, resilience_ratios]\n\nif __name__ == '__main__':\n    solve()\n```"
        },
        {
            "introduction": "Sharding is a leading scalability solution that aims to increase blockchain throughput by parallelizing transaction processing. However, the gains from parallelism are often limited by the overhead required to coordinate transactions that span multiple shards. This exercise guides you through building a performance model using queueing theory to quantify the achievable throughput of a sharded system, taking into account the costs of ensuring atomicity across shards .",
            "id": "4264584",
            "problem": "You are asked to construct a program that models the throughput scaling of a sharded blockchain with atomic cross-shard transactions under Two-Phase Commit (2PC) coordination. The system comprises $m$ identical shards that process transactions assigned uniformly at random. Transactions may touch multiple shards; atomicity is enforced via Two-Phase Commit (2PC), which introduces coordination latency and potential locking overhead across participating shards. You must derive the achievable steady-state throughput under basic queueing and capacity constraints and quantify the overhead relative to an ideal baseline with no coordination cost.\n\nFundamental base and assumptions:\n- Let each shard be modeled as a single-server queue with a memoryless arrival process and memoryless service time, known as an $M/M/1$ queue. For an $M/M/1$ system with arrival rate $\\lambda$ and service rate $\\mu$, the stability condition is $\\lambda  \\mu$, and the mean service time is $1/\\mu$.\n- Transactions are assigned uniformly to shards, and the per-shard arrival processes can be treated as approximately Poisson by aggregation under independence assumptions.\n- Each transaction touches a random number of shards $S \\in \\{1,2,\\dots\\}$ drawn from a specified distribution with probability mass function $p_s = \\mathbb{P}(S=s)$. Denote $\\mathbb{E}[S] = \\sum_s s \\, p_s$. The fraction of cross-shard transactions is $p_{\\mathrm{cross}} = \\sum_{s1} p_s$.\n- Coordination uses Two-Phase Commit (2PC) with two synchronous rounds (prepare and commit), each incurring a network latency of $L$ seconds. Processing overhead is parameterized for shards and for a coordinator as below.\n\nModeling definitions and parameters:\n- Let $m$ be the number of shards.\n- Let $\\mu_s$ be the service rate of a single shard in transactions per second (tps). The baseline mean shard service time (without cross-shard locking) is $s_{\\mathrm{intra}} = 1/\\mu_s$ seconds.\n- For a cross-shard transaction touching $s$ shards, define the lock-hold time per participant shard as $t_{\\mathrm{lock}}(s) = 2L + r \\, s$, where $L$ is the one-way network latency in seconds and $r$ is the per-shard processing time per participant in seconds. Let $\\alpha \\in [0,1]$ represent the fraction of $t_{\\mathrm{lock}}(s)$ that blocks the shard’s service (i.e., non-overlappable critical section time).\n- The per-involvement shard service time for a transaction touching $s$ shards is\n$$\nt_{\\mathrm{invol}}(s) = \n\\begin{cases}\ns_{\\mathrm{intra}},  s = 1, \\\\\ns_{\\mathrm{intra}} + \\alpha \\, t_{\\mathrm{lock}}(s),  s  1.\n\\end{cases}\n$$\n- The coordinator’s per-transaction service time for a transaction touching $s$ shards is\n$$\nt_{\\mathrm{coord}}(s) = 2L + c_0 + r_c \\, s,\n$$\nwhere $c_0$ is a base processing time in seconds and $r_c$ is per-participant coordinator processing time in seconds.\n- For workload mixing, the weight that a random shard involvement comes from transactions with shard-count $s$ is $w_s = \\frac{p_s \\, s}{\\mathbb{E}[S]}$, since such transactions produce $s$ participant-involvements.\n\nThroughput scaling derivation goals:\n- Starting from the $M/M/1$ stability condition $\\lambda  \\mu$ and the conservation of workload across shards, derive the per-shard involvement arrival rate $\\lambda_{\\mathrm{invol}} = \\lambda_{\\mathrm{tot}} \\, \\mathbb{E}[S]/m$, where $\\lambda_{\\mathrm{tot}}$ is the global transaction arrival rate in transactions per second.\n- Derive the mean per-involvement service time as $\\bar{t}_{\\mathrm{invol}} = \\sum_s w_s \\, t_{\\mathrm{invol}}(s)$, and thus the effective per-involvement shard service rate $\\mu_{\\mathrm{eff}} = 1/\\bar{t}_{\\mathrm{invol}}$.\n- Enforce the shard stability constraint $\\lambda_{\\mathrm{invol}}  \\mu_{\\mathrm{eff}}$, which implies an upper bound on achievable throughput\n$$\n\\lambda_{\\mathrm{tot}} \\le \\frac{\\mu_{\\mathrm{eff}} \\, m}{\\mathbb{E}[S]}.\n$$\n- Compute the expected coordinator service time over cross-shard transactions as $\\bar{t}_{\\mathrm{coord}} = \\frac{1}{p_{\\mathrm{cross}}} \\sum_{s1} p_s \\, t_{\\mathrm{coord}}(s)$, with effective coordinator service rate $\\mu_{\\mathrm{coord}} = 1/\\bar{t}_{\\mathrm{coord}}$. Enforce the coordinator stability constraint $p_{\\mathrm{cross}} \\, \\lambda_{\\mathrm{tot}}  \\mu_{\\mathrm{coord}}$, leading to\n$$\n\\lambda_{\\mathrm{tot}} \\le \\frac{\\mu_{\\mathrm{coord}}}{p_{\\mathrm{cross}}}.\n$$\n- The achievable steady-state throughput is the minimum of these constraints:\n$$\n\\lambda_{\\mathrm{max}} = \\min \\left( \\frac{\\mu_{\\mathrm{eff}} \\, m}{\\mathbb{E}[S]}, \\; \\frac{\\mu_{\\mathrm{coord}}}{p_{\\mathrm{cross}}} \\right),\n$$\nwith the convention that the coordinator bound is ignored if $p_{\\mathrm{cross}} = 0$.\n- Define the ideal baseline throughput (no coordination overhead and no locking, i.e., $\\alpha = 0$ and $t_{\\mathrm{coord}}(s) = 0$) as\n$$\n\\lambda_{\\mathrm{baseline}} = \\frac{\\mu_s \\, m}{\\mathbb{E}[S]}.\n$$\n- Quantify the overhead as the fractional loss in throughput\n$$\n\\delta = 1 - \\frac{\\lambda_{\\mathrm{max}}}{\\lambda_{\\mathrm{baseline}}},\n$$\nexpressed as a decimal.\n\nYour program must implement this model and produce results for the following test suite. All time quantities must be in seconds, and all service rates must be in transactions per second (tps). When a probability is required, it must be given as a decimal fraction. The workload distribution $p_s$ is specified by pairs $(s, p_s)$; the distribution must be normalized by your program.\n\nTest suite parameter sets:\n- Case A (general mixed workload):\n  - $m = 16$, $\\mu_s = 150.0$, $L = 0.03$, $r = 0.001$, $\\alpha = 0.8$, $c_0 = 0.002$, $r_c = 0.0005$, $p_s \\in \\{(1,0.7),(2,0.25),(4,0.05)\\}$.\n- Case B (no cross-shard transactions):\n  - $m = 32$, $\\mu_s = 200.0$, $L = 0.05$, $r = 0.001$, $\\alpha = 0.7$, $c_0 = 0.002$, $r_c = 0.0005$, $p_s \\in \\{(1,1.0)\\}$.\n- Case C (heavy cross-shard, large $s$):\n  - $m = 64$, $\\mu_s = 180.0$, $L = 0.05$, $r = 0.002$, $\\alpha = 0.9$, $c_0 = 0.005$, $r_c = 0.001$, $p_s \\in \\{(1,0.2),(8,0.8)\\}$.\n- Case D (coordinator-limited regime):\n  - $m = 64$, $\\mu_s = 1000.0$, $L = 0.05$, $r = 0.0005$, $\\alpha = 0.5$, $c_0 = 0.01$, $r_c = 0.002$, $p_s \\in \\{(1,0.5),(2,0.5)\\}$.\n- Case E (few shards, moderate cross-shard):\n  - $m = 4$, $\\mu_s = 100.0$, $L = 0.02$, $r = 0.0015$, $\\alpha = 1.0$, $c_0 = 0.001$, $r_c = 0.001$, $p_s \\in \\{(1,0.5),(3,0.5)\\}$.\n\nFinal output format:\n- For each test case, compute two quantities: the achievable throughput $\\lambda_{\\mathrm{max}}$ in transactions per second and the overhead fraction $\\delta$ as a decimal.\n- Your program should produce a single line of output containing all results flattened into one comma-separated list enclosed in square brackets, in the order of the test cases. For instance, the output must be of the form\n$[ \\lambda_{\\mathrm{max}}^{(A)}, \\delta^{(A)}, \\lambda_{\\mathrm{max}}^{(B)}, \\delta^{(B)}, \\dots ]$.",
            "solution": "The problem statement is valid. It presents a well-posed performance modeling task for a sharded blockchain system, grounded in standard queueing theory. The parameters are clearly defined, and the objectives are specific and mathematically derivable. All assumptions are explicitly stated, and a logical path from premises to conclusion is provided. We will proceed with a full derivation and solution.\n\nThe problem asks for the calculation of the maximum achievable throughput, $\\lambda_{\\mathrm{max}}$, and the associated overhead, $\\delta$, for a sharded blockchain system. The model is based on a set of parameters describing the system architecture, workload, and coordination protocol penalties. The derivation relies on the stability condition of $M/M/1$ queues, which states that the mean arrival rate must be less than the mean service rate.\n\nThe step-by-step procedure to derive the solution is as follows:\n\n**1. Workload Characterization**\nFirst, we must characterize the transaction workload. The workload is defined by a discrete probability mass function, $p_s = \\mathbb{P}(S=s)$, where $S$ is the random variable for the number of shards a transaction touches. The provided probabilities, $p_s$, are first normalized to ensure they sum to $1$.\nLet the set of shard counts be $\\mathcal{S}$. The normalized probability for a transaction touching $s \\in \\mathcal{S}$ shards is:\n$$ \\hat{p}_s = \\frac{p_s}{\\sum_{k \\in \\mathcal{S}} p_k} $$\nFrom this distribution, we compute two key statistics:\n-   The expected number of shards per transaction, $\\mathbb{E}[S]$:\n    $$ \\mathbb{E}[S] = \\sum_{s \\in \\mathcal{S}} s \\cdot \\hat{p}_s $$\n-   The fraction of transactions that are cross-shard, $p_{\\mathrm{cross}}$:\n    $$ p_{\\mathrm{cross}} = \\sum_{s \\in \\mathcal{S}, s > 1} \\hat{p}_s $$\n\n**2. Shard-Level Performance Analysis**\nEach of the $m$ shards is modeled as an $M/M/1$ queue. The total transactional workload, arriving at a global rate of $\\lambda_{\\mathrm{tot}}$ transactions per second, generates a total of $\\lambda_{\\mathrm{tot}} \\mathbb{E}[S]$ shard-level involvements per second. Assuming uniform distribution, the arrival rate of involvements at a single shard is:\n$$ \\lambda_{\\mathrm{invol}} = \\frac{\\lambda_{\\mathrm{tot}} \\mathbb{E}[S]}{m} $$\nThe service time for an involvement depends on whether the transaction is intra-shard ($s=1$) or cross-shard ($s>1$). The baseline intra-shard service time is $s_{\\mathrm{intra}} = 1/\\mu_s$. For a cross-shard transaction touching $s$ shards, a lock-hold time $t_{\\mathrm{lock}}(s) = 2L + r \\cdot s$ is incurred, where $L$ is network latency and $r$ is a per-participant processing overhead. A fraction $\\alpha$ of this time contributes to the service time.\nThe per-involvement service time, $t_{\\mathrm{invol}}(s)$, is thus:\n$$ t_{\\mathrm{invol}}(s) = \\begin{cases} s_{\\mathrm{intra}}  \\text{if } s=1 \\\\ s_{\\mathrm{intra}} + \\alpha \\cdot t_{\\mathrm{lock}}(s)  \\text{if } s>1 \\end{cases} $$\nTo find the mean service time at a shard, we must average over all types of involvements. The probability that a random involvement comes from a transaction of size $s$ is weighted by $s$, giving the weight $w_s$:\n$$ w_s = \\frac{\\hat{p}_s \\cdot s}{\\mathbb{E}[S]} $$\nThe mean per-involvement service time, $\\bar{t}_{\\mathrm{invol}}$, is the weighted average:\n$$ \\bar{t}_{\\mathrm{invol}} = \\sum_{s \\in \\mathcal{S}} w_s \\cdot t_{\\mathrm{invol}}(s) = \\frac{1}{\\mathbb{E}[S]} \\sum_{s \\in \\mathcal{S}} \\hat{p}_s \\cdot s \\cdot t_{\\mathrm{invol}}(s) $$\nThe effective service rate of a shard for these involvements is $\\mu_{\\mathrm{eff}} = 1/\\bar{t}_{\\mathrm{invol}}$.\nThe shard stability condition is $\\lambda_{\\mathrm{invol}}  \\mu_{\\mathrm{eff}}$. Substituting the expression for $\\lambda_{\\mathrm{invol}}$ gives:\n$$ \\frac{\\lambda_{\\mathrm{tot}} \\mathbb{E}[S]}{m}  \\mu_{\\mathrm{eff}} \\implies \\lambda_{\\mathrm{tot}}  \\frac{\\mu_{\\mathrm{eff}} \\cdot m}{\\mathbb{E}[S]} $$\nThis establishes the first upper bound on the total system throughput, which we denote $\\lambda_{\\mathrm{shards}}$.\n\n**3. Coordinator Performance Analysis**\nCross-shard transactions ($s>1$) require a coordinator. The arrival rate of transactions at the coordinator is $p_{\\mathrm{cross}} \\lambda_{\\mathrm{tot}}$. The coordinator's service time for a transaction involving $s$ shards is given by:\n$$ t_{\\mathrm{coord}}(s) = 2L + c_0 + r_c \\cdot s $$\nThe mean service time at the coordinator, $\\bar{t}_{\\mathrm{coord}}$, is averaged over all cross-shard transactions:\n$$ \\bar{t}_{\\mathrm{coord}} = \\frac{\\sum_{s \\in \\mathcal{S}, s>1} \\hat{p}_s \\cdot t_{\\mathrm{coord}}(s)}{\\sum_{s \\in \\mathcal{S}, s>1} \\hat{p}_s} = \\frac{1}{p_{\\mathrm{cross}}} \\sum_{s \\in \\mathcal{S}, s>1} \\hat{p}_s \\cdot t_{\\mathrm{coord}}(s) $$\nThis calculation is only meaningful if $p_{\\mathrm{cross}} > 0$. The effective service rate of the coordinator is $\\mu_{\\mathrm{coord}} = 1/\\bar{t}_{\\mathrm{coord}}$.\nThe coordinator stability condition is $p_{\\mathrm{cross}} \\lambda_{\\mathrm{tot}}  \\mu_{\\mathrm{coord}}$, which implies a second throughput bound:\n$$ \\lambda_{\\mathrm{tot}}  \\frac{\\mu_{\\mathrm{coord}}}{p_{\\mathrm{cross}}} $$\nThis bound, denoted $\\lambda_{\\mathrm{coord}}$, is considered only if $p_{\\mathrm{cross}} > 0$. If $p_{\\mathrm{cross}} = 0$, there is no coordinator load, and this bound can be treated as infinite.\n\n**4. Achievable Throughput and Overhead**\nThe overall system throughput is limited by the most constrained component. Therefore, the maximum achievable steady-state throughput, $\\lambda_{\\mathrm{max}}$, is the minimum of the shard-derived and coordinator-derived bounds:\n$$ \\lambda_{\\mathrm{max}} = \\min \\left( \\frac{\\mu_{\\mathrm{eff}} \\cdot m}{\\mathbb{E}[S]}, \\frac{\\mu_{\\mathrm{coord}}}{p_{\\mathrm{cross}}} \\right) $$\nIf $p_{\\mathrm{cross}} = 0$, the second term is ignored.\nTo quantify the performance loss due to coordination and locking, we compare this to an ideal baseline throughput, $\\lambda_{\\mathrm{baseline}}$. The baseline assumes no coordination overhead ($\\alpha=0$) and no coordinator bottleneck. In this ideal case, the effective shard service rate simply becomes the base rate $\\mu_s$.\n$$ \\lambda_{\\mathrm{baseline}} = \\frac{\\mu_s \\cdot m}{\\mathbb{E}[S]} $$\nThe overhead, $\\delta$, is the fractional reduction in throughput from this ideal baseline:\n$$ \\delta = 1 - \\frac{\\lambda_{\\mathrm{max}}}{\\lambda_{\\mathrm{baseline}}} $$\nA value of $\\delta=0$ indicates no overhead, while $\\delta=1$ indicates a complete loss of throughput.\n\nThese steps are systematically applied to each test case to generate the required results.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases.\n    \"\"\"\n    test_cases = [\n        # Case A (general mixed workload)\n        {\n            \"m\": 16, \"mu_s\": 150.0, \"L\": 0.03, \"r\": 0.001, \"alpha\": 0.8,\n            \"c_0\": 0.002, \"r_c\": 0.0005, \"p_s_raw\": [(1, 0.7), (2, 0.25), (4, 0.05)]\n        },\n        # Case B (no cross-shard transactions)\n        {\n            \"m\": 32, \"mu_s\": 200.0, \"L\": 0.05, \"r\": 0.001, \"alpha\": 0.7,\n            \"c_0\": 0.002, \"r_c\": 0.0005, \"p_s_raw\": [(1, 1.0)]\n        },\n        # Case C (heavy cross-shard, large s)\n        {\n            \"m\": 64, \"mu_s\": 180.0, \"L\": 0.05, \"r\": 0.002, \"alpha\": 0.9,\n            \"c_0\": 0.005, \"r_c\": 0.001, \"p_s_raw\": [(1, 0.2), (8, 0.8)]\n        },\n        # Case D (coordinator-limited regime)\n        {\n            \"m\": 64, \"mu_s\": 1000.0, \"L\": 0.05, \"r\": 0.0005, \"alpha\": 0.5,\n            \"c_0\": 0.01, \"r_c\": 0.002, \"p_s_raw\": [(1, 0.5), (2, 0.5)]\n        },\n        # Case E (few shards, moderate cross-shard)\n        {\n            \"m\": 4, \"mu_s\": 100.0, \"L\": 0.02, \"r\": 0.0015, \"alpha\": 1.0,\n            \"c_0\": 0.001, \"r_c\": 0.001, \"p_s_raw\": [(1, 0.5), (3, 0.5)]\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        lambda_max, delta = calculate_metrics(**case)\n        results.extend([lambda_max, delta])\n\n    # Format output as specified: [val1,val2,val3,...]\n    formatted_results = [f\"{r:.10f}\".rstrip('0').rstrip('.') for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\ndef calculate_metrics(m, mu_s, L, r, alpha, c_0, r_c, p_s_raw):\n    \"\"\"\n    Calculates lambda_max and delta for a single set of parameters.\n    \"\"\"\n    p_s_dict = dict(p_s_raw)\n    \n    # Step 1: Normalize p_s and compute workload statistics\n    total_prob = sum(p_s_dict.values())\n    p_s = {s: p / total_prob for s, p in p_s_dict.items()}\n    \n    exp_S = sum(s * p for s, p in p_s.items())\n    p_cross = sum(p for s, p in p_s.items() if s > 1)\n\n    # Step 2: Calculate shard-level service times and effective rate\n    s_intra = 1.0 / mu_s\n    \n    t_lock = {s: 2 * L + r * s for s in p_s.keys()}\n    t_invol = {s: s_intra if s == 1 else s_intra + alpha * t_lock[s] for s in p_s.keys()}\n    \n    # Weighted average of involvement service time\n    # sum( (p_s * s / exp_S) * t_invol(s) ) = (1 / exp_S) * sum(p_s * s * t_invol(s))\n    sum_p_s_t_invol = sum(p * s * t_invol[s] for s, p in p_s.items())\n    t_invol_bar = sum_p_s_t_invol / exp_S\n    \n    mu_eff = 1.0 / t_invol_bar\n    \n    # Step 3: Calculate coordinator-level service time and effective rate\n    t_coord = {s: 2 * L + c_0 + r_c * s for s, p in p_s.items() if s > 1}\n    \n    mu_coord = 0\n    if p_cross > 0:\n        sum_p_t_coord = sum(p * t_coord[s] for s, p in p_s.items() if s > 1)\n        t_coord_bar = sum_p_t_coord / p_cross\n        if t_coord_bar > 0:\n            mu_coord = 1.0 / t_coord_bar\n\n    # Step 4: Determine throughput bounds\n    lambda_shards = (mu_eff * m) / exp_S\n    \n    if p_cross > 0 and mu_coord > 0:\n        lambda_coordinator = mu_coord / p_cross\n    else:\n        # If no cross-shard txns, coordinator is not a bottleneck\n        lambda_coordinator = float('inf')\n\n    # Step 5: Calculate achievable throughput\n    lambda_max = min(lambda_shards, lambda_coordinator)\n    \n    # Step 6: Calculate baseline throughput and overhead\n    lambda_baseline = (mu_s * m) / exp_S\n    \n    delta = 0.0\n    if lambda_baseline > 0:\n        delta = 1.0 - (lambda_max / lambda_baseline)\n    \n    return lambda_max, delta\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}