{
    "hands_on_practices": [
        {
            "introduction": "To identify meaningful structure in a network, we must first understand the spectral signature of pure randomness. This practice establishes that baseline by examining the Erdős–Rényi random graph, a fundamental null model in network science. By computing the first few moments of the centered adjacency matrix's spectrum , you will gain firsthand insight into why it converges to the celebrated Wigner semicircle law, a cornerstone result connecting graph theory and random matrix theory.",
            "id": "4299563",
            "problem": "Consider an undirected Erdős–Rényi random graph $G(n,p)$ with $n$ vertices, where each of the $\\binom{n}{2}$ potential edges is present independently with probability $p \\in (0,1)$, and absent otherwise. Let $A \\in \\mathbb{R}^{n \\times n}$ be its adjacency matrix, with $A_{ij} = A_{ji} \\in \\{0,1\\}$ for $i \\neq j$, and $A_{ii} = 0$. Define the centered and normalized adjacency (a Wigner-type scaling) by\n$$\nW \\;=\\; \\frac{A - p\\,(J - I)}{\\sqrt{n\\,p\\,(1-p)}}\\,,\n$$\nwhere $I$ is the identity matrix and $J$ is the all-ones matrix. For $k \\in \\{1,2,3,4\\}$, define the $k$-th moment of the empirical spectral distribution via the trace power method:\n$$\nm_{k}(n) \\;=\\; \\frac{1}{n}\\,\\mathbb{E}\\!\\left[ \\operatorname{Tr}\\!\\left(W^{k}\\right) \\right]\\,.\n$$\nStarting from first principles and core definitions (independence of edges, zero mean after centering, and the variance normalization), derive explicit formulas for $m_{1}(n)$, $m_{2}(n)$, $m_{3}(n)$, and $m_{4}(n)$, and then determine the limits $\\lim_{n \\to \\infty} m_{k}(n)$ in the dense regime where $p$ is a fixed constant in $(0,1)$. Finally, verify that these limits coincide with the limiting moments of the Gaussian Orthogonal Ensemble (GOE) (Gaussian Orthogonal Ensemble (GOE) is the ensemble of real symmetric matrices with independent, identically distributed Gaussian entries on and above the diagonal, scaled so that off-diagonal variances are of order $1/n$), and provide the limiting values of the first four moments as your final answer.\n\nExpress your final answer as a single row vector containing the four limiting moment values in increasing order of $k$, using exact values. No rounding is required.",
            "solution": "We begin by recalling the setting. The adjacency matrix $A$ of $G(n,p)$ has independent off-diagonal entries $A_{ij} = A_{ji} \\in \\{0,1\\}$ for $i \\neq j$, with $\\mathbb{P}(A_{ij}=1) = p$ and $\\mathbb{P}(A_{ij}=0) = 1-p$, and diagonal entries $A_{ii} = 0$. Define the centered matrix $B = A - p\\,(J - I)$ so that, for $i \\neq j$, $B_{ij} = A_{ij} - p$ has mean $0$ and variance $\\operatorname{Var}(B_{ij}) = p(1-p)$, while $B_{ii} = 0$. The normalized matrix is\n$$\nW \\;=\\; \\frac{B}{\\sqrt{n\\,p\\,(1-p)}}\\,,\n$$\nso that for $i \\neq j$, $W_{ij}$ has mean $0$ and variance\n$$\n\\operatorname{Var}(W_{ij}) \\;=\\; \\frac{\\operatorname{Var}(B_{ij})}{n\\,p\\,(1-p)} \\;=\\; \\frac{1}{n}\\,,\n$$\nand $W_{ii} = 0$. The off-diagonal entries $\\{W_{ij} : 1 \\le i  j \\le n\\}$ are independent, and $W_{ij} = W_{ji}$.\n\nDefine the moments\n$$\nm_{k}(n) \\;=\\; \\frac{1}{n}\\,\\mathbb{E}\\!\\left[ \\operatorname{Tr}\\!\\left(W^{k}\\right) \\right] \\;=\\; \\frac{1}{n}\\,\\sum_{i_{1},\\ldots,i_{k} = 1}^{n} \\mathbb{E}\\!\\left[ W_{i_{1} i_{2}}\\,W_{i_{2} i_{3}} \\cdots W_{i_{k} i_{1}} \\right]\\,.\n$$\nThis expansion counts closed walks of length $k$ on the vertex set $\\{1,\\ldots,n\\}$, with contribution from products of entries along the walk. Because $\\mathbb{E}[W_{ij}] = 0$ for $i \\neq j$, any term survives the expectation only if each undirected edge (unordered pair) appearing in the product is used at least twice (so that variables can be paired). We now treat $k=1,2,3,4$.\n\nFirst moment ($k=1$). We have\n$$\nm_{1}(n) \\;=\\; \\frac{1}{n}\\,\\mathbb{E}\\!\\left[ \\operatorname{Tr}(W) \\right] \\;=\\; \\frac{1}{n}\\,\\sum_{i=1}^{n} \\mathbb{E}\\!\\left[ W_{ii} \\right] \\;=\\; 0\\,,\n$$\nsince $W_{ii} = 0$.\n\nSecond moment ($k=2$). We write\n$$\nm_{2}(n) \\;=\\; \\frac{1}{n}\\,\\mathbb{E}\\!\\left[ \\operatorname{Tr}(W^{2}) \\right] \\;=\\; \\frac{1}{n}\\,\\sum_{i,j=1}^{n} \\mathbb{E}\\!\\left[ W_{ij}\\,W_{ji} \\right] \\;=\\; \\frac{1}{n}\\,\\sum_{i \\neq j} \\mathbb{E}\\!\\left[ W_{ij}^{2} \\right]\\,,\n$$\nbecause $W_{ii}=0$ and $W_{ij} = W_{ji}$. For $i \\neq j$, $\\mathbb{E}[W_{ij}^{2}] = \\operatorname{Var}(W_{ij}) = 1/n$. Therefore,\n$$\nm_{2}(n) \\;=\\; \\frac{1}{n}\\,\\big(n(n-1)\\big)\\,\\frac{1}{n} \\;=\\; \\frac{n-1}{n} \\;\\xrightarrow[n\\to\\infty]{}\\; 1\\,.\n$$\n\nThird moment ($k=3$). We have\n$$\nm_{3}(n) \\;=\\; \\frac{1}{n}\\,\\sum_{i,j,k=1}^{n} \\mathbb{E}\\!\\left[ W_{ij}\\,W_{jk}\\,W_{ki} \\right]\\,.\n$$\nA product of three centered independent (across distinct undirected pairs) entries has zero expectation unless indices degenerate so that at least one variable is diagonal or one edge appears three times. Here any degeneracy that would force pairing also introduces a diagonal factor (e.g., $i=j$ makes $W_{ii}=0$), or yields an odd power of a single off-diagonal entry with zero mean. Thus every term has zero expectation, and\n$$\nm_{3}(n) \\;=\\; 0\\,.\n$$\n\nFourth moment ($k=4$). We expand\n$$\nm_{4}(n) \\;=\\; \\frac{1}{n}\\,\\sum_{i,j,k,\\ell = 1}^{n} \\mathbb{E}\\!\\left[ W_{ij}\\,W_{jk}\\,W_{k\\ell}\\,W_{\\ell i} \\right]\\,.\n$$\nLet $\\{e_{1},e_{2},e_{3},e_{4}\\}$ denote the unordered pairs corresponding to edges in the $4$-cycle: $e_{1} = \\{i,j\\}$, $e_{2} = \\{j,k\\}$, $e_{3} = \\{k,\\ell\\}$, $e_{4} = \\{ \\ell,i \\}$. A term contributes if each edge appears at least twice. The leading contributions come from pairings that use two distinct off-diagonal edges, each twice; these are the non-crossing pairings. We enumerate the surviving patterns and compute their contribution. We also bound all remaining contributions to show they vanish in the limit.\n\nNon-crossing pairings:\n- Pattern $\\mathsf{P1}$: $e_{1} = e_{2}$ and $e_{3} = e_{4}$. The condition $e_{1} = e_{2}$ forces $i = k$, and $e_{3} = e_{4}$ then holds automatically for any $\\ell \\neq i$; off-diagonal constraints require $i \\neq j$ and $i \\neq \\ell$. To ensure the two edges are distinct, we also require $j \\neq \\ell$. Counting choices gives $n$ choices for $i$, $(n-1)$ choices for $j \\neq i$, and $(n-2)$ choices for $\\ell \\neq i,j$, hence $n\\,(n-1)\\,(n-2)$ index tuples producing two distinct edges. For each such tuple, independence across distinct undirected pairs yields\n$$\n\\mathbb{E}\\!\\left[ W_{ij}\\,W_{jk}\\,W_{k\\ell}\\,W_{\\ell i} \\right] \\;=\\; \\mathbb{E}[W_{ij}^{2}]\\,\\mathbb{E}[W_{i\\ell}^{2}] \\;=\\; \\frac{1}{n}\\,\\cdot\\,\\frac{1}{n} \\;=\\; \\frac{1}{n^{2}}\\,.\n$$\nThus, the total contribution from $\\mathsf{P1}$ distinct-edge tuples is\n$$\n\\frac{1}{n}\\,\\big(n\\,(n-1)\\,(n-2)\\big)\\,\\frac{1}{n^{2}} \\;=\\; \\frac{(n-1)\\,(n-2)}{n^{2}}\\,.\n$$\nIf instead $j=\\ell$, then all four factors use the same edge $\\{i,j\\}$, giving a contribution proportional to $\\mathbb{E}[W_{ij}^{4}]$; there are $n\\,(n-1)$ such tuples, and each contributes $\\mathbb{E}[W_{ij}^{4}]$. We compute\n$$\n\\mathbb{E}[W_{ij}^{4}] \\;=\\; \\frac{\\mathbb{E}[B_{ij}^{4}]}{n^{2}\\,p^{2}\\,(1-p)^{2}} \\;=\\; \\frac{p\\,(1-p)^{4} + (1-p)\\,p^{4}}{n^{2}\\,p^{2}\\,(1-p)^{2}} \\;=\\; \\frac{(1-p)^{3} + p^{3}}{n^{2}\\,p\\,(1-p)}\\,,\n$$\nsince $B_{ij} = A_{ij} - p$ takes values $1-p$ with probability $p$ and $-p$ with probability $1-p$. Therefore the total contribution of these $\\mathsf{P1}$ degenerate tuples to $m_{4}(n)$ is\n$$\n\\frac{1}{n}\\,\\big(n\\,(n-1)\\big)\\,\\frac{(1-p)^{3} + p^{3}}{n^{2}\\,p\\,(1-p)} \\;=\\; \\frac{n-1}{n^{2}}\\,\\frac{(1-p)^{3} + p^{3}}{p\\,(1-p)} \\;\\xrightarrow[n\\to\\infty]{}\\; 0\\,.\n$$\n\n- Pattern $\\mathsf{P2}$: $e_{1} = e_{4}$ and $e_{2} = e_{3}$. The condition $e_{1} = e_{4}$ forces $j = \\ell$, and $e_{2} = e_{3}$ then holds for any $k \\neq j$; off-diagonal constraints require $j \\neq i$ and $k \\neq j$. To ensure the two edges are distinct, we require $k \\neq i$. Counting gives $n$ choices for $i$, $(n-1)$ choices for $j \\neq i$, and $(n-2)$ choices for $k \\neq i,j$, hence $n\\,(n-1)\\,(n-2)$ distinct-edge tuples. Each contributes $\\mathbb{E}[W_{ij}^{2}]\\,\\mathbb{E}[W_{jk}^{2}] = (1/n)^{2}$. Thus the total contribution from $\\mathsf{P2}$ distinct-edge tuples is also\n$$\n\\frac{(n-1)\\,(n-2)}{n^{2}}\\,.\n$$\nDegenerate tuples with $k = i$ produce a single edge used four times, contributing $\\mathbb{E}[W_{ij}^{4}]$ exactly as above, and their total contribution to $m_{4}(n)$ vanishes as $n \\to \\infty$.\n\nCrossing pairing:\n- Pattern $\\mathsf{P3}$: $e_{1} = e_{3}$ and $e_{2} = e_{4}$. The only way to satisfy off-diagonal constraints is $k = i$ and $\\ell = j$, which yields the single-edge quadruple product $W_{ij}^{4}$. There are $n\\,(n-1)$ such tuples, each contributing $\\mathbb{E}[W_{ij}^{4}]$. As above, their total contribution to $m_{4}(n)$ is $\\frac{n-1}{n^{2}}\\,\\frac{(1-p)^{3} + p^{3}}{p\\,(1-p)} \\to 0$.\n\nAll remaining index configurations introduce at least one diagonal factor $W_{ii} = 0$ or yield an odd moment in some off-diagonal entry and thus have zero expectation.\n\nCollecting the leading contributions from $\\mathsf{P1}$ and $\\mathsf{P2}$ and noting that all degenerate and crossing contributions vanish in the large $n$ limit, we obtain\n$$\nm_{4}(n) \\;=\\; \\frac{(n-1)\\,(n-2)}{n^{2}} \\;+\\; \\frac{(n-1)\\,(n-2)}{n^{2}} \\;+\\; o(1) \\;=\\; \\frac{2\\,(n-1)\\,(n-2)}{n^{2}} \\;+\\; o(1)\\,,\n$$\nhence\n$$\n\\lim_{n \\to \\infty} m_{4}(n) \\;=\\; 2\\,.\n$$\n\nSummary of limits. In the dense regime with fixed $p \\in (0,1)$,\n$$\n\\lim_{n \\to \\infty} m_{1}(n) \\;=\\; 0\\,, \\qquad \\lim_{n \\to \\infty} m_{2}(n) \\;=\\; 1\\,, \\qquad \\lim_{n \\to \\infty} m_{3}(n) \\;=\\; 0\\,, \\qquad \\lim_{n \\to \\infty} m_{4}(n) \\;=\\; 2\\,.\n$$\nThese are exactly the first four moments of the limiting empirical spectral distribution for a properly scaled Gaussian Orthogonal Ensemble (GOE), i.e., the semicircle law with unit variance, whose even moments are the Catalan numbers ($m_{2} = 1$, $m_{4} = 2$) and whose odd moments vanish.\n\nTherefore, the centered and normalized adjacency matrix of dense $G(n,p)$ has first four moments converging to the GOE moments:\n$$\n\\big(\\lim_{n\\to\\infty} m_{1}(n),\\,\\lim_{n\\to\\infty} m_{2}(n),\\,\\lim_{n\\to\\infty} m_{3}(n),\\,\\lim_{n\\to\\infty} m_{4}(n)\\big) \\;=\\; (0,\\,1,\\,0,\\,2)\\,.\n$$",
            "answer": "$$\\boxed{\\begin{pmatrix}0  1  0  2\\end{pmatrix}}$$"
        },
        {
            "introduction": "Once we have a baseline for randomness, the next question is how to detect significant structural patterns. This exercise delves into the heart of spectral detection by modeling a coherent signal as a simple rank-one matrix perturbation . You will derive the renowned Baik–Ben Arous–Péché (BBP) phase transition, determining the precise signal strength needed for an 'outlier' eigenvalue to emerge from the random spectral bulk. This principle is the theoretical foundation for using eigenvalues to identify hubs, communities, and other key network features.",
            "id": "4299560",
            "problem": "Consider a sequence of undirected networks with $n$ nodes, modeled as sparse Erdős–Rényi graphs with independent edges. Let $A^{(n)}$ be the adjacency matrix with off-diagonal entries $A^{(n)}_{ij} \\in \\{0,1\\}$ for $i \\neq j$, where $\\mathbb{P}(A^{(n)}_{ij} = 1) = p_{n}$ and $p_{n} = c/n$ for a fixed $c  0$. Define the centered and standardized adjacency-like matrix $X^{(n)}$ by\n$$\nX^{(n)}_{ij} = \\begin{cases}\n\\dfrac{A^{(n)}_{ij} - p_{n}}{\\sqrt{n p_{n} (1 - p_{n})}}  \\text{if } i \\neq j, \\\\\n0  \\text{if } i = j,\n\\end{cases}\n$$\nand consider a rank-one heterogeneity (signal) added to this bulk,\n$$\nS^{(n)} = X^{(n)} + \\theta\\, v v^{\\top},\n$$\nwhere $v \\in \\mathbb{R}^{n}$ is a deterministic unit vector ($\\|v\\|_{2} = 1$) independent of $X^{(n)}$, and $\\theta \\in \\mathbb{R}$ is a signal strength parameter. In the large-$n$ limit, $X^{(n)}$ is a Wigner-type matrix with variance of off-diagonal entries equal to $1/n$, and its empirical spectral distribution converges to the semicircle law supported on $[-2,2]$.\n\nUsing first principles of random matrix theory for networks, derive the outlier-eigenvalue condition associated with the Baik–Ben Arous–Péché (BBP) phase transition for $S^{(n)}$, expressed in terms of the bulk Stieltjes transform $m(z)$ of the limiting spectral distribution of $X^{(n)}$. Then, for adjacency-like scaling as defined above (i.e., the centered and standardized matrix yielding the semicircle law of variance $1$), solve the resulting self-consistency equation for a putative outlier on the right of the bulk support and identify the critical value of $\\theta$ at which an outlier first emerges from the right edge of the bulk.\n\nYour derivation must begin from the definition of the Stieltjes transform, the resolvent of a matrix, and the Sherman–Morrison formula for rank-one updates, and proceed to the limiting isotropic approximation $\\langle v, (\\cdot) v \\rangle \\to m(z)$. Explicitly use the Stieltjes transform $m(z)$ of the semicircle law at real arguments $z  2$. Report the critical signal strength as a single exact number. No rounding is required. The final answer must be a single real number.",
            "solution": "The problem asks for the critical signal strength $\\theta$ for the emergence of an outlier eigenvalue in a specific random matrix model. The derivation proceeds in three main stages: first, establishing the general equation for outlier eigenvalues in a rank-one perturbed matrix model; second, specializing this equation to the given matrix ensemble whose limiting spectral distribution is the semicircle law; and third, solving for the critical parameter value.\n\nLet $S^{(n)} = X^{(n)} + \\theta v v^{\\top}$ be the matrix in question, where $X^{(n)}$ is the random \"bulk\" matrix and $\\theta v v^{\\top}$ is the rank-one \"signal\" or perturbation. We are interested in the eigenvalues of $S^{(n)}$ in the large-$n$ limit. Outlier eigenvalues are those that lie outside the support of the limiting spectral distribution of the bulk matrix $X^{(n)}$.\n\nThe location of eigenvalues can be determined from the poles of the Stieltjes transform of the empirical spectral distribution. The Stieltjes transform for a matrix $M$ of size $n \\times n$ is defined as\n$$\nm_{M}(z) = \\frac{1}{n} \\text{Tr}\\left((M - zI_n)^{-1}\\right)\n$$\nwhere $z \\in \\mathbb{C} \\setminus \\text{spectrum}(M)$ and $I_n$ is the $n \\times n$ identity matrix. The matrix $(M-zI_n)^{-1}$ is known as the resolvent of $M$ at $z$.\n\nLet $R_X(z) = (X^{(n)} - zI_n)^{-1}$ be the resolvent of the bulk matrix $X^{(n)}$. The resolvent of the perturbed matrix $S^{(n)}$ can be found using the Sherman–Morrison formula for the inverse of a rank-one update. For an invertible matrix $A$ and vectors $u, w$, the formula states $(A+uw^{\\top})^{-1} = A^{-1} - \\frac{A^{-1}uw^{\\top}A^{-1}}{1+w^{\\top}A^{-1}u}$.\nApplying this to $(S^{(n)} - zI_n)^{-1}$ with $A = X^{(n)} - zI_n$, $u = \\theta v$, and $w = v$:\n$$\n(S^{(n)} - zI_n)^{-1} = (X^{(n)} - zI_n + \\theta v v^{\\top})^{-1} = R_X(z) - \\frac{R_X(z) (\\theta v) v^{\\top} R_X(z)}{1 + v^{\\top} R_X(z) (\\theta v)} = R_X(z) - \\frac{\\theta R_X(z) v v^{\\top} R_X(z)}{1 + \\theta v^{\\top} R_X(z) v}\n$$\nThe Stieltjes transform of $S^{(n)}$, denoted $m_S(z)$, is obtained by taking the normalized trace:\n$$\nm_S(z) = \\frac{1}{n} \\text{Tr}\\left( (S^{(n)} - zI_n)^{-1} \\right) = \\frac{1}{n}\\text{Tr}(R_X(z)) - \\frac{1}{n} \\text{Tr}\\left( \\frac{\\theta R_X(z) v v^{\\top} R_X(z)}{1 + \\theta v^{\\top} R_X(z) v} \\right)\n$$\nThe first term is the Stieltjes transform of $X^{(n)}$, $m_X(z)$. For the second term, the denominator is a scalar and can be factored out of the trace. Using the cyclic property of the trace, $\\text{Tr}(AB) = \\text{Tr}(BA)$, we can simplify the trace in the numerator: $\\text{Tr}(R_X(z) v v^{\\top} R_X(z)) = \\text{Tr}(v^{\\top} R_X(z)^2 v) = v^{\\top} R_X(z)^2 v$, as the trace of a scalar is the scalar itself. So,\n$$\nm_S(z) = m_X(z) - \\frac{\\theta}{n} \\frac{v^{\\top} R_X(z)^2 v}{1 + \\theta v^{\\top} R_X(z) v}\n$$\nAn outlier eigenvalue $\\lambda$ of $S^{(n)}$ is an isolated real-valued pole of $m_S(z)$. Such a pole can only arise from the denominator of the second term vanishing. Thus, the location $\\lambda$ of a putative outlier must satisfy the equation:\n$$\n1 + \\theta \\, v^{\\top} R_X(\\lambda) v = 0 \\quad \\implies \\quad v^{\\top} (X^{(n)} - \\lambda I_n)^{-1} v = -\\frac{1}{\\theta}\n$$\nThis is the fundamental equation for the outlier eigenvalues. In the limit of large $n$, the quadratic form $v^{\\top} R_X(z) v$ concentrates around its expectation. For a deterministic, delocalized unit vector $v$, this quantity converges to the limiting Stieltjes transform of the bulk matrix. The problem statement provides this as the \"isotropic approximation\" $\\langle v, (\\cdot) v \\rangle \\to m(z)$, which means $v^{\\top} R_X(z) v \\to m(z)$ where $m(z) = \\lim_{n \\to \\infty} m_X(z)$.\nThus, the limiting equation for an outlier eigenvalue $\\lambda$ is:\n$$\n1 + \\theta m(\\lambda) = 0 \\quad \\text{or} \\quad m(\\lambda) = -\\frac{1}{\\theta}\n$$\nThis is the general outlier-eigenvalue condition for this class of models, expressed in terms of the bulk Stieltjes transform.\n\nNext, we apply this to the specific problem. The matrix $X^{(n)}$ is defined such that its empirical spectral distribution converges to the Wigner semicircle law. The variance of the off-diagonal entries is specified as $1/n$. For a Wigner matrix with entry variance $\\sigma^2/n$, the limiting spectrum is supported on $[-2\\sigma, 2\\sigma]$. Here, $\\sigma^2=1$, so the support is $[-2, 2]$, as stated in the problem. The Stieltjes transform of the Wigner semicircle law supported on $[-2, 2]$ is given by:\n$$\nm(z) = \\frac{-z + \\sqrt{z^2 - 4}}{2}\n$$\nfor $z \\in \\mathbb{C} \\setminus [-2, 2]$. We are looking for an outlier emerging from the right edge of the bulk, so we consider real $z > 2$.\nSubstituting this into the outlier equation $m(\\lambda) = -1/\\theta$:\n$$\n\\frac{-\\lambda + \\sqrt{\\lambda^2 - 4}}{2} = -\\frac{1}{\\theta}\n$$\nWe solve for the outlier location $\\lambda > 2$ in terms of the signal strength $\\theta$. We assume $\\theta>0$, which is necessary for an outlier to appear on the right side of the spectrum (since for $\\lambda>2$, $m(\\lambda)0$).\n$$\n-\\lambda + \\sqrt{\\lambda^2 - 4} = -\\frac{2}{\\theta}\n$$\n$$\n\\sqrt{\\lambda^2 - 4} = \\lambda - \\frac{2}{\\theta}\n$$\nFor a real solution to exist, the right-hand side must be non-negative, $\\lambda \\ge 2/\\theta$. Squaring both sides:\n$$\n\\lambda^2 - 4 = \\left(\\lambda - \\frac{2}{\\theta}\\right)^2 = \\lambda^2 - \\frac{4\\lambda}{\\theta} + \\frac{4}{\\theta^2}\n$$\n$$\n-4 = -\\frac{4\\lambda}{\\theta} + \\frac{4}{\\theta^2}\n$$\nMultiplying by $-\\theta/4$:\n$$\n\\theta = \\lambda - \\frac{1}{\\theta}\n$$\nSolving for $\\lambda$, we find the location of the outlier eigenvalue:\n$$\n\\lambda = \\theta + \\frac{1}{\\theta}\n$$\nThe BBP phase transition occurs at the critical value of $\\theta$ where an outlier eigenvalue first emerges from the continuous spectrum of the bulk. For an outlier to exist on the right, its location $\\lambda$ must be strictly greater than the right edge of the bulk spectrum, which is at $2$. So, we need to find the condition on $\\theta$ such that $\\lambda  2$.\n$$\n\\theta + \\frac{1}{\\theta}  2\n$$\nFor $\\theta  0$, we can multiply by $\\theta$ without changing the inequality direction:\n$$\n\\theta^2 + 1  2\\theta \\quad \\implies \\quad \\theta^2 - 2\\theta + 1  0 \\quad \\implies \\quad (\\theta - 1)^2  0\n$$\nThis inequality holds for all positive $\\theta$ except for $\\theta=1$. Therefore, an outlier exists on the right of the bulk if and only if $\\theta  1$. The critical value, $\\theta_c$, is the threshold at which this transition occurs. This corresponds to the case of equality, $(\\theta_c - 1)^2 = 0$, which yields $\\theta_c = 1$. At this critical value, the outlier is located at $\\lambda = 1 + 1/1 = 2$, precisely at the edge of the bulk spectrum. For any $\\theta  1$, the outlier $\\lambda = \\theta+1/\\theta$ is strictly greater than $2$.\n\nThe critical value of the signal strength $\\theta$ at which an outlier first emerges from the right edge of the bulk is $1$.",
            "answer": "$$\n\\boxed{1}\n$$"
        },
        {
            "introduction": "This final practice bridges theory and application, demonstrating how RMT principles inform state-of-the-art algorithms for community detection in sparse networks. We will explore the Bethe Hessian matrix, a powerful tool for spectral clustering whose effectiveness depends on a tunable parameter, $r$ . By leveraging the deep connection between the Bethe Hessian and the non-backtracking operator, you will determine the optimal choice of $r$ that cleanly separates the community signal from the spectral bulk, turning a theoretical insight into a practical recipe for network analysis.",
            "id": "4299522",
            "problem": "Consider a symmetric two-community Stochastic Block Model (SBM) on $n$ vertices with equal-sized communities. Edges are generated independently with probabilities $p_{\\mathrm{in}} = \\frac{\\bar{d}(1+\\epsilon)}{n}$ within communities and $p_{\\mathrm{out}} = \\frac{\\bar{d}(1-\\epsilon)}{n}$ across communities, where $0\\epsilon1$ and $\\bar{d}0$ is the target average degree held constant as $n \\to \\infty$. Let $A$ be the $n \\times n$ adjacency matrix and $D$ the $n \\times n$ diagonal degree matrix, and define the Bethe Hessian (BH) matrix by $H(r) = (r^{2} - 1) I - r A + D$ for a scalar parameter $r0$. The Non-Backtracking (NB) operator $B$ is the $2m \\times 2m$ matrix indexed by directed edges, where $m$ is the number of undirected edges, with entries indicating allowed non-backtracking transitions.\n\nYou may use the following well-tested facts as foundational starting points: (i) the Ihara–Bass identity links the characteristic polynomials of the NB operator and the BH construction via $\\det(I - u B) = (1 - u^{2})^{m-n} \\det\\!\\big(I - u A + u^{2}(D - I)\\big)$ for $u \\in \\mathbb{R}$, and (ii) in sparse, locally tree-like graphs with average degree $\\bar{d}$, the bulk of the NB spectrum concentrates asymptotically within the disk $|\\lambda| \\leq \\sqrt{\\bar{d}}$ in the complex plane.\n\nStarting from these bases and without introducing any shortcut formulas, determine the choice of the scalar $r$ (expressed in terms of $\\bar{d}$) that maximizes the spectral separation of community-informative directions from the random bulk when performing spectral clustering with $H(r)$. Specifically, choose $r$ so that, under the Ihara–Bass mapping, the zero crossing of the BH spectrum aligns with the edge of the NB bulk, thereby ensuring that any negative eigenvalues of $H(r)$ arise from the informative subspace and not the random bulk.\n\nExpress your final answer as a single closed-form expression in terms of $\\bar{d}$. No numerical rounding is required. No physical units are involved.",
            "solution": "The problem requires us to find the optimal value of the parameter $r$ for the Bethe Hessian matrix $H(r) = (r^{2} - 1) I - r A + D$. The optimality condition is that this choice of $r$ should maximize the spectral separation between community-informative eigenvectors and those corresponding to the random bulk. Specifically, the choice of $r$ must ensure that any negative eigenvalues of $H(r)$ arise exclusively from the informative subspace. This is to be achieved by aligning the zero-crossing point of the Bethe Hessian spectrum with the edge of the Non-Backtracking (NB) operator's spectral bulk.\n\nThe foundation of the solution lies in establishing a precise relationship between the spectrum of the Bethe Hessian $H(r)$ and the spectrum of the Non-Backtracking operator $B$. The problem provides the Ihara–Bass identity as a starting point:\n$$\n\\det(I - u B) = (1 - u^{2})^{m-n} \\det\\!\\big(I - u A + u^{2}(D - I)\\big)\n$$\nwhere $u \\in \\mathbb{R}$, $A$ is the adjacency matrix, $D$ is the diagonal degree matrix, $I$ is the identity matrix, and $m$ is the number of edges.\n\nLet $\\lambda$ be an eigenvalue of the NB operator $B$. The characteristic polynomial of $B$ evaluated at $\\lambda$ is zero, which implies $\\det(B - \\lambda I) = 0$. If we set $u = 1/\\lambda$ (assuming $\\lambda \\neq 0$), then $\\det(I - uB) = \\det(I - B/\\lambda) = (-\\lambda)^{-2m} \\det(B-\\lambda I) = 0$.\nFor the identity to hold and be non-trivial, we also assume $\\lambda \\neq \\pm 1$, so $u \\neq \\pm 1$. In this case, the Ihara-Bass identity implies that if $\\det(I - uB) = 0$, then we must also have:\n$$\n\\det\\!\\big(I - u A + u^{2}(D - I)\\big) = 0\n$$\nLet's analyze the matrix inside the determinant, which we denote as $M(u)$:\n$$\nM(u) = I - u A + u^{2}(D - I) = (1 - u^{2})I - u A + u^{2}D\n$$\nThe problem asks about the spectrum of the Bethe Hessian, $H(r) = (r^{2} - 1) I - r A + D$. We are particularly interested in the condition under which $H(r)$ has a zero eigenvalue, which is equivalent to $\\det(H(r)) = 0$.\n\nWe seek to find a relationship between $M(u)$ and $H(r)$. Let us see if $H(r)$ is proportional to $M(u)$ for some mapping between $r$ and $u$. Let $H(r) = c \\cdot M(u)$ for some scalar $c$.\n$$\n(r^{2} - 1) I - r A + D = c \\big( (1-u^2)I - uA + u^2 D \\big)\n$$\nBy comparing the matrix coefficients for $A$, $D$, and $I$, we can establish the relationship.\nComparing the coefficients of $D$: $1 = c u^{2}$, which implies $c = 1/u^{2}$ (for $u \\neq 0$).\nComparing the coefficients of $A$: $-r = -c u = -(1/u^{2})u = -1/u$. This gives $r = 1/u$, or equivalently, $u = 1/r$.\nNow, we must verify that the coefficients of $I$ are consistent with this mapping.\nThe coefficient of $I$ in $H(r)$ is $(r^2 - 1)$.\nThe coefficient of $I$ in $c \\cdot M(u)$ is $c(1-u^2) = (1/u^2)(1-u^2) = 1/u^2 - 1$.\nSubstituting $u=1/r$, this becomes $r^{2} - 1$. The coefficients match.\n\nThus, we have established the relationship:\n$$\n\\frac{1}{r^2} H(r) = M(1/r)\n$$\nThis implies that $\\det(H(r)) = 0$ is equivalent to $\\det(M(1/r)) = 0$ (for $r \\neq 0$).\nFrom the Ihara-Bass identity, $\\det(M(u))=0$ occurs when $u$ is the reciprocal of an eigenvalue of $B$. Therefore, $\\det(H(r))=0$ if and only if $r$ is an eigenvalue of the non-backtracking matrix $B$.\n\nThis result provides the \"zero crossing\" insight: an eigenvalue of $H(r)$ passes through zero precisely when the parameter $r$ is equal to the magnitude of an eigenvalue of $B$. Let $\\mu_j(r)$ be the $j$-th eigenvalue of $H(r)$. The function $\\mu_j(r)$ is a continuous function of $r$. For large $r$, $H(r) \\approx r^2 I$, so all its eigenvalues are positive. As $r$ is decreased, the eigenvalues $\\mu_j(r)$ also decrease. An eigenvalue $\\mu_j(r)$ will cross zero and become negative as $r$ is decreased past a value $|\\lambda_j|$, where $\\lambda_j$ is a corresponding eigenvalue of $B$. More formally, for a given choice of $r$, the sign of an eigenvalue $\\mu_j(r)$ of $H(r)$ depends on the comparison between $r$ and the magnitude of the corresponding eigenvalue $\\lambda_j$ of $B$. Specifically, $\\mu_j(r)$ is positive if $r  |\\lambda_j|$ and negative if $r  |\\lambda_j|$.\n\nThe problem states that for sparse, locally tree-like graphs with average degree $\\bar{d}$, the spectrum of $B$ has two parts:\n$1$. A \"bulk\" of eigenvalues, corresponding to random, non-informative eigenvectors, which are concentrated in the disk $|\\lambda| \\leq \\sqrt{\\bar{d}}$ in the complex plane.\n$2$. A few \"informative\" eigenvalues, lying outside this disk, i.e., $|\\lambda|  \\sqrt{\\bar{d}}$. These eigenvalues are associated with the community structure.\n\nThe objective is to choose $r$ such that any negative eigenvalues of $H(r)$ arise only from the informative subspace.\nLet's analyze the conditions on the eigenvalues of $H(r)$:\n- For an eigenvalue $\\mu_{bulk}(r)$ corresponding to a bulk mode of $B$ with eigenvalue $\\lambda_{bulk}$, we require $\\mu_{bulk}(r) \\geq 0$. This will be true if we choose $r \\geq |\\lambda_{bulk}|$. To satisfy this for all bulk modes, we must choose $r \\geq \\max_{\\text{bulk}}|\\lambda_{bulk}|$.\n- For an eigenvalue $\\mu_{inf}(r)$ corresponding to an informative mode of $B$ with eigenvalue $\\lambda_{inf}$, we require $\\mu_{inf}(r)  0$. This will be true if we choose $r  |\\lambda_{inf}|$. To satisfy this for all informative modes, we must choose $r  \\min_{\\text{inf}}|\\lambda_{inf}|$.\n\nCombining these, the parameter $r$ must lie in the interval:\n$$\n\\max_{\\text{bulk}}|\\lambda_{bulk}| \\leq r  \\min_{\\text{inf}}|\\lambda_{inf}|\n$$\nThis interval is the \"spectral window\" for separation. The problem states that the edge of the NB bulk is at $|\\lambda| = \\sqrt{\\bar{d}}$. This implies that $\\max_{\\text{bulk}}|\\lambda_{bulk}| = \\sqrt{\\bar{d}}$ and for the SBM community detection is possible when $\\min_{\\text{inf}}|\\lambda_{inf}|  \\sqrt{\\bar{d}}$.\nThe problem provides a specific instruction: \"choose $r$ so that, under the Ihara–Bass mapping, the zero crossing of the BH spectrum aligns with the edge of the NB bulk\". This means we should set $r$ to be equal to the radius of the spectral bulk.\nTherefore, the optimal choice for $r$ is:\n$$\nr = \\sqrt{\\bar{d}}\n$$\nWith this choice, for any bulk mode, $|\\lambda_{bulk}| \\leq \\sqrt{\\bar{d}} = r$, which ensures the corresponding eigenvalue $\\mu_{bulk}(r)$ is non-negative. For any informative mode, $|\\lambda_{inf}|  \\sqrt{\\bar{d}} = r$, which ensures the corresponding eigenvalue $\\mu_{inf}(r)$ is negative. This choice of $r$ optimally separates the informative eigenvectors from the bulk eigenvectors based on the sign of their corresponding eigenvalues in the Bethe Hessian matrix, accomplishing the stated goal.\nThe final answer is requested as a closed-form expression in terms of $\\bar{d}$.",
            "answer": "$$\\boxed{\\sqrt{\\bar{d}}}$$"
        }
    ]
}