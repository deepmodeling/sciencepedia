{
    "hands_on_practices": [
        {
            "introduction": "Random-walk based embeddings begin by translating a graph's complex topology into a simpler, linear format: sequences of nodes. This exercise demystifies this foundational step by guiding you through a manual simulation of the DeepWalk sampling process. By tracing a specific walk on a small path graph and generating the corresponding context pairs, you will gain a concrete understanding of how structural proximity in a network is converted into the co-occurrence data that an embedding model learns from.",
            "id": "4300089",
            "problem": "Consider the path graph $P_{5}$ with vertex set $\\{v_{1}, v_{2}, v_{3}, v_{4}, v_{5}\\}$ and undirected edges $\\{(v_{1}, v_{2}), (v_{2}, v_{3}), (v_{3}, v_{4}), (v_{4}, v_{5})\\}$. A simple random walk on $P_{5}$ is the Markov chain that, at each interior vertex $v_{i}$ with $i \\in \\{2,3,4\\}$, moves to either neighbor with equal probability, and at each endpoint $v_{1}$ or $v_{5}$ moves deterministically to the unique neighbor. In the DeepWalk sampling scheme, a walk of length $L$ is a sequence of $L$ vertices produced by this random walk, starting from an initial vertex and truncating after $L$ vertices have been recorded.\n\nYou will simulate one truncated walk of length $L=6$ starting from $v_{3}$ under the following deterministic coin outcomes for interior moves: at $v_{3}$ move to the higher-index neighbor $v_{4}$, at $v_{4}$ move to the higher-index neighbor $v_{5}$, at $v_{5}$ the move is forced to $v_{4}$, at $v_{4}$ move to the lower-index neighbor $v_{3}$, and at $v_{3}$ move to the lower-index neighbor $v_{2}$. This produces a single sequence of $L=6$ vertices. Using Skip-Gram style contexts with symmetric window size $w=2$, define the empirical co-occurrence multiset of ordered pairs as follows: for each position $i \\in \\{1,\\dots,L\\}$ in the sequence with center vertex $s_{i}$, include all ordered pairs $(s_{i}, s_{j})$ for indices $j$ satisfying $1 \\leq |j-i| \\leq w$ and $1 \\leq j \\leq L$. The empirical co-occurrence count $C(u,v)$ is the number of times the ordered pair $(u,v)$ appears in this multiset.\n\nTasks:\n- Write down the resulting walk sequence of $L=6$ vertices.\n- List all ordered context pairs generated by the symmetric window of size $w=2$.\n- Compute the complete empirical co-occurrence counts $C(u,v)$ for all ordered pairs $(u,v)$ that appear.\n- Finally, report the single empirical co-occurrence count $C(v_{4}, v_{4})$.\n\nExpress the final answer as a single real number. No rounding is required, and no physical units are involved.",
            "solution": "We begin from the definitions of a simple random walk on a graph, which is a time-homogeneous Markov chain with transition probabilities determined by the adjacency structure. On the path graph $P_{5}$, the transition rule is: at interior vertex $v_{i}$ for $i \\in \\{2,3,4\\}$, move to $v_{i-1}$ or $v_{i+1}$ with probability $\\frac{1}{2}$ each; at endpoint $v_{1}$ (respectively $v_{5}$), move to $v_{2}$ (respectively $v_{4}$) with probability $1$. A truncated walk of length $L$ is a finite sequence $(s_{1}, s_{2}, \\dots, s_{L})$ where $s_{1}$ is the start vertex and $s_{i+1}$ is the next vertex according to the transition rule, until $L$ vertices have been recorded.\n\nThe problem specifies deterministic moves to avoid stochastic ambiguity: start at $v_{3}$, then move to $v_{4}$, then move to $v_{5}$, then (forced at endpoint) move to $v_{4}$, then move to $v_{3}$, then move to $v_{2}$. Therefore, the walk sequence of $L=6$ vertices is\n$$\n(s_{1}, s_{2}, s_{3}, s_{4}, s_{5}, s_{6}) = (v_{3}, v_{4}, v_{5}, v_{4}, v_{3}, v_{2}).\n$$\n\nNext, we construct the Skip-Gram style context pairs with symmetric window size $w=2$. For each position $i \\in \\{1,\\dots,6\\}$, we include ordered pairs $(s_{i}, s_{j})$ for indices $j$ such that $1 \\leq |j-i| \\leq 2$ and $1 \\leq j \\leq 6$. We enumerate all such pairs by position.\n\n- For $i=1$, $s_{1}=v_{3}$. Valid $j$ are $j=2$ and $j=3$ (since $|j-1| \\in \\{1,2\\}$ within bounds). Pairs:\n  $$\n  (v_{3}, v_{4}),\\quad (v_{3}, v_{5}).\n  $$\n\n- For $i=2$, $s_{2}=v_{4}$. Valid $j$ are $j=1,3,4$. Pairs:\n  $$ \n  (v_{4}, v_{3}),\\quad (v_{4}, v_{5}),\\quad (v_{4}, v_{4}).\n  $$\n\n- For $i=3$, $s_{3}=v_{5}$. Valid $j$ are $j=1,2,4,5$. Pairs:\n  $$\n  (v_{5}, v_{3}),\\quad (v_{5}, v_{4}),\\quad (v_{5}, v_{4}),\\quad (v_{5}, v_{3}).\n  $$\n\n- For $i=4$, $s_{4}=v_{4}$. Valid $j$ are $j=2,3,5,6$. Pairs:\n  $$\n  (v_{4}, v_{4}),\\quad (v_{4}, v_{5}),\\quad (v_{4}, v_{3}),\\quad (v_{4}, v_{2}).\n  $$\n\n- For $i=5$, $s_{5}=v_{3}$. Valid $j$ are $j=3,4,6$. Pairs:\n  $$\n  (v_{3}, v_{5}),\\quad (v_{3}, v_{4}),\\quad (v_{3}, v_{2}).\n  $$\n\n- For $i=6$, $s_{6}=v_{2}$. Valid $j$ are $j=4,5$. Pairs:\n  $$\n  (v_{2}, v_{4}),\\quad (v_{2}, v_{3}).\n  $$\n\nCollecting all ordered pairs across $i=1$ to $i=6$, the multiset of context pairs is\n$$\n\\{(v_{3}, v_{4}), (v_{3}, v_{5}), (v_{4}, v_{3}), (v_{4}, v_{5}), (v_{4}, v_{4}), (v_{5}, v_{3}), (v_{5}, v_{4}), (v_{5}, v_{4}), (v_{5}, v_{3}), (v_{4}, v_{4}), (v_{4}, v_{5}), (v_{4}, v_{3}), (v_{4}, v_{2}), (v_{3}, v_{5}), (v_{3}, v_{4}), (v_{3}, v_{2}), (v_{2}, v_{4}), (v_{2}, v_{3})\\}.\n$$\n\nWe now compute the empirical co-occurrence counts $C(u,v)$ for every ordered pair $(u,v)$ that appears. Grouping identical pairs and counting multiplicities:\n\n- Center $v_{2}$:\n  $$\n  C(v_{2}, v_{4}) = 1,\\quad C(v_{2}, v_{3}) = 1.\n  $$\n\n- Center $v_{3}$:\n  $$\n  C(v_{3}, v_{4}) = 2,\\quad C(v_{3}, v_{5}) = 2,\\quad C(v_{3}, v_{2}) = 1.\n  $$\n\n- Center $v_{4}$:\n  $$\n  C(v_{4}, v_{3}) = 2,\\quad C(v_{4}, v_{5}) = 2,\\quad C(v_{4}, v_{4}) = 2,\\quad C(v_{4}, v_{2}) = 1.\n  $$\n\n- Center $v_{5}$:\n  $$\n  C(v_{5}, v_{3}) = 2,\\quad C(v_{5}, v_{4}) = 2.\n  $$\n\nAll other ordered pairs $(u,v)$ not listed have empirical count $0$ in this single walk.\n\nThe quantity requested for the final answer is $C(v_{4}, v_{4})$, the empirical co-occurrence count of the ordered pair $(v_{4}, v_{4})$. From the tally above, we have\n$$\nC(v_{4}, v_{4}) = 2.\n$$",
            "answer": "$$\\boxed{2}$$"
        },
        {
            "introduction": "While simple random walks capture local connectivity, more advanced methods like node2vec introduce biases to better distinguish between different structural roles. This practice moves beyond the uniform transitions of DeepWalk to explore the core mechanism of node2vec's second-order random walk. You will manually calculate the biased transition probabilities governed by the return parameter $p$ and the in-out parameter $q$, providing a tangible feel for how these hyperparameters allow an algorithm to interpolate between breadth-first and depth-first exploration of the graph.",
            "id": "4300077",
            "problem": "Consider an undirected, weighted graph with node set $V=\\{t,v,a,b,c,d\\}$ embedded in a larger network. You are at the current node $v$ and the previous node in the second-order random walk was $t$. The neighborhood of $v$ is $N(v)=\\{a,b,c,d\\}$. The shortest-path distances from $t$ to these neighbors are $d(t,a)=1$, $d(t,b)=1$, $d(t,c)=2$, and $d(t,d)=2$. Assume the graph uses symmetric degree-normalized edge weights defined by $w_{uv}=1/\\sqrt{\\deg(u)\\deg(v)}$ for any adjacent $u$ and $v$, where $\\deg(\\cdot)$ denotes node degree. The degrees are $\\deg(v)=6$, $\\deg(a)=3$, $\\deg(b)=4$, $\\deg(c)=2$, and $\\deg(d)=5$. The node2vec hyperparameters are $(p,q)=(3,\\tfrac{1}{2})$. Using the standard node2vec second-order random-walk rule that biases transitions according to the shortest-path distance from $t$ to a candidate next step, compute:\n- the unnormalized transition weights from $v$ to each neighbor in the order $(a,b,c,d)$, and\n- the corresponding normalized transition probabilities from $v$ to $(a,b,c,d)$.\nExpress all results exactly (no rounding). Provide the final answer as a single row matrix in the order $[w(a),w(b),w(c),w(d),P(a),P(b),P(c),P(d)]$, where $w(\\cdot)$ are the unnormalized weights and $P(\\cdot)$ the normalized probabilities.",
            "solution": "The problem statement has been validated and is deemed self-contained, scientifically grounded, and well-posed. All necessary parameters and definitions are provided to compute the requested quantities.\n\nThe problem requires the computation of unnormalized transition weights and normalized transition probabilities for a `node2vec` second-order random walk. The walk is currently at node $v$, having come from node $t$. The potential next nodes are the neighbors of $v$, given as $N(v) = \\{a, b, c, d\\}$.\n\nThe unnormalized transition weight, which we denote as $\\pi_{vx}$ for a transition from $v$ to a neighbor $x$, is the product of the `node2vec` search bias $\\alpha_{pq}(t,x)$ and the base edge weight $w_{vx}$.\n$$ \\pi_{vx} = \\alpha_{pq}(t,x) \\cdot w_{vx} $$\n\nThe search bias $\\alpha_{pq}(t,x)$ is determined by the shortest-path distance $d(t,x)$ between the previous node $t$ and the candidate next node $x$. Given the hyperparameters $p=3$ and $q=\\frac{1}{2}$, the rules are:\n$$\n\\alpha_{pq}(t,x) =\n\\begin{cases}\n\\frac{1}{p} = \\frac{1}{3}  \\text{if } d(t,x) = 0 \\\\\n1  \\text{if } d(t,x) = 1 \\\\\n\\frac{1}{q} = 2  \\text{if } d(t,x) = 2\n\\end{cases}\n$$\n\nThe base edge weight $w_{uv}$ between any two adjacent nodes $u$ and $v$ is given as a symmetric degree-normalized weight:\n$$ w_{uv} = \\frac{1}{\\sqrt{\\deg(u)\\deg(v)}} $$\n\nThe given data includes:\n- Degrees of relevant nodes: $\\deg(v)=6$, $\\deg(a)=3$, $\\deg(b)=4$, $\\deg(c)=2$, $\\deg(d)=5$.\n- Shortest-path distances from $t$: $d(t,a)=1$, $d(t,b)=1$, $d(t,c)=2$, $d(t,d)=2$.\n\nWe will now compute the unnormalized transition weight $\\pi_{vx}$ for each neighbor $x \\in \\{a, b, c, d\\}$.\n\n1.  **For neighbor $a$**:\n    The distance is $d(t,a)=1$, so the bias factor is $\\alpha_{pq}(t,a)=1$.\n    The base edge weight is $w_{va} = \\frac{1}{\\sqrt{\\deg(v)\\deg(a)}} = \\frac{1}{\\sqrt{6 \\cdot 3}} = \\frac{1}{\\sqrt{18}} = \\frac{1}{3\\sqrt{2}}$.\n    Rationalizing the denominator gives $w_{va} = \\frac{\\sqrt{2}}{6}$.\n    The unnormalized transition weight is $\\pi_{va} = 1 \\cdot \\frac{\\sqrt{2}}{6} = \\frac{\\sqrt{2}}{6}$.\n\n2.  **For neighbor $b$**:\n    The distance is $d(t,b)=1$, so the bias factor is $\\alpha_{pq}(t,b)=1$.\n    The base edge weight is $w_{vb} = \\frac{1}{\\sqrt{\\deg(v)\\deg(b)}} = \\frac{1}{\\sqrt{6 \\cdot 4}} = \\frac{1}{\\sqrt{24}} = \\frac{1}{2\\sqrt{6}}$.\n    Rationalizing the denominator gives $w_{vb} = \\frac{\\sqrt{6}}{12}$.\n    The unnormalized transition weight is $\\pi_{vb} = 1 \\cdot \\frac{\\sqrt{6}}{12} = \\frac{\\sqrt{6}}{12}$.\n\n3.  **For neighbor $c$**:\n    The distance is $d(t,c)=2$, so the bias factor is $\\alpha_{pq}(t,c)=\\frac{1}{q} = 2$.\n    The base edge weight is $w_{vc} = \\frac{1}{\\sqrt{\\deg(v)\\deg(c)}} = \\frac{1}{\\sqrt{6 \\cdot 2}} = \\frac{1}{\\sqrt{12}} = \\frac{1}{2\\sqrt{3}}$.\n    Rationalizing the denominator gives $w_{vc} = \\frac{\\sqrt{3}}{6}$.\n    The unnormalized transition weight is $\\pi_{vc} = 2 \\cdot \\frac{\\sqrt{3}}{6} = \\frac{\\sqrt{3}}{3}$.\n\n4.  **For neighbor $d$**:\n    The distance is $d(t,d)=2$, so the bias factor is $\\alpha_{pq}(t,d)=\\frac{1}{q} = 2$.\n    The base edge weight is $w_{vd} = \\frac{1}{\\sqrt{\\deg(v)\\deg(d)}} = \\frac{1}{\\sqrt{6 \\cdot 5}} = \\frac{1}{\\sqrt{30}}$.\n    Rationalizing the denominator gives $w_{vd} = \\frac{\\sqrt{30}}{30}$.\n    The unnormalized transition weight is $\\pi_{vd} = 2 \\cdot \\frac{\\sqrt{30}}{30} = \\frac{\\sqrt{30}}{15}$.\n\nThe unnormalized transition weights are:\n$w(a) = \\frac{\\sqrt{2}}{6}$, $w(b) = \\frac{\\sqrt{6}}{12}$, $w(c) = \\frac{\\sqrt{3}}{3}$, $w(d) = \\frac{\\sqrt{30}}{15}$.\n\nNext, we calculate the normalized transition probabilities. The probability of transitioning to a neighbor $x$ is given by $P(v,x) = \\frac{\\pi_{vx}}{Z}$, where $Z$ is the normalization constant, i.e., the sum of all unnormalized weights for transitions from $v$.\n$$ Z = \\sum_{y \\in N(v)} \\pi_{vy} = \\pi_{va} + \\pi_{vb} + \\pi_{vc} + \\pi_{vd} $$\n$$ Z = \\frac{\\sqrt{2}}{6} + \\frac{\\sqrt{6}}{12} + \\frac{\\sqrt{3}}{3} + \\frac{\\sqrt{30}}{15} $$\nTo sum these fractions, we find a common denominator, which is $\\text{lcm}(6, 12, 3, 15) = 60$.\n$$ Z = \\frac{10\\sqrt{2}}{60} + \\frac{5\\sqrt{6}}{60} + \\frac{20\\sqrt{3}}{60} + \\frac{4\\sqrt{30}}{60} = \\frac{10\\sqrt{2} + 5\\sqrt{6} + 20\\sqrt{3} + 4\\sqrt{30}}{60} $$\n\nNow, we compute the normalized probabilities:\n1.  **Probability to $a$**:\n    $P(a) = \\frac{\\pi_{va}}{Z} = \\frac{\\sqrt{2}/6}{Z} = \\frac{\\sqrt{2}}{6} \\cdot \\frac{60}{10\\sqrt{2} + 5\\sqrt{6} + 20\\sqrt{3} + 4\\sqrt{30}} = \\frac{10\\sqrt{2}}{10\\sqrt{2} + 5\\sqrt{6} + 20\\sqrt{3} + 4\\sqrt{30}}$.\n\n2.  **Probability to $b$**:\n    $P(b) = \\frac{\\pi_{vb}}{Z} = \\frac{\\sqrt{6}/12}{Z} = \\frac{\\sqrt{6}}{12} \\cdot \\frac{60}{10\\sqrt{2} + 5\\sqrt{6} + 20\\sqrt{3} + 4\\sqrt{30}} = \\frac{5\\sqrt{6}}{10\\sqrt{2} + 5\\sqrt{6} + 20\\sqrt{3} + 4\\sqrt{30}}$.\n\n3.  **Probability to $c$**:\n    $P(c) = \\frac{\\pi_{vc}}{Z} = \\frac{\\sqrt{3}/3}{Z} = \\frac{\\sqrt{3}}{3} \\cdot \\frac{60}{10\\sqrt{2} + 5\\sqrt{6} + 20\\sqrt{3} + 4\\sqrt{30}} = \\frac{20\\sqrt{3}}{10\\sqrt{2} + 5\\sqrt{6} + 20\\sqrt{3} + 4\\sqrt{30}}$.\n\n4.  **Probability to $d$**:\n    $P(d) = \\frac{\\pi_{vd}}{Z} = \\frac{\\sqrt{30}/15}{Z} = \\frac{\\sqrt{30}}{15} \\cdot \\frac{60}{10\\sqrt{2} + 5\\sqrt{6} + 20\\sqrt{3} + 4\\sqrt{30}} = \\frac{4\\sqrt{30}}{10\\sqrt{2} + 5\\sqrt{6} + 20\\sqrt{3} + 4\\sqrt{30}}$.\n\nThe final result is the ordered set of unnormalized weights and normalized probabilities.",
            "answer": "$$ \\boxed{\\begin{pmatrix} \\frac{\\sqrt{2}}{6}  \\frac{\\sqrt{6}}{12}  \\frac{\\sqrt{3}}{3}  \\frac{\\sqrt{30}}{15}  \\frac{10\\sqrt{2}}{10\\sqrt{2} + 5\\sqrt{6} + 20\\sqrt{3} + 4\\sqrt{30}}  \\frac{5\\sqrt{6}}{10\\sqrt{2} + 5\\sqrt{6} + 20\\sqrt{3} + 4\\sqrt{30}}  \\frac{20\\sqrt{3}}{10\\sqrt{2} + 5\\sqrt{6} + 20\\sqrt{3} + 4\\sqrt{30}}  \\frac{4\\sqrt{30}}{10\\sqrt{2} + 5\\sqrt{6} + 20\\sqrt{3} + 4\\sqrt{30}} \\end{pmatrix}} $$"
        },
        {
            "introduction": "Once a corpus of node co-occurrences is generated, how are the vector representations actually learned? This final practice looks under the hood of the Skip-Gram with Negative Sampling (SGNS) objective, the learning model used by both DeepWalk and node2vec. By deriving the gradient of the loss function, you will uncover the mathematical engine driving the embedding process, revealing precisely how the model learns to pull embeddings of co-occurring nodes together while pushing them apart from non-co-occurring nodes.",
            "id": "4300053",
            "problem": "Consider a connected, undirected graph $G = (V, E)$ with $|V| = n$ nodes. A corpus of node sequences is generated by truncated random walks of fixed length $L$ and a symmetric context window of size $w$, producing training events where a central node $u \\in V$ is paired with a context node $c \\in V$ observed within the window along the walk. Assume the Skip-Gram with Negative Sampling (SGNS) objective is used to learn two embedding functions: a source embedding $\\mathbf{u} \\in \\mathbb{R}^{d}$ for the central node and a context embedding $\\mathbf{c} \\in \\mathbb{R}^{d}$ for the context node. For each observed positive pair $(u, c)$, draw $k$ independent negative context nodes $\\{n_{i}\\}_{i=1}^{k}$ from a fixed noise distribution $Q$ over $V$, and denote their context embeddings by $\\{\\mathbf{n}_{i}\\}_{i=1}^{k} \\subset \\mathbb{R}^{d}$. The SGNS loss for this single training event is\n$$\n\\ell(\\mathbf{u}, \\mathbf{c}, \\{\\mathbf{n}_{i}\\}_{i=1}^{k}) \\;=\\; -\\,\\ln\\!\\big(\\sigma(\\mathbf{u}^{\\top}\\mathbf{c})\\big)\\;-\\;\\sum_{i=1}^{k}\\ln\\!\\big(\\sigma(-\\,\\mathbf{u}^{\\top}\\mathbf{n}_{i})\\big),\n$$\nwhere $\\sigma(x) = \\frac{1}{1+\\exp(-x)}$ is the logistic sigmoid, and $\\exp(\\cdot)$ denotes the exponential function. Starting from first principles of binary logistic cross-entropy and using only fundamental calculus rules, derive the gradient of the loss with respect to the context embedding $\\mathbf{c}$. Express your final result as a closed-form symbolic expression in terms of $\\mathbf{u}$, $\\mathbf{c}$, and $\\sigma(\\cdot)$. Then, based on the derived expression and the structure of the SGNS loss, interpret how the sigmoid terms mediate attraction between the embeddings for positive pairs and repulsion relative to negative samples.\n\nYour final answer must be the analytic expression for $\\nabla_{\\mathbf{c}}\\ell$. No numerical evaluation is required.",
            "solution": "We begin by recalling the SGNS loss for a single training event with one positive pair $(u, c)$ and $k$ negative samples $\\{n_{i}\\}_{i=1}^{k}$:\n$$\n\\ell(\\mathbf{u}, \\mathbf{c}, \\{\\mathbf{n}_{i}\\}_{i=1}^{k}) \\;=\\; -\\,\\ln\\!\\big(\\sigma(\\mathbf{u}^{\\top}\\mathbf{c})\\big)\\;-\\;\\sum_{i=1}^{k}\\ln\\!\\big(\\sigma(-\\,\\mathbf{u}^{\\top}\\mathbf{n}_{i})\\big).\n$$\nWe will derive $\\nabla_{\\mathbf{c}} \\ell$ from first principles, using the chain rule and the derivative of the logistic sigmoid. Let $s = \\mathbf{u}^{\\top}\\mathbf{c}$. The first term depends on $\\mathbf{c}$ through $s$, while the second term depends only on $\\mathbf{u}$ and the negative context embeddings $\\{\\mathbf{n}_{i}\\}_{i=1}^{k}$; therefore, its gradient with respect to $\\mathbf{c}$ is zero.\n\nWe recall the logistic sigmoid and its derivative:\n$$\n\\sigma(x) \\;=\\; \\frac{1}{1+\\exp(-x)},\\qquad \\frac{d}{dx}\\sigma(x) \\;=\\; \\sigma(x)\\big(1 - \\sigma(x)\\big).\n$$\nThe derivative of the logarithm of the sigmoid is obtained via the chain rule:\n$$\n\\frac{d}{dx}\\ln\\!\\big(\\sigma(x)\\big) \\;=\\; \\frac{1}{\\sigma(x)}\\,\\sigma'(x) \\;=\\; \\frac{1}{\\sigma(x)}\\,\\sigma(x)\\big(1 - \\sigma(x)\\big) \\;=\\; 1 - \\sigma(x).\n$$\nWe can now compute the gradient of the first term with respect to $\\mathbf{c}$:\n$$\n\\nabla_{\\mathbf{c}}\\Big(-\\,\\ln\\!\\big(\\sigma(\\mathbf{u}^{\\top}\\mathbf{c})\\big)\\Big)\n\\;=\\;\n-\\,\\Big(1 - \\sigma(\\mathbf{u}^{\\top}\\mathbf{c})\\Big)\\,\\nabla_{\\mathbf{c}}\\!\\big(\\mathbf{u}^{\\top}\\mathbf{c}\\big)\n\\;=\\;\n-\\,\\Big(1 - \\sigma(\\mathbf{u}^{\\top}\\mathbf{c})\\Big)\\,\\mathbf{u}.\n$$\nBecause the negative-sample term does not depend on $\\mathbf{c}$, its gradient with respect to $\\mathbf{c}$ is zero:\n$$\n\\nabla_{\\mathbf{c}}\\Big(-\\,\\sum_{i=1}^{k}\\ln\\!\\big(\\sigma(-\\,\\mathbf{u}^{\\top}\\mathbf{n}_{i})\\big)\\Big) \\;=\\; \\mathbf{0}.\n$$\nThus, the total gradient with respect to $\\mathbf{c}$ is\n$$\n\\nabla_{\\mathbf{c}}\\ell \\;=\\; -\\,\\Big(1 - \\sigma(\\mathbf{u}^{\\top}\\mathbf{c})\\Big)\\,\\mathbf{u}\n\\;=\\;\n\\Big(\\sigma(\\mathbf{u}^{\\top}\\mathbf{c}) - 1\\Big)\\,\\mathbf{u}.\n$$\n\nInterpretation of attraction and repulsion mediated by sigmoid terms: The coefficient $\\sigma(\\mathbf{u}^{\\top}\\mathbf{c}) - 1$ multiplies the direction $\\mathbf{u}$. When $\\mathbf{u}^{\\top}\\mathbf{c}$ is small, $\\sigma(\\mathbf{u}^{\\top}\\mathbf{c})$ is close to $0$, so $\\sigma(\\mathbf{u}^{\\top}\\mathbf{c}) - 1$ is negative, and gradient descent updates move $\\mathbf{c}$ in the direction of $+\\mathbf{u}$ (because the update is $\\mathbf{c} \\leftarrow \\mathbf{c} - \\eta \\nabla_{\\mathbf{c}}\\ell$, with $\\eta  0$), increasing $\\mathbf{u}^{\\top}\\mathbf{c}$ and thereby producing attraction between the positive pair $(u, c)$. As $\\mathbf{u}^{\\top}\\mathbf{c}$ grows and $\\sigma(\\mathbf{u}^{\\top}\\mathbf{c})$ approaches $1$, the coefficient $\\sigma(\\mathbf{u}^{\\top}\\mathbf{c}) - 1$ tends to $0$, saturating the attraction.\n\nRepulsion relative to negative samples appears in the gradients of parameters that directly interact with negatives. For instance, for a single negative sample $\\mathbf{n}$ with score $t = \\mathbf{u}^{\\top}\\mathbf{n}$, the derivative\n$$\n\\nabla_{\\mathbf{n}}\\Big(-\\ln\\big(\\sigma(-t)\\big)\\Big) \\;=\\; \\Big(1 - \\sigma(-t)\\Big)\\,\\mathbf{u}\n\\;=\\; \\sigma(t)\\,\\mathbf{u}\n$$\nhas a positive coefficient $\\sigma(t)$ on $\\mathbf{u}$, so gradient descent updates on $\\mathbf{n}$ move it in the direction $-\\mathbf{u}$, decreasing $t$ and producing repulsion from $\\mathbf{u}$. Likewise, the gradient with respect to $\\mathbf{u}$ includes both the attractive term $(\\sigma(\\mathbf{u}^{\\top}\\mathbf{c}) - 1)\\mathbf{c}$ and repulsive terms $\\sum_{i=1}^{k}\\sigma(\\mathbf{u}^{\\top}\\mathbf{n}_{i})\\mathbf{n}_{i}$, balancing attraction to positives with repulsion from negatives. In contrast, for the context embedding $\\mathbf{c}$ of the positive pair considered here, only the attractive sigmoid-mediated term contributes.",
            "answer": "$$\\boxed{\\nabla_{\\mathbf{c}}\\ell \\;=\\; \\big(\\sigma(\\mathbf{u}^{\\top}\\mathbf{c}) - 1\\big)\\,\\mathbf{u}}$$"
        }
    ]
}