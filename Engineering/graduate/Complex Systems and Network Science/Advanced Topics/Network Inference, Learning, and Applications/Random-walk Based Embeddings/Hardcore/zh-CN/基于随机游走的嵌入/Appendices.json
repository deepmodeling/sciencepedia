{
    "hands_on_practices": [
        {
            "introduction": "基于随机游走的嵌入方法，其核心的第一步是将图的拓扑结构转化为线性的节点序列。本练习通过一个具体实例，让你亲手模拟这一过程，在一个简单的图上生成一段随机游走序列，并提取出用于模型训练的上下文共现对。通过这个练习，你将深入理解 DeepWalk 等算法如何从图结构中采样，为后续的嵌入学习准备原始数据。",
            "id": "4300089",
            "problem": "考虑路径图 $P_{5}$，其顶点集为 $\\{v_{1}, v_{2}, v_{3}, v_{4}, v_{5}\\}$，无向边集为 $\\{(v_{1}, v_{2}), (v_{2}, v_{3}), (v_{3}, v_{4}), (v_{4}, v_{5})\\}$。在 $P_{5}$ 上的简单随机游走是一个马尔可夫链，在每个内部顶点 $v_{i}$ (其中 $i \\in \\{2,3,4\\}$) 处，它以相等的概率移动到任一邻居；在每个端点 $v_{1}$ 或 $v_{5}$ 处，它确定性地移动到唯一的邻居。在 DeepWalk 采样方案中，长度为 $L$ 的游走是由此随机游走生成的一个包含 $L$ 个顶点的序列，从一个初始顶点开始，在记录了 $L$ 个顶点后截断。\n\n您将模拟一次从 $v_{3}$ 开始的长度为 $L=6$ 的截断游走，内部移动遵循以下确定性的“掷硬币”结果：在 $v_{3}$ 处，移动到索引较大的邻居 $v_{4}$；在 $v_{4}$ 处，移动到索引较大的邻居 $v_{5}$；在 $v_{5}$ 处，移动被强制到 $v_{4}$；在 $v_{4}$ 处，移动到索引较小的邻居 $v_{3}$；在 $v_{3}$ 处，移动到索引较小的邻居 $v_{2}$。这将产生一个长度为 $L=6$ 的顶点序列。使用对称窗口大小为 $w=2$ 的 Skip-Gram 风格上下文，按如下方式定义有序对的经验共现多重集：对于序列中每个位置 $i \\in \\{1,\\dots,L\\}$（中心顶点为 $s_{i}$），包含所有满足索引 $j$ 条件 $1 \\leq |j-i| \\leq w$ 且 $1 \\leq j \\leq L$ 的有序对 $(s_{i}, s_{j})$。经验共现计数 $C(u,v)$ 是有序对 $(u,v)$ 在此多重集中出现的次数。\n\n任务：\n- 写下生成的长度为 $L=6$ 的顶点游走序列。\n- 列出由大小为 $w=2$ 的对称窗口生成的所有有序上下文对。\n- 计算所有出现的有序对 $(u,v)$ 的完整经验共现计数 $C(u,v)$。\n- 最后，报告单个经验共现计数 $C(v_{4}, v_{4})$。\n\n将最终答案表示为单个实数。不需要四舍五入，也不涉及物理单位。",
            "solution": "我们从图上简单随机游走的定义开始，这是一个时间齐次的马尔可夫链，其转移概率由邻接结构决定。在路径图 $P_{5}$ 上，转移规则是：在内部顶点 $v_{i}$（其中 $i \\in \\{2,3,4\\}$）处，以各 $\\frac{1}{2}$ 的概率移动到 $v_{i-1}$ 或 $v_{i+1}$；在端点 $v_{1}$（或 $v_{5}$）处，以概率 $1$ 移动到 $v_{2}$（或 $v_{4}$）。长度为 $L$ 的截断游走是一个有限序列 $(s_{1}, s_{2}, \\dots, s_{L})$，其中 $s_{1}$ 是起始顶点，$s_{i+1}$ 是根据转移规则的下一个顶点，直到记录了 $L$ 个顶点为止。\n\n问题指定了确定性的移动以避免随机性歧义：从 $v_{3}$ 开始，然后移动到 $v_{4}$，再移动到 $v_{5}$，然后（在端点处被强制）移动到 $v_{4}$，再移动到 $v_{3}$，最后移动到 $v_{2}$。因此，长度为 $L=6$ 的顶点游走序列是\n$$\n(s_{1}, s_{2}, s_{3}, s_{4}, s_{5}, s_{6}) = (v_{3}, v_{4}, v_{5}, v_{4}, v_{3}, v_{2}).\n$$\n\n接下来，我们构造对称窗口大小为 $w=2$ 的 Skip-Gram 风格上下文对。对于每个位置 $i \\in \\{1,\\dots,6\\}$，我们包含所有满足索引 $j$ 条件 $1 \\leq |j-i| \\leq 2$ 且 $1 \\leq j \\leq 6$ 的有序对 $(s_{i}, s_{j})$。我们按位置枚举所有这样的对。\n\n- 对于 $i=1$，$s_{1}=v_{3}$。有效的 $j$ 是 $j=2$ 和 $j=3$（因为 $|j-1| \\in \\{1,2\\}$ 在界限内）。有序对：\n  $$\n  (v_{3}, v_{4}),\\quad (v_{3}, v_{5}).\n  $$\n\n- 对于 $i=2$，$s_{2}=v_{4}$。有效的 $j$ 是 $j=1,3,4$。有序对：\n  $$ \n  (v_{4}, v_{3}),\\quad (v_{4}, v_{5}),\\quad (v_{4}, v_{4}).\n  $$\n\n- 对于 $i=3$，$s_{3}=v_{5}$。有效的 $j$ 是 $j=1,2,4,5$。有序对：\n  $$\n  (v_{5}, v_{3}),\\quad (v_{5}, v_{4}),\\quad (v_{5}, v_{4}),\\quad (v_{5}, v_{3}).\n  $$\n\n- 对于 $i=4$，$s_{4}=v_{4}$。有效的 $j$ 是 $j=2,3,5,6$。有序对：\n  $$\n  (v_{4}, v_{4}),\\quad (v_{4}, v_{5}),\\quad (v_{4}, v_{3}),\\quad (v_{4}, v_{2}).\n  $$\n\n- 对于 $i=5$，$s_{5}=v_{3}$。有效的 $j$ 是 $j=3,4,6$。有序对：\n  $$\n  (v_{3}, v_{5}),\\quad (v_{3}, v_{4}),\\quad (v_{3}, v_{2}).\n  $$\n\n- 对于 $i=6$，$s_{6}=v_{2}$。有效的 $j$ 是 $j=4,5$。有序对：\n  $$\n  (v_{2}, v_{4}),\\quad (v_{2}, v_{3}).\n  $$\n\n收集从 $i=1$ 到 $i=6$ 的所有有序对，上下文对的多重集是\n$$\n\\{(v_{3}, v_{4}), (v_{3}, v_{5}), (v_{4}, v_{3}), (v_{4}, v_{5}), (v_{4}, v_{4}), (v_{5}, v_{3}), (v_{5}, v_{4}), (v_{5}, v_{4}), (v_{5}, v_{3}), (v_{4}, v_{4}), (v_{4}, v_{5}), (v_{4}, v_{3}), (v_{4}, v_{2}), (v_{3}, v_{5}), (v_{3}, v_{4}), (v_{3}, v_{2}), (v_{2}, v_{4}), (v_{2}, v_{3})\\}.\n$$\n\n我们现在计算每个出现的有序对 $(u,v)$ 的经验共现计数 $C(u,v)$。将相同的对分组并计算其重数：\n\n- 中心 $v_{2}$：\n  $$\n  C(v_{2}, v_{4}) = 1,\\quad C(v_{2}, v_{3}) = 1.\n  $$\n\n- 中心 $v_{3}$：\n  $$\n  C(v_{3}, v_{4}) = 2,\\quad C(v_{3}, v_{5}) = 2,\\quad C(v_{3}, v_{2}) = 1.\n  $$\n\n- 中心 $v_{4}$：\n  $$\n  C(v_{4}, v_{3}) = 2,\\quad C(v_{4}, v_{5}) = 2,\\quad C(v_{4}, v_{4}) = 2,\\quad C(v_{4}, v_{2}) = 1.\n  $$\n\n- 中心 $v_{5}$：\n  $$\n  C(v_{5}, v_{3}) = 2,\\quad C(v_{5}, v_{4}) = 2.\n  $$\n\n在此次单次游走中，所有未列出的其他有序对 $(u,v)$ 的经验计数均为 $0$。\n\n最终答案所要求的量是 $C(v_{4}, v_{4})$，即有序对 $(v_{4}, v_{4})$ 的经验共现计数。根据上面的统计，我们得到\n$$\nC(v_{4}, v_{4}) = 2.\n$$",
            "answer": "$$\\boxed{2}$$"
        },
        {
            "introduction": "在 DeepWalk 的基础上，node2vec 通过引入有偏随机游走，实现了对网络探索方式更精细的控制。本练习将带你深入 node2vec 算法的核心机制，亲手计算在给定的超参数 $p$ 和 $q$ 的影响下，游走在节点间的转移概率。完成此练习后，你将清晰地理解 node2vec 是如何通过调节参数来平衡不同探索策略，从而捕捉到更丰富的网络结构信息。",
            "id": "4300077",
            "problem": "考虑一个嵌入在更大网络中的无向加权图，其节点集为 $V=\\{t,v,a,b,c,d\\}$。在二阶随机游走中，您当前位于节点 $v$，前一个节点是 $t$。$v$ 的邻域是 $N(v)=\\{a,b,c,d\\}$。从 $t$ 到这些邻居的最短路径距离为 $d(t,a)=1$、$d(t,b)=1$、$d(t,c)=2$ 和 $d(t,d)=2$。假设该图使用对称的度归一化边权重，定义为 $w_{uv}=1/\\sqrt{\\deg(u)\\deg(v)}$，适用于任何相邻的 $u$ 和 $v$，其中 $\\deg(\\cdot)$ 表示节点度。各节点的度为 $\\deg(v)=6$、$\\deg(a)=3$、$\\deg(b)=4$、$\\deg(c)=2$ 和 $\\deg(d)=5$。node2vec 的超参数为 $(p,q)=(3,\\tfrac{1}{2})$。使用标准的 node2vec 二阶随机游走规则，该规则根据从 $t$ 到候选下一步的最短路径距离来偏置转移，请计算：\n- 从 $v$到每个邻居（按 $(a,b,c,d)$ 顺序）的未归一化转移权重，以及\n- 从 $v$到 $(a,b,c,d)$ 的相应归一化转移概率。\n所有结果均需精确表示（不进行四舍五入）。请以单行矩阵的形式提供最终答案，顺序为 $[w(a),w(b),w(c),w(d),P(a),P(b),P(c),P(d)]$，其中 $w(\\cdot)$ 是未归一化的权重，$P(\\cdot)$ 是归一化的概率。",
            "solution": "问题陈述已经过验证，被认为是自洽的、有科学依据且定义明确的。所有计算所需量的必要参数和定义均已提供。\n\n该问题要求计算 `node2vec` 二阶随机游走的未归一化转移权重和归一化转移概率。游走当前位于节点 $v$，前一个节点是 $t$。潜在的下一个节点是 $v$ 的邻居，即 $N(v) = \\{a, b, c, d\\}$。\n\n我们用 $\\pi_{vx}$ 表示从 $v$ 到邻居 $x$ 的未归一化转移权重，它是 `node2vec` 搜索偏置 $\\alpha_{pq}(t,x)$ 和基础边权重 $w_{vx}$ 的乘积。\n$$ \\pi_{vx} = \\alpha_{pq}(t,x) \\cdot w_{vx} $$\n\n搜索偏置 $\\alpha_{pq}(t,x)$ 由前一个节点 $t$ 和候选下一个节点 $x$ 之间的最短路径距离 $d(t,x)$ 决定。给定超参数 $p=3$ 和 $q=\\frac{1}{2}$，规则如下：\n$$\n\\alpha_{pq}(t,x) =\n\\begin{cases}\n\\frac{1}{p} = \\frac{1}{3}  \\text{若 } d(t,x) = 0 \\\\\n1  \\text{若 } d(t,x) = 1 \\\\\n\\frac{1}{q} = 2  \\text{若 } d(t,x) = 2\n\\end{cases}\n$$\n\n任意两个相邻节点 $u$ 和 $v$ 之间的基础边权重 $w_{uv}$ 定义为对称的度归一化权重：\n$$ w_{uv} = \\frac{1}{\\sqrt{\\deg(u)\\deg(v)}} $$\n\n给定的数据包括：\n- 相关节点的度：$\\deg(v)=6$、$\\deg(a)=3$、$\\deg(b)=4$、$\\deg(c)=2$、$\\deg(d)=5$。\n- 从 $t$ 出发的最短路径距离：$d(t,a)=1$、$d(t,b)=1$、$d(t,c)=2$、$d(t,d)=2$。\n\n我们现在为每个邻居 $x \\in \\{a, b, c, d\\}$ 计算未归一化的转移权重 $\\pi_{vx}$。\n\n1.  **对于邻居 $a$**：\n    距离为 $d(t,a)=1$，因此偏置因子为 $\\alpha_{pq}(t,a)=1$。\n    基础边权重为 $w_{va} = \\frac{1}{\\sqrt{\\deg(v)\\deg(a)}} = \\frac{1}{\\sqrt{6 \\cdot 3}} = \\frac{1}{\\sqrt{18}} = \\frac{1}{3\\sqrt{2}}$。\n    分母有理化后得到 $w_{va} = \\frac{\\sqrt{2}}{6}$。\n    未归一化的转移权重为 $\\pi_{va} = 1 \\cdot \\frac{\\sqrt{2}}{6} = \\frac{\\sqrt{2}}{6}$。\n\n2.  **对于邻居 $b$**：\n    距离为 $d(t,b)=1$，因此偏置因子为 $\\alpha_{pq}(t,b)=1$。\n    基础边权重为 $w_{vb} = \\frac{1}{\\sqrt{\\deg(v)\\deg(b)}} = \\frac{1}{\\sqrt{6 \\cdot 4}} = \\frac{1}{\\sqrt{24}} = \\frac{1}{2\\sqrt{6}}$。\n    分母有理化后得到 $w_{vb} = \\frac{\\sqrt{6}}{12}$。\n    未归一化的转移权重为 $\\pi_{vb} = 1 \\cdot \\frac{\\sqrt{6}}{12} = \\frac{\\sqrt{6}}{12}$。\n\n3.  **对于邻居 $c$**：\n    距离为 $d(t,c)=2$，因此偏置因子为 $\\alpha_{pq}(t,c)=\\frac{1}{q} = 2$。\n    基础边权重为 $w_{vc} = \\frac{1}{\\sqrt{\\deg(v)\\deg(c)}} = \\frac{1}{\\sqrt{6 \\cdot 2}} = \\frac{1}{\\sqrt{12}} = \\frac{1}{2\\sqrt{3}}$。\n    分母有理化后得到 $w_{vc} = \\frac{\\sqrt{3}}{6}$。\n    未归一化的转移权重为 $\\pi_{vc} = 2 \\cdot \\frac{\\sqrt{3}}{6} = \\frac{\\sqrt{3}}{3}$。\n\n4.  **对于邻居 $d$**：\n    距离为 $d(t,d)=2$，因此偏置因子为 $\\alpha_{pq}(t,d)=\\frac{1}{q} = 2$。\n    基础边权重为 $w_{vd} = \\frac{1}{\\sqrt{\\deg(v)\\deg(d)}} = \\frac{1}{\\sqrt{6 \\cdot 5}} = \\frac{1}{\\sqrt{30}}$。\n    分母有理化后得到 $w_{vd} = \\frac{\\sqrt{30}}{30}$。\n    未归一化的转移权重为 $\\pi_{vd} = 2 \\cdot \\frac{\\sqrt{30}}{30} = \\frac{\\sqrt{30}}{15}$。\n\n未归一化的转移权重为：\n$w(a) = \\frac{\\sqrt{2}}{6}$、$w(b) = \\frac{\\sqrt{6}}{12}$、$w(c) = \\frac{\\sqrt{3}}{3}$、$w(d) = \\frac{\\sqrt{30}}{15}$。\n\n接下来，我们计算归一化的转移概率。转移到邻居 $x$ 的概率由 $P(v,x) = \\frac{\\pi_{vx}}{Z}$ 给出，其中 $Z$ 是归一化常数，即从 $v$ 出发的所有未归一化权重的总和。\n$$ Z = \\sum_{y \\in N(v)} \\pi_{vy} = \\pi_{va} + \\pi_{vb} + \\pi_{vc} + \\pi_{vd} $$\n$$ Z = \\frac{\\sqrt{2}}{6} + \\frac{\\sqrt{6}}{12} + \\frac{\\sqrt{3}}{3} + \\frac{\\sqrt{30}}{15} $$\n为了对这些分数求和，我们找到一个公分母，即 $\\text{lcm}(6, 12, 3, 15) = 60$。\n$$ Z = \\frac{10\\sqrt{2}}{60} + \\frac{5\\sqrt{6}}{60} + \\frac{20\\sqrt{3}}{60} + \\frac{4\\sqrt{30}}{60} = \\frac{10\\sqrt{2} + 5\\sqrt{6} + 20\\sqrt{3} + 4\\sqrt{30}}{60} $$\n\n现在，我们计算归一化的概率：\n1.  **到 $a$ 的概率**：\n    $P(a) = \\frac{\\pi_{va}}{Z} = \\frac{\\sqrt{2}/6}{Z} = \\frac{\\sqrt{2}}{6} \\cdot \\frac{60}{10\\sqrt{2} + 5\\sqrt{6} + 20\\sqrt{3} + 4\\sqrt{30}} = \\frac{10\\sqrt{2}}{10\\sqrt{2} + 5\\sqrt{6} + 20\\sqrt{3} + 4\\sqrt{30}}$。\n\n2.  **到 $b$ 的概率**：\n    $P(b) = \\frac{\\pi_{vb}}{Z} = \\frac{\\sqrt{6}/12}{Z} = \\frac{\\sqrt{6}}{12} \\cdot \\frac{60}{10\\sqrt{2} + 5\\sqrt{6} + 20\\sqrt{3} + 4\\sqrt{30}} = \\frac{5\\sqrt{6}}{10\\sqrt{2} + 5\\sqrt{6} + 20\\sqrt{3} + 4\\sqrt{30}}$。\n\n3.  **到 $c$ 的概率**：\n    $P(c) = \\frac{\\pi_{vc}}{Z} = \\frac{\\sqrt{3}/3}{Z} = \\frac{\\sqrt{3}}{3} \\cdot \\frac{60}{10\\sqrt{2} + 5\\sqrt{6} + 20\\sqrt{3} + 4\\sqrt{30}} = \\frac{20\\sqrt{3}}{10\\sqrt{2} + 5\\sqrt{6} + 20\\sqrt{3} + 4\\sqrt{30}}$。\n\n4.  **到 $d$ 的概率**：\n    $P(d) = \\frac{\\pi_{vd}}{Z} = \\frac{\\sqrt{30}/15}{Z} = \\frac{\\sqrt{30}}{15} \\cdot \\frac{60}{10\\sqrt{2} + 5\\sqrt{6} + 20\\sqrt{3} + 4\\sqrt{30}} = \\frac{4\\sqrt{30}}{10\\sqrt{2} + 5\\sqrt{6} + 20\\sqrt{3} + 4\\sqrt{30}}$。\n\n最终结果是未归一化权重和归一化概率的有序集合。",
            "answer": "$$\\boxed{\\begin{pmatrix} \\frac{\\sqrt{2}}{6}  \\frac{\\sqrt{6}}{12}  \\frac{\\sqrt{3}}{3}  \\frac{\\sqrt{30}}{15}  \\frac{10\\sqrt{2}}{10\\sqrt{2} + 5\\sqrt{6} + 20\\sqrt{3} + 4\\sqrt{30}}  \\frac{5\\sqrt{6}}{10\\sqrt{2} + 5\\sqrt{6} + 20\\sqrt{3} + 4\\sqrt{30}}  \\frac{20\\sqrt{3}}{10\\sqrt{2} + 5\\sqrt{6} + 20\\sqrt{3} + 4\\sqrt{30}}  \\frac{4\\sqrt{30}}{10\\sqrt{2} + 5\\sqrt{6} + 20\\sqrt{3} + 4\\sqrt{30}} \\end{pmatrix}}$$"
        },
        {
            "introduction": "生成节点共现对之后，我们如何从中学习到有效的向量表示？本练习将揭示 Skip-Gram with Negative Sampling (SGNS) 学习机制的数学原理，它是在基于随机游走的嵌入方法中被广泛使用的优化目标。通过推导其损失函数相对于上下文向量的梯度，你将直观地看到模型是如何通过“吸引”正样本（共现节点）和“排斥”负样本来塑造嵌入空间的几何结构。",
            "id": "4300053",
            "problem": "考虑一个包含 $|V| = n$ 个节点的连通无向图 $G = (V, E)$。通过固定长度为 $L$ 的截断随机游走和大小为 $w$ 的对称上下文窗口生成节点序列语料库，从而产生训练事件，其中中心节点 $u \\in V$ 与在游走过程中窗口内观察到的上下文节点 $c \\in V$ 配对。假设使用带负采样的 Skip-Gram (SGNS) 目标来学习两个嵌入函数：中心节点的源嵌入 $\\mathbf{u} \\in \\mathbb{R}^{d}$ 和上下文节点的上下文嵌入 $\\mathbf{c} \\in \\mathbb{R}^{d}$。对于每个观察到的正样本对 $(u, c)$，从 $V$ 上的一个固定噪声分布 $Q$ 中抽取 $k$ 个独立的负采样的上下文节点 $\\{n_{i}\\}_{i=1}^{k}$，并将其上下文嵌入表示为 $\\{\\mathbf{n}_{i}\\}_{i=1}^{k} \\subset \\mathbb{R}^{d}$。这单个训练事件的 SGNS 损失为\n$$\n\\ell(\\mathbf{u}, \\mathbf{c}, \\{\\mathbf{n}_{i}\\}_{i=1}^{k}) \\;=\\; -\\,\\ln\\!\\big(\\sigma(\\mathbf{u}^{\\top}\\mathbf{c})\\big)\\;-\\;\\sum_{i=1}^{k}\\ln\\!\\big(\\sigma(-\\,\\mathbf{u}^{\\top}\\mathbf{n}_{i})\\big),\n$$\n其中 $\\sigma(x) = \\frac{1}{1+\\exp(-x)}$ 是 logistic sigmoid 函数，$\\exp(\\cdot)$ 表示指数函数。从二元逻辑交叉熵的基本原理出发，仅使用基本微积分法则，推导损失函数关于上下文嵌入 $\\mathbf{c}$ 的梯度。用 $\\mathbf{u}$、$\\mathbf{c}$ 和 $\\sigma(\\cdot)$ 将最终结果表示为一个闭式符号表达式。然后，根据推导出的表达式和 SGNS 损失的结构，解释 sigmoid 项如何调节正样本对嵌入之间的吸引力以及相对于负样本的排斥力。\n\n你的最终答案必须是 $\\nabla_{\\mathbf{c}}\\ell$ 的解析表达式。不需要进行数值计算。",
            "solution": "我们首先回顾单个训练事件的 SGNS 损失，该事件包含一个正样本对 $(u, c)$ 和 $k$ 个负样本 $\\{n_{i}\\}_{i=1}^{k}$：\n$$\n\\ell(\\mathbf{u}, \\mathbf{c}, \\{\\mathbf{n}_{i}\\}_{i=1}^{k}) \\;=\\; -\\,\\ln\\!\\big(\\sigma(\\mathbf{u}^{\\top}\\mathbf{c})\\big)\\;-\\;\\sum_{i=1}^{k}\\ln\\!\\big(\\sigma(-\\,\\mathbf{u}^{\\top}\\mathbf{n}_{i})\\big).\n$$\n我们将从基本原理出发，使用链式法则和 logistic sigmoid 函数的导数来推导 $\\nabla_{\\mathbf{c}} \\ell$。设 $s = \\mathbf{u}^{\\top}\\mathbf{c}$。第一项通过 $s$ 依赖于 $\\mathbf{c}$，而第二项仅依赖于 $\\mathbf{u}$ 和负采样的上下文嵌入 $\\{\\mathbf{n}_{i}\\}_{i=1}^{k}$；因此，它关于 $\\mathbf{c}$ 的梯度为零。\n\n我们回顾 logistic sigmoid 函数及其导数：\n$$\n\\sigma(x) \\;=\\; \\frac{1}{1+\\exp(-x)},\\qquad \\frac{d}{dx}\\sigma(x) \\;=\\; \\sigma(x)\\big(1 - \\sigma(x)\\big).\n$$\nsigmoid 函数的对数的导数可通过链式法则得到：\n$$\n\\frac{d}{dx}\\ln\\!\\big(\\sigma(x)\\big) \\;=\\; \\frac{1}{\\sigma(x)}\\,\\sigma'(x) \\;=\\; \\frac{1}{\\sigma(x)}\\,\\sigma(x)\\big(1 - \\sigma(x)\\big) \\;=\\; 1 - \\sigma(x).\n$$\n现在我们可以计算第一项关于 $\\mathbf{c}$ 的梯度：\n$$\n\\nabla_{\\mathbf{c}}\\Big(-\\,\\ln\\!\\big(\\sigma(\\mathbf{u}^{\\top}\\mathbf{c})\\big)\\Big)\n\\;=\\;\n-\\,\\Big(1 - \\sigma(\\mathbf{u}^{\\top}\\mathbf{c})\\Big)\\,\\nabla_{\\mathbf{c}}\\!\\big(\\mathbf{u}^{\\top}\\mathbf{c}\\big)\n\\;=\\;\n-\\,\\Big(1 - \\sigma(\\mathbf{u}^{\\top}\\mathbf{c})\\Big)\\,\\mathbf{u}.\n$$\n因为负样本项不依赖于 $\\mathbf{c}$，所以它关于 $\\mathbf{c}$ 的梯度为零：\n$$\n\\nabla_{\\mathbf{c}}\\Big(-\\,\\sum_{i=1}^{k}\\ln\\!\\big(\\sigma(-\\,\\mathbf{u}^{\\top}\\mathbf{n}_{i})\\big)\\Big) \\;=\\; \\mathbf{0}.\n$$\n因此，关于 $\\mathbf{c}$ 的总梯度为\n$$\n\\nabla_{\\mathbf{c}}\\ell \\;=\\; -\\,\\Big(1 - \\sigma(\\mathbf{u}^{\\top}\\mathbf{c})\\Big)\\,\\mathbf{u}\n\\;=\\;\n\\Big(\\sigma(\\mathbf{u}^{\\top}\\mathbf{c}) - 1\\Big)\\,\\mathbf{u}.\n$$\n\n由 sigmoid 项调节的吸引与排斥的解释：系数 $\\sigma(\\mathbf{u}^{\\top}\\mathbf{c}) - 1$ 乘以方向 $\\mathbf{u}$。当 $\\mathbf{u}^{\\top}\\mathbf{c}$ 很小时，$\\sigma(\\mathbf{u}^{\\top}\\mathbf{c})$ 接近于 $0$，所以 $\\sigma(\\mathbf{u}^{\\top}\\mathbf{c}) - 1$ 为负，梯度下降的更新会使 $\\mathbf{c}$ 沿着 $+\\mathbf{u}$ 的方向移动（因为更新规则是 $\\mathbf{c} \\leftarrow \\mathbf{c} - \\eta \\nabla_{\\mathbf{c}}\\ell$，其中 $\\eta  0$），从而增加 $\\mathbf{u}^{\\top}\\mathbf{c}$ 的值，并在正样本对 $(u, c)$ 之间产生吸引力。随着 $\\mathbf{u}^{\\top}\\mathbf{c}$ 的增大且 $\\sigma(\\mathbf{u}^{\\top}\\mathbf{c})$ 趋近于 $1$，系数 $\\sigma(\\mathbf{u}^{\\top}\\mathbf{c}) - 1$ 趋于 $0$，从而使吸引力饱和。\n\n相对于负样本的排斥力出现在与负样本直接相互作用的参数的梯度中。例如，对于分数为 $t = \\mathbf{u}^{\\top}\\mathbf{n}$ 的单个负样本 $\\mathbf{n}$，其导数\n$$\n\\nabla_{\\mathbf{n}}\\Big(-\\ln\\big(\\sigma(-t)\\big)\\Big) \\;=\\; \\Big(1 - \\sigma(-t)\\Big)\\,\\mathbf{u}\n\\;=\\; \\sigma(t)\\,\\mathbf{u}\n$$\n的系数 $\\sigma(t)$ 作用于 $\\mathbf{u}$，是正的，因此对 $\\mathbf{n}$ 的梯度下降更新会使其向 $-\\mathbf{u}$ 方向移动，从而减小 $t$ 并产生与 $\\mathbf{u}$ 的排斥力。同样地，关于 $\\mathbf{u}$ 的梯度既包含吸引项 $(\\sigma(\\mathbf{u}^{\\top}\\mathbf{c}) - 1)\\mathbf{c}$，也包含排斥项 $\\sum_{i=1}^{k}\\sigma(\\mathbf{u}^{\\top}\\mathbf{n}_{i})\\mathbf{n}_{i}$，从而在对正样本的吸引和对负样本的排斥之间取得平衡。相比之下，对于这里考虑的正样本对的上下文嵌入 $\\mathbf{c}$，只有由 sigmoid 调节的吸引项起作用。",
            "answer": "$$\\boxed{\\nabla_{\\mathbf{c}}\\ell \\;=\\; \\big(\\sigma(\\mathbf{u}^{\\top}\\mathbf{c}) - 1\\big)\\,\\mathbf{u}}$$"
        }
    ]
}