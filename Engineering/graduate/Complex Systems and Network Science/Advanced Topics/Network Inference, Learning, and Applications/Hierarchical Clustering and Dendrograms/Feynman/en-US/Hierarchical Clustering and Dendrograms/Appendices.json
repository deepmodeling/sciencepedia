{
    "hands_on_practices": [
        {
            "introduction": "The core of any agglomerative clustering algorithm is the update rule used after a merge. This exercise provides a hands-on calculation for the Unweighted Pair Group Method with Arithmetic Mean (UPGMA), also known as average linkage. By applying the Lance-Williams formula to a small, well-defined dataset, you will directly engage with the mechanics of how a new cluster's dissimilarity to its neighbors is determined .",
            "id": "4280687",
            "problem": "Consider a hierarchical agglomerative clustering process on a set of $n=5$ nodes labeled $\\{1,2,3,4,5\\}$ with a symmetric dissimilarity matrix $D=\\big(d(i,j)\\big)_{1 \\leq i,j \\leq 5}$ given by\n$$\nD=\\begin{pmatrix}\n0 & 1 & 4 & 7 & 10 \\\\\n1 & 0 & 3 & 6 & 9 \\\\\n4 & 3 & 0 & 3 & 6 \\\\\n7 & 6 & 3 & 0 & 3 \\\\\n10 & 9 & 6 & 3 & 0\n\\end{pmatrix}.\n$$\nAssume the dissimilarities arise from a metric on a one-dimensional Euclidean embedding, and perform hierarchical agglomerative clustering using unweighted average linkage, also known as the Unweighted Pair Group Method with Arithmetic Mean (UPGMA). At the first agglomeration step, merge the singleton clusters $\\{1\\}$ and $\\{2\\}$. Using the Lance-Williams formula appropriate to this linkage, compute the updated dissimilarities from the merged cluster $\\{1,2\\}$ to each remaining singleton cluster $\\{3\\}$, $\\{4\\}$, and $\\{5\\}$, and then compute the sum\n$$\nS \\;=\\; d\\big(\\{1,2\\},\\{3\\}\\big) \\;+\\; d\\big(\\{1,2\\},\\{4\\}\\big) \\;+\\; d\\big(\\{1,2\\},\\{5\\}\\big).\n$$\nProvide $S$ as a single real number. No rounding is required.",
            "solution": "The problem is validated as follows.\n\n### Step 1: Extract Givens\n- A set of $n=5$ nodes labeled $\\{1,2,3,4,5\\}$.\n- A symmetric dissimilarity matrix $D=\\big(d(i,j)\\big)_{1 \\leq i,j \\leq 5}$ given by\n$$\nD=\\begin{pmatrix}\n0 & 1 & 4 & 7 & 10 \\\\\n1 & 0 & 3 & 6 & 9 \\\\\n4 & 3 & 0 & 3 & 6 \\\\\n7 & 6 & 3 & 0 & 3 \\\\\n10 & 9 & 6 & 3 & 0\n\\end{pmatrix}.\n$$\n- An assumption that the dissimilarities arise from a metric on a one-dimensional Euclidean embedding.\n- The clustering method is hierarchical agglomerative clustering using unweighted average linkage (UPGMA).\n- The first agglomeration step is defined as the merge of singleton clusters $\\{1\\}$ and $\\{2\\}$.\n- The task is to compute the sum $S = d\\big(\\{1,2\\},\\{3\\}\\big) + d\\big(\\{1,2\\},\\{4\\}\\big) + d\\big(\\{1,2\\},\\{5\\}\\big)$ using the appropriate Lance-Williams formula.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is subjected to validation against the specified criteria.\n- **Scientifically Grounded:** The problem is set in the context of hierarchical clustering, a standard method in data analysis and network science. The specified method, UPGMA, is a well-established algorithm. The dissimilarity matrix $D$ is symmetric with a zero diagonal, as required. The statement that the dissimilarities arise from a 1D Euclidean embedding can be verified. Let the positions of the nodes on a line be $x_1, x_2, x_3, x_4, x_5$. We can set $x_1=0$. From $d(1,2)=1$, we have $x_2=1$. From $d(2,3)=3$ and $d(1,3)=4$, we get $x_3=4$. Continuing this process, we find a consistent embedding: $x_1=0, x_2=1, x_3=4, x_4=7, x_5=10$. Since $d(i,j)=|x_i-x_j|$ holds for all pairs, the premise is factually correct.\n- **Well-Posed:** The givens are sufficient and consistent. The task is to apply a standard, deterministic algorithm (UPGMA) to a given dataset and compute a specific quantity. This structure ensures a unique, stable, and meaningful solution exists. The problem instruction to merge $\\{1\\}$ and $\\{2\\}$ first is also consistent with the standard procedure, as $d(1,2)=1$ is the minimum non-zero dissimilarity in the matrix $D$.\n- **Objective:** The problem is stated using precise mathematical terminology and quantitative data. There are no subjective or ambiguous statements.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. It is scientifically sound, well-posed, objective, and self-contained. The solution process may proceed.\n\nThe specified clustering method is the Unweighted Pair Group Method with Arithmetic Mean (UPGMA), also known as unweighted average linkage. The distance between a newly merged cluster, say $C_{ij} = C_i \\cup C_j$, and any other cluster $C_k$ is the average of the distances between all pairs of points from $C_{ij}$ and $C_k$. If the clusters $C_i$ and $C_j$ have sizes $n_i = |C_i|$ and $n_j = |C_j|$ respectively, the update formula is:\n$$\nd(C_i \\cup C_j, C_k) = \\frac{1}{n_i n_k} \\sum_{p \\in C_i, q \\in C_k} d(p,q) + \\frac{1}{n_j n_k} \\sum_{p' \\in C_j, q \\in C_k} d(p',q)\n$$\nThis expression simplifies to the Lance-Williams update formula for UPGMA:\n$$\nd(C_i \\cup C_j, C_k) = \\frac{n_i}{n_i + n_j} d(C_i, C_k) + \\frac{n_j}{n_i + n_j} d(C_j, C_k)\n$$\nIn this problem, the first step is to merge the singleton clusters $C_1 = \\{1\\}$ and $C_2 = \\{2\\}$. The sizes of these clusters are $n_1 = |\\{1\\}| = 1$ and $n_2 = |\\{2\\}| = 1$. The new cluster is $C_{12} = \\{1,2\\}$. We need to compute the dissimilarities from this new cluster to the remaining singleton clusters $C_3=\\{3\\}$, $C_4=\\{4\\}$, and $C_5=\\{5\\}$.\n\nFor any remaining singleton cluster $C_k = \\{k\\}$, where $k \\in \\{3,4,5\\}$, the size is $n_k=1$. The distances $d(C_1, C_k)$ and $d(C_2, C_k)$ are simply the elemental distances $d(1,k)$ and $d(2,k)$ from the matrix $D$.\nApplying the UPGMA formula:\n$$\nd(\\{1,2\\}, \\{k\\}) = \\frac{1}{1+1} d(\\{1\\}, \\{k\\}) + \\frac{1}{1+1} d(\\{2\\}, \\{k\\}) = \\frac{1}{2} \\left[ d(1,k) + d(2,k) \\right]\n$$\nWe now compute the required dissimilarities for $k=3, 4, 5$.\n\n$1$. Dissimilarity to cluster $\\{3\\}$:\nUsing the values from the matrix $D$, $d(1,3) = 4$ and $d(2,3) = 3$.\n$$\nd(\\{1,2\\}, \\{3\\}) = \\frac{1}{2} \\left[ d(1,3) + d(2,3) \\right] = \\frac{1}{2} (4 + 3) = \\frac{7}{2}\n$$\n\n$2$. Dissimilarity to cluster $\\{4\\}$:\nUsing the values from the matrix $D$, $d(1,4) = 7$ and $d(2,4) = 6$.\n$$\nd(\\{1,2\\}, \\{4\\}) = \\frac{1}{2} \\left[ d(1,4) + d(2,4) \\right] = \\frac{1}{2} (7 + 6) = \\frac{13}{2}\n$$\n\n$3$. Dissimilarity to cluster $\\{5\\}$:\nUsing the values from the matrix $D$, $d(1,5) = 10$ and $d(2,5) = 9$.\n$$\nd(\\{1,2\\}, \\{5\\}) = \\frac{1}{2} \\left[ d(1,5) + d(2,5) \\right] = \\frac{1}{2} (10 + 9) = \\frac{19}{2}\n$$\n\nThe problem asks for the sum $S$:\n$$\nS \\;=\\; d\\big(\\{1,2\\},\\{3\\}\\big) \\;+\\; d\\big(\\{1,2\\},\\{4\\}\\big) \\;+\\; d\\big(\\{1,2\\},\\{5\\}\\big)\n$$\nSubstituting the computed values:\n$$\nS = \\frac{7}{2} + \\frac{13}{2} + \\frac{19}{2}\n$$\n$$\nS = \\frac{7+13+19}{2} = \\frac{20+19}{2} = \\frac{39}{2}\n$$\nIn decimal form, this value is $19.5$.",
            "answer": "$$\\boxed{19.5}$$"
        },
        {
            "introduction": "Linkage criteria dramatically influence the shape of the resulting clusters, and different methods can exhibit unique behaviors. This practice explores the \"chaining effect,\" a classic property of single linkage where it can connect two dense groups via a sparse bridge of intermediate points. Through a carefully constructed example, you will uncover the fundamental relationship between single-linkage clustering and the Minimum Spanning Tree (MST), quantifying the merge height as the weight of the most critical link in the chain .",
            "id": "4280700",
            "problem": "Consider the metric space $(X,d)$ where $X \\subset \\mathbb{R}$ and $d(x,y) = |x-y|$. Define hierarchical agglomerative clustering with single linkage as follows: for each threshold $\\tau \\geq 0$, construct a graph $G_{\\tau}$ on vertex set $X$ with an undirected edge between $x,y \\in X$ if and only if $d(x,y) \\leq \\tau$. The clusters at level $\\tau$ are the connected components of $G_{\\tau}$. The single-linkage ultrametric $u(x,y)$ between points $x,y \\in X$ is the minimal $\\tau$ at which $x$ and $y$ lie in the same connected component of $G_{\\tau}$.\n\nTo analyze the chaining effect of single linkage, consider a dataset composed of two dense groups connected by a sparse bridge:\n- Dense group $A = \\{0, 0.2, 0.4, 0.6, 0.8\\}$,\n- Sparse bridge $S = \\{2, 5.5, 9\\}$,\n- Dense group $B = \\{12, 12.2, 12.4, 12.6, 12.8\\}$.\n\nLet $a^{\\star} = 0.4 \\in A$ and $b^{\\star} = 12.4 \\in B$. Starting from the fundamental definitions above and the graph-theoretic notion of a Minimum Spanning Tree (MST) (Minimum Spanning Tree (MST) is a spanning tree of a weighted graph that minimizes the total edge weight), determine the exact value of the single-linkage ultrametric $u(a^{\\star}, b^{\\star})$ for this dataset, thereby quantifying the level at which the two dense groups merge through the sparse bridge in the single-linkage dendrogram.\n\nExpress your final answer as a single real number. No rounding is required.",
            "solution": "The problem as stated is formally and scientifically valid. It is a well-posed question within the domain of hierarchical clustering and network science, with all necessary data and definitions provided. We may therefore proceed with a solution.\n\nThe single-linkage ultrametric, $u(x,y)$, between two points $x$ and $y$ is defined as the minimum threshold $\\tau \\ge 0$ at which $x$ and $y$ reside in the same connected component of the graph $G_{\\tau}$. An edge exists between any two points $x_i, x_j \\in X$ in $G_{\\tau}$ if and only if their distance $d(x_i, x_j) \\le \\tau$. As $\\tau$ increases from $0$, edges are added to the graph in non-decreasing order of their corresponding distances. This process is identical to the construction of a Minimum Spanning Tree (MST) of the complete graph on $X$ using Kruskal's algorithm, where edge weights are given by the metric $d(x,y) = |x-y|$.\n\nA fundamental theorem of single-linkage clustering states that the ultrametric distance $u(x,y)$ is equal to the weight of the heaviest edge on the unique path between $x$ and $y$ in the MST of the dataset. Our task is thus transformed into finding this maximum edge weight for the specific points $a^{\\star} = 0.4$ and $b^{\\star} = 12.4$.\n\nThe dataset is the set of points $X = A \\cup S \\cup B$, where:\n- $A = \\{0, 0.2, 0.4, 0.6, 0.8\\}$\n- $S = \\{2, 5.5, 9\\}$\n- $B = \\{12, 12.2, 12.4, 12.6, 12.8\\}$\n\nWe will determine the edges of the MST by considering edges in increasing order of weight, analogous to Kruskal's algorithm.\n\n1.  **Intra-group edges:** The smallest distances in the entire dataset are between adjacent points within the dense groups $A$ and $B$.\n    - Within group $A$, the edges connecting adjacent points, such as $(0, 0.2)$, $(0.2, 0.4)$, $(0.4, 0.6)$, and $(0.6, 0.8)$, all have a weight of $0.2$. These edges will be part of the MST and will connect all points in $A$ into a single component.\n    - Similarly, within group $B$, the edges $(12, 12.2)$, $(12.2, 12.4)$, $(12.4, 12.6)$, and $(12.6, 12.8)$ all have a weight of $0.2$. These edges will connect all points in $B$ into a separate component.\n    After this step, we have three main components: the cluster of points in $A$, the cluster of points in $B$, and the three isolated points of $S$.\n\n2.  **Inter-group (bridging) edges:** Now we must find the lowest-weight edges that connect these components.\n    - The shortest connection between a point in group $A$ and a point in the bridge $S$ is $d(0.8, 2) = |0.8 - 2| = 1.2$. This edge, $(0.8, 2)$, is added to the MST.\n    - The shortest connection between a point in group $B$ and a point in the bridge $S$ is $d(9, 12) = |9 - 12| = 3$. This edge, $(9, 12)$, is added to the MST.\n    - We must connect the points within the sparse bridge $S = \\{2, 5.5, 9\\}$. The distances are:\n        - $d(2, 5.5) = |2 - 5.5| = 3.5$\n        - $d(5.5, 9) = |5.5 - 9| = 3.5$\n        - $d(2, 9) = |2 - 9| = 7$\n    The MST will include the edges $(2, 5.5)$ and $(5.5, 9)$ to connect these three points, as using these two edges (both of weight $3.5$) is preferable to using the single, heavier edge $(2, 9)$ of weight $7$.\n\nWith the structure of the MST established, we can identify the unique path from $a^{\\star} = 0.4$ to $b^{\\star} = 12.4$. This path must traverse the bridging edges we found. The sequence of vertices in the path is:\n$$0.4 \\leftrightarrow 0.6 \\leftrightarrow 0.8 \\leftrightarrow 2 \\leftrightarrow 5.5 \\leftrightarrow 9 \\leftrightarrow 12 \\leftrightarrow 12.2 \\leftrightarrow 12.4$$\nThe path from $0.4$ to $0.8$ is through its neighbor(s) in group A, and similarly for the path from $12$ to $12.4$ in group B.\n\nLet's list the weights of the edges on this unique path:\n- $d(0.4, 0.6) = 0.2$\n- $d(0.6, 0.8) = 0.2$\n- $d(0.8, 2) = 1.2$\n- $d(2, 5.5) = 3.5$\n- $d(5.5, 9) = 3.5$\n- $d(9, 12) = 3$\n- $d(12, 12.2) = 0.2$\n- $d(12.2, 12.4) = 0.2$\n\nThe set of edge weights along the path is $\\{0.2, 0.2, 1.2, 3.5, 3.5, 3.0, 0.2, 0.2\\}$.\n\nThe ultrametric distance $u(a^{\\star}, b^{\\star})$ is the maximum of these weights:\n$$ u(a^{\\star}, b^{\\star}) = \\max\\{0.2, 1.2, 3.0, 3.5\\} $$\n$$ u(a^{\\star}, b^{\\star}) = 3.5 $$\nThis value, $3.5$, is the minimal threshold $\\tau$ at which $a^{\\star}$ and $b^{\\star}$ become members of the same cluster. This occurs when the path between them is completed by adding the last required bridging edge of weight $3.5$. This quantifies the level at which the two dense groups merge via the sparse bridge.",
            "answer": "$$\\boxed{3.5}$$"
        },
        {
            "introduction": "A dendrogram is often interpreted as a hierarchy where dissimilarity monotonically increases, but this is not a universal property of all linkage methods. Certain criteria can produce \"inversions,\" where a later merge occurs at a smaller height than an earlier one. This exercise uses a simple geometric configuration to demonstrate how centroid linkage can lead to such a non-monotonic dendrogram, a critical and counter-intuitive phenomenon to recognize when interpreting clustering results .",
            "id": "4280672",
            "problem": "Consider hierarchical clustering with centroid linkage, also known as the Unweighted Pair Group Method with Centroid (UPGMC), on points in a Euclidean plane. The centroid linkage distance between two clusters is defined to be the Euclidean distance between their centroids, and the cophenetic distance between two points is defined to be the dendrogram height at which they first belong to the same cluster. Hierarchical agglomeration proceeds by repeatedly merging the pair of clusters with the smallest linkage distance at each step.\n\nConstruct the following explicit example in $\\mathbb{R}^{2}$. Let the point set be $S=\\{A,B,C\\}$ with\n$$\nA = (-1,0),\\quad B=(1,0),\\quad C=\\left(0,\\frac{7}{4}\\right).\n$$\nUsing the standard Euclidean metric $d(x,y)=\\|x-y\\|_{2}$ and centroid linkage, perform hierarchical agglomeration on $S$ and determine the dendrogram merge heights $h_{1}$ for the first merge and $h_{2}$ for the second merge. Then, define the inversion discrepancy\n$$\n\\Delta \\equiv d_{c}(A,B) - d_{c}(A,C),\n$$\nwhere $d_{c}(\\cdot,\\cdot)$ denotes the cophenetic distance induced by the centroid-linkage dendrogram on $S$. Compute $\\Delta$ for this example. Express the final answer as an exact fraction with no rounding.",
            "solution": "The problem asks us to perform hierarchical agglomeration using centroid linkage on a set of three points in $\\mathbb{R}^{2}$ and then compute a specified quantity called the \"inversion discrepancy.\" The problem is well-defined, scientifically grounded, and provides all necessary information. We proceed with the solution.\n\nThe set of points is $S=\\{A,B,C\\}$, with coordinates:\n$$\nA = (-1,0), \\quad B=(1,0), \\quad C=\\left(0,\\frac{7}{4}\\right).\n$$\nThe clustering algorithm is hierarchical agglomeration with centroid linkage, using the standard Euclidean distance $d(x,y)=\\|x-y\\|_{2}$.\n\n**Step 1: Initial State and Pairwise Distances**\n\nInitially, we have three clusters, each containing a single point: $\\{A\\}$, $\\{B\\}$, and $\\{C\\}$. The centroid of a single-point cluster is the point itself. We calculate the pairwise centroid linkage distances, which are simply the Euclidean distances between the points.\n\nThe distance between clusters $\\{A\\}$ and $\\{B\\}$ is:\n$$\nd(\\{A\\}, \\{B\\}) = \\|A-B\\|_{2} = \\sqrt{(-1-1)^{2} + (0-0)^{2}} = \\sqrt{(-2)^{2}} = 2.\n$$\nThe distance between clusters $\\{A\\}$ and $\\{C\\}$ is:\n$$\nd(\\{A\\}, \\{C\\}) = \\|A-C\\|_{2} = \\sqrt{(-1-0)^{2} + \\left(0-\\frac{7}{4}\\right)^{2}} = \\sqrt{1 + \\frac{49}{16}} = \\sqrt{\\frac{16}{16} + \\frac{49}{16}} = \\sqrt{\\frac{65}{16}} = \\frac{\\sqrt{65}}{4}.\n$$\nBy symmetry, the distance between clusters $\\{B\\}$ and $\\{C\\}$ is the same:\n$$\nd(\\{B\\}, \\{C\\}) = \\|B-C\\|_{2} = \\sqrt{(1-0)^{2} + \\left(0-\\frac{7}{4}\\right)^{2}} = \\sqrt{1 + \\frac{49}{16}} = \\frac{\\sqrt{65}}{4}.\n$$\n\n**Step 2: First Merge**\n\nThe algorithm merges the pair of clusters with the smallest linkage distance. We must compare the distances: $2$ and $\\frac{\\sqrt{65}}{4}$. To compare them, we can square both values' numerators (since they have a common denominator format if we write $2 = 8/4$):\n$$\n2 = \\frac{8}{4} = \\frac{\\sqrt{64}}{4}.\n$$\nSince $64 < 65$, we have $\\sqrt{64} < \\sqrt{65}$, and therefore $2 < \\frac{\\sqrt{65}}{4}$.\n\nThe smallest distance is $d(\\{A\\}, \\{B\\}) = 2$. Thus, the first merge combines clusters $\\{A\\}$ and $\\{B\\}$ to form a new cluster, which we denote as $\\{A,B\\}$. The height of this merge, $h_{1}$, is the distance at which the merge occurs:\n$$\nh_{1} = 2.\n$$\nThe cophenetic distance $d_{c}(A,B)$ is defined as the dendrogram height at which points $A$ and $B$ first belong to the same cluster. This is precisely $h_{1}$.\n$$\nd_{c}(A,B) = h_{1} = 2.\n$$\n\n**Step 3: Second Merge**\n\nAfter the first merge, we have two clusters: $K_{1} = \\{A, B\\}$ and $K_{2} = \\{C\\}$. To proceed, we must calculate the centroid of the new cluster $K_{1}$. The centroid is the mean of the coordinates of the points in the cluster.\n$$\nC_{K_{1}} = \\frac{A+B}{2} = \\frac{1}{2}((-1,0) + (1,0)) = \\frac{1}{2}(0,0) = (0,0).\n$$\nThe centroid of cluster $K_{2}$ is simply the point $C$: $C_{K_{2}} = C = \\left(0, \\frac{7}{4}\\right)$.\n\nThere are only two clusters left, so they must be merged. The height of this second merge, $h_{2}$, is the centroid linkage distance between $K_{1}$ and $K_{2}$.\n$$\nh_{2} = d(K_{1}, K_{2}) = \\|C_{K_{1}} - C_{K_{2}}\\|_{2} = \\left\\|(0,0) - \\left(0, \\frac{7}{4}\\right)\\right\\|_{2} = \\sqrt{(0-0)^{2} + \\left(0-\\frac{7}{4}\\right)^{2}} = \\sqrt{\\left(-\\frac{7}{4}\\right)^{2}} = \\frac{7}{4}.\n$$\nThis merge combines cluster $\\{A,B\\}$ with $\\{C\\}$. This is the first time points $A$ and $C$ (and also $B$ and $C$) belong to the same cluster. Therefore, their cophenetic distance is $h_{2}$.\n$$\nd_{c}(A,C) = h_{2} = \\frac{7}{4}.\n$$\nNote that $h_{2} = \\frac{7}{4} = 1.75$, which is less than $h_{1} = 2$. This phenomenon, where a later merge occurs at a lower height than an earlier one, is known as a dendrogram inversion and is a characteristic of the centroid linkage method.\n\n**Step 4: Compute the Inversion Discrepancy**\n\nThe problem asks for the inversion discrepancy $\\Delta$, defined as:\n$$\n\\Delta \\equiv d_{c}(A,B) - d_{c}(A,C).\n$$\nUsing the cophenetic distances derived from the merge heights:\n$$\n\\Delta = 2 - \\frac{7}{4}.\n$$\nTo compute this, we find a common denominator:\n$$\n\\Delta = \\frac{8}{4} - \\frac{7}{4} = \\frac{1}{4}.\n$$\n\nThe merge heights are $h_{1} = 2$ and $h_{2} = \\frac{7}{4}$. The cophenetic distances are $d_{c}(A,B) = 2$ and $d_{c}(A,C) = \\frac{7}{4}$. The inversion discrepancy is the difference between these two values.",
            "answer": "$$\\boxed{\\frac{1}{4}}$$"
        }
    ]
}