{
    "hands_on_practices": [
        {
            "introduction": "理解隐藏度量空间模型的第一步是将其理论参数与可观测的网络属性联系起来。本练习旨在通过一个核心推导，展示如何从一个节点的可观测度 $k_i$ 来推断其隐藏的流行度参数 $\\kappa_i$ 。通过运用最大似然估计和平均场近似，你将掌握校准几何网络模型的关键技能。",
            "id": "4289367",
            "problem": "考虑一类基于圆的隐藏度量空间模型（即所谓的$\\mathbb{S}^{1}$模型）中的网络生成机制，其中每个节点 $i$ 被赋予一个角度坐标 $\\theta_{i} \\in [0,2\\pi)$ 和一个流行度（隐藏度）参数 $\\kappa_{i}  0$。在给定 $\\{\\theta_{i}\\}$ 和 $\\{\\kappa_{i}\\}$ 的条件下，边是独立的，节点 $i$ 和 $j$ 之间的连接概率为\n$$\np_{ij} \\equiv \\frac{1}{1 + \\chi_{ij}^{\\beta}}, \\quad \\chi_{ij} \\equiv \\frac{N \\Delta\\theta_{ij}}{2\\pi \\mu \\kappa_{i} \\kappa_{j}},\n$$\n其中 $N$ 是节点数，$\\mu  0$ 是控制期望度的全局尺度参数，$\\beta  1$ 是一个聚类控制参数，$\\Delta\\theta_{ij} \\equiv \\pi - |\\pi - |\\theta_{i} - \\theta_{j}||$ 是在 $[0,\\pi]$ 内的测地角距离。流行度参数 $\\{\\kappa_{i}\\}$ 通过以下方式映射到双曲径向坐标 $\\{r_{i}\\}$\n$$\nr_{i} = R - 2 \\ln \\kappa_{i},\n$$\n其中 $R$ 是一个设定圆盘半径的固定常数。设 $A$ 是邻接矩阵，其元素为 $a_{ij} \\in \\{0,1\\}$，并用 $k_{i} \\equiv \\sum_{j \\neq i} a_{ij}$ 表示节点 $i$ 的观测度。\n\n从边的独立性以及网络似然的定义出发，回答以下问题：\n\n1. 推导对数似然函数关于径向坐标 $r_{i}$ 的导数，用 $k_{i}$、$\\kappa_{i}$ 和 $\\{p_{ij}\\}_{j \\neq i}$ 表示结果，除了上面明确定义的公式外，不引入任何简化公式。\n\n2. 假设角度 $\\{\\theta_{j}\\}_{j \\neq i}$ 在 $[0,2\\pi)$ 上独立同分布于均匀分布，并考虑适用积分近似的稀疏大$N$情况。证明节点 $i$ 的期望度的平均场近似可以写为\n$$\n\\sum_{j \\neq i} p_{ij} \\approx C_{\\beta} \\, \\kappa_{i},\n$$\n并求出 $C_{\\beta}$ 关于 $\\mu$、$\\beta$ 和平均流行度 $\\bar{\\kappa} \\equiv \\frac{1}{N} \\sum_{j=1}^{N} \\kappa_{j}$ 的显式表达式。\n\n3. 使用第1部分的结果和第2部分的平均场近似，当全局尺度 $\\mu$ 被校准以使模型再现观测到的平均度（即，$\\mathbb{E}[k_{i} \\mid \\kappa_{i}] = \\kappa_{i}$ 在网络上平均成立）时，以闭合形式确定 $\\kappa_{i}$ 的最大似然估计量。\n\n你的最终答案必须是关于 $\\kappa_{i}$ 的校准估计量的单个闭合形式解析表达式，用观测度 $k_{i}$ 表示。不需要四舍五入。如果出现角度，请用弧度表示。",
            "solution": "该问题经确认为具有科学依据且定义明确。我们将按要求分三部分进行解答。\n\n**第1部分：对数似然函数的导数**\n\n网络的对数似然函数 $\\mathcal{L}$ 由所有独立边的贡献加和而成：\n$$ \\mathcal{L} = \\sum_{j  k} \\left[ a_{jk} \\ln p_{jk} + (1 - a_{jk}) \\ln(1 - p_{jk}) \\right] $$\n我们求 $\\mathcal{L}$ 关于特定节点 $i$ 的径向坐标 $r_i$ 的导数。只有包含节点 $i$ 的边 $(i, j)$ 对此有贡献。\n$$ \\frac{\\partial \\mathcal{L}}{\\partial r_i} = \\sum_{j \\neq i} \\frac{\\partial}{\\partial r_i} \\left[ a_{ij} \\ln p_{ij} + (1 - a_{ij}) \\ln(1 - p_{ij}) \\right] $$\n使用链式法则，我们得到：\n$$ \\frac{\\partial \\mathcal{L}}{\\partial r_i} = \\sum_{j \\neq i} \\left( \\frac{a_{ij}}{p_{ij}} - \\frac{1-a_{ij}}{1-p_{ij}} \\right) \\frac{\\partial p_{ij}}{\\partial r_i} = \\sum_{j \\neq i} \\frac{a_{ij} - p_{ij}}{p_{ij}(1-p_{ij})} \\frac{\\partial p_{ij}}{\\partial r_i} $$\n接下来计算 $\\frac{\\partial p_{ij}}{\\partial r_i}$。我们有 $p_{ij} = (1 + \\chi_{ij}^{\\beta})^{-1}$。\n$$ \\frac{\\partial p_{ij}}{\\partial r_i} = -\\frac{\\beta \\chi_{ij}^{\\beta-1}}{(1+\\chi_{ij}^{\\beta})^2} \\frac{\\partial \\chi_{ij}}{\\partial r_i} = -\\beta \\, p_{ij} (1-p_{ij}) \\frac{1}{\\chi_{ij}} \\frac{\\partial \\chi_{ij}}{\\partial r_i} $$\n为了计算 $\\frac{\\partial \\chi_{ij}}{\\partial r_i}$，我们使用链式法则以及 $r_i = R - 2\\ln\\kappa_i$，这意味着 $\\kappa_i = e^{(R-r_i)/2}$ 并且 $\\frac{\\partial \\kappa_i}{\\partial r_i} = -\\frac{1}{2} \\kappa_i$。\n$$ \\frac{\\partial \\chi_{ij}}{\\partial r_i} = \\frac{\\partial}{\\partial r_i} \\left( \\frac{N \\Delta\\theta_{ij}}{2\\pi \\mu \\kappa_{i} \\kappa_{j}} \\right) = \\frac{N \\Delta\\theta_{ij}}{2\\pi \\mu \\kappa_{j}} \\left( -\\frac{1}{\\kappa_i^2} \\right) \\frac{\\partial \\kappa_i}{\\partial r_i} = \\chi_{ij} \\left( -\\frac{1}{\\kappa_i} \\right) \\left( -\\frac{1}{2} \\kappa_i \\right) = \\frac{\\chi_{ij}}{2} $$\n将此代入 $\\frac{\\partial p_{ij}}{\\partial r_i}$ 的表达式中，得到 $\\frac{\\partial p_{ij}}{\\partial r_i} = -\\frac{\\beta}{2} p_{ij}(1-p_{ij})$。\n最后，将此结果代回 $\\frac{\\partial \\mathcal{L}}{\\partial r_i}$ 的表达式：\n$$ \\frac{\\partial \\mathcal{L}}{\\partial r_i} = \\sum_{j \\neq i} \\frac{a_{ij} - p_{ij}}{p_{ij}(1-p_{ij})} \\left( -\\frac{\\beta}{2} p_{ij}(1-p_{ij}) \\right) = -\\frac{\\beta}{2} \\sum_{j \\neq i} (a_{ij} - p_{ij}) $$\n因为 $k_i = \\sum_{j \\neq i} a_{ij}$，我们得到最终结果：\n$$ \\frac{\\partial \\mathcal{L}}{\\partial r_i} = -\\frac{\\beta}{2} \\left( k_i - \\sum_{j \\neq i} p_{ij} \\right) $$\n\n**第2部分：期望度的平均场近似**\n\n节点 $i$ 的期望度 $\\mathbb{E}[k_i|\\kappa_i]$ 是与所有其他 $N-1$ 个节点连接概率的总和。在大 $N$ 和稀疏网络极限下，我们用积分来近似这个和。我们假设其他节点的角坐标在 $[0, 2\\pi)$ 上均匀分布，流行度从概率密度为 $\\rho(\\kappa)$ 的分布中抽取。\n$$ \\mathbb{E}[k_i|\\kappa_i] \\approx N \\int_0^\\infty d\\kappa_j \\rho(\\kappa_j) \\int_0^\\pi \\frac{d(\\Delta\\theta)}{\\pi} \\frac{1}{1 + \\left(\\frac{N \\Delta\\theta}{2\\pi \\mu \\kappa_i \\kappa_j}\\right)^\\beta} $$\n对关于 $\\Delta\\theta$ 的内层积分进行变量替换 $x = \\frac{N \\Delta\\theta}{2\\pi \\mu \\kappa_i \\kappa_j}$，则 $d(\\Delta\\theta) = \\frac{2\\pi \\mu \\kappa_i \\kappa_j}{N} dx$。当 $N \\to \\infty$ 时，积分上限也趋于无穷大。\n$$ \\int_0^\\pi \\frac{d(\\Delta\\theta)}{\\dots} \\approx \\int_0^\\infty \\frac{1}{1+x^\\beta} \\frac{2\\pi \\mu \\kappa_i \\kappa_j}{N} dx = \\frac{2\\pi \\mu \\kappa_i \\kappa_j}{N} \\left[ \\frac{\\pi/\\beta}{\\sin(\\pi/\\beta)} \\right] $$\n这里我们使用了标准积分 $\\int_0^\\infty \\frac{dx}{1+x^\\beta} = \\frac{\\pi/\\beta}{\\sin(\\pi/\\beta)}$ (对 $\\beta>1$ 成立)。代回期望度的表达式中：\n$$ \\mathbb{E}[k_i|\\kappa_i] \\approx \\frac{N}{\\pi} \\int_0^\\infty d\\kappa_j \\rho(\\kappa_j) \\left( \\frac{2\\pi \\mu \\kappa_i \\kappa_j}{N} \\frac{\\pi/\\beta}{\\sin(\\pi/\\beta)} \\right) = \\kappa_i \\left( 2\\mu \\frac{\\pi/\\beta}{\\sin(\\pi/\\beta)} \\int_0^\\infty \\kappa_j \\rho(\\kappa_j) d\\kappa_j \\right) $$\n积分部分是平均流行度 $\\bar{\\kappa}$。因此，我们得到 $\\mathbb{E}[k_i|\\kappa_i] \\approx C_\\beta \\kappa_i$，其中常数 $C_\\beta$ 是：\n$$ C_\\beta = 2\\mu \\bar{\\kappa} \\frac{\\pi/\\beta}{\\sin(\\pi/\\beta)} $$\n\n**第3部分：$\\kappa_i$ 的最大似然估计量**\n\n最大似然估计通过将对数似然函数的导数设为零来获得。从第1部分的结果：\n$$ \\frac{\\partial \\mathcal{L}}{\\partial r_i} = 0 \\implies k_i - \\sum_{j \\neq i} p_{ij} = 0 \\implies k_i = \\sum_{j \\neq i} p_{ij} $$\n在平均场意义上，我们将 $\\sum p_{ij}$ 替换为其期望值，即第2部分推导的期望度 $\\mathbb{E}[k_i|\\kappa_i]$。\n$$ k_i \\approx \\mathbb{E}[k_i|\\kappa_i] = C_\\beta \\kappa_i $$\n问题进一步规定，参数 $\\mu$ 被校准以使得“$\\mathbb{E}[k_i|\\kappa_i] = \\kappa_i$ 在网络上平均成立”。这个条件通常意味着设置 $C_\\beta = 1$。\n当 $C_\\beta=1$ 时，我们有 $\\mathbb{E}[k_i|\\kappa_i] = \\kappa_i$。将这个校准条件代入 MLE 方程 $k_i \\approx \\mathbb{E}[k_i|\\kappa_i]$，我们直接得到：\n$$ k_i \\approx \\kappa_i $$\n因此，$\\kappa_i$ 的最大似然估计量是：\n$$ \\hat{\\kappa}_i = k_i $$\n这意味着，在经过校准的模型中，一个节点的观测度是其隐藏流行度参数的最佳估计。",
            "answer": "$$\\boxed{k_{i}}$$"
        },
        {
            "introduction": "一旦建立了模型，我们便可以探索其动态过程，例如网络导航。这个练习将重点转向贪心路由（greedy routing）的效率，这是利用隐藏几何进行导航的主要策略 。通过计算“路由伸展”（routing stretch）$s(\\gamma,T)$，你将量化贪心路径与最优最短路径的差距，并理解其效率如何依赖于模型的关键参数，如度异质性指数 $\\gamma$ 和连接温度 $T$。",
            "id": "4289373",
            "problem": "考虑一类由单位周长圆上的隐藏度量空间模型生成的空间嵌入随机网络。每个节点被分配一个角坐标和一个隐藏度参数。隐藏度参数独立地从指数为 $\\gamma2$ 的幂律分布中抽取。两个节点之间以某一概率连接一条无向边，该概率是其角距离的减函数，是其隐藏度之积的增函数。具体来说，假设连接概率具有软阈值形式\n$$\np\\!\\left(d;\\kappa,\\kappa'\\right) \\;=\\; \\frac{1}{1+\\left(\\frac{d}{\\mu\\,\\kappa\\,\\kappa'}\\right)^{1/T}},\n$$\n其中 $d$ 是圆上的角距离，$\\kappa$ 和 $\\kappa'$ 是两个端点的隐藏度，$\\mu0$ 是一个固定的尺度，而 $T\\in[0,1)$ 是一个控制距离阈值软度的无量纲温度。该模型在低 $T$ 值时会产生一个具有高聚类性的连通稀疏网络，以及一个指数为 $\\gamma$ 的幂律度分布。\n\n将测地长度 $\\ell_{\\mathrm{geo}}(N;\\gamma)$ 定义为在大小为 $N$ 的网络中两个均匀随机节点之间最短路径的期望长度，并将贪婪长度 $\\ell_{\\mathrm{gr}}(N;\\gamma,T)$ 定义为贪婪路由中的期望跳数，该路由在每一步都将信息转发到使得到目标的隐藏度量距离最小的邻居节点。假设对于 $T\\in[0,1)$，当 $N\\to\\infty$ 时，贪婪路由成功的概率趋近于 $1$。\n\n使用以下关于具有隐藏度量空间和幂律度分布的网络的广泛观察到的渐近事实作为您推导的基础：\n- 对于大的 $N$，测地长度具有以下形式\n$$\n\\ell_{\\mathrm{geo}}(N;\\gamma)\\;\\sim\\;\\frac{C(\\gamma)}{H(\\gamma)}\\,g(N),\n$$\n其中 $g(N)$ 是 $N$ 的一个增函数，其形式取决于 $\\gamma\\in(2,3)$ 还是 $\\gamma3$（例如，$g(N)$ 可以是 $\\ln\\ln N$ 或 $\\ln N$ 阶），$C(\\gamma)$ 是一个依赖于 $\\gamma$ 的常数，而 $H(\\gamma)$ 是一个正函数，捕捉了由度异质性驱动的隐藏距离间隙的有效收缩率。对于幂律隐藏度，取 $H(\\gamma)=\\gamma-2$。\n- 由 $T0$ 引起的软阈值在每次跳跃中，相对于异质性控制的基线 $H(\\gamma)$，引入了一个与 $T$ 成正比的隐藏距离收缩的附加低效率。在大 $N$ 极限下，这导致了以下形式的渐近贪婪长度\n$$\n\\ell_{\\mathrm{gr}}(N;\\gamma,T)\\;\\sim\\;\\frac{C(\\gamma)}{H(\\gamma)}\\left(1+\\frac{T}{H(\\gamma)}\\right)g(N),\n$$\n直到在 $N\\to\\infty$ 时，在比率 $\\ell_{\\mathrm{gr}}/\\ell_{\\mathrm{geo}}$ 中消失的次导项修正。\n\n将路由伸展定义为大网络极限\n$$\ns(\\gamma,T)\\;=\\;\\lim_{N\\to\\infty}\\frac{\\ell_{\\mathrm{gr}}(N;\\gamma,T)}{\\ell_{\\mathrm{geo}}(N;\\gamma)}.\n$$\n\n仅使用给定的假设和渐近形式，推导 $s(\\gamma,T)$ 作为 $\\gamma$ 和 $T$ 的函数的封闭形式解析表达式。您的最终答案必须是一个单一的简化解析表达式。除了 $g(N)$ 在 $\\ell_{\\mathrm{geo}}$ 和 $\\ell_{\\mathrm{gr}}$ 中是相同的函数并且上述极限存在这一事实之外，不要假设 $g(N)$ 的任何特定形式。不需要四舍五入，也不涉及物理单位。",
            "solution": "该问题要求推导路由伸展 $s(\\gamma,T)$ 的封闭形式解析表达式，其定义为在大网络规模的极限下，期望贪婪路径长度与期望测地路径长度的比率。\n\n分析从问题陈述中提供的路由伸展的形式化定义开始：\n$$\ns(\\gamma,T) \\;=\\; \\lim_{N\\to\\infty}\\frac{\\ell_{\\mathrm{gr}}(N;\\gamma,T)}{\\ell_{\\mathrm{geo}}(N;\\gamma)}\n$$\n此处，$\\ell_{\\mathrm{gr}}(N;\\gamma,T)$ 是贪婪路由的期望跳数，$\\ell_{\\mathrm{geo}}(N;\\gamma)$ 是期望测地路径长度，$N$ 是网络中的节点数，$\\gamma$ 是幂律隐藏度分布的指数，而 $T$ 是控制连接概率的温度参数。\n\n问题提供了当 $N \\to \\infty$ 时两种路径长度的渐近形式。这些是推导的基本输入。\n\n期望测地长度由下式给出：\n$$\n\\ell_{\\mathrm{geo}}(N;\\gamma)\\;\\sim\\;\\frac{C(\\gamma)}{H(\\gamma)}\\,g(N)\n$$\n期望贪婪路由长度由下式给出：\n$$\n\\ell_{\\mathrm{gr}}(N;\\gamma,T)\\;\\sim\\;\\frac{C(\\gamma)}{H(\\gamma)}\\left(1+\\frac{T}{H(\\gamma)}\\right)g(N)\n$$\n符号 $f(N) \\sim h(N)$ 表示渐近等价，即 $\\lim_{N\\to\\infty} f(N)/h(N) = 1$。涉及渐近形式的极限的一个关键性质是，两个函数之比的极限等于其领头阶渐近表达式的比值，前提是极限存在且分母的领头项非零。问题陈述通过指出次导项修正量在比率中消失来保证了这一点。\n\n我们将这些渐近表达式代入 $s(\\gamma,T)$ 的定义中：\n$$\ns(\\gamma,T) \\;=\\; \\lim_{N\\to\\infty} \\frac{\\frac{C(\\gamma)}{H(\\gamma)}\\left(1+\\frac{T}{H(\\gamma)}\\right)g(N)}{\\frac{C(\\gamma)}{H(\\gamma)}\\,g(N)}\n$$\n极限内的表达式可以被简化。因子 $\\frac{C(\\gamma)}{H(\\gamma)}$ 和 $g(N)$ 同时存在于分子和分母中。问题陈述指出，$H(\\gamma)$ 是一个正函数，$C(\\gamma)$ 是一个常数，而 $g(N)$ 是 $N$ 的一个增函数，这确保了对于大的 $N$ 这些项是非零的。因此，我们可以消去这些公因式：\n$$\ns(\\gamma,T) \\;=\\; \\lim_{N\\to\\infty} \\left(1+\\frac{T}{H(\\gamma)}\\right)\n$$\n极限内剩余的表达式 $1+\\frac{T}{H(\\gamma)}$，相对于极限变量 $N$ 是一个常数。常数的极限就是其本身。这得到：\n$$\ns(\\gamma,T) \\;=\\; 1+\\frac{T}{H(\\gamma)}\n$$\n问题为函数 $H(\\gamma)$ 提供了一个特定的解析形式，该函数捕捉了度异质性对距离收缩的影响：\n$$\nH(\\gamma) = \\gamma - 2\n$$\n约束 $\\gamma2$ 确保了 $H(\\gamma)0$。将 $H(\\gamma)$ 的这个表达式代入 $s(\\gamma,T)$ 的方程中，得到最终的封闭形式结果：\n$$\ns(\\gamma,T) \\;=\\; 1+\\frac{T}{\\gamma-2}\n$$\n这个最终表达式仅是 $\\gamma$ 和 $T$ 的函数，符合要求，并且代表了由于温度 $T$ 引入的路由低效率，贪婪路径长度相对于最优测地路径被拉伸的乘性因子。当 $T=0$ 时，伸展为 $1$（最优路由），并随 $T$ 线性增加。当 $\\gamma$ 增加时，它会减小，这对应于一个更陡峭（异质性更低）的度分布，从而降低了高度节点作为路由枢纽的有效性。",
            "answer": "$$\n\\boxed{1+\\frac{T}{\\gamma-2}}\n$$"
        },
        {
            "introduction": "现实世界的应用必须考虑数据的不完美性，例如节点坐标估计中的噪声。本练习探讨了这种不确定性对导航决策的实际影响 。通过为单步导航误差推导确定性上界和概率性保证，你将深入理解贪心路由的鲁棒性，并学会使用集中不等式来分析在噪声存在下的性能。",
            "id": "4289366",
            "problem": "考虑一个网络，其节点嵌入在一个隐藏的欧几里得度量空间中，坐标位于 $\\mathbb{R}^{d}$，其中 $d \\in \\mathbb{N}$ 是一个固定的值。源节点 $i$ 必须通过贪心邻居选择将消息转发到目标节点 $t$，即在节点 $i$ 处，它选择能最小化到 $t$ 的估计距离的邻居 $j \\in \\mathcal{N}(i)$。设节点 $j$ 和目标 $t$ 的真实坐标为 $x_{j}$ 和 $x_{t}$，真实距离为 $D_{j} = \\|x_{j} - x_{t}\\|_{2}$。设可用的估计坐标为 $\\hat{x}_{j}$ 和 $\\hat{x}_{t}$，其坐标估计误差一致有界，即 $\\|\\hat{x}_{j} - x_{j}\\|_{2} \\leq \\delta$ 和 $\\|\\hat{x}_{t} - x_{t}\\|_{2} \\leq \\delta$，其中 $\\delta  0$ 是一个已知值。贪心规则使用估计距离 $\\hat{D}_{j} = \\|\\hat{x}_{j} - \\hat{x}_{t}\\|_{2}$ 来选择转发邻居。\n\n将单步导航误差定义为 $E_{\\text{step}} = D_{\\text{sel}} - \\min_{j \\in \\mathcal{N}(i)} D_{j}$，其中 $D_{\\text{sel}}$ 是根据估计距离选择的邻居的真实距离。\n\n任务1：仅使用欧几里得度量的核心性质和三角不等式，推导 $E_{\\text{step}}$ 的一个确定性上界，该上界仅以 $\\delta$ 表示。\n\n任务2：现在假设坐标估计是通过对 $m$ 个独立同分布 (i.i.d.) 的有界扰动进行平均得到的。具体来说，对于每个邻居 $j$，估计距离的形式为 $\\hat{D}_{j} = D_{j} + \\bar{\\xi}_{j}$，其中 $\\bar{\\xi}_{j} = \\frac{1}{m}\\sum_{\\ell=1}^{m}\\xi_{j\\ell}$，$\\xi_{j\\ell}$ 是均值为 $0$ 且支撑集在区间 $[-b, b]$ 内的独立同分布随机变量，其中 $b  0$ 是一个已知值。设最近邻居和次近邻居的真实距离之间的差值为 $\\gamma = D_{(2)} - D_{(1)}  0$，其中 $D_{(1)} = \\min_{j \\in \\mathcal{N}(i)} D_{j}$，$D_{(2)}$ 是第二小的距离。使用适用于有界变量的适当集中不等式，并结合对最近邻居的所有竞争者的并集界，推导出一个关于 $m$ 的充分条件，以保证根据 $\\hat{D}_{j}$ 选择的邻居确实是最近邻居的概率至少为 $1 - \\alpha$，其中 $\\alpha \\in (0,1)$ 是给定值。\n\n最后，在 $b = 0.5$、$\\gamma = 0.8$、 $|\\mathcal{N}(i)| = 20$ 和 $\\alpha = 0.01$ 的情况下评估您的充分条件，并报告满足该条件的最小整数 $m$ 作为最终答案。",
            "solution": "该问题陈述经确认为具有科学依据、问题明确、客观且完整。它提出了在不确定性下分析网络导航算法的两个标准、可形式化的任务。所有必要的参数和定义都已提供，没有矛盾或歧义。该问题是将基本数学原理（三角不等式、集中不等式）应用于网络科学问题的有效练习。\n\n任务1：推导 $E_{\\text{step}}$ 的确定性上界。\n\n设 $x_j, x_t \\in \\mathbb{R}^{d}$ 分别是邻居节点 $j$ 和目标节点 $t$ 的真实坐标。真实距离是 $D_j = \\|x_j - x_t\\|_2$。估计坐标是 $\\hat{x}_j$ 和 $\\hat{x}_t$，估计误差有界，即 $\\|\\hat{x}_j - x_j\\|_2 \\leq \\delta$ 和 $\\|\\hat{x}_t - x_t\\|_2 \\leq \\delta$。估计距离是 $\\hat{D}_j = \\|\\hat{x}_j - \\hat{x}_t\\|_2$。\n\n我们首先对任意节点 $j$ 建立真实距离 $D_j$ 和估计距离 $\\hat{D}_j$ 之间的关系。根据欧几里得范数 $\\|\\cdot\\|_2$ 的三角不等式：\n$$ \\hat{D}_j = \\|\\hat{x}_j - \\hat{x}_t\\|_2 = \\|(\\hat{x}_j - x_j) + (x_j - x_t) + (x_t - \\hat{x}_t)\\|_2 \\leq \\|\\hat{x}_j - x_j\\|_2 + \\|x_j - x_t\\|_2 + \\|x_t - \\hat{x}_t\\|_2 $$\n使用给定的误差界，我们有：\n$$ \\hat{D}_j \\leq \\delta + D_j + \\delta = D_j + 2\\delta $$\n同样地，我们可以用 $\\hat{D}_j$ 来表示 $D_j$：\n$$ D_j = \\|x_j - x_t\\|_2 = \\|(x_j - \\hat{x}_j) + (\\hat{x}_j - \\hat{x}_t) + (\\hat{x}_t - x_t)\\|_2 \\leq \\|x_j - \\hat{x}_j\\|_2 + \\|\\hat{x}_j - \\hat{x}_t\\|_2 + \\|\\hat{x}_t - x_t\\|_2 $$\n$$ D_j \\leq \\delta + \\hat{D}_j + \\delta = \\hat{D}_j + 2\\delta $$\n结合这两个不等式，得到 $|\\hat{D}_j - D_j| \\leq 2\\delta$。\n\n单步导航误差定义为 $E_{\\text{step}} = D_{\\text{sel}} - \\min_{j \\in \\mathcal{N}(i)} D_{j}$。设 $j^* \\in \\mathcal{N}(i)$ 是距离目标 $t$ 真正最近的邻居，使得 $D_{j^*} = \\min_{j \\in \\mathcal{N}(i)} D_j$。设 $j_{\\text{sel}} \\in \\mathcal{N}(i)$ 是由贪心算法选择的邻居，这意味着它最小化了估计距离：$\\hat{D}_{j_{\\text{sel}}} = \\min_{j \\in \\mathcal{N}(i)} \\hat{D}_j$。根据定义，$D_{\\text{sel}} = D_{j_{\\text{sel}}}$。\n\n为了找到 $E_{\\text{step}} = D_{j_{\\text{sel}}} - D_{j^*}$ 的上界，我们按以下步骤进行：\n1. 从 $D_j \\leq \\hat{D}_j + 2\\delta$ 出发，对所选邻居 $j_{\\text{sel}}$ 得出：$D_{j_{\\text{sel}}} \\leq \\hat{D}_{j_{\\text{sel}}} + 2\\delta$。\n2. 根据贪心选择的定义，$\\hat{D}_{j_{\\text{sel}}} \\leq \\hat{D}_{j^*}$。\n3. 从 $\\hat{D}_j \\leq D_j + 2\\delta$ 出发，对邻居 $j^*$ 得出：$\\hat{D}_{j^*} \\leq D_{j^*} + 2\\delta$。\n结合这些步骤：\n$$ D_{j_{\\text{sel}}} \\leq \\hat{D}_{j_{\\text{sel}}} + 2\\delta \\leq \\hat{D}_{j^*} + 2\\delta \\leq (D_{j^*} + 2\\delta) + 2\\delta = D_{j^*} + 4\\delta $$\n因此，单步导航误差的有界为：\n$$ E_{\\text{step}} = D_{j_{\\text{sel}}} - D_{j^*} \\leq 4\\delta $$\n$E_{\\text{step}}$ 的确定性上界是 $4\\delta$。\n\n任务2：推导关于 $m$ 的充分条件。\n\n设 $j_{(1)}$ 是具有最小真实距离 $D_{(1)}$ 的邻居。如果对于所有其他邻居 $j \\in \\mathcal{N}(i) \\setminus \\{j_{(1)}\\}$ 都有 $\\hat{D}_{j_{(1)}}  \\hat{D}_j$，那么就选择了正确的邻居。\n导航失败发生在存在至少一个邻居 $j \\neq j_{(1)}$ 使得 $\\hat{D}_j \\leq \\hat{D}_{j_{(1)}}$ 时。\n设 $N = |\\mathcal{N}(i)|$。使用并集界 (union bound)，失败的概率可以由单个错误事件的概率之和来界定：\n$$ P(\\text{failure}) = P\\left(\\bigcup_{j \\neq j_{(1)}} \\{\\hat{D}_j \\leq \\hat{D}_{j_{(1)}}\\}\\right) \\leq \\sum_{j \\neq j_{(1)}} P(\\hat{D}_j \\leq \\hat{D}_{j_{(1)}}) $$\n分析条件 $\\hat{D}_j \\leq \\hat{D}_{j_{(1)}}$。代入模型 $\\hat{D}_k = D_k + \\bar{\\xi}_k$：\n$ D_j + \\bar{\\xi}_j \\leq D_{j_{(1)}} + \\bar{\\xi}_{j_{(1)}} \\implies \\bar{\\xi}_{j_{(1)}} - \\bar{\\xi}_j \\geq D_j - D_{j_{(1)}} $。\n由于 $D_j - D_{j_{(1)}} \\geq D_{(2)} - D_{(1)} = \\gamma$，失败事件意味着 $\\bar{\\xi}_{j_{(1)}} - \\bar{\\xi}_j \\geq \\gamma$。\n变量 $Y_\\ell = \\xi_{j_{(1)}\\ell} - \\xi_{j\\ell}$ 均值为 $0$ 且支撑集在 $[-2b, 2b]$ 内。我们可以对均值 $\\bar{Y} = \\frac{1}{m}\\sum Y_\\ell = \\bar{\\xi}_{j_{(1)}} - \\bar{\\xi}_j$ 应用霍夫丁不等式 (Hoeffding's inequality)。\n$$ P(\\bar{Y} \\ge \\gamma) \\leq \\exp\\left( - \\frac{2m^2\\gamma^2}{\\sum_{\\ell=1}^m (4b)^2} \\right) = \\exp\\left( - \\frac{2m^2\\gamma^2}{m(16b^2)} \\right) = \\exp\\left( - \\frac{m\\gamma^2}{8b^2} \\right) $$\n对 $j_{(1)}$ 的 $N-1$ 个竞争者应用并集界：\n$$ P(\\text{failure}) \\leq \\sum_{j \\neq j_{(1)}} P(\\dots) \\leq (N-1) \\exp\\left( - \\frac{m\\gamma^2}{8b^2} \\right) $$\n我们要求成功的概率至少为 $1-\\alpha$，即 $P(\\text{failure}) \\leq \\alpha$。一个充分条件是：\n$$ (N-1) \\exp\\left( - \\frac{m\\gamma^2}{8b^2} \\right) \\leq \\alpha $$\n解出 $m$：\n$$ m \\geq \\frac{8b^2}{\\gamma^2} \\ln\\left(\\frac{N-1}{\\alpha}\\right) $$\n这就是关于 $m$ 的充分条件。\n\n最后，代入数值 $b = 0.5$、$\\gamma = 0.8$、$N = 20$ 和 $\\alpha = 0.01$：\n$$ m \\geq \\frac{8(0.5)^2}{(0.8)^2} \\ln\\left(\\frac{20-1}{0.01}\\right) = \\frac{8(0.25)}{0.64} \\ln\\left(\\frac{19}{0.01}\\right) = \\frac{2}{0.64} \\ln(1900) = 3.125 \\ln(1900) $$\n使用 $\\ln(1900) \\approx 7.5496$：\n$$ m \\geq 3.125 \\times 7.5496 \\approx 23.5925 $$\n由于 $m$ 必须是整数，满足此条件的最小整数 $m$ 是 $24$。\n$$ m_{\\min} = 24 $$",
            "answer": "$$\\boxed{24}$$"
        }
    ]
}