{
    "hands_on_practices": [
        {
            "introduction": "The power of Graph Neural Networks stems from the message passing paradigm, where nodes iteratively update their states by exchanging information with their neighbors. This first exercise breaks down the GNN update into its three essential components: message creation ($\\psi$), aggregation ($\\square$), and update ($\\phi$). By calculating a single node's new state on a simple path graph, you will gain a concrete, first-principles understanding of how information from a node's local neighborhood is integrated to form a new, richer representation .",
            "id": "4287364",
            "problem": "Consider a path graph on $3$ nodes with vertex set $\\{1,2,3\\}$ and undirected edges $\\{(1,2),(2,3)\\}$, each with unit edge weight. There are no self-loops. Each node $v$ carries a scalar feature $h_{v}^{(0)} \\in \\mathbb{R}$. A single message-passing layer of a Graph Neural Network (GNN) is applied in the following sense: messages are sent along edges from neighbors $u \\in \\mathcal{N}(v)$ to node $v$, aggregated by a permutation-invariant operator, and then used to update the state at node $v$. The neighbor set $\\mathcal{N}(v)$ is the open neighborhood of $v$. The message function is specified by $\\psi(h_{v}^{(t)},h_{u}^{(t)},e_{uv}) = h_{u}^{(t)}$, the aggregation operator is the sum $\\square = \\sum$, and the node update function is $\\phi(h_{v}^{(t)},m_{v}) = \\sigma\\!\\left(\\alpha\\,h_{v}^{(t)} + \\beta\\,m_{v}\\right)$, where $\\alpha \\in \\mathbb{R}$ and $\\beta \\in \\mathbb{R}$ are fixed parameters and $\\sigma:\\mathbb{R}\\to\\mathbb{R}$ is a fixed nonlinear activation. Using only these specifications and the core definition of message passing on graphs as an aggregation over neighbors followed by a local update, derive the closed-form expression for the updated state $h_{2}^{(1)}$ at node $2$ in terms of $\\alpha$, $\\beta$, $\\sigma$, $h_{1}^{(0)}$, $h_{2}^{(0)}$, and $h_{3}^{(0)}$. Express your final answer as a single simplified analytic expression. No rounding is required and no physical units are involved.",
            "solution": "The problem statement is formally valid, scientifically grounded, and well-posed. All components required for a unique solution are provided. We proceed with the derivation.\n\nThe process of updating a node's feature vector in a Graph Neural Network (GNN) layer is defined by a message-passing scheme. For any node $v$, its feature vector $h_v^{(t)}$ at layer $t$ is updated to $h_v^{(t+1)}$ at layer $t+1$ through a three-step process: message creation, message aggregation, and node update.\n\nThe general form of this update can be written as:\n$$h_{v}^{(t+1)} = \\phi\\left(h_{v}^{(t)}, m_{v}^{(t+1)}\\right)$$\nwhere $m_{v}^{(t+1)}$ is the aggregated message from the neighborhood of $v$, computed as:\n$$m_{v}^{(t+1)} = \\square_{u \\in \\mathcal{N}(v)} \\psi\\left(h_{v}^{(t)}, h_{u}^{(t)}, e_{uv}\\right)$$\n\nThe problem provides specific definitions for the components of this framework:\n1.  The message function $\\psi$ is given by $\\psi(h_{v}^{(t)}, h_{u}^{(t)}, e_{uv}) = h_{u}^{(t)}$. This means the message sent from a neighbor $u$ to a node $v$ is simply the feature vector of the neighbor $u$ at layer $t$.\n2.  The aggregation operator $\\square$ is the summation operator, $\\sum$.\n3.  The node update function $\\phi$ is given by $\\phi(h_{v}^{(t)}, m_{v}) = \\sigma(\\alpha h_{v}^{(t)} + \\beta m_{v})$, where $\\alpha$ and $\\beta$ are scalar parameters and $\\sigma$ is a nonlinear activation function.\n\nFirst, let's construct the specific formula for the aggregated message $m_{v}^{(t+1)}$. By substituting the given message function and aggregation operator into the general formula, we get:\n$$m_{v}^{(t+1)} = \\sum_{u \\in \\mathcal{N}(v)} h_{u}^{(t)}$$\nThis means the aggregated message for node $v$ is the sum of the feature vectors of its neighbors.\n\nNext, we substitute this aggregated message into the node update function to obtain the complete update rule for any node $v$:\n$$h_{v}^{(t+1)} = \\sigma\\left(\\alpha h_{v}^{(t)} + \\beta \\left(\\sum_{u \\in \\mathcal{N}(v)} h_{u}^{(t)}\\right)\\right)$$\n\nThe problem asks for the updated state $h_{2}^{(1)}$ for node $v=2$ after one layer of message passing. This corresponds to an update from the initial features at layer $t=0$ to the new features at layer $t+1=1$. Applying the update rule for $v=2$ and $t=0$:\n$$h_{2}^{(1)} = \\sigma\\left(\\alpha h_{2}^{(0)} + \\beta \\left(\\sum_{u \\in \\mathcal{N}(2)} h_{u}^{(0)}\\right)\\right)$$\n\nThe final step is to identify the neighborhood $\\mathcal{N}(2)$ of node $2$. The graph is a path graph on the vertex set $\\{1, 2, 3\\}$ with edges $\\{(1,2), (2,3)\\}$. The neighbors of a node are the nodes it is directly connected to by an edge. For node $2$, the edges are $(1,2)$ and $(2,3)$. Therefore, the open neighborhood of node $2$ is $\\mathcal{N}(2) = \\{1, 3\\}$.\n\nNow we can compute the sum over the neighbors of node $2$:\n$$\\sum_{u \\in \\mathcal{N}(2)} h_{u}^{(0)} = h_{1}^{(0)} + h_{3}^{(0)}$$\n\nSubstituting this sum back into the expression for $h_{2}^{(1)}$, we arrive at the final closed-form expression:\n$$h_{2}^{(1)} = \\sigma\\left(\\alpha h_{2}^{(0)} + \\beta (h_{1}^{(0)} + h_{3}^{(0)})\\right)$$\nThis expression gives the updated feature of node $2$ in terms of the initial features of itself and its neighbors, and the parameters of the GNN layer, as required.",
            "answer": "$$\\boxed{\\sigma(\\alpha h_{2}^{(0)} + \\beta (h_{1}^{(0)} + h_{3}^{(0)}))}$$"
        },
        {
            "introduction": "GNNs pass information along the edges of a graph, but what happens when the path for that information is narrow? This practice explores the concept of a graph \"bottleneck\" by calculating the impact of removing a critical bridge edge on the output of a two-layer GCN . The result provides a quantitative look at the \"over-squashing\" problem, where information from distant nodes gets exponentially diluted as it is compressed through a limited number of pathways, revealing a key limitation of the message passing framework.",
            "id": "4287359",
            "problem": "Consider a connected undirected graph with nodes labeled $1,2,3,4,5,6$ and edges $\\{(1,2),(2,3),(3,4),(4,5),(5,6)\\}$. The edge $(3,4)$ is a bridge that, if removed, disconnects the graph into two components $\\{1,2,3\\}$ and $\\{4,5,6\\}$. Let $\\mathbf{A}$ be the adjacency matrix of this graph and let $\\tilde{\\mathbf{A}} = \\mathbf{A} + \\mathbf{I}$ be the adjacency matrix with self-loops. Let $\\tilde{\\mathbf{D}}$ be the diagonal degree matrix of $\\tilde{\\mathbf{A}}$, and define the symmetric normalization operator $\\mathbf{S} = \\tilde{\\mathbf{D}}^{-1/2} \\tilde{\\mathbf{A}} \\tilde{\\mathbf{D}}^{-1/2}$.\n\nA two-layer Graph Convolutional Network (GCN) is defined by the layer-wise propagation rule $\\mathbf{H}^{(l+1)} = \\sigma\\!\\left(\\mathbf{S}\\,\\mathbf{H}^{(l)}\\,\\mathbf{W}^{(l)}\\right)$, where $\\sigma$ is the activation function, $\\mathbf{W}^{(l)}$ are trainable weight matrices, and $\\mathbf{H}^{(0)} = \\mathbf{X}$ is the input feature matrix. Consider scalar node features with $\\mathbf{X} \\in \\mathbb{R}^{6 \\times 1}$ and take $\\sigma$ to be the identity function, $\\mathbf{W}^{(0)} = [1]$, and $\\mathbf{W}^{(1)} = [1]$. The input features are specified as $x_1 = 1$, $x_2 = 1$, $x_3 = 1$, and $x_4 = x_5 = x_6 = 0$.\n\nUnder these conditions, the two-layer output at node $5$ is $h^{(2)}_5$ computed on the original graph with the bridge edge $(3,4)$ present. Now consider the modified graph obtained by removing the bridge edge $(3,4)$; denote the corresponding operator by $\\mathbf{S}'$ and the two-layer output at node $5$ by $h'^{(2)}_5$.\n\nStarting from core definitions of message passing and normalized adjacency, derive the two-layer outputs and compute the change due to bridge removal, defined as $\\Delta = h^{(2)}_5 - h'^{(2)}_5$. Provide an analytical value for $\\Delta$ in exact form. Then, explain how this calculation quantitatively reflects the concepts of over-squashing and bottleneck sensitivity in message passing on graphs.\n\nYour final answer must be the single exact value of $\\Delta$ with no units. No rounding is required. Express all intermediate derivations using mathematical notation.",
            "solution": "The problem asks for the change in the two-layer Graph Convolutional Network (GCN) output at node $5$ when a bridge edge is removed from a path graph. This change, denoted by $\\Delta$, will be calculated by first determining the output for the original graph, $h^{(2)}_5$, and then for the modified graph, $h'^{(2)}_5$. Finally, we will analyze this result in the context of message passing, bottlenecks, and over-squashing in GNNs.\n\nThe GCN propagation rule is given as $\\mathbf{H}^{(l+1)} = \\sigma\\!\\left(\\mathbf{S}\\,\\mathbf{H}^{(l)}\\,\\mathbf{W}^{(l)}\\right)$. With the activation function $\\sigma$ being the identity and the weights $\\mathbf{W}^{(0)} = [1]$ and $\\mathbf{W}^{(1)} = [1]$, the rule simplifies to $\\mathbf{H}^{(l+1)} = \\mathbf{S}\\,\\mathbf{H}^{(l)}$. For a two-layer network, the final output is $\\mathbf{H}^{(2)} = \\mathbf{S}\\,\\mathbf{H}^{(1)} = \\mathbf{S}(\\mathbf{S}\\,\\mathbf{H}^{(0)}) = \\mathbf{S}^2\\,\\mathbf{H}^{(0)}$, where $\\mathbf{H}^{(0)} = \\mathbf{X}$ is the input feature matrix.\n\n**Part 1: Analysis of the Original Graph**\n\nFirst, we define the matrices for the original graph with nodes $V=\\{1,2,3,4,5,6\\}$ and edges $E = \\{(1,2),(2,3),(3,4),(4,5),(5,6)\\}$.\n\nThe adjacency matrix $\\mathbf{A}$ is:\n$$ \\mathbf{A} = \\begin{pmatrix} 0 & 1 & 0 & 0 & 0 & 0 \\\\ 1 & 0 & 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 1 & 0 & 1 \\\\ 0 & 0 & 0 & 0 & 1 & 0 \\end{pmatrix} $$\nThe adjacency matrix with self-loops, $\\tilde{\\mathbf{A}} = \\mathbf{A} + \\mathbf{I}$, is:\n$$ \\tilde{\\mathbf{A}} = \\begin{pmatrix} 1 & 1 & 0 & 0 & 0 & 0 \\\\ 1 & 1 & 1 & 0 & 0 & 0 \\\\ 0 & 1 & 1 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 1 & 1 & 0 \\\\ 0 & 0 & 0 & 1 & 1 & 1 \\\\ 0 & 0 & 0 & 0 & 1 & 1 \\end{pmatrix} $$\nThe degree matrix $\\tilde{\\mathbf{D}}$ contains the degrees of nodes in the graph represented by $\\tilde{\\mathbf{A}}$ (i.e., $\\tilde{d}_i = 1 + \\deg(i)$). The diagonal entries are $\\tilde{d}_1=2$, $\\tilde{d}_2=3$, $\\tilde{d}_3=3$, $\\tilde{d}_4=3$, $\\tilde{d}_5=3$, $\\tilde{d}_6=2$.\n$$ \\tilde{\\mathbf{D}} = \\mathrm{diag}(2, 3, 3, 3, 3, 2) $$\nThe symmetric normalization operator $\\mathbf{S} = \\tilde{\\mathbf{D}}^{-1/2} \\tilde{\\mathbf{A}} \\tilde{\\mathbf{D}}^{-1/2}$ has elements $S_{ij} = \\frac{\\tilde{A}_{ij}}{\\sqrt{\\tilde{d}_i \\tilde{d}_j}}$.\n$$ \\mathbf{S} = \\begin{pmatrix} \\frac{1}{2} & \\frac{1}{\\sqrt{6}} & 0 & 0 & 0 & 0 \\\\ \\frac{1}{\\sqrt{6}} & \\frac{1}{3} & \\frac{1}{3} & 0 & 0 & 0 \\\\ 0 & \\frac{1}{3} & \\frac{1}{3} & \\frac{1}{3} & 0 & 0 \\\\ 0 & 0 & \\frac{1}{3} & \\frac{1}{3} & \\frac{1}{3} & 0 \\\\ 0 & 0 & 0 & \\frac{1}{3} & \\frac{1}{3} & \\frac{1}{\\sqrt{6}} \\\\ 0 & 0 & 0 & 0 & \\frac{1}{\\sqrt{6}} & \\frac{1}{2} \\end{pmatrix} $$\nThe input feature vector is $\\mathbf{H}^{(0)} = [1, 1, 1, 0, 0, 0]^T$.\n\nWe need to compute $h^{(2)}_5$, which is the fifth element of $\\mathbf{H}^{(2)} = \\mathbf{S}^2 \\mathbf{H}^{(0)}$. Let's compute this step-by-step.\nFirst, $\\mathbf{H}^{(1)} = \\mathbf{S} \\mathbf{H}^{(0)}$. The $i$-th element is $h^{(1)}_i = \\sum_{j} S_{ij} h^{(0)}_j$.\n$$ h^{(1)}_4 = S_{4,1}x_1 + S_{4,2}x_2 + S_{4,3}x_3 + S_{4,4}x_4 + S_{4,5}x_5 + S_{4,6}x_6 = 0 \\cdot 1 + 0 \\cdot 1 + \\frac{1}{3} \\cdot 1 + \\frac{1}{3} \\cdot 0 + \\frac{1}{3} \\cdot 0 + 0 \\cdot 0 = \\frac{1}{3} $$\n$$ h^{(1)}_5 = S_{5,3}x_3 + S_{5,4}x_4 + S_{5,5}x_5 + S_{5,6}x_6 = 0 \\cdot 1 + \\frac{1}{3} \\cdot 0 + \\frac{1}{3} \\cdot 0 + \\frac{1}{\\sqrt{6}} \\cdot 0 = 0 $$\n$$ h^{(1)}_6 = S_{6,5}x_5 + S_{6,6}x_6 = \\frac{1}{\\sqrt{6}} \\cdot 0 + \\frac{1}{2} \\cdot 0 = 0 $$\nNow, we compute $\\mathbf{H}^{(2)} = \\mathbf{S} \\mathbf{H}^{(1)}$. We only need the fifth element, $h^{(2)}_5$.\n$$ h^{(2)}_5 = S_{5,4}h^{(1)}_4 + S_{5,5}h^{(1)}_5 + S_{5,6}h^{(1)}_6 = \\frac{1}{3} \\cdot h^{(1)}_4 + \\frac{1}{3} \\cdot h^{(1)}_5 + \\frac{1}{\\sqrt{6}} \\cdot h^{(1)}_6 $$\nSubstituting the values for $\\mathbf{H}^{(1)}$:\n$$ h^{(2)}_5 = \\frac{1}{3} \\cdot \\frac{1}{3} + \\frac{1}{3} \\cdot 0 + \\frac{1}{\\sqrt{6}} \\cdot 0 = \\frac{1}{9} $$\n\n**Part 2: Analysis of the Modified Graph**\n\nNext, we consider the graph with the bridge edge $(3,4)$ removed. The graph is now disconnected into two components: $\\{1,2,3\\}$ and $\\{4,5,6\\}$.\n\nThe new adjacency matrix $\\mathbf{A}'$ and its version with self-loops $\\tilde{\\mathbf{A}}'$ are block-diagonal:\n$$ \\tilde{\\mathbf{A}}' = \\begin{pmatrix} 1 & 1 & 0 & 0 & 0 & 0 \\\\ 1 & 1 & 1 & 0 & 0 & 0 \\\\ 0 & 1 & 1 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 1 & 1 & 0 \\\\ 0 & 0 & 0 & 1 & 1 & 1 \\\\ 0 & 0 & 0 & 0 & 1 & 1 \\end{pmatrix} $$\nThe new degrees are $\\tilde{d}'_1=2, \\tilde{d}'_2=3, \\tilde{d}'_3=2$ and $\\tilde{d}'_4=2, \\tilde{d}'_5=3, \\tilde{d}'_6=2$.\nThe new degree matrix is $\\tilde{\\mathbf{D}}' = \\mathrm{diag}(2, 3, 2, 2, 3, 2)$.\nThe new normalization operator $\\mathbf{S}'$ is also block-diagonal, reflecting the two components. There are no non-zero entries $S'_{ij}$ where $i \\in \\{1,2,3\\}$ and $j \\in \\{4,5,6\\}$, or vice-versa.\n\nThe output is $\\mathbf{H}'^{(2)} = (\\mathbf{S}')^2 \\mathbf{H}^{(0)}$. Due to the block-diagonal structure of $\\mathbf{S}'$, message passing is confined within each component. Nodes in $\\{4,5,6\\}$ can only aggregate information from other nodes in $\\{4,5,6\\}$.\nThe input features for this component are $x_4=0, x_5=0, x_6=0$.\nAt the first layer, for any node $i \\in \\{4,5,6\\}$, the output is:\n$$ h'^{(1)}_i = \\sum_{j \\in \\{4,5,6\\}} S'_{ij} x_j = \\sum_{j \\in \\{4,5,6\\}} S'_{ij} \\cdot 0 = 0 $$\nSince all first-layer outputs in this component are zero, the second-layer outputs must also be zero:\n$$ h'^{(2)}_i = \\sum_{j \\in \\{4,5,6\\}} S'_{ij} h'^{(1)}_j = \\sum_{j \\in \\{4,5,6\\}} S'_{ij} \\cdot 0 = 0 $$\nTherefore, the output at node $5$ for the modified graph is $h'^{(2)}_5 = 0$.\n\n**Part 3: Calculation of the Change**\n\nThe change $\\Delta$ is the difference between the two outputs:\n$$ \\Delta = h^{(2)}_5 - h'^{(2)}_5 = \\frac{1}{9} - 0 = \\frac{1}{9} $$\n\n**Part 4: Conceptual Interpretation**\n\nThis calculation provides a quantitative insight into how GNNs handle information flow across graph bottlenecks.\n1.  **Message Passing and Receptive Fields**: A GCN updates a node's feature vector by aggregating messages from its local neighborhood. After $L$ layers, a node's representation is influenced by the initial features of all nodes within a distance of $L$, its $L$-hop receptive field. In our problem with $L=2$, the output $h^{(2)}_5$ is a function of the initial features of nodes up to $2$ hops away from node $5$.\n\n2.  **Bottleneck Sensitivity**: In the original graph, the path from node $3$ to node $5$ is $3-4-5$, which has a length of $2$. Thus, the initial feature $x_3$ can influence the final representation $h^{(2)}_5$. The edge $(3,4)$ acts as a structural **bottleneck**; it is the only path for information to travel from the subgraph $\\{1,2,3\\}$ to the subgraph $\\{4,5,6\\}$. Our calculation shows that the influence of $x_3=1$ propagates across this bottleneck to node $5$, resulting in $h^{(2)}_5 = 1/9$. This value $\\Delta = 1/9$ precisely quantifies the signal from the first component that reaches node $5$ after two aggregation steps.\n\n3.  **Over-squashing**: The term **over-squashing** describes the problem where, as information is passed over many layers (long distances), it can be exponentially diluted or \"squashed\", especially when forced through a narrow bottleneck. The message passed from node $u$ to $v$ is scaled by a normalization constant, here $1/\\sqrt{\\tilde{d}_u \\tilde{d}_v}$. In our case, the signal from $x_3$ is first scaled by $1/\\sqrt{\\tilde{d}_4 \\tilde{d}_3} = 1/3$ to contribute to $h^{(1)}_4$. This intermediate signal at node $4$ is then passed to node $5$, scaled by $1/\\sqrt{\\tilde{d}_5 \\tilde{d}_4} = 1/3$. The total attenuation over the two hops gives the factor of $1/9$, which is a direct measure of the \"squashing\" of the signal as it traverses the path $3-4-5$.\n\nBy removing the bridge $(3,4)$, we sever the only communication link. Consequently, the receptive field of node $5$ shrinks to only nodes within its own connected component, $\\{4,5,6\\}$. Since the initial features in this component are all zero, $h'^{(2)}_5$ becomes zero. The change $\\Delta=1/9$ represents the complete loss of information flow from the other side of the graph, highlighting the critical dependence of the GCN on the graph's connectivity and the severity of bottlenecks. This is the most extreme form of over-squashing, where the message is squashed to zero due to a disconnected path.",
            "answer": "$$\\boxed{\\frac{1}{9}}$$"
        },
        {
            "introduction": "After a GNN computes updated features for each node, this information is often aggregated, or \"read out,\" to make a single prediction for the entire graph. This final exercise demonstrates a crucial limitation of simple readout mechanisms like summation . You will see how two graphs with distinctly different node feature distributions can produce the exact same output, highlighting fundamental challenges related to the expressive power of GNN architectures and their ability to distinguish non-isomorphic graphs.",
            "id": "4287348",
            "problem": "Consider a single-step message passing Graph Neural Network (GNN) defined on an undirected simple graph with node set $\\{1,2,3\\}$ and edges $\\{(1,2),(2,3)\\}$ (a path of length $2$). Each node $i$ has a scalar feature $x_{i} \\in \\mathbb{R}$. The model performs one synchronous message-passing update per node and then applies a sum readout followed by a final linear map to produce a graph-level scalar prediction. The update and readout are specified as follows:\n\n- Per-node update for node $i$:\n$h_{i} \\leftarrow \\alpha\\, x_{i} + \\beta \\sum_{j \\in \\mathcal{N}(i)} x_{j} + b$.\n\n- Graph-level sum readout:\n$s \\leftarrow \\sum_{i=1}^{3} h_{i}$.\n\n- Final prediction:\n$y \\leftarrow \\gamma\\, s + \\delta$.\n\nIn this toy model, parameters are fixed to $\\alpha = 2$, $\\beta = 1$, $b = 0$, $\\gamma = \\tfrac{1}{2}$, and $\\delta = 1$. Let $\\deg(i)$ denote the degree of node $i$ and $\\mathcal{N}(i)$ its open neighborhood. You may assume the standard graph-theoretic definitions of degree and neighborhood and that the message passing uses the given update rule exactly once.\n\nTwo distinct node-feature assignments are considered:\n\n- Assignment A: $(x_{1}, x_{2}, x_{3}) = (2, 0, 1)$.\n- Assignment B: $(x_{1}, x_{2}, x_{3}) = \\left(1, 1, \\tfrac{2}{3}\\right)$.\n\nTasks:\n\n- Using only the stated definitions and the given update rule, derive from first principles a closed-form expression for the scalar prediction $y$ in terms of the node features $x_{1}, x_{2}, x_{3}$ and the graph structure.\n- Evaluate this expression for Assignments A and B, and verify that both lead to the same graph-level prediction, thereby illustrating a limitation of the sum readout in distinguishing different node-feature distributions.\n- Report the common value of $y$ as your final answer. Express your answer exactly; no rounding is required.",
            "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\nThe givens are:\n- A simple, undirected graph with node set $V = \\{1, 2, 3\\}$ and edge set $E = \\{(1,2), (2,3)\\}$.\n- Scalar node features $x_i \\in \\mathbb{R}$ for each node $i \\in V$.\n- A single-step GNN model with the following operations:\n  - Per-node update: $h_{i} \\leftarrow \\alpha\\, x_{i} + \\beta \\sum_{j \\in \\mathcal{N}(i)} x_{j} + b$.\n  - Graph-level sum readout: $s \\leftarrow \\sum_{i=1}^{3} h_{i}$.\n  - Final prediction: $y \\leftarrow \\gamma\\, s + \\delta$.\n- Fixed parameters: $\\alpha = 2$, $\\beta = 1$, $b = 0$, $\\gamma = \\frac{1}{2}$, $\\delta = 1$.\n- Two node-feature assignments:\n  - Assignment A: $(x_{1}, x_{2}, x_{3}) = (2, 0, 1)$.\n  - Assignment B: $(x_{1}, x_{2}, x_{3}) = \\left(1, 1, \\frac{2}{3}\\right)$.\n- $\\mathcal{N}(i)$ is the open neighborhood of node $i$, and $\\deg(i)$ is its degree.\n\nThe problem statement describes a standard \"toy\" model of a Graph Neural Network, a well-established concept in machine learning and network science. All terms are clearly defined, all necessary parameters and data are provided, and there are no internal contradictions or violations of mathematical or scientific principles. The problem is well-posed, complete, and objective. Therefore, it is deemed valid, and we may proceed with the solution.\n\nThe first task is to derive a closed-form expression for the prediction $y$. We begin by analyzing the graph structure. The graph is a path $1-2-3$. The neighborhoods and degrees of the nodes are:\n- Node $1$: $\\mathcal{N}(1) = \\{2\\}$, so $\\deg(1) = 1$.\n- Node $2$: $\\mathcal{N}(2) = \\{1, 3\\}$, so $\\deg(2) = 2$.\n- Node $3$: $\\mathcal{N}(3) = \\{2\\}$, so $\\deg(3) = 1$.\n\nNext, we write the expressions for the updated node-level representations $h_i$ by applying the update rule:\n$h_{1} = \\alpha x_{1} + \\beta \\sum_{j \\in \\mathcal{N}(1)} x_{j} + b = \\alpha x_{1} + \\beta x_{2} + b$\n$h_{2} = \\alpha x_{2} + \\beta \\sum_{j \\in \\mathcal{N}(2)} x_{j} + b = \\alpha x_{2} + \\beta (x_{1} + x_{3}) + b$\n$h_{3} = \\alpha x_{3} + \\beta \\sum_{j \\in \\mathcal{N}(3)} x_{j} + b = \\alpha x_{3} + \\beta x_{2} + b$\n\nThe graph-level sum readout $s$ is the sum of these representations:\n$s = h_{1} + h_{2} + h_{3}$\n$s = (\\alpha x_{1} + \\beta x_{2} + b) + (\\alpha x_{2} + \\beta x_{1} + \\beta x_{3} + b) + (\\alpha x_{3} + \\beta x_{2} + b)$\n\nWe group terms by $x_1$, $x_2$, and $x_3$:\n$s = (\\alpha x_{1} + \\beta x_{1}) + (\\beta x_{2} + \\alpha x_{2} + \\beta x_{2}) + (\\beta x_{3} + \\alpha x_{3}) + (b + b + b)$\n$s = (\\alpha + \\beta)x_{1} + (\\alpha + 2\\beta)x_{2} + (\\alpha + \\beta)x_{3} + 3b$\n\nThis expression can be written more generally by observing the coefficients in terms of node degrees:\n$s = (\\alpha + \\beta \\deg(1))x_{1} + (\\alpha + \\beta \\deg(2))x_{2} + (\\alpha + \\beta \\deg(3))x_{3} + 3b$\n$s = \\sum_{i=1}^{3} (\\alpha + \\beta \\deg(i))x_{i} + 3b$\n\nFinally, the prediction $y$ is given by:\n$y = \\gamma s + \\delta = \\gamma \\left( \\sum_{i=1}^{3} (\\alpha + \\beta \\deg(i))x_{i} + 3b \\right) + \\delta$\nThis is the required closed-form expression for $y$.\n\nNow, we substitute the given parameter values: $\\alpha = 2$, $\\beta = 1$, $b = 0$, $\\gamma = \\frac{1}{2}$, and $\\delta = 1$. The expression for $s$ becomes:\n$s = (2 + 1 \\cdot \\deg(1))x_{1} + (2 + 1 \\cdot \\deg(2))x_{2} + (2 + 1 \\cdot \\deg(3))x_{3} + 3(0)$\nUsing the degrees $\\deg(1)=1$, $\\deg(2)=2$, $\\deg(3)=1$:\n$s = (2 + 1)x_{1} + (2 + 2)x_{2} + (2 + 1)x_{3}$\n$s = 3x_{1} + 4x_{2} + 3x_{3}$\n\nThe expression for $y$ becomes:\n$y = \\frac{1}{2}s + 1 = \\frac{1}{2}(3x_{1} + 4x_{2} + 3x_{3}) + 1$\n\nThe second task is to evaluate this expression for the two given assignments and verify they yield the same result.\n\nFor Assignment A: $(x_{1}, x_{2}, x_{3}) = (2, 0, 1)$\n$s_{A} = 3(2) + 4(0) + 3(1) = 6 + 0 + 3 = 9$\n$y_{A} = \\frac{1}{2}(s_{A}) + 1 = \\frac{1}{2}(9) + 1 = 4.5 + 1 = 5.5$\n\nFor Assignment B: $(x_{1}, x_{2}, x_{3}) = \\left(1, 1, \\frac{2}{3}\\right)$\n$s_{B} = 3(1) + 4(1) + 3\\left(\\frac{2}{3}\\right) = 3 + 4 + 2 = 9$\n$y_{B} = \\frac{1}{2}(s_{B}) + 1 = \\frac{1}{2}(9) + 1 = 4.5 + 1 = 5.5$\n\nWe have verified that $y_{A} = y_{B} = 5.5$. This demonstrates that the model maps two distinct node-feature distributions, Assignment A and Assignment B, to the same graph-level prediction. This loss of information is a known limitation of simple sum-based readout functions, as they are insensitive to permutations of node features among nodes with the same aggregate coefficient in the sum (here, nodes $1$ and $3$ share the coefficient $3$). The model is only sensitive to the weighted sum $3x_1 + 4x_2 + 3x_3$, not to the individual values of $x_i$.\n\nThe final task is to report the common value of $y$. This value is $5.5$, which is exactly $\\frac{11}{2}$.",
            "answer": "$$\n\\boxed{\\frac{11}{2}}\n$$"
        }
    ]
}