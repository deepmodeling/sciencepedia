{
    "hands_on_practices": [
        {
            "introduction": "本节将通过一系列动手实践，加深你对信息论核心概念的理解。我们将从最基础的概念——香农熵——开始。通过直接从定义出发，为一个具有给定概率分布的分类变量计算熵，你将获得关于“平均意外程度”或不确定性的直观感受。这项练习  旨在阐明单个事件的自信息与其对总熵的贡献之间的区别，并将其与最大熵原理进行比较，从而为你后续学习更复杂的概念奠定坚实的基础。",
            "id": "4283610",
            "problem": "在一个复杂网络分析流程中，一个节点采样模块输出一个分类变量 $X$，该变量取三个标签 $\\{x_{1}, x_{2}, x_{3}\\}$，其概率分别为 $\\left(\\frac{1}{2}, \\frac{1}{3}, \\frac{1}{6}\\right)$。从自信息 $I(x) \\equiv -\\ln p(x)$ 的定义和香农熵 $H(X)$ 作为 $I(X)$ 在 $X$ 分布上的期望的定义出发，完成以下任务：\n\n1) 推导 $H(X)$ 的精确、闭式表达式（用自然对数表示），并根据给定概率明确计算其值。最终熵以“奈特 (nats)”为单位，并提供精确表达式（无数值近似）。\n\n2) 对于每个结果 $x_{i}$，计算其自信息 $I(x_{i})$ 及其对 $H(X)$ 的贡献 $p(x_{i})\\,I(x_{i})$。利用这些量，解释为什么概率较小的事件具有更高的单位事件惊奇度（surprise），但由于其稀有性，对总熵的贡献可能更小。\n\n3) 根据无附加约束的三结果分类变量的最大熵原理，确定可能的最大熵，并将其与您为 $H(X)$ 求得的值进行解析比较；解释其差异。\n\n最终报告的答案仅要求 $H(X)$ 的精确值（以奈特为单位）。不要对最终报告值进行近似或四舍五入。",
            "solution": "首先验证用户提供的问题的科学性、一致性和完整性。\n\n**第 1 步：提取已知条件**\n- 一个分类变量 $X$ 取三个标签 $\\{x_{1}, x_{2}, x_{3}\\}$。\n- 标签的概率为 $p(x_1) = \\frac{1}{2}$，$p(x_2) = \\frac{1}{3}$ 和 $p(x_3) = \\frac{1}{6}$。\n- 自信息的定义为 $I(x) \\equiv -\\ln p(x)$。\n- 香农熵 $H(X)$ 的定义是 $I(X)$ 在 $X$ 分布上的期望。\n- 熵的单位要求为奈特 (nats)，这意味着使用自然对数 ($\\ln$）。\n\n**第 2 步：使用提取的已知条件进行验证**\n- **科学基础**：该问题基于香农信息论的标准、公认的原理。自信息和熵的定义是正确的。\n- **适定性**：该问题是适定的。所提供的概率为正且总和为 1：$\\frac{1}{2} + \\frac{1}{3} + \\frac{1}{6} = \\frac{3}{6} + \\frac{2}{6} + \\frac{1}{6} = \\frac{6}{6} = 1$。计算所需的所有信息都已提供。\n- **客观性**：问题以客观的数学语言陈述，没有歧义或主观论断。\n- **结论**：该问题是有效的，因为它在科学上是合理的、自洽的且适定的。\n\n**第 3 步：进行求解**\n\n**1) 香农熵 $H(X)$ 的推导与计算**\n香农熵 $H(X)$ 定义为自信息 $I(X) = -\\ln p(X)$ 的期望值。对于具有结果 $\\{x_1, \\dots, x_n\\}$ 和概率 $p(x_i)$ 的离散随机变量 $X$，该期望计算如下：\n$$H(X) = E[I(X)] = \\sum_{i=1}^{n} p(x_i) I(x_i) = -\\sum_{i=1}^{n} p(x_i) \\ln p(x_i)$$\n问题指定了三个结果 $x_{1}, x_{2}, x_{3}$，其概率为 $p(x_1) = \\frac{1}{2}$，$p(x_2) = \\frac{1}{3}$ 和 $p(x_3) = \\frac{1}{6}$。将这些值代入熵公式：\n$$H(X) = -\\left( p(x_1)\\ln(p(x_1)) + p(x_2)\\ln(p(x_2)) + p(x_3)\\ln(p(x_3)) \\right)$$\n$$H(X) = -\\left( \\frac{1}{2}\\ln\\left(\\frac{1}{2}\\right) + \\frac{1}{3}\\ln\\left(\\frac{1}{3}\\right) + \\frac{1}{6}\\ln\\left(\\frac{1}{6}\\right) \\right)$$\n使用对数恒等式 $\\ln\\left(\\frac{1}{a}\\right) = -\\ln(a)$:\n$$H(X) = -\\left( \\frac{1}{2}(-\\ln 2) + \\frac{1}{3}(-\\ln 3) + \\frac{1}{6}(-\\ln 6) \\right)$$\n$$H(X) = \\frac{1}{2}\\ln 2 + \\frac{1}{3}\\ln 3 + \\frac{1}{6}\\ln 6$$\n为了获得更简化的闭式形式，我们使用恒等式 $\\ln(ab) = \\ln a + \\ln b$ 来展开 $\\ln 6$：\n$$H(X) = \\frac{1}{2}\\ln 2 + \\frac{1}{3}\\ln 3 + \\frac{1}{6}(\\ln 2 + \\ln 3)$$\n按对数项分组：\n$$H(X) = \\left(\\frac{1}{2} + \\frac{1}{6}\\right)\\ln 2 + \\left(\\frac{1}{3} + \\frac{1}{6}\\right)\\ln 3$$\n$$H(X) = \\left(\\frac{3}{6} + \\frac{1}{6}\\right)\\ln 2 + \\left(\\frac{2}{6} + \\frac{1}{6}\\right)\\ln 3$$\n$$H(X) = \\frac{4}{6}\\ln 2 + \\frac{3}{6}\\ln 3$$\n熵的精确、闭式表达式（以奈特为单位）为：\n$$H(X) = \\frac{2}{3}\\ln 2 + \\frac{1}{2}\\ln 3$$\n\n**2) 自信息及其对熵的贡献**\n对于每个结果 $x_i$，我们计算其自信息 $I(x_i)$ 及其对总熵的贡献 $p(x_i) I(x_i)$。\n\n- 对于 $x_1$，$p(x_1) = \\frac{1}{2}$：\n  - 自信息：$I(x_1) = -\\ln\\left(\\frac{1}{2}\\right) = \\ln 2$。\n  - 对熵的贡献：$p(x_1)I(x_1) = \\frac{1}{2} \\ln 2$。\n\n- 对于 $x_2$，$p(x_2) = \\frac{1}{3}$：\n  - 自信息：$I(x_2) = -\\ln\\left(\\frac{1}{3}\\right) = \\ln 3$。\n  - 对熵的贡献：$p(x_2)I(x_2) = \\frac{1}{3} \\ln 3$。\n\n- 对于 $x_3$，$p(x_3) = \\frac{1}{6}$：\n  - 自信息：$I(x_3) = -\\ln\\left(\\frac{1}{6}\\right) = \\ln 6$。\n  - 对熵的贡献：$p(x_3)I(x_3) = \\frac{1}{6} \\ln 6$。\n\n一个事件的自信息，或称“惊奇度”，与其概率成反比。由于 $\\frac{1}{6}  \\frac{1}{3}  \\frac{1}{2}$，且自然对数是单调递增函数，我们有 $\\ln 6 > \\ln 3 > \\ln 2$。因此，$I(x_3) > I(x_2) > I(x_1)$。概率最小的事件 $x_3$ 携带最高的自信息。\n\n然而，一个事件对总熵的贡献是其惊奇度与其发生概率的乘积，$p(x)I(x) = -p(x)\\ln p(x)$。这个函数不是单调的。当 $p(x)=0$ 和 $p(x)=1$ 时，它为零，并在 $p(x) = \\frac{1}{e} \\approx 0.368$ 时达到最大值。这些概率为 $p(x_1)=0.5$，$p(x_2) \\approx 0.333$ 和 $p(x_3) \\approx 0.167$。概率 $p(x_2)$ 最接近峰值 $\\frac{1}{e}$，因此它对总熵的贡献最大。事件 $x_3$ 具有最高的惊奇度 $I(x_3)$，但其稀有性（低概率 $p(x_3)$）削弱了它对平均惊奇度（即熵 $H(X)$）的贡献。在本例中，$\\frac{1}{3}\\ln 3 > \\frac{1}{2}\\ln 2 > \\frac{1}{6}\\ln 6$。\n\n**3) 与最大熵的比较**\n根据最大熵原理，对于一个有 $n$ 个结果的分类变量，当概率分布为均匀分布时，即对所有 $i$ 都有 $p(x_i) = \\frac{1}{n}$，熵 $H(X)$ 达到最大值。\n对于本问题，$n=3$，因此最大熵分布为 $p(x_1) = p(x_2) = p(x_3) = \\frac{1}{3}$。可能的最大熵 $H_{\\text{max}}$ 为：\n$$H_{\\text{max}} = -\\sum_{i=1}^{3} \\frac{1}{3}\\ln\\left(\\frac{1}{3}\\right) = -3 \\left(\\frac{1}{3}\\ln\\left(\\frac{1}{3}\\right)\\right) = -\\ln\\left(\\frac{1}{3}\\right) = \\ln 3$$\n让我们将其与计算出的熵 $H(X) = \\frac{2}{3}\\ln 2 + \\frac{1}{2}\\ln 3$ 进行比较。差值为：\n$$\\Delta H = H_{\\text{max}} - H(X) = \\ln 3 - \\left(\\frac{2}{3}\\ln 2 + \\frac{1}{2}\\ln 3\\right) = \\frac{1}{2}\\ln 3 - \\frac{2}{3}\\ln 2$$\n为了确认 $H(X)  H_{\\text{max}}$，我们需要证明 $\\Delta H > 0$。这等价于比较 $\\frac{1}{2}\\ln 3$ 与 $\\frac{2}{3}\\ln 2$。\n使用恒等式 $c \\ln a = \\ln(a^c)$，我们比较 $\\ln(3^{1/2})$ 和 $\\ln(2^{2/3})$。\n这等价于比较 $3^{1/2}$ 和 $2^{2/3}$。将两者都进行 6 次方：\n$$(3^{1/2})^6 \\quad \\text{与} \\quad (2^{2/3})^6 \\text{ 比较}$$\n$$3^3 \\quad \\text{与} \\quad 2^4 \\text{ 比较}$$\n$$27 \\quad \\text{与} \\quad 16 \\text{ 比较}$$\n由于 $27 > 16$，因此 $H(X)  H_{\\text{max}}$。\n差值 $\\Delta H = H_{\\text{max}} - H(X)$ 代表了系统中的冗余度，或者等效地，代表了从给定分布 $P$ 到均匀分布 $U$ 的库尔贝克-莱布勒散度（Kullback-Leibler divergence），即 $D_{KL}(P||U)$。它量化了由于系统偏离最大无知状态（均匀分布）而导致的不确定性（熵）的减少。非均匀概率 $(\\frac{1}{2}, \\frac{1}{3}, \\frac{1}{6})$ 代表了关于系统的先验信息，这使得系统更具可预测性，从而相对于可能的最大值降低了其熵。",
            "answer": "$$\\boxed{\\frac{2}{3}\\ln 2 + \\frac{1}{2}\\ln 3}$$"
        },
        {
            "introduction": "在掌握了如何量化单个变量的不确定性之后，我们自然会想知道如何衡量两个变量之间的关系。传统上，协方差或相关系数被用来衡量线性依赖关系，但它们无法捕捉非线性关联。这项练习  通过一个精心设计的例子，清晰地展示了互信息的优越性：两个变量可以做到完全不相关（协方差为零），但由于存在确定性的非线性关系，它们之间共享了大量信息。这个实践将帮助你理解为什么互信息是分析复杂系统中非线性相互作用的强大工具。",
            "id": "4283601",
            "problem": "考虑一个复杂网络中的节点级状态变量 $X$，其瞬时值被限制在离散集合 $\\{-1, 0, 1\\}$ 中。假设节点以概率 $P(X=0)=1-p$ 处于非活动状态，并以等可能的正或负符号处于活动状态，即 $P(X=1)=p/2$ 和 $P(X=-1)=p/2$，其中参数 $p \\in (0,1)$。定义一个观测通道，该通道报告平方幅值 $Y=X^{2}$，其取值于 $\\{0,1\\}$，旨在捕捉非线性活动同时忽略其符号。\n\n从信息论和概率论的核心定义出发（不使用任何快捷公式），完成以下任务：\n- 计算 $X$ 和 $Y$ 之间的协方差，并证明对于所有 $p \\in (0,1)$，该协方差为零，从而证实 $X$ 和 $Y$ 是不相关的。\n- 推导 $X$ 和 $Y$ 之间的互信息 (MI) $I(X;Y)$ 的闭式解析表达式，以自然单位（奈特）表示，并强调对于所有 $p \\in (0,1)$，有 $I(X;Y)0$。\n\n将您的最终答案报告为 $I(X;Y)$ 关于 $p$ 的单个闭式表达式。不需要四舍五入，也不涉及物理单位。",
            "solution": "首先验证问题，以确保其具有科学依据、提法恰当且内容自洽。\n\n### 第1步：提取已知条件\n- 一个离散随机变量 $X$ 在集合 $\\{-1, 0, 1\\}$ 中取值。\n- $X$ 的概率质量函数 (PMF) 由以下公式给出：\n  - $P(X=0) = 1-p$\n  - $P(X=1) = p/2$\n  - $P(X=-1) = p/2$\n- 参数 $p$ 被限制在 $p \\in (0,1)$。\n- 第二个随机变量 $Y$ 被定义为 $X$ 的函数：$Y = X^2$。\n- 任务是：\n  1. 计算协方差 $\\text{Cov}(X,Y)$ 并证明其为零。\n  2. 以奈特为单位推导互信息 $I(X;Y)$ 并证明其为正。\n- 最终答案是 $I(X;Y)$ 的闭式表达式。\n\n### 第2步：使用提取的已知条件进行验证\n该问题根植于标准的概率论和信息论。变量的定义清晰，且 $X$ 的概率分布是有效的，因为所有概率之和为1：$(1-p) + p/2 + p/2 = 1$。参数范围 $p \\in (0,1)$ 是明确定义的。这些任务涉及协方差和互信息的标准计算，这些都是基本概念。该问题是一个经典的例子，说明了零相关性并不意味着统计独立性。问题中没有科学矛盾、歧义或缺失信息。\n\n### 第3步：结论与行动\n问题有效。将提供解答。\n\n### 协方差计算\n两个随机变量 $X$ 和 $Y$ 之间的协方差定义为 $\\text{Cov}(X,Y) = E[XY] - E[X]E[Y]$。我们必须计算期望值 $E[X]$、$E[Y]$ 和 $E[XY]$。\n\n1.  **$X$ 的期望值：**\n    $X$ 的期望值由 $E[X] = \\sum_{x} x P(X=x)$ 给出。\n    $$E[X] = (-1) \\cdot P(X=-1) + (0) \\cdot P(X=0) + (1) \\cdot P(X=1)$$\n    $$E[X] = (-1) \\left(\\frac{p}{2}\\right) + (0)(1-p) + (1)\\left(\\frac{p}{2}\\right) = -\\frac{p}{2} + \\frac{p}{2} = 0$$\n\n2.  **$Y$ 的概率分布和期望值：**\n    变量 $Y$ 定义为 $Y=X^2$。$Y$ 的可能取值为 $0^2=0$ 和 $(\\pm 1)^2=1$。我们确定 $Y$ 的概率质量函数（PMF）。\n    -   $P(Y=0) = P(X^2=0) = P(X=0) = 1-p$。\n    -   $P(Y=1) = P(X^2=1) = P(X=1 \\text{ or } X=-1) = P(X=1) + P(X=-1) = \\frac{p}{2} + \\frac{p}{2} = p$。\n    $Y$ 的期望值为 $E[Y] = \\sum_{y} y P(Y=y)$。\n    $$E[Y] = (0) \\cdot P(Y=0) + (1) \\cdot P(Y=1) = (0)(1-p) + (1)(p) = p$$\n\n3.  **乘积 $XY$ 的期望值：**\n    乘积为 $XY = X \\cdot X^2 = X^3$。随机变量 $X^3$ 的取值为 $(-1)^3=-1$、$0^3=0$ 和 $1^3=1$。\n    期望值为 $E[XY] = E[X^3] = \\sum_{x} x^3 P(X=x)$。\n    $$E[XY] = (-1)^3 \\cdot P(X=-1) + (0)^3 \\cdot P(X=0) + (1)^3 \\cdot P(X=1)$$\n    $$E[XY] = (-1)\\left(\\frac{p}{2}\\right) + (0)(1-p) + (1)\\left(\\frac{p}{2}\\right) = -\\frac{p}{2} + \\frac{p}{2} = 0$$\n\n4.  **最终协方差计算：**\n    将期望值代入协方差公式：\n    $$\\text{Cov}(X,Y) = E[XY] - E[X]E[Y] = 0 - (0)(p) = 0$$\n    对于所有 $p \\in (0,1)$，协方差为零，这证实了 $X$ 和 $Y$ 是不相关的。\n\n### 互信息计算\n互信息 $I(X;Y)$ 衡量 $X$ 和 $Y$ 之间的统计依赖性。其定义为 $I(X;Y) = H(Y) - H(Y|X)$，其中 $H(\\cdot)$ 表示香农熵，$H(\\cdot|\\cdot)$ 表示条件熵。为了以奈特为单位，对数的底为 $e$。\n\n1.  **$Y$ 的熵, $H(Y)$：**\n    离散随机变量 $Y$ 的熵为 $H(Y) = -\\sum_{y} P(Y=y) \\ln(P(Y=y))$。\n    $$H(Y) = -[P(Y=0)\\ln(P(Y=0)) + P(Y=1)\\ln(P(Y=1))]$$\n    $$H(Y) = -[(1-p)\\ln(1-p) + p\\ln(p)]$$\n    这是二元熵函数。\n\n2.  **给定 $X$ 时 $Y$ 的条件熵, $H(Y|X)$：**\n    条件熵 $H(Y|X)$ 量化了在已知 $X$ 的情况下 $Y$ 的剩余不确定性。其定义为 $H(Y|X) = -\\sum_{x,y} P(X=x, Y=y) \\ln(P(Y=y|X=x))$。\n    然而，$Y$ 是 $X$ 的一个确定性函数（$Y=X^2$）。这意味着一旦知道了 $X$ 的值，$Y$ 的值就完全确定了。对于任何给定的 $x$，条件概率 $P(Y=y|X=x)$ 在 $y=x^2$ 时为 1，否则为 0。\n    一个确定性结果的熵为零。也就是说，对于任何特定的值 $x$，条件熵 $H(Y|X=x) = -\\sum_y P(Y=y|X=x)\\ln(P(Y=y|X=x)) = - (1 \\ln 1 + 0) = 0$。\n    由于对所有 $x$ 都有 $H(Y|X=x)=0$，总条件熵 $H(Y|X) = \\sum_x P(X=x) H(Y|X=x)$ 也为零。\n    $$H(Y|X) = 0$$\n\n3.  **最终互信息计算：**\n    将 $H(Y)$ 和 $H(Y|X)$ 的表达式代入互信息的定义中：\n    $$I(X;Y) = H(Y) - H(Y|X) = -[(1-p)\\ln(1-p) + p\\ln(p)] - 0$$\n    $$I(X;Y) = -p\\ln(p) - (1-p)\\ln(1-p)$$\n\n    这个结果突显了一个关键原则：不相关性并不意味着独立性。虽然 $\\text{Cov}(X,Y)=0$，但变量显然是相关的，因为知道 $X$ 就消除了关于 $Y$ 的所有不确定性。这种依赖性由互信息捕捉到。\n\n4.  **互信息的正性：**\n    互信息 $I(X;Y)$ 是非负的，即 $I(X;Y) \\ge 0$，当且仅当 $X$ 和 $Y$ 统计独立时等号成立。对于 $p \\in (0,1)$，我们有 $0  p  1$ 和 $0  1-p  1$。对于任何值 $z \\in (0,1)$，$\\ln(z)$ 是负数。\n    -   项 $-p\\ln(p)$ 由一个负数（$\\ln p$）乘以一个负数（$-p$）组成，所以它是正的。\n    -   项 $-(1-p)\\ln(1-p)$ 由一个负数（$\\ln(1-p)$）乘以一个负数（$-(1-p)$）组成，所以它也是正的。\n    两个正项的和是正的。因此，对于所有 $p \\in (0,1)$，有 $I(X;Y) > 0$，这证实了 $X$ 和 $Y$ 不是独立的。只有在边界 $p=0$ 和 $p=1$ 处，熵 $H(Y)$ 变为零时，才会出现独立性。",
            "answer": "$$\\boxed{-p \\ln(p) - (1-p) \\ln(1-p)}$$"
        },
        {
            "introduction": "信息论的精妙之处不仅在于衡量静态的依赖关系，更在于它能揭示当我们观察或控制系统时，这些关系会如何动态变化。这项高级实践  探讨了一个在数据分析中普遍存在但又非常微妙的现象，即“对撞机偏倚”（collider bias）或“伯克森悖论”（Berkson's paradox）。通过计算条件互信息，你将亲手证明，对两个独立原因的共同结果进行条件化（即筛选样本），会错误地在它们之间引入统计依赖性。这个练习对于任何处理复杂系统中观测数据的人来说，都是一个至关重要的教训，它提醒我们在解释数据时要格外小心选择性偏差。",
            "id": "4283620",
            "problem": "在一个社会-生态网络中，考虑与每个节点相关的三个二元属性：属性 $X \\in \\{0,1\\}$ 表示一种本地资源使用行为，属性 $Y \\in \\{0,1\\}$ 表示对外部市场的参与，以及一个选择指示变量 $Z \\in \\{0,1\\}$ 记录该节点是否被包含在研究样本中。假设潜在驱动因素 $X$ 和 $Y$ 在总体中是独立的，且各自等于 $1$ 的概率为 $\\tfrac{1}{2}$。抽样机制规定，一个节点被纳入研究当且仅当其至少存在一个属性，即 $Z = \\mathbf{1}\\{X=1 \\,\\text{or}\\, Y=1\\}$。\n\n从 Shannon 熵和互信息的核心定义出发，并且不使用任何特定情境下的简化公式，完成以下任务：\n\n1. 证明 $X$ 和 $Y$ 在总体中是独立的，并计算互信息 $I(X;Y)$。\n2. 证明以对撞变量 $Z$ 为条件会引起 $X$ 和 $Y$ 之间的统计依赖性，并计算条件互信息 $I(X;Y \\mid Z)$。\n\n所有信息论量必须使用自然对数表示，并以自然单位（奈特）计量。将你的最终答案以有序对 $\\big(I(X;Y),\\, I(X;Y \\mid Z)\\big)$ 的精确封闭形式报告。不要提供任何数值近似或四舍五入。",
            "solution": "该问题要求计算一个由三个二元随机变量 $X$、$Y$ 和 $Z$ 组成的系统的互信息 $I(X;Y)$ 和条件互信息 $I(X;Y \\mid Z)$。我们将首先根据给定信息建立该系统的完整概率分布，然后应用 Shannon 熵和互信息的基本定义。所有对数都将是自然对数 ($\\ln$)，信息将以奈特为单位计量。\n\n**1. 总体层面的分析与互信息 $I(X;Y)$**\n\n问题陈述，属性 $X$ 和 $Y$ 是二元变量，$X, Y \\in \\{0, 1\\}$，并且在总体中是独立的。每个属性存在的概率给定为 $\\frac{1}{2}$。\n$X$ 和 $Y$ 的边际概率分布为：\n$$P(X=1) = \\frac{1}{2}, \\quad P(X=0) = 1 - \\frac{1}{2} = \\frac{1}{2}$$\n$$P(Y=1) = \\frac{1}{2}, \\quad P(Y=0) = 1 - \\frac{1}{2} = \\frac{1}{2}$$\n$X$ 和 $Y$ 的独立性意味着它们的联合概率分布是其边际分布的乘积：$P(X=x, Y=y) = P(X=x)P(Y=y)$。\n这就得出了对 $(X, Y)$ 这对变量的四种可能结果的均匀联合分布：\n$$P(X=x, Y=y) = \\frac{1}{2} \\times \\frac{1}{2} = \\frac{1}{4} \\quad \\text{for all } (x,y) \\in \\{0,1\\} \\times \\{0,1\\}$$\n\n互信息 $I(X;Y)$ 量化了通过了解一个变量而减少的关于另一个变量的不确定性。其定义是：\n$$I(X;Y) = \\sum_{x \\in \\{0,1\\}} \\sum_{y \\in \\{0,1\\}} P(X=x, Y=y) \\ln\\left(\\frac{P(X=x, Y=y)}{P(X=x)P(Y=y)}\\right)$$\n如前所示，$X$ 和 $Y$ 的独立性意味着 $P(X=x, Y=y) = P(X=x)P(Y=y)$。将此代入定义中：\n$$I(X;Y) = \\sum_{x,y} P(x,y) \\ln\\left(\\frac{P(x)P(y)}{P(x)P(y)}\\right) = \\sum_{x,y} P(x,y) \\ln(1)$$\n由于 $\\ln(1) = 0$，求和中的每一项都为零。因此，互信息为：\n$$I(X;Y) = 0$$\n这个结果正式确认了，在总体层面上，$X$ 和 $Y$ 是独立的，不共享任何信息。\n\n**2. 条件分析与条件互信息 $I(X;Y \\mid Z)$**\n\n变量 $Z$ 是一个选择指示变量，定义为 $Z = \\mathbf{1}\\{X=1 \\,\\text{or}\\, Y=1\\}$。这意味着 $Z$ 是 $X$ 和 $Y$ 的一个确定性函数。具体来说，$Z=0$ 当且仅当 $X=0$ 且 $Y=0$。否则，$Z=1$。这种结构使得 $Z$ 在因果图 $X \\rightarrow Z \\leftarrow Y$ 中成为一个“对撞”变量。我们现在将证明，以此对撞变量为条件会引起 $X$ 和 $Y$ 之间的统计依赖性。\n\n首先，我们确定 $Z$ 的概率分布：\n$$P(Z=0) = P(X=0, Y=0) = \\frac{1}{4}$$\n$$P(Z=1) = 1 - P(Z=0) = 1 - \\frac{1}{4} = \\frac{3}{4}$$\n\n条件互信息 $I(X;Y \\mid Z)$ 定义为 $X$ 和 $Y$ 之间的互信息在 $Z$ 的分布上的期望：\n$$I(X;Y \\mid Z) = \\sum_{z \\in \\{0,1\\}} P(Z=z) I(X;Y \\mid Z=z)$$\n我们通过分别分析 $z=0$ 和 $z=1$ 的情况来计算它。\n\n*情况 1: $Z=0$*\n如果我们知道 $Z=0$，那么必定有 $X=0$ 和 $Y=0$。条件概率分布 $P(X,Y \\mid Z=0)$ 是一个在 $(0,0)$ 处的点分布：\n$$P(X=0, Y=0 \\mid Z=0) = \\frac{P(X=0, Y=0, Z=0)}{P(Z=0)} = \\frac{P(X=0, Y=0)}{P(Z=0)} = \\frac{1/4}{1/4} = 1$$\n给定 $Z=0$，$X$ 或 $Y$ 没有任何不确定性。条件熵 $H(X|Z=0)$、$H(Y|Z=0)$ 和 $H(X,Y|Z=0)$ 都为零。以此特定结果为条件的互信息是：\n$$I(X;Y \\mid Z=0) = H(X \\mid Z=0) + H(Y \\mid Z=0) - H(X,Y \\mid Z=0) = 0 + 0 - 0 = 0$$\n\n*情况 2: $Z=1$*\n如果我们知道 $Z=1$，则结果 $(X=0, Y=0)$ 被排除了。剩下的三个结果是 $(0,1)$、$(1,0)$ 和 $(1,1)$。条件联合分布是：\n$$P(X=x, Y=y \\mid Z=1) = \\frac{P(X=x, Y=y, Z=1)}{P(Z=1)} = \\frac{P(X=x, Y=y)}{P(Z=1)} = \\frac{1/4}{3/4} = \\frac{1}{3}$$\n对于三个点对 $(x,y) \\in \\{(0,1), (1,0), (1,1)\\}$ 中的每一个。\n\n为了证明以 $Z=1$ 为条件会引起依赖性，我们将条件联合概率与条件边际概率的乘积进行比较。\n给定 $Z=1$ 时 $X$ 和 $Y$ 的条件边际分布：\n$$P(X=0 \\mid Z=1) = P(X=0, Y=1 \\mid Z=1) = \\frac{1}{3}$$\n$$P(X=1 \\mid Z=1) = P(X=1, Y=0 \\mid Z=1) + P(X=1, Y=1 \\mid Z=1) = \\frac{1}{3} + \\frac{1}{3} = \\frac{2}{3}$$\n根据对称性，$Y$ 的分布是相同的：\n$$P(Y=0 \\mid Z=1) = \\frac{1}{3}, \\quad P(Y=1 \\mid Z=1) = \\frac{2}{3}$$\n现在我们检查独立性：\n$$P(X=0 \\mid Z=1)P(Y=1 \\mid Z=1) = \\frac{1}{3} \\times \\frac{2}{3} = \\frac{2}{9}$$\n然而，我们发现 $P(X=0, Y=1 \\mid Z=1) = \\frac{1}{3}$。由于 $\\frac{1}{3} \\neq \\frac{2}{9}$，我们已经正式证明当以 $Z=1$ 为条件时，$X$ 和 $Y$ 是统计相关的。\n\n为了量化这种依赖性，我们使用恒等式 $I(X;Y \\mid C) = H(X \\mid C) + H(Y \\mid C) - H(X,Y \\mid C)$ 来计算 $I(X;Y \\mid Z=1)$。\n联合分布 $P(X,Y \\mid Z=1)$ 的条件熵是一个在3个状态上均匀分布的熵：\n$$H(X,Y \\mid Z=1) = -\\sum_{(x,y)} P(x,y \\mid Z=1) \\ln(P(x,y \\mid Z=1)) = -3 \\times \\left(\\frac{1}{3} \\ln\\left(\\frac{1}{3}\\right)\\right) = \\ln(3)$$\n条件边际熵为：\n$$H(X \\mid Z=1) = -\\left(P(X=0 \\mid Z=1)\\ln(P(X=0 \\mid Z=1)) + P(X=1 \\mid Z=1)\\ln(P(X=1 \\mid Z=1))\\right)$$\n$$H(X \\mid Z=1) = -\\left(\\frac{1}{3}\\ln\\left(\\frac{1}{3}\\right) + \\frac{2}{3}\\ln\\left(\\frac{2}{3}\\right)\\right) = \\frac{1}{3}\\ln(3) + \\frac{2}{3}(\\ln(3) - \\ln(2)) = \\ln(3) - \\frac{2}{3}\\ln(2)$$\n根据对称性，$H(Y \\mid Z=1) = H(X \\mid Z=1) = \\ln(3) - \\frac{2}{3}\\ln(2)$。\n现在，我们求给定 $Z=1$ 时的互信息：\n$$I(X;Y \\mid Z=1) = H(X \\mid Z=1) + H(Y \\mid Z=1) - H(X,Y \\mid Z=1)$$\n$$I(X;Y \\mid Z=1) = \\left(\\ln(3) - \\frac{2}{3}\\ln(2)\\right) + \\left(\\ln(3) - \\frac{2}{3}\\ln(2)\\right) - \\ln(3) = \\ln(3) - \\frac{4}{3}\\ln(2)$$\n\n*$I(X;Y \\mid Z)$ 的最终计算*\n我们现在结合 $Z=0$ 和 $Z=1$ 的结果：\n$$I(X;Y \\mid Z) = P(Z=0)I(X;Y \\mid Z=0) + P(Z=1)I(X;Y \\mid Z=1)$$\n$$I(X;Y \\mid Z) = \\left(\\frac{1}{4} \\times 0\\right) + \\left(\\frac{3}{4} \\times \\left(\\ln(3) - \\frac{4}{3}\\ln(2)\\right)\\right)$$\n$$I(X;Y \\mid Z) = \\frac{3}{4}\\ln(3) - \\left(\\frac{3}{4} \\times \\frac{4}{3}\\right)\\ln(2) = \\frac{3}{4}\\ln(3) - \\ln(2)$$\n$I(X;Y \\mid Z)$ 的正值证实了，平均而言，以 $Z$ 为条件在 $X$ 和 $Y$ 之间产生了统计依赖性。\n\n最终结果是有序对 $\\big(I(X;Y),\\, I(X;Y \\mid Z)\\big)$。\n$$ \\left(0, \\frac{3}{4}\\ln(3) - \\ln(2)\\right) $$",
            "answer": "$$\n\\boxed{\n\\left(0, \\frac{3}{4}\\ln(3) - \\ln(2)\\right)\n}\n$$"
        }
    ]
}