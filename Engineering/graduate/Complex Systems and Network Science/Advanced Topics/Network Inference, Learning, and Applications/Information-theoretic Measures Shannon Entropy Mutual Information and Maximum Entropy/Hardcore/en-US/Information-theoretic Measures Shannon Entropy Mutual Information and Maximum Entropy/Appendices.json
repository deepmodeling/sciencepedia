{
    "hands_on_practices": [
        {
            "introduction": "Mastering information theory begins with a firm grasp of its cornerstone: Shannon entropy. This first exercise provides a foundational workout, guiding you through the calculation of entropy for a simple categorical variable directly from its definition. More importantly, this practice illuminates the subtle relationship between an event's probability, its \"surprise\" (self-information), and its actual contribution to the system's total uncertainty, clarifying a common point of confusion and connecting it to the vital maximum entropy principle .",
            "id": "4283610",
            "problem": "A node-sampling module in a complex network analysis pipeline outputs a categorical variable $X$ taking three labels $\\{x_{1}, x_{2}, x_{3}\\}$ with probabilities $\\left(\\frac{1}{2}, \\frac{1}{3}, \\frac{1}{6}\\right)$. Starting from the definition of self-information $I(x) \\equiv -\\ln p(x)$ and the definition of Shannon entropy $H(X)$ as the expectation of $I(X)$ over the distribution of $X$, do the following:\n\n1) Derive an exact, closed-form expression for $H(X)$ in terms of natural logarithms and compute it explicitly for the given probabilities. Express the final entropy in nats and provide the exact expression (no numerical approximation).\n\n2) For each outcome $x_{i}$, compute its self-information $I(x_{i})$ and its contribution $p(x_{i})\\,I(x_{i})$ to $H(X)$. Using these quantities, explain why smaller-probability events carry higher per-event surprise yet can contribute less to the total entropy due to their rarity.\n\n3) In light of the maximum entropy principle for a three-outcome categorical variable with no additional constraints, identify the maximum possible entropy and compare it analytically to the value you obtained for $H(X)$; interpret the difference.\n\nOnly the exact value of $H(X)$ in nats is required as the final reported answer. Do not approximate or round the final reported value.",
            "solution": "The user-provided problem is first validated for scientific soundness, consistency, and completeness.\n\n**Step 1: Extract Givens**\n- A categorical variable $X$ takes three labels $\\{x_{1}, x_{2}, x_{3}\\}$.\n- The probabilities of the labels are $p(x_1) = \\frac{1}{2}$, $p(x_2) = \\frac{1}{3}$, and $p(x_3) = \\frac{1}{6}$.\n- The definition of self-information is given as $I(x) \\equiv -\\ln p(x)$.\n- The definition of Shannon entropy $H(X)$ is the expectation of $I(X)$ over the distribution of $X$.\n- The required unit for entropy is nats, implying the use of the natural logarithm ($\\ln$).\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientific Grounding**: The problem is based on the standard, well-established principles of Shannon's information theory. The definitions of self-information and entropy are correct.\n- **Well-Posed**: The problem is well-posed. The provided probabilities are positive and sum to unity: $\\frac{1}{2} + \\frac{1}{3} + \\frac{1}{6} = \\frac{3}{6} + \\frac{2}{6} + \\frac{1}{6} = \\frac{6}{6} = 1$. All necessary information for the calculations is provided.\n- **Objective**: The problem is stated in objective, mathematical language without ambiguity or subjective claims.\n- **Conclusion**: The problem is valid as it is scientifically sound, self-contained, and well-posed.\n\n**Step 3: Proceed with Solution**\n\n**1) Derivation and Computation of Shannon Entropy $H(X)$**\nThe Shannon entropy $H(X)$ is defined as the expected value of the self-information $I(X) = -\\ln p(X)$. For a discrete random variable $X$ with outcomes $\\{x_1, \\dots, x_n\\}$ and probabilities $p(x_i)$, this expectation is calculated as:\n$$H(X) = E[I(X)] = \\sum_{i=1}^{n} p(x_i) I(x_i) = -\\sum_{i=1}^{n} p(x_i) \\ln p(x_i)$$\nThe problem specifies three outcomes $x_{1}, x_{2}, x_{3}$ with probabilities $p(x_1) = \\frac{1}{2}$, $p(x_2) = \\frac{1}{3}$, and $p(x_3) = \\frac{1}{6}$. Substituting these values into the entropy formula:\n$$H(X) = -\\left( p(x_1)\\ln(p(x_1)) + p(x_2)\\ln(p(x_2)) + p(x_3)\\ln(p(x_3)) \\right)$$\n$$H(X) = -\\left( \\frac{1}{2}\\ln\\left(\\frac{1}{2}\\right) + \\frac{1}{3}\\ln\\left(\\frac{1}{3}\\right) + \\frac{1}{6}\\ln\\left(\\frac{1}{6}\\right) \\right)$$\nUsing the logarithmic identity $\\ln\\left(\\frac{1}{a}\\right) = -\\ln(a)$:\n$$H(X) = -\\left( \\frac{1}{2}(-\\ln 2) + \\frac{1}{3}(-\\ln 3) + \\frac{1}{6}(-\\ln 6) \\right)$$\n$$H(X) = \\frac{1}{2}\\ln 2 + \\frac{1}{3}\\ln 3 + \\frac{1}{6}\\ln 6$$\nTo obtain a more simplified closed form, we use the identity $\\ln(ab) = \\ln a + \\ln b$ to expand $\\ln 6$:\n$$H(X) = \\frac{1}{2}\\ln 2 + \\frac{1}{3}\\ln 3 + \\frac{1}{6}(\\ln 2 + \\ln 3)$$\nGrouping terms by the logarithm:\n$$H(X) = \\left(\\frac{1}{2} + \\frac{1}{6}\\right)\\ln 2 + \\left(\\frac{1}{3} + \\frac{1}{6}\\right)\\ln 3$$\n$$H(X) = \\left(\\frac{3}{6} + \\frac{1}{6}\\right)\\ln 2 + \\left(\\frac{2}{6} + \\frac{1}{6}\\right)\\ln 3$$\n$$H(X) = \\frac{4}{6}\\ln 2 + \\frac{3}{6}\\ln 3$$\nThe exact, closed-form expression for the entropy in nats is:\n$$H(X) = \\frac{2}{3}\\ln 2 + \\frac{1}{2}\\ln 3$$\n\n**2) Self-Information and Contribution to Entropy**\nFor each outcome $x_i$, we compute its self-information $I(x_i)$ and its contribution to the total entropy, $p(x_i) I(x_i)$.\n\n- For $x_1$ with $p(x_1) = \\frac{1}{2}$:\n  - Self-information: $I(x_1) = -\\ln\\left(\\frac{1}{2}\\right) = \\ln 2$.\n  - Contribution to entropy: $p(x_1)I(x_1) = \\frac{1}{2} \\ln 2$.\n\n- For $x_2$ with $p(x_2) = \\frac{1}{3}$:\n  - Self-information: $I(x_2) = -\\ln\\left(\\frac{1}{3}\\right) = \\ln 3$.\n  - Contribution to entropy: $p(x_2)I(x_2) = \\frac{1}{3} \\ln 3$.\n\n- For $x_3$ with $p(x_3) = \\frac{1}{6}$:\n  - Self-information: $I(x_3) = -\\ln\\left(\\frac{1}{6}\\right) = \\ln 6$.\n  - Contribution to entropy: $p(x_3)I(x_3) = \\frac{1}{6} \\ln 6$.\n\nAn event's self-information, or \"surprise,\" is inversely related to its probability. Since $\\frac{1}{6}  \\frac{1}{3}  \\frac{1}{2}$, and the natural logarithm is a monotonically increasing function, we have $\\ln 6  \\ln 3  \\ln 2$. Thus, $I(x_3)  I(x_2)  I(x_1)$. The least probable event, $x_3$, carries the highest self-information.\n\nHowever, an event's contribution to the total entropy is the product of its surprise and its probability of occurrence, $p(x)I(x) = -p(x)\\ln p(x)$. This function is not monotonic. It is zero for $p(x)=0$ and $p(x)=1$, and it reaches its maximum value at $p(x) = \\frac{1}{e} \\approx 0.368$. The probabilities are $p(x_1)=0.5$, $p(x_2) \\approx 0.333$, and $p(x_3) \\approx 0.167$. The probability $p(x_2)$ is closest to the peak at $\\frac{1}{e}$, so it provides the largest contribution to the total entropy. The event $x_3$ has the highest surprise $I(x_3)$, but its rarity (low probability $p(x_3)$) diminishes its contribution to the average surprise, which is the entropy $H(X)$. In this case, $\\frac{1}{3}\\ln 3  \\frac{1}{2}\\ln 2  \\frac{1}{6}\\ln 6$.\n\n**3) Comparison with Maximum Entropy**\nAccording to the maximum entropy principle, for a categorical variable with $n$ outcomes, the entropy $H(X)$ is maximized when the probability distribution is uniform, i.e., $p(x_i) = \\frac{1}{n}$ for all $i$.\nFor this problem, $n=3$, so the maximum entropy distribution is $p(x_1) = p(x_2) = p(x_3) = \\frac{1}{3}$. The maximum possible entropy, $H_{\\text{max}}$, is:\n$$H_{\\text{max}} = -\\sum_{i=1}^{3} \\frac{1}{3}\\ln\\left(\\frac{1}{3}\\right) = -3 \\left(\\frac{1}{3}\\ln\\left(\\frac{1}{3}\\right)\\right) = -\\ln\\left(\\frac{1}{3}\\right) = \\ln 3$$\nLet's compare this with the calculated entropy $H(X) = \\frac{2}{3}\\ln 2 + \\frac{1}{2}\\ln 3$. The difference is:\n$$\\Delta H = H_{\\text{max}} - H(X) = \\ln 3 - \\left(\\frac{2}{3}\\ln 2 + \\frac{1}{2}\\ln 3\\right) = \\frac{1}{2}\\ln 3 - \\frac{2}{3}\\ln 2$$\nTo confirm that $H(X)  H_{\\text{max}}$, we need to show that $\\Delta H  0$. This is equivalent to comparing $\\frac{1}{2}\\ln 3$ with $\\frac{2}{3}\\ln 2$.\nUsing the identity $c \\ln a = \\ln(a^c)$, we compare $\\ln(3^{1/2})$ and $\\ln(2^{2/3})$.\nThis is equivalent to comparing $3^{1/2}$ and $2^{2/3}$. Raising both to the power of $6$:\n$$(3^{1/2})^6 \\quad \\text{vs.} \\quad (2^{2/3})^6$$\n$$3^3 \\quad \\text{vs.} \\quad 2^4$$\n$$27 \\quad \\text{vs.} \\quad 16$$\nSince $27  16$, it follows that $H(X)  H_{\\text{max}}$.\nThe difference $\\Delta H = H_{\\text{max}} - H(X)$ represents the redundancy in the system or, equivalently, the Kullback-Leibler divergence from the given distribution $P$ to the uniform distribution $U$, $D_{KL}(P||U)$. It quantifies the reduction in uncertainty (entropy) due to the system's deviation from a state of maximum ignorance (the uniform distribution). The non-uniform probabilities $(\\frac{1}{2}, \\frac{1}{3}, \\frac{1}{6})$ represent prior information about the system, which makes it more predictable and thus lowers its entropy relative to the maximum possible value.",
            "answer": "$$\\boxed{\\frac{2}{3}\\ln 2 + \\frac{1}{2}\\ln 3}$$"
        },
        {
            "introduction": "Having established the basics of entropy for a single variable, we now turn to the relationship between two variables. This next practice presents a classic and crucial demonstration of why mutual information, $I(X;Y)$, is an indispensable tool for complexity science. By working through this example, you will prove that two variables can be entirely uncorrelated in a linear sense (i.e., have zero covariance) while being strongly dependent, showcasing how mutual information uniquely captures the non-linear dependencies that are ubiquitous in complex systems .",
            "id": "4283601",
            "problem": "Consider a node-level state variable $X$ in a complex network whose instantaneous value is constrained to the discrete set $\\{-1, 0, 1\\}$. Suppose the node is inactive with probability $P(X=0)=1-p$ and active with equally likely positive or negative sign $P(X=1)=p/2$ and $P(X=-1)=p/2$, for a parameter $p \\in (0,1)$. Define an observation channel that reports the squared magnitude $Y=X^{2}$, which takes values in $\\{0,1\\}$ and is intended to capture nonlinear activity while ignoring sign.\n\nStarting from core definitions in information theory and probability (without using any shortcut formulas), do the following:\n- Compute the covariance between $X$ and $Y$ and show it vanishes for all $p \\in (0,1)$, thereby establishing that $X$ and $Y$ are uncorrelated.\n- Derive a closed-form analytic expression for the mutual information (MI) $I(X;Y)$ between $X$ and $Y$, expressed in natural units (nats), highlighting that $I(X;Y)0$ for all $p \\in (0,1)$.\n\nReport your final answer as a single closed-form expression for $I(X;Y)$ in terms of $p$. No rounding is required, and no physical units are involved.",
            "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and self-contained.\n\n### Step 1: Extract Givens\n- A discrete random variable $X$ takes values in the set $\\{-1, 0, 1\\}$.\n- The probability mass function (PMF) for $X$ is given by:\n  - $P(X=0) = 1-p$\n  - $P(X=1) = p/2$\n  - $P(X=-1) = p/2$\n- The parameter $p$ is constrained to $p \\in (0,1)$.\n- A second random variable $Y$ is defined as a function of $X$: $Y = X^2$.\n- The tasks are:\n  1. Compute the covariance $\\text{Cov}(X,Y)$ and show it is zero.\n  2. Derive the mutual information $I(X;Y)$ in nats and show it is positive.\n- The final answer is to be the closed-form expression for $I(X;Y)$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is rooted in standard probability and information theory. The definitions of the variables are clear, and the probability distribution for $X$ is valid, as the probabilities sum to unity: $(1-p) + p/2 + p/2 = 1$. The parameter range $p \\in (0,1)$ is well-defined. The tasks involve standard calculations of covariance and mutual information, which are fundamental concepts. The problem is a classic illustration of how zero correlation does not imply statistical independence. There are no scientific contradictions, ambiguities, or missing information.\n\n### Step 3: Verdict and Action\nThe problem is valid. A solution will be provided.\n\n### Covariance Calculation\nThe covariance between two random variables $X$ and $Y$ is defined as $\\text{Cov}(X,Y) = E[XY] - E[X]E[Y]$. We must compute the expected values $E[X]$, $E[Y]$, and $E[XY]$.\n\n1.  **Expected Value of $X$:**\n    The expected value of $X$ is given by $E[X] = \\sum_{x} x P(X=x)$.\n    $$E[X] = (-1) \\cdot P(X=-1) + (0) \\cdot P(X=0) + (1) \\cdot P(X=1)$$\n    $$E[X] = (-1) \\left(\\frac{p}{2}\\right) + (0)(1-p) + (1)\\left(\\frac{p}{2}\\right) = -\\frac{p}{2} + \\frac{p}{2} = 0$$\n\n2.  **Probability Distribution and Expected Value of $Y$:**\n    The variable $Y$ is defined as $Y=X^2$. The possible values for $Y$ are $0^2=0$ and $(\\pm 1)^2=1$. We determine the PMF for $Y$.\n    -   $P(Y=0) = P(X^2=0) = P(X=0) = 1-p$.\n    -   $P(Y=1) = P(X^2=1) = P(X=1 \\text{ or } X=-1) = P(X=1) + P(X=-1) = \\frac{p}{2} + \\frac{p}{2} = p$.\n    The expected value of $Y$ is $E[Y] = \\sum_{y} y P(Y=y)$.\n    $$E[Y] = (0) \\cdot P(Y=0) + (1) \\cdot P(Y=1) = (0)(1-p) + (1)(p) = p$$\n\n3.  **Expected Value of the Product $XY$:**\n    The product is $XY = X \\cdot X^2 = X^3$. The random variable $X^3$ takes values $(-1)^3=-1$, $0^3=0$, and $1^3=1$.\n    The expected value is $E[XY] = E[X^3] = \\sum_{x} x^3 P(X=x)$.\n    $$E[XY] = (-1)^3 \\cdot P(X=-1) + (0)^3 \\cdot P(X=0) + (1)^3 \\cdot P(X=1)$$\n    $$E[XY] = (-1)\\left(\\frac{p}{2}\\right) + (0)(1-p) + (1)\\left(\\frac{p}{2}\\right) = -\\frac{p}{2} + \\frac{p}{2} = 0$$\n\n4.  **Final Covariance Calculation:**\n    Substituting the expected values into the covariance formula:\n    $$\\text{Cov}(X,Y) = E[XY] - E[X]E[Y] = 0 - (0)(p) = 0$$\n    The covariance is zero for all $p \\in (0,1)$, which establishes that $X$ and $Y$ are uncorrelated.\n\n### Mutual Information Calculation\nThe mutual information $I(X;Y)$ measures the statistical dependence between $X$ and $Y$. It is defined as $I(X;Y) = H(Y) - H(Y|X)$, where $H(\\cdot)$ denotes Shannon entropy and $H(\\cdot|\\cdot)$ denotes conditional entropy. The base of the logarithm is $e$ for units of nats.\n\n1.  **Entropy of $Y$, $H(Y)$:**\n    The entropy of a discrete random variable $Y$ is $H(Y) = -\\sum_{y} P(Y=y) \\ln(P(Y=y))$.\n    $$H(Y) = -[P(Y=0)\\ln(P(Y=0)) + P(Y=1)\\ln(P(Y=1))]$$\n    $$H(Y) = -[(1-p)\\ln(1-p) + p\\ln(p)]$$\n    This is the binary entropy function.\n\n2.  **Conditional Entropy of $Y$ given $X$, $H(Y|X)$:**\n    The conditional entropy $H(Y|X)$ quantifies the remaining uncertainty in $Y$ when $X$ is known. It is defined as $H(Y|X) = -\\sum_{x,y} P(X=x, Y=y) \\ln(P(Y=y|X=x))$.\n    However, $Y$ is a deterministic function of $X$ ($Y=X^2$). This means that once the value of $X$ is known, the value of $Y$ is fully determined. For any given $x$, the conditional probability $P(Y=y|X=x)$ is $1$ for $y=x^2$ and $0$ otherwise.\n    The entropy of a deterministic outcome is zero. That is, for any specific value $x$, the conditional entropy $H(Y|X=x) = -\\sum_y P(Y=y|X=x)\\ln(P(Y=y|X=x)) = - (1 \\ln 1 + 0) = 0$.\n    Since $H(Y|X=x)=0$ for all $x$, the total conditional entropy $H(Y|X) = \\sum_x P(X=x) H(Y|X=x)$ is also zero.\n    $$H(Y|X) = 0$$\n\n3.  **Final Mutual Information Calculation:**\n    Substituting the expressions for $H(Y)$ and $H(Y|X)$ into the definition of mutual information:\n    $$I(X;Y) = H(Y) - H(Y|X) = -[(1-p)\\ln(1-p) + p\\ln(p)] - 0$$\n    $$I(X;Y) = -p\\ln(p) - (1-p)\\ln(1-p)$$\n\n    This result highlights a key principle: uncorrelatedness does not imply independence. While $\\text{Cov}(X,Y)=0$, the variables are clearly dependent, as knowing $X$ removes all uncertainty about $Y$. This dependence is captured by the mutual information.\n\n4.  **Positivity of Mutual Information:**\n    The mutual information $I(X;Y)$ is non-negative, $I(X;Y) \\ge 0$, with equality if and only if $X$ and $Y$ are statistically independent. For $p \\in (0,1)$, we have $0  p  1$ and $0  1-p  1$. For any value $z \\in (0,1)$, $\\ln(z)$ is negative.\n    -   The term $-p\\ln(p)$ consists of a negative number ($\\ln p$) multiplied by a negative number ($-p$), so it is positive.\n    -   The term $-(1-p)\\ln(1-p)$ consists of a negative number ($\\ln(1-p)$) multiplied by a negative number ($-(1-p)$), so it is also positive.\n    The sum of two positive terms is positive. Thus, $I(X;Y)  0$ for all $p \\in (0,1)$, confirming that $X$ and $Y$ are not independent. Independence only occurs at the boundaries $p=0$ and $p=1$, where the entropy $H(Y)$ becomes zero.",
            "answer": "$$\\boxed{-p \\ln(p) - (1-p) \\ln(1-p)}$$"
        },
        {
            "introduction": "The final practice transitions from analytical exercises to a hands-on computational application, bridging theory with a central challenge in modern network science. This problem asks you to investigate Exponential Random Graph Model (ERGM) degeneracy, a phenomenon where slight changes in parameters can cause the model to generate only empty or complete graphs. By programmatically enumerating a small graph space and computing various information-theoretic quantities, you will gain a concrete understanding of how model degeneracy manifests as a form of entropy collapse and develop an intuition for the information geometry of a statistical network model's parameter space .",
            "id": "4283598",
            "problem": "Consider the Exponential Random Graph Model (ERGM) for undirected, labeled, simple graphs on $n$ nodes, defined by the probability mass function $P_{\\boldsymbol{\\theta}}(G) = \\frac{1}{Z(\\boldsymbol{\\theta})} \\exp\\left(\\theta_{e} E(G) + \\theta_{t} T(G)\\right)$, where $E(G)$ is the number of edges in $G$, $T(G)$ is the number of triangles in $G$, $\\boldsymbol{\\theta} = (\\theta_{e}, \\theta_{t})$ are real-valued parameters, and $Z(\\boldsymbol{\\theta})$ is the normalizing constant (also called the partition function). Let $n = 6$. The set of all such graphs has size $2^{\\binom{n}{2}}$ because each of the $\\binom{n}{2}$ possible edges may be present or absent.\n\nYour task is to write a program that, for specified parameter pairs $\\boldsymbol{\\theta}$, computes the following quantities derived from first principles of information theory for random variables induced by $P_{\\boldsymbol{\\theta}}$:\n\n1. The distribution of the edge density $D(G) = \\frac{E(G)}{\\binom{n}{2}}$ induced by $P_{\\boldsymbol{\\theta}}$. From this distribution, compute the Shannon entropy $H_{D}(\\boldsymbol{\\theta})$ in natural logarithm units (nats), defined as $H_{D}(\\boldsymbol{\\theta}) = -\\sum_{d} p(d) \\log p(d)$, where $p(d)$ is the probability that $D(G)$ equals $d$.\n\n2. The Shannon entropy of the full graph distribution $H_{G}(\\boldsymbol{\\theta})$ in nats, defined as $H_{G}(\\boldsymbol{\\theta}) = -\\sum_{G} P_{\\boldsymbol{\\theta}}(G) \\log P_{\\boldsymbol{\\theta}}(G)$.\n\n3. The mutual information between the edge density $D$ and the triangle count $T$ in nats under $P_{\\boldsymbol{\\theta}}$, defined as $I(D;T) = \\sum_{d,t} p(d,t) \\log \\frac{p(d,t)}{p(d)\\,p(t)}$, where $p(d,t)$ is the joint distribution of $D$ and $T$, and $p(d)$ and $p(t)$ are the corresponding marginals. Note that the edge density $D$ is a deterministic function of the edge count $E$, so $I(D;T) = I(E;T)$ due to invariance of mutual information under bijective transformations.\n\n4. A bimodality flag for the edge-density distribution that is a boolean indicating whether the distribution over edge counts has two distinct local maxima separated by at least $3$ edges, and whose second-highest peak probability is at least $50\\%$ of the highest peak probability. Concretely, if $p_{k}$ is the probability mass for having $k$ edges (with $k \\in \\{0,1,\\dots,\\binom{n}{2}\\}$), we declare bimodality if there exist indices $i \\neq j$ such that both $p_{i}$ and $p_{j}$ are local maxima, $|i-j| \\ge 3$, and $\\min(p_{i},p_{j}) \\ge 0.5 \\cdot \\max_{m} p_{m}$.\n\nUse the following foundational base:\n- The Shannon entropy for a discrete distribution $P$ is defined as $H(P) = -\\sum_{x} P(x)\\log P(x)$, using the natural logarithm.\n- Mutual information between discrete random variables $X$ and $Y$ with joint distribution $P(x,y)$ and marginals $P(x)$ and $P(y)$ is defined as $I(X;Y) = \\sum_{x,y} P(x,y)\\log\\left(\\frac{P(x,y)}{P(x)P(y)}\\right)$.\n- The maximum entropy principle for distributions on discrete sets under linear constraints on expected sufficient statistics yields the exponential family form $P_{\\boldsymbol{\\lambda}}(G) \\propto \\exp\\left(\\sum_{r} \\lambda_{r} s_{r}(G)\\right)$, where $s_{r}(G)$ are the constrained statistics and $\\lambda_{r}$ are Lagrange multipliers. In this problem, choosing $s_{1}(G)=E(G)$ and $s_{2}(G)=T(G)$ leads to the ERGM form.\n\nYou must compute all quantities exactly by enumerating all graphs on $n=6$ nodes (i.e., no sampling approximations). To do this, represent each labeled undirected graph as a bit mask over the $\\binom{n}{2}$ possible edges, compute $E(G)$ by counting set bits, compute $T(G)$ by checking all $\\binom{n}{3}$ triplets of nodes, and then compute $P_{\\boldsymbol{\\theta}}(G)$ via normalization of weights $\\exp\\left(\\theta_{e} E(G) + \\theta_{t} T(G)\\right)$.\n\nTest Suite:\n- Case $1$: $\\theta_{e} = 0$, $\\theta_{t} = 0$.\n- Case $2$: $\\theta_{e} = -0.8$, $\\theta_{t} = 1.2$.\n- Case $3$: $\\theta_{e} = -1.0$, $\\theta_{t} = 0.0$.\n- Case $4$: $\\theta_{e} = -0.5$, $\\theta_{t} = 1.5$.\n\nFor each case, compute:\n- $H_{D}(\\boldsymbol{\\theta})$ in nats.\n- The bimodality flag as a boolean.\n- $I(D;T)$ in nats.\n- $H_{G}(\\boldsymbol{\\theta})$ in nats.\n\nScientific realism and interpretability requirements:\n- Explicitly use $n=6$, which makes exhaustive enumeration feasible because there are $2^{\\binom{6}{2}} = 2^{15}$ labeled graphs.\n- All logarithms must be natural logarithms, so entropies and mutual information are measured in nats.\n- The program must implement only the above definitions and the ERGM model; no approximations or external data are permitted.\n\nFinal Output Format:\n- Your program should produce a single line of output containing the results for the four test cases as a comma-separated list enclosed in square brackets.\n- Each test caseâ€™s result must be represented as a comma-free sublist of the form $[H_{D},\\text{bimodal},I(D;T),H_{G}]$, where $H_{D}$, $I(D;T)$, and $H_{G}$ are floats rounded to $6$ decimal places, and $\\text{bimodal}$ is a boolean.\n- Therefore, the final output should look like $[[h_{1},b_{1},i_{1},g_{1}],[h_{2},b_{2},i_{2},g_{2}],[h_{3},b_{3},i_{3},g_{3}],[h_{4},b_{4},i_{4},g_{4}]]$ with no spaces after commas.\n\nYour program must be self-contained, use $n=6$, and implement the above from first principles. No user input is permitted. All outputs must be floats in nats rounded to $6$ decimals and booleans as described, printed in exactly the specified single-line format.",
            "solution": "The user-provided problem is a well-defined computational task within the domain of complex systems and network science. It asks for the calculation of several information-theoretic quantities for an Exponential Random Graph Model (ERGM) on a small, fixed number of nodes.\n\n### **Problem Validation**\n\n1.  **Givens Extraction**:\n    *   **Model**: ERGM with probability mass function $P_{\\boldsymbol{\\theta}}(G) = \\frac{1}{Z(\\boldsymbol{\\theta})} \\exp\\left(\\theta_{e} E(G) + \\theta_{t} T(G)\\right)$, for undirected, labeled, simple graphs.\n    *   **Statistics**: $E(G)$ is the edge count, $T(G)$ is the triangle count.\n    *   **Parameters**: $\\boldsymbol{\\theta} = (\\theta_{e}, \\theta_{t})$.\n    *   **Nodes**: $n = 6$.\n    *   **Graph Space**: All $2^{\\binom{6}{2}} = 2^{15}$ graphs on $n=6$ nodes.\n    *   **Quantities to compute**:\n        1.  $H_{D}(\\boldsymbol{\\theta})$: Shannon entropy of the edge density distribution.\n        2.  $H_{G}(\\boldsymbol{\\theta})$: Shannon entropy of the full graph distribution.\n        3.  $I(D;T)$: Mutual information between edge density and triangle count. The problem correctly notes $I(D;T)=I(E;T)$.\n        4.  Bimodality flag for the edge count distribution, with specific conditions: two local maxima at indices $i,j$ with $|i-j| \\ge 3$ and $\\min(p_i, p_j) \\ge 0.5 \\cdot \\max_k p_k$.\n    *   **Method**: Exact enumeration of all $2^{15}$ graphs.\n    *   **Test Cases**: $(\\theta_e, \\theta_t) \\in \\{(0, 0), (-0.8, 1.2), (-1.0, 0.0), (-0.5, 1.5)\\}$.\n    *   **Logarithm Base**: Natural logarithm (nats).\n\n2.  **Validation against Criteria**:\n    *   **Scientifically Grounded**: The problem is based on the standard ERGM framework and fundamental definitions from information theory. All concepts are well-established in statistical physics and network science.\n    *   **Well-Posed**: The problem is specified on a finite state space with all parameters and functions clearly defined. The small size of the graph space ($n=6$) makes the stipulated method of exhaustive enumeration computationally feasible, ensuring a unique and stable solution can be found.\n    *   **Objective**: The problem is stated in precise mathematical language, free from ambiguity or subjective elements.\n    *   **Completeness**: All necessary information, including the model, parameters, constants ($n=6$), and precise definitions of quantities to be computed, is provided.\n    *   **Consistency**: There are no contradictions in the problem statement. The note regarding $I(D;T)=I(E;T)$ is correct and helpful.\n\n3.  **Verdict**:\n    *   The problem is **valid**. It is a rigorous, self-contained, and non-trivial computational exercise in statistical network modeling.\n\n---\n\n### **Methodology and Solution Design**\n\nThe solution proceeds in two main stages: a one-time pre-computation of graph statistics, followed by a calculation loop for each parameter set $\\boldsymbol{\\theta}$.\n\n**1. Pre-computation of Sufficient Statistics**\n\nSince the calculations must be performed for multiple parameter sets $(\\theta_e, \\theta_t)$, it is efficient to first enumerate all possible graphs and tabulate the joint frequency of the sufficient statistics, namely the edge count $E(G)$ and the triangle count $T(G)$.\nA graph on $n=6$ nodes has $\\binom{6}{2} = 15$ possible edges. The total number of distinct labeled graphs is $2^{15} = 32768$. Each graph can be uniquely represented by a $15$-bit integer, where each bit corresponds to the presence or absence of a specific edge.\n\nThe algorithm is as follows:\n*   Establish a canonical ordering of the $15$ possible edges. An edge $(u,v)$ with $uv$ can be mapped to a unique index.\n*   Pre-calculate masks for the $\\binom{6}{3}=20$ possible triangles. Each mask is a $15$-bit integer with three bits set, corresponding to the three edges forming a triangle.\n*   Initialize a 2D array, `counts[e][t]`, of size $(16 \\times 21)$ to store the number of graphs with $e$ edges and $t$ triangles. The maximum number of edges is $15$, and the maximum number of triangles is $20$.\n*   Iterate through all integers $g$ from $0$ to $2^{15}-1$:\n    *   The number of edges, $E(G_g)$, is the population count (number of set bits) of the integer $g$.\n    *   The number of triangles, $T(G_g)$, is found by checking how many of the pre-calculated triangle masks are subsets of the bits set in $g$.\n    *   Increment the corresponding entry `counts[E(G_g)][T(G_g)]$.\n\nThis `counts` matrix, let's call it $C_{et}$, contains all the necessary combinatorial information about the graph ensemble.\n\n**2. Calculation of Information-Theoretic Quantities for a given $\\boldsymbol{\\theta}$**\n\nFor each parameter pair $(\\theta_e, \\theta_t)$, we can now compute the required quantities without re-enumerating graphs. The probability of any specific graph $G$ with $e$ edges and $t$ triangles is $P(G) = \\frac{1}{Z} \\exp(\\theta_e e + \\theta_t t)$. The probability of observing any graph with $(e,t)$ statistics is the joint probability $p(e,t)$:\n$$p(e,t) = \\frac{C_{et} \\exp(\\theta_e e + \\theta_t t)}{Z(\\boldsymbol{\\theta})}$$\nwhere the partition function $Z(\\boldsymbol{\\theta})$ is the normalization constant:\n$$Z(\\boldsymbol{\\theta}) = \\sum_{e=0}^{15} \\sum_{t=0}^{20} C_{et} \\exp(\\theta_e e + \\theta_t t)$$\n\nTo maintain numerical stability, especially for large $|\\theta_e|$ or $|\\theta_t|$, these calculations are performed in log-space using the `logsumexp` technique.\nThe marginal probabilities $p(e) = \\sum_t p(e,t)$ and $p(t) = \\sum_e p(e,t)$ are then derived.\n\nWith these probability distributions, the four quantities are computed:\n\n*   **$H_D(\\boldsymbol{\\theta})$**: This is the entropy of the edge count distribution $p(e)$, as $D$ is a direct scaling of $E$.\n    $$H_D(\\boldsymbol{\\theta}) = H(E) = -\\sum_{e=0}^{15} p(e) \\log p(e)$$\n*   **Bimodality Flag**: This is determined by analyzing the marginal distribution $p(e)$. We identify all local maxima in the $p(e)$ series and check if any pair of maxima $(i, j)$ satisfies the conditions: $|i-j| \\ge 3$ and $\\min(p(i), p(j)) \\ge 0.5 \\cdot \\max_{k} p(k)$.\n*   **$I(D;T)$**: Calculated as $I(E;T)$ using the relation $I(X;Y) = H(X) + H(Y) - H(X,Y)$.\n    $$I(E;T) = H(E) + H(T) - H(E,T)$$\n    where $H(T) = -\\sum_t p(t) \\log p(t)$ and $H(E,T) = -\\sum_{e,t} p(e,t) \\log p(e,t)$.\n*   **$H_G(\\boldsymbol{\\theta})$**: The entropy of the full graph distribution can be expressed elegantly in terms of the partition function and the expected values of the statistics:\n    $$H_G(\\boldsymbol{\\theta}) = \\log Z(\\boldsymbol{\\theta}) - \\theta_e \\langle E \\rangle - \\theta_t \\langle T \\rangle$$\n    where $\\langle E \\rangle = \\sum_e e \\cdot p(e)$ and $\\langle T \\rangle = \\sum_t t \\cdot p(t)$. This formula avoids summing over all $2^{15}$ individual graph probabilities and is much more efficient.\n\nThis structured approach ensures correctness, numerical stability, and computational efficiency.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import logsumexp\nfrom itertools import combinations\n\ndef solve():\n    \"\"\"\n    Main function to solve the ERGM problem.\n    It orchestrates the pre-computation of graph statistics and then computes\n    the required information-theoretic quantities for each test case.\n    \"\"\"\n\n    def precompute_statistics():\n        \"\"\"\n        Enumerates all 32768 graphs on n=6 nodes to compute a joint histogram\n        of edge and triangle counts.\n\n        Returns:\n            np.ndarray: A (16 x 21) matrix where counts[e, t] is the number of\n                        graphs with 'e' edges and 't' triangles.\n        \"\"\"\n        n = 6\n        num_edges = n * (n - 1) // 2  # 15\n        num_graphs = 1  num_edges   # 32768\n        max_triangles = n * (n - 1) * (n - 2) // 6 # 20\n\n        nodes = range(n)\n        edge_list = list(combinations(nodes, 2))\n        edge_to_idx = {edge: i for i, edge in enumerate(edge_list)}\n\n        triplet_list = list(combinations(nodes, 3))\n        triplet_masks = []\n        for i, j, k in triplet_list:\n            idx1 = edge_to_idx[(i, j)]\n            idx2 = edge_to_idx[(i, k)]\n            idx3 = edge_to_idx[(j, k)]\n            mask = (1  idx1) | (1  idx2) | (1  idx3)\n            triplet_masks.append(mask)\n\n        counts = np.zeros((num_edges + 1, max_triangles + 1), dtype=np.int64)\n\n        for g_idx in range(num_graphs):\n            num_e = g_idx.bit_count()\n            num_t = 0\n            for mask in triplet_masks:\n                if (g_idx  mask) == mask:\n                    num_t += 1\n            counts[num_e, num_t] += 1\n        return counts\n\n    def solve_for_theta(theta_e, theta_t, counts):\n        \"\"\"\n        Computes the four required quantities for a given parameter set.\n\n        Args:\n            theta_e (float): The edge parameter.\n            theta_t (float): The triangle parameter.\n            counts (np.ndarray): The pre-computed statistics matrix.\n\n        Returns:\n            list: A list containing [H_D, bimodal_flag, I(D;T), H_G].\n        \"\"\"\n        num_edges_max = counts.shape[0] - 1\n        num_triangles_max = counts.shape[1] - 1\n        \n        e_vals = np.arange(num_edges_max + 1)\n        t_vals = np.arange(num_triangles_max + 1)\n\n        # Calculate log-weights and use logsumexp for numerical stability\n        log_w_matrix = theta_e * e_vals[:, np.newaxis] + theta_t * t_vals[np.newaxis, :]\n        \n        log_counts = np.full_like(counts, -np.inf, dtype=float)\n        non_zero_counts = counts > 0\n        log_counts[non_zero_counts] = np.log(counts[non_zero_counts])\n\n        log_unnormalized_p = log_counts + log_w_matrix\n        log_Z = logsumexp(log_unnormalized_p)\n        log_p_et = log_unnormalized_p - log_Z\n        p_et = np.exp(log_p_et)\n\n        # Marginal distributions\n        p_e = np.sum(p_et, axis=1)\n        p_t = np.sum(p_et, axis=0)\n\n        # 1. H_D (Shannon entropy of the edge density/count)\n        H_D = -np.sum(p_e[p_e > 0] * np.log(p_e[p_e > 0]))\n        \n        # 2. Bimodality Flag\n        local_max_indices = []\n        if num_edges_max > 0 and p_e[0] > p_e[1]:\n            local_max_indices.append(0)\n        for i in range(1, num_edges_max):\n            if p_e[i] > p_e[i-1] and p_e[i] > p_e[i+1]:\n                local_max_indices.append(i)\n        if num_edges_max > 0 and p_e[num_edges_max] > p_e[num_edges_max-1]:\n            local_max_indices.append(num_edges_max)\n        \n        bimodal = False\n        if len(local_max_indices) >= 2:\n            max_p_overall = np.max(p_e)\n            if max_p_overall > 1e-9: # Guard against floating point noise / all zero prob\n                for i, j in combinations(local_max_indices, 2):\n                    if abs(i - j) >= 3 and min(p_e[i], p_e[j]) >= 0.5 * max_p_overall:\n                        bimodal = True\n                        break\n        \n        # 3. I(D;T) (Mutual information between edge count and triangle count)\n        H_T = -np.sum(p_t[p_t > 0] * np.log(p_t[p_t > 0]))\n        H_ET = -np.sum(p_et[p_et > 0] * np.log(p_et[p_et > 0]))\n        I_ET = H_D + H_T - H_ET\n        I_ET = max(0, I_ET) # Correct for potential small negative values due to float precision\n\n        # 4. H_G (Shannon entropy of the full graph distribution)\n        exp_e = np.sum(p_e * e_vals)\n        exp_t = np.sum(p_t * t_vals)\n        H_G = log_Z - theta_e * exp_e - theta_t * exp_t\n\n        return [round(float(H_D), 6), bimodal, round(float(I_ET), 6), round(float(H_G), 6)]\n\n    # === Main execution block ===\n    \n    # 1. Perform the one-time expensive computation\n    counts_matrix = precompute_statistics()\n    \n    # 2. Define the test cases from the problem statement.\n    test_cases = [\n        (0.0, 0.0),\n        (-0.8, 1.2),\n        (-1.0, 0.0),\n        (-0.5, 1.5),\n    ]\n\n    # 3. Calculate results for each case\n    results = []\n    for theta_e, theta_t in test_cases:\n        result = solve_for_theta(theta_e, theta_t, counts_matrix)\n        results.append(result)\n        \n    # 4. Final print statement in the exact required format.\n    # The str() representation of a list of lists with booleans and floats is used,\n    # and spaces are removed to match the required comma-free sublist format.\n    print(str(results).replace(\" \", \"\"))\n\nsolve()\n```"
        }
    ]
}