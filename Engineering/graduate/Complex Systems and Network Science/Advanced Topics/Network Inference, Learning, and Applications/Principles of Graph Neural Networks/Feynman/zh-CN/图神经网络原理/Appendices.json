{
    "hands_on_practices": [
        {
            "introduction": "标准消息传递神经网络（MPNN）的表达能力受到一维Weisfeiler-Lehman（1-WL）同构测试的限制，这意味着它们无法区分某些具有相似局部邻域结构的非同构图。这个练习  通过一个经典的例子，具体展示了GNN在区分一个6元环和两个不相交的3元环时的局限性。通过这个计算，你将亲身体会到GNN表达能力的理论边界。",
            "id": "4298398",
            "problem": "考虑两个节点集为 $\\{1,2,3,4,5,6\\}$ 的无向简单图：\n- 图 $G_{1}$ 是边集为 $\\{(1,2),(2,3),(3,4),(4,5),(5,6),(6,1)\\}$ 的 $6$-环 $C_{6}$。\n- 图 $G_{2}$ 是两个 $3$-环 $C_{3} \\cup C_{3}$ 的不相交并集，其边集为 $\\{(1,2),(2,3),(3,1),(4,5),(5,6),(6,4)\\}$。\n\n两个图中每个节点 $i$ 都初始化为相同的标量特征 $h_{i}^{(0)} = 1$。考虑一个图神经网络 (GNN)，具体来说是一个消息传递神经网络 (MPNN)，它使用求和聚合，无边特征，并且每层的线性更新规则由下式给出\n$$\nh_{i}^{(t+1)} \\;=\\; \\alpha \\, h_{i}^{(t)} \\;+\\; \\beta \\sum_{j \\in \\mathcal{N}(i)} h_{j}^{(t)},\n$$\n其中 $\\alpha, \\beta \\in \\mathbb{R}$ 是固定的标量，$\\mathcal{N}(i)$ 表示节点 $i$ 的邻居集合。\n\n仅使用关于置换不变消息传递和一维Weisfeiler-Lehman (WL) 颜色细化测试的基本定义，计算 $G_1$ 和 $G_2$ 在两层之后的节点表示，并严格证明它们是否相同。你的最终答案必须是 $h_{i}^{(2)}$ 的共同值（如果所有节点的值都相同，则该值与 $i$ 无关），表示为关于 $\\alpha$ 和 $\\beta$ 的单一封闭形式解析表达式。无需四舍五入，不涉及单位。不要报告中间值；仅提供 $h_{i}^{(2)}$ 的最终表达式。",
            "solution": "该问题定义明确，具有科学依据，并提供了推导唯一解所需的所有信息。图的定义、初始节点特征以及消息传递更新规则都是精确且数学上一致的。因此，可以推导出一个完整的解。\n\n问题的核心在于将给定的消息传递神经网络 (MPNN) 更新规则迭代应用于两个不同的图结构 $G_1$ 和 $G_2$。在第 $t+1$ 层（或时间步），节点 $i$ 的特征向量 $h_i$ 的更新规则如下：\n$$\nh_{i}^{(t+1)} \\;=\\; \\alpha \\, h_{i}^{(t)} \\;+\\; \\beta \\sum_{j \\in \\mathcal{N}(i)} h_{j}^{(t)}\n$$\n其中 $\\alpha$ 和 $\\beta$ 是标量参数，$\\mathcal{N}(i)$ 是节点 $i$ 的邻居集合。\n\n给定初始条件，在第 $t=0$ 层，两个图中的所有节点都具有相同的标量特征 $h_{i}^{(0)} = 1$。任务是计算两层之后的节点特征，即 $h_{i}^{(2)}$。\n\n首先，我们必须分析两个图 $G_1$ 和 $G_2$ 的结构。\n图 $G_1$ 是 $6$-环, $C_6$。在环图中，每个节点都恰好与其他两个节点相连。因此，对于 $G_1$ 中的任何节点 $i$，其度为 $d(i) = |\\mathcal{N}(i)| = 2$。\n图 $G_2$ 是两个 $3$-环的不相交并集, $C_3 \\cup C_3$。在 $3$-环中，每个节点也恰好与其他两个节点相连。因此，对于 $G_2$ 中的任何节点 $i$，其度也为 $d(i) = |\\mathcal{N}(i)| = 2$。\n一个关键的观察是，$G_1$ 和 $G_2$ 都是 $2$-正则图。这个结构特性——即每个节点都具有相同的度——将是决定结果的核心。\n\n让我们逐层进行计算。\n\n**第1层计算 ($t=0 \\to t=1$):**\n\n我们使用所有 $i \\in \\{1, 2, 3, 4, 5, 6\\}$ 的初始特征 $h_{i}^{(0)} = 1$ 来计算 $h_{i}^{(1)}$。更新规则是：\n$$\nh_{i}^{(1)} \\;=\\; \\alpha \\, h_{i}^{(0)} \\;+\\; \\beta \\sum_{j \\in \\mathcal{N}(i)} h_{j}^{(0)}\n$$\n对于 $G_1$ 或 $G_2$ 中的任何节点 $i$，我们有 $d(i)=2$。设 $i$ 的邻居为 $j_1$ 和 $j_2$。求和项变为：\n$$\n\\sum_{j \\in \\mathcal{N}(i)} h_{j}^{(0)} \\;=\\; h_{j_1}^{(0)} + h_{j_2}^{(0)} \\;=\\; 1 + 1 \\;=\\; 2\n$$\n对于两个图中的所有节点，这个和是恒定的，因为每个节点恰好有两个邻居，并且所有邻居节点的初始特征值都为 $1$。\n\n将此代入 $h_{i}^{(1)}$ 的更新规则中：\n$$\nh_{i}^{(1)} \\;=\\; \\alpha (1) \\;+\\; \\beta (2) \\;=\\; \\alpha + 2\\beta\n$$\n这个结果与节点索引 $i$ 和图（$G_1$ 或 $G_2$）无关。经过一层消息传递后，两个图中的所有六个节点都具有相同的特征值 $h_{i}^{(1)} = \\alpha + 2\\beta$。\n\n**第2层计算 ($t=1 \\to t=2$):**\n\n接下来，我们使用第1层的特征 $h_{i}^{(1)} = \\alpha + 2\\beta$ 来计算 $h_{i}^{(2)}$。更新规则是：\n$$\nh_{i}^{(2)} \\;=\\; \\alpha \\, h_{i}^{(1)} \\;+\\; \\beta \\sum_{j \\in \\mathcal{N}(i)} h_{j}^{(1)}\n$$\n同样，对于 $G_1$ 或 $G_2$ 中的任何节点 $i$，其度为 $d(i)=2$。设其邻居为 $j_1$ 和 $j_2$。由于所有节点在上一步中获得了相同的特征值，我们有 $h_{j_1}^{(1)} = h_{j_2}^{(1)} = \\alpha + 2\\beta$。求和项为：\n$$\n\\sum_{j \\in \\mathcal{N}(i)} h_{j}^{(1)} \\;=\\; h_{j_1}^{(1)} + h_{j_2}^{(1)} \\;=\\; (\\alpha + 2\\beta) + (\\alpha + 2\\beta) \\;=\\; 2(\\alpha + 2\\beta)\n$$\n对于两个图中的所有节点，这个和再次是恒定的。\n\n现在，我们将这个和以及 $h_{i}^{(1)}$ 的值代入 $h_{i}^{(2)}$ 的更新规则中：\n$$\nh_{i}^{(2)} \\;=\\; \\alpha (\\alpha + 2\\beta) \\;+\\; \\beta [2(\\alpha + 2\\beta)]\n$$\n我们可以提取公因式 $(\\alpha + 2\\beta)$：\n$$\nh_{i}^{(2)} \\;=\\; (\\alpha + 2\\beta)(\\alpha + 2\\beta)\n$$\n这简化为最终的封闭形式表达式：\n$$\nh_{i}^{(2)} \\;=\\; (\\alpha + 2\\beta)^2\n$$\n或者，展开各项得到 $h_{i}^{(2)} = \\alpha^2 + 2\\alpha\\beta + 2\\alpha\\beta + 4\\beta^2 = \\alpha^2 + 4\\alpha\\beta + 4\\beta^2$，结果相同。\n\n计算表明，对于所有节点 $i$ 以及两个图 $G_1$ 和 $G_2$，节点表示 $h_{i}^{(2)}$ 都是相同的。共同的值为 $(\\alpha + 2\\beta)^2$。\n\n这个结果是MPNN架构表达能力的直接后果，已知其表达能力最强不超过一维Weisfeiler-Lehman (1-WL)图同构测试。1-WL测试基于邻居颜色的多重集迭代更新节点“颜色”。对于图 $G_1$（一个 $6$-环）和 $G_2$（两个不相交的 $3$-环），1-WL测试无法区分它们。两者都是 $2$-正则图，因此在WL测试的每次迭代中（以及MPNN的每一层），从聚合的角度来看，每个节点都具有相同的局部邻域结构。从相同的节点特征（颜色）开始，每一步中对每个节点的更新都是相同的，这使得GNN无法学习到这两个非同构图之间的任何结构差异。我们的显式计算证实了这一理论局限性。因此，节点表示是相同的。",
            "answer": "$$\n\\boxed{(\\alpha + 2\\beta)^{2}}\n$$"
        },
        {
            "introduction": "在消息传递框架中，聚合函数是决定GNN如何整合邻居信息的关键。不同的聚合函数（如求和与求平均）具有不同的表达能力，直接影响模型的性能。这个练习  将通过一个简单的多重集（multiset）示例，清晰地揭示求和聚合与求平均聚合在区分不同邻域结构上的能力差异。",
            "id": "4298447",
            "problem": "考虑图神经网络（GNN），它对节点特征的多重集进行邻域聚合，并需要使用置换不变的集合函数，以确保输出不依赖于邻居节点的顺序。多重集是一种可以包含重复元素的集合。对于一个实值特征的多重集 $X = \\{x_{1}, x_{2}, \\dots, x_{n}\\}$，两种广泛使用的置换不变聚合器是和聚合器与均值聚合器，其函数定义如下\n$$\\mathrm{SUM}(X) = \\sum_{i=1}^{n} x_{i}, \\quad \\mathrm{MEAN}(X) = \\frac{1}{n} \\sum_{i=1}^{n} x_{i}.$$\n这些聚合器源于一个原则：图神经网络（GNN）中的消息传递必须以一种对输入排列不变的方式聚合邻居特征，这在处理节点邻域本身无序的图时，是复杂系统和网络科学中的一个基本要求。从这些核心定义出发，构建一个具体的论证，说明均值聚合无法区分某些不同的多重集，而和聚合则可以。为此，考虑实值节点特征 $a = \\pi$ 和两个不同的多重集\n$$M_{1} = \\{\\pi, \\pi\\}, \\quad M_{2} = \\{\\pi\\}.$$\n使用上述定义，计算 $\\mathrm{MEAN}(M_{1})$、$\\mathrm{MEAN}(M_{2})$、$\\mathrm{SUM}(M_{1})$ 和 $\\mathrm{SUM}(M_{2})$。你的最终答案应将这四个值按顺序排列成一个单行矩阵。无需四舍五入。",
            "solution": "该问题提出了一个关于图神经网络（GNN）中使用的不同聚合函数的表达能力的有效且定义明确的问题。它在科学上基于网络科学和机器学习的原理，不包含任何矛盾，并为得出唯一解提供了所有必要信息。因此，我们可以进行推导。\n\n问题要求我们分析两种置换不变的聚合函数，即和聚合器与均值聚合器，它们是为实值特征的多重集 $X = \\{x_{1}, x_{2}, \\dots, x_{n}\\}$ 定义的。其定义如下：\n$$\n\\mathrm{SUM}(X) = \\sum_{i=1}^{n} x_{i}\n$$\n和\n$$\n\\mathrm{MEAN}(X) = \\frac{1}{n} \\sum_{i=1}^{n} x_{i}\n$$\n我们给定一个特定的实值节点特征 $a = \\pi$，以及由该特征构建的两个不同的多重集：\n$$\nM_{1} = \\{\\pi, \\pi\\}\n$$\n$$\nM_{2} = \\{\\pi\\}\n$$\n任务是计算 $\\mathrm{MEAN}(M_{1})$、$\\mathrm{MEAN}(M_{2})$、$\\mathrm{SUM}(M_{1})$ 和 $\\mathrm{SUM}(M_{2})$。\n\n首先，我们计算多重集 $M_{1} = \\{\\pi, \\pi\\}$ 的聚合值。\n$M_{1}$ 中的元素数量为 $n_{1} = 2$。\n$M_{1}$ 中元素的和为 $\\pi + \\pi = 2\\pi$。\n\n使用均值聚合器的定义，我们得到 $\\mathrm{MEAN}(M_{1})$：\n$$\n\\mathrm{MEAN}(M_{1}) = \\frac{1}{n_{1}} \\sum_{i=1}^{n_{1}} x_{i} = \\frac{1}{2}(\\pi + \\pi) = \\frac{1}{2}(2\\pi) = \\pi\n$$\n\n使用和聚合器的定义，我们得到 $\\mathrm{SUM}(M_{1})$：\n$$\n\\mathrm{SUM}(M_{1}) = \\sum_{i=1}^{n_{1}} x_{i} = \\pi + \\pi = 2\\pi\n$$\n\n接下来，我们计算多重集 $M_{2} = \\{\\pi\\}$ 的聚合值。\n$M_{2}$ 中的元素数量为 $n_{2} = 1$。\n$M_{2}$ 中元素的和就是 $\\pi$。\n\n使用均值聚合器的定义，我们得到 $\\mathrm{MEAN}(M_{2})$：\n$$\n\\mathrm{MEAN}(M_{2}) = \\frac{1}{n_{2}} \\sum_{i=1}^{n_{2}} x_{i} = \\frac{1}{1}(\\pi) = \\pi\n$$\n\n使用和聚合器的定义，我们得到 $\\mathrm{SUM}(M_{2})$：\n$$\n\\mathrm{SUM}(M_{2}) = \\sum_{i=1}^{n_{2}} x_{i} = \\pi\n$$\n\n计算出的四个值是：\n1. $\\mathrm{MEAN}(M_{1}) = \\pi$\n2. $\\mathrm{MEAN}(M_{2}) = \\pi$\n3. $\\mathrm{SUM}(M_{1}) = 2\\pi$\n4. $\\mathrm{SUM}(M_{2}) = \\pi$\n\n正如问题陈述所要求的，我们可以从这些结果中观察到两种聚合器之间的一个关键区别。多重集 $M_{1}$ 和 $M_{2}$ 是不同的，因为 $M_{1}$ 包含两个元素，而 $M_{2}$ 只包含一个。然而，均值聚合器对两者产生了相同的输出：$\\mathrm{MEAN}(M_{1}) = \\mathrm{MEAN}(M_{2}) = \\pi$。这表明均值聚合器无法区分这两个不同的多重集。相比之下，和聚合器产生了不同的输出：$\\mathrm{SUM}(M_{1}) = 2\\pi$ 和 $\\mathrm{SUM}(M_{2}) = \\pi$。由于 $\\mathrm{SUM}(M_{1}) \\neq \\mathrm{SUM}(M_{2})$，和聚合器能够区分 $M_{1}$ 和 $M_{2}$。这是一个简单但有力的例证，说明了对于多重集而言，和聚合器是比均值聚合器更具表达能力的函数，这是 GNN 设计中的一个关键考虑因素。问题要求按顺序给出这四个值：$\\mathrm{MEAN}(M_{1})$、$\\mathrm{MEAN}(M_{2})$、$\\mathrm{SUM}(M_{1})$ 和 $\\mathrm{SUM}(M_{2})$。\n\n计算出的值为 $\\pi$、$\\pi$、$2\\pi$ 和 $\\pi$。我们将按要求将它们呈现在一个单行矩阵中。",
            "answer": "$$\\boxed{\\begin{pmatrix} \\pi  \\pi  2\\pi  \\pi \\end{pmatrix}}$$"
        },
        {
            "introduction": "为了克服简单聚合的局限性，图注意力网络（GAT）引入了注意力机制，允许模型为不同的邻居节点动态地学习不同的权重。这个练习  将引导你详细地完成一个GAT层的计算过程，包括注意力系数的计算和节点嵌入的更新。通过这个实践，你将揭开注意力机制的神秘面纱，并掌握其核心工作原理。",
            "id": "4298405",
            "problem": "考虑一个单头图注意力网络 (GAT) 层，作用于一个包含节点 $\\{1,2,3\\}$ 和自环的简单无向三角形图，因此在节点 $i$ 上进行聚合时，其邻域为 $N(i)=\\{1,2,3\\}$。每个节点都有一个位于 $\\mathbb{R}^{2}$ 中的特征向量。设初始节点特征为\n$$\n\\mathbf{x}_{1}=\\begin{pmatrix}1\\\\0\\end{pmatrix},\\quad\n\\mathbf{x}_{2}=\\begin{pmatrix}0\\\\1\\end{pmatrix},\\quad\n\\mathbf{x}_{3}=\\begin{pmatrix}1\\\\1\\end{pmatrix}.\n$$\n该 GAT 层使用一个共享的线性映射 $\\mathbf{W}\\in\\mathbb{R}^{2\\times 2}$ 和一个由向量 $\\mathbf{a}\\in\\mathbb{R}^{4}$ 指定的加性注意力机制，该机制作用于变换后特征的拼接。具体来说，令\n$$\n\\mathbf{W}=\\begin{pmatrix}2  -1\\\\ 1  1\\end{pmatrix},\\qquad\n\\mathbf{a}=\\begin{pmatrix}1\\\\ -1\\\\ \\tfrac{1}{2}\\\\ 0\\end{pmatrix}.\n$$\n对于目标节点 $i$ 和邻居 $j\\in N(i)$，定义激活前分数为\n$$\ns_{ij}^{(\\mathrm{pre})}=\\mathbf{a}^{\\top}\\bigl[\\mathbf{W}\\mathbf{x}_{i}\\,\\|\\,\\mathbf{W}\\mathbf{x}_{j}\\bigr],\n$$\n其中 $\\|$ 表示拼接，激活函数为带泄露的修正线性单元 (LeakyReLU)，负斜率为 $\\lambda$，即\n$$\n\\operatorname{LReLU}_{\\lambda}(z)=\\begin{cases}\nz,  z \\ge 0 \\\\\n\\lambda z,  z  0\n\\end{cases}\n$$\n且 $\\lambda=0.2$。注意力系数则通过在邻域上进行 softmax 归一化得到，\n$$\n\\alpha_{ij}=\\frac{\\exp\\bigl(\\operatorname{LReLU}_{\\lambda}(s_{ij}^{(\\mathrm{pre})})\\bigr)}{\\sum\\limits_{k\\in N(i)}\\exp\\bigl(\\operatorname{LReLU}_{\\lambda}(s_{ik}^{(\\mathrm{pre})})\\bigr)}.\n$$\n节点 $i$ 的更新后嵌入是置换不变的加权和（输出非线性为恒等函数）\n$$\n\\mathbf{h}_{i}'=\\sum_{j\\in N(i)}\\alpha_{ij}\\,\\mathbf{W}\\mathbf{x}_{j}.\n$$\n\n仅根据以上定义，对节点 $i=1$ 执行以下操作：\n1. 计算三个注意力系数 $\\alpha_{1j}$（对于 $j\\in\\{1,2,3\\}$），并通过明确显示 $\\sum_{j\\in\\{1,2,3\\}}\\alpha_{1j}=1$ 来验证它们是归一化的。\n2. 计算更新后的嵌入 $\\mathbf{h}_{1}'$。\n3. 作为最终答案，提供 $\\mathbf{h}_{1}'$ 的第一个坐标，形式为单个用指数表示的封闭形式解析表达式。不要近似。\n\n将最终答案表示为没有数值近似的封闭形式表达式。不需要四舍五入。不带单位。最终答案必须是单一的解析表达式。",
            "solution": "首先验证问题，以确保其自洽、科学上合理且定义良好。\n\n**步骤 1：提取已知条件**\n- 图：包含节点 $\\{1,2,3\\}$ 的无向三角形图。\n- 带自环的邻域：对于任意节点 $i$，$N(i)=\\{1,2,3\\}$。\n- 初始节点特征：$\\mathbf{x}_{1}=\\begin{pmatrix}1\\\\0\\end{pmatrix}$, $\\mathbf{x}_{2}=\\begin{pmatrix}0\\\\1\\end{pmatrix}$, $\\mathbf{x}_{3}=\\begin{pmatrix}1\\\\1\\end{pmatrix}$。\n- 共享线性映射：$\\mathbf{W}=\\begin{pmatrix}2  -1\\\\ 1  1\\end{pmatrix}$。\n- 注意力机制向量：$\\mathbf{a}=\\begin{pmatrix}1\\\\ -1\\\\ \\frac{1}{2}\\\\ 0\\end{pmatrix}$。\n- 激活前分数：$s_{ij}^{(\\mathrm{pre})}=\\mathbf{a}^{\\top}\\bigl[\\mathbf{W}\\mathbf{x}_{i}\\,\\|\\,\\mathbf{W}\\mathbf{x}_{j}\\bigr]$。\n- 激活函数：负斜率为 $\\lambda=0.2$ 的 LeakyReLU。\n- 注意力系数：$\\alpha_{ij}=\\frac{\\exp\\bigl(\\operatorname{LReLU}_{\\lambda}(s_{ij}^{(\\mathrm{pre})})\\bigr)}{\\sum\\limits_{k\\in N(i)}\\exp\\bigl(\\operatorname{LReLU}_{\\lambda}(s_{ik}^{(\\mathrm{pre})})\\bigr)}$。\n- 更新后的嵌入：$\\mathbf{h}_{i}'=\\sum_{j\\in N(i)}\\alpha_{ij}\\,\\mathbf{W}\\mathbf{x}_{j}$。\n- 任务是为目标节点 $i=1$ 进行计算。\n\n**步骤 2：使用提取的已知条件进行验证**\n- **科学依据**：该问题描述了一个标准的单头图注意力网络 (GAT) 层，这是机器学习中一个有效且成熟的模型。其公式与开创性的文献一致。\n- **定义良好**：所有必要的数据（节点特征、权重矩阵、图结构）和函数形式都已提供。该问题是确定性的，并导向唯一解。\n- **客观性**：问题使用精确的数学语言陈述，没有任何主观或模糊的术语。\n\n**步骤 3：结论与行动**\n问题有效。将推导一个完整的解。\n\n总体目标是计算节点 1 的更新后嵌入的第一个坐标，记为 $\\mathbf{h}_{1}'$。该过程分几个连续步骤执行。\n\n首先，我们将共享线性变换 $\\mathbf{W}$ 应用于每个节点的特征向量 $\\mathbf{x}_j$，以获得变换后的特征 $\\mathbf{h}_j = \\mathbf{W}\\mathbf{x}_j$。\n对于 $j=1$：\n$$\n\\mathbf{h}_{1} = \\mathbf{W}\\mathbf{x}_{1} = \\begin{pmatrix}2  -1\\\\ 1  1\\end{pmatrix}\\begin{pmatrix}1\\\\0\\end{pmatrix} = \\begin{pmatrix}2(1) - 1(0)\\\\ 1(1) + 1(0)\\end{pmatrix} = \\begin{pmatrix}2\\\\1\\end{pmatrix}.\n$$\n对于 $j=2$：\n$$\n\\mathbf{h}_{2} = \\mathbf{W}\\mathbf{x}_{2} = \\begin{pmatrix}2  -1\\\\ 1  1\\end{pmatrix}\\begin{pmatrix}0\\\\1\\end{pmatrix} = \\begin{pmatrix}2(0) - 1(1)\\\\ 1(0) + 1(1)\\end{pmatrix} = \\begin{pmatrix}-1\\\\1\\end{pmatrix}.\n$$\n对于 $j=3$：\n$$\n\\mathbf{h}_{3} = \\mathbf{W}\\mathbf{x}_{3} = \\begin{pmatrix}2  -1\\\\ 1  1\\end{pmatrix}\\begin{pmatrix}1\\\\1\\end{pmatrix} = \\begin{pmatrix}2(1) - 1(1)\\\\ 1(1) + 1(1)\\end{pmatrix} = \\begin{pmatrix}1\\\\2\\end{pmatrix}.\n$$\n接下来，我们计算目标节点 $i=1$ 与其邻居 $j \\in N(1) = \\{1,2,3\\}$ 的激活前注意力分数 $s_{1j}^{(\\mathrm{pre})}$。公式为 $s_{1j}^{(\\mathrm{pre})}=\\mathbf{a}^{\\top}\\bigl[\\mathbf{h}_{1}\\,\\|\\,\\mathbf{h}_{j}\\bigr]$。\n对于 $j=1$：\n$$\ns_{11}^{(\\mathrm{pre})} = \\begin{pmatrix}1  -1  \\frac{1}{2}  0\\end{pmatrix} \\begin{pmatrix}2\\\\1\\\\2\\\\1\\end{pmatrix} = 1(2) - 1(1) + \\frac{1}{2}(2) + 0(1) = 2 - 1 + 1 = 2.\n$$\n对于 $j=2$：\n$$\ns_{12}^{(\\mathrm{pre})} = \\begin{pmatrix}1  -1  \\frac{1}{2}  0\\end{pmatrix} \\begin{pmatrix}2\\\\1\\\\-1\\\\1\\end{pmatrix} = 1(2) - 1(1) + \\frac{1}{2}(-1) + 0(1) = 2 - 1 - \\frac{1}{2} = \\frac{1}{2}.\n$$\n对于 $j=3$：\n$$\ns_{13}^{(\\mathrm{pre})} = \\begin{pmatrix}1  -1  \\frac{1}{2}  0\\end{pmatrix} \\begin{pmatrix}2\\\\1\\\\1\\\\2\\end{pmatrix} = 1(2) - 1(1) + \\frac{1}{2}(1) + 0(2) = 2 - 1 + \\frac{1}{2} = \\frac{3}{2}.\n$$\n然后将这些分数通过负斜率为 $\\lambda=0.2$ 的 LeakyReLU 激活函数。令 $e_{ij} = \\operatorname{LReLU}_{\\lambda}(s_{ij}^{(\\mathrm{pre})})$。由于所有激活前分数（$2$、$\\frac{1}{2}$、$\\frac{3}{2}$）都是非负的，LeakyReLU 函数的作用等同于恒等函数。\n$$\ne_{11} = \\operatorname{LReLU}_{0.2}(2) = 2.\n$$\n$$\ne_{12} = \\operatorname{LReLU}_{0.2}\\left(\\frac{1}{2}\\right) = \\frac{1}{2}.\n$$\n$$\ne_{13} = \\operatorname{LReLU}_{0.2}\\left(\\frac{3}{2}\\right) = \\frac{3}{2}.\n$$\n现在我们使用 softmax 函数在邻域 $j \\in \\{1,2,3\\}$ 上计算归一化的注意力系数 $\\alpha_{1j}$。softmax 的分母是激活后分数的指数和：\n$$\nD = \\sum_{k=1}^{3} \\exp(e_{1k}) = \\exp(e_{11}) + \\exp(e_{12}) + \\exp(e_{13}) = \\exp(2) + \\exp\\left(\\frac{1}{2}\\right) + \\exp\\left(\\frac{3}{2}\\right).\n$$\n注意力系数为：\n$$\n\\alpha_{11} = \\frac{\\exp(e_{11})}{D} = \\frac{\\exp(2)}{\\exp(2) + \\exp\\left(\\frac{1}{2}\\right) + \\exp\\left(\\frac{3}{2}\\right)}.\n$$\n$$\n\\alpha_{12} = \\frac{\\exp(e_{12})}{D} = \\frac{\\exp\\left(\\frac{1}{2}\\right)}{\\exp(2) + \\exp\\left(\\frac{1}{2}\\right) + \\exp\\left(\\frac{3}{2}\\right)}.\n$$\n$$\n\\alpha_{13} = \\frac{\\exp(e_{13})}{D} = \\frac{\\exp\\left(\\frac{3}{2}\\right)}{\\exp(2) + \\exp\\left(\\frac{1}{2}\\right) + \\exp\\left(\\frac{3}{2}\\right)}.\n$$\n这就完成了第一个任务。为了验证归一化：\n$$\n\\sum_{j=1}^{3} \\alpha_{1j} = \\alpha_{11} + \\alpha_{12} + \\alpha_{13} = \\frac{\\exp(2) + \\exp\\left(\\frac{1}{2}\\right) + \\exp\\left(\\frac{3}{2}\\right)}{\\exp(2) + \\exp\\left(\\frac{1}{2}\\right) + \\exp\\left(\\frac{3}{2}\\right)} = 1.\n$$\n系数确实是归一化的。\n\n对于第二个任务，我们计算更新后的嵌入 $\\mathbf{h}_{1}'$，即变换后特征的加权和：\n$$\n\\mathbf{h}_{1}' = \\sum_{j=1}^{3} \\alpha_{1j}\\mathbf{h}_j = \\alpha_{11}\\mathbf{h}_1 + \\alpha_{12}\\mathbf{h}_2 + \\alpha_{13}\\mathbf{h}_3.\n$$\n代入 $\\alpha_{1j}$ 的表达式和向量 $\\mathbf{h}_j$：\n$$\n\\mathbf{h}_{1}' = \\frac{1}{D} \\left( \\exp(2)\\begin{pmatrix}2\\\\1\\end{pmatrix} + \\exp\\left(\\frac{1}{2}\\right)\\begin{pmatrix}-1\\\\1\\end{pmatrix} + \\exp\\left(\\frac{3}{2}\\right)\\begin{pmatrix}1\\\\2\\end{pmatrix} \\right).\n$$\n合并向量：\n$$\n\\mathbf{h}_{1}' = \\frac{1}{D} \\begin{pmatrix} 2\\exp(2) - \\exp\\left(\\frac{1}{2}\\right) + \\exp\\left(\\frac{3}{2}\\right) \\\\ 1\\exp(2) + 1\\exp\\left(\\frac{1}{2}\\right) + 2\\exp\\left(\\frac{3}{2}\\right) \\end{pmatrix}.\n$$\n代入 $D$ 的表达式：\n$$\n\\mathbf{h}_{1}' = \\frac{1}{\\exp(2) + \\exp\\left(\\frac{1}{2}\\right) + \\exp\\left(\\frac{3}{2}\\right)} \\begin{pmatrix} 2\\exp(2) - \\exp\\left(\\frac{1}{2}\\right) + \\exp\\left(\\frac{3}{2}\\right) \\\\ \\exp(2) + \\exp\\left(\\frac{1}{2}\\right) + 2\\exp\\left(\\frac{3}{2}\\right) \\end{pmatrix}.\n$$\n这就完成了第二个任务。\n\n对于第三个任务和最终答案，我们提取 $\\mathbf{h}_{1}'$ 的第一个坐标。记作 $(\\mathbf{h}_{1}')_1$。\n$$\n(\\mathbf{h}_{1}')_1 = \\frac{2\\exp(2) - \\exp\\left(\\frac{1}{2}\\right) + \\exp\\left(\\frac{3}{2}\\right)}{\\exp(2) + \\exp\\left(\\frac{1}{2}\\right) + \\exp\\left(\\frac{3}{2}\\right)}.\n$$\n这就是所要求的最终封闭形式解析表达式。",
            "answer": "$$\n\\boxed{\\frac{2\\exp(2) - \\exp\\left(\\frac{1}{2}\\right) + \\exp\\left(\\frac{3}{2}\\right)}{\\exp(2) + \\exp\\left(\\frac{1}{2}\\right) + \\exp\\left(\\frac{3}{2}\\right)}}\n$$"
        }
    ]
}