{
    "hands_on_practices": [
        {
            "introduction": "在估计溢出效应之前，我们首先需要一个精确的方式来定义和衡量一个个体是如何“暴露”于其同伴的处理之下的。这个练习将介绍一种常见的曝光定义——受处理邻居的比例——并要求你推导其基本的统计特性。理解这些特性是构建更复杂模型的第一步 ()。",
            "id": "4266879",
            "problem": "考虑一个由$n$个单元组成的简单无向网络，该网络由一个二元邻接矩阵 $A \\in \\{0,1\\}^{n \\times n}$ 表示。该矩阵是对称的且无自环，因此对所有$i$，$A_{ij} = A_{ji}$ 且 $A_{ii} = 0$。令单元$i$的度为 $d_i = \\sum_{j=1}^{n} A_{ij}$，并且只关注度 $d_i \\geq 1$ 的单元。假设处理根据伯努利-$p$方案独立地分配给每个单元：对于每个单元$j$，处理指示变量 $Z_j \\in \\{0,1\\}$ 是独立同分布（IID）抽取的，其中 $\\mathbb{P}(Z_j = 1) = p$ 和 $\\mathbb{P}(Z_j = 0) = 1 - p$，且对所有$j$独立。\n\n在存在网络干预的情况下，考虑为单元$i$定义的邻域暴露映射\n$$\nE_i \\equiv \\frac{1}{d_i} \\sum_{j=1}^{n} A_{ij} Z_j,\n$$\n这是单元$i$的已处理邻居的比例。从独立随机变量的期望和方差的基本性质以及伯努利分布的已知事实出发，推导期望暴露 $\\mathbb{E}[E_i]$ 及其方差 $\\operatorname{Var}(E_i)$ 作为$p$和$d_i$的函数的闭式表达式。\n\n将你的最终答案表示为一个包含这两个量的单行矩阵的闭式解析表达式。无需四舍五入。",
            "solution": "本题要求推导网络中单元$i$的邻域暴露的期望 $\\mathbb{E}[E_i]$ 和方差 $\\operatorname{Var}(E_i)$。邻域暴露 $E_i$ 定义为：\n$$\nE_i \\equiv \\frac{1}{d_i} \\sum_{j=1}^{n} A_{ij} Z_j\n$$\n其中 $A$ 是邻接矩阵，$d_i = \\sum_{j=1}^{n} A_{ij}$ 是单元$i$的度（约束条件为 $d_i \\geq 1$），而 $Z_j$ 是参数为$p$的独立同分布（IID）伯努利随机变量。具体来说，$\\mathbb{P}(Z_j = 1) = p$ 且 $\\mathbb{P}(Z_j = 0) = 1 - p$。伯努利($p$)随机变量 $Z$ 的已知性质是其期望 $\\mathbb{E}[Z] = p$ 和方差 $\\operatorname{Var}(Z) = p(1-p)$。\n\n首先，我们推导 $E_i$ 的期望值。我们应用期望的线性性质，该性质指出对于随机变量 $X_k$ 和常数 $c_k$，有 $\\mathbb{E}\\left[\\sum_k c_k X_k\\right] = \\sum_k c_k \\mathbb{E}[X_k]$。在我们的例子中，随机变量是 $Z_j$，常数是系数 $\\frac{A_{ij}}{d_i}$。邻接矩阵 $A$ 和度 $d_i$ 被视为固定的、非随机的量。\n\n$$\n\\mathbb{E}[E_i] = \\mathbb{E}\\left[\\frac{1}{d_i} \\sum_{j=1}^{n} A_{ij} Z_j\\right]\n$$\n\n由于 $d_i$ 和 $A_{ij}$ 是常数，我们可以写出：\n$$\n\\mathbb{E}[E_i] = \\frac{1}{d_i} \\sum_{j=1}^{n} \\mathbb{E}[A_{ij} Z_j] = \\frac{1}{d_i} \\sum_{j=1}^{n} A_{ij} \\mathbb{E}[Z_j]\n$$\n\n每个 $Z_j$ 都是参数为 $p$ 的伯努利随机变量，因此其期望为 $\\mathbb{E}[Z_j] = p$。将此代入方程中得到：\n$$\n\\mathbb{E}[E_i] = \\frac{1}{d_i} \\sum_{j=1}^{n} A_{ij} p\n$$\n\n参数 $p$ 是一个相对于求和索引 $j$ 的常数，因此可以提取出来：\n$$\n\\mathbb{E}[E_i] = \\frac{p}{d_i} \\sum_{j=1}^{n} A_{ij}\n$$\n\n根据单元$i$的度的定义，我们有 $d_i = \\sum_{j=1}^{n} A_{ij}$。将此定义代入期望的表达式中得到：\n$$\n\\mathbb{E}[E_i] = \\frac{p}{d_i} (d_i) = p\n$$\n约束条件 $d_i \\ge 1$ 确保此表达式是良定的。因此，任何单元$i$的期望邻域暴露就是处理概率$p$。\n\n接下来，我们推导 $E_i$ 的方差。我们从方差的定义开始，并使用性质：对于常数 $c$ 和随机变量 $X$，有 $\\operatorname{Var}(cX) = c^2 \\operatorname{Var}(X)$。\n$$\n\\operatorname{Var}(E_i) = \\operatorname{Var}\\left(\\frac{1}{d_i} \\sum_{j=1}^{n} A_{ij} Z_j\\right) = \\left(\\frac{1}{d_i}\\right)^2 \\operatorname{Var}\\left(\\sum_{j=1}^{n} A_{ij} Z_j\\right) = \\frac{1}{d_i^2} \\operatorname{Var}\\left(\\sum_{j=1}^{n} A_{ij} Z_j\\right)\n$$\n\n和 $\\sum_{j=1}^{n} A_{ij} Z_j$ 可以被简化。由于如果 $j$ 是 $i$ 的邻居，则 $A_{ij}$ 为 $1$，否则为 $0$，所以这个和等价于对 $i$ 的邻居集合（我们可以表示为 $\\mathcal{N}_i = \\{j \\mid A_{ij}=1\\}$）中的 $Z_j$ 进行求和。邻居的数量为 $|\\mathcal{N}_i| = d_i$。\n$$\n\\sum_{j=1}^{n} A_{ij} Z_j = \\sum_{j \\in \\mathcal{N}_i} Z_j\n$$\n\n题目指出，对于所有 $j \\in \\{1, \\dots, n\\}$，变量 $Z_j$ 是独立同分布抽取的。因此，变量集合 $\\{Z_j \\mid j \\in \\mathcal{N}_i\\}$ 是一组相互独立的随机变量。方差的一个基本性质是，对于独立随机变量 $X_k$ 的和，和的方差等于方差的和：$\\operatorname{Var}\\left(\\sum_k X_k\\right) = \\sum_k \\operatorname{Var}(X_k)$。\n\n应用此性质，我们得到：\n$$\n\\operatorname{Var}\\left(\\sum_{j \\in \\mathcal{N}_i} Z_j\\right) = \\sum_{j \\in \\mathcal{N}_i} \\operatorname{Var}(Z_j)\n$$\n\n由于每个 $Z_j$ 都是独立同分布的伯努利($p$)变量，其方差为 $\\operatorname{Var}(Z_j) = p(1-p)$。这个方差对所有 $j$ 都是相同的。求和是在单元$i$的 $d_i$ 个邻居上进行的。\n$$\n\\sum_{j \\in \\mathcal{N}_i} \\operatorname{Var}(Z_j) = \\sum_{j \\in \\mathcal{N}_i} p(1-p) = d_i \\cdot p(1-p)\n$$\n\n最后，我们将此结果代回 $\\operatorname{Var}(E_i)$ 的表达式中：\n$$\n\\operatorname{Var}(E_i) = \\frac{1}{d_i^2} \\left[d_i \\cdot p(1-p)\\right]\n$$\n\n鉴于题目的约束条件 $d_i \\ge 1$，我们可以通过消去一个因子 $d_i$ 来简化表达式：\n$$\n\\operatorname{Var}(E_i) = \\frac{p(1-p)}{d_i}\n$$\n这个结果表明，邻域暴露的方差与单元的度成反比。\n\n推导出的量是期望 $\\mathbb{E}[E_i] = p$ 和方差 $\\operatorname{Var}(E_i) = \\frac{p(1-p)}{d_i}$。我们被要求将它们呈现在一个单行矩阵中。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\np & \\frac{p(1-p)}{d_i}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "在定义了曝光之后，下一步便是估计其效应。本练习将指导你使用逆概率加权（Inverse Probability Weighting, IPW）来推导一个识别公式，这是处理非实验数据中混杂因素的一项基石技术。这项实践展示了如何将我们关心的因果量与我们实际能观测到的数据正式地联系起来，这对于从观测研究中得出有效的因果结论至关重要 ()。",
            "id": "4266843",
            "problem": "考虑一个由 $n$ 个单元组成的总体，其中单元由 $i \\in \\{1,\\dots,n\\}$ 索引，并嵌入在一个网络中。每个单元有一个二元处理 $Z_i \\in \\{0,1\\}$、一个网络暴露 $E_i$（通过已知的暴露映射总结其邻居的处理）、基线协变量 $W_i$ 和一个观测结果 $Y_i$。假设暴露 $E_i$ 在一个离散的支持集 $\\mathcal{E}$ 中取值，并且在干涉下的潜在结果被良好地定义为 $Y_i(z,e)$，其中 $z \\in \\{0,1\\}$ 且 $e \\in \\mathcal{E}$。令观测数据服从一致性关系 $Y_i = Y_i(Z_i,E_i)$。\n\n将在暴露水平 $E \\in \\mathcal{E}$ 的因果直接效应定义为\n$$\n\\tau^{\\text{dir}}(E) = \\mathbb{E}\\big[ Y_i(1,E) - Y_i(0,E) \\big],\n$$\n其中期望是关于单元的总体分布。\n\n给定联合倾向得分\n$$\ne_i(z,E) = \\Pr\\big( Z_i = z,\\; E_i = E \\mid W_i \\big),\n$$\n只要 $\\Pr(W_i \\in \\mathcal{W}) > 0$ 且 $(z,E)$ 在给定 $W_i$ 的 $(Z_i,E_i)$ 的支持集内，该得分就严格介于 $0$ 和 $1$ 之间。假设满足以下识别条件：\n- 一致性：对于所有 $i$，有 $Y_i = Y_i(Z_i,E_i)$。\n- 条件可忽略性：对于所有 $E \\in \\mathcal{E}$，有 $\\{Y_i(1,E), Y_i(0,E)\\} \\perp (Z_i, E_i) \\mid W_i$。\n- 正值性：对于支持集中的所有 $z \\in \\{0,1\\}$ 和 $E \\in \\mathcal{E}$，几乎必然有 $0 < e_i(z,E) < 1$。\n\n从这些假设以及条件期望和重期望定律的定义出发，使用基于联合倾向得分 $e_i(z,E)$ 的逆概率加权法，推导 $\\tau^{\\text{dir}}(E)$ 的识别公式。你的最终答案必须是单一的闭式解析表达式。不需要四舍五入，最终表达式没有单位。",
            "solution": "我们的目标是使用逆概率加权法，在给定的识别假设下，为网络干预下的因果直接效应 $\\tau^{\\text{dir}}(E)$ 推导一个识别公式。\n\n直接效应定义为：\n$$\n\\tau^{\\text{dir}}(E) = \\mathbb{E}\\big[ Y_i(1,E) - Y_i(0,E) \\big]\n$$\n利用期望的线性性质，我们可以分别处理这两项：\n$$\n\\tau^{\\text{dir}}(E) = \\mathbb{E}\\big[ Y_i(1,E) \\big] - \\mathbb{E}\\big[ Y_i(0,E) \\big]\n$$\n我们致力于为一般的平均潜在结果 $\\mu(z,E) = \\mathbb{E}\\big[ Y_i(z,E) \\big]$ 推导一个识别公式，其中处理水平为 $z \\in \\{0,1\\}$，暴露水平为 $E \\in \\mathcal{E}$。\n\n我们从考虑以下加权平均值开始，并证明它等于我们所求的量。令 $\\mathbb{I}(\\cdot)$ 为指示函数。\n$$\n\\mathbb{E}\\left[ \\frac{\\mathbb{I}(Z_i=z, E_i=E)}{e_i(z,E)} Y_i \\right]\n$$\n首先，我们应用重期望定律，以协变量 $W_i$ 为条件：\n$$\n\\mathbb{E}\\left[ \\frac{\\mathbb{I}(Z_i=z, E_i=E)}{e_i(z,E)} Y_i \\right] = \\mathbb{E}_{W_i}\\left[ \\mathbb{E}\\left[ \\frac{\\mathbb{I}(Z_i=z, E_i=E)}{e_i(z,E)} Y_i \\;\\middle|\\; W_i \\right] \\right]\n$$\n在以 $W_i$ 为条件的内层期望中，倾向得分 $e_i(z,E) = \\Pr(Z_i=z, E_i=E \\mid W_i)$ 是一个常数，可以被提出来：\n$$\n= \\mathbb{E}_{W_i}\\left[ \\frac{1}{e_i(z,E)} \\mathbb{E}\\left[ \\mathbb{I}(Z_i=z, E_i=E) Y_i \\;\\middle|\\; W_i \\right] \\right]\n$$\n接下来，我们应用**一致性**假设：$Y_i = Y_i(Z_i, E_i)$。当 $Z_i=z$ 且 $E_i=E$ 时，我们有 $Y_i = Y_i(z,E)$。因此，我们可以写出 $\\mathbb{I}(Z_i=z, E_i=E) Y_i = \\mathbb{I}(Z_i=z, E_i=E) Y_i(z,E)$。代入得到：\n$$\n= \\mathbb{E}_{W_i}\\left[ \\frac{1}{e_i(z,E)} \\mathbb{E}\\left[ \\mathbb{I}(Z_i=z, E_i=E) Y_i(z,E) \\;\\middle|\\; W_i \\right] \\right]\n$$\n现在，我们应用**条件可忽略性**假设：$\\{Y_i(1,E), Y_i(0,E)\\} \\perp (Z_i, E_i) \\mid W_i$。这意味着在给定 $W_i$ 的条件下，潜在结果 $Y_i(z,E)$ 独立于处理-暴露对 $(Z_i, E_i)$。因此，我们可以分解内层期望：\n$$\n\\mathbb{E}\\left[ \\mathbb{I}(Z_i=z, E_i=E) Y_i(z,E) \\;\\middle|\\; W_i \\right] = \\mathbb{E}\\left[ Y_i(z,E) \\mid W_i \\right] \\mathbb{E}\\left[ \\mathbb{I}(Z_i=z, E_i=E) \\mid W_i \\right]\n$$\n第二项是给定 $W_i$ 时指示函数的期望，根据定义，它正是联合倾向得分 $e_i(z,E)$：\n$$\n\\mathbb{E}\\left[ \\mathbb{I}(Z_i=z, E_i=E) \\mid W_i \\right] = \\Pr(Z_i=z, E_i=E \\mid W_i) = e_i(z,E)\n$$\n代入回主表达式：\n$$\n= \\mathbb{E}_{W_i}\\left[ \\frac{1}{e_i(z,E)} \\mathbb{E}\\left[ Y_i(z,E) \\mid W_i \\right] e_i(z,E) \\right]\n$$\n**正值性**假设（$0  e_i(z,E)  1$）确保了分母不为零，允许我们消去 $e_i(z,E)$：\n$$\n= \\mathbb{E}_{W_i}\\left[ \\mathbb{E}\\left[ Y_i(z,E) \\mid W_i \\right] \\right]\n$$\n最后，再次应用重期望定律，我们得到我们所求的平均潜在结果：\n$$\n= \\mathbb{E}\\big[ Y_i(z,E) \\big] = \\mu(z,E)\n$$\n这样，我们就为平均潜在结果建立了识别公式：\n$$\n\\mathbb{E}\\big[ Y_i(z,E) \\big] = \\mathbb{E}\\left[ \\frac{\\mathbb{I}(Z_i=z, E_i=E)}{e_i(z,E)} Y_i \\right]\n$$\n现在，我们可以将此公式应用于 $\\tau^{\\text{dir}}(E)$ 的两个组成部分：\n$$\n\\mathbb{E}\\big[ Y_i(1,E) \\big] = \\mathbb{E}\\left[ \\frac{\\mathbb{I}(Z_i=1, E_i=E)}{e_i(1,E)} Y_i \\right]\n$$\n$$\n\\mathbb{E}\\big[ Y_i(0,E) \\big] = \\mathbb{E}\\left[ \\frac{\\mathbb{I}(Z_i=0, E_i=E)}{e_i(0,E)} Y_i \\right]\n$$\n将这两项代入 $\\tau^{\\text{dir}}(E)$ 的定义中，并利用期望的线性性质，我们将它们合并成一个单一的期望，得到最终的识别公式：\n$$\n\\tau^{\\text{dir}}(E) = \\mathbb{E}\\left[ \\frac{\\mathbb{I}(Z_i=1, E_i=E)}{e_i(1,E)} Y_i \\right] - \\mathbb{E}\\left[ \\frac{\\mathbb{I}(Z_i=0, E_i=E)}{e_i(0,E)} Y_i \\right] = \\mathbb{E}\\left[ \\left( \\frac{\\mathbb{I}(Z_i=1, E_i=E)}{e_i(1,E)} - \\frac{\\mathbb{I}(Z_i=0, E_i=E)}{e_i(0,E)} \\right) Y_i \\right]\n$$\n该表达式仅是可观测数据（$Z_i, E_i, Y_i$）和可从数据估计的倾向得分 $e_i(z,E)$ 的函数，因此可以识别 $\\tau^{\\text{dir}}(E)$。",
            "answer": "$$\n\\boxed{\\mathbb{E}\\left[ \\left( \\frac{\\mathbb{I}(Z_i=1, E_i=E)}{e_i(1,E)} - \\frac{\\mathbb{I}(Z_i=0, E_i=E)}{e_i(0,E)} \\right) Y_i \\right]}\n$$"
        },
        {
            "introduction": "我们将焦点从分析现有数据转向设计新的实验。集群随机试验是管理网络干扰的一项关键策略。这个计算练习将挑战你通过对网络进行分区以最小化集群间的溢出效应，来设计一个最优的实验，从而将抽象的因果原则转化为具体的实验设计方案 ()。",
            "id": "4266836",
            "problem": "给定一个无向简单图 $G=(V,E)$ 和一组簇大小约束。任务是设计一种基于簇的实验分配，该分配能够最小化跨簇连接性，从而在网络干预下的因果推断设置中减少簇之间的干预。您必须构建一个程序，对于每个提供的测试用例，找到一个满足指定大小的顶点簇划分，该划分能最小化跨簇边密度，然后计算在该簇随机化分配下的泄漏指标。\n\n定义和目标如下。\n\n- **图与划分**: 令 $V=\\{0,1,\\dots,n-1\\}$ 表示 $n$ 个顶点的集合，令 $E\\subseteq\\{\\{i,j\\}\\mid i,j\\in V, i \\neq j\\}$ 表示 $m = |E|$ 条无向边的集合。一个划分 $\\Pi=\\{C_1, \\dots, C_K\\}$ 是将 $V$ 分割成满足给定大小约束的不相交簇。\n- **跨簇密度**: 跨簇边集为 $E_{\\times}(\\Pi) = \\{\\{i,j\\} \\in E \\mid \\exists k \\neq l \\text{ s.t. } i \\in C_k, j \\in C_l\\}$。密度为 $d_{\\times}(\\Pi) = |E_{\\times}(\\Pi)| / m$。\n- **泄漏指标**: $L(\\Pi) = p \\cdot d_{\\times}(\\Pi)$，其中 $p$ 是将任何一个簇分配到处理组的概率。\n\n对于每个测试用例，找到最优划分 $\\Pi^\\star$ 并计算 $[d_{\\times}(\\Pi^\\star), L(\\Pi^\\star)]$。",
            "solution": "该问题是在给定簇大小的约束下，找到一个给定图的顶点划分，以最小化跨簇边密度，然后计算一个泄漏指标。此任务是在存在网络干预的情况下为因果推断设计实验的框架内进行的。\n\n### 科学论证\n\n网络上的因果推断需要处理稳定单元处理值假设 (SUTVA) 的失效问题，该假设假定一个单位的结果仅受其自身处理状态的影响。\n\n在网络环境下，当一个单位的处理影响其邻居的结果时，就会产生干预，这是一种由图的边所介导的现象。\n\n令 $V$ 为单位集合，令 $Z_i \\in \\{0, 1\\}$ 为分配给单位 $i \\in V$ 的处理。所有处理分配的向量为 $\\mathbf{Z} \\in \\{0,1\\}^{|V|}$。在干预下，单位 $i$ 的潜在结果表示为 $Y_i(\\mathbf{Z})$，明确地依赖于整个处理向量。这与无干预情况形成对比，在无干预情况下 $Y_i(\\mathbf{Z}) = Y_i(Z_i)$。\n\n一种缓解（尽管不能消除）干预并实现因果效应估计的常用策略是簇随机化分配。顶点集 $V$ 被划分为 $K$ 个不相交的簇，$\\Pi = \\{C_1, \\dots, C_K\\}$。然后在簇级别上分配处理，这意味着一个簇 $C_k$ 内的所有单位都接受相同的处理状态 $Z_k$。这种设计确保了簇*内部*没有干预（因为所有单位要么共同接受处理，要么共同作为对照），从而将干预局部化为簇间现象。\n\n目标是创建一个能最小化这种簇间干预的划分 $\\Pi$。两个簇 $C_a$ 和 $C_b$ 之间的干预是通过连接它们的边集来传递的，即边 $\\{i,j\\}$ 其中 $i \\in C_a$ 且 $j \\in C_b$。图中所有这类边的集合就是跨簇边集，$E_{\\times}(\\Pi) = \\big\\{\\{i,j\\}\\in E \\mid i\\in C_a, j\\in C_b, a\\neq b\\big\\}$。最小化该集合的基数 $|E_{\\times}(\\Pi)|$，可以直接减少处理在簇之间溢出的路径数量。由于对于给定的图，总边数 $|E|$ 是一个常数，因此最小化 $|E_{\\times}(\\Pi)|$ 等同于最小化跨簇边密度 $d_{\\times}(\\Pi) = |E_{\\times}(\\Pi)| / |E|$。因此，找到最小化 $d_{\\times}(\\Pi)$ 的划分 $\\Pi^\\star$ 是一种有原则的方法，可用于设计能最大程度遏制网络干预的实验。\n\n泄漏指标 $L(\\Pi) = p \\cdot d_{\\times}(\\Pi)$ 量化了来自外部簇的处理暴露的期望值。给定一条随机边 $\\{i,j\\} \\in E$，它跨越簇的概率是 $d_{\\times}(\\Pi)$。问题将 $L(\\Pi)$ 定义为“既跨越簇又连接到外部已处理簇中顶点的边的期望比例”。一个更精确的解释是，属于跨簇边且位于已处理簇中的*边端点*的期望比例。边端点的总数是 $2|E|$。跨簇边端点的数量是 $2|E_{\\times}(\\Pi)|$。由于每个簇以概率 $p$ 接受处理，属于已处理簇的跨簇端点的期望数量是 $p \\cdot 2|E_{\\times}(\\Pi)|$。因此，期望比例为 $\\frac{p \\cdot 2|E_{\\times}(\\Pi)|}{2|E|} = p \\cdot \\frac{|E_{\\times}(\\Pi)|}{|E|} = p \\cdot d_{\\times}(\\Pi)$。该指标可作为一个简单的综合度量，衡量处理“泄漏”到簇边界之外的潜力。\n\n### 算法方法\n\n将图划分以最小化切割边数的问题是一个著名的 NP 难问题。然而，所提供测试用例中的顶点集很小 ($n \\le 10$)，这使得暴力搜索在计算上是可行的。算法过程如下：\n1. 对于每个测试用例，表示图 $G=(V,E)$ 和簇大小约束 $\\{s_k\\}$。\n2. 系统地生成所有满足特定大小约束的顶点集 $V$ 的划分。这通过一个递归算法实现，该算法对于一个大小列表，迭代地为第一个簇大小选择可用节点的组合，然后对其余节点和大小进行递归。为了处理对称性（即，如果 $|C_1|=|C_2|$，则划分 $\\{C_1, C_2\\}$ 和 $\\{C_2, C_1\\}$ 是相同的），我们对每个生成的划分进行规范化（例如，通过对簇进行排序），并将其存储在一个集合中以避免重复计算。\n3. 对于每个唯一的划分 $\\Pi = \\{C_1, \\dots, C_K\\}$，计算跨簇边的数量 $|E_{\\times}(\\Pi)|$。这可以通过首先计算内部边的数量 $\\sum_k |E(C_k)|$，然后从总边数 $|E|$ 中减去此值来完成。\n4. 跟踪在所有划分中找到的最小跨簇边数 $|E_{\\times}(\\Pi^\\star)|$。\n5. 在检查完所有唯一划分后，计算最小化的跨簇密度：$d_{\\times}(\\Pi^\\star) = |E_{\\times}(\\Pi^\\star)| / |E|$。\n6. 使用给定的概率 $p=0.5$，计算相应的泄漏指标：$L(\\Pi^\\star) = p \\cdot d_{\\times}(\\Pi^\\star)$。\n\n### 分步推导\n\n**测试用例 1**\n- 图：$n=10$ 个顶点。边形成了两个密集的子图，$\\{0,1,2,3,4\\}$ 和 $\\{5,6,7,8,9\\}$，它们之间有几条边。总边数 $|E| = 8+8+4=20$。\n- 约束：$K=2$ 个簇，大小为 $s_1=5$, $s_2=5$。\n- 从 10 个顶点中为第一个簇选择 5 个顶点的方法有 $\\binom{10}{5} = 252$ 种。由于簇的大小相等，交换它们会得到相同的划分，所以有 $\\frac{1}{2}\\binom{10}{5} = 126$ 个唯一的划分。\n- 通过观察，最优划分 $\\Pi^\\star$ 与图的社群结构一致：$\\Pi^\\star = \\big\\{\\{0,1,2,3,4\\}, \\{5,6,7,8,9\\}\\big\\}$。\n- 对于这个划分，跨簇边恰好是连接两个社群的 4 条边：$\\{(4,5),(3,6),(1,7),(0,8)\\}$。因此，$|E_{\\times}(\\Pi^\\star)| = 4$。任何其他划分都会切断其中一个密集的内部结构，从而增加跨簇边的数量。\n- 跨簇密度：$d_{\\times}(\\Pi^\\star) = |E_{\\times}(\\Pi^\\star)| / |E| = 4 / 20 = 0.2$。\n- 泄漏指标：$L(\\Pi^\\star) = p \\cdot d_{\\times}(\\Pi^\\star) = 0.5 \\cdot 0.2 = 0.1$。\n- 结果：$[0.2, 0.1]$。\n\n**测试用例 2**\n- 图：一个在 $n=6$ 个顶点上的完全图 $K_6$。边数为 $|E| = \\binom{6}{2} = 15$。\n- 约束：$K=2$ 个簇，大小为 $s_1=3$, $s_2=3$。\n- 设一个划分为 $\\Pi = \\{C_1, C_2\\}$，其中 $|C_1| = 3$ 且 $|C_2| = 3$。\n- 在完全图中，一个大小为 $s$ 的簇内部的边数为 $\\binom{s}{2}$。\n- $C_1$ 内部的边数：$\\binom{3}{2} = 3$。\n- $C_2$ 内部的边数：$\\binom{3}{2} = 3$。\n- 总内部边数：$3+3=6$。对于任何这种大小结构的划分，这个值都是恒定的。\n- 跨簇边数：$|E_{\\times}(\\Pi)| = |E| - (\\text{内部边数}) = 15 - 6 = 9$。\n- 由于对于所有有效的划分，该值都相同，因此 $|E_{\\times}(\\Pi^\\star)| = 9$。\n- 跨簇密度：$d_{\\times}(\\Pi^\\star) = 9 / 15 = 0.6$。\n- 泄漏指标：$L(\\Pi^\\star) = p \\cdot d_{\\times}(\\Pi^\\star) = 0.5 \\cdot 0.6 = 0.3$。\n- 结果：$[0.6, 0.3]$。\n\n**测试用例 3**\n- 图：一个在 $n=8$ 个顶点上的路径图，$0-1-2-3-4-5-6-7$。边数为 $|E| = 7$。\n- 约束：$K=3$ 个簇，大小为 $s_1=3$, $s_2=3$, $s_3=2$。\n- 为了在路径图上最小化跨簇边，我们应该进行切割以断开路径。划分为 $K=3$ 个簇至少需要 $K-1=2$ 次切割（移除边）。\n- 考虑由连续块构成的划分：$\\Pi^\\star = \\big\\{\\{0,1,2\\}, \\{3,4,5\\}, \\{6,7\\}\\big\\}$。\n- 这个划分是有效的，因为大小分别为 3、3 和 2。\n- 被这个划分切割的边是 $\\{2,3\\}$ 和 $\\{5,6\\}$。恰好有 2 条这样的边。\n- 因此，$|E_{\\times}(\\Pi^\\star)| = 2$。这是可能的最小值。\n- 跨簇密度：$d_{\\times}(\\Pi^\\star) = 2 / 7$。\n- 泄漏指标：$L(\\Pi^\\star) = p \\cdot d_{\\times}(\\Pi^\\star) = 0.5 \\cdot (2 / 7) = 1 / 7$。\n- 数值上，$2/7 \\approx 0.2857142857$，$1/7 \\approx 0.1428571428$。\n- 结果：$[0.2857142857142857, 0.14285714285714285]$。\n\n以下程序实现了所述的暴力搜索算法来验证这些结果。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom itertools import combinations\n\ndef solve():\n    \"\"\"\n    Main function to orchestrate the solving of all test cases.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"nodes\": list(range(10)),\n            \"edges\": [\n                (0, 1), (1, 2), (2, 3), (3, 4), (4, 0), (0, 2), (1, 3), (2, 4),\n                (5, 6), (6, 7), (7, 8), (8, 9), (9, 5), (5, 7), (6, 8), (7, 9),\n                (4, 5), (3, 6), (1, 7), (0, 8)\n            ],\n            \"cluster_sizes\": [5, 5],\n            \"p\": 0.5\n        },\n        {\n            \"nodes\": list(range(6)),\n            \"edges\": list(combinations(range(6), 2)),\n            \"cluster_sizes\": [3, 3],\n            \"p\": 0.5\n        },\n        {\n            \"nodes\": list(range(8)),\n            \"edges\": [(i, i + 1) for i in range(7)],\n            \"cluster_sizes\": [3, 3, 2],\n            \"p\": 0.5\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result_pair = solve_case(\n            case[\"nodes\"],\n            case[\"edges\"],\n            case[\"cluster_sizes\"],\n            case[\"p\"]\n        )\n        results.append(result_pair)\n\n    # Format the final output string to match the problem specification.\n    # The default str() for a list adds spaces, e.g., '[0.1, 0.2]'.\n    # The required format is '[0.1,0.2]'.\n    formatted_results = []\n    for res_pair in results:\n        formatted_results.append(f\"[{res_pair[0]},{res_pair[1]}]\")\n    \n    print(f\"[{','.join(formatted_results)}]\")\n\ndef generate_partitions(nodes, sizes):\n    \"\"\"\n    Recursively generates all ordered partitions of a set of nodes\n    into clusters of specified sizes.\n    \"\"\"\n    if not sizes:\n        yield []\n        return\n\n    current_size = sizes[0]\n    remaining_sizes = sizes[1:]\n\n    # To avoid generating equivalent partitions, we only need to iterate over\n    # combinations for the first cluster.\n    for combo in combinations(nodes, current_size):\n        remaining_nodes = [n for n in nodes if n not in combo]\n        for rest_of_partition in generate_partitions(tuple(remaining_nodes), remaining_sizes):\n            yield [combo] + rest_of_partition\n\ndef solve_case(nodes, edges, cluster_sizes, p):\n    \"\"\"\n    Solves a single test case by finding the optimal partition.\n    This function implements a brute-force search over all valid partitions.\n    \"\"\"\n    num_edges = len(edges)\n    # Use a set of frozensets for efficient edge lookups\n    edge_set = {frozenset(e) for e in edges}\n    \n    min_cross_edges = float('inf')\n    visited_partitions = set()\n\n    for partition_list in generate_partitions(tuple(nodes), cluster_sizes):\n        # A partition is a list of tuples, e.g., [(0, 1), (2, 3)]\n        # To handle symmetries (partitions with identical-sized clusters),\n        # we canonicalize the partition by sorting each cluster and then\n        # sorting the list of clusters. A tuple of tuples is hashable.\n        canonical_partition = tuple(sorted(tuple(sorted(cluster)) for cluster in partition_list))\n        \n        if canonical_partition in visited_partitions:\n            continue\n        visited_partitions.add(canonical_partition)\n        \n        num_internal_edges = 0\n        for cluster in partition_list:\n            # Count edges fully contained within this cluster\n            for i in range(len(cluster)):\n                for j in range(i + 1, len(cluster)):\n                    u, v = cluster[i], cluster[j]\n                    if frozenset({u, v}) in edge_set:\n                        num_internal_edges += 1\n        \n        num_cross_edges = num_edges - num_internal_edges\n        if num_cross_edges  min_cross_edges:\n            min_cross_edges = num_cross_edges\n\n    if num_edges > 0:\n        d_cross_star = min_cross_edges / num_edges\n    else:\n        d_cross_star = 0.0\n\n    l_star = p * d_cross_star\n    \n    return [d_cross_star, l_star]\n\nsolve()\n```"
        }
    ]
}