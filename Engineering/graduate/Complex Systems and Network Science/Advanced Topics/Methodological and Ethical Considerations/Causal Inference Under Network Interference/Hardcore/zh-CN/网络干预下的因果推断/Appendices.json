{
    "hands_on_practices": [
        {
            "introduction": "为了开始我们的实践探索，我们首先必须掌握网络暴露 (network exposure) 这一基本概念，它是处理效应在个体之间溢出的机制。这个练习  要求你推导一个常用暴露度量的基本统计特性——均值和方差。通过这个过程，你将为理解处理分配如何在网络中传播以及如何对这一过程进行统计建模打下坚实的基础。",
            "id": "4266879",
            "problem": "考虑一个包含 $n$ 个单元的简单无向网络，该网络由一个二元邻接矩阵 $A \\in \\{0,1\\}^{n \\times n}$ 表示，该矩阵是对称的且无自环，因此对所有 $i$ 都有 $A_{ij} = A_{ji}$ 和 $A_{ii} = 0$。设单元 $i$ 的度为 $d_i = \\sum_{j=1}^{n} A_{ij}$，并只考虑度 $d_i \\geq 1$ 的单元。假设根据伯努利-$p$方案独立地为每个单元分配处理：对于每个单元 $j$，处理指示变量 $Z_j \\in \\{0,1\\}$ 是独立同分布 (IID) 抽取的，其中 $\\mathbb{P}(Z_j = 1) = p$ 和 $\\mathbb{P}(Z_j = 0) = 1 - p$，且对所有 $j$ 均独立。\n\n在存在网络干预的情况下，考虑为单元 $i$ 定义的邻域暴露映射\n$$\nE_i \\equiv \\frac{1}{d_i} \\sum_{j=1}^{n} A_{ij} Z_j,\n$$\n这是单元 $i$ 的邻居中被处理的邻居所占的比例。从独立随机变量的期望和方差的基本性质以及伯努利分布的已知事实出发，推导期望暴露 $\\mathbb{E}[E_i]$ 及其方差 $\\operatorname{Var}(E_i)$ 作为 $p$ 和 $d_i$ 的函数的闭式表达式。\n\n将您的最终答案表示为一个包含这两个量的单行矩阵形式的闭式解析表达式。无需进行四舍五入。",
            "solution": "问题要求推导网络中单元 $i$ 的邻域暴露的期望 $\\mathbb{E}[E_i]$ 和方差 $\\operatorname{Var}(E_i)$。邻域暴露 $E_i$ 定义为：\n$$\nE_i \\equiv \\frac{1}{d_i} \\sum_{j=1}^{n} A_{ij} Z_j\n$$\n其中 $A$ 是邻接矩阵，$d_i = \\sum_{j=1}^{n} A_{ij}$ 是单元 $i$ 的度（约束条件为 $d_i \\geq 1$），而 $Z_j$ 是参数为 $p$ 的独立同分布 (IID) 伯努利随机变量。具体来说，$\\mathbb{P}(Z_j = 1) = p$ 且 $\\mathbb{P}(Z_j = 0) = 1 - p$。伯努利($p$)随机变量 $Z$ 的已知性质是其期望 $\\mathbb{E}[Z] = p$ 和方差 $\\operatorname{Var}(Z) = p(1-p)$。\n\n首先，我们推导 $E_i$ 的期望值。我们应用期望的线性性质，该性质指出对于随机变量 $X_k$ 和常数 $c_k$，有 $\\mathbb{E}\\left[\\sum_k c_k X_k\\right] = \\sum_k c_k \\mathbb{E}[X_k]$。在本例中，随机变量是 $Z_j$，常数是系数 $\\frac{A_{ij}}{d_i}$。邻接矩阵 $A$ 和度 $d_i$ 被视为固定的、非随机的量。\n\n$$\n\\mathbb{E}[E_i] = \\mathbb{E}\\left[\\frac{1}{d_i} \\sum_{j=1}^{n} A_{ij} Z_j\\right]\n$$\n\n由于 $d_i$ 和 $A_{ij}$ 是常数，我们可以写出：\n$$\n\\mathbb{E}[E_i] = \\frac{1}{d_i} \\sum_{j=1}^{n} \\mathbb{E}[A_{ij} Z_j] = \\frac{1}{d_i} \\sum_{j=1}^{n} A_{ij} \\mathbb{E}[Z_j]\n$$\n\n每个 $Z_j$ 都是参数为 $p$ 的伯努利随机变量，因此其期望为 $\\mathbb{E}[Z_j] = p$。将此代入方程可得：\n$$\n\\mathbb{E}[E_i] = \\frac{1}{d_i} \\sum_{j=1}^{n} A_{ij} p\n$$\n\n参数 $p$ 相对于求和索引 $j$ 是一个常数，所以可以将其提取出来：\n$$\n\\mathbb{E}[E_i] = \\frac{p}{d_i} \\sum_{j=1}^{n} A_{ij}\n$$\n\n根据单元 $i$ 的度的定义，我们有 $d_i = \\sum_{j=1}^{n} A_{ij}$。将此定义代入期望的表达式中，得到：\n$$\n\\mathbb{E}[E_i] = \\frac{p}{d_i} (d_i) = p\n$$\n约束条件 $d_i \\ge 1$ 确保了此表达式是良定义的。因此，任何单元 $i$ 的期望邻域暴露就是处理概率 $p$。\n\n接下来，我们推导 $E_i$ 的方差。我们从方差的定义开始，并使用性质：对于常数 $c$ 和随机变量 $X$，有 $\\operatorname{Var}(cX) = c^2 \\operatorname{Var}(X)$。\n$$\n\\operatorname{Var}(E_i) = \\operatorname{Var}\\left(\\frac{1}{d_i} \\sum_{j=1}^{n} A_{ij} Z_j\\right) = \\left(\\frac{1}{d_i}\\right)^2 \\operatorname{Var}\\left(\\sum_{j=1}^{n} A_{ij} Z_j\\right) = \\frac{1}{d_i^2} \\operatorname{Var}\\left(\\sum_{j=1}^{n} A_{ij} Z_j\\right)\n$$\n\n求和项 $\\sum_{j=1}^{n} A_{ij} Z_j$ 可以被简化。由于如果 $j$ 是 $i$ 的邻居，则 $A_{ij}$ 为 $1$，否则为 $0$，因此这个求和等价于对 $i$ 的邻居集合（我们可将其记为 $\\mathcal{N}_i = \\{j \\mid A_{ij}=1\\}$）中的 $Z_j$ 进行求和。邻居的数量为 $|\\mathcal{N}_i| = d_i$。\n$$\n\\sum_{j=1}^{n} A_{ij} Z_j = \\sum_{j \\in \\mathcal{N}_i} Z_j\n$$\n\n问题陈述变量 $Z_j$ 对所有 $j \\in \\{1, \\dots, n\\}$ 都是独立同分布抽取的。因此，变量 $\\{Z_j \\mid j \\in \\mathcal{N}_i\\}$ 是一组相互独立的随机变量。方差的一个基本性质是，对于一组独立随机变量 $X_k$ 的和，其方差等于方差的和：$\\operatorname{Var}\\left(\\sum_k X_k\\right) = \\sum_k \\operatorname{Var}(X_k)$。\n\n应用此性质，我们得到：\n$$\n\\operatorname{Var}\\left(\\sum_{j \\in \\mathcal{N}_i} Z_j\\right) = \\sum_{j \\in \\mathcal{N}_i} \\operatorname{Var}(Z_j)\n$$\n\n由于每个 $Z_j$ 都是一个独立同分布的伯努利($p$)变量，其方差为 $\\operatorname{Var}(Z_j) = p(1-p)$。这个方差对所有 $j$ 都是相同的。求和是针对单元 $i$ 的 $d_i$ 个邻居进行的。\n$$\n\\sum_{j \\in \\mathcal{N}_i} \\operatorname{Var}(Z_j) = \\sum_{j \\in \\mathcal{N}_i} p(1-p) = d_i \\cdot p(1-p)\n$$\n\n最后，我们将此结果代回 $\\operatorname{Var}(E_i)$ 的表达式中：\n$$\n\\operatorname{Var}(E_i) = \\frac{1}{d_i^2} \\left[d_i \\cdot p(1-p)\\right]\n$$\n\n考虑到问题约束条件 $d_i \\ge 1$，我们可以通过消去一个因子 $d_i$ 来简化表达式：\n$$\n\\operatorname{Var}(E_i) = \\frac{p(1-p)}{d_i}\n$$\n这个结果表明，邻域暴露的方差与单元的度成反比。\n\n推导出的量是期望 $\\mathbb{E}[E_i] = p$ 和方差 $\\operatorname{Var}(E_i) = \\frac{p(1-p)}{d_i}$。题目要求我们将它们呈现在一个单行矩阵中。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\np  \\frac{p(1-p)}{d_i}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "一旦我们能够描述网络暴露，下一个关键步骤就是利用它从观测数据中识别因果效应。这涉及到将不可观测的潜在结果与我们实际可以测量的量进行形式化的联系。在此练习  中，你将使用逆概率加权 (Inverse Probability Weighting, IPW) 这一强大技术，为直接因果效应推导出一个识别公式。这个练习展示了诸如条件可忽略性和正值性等核心因果假设，如何让我们能够构建一个无偏估计量，从而架起理论因果关系与实际数据分析之间的桥梁。",
            "id": "4266843",
            "problem": "考虑一个由 $n$ 个单元组成的群体，单元索引为 $i \\in \\{1,\\dots,n\\}$，嵌入在一个网络中。每个单元有一个二元处理 $Z_i \\in \\{0,1\\}$，一个网络暴露 $E_i$（通过已知的暴露映射总结其邻居的处理），基线协变量 $W_i$，以及一个观测结果 $Y_i$。假设暴露 $E_i$ 在一个离散的支持集 $\\mathcal{E}$ 中取值，并且干预下的潜在结果被明确定义为 $Y_i(z,e)$，其中 $z \\in \\{0,1\\}$ 且 $e \\in \\mathcal{E}$。令观测数据服从一致性关系 $Y_i = Y_i(Z_i,E_i)$。\n\n将在暴露水平 $E \\in \\mathcal{E}$ 下的因果直接效应定义为\n$$\n\\tau^{\\text{dir}}(E) = \\mathbb{E}\\big[ Y_i(1,E) - Y_i(0,E) \\big],\n$$\n其中期望是相对于单元的群体分布。\n\n给定联合倾向得分\n$$\ne_i(z,E) = \\Pr\\big( Z_i = z,\\; E_i = E \\mid W_i \\big),\n$$\n当 $\\Pr(W_i \\in \\mathcal{W}) > 0$ 且 $(z,E)$ 在给定 $W_i$ 的 $(Z_i,E_i)$ 的支持集内时，该得分严格介于 $0$ 和 $1$ 之间。假设以下识别条件成立：\n- 一致性：对于所有 $i$，$Y_i = Y_i(Z_i,E_i)$。\n- 条件可忽略性：对于所有 $E \\in \\mathcal{E}$，$\\{Y_i(1,E), Y_i(0,E)\\} \\perp (Z_i, E_i) \\mid W_i$。\n- 正定性：对于支持集中的所有 $z \\in \\{0,1\\}$ 和 $E \\in \\mathcal{E}$，几乎必然有 $0  e_i(z,E)  1$。\n\n从这些假设以及条件期望和重期望定律的定义出发，使用基于联合倾向得分 $e_i(z,E)$ 的逆概率加权法，推导 $\\tau^{\\text{dir}}(E)$ 的一个识别公式。你的最终答案必须是一个单一的封闭形式解析表达式。无需四舍五入，最终表达式不带单位。",
            "solution": "用户希望使用逆概率加权法，推导在网络干预下的因果直接效应（表示为 $\\tau^{\\text{dir}}(E)$）的识别公式。\n\n### 步骤 1：问题验证\n\n**1. 提取已知条件**\n- 由 $n$ 个单元组成的群体，单元索引为 $i \\in \\{1,\\dots,n\\}$。\n- 变量：二元处理 $Z_i \\in \\{0,1\\}$，网络暴露 $E_i \\in \\mathcal{E}$，基线协变量 $W_i$，以及观测结果 $Y_i$。\n- 潜在结果：$Y_i(z,e)$，其中 $z \\in \\{0,1\\}$ 且 $e \\in \\mathcal{E}$。\n- 感兴趣的因果参数：在暴露水平 $E \\in \\mathcal{E}$ 下的直接效应，定义为 $\\tau^{\\text{dir}}(E) = \\mathbb{E}\\big[ Y_i(1,E) - Y_i(0,E) \\big]$。\n- 联合倾向得分：$e_i(z,E) = \\Pr\\big( Z_i = z, E_i = E \\mid W_i \\big)$。\n- 识别假设：\n    1.  **一致性**：对于所有 $i$，$Y_i = Y_i(Z_i,E_i)$。\n    2.  **条件可忽略性**：对于所有 $E \\in \\mathcal{E}$，$\\{Y_i(1,E), Y_i(0,E)\\} \\perp (Z_i, E_i) \\mid W_i$。\n    3.  **正定性**：对于支持集中的所有 $z \\in \\{0,1\\}$ 和 $E \\in \\mathcal{E}$，几乎必然有 $0  e_i(z,E)  1$。\n\n**2. 使用提取的已知条件进行验证**\n- **科学依据：** 该问题牢固地定位于因果推断领域，具体涉及网络中的干预。它使用了标准的潜在结果框架（Neyman-Rubin模型）和三个用于识别的经典假设（一致性、可忽略性、正定性）。这是一个完善且具有科学合理性的理论设置。\n- **适定性：** 这是一个适定问题。它要求在一组给定的假设下推导一个特定量 $\\tau^{\\text{dir}}(E)$。所提供的假设已知足以识别因果效应，从而确保存在唯一解。\n- **客观性：** 该问题以形式化的数学语言陈述，没有主观或模糊的术语。\n- **其他缺陷：** 该问题未违反任何指定的无效性标准。它是完整的、一致的、可形式化的且非平凡的。\n\n**3. 结论与行动**\n该问题是**有效的**。将推导一个解决方案。\n\n### 步骤 2：识别公式的推导\n\n目标是用可观测的量来表达因果直接效应 $\\tau^{\\text{dir}}(E)$。直接效应的定义是：\n$$\n\\tau^{\\text{dir}}(E) = \\mathbb{E}\\big[ Y_i(1,E) - Y_i(0,E) \\big]\n$$\n根据期望算子的线性性质，我们可以将其分解为两项：\n$$\n\\tau^{\\text{dir}}(E) = \\mathbb{E}\\big[ Y_i(1,E) \\big] - \\mathbb{E}\\big[ Y_i(0,E) \\big]\n$$\n让我们专注于为一个给定的处理水平 $z \\in \\{0,1\\}$ 和暴露水平 $E \\in \\mathcal{E}$ 推导一个通用的平均潜在结果 $\\mu(z,E) = \\mathbb{E}\\big[ Y_i(z,E) \\big]$ 的识别公式。我们将使用逆概率加权法（IPW）。\n\n令 $\\mathbb{I}(\\cdot)$ 为指示函数。我们使用观测数据构建一个加权平均值，并看它是否能识别 $\\mu(z,E)$。考虑以下期望：\n$$\n\\mathbb{E}\\left[ \\frac{\\mathbb{I}(Z_i=z, E_i=E)}{e_i(z,E)} Y_i \\right]\n$$\n我们可以通过对协变量 $W_i$ 取条件，使用重期望定律将其分解：\n$$\n\\mathbb{E}\\left[ \\frac{\\mathbb{I}(Z_i=z, E_i=E)}{e_i(z,E)} Y_i \\right] = \\mathbb{E}_{W_i}\\left[ \\mathbb{E}\\left[ \\frac{\\mathbb{I}(Z_i=z, E_i=E)}{e_i(z,E)} Y_i \\;\\middle|\\; W_i \\right] \\right]\n$$\n倾向得分 $e_i(z,E) = \\Pr(Z_i=z, E_i=E \\mid W_i)$ 是 $W_i$ 的函数，因此在内层期望中可以将其视为常数：\n$$\n= \\mathbb{E}_{W_i}\\left[ \\frac{1}{e_i(z,E)} \\mathbb{E}\\left[ \\mathbb{I}(Z_i=z, E_i=E) Y_i \\;\\middle|\\; W_i \\right] \\right]\n$$\n现在，我们应用**一致性**假设，$Y_i = Y_i(Z_i, E_i)$。乘积 $\\mathbb{I}(Z_i=z, E_i=E) Y_i$ 仅在 $Z_i=z$ 和 $E_i=E$ 时非零，此时 $Y_i = Y_i(z,E)$。因此，我们可以写成 $\\mathbb{I}(Z_i=z, E_i=E) Y_i = \\mathbb{I}(Z_i=z, E_i=E) Y_i(z,E)$。代入可得：\n$$\n= \\mathbb{E}_{W_i}\\left[ \\frac{1}{e_i(z,E)} \\mathbb{E}\\left[ \\mathbb{I}(Z_i=z, E_i=E) Y_i(z,E) \\;\\middle|\\; W_i \\right] \\right]\n$$\n接下来，我们引用**条件可忽略性**假设，该假设指出在给定协变量的条件下，潜在结果独立于处理/暴露分配：$\\{Y_i(1,E), Y_i(0,E)\\} \\perp (Z_i, E_i) \\mid W_i$。这意味着 $Y_i(z,E) \\perp (Z_i, E_i) \\mid W_i$。由于这种条件独立性，我们可以分解内层期望：\n$$\n\\mathbb{E}\\left[ \\mathbb{I}(Z_i=z, E_i=E) Y_i(z,E) \\;\\middle|\\; W_i \\right] = \\mathbb{E}\\left[ Y_i(z,E) \\mid W_i \\right] \\mathbb{E}\\left[ \\mathbb{I}(Z_i=z, E_i=E) \\mid W_i \\right]\n$$\n乘积中的第二项是指示函数的期望，也就是它所指示事件的概率。根据联合倾向得分的定义：\n$$\n\\mathbb{E}\\left[ \\mathbb{I}(Z_i=z, E_i=E) \\mid W_i \\right] = \\Pr(Z_i=z, E_i=E \\mid W_i) = e_i(z,E)\n$$\n将此代回我们的主表达式：\n$$\n= \\mathbb{E}_{W_i}\\left[ \\frac{1}{e_i(z,E)} \\mathbb{E}\\left[ Y_i(z,E) \\mid W_i \\right] e_i(z,E) \\right]\n$$\n**正定性**假设，$e_i(z,E) > 0$，确保了除法有意义，并允许我们消去 $e_i(z,E)$ 项：\n$$\n= \\mathbb{E}_{W_i}\\left[ \\mathbb{E}\\left[ Y_i(z,E) \\mid W_i \\right] \\right]\n$$\n最后，再次应用重期望定律，我们发现这简化为我们想要的量：\n$$\n= \\mathbb{E}\\big[ Y_i(z,E) \\big] = \\mu(z,E)\n$$\n因此，我们证明了平均潜在结果的识别公式：\n$$\n\\mathbb{E}\\big[ Y_i(z,E) \\big] = \\mathbb{E}\\left[ \\frac{\\mathbb{I}(Z_i=z, E_i=E)}{e_i(z,E)} Y_i \\right]\n$$\n现在我们可以将这个结果应用于 $\\tau^{\\text{dir}}(E)$ 的两项：\n$$\n\\mathbb{E}\\big[ Y_i(1,E) \\big] = \\mathbb{E}\\left[ \\frac{\\mathbb{I}(Z_i=1, E_i=E)}{e_i(1,E)} Y_i \\right]\n$$\n$$\n\\mathbb{E}\\big[ Y_i(0,E) \\big] = \\mathbb{E}\\left[ \\frac{\\mathbb{I}(Z_i=0, E_i=E)}{e_i(0,E)} Y_i \\right]\n$$\n将这些代回到 $\\tau^{\\text{dir}}(E)$ 的表达式中：\n$$\n\\tau^{\\text{dir}}(E) = \\mathbb{E}\\left[ \\frac{\\mathbb{I}(Z_i=1, E_i=E)}{e_i(1,E)} Y_i \\right] - \\mathbb{E}\\left[ \\frac{\\mathbb{I}(Z_i=0, E_i=E)}{e_i(0,E)} Y_i \\right]\n$$\n利用期望的线性性质，我们可以将它们合并成一个单一的期望，以获得最终的识别公式：\n$$\n\\tau^{\\text{dir}}(E) = \\mathbb{E}\\left[ \\left( \\frac{\\mathbb{I}(Z_i=1, E_i=E)}{e_i(1,E)} - \\frac{\\mathbb{I}(Z_i=0, E_i=E)}{e_i(0,E)} \\right) Y_i \\right]\n$$\n该表达式仅是观测数据（$Z_i, E_i, Y_i$）和倾向得分 $e_i(z,E)$（可从 $Z_i, E_i, W_i$ 估计）的函数，因此可以识别 $\\tau^{\\text{dir}}(E)$。\n由于 $Z_i$ 是在 $\\{0,1\\}$ 中取值的二元变量，我们可以写成 $\\mathbb{I}(Z_i=1) = Z_i$ 和 $\\mathbb{I}(Z_i=0) = 1-Z_i$。这允许一个更紧凑的表示：\n$$\n\\tau^{\\text{dir}}(E) = \\mathbb{E}\\left[ \\left( \\frac{Z_i}{e_i(1,E)} - \\frac{1-Z_i}{e_i(0,E)} \\right) \\mathbb{I}(E_i=E) Y_i \\right]\n$$\n这就是所要求的最终封闭形式解析表达式。",
            "answer": "$$\n\\boxed{\\mathbb{E}\\left[ \\left( \\frac{\\mathbb{I}(Z_i=1, E_i=E)}{e_i(1,E)} - \\frac{\\mathbb{I}(Z_i=0, E_i=E)}{e_i(0,E)} \\right) Y_i \\right]}\n$$"
        },
        {
            "introduction": "现实世界的数据很少是完美的，对任何研究者而言，一项关键技能是理解数据的不完美性如何影响其结论。我们现在转向网络测量误差这一实际挑战。这个问题  将引导你分析未观测到的网络连接如何导致溢出效应估计的偏差。通过推导一个朴素 OLS 估计量的渐近偏差，你将对网络环境下的“变量误差”(errors-in-variables) 问题有更深刻的理解，并学会量化估计偏差的方向和大小，这是稳健因果分析中的关键一步。",
            "id": "4266842",
            "problem": "考虑一个由 $n$ 个个体组成的网络，个体索引为 $i \\in \\{1,\\dots,n\\}$，其邻接矩阵 $A \\in \\{0,1\\}^{n \\times n}$ 是对称且无自环的，其中 $A_{ii} = 0$。对于 $i \\neq j$，$A_{ij} \\sim \\mathrm{Bernoulli}(r)$ 在有序对 $(i,j)$ 间独立同分布，边概率为 $r \\in (0,1)$。处理根据 $T_i \\sim \\mathrm{Bernoulli}(q)$（其中 $q \\in (0,1)$）在各个体间独立分配，且处理分配独立于网络 $A$。定义真实的暴露映射为\n$$\nE_i \\equiv \\sum_{j \\neq i} A_{ij} T_j,\n$$\n该式计算了个体 $i$ 的已处理邻居的数量。假设由于独立的边缺失导致边被错误测量，因此观测到的邻接关系为 $\\tilde{A}_{ij} \\equiv A_{ij} S_{ij}$，其中 $S_{ij} \\sim \\mathrm{Bernoulli}(1-p)$ 独立于 $(A,T)$ 且相互独立，$p \\in (0,1)$ 是边缺失率。观测到的（错误测量的）暴露为\n$$\n\\tilde{E}_i \\equiv \\sum_{j \\neq i} \\tilde{A}_{ij} T_j = \\sum_{j \\neq i} A_{ij} S_{ij} T_j.\n$$\n结果遵循一个带干扰的线性结构模型，\n$$\nY_i = \\alpha + \\beta T_i + \\gamma E_i + \\varepsilon_i,\n$$\n其中 $\\alpha,\\beta,\\gamma \\in \\mathbb{R}$ 是固定参数，$\\varepsilon_i$ 是均值为零的误差，独立于 $(A,T,S)$ 且同分布，方差有限。一位研究者在不知道边缺失过程的情况下，对 $Y_i$ 关于截距项、自身处理 $T_i$ 和错误测量的暴露 $\\tilde{E}_i$ 进行了普通最小二乘（OLS）回归。\n\n在上述数据生成过程下，当 $n \\to \\infty$ 时，推导 $\\tilde{E}_i$ 的OLS系数的渐近偏差（该系数被解释为溢出效应 $\\gamma$ 的估计量）。将偏差\n$$\n\\mathrm{Bias} \\equiv \\operatorname{plim}\\!\\left(\\hat{\\gamma}_{\\mathrm{naive}}\\right) - \\gamma\n$$\n表示为仅含 $p$、$r$ 和 $q$ 的单个闭式解析表达式。不需要数值近似或四舍五入。",
            "solution": "我们从网络和处理分配的定义、暴露映射以及带干扰的线性结果模型开始。真实暴露为 $E_i = \\sum_{j \\neq i} A_{ij} T_j$，而错误测量的暴露为 $\\tilde{E}_i = \\sum_{j \\neq i} A_{ij} S_{ij} T_j$，其中 $S_{ij} \\sim \\mathrm{Bernoulli}(1-p)$ 独立于 $(A,T)$。\n\n研究者使用普通最小二乘法（OLS）对 $Y_i$ 关于截距项、$T_i$ 和 $\\tilde{E}_i$ 来估计模型。根据 Frisch–Waugh–Lovell 定理，$\\tilde{E}_i$ 的 OLS 系数与将 $Y_i$ 剔除 $T_i$ 的贡献后的残差对 $\\tilde{E}_i$ 剔除 $T_i$ 的贡献后的残差进行回归所得到的系数相同。因为在数据生成过程中 $T_i$ 独立于 $\\tilde{E}_i$（因为 $T_i$ 独立于 $\\{T_j: j \\neq i\\}$ 以及 $A,S$），我们有 $\\operatorname{Cov}(T_i,\\tilde{E}_i)=0$。因此，相对于将 $Y_i$ 对 $\\tilde{E}_i$ 进行的简单回归，部分剔除 $T_i$ 不会改变斜率。因此，$\\tilde{E}_i$ 的大样本 OLS 系数（在概率上）收敛于\n$$\n\\operatorname{plim}\\!\\left(\\hat{\\gamma}_{\\mathrm{naive}}\\right) = \\gamma \\cdot \\frac{\\operatorname{Cov}(\\tilde{E}_i, E_i)}{\\operatorname{Var}(\\tilde{E}_i)},\n$$\n条件是 $\\varepsilon_i$ 均值为零且独立于 $\\tilde{E}_i$。\n\n我们现在在指定的生成模型下计算 $\\operatorname{Cov}(\\tilde{E}_i, E_i)$ 和 $\\operatorname{Var}(\\tilde{E}_i)$。令 $X_{ij} \\equiv A_{ij} T_j$ 且 $W_{ij} \\equiv A_{ij} S_{ij} T_j$。那么 $E_i = \\sum_{j \\neq i} X_{ij}$ 且 $\\tilde{E}_i = \\sum_{j \\neq i} W_{ij}$。对于 $j \\neq i$，$(A_{ij},S_{ij},T_j)$ 在不同的 $j$ 之间是相互独立的，因此不同邻居的项是独立的。\n\n首先，计算单个邻居 $j$ 的矩：\n- $X_{ij} = 1$ 的概率是 $\\Pr(X_{ij}=1) = \\Pr(A_{ij}=1)\\Pr(T_j=1) = r q$，所以\n$$\n\\mathbb{E}[X_{ij}] = r q.\n$$\n- $W_{ij} = 1$ 的概率是 $\\Pr(W_{ij}=1) = \\Pr(A_{ij}=1)\\Pr(S_{ij}=1)\\Pr(T_j=1) = r (1-p) q$，所以\n$$\n\\mathbb{E}[W_{ij}] = r (1-p) q, \\quad \\operatorname{Var}(W_{ij}) = r (1-p) q \\left(1 - r (1-p) q \\right).\n$$\n- 乘积 $W_{ij} X_{ij}$ 满足 $W_{ij} X_{ij} = A_{ij} S_{ij} T_j \\cdot A_{ij} T_j = A_{ij} S_{ij} T_j = W_{ij}$（因为 $A_{ij} \\in \\{0,1\\}$ 且 $T_j \\in \\{0,1\\}$），所以\n$$\n\\mathbb{E}[W_{ij} X_{ij}] = \\mathbb{E}[W_{ij}] = r (1-p) q.\n$$\n因此，\n$$\n\\operatorname{Cov}(W_{ij}, X_{ij}) = \\mathbb{E}[W_{ij} X_{ij}] - \\mathbb{E}[W_{ij}] \\mathbb{E}[X_{ij}] = r (1-p) q - \\big(r (1-p) q\\big)\\big(r q\\big) = r (1-p) q \\big(1 - r q\\big).\n$$\n\n因为不同邻居的项是独立的，对于 $j \\neq i$ 的求和，我们有：\n$$\n\\operatorname{Cov}(\\tilde{E}_i, E_i) = \\sum_{j \\neq i} \\operatorname{Cov}(W_{ij}, X_{ij}) = (n-1) \\, r (1-p) q \\big(1 - r q\\big),\n$$\n并且\n$$\n\\operatorname{Var}(\\tilde{E}_i) = \\sum_{j \\neq i} \\operatorname{Var}(W_{ij}) = (n-1) \\, r (1-p) q \\left(1 - r (1-p) q\\right).\n$$\n因此，该比率简化为\n$$\n\\frac{\\operatorname{Cov}(\\tilde{E}_i, E_i)}{\\operatorname{Var}(\\tilde{E}_i)} = \\frac{1 - r q}{1 - r (1-p) q}.\n$$\n因此，\n$$\n\\operatorname{plim}\\!\\left(\\hat{\\gamma}_{\\mathrm{naive}}\\right) = \\gamma \\cdot \\frac{1 - r q}{1 - r (1-p) q}.\n$$\n渐近偏差是这个概率极限与真实溢出效应 $\\gamma$ 之间的差值：\n$$\n\\mathrm{Bias} = \\gamma \\cdot \\frac{1 - r q}{1 - r (1-p) q} - \\gamma = \\gamma \\left( \\frac{1 - r q - \\big(1 - r (1-p) q\\big)}{1 - r (1-p) q} \\right) = \\gamma \\left( \\frac{- r q + r (1-p) q}{1 - r (1-p) q} \\right).\n$$\n化简分子，\n$$\n- r q + r (1-p) q = - r p q,\n$$\n我们得到闭式表达式\n$$\n\\mathrm{Bias} = - \\gamma \\cdot \\frac{r p q}{1 - r (1-p) q}.\n$$\n这表明，独立的边缺失（$p \\in (0,1)$）会导致对溢出效应的朴素 OLS 估计量产生衰减，其幅度由连接概率 $r$、处理概率 $q$ 和缺失率 $p$ 的相互作用决定。",
            "answer": "$$\\boxed{-\\gamma \\cdot \\frac{r p q}{1 - r (1-p) q}}$$"
        }
    ]
}