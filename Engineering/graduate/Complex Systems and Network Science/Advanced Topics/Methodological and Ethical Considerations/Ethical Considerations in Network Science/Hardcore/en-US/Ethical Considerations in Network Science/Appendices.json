{
    "hands_on_practices": [
        {
            "introduction": "Protecting the confidentiality of relationships is a cornerstone of ethical network science. A fundamental technique for achieving this is randomized response, where a degree of noise is intentionally introduced into the data before it is collected. This exercise  challenges you to quantify the privacy protection afforded by this method using the formal framework of $\\epsilon$-Local Differential Privacy, deriving the relationship between the mechanism's noise level and the strength of the privacy guarantee.",
            "id": "4274556",
            "problem": "An undirected simple graph on $n$ nodes is represented by a symmetric adjacency matrix $A \\in \\{0,1\\}^{n \\times n}$ with $A_{ii} = 0$ for all $i$. Each off-diagonal entry $A_{ij}$ for $i < j$ encodes a potentially sensitive dyadic relationship. To protect the confidentiality of individual relationships in compliance with ethical standards of privacy in network science, a researcher applies the following randomized response mechanism independently to each off-diagonal entry: for a given input bit $x \\in \\{0,1\\}$, the released bit $y$ equals $x$ with probability $1 - \\pi$ and equals $1 - x$ with probability $\\pi$, where $0 < \\pi \\leq \\tfrac{1}{2}$ is a fixed flip probability common to all entries.\n\nUsing the definition of Local Differential Privacy (LDP), which requires that for any two input bits $x, x' \\in \\{0,1\\}$ and any output $y \\in \\{0,1\\}$ the mechanism $\\mathcal{M}$ satisfies $\\Pr[\\mathcal{M}(x) = y] \\leq \\exp(\\epsilon)\\,\\Pr[\\mathcal{M}(x') = y]$, determine the minimal $\\epsilon$ (as a function of $\\pi$) that certifies this randomized response mechanism as $\\epsilon$-LDP for a single adjacency entry. Express your final answer as a closed-form analytic expression in terms of $\\pi$. No numerical rounding is required.",
            "solution": "The problem requires us to determine the minimal privacy parameter $\\epsilon$ for a given randomized response mechanism, such that the mechanism satisfies the definition of $\\epsilon$-Local Differential Privacy (LDP).\n\nThe mechanism $\\mathcal{M}$ takes a single binary input $x \\in \\{0, 1\\}$, representing the presence or absence of a sensitive relationship, and produces a binary output $y \\in \\{0, 1\\}$. The probabilistic mapping is defined by the flip probability $\\pi$, where $0 < \\pi \\leq \\frac{1}{2}$.\n\nThe probabilities of the output $y$ given the input $x$ are as follows:\n- The probability of the output being the same as the input (no flip) is $\\Pr[y = x] = 1 - \\pi$.\n- The probability of the output being different from the input (a flip) is $\\Pr[y = 1 - x] = \\pi$.\n\nWe can write down the four conditional probabilities explicitly:\n1.  For input $x=0$:\n    $\\Pr[\\mathcal{M}(0) = 0] = 1 - \\pi$\n    $\\Pr[\\mathcal{M}(0) = 1] = \\pi$\n2.  For input $x=1$:\n    $\\Pr[\\mathcal{M}(1) = 0] = \\pi$\n    $\\Pr[\\mathcal{M}(1) = 1] = 1 - \\pi$\n\nThe definition of $\\epsilon$-LDP, as provided, states that for any two inputs $x, x' \\in \\{0,1\\}$ and for any possible output $y \\in \\{0,1\\}$, the following inequality must hold:\n$$\n\\Pr[\\mathcal{M}(x) = y] \\leq \\exp(\\epsilon)\\,\\Pr[\\mathcal{M}(x') = y]\n$$\nTo find the minimal $\\epsilon$ that satisfies this condition, we must find the largest possible value of the ratio $\\frac{\\Pr[\\mathcal{M}(x) = y]}{\\Pr[\\mathcal{M}(x') = y]}$ over all possible combinations of distinct inputs $x, x'$ and all possible outputs $y$. Let us set $x=0$ and $x'=1$ without loss of generality, as the input domain is binary. The minimal $\\epsilon$ must satisfy:\n$$\n\\exp(\\epsilon) \\geq \\max_{y \\in \\{0,1\\}} \\left( \\max \\left( \\frac{\\Pr[\\mathcal{M}(0) = y]}{\\Pr[\\mathcal{M}(1) = y]}, \\frac{\\Pr[\\mathcal{M}(1) = y]}{\\Pr[\\mathcal{M}(0) = y]} \\right) \\right)\n$$\nLet's compute these ratios for each possible output $y$.\n\nCase 1: Output $y = 0$.\nThe two ratios are:\n$$\n\\frac{\\Pr[\\mathcal{M}(0) = 0]}{\\Pr[\\mathcal{M}(1) = 0]} = \\frac{1 - \\pi}{\\pi}\n$$\n$$\n\\frac{\\Pr[\\mathcal{M}(1) = 0]}{\\Pr[\\mathcal{M}(0) = 0]} = \\frac{\\pi}{1 - \\pi}\n$$\n\nCase 2: Output $y = 1$.\nThe two ratios are:\n$$\n\\frac{\\Pr[\\mathcal{M}(0) = 1]}{\\Pr[\\mathcal{M}(1) = 1]} = \\frac{\\pi}{1 - \\pi}\n$$\n$$\n\\frac{\\Pr[\\mathcal{M}(1) = 1]}{\\Pr[\\mathcal{M}(0) = 1]} = \\frac{1 - \\pi}{\\pi}\n$$\nThe set of all possible ratios to consider is therefore $\\left\\{ \\frac{1-\\pi}{\\pi}, \\frac{\\pi}{1-\\pi} \\right\\}$. To satisfy the LDP condition for all cases, $\\exp(\\epsilon)$ must be greater than or equal to the maximum of these ratios:\n$$\n\\exp(\\epsilon) \\geq \\max\\left(\\frac{1 - \\pi}{\\pi}, \\frac{\\pi}{1 - \\pi}\\right)\n$$\nWe are given the constraint $0 < \\pi \\leq \\frac{1}{2}$.\nIf $\\pi = \\frac{1}{2}$, then $1 - \\pi = \\frac{1}{2}$, and both ratios are equal to $1$.\nIf $0 < \\pi < \\frac{1}{2}$, then $1 - \\pi > \\pi$. Dividing by $\\pi$ (which is positive) and by $1-\\pi$ (which is also positive), we get:\n$$\n\\frac{1 - \\pi}{\\pi} > 1 \\quad \\text{and} \\quad \\frac{\\pi}{1 - \\pi} < 1\n$$\nTherefore, for the entire range $0 < \\pi \\leq \\frac{1}{2}$, the maximum of the two ratios is $\\frac{1 - \\pi}{\\pi}$.\n\nThe condition for $\\epsilon$ simplifies to:\n$$\n\\exp(\\epsilon) \\geq \\frac{1 - \\pi}{\\pi}\n$$\nTo find the minimal value of $\\epsilon$ that guarantees this inequality, we take the equality:\n$$\n\\exp(\\epsilon) = \\frac{1 - \\pi}{\\pi}\n$$\nSolving for $\\epsilon$ by taking the natural logarithm of both sides, we obtain the minimal $\\epsilon$ as a function of $\\pi$:\n$$\n\\epsilon = \\ln\\left(\\frac{1 - \\pi}{\\pi}\\right)\n$$\nThis expression provides the tightest possible privacy bound for the given randomized response mechanism.",
            "answer": "$$\\boxed{\\ln\\left(\\frac{1 - \\pi}{\\pi}\\right)}$$"
        },
        {
            "introduction": "When deploying interventions on a network, the effects of treatment can spill over between connected individuals, a phenomenon known as interference. This not only complicates causal analysis but also poses an ethical challenge by creating unintended exposures. This practice  guides you through the design of a cluster randomization scheme, an experimental design that can mitigate such spillovers, and demonstrates how the ethical goal of minimizing interference translates into a concrete network optimization problem.",
            "id": "4274605",
            "problem": "You are given a simple, undirected, finite graph $G=(V,E)$ with a known vertex set $V$ and edge set $E$, together with a budget constraint specifying an expected number of treated vertices $B \\in \\{0,1,2,\\dots,|V|\\}$. Consider two assignment mechanisms: individual randomization and cluster randomization. Under individual randomization, each vertex is independently assigned to treatment with the same probability $p = B/|V|$. Under cluster randomization, the vertex set $V$ is partitioned into exactly $K$ disjoint clusters $C_1,\\dots,C_K$ such that each $C_k$ induces a connected subgraph of $G$ and satisfies $|C_k| \\leq S_{\\max}$, and clusters are independently assigned to treatment with the same probability $p = B/|V|$, with all vertices in a cluster sharing the same assignment.\n\nDefine the ethical objective of the design as minimizing expected spillovers, where a spillover is any edge whose endpoints receive different assignments. Under either mechanism, the expected spillover count is the sum, over all edges, of the probability that the endpoints receive different assignments. The ethical aim is to choose a partition for cluster randomization that minimizes this expectation, subject to the connectivity and size constraints stated above. Let $m_{\\text{inter}}$ denote, for a given partition, the number of edges with endpoints in different clusters, and let $|E|$ be the total number of edges.\n\nYour task is to:\n- Derive, from first principles, the expected spillover count under individual randomization and under cluster randomization, and show how minimizing the expected spillovers under cluster randomization reduces to minimizing $m_{\\text{inter}}$ for fixed $p$.\n- Design and implement an algorithm that, for small graphs, searches over all feasible partitions into exactly $K$ connected clusters of size at most $S_{\\max}$ and returns the partition that minimizes $m_{\\text{inter}}$.\n- For each test case, compute the ratio $r = \\frac{\\text{expected spillovers under cluster randomization}}{\\text{expected spillovers under individual randomization}}$. When $|E| = 0$, define $r$ to be $0.0$.\n\nYour program must implement the above and produce results for the following test suite, each specified as $(G,K,S_{\\max},B)$:\n- Test case $1$ (happy path, modular structure): $G$ has $|V|=6$ vertices and edges forming two $3$-vertex cliques connected by a single bridging edge. Specifically, $E = \\{(0,1),(0,2),(1,2),(3,4),(3,5),(4,5),(2,3)\\}$, with $K=2$, $S_{\\max}=3$, and $B=3$.\n- Test case $2$ (dense graph boundary): $G$ is the complete graph on $|V|=5$ vertices, with $K=2$, $S_{\\max}=3$, and $B=2$.\n- Test case $3$ (no-edge boundary): $G$ has $|V|=4$ vertices and $E=\\varnothing$, with $K=2$, $S_{\\max}=2$, and $B=2$.\n- Test case $4$ (sparse path): $G$ is the path on $|V|=7$ vertices, $E=\\{(0,1),(1,2),(2,3),(3,4),(4,5),(5,6)\\}$, with $K=3$, $S_{\\max}=3$, and $B=3$.\n- Test case $5$ (star-like graph): $G$ has $|V|=5$ vertices with center $0$ connected to leaves $1,2,3,4$, so $E=\\{(0,1),(0,2),(0,3),(0,4)\\}$, with $K=3$, $S_{\\max}=3$, and $B=2$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets in the order of the test suite, where each element is the float $r$ for a test case. For example, the output should look like $[\\text{result}_1,\\text{result}_2,\\dots,\\text{result}_5]$ with each $\\text{result}_i$ being a decimal representation of $r$ for test case $i$.",
            "solution": "We begin from fundamental definitions of graphs and probability. Let $G=(V,E)$ be a simple, undirected graph. For any edge $(u,v) \\in E$, define the indicator random variable $I_{uv}$ that equals $1$ if vertices $u$ and $v$ receive different assignments (one treated, one control), and $0$ otherwise. The expected spillover count is the expected value of $\\sum_{(u,v) \\in E} I_{uv}$, which by linearity of expectation equals $\\sum_{(u,v) \\in E} \\mathbb{E}[I_{uv}]$, where $\\mathbb{E}[I_{uv}]$ is the probability that assignments at $u$ and $v$ differ.\n\nUnder individual randomization, each vertex is independently treated with probability $p = B/|V|$. For any edge $(u,v)$, the probability that $u$ and $v$ receive different assignments is the sum of the mutually exclusive events \"treat $u$ and control $v$\" and \"control $u$ and treat $v$\", each occurring with probability $p(1-p)$. Hence, for any edge $(u,v)$, $\\mathbb{E}[I_{uv}] = p(1-p) + (1-p)p = 2p(1-p)$. Summing over all edges gives the expected spillover count under individual randomization:\n$$\nE_{\\text{individual}} = \\sum_{(u,v)\\in E} 2p(1-p) = 2p(1-p)\\,|E|.\n$$\n\nUnder cluster randomization, we partition $V$ into $K$ disjoint clusters $C_1,\\dots,C_K$ such that each $C_k$ induces a connected subgraph and satisfies $|C_k| \\leq S_{\\max}$. Clusters are independently assigned to treatment with probability $p$, and all vertices in a cluster share the same assignment. Consider an edge $(u,v)$. If $u$ and $v$ lie in the same cluster, their assignments are always equal, so $\\mathbb{E}[I_{uv}] = 0$. If $u$ and $v$ lie in different clusters, they are assigned independently at the cluster level, so the probability that their assignments differ is again $2p(1-p)$. Therefore, writing $m_{\\text{inter}}$ for the number of edges in $E$ with endpoints belonging to different clusters, the expected spillover count is\n$$\nE_{\\text{cluster}} = \\sum_{(u,v) \\in E} \\mathbb{E}[I_{uv}] = 2p(1-p)\\,m_{\\text{inter}}.\n$$\nFor fixed $p$, minimizing $E_{\\text{cluster}}$ is equivalent to minimizing $m_{\\text{inter}}$, which is the count of edges cut by the partition. This converts the ethical design objective into a combinatorial optimization problem: choose a feasible partition into exactly $K$ connected clusters (each of size at most $S_{\\max}$) that minimizes the inter-cluster cut size $m_{\\text{inter}}$.\n\nAlgorithmic design proceeds as follows. For small graphs, we can perform an exhaustive search over feasible partitions:\n- Enumerate connected subsets that can form clusters, constrained by the size bound $|C_k| \\leq S_{\\max}$. A subset is connected if the subgraph induced by that subset has a single connected component. We generate connected subsets containing a designated anchor vertex by adding vertices that are adjacent to the current subset; this guarantees connectivity.\n- Recursively build a partition: pick the smallest unassigned vertex as an anchor, enumerate all connected subsets containing it (and lying within the set of remaining vertices) of size at most $S_{\\max}$, and branch by placing such a subset as the next cluster. For the final cluster, require that the remaining vertices form a connected subset and satisfy the size constraint. This recursion avoids duplicate partitions by always anchoring on the smallest remaining vertex.\n- For each complete partition, compute $m_{\\text{inter}}$ by counting edges in $E$ whose endpoints lie in different clusters. Keep the partition that attains the minimum $m_{\\text{inter}}$.\n\nOnce the minimal $m_{\\text{inter}}$ is found for a test case, compute $E_{\\text{cluster}} = 2p(1-p)\\,m_{\\text{inter}}$ and $E_{\\text{individual}} = 2p(1-p)\\,|E|$. Their ratio simplifies to\n$$\nr = \\frac{E_{\\text{cluster}}}{E_{\\text{individual}}} = \\begin{cases}\n\\frac{m_{\\text{inter}}}{|E|} & \\text{if } |E| > 0,\\\\\n0.0 & \\text{if } |E| = 0,\n\\end{cases}\n$$\nwhich is independent of $p$. The $|E|=0$ boundary is handled by definition since no spillovers can occur in edgeless graphs.\n\nEthical justification over individual randomization arises from principles of nonmaleficence and epistemic responsibility. Minimizing $m_{\\text{inter}}$ directly reduces the expected number of neighbor pairs experiencing discordant treatment assignments, thereby reducing potential interference-related harms such as conflict, contagion, or inequitable burdens transmitted across edges. Cluster-level assignments aligned with network connectivity reduce heterogeneity in exposures within tightly knit groups, promoting fairness by avoiding unequal treatment among close contacts. Further, by lowering interference, cluster randomization reduces bias in estimating causal effects such as the Average Treatment Effect (ATE), improving the reliability of knowledge used to inform policy; ethically, producing less biased evidence protects downstream populations from decisions based on distorted inferences. Finally, cluster designs that respect natural communities can better align with obligations reviewed by oversight such as an Institutional Review Board (IRB), balancing scientific rigor with participant welfare. In sum, optimizing cluster partitions to minimize cut edges is both statistically well-founded and ethically advantageous compared to individual randomization in networked settings.\n\nThe program implements the exhaustive search described above for the specified test suite and outputs the ratio $r$ for each case in the required single-line format.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef build_adjacency(n, edges):\n    adj = [set() for _ in range(n)]\n    for u, v in edges:\n        adj[u].add(v)\n        adj[v].add(u)\n    return adj\n\ndef is_connected_subset(subset, adj):\n    # subset: set of nodes\n    if not subset:\n        return False\n    if len(subset) == 1:\n        return True\n    # BFS within subset\n    start = next(iter(subset))\n    visited = set([start])\n    stack = [start]\n    while stack:\n        u = stack.pop()\n        for v in adj[u]:\n            if v in subset and v not in visited:\n                visited.add(v)\n                stack.append(v)\n    return visited == subset\n\ndef enumerate_connected_subsets(start, available, adj, smax):\n    # Generate connected subsets containing 'start', subset of 'available', size <= smax\n    results = set()\n    def dfs(current_set, frontier):\n        fs = frozenset(current_set)\n        if fs not in results:\n            results.add(fs)\n        if len(current_set) >= smax:\n            return\n        # Try adding each node in frontier\n        for v in sorted(frontier):\n            if v in current_set:\n                continue\n            new_set = set(current_set)\n            new_set.add(v)\n            # New frontier: neighbors of v plus existing frontier, within available, excluding current nodes\n            new_frontier = ((frontier | adj[v]) & available) - new_set\n            dfs(new_set, new_frontier)\n    initial_frontier = (adj[start] & available) - {start}\n    dfs(set([start]), initial_frontier)\n    # Return as list of frozensets\n    return [fs for fs in results if len(fs) <= smax]\n\ndef compute_inter_edges(partition, edges):\n    # partition: list of sets covering all nodes, disjoint\n    # Map each node to cluster id\n    cluster_id = {}\n    for idx, cluster in enumerate(partition):\n        for u in cluster:\n            cluster_id[u] = idx\n    cut = 0\n    for (u, v) in edges:\n        if cluster_id[u] != cluster_id[v]:\n            cut += 1\n    return cut\n\ndef minimize_cut_partition(n, edges, K, smax):\n    adj = build_adjacency(n, edges)\n    best_cut = None\n    best_partition = None\n\n    # Recursively build partition\n    def rec(remaining, clusters_left, current_partition):\n        nonlocal best_cut, best_partition\n        # Prune impossible: need at least one node per remaining cluster\n        if len(remaining) < clusters_left:\n            return\n        if clusters_left == 0:\n            if len(remaining) == 0:\n                # Evaluate\n                cut = compute_inter_edges(current_partition, edges)\n                if (best_cut is None) or (cut < best_cut):\n                    best_cut = cut\n                    best_partition = [set(c) for c in current_partition]\n            return\n        if len(remaining) == 0:\n            return\n        if clusters_left == 1:\n            # The last cluster must be remaining; check constraints\n            if len(remaining) <= smax and is_connected_subset(remaining, adj):\n                current_partition.append(set(remaining))\n                cut = compute_inter_edges(current_partition, edges)\n                if (best_cut is None) or (cut < best_cut):\n                    best_cut = cut\n                    best_partition = [set(c) for c in current_partition]\n                current_partition.pop()\n            return\n        # Choose smallest remaining node as anchor\n        start = min(remaining)\n        available = set(remaining)\n        subsets = enumerate_connected_subsets(start, available, adj, smax)\n        # Try each subset that is within remaining\n        for fs in subsets:\n            S = set(fs)\n            if not S.issubset(remaining):\n                continue\n            # Add S as a cluster\n            current_partition.append(S)\n            rec(remaining - S, clusters_left - 1, current_partition)\n            current_partition.pop()\n\n    rec(set(range(n)), K, [])\n    return best_cut, best_partition\n\ndef expected_ratio(n, edges, K, smax, B):\n    # Compute ratio r = E_cluster / E_individual = m_inter / |E| if |E|>0 else 0\n    m = len(edges)\n    if m == 0:\n        return 0.0\n    best_cut, best_partition = minimize_cut_partition(n, edges, K, smax)\n    # Safety: if no partition found (should not happen for valid tests), return NaN-like but we choose 0.0\n    if best_cut is None:\n        return 0.0\n    # Ratio independent of p\n    r = best_cut / m\n    return r\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each test case: (n, edges, K, Smax, B)\n    test_cases = [\n        # Test 1: two triangles bridged by one edge\n        (6, [(0,1),(0,2),(1,2),(3,4),(3,5),(4,5),(2,3)], 2, 3, 3),\n        # Test 2: complete graph K5\n        (5, [(i,j) for i in range(5) for j in range(i+1,5)], 2, 3, 2),\n        # Test 3: edgeless graph on 4 nodes\n        (4, [], 2, 2, 2),\n        # Test 4: path on 7 nodes\n        (7, [(0,1),(1,2),(2,3),(3,4),(4,5),(5,6)], 3, 3, 3),\n        # Test 5: star graph with center 0\n        (5, [(0,1),(0,2),(0,3),(0,4)], 3, 3, 2),\n    ]\n\n    results = []\n    for case in test_cases:\n        n, edges, K, Smax, B = case\n        r = expected_ratio(n, edges, K, Smax, B)\n        results.append(r)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Algorithmic systems built on network data risk perpetuating or even amplifying existing societal biases. In a network characterized by homophily, a link prediction model may become less accurate for connections between minority groups, a form of algorithmic unfairness. This exercise  provides a formal model to quantify this bias amplification, showing how a standard sampling method can systematically disadvantage certain groups, and prompts you to consider corrective design choices.",
            "id": "4274574",
            "problem": "Consider a simple undirected network with two protected groups, group $A$ with $n_A$ nodes and group $B$ with $n_B$ nodes. Assume homophily in the generative process: the probability of an edge between two nodes of the same group is $p_W$, and the probability of an edge between two nodes of different groups is $p_B$, where $0 < p_B < p_W < 1$. Let the total number of potential within-group pairs be $N_W = \\binom{n_A}{2} + \\binom{n_B}{2}$ and the total number of potential between-group pairs be $N_B = n_A n_B$. A link prediction training set is constructed as follows: all observed edges are used as positive examples, and negative examples are drawn uniformly at random from the set of all non-edges (without stratification by pair type).\n\nDefine the bias amplification factor $\\mathcal{A}$ as the ratio of the share of between-group pairs among the negative examples to the share of between-group pairs among the positive examples in this training set. Starting from first principles in network modeling and sampling, derive a closed-form expression for $\\mathcal{A}$ in terms of $n_A$, $n_B$, $p_W$, and $p_B$.\n\nAdditionally, propose a corrected negative sampling scheme that, in expectation, eliminates this amplification (that is, achieves $\\mathcal{A} = 1$), by specifying how to choose the relative selection probabilities for within-group versus between-group non-edges so that the negative class matches the positive class composition by pair type.\n\nExpress your final answer as the closed-form analytical expression for the amplification factor $\\mathcal{A}$. No rounding is required.",
            "solution": "The problem asks for the derivation of a bias amplification factor $\\mathcal{A}$ in a network with homophilic structure and a proposal for a corrected negative sampling scheme. The analysis will proceed from the first principles of the generative model provided. All calculations will be based on expected values over the distribution of graphs generated by this model.\n\nLet the set of nodes be $V$, with a partition into two groups $V_A$ and $V_B$ of sizes $n_A = |V_A|$ and $n_B = |V_B|$. The network is an instance of a Stochastic Block Model.\n\nThe total number of potential within-group pairs is $N_W = \\binom{n_A}{2} + \\binom{n_B}{2}$. The probability of an edge for any such pair is $p_W$.\nThe total number of potential between-group pairs is $N_B = n_A n_B$. The probability of an edge for any such pair is $p_B$. The problem states $0 < p_B < p_W < 1$.\n\nFirst, we calculate the expected number of edges (positive examples) for each type of pair.\nThe expected number of within-group edges, $L_W$, is the number of potential pairs multiplied by the probability of an edge forming:\n$$L_W = N_W p_W = \\left(\\binom{n_A}{2} + \\binom{n_B}{2}\\right) p_W$$\nThe expected number of between-group edges, $L_B$, is similarly:\n$$L_B = N_B p_B = (n_A n_B) p_B$$\nThe total expected number of positive examples (edges) is $L_{pos} = L_W + L_B$.\n\nThe share of between-group pairs among the positive examples, which we denote as $S_{pos, B}$, is the ratio of the expected number of between-group edges to the total expected number of edges:\n$$S_{pos, B} = \\frac{L_B}{L_{pos}} = \\frac{L_B}{L_W + L_B} = \\frac{n_A n_B p_B}{\\left(\\binom{n_A}{2} + \\binom{n_B}{2}\\right) p_W + n_A n_B p_B}$$\n\nNext, we calculate the expected number of non-edges for each type of pair. These form the pool from which negative examples are drawn.\nThe probability of a non-edge between a within-group pair is $1-p_W$. The expected number of within-group non-edges, $U_W$, is:\n$$U_W = N_W (1 - p_W) = \\left(\\binom{n_A}{2} + \\binom{n_B}{2}\\right) (1 - p_W)$$\nThe probability of a non-edge between a between-group pair is $1-p_B$. The expected number of between-group non-edges, $U_B$, is:\n$$U_B = N_B (1 - p_B) = n_A n_B (1 - p_B)$$\nThe total expected number of non-edges is $U_{tot} = U_W + U_B$.\n\nThe problem states that negative examples are drawn uniformly at random from the set of all non-edges. Therefore, the probability of a sampled negative example being a between-group pair is the ratio of the number of between-group non-edges to the total number of non-edges. The share of between-group pairs among the negative examples, $S_{neg, B}$, is:\n$$S_{neg, B} = \\frac{U_B}{U_{tot}} = \\frac{U_B}{U_W + U_B} = \\frac{n_A n_B (1 - p_B)}{\\left(\\binom{n_A}{2} + \\binom{n_B}{2}\\right) (1 - p_W) + n_A n_B (1 - p_B)}$$\n\nThe bias amplification factor, $\\mathcal{A}$, is defined as the ratio of these shares:\n$$\\mathcal{A} = \\frac{S_{neg, B}}{S_{pos, B}} = \\frac{\\frac{U_B}{U_W + U_B}}{\\frac{L_B}{L_W + L_B}}$$\nSubstituting the expressions for a more direct calculation:\n$$\\mathcal{A} = \\frac{U_B}{L_B} \\cdot \\frac{L_W + L_B}{U_W + U_B}$$\nNow, we substitute the formulas for $L_W, L_B, U_W, U_B$:\n$$\\mathcal{A} = \\frac{N_B (1 - p_B)}{N_B p_B} \\cdot \\frac{N_W p_W + N_B p_B}{N_W (1 - p_W) + N_B (1 - p_B)}$$\nThe $N_B$ terms in the first fraction cancel out:\n$$\\mathcal{A} = \\frac{1 - p_B}{p_B} \\cdot \\frac{N_W p_W + N_B p_B}{N_W (1 - p_W) + N_B (1 - p_B)}$$\nSubstituting the definitions of $N_W$ and $N_B$ yields the final closed-form expression for $\\mathcal{A}$:\n$$\\mathcal{A} = \\frac{1 - p_B}{p_B} \\cdot \\frac{\\left(\\binom{n_A}{2} + \\binom{n_B}{2}\\right) p_W + n_A n_B p_B}{\\left(\\binom{n_A}{2} + \\binom{n_B}{2}\\right) (1 - p_W) + n_A n_B (1 - p_B)}$$\nThe condition $p_W > p_B$ implies $p_W(1-p_B) > p_B(1-p_W)$, which ensures that $\\mathcal{A} > 1$, confirming the amplification of bias.\n\nFor the second part of the problem, we must propose a corrected negative sampling scheme to achieve $\\mathcal{A} = 1$. This requires the share of between-group pairs in the sampled negative set to be equal to their share in the positive set. Let the new share of between-group negative examples be $S'_{neg, B}$. We require $S'_{neg, B} = S_{pos, B}$.\n\nTo achieve this, we can use a stratified sampling approach where we sample non-edges with different probabilities depending on whether they are within-group or between-group. Let $w_W$ and $w_B$ be the relative selection probabilities (or weights) for sampling a single within-group non-edge and a single between-group non-edge, respectively.\n\nThe expected 'weight' of selected between-group non-edges will be proportional to $w_B U_B$, and for within-group non-edges, it will be proportional to $w_W U_W$. The corrected share of between-group pairs among the negative samples is:\n$$S'_{neg, B} = \\frac{w_B U_B}{w_W U_W + w_B U_B}$$\nWe set this equal to the share in the positive class, $S_{pos, B} = \\frac{L_B}{L_W + L_B}$:\n$$\\frac{w_B U_B}{w_W U_W + w_B U_B} = \\frac{L_B}{L_W + L_B}$$\nThis equality holds if the ratio of the components matches:\n$$\\frac{w_B U_B}{w_W U_W} = \\frac{L_B}{L_W}$$\nWe solve for the ratio of the relative selection probabilities, $\\frac{w_B}{w_W}$:\n$$\\frac{w_B}{w_W} = \\frac{L_B}{L_W} \\cdot \\frac{U_W}{U_B} = \\frac{N_B p_B}{N_W p_W} \\cdot \\frac{N_W(1 - p_W)}{N_B(1 - p_B)}$$\nThe terms $N_W$ and $N_B$ cancel, leading to a simple expression for the required ratio of probabilities:\n$$\\frac{w_B}{w_W} = \\frac{p_B(1 - p_W)}{p_W(1 - p_B)}$$\nTherefore, to eliminate the bias amplification (i.e., to achieve $\\mathcal{A} = 1$), one must sample between-group non-edges with a probability that is $\\frac{p_B(1 - p_W)}{p_W(1 - p_B)}$ times the probability of sampling a within-group non-edge. Since $p_W > p_B$, this ratio is less than $1$, meaning the corrected scheme must down-sample between-group non-edges relative to a uniform selection from all non-edges.",
            "answer": "$$\n\\boxed{\\frac{1-p_B}{p_B} \\frac{\\left(\\binom{n_A}{2} + \\binom{n_B}{2}\\right) p_W + n_A n_B p_B}{\\left(\\binom{n_A}{2} + \\binom{n_B}{2}\\right) (1-p_W) + n_A n_B (1-p_B)}}\n$$"
        }
    ]
}