{
    "hands_on_practices": [
        {
            "introduction": "Many network algorithms, from community detection to node embedding, rely on random walks to explore graph topology. Understanding the fundamental properties of this process is therefore critical. This exercise  guides you through a foundational derivation of the stationary distribution of a simple random walk, revealing an inherent and systematic bias towards high-degree nodes that has profound implications for algorithmic fairness.",
            "id": "4262519",
            "problem": "Consider a finite, simple, undirected, connected, and non-bipartite graph $G = (V, E)$ with $|V| = n$ nodes and $|E| = m$ edges. Let $A$ be its symmetric adjacency matrix, where $A_{ij} = 1$ if there is an edge between nodes $i$ and $j$, and $A_{ij} = 0$ otherwise. Let $k_i = \\sum_{j \\in V} A_{ij}$ denote the degree of node $i$. Define a discrete-time simple random walk on $G$ as a time-homogeneous Markov chain (MC) on the state space $V$ with transition probabilities $P_{ij} = A_{ij} / k_i$, meaning that at each step, the walk moves from its current node to a uniformly random neighbor.\n\nFrom the fundamental definitions of Markov chains on finite state spaces, a stationary distribution is a vector $\\pi = (\\pi_i)_{i \\in V}$ with $\\pi_i \\ge 0$ and $\\sum_{i \\in V} \\pi_i = 1$ that satisfies $\\pi^{\\top} P = \\pi^{\\top}$. For undirected graphs, the simple random walk is known to be reversible when a detailed balance condition holds, but you must derive any such condition you use starting from core definitions.\n\nUsing only these core definitions, derive a closed-form expression for the stationary probability $\\pi_i$ as a function of $k_i$ and $m$. Explain, based on your derivation, why long-run algorithms that sample nodes via this random walk are biased toward high-degree nodes compared to uniform node sampling. Your final reported answer must be the single closed-form expression for $\\pi_i$ in terms of $k_i$ and $m$. No numerical approximation is required and no units are involved.",
            "solution": "The problem asks for the derivation of the stationary distribution $\\pi = (\\pi_i)_{i \\in V}$ for a simple random walk on a finite, simple, undirected, connected, and non-bipartite graph $G=(V, E)$. The stationary distribution must satisfy the core definition $\\pi^{\\top} P = \\pi^{\\top}$, where $P$ is the transition matrix of the Markov chain. This matrix equation is equivalent to the system of linear equations for each state $i \\in V$:\n$$ \\pi_i = \\sum_{j \\in V} \\pi_j P_{ji} $$\nThe transition probabilities are given as $P_{ij} = A_{ij} / k_i$, where $A$ is the adjacency matrix and $k_i$ is the degree of node $i$. Since the graph is undirected, the adjacency matrix $A$ is symmetric, meaning $A_{ij} = A_{ji}$ for all pairs of nodes $(i, j)$. The transition probability from a node $j$ to a node $i$ is therefore $P_{ji} = A_{ji} / k_j$.\n\nLet us propose a candidate solution for the stationary probabilities of the form $\\pi_i = c \\cdot k_i$ for some normalization constant $c$. We must verify if this form satisfies the stationary condition. Substituting our proposed $\\pi_j = c \\cdot k_j$ and the definition of $P_{ji}$ into the right-hand side of the stationary equation:\n$$ \\sum_{j \\in V} \\pi_j P_{ji} = \\sum_{j \\in V} (c \\cdot k_j) \\left( \\frac{A_{ji}}{k_j} \\right) $$\nThe graph is connected, so for any node $i$, its degree $k_i > 0$. If $i$ were an isolated node, $k_i=0$, but then the graph would not be connected unless $|V|=1$. In a connected graph with more than one node, every node must have at least one edge, so $k_i \\ge 1$ for all $i$. Therefore, division by $k_j$ is well-defined. We can simplify the expression:\n$$ \\sum_{j \\in V} c \\cdot A_{ji} = c \\sum_{j \\in V} A_{ji} $$\nDue to the symmetry of the adjacency matrix for an undirected graph, $A_{ji} = A_{ij}$. The sum $\\sum_{j \\in V} A_{ij}$ is, by definition, the degree of node $i$, denoted as $k_i$. Therefore:\n$$ c \\sum_{j \\in V} A_{ji} = c \\sum_{j \\in V} A_{ij} = c \\cdot k_i $$\nThe left-hand side of the stationary equation is $\\pi_i$. Our proposed solution is $\\pi_i = c \\cdot k_i$. We have shown that the right-hand side, $\\sum_{j \\in V} \\pi_j P_{ji}$, evaluates to $c \\cdot k_i$. Since the left and right sides are equal, our proposed form $\\pi_i = c \\cdot k_i$ is a valid solution to the stationary equations.\n\nThe final step is to determine the value of the constant $c$ using the normalization condition that all probabilities must sum to $1$:\n$$ \\sum_{i \\in V} \\pi_i = 1 $$\nSubstituting our solution for $\\pi_i$:\n$$ \\sum_{i \\in V} c \\cdot k_i = 1 $$\n$$ c \\sum_{i \\in V} k_i = 1 $$\nThe sum of the degrees of all nodes in a graph is related to the total number of edges, $m = |E|$, by the handshaking lemma: $\\sum_{i \\in V} k_i = 2m$. Substituting this into our equation for $c$:\n$$ c \\cdot (2m) = 1 $$\n$$ c = \\frac{1}{2m} $$\nNow we substitute this constant back into our expression for $\\pi_i$:\n$$ \\pi_i = \\frac{k_i}{2m} $$\nThis is the closed-form expression for the stationary probability of being at node $i$.\n\nThe problem also asks for an explanation of the bias of this sampling method. The derived stationary probability $\\pi_i = k_i / (2m)$ shows that the long-run probability of a simple random walk being at a particular node $i$ is directly proportional to its degree $k_i$. In contrast, uniform node sampling would assign an equal probability of $1/|V| = 1/n$ to each node. For any non-regular graph, the degrees $k_i$ are not all equal. Algorithms that sample nodes based on a long random walk will therefore not sample nodes uniformly. Instead, they will sample nodes with a probability proportional to their degree. This creates a \"degree bias,\" where high-degree nodes (hubs) are visited more frequently and are thus over-sampled compared to a uniform distribution, while low-degree nodes are under-sampled. This property is fundamental to many network algorithms, such as PageRank, and also a key source of algorithmic bias when analyzing sampled network data.",
            "answer": "$$\\boxed{\\frac{k_i}{2m}}$$"
        },
        {
            "introduction": "The degree bias uncovered in random walks directly translates into biases within algorithms built upon them, such as centrality measures. This practice  makes this connection concrete by exploring Katz centrality, a sophisticated measure of influence based on counting weighted walks. By analyzing its behavior on a simple star graph, you will derive how a key parameter—the damping factor $\\alpha$—acts as a tuning knob for controlling the algorithmic preference for high-degree hubs.",
            "id": "4262489",
            "problem": "Consider undirected, simple networks represented by an adjacency matrix (AM) $A \\in \\mathbb{R}^{n \\times n}$ on $n$ nodes, where $A_{ij} = 1$ if nodes $i$ and $j$ are connected and $A_{ij} = 0$ otherwise. Eigenvector centrality (EC) is defined by the fixed-point relation that a node's centrality is proportional to the sum of its neighbors' centralities. Walk-counting centrality assigns influence according to all walks emanating from each node, weighted by a factor that decays with walk length. Algorithmic bias in network analysis arises when algorithmic design choices (e.g., length-weighting of walks) systematically privilege certain structures, such as hubs.\n\nStarting from the walk-counting perspective and using only foundational facts about linear systems on networks, construct a damped walk-based centrality (Katz centrality) that incorporates a nonnegative damping parameter $\\alpha$ multiplying the adjacency contributions at each step and an exogenous baseline vector $b \\in \\mathbb{R}^{n}$ with all entries equal to $1$. Assume the series defining the centrality converges when the damping parameter satisfies a spectral constraint tied to $A$. Then, apply your construction to the undirected star graph $K_{1,m}$ with one hub node connected to $m \\geq 2$ leaf nodes and no other edges. By symmetry, let the hub centrality be $h$ and every leaf centrality be $l$.\n\nDerive the closed-form expressions for $h$ and $l$ in terms of $\\alpha$ and $m$ from first principles, and from these derive the hub-to-leaf centrality ratio as a function of $\\alpha$ and $m$. The final answer must be a single closed-form analytic expression giving this ratio. State any necessary constraint on $\\alpha$ to ensure convergence, but do not include inequalities in the final boxed answer. No rounding is required.",
            "solution": "The problem requires the derivation of the hub-to-leaf centrality ratio for a star graph $K_{1,m}$ using a specific form of walk-based centrality, known as Katz centrality. The derivation must proceed from first principles.\n\nFirst, we formalize the concept of the walk-based centrality described. The problem states that a node's centrality is derived from all walks emanating from it, with a decay factor for length. This can be constructed as a recursive relationship. Let $c \\in \\mathbb{R}^{n}$ be the vector of centralities for the $n$ nodes in the network. The problem specifies a construction that adds a baseline contribution $b \\in \\mathbb{R}^{n}$ (with all entries equal to $1$) to a term proportional to the sum of neighbors' centralities. This leads to the defining equation for Katz centrality:\n$$c = \\alpha A^T c + b$$\nwhere $A$ is the adjacency matrix, $\\alpha$ is a non-negative damping parameter, and $b$ is the vector of exogenous baseline contributions. For an undirected graph, the adjacency matrix is symmetric, $A = A^T$, so the equation simplifies to:\n$$c = \\alpha A c + b$$\nThis equation can be rearranged to solve for $c$:\n$$(I - \\alpha A) c = b$$\n$$c = (I - \\alpha A)^{-1} b$$\nThe matrix inverse $(I - \\alpha A)^{-1}$ can be expressed as a Neumann series:\n$$(I - \\alpha A)^{-1} = \\sum_{k=0}^{\\infty} (\\alpha A)^k = I + \\alpha A + \\alpha^2 A^2 + \\alpha^3 A^3 + \\dots$$\nThis series converges if and only if the spectral radius of the matrix $\\alpha A$, denoted $\\rho(\\alpha A)$, is less than $1$. The spectral radius is the largest absolute value among the eigenvalues of the matrix. Since $\\rho(\\alpha A) = |\\alpha| \\rho(A)$ and the problem states $\\alpha$ is non-negative, the condition for convergence is $\\alpha \\rho(A) < 1$.\n\nSubstituting the series expansion back into the expression for $c$ yields:\n$$c = \\left( \\sum_{k=0}^{\\infty} \\alpha^k A^k \\right) b$$\nThe term $(A^k)_{ij}$ of the matrix $A^k$ counts the number of walks of length $k$ from node $j$ to node $i$. Since all entries of $b$ are $1$, the $i$-th component of the vector $A^k b$ is $\\sum_{j=1}^{n} (A^k)_{ij}$, which is the total number of walks of length $k$ ending at node $i$ from any node in the network. Thus, the centrality $c_i$ is a sum of these walk counts, weighted by $\\alpha^k$:\n$$c_i = \\sum_{k=0}^{\\infty} \\alpha^k \\sum_{j=1}^{n} (A^k)_{ij}$$\nThis confirms the construction is consistent with the walk-counting perspective. For our derivation, it is algebraically simpler to work with the linear system $c = \\alpha A c + b$.\n\nWe now apply this to the undirected star graph $K_{1,m}$, which has $n = m+1$ nodes, for $m \\ge 2$. Let the central hub be node $0$ and the $m$ peripheral leaf nodes be nodes $1, 2, \\dots, m$. By the symmetry of the graph, all leaf nodes must have the same centrality. Let $h$ be the centrality of the hub node and $l$ be the centrality of any leaf node. The centrality vector is $c = (h, l, l, \\dots, l)^T$. The baseline vector is $b = (1, 1, \\dots, 1)^T$.\n\nThe system of equations $c = \\alpha A c + b$ can be written for the hub node and for a representative leaf node.\n\nFor the hub node (node $0$):\nThe hub is connected to all $m$ leaves. The equation for $c_0 = h$ is:\n$$h = \\alpha \\sum_{j=1}^{n} A_{0j} c_j + b_0$$\nThe neighbors of node $0$ are nodes $1, \\dots, m$. So, $A_{0j}=1$ for $j \\in \\{1, \\dots, m\\}$ and $A_{00}=0$. The centralities of these neighbors are all $l$. The baseline contribution is $b_0=1$.\n$$h = \\alpha \\left( \\sum_{j=1}^{m} (1 \\cdot c_j) \\right) + 1 = \\alpha \\left( \\sum_{j=1}^{m} l \\right) + 1$$\n$$h = \\alpha m l + 1 \\quad (1)$$\n\nFor any leaf node (e.g., node $i$, where $i \\in \\{1, \\dots, m\\}$):\nA leaf node is connected only to the hub. The equation for $c_i = l$ is:\n$$l = \\alpha \\sum_{j=0}^{n-1} A_{ij} c_j + b_i$$\nThe only neighbor of node $i$ is node $0$. So, $A_{i0}=1$ and $A_{ij}=0$ for $j \\in \\{1, \\dots, m\\}$. The centrality of the hub is $h$. The baseline contribution is $b_i=1$.\n$$l = \\alpha (A_{i0} c_0) + 1 = \\alpha (1 \\cdot h) + 1$$\n$$l = \\alpha h + 1 \\quad (2)$$\n\nWe now have a system of two linear equations for $h$ and $l$:\n1. $h = \\alpha m l + 1$\n2. $l = \\alpha h + 1$\n\nTo solve this system, we substitute the expression for $l$ from equation $(2)$ into equation $(1)$:\n$$h = \\alpha m (\\alpha h + 1) + 1$$\n$$h = \\alpha^2 m h + \\alpha m + 1$$\nNow, we solve for $h$:\n$$h - \\alpha^2 m h = \\alpha m + 1$$\n$$h(1 - \\alpha^2 m) = \\alpha m + 1$$\n$$h = \\frac{\\alpha m + 1}{1 - \\alpha^2 m}$$\nNext, we substitute this expression for $h$ back into equation $(2)$ to find $l$:\n$$l = \\alpha \\left( \\frac{\\alpha m + 1}{1 - \\alpha^2 m} \\right) + 1$$\n$$l = \\frac{\\alpha(\\alpha m + 1)}{1 - \\alpha^2 m} + \\frac{1 - \\alpha^2 m}{1 - \\alpha^2 m}$$\n$$l = \\frac{\\alpha^2 m + \\alpha + 1 - \\alpha^2 m}{1 - \\alpha^2 m}$$\n$$l = \\frac{\\alpha + 1}{1 - \\alpha^2 m}$$\nThe problem requires the hub-to-leaf centrality ratio, $h/l$. Using the derived expressions:\n$$\\frac{h}{l} = \\frac{\\frac{\\alpha m + 1}{1 - \\alpha^2 m}}{\\frac{\\alpha + 1}{1 - \\alpha^2 m}}$$\nThe denominator $(1 - \\alpha^2 m)$ is non-zero due to the convergence condition, so it cancels out:\n$$\\frac{h}{l} = \\frac{\\alpha m + 1}{\\alpha + 1}$$\n\nFinally, we must state the necessary constraint on $\\alpha$ for these solutions to be valid. The convergence of the underlying series requires $\\alpha < 1/\\rho(A)$, where $\\rho(A)$ is the spectral radius of the adjacency matrix for $K_{1,m}$. The eigenvalues $\\lambda$ of $A$ for the star graph are the roots of the characteristic polynomial. A standard result for the star graph $K_{1,m}$ is that its spectrum consists of eigenvalues $\\pm\\sqrt{m}$ (each with multiplicity $1$) and $0$ (with multiplicity $m-1$). The largest absolute value of these eigenvalues is $\\sqrt{m}$. Therefore, the spectral radius is $\\rho(A) = \\sqrt{m}$.\nThe convergence condition is $\\alpha \\sqrt{m} < 1$, or $\\alpha < 1/\\sqrt{m}$. Combined with the non-negativity of $\\alpha$, the full constraint is $0 \\le \\alpha < 1/\\sqrt{m}$. This condition also ensures that the denominator $1 - \\alpha^2 m$ in the expressions for $h$ and $l$ is positive, guaranteeing positive centrality values.\n\nThe final closed-form analytic expression for the hub-to-leaf centrality ratio is thus derived.",
            "answer": "$$\\boxed{\\frac{\\alpha m + 1}{\\alpha + 1}}$$"
        },
        {
            "introduction": "Moving from theoretical analysis to practical application, this final exercise  challenges you to confront algorithmic bias in a computational setting. You will implement diffusion-based node classification, a common semi-supervised learning task, and quantify performance disparities between majority and minority groups. The core of this practice involves implementing and evaluating two prominent bias mitigation strategies, providing hands-on experience in both diagnosing and actively treating algorithmic unfairness on networks.",
            "id": "4262509",
            "problem": "You are given the task of quantifying and interpreting algorithmic bias in node classification on networks via diffusion-based propagation from labeled seeds. Consider an undirected simple graph with $n$ nodes, represented by an adjacency matrix $A \\in \\mathbb{R}^{n \\times n}$, where $A_{ij} \\in \\{0,1\\}$ and $A$ is symmetric with zero diagonal. Let $D$ be the diagonal degree matrix with entries $D_{ii} = \\sum_{j=1}^{n} A_{ij}$. Define the row-stochastic random-walk transition matrix $P = D^{-1}A$, where rows corresponding to nodes with zero degree are taken to be all zeros.\n\nClassification is performed via diffusion from labeled seeds. Let $C$ denote the number of classes (here $C=2$), and let $Y \\in \\{0,1\\}^{n}$ be the ground-truth class label vector with $Y_i \\in \\{0,1\\}$ for node $i$. Let $S$ denote the set of seed nodes whose one-hot labels are known. Define the seed label matrix $Y^{\\text{seed}} \\in \\{0,1\\}^{n \\times C}$ such that $(Y^{\\text{seed}})_{i,c} = 1$ if $i \\in S$ and $Y_i = c$, and $(Y^{\\text{seed}})_{i,c} = 0$ otherwise. Let $M \\in \\{0,1\\}^{n}$ be the binary minority membership indicator, where $M_i = 1$ indicates node $i$ belongs to the minority group and $M_i = 0$ indicates majority.\n\nYou must compute three prediction variants:\n- Baseline diffusion: Using a diffusion operator $S_{\\text{base}} = P^k$ for a given integer $k \\geq 1$, the predicted class scores are $\\hat{Y}^{\\text{base}} = S_{\\text{base}} \\, Y^{\\text{seed}}$ and the predicted class is $\\arg\\max$ over class scores for each node. Ties must be resolved by choosing the smallest class index.\n- Edge reweighting intervention: Construct a reweighted adjacency $A'$ by scaling edges based on group memberships. For $i \\neq j$, define\n$$\nA'_{ij} =\n\\begin{cases}\nw_{\\text{MM}} \\cdot A_{ij} & \\text{if } M_i = 0 \\text{ and } M_j = 0, \\\\\nw_{\\text{mm}} \\cdot A_{ij} & \\text{if } M_i = 1 \\text{ and } M_j = 1, \\\\\nw_{\\text{mM}} \\cdot A_{ij} & \\text{if } M_i \\neq M_j,\n\\end{cases}\n$$\nand set $A'_{ii} = 0$. Then define $P' = (D')^{-1} A'$ with $D'$ the degree matrix of $A'$, and $\\hat{Y}^{\\text{rew}} = (P')^k Y^{\\text{seed}}$; predict via $\\arg\\max$.\n- Personalized PageRank (PPR) intervention: Define the Personalized PageRank (PPR) diffusion operator as\n$$\nS_{\\text{ppr}} = (1 - \\alpha) \\sum_{t=0}^{T} \\alpha^t P^t,\n$$\nwith restart parameter $\\alpha \\in (0,1)$ and truncation horizon $T \\in \\mathbb{N}$. The predicted scores are $\\hat{Y}^{\\text{ppr}} = S_{\\text{ppr}} \\, Y^{\\text{seed}}$; predict via $\\arg\\max$.\n\nEvaluation protocol:\n- Let $E$ be a set of evaluation nodes disjoint from $S$.\n- Compute the minority accuracy as the fraction (expressed as a decimal) of correct predictions among nodes in $E$ with $M_i=1$.\n- Compute the majority accuracy as the fraction (expressed as a decimal) of correct predictions among nodes in $E$ with $M_i=0$.\n- Define the bias score as $\\text{bias} = \\text{accuracy}_{\\text{majority}} - \\text{accuracy}_{\\text{minority}}$.\n\nFor each test case, compute:\n- The change in minority accuracy after edge reweighting: $\\Delta_{\\text{min}}^{\\text{rew}} = \\text{accuracy}_{\\text{minority}}^{\\text{rew}} - \\text{accuracy}_{\\text{minority}}^{\\text{base}}$.\n- The change in minority accuracy after PPR: $\\Delta_{\\text{min}}^{\\text{ppr}} = \\text{accuracy}_{\\text{minority}}^{\\text{ppr}} - \\text{accuracy}_{\\text{minority}}^{\\text{base}}$.\n- The change in bias after edge reweighting: $\\Delta_{\\text{bias}}^{\\text{rew}} = \\text{bias}^{\\text{rew}} - \\text{bias}^{\\text{base}}$.\n- The change in bias after PPR: $\\Delta_{\\text{bias}}^{\\text{ppr}} = \\text{bias}^{\\text{ppr}} - \\text{bias}^{\\text{base}}$.\n\nYour program must implement the above computation for the following test suite. For each case, all arrays and parameters are specified exactly.\n\nTest Case $1$ (happy path with majority homophily and underrepresented minority):\n- $n = 8$, $C = 2$, $k = 2$, $\\alpha = 0.85$, $T = 20$.\n- Minority indicator $M = [0,0,0,0,0,0,1,1]$.\n- Ground-truth labels $Y = [0,0,1,1,0,1,1,1]$.\n- Seed set $S = \\{0,1,4,6\\}$.\n- Evaluation set $E = \\{2,3,5,7\\}$.\n- Adjacency $A$:\n$$\nA = \\begin{bmatrix}\n0 & 1 & 1 & 0 & 0 & 0 & 0 & 0 \\\\\n1 & 0 & 1 & 1 & 0 & 0 & 0 & 0 \\\\\n1 & 1 & 0 & 1 & 1 & 0 & 0 & 0 \\\\\n0 & 1 & 1 & 0 & 1 & 1 & 1 & 0 \\\\\n0 & 0 & 1 & 1 & 0 & 1 & 1 & 1 \\\\\n0 & 0 & 0 & 1 & 1 & 0 & 0 & 1 \\\\\n0 & 0 & 0 & 1 & 1 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 1 & 1 & 0 & 0\n\\end{bmatrix}.\n$$\n- Reweighting parameters: $w_{\\text{MM}} = 0.7$, $w_{\\text{mM}} = 1.3$, $w_{\\text{mm}} = 1.6$.\n\nTest Case $2$ (balanced, well-mixed ring; no reweight change):\n- $n = 6$, $C = 2$, $k = 3$, $\\alpha = 0.85$, $T = 20$.\n- Minority indicator $M = [0,0,0,1,1,1]$.\n- Ground-truth labels $Y = [0,1,0,1,0,1]$.\n- Seed set $S = \\{1,4\\}$.\n- Evaluation set $E = \\{0,2,3,5\\}$.\n- Adjacency $A$ (ring):\n$$\nA = \\begin{bmatrix}\n0 & 1 & 0 & 0 & 0 & 1 \\\\\n1 & 0 & 1 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1 & 0 & 1 \\\\\n1 & 0 & 0 & 0 & 1 & 0\n\\end{bmatrix}.\n$$\n- Reweighting parameters: $w_{\\text{MM}} = 1.0$, $w_{\\text{mM}} = 1.0$, $w_{\\text{mm}} = 1.0$.\n\nTest Case $3$ (edge case with an isolated minority node):\n- $n = 7$, $C = 2$, $k = 2$, $\\alpha = 0.85$, $T = 20$.\n- Minority indicator $M = [0,0,0,0,0,1,1]$.\n- Ground-truth labels $Y = [0,0,0,1,1,1,1]$.\n- Seed set $S = \\{0,3\\}$.\n- Evaluation set $E = \\{1,2,4,5,6\\}$.\n- Adjacency $A$:\n$$\nA = \\begin{bmatrix}\n0 & 1 & 0 & 0 & 0 & 0 & 0 \\\\\n1 & 0 & 1 & 0 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 1 & 0 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & 1 & 0 & 0 \\\\\n0 & 0 & 0 & 1 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 0 & 1 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0\n\\end{bmatrix}.\n$$\n- Reweighting parameters: $w_{\\text{MM}} = 0.6$, $w_{\\text{mM}} = 1.5$, $w_{\\text{mm}} = 1.8$.\n\nOutput specification:\n- For each test case, compute the vector of four values $[\\Delta_{\\text{min}}^{\\text{rew}}, \\Delta_{\\text{min}}^{\\text{ppr}}, \\Delta_{\\text{bias}}^{\\text{rew}}, \\Delta_{\\text{bias}}^{\\text{ppr}}]$ as real numbers (decimals).\n- Your program should produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets, where each entry is the list of four values for one test case in order from Test Case $1$ to Test Case $3$. For example, the format must be like $[[a_1,a_2,a_3,a_4],[b_1,b_2,b_3,b_4],[c_1,c_2,c_3,c_4]]$ with numerals printed in decimal form. No other text should be printed.",
            "solution": "The problem is valid as it is scientifically grounded in network science, well-posed with all necessary data and unambiguous definitions, and objective in its formulation. All parameters, matrices, and evaluation criteria are explicitly provided, allowing for a unique and verifiable solution. The problem statement handles edge cases, such as nodes with zero degree, by specifying that the corresponding rows in the transition matrix $P$ should be zero vectors. This ensures that the matrix $P=D^{-1}A$ is well-defined even when $D$ is not invertible. I will now proceed with the step-by-step solution.\n\nThe core of the problem is to compute predictions from three different graph-based diffusion models and evaluate their performance and bias with respect to a protected attribute (minority/majority group membership).\n\nLet's first formalize the calculation steps for a single test case.\nGiven: an adjacency matrix $A \\in \\{0,1\\}^{n \\times n}$, ground-truth labels $Y \\in \\{0,1\\}^n$, minority indicator $M \\in \\{0,1\\}^n$, seed set $S$, evaluation set $E$, and parameters $k, \\alpha, T,$ and reweighting factors $w_{\\text{MM}}, w_{\\text{mM}}, w_{\\text{mm}}$. The number of classes is $C=2$.\n\nFirst, we construct the seed label matrix $Y^{\\text{seed}} \\in \\{0,1\\}^{n \\times C}$. This matrix is initialized to all zeros. Then, for each seed node $i \\in S$, we set $(Y^{\\text{seed}})_{i,c} = 1$ where $c = Y_i$ is the true label of node $i$.\n\nNext, we define a common procedure to evaluate the predictions from any diffusion model. Let $\\hat{Y} \\in \\mathbb{R}^{n \\times C}$ be the matrix of predicted scores. The predicted class for node $i$ is $\\hat{y}_i = \\arg\\max_{c \\in \\{0, \\dots, C-1\\}} \\hat{Y}_{i,c}$. The problem states that ties are resolved by choosing the smallest class index, which is the default behavior of `argmax` operations in numerical libraries.\n\nTo evaluate, we consider only the nodes in the evaluation set $E$. For these nodes, we compute:\n- Majority accuracy: The fraction of correctly predicted labels for nodes $i \\in E$ where $M_i = 0$.\n$$ \\text{accuracy}_{\\text{majority}} = \\frac{|\\{ i \\in E \\mid M_i=0 \\text{ and } \\hat{y}_i=Y_i \\}|}{|\\{ i \\in E \\mid M_i=0 \\}|} $$\n- Minority accuracy: The fraction of correctly predicted labels for nodes $i \\in E$ where $M_i = 1$.\n$$ \\text{accuracy}_{\\text{minority}} = \\frac{|\\{ i \\in E \\mid M_i=1 \\text{ and } \\hat{y}_i=Y_i \\}|}{|\\{ i \\in E \\mid M_i=1 \\}|} $$\nThe denominators are guaranteed to be non-zero for the given test cases. The bias score is then calculated as $\\text{bias} = \\text{accuracy}_{\\text{majority}} - \\text{accuracy}_{\\text{minority}}$.\n\nWe now describe the three diffusion models and their corresponding score matrices.\n\n**1. Baseline Diffusion**\nThe baseline model relies on the random-walk transition matrix $P = D^{-1}A$. First, we compute the degree matrix $D$, which is a diagonal matrix with $D_{ii} = \\sum_j A_{ij}$. To handle nodes with degree zero, for which $D_{ii}=0$, we define $D^{-1}_{ii}$ to be $0$ if $D_{ii}=0$ and $1/D_{ii}$ otherwise. This ensures the corresponding row in $P$ is a zero vector.\nThe diffusion operator is $S_{\\text{base}} = P^k$, calculated using matrix exponentiation.\nThe predicted scores are $\\hat{Y}^{\\text{base}} = S_{\\text{base}} Y^{\\text{seed}}$.\nFrom these scores, we compute $\\text{accuracy}_{\\text{majority}}^{\\text{base}}$, $\\text{accuracy}_{\\text{minority}}^{\\text{base}}$, and $\\text{bias}^{\\text{base}}$.\n\n**2. Edge Reweighting Intervention**\nThis model first constructs a reweighted adjacency matrix $A'$. For each pair of distinct nodes $(i,j)$, the weight $A'_{ij}$ is determined by the group memberships of $i$ and $j$:\n$$\nA'_{ij} = A_{ij} \\times\n\\begin{cases}\nw_{\\text{MM}} & \\text{if } M_i = 0, M_j = 0 \\\\\nw_{\\text{mm}} & \\text{if } M_i = 1, M_j = 1 \\\\\nw_{\\text{mM}} & \\text{if } M_i \\neq M_j\n\\end{cases}\n$$\n$A'_{ii}$ remains $0$. From $A'$, we compute its degree matrix $D'$ and transition matrix $P' = (D')^{-1}A'$, again handling zero-degree nodes as before.\nThe diffusion operator is $S_{\\text{rew}} = (P')^k$.\nThe predicted scores are $\\hat{Y}^{\\text{rew}} = S_{\\text{rew}} Y^{\\text seed}$.\nFrom these, we compute $\\text{accuracy}_{\\text{majority}}^{\\text{rew}}$, $\\text{accuracy}_{\\text{minority}}^{\\text{rew}}$, and $\\text{bias}^{\\text{rew}}$.\n\n**3. Personalized PageRank (PPR) Intervention**\nThis model uses the original transition matrix $P$ but a different diffusion operator based on Personalized PageRank:\n$$ S_{\\text{ppr}} = (1 - \\alpha) \\sum_{t=0}^{T} \\alpha^t P^t $$\nwhere $\\alpha$ is the restart parameter and $T$ is the truncation horizon. This sum can be computed iteratively. Let $P^0 = I$ (identity matrix).\nThe predicted scores are $\\hat{Y}^{\\text{ppr}} = S_{\\text{ppr}} Y^{\\text{seed}}$.\nFrom these, we compute $\\text{accuracy}_{\\text{majority}}^{\\text{ppr}}$, $\\text{accuracy}_{\\text{minority}}^{\\text{ppr}}$, and $\\text{bias}^{\\text{ppr}}$.\n\n**Final Computation**\nFor each test case, after obtaining the accuracies and biases for the three models, we compute the four required delta values:\n- $\\Delta_{\\text{min}}^{\\text{rew}} = \\text{accuracy}_{\\text{minority}}^{\\text{rew}} - \\text{accuracy}_{\\text{minority}}^{\\text{base}}$\n- $\\Delta_{\\text{min}}^{\\text{ppr}} = \\text{accuracy}_{\\text{minority}}^{\\text{ppr}} - \\text{accuracy}_{\\text{minority}}^{\\text{base}}$\n- $\\Delta_{\\text{bias}}^{\\text{rew}} = \\text{bias}^{\\text{rew}} - \\text{bias}^{\\text{base}} = (\\text{accuracy}_{\\text{majority}}^{\\text{rew}} - \\text{accuracy}_{\\text{minority}}^{\\text{rew}}) - (\\text{accuracy}_{\\text{majority}}^{\\text{base}} - \\text{accuracy}_{\\text{minority}}^{\\text{base}})$\n- $\\Delta_{\\text{bias}}^{\\text{ppr}} = \\text{bias}^{\\text{ppr}} - \\text{bias}^{\\text{base}} = (\\text{accuracy}_{\\text{majority}}^{\\text{ppr}} - \\text{accuracy}_{\\text{minority}}^{\\text{ppr}}) - (\\text{accuracy}_{\\text{majority}}^{\\text{base}} - \\text{accuracy}_{\\text{minority}}^{\\text{base}})$\n\nThis process is repeated for each of the three test cases provided. The implementation will rely on `numpy` for efficient matrix operations.",
            "answer": "```python\nimport numpy as np\n\ndef get_transition_matrix(A):\n    \"\"\"Computes the random-walk transition matrix P = D^-1 * A.\"\"\"\n    n = A.shape[0]\n    degrees = np.sum(A, axis=1)\n    inv_degrees = np.zeros(n)\n    non_zero_mask = degrees > 0\n    inv_degrees[non_zero_mask] = 1.0 / degrees[non_zero_mask]\n    D_inv = np.diag(inv_degrees)\n    P = D_inv @ A\n    return P\n\ndef get_accuracies_and_bias(S_diff, Y_seed, Y_true, M, E_nodes):\n    \"\"\"Computes accuracies and bias for a given diffusion operator.\"\"\"\n    Y_scores = S_diff @ Y_seed\n    Y_pred = np.argmax(Y_scores, axis=1)\n\n    eval_nodes = np.array(list(E_nodes))\n    eval_pred = Y_pred[eval_nodes]\n    eval_true = Y_true[eval_nodes]\n    eval_minority_membership = M[eval_nodes]\n\n    minority_mask = (eval_minority_membership == 1)\n    majority_mask = (eval_minority_membership == 0)\n\n    num_minority = np.sum(minority_mask)\n    num_majority = np.sum(majority_mask)\n\n    if num_minority > 0:\n        minority_correct = np.sum(eval_pred[minority_mask] == eval_true[minority_mask])\n        acc_minority = minority_correct / num_minority\n    else:\n        acc_minority = 0.0\n\n    if num_majority > 0:\n        majority_correct = np.sum(eval_pred[majority_mask] == eval_true[majority_mask])\n        acc_majority = majority_correct / num_majority\n    else:\n        acc_majority = 0.0\n        \n    bias = acc_majority - acc_minority\n    \n    return acc_minority, acc_majority, bias\n\ndef process_case(A, Y, M, S, E, k, alpha, T, C, weights):\n    \"\"\"Processes a single test case.\"\"\"\n    n = A.shape[0]\n    w_MM, w_mM, w_mm = weights['w_MM'], weights['w_mM'], weights['w_mm']\n\n    # Construct seed label matrix\n    Y_seed = np.zeros((n, C))\n    for i in S:\n        label = Y[i]\n        Y_seed[i, label] = 1.0\n\n    # 1. Baseline diffusion\n    P = get_transition_matrix(A)\n    S_base = np.linalg.matrix_power(P, k)\n    acc_min_base, acc_maj_base, bias_base = get_accuracies_and_bias(S_base, Y_seed, Y, M, E)\n\n    # 2. Edge reweighting intervention\n    A_rew = np.zeros_like(A, dtype=float)\n    M_col = M.reshape(-1, 1)\n    M_row = M.reshape(1, -1)\n    \n    A_rew += A * (M_col == 0) * (M_row == 0) * w_MM\n    A_rew += A * (M_col == 1) * (M_row == 1) * w_mm\n    A_rew += A * (M_col != M_row) * w_mM\n    \n    P_rew = get_transition_matrix(A_rew)\n    S_rew = np.linalg.matrix_power(P_rew, k)\n    acc_min_rew, acc_maj_rew, bias_rew = get_accuracies_and_bias(S_rew, Y_seed, Y, M, E)\n\n    # 3. Personalized PageRank intervention\n    S_ppr_sum = np.zeros((n, n))\n    P_pow_t = np.eye(n)\n    for t in range(T + 1):\n        S_ppr_sum += (alpha**t) * P_pow_t\n        if t < T:\n            P_pow_t = P_pow_t @ P\n    S_ppr = (1 - alpha) * S_ppr_sum\n    acc_min_ppr, acc_maj_ppr, bias_ppr = get_accuracies_and_bias(S_ppr, Y_seed, Y, M, E)\n\n    # Calculate final deltas\n    delta_min_rew = acc_min_rew - acc_min_base\n    delta_min_ppr = acc_min_ppr - acc_min_base\n    delta_bias_rew = bias_rew - bias_base\n    delta_bias_ppr = bias_ppr - bias_base\n    \n    return [delta_min_rew, delta_min_ppr, delta_bias_rew, delta_bias_ppr]\n\ndef solve():\n    test_cases = [\n        {\n            \"A\": np.array([\n                [0, 1, 1, 0, 0, 0, 0, 0], [1, 0, 1, 1, 0, 0, 0, 0], [1, 1, 0, 1, 1, 0, 0, 0],\n                [0, 1, 1, 0, 1, 1, 1, 0], [0, 0, 1, 1, 0, 1, 1, 1], [0, 0, 0, 1, 1, 0, 0, 1],\n                [0, 0, 0, 1, 1, 0, 0, 0], [0, 0, 0, 0, 1, 1, 0, 0]\n            ]),\n            \"Y\": np.array([0, 0, 1, 1, 0, 1, 1, 1]),\n            \"M\": np.array([0, 0, 0, 0, 0, 0, 1, 1]),\n            \"S\": {0, 1, 4, 6}, \"E\": {2, 3, 5, 7}, \"k\": 2, \"alpha\": 0.85, \"T\": 20, \"C\": 2,\n            \"weights\": {\"w_MM\": 0.7, \"w_mM\": 1.3, \"w_mm\": 1.6}\n        },\n        {\n            \"A\": np.array([\n                [0, 1, 0, 0, 0, 1], [1, 0, 1, 0, 0, 0], [0, 1, 0, 1, 0, 0],\n                [0, 0, 1, 0, 1, 0], [0, 0, 0, 1, 0, 1], [1, 0, 0, 0, 1, 0]\n            ]),\n            \"Y\": np.array([0, 1, 0, 1, 0, 1]),\n            \"M\": np.array([0, 0, 0, 1, 1, 1]),\n            \"S\": {1, 4}, \"E\": {0, 2, 3, 5}, \"k\": 3, \"alpha\": 0.85, \"T\": 20, \"C\": 2,\n            \"weights\": {\"w_MM\": 1.0, \"w_mM\": 1.0, \"w_mm\": 1.0}\n        },\n        {\n            \"A\": np.array([\n                [0, 1, 0, 0, 0, 0, 0], [1, 0, 1, 0, 0, 0, 0], [0, 1, 0, 1, 0, 0, 0],\n                [0, 0, 1, 0, 1, 0, 0], [0, 0, 0, 1, 0, 1, 0], [0, 0, 0, 0, 1, 0, 0],\n                [0, 0, 0, 0, 0, 0, 0]\n            ]),\n            \"Y\": np.array([0, 0, 0, 1, 1, 1, 1]),\n            \"M\": np.array([0, 0, 0, 0, 0, 1, 1]),\n            \"S\": {0, 3}, \"E\": {1, 2, 4, 5, 6}, \"k\": 2, \"alpha\": 0.85, \"T\": 20, \"C\": 2,\n            \"weights\": {\"w_MM\": 0.6, \"w_mM\": 1.5, \"w_mm\": 1.8}\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = process_case(case[\"A\"], case[\"Y\"], case[\"M\"], case[\"S\"], case[\"E\"], case[\"k\"], case[\"alpha\"], case[\"T\"], case[\"C\"], case[\"weights\"])\n        results.append(result)\n\n    result_str_list = []\n    for res_list in results:\n        res_str = '[' + ','.join(map(str, res_list)) + ']'\n        result_str_list.append(res_str)\n    \n    print(f\"[{','.join(result_str_list)}]\")\n\nsolve()\n```"
        }
    ]
}