{
    "hands_on_practices": [
        {
            "introduction": "The most intuitive way to understand a generative model like the Hawkes process is to build a simulator from first principles. This exercise guides you through implementing a sampler based on the branching, or \"cluster,\" process construction. By simulating the arrival of \"immigrant\" events and their subsequent \"offspring,\" you will gain a deep, procedural understanding of how network interactions and self-excitation give rise to complex temporal patterns. ",
            "id": "4280121",
            "problem": "You are asked to formalize the branching construction sampler for a multivariate Hawkes process on a directed network and implement it as a complete, runnable program. Begin from first principles in stochastic point processes and derive the sampling strategy from the core definitions. Then, implement the sampling algorithm for a specific parametric family of kernels and produce outputs for a small test suite of parameter settings.\n\nA multivariate Hawkes process on $n$ nodes is defined by counting processes $\\{N_i(t)\\}_{i=1}^n$ with conditional intensities\n$$\n\\lambda_i(t) = \\mu_i + \\sum_{j=1}^n \\int_{0}^{t} \\phi_{ij}(t-s)\\,\\mathrm{d}N_j(s),\n$$\nwhere $\\mu_i \\ge 0$ is the exogenous baseline rate at node $i$, and $\\phi_{ij}(t) \\ge 0$ is the excitation kernel from node $j$ to node $i$ for $t  0$. The branching construction represents the Hawkes process as a Poisson cluster process: immigrant events arrive as independent homogeneous Poisson processes with rates $\\{\\mu_i\\}$, and each event independently generates offspring according to node-specific and time-delay distributions induced by the kernels. For scientific realism, assume the stability condition that the spectral radius of the matrix $A = [a_{ij}]$ of kernel integrals over $\\mathbb{R}_+$ is strictly less than $1$.\n\nYour task is to derive and implement a sampler using the branching construction on a finite observation window $[0,T]$ under the following assumptions:\n\n1. For all $(i,j)$, the kernel is exponential of the form\n$$\n\\phi_{ij}(t) = a_{ij}\\,\\beta\\,e^{-\\beta t}\\,\\mathbf{1}\\{t  0\\},\n$$\nwith $a_{ij} \\ge 0$ and shared decay parameter $\\beta  0$.\n2. Immigrant events at node $i$ are generated by a homogeneous Poisson process of rate $\\mu_i$ on $[0,T]$.\n3. Each event at node $j$ occurring at time $s$ generates independent offspring at node $i$ within the window $[0,T]$ as follows: the number of offspring is Poisson with mean\n$$\nm_{ij}(T-s) = \\int_{0}^{T-s} \\phi_{ij}(u)\\,\\mathrm{d}u = a_{ij}\\left(1 - e^{-\\beta (T-s)}\\right),\n$$\nand their time delays relative to $s$ are independent and identically distributed (i.i.d.) with truncated exponential density on $[0, T-s]$,\n$$\ng_{ij}(u \\mid T-s) = \\frac{\\beta\\,e^{-\\beta u}}{1 - e^{-\\beta (T-s)}}\\quad\\text{for }0 \\le u \\le T-s,\n$$\nso that the child times are $s + u$.\n\nFrom the core definitions of homogeneous Poisson processes and the conditional intensity of a Hawkes process, derive why the above sampling scheme reproduces the correct law on $[0,T]$ for the given exponential kernel family, and why the stability condition ensures finite expected cluster size. Your derivation must be principle-based and should start from the definition of the conditional intensity and the independence properties of Poisson processes; do not assume any specialized \"shortcut\" formulas beyond those justified from first principles.\n\nImplementation requirements:\n\n- Write a program that, for given parameters $(\\mu, A, \\beta, T)$ and a fixed integer random seed, generates immigrant events and recursively samples offspring using the truncated means $m_{ij}(T-s)$ and truncated exponential delays. The algorithm must:\n  - Sample immigrant counts at node $i$ as $N_i \\sim \\text{Poisson}(\\mu_i T)$ and immigrant times as i.i.d. uniform on $[0,T]$, then sort them.\n  - Use a first-in-first-out queue to recursively generate offspring for each event, truncating offspring generation by the residual window length $T - s$.\n  - Ensure that all generated event times lie within $[0,T]$.\n- For each test case, output the total number of events per node (immigrants plus offspring) as a list of integers.\n\nTest suite:\n\nProvide the following test cases, each specified by $(\\mu, A, \\beta, T, \\text{seed})$. All numbers below must be treated exactly as given:\n\n- Case $1$ (happy path, $n=3$): $\\mu = [0.5, 0.8, 0.3]$, $A = \\begin{bmatrix} 0.2  0.1  0.0 \\\\ 0.05  0.25  0.05 \\\\ 0.0  0.1  0.2 \\end{bmatrix}$, $\\beta = 2.0$, $T = 10.0$, seed $= 12345$.\n- Case $2$ (near-critical, $n=2$): $\\mu = [0.2, 0.2]$, $A = \\begin{bmatrix} 0.6  0.35 \\\\ 0.35  0.6 \\end{bmatrix}$, $\\beta = 1.5$, $T = 10.0$, seed $= 54321$.\n- Case $3$ (no interactions, $n=2$): $\\mu = [1.0, 0.5]$, $A = \\begin{bmatrix} 0.0  0.0 \\\\ 0.0  0.0 \\end{bmatrix}$, $\\beta = 2.0$, $T = 5.0$, seed $= 999$.\n- Case $4$ (no immigrants, $n=2$): $\\mu = [0.0, 0.0]$, $A = \\begin{bmatrix} 0.3  0.2 \\\\ 0.1  0.4 \\end{bmatrix}$, $\\beta = 1.0$, $T = 10.0$, seed $= 42$.\n- Case $5$ (short window, $n=1$): $\\mu = [0.8]$, $A = \\begin{bmatrix} 0.9 \\end{bmatrix}$, $\\beta = 0.5$, $T = 0.1$, seed $= 2024$.\n\nFinal output format:\n\nYour program should produce a single line of output containing the results as a comma-separated list of lists with no spaces, in the order of the test cases. For example, if the per-node counts for Case $1$ are $[x_1,x_2,x_3]$ and for Case $2$ are $[y_1,y_2]$, the output format is $[[x_1,x_2,x_3],[y_1,y_2],\\dots]$ with no additional text. All values in the output must be integers.",
            "solution": "The problem of sampling from a multivariate Hawkes process on a network using the branching construction is a well-posed and scientifically grounded question in the field of stochastic processes. The provided parameters and model specifications are consistent with established theory. We will first derive the sampling method from first principles and then provide a concrete implementation.\n\n### Derivation of the Branching Construction Sampler\n\nThe foundation of the branching construction lies in the superposition property of Poisson processes. We will demonstrate how the Hawkes process can be decomposed into a set of independent, simpler Poisson processes, which can be simulated directly.\n\n**1. Hawkes Process Definition and Superposition**\n\nA multivariate Hawkes process on a network of $n$ nodes is a collection of counting processes $\\{N_i(t)\\}_{i=1}^n$. The process is defined by its conditional intensity function $\\lambda_i(t)$ for each node $i$:\n$$\n\\lambda_i(t) = \\mu_i + \\sum_{j=1}^n \\int_{0}^{t} \\phi_{ij}(t-s)\\,\\mathrm{d}N_j(s)\n$$\nwhere $\\mu_i \\ge 0$ is the constant background (or exogenous) rate, and $\\phi_{ij}(t-s)$ is the kernel function describing the influence of a past event at node $j$ at time $s$ on the current rate at node $i$.\n\nThe superposition principle for Poisson processes states that the union of event times from a set of independent Poisson processes results in a new Poisson process whose rate function is the sum of the individual rate functions. We can apply this principle in reverse to decompose the Hawkes process. The intensity $\\lambda_i(t)$ is a sum of terms. This implies that the overall process $N_i(t)$ can be viewed as the superposition of independent components:\n- An \"immigrant\" process, which is a homogeneous Poisson process (HPP) with a constant rate of $\\mu_i$.\n- For each event that occurs at any node $j$ at time $s$ (whether it is an immigrant or an offspring), it triggers an \"offspring\" process on node $i$. This is an inhomogeneous Poisson process (IPP) whose rate at time $t  s$ is given by $\\phi_{ij}(t-s)$.\n\nThis decomposition reveals a natural genealogical structure. Each event can be classified either as an \"immigrant\" (arising from the background process) or as an \"offspring\" of a previous event. Every event can be traced back to a unique immigrant ancestor, forming a cluster of events. The entire Hawkes process is the collection of all such independent clusters.\n\n**2. Simulating the Process on a Finite Window $[0, T]$**\n\nThe branching construction provides a direct simulation algorithm. We simulate the process by first generating the immigrants and then recursively simulating their descendants, ensuring all events fall within the observation window $[0, T]$.\n\n**2a. Immigrant Generation**\nThe immigrant process for each node $i$ is an HPP with rate $\\mu_i$. To simulate an HPP on a finite interval $[0, T]$, we use two of its fundamental properties:\n1.  The total number of events in the interval $[0, T]$ follows a Poisson distribution with mean $\\mu_i T$.\n2.  Conditional on the number of events being $k$, the times of these $k$ events are independent and identically distributed (i.i.d.) random variables following a Uniform distribution on $[0, T]$.\n\nThe algorithm described in the problem statement correctly follows this: it samples the number of immigrants for node $i$, $N_i^{(0)}$, from a $\\text{Poisson}(\\mu_i T)$ distribution and then samples $N_i^{(0)}$ event times from $\\text{Uniform}(0, T)$. These events are the \"zeroth generation\" of the clusters.\n\n**2b. Offspring Generation**\nConsider a single parent event at node $j$ that occurred at time $s \\in [0, T]$. This parent can generate offspring on any node $i$. The offspring on node $i$ are generated by an IPP with rate function $\\phi_{ij}(t-s)$ for $t  s$. We are interested only in offspring that occur within the observation window, i.e., for $t \\in [s, T]$.\n\nTo simulate this IPP on $[s, T]$, we determine two things: the number of offspring and their timings.\n\n- **Number of Offspring**: For an IPP with rate function $\\lambda(t)$, the number of events in an interval $[t_a, t_b]$ is a Poisson-distributed random variable with a mean equal to the integrated rate, $\\int_{t_a}^{t_b} \\lambda(t) dt$. In our context, the expected number of offspring on node $i$ generated by a parent at $(j,s)$ is given by the integral of the rate function $\\phi_{ij}(t-s)$ over the interval $[s, T]$:\n$$\n\\mathbb{E}[\\text{offspring}] = \\int_{s}^{T} \\phi_{ij}(t-s)\\,\\mathrm{d}t\n$$\nUsing the substitution $u = t-s$, the integral transforms to:\n$$\nm_{ij}(T-s) = \\int_{0}^{T-s} \\phi_{ij}(u)\\,\\mathrm{d}u\n$$\nFor the specified exponential kernel, $\\phi_{ij}(u) = a_{ij} \\beta e^{-\\beta u}$, the integral evaluates to:\n$$\nm_{ij}(T-s) = \\int_{0}^{T-s} a_{ij} \\beta e^{-\\beta u}\\,\\mathrm{d}u = a_{ij} \\left[-e^{-\\beta u}\\right]_{0}^{T-s} = a_{ij} \\left( -e^{-\\beta(T-s)} - (-e^0) \\right) = a_{ij}(1 - e^{-\\beta (T-s)})\n$$\nThis confirms that the number of offspring is correctly modeled as a Poisson random variable with mean $m_{ij}(T-s)$.\n\n- **Timing of Offspring**: Given that $k \\ge 1$ events from an IPP occur in an interval, their times are i.i.d. draws from the probability distribution whose density is proportional to the rate function over that interval. The normalized probability density function for an event time $t \\in [s, T]$ is $p(t) = \\frac{\\phi_{ij}(t-s)}{\\int_s^T \\phi_{ij}(\\tau-s)\\mathrm{d}\\tau}$. We are typically interested in the time delay $u = t-s \\in [0, T-s]$. The density of the delay, $g_{ij}(u)$, is:\n$$\ng_{ij}(u \\mid T-s) = \\frac{\\phi_{ij}(u)}{\\int_0^{T-s} \\phi_{ij}(v)\\,\\mathrm{d}v} = \\frac{a_{ij} \\beta e^{-\\beta u}}{a_{ij} (1 - e^{-\\beta(T-s)})} = \\frac{\\beta e^{-\\beta u}}{1 - e^{-\\beta(T-s)}}\n$$\nThis is the probability density function for an exponential distribution truncated to the interval $[0, T-s]$, as specified in the problem. This distribution can be sampled using the inverse transform sampling method. The cumulative distribution function (CDF) is $F(u) = \\frac{1-e^{-\\beta u}}{1-e^{-\\beta(T-s)}}$. Setting $F(u) = U$ for a uniform random variable $U \\sim \\text{Uniform}(0,1)$ and solving for $u$ yields the sampling formula:\n$$\nu = -\\frac{1}{\\beta} \\ln\\left(1 - U\\left(1 - e^{-\\beta(T-s)}\\right)\\right)\n$$\nThis confirms the correctness of the specified offspring generation procedure.\n\n**3. Stability and Expected Cluster Size**\n\nThe stability of the Hawkes process is related to the expected size of the event clusters. For the process to be stable (i.e., not \"explode\" with an infinite number of events), the expected total number of descendants from any single event must be finite. This analysis is typically done on an infinite time horizon.\n\nThe expected number of direct (first-generation) offspring that an event at node $j$ produces on node $i$ is given by the total integral of the kernel:\n$$\na_{ij} = \\int_{0}^{\\infty} \\phi_{ij}(t)\\,\\mathrm{d}t = \\int_{0}^{\\infty} a_{ij} \\beta e^{-\\beta t}\\,\\mathrm{d}t = a_{ij}\n$$\nThe matrix $A = [a_{ij}]$ is the branching ratio matrix, where $A_{ij}$ is the expected number of type-$i$ children from a type-$j$ parent. This system is a multi-type Galton-Watson branching process. The expected number of descendants in the $k$-th generation is related to the $k$-th power of this matrix, $A^k$. The total expected number of descendants of all generations is given by the matrix sum $\\sum_{k=1}^{\\infty} A^k$.\n\nThis geometric matrix series converges if and only if the spectral radius of $A$, denoted $\\rho(A)$, is strictly less than $1$. If $\\rho(A)  1$, the sum is finite and equals $(I - A)^{-1} - I$, ensuring that the expected cluster size is finite. If $\\rho(A) \\ge 1$, the process is supercritical or critical, and the expected cluster size is infinite. Thus, the condition $\\rho(A)  1$ is the standard stability condition for the Hawkes process.",
            "answer": "```python\nimport numpy as np\nimport collections\n\ndef solve():\n    \"\"\"\n    Solves the problem of simulating a multivariate Hawkes process using the\n    branching construction for a suite of test cases.\n    \"\"\"\n    \n    test_cases = [\n        {\n            \"mu\": np.array([0.5, 0.8, 0.3]),\n            \"A\": np.array([\n                [0.2, 0.1, 0.0],\n                [0.05, 0.25, 0.05],\n                [0.0, 0.1, 0.2]\n            ]),\n            \"beta\": 2.0,\n            \"T\": 10.0,\n            \"seed\": 12345\n        },\n        {\n            \"mu\": np.array([0.2, 0.2]),\n            \"A\": np.array([\n                [0.6, 0.35],\n                [0.35, 0.6]\n            ]),\n            \"beta\": 1.5,\n            \"T\": 10.0,\n            \"seed\": 54321\n        },\n        {\n            \"mu\": np.array([1.0, 0.5]),\n            \"A\": np.array([\n                [0.0, 0.0],\n                [0.0, 0.0]\n            ]),\n            \"beta\": 2.0,\n            \"T\": 5.0,\n            \"seed\": 999\n        },\n        {\n            \"mu\": np.array([0.0, 0.0]),\n            \"A\": np.array([\n                [0.3, 0.2],\n                [0.1, 0.4]\n            ]),\n            \"beta\": 1.0,\n            \"T\": 10.0,\n            \"seed\": 42\n        },\n        {\n            \"mu\": np.array([0.8]),\n            \"A\": np.array([[0.9]]),\n            \"beta\": 0.5,\n            \"T\": 0.1,\n            \"seed\": 2024\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        mu, A, beta, T, seed = case[\"mu\"], case[\"A\"], case[\"beta\"], case[\"T\"], case[\"seed\"]\n        \n        n = len(mu)\n        rng = np.random.default_rng(seed)\n        \n        # Use a FIFO queue for breadth-first traversal of the event generations\n        event_queue = collections.deque()\n        \n        # Store all generated events for final counting\n        event_lists = [[] for _ in range(n)]\n\n        # --- Step 1: Generate immigrant (generation 0) events ---\n        for i in range(n):\n            if mu[i]  0:\n                # Number of immigrants follows a Poisson distribution\n                num_immigrants = rng.poisson(mu[i] * T)\n                \n                # Times of immigrants are uniform on [0, T]\n                immigrant_times = rng.uniform(0, T, size=num_immigrants)\n                \n                # Sorting is good practice but not essential for this algorithm's logic\n                immigrant_times.sort()\n                \n                for t in immigrant_times:\n                    # Add immigrant to the queue to generate its offspring\n                    event_queue.append((i, t))\n                    # Add immigrant to the final list of events\n                    event_lists[i].append(t)\n        \n        # --- Step 2: Recursively generate offspring ---\n        while event_queue:\n            parent_node, parent_time = event_queue.popleft()\n            \n            # Iterate over all possible child nodes\n            for child_node in range(n):\n                a_ij = A[child_node, parent_node]\n                \n                # No offspring if the branching ratio is zero\n                if a_ij == 0:\n                    continue\n                \n                # Time remaining in the observation window\n                T_residual = T - parent_time\n                \n                # Mean number of offspring within the residual window [0, T_residual]\n                # Use np.expm1 for numerical stability: 1 - exp(-x) = -expm1(-x)\n                truncation_term = -np.expm1(-beta * T_residual)\n                mean_offspring = a_ij * truncation_term\n                \n                if mean_offspring = 0:\n                    continue\n                \n                # Sample the number of offspring\n                num_offspring = rng.poisson(mean_offspring)\n                \n                if num_offspring  0:\n                    # Sample delays using inverse transform sampling\n                    # U ~ Uniform(0, 1)\n                    # delay = (-1/beta) * log(1 - U * (1 - exp(-beta*T_residual)))\n                    U_samples = rng.uniform(size=num_offspring)\n                    delays = (-1.0 / beta) * np.log(1.0 - U_samples * truncation_term)\n                    \n                    for delay in delays:\n                        child_time = parent_time + delay\n                        \n                        # Ensure child event is within the window [0, T]\n                        # This check is robust against floating point inaccuracies\n                        if child_time  T:\n                            event_queue.append((child_node, child_time))\n                            event_lists[child_node].append(child_time)\n                            \n        # --- Step 3: Collect results ---\n        # The result for this case is the total count of events per node\n        final_counts = [len(lst) for lst in event_lists]\n        results.append(final_counts)\n\n    # Format the final output string as specified\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "While simulation reveals the microscopic dynamics of event generation, it is often critical to predict the macroscopic, long-term behavior of a system. This practice focuses on calculating the stationary event rates of a networked Hawkes process, connecting the model's core parameters—baseline rates and the interaction kernel—to its steady-state activity level. You will also derive the fundamental stability condition that guarantees such a stationary state exists. ",
            "id": "4280109",
            "problem": "Consider a multivariate Hawkes process on a $3$-node line network with nodes labeled $1$, $2$, and $3$. The network is represented by the adjacency matrix $A \\in \\mathbb{R}^{3 \\times 3}$ given by\n$$\nA = \\begin{pmatrix}\n0  1  0 \\\\\n1  0  1 \\\\\n0  1  0\n\\end{pmatrix}.\n$$\nEach component (node) $i \\in \\{1,2,3\\}$ has a constant baseline rate $\\mu_i$, and the endogenous excitation is governed by a matrix-valued kernel $\\Phi(t) \\in \\mathbb{R}^{3 \\times 3}$ of the form\n$$\n\\Phi(t) = \\alpha \\exp(-\\beta t) A, \\quad t \\geq 0,\n$$\nwith $\\alpha  0$ and $\\beta  0$. The multivariate Hawkes process intensity vector $\\boldsymbol{\\lambda}(t) \\in \\mathbb{R}^{3}$ is defined by the causal integral relation\n$$\n\\boldsymbol{\\lambda}(t) = \\boldsymbol{\\mu} + \\int_{0}^{t} \\Phi(t-s) \\, d\\boldsymbol{N}(s),\n$$\nwhere $\\boldsymbol{N}(t) \\in \\mathbb{R}^{3}$ is the vector of counting processes and $\\boldsymbol{\\mu} = (\\mu_1,\\mu_2,\\mu_3)^{\\top}$ is the baseline rate vector. Assume the process admits a stationary regime with a constant expected rate vector $\\overline{\\boldsymbol{\\lambda}}$.\n\nStarting from the above definition of the Hawkes intensity and first principles of linear response under stationarity, derive the stationarity equation for $\\overline{\\boldsymbol{\\lambda}}$ in terms of the time integral of $\\Phi(t)$, and determine the stability condition in terms of the spectral radius of the matrix $\\frac{\\alpha}{\\beta} A$. Then, for the specific parameter values $\\alpha = 1$, $\\beta = 3$, and baseline rates $\\boldsymbol{\\mu} = (1, 2, 1)^{\\top}$, compute the stationary expected rate vector $\\overline{\\boldsymbol{\\lambda}}$. Express the final rates in events per unit time. The final numerical answer must be given exactly; no rounding is required. Additionally, state the stability condition using the spectral radius notation $\\rho\\!\\left(\\frac{\\alpha}{\\beta} A\\right)$, and verify whether it is satisfied for the given parameters.",
            "solution": "The user wants to solve a problem about a multivariate Hawkes process.\n\n### Step 1: Problem Validation\n\n**1. Extract Givens:**\n- **Network:** $3$-node line network, represented by the adjacency matrix $A = \\begin{pmatrix} 0  1  0 \\\\ 1  0  1 \\\\ 0  1  0 \\end{pmatrix}$.\n- **Baseline Rates:** A constant vector $\\boldsymbol{\\mu} = (\\mu_1, \\mu_2, \\mu_3)^{\\top}$.\n- **Kernel:** A matrix-valued function $\\Phi(t) = \\alpha \\exp(-\\beta t) A$ for $t \\geq 0$, with parameters $\\alpha  0$ and $\\beta  0$.\n- **Hawkes Intensity:** The intensity vector $\\boldsymbol{\\lambda}(t) \\in \\mathbb{R}^{3}$ is given by $\\boldsymbol{\\lambda}(t) = \\boldsymbol{\\mu} + \\int_{0}^{t} \\Phi(t-s) \\, d\\boldsymbol{N}(s)$, where $\\boldsymbol{N}(t)$ is the vector of counting processes.\n- **Stationarity Assumption:** The process admits a stationary regime with a constant expected rate vector $\\overline{\\boldsymbol{\\lambda}}$.\n- **Specific Parameters:** $\\alpha = 1$, $\\beta = 3$, and $\\boldsymbol{\\mu} = (1, 2, 1)^{\\top}$.\n- **Tasks:**\n  1. Derive the stationarity equation for $\\overline{\\boldsymbol{\\lambda}}$.\n  2. Determine the stability condition in terms of the spectral radius of $\\frac{\\alpha}{\\beta} A$.\n  3. Compute the stationary expected rate vector $\\overline{\\boldsymbol{\\lambda}}$ for the given parameters.\n  4. Verify if the stability condition is satisfied for the given parameters.\n\n**2. Validate Using Extracted Givens:**\n- **Scientific Grounding:** The problem describes a standard linear multivariate Hawkes process, a well-established and scientifically sound model in stochastic processes and complex systems. The formulation is canonical.\n- **Well-Posedness:** The problem is well-posed. It requests the derivation of a standard result and its application. The existence and uniqueness of the solution depend on a stability condition, which is a required part of the analysis.\n- **Objectivity:** The problem is stated in precise mathematical language, free from ambiguity or subjective content.\n- **Completeness and Consistency:** All necessary information (matrices, parameters, definitions) is provided. There are no contradictions.\n\n**3. Verdict and Action:**\nThe problem is deemed **valid**. It is a standard, well-posed problem in the theory of Hawkes processes. The solution will proceed.\n\n### Step 2: Derivation and Solution\n\nThe intensity vector of the multivariate Hawkes process is given by\n$$\n\\boldsymbol{\\lambda}(t) = \\boldsymbol{\\mu} + \\int_{0}^{t} \\Phi(t-s) \\, d\\boldsymbol{N}(s)\n$$\nTo find the stationary expected rate vector $\\overline{\\boldsymbol{\\lambda}}$, we take the expectation of both sides.\n$$\nE[\\boldsymbol{\\lambda}(t)] = E[\\boldsymbol{\\mu}] + E\\left[\\int_{0}^{t} \\Phi(t-s) \\, d\\boldsymbol{N}(s)\\right]\n$$\nSince $\\boldsymbol{\\mu}$ is a constant vector, $E[\\boldsymbol{\\mu}] = \\boldsymbol{\\mu}$. The expectation of the stochastic integral, by the properties of point processes (specifically, an extension of Campbell's theorem), is given by\n$$\nE\\left[\\int_{0}^{t} \\Phi(t-s) \\, d\\boldsymbol{N}(s)\\right] = \\int_{0}^{t} \\Phi(t-s) E[\\boldsymbol{\\lambda}(s)] \\, ds\n$$\nwhere $E[\\boldsymbol{\\lambda}(s)]$ is the expected rate of the process at time $s$. Thus, we have an integral equation for the expected intensity:\n$$\nE[\\boldsymbol{\\lambda}(t)] = \\boldsymbol{\\mu} + \\int_{0}^{t} \\Phi(t-s) E[\\boldsymbol{\\lambda}(s)] \\, ds\n$$\nThe problem states that the process admits a stationary regime. This implies that as $t \\to \\infty$, the expected intensity becomes constant, i.e., $\\lim_{t\\to\\infty} E[\\boldsymbol{\\lambda}(t)] = \\overline{\\boldsymbol{\\lambda}}$. For a process that has been running for an infinite amount of time up to time $t$, this relationship becomes:\n$$\n\\overline{\\boldsymbol{\\lambda}} = \\boldsymbol{\\mu} + \\int_{-\\infty}^{t} \\Phi(t-s) \\overline{\\boldsymbol{\\lambda}} \\, ds\n$$\nLet's perform a change of variables $u = t-s$. Then $s = t-u$ and $ds = -du$. The integration limits change from $s \\to -\\infty$ to $u \\to \\infty$ and from $s \\to t$ to $u \\to 0$.\n$$\n\\overline{\\boldsymbol{\\lambda}} = \\boldsymbol{\\mu} + \\int_{\\infty}^{0} \\Phi(u) \\overline{\\boldsymbol{\\lambda}} \\, (-du) = \\boldsymbol{\\mu} + \\int_{0}^{\\infty} \\Phi(u) \\overline{\\boldsymbol{\\lambda}} \\, du\n$$\nSince $\\overline{\\boldsymbol{\\lambda}}$ is a constant vector, we can pull it out of the integral:\n$$\n\\overline{\\boldsymbol{\\lambda}} = \\boldsymbol{\\mu} + \\left( \\int_{0}^{\\infty} \\Phi(u) \\, du \\right) \\overline{\\boldsymbol{\\lambda}}\n$$\nLet's define the matrix $\\Phi^* = \\int_{0}^{\\infty} \\Phi(t) \\, dt$. The equation becomes:\n$$\n\\overline{\\boldsymbol{\\lambda}} = \\boldsymbol{\\mu} + \\Phi^* \\overline{\\boldsymbol{\\lambda}}\n$$\nThis is the stationarity equation for $\\overline{\\boldsymbol{\\lambda}}$. Rearranging it gives:\n$$\n(I - \\Phi^*) \\overline{\\boldsymbol{\\lambda}} = \\boldsymbol{\\mu}\n$$\nwhere $I$ is the identity matrix.\n\nNext, we determine the stability condition. A unique, physically meaningful (i.e., non-negative) stationary rate vector $\\overline{\\boldsymbol{\\lambda}}$ exists if the matrix $I - \\Phi^*$ is invertible and its inverse has non-negative entries (since $\\boldsymbol{\\mu} \\ge 0$ and we require $\\overline{\\boldsymbol{\\lambda}} \\ge 0$). $\\Phi^*$ is a non-negative matrix because $\\alpha  0$ and all entries of $A$ are non-negative. For such a matrix, the inverse $(I - \\Phi^*)^{-1}$ can be expressed as a Neumann series:\n$$\n(I - \\Phi^*)^{-1} = \\sum_{k=0}^{\\infty} (\\Phi^*)^k\n$$\nThis series converges if and only if the spectral radius of $\\Phi^*$, denoted $\\rho(\\Phi^*)$, is less than $1$. Therefore, the stability condition for the Hawkes process is $\\rho(\\Phi^*)  1$.\n\nNow, let's compute $\\Phi^*$ for the given kernel $\\Phi(t) = \\alpha \\exp(-\\beta t) A$:\n$$\n\\Phi^* = \\int_{0}^{\\infty} \\alpha \\exp(-\\beta t) A \\, dt = \\alpha A \\int_{0}^{\\infty} \\exp(-\\beta t) \\, dt\n$$\nThe integral evaluates to:\n$$\n\\int_{0}^{\\infty} \\exp(-\\beta t) \\, dt = \\left[ -\\frac{1}{\\beta} \\exp(-\\beta t) \\right]_{0}^{\\infty} = 0 - \\left(-\\frac{1}{\\beta}\\right) = \\frac{1}{\\beta}\n$$\nSo, the integrated kernel matrix is $\\Phi^* = \\frac{\\alpha}{\\beta} A$.\nThe stability condition is therefore $\\rho\\left(\\frac{\\alpha}{\\beta} A\\right)  1$.\n\nNow we use the specific parameter values: $\\alpha = 1$, $\\beta = 3$, and $\\boldsymbol{\\mu} = (1, 2, 1)^{\\top}$.\nThe matrix $\\Phi^*$ becomes:\n$$\n\\Phi^* = \\frac{1}{3} A = \\frac{1}{3} \\begin{pmatrix} 0  1  0 \\\\ 1  0  1 \\\\ 0  1  0 \\end{pmatrix} = \\begin{pmatrix} 0  1/3  0 \\\\ 1/3  0  1/3 \\\\ 0  1/3  0 \\end{pmatrix}\n$$\nWe must first verify the stability condition. This requires finding the spectral radius of $\\Phi^*$.\n$\\rho(\\Phi^*) = \\rho\\left(\\frac{1}{3}A\\right) = \\frac{1}{3}\\rho(A)$.\nTo find $\\rho(A)$, we find the eigenvalues of $A$ by solving the characteristic equation $\\det(A - \\lambda I) = 0$:\n$$\n\\det\\begin{pmatrix} -\\lambda  1  0 \\\\ 1  -\\lambda  1 \\\\ 0  1  -\\lambda \\end{pmatrix} = -\\lambda(\\lambda^2 - 1) - 1(-\\lambda) = -\\lambda^3 + \\lambda + \\lambda = 2\\lambda - \\lambda^3 = \\lambda(2-\\lambda^2) = 0\n$$\nThe eigenvalues of $A$ are $\\lambda_1 = 0$, $\\lambda_2 = \\sqrt{2}$, and $\\lambda_3 = -\\sqrt{2}$.\nThe spectral radius of $A$ is the maximum of the absolute values of its eigenvalues: $\\rho(A) = \\max\\{|0|, |\\sqrt{2}|, |-\\sqrt{2}|\\} = \\sqrt{2}$.\nThe stability condition is $\\rho\\left(\\frac{1}{3}A\\right) = \\frac{1}{3}\\rho(A) = \\frac{\\sqrt{2}}{3}  1$.\nSince $\\sqrt{2} \\approx 1.414$, we have $\\frac{\\sqrt{2}}{3} \\approx 0.471$, which is indeed less than $1$. The condition is satisfied, so a unique stationary solution exists.\n\nFinally, we compute the stationary expected rate vector $\\overline{\\boldsymbol{\\lambda}} = (\\overline{\\lambda}_1, \\overline{\\lambda}_2, \\overline{\\lambda}_3)^{\\top}$ by solving the linear system $(I - \\Phi^*) \\overline{\\boldsymbol{\\lambda}} = \\boldsymbol{\\mu}$:\n$$\n(I - \\Phi^*) = \\begin{pmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 0  0  1 \\end{pmatrix} - \\begin{pmatrix} 0  1/3  0 \\\\ 1/3  0  1/3 \\\\ 0  1/3  0 \\end{pmatrix} = \\begin{pmatrix} 1  -1/3  0 \\\\ -1/3  1  -1/3 \\\\ 0  -1/3  1 \\end{pmatrix}\n$$\nThe system of equations is:\n$$\n\\begin{pmatrix} 1  -1/3  0 \\\\ -1/3  1  -1/3 \\\\ 0  -1/3  1 \\end{pmatrix} \\begin{pmatrix} \\overline{\\lambda}_1 \\\\ \\overline{\\lambda}_2 \\\\ \\overline{\\lambda}_3 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 1 \\end{pmatrix}\n$$\nThis gives the following equations:\n1. $\\overline{\\lambda}_1 - \\frac{1}{3}\\overline{\\lambda}_2 = 1$\n2. $-\\frac{1}{3}\\overline{\\lambda}_1 + \\overline{\\lambda}_2 - \\frac{1}{3}\\overline{\\lambda}_3 = 2$\n3. $-\\frac{1}{3}\\overline{\\lambda}_2 + \\overline{\\lambda}_3 = 1$\n\nFrom equation (1), we have $\\overline{\\lambda}_1 = 1 + \\frac{1}{3}\\overline{\\lambda}_2$.\nFrom equation (3), we have $\\overline{\\lambda}_3 = 1 + \\frac{1}{3}\\overline{\\lambda}_2$.\nNote that due to the symmetry of the network and the baseline rates ($\\mu_1 = \\mu_3$), we expect $\\overline{\\lambda}_1 = \\overline{\\lambda}_3$.\nSubstitute these expressions into equation (2):\n$$\n-\\frac{1}{3}\\left(1 + \\frac{1}{3}\\overline{\\lambda}_2\\right) + \\overline{\\lambda}_2 - \\frac{1}{3}\\left(1 + \\frac{1}{3}\\overline{\\lambda}_2\\right) = 2\n$$\n$$\n-\\frac{1}{3} - \\frac{1}{9}\\overline{\\lambda}_2 + \\overline{\\lambda}_2 - \\frac{1}{3} - \\frac{1}{9}\\overline{\\lambda}_2 = 2\n$$\n$$\n\\left(1 - \\frac{2}{9}\\right)\\overline{\\lambda}_2 - \\frac{2}{3} = 2\n$$\n$$\n\\frac{7}{9}\\overline{\\lambda}_2 = 2 + \\frac{2}{3} = \\frac{8}{3}\n$$\n$$\n\\overline{\\lambda}_2 = \\frac{8}{3} \\times \\frac{9}{7} = \\frac{24}{7}\n$$\nNow we can find $\\overline{\\lambda}_1$ and $\\overline{\\lambda}_3$:\n$$\n\\overline{\\lambda}_1 = \\overline{\\lambda}_3 = 1 + \\frac{1}{3}\\overline{\\lambda}_2 = 1 + \\frac{1}{3}\\left(\\frac{24}{7}\\right) = 1 + \\frac{8}{7} = \\frac{15}{7}\n$$\nThe stationary expected rate vector is $\\overline{\\boldsymbol{\\lambda}} = \\left(\\frac{15}{7}, \\frac{24}{7}, \\frac{15}{7}\\right)^{\\top}$. The rates are in units of events per unit time.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{15}{7}  \\frac{24}{7}  \\frac{15}{7}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "A model is only as good as its ability to describe observed data, making model validation a critical step in any analysis. This exercise introduces residual analysis, a powerful technique for diagnosing a model's goodness-of-fit. You will learn to derive and compute Pearson residuals to identify potential model misspecifications by comparing observed event counts to those predicted by the model. ",
            "id": "4280093",
            "problem": "Consider a $K$-dimensional multivariate Hawkes process on a directed network with node set $\\{1,\\dots,K\\}$ and counting processes $\\{N_i(t)\\}_{i=1}^K$. The predictable conditional intensity for component $i$ is defined by\n$$\n\\lambda_i(t) \\;=\\; \\mu_i \\;+\\; \\sum_{j=1}^K \\int_{0}^{t-} \\phi_{ij}(t-s)\\,\\mathrm{d}N_j(s),\n$$\nwhere $\\mu_i \\ge 0$ is a baseline rate and $\\phi_{ij}(u) \\ge 0$ are excitation kernels that encode the network interactions. Suppose the excitation kernels are of the parametric form $\\phi_{ij}(u) = a_{ij}\\,\\alpha\\,\\mathrm{e}^{-\\beta u}$ for $u0$, where $a_{ij} \\ge 0$ are adjacency weights, and $\\alpha,\\beta0$ are shape parameters. A fitted model provides estimates $\\widehat{\\mu}_i$, $\\widehat{a}_{ij}$, $\\widehat{\\alpha}$, $\\widehat{\\beta}$, and hence a fitted intensity $\\widehat{\\lambda}_i(t)$.\n\nOver an observation window $[0,T]$, partition time into $M$ disjoint bins $B_m=(t_m,t_{m+1}]$ of width $\\Delta=t_{m+1}-t_m$. For each component $i$, define the binned counts $Y_{i,m} := N_i(B_m)$ and the fitted bin mean $\\widehat{\\Lambda}_{i,m} := \\int_{B_m} \\widehat{\\lambda}_i(t)\\,\\mathrm{d}t$.\n\nUsing only first principles for point processes with predictable intensity—namely, that for sufficiently fine binning the expected number of events in a bin equals the integrated intensity, and the leading-order variance is governed by the same scale—develop an appropriate Pearson residual for binned multivariate Hawkes counts that is justified by the conditional distribution of increments. Then, for node $i=1$ and $M=5$ bins with data\n$$\nY_{1,1}=6,\\quad Y_{1,2}=0,\\quad Y_{1,3}=3,\\quad Y_{1,4}=7,\\quad Y_{1,5}=1,\n$$\nand\n$$\n\\widehat{\\Lambda}_{1,1}=2.1,\\quad \\widehat{\\Lambda}_{1,2}=0.5,\\quad \\widehat{\\Lambda}_{1,3}=1.6,\\quad \\widehat{\\Lambda}_{1,4}=3.0,\\quad \\widehat{\\Lambda}_{1,5}=0.9,\n$$\ncompute the Pearson residuals $r_{1,m}$ across bins and the empirical variance of these residuals across $m=1,\\dots,5$. Based on your derivation and computation, determine which of the following statements are correct. Select all that apply.\n\nA. The appropriate Pearson residual for bin $m$ and component $i$ is $r_{i,m} = \\big(Y_{i,m} - \\widehat{\\Lambda}_{i,m}\\big)/\\sqrt{\\widehat{\\Lambda}_{i,m}}$, justified by the conditional Poisson approximation of increments; under correct specification and sufficiently fine bins, $\\mathbb{E}[r_{i,m}] \\approx 0$ and $\\operatorname{Var}(r_{i,m}) \\approx 1$.\n\nB. Because of self-excitation, the conditional variance of $Y_{i,m}$ equals $\\widehat{\\Lambda}_{i,m}^2$, so the Pearson residual should be $r_{i,m} = \\big(Y_{i,m} - \\widehat{\\Lambda}_{i,m}\\big)/\\widehat{\\Lambda}_{i,m}$.\n\nC. For node $i=1$ with the given data, the empirical variance of the Pearson residuals across the $5$ bins is strictly greater than $1$, indicating overdispersion consistent with model misspecification or overly coarse binning.\n\nD. Overdispersion in Pearson residuals cannot indicate missing network edges, because the adjacency matrix $\\{a_{ij}\\}$ only controls the mean structure and not the variance.\n\nE. Even under a correctly specified Hawkes model, Pearson residuals across different components within the same bin must be exactly independent and uncorrelated for any bin width $\\Delta$.",
            "solution": "The user wants me to validate the problem statement and, if valid, solve the problem by deriving the Pearson residual for a multivariate Hawkes process, computing its empirical variance for a given data set, and evaluating several statements.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- **Process:** A $K$-dimensional multivariate Hawkes process on a directed network with node set $\\{1,\\dots,K\\}$.\n- **Counting Processes:** $\\{N_i(t)\\}_{i=1}^K$.\n- **Conditional Intensity:** $\\lambda_i(t) \\;=\\; \\mu_i \\;+\\; \\sum_{j=1}^K \\int_{0}^{t-} \\phi_{ij}(t-s)\\,\\mathrm{d}N_j(s)$.\n- **Parameters:** Baseline rate $\\mu_i \\ge 0$, excitation kernels $\\phi_{ij}(u) \\ge 0$.\n- **Kernel Form:** $\\phi_{ij}(u) = a_{ij}\\,\\alpha\\,\\mathrm{e}^{-\\beta u}$ for $u0$, with adjacency weights $a_{ij} \\ge 0$ and shape parameters $\\alpha,\\beta0$.\n- **Fitted Model:** Provides estimates $\\widehat{\\mu}_i$, $\\widehat{a}_{ij}$, $\\widehat{\\alpha}$, $\\widehat{\\beta}$, and a fitted intensity $\\widehat{\\lambda}_i(t)$.\n- **Binning:** Observation window $[0,T]$ is partitioned into $M$ disjoint bins $B_m=(t_m,t_{m+1}]$ of width $\\Delta=t_{m+1}-t_m$.\n- **Binned Data:** $Y_{i,m} := N_i(B_m)$ are the binned counts.\n- **Fitted Bin Mean:** $\\widehat{\\Lambda}_{i,m} := \\int_{B_m} \\widehat{\\lambda}_i(t)\\,\\mathrm{d}t$.\n- **Specific Data for Node $i=1$ and $M=5$ Bins:**\n  - Counts: $Y_{1,1}=6,\\quad Y_{1,2}=0,\\quad Y_{1,3}=3,\\quad Y_{1,4}=7,\\quad Y_{1,5}=1$.\n  - Fitted Means: $\\widehat{\\Lambda}_{1,1}=2.1,\\quad \\widehat{\\Lambda}_{1,2}=0.5,\\quad \\widehat{\\Lambda}_{1,3}=1.6,\\quad \\widehat{\\Lambda}_{1,4}=3.0,\\quad \\widehat{\\Lambda}_{1,5}=0.9$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem describes a standard multivariate Hawkes process with exponential kernels, a canonical model in complex systems, network science, and other fields. The concept of using binned counts and integrated intensity for diagnostics is standard. All definitions are correct and consistent with established theory.\n- **Well-Posed:** The problem provides all necessary definitions and data to perform the required derivation and calculation. The question is unambiguous and seeks a solution based on established statistical principles for point processes. A unique solution for the calculation exists.\n- **Objective:** The language is formal and precise. The problem is free from any subjective or non-scientific statements.\n\n**Step 3: Verdict and Action**\nThe problem statement is valid. It is scientifically sound, self-contained, and well-posed. I will proceed to the solution.\n\n### Derivation and Solution\n\n**1. Derivation of the Pearson Residual**\n\nThe problem asks for a derivation from first principles for a point process with a predictable intensity. Let $\\mathcal{H}_{t}$ denote the history of the entire multivariate process up to time $t$. The conditional intensity $\\lambda_i(t)$ is predictable, meaning it is a known function given the history $\\mathcal{H}_{t-}$.\n\nConsider a small time interval $(t, t+\\mathrm{d}t]$. The probability of an event from process $i$ occurring in this interval is $\\mathbb{P}(\\mathrm{d}N_i(t)=1|\\mathcal{H}_{t-}) = \\lambda_i(t)\\mathrm{d}t + o(\\mathrm{d}t)$.\n\nFor a finite bin $B_m=(t_m, t_{m+1}]$, the number of events is $Y_{i,m} = N_i(t_{m+1}) - N_i(t_m)$. The conditional expectation of this count, given the history up to the start of the bin $\\mathcal{H}_{t_m}$, is:\n$$\n\\mathbb{E}[Y_{i,m} | \\mathcal{H}_{t_m}] = \\mathbb{E}\\left[\\int_{t_m}^{t_{m+1}} \\mathrm{d}N_i(t) \\bigg| \\mathcal{H}_{t_m}\\right] = \\mathbb{E}\\left[\\int_{t_m}^{t_{m+1}} \\lambda_i(t)\\,\\mathrm{d}t \\bigg| \\mathcal{H}_{t_m}\\right]\n$$\nThe intensity $\\lambda_i(t)$ for $t \\in B_m$ depends on events that occurred prior to $t_m$ (which are fixed given $\\mathcal{H}_{t_m}$) and events that occur within $(t_m, t)$. If the bin width $\\Delta = t_{m+1}-t_m$ is sufficiently small, the probability of multiple events within the bin is low, and the intensity $\\lambda_i(t)$ does not vary drastically. The process within the bin can be approximated by a non-homogeneous Poisson process with rate function $\\lambda_i(t)$ conditioned on $\\mathcal{H}_{t_m}$. In this case, the count $Y_{i,m}$ is approximately Poisson distributed with a mean equal to the integrated intensity.\n$$\nY_{i,m} | \\mathcal{H}_{t_m} \\sim \\text{Poisson} \\left( \\int_{t_m}^{t_{m+1}} \\lambda_i(t)\\,\\mathrm{d}t \\right)\n$$\nLet $\\Lambda_{i,m} = \\int_{t_m}^{t_{m+1}} \\lambda_i(t)\\,\\mathrm{d}t$. A key property of the Poisson distribution is that its variance is equal to its mean. Therefore, conditioned on the history up to the start of the bin, we have:\n$$\n\\mathbb{E}[Y_{i,m} | \\mathcal{H}_{t_m}] \\approx \\Lambda_{i,m} \\quad \\text{and} \\quad \\operatorname{Var}(Y_{i,m} | \\mathcal{H}_{t_m}) \\approx \\Lambda_{i,m}\n$$\nThe Pearson residual is defined as the standardized difference between the observation and its expectation, where standardization is by the standard deviation. Using the fitted model, the expectation is estimated by $\\widehat{\\Lambda}_{i,m}$ and the conditional variance is also estimated by $\\widehat{\\Lambda}_{i,m}$. Thus, the Pearson residual is:\n$$\nr_{i,m} = \\frac{Y_{i,m} - \\widehat{\\Lambda}_{i,m}}{\\sqrt{\\widehat{\\Lambda}_{i,m}}}\n$$\nIf the model is correctly specified and the binning is sufficiently fine for the Poisson approximation to hold, these residuals should be realizations of a random variable with mean approximately $0$ and variance approximately $1$.\n\n### Option-by-Option Analysis\n\n**A. The appropriate Pearson residual for bin $m$ and component $i$ is $r_{i,m} = \\big(Y_{i,m} - \\widehat{\\Lambda}_{i,m}\\big)/\\sqrt{\\widehat{\\Lambda}_{i,m}}$, justified by the conditional Poisson approximation of increments; under correct specification and sufficiently fine bins, $\\mathbb{E}[r_{i,m}] \\approx 0$ and $\\operatorname{Var}(r_{i,m}) \\approx 1$.**\n\nThis statement is fully consistent with the derivation above. The formula for the residual is correct, the justification (conditional Poisson approximation) is correct, and the expected statistical properties (mean $\\approx 0$, variance $\\approx 1$) are correct.\n**Verdict: Correct.**\n\n**B. Because of self-excitation, the conditional variance of $Y_{i,m}$ equals $\\widehat{\\Lambda}_{i,m}^2$, so the Pearson residual should be $r_{i,m} = \\big(Y_{i,m} - \\widehat{\\Lambda}_{i,m}\\big)/\\widehat{\\Lambda}_{i,m}$.**\n\nThis is incorrect. Self-excitation leads to clustering of events, which causes the *unconditional* variance of counts over long periods to be greater than the mean (a property known as overdispersion relative to a Poisson process). However, the *conditional* variance, given the history up to the start of a small bin, is still governed by the Poisson-like property where variance equals the mean (integrated intensity). A variance of $\\widehat{\\Lambda}_{i,m}^2$ is not characteristic of a conditional count distribution. The proposed residual form standardizes by the mean, not the standard deviation.\n**Verdict: Incorrect.**\n\n**C. For node $i=1$ with the given data, the empirical variance of the Pearson residuals across the $5$ bins is strictly greater than $1$, indicating overdispersion consistent with model misspecification or overly coarse binning.**\n\nFirst, we must compute the Pearson residuals $r_{1,m}$ for $m=1, \\dots, 5$.\n$r_{1,1} = (6 - 2.1) / \\sqrt{2.1} \\approx 3.9 / 1.4491 \\approx 2.6912$\n$r_{1,2} = (0 - 0.5) / \\sqrt{0.5} \\approx -0.5 / 0.7071 \\approx -0.7071$\n$r_{1,3} = (3 - 1.6) / \\sqrt{1.6} \\approx 1.4 / 1.2649 \\approx 1.1068$\n$r_{1,4} = (7 - 3.0) / \\sqrt{3.0} \\approx 4.0 / 1.7321 \\approx 2.3094$\n$r_{1,5} = (1 - 0.9) / \\sqrt{0.9} \\approx 0.1 / 0.9487 \\approx 0.1054$\n\nThe set of residuals is approximately $\\{2.6912, -0.7071, 1.1068, 2.3094, 0.1054\\}$.\nNext, we compute the empirical sample variance. The sample mean $\\bar{r}$ is:\n$$\n\\bar{r} = \\frac{1}{5}(2.6912 - 0.7071 + 1.1068 + 2.3094 + 0.1054) = \\frac{5.5057}{5} \\approx 1.1011\n$$\nThe sample variance $s^2$ is:\n$$\ns^2 = \\frac{1}{M-1} \\sum_{m=1}^M (r_{1,m} - \\bar{r})^2 = \\frac{1}{4} \\sum_{m=1}^5 (r_{1,m} - 1.1011)^2\n$$\n$$\ns^2 \\approx \\frac{1}{4}\\left[ (1.5901)^2 + (-1.8082)^2 + (0.0057)^2 + (1.2083)^2 + (-0.9957)^2 \\right]\n$$\n$$\ns^2 \\approx \\frac{1}{4}\\left[ 2.5284 + 3.2696 + 0.00003 + 1.4600 + 0.9914 \\right] = \\frac{8.2494}{4} \\approx 2.0624\n$$\nThe empirical variance $s^2 \\approx 2.0624$, which is strictly greater than $1$. An empirical variance of residuals significantly different from $1$ is a sign of model inadequacy. A variance greater than $1$ specifically indicates overdispersion, meaning the observed data is more variable than the model predicts. This can be caused by model misspecification (e.g., wrong kernel, missing dependencies) or violation of the assumptions (e.g., bins are too wide, invalidating the Poisson approximation). The statement is therefore correct, both in its calculation and its interpretation.\n**Verdict: Correct.**\n\n**D. Overdispersion in Pearson residuals cannot indicate missing network edges, because the adjacency matrix $\\{a_{ij}\\}$ only controls the mean structure and not the variance.**\n\nThis statement is fundamentally flawed. The adjacency weights $a_{ij}$ are critical components of the conditional intensity $\\lambda_i(t)$, which in turn determines the conditional mean $\\widehat{\\Lambda}_{i,m} = \\int_{B_m} \\widehat{\\lambda}_i(t)\\,\\mathrm{d}t$. In a model where conditional variance is tied to the conditional mean (as in the Poisson approximation), any parameter that affects the mean also affects the variance. If a true network edge $a_{ji}  0$ is omitted from the model (i.e., we set $\\widehat{a}_{ji}=0$), the fitted intensity $\\widehat{\\lambda}_i(t)$ will not respond to events in node $j$. This will cause systematic errors in the predicted mean count $\\widehat{\\Lambda}_{i,m}$. For instance, after an event in node $j$, the true intensity $\\lambda_i(t)$ will be higher than the model's $\\widehat{\\lambda}_i(t)$, leading to larger-than-expected counts $Y_{i,m}$ and thus large residuals. This mismatch increases the overall variance of the residuals, causing overdispersion. Therefore, overdispersion can absolutely be an indicator of missing network edges.\n**Verdict: Incorrect.**\n\n**E. Even under a correctly specified Hawkes model, Pearson residuals across different components within the same bin must be exactly independent and uncorrelated for any bin width $\\Delta$.**\n\nThis is incorrect. The Hawkes process is defined by its triggering mechanism. An event in component $j$ at time $s$ increases the intensity of component $i$ at times $ts$. If an event from component $j$ occurs within a bin $B_m=(t_m, t_{m+1}]$, it can increase the probability of an event from component $i$ *within the same bin*. This creates a correlation between the counts $Y_{i,m}$ and $Y_{j,m}$ conditional on the history $\\mathcal{H}_{t_m}$. Since the residuals $r_{i,m}$ and $r_{j,m}$ are functions of these counts, they too will be correlated. This intra-bin correlation only vanishes in the limit as the bin width $\\Delta \\to 0$. For any finite bin width $\\Delta  0$, the residuals across components are not, in general, independent or uncorrelated if there is any interaction ($\\phi_{ij} \\neq 0$ or $\\phi_{ji} \\neq 0$).\n**Verdict: Incorrect.**",
            "answer": "$$\\boxed{AC}$$"
        }
    ]
}