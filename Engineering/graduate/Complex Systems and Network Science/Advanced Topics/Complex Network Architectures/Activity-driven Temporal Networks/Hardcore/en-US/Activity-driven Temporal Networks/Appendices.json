{
    "hands_on_practices": [
        {
            "introduction": "A crucial step in applying theoretical models to real-world phenomena is the ability to infer model parameters from observational data. This practice grounds the abstract concept of node activity in a concrete statistical task. By deriving the Maximum Likelihood Estimator for a node's activity rate under a Poisson assumption , you will connect the generative mechanism of the activity-driven model to the principles of statistical inference, a fundamental skill for any quantitative scientist.",
            "id": "4262221",
            "problem": "Consider an activity-driven temporal network in which each node initiates activation events that create ephemeral links. Focus on a single node $i$. Assume that the activation events of node $i$ form a homogeneous Poisson process with constant rate (activity) $a_i$ events per unit time. You observe an activation time series over a fixed window $[0,T]$, where $T>0$ is known and finite, consisting of the event timestamps $\\{t_1, t_2, \\dots, t_{N}\\}$ with $0 < t_1 < t_2 < \\dots < t_N \\leq T$, and $N$ is the total number of observed activations in $[0,T]$.\n\nStarting from fundamental definitions of a homogeneous Poisson process and the likelihood principle, derive the Maximum Likelihood Estimator (MLE) for the node activity $a_i$ based on the observed activation time series, and derive the variance of this estimator under the Poisson model. Your derivation must begin from first principles (definitions of a homogeneous Poisson process and the likelihood of observed point patterns), and proceed without invoking shortcut formulas that are not derived from these principles. Express your final answer as closed-form analytic expressions. No rounding is required. Do not include units in your final answer.",
            "solution": "The problem requires the derivation of the Maximum Likelihood Estimator (MLE) for the activity rate $a_i$ of a node in an activity-driven network, modeled as a homogeneous Poisson process, and the variance of this estimator. The derivation must proceed from first principles.\n\nLet the activation events of node $i$ be described by a homogeneous Poisson process with a constant rate $a_i > 0$. We observe this process over a fixed time interval $[0, T]$, where $T > 0$. The observation consists of $N$ event timestamps $\\{t_1, t_2, \\dots, t_N\\}$ such that $0 < t_1 < t_2 < \\dots < t_N \\leq T$.\n\nFirst, we establish the likelihood function for the observed event data. For a general one-dimensional point process characterized by a time-dependent intensity function $\\lambda(t)$, the likelihood of observing $N$ events at specific times $\\{t_1, \\dots, t_N\\}$ within the interval $[0, T]$ is given by the expression:\n$$L = \\left( \\prod_{j=1}^{N} \\lambda(t_j) \\right) \\exp\\left( -\\int_0^T \\lambda(s) ds \\right)$$\nThis formula arises from considering the probability of an event occurring in each infinitesimal interval $[t_j, t_j+dt_j]$ and no events occurring elsewhere.\n\nFor a homogeneous Poisson process, the intensity function is constant over time, i.e., $\\lambda(t) = a_i$ for all $t \\in [0, T]$. The parameter to be estimated is $a_i$. Substituting this constant rate into the general likelihood formula, we get the likelihood function for the parameter $a_i$ given the observed data:\n$$L(a_i | \\{t_1, \\dots, t_N\\}) = \\left( \\prod_{j=1}^{N} a_i \\right) \\exp\\left( -\\int_0^T a_i ds \\right)$$\nThe product term simplifies to $a_i^N$. The integral in the exponent evaluates to:\n$$\\int_0^T a_i ds = a_i \\int_0^T ds = a_i T$$\nThus, the likelihood function becomes:\n$$L(a_i) = a_i^N \\exp(-a_i T)$$\nIt is noteworthy that the likelihood function depends only on the total number of events $N$ and the duration of the observation window $T$, and not on the specific times $\\{t_1, \\dots, t_N\\}$ at which the events occurred. This is a fundamental property of the homogeneous Poisson process.\n\nTo find the Maximum Likelihood Estimator (MLE), denoted $\\hat{a}_i$, we must find the value of $a_i$ that maximizes $L(a_i)$. It is mathematically more convenient to maximize the natural logarithm of the likelihood function, known as the log-likelihood function, $\\ell(a_i) = \\ln(L(a_i))$. Since the logarithm is a monotonically increasing function, maximizing $\\ell(a_i)$ is equivalent to maximizing $L(a_i)$.\n$$\\ell(a_i) = \\ln(a_i^N \\exp(-a_i T)) = \\ln(a_i^N) + \\ln(\\exp(-a_i T))$$\n$$\\ell(a_i) = N \\ln(a_i) - a_i T$$\nTo find the maximum, we compute the first derivative of $\\ell(a_i)$ with respect to $a_i$ and set it to zero.\n$$\\frac{d\\ell}{da_i} = \\frac{d}{da_i} (N \\ln(a_i) - a_i T) = \\frac{N}{a_i} - T$$\nSetting the derivative to zero gives the equation for the MLE:\n$$\\frac{N}{\\hat{a}_i} - T = 0$$\n$$\\frac{N}{\\hat{a}_i} = T$$\nSolving for $\\hat{a}_i$, we obtain the Maximum Likelihood Estimator:\n$$\\hat{a}_i = \\frac{N}{T}$$\nTo confirm that this corresponds to a maximum, we examine the second derivative of the log-likelihood function:\n$$\\frac{d^2\\ell}{da_i^2} = \\frac{d}{da_i} \\left(\\frac{N}{a_i} - T\\right) = -\\frac{N}{a_i^2}$$\nSince the number of events $N \\geq 0$ and the rate $a_i$ must be positive, $a_i^2 > 0$. Therefore, $\\frac{d^2\\ell}{da_i^2} \\leq 0$. For any non-zero number of events ($N > 0$), the second derivative is strictly negative, confirming that $\\hat{a}_i = N/T$ is indeed the unique maximum.\n\nNext, we derive the variance of this estimator, $\\text{Var}(\\hat{a}_i)$. The estimator $\\hat{a}_i = \\frac{N}{T}$ is a function of the random variable $N$, the number of events in the interval $[0, T]$. The observation window duration $T$ is a known, fixed constant. Using the property of variance, $\\text{Var}(cX) = c^2 \\text{Var}(X)$ for a constant $c$ and random variable $X$, we can write:\n$$\\text{Var}(\\hat{a}_i) = \\text{Var}\\left(\\frac{N}{T}\\right) = \\left(\\frac{1}{T}\\right)^2 \\text{Var}(N) = \\frac{1}{T^2} \\text{Var}(N)$$\nTo proceed, we need the variance of $N$. According to the definition of a homogeneous Poisson process with rate $a_i$, the number of events $N$ in a time interval of length $T$ follows a Poisson distribution with a mean parameter $\\lambda = a_i T$.\n$$N \\sim \\text{Poisson}(a_i T)$$\nA fundamental property of the Poisson distribution is that its variance is equal to its mean. Therefore:\n$$E[N] = a_i T$$\n$$\\text{Var}(N) = a_i T$$\nSubstituting this result for $\\text{Var}(N)$ into our expression for the variance of the estimator:\n$$\\text{Var}(\\hat{a}_i) = \\frac{1}{T^2} (a_i T) = \\frac{a_i}{T}$$\nThis expression gives the theoretical variance of the MLE $\\hat{a}_i$. It depends on the true, unknown parameter $a_i$ and the observation duration $T$.\n\nIn summary, the MLE for the activity rate is the observed number of events divided by the duration of the observation window, and its variance is the true activity rate divided by the duration.\n\nThe two derived quantities are:\n1. The Maximum Likelihood Estimator for $a_i$: $\\hat{a}_i = \\frac{N}{T}$.\n2. The variance of this estimator: $\\text{Var}(\\hat{a}_i) = \\frac{a_i}{T}$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{N}{T} & \\frac{a_i}{T}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Activity-driven models are powerful because they explain how microscopic temporal dynamics shape the macroscopic structure of the integrated, static network. This exercise explores the emergence of a key structural feature: the heavy-tailed distribution of interaction weights. You will use a rate equation approach to show how a simple reinforcement rule—where past interactions make future ones more likely—naturally generates a scale-free weight distribution , providing insight into the origins of heterogeneity in complex networks.",
            "id": "4262207",
            "problem": "Consider an activity-driven temporal network with $N$ nodes. Each node $i$ is endowed with an activity rate $a_i$ (probability per unit time of initiating an interaction), and time evolves in discrete interaction events. When node $i$ becomes active, it generates exactly one interaction event, selecting a partner and updating the integrated network as follows. Under a simple reinforcement rule, at each activation event of node $i$:\n- With probability $\\nu \\in (0,1)$, node $i$ connects to a previously uncontacted partner $j$, thereby creating a new tie $(i,j)$ in the integrated network with initial weight $w_{ij}=1$.\n- With probability $1-\\nu$, node $i$ selects one of its existing partners $j$ with probability proportional to the current tie weight $w_{ij}$ of $(i,j)$ and reinforces this tie by increasing $w_{ij}$ by $1$.\n\nAssume that $N$ is sufficiently large so that the probability of exhausting new partners is negligible over the observation window. Let the integrated network after a long observation time be the undirected, weighted network obtained by counting every interaction as a unit increment to the weight of the corresponding tie. You may assume the following foundational facts and definitions:\n- In the activity-driven model, each activation of node $i$ produces exactly one interaction that contributes $1$ to the total incident weight of node $i$ in the integrated network.\n- Proportional selection to current weight implements linear reinforcement, so that ties with larger weights are more likely to be reinforced.\n\nStarting from these bases and using first principles, derive the asymptotic form of the distribution of tie weights $P(w)$ in the integrated network and determine the heavy-tailed exponent $\\gamma$ such that $P(w) \\propto w^{-\\gamma}$ for large $w$ under the simple reinforcement rule described above. Express your final answer for the exponent $\\gamma$ as a closed-form symbolic function of $\\nu$. No intermediate or final expressions should depend on $N$ or the $a_i$. If you introduce approximations, justify them using appropriate scaling arguments. The final answer must be a single closed-form analytical expression. If you choose to provide a numerical example, you must also provide the exact symbolic form and state any rounding requirements in significant figures; however, the requested final answer is symbolic and must not include units.",
            "solution": "The problem asks for the asymptotic exponent $\\gamma$ of the tie weight distribution $P(w) \\propto w^{-\\gamma}$ in an activity-driven network with a specific reinforcement mechanism. The process can be modeled using a rate equation (master equation) approach, which is a standard method for analyzing such stochastic growth processes in network science.\n\nLet us define $N_w(t)$ as the expected number of ties with weight $w$ at time $t$. The time variable $t$ will be measured in units of total interaction events in the network. At each event, one node becomes active and creates exactly one interaction, which increases the total weight in the network by $1$. Therefore, if the network starts with zero weight at $t=0$, the total weight of all ties in the network at time $t$ is exactly $t$. Let this total weight be $W(t)$.\n\n$$W(t) = \\sum_{w=1}^{\\infty} w N_w(t) = t$$\n\nWe can now formulate the master equation for the evolution of $N_w(t)$. A tie's weight changes from $w$ to $w+1$ upon reinforcement. A new tie is created with weight $w=1$.\n\nThe change in the number of ties with weight $w$, $N_w$, is determined by the balance of two processes: the rate at which ties of weight $w-1$ are reinforced to become weight $w$, and the rate at which ties of weight $w$ are reinforced to become weight $w+1$.\n\nAccording to the problem description, an interaction event consists of two possibilities:\n1.  With probability $\\nu$, a new tie is created. This tie has weight $w=1$. Thus, new ties are created at a rate of $\\nu$ per event (or per unit time $t$). This process increases $N_1$.\n2.  With probability $1-\\nu$, an existing tie is reinforced. The selection of the tie to be reinforced is proportional to its weight $w$. The total 'attractiveness' for reinforcement is the sum of all weights in the network, which is $W(t)=t$. The probability of picking a specific tie with weight $w$ is proportional to $w/W(t)$. The total weight of all ties that have weight $w$ is $w N_w(t)$. Therefore, the probability that a reinforcement event targets any of the ties of weight $w$ is $\\frac{w N_w(t)}{W(t)}$.\n\nThe rate at which ties of weight $w$ are reinforced (and thus removed from the population $N_w$) is the product of the probability of a reinforcement event, $1-\\nu$, and the probability of selecting a tie of weight $w$ for this reinforcement:\n$$ \\text{Rate of } w \\to w+1 = (1-\\nu) \\frac{w N_w(t)}{W(t)} = (1-\\nu) \\frac{w N_w(t)}{t} $$\nSimilarly, the rate at which ties of weight $w-1$ are reinforced to become weight $w$ (and thus added to the population $N_w$) is:\n$$ \\text{Rate of } w-1 \\to w = (1-\\nu) \\frac{(w-1) N_{w-1}(t)}{W(t)} = (1-\\nu) \\frac{(w-1) N_{w-1}(t)}{t} $$\n\nFor $w > 1$, the master equation for $N_w(t)$ is the balance of these two rates:\n$$ \\frac{dN_w}{dt} = (1-\\nu) \\frac{(w-1) N_{w-1}(t)}{t} - (1-\\nu) \\frac{w N_w(t)}{t} $$\nFor $w=1$, we must also include the creation of new ties at rate $\\nu$:\n$$ \\frac{dN_1}{dt} = \\nu - (1-\\nu) \\frac{1 \\cdot N_1(t)}{t} $$\n\nWe are interested in the asymptotic behavior for large $t$. In this limit, the network grows, and the number of ties of any given weight is expected to grow linearly with time. This suggests a scaling solution of the form $N_w(t) = c_w t$, where $c_w$ is the constant proportion of ties of weight $w$ relative to the total number of events. The total number of ties in the network is $N_{\\text{edges}}(t) = \\sum_w N_w(t)$. Since new ties are formed at a rate $\\nu$, we have $N_{\\text{edges}}(t) = \\nu t$. Thus, $\\sum_w c_w = \\nu$. The probability distribution of weights is $P(w) = N_w(t) / N_{\\text{edges}}(t) = (c_w t) / (\\nu t) = c_w/\\nu$.\n\nSubstituting $N_w(t) = c_w t$ into the master equations: $\\frac{d(c_w t)}{dt} = c_w$.\nFor $w > 1$:\n$$ c_w = (1-\\nu) \\frac{(w-1)c_{w-1}t}{t} - (1-\\nu) \\frac{w c_w t}{t} $$\n$$ c_w = (1-\\nu) \\left[ (w-1)c_{w-1} - w c_w \\right] $$\n$$ c_w \\left[ 1 + w(1-\\nu) \\right] = (1-\\nu)(w-1)c_{w-1} $$\nThis gives the recurrence relation for the coefficients $c_w$:\n$$ \\frac{c_w}{c_{w-1}} = \\frac{(w-1)(1-\\nu)}{1 + w(1-\\nu)} $$\n\nFor $w=1$:\n$$ c_1 = \\nu - (1-\\nu) \\frac{c_1 t}{t} = \\nu - (1-\\nu)c_1 $$\n$$ c_1 \\left[ 1 + (1-\\nu) \\right] = \\nu \\implies c_1(2-\\nu) = \\nu $$\n\nWe seek a power-law form for the distribution $P(w) \\propto w^{-\\gamma}$ for large $w$. Since $P(w) \\propto c_w$, we assume $c_w \\propto w^{-\\gamma}$.\nFor large $w$, we can analyze the asymptotic behavior of the recurrence relation:\n$$ \\frac{c_w}{c_{w-1}} = \\frac{(w-1)(1-\\nu)}{w(1-\\nu) + 1} = \\frac{w\\left(1 - \\frac{1}{w}\\right)(1-\\nu)}{w\\left(1-\\nu + \\frac{1}{w}\\right)} = \\frac{1-\\frac{1}{w}}{1 + \\frac{1}{w(1-\\nu)}} $$\nUsing the Taylor expansion $(1+x)^{-1} \\approx 1-x$ for small $x$:\n$$ \\frac{c_w}{c_{w-1}} \\approx \\left(1 - \\frac{1}{w}\\right) \\left(1 - \\frac{1}{w(1-\\nu)}\\right) \\approx 1 - \\frac{1}{w} - \\frac{1}{w(1-\\nu)} + O\\left(\\frac{1}{w^2}\\right) $$\n$$ \\frac{c_w}{c_{w-1}} \\approx 1 - \\frac{1}{w}\\left(1 + \\frac{1}{1-\\nu}\\right) $$\n\nNow, consider the ratio from the power-law assumption $c_w = A w^{-\\gamma}$:\n$$ \\frac{c_w}{c_{w-1}} = \\frac{A w^{-\\gamma}}{A (w-1)^{-\\gamma}} = \\left(\\frac{w-1}{w}\\right)^{\\gamma} = \\left(1 - \\frac{1}{w}\\right)^{\\gamma} $$\nUsing the binomial expansion $(1-x)^a \\approx 1-ax$ for small $x$:\n$$ \\frac{c_w}{c_{w-1}} \\approx 1 - \\frac{\\gamma}{w} $$\n\nBy comparing the coefficients of the $1/w$ term in both approximations for the ratio $c_w/c_{w-1}$, we find the exponent $\\gamma$:\n$$ \\gamma = 1 + \\frac{1}{1-\\nu} $$\nThis can be simplified to a single fraction:\n$$ \\gamma = \\frac{1-\\nu}{1-\\nu} + \\frac{1}{1-\\nu} = \\frac{1-\\nu+1}{1-\\nu} = \\frac{2-\\nu}{1-\\nu} $$\nThis expression for $\\gamma$ is a function of $\\nu$ only, as required, and holds for large $w$. The derivation relies on a mean-field rate equation approach, which is justified in the asymptotic limit for a large network.\nThe exponent for the distribution of tie weights $P(w)$ is therefore $\\gamma = \\frac{2-\\nu}{1-\\nu}$.",
            "answer": "$$\\boxed{\\frac{2-\\nu}{1-\\nu}}$$"
        },
        {
            "introduction": "While basic models offer foundational insights, real-world systems often exhibit more complex behaviors like memory. This advanced practice challenges you to extend the activity-driven framework by incorporating a memory-dependent probability of forming new connections. By constructing and solving a master equation using a mean-field approximation , you will analyze how a node's interaction history influences its future growth, developing the mathematical skills necessary to tackle more sophisticated, non-Markovian temporal network models.",
            "id": "4262244",
            "problem": "Consider an activity-driven temporal network with a very large number of nodes so that the probability of encountering an as-yet-unseen neighbor is limited only by the focal node’s own memory. Focus on a single focal node $i$ with constant activity rate $a&gt;0$. Time is continuous. When node $i$ activates, which occurs as a Poisson process of rate $a$, it initiates exactly one contact. Each initiated contact increases the node’s strength (total interaction count) $s_i(t)$ by $1$. With probability $p_{\\mathrm{new}}(s_i)$ the contact is with a previously unseen neighbor, in which case the node’s degree $k_i(t)$ increases by $1$ and a new edge with initial weight $1$ is created; with the complementary probability $1-p_{\\mathrm{new}}(s_i)$, the contact is with one of the existing neighbors chosen proportionally to the current edge weight, which increases that edge’s weight by $1$ but leaves $k_i(t)$ unchanged. Assume the memory rule\n$$\np_{\\mathrm{new}}(s) \\;=\\; \\left(1+\\frac{s}{s_0}\\right)^{-\\beta},\n$$\nwith parameters $s_0&gt;0$ and $\\beta\\in(0,1)$. Assume that at time $t=0$ the node has degree $k_i(0)=k_0\\ge 0$ and strength $s_i(0)=0$. Ignore contacts received when other nodes activate, i.e., only the focal node’s own activations contribute to $s_i(t)$ and $k_i(t)$.\n\n(a) Using only the definitions above and standard continuous-time Markov jump process principles, write a master equation for the joint probability $P(k,s,t)$ that the focal node has degree $k$ and strength $s$ at time $t$. You may treat $k$ and $s$ as nonnegative integers and express the evolution in terms of transitions induced by the focal node’s activations.\n\n(b) From your master equation, derive closed evolution equations for the expectations $\\langle s_i(t)\\rangle$ and $\\langle k_i(t)\\rangle$ under a mean-field closure that replaces $\\langle p_{\\mathrm{new}}(s_i)\\rangle$ by $p_{\\mathrm{new}}(\\langle s_i\\rangle)$. Justify the closure at the level of scaling for large $t$ and large $s_i(t)$.\n\n(c) Solve the resulting closed equations to obtain an explicit expression for $\\langle k_i(t)\\rangle$ and then extract its leading-order asymptotic growth for $t\\to\\infty$. Express your final answer as a single closed-form analytic expression in terms of $a$, $s_0$, and $\\beta$. No numerical evaluation is required. Provide your final answer as the leading-order asymptotic expression for $\\langle k_i(t)\\rangle$ as $t\\to\\infty$.",
            "solution": "The problem asks for a multi-part analysis of a specific activity-driven temporal network model focusing on the evolution of a single node's degree and strength. The process will be addressed in three steps: (a) deriving the master equation for the joint probability of degree and strength, (b) deriving and justifying mean-field equations for the expected degree and strength, and (c) solving these equations to find the long-time asymptotic behavior of the expected degree.\n\n(a) Master Equation for $P(k,s,t)$\n\nLet the state of the focal node $i$ at time $t$ be described by the pair of non-negative integers $(k,s)$, where $k=k_i(t)$ is its degree and $s=s_i(t)$ is its strength. The evolution of the system is a continuous-time Markov jump process. We denote the probability of being in state $(k,s)$ at time $t$ as $P(k,s,t)$.\n\nActivations of the focal node occur as a Poisson process with a constant rate $a>0$. Each activation causes a state transition. The strength $s$ increases by exactly $1$ at every activation. The degree $k$ increases by $1$ only if a new neighbor is contacted, which occurs with probability $p_{\\mathrm{new}}(s)$. Otherwise, $k$ remains unchanged.\n\nThe master equation describes the time evolution of $P(k,s,t)$ by balancing the probability flow into and out of the state $(k,s)$. The rate of change of $P(k,s,t)$ is given by:\n$$\n\\frac{\\partial P(k,s,t)}{\\partial t} = (\\text{rate of transitions into } (k,s)) - (\\text{rate of transitions out of } (k,s))\n$$\nA transition out of state $(k,s)$ occurs whenever the node activates, which happens with rate $a$. Thus, the total loss term is $-a P(k,s,t)$.\n\nA transition into state $(k,s)$ must have originated from a state with strength $s-1$ just before an activation. There are two possibilities for the preceding state:\n$1$. The system was in state $(k-1, s-1)$. The node activated (rate $a$), and a new neighbor was contacted. This occurs with probability $p_{\\mathrm{new}}(s-1)$. This contributes a gain term $a P(k-1, s-1, t) p_{\\mathrm{new}}(s-1)$.\n$2$. The system was in state $(k, s-1)$. The node activated (rate $a$), and an existing neighbor was contacted. This occurs with probability $1-p_{\\mathrm{new}}(s-1)$. This contributes a gain term $a P(k, s-1, t) (1 - p_{\\mathrm{new}}(s-1))$.\n\nCombining these terms, the master equation for $s \\ge 1$ is:\n$$\n\\frac{\\partial P(k,s,t)}{\\partial t} = a P(k-1, s-1, t) p_{\\mathrm{new}}(s-1) + a P(k, s-1, t) (1 - p_{\\mathrm{new}}(s-1)) - a P(k,s,t)\n$$\nThe initial conditions are $s_i(0)=0$ and $k_i(0)=k_0$. This means $P(k,s,0) = \\delta_{k,k_0} \\delta_{s,0}$, where $\\delta_{ij}$ is the Kronecker delta. The state space is restricted to integers $s \\ge 0$ and $k \\ge k_0$. We define $P(k,s,t)=0$ if $k < k_0$ or $s < 0$. This convention automatically handles the boundary at $k=k_0$, where the term $P(k_0-1, s-1, t)$ is zero. When $k=k_0$ and $s \\ge 1$, the equation becomes:\n$$\n\\frac{\\partial P(k_0,s,t)}{\\partial t} = a P(k_0, s-1, t) (1 - p_{\\mathrm{new}}(s-1)) - a P(k_0,s,t)\n$$\nFor the initial state $(k_0,0)$, the only process is a transition out. The equation for its probability is $\\frac{d P(k_0,0,t)}{dt} = -a P(k_0,0,t)$, which with the initial condition $P(k_0,0,0)=1$ yields $P(k_0,0,t) = \\exp(-at)$.\n\n(b) Evolution Equations for Expectations $\\langle s_i(t)\\rangle$ and $\\langle k_i(t)\\rangle$\n\nThe time evolution of the expectation of any function $f(k,s)$ can be derived from the master equation: $\\frac{d\\langle f \\rangle}{dt} = \\sum_{k,s} f(k,s) \\frac{\\partial P(k,s,t)}{\\partial t}$.\n\nFor the expected strength $\\langle s(t) \\rangle = \\sum_{k,s} s P(k,s,t)$:\n$$\n\\frac{d\\langle s \\rangle}{dt} = \\sum_{k,s \\ge 1} s \\left[ a P(k-1,s-1,t) p_{\\mathrm{new}}(s-1) + a P(k,s-1,t) (1-p_{\\mathrm{new}}(s-1)) \\right] - \\sum_{k,s} s a P(k,s,t)\n$$\nBy shifting summation indices in the gain terms ($k' = k-1, s' = s-1$ in the first term, and $s' = s-1$ in the second term), and then relabeling the dummy indices back to $k,s$, we get:\n$$\n\\frac{d\\langle s \\rangle}{dt} = a \\sum_{k,s} (s+1) P(k,s,t) p_{\\mathrm{new}}(s) + a \\sum_{k,s} (s+1) P(k,s,t) (1-p_{\\mathrm{new}}(s)) - a \\langle s \\rangle\n$$\n$$\n\\frac{d\\langle s \\rangle}{dt} = a \\sum_{k,s} (s+1) P(k,s,t) - a \\langle s \\rangle = a (\\langle s \\rangle + 1) - a \\langle s \\rangle = a\n$$\nSo, the evolution equation for the expected strength is $\\frac{d\\langle s(t) \\rangle}{dt} = a$.\n\nFor the expected degree $\\langle k(t) \\rangle = \\sum_{k,s} k P(k,s,t)$:\n$$\n\\fracd\\langle k \\rangle}{dt} = \\sum_{k,s} k \\left[ a P(k-1,s-1,t) p_{\\mathrm{new}}(s-1) + a P(k,s-1,t) (1-p_{\\mathrm{new}}(s-1)) \\right] - \\sum_{k,s} k a P(k,s,t)\n$$\nShifting indices as before ($k'=k-1, s'=s-1$ and $s'=s-1$):\n$$\n\\frac{d\\langle k \\rangle}{dt} = a \\sum_{k,s} (k+1) P(k,s,t) p_{\\mathrm{new}}(s) + a \\sum_{k,s} k P(k,s,t) (1-p_{\\mathrm{new}}(s)) - a \\langle k \\rangle\n$$\n$$\n\\frac{d\\langle k \\rangle}{dt} = a \\sum_{k,s} [ (k+1)p_{\\mathrm{new}}(s) + k(1-p_{\\mathrm{new}}(s)) - k ] P(k,s,t)\n$$\n$$\n\\frac{d\\langle k \\rangle}{dt} = a \\sum_{k,s} [ k p_{\\mathrm{new}}(s) + p_{\\mathrm{new}}(s) + k - k p_{\\mathrm{new}}(s) - k ] P(k,s,t) = a \\sum_{k,s} p_{\\mathrm{new}}(s) P(k,s,t)\n$$\nThis gives the exact equation $\\frac{d\\langle k(t) \\rangle}{dt} = a \\langle p_{\\mathrm{new}}(s) \\rangle$.\n\nWe now apply the mean-field closure approximation: $\\langle p_{\\mathrm{new}}(s) \\rangle \\approx p_{\\mathrm{new}}(\\langle s \\rangle)$. This approximation is justified for large times $t$. The strength $s(t)$ is the count of events in a Poisson process of rate $a$, so $\\langle s(t) \\rangle = at$ and $\\text{Var}(s(t)) = at$. The relative size of fluctuations, $\\sqrt{\\text{Var}(s(t))}/\\langle s(t) \\rangle = 1/\\sqrt{at}$, vanishes as $t \\to \\infty$. This implies that the distribution of $s(t)$ becomes sharply peaked around its mean $\\langle s(t) \\rangle$. For any sufficiently smooth function $f(s)$, a Taylor expansion around $\\langle s \\rangle$ gives $\\langle f(s) \\rangle \\approx f(\\langle s \\rangle) + \\frac{1}{2} \\text{Var}(s) f''(\\langle s \\rangle)$. For $p_{\\mathrm{new}}(s) = (1+s/s_0)^{-\\beta}$, the leading term is $p_{\\mathrm{new}}(\\langle s \\rangle) \\propto t^{-\\beta}$ for large $t$. The correction term is proportional to $\\text{Var}(s) \\cdot p_{\\mathrm{new}}''(\\langle s \\rangle) \\propto t \\cdot t^{-\\beta-2} = t^{-\\beta-1}$. Since $\\beta>0$, the correction term decays faster than the main term, justifying the closure for large $t$.\n\nThe closed system of equations for the expectations is:\n$$\n\\frac{d\\langle s(t) \\rangle}{dt} = a\n$$\n$$\n\\frac{d\\langle k(t) \\rangle}{dt} = a \\, p_{\\mathrm{new}}(\\langle s(t) \\rangle) = a \\left(1 + \\frac{\\langle s(t) \\rangle}{s_0}\\right)^{-\\beta}\n$$\n\n(c) Solution and Asymptotic Analysis\n\nWe solve the system of equations with initial conditions $\\langle s(0) \\rangle = 0$ and $\\langle k(0) \\rangle = k_0$.\nThe equation for $\\langle s(t) \\rangle$ is integrated directly:\n$$\n\\int_0^{\\langle s(t) \\rangle} d\\langle s \\rangle = \\int_0^t a \\, d\\tau \\implies \\langle s(t) \\rangle = at\n$$\nSubstituting this result into the equation for $\\langle k(t) \\rangle$:\n$$\n\\frac{d\\langle k(t) \\rangle}{dt} = a \\left(1 + \\frac{at}{s_0}\\right)^{-\\beta}\n$$\nWe integrate this equation with respect to time from $0$ to $t$:\n$$\n\\langle k(t) \\rangle - \\langle k(0) \\rangle = \\int_0^t a \\left(1 + \\frac{a\\tau}{s_0}\\right)^{-\\beta} d\\tau\n$$\n$$\n\\langle k(t) \\rangle = k_0 + a \\int_0^t \\left(1 + \\frac{a\\tau}{s_0}\\right)^{-\\beta} d\\tau\n$$\nTo evaluate the integral, we use the substitution $u = 1 + \\frac{a\\tau}{s_0}$, which implies $d\\tau = \\frac{s_0}{a} du$. The limits of integration become $u(0)=1$ and $u(t)=1+\\frac{at}{s_0}$.\n$$\n\\int_0^t a \\left(1 + \\frac{a\\tau}{s_0}\\right)^{-\\beta} d\\tau = a \\int_1^{1+at/s_0} u^{-\\beta} \\left(\\frac{s_0}{a}\\right) du = s_0 \\left[ \\frac{u^{1-\\beta}}{1-\\beta} \\right]_1^{1+at/s_0}\n$$\nSince $\\beta \\in (0,1)$, $1-\\beta \\neq 0$. The integral evaluates to:\n$$\n\\frac{s_0}{1-\\beta} \\left[ \\left(1 + \\frac{at}{s_0}\\right)^{1-\\beta} - 1^{1-\\beta} \\right] = \\frac{s_0}{1-\\beta} \\left[ \\left(1 + \\frac{at}{s_0}\\right)^{1-\\beta} - 1 \\right]\n$$\nThe full solution for the expected degree is:\n$$\n\\langle k(t) \\rangle = k_0 + \\frac{s_0}{1-\\beta} \\left[ \\left(1 + \\frac{at}{s_0}\\right)^{1-\\beta} - 1 \\right]\n$$\nTo find the leading-order asymptotic growth for $t \\to \\infty$, we analyze this expression for large $t$. As $t \\to \\infty$, the term $at/s_0$ dominates the $1$ inside the parenthesis:\n$$\n\\left(1 + \\frac{at}{s_0}\\right)^{1-\\beta} \\sim \\left(\\frac{at}{s_0}\\right)^{1-\\beta} \\quad \\text{for } t \\to \\infty\n$$\nSubstituting this into the expression for $\\langle k(t) \\rangle$:\n$$\n\\langle k(t) \\rangle \\sim k_0 + \\frac{s_0}{1-\\beta} \\left[ \\left(\\frac{at}{s_0}\\right)^{1-\\beta} - 1 \\right] = \\left( k_0 - \\frac{s_0}{1-\\beta} \\right) + \\frac{s_0}{1-\\beta} \\left(\\frac{a}{s_0}\\right)^{1-\\beta} t^{1-\\beta}\n$$\nSince $\\beta \\in (0,1)$, the exponent $1-\\beta$ is positive, so the term proportional to $t^{1-\\beta}$ grows with time and is the leading-order term. The constant terms are sub-leading. The leading-order asymptotic expression is therefore:\n$$\n\\langle k(t) \\rangle \\sim \\frac{s_0}{1-\\beta} \\left(\\frac{a}{s_0}\\right)^{1-\\beta} t^{1-\\beta}\n$$\nSimplifying the prefactor:\n$$\n\\frac{s_0}{1-\\beta} \\frac{a^{1-\\beta}}{s_0^{1-\\beta}} = \\frac{s_0^{1-(1-\\beta)} a^{1-\\beta}}{1-\\beta} = \\frac{s_0^{\\beta} a^{1-\\beta}}{1-\\beta}\n$$\nThus, the leading-order asymptotic growth is given by:\n$$\n\\langle k(t) \\rangle \\sim \\frac{a^{1-\\beta} s_0^{\\beta}}{1-\\beta} t^{1-\\beta}\n$$\nThis expression describes how the expected degree of the node grows sub-linearly in time, a characteristic feature of reinforcement and memory processes.",
            "answer": "$$\n\\boxed{\\frac{a^{1-\\beta} s_0^{\\beta}}{1-\\beta} t^{1-\\beta}}\n$$"
        }
    ]
}