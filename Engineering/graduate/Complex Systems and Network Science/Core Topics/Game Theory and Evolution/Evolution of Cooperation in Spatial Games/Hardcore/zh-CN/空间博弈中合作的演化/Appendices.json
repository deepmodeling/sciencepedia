{
    "hands_on_practices": [
        {
            "introduction": "合作策略能否在充满背叛者的环境中成功入侵，是演化博弈论中的一个核心问题。这个问题的答案通常取决于合作者与背叛者交界处的局部收益竞争。本练习将通过一个基础性的计算，让您推导在特定边界条件下合作策略的入侵适应度，从而揭示策略采纳背后的核心驱动力。通过这个练习，您将掌握分析策略传播微观动态的基本方法 。",
            "id": "4274998",
            "problem": "考虑一个无限 $k$-正则空间网络上的囚徒困境 (PD)，该困境采用捐赠博弈参数化，其中每个参与者都与其所有 $k$ 个邻居进行成对互动。在每次成对互动中，合作者支付成本 $c0$ 为伙伴带来收益 $bc$，而背叛者不支付成本也不提供收益。因此，对于任意单次互动，行参与者的收益为：双方合作得到 $R=b-c$，单方面合作对阵背叛得到 $S=-c$，单方面背叛对阵合作得到 $T=b$，双方背叛得到 $P=0$。一个参与者的总收益是其与所有 $k$ 个邻居进行成对互动所得收益的总和。\n\n关注一个半无限背叛者区域与其互补的合作者区域之间的笔直界面。考虑界面上相对两侧的一对相邻边界节点，一个为背叛者，一个为合作者，根据界面的规则性，两者都有完全相同的 $m$ 个合作者邻居和 $k-m$ 个背叛者邻居（其中 $1 \\leq m \\leq k-1$）。策略更新是异步的，并遵循强度参数为 $\\beta=1$ 的费米成对比较规则：当边界背叛者通过其共享边与边界合作者进行比较时，该背叛者会以一个取决于其收益差的概率采纳合作策略。\n\n将边界合作者的入侵适应度 $\\phi$ 定义为在所述比较事件中，边界背叛者采纳合作策略的条件概率。计算 $\\phi$ 的一个仅含 $k$ 和 $c$ 的闭式表达式。将最终答案表示为单个解析表达式；无需四舍五入。",
            "solution": "该问题陈述经评估具有科学依据、提法恰当且内容完整。它描述了网络演化博弈论中的一个经典模型。前提清晰，所要求的任务是直接应用所述原理。边界合作者和边界背叛者都拥有相同数量的合作者邻居 $m$ 这一关键条件，是一个特定的理论简化，其本身并不矛盾，并可由此进行明确的计算。因此，该问题被视为有效，可以推导出解。\n\n问题描述了捐赠博弈形式的囚徒困境。在单次成对互动中，合作者支付成本 $c0$ 为其伙伴提供收益 $bc$。背叛者不支付任何费用，也不提供任何收益。行参与者的收益由以下矩阵给出：\n$$\n\\begin{pmatrix}\nR  S \\\\\nT  P\n\\end{pmatrix}\n=\n\\begin{pmatrix}\nb-c  -c \\\\\nb  0\n\\end{pmatrix}\n$$\n其中 $R$ 为双方合作的收益，$S$ 为与背叛者合作的收益，$T$ 为对合作者背叛的收益，$P$ 为双方背叛的收益。在 $k$-正则网络上，一个参与者的总收益是其与所有 $k$ 个邻居互动所得收益的总和。\n\n我们需要考虑一个焦点合作者（记为 $C^*$）和一个相邻的焦点背叛者（记为 $D^*$）。根据问题陈述，这两个参与者都位于合作者区域和背叛者区域之间的边界上，并且具有相同的邻居构成：每个参与者都有 $m$ 个合作者邻居和 $k-m$ 个背叛者邻居，其中 $1 \\leq m \\leq k-1$。\n\n让我们计算焦点合作者 $C^*$ 的总收益 $\\Pi_C$。该参与者有 $m$ 个合作者邻居和 $k-m$ 个背叛者邻居。总收益是这些互动收益的总和：\n$$\n\\Pi_C = m \\cdot R + (k-m) \\cdot S\n$$\n代入 $R$ 和 $S$ 的表达式：\n$$\n\\Pi_C = m(b-c) + (k-m)(-c) = mb - mc - kc + mc\n$$\n$$\n\\Pi_C = mb - kc\n$$\n接下来，我们计算焦点背叛者 $D^*$ 的总收益 $\\Pi_D$。该参与者同样有 $m$ 个合作者邻居和 $k-m$ 个背叛者邻居。总收益是这些互动收益的总和：\n$$\n\\Pi_D = m \\cdot T + (k-m) \\cdot P\n$$\n代入 $T$ 和 $P$ 的表达式：\n$$\n\\Pi_D = m(b) + (k-m)(0)\n$$\n$$\n\\Pi_D = mb\n$$\n问题指出，策略更新遵循费米成对比较规则。入侵适应度 $\\phi$ 被定义为背叛者 $D^*$ 在与合作者 $C^*$ 比较时采纳其策略的条件概率。参与者 $i$ 采纳邻居 $j$ 策略的费米规则一般形式为：\n$$\nP(i \\to j) = \\frac{1}{1 + \\exp(-\\beta(\\Pi_j - \\Pi_i))}\n$$\n在本例中，参与者 $i$ 是背叛者 $D^*$，参与者 $j$ 是合作者 $C^*$。选择强度给定为 $\\beta = 1$。因此，概率 $\\phi$ 为：\n$$\n\\phi = \\frac{1}{1 + \\exp(-1 \\cdot (\\Pi_C - \\Pi_D))} = \\frac{1}{1 + \\exp(-(\\Pi_C - \\Pi_D))}\n$$\n为计算 $\\phi$，我们必须首先求出收益差 $\\Pi_C - \\Pi_D$：\n$$\n\\Pi_C - \\Pi_D = (mb - kc) - (mb) = -kc\n$$\n将这个差值代入 $\\phi$ 的表达式中：\n$$\n\\phi = \\frac{1}{1 + \\exp(-(-kc))}\n$$\n$$\n\\phi = \\frac{1}{1 + \\exp(kc)}\n$$\n根据问题要求，这个 $\\phi$ 的表达式仅含 $k$ 和 $c$。由于对两个焦点参与者作了对称邻居假设，参数 $b$ 和 $m$ 被消掉了。",
            "answer": "$$\n\\boxed{\\frac{1}{1 + \\exp(kc)}}\n$$"
        },
        {
            "introduction": "虽然简单的平均场（Mean-Field, MF）模型因其简洁性而广受欢迎，但它完全忽略了空间结构，因此预测往往存在偏差。为了获得更准确的结果，我们可以采用对近似（Pair Approximation, PA）等更高级的分析工具，它能够捕捉到网络中相邻节点间的局部策略关联。本练习要求您在雪堆博弈（Snowdrift game）的背景下，分别使用平均场和对近似方法计算合作者的稳态频率，并比较两者的差异，从而定量地理解引入最基本的空间结构信息将如何显著改变演化的预测结果 。",
            "id": "4274916",
            "problem": "考虑一个由度为 $k$ 的规则随机图 (RRG) 构成的大种群，其中每个节点与其所有邻居进行双策略、双玩家的雪堆博弈（也称鹰鸽博弈）。设合作的收益为 $b0$，合作的成本为 $c0$，支付矩阵的条目由标准的雪堆博弈参数化定义：相互合作给予每个合作者支付 $R=b-\\frac{c}{2}$，单方面合作给予合作者支付 $S=b-c$、给予背叛者支付 $T=b$，相互背叛则各得支付 $P=0$。策略更新遵循弱选择下的成对比较（模仿），其中采纳概率与支付差异成单调关系。\n\n使用以下广泛接受的事实作为基本依据：\n- 在复制子动态下的双策略博弈的完全混合（平均场，MF）极限中，合作者密度 $x$ 的一个内部稳态 $x_{\\mathrm{MF}}$ 由合作者和背叛者的期望支付相等来确定，即 $R x + S (1-x) = T x + P (1-x)$。\n- 在弱选择和成对比较（模仿）下的 $k$-规则图上，对近似 (PA) 产生一个图上的有效复制子方程，该方程等价于使用变换后支付矩阵 $\\mathbf{A}'=\\mathbf{A}+\\frac{1}{k}\\left(\\mathbf{A}-\\mathbf{A}^{\\top}\\right)$ 的MF复制子，其中 $\\mathbf{A}=\\begin{pmatrix}R  S \\\\ T  P\\end{pmatrix}$ 且 ${}^{\\top}$ 表示转置。内部稳态 $x_{\\mathrm{PA}}$ 由使用 $\\mathbf{A}'$ 计算的期望支付相等来确定。\n\n对于特定的图度数和参数化，设 $k=4$，$b=1$，$c=0.6$。推导MF预测值 $x_{\\mathrm{MF}}$ 和PA预测值 $x_{\\mathrm{PA}}$，并计算数值差异 $\\Delta=x_{\\mathrm{PA}}-x_{\\mathrm{MF}}$。将 $\\Delta$ 表示为无单位小数，并四舍五入至四位有效数字。",
            "solution": "该问题要求计算在雪堆博弈中，基于规则随机图的对近似 (PA) 预测的合作者频率与基于平均场 (MF) 理论预测的频率之间的差异。该问题定义明确，其前提与演化博弈论中的标准模型一致。\n\n首先，我们使用给定的参数 $b=1$ 和 $c=0.6$ 来定义支付矩阵 $\\mathbf{A}=\\begin{pmatrix} R  S \\\\ T  P \\end{pmatrix}$。\n支付条目如下：\n$R = b - \\frac{c}{2} = 1 - \\frac{0.6}{2} = 1 - 0.3 = 0.7$\n$S = b - c = 1 - 0.6 = 0.4$\n$T = b = 1$\n$P = 0$\n因此，支付矩阵为 $\\mathbf{A} = \\begin{pmatrix} 0.7  0.4 \\\\ 1  0 \\end{pmatrix}$。\n在雪堆博弈中，稳定混合策略均衡的条件是 $T  R  S  P$，对应于 $1  0.7  0.4  0$。此条件已满足。\n\n接下来，我们计算合作者频率的平均场 (MF) 预测值 $x_{\\mathrm{MF}}$。内部稳态出现在合作者期望支付 $E_C$ 与背叛者期望支付 $E_D$ 相等之处。设 $x$ 为合作者的频率。\n$$E_C = x R + (1-x) S$$\n$$E_D = x T + (1-x) P$$\n在 $x=x_{\\mathrm{MF}}$ 时令 $E_C = E_D$：\n$$x_{\\mathrm{MF}} R + (1-x_{\\mathrm{MF}}) S = x_{\\mathrm{MF}} T + (1-x_{\\mathrm{MF}}) P$$\n求解 $x_{\\mathrm{MF}}$ 得到通用公式：\n$$x_{\\mathrm{MF}} = \\frac{P-S}{R-S-T+P}$$\n代入数值支付值：\n$$x_{\\mathrm{MF}} = \\frac{0 - 0.4}{0.7 - 0.4 - 1 + 0} = \\frac{-0.4}{-0.7} = \\frac{4}{7}$$\n\n现在，我们计算对近似 (PA) 的预测值 $x_{\\mathrm{PA}}$。根据问题陈述，这由一个有效支付矩阵 $\\mathbf{A}' = \\mathbf{A} + \\frac{1}{k}(\\mathbf{A} - \\mathbf{A}^{\\top})$ 决定，其中图的度数 $k=4$。\n首先，我们计算 $\\mathbf{A}-\\mathbf{A}^{\\top}$：\n$$\\mathbf{A} - \\mathbf{A}^{\\top} = \\begin{pmatrix} 0.7  0.4 \\\\ 1  0 \\end{pmatrix} - \\begin{pmatrix} 0.7  1 \\\\ 0.4  0 \\end{pmatrix} = \\begin{pmatrix} 0  -0.6 \\\\ 0.6  0 \\end{pmatrix}$$\n然后我们计算 $k=4$ 时的修正项：\n$$\\frac{1}{k}(\\mathbf{A} - \\mathbf{A}^{\\top}) = \\frac{1}{4} \\begin{pmatrix} 0  -0.6 \\\\ 0.6  0 \\end{pmatrix} = \\begin{pmatrix} 0  -0.15 \\\\ 0.15  0 \\end{pmatrix}$$\n有效支付矩阵 $\\mathbf{A}'$ 为：\n$$\\mathbf{A}' = \\mathbf{A} + \\frac{1}{k}(\\mathbf{A} - \\mathbf{A}^{\\top}) = \\begin{pmatrix} 0.7  0.4 \\\\ 1  0 \\end{pmatrix} + \\begin{pmatrix} 0  -0.15 \\\\ 0.15  0 \\end{pmatrix} = \\begin{pmatrix} 0.7  0.25 \\\\ 1.15  0 \\end{pmatrix}$$\n设这个新矩阵的条目为 $\\mathbf{A}'=\\begin{pmatrix} R'  S' \\\\ T'  P' \\end{pmatrix}$。因此，$R'=0.7$，$S'=0.25$，$T'=1.15$，$P'=0$。\nPA稳态 $x_{\\mathrm{PA}}$ 使用相同的均衡条件形式，但使用有效支付来计算：\n$$x_{\\mathrm{PA}} = \\frac{P'-S'}{R'-S'-T'+P'}$$\n代入新的支付值：\n$$x_{\\mathrm{PA}} = \\frac{0 - 0.25}{0.7 - 0.25 - 1.15 + 0} = \\frac{-0.25}{0.45 - 1.15} = \\frac{-0.25}{-0.7} = \\frac{25}{70} = \\frac{5}{14}$$\n\n最后，我们计算差值 $\\Delta = x_{\\mathrm{PA}} - x_{\\mathrm{MF}}$。\n$$\\Delta = \\frac{5}{14} - \\frac{4}{7} = \\frac{5}{14} - \\frac{8}{14} = -\\frac{3}{14}$$\n为了提供数值答案，我们将此分数转换为小数：\n$$\\Delta = -\\frac{3}{14} \\approx -0.2142857...$$\n四舍五入到四位有效数字，我们得到：\n$$\\Delta \\approx -0.2143$$\n这个负值表明，在成对比较更新规则下的雪堆博弈中，由对近似所捕捉的空间结构导致合作者的频率低于完全混合种群中的频率。",
            "answer": "$$\\boxed{-0.2143}$$"
        },
        {
            "introduction": "当我们从解析模型转向直接的计算模拟时，一些看似微小的执行细节可能会对系统宏观的演化结果产生深远影响。本练习聚焦于空间博弈模型中最基本的一个设计选择：更新机制。通过亲手实现并比较同步更新（synchronous update）与异步更新（asynchronous update）两种模式，您将亲身体验到个体间互动的时间顺序是如何改变合作在空间格局上的演化路径的，这对于任何计算建模者来说都是至关重要的一课 。",
            "id": "4274870",
            "problem": "要求您设计并实现一个受控计算实验，比较二维晶格上空间博弈中合作演化的两种更新机制——同步更新和异步更新。其基本基础包括囚徒困境（PD）的定义以及空间演化博弈论中的局部模仿动态。囚徒困境（PD）由合作 $C$ 与背叛 $D$ 之间的双策略互动定义，其收益由矩阵条目给出：$R$（相互合作的奖励）、$S$（对阵背叛者时合作的傻瓜收益）、$T$（对阵合作者时背叛的诱惑）以及 $P$（相互背叛的惩罚），并受限于 $T  R  P  S$。\n\n您将考虑一个大小为 $L \\times L$ 的方形晶格，具有周期性边界条件（一个环面）。每个格点 $i$ 持有一个策略 $s_i \\in \\{0,1\\}$，其中 $1$ 表示合作，$0$ 表示背叛。互动仅限于冯·诺依曼邻域：对于坐标为 $(r,c)$ 的中心格点，其邻居是 $(r-1 \\bmod L, c)$、$(r+1 \\bmod L, c)$、$(r, c-1 \\bmod L)$ 和 $(r, c+1 \\bmod L)$。\n\n格点 $i$ 的总收益 $\\pi_i$ 是其与四个邻居进行成对互动所得收益的总和。格点 $i$ 从与邻居 $j$ 的互动中获得的成对收益为\n$$\n\\text{pay}(s_i, s_j) = s_i s_j R + s_i (1 - s_j) S + (1 - s_i) s_j T + (1 - s_i) (1 - s_j) P,\n$$\n格点 $i$ 的总收益为\n$$\n\\pi_i = \\sum_{j \\in \\mathcal{N}(i)} \\text{pay}(s_i, s_j),\n$$\n其中 $\\mathcal{N}(i)$ 表示 $i$ 的四个邻居的集合。\n\n必须在相同的初始条件和参数下比较两种更新机制：\n\n- 同步更新：在离散时间 $t$，使用当前构型 $s(t)$ 计算所有格点的收益 $\\pi_i(t)$。然后，对于所有格点 $i$，通过模仿在时间 $t$ 时 $\\{i\\} \\cup \\mathcal{N}(i)$ 中具有最大收益的候选者的策略来同时更新 $s_i(t+1)$。平局必须通过候选者的有序列表 $[i, \\text{up}, \\text{down}, \\text{left}, \\text{right}]$ 来确定性地打破，选择此列表中最先出现的具有最大收益的候选者。\n\n- 异步更新：在离散时间 $t$，按确定的行主序迭代格点：$(0,0), (0,1), \\dots, (0,L-1), (1,0), \\dots, (L-1,L-1)$。对于每个中心格点 $i$，在更新 $i$ 之前，使用当前构型计算 $k \\in \\{i\\} \\cup \\mathcal{N}(i)$ 的当前收益 $\\pi_k(t)$。然后，立即通过模仿具有最大收益的候选者的策略来更新 $s_i$；平局必须使用相同的确定性列表 $[i, \\text{up}, \\text{down}, \\text{left}, \\text{right}]$ 来解决。使用更新后的构型继续处理下一个格点。在所有格点都处理过一次之后，一个异步时间步完成。\n\n初始条件：初始构型 $s(0)$ 是通过独立地将每个格点以概率 $f_0$ 设置为合作（$1$），否则设置为背叛（$0$）来生成的。使用指定的伪随机数生成器种子，以确保在每个测试用例中，两种更新机制的初始条件完全相同。\n\n您的任务是实现一个程序，对于每个指定的测试用例，从相同的初始构型开始，对给定的时间步数 $T$ 运行两种更新机制，并输出同步和异步动态最终合作者比例的绝对差值：\n$$\n\\Delta = \\left| \\frac{1}{L^2} \\sum_{i=1}^{L^2} s_i^{\\text{sync}}(T) - \\frac{1}{L^2} \\sum_{i=1}^{L^2} s_i^{\\text{async}}(T) \\right|.\n$$\n\n设计约束：\n- 所有计算必须在具有冯·诺依曼邻域和周期性边界条件的晶格上执行。\n- 模仿严格限于具有最大收益的单个候选者；不允许随机决胜。\n- 在每个测试用例中，同步和异步运行的初始条件必须相同，由给定的种子和 $f_0$ 决定。\n\n测试套件：\n实现以下四个测试用例，每个用例由元组 $(L, T, R, S, T_{\\text{payoff}}, P, f_0, \\text{seed})$ 定义：\n1. $(20, 100, 1.0, 0.0, 1.4, 0.0, 0.5, 12345)$：中等背叛诱惑，初始混合均衡；预期出现非平凡动态。\n2. $(20, 0, 1.0, 0.0, 1.4, 0.0, 0.5, 54321)$：零更新步数；差值 $\\Delta$ 必须等于 $0$。\n3. $(20, 200, 1.0, 0.0, 1.8, 0.0, 0.8, 98765)$：强背叛诱惑，初始合作度高；预期合作会减少。\n4. $(25, 150, 1.0, 0.0, 1.1, 0.0, 0.5, 24680)$：弱背叛诱惑；预期合作集群会持续存在。\n\n输出规范：\n您的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，顺序与测试套件一致，每个条目是相应测试用例的 $\\Delta$ 值（例如，$[\\Delta_1,\\Delta_2,\\Delta_3,\\Delta_4]$）。每个 $\\Delta$ 都必须是浮点数。",
            "solution": "用户提供的问题是空间演化博弈论领域一个定义明确的计算实验。它要求在二维晶格上，对囚徒困境问题比较同步和异步两种更新机制。该问题具有科学依据，内容自洽，并且算法规范足够精确，可以得到唯一且可验证的解。所有参数、边界条件、更新规则和决胜机制都已明确定义。因此，该问题被认为是有效的，下面提供了解决方案。\n\n### I. 模型构建与收益结构\n\n模型定义在一个 $L \\times L$ 的方形晶格上，具有周期性边界条件，形成一个环面拓扑。晶格上的每个格点 $i$ 都被一个持有策略 $s_i \\in \\{0, 1\\}$ 的智能体占据，其中 $s_i=1$ 代表合作（$C$），$s_i=0$ 代表背叛（$D$）。智能体与其冯·诺依曼邻域 $\\mathcal{N}(i)$ 定义的四个近邻进行互动。\n\n互动遵循囚徒困境规则，中心智能体的收益由以下方式给出：$R$ 代表相互合作，$S$ 代表合作而对手背叛（傻瓜收益），$T$ 代表背叛而对手合作（诱惑），$P$ 代表相互背叛。囚徒困境的定义性不等式为 $T  R  P  S$。\n\n格点 $i$ 处智能体的总收益 $\\pi_i$ 是其与四个邻居互动所得收益的总和：\n$$\n\\pi_i = \\sum_{j \\in \\mathcal{N}(i)} \\text{pay}(s_i, s_j)\n$$\n其中，成对收益函数定义为：\n$$\n\\text{pay}(s_i, s_j) = s_i s_j R + s_i (1 - s_j) S + (1 - s_i) s_j T + (1 - s_i) (1 - s_j) P\n$$\n这可以被简化。设 $k_i$ 为格点 $i$ 的合作邻居数量。那么背叛邻居的数量就是 $4 - k_i$。\n- 如果智能体 $i$ 是合作者（$s_i=1$），其总收益为 $\\pi_i = k_i R + (4 - k_i) S$。\n- 如果智能体 $i$ 是背叛者（$s_i=0$），其总收益为 $\\pi_i = k_i T + (4 - k_i) P$。\n\n这种表述是高效实现的关键，因为它允许通过首先计算邻居合作者数量的网格 $k_i$ 来计算整个晶格上的所有收益。这可以通过矢量化操作实现，例如，通过向四个方向移动策略网格并将结果相加。\n\n### II. 初始条件\n\n对于每个测试用例，模拟从一个由概率 $f_0$ 和特定伪随机数生成器种子决定的初始策略构型 $s(0)$ 开始。每个格点以概率 $f_0$ 被独立分配为合作策略（$s_i=1$）。使用固定的种子确保了在给定测试用例中，同步和异步运行的初始构型完全相同，这是进行公平比较的关键要求。\n\n### III. 动态更新机制\n\n策略的演化模拟总共进行 $T$ 个时间步。在每个步骤中，智能体通过模仿其局部环境中最成功的智能体来更新其策略，局部环境包括智能体自身及其四个冯·诺依曼邻居。“最成功”的智能体是指总收益最高的那个。平局通过选择有序列表 [self, up, down, left, right] 中的第一个智能体来确定性地打破。\n\n#### A. 同步更新\n\n在同步机制中，所有策略更新都基于时间步开始时的系统状态 $s(t)$，并同时应用以产生下一个时间步的状态 $s(t+1)$。\n\n单个同步时间步的算法如下：\n1.  **计算收益**：基于策略网格 $s(t)$，计算一个包含每个格点 $i$ 收益 $\\pi_i(t)$ 的收益网格 $\\Pi(t)$。如第一节所述，使用矢量化方法可以高效完成此步骤。\n2.  **确定胜者**：对于每个格点 $i$，从网格 $\\Pi(t)$ 中收集五个候选者（格点 $i$ 及其四个邻居）的收益。确定收益最高的候选者。通过按指定顺序检查候选者来应用确定性决胜规则。\n3.  **更新策略**：构建一个新的策略网格 $s(t+1)$。对于每个格点 $i$，其新策略 $s_i(t+1)$ 设置为步骤2中其获胜候选者的策略，即 $s_{\\text{winner}}(t)$。\n4.  **应用更新**：用 `next` 网格完全替换 `current` 网格。\n\n整个过程可以使用 `numpy` 进行高度矢量化。通过滚动完整的收益网格，可以收集所有格点的候选者收益。决胜规则通过 `numpy.argmax` 自然实现，该函数返回找到的第一个最大元素的索引。\n\n#### B. 异步更新\n\n在异步机制中，格点按确定的行主序顺序处理。每次更新都会立即应用，从而改变网格的状态，该序列中后续的智能体在同一时间步内将面对这个新状态。\n\n单个异步时间步的算法如下：\n1.  **顺序迭代**：算法从 $(0,0)$ 到 $(L-1, L-1)$ 遍历每个格点 $i=(r, c)$。\n2.  **计算局部收益**：对于当前的中心格点 $i$，必须*即时*计算其自身及其四个邻居的收益。这是因为邻居的策略可能已在当前时间步中更新。不能使用从该步骤开始时预先计算的收益网格。\n3.  **确定胜者并更新**：使用最大收益和与同步情况相同的决胜规则来确定获胜候选者。然后，中心格点 $i$ 的策略 $s_i$ 会*立即*更新以匹配获胜者的策略。\n4.  **继续**：算法继续处理行主序列中的下一个格点，该格点现在将“看到”格点 $i$ 处已更新的策略。\n\n在所有 $L^2$ 个格点都被更新一次后，一个完整的时间步完成。由于其顺序性和状态依赖性，该机制无法完全矢量化，需要对所有格点进行显式循环。\n\n### IV. 最终输出计算\n\n经过 $T$ 个时间步后，获得了两种模拟的最终构型 $s^{\\text{sync}}(T)$ 和 $s^{\\text{async}}(T)$。每种机制的最终合作者比例计算为最终网格中策略值的平均值。输出是这两个比例之间的绝对差值：\n$$\n\\Delta = \\left| \\frac{1}{L^2} \\sum_{i} s_i^{\\text{sync}}(T) - \\frac{1}{L^2} \\sum_{i} s_i^{\\text{async}}(T) \\right|\n$$\n该指标量化了仅由更新时间协议的差异导致的宏观结果的分歧。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef create_initial_grid(L, f0, seed):\n    \"\"\"\n    Generates the initial L x L grid of strategies.\n    1 for cooperator, 0 for defector.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    random_grid = rng.random((L, L))\n    # A site is a cooperator (1) if its random value is less than f0.\n    initial_grid = (random_grid  f0).astype(int)\n    return initial_grid\n\ndef calculate_total_payoffs_vectorized(grid, L, R, S, T_pay, P):\n    \"\"\"\n    Calculates total payoffs for all sites on the grid using vectorized operations.\n    \"\"\"\n    # Count cooperating neighbors for each site using periodic boundary conditions.\n    neighbor_coop_count = (\n        np.roll(grid, 1, axis=0) +\n        np.roll(grid, -1, axis=0) +\n        np.roll(grid, 1, axis=1) +\n        np.roll(grid, -1, axis=1)\n    )\n\n    # Payoff for cooperators if they were at a given site: k*R + (4-k)*S\n    payoff_if_C = neighbor_coop_count * R + (4 - neighbor_coop_count) * S\n    # Payoff for defectors if they were at a given site: k*T + (4-k)*P\n    payoff_if_D = neighbor_coop_count * T_pay + (4 - neighbor_coop_count) * P\n\n    # Assign payoffs based on the actual strategy of the site.\n    total_payoffs = np.where(grid == 1, payoff_if_C, payoff_if_D)\n    return total_payoffs\n\ndef run_synchronous(grid_initial, n_steps, L, R, S, T_pay, P):\n    \"\"\"\n    Runs the simulation with synchronous updates for n_steps.\n    \"\"\"\n    if n_steps == 0:\n        return grid_initial.copy()\n\n    current_grid = grid_initial.copy()\n    I, J = np.ogrid[:L, :L]\n\n    for _ in range(n_steps):\n        # 1. Compute all payoffs based on the grid state at the start of the step.\n        payoffs = calculate_total_payoffs_vectorized(current_grid, L, R, S, T_pay, P)\n\n        # 2. Gather payoffs of all candidates (self, up, down, left, right) for each site.\n        candidate_payoffs = np.stack([\n            payoffs,\n            np.roll(payoffs, 1, axis=0),   # Up\n            np.roll(payoffs, -1, axis=0),  # Down\n            np.roll(payoffs, 1, axis=1),   # Left\n            np.roll(payoffs, -1, axis=1)   # Right\n        ], axis=-1)\n\n        # 3. Np.argmax finds the index of the first occurrence of the max value,\n        # which correctly implements the deterministic tie-breaking rule.\n        winner_indices = np.argmax(candidate_payoffs, axis=-1)\n\n        # 4. Gather the strategies of the corresponding candidates.\n        candidate_strategies = np.stack([\n            current_grid,\n            np.roll(current_grid, 1, axis=0),\n            np.roll(current_grid, -1, axis=0),\n            np.roll(current_grid, 1, axis=1),\n            np.roll(current_grid, -1, axis=1)\n        ], axis=-1)\n\n        # 5. Build the next grid by selecting the winner's strategy for each site.\n        next_grid = candidate_strategies[I, J, winner_indices]\n        current_grid = next_grid\n\n    return current_grid\n\ndef run_asynchronous(grid_initial, n_steps, L, R, S, T_pay, P):\n    \"\"\"\n    Runs the simulation with asynchronous updates for n_steps.\n    \"\"\"\n    if n_steps == 0:\n        return grid_initial.copy()\n\n    grid = grid_initial.copy()\n\n    def get_payoff_at(r, c, current_grid):\n        s_focal = current_grid[r, c]\n        r_up, r_down = (r - 1 + L) % L, (r + 1) % L\n        c_left, c_right = (c - 1 + L) % L, (c + 1) % L\n\n        k = (current_grid[r_up, c] + current_grid[r_down, c] +\n             current_grid[r, c_left] + current_grid[r, c_right])\n\n        if s_focal == 1:  # Cooperator\n            return k * R + (4 - k) * S\n        else:  # Defector\n            return k * T_pay + (4 - k) * P\n\n    for _ in range(n_steps):\n        for r in range(L):\n            for c in range(L):\n                r_up, r_down = (r - 1 + L) % L, (r + 1) % L\n                c_left, c_right = (c - 1 + L) % L, (c + 1) % L\n\n                candidate_coords = [\n                    (r, c), (r_up, c), (r_down, c), (r, c_left), (r, c_right)\n                ]\n\n                # Payoffs are computed on-the-fly using the changing grid state.\n                payoffs = [get_payoff_at(cr, cc, grid) for cr, cc in candidate_coords]\n                max_payoff = max(payoffs)\n\n                # Find the first candidate with max payoff to handle ties.\n                winner_index = payoffs.index(max_payoff)\n                winner_coords = candidate_coords[winner_index]\n\n                # Update the focal site's strategy immediately.\n                grid[r, c] = grid[winner_coords]\n\n    return grid\n\ndef solve():\n    \"\"\"\n    Defines test cases, runs simulations, and prints the formatted output.\n    \"\"\"\n    test_cases = [\n        # (L, T_steps, R, S, T_pay, P, f0, seed)\n        (20, 100, 1.0, 0.0, 1.4, 0.0, 0.5, 12345),\n        (20, 0, 1.0, 0.0, 1.4, 0.0, 0.5, 54321),\n        (20, 200, 1.0, 0.0, 1.8, 0.0, 0.8, 98765),\n        (25, 150, 1.0, 0.0, 1.1, 0.0, 0.5, 24680),\n    ]\n\n    results = []\n    for L, T_steps, R, S, T_pay, P, f0, seed in test_cases:\n        initial_grid = create_initial_grid(L, f0, seed)\n\n        final_grid_sync = run_synchronous(initial_grid, T_steps, L, R, S, T_pay, P)\n        final_grid_async = run_asynchronous(initial_grid, T_steps, L, R, S, T_pay, P)\n\n        f_coop_sync = np.mean(final_grid_sync)\n        f_coop_async = np.mean(final_grid_async)\n\n        delta = abs(f_coop_sync - f_coop_async)\n        results.append(delta)\n\n    # The final print statement must produce only the specified output format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}