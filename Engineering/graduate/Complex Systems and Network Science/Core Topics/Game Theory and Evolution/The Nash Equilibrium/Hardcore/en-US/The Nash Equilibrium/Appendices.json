{
    "hands_on_practices": [
        {
            "introduction": "Understanding Nash Equilibrium begins with the ability to calculate it. This first practice provides a foundational exercise in a linear-quadratic game, where you will derive the equilibrium by solving the first-order conditions for each player . More importantly, by comparing the Nash Equilibrium with the centralized, socially optimal solution, you will quantitatively explore the \"price of anarchy\"—the cost society pays for decentralized, self-interested behavior.",
            "id": "3154673",
            "problem": "Consider a two-player linear-quadratic game in which player $i \\in \\{1,2\\}$ chooses a scalar decision variable $x_i \\in \\mathbb{R}$. The cost functions are given by\n$$\nf_i(x_1,x_2) \\;=\\; \\frac{1}{2} \\, q_i \\, x_i^{2} \\;+\\; b_i \\, x_i \\, x_{-i} \\;+\\; c_i \\, x_i,\n$$\nwhere $x_{-i}$ denotes the decision of the other player. Let $q_1 = 2$, $q_2 = 4$, $b_1 = 1$, $b_2 = 1$, $c_1 = -5$, and $c_2 = 1$. Assume $q_i  0$ for each player so that each player’s optimization problem is strictly convex in their own decision variable.\n\nStarting from the definition of Nash equilibrium (NE), which states that no player can reduce their own cost by a unilateral deviation, and using first-order optimality conditions for differentiable convex functions, derive the block linear system that characterizes the NE and solve for the equilibrium $(x_1^{\\ast}, x_2^{\\ast})$. Then form the centralized optimization problem that minimizes the social cost\n$$\nF(x_1,x_2) \\;=\\; f_1(x_1,x_2) \\;+\\; f_2(x_1,x_2),\n$$\nderive its first-order optimality conditions, and solve for the centralized optimizer $(x_1^{c}, x_2^{c})$.\n\nProvide your final result as the two solutions concatenated into a single row vector $\\big(x_1^{\\ast}, \\, x_2^{\\ast}, \\, x_1^{c}, \\, x_2^{c}\\big)$. No rounding is required.",
            "solution": "The problem requires finding two distinct solutions for a two-player linear-quadratic game: the Nash equilibrium (NE) and the centralized (socially optimal) solution.\n\nFirst, we determine the Nash Equilibrium, denoted by $(x_1^{\\ast}, x_2^{\\ast})$. A Nash equilibrium is a state where neither player can improve their outcome (i.e., lower their cost) by unilaterally changing their own decision, assuming the other player's decision remains fixed. For player $i \\in \\{1,2\\}$ with a cost function $f_i(x_1, x_2)$ that is differentiable and convex in their own decision variable $x_i$, this condition is mathematically expressed by the first-order optimality condition, $\\frac{\\partial f_i}{\\partial x_i} = 0$.\n\nThe cost function for player $i$ is given as:\n$$\nf_i(x_1,x_2) = \\frac{1}{2} q_i x_i^{2} + b_i x_i x_{-i} + c_i x_i\n$$\nwhere $x_{-i}$ is the decision of the other player. The problem states that $q_i  0$, ensuring that $f_i$ is strictly convex with respect to $x_i$, which guarantees that the first-order condition identifies a unique minimum for player $i$'s optimization problem.\n\nLet's compute the partial derivatives for each player.\nFor player $1$:\n$$\nf_1(x_1,x_2) = \\frac{1}{2} q_1 x_1^{2} + b_1 x_1 x_2 + c_1 x_1\n$$\nThe first-order condition is:\n$$\n\\frac{\\partial f_1}{\\partial x_1} = q_1 x_1 + b_1 x_2 + c_1 = 0\n$$\n\nFor player $2$:\n$$\nf_2(x_1,x_2) = \\frac{1}{2} q_2 x_2^{2} + b_2 x_2 x_1 + c_2 x_2\n$$\nThe first-order condition is:\n$$\n\\frac{\\partial f_2}{\\partial x_2} = q_2 x_2 + b_2 x_1 + c_2 = 0\n$$\n\nThese two equations form a linear system for the Nash equilibrium $(x_1^{\\ast}, x_2^{\\ast})$:\n\\begin{align*}\nq_1 x_1^{\\ast} + b_1 x_2^{\\ast} = -c_1 \\\\\nb_2 x_1^{\\ast} + q_2 x_2^{\\ast} = -c_2\n\\end{align*}\nIn matrix form, this is:\n$$\n\\begin{pmatrix} q_1  b_1 \\\\ b_2  q_2 \\end{pmatrix} \\begin{pmatrix} x_1^{\\ast} \\\\ x_2^{\\ast} \\end{pmatrix} = \\begin{pmatrix} -c_1 \\\\ -c_2 \\end{pmatrix}\n$$\nWe are given the parameter values: $q_1 = 2$, $q_2 = 4$, $b_1 = 1$, $b_2 = 1$, $c_1 = -5$, and $c_2 = 1$. Substituting these values into the system gives:\n\\begin{align*}\n2 x_1^{\\ast} + 1 x_2^{\\ast} = -(-5) = 5 \\\\\n1 x_1^{\\ast} + 4 x_2^{\\ast} = -1\n\\end{align*}\nFrom the first equation, we can express $x_2^{\\ast}$ in terms of $x_1^{\\ast}$:\n$$\nx_2^{\\ast} = 5 - 2 x_1^{\\ast}\n$$\nSubstituting this into the second equation:\n$$\nx_1^{\\ast} + 4(5 - 2 x_1^{\\ast}) = -1\n$$\n$$\nx_1^{\\ast} + 20 - 8 x_1^{\\ast} = -1\n$$\n$$\n-7 x_1^{\\ast} = -21\n$$\n$$\nx_1^{\\ast} = 3\n$$\nNow, we find $x_2^{\\ast}$ by substituting the value of $x_1^{\\ast}$ back:\n$$\nx_2^{\\ast} = 5 - 2(3) = 5 - 6 = -1\n$$\nThus, the Nash equilibrium is $(x_1^{\\ast}, x_2^{\\ast}) = (3, -1)$.\n\nNext, we determine the centralized optimizer, denoted by $(x_1^{c}, x_2^{c})$. This is the pair of decisions that minimizes the total or social cost, $F(x_1,x_2) = f_1(x_1,x_2) + f_2(x_1,x_2)$.\nLet's write out the social cost function $F(x_1,x_2)$:\n$$\nF(x_1,x_2) = \\left(\\frac{1}{2} q_1 x_1^{2} + b_1 x_1 x_2 + c_1 x_1\\right) + \\left(\\frac{1}{2} q_2 x_2^{2} + b_2 x_1 x_2 + c_2 x_2\\right)\n$$\n$$\nF(x_1,x_2) = \\frac{1}{2} q_1 x_1^{2} + \\frac{1}{2} q_2 x_2^{2} + (b_1 + b_2) x_1 x_2 + c_1 x_1 + c_2 x_2\n$$\nThis is a joint optimization problem in the variables $(x_1, x_2)$. To find the minimum, we compute the gradient of $F$ and set it to zero. The first-order conditions are $\\frac{\\partial F}{\\partial x_1} = 0$ and $\\frac{\\partial F}{\\partial x_2} = 0$.\n$$\n\\frac{\\partial F}{\\partial x_1} = q_1 x_1 + (b_1 + b_2) x_2 + c_1 = 0\n$$\n$$\n\\frac{\\partial F}{\\partial x_2} = q_2 x_2 + (b_1 + b_2) x_1 + c_2 = 0\n$$\nThis gives us a system of linear equations for the centralized optimizer $(x_1^{c}, x_2^{c})$:\n\\begin{align*}\nq_1 x_1^{c} + (b_1 + b_2) x_2^{c} = -c_1 \\\\\n(b_1 + b_2) x_1^{c} + q_2 x_2^{c} = -c_2\n\\end{align*}\nTo ensure this is a minimum, we can check the Hessian matrix of $F$, which is $H_F = \\begin{pmatrix} q_1  b_1+b_2 \\\\ b_1+b_2  q_2 \\end{pmatrix}$. With the given values, $H_F = \\begin{pmatrix} 2  2 \\\\ 2  4 \\end{pmatrix}$. The principal minors are $2  0$ and $\\det(H_F) = 2(4) - 2(2) = 4  0$. Since the Hessian is positive definite, $F$ is strictly convex, and the solution to the first-order conditions is the unique global minimum.\n\nSubstituting the parameter values into the system for the centralized solution:\n\\begin{align*}\n2 x_1^{c} + (1+1) x_2^{c} = -(-5) \\implies 2 x_1^{c} + 2 x_2^{c} = 5 \\\\\n(1+1) x_1^{c} + 4 x_2^{c} = -1 \\implies 2 x_1^{c} + 4 x_2^{c} = -1\n\\end{align*}\nWe now solve this system. Subtracting the first equation from the second gives:\n$$\n(2 x_1^{c} + 4 x_2^{c}) - (2 x_1^{c} + 2 x_2^{c}) = -1 - 5\n$$\n$$\n2 x_2^{c} = -6\n$$\n$$\nx_2^{c} = -3\n$$\nSubstituting $x_2^{c} = -3$ into the first equation:\n$$\n2 x_1^{c} + 2(-3) = 5\n$$\n$$\n2 x_1^{c} - 6 = 5\n$$\n$$\n2 x_1^{c} = 11\n$$\n$$\nx_1^{c} = \\frac{11}{2}\n$$\nThus, the centralized optimal solution is $(x_1^{c}, x_2^{c}) = (\\frac{11}{2}, -3)$.\n\nThe problem asks for the final result as the concatenated row vector $\\big(x_1^{\\ast}, x_2^{\\ast}, x_1^{c}, x_2^{c}\\big)$.\nThe calculated values are $x_1^{\\ast} = 3$, $x_2^{\\ast} = -1$, $x_1^{c} = \\frac{11}{2}$, and $x_2^{c} = -3$.\nThe final vector is $\\big(3, -1, \\frac{11}{2}, -3\\big)$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 3  -1  \\frac{11}{2}  -3 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "In many complex systems, equilibrium is not centrally dictated but emerges from the local interactions of many adaptive agents. This practice moves us to a network context, exploring a classic nonatomic congestion game where a population of users chooses routes . You will implement a simple learning model, the imitative replicator dynamic, to simulate how agent choices evolve and converge towards the Wardrop equilibrium, a system-level state of balance.",
            "id": "4310494",
            "problem": "Consider a nonatomic congestion game on a network consisting of two parallel links connecting a single origin-destination pair. A continuum of agents with total mass normalized to $1$ chooses between link $1$ and link $2$. Let $p \\in [0,1]$ denote the population share using link $1$, so that the link flows are $x_1 = p$ and $x_2 = 1 - p$. Each link $k \\in \\{1,2\\}$ has an affine cost function $c_k(x_k) = a_k + b_k x_k$ with parameters $a_k \\in \\mathbb{R}$ and $b_k \\ge 0$. The payoff to agents choosing link $k$ is $u_k = -c_k(x_k)$ since they minimize cost.\n\nFundamental definitions and base:\n- A Nash equilibrium (Wardrop equilibrium in nonatomic routing games) is a population profile where all used strategies have minimal cost; specifically, when both links are used, the equilibrium condition is $c_1(x_1) = c_2(x_2)$, and when only one link is used, it has weakly lower cost than the other link.\n- In potential games, including congestion games, learning dynamics such as imitative replicator dynamics possess a Lyapunov structure. Consider the discrete-time imitative replicator dynamic defined by the update rule\n$$\np_{t+1} = p_t + \\eta \\, p_t \\, (1 - p_t) \\, \\big(u_1(p_t) - u_2(p_t)\\big),\n$$\nwhere $\\eta  0$ is a learning-rate parameter and $u_k(p) = -c_k(x_k)$ for $x_1 = p$ and $x_2 = 1 - p$. Stationary points of this dynamic satisfy either $p \\in \\{0,1\\}$ or $u_1(p) = u_2(p)$, which translates to $c_1(p) = c_2(1 - p)$, i.e., the Wardrop equilibrium condition.\n\nTask:\n- Derive the analytical Wardrop equilibrium share $p^\\star$ by equating the costs of the two links when both are used. For $b_1 + b_2  0$, the interior equilibrium is obtained by solving\n$$\na_1 + b_1 p^\\star = a_2 + b_2 (1 - p^\\star),\n$$\nwhich yields\n$$\np^\\star = \\frac{a_2 + b_2 - a_1}{b_1 + b_2}.\n$$\nIf the computed $p^\\star$ lies outside $[0,1]$, the equilibrium is at the boundary $p^\\star = 0$ or $p^\\star = 1$, depending on which link has lower cost at the extreme. If $b_1 + b_2 = 0$ and $a_1 = a_2$, then $c_1(x_1) = c_2(x_2)$ for all $p$, and the equilibrium set is the entire interval $[0,1]$; in that degenerate case, any $p$ is a Nash equilibrium.\n- Implement the discrete-time imitative replicator dynamic with the update rule given above. Use $u_1(p) = -\\big(a_1 + b_1 p\\big)$ and $u_2(p) = -\\big(a_2 + b_2 (1 - p)\\big)$, starting from an initial condition $p_0 \\in [0,1]$, and iterate for a specified number of steps $T \\in \\mathbb{N}$. The algorithm should clip iterates to stay within the interval $[0,1]$.\n\nFor each test case listed below, your program must:\n$1.$ Compute the analytical equilibrium $p^\\star$ based on the parameters.\n$2.$ Simulate the replicator dynamic for $T$ steps with learning rate $\\eta$ starting from $p_0$.\n$3.$ Compute the final absolute deviation $|p_T - p^\\star|$. In the degenerate case where $b_1 + b_2 = 0$ and $a_1 = a_2$, treat the equilibrium set as $[0,1]$ and report $0$ for the deviation.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,\\dots]$), where each $r_i$ is the floating-point deviation for the corresponding test case.\n\nTest suite:\nUse the following five test cases, each specified by $(a_1, b_1, a_2, b_2, \\eta, T, p_0)$:\n$1.$ $(0.2, 1.0, 0.8, 0.4, 0.05, 20000, 0.1)$\n$2.$ $(0.0, 1.0, 2.0, 1.0, 0.05, 20000, 0.5)$\n$3.$ $(1.5, 1.0, 0.0, 1.0, 0.05, 20000, 0.8)$\n$4.$ $(1.0, 0.0, 1.0, 0.0, 0.2, 1000, 0.3)$\n$5.$ $(0.0, 1.0, 0.1, 3.0, 0.03, 30000, 0.9)$\n\nFinal output format requirement:\nYour program must print a single line containing a list of five floating-point numbers $[r_1,r_2,r_3,r_4,r_5]$, where $r_i$ is the deviation for test case $i$. No additional text should be printed.",
            "solution": "We begin from core definitions in game theory and network science. In a nonatomic routing game on two parallel links with affine cost functions $c_k(x_k) = a_k + b_k x_k$, agents minimize cost, and the Wardrop equilibrium coincides with a Nash equilibrium: used strategies (links with positive flow) must have equal and minimal cost.\n\nLet $p \\in [0,1]$ denote the population share choosing link $1$, so that $x_1 = p$ and $x_2 = 1 - p$. The costs are $c_1(p) = a_1 + b_1 p$ and $c_2(1 - p) = a_2 + b_2 (1 - p)$. The Nash equilibrium conditions are:\n$1.$ Interior equilibrium: if both links are used, then $c_1(p^\\star) = c_2(1 - p^\\star)$.\n$2.$ Boundary equilibrium: if the interior solution lies outside $[0,1]$, the equilibrium must be at $p^\\star = 0$ or $p^\\star = 1$, depending on which boundary yields lower cost for used strategies.\n\nSolving the interior equation $a_1 + b_1 p^\\star = a_2 + b_2 (1 - p^\\star)$ yields\n$$\np^\\star = \\frac{a_2 + b_2 - a_1}{b_1 + b_2},\n$$\nprovided $b_1 + b_2  0$. If $p^\\star \\notin [0,1]$, the equilibrium is at the boundary: $p^\\star = 0$ if link $2$ is strictly cheaper at $p = 0$, and $p^\\star = 1$ if link $1$ is strictly cheaper at $p = 1$. In the degenerate case $b_1 + b_2 = 0$, both costs are constant. If $a_1 = a_2$, then $c_1 = c_2$ for all $p$, and every $p \\in [0,1]$ is a Nash equilibrium; otherwise, the equilibrium is at the boundary with all mass on the cheaper link.\n\nWe now connect learning and adaptive dynamics to equilibrium. The discrete-time imitative replicator dynamic for a two-strategy population game is\n$$\np_{t+1} = p_t + \\eta \\, p_t \\, (1 - p_t) \\, \\big(u_1(p_t) - u_2(p_t)\\big),\n$$\nwhere $\\eta  0$ is the learning rate, and $u_k(p) = -c_k(x_k)$ are the payoffs equal to the negative costs. Substituting $u_1(p) = -\\big(a_1 + b_1 p\\big)$ and $u_2(p) = -\\big(a_2 + b_2 (1 - p)\\big)$, the payoff difference $u_1(p) - u_2(p)$ becomes $c_2(1-p) - c_1(p)$, which expands to:\n$$\nu_1(p) - u_2(p) = \\big(a_2 + b_2(1-p)\\big) - \\big(a_1 + b_1 p\\big) = (a_2+b_2-a_1) - (b_1+b_2)p.\n$$\nEquivalently, the sign of $u_1(p) - u_2(p)$ mirrors the sign of $c_2(1 - p) - c_1(p)$, and the update moves $p$ in the direction of higher payoff (lower cost). The stationary points of the dynamic satisfy either $p \\in \\{0,1\\}$ or $u_1(p) = u_2(p)$, which is precisely $c_1(p) = c_2(1 - p)$. In potential games such as congestion games, imitative dynamics admit a Lyapunov function related to the potential, and under suitable step sizes, iterates approach the equilibrium set.\n\nAlgorithmic design:\n$1.$ For each test case $(a_1, b_1, a_2, b_2, \\eta, T, p_0)$, compute the analytical equilibrium $p^\\star$:\n$1.1.$ If $b_1 + b_2  0$, compute $p^\\star = \\dfrac{a_2 + b_2 - a_1}{b_1 + b_2}$ and project it onto $[0,1]$; if the projection changes the value, that indicates a boundary equilibrium.\n$1.2.$ If $b_1 + b_2 = 0$, check if $a_1 = a_2$. If yes, the equilibrium set is the entire interval $[0,1]$; if not, set $p^\\star = 1$ when $a_1  a_2$ and $p^\\star = 0$ when $a_2  a_1$.\n$2.$ Simulate the discrete-time replicator dynamic for $T$ iterations with learning rate $\\eta$ starting from $p_0$:\n$$\np_{t+1} = \\mathrm{clip}\\Big(p_t + \\eta \\, p_t \\, (1 - p_t) \\, \\big(u_1(p_t) - u_2(p_t)\\big), \\, 0, \\, 1\\Big),\n$$\nwhere $\\mathrm{clip}$ ensures $p_{t+1} \\in [0,1]$. This clipping is consistent with the simplex constraint and counteracts numerical drift.\n$3.$ Compute the final deviation $|p_T - p^\\star|$. In the degenerate case with constant and equal costs ($b_1 + b_2 = 0$ and $a_1 = a_2$), the equilibrium set is $[0,1]$; since any $p_T \\in [0,1]$ is an equilibrium, report $0$.\n\nCoverage rationale for the test suite:\n$1.$ $(0.2, 1.0, 0.8, 0.4, 0.05, 20000, 0.1)$ represents a typical interior equilibrium. The analytical solution is $p^\\star = \\dfrac{0.8 + 0.4 - 0.2}{1.0 + 0.4} = \\dfrac{1.0}{1.4} \\approx 0.7142857$, and replicator dynamics should approach this interior solution.\n$2.$ $(0.0, 1.0, 2.0, 1.0, 0.05, 20000, 0.5)$ yields $p^\\star = \\dfrac{2.0 + 1.0 - 0.0}{1.0 + 1.0} = \\dfrac{3.0}{2.0} = 1.5$, which lies outside $[0,1]$, hence the boundary equilibrium $p^\\star = 1$; dynamics should converge to $1$.\n$3.$ $(1.5, 1.0, 0.0, 1.0, 0.05, 20000, 0.8)$ yields $p^\\star = \\dfrac{0.0 + 1.0 - 1.5}{1.0 + 1.0} = \\dfrac{-0.5}{2.0} = -0.25$, hence the boundary equilibrium $p^\\star = 0$; dynamics should converge to $0$.\n$4.$ $(1.0, 0.0, 1.0, 0.0, 0.2, 1000, 0.3)$ has $b_1 + b_2 = 0$ and $a_1 = a_2$, making costs constant and equal for both links; the equilibrium set is the entire interval $[0,1]$, and the dynamic has $u_1(p) - u_2(p) = 0$, so $p_t$ is constant. The deviation reported is $0$.\n$5.$ $(0.0, 1.0, 0.1, 3.0, 0.03, 30000, 0.9)$ has $p^\\star = \\dfrac{0.1 + 3.0 - 0.0}{1.0 + 3.0} = \\dfrac{3.1}{4.0} = 0.775$, an interior solution near the boundary where link slopes are asymmetric; dynamics should converge close to this value.\n\nThe final program carries out these computations deterministically and outputs the deviations as a single comma-separated list enclosed in brackets, ensuring reproducibility and precise numerical answers.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef compute_equilibrium(a1, b1, a2, b2):\n    \"\"\"\n    Compute the Wardrop (Nash) equilibrium share p* for a two-link affine congestion game.\n    Returns (p_star, continuum_flag).\n    continuum_flag is True only when b1 + b2 == 0 and a1 == a2 (continuum of equilibria).\n    \"\"\"\n    denom = b1 + b2\n    if denom == 0.0:\n        # Costs are constant. If equal constants, continuum of equilibria.\n        if np.isclose(a1, a2):\n            return None, True  # Any p in [0,1] is equilibrium\n        else:\n            # All mass on cheaper link\n            return (1.0 if a1  a2 else 0.0), False\n    else:\n        p_star = (a2 + b2 - a1) / denom\n        # Project onto [0,1] due to boundary equilibria\n        if p_star  0.0:\n            p_star = 0.0\n        elif p_star  1.0:\n            p_star = 1.0\n        return p_star, False\n\ndef simulate_replicator(a1, b1, a2, b2, eta, T, p0):\n    \"\"\"\n    Simulate discrete-time imitative replicator dynamics for T steps:\n        p_{t+1} = p_t + eta * p_t * (1 - p_t) * (u1(p_t) - u2(p_t)),\n    with u1 = -(a1 + b1 * p), u2 = -(a2 + b2 * (1 - p)).\n    Returns p_T.\n    \"\"\"\n    p = float(p0)\n    for _ in range(T):\n        # Payoffs are negative costs\n        u1 = -(a1 + b1 * p)\n        u2 = -(a2 + b2 * (1.0 - p))\n        delta = eta * p * (1.0 - p) * (u1 - u2)\n        p = p + delta\n        # Clip to [0,1] to respect the simplex constraint\n        if p  0.0:\n            p = 0.0\n        elif p  1.0:\n            p = 1.0\n    return p\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (a1, b1, a2, b2, eta, T, p0)\n    test_cases = [\n        (0.2, 1.0, 0.8, 0.4, 0.05, 20000, 0.1),\n        (0.0, 1.0, 2.0, 1.0, 0.05, 20000, 0.5),\n        (1.5, 1.0, 0.0, 1.0, 0.05, 20000, 0.8),\n        (1.0, 0.0, 1.0, 0.0, 0.2, 1000, 0.3),\n        (0.0, 1.0, 0.1, 3.0, 0.03, 30000, 0.9),\n    ]\n\n    results = []\n    for a1, b1, a2, b2, eta, T, p0 in test_cases:\n        p_star, continuum = compute_equilibrium(a1, b1, a2, b2)\n        p_T = simulate_replicator(a1, b1, a2, b2, eta, T, p0)\n        if continuum:\n            deviation = 0.0  # Any p in [0,1] is an equilibrium\n        else:\n            deviation = abs(p_T - p_star)\n        # Format with fixed precision for deterministic output\n        results.append(f\"{deviation:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The search for equilibrium is not always a smooth descent; the dynamics of learning can be complex and counter-intuitive. This final practice delves into the world of zero-sum minimax games, reminiscent of the training dynamics of Generative Adversarial Networks (GANs) . You will implement an advanced learning algorithm, Optimistic Gradient Descent-Ascent (OGDA), and analyze its behavior to understand and programmatically detect phenomena like cycling, where players' strategies fail to converge and instead orbit the equilibrium.",
            "id": "3154601",
            "problem": "Consider a two-player zero-sum minimax game defined on one-dimensional continuous strategies where player $1$ chooses $x \\in \\mathbb{R}$ to minimize a loss and player $2$ chooses $y \\in \\mathbb{R}$ to maximize the same loss. The interaction is governed by the differentiable loss function\n$$\nL(x,y) = \\alpha\\, x y + \\frac{\\mu}{2} x^2 - \\frac{\\nu}{2} y^2,\n$$\nwhere $\\alpha \\in \\mathbb{R}$ models the strength of coupling between the players, and $\\mu \\ge 0$, $\\nu \\ge 0$ model quadratic regularization terms for the minimizer and the maximizer respectively.\n\nFundamental base:\n- A Nash equilibrium for a zero-sum minimax game is a pair $(x^\\star, y^\\star)$ such that for all $x \\in \\mathbb{R}$ and $y \\in \\mathbb{R}$,\n$$\nL(x^\\star, y) \\le L(x^\\star, y^\\star) \\le L(x, y^\\star).\n$$\nFor convex-concave $L$ (convex in $x$ and concave in $y$), this saddle-point condition coincides with the solution of the stationarity conditions $\\nabla_x L(x^\\star, y^\\star) = 0$ and $\\nabla_y L(x^\\star, y^\\star) = 0$, by well-tested results in convex analysis and variational inequalities.\n- Gradient-based methods follow the vector field defined by the gradients $\\nabla_x L$ and $\\nabla_y L$. In minimax dynamics, naive gradient descent-ascent often exhibits rotational behavior around saddle points due to the off-diagonal coupling terms, which can lead to cycling. Optimistic Gradient Descent-Ascent (OGDA) modifies the update by incorporating the previous gradient to counteract rotational effects.\n\nYour tasks:\n1. From first principles, use the saddle-point and stationarity conditions to identify the candidate Nash equilibrium $(x^\\star,y^\\star)$ for the loss $L(x,y)$ defined above.\n2. Implement Optimistic Gradient Descent-Ascent (OGDA) with step size $\\eta  0$, which updates the strategies $(x_t, y_t)$ using current and previous gradients. Let $g_t^x = \\nabla_x L(x_t, y_t)$ and $g_t^y = \\nabla_y L(x_t, y_t)$. The OGDA update is\n$$\nx_{t+1} = x_t - \\eta \\big( 2 g_t^x - g_{t-1}^x \\big), \\quad\ny_{t+1} = y_t + \\eta \\big( 2 g_t^y - g_{t-1}^y \\big).\n$$\nInitialize $g_{-1}^x$ and $g_{-1}^y$ consistently using the gradient at the initial point $(x_0,y_0)$ so that the very first update reduces to a single-gradient step.\n3. Simulate the OGDA iterates for a specified number of steps and report the following quantities for each test case:\n   - The Euclidean distance $d_T = \\sqrt{(x_T - x^\\star)^2 + (y_T - y^\\star)^2}$ between the final iterate and the candidate Nash equilibrium, as a real number.\n   - A boolean indicating whether the sequence has converged to the Nash equilibrium according to the criterion $d_T \\le 10^{-3}$.\n   - A boolean indicating the presence of cycling, defined as the case where convergence does not occur, the trajectory remains bounded in the last portion of iterations relative to its start, and the radial distance sequence $r_t = \\sqrt{x_t^2 + y_t^2}$ exhibits oscillations characterized by a significant fraction of sign changes in successive differences $\\Delta r_t = r_{t+1} - r_t$ over the last half of the trajectory. No physical units apply in this problem; all quantities are dimensionless real numbers.\n\nTest suite:\nRun your program on the following parameter sets. Each test case is specified by $(\\alpha,\\mu,\\nu,\\eta,T,x_0,y_0)$, where $T$ is the total number of iterations:\n- Case 1: ($\\alpha=1$, $\\mu=1/2$, $\\nu=1/2$, $\\eta=0.2$, $T=400$, $x_0=1$, $y_0=-1$).\n- Case 2: ($\\alpha=1$, $\\mu=0$, $\\nu=0$, $\\eta=0.2$, $T=400$, $x_0=1$, $y_0=1$).\n- Case 3: ($\\alpha=1$, $\\mu=0$, $\\nu=0$, $\\eta=1.0$, $T=400$, $x_0=1$, $y_0=1$).\n- Case 4: ($\\alpha=3$, $\\mu=0.1$, $\\nu=0.1$, $\\eta=0.3$, $T=400$, $x_0=0.5$, $y_0=-0.5$).\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to a test case and is itself a list of the form $[d_T, \\text{converged}, \\text{cycled}]$. For example, the output must resemble\n$$\n[[d_1,\\text{True},\\text{False}],[d_2,\\text{False},\\text{True}],\\dots]\n$$\nwith the actual numeric values for $d_i$.",
            "solution": "We begin with the fundamental definition of Nash equilibrium for a two-player zero-sum minimax game: a pair $(x^\\star,y^\\star)$ satisfying\n$$\nL(x^\\star,y) \\le L(x^\\star,y^\\star) \\le L(x,y^\\star) \\quad \\text{for all } x \\in \\mathbb{R}, y \\in \\mathbb{R}.\n$$\nFor convexity in $x$ and concavity in $y$, and differentiable $L$, a well-tested fact from convex analysis states that this saddle-point condition is equivalent to the solution of the first-order stationarity conditions:\n$$\n\\nabla_x L(x^\\star,y^\\star) = 0, \\quad \\nabla_y L(x^\\star,y^\\star) = 0.\n$$\n\nGiven\n$$\nL(x,y) = \\alpha\\, x y + \\frac{\\mu}{2} x^2 - \\frac{\\nu}{2} y^2,\n$$\nthe partial derivatives (gradients) are\n$$\n\\nabla_x L(x,y) = \\alpha\\, y + \\mu\\, x, \\quad \\nabla_y L(x,y) = \\alpha\\, x - \\nu\\, y.\n$$\nSetting these equal to zero yields the linear system\n$$\n\\alpha\\, y + \\mu\\, x = 0, \\quad \\alpha\\, x - \\nu\\, y = 0.\n$$\nWhen $\\mu \\ge 0$ and $\\nu \\ge 0$, the unique solution to this system in the real domain is $x^\\star = 0$ and $y^\\star = 0$. Specifically, from $\\alpha\\, y + \\mu\\, x = 0$ we have $y = -\\frac{\\mu}{\\alpha} x$ (for $\\alpha \\neq 0$), and substituting into $\\alpha\\, x - \\nu\\, y = 0$ gives\n$$\n\\alpha\\, x - \\nu \\left( -\\frac{\\mu}{\\alpha} x \\right) = \\left(\\alpha + \\frac{\\nu \\mu}{\\alpha}\\right) x = 0,\n$$\nso $x=0$ and thus $y=0$. When $\\alpha=0$, the coupling vanishes, and the stationarity directly yields $x=0$ and $y=0$. Therefore, the candidate Nash equilibrium is $(x^\\star,y^\\star) = (0,0)$.\n\nOptimization dynamics for minimax problems commonly use Gradient Descent-Ascent (GDA):\n$$\nx_{t+1} = x_t - \\eta \\nabla_x L(x_t,y_t), \\quad y_{t+1} = y_t + \\eta \\nabla_y L(x_t,y_t),\n$$\nbut in games with off-diagonal coupling (e.g., bilinear terms), the gradient field exhibits rotational components that can cause cycling around the saddle $(x^\\star,y^\\star)$. To mitigate this, Optimistic Gradient Descent-Ascent (OGDA) incorporates an extrapolation using the previous gradient:\n$$\nx_{t+1} = x_t - \\eta \\big( 2 g_t^x - g_{t-1}^x \\big), \\quad\ny_{t+1} = y_t + \\eta \\big( 2 g_t^y - g_{t-1}^y \\big),\n$$\nwhere $g_t^x = \\nabla_x L(x_t, y_t)$ and $g_t^y = \\nabla_y L(x_t, y_t)$. This modification cancels a portion of the rotational component of the gradient field, producing more stable trajectories toward the saddle point under suitable step sizes and smoothness.\n\nAlgorithmic implementation:\n- Initialize $(x_0,y_0)$ and compute $g_{-1}^x = \\nabla_x L(x_0,y_0)$ and $g_{-1}^y = \\nabla_y L(x_0,y_0)$. With this choice, the first update reduces to a single-gradient step because $2 g_0 - g_{-1} = g_0$ when $g_{-1}$ is set to $g_0$.\n- Iterate for $t = 0, 1, \\dots, T-1$:\n  1. Compute $g_t^x = \\alpha\\, y_t + \\mu\\, x_t$, $g_t^y = \\alpha\\, x_t - \\nu\\, y_t$.\n  2. Update\n     $$\n     x_{t+1} = x_t - \\eta \\big( 2 g_t^x - g_{t-1}^x \\big), \\quad\n     y_{t+1} = y_t + \\eta \\big( 2 g_t^y - g_{t-1}^y \\big).\n     $$\n  3. Set $g_{t-1}^x \\leftarrow g_t^x$, $g_{t-1}^y \\leftarrow g_t^y$ for the next iteration.\n- Record the radial distances $r_t = \\sqrt{x_t^2 + y_t^2}$ during the evolution and compute the final Euclidean distance to the Nash equilibrium:\n  $$\n  d_T = \\sqrt{(x_T - x^\\star)^2 + (y_T - y^\\star)^2} = \\sqrt{x_T^2 + y_T^2}.\n  $$\n\nConvergence and cycling diagnostics:\n- Convergence criterion: declare convergence if $d_T \\le 10^{-3}$.\n- Cycling detection: if convergence does not occur, declare cycling when the trajectory is bounded over the last half of iterations relative to the start and the radial distance sequence oscillates. Concretely:\n  - Boundedness: let $r_t$ be the radius; consider the last half of indices $t \\in \\{ \\lfloor T/2 \\rfloor, \\dots, T \\}$. If $\\max r_t$ over this window is less than a fixed multiple of the initial radius $r_0$ (e.g., a factor of $20$), the sequence is considered bounded. This excludes divergence.\n  - Oscillation: compute successive differences $\\Delta r_t = r_{t+1} - r_t$ over the same window and count the fraction of sign changes between consecutive differences. If this fraction exceeds a threshold (e.g., $0.4$) and the amplitude range $\\max r_t - \\min r_t$ in the window exceeds a small floor (e.g., $10^{-3}$), label the behavior as cycling.\n\nThe program applies these rules to each test case defined in the problem statement, computing $(x^\\star,y^\\star)$, simulating OGDA, and returning for each case the triple $[d_T, \\text{converged}, \\text{cycled}]$. The final output aggregates all triples in a single bracketed list on one line. This design directly relates OGDA dynamics to Nash equilibrium computation in a minimax setting and programmatically identifies cycling behavior indicative of rotational dynamics.",
            "answer": "```python\n# Python 3.12\n# Libraries: numpy 1.23.5, scipy 1.11.4 (not used)\nimport numpy as np\n\ndef grad_L(alpha, mu, nu, x, y):\n    # Gradients of L with respect to x and y\n    gx = alpha * y + mu * x\n    gy = alpha * x - nu * y\n    return gx, gy\n\ndef ogda(alpha, mu, nu, eta, T, x0, y0):\n    # Candidate Nash equilibrium for this quadratic-concave structure\n    x_star, y_star = 0.0, 0.0\n\n    # Initialize\n    x, y = float(x0), float(y0)\n    gx_prev, gy_prev = grad_L(alpha, mu, nu, x, y)  # use initial gradient as \"previous\" for OGDA start\n\n    radii = [np.hypot(x, y)]\n\n    for t in range(T):\n        gx, gy = grad_L(alpha, mu, nu, x, y)\n        # OGDA update: x_{t+1} = x_t - eta * (2 gx_t - gx_{t-1})\n        #              y_{t+1} = y_t + eta * (2 gy_t - gy_{t-1})\n        x_next = x - eta * (2.0 * gx - gx_prev)\n        y_next = y + eta * (2.0 * gy - gy_prev)\n        # Shift previous gradients\n        gx_prev, gy_prev = gx, gy\n        # Move to next\n        x, y = x_next, y_next\n        radii.append(np.hypot(x, y))\n\n    # Final distance to Nash equilibrium (which is at origin)\n    d_T = np.hypot(x - x_star, y - y_star)\n\n    # Convergence criterion\n    converged = d_T = 1e-3\n\n    # Cycling detection over last half of iterations\n    half_idx = len(radii) // 2\n    last_half = np.array(radii[half_idx:])\n    diffs = np.diff(last_half)\n\n    # Boundedness relative to start\n    r0 = radii[0]\n    bounded = (np.max(last_half)  20.0 * max(r0, 1e-12))\n\n    # Oscillation: fraction of sign changes in successive differences and amplitude range\n    if len(diffs) = 2:\n        signs = np.sign(diffs)\n        # treat zeros as no sign change with neighbors; adjust by small epsilon\n        signs[signs == 0] = 0\n        sign_changes = 0\n        for i in range(len(signs) - 1):\n            if signs[i] * signs[i+1]  0:\n                sign_changes += 1\n        frac_changes = sign_changes / max(len(signs) - 1, 1)\n    else:\n        frac_changes = 0.0\n    amplitude = float(np.max(last_half) - np.min(last_half))\n    oscillatory = (frac_changes = 0.4) and (amplitude  1e-3)\n\n    cycled = (not converged) and bounded and oscillatory\n\n    return [d_T, converged, cycled]\n\ndef solve():\n    # Define the test cases from the problem statement:\n    # Each case is (alpha, mu, nu, eta, T, x0, y0)\n    test_cases = [\n        (1.0, 0.5, 0.5, 0.2, 400, 1.0, -1.0),   # Case 1\n        (1.0, 0.0, 0.0, 0.2, 400, 1.0, 1.0),    # Case 2\n        (1.0, 0.0, 0.0, 1.0, 400, 1.0, 1.0),    # Case 3\n        (3.0, 0.1, 0.1, 0.3, 400, 0.5, -0.5),   # Case 4\n    ]\n\n    results = []\n    for case in test_cases:\n        alpha, mu, nu, eta, T, x0, y0 = case\n        res = ogda(alpha, mu, nu, eta, T, x0, y0)\n        # Ensure float is not numpy type for clean printing\n        res = [float(res[0]), bool(res[1]), bool(res[2])]\n        results.append(res)\n\n    # Final print statement in the exact required format.\n    # Print as a single list of lists, with booleans and floats.\n    # Example: [[0.00123,True,False],[...],...]\n    def format_item(item):\n        # item is [float, bool, bool]\n        d = f\"{item[0]}\"\n        b1 = \"True\" if item[1] else \"False\"\n        b2 = \"True\" if item[2] else \"False\"\n        return f\"[{d},{b1},{b2}]\"\n\n    print(f\"[{','.join(format_item(r) for r in results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}