{
    "hands_on_practices": [
        {
            "introduction": "A common first-pass measure of a node's importance is its degree—a simple count of its connections. Eigenvector centrality, however, offers a more sophisticated perspective by asserting that a node's influence is derived from the influence of its neighbors. This practice challenges you to find a network where these two measures diverge, building a concrete intuition for how network topology can concentrate influence in ways that a simple degree count misses. ",
            "id": "1501057",
            "problem": "In a network represented by a graph $G=(V, E)$, the importance of a vertex can be measured in various ways. One simple measure is the **degree** of a vertex, which is the number of edges connected to it. A more subtle measure is **eigenvector centrality**.\n\nFor a connected, undirected graph, the eigenvector centrality $c_i$ for each vertex $v_i \\in V$ is given by the components of the principal eigenvector of the graph's adjacency matrix. This vector is unique up to a scaling factor and has all positive components. The defining property of eigenvector centralities is that the centrality of any vertex is proportional to the sum of the centralities of its neighbors. That is, for some constant $\\lambda  0$:\n$$ \\lambda c_i = \\sum_{j \\text{ is a neighbor of } i} c_j $$\n\nThis property implies that a vertex is influential if it is connected to other influential vertices. While degree and eigenvector centrality are often correlated, they are not always equivalent.\n\nConsider a graph with the set of vertices $V = \\{v_1, v_2, v_3, v_4, v_5\\}$. Which of the following graphs, defined by their edge sets $E$, has at least one vertex whose degree is maximal within the graph, but whose eigenvector centrality is not?\n\nA. $E = \\{(v_1, v_3), (v_1, v_4), (v_1, v_5), (v_2, v_3), (v_2, v_4)\\}$\n\nB. $E = \\{(v_1, v_2), (v_1, v_3), (v_1, v_4), (v_1, v_5)\\}$\n\nC. $E = \\{(v_1, v_2), (v_2, v_3), (v_3, v_4), (v_4, v_5)\\}$\n\nD. $E = \\{(v_1, v_2), (v_2, v_3), (v_3, v_1), (v_3, v_4), (v_4, v_5)\\}$",
            "solution": "We analyze each option for whether there exists a vertex whose degree is maximal in the graph but whose eigenvector centrality is not maximal. For a connected, undirected graph with adjacency matrix $A$, the eigenvector centrality vector $c$ satisfies $A c = \\lambda c$ for the principal eigenvalue $\\lambda > 0$, and all components of $c$ are positive.\n\nOption B: $E = \\{(v_{1},v_{2}),(v_{1},v_{3}),(v_{1},v_{4}),(v_{1},v_{5})\\}$. This is the star on $5$ vertices centered at $v_{1}$. By symmetry, let $c_{1}=x$ and $c_{2}=c_{3}=c_{4}=c_{5}=y$. The eigenvector equations give\n$$\n\\lambda x = 4y,\\quad \\lambda y = x.\n$$\nEliminating $x$ yields $\\lambda^{2} y = 4y$, hence $\\lambda^{2}=4$ and thus $x=\\lambda y=2y>y$. Therefore $v_{1}$ has both maximal degree and maximal eigenvector centrality. This option does not satisfy the condition.\n\nOption C: $E = \\{(v_{1},v_{2}),(v_{2},v_{3}),(v_{3},v_{4}),(v_{4},v_{5})\\}$. This is the path $P_{5}$. By symmetry, let $c_{1}=c_{5}=a$, $c_{2}=c_{4}=b$, and $c_{3}=c$. The eigenvector equations are\n$$\n\\lambda a = b,\\quad \\lambda b = a + c,\\quad \\lambda c = 2b.\n$$\nFrom the first, $a=b/\\lambda$. Substituting into the second and multiplying by $\\lambda$ gives $\\lambda^{2} b = b + \\lambda c$. Using the third equation, $\\lambda c = 2b$, so $\\lambda^{2} b = b + 2b = 3b$, hence $\\lambda^{2}=3$. Then $c = \\frac{2b}{\\lambda}$ and $a=\\frac{b}{\\lambda}$ with $\\lambda=\\sqrt{3}$. Since $1\\lambda2$, it follows that $c > b > a$. The maximal degree in $P_{5}$ is $2$, attained by $v_{2},v_{3},v_{4}$, but only $v_{3}$ has the maximal eigenvector centrality; $v_{2}$ and $v_{4}$ have degree maximal yet eigenvector centrality not maximal. This option satisfies the condition.\n\nOption A: $E = \\{(v_{1},v_{3}),(v_{1},v_{4}),(v_{1},v_{5}),(v_{2},v_{3}),(v_{2},v_{4})\\}$. The degrees are $d(v_1)=3, d(v_2)=2, d(v_3)=2, d(v_4)=2, d(v_5)=1$. By symmetry, let $c_{3}=c_{4}=a$, $c_{5}=b$, $c_{1}=x$, $c_{2}=y$. The eigenvector equations are\n$$\n\\lambda x = 2a + b,\\quad \\lambda y = 2a,\\quad \\lambda a = x + y,\\quad \\lambda b = x.\n$$\nFrom the last, $b = \\frac{x}{\\lambda}$. The second gives $y = \\frac{2a}{\\lambda}$. The third gives $x = a\\left(\\lambda - \\frac{2}{\\lambda}\\right)$. Substituting into the first yields\n$$\n\\lambda x - \\frac{x}{\\lambda} = 2a \\quad \\Longrightarrow \\quad x = \\frac{2a}{\\lambda - \\frac{1}{\\lambda}}.\n$$\nEquating the two expressions for $x$ and simplifying leads to\n$$\n\\lambda^{4} - 5\\lambda^{2} + 2 = 0.\n$$\nLet $t=\\lambda^{2}$; then $t^{2} - 5t + 2 = 0$, so $t = \\frac{5 \\pm \\sqrt{17}}{2}$. The principal eigenvalue satisfies $\\lambda^{2} = \\frac{5 + \\sqrt{17}}{2}$, hence $\\lambda^{2} > 4$ because\n$$\n\\lambda^{2} - 4 = \\frac{5 + \\sqrt{17} - 8}{2} = \\frac{\\sqrt{17} - 3}{2} > 0.\n$$\nNow compare centralities. Using $x = a\\left(\\lambda - \\frac{2}{\\lambda}\\right)$ and $y = \\frac{2a}{\\lambda}$,\n$$\n\\frac{y}{x} = \\frac{\\frac{2}{\\lambda}}{\\lambda - \\frac{2}{\\lambda}} = \\frac{2}{\\lambda^{2} - 2}  1 \\quad \\text{since} \\quad \\lambda^{2} > 4.\n$$\nSimilarly,\n$$\n\\frac{a}{x} = \\frac{1}{\\lambda - \\frac{2}{\\lambda}} = \\frac{\\lambda}{\\lambda^{2} - 2}  1 \\quad \\text{since} \\quad \\lambda > 2.\n$$\nAlso $b = \\frac{x}{\\lambda}  x$. Therefore $x$ is the largest component, so $v_{1}$ (the unique maximal-degree vertex) has the maximal eigenvector centrality. This option does not satisfy the condition.\n\nOption D: $E = \\{(v_{1},v_{2}),(v_{2},v_{3}),(v_{3},v_{1}),(v_{3},v_{4}),(v_{4},v_{5})\\}$. This is a triangle on $\\{v_{1},v_{2},v_{3}\\}$ with a path of length $2$ attached at $v_{3}$. By symmetry, let $c_{1}=c_{2}=x$, $c_{3}=y$, $c_{4}=z$, $c_{5}=w$. The eigenvector equations are\n$$\n\\lambda x = x + y,\\quad \\lambda y = x + x + z = 2x + z,\\quad \\lambda z = y + w,\\quad \\lambda w = z.\n$$\nFrom the last, $w = \\frac{z}{\\lambda}$, and from $\\lambda x = x + y$ we have $y = (\\lambda - 1) x$. Substituting into $\\lambda z = y + w$ gives $z = \\frac{y}{\\lambda - \\frac{1}{\\lambda}}$. The graph strictly contains $K_{3}$ as a proper subgraph, and by Perron–Frobenius monotonicity, its spectral radius $\\lambda$ strictly exceeds that of $K_{3}$, which is $2$. Hence $\\lambda > 2$, so $y = (\\lambda - 1) x > x$. Moreover, $z = \\frac{y}{\\lambda - \\frac{1}{\\lambda}}  y$ and $w = \\frac{z}{\\lambda}  z$. Thus $v_{3}$ is the unique vertex with maximal eigenvector centrality and is also the unique vertex with maximal degree. This option does not satisfy the condition.\n\nTherefore, the only graph among the options that has at least one vertex whose degree is maximal but whose eigenvector centrality is not is Option C.",
            "answer": "$$\\boxed{C}$$"
        },
        {
            "introduction": "After developing an intuition for what eigenvector centrality represents, the next step is to master its mathematical foundation. This exercise guides you through an analytical derivation of centrality for the star graph, a canonical network structure. By solving for a general number of nodes $n$, you will not only find an exact solution but also uncover how influence scales in a hub-and-spoke system, a fundamental insight in network science. ",
            "id": "4273818",
            "problem": "Consider an undirected, unweighted star graph $S_{n}$ with $n \\geq 2$ vertices, consisting of a single hub vertex connected to $n-1$ leaf vertices. Let $A \\in \\mathbb{R}^{n \\times n}$ denote the adjacency matrix of $S_{n}$, indexed so that vertex $1$ is the hub and vertices $2,\\dots,n$ are leaves. The eigenvector centrality of the graph is defined as a nonnegative vector $c \\in \\mathbb{R}^{n}$ that satisfies $A c = \\lambda c$ for some largest eigenvalue $\\lambda  0$, with normalization by the Euclidean norm (L2), i.e., $\\sum_{i=1}^{n} c_{i}^{2} = 1$. Using only the definition of eigenvector centrality and basic properties of symmetric nonnegative matrices (including the Perron–Frobenius theorem), derive the explicit closed-form eigenvector centrality values for the hub and for any leaf. Then, determine the ratio of the hub’s centrality to a leaf’s centrality as a function of $n$ under the Euclidean (L2) normalization. Report only the ratio $c_{\\mathrm{hub}}/c_{\\mathrm{leaf}}$ as your final answer. No rounding is required.",
            "solution": "The eigenvector centrality of a graph is given by the principal eigenvector of its adjacency matrix. Let the star graph be $S_n$ with $n$ vertices. The vertices are indexed such that vertex $1$ is the central hub and vertices $i \\in \\{2, 3, \\ldots, n\\}$ are the $n-1$ peripheral leaf nodes. The adjacency matrix $A$ of this unweighted, undirected graph is an $n \\times n$ matrix where $A_{ij} = 1$ if there is an edge between vertex $i$ and vertex $j$, and $A_{ij} = 0$ otherwise.\n\nThe structure of $A$ is as follows:\n- The hub (vertex $1$) is connected to all $n-1$ leaves: $A_{1,i} = A_{i,1} = 1$ for $i \\in \\{2, \\ldots, n\\}$.\n- The leaves are not connected to each other: $A_{i,j} = 0$ for $i, j \\in \\{2, \\ldots, n\\}$ and $i \\neq j$.\n- There are no self-loops: $A_{i,i} = 0$ for all $i \\in \\{1, \\ldots, n\\}$.\n\nLet $c = (c_1, c_2, \\ldots, c_n)^T$ be the eigenvector centrality vector, where $c_i \\geq 0$ for all $i$. The eigenvector equation is $A c = \\lambda c$, where $\\lambda$ is the largest eigenvalue of $A$. This matrix equation represents a system of $n$ linear equations:\n1.  For the hub vertex ($i=1$): $\\sum_{j=1}^{n} A_{1,j} c_j = \\lambda c_1 \\implies \\sum_{j=2}^{n} c_j = \\lambda c_1$.\n2.  For a leaf vertex ($i \\in \\{2, \\ldots, n\\}$): $\\sum_{j=1}^{n} A_{i,j} c_j = \\lambda c_i \\implies A_{i,1} c_1 = \\lambda c_i \\implies c_1 = \\lambda c_i$.\n\nThe leaf vertices $2, \\ldots, n$ are topologically equivalent; they are structurally indistinguishable in the graph. Due to this symmetry, the components of the principal eigenvector corresponding to these vertices must be equal. Let $c_1 = c_{\\mathrm{hub}}$ and $c_i = c_{\\mathrm{leaf}}$ for all $i \\in \\{2, \\ldots, n\\}$.\n\nThe system of equations simplifies to two distinct equations:\n(1) $(n-1)c_{\\mathrm{leaf}} = \\lambda c_{\\mathrm{hub}}$\n(2) $c_{\\mathrm{hub}} = \\lambda c_{\\mathrm{leaf}}$\n\nFor a non-trivial solution ($c \\neq 0$), we must have $c_{\\mathrm{leaf}} \\neq 0$ and $c_{\\mathrm{hub}} \\neq 0$. We can substitute equation (2) into equation (1):\n$(n-1)c_{\\mathrm{leaf}} = \\lambda (\\lambda c_{\\mathrm{leaf}})$\n$(n-1)c_{\\mathrm{leaf}} = \\lambda^2 c_{\\mathrm{leaf}}$\n\nSince $c_{\\mathrm{leaf}} \\neq 0$, we can divide both sides by $c_{\\mathrm{leaf}}$ to find the eigenvalues:\n$\\lambda^2 = n-1 \\implies \\lambda = \\pm\\sqrt{n-1}$.\n\nThe other eigenvalues of the star graph are $0$ with multiplicity $n-2$. According to the definition of eigenvector centrality and the Perron-Frobenius theorem for non-negative, irreducible matrices, the relevant eigenvalue is the unique largest positive eigenvalue. Thus, we take $\\lambda = \\sqrt{n-1}$.\n\nUsing this value of $\\lambda$ in equation (2), we find the relationship between the hub and leaf centralities:\n$c_{\\mathrm{hub}} = \\sqrt{n-1} \\, c_{\\mathrm{leaf}}$.\n\nThe problem specifies normalization by the Euclidean (L2) norm: $\\sum_{i=1}^{n} c_i^2 = 1$. We can write this as:\n$c_1^2 + \\sum_{i=2}^{n} c_i^2 = 1$\n$c_{\\mathrm{hub}}^2 + (n-1) c_{\\mathrm{leaf}}^2 = 1$.\n\nNow, substitute $c_{\\mathrm{hub}} = \\sqrt{n-1} \\, c_{\\mathrm{leaf}}$ into the normalization equation:\n$(\\sqrt{n-1} \\, c_{\\mathrm{leaf}})^2 + (n-1) c_{\\mathrm{leaf}}^2 = 1$\n$(n-1) c_{\\mathrm{leaf}}^2 + (n-1) c_{\\mathrm{leaf}}^2 = 1$\n$2(n-1) c_{\\mathrm{leaf}}^2 = 1$\n$c_{\\mathrm{leaf}}^2 = \\frac{1}{2(n-1)}$.\n\nSince centrality components are non-negative, we take the positive square root:\n$c_{\\mathrm{leaf}} = \\frac{1}{\\sqrt{2(n-1)}}$.\n\nNow we can determine the explicit value for the hub's centrality:\n$c_{\\mathrm{hub}} = \\sqrt{n-1} \\, c_{\\mathrm{leaf}} = \\sqrt{n-1} \\left( \\frac{1}{\\sqrt{2(n-1)}} \\right) = \\frac{\\sqrt{n-1}}{\\sqrt{2}\\sqrt{n-1}} = \\frac{1}{\\sqrt{2}}$.\n\nThe explicit centrality values are $c_{\\mathrm{hub}} = \\frac{1}{\\sqrt{2}}$ and $c_{\\mathrm{leaf}} = \\frac{1}{\\sqrt{2(n-1)}}$.\n\nFinally, we compute the ratio of the hub's centrality to a leaf's centrality:\n$\\frac{c_{\\mathrm{hub}}}{c_{\\mathrm{leaf}}} = \\frac{\\frac{1}{\\sqrt{2}}}{\\frac{1}{\\sqrt{2(n-1)}}} = \\frac{\\sqrt{2(n-1)}}{\\sqrt{2}} = \\sqrt{\\frac{2(n-1)}{2}} = \\sqrt{n-1}$.\nThis result can also be obtained directly from the relation $c_{\\mathrm{hub}} = \\lambda c_{\\mathrm{leaf}}$ where $\\lambda = \\sqrt{n-1}$.",
            "answer": "$$\\boxed{\\sqrt{n-1}}$$"
        },
        {
            "introduction": "While analytical solutions are enlightening for simple networks, most real-world systems are too large and complex for this approach. This final practice bridges the gap between theory and application by exploring how eigenvector centrality can be calculated in a decentralized fashion. You will evaluate distributed algorithms that approximate centrality using only local message-passing, a concept at the heart of modern, large-scale network analysis like Google's PageRank. ",
            "id": "4273758",
            "problem": "Consider a weighted, undirected, connected network $G = (V, E)$ with $|V| = n$ nodes and a symmetric, nonnegative adjacency matrix $W = [w_{ij}]$, where $w_{ij}  0$ if and only if $\\{i,j\\} \\in E$. Each node $i$ knows its incident weights $\\{w_{ij}\\}_{j \\in \\mathcal{N}(i)}$ and can exchange messages with neighbors $j \\in \\mathcal{N}(i)$ at discrete time steps $t = 0, 1, 2, \\dots$. The goal is to compute the eigenvector centrality of the network, namely the unique positive centrality vector whose entries at node $i$ are determined by the centralities of its neighbors and the incident weights. Design a distributed algorithm that uses only local message passing and repeated neighbor-weighted averaging to approximate the centrality vector, including a normalization to avoid trivial divergence or collapse, and a stopping criterion. Under mild spectral-gap conditions and with a strictly positive initialization, the correct algorithm should converge to the desired centrality direction.\n\nSelect all options that correctly outline such an algorithm, satisfying the constraints above. In each option, an \"Average-Consensus (AC)\" subroutine refers to a standard linear iterative protocol in which nodes repeatedly average their local states with neighbors to compute the global average of an initial scalar quantity using only local message passing; assume each node knows $n$ to recover the global sum from the average. Let $\\varepsilon  0$ be a prescribed tolerance.\n\nA. Synchronous neighbor-weighted sum with distributed global $2$-norm normalization:\n- Initialize $x_i^{(0)}  0$ for all $i \\in V$, and set $t = 0$.\n- At time $t$, each node $i$ receives $x_j^{(t)}$ from $j \\in \\mathcal{N}(i)$ and computes the neighbor-weighted sum $s_i^{(t)} = \\sum_{j \\in \\mathcal{N}(i)} w_{ij} x_j^{(t)}$.\n- Each node runs the Average-Consensus (AC) subroutine with initial scalar $(s_i^{(t)})^2$ to obtain the average $\\bar{q}^{(t)} = \\frac{1}{n} \\sum_{k=1}^n (s_k^{(t)})^2$. Recover the global sum $q^{(t)} = n \\bar{q}^{(t)}$, and set the normalization $m^{(t)} = \\sqrt{q^{(t)}}$.\n- Update $x_i^{(t+1)} = \\dfrac{s_i^{(t)}}{m^{(t)}}$ for all $i \\in V$.\n- If $\\|x^{(t+1)} - x^{(t)}\\|_2 \\le \\varepsilon$, stop; otherwise set $t \\leftarrow t + 1$ and repeat.\n\nB. Degree-normalized neighbor average without global normalization:\n- Initialize $x_i^{(0)}  0$ and set $t = 0$.\n- At time $t$, each node $i$ updates $x_i^{(t+1)} = \\dfrac{1}{d_i} \\sum_{j \\in \\mathcal{N}(i)} w_{ij} x_j^{(t)}$, where $d_i = \\sum_{j \\in \\mathcal{N}(i)} w_{ij}$.\n- Repeat until $\\max_{i} |x_i^{(t+1)} - x_i^{(t)}| \\le \\varepsilon$.\n\nC. Damped neighbor-weighted averaging with distributed global $2$-norm normalization:\n- Fix a damping parameter $\\alpha \\in (0,1)$, initialize $x_i^{(0)}  0$ for all $i$, and set $t = 0$.\n- At time $t$, each node $i$ computes $s_i^{(t)} = \\sum_{j \\in \\mathcal{N}(i)} w_{ij} x_j^{(t)}$.\n- Form the damped update $y_i^{(t)} = \\alpha s_i^{(t)} + (1 - \\alpha) x_i^{(t)}$.\n- Run the Average-Consensus (AC) subroutine with initial scalar $(y_i^{(t)})^2$ to obtain $\\bar{r}^{(t)} = \\frac{1}{n} \\sum_{k=1}^n (y_k^{(t)})^2$, recover $r^{(t)} = n \\bar{r}^{(t)}$, and set $m^{(t)} = \\sqrt{r^{(t)}}$.\n- Normalize $x_i^{(t+1)} = \\dfrac{y_i^{(t)}}{m^{(t)}}$ for all $i \\in V$.\n- If $\\|x^{(t+1)} - x^{(t)}\\|_2 \\le \\varepsilon$, stop; else set $t \\leftarrow t + 1$ and repeat.\n\nD. Local ratio normalization by neighbor-wise maxima:\n- Initialize $x_i^{(0)}  0$ and set $t = 0$.\n- At time $t$, each node $i$ computes $s_i^{(t)} = \\sum_{j \\in \\mathcal{N}(i)} w_{ij} x_j^{(t)}$ and then updates $x_i^{(t+1)} = \\dfrac{s_i^{(t)}}{\\max\\{s_j^{(t)} : j \\in \\mathcal{N}(i)\\}}$.\n- Repeat until $\\|x^{(t+1)} - x^{(t)}\\|_2 \\le \\varepsilon$.\n\nAssume the network remains static during the iterations, all message exchanges are reliable and synchronous within each iteration, and Average-Consensus (AC) is run to sufficient accuracy at each outer iteration so that the induced normalization is well-defined and consistent across nodes. Choose the option(s) that define an algorithm which converge, under mild spectral-gap conditions and strictly positive initialization, to the correct eigenvector centrality direction using only local message passing and neighbor-weighted averaging with appropriate normalization.",
            "solution": "We begin from the core definition of eigenvector centrality in a weighted, undirected, connected network with nonnegative weights. Let $W \\in \\mathbb{R}^{n \\times n}$ be symmetric and entrywise nonnegative. By the Perron–Frobenius theorem, $W$ has a largest eigenvalue $\\lambda_{1} = \\rho(W)$ (the spectral radius) and an associated eigenvector $v_{1} \\in \\mathbb{R}^{n}$ that can be chosen strictly positive. The eigenvector centrality direction is the ray spanned by $v_{1}$, and any positive scaling of $v_{1}$ represents the same centrality ranking. Therefore, the problem reduces to recovering the direction of $v_{1}$ in a distributed manner.\n\nA standard principle to obtain $v_{1}$ is the power iteration: for a generic initial vector $x^{(0)}$ with nonzero component in the $v_{1}$ direction, the sequence\n$$\ny^{(t)} = W x^{(t)}, \\quad x^{(t+1)} = \\frac{y^{(t)}}{\\|y^{(t)}\\|_2}\n$$\nconverges to $v_{1}/\\|v_{1}\\|_2$ if $\\lambda_{1}  |\\lambda_{2}|$ (simple dominant eigenvalue and spectral gap). The key structural property is that multiplication by $W$ is localizable: the $i$-th component satisfies\n$$\ny_i^{(t)} = \\sum_{j \\in \\mathcal{N}(i)} w_{ij} x_j^{(t)},\n$$\nso each node $i$ can compute $y_i^{(t)}$ using only neighbor messages. The normalization $\\|y^{(t)}\\|_2$ is a global quantity, but it can be approximated via local message passing by running a consensus protocol on the scalar $(y_i^{(t)})^2$, and then multiplying the resulting average by $n$ to recover the sum $\\sum_{k=1}^n (y_k^{(t)})^2$ (this assumes each node knows $n$). Thus, the distributed power iteration consists of neighbor-weighted averaging to implement $W$, followed by a distributed normalization to keep the direction and prevent divergence. Under the conditions specified and strictly positive initialization, the iterates remain positive and converge to the eigenvector centrality direction.\n\nWe now analyze each option.\n\nOption A (Synchronous neighbor-weighted sum with distributed global $2$-norm normalization):\n- The update $s_i^{(t)} = \\sum_{j} w_{ij} x_j^{(t)}$ implements the local component of the matrix–vector multiplication $W x^{(t)}$.\n- The Average-Consensus (AC) subroutine on $(s_i^{(t)})^2$ computes the average $\\bar{q}^{(t)} = \\frac{1}{n} \\sum_{k} (s_k^{(t)})^2$. Multiplying by $n$ yields $q^{(t)} = \\sum_{k=1}^n (s_k^{(t)})^2 = \\|s^{(t)}\\|_2^2$. Taking the square root gives $m^{(t)} = \\|s^{(t)}\\|_2$.\n- The normalization $x^{(t+1)} = s^{(t)} / m^{(t)}$ matches the normalized power iteration. Under the Perron–Frobenius conditions ($W$ symmetric nonnegative, connected network implying irreducibility), and a simple largest eigenvalue $\\lambda_{1}$, we have that for any strictly positive $x^{(0)}$, the normalized iterates converge to $v_{1}/\\|v_{1}\\|_2$.\n- The stopping criterion $\\|x^{(t+1)} - x^{(t)}\\|_2 \\le \\varepsilon$ is standard.\nVerdict: Correct. It is a faithful distributed implementation of the power iteration using neighbor-weighted averaging and consensus-based global normalization.\n\nOption B (Degree-normalized neighbor average without global normalization):\n- The update $x^{(t+1)} = D^{-1} W x^{(t)}$ with $D = \\mathrm{diag}(d_1, \\dots, d_n)$ and $d_i = \\sum_{j} w_{ij}$ defines multiplication by the row-stochastic matrix $P = D^{-1} W$. For symmetric $W$, $P$ is typically not symmetric but has largest eigenvalue $1$ with an eigenvector $u$ that corresponds to the stationary distribution of the random walk defined by $P$ (for undirected graphs, the stationary distribution is proportional to degrees: $u_i \\propto d_i$).\n- Without normalization, repeated multiplication by $P$ converges (under standard Markov chain conditions) to the stationary distribution on the simplex if the initial vector is normalized appropriately, or otherwise may collapse to zero or diverge in norm depending on scaling. Even if normalized, the limit is not the eigenvector centrality of $W$ but the stationary distribution of $P$.\n- Hence, this procedure does not converge to the eigenvector centrality direction of $W$ in general.\nVerdict: Incorrect. It computes a random-walk stationary measure (or a degree-based measure), not eigenvector centrality.\n\nOption C (Damped neighbor-weighted averaging with distributed global $2$-norm normalization):\n- The update defines $y^{(t)} = [(1 - \\alpha) I + \\alpha W] x^{(t)}$. The matrix $M(\\alpha) = (1 - \\alpha) I + \\alpha W$ is a convex combination of $I$ and $W$, hence has the same eigenvectors as $W$, with eigenvalues $\\mu_i(\\alpha) = (1 - \\alpha) + \\alpha \\lambda_i$. In particular, the dominant eigenvector is $v_{1}$, associated with the largest eigenvalue $\\mu_{1}(\\alpha) = (1 - \\alpha) + \\alpha \\lambda_{1}$ (which remains strictly larger in magnitude than the others if $\\lambda_{1}$ is simple and dominant and $\\alpha \\in (0,1)$).\n- Normalizing $y^{(t)}$ by its global $2$-norm via AC (as in Option A) yields a normalized power iteration for $M(\\alpha)$, which converges to $v_{1}/\\|v_{1}\\|_2$. Since $v_{1}$ is the same for $W$ and $M(\\alpha)$, the direction recovered is the eigenvector centrality direction of $W$.\n- The damping can improve robustness to noise or numerical issues while preserving the limit direction.\nVerdict: Correct. It preserves the eigenvector direction and converges under the same mild conditions using neighbor-weighted averaging plus distributed normalization.\n\nOption D (Local ratio normalization by neighbor-wise maxima):\n- The proposed normalization $x_i^{(t+1)} = s_i^{(t)} / \\max\\{s_j^{(t)} : j \\in \\mathcal{N}(i)\\}$ uses a local, neighborhood-wise scaling that is not consistent across nodes. This destroys the global direction of the iterate by inducing heterogeneous scalings that depend on local maxima, which need not be proportional to the global norm or any quantity that preserves the eigenvector direction.\n- As a consequence, even if $s^{(t)}$ aligns with $v_{1}$, the heterogeneous local normalizations can distort the relative ratios between components and prevent convergence to the true eigenvector centrality direction. There is no guarantee of convergence to the desired centrality vector under this normalization.\nVerdict: Incorrect. The local maxima normalization is not a consistent global normalization and does not implement a valid power-like iteration toward eigenvector centrality.\n\nConclusion: Options A and C correctly outline distributed algorithms that implement repeated neighbor-weighted averaging with appropriate global normalization via consensus, and converge (under the usual Perron–Frobenius and spectral-gap conditions with strictly positive initialization) to the eigenvector centrality direction. Options B and D do not converge to eigenvector centrality in general.",
            "answer": "$$\\boxed{A, C}$$"
        }
    ]
}