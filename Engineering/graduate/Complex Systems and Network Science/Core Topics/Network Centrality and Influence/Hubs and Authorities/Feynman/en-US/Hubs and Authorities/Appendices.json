{
    "hands_on_practices": [
        {
            "introduction": "To truly understand an algorithm, there is no substitute for working through it by hand. This first exercise provides a direct, hands-on application of the Hyperlink-Induced Topic Search (HITS) algorithm's core mechanics on a small, concrete network. By manually performing the iterative updates and normalization steps, you will solidify your understanding of how hub and authority scores mutually reinforce and evolve over time .",
            "id": "4281863",
            "problem": "Consider a directed network of $4$ nodes with adjacency matrix $A \\in \\mathbb{R}^{4 \\times 4}$, where $A_{ij} = 1$ if there is a directed edge from node $i$ to node $j$, and $A_{ij} = 0$ otherwise. The network is given by\n$$\nA \\;=\\;\n\\begin{pmatrix}\n0 & 1 & 1 & 0 \\\\\n0 & 0 & 1 & 1 \\\\\n0 & 1 & 0 & 1 \\\\\n0 & 1 & 0 & 0\n\\end{pmatrix}.\n$$\nIn the Hyperlink-Induced Topic Search (HITS) algorithm, each node is assigned a hub score and an authority score. The fundamental definitions are: an authority score of a node is proportional to the sum of the hub scores of nodes that point to it, and a hub score of a node is proportional to the sum of the authority scores of nodes it points to.\n\nStarting from the initial hub vector $h^{(0)} \\in \\mathbb{R}^{4}$ chosen to be unit length with equal components, that is\n$$\nh^{(0)} \\;=\\; \\frac{1}{\\sqrt{4}} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{pmatrix} \\;=\\; \\begin{pmatrix} \\tfrac{1}{2} \\\\ \\tfrac{1}{2} \\\\ \\tfrac{1}{2} \\\\ \\tfrac{1}{2} \\end{pmatrix},\n$$\nperform exactly $2$ full iterations of the HITS update process as follows. In each full iteration, first compute the authority vector from the hub vector using the proportionality definition, then normalize the authority vector to have Euclidean norm $1$; next compute the hub vector from the authority vector using the proportionality definition, then normalize the hub vector to have Euclidean norm $1$. At each sub-step, explicitly show the unnormalized vector, the normalization factor, and the normalized vector, thereby tracking the evolution of $a$ and $h$.\n\nUsing only the fundamental definitions stated above as the starting point, derive the linear updates needed and carry out the computations with the given $A$ and $h^{(0)}$. At the end of the second authority-update normalization, what is the exact value of the authority score of node $2$, namely $a^{(2)}_{2}$? Provide your answer as a single closed-form expression. Do not round your answer.",
            "solution": "The problem asks for a specific authority score after two full iterations of the Hyperlink-Induced Topic Search (HITS) algorithm on a given network. First, we must formalize the update rules based on the provided definitions.\n\nLet $a = (a_1, a_2, \\dots, a_N)^T$ be the authority vector and $h = (h_1, h_2, \\dots, h_N)^T$ be the hub vector for a network of $N$ nodes. The adjacency matrix is $A$, where $A_{ij}=1$ if a directed edge exists from node $i$ to node $j$.\n\nThe definitions provided are:\n1.  The authority score of a node $j$, $a_j$, is proportional to the sum of the hub scores of nodes that point to it. The nodes $i$ that point to node $j$ are those for which $A_{ij}=1$. Thus,\n    $$a_j \\propto \\sum_{i=1}^N A_{ij} h_i$$\n    This is the $j$-th component of the vector product $A^T h$. So, the unnormalized authority vector, which we denote as $\\tilde{a}$, is given by the update rule:\n    $$\\tilde{a} = A^T h$$\n\n2.  The hub score of a node $i$, $h_i$, is proportional to the sum of the authority scores of nodes it points to. The nodes $j$ that node $i$ points to are those for which $A_{ij}=1$. Thus,\n    $$h_i \\propto \\sum_{j=1}^N A_{ij} a_j$$\n    This is the $i$-th component of the vector product $A a$. So, the unnormalized hub vector, $\\tilde{h}$, is given by the update rule:\n    $$\\tilde{h} = A a$$\n\nA full iteration, updating from step $k$ to $k+1$, consists of an authority update followed by a hub update, with normalization at each stage.\nLet $h^{(k)}$ be the normalized hub vector at step $k$.\n1.  Compute the unnormalized authority vector: $\\tilde{a}^{(k+1)} = A^T h^{(k)}$.\n2.  Normalize to get the authority vector: $a^{(k+1)} = \\frac{\\tilde{a}^{(k+1)}}{\\|\\tilde{a}^{(k+1)}\\|}$, where $\\|\\cdot\\|$ denotes the Euclidean norm.\n3.  Compute the unnormalized hub vector: $\\tilde{h}^{(k+1)} = A a^{(k+1)}$.\n4.  Normalize to get the hub vector: $h^{(k+1)} = \\frac{\\tilde{h}^{(k+1)}}{\\|\\tilde{h}^{(k+1)}\\|}$.\n\nThe problem provides the adjacency matrix for a network of $N=4$ nodes:\n$$A = \\begin{pmatrix} 0 & 1 & 1 & 0 \\\\ 0 & 0 & 1 & 1 \\\\ 0 & 1 & 0 & 1 \\\\ 0 & 1 & 0 & 0 \\end{pmatrix}$$\nThe transpose of the adjacency matrix is:\n$$A^T = \\begin{pmatrix} 0 & 0 & 0 & 0 \\\\ 1 & 0 & 1 & 1 \\\\ 1 & 1 & 0 & 0 \\\\ 0 & 1 & 1 & 0 \\end{pmatrix}$$\nThe initial hub vector is given as:\n$$h^{(0)} = \\begin{pmatrix} 1/2 \\\\ 1/2 \\\\ 1/2 \\\\ 1/2 \\end{pmatrix}$$\n\nWe will now perform two full iterations.\n\n**Iteration 1:**\n\nFirst, we compute the unnormalized authority vector $\\tilde{a}^{(1)}$ using $h^{(0)}$:\n$$\\tilde{a}^{(1)} = A^T h^{(0)} = \\begin{pmatrix} 0 & 0 & 0 & 0 \\\\ 1 & 0 & 1 & 1 \\\\ 1 & 1 & 0 & 0 \\\\ 0 & 1 & 1 & 0 \\end{pmatrix} \\begin{pmatrix} 1/2 \\\\ 1/2 \\\\ 1/2 \\\\ 1/2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1/2 + 1/2 + 1/2 \\\\ 1/2 + 1/2 \\\\ 1/2 + 1/2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 3/2 \\\\ 1 \\\\ 1 \\end{pmatrix}$$\nNext, we normalize $\\tilde{a}^{(1)}$. The normalization factor is its Euclidean norm:\n$$\\|\\tilde{a}^{(1)}\\| = \\sqrt{0^2 + \\left(\\frac{3}{2}\\right)^2 + 1^2 + 1^2} = \\sqrt{\\frac{9}{4} + 1 + 1} = \\sqrt{\\frac{9}{4} + \\frac{8}{4}} = \\sqrt{\\frac{17}{4}} = \\frac{\\sqrt{17}}{2}$$\nThe normalized authority vector $a^{(1)}$ is:\n$$a^{(1)} = \\frac{1}{\\|\\tilde{a}^{(1)}\\|} \\tilde{a}^{(1)} = \\frac{2}{\\sqrt{17}} \\begin{pmatrix} 0 \\\\ 3/2 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\frac{1}{\\sqrt{17}} \\begin{pmatrix} 0 \\\\ 3 \\\\ 2 \\\\ 2 \\end{pmatrix}$$\nNow, we compute the unnormalized hub vector $\\tilde{h}^{(1)}$ using $a^{(1)}$:\n$$\\tilde{h}^{(1)} = A a^{(1)} = \\begin{pmatrix} 0 & 1 & 1 & 0 \\\\ 0 & 0 & 1 & 1 \\\\ 0 & 1 & 0 & 1 \\\\ 0 & 1 & 0 & 0 \\end{pmatrix} \\frac{1}{\\sqrt{17}} \\begin{pmatrix} 0 \\\\ 3 \\\\ 2 \\\\ 2 \\end{pmatrix} = \\frac{1}{\\sqrt{17}} \\begin{pmatrix} 3+2 \\\\ 2+2 \\\\ 3+2 \\\\ 3 \\end{pmatrix} = \\frac{1}{\\sqrt{17}} \\begin{pmatrix} 5 \\\\ 4 \\\\ 5 \\\\ 3 \\end{pmatrix}$$\nFinally for this iteration, we normalize $\\tilde{h}^{(1)}$. The norm is:\n$$\\|\\tilde{h}^{(1)}\\| = \\left\\| \\frac{1}{\\sqrt{17}} \\begin{pmatrix} 5 \\\\ 4 \\\\ 5 \\\\ 3 \\end{pmatrix} \\right\\| = \\frac{1}{\\sqrt{17}} \\sqrt{5^2 + 4^2 + 5^2 + 3^2} = \\frac{1}{\\sqrt{17}} \\sqrt{25 + 16 + 25 + 9} = \\frac{\\sqrt{75}}{\\sqrt{17}} = \\frac{5\\sqrt{3}}{\\sqrt{17}}$$\nThe normalized hub vector $h^{(1)}$ is:\n$$h^{(1)} = \\frac{1}{\\|\\tilde{h}^{(1)}\\|} \\tilde{h}^{(1)} = \\frac{\\sqrt{17}}{5\\sqrt{3}} \\left( \\frac{1}{\\sqrt{17}} \\begin{pmatrix} 5 \\\\ 4 \\\\ 5 \\\\ 3 \\end{pmatrix} \\right) = \\frac{1}{5\\sqrt{3}} \\begin{pmatrix} 5 \\\\ 4 \\\\ 5 \\\\ 3 \\end{pmatrix}$$\n\n**Iteration 2:**\n\nWe start the second iteration with $h^{(1)}$ to compute $\\tilde{a}^{(2)}$:\n$$\\tilde{a}^{(2)} = A^T h^{(1)} = \\begin{pmatrix} 0 & 0 & 0 & 0 \\\\ 1 & 0 & 1 & 1 \\\\ 1 & 1 & 0 & 0 \\\\ 0 & 1 & 1 & 0 \\end{pmatrix} \\frac{1}{5\\sqrt{3}} \\begin{pmatrix} 5 \\\\ 4 \\\\ 5 \\\\ 3 \\end{pmatrix} = \\frac{1}{5\\sqrt{3}} \\begin{pmatrix} 0 \\\\ 5+0+5+3 \\\\ 5+4+0+0 \\\\ 0+4+5+0 \\end{pmatrix} = \\frac{1}{5\\sqrt{3}} \\begin{pmatrix} 0 \\\\ 13 \\\\ 9 \\\\ 9 \\end{pmatrix}$$\nNext, we normalize $\\tilde{a}^{(2)}$. The normalization factor is:\n$$\\|\\tilde{a}^{(2)}\\| = \\left\\| \\frac{1}{5\\sqrt{3}} \\begin{pmatrix} 0 \\\\ 13 \\\\ 9 \\\\ 9 \\end{pmatrix} \\right\\| = \\frac{1}{5\\sqrt{3}} \\sqrt{0^2 + 13^2 + 9^2 + 9^2} = \\frac{1}{5\\sqrt{3}} \\sqrt{169 + 81 + 81} = \\frac{\\sqrt{331}}{5\\sqrt{3}}$$\nThe normalized authority vector $a^{(2)}$ is:\n$$a^{(2)} = \\frac{1}{\\|\\tilde{a}^{(2)}\\|} \\tilde{a}^{(2)} = \\frac{5\\sqrt{3}}{\\sqrt{331}} \\left( \\frac{1}{5\\sqrt{3}} \\begin{pmatrix} 0 \\\\ 13 \\\\ 9 \\\\ 9 \\end{pmatrix} \\right) = \\frac{1}{\\sqrt{331}} \\begin{pmatrix} 0 \\\\ 13 \\\\ 9 \\\\ 9 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 13/\\sqrt{331} \\\\ 9/\\sqrt{331} \\\\ 9/\\sqrt{331} \\end{pmatrix}$$\nThe problem asks for the authority score of node $2$ at the end of the second authority-update normalization, which is the component $a^{(2)}_{2}$. From the vector $a^{(2)}$ we calculated, this value is:\n$$a^{(2)}_{2} = \\frac{13}{\\sqrt{331}}$$\nThe number $331$ is prime, so this expression cannot be simplified further.",
            "answer": "$$\\boxed{\\frac{13}{\\sqrt{331}}}$$"
        },
        {
            "introduction": "After mastering the iterative mechanics, we can now build intuition by applying HITS to an idealized network structure. This exercise analyzes a directed star graph, a topology where the roles of hub and authority are perfectly segregated. By deriving the hub and authority scores for this network, you will see how the algorithm's eigenvector formulation elegantly captures these distinct roles and clarifies the conceptual foundation of HITS .",
            "id": "4281856",
            "problem": "Consider a directed star graph $G_n$ with node set $\\{v_0,v_1,\\dots,v_n\\}$, where $v_0$ is the center and for each $i \\in \\{1,\\dots,n\\}$ there is a single directed edge from $v_0$ to $v_i$. There are no other edges. Let $A \\in \\{0,1\\}^{(n+1)\\times(n+1)}$ be the adjacency matrix with the convention that $A_{ij}=1$ if there is a directed edge from node $i$ to node $j$, and $A_{ij}=0$ otherwise.\n\nThe Hyperlink-Induced Topic Search (HITS) framework defines two score vectors: a hub score vector $h$ and an authority score vector $a$. In the canonical linear-algebraic formulation, these arise from the dominant spectral structure of two-step path operators built from $A$; in particular, co-reference and co-citation similarities are captured by $A A^{\\top}$ and $A^{\\top} A$, respectively. In the standard normalization, the hub and authority score vectors are scaled to have unit Euclidean norm, that is, $\\|h\\|_2 = 1$ and $\\|a\\|_2 = 1$.\n\nStarting from the interpretation that $(A A^{\\top})_{ij}$ counts two-step paths $i \\to \\cdot \\to j$ and $(A^{\\top} A)_{ij}$ counts two-step paths $\\cdot \\to i \\leftarrow \\cdot$ (co-citation), derive why the principal eigenvectors of $A A^{\\top}$ and $A^{\\top} A$ encode hub and authority centralities, respectively. Then, for the graph $G_n$ described above:\n- Compute the unit-norm hub score vector $h$ and the unit-norm authority score vector $a$ explicitly in terms of $n$.\n- Explain qualitatively the disparity between the hub and authority roles for the center $v_0$ and a leaf $v_i$.\n\nFinally, let $R(n)$ denote the ratio of the center’s hub score to any leaf’s authority score under the unit-norm normalization, that is, $R(n) = \\frac{h(v_0)}{a(v_i)}$ for any $i \\in \\{1,\\dots,n\\}$. Provide $R(n)$ as a single closed-form analytic expression in terms of $n$. No rounding is required.",
            "solution": "The problem statement is found to be valid as it is scientifically grounded in the established HITS algorithm from network science, well-posed with sufficient information for a unique solution, and expressed in objective, formal language. There are no contradictions, ambiguities, or factual inaccuracies that would invalidate the problem.\n\nThe solution proceeds by first providing the theoretical underpinnings of the HITS algorithm as requested, then constructing the specific matrices for the given graph, calculating the hub and authority score vectors, and finally using these results to provide a qualitative explanation and compute the required ratio.\n\n### The HITS Algorithm and its Eigenvector Formulation\n\nThe Hyperlink-Induced Topic Search (HITS) algorithm is based on a mutually reinforcing relationship between two types of nodes in a directed network: hubs and authorities.\n- A good authority is a node that is pointed to by many good hubs.\n- A good hub is a node that points to many good authorities.\n\nLet the authority score of node $j$ be $a_j$ and the hub score of node $i$ be $h_i$. Let $A$ be the adjacency matrix where $A_{ij}=1$ if there is a directed edge from $i$ to $j$. The iterative updates are derived from the definitions:\n1. The authority score of a node $j$ is the sum of the hub scores of nodes that point to it. A node $k$ points to $j$ if $A_{kj}=1$. Thus:\n$$a_j \\propto \\sum_k A_{kj} h_k$$\nIn vector form, this is $a \\propto A^{\\top} h$.\n\n2. The hub score of a node $i$ is the sum of the authority scores of nodes it points to. Node $i$ points to $j$ if $A_{ij}=1$. Thus:\n$$h_i \\propto \\sum_j A_{ij} a_j$$\nIn vector form, this is $h \\propto A a$.\n\nBy substituting these two relations into each other, we obtain the eigenvector equations:\n$$a \\propto A^{\\top}h \\propto A^{\\top}(Aa) = (A^{\\top}A)a$$\n$$h \\propto Aa \\propto A(A^{\\top}h) = (AA^{\\top})h$$\nThese show that the authority vector $a$ is an eigenvector of the matrix $A^{\\top}A$, and the hub vector $h$ is an eigenvector of the matrix $AA^{\\top}$. To ensure non-negative scores, we select the principal eigenvector, which, by the Perron-Frobenius theorem for non-negative matrices, corresponds to the largest non-negative eigenvalue and has non-negative entries.\n\nThe problem asks for an interpretation based on path counting. Let's analyze the entries of these matrices:\n- The matrix $A^{\\top}A$ is the co-citation matrix. Its $(i,j)$-th entry is $(A^{\\top}A)_{ij} = \\sum_k (A^{\\top})_{ik} A_{kj} = \\sum_k A_{ki} A_{kj}$. This sum counts the number of nodes $k$ that have directed edges to both node $i$ and node $j$ (the pattern $k \\to i$ and $k \\to j$). Nodes praised by the same sources are considered similar authorities. The principal eigenvector of this matrix thus identifies the most prominent authorities.\n- The matrix $AA^{\\top}$ is the co-reference matrix (or bibliographic coupling matrix). Its $(i,j)$-th entry is $(AA^{\\top})_{ij} = \\sum_k A_{ik} (A^{\\top})_{kj} = \\sum_k A_{ik} A_{jk}$. This sum counts the number of nodes $k$ to which both node $i$ and node $j$ point (the pattern $i \\to k$ and $j \\to k$). Nodes that point to the same targets are considered similar hubs. The principal eigenvector of this matrix identifies the most prominent hubs.\n\n### Matrix Construction for Graph $G_n$\n\nThe graph $G_n$ has $n+1$ nodes, which we label as $\\{0, 1, \\dots, n\\}$, corresponding to $\\{v_0, v_1, \\dots, v_n\\}$. Node $v_0$ is the center. There is a directed edge from $v_0$ to each $v_i$ for $i \\in \\{1, \\dots, n\\}$.\nThe adjacency matrix $A$ is an $(n+1) \\times (n+1)$ matrix. The only non-zero entries are $A_{0i}=1$ for $i \\in \\{1, \\dots, n\\}$.\n$$\nA =\n\\begin{pmatrix}\n0 & 1 & 1 & \\dots & 1 \\\\\n0 & 0 & 0 & \\dots & 0 \\\\\n0 & 0 & 0 & \\dots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & 0 & \\dots & 0\n\\end{pmatrix}\n$$\nThe transpose of $A$, $A^{\\top}$, is:\n$$\nA^{\\top} =\n\\begin{pmatrix}\n0 & 0 & 0 & \\dots & 0 \\\\\n1 & 0 & 0 & \\dots & 0 \\\\\n1 & 0 & 0 & \\dots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & 0 & 0 & \\dots & 0\n\\end{pmatrix}\n$$\n\n### Hub Score Vector Calculation\n\nThe hub scores are given by the principal eigenvector of $AA^{\\top}$.\n$$\nAA^{\\top} =\n\\begin{pmatrix}\n0 & 1 & \\dots & 1 \\\\\n0 & 0 & \\dots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\dots & 0\n\\end{pmatrix}\n\\begin{pmatrix}\n0 & 0 & \\dots & 0 \\\\\n1 & 0 & \\dots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & 0 & \\dots & 0\n\\end{pmatrix}\n$$\nThe only non-zero element is $(AA^{\\top})_{00} = \\sum_{k=1}^{n} 1 \\cdot 1 = n$. All other elements are $0$.\n$$\nAA^{\\top} =\n\\begin{pmatrix}\nn & 0 & 0 & \\dots & 0 \\\\\n0 & 0 & 0 & \\dots & 0 \\\\\n0 & 0 & 0 & \\dots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & 0 & \\dots & 0\n\\end{pmatrix}\n$$\nThis is a diagonal matrix. Its eigenvalues are the diagonal entries: $n$ with multiplicity $1$, and $0$ with multiplicity $n$. The principal eigenvalue is $\\lambda_{hub} = n$. The corresponding eigenvector $h = (h_0, h_1, \\dots, h_n)^{\\top}$ satisfies $(AA^{\\top})h = nh$.\nThis yields the system of equations: $nh_0 = nh_0$, and $0 = nh_i$ for $i \\in \\{1, \\dots, n\\}$.\nThis implies $h_i = 0$ for $i \\ge 1$. The vector $h$ is of the form $(c, 0, \\dots, 0)^{\\top}$ for some constant $c$.\nWe apply the normalization condition $\\|h\\|_2=1$: $\\sqrt{c^2 + 0^2 + \\dots + 0^2} = 1$, which gives $|c|=1$. Choosing the customary non-negative eigenvector, we get $c=1$.\nThe unit-norm hub score vector is $h = (1, 0, \\dots, 0)^{\\top}$.\nSo, $h(v_0) = 1$ and $h(v_i) = 0$ for $i \\in \\{1,\\dots,n\\}$.\n\n### Authority Score Vector Calculation\n\nThe authority scores are given by the principal eigenvector of $A^{\\top}A$.\n$$\nA^{\\top}A =\n\\begin{pmatrix}\n0 & 0 & \\dots & 0 \\\\\n1 & 0 & \\dots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & 0 & \\dots & 0\n\\end{pmatrix}\n\\begin{pmatrix}\n0 & 1 & \\dots & 1 \\\\\n0 & 0 & \\dots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\dots & 0\n\\end{pmatrix}\n$$\nLet's compute the entries $(A^{\\top}A)_{ij} = \\sum_k A_{ki} A_{kj}$.\n- If $i=0$ or $j=0$, the corresponding column in $A$ is all zeros, so $(A^{\\top}A)_{ij}=0$.\n- If $i,j \\in \\{1, \\dots, n\\}$, the only non-zero term in the sum is for $k=0$. Then $(A^{\\top}A)_{ij} = A_{0i} A_{0j} = 1 \\cdot 1 = 1$.\n$$\nA^{\\top}A =\n\\begin{pmatrix}\n0 & 0 & 0 & \\dots & 0 \\\\\n0 & 1 & 1 & \\dots & 1 \\\\\n0 & 1 & 1 & \\dots & 1 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 1 & 1 & \\dots & 1\n\\end{pmatrix}\n$$\nTo find the eigenvalues $\\lambda$ and eigenvectors $a$ of $A^{\\top}A$: $(A^{\\top}A)a = \\lambda a$.\nFor $a = (a_0, a_1, \\dots, a_n)^{\\top}$, this gives:\n$0 = \\lambda a_0$.\nFor $i \\in \\{1, \\dots, n\\}$, $\\sum_{j=1}^{n} a_j = \\lambda a_i$.\nThe principal eigenvalue must be non-zero (since $A^{\\top}A$ is not the zero matrix for $n \\ge 1$), so from the first equation, $a_0 = 0$.\nThe remaining equations imply that $\\lambda a_1 = \\lambda a_2 = \\dots = \\lambda a_n = \\sum_{j=1}^{n} a_j$.\nThis means $a_1=a_2=\\dots=a_n$. Let $a_i=c$ for $i \\in \\{1, \\dots, n\\}$.\nThen the equation becomes $\\lambda c = \\sum_{j=1}^{n} c = nc$. For a non-trivial solution ($c \\ne 0$), we must have $\\lambda = n$.\nThe principal eigenvalue is $\\lambda_{auth} = n$. The corresponding eigenvector is of the form $(0, c, c, \\dots, c)^{\\top}$.\nApplying the normalization $\\|a\\|_2=1$: $\\sqrt{0^2 + nc^2} = 1 \\implies |c|\\sqrt{n} = 1$.\nFor a non-negative eigenvector, we choose $c = 1/\\sqrt{n}$.\nThe unit-norm authority score vector is $a = (0, 1/\\sqrt{n}, \\dots, 1/\\sqrt{n})^{\\top}$.\nSo, $a(v_0) = 0$ and $a(v_i) = 1/\\sqrt{n}$ for $i \\in \\{1,\\dots,n\\}$.\n\n### Qualitative Explanation of Roles\n\nThe calculated scores perfectly reflect the topology of the star graph $G_n$.\n- **Center Node $v_0$**: This node has a maximum hub score of $h(v_0)=1$ and a zero authority score of $a(v_0)=0$. This is because $v_0$ is the source of all links in the network; it acts as a pure \"hub\" by pointing to all other nodes. It receives no incoming links, so it has no \"authority\" status.\n- **Leaf Nodes $v_i$ (for $i \\ge 1$)**: These nodes have zero hub scores, $h(v_i)=0$, and positive, equal authority scores, $a(v_i)=1/\\sqrt{n}$. This is because they do not point to any nodes, hence they are not hubs. However, they all receive a link from the single, maximal hub ($v_0$), making them equally important \"authorities\" in the network. Their authority is distributed equally among them.\n\nThe disparity is maximal: $v_0$ is a pure hub, while the leaves $v_i$ are pure authorities.\n\n### Calculation of the Ratio $R(n)$\n\nThe problem defines the ratio $R(n)$ as $R(n) = \\frac{h(v_0)}{a(v_i)}$ for any $i \\in \\{1,\\dots,n\\}$.\nUsing the scores derived above:\n$h(v_0) = 1$\n$a(v_i) = \\frac{1}{\\sqrt{n}}$\nThe ratio is:\n$$R(n) = \\frac{1}{\\frac{1}{\\sqrt{n}}} = \\sqrt{n}$$\nThis is the closed-form analytic expression for the ratio.",
            "answer": "$$\\boxed{\\sqrt{n}}$$"
        },
        {
            "introduction": "This final practice elevates the analysis from specific examples to the statistical principles governing the algorithm's performance. You are tasked with designing a synthetic benchmark, a random graph model with a 'planted' structure, to stress-test HITS's ability to recover the correct rankings amidst noise. This problem connects HITS to advanced topics in random matrix theory and is essential for understanding how to validate and determine the fundamental limits of spectral algorithms .",
            "id": "4281925",
            "problem": "Consider the Hyperlink-Induced Topic Search (HITS) algorithm, which iteratively computes hub and authority scores on a directed graph with adjacency matrix $A \\in \\{0,1\\}^{n_h \\times n_a}$, where $n_h$ is the number of hub nodes and $n_a$ is the number of authority nodes. In HITS, the hub score vector $h \\in \\mathbb{R}^{n_h}$ and the authority score vector $a \\in \\mathbb{R}^{n_a}$ are updated by the fundamental definitions $h^{(t)} = A a^{(t-1)}$ and $a^{(t)} = A^{\\top} h^{(t)}$, with normalization after each iteration to prevent divergence. Assume $h^{(0)}$ and $a^{(0)}$ are nonzero and nonnegative. These updates implement a power iteration on $A A^{\\top}$ and $A^{\\top} A$, respectively.\n\nYou are tasked with designing a synthetic benchmark generator to stress-test HITS’s ability to recover planted hub and authority rankings. Let the planted hub and authority score vectors be $u \\in \\mathbb{R}^{n_h}$ and $v \\in \\mathbb{R}^{n_a}$, respectively, both nonnegative and normalized to unit $\\ell_2$-norm; that is, $\\|u\\|_2 = 1$ and $\\|v\\|_2 = 1$. A generator should specify, for each pair $(i,j)$ with $i \\in \\{1,\\dots,n_h\\}$ and $j \\in \\{1,\\dots,n_a\\}$, the probability $p_{ij} \\in [0,1]$ of a directed edge from hub $i$ to authority $j$. The benchmark should allow control of signal strength and noise so that one can tune the difficulty of recovery.\n\nAssume independence of edges conditioned on the model parameters. Recovery is defined as convergence of HITS to score vectors whose cosine similarity with $u$ and $v$ tends to $1$ as $n_h$ and $n_a$ grow, up to global scaling inherent in HITS.\n\nWhich of the following benchmark designs and parameter regimes guarantee, with high probability as $n_h, n_a \\to \\infty$, that HITS recovers the planted $u$ and $v$ (up to scaling), by ensuring that the planted rank-$1$ signal dominates both the mean background and the random fluctuations?\n\nA. Directed bipartite generator with independent edges and probabilities $p_{ij} = \\eta + \\lambda u_i v_j$, with parameters $\\eta \\in [0,1]$ and $\\lambda \\ge 0$ chosen so that $p_{ij} \\in [0,1]$ for all $(i,j)$. Under the asymptotic regime with $n_h, n_a \\to \\infty$, require simultaneously that the rank-$1$ signal singular value exceeds both the uniform-background singular value and the fluctuation spectral norm, namely\n$$\n\\lambda \\|u\\|_2 \\|v\\|_2 > \\eta \\sqrt{n_h n_a} \\quad \\text{and} \\quad \\lambda \\|u\\|_2 \\|v\\|_2 > C \\sqrt{\\eta (1-\\eta)}\\left(\\sqrt{n_h} + \\sqrt{n_a}\\right),\n$$\nfor some absolute constant $C > 1$. Under these conditions, HITS applied to $A$ recovers $u$ and $v$ up to scaling with high probability.\n\nB. Undirected generator with symmetric adjacency, where edges are placed with probabilities $p_{ij} = \\eta + \\lambda u_i v_j$ and HITS is run on the symmetrized matrix $(A + A^{\\top})/2$. Because the matrix is symmetric, recovery is guaranteed regardless of $\\eta$ and $\\lambda$.\n\nC. Directed generator with heavy-tailed degrees induced by preferential attachment mechanics, where $p_{ij} \\propto u_i v_j$, but a global additive noise level $\\eta$ increases the in-degrees and out-degrees of all nodes uniformly. Recovery improves monotonically with increasing $\\eta$ because higher degrees stabilize HITS.\n\nD. Directed generator with additive separable signal, $p_{ij} = \\eta + \\lambda(u_i + v_j)$, with $\\eta \\in [0,1]$ and $\\lambda \\ge 0$ chosen so that $p_{ij} \\in [0,1]$. As $n_h, n_a \\to \\infty$, HITS recovers $u$ and $v$ up to scaling because the top singular vector of the expected adjacency aligns with the planted rankings.\n\nSelect the correct option(s).",
            "solution": "The problem asks to identify a benchmark generation scheme for the Hyperlink-Induced Topic Search (HITS) algorithm that guarantees, under certain parameter regimes, the recovery of planted hub and authority score vectors, $u$ and $v$.\n\nFirst, let us formalize the core principles of HITS and the recovery problem. The HITS algorithm operates on a directed graph represented by an adjacency matrix $A \\in \\{0, 1\\}^{n_h \\times n_a}$. The iterative updates are given by $h^{(t)} = A a^{(t-1)}$ and $a^{(t)} = A^{\\top} h^{(t)}$, followed by normalization. Substituting one update into the other reveals the underlying mechanics:\n$$h^{(t+1)} = A a^{(t)} = A (A^{\\top} h^{(t)}) = (A A^{\\top}) h^{(t)}$$\n$$a^{(t)} = A^{\\top} h^{(t)} = A^{\\top} (A a^{(t-1)}) = (A^{\\top} A) a^{(t-1)}$$\nThese are the equations for the power iteration method. Consequently, the hub vector $h$ converges to the principal eigenvector of the matrix $A A^{\\top}$, and the authority vector $a$ converges to the principal eigenvector of $A^{\\top} A$. The principal eigenvectors of $A A^{\\top}$ and $A^{\\top} A$ are, by definition, the principal left and right singular vectors of the matrix $A$, respectively.\n\nThe problem of recovering the planted vectors $u \\in \\mathbb{R}^{n_h}$ and $v \\in \\mathbb{R}^{n_a}$ thus translates to finding conditions under which the principal left and right singular vectors of the randomly generated adjacency matrix $A$ are aligned with $u$ and $v$. The matrix $A$ is random, with entries $A_{ij}$ being independent Bernoulli random variables with success probability $p_{ij}$, i.e., $A_{ij} \\sim \\text{Bernoulli}(p_{ij})$.\n\nWe can decompose the matrix $A$ into its expectation and a zero-mean fluctuation matrix:\n$$A = \\mathbb{E}[A] + (A - \\mathbb{E}[A]) = P + W$$\nwhere $P$ is a matrix with entries $P_{ij} = p_{ij}$, and $W$ is the noise matrix. For recovery to be successful, the \"signal\" within $P$ corresponding to the planted vectors $(u, v)$ must be strong enough to dominate both any other structural components within $P$ and the random fluctuations $W$. The strength of these matrix components is measured by their spectral norm (the largest singular value).\n\nWe will now evaluate each option based on this framework.\n\n**Analysis of Option A:**\n\nThe proposed generator uses edge probabilities $p_{ij} = \\eta + \\lambda u_i v_j$. The parameters $\\eta \\in [0,1]$ and $\\lambda \\ge 0$ are constrained such that $p_{ij} \\in [0,1]$. The expected adjacency matrix is:\n$$P = \\mathbb{E}[A] = \\eta J + \\lambda u v^{\\top}$$\nwhere $J$ is the $n_h \\times n_a$ matrix of all ones. The term $\\lambda u v^{\\top}$ is the planted rank-$1$ signal. Since $\\|u\\|_2 = 1$ and $\\|v\\|_2 = 1$, the singular value of this signal matrix is $\\lambda$. Its corresponding singular vectors are $u$ and $v$.\n\nThe signal must contend with two sources of \"noise\":\n1.  **The deterministic background:** The term $\\eta J$ is also a rank-$1$ matrix. Its singular value is $\\|\\eta J\\|_2 = \\eta \\|J\\|_2 = \\eta \\sqrt{n_h n_a}$. Its singular vectors are proportional to the all-ones vectors $\\mathbf{1}_{n_h}$ and $\\mathbf{1}_{n_a}$.\n2.  **The random fluctuations:** The matrix $W = A - P$ is a zero-mean random matrix. The variance of its entries is $\\text{Var}(A_{ij}) = p_{ij}(1-p_{ij})$. If we assume $\\lambda u_i v_j$ is small, the variance is approximately $\\eta(1-\\eta)$. From random matrix theory, the spectral norm of such a matrix is, with high probability, bounded by $\\|W\\|_2 \\le C \\sqrt{\\text{Var}_{max}}(\\sqrt{n_h} + \\sqrt{n_a})$ for some constant $C$. Using $\\sqrt{\\eta(1-\\eta)}$ as the characteristic standard deviation is a valid approximation.\n\nOption A proposes two conditions for recovery:\n$$\n\\lambda \\|u\\|_2 \\|v\\|_2 > \\eta \\sqrt{n_h n_a} \\quad \\text{and} \\quad \\lambda \\|u\\|_2 \\|v\\|_2 > C \\sqrt{\\eta (1-\\eta)}\\left(\\sqrt{n_h} + \\sqrt{n_a}\\right)\n$$\nGiven $\\|u\\|_2 = 1$ and $\\|v\\|_2 = 1$, these simplify to:\n$$\n\\lambda > \\eta \\sqrt{n_h n_a} \\quad \\text{and} \\quad \\lambda > C \\sqrt{\\eta (1-\\eta)}\\left(\\sqrt{n_h} + \\sqrt{n_a}\\right)\n$$\nThe first condition ensures that the singular value of the planted signal ($\\lambda$) is greater than the singular value of the uniform background component of the expectation ($\\eta\\sqrt{n_h n_a}$). This ensures that the principal singular vectors of the expected matrix $P$ are primarily determined by the $u v^{\\top}$ term, not the $J$ term.\nThe second condition ensures that the signal's singular value also exceeds the spectral norm of the random fluctuation matrix $W$. According to perturbation theory for singular vectors (e.g., the Davis-Kahan theorem), this condition is necessary to prevent the random noise from overwhelming the signal and rotating the singular vectors away from the desired $(u, v)$ directions.\nThese two conditions correctly capture the requirements for signal recovery in this \"spiked\" random matrix model. Therefore, this option presents a valid regime for HITS to recover the planted structure.\n\nVerdict: **Correct**.\n\n**Analysis of Option B:**\n\nThis option proposes an undirected graph ($A=A^\\top, n_h=n_a=n$) and running HITS on the symmetrized matrix $(A+A^\\top)/2$, which is just $A$ itself. In this symmetric case, HITS computes the principal eigenvector of $A^2$, which, for a non-negative matrix $A$, is the principal eigenvector of $A$. The central claim is that \"recovery is guaranteed regardless of $\\eta$ and $\\lambda$\".\nThis claim is fundamentally incorrect. Spectral methods for recovering planted structures always depend on a sufficiently high signal-to-noise ratio. If $\\lambda=0$, the graph model is $p_{ij} = \\eta$, which is the Erdős-Rényi model $G(n, \\eta)$. The principal eigenvector of its adjacency matrix is known to be highly correlated with the all-ones vector and carries no information about the planted vectors $u$ and $v$ (which were not used in its generation). If $\\lambda$ is positive but very small, the signal $\\lambda u v^\\top$ will be drowned out by the background noise $\\eta J$ and random fluctuations. A phase transition exists, below which recovery is impossible. The statement that recovery is independent of $\\eta$ and $\\lambda$ violates this basic principle.\n\nVerdict: **Incorrect**.\n\n**Analysis of Option C:**\n\nThis option correctly notes that a model like $p_{ij} \\propto u_i v_j$ can produce heavy-tailed degree distributions if $u$ and $v$ are chosen appropriately. It then considers an additive noise model, likely $p_{ij} = \\eta' + \\lambda u_i v_j$. The core claim is that \"Recovery improves monotonically with increasing $\\eta$\".\nThis is contrary to the physics of the problem. As analyzed for Option A, the term $\\eta$ introduces two competing effects, both of which degrade the signal-to-noise ratio.\n1.  It adds a deterministic background matrix $\\eta J$ to the expectation, whose spectral norm $\\eta \\sqrt{n_h n_a}$ grows with $\\eta$. This provides a stronger competing structure that masks the desired signal.\n2.  It affects the variance of the random fluctuations, $\\text{Var}(A_{ij}) = p_{ij}(1-p_{ij})$. For a fixed signal part, increasing $\\eta$ from $0$ towards $0.5$ increases the variance, thus increasing the spectral norm of the noise matrix $W$.\nBoth effects make it *harder*, not easier, to recover the signal. The claim that higher degrees stabilize HITS is a misapplication of a general idea; in this context, uniformly increasing edge density with unstructured noise $\\eta$ is detrimental to recovering a specific planted structure.\n\nVerdict: **Incorrect**.\n\n**Analysis of Option D:**\n\nThis option proposes an additive signal model, $p_{ij} = \\eta + \\lambda(u_i + v_j)$. The expected adjacency matrix is:\n$$P = \\mathbb{E}[A] = \\eta J + \\lambda u \\mathbf{1}_{n_a}^{\\top} + \\lambda \\mathbf{1}_{n_h} v^{\\top}$$\nwhere $\\mathbf{1}_{n_a}$ and $\\mathbf{1}_{n_h}$ are column vectors of ones of the appropriate dimension. The goal of HITS is to find the principal singular vectors of $A$, which are approximated by those of $P$. For recovery to work, the principal singular vectors of $P$ must be $(u,v)$. Let us check if $(u,v)$ form a singular vector pair for $P$:\n$$P v = (\\eta J + \\lambda u \\mathbf{1}_{n_a}^{\\top} + \\lambda \\mathbf{1}_{n_h} v^{\\top}) v = \\eta \\mathbf{1}_{n_h}(\\mathbf{1}_{n_a}^{\\top}v) + \\lambda u (\\mathbf{1}_{n_a}^{\\top}v) + \\lambda \\mathbf{1}_{n_h}(v^{\\top}v)$$\nSince $v^{\\top}v = \\|v\\|_2^2=1$, this simplifies to:\n$$P v = \\eta (\\mathbf{1}_{n_a}^{\\top}v) \\mathbf{1}_{n_h} + \\lambda (\\mathbf{1}_{n_a}^{\\top}v) u + \\lambda \\mathbf{1}_{n_h}$$\nThis resulting vector is a linear combination of $u$ and $\\mathbf{1}_{n_h}$. It is not, in general, proportional to $u$. Similarly, one can show that $P^{\\top} u$ is not proportional to $v$. Because $(u,v)$ is not a singular vector pair of the expected matrix $P$, the HITS algorithm will not converge to score vectors proportional to $u$ and $v$. The fundamental structure of the signal in this model is mismatched with the operator (SVD) used for recovery. The correct signal structure for this problem is multiplicative, $u_i v_j$, not additive, $u_i+v_j$.\n\nVerdict: **Incorrect**.\n\nBased on this detailed analysis, only Option A correctly describes a generative model and a valid set of conditions, grounded in random matrix theory, that ensure the recovery of the planted vectors.",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}