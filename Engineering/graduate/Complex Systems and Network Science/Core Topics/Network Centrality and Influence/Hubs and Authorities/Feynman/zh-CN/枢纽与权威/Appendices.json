{
    "hands_on_practices": [
        {
            "introduction": "要真正掌握HITS算法，没有什么比直接计算更有效。第一个练习将引导你体验其核心机制，在一个小而具体的网络上执行两轮完整的迭代。通过手动计算更新和归一化步骤，你将亲眼见证中心度（hub）和权威度（authority）分数如何相互增强并逐步演变。",
            "id": "4281863",
            "problem": "考虑一个包含$4$个节点的有向网络，其邻接矩阵为$A \\in \\mathbb{R}^{4 \\times 4}$，其中如果存在从节点$i$到节点$j$的有向边，则$A_{ij} = 1$，否则$A_{ij} = 0$。该网络由以下矩阵给出：\n$$\nA \\;=\\;\n\\begin{pmatrix}\n0  1  1  0 \\\\\n0  0  1  1 \\\\\n0  1  0  1 \\\\\n0  1  0  0\n\\end{pmatrix}.\n$$\n在超链接诱导主题搜索（HITS）算法中，每个节点被分配一个中心度分数（hub score）和一个权威度分数（authority score）。基本定义如下：一个节点的权威度分数与指向该节点的那些节点的中心度分数之和成正比，而一个节点的中心度分数与该节点指向的那些节点的权威度分数之和成正比。\n\n从初始中心度向量 $h^{(0)} \\in \\mathbb{R}^{4}$ 开始，该向量被选为单位长度且各分量相等，即\n$$\nh^{(0)} \\;=\\; \\frac{1}{\\sqrt{4}} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{pmatrix} \\;=\\; \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{1}{2} \\\\ \\frac{1}{2} \\\\ \\frac{1}{2} \\end{pmatrix},\n$$\n按照以下步骤执行恰好$2$次完整的HITS更新过程。在每次完整迭代中，首先使用比例定义从中心度向量计算权威度向量，然后将权威度向量归一化使其欧几里得范数为$1$；接着使用比例定义从权威度向量计算中心度向量，然后将中心度向量归一化使其欧几里得范数为$1$。在每个子步骤中，明确展示未归一化的向量、归一化因子和归一化的向量，从而追踪$a$和$h$的演变过程。\n\n仅以上述基本定义为出发点，推导出所需的线性更新规则，并使用给定的$A$和$h^{(0)}$进行计算。在第二次权威度更新归一化结束时，节点$2$的权威度分数$a^{(2)}_{2}$的精确值是多少？以单一闭式表达式给出你的答案。不要对答案进行四舍五入。",
            "solution": "问题要求在给定网络上执行两次完整的超链接诱导主题搜索（HITS）算法迭代后，计算一个特定的权威度分数。首先，我们必须根据给定的定义来形式化更新规则。\n\n对于一个有$N$个节点的网络，设$a = (a_1, a_2, \\dots, a_N)^T$为权威度向量，$h = (h_1, h_2, \\dots, h_N)^T$为中心度向量。邻接矩阵为$A$，其中如果存在从节点$i$到节点$j$的有向边，则$A_{ij}=1$。\n\n提供的定义如下：\n1.  节点$j$的权威度分数$a_j$与指向它的节点的中心度分数之和成正比。指向节点$j$的节点$i$是那些满足$A_{ij}=1$的节点。因此，\n    $$a_j \\propto \\sum_{i=1}^N A_{ij} h_i$$\n    这是向量积$A^T h$的第$j$个分量。所以，我们记为$\\tilde{a}$的未归一化权威度向量由以下更新规则给出：\n    $$\\tilde{a} = A^T h$$\n\n2.  节点$i$的中心度分数$h_i$与它指向的节点的权威度分数之和成正比。节点$i$指向的节点$j$是那些满足$A_{ij}=1$的节点。因此，\n    $$h_i \\propto \\sum_{j=1}^N A_{ij} a_j$$\n    这是向量积$A a$的第$i$个分量。所以，我们记为$\\tilde{h}$的未归一化中心度向量由以下更新规则给出：\n    $$\\tilde{h} = A a$$\n\n一次从步骤$k$到$k+1$的完整迭代，包括一次权威度更新和一次中心度更新，并在每个阶段进行归一化。\n设$h^{(k)}$为步骤$k$的归一化中心度向量。\n1.  计算未归一化的权威度向量：$\\tilde{a}^{(k+1)} = A^T h^{(k)}$。\n2.  归一化得到权威度向量：$a^{(k+1)} = \\frac{\\tilde{a}^{(k+1)}}{\\|\\tilde{a}^{(k+1)}\\|}$，其中$\\|\\cdot\\|$表示欧几里得范数。\n3.  计算未归一化的中心度向量：$\\tilde{h}^{(k+1)} = A a^{(k+1)}$。\n4.  归一化得到中心度向量：$h^{(k+1)} = \\frac{\\tilde{h}^{(k+1)}}{\\|\\tilde{h}^{(k+1)}\\|}$。\n\n问题给出了一个$N=4$个节点的网络的邻接矩阵：\n$$A = \\begin{pmatrix} 0  1  1  0 \\\\ 0  0  1  1 \\\\ 0  1  0  1 \\\\ 0  1  0  0 \\end{pmatrix}$$\n邻接矩阵的转置是：\n$$A^T = \\begin{pmatrix} 0  0  0  0 \\\\ 1  0  1  1 \\\\ 1  1  0  0 \\\\ 0  1  1  0 \\end{pmatrix}$$\n初始中心度向量给定为：\n$$h^{(0)} = \\begin{pmatrix} 1/2 \\\\ 1/2 \\\\ 1/2 \\\\ 1/2 \\end{pmatrix}$$\n\n我们现在执行两次完整迭代。\n\n**第1次迭代：**\n\n首先，我们使用$h^{(0)}$计算未归一化的权威度向量$\\tilde{a}^{(1)}$：\n$$\\tilde{a}^{(1)} = A^T h^{(0)} = \\begin{pmatrix} 0  0  0  0 \\\\ 1  0  1  1 \\\\ 1  1  0  0 \\\\ 0  1  1  0 \\end{pmatrix} \\begin{pmatrix} 1/2 \\\\ 1/2 \\\\ 1/2 \\\\ 1/2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1/2 + 1/2 + 1/2 \\\\ 1/2 + 1/2 \\\\ 1/2 + 1/2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 3/2 \\\\ 1 \\\\ 1 \\end{pmatrix}$$\n接下来，我们归一化$\\tilde{a}^{(1)}$。归一化因子是其欧几里得范数：\n$$\\|\\tilde{a}^{(1)}\\| = \\sqrt{0^2 + \\left(\\frac{3}{2}\\right)^2 + 1^2 + 1^2} = \\sqrt{\\frac{9}{4} + 1 + 1} = \\sqrt{\\frac{9}{4} + \\frac{8}{4}} = \\sqrt{\\frac{17}{4}} = \\frac{\\sqrt{17}}{2}$$\n归一化的权威度向量$a^{(1)}$是：\n$$a^{(1)} = \\frac{1}{\\|\\tilde{a}^{(1)}\\|} \\tilde{a}^{(1)} = \\frac{2}{\\sqrt{17}} \\begin{pmatrix} 0 \\\\ 3/2 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\frac{1}{\\sqrt{17}} \\begin{pmatrix} 0 \\\\ 3 \\\\ 2 \\\\ 2 \\end{pmatrix}$$\n现在，我们使用$a^{(1)}$计算未归一化的中心度向量$\\tilde{h}^{(1)}$：\n$$\\tilde{h}^{(1)} = A a^{(1)} = \\begin{pmatrix} 0  1  1  0 \\\\ 0  0  1  1 \\\\ 0  1  0  1 \\\\ 0  1  0  0 \\end{pmatrix} \\frac{1}{\\sqrt{17}} \\begin{pmatrix} 0 \\\\ 3 \\\\ 2 \\\\ 2 \\end{pmatrix} = \\frac{1}{\\sqrt{17}} \\begin{pmatrix} 3+2 \\\\ 2+2 \\\\ 3+2 \\\\ 3 \\end{pmatrix} = \\frac{1}{\\sqrt{17}} \\begin{pmatrix} 5 \\\\ 4 \\\\ 5 \\\\ 3 \\end{pmatrix}$$\n本次迭代的最后一步，我们归一化$\\tilde{h}^{(1)}$。其范数为：\n$$\\|\\tilde{h}^{(1)}\\| = \\left\\| \\frac{1}{\\sqrt{17}} \\begin{pmatrix} 5 \\\\ 4 \\\\ 5 \\\\ 3 \\end{pmatrix} \\right\\| = \\frac{1}{\\sqrt{17}} \\sqrt{5^2 + 4^2 + 5^2 + 3^2} = \\frac{1}{\\sqrt{17}} \\sqrt{25 + 16 + 25 + 9} = \\frac{\\sqrt{75}}{\\sqrt{17}} = \\frac{5\\sqrt{3}}{\\sqrt{17}}$$\n归一化的中心度向量$h^{(1)}$是：\n$$h^{(1)} = \\frac{1}{\\|\\tilde{h}^{(1)}\\|} \\tilde{h}^{(1)} = \\frac{\\sqrt{17}}{5\\sqrt{3}} \\left( \\frac{1}{\\sqrt{17}} \\begin{pmatrix} 5 \\\\ 4 \\\\ 5 \\\\ 3 \\end{pmatrix} \\right) = \\frac{1}{5\\sqrt{3}} \\begin{pmatrix} 5 \\\\ 4 \\\\ 5 \\\\ 3 \\end{pmatrix}$$\n\n**第2次迭代：**\n\n我们以$h^{(1)}$开始第二次迭代，计算$\\tilde{a}^{(2)}$：\n$$\\tilde{a}^{(2)} = A^T h^{(1)} = \\begin{pmatrix} 0  0  0  0 \\\\ 1  0  1  1 \\\\ 1  1  0  0 \\\\ 0  1  1  0 \\end{pmatrix} \\frac{1}{5\\sqrt{3}} \\begin{pmatrix} 5 \\\\ 4 \\\\ 5 \\\\ 3 \\end{pmatrix} = \\frac{1}{5\\sqrt{3}} \\begin{pmatrix} 0 \\\\ 5+0+5+3 \\\\ 5+4+0+0 \\\\ 0+4+5+0 \\end{pmatrix} = \\frac{1}{5\\sqrt{3}} \\begin{pmatrix} 0 \\\\ 13 \\\\ 9 \\\\ 9 \\end{pmatrix}$$\n接下来，我们归一化$\\tilde{a}^{(2)}$。归一化因子是：\n$$\\|\\tilde{a}^{(2)}\\| = \\left\\| \\frac{1}{5\\sqrt{3}} \\begin{pmatrix} 0 \\\\ 13 \\\\ 9 \\\\ 9 \\end{pmatrix} \\right\\| = \\frac{1}{5\\sqrt{3}} \\sqrt{0^2 + 13^2 + 9^2 + 9^2} = \\frac{1}{5\\sqrt{3}} \\sqrt{169 + 81 + 81} = \\frac{\\sqrt{331}}{5\\sqrt{3}}$$\n归一化的权威度向量$a^{(2)}$是：\n$$a^{(2)} = \\frac{1}{\\|\\tilde{a}^{(2)}\\|} \\tilde{a}^{(2)} = \\frac{5\\sqrt{3}}{\\sqrt{331}} \\left( \\frac{1}{5\\sqrt{3}} \\begin{pmatrix} 0 \\\\ 13 \\\\ 9 \\\\ 9 \\end{pmatrix} \\right) = \\frac{1}{\\sqrt{331}} \\begin{pmatrix} 0 \\\\ 13 \\\\ 9 \\\\ 9 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 13/\\sqrt{331} \\\\ 9/\\sqrt{331} \\\\ 9/\\sqrt{331} \\end{pmatrix}$$\n问题要求的是第二次权威度更新归一化结束时节点$2$的权威度分数，即分量$a^{(2)}_{2}$。根据我们计算出的向量$a^{(2)}$，这个值是：\n$$a^{(2)}_{2} = \\frac{13}{\\sqrt{331}}$$\n数字$331$是质数，所以这个表达式不能进一步简化。",
            "answer": "$$\\boxed{\\frac{13}{\\sqrt{331}}}$$"
        },
        {
            "introduction": "熟悉了迭代过程之后，我们现在转向建立关于中心度和权威度分数所代表含义的直观理解。这个问题分析一个有向星形图，这是一种理想化的结构，它完美地诠释了中心度与权威度的概念。通过分析这种极端情况，我们将揭示网络拓扑如何将节点清晰地划分为“纯粹的中心”和“纯粹的权威”。",
            "id": "4281856",
            "problem": "考虑一个有向星形图 $G_n$，其节点集为 $\\{v_0,v_1,\\dots,v_n\\}$，其中 $v_0$ 是中心节点，且对于每个 $i \\in \\{1,\\dots,n\\}$，都有一条从 $v_0$ 到 $v_i$ 的有向边。图中没有其他边。令 $A \\in \\{0,1\\}^{(n+1)\\times(n+1)}$ 为邻接矩阵，其定义为：如果存在从节点 $i$ 到节点 $j$ 的有向边，则 $A_{ij}=1$，否则 $A_{ij}=0$。\n\n超链接诱导主题搜索（Hyperlink-Induced Topic Search, HITS）框架定义了两个得分向量：一个中心度（hub）得分向量 $h$ 和一个权威度（authority）得分向量 $a$。在标准的线性代数表述中，这些向量源于由 $A$ 构建的两步路径算子的主导谱结构；特别地，共参考文献（co-reference）相似性和同被引（co-citation）相似性分别由 $A A^{\\top}$ 和 $A^{\\top} A$ 捕获。在标准归一化中，中心度得分向量和权威度得分向量被缩放至单位欧几里得范数，即 $\\|h\\|_2 = 1$ 和 $\\|a\\|_2 = 1$。\n\n从 $(A A^{\\top})_{ij}$ 计数两步路径 $i \\to \\cdot \\to j$ 以及 $(A^{\\top} A)_{ij}$ 计数两步路径 $\\cdot \\to i \\leftarrow \\cdot$（同被引）的解释出发，推导为什么 $A A^{\\top}$ 和 $A^{\\top} A$ 的主特征向量分别编码了中心度和权威度中心性。然后，对于上述图 $G_n$：\n- 明确计算单位范数的中心度得分向量 $h$ 和单位范数的权威度得分向量 $a$（用 $n$ 表示）。\n- 定性解释中心节点 $v_0$ 和叶节点 $v_i$ 在中心度和权威度角色上的差异。\n\n最后，令 $R(n)$ 表示在单位范数归一化下，中心节点的中心度得分与任意叶节点的权威度得分之比，即 $R(n) = \\frac{h(v_0)}{a(v_i)}$（对于任意 $i \\in \\{1,\\dots,n\\}$）。请以 $n$ 的单个闭式解析表达式给出 $R(n)$。无需四舍五入。",
            "solution": "解答过程首先按要求提供HITS算法的理论基础，然后为给定图构建具体矩阵，计算中心度和权威度得分向量，最后利用这些结果进行定性解释并计算所需比率。\n\n### HITS算法及其特征向量表述\n\n超链接诱导主题搜索（HITS）算法基于有向网络中两类节点之间的相互增强关系：中心（hubs）和权威（authorities）。\n一个好的权威节点是被许多好的中心节点指向的节点。\n一个好的中心节点是指向许多好的权威节点的节点。\n\n令节点 $j$ 的权威度得分为 $a_j$，节点 $i$ 的中心度得分为 $h_i$。这些得分由以下迭代更新定义：\n1. 一个节点的权威度得分是所有指向该节点的节点的中心度得分之和。\n$$a_j \\propto \\sum_{i: i \\to j} h_i$$\n令 $A$ 为邻接矩阵，其中如果存在从 $i$ 到 $j$ 的边，则 $A_{ij}=1$。问题陈述 $A_{ij}=1$ 对应边 $i \\to j$。所以，求和应该是对所有满足 $k \\to j$ 的节点 $k$ 进行，这由 $A_{kj}=1$ 给出。\n$$a_j \\propto \\sum_k A_{kj} h_k$$\n用向量形式表示，即 $a \\propto A^{\\top} h$。\n\n2. 一个节点的中心度得分是它指向的所有节点的权威度得分之和。\n$$h_i \\propto \\sum_{j: i \\to j} a_j = \\sum_j A_{ij} a_j$$\n用向量形式表示，即 $h \\propto A a$。\n\n将这两个关系相互代入，我们得到特征向量方程：\n$$a \\propto A^{\\top}h \\propto A^{\\top}(Aa) = (A^{\\top}A)a$$\n$$h \\propto Aa \\propto A(A^{\\top}h) = (AA^{\\top})h$$\n这表明权威度向量 $a$ 是矩阵 $A^{\\top}A$ 的一个特征向量，中心度向量 $h$ 是矩阵 $AA^{\\top}$ 的一个特征向量。为确保得分非负，我们选择主特征向量。根据非负矩阵的Perron-Frobenius定理，该向量对应于最大的非负特征值，并且其所有分量都是非负的。\n\n问题要求基于路径计数的解释。我们来分析这些矩阵的元素：\n- 矩阵 $A^{\\top}A$ 是同被引（co-citation）矩阵。其第 $(i,j)$ 个元素是 $(A^{\\top}A)_{ij} = \\sum_k (A^{\\top})_{ik} A_{kj} = \\sum_k A_{ki} A_{kj}$。这个和计算了同时有有向边指向节点 $i$ 和节点 $j$ 的节点 $k$ 的数量（即模式 $k \\to i$ 和 $k \\to j$）。被相同来源引用的节点被认为是相似的权威。因此，该矩阵的主特征向量标识了最突出的权威。\n- 矩阵 $AA^{\\top}$ 是共参考文献（co-reference）矩阵（或文献耦合矩阵）。其第 $(i,j)$ 个元素是 $(AA^{\\top})_{ij} = \\sum_k A_{ik} (A^{\\top})_{kj} = \\sum_k A_{ik} A_{jk}$。这个和计算了节点 $i$ 和节点 $j$ 同时指向的节点 $k$ 的数量（即模式 $i \\to k$ 和 $j \\to k$）。指向相同目标的节点被认为是相似的中心。该矩阵的主特征向量标识了最突出的中心。\n\n### 图 $G_n$ 的矩阵构建\n\n图 $G_n$ 有 $n+1$ 个节点，我们将其标记为 $\\{0, 1, \\dots, n\\}$，对应于 $\\{v_0, v_1, \\dots, v_n\\}$。节点 $v_0$ 是中心。对于 $i \\in \\{1, \\dots, n\\}$，都有一条从 $v_0$ 到 $v_i$ 的有向边。\n邻接矩阵 $A$ 是一个 $(n+1) \\times (n+1)$ 矩阵。唯一的非零元素是 $A_{0i}=1$，其中 $i \\in \\{1, \\dots, n\\}$。\n$$\nA =\n\\begin{pmatrix}\n0  1  1  \\dots  1 \\\\\n0  0  0  \\dots  0 \\\\\n0  0  0  \\dots  0 \\\\\n\\vdots  \\vdots  \\vdots  \\ddots  \\vdots \\\\\n0  0  0  \\dots  0\n\\end{pmatrix}\n$$\n$A$ 的转置矩阵 $A^{\\top}$ 是：\n$$\nA^{\\top} =\n\\begin{pmatrix}\n0  0  0  \\dots  0 \\\\\n1  0  0  \\dots  0 \\\\\n1  0  0  \\dots  0 \\\\\n\\vdots  \\vdots  \\vdots  \\ddots  \\vdots \\\\\n1  0  0  \\dots  0\n\\end{pmatrix}\n$$\n\n### 中心度得分向量计算\n\n中心度得分由 $AA^{\\top}$ 的主特征向量给出。\n$$\nAA^{\\top} =\n\\begin{pmatrix}\n0  1  \\dots  1 \\\\\n0  0  \\dots  0 \\\\\n\\vdots  \\vdots  \\ddots  \\vdots \\\\\n0  0  \\dots  0\n\\end{pmatrix}\n\\begin{pmatrix}\n0  0  \\dots  0 \\\\\n1  0  \\dots  0 \\\\\n\\vdots  \\vdots  \\ddots  \\vdots \\\\\n1  0  \\dots  0\n\\end{pmatrix}\n$$\n唯一的非零元素是 $(AA^{\\top})_{00} = \\sum_{k=1}^{n} 1 \\cdot 1 = n$。所有其他元素均为 $0$。\n$$\nAA^{\\top} =\n\\begin{pmatrix}\nn  0  0  \\dots  0 \\\\\n0  0  0  \\dots  0 \\\\\n0  0  0  \\dots  0 \\\\\n\\vdots  \\vdots  \\vdots  \\ddots  \\vdots \\\\\n0  0  0  \\dots  0\n\\end{pmatrix}\n$$\n这是一个对角矩阵。其特征值是对角线上的元素：$n$（重数为1）和 $0$（重数为 $n$）。主特征值为 $\\lambda_{hub} = n$。对应的特征向量 $h = (h_0, h_1, \\dots, h_n)^{\\top}$ 满足 $(AA^{\\top})h = nh$。\n这得到方程组：$nh_0 = nh_0$，以及对于 $i \\in \\{1, \\dots, n\\}$，$0 = nh_i$。\n这意味着对于 $i \\ge 1$，$h_i = 0$。向量 $h$ 的形式为 $(c, 0, \\dots, 0)^{\\top}$，其中 $c$ 是某个常数。\n我们应用归一化条件 $\\|h\\|_2=1$：$\\sqrt{c^2 + 0^2 + \\dots + 0^2} = 1$，这给出 $|c|=1$。选择通常的非负特征向量，我们得到 $c=1$。\n单位范数的中心度得分向量为 $h = (1, 0, \\dots, 0)^{\\top}$。\n所以，$h(v_0) = 1$ 且对于 $i \\in \\{1,\\dots,n\\}$，$h(v_i) = 0$。\n\n### 权威度得分向量计算\n\n权威度得分由 $A^{\\top}A$ 的主特征向量给出。\n$$\nA^{\\top}A =\n\\begin{pmatrix}\n0  0  \\dots  0 \\\\\n1  0  \\dots  0 \\\\\n\\vdots  \\vdots  \\ddots  \\vdots \\\\\n1  0  \\dots  0\n\\end{pmatrix}\n\\begin{pmatrix}\n0  1  \\dots  1 \\\\\n0  0  \\dots  0 \\\\\n\\vdots  \\vdots  \\ddots  \\vdots \\\\\n0  0  \\dots  0\n\\end{pmatrix}\n$$\n我们来计算元素 $(A^{\\top}A)_{ij} = \\sum_k A_{ki} A_{kj}$。\n- 如果 $i=0$ 或 $j=0$，则 $A$ 中对应的列全为零，所以 $(A^{\\top}A)_{ij}=0$。\n- 如果 $i,j \\in \\{1, \\dots, n\\}$，则求和中唯一的非零项是 $k=0$ 时。那么 $(A^{\\top}A)_{ij} = A_{0i} A_{0j} = 1 \\cdot 1 = 1$。\n$$\nA^{\\top}A =\n\\begin{pmatrix}\n0  0  0  \\dots  0 \\\\\n0  1  1  \\dots  1 \\\\\n0  1  1  \\dots  1 \\\\\n\\vdots  \\vdots  \\vdots  \\ddots  \\vdots \\\\\n0  1  1  \\dots  1\n\\end{pmatrix}\n$$\n为了找到 $A^{\\top}A$ 的特征值 $\\lambda$ 和特征向量 $a$：$(A^{\\top}A)a = \\lambda a$。\n对于 $a = (a_0, a_1, \\dots, a_n)^{\\top}$，这给出：\n$0 = \\lambda a_0$。\n对于 $i \\in \\{1, \\dots, n\\}$，$\\sum_{j=1}^{n} a_j = \\lambda a_i$。\n主特征值必须非零（因为当 $n \\ge 1$ 时，$A^{\\top}A$ 不是零矩阵），所以从第一个方程得出 $a_0 = 0$。\n其余方程意味着 $\\lambda a_1 = \\lambda a_2 = \\dots = \\lambda a_n = \\sum_{j=1}^{n} a_j$。\n这意味着 $a_1=a_2=\\dots=a_n$。令对于 $i \\in \\{1, \\dots, n\\}$，$a_i=c$。\n则方程变为 $\\lambda c = \\sum_{j=1}^{n} c = nc$。对于非平凡解（$c \\ne 0$），我们必须有 $\\lambda = n$。\n主特征值为 $\\lambda_{auth} = n$。对应的特征向量形式为 $(0, c, c, \\dots, c)^{\\top}$。\n应用归一化 $\\|a\\|_2=1$：$\\sqrt{0^2 + nc^2} = 1 \\implies |c|\\sqrt{n} = 1$。\n对于非负特征向量，我们选择 $c = 1/\\sqrt{n}$。\n单位范数的权威度得分向量为 $a = (0, 1/\\sqrt{n}, \\dots, 1/\\sqrt{n})^{\\top}$。\n所以，$a(v_0) = 0$ 且对于 $i \\in \\{1,\\dots,n\\}$，$a(v_i) = 1/\\sqrt{n}$。\n\n### 角色的定性解释\n\n计算出的得分完美地反映了星形图 $G_n$ 的拓扑结构。\n- **中心节点 $v_0$**：该节点具有最大的中心度得分 $h(v_0)=1$ 和零权威度得分 $a(v_0)=0$。这是因为 $v_0$ 是网络中所有链接的源头；它通过指向所有其他节点，充当一个纯粹的“中心”（hub）。它没有入链，因此没有“权威”（authority）地位。\n- **叶节点 $v_i$（对于 $i \\ge 1$）**：这些节点具有零中心度得分 $h(v_i)=0$，以及相等且为正的权威度得分 $a(v_i)=1/\\sqrt{n}$。这是因为它们不指向任何节点，因此不是中心。然而，它们都接收来自唯一的、最大的中心节点（$v_0$）的链接，这使它们在网络中成为同等重要的“权威”。它们的权威度在它们之间被平均分配。\n\n这种差异是最大的：$v_0$ 是一个纯粹的中心，而叶节点 $v_i$ 是纯粹的权威。\n\n### $R(n)$ 的比值计算\n\n问题将比率 $R(n)$ 定义为 $R(n) = \\frac{h(v_0)}{a(v_i)}$，对于任意 $i \\in \\{1,\\dots,n\\}$。\n使用上面推导的得分：\n$h(v_0) = 1$\n$a(v_i) = \\frac{1}{\\sqrt{n}}$\n该比率为：\n$$R(n) = \\frac{1}{\\frac{1}{\\sqrt{n}}} = \\sqrt{n}$$\n这是该比率的闭式解析表达式。",
            "answer": "$$\\boxed{\\sqrt{n}}$$"
        },
        {
            "introduction": "我们最后的练习将从分析特定图形转向网络分析的理论前沿。这个高级问题挑战你像研究员一样思考：如何设计综合基准来测试HITS算法的极限。你将探索植根于随机矩阵理论的条件，在这些条件下，一个“植入”的信号（即真实的中心-权威结构）能够成功地从随机噪声中被恢复出来。",
            "id": "4281925",
            "problem": "考虑超链接诱导主题搜索（HITS）算法，该算法在邻接矩阵为 $A \\in \\{0,1\\}^{n_h \\times n_a}$ 的有向图上迭代计算中心（hub）分数和权威（authority）分数，其中 $n_h$ 是中心节点数，$n_a$ 是权威节点数。在 HITS 中，中心分数向量 $h \\in \\mathbb{R}^{n_h}$ 和权威分数向量 $a \\in \\mathbb{R}^{n_a}$ 通过基本定义 $h^{(t)} = A a^{(t-1)}$ 和 $a^{(t)} = A^{\\top} h^{(t)}$ 进行更新，每次迭代后进行归一化以防止发散。假设 $h^{(0)}$ 和 $a^{(0)}$ 是非零且非负的。这些更新分别在 $A A^{\\top}$ 和 $A^{\\top} A$ 上实现了幂迭代。\n\n你的任务是设计一个合成基准生成器，以压力测试 HITS 恢复植入的中心和权威排名的能力。设植入的中心和权威分数向量分别为 $u \\in \\mathbb{R}^{n_h}$ 和 $v \\in \\mathbb{R}^{n_a}$，两者都是非负的，并归一化为单位 $\\ell_2$-范数；即 $\\|u\\|_2 = 1$ 和 $\\|v\\|_2 = 1$。生成器应为每对 $(i,j)$（其中 $i \\in \\{1,\\dots,n_h\\}$，$j \\in \\{1,\\dots,n_a\\}$）指定从中心 $i$ 到权威 $j$ 的有向边的概率 $p_{ij} \\in [0,1]$。该基准应允许控制信号强度和噪声，以便可以调整恢复的难度。\n\n假设在模型参数给定的条件下，边是独立的。恢复被定义为：随着 $n_h$ 和 $n_a$ 的增长，HITS 收敛到的分数向量与 $u$ 和 $v$ 的余弦相似度趋于 $1$（在 HITS 固有的全局缩放因子下）。\n\n下列哪种基准设计和参数体系能够以高概率（当 $n_h, n_a \\to \\infty$ 时）保证 HITS 恢复植入的 $u$ 和 $v$（在缩放因子下），通过确保植入的秩-1信号主导平均背景和随机波动？\n\nA. 具有独立边的有向二分生成器，概率为 $p_{ij} = \\eta + \\lambda u_i v_j$，其中参数 $\\eta \\in [0,1]$ 和 $\\lambda \\ge 0$ 的选择使得对所有 $(i,j)$ 都有 $p_{ij} \\in [0,1]$。在 $n_h, n_a \\to \\infty$ 的渐近体系下，同时要求秩-1信号的奇异值超过均匀背景的奇异值和波动谱范数，即\n$$\n\\lambda \\|u\\|_2 \\|v\\|_2 > \\eta \\sqrt{n_h n_a} \\quad \\text{和} \\quad \\lambda \\|u\\|_2 \\|v\\|_2 > C \\sqrt{\\eta (1-\\eta)}\\left(\\sqrt{n_h} + \\sqrt{n_a}\\right),\n$$\n对于某个绝对常数 $C > 1$。在这些条件下，应用于 $A$ 的 HITS 以高概率恢复 $u$ 和 $v$（在缩放因子下）。\n\nB. 具有对称邻接关系的无向生成器，其中边以概率 $p_{ij} = \\eta + \\lambda u_i v_j$ 放置，并且 HITS 在对称化矩阵 $(A + A^{\\top})/2$ 上运行。因为矩阵是对称的，所以无论 $\\eta$ 和 $\\lambda$ 的值如何，恢复都能得到保证。\n\nC. 由优先连接机制引起的具有重尾度分布的有向生成器，其中 $p_{ij} \\propto u_i v_j$，但一个全局加性噪声水平 $\\eta$ 统一增加了所有节点的入度和出度。随着 $\\eta$ 的增加，恢复效果单调改善，因为更高的度数可以稳定 HITS。\n\nD. 具有加性可分信号的有向生成器，$p_{ij} = \\eta + \\lambda(u_i + v_j)$，其中 $\\eta \\in [0,1]$ 和 $\\lambda \\ge 0$ 的选择使得 $p_{ij} \\in [0,1]$。当 $n_h, n_a \\to \\infty$ 时，HITS 能够恢复 $u$ 和 $v$（在缩放因子下），因为期望邻接矩阵的顶层奇异向量与植入的排名对齐。\n\n选择正确的选项。",
            "solution": "该问题要求确定一种用于超链接诱导主题搜索（HITS）算法的基准生成方案，该方案在特定的参数体系下能保证恢复植入的中心和权威分数向量 $u$ 和 $v$。\n\n首先，我们形式化 HITS 的核心原理和恢复问题。HITS 算法作用于一个由邻接矩阵 $A \\in \\{0, 1\\}^{n_h \\times n_a}$ 表示的有向图。迭代更新由 $h^{(t)} = A a^{(t-1)}$ 和 $a^{(t)} = A^{\\top} h^{(t)}$ 给出，之后进行归一化。将一个更新代入另一个，揭示了其底层机制：\n$$h^{(t+1)} = A a^{(t)} = A (A^{\\top} h^{(t)}) = (A A^{\\top}) h^{(t)}$$\n$$a^{(t)} = A^{\\top} h^{(t)} = A^{\\top} (A a^{(t-1)}) = (A^{\\top} A) a^{(t-1)}$$\n这些是幂迭代法的方程。因此，中心向量 $h$ 收敛到矩阵 $A A^{\\top}$ 的主特征向量，权威向量 $a$ 收敛到矩阵 $A^{\\top} A$ 的主特征向量。根据定义，$A A^{\\top}$ 和 $A^{\\top} A$ 的主特征向量分别是矩阵 $A$ 的主左奇异向量和主右奇异向量。\n\n因此，恢复植入向量 $u \\in \\mathbb{R}^{n_h}$ 和 $v \\in \\mathbb{R}^{n_a}$ 的问题，就转化为寻找条件，使得随机生成的邻接矩阵 $A$ 的主左奇异向量和主右奇异向量与 $u$ 和 $v$ 对齐。矩阵 $A$ 是随机的，其元素 $A_{ij}$ 是独立的伯努利随机变量，成功概率为 $p_{ij}$，即 $A_{ij} \\sim \\text{Bernoulli}(p_{ij})$。\n\n我们可以将矩阵 $A$ 分解为其期望和一个零均值波动矩阵：\n$$A = \\mathbb{E}[A] + (A - \\mathbb{E}[A]) = P + W$$\n其中 $P$ 是一个元素为 $P_{ij} = p_{ij}$ 的矩阵，而 $W$ 是噪声矩阵。为了成功恢复，对应于植入向量 $(u, v)$ 的 $P$ 中的“信号”必须足够强，以主导 $P$ 内部的任何其他结构化分量以及随机波动 $W$。这些矩阵分量的强度由它们的谱范数（最大奇异值）来衡量。\n\n我们现在将基于此框架评估每个选项。\n\n**选项 A 的分析：**\n\n所提出的生成器使用边概率 $p_{ij} = \\eta + \\lambda u_i v_j$。参数 $\\eta \\in [0,1]$ 和 $\\lambda \\ge 0$ 受到约束，以使 $p_{ij} \\in [0,1]$。期望邻接矩阵为：\n$$P = \\mathbb{E}[A] = \\eta J + \\lambda u v^{\\top}$$\n其中 $J$ 是 $n_h \\times n_a$ 的全一矩阵。项 $\\lambda u v^{\\top}$ 是植入的秩-1信号。由于 $\\|u\\|_2 = 1$ 和 $\\|v\\|_2 = 1$，该信号矩阵的奇异值为 $\\lambda$。其对应的奇异向量是 $u$ 和 $v$。\n\n信号必须与两种“噪声”来源抗衡：\n1.  **确定性背景：** 项 $\\eta J$ 也是一个秩-1矩阵。其奇异值为 $\\|\\eta J\\|_2 = \\eta \\|J\\|_2 = \\eta \\sqrt{n_h n_a}$。其奇异向量与全一向量 $\\mathbf{1}_{n_h}$ 和 $\\mathbf{1}_{n_a}$ 成比例。\n2.  **随机波动：** 矩阵 $W = A - P$ 是一个零均值随机矩阵。其元素的方差是 $\\text{Var}(A_{ij}) = p_{ij}(1-p_{ij})$。如果我们假设 $\\lambda u_i v_j$ 很小，则方差近似为 $\\eta(1-\\eta)$。根据随机矩阵理论，这样一个矩阵的谱范数以高概率被 $\\|W\\|_2 \\le C \\sqrt{\\text{Var}_{max}}(\\sqrt{n_h} + \\sqrt{n_a})$ 所界定，其中 $C$ 是某个常数。使用 $\\sqrt{\\eta(1-\\eta)}$ 作为特征标准差是一个有效的近似。\n\n选项 A 提出了两个恢复条件：\n$$\n\\lambda \\|u\\|_2 \\|v\\|_2 > \\eta \\sqrt{n_h n_a} \\quad \\text{和} \\quad \\lambda \\|u\\|_2 \\|v\\|_2 > C \\sqrt{\\eta (1-\\eta)}\\left(\\sqrt{n_h} + \\sqrt{n_a}\\right)\n$$\n鉴于 $\\|u\\|_2 = 1$ 和 $\\|v\\|_2 = 1$，这些条件简化为：\n$$\n\\lambda > \\eta \\sqrt{n_h n_a} \\quad \\text{和} \\quad \\lambda > C \\sqrt{\\eta (1-\\eta)}\\left(\\sqrt{n_h} + \\sqrt{n_a}\\right)\n$$\n第一个条件确保植入信号的奇异值 ($\\lambda$) 大于期望的均匀背景分量的奇异值 ($\\eta\\sqrt{n_h n_a}$)。这确保了期望矩阵 $P$ 的主奇异向量主要由 $u v^{\\top}$ 项决定，而不是 $J$ 项。\n第二个条件确保信号的奇异值也超过随机波动矩阵 $W$ 的谱范数。根据奇异向量的扰动理论（例如，Davis-Kahan 定理），这个条件是必要的，以防止随机噪声压倒信号并将奇异向量从期望的 $(u, v)$ 方向旋转开。\n这两个条件正确地捕捉了在这种“加钉”随机矩阵模型中信号恢复的要求。因此，该选项为 HITS 恢复植入结构提供了一个有效的体系。\n\n结论：**正确**。\n\n**选项 B 的分析：**\n\n该选项提出了一个无向图（$A=A^\\top, n_h=n_a=n$），并在对称化矩阵 $(A+A^\\top)/2$（即 $A$ 本身）上运行 HITS。在这种对称情况下，HITS 计算 $A^2$ 的主特征向量，对于非负矩阵 $A$，这也是 $A$ 的主特征向量。其核心主张是“无论 $\\eta$ 和 $\\lambda$ 如何，恢复都能得到保证”。\n这个主张根本上是错误的。用于恢复植入结构的谱方法总是依赖于足够高的信噪比。如果 $\\lambda=0$，图模型是 $p_{ij} = \\eta$，即 Erdős-Rényi 模型 $G(n, \\eta)$。已知其邻接矩阵的主特征向量与全一向量高度相关，并且不携带关于植入向量 $u$ 和 $v$ 的任何信息（因为在生成过程中未使用它们）。如果 $\\lambda$ 为正但非常小，信号 $\\lambda u v^\\top$ 将被背景噪声 $\\eta J$ 和随机波动所淹没。存在一个相变点，低于此点恢复是不可能的。恢复独立于 $\\eta$ 和 $\\lambda$ 的说法违反了这一基本原则。\n\n结论：**错误**。\n\n**选项 C 的分析：**\n\n该选项正确地指出，如果 $u$ 和 $v$ 被适当地选择，像 $p_{ij} \\propto u_i v_j$ 这样的模型可以产生重尾度分布。然后它考虑一个加性噪声模型，可能是 $p_{ij} = \\eta' + \\lambda u_i v_j$。核心主张是“随着 $\\eta$ 的增加，恢复效果单调改善”。\n这与问题的物理原理相反。如对选项 A 的分析，项 $\\eta$ 引入了两种竞争效应，两者都会降低信噪比。\n1.  它向期望中添加了一个确定性背景矩阵 $\\eta J$，其谱范数 $\\eta \\sqrt{n_h n_a}$ 随 $\\eta$ 增长。这提供了一个更强的竞争结构，掩盖了期望的信号。\n2.  它影响了随机波动的方差，$\\text{Var}(A_{ij}) = p_{ij}(1-p_{ij})$。对于固定的信号部分，将 $\\eta$ 从 $0$ 增加到 $0.5$ 会增加方差，从而增加噪声矩阵 $W$ 的谱范数。\n两种效应都使得恢复信号变得*更难*，而不是更容易。声称更高的度数可以稳定 HITS 是对一个普遍想法的误用；在这种情况下，用非结构化噪声 $\\eta$ 统一增加边密度对于恢复特定的植入结构是有害的。\n\n结论：**错误**。\n\n**选项 D 的分析：**\n\n该选项提出了一个加性可分信号模型，$p_{ij} = \\eta + \\lambda(u_i + v_j)$。期望邻接矩阵是：\n$$P = \\mathbb{E}[A] = \\eta J + \\lambda u \\mathbf{1}_{n_a}^{\\top} + \\lambda \\mathbf{1}_{n_h} v^{\\top}$$\n其中 $\\mathbf{1}_{n_a}$ 和 $\\mathbf{1}_{n_h}$ 是相应维度的全一列向量。HITS 的目标是找到 $A$ 的主奇异向量，这些向量由 $P$ 的主奇异向量近似。要使恢复起作用，$P$ 的主奇异向量必须是 $(u,v)$。让我们检查 $(u,v)$ 是否构成 $P$ 的奇异向量对：\n$$P v = (\\eta J + \\lambda u \\mathbf{1}_{n_a}^{\\top} + \\lambda \\mathbf{1}_{n_h} v^{\\top}) v = \\eta \\mathbf{1}_{n_h}(\\mathbf{1}_{n_a}^{\\top}v) + \\lambda u (\\mathbf{1}_{n_a}^{\\top}v) + \\lambda \\mathbf{1}_{n_h}(v^{\\top}v)$$\n由于 $v^{\\top}v = \\|v\\|_2^2=1$，这简化为：\n$$P v = \\eta (\\mathbf{1}_{n_a}^{\\top}v) \\mathbf{1}_{n_h} + \\lambda (\\mathbf{1}_{n_a}^{\\top}v) u + \\lambda \\mathbf{1}_{n_h}$$\n所得向量是 $u$ 和 $\\mathbf{1}_{n_h}$ 的线性组合。通常情况下，它不与 $u$ 成比例。类似地，可以证明 $P^{\\top} u$ 不与 $v$ 成比例。因为 $(u,v)$ 不是期望矩阵 $P$ 的奇异向量对，所以 HITS 算法不会收敛到与 $u$ 和 $v$ 成比例的分数向量。该模型中信号的基本结构与用于恢复的算子（SVD）不匹配。这个问题的正确信号结构是乘性的 $u_i v_j$，而不是加性的 $u_i+v_j$。\n\n结论：**错误**。\n\n基于此详细分析，只有选项 A 正确描述了一个生成模型和一组基于随机矩阵理论的有效条件，这些条件确保了植入向量的恢复。",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}