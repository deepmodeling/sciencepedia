{
    "hands_on_practices": [
        {
            "introduction": "为了真正掌握 PageRank 算法的运作方式，我们首先通过一个具体的计算练习来分解其核心迭代过程。这个实践将引导你手动构建转移矩阵并执行 PageRank 更新，特别关注如何处理“悬挂节点”（dangling nodes），这是在真实网络中必须解决的一个常见问题。通过这个练习，你将对算法的每一步有更清晰的认识。",
            "id": "4296061",
            "problem": "考虑一个有 $n=4$ 个节点（标记为 $\\{1,2,3,4\\}$）的有向网络。节点 $2$ 和 $4$ 是悬挂节点（它们的出度为零）。观测到的超链接如下：节点 $1$ 链接到节点 $2$ 和 $3$，节点 $3$ 链接到节点 $1$。采用节点上的离散时间马尔可夫链的随机冲浪者模型，其中在每一步，冲浪者以概率 $\\alpha \\in (0,1)$ 跟随一个出站超链接，并以概率 $1-\\alpha$ 根据个性化分布 $\\mathbf{v} \\in \\mathbb{R}^{4}$ 进行传送。个性化向量是非均匀的，由 $\\mathbf{v} = \\big(\\frac{1}{2}, \\frac{1}{6}, \\frac{1}{3}, 0\\big)^{\\top}$ 给出，阻尼因子为 $\\alpha = \\frac{17}{20}$。马尔可夫链的转移机制必须根据以下第一性原理构建：\n- 对于任何非悬挂节点 $j$，从节点 $j$ 转移到任何节点 $i$ 的概率与节点 $j$ 观测到的出站链接成正比，并进行归一化，形成一个总和为 $1$ 的列。\n- 对于任何悬挂节点 $j$，随机转移核的对应列被个性化分布 $\\mathbf{v}$ 替代。\n\n令 $S \\in \\mathbb{R}^{4 \\times 4}$ 表示根据上述规则构建的列随机超链接矩阵。从个性化分布作为初始状态 $x^{(0)} = \\mathbf{v}$ 开始，考虑 PageRank 向量的线性迭代，其公式为 $x^{(k+1)} = \\alpha S x^{(k)} + (1-\\alpha)\\mathbf{v}$。为该网络显式地构建矩阵 $S$，验证其列随机性，并精确计算前两次迭代 $x^{(1)}$ 和 $x^{(2)}$（作为有理数）。给出 $x^{(2)}$ 的第三个分量的精确值，表示为最简分数。不需要四舍五入，也不涉及单位。",
            "solution": "问题陈述经评估有效。它在科学上基于马尔可夫链理论和 PageRank 算法，问题设定良好，包含所有必要数据和明确规则，并以客观、正式的语言表述。没有矛盾、信息缺失或违反科学原则之处。因此，我们可以进行完整解答。\n\n该问题要求构建超链接矩阵 $S$，验证其性质，并计算 PageRank 向量的前两次迭代，最终求出第二次迭代的第三个分量。\n\n首先，我们构建列随机超链接矩阵 $S \\in \\mathbb{R}^{4 \\times 4}$。该网络有 $n=4$ 个节点。节点标记为 $\\{1,2,3,4\\}$。\n\n- **节点 1** 是非悬挂节点，有指向节点 $2$ 和 $3$ 的出站链接。其出度为 $k_1=2$。$S$ 的第一列表示从节点 $1$ 开始的转移。概率均等地分布在其出站链接上。因此，该列的元素为 $S_{21} = \\frac{1}{2}$，$S_{31} = \\frac{1}{2}$，所有其他元素为 $0$。\n\n- **节点 2** 是悬挂节点。根据规则，$S$ 的第二列被个性化向量 $\\mathbf{v} = \\left(\\frac{1}{2}, \\frac{1}{6}, \\frac{1}{3}, 0\\right)^{\\top}$ 替代。\n\n- **节点 3** 是非悬挂节点，有一个指向节点 $1$ 的出站链接。其出度为 $k_3=1$。$S$ 的第三列只有一个非零元素，$S_{13} = 1$。\n\n- **节点 4** 是悬挂节点。$S$ 的第四列也被个性化向量 $\\mathbf{v}$ 替代。\n\n综合这些列，矩阵 $S$ 构建如下：\n$$S = \\begin{pmatrix} 0  \\frac{1}{2}  1  \\frac{1}{2} \\\\ \\frac{1}{2}  \\frac{1}{6}  0  \\frac{1}{6} \\\\ \\frac{1}{2}  \\frac{1}{3}  0  \\frac{1}{3} \\\\ 0  0  0  0 \\end{pmatrix}$$\n\n接下来，我们通过检查每列元素之和是否等于 $1$ 来验证 $S$ 是列随机的。\n- 第一列：$0 + \\frac{1}{2} + \\frac{1}{2} + 0 = 1$。\n- 第二列：$\\frac{1}{2} + \\frac{1}{6} + \\frac{1}{3} + 0 = \\frac{3}{6} + \\frac{1}{6} + \\frac{2}{6} = \\frac{6}{6} = 1$。\n- 第三列：$1 + 0 + 0 + 0 = 1$。\n- 第四列：$\\frac{1}{2} + \\frac{1}{6} + \\frac{1}{3} + 0 = 1$。\n矩阵 $S$ 被确认为列随机矩阵。\n\nPageRank 迭代公式为 $x^{(k+1)} = \\alpha S x^{(k)} + (1-\\alpha)\\mathbf{v}$。\n给定阻尼因子 $\\alpha = \\frac{17}{20}$，因此 $1-\\alpha = \\frac{3}{20}$。初始状态为 $x^{(0)} = \\mathbf{v} = \\left(\\frac{1}{2}, \\frac{1}{6}, \\frac{1}{3}, 0\\right)^{\\top}$。\n\n我们计算第一次迭代 $x^{(1)}$。首先，我们计算乘积 $S x^{(0)} = S\\mathbf{v}$：\n$$S\\mathbf{v} = \\begin{pmatrix} 0  \\frac{1}{2}  1  \\frac{1}{2} \\\\ \\frac{1}{2}  \\frac{1}{6}  0  \\frac{1}{6} \\\\ \\frac{1}{2}  \\frac{1}{3}  0  \\frac{1}{3} \\\\ 0  0  0  0 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{1}{6} \\\\ \\frac{1}{3} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0(\\frac{1}{2}) + \\frac{1}{2}(\\frac{1}{6}) + 1(\\frac{1}{3}) + \\frac{1}{2}(0) \\\\ \\frac{1}{2}(\\frac{1}{2}) + \\frac{1}{6}(\\frac{1}{6}) + 0(\\frac{1}{3}) + \\frac{1}{6}(0) \\\\ \\frac{1}{2}(\\frac{1}{2}) + \\frac{1}{3}(\\frac{1}{6}) + 0(\\frac{1}{3}) + \\frac{1}{3}(0) \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{12} + \\frac{1}{3} \\\\ \\frac{1}{4} + \\frac{1}{36} \\\\ \\frac{1}{4} + \\frac{1}{18} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{5}{12} \\\\ \\frac{10}{36} \\\\ \\frac{11}{36} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{5}{12} \\\\ \\frac{5}{18} \\\\ \\frac{11}{36} \\\\ 0 \\end{pmatrix}$$\n现在，我们求 $x^{(1)}$：\n$$x^{(1)} = \\alpha S x^{(0)} + (1-\\alpha)\\mathbf{v} = \\frac{17}{20} \\begin{pmatrix} \\frac{5}{12} \\\\ \\frac{5}{18} \\\\ \\frac{11}{36} \\\\ 0 \\end{pmatrix} + \\frac{3}{20} \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{1}{6} \\\\ \\frac{1}{3} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{85}{240} + \\frac{3}{40} \\\\ \\frac{85}{360} + \\frac{3}{120} \\\\ \\frac{187}{720} + \\frac{3}{60} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{85}{240} + \\frac{18}{240} \\\\ \\frac{85}{360} + \\frac{9}{360} \\\\ \\frac{187}{720} + \\frac{36}{720} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{103}{240} \\\\ \\frac{94}{360} \\\\ \\frac{223}{720} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{103}{240} \\\\ \\frac{47}{180} \\\\ \\frac{223}{720} \\\\ 0 \\end{pmatrix}$$\n\n最后，我们计算第二次迭代 $x^{(2)}$ 以求其第三个分量。\n$x^{(2)} = \\alpha S x^{(1)} + (1-\\alpha)\\mathbf{v}$。\n第三个分量是 $x^{(2)}_3 = \\alpha (S x^{(1)})_3 + (1-\\alpha)v_3$。\n首先，我们求乘积 $S x^{(1)}$ 的第三个分量：\n$$(S x^{(1)})_3 = S_{3,:} \\cdot x^{(1)} = \\begin{pmatrix} \\frac{1}{2}  \\frac{1}{3}  0  \\frac{1}{3} \\end{pmatrix} \\begin{pmatrix} \\frac{103}{240} \\\\ \\frac{47}{180} \\\\ \\frac{223}{720} \\\\ 0 \\end{pmatrix} = \\frac{1}{2}\\left(\\frac{103}{240}\\right) + \\frac{1}{3}\\left(\\frac{47}{180}\\right) = \\frac{103}{480} + \\frac{47}{540}$$\n为了将这些分数相加，我们找到一个公分母，$\\text{lcm}(480, 540) = 4320$。\n$$(S x^{(1)})_3 = \\frac{103 \\cdot 9}{4320} + \\frac{47 \\cdot 8}{4320} = \\frac{927 + 376}{4320} = \\frac{1303}{4320}$$\n现在我们可以计算 $x^{(2)}_3$：\n$$x^{(2)}_3 = \\frac{17}{20} \\left( \\frac{1303}{4320} \\right) + \\frac{3}{20} \\left( \\frac{1}{3} \\right) = \\frac{17 \\cdot 1303}{20 \\cdot 4320} + \\frac{1}{20} = \\frac{22151}{86400} + \\frac{1}{20}$$\n为了将它们相加，我们使用公分母 $86400$：\n$$x^{(2)}_3 = \\frac{22151}{86400} + \\frac{4320}{86400} = \\frac{22151 + 4320}{86400} = \\frac{26471}{86400}$$\n分母是 $86400 = 2^7 \\cdot 3^3 \\cdot 5^2$。分子 $26471$ 不能被 $2$、$3$（数字之和为 $20$）或 $5$ 整除。因此，该分数是最简形式。\n$x^{(2)}$ 的第三个分量的精确值是 $\\frac{26471}{86400}$。",
            "answer": "$$\\boxed{\\frac{26471}{86400}}$$"
        },
        {
            "introduction": "在理解了 PageRank 的迭代计算之后，一个更深层次的问题是：算法的关键参数如何影响最终的排名结果？本练习将引导你从数值计算转向解析分析。你将通过对一个星形网络进行微积分推导，得出一个节点 PageRank 值对阻尼系数 $\\alpha$ 敏感度的闭式解，从而揭示模型参数与节点重要性之间的深刻关系。",
            "id": "4296064",
            "problem": "考虑一个有向星形网络，该网络包含一个中心节点和一组叶节点。设节点总数为 $N \\geq 3$，其中一个中心节点标记为 $0$，以及 $N-1$ 个叶节点标记为 $1, 2, \\dots, N-1$。有向边的定义如下：中心节点有指向所有叶节点的出边，每个叶节点有唯一一条指向中心节点的出边。不存在其他边。一个随机冲浪者在该网络上根据以下规则移动：以概率 $\\alpha \\in (0,1)$，冲浪者从当前节点的出链中均匀随机选择一条并跟随；以概率 $1-\\alpha$，冲浪者瞬移到一个从均匀偏好向量中抽取的节点。该过程产生一个依赖于 $\\alpha$ 的节点上的平稳分布，记为 $r(\\alpha)$，称为 PageRank。\n\n从带有瞬移机制和均匀偏好向量的离散时间马尔可夫链的基本定义出发，利用第一性原理推导出决定该网络平稳分布 $r(\\alpha)$ 的线性方程组。利用对称性将系统简化为两个变量：中心节点的 PageRank，记为 $r_{h}(\\alpha)$，以及任意叶节点的 PageRank，记为 $r_{\\ell}(\\alpha)$。\n\n然后，仅使用这些第一性原理和线性系统的基本性质，对平稳条件关于 $\\alpha$ 进行隐式微分，以获得一个关于 $\\frac{d r(\\alpha)}{d \\alpha}$ 的通用灵敏度公式，该公式用网络的列随机转移矩阵和偏好向量表示。将此灵敏度分析应用于该有向星形网络，以获得中心节点的 PageRank 相对于 $\\alpha$ 的导数，即 $\\frac{d r_{h}(\\alpha)}{d \\alpha}$ 的一个闭式解析表达式，该表达式对所有 $N \\geq 3$ 和所有 $\\alpha \\in (0,1)$ 均有效。\n\n最后，利用此结果定性地讨论 $r_{h}(\\alpha)$ 在区间 $\\alpha \\in \\left[\\frac{1}{2}, \\frac{99}{100}\\right]$ 上如何变化（无需数值近似），并给出 $\\frac{d r_{h}(\\alpha)}{d \\alpha}$ 的精确闭式表达式作为最终答案。",
            "solution": "该问题是良定的，并且在马尔可夫链和网络科学理论中有坚实的科学基础。所有必要条件均已提供。我们开始求解。\n\nPageRank 向量，记为 $r(\\alpha)$，是由谷歌矩阵 $G(\\alpha)$ 定义的离散时间马尔可夫链的唯一平稳分布。平稳分布 $r$ 的定义方程是一个特征向量方程：\n$$r = G r$$\n其中 $r$ 是所有节点 PageRank 分数的列向量。谷歌矩阵 $G$ 是由基于网络链接结构的转移矩阵 $P$ 和一个瞬移矩阵构成的凸组合。对于一个均匀偏好向量 $v = \\frac{1}{N} \\mathbf{1}$，其中 $\\mathbf{1}$ 是全1列向量，$N$ 是节点总数，谷歌矩阵为：\n$$G(\\alpha) = \\alpha P + (1-\\alpha) \\frac{1}{N} J$$\n其中 $J = \\mathbf{1} \\mathbf{1}^T$ 是全1矩阵，$\\alpha \\in (0,1)$ 是阻尼因子。\n平稳条件 $r = G r$ 可以按分量写出。向量 $r$ 的第 $i$ 个分量是：\n$$r_i = \\sum_{j=1}^{N} G_{ij} r_j = \\sum_{j=1}^{N} \\left( \\alpha P_{ij} + \\frac{1-\\alpha}{N} \\right) r_j = \\alpha \\sum_{j=1}^{N} P_{ij} r_j + \\frac{1-\\alpha}{N} \\sum_{j=1}^{N} r_j$$\n由于 $r$ 是一个概率分布，其分量之和必须为1：$\\sum_{j=1}^{N} r_j = 1$。该方程简化为：\n$$r_i = \\alpha \\sum_{j=1}^{N} P_{ij} r_j + \\frac{1-\\alpha}{N}$$\n矩阵 $P$ 是图上随机游走的列随机转移矩阵。元素 $P_{ij}$ 是从节点 $j$ 转移到节点 $i$ 的概率。如果节点 $j$ 的出度为 $k_j^{\\text{out}}$，它以概率 $P_{ij} = 1/k_j^{\\text{out}}$ 连接到节点 $i$。因此，求和是对所有有指向节点 $i$ 的有向边的节点 $j$ 进行的：\n$$r_i = \\alpha \\sum_{j \\to i} \\frac{r_j}{k_j^{\\text{out}}} + \\frac{1-\\alpha}{N}$$\n这就是从第一性原理推导出的基本方程。\n\n现在我们将此应用于指定的包含 $N \\geq 3$ 个节点的有向星形网络。中心节点是节点 $0$，叶节点是节点 $1, 2, \\dots, N-1$。\n出度为：\n- 中心节点（节点 $0$）：$k_0^{\\text{out}} = N-1$（连接到所有叶节点）。\n- 叶节点（节点 $j=1, \\dots, N-1$）：$k_j^{\\text{out}} = 1$（每个都连接到中心节点）。\n\n根据网络的对称性，所有叶节点都是等价的，因此它们的 PageRank 分数必须相同。我们将中心节点的 PageRank 记为 $r_h(\\alpha) = r_0(\\alpha)$，将任意叶节点的 PageRank 记为 $r_{\\ell}(\\alpha) = r_i(\\alpha)$（对于 $i=1, \\dots, N-1$）。\n\n我们推导这两个变量的线性方程组：\n对于中心节点的 PageRank $r_h$：连接到中心节点的节点是所有 $N-1$ 个叶节点。\n$$r_h = \\alpha \\sum_{j=1}^{N-1} \\frac{r_j}{k_j^{\\text{out}}} + \\frac{1-\\alpha}{N} = \\alpha \\sum_{j=1}^{N-1} \\frac{r_{\\ell}}{1} + \\frac{1-\\alpha}{N}$$\n$$r_h = \\alpha(N-1)r_{\\ell} + \\frac{1-\\alpha}{N} \\quad (1)$$\n对于叶节点的 PageRank $r_{\\ell}$：唯一连接到任意叶节点 $i$ 的节点是中心节点。\n$$r_{\\ell} = \\alpha \\frac{r_0}{k_0^{\\text{out}}} + \\frac{1-\\alpha}{N} = \\alpha \\frac{r_h}{N-1} + \\frac{1-\\alpha}{N} \\quad (2)$$\n这两个方程构成了关于 $r_h$ 和 $r_{\\ell}$ 的简化线性系统。作为检验，我们还有归一化条件 $r_h + (N-1)r_{\\ell} = 1$。该系统是相容的，并且足以确定 $r_h$ 和 $r_{\\ell}$。\n\n接下来，我们推导 $\\frac{d r(\\alpha)}{d \\alpha}$ 的通用灵敏度公式。平稳条件 $r=Gr$ 可以写成：\n$$r(\\alpha) = \\left(\\alpha P + (1-\\alpha)v\\mathbf{1}^T\\right)r(\\alpha)$$\n其中 $v$ 是偏好向量。由于 $\\mathbf{1}^T r(\\alpha) = 1$，项 $(v\\mathbf{1}^T)r(\\alpha)$ 简化为 $v$。\n$$r(\\alpha) = \\alpha P r(\\alpha) + (1-\\alpha)v$$\n我们对 $\\alpha$ 进行隐式微分。令 $r'(\\alpha) = \\frac{dr(\\alpha)}{d\\alpha}$。\n$$r'(\\alpha) = \\frac{d}{d\\alpha} \\left( \\alpha P r(\\alpha) + (1-\\alpha)v \\right) = [P r(\\alpha) + \\alpha P r'(\\alpha)] - v$$\n整理以求解 $r'(\\alpha)$：\n$$r'(\\alpha) - \\alpha P r'(\\alpha) = P r(\\alpha) - v$$\n$$(I - \\alpha P) r'(\\alpha) = P r(\\alpha) - v$$\n矩阵 $(I - \\alpha P)$ 是可逆的。列随机矩阵 $P$ 的特征值 $\\lambda$ 的模满足 $|\\lambda| \\leq 1$。由于 $\\alpha \\in (0,1)$，所以 $1/\\alpha  1$。因此，$1/\\alpha$ 不可能是 $P$ 的特征值，这意味着 $\\det(P - \\frac{1}{\\alpha}I) \\neq 0$，从而 $\\det(I - \\alpha P) \\neq 0$。\n因此，我们可以写出通用灵敏度公式：\n$$r'(\\alpha) = (I - \\alpha P)^{-1} (P r(\\alpha) - v)$$\n\n现在，我们将此公式应用于星形网络以求得 $\\frac{dr_h(\\alpha)}{d\\alpha}$。我们需要求解线性系统 $(I - \\alpha P) r' = y$，其中 $y = P r - v$。\n节点排序为（中心节点, 叶节点1, ..., 叶节点N-1）。向量 $r'(\\alpha)$ 和 $y$ 是：\n$$r'(\\alpha) = \\begin{pmatrix} r'_h \\\\ r'_{\\ell} \\\\ \\vdots \\\\ r'_{\\ell} \\end{pmatrix}, \\quad y = P r(\\alpha) - v$$\n偏好向量是均匀的：$v = \\frac{1}{N}\\mathbf{1}$。转移矩阵 $P$ 是：\n$$P = \\begin{pmatrix} 0  1  \\dots  1 \\\\ \\frac{1}{N-1}  0  \\dots  0 \\\\ \\vdots  \\vdots  \\ddots  \\vdots \\\\ \\frac{1}{N-1}  0  \\dots  0 \\end{pmatrix}$$\n我们来计算向量 $y = P r(\\alpha) - v$：\n$$P r(\\alpha) = \\begin{pmatrix} \\sum_{j=1}^{N-1} r_{\\ell} \\\\ \\frac{1}{N-1} r_h \\\\ \\vdots \\\\ \\frac{1}{N-1} r_h \\end{pmatrix} = \\begin{pmatrix} (N-1)r_{\\ell} \\\\ \\frac{r_h}{N-1} \\\\ \\vdots \\\\ \\frac{r_h}{N-1} \\end{pmatrix}$$\n$$y = \\begin{pmatrix} (N-1)r_{\\ell} - \\frac{1}{N} \\\\ \\frac{r_h}{N-1} - \\frac{1}{N} \\\\ \\vdots \\\\ \\frac{r_h}{N-1} - \\frac{1}{N} \\end{pmatrix}$$\n方程组 $(I - \\alpha P) r' = y$ 变为：\n1. （中心节点行）：$r'_h - \\alpha \\sum_{j=1}^{N-1} r'_{\\ell} = y_0 \\implies r'_h - \\alpha(N-1)r'_{\\ell} = (N-1)r_{\\ell} - \\frac{1}{N}$\n2. （叶节点行）：$-\\frac{\\alpha}{N-1}r'_h + r'_{\\ell} = y_1 \\implies r'_{\\ell} = \\frac{\\alpha}{N-1}r'_h + \\frac{r_h}{N-1} - \\frac{1}{N}$\n\n我们将第二个方程中 $r'_{\\ell}$ 的表达式代入第一个方程：\n$$r'_h - \\alpha(N-1)\\left(\\frac{\\alpha}{N-1}r'_h + \\frac{r_h}{N-1} - \\frac{1}{N}\\right) = (N-1)r_{\\ell} - \\frac{1}{N}$$\n$$r'_h - \\alpha^2 r'_h - \\alpha r_h + \\frac{\\alpha(N-1)}{N} = (N-1)r_{\\ell} - \\frac{1}{N}$$\n$$r'_h(1-\\alpha^2) = \\alpha r_h + (N-1)r_{\\ell} - \\frac{\\alpha(N-1)}{N} - \\frac{1}{N}$$\n从方程(1)，我们有 $(N-1)r_{\\ell} = \\frac{r_h}{\\alpha} - \\frac{1-\\alpha}{\\alpha N}$。这似乎比使用 $r_h$ 和 $r_l$ 的已解出的形式更复杂。让我们使用通过求解方程(1)和(2)得到的 $r_h$ 的表达式。\n求解 $r_h$ 和 $r_{\\ell}$ 得到：\n$r_h(\\alpha) = \\frac{1+(N-1)\\alpha}{N(1+\\alpha)}$ 和 $r_{\\ell}(\\alpha) = \\frac{N-1+\\alpha}{N(N-1)(1+\\alpha)}$。\n让我们将这些代入 $r'_h(1-\\alpha^2)$ 方程的右侧（RHS）：\nRHS $= \\alpha \\frac{1+(N-1)\\alpha}{N(1+\\alpha)} + (N-1)\\frac{N-1+\\alpha}{N(N-1)(1+\\alpha)} - \\frac{1+\\alpha(N-1)}{N}$\nRHS $= \\frac{\\alpha(1+(N-1)\\alpha) + (N-1+\\alpha)}{N(1+\\alpha)} - \\frac{1+\\alpha(N-1)}{N}$\nRHS $= \\frac{\\alpha+\\alpha^2(N-1) + N-1+\\alpha}{N(1+\\alpha)} - \\frac{(1+\\alpha(N-1))(1+\\alpha)}{N(1+\\alpha)}$\nRHS $= \\frac{N-1+2\\alpha+\\alpha^2(N-1) - (1+\\alpha+\\alpha(N-1)+\\alpha^2(N-1))}{N(1+\\alpha)}$\nRHS $= \\frac{N-1+2\\alpha - (1+\\alpha+ (N-1)\\alpha)}{N(1+\\alpha)} = \\frac{N-1+2\\alpha - (1+N\\alpha)}{N(1+\\alpha)}$\nRHS $= \\frac{N-2 + (2-N)\\alpha}{N(1+\\alpha)} = \\frac{(N-2)(1-\\alpha)}{N(1+\\alpha)}$\n现在我们有：\n$$r'_h(1-\\alpha^2) = \\frac{(N-2)(1-\\alpha)}{N(1+\\alpha)}$$\n$$r'_h(1-\\alpha)(1+\\alpha) = \\frac{(N-2)(1-\\alpha)}{N(1+\\alpha)}$$\n由于 $\\alpha \\in (0,1)$，所以 $1-\\alpha \\neq 0$，我们可以除以它：\n$$r'_h(1+\\alpha) = \\frac{N-2}{N(1+\\alpha)}$$\n$$\\frac{dr_{h}(\\alpha)}{d\\alpha} = \\frac{N-2}{N(1+\\alpha)^2}$$\n这就是导数的闭式表达式。\n\n最后，我们定性地讨论 $r_h(\\alpha)$ 在 $\\alpha \\in \\left[\\frac{1}{2}, \\frac{99}{100}\\right]$ 上如何变化。\n导数为 $\\frac{dr_h}{d\\alpha} = \\frac{N-2}{N(1+\\alpha)^2}$。\n给定 $N \\geq 3$，分子 $N-2$ 是非负的（$N-2 \\geq 1$）。对于 $\\alpha \\in (0,1)$，分母 $N(1+\\alpha)^2$ 是严格为正的。\n因此，对于所有 $\\alpha \\in (0,1)$ 和 $N \\geq 3$，$\\frac{dr_h}{d\\alpha}  0$。这表明 $r_h(\\alpha)$ 是 $\\alpha$ 的一个严格递增函数。随着冲浪者更倾向于跟随链接（即 $\\alpha$ 增大），更多的排名值“流向”中心节点，增加了其重要性。\n二阶导数是：\n$$\\frac{d^2 r_h}{d\\alpha^2} = \\frac{d}{d\\alpha}\\left(\\frac{N-2}{N}(1+\\alpha)^{-2}\\right) = \\frac{N-2}{N}(-2)(1+\\alpha)^{-3} = -\\frac{2(N-2)}{N(1+\\alpha)^3}$$\n对于 $N \\geq 3$ 和 $\\alpha0$，二阶导数是严格为负的。这意味着 $r_h(\\alpha)$ 是 $\\alpha$ 的一个严格凹函数。\n因此，在区间 $\\alpha \\in \\left[\\frac{1}{2}, \\frac{99}{100}\\right]$ 上，$r_h(\\alpha)$ 是连续递增的，但增速递减。当 $\\alpha$ 增大时，该函数值上升但趋于平缓。",
            "answer": "$$\\boxed{\\frac{N-2}{N(1+\\alpha)^2}}$$"
        },
        {
            "introduction": "理论的价值最终体现在实践中。这个练习旨在将 PageRank 的理论与计算实践相结合，你将使用幂法（Power Method）——计算 PageRank 的标准算法——来实现它。除了编写代码，你还将经验性地测量算法在不同网络结构上的收敛速度，这是一个衡量算法效率并与网络谱属性紧密相关的关键指标。",
            "id": "4296002",
            "problem": "考虑一个包含 $5$ 个节点的有向网络，以及建模为离散时间马尔可夫链的 PageRank 过程。使用以下基本原理：马尔可夫链由一个列随机转移矩阵定义，其中条目 $P_{ij}$ 表示从节点 $j$ 到节点 $i$ 的转移概率，概率分布向量 $x^{(k)}$ 通过乘法 $x^{(k)} = P \\, x^{(k-1)}$ 演化。在随机冲浪者模型中，每一步，冲浪者以概率 $\\alpha$ 从当前节点随机均匀地选择一条出边进行跳转，并以概率 $1 - \\alpha$ 随机均匀地传送到任意一个节点。如果一个节点没有出边（即悬挂节点），那么在尝试跟随链接时，冲浪者会在所有节点中进行均匀选择。将过程初始化为均匀分布。任务是对此过程计算幂法（PM）的几次迭代，并根据连续差分的 $L^1$ 范数的衰减来经验性地估计收敛速率。\n\n通过以下从随机冲浪者模型派生的过程来构建转移矩阵：\n- 设 $A$ 为邻接矩阵，如果存在从节点 $j$ 到节点 $i$ 的有向边，则其条目 $A_{ij} = 1$，否则 $A_{ij} = 0$。\n- 通过对 $A$ 的每一列进行归一化来形成一个列随机矩阵 $S$；如果某一列 $j$ 的和为零（节点 $j$ 是悬挂节点），则将该列替换为在所有 $5$ 个节点上的均匀分布。\n- 通过将 $S$ 与均匀传送相结合来形成随机冲浪者转移矩阵 $G$：$G$ 的每一列分别是 $S$ 相应列与所有 $5$ 个节点上均匀分布的凸组合，权重分别为 $\\alpha$ 和 $1 - \\alpha$。\n\n对于下面的每个图，设置 $\\alpha = 0.85$，将 $x^{(0)}$ 初始化为 $5$ 个节点上的均匀分布，并对 $k = 1,2,\\dots,K$（其中 $K = 40$）进行迭代 $x^{(k)} = G \\, x^{(k-1)}$。令 $d^{(k)} = \\lVert x^{(k)} - x^{(k-1)} \\rVert_{1}$（对于 $k \\geq 1$）。对于 $k \\geq 2$，定义步进衰减比 $r^{(k)} = d^{(k)} / d^{(k-1)}$。将收敛速率经验性地估计为 $r^{(k)}$ 在尾部窗口 $k \\in \\{20,21,\\dots,40\\}$ 上的中位数。\n\n三个测试图（每个都有 $5$ 个节点）如下：\n- 测试用例 1（无悬挂节点，中等密度）：\n  - 有向边：$1 \\to 2$, $1 \\to 3$; $2 \\to 3$, $2 \\to 4$; $3 \\to 1$, $3 \\to 5$; $4 \\to 3$, $4 \\to 5$; $5 \\to 1$, $5 \\to 2$。\n- 测试用例 2（一个悬挂节点）：\n  - 有向边：$1 \\to 2$; $2 \\to 3$, $2 \\to 5$; $3 \\to 1$; $4 \\to 3$, $4 \\to 5$; $5$ 没有出边。\n- 测试用例 3（两个社群，连接稀疏）：\n  - 有向边：$1 \\to 2$, $1 \\to 3$; $2 \\to 1$; $3 \\to 2$, $3 \\to 4$; $4 \\to 5$; $5 \\to 4$。\n\n您的程序必须：\n- 根据所述规则为每个测试用例构建 $S$ 和 $G$。\n- 在 $x^{(0)}$ 为 $5$ 个节点上的均匀分布的情况下，计算 $k = 1,\\dots,40$ 时的 $x^{(k)}$。\n- 按规定计算 $d^{(k)}$ 和 $r^{(k)}$。\n- 对每个测试用例，输出经验收敛速率估计值，该值定义为 $r^{(k)}$ 在 $k \\in \\{20,\\dots,40\\}$ 上的中位数。\n\n最终输出格式：\n- 生成一行内容，其中包含测试用例 1、2 和 3 的三个估计收敛速率，按顺序排列，形式为用方括号括起来的逗号分隔列表，每个浮点数四舍五入到六位小数，例如 `[0.731234,0.812345,0.843210]`。",
            "solution": "该问题提出了一个来自网络科学领域的有效且定义明确的任务，具体涉及 PageRank 算法。它要求对用于计算三个给定网络结构的 PageRank 分数的幂法的收敛速率进行经验性估计。该问题在科学上基于马尔可夫链和线性代数的理论，包含了所有必要的数据和定义，并且没有歧义或矛盾。我们将继续提供一个完整的解决方案。\n\n问题的核心是模拟 PageRank 过程，该过程被建模为在一个包含 $N=5$ 个节点的网络上的离散时间马尔可夫链。在第 $k$ 步，系统的状态是一个概率分布向量 $x^{(k)} \\in \\mathbb{R}^N$，其中第 $i$ 个分量 $x_i^{(k)}$ 是“随机冲浪者”位于节点 $i$ 的概率。该状态的演化由方程 $x^{(k)} = G x^{(k-1)}$ 控制，其中 $G$ 是 Google 矩阵，一个列随机转移矩阵。稳态分布 $x^* = \\lim_{k\\to\\infty} x^{(k)}$，即 PageRank 向量。\n\n该过程涉及几个步骤：\n1.  为每个图构建邻接矩阵 $A$。\n2.  从 $A$ 构建链接跟随随机矩阵 $S$，并处理悬挂节点。\n3.  使用阻尼因子 $\\alpha$ 形成 Google 矩阵 $G$。\n4.  迭代应用幂法以找到 PageRank 向量。\n5.  从迭代序列中经验性地估计收敛速率。\n\n让我们详细说明这些步骤。为便于计算，我们将节点索引从 $0$ 到 $4$，而不是问题中的 $1$ 到 $5$。\n\n**1. 邻接矩阵 $A$**\n邻接矩阵 $A$ 编码了网络的链接结构。根据问题的定义，如果存在从节点 $j$ 到节点 $i$ 的有向边，则条目 $A_{ij}$ 为 $1$，否则为 $0$。因此，$A$ 的每一列 $j$ 代表从节点 $j$ 出发的出边。\n\n**2. 随机矩阵 $S$**\n矩阵 $S$ 模拟了只跟随链接的冲浪者的转移。它由 $A$ 派生而来。对于每个节点 $j$，我们计算其出度 $d_j^{out} = \\sum_{i=1}^{N} A_{ij}$。这是 $A$ 的第 $j$ 列中元素的总和。\n- 如果 $d_j^{out}  0$，在节点 $j$ 的冲浪者会从 $d_j^{out}$ 条出边中随机均匀地选择一条。$S$ 的第 $j$ 列，记为 $S_j$，通过对 $A$ 的相应列进行归一化得到：$S_{ij} = A_{ij} / d_j^{out}$。\n- 如果 $d_j^{out} = 0$，则节点 $j$ 是一个“悬挂节点”。问题规定，在这种情况下，冲浪者会以均匀概率跳转到网络中的任何节点。因此，$S$ 的第 $j$ 列被设置为一个均匀概率向量，即对于所有 $i$，$S_{ij} = 1/N$。\n\n**3. Google 矩阵 $G$**\nGoogle 矩阵 $G$ 包含了“随机传送”机制，这确保了底层的马尔可夫链是遍历的，因此具有唯一的稳态分布。它被构建为一个凸组合：\n$$G = \\alpha S + (1 - \\alpha) \\frac{1}{N} J$$\n其中 $\\alpha = 0.85$ 是阻尼因子，$S$ 是上一步得到的矩阵，$N=5$ 是节点数，$J$ 是所有元素都为 1 的 $N \\times N$ 矩阵。项 $\\frac{1}{N}J$ 表示向任意节点的均匀传送。$G$ 的每一列都是一个有效的概率分布，使得 $G$ 是列随机的。\n\n**4. 幂法迭代**\nPageRank 向量是 $G$ 的主特征向量（对应于特征值 $\\lambda_1 = 1$）。幂法是找到该特征向量的一种迭代算法。我们从一个初始概率分布 $x^{(0)}$ 开始，并重复应用转移矩阵：\n$$x^{(k)} = G x^{(k-1)}$$\n问题指定初始状态为均匀分布：对于所有 $i \\in \\{0, 1, 2, 3, 4\\}$，$x_i^{(0)} = 1/N = 1/5$。我们需要执行 $K=40$ 次迭代。\n\n**5. 经验收敛速率估计**\n幂法的理论收敛速率由 $G$ 的第二大特征值的模（记为 $|\\lambda_2|$）决定。当 $k$ 变大时，误差项由与 $\\lambda_2$ 相关的分量主导，连续迭代之间的距离呈几何级数衰减：\n$$\\lVert x^{(k)} - x^{(k-1)} \\rVert \\propto |\\lambda_2|^k$$\n问题要求对这个速率进行经验估计。我们计算连续迭代之间差值的 $L^1$-范数：\n$$d^{(k)} = \\lVert x^{(k)} - x^{(k-1)} \\rVert_{1} = \\sum_{i=0}^{N-1} |x_i^{(k)} - x_i^{(k-1)}| \\quad \\text{对于 } k \\geq 1$$\n从这个差值序列中，我们计算连续项的比率：\n$$r^{(k)} = \\frac{d^{(k)}}{d^{(k-1)}} \\quad \\text{对于 } k \\geq 2$$\n对于大的 $k$，这个比率 $r^{(k)}$ 应该趋近于 $|\\lambda_2|$。为了获得一个稳定的估计，我们计算这些比率在迭代的尾部窗口（具体为 $k \\in \\{20, 21, \\dots, 40\\}$）上的中位数。这个中位数作为我们对收敛速率的经验估计。\n\n这个过程将应用于提供的三个测试用例中的每一个。\n- **测试用例 1：** 一个没有悬挂节点的连接良好的图。\n- **测试用例 2：** 一个有一个悬挂节点（节点 $5$，或索引 $4$）的图，在构建 $S$ 时需要特殊处理。\n- **测试用例 3：** 一个具有社群结构的图，已知这可能会减慢收敛速度，导致经验速率更接近于 $\\alpha$。\n\n实现将精确地遵循这些步骤来计算每种情况下的估计收敛速率。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the empirical convergence rate for the PageRank algorithm on three test graphs.\n    \"\"\"\n\n    # Node count and algorithm parameters\n    N = 5\n    ALPHA = 0.85\n    K = 40\n    \n    # Define the test cases from the problem statement.\n    # Edges are (source, destination) using 1-based indexing as in the problem.\n    test_cases_edges = [\n        # Test case 1 (no dangling nodes, moderately dense)\n        [(1, 2), (1, 3), (2, 3), (2, 4), (3, 1), (3, 5), (4, 3), (4, 5), (5, 1), (5, 2)],\n        # Test case 2 (one dangling node)\n        [(1, 2), (2, 3), (2, 5), (3, 1), (4, 3), (4, 5)], # Node 5 is dangling\n        # Test case 3 (two communities with sparse interconnection)\n        [(1, 2), (1, 3), (2, 1), (3, 2), (3, 4), (4, 5), (5, 4)],\n    ]\n\n    results = []\n    \n    for edges in test_cases_edges:\n        # Step 1: Construct Adjacency Matrix A (using 0-based indexing)\n        # A[i, j] = 1 if there is an edge from j to i\n        A = np.zeros((N, N), dtype=float)\n        for src, dest in edges:\n            A[dest - 1, src - 1] = 1.0\n\n        # Step 2: Construct Stochastic Matrix S\n        S = A.copy()\n        out_degrees = S.sum(axis=0)\n        \n        for j in range(N):\n            if out_degrees[j] > 0:\n                S[:, j] /= out_degrees[j]\n            else: # Dangling node\n                S[:, j] = 1.0 / N\n\n        # Step 3: Construct Google Matrix G\n        J = np.ones((N, N), dtype=float)\n        G = ALPHA * S + (1 - ALPHA) * (1.0 / N) * J\n\n        # Step 4: Power Method Iteration\n        # Initialize x^(0) as the uniform distribution\n        x_prev = np.ones(N) / N\n        \n        # Store L1 norms of differences\n        d_k = []\n        for _ in range(K):\n            x_curr = G @ x_prev\n            # Calculate d^(k) = ||x^(k) - x^(k-1)||_1\n            diff_norm = np.linalg.norm(x_curr - x_prev, ord=1)\n            d_k.append(diff_norm)\n            x_prev = x_curr\n\n        # Step 5: Empirical Convergence Rate Estimation\n        # Calculate ratios r^(k) = d^(k) / d^(k-1)\n        r_k = []\n        # Loop for k from 2 to K (which is index 1 to K-1 of d_k)\n        for i in range(1, len(d_k)):\n            # Avoid division by zero, although not expected for this problem\n            if d_k[i-1] > 1e-15:\n                ratio = d_k[i] / d_k[i-1]\n                r_k.append(ratio)\n            else:\n                # If a difference becomes zero, convergence is perfect (rate 0)\n                r_k.append(0.0)\n\n        # The problem asks for the median of r^(k) for k in {20, ..., 40}.\n        # r_k[i] corresponds to r^(i+2). So k=20 corresponds to index 18.\n        # The tail window is from index 18 (for k=20) to 38 (for k=40).\n        tail_window_start_index = 18\n        tail_ratios = r_k[tail_window_start_index:]\n        \n        convergence_rate = np.median(tail_ratios)\n        results.append(convergence_rate)\n\n    # Final print statement in the exact required format.\n    formatted_results = [f'{r:.6f}' for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}