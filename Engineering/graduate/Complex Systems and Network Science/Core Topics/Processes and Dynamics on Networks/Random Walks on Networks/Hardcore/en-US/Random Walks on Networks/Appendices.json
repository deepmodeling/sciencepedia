{
    "hands_on_practices": [
        {
            "introduction": "This first exercise is designed to build a solid foundation by walking through the fundamental mechanics of a random walk. By working with a simple path graph, you will manually construct the core mathematical objects—the adjacency, degree, and transition matrices—that govern the walker's movement. This practice  directly illustrates the crucial link between a graph's local structure (node degrees) and the emergent, long-term behavior captured by the stationary distribution.",
            "id": "4299832",
            "problem": "Consider the undirected simple path graph with nodes $1$, $2$, $3$ connected as $1-2-3$. A simple random walk on this graph is a time-homogeneous discrete-time Markov chain (MC) whose states are the nodes, and at each time step the walker at node $i$ moves uniformly at random to one of its neighbors.\n\nUsing only fundamental definitions, do the following:\n\n- Define the adjacency matrix $A$ and the degree matrix $D$ for this graph.\n- Starting from the definition that the transition probability from node $i$ to node $j$ equals $1/k_{i}$ if $j$ is a neighbor of $i$ and equals $0$ otherwise (where $k_{i}$ is the degree of node $i$), derive a compact matrix expression for the transition matrix $P$ in terms of $A$ and $D$, and compute $P$ explicitly for this graph.\n- Verify that $P \\mathbf{1} = \\mathbf{1}$ where $\\mathbf{1}$ is the all-ones vector, thereby showing that each row of $P$ sums to $1$.\n- Using the concept of reversibility and detailed balance for random walks on undirected graphs, derive a stationary distribution $\\pi$ and interpret the difference between the endpoints and the middle node in terms of the transition probabilities and degree heterogeneity.\n- Finally, compute the exact two-step return probability to node $2$, that is, the $(2,2)$ entry of $P^{2}$, and report that single number as your final answer.\n\nAll numerical values must be provided in exact form. The final answer should be the exact value of the two-step return probability to node $2$.",
            "solution": "The problem is well-posed and grounded in the fundamental principles of Markov chains and random walks on graphs. We proceed by following the specified steps.\n\nThe graph is an undirected simple path with nodes labeled $1$, $2$, and $3$, connected as $1-2-3$. The set of nodes is $V = \\{1, 2, 3\\}$ and the set of edges is $E = \\{\\{1, 2\\}, \\{2, 3\\}\\}$.\n\n**Adjacency Matrix $A$ and Degree Matrix $D$**\nThe adjacency matrix $A$ for an undirected graph is a symmetric matrix where the entry $A_{ij}$ is $1$ if there is an edge connecting node $i$ and node $j$, and $0$ otherwise. For a simple graph, the diagonal entries $A_{ii}$ are all $0$.\nFor the given graph, the connections are between node $1$ and $2$, and between node $2$ and $3$. Thus, the adjacency matrix is:\n$$\nA = \\begin{pmatrix} 0  1  0 \\\\ 1  0  1 \\\\ 0  1  0 \\end{pmatrix}\n$$\nThe degree of a node $i$, denoted $k_i$, is the number of edges connected to it. It can be calculated from the adjacency matrix as $k_i = \\sum_{j} A_{ij}$.\nThe degrees of the nodes are:\n$k_1 = A_{11} + A_{12} + A_{13} = 0 + 1 + 0 = 1$\n$k_2 = A_{21} + A_{22} + A_{23} = 1 + 0 + 1 = 2$\n$k_3 = A_{31} + A_{32} + A_{33} = 0 + 1 + 0 = 1$\nThe degree matrix $D$ is a diagonal matrix with the degrees of the nodes on its diagonal, i.e., $D_{ii} = k_i$.\n$$\nD = \\begin{pmatrix} 1  0  0 \\\\ 0  2  0 \\\\ 0  0  1 \\end{pmatrix}\n$$\n\n**Transition Matrix $P$**\nThe transition probability $P_{ij}$ from node $i$ to node $j$ is given as $1/k_i$ if $j$ is a neighbor of $i$, and $0$ otherwise. This can be written in terms of the adjacency matrix as $P_{ij} = A_{ij}/k_i$.\nTo express this in matrix form, we observe that the $i$-th row of the transition matrix $P$ is obtained by dividing the $i$-th row of the adjacency matrix $A$ by the degree $k_i$. This operation corresponds to left-multiplication of $A$ by the inverse of the degree matrix, $D^{-1}$. The inverse $D^{-1}$ is a diagonal matrix with entries $(D^{-1})_{ii} = 1/k_i$.\nThe matrix expression is therefore $P = D^{-1}A$.\nLet's compute $D^{-1}$:\n$$\nD^{-1} = \\begin{pmatrix} 1/1  0  0 \\\\ 0  1/2  0 \\\\ 0  0  1/1 \\end{pmatrix} = \\begin{pmatrix} 1  0  0 \\\\ 0  1/2  0 \\\\ 0  0  1 \\end{pmatrix}\n$$\nNow we compute $P$:\n$$\nP = D^{-1}A = \\begin{pmatrix} 1  0  0 \\\\ 0  1/2  0 \\\\ 0  0  1 \\end{pmatrix} \\begin{pmatrix} 0  1  0 \\\\ 1  0  1 \\\\ 0  1  0 \\end{pmatrix} = \\begin{pmatrix} 0  1  0 \\\\ 1/2  0  1/2 \\\\ 0  1  0 \\end{pmatrix}\n$$\n\n**Verification that Row Sums are $1$**\nTo show that each row of $P$ sums to $1$, we verify that $P \\mathbf{1} = \\mathbf{1}$, where $\\mathbf{1}$ is the column vector of all ones.\n$$\nP \\mathbf{1} = \\begin{pmatrix} 0  1  0 \\\\ 1/2  0  1/2 \\\\ 0  1  0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0\\cdot1 + 1\\cdot1 + 0\\cdot1 \\\\ (1/2)\\cdot1 + 0\\cdot1 + (1/2)\\cdot1 \\\\ 0\\cdot1 + 1\\cdot1 + 0\\cdot1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\mathbf{1}\n$$\nThis confirms that $P$ is a valid stochastic matrix.\n\n**Stationary Distribution $\\pi$**\nFor a random walk on an undirected graph, the process is reversible. The stationary distribution $\\pi$ must satisfy the detailed balance condition: $\\pi_i P_{ij} = \\pi_j P_{ji}$ for all pairs of states $i, j$.\nSubstituting $P_{ij} = A_{ij}/k_i$ and $P_{ji} = A_{ji}/k_j$:\n$$\n\\pi_i \\frac{A_{ij}}{k_i} = \\pi_j \\frac{A_{ji}}{k_j}\n$$\nSince the graph is undirected, $A$ is symmetric, so $A_{ij} = A_{ji}$. If there is an edge between $i$ and $j$ ($A_{ij}=1$), the condition simplifies to $\\pi_i/k_i = \\pi_j/k_j$. Since the graph is connected, this implies that the ratio $\\pi_i/k_i$ is a constant, say $C$, for all nodes $i$. Thus, $\\pi_i = C k_i$. The stationary probability is proportional to the degree of the node.\nTo find the constant $C$, we use the normalization condition $\\sum_i \\pi_i = 1$:\n$$\n\\sum_{i=1}^{3} C k_i = C (k_1 + k_2 + k_3) = 1\n$$\nUsing the degrees we calculated, $k_1=1$, $k_2=2$, $k_3=1$:\n$$\nC(1 + 2 + 1) = C \\cdot 4 = 1 \\implies C = \\frac{1}{4}\n$$\nThe components of the stationary distribution are $\\pi_i = k_i / \\sum_j k_j$.\n$$\n\\pi_1 = \\frac{k_1}{4} = \\frac{1}{4}\n$$\n$$\n\\pi_2 = \\frac{k_2}{4} = \\frac{2}{4} = \\frac{1}{2}\n$$\n$$\n\\pi_3 = \\frac{k_3}{4} = \\frac{1}{4}\n$$\nThe stationary distribution is $\\pi = \\begin{pmatrix} 1/4  1/2  1/4 \\end{pmatrix}$.\n**Interpretation:** The stationary probability $\\pi_i$ represents the long-term fraction of time the walker spends at node $i$. The result shows the walker spends half its time at the central node $2$ and a quarter of its time at each endpoint ($1$ and $3$). This is a direct consequence of the degree heterogeneity: node $2$ has a degree of $2$, while nodes $1$ and $3$ have degrees of $1$. From the low-degree endpoints, a walker is forced to move to the high-degree center, whereas from the center, the walk can diffuse to either end. This structural feature causes the walker to visit the more connected central node more frequently, leading to a higher stationary probability.\n\n**Two-Step Return Probability to Node $2$**\nThe probability of being at node $j$ after $n$ steps, starting from node $i$, is given by the $(i,j)$ entry of the matrix $P^n$. We need to compute the $(2,2)$ entry of $P^2$, which we denote as $(P^2)_{22}$.\nThis can be computed by considering all possible paths of length $2$ from node $2$ back to node $2$. A path of length $2$ involves an intermediate node $j$. The probability is given by $(P^2)_{22} = \\sum_{j=1}^3 P_{2j}P_{j2}$.\nThe possible paths are:\n1.  $2 \\to 1 \\to 2$: The probability is $P_{21} \\cdot P_{12}$. From matrix $P$, $P_{21} = 1/2$ and $P_{12}=1$. The probability is $(1/2) \\cdot 1 = 1/2$.\n2.  $2 \\to 2 \\to 2$: The probability is $P_{22} \\cdot P_{22}$. Since there are no self-loops, $P_{22}=0$. The probability is $0 \\cdot 0 = 0$.\n3.  $2 \\to 3 \\to 2$: The probability is $P_{23} \\cdot P_{32}$. From matrix $P$, $P_{23} = 1/2$ and $P_{32}=1$. The probability is $(1/2) \\cdot 1 = 1/2$.\nThe total two-step return probability is the sum of probabilities of these mutually exclusive paths:\n$$\n(P^2)_{22} = P_{21}P_{12} + P_{22}P_{22} + P_{23}P_{32} = \\left(\\frac{1}{2}\\right)(1) + (0)(0) + \\left(\\frac{1}{2}\\right)(1) = \\frac{1}{2} + \\frac{1}{2} = 1\n$$\nAlternatively, we can compute the matrix $P^2$:\n$$\nP^2 = P \\cdot P = \\begin{pmatrix} 0  1  0 \\\\ 1/2  0  1/2 \\\\ 0  1  0 \\end{pmatrix} \\begin{pmatrix} 0  1  0 \\\\ 1/2  0  1/2 \\\\ 0  1  0 \\end{pmatrix} = \\begin{pmatrix} 1/2  0  1/2 \\\\ 0  1  0 \\\\ 1/2  0  1/2 \\end{pmatrix}\n$$\nThe $(2,2)$ entry of $P^2$ is $1$.",
            "answer": "$$\n\\boxed{1}\n$$"
        },
        {
            "introduction": "Building upon the basics, this practice introduces the concept of absorbing states, which are essential for modeling processes with terminal outcomes like trapping or task completion. You will tackle a classic problem: calculating the probability of a random walker reaching a specific target node before falling into a trap . The exercise utilizes first-step analysis, a powerful and intuitive method that leverages the memoryless nature of Markov chains to set up a solvable system of linear equations.",
            "id": "4299835",
            "problem": "Consider a discrete-time random walk on a finite directed network with node set $\\{S, A, B, C, T, U, V\\}$. The node $T$ is a designated target, and $U$ and $V$ are traps (absorbing states). The random walk evolves as a time-homogeneous Markov chain, with transitions determined by the following rules from the transient nodes $S$, $A$, $B$, and $C$:\n- From $S$: move to $A$ with probability $1/2$, to $B$ with probability $1/3$, and to $V$ with probability $1/6$.\n- From $A$: move to $T$ with probability $1/3$, to $B$ with probability $1/3$, and to $U$ with probability $1/3$.\n- From $B$: move to $A$ with probability $1/4$, to $C$ with probability $1/2$, and to $U$ with probability $1/4$.\n- From $C$: move to $T$ with probability $1/2$, to $B$ with probability $1/3$, and to $V$ with probability $1/6$.\n\nThe nodes $T$, $U$, and $V$ are absorbing: once entered, the walk remains there forever.\n\nStarting from $S$, compute the probability that the walk reaches $T$ before ever hitting either $U$ or $V$. Derive your result from first principles using first-step analysis based on the Markov property, and explain how the same quantity can be written using the fundamental matrix of an absorbing Markov chain. Express the final probability as a reduced fraction. No rounding is required.",
            "solution": "The problem asks for the probability that a random walk starting at node $S$ reaches the target node $T$ before being absorbed by the trap nodes $U$ or $V$. This can be formulated as a hitting probability problem on an absorbing Markov chain. The set of transient states is $\\mathcal{T} = \\{S, A, B, C\\}$ and the set of absorbing states is $\\mathcal{A} = \\{T, U, V\\}$.\n\nLet $h_i$ be the probability of reaching the target state $T$ before any other absorbing state ($U$ or $V$), given that the walk starts at node $i \\in \\mathcal{T}$. We are asked to find $h_S$.\n\nBy definition, for the absorbing states, the probabilities are:\n- $h_T = 1$, as the walk has successfully reached the target.\n- $h_U = 0$, as the walk has been trapped and failed to reach the target first.\n- $h_V = 0$, for the same reason.\n\nWe will solve this problem using two methods as requested: first-step analysis and the fundamental matrix approach.\n\n### 1. Derivation using First-Step Analysis\n\nThe first-step analysis is based on the Markov property. The probability of reaching the target from a state $i$ is a weighted average of the probabilities of reaching the target from the states accessible in one step from $i$. The weights are the transition probabilities.\nFor any transient state $i \\in \\mathcal{T}$, the probability $h_i$ is given by the equation:\n$$h_i = \\sum_{j \\in \\mathcal{T} \\cup \\mathcal{A}} P_{ij} h_j$$\nwhere $P_{ij}$ is the probability of transitioning from state $i$ to state $j$.\n\nApplying this principle to each transient state, we obtain a system of linear equations for the unknown probabilities $h_S, h_A, h_B, h_C$.\n\n1.  **Starting from S:** The walk can transition to $A$ (with probability $1/2$), $B$ (with probability $1/3$), or $V$ (with probability $1/6$).\n    $$h_S = P_{SA} h_A + P_{SB} h_B + P_{SV} h_V = \\frac{1}{2} h_A + \\frac{1}{3} h_B + \\frac{1}{6} (0)$$\n    $$h_S = \\frac{1}{2} h_A + \\frac{1}{3} h_B \\quad (1)$$\n\n2.  **Starting from A:** The walk can transition to $T$ (with probability $1/3$), $B$ (with probability $1/3$), or $U$ (with probability $1/3$).\n    $$h_A = P_{AT} h_T + P_{AB} h_B + P_{AU} h_U = \\frac{1}{3} (1) + \\frac{1}{3} h_B + \\frac{1}{3} (0)$$\n    $$h_A = \\frac{1}{3} + \\frac{1}{3} h_B \\quad (2)$$\n\n3.  **Starting from B:** The walk can transition to $A$ (with probability $1/4$), $C$ (with probability $1/2$), or $U$ (with probability $1/4$).\n    $$h_B = P_{BA} h_A + P_{BC} h_C + P_{BU} h_U = \\frac{1}{4} h_A + \\frac{1}{2} h_C + \\frac{1}{4} (0)$$\n    $$h_B = \\frac{1}{4} h_A + \\frac{1}{2} h_C \\quad (3)$$\n\n4.  **Starting from C:** The walk can transition to $T$ (with probability $1/2$), $B$ (with probability $1/3$), or $V$ (with probability $1/6$).\n    $$h_C = P_{CT} h_T + P_{CB} h_B + P_{CV} h_V = \\frac{1}{2} (1) + \\frac{1}{3} h_B + \\frac{1}{6} (0)$$\n    $$h_C = \\frac{1}{2} + \\frac{1}{3} h_B \\quad (4)$$\n\nWe now solve this system of four linear equations. It is most efficient to solve for $h_A, h_B, h_C$ first.\nFrom equation (2), we can express $h_B$ in terms of $h_A$:\n$$3h_A = 1 + h_B \\implies h_B = 3h_A - 1$$\nSubstitute this expression for $h_B$ into equation (4) to find $h_C$ in terms of $h_A$:\n$$h_C = \\frac{1}{2} + \\frac{1}{3} (3h_A - 1) = \\frac{1}{2} + h_A - \\frac{1}{3} = h_A + \\frac{1}{6}$$\nNow, substitute the expressions for $h_B$ and $h_C$ into equation (3):\n$$3h_A - 1 = \\frac{1}{4} h_A + \\frac{1}{2} \\left(h_A + \\frac{1}{6}\\right)$$\n$$3h_A - 1 = \\frac{1}{4} h_A + \\frac{1}{2} h_A + \\frac{1}{12}$$\nTo eliminate fractions, multiply the entire equation by $12$:\n$$12(3h_A - 1) = 12\\left(\\frac{1}{4} h_A\\right) + 12\\left(\\frac{1}{2} h_A\\right) + 12\\left(\\frac{1}{12}\\right)$$\n$$36h_A - 12 = 3h_A + 6h_A + 1$$\n$$36h_A - 12 = 9h_A + 1$$\n$$27h_A = 13 \\implies h_A = \\frac{13}{27}$$\nNow we can back-substitute to find $h_B$:\n$$h_B = 3h_A - 1 = 3\\left(\\frac{13}{27}\\right) - 1 = \\frac{13}{9} - 1 = \\frac{4}{9}$$\nFinally, we compute the desired probability, $h_S$, using equation (1):\n$$h_S = \\frac{1}{2} h_A + \\frac{1}{3} h_B = \\frac{1}{2}\\left(\\frac{13}{27}\\right) + \\frac{1}{3}\\left(\\frac{4}{9}\\right)$$\n$$h_S = \\frac{13}{54} + \\frac{4}{27} = \\frac{13}{54} + \\frac{8}{54} = \\frac{21}{54}$$\nReducing the fraction by dividing the numerator and denominator by their greatest common divisor, which is $3$:\n$$h_S = \\frac{21 \\div 3}{54 \\div 3} = \\frac{7}{18}$$\n\n### 2. Explanation using the Fundamental Matrix\n\nThe theory of absorbing Markov chains provides a matrix formulation for this problem. The transition matrix $P$ can be written in canonical form by ordering the states with transient states first, $\\mathcal{T}=\\{S, A, B, C\\}$, followed by absorbing states, $\\mathcal{A}=\\{T, U, V\\}$.\n$$P = \\begin{pmatrix} Q  R \\\\ 0  I \\end{pmatrix}$$\nHere, $Q$ is the $4 \\times 4$ submatrix of transition probabilities between transient states, $R$ is the $4 \\times 3$ submatrix of probabilities from transient to absorbing states, $0$ is a $3 \\times 4$ zero matrix, and $I$ is the $3 \\times 3$ identity matrix.\n\nBased on the problem statement, these matrices are:\n$$Q = \\begin{pmatrix} P_{SS}  P_{SA}  P_{SB}  P_{SC} \\\\ P_{AS}  P_{AA}  P_{AB}  P_{AC} \\\\ P_{BS}  P_{BA}  P_{BB}  P_{BC} \\\\ P_{CS}  P_{CA}  P_{CB}  P_{CC} \\end{pmatrix} = \\begin{pmatrix} 0  1/2  1/3  0 \\\\ 0  0  1/3  0 \\\\ 0  1/4  0  1/2 \\\\ 0  0  1/3  0 \\end{pmatrix}$$\n$$R = \\begin{pmatrix} P_{ST}  P_{SU}  P_{SV} \\\\ P_{AT}  P_{AU}  P_{AV} \\\\ P_{BT}  P_{BU}  P_{BV} \\\\ P_{CT}  P_{CU}  P_{CV} \\end{pmatrix} = \\begin{pmatrix} 0  0  1/6 \\\\ 1/3  1/3  0 \\\\ 0  1/4  0 \\\\ 1/2  0  1/6 \\end{pmatrix}$$\n\nThe fundamental matrix of the absorbing chain is $N = (I - Q)^{-1}$. The entry $N_{ij}$ represents the expected number of times the process is in transient state $j$, given it starts in transient state $i$.\n\nThe probability of being absorbed in a specific absorbing state $k$, starting from a transient state $i$, is given by the $(i,k)$-th entry of the matrix product $B = NR$. Let $\\mathbf{h} = (h_S, h_A, h_B, h_C)^T$ be the column vector of absorption probabilities into the target state $T$ from each of the transient states. This vector corresponds to the first column of the matrix $B$ (since we ordered $T$ as the first absorbing state). It can be computed as:\n$$\\mathbf{h} = N \\mathbf{r}_T$$\nwhere $\\mathbf{r}_T$ is the first column of the matrix $R$:\n$$\\mathbf{r}_T = (P_{ST}, P_{AT}, P_{BT}, P_{CT})^T = (0, 1/3, 0, 1/2)^T$$\nSubstituting $N = (I-Q)^{-1}$, we get:\n$$\\mathbf{h} = (I-Q)^{-1} \\mathbf{r}_T$$\nMultiplying by $(I-Q)$ from the left gives:\n$$(I-Q)\\mathbf{h} = \\mathbf{r}_T$$\nThis matrix equation is:\n$$\\begin{pmatrix} 1  -1/2  -1/3  0 \\\\ 0  1  -1/3  0 \\\\ 0  -1/4  1  -1/2 \\\\ 0  0  -1/3  1 \\end{pmatrix} \\begin{pmatrix} h_S \\\\ h_A \\\\ h_B \\\\ h_C \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1/3 \\\\ 0 \\\\ 1/2 \\end{pmatrix}$$\nExpanding this matrix equation yields the following system:\n- $h_S - \\frac{1}{2}h_A - \\frac{1}{3}h_B = 0 \\implies h_S = \\frac{1}{2}h_A + \\frac{1}{3}h_B$\n- $h_A - \\frac{1}{3}h_B = 1/3 \\implies h_A = \\frac{1}{3} + \\frac{1}{3}h_B$\n- $-\\frac{1}{4}h_A + h_B - \\frac{1}{2}h_C = 0 \\implies h_B = \\frac{1}{4}h_A + \\frac{1}{2}h_C$\n- $-\\frac{1}{3}h_B + h_C = 1/2 \\implies h_C = \\frac{1}{2} + \\frac{1}{3}h_B$\nThis system is identical to the one derived using first-step analysis. Therefore, the fundamental matrix formalism provides an equivalent, more general framework for calculating the absorption probabilities, leading to the same result. The solution for $h_S$ remains $\\frac{7}{18}$.",
            "answer": "$$\\boxed{\\frac{7}{18}}$$"
        },
        {
            "introduction": "This advanced practice shifts our focus from analyzing a single walk to comparing the fundamental efficiency of different network topologies. It introduces the profound connection between random walks and electrical networks, using effective resistance to measure the efficiency of transport between nodes. By deriving and comparing the commute time and resistance for a simple path and a complete graph , you will gain a deeper, quantitative understanding of how network structure dictates the speed of exploration and information flow.",
            "id": "4299826",
            "problem": "Consider two undirected, connected, simple graphs with unit weights on each edge so that each edge can be viewed as a unit resistor in an electrical network: the path graph $P_{n}$ on $n$ vertices (a single chain) and the complete graph $K_{n}$ on $n$ vertices (every pair connected). Let $R_{ij}$ denote the effective resistance between nodes $i$ and $j$, defined as the voltage difference required to drive a unit current from $i$ to $j$ under Kirchhoff’s circuit laws and Ohm’s law. Consider the discrete-time simple random walk on each graph, defined by the transition probability $P_{uv} = A_{uv} / d_{u}$, where $A_{uv}$ is the adjacency matrix and $d_{u}$ is the degree of node $u$. Let $H_{ij}$ denote the hitting time from $i$ to $j$ (the expected number of steps to reach $j$ starting from $i$), and define the commute time $C_{ij} = H_{ij} + H_{ji}$.\n\nStarting from fundamental definitions and laws (Kirchhoff’s laws for effective resistance and the linear hitting-time equations for random walks), derive closed-form expressions for:\n- The effective resistance $R_{ij}$ between the endpoints of $P_{n}$.\n- The commute time $C_{ij}$ between the endpoints of $P_{n}$.\n- The effective resistance $R_{ij}$ between any two distinct nodes of $K_{n}$.\n- The commute time $C_{ij}$ between any two distinct nodes of $K_{n}$.\n\nThen, compare the asymptotic scaling of each quantity with $n$ for $P_{n}$ versus $K_{n}$, explaining the qualitative difference in transport and exploration efficiency implied by the results. Express your final answer as a single row matrix listing the four expressions in the order $\\left(R_{P_{n}},\\, C_{P_{n}},\\, R_{K_{n}},\\, C_{K_{n}}\\right)$. No numerical rounding is required; provide exact expressions in terms of $n$.",
            "solution": "The problem requires the derivation of the effective resistance $R_{ij}$ and commute time $C_{ij}$ for two specific graph structures: the path graph $P_n$ and the complete graph $K_n$. We will address each case systematically.\n\n**1. Path Graph $P_n$**\n\nLet the vertices of the path graph $P_n$ be labeled sequentially from $i=1$ to $i=n$. The edges connect vertex $i$ to vertex $i+1$ for $i=1, 2, \\dots, n-1$. The endpoints are vertices $1$ and $n$.\n\n**Effective Resistance on $P_n$ ($R_{1,n}$)**\nIn the electrical network analogy, each edge is a resistor with resistance $r=1$. To find the effective resistance $R_{1,n}$ between the endpoints (nodes $1$ and $n$), we can observe that the graph represents a simple series circuit. The path from node $1$ to node $n$ is unique and traverses $n-1$ edges. The total resistance of resistors in series is their sum. Therefore, the effective resistance is the sum of the resistances of the $n-1$ edges along the path.\n$$R_{1,n} = \\sum_{k=1}^{n-1} r = \\sum_{k=1}^{n-1} 1 = n-1$$\n\n**Commute Time on $P_n$ ($C_{1,n}$)**\nThe commute time is defined as $C_{ij} = H_{ij} + H_{ji}$, where $H_{ij}$ is the hitting time. We derive $H_{ij}$ from its fundamental definition. For a simple random walk on a graph, the hitting time $H_{i,j}$ from a starting node $i$ to a target node $j$ satisfies the following system of linear equations. Let $h_k = H_{k,j}$ for a fixed target $j$:\n$$h_k = \\begin{cases} 0  \\text{if } k=j \\\\ 1 + \\sum_{l \\sim k} P_{kl} h_l  \\text{if } k \\neq j \\end{cases}$$\nwhere $P_{kl} = 1/d_k$ is the transition probability from $k$ to a neighbor $l$, and $d_k$ is the degree of vertex $k$.\n\nLet's find $H_{1,n}$ on $P_n$. The target is vertex $n$. Let $h_i = H_{i,n}$. The degrees are $d_1 = d_n = 1$ and $d_i = 2$ for $i \\in \\{2, \\dots, n-1\\}$. The system of equations is:\n1. $h_n = 0$\n2. For $i \\in \\{2, \\dots, n-1\\}$: $h_i = 1 + \\frac{1}{2}(h_{i-1} + h_{i+1})$\n3. For $i=1$: $h_1 = 1 + h_2$\n\nRearranging the equation for the interior nodes gives the second-order linear non-homogeneous difference equation:\n$$h_{i+1} - 2h_i + h_{i-1} = -2$$\nThe general solution is the sum of a homogeneous solution $h_i^{(h)}$ and a particular solution $h_i^{(p)}$. The homogeneous equation is $h_{i+1} - 2h_i + h_{i-1} = 0$, which has a characteristic equation $\\lambda^2 - 2\\lambda + 1 = 0 \\Rightarrow (\\lambda-1)^2 = 0$. For the repeated root $\\lambda=1$, the homogeneous solution is $h_i^{(h)} = A \\cdot 1^i + B \\cdot i \\cdot 1^i = A + Bi$.\nFor the particular solution, we try a polynomial $h_i^{(p)} = Ci^2$. Substituting this into the equation:\n$C(i+1)^2 - 2Ci^2 + C(i-1)^2 = -2 \\Rightarrow C(i^2+2i+1) - 2Ci^2 + C(i^2-2i+1) = -2 \\Rightarrow 2C = -2 \\Rightarrow C = -1$.\nSo, $h_i^{(p)} = -i^2$.\nThe general solution is $h_i = A + Bi - i^2$.\n\nWe find the constants $A$ and $B$ using the boundary conditions:\n1. $h_n = 0 \\Rightarrow A + Bn - n^2 = 0$\n2. $h_1 = 1 + h_2 \\Rightarrow A+B(1)-1^2 = 1 + (A+B(2)-2^2) \\Rightarrow A+B-1 = 1+A+2B-4 \\Rightarrow B-1 = 2B-3 \\Rightarrow B=2$.\n\nSubstituting $B=2$ into the first condition: $A + 2n - n^2 = 0 \\Rightarrow A = n^2 - 2n$.\nThus, the hitting time from node $i$ to node $n$ is $h_i = (n^2 - 2n) + 2i - i^2 = n^2 - 2n + 2i - i^2 = (n-i)(n+i-2)$.\nWe are interested in $H_{1,n}$, which corresponds to $i=1$:\n$$H_{1,n} = (n-1)(n+1-2) = (n-1)(n-1) = (n-1)^2$$\nDue to the symmetry of the graph (relabeling vertices as $j' = n+1-j$), the hitting time from $n$ to $1$ is identical to the hitting time from $1$ to $n$:\n$$H_{n,1} = H_{1,n} = (n-1)^2$$\nThe commute time between the endpoints is therefore:\n$$C_{1,n} = H_{1,n} + H_{n,1} = (n-1)^2 + (n-1)^2 = 2(n-1)^2$$\n\n**2. Complete Graph $K_n$**\n\nIn the complete graph $K_n$, every pair of distinct vertices is connected by an edge. Let each edge have a resistance of $r=1$.\n\n**Effective Resistance on $K_n$ ($R_{ij}$)**\nWe wish to find the effective resistance between any two distinct nodes, say $i$ and $j$. By Ohm's Law, $R_{ij} = V_{ij}/I$, where $V_{ij}$ is the potential difference. We can find this by injecting $I=1$ unit of current at node $i$ and extracting it at node $j$. Let $\\phi_k$ be the electric potential at node $k$. We can set $\\phi_j = 0$ without loss of generality.\nDue to the symmetry of the graph, all other nodes $k \\notin \\{i, j\\}$ must have the same potential, which we denote by $\\phi_x$. So we have potentials $\\phi_i$, $\\phi_j=0$, and $\\phi_k = \\phi_x$ for $k\\neq i,j$.\nWe apply Kirchhoff's Current Law: the sum of currents flowing out of any node is zero (unless it is a source or sink).\nAt node $i$ (source of $1$ unit of current):\n$$1 = \\frac{\\phi_i - \\phi_j}{r} + \\sum_{k \\neq i,j} \\frac{\\phi_i - \\phi_k}{r} = (\\phi_i - 0) + (n-2)(\\phi_i - \\phi_x)$$\n$$1 = \\phi_i + (n-2)(\\phi_i - \\phi_x) \\quad (1)$$\nAt any intermediate node $k \\neq i,j$ (no net current):\n$$0 = \\frac{\\phi_k - \\phi_i}{r} + \\frac{\\phi_k - \\phi_j}{r} + \\sum_{l \\neq i,j,k} \\frac{\\phi_k - \\phi_l}{r}$$\n$$0 = (\\phi_x - \\phi_i) + (\\phi_x - 0) + \\sum_{l \\neq i,j,k} (\\phi_x - \\phi_x)$$\n$$0 = 2\\phi_x - \\phi_i \\Rightarrow \\phi_i = 2\\phi_x$$\nSubstitute $\\phi_i = 2\\phi_x$ into equation $(1)$:\n$$1 = (2\\phi_x) + (n-2)(2\\phi_x - \\phi_x) = 2\\phi_x + (n-2)\\phi_x = (2 + n - 2)\\phi_x = n\\phi_x$$\nThis gives $\\phi_x = 1/n$. Consequently, $\\phi_i = 2\\phi_x = 2/n$.\nThe effective resistance is the potential difference needed for a unit current, which is:\n$$R_{ij} = \\frac{\\phi_i - \\phi_j}{1} = \\frac{2/n - 0}{1} = \\frac{2}{n}$$\n\n**Commute Time on $K_n$ ($C_{ij}$)**\nWe again use the fundamental equations for hitting time $h_k = H_{k,j}$. For $K_n$, the degree of every vertex is $d_k = n-1$. A random walker at any node $k$ moves to any of its $n-1$ neighbors with probability $1/(n-1)$.\nLet's find $H_{i,j}$ for distinct $i,j$. Let $h_k = H_{k,j}$. We have $h_j = 0$.\nDue to the symmetry of the graph, for any starting node $k \\neq j$, the time to reach $j$ must be the same. So let $h_k = H$ for all $k \\neq j$.\nFor any starting node $i \\neq j$, the hitting time equation is:\n$$h_i = 1 + \\sum_{l \\sim i} P_{il}h_l = 1 + \\frac{1}{n-1} \\sum_{l \\neq i} h_l$$\nThe sum is over all vertices $l \\neq i$. One of these vertices is the target $j$, for which $h_j=0$. The other $n-2$ vertices $l$ are not $j$, so $h_l=H$.\nSubstituting $h_i=H$ into the equation:\n$$H = 1 + \\frac{1}{n-1} \\left( h_j + \\sum_{l \\neq i,j} h_l \\right) = 1 + \\frac{1}{n-1} \\left( 0 + (n-2)H \\right)$$\n$$H = 1 + \\frac{n-2}{n-1}H$$\nSolving for $H$:\n$$H \\left( 1 - \\frac{n-2}{n-1} \\right) = 1 \\Rightarrow H \\left( \\frac{n-1 - (n-2)}{n-1} \\right) = 1 \\Rightarrow H \\left( \\frac{1}{n-1} \\right) = 1$$\n$$H = n-1$$\nSo, for any distinct $i,j$, the hitting time is $H_{ij} = n-1$. By symmetry, $H_{ji} = n-1$.\nThe commute time is:\n$$C_{ij} = H_{ij} + H_{ji} = (n-1) + (n-1) = 2(n-1)$$\n\n**3. Comparison and Interpretation**\n\nWe summarize the results for large $n$:\n- **Path Graph $P_n$ (endpoints):**\n  - $R_{1,n} = n-1 \\implies R \\sim O(n)$\n  - $C_{1,n} = 2(n-1)^2 \\implies C \\sim O(n^2)$\n- **Complete Graph $K_n$ (any pair):**\n  - $R_{ij} = 2/n \\implies R \\sim O(n^{-1})$\n  - $C_{ij} = 2(n-1) \\implies C \\sim O(n)$\n\n**Transport Efficiency ($R_{ij}$):** The effective resistance measures the opposition to flow between two points.\n- On $P_n$, the resistance grows linearly with the size of the graph. This signifies poor transport efficiency, as flow is confined to a single, long path. This is characteristic of a spatially extended, one-dimensional system.\n- On $K_n$, the resistance vanishes as $n$ increases. The massive number of parallel paths makes transport exceedingly efficient. This high connectivity is a hallmark of \"small-world\" networks where the effective distance between any two nodes is small.\n\n**Exploration Efficiency ($C_{ij}$):** The commute time measures the expected time for a random walker to travel between two nodes and back again, indicating how easily a walker explores the network.\n- On $P_n$, the commute time grows quadratically with size. A random walker on a line is highly likely to revisit its steps many times, making the traversal between distant ends a very slow, diffusive process. This reflects poor exploration efficiency.\n- On $K_n$, the commute time grows only linearly with size. Although the number of nodes increases, the high connectivity ensures a random walker can reach any other node relatively quickly. The hitting time $H_{ij}=n-1$ is the time it takes, on average, to pick the correct one out of $n-1$ options. This is vastly more efficient exploration than on the path graph.\n\nThe scaling laws for $R_{ij}$ and $C_{ij}$ quantitatively demonstrate the profound impact of network topology on dynamics. The path graph is an inefficient structure for transport and exploration, while the complete graph is maximally efficient.",
            "answer": "$$\\boxed{\\begin{pmatrix} n-1  2(n-1)^{2}  \\frac{2}{n}  2(n-1) \\end{pmatrix}}$$"
        }
    ]
}