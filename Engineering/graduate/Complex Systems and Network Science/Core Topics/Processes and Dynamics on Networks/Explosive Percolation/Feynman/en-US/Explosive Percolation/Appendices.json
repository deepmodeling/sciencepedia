{
    "hands_on_practices": [
        {
            "introduction": "To truly grasp what makes explosive percolation unique, we must first have a firm understanding of its classical counterpart. This practice begins with the canonical Erdős–Rényi random graph, the benchmark for random network models. By connecting the graph structure at criticality to the mathematics of Galton-Watson branching processes, you will derive the famous power-law exponent $\\tau$ that governs the distribution of finite clusters . This foundational exercise not only sharpens your analytical skills but also provides the essential baseline needed to appreciate how competitive selection rules fundamentally alter the nature of the percolation transition.",
            "id": "4275747",
            "problem": "Consider the Erdős–Rényi (ER) random graph $G(n, c/n)$ with $0  c \\leq 1$, where $n$ is the number of vertices and $c$ is a fixed control parameter. In the subcritical and critical regimes, finite connected components are tree-like and can be represented by a Galton–Watson branching process with Poisson$(c)$ offspring. Let $n_s$ denote the expected number of clusters of size $s$ per vertex, and define the susceptibility as $\\chi = \\sum_{s \\geq 1} s^{2} n_s$. The precritical tail of the cluster-size distribution is often written in the scaling form $n_s \\sim s^{-\\tau} \\exp(-s/s_c)$, where $s_c$ is a cutoff that diverges at criticality.\n\nStarting from the foundational connection between $G(n, c/n)$ and Galton–Watson branching processes with finite offspring variance, derive the asymptotic form of the per-vertex cluster-size number density $n_s$ at the critical point $c = 1$, and compute the tail exponent $\\tau$ in the scaling $n_s \\sim s^{-\\tau}$. Use only well-tested facts from branching processes, generating functions, and asymptotics, and demonstrate the consistency of your result with the divergence of the susceptibility $\\chi$ near criticality via standard scaling arguments for mean-field percolation.\n\nThen, analyze how Achlioptas edge-selection rules (for example, selecting one edge from two candidates by minimizing the product of the sizes of the clusters the endpoints belong to) bias the formation of large clusters. Based on first principles of near-critical mean-field growth with finite offspring variance and tree-like local structure, reason whether such selection is expected to change the leading power-law exponent $\\tau$ in the complete-graph limit, or instead to introduce subleading, slowly varying corrections to scaling (for example, multiplicative logarithmic factors) in the precritical tail and the susceptibility. Your analysis must clearly separate universal leading behavior from non-universal metric factors and possible logarithmic corrections.\n\nReport the value of the tail exponent $\\tau$ as a single exact number. No rounding is required. The final answer must be unitless.",
            "solution": "The Erdős–Rényi (ER) random graph $G(n, c/n)$ with $0  c \\leq 1$ has a well-known correspondence with Galton–Watson branching processes: the connected component containing a uniformly random vertex is distributed as the total progeny of a Galton–Watson process with Poisson$(c)$ offspring, provided the component is finite and cycles are negligible, which is true in the subcritical and critical regimes when $n$ is large.\n\nLet $S$ denote the total progeny of the Galton–Watson process starting from a single ancestor. The probability generating function $f(z)$ of the offspring distribution is $f(z) = \\exp(c(z - 1))$, and the generating function $H(z)$ for $S$ satisfies the classical functional equation\n$$\nH(z) = z f(H(z)) = z \\exp\\!\\big(c(H(z) - 1)\\big).\n$$\nThe coefficient $p_s = \\Pr(S = s)$ is given by the Lagrange inversion formula,\n$$\np_s = [z^s] H(z) = \\frac{1}{s} [u^{s-1}] f(u)^{s} = \\frac{1}{s} [u^{s-1}] \\exp\\!\\big(c s (u - 1)\\big).\n$$\nEvaluating the coefficient of $u^{s-1}$ in $\\exp\\!\\big(c s u\\big)$ yields\n$$\np_s = \\frac{1}{s} \\exp(-c s) \\frac{(c s)^{s-1}}{(s-1)!} = \\exp(-c s) \\frac{(c s)^{s-1}}{s!}.\n$$\nAt criticality, $c = 1$, so\n$$\np_s = \\exp(-s) \\frac{s^{s-1}}{s!}.\n$$\nThis is the Borel distribution for the total progeny at criticality. To obtain the asymptotic tail, we apply Stirling's approximation to $s!$,\n$$\ns! \\sim \\sqrt{2 \\pi} \\, s^{s + 1/2} \\exp(-s),\n$$\nwhich yields\n$$\np_s \\sim \\frac{\\exp(-s) s^{s-1}}{\\sqrt{2 \\pi} \\, s^{s + 1/2} \\exp(-s)} = \\frac{1}{\\sqrt{2 \\pi}} s^{-3/2}.\n$$\nThe quantity $n_s$, the expected number of clusters of size $s$ per vertex, relates to $p_s$ by the identity $s n_s = \\Pr(\\text{a random vertex is in a cluster of size } s) = p_s$, because $s n_s$ is the fraction of vertices belonging to clusters of size $s$. Therefore,\n$$\nn_s = \\frac{p_s}{s} \\sim \\frac{1}{\\sqrt{2 \\pi}} s^{-5/2}.\n$$\nHence, the tail exponent in the scaling $n_s \\sim s^{-\\tau}$ at criticality is\n$$\n\\tau = \\frac{5}{2}.\n$$\n\nTo check consistency with susceptibility scaling, define the susceptibility $\\chi = \\sum_{s \\geq 1} s^{2} n_s$. At criticality, substituting $n_s \\sim s^{-5/2}$ gives\n$$\n\\chi \\sim \\sum_{s \\geq 1} s^{2} \\cdot s^{-5/2} = \\sum_{s \\geq 1} s^{-1/2},\n$$\nwhich diverges, as expected. Away from criticality in the subcritical regime $c  1$, the cluster-size distribution acquires an exponential cutoff $s_c$,\n$$\nn_s \\sim s^{-\\tau} \\exp(-s/s_c),\n$$\nand the susceptibility scales as\n$$\n\\chi \\sim \\int_{1}^{s_c} s^{2 - \\tau} \\, ds \\sim s_c^{3 - \\tau}.\n$$\nIn mean-field percolation (including ER), the cutoff obeys $s_c \\sim |t|^{-1/\\sigma}$ with $t$ a measure of distance to criticality and $\\sigma = \\frac{1}{2}$, whence\n$$\n\\chi \\sim |t|^{-(3 - \\tau)/\\sigma}.\n$$\nWith $\\tau = \\frac{5}{2}$ and $\\sigma = \\frac{1}{2}$, this gives\n$$\n\\chi \\sim |t|^{-1},\n$$\nwhich matches the mean-field susceptibility exponent $\\gamma = 1$, confirming the internal consistency of the derived $\\tau$ with standard scaling.\n\nWe now analyze Achlioptas edge-selection rules (for instance, picking between two candidate edges by minimizing the product of the sizes of the clusters that their endpoints belong to) in the complete-graph limit. Such rules suppress the early coalescence of large clusters, thereby delaying the emergence of a giant component and narrowing the apparent transition window. However, the local structure of finite clusters remains tree-like, and the offspring distribution retains finite variance. In the near-critical regime, this implies that the universal features dictated by the Galton–Watson fixed point with finite variance persist. Consequently, the leading power-law tail of $n_s$ at criticality is governed by the same branching-process asymptotics, which yields $\\tau = \\frac{5}{2}$ as the universal mean-field value.\n\nWhat the selection bias can change are non-universal metric factors and the approach to criticality, often introducing slowly varying corrections encapsulated by multiplicative functions that vary logarithmically with size or control parameter. A consistent way to capture such effects is to write\n$$\nn_s \\sim s^{-\\tau} L(s), \\quad L(s) \\text{ slowly varying, e.g., } L(s) \\sim (\\ln s)^{\\hat{\\alpha}},\n$$\nand similarly, in the subcritical regime,\n$$\n\\chi \\sim |t|^{-1} \\, (\\ln |t|^{-1})^{\\hat{\\gamma}},\n$$\nfor some correction exponents $\\hat{\\alpha}$ and $\\hat{\\gamma}$ determined by the detailed selection dynamics. These logarithmic corrections arise because the bias can cause the effective control parameter to approach criticality with a drift that depends on system size in a slowly varying manner (for example, proportional to $1/\\ln n$), without changing the underlying branching fixed point that enforces the universal $\\tau = \\frac{5}{2}$ in the complete-graph limit. Therefore, Achlioptas selection biases are expected to modify subleading corrections while leaving the leading mean-field tail exponent $\\tau$ unchanged.\n\nIn summary, the computation from first principles via critical branching-process asymptotics gives $\\tau = \\frac{5}{2}$ for ER at criticality, and Achlioptas selection rules in the complete-graph limit are expected to preserve this leading exponent while potentially introducing logarithmic corrections in the slowly varying part of the distribution and susceptibility.",
            "answer": "$$\\boxed{\\frac{5}{2}}$$"
        },
        {
            "introduction": "Theoretical models are powerful, but their predictions are often verified and explored through computer simulation. The feasibility of such simulations, especially for large networks, hinges on algorithmic efficiency. This practice moves from theory to implementation by examining the computational cost of simulating the Achlioptas process . You will analyze the time complexity by dissecting the role of the Disjoint Set Union (DSU) data structure, an indispensable tool for efficiently tracking network components. This exercise builds a crucial bridge between abstract physical models and the practical realities of computational science.",
            "id": "4275775",
            "problem": "Consider simulating explosive percolation on an initially empty, simple undirected graph with $N$ labeled vertices using the Achlioptas best-of-$m$ product rule. At each discrete step, you sample $m$ candidate edges uniformly without replacement from the set of currently absent edges. For each candidate edge $\\{u,v\\}$, you compute the product rule score as the product $s_{u} s_{v}$, where $s_{u}$ and $s_{v}$ are the sizes of the connected components containing $u$ and $v$, respectively. The selected edge is the candidate with minimal score among those that connect two different components; ties are broken arbitrarily, and the selected edge is then added. Connected components and their sizes are maintained via Disjoint Set Union (DSU), also known as union–find, implemented with union by size (or union by rank) and path compression. The size of a component is maintained as an integer stored at its root.\n\nAssume the following standard facts: each DSU find operation with union by size (or union by rank) and path compression runs in amortized time proportional to the inverse Ackermann function $\\alpha(N)$, DSU union given two root identifiers runs in constant time, and reading the size stored at a root is constant time. Arithmetic operations on integers and comparisons are constant time. You may assume that the $m$ candidate edges are independently processed by first computing both endpoint roots via DSU find, then reading component sizes, and finally computing the product rule score in constant time.\n\nDerive, from these primitives and definitions, the exact count of DSU find operations per simulation step in the described implementation, and use it to obtain the leading-order amortized time per step as a closed-form expression in terms of $m$ and $\\alpha(N)$, ignoring lower-order additive contributions from unions and constant-time arithmetic or comparisons. Additionally, briefly justify two implementation-level optimizations that ensure no redundant DSU calls are performed during selection and merging so that the derived bound is achieved.\n\nExpress your final answer as a single closed-form analytic expression in terms of $m$ and $\\alpha(N)$. No rounding is required. No physical units are involved.",
            "solution": "A single step involves selecting one edge from $m$ candidates and adding it to the graph. The analysis hinges on counting the number of DSU `find` operations, as these are the dominant cost factor according to the problem statement. A step can be divided into two phases: the selection phase and the merging phase.\n\n**1. Analysis of DSU `find` Operations**\n\n**Selection Phase:**\nAt each step, $m$ candidate edges are sampled. For each candidate edge, denoted $\\{u, v\\}$, the algorithm must compute a score equal to the product of the sizes of the components containing $u$ and $v$, written as $s_u s_v$. Crucially, the selection rule only considers edges that connect two different components. To satisfy these requirements for a single edge $\\{u, v\\}$, the following sub-steps are necessary:\n1.  Determine the root of the component containing vertex $u$. This is achieved by one DSU operation: `find(u)`.\n2.  Determine the root of the component containing vertex $v$. This is achieved by a second DSU operation: `find(v)`.\n3.  Compare the two roots. If they are identical, the edge connects vertices within the same component and is not considered for selection.\n4.  If the roots are different, their respective component sizes, $s_u$ and $s_v$, are read from the data stored at each root. The problem states this is a constant-time operation. The score $s_u s_v$ is then computed.\n\nThe problem specifies that \"the $m$ candidate edges are independently processed by first computing both endpoint roots via DSU find\". This instruction implies a direct loop over the $m$ candidates, where for each candidate, two `find` operations are executed. Therefore, the total number of `find` operations performed during the selection phase is exactly $2m$.\n\n**Merging Phase:**\nAfter iterating through all $m$ candidates, the edge $\\{u^*, v^*\\}$ with the minimal score that connects two distinct components is selected. This edge must then be added to the graph, which corresponds to merging the two components in the DSU data structure. A naive implementation of a `union(u, v)` function would internally call `find(u)` and `find(v)` before performing the merge, resulting in two additional `find` operations.\n\nHowever, the problem requires deriving the complexity for an implementation where \"no redundant DSU calls are performed\" and explicitly states that \"DSU union given two root identifiers runs in constant time\". This points towards an optimized implementation where the root identifiers for the selected edge $\\{u^*, v^*\\}$, which were already computed and are known from the selection phase, are passed directly to the union function. Such a `union_by_root` operation does not require any `find` calls. Consequently, the merging phase contributes $0$ DSU `find` operations.\n\n**Total `find` Operations:**\nCombining the two phases, the total exact count of DSU `find` operations per simulation step in an optimized implementation is the sum of operations from the selection and merging phases: $2m + 0 = 2m$.\n\n**2. Leading-Order Amortized Time**\n\nThe problem provides that a DSU `find` operation with the specified optimizations (union by size/rank and path compression) runs in an amortized time proportional to the inverse Ackermann function, $\\alpha(N)$. Let the time for a single `find` operation be $T_{\\text{find}} = c \\cdot \\alpha(N)$, where $c$ is a constant of proportionality.\n\nThe total amortized time per step is dominated by the $2m$ `find` operations. Other operations, such as the constant-time union operation, arithmetic operations, and comparisons, contribute lower-order additive terms. Ignoring these per the problem's instruction, the leading-order amortized time per step, $T_{\\text{step}}$, is:\n$$T_{\\text{step}} \\propto 2m \\cdot T_{\\text{find}}$$\n$$T_{\\text{step}} \\propto 2m \\cdot \\alpha(N)$$\nThe final expression should be in terms of $m$ and $\\alpha(N)$, so we can write the leading-order time as $2m \\alpha(N)$.\n\n**3. Justification of Optimizations**\n\nThe derived count of $2m$ `find` operations and the corresponding time complexity are achieved through specific implementation choices that eliminate redundant computations. A completely naive implementation would perform $2m+2$ `find` operations. The two key optimizations to reduce this to $2m$ are:\n\n1.  **Caching Roots for the Winning Edge:** During the selection loop, as each candidate edge $\\{u,v\\}$ is evaluated, its roots `root_u` and `root_v` are computed. When an edge is identified as having the minimum score so far, its root identifiers (`root_u`, `root_v`) must be cached along with the vertex identifiers. Simply storing $\\{u, v\\}$ would necessitate re-computing the roots later.\n2.  **Using a Root-Based Union Function:** After the selection loop finishes, the merge operation must be performed using a function variant, say `union_by_root(root_u, root_v)`, that accepts the pre-computed and cached root identifiers of the winning edge. This directly leverages the problem's stated primitive that \"union given two root identifiers runs in constant time\" and avoids the two redundant `find` calls a generic `union(u, v)` function would implicitly make.\n\nThese two optimizations ensure that the merging phase adds no `find` operations to the $2m$ operations required by the selection phase, thereby achieving the derived bound.",
            "answer": "$$\\boxed{2m\\alpha(N)}$$"
        },
        {
            "introduction": "A defining feature of explosive percolation is the sudden and dramatic emergence of a giant component. This problem focuses on quantifying this \"jump\" by employing the tools of phenomenological modeling and extreme-value statistics. Based on a plausible, theoretically-motivated description of the cluster sizes just before the transition, you will derive the scaling behavior of the largest jump in the giant component size . This exercise showcases a key skill in modern theoretical physics: the ability to construct and analyze simplified, effective models to capture the essential scaling properties of a complex system.",
            "id": "4275815",
            "problem": "Consider an Achlioptas process (AP) on an Erdős–Rényi random graph with $N$ nodes, where at each step two candidate edges are selected uniformly at random, and the product rule is used to add the edge that minimizes the product of the sizes of the clusters that would be joined. Let $S_1(t)$ denote the size of the largest connected cluster after $t$ edges have been added, and let $\\Delta S_1$ denote the maximum single-edge jump in $S_1(t)$ over the entire evolution.\n\nIn the precritical regime immediately prior to the onset of a macroscopic cluster, assume the following phenomenological description holds due to the suppression of large-cluster mergers by the product rule:\n- The number of clusters is $M(N) = \\rho N$ with a finite constant density $\\rho  0$.\n- The distribution of cluster sizes has a stretched-exponential upper tail characterized by\n$$\n\\mathbb{P}\\{S \\ge s\\} \\approx \\exp\\!\\left[-\\left(\\frac{s}{s_c(N)}\\right)^{\\delta}\\right],\n$$\nwith tail-shape parameter $\\delta  0$ and a finite-size cutoff scale $s_c(N)$ growing sublinearly as\n$$\ns_c(N) = a\\, N^{\\chi},\n$$\nwhere $a  0$ and $0  \\chi  1$ are constants determined by the product-rule dynamics in the precritical regime.\n\nModel the largest precritical clusters by extreme-value statistics under the independence approximation for the $M(N)$ clusters. Further, assume that the dominant contribution to the maximum jump $\\Delta S_1$ arises from the merger of the two largest precritical clusters, so that the increment is of the order of the second-largest precritical cluster size.\n\nUsing only the above foundations and approximations, derive the leading-order scaling, as a function of $N$, for the maximum relative jump $\\Delta S_1/N$. Ignore all multiplicative constants and subleading corrections, and express your final answer as a single closed-form analytic expression in terms of $N$, $\\chi$, and $\\delta$ only. No other parameters may appear in the final expression. The answer must be an expression without an equality or inequality sign.",
            "solution": "To determine the leading-order scaling of the maximum relative jump $\\Delta S_1/N$, we start from the assumption that the jump's magnitude $\\Delta S_1$ is on the order of the second-largest precritical cluster size, $S_{(2)}$. The scaling of $S_{(2)}$ is found using extreme-value statistics. For an ensemble of $M(N)$ clusters treated as independent random variables, the characteristic size of the second-largest cluster, $S_{(2)}$, is given by the condition that the expected number of clusters of at least this size is of order unity (specifically, 2):\n$$\nM(N) \\mathbb{P}\\{S \\ge S_{(2)}\\} \\approx 2\n$$\nWe substitute the given expressions for the number of clusters, $M(N) = \\rho N$, and the tail probability, $\\mathbb{P}\\{S \\ge s\\} \\approx \\exp\\left[-\\left(\\frac{s}{s_c(N)}\\right)^{\\delta}\\right]$, into this relation:\n$$\n(\\rho N) \\exp\\left[-\\left(\\frac{S_{(2)}}{s_c(N)}\\right)^{\\delta}\\right] \\approx 2\n$$\nOur goal is to solve this equation for $S_{(2)}$. We begin by isolating the exponential term:\n$$\n\\exp\\left[-\\left(\\frac{S_{(2)}}{s_c(N)}\\right)^{\\delta}\\right] \\approx \\frac{2}{\\rho N}\n$$\nTaking the natural logarithm of both sides gives:\n$$\n-\\left(\\frac{S_{(2)}}{s_c(N)}\\right)^{\\delta} \\approx \\ln\\left(\\frac{2}{\\rho N}\\right) = \\ln(2) - \\ln(\\rho) - \\ln(N)\n$$\nFor large $N$, the $\\ln(N)$ term is the dominant term on the right-hand side. The terms $\\ln(2)$ and $\\ln(\\rho)$ are constants and thus constitute subleading corrections. As we are interested in the leading-order scaling, we can make the approximation:\n$$\n-\\left(\\frac{S_{(2)}}{s_c(N)}\\right)^{\\delta} \\sim -\\ln(N)\n$$\nThis simplifies to:\n$$\n\\left(\\frac{S_{(2)}}{s_c(N)}\\right)^{\\delta} \\sim \\ln(N)\n$$\nNow we solve for $S_{(2)}$:\n$$\n\\frac{S_{(2)}}{s_c(N)} \\sim (\\ln(N))^{\\frac{1}{\\delta}}\n$$\n$$\nS_{(2)} \\sim s_c(N) (\\ln(N))^{\\frac{1}{\\delta}}\n$$\nNext, we substitute the given scaling for the cutoff size, $s_c(N) = a N^{\\chi}$. We ignore the multiplicative constant $a$ as per the problem instructions.\n$$\nS_{(2)} \\sim N^{\\chi} (\\ln(N))^{\\frac{1}{\\delta}}\n$$\nSince $\\Delta S_1 \\sim S_{(2)}$, we have:\n$$\n\\Delta S_1 \\sim N^{\\chi} (\\ln(N))^{\\frac{1}{\\delta}}\n$$\nThe final step is to find the scaling of the *relative* jump, $\\frac{\\Delta S_1}{N}$:\n$$\n\\frac{\\Delta S_1}{N} \\sim \\frac{N^{\\chi} (\\ln(N))^{\\frac{1}{\\delta}}}{N}\n$$\nUsing the law of exponents, we obtain the final scaling expression:\n$$\n\\frac{\\Delta S_1}{N} \\sim N^{\\chi - 1} (\\ln(N))^{\\frac{1}{\\delta}}\n$$\nThe problem asks for the expression itself, which is $N^{\\chi - 1} (\\ln(N))^{\\frac{1}{\\delta}}$.",
            "answer": "$$\n\\boxed{N^{\\chi - 1} (\\ln(N))^{\\frac{1}{\\delta}}}\n$$"
        }
    ]
}