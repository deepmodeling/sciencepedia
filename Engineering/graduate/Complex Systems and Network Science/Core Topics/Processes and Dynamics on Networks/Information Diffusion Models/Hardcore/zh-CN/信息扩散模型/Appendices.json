{
    "hands_on_practices": [
        {
            "introduction": "理论学习之后，最好的检验方式莫过于动手实践。此练习将引导您深入独立级联（Independent Cascade, IC）模型的核心机制。您将通过枚举所有可能的“活性边”（live-edge）图来精确计算一个小型网络中信息传播最终规模的完整概率分布，而不仅仅是期望值。这项实践旨在将抽象的概率模型转化为具体的计算过程，从而巩固您对 IC 模型随机性的理解，这对于后续更高级的分析和算法设计至关重要 ()。",
            "id": "4283251",
            "problem": "考虑一个由图 $G=(V,E)$ 表示的有向网络，其中 $V$ 是一个有限的节点集合，$E$ 是一个有限的有向边集合。每条有向边 $(i,j)\\in E$ 都有一个相关的激活概率 $p_{ij}\\in[0,1]$。我们研究独立级联 (IC) 模型下的信息扩散，其定义如下：从时间 $t=0$ 时的一个初始激活节点集 $S\\subseteq V$ 开始。在随后的每个离散时间步，任何在时间 $t$ 被激活的节点 $i$ 都有且仅有一次机会，通过边 $(i,j)$ 激活其每个当前未激活的出邻居 $j$，每次激活尝试对于不同的边是独立的，并以概率 $p_{ij}$ 成功。所有激活尝试每条边只发生一次，一旦一个节点被激活，它将永远保持激活状态。当没有新的激活发生时，该过程终止，产生一个最终的激活集 $A_\\infty\\subseteq V$。最终的级联规模为 $|A_\\infty|$。\n\n从上述概率和独立级联模型的基本定义出发，通过活边视角枚举激活路径，推导出一个有原则的算法来计算最终级联规模的精确分布。然后，为指定的小型网络和参数集实现该算法。您的程序必须为下面的每个测试用例计算概率列表 $\\left[\\Pr\\left(|A_\\infty|=s_{\\min}\\right),\\Pr\\left(|A_\\infty|=s_{\\min}+1\\right),\\ldots,\\Pr\\left(|A_\\infty|=|V|\\right)\\right]$，其中 $s_{\\min}=|S|$ 是可能的最小级联规模。所有概率必须表示为小数（即浮点数），并四舍五入到10位小数。您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表（例如，$[result1,result2,result3]$），其中每个 $resultk$ 是测试用例 $k$ 的列表。\n\n网络规范：\n- 节点：$V=\\{0,1,2,3,4\\}$，因此 $|V|=5$。\n- 有向边 $E$（按固定顺序列举）：$(0,1)$, $(0,2)$, $(1,2)$, $(1,3)$, $(2,3)$, $(2,4)$, $(3,4)$, $(4,1)$。共有 $|E|=8$ 条边。\n\n测试套件：\n- 测试用例 1（一般重叠路径，单个种子）：\n  - 种子集：$S=\\{0\\}$，因此 $s_{\\min}=1$。\n  - 边激活概率：$p_{0,1}=0.3$, $p_{0,2}=0.6$, $p_{1,2}=0.5$, $p_{1,3}=0.4$, $p_{2,3}=0.7$, $p_{2,4}=0.2$, $p_{3,4}=0.9$, $p_{4,1}=0.1$。\n- 测试用例 2（边界情况，所有边几乎必然不激活）：\n  - 种子集：$S=\\{0\\}$，因此 $s_{\\min}=1$。\n  - 边激活概率：对于所有 $(i,j)\\in E$，$p_{i,j}=0$。\n- 测试用例 3（边界情况，所有边几乎必然激活）：\n  - 种子集：$S=\\{0\\}$，因此 $s_{\\min}=1$。\n  - 边激活概率：对于所有 $(i,j)\\in E$，$p_{i,j}=1$。\n- 测试用例 4（多个种子，一般概率）：\n  - 种子集：$S=\\{0,2\\}$，因此 $s_{\\min}=2$。\n  - 边激活概率：与测试用例1相同。\n\n算法要求：\n- 您的算法必须通过活边视角使用路径枚举来计算精确分布：将每条边 $(i,j)$ 视为独立地以概率 $p_{ij}$ 存活（live），以概率 $1-p_{ij}$ 死亡（dead），枚举所有子集 $F\\subseteq E$ 作为可能的活边实现，计算在 $(V,F)$ 中从 $S$ 出发的可达性，并根据边概率的乘积，聚合每个实现 $F$ 的概率质量。\n- 最终输出必须是单行，包含一个由四个列表组成的列表，每个测试用例一个，其中每个内部列表按升序给出级联规模 $s_{\\min},s_{\\min}+1,\\ldots,|V|$ 的概率，四舍五入到10位小数。\n\n您的程序必须是完整的、可运行的，并且不需要任何用户输入。每种情况的输出类型是浮点数列表。程序必须内部验证归一化，确保每个测试用例的概率总和在小的数值容差内为1。最终的打印必须严格按照指定格式生成一行。",
            "solution": "用户希望在一个小型网络中，根据独立级联 (IC) 模型计算最终级联规模的精确概率分布。这需要严格应用概率论和图论。\n\n### 第1步：提取给定信息\n- **模型**：有向图 $G=(V, E)$ 上的独立级联 (IC) 模型。\n- **网络结构**：\n    - 节点：$V=\\{0,1,2,3,4\\}$，因此 $|V|=5$。\n    - 有向边：$E = \\{(0,1), (0,2), (1,2), (1,3), (2,3), (2,4), (3,4), (4,1)\\}$，因此 $|E|=8$。\n- **扩散过程**：一个初始种子集 $S$ 在 $t=0$ 时被激活。在每一步中，新激活的节点尝试以给定的概率 $p_{ij}$ 激活其未激活的邻居。一条边 $(i,j)$ 只能用于一次激活尝试。\n- **算法要求**：通过枚举所有可能的活边子图来计算最终级联规模 $|A_\\infty|$ 的精确分布。最小级联规模为 $s_{\\min}=|S|$。\n- **测试用例**：\n    1. **种子集**：$S=\\{0\\}$。**概率**：$p_{0,1}=0.3$, $p_{0,2}=0.6$, $p_{1,2}=0.5$, $p_{1,3}=0.4$, $p_{2,3}=0.7$, $p_{2,4}=0.2$, $p_{3,4}=0.9$, $p_{4,1}=0.1$。\n    2. **种子集**：$S=\\{0\\}$。**概率**：对于所有 $(i,j)\\in E$，$p_{ij}=0$。\n    3. **种子集**：$S=\\{0\\}$。**概率**：对于所有 $(i,j)\\in E$，$p_{ij}=1$。\n    4. **种子集**：$S=\\{0,2\\}$。**概率**：与测试用例1相同。\n- **输出格式**：单行包含一个列表的列表，例如 `[[dist1], [dist2], [dist3], [dist4]]`。每个内部列表 `[dist_k]` 包含从 $s=s_{\\min}, s_{\\min}+1, \\ldots, |V|$ 的概率 $\\Pr(|A_\\infty|=s)$，四舍五入到10位小数。\n\n### 第2步：使用提取的信息进行验证\n根据指定标准对问题进行验证。\n- **科学基础**：独立级联模型是网络科学和复杂系统研究中的一个基础且被广泛研究的模型。“活边模型”视角是一种标准的、数学上等效的表述，便于进行精确分析。该问题牢固地建立在公认的科学原理之上。\n- **良态问题**：网络是有限的（$|V|=5, |E|=8$），并且每个测试用例的边激活概率集都已完全指定。目标是计算一个有限结果集（级联规模从 $s_{\\min}$ 到 $|V|$）上的离散概率分布。由于活边配置的数量是有限的（$2^{|E|}$），并且对于每种配置，级联规模是唯一确定的，因此存在唯一的概率分布。该问题是良态的。\n- **客观性**：问题使用精确的数学定义和客观的数值数据进行陈述。没有歧义、主观性或基于意见的语言。\n\n该问题没有表现出任何已识别的缺陷（例如，科学上不健全、不完整、不切实际等）。对于给定的网络规模（$2^8 = 256$ 个实现），枚举方法在计算上是可行的。\n\n### 第3步：结论与行动\n问题陈述是有效的。将开发并实现一个完整的解决方案。\n\n### 基于原则的算法设计\n\n解决这个问题的核心原则是，动态的、随时间展开的独立级联 (IC) 过程与“活边”图上的静态可达性问题是等价的。\n\n1.  **活边模型**：IC模型的随机过程可以被重新概念化如下：在扩散开始之前，自然为每条边 $(i, j) \\in E$“抛硬币”。边以其给定的概率 $p_{ij}$ 被宣告为**活边**，以概率 $1 - p_{ij}$ 被宣告为**死边**。所有边的这些结果都是独立的。这个单一的随机实验生成了一个子图 $G_F = (V, F)$ 的特定实现，其中 $F \\subseteq E$ 是所有活边的集合。一旦这个子图确定，信息就从种子集 $S$ 确定性地传播到所有可以从 $S$ 中任何节点通过完全由活边组成的路径到达的节点。最终的激活集 $A_\\infty$ 正是这个可达节点的集合。\n\n2.  **枚举所有可能性**：因为边是否为活边的事件是独立的，所以任何特定活边子图 $G_F = (V, F)$ 的概率是各个边结果概率的乘积：\n    $$ \\Pr(F) = \\left( \\prod_{(i,j) \\in F} p_{ij} \\right) \\left( \\prod_{(i,j) \\in E \\setminus F} (1 - p_{ij}) \\right) $$\n    可能的活边子图总数为 $2^{|E|}$。对于给定的网络， $|E|=8$，因此有 $2^8 = 256$ 个可能的结果（实现）。这个数字足够小，可以进行完全枚举。\n\n3.  **算法**：基于以上原则，计算精确级联规模分布的算法如下：\n    a. 初始化一个大小为 $|V| - s_{\\min} + 1$ 的数组 `size_distribution`，所有元素为零。该数组将存储从 $s_{\\min}$ 到 $|V|$ 的每个可能级联规模的聚合概率。此数组的索引 $k$ 对应于 $s_{\\min} + k$ 的级联规模。\n    b. 遍历边的每个可能子集 $F \\subseteq E$。这可以通过将一个整数计数器从 $0$ 循环到 $2^{|E|}-1$ 并使用其二进制表示来选择活边来高效实现。\n    c. 对于每个实现 $F$：\n        i.  使用上述公式计算此特定实现的概率 $\\Pr(F)$。\n        ii. 构造相应的活边图 $G_F = (V, F)$。\n        iii. 确定在 $G_F$ 中从种子集 $S$ 可达的节点集。这是一个标准的图可达性问题，可以使用广度优先搜索 (BFS) 或深度优先搜索 (DFS) 高效解决，从所有种子节点同时开始。\n        iv. 设可达集的大小为 $k = |A_\\infty|$。\n        v. 将概率 $\\Pr(F)$ 添加到 `size_distribution` 数组的相应条目中。具体来说，如果大小为 $k$，我们更新索引为 $k - s_{\\min}$ 的条目：`size_distribution[k - s_min] += Pr(F)`。\n    d. 遍历所有 $2^{|E|}$ 个实现后，`size_distribution` 数组将包含最终级联规模的完整概率分布。\n\n4.  **最终处理**：将得到的概率四舍五入到指定的精度（10位小数），为每个测试用例生成最终输出。内部检查概率总和是否为1.0（在浮点运算的小容差范围内），可作为算法正确性的验证。",
            "answer": "```python\nimport numpy as np\nfrom collections import deque\n\ndef solve():\n    \"\"\"\n    Main function to define the network, test cases, and compute the exact \n    cascade size distributions for the Independent Cascade model via live-edge enumeration.\n    \"\"\"\n\n    # 1. Define the network structure.\n    # Nodes are represented by integers from 0 to 4.\n    nodes = [0, 1, 2, 3, 4]\n    # Edges are given in a fixed order for deterministic enumeration.\n    edges = [(0, 1), (0, 2), (1, 2), (1, 3), (2, 3), (2, 4), (3, 4), (4, 1)]\n    num_nodes = len(nodes)\n    num_edges = len(edges)\n\n    # 2. Define the test cases as specified in the problem statement.\n    # Probabilities for Test Case 1 and 4.\n    p_case1 = {\n        (0, 1): 0.3, (0, 2): 0.6, (1, 2): 0.5, (1, 3): 0.4,\n        (2, 3): 0.7, (2, 4): 0.2, (3, 4): 0.9, (4, 1): 0.1\n    }\n    # Probabilities for Test Case 2 (all edges inactive).\n    p_case2 = {edge: 0.0 for edge in edges}\n    # Probabilities for Test Case 3 (all edges active).\n    p_case3 = {edge: 1.0 for edge in edges}\n\n    test_cases = [\n        {'S': {0}, 'p': p_case1},    # Test Case 1\n        {'S': {0}, 'p': p_case2},    # Test Case 2\n        {'S': {0}, 'p': p_case3},    # Test Case 3\n        {'S': {0, 2}, 'p': p_case1} # Test Case 4\n    ]\n\n    def compute_distribution(seed_set, prob_map):\n        \"\"\"\n        Computes the cascade size distribution for a single test case.\n        \"\"\"\n        s_min = len(seed_set)\n        # Initialize an array to store the probability mass for each cascade size.\n        # Index k corresponds to size s_min + k.\n        size_distribution = np.zeros(num_nodes - s_min + 1)\n\n        # Enumerate all 2^|E| live-edge graph realizations.\n        for i in range(1  num_edges):\n            live_adj = {node: [] for node in nodes}\n            realization_prob = 1.0\n\n            # For each realization, determine the live-edge graph and its probability.\n            for j in range(num_edges):\n                edge = edges[j]\n                edge_prob = prob_map[edge]\n\n                # The j-th bit of i determines if the j-th edge is live.\n                if (i >> j)  1:  # Edge is live.\n                    u, v = edge\n                    live_adj[u].append(v)\n                    realization_prob *= edge_prob\n                else:  # Edge is dead.\n                    realization_prob *= (1.0 - edge_prob)\n            \n            # If a realization has zero probability, it can be skipped.\n            if realization_prob == 0.0:\n                continue\n            \n            # Compute the cascade size for this live-edge graph using BFS.\n            queue = deque(seed_set)\n            reachable_nodes = set(seed_set)\n            \n            while queue:\n                u = queue.popleft()\n                for v in live_adj.get(u, []):\n                    if v not in reachable_nodes:\n                        reachable_nodes.add(v)\n                        queue.append(v)\n            \n            cascade_size = len(reachable_nodes)\n            \n            # Add the probability of this realization to the corresponding size bin.\n            if cascade_size >= s_min:\n                size_distribution[cascade_size - s_min] += realization_prob\n        \n        # Internal verification that probabilities sum to 1.\n        if not np.isclose(np.sum(size_distribution), 1.0):\n             # This assert will fail if there is a logical error or significant floating-point drift.\n             # For this problem's scale, drift should be negligible.\n             raise AssertionError(f\"Probabilities do not sum to 1. Sum is {np.sum(size_distribution)}\")\n        \n        return size_distribution\n\n    # 3. Process each test case and collect the results.\n    all_results = []\n    for case in test_cases:\n        seed_set = case['S']\n        prob_map = case['p']\n        \n        dist = compute_distribution(seed_set, prob_map)\n        \n        # Round probabilities to 10 decimal places and convert to a standard Python list.\n        rounded_dist = np.round(dist, 10).tolist()\n        all_results.append(rounded_dist)\n\n    # 4. Print the final output in the exact specified format.\n    # The str() representation of a Python list is '[elem1, elem2, ...]', which\n    # naturally creates the desired format when joining a list of lists.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "理解了信息传播的动力学后，一个自然的问题是如何有效地利用它。此练习将您带入影响力最大化这一核心课题，要求您在一个精心设计的网络上， analytically 对比简单的“高出度”种子选择策略与经典的贪心算法。通过精确计算两种策略下的期望影响力 ()，您将亲身体会到子模性（submodularity）和“收益递减”原则的实际意义，并理解为何基于模型特性的贪心算法远优于直观的启发式方法。",
            "id": "4283432",
            "problem": "考虑以下有向图，该图旨在比较朴素高出度种子选择与标准贪婪算法在独立级联（IC）模型下进行影响力最大化的效果。节点集为 $V=\\{a,b,h,o,t_{1},t_{2},t_{3},t_{4},t_{5},t_{6}\\}$。只有节点 $\\{a,b,h,o\\}$ 有出边（构成候选种子集），且所有边都指向目标节点 $\\{t_{1},\\dots,t_{6}\\}$。目标节点之间没有边，也没有任何边指向 $\\{a,b,h,o\\}$ 中的任何节点。带IC激活概率的有向边如下：\n- 从 $a$到 $t_{1},t_{2},t_{3}$，每条边的概率为 $0.9$。\n- 从 $b$到 $t_{4},t_{5},t_{6}$，每条边的概率为 $0.9$。\n- 从 $o$到 $t_{1},t_{2},t_{3},t_{4}$，每条边的概率为 $0.2$。\n- 从 $h$到 $t_{1},t_{2},t_{3},t_{4},t_{5},t_{6}$，每条边的概率为 $0.1$。\n\n我们采用独立级联（IC）模型：初始时，种子节点在时间 $t=0$ 是激活的；每个激活节点在下一个时间步有一次机会独立地激活其每个当前未激活的出度邻居，激活概率使用该边的激活概率；由于该图中第一步之后没有更多的边，过程随之终止。总影响力（传播范围）定义为过程终止时期望的激活节点总数，种子节点本身也计为激活。\n\n设种子预算为 $k=2$。定义两种种子选择策略：\n- 朴素高出度种子选择：在 $\\{a,b,h,o\\}$ 中选择出度最大的 $k$ 个节点，忽略边的概率。\n- 贪婪子模种子选择：一次迭代地添加一个种子，每次选择 $\\{a,b,h,o\\}$ 中能够最大化IC模型下期望传播范围边际增量的节点。\n\n仅使用IC模型的定义和规则以及标准概率法则，计算该图上贪婪子模种子选择策略的期望总影响力与朴素高出度策略的期望总影响力之间的差异。将你的最终答案表示为一个精确的、未经四舍五入的小数。",
            "solution": "该问题按如下方式进行验证：\n### 第1步：提取给定信息\n- 节点集：$V=\\{a,b,h,o,t_{1},t_{2},t_{3},t_{4},t_{5},t_{6}\\}$\n- 候选种子集：$S_{cand}=\\{a,b,h,o\\}$\n- 目标节点集：$T=\\{t_{1},t_{2},t_{3},t_{4},t_{5},t_{6}\\}$\n- 边结构与概率：\n  - 从 $a$ 到 $t_{1},t_{2},t_{3}$ 的边，概率 $p=0.9$。\n  - 从 $b$ 到 $t_{4},t_{5},t_{6}$ 的边，概率 $p=0.9$。\n  - 从 $o$ 到 $t_{1},t_{2},t_{3},t_{4}$ 的边，概率 $p=0.2$。\n  - 从 $h$ 到 $t_{1},t_{2},t_{3},t_{4},t_{5},t_{6}$ 的边，概率 $p=0.1$。\n- 模型：独立级联（IC）模型，激活过程仅一步。\n- 影响力（传播范围）：期望的激活节点总数（种子 + 新激活的节点）。\n- 种子预算：$k=2$。\n- 策略1（朴素）：从 $\\{a,b,h,o\\}$ 中选择出度最大的 $k=2$ 个节点。\n- 策略2（贪婪）：迭代选择 $k=2$ 个种子，每次选择都使期望传播范围的边际增量最大化。\n\n### 第2步：使用提取的信息进行验证\n根据验证标准对问题进行评估：\n- **科学依据：** 问题基于网络科学和信息传播领域中公认的独立级联（IC）模型和标准算法（子模最大化的贪婪算法）。这些概念是合理的。\n- **问题明确：** 提供了所有必要信息（图结构、概率、预算、算法定义）。目标——计算一个具体的数值差异——清晰明确。存在唯一解。\n- **客观性：** 问题以精确、正式的语言陈述，不含主观或基于观点的内容。\n- **完整性与一致性：** 问题是自洽的，没有矛盾。图结构简单且被完全描述。\n- **现实性：** 该问题是为阐述一个概念而构建的例子，这是标准的学术实践。概率和图结构在物理上并非不可能。\n\n### 第3步：结论与操作\n问题有效。将提供完整解答。\n\n对于给定的种子集 $S$，期望总影响力（传播范围）记为 $\\sigma(S)$，是种子数量与期望新激活节点数量之和。\n$$ \\sigma(S) = |S| + \\sum_{v \\in V \\setminus S} P(v \\text{ 被激活} | S) $$\n在这个特定的图结构中，只有 $T = \\{t_1, \\dots, t_6\\}$ 中的节点可以被新激活。目标节点 $t_i$ 被种子集 $S$ 激活的概率等于 $1$ 减去它不被 $S$ 中任何种子激活的概率。设 $N_{in}(t_i)$ 为 $t_i$ 的入度邻居集合。\n$$ P(t_i \\text{ 被激活} | S) = 1 - \\prod_{s \\in S \\cap N_{in}(t_i)} (1 - p_{s,t_i}) $$\n其中 $p_{s,t_i}$ 是从 $s$ 到 $t_i$ 的边上的激活概率。总期望影响力为：\n$$ \\sigma(S) = |S| + \\sum_{i=1}^{6} \\left( 1 - \\prod_{s \\in S \\cap N_{in}(t_i)} (1 - p_{s,t_i}) \\right) $$\n\n**1. 朴素高出度种子选择策略**\n首先，我们确定候选种子节点 $\\{a,b,h,o\\}$ 的出度：\n- $a$ 的出度：$\\text{deg}^+(a) = 3$ (到 $t_1, t_2, t_3$)。\n- $b$ 的出度：$\\text{deg}^+(b) = 3$ (到 $t_4, t_5, t_6$)。\n- $h$ 的出度：$\\text{deg}^+(h) = 6$ (到 $t_1, \\dots, t_6$)。\n- $o$ 的出度：$\\text{deg}^+(o) = 4$ (到 $t_1, \\dots, t_4$)。\n\n出度最高的两个节点是 $h$ (出度为6) 和 $o$ (出度为4)。因此，朴素种子集是 $S_{naive} = \\{h, o\\}$。预算为 $k=2$。\n期望影响力 $\\sigma(S_{naive})$ 计算如下：\n$$ \\sigma(S_{naive}) = |S_{naive}| + \\sum_{i=1}^{6} P(t_i \\text{ 被激活} | \\{h,o\\}) $$\n$|S_{naive}|=2$。我们来计算每个目标节点的激活概率：\n- 对于 $t_1, t_2, t_3$：这些节点同时是 $h$ (概率 $p=0.1$) 和 $o$ (概率 $p=0.2$) 的邻居。\n  $P(\\text{被激活}) = 1 - (1 - p_{h,t_i})(1 - p_{o,t_i}) = 1 - (1 - 0.1)(1 - 0.2) = 1 - (0.9)(0.8) = 1 - 0.72 = 0.28$。\n- 对于 $t_4$：这个节点也同时是 $h$ ($p=0.1$) 和 $o$ ($p=0.2$) 的邻居。\n  $P(\\text{被激活}) = 1 - (0.9)(0.8) = 0.28$。\n- 对于 $t_5, t_6$：这些节点仅是 $h$ (概率 $p=0.1$) 的邻居。\n  $P(\\text{被激活}) = p_{h,t_i} = 0.1$。\n\n对期望的新激活数求和：\n$$ \\sum_{i=1}^{6} P(t_i \\text{ 被激活}) = 3 \\times (0.28) + 1 \\times (0.28) + 2 \\times (0.1) = 4 \\times 0.28 + 0.2 = 1.12 + 0.2 = 1.32 $$\n朴素策略的总期望影响力为：\n$$ \\sigma(S_{naive}) = 2 + 1.32 = 3.32 $$\n\n**2. 贪婪子模种子选择策略**\n贪婪算法逐个选择种子。预算为 $k=2$，我们进行两次选择。\n\n**选择1：** 找到具有最高期望影响力的单个种子。\n$\\sigma(\\{s\\}) = 1 + \\sum_{t_i \\in T} p_{s,t_i}$。\n- $\\sigma(\\{a\\}) = 1 + 3 \\times 0.9 = 1 + 2.7 = 3.7$。\n- $\\sigma(\\{b\\}) = 1 + 3 \\times 0.9 = 1 + 2.7 = 3.7$。\n- $\\sigma(\\{h\\}) = 1 + 6 \\times 0.1 = 1 + 0.6 = 1.6$。\n- $\\sigma(\\{o\\}) = 1 + 4 \\times 0.2 = 1 + 0.8 = 1.8$。\n最大影响力为 $3.7$，由 $a$ 和 $b$ 同时达到。我们通过选择 $a$ 来打破平局。第一个种子是 $s_1 = a$。设当前种子集为 $S_1 = \\{a\\}$。\n\n**选择2：** 找到添加到 $S_1$ 时能提供最大边际影响力增益的节点。\n将节点 $s$ 添加到 $S_1$ 的边际增益为 $\\sigma(S_1 \\cup \\{s\\}) - \\sigma(S_1)$。我们对 $s \\in \\{b,h,o\\}$ 进行评估。\n- **将 $b$ 添加到 $\\{a\\}$**：$S=\\{a,b\\}$。$a$ 和 $b$ 的邻居是不相交的。\n  $\\sigma(\\{a,b\\}) = 2 + \\sum P(t_i \\text{ act.} | \\{a,b\\})$。\n  对于 $t_1,t_2,t_3$：$P(\\text{act.}) = p_{a,t_i} = 0.9$。\n  对于 $t_4,t_5,t_6$：$P(\\text{act.}) = p_{b,t_i} = 0.9$。\n  $\\sum P = 3 \\times 0.9 + 3 \\times 0.9 = 5.4$。\n  $\\sigma(\\{a,b\\}) = 2 + 5.4 = 7.4$。\n  来自 $b$ 的边际增益：$\\sigma(\\{a,b\\}) - \\sigma(\\{a\\}) = 7.4 - 3.7 = 3.7$。\n\n- **将 $h$ 添加到 $\\{a\\}$**：$S=\\{a,h\\}$。\n  $\\sigma(\\{a,h\\}) = 2 + \\sum P(t_i \\text{ act.} | \\{a,h\\})$。\n  对于 $t_1,t_2,t_3$：是 $a$ ($p=0.9$) 和 $h$ ($p=0.1$) 的邻居。\n  $P(\\text{act.}) = 1 - (1-0.9)(1-0.1) = 1 - (0.1)(0.9) = 1 - 0.09 = 0.91$。\n  对于 $t_4,t_5,t_6$：仅是 $h$ ($p=0.1$) 的邻居。$P(\\text{act.}) = 0.1$。\n  $\\sum P = 3 \\times 0.91 + 3 \\times 0.1 = 2.73 + 0.3 = 3.03$。\n  $\\sigma(\\{a,h\\}) = 2 + 3.03 = 5.03$。\n  来自 $h$ 的边际增益：$\\sigma(\\{a,h\\}) - \\sigma(\\{a\\}) = 5.03 - 3.7 = 1.33$。\n\n- **将 $o$ 添加到 $\\{a\\}$**：$S=\\{a,o\\}$。\n  $\\sigma(\\{a,o\\}) = 2 + \\sum P(t_i \\text{ act.} | \\{a,o\\})$。\n  对于 $t_1,t_2,t_3$：是 $a$ ($p=0.9$) 和 $o$ ($p=0.2$) 的邻居。\n  $P(\\text{act.}) = 1 - (1-0.9)(1-0.2) = 1 - (0.1)(0.8) = 1 - 0.08 = 0.92$。\n  对于 $t_4$：仅是 $o$ ($p=0.2$) 的邻居。$P(\\text{act.}) = 0.2$。\n  对于 $t_5,t_6$：没有来自 $\\{a,o\\}$ 的入边。$P(\\text{act.}) = 0$。\n  $\\sum P = 3 \\times 0.92 + 1 \\times 0.2 = 2.76 + 0.2 = 2.96$。\n  $\\sigma(\\{a,o\\}) = 2 + 2.96 = 4.96$。\n  来自 $o$ 的边际增益：$\\sigma(\\{a,o\\}) - \\sigma(\\{a\\}) = 4.96 - 3.7 = 1.26$。\n\n比较边际增益：$3.7$ (来自 $b$) $ 1.33$ (来自 $h$) $ 1.26$ (来自 $o$)。\n贪婪算法选择 $b$ 作为第二个种子。贪婪种子集为 $S_{greedy} = \\{a, b\\}$。\n贪婪策略的总期望影响力为 $\\sigma(S_{greedy}) = \\sigma(\\{a,b\\}) = 7.4$。\n\n**3. 期望影响力的差异**\n差异为贪婪策略的期望影响力减去朴素策略的期望影响力。\n$$ \\text{差异} = \\sigma(S_{greedy}) - \\sigma(S_{naive}) = 7.4 - 3.32 = 4.08 $$\n最终答案是一个精确的小数。",
            "answer": "$$ \\boxed{4.08} $$"
        },
        {
            "introduction": "前述的精确计算方法虽然有助于理解理论，但在真实的大规模网络中并不可行。本练习旨在将您从理论计算推向实际应用，要求您实现影响力最大化问题中广泛使用的可扩展算法。您将通过编写代码，使用蒙特卡洛（Monte Carlo）模拟来估计期望影响力，并实现“惰性贪心”（lazy-greedy）算法这一关键优化，它能显著减少计算量。完成这项练习 () 意味着您掌握了一套在真实世界数据上寻找影响力种子的实用计算工具。",
            "id": "4283386",
            "problem": "您接到的指令是实现一个程序，该程序在独立级联模型下为信息扩散构建一个种子节点的惰性贪心选择序列。该程序必须使用一个基于样本的预言机来估计边际增益。该任务以纯粹的数学和算法术语进行描述，基于子模最大化和网络扩散过程的理论，并应遵循以下规范。\n\n一个有向网络表示为一个有限有向图 $G = (V, E)$，其中 $V$ 是一个由 $0, 1, 2, \\dots, n-1$ 标记的有限节点集，而 $E \\subseteq V \\times V$ 是有向边的集合。每条边 $(u, v) \\in E$ 都被分配一个激活概率 $p_{uv} \\in [0, 1]$。在独立级联模型下，扩散在离散的时间步中展开；如果一个节点 $u$ 在时间 $t$ 新激活，它会在时间 $t+1$ 尝试激活其每个出邻居 $v$ 恰好一次，每次的成功概率为 $p_{uv}$，且各次尝试相互独立。一个初始激活的种子节点集 $S \\subseteq V$ 在时间 $t=0$ 启动该过程。期望扩散度 $f(S)$ 定义为到过程终止时变为激活状态的不同节点的期望数量。目标是使用一种惰性贪心算法构建一个包含 $k$ 个种子节点的序列，该算法依赖于对边际增益的基于样本的估计，在每一步选择节点 $u \\in V \\setminus S$，以最大化边际增益 $f(S \\cup \\{u\\}) - f(S)$。\n\n基本基础和要求：\n- 您必须使用独立级联模型和期望扩散度 $f(S)$ 的定义作为经过充分测试的基础。\n- 期望扩散度 $f(S)$ 必须通过基于样本的预言机进行估计，该预言机每次扩散度估计使用蒙特卡洛模拟，并进行恰好 $R$ 次独立试验。在每次试验中，模拟级联过程直到没有新的激活发生，并记录总激活数；然后计算 $R$ 次试验的算术平均值以估计 $f(S)$。\n- 实现惰性贪心选择：维护一个候选节点的优先队列，其中包含它们最后估计的边际增益和一个表明该增益是为哪个种子集大小计算的标记。在每次迭代中，弹出优先级最高的候选者；如果其估计值是过时的（即为较小编号的种子集计算的），则为当前种子集重新计算其边际增益并将其重新推入队列；如果该估计值是最新的，则接受它，将其添加到 $S$ 中，并进入下一次迭代。此过程必须持续到 $|S| = k$ 或直到没有候选节点为止。\n- 如果在选择时多个节点具有相等的估计边际增益，则通过选择具有最小标识符的节点来确定性地打破平局。\n- 对于 $k = 0$，输出必须是空种子集。\n- 所有节点都用从零开始的整数标记；所有输出都必须使用这些标签。\n\n您的程序必须实现上述逻辑，并为以下测试套件生成最终的种子集。对于所有测试用例，您必须使用指定的固定伪随机种子，以确保基于样本的预言机具有确定性行为。\n\n测试套件：\n1. 正常流程案例：\n   - 图 $G_1$，节点集 $V_1 = \\{0,1,2,3,4,5,6\\}$，其有向边和激活概率如下：\n     - $(0,1)$，概率 $p_{01} = 0.4$；$(0,2)$，概率 $p_{02} = 0.2$\n     - $(1,2)$，概率 $p_{12} = 0.4$\n     - $(2,0)$，概率 $p_{20} = 0.1$；$(2,3)$，概率 $p_{23} = 0.2$\n     - $(3,4)$，概率 $p_{34} = 0.3$\n     - $(4,5)$，概率 $p_{45} = 0.3$；$(4,6)$，概率 $p_{46} = 0.2$\n     - $(5,3)$，概率 $p_{53} = 0.3$\n   - 预算 $k = 2$，样本数 $R = 800$，伪随机种子 $s = 20251$。\n\n2. 在同一图 $G_1$ 上的边界条件 $k=0$：\n   - 预算 $k = 0$，样本数 $R = 800$，伪随机种子 $s = 20252$。\n\n3. 全零概率（无传播的边缘案例）：\n   - 图 $G_2$，节点集 $V_2 = \\{0,1,2,3,4\\}$，其有向边和激活概率如下：\n     - $(0,1)$，概率 $p_{01} = 0.0$；$(0,2)$，概率 $p_{02} = 0.0$\n     - $(1,2)$，概率 $p_{12} = 0.0$\n     - $(2,3)$，概率 $p_{23} = 0.0$\n     - $(3,4)$，概率 $p_{34} = 0.0$\n   - 预算 $k = 3$，样本数 $R = 200$，伪随机种子 $s = 20253$。\n\n4. 具有确定性激活的确定性星形图：\n   - 图 $G_3$，节点集 $V_3 = \\{0,1,2,3,4,5\\}$，其有向边和激活概率如下：\n     - $(0,1)$，概率 $p_{01} = 1.0$；$(0,2)$，概率 $p_{02} = 1.0$；$(0,3)$，概率 $p_{03} = 1.0$；$(0,4)$，概率 $p_{04} = 1.0$；$(0,5)$，概率 $p_{05} = 1.0$。\n   - 预算 $k = 1$，样本数 $R = 50$，伪随机种子 $s = 20254$。\n\n5. 不连通分量：\n   - 图 $G_4$，节点集 $V_4 = \\{0,1,2,3,4,5,6,7\\}$，其有向边和激活概率如下：\n     - 分量 A：$(0,1)$，概率 $p_{01} = 0.5$；$(1,2)$，概率 $p_{12} = 0.5$；$(2,3)$，概率 $p_{23} = 0.5$。\n     - 分量 B：$(4,5)$，概率 $p_{45} = 0.5$；$(5,6)$，概率 $p_{56} = 0.5$；$(6,7)$，概率 $p_{67} = 0.5$。\n   - 预算 $k = 2$，样本数 $R = 400$，伪随机种子 $s = 20255$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果（例如 $[result_1, result_2, result_3]$），其中每个 $result_i$ 本身是代表第 $i$ 个测试用例所选种子节点的整数列表。因此，输出必须是 $[[s_{1,1},\\dots,s_{1,k_1}],[s_{2,1},\\dots],[\\dots]]$ 形式的单行文本，不含任何额外文字。此问题中没有物理单位或角度；所有值都是纯粹的离散和无量纲的。",
            "solution": "该问题要求实现一个惰性贪心算法，以在有向网络中找到一个近乎最优的大小为 $k$ 的种子节点集，用于信息扩散，该过程由独立级联（IC）模型控制。目标是最大化期望扩散度 $f(S)$，即从种子集 $S$ 开始被激活的节点的期望数量。\n\n该问题定义明确，并基于成熟的子模函数最大化理论。对于独立级联模型，期望扩散函数 $f(S)$ 已知是单调的（对于任何 $A \\subseteq B \\subseteq V$，$f(A) \\le f(B)$）和子模的（对于任何 $A \\subseteq B \\subseteq V$ 和 $u \\in V \\setminus B$，边际增益满足 $f(A \\cup \\{u\\}) - f(A) \\ge f(B \\cup \\{u\\}) - f(B)$）。子模性形式化了收益递减的概念，是保证简单的贪心算法能提供一个解，其期望扩散度至少是最优解扩散度的 $(1 - 1/e)$ 倍的关键属性。\n\n惰性贪心算法是标准贪心算法的一个著名计算优化。标准贪心方法会在 $k$ 步中的每一步重新评估每个非种子节点的边际增益，这在计算上是昂贵的。惰性贪心方法利用了子模性这一属性：随着更多种子被添加到集合 $S$ 中，一个节点的边际增益只会减少。因此，一个对于小种子集具有高边际增益的节点很可能仍然是顶级竞争者。该算法维护一个按最后计算的边际增益排序的节点优先队列。在每一步，它只检查顶部的节点。如果其增益是为当前种子集计算的，那么它被证明是最佳选择。如果不是（即它是过时的），则重新评估其增益，并将其放回队列中。此过程重复进行，直到在队列顶部找到一个最新的节点。与标准贪心方法相比，这显著减少了扩散估计的次数。\n\n实现包含两个主要组件：一个用于估计 $f(S)$ 的基于样本的预言机，以及主要的惰性贪心选择逻辑。\n\n1.  **用于期望扩散估计的基于样本的预言机**：\n    $f(S)$ 的解析计算是 $\\#P$-难的。因此，我们使用蒙特卡洛模拟来估计它。预言机函数，我们称之为 $\\text{estimate\\_spread}(S, G, R)$，在图 $G$ 上从种子集 $S$ 开始，执行 $R$ 次独立的 IC 过程模拟。每次模拟过程如下：\n    -   用种子集 $S$ 初始化激活节点集 $\\mathcal{A}$。一个新激活节点的队列也用 $S$ 初始化。\n    -   该过程以离散步骤展开。在每个步骤中，每个新激活的节点都试图激活其未激活的出邻居。对于一个新激活的节点 $u$ 和其出邻居 $v$，以指定的概率 $p_{uv}$ 发生成功激活。这是通过从 $[0, 1)$ 上的均匀分布中抽取一个随机数并检查它是否小于 $p_{uv}$ 来模拟的。\n    -   一个步骤中所有新激活的节点被收集起来。该过程持续进行，直到出现没有新节点被激活的步骤为止。\n    -   模拟结束时 $\\mathcal{A}$ 中的节点总数是该单次试验的扩散度。\n    -   预言机返回从所有 $R$ 次试验中获得的扩散度值的算术平均值。使用固定的伪随机种子确保了这些概率模拟的结果是确定性和可复现的。\n\n2.  **惰性贪心选择算法**：\n    该算法按以下步骤选择一个大小为 $k$ 的种子集 $S$：\n    -   **基本情况**：如果 $k=0$，则返回一个空集。\n    -   **初始化**：种子集 $S$ 初始化为空。创建一个优先队列 $\\mathcal{Q}$ 来存储形式为 $(\\delta, u, i)$ 的元组，其中 $\\delta$ 是节点 $u$ 的估计边际增益，而 $i$ 是一个“标记”，表示该增益是在 $|S|=i$ 时计算的。队列按优先处理最大增益 $\\delta$ 的顺序排列。通过选择具有最小整数标签的节点 $u$ 来打破平局。对于 $S = \\emptyset$ 的初始状态，我们为每个节点 $u \\in V$ 计算初始边际增益，即 $f(\\{u\\}) - f(\\emptyset) = f(\\{u\\})$，因为 $f(\\emptyset)=0$。我们将 $(f(\\{u\\}), u, 0)$ 推入 $\\mathcal{Q}$ 中，对所有 $u \\in V$ 均如此。\n    -   **迭代选择**：算法从 $i=0$ 到 $k-1$ 进行迭代，以选择 $k$ 个种子。在每次迭代 $i$ 中：\n        1.  启动一个循环以找到当前种子集 $S$（大小为 $i$）的最佳节点。\n        2.  从 $\\mathcal{Q}$ 中提取顶部元素 $(\\delta, u, \\text{stamp})$。\n        3.  如果节点 $u$ 已经在 $S$ 中，则丢弃它，循环继续。\n        4.  检查标记。如果 $\\text{stamp} = i$，则增益估计是及时的。节点 $u$ 是最佳选择。它被添加到 $S$ 中，内循环终止，算法进入下一个主迭代（选择第 $i+1$ 个种子）。\n        5.  如果 $\\text{stamp}  i$，则增益估计是过时的。它是为较小的种子集计算的。必须重新评估 $u$ 相对于当前集 $S$ 的边际增益：$\\delta_{\\text{new}} = f(S \\cup \\{u\\}) - f(S)$。这需要两次调用预言机。然后将该节点以更新后的信息推回 $\\mathcal{Q}$ 中：$(\\delta_{\\text{new}}, u, i)$。内循环继续，从 $\\mathcal{Q}$ 中提取新的顶部元素。\n    -   **终止**：在选择了 $k$ 个种子后，返回最终的集合 $S$。\n\n实现将精确遵循此逻辑，为每个测试用例使用指定的参数来生成所需的种子集。图将表示为邻接表。优先队列将使用 Python 的 `heapq` 模块实现，存储 `(-gain, node_id, stamp)` 形式的元组，以模拟最大堆并正确处理平局。",
            "answer": "```python\nimport heapq\nimport numpy as np\nfrom collections import deque\n\ndef estimate_spread(seeds, graph, n_nodes, R, rng):\n    \"\"\"\n    Estimates the expected spread f(S) for a given seed set S using Monte Carlo simulation.\n    Args:\n        seeds (list or set): The initial set of active nodes.\n        graph (dict): Adjacency list representation of the graph.\n                      {u: [(v1, p1), (v2, p2), ...]}\n        n_nodes (int): The total number of nodes in the graph.\n        R (int): The number of Monte Carlo simulations to run.\n        rng (numpy.random.Generator): The random number generator.\n    Returns:\n        float: The estimated expected spread.\n    \"\"\"\n    if not seeds:\n        return 0.0\n\n    total_spread = 0\n    for _ in range(R):\n        active = set(seeds)\n        \n        # Use a deque for efficient BFS-like propagation\n        q = deque(seeds)\n        \n        while q:\n            u = q.popleft()\n            \n            if u not in graph:\n                continue\n\n            for v, p_uv in graph[u]:\n                if v not in active:\n                    if rng.random()  p_uv:\n                        active.add(v)\n                        q.append(v)\n        \n        total_spread += len(active)\n    \n    return total_spread / R\n\ndef lazy_greedy_selector(graph_edges, n_nodes, k, R, seed):\n    \"\"\"\n    Constructs a seed set of size k using the lazy-greedy algorithm.\n    Args:\n        graph_edges (list): List of tuples (u, v, p) representing directed edges.\n        n_nodes (int): The number of nodes in the graph.\n        k (int): The desired size of the seed set (budget).\n        R (int): The number of samples for spread estimation.\n        seed (int): The seed for the pseudorandom number generator.\n    Returns:\n        list: The selected seed set of size k.\n    \"\"\"\n    if k == 0:\n        return []\n\n    # Build graph as adjacency list\n    graph = {i: [] for i in range(n_nodes)}\n    for u, v, p in graph_edges:\n        graph[u].append((v, p))\n\n    rng = np.random.default_rng(seed)\n    \n    S = []\n    pq = [] # Max-heap simulated with negative gains in a min-heap\n    \n    # 1. Initial population of the priority queue\n    # The stamp '0' indicates the gain is computed w.r.t. S of size 0 (empty set)\n    for u in range(n_nodes):\n        # Marginal gain for an empty S is just f({u})\n        gain = estimate_spread({u}, graph, n_nodes, R, rng)\n        # Tie-breaking by smaller node_id is implicitly handled by tuple comparison\n        heapq.heappush(pq, (-gain, u, 0))\n\n    # 2. Iteratively select k seeds\n    for i in range(k):\n        # We need the spread of the current set S to calculate future marginal gains.\n        # This is computed only once per main iteration i if needed.\n        spread_S = -1.0 \n\n        while pq:\n            neg_gain, u_top, stamp = heapq.heappop(pq)\n            \n            if u_top in S:\n                continue\n\n            # If the stamp is up-to-date, this is our best node\n            if stamp == i:\n                S.append(u_top)\n                break\n            else: # stamp  i, so the gain estimate is stale\n                if spread_S  0: # Lazy computation of spread_S\n                    spread_S = estimate_spread(S, graph, n_nodes, R, rng)\n\n                # Recompute marginal gain w.r.t. the current S\n                spread_S_union_u = estimate_spread(S + [u_top], graph, n_nodes, R, rng)\n                new_gain = spread_S_union_u - spread_S\n                \n                # Push back with updated gain and stamp\n                new_stamp = i\n                heapq.heappush(pq, (-new_gain, u_top, new_stamp))\n    \n    return S\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final output.\n    \"\"\"\n    test_cases = [\n        # 1. Happy-path case\n        {\n            \"n_nodes\": 7,\n            \"edges\": [\n                (0, 1, 0.4), (0, 2, 0.2), (1, 2, 0.4), (2, 0, 0.1), \n                (2, 3, 0.2), (3, 4, 0.3), (4, 5, 0.3), (4, 6, 0.2), \n                (5, 3, 0.3)\n            ],\n            \"k\": 2, \"R\": 800, \"seed\": 20251\n        },\n        # 2. Boundary condition k=0\n        {\n            \"n_nodes\": 7,\n            \"edges\": [\n                (0, 1, 0.4), (0, 2, 0.2), (1, 2, 0.4), (2, 0, 0.1), \n                (2, 3, 0.2), (3, 4, 0.3), (4, 5, 0.3), (4, 6, 0.2), \n                (5, 3, 0.3)\n            ],\n            \"k\": 0, \"R\": 800, \"seed\": 20252\n        },\n        # 3. All-zero probabilities\n        {\n            \"n_nodes\": 5,\n            \"edges\": [\n                (0, 1, 0.0), (0, 2, 0.0), (1, 2, 0.0), (2, 3, 0.0), (3, 4, 0.0)\n            ],\n            \"k\": 3, \"R\": 200, \"seed\": 20253\n        },\n        # 4. Deterministic star\n        {\n            \"n_nodes\": 6,\n            \"edges\": [\n                (0, 1, 1.0), (0, 2, 1.0), (0, 3, 1.0), (0, 4, 1.0), (0, 5, 1.0)\n            ],\n            \"k\": 1, \"R\": 50, \"seed\": 20254\n        },\n        # 5. Disconnected components\n        {\n            \"n_nodes\": 8,\n            \"edges\": [\n                (0, 1, 0.5), (1, 2, 0.5), (2, 3, 0.5),\n                (4, 5, 0.5), (5, 6, 0.5), (6, 7, 0.5)\n            ],\n            \"k\": 2, \"R\": 400, \"seed\": 20255\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = lazy_greedy_selector(\n            case[\"edges\"],\n            case[\"n_nodes\"],\n            case[\"k\"],\n            case[\"R\"],\n            case[\"seed\"]\n        )\n        results.append(result)\n\n    # Format the final output string\n    # e.g., [[0, 4], [], [0, 1, 2], [0], [0, 4]]\n    output_str = \"[\" + \",\".join(map(str, results)) + \"]\"\n    output_str = output_str.replace(\" \", \"\") # Remove spaces for exact formatting\n    print(output_str)\n\nsolve()\n```"
        }
    ]
}