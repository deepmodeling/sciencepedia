{
    "hands_on_practices": [
        {
            "introduction": "A central goal in studying diffusion is influence maximization: identifying a small set of initial seeds to trigger the largest possible cascade. A tempting and intuitive heuristic is to simply choose the nodes with the highest number of connections. This practice challenges that intuition by presenting a carefully constructed scenario where you will compute the expected influence for different seed sets under the Independent Cascade (IC) model . By comparing a naive high-degree strategy to one based on a greedy, principled calculation, you will gain a hands-on appreciation for why influence is a more subtle quantity than simple degree centrality.",
            "id": "4283432",
            "problem": "Consider the following directed graph designed to compare naive high-degree seeding with the standard greedy algorithm for influence maximization under the Independent Cascade (IC) model. The node set is $V=\\{a,b,h,o,t_{1},t_{2},t_{3},t_{4},t_{5},t_{6}\\}$. Only the nodes $\\{a,b,h,o\\}$ have outgoing edges (the candidate seed set), and all edges point to the target nodes $\\{t_{1},\\dots,t_{6}\\}$. There are no edges among the target nodes and no edges into any of $\\{a,b,h,o\\}$. The directed edges with their IC activation probabilities are:\n- From $a$ to $t_{1},t_{2},t_{3}$ with probability $0.9$ each.\n- From $b$ to $t_{4},t_{5},t_{6}$ with probability $0.9$ each.\n- From $o$ to $t_{1},t_{2},t_{3},t_{4}$ with probability $0.2$ each.\n- From $h$ to $t_{1},t_{2},t_{3},t_{4},t_{5},t_{6}$ with probability $0.1$ each.\n\nWe adopt the Independent Cascade (IC) model: initially seeded nodes are active at time $t=0$; each active node gets a single chance to activate each currently inactive out-neighbor once in the next time step, independently, using the edge’s activation probability; the process then terminates because there are no edges beyond the first step in this graph. The total influence (spread) is defined as the expected number of active nodes at termination, counting seed nodes as active.\n\nLet the seed budget be $k=2$. Define two seeding strategies:\n- Naive high-degree seeding: choose the $k$ nodes in $\\{a,b,h,o\\}$ with largest out-degree, ignoring edge probabilities.\n- Greedy submodular seeding: iteratively add one seed at a time, each time selecting the node in $\\{a,b,h,o\\}$ that maximizes the marginal increase in expected spread under the IC model.\n\nUsing only the definitions and rules of the IC model and standard probability laws, compute the difference between the expected total influence of the greedy submodular seeding strategy and that of the naive high-degree strategy for this graph. Express your final answer as a single exact decimal number with no rounding.",
            "solution": "The problem is validated as follows:\n### Step 1: Extract Givens\n- Node set: $V=\\{a,b,h,o,t_{1},t_{2},t_{3},t_{4},t_{5},t_{6}\\}$\n- Candidate seed set: $S_{cand}=\\{a,b,h,o\\}$\n- Target node set: $T=\\{t_{1},t_{2},t_{3},t_{4},t_{5},t_{6}\\}$\n- Edge structure and probabilities:\n  - Edges from $a$ to $t_{1},t_{2},t_{3}$ with probability $p=0.9$ each.\n  - Edges from $b$ to $t_{4},t_{5},t_{6}$ with probability $p=0.9$ each.\n  - Edges from $o$ to $t_{1},t_{2},t_{3},t_{4}$ with probability $p=0.2$ each.\n  - Edges from $h$ to $t_{1},t_{2},t_{3},t_{4},t_{5},t_{6}$ with probability $p=0.1$ each.\n- Model: Independent Cascade (IC) model with one-step activation.\n- Influence (spread): Expected number of total active nodes (seeds + newly activated).\n- Seed budget: $k=2$.\n- Strategy 1 (Naive): Choose the $k=2$ nodes from $\\{a,b,h,o\\}$ with the largest out-degree.\n- Strategy 2 (Greedy): Iteratively choose $k=2$ seeds, where each choice maximizes the marginal increase in expected spread.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is evaluated against the validation criteria:\n- **Scientifically Grounded:** The problem is based on the well-established Independent Cascade (IC) model and standard algorithms (greedy algorithm for submodular maximization) from the field of network science and information diffusion. The concepts are sound.\n- **Well-Posed:** All necessary information (graph structure, probabilities, budget, algorithm definitions) is provided. The objective—to compute a specific numerical difference—is clear and unambiguous. A unique solution exists.\n- **Objective:** The problem is stated in precise, formal language without subjective or opinion-based content.\n- **Completeness and Consistency:** The problem is self-contained and free of contradictions. The graph structure is simple and fully described.\n- **Realism:** The problem is a constructed example to illustrate a concept, which is standard academic practice. The probabilities and graph structure are not physically impossible.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\nThe expected total influence (spread), denoted $\\sigma(S)$, for a given seed set $S$ is the sum of the number of seeds and the expected number of newly activated nodes.\n$$ \\sigma(S) = |S| + \\sum_{v \\in V \\setminus S} P(v \\text{ is activated} | S) $$\nIn this specific graph structure, only nodes in $T = \\{t_1, \\dots, t_6\\}$ can be newly activated. The probability that a target node $t_i$ is activated by the seed set $S$ is given by $1$ minus the probability that it is not activated by any seed in $S$. Let $N_{in}(t_i)$ be the set of in-neighbors of $t_i$.\n$$ P(t_i \\text{ is activated} | S) = 1 - \\prod_{s \\in S \\cap N_{in}(t_i)} (1 - p_{s,t_i}) $$\nwhere $p_{s,t_i}$ is the activation probability on the edge from $s$ to $t_i$. The total expected influence is:\n$$ \\sigma(S) = |S| + \\sum_{i=1}^{6} \\left( 1 - \\prod_{s \\in S \\cap N_{in}(t_i)} (1 - p_{s,t_i}) \\right) $$\n\n**1. Naive High-Degree Seeding Strategy**\nFirst, we determine the out-degrees for the candidate seed nodes $\\{a,b,h,o\\}$:\n- Out-degree of $a$: $\\text{deg}^+(a) = 3$ (to $t_1, t_2, t_3$).\n- Out-degree of $b$: $\\text{deg}^+(b) = 3$ (to $t_4, t_5, t_6$).\n- Out-degree of $h$: $\\text{deg}^+(h) = 6$ (to $t_1, \\dots, t_6$).\n- Out-degree of $o$: $\\text{deg}^+(o) = 4$ (to $t_1, \\dots, t_4$).\n\nThe two nodes with the highest out-degrees are $h$ (degree $6$) and $o$ (degree $4$). Therefore, the naive seed set is $S_{naive} = \\{h, o\\}$. The budget is $k=2$.\nThe expected influence $\\sigma(S_{naive})$ is calculated as:\n$$ \\sigma(S_{naive}) = |S_{naive}| + \\sum_{i=1}^{6} P(t_i \\text{ is activated} | \\{h,o\\}) $$\n$|S_{naive}|=2$. Let's calculate the activation probability for each target node:\n- For $t_1, t_2, t_3$: These are neighbors of both $h$ (with $p=0.1$) and $o$ (with $p=0.2$).\n  $P(\\text{activated}) = 1 - (1 - p_{h,t_i})(1 - p_{o,t_i}) = 1 - (1 - 0.1)(1 - 0.2) = 1 - (0.9)(0.8) = 1 - 0.72 = 0.28$.\n- For $t_4$: This node is also a neighbor of both $h$ ($p=0.1$) and $o$ ($p=0.2$).\n  $P(\\text{activated}) = 1 - (0.9)(0.8) = 0.28$.\n- For $t_5, t_6$: These are neighbors of only $h$ (with $p=0.1$).\n  $P(\\text{activated}) = p_{h,t_i} = 0.1$.\n\nSumming the expected new activations:\n$$ \\sum_{i=1}^{6} P(t_i \\text{ is activated}) = 3 \\times (0.28) + 1 \\times (0.28) + 2 \\times (0.1) = 4 \\times 0.28 + 0.2 = 1.12 + 0.2 = 1.32 $$\nThe total expected influence for the naive strategy is:\n$$ \\sigma(S_{naive}) = 2 + 1.32 = 3.32 $$\n\n**2. Greedy Submodular Seeding Strategy**\nThe greedy algorithm selects seeds one by one. With a budget of $k=2$, we perform two selections.\n\n**Selection 1:** Find the single seed with the highest expected influence.\n$\\sigma(\\{s\\}) = 1 + \\sum_{t_i \\in T} p_{s,t_i}$.\n- $\\sigma(\\{a\\}) = 1 + 3 \\times 0.9 = 1 + 2.7 = 3.7$.\n- $\\sigma(\\{b\\}) = 1 + 3 \\times 0.9 = 1 + 2.7 = 3.7$.\n- $\\sigma(\\{h\\}) = 1 + 6 \\times 0.1 = 1 + 0.6 = 1.6$.\n- $\\sigma(\\{o\\}) = 1 + 4 \\times 0.2 = 1 + 0.8 = 1.8$.\nThe maximum influence is $3.7$, achieved by both $a$ and $b$. We break the tie by choosing $a$. The first seed is $s_1 = a$. Let the current seed set be $S_1 = \\{a\\}$.\n\n**Selection 2:** Find the node that provides the maximum marginal gain in influence when added to $S_1$.\nThe marginal gain of adding node $s$ to $S_1$ is $\\sigma(S_1 \\cup \\{s\\}) - \\sigma(S_1)$. We evaluate this for $s \\in \\{b,h,o\\}$.\n- **Add $b$ to $\\{a\\}$**: $S=\\{a,b\\}$. The neighbors of $a$ and $b$ are disjoint.\n  $\\sigma(\\{a,b\\}) = 2 + \\sum P(t_i \\text{ act.} | \\{a,b\\})$.\n  For $t_1,t_2,t_3$: $P(\\text{act.}) = p_{a,t_i} = 0.9$.\n  For $t_4,t_5,t_6$: $P(\\text{act.}) = p_{b,t_i} = 0.9$.\n  $\\sum P = 3 \\times 0.9 + 3 \\times 0.9 = 5.4$.\n  $\\sigma(\\{a,b\\}) = 2 + 5.4 = 7.4$.\n  Marginal gain from $b$: $\\sigma(\\{a,b\\}) - \\sigma(\\{a\\}) = 7.4 - 3.7 = 3.7$.\n\n- **Add $h$ to $\\{a\\}$**: $S=\\{a,h\\}$.\n  $\\sigma(\\{a,h\\}) = 2 + \\sum P(t_i \\text{ act.} | \\{a,h\\})$.\n  For $t_1,t_2,t_3$: neighbors of $a$ ($p=0.9$) and $h$ ($p=0.1$).\n  $P(\\text{act.}) = 1 - (1-0.9)(1-0.1) = 1 - (0.1)(0.9) = 1 - 0.09 = 0.91$.\n  For $t_4,t_5,t_6$: neighbors of only $h$ ($p=0.1$). $P(\\text{act.}) = 0.1$.\n  $\\sum P = 3 \\times 0.91 + 3 \\times 0.1 = 2.73 + 0.3 = 3.03$.\n  $\\sigma(\\{a,h\\}) = 2 + 3.03 = 5.03$.\n  Marginal gain from $h$: $\\sigma(\\{a,h\\}) - \\sigma(\\{a\\}) = 5.03 - 3.7 = 1.33$.\n\n- **Add $o$ to $\\{a\\}$**: $S=\\{a,o\\}$.\n  $\\sigma(\\{a,o\\}) = 2 + \\sum P(t_i \\text{ act.} | \\{a,o\\})$.\n  For $t_1,t_2,t_3$: neighbors of $a$ ($p=0.9$) and $o$ ($p=0.2$).\n  $P(\\text{act.}) = 1 - (1-0.9)(1-0.2) = 1 - (0.1)(0.8) = 1 - 0.08 = 0.92$.\n  For $t_4$: neighbor of only $o$ ($p=0.2$). $P(\\text{act.}) = 0.2$.\n  For $t_5,t_6$: no incoming edges from $\\{a,o\\}$. $P(\\text{act.}) = 0$.\n  $\\sum P = 3 \\times 0.92 + 1 \\times 0.2 = 2.76 + 0.2 = 2.96$.\n  $\\sigma(\\{a,o\\}) = 2 + 2.96 = 4.96$.\n  Marginal gain from $o$: $\\sigma(\\{a,o\\}) - \\sigma(\\{a\\}) = 4.96 - 3.7 = 1.26$.\n\nComparing the marginal gains: $3.7$ (from $b$) $ 1.33$ (from $h$) $ 1.26$ (from $o$).\nThe greedy algorithm selects $b$ as the second seed. The greedy seed set is $S_{greedy} = \\{a, b\\}$.\nThe total expected influence for the greedy strategy is $\\sigma(S_{greedy}) = \\sigma(\\{a,b\\}) = 7.4$.\n\n**3. Difference in Expected Influence**\nThe difference is the expected influence of the greedy strategy minus the expected influence of the naive strategy.\n$$ \\text{Difference} = \\sigma(S_{greedy}) - \\sigma(S_{naive}) = 7.4 - 3.32 = 4.08 $$\nThe final answer is an exact decimal.",
            "answer": "$$ \\boxed{4.08} $$"
        },
        {
            "introduction": "While the expected spread provides a single, useful number, it hides the full range of possible outcomes in a stochastic cascade. To gain a deeper understanding, we must explore the entire probability distribution of the final cascade size. This exercise guides you through computing this exact distribution for a small network by implementing an algorithm based on the 'live-edge' model . This fundamental perspective, where the stochasticity is viewed as a static property of the edges, is a cornerstone of theoretical analysis and provides a powerful tool for understanding the IC model from first principles.",
            "id": "4283251",
            "problem": "Consider a directed network represented by a graph $G=(V,E)$ where $V$ is a finite set of nodes and $E$ is a finite set of directed edges. Each directed edge $(i,j)\\in E$ has an associated activation probability $p_{ij}\\in[0,1]$. We study information diffusion under the Independent Cascade (IC) model, defined as follows: start with an initial set of active nodes $S\\subseteq V$ at time $t=0$. At each subsequent discrete time step, any node $i$ that became active at time $t$ is given exactly one chance to activate each currently inactive out-neighbor $j$ via edge $(i,j)$, independently for different edges, and succeeds with probability $p_{ij}$. All activation attempts occur only once per edge, and once a node is activated it remains active forever. The process terminates when no new activations occur, yielding a final active set $A_\\infty\\subseteq V$. The final cascade size is $|A_\\infty|$.\n\nStarting from the above fundamental definitions of probability and the Independent Cascade model, derive a principled algorithm to compute the exact distribution of final cascade sizes by enumerating activation paths via the live-edge perspective. Then implement it for the specified small network and parameter sets. Your program must compute, for each test case below, the list of probabilities $\\left[\\Pr\\left(|A_\\infty|=s_{\\min}\\right),\\Pr\\left(|A_\\infty|=s_{\\min}+1\\right),\\ldots,\\Pr\\left(|A_\\infty|=|V|\\right)\\right]$, where $s_{\\min}=|S|$ is the minimum possible cascade size. All probabilities must be expressed as decimals (i.e., floating-point numbers), rounded to $10$ decimal places. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result1,result2,result3]$), where each $resultk$ is the list for test case $k$.\n\nNetwork specification:\n- Nodes: $V=\\{0,1,2,3,4\\}$, so $|V|=5$.\n- Directed edges $E$ (in fixed order for enumeration): $(0,1)$, $(0,2)$, $(1,2)$, $(1,3)$, $(2,3)$, $(2,4)$, $(3,4)$, $(4,1)$. There are $|E|=8$ edges.\n\nTest suite:\n- Test case $1$ (general overlapping paths, single seed):\n  - Seed set: $S=\\{0\\}$, so $s_{\\min}=1$.\n  - Edge activation probabilities: $p_{0,1}=0.3$, $p_{0,2}=0.6$, $p_{1,2}=0.5$, $p_{1,3}=0.4$, $p_{2,3}=0.7$, $p_{2,4}=0.2$, $p_{3,4}=0.9$, $p_{4,1}=0.1$.\n- Test case $2$ (boundary case, all edges inactive almost surely):\n  - Seed set: $S=\\{0\\}$, so $s_{\\min}=1$.\n  - Edge activation probabilities: $p_{i,j}=0$ for all $(i,j)\\in E$.\n- Test case $3$ (boundary case, all edges active almost surely):\n  - Seed set: $S=\\{0\\}$, so $s_{\\min}=1$.\n  - Edge activation probabilities: $p_{i,j}=1$ for all $(i,j)\\in E$.\n- Test case $4$ (multiple seeds, general probabilities):\n  - Seed set: $S=\\{0,2\\}$, so $s_{\\min}=2$.\n  - Edge activation probabilities: identical to test case $1$.\n\nAlgorithmic requirements:\n- Your algorithm must compute the exact distribution using path enumeration via the live-edge view: treat each edge $(i,j)$ as independently live with probability $p_{ij}$ and dead with probability $1-p_{ij}$, enumerate all subsets $F\\subseteq E$ as possible live-edge realizations, compute reachability from $S$ in $(V,F)$, and aggregate the probability mass of each realization $F$ according to the product of edge-wise probabilities.\n- The final output must be a single line containing a list of four lists, one per test case, where each inner list gives probabilities for cascade sizes $s_{\\min},s_{\\min}+1,\\ldots,|V|$ in ascending order, rounded to $10$ decimal places.\n\nYour program must be complete, runnable, and must not require any user input. The output types for each case are lists of floats. The program must internally verify normalization by ensuring that for each test case the probabilities sum to $1$ within a small numerical tolerance. The final print must produce exactly one line in the specified format.",
            "solution": "The user wants to compute the exact probability distribution of the final cascade size in a small network under the Independent Cascade (IC) model. This requires a rigorous application of probability theory and graph theory.\n\n### Step 1: Extract Givens\n- **Model**: The Independent Cascade (IC) model on a directed graph $G=(V, E)$.\n- **Network Structure**:\n    - Nodes: $V=\\{0,1,2,3,4\\}$, so $|V|=5$.\n    - Directed edges: $E = \\{(0,1), (0,2), (1,2), (1,3), (2,3), (2,4), (3,4), (4,1)\\}$, so $|E|=8$.\n- **Diffusion Process**: An initial seed set $S$ is active at $t=0$. At each step, newly activated nodes attempt to activate their inactive neighbors with a given probability $p_{ij}$. An edge $(i,j)$ can only be used for one activation attempt.\n- **Algorithmic Requirement**: Compute the exact distribution of the final cascade size $|A_\\infty|$ by enumerating all possible live-edge subgraphs. The minimum cascade size is $s_{\\min}=|S|$.\n- **Test Cases**:\n    1.  **Seed Set**: $S=\\{0\\}$. **Probabilities**: $p_{0,1}=0.3$, $p_{0,2}=0.6$, $p_{1,2}=0.5$, $p_{1,3}=0.4$, $p_{2,3}=0.7$, $p_{2,4}=0.2$, $p_{3,4}=0.9$, $p_{4,1}=0.1$.\n    2.  **Seed Set**: $S=\\{0\\}$. **Probabilities**: $p_{ij}=0$ for all $(i,j)\\in E$.\n    3.  **Seed Set**: $S=\\{0\\}$. **Probabilities**: $p_{ij}=1$ for all $(i,j)\\in E$.\n    4.  **Seed Set**: $S=\\{0,2\\}$. **Probabilities**: Same as Test Case 1.\n- **Output Format**: A single line containing a list of lists of probabilities, e.g., `[[dist1], [dist2], [dist3], [dist4]]`. Each inner list `[dist_k]` contains the probabilities $\\Pr(|A_\\infty|=s)$ for $s=s_{\\min}, s_{\\min}+1, \\ldots, |V|$, rounded to $10$ decimal places.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is subjected to validation against the specified criteria.\n- **Scientifically Grounded**: The Independent Cascade model is a fundamental and widely-studied model in network science and the study of complex systems. The \"live-edge model\" perspective is a standard, mathematically equivalent formulation that facilitates exact analysis. The problem is firmly grounded in established scientific principles.\n- **Well-Posed**: The network is finite ($|V|=5, |E|=8$), and the set of edge activation probabilities is fully specified for each test case. The objective is to compute a discrete probability distribution over a finite set of outcomes (cascade sizes $s_{\\min}$ to $|V|$). Since there are a finite number of live-edge configurations ($2^{|E|}$), and for each configuration, the cascade size is uniquely determined, a unique probability distribution exists. The problem is well-posed.\n- **Objective**: The problem is stated using precise mathematical definitions and objective numerical data. There is no ambiguity, subjectivity, or opinion-based language.\n\nThe problem does not exhibit any of the identified flaws (e.g., scientific unsoundness, incompleteness, unrealism, etc.). The enumeration approach is computationally feasible for the given network size ($2^8 = 256$ realizations).\n\n### Step 3: Verdict and Action\nThe problem statement is **valid**. A complete solution will be developed and implemented.\n\n### Principle-Based Algorithmic Design\n\nThe core principle for solving this problem is the equivalence between the dynamic, time-unfolding Independent Cascade (IC) process and a static reachability problem on a \"live-edge\" graph.\n\n1.  **The Live-Edge Model**: The stochastic process of the IC model can be re-conceptualized as follows: before the diffusion begins, nature \"flips a coin\" for each edge $(i, j) \\in E$. The edge is declared **live** with its given probability $p_{ij}$ and **dead** with probability $1 - p_{ij}$. These outcomes are independent for all edges. This single random experiment generates a particular realization of a subgraph $G_F = (V, F)$, where $F \\subseteq E$ is the set of all live edges. Once this subgraph is determined, the information spreads deterministically from the seed set $S$ to all nodes that are reachable from any node in $S$ through paths composed entirely of live edges. The final active set $A_\\infty$ is precisely this set of reachable nodes.\n\n2.  **Enumeration of All Possibilities**: Because the events of edges being live are independent, the probability of any specific live-edge subgraph $G_F = (V, F)$ is the product of the probabilities of the individual edge outcomes:\n    $$ \\Pr(F) = \\left( \\prod_{(i,j) \\in F} p_{ij} \\right) \\left( \\prod_{(i,j) \\in E \\setminus F} (1 - p_{ij}) \\right) $$\n    The total number of possible live-edge subgraphs is $2^{|E|}$. For the given network, $|E|=8$, so there are $2^8 = 256$ possible outcomes (realizations). This number is small enough to allow for complete enumeration.\n\n3.  **Algorithm**: Based on the principles above, the algorithm to compute the exact cascade size distribution is as follows:\n    a. Initialize an array, `size_distribution`, of size $|V| - s_{\\min} + 1$ to all zeros. This array will store the aggregated probability for each possible cascade size from $s_{\\min}$ to $|V|$. The index $k$ of this array corresponds to a cascade size of $s_{\\min} + k$.\n    b. Iterate through every possible subset of edges $F \\subseteq E$. This can be efficiently implemented by looping an integer counter from $0$ to $2^{|E|}-1$ and using its binary representation to select the live edges.\n    c. For each realization $F$:\n        i.  Calculate the probability of this specific realization, $\\Pr(F)$, using the formula above.\n        ii. Construct the corresponding live-edge graph $G_F = (V, F)$.\n        iii. Determine the set of nodes reachable from the seed set $S$ in $G_F$. This is a standard graph reachability problem that can be solved efficiently using Breadth-First Search (BFS) or Depth-First Search (DFS), starting from all seed nodes simultaneously.\n        iv. Let the size of the reachable set be $k = |A_\\infty|$.\n        v. Add the probability $\\Pr(F)$ to the appropriate entry in the `size_distribution` array. Specifically, if the size is $k$, we update the entry at index $k - s_{\\min}$: `size_distribution[k - s_min] += Pr(F)`.\n    d. After iterating through all $2^{|E|}$ realizations, the `size_distribution` array will contain the complete probability distribution of the final cascade size.\n\n4.  **Finalization**: The resulting probabilities are rounded to the specified precision ($10$ decimal places) to produce the final output for each test case. An internal check that the probabilities sum to $1.0$ (within a small tolerance for floating-point arithmetic) serves as a verification of the algorithm's correctness.",
            "answer": "```python\nimport numpy as np\nfrom collections import deque\n\ndef solve():\n    \"\"\"\n    Main function to define the network, test cases, and compute the exact \n    cascade size distributions for the Independent Cascade model via live-edge enumeration.\n    \"\"\"\n\n    # 1. Define the network structure.\n    # Nodes are represented by integers from 0 to 4.\n    nodes = [0, 1, 2, 3, 4]\n    # Edges are given in a fixed order for deterministic enumeration.\n    edges = [(0, 1), (0, 2), (1, 2), (1, 3), (2, 3), (2, 4), (3, 4), (4, 1)]\n    num_nodes = len(nodes)\n    num_edges = len(edges)\n\n    # 2. Define the test cases as specified in the problem statement.\n    # Probabilities for Test Case 1 and 4.\n    p_case1 = {\n        (0, 1): 0.3, (0, 2): 0.6, (1, 2): 0.5, (1, 3): 0.4,\n        (2, 3): 0.7, (2, 4): 0.2, (3, 4): 0.9, (4, 1): 0.1\n    }\n    # Probabilities for Test Case 2 (all edges inactive).\n    p_case2 = {edge: 0.0 for edge in edges}\n    # Probabilities for Test Case 3 (all edges active).\n    p_case3 = {edge: 1.0 for edge in edges}\n\n    test_cases = [\n        {'S': {0}, 'p': p_case1},    # Test Case 1\n        {'S': {0}, 'p': p_case2},    # Test Case 2\n        {'S': {0}, 'p': p_case3},    # Test Case 3\n        {'S': {0, 2}, 'p': p_case1} # Test Case 4\n    ]\n\n    def compute_distribution(seed_set, prob_map):\n        \"\"\"\n        Computes the cascade size distribution for a single test case.\n        \"\"\"\n        s_min = len(seed_set)\n        # Initialize an array to store the probability mass for each cascade size.\n        # Index k corresponds to size s_min + k.\n        size_distribution = np.zeros(num_nodes - s_min + 1)\n\n        # Enumerate all 2^|E| live-edge graph realizations.\n        for i in range(1  num_edges):\n            live_adj = {node: [] for node in nodes}\n            realization_prob = 1.0\n\n            # For each realization, determine the live-edge graph and its probability.\n            for j in range(num_edges):\n                edge = edges[j]\n                edge_prob = prob_map[edge]\n\n                # The j-th bit of i determines if the j-th edge is live.\n                if (i  j)  1:  # Edge is live.\n                    u, v = edge\n                    live_adj[u].append(v)\n                    realization_prob *= edge_prob\n                else:  # Edge is dead.\n                    realization_prob *= (1.0 - edge_prob)\n            \n            # If a realization has zero probability, it can be skipped.\n            if realization_prob == 0.0:\n                continue\n            \n            # Compute the cascade size for this live-edge graph using BFS.\n            queue = deque(seed_set)\n            reachable_nodes = set(seed_set)\n            \n            while queue:\n                u = queue.popleft()\n                for v in live_adj.get(u, []):\n                    if v not in reachable_nodes:\n                        reachable_nodes.add(v)\n                        queue.append(v)\n            \n            cascade_size = len(reachable_nodes)\n            \n            # Add the probability of this realization to the corresponding size bin.\n            if cascade_size = s_min:\n                size_distribution[cascade_size - s_min] += realization_prob\n        \n        # Internal verification that probabilities sum to 1.\n        if not np.isclose(np.sum(size_distribution), 1.0):\n             # This assert will fail if there is a logical error or significant floating-point drift.\n             # For this problem's scale, drift should be negligible.\n             raise AssertionError(f\"Probabilities do not sum to 1. Sum is {np.sum(size_distribution)}\")\n        \n        return size_distribution\n\n    # 3. Process each test case and collect the results.\n    all_results = []\n    for case in test_cases:\n        seed_set = case['S']\n        prob_map = case['p']\n        \n        dist = compute_distribution(seed_set, prob_map)\n        \n        # Round probabilities to 10 decimal places and convert to a standard Python list.\n        rounded_dist = np.round(dist, 10).tolist()\n        all_results.append(rounded_dist)\n\n    # 4. Print the final output in the exact specified format.\n    # The str() representation of a Python list is '[elem1, elem2, ...]', which\n    # naturally creates the desired format when joining a list of lists.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The methods for calculating exact spread are computationally explosive and impractical for real-world networks. This final practice brings you into the realm of scalable, approximate solutions, which are essential for any practical application. You will implement the lazy-greedy algorithm, a highly efficient industry-standard method for influence maximization that provides strong theoretical guarantees . By integrating a Monte Carlo simulation oracle to estimate marginal gains, you will build a complete, practical tool that demonstrates how to balance computational feasibility with near-optimal performance on large graphs.",
            "id": "4283386",
            "problem": "You are given a directive to implement a program that constructs a lazy-greedy selection sequence of seed nodes for information diffusion under the Independent Cascade model. The program must use a sample-based oracle to estimate marginal gains. The task is framed in purely mathematical and algorithmic terms, grounded in the theory of submodular maximization and diffusion processes on networks, and should adhere to the following specifications.\n\nA directed network is represented as a finite directed graph $G = (V, E)$, where $V$ is a finite set of nodes labeled by $0, 1, 2, \\dots, n-1$, and $E \\subseteq V \\times V$ is the set of directed edges. Each edge $(u, v) \\in E$ is assigned an activation probability $p_{uv} \\in [0, 1]$. Under the Independent Cascade model, diffusion unfolds in discrete time steps; if a node $u$ is newly active at time $t$, it attempts to activate each out-neighbor $v$ exactly once with independent success probability $p_{uv}$ at time $t+1$. An initially active set of seed nodes $S \\subseteq V$ initiates the process at time $t=0$. The expected spread $f(S)$ is defined as the expected number of distinct nodes that become active by termination of the process. The goal is to construct a sequence of $k$ seed nodes using a lazy-greedy algorithm that relies on sample-based estimates of marginal gains to select, at each step, the node $u \\in V \\setminus S$ that maximizes the marginal gain $f(S \\cup \\{u\\}) - f(S)$.\n\nFundamental base and requirements:\n- You must use the Independent Cascade model and the definition of expected spread $f(S)$ as a well-tested foundation.\n- The expected spread $f(S)$ must be estimated by a sample-based oracle using Monte Carlo simulation with exactly $R$ independent trials per spread estimation. In each trial, simulate the cascade process until no new activations occur and record the total activated count; then take the arithmetic mean over $R$ trials to estimate $f(S)$.\n- Implement lazy-greedy selection: maintain a priority queue of candidate nodes with their last estimated marginal gains and a stamp indicating for which seed set size they were computed. In each iteration, pop the highest-priority candidate; if its estimate is stale (computed for a smaller seed set size), recompute its marginal gain for the current seed set and push it back; if up-to-date, accept it, add it to $S$, and proceed to the next iteration. This procedure must continue until $|S| = k$ or until no candidates remain.\n- In case multiple nodes have equal estimated marginal gains at selection time, break ties deterministically by selecting the node with the smallest identifier.\n- For $k = 0$, the output must be the empty seed set.\n- All nodes are labeled with zero-based integers; all outputs must use these labels.\n\nYour program must implement the above logic and produce final seed sets for the following test suite. For all test cases, you must use a fixed pseudorandom seed as specified, to ensure deterministic behavior of the sample-based oracle.\n\nTest Suite:\n1. Happy-path case:\n   - Graph $G_1$ on $V_1 = \\{0,1,2,3,4,5,6\\}$ with directed edges and activation probabilities:\n     - $(0,1)$ with $p_{01} = 0.4$, $(0,2)$ with $p_{02} = 0.2$,\n     - $(1,2)$ with $p_{12} = 0.4$,\n     - $(2,0)$ with $p_{20} = 0.1$, $(2,3)$ with $p_{23} = 0.2$,\n     - $(3,4)$ with $p_{34} = 0.3$,\n     - $(4,5)$ with $p_{45} = 0.3$, $(4,6)$ with $p_{46} = 0.2$,\n     - $(5,3)$ with $p_{53} = 0.3$.\n   - Budget $k = 2$, samples $R = 800$, pseudorandom seed $s = 20251$.\n\n2. Boundary condition $k=0$ on the same graph $G_1$:\n   - Budget $k = 0$, samples $R = 800$, pseudorandom seed $s = 20252$.\n\n3. All-zero probabilities (edge-case of no propagation):\n   - Graph $G_2$ on $V_2 = \\{0,1,2,3,4\\}$ with directed edges and activation probabilities:\n     - $(0,1)$ with $p_{01} = 0.0$, $(0,2)$ with $p_{02} = 0.0$,\n     - $(1,2)$ with $p_{12} = 0.0$,\n     - $(2,3)$ with $p_{23} = 0.0$,\n     - $(3,4)$ with $p_{34} = 0.0$.\n   - Budget $k = 3$, samples $R = 200$, pseudorandom seed $s = 20253$.\n\n4. Deterministic star with certain activation:\n   - Graph $G_3$ on $V_3 = \\{0,1,2,3,4,5\\}$ with directed edges and activation probabilities:\n     - $(0,1)$ with $p_{01} = 1.0$, $(0,2)$ with $p_{02} = 1.0$, $(0,3)$ with $p_{03} = 1.0$, $(0,4)$ with $p_{04} = 1.0$, $(0,5)$ with $p_{05} = 1.0$.\n   - Budget $k = 1$, samples $R = 50$, pseudorandom seed $s = 20254$.\n\n5. Disconnected components:\n   - Graph $G_4$ on $V_4 = \\{0,1,2,3,4,5,6,7\\}$ with directed edges and activation probabilities:\n     - Component A: $(0,1)$ with $p_{01} = 0.5$, $(1,2)$ with $p_{12} = 0.5$, $(2,3)$ with $p_{23} = 0.5$.\n     - Component B: $(4,5)$ with $p_{45} = 0.5$, $(5,6)$ with $p_{56} = 0.5$, $(6,7)$ with $p_{67} = 0.5$.\n   - Budget $k = 2$, samples $R = 400$, pseudorandom seed $s = 20255$.\n\nFinal Output Format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[result_1, result_2, result_3]$), where each $result_i$ is itself a list of integers representing the selected seed nodes for the $i$-th test case. The output must therefore be a single line of the form $[[s_{1,1},\\dots,s_{1,k_1}],[s_{2,1},\\dots],[\\dots]]$ with no additional text. There are no physical units or angles in this problem; all values are purely discrete and dimensionless.",
            "solution": "The problem requires the implementation of a lazy-greedy algorithm to find a near-optimal set of $k$ seed nodes for information diffusion in a directed network, governed by the Independent Cascade (IC) model. The objective is to maximize the expected spread, $f(S)$, which is the expected number of nodes activated starting from a seed set $S$.\n\nThe problem is well-defined and grounded in the established theory of submodular function maximization. The expected spread function $f(S)$ for the IC model is known to be monotone ($f(A) \\le f(B)$ for any $A \\subseteq B \\subseteq V$) and submodular (for any $A \\subseteq B \\subseteq V$ and $u \\in V \\setminus B$, the marginal gain satisfies $f(A \\cup \\{u\\}) - f(A) \\ge f(B \\cup \\{u\\}) - f(B)$). Submodularity formalizes the notion of diminishing returns and is the key property that guarantees a simple greedy algorithm provides a solution with an expected spread of at least $(1 - 1/e)$ times the optimal solution's spread.\n\nThe lazy-greedy algorithm is a well-known computational optimization of the standard greedy algorithm. The standard greedy approach would, at each of the $k$ steps, re-evaluate the marginal gain of every non-seed node, which is computationally expensive. The lazy-greedy approach leverages the submodularity property: the marginal gain of a node can only decrease as more seeds are added to the set $S$. Thus, a node that had a high marginal gain for a small seed set is likely to remain a top contender. The algorithm maintains a priority queue of nodes ordered by their last computed marginal gains. At each step, it inspects only the top node. If its gain was computed for the current seed set, it is provably the best choice. If not (i.e., it is stale), its gain is re-evaluated, and it is placed back into the queue. This process repeats until an up-to-date node is found at the top of the queue. This significantly reduces the number of spread estimations compared to the standard greedy method.\n\nThe implementation consists of two primary components: a sample-based oracle to estimate $f(S)$ and the main lazy-greedy selection logic.\n\n1.  **Sample-Based Oracle for Expected Spread Estimation**:\n    The analytical calculation of $f(S)$ is $\\#P$-hard. Therefore, we use a Monte Carlo simulation to estimate it. The oracle function, let's call it $\\text{estimate\\_spread}(S, G, R)$, performs $R$ independent simulations of the IC process on the graph $G$ starting from the seed set $S$. Each simulation proceeds as follows:\n    -   Initialize the set of active nodes, $\\mathcal{A}$, with the seed set $S$. A queue of newly activated nodes is also initialized with $S$.\n    -   The process unfolds in discrete steps. In each step, every newly activated node attempts to activate its inactive out-neighbors. For a newly active node $u$ and its out-neighbor $v$, a successful activation occurs with the specified probability $p_{uv}$. This is simulated by drawing a random number from a uniform distribution on $[0, 1)$ and checking if it is less than $p_{uv}$.\n    -   All newly activated nodes in a step are collected. The process continues until a step occurs where no new nodes are activated.\n    -   The total number of nodes in $\\mathcal{A}$ at the end of the simulation is the spread for that single trial.\n    -   The oracle returns the arithmetic mean of the spread values obtained from all $R$ trials. The use of a fixed pseudorandom seed ensures that the outcomes of these probabilistic simulations are deterministic and reproducible.\n\n2.  **Lazy-Greedy Selection Algorithm**:\n    The algorithm proceeds as follows to select a seed set $S$ of size $k$:\n    -   **Base Case**: If $k=0$, an empty set is returned.\n    -   **Initialization**: The seed set $S$ is initialized as empty. A priority queue, $\\mathcal{Q}$, is created to store tuples of the form $(\\delta, u, i)$, where $\\delta$ is the estimated marginal gain of node $u$, and $i$ is the \"stamp\" indicating that this gain was calculated when $|S|=i$. The queue is ordered to prioritize the largest gain $\\delta$. Ties are broken by choosing the node $u$ with the smallest integer label. For the initial state where $S = \\emptyset$, we compute the initial marginal gain for each node $u \\in V$, which is simply $f(\\{u\\}) - f(\\emptyset) = f(\\{u\\})$, as $f(\\emptyset)=0$. We push $(f(\\{u\\}), u, 0)$ into $\\mathcal{Q}$ for all $u \\in V$.\n    -   **Iterative Selection**: The algorithm iterates from $i=0$ to $k-1$ to select $k$ seeds. In each iteration $i$:\n        1.  A loop is initiated to find the best node for the current seed set $S$ (of size $i$).\n        2.  The top element $(\\delta, u, \\text{stamp})$ is extracted from $\\mathcal{Q}$.\n        3.  If node $u$ is already in $S$, it is discarded, and the loop continues.\n        4.  The stamp is checked. If $\\text{stamp} = i$, the gain estimate is up-to-date. Node $u$ is the best choice. It is added to $S$, the inner loop terminates, and the algorithm proceeds to the next main iteration (for seed $i+1$).\n        5.  If $\\text{stamp}  i$, the gain estimate is stale. It was calculated for a smaller seed set. The marginal gain of $u$ with respect to the current set $S$ must be re-evaluated: $\\delta_{\\text{new}} = f(S \\cup \\{u\\}) - f(S)$. This requires two calls to the oracle. The node is then pushed back into $\\mathcal{Q}$ with the updated information: $(\\delta_{\\text{new}}, u, i)$. The inner loop continues, extracting the new top element from $\\mathcal{Q}$.\n    -   **Termination**: After $k$ seeds have been selected, the final set $S$ is returned.\n\nThe implementation will precisely follow this logic, using the specified parameters for each test case to generate the required seed sets. The graph will be represented as an adjacency list. The priority queue will be implemented using Python's `heapq` module, storing tuples of `(-gain, node_id, stamp)` to simulate a max-heap and handle tie-breaking correctly.",
            "answer": "```python\nimport heapq\nimport numpy as np\nfrom collections import deque\n\ndef estimate_spread(seeds, graph, n_nodes, R, rng):\n    \"\"\"\n    Estimates the expected spread f(S) for a given seed set S using Monte Carlo simulation.\n    Args:\n        seeds (list or set): The initial set of active nodes.\n        graph (dict): Adjacency list representation of the graph.\n                      {u: [(v1, p1), (v2, p2), ...]}\n        n_nodes (int): The total number of nodes in the graph.\n        R (int): The number of Monte Carlo simulations to run.\n        rng (numpy.random.Generator): The random number generator.\n    Returns:\n        float: The estimated expected spread.\n    \"\"\"\n    if not seeds:\n        return 0.0\n\n    total_spread = 0\n    for _ in range(R):\n        active = set(seeds)\n        \n        # Use a deque for efficient BFS-like propagation\n        q = deque(seeds)\n        \n        while q:\n            u = q.popleft()\n            \n            if u not in graph:\n                continue\n\n            for v, p_uv in graph[u]:\n                if v not in active:\n                    if rng.random()  p_uv:\n                        active.add(v)\n                        q.append(v)\n        \n        total_spread += len(active)\n    \n    return total_spread / R\n\ndef lazy_greedy_selector(graph_edges, n_nodes, k, R, seed):\n    \"\"\"\n    Constructs a seed set of size k using the lazy-greedy algorithm.\n    Args:\n        graph_edges (list): List of tuples (u, v, p) representing directed edges.\n        n_nodes (int): The number of nodes in the graph.\n        k (int): The desired size of the seed set (budget).\n        R (int): The number of samples for spread estimation.\n        seed (int): The seed for the pseudorandom number generator.\n    Returns:\n        list: The selected seed set of size k.\n    \"\"\"\n    if k == 0:\n        return []\n\n    # Build graph as adjacency list\n    graph = {i: [] for i in range(n_nodes)}\n    for u, v, p in graph_edges:\n        graph[u].append((v, p))\n\n    rng = np.random.default_rng(seed)\n    \n    S = []\n    pq = [] # Max-heap simulated with negative gains in a min-heap\n    \n    # 1. Initial population of the priority queue\n    # The stamp '0' indicates the gain is computed w.r.t. S of size 0 (empty set)\n    for u in range(n_nodes):\n        # Marginal gain for an empty S is just f({u})\n        gain = estimate_spread({u}, graph, n_nodes, R, rng)\n        # Tie-breaking by smaller node_id is implicitly handled by tuple comparison\n        heapq.heappush(pq, (-gain, u, 0))\n\n    # 2. Iteratively select k seeds\n    for i in range(k):\n        # We need the spread of the current set S to calculate future marginal gains.\n        # This is computed only once per main iteration i if needed.\n        spread_S = -1.0 \n\n        while pq:\n            neg_gain, u_top, stamp = heapq.heappop(pq)\n            \n            if u_top in S:\n                continue\n\n            # If the stamp is up-to-date, this is our best node\n            if stamp == i:\n                S.append(u_top)\n                break\n            else: # stamp  i, so the gain estimate is stale\n                if spread_S  0: # Lazy computation of spread_S\n                    spread_S = estimate_spread(S, graph, n_nodes, R, rng)\n\n                # Recompute marginal gain w.r.t. the current S\n                spread_S_union_u = estimate_spread(S + [u_top], graph, n_nodes, R, rng)\n                new_gain = spread_S_union_u - spread_S\n                \n                # Push back with updated gain and stamp\n                new_stamp = i\n                heapq.heappush(pq, (-new_gain, u_top, new_stamp))\n    \n    return S\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final output.\n    \"\"\"\n    test_cases = [\n        # 1. Happy-path case\n        {\n            \"n_nodes\": 7,\n            \"edges\": [\n                (0, 1, 0.4), (0, 2, 0.2), (1, 2, 0.4), (2, 0, 0.1), \n                (2, 3, 0.2), (3, 4, 0.3), (4, 5, 0.3), (4, 6, 0.2), \n                (5, 3, 0.3)\n            ],\n            \"k\": 2, \"R\": 800, \"seed\": 20251\n        },\n        # 2. Boundary condition k=0\n        {\n            \"n_nodes\": 7,\n            \"edges\": [\n                (0, 1, 0.4), (0, 2, 0.2), (1, 2, 0.4), (2, 0, 0.1), \n                (2, 3, 0.2), (3, 4, 0.3), (4, 5, 0.3), (4, 6, 0.2), \n                (5, 3, 0.3)\n            ],\n            \"k\": 0, \"R\": 800, \"seed\": 20252\n        },\n        # 3. All-zero probabilities\n        {\n            \"n_nodes\": 5,\n            \"edges\": [\n                (0, 1, 0.0), (0, 2, 0.0), (1, 2, 0.0), (2, 3, 0.0), (3, 4, 0.0)\n            ],\n            \"k\": 3, \"R\": 200, \"seed\": 20253\n        },\n        # 4. Deterministic star\n        {\n            \"n_nodes\": 6,\n            \"edges\": [\n                (0, 1, 1.0), (0, 2, 1.0), (0, 3, 1.0), (0, 4, 1.0), (0, 5, 1.0)\n            ],\n            \"k\": 1, \"R\": 50, \"seed\": 20254\n        },\n        # 5. Disconnected components\n        {\n            \"n_nodes\": 8,\n            \"edges\": [\n                (0, 1, 0.5), (1, 2, 0.5), (2, 3, 0.5),\n                (4, 5, 0.5), (5, 6, 0.5), (6, 7, 0.5)\n            ],\n            \"k\": 2, \"R\": 400, \"seed\": 20255\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = lazy_greedy_selector(\n            case[\"edges\"],\n            case[\"n_nodes\"],\n            case[\"k\"],\n            case[\"R\"],\n            case[\"seed\"]\n        )\n        results.append(result)\n\n    # Format the final output string\n    # e.g., [[0, 4], [], [0, 1, 2], [0], [0, 4]]\n    output_str = \"[\" + \",\".join(map(str, results)) + \"]\"\n    output_str = output_str.replace(\" \", \"\") # Remove spaces for exact formatting\n    print(output_str)\n\nsolve()\n```"
        }
    ]
}