{
    "hands_on_practices": [
        {
            "introduction": "To begin our hands-on exploration, we will investigate the theoretical foundation that makes spectral clustering such a powerful tool for community detection. This exercise connects the abstract parameters of the Stochastic Block Model (SBM) to the concrete spectral properties of network matrices, like the adjacency and Laplacian. By deriving the informative eigenvalues associated with the community structure, you will gain a first-principles understanding of how these matrices encode community information and why their eigenvectors can be used to partition a network .",
            "id": "4304747",
            "problem": "Consider a two-community Stochastic Block Model (SBM) with equal-sized blocks, where each of the $n$ vertices belongs to one of two communities of size $n/2$. Edges are placed independently with probabilities $p_{\\text{in}} = \\alpha/n$ for pairs within the same community and $p_{\\text{out}} = \\beta/n$ for pairs across communities, with parameters $\\alpha > \\beta > 0$ that do not depend on $n$. There are no self-loops.\n\nLet $A \\in \\mathbb{R}^{n \\times n}$ be the adjacency matrix, $D$ be the diagonal degree matrix, and $L_{\\text{sym}} = I - D^{-1/2} A D^{-1/2}$ be the symmetric normalized Laplacian. Define the community indicator vector $s \\in \\{-1,+1\\}^{n}$ with $s_{i} = +1$ if vertex $i$ is in community $1$ and $s_{i} = -1$ if it is in community $2$, so that $\\sum_{i=1}^{n} s_{i} = 0$.\n\nYou will use the following context-appropriate fundamental base:\n\n- The definition of the Stochastic Block Model (SBM) and independence of edges.\n- The definitions of the adjacency matrix $A$, degree matrix $D$, and normalized Laplacian $L_{\\text{sym}}$.\n- The mean-field approximation for sparse graphs: in the limit $n \\to \\infty$, the degrees concentrate around the expected degree and thus $D \\approx \\bar{c} I$, where $\\bar{c} = (\\alpha + \\beta)/2$ is the expected degree.\n\nWork in the limit $n \\to \\infty$ and assume the mean-field approximation $D \\approx \\bar{c} I$ holds. Starting from first principles and the above definitions (without invoking any shortcut formulas), derive:\n\n1) The informative eigenvalue $\\lambda_{\\text{sig}}(A)$ of the expected adjacency operator acting on the community vector $s$.\n\n2) The informative eigenvalue $\\mu_{\\text{sig}}(L_{\\text{sym}})$ of the expected normalized Laplacian acting on $s$.\n\nDefine the informative contrast for the normalized Laplacian as the deviation from $1$, namely $1 - \\mu_{\\text{sig}}(L_{\\text{sym}})$. Finally, compute the ratio\n$$\nR = \\frac{|\\lambda_{\\text{sig}}(A)|}{1 - \\mu_{\\text{sig}}(L_{\\text{sym}})}\n$$\nand provide $R$ as a closed-form analytic expression in terms of $\\alpha$ and $\\beta$.\n\nYour final answer must be a single analytic expression. No rounding is required.",
            "solution": "The problem is validated as self-contained, scientifically grounded, and well-posed. We may proceed with the solution.\n\nThe problem requires us to compute a ratio $R$ involving informative eigenvalues of the expected adjacency and normalized Laplacian operators for a two-community Stochastic Block Model (SBM). We will derive the required quantities step-by-step, starting from first principles as requested.\n\nFirst, we determine the structure of the expected adjacency matrix, which we denote by $\\bar{A} = \\mathbb{E}[A]$. The entry $\\bar{A}_{ij}$ for $i \\neq j$ is the probability of an edge between vertices $i$ and $j$. For $i=j$, $\\bar{A}_{ii}=0$ as there are no self-loops. The communities are of equal size, $n/2$.\nLet $C(i)$ denote the community of vertex $i$.\nThe edge probabilities are given as $p_{\\text{in}} = \\alpha/n$ for within-community pairs and $p_{\\text{out}} = \\beta/n$ for between-community pairs.\n$$\n\\bar{A}_{ij} = \\mathbb{E}[A_{ij}] = \\begin{cases} p_{\\text{in}} = \\alpha/n  \\text{if } C(i) = C(j) \\text{ and } i \\neq j \\\\ p_{\\text{out}} = \\beta/n  \\text{if } C(i) \\neq C(j) \\\\ 0  \\text{if } i=j \\end{cases}\n$$\n\nThe first task is to find the informative eigenvalue $\\lambda_{\\text{sig}}(A)$ associated with the action of $\\bar{A}$ on the community indicator vector $s \\in \\{-1, +1\\}^n$. We compute the $i$-th component of the vector $\\bar{A}s$, which is given by $(\\bar{A}s)_i = \\sum_{j=1}^{n} \\bar{A}_{ij} s_j$.\n\nLet's consider a vertex $i$ in community $1$, for which $s_i = +1$. The sum for $(\\bar{A}s)_i$ can be split over vertices in community $1$ and community $2$. There are $(n/2 - 1)$ other vertices in community $1$ and $n/2$ vertices in community $2$.\nFor $j \\in \\text{community } 1$ ($j \\neq i$), we have $s_j = +1$ and $\\bar{A}_{ij} = p_{\\text{in}}$.\nFor $j \\in \\text{community } 2$, we have $s_j = -1$ and $\\bar{A}_{ij} = p_{\\text{out}}$.\n$$\n(\\bar{A}s)_i = \\sum_{j \\in C(1), j \\neq i} p_{\\text{in}} s_j + \\sum_{j \\in C(2)} p_{\\text{out}} s_j = \\left(\\frac{n}{2}-1\\right) \\frac{\\alpha}{n} (+1) + \\left(\\frac{n}{2}\\right) \\frac{\\beta}{n} (-1)\n$$\n$$\n(\\bar{A}s)_i = \\frac{\\alpha}{2} - \\frac{\\alpha}{n} - \\frac{\\beta}{2} = \\left(\\frac{\\alpha - \\beta}{2} - \\frac{\\alpha}{n}\\right) s_i \\quad (\\text{since } s_i=+1)\n$$\nNow, consider a vertex $i$ in community $2$, for which $s_i = -1$.\nFor $j \\in \\text{community } 1$, we have $s_j = +1$ and $\\bar{A}_{ij} = p_{\\text{out}}$.\nFor $j \\in \\text{community } 2$ ($j \\neq i$), we have $s_j = -1$ and $\\bar{A}_{ij} = p_{\\text{in}}$.\n$$\n(\\bar{A}s)_i = \\sum_{j \\in C(1)} p_{\\text{out}} s_j + \\sum_{j \\in C(2), j \\neq i} p_{\\text{in}} s_j = \\left(\\frac{n}{2}\\right) \\frac{\\beta}{n} (+1) + \\left(\\frac{n}{2}-1\\right) \\frac{\\alpha}{n} (-1)\n$$\n$$\n(\\bar{A}s)_i = \\frac{\\beta}{2} - \\frac{\\alpha}{2} + \\frac{\\alpha}{n} = -\\left(\\frac{\\alpha - \\beta}{2} - \\frac{\\alpha}{n}\\right) = \\left(\\frac{\\alpha - \\beta}{2} - \\frac{\\alpha}{n}\\right) s_i \\quad (\\text{since } s_i=-1)\n$$\nIn both cases, we have $\\bar{A}s = \\left(\\frac{\\alpha - \\beta}{2} - \\frac{\\alpha}{n}\\right) s$. In the limit $n \\to \\infty$, the term $\\alpha/n$ vanishes. Therefore, $s$ is an eigenvector of $\\bar{A}$ with eigenvalue:\n$$\n\\lambda_{\\text{sig}}(A) = \\frac{\\alpha - \\beta}{2}\n$$\n\nNext, we find the informative eigenvalue $\\mu_{\\text{sig}}(L_{\\text{sym}})$ of the expected normalized Laplacian. The symmetric normalized Laplacian is $L_{\\text{sym}} = I - D^{-1/2} A D^{-1/2}$. The problem specifies using the mean-field approximation $D \\approx \\bar{c}I$ for large $n$, where $\\bar{c}$ is the expected degree. The expected degree for any vertex $i$ is:\n$$\n\\mathbb{E}[d_i] = \\sum_{j \\neq i} \\mathbb{E}[A_{ij}] = \\left(\\frac{n}{2}-1\\right)p_{\\text{in}} + \\frac{n}{2}p_{\\text{out}} = \\left(\\frac{n}{2}-1\\right)\\frac{\\alpha}{n} + \\frac{n}{2}\\frac{\\beta}{n}\n$$\nIn the limit $n \\to \\infty$, $\\mathbb{E}[d_i] \\to \\frac{\\alpha}{2} + \\frac{\\beta}{2}$. Thus, the average expected degree is $\\bar{c} = \\frac{\\alpha + \\beta}{2}$.\nUnder the specified approximation, the normalized Laplacian becomes:\n$$\nL_{\\text{sym}} \\approx I - (\\bar{c}I)^{-1/2} A (\\bar{c}I)^{-1/2} = I - \\frac{1}{\\bar{c}} A\n$$\nWe are interested in the expected operator, $\\bar{L}_{\\text{sym}} = \\mathbb{E}[L_{\\text{sym}}]$. Using the approximation:\n$$\n\\bar{L}_{\\text{sym}} \\approx \\mathbb{E}\\left[I - \\frac{1}{\\bar{c}} A\\right] = I - \\frac{1}{\\bar{c}} \\mathbb{E}[A] = I - \\frac{1}{\\bar{c}} \\bar{A}\n$$\nNow we find the eigenvalue of $\\bar{L}_{\\text{sym}}$ corresponding to the eigenvector $s$.\n$$\n\\bar{L}_{\\text{sym}} s = \\left(I - \\frac{1}{\\bar{c}} \\bar{A}\\right)s = Is - \\frac{1}{\\bar{c}}(\\bar{A}s)\n$$\nUsing our previous result that $\\bar{A}s = \\lambda_{\\text{sig}}(A) s$:\n$$\n\\bar{L}_{\\text{sym}} s = s - \\frac{1}{\\bar{c}} \\lambda_{\\text{sig}}(A) s = \\left(1 - \\frac{\\lambda_{\\text{sig}}(A)}{\\bar{c}}\\right)s\n$$\nThus, the informative eigenvalue of the expected normalized Laplacian is:\n$$\n\\mu_{\\text{sig}}(L_{\\text{sym}}) = 1 - \\frac{\\lambda_{\\text{sig}}(A)}{\\bar{c}}\n$$\nSubstituting the expressions for $\\lambda_{\\text{sig}}(A)$ and $\\bar{c}$:\n$$\n\\mu_{\\text{sig}}(L_{\\text{sym}}) = 1 - \\frac{(\\alpha - \\beta)/2}{(\\alpha + \\beta)/2} = 1 - \\frac{\\alpha - \\beta}{\\alpha + \\beta}\n$$\n\nFinally, we compute the ratio $R = \\frac{|\\lambda_{\\text{sig}}(A)|}{1 - \\mu_{\\text{sig}}(L_{\\text{sym}})}$.\nThe numerator is $|\\lambda_{\\text{sig}}(A)| = \\left|\\frac{\\alpha - \\beta}{2}\\right|$. Since it is given that $\\alpha > \\beta$, this is $\\frac{\\alpha - \\beta}{2}$.\nThe denominator is $1 - \\mu_{\\text{sig}}(L_{\\text{sym}})$, which we can calculate using our result for $\\mu_{\\text{sig}}(L_{\\text{sym}})$:\n$$\n1 - \\mu_{\\text{sig}}(L_{\\text{sym}}) = 1 - \\left(1 - \\frac{\\alpha - \\beta}{\\alpha + \\beta}\\right) = \\frac{\\alpha - \\beta}{\\alpha + \\beta}\n$$\nNow we compute the ratio $R$:\n$$\nR = \\frac{\\frac{\\alpha - \\beta}{2}}{\\frac{\\alpha - \\beta}{\\alpha + \\beta}} = \\frac{\\alpha - \\beta}{2} \\cdot \\frac{\\alpha + \\beta}{\\alpha - \\beta}\n$$\nThe term $(\\alpha - \\beta)$ cancels out, leaving:\n$$\nR = \\frac{\\alpha+\\beta}{2}\n$$\nThis expression is the closed-form analytic result for the ratio $R$ in terms of $\\alpha$ and $\\beta$.",
            "answer": "$$\n\\boxed{\\frac{\\alpha+\\beta}{2}}\n$$"
        },
        {
            "introduction": "Having established the theoretical link between SBM parameters and network spectra, we now turn to a more practical challenge: how do we estimate the model's parameters and uncover the latent community structure from observed network data? This exercise guides you through the derivation of the Expectation-Maximization (EM) algorithm, a cornerstone of statistical inference for latent variable models. By working through the M-step update equations, you will learn how to iteratively refine estimates for community mixing proportions and block connection probabilities, a fundamental skill for applying SBMs in the real world .",
            "id": "4304753",
            "problem": "Consider a symmetric Stochastic Block Model (SBM) with $2$ latent groups for a simple undirected graph on $n$ nodes with adjacency matrix $A = (A_{ij})_{1 \\leq i,j \\leq n}$, where $A_{ij} = A_{ji} \\in \\{0,1\\}$ and $A_{ii} = 0$. Each node $i$ has a latent group label $z_i \\in \\{1,2\\}$ drawn independently from a categorical distribution with mixing proportions $\\pi = (\\pi_1,\\pi_2)$, where $\\pi_a \\in (0,1)$ and $\\pi_1 + \\pi_2 = 1$. Conditional on $(z_i)_{i=1}^n$, edges are independent and for $ij$ we have $\\mathbb{P}(A_{ij} = 1 \\mid z_i = a, z_j = b) = p_{ab}$, where $p_{ab} \\in (0,1)$ and $p_{ab} = p_{ba}$ (symmetry).\n\nYou apply the Expectation-Maximization (EM) algorithm, where the E-step uses a mean-field variational family $q(z_1,\\ldots,z_n) = \\prod_{i=1}^n q_i(z_i)$ with responsibilities $r_{ia} = q_i(z_i = a)$ that satisfy $r_{i1} + r_{i2} = 1$ and $r_{ia} \\in (0,1)$ for all $i$ and $a \\in \\{1,2\\}$. The M-step maximizes the expected complete-data log-likelihood under $q$.\n\nStarting only from the foundational definitions of the complete-data likelihood for the SBM and the conditional independence structure described above, derive the closed-form M-step update formulas for the mixing proportions $\\pi_1, \\pi_2$ and the block connection probabilities $p_{11}, p_{22}, p_{12}$ in terms of $(A_{ij})$ and $(r_{ia})$. Assume there are no self-loops, the graph is undirected, and all denominators that appear are strictly positive.\n\nProvide your final result as analytic expressions. Express your answer as a single row matrix using the LaTeX $\\texttt{pmatrix}$ environment, listing in order\n$\\pi_1^{\\mathrm{new}}, \\pi_2^{\\mathrm{new}}, p_{11}^{\\mathrm{new}}, p_{22}^{\\mathrm{new}}, p_{12}^{\\mathrm{new}}$. Do not include units. No rounding is required.",
            "solution": "The M-step of the Expectation-Maximization (EM) algorithm maximizes the expected complete-data log-likelihood, $Q(\\theta|\\theta^{\\text{old}})$, with respect to the model parameters $\\theta = (\\pi, P)$, where $\\pi=(\\pi_1, \\pi_2)$ and $P=(p_{ab})$. The expectation is taken over the posterior distribution of the latent variables $z$, which is approximated by the variational distribution $q(z) = \\prod_i q_i(z_i)$ with responsibilities $r_{ia} = q_i(z_i=a)$.\n\nThe complete-data log-likelihood $\\mathcal{L}_c(\\theta; A, z)$ is given by the sum of the log-likelihood of the latent assignments and the log-likelihood of the graph given the assignments:\n$$\n\\mathcal{L}_c(\\theta; A, z) = \\log P(z|\\pi) + \\log P(A|z, P)\n$$\nwhere\n$$\n\\log P(z|\\pi) = \\sum_{i=1}^n \\sum_{a=1}^2 \\mathbb{I}(z_i=a) \\log \\pi_a\n$$\nand for an undirected graph with no self-loops ($ij$):\n$$\n\\log P(A|z, P) = \\sum_{1 \\le i  j \\le n} \\sum_{a=1}^2 \\sum_{b=1}^2 \\mathbb{I}(z_i=a, z_j=b) \\left[ A_{ij} \\log p_{ab} + (1-A_{ij}) \\log(1-p_{ab}) \\right]\n$$\n\nThe M-step objective function is $Q(\\theta|\\theta^{\\text{old}}) = \\mathbb{E}_{q(z)}[\\mathcal{L}_c(\\theta; A, z)]$. Using the factorization of $q(z)$ and the responsibilities $r_{ia}$, the expectation is:\n$$\nQ(\\theta|\\theta^{\\text{old}}) = \\sum_{i=1}^n \\sum_{a=1}^2 r_{ia} \\log \\pi_a + \\sum_{1 \\le i  j \\le n} \\sum_{a=1}^2 \\sum_{b=1}^2 r_{ia} r_{jb} \\left[ A_{ij} \\log p_{ab} + (1-A_{ij}) \\log(1-p_{ab}) \\right]\n$$\nWe maximize this function with respect to $\\pi_a$ and $p_{ab}$ separately.\n\n**1. Update for mixing proportions $\\pi_a$:**\nWe maximize $\\sum_{i=1}^n \\sum_{a=1}^2 r_{ia} \\log \\pi_a$ subject to the constraint $\\pi_1 + \\pi_2 = 1$. Using a Lagrange multiplier $\\lambda$, we optimize:\n$$\n\\mathcal{J}(\\pi, \\lambda) = \\sum_{i=1}^n (r_{i1} \\log \\pi_1 + r_{i2} \\log \\pi_2) + \\lambda(1 - \\pi_1 - \\pi_2)\n$$\nSetting the derivative with respect to $\\pi_a$ to zero:\n$$\n\\frac{\\partial \\mathcal{J}}{\\partial \\pi_a} = \\frac{\\sum_{i=1}^n r_{ia}}{\\pi_a} - \\lambda = 0 \\implies \\pi_a = \\frac{1}{\\lambda} \\sum_{i=1}^n r_{ia}\n$$\nSumming over $a$ and using the constraint $\\sum_a \\pi_a=1$:\n$$\n1 = \\sum_{a=1}^2 \\pi_a = \\frac{1}{\\lambda} \\sum_{a=1}^2 \\sum_{i=1}^n r_{ia} = \\frac{1}{\\lambda} \\sum_{i=1}^n (r_{i1}+r_{i2}) = \\frac{n}{\\lambda} \\implies \\lambda = n\n$$\nThus, the M-step update for $\\pi_a$ is:\n$$\n\\pi_a^{\\mathrm{new}} = \\frac{1}{n} \\sum_{i=1}^n r_{ia}\n$$\n\n**2. Update for block connection probabilities $p_{ab}$:**\nThe terms in $Q$ involving $p_{ab}$ are decoupled. For each $(a,b)$ pair, we maximize terms of the form:\n$$\nQ_{ab}(p_{ab}) = \\sum_{1 \\le i  j \\le n} r_{ia} r_{jb} \\left[ A_{ij} \\log p_{ab} + (1-A_{ij}) \\log(1-p_{ab}) \\right]\n$$\nSetting the derivative with respect to $p_{ab}$ to zero:\n$$\n\\frac{\\partial Q_{ab}}{\\partial p_{ab}} = \\sum_{1 \\le i  j \\le n} r_{ia} r_{jb} \\left[ \\frac{A_{ij}}{p_{ab}} - \\frac{1-A_{ij}}{1-p_{ab}} \\right] = 0\n$$\nThis yields:\n$$\np_{ab} \\sum_{1 \\le i  j \\le n} r_{ia} r_{jb} (1-A_{ij}) = (1-p_{ab}) \\sum_{1 \\le i  j \\le n} r_{ia} r_{jb} A_{ij}\n$$\n$$\np_{ab} \\left( \\sum_{1 \\le i  j \\le n} r_{ia} r_{jb} \\right) = \\sum_{1 \\le i  j \\le n} r_{ia} r_{jb} A_{ij}\n$$\nTherefore, the M-step update for $p_{ab}$ is:\n$$\np_{ab}^{\\mathrm{new}} = \\frac{\\sum_{1 \\le i  j \\le n} r_{ia} r_{jb} A_{ij}}{\\sum_{1 \\le i  j \\le n} r_{ia} r_{jb}}\n$$\nDue to symmetry ($p_{ab}=p_{ba}$), the update for $p_{12}$ involves pairs from both $(1,2)$ and $(2,1)$ communities:\n$$\np_{12}^{\\mathrm{new}} = \\frac{\\sum_{1 \\le i  j \\le n} (r_{i1} r_{j2} + r_{i2} r_{j1}) A_{ij}}{\\sum_{1 \\le i  j \\le n} (r_{i1} r_{j2} + r_{i2} r_{j1})}\n$$\nThe updates for $p_{11}$ and $p_{22}$ follow the general form directly.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{\\sum_{i=1}^n r_{i1}}{n}  \\frac{\\sum_{i=1}^n r_{i2}}{n}  \\frac{\\sum_{1 \\le i  j \\le n} r_{i1} r_{j1} A_{ij}}{\\sum_{1 \\le i  j \\le n} r_{i1} r_{j1}}  \\frac{\\sum_{1 \\le i  j \\le n} r_{i2} r_{j2} A_{ij}}{\\sum_{1 \\le i  j \\le n} r_{i2} r_{j2}}  \\frac{\\sum_{1 \\le i  j \\le n} (r_{i1} r_{j2} + r_{i2} r_{j1}) A_{ij}}{\\sum_{1 \\le i  j \\le n} (r_{i1} r_{j2} + r_{i2} r_{j1})}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "After learning how to fit an SBM to data, a crucial final step is to ask: is the SBM the right model for the job? In this exercise, we tackle the critical task of model selection by comparing the SBM against two other foundational generative models: the simple Erdős–Rényi model and the degree-heterogeneous Chung–Lu model. Using the Bayesian Information Criterion (BIC) on a small, concrete network, you will learn to quantitatively balance a model's goodness-of-fit with its complexity. This practice provides a clear, hands-on demonstration of how to justify the choice of a more complex model like the SBM in a principled manner .",
            "id": "4278496",
            "problem": "Consider a single observed, undirected, simple network on $n=6$ labeled nodes $\\{1,2,3,4,5,6\\}$ with edge set $\\{(1,2),(1,3),(2,3),(4,5),(5,6),(3,4),(2,5)\\}$. Let the node metadata define two blocks $A=\\{1,2,3\\}$ and $B=\\{4,5,6\\}$, which are to be treated as known and fixed. You will compare three generative models: the Erdős–Rényi (ER) random graph model (ER) with a single edge probability, the Chung–Lu expected-degree model (CL) with independent edges and node-specific expected degrees, and the Stochastic Block Model (SBM) with two blocks and three block-pair probabilities. For clarity, the acronyms are defined at first appearance: Bayesian Information Criterion (BIC), Erdős–Rényi (ER), Chung–Lu (CL), and Stochastic Block Model (SBM).\n\nStarting from the Bayesian model comparison principle that ranks models by the marginal likelihood $\\int L(\\boldsymbol{\\theta})\\pi(\\boldsymbol{\\theta})\\,d\\boldsymbol{\\theta}$, where $L(\\boldsymbol{\\theta})$ is the likelihood of the observed data under parameter vector $\\boldsymbol{\\theta}$ and $\\pi(\\boldsymbol{\\theta})$ is a regular prior density, use a second-order Laplace approximation around the maximum likelihood estimate to derive the large-sample criterion that penalizes model complexity by the number of free parameters and scales with the logarithm of the effective number of independent observations. Explain why, under independent-edge models for undirected simple graphs on $n$ nodes, the effective sample size is the number of dyads $N=\\binom{n}{2}$.\n\nThen, for the given network:\n\n- Specify the log-likelihoods for the Erdős–Rényi (ER), Chung–Lu (CL), and Stochastic Block Model (SBM) models assuming independent Bernoulli edges, and determine the maximum likelihood estimates for all model parameters under each model. For ER, use a single parameter $p$ for all dyads. For CL, use independent edges with probabilities $p_{ij}$ parameterized to match expected degrees to the observed degrees; justify a choice of $p_{ij}$ that achieves this. For the two-block SBM with known blocks $A$ and $B$, use parameters $p_{AA}$, $p_{BB}$, and $p_{AB}$ for within-$A$, within-$B$, and between-block probabilities, respectively.\n- Compute the Bayesian Information Criterion (BIC) for ER, CL, and SBM on this network, carefully stating your choices for the parameter count $k$ in each case and the effective sample size $N$.\n- Decide which model is preferred by BIC and justify the ranking.\n\nChoose the correct option:\n\nA. $BIC_{\\mathrm{SBM}}\\approx 21.48$, $BIC_{\\mathrm{ER}}\\approx 23.44$, $BIC_{\\mathrm{CL}}\\approx 34.43$; the Stochastic Block Model (SBM) is preferred.\n\nB. $BIC_{\\mathrm{ER}}\\approx 23.44$ is the smallest because the Erdős–Rényi (ER) model has the fewest parameters; $BIC_{\\mathrm{SBM}}\\approx 25.00$, $BIC_{\\mathrm{CL}}\\approx 34.43$.\n\nC. $BIC_{\\mathrm{CL}}\\approx 18.18$ is the smallest because the Chung–Lu (CL) model achieves the highest likelihood; $BIC_{\\mathrm{ER}}\\approx 23.44$, $BIC_{\\mathrm{SBM}}\\approx 21.48$.\n\nD. All three models have identical $BIC$ because they can match the network’s overall density; $BIC_{\\mathrm{ER}}=BIC_{\\mathrm{CL}}=BIC_{\\mathrm{SBM}}$.",
            "solution": "The Bayesian Information Criterion (BIC) arises as a large-sample approximation to the log marginal likelihood. The marginal likelihood of a model $\\mathcal{M}$ with parameter vector $\\boldsymbol{\\theta}\\in\\mathbb{R}^{k}$ and prior density $\\pi(\\boldsymbol{\\theta})$ is\n$$\n\\mathcal{Z} = \\int L(\\boldsymbol{\\theta}) \\, \\pi(\\boldsymbol{\\theta}) \\, d\\boldsymbol{\\theta},\n$$\nwhere $L(\\boldsymbol{\\theta})$ is the likelihood of the observed data under $\\boldsymbol{\\theta}$. Under standard regularity conditions, with $N$ independent observations, the log-likelihood admits a quadratic expansion around the maximum likelihood estimate (MLE) $\\hat{\\boldsymbol{\\theta}}$:\n$$\n\\log L(\\boldsymbol{\\theta}) \\approx \\log L(\\hat{\\boldsymbol{\\theta}}) - \\tfrac{1}{2} (\\boldsymbol{\\theta}-\\hat{\\boldsymbol{\\theta}})^\\top \\, \\mathcal{I}_N(\\hat{\\boldsymbol{\\theta}}) \\, (\\boldsymbol{\\theta}-\\hat{\\boldsymbol{\\theta}}),\n$$\nwhere $\\mathcal{I}_N(\\hat{\\boldsymbol{\\theta}})$ is the observed Fisher information, which scales linearly with $N$ (for independent observations, $\\mathcal{I}_N \\approx N \\, \\mathcal{I}_1$). Applying Laplace’s method,\n$$\n\\log \\mathcal{Z} \\approx \\log L(\\hat{\\boldsymbol{\\theta}}) + \\log \\pi(\\hat{\\boldsymbol{\\theta}}) + \\tfrac{k}{2}\\log(2\\pi) - \\tfrac{1}{2}\\log\\det \\mathcal{I}_N(\\hat{\\boldsymbol{\\theta}}).\n$$\nSince $\\det \\mathcal{I}_N \\approx N^{k}\\det \\mathcal{I}_1$, the leading $N$-dependent term is $-\\tfrac{k}{2}\\log N$. Up to constants that do not depend on $N$ (and thus cancel in model comparisons on the same dataset), this yields the Bayesian Information Criterion\n$$\n\\mathrm{BIC} = -2 \\log L(\\hat{\\boldsymbol{\\theta}}) + k \\log N,\n$$\nwhere $k$ is the number of free parameters and $N$ is the effective number of independent observations.\n\nFor undirected simple graphs with independent Bernoulli edges across dyads, the natural independent observational units are the unordered node pairs $(i,j)$, $ij$, so $N=\\binom{n}{2}$. Here, $n=6$, so $N=\\binom{6}{2}=15$.\n\nWe proceed to compute the log-likelihoods, MLEs, and BICs for each model.\n\nErdős–Rényi (ER). Model and MLE:\n- The ER model assumes each dyad $(i,j)$ is an independent Bernoulli($p$) with common probability $p\\in(0,1)$.\n- Let $m$ be the number of edges. From the observed edge set, $m=7$ and $N-m=8$ nonedges.\n- The log-likelihood is\n$$\n\\log L_{\\mathrm{ER}}(p) = m \\log p + (N-m)\\log(1-p).\n$$\n- The MLE is obtained by differentiating and setting to zero, yielding $\\hat{p}=\\tfrac{m}{N}=\\tfrac{7}{15}$.\n\nNumerical log-likelihood at $\\hat{p}$:\n- $\\log\\left(\\tfrac{7}{15}\\right) = \\log 7 - \\log 15 \\approx 1.945910 - 2.708050 = -0.762140$.\n- $\\log\\left(1-\\tfrac{7}{15}\\right) = \\log\\left(\\tfrac{8}{15}\\right)= \\log 8 - \\log 15 \\approx 2.079442 - 2.708050 = -0.628608$.\n- Thus $\\log L_{\\mathrm{ER}}(\\hat{p}) = 7(-0.762140) + 8(-0.628608) \\approx -10.363844$.\n- Then $-2\\log L_{\\mathrm{ER}}(\\hat{p}) \\approx 20.727688$.\n\nParameter count and BIC for ER:\n- The ER model has $k_{\\mathrm{ER}}=1$ free parameter ($p$).\n- With $N=15$ and $\\log N \\approx 2.708050$, the BIC is\n$$\n\\mathrm{BIC}_{\\mathrm{ER}} = -2\\log L_{\\mathrm{ER}}(\\hat{p}) + k_{\\mathrm{ER}}\\log N \\approx 20.727688 + 1\\times 2.708050 \\approx 23.435738.\n$$\n\nChung–Lu (CL). Model, parameterization, and MLE:\n- The CL model assumes independent Bernoulli edges with dyad-specific probabilities $p_{ij}\\in(0,1)$ that factorize through node-specific parameters so that expected degrees are matched. A widely used parameterization in the generalized random graph is\n$$\np_{ij} = \\frac{d_i d_j}{\\sum_{k=1}^{n} d_k} = \\frac{d_i d_j}{2m},\n$$\nwhere $d_i$ are the node-specific expected degrees. Under this choice, the expected degree satisfies\n$$\n\\mathbb{E}[d_i] = \\sum_{j\\neq i} p_{ij} = \\sum_{j\\neq i} \\frac{d_i d_j}{2m} = \\frac{d_i}{2m}\\sum_{j\\neq i} d_j = \\frac{d_i}{2m} \\left(\\sum_{j=1}^{n} d_j - d_i\\right) \\approx d_i,\n$$\nand exactly equals $d_i$ when we allow self-consistency with the observed degree sequence and work in sparse regimes where $p_{ij}\\ll 1$. In our network the degrees are $d_1=2$, $d_2=3$, $d_3=3$, $d_4=2$, $d_5=3$, $d_6=1$, with $\\sum_i d_i=2m=14$. Choosing $p_{ij}=\\tfrac{d_i d_j}{14}$ exactly matches the expected degrees to the observed degrees; this maximum-entropy construction coincides with the maximum likelihood solution for the exponential family that enforces expected degrees under independent edges.\n\nNumerical log-likelihood for CL:\n- For each dyad $(i,j)$, $\\log L_{\\mathrm{CL}} = \\sum_{ij} \\big[ A_{ij}\\log p_{ij} + (1-A_{ij}) \\log(1-p_{ij}) \\big]$, where $A_{ij}\\in\\{0,1\\}$ indicates edge presence.\n- Compute $p_{ij}$ using $p_{ij}=\\tfrac{d_i d_j}{14}$:\n  - $(1,2),(1,3),(4,5),(3,4)$: $p=\\tfrac{6}{14}=\\tfrac{3}{7}\\approx 0.428571$, so $\\log p\\approx -0.847298$, $\\log(1-p)=\\log(\\tfrac{4}{7})\\approx -0.559616$.\n  - $(2,3),(2,5)$: $p=\\tfrac{9}{14}\\approx 0.642857$, so $\\log p\\approx -0.441832$, $\\log(1-p)=\\log(\\tfrac{5}{14})\\approx -1.029619$.\n  - $(5,6)$: $p=\\tfrac{3}{14}\\approx 0.214286$, so $\\log p\\approx -1.540445$, $\\log(1-p)=\\log(\\tfrac{11}{14})\\approx -0.241162$.\n  - Remaining nonedges: $(1,4)$ has $p=\\tfrac{4}{14}=\\tfrac{2}{7}$, $\\log(1-p)=\\log(\\tfrac{5}{7})\\approx -0.336472$; $(1,6)$ and $(4,6)$ have $p=\\tfrac{2}{14}=\\tfrac{1}{7}$, $\\log(1-p)=\\log(\\tfrac{6}{7})\\approx -0.154151$; $(2,4)$ and $(1,5)$ have $p=\\tfrac{6}{14}$, $\\log(1-p)\\approx -0.559616$; $(2,6)$ and $(3,6)$ have $p=\\tfrac{3}{14}$, $\\log(1-p)\\approx -0.241162$; $(3,5)$ has $p=\\tfrac{9}{14}$, $\\log(1-p)\\approx -1.029619$.\n- Sum over edges: $4\\times(-0.847298) + 2\\times(-0.441832) + 1\\times(-1.540445) = -5.813301$.\n- Sum over nonedges: $-0.336472 - 0.559616 - 0.154151 - 0.559616 - 0.241162 - 1.029619 - 0.241162 - 0.154151 = -3.275949$.\n- Therefore $\\log L_{\\mathrm{CL}} \\approx -5.813301 + (-3.275949) = -9.089250$, and $-2\\log L_{\\mathrm{CL}} \\approx 18.178500$.\n\nParameter count and BIC for CL:\n- The CL model uses one parameter per node to control expected degree; a common and defensible choice for parameter count is $k_{\\mathrm{CL}}=n=6$ (the slight alternative $k_{\\mathrm{CL}}=n-1$ due to a normalization constraint changes the penalty by $\\log N$ but does not affect the qualitative ranking here).\n- With $N=15$, the BIC is\n$$\n\\mathrm{BIC}_{\\mathrm{CL}} = -2\\log L_{\\mathrm{CL}} + k_{\\mathrm{CL}} \\log N \\approx 18.178500 + 6\\times 2.708050 \\approx 34.426800.\n$$\n\nStochastic Block Model (SBM). Model, MLE, and log-likelihood:\n- With blocks $A=\\{1,2,3\\}$ and $B=\\{4,5,6\\}$ known, the SBM assumes independent Bernoulli edges with $p_{AA}$ for dyads within $A$, $p_{BB}$ for dyads within $B$, and $p_{AB}$ for dyads across blocks.\n- Count block-pair dyads and edges:\n  - Within $A$: there are $\\binom{3}{2}=3$ dyads, and all $3$ are edges, so $m_{AA}=3$ and $N_{AA}=3$.\n  - Within $B$: there are $\\binom{3}{2}=3$ dyads, with edges $(4,5)$ and $(5,6)$, so $m_{BB}=2$ and $N_{BB}=3$.\n  - Between $A$ and $B$: there are $3\\times 3=9$ dyads, with edges $(3,4)$ and $(2,5)$, so $m_{AB}=2$ and $N_{AB}=9$.\n- The block-pair Bernoulli log-likelihood is\n$$\n\\log L_{\\mathrm{SBM}}(p_{AA},p_{BB},p_{AB}) = m_{AA}\\log p_{AA} + (N_{AA}-m_{AA})\\log(1-p_{AA}) + m_{BB}\\log p_{BB} + (N_{BB}-m_{BB})\\log(1-p_{BB}) + m_{AB}\\log p_{AB} + (N_{AB}-m_{AB})\\log(1-p_{AB}).\n$$\n- The MLEs for Bernoulli probabilities in grouped independent trials are sample proportions: $\\hat{p}_{AA}=\\tfrac{m_{AA}}{N_{AA}}=\\tfrac{3}{3}=1$, $\\hat{p}_{BB}=\\tfrac{2}{3}$, $\\hat{p}_{AB}=\\tfrac{2}{9}$.\n\nNumerical log-likelihood at the MLE:\n- Within $A$: $m_{AA}=3$ edges with $\\log(1)=0$, no nonedges, contribution $0$.\n- Within $B$: $2\\log(\\tfrac{2}{3}) + 1\\log(\\tfrac{1}{3}) \\approx 2(-0.405465) + (-1.098612) = -1.909542$.\n- Between $A$ and $B$: $2\\log(\\tfrac{2}{9}) + 7\\log(\\tfrac{7}{9}) \\approx 2(-1.504078) + 7(-0.251315) = -3.008156 - 1.759205 = -4.767361$.\n- Therefore $\\log L_{\\mathrm{SBM}}(\\hat{p}_{AA},\\hat{p}_{BB},\\hat{p}_{AB}) \\approx -1.909542 - 4.767361 = -6.676903$, and $-2\\log L_{\\mathrm{SBM}} \\approx 13.353806$.\n\nParameter count and BIC for SBM:\n- With known block assignments, the model has $k_{\\mathrm{SBM}}=3$ free parameters ($p_{AA},p_{BB},p_{AB}$).\n- With $N=15$, the BIC is\n$$\n\\mathrm{BIC}_{\\mathrm{SBM}} = -2\\log L_{\\mathrm{SBM}} + k_{\\mathrm{SBM}} \\log N \\approx 13.353806 + 3\\times 2.708050 \\approx 21.477956.\n$$\n\nComparison and ranking:\n- $\\mathrm{BIC}_{\\mathrm{SBM}} \\approx 21.48$.\n- $\\mathrm{BIC}_{\\mathrm{ER}} \\approx 23.44$.\n- $\\mathrm{BIC}_{\\mathrm{CL}} \\approx 34.43$.\n\nThus the Stochastic Block Model (SBM) is preferred by BIC, followed by Erdős–Rényi (ER), with Chung–Lu (CL) worst due to its larger parameter penalty despite a higher maximized likelihood.\n\nOption-by-option analysis:\n- Option A states $BIC_{\\mathrm{SBM}}\\approx 21.48$, $BIC_{\\mathrm{ER}}\\approx 23.44$, $BIC_{\\mathrm{CL}}\\approx 34.43$; SBM preferred. These values and the ranking match the computations above. Verdict: Correct.\n- Option B claims ER has the smallest BIC because it has the fewest parameters and lists $BIC_{\\mathrm{ER}}\\approx 23.44$ as smallest, with $BIC_{\\mathrm{SBM}}\\approx 25.00$. This contradicts the computed $BIC_{\\mathrm{SBM}}\\approx 21.48$. Verdict: Incorrect.\n- Option C claims CL has the smallest BIC, using the fact that its maximized likelihood is highest ($-2\\log L_{\\mathrm{CL}}\\approx 18.18$), but it ignores the much larger penalty $k\\log N$ with $k=6$, which yields $BIC_{\\mathrm{CL}}\\approx 34.43$. Verdict: Incorrect.\n- Option D claims all three BICs are identical because they match overall density. This ignores model-specific likelihoods and parameter penalties; our computations show clear differences. Verdict: Incorrect.",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}