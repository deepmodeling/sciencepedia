{
    "hands_on_practices": [
        {
            "introduction": "The core challenge in applying the Stochastic Block Model to real-world data is that community assignments are latent or unobserved. This exercise takes you to the heart of the fitting procedure by deriving the update rules for the Expectation-Maximization (EM) algorithm, a cornerstone for statistical inference with latent variables. By working through the M-step, you will gain a foundational understanding of how SBM parameters are learned from data when group labels are unknown .",
            "id": "4304753",
            "problem": "Consider a symmetric Stochastic Block Model (SBM) with $2$ latent groups for a simple undirected graph on $n$ nodes with adjacency matrix $A = (A_{ij})_{1 \\leq i,j \\leq n}$, where $A_{ij} = A_{ji} \\in \\{0,1\\}$ and $A_{ii} = 0$. Each node $i$ has a latent group label $z_i \\in \\{1,2\\}$ drawn independently from a categorical distribution with mixing proportions $\\pi = (\\pi_1,\\pi_2)$, where $\\pi_a \\in (0,1)$ and $\\pi_1 + \\pi_2 = 1$. Conditional on $(z_i)_{i=1}^n$, edges are independent and for $i<j$ we have $\\mathbb{P}(A_{ij} = 1 \\mid z_i = a, z_j = b) = p_{ab}$, where $p_{ab} \\in (0,1)$ and $p_{ab} = p_{ba}$ (symmetry).\n\nYou apply the Expectation-Maximization (EM) algorithm, where the E-step uses a mean-field variational family $q(z_1,\\ldots,z_n) = \\prod_{i=1}^n q_i(z_i)$ with responsibilities $r_{ia} = q_i(z_i = a)$ that satisfy $r_{i1} + r_{i2} = 1$ and $r_{ia} \\in (0,1)$ for all $i$ and $a \\in \\{1,2\\}$. The M-step maximizes the expected complete-data log-likelihood under $q$.\n\nStarting only from the foundational definitions of the complete-data likelihood for the SBM and the conditional independence structure described above, derive the closed-form M-step update formulas for the mixing proportions $\\pi_1, \\pi_2$ and the block connection probabilities $p_{11}, p_{22}, p_{12}$ in terms of $(A_{ij})$ and $(r_{ia})$. Assume there are no self-loops, the graph is undirected, and all denominators that appear are strictly positive.\n\nProvide your final result as analytic expressions. Express your answer as a single row matrix using the LaTeX $\\texttt{pmatrix}$ environment, listing in order\n$\\pi_1^{\\mathrm{new}}, \\pi_2^{\\mathrm{new}}, p_{11}^{\\mathrm{new}}, p_{22}^{\\mathrm{new}}, p_{12}^{\\mathrm{new}}$. Do not include units. No rounding is required.",
            "solution": "The M-step of the EM algorithm maximizes the expected complete-data log-likelihood, $Q(\\theta|\\theta^{\\text{old}}) = \\mathbb{E}_{q(z)}[\\log P(A, z | \\theta)]$, with respect to the parameters $\\theta = (\\pi, P)$.\n\nFirst, we write the complete-data log-likelihood using indicator functions $I(z_i=a)$ for the latent assignments:\n$$\n\\log P(A, z | \\theta) = \\sum_{i=1}^n \\sum_{a=1}^2 I(z_i=a)\\log \\pi_a + \\sum_{1 \\le i  j \\le n} \\sum_{a=1}^2 \\sum_{b=1}^2 I(z_i=a, z_j=b) [A_{ij} \\log p_{ab} + (1-A_{ij}) \\log(1-p_{ab})]\n$$\nNext, we take the expectation with respect to the variational distribution $q(z) = \\prod_i q_i(z_i)$. Due to the mean-field assumption, $\\mathbb{E}_q[I(z_i=a)] = r_{ia}$ and $\\mathbb{E}_q[I(z_i=a, z_j=b)] = r_{ia}r_{jb}$ for $i \\neq j$. This gives the objective function $Q$:\n$$\nQ(\\theta|\\theta^{\\text{old}}) = \\sum_{i=1}^n \\sum_{a=1}^2 r_{ia}\\log \\pi_a + \\sum_{1 \\le i  j \\le n} \\sum_{a=1}^2 \\sum_{b=1}^2 r_{ia}r_{jb} [A_{ij} \\log p_{ab} + (1-A_{ij}) \\log(1-p_{ab})]\n$$\nWe maximize $Q$ for $\\pi$ and $P$ separately.\n\n**1. Maximization for mixing proportions $\\pi$:**\nWe maximize the first term, $\\sum_{i=1}^n \\sum_{a=1}^2 r_{ia}\\log \\pi_a$, subject to the constraint $\\sum_{a=1}^2 \\pi_a = 1$. Using a Lagrange multiplier $\\lambda$, the Lagrangian is:\n$$\n\\mathcal{L}(\\pi, \\lambda) = \\sum_{a=1}^2 \\left(\\sum_{i=1}^n r_{ia}\\right)\\log \\pi_a + \\lambda\\left(1 - \\sum_{a=1}^2 \\pi_a\\right)\n$$\nSetting the derivative with respect to $\\pi_a$ to zero gives $\\frac{\\sum_i r_{ia}}{\\pi_a} - \\lambda = 0$, so $\\pi_a = \\frac{\\sum_i r_{ia}}{\\lambda}$. Summing over $a$ and using the constraint, we find $\\lambda = \\sum_a \\sum_i r_{ia} = \\sum_i (\\sum_a r_{ia}) = \\sum_i 1 = n$. Thus, the update rules are:\n$$\n\\pi_1^{\\text{new}} = \\frac{\\sum_{i=1}^n r_{i1}}{n}, \\quad \\pi_2^{\\text{new}} = \\frac{\\sum_{i=1}^n r_{i2}}{n}\n$$\n\n**2. Maximization for block probabilities $P$:**\nThe optimization for each $p_{ab}$ is independent. We use the symmetry $p_{ab} = p_{ba}$.\nFor $p_{11}$, we maximize the part of $Q$ that depends on it:\n$$\n\\sum_{1 \\le i  j \\le n} r_{i1}r_{j1} [A_{ij} \\log p_{11} + (1-A_{ij}) \\log(1-p_{11})]\n$$\nThis is the log-likelihood for a Bernoulli parameter based on weighted observations. The maximum is the weighted average of successes:\n$$\np_{11}^{\\text{new}} = \\frac{\\sum_{1 \\le i  j \\le n} r_{i1}r_{j1} A_{ij}}{\\sum_{1 \\le i  j \\le n} r_{i1}r_{j1}}\n$$\nThe update for $p_{22}$ is analogous:\n$$\np_{22}^{\\text{new}} = \\frac{\\sum_{1 \\le i  j \\le n} r_{i2}r_{j2} A_{ij}}{\\sum_{1 \\le i  j \\le n} r_{i2}r_{j2}}\n$$\nFor $p_{12}$, we combine terms for $(a,b)=(1,2)$ and $(a,b)=(2,1)$:\n$$\n\\sum_{1 \\le i  j \\le n} (r_{i1}r_{j2} + r_{i2}r_{j1}) [A_{ij} \\log p_{12} + (1-A_{ij}) \\log(1-p_{12})]\n$$\nMaximizing this yields:\n$$\np_{12}^{\\text{new}} = \\frac{\\sum_{1 \\le i  j \\le n} (r_{i1}r_{j2} + r_{i2}r_{j1}) A_{ij}}{\\sum_{1 \\le i  j \\le n} (r_{i1}r_{j2} + r_{i2}r_{j1})}\n$$\nThese five expressions constitute the M-step update formulas.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{\\sum_{i=1}^n r_{i1}}{n}  \\frac{\\sum_{i=1}^n r_{i2}}{n}  \\frac{\\sum_{1 \\le i  j \\le n} r_{i1} r_{j1} A_{ij}}{\\sum_{1 \\le i  j \\le n} r_{i1} r_{j1}}  \\frac{\\sum_{1 \\le i  j \\le n} r_{i2} r_{j2} A_{ij}}{\\sum_{1 \\le i  j \\le n} r_{i2} r_{j2}}  \\frac{\\sum_{1 \\le i  j \\le n} (r_{i1} r_{j2} + r_{i2} r_{j1}) A_{ij}}{\\sum_{1 \\le i  j \\le n} (r_{i1} r_{j2} + r_{i2} r_{j1})}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "While the EM algorithm provides a general fitting procedure, spectral methods offer an elegant and computationally efficient alternative for community detection. This practice explores the theoretical underpinnings that explain why these methods are so effective for networks generated by an SBM. By analyzing the eigenvectors and eigenvalues of the expected adjacency and Laplacian matrices, you will uncover how the community structure is directly encoded in the graph's spectrum, providing a deep insight into the connection between a network's structure and its algebraic properties .",
            "id": "4304747",
            "problem": "Consider a two-community Stochastic Block Model (SBM) with equal-sized blocks, where each of the $n$ vertices belongs to one of two communities of size $n/2$. Edges are placed independently with probabilities $p_{\\text{in}} = \\alpha/n$ for pairs within the same community and $p_{\\text{out}} = \\beta/n$ for pairs across communities, with parameters $\\alpha  \\beta  0$ that do not depend on $n$. There are no self-loops.\n\nLet $A \\in \\mathbb{R}^{n \\times n}$ be the adjacency matrix, $D$ be the diagonal degree matrix, and $L_{\\text{sym}} = I - D^{-1/2} A D^{-1/2}$ be the symmetric normalized Laplacian. Define the community indicator vector $s \\in \\{-1,+1\\}^{n}$ with $s_{i} = +1$ if vertex $i$ is in community $1$ and $s_{i} = -1$ if it is in community $2$, so that $\\sum_{i=1}^{n} s_{i} = 0$.\n\nYou will use the following context-appropriate fundamental base:\n\n- The definition of the Stochastic Block Model (SBM) and independence of edges.\n- The definitions of the adjacency matrix $A$, degree matrix $D$, and normalized Laplacian $L_{\\text{sym}}$.\n- The mean-field approximation for sparse graphs: in the limit $n \\to \\infty$, the degrees concentrate around the expected degree and thus $D \\approx \\bar{c} I$, where $\\bar{c} = (\\alpha + \\beta)/2$ is the expected degree.\n\nWork in the limit $n \\to \\infty$ and assume the mean-field approximation $D \\approx \\bar{c} I$ holds. Starting from first principles and the above definitions (without invoking any shortcut formulas), derive:\n\n1) The informative eigenvalue $\\lambda_{\\text{sig}}(A)$ of the expected adjacency operator acting on the community vector $s$.\n\n2) The informative eigenvalue $\\mu_{\\text{sig}}(L_{\\text{sym}})$ of the expected normalized Laplacian acting on $s$.\n\nDefine the informative contrast for the normalized Laplacian as the deviation from $1$, namely $1 - \\mu_{\\text{sig}}(L_{\\text{sym}})$. Finally, compute the ratio\n$$\nR \\;=\\; \\frac{|\\lambda_{\\text{sig}}(A)|}{\\,1 - \\mu_{\\text{sig}}(L_{\\text{sym}})\\,}\n$$\nand provide $R$ as a closed-form analytic expression in terms of $\\alpha$ and $\\beta$.\n\nYour final answer must be a single analytic expression. No rounding is required.",
            "solution": "The problem is validated as self-contained, scientifically grounded, and well-posed. We may proceed with the solution.\n\nThe problem requires us to compute a ratio $R$ involving informative eigenvalues of the expected adjacency and normalized Laplacian operators for a two-community Stochastic Block Model (SBM). We will derive the required quantities step-by-step, starting from first principles as requested.\n\nFirst, we determine the structure of the expected adjacency matrix, which we denote by $\\bar{A} = \\mathbb{E}[A]$. The entry $\\bar{A}_{ij}$ for $i \\neq j$ is the probability of an edge between vertices $i$ and $j$. For $i=j$, $\\bar{A}_{ii}=0$ as there are no self-loops. The communities are of equal size, $n/2$.\nLet $C(i)$ denote the community of vertex $i$.\nThe edge probabilities are given as $p_{\\text{in}} = \\alpha/n$ for within-community pairs and $p_{\\text{out}} = \\beta/n$ for between-community pairs.\n$$\n\\bar{A}_{ij} = \\mathbb{E}[A_{ij}] = \\begin{cases} p_{\\text{in}} = \\alpha/n  \\text{if } C(i) = C(j) \\text{ and } i \\neq j \\\\ p_{\\text{out}} = \\beta/n  \\text{if } C(i) \\neq C(j) \\\\ 0  \\text{if } i=j \\end{cases}\n$$\n\nThe first task is to find the informative eigenvalue $\\lambda_{\\text{sig}}(A)$ associated with the action of $\\bar{A}$ on the community indicator vector $s \\in \\{-1, +1\\}^n$. We compute the $i$-th component of the vector $\\bar{A}s$, which is given by $(\\bar{A}s)_i = \\sum_{j=1}^{n} \\bar{A}_{ij} s_j$.\n\nLet's consider a vertex $i$ in community $1$, for which $s_i = +1$. The sum for $(\\bar{A}s)_i$ can be split over vertices in community $1$ and community $2$. There are $(n/2 - 1)$ other vertices in community $1$ and $n/2$ vertices in community $2$.\nFor $j \\in \\text{community } 1$ ($j \\neq i$), we have $s_j = +1$ and $\\bar{A}_{ij} = p_{\\text{in}}$.\nFor $j \\in \\text{community } 2$, we have $s_j = -1$ and $\\bar{A}_{ij} = p_{\\text{out}}$.\n$$\n(\\bar{A}s)_i = \\sum_{j \\in C(1), j \\neq i} p_{\\text{in}} s_j + \\sum_{j \\in C(2)} p_{\\text{out}} s_j = \\left(\\frac{n}{2}-1\\right) \\frac{\\alpha}{n} (+1) + \\left(\\frac{n}{2}\\right) \\frac{\\beta}{n} (-1)\n$$\n$$\n(\\bar{A}s)_i = \\frac{\\alpha}{2} - \\frac{\\alpha}{n} - \\frac{\\beta}{2} = \\left(\\frac{\\alpha - \\beta}{2} - \\frac{\\alpha}{n}\\right) s_i \\quad (\\text{since } s_i=+1)\n$$\nNow, consider a vertex $i$ in community $2$, for which $s_i = -1$.\nFor $j \\in \\text{community } 1$, we have $s_j = +1$ and $\\bar{A}_{ij} = p_{\\text{out}}$.\nFor $j \\in \\text{community } 2$ ($j \\neq i$), we have $s_j = -1$ and $\\bar{A}_{ij} = p_{\\text{in}}$.\n$$\n(\\bar{A}s)_i = \\sum_{j \\in C(1)} p_{\\text{out}} s_j + \\sum_{j \\in C(2), j \\neq i} p_{\\text{in}} s_j = \\left(\\frac{n}{2}\\right) \\frac{\\beta}{n} (+1) + \\left(\\frac{n}{2}-1\\right) \\frac{\\alpha}{n} (-1)\n$$\n$$\n(\\bar{A}s)_i = \\frac{\\beta}{2} - \\frac{\\alpha}{2} + \\frac{\\alpha}{n} = -\\left(\\frac{\\alpha - \\beta}{2} - \\frac{\\alpha}{n}\\right) = \\left(\\frac{\\alpha - \\beta}{2} - \\frac{\\alpha}{n}\\right) s_i \\quad (\\text{since } s_i=-1)\n$$\nIn both cases, we have $\\bar{A}s = \\left(\\frac{\\alpha - \\beta}{2} - \\frac{\\alpha}{n}\\right) s$. In the limit $n \\to \\infty$, the term $\\alpha/n$ vanishes. Therefore, $s$ is an eigenvector of $\\bar{A}$ with eigenvalue:\n$$\n\\lambda_{\\text{sig}}(A) = \\frac{\\alpha - \\beta}{2}\n$$\n\nNext, we find the informative eigenvalue $\\mu_{\\text{sig}}(L_{\\text{sym}})$ of the expected normalized Laplacian. The symmetric normalized Laplacian is $L_{\\text{sym}} = I - D^{-1/2} A D^{-1/2}$. The problem specifies using the mean-field approximation $D \\approx \\bar{c}I$ for large $n$, where $\\bar{c}$ is the expected degree. The expected degree for any vertex $i$ is:\n$$\n\\mathbb{E}[d_i] = \\sum_{j \\neq i} \\mathbb{E}[A_{ij}] = \\left(\\frac{n}{2}-1\\right)p_{\\text{in}} + \\frac{n}{2}p_{\\text{out}} = \\left(\\frac{n}{2}-1\\right)\\frac{\\alpha}{n} + \\frac{n}{2}\\frac{\\beta}{n}\n$$\nIn the limit $n \\to \\infty$, $\\mathbb{E}[d_i] \\to \\frac{\\alpha}{2} + \\frac{\\beta}{2}$. Thus, the average expected degree is $\\bar{c} = \\frac{\\alpha + \\beta}{2}$.\nUnder the specified approximation, the normalized Laplacian becomes:\n$$\nL_{\\text{sym}} \\approx I - (\\bar{c}I)^{-1/2} A (\\bar{c}I)^{-1/2} = I - \\frac{1}{\\bar{c}} A\n$$\nWe are interested in the expected operator, $\\bar{L}_{\\text{sym}} = \\mathbb{E}[L_{\\text{sym}}]$. Using the approximation:\n$$\n\\bar{L}_{\\text{sym}} \\approx \\mathbb{E}\\left[I - \\frac{1}{\\bar{c}} A\\right] = I - \\frac{1}{\\bar{c}} \\mathbb{E}[A] = I - \\frac{1}{\\bar{c}} \\bar{A}\n$$\nNow we find the eigenvalue of $\\bar{L}_{\\text{sym}}$ corresponding to the eigenvector $s$.\n$$\n\\bar{L}_{\\text{sym}} s = \\left(I - \\frac{1}{\\bar{c}} \\bar{A}\\right)s = Is - \\frac{1}{\\bar{c}}(\\bar{A}s)\n$$\nUsing our previous result that $\\bar{A}s = \\lambda_{\\text{sig}}(A) s$:\n$$\n\\bar{L}_{\\text{sym}} s = s - \\frac{1}{\\bar{c}} \\lambda_{\\text{sig}}(A) s = \\left(1 - \\frac{\\lambda_{\\text{sig}}(A)}{\\bar{c}}\\right)s\n$$\nThus, the informative eigenvalue of the expected normalized Laplacian is:\n$$\n\\mu_{\\text{sig}}(L_{\\text{sym}}) = 1 - \\frac{\\lambda_{\\text{sig}}(A)}{\\bar{c}}\n$$\nSubstituting the expressions for $\\lambda_{\\text{sig}}(A)$ and $\\bar{c}$:\n$$\n\\mu_{\\text{sig}}(L_{\\text{sym}}) = 1 - \\frac{(\\alpha - \\beta)/2}{(\\alpha + \\beta)/2} = 1 - \\frac{\\alpha - \\beta}{\\alpha + \\beta}\n$$\n\nFinally, we compute the ratio $R = \\frac{|\\lambda_{\\text{sig}}(A)|}{1 - \\mu_{\\text{sig}}(L_{\\text{sym}})}$.\nThe numerator is $|\\lambda_{\\text{sig}}(A)| = \\left|\\frac{\\alpha - \\beta}{2}\\right|$. Since it is given that $\\alpha  \\beta$, this is $\\frac{\\alpha - \\beta}{2}$.\nThe denominator is $1 - \\mu_{\\text{sig}}(L_{\\text{sym}})$, which we can calculate using our result for $\\mu_{\\text{sig}}(L_{\\text{sym}})$:\n$$\n1 - \\mu_{\\text{sig}}(L_{\\text{sym}}) = 1 - \\left(1 - \\frac{\\alpha - \\beta}{\\alpha + \\beta}\\right) = \\frac{\\alpha - \\beta}{\\alpha + \\beta}\n$$\nNow we compute the ratio $R$:\n$$\nR = \\frac{\\frac{\\alpha - \\beta}{2}}{\\frac{\\alpha - \\beta}{\\alpha + \\beta}} = \\frac{\\alpha - \\beta}{2} \\cdot \\frac{\\alpha + \\beta}{\\alpha - \\beta}\n$$\nThe term $(\\alpha - \\beta)$ cancels out, leaving:\n$$\nR = \\frac{\\alpha+\\beta}{2}\n$$\nThis expression is the closed-form analytic result for the ratio $R$ in terms of $\\alpha$ and $\\beta$.",
            "answer": "$$\n\\boxed{\\frac{\\alpha+\\beta}{2}}\n$$"
        },
        {
            "introduction": "A complex model is not always a better model; a good scientist must justify their choice. This final practice provides a concrete scenario for model selection, weighing the descriptive power of an SBM against its parametric complexity. By applying the Bayesian Information Criterion (BIC) to compare an SBM with simpler alternatives like the Erdős–Rényi and Chung-Lu models, you will engage in the critical task of balancing model fit with parsimony, a key skill in any data-driven scientific field .",
            "id": "4278496",
            "problem": "Consider a single observed, undirected, simple network on $n=6$ labeled nodes $\\{1,2,3,4,5,6\\}$ with edge set $\\{(1,2),(1,3),(2,3),(4,5),(5,6),(3,4),(2,5)\\}$. Let the node metadata define two blocks $A=\\{1,2,3\\}$ and $B=\\{4,5,6\\}$, which are to be treated as known and fixed. You will compare three generative models: the Erdős–Rényi (ER) random graph model with a single edge probability, the Chung–Lu (CL) expected-degree model with independent edges and node-specific expected degrees, and the Stochastic Block Model (SBM) with two blocks and three block-pair probabilities. For clarity, the acronyms are defined at first appearance: Bayesian Information Criterion (BIC), Erdős–Rényi (ER), Chung–Lu (CL), and Stochastic Block Model (SBM).\n\nStarting from the Bayesian model comparison principle that ranks models by the marginal likelihood $\\int L(\\boldsymbol{\\theta})\\pi(\\boldsymbol{\\theta})\\,d\\boldsymbol{\\theta}$, where $L(\\boldsymbol{\\theta})$ is the likelihood of the observed data under parameter vector $\\boldsymbol{\\theta}$ and $\\pi(\\boldsymbol{\\theta})$ is a regular prior density, use a second-order Laplace approximation around the maximum likelihood estimate to derive the large-sample criterion that penalizes model complexity by the number of free parameters and scales with the logarithm of the effective number of independent observations. Explain why, under independent-edge models for undirected simple graphs on $n$ nodes, the effective sample size is the number of dyads $N=\\binom{n}{2}$.\n\nThen, for the given network:\n\n- Specify the log-likelihoods for the Erdős–Rényi (ER), Chung–Lu (CL), and Stochastic Block Model (SBM) models assuming independent Bernoulli edges, and determine the maximum likelihood estimates for all model parameters under each model. For ER, use a single parameter $p$ for all dyads. For CL, use independent edges with probabilities $p_{ij}$ parameterized to match expected degrees to the observed degrees; justify a choice of $p_{ij}$ that achieves this. For the two-block SBM with known blocks $A$ and $B$, use parameters $p_{AA}$, $p_{BB}$, and $p_{AB}$ for within-$A$, within-$B$, and between-block probabilities, respectively.\n- Compute the Bayesian Information Criterion (BIC) for ER, CL, and SBM on this network, carefully stating your choices for the parameter count $k$ in each case and the effective sample size $N$.\n- Decide which model is preferred by BIC and justify the ranking.\n\nChoose the correct option:\n\nA. $BIC_{\\mathrm{SBM}}\\approx 21.48$, $BIC_{\\mathrm{ER}}\\approx 23.44$, $BIC_{\\mathrm{CL}}\\approx 34.43$; the Stochastic Block Model (SBM) is preferred.\n\nB. $BIC_{\\mathrm{ER}}\\approx 23.44$ is the smallest because the Erdős–Rényi (ER) model has the fewest parameters; $BIC_{\\mathrm{SBM}}\\approx 25.00$, $BIC_{\\mathrm{CL}}\\approx 34.43$.\n\nC. $BIC_{\\mathrm{CL}}\\approx 18.18$ is the smallest because the Chung–Lu (CL) model achieves the highest likelihood; $BIC_{\\mathrm{ER}}\\approx 23.44$, $BIC_{\\mathrm{SBM}}\\approx 21.48$.\n\nD. All three models have identical $BIC$ because they can match the network’s overall density; $BIC_{\\mathrm{ER}}=BIC_{\\mathrm{CL}}=BIC_{\\mathrm{SBM}}$.",
            "solution": "The Bayesian Information Criterion (BIC) arises as a large-sample approximation to the log marginal likelihood. The marginal likelihood of a model $\\mathcal{M}$ with parameter vector $\\boldsymbol{\\theta}\\in\\mathbb{R}^{k}$ and prior density $\\pi(\\boldsymbol{\\theta})$ is\n$$\n\\mathcal{Z} = \\int L(\\boldsymbol{\\theta}) \\, \\pi(\\boldsymbol{\\theta}) \\, d\\boldsymbol{\\theta},\n$$\nwhere $L(\\boldsymbol{\\theta})$ is the likelihood of the observed data under $\\boldsymbol{\\theta}$. Under standard regularity conditions, with $N$ independent observations, the log-likelihood admits a quadratic expansion around the maximum likelihood estimate (MLE) $\\hat{\\boldsymbol{\\theta}}$:\n$$\n\\log L(\\boldsymbol{\\theta}) \\approx \\log L(\\hat{\\boldsymbol{\\theta}}) - \\tfrac{1}{2} (\\boldsymbol{\\theta}-\\hat{\\boldsymbol{\\theta}})^\\top \\, \\mathcal{I}_N(\\hat{\\boldsymbol{\\theta}}) \\, (\\boldsymbol{\\theta}-\\hat{\\boldsymbol{\\theta}}),\n$$\nwhere $\\mathcal{I}_N(\\hat{\\boldsymbol{\\theta}})$ is the observed Fisher information, which scales linearly with $N$ (for independent observations, $\\mathcal{I}_N \\approx N \\, \\mathcal{I}_1$). Applying Laplace’s method,\n$$\n\\log \\mathcal{Z} \\approx \\log L(\\hat{\\boldsymbol{\\theta}}) + \\log \\pi(\\hat{\\boldsymbol{\\theta}}) + \\tfrac{k}{2}\\log(2\\pi) - \\tfrac{1}{2}\\log\\det \\mathcal{I}_N(\\hat{\\boldsymbol{\\theta}}).\n$$\nSince $\\det \\mathcal{I}_N(\\hat{\\boldsymbol{\\theta}}) \\approx N^{k}\\det \\mathcal{I}_1(\\hat{\\boldsymbol{\\theta}})$, the leading $N$-dependent term is $-\\tfrac{k}{2}\\log N$. Up to constants that do not depend on $N$ (and thus cancel in model comparisons on the same dataset), this yields the Bayesian Information Criterion\n$$\n\\mathrm{BIC} = -2 \\log L(\\hat{\\boldsymbol{\\theta}}) + k \\log N,\n$$\nwhere $k$ is the number of free parameters and $N$ is the effective number of independent observations.\n\nFor undirected simple graphs with independent Bernoulli edges across dyads, the natural independent observational units are the unordered node pairs $(i,j)$, $ij$, so $N=\\binom{n}{2}$. Here, $n=6$, so $N=\\binom{6}{2}=15$.\n\nWe proceed to compute the log-likelihoods, MLEs, and BICs for each model.\n\n**Erdős–Rényi (ER). Model and MLE:**\n- The ER model assumes each dyad $(i,j)$ is an independent Bernoulli($p$) trial with common probability $p\\in(0,1)$.\n- Let $m$ be the number of edges. From the observed edge set, $m=7$ and $N-m=8$ non-edges.\n- The log-likelihood is\n$$\n\\log L_{\\mathrm{ER}}(p) = m \\log p + (N-m)\\log(1-p).\n$$\n- The MLE is obtained by differentiating and setting to zero, yielding $\\hat{p}=\\tfrac{m}{N}=\\tfrac{7}{15}$.\n\n**Numerical log-likelihood at $\\hat{p}$:**\n- $\\log(\\tfrac{7}{15}) \\approx -0.762140$.\n- $\\log(1-\\tfrac{7}{15}) = \\log(\\tfrac{8}{15}) \\approx -0.628608$.\n- $\\log L_{\\mathrm{ER}}(\\hat{p}) = 7(-0.762140) + 8(-0.628608) \\approx -10.363844$.\n- Then $-2\\log L_{\\mathrm{ER}}(\\hat{p}) \\approx 20.727688$.\n\n**Parameter count and BIC for ER:**\n- The ER model has $k_{\\mathrm{ER}}=1$ free parameter ($p$).\n- With $N=15$ and $\\log N \\approx 2.708050$, the BIC is\n$$\n\\mathrm{BIC}_{\\mathrm{ER}} = -2\\log L_{\\mathrm{ER}}(\\hat{p}) + k_{\\mathrm{ER}}\\log N \\approx 20.727688 + 1\\times 2.708050 \\approx 23.435738.\n$$\n\n**Chung–Lu (CL). Model, parameterization, and MLE:**\n- The CL model assumes independent Bernoulli edges with dyad-specific probabilities $p_{ij}\\in(0,1)$ that factorize through node-specific parameters to match expected degrees. A standard parameterization is\n$$\np_{ij} = \\frac{d_i d_j}{\\sum_{k=1}^{n} d_k} = \\frac{d_i d_j}{2m},\n$$\nwhere the expected degrees are matched to the observed degrees $d_i$. In our network the degrees are $d_1=2$, $d_2=3$, $d_3=3$, $d_4=2$, $d_5=3$, $d_6=1$, with $\\sum_i d_i=2m=14$. This choice maximizes the likelihood within the exponential family of random graphs constrained to have this expected degree sequence.\n\n**Numerical log-likelihood for CL:**\n- The log-likelihood is $\\log L_{\\mathrm{CL}} = \\sum_{ij} \\big[ A_{ij}\\log p_{ij} + (1-A_{ij}) \\log(1-p_{ij}) \\big]$.\n- Summing these log-probabilities over all 7 edges and 8 non-edges gives $\\log L_{\\mathrm{CL}} \\approx -9.089250$, so $-2\\log L_{\\mathrm{CL}} \\approx 18.178500$.\n\n**Parameter count and BIC for CL:**\n- The CL model has one parameter per node, so $k_{\\mathrm{CL}}=n=6$.\n- With $N=15$, the BIC is\n$$\n\\mathrm{BIC}_{\\mathrm{CL}} = -2\\log L_{\\mathrm{CL}} + k_{\\mathrm{CL}} \\log N \\approx 18.178500 + 6\\times 2.708050 \\approx 34.426800.\n$$\n\n**Stochastic Block Model (SBM). Model, MLE, and log-likelihood:**\n- With blocks $A=\\{1,2,3\\}$ and $B=\\{4,5,6\\}$ known, the SBM assumes independent Bernoulli edges with probabilities $p_{AA}$, $p_{BB}$, and $p_{AB}$.\n- Count block-pair dyads and edges:\n  - Within $A$: $N_{AA}=\\binom{3}{2}=3$ dyads, $m_{AA}=3$ edges.\n  - Within $B$: $N_{BB}=\\binom{3}{2}=3$ dyads, $m_{BB}=2$ edges.\n  - Between $A$ and $B$: $N_{AB}=3\\times 3=9$ dyads, $m_{AB}=2$ edges.\n- The MLEs are sample proportions: $\\hat{p}_{AA}=\\tfrac{m_{AA}}{N_{AA}}=\\tfrac{3}{3}=1$, $\\hat{p}_{BB}=\\tfrac{2}{3}$, $\\hat{p}_{AB}=\\tfrac{2}{9}$.\n\n**Numerical log-likelihood at the MLE:**\n- The total log-likelihood is the sum of log-likelihoods for each block pair.\n- Within $A$: $3\\log(1) + 0\\log(0) = 0$. (Using the convention $0\\log 0 = 0$)\n- Within $B$: $2\\log(\\tfrac{2}{3}) + 1\\log(\\tfrac{1}{3}) \\approx -1.909542$.\n- Between $A$ and $B$: $2\\log(\\tfrac{2}{9}) + 7\\log(\\tfrac{7}{9}) \\approx -4.767361$.\n- Thus $\\log L_{\\mathrm{SBM}}(\\hat{P}) \\approx -6.676903$, and $-2\\log L_{\\mathrm{SBM}} \\approx 13.353806$.\n\n**Parameter count and BIC for SBM:**\n- The SBM has $k_{\\mathrm{SBM}}=3$ free parameters ($p_{AA}, p_{BB}, p_{AB}$).\n- With $N=15$, the BIC is\n$$\n\\mathrm{BIC}_{\\mathrm{SBM}} = -2\\log L_{\\mathrm{SBM}} + k_{\\mathrm{SBM}} \\log N \\approx 13.353806 + 3\\times 2.708050 \\approx 21.477956.\n$$\n\n**Comparison and ranking:**\n- $\\mathrm{BIC}_{\\mathrm{SBM}} \\approx 21.48$.\n- $\\mathrm{BIC}_{\\mathrm{ER}} \\approx 23.44$.\n- $\\mathrm{BIC}_{\\mathrm{CL}} \\approx 34.43$.\n\nA lower BIC value indicates a better model fit, penalizing for complexity. The ranking is SBM  ER  CL. The Stochastic Block Model is preferred. This matches Option A.\n- Option A is correct.\n- Option B is incorrect; $BIC_{\\mathrm{SBM}}$ is lower than $BIC_{\\mathrm{ER}}$.\n- Option C is incorrect; it ignores the large parameter penalty for the CL model.\n- Option D is incorrect; the BIC values are clearly different.",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}