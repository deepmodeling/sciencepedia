{
    "hands_on_practices": [
        {
            "introduction": "要真正掌握一个算法，亲手执行一遍是必不可少的。这项练习将提供一个小型加权网络，引导您完成标签传播算法（LPA）的一次同步更新。通过手动计算每个节点的新标签，并评估最终社区划分的模块度（modularity），您将对算法的核心机制——包括邻居权重和决胜规则如何影响最终结果——建立起具体而深入的理解。",
            "id": "4285516",
            "problem": "考虑一个无向加权网络，其节点集为 $\\{1,2,3,4,5,6\\}$，加权邻接矩阵 $W \\in \\mathbb{R}^{6 \\times 6}$ 如下所示：\n$$\nW =\n\\begin{pmatrix}\n0  & 1  & 1  & 0  & 0  & 0 \\\\\n1  & 0  & 1  & 0  & 2  & 0 \\\\\n1  & 1  & 0  & 1  & 0  & 0 \\\\\n0  & 0  & 1  & 0  & 1  & 1 \\\\\n0  & 2  & 0  & 1  & 0  & 1 \\\\\n0  & 0  & 0  & 1  & 1  & 0\n\\end{pmatrix}.\n$$\n我们将应用一次标签传播算法 (LPA) 的同步迭代，其定义如下：在迭代 $t$ 时，每个节点 $i$ 将其标签更新为能使其与当前携带该标签的邻居 $j$ 之间的边权重 $W_{ij}$ 之和最大的标签。如果两个或多个标签出现平局（即最大化权重和相等），则通过选择索引最小的标签来打破平局。设标签集为 $\\mathcal{L} = \\{\\ell_1, \\ell_2\\}$，索引顺序为 $\\ell_1 \\prec \\ell_2$。初始标签为\n$$\nL^{(0)} = (\\ell_1, \\ell_2, \\ell_2, \\ell_1, \\ell_2, \\ell_1),\n$$\n分别分配给节点 $(1,2,3,4,5,6)$。\n\n执行恰好一次 LPA 的同步迭代以获得 $L^{(1)}$。然后，将 $L^{(1)}$ 视为节点的划分，计算该划分在标准无向加权配置模型零基准下的模块度 $Q$。将最终的模块度表示为一个精确的有理数。无需四舍五入。最终答案必须是单个实数值。",
            "solution": "我们从标签传播算法 (LPA) 的定义开始：每个节点选择能使其与携带该标签的邻居之间的入射边权重之和最大的标签。更新是同步的，因此所有节点都根据 $L^{(0)}$ 计算它们的新标签，并同时更新以形成 $L^{(1)}$。打破平局的规则是当最大值相等时，选择索引最小的标签，这里 $\\ell_1 \\prec \\ell_2$。\n\n第 1 步：对于 $L^{(0)}$ 下的每个节点，通过按标签聚合其邻居的权重来计算 $L^{(1)}$。\n\n- 节点 $1$ 的邻居是 $\\{2,3\\}$，权重为 $W_{12} = 1$ 和 $W_{13} = 1$。在 $L^{(0)}$ 下，节点 2 携带标签 $\\ell_2$，节点 3 携带标签 $\\ell_2$。聚合权重为：$\\ell_1$: $0$，$\\ell_2$: $1+1 = 2$。节点 $1$ 更新为 $\\ell_2$。\n\n- 节点 $2$ 的邻居是 $\\{1,3,5\\}$，权重为 $W_{21} = 1$，$W_{23} = 1$，$W_{25} = 2$。在 $L^{(0)}$ 下，标签为 $L^{(0)}(1) = \\ell_1$，$L^{(0)}(3) = \\ell_2$，$L^{(0)}(5) = \\ell_2$。聚合权重为：$\\ell_1$: $1$，$\\ell_2$: $1+2 = 3$。节点 $2$ 更新为 $\\ell_2$（无变化）。\n\n- 节点 $3$ 的邻居是 $\\{1,2,4\\}$，权重为 $W_{31} = 1$，$W_{32} = 1$，$W_{34} = 1$。在 $L^{(0)}$ 下，标签为 $L^{(0)}(1) = \\ell_1$，$L^{(0)}(2) = \\ell_2$，$L^{(0)}(4) = \\ell_1$。聚合权重为：$\\ell_1$: $1+1 = 2$，$\\ell_2$: $1$。节点 $3$ 更新为 $\\ell_1$。\n\n- 节点 $4$ 的邻居是 $\\{3,5,6\\}$，权重为 $W_{43} = 1$，$W_{45} = 1$，$W_{46} = 1$。在 $L^{(0)}$ 下，标签为 $L^{(0)}(3) = \\ell_2$，$L^{(0)}(5) = \\ell_2$，$L^{(0)}(6) = \\ell_1$。聚合权重为：$\\ell_1$: $1$，$\\ell_2$: $1+1 = 2$。节点 $4$ 更新为 $\\ell_2$。\n\n- 节点 $5$ 的邻居是 $\\{2,4,6\\}$，权重为 $W_{52} = 2$，$W_{54} = 1$，$W_{56} = 1$。在 $L^{(0)}$ 下，标签为 $L^{(0)}(2) = \\ell_2$，$L^{(0)}(4) = \\ell_1$，$L^{(0)}(6) = \\ell_1$。聚合权重为：$\\ell_1$: $1+1 = 2$，$\\ell_2$: $2$。在总权重为 2 时，$\\ell_1$ 和 $\\ell_2$ 之间出现平局。根据打破平局规则，节点 5 更新为 $\\ell_1$（索引最小的标签）。\n\n- 节点 $6$ 的邻居是 $\\{4,5\\}$，权重为 $W_{64} = 1$ 和 $W_{65} = 1$。在 $L^{(0)}$ 下，标签为 $L^{(0)}(4) = \\ell_1$，$L^{(0)}(5) = \\ell_2$。聚合权重为：$\\ell_1$: $1$，$\\ell_2$: $1$。这是一个平局；根据打破平局规则，节点 $6$ 更新为 $\\ell_1$（无变化）。\n\n收集所有更新，同步迭代得到\n$$\nL^{(1)} = (\\ell_2, \\ell_2, \\ell_1, \\ell_2, \\ell_1, \\ell_1).\n$$\n\n第 2 步：在标准无向加权配置模型基准下，计算由 $L^{(1)}$ 导出的划分的模块度 $Q$。模块度是通过比较观测到的社群内部边权重与零模型下的期望权重来定义的。对于一个无向加权图，令 $m$ 为所有边权重的总和，令 $k_i = \\sum_{j} W_{ij}$ 为节点 $i$ 的强度（加权度）。模块度的一个等价标准表达式为\n$$\nQ = \\sum_{s} \\left( \\frac{l_s}{m} - \\left( \\frac{d_s}{2m} \\right)^{2} \\right),\n$$\n其中求和是对所有社群 $s$ 进行的，$l_s$ 是两个端点都在社群 $s$ 内的边的总权重，$d_s$ 是社群 $s$ 中所有节点的强度 $k_i$ 的总和。我们通过对每个社群内的节点对求和，并认识到 $\\sum_{i,j \\in s} W_{ij} = 2 l_s$ 和 $\\sum_{i \\in s} k_i = d_s$，从基准公式 $\\left(W_{ij} - \\frac{k_i k_j}{2m}\\right)$ 推导出这个恒等式。\n\n计算 $m$，即总边权重：\n$$\nm = W_{12} + W_{13} + W_{23} + W_{45} + W_{56} + W_{46} + W_{34} + W_{25} = 1+1+1+1+1+1+1+2 = 9.\n$$\n因此 $2m = 18$。\n\n计算 $i \\in \\{1,\\dots,6\\}$ 的强度 $k_i$：\n\\begin{align*}\nk_1 &= W_{12} + W_{13} = 1 + 1 = 2, \\\\\nk_2 &= W_{21} + W_{23} + W_{25} = 1 + 1 + 2 = 4, \\\\\nk_3 &= W_{31} + W_{32} + W_{34} = 1 + 1 + 1 = 3, \\\\\nk_4 &= W_{43} + W_{45} + W_{46} = 1 + 1 + 1 = 3, \\\\\nk_5 &= W_{52} + W_{54} + W_{56} = 2 + 1 + 1 = 4, \\\\\nk_6 &= W_{64} + W_{65} = 1 + 1 = 2.\n\\end{align*}\n检查一致性：$\\sum_{i=1}^{6} k_i = 2+4+3+3+4+2 = 18 = 2m$。\n\n在 $L^{(1)}$ 下，社群 $\\ell_1$ 包含节点 $\\{3,5,6\\}$，社群 $\\ell_2$ 包含节点 $\\{1,2,4\\}$。\n\n计算社群内部权重：\n- 对于社群 $\\ell_1$（节点 $\\{3,5,6\\}$），两个端点都在社群内部的边是 $(5,6)$，其权重为 $W_{56} = 1$。在 $W$ 中没有边 $(3,5)$ 或 $(3,6)$。因此 $l_{\\ell_1} = 1$。\n\n- 对于社群 $\\ell_2$（节点 $\\{1,2,4\\}$），两个端点都在社群内部的边是 $(1,2)$，其权重为 $W_{12} = 1$。没有边 $(1,4)$ 或 $(2,4)$。因此 $l_{\\ell_2} = 1$。\n\n计算社群强度：\n\\begin{align*}\nd_{\\ell_1} &= k_3 + k_5 + k_6 = 3 + 4 + 2 = 9, \\\\\nd_{\\ell_2} &= k_1 + k_2 + k_4 = 2 + 4 + 3 = 9.\n\\end{align*}\n\n现在计算 $Q$：\n\\begin{align*}\nQ &= \\left( \\frac{l_{\\ell_1}}{m} - \\left( \\frac{d_{\\ell_1}}{2m} \\right)^{2} \\right)\n   + \\left( \\frac{l_{\\ell_2}}{m} - \\left( \\frac{d_{\\ell_2}}{2m} \\right)^{2} \\right) \\\\\n  &= \\left( \\frac{1}{9} - \\left( \\frac{9}{18} \\right)^{2} \\right)\n   + \\left( \\frac{1}{9} - \\left( \\frac{9}{18} \\right)^{2} \\right) \\\\\n  &= 2 \\left( \\frac{1}{9} - \\frac{1}{4} \\right) \\\\\n  &= 2 \\left( \\frac{4 - 9}{36} \\right) \\\\\n  &= 2 \\left( -\\frac{5}{36} \\right) \\\\\n  &= -\\frac{10}{36} \\\\\n  &= -\\frac{5}{18}.\n\\end{align*}\n\n因此，通过一次同步LPA迭代并使用指定的打破平局规则所产生的划分的模块度为 $-\\frac{5}{18}$。",
            "answer": "$$\\boxed{-\\frac{5}{18}}$$"
        },
        {
            "introduction": "标签传播算法在大型网络上的行为可以通过简化模型来分析。本实践探讨了一种关键现象：大社区“吞并”小而密集的社区，这与算法的分辨率限制密切相关。通过运用平均场（mean-field）方法，您将推导出发生社区吞并的临界社区间连接密度，从而洞察网络结构是如何决定最终社区检测结果的。",
            "id": "4285519",
            "problem": "考虑一个简单的双社群网络模型，旨在研究标签传播算法（Label Propagation Algorithm, LPA）的动态。令 $C_A$ 是一个大社群，$C_B$ 是一个小而稠密的子图。社群 $C_A$ 被建模为一个有 $n_A$ 个节点、社群内连边概率为 $q_A$ 的 Erdős–Rényi (ER) 随机图 $G(n_A, q_A)$。社群 $C_B$ 被建模为一个有 $n_B$ 个节点的完全图（团）$K_{n_B}$。对于任意节点对 $(i, j)$，其中 $i \\in C_A$ 且 $j \\in C_B$，它们之间以概率 $r$ 独立地存在一条社群间连边。\n\n所有边都是无向且无权的。在时间 $t=0$ 时，$C_A$ 中的每个节点都带有标签 $a$，而 $C_B$ 中的每个节点都带有标签 $b$。然后，网络根据标签传播算法进行一次同步更新：每个节点采纳其邻居中当前持有数量最多的标签；若出现平局，则节点保留其当前标签。\n\n我们定义小而稠密的子图 $C_B$ 被大社群 $C_A$ 吸收的现象为：在第一次同步更新后，根据平均场假设（即邻居数被其期望值替代），$C_B$ 中的一个典型节点将采纳标签 $a$ 而不是保留标签 $b$。临界社群间连边密度 $r_c$ 被定义为这样的 $r$ 值：在该值下，$C_B$ 中的一个节点看到的带有标签 $a$ 的邻居的期望数量等于其看到的带有标签 $b$ 的邻居的期望数量。因此，当 $r > r_c$ 时，小而稠密的子图被吸收；当 $r < r_c$ 时，它抵抗吸收。\n\n假设参数值为 $n_A = 1200$，$q_A = 0.15$ 和 $n_B = 25$。$C_A$ 的 ER 连通性足够强，以至于对于所有相关的 $r$ 值，它对于标签变化都是内部自稳定的，即 $C_A$ 中的一个节点，在期望上，其在 $C_A$ 内部带有标签 $a$ 的邻居数量严格多于其在 $C_B$ 中带有标签 $b$ 的邻居数量。\n\n在上述平均场假设下，计算临界社群间连边密度 $r_c$。请将您的最终答案表示为小数，并四舍五入到四位有效数字。",
            "solution": "问题陈述被认为是有效的，因为它在网络科学中有科学依据，提法良好并包含了所有必要信息，且没有矛盾或歧义。\n\n目标是计算临界社群间连边密度，记为 $r_c$。此临界值由以下条件定义：在初始时间 $t=0$ 时，小社群 $C_B$ 中的一个典型节点拥有相同期望数量的标签为 $a$ 的邻居和标签为 $b$ 的邻居。根据指定的平均场假设，这是标签传播算法同步更新的临界点。\n\n让我们考虑一个属于社群 $C_B$ 的通用节点 $j$。我们需要计算其标签为 $a$ 和标签为 $b$ 的邻居的期望数量。\n\n首先，我们确定标签为 $b$ 的邻居的期望数量。在时间 $t=0$ 时，$C_B$ 中的所有节点都带有标签 $b$。社群 $C_B$ 被建模为一个完全图（团）$K_{n_B}$。在一个大小为 $n_B$ 的团中，每个节点都与其他所有节点相连。因此，任何节点 $j \\in C_B$ 都与 $C_B$ 内的所有其他 $n_B - 1$ 个节点相连。由于所有这些节点都带有标签 $b$，所以节点 $j$ 的标签为 $b$ 的邻居数量确定地是 $n_B - 1$。一个常数的期望就是它本身。设 $E[N_b(j)]$ 为节点 $j$ 的标签为 $b$ 的邻居的期望数量。\n$$E[N_b(j)] = n_B - 1$$\n\n接下来，我们确定标签为 $a$ 的邻居的期望数量。在时间 $t=0$ 时，社群 $C_A$ 中的所有节点都带有标签 $a$。$C_A$ 中的节点数为 $n_A$。根据问题陈述，任何节点 $j \\in C_B$ 和任何节点 $i \\in C_A$ 之间的边以概率 $r$ 独立存在。对于节点 $j$ 来说，在 $C_A$ 中有 $n_A$ 个潜在邻居。这 $n_A$ 条边中每一条的形成都可以看作是一次成功概率为 $r$ 的独立伯努利试验。因此，$j$ 在 $C_A$ 中的邻居总数是一个服从二项分布 $B(n_A, r)$ 的随机变量。该分布的期望值是试验次数与成功概率的乘积。设 $E[N_a(j)]$ 为节点 $j$ 的标签为 $a$ 的邻居的期望数量。\n$$E[N_a(j)] = n_A r$$\n参数 $q_A$，即 $C_A$ 的社群内连边概率，与计算 $C_B$ 中节点的邻居期望数量无关。\n\n临界社群间连边密度 $r_c$ 被定义为这两个期望值相等时的 $r$ 值。\n$$E[N_a(j)] = E[N_b(j)]$$\n代入上面推导出的表达式，我们设定 $r=r_c$ 的条件：\n$$n_A r_c = n_B - 1$$\n\n现在我们可以解出 $r_c$：\n$$r_c = \\frac{n_B - 1}{n_A}$$\n\n问题给出了以下参数值：$n_A = 1200$ 和 $n_B = 25$。将这些值代入 $r_c$ 的表达式中：\n$$r_c = \\frac{25 - 1}{1200} = \\frac{24}{1200}$$\n\n进行除法运算：\n$$r_c = \\frac{24}{1200} = \\frac{1}{50} = 0.02$$\n\n问题要求答案以小数形式表示，并四舍五入到四位有效数字。要将 $0.02$ 表示为四位有效数字，我们写作 $0.02000$。",
            "answer": "$$\\boxed{0.02000}$$"
        },
        {
            "introduction": "标签传播算法的一个显著特点是它对初始标签分配的敏感性，这可能导致算法收敛到不同的最终社区划分。这个编程练习要求您实现该算法，并使用一个基于信息论的度量——信息变差（Variation of Information, VI）——来量化这种不确定性。通过在不同类型的图结构上比较多次运行的结果，您将掌握一种评估社区检测算法稳定性和可靠性的标准方法。",
            "id": "4285521",
            "problem": "考虑一个无向简单图 $G = (V,E)$，其中 $|V| = n$ 个节点标记为 $0,1,\\ldots,n-1$。社区分配由一个 $V$ 的划分表示，该划分被编码为一个整数标签向量 $\\ell \\in \\{0,1,2,\\ldots\\}^{n}$，其中 $\\ell_i$ 是节点 $i$ 的社区标签。标签传播算法 (LPA) 通过局部多数投票来分配标签，其过程如下。从一个初始标签 $\\ell^{(0)}$ 开始。在每次迭代 $t$ 中，按固定顺序 $0,1,\\ldots,n-1$ 遍历节点，并使用以下规则更新节点 $i$：将 $\\ell^{(t+)}_i$ 设置为在 $G$ 中节点 $i$ 的邻居中出现最频繁的标签；如果出现平局，则在并列的标签中选择最小的标签值；如果节点 $i$ 没有邻居，则保持 $\\ell^{(t+)}_i$ 不变。在遍历过程中，原地使用更新后的 $\\ell^{(t+)}$。重复遍历，直到一次完整的遍历不产生任何变化，或达到最多 $1000$ 次遍历。\n\n为了量化 LPA 对初始化的敏感性，考虑在同一图上由不同初始标签产生的两个最终划分。设 $X$ 为在第一个划分下均匀随机抽取一个节点的社区标签所对应的离散随机变量，设 $Y$ 为在第二个划分下对应的变量。使用基于香农熵和互信息的标准信息论框架，推导并实现一个有原则的、用于衡量两个划分之间距离的度量。推导必须从基本定义开始：对于一个概率质量函数为 $p_Z$ 的离散随机变量 $Z$，香农熵为 $H(Z) = -\\sum_{z} p_Z(z)\\,\\log p_Z(z)$；对于一个联合质量函数为 $p_{X,Y}$、边际概率为 $p_X$ 和 $p_Y$ 的对 $(X,Y)$，互信息为 $I(X;Y) = \\sum_{x,y} p_{X,Y}(x,y)\\,\\log\\left(\\frac{p_{X,Y}(x,y)}{p_X(x)\\,p_Y(y)}\\right)$。对数必须是自然对数。\n\n按照规定，实现上述带有确定性平局打破规则的异步 LPA。对于每个测试用例，从每个提供的初始标签开始运行 LPA 直至收敛，收集所有最终划分的集合，并计算所有不同最终划分对之间的成对距离的平均值。将每个测试用例的答案表示为一个四舍五入到六位小数的实数。\n\n测试套件由四个图及其相应的初始化集组成：\n\n- 测试用例 1（完全图 $K_5$）：\n    - 节点数：$n = 5$。\n    - 边：所有满足 $0 \\le i  j \\le 4$ 的无序对 $(i,j)$，即 $\\{(0,1),(0,2),(0,3),(0,4),(1,2),(1,3),(1,4),(2,3),(2,4),(3,4)\\}$。\n    - 初始标签：\n        - $\\ell^{(0,1)} = [0,1,2,3,4]$,\n        - $\\ell^{(0,2)} = [0,0,1,1,1]$,\n        - $\\ell^{(0,3)} = [2,2,2,2,2]$。\n\n- 测试用例 2（循环图 $C_{10}$）：\n    - 节点数：$n = 10$。\n    - 边：$\\{(i,(i+1)\\bmod 10) \\mid i \\in \\{0,1,2,3,4,5,6,7,8,9\\}\\}$，具体为 $\\{(0,1),(1,2),(2,3),(3,4),(4,5),(5,6),(6,7),(7,8),(8,9),(9,0)\\}$。\n    - 初始标签：\n        - $\\ell^{(0,1)} = [0,1,2,3,4,5,6,7,8,9]$,\n        - $\\ell^{(0,2)} = [0,1,0,1,0,1,0,1,0,1]$,\n        - $\\ell^{(0,3)} = [0,0,0,0,0,1,1,1,1,1]$。\n\n- 测试用例 3（由一座桥连接的两个大小为 4 的团）：\n    - 节点数：$n = 8$。\n    - 节点 $\\{0,1,2,3\\}$ 上的团边：所有满足 $0 \\le i  j \\le 3$ 的无序对 $(i,j)$，即 $\\{(0,1),(0,2),(0,3),(1,2),(1,3),(2,3)\\}$。\n    - 节点 $\\{4,5,6,7\\}$ 上的团边：所有满足 $4 \\le i  j \\le 7$ 的无序对 $(i,j)$，即 $\\{(4,5),(4,6),(4,7),(5,6),(5,7),(6,7)\\}$。\n    - 桥边：$(3,4)$。\n    - 初始标签：\n        - $\\ell^{(0,1)} = [0,1,2,3,4,5,6,7]$,\n        - $\\ell^{(0,2)} = [0,0,0,0,1,1,1,1]$,\n        - $\\ell^{(0,3)} = [1,0,1,0,2,2,3,3]$。\n\n- 测试用例 4（单个孤立节点）：\n    - 节点数：$n = 1$。\n    - 边：$\\varnothing$。\n    - 初始标签：\n        - $\\ell^{(0,1)} = [0]$,\n        - $\\ell^{(0,2)} = [1]$,\n        - $\\ell^{(0,3)} = [5]$。\n\n您的程序必须为每个测试用例计算从所列初始标签获得的全部不同最终划分对之间的成对划分距离的平均值。您的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表，按测试用例 1, 2, 3, 4 的顺序排列结果，每个值都四舍五入到六位小数。例如，输出格式应精确地像这样：$[x_1,x_2,x_3,x_4]$，其中每个 $x_i$ 是一个如上指定的浮点数。",
            "solution": "该问题要求实现一种异步标签传播算法（LPA），并推导和实现一种基于信息论的距离度量，以比较最终的社区划分。解决方案分为三部分：首先，推导距离度量；其次，描述 LPA 的算法实现；第三，将这些方法应用于具体的测试用例。\n\n### 第一部分：划分距离度量的推导\n\n我们的任务是推导一个有原则的距离，用于衡量同一组 $n$ 个节点的两个划分 $\\mathcal{C}_1$ 和 $\\mathcal{C}_2$ 之间的差异。这些划分由标签向量 $\\ell^{(1)}$ 和 $\\ell^{(2)}$ 表示。设 $X$ 为根据划分 $\\mathcal{C}_1$ 从 $V$ 中均匀随机选择一个节点的社区标签所对应的离散随机变量，设 $Y$ 为根据划分 $\\mathcal{C}_2$ 对应的变量。\n\n概率质量函数由以下公式给出：\n- 在 $\\mathcal{C}_1$ 中标签 $c_x$ 的边际概率：$p_X(c_x) = \\frac{|\\{i \\mid \\ell^{(1)}_i = c_x\\}|}{n}$\n- 在 $\\mathcal{C}_2$ 中标签 $c_y$ 的边际概率：$p_Y(c_y) = \\frac{|\\{i \\mid \\ell^{(2)}_i = c_y\\}|}{n}$\n- 同时观察到标签 $c_x$ 和 $c_y$ 的联合概率：$p_{X,Y}(c_x, c_y) = \\frac{|\\{i \\mid \\ell^{(1)}_i = c_x \\text{ 且 } \\ell^{(2)}_i = c_y\\}|}{n}$\n\n一个划分的香农熵衡量其不确定性。对于划分 $\\mathcal{C}_1$，其熵为：\n$$ H(X) = -\\sum_{c_x} p_X(c_x) \\log p_X(c_x) $$\n其中对数为指定的自然对数。类似地，$H(Y)$ 是 $\\mathcal{C}_2$ 的熵。\n\n互信息 $I(X;Y)$ 量化了两个划分之间共享的信息：\n$$ I(X;Y) = \\sum_{c_x, c_y} p_{X,Y}(c_x, c_y) \\log \\left( \\frac{p_{X,Y}(c_x, c_y)}{p_X(c_x) p_Y(c_y)} \\right) $$\n\n一个有原则的距离度量应衡量两个划分之间的信息差异。一个符合此描述的标准度量是**信息变差（Variation of Information, VI）**。它被定义为条件熵之和：\n$$ D(\\mathcal{C}_1, \\mathcal{C}_2) \\equiv VI(X,Y) = H(X|Y) + H(Y|X) $$\n$H(X|Y)$ 是在给定划分 $\\mathcal{C}_2$ 的情况下，划分 $\\mathcal{C}_1$ 中剩余的不确定性，而 $H(Y|X)$ 则反之。这个和表示当其中一个划分被揭示时，我们获得的关于另一个划分的总信息量。$VI$ 是一个真正的度量，满足非负性、不可区分者同一性、对称性和三角不等式。\n\n利用熵的链式法则 $H(X,Y) = H(X) + H(Y|X) = H(Y) + H(X|Y)$，以及互信息的定义 $I(X;Y) = H(X) + H(Y) - H(X,Y)$，我们可以用边际熵和互信息来表示条件熵：\n- $H(X|Y) = H(X) - I(X;Y)$\n- $H(Y|X) = H(Y) - I(X;Y)$\n\n将这些代入 $VI$ 的定义，得到我们将用于计算的形式：\n$$ D(\\mathcal{C}_1, \\mathcal{C}_2) = H(X) + H(Y) - 2I(X;Y) $$\n这个表达式源于基本的信息论原理，提供了一个稳健、对称的划分间距离度量。它与标签的具体整数值无关，仅取决于节点的组合方式。如果两个划分相同（在重新标记的意义下），它们的距离为 $0$。\n\n### 第二部分：算法实现\n\n**标签传播算法（LPA）：**\n异步 LPA 按题目描述实现。图用邻接表表示。算法按固定顺序 $i=0, 1, \\ldots, n-1$ 遍历节点。对每个节点 $i$，我们识别其邻居的标签。节点 $i$ 的新标签 $\\ell_i'$ 是其邻居当前标签中出现最频繁的一个。更新是原地进行的，这意味着如果我们更新了节点 $i$，那么在同一次遍历中任何后续的节点 $j > i$ 在考虑其邻居时将看到新的标签 $\\ell_i'$。这使得更新顺序变得重要。平局通过选择最小的整数标签值来打破。如果一个节点没有邻居，其标签保持不变。该过程迭代进行，直到一次完整的遍历不产生任何标签变化，或达到最多 $1000$ 次遍历。\n\n**距离计算：**\n为了计算 $D(\\mathcal{C}_1, \\mathcal{C}_2)$，我们首先构建一个列联表（或混淆矩阵）$M$，其中 $M_{ij}$ 统计了在 $\\mathcal{C}_1$ 中被分配到社区 $i$ 且在 $\\mathcal{C}_2$ 中被分配到社区 $j$ 的节点数量。将该矩阵除以 $n$ 得到联合概率分布 $p_{X,Y}$。边际分布 $p_X$ 和 $p_Y$ 分别通过对 $p_{X,Y}$ 矩阵的行和列求和得到。然后我们使用这些分布计算 $H(X)$、$H(Y)$ 和 $I(X;Y)$，注意处理概率为零的项以避免数值错误（例如，如果 $p=0$，则 $p \\log p$ 为 $0$）。\n\n### 第三部分：应用于测试用例\n\n对于每个测试用例，我们给定一个图和一组三个初始标签。\n1.  对于三个初始标签中的每一个，我们运行 LPA 直至收敛，以获得一个最终的、稳定的划分。这会产生一个包含三个最终划分的集合 $\\{\\mathcal{P}_1, \\mathcal{P}_2, \\mathcal{P}_3\\}$。\n2.  然后，我们计算这些最终划分的所有不同对之间的成对距离：$d_{12} = D(\\mathcal{P}_1, \\mathcal{P}_2)$，$d_{13} = D(\\mathcal{P}_1, \\mathcal{P}_3)$ 和 $d_{23} = D(\\mathcal{P}_2, \\mathcal{P}_3)$。\n3.  该测试用例的最终结果是这三个距离的平均值：$\\frac{1}{3}(d_{12} + d_{13} + d_{23})$。\n\n对四个提供的测试用例均遵循此过程。例如，在测试用例 3（两个团加一座桥）中，初始标签 $[0,0,0,0,1,1,1,1]$ 已经是一个对应于真实社区的稳定配置，LPA 不会做任何改变。其他初始化，如 $[0,1,2,3,4,5,6,7]$，会收敛到一个单一的大社区。第三个初始化 $[1,0,1,0,2,2,3,3]$ 会收敛到一个与那个稳定划分结构相同但标签值不同的双社区结构。这两个双社区划分之间的距离为 $0$，而它们中任意一个与单社区划分的距离为 $\\log(2) \\approx 0.693147$。因此，该测试用例的平均距离是 $\\frac{1}{3}(\\log(2) + \\log(2) + 0) = \\frac{2}{3}\\log(2) \\approx 0.462098$。其余案例的计算方式类似。",
            "answer": "```python\nimport numpy as np\nfrom itertools import combinations\n\ndef compute_distance(labels1, labels2):\n    \"\"\"\n    Computes the Variation of Information (VI) distance between two partitions.\n    VI(X,Y) = H(X) + H(Y) - 2*I(X,Y)\n    \"\"\"\n    n = len(labels1)\n    if n == 0:\n        return 0.0\n\n    # Map labels to 0-indexed integers for matrix construction\n    unique_l1, inv_l1 = np.unique(labels1, return_inverse=True)\n    unique_l2, inv_l2 = np.unique(labels2, return_inverse=True)\n    \n    num_c1 = len(unique_l1)\n    num_c2 = len(unique_l2)\n    \n    # Build contingency table (joint counts)\n    contingency = np.zeros((num_c1, num_c2), dtype=float)\n    np.add.at(contingency, (inv_l1, inv_l2), 1)\n\n    # Convert to joint probability distribution\n    p_xy = contingency / n\n    \n    # Calculate marginal probabilities\n    p_x = np.sum(p_xy, axis=1)\n    p_y = np.sum(p_xy, axis=0)\n    \n    # Calculate H(X)\n    nz_x = p_x > 0\n    h_x = -np.sum(p_x[nz_x] * np.log(p_x[nz_x]))\n    \n    # Calculate H(Y)\n    nz_y = p_y > 0\n    h_y = -np.sum(p_y[nz_y] * np.log(p_y[nz_y]))\n    \n    # Calculate I(X;Y)\n    nz_xy = p_xy > 0\n    p_x_p_y = np.outer(p_x, p_y)\n    \n    mi = np.sum(p_xy[nz_xy] * np.log(p_xy[nz_xy] / p_x_p_y[nz_xy]))\n    \n    # VI distance\n    vi = h_x + h_y - 2 * mi\n    \n    # Clamp small negative values that can arise from floating point errors\n    return max(0.0, vi)\n\n\ndef run_lpa(n, edges, initial_labels):\n    \"\"\"\n    Runs the asynchronous Label Propagation Algorithm.\n    \"\"\"\n    adj = [[] for _ in range(n)]\n    for u, v in edges:\n        adj[u].append(v)\n        adj[v].append(u)\n\n    labels = np.array(initial_labels, dtype=int)\n    max_sweeps = 1000\n\n    for _ in range(max_sweeps):\n        num_changes = 0\n        for i in range(n):\n            if not adj[i]:\n                continue\n            \n            neighbor_labels = labels[adj[i]]\n            \n            unique_labs, counts = np.unique(neighbor_labels, return_counts=True)\n            max_count = np.max(counts)\n            tied_labels = unique_labs[counts == max_count]\n            new_label = np.min(tied_labels)\n            \n            if labels[i] != new_label:\n                labels[i] = new_label\n                num_changes += 1\n        \n        if num_changes == 0:\n            break\n            \n    return labels.tolist()\n\n\ndef solve():\n    test_cases = [\n        # Test case 1: K_5\n        {\n            \"n\": 5,\n            \"edges\": [(0, 1), (0, 2), (0, 3), (0, 4), (1, 2), (1, 3), (1, 4), (2, 3), (2, 4), (3, 4)],\n            \"initial_labelings\": [\n                [0, 1, 2, 3, 4],\n                [0, 0, 1, 1, 1],\n                [2, 2, 2, 2, 2]\n            ]\n        },\n        # Test case 2: C_10\n        {\n            \"n\": 10,\n            \"edges\": [(0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7), (7, 8), (8, 9), (9, 0)],\n            \"initial_labelings\": [\n                [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n                [0, 1, 0, 1, 0, 1, 0, 1, 0, 1],\n                [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n            ]\n        },\n        # Test case 3: two cliques + bridge\n        {\n            \"n\": 8,\n            \"edges\": [\n                (0, 1), (0, 2), (0, 3), (1, 2), (1, 3), (2, 3), # Clique 1\n                (4, 5), (4, 6), (4, 7), (5, 6), (5, 7), (6, 7), # Clique 2\n                (3, 4)                                          # Bridge\n            ],\n            \"initial_labelings\": [\n                [0, 1, 2, 3, 4, 5, 6, 7],\n                [0, 0, 0, 0, 1, 1, 1, 1],\n                [1, 0, 1, 0, 2, 2, 3, 3]\n            ]\n        },\n        # Test case 4: single isolated node\n        {\n            \"n\": 1,\n            \"edges\": [],\n            \"initial_labelings\": [\n                [0],\n                [1],\n                [5]\n            ]\n        }\n    ]\n\n    final_results = []\n    for case in test_cases:\n        final_partitions = []\n        for init_labels in case[\"initial_labelings\"]:\n            final_p = run_lpa(case[\"n\"], case[\"edges\"], init_labels)\n            final_partitions.append(final_p)\n            \n        pairwise_distances = []\n        for p1_idx, p2_idx in combinations(range(len(final_partitions)), 2):\n            dist = compute_distance(final_partitions[p1_idx], final_partitions[p2_idx])\n            pairwise_distances.append(dist)\n            \n        if not pairwise_distances:\n            avg_dist = 0.0\n        else:\n            avg_dist = sum(pairwise_distances) / len(pairwise_distances)\n            \n        final_results.append(f\"{avg_dist:.6f}\")\n        \n    print(f\"[{','.join(final_results)}]\")\n\nsolve()\n\n```"
        }
    ]
}