{
    "hands_on_practices": [
        {
            "introduction": "To build a solid understanding of the giant component, we first analyze its absence. This practice explores the subcritical regime of the Erdős–Rényi random graph, where the average degree $c$ is less than one. By approximating the component exploration process as a Galton-Watson branching process, you will derive the expected size of a connected component, providing a quantitative grasp of network structure before the phase transition .",
            "id": "4270116",
            "problem": "Consider an undirected simple network modeled as an Erdős–Rényi (ER) random graph $G(n,p)$, where $p = c/n$ with $0 < c < 1$ fixed and $n$ is large. A single node is chosen uniformly at random. Using only fundamental definitions of the ER model, the exploration process of a component from a seed node, and a Galton–Watson (GW) branching process approximation justified by the local tree-like structure of $G(n,p)$ in the subcritical regime $c<1$, derive a closed-form analytic expression for the expected size of the connected component containing the chosen node in the limit $n \\to \\infty$.\n\nYour derivation must begin from first principles: define the exploration process (for example, Breadth-First Search (BFS)), state how the offspring distribution arises from $G(n,p)$, and justify why cycles are negligible for $c<1$ at finite exploration depth. Then compute the expected total number of nodes discovered by summing the expected population across generations. Express your final answer as a function of $c$. No rounding is required, and no units are involved.",
            "solution": "We are tasked with deriving the expected size of the connected component containing a randomly chosen node in an Erdős–Rényi (ER) random graph $G(n,p)$ in the limit $n \\to \\infty$, where the edge probability is $p = c/n$ and $c$ is a fixed constant such that $0 < c < 1$. This regime is known as the subcritical regime. The derivation will use an exploration process modeled as a Galton-Watson (GW) branching process.\n\nFirst, we define the exploration process to find the connected component containing a single, randomly chosen seed node, let's call it $v_0$. A Breadth-First Search (BFS) is a natural way to conceptualize this exploration. Let $S_k$ be the set of nodes discovered up to step $k$ of the process, and let $Q_k$ be the set of \"active\" nodes at step $k$ whose neighbors are yet to be explored.\n\nThe process begins at generation $k=0$:\n- The initial set of discovered nodes is $S_0 = \\{v_0\\}$.\n- The initial set of active nodes is $Q_0 = \\{v_0\\}$.\n- The size of generation $0$ is $|Q_0| = 1$.\n\nAt each subsequent step $k \\ge 1$:\n- We form the set of active nodes $Q_k$ by identifying all neighbors of the nodes in $Q_{k-1}$ that have not yet been discovered. That is, for each node $u \\in Q_{k-1}$, we examine all other nodes $w \\in V \\setminus S_{k-1}$ (where $V$ is the set of all $n$ nodes) and add $w$ to $Q_k$ if an edge $(u,w)$ exists.\n- The set of discovered nodes is updated: $S_k = S_{k-1} \\cup Q_k$.\n- The process terminates when $Q_k$ is empty, at which point $S_{k-1}$ is the complete connected component.\n\nThe total size of the component is $\\sum_{k=0}^{\\infty} |Q_k|$. We want to find the expected value of this sum.\n\nNow, we connect this exploration process to a Galton-Watson branching process. A GW process models population growth where each individual in generation $k$ independently produces a random number of offspring for generation $k+1$, drawn from a common offspring distribution.\n\nLet's determine the offspring distribution for our exploration process. Consider a node $u$ in generation $k-1$ (i.e., $u \\in Q_{k-1}$). Its \"offspring\" are its neighbors that are not in $S_{k-1}$. The number of nodes in $S_{k-1}$ is the size of the component discovered so far. In the subcritical regime $c<1$, the components are small, with sizes that do not scale with $n$. The expected size, as we will show, is finite. Therefore, for any finite number of exploration steps $k$, the size of $S_{k-1}$ is of order $O(1)$. This means $|S_{k-1}| \\ll n$ as $n \\to \\infty$.\n\nThe number of potential offspring for node $u$ is the number of nodes not yet discovered, which is $n - |S_{k-1}|$. Since $|S_{k-1}|$ is negligible compared to $n$, we can approximate this by $n-1$ or even $n$. For each of these $n-|S_{k-1}|$ nodes, say $w$, the probability of an edge $(u,w)$ existing is $p$. The number of new neighbors discovered from node $u$ thus follows a binomial distribution $B(n-|S_{k-1}|, p)$.\n\nGiven $p = c/n$ and large $n$, we can approximate the binomial distribution $B(m,p)$ with a Poisson distribution of mean $\\lambda = mp$. Here, $m = n - |S_{k-1}| \\approx n$. Thus, the mean is $\\lambda \\approx np = n(c/n) = c$. So, the number of offspring for any given node is well-approximated by a Poisson distribution with mean $c$, denoted as $\\text{Poisson}(c)$.\n\nThe crucial justification for using a GW process is the local tree-like structure of the ER graph in the sparse regime ($p=c/n$). The GW model assumes that the offspring counts of different individuals are independent. In our graph exploration, this translates to the assumption that the neighborhoods of two different discovered nodes $u_1, u_2 \\in S_k$ are disjoint from each other and from previously discovered nodes (apart from their \"parents\"). This would be violated if there were short cycles in the graph. For example, if two nodes $u_1, u_2 \\in Q_{k-1}$ have a common neighbor $w \\notin S_{k-1}$, this is allowed. But if a neighbor of $u_1$ is already in $S_{k-1}$ (and is not $u_1$'s parent), this forms a cycle. The probability of such short cycles is vanishingly small as $n \\to \\infty$ for any fixed $c$. A cycle of length $k$ requires $k$ specific edges to exist in a graph with $\\binom{n}{k}$ possible locations for such a cycle. The expected number of cycles of a fixed length $k$ is approximately $\\frac{(n-1)!}{(n-k)! \\cdot 2k} p^k \\approx \\frac{n^k}{2k} (c/n)^k = \\frac{c^k}{2k}$, which is a constant independent of $n$. The probability of a randomly chosen node belonging to such a cycle is proportional to $1/n$. Thus, for the exploration of a finite-sized component, the probability of encountering a cycle is negligible in the $n \\to \\infty$ limit.\n\nThis justifies modeling the component exploration as a GW branching process where the number of offspring of each individual is drawn independently from a $\\text{Poisson}(c)$ distribution.\n\nLet $Z_k$ be the random variable for the number of nodes in generation $k$ (i.e., $Z_k = |Q_k|$). We start with a single node, so $Z_0=1$.\nThe expected number of offspring of a single individual is the mean of the offspring distribution, which is $\\mu = E[\\text{Poisson}(c)] = c$.\nThe expected size of generation $k$, $E[Z_k]$, can be computed recursively.\n$E[Z_1] = E[\\text{number of offspring of the initial node}] = c$.\n$E[Z_2] = E[E[Z_2 | Z_1]]$. Given $Z_1$ individuals in generation 1, each produces an expected $c$ offspring, so $E[Z_2 | Z_1] = c Z_1$.\nThus, $E[Z_2] = E[c Z_1] = c E[Z_1] = c \\cdot c = c^2$.\nBy induction, the expected size of generation $k$ is $E[Z_k] = c^k E[Z_0] = c^k$.\n\nThe total size of the connected component, $S$, is the sum of the sizes of all generations until the process dies out:\n$$ S = \\sum_{k=0}^{\\infty} Z_k $$\nWe are asked for the expected size of this component, $E[S]$. By the linearity of expectation, we can write:\n$$ E[S] = E\\left[\\sum_{k=0}^{\\infty} Z_k\\right] = \\sum_{k=0}^{\\infty} E[Z_k] $$\nSubstituting the expression for $E[Z_k]$:\n$$ E[S] = \\sum_{k=0}^{\\infty} c^k $$\nThis is a geometric series. The problem states that $0 < c < 1$. For this range of $c$, the series converges. The sum of the geometric series is given by the formula $\\frac{a}{1-r}$, where $a$ is the first term and $r$ is the common ratio.\nIn our case, the first term ($k=0$) is $c^0 = 1$, and the common ratio is $c$.\nTherefore, the expected size of the component is:\n$$ E[S] = \\frac{1}{1-c} $$\nThis result holds for the subcritical regime ($c<1$). When $c>1$, the GW process has a non-zero probability of surviving forever, which corresponds to the emergence of a \"giant component\" whose size is proportional to $n$. In that case, the sum $\\sum c^k$ would diverge, reflecting the infinite expected size of the branching process (though the component size in the graph remains finite, scaling with $n$). The analysis here is strictly for the subcritical case where components are small.",
            "answer": "$$\\boxed{\\frac{1}{1-c}}$$"
        },
        {
            "introduction": "Moving from the statistical properties of random ensembles to the analysis of a specific, given network, we need efficient algorithms to probe its structure. This exercise challenges you to design and implement a classic algorithm based on Depth-First Search (DFS) to identify all articulation points, or cut vertices, in a graph. Mastering this low-link algorithm provides a fundamental tool for assessing network vulnerability and understanding the nodes that are critical for maintaining connectivity .",
            "id": "4270105",
            "problem": "Consider a finite, simple, undirected graph $G = (V, E)$ with $|V| = n$ and $|E| = m$, where $V = \\{0,1,\\dots,n-1\\}$ and $E \\subseteq \\{\\{u,v\\} \\mid u,v \\in V, u \\neq v\\}$. A connected component is a maximal subset $C \\subseteq V$ such that every pair of vertices in $C$ is connected by a path in $G$. An articulation point (also called a cut vertex) is a vertex $a \\in V$ whose removal (together with its incident edges) strictly increases the number of connected components. Design an algorithm based on Depth-First Search (DFS) low-link values to identify all articulation points in $G$ and analyze its complexity from first principles, starting only from core definitions of connected components, DFS tree structure, and the notion of back-edges and tree-edges. Your algorithm must handle disconnected graphs by initiating Depth-First Search (DFS) from each unvisited vertex and must justify the articulation criteria in terms of the low-link computation and discovery indices without relying on pre-stated shortcut formulas.\n\nYour program must implement the designed algorithm and, for each test graph provided below, output two quantities:\n- The sorted list of all articulation points (as vertex indices).\n- The size (an integer) of the largest connected component after removing all articulation points (if all vertices are removed, the size is $0$).\n\nUse the following test suite. For each test case, the graph is specified by $n$ and a list of undirected edges given as unordered pairs:\n- Test case $1$ (general multi-block case): $n = 8$, edges $E = \\{(0,1),(1,2),(2,0),(2,3),(3,4),(4,5),(5,3),(5,6),(6,7)\\}$.\n- Test case $2$ (tree star): $n = 5$, edges $E = \\{(0,1),(0,2),(0,3),(0,4)\\}$.\n- Test case $3$ (complete graph): $n = 5$, edges $E = \\{(u,v) \\mid 0 \\le u < v \\le 4\\}$, i.e., all pairs on $\\{0,1,2,3,4\\}$.\n- Test case $4$ (disconnected mixed structure): $n = 6$, edges $E = \\{(0,1),(1,2),(2,0),(3,4),(4,5)\\}$.\n- Test case $5$ (single vertex): $n = 1$, edges $E = \\emptyset$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element of this list corresponds to one test case and must itself be a list with two elements: the first element is the sorted list of articulation points, and the second element is the integer size of the largest connected component after removing those articulation points. For example, the output format must be\n$[r_1,r_2,\\dots,r_5]$\nwhere each $r_i$ is of the form $[\\text{list of integers}, \\text{integer}]$.\n\nNo physical units are involved. Angles are not involved. Percentages are not involved. The answers must be expressed only using fundamental types (booleans, integers, floats, or lists of these types), as specified above.\n\nThe problem requires:\n- Starting from core definitions of connected components and Depth-First Search (DFS), derive a low-link-based criterion that correctly identifies articulation points, and explain why it is correct.\n- Describe and justify the algorithmic steps needed to compute discovery indices and low-link values over an undirected graph, including handling of back-edges and tree-edges, and the special role of DFS roots versus non-root vertices.\n- Analyze the time and space complexity in terms of $n$ and $m$ without resorting to shortcut formulas, and relate the complexity to the underlying per-edge and per-vertex operations.\n\nYour implementation must then apply the algorithm to the test suite and print the results in the specified single-line format.",
            "solution": "The problem requires the design, analysis, and implementation of an algorithm to identify all articulation points (or cut vertices) in a finite, simple, undirected graph $G = (V, E)$, and subsequently to determine the size of the largest component after the removal of these articulation points. The algorithm must be derived from first principles using Depth-First Search (DFS) and the concept of low-link values.\n\n### 1. Foundational Concepts: DFS Tree and Edge Classification\n\nA Depth-First Search traversal of a graph $G$ systematically explores the graph's vertices and edges. The traversal begins at an arbitrary starting vertex and explores as far as possible along each branch before backtracking. This process naturally defines a spanning forest of the graph, known as the DFS forest (or a single DFS tree if the graph is connected). The edges of the graph $E$ can be classified based on this DFS forest:\n\n- **Tree Edge:** An edge $(u, v)$ is a tree edge if vertex $v$ was discovered for the first time during the exploration of vertex $u$. The set of all tree edges constitutes the DFS spanning forest.\n- **Back Edge:** An edge $(u, v)$ is a back edge if vertex $v$ is an ancestor of vertex $u$ in the DFS tree. A crucial special case in undirected graphs is that the edge leading from a vertex $u$ to its parent in the DFS tree is technically a back edge by this definition, but we must exclude it from consideration as a back edge during our analysis to avoid trivial cycles.\n\nAn articulation point is a vertex whose removal increases the number of connected components. We can derive the conditions for a vertex to be an articulation point by analyzing its role within the DFS tree structure.\n\n### 2. Derivation of Articulation Point Conditions\n\nLet us consider an arbitrary vertex $u \\in V$. We analyze the effect of removing $u$ based on its position in the DFS tree.\n\n**Case 1: The vertex $u$ is the root of a DFS tree.**\nThe root of a DFS tree is the starting point of a traversal for a connected component. When we perform a DFS from $u$, we generate a set of subtrees rooted at its children. All paths between any two of these distinct subtrees must pass through $u$. If we remove $u$, these subtrees become disconnected from each other. If the root $u$ has two or more children in the DFS tree, its removal will split the single component into at least two components (one for each child's subtree). If $u$ has zero or one child, its removal does not disconnect its descendants from each other (as there is at most one group of descendants). Therefore, a root vertex $u$ is an articulation point if and only if it has more than one child in the DFS tree.\n\n**Case 2: The vertex $u$ is not the root of a DFS tree.**\nA non-root vertex $u$ has a unique parent, let's call it $p$, in the DFS tree. The removal of $u$ disconnects its parent $p$ from any of its children in the DFS tree, say $v$. The component containing $p$ and the component containing the subtree rooted at $v$ will become separated unless there is an alternative path between the subtree at $v$ and the part of the graph containing $p$ (and $u$'s other ancestors) that does not pass through $u$. Such an alternative path must be formed by a back edge from some vertex $w$ in the subtree of $v$ (where $w$ could be $v$ itself) to a proper ancestor of $u$ (i.e., $p$ or an ancestor of $p$). If for some child $v$ of $u$, no vertex in the subtree rooted at $v$ has a back edge to a proper ancestor of $u$, then all paths from that subtree to the rest of the graph must pass through $u$. In this scenario, removing $u$ will sever the connection, increasing the number of components.\nTherefore, a non-root vertex $u$ is an articulation point if and only if it has at least one child $v$ in the DFS tree such that there is no back edge from the subtree rooted at $v$ to any proper ancestor of $u$.\n\n### 3. Formalization with Discovery Times and Low-Link Values\n\nTo formalize this condition algorithmically, we introduce two metrics computed during the DFS traversal:\n- **Discovery Time `discovery[u]`**: A timestamp assigned to a vertex $u$ when it is first visited. We can use a simple counter that increments for each newly discovered vertex. By definition, if $u$ is an ancestor of $v$ in the DFS tree, then `discovery[u]` $<$ `discovery[v]`.\n- **Low-Link Value `lowlink[u]`**: The lowest discovery time reachable from $u$ by traversing zero or more tree edges and then at most one back edge.\n\nThe `lowlink[u]` value can be computed recursively. During the DFS from a vertex $u$:\n1.  Initialize `lowlink[u]` to `discovery[u]`. This represents the case of using zero back edges.\n2.  For each neighbor $v$ of $u$:\n    a. If $(u,v)$ is a tree edge (i.e., $v$ is an unvisited child of $u$): we recursively compute `lowlink[v]`. After the recursive call returns, we update `lowlink[u] = min(lowlink[u], lowlink[v])`. This is because if $v$'s subtree can reach an ancestor with a low discovery time, so can $u$ by going through $v$.\n    b. If $(u,v)$ is a back edge (and $v$ is not the parent of $u$): we update `lowlink[u] = min(lowlink[u], discovery[v])`. We use `discovery[v]` because $v$ is an ancestor, and this back edge provides a shortcut to it.\n\nNow, we can rephrase the articulation point condition for a non-root vertex $u$ and its child $v$:\nThe condition that the subtree at $v$ has no back edge to a *proper ancestor* of $u$ is equivalent to stating that the lowest discovery time reachable from $v$'s subtree is at least `discovery[u]`. In our terminology, this is exactly the condition `lowlink[v] >= discovery[u]`. If this inequality holds, it means the subtree at $v$ can, at best, reach back to $u$ itself but no higher up in the DFS tree. Thus, $u$ is the sole connection point for that subtree to the rest of the graph.\n\n### 4. The Algorithm\n\n**Part 1: Finding Articulation Points**\nThe complete algorithm combines these ideas:\n1.  Initialize data structures: an adjacency list for the graph, `discovery` and `lowlink` arrays of size $n$ (initialized to a sentinel value like $-1$), a `parent` array, a set `articulation_points`, and a time counter `time = 0`.\n2.  Iterate through each vertex $i \\in \\{0, 1, \\dots, n-1\\}$. If vertex $i$ has not been visited, start a DFS from $i$. This ensures all connected components (and thus all DFS trees in the forest) are visited.\n3.  The recursive DFS function, `DFS(u, p)`, performs the following:\n    a. Mark $u$ as visited. Set `discovery[u] = lowlink[u] = time`, and increment `time`. Set `parent[u] = p`.\n    b. Initialize a counter for children in the DFS tree, `children_count = 0`.\n    c. For each neighbor $v$ of $u$:\n        i. If $v$ is the parent $p$, ignore this edge.\n        ii. If $v$ has been visited, it means $(u, v)$ is a back edge. Update `lowlink[u] = min(lowlink[u], discovery[v])`.\n        iii. If $v$ has not been visited:\n            - Increment `children_count`.\n            - Recursively call `DFS(v, u)`.\n            - After the call returns, update `lowlink[u] = min(lowlink[u], lowlink[v])`.\n            - **Articulation Point Check**:\n                - If $p$ is null (i.e., $u$ is a root) and `children_count > 1`, add $u$ to `articulation_points`.\n                - If $p$ is not null (i.e., $u$ is not a root) and `lowlink[v] >= discovery[u]`, add $u$ to `articulation_points`.\n4.  After the initial loop completes, `articulation_points` contains the full set of cut vertices.\n\n**Part 2: Computing the Largest Component Size**\n1.  Let $A$ be the set of identified articulation points.\n2.  Construct the remaining graph $G'$ which consists of the vertices $V' = V \\setminus A$ and all edges from $E$ that connect two vertices in $V'$.\n3.  Find the sizes of all connected components in $G'$. This can be done by another series of traversals (DFS or BFS) on the vertices of $V'$.\n4.  Maintain a `max_size` variable. For each component found, update `max_size` to be the maximum of its current value and the newly found component's size.\n5.  If $V'$ is empty (i.e., all vertices were articulation points), the size is $0$. Otherwise, the final `max_size` is the answer.\n\n### 5. Complexity Analysis\n\n- **Time Complexity:** The core of the algorithm is a single DFS traversal over the entire graph. The main loop ensures that every vertex is visited exactly once. The DFS function, for each vertex `u`, iterates through its adjacency list. Over the entire execution, each edge $(u, v)$ is considered twice: once from $u$'s adjacency list and once from $v$'s. Thus, the total work for the DFS part is proportional to the sum of the degrees of all vertices, which is $2m$. The initialization of arrays takes $O(n)$ time. Therefore, finding all articulation points takes $O(n+m)$ time. The subsequent step of finding the largest component involves another graph traversal on the subgraph, which also takes at an upper bound $O(n+m)$ time. The total time complexity is therefore $O(n+m)$.\n\n- **Space Complexity:** The algorithm requires storage for:\n    - The adjacency list representation of the graph: $O(n+m)$.\n    - The `discovery`, `lowlink`, `parent`, and `visited` arrays: each takes $O(n)$ space.\n    - The recursion stack for DFS: in the worst case (a path graph), the recursion depth can be $n$, requiring $O(n)$ space.\n    - The set of articulation points: at most $O(n)$ space.\nThe dominant term is the adjacency list, so the total space complexity is $O(n+m)$.",
            "answer": "```python\nimport sys\nimport numpy as np\n\n# It is a good practice to increase recursion limit for deep graphs in DFS.\nsys.setrecursionlimit(2000)\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test suite.\n    \"\"\"\n    test_cases = [\n        # Test case 1 (general multi-block case)\n        (8, [(0, 1), (1, 2), (2, 0), (2, 3), (3, 4), (4, 5), (5, 3), (5, 6), (6, 7)]),\n        # Test case 2 (tree star)\n        (5, [(0, 1), (0, 2), (0, 3), (0, 4)]),\n        # Test case 3 (complete graph)\n        (5, [(u, v) for u in range(5) for v in range(u + 1, 5)]),\n        # Test case 4 (disconnected mixed structure)\n        (6, [(0, 1), (1, 2), (2, 0), (3, 4), (4, 5)]),\n        # Test case 5 (single vertex)\n        (1, []),\n    ]\n\n    results = []\n    for n, edges in test_cases:\n        result = find_aps_and_largest_component(n, edges)\n        results.append(result)\n\n    # Convert results to the required string format\n    # The string representation of a list in Python matches the required format '[item1, item2]'.\n    output_str = f\"[{','.join(map(str, results))}]\"\n    print(output_str)\n\ndef find_aps_and_largest_component(n, edges):\n    \"\"\"\n    Finds articulation points and the size of the largest component after their removal.\n\n    Args:\n        n (int): The number of vertices.\n        edges (list of tuples): The list of edges in the graph.\n\n    Returns:\n        list: A list containing two elements:\n              - A sorted list of articulation points.\n              - An integer representing the size of the largest component.\n    \"\"\"\n    if n == 0:\n        return [[], 0]\n    \n    adj = [[] for _ in range(n)]\n    for u, v in edges:\n        adj[u].append(v)\n        adj[v].append(u)\n\n    # --- Part 1: Find Articulation Points using Tarjan's low-link algorithm ---\n    discovery = np.full(n, -1, dtype=int)\n    lowlink = np.full(n, -1, dtype=int)\n    parent = np.full(n, -1, dtype=int)\n    articulation_points = set()\n    time = 0\n\n    def find_aps_dfs(u):\n        nonlocal time\n        discovery[u] = lowlink[u] = time\n        time += 1\n        children_count = 0\n\n        for v in adj[u]:\n            if v == parent[u]:\n                continue\n            \n            if discovery[v] != -1:  # Back edge\n                lowlink[u] = min(lowlink[u], discovery[v])\n            else:  # Tree edge\n                children_count += 1\n                parent[v] = u\n                find_aps_dfs(v)\n                lowlink[u] = min(lowlink[u], lowlink[v])\n\n                # Articulation point conditions\n                # 1. u is the root of DFS tree and has more than one child.\n                if parent[u] == -1 and children_count > 1:\n                    articulation_points.add(u)\n                # 2. u is not root and low value of one of its children is more\n                #    than or equal to discovery time of u.\n                if parent[u] != -1 and lowlink[v] >= discovery[u]:\n                    articulation_points.add(u)\n\n    for i in range(n):\n        if discovery[i] == -1:\n            find_aps_dfs(i)\n    \n    sorted_aps = sorted(list(articulation_points))\n\n    # --- Part 2: Calculate size of largest component after removing APs ---\n    if len(sorted_aps) == n:\n        # If all vertices are articulation points, the remaining graph is empty.\n        largest_component_size = 0\n    else:\n        ap_set = set(sorted_aps)\n        visited_cc = set()\n        largest_component_size = 0\n\n        for i in range(n):\n            if i not in ap_set and i not in visited_cc:\n                current_component_size = 0\n                q = [i]\n                visited_cc.add(i)\n                head = 0\n                while head < len(q):\n                    u = q[head]\n                    head += 1\n                    current_component_size += 1\n                    for v in adj[u]:\n                        if v not in ap_set and v not in visited_cc:\n                            visited_cc.add(v)\n                            q.append(v)\n                largest_component_size = max(largest_component_size, current_component_size)\n\n    return [sorted_aps, largest_component_size]\n\nsolve()\n```"
        },
        {
            "introduction": "This final practice introduces a powerful and elegant perspective from spectral graph theory, connecting a graph's connectivity directly to the properties of its Laplacian matrix. You will first prove the fundamental theorem that the number of connected components equals the multiplicity of the zero eigenvalue of the graph Laplacian. You will then implement a numerical algorithm using sparse eigensolvers to count components, a method that is both theoretically insightful and highly practical for analyzing large-scale networks .",
            "id": "4270126",
            "problem": "Consider a finite, undirected, weighted graph with $n$ vertices labeled $0,1,\\dots,n-1$ and a symmetric weight matrix $W \\in \\mathbb{R}^{n \\times n}$ with entries $W_{ij} \\ge 0$ that represent edge weights. There are no self-loops, so $W_{ii} = 0$ for all $i$. Define the degree of vertex $i$ as $d_i = \\sum_{j=0}^{n-1} W_{ij}$ and the combinatorial graph Laplacian $L \\in \\mathbb{R}^{n \\times n}$ as $L = D - W$, where $D$ is the diagonal matrix with diagonal entries $d_0,\\dots,d_{n-1}$. A connected component is a maximal subset of vertices such that for any two vertices $u$ and $v$ in the subset there exists a path of edges with strictly positive weights connecting $u$ to $v$. The multiplicity of the zero eigenvalue of $L$ is defined as the dimension of the nullspace of $L$, that is, the number of linearly independent eigenvectors associated to the eigenvalue $0$.\n\nPart $1$: Starting only from the definitions above and the basic properties of real symmetric matrices, derive why the number of connected components of the graph equals the multiplicity of the zero eigenvalue of the Laplacian $L$. Your derivation must proceed from first principles without using any shortcut formulas beyond what is stated, and it must explain what the property is, why it is true, and how it follows from the definitions.\n\nPart $2$: Design an algorithm that, given a symmetric sparse weight matrix $W$ with nonnegative entries and zero diagonal, constructs $L$ and uses a sparse eigensolver to count the number of connected components by estimating the multiplicity of the zero eigenvalue. The algorithm must:\n- Use an eigensolver that targets the algebraically smallest eigenvalues of $L$ and count how many are numerically equal to zero within a tolerance that is scaled by the magnitude of $L$.\n- Adapt the number $k$ of requested eigenvalues so that, when the number of numerically zero eigenvalues equals $k$ and $k < n-1$, the algorithm increases $k$ until either $k = n-1$ or the count of zero eigenvalues is strictly less than $k$, ensuring all zero eigenvalues are captured.\n- Correctly handle the edge case of an empty graph where $W$ has no nonzero entries, for which the Laplacian is the zero matrix and the number of connected components is $n$.\n\nImplement this algorithm as a complete, runnable program that takes no input and uses a fixed internal test suite. Use the following five test cases, each specified by the number of vertices $n$ and an undirected edge list $(i,j,w)$ with $i \\neq j$, $w > 0$, and implicit symmetry $(j,i,w)$:\n\nTest case $1$ (a single path, connected): $n=5$; edges $\\{(0,1,1),(1,2,1),(2,3,1),(3,4,1)\\}$.\n\nTest case $2$ (two components: a path of length $2$ and a path of length $1$): $n=5$; edges $\\{(0,1,1),(1,2,1),(3,4,1)\\}$.\n\nTest case $3$ (all isolated vertices): $n=4$; edges $\\{\\}$.\n\nTest case $4$ (three components: a triangle, a single edge, and an isolated vertex): $n=6$; edges $\\{(0,1,1),(1,2,1),(2,0,1),(3,4,1)\\}$.\n\nTest case $5$ (weighted: a $4$-cycle with positive weights plus one isolated vertex): $n=5$; edges $\\{(0,1,0.5),(1,2,2.0),(2,3,1.5),(3,0,1.0)\\}$.\n\nYour program must:\n- Construct $W$ from each test case and form $L = D - W$.\n- Use a sparse eigensolver to compute the algebraically smallest eigenvalues of $L$ and count how many are numerically zero based on a tolerance scaled by the largest diagonal entry of $L$.\n- Aggregate the integer results for the five test cases into a single line of output in the exact format: a comma-separated list enclosed in square brackets. For example, if the results are $a_1,a_2,a_3,a_4,a_5$, print the single line $[a_1,a_2,a_3,a_4,a_5]$.",
            "solution": "### Part 1: Derivation\n\nThe objective is to prove that for a weighted undirected graph, the number of its connected components, denoted by $c$, is equal to the multiplicity of the eigenvalue $0$ of its combinatorial Laplacian matrix $L$. The multiplicity is the dimension of the nullspace of $L$, denoted $\\text{dim}(\\text{null}(L))$. We are given $L = D - W$, where $W$ is the symmetric weight matrix with $W_{ij} \\ge 0$ and $W_{ii} = 0$, and $D$ is the diagonal degree matrix with $d_i = \\sum_{j} W_{ij}$. The nullspace of $L$ is the set of vectors $\\mathbf{v} \\in \\mathbb{R}^n$ such that $L\\mathbf{v} = \\mathbf{0}$. We must show that $\\text{dim}(\\text{null}(L)) = c$.\n\nThe proof proceeds in two main steps: first, we establish a fundamental property of the Laplacian matrix via its quadratic form, and second, we use this property to relate the structure of vectors in its nullspace to the connected components of the graph.\n\n**Step 1: The Laplacian Quadratic Form**\n\nLet $\\mathbf{v} \\in \\mathbb{R}^n$ be an arbitrary vector, with components $v_0, v_1, \\dots, v_{n-1}$. Consider the quadratic form $\\mathbf{v}^T L \\mathbf{v}$.\n$$ \\mathbf{v}^T L \\mathbf{v} = \\mathbf{v}^T (D-W) \\mathbf{v} = \\mathbf{v}^T D \\mathbf{v} - \\mathbf{v}^T W \\mathbf{v} $$\nThe first term is $\\mathbf{v}^T D \\mathbf{v} = \\sum_{i=0}^{n-1} d_i v_i^2$. By the definition of the degree $d_i$, we can write this as:\n$$ \\sum_{i=0}^{n-1} d_i v_i^2 = \\sum_{i=0}^{n-1} \\left( \\sum_{j=0}^{n-1} W_{ij} \\right) v_i^2 = \\sum_{i,j} W_{ij} v_i^2 $$\nThe second term is $\\mathbf{v}^T W \\mathbf{v} = \\sum_{i,j} W_{ij} v_i v_j$.\nSubstituting these into the expression for $\\mathbf{v}^T L \\mathbf{v}$:\n$$ \\mathbf{v}^T L \\mathbf{v} = \\sum_{i,j} W_{ij} v_i^2 - \\sum_{i,j} W_{ij} v_i v_j $$\nSince the weight matrix $W$ is symmetric ($W_{ij} = W_{ji}$), we can manipulate the first sum:\n$$ \\sum_{i,j} W_{ij} v_i^2 = \\frac{1}{2} \\sum_{i,j} W_{ij} v_i^2 + \\frac{1}{2} \\sum_{i,j} W_{ij} v_i^2 $$\nBy swapping the summation indices $i$ and $j$ in the second part and using symmetry of $W$:\n$$ \\frac{1}{2} \\sum_{j,i} W_{ji} v_j^2 = \\frac{1}{2} \\sum_{i,j} W_{ij} v_j^2 $$\nSo, we can write:\n$$ \\mathbf{v}^T L \\mathbf{v} = \\frac{1}{2} \\sum_{i,j} W_{ij} (v_i^2 + v_j^2) - \\sum_{i,j} W_{ij} v_i v_j $$\nCombining the terms under a single summation:\n$$ \\mathbf{v}^T L \\mathbf{v} = \\frac{1}{2} \\sum_{i,j} W_{ij} (v_i^2 - 2v_i v_j + v_j^2) = \\frac{1}{2} \\sum_{i,j} W_{ij} (v_i - v_j)^2 $$\nThis identity is fundamental. Since edge weights are non-negative ($W_{ij} \\ge 0$) and $(v_i - v_j)^2 \\ge 0$, it follows that $\\mathbf{v}^T L \\mathbf{v} \\ge 0$ for any vector $\\mathbf{v}$. This demonstrates that the Laplacian matrix $L$ is positive semi-definite, which implies all its eigenvalues $\\lambda$ are non-negative ($\\lambda \\ge 0$).\n\n**Step 2: Relating the Nullspace to Connected Components**\n\nAn eigenvector $\\mathbf{v}$ corresponding to the eigenvalue $\\lambda = 0$ must satisfy $L\\mathbf{v} = \\mathbf{0}$. For such a vector, the quadratic form must be zero: $\\mathbf{v}^T L \\mathbf{v} = 0$.\nUsing the identity from Step 1:\n$$ \\frac{1}{2} \\sum_{i,j} W_{ij} (v_i - v_j)^2 = 0 $$\nSince this is a sum of non-negative terms, for the sum to be zero, every individual term must be zero. That is, for every pair of indices $(i, j)$:\n$$ W_{ij} (v_i - v_j)^2 = 0 $$\nThis condition has a crucial implication: if there is an edge with strictly positive weight between vertex $i$ and vertex $j$ (i.e., $W_{ij} > 0$), then it must be that $(v_i - v_j)^2 = 0$, which implies $v_i = v_j$.\n\nBy definition, a connected component is a maximal set of vertices where any two vertices are connected by a path of positive-weight edges. Let's consider a single connected component. If vertices $u$ and $v$ are in the same component, there exists a path $p_0, p_1, \\dots, p_k$ where $u=p_0$, $v=p_k$, and $W_{p_{m}p_{m+1}} > 0$ for all $m \\in \\{0, \\dots, k-1\\}$. Applying the condition $v_i = v_j$ for each edge along this path, we get $v_{p_0} = v_{p_1} = \\dots = v_{p_k}$. Therefore, for any eigenvector $\\mathbf{v}$ associated with eigenvalue $0$, its components must be constant across all vertices within a single connected component.\n\nLet the graph have $c$ connected components, $C_1, C_2, \\dots, C_c$. Based on the above reasoning, any vector $\\mathbf{v} \\in \\text{null}(L)$ must take a constant value on the vertices of each component. Let this constant value for component $C_k$ be $\\alpha_k$. So, for any $i \\in C_k$, $v_i = \\alpha_k$.\n\nThis means any vector $\\mathbf{v}$ in the nullspace can be expressed as a linear combination of a specific set of vectors. Let us define a set of $c$ indicator vectors $\\{\\mathbf{u}_1, \\mathbf{u}_2, \\dots, \\mathbf{u}_c\\}$, where for each $k \\in \\{1, \\dots, c\\}$, the $i$-th component of $\\mathbf{u}_k$ is:\n$$ (\\mathbf{u}_k)_i = \\begin{cases} 1 & \\text{if vertex } i \\in C_k \\\\ 0 & \\text{if vertex } i \\notin C_k \\end{cases} $$\nThen any vector $\\mathbf{v} \\in \\text{null}(L)$ can be written as:\n$$ \\mathbf{v} = \\sum_{k=1}^c \\alpha_k \\mathbf{u}_k $$\nThis shows that the vectors $\\{\\mathbf{u}_1, \\dots, \\mathbf{u}_c\\}$ span the nullspace of $L$.\n\nTo complete the proof, we must show that these vectors are linearly independent. Consider a linear combination of these vectors that equals the zero vector:\n$$ \\sum_{k=1}^c \\beta_k \\mathbf{u}_k = \\mathbf{0} $$\nThe vectors $\\mathbf{u}_k$ have disjoint support; that is, for any vertex $i$, $(\\mathbf{u}_k)_i$ is non-zero for exactly one value of $k$. If we consider the $i$-th component of the equation, where $i \\in C_j$, the sum reduces to $\\beta_j (\\mathbf{u}_j)_i = 0$. Since $(\\mathbf{u}_j)_i=1$, it must be that $\\beta_j=0$. Since this holds for any component $j$, all coefficients $\\beta_k$ must be zero. Thus, the set $\\{\\mathbf{u}_1, \\dots, \\mathbf{u}_c\\}$ is linearly independent.\n\nSince the set $\\{\\mathbf{u}_1, \\dots, \\mathbf{u}_c\\}$ is a linearly independent set that spans the nullspace of $L$, it forms a basis for $\\text{null}(L)$. The dimension of the nullspace is the number of vectors in its basis, which is $c$.\nTherefore, we have proven that the multiplicity of the zero eigenvalue of the graph Laplacian $L$ is equal to the number of connected components $c$ of the graph.\n\n### Part 2: Algorithm Design\n\nThe algorithm counts the number of connected components by computing the multiplicity of the zero eigenvalue of the graph Laplacian $L$. It utilizes a sparse representation for matrices to handle potentially large graphs efficiently and an iterative eigensolver to find the smallest eigenvalues.\n\n1.  **Matrix Construction**: Given the number of vertices $n$ and a list of weighted edges, a sparse weight matrix $W$ is constructed using a coordinate format, which is then converted to a Compressed Sparse Row (CSR) matrix for efficient arithmetic operations. Since the graph is undirected, for each edge $(i, j, w)$, entries for both $W_{ij}$ and $W_{ji}$ are set to $w$. The diagonal degree matrix $D$ is formed by summing the rows of $W$ and placing the resulting degrees on the diagonal of a sparse matrix. The Laplacian is then $L = D - W$.\n\n2.  **Eigensolver Strategy**: The core of the algorithm is to find the number of eigenvalues of $L$ that are numerically zero. Since $L$ is symmetric and positive semi-definite, its eigenvalues are real and non-negative. We use `scipy.sparse.linalg.eigsh`, an eigensolver for real symmetric matrices, configured to find the eigenvalues with the smallest algebraic value ('SA').\n\n3.  **Numerical Tolerance**: A direct comparison to zero is not robust for floating-point numbers. A tolerance $\\tau$ is established, scaled by the magnitude of $L$. A suitable scale factor is the largest diagonal entry of $L$, which is the maximum vertex degree, $\\max(d_i)$. The tolerance is $\\tau = \\max(d_i) \\cdot \\epsilon$, where $\\epsilon$ is a small machine-precision-related constant (e.g., $10^{-9}$). If all degrees are zero (empty graph), a small absolute tolerance is used. An eigenvalue $\\lambda$ is considered numerically zero if $|\\lambda| < \\tau$.\n\n4.  **Adaptive Eigenvalue Search**: It is not known a priori how many zero eigenvalues exist. The algorithm must adaptively determine this. It starts by requesting a small number of eigenvalues, $k=1$.\n    -   It computes the $k$ smallest eigenvalues. Let the number of these that are numerically zero be $c_0$.\n    -   If $c_0 < k$, it means at least one non-zero eigenvalue was found. Since the eigenvalues are sorted, all eigenvalues smaller than this one must be zero. Thus, $c_0$ is the total count of zero eigenvalues, and the search terminates.\n    -   If $c_0 = k$, all eigenvalues found are zero. It is possible that more zero eigenvalues exist. The algorithm increases $k$ (e.g., by doubling it, $k \\to 2k$) and repeats the process. This continues until the condition $c_0 < k$ is met or until $k$ reaches its maximum possible value.\n\n5.  **Edge Cases**:\n    -   The `eigsh` solver requires the number of requested eigenvalues $k$ to be less than $n$. The adaptive search for $k$ is therefore capped at $n-1$.\n    -   If the algorithm requests $k=n-1$ eigenvalues and finds all of them to be zero, a special condition arises. For a non-empty graph, the trace of $L$, $\\text{Tr}(L) = \\sum d_i$, is positive and equals the sum of all eigenvalues. If $n-1$ eigenvalues are zero, the last eigenvalue must be $\\lambda_n = \\text{Tr}(L) > 0$. So the multiplicity is $n-1$. However, if the graph is empty ($W=0$), then $L=0$, $\\text{Tr}(L)=0$, and all $n$ eigenvalues are zero. The adaptive logic handles this: if it finds $n-1$ zero eigenvalues, it checks the trace. If the trace is numerically zero, it concludes there are $n$ components (isolated vertices); otherwise, there are $n-1$. This correctly implements the logic from the problem description for handling the empty graph case.\n\nThis adaptive sparse eigensolver approach is both computationally efficient for large sparse networks and numerically robust.",
            "answer": "```python\nimport numpy as np\nfrom scipy.sparse import csr_matrix, diags\nfrom scipy.sparse.linalg import eigsh, norm\n\ndef count_connected_components(n, edges):\n    \"\"\"\n    Counts the number of connected components of a graph using its Laplacian matrix.\n\n    This function constructs the graph Laplacian L=D-W and finds the multiplicity\n    of its zero eigenvalue, which equals the number of connected components. It\n    uses an adaptive sparse eigensolver to find all numerically zero eigenvalues.\n\n    Args:\n        n (int): The number of vertices in the graph.\n        edges (list of tuples): A list of (u, v, w) tuples representing edges.\n\n    Returns:\n        int: The number of connected components.\n    \"\"\"\n    if n == 0:\n        return 0\n    if n == 1:\n        return 1\n\n    # If there are no edges, every vertex is a component.\n    if not edges:\n        return n\n\n    # Construct the sparse weight matrix W\n    row, col, data = [], [], []\n    for u, v, w in edges:\n        row.extend([u, v])\n        col.extend([v, u])\n        data.extend([w, w])\n    \n    W = csr_matrix((data, (row, col)), shape=(n, n))\n\n    # Construct the sparse degree matrix D and Laplacian L\n    degrees = np.array(W.sum(axis=1)).flatten()\n    D = diags(degrees, 0)\n    L = D - W\n    \n    # Tolerance for a value to be considered zero.\n    # The problem suggests scaling by the largest diagonal entry of L, which is max(degrees).\n    max_degree = degrees.max()\n    # Use a small absolute tolerance if graph is empty (max_degree=0),\n    # although the initial check for empty edge list handles this.\n    tol = max_degree * 1e-9 if max_degree > 0 else 1e-9\n\n    # Adaptive algorithm to find the number of zero eigenvalues.\n    k = 1 # Start by searching for 1 eigenvalue.\n    while True:\n        # eigsh requires k to be less than the matrix dimension n.\n        k_to_ask = min(k, n - 1)\n        \n        # We assume convergence for these well-posed test cases.\n        # In a production setting, a try-except block for ArpackError would be robust.\n        eigenvalues = eigsh(L, k=k_to_ask, which='SA', return_eigenvectors=False)\n        \n        num_zero = np.sum(np.abs(eigenvalues) < tol)\n\n        if num_zero < k_to_ask:\n            # We found a non-zero eigenvalue, so all smaller ones (which we also\n            # have) constitute the full set of zero eigenvalues.\n            return num_zero\n        \n        # If we asked for k_to_ask eigenvalues and all were zero:\n        if k_to_ask == n - 1:\n            # We found n-1 zero eigenvalues. The nth eigenvalue is Tr(L) - sum(n-1 zeros).\n            # Tr(L) = sum(degrees). This is zero only for an empty graph.\n            # If trace is zero, all n eigenvalues are zero -> n components.\n            # Otherwise, the nth eigenvalue is > 0 -> n-1 components.\n            if np.isclose(degrees.sum(), 0.0, atol=tol):\n                return n\n            else:\n                return n - 1\n        \n        # All k_to_ask eigenvalues were zero, and we haven't checked up to n-1 yet.\n        # Increase k to search for more. Double it for efficiency.\n        k *= 2\n\n\ndef solve():\n    \"\"\"\n    Main function to run the algorithm on the specified test cases.\n    \"\"\"\n    test_cases = [\n        (5, [(0, 1, 1), (1, 2, 1), (2, 3, 1), (3, 4, 1)]),\n        (5, [(0, 1, 1), (1, 2, 1), (3, 4, 1)]),\n        (4, []),\n        (6, [(0, 1, 1), (1, 2, 1), (2, 0, 1), (3, 4, 1)]),\n        (5, [(0, 1, 0.5), (1, 2, 2.0), (2, 3, 1.5), (3, 0, 1.0)]),\n    ]\n\n    results = []\n    for n, edges in test_cases:\n        num_components = count_connected_components(n, edges)\n        results.append(num_components)\n\n    # Print the final result in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        }
    ]
}