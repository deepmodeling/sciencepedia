{
    "hands_on_practices": [
        {
            "introduction": "Hierarchy in networks is often characterized by explicit levels. A fundamental question is how these levels influence connection patterns, and this exercise introduces the concept of *level assortativity* to quantify this. By deriving and applying the formula for assortativity based on the Pearson correlation coefficient, you will gain hands-on experience in formalizing and measuring the tendency of nodes to connect to others at similar or different hierarchical levels. ",
            "id": "4281144",
            "problem": "Consider an undirected network representing a hierarchical system with nodes labeled $1$ through $8$, and an attribute $\\ell_i \\in \\mathbb{N}$ indicating the hierarchical level of node $i$. The levels are defined as $\\ell_1 = 0$, $\\ell_2 = 1$, $\\ell_3 = 1$, $\\ell_4 = 2$, $\\ell_5 = 2$, $\\ell_6 = 2$, $\\ell_7 = 2$, and $\\ell_8 = 3$. The network has the following undirected edges:\n$$(1,2),\\ (1,3),\\ (2,4),\\ (2,5),\\ (3,6),\\ (3,7),\\ (4,5),\\ (6,7),\\ (5,8),\\ (7,8),\\ (2,3).$$\nUsing the definition of the Pearson correlation coefficient (PCC), define the level assortativity coefficient $r_{\\mathrm{level}}$ of an undirected network as the correlation between the node levels at the two ends of an edge when edges are sampled uniformly at random without orientation. Starting from the basic definition of the correlation between two real-valued random variables, derive a formula for $r_{\\mathrm{level}}$ applicable to undirected networks with a scalar attribute on nodes. Then, compute $r_{\\mathrm{level}}$ exactly for the network above. Express the final answer as a single reduced rational number. No rounding is required.",
            "solution": "The problem is assessed to be valid as it is scientifically grounded in network science, well-posed, objective, and contains all necessary information for a unique solution.\n\nThe problem requires the derivation of a general formula for the level assortativity coefficient $r_{\\mathrm{level}}$ and its computation for a specific network. The level assortativity coefficient is defined as the Pearson correlation coefficient (PCC) of the levels of nodes at the ends of an edge.\n\nFirst, we derive the general formula. Let the network be described by a set of nodes $V$ and a set of undirected edges $E$. The number of nodes is $N = |V|$ and the number of edges is $M = |E|$. Each node $i \\in V$ has a scalar attribute, its level, denoted by $\\ell_i$.\n\nThe Pearson correlation coefficient $\\rho_{X,Y}$ for two random variables $X$ and $Y$ is defined as:\n$$\n\\rho_{X,Y} = \\frac{\\mathrm{Cov}(X,Y)}{\\sigma_X \\sigma_Y} = \\frac{E[XY] - E[X]E[Y]}{\\sqrt{E[X^2] - (E[X])^2} \\sqrt{E[Y^2] - (E[Y])^2}}\n$$\nTo apply this to our network, we must define the sample space and the random variables. The problem states we are sampling edges \"uniformly at random without orientation\". A standard and rigorous way to formalize this for an undirected network is to consider a sample space of $2M$ directed edges, where each undirected edge $(i,j) \\in E$ corresponds to two directed edges, $i \\to j$ and $j \\to i$. Each of these $2M$ directed edges is chosen with equal probability $\\frac{1}{2M}$.\n\nLet $X$ and $Y$ be the random variables for the level of the source and target node of a randomly chosen directed edge, respectively. For a directed edge $i \\to j$, the observed values are $(\\ell_i, \\ell_j)$.\n\nThe set of values for $X$ is $\\{\\ell_i | \\text{for each directed edge } i \\to j\\}$, and for $Y$ is $\\{\\ell_j | \\text{for each directed edge } i \\to j\\}$. Because the network is undirected, the set of all source nodes is the same as the set of all target nodes, and the distributions of $X$ and $Y$ are identical. Thus, $E[X] = E[Y]$ and $\\sigma_X = \\sigma_Y$. The formula for $r_{\\mathrm{level}}$ simplifies to:\n$$\nr_{\\mathrm{level}} = \\frac{E[XY] - (E[X])^2}{E[X^2] - (E[X])^2}\n$$\nWe now express the expectation values in terms of network properties. Let $d_i$ be the degree of node $i$. In the set of $2M$ directed edges, a node $i$ appears as a source node $d_i$ times.\nThe expected value of $X$ is:\n$$\nE[X] = \\sum_{i \\to j} \\frac{1}{2M} \\ell_i = \\frac{1}{2M} \\sum_{i \\in V} d_i \\ell_i\n$$\nThe expected value of $X^2$ is:\n$$\nE[X^2] = \\sum_{i \\to j} \\frac{1}{2M} \\ell_i^2 = \\frac{1}{2M} \\sum_{i \\in V} d_i \\ell_i^2\n$$\nThe joint expectation $E[XY]$ is taken over all $2M$ directed edges:\n$$\nE[XY] = \\sum_{i \\to j} \\frac{1}{2M} \\ell_i \\ell_j = \\frac{1}{2M} \\sum_{(i,j) \\in E} (\\ell_i \\ell_j + \\ell_j \\ell_i) = \\frac{2}{2M} \\sum_{(i,j) \\in E} \\ell_i \\ell_j = \\frac{1}{M} \\sum_{(i,j) \\in E} \\ell_i \\ell_j\n$$\nSubstituting these expressions into the formula for $r_{\\mathrm{level}}$:\n$$\nr_{\\mathrm{level}} = \\frac{\\frac{1}{M} \\sum_{(i,j) \\in E} \\ell_i \\ell_j - \\left(\\frac{1}{2M} \\sum_{k \\in V} d_k \\ell_k\\right)^2}{\\frac{1}{2M} \\sum_{k \\in V} d_k \\ell_k^2 - \\left(\\frac{1}{2M} \\sum_{k \\in V} d_k \\ell_k\\right)^2}\n$$\nTo simplify the expression, we multiply the numerator and denominator by $4M^2$:\n$$\nr_{\\mathrm{level}} = \\frac{4M \\sum_{(i,j) \\in E} \\ell_i \\ell_j - \\left(\\sum_{k \\in V} d_k \\ell_k\\right)^2}{2M \\sum_{k \\in V} d_k \\ell_k^2 - \\left(\\sum_{k \\in V} d_k \\ell_k\\right)^2}\n$$\nThis is the general formula for the assortativity coefficient for a scalar attribute on the nodes of an undirected network.\n\nNow, we compute $r_{\\mathrm{level}}$ for the given network.\nThe network has $N=8$ nodes and $M=11$ edges.\nThe levels are: $\\ell_1=0$, $\\ell_2=1$, $\\ell_3=1$, $\\ell_4=2$, $\\ell_5=2$, $\\ell_6=2$, $\\ell_7=2$, $\\ell_8=3$.\nThe edges are: $(1,2), (1,3), (2,4), (2,5), (3,6), (3,7), (4,5), (6,7), (5,8), (7,8), (2,3)$.\n\nFirst, we calculate the degree $d_i$ for each node $i$:\n$d_1 = 2$ (edges to $2, 3$)\n$d_2 = 4$ (edges to $1, 4, 5, 3$)\n$d_3 = 4$ (edges to $1, 6, 7, 2$)\n$d_4 = 2$ (edges to $2, 5$)\n$d_5 = 3$ (edges to $2, 4, 8$)\n$d_6 = 2$ (edges to $3, 7$)\n$d_7 = 3$ (edges to $3, 6, 8$)\n$d_8 = 2$ (edges to $5, 7$)\nThe sum of degrees is $\\sum_{i=1}^8 d_i = 2+4+4+2+3+2+3+2=22$, which correctly equals $2M = 2 \\times 11$.\n\nNext, we compute the three necessary sums for the formula:\n$1$. $S_1 = \\sum_{k \\in V} d_k \\ell_k$:\n$S_1 = d_1\\ell_1 + d_2\\ell_2 + d_3\\ell_3 + d_4\\ell_4 + d_5\\ell_5 + d_6\\ell_6 + d_7\\ell_7 + d_8\\ell_8$\n$S_1 = (2)(0) + (4)(1) + (4)(1) + (2)(2) + (3)(2) + (2)(2) + (3)(2) + (2)(3)$\n$S_1 = 0 + 4 + 4 + 4 + 6 + 4 + 6 + 6 = 34$\n\n$2$. $S_2 = \\sum_{k \\in V} d_k \\ell_k^2$:\n$S_2 = d_1\\ell_1^2 + d_2\\ell_2^2 + d_3\\ell_3^2 + d_4\\ell_4^2 + d_5\\ell_5^2 + d_6\\ell_6^2 + d_7\\ell_7^2 + d_8\\ell_8^2$\n$S_2 = (2)(0^2) + (4)(1^2) + (4)(1^2) + (2)(2^2) + (3)(2^2) + (2)(2^2) + (3)(2^2) + (2)(3^2)$\n$S_2 = 0 + 4 + 4 + 8 + 12 + 8 + 12 + 18 = 66$\n\n$3$. $S_3 = \\sum_{(i,j) \\in E} \\ell_i \\ell_j$:\n$S_3 = \\ell_1\\ell_2 + \\ell_1\\ell_3 + \\ell_2\\ell_4 + \\ell_2\\ell_5 + \\ell_3\\ell_6 + \\ell_3\\ell_7 + \\ell_4\\ell_5 + \\ell_6\\ell_7 + \\ell_5\\ell_8 + \\ell_7\\ell_8 + \\ell_2\\ell_3$\n$S_3 = (0)(1) + (0)(1) + (1)(2) + (1)(2) + (1)(2) + (1)(2) + (2)(2) + (2)(2) + (2)(3) + (2)(3) + (1)(1)$\n$S_3 = 0 + 0 + 2 + 2 + 2 + 2 + 4 + 4 + 6 + 6 + 1 = 29$\n\nNow we substitute these values into the formula for $r_{\\mathrm{level}}$, with $M=11$:\n$$\nr_{\\mathrm{level}} = \\frac{4M S_3 - S_1^2}{2M S_2 - S_1^2} = \\frac{4(11)(29) - (34)^2}{2(11)(66) - (34)^2}\n$$\nCalculate the numerator:\n$4(11)(29) = 44 \\times 29 = 1276$\n$34^2 = 1156$\nNumerator = $1276 - 1156 = 120$\n\nCalculate the denominator:\n$2(11)(66) = 22 \\times 66 = 1452$\n$34^2 = 1156$\nDenominator = $1452 - 1156 = 296$\n\nSo, the level assortativity coefficient is:\n$$\nr_{\\mathrm{level}} = \\frac{120}{296}\n$$\nFinally, we reduce the fraction to its simplest form. We find the greatest common divisor of $120$ and $296$:\n$120 = 12 \\times 10 = (2^2 \\times 3) \\times (2 \\times 5) = 2^3 \\times 3 \\times 5$\n$296 = 4 \\times 74 = 2^2 \\times (2 \\times 37) = 2^3 \\times 37$\nThe greatest common divisor is $2^3 = 8$.\n$$\nr_{\\mathrm{level}} = \\frac{120 \\div 8}{296 \\div 8} = \\frac{15}{37}\n$$\nThe exact value of the level assortativity coefficient for the given network is $\\frac{15}{37}$.",
            "answer": "$$\\boxed{\\frac{15}{37}}$$"
        },
        {
            "introduction": "Beyond explicit levels, hierarchy can be understood through the lens of influence and control, such as which nodes can reach many others. This practice introduces Global Reaching Centrality (GRC), a metric that captures the heterogeneity of reachability in a directed network. You will then employ degree-preserving rewiring—a standard technique for generating null models—to investigate how much of this hierarchical structure is a non-trivial feature of the network's specific wiring, versus being a simple consequence of its degree distribution. ",
            "id": "4281098",
            "problem": "You are given the task to formalize and compute how degree-preserving rewiring in directed networks impacts a hierarchy measure known as Global Reaching Centrality. Your program must start from core definitions in complex systems and network science, without relying on any shortcuts or heuristics that assume the answer. A directed graph is defined by a node set of size $N$ and a set of directed edges. The in-degree and out-degree sequences of a directed graph are the per-node counts of incoming and outgoing edges, respectively. A degree-preserving rewiring is any transformation of the edge set that leaves both the in-degree and out-degree sequences unchanged for every node. A standard microscopic move that preserves the degree sequences is the double-edge swap: pick two distinct edges $(u \\to v)$ and $(x \\to y)$ and, if valid, replace them by $(u \\to y)$ and $(x \\to v)$; validity requires that no self-loops are created, no parallel edges are introduced (i.e., the graph remains a simple directed graph), and the two replaced edges are not identical to the originals.\n\nA node $i$'s local reaching centrality is defined as the fraction of other nodes it can reach via directed paths. For a directed graph with $N$ nodes, let the reachability count $R_i$ be the number of distinct nodes reachable from node $i$ along directed paths, not counting $i$ itself. The local reaching centrality is then $c(i) = R_i / (N-1)$. The Global Reaching Centrality (GRC) characterizes the heterogeneity of local reaching centralities by aggregating the distances from the maximum local reaching centrality $c_{\\max} = \\max_j c(j)$ across all nodes. You must derive and implement the computation of this network-level measure based on these definitions.\n\nYour program must:\n- Implement a computation of directed reachability for each node $i$ that counts $R_i$, and from that compute $c(i)$ for every node $i$, and then compute the Global Reaching Centrality as an aggregation driven by $c_{\\max}$ and the set $\\{c(i)\\}$.\n- Implement degree-preserving rewiring based on repeated valid double-edge swaps, as specified above. Let $E$ denote the number of directed edges. For a given rewiring intensity parameter $f \\in [0,1]$, perform exactly $S = \\lfloor f \\cdot E \\rfloor$ successful swaps. Randomness must be controlled by a given integer seed $s$ for reproducibility.\n- For each test case provided below, compute the Global Reaching Centrality for the original graph, rewire the graph with the specified $(f,s)$, and compute the Global Reaching Centrality again. Return the triple consisting of the initial value, the post-rewiring value, and their difference (post minus pre).\n\nUse the following test suite, where nodes are labeled $0,1,\\dots,N-1$ and each edge is an ordered pair $(u,v)$ representing a directed edge $u \\to v$. All numbers provided here (including $N$, $E$, $f$, and $s$ values) must be treated literally.\n\n- Test case $1$ (hierarchical arborescence):\n  - $N = 9$\n  - Edge set: $(0,1)$, $(0,2)$, $(0,3)$, $(1,4)$, $(1,5)$, $(2,6)$, $(3,7)$, $(3,8)$\n  - $f = 1.0$\n  - Seed $s = 42$\n\n- Test case $2$ (directed ring):\n  - $N = 8$\n  - Edge set: $(0,1)$, $(1,2)$, $(2,3)$, $(3,4)$, $(4,5)$, $(5,6)$, $(6,7)$, $(7,0)$\n  - $f = 1.0$\n  - Seed $s = 7$\n\n- Test case $3$ (feedforward directed acyclic graph):\n  - $N = 10$\n  - Edge set: $(0,2)$, $(0,3)$, $(1,3)$, $(1,4)$, $(2,5)$, $(2,6)$, $(3,6)$, $(4,7)$, $(5,8)$, $(6,8)$, $(7,9)$, $(8,9)$\n  - $f = 0.5$\n  - Seed $s = 123$\n\n- Test case $4$ (boundary no-rewire case):\n  - $N = 7$\n  - Edge set: $(0,1)$, $(0,2)$, $(1,3)$, $(2,3)$, $(3,4)$, $(4,5)$, $(5,6)$\n  - $f = 0.0$\n  - Seed $s = 99$\n\nFinal output format:\n- Your program must produce a single line containing a flat list of $3$ floating-point numbers per test case in order, concatenated into one list. For each test case, output the initial Global Reaching Centrality, the post-rewiring Global Reaching Centrality, and their difference (post minus pre). Concatenate these triples for all test cases into a single list. Each floating-point number must be rounded to exactly $6$ decimal places. The final line must look like\n  - $[\\text{r}_{1,1},\\text{r}_{1,2},\\text{r}_{1,3},\\text{r}_{2,1},\\dots,\\text{r}_{4,3}]$\n  where $\\text{r}_{k,j}$ is the $j$-th result for test case $k$.",
            "solution": "The problem is deemed valid after a rigorous inspection. It is scientifically grounded in network science, well-posed, and objective. The definition of Global Reaching Centrality (GRC) is stated as an aggregation of distances from the maximum local centrality. While the exact aggregation function is not specified, there is a standard definition in the scientific literature which I will adopt. The problem is solvable through a combination of standard graph algorithms.\n\nThe solution proceeds in three main stages: first, formalizing and calculating the Global Reaching Centrality (GRC); second, implementing the degree-preserving rewiring process; and third, applying these to each test case.\n\n### 1. Global Reaching Centrality (GRC)\n\nThe problem defines the local reaching centrality $c(i)$ for a node $i$ in a directed graph with $N$ nodes as the fraction of other nodes it can reach. Let $R_i$ be the number of distinct nodes reachable from node $i$ via directed paths (excluding node $i$ itself). Then, the local reaching centrality is:\n$$c(i) = \\frac{R_i}{N-1}$$\nThis value is well-defined for $N > 1$.\n\nThe Global Reaching Centrality (GRC) is described as a measure of the heterogeneity of these local centralities, based on the deviation from the maximum observed centrality, $c_{\\max} = \\max_j c(j)$. Following the standard definition from complex systems literature, we formalize GRC as the normalized sum of these deviations over all nodes in the network:\n$$GRC = \\frac{\\sum_{i=0}^{N-1} (c_{\\max} - c(i))}{N-1}$$\nThis formulation is also well-defined for $N>1$. A GRC of $0$ indicates that all nodes have the same local reaching centrality (an \"egalitarian\" structure), while a higher GRC indicates greater heterogeneity, characteristic of a hierarchical structure where a few nodes can reach many others, while most nodes have very limited reach.\n\nTo compute the GRC, we must first compute $R_i$ for every node $i \\in \\{0, 1, \\dots, N-1\\}$. This is a standard reachability problem on a directed graph. For each node $i$, we can perform a graph traversal, such as Breadth-First Search (BFS) or Depth-First Search (DFS), starting from $i$. The set of all visited nodes, excluding the start node $i$, constitutes the set of reachable nodes. The size of this set is $R_i$. The overall algorithm to calculate GRC is:\n1. For each node $i=0, \\dots, N-1$:\n   a. Perform a graph traversal (e.g., BFS) starting from $i$ to find all reachable nodes.\n   b. Count the number of reachable nodes to get $R_i$.\n   c. Calculate $c(i) = R_i / (N-1)$.\n2. Find the maximum local centrality, $c_{\\max} = \\max_i c(i)$.\n3. Compute the GRC using the formula: $GRC = (\\sum_i (c_{\\max} - c(i))) / (N-1)$.\n\n### 2. Degree-Preserving Rewiring\n\nThe problem specifies a degree-preserving rewiring mechanism: the double-edge swap. This procedure modifies the network's topology while ensuring that the in-degree and out-degree of every node remain unchanged. The process is as follows:\n1. Two distinct directed edges, $(u \\to v)$ and $(x \\to y)$, are selected randomly from the graph's edge set.\n2. An attempt is made to replace them with a new pair of edges: $(u \\to y)$ and $(x \\to v)$.\n3. This swap is considered \"successful\" only if it adheres to the following validity conditions for a simple directed graph:\n   a. **No self-loops**: The new edges must not be self-loops, meaning $u \\neq y$ and $x \\neq v$.\n   b. **No parallel edges**: The new edges $(u \\to y)$ and $(x \\to v)$ must not already exist in the graph.\n   c. **Non-triviality**: The set of new edges must be different from the set of original edges. This is true if and only if $u \\neq x$ and $v \\neq y$. This condition ensures the swap actually modifies the graph structure.\n\nFor a given graph with $E$ edges and a rewiring intensity parameter $f \\in [0,1]$, the algorithm must perform exactly $S = \\lfloor f \\cdot E \\rfloor$ successful swaps. To achieve this, we repeatedly select pairs of edges and test them against the validity conditions until $S$ successful swaps have been accumulated. Randomness is managed by a seed $s$ for reproducibility.\n\nThe algorithm for rewiring is thus:\n1. Initialize a pseudorandom number generator with the given seed $s$.\n2. Let the current edge set be $\\mathcal{E}$. Let $E = |\\mathcal{E}|$.\n3. Calculate the target number of swaps, $S = \\lfloor f \\cdot E \\rfloor$.\n4. If $S > 0$, enter a loop to perform $S$ successful swaps:\n   a. Repeatedly select two distinct edges $(u, v)$ and $(x, y)$ from the current edge list.\n   b. Check if the swap to $(u, y)$ and $(x, v)$ is valid (no self-loops, no parallel edges, non-trivial).\n   c. If valid, update the edge set: $\\mathcal{E} \\leftarrow (\\mathcal{E} \\setminus \\{(u,v), (x,y)\\}) \\cup \\{(u,y), (x,v)\\}$. This constitutes one successful swap. Increment the success counter and proceed to the next swap.\n   d. If invalid, discard the selection and try again with a new pair of random edges.\n\n### 3. Overall Procedure for each Test Case\n\nFor each test case, defined by the number of nodes $N$, the initial edge set, the rewiring intensity $f$, and the seed $s$, the following steps are executed:\n1. Construct the initial graph from the given $N$ and edge set.\n2. Compute the initial Global Reaching Centrality, $GRC_{initial}$, using the method described in section 1.\n3. Perform the degree-preserving rewiring algorithm as described in section 2, using parameters $f$ and $s$, to obtain a new, rewired graph. If $f=0$, no rewiring occurs, and the graph remains unchanged.\n4. Compute the Global Reaching Centrality of the rewired graph, $GRC_{post}$.\n5. Calculate the difference, $\\Delta_{GRC} = GRC_{post} - GRC_{initial}$.\n6. The final result for the test case is the triple $(GRC_{initial}, GRC_{post}, \\Delta_{GRC})$. These triples are concatenated for all test cases to produce the final output.",
            "answer": "```python\nimport numpy as np\nfrom collections import deque\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases.\n    \"\"\"\n    test_cases = [\n        {\n            \"N\": 9,\n            \"edges\": [(0, 1), (0, 2), (0, 3), (1, 4), (1, 5), (2, 6), (3, 7), (3, 8)],\n            \"f\": 1.0,\n            \"s\": 42\n        },\n        {\n            \"N\": 8,\n            \"edges\": [(0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7), (7, 0)],\n            \"f\": 1.0,\n            \"s\": 7\n        },\n        {\n            \"N\": 10,\n            \"edges\": [\n                (0, 2), (0, 3), (1, 3), (1, 4), (2, 5), (2, 6), \n                (3, 6), (4, 7), (5, 8), (6, 8), (7, 9), (8, 9)\n            ],\n            \"f\": 0.5,\n            \"s\": 123\n        },\n        {\n            \"N\": 7,\n            \"edges\": [(0, 1), (0, 2), (1, 3), (2, 3), (3, 4), (4, 5), (5, 6)],\n            \"f\": 0.0,\n            \"s\": 99\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        N, edges, f, s = case[\"N\"], case[\"edges\"], case[\"f\"], case[\"s\"]\n        initial_grc, post_grc, diff_grc = process_case(N, edges, f, s)\n        results.extend([initial_grc, post_grc, diff_grc])\n\n    # Format output to exactly 6 decimal places.\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\ndef _get_reachable_set(start_node, N, adj):\n    \"\"\"\n    Computes the set of reachable nodes from a given start_node using BFS.\n    \"\"\"\n    if start_node not in adj:\n        return set()\n    \n    q = deque([start_node])\n    visited = {start_node}\n    \n    while q:\n        u = q.popleft()\n        if u in adj:\n            for v in adj[u]:\n                if v not in visited:\n                    visited.add(v)\n                    q.append(v)\n                    \n    return visited - {start_node}\n\ndef calculate_grc(N, edges):\n    \"\"\"\n    Calculates the Global Reaching Centrality for a given graph.\n    \"\"\"\n    if N <= 1:\n        return 0.0\n\n    adj = {i: [] for i in range(N)}\n    for u, v in edges:\n        adj[u].append(v)\n\n    local_centralities = np.zeros(N)\n    for i in range(N):\n        reachable_nodes = _get_reachable_set(i, N, adj)\n        R_i = len(reachable_nodes)\n        local_centralities[i] = R_i / (N - 1)\n\n    if len(local_centralities) == 0:\n        return 0.0\n        \n    c_max = np.max(local_centralities)\n    \n    grc_numerator = np.sum(c_max - local_centralities)\n    grc = grc_numerator / (N - 1)\n    \n    return grc\n\ndef process_case(N, edges, f, s):\n    \"\"\"\n    Processes a single test case: calculates initial GRC, performs rewiring,\n    and calculates post-rewiring GRC.\n    \"\"\"\n    # Calculate initial GRC\n    initial_grc = calculate_grc(N, edges)\n\n    # Perform rewiring\n    E = len(edges)\n    num_swaps = int(f * E)\n    \n    if num_swaps == 0:\n        post_grc = initial_grc\n    else:\n        rng = np.random.default_rng(seed=s)\n        edge_set = set(edges)\n        edge_list = list(edges)\n\n        successful_swaps = 0\n        # Add a failsafe for potentially difficult-to-rewire topologies\n        max_attempts = 100 * E * num_swaps if num_swaps > 0 else 1\n        attempts = 0\n\n        while successful_swaps < num_swaps and attempts < max_attempts:\n            attempts += 1\n            if len(edge_list) < 2:\n                break # Not enough edges to perform a swap\n            \n            # 1. Pick two distinct edges\n            idx1, idx2 = rng.choice(len(edge_list), size=2, replace=False)\n            u, v = edge_list[idx1]\n            x, y = edge_list[idx2]\n\n            # 2. Check for non-triviality\n            if u == x or v == y:\n                continue\n\n            # Proposed new edges\n            new_edge1 = (u, y)\n            new_edge2 = (x, v)\n\n            # 3. Validity checks\n            # a. No self-loops\n            if u == y or x == v:\n                continue\n            # b. No parallel edges\n            if new_edge1 in edge_set or new_edge2 in edge_set:\n                continue\n            \n            # The swap is valid and successful\n            # Update edge_set\n            edge_set.remove((u, v))\n            edge_set.remove((x, y))\n            edge_set.add(new_edge1)\n            edge_set.add(new_edge2)\n            \n            # Update edge_list for next random sampling\n            edge_list[idx1] = new_edge1\n            edge_list[idx2] = new_edge2\n\n            successful_swaps += 1\n\n        # Calculate post-rewiring GRC\n        post_grc = calculate_grc(N, list(edge_set))\n\n    # Calculate difference\n    diff_grc = post_grc - initial_grc\n\n    return initial_grc, post_grc, diff_grc\n\nsolve()\n```"
        },
        {
            "introduction": "Many hierarchical systems, from biological networks to the internet, lack explicit levels but exhibit self-similarity across different scales. This practice explores this deep connection between hierarchy and fractal geometry by implementing a box-covering algorithm to measure a network's fractal dimension. By analyzing how the number of boxes $N_B(r)$ scales with their radius $r$, and by critically assessing the stability of this scaling, you will learn a powerful, general method for identifying and quantifying implicit hierarchical organization. ",
            "id": "4281053",
            "problem": "You are given the task of implementing a generalized box-covering procedure on undirected, unweighted graphs to study hierarchical self-similarity through the stability of the fractal (box) dimension. Start from the following foundational bases: the definition of a graph, the definition of shortest path distance, and the empirical scaling relation for fractal networks. The goal is to compute, for multiple box radii, the minimal number of boxes required to cover the graph, fit the box-scaling exponent, and decide whether the exponent is stable across scales.\n\nDefinitions and requirements:\n- Let a graph be denoted by $G = (V, E)$, where $V$ is a finite set of nodes with $\\lvert V \\rvert = n$, and $E \\subseteq \\{ \\{u,v\\} \\mid u,v \\in V, u \\neq v \\}$ is a set of undirected edges with no self-loops or multiple edges.\n- Let $d(u,v)$ be the shortest path distance between nodes $u$ and $v$, defined as the minimal number of edges along any simple path from $u$ to $v$ in $G$. If no path exists, the distance is defined as infinite; however, all graphs considered here are connected, so all pairwise distances are finite.\n- For an integer radius $r \\geq 1$, define a radius-$r$ box centered at node $c \\in V$ as the set $B_r(c) = \\{ v \\in V \\mid d(c,v) \\leq r \\}$. A radius-$r$ box covering of the graph is any family of boxes $\\{ B_r(c_i) \\}_{i=1}^{N}$ such that $\\bigcup_{i=1}^{N} B_r(c_i) = V$. The minimal number of boxes for radius $r$ is denoted $N_B(r)$.\n- The box (fractal) dimension $d_B$ is operationally defined by the scaling $N_B(r) \\propto r^{-d_B}$ when this scaling is approximately valid across multiple scales of $r$. In practice, the exponent $d_B$ is estimated from a linear regression of $\\log N_B(r)$ versus $\\log r$, with $d_B$ given by the negative of the slope.\n\nAlgorithmic constraints:\n- Computing the exact minimal $N_B(r)$ is combinatorially hard for general graphs. You must implement a principled greedy approximation grounded in the Maximum Excluded Mass Burning logic: iteratively select as a box center the node that covers the largest number of currently uncovered nodes within radius $r$, mark those nodes as covered, and continue until all nodes are covered. This greedy procedure defines an approximate $N_B(r)$ for each $r$.\n- Compute all-pairs shortest path distances $d(u,v)$ using breadth-first search from each node, ensuring correctness in unweighted graphs.\n\nStability assessment:\n- For a given set of radii $\\{ r_1, r_2, \\dots, r_m \\}$ with $r_i \\in \\mathbb{N}$ and $r_i \\geq 1$, compute $N_B(r_i)$ for all $i$.\n- Fit the line $y = a + b x$ to the points $(x_i, y_i)$ where $x_i = \\log r_i$ and $y_i = \\log N_B(r_i)$ using least squares. The estimated box dimension is $d_B = -b$.\n- Compute the coefficient of determination $R^2$ of the fit, defined as $R^2 = 1 - \\frac{\\sum_{i=1}^{m} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{m} (y_i - \\bar{y})^2}$, where $\\hat{y}_i = a + b x_i$ and $\\bar{y}$ is the mean of the $y_i$ values.\n- Define local slopes between successive scales by $s_i = - \\frac{y_{i+1} - y_i}{x_{i+1} - x_i}$ for $i = 1, \\dots, m-1$. Let $\\mu_s$ be the mean of $\\{ s_i \\}$ and $\\sigma_s$ be their standard deviation. Define the coefficient of variation $\\mathrm{CV} = \\sigma_s / \\mu_s$ when $\\mu_s > 0$.\n- Declare the estimate to be stably fractal (interpreted as evidence of hierarchical self-similarity) if and only if the following conditions hold simultaneously: (i) $m \\geq 3$, (ii) $\\mu_s > 0$, (iii) $\\mathrm{CV} \\leq \\tau$, and (iv) $R^2 \\geq \\rho$. Use $\\tau = 0.15$ and $\\rho = 0.90$.\n- If $m < 3$, you must set the stability decision to false, since stability cannot be assessed across fewer than three scales.\n\nYour program must implement the above and produce results for the following test suite:\n\nTest Suite:\n1. Square lattice graph: Let $G$ be the two-dimensional grid of side length $s = 6$ with $n = 36$ nodes, where nodes are connected to their four Manhattan neighbors when they exist. Use radii $\\{ r \\in \\mathbb{N} \\mid r \\in \\{ 1, 2, 3 \\} \\}$.\n2. Star graph: Let $G$ be a star with one hub and $L = 20$ leaves, so $n = 21$ nodes, with edges $\\{ \\{0,i\\} \\mid i \\in \\{1, \\dots, 20\\} \\}$. Use radii $\\{ r \\in \\mathbb{N} \\mid r \\in \\{ 1, 2 \\} \\}$.\n3. Balanced rooted tree: Let $G$ be a rooted tree with branching factor $b = 3$ and height $h = 3$, with $n = 1 + 3 + 9 + 27 = 40$ nodes, constructed deterministically by connecting each node at level $\\ell$ to $b$ children at level $\\ell + 1$ for $\\ell \\in \\{ 0,1,2 \\}$. Use radii $\\{ r \\in \\mathbb{N} \\mid r \\in \\{ 1, 2, 3, 4 \\} \\}$.\n\nFinal output specification:\n- For each test case, output a two-element list $[d_B, \\text{stable}]$ where $d_B$ is a floating-point number rounded to four decimal places and $\\text{stable}$ is a boolean value with textual form $\\mathrm{True}$ or $\\mathrm{False}$.\n- Aggregate the three per-case outputs into a single list and print it as one line with no spaces, in the exact format $[[d_{B,1},\\mathrm{stable}_1],[d_{B,2},\\mathrm{stable}_2],[d_{B,3},\\mathrm{stable}_3]]$.\n- There are no physical units involved. Angles are not used. All answers are purely numerical or boolean.\n\nYour implementation must be a complete, runnable program that constructs the graphs, computes $N_B(r)$ using the specified greedy algorithm, estimates $d_B$ and $R^2$ via least squares on $\\log$-$\\log$ data, assesses stability using the specified thresholds, and prints the final result in the exact required format.",
            "solution": "The task is to implement a generalized box-covering procedure on graphs to assess hierarchical self-similarity by analyzing the stability of the fractal dimension. This involves several steps: constructing the specified graphs, computing all-pairs shortest paths, applying a greedy algorithm to approximate the minimum number of boxes $N_B(r)$ for various radii $r$, and finally, performing a scaling analysis on the $(r, N_B(r))$ data to determine the box dimension $d_B$ and its stability across scales.\n\nFirst, we represent the graphs $G=(V, E)$ using adjacency lists, a standard and efficient data structure for sparse graphs. The number of nodes is denoted by $n = |V|$. We require functions to construct the three specified graph topologies:\n1.  A two-dimensional square lattice of side length $s$, resulting in $n = s^2$ nodes. Each node $(i, j)$ is connected to its Manhattan neighbors $(i \\pm 1, j)$ and $(i, j \\pm 1)$, provided they are within the grid boundaries.\n2.  A star graph with one central hub node and $L$ leaf nodes, for a total of $n = L+1$ nodes. The hub is connected to all leaves, and there are no connections between leaves.\n3.  A balanced rooted tree with branching factor $b$ and height $h$. The total number of nodes is $n = \\sum_{i=0}^{h} b^i = \\frac{b^{h+1}-1}{b-1}$. The graph is constructed level by level, starting from the root at level $0$.\n\nThe foundation for the box-covering method is the shortest path distance $d(u,v)$ between any two nodes $u, v \\in V$. Since the graphs are unweighted, the shortest path distance is simply the minimum number of edges in a path connecting the nodes. The most direct method to compute all-pairs shortest paths (APSP) in an unweighted graph is to execute a Breadth-First Search (BFS) starting from each node in the graph. For each source node $s$, BFS explores the graph layer by layer, guaranteeing that any node $v$ is reached via a shortest path. The distances from $s$ to all other nodes are stored. By repeating this for all $n$ nodes as the source, we populate an $n \\times n$ distance matrix that stores all $d(u,v)$ values.\n\nThe core of the problem is to determine $N_B(r)$, the minimum number of radius-$r$ boxes needed to cover the graph. A box $B_r(c)$ is the set of all nodes within distance $r$ from a center node $c$. Finding the exact minimum is equivalent to the Set Cover problem, which is NP-hard. Therefore, we use the specified greedy approximation:\n1.  Initialize the set of uncovered nodes $U$ to be the entire set of nodes $V$. Initialize the number of boxes to $0$.\n2.  While $U$ is not empty:\n    a. Increment the box count.\n    b. Identify the optimal box center $c^* \\in V$ that maximizes the number of currently uncovered nodes it can cover, i.e., $c^* = \\arg\\max_{c \\in V} |B_r(c) \\cap U|$.\n    c. Update the set of uncovered nodes by removing all nodes contained in the chosen box: $U \\leftarrow U \\setminus B_r(c^*)$.\n3.  The final box count is the algorithm's estimate for $N_B(r)$.\nThis procedure is repeated for each radius $r$ in the specified set $\\{r_1, r_2, \\dots, r_m\\}$.\n\nThe final step is the scaling analysis. A graph is considered fractal-like if it exhibits power-law scaling $N_B(r) \\propto r^{-d_B}$ over a range of scales $r$. Taking the logarithm of this relation yields a linear equation: $\\log N_B(r) = C - d_B \\log r$, where C is a constant. We can estimate the box dimension $d_B$ by performing a linear least-squares regression on the data points $(x_i, y_i) = (\\log r_i, \\log N_B(r_i))$. The estimated dimension $d_B$ is the negative of the calculated slope.\n\nThe stability of this scaling is assessed using two metrics. First, the coefficient of determination, $R^2 = 1 - \\frac{\\sum (y_i - \\hat{y}_i)^2}{\\sum (y_i - \\bar{y})^2}$, quantifies the goodness of the linear fit. An $R^2$ value close to $1$ indicates that the power-law model is a good description of the data. Second, we analyze the consistency of the scaling exponent across different scales by calculating the local slopes $s_i = - \\frac{\\log N_B(r_{i+1}) - \\log N_B(r_i)}{\\log r_{i+1} - \\log r_i}$. If the scaling is stable, these local slopes should be nearly constant. We quantify their variation using the coefficient of variation, $\\mathrm{CV} = \\sigma_s / \\mu_s$, where $\\mu_s$ and $\\sigma_s$ are the mean and standard deviation of the local slopes $\\{s_i\\}$. A low CV indicates stable scaling.\n\nA graph is declared to have a stable fractal dimension if it is measured across at least $m \\geq 3$ scales, the mean local slope $\\mu_s$ is positive, the fit quality is high ($R^2 \\geq \\rho=0.90$), and the local slope variation is low ($\\mathrm{CV} \\leq \\tau=0.15$). If $m < 3$, a stability assessment is not meaningful, and the result is defined as not stable.\nThe implementation will follow these principles, using `numpy` for efficient numerical calculations such as linear regression, mean, and standard deviation.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom collections import deque\n\ndef create_square_lattice(s):\n    \"\"\"Creates an adjacency list for an s x s square lattice graph.\"\"\"\n    n = s * s\n    adj_list = [[] for _ in range(n)]\n    for i in range(s):\n        for j in range(s):\n            node_idx = i * s + j\n            if i > 0:\n                adj_list[node_idx].append((i - 1) * s + j)\n            if i < s - 1:\n                adj_list[node_idx].append((i + 1) * s + j)\n            if j > 0:\n                adj_list[node_idx].append(i * s + (j - 1))\n            if j < s - 1:\n                adj_list[node_idx].append(i * s + (j + 1))\n    return adj_list\n\ndef create_star_graph(L):\n    \"\"\"Creates an adjacency list for a star graph with L leaves.\"\"\"\n    n = L + 1\n    adj_list = [[] for _ in range(n)]\n    hub = 0\n    for leaf in range(1, n):\n        adj_list[hub].append(leaf)\n        adj_list[leaf].append(hub)\n    return adj_list\n\ndef create_balanced_tree(b, h):\n    \"\"\"Creates an adjacency list for a balanced rooted tree.\"\"\"\n    if b == 1:\n        n = h + 1\n    else:\n        n = (b**(h + 1) - 1) // (b - 1)\n\n    adj_list = [[] for _ in range(n)]\n    if n <= 1:\n        return adj_list\n    \n    current_node = 1\n    parents = [0]\n    for _ in range(h):\n        next_parents = []\n        for p in parents:\n            for _ in range(b):\n                if current_node < n:\n                    adj_list[p].append(current_node)\n                    adj_list[current_node].append(p)\n                    next_parents.append(current_node)\n                    current_node += 1\n        parents = next_parents\n    return adj_list\n\ndef calculate_apsp(adj_list):\n    \"\"\"Calculates all-pairs shortest paths using BFS from each node.\"\"\"\n    n = len(adj_list)\n    dist_matrix = np.full((n, n), -1, dtype=int)\n    for i in range(n):\n        dist_matrix[i, i] = 0\n        q = deque([(i, 0)])\n        visited = {i}\n        while q:\n            u, d = q.popleft()\n            for v in adj_list[u]:\n                if v not in visited:\n                    visited.add(v)\n                    dist_matrix[i, v] = d + 1\n                    q.append((v, d + 1))\n    return dist_matrix\n\ndef compute_nb_greedy(dist_matrix, r):\n    \"\"\"Computes N_B(r) using the specified greedy algorithm.\"\"\"\n    n = dist_matrix.shape[0]\n    uncovered_nodes = set(range(n))\n    num_boxes = 0\n    \n    potential_covers = [\n        {v for v in range(n) if dist_matrix[c, v] <= r} for c in range(n)\n    ]\n\n    while uncovered_nodes:\n        num_boxes += 1\n        best_center = -1\n        max_newly_covered_count = -1\n        \n        for c in range(n):\n            newly_covered_count = len(potential_covers[c].intersection(uncovered_nodes))\n            if newly_covered_count > max_newly_covered_count:\n                max_newly_covered_count = newly_covered_count\n                best_center = c\n        \n        if max_newly_covered_count == 0:\n            break\n            \n        nodes_in_chosen_box = potential_covers[best_center]\n        uncovered_nodes.difference_update(nodes_in_chosen_box)\n        \n    return num_boxes\n\ndef analyze_scaling(radii, nb_values):\n    \"\"\"Analyzes scaling to find d_B and stability.\"\"\"\n    m = len(radii)\n    tau = 0.15\n    rho = 0.90\n    \n    if m < 3:\n        stable = False\n        if m < 2:\n            db = 0.0\n        else: # m == 2\n            x = np.log(radii)\n            # This check is for safety, problem constraints should prevent it.\n            if np.isclose(x[1], x[0]): \n                db = 0.0\n                return db, stable\n            y = np.log(nb_values)\n            if np.isclose(y[1], y[0]):\n                db = 0.0\n            else:\n                b = (y[1] - y[0]) / (x[1] - x[0])\n                db = -b\n        return db, stable\n\n    x = np.log(radii)\n    y = np.log(nb_values)\n    \n    # Handle cases with no variation in N_B(r)\n    if np.allclose(y, y[0]):\n        return 0.0, False\n\n    b, a = np.polyfit(x, y, 1)\n    db = -b\n    \n    y_pred = b * x + a\n    ss_res = np.sum((y - y_pred)**2)\n    ss_tot = np.sum((y - np.mean(y))**2)\n    r_squared = 1 - (ss_res / ss_tot)\n        \n    local_slopes = -np.diff(y) / np.diff(x)\n    \n    mu_s = np.mean(local_slopes)\n    \n    if mu_s <= 0 or len(local_slopes) < 2:\n        return db, False\n        \n    # Use ddof=1 for sample standard deviation\n    sigma_s = np.std(local_slopes, ddof=1)\n    cv = sigma_s / mu_s\n    \n    stable = (mu_s > 0) and (cv <= tau) and (r_squared >= rho)\n    \n    return db, stable\n\ndef solve():\n    test_suite = [\n        {'type': 'lattice', 'params': {'s': 6}, 'radii': [1, 2, 3]},\n        {'type': 'star', 'params': {'L': 20}, 'radii': [1, 2]},\n        {'type': 'tree', 'params': {'b': 3, 'h': 3}, 'radii': [1, 2, 3, 4]} \n    ]\n    \n    final_results = []\n    \n    for test in test_suite:\n        if test['type'] == 'lattice':\n            adj = create_square_lattice(**test['params'])\n        elif test['type'] == 'star':\n            adj = create_star_graph(**test['params'])\n        elif test['type'] == 'tree':\n            adj = create_balanced_tree(**test['params'])\n            \n        dist_matrix = calculate_apsp(adj)\n        \n        radii = test['radii']\n        nb_values = [compute_nb_greedy(dist_matrix, r) for r in radii]\n            \n        db, stable = analyze_scaling(radii, nb_values)\n        \n        final_results.append(f\"[{db:.4f},{str(stable)}]\")\n        \n    print(f\"[[{','.join(final_results)}]]\")\n\nsolve()\n```"
        }
    ]
}