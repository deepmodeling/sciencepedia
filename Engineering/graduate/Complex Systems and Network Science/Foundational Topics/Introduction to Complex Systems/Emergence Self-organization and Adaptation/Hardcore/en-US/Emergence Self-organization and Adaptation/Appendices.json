{
    "hands_on_practices": [
        {
            "introduction": "Spontaneous synchronization is a canonical example of emergence in complex systems, observed in phenomena ranging from the coordinated flashing of fireflies to the stable operation of power grids. The Kuramoto model offers a foundational and mathematically elegant framework for understanding how a population of diverse, weakly coupled oscillators can spontaneously transition from an incoherent state to one of collective synchrony. This exercise  challenges you to apply self-consistent mean-field theory, a powerful analytical tool in statistical physics, to derive the precise critical coupling strength at which this emergent order appears, connecting macroscopic behavior to microscopic interaction rules.",
            "id": "4274111",
            "problem": "Consider a population of $N$ phase oscillators with natural frequencies $\\{\\omega_i\\}_{i=1}^{N}$ drawn independently from a normalized, unimodal, even probability density function (PDF) $g(\\omega)$ that is continuous at $\\omega=0$ and satisfies $\\int_{-\\infty}^{\\infty} g(\\omega)\\,d\\omega = 1$. The oscillators evolve under all-to-all sinusoidal coupling according to the Kuramoto model\n$$\n\\frac{d\\theta_i}{dt} \\;=\\; \\omega_i \\;+\\; \\frac{K}{N} \\sum_{j=1}^{N} \\sin\\!\\big(\\theta_j - \\theta_i\\big),\n$$\nwhere $\\theta_i(t)\\in \\mathbb{R}$ is the phase of oscillator $i$ and $K\\ge 0$ is the uniform coupling strength. Define the complex order parameter\n$$\nz(t) \\;=\\; r(t)\\,\\exp\\!\\big(i\\psi(t)\\big) \\;=\\; \\frac{1}{N} \\sum_{j=1}^{N} \\exp\\!\\big(i\\theta_j(t)\\big),\n$$\nwith coherence $r(t)\\in[0,1]$ and mean phase $\\psi(t)\\in\\mathbb{R}$. In the thermodynamic limit $N\\to\\infty$, adopt the continuum description in terms of a phase density $f(\\theta,\\omega,t)$ for oscillators with frequency $\\omega$, satisfying $\\int_{0}^{2\\pi} f(\\theta,\\omega,t)\\,d\\theta = 1$ for each $\\omega$ and evolving under the continuity equation associated with the deterministic velocity field generated by the coupling.\n\nUsing self-consistent mean-field analysis, and starting from the above foundational definitions and laws, derive an explicit closed-form analytic expression for the critical coupling strength $K$ at which the incoherent state with $r=0$ loses stability and a nonzero coherence $r>0$ first emerges continuously. Express your final answer solely in terms of $g(0)$. No numerical approximation or rounding is required, and no physical units should be included in the final expression. Clearly state any regularity assumptions on $g(\\omega)$ that you use in the derivation and justify any limiting procedures involved in the onset analysis.",
            "solution": "The starting point is the Kuramoto model for a population of $N$ oscillators, described by the equations:\n$$\n\\frac{d\\theta_i}{dt} \\;=\\; \\omega_i \\;+\\; \\frac{K}{N} \\sum_{j=1}^{N} \\sin(\\theta_j - \\theta_i)\n$$\nThe complex order parameter $z(t) = r(t)e^{i\\psi(t)}$ is defined as $z(t) = \\frac{1}{N} \\sum_{j=1}^{N} e^{i\\theta_j(t)}$. We can rewrite the sine term using this definition.\nThe sum over $j$ is $\\sum_{j=1}^{N} \\sin(\\theta_j - \\theta_i) = \\sum_{j=1}^{N} \\text{Im}[e^{i(\\theta_j - \\theta_i)}] = \\text{Im}[e^{-i\\theta_i} \\sum_{j=1}^{N} e^{i\\theta_j}] = \\text{Im}[e^{-i\\theta_i} N z(t)]$.\nSubstituting $z(t) = r(t)e^{i\\psi(t)}$, this becomes $N \\cdot \\text{Im}[e^{-i\\theta_i} r(t)e^{i\\psi(t)}] = N r(t) \\sin(\\psi(t) - \\theta_i)$.\nThe equation of motion for a single oscillator is thus effectively coupled to the mean field represented by $r(t)$ and $\\psi(t)$:\n$$\n\\frac{d\\theta_i}{dt} \\;=\\; \\omega_i \\;+\\; K r(t) \\sin(\\psi(t) - \\theta_i)\n$$\nIn the thermodynamic limit $N\\to\\infty$, we describe the system using a probability density function $f(\\theta, \\omega, t)$ for oscillators with natural frequency $\\omega$ to have phase $\\theta$ at time $t$. The dynamics of any oscillator with frequency $\\omega$ is governed by the same mean-field equation:\n$$\n\\frac{d\\theta}{dt} \\;=\\; v(\\theta, \\omega, t) \\;=\\; \\omega \\;+\\; K r(t) \\sin(\\psi(t) - \\theta)\n$$\nThe order parameter is now given by an integral over all frequencies and phases:\n$$\nr(t)e^{i\\psi(t)} \\;=\\; \\int_{-\\infty}^{\\infty} g(\\omega) \\left[ \\int_{0}^{2\\pi} f(\\theta, \\omega, t) e^{i\\theta} d\\theta \\right] d\\omega\n$$\nWe seek a stationary solution where the coherence $r$ is constant and the mean phase $\\psi$ rotates at a constant frequency $\\Omega$, i.e., $\\psi(t) = \\Omega t + \\psi_0$. Due to the rotational symmetry of the system, we can move to a co-rotating frame by defining a new phase variable $\\phi = \\theta - \\psi(t)$. The mean phase in this frame is $0$. Without loss of generality, let's assume the overall mean frequency is zero by a suitable shift of all $\\omega_i$. Since the given frequency distribution $g(\\omega)$ is even, its mean is already zero, which implies $\\Omega = 0$ for the stationary synchronized state. Thus, we can set $\\psi(t)=0$ and look for a time-independent solution.\n\nThe stationary velocity field is $v(\\theta, \\omega) = \\omega - K r \\sin(\\theta)$. The phase density $f(\\theta, \\omega)$ must satisfy the stationary continuity equation $\\frac{\\partial}{\\partial \\theta}[f \\cdot v] = 0$. This implies $f(\\theta, \\omega) \\cdot (\\omega - Kr\\sin\\theta)$ is a constant with respect to $\\theta$. For the density to be normalizable over the periodic domain $[0, 2\\pi]$, this constant must be zero. This leads to two types of solutions:\n1.  Locked oscillators: For these, $v(\\theta, \\omega) = 0$ at a stable phase $\\theta_s$. This requires $|\\omega| \\le Kr$. The stable fixed point is $\\theta_s = \\arcsin(\\omega / (Kr))$. The density is a delta function: $f(\\theta, \\omega) = \\delta(\\theta - \\arcsin(\\omega/(Kr)))$.\n2.  Drifting oscillators: For these, $|\\omega| > Kr$, the velocity $\\omega - Kr\\sin\\theta$ never becomes zero. The oscillators drift around the circle with a non-uniform speed. Their stationary distribution is $f(\\theta, \\omega) = \\frac{C}{\\omega - Kr\\sin\\theta}$, where $C$ is a normalization constant determined by $\\int_0^{2\\pi} f(\\theta, \\omega)d\\theta=1$. This gives $f(\\theta, \\omega) = \\frac{\\sqrt{\\omega^2 - (Kr)^2}}{2\\pi(\\omega - Kr\\sin\\theta)}$.\n\nThe self-consistency equation for the order parameter $r$ is obtained by substituting these stationary distributions back into the definition of $r$ (with $\\psi=0$, so $z=r$ is real):\n$$\nr \\;=\\; \\int_{-\\infty}^{\\infty} g(\\omega) \\left[ \\int_{0}^{2\\pi} f(\\theta, \\omega) \\cos\\theta \\, d\\theta \\right] d\\omega\n$$\nThe integral splits into contributions from the locked ($|\\omega| \\le Kr$) and drifting ($|\\omega|>Kr$) subpopulations.\nWe are interested in the critical coupling strength $K_c$ where the incoherent state ($r=0$) loses stability and a coherent state ($r>0$) emerges continuously. This corresponds to the bifurcation point, which we can find by analyzing the self-consistency equation for arbitrarily small $r > 0$.\n\nFor $r \\to 0^+$, the interval of locked oscillators $[-Kr, Kr]$ becomes infinitesimally small.\nThe contribution from locked oscillators to the order parameter is:\n$$\nr_{\\text{locked}} = \\int_{-Kr}^{Kr} g(\\omega) \\left[ \\int_0^{2\\pi} \\delta\\left(\\theta - \\arcsin\\left(\\frac{\\omega}{Kr}\\right)\\right) \\cos\\theta \\, d\\theta \\right] d\\omega\n$$\n$$\nr_{\\text{locked}} = \\int_{-Kr}^{Kr} g(\\omega) \\cos\\left(\\arcsin\\left(\\frac{\\omega}{Kr}\\right)\\right) d\\omega = \\int_{-Kr}^{Kr} g(\\omega) \\sqrt{1 - \\left(\\frac{\\omega}{Kr}\\right)^2} \\, d\\omega\n$$\nThe contribution from drifting oscillators can be shown to be of order $O(r^3)$ or higher, and is thus negligible compared to the locked oscillators' contribution near the bifurcation point. We make the standard assumption that this contribution vanishes faster than $r$ as $r \\to 0$.\n\nAnalyzing the locked contribution for $r \\to 0^+$: the integration range $[-Kr, Kr]$ is very narrow. As $g(\\omega)$ is continuous at $\\omega=0$ (a given condition), we can approximate $g(\\omega) \\approx g(0)$ over this interval.\n$$\nr \\approx g(0) \\int_{-Kr}^{Kr} \\sqrt{1 - \\left(\\frac{\\omega}{Kr}\\right)^2} \\, d\\omega\n$$\nWe perform a change of variables: let $u = \\omega / (Kr)$, so $d\\omega = Kr \\, du$. The integration limits become $-1$ and $1$.\n$$\nr \\approx g(0) (Kr) \\int_{-1}^{1} \\sqrt{1 - u^2} \\, du\n$$\nThe integral $\\int_{-1}^{1} \\sqrt{1 - u^2} \\, du$ represents the area of a semicircle of radius $1$, which is $\\frac{1}{2}\\pi(1)^2 = \\frac{\\pi}{2}$.\nSubstituting this value back:\n$$\nr \\approx g(0) (Kr) \\frac{\\pi}{2}\n$$\nThis equation has two solutions for $r$. One is the trivial incoherent solution, $r=0$. A non-trivial solution for $r>0$ can exist if we can divide both sides by $r$:\n$$\n1 \\approx \\frac{K \\pi g(0)}{2}\n$$\nThis relationship must hold at the critical point $K=K_c$ where the non-zero solution first appears. Thus, we have an exact equality at the bifurcation point:\n$$\n1 = \\frac{K_c \\pi g(0)}{2}\n$$\nSolving for the critical coupling strength $K_c$:\n$$\nK_c = \\frac{2}{\\pi g(0)}\n$$\nThis derivation relies on the assumption that $g(\\omega)$ is continuous at $\\omega=0$, which was provided. The evenness of $g(\\omega)$ ensures the stationary state does not drift, simplifying the analysis. The unimodal property is typical but not strictly necessary for this specific calculation, as long as the distribution is well-behaved at the origin.",
            "answer": "$$\\boxed{\\frac{2}{\\pi g(0)}}$$"
        },
        {
            "introduction": "In biological and social systems, adaptation is a collective process where the success of an individual's strategy is contingent upon the strategies employed by others in the population. Evolutionary game theory formalizes this interdependence, with the concept of an Evolutionarily Stable Strategy (ESS) defining a population state that is robust against invasion by rare, alternative strategies. By deriving the conditions for a mixed ESS directly from the replicator dynamics , you will develop a fundamental understanding of how stable behavioral patterns and population-level equilibria emerge from the interplay of individual payoffs and selection pressures.",
            "id": "4274006",
            "problem": "Consider an infinitely large, well-mixed population engaged in repeated, random, pairwise interactions described by a symmetric $2\\times 2$ game with payoff matrix $A=\\begin{pmatrix}a & b\\\\ c & d\\end{pmatrix}$, where $a$, $b$, $c$, and $d$ are real parameters. Let the population state be the frequency $x\\in[0,1]$ of strategy $1$ (and $1-x$ of strategy $2$). Assume that adaptation occurs via the replicator dynamics from evolutionary game theory, in which the growth rate of a strategy’s frequency is proportional to its payoff advantage over the population mean payoff. An Evolutionarily Stable Strategy (ESS) is a population composition $x^{\\ast}$ such that, when almost all individuals play according to $x^{\\ast}$, any sufficiently small fraction of mutants earns a strictly lower expected payoff.\n\nStarting from the definitions of expected payoffs and the replicator dynamics, and without invoking pre-packaged formulas, do the following:\n\n1. Derive the expected payoff to each strategy as a function of $x$ and write the one-dimensional replicator equation governing $x(t)$.\n2. Show that an interior candidate for a mixed ESS must satisfy the equal-payoff condition and solve for the interior fixed point $x^{\\ast}$ in closed form.\n3. Derive the stability condition for the interior fixed point under the replicator dynamics and translate it into conditions on $(a,b,c,d)$. Combine this with the feasibility requirement $x^{\\ast}\\in(0,1)$ to obtain necessary and sufficient conditions on $(a,b,c,d)$ under which a mixed ESS exists.\n\nReport the mixed-strategy probability $x^{\\ast}$ of strategy $1$ as a single, closed-form analytic expression in terms of $a$, $b$, $c$, and $d$. If a mixed ESS does not exist, your derivation should make this clear, but your final reported expression should be the closed-form $x^{\\ast}$ assuming the mixed ESS feasibility and stability conditions are satisfied. No numerical evaluation is required. Express your final answer exactly (no rounding).",
            "solution": "The problem asks for the derivation of the mixed-strategy probability, $x^{\\ast}$, for an Evolutionarily Stable Strategy (ESS) in a symmetric $2 \\times 2$ game under replicator dynamics. We will proceed by first deriving the replicator equation, then finding its interior fixed point, and finally establishing the conditions under which this fixed point corresponds to a stable mixed ESS.\n\nLet the two strategies be denoted by $1$ and $2$. The state of the population is given by the frequency $x \\in [0,1]$ of individuals playing strategy $1$, and thus a frequency of $1-x$ for strategy $2$. The interactions are governed by the symmetric payoff matrix $A$, where the entry $A_{ij}$ is the payoff to a player using strategy $i$ against an opponent using strategy $j$.\n$$\nA = \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}\n$$\n\n**1. Expected Payoffs and the Replicator Equation**\n\nFirst, we determine the expected payoff for an individual playing each of the pure strategies in a population with state $x$. An individual playing strategy $1$ will encounter another strategy $1$ player with probability $x$ (earning payoff $a$) and a strategy $2$ player with probability $1-x$ (earning payoff $b$). The expected payoff for strategy $1$, denoted $E_1(x)$, is therefore:\n$$\nE_1(x) = a \\cdot x + b \\cdot (1-x)\n$$\nSimilarly, the expected payoff for strategy $2$, denoted $E_2(x)$, is:\n$$\nE_2(x) = c \\cdot x + d \\cdot (1-x)\n$$\nThe average payoff in the entire population, $\\bar{E}(x)$, is the weighted average of the payoffs of the two strategies:\n$$\n\\bar{E}(x) = x E_1(x) + (1-x) E_2(x)\n$$\nThe problem states that the replicator dynamics describe the evolution of the frequency $x$. The growth rate of a strategy's frequency is proportional to its payoff advantage over the mean population payoff. For strategy $1$, this is written as:\n$$\n\\frac{dx}{dt} = kx(E_1(x) - \\bar{E}(x))\n$$\nwhere $k$ is a positive constant of proportionality, which we can set to $1$ by rescaling time. Substituting the expression for $\\bar{E}(x)$:\n$$\n\\frac{dx}{dt} = x(E_1(x) - [x E_1(x) + (1-x) E_2(x)])\n$$\n$$\n\\frac{dx}{dt} = x(E_1(x)(1-x) - E_2(x)(1-x))\n$$\n$$\n\\frac{dx}{dt} = x(1-x)(E_1(x) - E_2(x))\n$$\nThis is the one-dimensional replicator equation. To complete its derivation, we substitute the expressions for $E_1(x)$ and $E_2(x)$:\n$$\nE_1(x) - E_2(x) = (ax + b(1-x)) - (cx + d(1-x)) = (a-c)x - (d-b)(1-x) = (a-c)x - (d-b) + (d-b)x\n$$\n$$\nE_1(x) - E_2(x) = (a-c+d-b)x + (b-d)\n$$\nThus, the replicator equation governing the frequency $x(t)$ is:\n$$\n\\frac{dx}{dt} = x(1-x)[(a-b-c+d)x + (b-d)]\n$$\n\n**2. Interior Fixed Point and the Equal-Payoff Condition**\n\nFixed points of the dynamics are population states $x^{\\ast}$ where $\\frac{dx}{dt} = 0$. From the equation above, fixed points can occur at:\n- $x^{\\ast} = 0$ (a pure-strategy population of type $2$)\n- $x^{\\ast} = 1$ (a pure-strategy population of type $1$)\n- An interior fixed point $x^{\\ast} \\in (0,1)$ where the term in brackets is zero: $(a-b-c+d)x^{\\ast} + (b-d) = 0$.\n\nThis last condition is equivalent to $E_1(x^{\\ast}) - E_2(x^{\\ast}) = 0$, or $E_1(x^{\\ast}) = E_2(x^{\\ast})$. This is the equal-payoff condition. An interior mixed ESS candidate must satisfy this condition. If payoffs were not equal, say $E_1(x^{\\ast}) > E_2(x^{\\ast})$, then in a population playing the mixed strategy $x^{\\ast}$, any individual would be better off switching to pure strategy $1$. The mixed strategy would not be stable against individual deviations, which is a necessary condition for an ESS. Thus, for an interior mixed strategy to be a candidate for an ESS, both pure strategies must yield the same expected payoff.\n\nSolving the equal-payoff condition for $x^{\\ast}$:\n$$\n(a-b-c+d)x^{\\ast} = d-b\n$$\nAssuming $a-b-c+d \\neq 0$, we find the interior fixed point:\n$$\nx^{\\ast} = \\frac{d-b}{a-b-c+d}\n$$\n\n**3. Stability and Feasibility Conditions**\n\nFor $x^{\\ast}$ to be a stable mixed ESS, two sets of conditions must be met:\n1.  **Feasibility**: $x^{\\ast}$ must be a valid probability, meaning $0 < x^{\\ast} < 1$.\n2.  **Stability**: The fixed point $x^{\\ast}$ must be dynamically stable. Small perturbations away from $x^{\\ast}$ should decay back to $x^{\\ast}$.\n\nLet's analyze the stability first. A fixed point $x^{\\ast}$ of the dynamical system $\\frac{dx}{dt} = f(x)$ is locally stable if $f'(x^{\\ast}) < 0$. Let $f(x) = x(1-x)[(a-b-c+d)x + (b-d)]$.\nLet $g(x) = (a-b-c+d)x + (b-d)$. Then $f(x) = x(1-x)g(x)$.\nThe derivative is $f'(x) = (1-2x)g(x) + x(1-x)g'(x)$.\nAt the interior fixed point $x^{\\ast}$, by definition, $g(x^{\\ast})=0$. So the first term vanishes:\n$$\nf'(x^{\\ast}) = x^{\\ast}(1-x^{\\ast})g'(x^{\\ast})\n$$\nThe derivative of $g(x)$ is $g'(x) = a-b-c+d$.\nSince $x^{\\ast} \\in (0,1)$, the term $x^{\\ast}(1-x^{\\ast})$ is strictly positive. Therefore, the sign of $f'(x^{\\ast})$ is determined by the sign of $g'(x^{\\ast})$. The stability condition $f'(x^{\\ast}) < 0$ thus requires:\n$$\na-b-c+d < 0\n$$\nThis condition is also equivalent to the second requirement for a static ESS: that the equilibrium strategy $x^{\\ast}$ has a higher payoff against any mutant strategy than the mutant strategy has against itself within the invaded population.\n\nNow, we combine this stability condition with the feasibility requirement $0 < x^{\\ast} < 1$.\nUsing $x^{\\ast} = \\frac{d-b}{a-b-c+d}$:\n\n-   **Condition $x^{\\ast} > 0$**:\n    We have $\\frac{d-b}{a-b-c+d} > 0$. Since the stability condition requires the denominator $(a-b-c+d)$ to be negative, the numerator $(d-b)$ must also be negative for the fraction to be positive.\n    $d-b < 0 \\implies b > d$.\n\n-   **Condition $x^{\\ast} < 1$**:\n    We have $\\frac{d-b}{a-b-c+d} < 1$. Since the denominator is negative, multiplying both sides by it reverses the inequality:\n    $d-b > a-b-c+d$\n    $0 > a-c \\implies a < c$.\n\nIn summary, for a stable mixed ESS to exist, the payoffs must satisfy $a < c$ and $b > d$. These are the conditions for a Hawk-Dove game structure, where each pure strategy is the best response to the other, preventing either from taking over the population and leading to a stable coexistence.\n\nThe problem asks for the closed-form expression for $x^{\\ast}$, which we have derived. This expression is valid under the necessary and sufficient conditions $a < c$ and $b > d$.\n\nThe probability of strategy $1$ in the mixed ESS is:\n$$\nx^{\\ast} = \\frac{d-b}{a-b-c+d}\n$$",
            "answer": "$$\n\\boxed{\\frac{d-b}{a-b-c+d}}\n$$"
        },
        {
            "introduction": "When we observe a non-random pattern in real-world data, such as the high prevalence of triangles in social networks, we must ask a critical scientific question: is this structure a sign of genuine self-organizing mechanisms, or is it merely an artifact of simpler constraints? Null models provide the statistical framework to answer this, serving as randomized benchmarks that preserve certain basic properties of the data while erasing the specific higher-order structure under investigation. This hands-on computational practice  guides you through implementing a rigorous hypothesis test to distinguish meaningful emergent structure from what might be expected by chance, a crucial skill for empirical research in any complex system.",
            "id": "4274115",
            "problem": "You are given the task of formalizing and testing the claim that an observed macro-pattern in a network requires self-organization rather than arising from random aggregation. Consider an undirected simple graph without self-loops, represented by an adjacency matrix $A \\in \\{0,1\\}^{n \\times n}$ that is symmetric with zero diagonal. The nodes are indexed by $\\{0,1,\\dots,n-1\\}$, and edges are unordered pairs $\\{i,j\\}$ for $i \\neq j$. The degree sequence is $k_i = \\sum_{j=0}^{n-1} A_{ij}$.\n\nThe fundamental base of your derivation must employ the following standard definitions:\n- A triangle is a set of three nodes $\\{i,j,\\ell\\}$ with all three edges present among them. The total number of triangles in $A$ is $T = \\frac{1}{6} \\mathrm{tr}(A^3)$.\n- A connected triple (also called a wedge) is a path of length two centered at a node, counted as $\\binom{k_i}{2}$ for node $i$. The total number of connected triples is $W = \\sum_{i=0}^{n-1} \\binom{k_i}{2} = \\frac{1}{2} \\sum_{i=0}^{n-1} k_i (k_i - 1)$.\n- The global clustering coefficient (also called transitivity) is defined as $C = \\frac{3T}{W}$, with the convention that if $W = 0$, then $C = 0$.\n\nTo test whether an observed macro-pattern (here, elevated clustering) requires self-organization, construct a null model that matches only trivial statistics but lacks interaction mechanisms. Use the degree-preserving random edge-swap model (also known as the Maslov–Sneppen rewiring) to sample random graphs uniformly from the space of simple graphs with the same degree sequence as the observed network. This null model preserves the degree sequence and edge count, but erases local interaction structure by repeatedly swapping endpoints of randomly chosen edge pairs while forbidding self-loops and multi-edges.\n\nFor each observed network, estimate the empirical null distribution of $C$ by generating $R$ independent samples from the null model, and compute:\n- The empirical $p$-value $p = \\frac{1 + \\sum_{r=1}^R \\mathbf{1}\\{C^{(r)} \\ge C^{\\mathrm{obs}}\\}}{R + 1}$, where $C^{(r)}$ is the clustering coefficient of the $r$-th null sample and $C^{\\mathrm{obs}}$ is the observed clustering coefficient.\n- The standardized effect size $z = \\frac{C^{\\mathrm{obs}} - \\mu}{\\sigma}$, where $\\mu$ and $\\sigma$ are the mean and standard deviation of the null distribution of $C$. If $\\sigma = 0$, define $z = +\\infty$ if $C^{\\mathrm{obs}} > \\mu$, and $z = -\\infty$ otherwise.\n\nDecision rule: Conclude that the macro-pattern requires self-organization if and only if $p < \\alpha$ and $z \\ge z_{\\mathrm{thr}}$, with $\\alpha = 0.05$ and $z_{\\mathrm{thr}} = 2$.\n\nImplement the following generative procedures for observed networks (all graphs are undirected, simple, and unweighted):\n- Ring-lattice: For parameters $(n,k)$ with $k$ even and $k \\ll n$, connect each node $i$ to its $k/2$ nearest neighbors on either side on the ring, creating edges $(i,(i+d)\\bmod n)$ for $d = 1,2,\\dots,k/2$.\n- Degree-preserving randomization: Starting from the ring-lattice with the same $(n,k)$, perform repeated degree-preserving edge swaps to erase local structure while keeping the degree sequence.\n- Preferential attachment tree: Initialize with a single edge among two nodes. For each new node until there are $n$ nodes, attach it to one existing node chosen with probability proportional to its current degree, producing a tree (Barabási–Albert model with parameter $m=1$).\n- Two-dimensional grid with diagonals: For parameters $(s_x,s_y)$, construct a rectangular lattice of $n = s_x s_y$ nodes with edges between horizontal and vertical neighbors, and add diagonal edges of each cell to create triangles (e.g., add edges from $(x,y)$ to $(x+1,y+1)$ where valid).\n\nYour program must implement:\n- Exact computation of $T$, $W$, and $C$ for any given $A$.\n- Null sampling via degree-preserving edge swaps, starting from the observed $A$. Use $S$ swaps per null sample, with $S$ set proportional to the number of edges $m$ (for example, $S = 10m$).\n- Empirical $p$-value and $z$-score computation and the decision rule specified above.\n\nTest suite. For each of the following parameter sets, construct the corresponding observed network, run the null-model test with $R = 64$ samples, $\\alpha = 0.05$, $z_{\\mathrm{thr}} = 2$, and $S = 10m$ swaps per null sample, and output a boolean indicating whether self-organization is required:\n- Case $1$ (self-organized via local clustering): Ring-lattice with $(n,k) = (60,4)$.\n- Case $2$ (random aggregation with matched degrees): Degree-preserving randomization of the ring-lattice from Case $1$ using $S = 20m$ swaps to produce the observed network.\n- Case $3$ (sparse tree without triadic closure): Preferential attachment tree with $n = 60$, parameter $m = 1$.\n- Case $4$ (locally clustered planar structure): Two-dimensional grid with diagonals for $(s_x,s_y) = (7,7)$, so $n = 49$.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of Cases $1$ through $4$, for example, $[r_1,r_2,r_3,r_4]$, where each $r_i$ is a boolean computed according to the decision rule.",
            "solution": "The problem requires us to formalize and apply a statistical hypothesis test to determine if an observed network pattern, specifically high clustering, is a result of self-organization or could have arisen from a random process constrained by basic network properties. This is a fundamental question in complex systems, where emergent macro-patterns are often distinguished from trivial aggregation by comparing them against a suitable null model.\n\nThe core of the methodology is a hypothesis test. The null hypothesis, $H_0$, posits that the observed network is a random realization from an ensemble of graphs that only share the same degree sequence as the observed network. The alternative hypothesis, $H_1$, is that the network possesses additional structure—in this case, significantly higher clustering—not explained by the degree sequence alone. Such excess structure is interpreted as a signature of self-organization, i.e., non-random interaction rules.\n\nThe chosen test statistic is the global clustering coefficient, $C$, which quantifies the tendency of nodes to form triangles. It is defined as the ratio of \"closed\" triples (triangles) to all connected triples (wedges).\n- The number of triangles, $T$, is given by $T = \\frac{1}{6} \\mathrm{tr}(A^3)$, where $A$ is the adjacency matrix. The term $A^3_{ii}$ counts the number of paths of length $3$ from node $i$ back to itself. Each triangle involving node $i$ contributes two such paths (one clockwise, one counter-clockwise). Since this is counted for each of the $3$ nodes in a triangle, and the trace sums these counts, we divide by $3 \\times 2 = 6$.\n- The number of connected triples (or wedges), $W$, centered at node $i$ is the number of pairs of edges connected to $i$, which is $\\binom{k_i}{2}$, where $k_i$ is the degree of node $i$. Summing over all nodes gives the total number of wedges: $W = \\sum_{i=0}^{n-1} \\binom{k_i}{2}$.\n- The global clustering coefficient is then $C = \\frac{3T}{W}$. The factor of $3$ normalizes the ratio, as one triangle consists of three closed wedges.\n\nTo test the null hypothesis, we generate a null distribution of the statistic $C$. This is done using the degree-preserving random edge-swap model (Maslov–Sneppen rewiring). This procedure starts with the observed network and repeatedly swaps the endpoints of two randomly chosen edges, say $\\{i, j\\}$ and $\\{u, v\\}$, to form new edges $\\{i, v\\}$ and $\\{u, j\\}$, provided these new edges do not already exist and do not create self-loops. This process randomizes the network's topology while preserving the degree of every node, thereby creating an ensemble of random graphs that match the observed network's degree sequence exactly. This is crucial because many network properties, including clustering, are strongly influenced by the degree sequence. By preserving it, we isolate the effect of higher-order correlations beyond degrees.\n\nFor each of the $R$ random graphs generated by this process, we compute its clustering coefficient $C^{(r)}$. This ensemble of values $\\{C^{(1)}, C^{(2)}, \\dots, C^{(R)}\\}$ constitutes an empirical null distribution. We then compare the observed clustering coefficient, $C^{\\text{obs}}$, to this distribution using two metrics:\n\n1.  **The empirical $p$-value**: $p = \\frac{1 + \\sum_{r=1}^R \\mathbf{1}\\{C^{(r)} \\ge C^{\\mathrm{obs}}\\}}{R + 1}$. This measures the probability of observing a clustering coefficient at least as large as $C^{\\text{obs}}$ under the null hypothesis. The `+1` terms are a standard correction to avoid $p=0$ and to account for the observed sample itself. A small $p$-value indicates that the observed clustering is surprisingly high.\n2.  **The standardized effect size ($z$-score)**: $z = \\frac{C^{\\mathrm{obs}} - \\mu}{\\sigma}$, where $\\mu$ and $\\sigma$ are the mean and standard deviation of the null distribution. The $z$-score measures how many standard deviations the observed value is from the mean of the null distribution. A large positive $z$-score indicates a large effect size.\n\nThe decision rule is to declare the pattern a result of self-organization if it is both statistically significant ($p < \\alpha$) and has a sufficiently large effect size ($z \\ge z_{\\mathrm{thr}}$), with given thresholds $\\alpha=0.05$ and $z_{\\mathrm{thr}}=2$.\n\nA computational framework is implemented to test four distinct network models:\n- **Case 1: Ring-lattice $(n=60, k=4)$**: Each node is connected to its nearest and next-nearest neighbors on a ring. This creates a regular, locally ordered structure with a high density of triangles, leading to a high $C^{\\text{obs}}$. This structure is not expected in a random graph with the same degrees, so we anticipate rejecting the null hypothesis (Result: `True`).\n- **Case 2: Randomized ring-lattice**: This network is explicitly constructed by taking the ring-lattice from Case 1 and performing a large number of degree-preserving swaps. By construction, it is a sample from the null model ensemble. Therefore, its $C^{\\text{obs}}$ should be typical for the null distribution, leading to a high $p$-value and a $z$-score near $0$. We anticipate failing to reject the null hypothesis (Result: `False`).\n- **Case 3: Preferential attachment tree $(n=60, m=1)$**: This generates a tree structure, which by definition has no cycles and thus no triangles. Therefore, $T^{\\text{obs}}=0$ and $C^{\\text{obs}}=0$. The null model graphs, having the same heterogeneous degree sequence, will almost certainly contain some triangles formed by chance. Thus, $C^{(r)}$ will likely be greater than $C^{\\text{obs}}$ for all null samples, leading to $p \\approx 1$ and a negative $z$-score. We anticipate failing to reject the null hypothesis (Result: `False`).\n- **Case 4: 2D grid with diagonals $((s_x,s_y)=(7,7))$**: This network models a spatially embedded system with local connections. The grid structure with added diagonals creates a high density of triangles. For instance, adding an edge between nodes $(x,y)$ and $(x+1,y+1)$ to a grid cell creates two triangles: $((x,y), (x+1,y), (x+1,y+1))$ and $((x,y), (x,y+1), (x+1,y+1))$. This highly clustered structure is a form of self-organization (based on spatial proximity) and is not expected to arise from a random rewiring process. We anticipate rejecting the null hypothesis (Result: `True`).\n\nThe implementation proceeds by first generating the adjacency matrix for each case. Then, for each matrix, a testing function calculates its observed clustering coefficient $C^{\\text{obs}}$, generates $R=64$ null model samples through rewiring, calculates the null distribution of $C$, and finally computes the $p$-value and $z$-score to apply the decision rule.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ...\n\ndef solve():\n    \"\"\"\n    Main function to run the four test cases and print the results.\n    \"\"\"\n    np.random.seed(42)  # For reproducibility\n\n    # Define test case parameters\n    alpha = 0.05\n    z_thr = 2.0\n    R = 64\n    S_multiplier_null = 10\n\n    results = []\n\n    # Case 1: Ring-lattice\n    n1, k1 = 60, 4\n    A1 = generate_ring_lattice(n1, k1)\n    res1 = run_null_model_test(A1, R, alpha, z_thr, S_multiplier_null)\n    results.append(res1)\n\n    # Case 2: Randomized ring-lattice\n    m2 = (n1 * k1) // 2\n    S_randomize = 20 * m2\n    A2 = degree_preserving_swap(A1.copy(), S_randomize)\n    res2 = run_null_model_test(A2, R, alpha, z_thr, S_multiplier_null)\n    results.append(res2)\n\n    # Case 3: Preferential attachment tree\n    n3 = 60\n    A3 = generate_pa_tree(n3)\n    res3 = run_null_model_test(A3, R, alpha, z_thr, S_multiplier_null)\n    results.append(res3)\n\n    # Case 4: 2D grid with diagonals\n    sx4, sy4 = 7, 7\n    A4 = generate_grid_with_diagonals(sx4, sy4)\n    res4 = run_null_model_test(A4, R, alpha, z_thr, S_multiplier_null)\n    results.append(res4)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\n\ndef calculate_c(A):\n    \"\"\"\n    Calculates the global clustering coefficient C for a graph with adjacency matrix A.\n    \"\"\"\n    if A.shape[0] < 3:\n        return 0.0\n        \n    # Using np.linalg.matrix_power for A^3\n    A_cubed = np.linalg.matrix_power(A.astype(np.float64), 3)\n    T = np.trace(A_cubed) / 6.0\n    \n    k = A.sum(axis=1)\n    # Filter degrees less than 2, as they don't contribute to wedges\n    k_ge_2 = k[k >= 2]\n    W = np.sum(k_ge_2 * (k_ge_2 - 1.0)) / 2.0\n    \n    if W == 0:\n        return 0.0\n    \n    return (3.0 * T) / W\n\ndef degree_preserving_swap(A, num_swaps):\n    \"\"\"\n    Performs degree-preserving random edge swaps (Maslov-Sneppen rewiring).\n    \"\"\"\n    n = A.shape[0]\n    A_rewired = A.copy()\n    \n    # Get edge set for efficient lookups and modification\n    # Use tuples with sorted nodes to have a canonical representation\n    edge_set = {tuple(sorted(edge)) for edge in np.argwhere(np.triu(A_rewired))}\n\n    if not edge_set or len(edge_set) < 2:\n        return A_rewired\n\n    edges_list = list(edge_set)\n    m = len(edges_list)\n    \n    swaps_done = 0\n    attempts = 0\n    max_attempts = num_swaps * 10 # To avoid infinite loops in dense graphs\n\n    while swaps_done < num_swaps and attempts < max_attempts:\n        attempts += 1\n        \n        # Choose two distinct edges\n        idx1, idx2 = np.random.choice(m, 2, replace=False)\n        e1 = edges_list[idx1]\n        e2 = edges_list[idx2]\n        \n        i, j = e1\n        u, v = e2\n\n        # Ensure all four nodes are distinct\n        if len({i, j, u, v}) != 4:\n            continue\n\n        # Propose new edges (i,v) and (u,j). Sorting for canonical check.\n        new_e1 = tuple(sorted((i, v)))\n        new_e2 = tuple(sorted((u, j)))\n\n        # Check if new edges would create multi-edges\n        if new_e1 in edge_set or new_e2 in edge_set:\n            continue\n        \n        # Perform the swap\n        # Remove old edges from matrix and set\n        A_rewired[i, j] = A_rewired[j, i] = 0\n        A_rewired[u, v] = A_rewired[v, u] = 0\n        edge_set.remove(e1)\n        edge_set.remove(e2)\n        \n        # Add new edges to matrix and set\n        A_rewired[i, v] = A_rewired[v, i] = 1\n        A_rewired[u, j] = A_rewired[j, u] = 1\n        edge_set.add(new_e1)\n        edge_set.add(new_e2)\n        \n        # Update the list from which we sample\n        edges_list[idx1] = new_e1\n        edges_list[idx2] = new_e2\n\n        swaps_done += 1\n        \n    return A_rewired\n\n\ndef run_null_model_test(A_obs, R, alpha, z_thr, S_multiplier):\n    \"\"\"\n    Runs the null model test and returns the decision.\n    \"\"\"\n    C_obs = calculate_c(A_obs)\n    \n    m = np.sum(np.triu(A_obs))\n    num_swaps = int(S_multiplier * m)\n\n    C_null_dist = []\n    for _ in range(R):\n        A_rewired = degree_preserving_swap(A_obs.copy(), num_swaps)\n        C_null = calculate_c(A_rewired)\n        C_null_dist.append(C_null)\n        \n    C_null_dist = np.array(C_null_dist)\n    \n    # Calculate p-value\n    ge_count = np.sum(C_null_dist >= C_obs)\n    p_value = (1.0 + ge_count) / (R + 1.0)\n    \n    # Calculate z-score\n    mu = np.mean(C_null_dist)\n    sigma = np.std(C_null_dist)\n    \n    if sigma == 0:\n        if C_obs > mu:\n            z_score = np.inf\n        else: # Covers C_obs <= mu\n            z_score = -np.inf\n    else:\n        z_score = (C_obs - mu) / sigma\n\n    # Decision rule\n    return p_value < alpha and z_score >= z_thr\n\n# --- Graph Generation Functions ---\n\ndef generate_ring_lattice(n, k):\n    A = np.zeros((n, n), dtype=int)\n    for i in range(n):\n        for d in range(1, k // 2 + 1):\n            j = (i + d) % n\n            A[i, j] = A[j, i] = 1\n    return A\n\ndef generate_pa_tree(n):\n    A = np.zeros((n, n), dtype=int)\n    if n < 2:\n        return A\n    \n    # Initial state: edge between node 0 and 1\n    A[0, 1] = A[1, 0] = 1\n    edge_endpoints = [0, 1]\n    \n    for i in range(2, n):\n        target_node = np.random.choice(edge_endpoints)\n        A[i, target_node] = A[target_node, i] = 1\n        edge_endpoints.extend([i, target_node])\n        \n    return A\n\ndef generate_grid_with_diagonals(sx, sy):\n    n = sx * sy\n    A = np.zeros((n, n), dtype=int)\n    \n    for y in range(sy):\n        for x in range(sx):\n            i = x + y * sx\n            \n            # Horizontal neighbor\n            if x < sx - 1:\n                j_h = (x + 1) + y * sx\n                A[i, j_h] = A[j_h, i] = 1\n                \n            # Vertical neighbor\n            if y < sy - 1:\n                j_v = x + (y + 1) * sx\n                A[i, j_v] = A[j_v, i] = 1\n            \n            # Diagonal neighbor: (x,y) to (x+1,y+1)\n            if x < sx - 1 and y < sy - 1:\n                j_d = (x + 1) + (y + 1) * sx\n                A[i, j_d] = A[j_d, i] = 1\n                \n    return A\n\nif __name__ == '__main__':\n    solve()\n```"
        }
    ]
}