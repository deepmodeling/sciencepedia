{
    "hands_on_practices": [
        {
            "introduction": "To truly understand the \"rich-get-richer\" phenomenon, we must move beyond qualitative descriptions and engage with its mathematical formulation. This practice guides you through a cornerstone derivation in network science: using the continuum method to determine the degree distribution of a growing network. By modeling degree evolution with a rate equation, you will derive the famous power-law exponent $\\gamma$ for a generalized preferential attachment model, providing a quantitative explanation for the emergence of hubs .",
            "id": "4298183",
            "problem": "Consider a growing undirected network initialized at time $t=0$ with a finite seed of $N_{0}$ nodes and $E_{0}$ edges. At each discrete time step $t \\mapsto t+1$, a single new node is introduced that connects to exactly $m \\ge 1$ pre-existing nodes by $m$ new edges. The targets of these $m$ edges are chosen independently without self-loops with probability proportional to their current degree plus a constant attractiveness, that is, a node $i$ with degree $k_{i}(t)$ is selected as an endpoint of a new edge with instantaneous probability proportional to $k_{i}(t) + A$, where $A$ is a real constant independent of $i$ and $t$. Assume that there is no edge deletion or rewiring and that no edges are created between old nodes other than through attachment from the new node.\n\nUsing the continuum method based on rate equations and the large-network limit (i.e., replacing sums by their expectations and $k_{i}(t)$ by its mean-field trajectory), derive the asymptotic power-law exponent $\\gamma$ of the degree distribution $P(k) \\sim k^{-\\gamma}$ as a function of $m$ and $A$. Clearly articulate the modeling and parameter conditions under which your derivation is valid. Provide your final answer for $\\gamma$ as a single analytic expression in terms of $m$ and $A$.",
            "solution": "The problem asks for the derivation of the asymptotic power-law exponent $\\gamma$ for the degree distribution $P(k)$ of a growing network. The network starts with $N_0$ nodes and $E_0$ edges. At each time step, a new node is added, which forms $m \\ge 1$ edges to existing nodes. The probability of an existing node $i$ with degree $k_i(t)$ being chosen as a target for a new edge is proportional to $k_i(t) + A$, where $A$ is a constant. We will use the continuum method in the large-network limit as specified.\n\nFirst, we formalize the attachment probability. The probability $\\Pi_i(t)$ that one of the $m$ new edges connects to a specific existing node $i$ is given by:\n$$\n\\Pi_i(t) = \\frac{k_i(t) + A}{\\sum_{j=1}^{N(t)} (k_j(t) + A)}\n$$\nwhere $N(t)$ is the total number of nodes at time $t$. For the probabilistic interpretation to be valid, the attachment kernel $k_i(t)+A$ must be non-negative for all nodes. The minimum degree a node can have after its creation is $m$. Thus, we must impose the condition $k+A \\ge 0$ for all $k \\ge m$, which leads to the constraint $m+A \\ge 0$, or $A \\ge -m$.\n\nThe total number of nodes at time $t$ is $N(t) = N_0 + t$.\nThe total number of edges at time $t$ is $E(t) = E_0 + mt$.\nThe sum of all degrees in the network is $\\sum_j k_j(t) = 2E(t) = 2(E_0 + mt)$.\n\nWe can now evaluate the denominator of $\\Pi_i(t)$:\n$$\n\\sum_{j=1}^{N(t)} (k_j(t) + A) = \\sum_{j=1}^{N(t)} k_j(t) + \\sum_{j=1}^{N(t)} A = 2E(t) + A \\cdot N(t)\n$$\nSubstituting the expressions for $N(t)$ and $E(t)$:\n$$\n\\sum_{j=1}^{N(t)} (k_j(t) + A) = 2(E_0 + mt) + A(N_0 + t) = (2m+A)t + 2E_0 + AN_0\n$$\nIn the large-network limit ($t \\to \\infty$), we can neglect the constant terms:\n$$\n\\sum_{j=1}^{N(t)} (k_j(t) + A) \\approx (2m+A)t\n$$\nThis approximation is valid for $2m+A \\ne 0$. The condition $A \\ge -m$ with $m \\ge 1$ ensures that $2m+A \\ge 2m-m = m \\ge 1$, so the denominator is always positive and non-zero.\n\nWe now apply the continuum method to model the degree evolution of a node $i$. Let $k_i(t)$ be the expected degree of node $i$ at time $t$. Its rate of change is proportional to the probability of it receiving a new link. Since $m$ new edges are added per unit time, the rate equation is:\n$$\n\\frac{dk_i}{dt} = m \\cdot \\Pi_i(t) = m \\frac{k_i(t) + A}{(2m+A)t}\n$$\nThis is a first-order linear ordinary differential equation for $k_i(t)$. We can solve it by separation of variables:\n$$\n\\frac{dk_i}{k_i + A} = \\frac{m}{2m+A} \\frac{dt}{t}\n$$\nLet's integrate this equation for a node $i$ that was introduced at time $t_i$. The initial condition for this node is that its degree at the moment of its creation is $m$, so $k_i(t_i) = m$. We integrate from the time of introduction $t_i$ to the current time $t$:\n$$\n\\int_{m}^{k_i(t)} \\frac{dk}{k+A} = \\int_{t_i}^{t} \\frac{m}{2m+A} \\frac{d\\tau}{\\tau}\n$$\nThe integration yields:\n$$\n\\left[ \\ln(k+A) \\right]_{m}^{k_i(t)} = \\frac{m}{2m+A} \\left[ \\ln(\\tau) \\right]_{t_i}^{t}\n$$\n$$\n\\ln(k_i(t)+A) - \\ln(m+A) = \\frac{m}{2m+A} (\\ln(t) - \\ln(t_i))\n$$\nRearranging the terms:\n$$\n\\ln\\left(\\frac{k_i(t)+A}{m+A}\\right) = \\ln\\left(\\left(\\frac{t}{t_i}\\right)^{\\frac{m}{2m+A}}\\right)\n$$\nExponentiating both sides gives the solution for $k_i(t)$:\n$$\nk_i(t) = (m+A)\\left(\\frac{t}{t_i}\\right)^{\\beta} - A\n$$\nwhere the exponent $\\beta$ is defined as:\n$$\n\\beta = \\frac{m}{2m+A}\n$$\nThis equation describes the deterministic evolution of the degree of a node created at time $t_i$. To find the degree distribution $P(k)$, we relate the degree $k$ to the creation time $t_i$. Since nodes are added at a constant rate, the distribution of creation times $t_i$ for a randomly selected node at time $t$ is uniform over the interval $[0, t]$ (for large $N_0$ and $t$, we can approximate the starting time as $0$). The probability that a node was created before time $t_i$ is $P(\\text{creation time}  t_i) = t_i/t$.\n\nThe cumulative degree distribution $P(Kk)$ is the probability that a randomly chosen node has a degree greater than $k$. This is equivalent to the probability that its creation time $t_i$ is small enough for its degree to have grown larger than $k$.\n$$\nP(Kk) = P(k_i(t)  k)\n$$\nUsing the expression for $k_i(t)$:\n$$\n(m+A)\\left(\\frac{t}{t_i}\\right)^{\\beta} - A  k \\implies \\left(\\frac{t}{t_i}\\right)^{\\beta}  \\frac{k+A}{m+A}\n$$\n$$\n\\frac{t}{t_i}  \\left(\\frac{k+A}{m+A}\\right)^{1/\\beta} \\implies t_i  t \\left(\\frac{k+A}{m+A}\\right)^{-1/\\beta}\n$$\nSo, $P(Kk)$ is the probability that a node's creation time $t_i$ is less than this value:\n$$\nP(Kk) = \\frac{t \\left(\\frac{k+A}{m+A}\\right)^{-1/\\beta}}{t} = \\left(\\frac{k+A}{m+A}\\right)^{-1/\\beta}\n$$\nThe probability density function $P(k)$ is the negative derivative of the cumulative distribution $P(Kk)$ with respect to $k$:\n$$\nP(k) = -\\frac{d}{dk} P(Kk) = -\\frac{d}{dk} \\left[ (m+A)^{1/\\beta} (k+A)^{-1/\\beta} \\right]\n$$\n$$\nP(k) = - (m+A)^{1/\\beta} \\left(-\\frac{1}{\\beta}\\right) (k+A)^{-1/\\beta - 1} = \\frac{(m+A)^{1/\\beta}}{\\beta} (k+A)^{-(1+1/\\beta)}\n$$\nFor large degrees $k$, the term $(k+A)$ is asymptotically proportional to $k$. Thus, the degree distribution follows a power law:\n$$\nP(k) \\sim k^{-\\gamma}\n$$\nBy comparing the exponents, we identify the power-law exponent $\\gamma$:\n$$\n\\gamma = 1 + \\frac{1}{\\beta}\n$$\nSubstituting the expression for $\\beta = \\frac{m}{2m+A}$:\n$$\n\\gamma = 1 + \\frac{2m+A}{m} = 1 + 2 + \\frac{A}{m} = 3 + \\frac{A}{m}\n$$\nThis derivation is valid under the following conditions:\n1.  **Large-Network and Large-Time Limit**: The derivation assumes $t \\to \\infty$ and $N(t) \\to \\infty$, which justifies the continuum approximation and the neglect of initial conditions $N_0$ and $E_0$.\n2.  **Mean-Field Approximation**: The stochastic nature of node degree evolution is replaced by a deterministic mean-field trajectory $k_i(t)$. This ignores fluctuations, which is valid for large networks.\n3.  **Physical Probability**: The attachment kernel $k+A$ must be non-negative for all possible degrees $k \\ge m$. This imposes the condition $m+A \\ge 0$, or $A \\ge -m$. If this condition is met, the denominator $(2m+A)t$ is also guaranteed to be positive for $t0$ and $m\\ge1$.\n\nTherefore, the asymptotic power-law exponent of the degree distribution is $\\gamma = 3 + A/m$.",
            "answer": "$$ \\boxed{3 + \\frac{A}{m}} $$"
        },
        {
            "introduction": "A powerful model becomes even more insightful when we understand its full range of behaviors by tuning its parameters. This exercise explores the generalized attachment model at its extremes, investigating what happens as the initial attractiveness parameter $A$ approaches zero or infinity. By analyzing these limits, you will see how this single framework unifies the classic scale-free Barabási-Albert model with the exponential networks produced by uniform random attachment, deepening your intuition for the role of the attachment kernel .",
            "id": "4298187",
            "problem": "Consider a growing network in which at each discrete time step $t$ a single new node is added with $m$ edges that connect to existing nodes. The attachment kernel is $\\Pi_i \\propto k_i + A$, where $k_i$ is the degree of node $i$ and $A \\ge 0$ is a constant initial attractiveness. Use the following foundational facts: the sum of degrees satisfies $\\sum_j k_j = 2mt$ and the number of nodes satisfies $N(t) \\approx t$ for large $t$. In the continuum limit, interpret the two extremal regimes $A \\to 0$ and $A \\to \\infty$ by deriving the qualitative and quantitative forms of the resulting degree distribution $P(k)$ for large $k$ and the typical time evolution $k_i(t)$ of the degree of a node born at time $t_i$. Which option best captures the correct limiting behaviors?\n\nA. As $A \\to 0$, $P(k) \\sim k^{-3}$; as $A \\to \\infty$, $P(k)$ is exponential with characteristic scale $\\approx A$, and $k_i(t)$ grows linearly with age.\n\nB. As $A \\to 0$, $P(k) \\sim k^{-2}$; as $A \\to \\infty$, $P(k)$ remains a power law with exponent independent of $A$.\n\nC. As $A \\to 0$, $P(k)$ is exponential because initial attractiveness eliminates preferential bias; as $A \\to \\infty$, $P(k)$ becomes a power law with exponent $\\gamma = 3$.\n\nD. As $A \\to 0$, $P(k) \\sim k^{-3}$; as $A \\to \\infty$, attachment becomes uniform, $k_i(t) = m + m \\ln\\!\\big(t/t_i\\big)$, and $P(k) \\sim \\exp\\!\\big(- (k - m)/m \\big)$.\n\nE. As $A \\to 0$, $P(k) \\sim (k + A)^{-(3 + A/m)}$; as $A \\to \\infty$, $P(k)$ approaches a power law with $\\gamma \\to \\infty$ while remaining heavy-tailed.",
            "solution": "The problem statement is a valid exercise in the study of growing networks using the rate equation formalism. It is scientifically grounded in the established theory of preferential attachment, well-posed, and objective. All provided information is self-contained and consistent. We may proceed with the solution.\n\nThe core of the problem lies in the attachment kernel $\\Pi_i$, which is the probability that a new edge connects to an existing node $i$. This is given by:\n$$\n\\Pi_i = \\frac{k_i + A}{\\sum_{j} (k_j + A)}\n$$\nThe denominator can be calculated using the provided facts. At a large time $t$, there are $N(t) \\approx t$ nodes in the network. The sum of degrees is $\\sum_j k_j = 2mt$.\n$$\n\\sum_{j=1}^{N(t)} (k_j + A) = \\sum_{j=1}^{N(t)} k_j + \\sum_{j=1}^{N(t)} A = 2mt + N(t)A \\approx 2mt + tA = t(2m+A)\n$$\nThus, the attachment probability is:\n$$\n\\Pi_i = \\frac{k_i + A}{t(2m+A)}\n$$\nWe use the continuum approximation, where the degree $k_i$ of a node $i$ is treated as a continuous variable evolving in time $t$. The rate of change of $k_i$ is given by the number of new edges per time step, $m$, multiplied by the probability of attachment to node $i$:\n$$\n\\frac{dk_i}{dt} = m \\Pi_i = m \\frac{k_i + A}{t(2m+A)}\n$$\nThis is a separable first-order ordinary differential equation. We can solve it for a node $i$ that is introduced at time $t_i$ with an initial degree $k_i(t_i)=m$.\n$$\n\\int_{m}^{k_i(t)} \\frac{dk'}{k' + A} = \\int_{t_i}^{t} \\frac{m}{2m+A} \\frac{d\\tau}{\\tau}\n$$\nIntegrating both sides yields:\n$$\n\\big[\\ln(k' + A)\\big]_{m}^{k_i(t)} = \\frac{m}{2m+A} \\big[\\ln(\\tau)\\big]_{t_i}^{t}\n$$\n$$\n\\ln(k_i(t) + A) - \\ln(m + A) = \\frac{m}{2m+A} \\big(\\ln(t) - \\ln(t_i)\\big)\n$$\n$$\n\\ln\\left(\\frac{k_i(t) + A}{m + A}\\right) = \\ln\\left(\\left(\\frac{t}{t_i}\\right)^{\\frac{m}{2m+A}}\\right)\n$$\nExponentiating both sides gives the time evolution of the degree of node $i$:\n$$\nk_i(t) = (m+A)\\left(\\frac{t}{t_i}\\right)^{\\beta} - A, \\quad \\text{where} \\quad \\beta = \\frac{m}{2m+A}\n$$\nTo find the degree distribution $P(k)$, we can use the time evolution of $k_i(t)$. The cumulative probability $P(K  k)$ is the fraction of nodes with degree greater than $k$. Since nodes are added at a constant rate, the age distribution is uniform. The fraction of nodes born before some time $t_c$ is $t_c/t$. We find the time $t_c$ at which a node must be born to have degree exactly $k$ at time $t$.\n$$\nk = (m+A)\\left(\\frac{t}{t_c}\\right)^{\\beta} - A \\implies t_c = t \\left(\\frac{m+A}{k+A}\\right)^{1/\\beta}\n$$\nThe cumulative distribution is $P(K  k) \\approx t_c/t$:\n$$\nP(K  k) = \\left(\\frac{m+A}{k+A}\\right)^{1/\\beta}\n$$\nThe probability density function $P(k)$ is obtained by differentiation: $P(k) = - \\frac{d}{dk}P(Kk)$.\n$$\nP(k) = - \\frac{d}{dk} \\left[ (m+A)^{1/\\beta} (k+A)^{-1/\\beta} \\right] = (m+A)^{1/\\beta} \\left(\\frac{1}{\\beta}\\right) (k+A)^{-(1+1/\\beta)}\n$$\nThe exponent is $\\gamma = 1 + 1/\\beta = 1 + \\frac{2m+A}{m} = 1 + 2 + \\frac{A}{m} = 3 + \\frac{A}{m}$.\nSo, the general form of the degree distribution is:\n$$\nP(k) \\propto (k+A)^{-(3+A/m)}\n$$\n\nNow we analyze the two extremal regimes.\n\n**Regime 1: $A \\to 0$ (Pure Preferential Attachment)**\nIn this limit, the model reduces to the standard Barabási-Albert model.\n- For the degree evolution, the exponent $\\beta = \\frac{m}{2m+A} \\to \\frac{m}{2m} = \\frac{1}{2}$.\n  The degree evolution equation becomes $k_i(t) = m\\left(\\frac{t}{t_i}\\right)^{1/2}$. The degree grows as the square root of the node's age.\n- For the degree distribution, the exponent $\\gamma = 3 + \\frac{A}{m} \\to 3$.\n  The distribution becomes a power law, $P(k) \\propto k^{-3}$. This is the characteristic scale-free distribution with exponent $\\gamma=3$.\n\n**Regime 2: $A \\to \\infty$ (Uniform Attachment)**\nIn this limit, the initial attractiveness $A$ dominates the degree $k_i$. The attachment kernel $\\Pi_i \\propto k_i + A \\approx A$. This means the attachment probability is independent of the node's degree, so attachment is uniform across all existing nodes.\n- For the degree evolution, we can directly analyze the rate equation with uniform attachment probability $\\Pi_i \\approx 1/N(t) \\approx 1/t$:\n  $$\n  \\frac{dk_i}{dt} = m \\Pi_i \\approx \\frac{m}{t}\n  $$\n  Integrating from $t_i$ to $t$ with $k_i(t_i) = m$:\n  $$\n  \\int_m^{k_i(t)} dk' = \\int_{t_i}^t \\frac{m}{\\tau}d\\tau \\implies k_i(t) - m = m \\ln(t/t_i)\n  $$\n  $$\n  k_i(t) = m + m \\ln(t/t_i)\n  $$\n  The degree grows logarithmically with the node's age.\n- For the degree distribution, we use this logarithmic growth law to find $P(Kk)$.\n  $$\n  k = m + m \\ln(t/t_c) \\implies \\frac{k-m}{m} = \\ln(t/t_c) \\implies t_c = t \\exp\\left(-\\frac{k-m}{m}\\right)\n  $$\n  The cumulative distribution is $P(K  k) \\approx t_c/t = \\exp\\left(-\\frac{k-m}{m}\\right)$.\n  Differentiating gives the probability density:\n  $$\n  P(k) = -\\frac{d}{dk} \\exp\\left(-\\frac{k-m}{m}\\right) = \\frac{1}{m} \\exp\\left(-\\frac{k-m}{m}\\right)\n  $$\n  For large $k$, this is an exponential distribution, $P(k) \\sim \\exp(-k/m)$.\n\n**Option-by-Option Analysis**\n\n*   **A. As $A \\to 0$, $P(k) \\sim k^{-3}$; as $A \\to \\infty$, $P(k)$ is exponential with characteristic scale $\\approx A$, and $k_i(t)$ grows linearly with age.**\n    - The $A \\to 0$ part is correct.\n    - For $A \\to \\infty$, $P(k)$ is indeed exponential, but the characteristic scale (decay constant) is $m$, not $A$. The claim that $k_i(t)$ grows linearly with age is also incorrect; it grows logarithmically.\n    - Verdict: **Incorrect**.\n\n*   **B. As $A \\to 0$, $P(k) \\sim k^{-2}$; as $A \\to \\infty$, $P(k)$ remains a power law with exponent independent of $A$.**\n    - The exponent for the $A \\to 0$ case is $3$, not $2$.\n    - The distribution for $A \\to \\infty$ is exponential, not a power law.\n    - Verdict: **Incorrect**.\n\n*   **C. As $A \\to 0$, $P(k)$ is exponential because initial attractiveness eliminates preferential bias; as $A \\to \\infty$, $P(k)$ becomes a power law with exponent $\\gamma = 3$.**\n    - This option reverses the behaviors of the two limits. For $A \\to 0$, we have a power law. Preferential bias is strongest, not eliminated. For $A \\to \\infty$, the distribution is exponential. The power law with $\\gamma=3$ corresponds to $A \\to 0$.\n    - Verdict: **Incorrect**.\n\n*   **D. As $A \\to 0$, $P(k) \\sim k^{-3}$; as $A \\to \\infty$, attachment becomes uniform, $k_i(t) = m + m \\ln\\!\\big(t/t_i\\big)$, and $P(k) \\sim \\exp\\!\\big(- (k - m)/m \\big)$.**\n    - For $A \\to 0$, $P(k) \\sim k^{-3}$ is correct.\n    - For $A \\to \\infty$, the statement that attachment becomes uniform is correct. The derived equation for degree evolution $k_i(t) = m + m \\ln(t/t_i)$ is correct. The resulting degree distribution form $P(k) \\sim \\exp(-(k-m)/m)$ is also correct.\n    - Verdict: **Correct**.\n\n*   **E. As $A \\to 0$, $P(k) \\sim (k + A)^{-(3 + A/m)}$; as $A \\to \\infty$, $P(k)$ approaches a power law with $\\gamma \\to \\infty$ while remaining heavy-tailed.**\n    - The first part provides the general formula for $P(k)$, which is true for any finite $A \\ge 0$, but the question asks for the behavior in the limit $A \\to 0$, which is $P(k) \\sim k^{-3}$.\n    - The second part is incorrect. While the exponent $\\gamma = 3+A/m$ does go to infinity as $A \\to \\infty$, this does not mean the limiting distribution is a power law. The functional form of the distribution changes from a power law to an exponential. Furthermore, a distribution with an infinitely fast-decaying power-law tail is the antithesis of \"heavy-tailed.\" Exponential decay is considered light-tailed.\n    - Verdict: **Incorrect**.",
            "answer": "$$\\boxed{D}$$"
        },
        {
            "introduction": "Theoretical models are powerful, but their ultimate test lies in their ability to explain real-world data. This final practice bridges the gap between theory and empirical analysis by introducing a key statistical technique: Maximum Likelihood Estimation (MLE). You will construct the log-likelihood function for a generalized preferential attachment process and use it to estimate the model's parameters from observed network growth data, a fundamental skill for validating and fitting generative models in any scientific domain .",
            "id": "4298151",
            "problem": "You are given a discrete-time network growth process consistent with a generalized preferential attachment mechanism. At each event, a new node arrives and attaches to exactly one existing node. Let $k_i^{(t)}$ denote the degree of existing node $i$ immediately before event $t$ (that is, strictly prior to the arrival and attachment of the new node at event $t$). The attachment probability that the new node selects existing node $i$ at event $t$ is assumed proportional to the nonnegative kernel $(k_i^{(t)} + A)^\\alpha$, where $A \\ge 0$ is the initial attractiveness and $\\alpha  0$ controls the nonlinearity of preferential attachment.\n\nFundamental base:\n- Core definition of preferential attachment: the probability to attach to an existing node is proportional to a nondecreasing function of its current degree.\n- Well-tested modeling fact used here: generalized preferential attachment kernels of the form $(k + A)^\\alpha$, with $A \\ge 0$ and $\\alpha  0$, capture initial attractiveness and nonlinear rich-get-richer mechanisms.\n\nYou observe a full growth dataset consisting of:\n- An initial degree vector for the pre-existing nodes at time $t=0$.\n- A sequence of chosen indices, one per event, indicating which existing node received the new edge at that event.\n\nAssume:\n- Each event adds exactly one new node and exactly one new edge, connecting the new node to the chosen existing node from the set of nodes present before the event.\n- Node labels are fixed in arrival order: initial nodes are labeled starting at $0$, and the new node introduced at event $t$ receives label equal to the current number of nodes before the event.\n- Degrees update deterministically: when node $i$ is chosen at event $t$, its degree increases by $1$, and the newly added node starts with degree $1$ and becomes available in subsequent events.\n\nTask:\n1. Construct the log-likelihood function for parameters $A$ and $\\alpha$ under the assumption that, conditional on the degree configuration immediately before each event, the observed attachment event is drawn from the kernel $(k_i^{(t)} + A)^\\alpha$ normalized over all existing nodes.\n2. Implement Maximum Likelihood Estimation (MLE) to estimate $A$ and $\\alpha$ from the given datasets by numerically maximizing the log-likelihood with respect to $A$ and $\\alpha$ subject to the constraints $A \\ge 0$ and $\\alpha  0$.\n3. Use gradient-based numerical optimization. You must provide and use analytic gradients of the log-likelihood with respect to $A$ and $\\alpha$.\n\nTest suite:\nProvide MLE results for the following three datasets. In each dataset, the initial degree vector and the sequence of chosen indices are specified.\n\n- Dataset $1$ (general case):\n  - Initial degrees: $[1,1]$.\n  - Chosen indices by event: $[0,0,1,0,1,0,0,1,0,0]$.\n\n- Dataset $2$ (balanced attachments, exercising sensitivity to $A$):\n  - Initial degrees: $[1,1,1]$.\n  - Chosen indices by event: $[0,1,2,0,1,2,0,1,2,0,1,2]$.\n\n- Dataset $3$ (extreme concentration, exercising sensitivity to $\\alpha$):\n  - Initial degrees: $[1,1,1]$.\n  - Chosen indices by event: $[0,0,0,0,0,0,0,0,0,0,0,0]$.\n\nNumerical and output requirements:\n- Optimize over $A$ and $\\alpha$ with bounds $A \\in [0,100]$ and $\\alpha \\in [0.1,2.0]$.\n- Each dataset should yield an estimated pair $\\widehat{A}$ and $\\widehat{\\alpha}$.\n- Express each estimate as a float rounded to $6$ decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each dataset’s result is itself a two-element list in the order $[\\widehat{A},\\widehat{\\alpha}]$. For example: $[[\\widehat{A}_1,\\widehat{\\alpha}_1],[\\widehat{A}_2,\\widehat{\\alpha}_2],[\\widehat{A}_3,\\widehat{\\alpha}_3]]$.",
            "solution": "The problem requires performing Maximum Likelihood Estimation (MLE) for the parameters of a generalized preferential attachment model. The model is defined by an attachment probability kernel proportional to $(k+A)^\\alpha$, where $k$ is the node degree, $A \\ge 0$ is an initial attractiveness parameter, and $\\alpha  0$ controls the super- or sub-linearity of the attachment preference. We are given full observational data for three distinct network growth histories and must estimate the parameter pair $(A, \\alpha)$ for each. This requires deriving the log-likelihood function and its gradients, then using a numerical optimization algorithm to find the parameters that maximize this function.\n\n**1. Log-Likelihood Function Derivation**\n\nLet the parameters be denoted by the vector $\\theta = (A, \\alpha)$. The observed data for a single growth process consists of an initial degree vector $\\mathbf{k}^{(0)} = (k_0^{(0)}, \\dots, k_{N_0-1}^{(0)})$ for $N_0$ initial nodes, and a sequence of chosen nodes $\\mathbf{c} = (c_1, c_2, \\dots, c_T)$ over $T$ discrete time events.\n\nAt each event $t \\in \\{1, 2, \\dots, T\\}$, a new node arrives and attaches to one of the $N^{(t)} = N_0 + t - 1$ existing nodes. The set of nodes present just before event $t$ is $V^{(t)} = \\{0, 1, \\dots, N^{(t)}-1\\}$, and their degrees are given by the vector $\\mathbf{k}^{(t-1)}$.\n\nAccording to the model, the probability of the new node attaching to a specific existing node $i \\in V^{(t)}$ is given by the normalized kernel:\n$$ P(\\text{choose } i | \\mathbf{k}^{(t-1)}, A, \\alpha) = \\frac{(k_i^{(t-1)} + A)^\\alpha}{\\sum_{j=0}^{N^{(t)}-1} (k_j^{(t-1)} + A)^\\alpha} $$\nThe events are assumed to be independent conditional on the state of the network at each step. Therefore, the total likelihood of observing the entire sequence of chosen nodes $\\mathbf{c}$ is the product of the probabilities of each individual event:\n$$ L(A, \\alpha | \\mathbf{k}^{(0)}, \\mathbf{c}) = \\prod_{t=1}^{T} P(\\text{chosen node is } c_t | \\mathbf{k}^{(t-1)}, A, \\alpha) $$\nSubstituting the probability expression, we get:\n$$ L(A, \\alpha) = \\prod_{t=1}^{T} \\frac{(k_{c_t}^{(t-1)} + A)^\\alpha}{\\sum_{j=0}^{N^{(t)}-1} (k_j^{(t-1)} + A)^\\alpha} $$\nFor numerical stability and mathematical convenience, we work with the log-likelihood function, $\\mathcal{L}(A, \\alpha) = \\ln L(A, \\alpha)$. The logarithm transforms the product into a sum:\n$$ \\mathcal{L}(A, \\alpha) = \\sum_{t=1}^{T} \\ln \\left( \\frac{(k_{c_t}^{(t-1)} + A)^\\alpha}{\\sum_{j=0}^{N^{(t)}-1} (k_j^{(t-1)} + A)^\\alpha} \\right) $$\n$$ \\mathcal{L}(A, \\alpha) = \\sum_{t=1}^{T} \\left[ \\alpha \\ln(k_{c_t}^{(t-1)} + A) - \\ln \\left( \\sum_{j=0}^{N^{(t)}-1} (k_j^{(t-1)} + A)^\\alpha \\right) \\right] $$\nThis function represents the log-probability of observing the given attachment sequence for a specific choice of parameters $A$ and $\\alpha$. To find the MLE estimates $(\\widehat{A}, \\widehat{\\alpha})$, we must find the values of $A$ and $\\alpha$ that maximize this function, subject to the given constraints.\n\nThe degree vectors $\\mathbf{k}^{(t-1)}$ are not fixed but evolve according to the process. Starting with $\\mathbf{k}^{(0)}$, the degree vector at the end of event $t$, denoted $\\mathbf{k}^{(t)}$, is obtained by updating $\\mathbf{k}^{(t-1)}$: the degree of the chosen node $c_t$ is incremented by $1$, and a new node with degree $1$ is added to the network.\n\n**2. Gradient Derivation**\nFor efficient numerical optimization, we must compute the partial derivatives of the log-likelihood function with respect to $A$ and $\\alpha$.\n\nLet the normalization sum at event $t$ be $S_t(A, \\alpha) = \\sum_{j=0}^{N^{(t)}-1} (k_j^{(t-1)} + A)^\\alpha$. The log-likelihood is:\n$$ \\mathcal{L}(A, \\alpha) = \\sum_{t=1}^{T} \\left[ \\alpha \\ln(k_{c_t}^{(t-1)} + A) - \\ln(S_t(A, \\alpha)) \\right] $$\n\n**Gradient with respect to $A$**:\nWe differentiate $\\mathcal{L}$ with respect to $A$:\n$$ \\frac{\\partial \\mathcal{L}}{\\partial A} = \\sum_{t=1}^{T} \\left[ \\frac{\\partial}{\\partial A} \\left(\\alpha \\ln(k_{c_t}^{(t-1)} + A)\\right) - \\frac{\\partial}{\\partial A} \\left(\\ln(S_t)\\right) \\right] $$\n$$ \\frac{\\partial \\mathcal{L}}{\\partial A} = \\sum_{t=1}^{T} \\left[ \\frac{\\alpha}{k_{c_t}^{(t-1)} + A} - \\frac{1}{S_t} \\frac{\\partial S_t}{\\partial A} \\right] $$\nThe derivative of the sum $S_t$ with respect to $A$ is:\n$$ \\frac{\\partial S_t}{\\partial A} = \\sum_{j=0}^{N^{(t)}-1} \\frac{\\partial}{\\partial A} (k_j^{(t-1)} + A)^\\alpha = \\sum_{j=0}^{N^{(t)}-1} \\alpha (k_j^{(t-1)} + A)^{\\alpha - 1} $$\nSubstituting this back, the full gradient with respect to $A$ is:\n$$ \\frac{\\partial \\mathcal{L}}{\\partial A} = \\alpha \\sum_{t=1}^{T} \\left[ \\frac{1}{k_{c_t}^{(t-1)} + A} - \\frac{\\sum_{j=0}^{N^{(t)}-1} (k_j^{(t-1)} + A)^{\\alpha - 1}}{\\sum_{j=0}^{N^{(t)}-1} (k_j^{(t-1)} + A)^\\alpha} \\right] $$\n\n**Gradient with respect to $\\alpha$**:\nSimilarly, we differentiate $\\mathcal{L}$ with respect to $\\alpha$:\n$$ \\frac{\\partial \\mathcal{L}}{\\partial \\alpha} = \\sum_{t=1}^{T} \\left[ \\frac{\\partial}{\\partial \\alpha} \\left(\\alpha \\ln(k_{c_t}^{(t-1)} + A)\\right) - \\frac{\\partial}{\\partial \\alpha} \\left(\\ln(S_t)\\right) \\right] $$\n$$ \\frac{\\partial \\mathcal{L}}{\\partial \\alpha} = \\sum_{t=1}^{T} \\left[ \\ln(k_{c_t}^{(t-1)} + A) - \\frac{1}{S_t} \\frac{\\partial S_t}{\\partial \\alpha} \\right] $$\nThe derivative of the sum $S_t$ with respect to $\\alpha$ is (using $\\frac{d}{dx} a^x = a^x \\ln a$):\n$$ \\frac{\\partial S_t}{\\partial \\alpha} = \\sum_{j=0}^{N^{(t)}-1} \\frac{\\partial}{\\partial \\alpha} (k_j^{(t-1)} + A)^\\alpha = \\sum_{j=0}^{N^{(t)}-1} (k_j^{(t-1)} + A)^\\alpha \\ln(k_j^{(t-1)} + A) $$\nSubstituting this back, the full gradient with respect to $\\alpha$ is:\n$$ \\frac{\\partial \\mathcal{L}}{\\partial \\alpha} = \\sum_{t=1}^{T} \\left[ \\ln(k_{c_t}^{(t-1)} + A) - \\frac{\\sum_{j=0}^{N^{(t)}-1} (k_j^{(t-1)} + A)^\\alpha \\ln(k_j^{(t-1)} + A)}{\\sum_{j=0}^{N^{(t)}-1} (k_j^{(t-1)} + A)^\\alpha} \\right] $$\nThe fractional term is the expected value of the log-attractiveness, $E[\\ln(k+A)]$, under the model's probability distribution at event $t$.\n\n**3. Numerical Maximization**\nWe will find the MLE parameters $(\\widehat{A}, \\widehat{\\alpha})$ by numerically minimizing the negative log-likelihood, $-\\mathcal{L}(A, \\alpha)$, subject to the bounds $A \\in [0, 100]$ and $\\alpha \\in [0.1, 2.0]$. We use the L-BFGS-B algorithm, a quasi-Newton method that is well-suited for optimization with box constraints and utilizes the analytic gradients we have derived.\n\nThe core of the implementation is a function that, for a given parameter set $(A, \\alpha)$ and an observed dataset, simulates the network growth event by event. At each event, it calculates the contribution to the total log-likelihood and its gradients, and accumulates these values. The function returns the total negative log-likelihood and its corresponding negative gradient vector $[-\\frac{\\partial \\mathcal{L}}{\\partial A}, -\\frac{\\partial \\mathcal{L}}{\\partial \\alpha}]$, which are then passed to the optimizer. This process is repeated for each of the three provided datasets to find their respective parameter estimates.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import optimize\n\ndef solve():\n    \"\"\"\n    Main function to perform MLE for the generalized preferential attachment model\n    on three datasets.\n    \"\"\"\n    test_cases = [\n        # Dataset 1 (general case)\n        {\n            \"initial_degrees\": [1, 1],\n            \"chosen_indices\": [0, 0, 1, 0, 1, 0, 0, 1, 0, 0]\n        },\n        # Dataset 2 (balanced attachments)\n        {\n            \"initial_degrees\": [1, 1, 1],\n            \"chosen_indices\": [0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2]\n        },\n        # Dataset 3 (extreme concentration)\n        {\n            \"initial_degrees\": [1, 1, 1],\n            \"chosen_indices\": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n        }\n    ]\n\n    def neg_log_likelihood_and_grad(params, initial_degrees, chosen_indices):\n        \"\"\"\n        Calculates the negative log-likelihood and its gradient for the given data.\n        \n        This function simulates the network growth process step-by-step based on the\n        observed attachment sequence. At each step, it calculates and accumulates\n        the log-likelihood and its partial derivatives with respect to A and alpha.\n        \"\"\"\n        A, alpha = params\n        \n        # Ensure parameters are within valid domain for calculation.\n        # This is mainly a safeguard, as bounds are handled by the optimizer.\n        if A  0 or alpha = 0:\n            return np.inf, np.zeros(2)\n\n        log_likelihood = 0.0\n        grad_A = 0.0\n        grad_alpha = 0.0\n        \n        # Use a list for degrees as it grows dynamically\n        degrees = list(initial_degrees)\n        \n        for t, chosen_node_idx in enumerate(chosen_indices):\n            # Convert to numpy array for vectorized operations\n            k_vals = np.array(degrees, dtype=np.float64)\n            \n            # The base of the attachment kernel\n            base = k_vals + A\n            \n            # The attachment kernel (k+A)^alpha\n            kernel_vals = base**alpha\n            S_t = np.sum(kernel_vals)\n\n            if S_t == 0: # Avoid division by zero\n                # This case should not be reached with the given constraints k=1, A=0, alpha0\n                return np.inf, np.zeros(2)\n\n            k_chosen = k_vals[chosen_node_idx]\n            base_chosen = k_chosen + A\n\n            # Log-likelihood contribution for event t\n            log_likelihood += alpha * np.log(base_chosen) - np.log(S_t)\n            \n            # Gradient contribution for event t\n            # Gradient w.r.t. A\n            grad_S_t_A_num = np.sum(alpha * base**(alpha - 1.0))\n            grad_A += alpha * (1.0 / base_chosen - grad_S_t_A_num / S_t)\n            \n            # Gradient w.r.t. alpha\n            log_base = np.log(base)\n            grad_S_t_alpha_num = np.sum(kernel_vals * log_base)\n            grad_alpha += np.log(base_chosen) - grad_S_t_alpha_num / S_t\n            \n            # Update degrees for the next event\n            degrees[chosen_node_idx] += 1\n            degrees.append(1) # New node with degree 1\n            \n        return -log_likelihood, -np.array([grad_A, grad_alpha])\n\n    results = []\n    bounds = [(0, 100), (0.1, 2.0)]\n    initial_guess = [1.0, 1.0]\n\n    for case in test_cases:\n        args = (case[\"initial_degrees\"], case[\"chosen_indices\"])\n        \n        res = optimize.minimize(\n            fun=neg_log_likelihood_and_grad,\n            x0=initial_guess,\n            args=args,\n            method='L-BFGS-B',\n            jac=True,  # Our function returns the jacobian (gradient)\n            bounds=bounds,\n            options={'disp': False}\n        )\n        \n        A_hat, alpha_hat = res.x\n        results.append([round(A_hat, 6), round(alpha_hat, 6)])\n\n    # The final print statement must match the required format exactly.\n    # repr() creates a string representation, and .replace removes spaces\n    # to match the symbolic format in the prompt.\n    final_output_string = repr(results).replace(\" \", \"\")\n    print(final_output_string)\n\nsolve()\n```"
        }
    ]
}