{
    "hands_on_practices": [
        {
            "introduction": "为了扎实掌握优先连接机制，我们从一个基础计算开始。这个练习将帮助你应用“富者愈富”原则的核心概率公式，在一个简单、可控的网络初始阶段进行实践。通过精确计算一个新节点连接到现有节点的概率，你将对该机制的运作方式建立直观的理解 。",
            "id": "1471153",
            "problem": "许多复杂生物网络（从蛋白质相互作用到代谢途径）的生长可以用简单的生成模型来描述。其中最著名的模型之一是 Barabási-Albert (BA) 模型，它依赖于一种称为“择优连接”的“富者愈富”机制。根据该模型，当一个新节点加入网络时，它与一个现有节点 $i$ 连接的概率与该节点的度 $k_i$ 成正比。该概率由公式 $P(i) = \\frac{k_i}{\\sum_j k_j}$ 给出，其中分母中的求和是针对网络中所有已存在的节点 $j$ 进行的。\n\n考虑这样一个网络生长的第一步。初始状态由一个包含 $m_0 = 3$ 个节点的小型核心组成，我们将其标记为节点1、节点2和节点3。这三个节点完全互连，形成一个完全图（一个三角形）。这意味着节点1连接到节点2和节点3，节点2连接到节点1和节点3，节点3连接到节点1和节点2。\n\n现在，一个新节点（节点4）被添加到网络中。它将形成恰好 $m=1$ 条新边，连接到最初的三个节点之一。根据择优连接的规则，确定节点4连接到节点1的概率。\n\n将您的答案表示为最简分数。",
            "solution": "问题要求计算在一个根据 Barabási-Albert 择优连接模型增长的网络中，一个新节点（节点4）连接到一个特定现有节点（节点1）的概率。\n\n择优连接的核心原则指出，新节点连接到现有节点 $i$ 的概率 $P(i)$ 由以下公式给出：\n$$P(i) = \\frac{k_i}{\\sum_j k_j}$$\n其中 $k_i$ 是节点 $i$ 的度（即与其连接的边的数量），分母是网络中所有现有节点度的总和。\n\n首先，我们必须确定初始网络中所有节点的度。该网络由三个完全互连的节点（1、2和3）组成，形成一个三角形。\n\n节点1的度 $k_1$ 是它所具有的连接数。节点1连接到节点2和节点3。因此，它的度为 $k_1 = 2$。\n\n节点2的度 $k_2$ 是它所具有的连接数。节点2连接到节点1和节点3。因此，它的度为 $k_2 = 2$。\n\n节点3的度 $k_3$ 是它所具有的连接数。节点3连接到节点1和节点2。因此，它的度为 $k_3 = 2$。\n\n接下来，我们计算网络中所有现有节点度的总和，它构成了概率公式的分母。\n$$\\sum_j k_j = k_1 + k_2 + k_3 = 2 + 2 + 2 = 6$$\n\n现在，我们可以计算新节点4连接到节点1的概率。对于此计算，目标节点是节点1，所以我们在分子中使用它的度 $k_1=2$。\n$$P(\\text{节点4连接到节点1}) = \\frac{k_1}{\\sum_j k_j} = \\frac{2}{6}$$\n\n最后，我们按要求将所得分数化简。\n$$P(\\text{节点4连接到节点1}) = \\frac{2}{6} = \\frac{1}{3}$$\n\n因此，新节点连接到节点1的概率是 $\\frac{1}{3}$。",
            "answer": "$$\\boxed{\\frac{1}{3}}$$"
        },
        {
            "introduction": "在理解了基本规则之后，下一步是发展分析工具来预测网络在宏观尺度上的行为。本练习将引导你使用连续介质方法 (continuum method)，这是一种在网络科学中推导度分布等关键属性的强大技术。你将通过求解速率方程，推导出带有初始吸引力项的广义优先连接模型的度指数 ，从而深入理解模型参数如何决定最终的网络结构。",
            "id": "4298183",
            "problem": "考虑一个增长的无向网络，该网络在时间 $t=0$ 时初始化，包含一个由 $N_{0}$ 个节点和 $E_{0}$ 条边组成的有限种子。在每个离散时间步 $t \\mapsto t+1$，引入一个新节点，该节点通过 $m$ 条新边与 $m \\ge 1$ 个已存在的节点相连。这 $m$ 条边的目标节点是独立选择的，无自环，其选择概率与其当前度加上一个恒定的吸引力成正比。也就是说，一个度为 $k_{i}(t)$ 的节点 $i$ 被选为新边端点的瞬时概率正比于 $k_{i}(t) + A$，其中 $A$ 是一个与 $i$ 和 $t$ 无关的实数常数。假设没有边的删除或重连，并且除了新节点的连接外，旧节点之间不会创建新边。\n\n使用基于速率方程的连续方法和大型网络极限（即，用期望值代替求和，用平均场轨迹代替 $k_{i}(t)$），推导度分布 $P(k) \\sim k^{-\\gamma}$ 的渐近幂律指数 $\\gamma$ 作为 $m$ 和 $A$ 的函数。清晰地阐述你的推导所依据的建模和参数条件。以 $m$ 和 $A$ 的单一解析表达式形式给出 $\\gamma$ 的最终答案。",
            "solution": "该问题要求推导一个增长网络的度分布 $P(k)$ 的渐近幂律指数 $\\gamma$。网络初始有 $N_0$ 个节点和 $E_0$ 条边。在每个时间步，会添加一个新节点，该节点与已存在的节点形成 $m \\ge 1$ 条边。一个度为 $k_i(t)$ 的已存在节点 $i$ 被选为新边目标的概率正比于 $k_i(t) + A$，其中 $A$ 是一个常数。我们将按照要求，在大型网络极限下使用连续方法。\n\n首先，我们将连接概率形式化。$m$ 条新边中有一条连接到特定已存在节点 $i$ 的概率 $\\Pi_i(t)$ 为：\n$$\n\\Pi_i(t) = \\frac{k_i(t) + A}{\\sum_{j=1}^{N(t)} (k_j(t) + A)}\n$$\n其中 $N(t)$ 是时间 $t$ 时的节点总数。为了使概率解释有效，连接核 $k_i(t)+A$ 对所有节点都必须是非负的。一个节点在创建后所能拥有的最小度为 $m$。因此，我们必须施加条件 $k+A \\ge 0$ 对所有 $k \\ge m$ 成立，这导出的约束为 $m+A \\ge 0$，即 $A \\ge -m$。\n\n在时间 $t$ 的节点总数是 $N(t) = N_0 + t$。\n在时间 $t$ 的边总数是 $E(t) = E_0 + mt$。\n网络中所有度的总和是 $\\sum_j k_j(t) = 2E(t) = 2(E_0 + mt)$。\n\n现在我们可以计算 $\\Pi_i(t)$ 的分母：\n$$\n\\sum_{j=1}^{N(t)} (k_j(t) + A) = \\sum_{j=1}^{N(t)} k_j(t) + \\sum_{j=1}^{N(t)} A = 2E(t) + A \\cdot N(t)\n$$\n代入 $N(t)$ 和 $E(t)$ 的表达式：\n$$\n\\sum_{j=1}^{N(t)} (k_j(t) + A) = 2(E_0 + mt) + A(N_0 + t) = (2m+A)t + 2E_0 + AN_0\n$$\n在大型网络极限下（$t \\to \\infty$），我们可以忽略常数项：\n$$\n\\sum_{j=1}^{N(t)} (k_j(t) + A) \\approx (2m+A)t\n$$\n此近似在 $2m+A \\ne 0$ 时有效。条件 $A \\ge -m$ 和 $m \\ge 1$ 确保 $2m+A \\ge 2m-m = m \\ge 1$，因此分母始终为正且非零。\n\n我们现在应用连续方法来为节点 $i$ 的度演化建模。设 $k_i(t)$ 为节点 $i$ 在时间 $t$ 的期望度。其变化率正比于它接收到新连接的概率。由于每单位时间增加 $m$ 条新边，速率方程为：\n$$\n\\frac{dk_i}{dt} = m \\cdot \\Pi_i(t) = m \\frac{k_i(t) + A}{(2m+A)t}\n$$\n这是一个关于 $k_i(t)$ 的一阶线性常微分方程。我们可以用分离变量法求解：\n$$\n\\frac{dk_i}{k_i + A} = \\frac{m}{2m+A} \\frac{dt}{t}\n$$\n让我们对一个在时间 $t_i$ 引入的节点 $i$ 对该方程进行积分。该节点的初始条件是其在创建时刻的度为 $m$，即 $k_i(t_i) = m$。我们从引入时间 $t_i$ 积分到当前时间 $t$：\n$$\n\\int_{m}^{k_i(t)} \\frac{dk}{k+A} = \\int_{t_i}^{t} \\frac{m}{2m+A} \\frac{d\\tau}{\\tau}\n$$\n积分得到：\n$$\n\\left[ \\ln(k+A) \\right]_{m}^{k_i(t)} = \\frac{m}{2m+A} \\left[ \\ln(\\tau) \\right]_{t_i}^{t}\n$$\n$$\n\\ln(k_i(t)+A) - \\ln(m+A) = \\frac{m}{2m+A} (\\ln(t) - \\ln(t_i))\n$$\n整理各项：\n$$\n\\ln\\left(\\frac{k_i(t)+A}{m+A}\\right) = \\ln\\left(\\left(\\frac{t}{t_i}\\right)^{\\frac{m}{2m+A}}\\right)\n$$\n对两边取指数，得到 $k_i(t)$ 的解：\n$$\nk_i(t) = (m+A)\\left(\\frac{t}{t_i}\\right)^{\\beta} - A\n$$\n其中指数 $\\beta$ 定义为：\n$$\n\\beta = \\frac{m}{2m+A}\n$$\n该方程描述了在时间 $t_i$ 创建的节点的度的确定性演化。为了找到度分布 $P(k)$，我们将度 $k$ 与创建时间 $t_i$ 联系起来。由于节点以恒定速率增加，在时间 $t$ 随机选择一个节点的创建时间 $t_i$ 的分布在区间 $[0, t]$ 上是均匀的（对于大的 $N_0$ 和 $t$，我们可以将起始时间近似为 $0$）。一个节点在时间 $t_i$ 之前被创建的概率是 $P(\\text{创建时间} \\le t_i) = t_i/t$。\n\n累积度分布 $P(K>k)$ 是一个随机选择的节点度大于 $k$ 的概率。这等价于其创建时间 $t_i$ 足够小，以使其度增长到大于 $k$ 的概率。\n$$\nP(K>k) = P(k_i(t) > k)\n$$\n使用 $k_i(t)$ 的表达式：\n$$\n(m+A)\\left(\\frac{t}{t_i}\\right)^{\\beta} - A > k \\implies \\left(\\frac{t}{t_i}\\right)^{\\beta} > \\frac{k+A}{m+A}\n$$\n$$\n\\frac{t}{t_i} > \\left(\\frac{k+A}{m+A}\\right)^{1/\\beta} \\implies t_i  t \\left(\\frac{k+A}{m+A}\\right)^{-1/\\beta}\n$$\n因此，$P(Kk)$ 是一个节点的创建时间 $t_i$ 小于此值的概率：\n$$\nP(Kk) = \\frac{t \\left(\\frac{k+A}{m+A}\\right)^{-1/\\beta}}{t} = \\left(\\frac{k+A}{m+A}\\right)^{-1/\\beta}\n$$\n概率密度函数 $P(k)$ 是累积分布 $P(Kk)$ 关于 $k$ 的负导数：\n$$\nP(k) = -\\frac{d}{dk} P(Kk) = -\\frac{d}{dk} \\left[ (m+A)^{1/\\beta} (k+A)^{-1/\\beta} \\right]\n$$\n$$\nP(k) = - (m+A)^{1/\\beta} \\left(-\\frac{1}{\\beta}\\right) (k+A)^{-1/\\beta - 1} = \\frac{(m+A)^{1/\\beta}}{\\beta} (k+A)^{-(1+1/\\beta)}\n$$\n对于大的度 $k$，项 $(k+A)$ 渐近正比于 $k$。因此，度分布服从幂律：\n$$\nP(k) \\sim k^{-\\gamma}\n$$\n通过比较指数，我们确定幂律指数 $\\gamma$：\n$$\n\\gamma = 1 + \\frac{1}{\\beta}\n$$\n代入 $\\beta = \\frac{m}{2m+A}$ 的表达式：\n$$\n\\gamma = 1 + \\frac{2m+A}{m} = 1 + 2 + \\frac{A}{m} = 3 + \\frac{A}{m}\n$$\n该推导在以下条件下有效：\n1.  **大网络和大时间极限**：推导假设 $t \\to \\infty$ 和 $N(t) \\to \\infty$，这使得连续近似和忽略初始条件 $N_0$ 和 $E_0$ 成为合理。\n2.  **平均场近似**：节点度演化的随机性被确定性的平均场轨迹 $k_i(t)$ 所取代。这忽略了涨落，对于大型网络是有效的。\n3.  **物理概率**：连接核 $k+A$ 对于所有可能的度 $k \\ge m$ 都必须是非负的。这施加了条件 $m+A \\ge 0$，即 $A \\ge -m$。如果此条件满足，分母 $(2m+A)t$ 在 $t0$ 和 $m\\ge1$ 时也保证为正。\n\n因此，度分布的渐近幂律指数为 $\\gamma = 3 + A/m$。",
            "answer": "$$ \\boxed{3 + \\frac{A}{m}} $$"
        },
        {
            "introduction": "理论模型最终需要与真实世界的数据相结合。这个高级实践将带你从理论走向应用，解决一个核心的“逆问题”：给定一个网络生长的观测数据，我们如何推断出最可能生成该网络的模型参数？你将构建一个对数似然函数，并实现最大似然估计 (MLE) 来从数据中估计模型的非线性参数和初始吸引力 ，这是一个典型的计算社会科学和系统生物学中的数据驱动建模任务。",
            "id": "4298151",
            "problem": "给定一个符合广义优先连接机制的离散时间网络增长过程。在每个事件中，一个新节点到达并仅与一个现有节点相连。令 $k_i^{(t)}$ 表示在事件 $t$ 之前（即，在事件 $t$ 新节点到达并连接之前）现有节点 $i$ 的度。假设在事件 $t$，新节点选择现有节点 $i$ 的连接概率与非负核 $(k_i^{(t)} + A)^\\alpha$ 成正比，其中 $A \\ge 0$ 是初始吸引力，$\\alpha  0$ 控制优先连接的非线性程度。\n\n背景：\n- 优先连接的核心定义：与现有节点连接的概率是其当前度的一个非递减函数。\n模型设定：\n- 形式为 $(k + A)^\\alpha$（其中 $A \\ge 0$ 且 $\\alpha  0$）的广义优先连接核，能够捕捉初始吸引力和非线性的“富者愈富”机制。\n\n你观察到一个完整的增长数据集，包括：\n- 在时间 $t=0$ 时，预先存在的节点的初始度向量。\n- 每个事件一个所选索引的序列，指示在该事件中哪个现有节点接收了新边。\n\n假设：\n- 每个事件恰好增加一个新节点和一条新边，将新节点连接到从事件前存在的节点集合中选择的那个现有节点。\n- 节点标签按到达顺序固定：初始节点从 $0$ 开始标记，在事件 $t$ 引入的新节点接收的标签等于事件前当前节点数。\n- 度确定性地更新：当节点 $i$ 在事件 $t$ 被选中时，其度增加 $1$，新加入的节点以度 $1$ 开始，并可在后续事件中被选择。\n\n任务：\n1. 构建参数 $A$ 和 $\\alpha$ 的对数似然函数。该函数基于以下假设：在每个事件发生前，以度的配置为条件，观测到的连接事件是从在所有现有节点上归一化的核 $(k_i^{(t)} + A)^\\alpha$ 中抽取的。\n2. 实现最大似然估计（MLE），通过在约束条件 $A \\ge 0$ 和 $\\alpha  0$ 下数值最大化对数似然函数，从而从给定的数据集中估计 $A$ 和 $\\alpha$。\n3. 使用基于梯度的数值优化。你必须提供并使用对数似然函数关于 $A$ 和 $\\alpha$ 的解析梯度。\n\n测试套件：\n为以下三个数据集提供MLE结果。在每个数据集中，都指定了初始度向量和所选索引的序列。\n\n- 数据集 $1$（一般情况）：\n  - 初始度：$[1,1]$。\n  - 按事件排列的所选索引：$[0,0,1,0,1,0,0,1,0,0]$。\n\n- 数据集 $2$（平衡连接，检验对 $A$ 的敏感度）：\n  - 初始度：$[1,1,1]$。\n  - 按事件排列的所选索引：$[0,1,2,0,1,2,0,1,2,0,1,2]$。\n\n- 数据集 $3$（极端集中，检验对 $\\alpha$ 的敏感度）：\n  - 初始度：$[1,1,1]$。\n  - 按事件排列的所选索引：$[0,0,0,0,0,0,0,0,0,0,0,0]$。\n\n数值与输出要求：\n- 在边界 $A \\in [0,100]$ 和 $\\alpha \\in [0.1,2.0]$ 内优化 $A$ 和 $\\alpha$。\n- 每个数据集应产生一对估计值 $\\widehat{A}$ 和 $\\widehat{\\alpha}$。\n- 将每个估计值表示为保留 $6$ 位小数的浮点数。\n- 你的程序应生成单行输出，包含一个逗号分隔的列表，列表被方括号包围，其中每个数据集的结果本身是一个包含两个元素的列表，顺序为 $[\\widehat{A},\\widehat{\\alpha}]$。例如：$[[\\widehat{A}_1,\\widehat{\\alpha}_1],[\\widehat{A}_2,\\widehat{\\alpha}_2],[\\widehat{A}_3,\\widehat{\\alpha}_3]]$。",
            "solution": "该问题要求对广义优先连接模型的参数执行最大似然估计（MLE）。该模型由一个连接概率核定义，该核与 $(k+A)^\\alpha$ 成正比，其中 $k$ 是节点度，$A \\ge 0$ 是一个初始吸引力参数，$\\alpha  0$ 控制连接偏好的超线性或次线性。我们获得了三个不同网络增长历史的完整观测数据，并且必须为每个历史估计参数对 $(A, \\alpha)$。这需要推导对数似然函数及其梯度，然后使用数值优化算法找到使该函数最大化的参数。\n\n**1. 对数似然函数推导**\n\n令参数由向量 $\\theta = (A, \\alpha)$ 表示。单个增长过程的观测数据包括 $N_0$ 个初始节点的初始度向量 $\\mathbf{k}^{(0)} = (k_0^{(0)}, \\dots, k_{N_0-1}^{(0)})$，以及在 $T$ 个离散时间事件中选择的节点序列 $\\mathbf{c} = (c_1, c_2, \\dots, c_T)$。\n\n在每个事件 $t \\in \\{1, 2, \\dots, T\\}$，一个新节点到达并与 $N^{(t)} = N_0 + t - 1$ 个现有节点中的一个相连。事件 $t$ 之前存在的节点集为 $V^{(t)} = \\{0, 1, \\dots, N^{(t)}-1\\}$，它们的度由向量 $\\mathbf{k}^{(t-1)}$ 给出。\n\n根据该模型，新节点连接到特定现有节点 $i \\in V^{(t)}$ 的概率由归一化核给出：\n$$ P(\\text{choose } i | \\mathbf{k}^{(t-1)}, A, \\alpha) = \\frac{(k_i^{(t-1)} + A)^\\alpha}{\\sum_{j=0}^{N^{(t)}-1} (k_j^{(t-1)} + A)^\\alpha} $$\n假设在每一步中，事件在给定网络状态的条件下是独立的。因此，观测到整个所选节点序列 $\\mathbf{c}$ 的总似然是每个独立事件概率的乘积：\n$$ L(A, \\alpha | \\mathbf{k}^{(0)}, \\mathbf{c}) = \\prod_{t=1}^{T} P(\\text{chosen node is } c_t | \\mathbf{k}^{(t-1)}, A, \\alpha) $$\n代入概率表达式，我们得到：\n$$ L(A, \\alpha) = \\prod_{t=1}^{T} \\frac{(k_{c_t}^{(t-1)} + A)^\\alpha}{\\sum_{j=0}^{N^{(t)}-1} (k_j^{(t-1)} + A)^\\alpha} $$\n为了数值稳定性和数学上的便利，我们使用对数似然函数 $\\mathcal{L}(A, \\alpha) = \\ln L(A, \\alpha)$。对数将乘积转换为和：\n$$ \\mathcal{L}(A, \\alpha) = \\sum_{t=1}^{T} \\ln \\left( \\frac{(k_{c_t}^{(t-1)} + A)^\\alpha}{\\sum_{j=0}^{N^{(t)}-1} (k_j^{(t-1)} + A)^\\alpha} \\right) $$\n$$ \\mathcal{L}(A, \\alpha) = \\sum_{t=1}^{T} \\left[ \\alpha \\ln(k_{c_t}^{(t-1)} + A) - \\ln \\left( \\sum_{j=0}^{N^{(t)}-1} (k_j^{(t-1)} + A)^\\alpha \\right) \\right] $$\n此函数表示在特定参数选择 $A$ 和 $\\alpha$ 下，观测到给定连接序列的对数概率。为了找到MLE估计值 $(\\widehat{A}, \\widehat{\\alpha})$，我们必须找到使此函数在给定约束条件下最大化的 $A$ 和 $\\alpha$ 的值。\n\n度向量 $\\mathbf{k}^{(t-1)}$ 不是固定的，而是根据过程演化。从 $\\mathbf{k}^{(0)}$ 开始，事件 $t$ 结束时的度向量（表示为 $\\mathbf{k}^{(t)}$）是通过更新 $\\mathbf{k}^{(t-1)}$ 得到的：所选节点 $c_t$ 的度增加 $1$，一个度为 $1$ 的新节点被添加到网络中。\n\n**2. 梯度推导**\n为了高效的数值优化，我们必须计算对数似然函数关于 $A$ 和 $\\alpha$ 的偏导数。\n\n令事件 $t$ 的归一化和为 $S_t(A, \\alpha) = \\sum_{j=0}^{N^{(t)}-1} (k_j^{(t-1)} + A)^\\alpha$。对数似然函数为：\n$$ \\mathcal{L}(A, \\alpha) = \\sum_{t=1}^{T} \\left[ \\alpha \\ln(k_{c_t}^{(t-1)} + A) - \\ln(S_t(A, \\alpha)) \\right] $$\n\n**关于 $A$ 的梯度**：\n我们将 $\\mathcal{L}$ 对 $A$ 求导：\n$$ \\frac{\\partial \\mathcal{L}}{\\partial A} = \\sum_{t=1}^{T} \\left[ \\frac{\\partial}{\\partial A} \\left(\\alpha \\ln(k_{c_t}^{(t-1)} + A)\\right) - \\frac{\\partial}{\\partial A} \\left(\\ln(S_t)\\right) \\right] $$\n$$ \\frac{\\partial \\mathcal{L}}{\\partial A} = \\sum_{t=1}^{T} \\left[ \\frac{\\alpha}{k_{c_t}^{(t-1)} + A} - \\frac{1}{S_t} \\frac{\\partial S_t}{\\partial A} \\right] $$\n和 $S_t$ 关于 $A$ 的导数为：\n$$ \\frac{\\partial S_t}{\\partial A} = \\sum_{j=0}^{N^{(t)}-1} \\frac{\\partial}{\\partial A} (k_j^{(t-1)} + A)^\\alpha = \\sum_{j=0}^{N^{(t)}-1} \\alpha (k_j^{(t-1)} + A)^{\\alpha - 1} $$\n将其代回，得到关于 $A$ 的完整梯度：\n$$ \\frac{\\partial \\mathcal{L}}{\\partial A} = \\sum_{t=1}^{T} \\left[ \\frac{\\alpha}{k_{c_t}^{(t-1)} + A} - \\frac{\\sum_{j=0}^{N^{(t)}-1} \\alpha (k_j^{(t-1)} + A)^{\\alpha - 1}}{\\sum_{j=0}^{N^{(t)}-1} (k_j^{(t-1)} + A)^\\alpha} \\right] = \\alpha \\sum_{t=1}^{T} \\left[ \\frac{1}{k_{c_t}^{(t-1)} + A} - \\frac{\\sum_{j=0}^{N^{(t)}-1} (k_j^{(t-1)} + A)^{\\alpha - 1}}{\\sum_{j=0}^{N^{(t)}-1} (k_j^{(t-1)} + A)^\\alpha} \\right] $$\n\n**关于 $\\alpha$ 的梯度**：\n类似地，我们将 $\\mathcal{L}$ 对 $\\alpha$ 求导：\n$$ \\frac{\\partial \\mathcal{L}}{\\partial \\alpha} = \\sum_{t=1}^{T} \\left[ \\frac{\\partial}{\\partial \\alpha} \\left(\\alpha \\ln(k_{c_t}^{(t-1)} + A)\\right) - \\frac{\\partial}{\\partial \\alpha} \\left(\\ln(S_t)\\right) \\right] $$\n$$ \\frac{\\partial \\mathcal{L}}{\\partial \\alpha} = \\sum_{t=1}^{T} \\left[ \\ln(k_{c_t}^{(t-1)} + A) - \\frac{1}{S_t} \\frac{\\partial S_t}{\\partial \\alpha} \\right] $$\n和 $S_t$ 关于 $\\alpha$ 的导数为（使用 $\\frac{d}{dx} a^x = a^x \\ln a$）：\n$$ \\frac{\\partial S_t}{\\partial \\alpha} = \\sum_{j=0}^{N^{(t)}-1} \\frac{\\partial}{\\partial \\alpha} (k_j^{(t-1)} + A)^\\alpha = \\sum_{j=0}^{N^{(t)}-1} (k_j^{(t-1)} + A)^\\alpha \\ln(k_j^{(t-1)} + A) $$\n将其代回，得到关于 $\\alpha$ 的完整梯度：\n$$ \\frac{\\partial \\mathcal{L}}{\\partial \\alpha} = \\sum_{t=1}^{T} \\left[ \\ln(k_{c_t}^{(t-1)} + A) - \\frac{\\sum_{j=0}^{N^{(t)}-1} (k_j^{(t-1)} + A)^\\alpha \\ln(k_j^{(t-1)} + A)}{\\sum_{j=0}^{N^{(t)}-1} (k_j^{(t-1)} + A)^\\alpha} \\right] $$\n分数项是在事件 $t$ 时，模型概率分布下对数吸引力 $E[\\ln(k+A)]$ 的期望值。\n\n**3. 数值最大化**\n我们将通过数值最小化负对数似然函数 $-\\mathcal{L}(A, \\alpha)$ 来找到MLE参数 $(\\widehat{A}, \\widehat{\\alpha})$，约束条件为 $A \\in [0, 100]$ 和 $\\alpha \\in [0.1, 2.0]$。我们使用 L-BFGS-B 算法，这是一种拟牛顿法，非常适合处理带边界约束的优化问题，并能利用我们推导出的解析梯度。\n\n实现的核心是一个函数，它针对给定的参数集 $(A, \\alpha)$ 和一个观测数据集，逐个事件地模拟网络增长。在每个事件中，它计算对总对数似然及其梯度的贡献，并累积这些值。该函数返回总负对数似然及其对应的负梯度向量 $[-\\frac{\\partial \\mathcal{L}}{\\partial A}, -\\frac{\\partial \\mathcal{L}}{\\partial \\alpha}]$，这些值随后被传递给优化器。对三个给定的数据集重复此过程，以找到它们各自的参数估计值。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import optimize\n\ndef solve():\n    \"\"\"\n    Main function to perform MLE for the generalized preferential attachment model\n    on three datasets.\n    \"\"\"\n    test_cases = [\n        # Dataset 1 (general case)\n        {\n            \"initial_degrees\": [1, 1],\n            \"chosen_indices\": [0, 0, 1, 0, 1, 0, 0, 1, 0, 0]\n        },\n        # Dataset 2 (balanced attachments)\n        {\n            \"initial_degrees\": [1, 1, 1],\n            \"chosen_indices\": [0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2]\n        },\n        # Dataset 3 (extreme concentration)\n        {\n            \"initial_degrees\": [1, 1, 1],\n            \"chosen_indices\": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n        }\n    ]\n\n    def neg_log_likelihood_and_grad(params, initial_degrees, chosen_indices):\n        \"\"\"\n        Calculates the negative log-likelihood and its gradient for the given data.\n        \n        This function simulates the network growth process step-by-step based on the\n        observed attachment sequence. At each step, it calculates and accumulates\n        the log-likelihood and its partial derivatives with respect to A and alpha.\n        \"\"\"\n        A, alpha = params\n        \n        # Ensure parameters are within valid domain for calculation.\n        # This is mainly a safeguard, as bounds are handled by the optimizer.\n        if A  0 or alpha = 0:\n            return np.inf, np.zeros(2)\n\n        log_likelihood = 0.0\n        grad_A = 0.0\n        grad_alpha = 0.0\n        \n        # Use a list for degrees as it grows dynamically\n        degrees = list(initial_degrees)\n        \n        for t, chosen_node_idx in enumerate(chosen_indices):\n            # Convert to numpy array for vectorized operations\n            k_vals = np.array(degrees, dtype=np.float64)\n            \n            # The base of the attachment kernel\n            base = k_vals + A\n            \n            # The attachment kernel (k+A)^alpha\n            kernel_vals = base**alpha\n            S_t = np.sum(kernel_vals)\n\n            if S_t == 0: # Avoid division by zero\n                # This case should not be reached with the given constraints k>=1, A>=0, alpha>0\n                return np.inf, np.zeros(2)\n\n            k_chosen = k_vals[chosen_node_idx]\n            base_chosen = k_chosen + A\n\n            # Log-likelihood contribution for event t\n            log_likelihood += alpha * np.log(base_chosen) - np.log(S_t)\n            \n            # Gradient contribution for event t\n            # Gradient w.r.t. A\n            grad_S_t_A_num = np.sum(alpha * base**(alpha - 1.0))\n            grad_A += alpha / base_chosen - grad_S_t_A_num / S_t\n            \n            # Gradient w.r.t. alpha\n            log_base = np.log(base)\n            grad_S_t_alpha_num = np.sum(kernel_vals * log_base)\n            grad_alpha += np.log(base_chosen) - grad_S_t_alpha_num / S_t\n            \n            # Update degrees for the next event\n            degrees[chosen_node_idx] += 1\n            degrees.append(1) # New node with degree 1\n            \n        return -log_likelihood, -np.array([grad_A, grad_alpha])\n\n    results = []\n    bounds = [(0, 100), (0.1, 2.0)]\n    initial_guess = [1.0, 1.0]\n\n    for case in test_cases:\n        args = (case[\"initial_degrees\"], case[\"chosen_indices\"])\n        \n        res = optimize.minimize(\n            fun=neg_log_likelihood_and_grad,\n            x0=initial_guess,\n            args=args,\n            method='L-BFGS-B',\n            jac=True,  # Our function returns the jacobian (gradient)\n            bounds=bounds,\n            options={'disp': False}\n        )\n        \n        A_hat, alpha_hat = res.x\n        results.append([round(A_hat, 6), round(alpha_hat, 6)])\n\n    # The final print statement must match the required format exactly.\n    # repr() creates a string representation, and .replace removes spaces\n    # to match the symbolic format in the prompt.\n    final_output_string = repr(results).replace(\" \", \"\")\n    print(final_output_string)\n\nsolve()\n```"
        }
    ]
}