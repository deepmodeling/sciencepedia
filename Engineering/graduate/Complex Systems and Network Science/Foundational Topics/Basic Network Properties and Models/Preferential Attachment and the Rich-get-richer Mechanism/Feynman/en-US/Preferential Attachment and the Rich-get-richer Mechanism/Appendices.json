{
    "hands_on_practices": [
        {
            "introduction": "To truly grasp the \"rich-get-richer\" principle, it is essential to work through its mechanics at the most fundamental level. This first exercise  invites you to calculate the attachment probability at the very beginning of a network's life, using the canonical Barabási-Albert model. By focusing on a single growth step, you will solidify your understanding of how a node's degree, $k$, directly and proportionally influences its chances of acquiring new links.",
            "id": "1471153",
            "problem": "The growth of many complex biological networks, from protein interactions to metabolic pathways, can be described by simple generative models. One of the most well-known is the Barabási-Albert (BA) model, which relies on a 'rich-get-richer' mechanism known as preferential attachment. According to this model, when a new node is added to a network, the probability of it connecting to an existing node $i$ is proportional to the degree of that node, $k_i$. The probability is given by the formula $P(i) = \\frac{k_i}{\\sum_j k_j}$, where the sum in the denominator is taken over all nodes $j$ already in the network.\n\nConsider the very first step in the growth of such a network. The initial state consists of a small core of $m_0 = 3$ nodes, which we will label Node 1, Node 2, and Node 3. These three nodes are fully interconnected, forming a complete graph (a triangle). This means Node 1 is connected to both Node 2 and Node 3, Node 2 is connected to both Node 1 and Node 3, and Node 3 is connected to both Node 1 and Node 2.\n\nA new node, Node 4, is now added to the network. It will form exactly $m=1$ new edge, connecting to one of the original three nodes. Following the rule of preferential attachment, determine the probability that Node 4 connects to Node 1.\n\nExpress your answer as a simplified fraction.",
            "solution": "The problem asks for the probability that a new node (Node 4) connects to a specific existing node (Node 1) in a network growing according to the Barabási-Albert model of preferential attachment.\n\nThe core principle of preferential attachment states that the probability $P(i)$ for a new node to connect to an existing node $i$ is given by:\n$$P(i) = \\frac{k_i}{\\sum_j k_j}$$\nwhere $k_i$ is the degree of node $i$ (i.e., the number of edges connected to it), and the denominator is the sum of the degrees of all existing nodes in the network.\n\nFirst, we must determine the degrees of all nodes in the initial network. The network consists of three nodes (1, 2, and 3) that are fully interconnected, forming a triangle.\n\nThe degree of Node 1, $k_1$, is the number of connections it has. Node 1 is connected to Node 2 and Node 3. Therefore, its degree is $k_1 = 2$.\n\nThe degree of Node 2, $k_2$, is the number of connections it has. Node 2 is connected to Node 1 and Node 3. Therefore, its degree is $k_2 = 2$.\n\nThe degree of Node 3, $k_3$, is the number of connections it has. Node 3 is connected to Node 1 and Node 2. Therefore, its degree is $k_3 = 2$.\n\nNext, we calculate the sum of the degrees of all existing nodes in the network, which forms the denominator of the probability formula.\n$$\\sum_j k_j = k_1 + k_2 + k_3 = 2 + 2 + 2 = 6$$\n\nNow, we can calculate the probability that the new Node 4 connects to Node 1. For this calculation, the target node is Node 1, so we use its degree, $k_1=2$, in the numerator.\n$$P(\\text{Node 4 connects to Node 1}) = \\frac{k_1}{\\sum_j k_j} = \\frac{2}{6}$$\n\nFinally, we simplify the resulting fraction as requested.\n$$P(\\text{Node 4 connects to Node 1}) = \\frac{2}{6} = \\frac{1}{3}$$\n\nThus, the probability that the new node connects to Node 1 is $\\frac{1}{3}$.",
            "answer": "$$\\boxed{\\frac{1}{3}}$$"
        },
        {
            "introduction": "While single-step calculations build intuition, the true power of generative models lies in predicting the large-scale statistical properties of the resulting network. This problem  challenges you to use the continuum method, a powerful mean-field technique, to derive the asymptotic degree distribution for a generalized preferential attachment model with an attachment probability proportional to $k_i + A$. Mastering this derivation connects the microscopic attachment rule to the emergent macroscopic power-law exponent, $\\gamma$, revealing the mathematical heart of how scale-free networks emerge.",
            "id": "4298183",
            "problem": "Consider a growing undirected network initialized at time $t=0$ with a finite seed of $N_{0}$ nodes and $E_{0}$ edges. At each discrete time step $t \\mapsto t+1$, a single new node is introduced that connects to exactly $m \\ge 1$ pre-existing nodes by $m$ new edges. The targets of these $m$ edges are chosen independently without self-loops with probability proportional to their current degree plus a constant attractiveness, that is, a node $i$ with degree $k_{i}(t)$ is selected as an endpoint of a new edge with instantaneous probability proportional to $k_{i}(t) + A$, where $A$ is a real constant independent of $i$ and $t$. Assume that there is no edge deletion or rewiring and that no edges are created between old nodes other than through attachment from the new node.\n\nUsing the continuum method based on rate equations and the large-network limit (i.e., replacing sums by their expectations and $k_{i}(t)$ by its mean-field trajectory), derive the asymptotic power-law exponent $\\gamma$ of the degree distribution $P(k) \\sim k^{-\\gamma}$ as a function of $m$ and $A$. Clearly articulate the modeling and parameter conditions under which your derivation is valid. Provide your final answer for $\\gamma$ as a single analytic expression in terms of $m$ and $A$.",
            "solution": "The problem asks for the derivation of the asymptotic power-law exponent $\\gamma$ for the degree distribution $P(k)$ of a growing network. The network starts with $N_0$ nodes and $E_0$ edges. At each time step, a new node is added, which forms $m \\ge 1$ edges to existing nodes. The probability of an existing node $i$ with degree $k_i(t)$ being chosen as a target for a new edge is proportional to $k_i(t) + A$, where $A$ is a constant. We will use the continuum method in the large-network limit as specified.\n\nFirst, we formalize the attachment probability. The probability $\\Pi_i(t)$ that one of the $m$ new edges connects to a specific existing node $i$ is given by:\n$$\n\\Pi_i(t) = \\frac{k_i(t) + A}{\\sum_{j=1}^{N(t)} (k_j(t) + A)}\n$$\nwhere $N(t)$ is the total number of nodes at time $t$. For the probablistic interpretation to be valid, the attachment kernel $k_i(t)+A$ must be non-negative for all nodes. The minimum degree a node can have after its creation is $m$. Thus, we must impose the condition $k+A \\ge 0$ for all $k \\ge m$, which leads to the constraint $m+A \\ge 0$, or $A \\ge -m$.\n\nThe total number of nodes at time $t$ is $N(t) = N_0 + t$.\nThe total number of edges at time $t$ is $E(t) = E_0 + mt$.\nThe sum of all degrees in the network is $\\sum_j k_j(t) = 2E(t) = 2(E_0 + mt)$.\n\nWe can now evaluate the denominator of $\\Pi_i(t)$:\n$$\n\\sum_{j=1}^{N(t)} (k_j(t) + A) = \\sum_{j=1}^{N(t)} k_j(t) + \\sum_{j=1}^{N(t)} A = 2E(t) + A \\cdot N(t)\n$$\nSubstituting the expressions for $N(t)$ and $E(t)$:\n$$\n\\sum_{j=1}^{N(t)} (k_j(t) + A) = 2(E_0 + mt) + A(N_0 + t) = (2m+A)t + 2E_0 + AN_0\n$$\nIn the large-network limit ($t \\to \\infty$), we can neglect the constant terms:\n$$\n\\sum_{j=1}^{N(t)} (k_j(t) + A) \\approx (2m+A)t\n$$\nThis approximation is valid for $2m+A \\ne 0$. The condition $A \\ge -m$ with $m \\ge 1$ ensures that $2m+A \\ge 2m-m = m \\ge 1$, so the denominator is always positive and non-zero.\n\nWe now apply the continuum method to model the degree evolution of a node $i$. Let $k_i(t)$ be the expected degree of node $i$ at time $t$. Its rate of change is proportional to the probability of it receiving a new link. Since $m$ new edges are added per unit time, the rate equation is:\n$$\n\\frac{dk_i}{dt} = m \\cdot \\Pi_i(t) = m \\frac{k_i(t) + A}{(2m+A)t}\n$$\nThis is a first-order linear ordinary differential equation for $k_i(t)$. We can solve it by separation of variables:\n$$\n\\frac{dk_i}{k_i + A} = \\frac{m}{2m+A} \\frac{dt}{t}\n$$\nLet's integrate this equation for a node $i$ that was introduced at time $t_i$. The initial condition for this node is that its degree at the moment of its creation is $m$, so $k_i(t_i) = m$. We integrate from the time of introduction $t_i$ to the current time $t$:\n$$\n\\int_{m}^{k_i(t)} \\frac{dk}{k+A} = \\int_{t_i}^{t} \\frac{m}{2m+A} \\frac{d\\tau}{\\tau}\n$$\nThe integration yields:\n$$\n\\left[ \\ln(k+A) \\right]_{m}^{k_i(t)} = \\frac{m}{2m+A} \\left[ \\ln(\\tau) \\right]_{t_i}^{t}\n$$\n$$\n\\ln(k_i(t)+A) - \\ln(m+A) = \\frac{m}{2m+A} (\\ln(t) - \\ln(t_i))\n$$\nRearranging the terms:\n$$\n\\ln\\left(\\frac{k_i(t)+A}{m+A}\\right) = \\ln\\left(\\left(\\frac{t}{t_i}\\right)^{\\frac{m}{2m+A}}\\right)\n$$\nExponentiating both sides gives the solution for $k_i(t)$:\n$$\nk_i(t) = (m+A)\\left(\\frac{t}{t_i}\\right)^{\\beta} - A\n$$\nwhere the exponent $\\beta$ is defined as:\n$$\n\\beta = \\frac{m}{2m+A}\n$$\nThis equation describes the deterministic evolution of the degree of a node created at time $t_i$. To find the degree distribution $P(k)$, we relate the degree $k$ to the creation time $t_i$. Since nodes are added at a constant rate, the distribution of creation times $t_i$ for a randomly selected node at time $t$ is uniform over the interval $[0, t]$ (for large $N_0$ and $t$, we can approximate the starting time as $0$). The probability that a node was created before time $t_i$ is $P(\\text{creation time}  t_i) = t_i/t$.\n\nThe cumulative degree distribution $P(Kk)$ is the probability that a randomly chosen node has a degree greater than $k$. This is equivalent to the probability that its creation time $t_i$ is small enough for its degree to have grown larger than $k$.\n$$\nP(Kk) = P(k_i(t)  k)\n$$\nUsing the expression for $k_i(t)$:\n$$\n(m+A)\\left(\\frac{t}{t_i}\\right)^{\\beta} - A  k \\implies \\left(\\frac{t}{t_i}\\right)^{\\beta}  \\frac{k+A}{m+A}\n$$\n$$\n\\frac{t}{t_i}  \\left(\\frac{k+A}{m+A}\\right)^{1/\\beta} \\implies t_i  t \\left(\\frac{k+A}{m+A}\\right)^{-1/\\beta}\n$$\nSo, $P(Kk)$ is the probability that a node's creation time $t_i$ is less than this value:\n$$\nP(Kk) = \\frac{t \\left(\\frac{k+A}{m+A}\\right)^{-1/\\beta}}{t} = \\left(\\frac{k+A}{m+A}\\right)^{-1/\\beta}\n$$\nThe probability density function $P(k)$ is the negative derivative of the cumulative distribution $P(Kk)$ with respect to $k$:\n$$\nP(k) = -\\frac{d}{dk} P(Kk) = -\\frac{d}{dk} \\left[ (m+A)^{1/\\beta} (k+A)^{-1/\\beta} \\right]\n$$\n$$\nP(k) = - (m+A)^{1/\\beta} \\left(-\\frac{1}{\\beta}\\right) (k+A)^{-1/\\beta - 1} = \\frac{(m+A)^{1/\\beta}}{\\beta} (k+A)^{-(1+1/\\beta)}\n$$\nFor large degrees $k$, the term $(k+A)$ is asymptotically proportional to $k$. Thus, the degree distribution follows a power law:\n$$\nP(k) \\sim k^{-\\gamma}\n$$\nBy comparing the exponents, we identify the power-law exponent $\\gamma$:\n$$\n\\gamma = 1 + \\frac{1}{\\beta}\n$$\nSubstituting the expression for $\\beta = \\frac{m}{2m+A}$:\n$$\n\\gamma = 1 + \\frac{2m+A}{m} = 1 + 2 + \\frac{A}{m} = 3 + \\frac{A}{m}\n$$\nThis derivation is valid under the following conditions:\n1.  **Large-Network and Large-Time Limit**: The derivation assumes $t \\to \\infty$ and $N(t) \\to \\infty$, which justifies the continuum approximation and the neglect of initial conditions $N_0$ and $E_0$.\n2.  **Mean-Field Approximation**: The stochastic nature of node degree evolution is replaced by a deterministic mean-field trajectory $k_i(t)$. This ignores fluctuations, which is valid for large networks.\n3.  **Physical Probability**: The attachment kernel $k+A$ must be non-negative for all possible degrees $k \\ge m$. This imposes the condition $m+A \\ge 0$, or $A \\ge -m$. If this condition is met, the denominator $(2m+A)t$ is also guaranteed to be positive for $t0$ and $m\\ge1$.\n\nTherefore, the asymptotic power-law exponent of the degree distribution is $\\gamma = 3 + A/m$.",
            "answer": "$$ \\boxed{3 + \\frac{A}{m}} $$"
        },
        {
            "introduction": "Theoretical models are most powerful when they can be tested against and inferred from real-world data. This advanced practice  moves from abstract theory to empirical application, asking you to perform parameter estimation for a generalized preferential attachment model where the attachment kernel is proportional to $(k + A)^{\\alpha}$. You will construct a log-likelihood function and use it to find the model parameters that best explain a given attachment history, a core skill in computational social science and systems biology.",
            "id": "4298151",
            "problem": "You are given a discrete-time network growth process consistent with a generalized preferential attachment mechanism. At each event, a new node arrives and attaches to exactly one existing node. Let $k_i^{(t)}$ denote the degree of existing node $i$ immediately before event $t$ (that is, strictly prior to the arrival and attachment of the new node at event $t$). The attachment probability that the new node selects existing node $i$ at event $t$ is assumed proportional to the nonnegative kernel $(k_i^{(t)} + A)^\\alpha$, where $A \\ge 0$ is the initial attractiveness and $\\alpha  0$ controls the nonlinearity of preferential attachment.\n\nFundamental base:\n- Core definition of preferential attachment: the probability to attach to an existing node is proportional to a nondecreasing function of its current degree.\n- Well-tested modeling fact used here: generalized preferential attachment kernels of the form $(k + A)^\\alpha$, with $A \\ge 0$ and $\\alpha  0$, capture initial attractiveness and nonlinear rich-get-richer mechanisms.\n\nYou observe a full growth dataset consisting of:\n- An initial degree vector for the pre-existing nodes at time $t=0$.\n- A sequence of chosen indices, one per event, indicating which existing node received the new edge at that event.\n\nAssume:\n- Each event adds exactly one new node and exactly one new edge, connecting the new node to the chosen existing node from the set of nodes present before the event.\n- Node labels are fixed in arrival order: initial nodes are labeled starting at $0$, and the new node introduced at event $t$ receives label equal to the current number of nodes before the event.\n- Degrees update deterministically: when node $i$ is chosen at event $t$, its degree increases by $1$, and the newly added node starts with degree $1$ and becomes available in subsequent events.\n\nTask:\n1. Construct the log-likelihood function for parameters $A$ and $\\alpha$ under the assumption that, conditional on the degree configuration immediately before each event, the observed attachment event is drawn from the kernel $(k_i^{(t)} + A)^\\alpha$ normalized over all existing nodes.\n2. Implement Maximum Likelihood Estimation (MLE) to estimate $A$ and $\\alpha$ from the given datasets by numerically maximizing the log-likelihood with respect to $A$ and $\\alpha$ subject to the constraints $A \\ge 0$ and $\\alpha  0$.\n3. Use gradient-based numerical optimization. You must provide and use analytic gradients of the log-likelihood with respect to $A$ and $\\alpha$.\n\nTest suite:\nProvide MLE results for the following three datasets. In each dataset, the initial degree vector and the sequence of chosen indices are specified.\n\n- Dataset $1$ (general case):\n  - Initial degrees: $[1,1]$.\n  - Chosen indices by event: $[0,0,1,0,1,0,0,1,0,0]$.\n\n- Dataset $2$ (balanced attachments, exercising sensitivity to $A$):\n  - Initial degrees: $[1,1,1]$.\n  - Chosen indices by event: $[0,1,2,0,1,2,0,1,2,0,1,2]$.\n\n- Dataset $3$ (extreme concentration, exercising sensitivity to $\\alpha$):\n  - Initial degrees: $[1,1,1]$.\n  - Chosen indices by event: $[0,0,0,0,0,0,0,0,0,0,0,0]$.\n\nNumerical and output requirements:\n- Optimize over $A$ and $\\alpha$ with bounds $A \\in [0,100]$ and $\\alpha \\in [0.1,2.0]$.\n- Each dataset should yield an estimated pair $\\widehat{A}$ and $\\widehat{\\alpha}$.\n- Express each estimate as a float rounded to $6$ decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each dataset’s result is itself a two-element list in the order $[\\widehat{A},\\widehat{\\alpha}]$. For example: $[[\\widehat{A}_1,\\widehat{\\alpha}_1],[\\widehat{A}_2,\\widehat{\\alpha}_2],[\\widehat{A}_3,\\widehat{\\alpha}_3]]$.",
            "solution": "The problem requires performing Maximum Likelihood Estimation (MLE) for the parameters of a generalized preferential attachment model. The model is defined by an attachment probability kernel proportional to $(k+A)^\\alpha$, where $k$ is the node degree, $A \\ge 0$ is an initial attractiveness parameter, and $\\alpha  0$ controls the super- or sub-linearity of the attachment preference. We are given full observational data for three distinct network growth histories and must estimate the parameter pair $(A, \\alpha)$ for each. This requires deriving the log-likelihood function and its gradients, then using a numerical optimization algorithm to find the parameters that maximize this function.\n\n**1. Log-Likelihood Function Derivation**\n\nLet the parameters be denoted by the vector $\\theta = (A, \\alpha)$. The observed data for a single growth process consists of an initial degree vector $\\mathbf{k}^{(0)} = (k_0^{(0)}, \\dots, k_{N_0-1}^{(0)})$ for $N_0$ initial nodes, and a sequence of chosen nodes $\\mathbf{c} = (c_1, c_2, \\dots, c_T)$ over $T$ discrete time events.\n\nAt each event $t \\in \\{1, 2, \\dots, T\\}$, a new node arrives and attaches to one of the $N^{(t)} = N_0 + t - 1$ existing nodes. The set of nodes present just before event $t$ is $V^{(t)} = \\{0, 1, \\dots, N^{(t)}-1\\}$, and their degrees are given by the vector $\\mathbf{k}^{(t-1)}$.\n\nAccording to the model, the probability of the new node attaching to a specific existing node $i \\in V^{(t)}$ is given by the normalized kernel:\n$$ P(\\text{choose } i | \\mathbf{k}^{(t-1)}, A, \\alpha) = \\frac{(k_i^{(t-1)} + A)^\\alpha}{\\sum_{j=0}^{N^{(t)}-1} (k_j^{(t-1)} + A)^\\alpha} $$\nThe events are assumed to be independent conditional on the state of the network at each step. Therefore, the total likelihood of observing the entire sequence of chosen nodes $\\mathbf{c}$ is the product of the probabilities of each individual event:\n$$ L(A, \\alpha | \\mathbf{k}^{(0)}, \\mathbf{c}) = \\prod_{t=1}^{T} P(\\text{chosen node is } c_t | \\mathbf{k}^{(t-1)}, A, \\alpha) $$\nSubstituting the probability expression, we get:\n$$ L(A, \\alpha) = \\prod_{t=1}^{T} \\frac{(k_{c_t}^{(t-1)} + A)^\\alpha}{\\sum_{j=0}^{N^{(t)}-1} (k_j^{(t-1)} + A)^\\alpha} $$\nFor numerical stability and mathematical convenience, we work with the log-likelihood function, $\\mathcal{L}(A, \\alpha) = \\ln L(A, \\alpha)$. The logarithm transforms the product into a sum:\n$$ \\mathcal{L}(A, \\alpha) = \\sum_{t=1}^{T} \\ln \\left( \\frac{(k_{c_t}^{(t-1)} + A)^\\alpha}{\\sum_{j=0}^{N^{(t)}-1} (k_j^{(t-1)} + A)^\\alpha} \\right) $$\n$$ \\mathcal{L}(A, \\alpha) = \\sum_{t=1}^{T} \\left[ \\alpha \\ln(k_{c_t}^{(t-1)} + A) - \\ln \\left( \\sum_{j=0}^{N^{(t)}-1} (k_j^{(t-1)} + A)^\\alpha \\right) \\right] $$\nThis function represents the log-probability of observing the given attachment sequence for a specific choice of parameters $A$ and $\\alpha$. To find the MLE estimates $(\\widehat{A}, \\widehat{\\alpha})$, we must find the values of $A$ and $\\alpha$ that maximize this function, subject to the given constraints.\n\nThe degree vectors $\\mathbf{k}^{(t-1)}$ are not fixed but evolve according to the process. Starting with $\\mathbf{k}^{(0)}$, the degree vector at the end of event $t$, denoted $\\mathbf{k}^{(t)}$, is obtained by updating $\\mathbf{k}^{(t-1)}$: the degree of the chosen node $c_t$ is incremented by $1$, and a new node with degree $1$ is added to the network.\n\n**2. Gradient Derivation**\nFor efficient numerical optimization, we must compute the partial derivatives of the log-likelihood function with respect to $A$ and $\\alpha$.\n\nLet the normalization sum at event $t$ be $S_t(A, \\alpha) = \\sum_{j=0}^{N^{(t)}-1} (k_j^{(t-1)} + A)^\\alpha$. The log-likelihood is:\n$$ \\mathcal{L}(A, \\alpha) = \\sum_{t=1}^{T} \\left[ \\alpha \\ln(k_{c_t}^{(t-1)} + A) - \\ln(S_t(A, \\alpha)) \\right] $$\n\n**Gradient with respect to $A$**:\nWe differentiate $\\mathcal{L}$ with respect to $A$:\n$$ \\frac{\\partial \\mathcal{L}}{\\partial A} = \\sum_{t=1}^{T} \\left[ \\frac{\\partial}{\\partial A} \\left(\\alpha \\ln(k_{c_t}^{(t-1)} + A)\\right) - \\frac{\\partial}{\\partial A} \\left(\\ln(S_t)\\right) \\right] $$\n$$ \\frac{\\partial \\mathcal{L}}{\\partial A} = \\sum_{t=1}^{T} \\left[ \\frac{\\alpha}{k_{c_t}^{(t-1)} + A} - \\frac{1}{S_t} \\frac{\\partial S_t}{\\partial A} \\right] $$\nThe derivative of the sum $S_t$ with respect to $A$ is:\n$$ \\frac{\\partial S_t}{\\partial A} = \\sum_{j=0}^{N^{(t)}-1} \\frac{\\partial}{\\partial A} (k_j^{(t-1)} + A)^\\alpha = \\sum_{j=0}^{N^{(t)}-1} \\alpha (k_j^{(t-1)} + A)^{\\alpha - 1} $$\nSubstituting this back, the full gradient with respect to $A$ is:\n$$ \\frac{\\partial \\mathcal{L}}{\\partial A} = \\alpha \\sum_{t=1}^{T} \\left[ \\frac{1}{k_{c_t}^{(t-1)} + A} - \\frac{\\sum_{j=0}^{N^{(t)}-1} (k_j^{(t-1)} + A)^{\\alpha - 1}}{\\sum_{j=0}^{N^{(t)}-1} (k_j^{(t-1)} + A)^\\alpha} \\right] $$\n\n**Gradient with respect to $\\alpha$**:\nSimilarly, we differentiate $\\mathcal{L}$ with respect to $\\alpha$:\n$$ \\frac{\\partial \\mathcal{L}}{\\partial \\alpha} = \\sum_{t=1}^{T} \\left[ \\frac{\\partial}{\\partial \\alpha} \\left(\\alpha \\ln(k_{c_t}^{(t-1)} + A)\\right) - \\frac{\\partial}{\\partial \\alpha} \\left(\\ln(S_t)\\right) \\right] $$\n$$ \\frac{\\partial \\mathcal{L}}{\\partial \\alpha} = \\sum_{t=1}^{T} \\left[ \\ln(k_{c_t}^{(t-1)} + A) - \\frac{1}{S_t} \\frac{\\partial S_t}{\\partial \\alpha} \\right] $$\nThe derivative of the sum $S_t$ with respect to $\\alpha$ is (using $\\frac{d}{dx} a^x = a^x \\ln a$):\n$$ \\frac{\\partial S_t}{\\partial \\alpha} = \\sum_{j=0}^{N^{(t)}-1} \\frac{\\partial}{\\partial \\alpha} (k_j^{(t-1)} + A)^\\alpha = \\sum_{j=0}^{N^{(t)}-1} (k_j^{(t-1)} + A)^\\alpha \\ln(k_j^{(t-1)} + A) $$\nSubstituting this back, the full gradient with respect to $\\alpha$ is:\n$$ \\frac{\\partial \\mathcal{L}}{\\partial \\alpha} = \\sum_{t=1}^{T} \\left[ \\ln(k_{c_t}^{(t-1)} + A) - \\frac{\\sum_{j=0}^{N^{(t)}-1} (k_j^{(t-1)} + A)^\\alpha \\ln(k_j^{(t-1)} + A)}{\\sum_{j=0}^{N^{(t)}-1} (k_j^{(t-1)} + A)^\\alpha} \\right] $$\nThe fractional term is the expected value of the log-attractiveness, $E[\\ln(k+A)]$, under the model's probability distribution at event $t$.\n\n**3. Numerical Maximization**\nWe will find the MLE parameters $(\\widehat{A}, \\widehat{\\alpha})$ by numerically minimizing the negative log-likelihood, $-\\mathcal{L}(A, \\alpha)$, subject to the bounds $A \\in [0, 100]$ and $\\alpha \\in [0.1, 2.0]$. We use the L-BFGS-B algorithm, a quasi-Newton method that is well-suited for optimization with box constraints and utilizes the analytic gradients we have derived.\n\nThe core of the implementation is a function that, for a given parameter set $(A, \\alpha)$ and an observed dataset, simulates the network growth event by event. At each event, it calculates the contribution to the total log-likelihood and its gradients, and accumulates these values. The function returns the total negative log-likelihood and its corresponding negative gradient vector $[-\\frac{\\partial \\mathcal{L}}{\\partial A}, -\\frac{\\partial \\mathcal{L}}{\\partial \\alpha}]$, which are then passed to the optimizer. This process is repeated for each of the three provided datasets to find their respective parameter estimates.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import optimize\n\ndef solve():\n    \"\"\"\n    Main function to perform MLE for the generalized preferential attachment model\n    on three datasets.\n    \"\"\"\n    test_cases = [\n        # Dataset 1 (general case)\n        {\n            \"initial_degrees\": [1, 1],\n            \"chosen_indices\": [0, 0, 1, 0, 1, 0, 0, 1, 0, 0]\n        },\n        # Dataset 2 (balanced attachments)\n        {\n            \"initial_degrees\": [1, 1, 1],\n            \"chosen_indices\": [0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2]\n        },\n        # Dataset 3 (extreme concentration)\n        {\n            \"initial_degrees\": [1, 1, 1],\n            \"chosen_indices\": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n        }\n    ]\n\n    def neg_log_likelihood_and_grad(params, initial_degrees, chosen_indices):\n        \"\"\"\n        Calculates the negative log-likelihood and its gradient for the given data.\n        \n        This function simulates the network growth process step-by-step based on the\n        observed attachment sequence. At each step, it calculates and accumulates\n        the log-likelihood and its partial derivatives with respect to A and alpha.\n        \"\"\"\n        A, alpha = params\n        \n        # Ensure parameters are within valid domain for calculation.\n        # This is mainly a safeguard, as bounds are handled by the optimizer.\n        if A  0 or alpha = 0:\n            return np.inf, np.zeros(2)\n\n        log_likelihood = 0.0\n        grad_A = 0.0\n        grad_alpha = 0.0\n        \n        # Use a list for degrees as it grows dynamically\n        degrees = list(initial_degrees)\n        \n        for t, chosen_node_idx in enumerate(chosen_indices):\n            # Convert to numpy array for vectorized operations\n            k_vals = np.array(degrees, dtype=np.float64)\n            \n            # The base of the attachment kernel\n            base = k_vals + A\n            \n            # The attachment kernel (k+A)^alpha\n            kernel_vals = base**alpha\n            S_t = np.sum(kernel_vals)\n\n            if S_t == 0: # Avoid division by zero\n                # This case should not be reached with the given constraints k=1, A=0, alpha0\n                return np.inf, np.zeros(2)\n\n            k_chosen = k_vals[chosen_node_idx]\n            base_chosen = k_chosen + A\n\n            # Log-likelihood contribution for event t\n            log_likelihood += alpha * np.log(base_chosen) - np.log(S_t)\n            \n            # Gradient contribution for event t\n            # Gradient w.r.t. A\n            grad_S_t_A_num = np.sum(alpha * base**(alpha - 1.0))\n            grad_A += alpha * (1.0 / base_chosen - grad_S_t_A_num / S_t)\n            \n            # Gradient w.r.t. alpha\n            log_base = np.log(base)\n            grad_S_t_alpha_num = np.sum(kernel_vals * log_base)\n            grad_alpha += np.log(base_chosen) - grad_S_t_alpha_num / S_t\n            \n            # Update degrees for the next event\n            degrees[chosen_node_idx] += 1\n            degrees.append(1) # New node with degree 1\n            \n        return -log_likelihood, -np.array([grad_A, grad_alpha])\n\n    results = []\n    bounds = [(0, 100), (0.1, 2.0)]\n    initial_guess = [1.0, 1.0]\n\n    for case in test_cases:\n        args = (case[\"initial_degrees\"], case[\"chosen_indices\"])\n        \n        res = optimize.minimize(\n            fun=neg_log_likelihood_and_grad,\n            x0=initial_guess,\n            args=args,\n            method='L-BFGS-B',\n            jac=True,  # Our function returns the jacobian (gradient)\n            bounds=bounds,\n            options={'disp': False}\n        )\n        \n        A_hat, alpha_hat = res.x\n        results.append([round(A_hat, 6), round(alpha_hat, 6)])\n\n    # The final print statement must match the required format exactly.\n    # repr() creates a string representation, and .replace removes spaces\n    # to match the symbolic format in the prompt.\n    final_output_string = repr(results).replace(\" \", \"\")\n    print(final_output_string)\n\nsolve()\n```"
        }
    ]
}