{
    "hands_on_practices": [
        {
            "introduction": "Power-law distributions are fundamentally characterized by their extreme right-skewness, a property that can be difficult to grasp intuitively. This exercise makes the concept concrete by guiding you through a first-principles derivation of the mean and median for the canonical Pareto distribution . By deriving the ratio of the mean to the median, you will see precisely how the tail exponent $\\alpha$ governs the separation between these two measures of central tendency, highlighting why the mean can be a misleading statistic for heavy-tailed systems.",
            "id": "4297927",
            "problem": "A researcher studying a large technological network finds that the complementary cumulative distribution function (CCDF) of node degrees is approximately linear on a log-log plot over several orders of magnitude beyond a cutoff degree $x_{\\min}$. Motivated by this, assume the degree distribution above $x_{\\min}$ follows a Pareto (Type I) distribution with scale parameter $x_{\\min}  0$ and shape parameter $\\alpha  0$, having probability density function $f(x)$ supported on $x \\ge x_{\\min}$. Starting only from the definitions of the probability density function $f(x)$, the cumulative distribution function $F(x)$, and the definitions of the median and the mean, perform the following:\n\n- Using $f(x) = \\alpha x_{\\min}^{\\alpha} x^{-(\\alpha+1)}$ for $x \\ge x_{\\min}$ and $f(x) = 0$ otherwise, derive the cumulative distribution function $F(x)$.\n- Using the definition that the median $m$ satisfies $F(m) = \\tfrac{1}{2}$, solve for $m$ in terms of $x_{\\min}$ and $\\alpha$.\n- Assuming $\\alpha  1$ so that the mean exists, compute the mean $\\mu$ from first principles and then provide the exact expression for the ratio $\\mu/m$ as a function of $\\alpha$.\n\nYour final answer must be a single closed-form analytic expression for $\\mu/m$ in terms of $\\alpha$. Do not approximate or round. Do not include units. Explain each derivation step using only the fundamental definitions stated above. Conclude by briefly stating, in one sentence, how the value of $\\mu/m$ reflects right-skewness for $\\alpha  1$ in heavy-tailed data observed on log-log plots.",
            "solution": "The problem requires the derivation of the ratio of the mean to the median for a Pareto (Type I) distribution, starting from first principles. The problem is well-posed, scientifically sound, and contains all necessary information for a unique solution.\n\nFirst, we derive the cumulative distribution function (CDF), $F(x)$, from the given probability density function (PDF), $f(x) = \\alpha x_{\\min}^{\\alpha} x^{-(\\alpha+1)}$ for $x \\ge x_{\\min}$. The CDF is defined as $F(x) = \\int_{-\\infty}^{x} f(t) dt$. Since the support of the distribution is $[x_{\\min}, \\infty)$, the PDF is zero for $t  x_{\\min}$. Therefore, for any $x \\ge x_{\\min}$, the CDF is given by the integral:\n$$F(x) = \\int_{x_{\\min}}^{x} f(t) dt = \\int_{x_{\\min}}^{x} \\alpha x_{\\min}^{\\alpha} t^{-(\\alpha+1)} dt$$\nWe can factor out the constants with respect to the integration variable $t$:\n$$F(x) = \\alpha x_{\\min}^{\\alpha} \\int_{x_{\\min}}^{x} t^{-(\\alpha+1)} dt$$\nThe integral of $t^n$ is $\\frac{t^{n+1}}{n+1}$. Here, the exponent is $-(\\alpha+1)$, so the integral evaluates to:\n$$F(x) = \\alpha x_{\\min}^{\\alpha} \\left[ \\frac{t^{-(\\alpha+1)+1}}{-(\\alpha+1)+1} \\right]_{x_{\\min}}^{x} = \\alpha x_{\\min}^{\\alpha} \\left[ \\frac{t^{-\\alpha}}{-\\alpha} \\right]_{x_{\\min}}^{x}$$\nEvaluating the expression at the limits of integration:\n$$F(x) = \\alpha x_{\\min}^{\\alpha} \\left( \\frac{x^{-\\alpha}}{-\\alpha} - \\frac{x_{\\min}^{-\\alpha}}{-\\alpha} \\right) = -x_{\\min}^{\\alpha} (x^{-\\alpha} - x_{\\min}^{-\\alpha})$$\n$$F(x) = -x_{\\min}^{\\alpha} x^{-\\alpha} + x_{\\min}^{\\alpha} x_{\\min}^{-\\alpha} = - \\left(\\frac{x_{\\min}}{x}\\right)^{\\alpha} + 1$$\nSo, the cumulative distribution function for $x \\ge x_{\\min}$ is:\n$$F(x) = 1 - \\left(\\frac{x_{\\min}}{x}\\right)^{\\alpha}$$\nFor $x  x_{\\min}$, $F(x) = 0$.\n\nNext, we find the median, $m$. By definition, the median is the value of $x$ for which $F(x) = \\frac{1}{2}$. Since $\\alpha  0$ and $x_{\\min}  0$, the CDF is strictly increasing from $F(x_{\\min}) = 0$ to $\\lim_{x \\to \\infty} F(x) = 1$, so a unique median exists and is greater than $x_{\\min}$. We set $F(m) = \\frac{1}{2}$:\n$$1 - \\left(\\frac{x_{\\min}}{m}\\right)^{\\alpha} = \\frac{1}{2}$$\nRearranging the terms to solve for $m$:\n$$\\left(\\frac{x_{\\min}}{m}\\right)^{\\alpha} = 1 - \\frac{1}{2} = \\frac{1}{2}$$\nTaking the $\\alpha$-th root of both sides:\n$$\\frac{x_{\\min}}{m} = \\left(\\frac{1}{2}\\right)^{1/\\alpha} = 2^{-1/\\alpha}$$\nSolving for $m$:\n$$m = \\frac{x_{\\min}}{2^{-1/\\alpha}} = x_{\\min} 2^{1/\\alpha}$$\n\nNow, we compute the mean, $\\mu$, of the distribution. The mean is the expected value of the random variable $X$, defined by $\\mu = E[X] = \\int_{-\\infty}^{\\infty} x f(x) dx$. The problem states that we assume $\\alpha  1$, which is the condition required for the mean to be finite.\n$$\\mu = \\int_{x_{\\min}}^{\\infty} x \\left( \\alpha x_{\\min}^{\\alpha} x^{-(\\alpha+1)} \\right) dx$$\nCombining the terms involving $x$:\n$$\\mu = \\alpha x_{\\min}^{\\alpha} \\int_{x_{\\min}}^{\\infty} x \\cdot x^{-(\\alpha+1)} dx = \\alpha x_{\\min}^{\\alpha} \\int_{x_{\\min}}^{\\infty} x^{-\\alpha} dx$$\nWe integrate $x^{-\\alpha}$:\n$$\\mu = \\alpha x_{\\min}^{\\alpha} \\left[ \\frac{x^{-\\alpha+1}}{-\\alpha+1} \\right]_{x_{\\min}}^{\\infty}$$\nThe upper limit requires evaluating $\\lim_{x \\to \\infty} x^{1-\\alpha}$. Since we assumed $\\alpha  1$, the exponent $(1-\\alpha)$ is negative, so this limit is $0$.\n$$\\mu = \\frac{\\alpha x_{\\min}^{\\alpha}}{1-\\alpha} \\left( 0 - x_{\\min}^{1-\\alpha} \\right) = \\frac{\\alpha x_{\\min}^{\\alpha}}{-(\\alpha-1)} (-x_{\\min}^{1-\\alpha})$$\n$$\\mu = \\frac{\\alpha}{\\alpha-1} x_{\\min}^{\\alpha} x_{\\min}^{1-\\alpha} = \\frac{\\alpha}{\\alpha-1} x_{\\min}^{\\alpha+1-\\alpha}$$\nThis simplifies to the expression for the mean:\n$$\\mu = \\frac{\\alpha}{\\alpha-1} x_{\\min}$$\n\nFinally, we find the ratio of the mean to the median, $\\mu/m$.\n$$\\frac{\\mu}{m} = \\frac{\\frac{\\alpha}{\\alpha-1} x_{\\min}}{x_{\\min} 2^{1/\\alpha}}$$\nThe parameter $x_{\\min}$ cancels out, yielding the final expression as a function of $\\alpha$ only:\n$$\\frac{\\mu}{m} = \\frac{\\alpha}{(\\alpha-1) 2^{1/\\alpha}}$$\n\nFor $\\alpha  1$, the ratio $\\mu/m$ is always greater than $1$, confirming the right-skewness of the distribution where the mean is pulled above the median by the heavy right tail.",
            "answer": "$$\\boxed{\\frac{\\alpha}{(\\alpha-1) 2^{1/\\alpha}}}$$"
        },
        {
            "introduction": "Once a heavy-tailed phenomenon is identified, the next critical step is to quantify its properties by estimating the power-law exponent $\\alpha$ from data. This practice introduces the gold standard for this task: the principle of Maximum Likelihood Estimation (MLE). You will derive the celebrated closed-form estimator for the exponent of a continuous Pareto tail and establish its asymptotic variance, providing a measure of the estimator's precision . This exercise is fundamental for anyone aiming to move from observing power laws to rigorously modeling them.",
            "id": "4297975",
            "problem": "A researcher studies the heavy-tailed distribution of node strengths in a communication network and models the upper tail using a continuous Pareto (Type I) law. Let $x_{\\min}  0$ be a known lower cutoff and assume the tail is described by the survival function $P(X \\geq x) = \\left(\\frac{x_{\\min}}{x}\\right)^{\\alpha}$ for $x \\geq x_{\\min}$, where $\\alpha  0$ is the unknown tail index. The researcher collects an independent and identically distributed sample $\\{x_{i}\\}_{i=1}^{n}$ from the tail, where each observation satisfies $x_{i} \\geq x_{\\min}$. \n\nStarting only from the survival function definition and the principle of maximum likelihood estimation, derive the closed-form maximum likelihood estimator for the tail index $\\alpha$ and establish the asymptotic variance of this estimator under standard regularity conditions. Express your final results as analytic expressions involving $n$, $x_{\\min}$, and the sample $\\{x_{i}\\}_{i=1}^{n}$. No numerical evaluation is required. If you provide more than one expression, list them in a single row as a matrix. Do not include any units in your final expressions.",
            "solution": "### Derivation of the Maximum Likelihood Estimator (MLE)\n\nThe derivation proceeds by first finding the probability density function (PDF) from the given survival function, then constructing the log-likelihood function for the sample, and finally maximizing this function with respect to the parameter $\\alpha$.\n\n1.  **Probability Density Function (PDF):**\n    The cumulative distribution function (CDF) is $F(x) = P(X \\leq x) = 1 - P(X  x)$. Since the distribution is continuous, $P(Xx) = P(X\\geq x)$. Thus, for $x \\geq x_{\\min}$, the CDF is:\n    $$F(x; \\alpha) = 1 - S(x) = 1 - \\left(\\frac{x_{\\min}}{x}\\right)^{\\alpha}$$\n    The PDF, $p(x; \\alpha)$, is the derivative of the CDF with respect to $x$:\n    $$p(x; \\alpha) = \\frac{d}{dx} F(x; \\alpha) = \\frac{d}{dx} \\left(1 - x_{\\min}^{\\alpha}x^{-\\alpha}\\right) = -x_{\\min}^{\\alpha} (-\\alpha) x^{-\\alpha-1}$$\n    This simplifies to the PDF for the Pareto Type I distribution:\n    $$p(x; \\alpha) = \\frac{\\alpha x_{\\min}^{\\alpha}}{x^{\\alpha+1}} \\quad \\text{for } x \\geq x_{\\min}$$\n\n2.  **Likelihood and Log-Likelihood Functions:**\n    Given an i.i.d. sample $\\{x_{i}\\}_{i=1}^{n}$, the likelihood function $L(\\alpha; \\{x_i\\})$ is the joint probability of observing the data:\n    $$L(\\alpha; \\{x_i\\}) = \\prod_{i=1}^{n} p(x_i; \\alpha) = \\prod_{i=1}^{n} \\frac{\\alpha x_{\\min}^{\\alpha}}{x_i^{\\alpha+1}}$$\n    To simplify maximization, we work with the log-likelihood function, $\\ell(\\alpha) = \\ln(L(\\alpha))$:\n    $$\\ell(\\alpha; \\{x_i\\}) = \\ln\\left( \\prod_{i=1}^{n} \\frac{\\alpha x_{\\min}^{\\alpha}}{x_i^{\\alpha+1}} \\right) = \\sum_{i=1}^{n} \\ln\\left( \\frac{\\alpha x_{\\min}^{\\alpha}}{x_i^{\\alpha+1}} \\right)$$\n    $$\\ell(\\alpha) = \\sum_{i=1}^{n} \\left[ \\ln(\\alpha) + \\alpha \\ln(x_{\\min}) - (\\alpha+1) \\ln(x_i) \\right]$$\n    $$\\ell(\\alpha) = n \\ln(\\alpha) + n\\alpha \\ln(x_{\\min}) - (\\alpha+1) \\sum_{i=1}^{n} \\ln(x_i)$$\n\n3.  **Maximization:**\n    To find the MLE, $\\hat{\\alpha}_{ML}$, we differentiate $\\ell(\\alpha)$ with respect to $\\alpha$ and set the result to zero (the first-order condition):\n    $$\\frac{d\\ell}{d\\alpha} = \\frac{n}{\\alpha} + n \\ln(x_{\\min}) - \\sum_{i=1}^{n} \\ln(x_i) = 0$$\n    Solving for $\\alpha$ gives the estimator $\\hat{\\alpha}_{ML}$:\n    $$\\frac{n}{\\hat{\\alpha}_{ML}} = \\sum_{i=1}^{n} \\ln(x_i) - n \\ln(x_{\\min}) = \\sum_{i=1}^{n} \\left( \\ln(x_i) - \\ln(x_{\\min}) \\right)$$\n    $$\\frac{n}{\\hat{\\alpha}_{ML}} = \\sum_{i=1}^{n} \\ln\\left(\\frac{x_i}{x_{\\min}}\\right)$$\n    Thus, the closed-form MLE for $\\alpha$ is:\n    $$\\hat{\\alpha}_{ML} = \\frac{n}{\\sum_{i=1}^{n} \\ln\\left(\\frac{x_i}{x_{\\min}}\\right)}$$\n    To confirm this is a maximum, we check the second derivative:\n    $$\\frac{d^2\\ell}{d\\alpha^2} = -\\frac{n}{\\alpha^2}$$\n    Since $n0$ and $\\alpha^20$, the second derivative is always negative, confirming the log-likelihood function is concave and the stationary point is a global maximum.\n\n### Derivation of the Asymptotic Variance\n\nThe asymptotic variance of an MLE is given by the inverse of the Fisher information.\n\n1.  **Fisher Information:**\n    The Fisher information $I(\\alpha)$ for a sample of size $n$ is $n$ times the Fisher information for a single observation, $I_1(\\alpha)$. For a single parameter, $I_1(\\alpha)$ can be calculated as:\n    $$I_1(\\alpha) = -E\\left[ \\frac{d^2}{d\\alpha^2} \\ln(p(x; \\alpha)) \\right]$$\n    From our earlier work, the log-PDF for a single observation is $\\ln(p(x; \\alpha)) = \\ln(\\alpha) + \\alpha \\ln(x_{\\min}) - (\\alpha+1) \\ln(x)$.\n    The second derivative with respect to $\\alpha$ is:\n    $$\\frac{d^2}{d\\alpha^2} \\ln(p(x; \\alpha)) = \\frac{d}{d\\alpha} \\left(\\frac{1}{\\alpha} + \\ln(x_{\\min}) - \\ln(x)\\right) = -\\frac{1}{\\alpha^2}$$\n    Since this expression does not depend on the random variable $x$, its expectation is the expression itself:\n    $$E\\left[ \\frac{d^2}{d\\alpha^2} \\ln(p(x; \\alpha)) \\right] = -\\frac{1}{\\alpha^2}$$\n    Therefore, the Fisher information for a single observation is:\n    $$I_1(\\alpha) = - \\left(-\\frac{1}{\\alpha^2}\\right) = \\frac{1}{\\alpha^2}$$\n    The total Fisher information for the sample of size $n$ is:\n    $$I(\\alpha) = n I_1(\\alpha) = \\frac{n}{\\alpha^2}$$\n\n2.  **Asymptotic Variance:**\n    Under standard regularity conditions, the MLE $\\hat{\\alpha}_{ML}$ is asymptotically normally distributed with a variance equal to the Cram√©r-Rao Lower Bound, which is the inverse of the Fisher information.\n    $$\\text{Var}(\\hat{\\alpha}_{ML}) \\approx [I(\\alpha)]^{-1} = \\frac{\\alpha^2}{n}$$\n    This is the theoretical asymptotic variance of the estimator, which depends on the true unknown parameter $\\alpha$. A consistent estimator for the asymptotic variance is obtained by substituting the MLE $\\hat{\\alpha}_{ML}$ for the true parameter $\\alpha$:\n    $$\\widehat{\\text{Var}}(\\hat{\\alpha}_{ML}) = \\frac{\\hat{\\alpha}_{ML}^2}{n} = \\frac{1}{n} \\left( \\frac{n}{\\sum_{i=1}^{n} \\ln\\left(\\frac{x_i}{x_{\\min}}\\right)} \\right)^2 = \\frac{n}{\\left( \\sum_{i=1}^{n} \\ln\\left(\\frac{x_i}{x_{\\min}}\\right) \\right)^2}$$\n    This expression provides the estimated asymptotic variance based on the sample data.\n\nThe two required expressions are the estimator $\\hat{\\alpha}_{ML}$ and its estimated asymptotic variance.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{n}{\\sum_{i=1}^{n} \\ln\\left(\\frac{x_i}{x_{\\min}}\\right)}  \\frac{n}{\\left(\\sum_{i=1}^{n} \\ln\\left(\\frac{x_i}{x_{\\min}}\\right)\\right)^2}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "The straight-line signature on a log-log plot is the most famous visual test for a power law, yet it is a frequent source of error. This is because the visual pattern is sensitive to how the data is binned and plotted. This exercise tackles a critical and common pitfall by demonstrating mathematically why plotting raw histogram counts on logarithmic axes leads to a systematically biased estimate of the exponent, and how normalizing by the bin width provides the necessary correction . Completing this practice will equip you with the skills to create statistically sound visualizations and avoid misinterpreting log-log plots.",
            "id": "4297931",
            "problem": "Consider a positive random variable $X$ supported on $[x_{\\min}, \\infty)$ whose tail follows a continuous power-law probability density function $p(x)$ with exponent $\\alpha1$,\n$$\np(x) = (\\alpha - 1)\\, x_{\\min}^{\\alpha - 1}\\, x^{-\\alpha}, \\quad x \\ge x_{\\min}.\n$$\nA researcher draws $n$ independent and identically distributed samples from $X$ and constructs a histogram using logarithmically spaced bins defined by edges $\\{b_i\\}_{i=0}^{K}$ satisfying $b_0 = x_{\\min}$ and $b_{i+1} = r\\, b_i$ with fixed ratio $r1$. Let $N_i$ denote the count of samples falling in bin $i$, i.e., in $[b_i, b_{i+1})$, and let $m_i$ denote the geometric bin midpoint $m_i = \\sqrt{b_i b_{i+1}}$.\n\nTwo log-log plots are considered:\n\n1. A naive plot of raw counts $N_i$ versus $m_i$, on which the researcher fits a straight line by ordinary least squares in $(\\ln m_i, \\ln N_i)$ and obtains a slope $s_{\\mathrm{naive}}$.\n\n2. A corrected plot that estimates the probability density by rescaling the raw counts by the bin width in $x$, $\\Delta x_i = b_{i+1} - b_i$, namely $\\hat{p}_i = \\frac{N_i}{n\\, \\Delta x_i}$. The researcher fits a straight line in $(\\ln m_i, \\ln \\hat{p}_i)$ and obtains a slope $s_{\\mathrm{corr}}$.\n\nStarting from the fundamental definition that the expected bin count equals $n$ times the probability mass in the bin, and the definition of a histogram-based density estimator as counts divided by bin width, derive the expected dependence of $N_i$ and $\\hat{p}_i$ on $b_i$ for fixed $r$, and use these scalings to show the bias of the naive slope on log-log axes and the correction provided by density rescaling in logarithmic bins. Express the true exponent $\\alpha$ as closed-form analytic functions of $s_{\\mathrm{naive}}$ and $s_{\\mathrm{corr}}$.\n\nYour final answer must be an analytic expression. If you provide both relations, write them together as a row matrix using the LaTeX $\\texttt{pmatrix}$ environment. No numerical approximation is required.",
            "solution": "The problem asks for an analysis of two methods for estimating the exponent $\\alpha$ of a power-law probability density function, $p(x) = (\\alpha - 1) x_{\\min}^{\\alpha - 1} x^{-\\alpha}$ for $x \\ge x_{\\min}$, from binned data. We will derive the expected scaling behavior for each method to find the relationship between the measured slopes, $s_{\\mathrm{naive}}$ and $s_{\\mathrm{corr}}$, and the true exponent $\\alpha$. The analysis will be based on the expected value of the counts in each bin, which neglects finite-sample fluctuations and reveals the systematic properties of each estimator.\n\nLet $P_i$ be the probability that a single sample $X$ falls into the $i$-th bin, $[b_i, b_{i+1})$, where $b_{i+1} = r b_i$ for a constant ratio $r  1$. This probability is the integral of the probability density function $p(x)$ over the bin interval.\nLet $C = (\\alpha - 1) x_{\\min}^{\\alpha - 1}$ be the normalization constant. Since $\\alpha  1$, the integral is:\n$$\nP_i = \\int_{b_i}^{b_{i+1}} p(x) \\, dx = C \\int_{b_i}^{r b_i} x^{-\\alpha} \\, dx\n$$\n$$\nP_i = C \\left[ \\frac{x^{1-\\alpha}}{1-\\alpha} \\right]_{b_i}^{r b_i} = \\frac{C}{1-\\alpha} \\left( (r b_i)^{1-\\alpha} - b_i^{1-\\alpha} \\right)\n$$\nSubstituting $C = (\\alpha - 1) x_{\\min}^{\\alpha - 1} = -(1-\\alpha) x_{\\min}^{\\alpha - 1}$:\n$$\nP_i = \\frac{-(1-\\alpha) x_{\\min}^{\\alpha - 1}}{1-\\alpha} b_i^{1-\\alpha} (r^{1-\\alpha} - 1) = -x_{\\min}^{\\alpha - 1} b_i^{1-\\alpha} (r^{1-\\alpha} - 1)\n$$\n$$\nP_i = x_{\\min}^{\\alpha - 1} (1 - r^{1-\\alpha}) b_i^{1-\\alpha}\n$$\nFor a total of $n$ independent samples, the expected number of counts in bin $i$, $E[N_i]$, is $n$ times this probability:\n$$\nE[N_i] = n P_i = n x_{\\min}^{\\alpha - 1} (1 - r^{1-\\alpha}) b_i^{1-\\alpha}\n$$\n\n**Analysis of the Naive Method**\n\nThe naive method involves a log-log plot of the raw counts $N_i$ versus the geometric bin midpoint $m_i = \\sqrt{b_i b_{i+1}}$. We analyze the expected relationship between $\\ln E[N_i]$ and $\\ln m_i$.\nFirst, express $m_i$ in terms of $b_i$:\n$$\nm_i = \\sqrt{b_i (r b_i)} = \\sqrt{r} b_i\n$$\nTaking the natural logarithm:\n$$\n\\ln m_i = \\ln(\\sqrt{r}) + \\ln b_i = \\frac{1}{2}\\ln r + \\ln b_i \\implies \\ln b_i = \\ln m_i - \\frac{1}{2}\\ln r\n$$\nNow, take the natural logarithm of the expected count $E[N_i]$:\n$$\n\\ln E[N_i] = \\ln\\left( n x_{\\min}^{\\alpha - 1} (1 - r^{1-\\alpha}) \\right) + (1-\\alpha) \\ln b_i\n$$\nThe term in the logarithm is a constant. Let's substitute the expression for $\\ln b_i$:\n$$\n\\ln E[N_i] = \\ln\\left( n x_{\\min}^{\\alpha - 1} (1 - r^{1-\\alpha}) \\right) + (1-\\alpha) \\left(\\ln m_i - \\frac{1}{2}\\ln r\\right)\n$$\n$$\n\\ln E[N_i] = (1-\\alpha) \\ln m_i + \\left[ \\ln\\left( n x_{\\min}^{\\alpha - 1} (1 - r^{1-\\alpha}) \\right) - \\frac{1-\\alpha}{2}\\ln r \\right]\n$$\nThis is an equation of a straight line of the form $y = s x + c$, where $y = \\ln E[N_i]$ and $x = \\ln m_i$. The slope of this line, $s_{\\mathrm{naive}}$, is the coefficient of $\\ln m_i$.\n$$\ns_{\\mathrm{naive}} = 1 - \\alpha\n$$\nThis result shows the origin of the bias in the naive method. The count $N_i$ is proportional not only to the density $p(x)$ but also to the bin width $\\Delta x_i$, which increases with $x$. This increasing width contributes a factor of $b_i^1$ to the scaling, shifting the exponent from $-\\alpha$ to $1-\\alpha$. Solving for $\\alpha$ gives the first required relationship:\n$$\n\\alpha = 1 - s_{\\mathrm{naive}}\n$$\n\n**Analysis of the Corrected Method**\n\nThe corrected method estimates the probability density by normalizing the counts $N_i$ by the total number of samples $n$ and the bin width $\\Delta x_i$. The bin width is:\n$$\n\\Delta x_i = b_{i+1} - b_i = r b_i - b_i = (r-1) b_i\n$$\nThe density estimator for bin $i$ is $\\hat{p}_i = \\frac{N_i}{n \\Delta x_i}$. Its expected value is:\n$$\nE[\\hat{p}_i] = \\frac{E[N_i]}{n \\Delta x_i} = \\frac{n P_i}{n \\Delta x_i} = \\frac{P_i}{\\Delta x_i}\n$$\nSubstituting the expressions for $P_i$ and $\\Delta x_i$:\n$$\nE[\\hat{p}_i] = \\frac{x_{\\min}^{\\alpha - 1} (1 - r^{1-\\alpha}) b_i^{1-\\alpha}}{(r-1) b_i} = \\left( \\frac{x_{\\min}^{\\alpha - 1} (1 - r^{1-\\alpha})}{r-1} \\right) b_i^{-\\alpha}\n$$\nThis corrected quantity $E[\\hat{p}_i]$ now scales as $b_i^{-\\alpha}$, correctly reflecting the scaling of the underlying probability density function.\nTo find the slope on the log-log plot of $\\hat{p}_i$ versus $m_i$, we take the logarithm of $E[\\hat{p}_i]$:\n$$\n\\ln E[\\hat{p}_i] = \\ln\\left( \\frac{x_{\\min}^{\\alpha - 1} (1 - r^{1-\\alpha})}{r-1} \\right) - \\alpha \\ln b_i\n$$\nAgain, substituting $\\ln b_i = \\ln m_i - \\frac{1}{2}\\ln r$:\n$$\n\\ln E[\\hat{p}_i] = \\ln\\left( \\frac{x_{\\min}^{\\alpha - 1} (1 - r^{1-\\alpha})}{r-1} \\right) - \\alpha \\left(\\ln m_i - \\frac{1}{2}\\ln r\\right)\n$$\n$$\n\\ln E[\\hat{p}_i] = -\\alpha \\ln m_i + \\left[ \\ln\\left( \\frac{x_{\\min}^{\\alpha - 1} (1 - r^{1-\\alpha})}{r-1} \\right) + \\frac{\\alpha}{2}\\ln r \\right]\n$$\nThis is again the equation of a straight line, $y=sx+c$, where $y = \\ln E[\\hat{p}_i]$ and $x = \\ln m_i$. The slope $s_{\\mathrm{corr}}$ is the coefficient of $\\ln m_i$.\n$$\ns_{\\mathrm{corr}} = -\\alpha\n$$\nThis method removes the bias introduced by the varying bin widths. Solving for $\\alpha$ provides the second required relationship:\n$$\n\\alpha = -s_{\\mathrm{corr}}\n$$\n\nIn summary, plotting raw counts on a log-log scale against bin centers yields a slope that is not the power-law exponent, but is shifted by $+1$ due to the linearly increasing width of the logarithmic bins. Normalizing the counts by the bin width corrects for this artifact, and the resulting slope on a log-log plot is an unbiased estimator (in expectation) of the negative exponent, $-\\alpha$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 1 - s_{\\mathrm{naive}}  -s_{\\mathrm{corr}} \\end{pmatrix}}\n$$"
        }
    ]
}