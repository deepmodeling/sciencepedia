{
    "hands_on_practices": [
        {
            "introduction": "Erdős-Rényi (ER) 模型是最简单、最基础的随机图模型，是与真实世界网络进行比较的重要基准。本练习将引导你从第一性原理出发，推导期望度和度方差。掌握这一计算过程，可以巩固你对网络属性如何从独立的边形成概率中涌现的理解，以及如何分析不同网络密度下的度波动。",
            "id": "4272033",
            "problem": "考虑一个由 Erdős–Rényi (ER) 模型 $G(n,p)$ 生成的无向简单图，该图有 $n$ 个标记节点，其中 $\\binom{n}{2}$ 条可能的无向边中的每一条都以概率 $p \\in (0,1)$ 独立存在。从图中均匀随机地选择一个节点，并将其度记为 $k$。仅使用度的定义、边指示符的独立性以及独立伯努利随机变量之和的性质，推导出期望度 $\\langle k \\rangle$ 和度方差 $\\mathrm{Var}(k)$ 作为 $n$ 和 $p$ 的函数的精确表达式。然后，将度涨落尺度定义为标准差 $\\sigma_{k} = \\sqrt{\\mathrm{Var}(k)}$，并解释在两种科学研究中的相关情形下，$\\sigma_{k}$ 和相对涨落 $\\sigma_{k}/\\langle k \\rangle$ 如何随 $n$ 和 $p$ 变化：一种是 $p$ 在 $(0,1)$ 内固定（稠密图），另一种是 $p$ 按 $p = c/(n-1)$ 的形式变化，其中 $c \\in (0,\\infty)$ 为常数（稀疏图）。将最终答案表示为 $\\langle k \\rangle$ 和 $\\mathrm{Var}(k)$ 的精确闭式解析表达式的有序对。无需进行数值计算。",
            "solution": "Erdős–Rényi (ER) 模型 $G(n,p)$ 定义了一个包含 $n$ 个节点的无向简单图，其中每对不同的节点都以概率 $p$ 独立连接。设节点索引为 $i \\in \\{1,\\dots,n\\}$，并均匀随机地选择一个节点 $i$。对于每个 $j \\neq i$，定义边指示符随机变量 $X_{ij}$：如果节点 $i$ 和 $j$ 之间的无向边存在，则 $X_{ij} = 1$，否则 $X_{ij} = 0$。根据构造，对于固定的 $i$，族 $\\{X_{ij}\\}_{j \\neq i}$ 是独立同分布的，其中 $X_{ij} \\sim \\mathrm{Bernoulli}(p)$，并且恰好有 $n-1$ 个这样的指示符。\n\n根据度的定义，节点 $i$ 的度 $k$ 由下式给出\n$$\nk \\;=\\; \\sum_{j \\neq i} X_{ij}.\n$$\n这是一个具有相同成功概率 $p$ 的 $n-1$ 个独立伯努利随机变量之和。因此，$k$ 服从二项分布，\n$$\nk \\sim \\mathrm{Binomial}(n-1, p).\n$$\n我们现在使用期望的线性和独立性从第一性原理计算期望和方差。\n\n对于期望，\n$$\n\\langle k \\rangle \\;=\\; \\left\\langle \\sum_{j \\neq i} X_{ij} \\right\\rangle \\;=\\; \\sum_{j \\neq i} \\langle X_{ij} \\rangle \\;=\\; \\sum_{j \\neq i} p \\;=\\; (n-1)\\, p.\n$$\n\n对于方差，利用独立性对各方差求和：\n$$\n\\mathrm{Var}(k) \\;=\\; \\mathrm{Var}\\!\\left(\\sum_{j \\neq i} X_{ij}\\right) \\;=\\; \\sum_{j \\neq i} \\mathrm{Var}(X_{ij}) \\;=\\; \\sum_{j \\neq i} p(1-p) \\;=\\; (n-1)\\, p(1-p).\n$$\n因此，由标准差定义的涨落尺度为\n$$\n\\sigma_{k} \\;=\\; \\sqrt{\\mathrm{Var}(k)} \\;=\\; \\sqrt{(n-1)\\, p(1-p)}.\n$$\n\n接下来，我们通过考察两种情形下 $\\sigma_{k}$ 和相对涨落 $\\sigma_{k}/\\langle k \\rangle$ 来解释涨落随 $n$ 和 $p$ 的标度行为。\n\n稠密情形，其中 $p \\in (0,1)$ 固定且与 $n$ 无关：此时，\n$$\n\\langle k \\rangle \\;=\\; (n-1)\\, p \\;\\sim\\; n\\, p, \n\\quad \\sigma_{k} \\;=\\; \\sqrt{(n-1)\\, p(1-p)} \\;\\sim\\; \\sqrt{n}\\, \\sqrt{p(1-p)}.\n$$\n相对涨落的标度行为如下\n$$\n\\frac{\\sigma_{k}}{\\langle k \\rangle} \\;=\\; \\frac{\\sqrt{(n-1)\\, p(1-p)}}{(n-1)\\, p} \\;=\\; \\sqrt{\\frac{1-p}{(n-1)\\, p}} \\;\\sim\\; \\frac{\\mathrm{const}}{\\sqrt{n}},\n$$\n因此当 $n$ 增大时它们趋于零。这表明在稠密 ER 模型中度的自平均现象：度集中在其均值附近，且变异系数以 $n^{-1/2}$ 的速率衰减。\n\n稀疏情形，其中 $p = \\frac{c}{n-1}$ 且 $c \\in (0,\\infty)$ 为常数：此时，\n$$\n\\langle k \\rangle \\;=\\; (n-1)\\, \\frac{c}{n-1} \\;=\\; c,\n\\quad \n\\mathrm{Var}(k) \\;=\\; (n-1)\\, \\frac{c}{n-1}\\!\\left(1-\\frac{c}{n-1}\\right) \\;=\\; c \\left(1-\\frac{c}{n-1}\\right),\n$$\n并且\n$$\n\\sigma_{k} \\;=\\; \\sqrt{c \\left(1-\\frac{c}{n-1}\\right)} \\;\\xrightarrow[n \\to \\infty]{} \\sqrt{c}.\n$$\n相对涨落为\n$$\n\\frac{\\sigma_{k}}{\\langle k \\rangle} \\;=\\; \\frac{\\sqrt{c \\left(1-\\frac{c}{n-1}\\right)}}{c} \\;=\\; \\frac{1}{\\sqrt{c}} \\sqrt{1-\\frac{c}{n-1}} \\;\\xrightarrow[n \\to \\infty]{} \\frac{1}{\\sqrt{c}},\n$$\n当 $n$ 增长时，其量级保持为 $O(1)$，这反映了在稀疏情形下，度不具有相同意义上的自平均效应；它们表现出的涨落与其均值是同阶的。在 $p = \\frac{c}{n-1}$ 的典型稀疏标度下，度分布收敛到均值为 $c$ 的泊松分布，这与极限方差和涨落标度是一致的。\n\n总而言之，通过从 $G(n,p)$ 中的独立伯努利边指示符进行第一性原理推导，得到的精确公式为 $\\langle k \\rangle = (n-1)\\, p$ 和 $\\mathrm{Var}(k) = (n-1)\\, p(1-p)$，其涨落标度由 $\\sigma_{k} = \\sqrt{(n-1)\\, p(1-p)}$ 给出，并且相对涨落在稠密情形下以 $n^{-1/2}$ 的速率衰减，但在稀疏情形 $p = c/(n-1)$ 下保持为 $O(1)$。",
            "answer": "$$\\boxed{\\begin{pmatrix}(n-1)\\, p  (n-1)\\, p(1-p)\\end{pmatrix}}$$"
        },
        {
            "introduction": "与同质化的ER模型不同，许多真实网络呈现出重尾的“无标度”度分布。优先连接模型基于“富者愈富”的增长机制，为这一现象提供了强有力的解释。在本练习中，你将使用统计物理学中的核心方法——主方程，来推导著名的幂律度分布，从而揭示局部增长规则与全局网络结构之间的深刻联系。",
            "id": "4271972",
            "problem": "考虑一个无向网络增长过程，其定义如下。从一个具有 $N_{0}$ 个节点和 $M_{0}$ 条边的任意有限连通种子网络开始。在每个离散时间步 $t \\mapsto t+1$，添加一个新节点，并将该节点与 $m$ 个不同的已存在节点连接，形成 $m \\in \\mathbb{N}$ 条新边。连接方式为带有初始吸引力的线性优先连接：一个度为 $k$ 的已存在节点被选中连接的概率与 $k + \\delta$ 成正比，其中 $\\delta  -m$ 是一个固定的吸引力参数，确保所有节点的连接权重为非负。\n\n令 $N_{k}(t)$ 表示在时间 $t$ 时度为 $k$ 的节点的期望数量，并将稳态度分布 $P_{k}$ 定义为大时间极限 $P_{k} = \\lim_{t \\to \\infty} \\frac{N_{k}(t)}{t}$，假设该极限存在。仅使用度的基本定义、无向网络中边的守恒以及所述的连接机制，完成以下任务：\n\n- 推导控制 $k \\geq m$ 时 $N_{k}(t)$ 演化的主方程。\n- 根据此主方程，确定连接概率的归一化因子（用 $t$ 表示），并展示其渐近行为。\n- 利用这些结果，获得大 $k$ 时 $P_{k}$ 的渐近形式，证明它是一个幂律 $P_{k} \\sim k^{-\\gamma}$，并将指数 $\\gamma$ 表示为 $m$ 和 $\\delta$ 的函数。\n\n请以 $m$ 和 $\\delta$ 的单个闭式表达式形式提供最终答案。不需要数值近似或四舍五入。",
            "solution": "该问题要求推导网络增长过程的主方程，分析连接概率的归一化，并推导稳态度分布 $P_k$ 的幂律指数 $\\gamma$。我们首先验证问题的有效性，然后提供分步推导。\n\n问题陈述描述了一个定义明确的网络增长模型，它是 Barabási-Albert 模型的一个变体，带有一个初始吸引力参数 $\\delta$。问题的各个组成部分——主方程形式、稳态度分布的概念以及渐近幂律行为——都是复杂系统和网络科学领域的标准核心课题。该问题具有科学依据，提法明确，并且为渐近分析提供了所有必要的参数和条件。条件 $\\delta  -m$ 确保了对于任何度为 $k \\geq m$ 的节点，其连接权重 $k+\\delta$ 均为正。因此，该问题被认为是有效的。\n\n我们首先确定网络在时间 $t$ 的属性。网络初始有 $N_0$ 个节点和 $M_0$ 条边。在每个时间步，增加一个节点和 $m$ 条边。\n在时间 $t$ 的总节点数是 $N(t) = N_0 + t$。\n在时间 $t$ 的总边数是 $M(t) = M_0 + mt$。\n对于无向网络，所有节点度之和是边数的两倍：$\\sum_{i} k_i(t) = 2M(t) = 2(M_0 + mt)$。\n\n新节点连接到度为 $k_i$ 的已存在节点 $i$ 的连接概率与 $k_i + \\delta$ 成正比。归一化概率为：\n$$\n\\Pi_i = \\frac{k_i + \\delta}{\\sum_{j} (k_j + \\delta)}\n$$\n分母是归一化因子，我们将其表示为 $Z(t)$。我们可以用网络的宏观属性来表示 $Z(t)$：\n$$\nZ(t) = \\sum_{j \\in V(t)} (k_j(t) + \\delta) = \\sum_{j} k_j(t) + \\sum_{j} \\delta = 2M(t) + \\delta N(t)\n$$\n代入 $N(t)$ 和 $M(t)$ 的表达式：\n$$\nZ(t) = 2(M_0 + mt) + \\delta(N_0 + t) = (2m + \\delta)t + (2M_0 + \\delta N_0)\n$$\n对于大时间 $t \\to \\infty$，常数项变得可以忽略，归一化因子的渐近行为如下：\n$$\nZ(t) \\sim (2m + \\delta)t\n$$\n这就完成了问题要求的第二部分。\n\n接下来，我们推导 $k \\geq m$ 时，在时间 $t$ 度为 $k$ 的节点的期望数量 $N_k(t)$ 的主方程。我们使用连续时间近似，其中 $N_k(t+1) - N_k(t) \\approx \\frac{dN_k(t)}{dt}$。对于 $km$，$N_k$ 的变化由两个过程引起：\n1.  当一个度为 $k-1$ 的节点被选中连接时，$N_k$ 增加，其度变为 $k$。\n2.  当一个度为 $k$ 的节点被选中连接时，$N_k$ 减少，其度变为 $k+1$。\n\n$m$ 条新边中的一条连接到 $N_{k-1}(t)$ 个度为 $k-1$ 的节点中的任意一个的概率由 $m \\times \\frac{N_{k-1}(t)((k-1)+\\delta)}{Z(t)}$ 给出。这是由于此过程导致 $N_k$ 增加的速率。\n类似地，$N_k$ 减少的速率是度为 $k$ 的节点被选中连接的速率：$m \\times \\frac{N_k(t)(k+\\delta)}{Z(t)}$。\n\n因此，对于 $k  m$，主方程为：\n$$\n\\frac{dN_k}{dt} = m \\frac{(k-1+\\delta)N_{k-1}(t)}{Z(t)} - m \\frac{(k+\\delta)N_k(t)}{Z(t)}\n$$\n对于 $k=m$ 的特殊情况，我们还必须考虑每个时间步引入的新节点，其度为 $m$。这为 $N_m$ 的速率方程增加了一个源项 $1$。在此过程中（对于 $t0$），不会产生度小于 $m$ 的节点，因此没有从更低度的状态流入 $k=m$ 状态。$k = m$ 的主方程为：\n$$\n\\frac{dN_m}{dt} = 1 - m \\frac{(m+\\delta)N_m(t)}{Z(t)}\n$$\n这两个方程构成了 $k \\geq m$ 的主方程，这是问题要求的第一部分。\n\n为了找到稳态度分布 $P_k = \\lim_{t \\to \\infty} \\frac{N_k(t)}{t}$，我们假设对于大时间 $t$，存在一个标度解，形式为 $N_k(t) = P_k t$。注意 $\\lim_{t \\to \\infty} \\frac{N_k(t)}{N(t)} = \\lim_{t \\to \\infty} \\frac{P_k t}{N_0 + t} = P_k$。\n对 $t$ 求导得到 $\\frac{dN_k}{dt} = P_k$。\n将此拟设和渐近形式 $Z(t) \\sim (2m+\\delta)t$ 代入 $km$ 的主方程中：\n$$\nP_k = m \\frac{(k-1+\\delta)P_{k-1} t}{(2m+\\delta)t} - m \\frac{(k+\\delta)P_k t}{(2m+\\delta)t}\n$$\n$$\nP_k = \\frac{m}{2m+\\delta} \\left[ (k-1+\\delta)P_{k-1} - (k+\\delta)P_k \\right]\n$$\n我们可以重新整理这个方程，以找到 $P_k$ 的递推关系：\n$$\nP_k \\left( 1 + \\frac{m(k+\\delta)}{2m+\\delta} \\right) = \\frac{m(k-1+\\delta)}{2m+\\delta} P_{k-1}\n$$\n$$\nP_k \\left( \\frac{2m+\\delta + mk + m\\delta}{2m+\\delta} \\right) = \\frac{m(k-1+\\delta)}{2m+\\delta} P_{k-1}\n$$\n$$\nP_k \\left( mk + 2m + \\delta + m\\delta \\right) = m(k-1+\\delta)P_{k-1}\n$$\n这给出了比率：\n$$\n\\frac{P_k}{P_{k-1}} = \\frac{m(k-1+\\delta)}{mk + 2m + \\delta + m\\delta} = \\frac{k-1+\\delta}{k + 2 + \\frac{\\delta}{m} + \\delta}\n$$\n我们关心的是大 $k$ 时的渐近行为。为此，我们分析比率 $P_k / P_{k-1}$。\n首先，从递推关系出发，对于大 $k$：\n$$ \\frac{P_k}{P_{k-1}} = \\frac{k-1+\\delta}{k + 2 + \\frac{\\delta}{m} + \\delta} = \\frac{k(1 + (\\delta-1)/k)}{k(1 + (2+\\delta+\\delta/m)/k)} \\approx \\left(1 + \\frac{\\delta-1}{k}\\right)\\left(1 - \\frac{2+\\delta+\\delta/m}{k}\\right) $$\n展开并只保留到 $k^{-1}$ 阶：\n$$ \\frac{P_k}{P_{k-1}} \\approx 1 + \\frac{\\delta-1}{k} - \\frac{2+\\delta+\\delta/m}{k} = 1 - \\frac{3+\\delta/m}{k} $$\n其次，我们假设稳态解是幂律形式 $P_k \\sim k^{-\\gamma}$。对于大 $k$，该形式的比率为：\n$$ \\frac{P_k}{P_{k-1}} \\sim \\frac{k^{-\\gamma}}{(k-1)^{-\\gamma}} = \\left(\\frac{k-1}{k}\\right)^\\gamma = \\left(1 - \\frac{1}{k}\\right)^\\gamma \\approx 1 - \\frac{\\gamma}{k} $$\n比较这两个渐近表达式，我们得到 $1 - \\frac{\\gamma}{k} \\approx 1 - \\frac{3+\\delta/m}{k}$，由此可得幂律指数：\n$$\n\\gamma = 3 + \\frac{\\delta}{m}\n$$\n这是大 $k$ 时幂律度分布的指数。当初始吸引力为零（$\\delta=0$）时，此结果正确地恢复了标准的 Barabási-Albert 模型指数 $\\gamma=3$。",
            "answer": "$$\\boxed{3 + \\frac{\\delta}{m}}$$"
        },
        {
            "introduction": "在观察到网络的度分布后，一项关键任务是确定最拟合的数学模型。这通常涉及比较相互竞争的假设，例如幂律分布和对数正态分布，它们在经验数据中可能表现得惊人地相似。这项高级练习介绍了用于模型选择的强大方法——最大似然估计（MLE）和对数似然比检验。通过解决这个问题，你将获得严谨数据分析的实践技能，并体会到重尾数据中统计可辨识性的微妙挑战。",
            "id": "4271979",
            "problem": "您正在观察一个大型无向网络的上尾度，该观察是在复杂系统与网络科学中常用的标准连续尾部近似下进行的。具体来说，设 $k_{1},\\dots,k_{n}$ 是节点度的独立同分布实现，满足 $k_{i}\\geq k_{\\min}0$，这些实现是通过丢弃所有低于阈值 $k_{\\min}$ 的观测值得到的。您希望比较两种关于尾部度分布的竞争模型：\n\n1. 一个在 $[k_{\\min},\\infty)$ 上的连续幂律尾分布，其参数为 $\\alpha1$，密度函数为 $f_{\\mathrm{PL}}(k\\mid \\alpha,k_{\\min})$，支撑集为 $k\\geq k_{\\min}$。\n\n2. 一个在 $(0,\\infty)$ 上的对数正态模型，其参数为 $\\mu\\in\\mathbb{R}$ 和 $\\sigma0$，密度函数为 $f_{\\mathrm{LN}}(k\\mid \\mu,\\sigma)$，支撑集为 $k0$。\n\n假设观测是独立的，并采用最大似然估计 (MLE)。仅从独立样本的似然性的基本定义和模型密度出发，完成以下任务：\n\n(a) 写出 $f_{\\mathrm{PL}}(k\\mid \\alpha,k_{\\min})$ 在 $[k_{\\min},\\infty)$ 上和 $f_{\\mathrm{LN}}(k\\mid \\mu,\\sigma)$ 在 $(0,\\infty)$ 上的显式形式。利用这些形式，推导出观测样本 $\\{k_{i}\\}_{i=1}^{n}$ 的对数似然函数 $\\ell_{\\mathrm{PL}}(\\alpha)$ 和 $\\ell_{\\mathrm{LN}}(\\mu,\\sigma)$。\n\n(b) 推导幂律模型的最大似然估计量 $\\hat{\\alpha}$ 和对数正态模型的最大似然估计量 $(\\hat{\\mu},\\hat{\\sigma})$。\n\n(c) 引入统计量 $S_{0}=\\sum_{i=1}^{n}\\ln k_{i}$、$S_{1}=\\sum_{i=1}^{n}\\ln\\!\\big(k_{i}/k_{\\min}\\big)=S_{0}-n\\ln k_{\\min}$ 和 $s^{2}=\\frac{1}{n}\\sum_{i=1}^{n}(\\ln k_{i}-\\bar{y})^{2}$，其中 $\\bar{y}=\\frac{1}{n}\\sum_{i=1}^{n}\\ln k_{i}$。用 $n$、$k_{\\min}$、$S_{0}$、$S_{1}$ 和 $s^{2}$ 表示最大化对数似然 $\\ell_{\\mathrm{PL}}(\\hat{\\alpha})$ 和 $\\ell_{\\mathrm{LN}}(\\hat{\\mu},\\hat{\\sigma})$。\n\n(d) 定义对数似然比统计量 $\\Lambda=2\\big(\\ell_{\\mathrm{PL}}(\\hat{\\alpha})-\\ell_{\\mathrm{LN}}(\\hat{\\mu},\\hat{\\sigma})\\big)$。仅用 $n$、$k_{\\min}$、$S_{1}$ 和 $s$ 为 $\\Lambda$ 提供一个单一的闭式解析表达式。这是您必须作为最终答案提交的唯一量。\n\n(e) 从第一性原理出发，不进行数值计算，讨论在尝试区分重尾样本中的幂律尾和对数正态尾时出现的可辨识性挑战。您的讨论应基于对数正态尾的渐近行为，以及它如何在有限样本的有限 $k$ 值范围内近似幂律分布。\n\n您的最终答案必须是 (d) 部分中 $\\Lambda$ 的表达式。不需要进行数值评估或四舍五入。",
            "solution": "问题陈述已经过验证，被认为是应用于复杂网络分析的统计推断中的一个良定的、有科学依据的问题。所有给定条件都是一致且充分的，足以得到唯一解。\n\n(a) 密度函数和对数似然函数的推导\n\n首先，我们推导连续幂律模型的归一化概率密度函数 (PDF)。对于 $k \\geq k_{\\min}$，密度 $f_{\\mathrm{PL}}(k)$ 与 $k^{-\\alpha}$ 成正比。我们通过将密度从 $k_{\\min}$ 积分到 $\\infty$ 并将结果设为 1 来求得归一化常数 $C$。\n$$ \\int_{k_{\\min}}^{\\infty} C k^{-\\alpha} \\, dk = 1 $$\n为保证积分收敛，要求 $\\alpha  1$。\n$$ C \\left[ \\frac{k^{1-\\alpha}}{1-\\alpha} \\right]_{k_{\\min}}^{\\infty} = C \\left( 0 - \\frac{k_{\\min}^{1-\\alpha}}{1-\\alpha} \\right) = C \\frac{k_{\\min}^{1-\\alpha}}{\\alpha-1} = 1 $$\n解出 $C$ 得到 $C = (\\alpha-1) k_{\\min}^{\\alpha-1}$。因此，幂律 PDF 为：\n$$ f_{\\mathrm{PL}}(k \\mid \\alpha, k_{\\min}) = (\\alpha-1)k_{\\min}^{\\alpha-1} k^{-\\alpha}, \\quad k \\geq k_{\\min} $$\n随机变量 $k$ 的对数正态 PDF 的定义是，$\\ln k$ 服从均值为 $\\mu$、方差为 $\\sigma^2$ 的正态分布。其标准形式为：\n$$ f_{\\mathrm{LN}}(k \\mid \\mu, \\sigma) = \\frac{1}{k \\sigma \\sqrt{2\\pi}} \\exp\\left(-\\frac{(\\ln k - \\mu)^2}{2\\sigma^2}\\right), \\quad k  0 $$\n\n接下来，我们推导对数似然函数。对于 $n$ 个独立同分布的观测样本 $\\{k_i\\}_{i=1}^n$，似然是各个概率的乘积，即 $L = \\prod_{i=1}^n f(k_i)$。对数似然 $\\ell$ 是似然的自然对数，即 $\\ell = \\ln L = \\sum_{i=1}^n \\ln f(k_i)$。\n\n对于幂律模型，对数似然函数 $\\ell_{\\mathrm{PL}}(\\alpha)$ 为：\n$$ \\ell_{\\mathrm{PL}}(\\alpha) = \\sum_{i=1}^{n} \\ln\\left( (\\alpha-1)k_{\\min}^{\\alpha-1} k_i^{-\\alpha} \\right) $$\n$$ \\ell_{\\mathrm{PL}}(\\alpha) = \\sum_{i=1}^{n} \\left( \\ln(\\alpha-1) + (\\alpha-1)\\ln k_{\\min} - \\alpha\\ln k_i \\right) $$\n$$ \\ell_{\\mathrm{PL}}(\\alpha) = n\\ln(\\alpha-1) + n(\\alpha-1)\\ln k_{\\min} - \\alpha \\sum_{i=1}^{n} \\ln k_i $$\n\n对于对数正态模型，对数似然函数 $\\ell_{\\mathrm{LN}}(\\mu, \\sigma)$ 为：\n$$ \\ell_{\\mathrm{LN}}(\\mu, \\sigma) = \\sum_{i=1}^{n} \\ln\\left( \\frac{1}{k_i \\sigma \\sqrt{2\\pi}} \\exp\\left(-\\frac{(\\ln k_i - \\mu)^2}{2\\sigma^2}\\right) \\right) $$\n$$ \\ell_{\\mathrm{LN}}(\\mu, \\sigma) = \\sum_{i=1}^{n} \\left( -\\ln k_i - \\ln\\sigma - \\frac{1}{2}\\ln(2\\pi) - \\frac{(\\ln k_i - \\mu)^2}{2\\sigma^2} \\right) $$\n$$ \\ell_{\\mathrm{LN}}(\\mu, \\sigma) = -\\sum_{i=1}^{n} \\ln k_i - n\\ln\\sigma - \\frac{n}{2}\\ln(2\\pi) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^{n} (\\ln k_i - \\mu)^2 $$\n\n(b) 最大似然估计量 (MLEs) 的推导\n\n为了求得最大似然估计量 $\\hat{\\alpha}$，我们将 $\\ell_{\\mathrm{PL}}(\\alpha)$ 对 $\\alpha$ 求导，并令结果为零：\n$$ \\frac{d\\ell_{\\mathrm{PL}}}{d\\alpha} = \\frac{n}{\\alpha-1} + n\\ln k_{\\min} - \\sum_{i=1}^{n} \\ln k_i = 0 $$\n$$ \\frac{n}{\\alpha-1} = \\sum_{i=1}^{n} \\ln k_i - n\\ln k_{\\min} = \\sum_{i=1}^{n} (\\ln k_i - \\ln k_{\\min}) = \\sum_{i=1}^{n} \\ln\\left(\\frac{k_i}{k_{\\min}}\\right) $$\n解出 $\\alpha$ 得到最大似然估计量 $\\hat{\\alpha}$：\n$$ \\hat{\\alpha} = 1 + \\frac{n}{\\sum_{i=1}^{n} \\ln\\left(\\frac{k_i}{k_{\\min}}\\right)} $$\n\n为了求得最大似然估计量 $(\\hat{\\mu}, \\hat{\\sigma})$，我们对 $\\ell_{\\mathrm{LN}}(\\mu, \\sigma)$ 分别求关于 $\\mu$ 和 $\\sigma$ 的偏导数，并令它们为零。\n$$ \\frac{\\partial\\ell_{\\mathrm{LN}}}{\\partial \\mu} = -\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} 2(\\ln k_i - \\mu)(-1) = \\frac{1}{\\sigma^2}\\sum_{i=1}^{n}(\\ln k_i - \\mu) = 0 $$\n这意味着 $\\sum_{i=1}^{n} \\ln k_i - n\\mu = 0$。解出 $\\mu$ 得到最大似然估计量 $\\hat{\\mu}$：\n$$ \\hat{\\mu} = \\frac{1}{n}\\sum_{i=1}^{n} \\ln k_i $$\n现在，对 $\\sigma$ 求导：\n$$ \\frac{\\partial\\ell_{\\mathrm{LN}}}{\\partial \\sigma} = -\\frac{n}{\\sigma} + \\frac{1}{\\sigma^3}\\sum_{i=1}^{n}(\\ln k_i - \\mu)^2 = 0 $$\n$$ n\\sigma^2 = \\sum_{i=1}^{n}(\\ln k_i - \\mu)^2 $$\n将 $\\hat{\\mu}$ 代入 $\\mu$ 并解出 $\\sigma^2$ 得到最大似然估计量 $\\hat{\\sigma}^2$：\n$$ \\hat{\\sigma}^2 = \\frac{1}{n}\\sum_{i=1}^{n}(\\ln k_i - \\hat{\\mu})^2 $$\n所以，$\\hat{\\sigma} = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(\\ln k_i - \\hat{\\mu})^2}$。\n\n(c) 最大化对数似然\n\n我们现在用给定的统计量来表示最大化对数似然：$S_{0}=\\sum_{i=1}^{n}\\ln k_{i}$、$S_{1}=\\sum_{i=1}^{n}\\ln(k_{i}/k_{\\min})$、$\\bar{y}=S_0/n$ 和 $s^{2}=\\frac{1}{n}\\sum_{i=1}^{n}(\\ln k_{i}-\\bar{y})^{2}$。\n\n从 (b) 部分可知，$\\hat{\\alpha} = 1 + n/S_1$，因此 $\\hat{\\alpha}-1 = n/S_1$。我们可以先简化 $\\ell_{\\mathrm{PL}}(\\alpha)$ 的表达式：\n$ \\ell_{\\mathrm{PL}}(\\alpha) = n\\ln(\\alpha-1) + n(\\alpha-1)\\ln k_{\\min} - \\alpha S_0 = n\\ln(\\alpha-1) + n(\\alpha-1)\\ln k_{\\min} - \\alpha (S_1 + n\\ln k_{\\min}) = n\\ln(\\alpha-1) - n\\ln k_{\\min} - \\alpha S_1 $。\n代入 $\\hat{\\alpha}$：\n$$ \\ell_{\\mathrm{PL}}(\\hat{\\alpha}) = n\\ln\\left(\\frac{n}{S_1}\\right) - n\\ln k_{\\min} - \\left(1 + \\frac{n}{S_1}\\right)S_1 $$\n$$ \\ell_{\\mathrm{PL}}(\\hat{\\alpha}) = n(\\ln n - \\ln S_1) - n\\ln k_{\\min} - (S_1 + n) $$\n$$ \\ell_{\\mathrm{PL}}(\\hat{\\alpha}) = n\\ln n - n\\ln S_1 - n\\ln k_{\\min} - S_1 - n $$\n\n对于对数正态模型，最大似然估计量为 $\\hat{\\mu} = \\bar{y} = S_0/n$ 和 $\\hat{\\sigma}^2 = s^2$，因此 $\\hat{\\sigma} = s$。我们将这些代入 $\\ell_{\\mathrm{LN}}(\\mu,\\sigma)$：\n$$ \\ell_{\\mathrm{LN}}(\\hat{\\mu}, \\hat{\\sigma}) = -S_0 - n\\ln s - \\frac{n}{2}\\ln(2\\pi) - \\frac{1}{2s^2}\\sum_{i=1}^{n} (\\ln k_i - \\bar{y})^2 $$\n由于 $\\sum_{i=1}^{n} (\\ln k_i - \\bar{y})^2 = ns^2$，上式可简化为：\n$$ \\ell_{\\mathrm{LN}}(\\hat{\\mu}, \\hat{\\sigma}) = -S_0 - n\\ln s - \\frac{n}{2}\\ln(2\\pi) - \\frac{ns^2}{2s^2} $$\n$$ \\ell_{\\mathrm{LN}}(\\hat{\\mu}, \\hat{\\sigma}) = -S_0 - n\\ln s - \\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2} $$\n使用 $S_0 = S_1 + n\\ln k_{\\min}$：\n$$ \\ell_{\\mathrm{LN}}(\\hat{\\mu}, \\hat{\\sigma}) = -(S_1 + n\\ln k_{\\min}) - n\\ln s - \\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2} $$\n\n(d) 对数似然比统计量 $\\Lambda$\n\n我们计算 $\\Lambda = 2(\\ell_{\\mathrm{PL}}(\\hat{\\alpha}) - \\ell_{\\mathrm{LN}}(\\hat{\\mu}, \\hat{\\sigma}))$。\n$$ \\ell_{\\mathrm{PL}}(\\hat{\\alpha}) - \\ell_{\\mathrm{LN}}(\\hat{\\mu}, \\hat{\\sigma}) = (n\\ln n - n\\ln S_1 - n\\ln k_{\\min} - S_1 - n) - \\left(-(S_1 + n\\ln k_{\\min}) - n\\ln s - \\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\right) $$\n$$ = n\\ln n - n\\ln S_1 - n\\ln k_{\\min} - S_1 - n + S_1 + n\\ln k_{\\min} + n\\ln s + \\frac{n}{2}\\ln(2\\pi) + \\frac{n}{2} $$\n项 $-n\\ln k_{\\min}$ 和 $+n\\ln k_{\\min}$ 抵消。项 $-S_1$ 和 $+S_1$ 抵消。\n$$ = n\\ln n - n\\ln S_1 + n\\ln s - n + \\frac{n}{2} + \\frac{n}{2}\\ln(2\\pi) $$\n$$ = n\\ln n - n\\ln S_1 + n\\ln s - \\frac{n}{2} + \\frac{n}{2}\\ln(2\\pi) $$\n对数似然比统计量 $\\Lambda$ 是此数量的两倍：\n$$ \\Lambda = 2\\left( n\\ln n - n\\ln S_1 + n\\ln s - \\frac{n}{2} + \\frac{n}{2}\\ln(2\\pi) \\right) $$\n$$ \\Lambda = 2n\\ln n - 2n\\ln S_1 + 2n\\ln s - n + n\\ln(2\\pi) $$\n我们可以利用对数的性质对各项进行分组：\n$$ \\Lambda = n\\left( 2\\ln n - 2\\ln S_1 + 2\\ln s + \\ln(2\\pi) - 1 \\right) $$\n$$ \\Lambda = n\\left( 2(\\ln n - \\ln S_1 + \\ln s) + \\ln(2\\pi) - 1 \\right) $$\n$$ \\Lambda = n\\left( 2\\ln\\left(\\frac{ns}{S_1}\\right) + \\ln(2\\pi) - 1 \\right) $$\n此表达式仅依赖于 $n$、$S_1$ 和 $s$，符合要求（$k_{\\min}$ 已抵消，无需包含）。\n\n(e) 可辨识性挑战\n\n区分幂律尾和对数正态尾的困难，尤其是从有限的经验样本中进行区分，是一个根本性的可辨识性挑战。这源于对数正态概率密度函数的数学行为。\n\n幂律分布的 PDF 为 $f(k) \\propto k^{-\\alpha}$，当其对数 $\\ln f(k)$ 对 $\\ln k$ 作图时，会得到一条斜率为 $-\\alpha$ 的直线：$\\ln f(k) = C - \\alpha \\ln k$。\n对数正态 PDF $f_{\\mathrm{LN}}(k) = \\frac{1}{k \\sigma \\sqrt{2\\pi}} \\exp\\left(-\\frac{(\\ln k - \\mu)^2}{2\\sigma^2}\\right)$ 在全局范围内不具有此性质。对其取对数可得：\n$$ \\ln f_{\\mathrm{LN}}(k) = -\\ln k - \\frac{(\\ln k)^2 - 2\\mu\\ln k + \\mu^2}{2\\sigma^2} - \\ln(\\sigma\\sqrt{2\\pi}) $$\n$$ \\ln f_{\\mathrm{LN}}(k) = -\\frac{1}{2\\sigma^2}(\\ln k)^2 + \\left(\\frac{\\mu}{\\sigma^2} - 1\\right)\\ln k - \\left(\\frac{\\mu^2}{2\\sigma^2} + \\ln(\\sigma\\sqrt{2\\pi})\\right) $$\n此表达式是 $\\ln k$ 的二次函数，这意味着 $\\ln f_{\\mathrm{LN}}(k)$ 关于 $\\ln k$ 的图像是一条抛物线。\n\n挑战在于，在有限区间内，抛物线的一段可以被直线任意好地近似。对数正态分布的对数-对数图的局部斜率可以解释为一个“局部”幂律指数，$\\alpha(k) = -d(\\ln f(k))/d(\\ln k)$。对于对数正态分布，该指数为：\n$$ \\alpha(k) = -\\frac{d}{d(\\ln k)}\\left[ \\ln f_{\\mathrm{LN}}(k) \\right] = 1 + \\frac{\\ln k - \\mu}{\\sigma^2} $$\n这个局部指数 $\\alpha(k)$ 不是常数；它随 $\\ln k$ 线性变化。然而，对于重尾现象，拟合的对数正态模型中的参数 $\\sigma$ 通常很大。当 $\\sigma$ 很大时，$\\alpha(k)$ 相对于 $\\ln k$ 的斜率（即 $1/\\sigma^2$）变得非常小。这意味着 $\\alpha(k)$ 变化得非常缓慢。\n\n因此，在有限的观测 $k$ 值范围内——即使该范围跨越了几个数量级——对数正态密度在对数-对数图上的抛物线曲线也近似于一条直线。因此，一个由大 $\\sigma$ 值的对数正态分布生成的经验数据集，看起来可能像是从幂律分布中抽取的。像 MLE 这样的 statistical 方法会为每个模型找到最佳拟合参数，但最大化似然值 $\\ell_{\\mathrm{PL}}(\\hat{\\alpha})$ 和 $\\ell_{\\mathrm{LN}}(\\hat{\\mu}, \\hat{\\sigma})$ 可能会非常接近。这导致对数似然比统计量 $\\Lambda$ 接近于零，使得在统计上难以高置信度地偏好其中一个模型。这种混淆效应是模型模仿的一个典型例子，代表了重尾系统实证研究中的一个核心挑战。",
            "answer": "$$\n\\boxed{n\\left( 2\\ln\\left(\\frac{ns}{S_1}\\right) + \\ln(2\\pi) - 1 \\right)}\n$$"
        }
    ]
}