{
    "hands_on_practices": [
        {
            "introduction": "The Ornstein-Uhlenbeck (OU) process is a cornerstone of stochastic parameterization, modeling unresolved tendencies that exhibit a relaxation towards a mean state amidst random forcing. Before applying such a model, it's essential to understand its fundamental statistical behavior. This exercise guides you through a foundational derivation of the stationary mean and variance of an OU process, providing critical practice with the tools of Itô calculus that underpin stochastic climate modeling .",
            "id": "3916404",
            "problem": "In a simplified reduced-order atmosphere–ocean column model, the unresolved subgrid tendency for a conserved tracer is represented by the linear stochastic differential equation (SDE), driven by a standard Wiener process, given by $dx(t)=-\\lambda\\,x(t)\\,dt+\\sigma\\,dW_t$, where $x(t)$ is the stochastic tendency emulator, $\\lambda > 0$ is a constant linear relaxation rate, $\\sigma > 0$ is a constant noise amplitude, and $W_t$ is a standard Wiener process. This representation follows widely used stochastic parameterizations in environmental and earth system modeling. Assume the coefficients are time-invariant and the process is ergodic.\n\nStarting from the definitions of expectation and variance, the properties of the standard Wiener process $W_t$ and Itô calculus (including quadratic variation), derive the closed-form stationary mean and stationary variance of $x(t)$ under the assumption that a stationary distribution exists. Express your final answer as analytic expressions in terms of $\\lambda$ and $\\sigma$. No rounding is required, and no physical units need to be reported for the final expressions.",
            "solution": "The problem statement is a valid, well-posed problem in the field of stochastic processes, specifically concerning the Ornstein-Uhlenbeck process, which is a cornerstone of stochastic modeling in the physical sciences. All provided information is consistent and sufficient for a rigorous derivation.\n\nThe stochastic process is described by the linear Itô stochastic differential equation (SDE):\n$$dx(t) = -\\lambda\\,x(t)\\,dt + \\sigma\\,dW_t$$\nwhere $\\lambda > 0$ and $\\sigma > 0$ are constants, and $W_t$ is a standard Wiener process. We are tasked with deriving the mean and variance of $x(t)$ in the stationary state, under the assumption that such a state exists.\n\n**1. Derivation of the Stationary Mean**\n\nLet $\\mu(t)$ denote the expectation of the process $x(t)$, i.e., $\\mu(t) = \\mathbb{E}[x(t)]$. We take the expectation of the entire SDE. By the linearity of the expectation operator, we can write:\n$$\\mathbb{E}[dx(t)] = \\mathbb{E}[-\\lambda\\,x(t)\\,dt + \\sigma\\,dW_t]$$\n$$\\mathbb{E}[dx(t)] = -\\lambda\\,\\mathbb{E}[x(t)]\\,dt + \\sigma\\,\\mathbb{E}[dW_t]$$\nAssuming sufficient regularity to interchange the expectation and differential operators, we have $d\\mathbb{E}[x(t)] = \\mathbb{E}[dx(t)]$. This gives:\n$$d\\mu(t) = -\\lambda\\,\\mu(t)\\,dt + \\sigma\\,\\mathbb{E}[dW_t]$$\nA fundamental property of the Itô integral with respect to a standard Wiener process is that it is a martingale with zero mean. The increment $dW_t = W_{t+dt} - W_t$ is a random variable with a normal distribution of mean $0$ and variance $dt$. Therefore, its expectation is zero:\n$$\\mathbb{E}[dW_t] = 0$$\nSubstituting this into the equation for $d\\mu(t)$ yields a deterministic ordinary differential equation (ODE) for the mean:\n$$d\\mu(t) = -\\lambda\\,\\mu(t)\\,dt$$\n$$\\frac{d\\mu(t)}{dt} = -\\lambda\\,\\mu(t)$$\nIn the stationary state, the statistical properties of the process are time-independent. Thus, the mean must be a constant, which we denote as $\\mu_{ss}$. This implies that its time derivative is zero:\n$$\\frac{d\\mu_{ss}}{dt} = 0$$\nSubstituting this condition into the ODE gives:\n$$0 = -\\lambda\\,\\mu_{ss}$$\nGiven the problem statement that $\\lambda > 0$, the only possible solution is:\n$$\\mu_{ss} = 0$$\nThus, the stationary mean of the process $x(t)$ is $0$.\n\n**2. Derivation of the Stationary Variance**\n\nThe variance of $x(t)$ is defined as $V(t) = \\text{Var}[x(t)] = \\mathbb{E}[x(t)^2] - (\\mathbb{E}[x(t)])^2$. Since we are interested in the stationary variance, $V_{ss}$, we can use the stationary mean $\\mu_{ss}=0$. The definition simplifies to:\n$$V_{ss} = \\mathbb{E}_{ss}[x(t)^2]$$\nwhere $\\mathbb{E}_{ss}[\\cdot]$ denotes the expectation in the stationary state. We need to find the stationary second moment, $M_{2,ss} = \\mathbb{E}_{ss}[x(t)^2]$. To do this, we derive the dynamics of the second moment, $\\mathbb{E}[x(t)^2]$.\n\nWe apply Itô's lemma for a function $f(x(t)) = x(t)^2$. For a general Itô process $dx(t) = a(x,t)dt + b(x,t)dW_t$, Itô's lemma states:\n$$df(x(t)) = \\left( a(x,t)\\frac{\\partial f}{\\partial x} + \\frac{1}{2}b(x,t)^2\\frac{\\partial^2 f}{\\partial x^2} \\right)dt + b(x,t)\\frac{\\partial f}{\\partial x}dW_t$$\nIn our case, the drift coefficient is $a(x,t) = -\\lambda x(t)$ and the diffusion coefficient is $b(x,t) = \\sigma$. The derivatives of $f(x) = x^2$ are $\\frac{\\partial f}{\\partial x} = 2x$ and $\\frac{\\partial^2 f}{\\partial x^2} = 2$. The term involving the second derivative arises from the non-zero quadratic variation of the Wiener process, where $(dW_t)^2 = dt$.\n\nSubstituting these into Itô's lemma:\n$$d(x^2) = \\left( (-\\lambda x)(2x) + \\frac{1}{2}\\sigma^2(2) \\right)dt + \\sigma(2x)dW_t$$\n$$d(x^2) = (-2\\lambda x^2 + \\sigma^2)dt + 2\\sigma x dW_t$$\nNow, we take the expectation of this equation to find the dynamics of the second moment, $M_2(t) = \\mathbb{E}[x(t)^2]$:\n$$\\mathbb{E}[d(x^2)] = \\mathbb{E}[(-2\\lambda x^2 + \\sigma^2)dt + 2\\sigma x dW_t]$$\n$$d\\mathbb{E}[x^2] = (-2\\lambda \\mathbb{E}[x^2] + \\sigma^2)dt + 2\\sigma \\mathbb{E}[x dW_t]$$\nThe final term is the expectation of an Itô integral. The process $x(t)$ is adapted to the filtration generated by $W_t$, meaning it is non-anticipating. A key property of Itô integrals is that the expectation of the integral of a non-anticipating process with respect to $dW_t$ is zero. Formally, $\\mathbb{E}[\\int_0^t g(s)dW_s] = 0$ for a suitable non-anticipating process $g(s)$. In differential form, this implies:\n$$\\mathbb{E}[x(t)dW_t] = 0$$\nThis leaves us with the following ODE for the second moment $M_2(t)$:\n$$\\frac{dM_2(t)}{dt} = -2\\lambda M_2(t) + \\sigma^2$$\nIn the stationary state, the second moment is constant, $M_{2,ss}$, so its time derivative is zero:\n$$\\frac{dM_{2,ss}}{dt} = 0$$\nSubstituting this into the ODE for the second moment gives:\n$$0 = -2\\lambda M_{2,ss} + \\sigma^2$$\nSolving for $M_{2,ss}$:\n$$2\\lambda M_{2,ss} = \\sigma^2$$\n$$M_{2,ss} = \\frac{\\sigma^2}{2\\lambda}$$\nAs established earlier, the stationary variance $V_{ss}$ is equal to the stationary second moment since the stationary mean is zero.\n$$V_{ss} = M_{2,ss} - \\mu_{ss}^2 = \\frac{\\sigma^2}{2\\lambda} - 0^2$$\nTherefore, the stationary variance is:\n$$V_{ss} = \\frac{\\sigma^2}{2\\lambda}$$\nThe problem asks for the stationary mean and stationary variance. The derived expressions are $\\mu_{ss} = 0$ and $V_{ss} = \\frac{\\sigma^2}{2\\lambda}$, respectively. Both are expressed in terms of the given parameters $\\lambda$ and $\\sigma$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0 & \\frac{\\sigma^2}{2\\lambda}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Moving from theory to practice involves implementing stochastic concepts within a numerical model. This exercise demonstrates how to replace a constant, deterministic parameter—in this case, eddy diffusivity—with a stochastic counterpart in a simple 1D diffusion model. By writing code to simulate an ensemble of model runs, you will directly observe and quantify how stochastic parameterization generates ensemble spread, which is a key goal for representing model uncertainty .",
            "id": "3916381",
            "problem": "Consider the one-dimensional diffusion of a passive scalar field $u(x,t)$ on the periodic interval $x \\in [0,1)$ governed by the conservation law $\\partial_t u = -\\partial_x J$ and Fick’s law $J = -K(t)\\partial_x u$. For spatially uniform but time-dependent eddy diffusivity $K(t)$, this yields the diffusion equation $$u_t = \\partial_x\\left(K(t)\\partial_x u\\right) = K(t)\\,\\partial_{xx}u$$ Two closures for $K(t)$ are considered:\n- Deterministic eddy diffusivity closure: $K(t) = \\bar{K}$, a constant.\n- Stochastic eddy diffusivity closure: $K(t) = \\bar{K} + \\sigma\\,\\xi(t)$, where $\\xi(t)$ is Gaussian White Noise (GWN), modeled in discrete time as independent, identically distributed standard normal variables at each time step. To ensure physical realism (non-negative diffusivity), impose $K(t) \\ge K_{\\min}$ by truncation, where $K_{\\min} > 0$ is a small constant lower bound.\n\nStarting from the conservation law and Fick’s law, derive a consistent explicit time-stepping and second-order central-difference spatial discretization for the periodic domain. Let $N_x$ be the number of spatial grid points, $\\Delta x = 1/N_x$ the spatial step, $N_t$ the number of time steps, and $\\Delta t$ the time step such that $t_n = n\\,\\Delta t$ and $N_t = T/\\Delta t$ is an integer. Use the update\n$$\nu^{n+1}_m = u^{n}_m + \\Delta t\\,K^n_m\\,\\mathcal{L}u^{n}_m,\n$$\nwhere $m$ indexes the ensemble member, $n$ indexes the time step, and $\\mathcal{L}$ is the discrete periodic Laplacian\n$$\n\\left(\\mathcal{L}u\\right)_i = \\frac{u_{i+1} - 2\\,u_i + u_{i-1}}{\\Delta x^2},\n$$\nwith periodic indexing $u_{-1} \\equiv u_{N_x-1}$ and $u_{N_x} \\equiv u_0$.\n\nUse the initial condition\n$$\nu(x,0) = \\sin\\left(2\\pi x\\right) + \\frac{1}{2}\\sin\\left(4\\pi x\\right),\n$$\nsampled at $x_i = i\\,\\Delta x$ for $i = 0,1,\\dots,N_x-1$.\n\nDefine the ensemble mean at final time $T$ by\n$$\n\\bar{u}(x_i,T) = \\frac{1}{M}\\sum_{m=1}^M u^{(m)}(x_i,T),\n$$\nand the ensemble variance field by\n$$\n\\mathrm{Var}[u](x_i,T) = \\frac{1}{M}\\sum_{m=1}^M \\left(u^{(m)}(x_i,T) - \\bar{u}(x_i,T)\\right)^2.\n$$\nQuantify the scalar ensemble variance by spatial averaging,\n$$\nV(T) = \\frac{1}{N_x}\\sum_{i=0}^{N_x-1}\\mathrm{Var}[u](x_i,T).\n$$\nCompute, for each test case, the difference between the stochastic and deterministic closures’ ensemble variances at final time $T$:\n$$\n\\Delta V = V_{\\mathrm{stoch}}(T) - V_{\\mathrm{det}}(T).\n$$\nNote that for the deterministic closure with identical ensemble members, $V_{\\mathrm{det}}(T) = 0$ exactly.\n\nNumerical implementation requirements:\n- Discretize time using explicit Euler and space using second-order central differences on the periodic grid as specified.\n- Model the stochastic eddy diffusivity closure as $K^n_m = \\max\\!\\left(K_{\\min},\\,\\bar{K} + \\sigma\\,\\xi^n_m\\right)$ with independent $\\xi^n_m \\sim \\mathcal{N}(0,1)$ for each ensemble member $m$ and time index $n$.\n- Use a small positive lower bound $K_{\\min} = 10^{-8}$.\n- Ensure numerical stability by choosing $\\Delta t$ values that satisfy the standard explicit diffusion stability condition $\\Delta t \\lesssim \\frac{\\Delta x^2}{2 K_{\\text{max}}}$, where $K_{\\text{max}}$ is an upper bound on $K(t)$ over the simulation. In the test suite below, $\\Delta t$ is chosen accordingly.\n\nTest suite:\n- Case $1$: $N_x = 64$, $\\Delta t = 0.0005$, $T = 0.1$, $M = 200$, $\\bar{K} = 0.05$, $\\sigma = 0.01$, random seed $= 42$.\n- Case $2$ (boundary case, purely deterministic): $N_x = 64$, $\\Delta t = 0.0005$, $T = 0.1$, $M = 200$, $\\bar{K} = 0.05$, $\\sigma = 0$, random seed $= 123$.\n- Case $3$ (edge case with low mean diffusivity and larger noise, enforcing positivity by truncation): $N_x = 128$, $\\Delta t = 0.0003$, $T = 0.06$, $M = 300$, $\\bar{K} = 0.01$, $\\sigma = 0.015$, random seed $= 2025$.\n\nYour program must:\n- Implement both closures as specified.\n- For each test case, compute $\\Delta V$ as a float.\n- Produce a single line of output containing the results as a comma-separated list enclosed in square brackets, rounded to $6$ decimal places, in the order of the test cases: $[\\Delta V_1, \\Delta V_2, \\Delta V_3]$.",
            "solution": "The user has provided a problem that is scientifically grounded, well-posed, and objective. It is a standard numerical exercise in the field of computational fluid dynamics and geophysical modeling, focusing on the concept of stochastic parameterization for unresolved processes. All parameters, equations, and numerical methods are clearly and correctly specified. The problem is valid and can be solved as stated.\n\n### Principle-Based Solution Design\n\nThe problem requires the numerical solution of the one-dimensional diffusion equation for a passive scalar $u(x,t)$ on a periodic domain $x \\in [0,1)$. The governing equation is derived from the conservation law $\\partial_t u = -\\partial_x J$ and Fick's law of diffusion $J = -K(t)\\partial_x u$. For a spatially uniform but time-dependent diffusivity $K(t)$, the equation simplifies to:\n$$\n\\frac{\\partial u}{\\partial t} = K(t) \\frac{\\partial^2 u}{\\partial x^2}\n$$\nThe problem compares two different models, or closures, for the eddy diffusivity $K(t)$: a simple deterministic model and a more complex stochastic model.\n\n1.  **Deterministic Closure**: $K(t) = \\bar{K}$, a constant mean diffusivity.\n2.  **Stochastic Closure**: $K(t) = \\bar{K} + \\sigma\\,\\xi(t)$, where $\\bar{K}$ is the mean, $\\sigma$ is the noise amplitude, and $\\xi(t)$ is Gaussian White Noise. This closure represents unresolved turbulent fluctuations affecting the effective diffusivity. A physical constraint $K(t) \\ge K_{\\min} > 0$ is enforced to prevent non-physical negative diffusivity, which would correspond to unphysical anti-diffusion (spontaneous sharpening of gradients).\n\nThe core task is to implement a numerical solver for this equation and use it to quantify the impact of the stochastic parameterization on the ensemble statistics of the solution.\n\n#### Discretization Strategy\nThe problem specifies the numerical scheme:\n-   **Spatial Discretization**: A second-order central difference scheme on a uniform periodic grid with $N_x$ points. The grid spacing is $\\Delta x = 1/N_x$. The discrete Laplacian operator $\\mathcal{L}$ acting on a grid function $u_i$ is given by:\n    $$\n    (\\mathcal{L}u)_i = \\frac{u_{i+1} - 2u_i + u_{i-1}}{\\Delta x^2}\n    $$\n    Periodicity requires that indices are handled modulo $N_x$, such that $u_{N_x} \\equiv u_0$ and $u_{-1} \\equiv u_{N_x-1}$.\n-   **Temporal Discretization**: An explicit Euler (forward Euler) method with a time step $\\Delta t$. The update rule for the solution $u_i^n \\approx u(x_i, t_n)$ from time step $n$ to $n+1$ for a single ensemble member $m$ is:\n    $$\n    u_m^{n+1} = u_m^n + \\Delta t \\cdot K_m^n \\cdot (\\mathcal{L}u_m^n)\n    $$\n    where $K_m^n$ is the realization of the diffusivity for member $m$ at time step $n$.\n\n#### Ensemble Simulation\nTo study the stochastic closure, an ensemble of $M$ simulations is performed. Each member of the ensemble represents a possible realization of the history of the stochastic process $K(t)$.\n-   **Initial Condition**: All ensemble members start from the identical initial state, given by sampling the function $u(x,0) = \\sin(2\\pi x) + \\frac{1}{2}\\sin(4\\pi x)$ on the spatial grid.\n-   **Stochastic Forcing**: For the stochastic closure, at each time step $n$, a unique diffusivity $K_m^n$ is calculated for each ensemble member $m$. This is done by drawing an independent random number $\\xi_m^n$ from the standard normal distribution $\\mathcal{N}(0,1)$ and applying the truncated formula:\n    $$\n    K_m^n = \\max(K_{\\min}, \\bar{K} + \\sigma \\xi_m^n)\n    $$\n-   **Deterministic Case**: For the deterministic case, $K_m^n = \\bar{K}$ for all $m$ and $n$. Since all members start identically and evolve under the same deterministic equation, they remain identical for all time.\n\n#### Analysis of Results\nAfter simulating the ensemble for a total time $T$, we analyze the statistics at the final time.\n-   **Ensemble Mean**: $\\bar{u}(x_i, T) = \\frac{1}{M}\\sum_{m=1}^M u_m(x_i, T)$. This represents the expected evolution of the scalar field.\n-   **Ensemble Variance Field**: $\\mathrm{Var}[u](x_i, T) = \\frac{1}{M}\\sum_{m=1}^M (u_m(x_i, T) - \\bar{u}(x_i, T))^2$. This field quantifies the uncertainty or spread of the ensemble at each grid point, which arises directly from the randomness in $K(t)$.\n-   **Spatially Averaged Variance**: $V(T) = \\frac{1}{N_x}\\sum_{i=0}^{N_x-1} \\mathrm{Var}[u](x_i, T)$. This scalar quantity provides a single measure of the total ensemble spread.\n\nThe target quantity is $\\Delta V = V_{\\mathrm{stoch}}(T) - V_{\\mathrm{det}}(T)$. As noted, for the deterministic closure, all ensemble members are identical, so their variance is zero, $V_{\\mathrm{det}}(T) = 0$. Therefore, the problem reduces to calculating $V_{\\mathrm{stoch}}(T)$ for each test case. This is achieved by running the ensemble simulation with the specified stochastic parameters. For the test case where $\\sigma=0$, the \"stochastic\" run becomes deterministic, and the computed variance is expected to be zero, serving as a self-consistency check.\n\n#### Algorithmic Implementation\nThe algorithm is implemented in Python using the `numpy` library for efficient vectorized computations.\n1.  A function `simulate` is defined to encapsulate the logic for one full ensemble simulation. It takes all physical and numerical parameters as arguments.\n2.  Inside `simulate`, a random seed is set for reproducibility.\n3.  The spatial grid and the initial condition array `u` of shape $(M, N_x)$ are created. All $M$ rows (ensemble members) are initialized to be identical.\n4.  The main simulation loop iterates $N_t = T/\\Delta t$ times. In each iteration:\n    a. The discrete Laplacian is computed for all $M$ members simultaneously using `numpy.roll` for periodic boundary conditions and vectorized arithmetic.\n    b. The diffusivities $K$ for all $M$ members are computed. If $\\sigma > 0$, this involves generating an array of $M$ random numbers. The result is an array of shape $(M, 1)$ to allow broadcasting.\n    c. The state `u` is updated using the explicit Euler step. Broadcasting ensures that each member's state in the `u` array is updated with its own specific value of $K$.\n5.  After the loop, the final ensemble `u` is used to compute the ensemble mean and variance fields, and finally the scalar spatially-averaged variance $V(T)$, following the provided formulas. `numpy.mean` is used for all averaging operations.\n6.  A main script iterates through the test cases, calls the `simulate` function for each, and collects the results ($\\Delta V = V(T)$).\n7.  The final results are formatted to $6$ decimal places and printed in the required list format.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef simulate(Nx, dt, T, M, K_bar, sigma, K_min, seed):\n    \"\"\"\n    Runs an ensemble simulation of the 1D stochastic diffusion equation.\n\n    Args:\n        Nx (int): Number of spatial grid points.\n        dt (float): Time step size.\n        T (float): Total simulation time.\n        M (int): Number of ensemble members.\n        K_bar (float): Mean diffusivity.\n        sigma (float): Noise amplitude for diffusivity.\n        K_min (float): Minimum allowed diffusivity.\n        seed (int): Random seed for reproducibility.\n\n    Returns:\n        float: The spatially averaged ensemble variance V(T) at final time T.\n    \"\"\"\n    # 1. Set seed for reproducibility\n    np.random.seed(seed)\n\n    # 2. Grid and parameter setup\n    dx = 1.0 / Nx\n    x = np.arange(Nx) * dx\n    Nt = int(round(T / dt))\n\n    # 3. Initial Condition\n    # All M ensemble members start with the same initial condition.\n    # The array u has shape (M, Nx).\n    u0 = np.sin(2 * np.pi * x) + 0.5 * np.sin(4 * np.pi * x)\n    u = np.tile(u0, (M, 1))\n\n    # 4. Time stepping loop\n    for _ in range(Nt):\n        # 4a. Compute discrete Laplacian for all ensemble members (vectorized)\n        # np.roll handles periodic boundary conditions.\n        u_lap = (np.roll(u, -1, axis=1) - 2 * u + np.roll(u, 1, axis=1)) / (dx**2)\n\n        # 4b. Compute stochastic diffusivity K for each member\n        if sigma == 0:\n            # Deterministic case, K is a scalar applied to all members.\n            K = K_bar\n        else:\n            # Stochastic case: one K per member.\n            # Generate M random numbers from N(0,1).\n            xi = np.random.randn(M, 1)\n            # Calculate K and apply truncation. Shape is (M, 1) for broadcasting.\n            K = np.maximum(K_min, K_bar + sigma * xi)\n        \n        # 4c. Update u using explicit Euler step (vectorized)\n        # K (M,1) or scalar is broadcast across u_lap (M, Nx).\n        u += dt * K * u_lap\n    \n    # 5. Compute final statistics\n    if M <= 1:\n        # Variance requires at least 2 members.\n        return 0.0\n\n    # 5a. Ensemble mean field over all members (axis=0)\n    u_mean = np.mean(u, axis=0)  # Shape: (Nx,)\n\n    # 5b. Ensemble variance field. u_mean is broadcast for subtraction.\n    u_var_field = np.mean((u - u_mean)**2, axis=0) # Shape: (Nx,)\n\n    # 5c. Spatially averaged variance\n    V_T = np.mean(u_var_field)\n\n    return V_T\n\ndef solve():\n    \"\"\"\n    Sets up and runs the test cases, then prints the results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1\n        {'Nx': 64, 'dt': 0.0005, 'T': 0.1, 'M': 200, 'K_bar': 0.05, 'sigma': 0.01, 'seed': 42},\n        # Case 2\n        {'Nx': 64, 'dt': 0.0005, 'T': 0.1, 'M': 200, 'K_bar': 0.05, 'sigma': 0, 'seed': 123},\n        # Case 3\n        {'Nx': 128, 'dt': 0.0003, 'T': 0.06, 'M': 300, 'K_bar': 0.01, 'sigma': 0.015, 'seed': 2025},\n    ]\n\n    K_min = 1e-8\n    results = []\n    \n    for case in test_cases:\n        # Per the problem statement, Delta_V = V_stoch(T) - V_det(T).\n        # Since V_det(T) is exactly 0, we only need to compute V_stoch(T).\n        # The simulate function calculates this value for the parameters of each test case.\n        delta_V = simulate(\n            Nx=case['Nx'],\n            dt=case['dt'],\n            T=case['T'],\n            M=case['M'],\n            K_bar=case['K_bar'],\n            sigma=case['sigma'],\n            K_min=K_min,\n            seed=case['seed']\n        )\n        results.append(delta_V)\n\n    # Format the results to 6 decimal places and print in the required format.\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A stochastic model is only as good as its parameters, but how are these determined in practice? This exercise tackles this crucial question by showing how to estimate the parameters of an OU process from a time series of data, such as forecast-minus-analysis residuals. You will derive and implement a Maximum Likelihood Estimator to find the model's decorrelation time ($T$) and variance ($\\sigma^2$), connecting the abstract mathematical model to real-world data and closing the loop between theory and application .",
            "id": "4094933",
            "problem": "Consider an Ornstein–Uhlenbeck (OU) model for forecast-minus-analysis residuals used in stochastic parameterizations such as Stochastic Kinetic Energy Backscatter (SKEB) and stochastic convection. Let the continuous-time residual process be modeled by the linear stochastic differential equation\n$$\n\\mathrm{d}X_t = -\\frac{1}{T} X_t \\,\\mathrm{d}t + \\sqrt{\\frac{2\\sigma^2}{T}} \\,\\mathrm{d}W_t,\n$$\nwhere $T$ is the decorrelation time, $\\sigma^2$ is the stationary variance of $X_t$, and $W_t$ is a standard Wiener process. Assume the process is strictly stationary. For a fixed sampling interval $\\Delta t$, the exact discrete-time transition of this OU process can be represented as a first-order autoregression with Gaussian innovations. You are asked to derive the likelihood function for a finite sample of residuals and then design an algorithm that estimates $T$ and $\\sigma^2$ from time series data by maximum likelihood.\n\nStarting from fundamental definitions of the OU process and the properties of Gaussian Markov processes, derive the conditional likelihood of a discrete-time sample $\\{x_0, x_1, \\dots, x_N\\}$ at times $t_n = n \\Delta t$ under the OU model. Use this likelihood to construct a maximum likelihood estimator for the parameters $T$ and $\\sigma^2$. Your derivation must proceed from the linearity of the OU dynamics, the Gaussianity of the increments, and the Markov property, without using pre-quoted formulas for autoregressive estimators.\n\nImplement your estimator in a complete, runnable program. The program must:\n- Simulate synthetic forecast-minus-analysis residual series under the OU model for a given parameter set using the exact discrete-time representation implied by the continuous-time OU process.\n- Estimate $\\hat{T}$ and $\\hat{\\sigma}^2$ from each simulated series using the derived maximum likelihood method.\n- Produce the results in a single line of output as a comma-separated list enclosed in square brackets, in the order $[\\hat{T}_1, \\hat{\\sigma}^2_1, \\hat{T}_2, \\hat{\\sigma}^2_2, \\hat{T}_3, \\hat{\\sigma}^2_3, \\hat{T}_4, \\hat{\\sigma}^2_4]$.\n\nPhysical units:\n- Report all decorrelation times $T$ and $\\hat{T}$ in seconds.\n- Variance $\\sigma^2$ and $\\hat{\\sigma}^2$ are unit-consistent with the residual variable; treat them as dimensionally consistent scalars and report their numerical values without units.\n\nAngle units are not required. Any quantity expressed as a proportion must be provided as a decimal.\n\nTest suite:\nUse the following parameter sets to generate the synthetic time series. For each case, simulate from the stationary distribution at $t_0$ and then follow the exact discrete-time OU dynamics for $N$ subsequent steps.\n- Case $1$: $\\Delta t = 300$ s, $T = 3600$ s, $\\sigma^2 = 4.0$, $N = 2000$, random seed $= 42$.\n- Case $2$: $\\Delta t = 300$ s, $T = 600$ s, $\\sigma^2 = 1.0$, $N = 1500$, random seed $= 123$.\n- Case $3$: $\\Delta t = 600$ s, $T = 1800$ s, $\\sigma^2 = 9.0$, $N = 30$, random seed $= 7$.\n- Case $4$: $\\Delta t = 60$ s, $T = 86400$ s, $\\sigma^2 = 2.25$, $N = 5000$, random seed $= 31415$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the exact order specified above. For example, an output line should look like $[r_1,r_2,r_3,r_4,r_5,r_6,r_7,r_8]$ where each $r_k$ is a floating-point number.",
            "solution": "The problem requires the derivation and implementation of a Maximum Likelihood Estimator (MLE) for the parameters of an Ornstein-Uhlenbeck (OU) process from a discrete-time series of observations.\n\nThe continuous-time OU process is defined by the stochastic differential equation (SDE):\n$$\n\\mathrm{d}X_t = -\\frac{1}{T} X_t \\,\\mathrm{d}t + \\sqrt{\\frac{2\\sigma^2}{T}} \\,\\mathrm{d}W_t\n$$\nwhere $T$ is the decorrelation time, $\\sigma^2$ is the stationary variance of the process $X_t$, and $W_t$ represents a standard Wiener process.\n\nFirst, we derive the exact discrete-time representation of this process. The SDE is a linear first-order equation. Its solution over a time interval $\\Delta t = t_{n+1} - t_n$ gives the relationship between $X_{t_{n+1}}$ and $X_{t_n}$. Let $X_n = X_{t_n}$. The solution is:\n$$\nX_{n+1} = X_n e^{- \\frac{\\Delta t}{T}} + \\sqrt{\\frac{2\\sigma^2}{T}} \\int_{t_n}^{t_{n+1}} e^{-\\frac{1}{T}(t_{n+1}-s)} \\,\\mathrm{d}W_s\n$$\nThis equation describes a first-order autoregressive (AR(1)) process of the form $X_{n+1} = a X_n + \\epsilon_n$.\n\nThe autoregressive parameter, $a$, is identified by inspection:\n$$\na = e^{-\\frac{\\Delta t}{T}}\n$$\nThe innovation term, $\\epsilon_n$, is the stochastic integral. As an Itō integral of a deterministic function, it is a Gaussian random variable with a mean of zero, $E[\\epsilon_n] = 0$. Its variance, $\\sigma_\\epsilon^2 = E[\\epsilon_n^2]$, is determined using the Itō isometry property:\n$$\n\\sigma_\\epsilon^2 = \\left(\\sqrt{\\frac{2\\sigma^2}{T}}\\right)^2 \\int_{t_n}^{t_{n+1}} \\left(e^{-\\frac{1}{T}(t_{n+1}-s)}\\right)^2 \\,\\mathrm{d}s = \\frac{2\\sigma^2}{T} \\int_{t_n}^{t_{n+1}} e^{-\\frac{2}{T}(t_{n+1}-s)} \\,\\mathrm{d}s\n$$\nEvaluating the integral yields:\n$$\n\\sigma_\\epsilon^2 = \\frac{2\\sigma^2}{T} \\left[ \\frac{T}{2} e^{-\\frac{2}{T}(t_{n+1}-s)} \\right]_{s=t_n}^{s=t_{n+1}} = \\sigma^2 \\left( e^0 - e^{-\\frac{2\\Delta t}{T}} \\right) = \\sigma^2 \\left(1 - \\left(e^{-\\frac{\\Delta t}{T}}\\right)^2 \\right)\n$$\nThus, the variance of the innovations is $\\sigma_\\epsilon^2 = \\sigma^2(1-a^2)$.\n\nThe problem asks for the conditional likelihood of a discrete sample $\\{x_0, x_1, \\dots, x_N\\}$, conditioned on the initial observation $x_0$. Due to the Markov property of the AR(1) process, the joint probability density function of $\\{x_1, \\dots, x_N\\}$ given $x_0$ is the product of the individual conditional probabilities:\n$$\np(x_1, \\dots, x_N | x_0; T, \\sigma^2) = \\prod_{n=0}^{N-1} p(x_{n+1} | x_n; T, \\sigma^2)\n$$\nThe conditional distribution $p(x_{n+1} | x_n)$ is Gaussian, with mean $E[X_{n+1}|X_n=x_n] = a x_n$ and variance $\\sigma_\\epsilon^2$. The conditional log-likelihood function, $\\ell_c$, is:\n$$\n\\ell_c(T, \\sigma^2) = \\log p(x_1, \\dots, x_N | x_0) = \\sum_{n=0}^{N-1} \\log p(x_{n+1} | x_n)\n$$\n$$\n\\ell_c = \\sum_{n=0}^{N-1} \\left( -\\frac{1}{2}\\log(2\\pi\\sigma_\\epsilon^2) - \\frac{(x_{n+1} - a x_n)^2}{2\\sigma_\\epsilon^2} \\right) = -\\frac{N}{2}\\log(2\\pi\\sigma_\\epsilon^2) - \\frac{1}{2\\sigma_\\epsilon^2} \\sum_{n=0}^{N-1} (x_{n+1} - a x_n)^2\n$$\nTo find the MLEs, we express $\\ell_c$ in terms of the fundamental parameters $a$ and $\\sigma^2$ using $\\sigma_\\epsilon^2 = \\sigma^2(1-a^2)$:\n$$\n\\ell_c(a, \\sigma^2) = -\\frac{N}{2}\\log(2\\pi\\sigma^2(1-a^2)) - \\frac{1}{2\\sigma^2(1-a^2)} \\sum_{n=0}^{N-1} (x_{n+1} - a x_n)^2\n$$\nWe maximize this function with respect to $a$ and $\\sigma^2$. Differentiating with respect to $\\sigma^2$ and setting the result to zero gives the MLE for $\\sigma^2$ as a function of $a$:\n$$\n\\frac{\\partial \\ell_c}{\\partial \\sigma^2} = -\\frac{N}{2\\sigma^2} + \\frac{1}{2(\\sigma^2)^2(1-a^2)} \\sum_{n=0}^{N-1} (x_{n+1} - a x_n)^2 = 0 \\implies \\hat{\\sigma}^2(a) = \\frac{1}{N(1-a^2)} \\sum_{n=0}^{N-1} (x_{n+1} - a x_n)^2\n$$\nSubstituting this back into $\\ell_c$ yields the profile log-likelihood for $a$. Maximizing $\\ell_c(a, \\hat{\\sigma}^2(a))$ with respect to $a$ is equivalent to minimizing the term $\\hat{\\sigma}^2(a)(1-a^2)$:\n$$\n\\hat{\\sigma}^2(a)(1-a^2) = \\frac{1}{N} \\sum_{n=0}^{N-1} (x_{n+1} - a x_n)^2\n$$\nThus, the MLE for $a$, denoted $\\hat{a}$, is the value that minimizes the sum of squared residuals $S(a) = \\sum_{n=0}^{N-1} (x_{n+1} - a x_n)^2$. This corresponds to an ordinary least squares (OLS) problem. Setting the derivative of $S(a)$ to zero gives:\n$$\n\\frac{\\mathrm{d}S(a)}{\\mathrm{d}a} = -2\\sum_{n=0}^{N-1} x_n(x_{n+1} - a x_n) = 0 \\implies \\hat{a}\\sum_{n=0}^{N-1} x_n^2 = \\sum_{n=0}^{N-1} x_n x_{n+1}\n$$\nThe estimator $\\hat{a}$ is therefore:\n$$\n\\hat{a} = \\frac{\\sum_{n=0}^{N-1} x_n x_{n+1}}{\\sum_{n=0}^{N-1} x_n^2}\n$$\nWith $\\hat{a}$, we find the MLE for the decorrelation time, $\\hat{T}$, by inverting the relation $a = e^{-\\Delta t/T}$:\n$$\n\\hat{T} = -\\frac{\\Delta t}{\\log(\\hat{a})}\n$$\nFinally, the MLE for the stationary variance, $\\hat{\\sigma}^2$, is obtained by substituting $\\hat{a}$ back into the expression for $\\hat{\\sigma}^2(a)$:\n$$\n\\hat{\\sigma}^2 = \\frac{1}{N(1-\\hat{a}^2)} \\sum_{n=0}^{N-1} (x_{n+1} - \\hat{a} x_n)^2\n$$\nThese formulae provide a complete algorithm for estimating $T$ and $\\sigma^2$ from the data. The algorithm will be implemented to simulate time series based on the OU model and then estimate the parameters from these synthetic data.",
            "answer": "```python\nimport numpy as np\n\ndef simulate_ou_series(T, sigma2, dt, N, seed):\n    \"\"\"\n    Simulates a time series from an Ornstein-Uhlenbeck process.\n\n    Args:\n        T (float): Decorrelation time in seconds.\n        sigma2 (float): Stationary variance.\n        dt (float): Sampling interval in seconds.\n        N (int): Number of steps to simulate after the initial point.\n        seed (int): Seed for the random number generator.\n\n    Returns:\n        numpy.ndarray: A time series of length N+1.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    \n    # Calculate AR(1) parameters from OU parameters\n    a = np.exp(-dt / T)\n    sigma_epsilon_sq = sigma2 * (1 - a**2)\n    sigma_epsilon = np.sqrt(sigma_epsilon_sq)\n    \n    # Initialize the time series array\n    x = np.zeros(N + 1)\n    \n    # The process is stationary, so the first point is drawn from the stationary distribution\n    x[0] = rng.normal(loc=0, scale=np.sqrt(sigma2))\n    \n    # Generate all innovations at once for efficiency\n    epsilon = rng.normal(loc=0, scale=sigma_epsilon, size=N)\n    \n    # Propagate the process using the discrete-time AR(1) formula\n    for n in range(N):\n        x[n+1] = a * x[n] + epsilon[n]\n        \n    return x\n\ndef estimate_ou_params_mle(x, dt):\n    \"\"\"\n    Estimates OU parameters T and sigma2 from a time series using Maximum Likelihood.\n\n    Args:\n        x (numpy.ndarray): The time series data, of length N+1.\n        dt (float): The sampling interval in seconds.\n\n    Returns:\n        tuple[float, float]: A tuple containing the estimated T_hat and sigma2_hat.\n    \"\"\"\n    N = len(x) - 1\n    if N < 1:\n        return (np.nan, np.nan)\n\n    # Create lagged and current series for autoregression\n    # x_n for n from 0 to N-1\n    x_lag = x[:-1]\n    # x_{n+1} for n from 0 to N-1\n    x_t = x[1:]\n\n    # Calculate sums required for the OLS estimate of the AR(1) parameter 'a'\n    # sum(x_n^2) from n=0 to N-1\n    C0 = np.sum(x_lag**2)\n    # sum(x_n * x_{n+1}) from n=0 to N-1\n    C1 = np.sum(x_lag * x_t)\n\n    # Handle the case of a zero-valued series\n    if C0 == 0:\n        return (np.nan, np.nan)\n\n    # Estimate 'a' using the derived MLE (which is the OLS estimator)\n    a_hat = C1 / C0\n    \n    # For a stationary process with T > 0, we require 0 < a_hat < 1.\n    # If a_hat is outside this range, the log will be undefined or yield a non-physical T.\n    # This can occur with small N or non-stationary data.\n    if not (0 < a_hat < 1):\n        return (np.nan, np.nan)\n\n    # Estimate T from a_hat\n    T_hat = -dt / np.log(a_hat)\n\n    # Calculate the sum of squared residuals for the AR(1) fit\n    residuals = x_t - a_hat * x_lag\n    SSR = np.sum(residuals**2)\n    \n    # Estimate sigma^2 (stationary variance) using its MLE formula\n    sigma2_hat = SSR / (N * (1 - a_hat**2))\n    \n    return T_hat, sigma2_hat\n\ndef solve():\n    \"\"\"\n    Main function to run simulations and estimations for the given test cases.\n    \"\"\"\n    test_cases = [\n        # (dt, T, sigma2, N, seed)\n        (300, 3600, 4.0, 2000, 42),\n        (300, 600, 1.0, 1500, 123),\n        (600, 1800, 9.0, 30, 7),\n        (60, 86400, 2.25, 5000, 31415),\n    ]\n\n    results = []\n    for dt, T_true, sigma2_true, N, seed in test_cases:\n        # 1. Simulate synthetic data\n        time_series = simulate_ou_series(T_true, sigma2_true, dt, N, seed)\n        \n        # 2. Estimate parameters from the synthetic data\n        T_hat, sigma2_hat = estimate_ou_params_mle(time_series, dt)\n        \n        results.extend([T_hat, sigma2_hat])\n\n    # 3. Format and print the final output as a single line\n    print(f\"[{','.join(f'{r:.6f}' for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}