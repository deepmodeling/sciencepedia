## Applications and Interdisciplinary Connections

Having established the foundational principles and mechanisms for discovering physical parameterizations with deep learning, we now turn our attention to the application of these methods in diverse and complex settings. This chapter serves not to reiterate the core concepts, but to explore their utility, extension, and integration in a variety of scientific and engineering disciplines. Through a series of case studies drawn from atmospheric science, oceanography, systems biology, and engineering, we will demonstrate how the abstract principles of [physics-informed machine learning](@entry_id:137926) are instantiated to solve concrete problems. The recurring theme is that the most robust and insightful applications are not those that treat deep learning as a black-box replacement for physical theory, but rather those that forge a deep synergy between data-driven techniques and the enduring truths of conservation laws, symmetries, and physical constraints.

### Parameterizing Core Processes in Earth System Models

The initial impetus and most mature applications of deep learning for parameterization discovery are found within Earth System Models (ESMs), where longstanding challenges in representing subgrid-scale processes have limited predictive skill. Here, we examine applications to the atmosphere, oceans, and cryosphere, illustrating how deep learning is being tailored to the unique physics of each domain.

#### Atmospheric Processes

The atmosphere is a multi-scale fluid system where [critical phenomena](@entry_id:144727) like convection, turbulence, and cloud formation occur at scales far smaller than the grid cells of global models.

**Moist Convection and Cloud Formation**
The vertical transport of heat and moisture by [moist convection](@entry_id:1128092) is a principal driver of global circulation and weather patterns. Traditional parameterizations struggle to capture the complex lifecycle of convective clouds. A deep learning approach, when designed with care, can learn this behavior from high-fidelity simulations. A crucial design consideration is the choice of predictors and targets. To ensure thermodynamic consistency, the learned closure must respect the conservation of energy and water. This is best achieved by formulating the problem in terms of [conserved variables](@entry_id:747720). For instance, rather than predicting temperature and humidity tendencies separately, a more robust model predicts the subgrid tendencies of moist static energy ($h = c_p T + gz + L_v q_v$) and total water ($q_t$). The inputs to such a model must be physically sufficient to determine the convective state, including not only the vertical profiles of the thermodynamic state but also the large-scale forcings such as vertical velocity, [radiative heating](@entry_id:754016), and surface fluxes that drive the system. By learning the mapping from the complete atmospheric state and its forcings to the tendencies of conserved quantities, the neural network acts as a physically consistent closure that can be stably integrated within an ESM .

**Turbulence and Boundary Layer Dynamics**
Turbulent eddies in the atmospheric boundary layer are responsible for the vertical exchange of momentum, heat, and moisture between the surface and the free atmosphere. A particularly challenging phenomenon is countergradient transport, where turbulent fluxes are directed against the mean gradient, a behavior that simple downgradient diffusion models (e.g., $\overline{w' \theta'} = -K \partial \bar{\theta} / \partial z$) fundamentally cannot capture. This nonlocality arises from large, coherent eddies that transport properties over significant vertical distances.

A successful learned parameterization must therefore be nonlocal. One effective strategy is to decompose the turbulent flux into a local, downgradient diffusive component and a nonlocal, countergradient component. A neural network can be trained to predict the functional forms of both parts, conditioned on the state of the boundary layer. Crucially, such a model must be trained to respect fundamental physical laws, such as the [entrainment](@entry_id:275487) relation at the top of the boundary layer, which connects the turbulent flux to the growth rate of the layer and the strength of the overlying inversion. By incorporating the [integral conservation laws](@entry_id:202878) and boundary flux conditions directly into the training objective, the learned nonlocal closure can accurately represent complex phenomena like countergradient transport while remaining physically consistent .

Near the Earth's surface, in the so-called surface layer, the interaction between mechanical shear and thermal buoyancy governs the turbulent exchange. Monin–Obukhov similarity theory (MOST) provides a canonical framework for this, describing how dimensionless gradients of wind and temperature are universal functions of a stability parameter, $\zeta = z/L$, where $L$ is the Obukhov length. Rather than hard-coding the empirically derived similarity functions, a neural network can learn them from data. By providing the network with dimensionless inputs that characterize stability (such as $\zeta$ itself or the gradient Richardson number) and training it to predict dimensionless gradients, the network implicitly discovers the stability dependence. Enforcing known physical limits, such as the logarithmic profiles in neutral conditions ($\zeta \to 0$), as constraints on the [network architecture](@entry_id:268981) or loss function further ensures physical realism .

**Cloud Microphysics**
The formation of precipitation within clouds involves a complex web of interactions between water in its vapor, liquid, and ice phases. Processes like the conversion of cloud droplets to raindrops (autoconversion) and the collection of cloud droplets by falling rain (accretion) are described by highly uncertain parameterizations. A key challenge for a data-driven surrogate is to strictly enforce physical constraints, namely the conservation of total water mass, the non-negativity of all water species, and [thermodynamic consistency](@entry_id:138886) with the saturation state of water vapor.

A powerful architectural choice for this task is a reaction-network formulation. In this approach, each microphysical process is represented as a reaction with a specific stoichiometry. For example, [autoconversion and accretion](@entry_id:1121258) both transfer mass from cloud water ($q_c$) to rainwater ($q_r$), which can be encoded in a stoichiometric matrix. A neural network predicts the rates of these reactions, which are then passed through architectural layers that guarantee positivity and prevent the removal of more mass from a species than is available (donor limitation). The updates to the water species are then computed using the stoichiometric matrix, which guarantees mass conservation by construction. Finally, a separate, differentiable physical adjustment step ensures that any resulting [supersaturation](@entry_id:200794) is condensed, maintaining [thermodynamic consistency](@entry_id:138886). This "hard-constraint" approach is superior to simply penalizing constraint violations in the loss function, as it builds the physics into the very fabric of the model .

#### Oceanic and Cryospheric Dynamics

The principles of discovering physics-constrained parameterizations extend naturally to the ocean and [cryosphere](@entry_id:1123254).

**Ocean Mesoscale Eddies**
In the ocean, mesoscale eddies—the oceanic equivalent of weather systems—are a dominant mechanism for transporting heat, salt, and biogeochemical tracers over vast distances. Because global ocean models cannot resolve these eddies, their effects must be parameterized. The Gent–McWilliams (GM) parameterization is a cornerstone of ocean modeling, capturing the tendency of eddies to flatten isopycnal (constant-density) surfaces, thereby releasing [available potential energy](@entry_id:1121282).

A learned parameterization can be designed to emulate and potentially improve upon the GM scheme. The key is to adopt a mathematical structure that hard-codes the fundamental principles of the GM parameterization. This involves parameterizing an eddy-induced "bolus" velocity, $\mathbf{u}^*$, which is added to the resolved flow. This velocity field must be constructed to be non-divergent (to conserve mass) and to transport tracers purely along isopycnal surfaces (the "neutrality" principle). A specific mathematical form for $\mathbf{u}^*$ involving the curl of a [vector potential](@entry_id:153642) automatically satisfies these constraints. The role of the neural network is then not to learn the entire velocity field from scratch, but to predict the magnitude of the underlying process—a scalar thickness diffusivity, $K_\theta$. This scalar coefficient can be learned as a function of local flow properties like stratification and eddy kinetic energy. By incorporating scale awareness, for instance by making $K_\theta$ dependent on the ratio of grid spacing to the Rossby deformation radius, the parameterization can become resolution-aware, a key goal for modern ocean models .

**Sea-Ice Mechanics**
The dynamics of the sea-ice pack are governed by its response to wind and ocean forcing, which is mediated by the internal stresses generated by ice floes colliding, shearing, and ridging. The relationship between the internal stress and the ice deformation rate is known as the sea-ice [rheology](@entry_id:138671). Learning this rheology from data presents a challenge in respecting the principles of continuum mechanics.

A physically consistent learned closure for the [internal stress](@entry_id:190887) tensor, $\boldsymbol{\sigma}$, must guarantee conservation of linear and angular momentum. In a discrete numerical model, linear [momentum conservation](@entry_id:149964) is achieved if the divergence of the stress, $\nabla \cdot \boldsymbol{\sigma}$, is formulated as a flux across cell faces, ensuring that [internal forces](@entry_id:167605) for the entire domain sum to zero. Conservation of angular momentum requires that the stress tensor be symmetric. Finally, the [second law of thermodynamics](@entry_id:142732) demands that mechanical deformation processes like ridging result in non-negative [energy dissipation](@entry_id:147406). A neural network can be trained to predict the parameters of the [rheology](@entry_id:138671) (e.g., effective bulk and shear viscosities) as positive functions of the local ice state and strain-rate invariants. By using these learned viscosities to construct a symmetric stress tensor and then discretizing its divergence in a flux-conservative manner, the learned model can emulate complex mechanical behavior while rigorously adhering to fundamental conservation laws .

### Advanced Architectures and Learning Paradigms

The challenge of learning physical laws has spurred the development of novel deep learning architectures and methods that move beyond standard [function approximation](@entry_id:141329). These techniques embed physical priors directly into the model's structure.

#### Designing Physics-Aware Architectures

**Exploiting Symmetries and Anisotropies**
The laws of physics exhibit fundamental symmetries, and a successful learning architecture must respect them. For many geophysical fluid dynamics problems, the governing equations are approximately equivariant to horizontal translation—the physics doesn't depend on the absolute longitude or latitude. In contrast, the vertical direction is fundamentally different due to gravity and stratification, creating a strong anisotropy.

A learning architecture can be designed to reflect this physical reality. For a task like predicting a column-integrated tendency from both vertical profiles and a horizontal patch of data, a "two-tower" architecture is effective. One tower, a two-dimensional Convolutional Neural Network (2D CNN), processes the horizontal fields. The shared weights of the [convolution kernels](@entry_id:204701) give this tower the property of [translation equivariance](@entry_id:634519) by construction. The second tower, a one-dimensional sequence model (like an RNN or a 1D CNN), processes the vertical profiles, explicitly treating the vertical as an ordered sequence and using the height or pressure of each level as an input to account for the vertical anisotropy. Information from both towers can be fused, and the final output can be constructed using a non-trainable "physics layer" that performs a density-weighted vertical integral, directly implementing the known physical definition of a column-integrated quantity .

**Molecular-Scale Symmetries: The Case of Proteins**
The same principles of symmetry and physical constraint are paramount when applying deep learning to the molecular world. In [protein structure prediction](@entry_id:144312), the goal is to predict the 3D coordinates of atoms from an [amino acid sequence](@entry_id:163755). The energy of a protein, a scalar quantity, must be invariant to global rotations and translations of the molecule. Forces on atoms, being vector quantities, must co-rotate with the molecule. This is the principle of SE(3)-equivariance, where SE(3) is the special Euclidean group of 3D rotations and translations. Modern architectures for [molecular modeling](@entry_id:172257) build this equivariance directly into the network layers, ensuring that predictions are physically consistent regardless of the protein's orientation. Furthermore, models incorporate other deep physical priors, such as the locality of chemical interactions (by restricting message-passing to nearby atoms) and inviolable stereochemical rules, like fixed covalent bond lengths and angles, and the [chirality](@entry_id:144105) ("handedness") of amino acid residues. These are not patterns to be learned from data, but fundamental inductive biases that dramatically constrain the search space and enable the learning of valid protein structures .

#### Learning Operators and Nonlocal Dynamics

Many physical parameterizations are not [simple functions](@entry_id:137521) of the local state but depend on the state over a wider region. The subgrid flux at a point $\boldsymbol{x}$ may depend on the entire field $\overline{u}(\boldsymbol{y})$ over the domain. This requires learning an operator, a mapping from one function to another.

**From Pointwise Mappings to Neural Operators**
A traditional, local closure is a pointwise mapping: the flux at $\boldsymbol{x}$ is a function of the state (and its derivatives) at $\boldsymbol{x}$. However, many subgrid processes are nonlocal. For instance, the filtering operation in Large Eddy Simulation itself averages information over a neighborhood, inducing nonlocal dependencies in the subgrid terms. An operator-valued closure, such as one defined by an integral over the domain, $\boldsymbol{F}(\boldsymbol{x}) = \int_{\Omega} \boldsymbol{K}(\boldsymbol{x}, \boldsymbol{y}) h(\overline{u}(\boldsymbol{y})) d\boldsymbol{y}$, can capture these nonlocal interactions. Neural Operators are a class of deep learning architectures designed specifically to learn such mappings between [function spaces](@entry_id:143478) .

**A Tale of Two Operators: FNO and DeepONet**
Two prominent architectures for learning operators are the Fourier Neural Operator (FNO) and the Deep Operator Network (DeepONet). They embody different structural priors and are suited to different problem types.
-   **Fourier Neural Operator (FNO)** works by performing convolution in the Fourier domain. This makes it inherently global and exceptionally efficient at capturing nonlocal interactions. Because convolution is translation-equivariant, FNO has a very strong and appropriate [inductive bias](@entry_id:137419) for problems on [periodic domains](@entry_id:753347) with [statistical homogeneity](@entry_id:136481), such as idealized turbulence. Its parameterization in Fourier space also makes it resolution-independent, allowing a model trained on one grid to be applied to another.
-   **Deep Operator Network (DeepONet)** approximates an operator using a separable structure, combining a "branch" network that encodes the input function (evaluated at a set of sensor points) and a "trunk" network that depends on the coordinate where the output is to be evaluated. This architecture is mesh-free and highly flexible, making it advantageous for problems with complex, irregular geometries or when data is sparse and unstructured. Its explicit low-rank structure can also be a powerful prior in low-data regimes if the true operator has this property.

The choice between FNO and DeepONet depends on the known properties of the system: for periodic, homogeneous problems, FNO's equivariance is a powerful asset; for geometrically complex, mesh-free problems, DeepONet's flexibility is key .

### Interdisciplinary Frontiers and Grand Challenges

The paradigm of discovering parameterized physics from data extends far beyond Earth system science, with transformative applications emerging in engineering, biology, and beyond. This section highlights some of these frontiers and the grand challenges that unite them.

#### Engineering and Systems Biology

**Augmenting Classical Engineering Correlations**
In fields like heat transfer, decades of experimental work have produced a wealth of empirical correlations, for example, for the Nusselt number ($\mathrm{Nu}$) as a function of the Reynolds ($\mathrm{Re}$) and Prandtl ($\mathrm{Pr}$) numbers. Rather than replacing this accumulated knowledge, machine learning can augment it. A deep learning model can be trained to predict a multiplicative correction to a classical correlation. A successful approach must embed core physics: the model should be dimensionally consistent (acting on [dimensionless groups](@entry_id:156314)), enforce positivity constraints (from the [second law of thermodynamics](@entry_id:142732)), respect known monotonic trends (e.g., heat transfer increasing with $\mathrm{Re}$), and recover the correct, theoretically-known asymptotic limits (e.g., at very high or very low $\mathrm{Pr}$). Frameworks like Gaussian Processes with physics-informed kernels or neural networks with structured outputs and regularization can learn highly accurate corrections from limited high-fidelity data while guaranteeing consistency with these bedrock physical principles .

**Discovering Dynamics in Biological Systems**
Many biological processes, from cancer growth to the formation of patterns in developing embryos, are governed by reaction-diffusion equations. A classic example is the formation of Turing patterns, where two interacting and diffusing chemical [morphogens](@entry_id:149113) can spontaneously form stable, spatially periodic patterns from a homogeneous state. The governing PDEs contain unknown parameters, such as diffusion coefficients and reaction rates. Physics-Informed Neural Networks (PINNs) provide a powerful framework for inferring these parameters from sparse and noisy experimental observations. A PINN represents the morphogen concentration fields as neural networks and trains them to simultaneously fit the sparse observations and satisfy the governing PDEs over the entire spatiotemporal domain. The unknown physical parameters are included as trainable variables in the network. By minimizing a composite loss function that includes data mismatch and PDE residuals, the network learns a solution consistent with both the data and the underlying physical law, yielding estimates of the unknown parameters in the process .

**The Quest for Explainable Models: SINDy and Symbolic Regression**
For many applications, particularly in engineering digital twins and cyber-physical systems, predictive accuracy alone is insufficient. The learned model must be interpretable, transparent, and amenable to formal analysis for safety and control design. This has spurred interest in methods that discover explicit, parsimonious governing equations.
-   **Sparse Identification of Nonlinear Dynamics (SINDy)** assumes the dynamics can be expressed as a sparse linear combination of terms from a large, user-defined library of candidate functions (e.g., polynomials, [trigonometric functions](@entry_id:178918)). It then uses [sparse regression](@entry_id:276495) to find the few terms that best describe the data, yielding an explicit differential equation.
-   **Symbolic Regression**, often implemented with genetic programming, searches the space of mathematical expressions to find a closed-form equation that balances accuracy and complexity.

These "white-box" or "gray-box" methods stand in contrast to "black-box" [deep learning models](@entry_id:635298). While less expressive than a large neural network, their output—a simple, explicit equation—is directly interpretable and provides a deeper understanding of the system's underlying dynamics .

#### Integrating with Data Assimilation and Addressing Climate Change

Two of the grandest challenges in environmental modeling are the optimal fusion of models with observations and the prediction of future climate states. Deep learning is poised to make significant contributions to both.

**Learning from Model-Data Discrepancies**
Data Assimilation (DA) is the process of optimally combining a forecast model with incoming observations to produce the best possible estimate of the state of a system. The difference between the observations and the model forecast—the innovation or analysis increment—is a rich source of information about [model error](@entry_id:175815). If a model has a systematic bias due to a missing or incorrect parameterization, this bias will manifest as a systematic, non-zero-mean signal in the analysis increments.

This provides a powerful opportunity to learn the [model error](@entry_id:175815) online. A machine learning model can be trained to predict the missing physics tendency by using the analysis increments as a learning target. The key insight is that the analysis increment is a projection of the true [model error](@entry_id:175815) into the observable subspace. A learning objective can be formulated to train a closure parameterization such that its predicted tendency, when propagated through the DA system, best matches the observed analysis increments. This approach allows for the direct improvement of a forecast model from the very data streams used to initialize it, without requiring separate high-resolution simulations to provide "truth" labels for the [subgrid physics](@entry_id:755602) .

**The Extrapolation Challenge: Generalizing to Future Climates**
A major criticism of using machine learning in climate modeling is its perceived inability to extrapolate to future, warmer climates that lie outside the distribution of the training data. However, this challenge can be addressed by leveraging physics. Two primary strategies have emerged:
1.  **Physics-Based Nondimensionalization:** As dictated by the Buckingham $\Pi$ theorem, physical laws can often be expressed in a universal form using dimensionless variables. By training a neural network on these dimensionless inputs, the model learns a single, climate-invariant function. If training across a diverse set of present-day and perturbed climates (e.g., with varying sea surface temperatures or greenhouse gas concentrations) covers the range of dimensionless inputs expected in a future climate, the problem is transformed from [extrapolation](@entry_id:175955) in the climate parameter space to interpolation in the dimensionless physical space, a much more tractable task.
2.  **Interpolation in Parameter Space:** An alternative is to explicitly include the climate parameters (e.g., global mean temperature) as inputs to the neural network. By training the model on data from a range of different climates, the network can learn how the [subgrid physics](@entry_id:755602) depends on the climate state. If the future climate state lies within the convex hull of the training climates, the model can make a robust prediction by interpolating in the parameter space.

Enforcing physical constraints like energy conservation is critical in both strategies, as it guides the model to generalize in a physically plausible manner. These approaches provide a pathway toward building learned parameterizations that are robust and credible for climate change projection .

### Conclusion

The applications explored in this chapter paint a clear picture: the discovery of physical parameterizations with deep learning is a field defined by its deep integration with first principles. From the atmospheric boundary layer to the internal mechanics of sea ice, from the folding of proteins to the formation of biological patterns, the most successful and promising approaches are those that embed known physics—conservation laws, symmetries, [dimensional consistency](@entry_id:271193), and asymptotic limits—directly into the learning framework. This synergy transforms deep learning from a generic function approximator into a targeted tool for scientific discovery, enabling the creation of models that are not only accurate but also physically consistent, interpretable, and robust. As this synergy deepens, its impact will continue to grow across the entire landscape of science and engineering.