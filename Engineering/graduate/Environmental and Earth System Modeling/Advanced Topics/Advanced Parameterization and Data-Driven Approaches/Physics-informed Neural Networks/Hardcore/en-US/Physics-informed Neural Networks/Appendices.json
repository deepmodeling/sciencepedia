{
    "hands_on_practices": [
        {
            "introduction": "The foundational step in applying a Physics-Informed Neural Network is to correctly translate the governing physical laws and boundary conditions into a loss function. This exercise guides you through this essential process for a 1D advection equation, a common model for transport phenomena in Earth systems. By constructing the complete loss function, you will practice how to enforce the PDE residual, initial conditions, and boundary conditions simultaneously during training .",
            "id": "2126319",
            "problem": "A team of computational scientists is developing a Physics-Informed Neural Network (PINN) to solve a partial differential equation. A PINN is a neural network whose loss function includes a term that penalizes deviations from the governing physical laws.\n\nThe specific problem is to model the one-dimensional advection equation, which describes the transport of a quantity. The governing equation is:\n$$\n\\frac{\\partial u}{\\partial t} + c \\frac{\\partial u}{\\partial x} = 0\n$$\nwhere $u(x, t)$ is the quantity of interest, and $c$ is a constant positive wave speed. The solution is sought on the spatio-temporal domain defined by $x \\in [X_0, X_1]$ and $t \\in [0, T]$.\n\nThe neural network, denoted by $\\hat{u}(x, t; \\theta)$, approximates the true solution $u(x, t)$. Here, $\\theta$ represents all the trainable parameters (weights and biases) of the network. The goal is to find the optimal parameters $\\theta$ by minimizing a total loss function, $\\mathcal{L}(\\theta)$.\n\nThe total loss function is a weighted sum of three components:\n1.  **PDE Loss ($\\mathcal{L}_{PDE}$)**: Enforces the advection equation inside the domain.\n2.  **Initial Condition Loss ($\\mathcal{L}_{IC}$)**: Enforces the state of the system at $t=0$.\n3.  **Boundary Condition Loss ($\\mathcal{L}_{BC}$)**: Enforces the behavior at the spatial boundaries $x=X_0$ and $x=X_1$.\n\nThe specific conditions are:\n-   **Initial Condition (IC)**: At $t=0$, the profile is a Gaussian pulse, given by $u(x, 0) = f(x) = A \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right)$.\n-   **Boundary Condition (BC)**: The system has periodic boundaries, meaning $u(X_0, t) = u(X_1, t)$ for all $t \\in [0, T]$.\n\nTo compute the loss, the domain is sampled at a finite number of points:\n-   A set of $N_r$ \"residual\" or \"collocation\" points $\\{ (x_i^r, t_i^r) \\}_{i=1}^{N_r}$ are sampled from the interior of the domain, $(X_0, X_1) \\times (0, T]$.\n-   A set of $N_{ic}$ initial points $\\{ (x_i^{ic}, 0) \\}_{i=1}^{N_{ic}}$ are sampled along the initial time line.\n-   A set of $N_{bc}$ boundary time-points $\\{ t_i^{bc} \\}_{i=1}^{N_{bc}}$ are sampled along the temporal domain, for which the boundary condition is evaluated at $x=X_0$ and $x=X_1$.\n\nThe loss for each component is defined as the mean squared error. The total loss function is given by the weighted sum:\n$$\n\\mathcal{L}(\\theta) = w_{PDE} \\mathcal{L}_{PDE} + w_{IC} \\mathcal{L}_{IC} + w_{BC} \\mathcal{L}_{BC}\n$$\nwhere $w_{PDE}$, $w_{IC}$, and $w_{BC}$ are positive constant weights.\n\nYour task is to write down the complete mathematical expression for the total loss function $\\mathcal{L}(\\theta)$. Your expression should be in terms of the neural network approximation $\\hat{u}$ and its partial derivatives, the given parameters and functions ($c, A, \\mu, \\sigma, X_0, X_1$), the sample points, and the weights.",
            "solution": "The goal is to construct the total loss function $\\mathcal{L}(\\theta)$ by defining its three components: the PDE loss $\\mathcal{L}_{PDE}$, the initial condition loss $\\mathcal{L}_{IC}$, and the boundary condition loss $\\mathcal{L}_{BC}$.\n\nFirst, we define the PDE loss, $\\mathcal{L}_{PDE}$. This term measures how well the neural network's output $\\hat{u}(x, t; \\theta)$ satisfies the governing advection equation. We start by defining the PDE residual, $R(x, t; \\theta)$, which is what the equation equals when the approximate solution $\\hat{u}$ is substituted into it:\n$$\nR(x, t; \\theta) = \\frac{\\partial \\hat{u}}{\\partial t}(x, t; \\theta) + c \\frac{\\partial \\hat{u}}{\\partial x}(x, t; \\theta)\n$$\nThe PDE loss is the mean squared error of this residual, evaluated over the $N_r$ collocation points $\\{ (x_i^r, t_i^r) \\}_{i=1}^{N_r}$.\n$$\n\\mathcal{L}_{PDE} = \\frac{1}{N_r} \\sum_{i=1}^{N_r} \\left( R(x_i^r, t_i^r; \\theta) \\right)^2 = \\frac{1}{N_r} \\sum_{i=1}^{N_r} \\left( \\frac{\\partial \\hat{u}}{\\partial t}(x_i^r, t_i^r; \\theta) + c \\frac{\\partial \\hat{u}}{\\partial x}(x_i^r, t_i^r; \\theta) \\right)^2\n$$\n\nSecond, we define the initial condition loss, $\\mathcal{L}_{IC}$. This term measures the discrepancy between the network's prediction at time $t=0$ and the true initial condition, $u(x, 0) = f(x)$. The initial condition is given as $f(x) = A \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)$. The loss is the mean squared error between $\\hat{u}(x_i^{ic}, 0; \\theta)$ and $f(x_i^{ic})$ over the $N_{ic}$ initial points.\n$$\n\\mathcal{L}_{IC} = \\frac{1}{N_{ic}} \\sum_{i=1}^{N_{ic}} \\left( \\hat{u}(x_i^{ic}, 0; \\theta) - f(x_i^{ic}) \\right)^2 = \\frac{1}{N_{ic}} \\sum_{i=1}^{N_{ic}} \\left( \\hat{u}(x_i^{ic}, 0; \\theta) - A \\exp\\left(-\\frac{(x_i^{ic} - \\mu)^2}{2\\sigma^2}\\right) \\right)^2\n$$\n\nThird, we define the boundary condition loss, $\\mathcal{L}_{BC}$. This term enforces the periodic boundary condition, $u(X_0, t) = u(X_1, t)$. The loss is the mean squared error of the difference between the network's predictions at the two boundaries, evaluated at the $N_{bc}$ sample time points $\\{ t_i^{bc} \\}_{i=1}^{N_{bc}}$.\n$$\n\\mathcal{L}_{BC} = \\frac{1}{N_{bc}} \\sum_{i=1}^{N_{bc}} \\left( \\hat{u}(X_0, t_i^{bc}; \\theta) - \\hat{u}(X_1, t_i^{bc}; \\theta) \\right)^2\n$$\n\nFinally, we construct the total loss function $\\mathcal{L}(\\theta)$ by taking the weighted sum of these three components:\n$$\n\\mathcal{L}(\\theta) = w_{PDE} \\mathcal{L}_{PDE} + w_{IC} \\mathcal{L}_{IC} + w_{BC} \\mathcal{L}_{BC}\n$$\nSubstituting the expressions for each component gives the final expression for the total loss function:\n$$\n\\mathcal{L}(\\theta) = \\frac{w_{PDE}}{N_r} \\sum_{i=1}^{N_r} \\left( \\frac{\\partial \\hat{u}}{\\partial t}(x_i^r, t_i^r; \\theta) + c \\frac{\\partial \\hat{u}}{\\partial x}(x_i^r, t_i^r; \\theta) \\right)^2 + \\frac{w_{IC}}{N_{ic}} \\sum_{i=1}^{N_{ic}} \\left( \\hat{u}(x_i^{ic}, 0; \\theta) - A \\exp\\left(-\\frac{(x_i^{ic} - \\mu)^2}{2\\sigma^2}\\right) \\right)^2 + \\frac{w_{BC}}{N_{bc}} \\sum_{i=1}^{N_{bc}} \\left( \\hat{u}(X_0, t_i^{bc}; \\theta) - \\hat{u}(X_1, t_i^{bc}; \\theta) \\right)^2\n$$",
            "answer": "$$\\boxed{\\frac{w_{PDE}}{N_r} \\sum_{i=1}^{N_r} \\left( \\frac{\\partial \\hat{u}}{\\partial t}(x_i^r, t_i^r; \\theta) + c \\frac{\\partial \\hat{u}}{\\partial x}(x_i^r, t_i^r; \\theta) \\right)^2 + \\frac{w_{IC}}{N_{ic}} \\sum_{i=1}^{N_{ic}} \\left( \\hat{u}(x_i^{ic}, 0; \\theta) - A \\exp\\left(-\\frac{(x_i^{ic} - \\mu)^2}{2\\sigma^2}\\right) \\right)^2 + \\frac{w_{BC}}{N_{bc}} \\sum_{i=1}^{N_{bc}} \\left( \\hat{u}(X_0, t_i^{bc}; \\theta) - \\hat{u}(X_1, t_i^{bc}; \\theta) \\right)^2}$$"
        },
        {
            "introduction": "While adding boundary errors to the loss function is common, a more elegant and often robust approach is to enforce these conditions by construction. This practice challenges you to modify the neural network's output architecture itself, guaranteeing that it satisfies the specified Dirichlet boundary conditions regardless of the network's learned parameters. This \"hard constraint\" method can significantly improve training stability and accuracy for many problems .",
            "id": "2126300",
            "problem": "In the field of scientific computing, Physics-Informed Neural Networks (PINNs) have emerged as a powerful tool for solving differential equations. A key aspect of designing a PINN is ensuring that its output, which approximates the solution, respects the given boundary conditions. One robust method to achieve this is to structure the network's final output function so that it satisfies these conditions by construction.\n\nConsider a one-dimensional problem on the spatial domain $x \\in [0, L]$. A neural network provides a raw, unconstrained output function denoted by $\\hat{u}_{NN}(x)$. We wish to use this network to find an approximate solution, $u(x)$, to a differential equation that is subject to the following non-homogeneous Dirichlet boundary conditions:\n$$u(0) = A$$\n$$u(L) = B$$\nHere, $A$, $B$, and $L > 0$ are given real constants.\n\nYour task is to devise a transformation that takes the raw network output $\\hat{u}_{NN}(x)$ and produces a new function, $u_{NN}(x)$, that serves as the final approximation. This transformation must guarantee that $u_{NN}(x)$ strictly satisfies the specified boundary conditions, regardless of the function $\\hat{u}_{NN}(x)$ produced by the network.\n\nProvide an expression for $u_{NN}(x)$ in terms of the raw network output $\\hat{u}_{NN}(x)$ and the parameters $x$, $L$, $A$, and $B$.",
            "solution": "We seek a transformation that maps the raw network output $\\hat{u}_{NN}(x)$ to a function $u_{NN}(x)$ that enforces the Dirichlet boundary conditions $u_{NN}(0)=A$ and $u_{NN}(L)=B$ for any $\\hat{u}_{NN}(x)$. A standard construction is to decompose $u_{NN}(x)$ as\n$$\nu_{NN}(x)=g(x)+s(x)\\,\\hat{u}_{NN}(x),\n$$\nwhere $g(x)$ is any fixed function that satisfies the boundary conditions and $s(x)$ is any function that vanishes at both boundaries. Specifically, we require\n$$\ng(0)=A,\\quad g(L)=B,\\quad s(0)=0,\\quad s(L)=0.\n$$\nA convenient choice is the linear interpolant for $g(x)$,\n$$\ng(x)=A\\left(1-\\frac{x}{L}\\right)+B\\left(\\frac{x}{L}\\right)=A+\\frac{B-A}{L}\\,x,\n$$\nand the simple vanishing factor\n$$\ns(x)=x(L-x),\n$$\nwhich satisfies $s(0)=0$ and $s(L)=0$. Therefore, define\n$$\nu_{NN}(x)=A\\left(1-\\frac{x}{L}\\right)+B\\left(\\frac{x}{L}\\right)+x(L-x)\\,\\hat{u}_{NN}(x).\n$$\nTo verify the boundary conditions, evaluate at $x=0$ and $x=L$:\n$$\nu_{NN}(0)=A\\left(1-0\\right)+B\\left(0\\right)+0\\cdot L\\,\\hat{u}_{NN}(0)=A,\n$$\n$$\nu_{NN}(L)=A\\left(1-1\\right)+B\\left(\\frac{L}{L}\\right)+L( L-L)\\,\\hat{u}_{NN}(L)=B.\n$$\nThus, for any $\\hat{u}_{NN}(x)$, the constructed $u_{NN}(x)$ strictly satisfies $u_{NN}(0)=A$ and $u_{NN}(L)=B$.",
            "answer": "$$\\boxed{A\\left(1-\\frac{x}{L}\\right)+B\\left(\\frac{x}{L}\\right)+x\\left(L-x\\right)\\hat{u}_{NN}(x)}$$"
        },
        {
            "introduction": "The performance of a PINN often hinges on the delicate balance between its different loss components, which is controlled by user-defined weights. This thought experiment explores the consequences of this balancing act when solving the heat equation. By analyzing the behavior of two differently trained models, you will develop a critical intuition for how loss weights influence the solution and diagnose common failure modes in PINN training .",
            "id": "2126325",
            "problem": "A student is exploring the use of a Physics-Informed Neural Network (PINN) to approximate the solution $u(x, t)$ of a physical process. A PINN is a type of neural network trained to find a function that solves a given Partial Differential Equation (PDE). The training process involves minimizing a composite loss function, $\\mathcal{L}_{total}$, which ensures the network's output respects both the governing PDE and its associated boundary and initial conditions.\n\nThe specific problem is the one-dimensional heat equation:\n$$ \\frac{\\partial u}{\\partial t} - \\alpha \\frac{\\partial^2 u}{\\partial x^2} = 0 $$\non the spatial domain $x \\in [0, 1]$ and for time $t \\ge 0$. The thermal diffusivity $\\alpha$ is a positive constant. The initial condition is given by $u(x, 0) = g(x)$ and the boundary conditions are given by $u(0, t) = h_0(t)$ and $u(1, t) = h_1(t)$ for known functions $g$, $h_0$, and $h_1$.\n\nThe total loss function for the PINN is a weighted sum of three components:\n$$ \\mathcal{L}_{total} = \\lambda_{PDE} \\mathcal{L}_{PDE} + \\lambda_{BC} \\mathcal{L}_{BC} + \\lambda_{IC} \\mathcal{L}_{IC} $$\nHere:\n- $\\mathcal{L}_{PDE}$ is a loss term that measures how well the network's output $u(x,t)$ satisfies the heat equation in the interior of the domain. It is small when $\\frac{\\partial u}{\\partial t} - \\alpha \\frac{\\partial^2 u}{\\partial x^2}$ is close to zero.\n- $\\mathcal{L}_{BC}$ is a loss term that measures the mismatch at the spatial boundaries. It is small when $u(0, t)$ is close to $h_0(t)$ and $u(1, t)$ is close to $h_1(t)$.\n- $\\mathcal{L}_{IC}$ is a loss term that measures the mismatch at the initial time. It is small when $u(x, 0)$ is close to $g(x)$.\nThe positive constants $\\lambda_{PDE}$, $\\lambda_{BC}$, and $\\lambda_{IC}$ are user-defined weights that control the relative importance of each loss component during training.\n\nThe student trains two different models, Model 1 and Model 2, by choosing different sets of weights. After training is complete, the student makes the following observations:\n\n- **Model 1**: The resulting approximation $u_1(x,t)$ is an excellent match for the initial and boundary conditions. However, a plot of the PDE residual, defined as $R_1(x,t) = \\left| \\frac{\\partial u_1}{\\partial t} - \\alpha \\frac{\\partial^2 u_1}{\\partial x^2} \\right|$, shows large, non-zero values across most of the domain's interior, indicating the governing physics are not well-captured.\n- **Model 2**: The resulting approximation $u_2(x,t)$ produces an extremely small PDE residual, $R_2(x,t) = \\left| \\frac{\\partial u_2}{\\partial t} - \\alpha \\frac{\\partial^2 u_2}{\\partial x^2} \\right| \\approx 0$, across the entire domain interior. However, the solution behaves poorly at the boundaries, where $u_2(0,t)$ and $u_2(1,t)$ significantly deviate from the required functions $h_0(t)$ and $h_1(t)$.\n\nBased on these observations, which of the following statements most accurately describes the relative weighting used for each model? The notation $A \\gg B$ means 'A is significantly larger than B', and $A \\ll B$ means 'A is significantly smaller than B'. For simplicity, assume $\\lambda_{BC}$ and $\\lambda_{IC}$ are of a similar order of magnitude and refer to them collectively by their effect on boundary/initial conditions.\n\nA. Model 1: $\\lambda_{PDE} \\ll \\lambda_{BC}$; Model 2: $\\lambda_{PDE} \\gg \\lambda_{BC}$\n\nB. Model 1: $\\lambda_{PDE} \\gg \\lambda_{BC}$; Model 2: $\\lambda_{PDE} \\ll \\lambda_{BC}$\n\nC. Model 1: $\\lambda_{PDE} \\approx \\lambda_{BC}$; Model 2: $\\lambda_{PDE} \\gg \\lambda_{BC}$\n\nD. Model 1: $\\lambda_{PDE} \\ll \\lambda_{BC}$; Model 2: $\\lambda_{PDE} \\approx \\lambda_{BC}$\n\nE. Both models were likely trained with $\\lambda_{PDE} \\approx \\lambda_{BC}$, and the difference in outcome is due to random initialization of the networks.",
            "solution": "The PINN is trained by minimizing the weighted sum\n$$\n\\mathcal{L}_{total}=\\lambda_{PDE}\\mathcal{L}_{PDE}+\\lambda_{BC}\\mathcal{L}_{BC}+\\lambda_{IC}\\mathcal{L}_{IC},\n$$\nwhere $\\mathcal{L}_{PDE}$ penalizes violations of the heat equation $\\frac{\\partial u}{\\partial t}-\\alpha \\frac{\\partial^{2}u}{\\partial x^{2}}=0$ in the interior, and $\\mathcal{L}_{BC},\\mathcal{L}_{IC}$ penalize mismatches at the boundaries and initial time. The optimization balances the weighted gradients, so at a trained solution the dominant contributions come from terms with larger weights:\n$$\n\\lambda_{PDE}\\nabla \\mathcal{L}_{PDE}+\\lambda_{BC}\\nabla \\mathcal{L}_{BC}+\\lambda_{IC}\\nabla \\mathcal{L}_{IC}\\approx 0.\n$$\nIf $\\lambda_{PDE}\\ll \\lambda_{BC}$ and $\\lambda_{PDE}\\ll \\lambda_{IC}$, then reductions in $\\mathcal{L}_{BC}$ and $\\mathcal{L}_{IC}$ are prioritized over reductions in $\\mathcal{L}_{PDE}$; thus the model will match boundary and initial conditions well while tolerating a large PDE residual in the interior. This matches Model 1: excellent boundary/initial agreement but large interior residual $R_{1}(x,t)=\\left|\\frac{\\partial u_{1}}{\\partial t}-\\alpha \\frac{\\partial^{2}u_{1}}{\\partial x^{2}}\\right|$.\n\nConversely, if $\\lambda_{PDE}\\gg \\lambda_{BC}$ and $\\lambda_{PDE}\\gg \\lambda_{IC}$, the optimization strongly penalizes interior PDE violations, driving $\\mathcal{L}_{PDE}$ (and thus the residual $R_{2}(x,t)$) to be extremely small while allowing significant boundary and initial mismatches. This matches Model 2: $R_{2}(x,t)\\approx 0$ in the interior but poor agreement at $x=0$ and $x=1$ with $h_{0}(t)$ and $h_{1}(t)$.\n\nSince the problem states to treat $\\lambda_{BC}$ and $\\lambda_{IC}$ as comparable and refer collectively to their effect, the qualitative conclusions reduce to\n$$\n\\text{Model 1: }\\lambda_{PDE}\\ll \\lambda_{BC},\\qquad \\text{Model 2: }\\lambda_{PDE}\\gg \\lambda_{BC}.\n$$\nAmong the options, this is exactly option A. Options B, C, and D invert or weaken the necessary dominance relations, and E contradicts the systematic, opposing behaviors observed across the two models, which are better explained by different weightings rather than random initialization.",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}