{
    "hands_on_practices": [
        {
            "introduction": "Gaussian Processes (GPs) are a cornerstone of modern surrogate modeling, offering a flexible, non-parametric approach to emulating complex functions. This first practice provides a foundational, hands-on implementation of a GP emulator. You will move from the theoretical formulas to practical code, calculating the predictive mean and variance for a given environmental response, and in doing so, gain a concrete understanding of how a GP interpolates data and quantifies its own uncertainty . The exercise extends this by introducing a key concept in active learning: you will strategically select a new data point to maximally reduce the emulator's uncertainty, demonstrating how these models can be used to guide efficient data acquisition.",
            "id": "3891179",
            "problem": "Consider a one-dimensional Gaussian Process (GP) emulator used as a surrogate model of a smooth environmental response function. A Gaussian Process (GP) is defined by a mean function and a covariance function. Assume a zero mean prior and a squared exponential covariance function given by $$k(x,x') = \\sigma_f^2 \\exp\\left(-\\frac{(x-x')^2}{2\\ell^2}\\right),$$ where $\\sigma_f^2$ is the signal variance and $\\ell$ is the characteristic length scale. Observations are subject to independent, identically distributed Gaussian noise of variance $\\sigma_n^2$. For a given training dataset $(\\mathbf{X}, \\mathbf{y})$ with $\\mathbf{X} \\in \\mathbb{R}^n$ and $\\mathbf{y} \\in \\mathbb{R}^n$, the joint distribution of the training outputs and the function value at a test location $x^\\star$ is multivariate normal. Therefore, the predictive distribution at $x^\\star$ can be obtained by conditioning properties of multivariate normal distributions.\n\nYour task is to implement a program that:\n- Computes the GP predictive mean and variance at a specified test location $x^\\star$ for a given small dataset and hyperparameters.\n- Quantifies the uncertainty reduction at $x^\\star$ when adding one additional observation at a strategically chosen location $x_{\\text{new}}$ selected from a finite candidate set so as to minimize the predictive variance at $x^\\star$ after augmentation. The new observation is assumed to have the same noise variance $\\sigma_n^2$ as the training data. The uncertainty reduction is defined as the nonnegative difference between the predictive variance at $x^\\star$ before and after adding the new observation.\n\nUse only the foundations of multivariate normal conditioning, the squared exponential covariance function, and independent Gaussian observation noise. All quantities are dimensionless.\n\nFor each test case below, compute:\n- The predictive mean at $x^\\star$, denoted by $\\mu(x^\\star)$, as a real number.\n- The predictive variance at $x^\\star$, denoted by $\\sigma^2(x^\\star)$, as a nonnegative real number.\n- The uncertainty reduction at $x^\\star$ from adding the optimally chosen single new observation (selected from the provided candidate set), denoted by $\\Delta\\sigma^2(x^\\star) = \\sigma^2(x^\\star) - \\sigma^2_{\\text{aug}}(x^\\star)$, where $\\sigma^2_{\\text{aug}}(x^\\star)$ is the predictive variance at $x^\\star$ after augmenting the dataset with that single observation.\n\nTest suite (each test case is specified as $(\\mathbf{X}, \\mathbf{y}, \\sigma_f, \\ell, \\sigma_n, x^\\star, \\mathcal{C})$):\n1. $(\\mathbf{X}=[0.1,0.4,0.7],\\,\\mathbf{y}=[0.2,0.6,0.1],\\,\\sigma_f=1.0,\\,\\ell=0.2,\\,\\sigma_n=0.05,\\,x^\\star=0.5,\\,\\mathcal{C}=\\{0.0,0.1,0.2,\\ldots,1.0\\})$.\n2. $(\\mathbf{X}=[0.1,0.4,0.7],\\,\\mathbf{y}=[0.2,0.6,0.1],\\,\\sigma_f=1.0,\\,\\ell=0.05,\\,\\sigma_n=0.05,\\,x^\\star=0.5,\\,\\mathcal{C}=\\{0.0,0.1,0.2,\\ldots,1.0\\})$.\n3. $(\\mathbf{X}=[0.0,0.3,0.9],\\,\\mathbf{y}=[0.0,0.5,-0.2],\\,\\sigma_f=1.5,\\,\\ell=0.5,\\,\\sigma_n=0.01,\\,x^\\star=0.25,\\,\\mathcal{C}=\\{0.0,0.1,0.2,\\ldots,1.0\\})$.\n4. $(\\mathbf{X}=[0.1,0.4,0.7],\\,\\mathbf{y}=[0.2,0.6,0.1],\\,\\sigma_f=1.0,\\,\\ell=0.2,\\,\\sigma_n=0.10,\\,x^\\star=0.4,\\,\\mathcal{C}=\\{0.0,0.1,0.2,\\ldots,1.0\\})$.\n\nYour program must:\n- For each test case, compute $\\mu(x^\\star)$, $\\sigma^2(x^\\star)$, and $\\Delta\\sigma^2(x^\\star)$, where $x_{\\text{new}}$ is selected from $\\mathcal{C}$ to minimize the augmented predictive variance at $x^\\star$.\n- Use numerically stable linear algebra operations and ensure that predictive variances are clipped to be nonnegative if numerical round-off induces negative values.\n- Produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is an inner list of three decimal numbers $[\\mu(x^\\star),\\sigma^2(x^\\star),\\Delta\\sigma^2(x^\\star)]$ for each test case, with six digits after the decimal point, and no spaces. For example: \"[[0.123456,0.234567,0.001000],[...],...]\".",
            "solution": "The problem is well-posed, scientifically grounded, and contains all necessary information to proceed with a solution.\n\nThe objective is to compute the predictive mean and variance of a Gaussian Process (GP) emulator at a specific point $x^\\star$, and then to quantify the maximal uncertainty reduction at that same point by adding one new observation. The location of this new observation, $x_{\\text{new}}$, is chosen from a finite candidate set $\\mathcal{C}$ to greedily minimize the posterior predictive variance at $x^\\star$.\n\nA Gaussian Process defines a prior distribution over functions. A function $f(x)$ drawn from a GP is specified by a mean function $m(x)$ and a covariance function (or kernel) $k(x, x')$. In this problem, we are given a zero mean prior, $m(x) = 0$, and a squared exponential covariance function:\n$$k(x,x') = \\sigma_f^2 \\exp\\left(-\\frac{(x-x')^2}{2\\ell^2}\\right)$$\nHere, $\\sigma_f^2$ is the signal variance, which controls the overall amplitude of the function, and $\\ell$ is the characteristic length scale, which governs the smoothness or \"wiggliness\" of the function.\n\nWe are given a set of $n$ training observations $(\\mathbf{X}, \\mathbf{y})$, where $\\mathbf{X} = \\{x_1, \\dots, x_n\\}$ are the input locations and $\\mathbf{y} = \\{y_1, \\dots, y_n\\}$ are the corresponding noisy outputs. The observation model is given by $y_i = f(x_i) + \\epsilon_i$, where the noise $\\epsilon_i$ is independent and identically distributed as $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma_n^2)$.\n\nThe core of GP regression lies in conditioning a joint multivariate normal distribution. The joint distribution of the latent function values at the training points, $\\mathbf{f} = [f(x_1), \\dots, f(x_n)]^T$, and the latent function value at a new test point $x^\\star$, denoted $f^\\star = f(x^\\star)$, is given by:\n$$\n\\begin{pmatrix} \\mathbf{f} \\\\ f^\\star \\end{pmatrix} \\sim \\mathcal{N}\n\\left(\n\\mathbf{0},\n\\begin{pmatrix}\nK(\\mathbf{X}, \\mathbf{X}) & K(\\mathbf{X}, x^\\star) \\\\\nK(x^\\star, \\mathbf{X}) & k(x^\\star, x^\\star)\n\\end{pmatrix}\n\\right)\n$$\nwhere $K(\\mathbf{X}, \\mathbf{X})$ is the $n \\times n$ covariance matrix with entries $[K(\\mathbf{X}, \\mathbf{X})]_{ij} = k(x_i, x_j)$, $K(\\mathbf{X}, x^\\star)$ is the $n \\times 1$ vector of covariances with entries $[K(\\mathbf{X}, x^\\star)]_i = k(x_i, x^\\star)$, and $k(x^\\star, x^\\star)$ is the prior variance at the test point.\n\nThe observed training outputs $\\mathbf{y}$ relate to the latent values $\\mathbf{f}$ via $\\mathbf{y} \\sim \\mathcal{N}(\\mathbf{f}, \\sigma_n^2 \\mathbf{I})$. By integrating out the latent variables $\\mathbf{f}$, we find the joint distribution of the observed outputs $\\mathbf{y}$ and the latent test value $f^\\star$:\n$$\n\\begin{pmatrix} \\mathbf{y} \\\\ f^\\star \\end{pmatrix} \\sim \\mathcal{N}\n\\left(\n\\mathbf{0},\n\\begin{pmatrix}\nK(\\mathbf{X}, \\mathbf{X}) + \\sigma_n^2 \\mathbf{I} & K(\\mathbf{X}, x^\\star) \\\\\nK(x^\\star, \\mathbf{X}) & k(x^\\star, x^\\star)\n\\end{pmatrix}\n\\right)\n$$\nwhere $\\mathbf{I}$ is the $n \\times n$ identity matrix.\n\nUsing the standard rules for conditioning a multivariate normal distribution, the conditional (posterior) distribution $p(f^\\star | \\mathbf{X}, \\mathbf{y}, x^\\star)$ is also a Gaussian, $p(f^\\star | \\mathbf{X}, \\mathbf{y}, x^\\star) = \\mathcal{N}(\\mu(x^\\star), \\sigma^2(x^\\star))$, with mean and variance given by:\n$$ \\mu(x^\\star) = K(x^\\star, \\mathbf{X}) [K(\\mathbf{X}, \\mathbf{X}) + \\sigma_n^2 \\mathbf{I}]^{-1} \\mathbf{y} $$\n$$ \\sigma^2(x^\\star) = k(x^\\star, x^\\star) - K(x^\\star, \\mathbf{X}) [K(\\mathbf{X}, \\mathbf{X}) + \\sigma_n^2 \\mathbf{I}]^{-1} K(\\mathbf{X}, x^\\star) $$\nFor numerical stability, the matrix inversion $[K(\\mathbf{X}, \\mathbf{X}) + \\sigma_n^2 \\mathbf{I}]^{-1}$ is not performed explicitly. Instead, we solve a system of linear equations. Let $K_y = K(\\mathbf{X}, \\mathbf{X}) + \\sigma_n^2 \\mathbf{I}$. This matrix is symmetric positive definite. We can use Cholesky decomposition, $K_y = L L^T$, where $L$ is a lower triangular matrix. The computations become:\n1. Solve $L \\boldsymbol{\\alpha} = \\mathbf{y}$ for $\\boldsymbol{\\alpha}$ using forward substitution.\n2. Solve $L^T \\mathbf{w} = \\boldsymbol{\\alpha}$ for $\\mathbf{w}$ using backward substitution. Now $\\mathbf{w} = K_y^{-1} \\mathbf{y}$.\n3. The mean is $\\mu(x^\\star) = K(x^\\star, \\mathbf{X}) \\mathbf{w}$.\n4. Solve $L \\mathbf{v} = K(\\mathbf{X}, x^\\star)$ for $\\mathbf{v}$ using forward substitution.\n5. The variance is $\\sigma^2(x^\\star) = k(x^\\star, x^\\star) - \\mathbf{v}^T \\mathbf{v}$.\n\nThe procedure for each test case is as follows:\n\n1.  **Compute Initial Prediction**: Given the initial training set $(\\mathbf{X}, \\mathbf{y})$ and hyperparameters $(\\sigma_f, \\ell, \\sigma_n)$, calculate the initial predictive mean $\\mu(x^\\star)$ and variance $\\sigma^2(x^\\star)$ at the test point $x^\\star$ using the formulas above.\n\n2.  **Find Optimal Augmentation**: To find the optimal new observation point $x_{\\text{new}}$ from the candidate set $\\mathcal{C}$, we iterate through each candidate $x_c \\in \\mathcal{C}$. For each $x_c$, we form an augmented input set $\\mathbf{X}_{\\text{aug}} = \\mathbf{X} \\cup \\{x_c\\}$. The predictive variance $\\sigma^2(x^\\star)$ depends only on the input locations, not the observed output values $\\mathbf{y}$. Thus, we can calculate the hypothetical posterior variance at $x^\\star$, denoted $\\sigma^2_{\\text{aug}}(x^\\star; x_c)$, that would result from augmenting the dataset with an observation at $x_c$. This is computed using the same variance formula, but with $\\mathbf{X}_{\\text{aug}}$ instead of $\\mathbf{X}$:\n    $$ \\sigma^2_{\\text{aug}}(x^\\star; x_c) = k(x^\\star, x^\\star) - K(x^\\star, \\mathbf{X}_{\\text{aug}}) [K(\\mathbf{X}_{\\text{aug}}, \\mathbf{X}_{\\text{aug}}) + \\sigma_n^2 \\mathbf{I}]^{-1} K(\\mathbf{X}_{\\text{aug}}, x^\\star) $$\n    We then identify the minimal possible augmented variance, $\\sigma^2_{\\text{min,aug}}(x^\\star) = \\min_{x_c \\in \\mathcal{C}} \\sigma^2_{\\text{aug}}(x^\\star; x_c)$.\n\n3.  **Calculate Uncertainty Reduction**: The uncertainty reduction is the difference between the initial variance and the minimal achievable variance after augmentation:\n    $$ \\Delta\\sigma^2(x^\\star) = \\sigma^2(x^\\star) - \\sigma^2_{\\text{min,aug}}(x^\\star) $$\n    By construction, adding data can only decrease (or maintain) the posterior variance, so $\\Delta\\sigma^2(x^\\star) \\ge 0$. Any small negative values arising from numerical floating-point inaccuracies will be clipped to $0$.\n\nThis entire process is repeated for each test case to produce the required triplet of values: $[\\mu(x^\\star), \\sigma^2(x^\\star), \\Delta\\sigma^2(x^\\star)]$.",
            "answer": "```python\nimport numpy as np\nfrom scipy import linalg\n\ndef solve():\n    \"\"\"\n    Solves the Gaussian Process regression problem for all specified test cases.\n    \"\"\"\n    test_cases = [\n        # (X, y, sigma_f, l, sigma_n, x_star, C)\n        ([0.1, 0.4, 0.7], [0.2, 0.6, 0.1], 1.0, 0.2, 0.05, 0.5, np.linspace(0.0, 1.0, 11)),\n        ([0.1, 0.4, 0.7], [0.2, 0.6, 0.1], 1.0, 0.05, 0.05, 0.5, np.linspace(0.0, 1.0, 11)),\n        ([0.0, 0.3, 0.9], [0.0, 0.5, -0.2], 1.5, 0.5, 0.01, 0.25, np.linspace(0.0, 1.0, 11)),\n        ([0.1, 0.4, 0.7], [0.2, 0.6, 0.1], 1.0, 0.2, 0.10, 0.4, np.linspace(0.0, 1.0, 11)),\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        X, y, sigma_f, l, sigma_n, x_star, C = case\n        \n        # Convert inputs to numpy arrays\n        X = np.asarray(X)\n        y = np.asarray(y)\n        C = np.asarray(C)\n\n        def squared_exp_kernel(x1, x2, sf, sl):\n            \"\"\"\n            Computes the squared exponential kernel between two sets of 1D points.\n            \"\"\"\n            x1 = x1.reshape(-1, 1)\n            x2 = x2.reshape(-1, 1)\n            # Use broadcasting to compute squared Euclidean distance matrix\n            sqdist = np.sum(x1**2, 1).reshape(-1, 1) + np.sum(x2**2, 1) - 2 * np.dot(x1, x2.T)\n            return (sf**2) * np.exp(-0.5 * sqdist / (sl**2))\n\n        def get_pred_mean_and_var(x_train, y_train, x_test, sf, sl, sn):\n            \"\"\"\n            Computes the GP predictive mean and variance at x_test.\n            \"\"\"\n            n = len(x_train)\n            K = squared_exp_kernel(x_train, x_train, sf, sl)\n            Ky = K + (sn**2) * np.eye(n)\n            \n            k_star = squared_exp_kernel(x_train, x_test, sf, sl)\n            k_star_star = squared_exp_kernel(x_test, x_test, sf, sl)[0, 0]\n\n            try:\n                # Use Cholesky decomposition for stable computation\n                L, lower = linalg.cho_factor(Ky, lower=True, overwrite_a=False)\n                \n                # Compute mean: k_star.T @ inv(Ky) @ y\n                alpha = linalg.cho_solve((L, lower), y_train)\n                mean = k_star.T @ alpha\n                \n                # Compute variance: k_star_star - k_star.T @ inv(Ky) @ k_star\n                v = linalg.cho_solve((L, lower), k_star)\n                var = k_star_star - k_star.T @ v\n\n            except linalg.LinAlgError:\n                # Fallback in case of numerical instability, though unlikely here\n                inv_Ky = linalg.inv(Ky)\n                mean = k_star.T @ inv_Ky @ y_train\n                var = k_star_star - k_star.T @ inv_Ky @ k_star\n\n            # Ensure variance is non-negative due to potential floating point errors\n            var = np.maximum(0, var.item())\n            \n            return mean.item(), var\n\n        def get_pred_var(x_train, x_test, sf, sl, sn):\n            \"\"\"\n            Computes only the GP predictive variance.\n            \"\"\"\n            n = len(x_train)\n            K = squared_exp_kernel(x_train, x_train, sf, sl)\n            Ky = K + (sn**2) * np.eye(n)\n            \n            k_star = squared_exp_kernel(x_train, x_test, sf, sl)\n            k_star_star = squared_exp_kernel(x_test, x_test, sf, sl)[0, 0]\n\n            try:\n                L, lower = linalg.cho_factor(Ky, lower=True, overwrite_a=False)\n                v = linalg.cho_solve((L, lower), k_star)\n                var = k_star_star - k_star.T @ v\n            except linalg.LinAlgError:\n                inv_Ky = linalg.inv(Ky)\n                var = k_star_star - k_star.T @ inv_Ky @ k_star\n\n            var = np.maximum(0, var.item())\n            return var\n\n        # Step 1: Compute initial predictive mean and variance\n        x_star_np = np.array([x_star])\n        initial_mean, initial_var = get_pred_mean_and_var(X, y, x_star_np, sigma_f, l, sigma_n)\n\n        # Step 2: Find optimal new point and minimal augmented variance\n        min_aug_var = float('inf')\n        for x_new in C:\n            X_aug = np.append(X, x_new)\n            # Sorting might help stability but is not strictly necessary\n            # X_aug = np.unique(np.sort(X_aug))\n            var_aug = get_pred_var(X_aug, x_star_np, sigma_f, l, sigma_n)\n            if var_aug  min_aug_var:\n                min_aug_var = var_aug\n        \n        # Step 3: Compute uncertainty reduction\n        uncertainty_reduction = initial_var - min_aug_var\n        uncertainty_reduction = max(0.0, uncertainty_reduction)\n\n        results.append([initial_mean, initial_var, uncertainty_reduction])\n\n    # Format output string\n    formatted_results = []\n    for res in results:\n        formatted_results.append(f\"[{res[0]:.6f},{res[1]:.6f},{res[2]:.6f}]\")\n    \n    final_output_string = f\"[{','.join(formatted_results)}]\"\n    print(final_output_string)\n\nsolve()\n```"
        },
        {
            "introduction": "While Gaussian Processes offer non-parametric flexibility, Polynomial Chaos Expansions (PCEs) provide a powerful parametric framework, especially when input uncertainties are well-described by standard probability distributions. This practice shifts our focus to building a PCE surrogate from scratch using a regression-based approach . You will construct the appropriate multivariate Hermite polynomial basis and use ordinary least squares to estimate the expansion coefficients. This exercise emphasizes the practical challenges of fitting a PCE, prompting you to analyze how the number of training samples and the conditioning of the problem affect the variance and reliability of the resulting emulator coefficients.",
            "id": "3891168",
            "problem": "You are asked to implement a Polynomial Chaos Expansion (PCE) emulator and quantify estimator uncertainty in a controlled setting relevant to environmental and earth system modeling. Consider a vector of uncertain, standardized environmental drivers $\\mathbf{Z} = (Z_1,\\dots,Z_d)$ where each $Z_i$ is independent and identically distributed as standard normal, $Z_i \\sim \\mathcal{N}(0,1)$. The surrogate response represents a dimensionless environmental quantity and is defined by a deterministic mapping $g:\\mathbb{R}^d \\to \\mathbb{R}$ given by\n$$\ng(\\mathbf{z}) = a_0 + a_1 z_1 + a_2 z_2 + a_3 z_3 + a_{12} z_1 z_2 + a_{23} z_2 z_3 + b_1 \\tanh(c_1 z_1 + c_2 z_2) + d_1 z_1^2 + d_2 z_2^3 + e_1 z_3 z_4 + e_2 z_4^2 + e_3 z_5^3,\n$$\nwhere coefficients are fixed as $a_0=1$, $a_1=0.5$, $a_2=0.25$, $a_3=-0.15$, $a_{12}=0.2$, $a_{23}=-0.1$, $b_1=0.2$, $c_1=0.5$, $c_2=0.3$, $d_1=0.1$, $d_2=0.05$, $e_1=0.07$, $e_2=0.03$, $e_3=-0.02$. When $d  4$ or $d  5$, define $z_4=0$ or $z_5=0$ respectively so that terms involving $z_4$ or $z_5$ vanish consistently.\n\nYour task is to construct a Polynomial Chaos Expansion (PCE) surrogate for $g(\\mathbf{Z})$ using multivariate Hermite basis functions orthonormal with respect to the standard normal measure. Let $\\mathrm{He}_k(x)$ denote the probabilists’ Hermite polynomial of degree $k$ so that $\\mathrm{He}_0(x)=1$, $\\mathrm{He}_1(x)=x$, and the recursion $\\,\\mathrm{He}_{k+1}(x)=x\\,\\mathrm{He}_k(x)-k\\,\\mathrm{He}_{k-1}(x)\\,$ holds. The univariate orthonormal basis functions are\n$$\n\\phi_k(x) = \\frac{\\mathrm{He}_k(x)}{\\sqrt{k!}},\n$$\nand the multivariate basis indexed by the multi-index $\\boldsymbol{\\alpha}=(\\alpha_1,\\dots,\\alpha_d)$ is\n$$\n\\Phi_{\\boldsymbol{\\alpha}}(\\mathbf{z}) = \\prod_{i=1}^d \\phi_{\\alpha_i}(z_i).\n$$\nLet the total-order truncation set be\n$$\n\\mathcal{A}_{d,p} = \\left\\{ \\boldsymbol{\\alpha}\\in\\mathbb{N}_0^d : \\sum_{i=1}^d \\alpha_i \\le p \\right\\},\n$$\nwhose cardinality is $M=\\binom{d+p}{p}$.\n\nGiven a training sample $\\{\\mathbf{z}^{(j)}\\}_{j=1}^n$ drawn independently from $\\mathcal{N}(\\mathbf{0},\\mathbf{I}_d)$ and the corresponding responses $y^{(j)}=g(\\mathbf{z}^{(j)})$, fit the PCE coefficients $\\mathbf{c}\\in\\mathbb{R}^M$ by ordinary least squares regression onto the basis $\\{\\Phi_{\\boldsymbol{\\alpha}}\\}_{\\boldsymbol{\\alpha}\\in\\mathcal{A}_{d,p}}$. Form the design matrix $\\mathbf{X}\\in\\mathbb{R}^{n\\times M}$ with entries\n$$\nX_{j,m} = \\Phi_{\\boldsymbol{\\alpha}^{(m)}}\\!\\left(\\mathbf{z}^{(j)}\\right),\n$$\nwhere $\\boldsymbol{\\alpha}^{(m)}$ enumerates $\\mathcal{A}_{d,p}$. The ordinary least squares estimator solves\n$$\n\\widehat{\\mathbf{c}} = \\arg\\min_{\\mathbf{c}\\in\\mathbb{R}^M} \\left\\| \\mathbf{X}\\mathbf{c} - \\mathbf{y}\\right\\|_2^2,\n$$\nwhere $\\mathbf{y}=(y^{(1)},\\dots,y^{(n)})^\\top$. Under the Gauss–Markov framework for linear regression with random design and mean-zero residuals, an estimator for the residual variance is\n$$\n\\widehat{\\sigma}^2 = \\frac{\\mathrm{RSS}}{\\max(n - M, 1)}, \\quad \\mathrm{RSS} = \\left\\| \\mathbf{y} - \\mathbf{X}\\widehat{\\mathbf{c}}\\right\\|_2^2,\n$$\nand an estimator for the coefficient covariance matrix is\n$$\n\\widehat{\\mathrm{Cov}}(\\widehat{\\mathbf{c}}) = \\widehat{\\sigma}^2\\left(\\mathbf{X}^\\top \\mathbf{X}\\right)^{\\dagger},\n$$\nwhere $\\left(\\cdot\\right)^{\\dagger}$ denotes the Moore–Penrose pseudoinverse. Define the average estimator variance as\n$$\n\\overline{v} = \\frac{1}{M}\\sum_{m=1}^M \\left[ \\widehat{\\mathrm{Cov}}(\\widehat{\\mathbf{c}}) \\right]_{m,m},\n$$\nthe maximum estimator variance as\n$$\nv_{\\max} = \\max_{1\\le m \\le M} \\left[ \\widehat{\\mathrm{Cov}}(\\widehat{\\mathbf{c}}) \\right]_{m,m},\n$$\nand the spectral condition number of $\\mathbf{X}^\\top \\mathbf{X}$ as\n$$\n\\kappa = \\|\\mathbf{X}^\\top \\mathbf{X}\\|_2 \\cdot \\left\\| \\left(\\mathbf{X}^\\top \\mathbf{X}\\right)^{-1} \\right\\|_2,\n$$\ncomputed via the $2$-norm.\n\nImplement a complete program that:\n- Generates training samples $\\mathbf{z}^{(j)}$ using the independent standard normal law with a fixed seed $s=314159$ for reproducibility (use the same seed across all test cases, but you may vary the generator state deterministically by an offset per test case).\n- Constructs the multivariate orthonormal Hermite basis up to total order $p$.\n- Builds the design matrix $\\mathbf{X}$ and fits $\\widehat{\\mathbf{c}}$ using ordinary least squares.\n- Computes $\\overline{v}$, $v_{\\max}$, and $\\kappa$ using the formulas above.\n\nTest suite:\n- Case $1$: $(d,p,n)=(3,2,10)$, where $M=\\binom{3+2}{2}=10$ is equal to $n$ (a square design).\n- Case $2$: $(d,p,n)=(3,2,50)$, where $n=5M$ (oversampled).\n- Case $3$: $(d,p,n)=(3,3,40)$, where $M=\\binom{3+3}{3}=20$ and $n=2M$.\n- Case $4$: $(d,p,n)=(5,2,63)$, where $M=\\binom{5+2}{2}=21$ and $n=3M$.\n- Case $5$: $(d,p,n)=(5,3,60)$, where $M=\\binom{5+3}{3}=56$ and $n$ is slightly above $M$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes one inner list of three floats in the order $[\\overline{v}, v_{\\max}, \\kappa]$. For example, the output format must be exactly like\n$[ [\\overline{v}_1, v_{\\max,1}, \\kappa_1], [\\overline{v}_2, v_{\\max,2}, \\kappa_2], \\dots ]$,\nwith no additional text.",
            "solution": "The problem requires the construction of a Polynomial Chaos Expansion (PCE) surrogate for a given deterministic function $g(\\mathbf{z})$ of uncertain inputs $\\mathbf{Z} = (Z_1, \\dots, Z_d)$, where each $Z_i$ is an independent standard normal random variable, $Z_i \\sim \\mathcal{N}(0,1)$. The task further involves quantifying the uncertainty of the PCE coefficient estimators. The problem is well-posed, scientifically sound, and provides all necessary information for a unique, reproducible solution. We will proceed by implementing the specified procedure step-by-step.\n\nThe core of the task is to approximate the complex model $g(\\mathbf{Z})$, which represents some environmental quantity, with a simpler polynomial model. This surrogate, the PCE, is given by a truncated series expansion:\n$$\n\\widehat{g}(\\mathbf{Z}) = \\sum_{\\boldsymbol{\\alpha} \\in \\mathcal{A}_{d,p}} c_{\\boldsymbol{\\alpha}} \\Phi_{\\boldsymbol{\\alpha}}(\\mathbf{Z})\n$$\nwhere $\\{c_{\\boldsymbol{\\alpha}}\\}$ are the coefficients to be determined, $\\{\\Phi_{\\boldsymbol{\\alpha}}\\}$ are multivariate polynomial basis functions, and $\\mathcal{A}_{d,p}$ is a truncation set of multi-indices.\n\nThe procedural steps are as follows:\n\n1.  **Basis Function Definition**: The inputs $Z_i$ are standard normal variables. The natural choice for orthogonal polynomials with respect to the standard normal probability measure is the probabilists' Hermite polynomials, denoted $\\mathrm{He}_k(x)$. The problem specifies their recursive definition:\n    $$\n    \\mathrm{He}_0(x) = 1 \\\\\n    \\mathrm{He}_1(x) = x \\\\\n    \\mathrm{He}_{k+1}(x) = x\\,\\mathrm{He}_k(x) - k\\,\\mathrm{He}_{k-1}(x)\n    $$\n    These polynomials are orthogonal, but not orthonormal. The corresponding orthonormal basis functions $\\phi_k(x)$ are defined as:\n    $$\n    \\phi_k(x) = \\frac{\\mathrm{He}_k(x)}{\\sqrt{k!}}\n    $$\n    such that $\\mathbb{E}[\\phi_j(Z) \\phi_k(Z)] = \\delta_{jk}$ when $Z \\sim \\mathcal{N}(0,1)$. For a $d$-dimensional input vector $\\mathbf{z}=(z_1, \\dots, z_d)$, the multivariate basis functions $\\Phi_{\\boldsymbol{\\alpha}}(\\mathbf{z})$ are formed by the tensor product of the univariate functions, indexed by a multi-index $\\boldsymbol{\\alpha}=(\\alpha_1, \\dots, \\alpha_d) \\in \\mathbb{N}_0^d$:\n    $$\n    \\Phi_{\\boldsymbol{\\alpha}}(\\mathbf{z}) = \\prod_{i=1}^d \\phi_{\\alpha_i}(z_i)\n    $$\n    We will implement a function to evaluate $\\mathrm{He}_k(x)$ for multiple values of $x$ and degrees $k$ simultaneously, and another function to generate the required factorials $\\sqrt{k!}$ for normalization.\n\n2.  **Truncation Set Generation**: The infinite PCE sum must be truncated for practical computation. The problem specifies a total-order truncation scheme, where the set of active multi-indices $\\mathcal{A}_{d,p}$ includes all $\\boldsymbol{\\alpha}$ whose components sum to at most a total degree $p$:\n    $$\n    \\mathcal{A}_{d,p} = \\left\\{ \\boldsymbol{\\alpha} \\in \\mathbb{N}_0^d : \\sum_{i=1}^d \\alpha_i \\le p \\right\\}\n    $$\n    The number of basis functions, $M$, is the cardinality of this set, given by $M = |\\mathcal{A}_{d,p}| = \\binom{d+p}{p}$. We will implement a recursive algorithm to generate these multi-indices for given parameters $d$ and $p$.\n\n3.  **Data Generation**: To fit the PCE coefficients, we require a training dataset. This consists of $n$ samples of the input vector, $\\{\\mathbf{z}^{(j)}\\}_{j=1}^n$, drawn from the specified distribution $\\mathcal{N}(\\mathbf{0}, \\mathbf{I}_d)$, and the corresponding model responses, $y^{(j)} = g(\\mathbf{z}^{(j)})$. The function $g(\\mathbf{z})$ is provided:\n    $$\n    g(\\mathbf{z}) = 1 + 0.5 z_1 + 0.25 z_2 - 0.15 z_3 + 0.2 z_1 z_2 - 0.1 z_2 z_3 + 0.2 \\tanh(0.5 z_1 + 0.3 z_2) + 0.1 z_1^2 + 0.05 z_2^3 + 0.07 z_3 z_4 + 0.03 z_4^2 - 0.02 z_5^3\n    $$\n    with the rule that $z_i=0$ for $i  d$. We will use a seeded pseudo-random number generator for reproducibility, with a deterministic offset for each test case as per the problem description.\n\n4.  **Least-Squares Regression**: The coefficients $\\mathbf{c} = (c_{\\boldsymbol{\\alpha}^{(1)}}, \\dots, c_{\\boldsymbol{\\alpha}^{(M)}})^\\top$ are estimated by minimizing the squared error between the PCE surrogate and the true model outputs on the training set. This is an ordinary least squares (OLS) problem:\n    $$\n    \\widehat{\\mathbf{c}} = \\arg\\min_{\\mathbf{c}\\in\\mathbb{R}^M} \\left\\| \\mathbf{X}\\mathbf{c} - \\mathbf{y}\\right\\|_2^2\n    $$\n    Here, $\\mathbf{y} = (y^{(1)}, \\dots, y^{(n)})^\\top$ is the vector of model responses, and $\\mathbf{X}$ is the $n \\times M$ design matrix. Each entry $X_{j,m}$ of the design matrix is the value of the $m$-th basis function $\\Phi_{\\boldsymbol{\\alpha}^{(m)}}$ evaluated at the $j$-th input sample $\\mathbf{z}^{(j)}$:\n    $$\n    X_{j,m} = \\Phi_{\\boldsymbol{\\alpha}^{(m)}}\\!\\left(\\mathbf{z}^{(j)}\\right)\n    $$\n    We will construct this matrix and solve for $\\widehat{\\mathbf{c}}$ using a standard numerical linear algebra solver, `numpy.linalg.lstsq`, which is robust and suitable for this task.\n\n5.  **Estimator Uncertainty Analysis**: The final step is to quantify the statistical uncertainty of the estimated coefficients $\\widehat{\\mathbf{c}}$. This uncertainty arises from the finite sample size $n$ and model inadequacy (i.e., the truncation error of the PCE). The required metrics are derived from the covariance matrix of the estimator, $\\widehat{\\mathrm{Cov}}(\\widehat{\\mathbf{c}})$.\n    -   First, we compute the residual sum of squares: $\\mathrm{RSS} = \\left\\| \\mathbf{y} - \\mathbf{X}\\widehat{\\mathbf{c}}\\right\\|_2^2$.\n    -   Next, we estimate the residual variance: $\\widehat{\\sigma}^2 = \\frac{\\mathrm{RSS}}{\\max(n - M, 1)}$. The denominator $\\max(n - M, 1)$ correctly handles the degrees of freedom, including the edge case $n \\le M$.\n    -   The estimator for the coefficient covariance matrix is then $\\widehat{\\mathrm{Cov}}(\\widehat{\\mathbf{c}}) = \\widehat{\\sigma}^2\\left(\\mathbf{X}^\\top \\mathbf{X}\\right)^{\\dagger}$, where $(\\cdot)^{\\dagger}$ denotes the Moore-Penrose pseudoinverse. We use the pseudoinverse for numerical stability and correctness, especially if $\\mathbf{X}^\\top \\mathbf{X}$ is singular or ill-conditioned.\n    -   From this covariance matrix, we compute two metrics:\n        -   The average estimator variance: $\\overline{v} = \\frac{1}{M}\\mathrm{Tr}\\left(\\widehat{\\mathrm{Cov}}(\\widehat{\\mathbf{c}})\\right) = \\frac{1}{M}\\sum_{m=1}^M \\left[ \\widehat{\\mathrm{Cov}}(\\widehat{\\mathbf{c}}) \\right]_{m,m}$.\n        -   The maximum estimator variance: $v_{\\max} = \\max_{1\\le m \\le M} \\left[ \\widehat{\\mathrm{Cov}}(\\widehat{\\mathbf{c}}) \\right]_{m,m}$.\n    -   Finally, we assess the conditioning of the regression problem by computing the spectral condition number of the Gram matrix: $\\kappa = \\mathrm{cond}(\\mathbf{X}^\\top \\mathbf{X}) = \\|\\mathbf{X}^\\top \\mathbf{X}\\|_2 \\cdot \\left\\| \\left(\\mathbf{X}^\\top \\mathbf{X}\\right)^{\\dagger} \\right\\|_2$. A high value of $\\kappa$ indicates that the design matrix is nearly collinear, which can inflate the variance of the coefficient estimates.\n\nThese steps will be encapsulated in a program that iterates through the five specified test cases, computing $[\\overline{v}, v_{\\max}, \\kappa]$ for each, and formats the output as a single-line list of lists.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import factorial\n\ndef solve():\n    \"\"\"\n    Main function to execute the Polynomial Chaos Expansion analysis for all test cases.\n    \"\"\"\n\n    # Coefficients for the g-function as defined in the problem.\n    COEFFS = {\n        'a0': 1.0, 'a1': 0.5, 'a2': 0.25, 'a3': -0.15,\n        'a12': 0.2, 'a23': -0.1,\n        'b1': 0.2, 'c1': 0.5, 'c2': 0.3,\n        'd1': 0.1, 'd2': 0.05,\n        'e1': 0.07, 'e2': 0.03, 'e3': -0.02\n    }\n    SEED = 314159\n\n    def g_model(z):\n        \"\"\"\n        Computes the deterministic response function g(z).\n        z is expected to be an array of shape (n_samples, d).\n        \"\"\"\n        n_samples, d = z.shape\n        # Pad with zeros to handle dimensions up to 5 consistently.\n        z_pad = np.zeros((n_samples, 5))\n        z_pad[:, :d] = z\n\n        z1, z2, z3, z4, z5 = z_pad.T\n        c = COEFFS\n\n        term_a = c['a0'] + c['a1'] * z1 + c['a2'] * z2 + c['a3'] * z3\n        term_a12_23 = c['a12'] * z1 * z2 + c['a23'] * z2 * z3\n        term_b = c['b1'] * np.tanh(c['c1'] * z1 + c['c2'] * z2)\n        term_d = c['d1'] * z1**2 + c['d2'] * z2**3\n        term_e = c['e1'] * z3 * z4 + c['e2'] * z4**2 + c['e3'] * z5**3\n\n        return term_a + term_a12_23 + term_b + term_d + term_e\n\n    def generate_multi_indices(d, p):\n        \"\"\"\n        Generates total-order multi-indices recursively.\n        \"\"\"\n        if d == 1:\n            return [[i] for i in range(p + 1)]\n        indices = []\n        for i in range(p + 1):\n            sub_indices = generate_multi_indices(d - 1, p - i)\n            for sub_index in sub_indices:\n                indices.append([i] + sub_index)\n        return indices\n\n    def hermite_poly_val(x, k_max):\n        \"\"\"\n        Evaluates probabilists' Hermite polynomials He_k(x) up to degree k_max.\n        x is a 1D array. Returns a matrix of shape (len(x), k_max+1).\n        \"\"\"\n        vals = np.zeros((len(x), k_max + 1))\n        if k_max >= 0:\n            vals[:, 0] = 1.0\n        if k_max >= 1:\n            vals[:, 1] = x\n        for k in range(1, k_max):\n            vals[:, k + 1] = x * vals[:, k] - k * vals[:, k - 1]\n        return vals\n\n    test_cases = [\n        (3, 2, 10),\n        (3, 2, 50),\n        (3, 3, 40),\n        (5, 2, 63),\n        (5, 3, 60),\n    ]\n\n    all_results = []\n    for i, (d, p, n) in enumerate(test_cases):\n        # Use seed with deterministic offset for each case for reproducibility.\n        rng = np.random.default_rng(SEED + i)\n\n        # 1. Generate multi-indices for the basis.\n        alphas = generate_multi_indices(d, p)\n        M = len(alphas)\n\n        # 2. Generate training data.\n        z_train = rng.standard_normal(size=(n, d))\n        y_train = g_model(z_train)\n\n        # 3. Construct the design matrix X.\n        X = np.ones((n, M))\n        \n        # Pre-compute univariate orthonormal basis function values.\n        max_deg = p\n        phi_vals = np.zeros((n, d, max_deg + 1))\n        fact_sqrt = np.sqrt(factorial(np.arange(max_deg + 1)))\n\n        for i_dim in range(d):\n            # hermite_poly_val returns shape (n, max_deg+1) for each dimension.\n            he_vals_dim = hermite_poly_val(z_train[:, i_dim], max_deg)\n            phi_vals[:, i_dim, :] = he_vals_dim / fact_sqrt\n\n        # Populate X using the tensor product structure.\n        for m, alpha in enumerate(alphas):\n            term_prod = np.ones(n)\n            for i_dim in range(d):\n                term_prod *= phi_vals[:, i_dim, alpha[i_dim]]\n            X[:, m] = term_prod\n            \n        # 4. Fit coefficients using Ordinary Least Squares.\n        c_hat, _, _, _ = np.linalg.lstsq(X, y_train, rcond=None)\n\n        # 5. Compute the required uncertainty quantification metrics.\n        y_pred = X @ c_hat\n        rss = np.sum((y_train - y_pred)**2)\n        \n        sigma2_hat = rss / max(n - M, 1)\n        \n        XtX = X.T @ X\n        XtX_pinv = np.linalg.pinv(XtX)\n        \n        cov_c_hat = sigma2_hat * XtX_pinv\n        \n        variances = np.diag(cov_c_hat)\n        v_bar = np.mean(variances)\n        v_max = np.max(variances)\n        \n        kappa = np.linalg.cond(XtX)\n        \n        all_results.append([v_bar, v_max, kappa])\n\n    # Format output as a single string line\n    outer_parts = []\n    for res in all_results:\n        inner_str = f\"[{','.join(map(str, res))}]\"\n        outer_parts.append(inner_str)\n    final_output = f\"[{','.join(outer_parts)}]\"\n    \n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "Having explored methods for building emulators, we now turn to their primary application in uncertainty quantification: propagating multiple, interacting sources of uncertainty through a model. In real-world scenarios, we must contend with both uncertainty in model inputs and the inherent predictive uncertainty of the emulator itself. This exercise guides you through the crucial task of decomposing and combining these uncertainties using the law of total variance . By deriving and implementing a solution from first principles, you will develop a robust methodology for computing a total credible interval for a quantity of interest, distinguishing it from a naive interval that ignores the emulator's contribution to uncertainty.",
            "id": "3891130",
            "problem": "You are modeling a scalar quantity of interest in environmental and earth system modeling, such as basin-averaged precipitation change, using a statistical surrogate for a computationally expensive simulator. The surrogate emulator is built from a Gaussian Process (GP) emulator and is used to predict the quantity of interest given a vector of uncertain inputs. You must compute a $0.95$ credible interval that accounts for both input uncertainty and emulator predictive uncertainty, and compare it to an interval that ignores emulator uncertainty.\n\nAssume the following hierarchical predictive model for the scalar output $Y$:\n1. The uncertain inputs are represented by a random vector $X \\in \\mathbb{R}^d$ with a multivariate normal distribution $X \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$.\n2. The emulator provides a predictive mean that is linear in $X$, given by $m(X) = \\beta_0 + \\boldsymbol{\\beta}^\\top X$, where $\\beta_0 \\in \\mathbb{R}$ and $\\boldsymbol{\\beta} \\in \\mathbb{R}^d$.\n3. The emulator predictive variance is modeled as an additive term $v_{\\mathrm{em}}(X) = \\gamma_0 + \\sum_{i=1}^d \\gamma_i X_i^2$, where $\\gamma_0 \\ge 0$ and $\\gamma_i \\ge 0$ for $i=1,\\dots,d$.\n\nLet $Y \\mid X \\sim \\mathcal{N}(m(X), v_{\\mathrm{em}}(X))$. The marginal predictive distribution of $Y$ integrates over the uncertainty in $X$ and combines uncertainty from both $X$ and the emulator. A $0.95$ credible interval for $Y$ that ignores emulator uncertainty is obtained by setting $v_{\\mathrm{em}}(X) = 0$.\n\nStarting only from the fundamental laws of total expectation and total variance and standard properties of the normal distribution, derive an algorithm to compute the $0.95$ credible interval endpoints for $Y$ in millimeters per day (mm/day), rounded to six decimal places, in the two cases: (a) including emulator uncertainty and (b) ignoring emulator uncertainty.\n\nYour program must implement the derivation and compute the intervals for the following test suite. In all cases, the unit of $Y$ is mm/day and all interval endpoints must be expressed in mm/day.\n\n- Test Case A (two-dimensional inputs, correlated, nonzero emulator uncertainty):\n  - $d = 2$,\n  - $\\boldsymbol{\\mu} = [1.0, 0.5]$,\n  - $\\boldsymbol{\\Sigma} = \\begin{bmatrix}0.09  0.03 \\\\ 0.03  0.04\\end{bmatrix}$,\n  - $\\beta_0 = 0.0$, $\\boldsymbol{\\beta} = [2.0, -1.5]$,\n  - $\\gamma_0 = 0.05$, $\\boldsymbol{\\gamma} = [0.02, 0.01]$.\n\n- Test Case B (same inputs, emulator uncertainty set to zero; boundary where intervals coincide if correctly derived):\n  - $d = 2$,\n  - $\\boldsymbol{\\mu} = [1.0, 0.5]$,\n  - $\\boldsymbol{\\Sigma} = \\begin{bmatrix}0.09  0.03 \\\\ 0.03  0.04\\end{bmatrix}$,\n  - $\\beta_0 = 0.0$, $\\boldsymbol{\\beta} = [2.0, -1.5]$,\n  - $\\gamma_0 = 0.0$, $\\boldsymbol{\\gamma} = [0.0, 0.0]$.\n\n- Test Case C (deterministic inputs, emulator variance constant; boundary where ignoring emulator uncertainty yields a degenerate interval):\n  - $d = 2$,\n  - $\\boldsymbol{\\mu} = [0.8, -0.4]$,\n  - $\\boldsymbol{\\Sigma} = \\begin{bmatrix}0.0  0.0 \\\\ 0.0  0.0\\end{bmatrix}$,\n  - $\\beta_0 = 0.3$, $\\boldsymbol{\\beta} = [1.2, 0.5]$,\n  - $\\gamma_0 = 0.2$, $\\boldsymbol{\\gamma} = [0.0, 0.0]$.\n\n- Test Case D (one-dimensional inputs, large emulator variance increasing with $X^2$):\n  - $d = 1$,\n  - $\\boldsymbol{\\mu} = [-0.2]$,\n  - $\\boldsymbol{\\Sigma} = \\begin{bmatrix}0.25\\end{bmatrix}$,\n  - $\\beta_0 = 1.0$, $\\boldsymbol{\\beta} = [1.0]$,\n  - $\\gamma_0 = 0.1$, $\\boldsymbol{\\gamma} = [0.5]$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test-case result is a sublist of four floats in the order $[L_{\\mathrm{with}}, U_{\\mathrm{with}}, L_{\\mathrm{ignore}}, U_{\\mathrm{ignore}}]$, all in mm/day and rounded to six decimal places. For example, the output should look like $[[a,b,c,d],[e,f,g,h],\\dots]$ with no spaces.",
            "solution": "The problem is valid as it is scientifically grounded in statistical theory, well-posed, objective, and provides a complete and consistent set of information for deriving a solution. The task is to compute a $0.95$ credible interval for a scalar quantity of interest, $Y$, based on a hierarchical predictive model. The marginal distribution of $Y$ must account for uncertainty in the model inputs, $X$, and uncertainty in the statistical emulator used to predict $Y$. We will first derive the marginal mean and variance of $Y$ and then use these moments to construct the credible interval, assuming the marginal distribution of $Y$ can be approximated by a normal distribution.\n\nThe model is defined by the following hierarchy:\n1.  Input uncertainty: The input vector $X \\in \\mathbb{R}^d$ follows a multivariate normal distribution, $X \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$.\n2.  Emulator model: The conditional distribution of the output $Y$ given the inputs $X$ is normal, $Y \\mid X \\sim \\mathcal{N}(m(X), v_{\\mathrm{em}}(X))$, where the predictive mean is $m(X) = \\beta_0 + \\boldsymbol{\\beta}^\\top X$ and the predictive variance is $v_{\\mathrm{em}}(X) = \\gamma_0 + \\sum_{i=1}^d \\gamma_i X_i^2$.\n\nOur objective is to find the parameters of the marginal distribution of $Y$, which is obtained by integrating out the input vector $X$. The credible interval is then constructed based on the marginal mean $\\mathbb{E}[Y]$ and the marginal variance $\\mathrm{Var}(Y)$.\n\n**Derivation of the Marginal Mean, $\\mathbb{E}[Y]$**\n\nWe apply the law of total expectation, which states that $\\mathbb{E}[Y] = \\mathbb{E}_X[\\mathbb{E}[Y \\mid X]]$.\nThe inner expectation is the mean of the conditional distribution of $Y$ given $X$, which is the emulator's mean function:\n$$\n\\mathbb{E}[Y \\mid X] = m(X) = \\beta_0 + \\boldsymbol{\\beta}^\\top X\n$$\nThe outer expectation is taken over the distribution of $X$. Since expectation is a linear operator:\n$$\n\\mathbb{E}[Y] = \\mathbb{E}_X[\\beta_0 + \\boldsymbol{\\beta}^\\top X] = \\beta_0 + \\boldsymbol{\\beta}^\\top \\mathbb{E}_X[X]\n$$\nGiven that $X \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$, we have $\\mathbb{E}_X[X] = \\boldsymbol{\\mu}$. Therefore, the marginal mean of $Y$ is:\n$$\n\\mu_Y = \\mathbb{E}[Y] = \\beta_0 + \\boldsymbol{\\beta}^\\top \\boldsymbol{\\mu}\n$$\nThis result is independent of the emulator variance $v_{\\mathrm{em}}(X)$ and thus holds for both cases considered in the problem.\n\n**Derivation of the Marginal Variance, $\\mathrm{Var}(Y)$**\n\nWe apply the law of total variance, which decomposes the total variance of $Y$ into two components:\n$$\n\\mathrm{Var}(Y) = \\mathbb{E}_X[\\mathrm{Var}(Y \\mid X)] + \\mathrm{Var}_X(\\mathbb{E}[Y \\mid X])\n$$\nLet us evaluate each term separately.\n\n1.  **First Term: $\\mathbb{E}_X[\\mathrm{Var}(Y \\mid X)]$**\n    This term represents the contribution from the emulator's predictive uncertainty, averaged over the distribution of the inputs $X$. The conditional variance of $Y$ given $X$ is given by the emulator's predictive variance function:\n    $$\n    \\mathrm{Var}(Y \\mid X) = v_{\\mathrm{em}}(X) = \\gamma_0 + \\sum_{i=1}^d \\gamma_i X_i^2\n    $$\n    We take the expectation of this expression with respect to $X$:\n    $$\n    \\mathbb{E}_X[\\mathrm{Var}(Y \\mid X)] = \\mathbb{E}_X\\left[\\gamma_0 + \\sum_{i=1}^d \\gamma_i X_i^2\\right] = \\gamma_0 + \\sum_{i=1}^d \\gamma_i \\mathbb{E}_X[X_i^2]\n    $$\n    For any random variable $Z$, we know that $\\mathbb{E}[Z^2] = \\mathrm{Var}(Z) + (\\mathbb{E}[Z])^2$. For the $i$-th component of the random vector $X$, we have $\\mathbb{E}[X_i] = \\mu_i$ (the $i$-th element of $\\boldsymbol{\\mu}$) and $\\mathrm{Var}(X_i) = \\Sigma_{ii}$ (the $i$-th diagonal element of $\\boldsymbol{\\Sigma}$).\n    Substituting this into our expression gives:\n    $$\n    \\mathbb{E}_X[X_i^2] = \\Sigma_{ii} + \\mu_i^2\n    $$\n    Therefore, the first term of the total variance is:\n    $$\n    \\mathbb{E}_X[\\mathrm{Var}(Y \\mid X)] = \\gamma_0 + \\sum_{i=1}^d \\gamma_i (\\Sigma_{ii} + \\mu_i^2)\n    $$\n\n2.  **Second Term: $\\mathrm{Var}_X(\\mathbb{E}[Y \\mid X])$**\n    This term represents the uncertainty propagated from the inputs $X$ through the emulator's mean function $m(X)$. We need to calculate the variance of $\\mathbb{E}[Y \\mid X] = m(X) = \\beta_0 + \\boldsymbol{\\beta}^\\top X$ with respect to $X$:\n    $$\n    \\mathrm{Var}_X(\\mathbb{E}[Y \\mid X]) = \\mathrm{Var}_X(\\beta_0 + \\boldsymbol{\\beta}^\\top X) = \\mathrm{Var}_X(\\boldsymbol{\\beta}^\\top X)\n    $$\n    For a random vector $X$ with covariance matrix $\\boldsymbol{\\Sigma}$, the variance of a linear transformation $\\boldsymbol{\\beta}^\\top X$ is given by the quadratic form $\\boldsymbol{\\beta}^\\top \\boldsymbol{\\Sigma} \\boldsymbol{\\beta}$.\n    Thus, the second term is:\n    $$\n    \\mathrm{Var}_X(\\mathbb{E}[Y \\mid X]) = \\boldsymbol{\\beta}^\\top \\boldsymbol{\\Sigma} \\boldsymbol{\\beta}\n    $$\n\n**Total Marginal Variance and Interval Construction**\n\nCombining the two terms, the total marginal variance of $Y$ is:\n$$\n\\sigma_Y^2 = \\mathrm{Var}(Y) = \\left( \\gamma_0 + \\sum_{i=1}^d \\gamma_i (\\Sigma_{ii} + \\mu_i^2) \\right) + \\boldsymbol{\\beta}^\\top \\boldsymbol{\\Sigma} \\boldsymbol{\\beta}\n$$\n\nWe now define the variance for the two cases required:\n\n**(a) Including Emulator Uncertainty:**\nThe variance is the total variance derived above:\n$$\n\\sigma_{\\mathrm{with}}^2 = \\left( \\gamma_0 + \\sum_{i=1}^d \\gamma_i (\\Sigma_{ii} + \\mu_i^2) \\right) + \\boldsymbol{\\beta}^\\top \\boldsymbol{\\Sigma} \\boldsymbol{\\beta}\n$$\n\n**(b) Ignoring Emulator Uncertainty:**\nThis case corresponds to setting the emulator variance $v_{\\mathrm{em}}(X)$ to $0$. This is equivalent to setting $\\gamma_0=0$ and all $\\gamma_i=0$ for $i=1,\\dots,d$. The first term of the total variance vanishes, leaving only the variance propagated through the mean function:\n$$\n\\sigma_{\\mathrm{ignore}}^2 = \\boldsymbol{\\beta}^\\top \\boldsymbol{\\Sigma} \\boldsymbol{\\beta}\n$$\n\n**Credible Interval Calculation**\nApproximating the marginal distribution of $Y$ as a normal distribution $\\mathcal{N}(\\mu_Y, \\sigma_Y^2)$, a $0.95$ credible interval for $Y$ is given by $[\\mu_Y - z_{0.975} \\sigma_Y, \\mu_Y + z_{0.975} \\sigma_Y]$, where $z_{0.975}$ is the $0.975$ quantile of the standard normal distribution, approximately $1.959964$.\n\nThe endpoints for the two cases are:\n*   **With uncertainty:** $L_{\\mathrm{with}} = \\mu_Y - z_{0.975} \\sigma_{\\mathrm{with}}$ and $U_{\\mathrm{with}} = \\mu_Y + z_{0.975} \\sigma_{\\mathrm{with}}$.\n*   **Ignoring uncertainty:** $L_{\\mathrm{ignore}} = \\mu_Y - z_{0.975} \\sigma_{\\mathrm{ignore}}$ and $U_{\\mathrm{ignore}} = \\mu_Y + z_{0.975} \\sigma_{\\mathrm{ignore}}$.\n\nThe following program implements this derived algorithm.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Computes 0.95 credible intervals for a scalar output from a GP emulator,\n    accounting for input and emulator uncertainty.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test Case A\n        {\n            \"d\": 2, \"mu\": [1.0, 0.5], \"Sigma\": [[0.09, 0.03], [0.03, 0.04]],\n            \"beta0\": 0.0, \"beta\": [2.0, -1.5], \"gamma0\": 0.05, \"gamma\": [0.02, 0.01]\n        },\n        # Test Case B\n        {\n            \"d\": 2, \"mu\": [1.0, 0.5], \"Sigma\": [[0.09, 0.03], [0.03, 0.04]],\n            \"beta0\": 0.0, \"beta\": [2.0, -1.5], \"gamma0\": 0.0, \"gamma\": [0.0, 0.0]\n        },\n        # Test Case C\n        {\n            \"d\": 2, \"mu\": [0.8, -0.4], \"Sigma\": [[0.0, 0.0], [0.0, 0.0]],\n            \"beta0\": 0.3, \"beta\": [1.2, 0.5], \"gamma0\": 0.2, \"gamma\": [0.0, 0.0]\n        },\n        # Test Case D\n        {\n            \"d\": 1, \"mu\": [-0.2], \"Sigma\": [[0.25]],\n            \"beta0\": 1.0, \"beta\": [1.0], \"gamma0\": 0.1, \"gamma\": [0.5]\n        }\n    ]\n\n    results = []\n    # Standard normal quantile for a 95% interval\n    z_quantile = norm.ppf(1 - (1 - 0.95) / 2)\n\n    for case in test_cases:\n        # Extract and convert parameters to numpy arrays\n        mu = np.array(case[\"mu\"])\n        Sigma = np.array(case[\"Sigma\"])\n        beta0 = case[\"beta0\"]\n        beta = np.array(case[\"beta\"])\n        gamma0 = case[\"gamma0\"]\n        gamma = np.array(case[\"gamma\"])\n\n        # --- DERIVATION IMPLEMENTATION ---\n\n        # 1. Calculate the marginal mean of Y, which is common to both cases.\n        # E[Y] = beta0 + beta.T @ E[X] = beta0 + beta.T @ mu\n        mu_Y = beta0 + beta.T @ mu\n\n        # 2. Calculate the variance for the case IGNORING emulator uncertainty.\n        # This is the variance of the emulator mean function, Var(m(X)).\n        # Var_X(E[Y|X]) = Var_X(beta0 + beta.T @ X) = beta.T @ Var(X) @ beta\n        var_ignore = beta.T @ Sigma @ beta\n\n        # 3. Calculate the variance for the case INCLUDING emulator uncertainty.\n        # This adds the average emulator variance, E[Var(Y|X)].\n        # E_X[v_em(X)] = E_X[gamma0 + sum(gamma_i * X_i^2)]\n        #             = gamma0 + sum(gamma_i * E[X_i^2])\n        # E[X_i^2] = Var(X_i) + (E[X_i])^2 = Sigma_ii + mu_i^2\n        E_v_em = gamma0 + gamma.T @ (np.diag(Sigma) + mu**2)\n        var_with = E_v_em + var_ignore\n\n        # 4. Calculate standard deviations\n        std_dev_with = np.sqrt(var_with)\n        std_dev_ignore = np.sqrt(var_ignore)\n\n        # 5. Compute interval endpoints for both cases\n        L_with = mu_Y - z_quantile * std_dev_with\n        U_with = mu_Y + z_quantile * std_dev_with\n        L_ignore = mu_Y - z_quantile * std_dev_ignore\n        U_ignore = mu_Y + z_quantile * std_dev_ignore\n\n        # 6. Format the results for the current case\n        case_result = [\n            round(L_with, 6),\n            round(U_with, 6),\n            round(L_ignore, 6),\n            round(U_ignore, 6)\n        ]\n        results.append(case_result)\n\n    # Final print statement in the exact required format.\n    # The str() representation of a list of lists creates the desired format\n    # with spaces, which are then removed.\n    print(str(results).replace(\" \", \"\"))\n\nsolve()\n```"
        }
    ]
}