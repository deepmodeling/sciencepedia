## Applications and Interdisciplinary Connections

Having established the principles and mechanisms of [machine learning emulation](@entry_id:1127546) in the preceding chapters, we now turn to their application. The true power of [surrogate modeling](@entry_id:145866) lies not merely in accelerating computations, but in its capacity to enable new modes of scientific inquiry, facilitate more rigorous [uncertainty quantification](@entry_id:138597), and forge connections between disparate computational disciplines. This chapter explores how the foundational concepts of emulation are deployed in diverse, real-world scientific and engineering contexts. We will examine how emulators enhance the modeling workflow, enable advanced analytical techniques that would otherwise be intractable, and create synergies at the frontiers of interdisciplinary science.

### Enhancing the Computational Modeling Workflow

Before an emulator can be used for analysis, it must be constructed. This process involves a series of deliberate choices, from the design of the initial computer experiments to the representation of complex inputs and outputs. These steps are themselves significant applications of statistical and numerical methods, tailored to the unique demands of surrogate modeling.

#### Designing and Pre-processing for Emulation

The quality of an emulator is fundamentally constrained by the quality of the training data generated by the high-fidelity simulator. Because each run of the simulator is expensive, a crucial first step is the strategic selection of input parameter settings, a field known as Design of Experiments (DoE). The goal is to choose a set of points that efficiently explores the parameter space. For this purpose, space-filling designs are preferred over classical designs that concentrate points at the boundaries or center.

Two prominent space-filling strategies are Latin Hypercube Sampling (LHS) and the use of Low-Discrepancy Sequences (LDS). An LHS design ensures that the projection of the sample points onto each one-dimensional axis is perfectly stratified, meaning each axis is divided into $N$ intervals of equal probability, with exactly one point falling in each interval. This property makes LHS exceptionally effective at reducing the [variance of estimators](@entry_id:167223) for functions that are nearly additive (i.e., dominated by the main effects of individual parameters). Consequently, LHS is an attractive choice for training an emulator when the primary goal is to accurately capture these [main effects](@entry_id:169824). In contrast, Low-Discrepancy Sequences, such as Sobol’ or Halton sequences, are deterministically constructed to minimize the "discrepancy" of the point set, a measure of its deviation from perfect uniformity across the multi-dimensional space. According to the Koksma-Hlawka inequality, lower discrepancy leads to a tighter [worst-case error](@entry_id:169595) bound for quasi-Monte Carlo (QMC) integration. While an LHS design provides excellent one-dimensional projections, its multi-dimensional discrepancy is not guaranteed to be as low as that of an LDS. Therefore, when the emulator will be used for high-dimensional [numerical integration](@entry_id:142553), LDS-based designs are often theoretically preferred, whereas LHS is favored for its [variance reduction](@entry_id:145496) properties for functions with strong additive structure .

Many modern simulators, such as Earth system models, have hundreds or even thousands of input parameters. Training an emulator in such a high-dimensional space is often infeasible due to the "curse of dimensionality." A common strategy is to first apply a [dimensionality reduction](@entry_id:142982) technique to the input parameter space. Principal Component Analysis (PCA) is a powerful, unsupervised method for this purpose. The procedure involves standardizing the input parameters (to account for different units and scales), computing the covariance or [correlation matrix](@entry_id:262631) of the training inputs, and finding its eigenvectors. These eigenvectors, or principal directions, represent orthogonal directions of maximum variance in the input data. By projecting the input data onto the first few leading eigenvectors, one can create a low-dimensional set of features (the principal components) that capture the majority of the input variability. The emulator is then trained on these principal components instead of the original high-dimensional inputs. This is optimal in the sense that it minimizes the mean squared reconstruction error for any given level of linear [dimension reduction](@entry_id:162670). It is essential that the PCA transformation (including standardization parameters and principal directions) is learned *only* from the training data and then applied to any new data to avoid data leakage and biased performance estimates .

#### Emulating Spatiotemporal and Dynamic Systems

The challenge of emulation becomes more acute when the simulator output is not a single scalar but a high-dimensional object like a spatiotemporal field or a time series. Building a separate emulator for the value at each grid point or time step is unworkable. The solution again lies in dimensionality reduction, but this time applied to the *output* space. A standard technique for this is Proper Orthogonal Decomposition (POD), which is mathematically equivalent to performing a Singular Value Decomposition (SVD) on a "[snapshot matrix](@entry_id:1131792)." This matrix is formed by arranging the simulator's output fields (flattened into vectors) from different time points or parameter settings as its columns. The SVD, $Y = U \Sigma V^\top$, yields a set of orthonormal spatial basis functions (the columns of $U$, also called POD modes) that optimally capture the spatial variance of the snapshot ensemble. By retaining only the first $k$ modes corresponding to the largest singular values—often chosen based on a criterion of capturing a certain fraction of the total "energy" ($\sum \sigma_i^2$)—one can approximate any output field as a linear combination of these few basis functions. The emulation task is then reduced to building a surrogate model for the $k$ time-dependent or parameter-dependent coefficients of this expansion. This transforms an intractable function-to-function emulation problem into a manageable vector-to-vector one .

This approach extends naturally to emulating dynamic systems, such as the transient thermal response of a component in a nuclear reactor. The temperature at a given time depends on the entire history of the heat source, not just its instantaneous value. This "memory" effect is a hallmark of systems governed by parabolic PDEs like the heat equation. A naive emulator that maps instantaneous input to instantaneous output will fail. A principled approach must incorporate the system's history. This can be achieved by [engineering physics](@entry_id:264215)-informed features, such as an exponentially weighted moving average of the power history, which serves as a proxy for the thermal state. A more sophisticated method uses a physics-informed mean function for a Gaussian Process. If the system can be approximated as a Linear Time-Invariant (LTI) system, its response is a convolution of the input history with an [impulse response function](@entry_id:137098). This convolution can be used as the mean function of the GP, leaving the GP to learn only the non-linear deviations and unmodeled effects, a much more data-efficient strategy .

The complexity can be further compounded when inputs are themselves functions, such as a spatially varying boundary condition for a PDE. Here too, basis expansions are key. The input function can be projected onto a basis (e.g., Fourier series, or a Karhunen-Loève basis derived from data), and the emulator is trained on the resulting vector of coefficients. For problems with multiple, correlated outputs (e.g., average temperature, max temperature, and heat flux from a thermal simulation), one can train independent emulators for each. However, a more powerful approach is to use a multi-output GP model, such as one based on a Linear Model of Coregionalization (LMC). Such a model can learn the correlation structure between the outputs, allowing it to "borrow statistical strength" across them and often leading to more accurate predictions, especially when data is unevenly sampled across the outputs  .

### Paradigms of Hybrid Physics–Machine Learning Modeling

The integration of machine learning with physics-based models is not monolithic. It represents a spectrum of strategies, each making different assumptions about the roles of data and prior physical knowledge. In the context of emulating complex systems like those in [numerical weather prediction](@entry_id:191656), three main paradigms can be distinguished .

#### Black-Box Emulation

In the black-box approach, the ML model serves as a complete replacement for the original physics-based numerical model, $\mathcal{M}_{\Delta t}$. The emulator learns the mapping from the state at one time step, $\mathbf{x}_t$, to the state at the next, $\mathbf{x}_{t+\Delta t}$, directly from input-output data pairs. This strategy makes no explicit use of the known governing equations, $\mathcal{F}$. Its primary assumption is that the training data (e.g., from long-running simulations or historical reanalyses) sufficiently covers the dynamical attractor of the system. A major challenge for this approach is ensuring physical consistency; conservation laws (e.g., for mass or energy) are not automatically respected and must be enforced through specialized network architectures or by adding penalty terms to the loss function.

#### Gray-Box Modeling: Learning the Residual

The gray-box strategy assumes that an existing physics-based model, $\mathcal{M}_{\Delta t}$, is largely correct but suffers from systematic errors, $\boldsymbol{\epsilon}_{\Delta t}$. These errors may arise from unresolved subgrid-scale processes, numerical truncation, or imperfect parameterizations. Instead of replacing the entire model, the ML component learns a corrective term, $\hat{R}_{\boldsymbol{\phi}}(\mathbf{x}_t)$, that approximates this error. The hybrid forecast is then $\mathbf{x}_{t+\Delta t} \approx \mathcal{M}_{\Delta t}(\mathbf{x}_t) + \hat{R}_{\boldsymbol{\phi}}(\mathbf{x}_t)$. This approach is highly data-efficient as it leverages the vast body of existing physical knowledge encoded in $\mathcal{M}_{\Delta t}$ and only asks the ML model to learn what is missing. Training requires data that can be used to estimate the one-step error, such as pairs of short-term forecasts and verifying analyses.

#### Physics-Informed Machine Learning (PINNs)

Physics-Informed Neural Networks (PINNs) represent a third paradigm, where the governing equations are directly embedded into the training process. Instead of learning a [discrete time](@entry_id:637509)-step map, a PINN learns a continuous solution, $\mathbf{x}_{\boldsymbol{\theta}}(t, \text{space})$, that is parameterized by a neural network. The loss function includes a term that penalizes the PDE residual, $\partial_t \mathbf{x} - \mathcal{F}(\mathbf{x}, \nabla \mathbf{x}, \ldots)$. For example, for a steady, incompressible flow where conservation of mass dictates that the divergence of the flux field $\mathbf{q}$ must be zero, the loss function for a neural network surrogate $\mathbf{q}_{\boldsymbol{\theta}}$ would combine a data-fitting term with a physics-based penalty. This penalty is often the mean squared divergence, computed using [automatic differentiation](@entry_id:144512) at a set of collocation points in the domain's interior: $\mathcal{L}_{\text{phys}} = \lambda \frac{1}{N_c}\sum_{j=1}^{N_c}\left(\nabla \cdot \mathbf{q}_{\boldsymbol{\theta}}(\tilde{\mathbf{x}}_j)\right)^2$. This approach can, in principle, allow the model to be trained with very sparse observational data, as the PDE constraint provides a strong source of regularization. The key assumptions are that the form of the governing equations $\mathcal{F}$ is known and that the network is expressive enough to represent the solution. Careful handling of loss function weighting and the [numerical stiffness](@entry_id:752836) of the PDEs is critical for success .

### Enabling Advanced Uncertainty Quantification and Analysis

Perhaps the most profound impact of emulation is its role as an enabling technology for Uncertainty Quantification (UQ). Many UQ methods, such as Bayesian inference and sensitivity analysis, require a vast number of model evaluations, rendering them impractical for all but the simplest high-fidelity models. Fast emulators break this computational bottleneck.

#### Global Sensitivity Analysis

Global Sensitivity Analysis (GSA) aims to apportion the uncertainty in a model's output to the uncertainty in its various inputs. Sobol' sensitivity indices, for instance, quantify the fraction of output variance attributable to individual inputs (main effects) and their interactions. Calculating these indices via standard Monte Carlo methods requires hundreds of thousands of model runs. However, if a Polynomial Chaos Expansion (PCE) is used as an emulator, the GSA becomes trivial. A PCE represents the model output as a weighted sum of [orthogonal polynomials](@entry_id:146918) of the input variables. Due to the orthogonality of the basis, the total variance of the model output is simply the sum of the squares of the non-constant PCE coefficients. The partial variance contributed by any subset of input variables can be calculated by summing the squared coefficients of all basis terms that depend on precisely that subset of variables. The [total-order index](@entry_id:166452) for a given input, which captures its main effect plus all its interactions, can be found by summing the squared coefficients of all terms involving that input. This turns a computationally intensive integration problem into a simple algebraic post-processing step on the emulator's coefficients .

#### Bayesian Calibration and Inference

Bayesian calibration is a powerful framework for inferring model parameters by combining prior knowledge with observational data. However, it typically requires embedding the model within a Markov chain Monte Carlo (MCMC) sampler, which may need tens of thousands of model evaluations. Replacing the expensive model with a fast emulator makes this feasible. The key is to do so in a statistically coherent manner that accounts for the emulator's own uncertainty. A Gaussian Process (GP) emulator, which provides a predictive mean and variance, is ideal for this. In what is often known as the Kennedy and O'Hagan framework, the [likelihood function](@entry_id:141927) is constructed to account for three sources of uncertainty: observational error, emulator predictive uncertainty, and potentially a [model discrepancy](@entry_id:198101) term. For an observation $y$, the emulator prediction for parameter set $\theta$ is $\mathcal{N}(\hat{\mu}(\theta), \Sigma_e(\theta))$, and the measurement error is $\mathcal{N}(0, \Sigma_m)$. The effective likelihood for the observation, marginalizing over the unknown true model output, becomes a convolution of these two distributions. For Gaussian uncertainties, this results in a likelihood where the total covariance is the sum of the emulator and measurement error covariances: $p(y \mid \theta) = \mathcal{N}(y; \hat{\mu}(\theta), \Sigma_m + \Sigma_e(\theta))$. This likelihood can then be used within an MCMC algorithm to obtain the posterior distribution of the parameters $\theta$, properly propagating all known sources of uncertainty .

#### Uncertainty Propagation and Decision Support

The output of an emulator is often an intermediate quantity that feeds into a larger analysis or decision-making process. It is critical to propagate the emulator's predictive uncertainty through this downstream analysis. For instance, a GP emulator for a vector of regional climate anomalies might predict the output as a random vector $\hat{f}(X_0) \sim \mathcal{N}(m, S)$. If this vector is the input to a nonlinear climate impact functional $g(\cdot)$, the uncertainty in $\hat{f}$ induces uncertainty in the impact assessment. This can be quantified using various techniques. The [delta method](@entry_id:276272) provides a first-order approximation to the variance of the impact, given by the [quadratic form](@entry_id:153497) $(\nabla g(m))^T S (\nabla g(m))$, where $\nabla g(m)$ is the gradient of the functional evaluated at the mean prediction. Alternatively, and more robustly, one can use a Monte Carlo approach: draw a large number of samples from the emulator's predictive distribution $\mathcal{N}(m, S)$, pass each sample through the functional $g$, and compute the [sample variance](@entry_id:164454) of the resulting outputs .

This propagation is paramount when emulators are used to inform policy or risk assessment. Metrics like the cross-validated $R^2$ of an emulator are insufficient to determine if it is "fit for purpose." A decision, such as whether to implement flood defenses, may depend on whether an exceedance probability $\mathbb{P}(Y > T)$ is greater than a critical threshold $p^\star$. A principled approach requires propagating all sources of uncertainty—both from the model inputs and from the emulator itself—into the final probability calculation. This yields not a single [point estimate](@entry_id:176325) for $p$, but a full probability distribution. The emulator can be deemed fit for purpose if this distribution allows for a decision with a sufficiently low risk of being wrong (e.g., if the $(1 - \alpha)$ [credible interval](@entry_id:175131) for $p$ lies entirely on one side of $p^\star$) .

### Interdisciplinary Frontiers

The use of emulation extends beyond accelerating individual models, creating novel linkages between distinct computational fields and enabling methodologies that were previously out of reach.

#### Integration with Data Assimilation

Data assimilation (DA) is the process of optimally combining a physical model forecast with incoming observations to produce an improved estimate of the system's state. In many DA systems, especially in remote sensing, a computationally expensive "observation operator," $h(x)$, is needed to map the model's state variables to the space of the observations (e.g., a radiative transfer model that predicts satellite brightness temperatures). Replacing this forward operator with a fast ML surrogate can dramatically speed up the assimilation cycle. To maintain Bayesian consistency, the surrogate's predictive uncertainty must be properly incorporated. If the surrogate's error $\delta(x)$ is modeled as Gaussian, $\delta(x) \sim \mathcal{N}(m_\delta(x), \Sigma_\delta(x))$, then the effective [observation error covariance](@entry_id:752872) in the DA scheme (such as an Ensemble Kalman Filter) is no longer just the sensor [noise covariance](@entry_id:1128754) $R$, but the sum $R + \Sigma_\delta(x)$. This correctly informs the filter that the surrogate-based observations are more uncertain than the true operator's would be, appropriately down-weighting their influence in the state update. Any systematic bias in the surrogate, $m_\delta(x)$, can be handled by augmenting the state vector with a bias parameter and estimating it jointly with the system state .

#### Optimal Experimental Design

A frontier application of emulation is in accelerating Optimal Experimental Design (OED). The goal of OED is to choose experimental conditions (the "design") that will be maximally informative for a given scientific goal, such as discriminating between competing models or calibrating parameters. This typically involves maximizing an expected utility function, where the expectation is taken over all possible experimental outcomes. Evaluating this utility is extremely costly, as it requires a nested Monte Carlo loop: for each candidate design, one must simulate many possible future datasets, and for each of those, perform an expensive posterior inference. Emulators can break this "double loop" intractability in several ways. In one strategy, one can emulate the expensive-to-compute summary statistics (e.g., likelihoods or posterior moments) that are needed as inputs to the utility calculation. A more direct approach is to treat the expected utility function $U(d)$ itself as an expensive [black-box function](@entry_id:163083) of the design $d$. One can then use an emulator, often within a Bayesian Optimization framework, to build a surrogate for $U(d)$ and efficiently find the design $d^*$ that maximizes it. This creates a powerful "meta-modeling" loop, where emulators are used to design better experiments, which in turn produce data to build better primary models .

#### Multi-Fidelity Emulation

Often, scientists have access to a hierarchy of models of varying cost and accuracy. For instance, a coarse-grid simulation might be cheap but biased, while a fine-grid simulation is accurate but expensive. Multi-fidelity emulation aims to combine information from all these sources to produce a surrogate that is more accurate than one built from any single source alone. A common approach, based on [co-kriging](@entry_id:747413), is to model the high-fidelity output $y_H$ as an autoregressive function of the low-fidelity output $y_L$, plus a discrepancy term: $y_H(\theta) = \rho(\theta) \, y_L(\theta) + \delta(\theta)$. A GP framework can be used to model this entire structure, learning the relationship between the fidelities and the discrepancy from a small number of high-fidelity runs and a larger number of low-fidelity runs. The resulting multi-fidelity emulator can provide high-accuracy predictions with well-calibrated uncertainty, at a fraction of the cost of relying on the high-fidelity model alone . This principle of "[borrowing strength](@entry_id:167067)" is also at the heart of multi-output GPs, which can leverage correlations between different outputs (e.g., temperature and precipitation) to improve predictions, especially when one output is more densely sampled than another .

#### Emulation and Causal Inference

A critical question at the intersection of ML and science is whether an emulator, trained on observational or simulated data, can be used for causal reasoning—that is, to predict the effect of an intervention, $P(Y \mid \text{do}(X=x))$. High predictive accuracy on observational data ("predictive adequacy") is not sufficient for this task. A model can be an excellent interpolator of correlations in a dataset but fail catastrophically at predicting the effects of an intervention that breaks those correlations. For an emulator to be "causally adequate," the interventional quantity of interest must first be *identifiable* from the observational data. This is a structural condition on the underlying causal graph of the system. For instance, if a set of covariates $Z$ satisfies the [back-door criterion](@entry_id:926460), then the causal effect is identifiable via the adjustment formula, which relies on the observational [conditional expectation](@entry_id:159140) $\mathbb{E}[Y \mid X=x, Z=z]$. An emulator can then be considered causally adequate if it provides a good approximation to this specific, invariant [conditional expectation](@entry_id:159140). This is a far more stringent condition than simply predicting $Y$ well, but it is also a far weaker condition than full "mechanistic adequacy," which would require the emulator to replicate the generative rules of the underlying system. Causal adequacy thus occupies a crucial middle ground, enabling interventional predictions without requiring a full mechanistic replica, provided the assumptions of identifiability and invariance hold .

### Conclusion

The applications of machine learning emulators are as broad as the landscape of computational science itself. Moving beyond their initial role as simple speed-up tools, they have become integral components of the modern scientific workflow. They enable the rigorous application of Bayesian UQ and sensitivity analysis to previously [intractable models](@entry_id:750783). They form bridges to other computational disciplines, creating powerful hybrid methods in data assimilation, experimental design, and [causal inference](@entry_id:146069). As we have seen, the successful deployment of emulators is not a trivial black-box exercise. It requires a deep understanding of both the machine learning methodology and the physical or system-level principles of the model being emulated. By thoughtfully combining data-driven techniques with prior scientific knowledge, surrogate modeling provides a powerful framework for navigating the complexity of modern scientific challenges.