{
    "hands_on_practices": [
        {
            "introduction": "A primary application of surrogate modeling in environmental science is optimizing computationally expensive simulations. This practice introduces Bayesian Optimization, a powerful framework for finding the minimum of a \"black-box\" function. You will implement a Gaussian Process (GP) emulator from scratch and use its predictive mean and variance to guide the search for an optimal input setting. This exercise will give you hands-on experience with the critical exploration-exploitation tradeoff by implementing the Lower Confidence Bound (LCB) acquisition function, a cornerstone of sequential design strategies .",
            "id": "3891118",
            "problem": "Consider a one-dimensional environmental response modeled by a stochastic process prior, and an emulator based on Gaussian Process (GP) regression for a scalar output as a function of a normalized control variable $x \\in [0,1]$. Suppose there is a set of existing training observations $\\{(x_i, y_i)\\}_{i=1}^n$, with a prior covariance specified by the squared-exponential kernel $k(x,x') = s^2 \\exp\\left( -\\frac{(x-x')^2}{2\\ell^2} \\right)$ and independent Gaussian observational noise of variance $\\sigma_n^2$. The emulator provides a posterior predictive distribution at any $x$ that is Gaussian with mean $ \\mu(x) $ and standard deviation $ \\sigma(x) $, derived from the standard Gaussian Process regression framework given the specified kernel and the training data.\n\nIn sequential design for minimization, the next sampling location is chosen by minimizing an acquisition function that encodes an exploration-exploitation tradeoff. One principled choice is an acquisition that equals a one-sided credible lower bound of the posterior predictive distribution. Define the acquisition in terms of the posterior mean and posterior standard deviation and a nonnegative exploration parameter $ \\kappa \\ge 0 $ (with the convention that larger $ \\kappa $ encourages exploration). Assume ties in the acquisition values are resolved by selecting the smallest $x$ among minimizers.\n\nUsing the following scientifically plausible synthetic environmental response $ f(x) = \\sin(2\\pi x) + 0.2 x^2 - 0.1 $ to generate the training outputs $y_i = f(x_i)$, implement the Gaussian Process predictive mean $ \\mu(x) $ and standard deviation $ \\sigma(x) $ based on the squared-exponential kernel and Gaussian noise described above, and then implement the acquisition for minimization as described. For each test case below, compute the next sampling location as the $x$ in the candidate set that minimizes the acquisition (apply the tie-breaking rule if necessary).\n\nAll parameters, variables, and values are specified in the units of the normalized domain $[0,1]$, and angles within $ \\sin(\\cdot) $ are in radians. There are no physical units required beyond these normalized units.\n\nTest suite:\n- Case $1$ (happy path): training inputs $x = [0.1, 0.4, 0.8]$, kernel amplitude $s^2 = 1.0$, length scale $\\ell = 0.25$, noise variance $\\sigma_n^2 = 0.05^2$, exploration parameter $\\kappa = 1.0$, candidate set $X_{\\text{cand}}$ is $101$ equally spaced points from $0.0$ to $1.0$ inclusive.\n- Case $2$ (pure exploitation boundary): same as Case $1$ but with exploration parameter $\\kappa = 0.0$.\n- Case $3$ (strong exploration): same as Case $1$ but with exploration parameter $\\kappa = 5.0$.\n- Case $4$ (tie scenario via nearly constant kernel): training inputs $x = [0.1, 0.4, 0.8]$, kernel amplitude $s^2 = 1.0$, length scale $\\ell = 10^9$, noise variance $\\sigma_n^2 = 0.1^2$, exploration parameter $\\kappa = 2.0$, candidate set $X_{\\text{cand}} = [0.0, 0.3, 0.6, 1.0]$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,result4]\"), where each result is the selected next sampling location for the corresponding test case as a floating-point number rounded to four decimal places.",
            "solution": "The task is to determine the next sampling location in a sequential design problem for function minimization. This is a classic application of Bayesian optimization, where a Gaussian Process (GP) serves as a surrogate model for an expensive-to-evaluate or unknown objective function. The selection of the next point is guided by an acquisition function, which balances exploiting known good regions (low function value) and exploring regions of high uncertainty.\n\nFirst, we define the probabilistic model. We are given $n$ training observations $D = \\{(x_i, y_i)\\}_{i=1}^n$. The observations $y_i$ are assumed to be generated from a latent function $f(x)$ with additive Gaussian noise: $y_i = f(x_i) + \\epsilon_i$, where $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma_n^2)$. A GP prior is placed on the function $f(x)$, which is fully specified by a mean function (assumed to be zero) and a covariance (or kernel) function $k(x, x')$. For this problem, the specified kernel is the squared-exponential kernel:\n$$\nk(x, x') = s^2 \\exp\\left( -\\frac{(x - x')^2}{2\\ell^2} \\right)\n$$\nwhere $s^2$ is the signal variance and $\\ell$ is the characteristic length-scale.\n\nGiven the training data, we can derive the posterior predictive distribution for the function value $f_*$ at a new point $x_*$. This distribution is a Gaussian, $p(f_* | x_*, X, \\mathbf{y}) = \\mathcal{N}(\\mu(x_*), \\sigma^2(x_*))$, with a predictive mean $\\mu(x_*)$ and predictive variance $\\sigma^2(x_*)$ given by the standard GP regression equations:\n$$\n\\mu(x_*) = \\mathbf{k}_*^T (K + \\sigma_n^2 I)^{-1} \\mathbf{y}\n$$\n$$\n\\sigma^2(x_*) = k(x_*, x_*) - \\mathbf{k}_*^T (K + \\sigma_n^2 I)^{-1} \\mathbf{k}_*\n$$\nIn these equations, $\\mathbf{y}$ is the vector of $n$ training outputs. $K$ is the $n \\times n$ covariance matrix constructed by applying the kernel to all pairs of training inputs, $K_{ij} = k(x_i, x_j)$. $I$ is the $n \\times n$ identity matrix. $\\mathbf{k}_*$ is the $n \\times 1$ vector of covariances between the test point $x_*$ and each training input, $(\\mathbf{k}_*)_i = k(x_*, x_i)$. The term $k(x_*, x_*) = s^2$ represents the prior variance at $x_*$.\n\nFor numerical stability and efficiency, the inverse term is not computed directly. Instead, one solves a system of linear equations. Letting $A = K + \\sigma_n^2 I$, we first solve $A \\boldsymbol{\\alpha} = \\mathbf{y}$ for the vector $\\boldsymbol{\\alpha}$. The predictive mean is then simply $\\mu(x_*) = \\mathbf{k}_*^T \\boldsymbol{\\alpha}$. A robust method to solve this system is via Cholesky decomposition of $A$, since it is symmetric and positive definite. Let $A = LL^T$. The variance calculation also benefits from this decomposition.\n\nThe next sampling location is chosen by minimizing an acquisition function over a set of candidate points $X_{\\text{cand}}$. The problem specifies a one-sided credible lower bound, a common choice for minimization problems, often called the Lower Confidence Bound (LCB) acquisition function:\n$$\na(x) = \\mu(x) - \\kappa \\sigma(x)\n$$\nwhere $\\sigma(x) = \\sqrt{\\sigma^2(x)}$ is the posterior predictive standard deviation. The parameter $\\kappa \\ge 0$ balances the trade-off between exploitation (favoring points with a low predicted mean $\\mu(x)$) and exploration (favoring points with high uncertainty $\\sigma(x)$). A larger $\\kappa$ increases the weight of the uncertainty term, promoting exploration. The optimal next point, $x_{\\text{next}}$, is found as:\n$$\nx_{\\text{next}} = \\arg\\min_{x \\in X_{\\text{cand}}} a(x)\n$$\nIn case of ties for the minimum value, the tie is broken by choosing the smallest $x$ among the minimizers.\n\nThe procedural steps for solving each test case are:\n1.  Generate the training outputs $\\mathbf{y}$ by evaluating the function $f(x) = \\sin(2\\pi x) + 0.2 x^2 - 0.1$ at the given training inputs $X$.\n2.  Construct the covariance matrix $K$ from the training inputs $X$ and kernel hyperparameters $s^2$ and $\\ell$.\n3.  Form the matrix $A = K + \\sigma_n^2 I$ using the given noise variance $\\sigma_n^2$.\n4.  Perform a Cholesky decomposition of $A = LL^T$.\n5.  Solve for $\\boldsymbol{\\alpha}$ in $L L^T \\boldsymbol{\\alpha} = \\mathbf{y}$ using forward and backward substitution.\n6.  For each candidate point $x_* \\in X_{\\text{cand}}$:\n    a. Compute the covariance vector $\\mathbf{k}_* = K(x_*, X)$.\n    b. Calculate the predictive mean: $\\mu(x_*) = \\mathbf{k}_*^T \\boldsymbol{\\alpha}$.\n    c. Solve $L\\mathbf{v} = \\mathbf{k}_*$ for $\\mathbf{v}$ and compute the predictive variance: $\\sigma^2(x_*) = s^2 - \\mathbf{v}^T\\mathbf{v}$.\n    d. Calculate the acquisition function value: $a(x_*) = \\mu(x_*) - \\kappa \\sqrt{\\sigma^2(x_*)}$.\n7. Find the candidate point $x_*$ that minimizes $a(x_*)$, respecting the tie-breaking rule. This is the desired result.\n\nThis procedure is applied to each of the four test cases provided, using the specified parameters and candidate sets.",
            "answer": "```python\nimport numpy as np\nfrom scipy.linalg import cholesky, solve_triangular\n\ndef solve():\n    \"\"\"\n    Implements Gaussian Process regression and a Lower Confidence Bound (LCB)\n    acquisition function to find the next sampling point in a sequential design\n    problem for function minimization.\n    \"\"\"\n\n    def response_function(x):\n        \"\"\" The synthetic environmental response function. \"\"\"\n        return np.sin(2 * np.pi * x) + 0.2 * x**2 - 0.1\n\n    def squared_exponential_kernel(X1, X2, s2, ell):\n        \"\"\"\n        Computes the squared-exponential kernel matrix between two sets of points.\n        X1: (N, 1) array of points\n        X2: (M, 1) array of points\n        s2: signal variance (amplitude squared)\n        ell: length scale\n        Returns: (N, M) kernel matrix\n        \"\"\"\n        # The expression `(X1 - X2.T)**2` uses broadcasting to compute the squared\n        # Euclidean distance matrix between all pairs of points.\n        sqdist = (X1 - X2.T)**2\n        return s2 * np.exp(-0.5 * sqdist / ell**2)\n\n    def gp_predict(X_train, y_train, X_cand, s2, ell, sigma_n2):\n        \"\"\"\n        Computes the posterior predictive mean and standard deviation of a GP.\n        \"\"\"\n        n_train = len(X_train)\n        \n        # Ensure inputs are 2D arrays\n        X_train_2d = X_train.reshape(-1, 1)\n        X_cand_2d = X_cand.reshape(-1, 1)\n        \n        # Build kernel matrix for training data\n        K = squared_exponential_kernel(X_train_2d, X_train_2d, s2, ell)\n        \n        # Add noise to the diagonal for the K(X,X) + sigma_n^2*I matrix\n        A = K + sigma_n2 * np.eye(n_train)\n        \n        # Use Cholesky decomposition for stability and efficiency\n        try:\n            L = cholesky(A, lower=True)\n        except np.linalg.LinAlgError:\n            # Fallback for ill-conditioned matrix, although not expected here\n            # Add a small jitter to the diagonal\n            A += 1e-6 * np.eye(n_train)\n            L = cholesky(A, lower=True)\n\n        # Solve for alpha = (K + sigma_n^2*I)^-1 * y\n        alpha = solve_triangular(L.T, solve_triangular(L, y_train, lower=True))\n        \n        # Compute kernel matrix between candidate and training points\n        K_star = squared_exponential_kernel(X_cand_2d, X_train_2d, s2, ell)\n        \n        # Predictive mean\n        mu_star = K_star @ alpha\n        \n        # Predictive variance\n        v = solve_triangular(L, K_star.T, lower=True)\n        # Prior variance k(x_*, x_*) is s2 for all candidate points\n        var_star = s2 - np.sum(v**2, axis=0)\n        \n        # Ensure variance is non-negative due to potential floating point errors\n        var_star[var_star  0] = 0\n        std_star = np.sqrt(var_star)\n        \n        return mu_star, std_star\n\n    test_cases = [\n        # Case 1: happy path\n        {'x_train': np.array([0.1, 0.4, 0.8]), 's2': 1.0, 'ell': 0.25, \n         'sigma_n2': 0.05**2, 'kappa': 1.0, \n         'x_cand': np.linspace(0.0, 1.0, 101)},\n        # Case 2: pure exploitation\n        {'x_train': np.array([0.1, 0.4, 0.8]), 's2': 1.0, 'ell': 0.25, \n         'sigma_n2': 0.05**2, 'kappa': 0.0, \n         'x_cand': np.linspace(0.0, 1.0, 101)},\n        # Case 3: strong exploration\n        {'x_train': np.array([0.1, 0.4, 0.8]), 's2': 1.0, 'ell': 0.25, \n         'sigma_n2': 0.05**2, 'kappa': 5.0, \n         'x_cand': np.linspace(0.0, 1.0, 101)},\n        # Case 4: tie scenario\n        {'x_train': np.array([0.1, 0.4, 0.8]), 's2': 1.0, 'ell': 1e9, \n         'sigma_n2': 0.1**2, 'kappa': 2.0, \n         'x_cand': np.array([0.0, 0.3, 0.6, 1.0])}\n    ]\n\n    results = []\n    for case in test_cases:\n        x_train = case['x_train']\n        y_train = response_function(x_train)\n        \n        mu, std = gp_predict(\n            x_train, y_train, case['x_cand'],\n            case['s2'], case['ell'], case['sigma_n2']\n        )\n        \n        # LCB Acquisition function\n        acquisition_values = mu - case['kappa'] * std\n        \n        # Find the index of the minimum acquisition value.\n        # np.argmin() breaks ties by returning the index of the first minimum,\n        # which corresponds to the smallest x as candidates are sorted.\n        min_idx = np.argmin(acquisition_values)\n        \n        next_sample_location = case['x_cand'][min_idx]\n        results.append(f\"{next_sample_location:.4f}\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "While Gaussian Processes provide a flexible non-parametric approach, Polynomial Chaos Expansions (PCE) offer a powerful alternative, especially when dealing with uncertain inputs that follow known probability distributions. This exercise focuses on building a PCE surrogate by projecting a model onto a basis of orthogonal polynomials—in this case, Hermite polynomials suited for Gaussian inputs. You will perform the coefficient fitting via least-squares regression and, importantly, analyze how the quality of the fit and the uncertainty in the PCE coefficients depend on the size of the training dataset relative to the complexity of the polynomial basis .",
            "id": "3891168",
            "problem": "You are asked to implement a Polynomial Chaos Expansion (PCE) emulator and quantify estimator uncertainty in a controlled setting relevant to environmental and earth system modeling. Consider a vector of uncertain, standardized environmental drivers $\\mathbf{Z} = (Z_1,\\dots,Z_d)$ where each $Z_i$ is independent and identically distributed as standard normal, $Z_i \\sim \\mathcal{N}(0,1)$. The surrogate response represents a dimensionless environmental quantity and is defined by a deterministic mapping $g:\\mathbb{R}^d \\to \\mathbb{R}$ given by\n$$\ng(\\mathbf{z}) = a_0 + a_1 z_1 + a_2 z_2 + a_3 z_3 + a_{12} z_1 z_2 + a_{23} z_2 z_3 + b_1 \\tanh(c_1 z_1 + c_2 z_2) + d_1 z_1^2 + d_2 z_2^3 + e_1 z_3 z_4 + e_2 z_4^2 + e_3 z_5^3,\n$$\nwhere coefficients are fixed as $a_0=1$, $a_1=0.5$, $a_2=0.25$, $a_3=-0.15$, $a_{12}=0.2$, $a_{23}=-0.1$, $b_1=0.2$, $c_1=0.5$, $c_2=0.3$, $d_1=0.1$, $d_2=0.05$, $e_1=0.07$, $e_2=0.03$, $e_3=-0.02$. When $d  4$ or $d  5$, define $z_4=0$ or $z_5=0$ respectively so that terms involving $z_4$ or $z_5$ vanish consistently.\n\nYour task is to construct a Polynomial Chaos Expansion (PCE) surrogate for $g(\\mathbf{Z})$ using multivariate Hermite basis functions orthonormal with respect to the standard normal measure. Let $\\mathrm{He}_k(x)$ denote the probabilists’ Hermite polynomial of degree $k$ so that $\\mathrm{He}_0(x)=1$, $\\mathrm{He}_1(x)=x$, and the recursion $\\,\\mathrm{He}_{k+1}(x)=x\\,\\mathrm{He}_k(x)-k\\,\\mathrm{He}_{k-1}(x)\\,$ holds. The univariate orthonormal basis functions are\n$$\n\\phi_k(x) = \\frac{\\mathrm{He}_k(x)}{\\sqrt{k!}},\n$$\nand the multivariate basis indexed by the multi-index $\\boldsymbol{\\alpha}=(\\alpha_1,\\dots,\\alpha_d)$ is\n$$\n\\Phi_{\\boldsymbol{\\alpha}}(\\mathbf{z}) = \\prod_{i=1}^d \\phi_{\\alpha_i}(z_i).\n$$\nLet the total-order truncation set be\n$$\n\\mathcal{A}_{d,p} = \\left\\{ \\boldsymbol{\\alpha}\\in\\mathbb{N}_0^d : \\sum_{i=1}^d \\alpha_i \\le p \\right\\},\n$$\nwhose cardinality is $M=\\binom{d+p}{p}$.\n\nGiven a training sample $\\{\\mathbf{z}^{(j)}\\}_{j=1}^n$ drawn independently from $\\mathcal{N}(\\mathbf{0},\\mathbf{I}_d)$ and the corresponding responses $y^{(j)}=g(\\mathbf{z}^{(j)})$, fit the PCE coefficients $\\mathbf{c}\\in\\mathbb{R}^M$ by ordinary least squares regression onto the basis $\\{\\Phi_{\\boldsymbol{\\alpha}}\\}_{\\boldsymbol{\\alpha}\\in\\mathcal{A}_{d,p}}$. Form the design matrix $\\mathbf{X}\\in\\mathbb{R}^{n\\times M}$ with entries\n$$\nX_{j,m} = \\Phi_{\\boldsymbol{\\alpha}^{(m)}}\\!\\left(\\mathbf{z}^{(j)}\\right),\n$$\nwhere $\\boldsymbol{\\alpha}^{(m)}$ enumerates $\\mathcal{A}_{d,p}$. The ordinary least squares estimator solves\n$$\n\\widehat{\\mathbf{c}} = \\arg\\min_{\\mathbf{c}\\in\\mathbb{R}^M} \\left\\| \\mathbf{X}\\mathbf{c} - \\mathbf{y}\\right\\|_2^2,\n$$\nwhere $\\mathbf{y}=(y^{(1)},\\dots,y^{(n)})^\\top$. Under the Gauss–Markov framework for linear regression with random design and mean-zero residuals, an estimator for the residual variance is\n$$\n\\widehat{\\sigma}^2 = \\frac{\\mathrm{RSS}}{\\max(n - M, 1)}, \\quad \\mathrm{RSS} = \\left\\| \\mathbf{y} - \\mathbf{X}\\widehat{\\mathbf{c}}\\right\\|_2^2,\n$$\nand an estimator for the coefficient covariance matrix is\n$$\n\\widehat{\\mathrm{Cov}}(\\widehat{\\mathbf{c}}) = \\widehat{\\sigma}^2\\left(\\mathbf{X}^\\top \\mathbf{X}\\right)^{\\dagger},\n$$\nwhere $\\left(\\cdot\\right)^{\\dagger}$ denotes the Moore–Penrose pseudoinverse. Define the average estimator variance as\n$$\n\\overline{v} = \\frac{1}{M}\\sum_{m=1}^M \\left[ \\widehat{\\mathrm{Cov}}(\\widehat{\\mathbf{c}}) \\right]_{m,m},\n$$\nthe maximum estimator variance as\n$$\nv_{\\max} = \\max_{1\\le m \\le M} \\left[ \\widehat{\\mathrm{Cov}}(\\widehat{\\mathbf{c}}) \\right]_{m,m},\n$$\nand the spectral condition number of $\\mathbf{X}^\\top \\mathbf{X}$ as\n$$\n\\kappa = \\|\\mathbf{X}^\\top \\mathbf{X}\\|_2 \\cdot \\left\\| \\left(\\mathbf{X}^\\top \\mathbf{X}\\right)^{\\dagger} \\right\\|_2,\n$$\ncomputed via the $2$-norm.\n\nImplement a complete program that:\n- Generates training samples $\\mathbf{z}^{(j)}$ using the independent standard normal law with a fixed seed $s=314159$ for reproducibility (use the same seed across all test cases, but you may vary the generator state deterministically by an offset per test case).\n- Constructs the multivariate orthonormal Hermite basis up to total order $p$.\n- Builds the design matrix $\\mathbf{X}$ and fits $\\widehat{\\mathbf{c}}$ using ordinary least squares.\n- Computes $\\overline{v}$, $v_{\\max}$, and $\\kappa$ using the formulas above.\n\nTest suite:\n- Case $1$: $(d,p,n)=(3,2,10)$, where $M=\\binom{3+2}{2}=10$ is equal to $n$ (a square design).\n- Case $2$: $(d,p,n)=(3,2,50)$, where $n=5M$ (oversampled).\n- Case $3$: $(d,p,n)=(3,3,40)$, where $M=\\binom{3+3}{3}=20$ and $n=2M$.\n- Case $4$: $(d,p,n)=(5,2,63)$, where $M=\\binom{5+2}{2}=21$ and $n=3M$.\n- Case $5$: $(d,p,n)=(5,3,60)$, where $M=\\binom{5+3}{3}=56$ and $n$ is slightly above $M$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes one inner list of three floats in the order $[\\overline{v}, v_{\\max}, \\kappa]$. For example, the output format must be exactly like\n$[ [\\overline{v}_1, v_{\\max,1}, \\kappa_1], [\\overline{v}_2, v_{\\max,2}, \\kappa_2], \\dots ]$,\nwith no additional text.",
            "solution": "The problem requires the construction of a Polynomial Chaos Expansion (PCE) surrogate for a given deterministic function $g(\\mathbf{z})$ of uncertain inputs $\\mathbf{Z} = (Z_1, \\dots, Z_d)$, where each $Z_i$ is an independent standard normal random variable, $Z_i \\sim \\mathcal{N}(0,1)$. The task further involves quantifying the uncertainty of the PCE coefficient estimators. The problem is well-posed, scientifically sound, and provides all necessary information for a unique, reproducible solution. We will proceed by implementing the specified procedure step-by-step.\n\nThe core of the task is to approximate the complex model $g(\\mathbf{Z})$, which represents some environmental quantity, with a simpler polynomial model. This surrogate, the PCE, is given by a truncated series expansion:\n$$\n\\widehat{g}(\\mathbf{Z}) = \\sum_{\\boldsymbol{\\alpha} \\in \\mathcal{A}_{d,p}} c_{\\boldsymbol{\\alpha}} \\Phi_{\\boldsymbol{\\alpha}}(\\mathbf{Z})\n$$\nwhere $\\{c_{\\boldsymbol{\\alpha}}\\}$ are the coefficients to be determined, $\\{\\Phi_{\\boldsymbol{\\alpha}}\\}$ are multivariate polynomial basis functions, and $\\mathcal{A}_{d,p}$ is a truncation set of multi-indices.\n\nThe procedural steps are as follows:\n\n1.  **Basis Function Definition**: The inputs $Z_i$ are standard normal variables. The natural choice for orthogonal polynomials with respect to the standard normal probability measure is the probabilists' Hermite polynomials, denoted $\\mathrm{He}_k(x)$. The problem specifies their recursive definition:\n    $$\n    \\mathrm{He}_0(x) = 1 \\\\\n    \\mathrm{He}_1(x) = x \\\\\n    \\mathrm{He}_{k+1}(x) = x\\,\\mathrm{He}_k(x) - k\\,\\mathrm{He}_{k-1}(x)\n    $$\n    These polynomials are orthogonal, but not orthonormal. The corresponding orthonormal basis functions $\\phi_k(x)$ are defined as:\n    $$\n    \\phi_k(x) = \\frac{\\mathrm{He}_k(x)}{\\sqrt{k!}}\n    $$\n    such that $\\mathbb{E}[\\phi_j(Z) \\phi_k(Z)] = \\delta_{jk}$ when $Z \\sim \\mathcal{N}(0,1)$. For a $d$-dimensional input vector $\\mathbf{z}=(z_1, \\dots, z_d)$, the multivariate basis functions $\\Phi_{\\boldsymbol{\\alpha}}(\\mathbf{z})$ are formed by the tensor product of the univariate functions, indexed by a multi-index $\\boldsymbol{\\alpha}=(\\alpha_1, \\dots, \\alpha_d) \\in \\mathbb{N}_0^d$:\n    $$\n    \\Phi_{\\boldsymbol{\\alpha}}(\\mathbf{z}) = \\prod_{i=1}^d \\phi_{\\alpha_i}(z_i)\n    $$\n    We will implement a function to evaluate $\\mathrm{He}_k(x)$ for multiple values of $x$ and degrees $k$ simultaneously, and another function to generate the required factorials $\\sqrt{k!}$ for normalization.\n\n2.  **Truncation Set Generation**: The infinite PCE sum must be truncated for practical computation. The problem specifies a total-order truncation scheme, where the set of active multi-indices $\\mathcal{A}_{d,p}$ includes all $\\boldsymbol{\\alpha}$ whose components sum to at most a total degree $p$:\n    $$\n    \\mathcal{A}_{d,p} = \\left\\{ \\boldsymbol{\\alpha} \\in \\mathbb{N}_0^d : \\sum_{i=1}^d \\alpha_i \\le p \\right\\}\n    $$\n    The number of basis functions, $M$, is the cardinality of this set, given by $M = |\\mathcal{A}_{d,p}| = \\binom{d+p}{p}$. We will implement a recursive algorithm to generate these multi-indices for given parameters $d$ and $p$.\n\n3.  **Data Generation**: To fit the PCE coefficients, we require a training dataset. This consists of $n$ samples of the input vector, $\\{\\mathbf{z}^{(j)}\\}_{j=1}^n$, drawn from the specified distribution $\\mathcal{N}(\\mathbf{0}, \\mathbf{I}_d)$, and the corresponding model responses, $y^{(j)} = g(\\mathbf{z}^{(j)})$. The function $g(\\mathbf{z})$ is provided:\n    $$\n    g(\\mathbf{z}) = 1 + 0.5 z_1 + 0.25 z_2 - 0.15 z_3 + 0.2 z_1 z_2 - 0.1 z_2 z_3 + 0.2 \\tanh(0.5 z_1 + 0.3 z_2) + 0.1 z_1^2 + 0.05 z_2^3 + 0.07 z_3 z_4 + 0.03 z_4^2 - 0.02 z_5^3\n    $$\n    with the rule that $z_i=0$ for $i  d$. We will use a seeded pseudo-random number generator for reproducibility, with a deterministic offset for each test case as per the problem description.\n\n4.  **Least-Squares Regression**: The coefficients $\\mathbf{c} = (c_{\\boldsymbol{\\alpha}^{(1)}}, \\dots, c_{\\boldsymbol{\\alpha}^{(M)}})^\\top$ are estimated by minimizing the squared error between the PCE surrogate and the true model outputs on the training set. This is an ordinary least squares (OLS) problem:\n    $$\n    \\widehat{\\mathbf{c}} = \\arg\\min_{\\mathbf{c}\\in\\mathbb{R}^M} \\left\\| \\mathbf{X}\\mathbf{c} - \\mathbf{y}\\right\\|_2^2\n    $$\n    Here, $\\mathbf{y} = (y^{(1)}, \\dots, y^{(n)})^\\top$ is the vector of model responses, and $\\mathbf{X}$ is the $n \\times M$ design matrix. Each entry $X_{j,m}$ of the design matrix is the value of the $m$-th basis function $\\Phi_{\\boldsymbol{\\alpha}^{(m)}}$ evaluated at the $j$-th input sample $\\mathbf{z}^{(j)}$:\n    $$\n    X_{j,m} = \\Phi_{\\boldsymbol{\\alpha}^{(m)}}\\!\\left(\\mathbf{z}^{(j)}\\right)\n    $$\n    We will construct this matrix and solve for $\\widehat{\\mathbf{c}}$ using a standard numerical linear algebra solver, `numpy.linalg.lstsq`, which is robust and suitable for this task.\n\n5.  **Estimator Uncertainty Analysis**: The final step is to quantify the statistical uncertainty of the estimated coefficients $\\widehat{\\mathbf{c}}$. This uncertainty arises from the finite sample size $n$ and model inadequacy (i.e., the truncation error of the PCE). The required metrics are derived from the covariance matrix of the estimator, $\\widehat{\\mathrm{Cov}}(\\widehat{\\mathbf{c}})$.\n    -   First, we compute the residual sum of squares: $\\mathrm{RSS} = \\left\\| \\mathbf{y} - \\mathbf{X}\\widehat{\\mathbf{c}}\\right\\|_2^2$.\n    -   Next, we estimate the residual variance: $\\widehat{\\sigma}^2 = \\frac{\\mathrm{RSS}}{\\max(n - M, 1)}$. The denominator $\\max(n - M, 1)$ correctly handles the degrees of freedom, including the edge case $n \\le M$.\n    -   The estimator for the coefficient covariance matrix is then $\\widehat{\\mathrm{Cov}}(\\widehat{\\mathbf{c}}) = \\widehat{\\sigma}^2\\left(\\mathbf{X}^\\top \\mathbf{X}\\right)^{\\dagger}$, where $(\\cdot)^{\\dagger}$ denotes the Moore-Penrose pseudoinverse. We use the pseudoinverse for numerical stability and correctness, especially if $\\mathbf{X}^\\top \\mathbf{X}$ is singular or ill-conditioned.\n    -   From this covariance matrix, we compute two metrics:\n        -   The average estimator variance: $\\overline{v} = \\frac{1}{M}\\mathrm{Tr}\\left(\\widehat{\\mathrm{Cov}}(\\widehat{\\mathbf{c}})\\right) = \\frac{1}{M}\\sum_{m=1}^M \\left[ \\widehat{\\mathrm{Cov}}(\\widehat{\\mathbf{c}}) \\right]_{m,m}$.\n        -   The maximum estimator variance: $v_{\\max} = \\max_{1\\le m \\le M} \\left[ \\widehat{\\mathrm{Cov}}(\\widehat{\\mathbf{c}}) \\right]_{m,m}$.\n    -   Finally, we assess the conditioning of the regression problem by computing the spectral condition number of the Gram matrix: $\\kappa = \\mathrm{cond}(\\mathbf{X}^\\top \\mathbf{X}) = \\|\\mathbf{X}^\\top \\mathbf{X}\\|_2 \\cdot \\left\\| \\left(\\mathbf{X}^\\top \\mathbf{X}\\right)^{\\dagger} \\right\\|_2$. A high value of $\\kappa$ indicates that the design matrix is nearly collinear, which can inflate the variance of the coefficient estimates.\n\nThese steps will be encapsulated in a program that iterates through the five specified test cases, computing $[\\overline{v}, v_{\\max}, \\kappa]$ for each, and formats the output as a single-line list of lists.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import factorial\n\ndef solve():\n    \"\"\"\n    Main function to execute the Polynomial Chaos Expansion analysis for all test cases.\n    \"\"\"\n\n    # Coefficients for the g-function as defined in the problem.\n    COEFFS = {\n        'a0': 1.0, 'a1': 0.5, 'a2': 0.25, 'a3': -0.15,\n        'a12': 0.2, 'a23': -0.1,\n        'b1': 0.2, 'c1': 0.5, 'c2': 0.3,\n        'd1': 0.1, 'd2': 0.05,\n        'e1': 0.07, 'e2': 0.03, 'e3': -0.02\n    }\n    SEED = 314159\n\n    def g_model(z):\n        \"\"\"\n        Computes the deterministic response function g(z).\n        z is expected to be an array of shape (n_samples, d).\n        \"\"\"\n        n_samples, d = z.shape\n        # Pad with zeros to handle dimensions up to 5 consistently.\n        z_pad = np.zeros((n_samples, 5))\n        z_pad[:, :d] = z\n\n        z1, z2, z3, z4, z5 = z_pad.T\n        c = COEFFS\n\n        term_a = c['a0'] + c['a1'] * z1 + c['a2'] * z2 + c['a3'] * z3\n        term_a12_23 = c['a12'] * z1 * z2 + c['a23'] * z2 * z3\n        term_b = c['b1'] * np.tanh(c['c1'] * z1 + c['c2'] * z2)\n        term_d = c['d1'] * z1**2 + c['d2'] * z2**3\n        term_e = c['e1'] * z3 * z4 + c['e2'] * z4**2 + c['e3'] * z5**3\n\n        return term_a + term_a12_23 + term_b + term_d + term_e\n\n    def generate_multi_indices(d, p):\n        \"\"\"\n        Generates total-order multi-indices recursively.\n        \"\"\"\n        if d == 1:\n            return [[i] for i in range(p + 1)]\n        indices = []\n        for i in range(p + 1):\n            sub_indices = generate_multi_indices(d - 1, p - i)\n            for sub_index in sub_indices:\n                indices.append([i] + sub_index)\n        return indices\n\n    def hermite_poly_val(x, k_max):\n        \"\"\"\n        Evaluates probabilists' Hermite polynomials He_k(x) up to degree k_max.\n        x is a 1D array. Returns a matrix of shape (len(x), k_max+1).\n        \"\"\"\n        vals = np.zeros((len(x), k_max + 1))\n        if k_max >= 0:\n            vals[:, 0] = 1.0\n        if k_max >= 1:\n            vals[:, 1] = x\n        for k in range(1, k_max):\n            vals[:, k + 1] = x * vals[:, k] - k * vals[:, k - 1]\n        return vals\n\n    test_cases = [\n        (3, 2, 10),\n        (3, 2, 50),\n        (3, 3, 40),\n        (5, 2, 63),\n        (5, 3, 60),\n    ]\n\n    all_results = []\n    for i, (d, p, n) in enumerate(test_cases):\n        # Use seed with deterministic offset for each case for reproducibility.\n        rng = np.random.default_rng(SEED + i)\n\n        # 1. Generate multi-indices for the basis.\n        alphas = generate_multi_indices(d, p)\n        M = len(alphas)\n\n        # 2. Generate training data.\n        z_train = rng.standard_normal(size=(n, d))\n        y_train = g_model(z_train)\n\n        # 3. Construct the design matrix X.\n        X = np.ones((n, M))\n        \n        # Pre-compute univariate orthonormal basis function values.\n        max_deg = p\n        phi_vals = np.zeros((n, d, max_deg + 1))\n        fact_sqrt = np.sqrt(factorial(np.arange(max_deg + 1)))\n\n        for i_dim in range(d):\n            # hermite_poly_val returns shape (n, max_deg+1) for each dimension.\n            he_vals_dim = hermite_poly_val(z_train[:, i_dim], max_deg)\n            phi_vals[:, i_dim, :] = he_vals_dim / fact_sqrt\n\n        # Populate X using the tensor product structure.\n        for m, alpha in enumerate(alphas):\n            term_prod = np.ones(n)\n            for i_dim in range(d):\n                term_prod *= phi_vals[:, i_dim, alpha[i_dim]]\n            X[:, m] = term_prod\n            \n        # 4. Fit coefficients using Ordinary Least Squares.\n        c_hat, _, _, _ = np.linalg.lstsq(X, y_train, rcond=None)\n\n        # 5. Compute the required uncertainty quantification metrics.\n        y_pred = X @ c_hat\n        rss = np.sum((y_train - y_pred)**2)\n        \n        sigma2_hat = rss / max(n - M, 1)\n        \n        XtX = X.T @ X\n        XtX_pinv = np.linalg.pinv(XtX)\n        \n        cov_c_hat = sigma2_hat * XtX_pinv\n        \n        variances = np.diag(cov_c_hat)\n        v_bar = np.mean(variances)\n        v_max = np.max(variances)\n        \n        kappa = np.linalg.cond(XtX)\n        \n        all_results.append([v_bar, v_max, kappa])\n\n    # Format output as a single string line\n    outer_parts = []\n    for res in all_results:\n        inner_str = f\"[{','.join(map(str, res))}]\"\n        outer_parts.append(inner_str)\n    final_output = f\"[{','.join(outer_parts)}]\"\n    \n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "Once an emulator is built, its ultimate purpose is often to perform uncertainty quantification that would be infeasible with the original simulator. This final practice focuses on the crucial task of uncertainty propagation, where we seek to understand the distribution of a model output given uncertainty in its inputs. Using the fundamental law of total variance, you will derive and implement a method to correctly combine the uncertainty stemming from the inputs with the predictive uncertainty of the emulator itself. This exercise provides a clear, quantitative demonstration of why accounting for emulator uncertainty is essential for producing credible and robust predictions .",
            "id": "3891130",
            "problem": "You are modeling a scalar quantity of interest in environmental and earth system modeling, such as basin-averaged precipitation change, using a statistical surrogate for a computationally expensive simulator. The surrogate emulator is built from a Gaussian Process (GP) emulator (Gaussian Process (GP)) and is used to predict the quantity of interest given a vector of uncertain inputs. You must compute a $0.95$ credible interval that accounts for both input uncertainty and emulator predictive uncertainty, and compare it to an interval that ignores emulator uncertainty.\n\nAssume the following hierarchical predictive model for the scalar output $Y$:\n1. The uncertain inputs are represented by a random vector $X \\in \\mathbb{R}^d$ with a multivariate normal distribution $X \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$.\n2. The emulator provides a predictive mean that is linear in $X$, given by $m(X) = \\beta_0 + \\boldsymbol{\\beta}^\\top X$, where $\\beta_0 \\in \\mathbb{R}$ and $\\boldsymbol{\\beta} \\in \\mathbb{R}^d$.\n3. The emulator predictive variance is modeled as an additive term $v_{\\mathrm{em}}(X) = \\gamma_0 + \\sum_{i=1}^d \\gamma_i X_i^2$, where $\\gamma_0 \\ge 0$ and $\\gamma_i \\ge 0$ for $i=1,\\dots,d$.\n\nLet $Y \\mid X \\sim \\mathcal{N}(m(X), v_{\\mathrm{em}}(X))$. The marginal predictive distribution of $Y$ integrates over the uncertainty in $X$ and combines uncertainty from both $X$ and the emulator. A $0.95$ credible interval for $Y$ that ignores emulator uncertainty is obtained by setting $v_{\\mathrm{em}}(X) = 0$.\n\nStarting only from the fundamental laws of total expectation and total variance and standard properties of the normal distribution, derive an algorithm to compute the $0.95$ credible interval endpoints for $Y$ in millimeters per day (mm/day), rounded to six decimal places, in the two cases: (a) including emulator uncertainty and (b) ignoring emulator uncertainty.\n\nYour program must implement the derivation and compute the intervals for the following test suite. In all cases, the unit of $Y$ is mm/day and all interval endpoints must be expressed in mm/day.\n\n- Test Case A (two-dimensional inputs, correlated, nonzero emulator uncertainty):\n  - $d = 2$,\n  - $\\boldsymbol{\\mu} = [1.0, 0.5]$,\n  - $\\boldsymbol{\\Sigma} = \\begin{bmatrix}0.09  0.03 \\\\ 0.03  0.04\\end{bmatrix}$,\n  - $\\beta_0 = 0.0$, $\\boldsymbol{\\beta} = [2.0, -1.5]$,\n  - $\\gamma_0 = 0.05$, $\\boldsymbol{\\gamma} = [0.02, 0.01]$.\n\n- Test Case B (same inputs, emulator uncertainty set to zero; boundary where intervals coincide if correctly derived):\n  - $d = 2$,\n  - $\\boldsymbol{\\mu} = [1.0, 0.5]$,\n  - $\\boldsymbol{\\Sigma} = \\begin{bmatrix}0.09  0.03 \\\\ 0.03  0.04\\end{bmatrix}$,\n  - $\\beta_0 = 0.0$, $\\boldsymbol{\\beta} = [2.0, -1.5]$,\n  - $\\gamma_0 = 0.0$, $\\boldsymbol{\\gamma} = [0.0, 0.0]$.\n\n- Test Case C (deterministic inputs, emulator variance constant; boundary where ignoring emulator uncertainty yields a degenerate interval):\n  - $d = 2$,\n  - $\\boldsymbol{\\mu} = [0.8, -0.4]$,\n  - $\\boldsymbol{\\Sigma} = \\begin{bmatrix}0.0  0.0 \\\\ 0.0  0.0\\end{bmatrix}$,\n  - $\\beta_0 = 0.3$, $\\boldsymbol{\\beta} = [1.2, 0.5]$,\n  - $\\gamma_0 = 0.2$, $\\boldsymbol{\\gamma} = [0.0, 0.0]$.\n\n- Test Case D (one-dimensional inputs, large emulator variance increasing with $X^2$):\n  - $d = 1$,\n  - $\\boldsymbol{\\mu} = [-0.2]$,\n  - $\\boldsymbol{\\Sigma} = \\begin{bmatrix}0.25\\end{bmatrix}$,\n  - $\\beta_0 = 1.0$, $\\boldsymbol{\\beta} = [1.0]$,\n  - $\\gamma_0 = 0.1$, $\\boldsymbol{\\gamma} = [0.5]$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test-case result is a sublist of four floats in the order $[L_{\\mathrm{with}}, U_{\\mathrm{with}}, L_{\\mathrm{ignore}}, U_{\\mathrm{ignore}}]$, all in mm/day and rounded to six decimal places. For example, the output should look like $[[a,b,c,d],[e,f,g,h],\\dots]$ with no spaces.",
            "solution": "The problem is valid as it is scientifically grounded in statistical theory, well-posed, objective, and provides a complete and consistent set of information for deriving a solution. The task is to compute a $0.95$ credible interval for a scalar quantity of interest, $Y$, based on a hierarchical predictive model. The marginal distribution of $Y$ must account for uncertainty in the model inputs, $X$, and uncertainty in the statistical emulator used to predict $Y$. We will first derive the marginal mean and variance of $Y$ and then use these moments to construct the credible interval, assuming the marginal distribution of $Y$ can be approximated by a normal distribution.\n\nThe model is defined by the following hierarchy:\n1.  Input uncertainty: The input vector $X \\in \\mathbb{R}^d$ follows a multivariate normal distribution, $X \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$.\n2.  Emulator model: The conditional distribution of the output $Y$ given the inputs $X$ is normal, $Y \\mid X \\sim \\mathcal{N}(m(X), v_{\\mathrm{em}}(X))$, where the predictive mean is $m(X) = \\beta_0 + \\boldsymbol{\\beta}^\\top X$ and the predictive variance is $v_{\\mathrm{em}}(X) = \\gamma_0 + \\sum_{i=1}^d \\gamma_i X_i^2$.\n\nOur objective is to find the parameters of the marginal distribution of $Y$, which is obtained by integrating out the input vector $X$. The credible interval is then constructed based on the marginal mean $\\mathbb{E}[Y]$ and the marginal variance $\\mathrm{Var}(Y)$.\n\n**Derivation of the Marginal Mean, $\\mathbb{E}[Y]$**\n\nWe apply the law of total expectation, which states that $\\mathbb{E}[Y] = \\mathbb{E}_X[\\mathbb{E}[Y \\mid X]]$.\nThe inner expectation is the mean of the conditional distribution of $Y$ given $X$, which is the emulator's mean function:\n$$\n\\mathbb{E}[Y \\mid X] = m(X) = \\beta_0 + \\boldsymbol{\\beta}^\\top X\n$$\nThe outer expectation is taken over the distribution of $X$. Since expectation is a linear operator:\n$$\n\\mathbb{E}[Y] = \\mathbb{E}_X[\\beta_0 + \\boldsymbol{\\beta}^\\top X] = \\beta_0 + \\boldsymbol{\\beta}^\\top \\mathbb{E}_X[X]\n$$\nGiven that $X \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$, we have $\\mathbb{E}_X[X] = \\boldsymbol{\\mu}$. Therefore, the marginal mean of $Y$ is:\n$$\n\\mu_Y = \\mathbb{E}[Y] = \\beta_0 + \\boldsymbol{\\beta}^\\top \\boldsymbol{\\mu}\n$$\nThis result is independent of the emulator variance $v_{\\mathrm{em}}(X)$ and thus holds for both cases considered in the problem.\n\n**Derivation of the Marginal Variance, $\\mathrm{Var}(Y)$**\n\nWe apply the law of total variance, which decomposes the total variance of $Y$ into two components:\n$$\n\\mathrm{Var}(Y) = \\mathbb{E}_X[\\mathrm{Var}(Y \\mid X)] + \\mathrm{Var}_X(\\mathbb{E}[Y \\mid X])\n$$\nLet us evaluate each term separately.\n\n1.  **First Term: $\\mathbb{E}_X[\\mathrm{Var}(Y \\mid X)]$**\n    This term represents the contribution from the emulator's predictive uncertainty, averaged over the distribution of the inputs $X$. The conditional variance of $Y$ given $X$ is given by the emulator's predictive variance function:\n    $$\n    \\mathrm{Var}(Y \\mid X) = v_{\\mathrm{em}}(X) = \\gamma_0 + \\sum_{i=1}^d \\gamma_i X_i^2\n    $$\n    We take the expectation of this expression with respect to $X$:\n    $$\n    \\mathbb{E}_X[\\mathrm{Var}(Y \\mid X)] = \\mathbb{E}_X\\left[\\gamma_0 + \\sum_{i=1}^d \\gamma_i X_i^2\\right] = \\gamma_0 + \\sum_{i=1}^d \\gamma_i \\mathbb{E}_X[X_i^2]\n    $$\n    For any random variable $Z$, we know that $\\mathbb{E}[Z^2] = \\mathrm{Var}(Z) + (\\mathbb{E}[Z])^2$. For the $i$-th component of the random vector $X$, we have $\\mathbb{E}[X_i] = \\mu_i$ (the $i$-th element of $\\boldsymbol{\\mu}$) and $\\mathrm{Var}(X_i) = \\Sigma_{ii}$ (the $i$-th diagonal element of $\\boldsymbol{\\Sigma}$).\n    Substituting this into our expression gives:\n    $$\n    \\mathbb{E}_X[X_i^2] = \\Sigma_{ii} + \\mu_i^2\n    $$\n    Therefore, the first term of the total variance is:\n    $$\n    \\mathbb{E}_X[\\mathrm{Var}(Y \\mid X)] = \\gamma_0 + \\sum_{i=1}^d \\gamma_i (\\Sigma_{ii} + \\mu_i^2)\n    $$\n\n2.  **Second Term: $\\mathrm{Var}_X(\\mathbb{E}[Y \\mid X])$**\n    This term represents the uncertainty propagated from the inputs $X$ through the emulator's mean function $m(X)$. We need to calculate the variance of $\\mathbb{E}[Y \\mid X] = m(X) = \\beta_0 + \\boldsymbol{\\beta}^\\top X$ with respect to $X$:\n    $$\n    \\mathrm{Var}_X(\\mathbb{E}[Y \\mid X]) = \\mathrm{Var}_X(\\beta_0 + \\boldsymbol{\\beta}^\\top X) = \\mathrm{Var}_X(\\boldsymbol{\\beta}^\\top X)\n    $$\n    For a random vector $X$ with covariance matrix $\\boldsymbol{\\Sigma}$, the variance of a linear transformation $\\boldsymbol{\\beta}^\\top X$ is given by the quadratic form $\\boldsymbol{\\beta}^\\top \\boldsymbol{\\Sigma} \\boldsymbol{\\beta}$.\n    Thus, the second term is:\n    $$\n    \\mathrm{Var}_X(\\mathbb{E}[Y \\mid X]) = \\boldsymbol{\\beta}^\\top \\boldsymbol{\\Sigma} \\boldsymbol{\\beta}\n    $$\n\n**Total Marginal Variance and Interval Construction**\n\nCombining the two terms, the total marginal variance of $Y$ is:\n$$\n\\sigma_Y^2 = \\mathrm{Var}(Y) = \\left( \\gamma_0 + \\sum_{i=1}^d \\gamma_i (\\Sigma_{ii} + \\mu_i^2) \\right) + \\boldsymbol{\\beta}^\\top \\boldsymbol{\\Sigma} \\boldsymbol{\\beta}\n$$\n\nWe now define the variance for the two cases required:\n\n**(a) Including Emulator Uncertainty:**\nThe variance is the total variance derived above:\n$$\n\\sigma_{\\mathrm{with}}^2 = \\left( \\gamma_0 + \\sum_{i=1}^d \\gamma_i (\\Sigma_{ii} + \\mu_i^2) \\right) + \\boldsymbol{\\beta}^\\top \\boldsymbol{\\Sigma} \\boldsymbol{\\beta}\n$$\n\n**(b) Ignoring Emulator Uncertainty:**\nThis case corresponds to setting the emulator variance $v_{\\mathrm{em}}(X)$ to $0$. This is equivalent to setting $\\gamma_0=0$ and all $\\gamma_i=0$ for $i=1,\\dots,d$. The first term of the total variance vanishes, leaving only the variance propagated through the mean function:\n$$\n\\sigma_{\\mathrm{ignore}}^2 = \\boldsymbol{\\beta}^\\top \\boldsymbol{\\Sigma} \\boldsymbol{\\beta}\n$$\n\n**Credible Interval Calculation**\nApproximating the marginal distribution of $Y$ as a normal distribution $\\mathcal{N}(\\mu_Y, \\sigma_Y^2)$, a $0.95$ credible interval for $Y$ is given by $[\\mu_Y - z_{0.975} \\sigma_Y, \\mu_Y + z_{0.975} \\sigma_Y]$, where $z_{0.975}$ is the $0.975$ quantile of the standard normal distribution, approximately $1.959964$.\n\nThe endpoints for the two cases are:\n*   **With uncertainty:** $L_{\\mathrm{with}} = \\mu_Y - z_{0.975} \\sigma_{\\mathrm{with}}$ and $U_{\\mathrm{with}} = \\mu_Y + z_{0.975} \\sigma_{\\mathrm{with}}$.\n*   **Ignoring uncertainty:** $L_{\\mathrm{ignore}} = \\mu_Y - z_{0.975} \\sigma_{\\mathrm{ignore}}$ and $U_{\\mathrm{ignore}} = \\mu_Y + z_{0.975} \\sigma_{\\mathrm{ignore}}$.\n\nThe following program implements this derived algorithm.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Computes 0.95 credible intervals for a scalar output from a GP emulator,\n    accounting for input and emulator uncertainty.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test Case A\n        {\n            \"d\": 2, \"mu\": [1.0, 0.5], \"Sigma\": [[0.09, 0.03], [0.03, 0.04]],\n            \"beta0\": 0.0, \"beta\": [2.0, -1.5], \"gamma0\": 0.05, \"gamma\": [0.02, 0.01]\n        },\n        # Test Case B\n        {\n            \"d\": 2, \"mu\": [1.0, 0.5], \"Sigma\": [[0.09, 0.03], [0.03, 0.04]],\n            \"beta0\": 0.0, \"beta\": [2.0, -1.5], \"gamma0\": 0.0, \"gamma\": [0.0, 0.0]\n        },\n        # Test Case C\n        {\n            \"d\": 2, \"mu\": [0.8, -0.4], \"Sigma\": [[0.0, 0.0], [0.0, 0.0]],\n            \"beta0\": 0.3, \"beta\": [1.2, 0.5], \"gamma0\": 0.2, \"gamma\": [0.0, 0.0]\n        },\n        # Test Case D\n        {\n            \"d\": 1, \"mu\": [-0.2], \"Sigma\": [[0.25]],\n            \"beta0\": 1.0, \"beta\": [1.0], \"gamma0\": 0.1, \"gamma\": [0.5]\n        }\n    ]\n\n    results = []\n    # Standard normal quantile for a 95% interval\n    z_quantile = norm.ppf(1 - (1 - 0.95) / 2)\n\n    for case in test_cases:\n        # Extract and convert parameters to numpy arrays\n        mu = np.array(case[\"mu\"])\n        Sigma = np.array(case[\"Sigma\"])\n        beta0 = case[\"beta0\"]\n        beta = np.array(case[\"beta\"])\n        gamma0 = case[\"gamma0\"]\n        gamma = np.array(case[\"gamma\"])\n\n        # --- DERIVATION IMPLEMENTATION ---\n\n        # 1. Calculate the marginal mean of Y, which is common to both cases.\n        # E[Y] = beta0 + beta.T @ E[X] = beta0 + beta.T @ mu\n        mu_Y = beta0 + beta.T @ mu\n\n        # 2. Calculate the variance for the case IGNORING emulator uncertainty.\n        # This is the variance of the emulator mean function, Var(m(X)).\n        # Var_X(E[Y|X]) = Var_X(beta0 + beta.T @ X) = beta.T @ Var(X) @ beta\n        var_ignore = beta.T @ Sigma @ beta\n\n        # 3. Calculate the variance for the case INCLUDING emulator uncertainty.\n        # This adds the average emulator variance, E[Var(Y|X)].\n        # E_X[v_em(X)] = E_X[gamma0 + sum(gamma_i * X_i^2)]\n        #             = gamma0 + sum(gamma_i * E[X_i^2])\n        # E[X_i^2] = Var(X_i) + (E[X_i])^2 = Sigma_ii + mu_i^2\n        E_v_em = gamma0 + gamma.T @ (np.diag(Sigma) + mu**2)\n        var_with = E_v_em + var_ignore\n\n        # 4. Calculate standard deviations\n        std_dev_with = np.sqrt(var_with)\n        std_dev_ignore = np.sqrt(var_ignore)\n\n        # 5. Compute interval endpoints for both cases\n        L_with = mu_Y - z_quantile * std_dev_with\n        U_with = mu_Y + z_quantile * std_dev_with\n        L_ignore = mu_Y - z_quantile * std_dev_ignore\n        U_ignore = mu_Y + z_quantile * std_dev_ignore\n\n        # 6. Format the results for the current case\n        case_result = [\n            round(L_with, 6),\n            round(U_with, 6),\n            round(L_ignore, 6),\n            round(U_ignore, 6)\n        ]\n        results.append(case_result)\n\n    # Final print statement in the exact required format.\n    # The str() representation of a list of lists creates the desired format\n    # with spaces, which are then removed.\n    print(str(results).replace(\" \", \"\"))\n\nsolve()\n```"
        }
    ]
}