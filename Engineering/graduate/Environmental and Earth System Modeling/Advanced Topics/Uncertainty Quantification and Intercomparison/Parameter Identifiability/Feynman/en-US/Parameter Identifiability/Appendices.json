{
    "hands_on_practices": [
        {
            "introduction": "This first exercise provides a foundational look at structural non-identifiability. Using a simple linear model, we will see how multiple, distinct parameter sets can produce identical model outputs, violating the one-to-one mapping required for unique estimation. This practice will guide you through diagnosing this issue from first principles and resolving it via reparameterization, a crucial first step in model analysis .",
            "id": "3904624",
            "problem": "An environmental monitoring station measures streamflow response to rainfall using a lumped linear input–output relationship. Let the measured output be $y(t)$ and the known exogenous input be $u(t)$, where $t$ denotes time. The model used by the analyst is\n$$\ny(t) \\;=\\; \\theta_{1}\\,\\theta_{2}\\,u(t),\n$$\nwhere $\\theta_{1}$ represents an effective runoff coefficient and $\\theta_{2}$ represents a calibration gain from instrumentation, both assumed constant in time. Measurements are taken at $N$ distinct times $\\{t_{i}\\}_{i=1}^{N}$, and the recorded data obey\n$$\ny_{i} \\;=\\; \\theta_{1}\\,\\theta_{2}\\,u(t_{i}) \\;+\\; \\varepsilon_{i}, \\quad i \\in \\{1,\\dots,N\\},\n$$\nwith $\\varepsilon_{i}$ being independent, identically distributed zero-mean Gaussian noise with variance $\\sigma^{2}$.\n\nStarting from first principles of structural identifiability in parameter estimation, and using the definition that a parameter (or function of parameters) is structurally identifiable if the input–output mapping is one-to-one with respect to that parameter under ideal noise-free conditions and known input, do the following:\n\n- Explain whether $\\theta_{1}$ and $\\theta_{2}$ are individually structurally identifiable under this model and input, and justify your conclusion using the injectivity of the parameter-to-output map.\n- Use the sensitivity matrix and the Fisher Information Matrix (FIM) under the Gaussian assumption to assess the rank properties of the information content for $(\\theta_{1},\\theta_{2})$ versus a scalar reparameterization. Explicitly construct the sensitivity vectors and show the resulting rank.\n- Propose a reparameterization that expresses the model in terms of a single structurally identifiable parameter $\\phi$ and write the reparameterized model. Justify why $\\phi$ is structurally identifiable by appealing to the one-to-one mapping from $\\phi$ to the output for nontrivial input.\n- Provide the single identifiable parameter $\\phi$ explicitly as a closed-form expression in terms of $\\theta_{1}$ and $\\theta_{2}$.\n\nYour final answer must be the closed-form expression for $\\phi$ only. Do not include any units in your final answer. Do not provide any inequalities or equations as your final answer beyond the single expression for $\\phi$.",
            "solution": "The validity of the problem statement is confirmed. It is scientifically grounded, well-posed, and objective, presenting a standard problem in system identification and parameter estimation. All necessary information is provided, and the problem is free of contradictions or ambiguities.\n\nThe problem asks for an analysis of the structural identifiability of the parameters in the linear model $y(t) = \\theta_{1}\\theta_{2}u(t)$. Structural identifiability concerns whether the model parameters can be uniquely determined from perfect, noise-free input-output data.\n\nThe analysis will proceed in three stages as requested: first, assessing the individual identifiability of $\\theta_{1}$ and $\\theta_{2}$ using the definition of injectivity; second, using the sensitivity and Fisher Information Matrix (FIM) to analyze the information content; and third, proposing and justifying a reparameterization into a single identifiable parameter.\n\n**1. Individual Structural Identifiability of $\\theta_{1}$ and $\\theta_{2}$**\n\nStructural identifiability is assessed under ideal conditions of noise-free data, i.e., $\\varepsilon_{i} = 0$ for all $i$. The model is given by the input-output mapping $y(t) = f(\\boldsymbol{\\theta}, u(t))$, where the parameter vector is $\\boldsymbol{\\theta} = (\\theta_{1}, \\theta_{2})$. The parameters are structurally identifiable if the mapping from the parameters to the output is one-to-one (injective) for a given known, non-trivial input $u(t)$.\n\nLet's test for injectivity. Suppose we have two distinct parameter vectors, $\\boldsymbol{\\theta} = (\\theta_{1}, \\theta_{2})$ and $\\boldsymbol{\\theta}' = (\\theta'_{1}, \\theta'_{2})$. We check if observing the same output, $y(t, \\boldsymbol{\\theta}) = y(t, \\boldsymbol{\\theta}')$ for all $t$, necessarily implies that $\\boldsymbol{\\theta} = \\boldsymbol{\\theta}'$.\nThe condition $y(t, \\boldsymbol{\\theta}) = y(t, \\boldsymbol{\\theta}')$ translates to:\n$$\n\\theta_{1}\\theta_{2}u(t) = \\theta'_{1}\\theta'_{2}u(t)\n$$\nFor a non-trivial input, there exists at least one time $t$ for which $u(t) \\neq 0$. At such a time, we can divide by $u(t)$ to obtain:\n$$\n\\theta_{1}\\theta_{2} = \\theta'_{1}\\theta'_{2}\n$$\nThis equation shows that the model output depends only on the product of the parameters, $\\theta_{1}\\theta_{2}$. It does not require $\\theta_{1} = \\theta'_{1}$ and $\\theta_{2} = \\theta'_{2}$. For any given pair $(\\theta_{1}, \\theta_{2})$, one can find infinitely many other pairs $(\\theta'_{1}, \\theta'_{2})$ that yield the same product and thus the same model output. For example, if $\\boldsymbol{\\theta} = (2, 3)$, the product is $6$. Another parameter vector, $\\boldsymbol{\\theta}' = (1.5, 4)$, also has a product of $6$. Since $\\boldsymbol{\\theta} \\neq \\boldsymbol{\\theta}'$ but $y(t, \\boldsymbol{\\theta}) = y(t, \\boldsymbol{\\theta}')$, the mapping from the parameter vector $\\boldsymbol{\\theta}$ to the output $y(t)$ is not injective. Therefore, the parameters $\\theta_{1}$ and $\\theta_{2}$ are not individually structurally identifiable. Only the functional combination $\\theta_{1}\\theta_{2}$ is identifiable.\n\n**2. Sensitivity Matrix and Fisher Information Matrix (FIM) Analysis**\n\nThis lack of identifiability can also be demonstrated by analyzing the rank of the sensitivity matrix and the Fisher Information Matrix (FIM). The sensitivity of the noise-free output $y(t)$ with respect to the parameters indicates how the output changes with an infinitesimal change in each parameter. The sensitivity vector at time $t$ for the parameter vector $\\boldsymbol{\\theta} = (\\theta_{1}, \\theta_{2})$ is:\n$$\n\\mathbf{s}(t) = \\begin{pmatrix} \\frac{\\partial y(t)}{\\partial \\theta_{1}} & \\frac{\\partial y(t)}{\\partial \\theta_{2}} \\end{pmatrix}\n$$\nCalculating the partial derivatives of $y(t) = \\theta_{1}\\theta_{2}u(t)$:\n$$\n\\frac{\\partial y(t)}{\\partial \\theta_{1}} = \\theta_{2}u(t)\n$$\n$$\n\\frac{\\partial y(t)}{\\partial \\theta_{2}} = \\theta_{1}u(t)\n$$\nSo, the sensitivity vector is $\\mathbf{s}(t) = \\begin{pmatrix} \\theta_{2}u(t) & \\theta_{1}u(t) \\end{pmatrix}$.\n\nThe sensitivity matrix, $\\mathbf{J}$, is constructed by stacking these sensitivity vectors for each of the $N$ measurement times $\\{t_{i}\\}_{i=1}^{N}$:\n$$\n\\mathbf{J} = \\begin{pmatrix} \\frac{\\partial y_{1}}{\\partial \\theta_{1}} & \\frac{\\partial y_{1}}{\\partial \\theta_{2}} \\\\ \\vdots & \\vdots \\\\ \\frac{\\partial y_{N}}{\\partial \\theta_{1}} & \\frac{\\partial y_{N}}{\\partial \\theta_{2}} \\end{pmatrix} = \\begin{pmatrix} \\theta_{2}u(t_{1}) & \\theta_{1}u(t_{1}) \\\\ \\vdots & \\vdots \\\\ \\theta_{2}u(t_{N}) & \\theta_{1}u(t_{N}) \\end{pmatrix}\n$$\nThe first column of $\\mathbf{J}$ is the vector $\\theta_{2}\\mathbf{u}$, and the second column is $\\theta_{1}\\mathbf{u}$, where $\\mathbf{u} = (u(t_{1}), \\dots, u(t_{N}))^{T}$. Assuming $\\theta_{1}$ and $\\theta_{2}$ are non-zero, the second column is a scalar multiple of the first column: $(\\text{column } 2) = \\frac{\\theta_{1}}{\\theta_{2}} (\\text{column } 1)$. The columns of $\\mathbf{J}$ are linearly dependent. Therefore, the rank of the sensitivity matrix $\\mathbf{J}$ is $1$, provided that the input vector $\\mathbf{u}$ is not the zero vector. A rank of $1$ is less than the number of parameters ($2$), which indicates structural non-identifiability.\n\nUnder the assumption of i.i.d. Gaussian noise, the Fisher Information Matrix (FIM) is given by $\\mathbf{F} = \\frac{1}{\\sigma^{2}}\\mathbf{J}^{T}\\mathbf{J}$. The FIM is a $2 \\times 2$ matrix:\n$$\n\\mathbf{F} = \\frac{1}{\\sigma^{2}} \\begin{pmatrix} \\theta_{2}u(t_{1}) & \\dots & \\theta_{2}u(t_{N}) \\\\ \\theta_{1}u(t_{1}) & \\dots & \\theta_{1}u(t_{N}) \\end{pmatrix} \\begin{pmatrix} \\theta_{2}u(t_{1}) & \\theta_{1}u(t_{1}) \\\\ \\vdots & \\vdots \\\\ \\theta_{2}u(t_{N}) & \\theta_{1}u(t_{N}) \\end{pmatrix}\n$$\n$$\n\\mathbf{F} = \\frac{1}{\\sigma^{2}} \\begin{pmatrix} \\sum_{i=1}^{N} (\\theta_{2}u(t_{i}))^{2} & \\sum_{i=1}^{N} (\\theta_{2}u(t_{i}))(\\theta_{1}u(t_{i})) \\\\ \\sum_{i=1}^{N} (\\theta_{1}u(t_{i}))(\\theta_{2}u(t_{i})) & \\sum_{i=1}^{N} (\\theta_{1}u(t_{i}))^{2} \\end{pmatrix}\n$$\nFactoring out the parameters and the sum over inputs:\n$$\n\\mathbf{F} = \\frac{\\sum_{i=1}^{N} u(t_{i})^{2}}{\\sigma^{2}} \\begin{pmatrix} \\theta_{2}^{2} & \\theta_{1}\\theta_{2} \\\\ \\theta_{1}\\theta_{2} & \\theta_{1}^{2} \\end{pmatrix}\n$$\nThe rank of the FIM is determined by the rank of the matrix component. The determinant of this matrix is $\\det(\\begin{pmatrix} \\theta_{2}^{2} & \\theta_{1}\\theta_{2} \\\\ \\theta_{1}\\theta_{2} & \\theta_{1}^{2} \\end{pmatrix}) = (\\theta_{2}^{2})(\\theta_{1}^{2}) - (\\theta_{1}\\theta_{2})(\\theta_{1}\\theta_{2}) = 0$. A matrix with a determinant of zero is singular. Since the FIM is not the zero matrix (assuming non-trivial input and non-zero parameters), its rank is $1$. A rank-deficient FIM confirms that the parameters $\\theta_1$ and $\\theta_2$ cannot be uniquely estimated.\n\n**3. Reparameterization and Justification**\n\nThe analysis above shows that while $\\theta_{1}$ and $\\theta_{2}$ are not individually identifiable, their product is. This suggests a reparameterization of the model in terms of a single, identifiable parameter. Let's define a new parameter $\\phi$ as the product of the original parameters:\n$$\n\\phi = \\theta_{1}\\theta_{2}\n$$\nThe reparameterized model is:\n$$\ny(t) = \\phi u(t)\n$$\nTo justify that $\\phi$ is structurally identifiable, we again appeal to the definition of a one-to-one mapping. Let us consider two different parameter values, $\\phi$ and $\\phi'$. If they produce the same output for a given non-trivial input $u(t)$, we have:\n$$\n\\phi u(t) = \\phi' u(t)\n$$\nFor a non-trivial input, there exists a time $t$ such that $u(t) \\neq 0$. At this time, we can divide by $u(t)$ to find:\n$$\n\\phi = \\phi'\n$$\nThis demonstrates that if two parameter values produce the same output, the parameter values must be identical. The mapping from the parameter $\\phi$ to the output $y(t)$ is injective. Therefore, $\\phi$ is structurally identifiable.\n\nThe single identifiable parameter $\\phi$ is explicitly given as the closed-form expression in terms of $\\theta_{1}$ and $\\theta_{2}$ by the definition used for the reparameterization.",
            "answer": "$$\\boxed{\\theta_{1}\\theta_{2}}$$"
        },
        {
            "introduction": "Building on the previous example, this practice explores identifiability in a simple dynamic system representing biogeochemical decay. Here, you will see how parameters governing parallel processes can become unidentifiable, with only their sum being constrained by observable data. This exercise bridges theory with practice by requiring you to not only analyze the model's structural properties but also to use a synthetic dataset to estimate the identifiable parameter combination through maximum likelihood estimation .",
            "id": "3904642",
            "problem": "In a well-mixed surface ocean parcel, consider a single observed pool of dissolved organic carbon with concentration $C(t)$ (in $\\mathrm{mg\\ C\\ L^{-1}}$). The pool loses mass through two parallel, unobserved processes: microbial mineralization with first-order rate constant $k_{m}$ (in $\\mathrm{day^{-1}}$) and photodegradation with first-order rate constant $k_{p}$ (in $\\mathrm{day^{-1}}$). Only $C(t)$ is measured at discrete times, and the product pools are not measured. Assume the parcel volume is constant and there are no external sources or sinks for carbon other than these two processes.\n\nStarting from mass balance and the definition of first-order kinetics, derive the deterministic expression for $C(t)$ in terms of $C(0)$, $k_{m}$, and $k_{p}$. Using this expression and assuming that measurement errors are multiplicative and log-normal, i.e., the logarithm of measured concentrations satisfies\n$$\n\\ln\\big(C_{\\text{obs}}(t_{i})\\big) \\;=\\; \\ln\\big(C(t_{i})\\big) \\;+\\; \\varepsilon_{i},\n$$\nwith $\\varepsilon_{i}$ independent and identically distributed (i.i.d.) Gaussian of mean $0$ and variance $\\sigma^{2}$, show that only the sum $K \\equiv k_{m} + k_{p}$ is structurally identifiable from $C(t)$ data alone. Then, under the assumption that $C(0)$ is known exactly, derive the maximum likelihood estimator (MLE) for $K$ and compute its numerical value using the following time series measurements:\n- Times $t_{i}$ (in $\\mathrm{days}$): $0$, $1$, $3$, $5$, $7$.\n- Observed concentrations $C_{\\text{obs}}(t_{i})$ (in $\\mathrm{mg\\ C\\ L^{-1}}$): $100$, $79.5$, $50.2$, $31.6$, $20.0$.\n\nRound your numerical answer to four significant figures. Express the final rate constant in $\\mathrm{day^{-1}}$.",
            "solution": "The parcel is well mixed, and the only losses are the two parallel first-order processes. The mass balance for $C(t)$ is given by the sum of the first-order loss terms:\n$$\n\\frac{d C(t)}{d t} \\;=\\; -k_{m} C(t) \\;-\\; k_{p} C(t) \\;=\\; -\\big(k_{m} + k_{p}\\big)\\, C(t).\n$$\nDefine the combined rate constant $K \\equiv k_{m} + k_{p}$. The ordinary differential equation becomes\n$$\n\\frac{d C(t)}{d t} \\;=\\; -K\\, C(t),\n$$\nwhich has the solution, using separation of variables and the initial condition $C(0)$,\n$$\nC(t) \\;=\\; C(0)\\, \\exp\\big(-K t\\big).\n$$\nThus the forward model for the observed concentration, in the absence of measurement error, depends on $k_{m}$ and $k_{p}$ only through their sum $K$. This immediately implies structural identifiability of $K$ but not of $k_{m}$ and $k_{p}$ individually, because any pair $(k_{m}, k_{p})$ producing the same $K$ yields the same $C(t)$.\n\nTo formalize this identifiability claim, consider the sensitivity of $C(t)$ with respect to the parameters. The partial derivatives are\n$$\n\\frac{\\partial C(t)}{\\partial k_{m}} \\;=\\; -t\\, C(0)\\, \\exp(-K t), \\qquad\n\\frac{\\partial C(t)}{\\partial k_{p}} \\;=\\; -t\\, C(0)\\, \\exp(-K t).\n$$\nThe two sensitivity functions are identical for all $t$, so the sensitivity matrix across times has two identical columns. The Fisher Information Matrix (FIM) for $(k_{m}, k_{p})$ under additive Gaussian errors on $\\ln C$ is, up to a positive scalar factor,\n$$\n\\mathbf{I} \\;=\\; \\sum_{i} \\left( \\begin{array}{cc}\n\\left(\\frac{\\partial \\ln C(t_{i})}{\\partial k_{m}}\\right)^{2} & \\frac{\\partial \\ln C(t_{i})}{\\partial k_{m}} \\frac{\\partial \\ln C(t_{i})}{\\partial k_{p}} \\\\\n\\frac{\\partial \\ln C(t_{i})}{\\partial k_{m}} \\frac{\\partial \\ln C(t_{i})}{\\partial k_{p}} & \\left(\\frac{\\partial \\ln C(t_{i})}{\\partial k_{p}}\\right)^{2}\n\\end{array} \\right) \\;=\\; \\sum_{i} t_{i}^{2} \\left( \\begin{array}{cc} 1 & 1 \\\\ 1 & 1 \\end{array} \\right),\n$$\nwhich is rank-$1$, with one zero eigenvalue. Therefore, only the direction proportional to $(1,1)$—that is, the sum $K$—is identifiable.\n\nNext, derive the Maximum Likelihood Estimator (MLE) for $K$ under log-normal errors with known $C(0)$. The observation model in log-space is\n$$\n\\ln\\big(C_{\\text{obs}}(t_{i})\\big) \\;=\\; \\ln\\big(C(t_{i})\\big) \\;+\\; \\varepsilon_{i}\n\\;=\\; \\ln\\big(C(0)\\big) \\;-\\; K t_{i} \\;+\\; \\varepsilon_{i},\n$$\nwith $\\varepsilon_{i} \\sim \\mathcal{N}(0,\\sigma^{2})$ i.i.d. The log-likelihood is proportional to the negative sum of squared residuals in log-space. Because $\\ln\\big(C(0)\\big)$ is known, the MLE for $K$ reduces to the least squares estimator in a simple linear regression with fixed intercept:\n$$\n\\hat{K} \\;=\\; \\frac{\\sum_{i} t_{i} \\left( \\ln\\big(C(0)\\big) \\;-\\; \\ln\\big(C_{\\text{obs}}(t_{i})\\big) \\right)}{\\sum_{i} t_{i}^{2}}.\n$$\nCompute $\\hat{K}$ using the provided data. First, record the times and observed concentrations:\n- $t_{i}$: $0$, $1$, $3$, $5$, $7$.\n- $C_{\\text{obs}}(t_{i})$: $100$, $79.5$, $50.2$, $31.6$, $20.0$.\nWe will use only the terms with $t_{i} > 0$, since $t_{i} = 0$ contributes zero to both numerator and denominator. Compute the denominator:\n$$\n\\sum_{i} t_{i}^{2} \\;=\\; 1^{2} \\;+\\; 3^{2} \\;+\\; 5^{2} \\;+\\; 7^{2} \\;=\\; 1 \\;+\\; 9 \\;+\\; 25 \\;+\\; 49 \\;=\\; 84.\n$$\nCompute the numerator $\\sum_{i} t_{i}\\left(\\ln C(0) - \\ln C_{\\text{obs}}(t_{i})\\right)$ term by term. First, $\\ln\\big(C(0)\\big) = \\ln(100) = 4.605170186$.\n\nFor $t_{1} = 1$ and $C_{\\text{obs}}(1) = 79.5$:\n$$\n\\ln(79.5) \\approx 4.375756635,\\quad \\ln(100) - \\ln(79.5) \\approx 0.229413551,\\quad \\text{contribution} = 1 \\times 0.229413551 = 0.229413551.\n$$\nFor $t_{2} = 3$ and $C_{\\text{obs}}(3) = 50.2$:\n$$\n\\ln(50.2) \\approx 3.916015000,\\quad \\ln(100) - \\ln(50.2) \\approx 0.689155186,\\quad \\text{contribution} = 3 \\times 0.689155186 = 2.067465558.\n$$\nFor $t_{3} = 5$ and $C_{\\text{obs}}(5) = 31.6$:\n$$\n\\ln(31.6) \\approx 3.453157777,\\quad \\ln(100) - \\ln(31.6) \\approx 1.152012409,\\quad \\text{contribution} = 5 \\times 1.152012409 = 5.760062045.\n$$\nFor $t_{4} = 7$ and $C_{\\text{obs}}(7) = 20.0$:\n$$\n\\ln(20.0) = 2.995732274,\\quad \\ln(100) - \\ln(20.0) = 1.609437912,\\quad \\text{contribution} = 7 \\times 1.609437912 = 11.266065384.\n$$\nSum the contributions:\n$$\n\\sum_{i} t_{i}\\left(\\ln C(0) - \\ln C_{\\text{obs}}(t_{i})\\right) \\approx 0.229413551 + 2.067465558 + 5.760062045 + 11.266065384 = 19.323006538.\n$$\nTherefore,\n$$\n\\hat{K} \\;=\\; \\frac{19.323006538}{84} \\;\\approx\\; 0.230035792.\n$$\nRounded to four significant figures, the estimate is\n$$\n\\hat{K} \\;\\approx\\; 0.2300,\n$$\nto be expressed in $\\mathrm{day^{-1}}$.\n\nThis construction demonstrates that $k_{m}$ and $k_{p}$ are perfectly correlated in the sense of structural identifiability from $C(t)$ alone, and only their sum $K$ is identifiable. The numerical MLE for $K$ under the provided data and assumptions is approximately $0.2300$.",
            "answer": "$$\\boxed{0.2300}$$"
        },
        {
            "introduction": "Many complex environmental models are \"sloppy,\" a term indicating that their behavior is sensitive to only a few combinations of their many parameters. This advanced practice introduces a powerful technique for diagnosing such sloppiness by analyzing the eigenspectrum of the Fisher Information Matrix (FIM). By interpreting the eigenvalues as measures of parameter constraint, you will learn to distinguish well-constrained (\"stiff\") from poorly-constrained (\"sloppy\") directions in parameter space and propose a more robust, reduced model .",
            "id": "3904551",
            "problem": "Consider a linearized single-box carbon cycle model used in environmental and earth system modeling to interpret atmospheric carbon dioxide time series. The box represents the atmospheric carbon stock, with fluxes parameterized by three unknown parameters: photosynthetic uptake sensitivity $a$, heterotrophic respiration rate coefficient $b$, and lateral export coefficient $c$. Under small perturbations around a nominal trajectory, the model-predicted observation vector $\\mathbf{y} \\in \\mathbb{R}^{n}$ can be approximated by a first-order Taylor expansion in the parameter vector $\\boldsymbol{\\theta} = (a, b, c)$, as $\\mathbf{y}(\\boldsymbol{\\theta}) \\approx \\mathbf{y}(\\boldsymbol{\\theta}_{0}) + \\mathbf{S}(\\boldsymbol{\\theta} - \\boldsymbol{\\theta}_{0})$, where $\\mathbf{S} \\in \\mathbb{R}^{n \\times 3}$ is the sensitivity matrix whose $i$-th column contains the partial derivatives of the model outputs with respect to the $i$-th parameter. Assume additive independent Gaussian observation noise with variance $\\sigma^{2}$ in each component. In this setting, the Fisher Information Matrix (FIM) $\\mathbf{F}$ associated with $\\boldsymbol{\\theta}$ under Gaussian errors is given by $\\mathbf{F} = \\sigma^{-2} \\mathbf{S}^{\\top} \\mathbf{S}$.\n\nA data assimilation experiment yields the following symmetric positive semidefinite Fisher Information Matrix for the ordered parameter vector $(a, b, c)$:\n$$\n\\mathbf{F} = \n\\begin{pmatrix}\n100 & 95 & 0 \\\\\n95 & 100 & 0 \\\\\n0 & 0 & 0.01\n\\end{pmatrix}.\n$$\nAssume the noise variance is $\\sigma^{2} = 1$, so the matrix above is already scaled by $\\sigma^{-2}$. Define an identifiability threshold based on a curvature criterion: retain only those parameter directions whose corresponding FIM eigenvalues are at least $\\tau = 10$. Using first principles of likelihood-based inference and eigenanalysis, determine a reduced parameter set by projecting onto the identifiable subspace of the FIM. Specifically, compute the unit-norm linear combination vector of $(a, b, c)$ associated with the identifiable direction(s) as indicated by the threshold $\\tau$. Express your final answer as a single row vector of the coefficients multiplying $(a, b, c)$, written exactly with no rounding. No physical units are required for this vector. The final answer must be a single analytical expression.",
            "solution": "We begin from the standard assumptions for parameter estimation in environmental and earth system modeling under Gaussian observation errors. The model is linearized around a nominal parameter vector $\\boldsymbol{\\theta}_{0}$ so that the observation vector $\\mathbf{y}(\\boldsymbol{\\theta})$ depends approximately linearly on $\\boldsymbol{\\theta}$ via the sensitivity matrix $\\mathbf{S}$. Under independent Gaussian errors with variance $\\sigma^{2}$, the log-likelihood for $\\boldsymbol{\\theta}$ is, up to an additive constant,\n$$\n\\mathcal{L}(\\boldsymbol{\\theta}) = -\\frac{1}{2 \\sigma^{2}} \\|\\mathbf{y}_{\\text{obs}} - \\mathbf{y}(\\boldsymbol{\\theta})\\|^{2},\n$$\nand the curvature of the negative log-likelihood at $\\boldsymbol{\\theta}_{0}$ is governed by the Fisher Information Matrix, which for the linearized model is\n$$\n\\mathbf{F} = \\sigma^{-2} \\mathbf{S}^{\\top} \\mathbf{S}.\n$$\nThe Fisher Information Matrix $\\mathbf{F}$ is symmetric positive semidefinite. Its eigenvalues and eigenvectors encode local identifiability: large eigenvalues correspond to directions in parameter space with high curvature of the log-likelihood (stiff directions), and small eigenvalues correspond to directions with low curvature (sloppy directions). In practice, a threshold $\\tau$ can be used to separate identifiable and non-identifiable directions: retain eigenvectors whose eigenvalues satisfy $\\lambda \\ge \\tau$.\n\nWe are given\n$$\n\\mathbf{F} =\n\\begin{pmatrix}\n100 & 95 & 0 \\\\\n95 & 100 & 0 \\\\\n0 & 0 & 0.01\n\\end{pmatrix},\n$$\nwith $\\sigma^{2} = 1$, so no further scaling is necessary. To perform eigenanalysis, note that $\\mathbf{F}$ is block diagonal with respect to the third parameter $c$; the top-left $2 \\times 2$ block couples $a$ and $b$, and the third parameter $c$ is decoupled with a small diagonal entry $0.01$.\n\nFirst, analyze the $2 \\times 2$ block:\n$$\n\\mathbf{B} = \n\\begin{pmatrix}\n100 & 95 \\\\\n95 & 100\n\\end{pmatrix}.\n$$\nThe eigenvalues of $\\mathbf{B}$ are obtained by solving\n$$\n\\det(\\mathbf{B} - \\lambda \\mathbf{I}) = 0,\n$$\nwhich yields\n$$\n\\det\\!\\begin{pmatrix}\n100 - \\lambda & 95 \\\\\n95 & 100 - \\lambda\n\\end{pmatrix}\n= (100 - \\lambda)^{2} - 95^{2} = 0.\n$$\nThus,\n$$\n(100 - \\lambda)^{2} = 95^{2} \\quad \\Rightarrow \\quad 100 - \\lambda = \\pm 95,\n$$\nleading to\n$$\n\\lambda_{1} = 100 + 95 = 195, \\quad \\lambda_{2} = 100 - 95 = 5.\n$$\nThe corresponding eigenvectors for $\\mathbf{B}$ can be found as follows. For $\\lambda_{1} = 195$,\n$$\n(\\mathbf{B} - 195 \\mathbf{I}) \\mathbf{v} = \\mathbf{0} \\quad \\Rightarrow \\quad\n\\begin{pmatrix}\n-95 & 95 \\\\\n95 & -95\n\\end{pmatrix}\n\\begin{pmatrix}\nv_{a} \\\\\nv_{b}\n\\end{pmatrix}\n= \n\\begin{pmatrix}\n0 \\\\\n0\n\\end{pmatrix}.\n$$\nThis yields $-95 v_{a} + 95 v_{b} = 0$, so $v_{a} = v_{b}$. A representative eigenvector is $(1, 1)$; the corresponding unit-norm vector in the $(a, b)$ subspace is $(1/\\sqrt{2}, 1/\\sqrt{2})$.\n\nFor $\\lambda_{2} = 5$,\n$$\n(\\mathbf{B} - 5 \\mathbf{I}) \\mathbf{v} = \\mathbf{0} \\quad \\Rightarrow \\quad\n\\begin{pmatrix}\n95 & 95 \\\\\n95 & 95\n\\end{pmatrix}\n\\begin{pmatrix}\nv_{a} \\\\\nv_{b}\n\\end{pmatrix}\n= \n\\begin{pmatrix}\n0 \\\\\n0\n\\end{pmatrix}.\n$$\nThis yields $95 v_{a} + 95 v_{b} = 0$, so $v_{a} = -v_{b}$. A representative eigenvector is $(1, -1)$; the corresponding unit-norm vector in the $(a, b)$ subspace is $(1/\\sqrt{2}, -1/\\sqrt{2})$.\n\nThe full $3 \\times 3$ matrix $\\mathbf{F}$ augments these with the third parameter $c$. Because the off-diagonal entries involving $c$ are $0$, the eigenvalues and eigenvectors extend to three dimensions by appending $0$ for the $c$ component to the first two eigenvectors, and using the standard basis vector for $c$. Therefore, the eigenvalues of $\\mathbf{F}$ are\n$$\n\\lambda_{1} = 195, \\quad \\lambda_{2} = 5, \\quad \\lambda_{3} = 0.01,\n$$\nwith associated unit-norm eigenvectors\n$$\n\\mathbf{v}_{1} = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\\\ 0 \\end{pmatrix}, \\quad\n\\mathbf{v}_{2} = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ -\\frac{1}{\\sqrt{2}} \\\\ 0 \\end{pmatrix}, \\quad\n\\mathbf{v}_{3} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}.\n$$\n\nApply the identifiability threshold $\\tau = 10$. We assess each eigenvalue:\n- $\\lambda_{1} = 195 \\ge 10$: the direction spanned by $\\mathbf{v}_{1}$ is identifiable (stiff).\n- $\\lambda_{2} = 5 < 10$: the direction spanned by $\\mathbf{v}_{2}$ is sloppy and not retained.\n- $\\lambda_{3} = 0.01 < 10$: the direction spanned by $\\mathbf{v}_{3}$ is sloppy and not retained.\n\nTherefore, the identifiable subspace is one-dimensional and is spanned by $\\mathbf{v}_{1}$. A reduced parameter set is obtained by projecting $(a, b, c)$ onto this identifiable direction, yielding a single identifiable linear combination of parameters. The requested output is the unit-norm linear combination vector of $(a, b, c)$ associated with the retained direction, expressed as a row vector of coefficients. This is\n$$\n\\begin{pmatrix} \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} & 0 \\end{pmatrix}.\n$$\n\nJustification for the selection is as follows. Under Gaussian errors, the Fisher Information Matrix encodes the local curvature of the negative log-likelihood in parameter space. Eigenvalues larger than the threshold $\\tau$ indicate directions with sufficiently high curvature relative to noise, implying estimability with finite precision and meaningful constraint from the data. Here, only the symmetric combination of $a$ and $b$ along $\\mathbf{v}_{1}$ is identifiable, while the antisymmetric combination of $a$ and $b$ and the parameter $c$ are sloppy, consistent with a sloppily parameterized model in which many directions are poorly constrained. The reduced parameter set retains the single stiff combination, improving robustness by removing unidentifiable directions.",
            "answer": "$$\\boxed{\\begin{pmatrix}\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} & 0\\end{pmatrix}}$$"
        }
    ]
}