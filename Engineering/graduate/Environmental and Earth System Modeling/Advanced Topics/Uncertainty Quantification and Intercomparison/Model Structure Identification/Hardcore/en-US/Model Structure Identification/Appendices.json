{
    "hands_on_practices": [
        {
            "introduction": "Before we can apply sophisticated algorithms to identify a model's structure, we must understand the fundamental conditions that make identification possible in the first place. This exercise explores how structural identifiability of terms in a Partial Differential Equation (PDE) is fundamentally a question of linear independence, which is directly influenced by the nature of the data collected . By manipulating boundary conditions and the system's excitation, you will gain hands-on insight into why the design of an experiment or observation strategy is a critical prerequisite for successful model discovery.",
            "id": "3895645",
            "problem": "You are given a one-dimensional, dimensionless environmental tracer field $u(x,t)$ on a spatial interval $x \\in [0,1]$ and a time interval $t \\in [0,1]$. The task is to assess which candidate terms in a structural model of a Partial Differential Equation (PDE) are uniquely identifiable under different Boundary Conditions (BC) and excitation choices. The fundamental base is the mass conservation statement in one spatial dimension, which implies that any physically realistic evolution of a scalar tracer can be written as a combination of spatial flux divergence and local sources or sinks. The candidate structure is restricted to terms that are ubiquitous in environmental and earth system modeling: the second spatial derivative $u_{xx}$, the first spatial derivative $u_{x}$, and the zero-order term $u$. You must determine structural identifiability of these candidate terms using only the data $u(x,t)$ and finite-difference approximations to its derivatives.\n\nDefinitions and constraints:\n- Define the PDE candidate library $\\mathcal{L} = \\{u, u_{x}, u_{xx}\\}$.\n- Structural identifiability in this context means: for a linear-in-terms PDE model with coefficients multiplying the terms in $\\mathcal{L}$, a term is identifiable if and only if its associated column in the regression matrix constructed from $\\mathcal{L}$ is not a linear combination of the remaining columns. Equivalently, if the regression matrix has full column rank, all coefficients are identifiable; if the matrix is rank-deficient, at least one coefficient is not identifiable. You must decide identifiability term-by-term.\n- Derivative approximations must be computed using central differences on interior grid points. Boundary conditions must be obeyed by the excitation functions $u(x,t)$; angles in trigonometric functions must be in radians.\n- Do not estimate any physical parameters. Only determine identifiability based on the linear independence of columns constructed from $u$, $u_{x}$, and $u_{xx}$.\n\nComputational procedure:\n1. Discretize the domain with a uniform spatial grid of $N_x$ points on $[0,1]$ and a uniform time grid of $N_t$ points on $[0,1]$.\n2. For each test case, construct $u(x,t)$ on the grid such that it respects the specified boundary conditions. Use interior central differences to approximate $u_{x}$, $u_{xx}$, and $u_{t}$, and restrict all quantities to interior points in both space and time (i.e., exclude boundary indices where one-sided differences would be required).\n3. Construct the regression matrix $\\Theta$ by stacking the columns corresponding to $u_{xx}$, $u_{x}$, and $u$ evaluated at the interior spacetime points. Let $\\Theta \\in \\mathbb{R}^{M \\times 3}$, where $M$ is the number of interior spacetime samples.\n4. Determine structural identifiability for each term in $\\mathcal{L}$ by checking if its column is in the span of the other two columns. A column is considered non-identifiable if it is numerically zero or if its least-squares projection onto the span of the other columns has a relative residual smaller than a prescribed numerical tolerance. Otherwise, it is identifiable. Use a tolerance on the relative residual of $10^{-8}$ and treat any column with relative norm smaller than $10^{-12}$ as numerically zero.\n5. Your program must output, for each test case, a list of three booleans $[I_{xx}, I_{x}, I_{0}]$ indicating identifiability of the $u_{xx}$, $u_{x}$, and $u$ terms, respectively.\n\nTest suite:\nUse the following four scientifically realistic and self-consistent test cases that cover different boundary conditions and excitation structures. All angles in trigonometric functions must be in radians. There are no physical units in this problem; all quantities are dimensionless.\n\n- Test Case 1 (General “happy path” under periodic BC): $N_x = 128$, $N_t = 64$, periodic BC, and excitation\n  $$u(x,t) = e^{-t}\\left(\\sin(2\\pi x) + \\tfrac{1}{2}\\sin(4\\pi x)\\right).$$\n  This combines two spatial Fourier modes so that $u$ and $u_{xx}$ are not collinear, and $u_{x}$ is distinct, promoting full identifiability.\n\n- Test Case 2 (Boundary-induced collinearity under periodic BC): $N_x = 128$, $N_t = 64$, periodic BC, and excitation\n  $$u(x,t) = e^{-t}\\sin(2\\pi x).$$\n  Here $u_{xx} = -(2\\pi)^2 u$ for the spatial mode, making $u$ and $u_{xx}$ collinear across spacetime and preventing separate identification of reaction and diffusion structures.\n\n- Test Case 3 (Dirichlet zero value BC with polynomial excitation): $N_x = 129$, $N_t = 64$, Dirichlet BC with $u(0,t)=0$ and $u(1,t)=0$, and excitation\n  $$u(x,t) = e^{-t}x(1-x).$$\n  This satisfies the boundary condition and yields linearly independent samples for $u$, $u_{x}$, and $u_{xx}$ across the interior.\n\n- Test Case 4 (Neumann zero-flux BC with spatially constant excitation): $N_x = 129$, $N_t = 64$, Neumann BC with $u_{x}(0,t)=0$ and $u_{x}(1,t)=0$, and excitation\n  $$u(x,t) = e^{-t}.$$\n  This yields $u_{x} \\equiv 0$ and $u_{xx} \\equiv 0$, making advection and diffusion non-identifiable while leaving the zero-order term identifiable.\n\nFinal output format:\nYour program should produce a single line of output containing the results for the four test cases as a comma-separated list enclosed in square brackets, where each element is the three-boolean list for the corresponding test case. For example: \n$$[\\,[I_{xx}^{(1)},I_{x}^{(1)},I_{0}^{(1)}],\\,[I_{xx}^{(2)},I_{x}^{(2)},I_{0}^{(2)}],\\,[I_{xx}^{(3)},I_{x}^{(3)},I_{0}^{(3)}],\\,[I_{xx}^{(4)},I_{x}^{(4)},I_{0}^{(4)}]\\,]$$\nprinted as a single Python list literal in one line.",
            "solution": "The starting point is the one-dimensional mass conservation for a scalar tracer, which states that the rate of change of the tracer density $u(x,t)$ within a small control volume equals the net inflow due to flux divergence plus local sources or sinks. Denoting the flux as $J(x,t)$ and source as $S(x,t)$, the conservation statement is\n$$\\frac{\\partial u}{\\partial t} = -\\frac{\\partial J}{\\partial x} + S.$$\nIn environmental modeling, a widely accepted and fundamental decomposition of the flux in the absence of external forcing is the sum of a Fickian diffusive component and an advective component:\n$$J = -D\\frac{\\partial u}{\\partial x} + v\\,u,$$\nwith $D$ a diffusion parameter and $v$ an advection velocity. A linear reaction source is commonly represented as\n$$S = r\\,u,$$\nwith $r$ a reaction parameter. Substituting these into the conservation law yields\n$$\\frac{\\partial u}{\\partial t} = D\\,\\frac{\\partial^2 u}{\\partial x^2} - v\\,\\frac{\\partial u}{\\partial x} + r\\,u.$$\nWe do not estimate the parameters $D$, $v$, and $r$ here; rather, we assess structural identifiability of the terms $u_{xx}$, $u_{x}$, and $u$ from data. The reason this is a principled approach is that, under linear-in-terms representation, interpretability and uniqueness of term coefficients reduce to the linear independence of the columns of a regression matrix built from the library of candidate terms evaluated at observed spacetime samples.\n\nGiven sampled fields $u(x_i,t_j)$ on a two-dimensional grid, finite differences provide approximations of the required derivatives. We select central differences for both space and time to minimize bias and increase accuracy:\n- For interior spatial points $x_i$ with $i=1,\\dots,N_x-2$, the first derivative is approximated by\n$$u_{x}(x_i,t_j) \\approx \\frac{u(x_{i+1},t_j) - u(x_{i-1},t_j)}{2\\Delta x},$$\nand the second derivative by\n$$u_{xx}(x_i,t_j) \\approx \\frac{u(x_{i+1},t_j) - 2u(x_i,t_j) + u(x_{i-1},t_j)}{\\Delta x^2},$$\nwhere $\\Delta x$ is the spatial grid spacing.\n- For interior temporal points $t_j$ with $j=1,\\dots,N_t-2$, the time derivative is approximated by\n$$u_{t}(x_i,t_j) \\approx \\frac{u(x_i,t_{j+1}) - u(x_i,t_{j-1})}{2\\Delta t},$$\nwhere $\\Delta t$ is the time step.\n\nThese approximations are consistent with boundary conditions when we restrict attention to interior points; the excitation functions $u(x,t)$ are constructed to satisfy the stated boundary conditions exactly, which ensures physical realism without having to use one-sided differences at the boundaries.\n\nTo build the regression matrix, we stack the interior spacetime samples of $u_{xx}$, $u_{x}$, and $u$ into columns:\n$$\\Theta = \\begin{bmatrix}\n\\vdots & \\vdots & \\vdots \\\\\nu_{xx}(x_i,t_j) & u_{x}(x_i,t_j) & u(x_i,t_j) \\\\\n\\vdots & \\vdots & \\vdots\n\\end{bmatrix} \\in \\mathbb{R}^{M \\times 3},$$\nwhere $M$ is the number of interior spacetime samples.\n\nStructural identifiability criteria follow from linear algebra:\n- If $\\mathrm{rank}(\\Theta) = 3$, all terms are identifiable; each coefficient is uniquely determined in principle from the data via least squares because the columns are linearly independent.\n- If $\\mathrm{rank}(\\Theta) < 3$, at least one column is a linear combination of the others (or numerically zero), which implies non-identifiability for the corresponding coefficient(s). More finely, a given term is non-identifiable if its column lies in the span of the remaining columns. Conversely, a term is identifiable if its column is not in the span of the others.\n\nTo robustly assess identifiability term-by-term in finite precision arithmetic:\n1. Compute the relative norm of each column. If the relative norm is below a tiny threshold (e.g., $10^{-12}$), treat the column as numerically zero and declare the term non-identifiable.\n2. For each column $j$, solve the least squares problem\n$$\\min_{\\beta}\\left\\| \\Theta_{\\cdot j} - \\Theta_{\\cdot,-j}\\,\\beta \\right\\|_2,$$\nwhere $\\Theta_{\\cdot j}$ denotes column $j$ and $\\Theta_{\\cdot,-j}$ denotes the matrix of the other two columns. Compute the relative residual\n$$\\rho_j = \\frac{\\left\\| \\Theta_{\\cdot j} - \\Theta_{\\cdot,-j}\\,\\hat{\\beta} \\right\\|_2}{\\left\\| \\Theta_{\\cdot j} \\right\\|_2}.$$\nIf $\\rho_j < 10^{-8}$, declare column $j$ dependent and the corresponding term non-identifiable; otherwise, declare it identifiable.\n\nWhy boundary conditions matter:\n- Under periodic or Dirichlet boundary conditions, spatial eigenfunctions of the Laplacian, such as $\\sin(k\\pi x)$, satisfy $u_{xx} = -\\lambda u$ with $\\lambda = (k\\pi)^2$ for Dirichlet or $\\lambda = (2\\pi k)^2$ for periodic modes on $[0,1]$. If the excitation $u$ is a single eigenfunction modulated in time, the columns corresponding to $u$ and $u_{xx}$ are collinear, making diffusion and reaction inseparable structurally. Adding multiple distinct eigenmodes breaks this proportionality and restores independence.\n- Under Neumann zero-flux boundary conditions, a spatially constant excitation yields $u_{x} \\equiv 0$ and $u_{xx} \\equiv 0$, immediately removing advection and diffusion from the identifiable set while leaving the zero-order term identifiable.\n- Polynomial excitations satisfying Dirichlet boundary conditions (e.g., $u = e^{-t}x(1-x)$) produce distinct spatial patterns for $u$, $u_{x}$, and $u_{xx}$, promoting full identifiability.\n\nApplying the procedure to the test suite:\n- Test Case 1 combines two periodic Fourier modes, making $u$ and $u_{xx}$ non-collinear and $u_{x}$ distinct. The algorithm will find column-wise independence for all three terms and return $[{\\rm True},{\\rm True},{\\rm True}]$.\n- Test Case 2 uses a single periodic Fourier mode, so $u_{xx}$ is a scalar multiple of $u$. The least squares projection will yield near-zero residual for both directions of dependence, declaring $u_{xx}$ and $u$ non-identifiable, while $u_{x}$ remains identifiable. The result is $[{\\rm False},{\\rm True},{\\rm False}]$.\n- Test Case 3 uses a polynomial under Dirichlet boundary conditions; the spatial patterns for $u$, $u_{x}$, and $u_{xx}$ are distinct across interior points, leading to $[{\\rm True},{\\rm True},{\\rm True}]$.\n- Test Case 4 uses a spatially constant excitation under Neumann boundary conditions; $u_{x}$ and $u_{xx}$ columns are numerically zero and therefore non-identifiable, while the $u$ column remains independent, leading to $[{\\rm False},{\\rm False},{\\rm True}]$.\n\nThe program implements the above finite-difference approximations, constructs $\\Theta$, and performs the identifiability checks with the specified numerical tolerances. It outputs the list of three-booleans per test case as a single Python list literal on one line, in the order of the four test cases.",
            "answer": "```python\n# Python 3.12\n# Libraries: numpy 1.23.5, scipy 1.11.4 (not used)\nimport numpy as np\n\ndef generate_u(case, x, t):\n    \"\"\"\n    Generate u(x,t) for a given test case definition.\n    case: dict with keys 'bc' and 'excitation'\n    x: 1D array of spatial points\n    t: 1D array of time points\n    Returns u as a 2D array of shape (Nx, Nt) where Nx=len(x), Nt=len(t).\n    \"\"\"\n    X, T = np.meshgrid(x, t, indexing='ij')\n    exc = case['excitation']\n    if exc == 'periodic_mixture':\n        u = np.exp(-T) * (np.sin(2*np.pi*X) + 0.5*np.sin(4*np.pi*X))\n        # Periodic BC is satisfied by construction.\n    elif exc == 'periodic_single':\n        u = np.exp(-T) * np.sin(2*np.pi*X)\n        # Periodic BC satisfied.\n    elif exc == 'dirichlet_quadratic':\n        u = np.exp(-T) * X * (1.0 - X)\n        # Dirichlet BC: u(0,t)=0 and u(1,t)=0 satisfied.\n    elif exc == 'neumann_constant':\n        u = np.exp(-T) * np.ones_like(X)\n        # Neumann BC: ux=0 at boundaries satisfied since u is constant in space.\n    else:\n        raise ValueError(\"Unknown excitation type\")\n    return u\n\ndef finite_differences(u, x, t):\n    \"\"\"\n    Compute central-difference approximations of ux, uxx, ut on interior points.\n    Returns flattened vectors for the interior grid: ux_vec, uxx_vec, u_vec\n    \"\"\"\n    dx = x[1] - x[0]\n    dt = t[1] - t[0]\n\n    # Interior indices\n    ix0, ix1 = 1, len(x) - 2\n    it0, it1 = 1, len(t) - 2\n\n    # Slicing interior\n    u_int = u[ix0:ix1+1, it0:it1+1]\n\n    # Spatial derivatives using central differences on interior spatial points\n    ux_int = (u[ix0+1:ix1+2, it0:it1+1] - u[ix0-1:ix1, it0:it1+1]) / (2.0 * dx)\n    uxx_int = (u[ix0+1:ix1+2, it0:it1+1] - 2.0*u[ix0:ix1+1, it0:it1+1] + u[ix0-1:ix1, it0:it1+1]) / (dx*dx)\n\n    # Time derivative using central differences on interior time points\n    ut_int = (u[ix0:ix1+1, it0+1:it1+2] - u[ix0:ix1+1, it0-1:it1]) / (2.0 * dt)\n\n    # To align shapes, restrict u_int, ux_int, uxx_int and ut_int to common interior\n    # u_int is already (ix1-ix0+1, it1-it0+1)\n    # ux_int and uxx_int have same shape as u_int\n    # ut_int has same shape as u_int\n\n    # Flatten vectors\n    u_vec = u_int.reshape(-1)\n    ux_vec = ux_int.reshape(-1)\n    uxx_vec = uxx_int.reshape(-1)\n    ut_vec = ut_int.reshape(-1)\n\n    return ux_vec, uxx_vec, u_vec, ut_vec\n\ndef term_identifiability(theta, tol_zero=1e-12, tol_dep=1e-8):\n    \"\"\"\n    Determine identifiability of each column in theta by checking if the column is\n    numerically zero or lies in the span of the other columns.\n    Returns a list of booleans [I_xx, I_x, I_0] for columns in order [uxx, ux, u].\n    \"\"\"\n    M, K = theta.shape\n    assert K == 3, \"Theta must have 3 columns for [uxx, ux, u]\"\n    id_flags = []\n\n    # Pre-compute norms\n    col_norms = np.linalg.norm(theta, axis=0)\n    # Relative norms (scaled by overall matrix norm to guard)\n    mat_norm = np.linalg.norm(theta)\n    rel_norms = col_norms / (mat_norm + 1e-30)\n\n    for j in range(K):\n        # Zero column check\n        if rel_norms[j] < tol_zero:\n            id_flags.append(False)\n            continue\n        # Build matrix of other columns\n        other_indices = [i for i in range(K) if i != j]\n        Theta_others = theta[:, other_indices]\n        col_j = theta[:, j]\n\n        # If other columns are all near-zero, then col_j is independent\n        if np.linalg.matrix_rank(Theta_others) == 0 and np.linalg.norm(Theta_others) < tol_zero:\n            id_flags.append(True)\n            continue\n\n        # Least squares projection of col_j onto span of others\n        beta, *_ = np.linalg.lstsq(Theta_others, col_j, rcond=None)\n        residual = col_j - Theta_others @ beta\n        r_rel = np.linalg.norm(residual) / (np.linalg.norm(col_j) + 1e-30)\n        # If residual is tiny, column is dependent -> not identifiable\n        id_flags.append(r_rel >= tol_dep)\n    return id_flags\n\ndef run_case(case):\n    Nx = case['Nx']\n    Nt = case['Nt']\n    # Uniform grids on [0,1]\n    x = np.linspace(0.0, 1.0, Nx)\n    t = np.linspace(0.0, 1.0, Nt)\n    u = generate_u(case, x, t)\n    ux_vec, uxx_vec, u_vec, ut_vec = finite_differences(u, x, t)\n    # Build Theta with columns [uxx, ux, u]\n    Theta = np.column_stack([uxx_vec, ux_vec, u_vec])\n    # Determine identifiability\n    id_flags = term_identifiability(Theta, tol_zero=1e-12, tol_dep=1e-8)\n    return id_flags\n\ndef solve():\n    # Define test cases from the problem statement\n    test_cases = [\n        # Test Case 1: Periodic BC, mixture of Fourier modes\n        {'bc': 'periodic', 'excitation': 'periodic_mixture', 'Nx': 128, 'Nt': 64},\n        # Test Case 2: Periodic BC, single Fourier mode\n        {'bc': 'periodic', 'excitation': 'periodic_single', 'Nx': 128, 'Nt': 64},\n        # Test Case 3: Dirichlet zero value BC, polynomial excitation\n        {'bc': 'dirichlet', 'excitation': 'dirichlet_quadratic', 'Nx': 129, 'Nt': 64},\n        # Test Case 4: Neumann zero-flux BC, spatially constant excitation\n        {'bc': 'neumann', 'excitation': 'neumann_constant', 'Nx': 129, 'Nt': 64},\n    ]\n\n    results = []\n    for case in test_cases:\n        res = run_case(case)\n        results.append(res)\n\n    # Print single-line output in the exact required format\n    # e.g., [[True,True,True],[False,True,False],...]\n    def list_to_str(lst):\n        return \"[\" + \",\".join([\"True\" if v else \"False\" for v in lst]) + \"]\"\n    print(\"[\" + \",\".join([list_to_str(r) for r in results]) + \"]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "While the previous exercise focused on the deterministic conditions for identifiability, real-world model selection is a statistical challenge. This practice delves into the heart of statistical distinguishability using the Kullback-Leibler ($D_{\\mathrm{KL}}$) divergence, a key concept from information theory that measures the \"distance\" between two probability distributions . You will implement a computational workflow to determine the minimum sample size needed to tell two candidate model structures apart, linking fundamental theory to the practical design of data collection campaigns.",
            "id": "3895641",
            "problem": "Consider the task of model structure identification in environmental and earth system modeling, where the basic goal is to decide, from independent and identically distributed observations, whether the data-generating mechanism matches a particular candidate model structure. A candidate structure is represented by a parametric probability model for residuals or fluxes (for example, the distribution of daily runoff increments), and distinguishability is assessed via the Kullback-Leibler divergence. You must write a complete, runnable program that, for each test case below, computes the minimal independent sample size required to achieve a specified Type II error tolerance in a likelihood-ratio decision between a true model structure and a candidate model structure. The program must produce a single line of output containing the results in a specified format.\n\nThe foundational base for this problem consists of widely accepted principles of statistical decision theory and information theory: the Neyman-Pearson lemma for most powerful tests between two simple hypotheses, the Strong Law of Large Numbers for sums of independent quantities, and the large-sample error-exponent characterization of the likelihood-ratio test via the Kullback-Leibler divergence. Rely only on these fundamental bases; do not assume any particular shortcut formulas. All derivations and computations must be valid within the setting of independent and identically distributed continuous random variables with densities.\n\nYour program must implement the following requirements.\n\n1. For each test case, you are provided a true data-generating model structure denoted $P$ and a candidate model structure denoted $Q$, each specified by a probability distribution family and its parameters. Your program must compute the Kullback-Leibler divergence $D_{\\mathrm{KL}}(P\\parallel Q)$ defined for densities $p$ and $q$ by\n$$\nD_{\\mathrm{KL}}(P\\parallel Q) = \\int p(x)\\,\\log\\!\\left(\\frac{p(x)}{q(x)}\\right)\\,\\mathrm{d}x,\n$$\nand use it to determine the minimal independent sample size $n_{\\min}$ such that the Type II error of the most powerful test does not exceed a user-specified tolerance $\\delta$ in the large-sample regime. If the divergence is $0$, then the minimal sample size is infinite since the two model structures are indistinguishable based on the given data. All logarithms are natural logarithms.\n\n2. Your program must implement exact formulas for $D_{\\mathrm{KL}}(P\\parallel Q)$ where available for standard families, and must otherwise compute an unbiased Monte Carlo estimator of $D_{\\mathrm{KL}}(P\\parallel Q)$ using independent samples from the true model $P$ and the identity\n$$\nD_{\\mathrm{KL}}(P\\parallel Q)=\\mathbb{E}_P\\!\\left[\\log p(X) - \\log q(X)\\right].\n$$\nUse a fixed random seed to guarantee reproducibility. Ensure that your estimator variance is controlled by using sufficiently many samples to provide a stable estimate.\n\n3. For each test case, given the divergence $D_{\\mathrm{KL}}(P\\parallel Q)$ and the tolerance $\\delta$, compute the minimal independent sample size as\n$$\nn_{\\min} = \\left\\lceil \\frac{\\log(1/\\delta)}{D_{\\mathrm{KL}}(P\\parallel Q)} \\right\\rceil,\n$$\nwith the convention that if $D_{\\mathrm{KL}}(P\\parallel Q) = 0$ then $n_{\\min} = +\\infty$.\n\n4. Treat all probabilities and divergences as dimensionless quantities; no physical units are required in the final answer.\n\nImplement the following test suite. In each case, report the minimal sample size as an integer when finite, and $+\\infty$ when infinite.\n\nTest Case A (general case): $P$ is Gaussian with mean $0$ and standard deviation $1$; $Q$ is Gaussian with mean $0.5$ and standard deviation $1$; use $\\delta = 0.01$.\n\nTest Case B (mismatched families): $P$ is Lognormal with shape parameter $0.5$ and log-scale mean $0$ (that is, scale $e^{0}=1$); $Q$ is Gamma with shape $2$ and scale $0.5$; use $\\delta = 0.05$.\n\nTest Case C (boundary indistinguishability): $P$ is Gaussian with mean $0$ and standard deviation $1$; $Q$ is Gaussian with mean $0$ and standard deviation $1$; use $\\delta = 0.1$.\n\nTest Case D (edge case with small divergence): $P$ is Exponential with rate $1$; $Q$ is Exponential with rate $0.98$; use $\\delta = 0.01$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered as $[A,B,C,D]$ corresponding to the test cases above. For example, the output should look like $[n_A,n_B,n_C,n_D]$ where each $n$ is an integer when finite and $+\\infty$ is represented by the floating-point infinity value.",
            "solution": "The problem requires the computation of the minimal sample size, $n_{\\min}$, needed to distinguish between a true data-generating probability model, $P$, and an alternative candidate model, $Q$, with a specified reliability. This is a canonical problem in statistical hypothesis testing and model selection. The framework provided is based on the large-sample properties of the likelihood-ratio test, where the Kullback-Leibler (KL) divergence, $D_{\\mathrm{KL}}(P\\parallel Q)$, quantifies the statistical distinguishability between the two models.\n\nThe decision problem can be formulated as a hypothesis test between two simple hypotheses: the null hypothesis $H_0$ that the observed data $X_1, X_2, \\dots, X_n$ are drawn independently from the distribution $P$ with density $p(x)$, and the alternative hypothesis $H_1$ that they are from $Q$ with density $q(x)$. According to the Neyman-Pearson lemma, the most powerful test for a given Type I error rate is based on the log-likelihood ratio, $\\Lambda_n = \\sum_{i=1}^n \\log(p(X_i)/q(X_i))$.\n\nThe problem specifies a relationship between the minimal sample size $n_{\\min}$, the Type II error tolerance $\\delta$, and the KL divergence, given by:\n$$\nn_{\\min} = \\left\\lceil \\frac{\\log(1/\\delta)}{D_{\\mathrm{KL}}(P\\parallel Q)} \\right\\rceil\n$$\nThis formula is a direct consequence of large deviation theory, specifically results related to Stein's Lemma or Chernoff bounds. In the large-sample limit ($n \\to \\infty$), the probability of committing a Type II error (i.e., failing to distinguish $P$ from $Q$ when $P$ is true) decays exponentially with a rate given by the KL divergence. The probability of this error, $\\delta$, is approximately $\\exp(-n D_{\\mathrm{KL}}(P\\parallel Q))$. Solving for $n$ yields the provided expression for the minimal sample size. The ceiling function, $\\lceil \\cdot \\rceil$, ensures that $n_{\\min}$ is the smallest integer satisfying the tolerance constraint.\n\nThe core of the computation is the KL divergence, defined for continuous distributions with probability density functions $p(x)$ and $q(x)$ as:\n$$\nD_{\\mathrm{KL}}(P\\parallel Q) = \\int_{-\\infty}^{\\infty} p(x)\\,\\log\\left(\\frac{p(x)}{q(x)}\\right)\\,\\mathrm{d}x = \\mathbb{E}_P[\\log p(X) - \\log q(X)]\n$$\nThe value of $D_{\\mathrm{KL}}(P\\parallel Q)$ is non-negative, and $D_{\\mathrm{KL}}(P\\parallel Q) = 0$ if and only if $P$ and $Q$ are identical (almost everywhere). If $D_{\\mathrm{KL}}(P\\parallel Q) = 0$, the models are indistinguishable, and the required sample size $n_{\\min}$ is infinite.\n\nFor certain pairs of distribution families, this integral has a closed-form analytical solution. In other cases, it must be estimated numerically. The problem directs the use of a Monte Carlo estimator based on the expectation form, where a large number of samples $x_i$ are drawn from the true distribution $P$:\n$$\n\\hat{D}_{\\mathrm{KL}}(P\\parallel Q) = \\frac{1}{N}\\sum_{i=1}^{N} \\left[\\log p(x_i) - \\log q(x_i)\\right], \\quad x_i \\stackrel{\\text{i.i.d.}}{\\sim} P\n$$\nBy the Strong Law of Large Numbers, this estimator converges to the true $D_{\\mathrm{KL}}(P\\parallel Q)$ as the number of Monte Carlo samples $N \\to \\infty$.\n\nWe now proceed to solve each test case.\n\n**Test Case A: Gaussian vs. Gaussian**\nThe true model is $P = \\mathcal{N}(\\mu_P=0, \\sigma_P=1)$ and the candidate is $Q = \\mathcal{N}(\\mu_Q=0.5, \\sigma_Q=1)$. The error tolerance is $\\delta = 0.01$. For two Gaussian distributions $P = \\mathcal{N}(\\mu_P, \\sigma_P^2)$ and $Q = \\mathcal{N}(\\mu_Q, \\sigma_Q^2)$, the KL divergence has the analytical form:\n$$\nD_{\\mathrm{KL}}(P\\parallel Q) = \\log\\frac{\\sigma_Q}{\\sigma_P} + \\frac{\\sigma_P^2 + (\\mu_P - \\mu_Q)^2}{2\\sigma_Q^2} - \\frac{1}{2}\n$$\nSubstituting the given parameters $\\mu_P=0$, $\\sigma_P=1$, $\\mu_Q=0.5$, and $\\sigma_Q=1$:\n$$\nD_{\\mathrm{KL}}(P\\parallel Q) = \\log\\frac{1}{1} + \\frac{1^2 + (0 - 0.5)^2}{2 \\cdot 1^2} - \\frac{1}{2} = 0 + \\frac{1 + 0.25}{2} - 0.5 = 0.625 - 0.5 = 0.125\n$$\nThe minimal sample size is then:\n$$\nn_{\\min, A} = \\left\\lceil \\frac{\\log(1/0.01)}{0.125} \\right\\rceil = \\left\\lceil \\frac{\\log(100)}{0.125} \\right\\rceil \\approx \\left\\lceil \\frac{4.60517}{0.125} \\right\\rceil = \\lceil 36.841 \\rceil = 37\n$$\n\n**Test Case B: Lognormal vs. Gamma**\nThe true model is $P \\sim \\text{Lognormal}(\\mu_{\\ln}=0, \\sigma_{\\ln}=0.5)$ and the candidate is $Q \\sim \\text{Gamma}(k=2, \\theta=0.5)$. The tolerance is $\\delta = 0.05$. There is no simple closed-form expression for the KL divergence between these two families. Therefore, we employ Monte Carlo estimation with a large number of samples, $N=10^7$, drawn from $P$.\nThe densities are $p(x) = \\frac{1}{x \\sigma_{\\ln} \\sqrt{2\\pi}} \\exp\\left(-\\frac{(\\log x - \\mu_{\\ln})^2}{2 \\sigma_{\\ln}^2}\\right)$ and $q(x) = \\frac{1}{\\Gamma(k) \\theta^k} x^{k-1} e^{-x/\\theta}$.\nThe Monte Carlo estimate is calculated as $\\hat{D}_{\\mathrm{KL}} = \\frac{1}{N} \\sum_{i=1}^N (\\log p(x_i) - \\log q(x_i))$, where $x_i \\sim \\text{Lognormal}(0, 0.5)$. A fixed random seed is used for reproducibility. The computation yields:\n$$\nD_{\\mathrm{KL}}(P\\parallel Q) \\approx 0.15426\n$$\nUsing this value, the minimal sample size is:\n$$\nn_{\\min, B} = \\left\\lceil \\frac{\\log(1/0.05)}{0.15426} \\right\\rceil = \\left\\lceil \\frac{\\log(20)}{0.15426} \\right\\rceil \\approx \\left\\lceil \\frac{2.99573}{0.15426} \\right\\rceil = \\lceil 19.420 \\rceil = 20\n$$\n\n**Test Case C: Identical Gaussian Models**\nThe true model is $P = \\mathcal{N}(\\mu_P=0, \\sigma_P=1)$ and the candidate is $Q = \\mathcal{N}(\\mu_Q=0, \\sigma_Q=1)$. The tolerance is $\\delta = 0.1$. Since the distributions $P$ and $Q$ are identical, their densities $p(x)$ and $q(x)$ are equal. Thus, the ratio $p(x)/q(x) = 1$ and its logarithm is $0$. The KL divergence is:\n$$\nD_{\\mathrm{KL}}(P\\parallel Q) = \\int p(x) \\log(1) \\,\\mathrm{d}x = 0\n$$\nAs specified, if the KL divergence is zero, the models are indistinguishable with any finite sample size.\n$$\nn_{\\min, C} = +\\infty\n$$\n\n**Test Case D: Exponential vs. Exponential**\nThe true model is $P \\sim \\text{Exponential}(\\lambda_P=1)$ and the candidate is $Q \\sim \\text{Exponential}(\\lambda_Q=0.98)$. The tolerance is $\\delta = 0.01$. For two exponential distributions $P \\sim \\text{Exponential}(\\lambda_P)$ and $Q \\sim \\text{Exponential}(\\lambda_Q)$, the KL divergence has the analytical form:\n$$\nD_{\\mathrm{KL}}(P\\parallel Q) = \\log\\left(\\frac{\\lambda_P}{\\lambda_Q}\\right) - 1 + \\frac{\\lambda_Q}{\\lambda_P}\n$$\nSubstituting the parameters $\\lambda_P=1$ and $\\lambda_Q=0.98$:\n$$\nD_{\\mathrm{KL}}(P\\parallel Q) = \\log\\left(\\frac{1}{0.98}\\right) - 1 + \\frac{0.98}{1} = \\log(1/0.98) - 0.02 \\approx 0.0202027 - 0.02 = 0.0002027\n$$\nThis small positive divergence indicates the models are very similar but distinguishable. The minimal sample size is:\n$$\nn_{\\min, D} = \\left\\lceil \\frac{\\log(1/0.01)}{0.0002027} \\right\\rceil = \\left\\lceil \\frac{\\log(100)}{0.0002027} \\right\\rceil \\approx \\left\\lceil \\frac{4.60517}{0.0002027} \\right\\rceil = \\lceil 22725.6 \\rceil = 22726\n$$",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import lognorm, gamma\nimport math\n\ndef calculate_n_min(d_kl, delta):\n    \"\"\"\n    Computes the minimal sample size n_min from KL divergence and Type II error.\n    \"\"\"\n    if d_kl <= 0:\n        # A KL divergence of 0 means models are indistinguishable.\n        # Negative values can arise from MC estimation with insufficient samples, but\n        # should be treated as indistinguishable for this problem's purpose.\n        return float('inf')\n    \n    # Calculate n_min using the formula provided in the problem.\n    # log is the natural logarithm, as specified.\n    return math.ceil(math.log(1 / delta) / d_kl)\n\ndef kl_gaussian_gaussian(p_params, q_params):\n    \"\"\"\n    Computes the analytical KL divergence between two Gaussian distributions.\n    P = N(mu_p, sigma_p^2), Q = N(mu_q, sigma_q^2)\n    \"\"\"\n    mu_p, sigma_p = p_params['mean'], p_params['std']\n    mu_q, sigma_q = q_params['mean'], q_params['std']\n\n    # Using the standard formula for KL divergence between two Gaussians.\n    log_term = math.log(sigma_q / sigma_p)\n    frac_term = (sigma_p**2 + (mu_p - mu_q)**2) / (2 * sigma_q**2)\n    \n    return log_term + frac_term - 0.5\n\ndef kl_exponential_exponential(p_params, q_params):\n    \"\"\"\n    Computes the analytical KL divergence between two Exponential distributions.\n    P ~ Exp(lambda_p), Q ~ Exp(lambda_q)\n    \"\"\"\n    lambda_p = p_params['rate']\n    lambda_q = q_params['rate']\n\n    # Using the standard formula for KL divergence between two Exponentials.\n    log_term = math.log(lambda_p / lambda_q)\n    \n    return log_term - 1 + (lambda_q / lambda_p)\n\ndef kl_lognormal_gamma_mc(p_params, q_params, n_samples=10**7, seed=42):\n    \"\"\"\n    Estimates KL divergence between Lognormal(P) and Gamma(Q) via Monte Carlo.\n    D_KL(P || Q) = E_P[log(p(x)/q(x))]\n    \"\"\"\n    # P: Lognormal distribution parameters\n    # The problem specifies shape (sigma_ln) and log-scale mean (mu_ln).\n    # scipy.stats.lognorm uses shape 's' (sigma_ln) and 'scale' (exp(mu_ln)).\n    s_p = p_params['s']\n    mu_ln_p = math.log(p_params['scale'])\n\n    # Q: Gamma distribution parameters\n    # The problem specifies shape 'k' and scale 'theta'.\n    # scipy.stats.gamma uses shape 'a' (k) and 'scale' (theta).\n    a_q = q_params['a']\n    scale_q = q_params['scale']\n\n    # Set seed for reproducible results\n    np.random.seed(seed)\n    \n    # 1. Generate samples from the true distribution P (Lognormal)\n    samples = np.random.lognormal(mean=mu_ln_p, sigma=s_p, size=n_samples)\n\n    # 2. Calculate log-likelihood of samples under P and Q\n    # Use scipy.stats for robust log-pdf calculations.\n    log_p_vals = lognorm.logpdf(samples, s=s_p, scale=p_params['scale'])\n    log_q_vals = gamma.logpdf(samples, a=a_q, scale=scale_q)\n\n    # 3. Compute the mean of the log-likelihood differences\n    # This is the Monte Carlo estimate of the KL divergence.\n    d_kl_estimate = np.mean(log_p_vals - log_q_vals)\n    \n    return d_kl_estimate\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test Case A: Gaussian vs. Gaussian\n        {'handler': kl_gaussian_gaussian, 'p_params': {'mean': 0, 'std': 1}, 'q_params': {'mean': 0.5, 'std': 1}, 'delta': 0.01},\n        # Test Case B: Lognormal vs. Gamma (Monte Carlo)\n        {'handler': kl_lognormal_gamma_mc, 'p_params': {'s': 0.5, 'scale': math.exp(0)}, 'q_params': {'a': 2, 'scale': 0.5}, 'delta': 0.05},\n        # Test Case C: Identical Gaussians\n        {'handler': kl_gaussian_gaussian, 'p_params': {'mean': 0, 'std': 1}, 'q_params': {'mean': 0, 'std': 1}, 'delta': 0.1},\n        # Test Case D: Exponential vs. Exponential\n        {'handler': kl_exponential_exponential, 'p_params': {'rate': 1}, 'q_params': {'rate': 0.98}, 'delta': 0.01},\n    ]\n\n    results = []\n    for case in test_cases:\n        # Calculate the KL divergence using the appropriate handler for the case.\n        d_kl = case['handler'](case['p_params'], case['q_params'])\n        \n        # Calculate the minimal sample size based on the divergence and tolerance.\n        n_min = calculate_n_min(d_kl, case['delta'])\n        \n        # For finite integer results, ensure they are stored as int type.\n        if n_min != float('inf'):\n            n_min = int(n_min)\n            \n        results.append(n_min)\n\n    # Final print statement in the exact required format.\n    # The map(str, ...) correctly handles integers and float('inf') -> 'inf'.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Equipped with an understanding of both linear algebraic and statistical identifiability, we now turn to a powerful framework for automated structure discovery from data. This practice introduces the Group LASSO, a sparse regression technique that can automatically select entire physical processes—such as advection, diffusion, or reaction—from a large library of candidate operators . By deriving and applying the selection conditions for this estimator, you will engage with a state-of-the-art method that operationalizes the principle of parsimony to find the simplest governing equation consistent with observations.",
            "id": "3895649",
            "problem": "Consider a two-dimensional advection–diffusion–reaction process for a conserved scalar field $c(\\boldsymbol{x}, t)$, governed by the conservation of mass and constitutive relationships that lead to the canonical transport form\n$$\n\\frac{\\partial c}{\\partial t} + \\nabla \\cdot (\\boldsymbol{u} c) = \\nabla \\cdot \\left(D \\nabla c \\right) + R(c),\n$$\nwhere $\\boldsymbol{u}$ is the velocity field, $D$ is the diffusivity, and $R(c)$ is a reaction term. In data-driven model structure identification for environmental and earth system modeling, one constructs an operator library and fits a parameterized model that is linear in the operator coefficients. Aggregating spatiotemporal samples yields a regression of the form\n$$\n\\boldsymbol{y} = X_{\\mathrm{adv}} \\boldsymbol{\\beta}_{\\mathrm{adv}} + X_{\\mathrm{diff}} \\boldsymbol{\\beta}_{\\mathrm{diff}} + X_{\\mathrm{react}} \\boldsymbol{\\beta}_{\\mathrm{react}} + \\boldsymbol{\\varepsilon},\n$$\nwhere $\\boldsymbol{y} \\in \\mathbb{R}^{n}$ are the measured time tendencies of $c$, and $X_{\\mathrm{adv}} \\in \\mathbb{R}^{n \\times d_{\\mathrm{adv}}}$, $X_{\\mathrm{diff}} \\in \\mathbb{R}^{n \\times d_{\\mathrm{diff}}}$, and $X_{\\mathrm{react}} \\in \\mathbb{R}^{n \\times d_{\\mathrm{react}}}$ collect the evaluated operator templates for advection, diffusion, and reaction, respectively. To identify which operators are structurally present, we pose a group-sparse estimator using the Least Absolute Shrinkage and Selection Operator (LASSO) in its group form:\n$$\n\\min_{\\boldsymbol{\\beta}_{\\mathrm{adv}},\\,\\boldsymbol{\\beta}_{\\mathrm{diff}},\\,\\boldsymbol{\\beta}_{\\mathrm{react}}} \\;\\; \\frac{1}{2n} \\left\\| \\boldsymbol{y} - X_{\\mathrm{adv}} \\boldsymbol{\\beta}_{\\mathrm{adv}} - X_{\\mathrm{diff}} \\boldsymbol{\\beta}_{\\mathrm{diff}} - X_{\\mathrm{react}} \\boldsymbol{\\beta}_{\\mathrm{react}} \\right\\|_{2}^{2} + \\lambda \\left( w_{\\mathrm{adv}} \\left\\|\\boldsymbol{\\beta}_{\\mathrm{adv}}\\right\\|_{2} + w_{\\mathrm{diff}} \\left\\|\\boldsymbol{\\beta}_{\\mathrm{diff}}\\right\\|_{2} + w_{\\mathrm{react}} \\left\\|\\boldsymbol{\\beta}_{\\mathrm{react}}\\right\\|_{2} \\right),\n$$\nwhere $\\lambda \\geq 0$ is a regularization level, and $w_{\\mathrm{adv}}$, $w_{\\mathrm{diff}}$, and $w_{\\mathrm{react}}$ are positive group weights. Suppose the following scientifically consistent pre-processing and data conditions hold:\n- The number of aggregated samples is $n = 50$.\n- Column sets of $X_{\\mathrm{adv}}$, $X_{\\mathrm{diff}}$, and $X_{\\mathrm{react}}$ have been orthonormalized and mutually orthogonalized in the sense that $(1/n) X_{g}^{\\top} X_{g} = I_{d_{g}}$ for each group $g \\in \\{\\mathrm{adv}, \\mathrm{diff}, \\mathrm{react}\\}$, and $X_{g}^{\\top} X_{h} = 0$ for $g \\neq h$.\n- The group dimensions are $d_{\\mathrm{adv}} = 2$, $d_{\\mathrm{diff}} = 1$, and $d_{\\mathrm{react}} = 3$.\n- The group weights are chosen as $w_{g} = \\sqrt{d_{g}}$ to balance the penalty across groups of different sizes.\n- The empirically computed operator–response inner products (from the data and the orthonormalized design) are $X_{\\mathrm{adv}}^{\\top} \\boldsymbol{y} = \\begin{pmatrix} 9 \\\\ -3 \\end{pmatrix}$, $X_{\\mathrm{diff}}^{\\top} \\boldsymbol{y} = \\begin{pmatrix} 22 \\end{pmatrix}$, and $X_{\\mathrm{react}}^{\\top} \\boldsymbol{y} = \\begin{pmatrix} 4 \\\\ -2 \\\\ 1 \\end{pmatrix}$.\n\nStarting from the conservation law and least-squares data fitting principle, derive the necessary and sufficient selection conditions for a group to be excluded by the group penalty, and use them to determine the minimal regularization level $\\lambda_{\\min}$ (dimensionless) such that, at the optimizer of the above problem, only the diffusion operator group remains active while the advection and reaction groups are excluded. Express the final answer as an exact analytic expression and in dimensionless units. No rounding is required. The final answer must be a single real-valued expression.",
            "solution": "The optimal coefficient vector $\\boldsymbol{\\beta}^*$ is found by minimizing the Group LASSO objective function:\n$$ J(\\boldsymbol{\\beta}) = \\frac{1}{2n} \\left\\| \\boldsymbol{y} - \\sum_{g} X_g \\boldsymbol{\\beta}_g \\right\\|_{2}^{2} + \\lambda \\sum_{g} w_g \\|\\boldsymbol{\\beta}_g\\|_2 $$\nThe solution must satisfy the Karush-Kuhn-Tucker (KKT) conditions. Due to the block-orthogonal design of the matrix $X$ (where $X_g^\\top X_h = 0$ for $g \\neq h$ and $(1/n)X_g^\\top X_g = I_{d_g}$), the optimality conditions for each group of coefficients $\\boldsymbol{\\beta}_g$ can be considered separately.\n\nThe subgradient of the objective function with respect to a group $\\boldsymbol{\\beta}_g$ is:\n$$ \\partial_{\\boldsymbol{\\beta}_g} J = \\nabla_{\\boldsymbol{\\beta}_g} \\left( \\frac{1}{2n} \\| \\boldsymbol{y} - X \\boldsymbol{\\beta} \\|_2^2 \\right) + \\partial \\left( \\lambda w_g \\|\\boldsymbol{\\beta}_g\\|_2 \\right) $$\nThe gradient of the least-squares term simplifies thanks to orthogonality:\n$$ \\nabla_{\\boldsymbol{\\beta}_g} (\\dots) = -\\frac{1}{n} X_g^{\\top} (\\boldsymbol{y} - X_g \\boldsymbol{\\beta}_g) = -\\frac{1}{n} X_g^{\\top} \\boldsymbol{y} + \\boldsymbol{\\beta}_g $$\nThe subgradient of the penalty term is:\n$$ \\partial \\left( \\lambda w_g \\|\\boldsymbol{\\beta}_g\\|_2 \\right) = \\begin{cases} \\{ \\lambda w_g \\frac{\\boldsymbol{\\beta}_g}{\\|\\boldsymbol{\\beta}_g\\|_2} \\} & \\text{if } \\boldsymbol{\\beta}_g \\neq \\boldsymbol{0} \\\\ \\{ \\boldsymbol{v} \\in \\mathbb{R}^{d_g} : \\|\\boldsymbol{v}\\|_2 \\le \\lambda w_g \\} & \\text{if } \\boldsymbol{\\beta}_g = \\boldsymbol{0} \\end{cases} $$\nSetting the subgradient to zero, we derive the conditions for group selection.\n\n**Condition for Group Exclusion ($\\boldsymbol{\\beta}_g^* = \\boldsymbol{0}$):**\nIf the optimal coefficient vector is zero, the KKT condition becomes $\\boldsymbol{0} \\in -\\frac{1}{n} X_g^{\\top} \\boldsymbol{y} + \\{ \\boldsymbol{v} : \\|\\boldsymbol{v}\\|_2 \\le \\lambda w_g \\}$. This implies that the term $-\\frac{1}{n} X_g^{\\top} \\boldsymbol{y}$ must be contained within the subdifferential ball, which leads to the necessary and sufficient condition for exclusion:\n$$ \\left\\| \\frac{1}{n} X_g^{\\top} \\boldsymbol{y} \\right\\|_2 \\le \\lambda w_g $$\n\n**Condition for Group Inclusion ($\\boldsymbol{\\beta}_g^* \\neq \\boldsymbol{0}$):**\nIf the optimal coefficient vector is non-zero, the subgradient is a single value, and the KKT condition is an equality: $\\boldsymbol{0} = -\\frac{1}{n} X_g^{\\top} \\boldsymbol{y} + \\boldsymbol{\\beta}_g^* + \\lambda w_g \\frac{\\boldsymbol{\\beta}_g^*}{\\|\\boldsymbol{\\beta}_g^*\\|_2}$. This implies that $\\boldsymbol{\\beta}_g^* \\neq \\boldsymbol{0}$ if and only if:\n$$ \\left\\| \\frac{1}{n} X_g^{\\top} \\boldsymbol{y} \\right\\|_2 > \\lambda w_g $$\nThe solution is given by $\\boldsymbol{\\beta}_g^* = \\left(1 - \\frac{\\lambda w_g}{\\| \\frac{1}{n} X_g^{\\top} \\boldsymbol{y} \\|_2}\\right) \\frac{1}{n} X_g^{\\top} \\boldsymbol{y}$, a group-wise soft thresholding operation.\n\nTo find the minimal $\\lambda$ that excludes the advection and reaction groups but includes the diffusion group, we must satisfy:\n1.  Advection excluded: $\\lambda \\ge \\frac{1}{w_{\\mathrm{adv}}} \\left\\| \\frac{1}{n} X_{\\mathrm{adv}}^{\\top} \\boldsymbol{y} \\right\\|_2$\n2.  Reaction excluded: $\\lambda \\ge \\frac{1}{w_{\\mathrm{react}}} \\left\\| \\frac{1}{n} X_{\\mathrm{react}}^{\\top} \\boldsymbol{y} \\right\\|_2$\n3.  Diffusion active: $\\lambda < \\frac{1}{w_{\\mathrm{diff}}} \\left\\| \\frac{1}{n} X_{\\mathrm{diff}}^{\\top} \\boldsymbol{y} \\right\\|_2$\n\nThis requires $\\max\\left(\\frac{1}{w_{\\mathrm{adv}}n}\\|X_{\\mathrm{adv}}^{\\top} \\boldsymbol{y}\\|_2, \\frac{1}{w_{\\mathrm{react}}n}\\|X_{\\mathrm{react}}^{\\top} \\boldsymbol{y}\\|_2\\right) \\le \\lambda < \\frac{1}{w_{\\mathrm{diff}}n}\\|X_{\\mathrm{diff}}^{\\top} \\boldsymbol{y}\\|_2$. The minimal such $\\lambda$ is the lower bound of this interval.\n    \nUsing the given data ($n=50$, $d_{\\mathrm{adv}}=2, d_{\\mathrm{diff}}=1, d_{\\mathrm{react}}=3$, and $w_g=\\sqrt{d_g}$):\n- $\\|X_{\\mathrm{adv}}^{\\top} \\boldsymbol{y}\\|_2 = \\sqrt{9^2 + (-3)^2} = \\sqrt{90} = 3\\sqrt{10}$\n- $\\|X_{\\mathrm{diff}}^{\\top} \\boldsymbol{y}\\|_2 = |22| = 22$\n- $\\|X_{\\mathrm{react}}^{\\top} \\boldsymbol{y}\\|_2 = \\sqrt{4^2 + (-2)^2 + 1^2} = \\sqrt{21}$\n\nThe critical thresholds for each group are:\n- Advection: $\\frac{1}{w_{\\mathrm{adv}} n} \\|X_{\\mathrm{adv}}^{\\top} \\boldsymbol{y}\\|_2 = \\frac{1}{\\sqrt{2} \\cdot 50} (3\\sqrt{10}) = \\frac{3\\sqrt{5}}{50}$\n- Reaction: $\\frac{1}{w_{\\mathrm{react}} n} \\|X_{\\mathrm{react}}^{\\top} \\boldsymbol{y}\\|_2 = \\frac{1}{\\sqrt{3} \\cdot 50} (\\sqrt{21}) = \\frac{\\sqrt{7}}{50}$\n- Diffusion: $\\frac{1}{w_{\\mathrm{diff}} n} \\|X_{\\mathrm{diff}}^{\\top} \\boldsymbol{y}\\|_2 = \\frac{1}{1 \\cdot 50} (22) = \\frac{22}{50}$\n\nWe need to find $\\lambda_{\\min} = \\max\\left(\\frac{3\\sqrt{5}}{50}, \\frac{\\sqrt{7}}{50}\\right)$. Comparing the numerators by squaring them: $(3\\sqrt{5})^2 = 45$ and $(\\sqrt{7})^2 = 7$. Since $45 > 7$, the maximum is the advection threshold.\n$$ \\lambda_{\\min} = \\frac{3\\sqrt{5}}{50} $$\nWe check that this value is less than the diffusion threshold: $3\\sqrt{5} \\approx 3 \\times 2.236 = 6.708$, which is less than $22$. The condition is satisfied, so an interval for $\\lambda$ exists. The minimal value is indeed the point where the advection group is set to zero.",
            "answer": "$$\\boxed{\\frac{3\\sqrt{5}}{50}}$$"
        }
    ]
}