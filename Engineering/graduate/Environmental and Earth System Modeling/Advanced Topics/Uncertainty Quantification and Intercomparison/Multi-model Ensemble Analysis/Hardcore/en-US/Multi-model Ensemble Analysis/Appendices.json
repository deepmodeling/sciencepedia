{
    "hands_on_practices": [
        {
            "introduction": "This practice builds the foundation of ensemble weighting. We start from the core objective of minimizing the mean squared error to derive the famous inverse-variance weighting scheme, demonstrating why more precise models should receive greater influence in an ensemble forecast. This exercise solidifies the connection between a statistical goal and the resulting optimal combination of independent information sources. ",
            "id": "3897949",
            "problem": "Consider a Multi-Model Ensemble (MME) forecast for seasonal basin-mean precipitation anomaly in an environmental and earth system modeling context. Let the unknown true anomaly be a scalar quantity denoted by $y$. Suppose $K$ independent numerical models produce scalar predictors $X_{1}, X_{2}, \\dots, X_{K}$ for the same target $y$. Each predictor is unbiased for $y$, meaning $\\mathbb{E}[X_{i}] = y$, and has finite variance $\\operatorname{Var}(X_{i}-y) = \\sigma_{i}^{2}$. Define a linear ensemble estimator $\\hat{y} = \\sum_{i=1}^{K} w_{i} X_{i}$ subject to the normalization constraint $\\sum_{i=1}^{K} w_{i} = 1$ to preserve unbiasedness of the ensemble estimate under the unbiasedness of the individual predictors.\n\nStarting from the definition of mean squared error, derive the weights $w_{i}$ that minimize the expected squared error $\\mathbb{E}\\left[(\\hat{y} - y)^{2}\\right]$ under the assumptions stated above. Your derivation must use first principles: the definition of mean squared error, the independence of model errors, and the normalization constraint. Clearly justify the convexity of the optimization problem and the uniqueness of the solution.\n\nThen, apply your derived formula to the following scientifically realistic hindcast-based variances for $K=4$ independent models (expressed in $\\text{mm}^{2}$ for seasonal anomalies):\n$$\\sigma_{1}^{2} = 1,\\quad \\sigma_{2}^{2} = 4,\\quad \\sigma_{3}^{2} = 9,\\quad \\sigma_{4}^{2} = 16.$$\nCompute the normalized weights $w_{1}, w_{2}, w_{3}, w_{4}$ for the ensemble estimator. Express the weights as dimensionless numbers, and round each to four significant figures.\n\nFinally, explain the conditions under which the derived weights are optimal in the sense of minimizing mean squared error, and articulate how the solution would generalize if the model errors were not independent but had a known, positive-definite covariance matrix.\n\nThe final answer must be the computed weight vector for the given numerical variances.",
            "solution": "We start from the definition of the ensemble estimator $\\hat{y} = \\sum_{i=1}^{K} w_{i} X_{i}$ with the normalization constraint $\\sum_{i=1}^{K} w_{i} = 1$. Because each predictor is unbiased, $\\mathbb{E}[X_{i}] = y$, the ensemble estimator is unbiased if $\\sum_{i=1}^{K} w_{i} = 1$, since\n$$\n\\mathbb{E}[\\hat{y}] = \\mathbb{E}\\left[\\sum_{i=1}^{K} w_{i} X_{i}\\right] = \\sum_{i=1}^{K} w_{i} \\mathbb{E}[X_{i}] = \\sum_{i=1}^{K} w_{i} y = y.\n$$\nDefine the individual errors as $e_{i} = X_{i} - y$. By assumption, $\\mathbb{E}[e_{i}] = 0$ and $\\operatorname{Var}(e_{i}) = \\sigma_{i}^{2}$, and the errors $\\{e_{i}\\}$ are mutually independent. The ensemble error is\n$$\n\\hat{y} - y = \\sum_{i=1}^{K} w_{i} X_{i} - y = \\sum_{i=1}^{K} w_{i} (y + e_{i}) - y = \\sum_{i=1}^{K} w_{i} e_{i}.\n$$\nThe mean squared error of $\\hat{y}$ is\n$$\n\\mathbb{E}\\left[(\\hat{y} - y)^{2}\\right] = \\mathbb{E}\\left[\\left(\\sum_{i=1}^{K} w_{i} e_{i}\\right)^{2}\\right].\n$$\nBecause $\\mathbb{E}[e_{i}] = 0$, $\\mathbb{E}\\left[(\\hat{y} - y)^{2}\\right]$ equals the variance of the ensemble error:\n$$\n\\operatorname{Var}\\left(\\sum_{i=1}^{K} w_{i} e_{i}\\right) = \\sum_{i=1}^{K} w_{i}^{2} \\operatorname{Var}(e_{i}) + 2 \\sum_{1 \\leq i < j \\leq K} w_{i} w_{j} \\operatorname{Cov}(e_{i}, e_{j}).\n$$\nUnder independence, $\\operatorname{Cov}(e_{i}, e_{j}) = 0$ for $i \\neq j$, so\n$$\n\\mathbb{E}\\left[(\\hat{y} - y)^{2}\\right] = \\sum_{i=1}^{K} w_{i}^{2} \\sigma_{i}^{2}.\n$$\nThe optimization problem is thus\n$$\n\\min_{w_{1},\\dots,w_{K}} \\sum_{i=1}^{K} w_{i}^{2} \\sigma_{i}^{2} \\quad \\text{subject to} \\quad \\sum_{i=1}^{K} w_{i} = 1.\n$$\nThis is a convex quadratic program, since the objective is a quadratic form with Hessian matrix $\\mathbf{H} = 2 \\operatorname{diag}(\\sigma_{1}^{2}, \\dots, \\sigma_{K}^{2})$, which is positive definite because each $\\sigma_{i}^{2} > 0$. Therefore, any stationary point subject to the linear equality constraint is the unique global minimizer.\n\nWe solve using the method of Lagrange multipliers. Define the Lagrangian\n$$\n\\mathcal{L}(w_{1}, \\dots, w_{K}, \\lambda) = \\sum_{i=1}^{K} w_{i}^{2} \\sigma_{i}^{2} - \\lambda\\left(\\sum_{i=1}^{K} w_{i} - 1\\right).\n$$\nSet partial derivatives to zero:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial w_{i}} = 2 w_{i} \\sigma_{i}^{2} - \\lambda = 0 \\quad \\Rightarrow \\quad w_{i} = \\frac{\\lambda}{2 \\sigma_{i}^{2}} \\quad \\text{for each } i,\n$$\nand\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\lambda} = -\\left(\\sum_{i=1}^{K} w_{i} - 1\\right) = 0 \\quad \\Rightarrow \\quad \\sum_{i=1}^{K} w_{i} = 1.\n$$\nSubstitute $w_{i}$ into the constraint:\n$$\n\\sum_{i=1}^{K} \\frac{\\lambda}{2 \\sigma_{i}^{2}} = 1 \\quad \\Rightarrow \\quad \\frac{\\lambda}{2} \\sum_{i=1}^{K} \\frac{1}{\\sigma_{i}^{2}} = 1 \\quad \\Rightarrow \\quad \\lambda = \\frac{2}{\\sum_{j=1}^{K} \\sigma_{j}^{-2}}.\n$$\nTherefore,\n$$\nw_{i} = \\frac{\\lambda}{2 \\sigma_{i}^{2}} = \\frac{\\sigma_{i}^{-2}}{\\sum_{j=1}^{K} \\sigma_{j}^{-2}}.\n$$\nThus the optimal weights are proportional to the inverse variances and normalized to sum to one:\n$$\nw_{i} \\propto \\sigma_{i}^{-2}, \\quad \\text{with} \\quad w_{i} = \\frac{\\sigma_{i}^{-2}}{\\sum_{j=1}^{K} \\sigma_{j}^{-2}}.\n$$\nUniqueness follows from the strict convexity noted earlier.\n\nApply to the given numerical variances $\\sigma_{1}^{2} = 1$, $\\sigma_{2}^{2} = 4$, $\\sigma_{3}^{2} = 9$, $\\sigma_{4}^{2} = 16$. Compute inverse variances:\n$$\n\\sigma_{1}^{-2} = 1,\\quad \\sigma_{2}^{-2} = \\frac{1}{4},\\quad \\sigma_{3}^{-2} = \\frac{1}{9},\\quad \\sigma_{4}^{-2} = \\frac{1}{16}.\n$$\nSum of inverse variances:\n$$\nS = 1 + \\frac{1}{4} + \\frac{1}{9} + \\frac{1}{16} = \\frac{144 + 36 + 16 + 9}{144} = \\frac{205}{144}.\n$$\nNormalized weights:\n$$\nw_{1} = \\frac{1}{S} = \\frac{144}{205},\\quad\nw_{2} = \\frac{\\frac{1}{4}}{S} = \\frac{36}{205},\\quad\nw_{3} = \\frac{\\frac{1}{9}}{S} = \\frac{16}{205},\\quad\nw_{4} = \\frac{\\frac{1}{16}}{S} = \\frac{9}{205}.\n$$\nConvert to decimal form and round each to four significant figures:\n$$\nw_{1} \\approx 0.702439024 \\to 0.7024,\\quad\nw_{2} \\approx 0.175609756 \\to 0.1756,\\quad\nw_{3} \\approx 0.078048780 \\to 0.07805,\\quad\nw_{4} \\approx 0.043902439 \\to 0.04390.\n$$\n\nConditions for optimality: The inverse-variance normalized weights minimize $\\mathbb{E}\\left[(\\hat{y} - y)^{2}\\right]$ under the following conditions:\n- The individual predictors are unbiased, $\\mathbb{E}[X_{i}] = y$, so the ensemble can be unbiased with $\\sum_{i} w_{i} = 1$.\n- The errors $e_{i} = X_{i} - y$ are mutually independent, so the ensemble variance reduces to $\\sum_{i} w_{i}^{2} \\sigma_{i}^{2}$ without cross terms.\n- The variances $\\sigma_{i}^{2}$ are correctly specified (or consistently estimable), and are positive.\n- The objective is mean squared error; no alternative loss or constraints are imposed.\n\nIf the errors are correlated with a known positive-definite covariance matrix $\\boldsymbol{\\Sigma} = \\operatorname{Cov}(\\mathbf{e})$, where $\\mathbf{e} = (e_{1},\\dots,e_{K})^{\\top}$, then the ensemble MSE becomes\n$$\n\\mathbb{E}\\left[(\\hat{y} - y)^{2}\\right] = \\operatorname{Var}\\left(\\sum_{i=1}^{K} w_{i} e_{i}\\right) = \\mathbf{w}^{\\top} \\boldsymbol{\\Sigma} \\mathbf{w},\n$$\nwhere $\\mathbf{w} = (w_{1},\\dots,w_{K})^{\\top}$ and the unbiasedness constraint is $\\mathbf{1}^{\\top} \\mathbf{w} = 1$ with $\\mathbf{1}$ the vector of ones. The unique minimizer is obtained by solving the constrained quadratic program via Lagrange multipliers, yielding the generalized inverse-covariance weights\n$$\n\\mathbf{w}^{\\star} = \\frac{\\boldsymbol{\\Sigma}^{-1} \\mathbf{1}}{\\mathbf{1}^{\\top} \\boldsymbol{\\Sigma}^{-1} \\mathbf{1}},\n$$\nwhich reduces to the inverse-variance formula derived above when $\\boldsymbol{\\Sigma}$ is diagonal (independent errors). These weights are the Best Linear Unbiased Estimator (BLUE) under the Gaussâ€“Markov assumptions; normality is not required for MSE optimality, only finite second moments and the stated covariance structure.",
            "answer": "$$\\boxed{\\begin{pmatrix}0.7024 & 0.1756 & 0.07805 & 0.04390\\end{pmatrix}}$$"
        },
        {
            "introduction": "Real-world models are rarely fully independent, as they often share code, parameterizations, or underlying assumptions. This exercise extends our understanding by incorporating model dependence, quantified by error correlation, into the optimization problem. You will derive how optimal weights must be adjusted when models are correlated and quantify the resulting change in the ensemble's performance, highlighting the critical importance of accounting for model genealogy. ",
            "id": "3897961",
            "problem": "Consider two climate model simulators indexed by $i \\in \\{1,2\\}$ that output monthly regional mean surface air temperature anomalies $Y_i$ (in $\\mathrm{K}$). Let the true but unknown anomaly be $X$ (in $\\mathrm{K}$). Each model has a known additive bias $b_i$ (in $\\mathrm{K}$), so that a bias-corrected model output is $Y_i - b_i$. Assume the following data-generating representation: $Y_i = X + b_i + \\varepsilon_i$, where $\\mathbb{E}[\\varepsilon_i] = 0$ and $\\operatorname{Var}(\\varepsilon_i) = \\sigma_i^2$. You are given $b_1 = -0.3\\,\\mathrm{K}$, $b_2 = 0.1\\,\\mathrm{K}$, $\\sigma_1^2 = 1.44\\,\\mathrm{K}^2$, and $\\sigma_2^2 = 0.81\\,\\mathrm{K}^2$.\n\nYour tasks are:\n\n- Using only first principles of unbiasedness and variance minimization for linear estimators, construct the minimum-variance unbiased linear estimator of $X$ under the assumption that the model errors are independent. The estimator must have the form $\\hat{X}_{\\mathrm{ind}} = w_1\\,(Y_1 - b_1) + w_2\\,(Y_2 - b_2)$ with $w_1 + w_2 = 1$.\n\n- Now allow for model dependence through a nonzero error correlation, with $\\operatorname{Cov}(\\varepsilon_1,\\varepsilon_2) = \\rho\\,\\sigma_1\\sigma_2$, where $\\rho \\in (-1,1)$ is an unknown but fixed correlation coefficient. Construct the corresponding minimum-variance unbiased linear estimator $\\hat{X}_{\\mathrm{dep}} = w_1(\\rho)\\,(Y_1 - b_1) + w_2(\\rho)\\,(Y_2 - b_2)$ with $w_1(\\rho) + w_2(\\rho) = 1$.\n\n- Quantify the impact of dependence by deriving a closed-form analytic expression, as a function of $\\rho$ only, for the ratio of the estimator variance under dependence to that under independence, using the given $\\sigma_1^2$ and $\\sigma_2^2$. Your final answer must be this single simplified analytic expression in terms of $\\rho$ alone. Do not evaluate numerically and do not include units in the final answer.",
            "solution": "Let the bias-corrected model outputs be denoted by $Z_i = Y_i - b_i$. From the data-generating model $Y_i = X + b_i + \\varepsilon_i$, we can write $Z_i = (X + b_i + \\varepsilon_i) - b_i = X + \\varepsilon_i$. The random variable $X$ representing the true anomaly is treated as a fixed, unknown constant for the purpose of constructing the estimator. The expectation and variance of $Z_i$ are:\n$$\n\\mathbb{E}[Z_i] = \\mathbb{E}[X + \\varepsilon_i] = \\mathbb{E}[X] + \\mathbb{E}[\\varepsilon_i] = X + 0 = X\n$$\n$$\n\\operatorname{Var}(Z_i) = \\operatorname{Var}(X + \\varepsilon_i) = \\operatorname{Var}(\\varepsilon_i) = \\sigma_i^2\n$$\nThe problem considers a linear estimator of the form $\\hat{X} = w_1 Z_1 + w_2 Z_2$, which is equivalent to the requested form $\\hat{X} = w_1(Y_1 - b_1) + w_2(Y_2 - b_2)$.\n\nFirst, we verify the condition for unbiasedness. An estimator $\\hat{X}$ is unbiased for $X$ if $\\mathbb{E}[\\hat{X}] = X$.\n$$\n\\mathbb{E}[\\hat{X}] = \\mathbb{E}[w_1 Z_1 + w_2 Z_2] = w_1 \\mathbb{E}[Z_1] + w_2 \\mathbb{E}[Z_2] = w_1 X + w_2 X = (w_1 + w_2)X\n$$\nFor this to equal $X$ for any value of $X$, it is required that $w_1 + w_2 = 1$. This constraint is explicitly provided in the problem statement for both the independent and dependent cases, ensuring any estimator of the prescribed form is unbiased.\n\nThe variance of the general linear estimator is given by:\n$$\n\\operatorname{Var}(\\hat{X}) = \\operatorname{Var}(w_1 Z_1 + w_2 Z_2) = \\operatorname{Var}(w_1 (X + \\varepsilon_1) + w_2 (X + \\varepsilon_2)) = \\operatorname{Var}(w_1\\varepsilon_1 + w_2\\varepsilon_2)\n$$\n$$\n\\operatorname{Var}(\\hat{X}) = w_1^2 \\operatorname{Var}(\\varepsilon_1) + w_2^2 \\operatorname{Var}(\\varepsilon_2) + 2 w_1 w_2 \\operatorname{Cov}(\\varepsilon_1, \\varepsilon_2)\n$$\n$$\n\\operatorname{Var}(\\hat{X}) = w_1^2 \\sigma_1^2 + w_2^2 \\sigma_2^2 + 2 w_1 w_2 \\rho\\sigma_1\\sigma_2\n$$\n\nWe are tasked with minimizing this variance subject to the constraint $w_1 + w_2 = 1$. We can express $w_2 = 1 - w_1$ and minimize the variance as a function of $w_1$ alone.\n\n**Case 1: Independent Errors ($\\hat{X}_{\\mathrm{ind}}$)**\n\nFor this case, the errors are independent, which means their covariance is zero. The correlation coefficient $\\rho$ is $0$. The variance of the estimator, which we denote $V_{\\mathrm{ind}}$, is:\n$$\nV_{\\mathrm{ind}}(w_1) = w_1^2 \\sigma_1^2 + w_2^2 \\sigma_2^2 = w_1^2 \\sigma_1^2 + (1-w_1)^2 \\sigma_2^2\n$$\nTo find the weight $w_1$ that minimizes this variance, we take the derivative with respect to $w_1$ and set it to zero:\n$$\n\\frac{dV_{\\mathrm{ind}}}{dw_1} = 2w_1 \\sigma_1^2 - 2(1-w_1)\\sigma_2^2 = 0\n$$\n$$\nw_1 \\sigma_1^2 = (1-w_1)\\sigma_2^2 \\implies w_1 \\sigma_1^2 = \\sigma_2^2 - w_1\\sigma_2^2\n$$\n$$\nw_1(\\sigma_1^2 + \\sigma_2^2) = \\sigma_2^2 \\implies w_1 = \\frac{\\sigma_2^2}{\\sigma_1^2 + \\sigma_2^2}\n$$\nThe corresponding weight $w_2$ is:\n$$\nw_2 = 1 - w_1 = 1 - \\frac{\\sigma_2^2}{\\sigma_1^2 + \\sigma_2^2} = \\frac{\\sigma_1^2}{\\sigma_1^2 + \\sigma_2^2}\n$$\nThe minimum variance under independence, $\\operatorname{Var}(\\hat{X}_{\\mathrm{ind}})$, is found by substituting these optimal weights back into the variance expression:\n$$\n\\operatorname{Var}(\\hat{X}_{\\mathrm{ind}}) = \\left(\\frac{\\sigma_2^2}{\\sigma_1^2 + \\sigma_2^2}\\right)^2 \\sigma_1^2 + \\left(\\frac{\\sigma_1^2}{\\sigma_1^2 + \\sigma_2^2}\\right)^2 \\sigma_2^2\n$$\n$$\n\\operatorname{Var}(\\hat{X}_{\\mathrm{ind}}) = \\frac{\\sigma_2^4 \\sigma_1^2 + \\sigma_1^4 \\sigma_2^2}{(\\sigma_1^2 + \\sigma_2^2)^2} = \\frac{\\sigma_1^2 \\sigma_2^2 (\\sigma_2^2 + \\sigma_1^2)}{(\\sigma_1^2 + \\sigma_2^2)^2} = \\frac{\\sigma_1^2 \\sigma_2^2}{\\sigma_1^2 + \\sigma_2^2}\n$$\n\n**Case 2: Dependent Errors ($\\hat{X}_{\\mathrm{dep}}$)**\n\nNow, we allow for $\\rho \\in (-1,1)$. The variance of the estimator, $V_{\\mathrm{dep}}$, is:\n$$\nV_{\\mathrm{dep}}(w_1) = w_1^2 \\sigma_1^2 + (1-w_1)^2 \\sigma_2^2 + 2w_1(1-w_1)\\rho\\sigma_1\\sigma_2\n$$\nTo find the optimal weight $w_1(\\rho)$, we again differentiate with respect to $w_1$ and set to zero:\n$$\n\\frac{dV_{\\mathrm{dep}}}{dw_1} = 2w_1 \\sigma_1^2 - 2(1-w_1)\\sigma_2^2 + 2(1-2w_1)\\rho\\sigma_1\\sigma_2 = 0\n$$\n$$\nw_1 \\sigma_1^2 - \\sigma_2^2 + w_1\\sigma_2^2 + \\rho\\sigma_1\\sigma_2 - 2w_1\\rho\\sigma_1\\sigma_2 = 0\n$$\n$$\nw_1(\\sigma_1^2 + \\sigma_2^2 - 2\\rho\\sigma_1\\sigma_2) = \\sigma_2^2 - \\rho\\sigma_1\\sigma_2\n$$\n$$\nw_1(\\rho) = \\frac{\\sigma_2^2 - \\rho\\sigma_1\\sigma_2}{\\sigma_1^2 + \\sigma_2^2 - 2\\rho\\sigma_1\\sigma_2}\n$$\nThe corresponding weight $w_2(\\rho)$ is:\n$$\nw_2(\\rho) = 1 - w_1(\\rho) = \\frac{(\\sigma_1^2 + \\sigma_2^2 - 2\\rho\\sigma_1\\sigma_2) - (\\sigma_2^2 - \\rho\\sigma_1\\sigma_2)}{\\sigma_1^2 + \\sigma_2^2 - 2\\rho\\sigma_1\\sigma_2} = \\frac{\\sigma_1^2 - \\rho\\sigma_1\\sigma_2}{\\sigma_1^2 + \\sigma_2^2 - 2\\rho\\sigma_1\\sigma_2}\n$$\nThe minimum variance is $\\frac{\\sigma_1^2\\sigma_2^2(1-\\rho^2)}{\\sigma_1^2 + \\sigma_2^2 - 2\\rho\\sigma_1\\sigma_2}$.\n\n**Quantifying the Impact of Dependence**\n\nThe final task is to derive the ratio $R(\\rho) = \\frac{\\operatorname{Var}(\\hat{X}_{\\mathrm{dep}})}{\\operatorname{Var}(\\hat{X}_{\\mathrm{ind}})}$.\n$$\nR(\\rho) = \\frac{\\frac{\\sigma_1^2 \\sigma_2^2(1-\\rho^2)}{\\sigma_1^2 + \\sigma_2^2 - 2\\rho\\sigma_1\\sigma_2}}{\\frac{\\sigma_1^2 \\sigma_2^2}{\\sigma_1^2 + \\sigma_2^2}}\n$$\nThe term $\\sigma_1^2\\sigma_2^2$ cancels out:\n$$\nR(\\rho) = \\frac{1-\\rho^2}{\\sigma_1^2 + \\sigma_2^2 - 2\\rho\\sigma_1\\sigma_2} \\times (\\sigma_1^2 + \\sigma_2^2) = \\frac{(1-\\rho^2)(\\sigma_1^2 + \\sigma_2^2)}{\\sigma_1^2 + \\sigma_2^2 - 2\\rho\\sigma_1\\sigma_2}\n$$\nNow, we substitute the given values: $\\sigma_1^2 = 1.44$ and $\\sigma_2^2 = 0.81$. This implies $\\sigma_1 = \\sqrt{1.44} = 1.2$ and $\\sigma_2 = \\sqrt{0.81} = 0.9$.\nThe required components are:\n$$\n\\sigma_1^2 + \\sigma_2^2 = 1.44 + 0.81 = 2.25\n$$\n$$\n\\sigma_1\\sigma_2 = 1.2 \\times 0.9 = 1.08\n$$\n$$\n2\\sigma_1\\sigma_2 = 2 \\times 1.08 = 2.16\n$$\nSubstituting these into the expression for $R(\\rho)$:\n$$\nR(\\rho) = \\frac{(1-\\rho^2)(2.25)}{2.25 - 2.16\\rho}\n$$\nTo simplify this expression, we can multiply the numerator and denominator by $100$ to eliminate decimals:\n$$\nR(\\rho) = \\frac{225(1-\\rho^2)}{225 - 216\\rho}\n$$\nWe find the greatest common divisor of $225$ and $216$. The prime factorization of $225$ is $3^2 \\times 5^2$. The prime factorization of $216$ is $2^3 \\times 3^3$. The greatest common divisor is $3^2 = 9$. Dividing the coefficients by $9$:\n$$\n225 / 9 = 25\n$$\n$$\n216 / 9 = 24\n$$\nThus, the simplified closed-form expression for the ratio is:\n$$\nR(\\rho) = \\frac{25(1-\\rho^2)}{25 - 24\\rho}\n$$\nThis expression quantifies the ratio of estimator variances solely as a function of the error correlation $\\rho$, given the specified model error characteristics.",
            "answer": "$$\\boxed{\\frac{25(1 - \\rho^2)}{25 - 24\\rho}}$$"
        },
        {
            "introduction": "After deriving theoretical weights, a crucial step is to empirically verify if one weighting scheme truly outperforms another on a set of forecasts. This hands-on coding practice guides you through implementing a paired block bootstrap test, a robust statistical tool for comparing time series of forecast scores while accounting for temporal autocorrelation. By completing this exercise, you will gain a practical skill for rigorously evaluating and comparing the performance of different multi-model ensemble strategies. ",
            "id": "3897964",
            "problem": "You are given two competing ensemble weighting schemes in environmental and earth system modeling and asked to assess whether one significantly outperforms the other according to the Continuous Ranked Probability Score (CRPS). By definition, for a probabilistic forecast with cumulative distribution function $F(z)$ and a realized observation $y$, the Continuous Ranked Probability Score (CRPS) is the integral $$\\mathrm{CRPS}(F,y) = \\int_{-\\infty}^{\\infty} \\left(F(z) - \\mathbb{1}\\{z \\ge y\\}\\right)^2 \\, \\mathrm{d}z,$$ which has the same physical units as the forecasted quantity. Lower values of $\\mathrm{CRPS}$ indicate better probabilistic forecasts. In this problem, the CRPS is already computed for each day and scheme; you will not compute CRPS from $F$ and $y$.\n\nYou must construct a paired block bootstrap hypothesis test to assess whether ensemble weighting scheme $A$ significantly outperforms scheme $B$ over a specified period, accounting for temporal autocorrelation. Let there be a sequence of $N$ days indexed by $t \\in \\{1,2,\\dots,N\\}$. Let $C^{A}_t$ and $C^{B}_t$ denote the daily CRPS values for scheme $A$ and scheme $B$ respectively. Define the paired daily improvement $$d_t = C^{B}_t - C^{A}_t,$$ which is positive when scheme $A$ has lower CRPS than scheme $B$ on day $t$. The inferential target is the mean improvement $$\\mu_d = \\mathbb{E}[d_t].$$ You are to test the one-sided hypothesis $$H_0: \\mu_d \\le 0 \\quad \\text{versus} \\quad H_1: \\mu_d > 0,$$ where rejecting the null hypothesis supports that scheme $A$ significantly outperforms scheme $B$.\n\nTo maintain scientific realism, use the block bootstrap to handle autocorrelation in $\\{d_t\\}$. Partition the time series into non-overlapping contiguous blocks of length $L$ days; the final block may be shorter if $L$ does not divide $N$. Let the sample mean be $$\\bar{d} = \\frac{1}{N} \\sum_{t=1}^N d_t.$$ Create a mean-zero series under the null hypothesis by centering the differences: $$r_t = d_t - \\bar{d}.$$ For each bootstrap replicate $i \\in \\{1,2,\\dots,R\\}$, sample blocks with replacement from the set of original blocks, concatenate the sampled blocks of $\\{r_t\\}$ until you reach at least $N$ elements, truncate to $N$ elements, and compute the replicate mean $\\bar{r}^{*(i)}$. An approximate one-sided p-value for $H_0: \\mu_d \\le 0$ is then $$p = \\frac{1}{R} \\sum_{i=1}^R \\mathbb{1}\\left\\{\\bar{r}^{*(i)} \\ge \\bar{d}\\right\\}.$$ Decide that scheme $A$ significantly outperforms scheme $B$ if $p < \\alpha$, for a chosen significance level $\\alpha \\in (0,1)$.\n\nYou must implement this test in a complete, runnable program. The program must generate synthetic but scientifically plausible daily CRPS sequences for each test case, using autoregressive processes to emulate temporal autocorrelation. Specifically, for each test case:\n- Generate a baseline autoregressive process $\\{X_t\\}$ of order $1$ (AR(1)) with parameter $\\phi \\in (-1, 1)$ and Gaussian innovations with standard deviation $\\sigma$, namely $$X_t = \\phi X_{t-1} + \\epsilon_t, \\quad \\epsilon_t \\sim \\mathcal{N}(0, \\sigma_t^2),$$ with $X_0 = 0$ and either constant $\\sigma_t = \\sigma$ or a specified piecewise-constant heteroskedastic sequence.\n- Generate scheme-specific errors $\\{E^A_t\\}$ and $\\{E^B_t\\}$ as independent AR(1) processes with fixed parameter $\\phi_s = 0.4$ and Gaussian innovations with fixed standard deviation $\\sigma_s = 0.08$ unless otherwise stated.\n- Construct daily CRPS values as $$C^{B}_t = \\beta + X_t + E^B_t, \\qquad C^{A}_t = \\beta + X_t + E^A_t - \\Delta,$$ with $\\beta = 1.0$ ensuring non-negativity, and then enforce non-negativity by replacing negative values with $0$ if any arise due to noise. The quantity $\\Delta$ encodes the expected improvement of scheme $A$ over scheme $B$; positive $\\Delta$ means $A$ tends to have lower CRPS than $B$.\n\nYou must then apply the paired block bootstrap test described above to compute the p-value $p$ and return a boolean decision for each test case indicating whether $p < \\alpha$.\n\nAngle units are not applicable. Physical units for CRPS are the units of the forecasted quantity, but the final outputs are boolean and hence unitless.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3]\").\n\nUse the following test suite, each specified by $(N, \\phi, \\sigma, \\Delta, L, R, \\alpha, \\text{seed}, \\text{heteroskedastic})$, where heteroskedastic indicates whether $\\sigma_t$ changes over time as specified:\n- Case $1$ (general \"happy path\"): $(N = 180, \\phi = 0.6, \\sigma = 0.2, \\Delta = 0.15, L = 10, R = 5000, \\alpha = 0.05, \\text{seed} = 123, \\text{heteroskedastic} = \\text{False})$.\n- Case $2$ (boundary case, independent days): $(N = 180, \\phi = 0.0, \\sigma = 0.25, \\Delta = 0.0, L = 1, R = 3000, \\alpha = 0.05, \\text{seed} = 456, \\text{heteroskedastic} = \\text{False})$.\n- Case $3$ (non-divisible block length, small sample): $(N = 23, \\phi = 0.7, \\sigma = 0.3, \\Delta = 0.10, L = 10, R = 5000, \\alpha = 0.05, \\text{seed} = 321, \\text{heteroskedastic} = \\text{False})$.\n- Case $4$ (opposite effect, scheme $A$ worse): $(N = 120, \\phi = 0.5, \\sigma = 0.25, \\Delta = -0.10, L = 7, R = 4000, \\alpha = 0.05, \\text{seed} = 789, \\text{heteroskedastic} = \\text{False})$.\n- Case $5$ (heteroskedastic autocorrelation): $(N = 240, \\phi = 0.6, \\sigma = 0.1 \\text{ for } t \\le N/2 \\text{ and } \\sigma = 0.3 \\text{ for } t > N/2, \\Delta = 0.12, L = 12, R = 6000, \\alpha = 0.05, \\text{seed} = 2024, \\text{heteroskedastic} = \\text{True})$.\n\nFor each case, generate $\\{C^{A}_t\\}$ and $\\{C^{B}_t\\}$ according to the rules above using the specified parameters, compute the paired differences $\\{d_t\\}$, perform the paired block bootstrap test, and return a boolean indicating whether scheme $A$ significantly outperforms scheme $B$ at level $\\alpha$.\n\nYour program should produce a single line of output containing the list of five booleans corresponding to the five cases, ordered from Case $1$ to Case $5$, as a comma-separated list enclosed in square brackets, for example \"[True,False,True,False,True]\".",
            "solution": "The objective is to implement a paired block bootstrap hypothesis test to determine if a forecast ensemble weighting scheme, denoted as scheme $A$, provides a statistically significant improvement over a competing scheme, $B$. The performance metric is the Continuous Ranked Probability Score (CRPS), where lower values indicate better performance. The improvement of scheme $A$ over scheme $B$ on day $t$ is measured by the difference $d_t = C^B_t - C^A_t$, where $C^A_t$ and $C^B_t$ are the respective CRPS values. A positive value for $d_t$ signifies that scheme $A$ performed better on that day.\n\nThe core inferential task is to test the one-sided hypothesis regarding the true mean improvement, $\\mu_d = \\mathbb{E}[d_t]$:\n$$H_0: \\mu_d \\le 0 \\quad \\text{(Scheme A is not better than Scheme B)}$$\n$$H_1: \\mu_d > 0 \\quad \\text{(Scheme A is significantly better than Scheme B)}$$\n\nThe solution proceeds in three principal stages: synthetic data generation, implementation of the block bootstrap test, and the final decision based on the computed p-value.\n\n**1. Synthetic Data Generation**\n\nTo create a realistic testbed, we first generate synthetic daily CRPS time series, $\\{C^A_t\\}$ and $\\{C^B_t\\}$, for $N$ days. This process emulates the characteristics of real-world environmental data, particularly temporal autocorrelation.\n\nThe daily CRPS values are constructed as follows:\n$$C^{B}_t = \\beta + X_t + E^B_t$$\n$$C^{A}_t = \\beta + X_t + E^A_t - \\Delta$$\n\nHere, $\\beta = 1.0$ is a baseline CRPS value, ensuring the scores are generally positive. $\\{X_t\\}$ represents a common underlying environmental process affecting both schemes. It is modeled as a first-order autoregressive (AR(1)) process:\n$$X_t = \\phi X_{t-1} + \\epsilon_t, \\quad \\text{with } X_0 = 0$$\nwhere $\\phi$ is the autocorrelation parameter and $\\epsilon_t$ are Gaussian innovations, $\\epsilon_t \\sim \\mathcal{N}(0, \\sigma_t^2)$. The standard deviation $\\sigma_t$ can be constant or vary over time (heteroskedasticity).\n\n$\\{E^A_t\\}$ and $\\{E^B_t\\}$ are independent, scheme-specific error terms, also modeled as AR(1) processes with fixed parameters $\\phi_s = 0.4$ and innovation standard deviation $\\sigma_s = 0.08$.\n\nThe parameter $\\Delta$ represents the systematic improvement of scheme $A$ over scheme $B$. If $\\Delta > 0$, scheme $A$ is designed to have a lower CRPS on average. The expected value of the daily difference is $\\mathbb{E}[d_t] = \\mathbb{E}[C^B_t - C^A_t] = \\mathbb{E}[(\\beta + X_t + E^B_t) - (\\beta + X_t + E^A_t - \\Delta)] = \\Delta$. Thus, testing for $\\mu_d > 0$ is equivalent to testing for $\\Delta > 0$.\n\nFinally, since CRPS values cannot be negative, any generated value less than $0$ is set to $0$.\n\n**2. Paired Block Bootstrap Test**\n\nThe block bootstrap is employed because the daily difference series, $d_t = C^B_t - C^A_t = (E^B_t - E^A_t) + \\Delta$, inherits the temporal autocorrelation from the AR(1) error terms. This method preserves the dependence structure by resampling blocks of consecutive observations rather than individual data points.\n\nThe procedure is as follows:\n- **Compute the observed statistic**: Calculate the sample mean of the daily differences, $\\bar{d} = \\frac{1}{N} \\sum_{t=1}^N d_t$. This is our best estimate of $\\mu_d$.\n- **Center the data**: To simulate the null hypothesis ($H_0: \\mu_d = 0$), we create a recentered series $r_t = d_t - \\bar{d}$. This series has a sample mean of exactly $0$ and preserves the autocorrelation structure of the original $\\{d_t\\}$ series.\n- **Form blocks**: The recentered series $\\{r_t\\}$ is partitioned into non-overlapping blocks of length $L$. If $N$ is not perfectly divisible by $L$, the final block will be shorter.\n- **Bootstrap resampling**: For each of $R$ bootstrap replicates:\n    1. A new time series of length at least $N$ is created by sampling blocks with replacement from the set of original blocks. The number of blocks to draw is $\\lceil N/L \\rceil$.\n    2. The resulting concatenated series is truncated to the original length $N$.\n    3. The mean of this bootstrap replicate series, $\\bar{r}^{*(i)}$, is calculated. This process generates an empirical sampling distribution of the mean under the null hypothesis.\n\n**3. P-value Calculation and Decision**\n\nThe one-sided p-value is the proportion of bootstrap means, $\\bar{r}^{*(i)}$, that are as extreme or more extreme than the observed sample mean, $\\bar{d}$. For our alternative hypothesis $H_1: \\mu_d > 0$, this is:\n$$p = \\frac{1}{R} \\sum_{i=1}^R \\mathbb{1}\\left\\{\\bar{r}^{*(i)} \\ge \\bar{d}\\right\\}$$\nwhere $\\mathbb{1}\\{\\cdot\\}$ is the indicator function. This p-value represents the probability of observing a mean improvement of $\\bar{d}$ or greater, given that the true improvement was actually zero.\n\nThe final decision is made by comparing the p-value to a pre-defined significance level, $\\alpha$. If $p < \\alpha$, we reject the null hypothesis $H_0$ and conclude that scheme $A$ provides a statistically significant improvement over scheme $B$. Otherwise, we fail to reject $H_0$. This entire procedure will be implemented for each test case using the specified parameters.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef generate_ar1(n_samples: int, phi: float, sigma_vec: np.ndarray, rng: np.random.Generator) -> np.ndarray:\n    \"\"\"\n    Generates a first-order autoregressive (AR(1)) time series.\n    X_t = phi * X_{t-1} + innovation_t\n    \"\"\"\n    if n_samples == 0:\n        return np.array([])\n    \n    innovations = rng.normal(0, sigma_vec, n_samples)\n    x = np.zeros(n_samples)\n    # Per problem statement X_0 = 0, so the first term is just the first innovation\n    x[0] = innovations[0]\n    for t in range(1, n_samples):\n        x[t] = phi * x[t-1] + innovations[t]\n    return x\n\ndef perform_bootstrap_test(case_params):\n    \"\"\"\n    Generates synthetic data and performs the paired block bootstrap test for a single case.\n    \"\"\"\n    N, phi, sigma, Delta, L, R, alpha, seed, heteroskedastic = case_params\n\n    rng = np.random.default_rng(seed)\n\n    # 1. Generate synthetic CRPS data\n    # Define baseline AR(1) process standard deviation\n    if heteroskedastic:\n        # Case 5 specifics: sigma=0.1 for first half, 0.3 for second half\n        sigma_t = np.full(N, sigma) \n        sigma_t[N//2:] = 0.3\n    else:\n        sigma_t = np.full(N, sigma)\n\n    X_t = generate_ar1(N, phi, sigma_t, rng)\n\n    # Define scheme-specific AR(1) errors\n    phi_s = 0.4\n    sigma_s = 0.08\n    sigma_s_vec = np.full(N, sigma_s)\n    E_A_t = generate_ar1(N, phi_s, sigma_s_vec, rng)\n    E_B_t = generate_ar1(N, phi_s, sigma_s_vec, rng)\n\n    # Construct CRPS values\n    beta = 1.0\n    C_B_t = beta + X_t + E_B_t\n    C_A_t = beta + X_t + E_A_t - Delta\n    \n    # Enforce non-negativity of CRPS\n    C_A_t = np.maximum(0, C_A_t)\n    C_B_t = np.maximum(0, C_B_t)\n\n    # 2. Implement the Paired Block Bootstrap Test\n    d_t = C_B_t - C_A_t\n    \n    if N == 0:\n        return False\n\n    d_bar = np.mean(d_t)\n    \n    # Center the series to simulate the null hypothesis (mean = 0)\n    r_t = d_t - d_bar\n\n    # Partition the centered series into non-overlapping blocks\n    # np.array_split handles non-divisible lengths correctly\n    split_indices = np.arange(L, N, L)\n    blocks = np.array_split(r_t, split_indices)\n\n    # Perform bootstrap resampling\n    bootstrap_means = np.zeros(R)\n    num_blocks_to_sample = int(np.ceil(N / L))\n    \n    for i in range(R):\n        # Sample block indices with replacement\n        block_indices = rng.choice(len(blocks), size=num_blocks_to_sample, replace=True)\n        \n        # Concatenate sampled blocks and truncate to length N\n        bootstrap_series = np.concatenate([blocks[j] for j in block_indices])[:N]\n        \n        # Compute mean of the bootstrap replicate\n        bootstrap_means[i] = np.mean(bootstrap_series)\n\n    # 3. Calculate p-value and determine significance\n    # p = proportion of bootstrap means >= observed mean\n    p_value = np.mean(bootstrap_means >= d_bar)\n    \n    return p_value < alpha\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        # (N, phi, sigma, Delta, L, R, alpha, seed, heteroskedastic)\n        (180, 0.6, 0.2, 0.15, 10, 5000, 0.05, 123, False),\n        (180, 0.0, 0.25, 0.0, 1, 3000, 0.05, 456, False),\n        (23, 0.7, 0.3, 0.10, 10, 5000, 0.05, 321, False),\n        (120, 0.5, 0.25, -0.10, 7, 4000, 0.05, 789, False),\n        (240, 0.6, 0.1, 0.12, 12, 6000, 0.05, 2024, True)\n    ]\n\n    results = []\n    for case in test_cases:\n        result = perform_bootstrap_test(case)\n        results.append(str(result).capitalize())\n\n    # Print the final output in the required format\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}