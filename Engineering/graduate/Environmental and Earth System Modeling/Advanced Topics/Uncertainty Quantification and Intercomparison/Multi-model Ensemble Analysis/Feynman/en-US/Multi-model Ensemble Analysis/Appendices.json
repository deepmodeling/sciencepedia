{
    "hands_on_practices": [
        {
            "introduction": "In multi-model ensemble analysis, a fundamental task is to combine predictions from various models to produce a single, superior forecast. A simple linear combination, or weighted average, is a powerful and interpretable approach. This exercise guides you through the foundational derivation of optimal weights by minimizing the mean squared error, revealing the classic principle of inverse-variance weighting . Mastering this derivation provides a crucial theoretical baseline for understanding more complex weighting schemes and their underlying assumptions.",
            "id": "3897949",
            "problem": "Consider a Multi-Model Ensemble (MME) forecast for seasonal basin-mean precipitation anomaly in an environmental and earth system modeling context. Let the unknown true anomaly be a scalar quantity denoted by $y$. Suppose $K$ independent numerical models produce scalar predictors $X_{1}, X_{2}, \\dots, X_{K}$ for the same target $y$. Each predictor is unbiased for $y$, meaning $\\mathbb{E}[X_{i}] = y$, and has finite variance $\\operatorname{Var}(X_{i}-y) = \\sigma_{i}^{2}$. Define a linear ensemble estimator $\\hat{y} = \\sum_{i=1}^{K} w_{i} X_{i}$ subject to the normalization constraint $\\sum_{i=1}^{K} w_{i} = 1$ to preserve unbiasedness of the ensemble estimate under the unbiasedness of the individual predictors.\n\nStarting from the definition of mean squared error, derive the weights $w_{i}$ that minimize the expected squared error $\\mathbb{E}\\left[(\\hat{y} - y)^{2}\\right]$ under the assumptions stated above. Your derivation must use first principles: the definition of mean squared error, the independence of model errors, and the normalization constraint. Clearly justify the convexity of the optimization problem and the uniqueness of the solution.\n\nThen, apply your derived formula to the following scientifically realistic hindcast-based variances for $K=4$ independent models (expressed in $\\text{mm}^{2}$ for seasonal anomalies):\n$$\\sigma_{1}^{2} = 1,\\quad \\sigma_{2}^{2} = 4,\\quad \\sigma_{3}^{2} = 9,\\quad \\sigma_{4}^{2} = 16.$$\nCompute the normalized weights $w_{1}, w_{2}, w_{3}, w_{4}$ for the ensemble estimator. Express the weights as dimensionless numbers, and round each to four significant figures.\n\nFinally, explain the conditions under which the derived weights are optimal in the sense of minimizing mean squared error, and articulate how the solution would generalize if the model errors were not independent but had a known, positive-definite covariance matrix.\n\nThe final answer must be the computed weight vector for the given numerical variances.",
            "solution": "We start from the definition of the ensemble estimator $\\hat{y} = \\sum_{i=1}^{K} w_{i} X_{i}$ with the normalization constraint $\\sum_{i=1}^{K} w_{i} = 1$. Because each predictor is unbiased, $\\mathbb{E}[X_{i}] = y$, the ensemble estimator is unbiased if $\\sum_{i=1}^{K} w_{i} = 1$, since\n$$\n\\mathbb{E}[\\hat{y}] = \\mathbb{E}\\left[\\sum_{i=1}^{K} w_{i} X_{i}\\right] = \\sum_{i=1}^{K} w_{i} \\mathbb{E}[X_{i}] = \\sum_{i=1}^{K} w_{i} y = y.\n$$\nDefine the individual errors as $e_{i} = X_{i} - y$. By assumption, $\\mathbb{E}[e_{i}] = 0$ and $\\operatorname{Var}(e_{i}) = \\sigma_{i}^{2}$, and the errors $\\{e_{i}\\}$ are mutually independent. The ensemble error is\n$$\n\\hat{y} - y = \\sum_{i=1}^{K} w_{i} X_{i} - y = \\sum_{i=1}^{K} w_{i} (y + e_{i}) - y = \\sum_{i=1}^{K} w_{i} e_{i}.\n$$\nThe mean squared error of $\\hat{y}$ is\n$$\n\\mathbb{E}\\left[(\\hat{y} - y)^{2}\\right] = \\mathbb{E}\\left[\\left(\\sum_{i=1}^{K} w_{i} e_{i}\\right)^{2}\\right].\n$$\nBecause $\\mathbb{E}[e_{i}] = 0$, $\\mathbb{E}\\left[(\\hat{y} - y)^{2}\\right]$ equals the variance of the ensemble error:\n$$\n\\operatorname{Var}\\left(\\sum_{i=1}^{K} w_{i} e_{i}\\right) = \\sum_{i=1}^{K} w_{i}^{2} \\operatorname{Var}(e_{i}) + 2 \\sum_{1 \\leq i < j \\leq K} w_{i} w_{j} \\operatorname{Cov}(e_{i}, e_{j}).\n$$\nUnder independence, $\\operatorname{Cov}(e_{i}, e_{j}) = 0$ for $i \\neq j$, so\n$$\n\\mathbb{E}\\left[(\\hat{y} - y)^{2}\\right] = \\sum_{i=1}^{K} w_{i}^{2} \\sigma_{i}^{2}.\n$$\nThe optimization problem is thus\n$$\n\\min_{w_{1},\\dots,w_{K}} \\sum_{i=1}^{K} w_{i}^{2} \\sigma_{i}^{2} \\quad \\text{subject to} \\quad \\sum_{i=1}^{K} w_{i} = 1.\n$$\nThis is a convex quadratic program, since the objective is a quadratic form with Hessian matrix $\\mathbf{H} = 2 \\operatorname{diag}(\\sigma_{1}^{2}, \\dots, \\sigma_{K}^{2})$, which is positive definite because each $\\sigma_{i}^{2} > 0$. Therefore, any stationary point subject to the linear equality constraint is the unique global minimizer.\n\nWe solve using the method of Lagrange multipliers. Define the Lagrangian\n$$\n\\mathcal{L}(w_{1}, \\dots, w_{K}, \\lambda) = \\sum_{i=1}^{K} w_{i}^{2} \\sigma_{i}^{2} - \\lambda\\left(\\sum_{i=1}^{K} w_{i} - 1\\right).\n$$\nSet partial derivatives to zero:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial w_{i}} = 2 w_{i} \\sigma_{i}^{2} - \\lambda = 0 \\quad \\Rightarrow \\quad w_{i} = \\frac{\\lambda}{2 \\sigma_{i}^{2}} \\quad \\text{for each } i,\n$$\nand\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\lambda} = -\\left(\\sum_{i=1}^{K} w_{i} - 1\\right) = 0 \\quad \\Rightarrow \\quad \\sum_{i=1}^{K} w_{i} = 1.\n$$\nSubstitute $w_{i}$ into the constraint:\n$$\n\\sum_{i=1}^{K} \\frac{\\lambda}{2 \\sigma_{i}^{2}} = 1 \\quad \\Rightarrow \\quad \\frac{\\lambda}{2} \\sum_{i=1}^{K} \\frac{1}{\\sigma_{i}^{2}} = 1 \\quad \\Rightarrow \\quad \\lambda = \\frac{2}{\\sum_{j=1}^{K} \\sigma_{j}^{-2}}.\n$$\nTherefore,\n$$\nw_{i} = \\frac{\\lambda}{2 \\sigma_{i}^{2}} = \\frac{\\sigma_{i}^{-2}}{\\sum_{j=1}^{K} \\sigma_{j}^{-2}}.\n$$\nThus the optimal weights are proportional to the inverse variances and normalized to sum to one:\n$$\nw_{i} \\propto \\sigma_{i}^{-2}, \\quad \\text{with} \\quad w_{i} = \\frac{\\sigma_{i}^{-2}}{\\sum_{j=1}^{K} \\sigma_{j}^{-2}}.\n$$\nUniqueness follows from the strict convexity noted earlier.\n\nApply to the given numerical variances $\\sigma_{1}^{2} = 1$, $\\sigma_{2}^{2} = 4$, $\\sigma_{3}^{2} = 9$, $\\sigma_{4}^{2} = 16$. Compute inverse variances:\n$$\n\\sigma_{1}^{-2} = 1,\\quad \\sigma_{2}^{-2} = \\frac{1}{4},\\quad \\sigma_{3}^{-2} = \\frac{1}{9},\\quad \\sigma_{4}^{-2} = \\frac{1}{16}.\n$$\nSum of inverse variances:\n$$\nS = 1 + \\frac{1}{4} + \\frac{1}{9} + \\frac{1}{16} = \\frac{144 + 36 + 16 + 9}{144} = \\frac{205}{144}.\n$$\nNormalized weights:\n$$\nw_{1} = \\frac{1}{S} = \\frac{144}{205},\\quad\nw_{2} = \\frac{\\frac{1}{4}}{S} = \\frac{36}{205},\\quad\nw_{3} = \\frac{\\frac{1}{9}}{S} = \\frac{16}{205},\\quad\nw_{4} = \\frac{\\frac{1}{16}}{S} = \\frac{9}{205}.\n$$\nConvert to decimal form and round each to four significant figures:\n$$\nw_{1} \\approx 0.702439024 \\to 0.7024,\\quad\nw_{2} \\approx 0.175609756 \\to 0.1756,\\quad\nw_{3} \\approx 0.078048780 \\to 0.07805,\\quad\nw_{4} \\approx 0.043902439 \\to 0.04390.\n$$\n\nConditions for optimality: The inverse-variance normalized weights minimize $\\mathbb{E}\\left[(\\hat{y} - y)^{2}\\right]$ under the following conditions:\n- The individual predictors are unbiased, $\\mathbb{E}[X_{i}] = y$, so the ensemble can be unbiased with $\\sum_{i} w_{i} = 1$.\n- The errors $e_{i} = X_{i} - y$ are mutually independent, so the ensemble variance reduces to $\\sum_{i} w_{i}^{2} \\sigma_{i}^{2}$ without cross terms.\n- The variances $\\sigma_{i}^{2}$ are correctly specified (or consistently estimable), and are positive.\n- The objective is mean squared error; no alternative loss or constraints are imposed.\n\nIf the errors are correlated with a known positive-definite covariance matrix $\\boldsymbol{\\Sigma} = \\operatorname{Cov}(\\mathbf{e})$, where $\\mathbf{e} = (e_{1},\\dots,e_{K})^{\\top}$, then the ensemble MSE becomes\n$$\n\\mathbb{E}\\left[(\\hat{y} - y)^{2}\\right] = \\operatorname{Var}\\left(\\sum_{i=1}^{K} w_{i} e_{i}\\right) = \\mathbf{w}^{\\top} \\boldsymbol{\\Sigma} \\mathbf{w},\n$$\nwhere $\\mathbf{w} = (w_{1},\\dots,w_{K})^{\\top}$ and the unbiasedness constraint is $\\mathbf{1}^{\\top} \\mathbf{w} = 1$ with $\\mathbf{1}$ the vector of ones. The unique minimizer is obtained by solving the constrained quadratic program via Lagrange multipliers, yielding the generalized inverse-covariance weights\n$$\n\\mathbf{w}^{\\star} = \\frac{\\boldsymbol{\\Sigma}^{-1} \\mathbf{1}}{\\mathbf{1}^{\\top} \\boldsymbol{\\Sigma}^{-1} \\mathbf{1}},\n$$\nwhich reduces to the inverse-variance formula derived above when $\\boldsymbol{\\Sigma}$ is diagonal (independent errors). These weights are the Best Linear Unbiased Estimator (BLUE) under the Gauss–Markov assumptions; normality is not required for MSE optimality, only finite second moments and the stated covariance structure.",
            "answer": "$$\\boxed{\\begin{pmatrix}0.7024 & 0.1756 & 0.07805 & 0.04390\\end{pmatrix}}$$"
        },
        {
            "introduction": "The inverse-variance weights derived in the ideal case require perfect knowledge of each model's true error variance, a luxury we rarely have. In reality, we must estimate model performance from finite, noisy data. This practice explores the critical statistical pitfall of overfitting that can arise when using these estimated performance metrics directly, leading to unstable weights that perform poorly out-of-sample . Through this exercise, you will diagnose the source of this instability and evaluate regularization strategies, such as shrinkage, that are essential for building robust and reliable ensemble weights.",
            "id": "3897954",
            "problem": "Consider a multi-model ensemble in environmental and earth system modeling with $K$ deterministic models $\\{M_1,\\dots,M_K\\}$. For a validation set of $n$ independent targets $\\{y_i\\}_{i=1}^n$, each model $M_m$ produces predictions $\\{\\hat{y}_i^{(m)}\\}_{i=1}^n$ and errors $e_i^{(m)} = y_i - \\hat{y}_i^{(m)}$. Assume the errors within each model are independent and identically distributed as Gaussian with zero mean and variance $\\sigma_m^2$, and are independent across models. Define the Root Mean Squared Error (RMSE) for model $m$ computed by $n$-fold cross-validation (CV) on the validation set as $\\widehat{\\mathrm{RMSE}}_m = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n (e_i^{(m)})^2}$, and let $s_m^2 = \\widehat{\\mathrm{RMSE}}_m^2$ denote the mean squared error estimate. Consider performance-based ensemble weights $w_m$ that are inversely proportional to $s_m^2$, normalized to the simplex: $\\tilde{w}_m = \\frac{1/s_m^2}{\\sum_{j=1}^K 1/s_j^2}$ with $\\tilde{w}_m \\ge 0$ and $\\sum_{m=1}^K \\tilde{w}_m = 1$.\n\nStarting from the definitions above and standard distributional results for Gaussian errors, derive expressions for $\\mathbb{E}[1/s_m^2]$ and $\\mathrm{Var}(1/s_m^2)$ as functions of $n$ and $\\sigma_m^2$, and use these to explain why small $n$ can lead to overfitting when weights are based on inverse CV RMSE. In particular, consider the case $K=3$ with true variances $\\sigma_1^2 = 1$, $\\sigma_2^2 = 1.44$, and $\\sigma_3^2 = 2.25$, and $n=10$. Quantify the coefficient of variation of $1/s_1^2$, and argue (using distributional reasoning for ratios of chi-square random variables) that the probability that model $2$ receives a larger weight than model $1$ purely due to sampling variability is non-negligible.\n\nWhich option provides a correct explanation of the overfitting mechanism for small $n$, together with a mathematically defensible regularization strategy that stabilizes the weights, including a derivation of its variance-reducing effect?\n\nA. Replace $n$-fold cross-validation with leave-one-out cross-validation and keep $w_m \\propto 1/\\widehat{\\mathrm{RMSE}}_m^2$. Because leave-one-out uses the largest possible training sets, it eliminates the bias in $1/s_m^2$ and thus removes overfitting when $n$ is small.\n\nB. Solve a constrained optimization with an $L_1$ penalty on weights: minimize $\\sum_{m=1}^K w_m s_m^2 + \\lambda \\sum_{m=1}^K |w_m|$ subject to $w_m \\ge 0$ and $\\sum_{m=1}^K w_m = 1$. The $L_1$ penalty induces sparsity in $w_m$ and therefore reduces variance and overfitting for small $n$.\n\nC. Use convex shrinkage toward equal weights: define $\\tilde{w}_m = \\frac{1/s_m^2}{\\sum_{j=1}^K 1/s_j^2}$ and $w_m(\\lambda) = (1-\\lambda)\\frac{1}{K} + \\lambda \\tilde{w}_m$ for $0 \\le \\lambda \\le 1$, choosing $\\lambda$ by nested cross-validation. Show from $\\mathbb{E}[1/s_m^2]$ and $\\mathrm{Var}(1/s_m^2)$ that small $n$ inflates the variance of $\\tilde{w}_m$, and derive that $\\mathrm{Var}(w_m(\\lambda)) = \\lambda^2 \\mathrm{Var}(\\tilde{w}_m)$, which stabilizes weights when $\\lambda<1$.\n\nD. Add Tikhonov regularization to RMSE by defining $w_m^{(\\alpha)} \\propto \\frac{1}{s_m^2 + \\alpha}$ with $\\alpha$ set to $\\alpha = 2\\sigma_m^4/n$ for each model, which exactly offsets the sampling variance of $s_m^2$, making $w_m^{(\\alpha)}$ unbiased and variance-free even for small $n$.",
            "solution": "The user has requested a critical validation of the problem statement, followed by a full derivation and evaluation of the provided options.\n\n**Problem Validation**\n\n**Step 1: Extract Givens**\n- Ensemble of $K$ deterministic models: $\\{M_1, \\dots, M_K\\}$.\n- Validation set of $n$ independent targets: $\\{y_i\\}_{i=1}^n$.\n- Model $M_m$ predictions: $\\{\\hat{y}_i^{(m)}\\}_{i=1}^n$.\n- Errors: $e_i^{(m)} = y_i - \\hat{y}_i^{(m)}$.\n- Error distribution: For a fixed model $m$, errors $e_i^{(m)}$ are independent and identically distributed (i.i.d.) as Gaussian with zero mean and variance $\\sigma_m^2$, i.e., $e_i^{(m)} \\sim N(0, \\sigma_m^2)$.\n- Error independence: Errors are independent across models.\n- Root Mean Squared Error (RMSE) estimate: $\\widehat{\\mathrm{RMSE}}_m = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n (e_i^{(m)})^2}$.\n- Mean Squared Error (MSE) estimate: $s_m^2 = \\widehat{\\mathrm{RMSE}}_m^2 = \\frac{1}{n}\\sum_{i=1}^n (e_i^{(m)})^2$.\n- Ensemble weights definition: $\\tilde{w}_m = \\frac{1/s_m^2}{\\sum_{j=1}^K 1/s_j^2}$, with $\\tilde{w}_m \\ge 0$ and $\\sum_{m=1}^K \\tilde{w}_m = 1$.\n- Specific case for analysis: $K=3$, $\\sigma_1^2 = 1$, $\\sigma_2^2 = 1.44$, $\\sigma_3^2 = 2.25$, and $n=10$.\n- The task is to derive $\\mathbb{E}[1/s_m^2]$ and $\\mathrm{Var}(1/s_m^2)$, explain the overfitting phenomenon for small $n$, quantify the instability for the given case, and then evaluate the options.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem statement is scientifically grounded, well-posed, and objective. It is rooted in standard statistical theory applied to the common problem of ensemble weighting in modeling. All terms are defined mathematically. The assumptions (i.i.d. Gaussian errors, independence across models) are standard simplifying assumptions in this context.\n\nThere is a minor terminological ambiguity: \"computed by $n$-fold cross-validation (CV) on the validation set\". Typically, one computes an error metric on a validation set directly, without performing a further CV procedure *on* it. $n$-fold CV (or more accurately, leave-one-out CV) is a procedure for generating out-of-sample errors from a single dataset. However, the ambiguity is resolved by the explicit formula provided: $s_m^2 = \\frac{1}{n}\\sum_{i=1}^n (e_i^{(m)})^2$. This is the standard definition of the Mean Squared Error on a set of $n$ error terms. We will proceed by using this explicit mathematical definition, interpreting the phrase \"on the validation set\" as specifying that the $e_i^{(m)}$ are the errors on this set.\n\nThe problem is computationally and theoretically tractable and does not violate any scientific or logical principles.\n\n**Step 3: Verdict and Action**\nThe problem is **valid**. We proceed to the solution.\n\n**Derivation and Analysis**\n\n**1. Distribution of $s_m^2$ and moments of $1/s_m^2$**\nGiven that the errors $e_i^{(m)}$ for model $m$ are i.i.d. $N(0, \\sigma_m^2)$, the standardized errors $z_i^{(m)} = e_i^{(m)} / \\sigma_m$ are i.i.d. $N(0, 1)$. The squares of these, $(z_i^{(m)})^2$, are i.i.d. chi-squared random variables with one degree of freedom, $\\chi^2(1)$.\n\nThe sum of $n$ such independent variables follows a chi-squared distribution with $n$ degrees of freedom:\n$$ \\sum_{i=1}^n \\left(\\frac{e_i^{(m)}}{\\sigma_m}\\right)^2 \\sim \\chi^2(n) $$\nThe estimated mean squared error is $s_m^2 = \\frac{1}{n} \\sum_{i=1}^n (e_i^{(m)})^2$. We can rewrite the sum of squares as $n s_m^2$. Substituting this into the previous expression gives:\n$$ \\frac{n s_m^2}{\\sigma_m^2} \\sim \\chi^2(n) $$\nLet the random variable $X = \\frac{n s_m^2}{\\sigma_m^2}$. We are interested in the moments of $1/s_m^2$. From the relationship above, $s_m^2 = \\frac{\\sigma_m^2}{n} X$, which implies $1/s_m^2 = \\frac{n}{\\sigma_m^2} X^{-1}$.\n\nThe random variable $X$ follows a $\\chi^2(n)$ distribution. The moments of its inverse, $X^{-1}$, are related to the moments of an inverse-gamma distribution. The $k$-th moment of $X^{-1}$ is $\\mathbb{E}[X^{-k}] = \\frac{\\Gamma(n/2 - k)}{\\Gamma(n/2) 2^k}$, provided $n/2 > k$.\n\nFor the expectation of $1/s_m^2$, we need $\\mathbb{E}[X^{-1}]$. Using $k=1$:\n$$ \\mathbb{E}[X^{-1}] = \\frac{\\Gamma(n/2 - 1)}{\\Gamma(n/2) \\cdot 2} = \\frac{\\Gamma(n/2 - 1)}{(n/2-1)\\Gamma(n/2 - 1) \\cdot 2} = \\frac{1}{n-2} \\quad (\\text{for } n>2) $$\nTherefore, the expectation of $1/s_m^2$ is:\n$$ \\mathbb{E}\\left[\\frac{1}{s_m^2}\\right] = \\frac{n}{\\sigma_m^2} \\mathbb{E}[X^{-1}] = \\frac{n}{\\sigma_m^2} \\frac{1}{n-2} = \\frac{n}{n-2} \\frac{1}{\\sigma_m^2} $$\n\nFor the variance of $1/s_m^2$, we first need the second moment, $\\mathbb{E}[(1/s_m^2)^2] = (\\frac{n}{\\sigma_m^2})^2 \\mathbb{E}[X^{-2}]$. Using $k=2$:\n$$ \\mathbb{E}[X^{-2}] = \\frac{\\Gamma(n/2 - 2)}{\\Gamma(n/2) \\cdot 2^2} = \\frac{\\Gamma(n/2 - 2)}{(n/2-1)(n/2-2)\\Gamma(n/2-2) \\cdot 4} = \\frac{1}{(n-2)(n-4)} \\quad (\\text{for } n>4) $$\nSo, the second moment is:\n$$ \\mathbb{E}\\left[\\left(\\frac{1}{s_m^2}\\right)^2\\right] = \\left(\\frac{n}{\\sigma_m^2}\\right)^2 \\frac{1}{(n-2)(n-4)} = \\frac{n^2}{(n-2)(n-4)} \\frac{1}{\\sigma_m^4} $$\nThe variance is then $\\mathrm{Var}(Y) = \\mathbb{E}[Y^2] - (\\mathbb{E}[Y])^2$:\n$$ \\mathrm{Var}\\left(\\frac{1}{s_m^2}\\right) = \\frac{n^2}{(n-2)(n-4)\\sigma_m^4} - \\left(\\frac{n}{(n-2)\\sigma_m^2}\\right)^2 = \\frac{n^2}{\\sigma_m^4} \\left( \\frac{1}{(n-2)(n-4)} - \\frac{1}{(n-2)^2} \\right) $$\n$$ \\mathrm{Var}\\left(\\frac{1}{s_m^2}\\right) = \\frac{n^2}{\\sigma_m^4(n-2)^2} \\left( \\frac{n-2}{n-4} - 1 \\right) = \\frac{n^2}{\\sigma_m^4(n-2)^2} \\left( \\frac{(n-2)-(n-4)}{n-4} \\right) = \\frac{2n^2}{(n-2)^2(n-4)\\sigma_m^4} $$\nThis expression is valid for $n > 4$.\n\n**2. Overfitting for small $n$**\nThe derived moments reveal two key issues for small $n$:\n- **Bias:** $\\mathbb{E}[1/s_m^2] = \\frac{n}{n-2} \\frac{1}{\\sigma_m^2}$. The factor $\\frac{n}{n-2}$ is greater than $1$, meaning $1/s_m^2$ is a biased estimator of the true inverse variance $1/\\sigma_m^2$, and this bias is severe for small $n$ (it is infinite for $n=2$).\n- **Variance:** $\\mathrm{Var}(1/s_m^2)$ has a factor of $(n-4)$ in the denominator, causing the variance to be extremely large for small $n$ (approaching infinity as $n \\to 4^+$).\n\nHigh variance means that the estimate $1/s_m^2$ is very noisy and can deviate substantially from its expected value. Since the weights $\\tilde{w}_m$ are proportional to $1/s_m^2$, a model that is truly poor (large $\\sigma_m^2$) can, by chance on a small validation set, produce a small $s_m^2$, thus receiving an unduly large weight. This is a form of overfitting: the weights are tailored to the random noise in the small validation set, not the true underlying model skill.\n\n**3. Quantifying Instability for the Specific Case**\nGiven $K=3$, $n=10$, $\\sigma_1^2=1$, $\\sigma_2^2=1.44$.\nWe calculate the coefficient of variation (CV) for $1/s_1^2$: $\\mathrm{CV} = \\frac{\\sqrt{\\mathrm{Var}(1/s_1^2)}}{\\mathbb{E}[1/s_1^2]}$.\n$$ \\mathbb{E}[1/s_1^2] = \\frac{10}{10-2} \\frac{1}{1} = \\frac{10}{8} = 1.25 $$\n$$ \\mathrm{Var}(1/s_1^2) = \\frac{2(10^2)}{(10-2)^2(10-4)} \\frac{1}{1^2} = \\frac{200}{8^2 \\cdot 6} = \\frac{200}{384} = \\frac{25}{48} $$\n$$ \\mathrm{CV}(1/s_1^2) = \\frac{\\sqrt{25/48}}{1.25} = \\frac{5/\\sqrt{48}}{5/4} = \\frac{4}{\\sqrt{48}} = \\frac{4}{4\\sqrt{3}} = \\frac{1}{\\sqrt{3}} \\approx 0.577 $$\nA CV of approximately $57.7\\%$ is extremely high, indicating massive sampling variability.\n\nNext, we argue that the probability of the worse model $2$ getting a larger weight than model $1$ is non-negligible. This corresponds to the event $\\tilde{w}_2 > \\tilde{w}_1$, which is equivalent to $1/s_2^2 > 1/s_1^2$, or $s_1^2 > s_2^2$.\nWe know $\\frac{n s_1^2}{\\sigma_1^2} \\sim \\chi^2(n)$ and $\\frac{n s_2^2}{\\sigma_2^2} \\sim \\chi^2(n)$ and they are independent. Let $X_1$ and $X_2$ be independent $\\chi^2(n)$ random variables. Then $s_1^2 = \\frac{\\sigma_1^2}{n} X_1$ and $s_2^2 = \\frac{\\sigma_2^2}{n} X_2$.\nThe condition $s_1^2 > s_2^2$ becomes $\\frac{\\sigma_1^2}{n} X_1 > \\frac{\\sigma_2^2}{n} X_2$, which simplifies to $\\frac{X_1}{X_2} > \\frac{\\sigma_2^2}{\\sigma_1^2}$.\nThe ratio of two independent $\\chi^2$ variables each divided by their degrees of freedom $n$ follows an F-distribution with $(n, n)$ degrees of freedom. So, $\\frac{X_1/n}{X_2/n} = \\frac{X_1}{X_2} \\sim F(n, n)$.\nWe need to calculate $P(F(10, 10) > \\frac{1.44}{1}) = P(F(10, 10) > 1.44)$.\nThe median of an $F(10, 10)$ distribution is $1$. The mean is $n/(n-2) = 1.25$. Since $1.44$ is not far from the mean, we expect a substantial probability. Standard statistical tables or software show that the 75th percentile of $F(10, 10)$ is approximately $1.53$. Since $1.44 < 1.53$, the probability $P(F(10, 10) > 1.44)$ is greater than $0.25$. A precise calculation yields approximately $0.26$. A probability of $26\\%$ is certainly non-negligible.\n\n**Option-by-Option Analysis**\n\n**A. Replace $n$-fold cross-validation with leave-one-out cross-validation and keep $w_m \\propto 1/\\widehat{\\mathrm{RMSE}}_m^2$. Because leave-one-out uses the largest possible training sets, it eliminates the bias in $1/s_m^2$ and thus removes overfitting when $n$ is small.**\nLeave-one-out cross-validation (LOOCV) is known to provide a nearly unbiased estimate of the true prediction error, but the estimate itself often has high variance. The fundamental problem here is the high variance of the performance estimate (and its inverse) on a small sample, which leads to unstable weights. LOOCV does not solve this high-variance problem. In fact, due to the high correlation between the $n$ training sets, the variance of the LOOCV error estimate can be higher than that from $k$-fold CV with smaller $k$. The claim that it \"eliminates the bias in $1/s_m^2$\" is incorrect; the bias arises from the nonlinear inverse function, which is not removed. The claim that it \"removes overfitting\" is a false overstatement.\n**Verdict: Incorrect.**\n\n**B. Solve a constrained optimization with an $L_1$ penalty on weights: minimize $\\sum_{m=1}^K w_m s_m^2 + \\lambda \\sum_{m=1}^K |w_m|$ subject to $w_m \\ge 0$ and $\\sum_{m=1}^K w_m = 1$. The $L_1$ penalty induces sparsity in $w_m$ and therefore reduces variance and overfitting for small $n$.**\nThis formulation is fundamentally flawed. The constraints $w_m \\ge 0$ and $\\sum_{m=1}^K w_m = 1$ imply that $\\sum_{m=1}^K |w_m| = \\sum_{m=1}^K w_m = 1$. Therefore, the penalty term $\\lambda \\sum |w_m|$ is just a constant $\\lambda$. Adding a constant to an objective function does not change the solution. The problem reduces to minimizing $\\sum_{m=1}^K w_m s_m^2$ on the simplex. The solution to this linear program is to place all weight ($w_m=1$) on the model with the minimum $s_m^2$ and zero on all others. This \"winner-take-all\" approach is the most extreme form of overfitting to the validation set and exhibits maximum variance, the exact opposite of the desired stabilization.\n**Verdict: Incorrect.**\n\n**C. Use convex shrinkage toward equal weights: define $\\tilde{w}_m = \\frac{1/s_m^2}{\\sum_{j=1}^K 1/s_j^2}$ and $w_m(\\lambda) = (1-\\lambda)\\frac{1}{K} + \\lambda \\tilde{w}_m$ for $0 \\le \\lambda \\le 1$, choosing $\\lambda$ by nested cross-validation. Show from $\\mathbb{E}[1/s_m^2]$ and $\\mathrm{Var}(1/s_m^2)$ that small $n$ inflates the variance of $\\tilde{w}_m$, and derive that $\\mathrm{Var}(w_m(\\lambda)) = \\lambda^2 \\mathrm{Var}(\\tilde{w}_m)$, which stabilizes weights when $\\lambda<1$.**\nThis option correctly identifies the issue: small $n$ inflates the variance of the data-driven weights $\\tilde{w}_m$. The proposed solution is a standard and effective regularization technique: shrinking the high-variance estimate $\\tilde{w}_m$ towards a stable, low-variance target (equal weights, $1/K$). The parameter $\\lambda$ controls the amount of shrinkage. The derivation of the variance of the shrunken estimator is correct. For a constant $\\lambda$, $\\mathrm{Var}(w_m(\\lambda)) = \\mathrm{Var}((1-\\lambda)\\frac{1}{K} + \\lambda \\tilde{w}_m)$. Since $(1-\\lambda)\\frac{1}{K}$ is a constant, this simplifies to $\\mathrm{Var}(\\lambda \\tilde{w}_m) = \\lambda^2 \\mathrm{Var}(\\tilde{w}_m)$. As $0 \\le \\lambda < 1$, the factor $\\lambda^2$ is less than $1$, which demonstrates a guaranteed reduction in the variance of the weights. This addresses the core problem of instability. Choosing $\\lambda$ via nested cross-validation is the appropriate procedure for finding a good bias-variance trade-off.\n**Verdict: Correct.**\n\n**D. Add Tikhonov regularization to RMSE by defining $w_m^{(\\alpha)} \\propto \\frac{1}{s_m^2 + \\alpha}$ with $\\alpha$ set to $\\alpha = 2\\sigma_m^4/n$ for each model, which exactly offsets the sampling variance of $s_m^2$, making $w_m^{(\\alpha)}$ unbiased and variance-free even for small $n$.**\nThis option proposes a form of regularization (damped inverse), which is a valid idea. However, its claims are incorrect and exaggerated. First, the proposed value for $\\alpha$ depends on the true variance $\\sigma_m^2$, which is unknown in practice. Second, while $\\alpha = \\frac{2\\sigma_m^4}{n}$ is indeed equal to $\\mathrm{Var}(s_m^2)$, adding it to the denominator does not make the resulting estimator for $1/\\sigma_m^2$ unbiased. Due to the nonlinear transformation, $\\mathbb{E}[1/(s_m^2 + \\alpha)] \\neq 1/(\\mathbb{E}[s_m^2] + \\alpha)$, and it is not generally equal to $1/\\sigma_m^2$. Third, the claim that the resulting weight is \"variance-free\" is patently false. The weight $w_m^{(\\alpha)}$ is still a function of the random variable $s_m^2$, so it must have non-zero variance. The regularization reduces variance but does not eliminate it.\n**Verdict: Incorrect.**",
            "answer": "$$\\boxed{C}$$"
        },
        {
            "introduction": "After developing and regularizing ensemble weighting schemes, the final step is to rigorously verify their performance. How can we prove that a new, complex weighting method is statistically superior to a simpler baseline, especially when dealing with autocorrelated time series data common in earth systems? This hands-on coding practice tasks you with implementing a paired block bootstrap test, a powerful computational method for hypothesis testing in forecast verification . By translating the statistical theory into a working program, you will gain a practical tool for the critical evaluation of ensemble forecasting systems.",
            "id": "3897964",
            "problem": "You are given two competing ensemble weighting schemes in environmental and earth system modeling and asked to assess whether one significantly outperforms the other according to the Continuous Ranked Probability Score (CRPS). By definition, for a probabilistic forecast with cumulative distribution function $F(z)$ and a realized observation $y$, the Continuous Ranked Probability Score (CRPS) is the integral $$\\mathrm{CRPS}(F,y) = \\int_{-\\infty}^{\\infty} \\left(F(z) - \\mathbb{1}\\{z \\ge y\\}\\right)^2 \\, \\mathrm{d}z,$$ which has the same physical units as the forecasted quantity. Lower values of $\\mathrm{CRPS}$ indicate better probabilistic forecasts. In this problem, the CRPS is already computed for each day and scheme; you will not compute CRPS from $F$ and $y$.\n\nYou must construct a paired block bootstrap hypothesis test to assess whether ensemble weighting scheme $A$ significantly outperforms scheme $B$ over a specified period, accounting for temporal autocorrelation. Let there be a sequence of $N$ days indexed by $t \\in \\{1,2,\\dots,N\\}$. Let $C^{A}_t$ and $C^{B}_t$ denote the daily CRPS values for scheme $A$ and scheme $B$ respectively. Define the paired daily improvement $$d_t = C^{B}_t - C^{A}_t,$$ which is positive when scheme $A$ has lower CRPS than scheme $B$ on day $t$. The inferential target is the mean improvement $$\\mu_d = \\mathbb{E}[d_t].$$ You are to test the one-sided hypothesis $$H_0: \\mu_d \\le 0 \\quad \\text{versus} \\quad H_1: \\mu_d > 0,$$ where rejecting the null hypothesis supports that scheme $A$ significantly outperforms scheme $B$.\n\nTo maintain scientific realism, use the block bootstrap to handle autocorrelation in $\\{d_t\\}$. Partition the time series into non-overlapping contiguous blocks of length $L$ days; the final block may be shorter if $L$ does not divide $N$. Let the sample mean be $$\\bar{d} = \\frac{1}{N} \\sum_{t=1}^N d_t.$$ Create a mean-zero series under the null hypothesis by centering the differences: $$r_t = d_t - \\bar{d}.$$ For each bootstrap replicate $i \\in \\{1,2,\\dots,R\\}$, sample blocks with replacement from the set of original blocks, concatenate the sampled blocks of $\\{r_t\\}$ until you reach at least $N$ elements, truncate to $N$ elements, and compute the replicate mean $\\bar{r}^{*(i)}$. An approximate one-sided p-value for $H_0: \\mu_d \\le 0$ is then $$p = \\frac{1}{R} \\sum_{i=1}^R \\mathbb{1}\\left\\{\\bar{r}^{*(i)} \\ge \\bar{d}\\right\\}.$$ Decide that scheme $A$ significantly outperforms scheme $B$ if $p < \\alpha$, for a chosen significance level $\\alpha \\in (0,1)$.\n\nYou must implement this test in a complete, runnable program. The program must generate synthetic but scientifically plausible daily CRPS sequences for each test case, using autoregressive processes to emulate temporal autocorrelation. Specifically, for each test case:\n- Generate a baseline autoregressive process $\\{X_t\\}$ of order $1$ (AR(1)) with parameter $\\phi \\in (-1, 1)$ and Gaussian innovations with standard deviation $\\sigma$, namely $$X_t = \\phi X_{t-1} + \\epsilon_t, \\quad \\epsilon_t \\sim \\mathcal{N}(0, \\sigma_t^2),$$ with $X_0 = 0$ and either constant $\\sigma_t = \\sigma$ or a specified piecewise-constant heteroskedastic sequence.\n- Generate scheme-specific errors $\\{E^A_t\\}$ and $\\{E^B_t\\}$ as independent AR(1) processes with fixed parameter $\\phi_s = 0.4$ and Gaussian innovations with fixed standard deviation $\\sigma_s = 0.08$ unless otherwise stated.\n- Construct daily CRPS values as $$C^{B}_t = \\beta + X_t + E^B_t, \\qquad C^{A}_t = \\beta + X_t + E^A_t - \\Delta,$$ with $\\beta = 1.0$ ensuring non-negativity, and then enforce non-negativity by replacing negative values with $0$ if any arise due to noise. The quantity $\\Delta$ encodes the expected improvement of scheme $A$ over scheme $B$; positive $\\Delta$ means $A$ tends to have lower CRPS than $B$.\n\nYou must then apply the paired block bootstrap test described above to compute the p-value $p$ and return a boolean decision for each test case indicating whether $p < \\alpha$.\n\nAngle units are not applicable. Physical units for CRPS are the units of the forecasted quantity, but the final outputs are boolean and hence unitless.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3]\").\n\nUse the following test suite, each specified by $(N, \\phi, \\sigma, \\Delta, L, R, \\alpha, \\text{seed}, \\text{heteroskedastic})$, where heteroskedastic indicates whether $\\sigma_t$ changes over time as specified:\n- Case $1$ (general \"happy path\"): $(N = 180, \\phi = 0.6, \\sigma = 0.2, \\Delta = 0.15, L = 10, R = 5000, \\alpha = 0.05, \\text{seed} = 123, \\text{heteroskedastic} = \\text{False})$.\n- Case $2$ (boundary case, independent days): $(N = 180, \\phi = 0.0, \\sigma = 0.25, \\Delta = 0.0, L = 1, R = 3000, \\alpha = 0.05, \\text{seed} = 456, \\text{heteroskedastic} = \\text{False})$.\n- Case $3$ (non-divisible block length, small sample): $(N = 23, \\phi = 0.7, \\sigma = 0.3, \\Delta = 0.10, L = 10, R = 5000, \\alpha = 0.05, \\text{seed} = 321, \\text{heteroskedastic} = \\text{False})$.\n- Case $4$ (opposite effect, scheme $A$ worse): $(N = 120, \\phi = 0.5, \\sigma = 0.25, \\Delta = -0.10, L = 7, R = 4000, \\alpha = 0.05, \\text{seed} = 789, \\text{heteroskedastic} = \\text{False})$.\n- Case $5$ (heteroskedastic autocorrelation): $(N = 240, \\phi = 0.6, \\sigma = 0.1 \\text{ for } t \\le N/2 \\text{ and } \\sigma = 0.3 \\text{ for } t > N/2, \\Delta = 0.12, L = 12, R = 6000, \\alpha = 0.05, \\text{seed} = 2024, \\text{heteroskedastic} = \\text{True})$.\n\nFor each case, generate $\\{C^{A}_t\\}$ and $\\{C^{B}_t\\}$ according to the rules above using the specified parameters, compute the paired differences $\\{d_t\\}$, perform the paired block bootstrap test, and return a boolean indicating whether scheme $A$ significantly outperforms scheme $B$ at level $\\alpha$.\n\nYour program should produce a single line of output containing the list of five booleans corresponding to the five cases, ordered from Case $1$ to Case $5$, as a comma-separated list enclosed in square brackets, for example \"[True,False,True,False,True]\".",
            "solution": "The problem statement has been analyzed and is deemed valid. It is scientifically grounded, well-posed, objective, and contains all necessary information to construct a unique and verifiable solution. The methodology described—a paired block bootstrap hypothesis test applied to simulated time series data—is a standard and appropriate technique in statistical climatology and environmental science for comparing model or forecast performances while accounting for temporal autocorrelation. All parameters for data generation and the statistical test are explicitly provided.\n\nThe objective is to implement a paired block bootstrap hypothesis test to determine if a forecast ensemble weighting scheme, denoted as scheme $A$, provides a statistically significant improvement over a competing scheme, $B$. The performance metric is the Continuous Ranked Probability Score (CRPS), where lower values indicate better performance. The improvement of scheme $A$ over scheme $B$ on day $t$ is measured by the difference $d_t = C^B_t - C^A_t$, where $C^A_t$ and $C^B_t$ are the respective CRPS values. A positive value for $d_t$ signifies that scheme $A$ performed better on that day.\n\nThe core inferential task is to test the one-sided hypothesis regarding the true mean improvement, $\\mu_d = \\mathbb{E}[d_t]$:\n$$H_0: \\mu_d \\le 0 \\quad \\text{(Scheme A is not better than Scheme B)}$$\n$$H_1: \\mu_d > 0 \\quad \\text{(Scheme A is significantly better than Scheme B)}$$\n\nThe solution proceeds in three principal stages: synthetic data generation, implementation of the block bootstrap test, and the final decision based on the computed p-value.\n\n**1. Synthetic Data Generation**\n\nTo create a realistic testbed, we first generate synthetic daily CRPS time series, $\\{C^A_t\\}$ and $\\{C^B_t\\}$, for $N$ days. This process emulates the characteristics of real-world environmental data, particularly temporal autocorrelation.\n\nThe daily CRPS values are constructed as follows:\n$$C^{B}_t = \\beta + X_t + E^B_t$$\n$$C^{A}_t = \\beta + X_t + E^A_t - \\Delta$$\n\nHere, $\\beta = 1.0$ is a baseline CRPS value, ensuring the scores are generally positive. $\\{X_t\\}$ represents a common underlying environmental process affecting both schemes. It is modeled as a first-order autoregressive (AR(1)) process:\n$$X_t = \\phi X_{t-1} + \\epsilon_t, \\quad \\text{with } X_0 = 0$$\nwhere $\\phi$ is the autocorrelation parameter and $\\epsilon_t$ are Gaussian innovations, $\\epsilon_t \\sim \\mathcal{N}(0, \\sigma_t^2)$. The standard deviation $\\sigma_t$ can be constant or vary over time (heteroskedasticity).\n\n$\\{E^A_t\\}$ and $\\{E^B_t\\}$ are independent, scheme-specific error terms, also modeled as AR(1) processes with fixed parameters $\\phi_s = 0.4$ and innovation standard deviation $\\sigma_s = 0.08$.\n\nThe parameter $\\Delta$ represents the systematic improvement of scheme $A$ over scheme $B$. If $\\Delta > 0$, scheme $A$ is designed to have a lower CRPS on average. The expected value of the daily difference is $\\mathbb{E}[d_t] = \\mathbb{E}[C^B_t - C^A_t] = \\mathbb{E}[(\\beta + X_t + E^B_t) - (\\beta + X_t + E^A_t - \\Delta)] = \\Delta$. Thus, testing for $\\mu_d > 0$ is equivalent to testing for $\\Delta > 0$.\n\nFinally, since CRPS values cannot be negative, any generated value less than $0$ is set to $0$.\n\n**2. Paired Block Bootstrap Test**\n\nThe block bootstrap is employed because the daily difference series, $d_t = C^B_t - C^A_t = (E^B_t - E^A_t) + \\Delta$, inherits the temporal autocorrelation from the AR(1) error terms. This method preserves the dependence structure by resampling blocks of consecutive observations rather than individual data points.\n\nThe procedure is as follows:\n- **Compute the observed statistic**: Calculate the sample mean of the daily differences, $\\bar{d} = \\frac{1}{N} \\sum_{t=1}^N d_t$. This is our best estimate of $\\mu_d$.\n- **Center the data**: To simulate the null hypothesis ($H_0: \\mu_d = 0$), we create a recentered series $r_t = d_t - \\bar{d}$. This series has a sample mean of exactly $0$ and preserves the autocorrelation structure of the original $\\{d_t\\}$ series.\n- **Form blocks**: The recentered series $\\{r_t\\}$ is partitioned into non-overlapping blocks of length $L$. If $N$ is not perfectly divisible by $L$, the final block will be shorter.\n- **Bootstrap resampling**: For each of $R$ bootstrap replicates:\n    1. A new time series of length at least $N$ is created by sampling blocks with replacement from the set of original blocks. The number of blocks to draw is $\\lceil N/L \\rceil$.\n    2. The resulting concatenated series is truncated to the original length $N$.\n    3. The mean of this bootstrap replicate series, $\\bar{r}^{*(i)}$, is calculated. This process generates an empirical sampling distribution of the mean under the null hypothesis.\n\n**3. P-value Calculation and Decision**\n\nThe one-sided p-value is the proportion of bootstrap means, $\\bar{r}^{*(i)}$, that are as extreme or more extreme than the observed sample mean, $\\bar{d}$. For our alternative hypothesis $H_1: \\mu_d > 0$, this is:\n$$p = \\frac{1}{R} \\sum_{i=1}^R \\mathbb{1}\\left\\{\\bar{r}^{*(i)} \\ge \\bar{d}\\right\\}$$\nwhere $\\mathbb{1}\\{\\cdot\\}$ is the indicator function. This p-value represents the probability of observing a mean improvement of $\\bar{d}$ or greater, given that the true improvement was actually zero.\n\nThe final decision is made by comparing the p-value to a pre-defined significance level, $\\alpha$. If $p < \\alpha$, we reject the null hypothesis $H_0$ and conclude that scheme $A$ provides a statistically significant improvement over scheme $B$. Otherwise, we fail to reject $H_0$. This entire procedure will be implemented for each test case using the specified parameters.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef generate_ar1(n_samples: int, phi: float, sigma_vec: np.ndarray, rng: np.random.Generator) -> np.ndarray:\n    \"\"\"\n    Generates a first-order autoregressive (AR(1)) time series.\n    X_t = phi * X_{t-1} + innovation_t\n    \"\"\"\n    if n_samples == 0:\n        return np.array([])\n    \n    innovations = rng.normal(0, sigma_vec, n_samples)\n    x = np.zeros(n_samples)\n    # Per problem statement X_0 = 0, so the first term is just the first innovation\n    x[0] = innovations[0]\n    for t in range(1, n_samples):\n        x[t] = phi * x[t-1] + innovations[t]\n    return x\n\ndef perform_bootstrap_test(case_params):\n    \"\"\"\n    Generates synthetic data and performs the paired block bootstrap test for a single case.\n    \"\"\"\n    N, phi, sigma, Delta, L, R, alpha, seed, heteroskedastic = case_params\n\n    rng = np.random.default_rng(seed)\n\n    # 1. Generate synthetic CRPS data\n    # Define baseline AR(1) process standard deviation\n    if heteroskedastic:\n        # Case 5 specifics: sigma=0.1 for first half, 0.3 for second half\n        sigma_t = np.full(N, sigma) \n        sigma_t[N//2:] = 0.3\n    else:\n        sigma_t = np.full(N, sigma)\n\n    X_t = generate_ar1(N, phi, sigma_t, rng)\n\n    # Define scheme-specific AR(1) errors\n    phi_s = 0.4\n    sigma_s = 0.08\n    sigma_s_vec = np.full(N, sigma_s)\n    E_A_t = generate_ar1(N, phi_s, sigma_s_vec, rng)\n    E_B_t = generate_ar1(N, phi_s, sigma_s_vec, rng)\n\n    # Construct CRPS values\n    beta = 1.0\n    C_B_t = beta + X_t + E_B_t\n    C_A_t = beta + X_t + E_A_t - Delta\n    \n    # Enforce non-negativity of CRPS\n    C_A_t = np.maximum(0, C_A_t)\n    C_B_t = np.maximum(0, C_B_t)\n\n    # 2. Implement the Paired Block Bootstrap Test\n    d_t = C_B_t - C_A_t\n    \n    if N == 0:\n        return False\n\n    d_bar = np.mean(d_t)\n    \n    # Center the series to simulate the null hypothesis (mean = 0)\n    r_t = d_t - d_bar\n\n    # Partition the centered series into non-overlapping blocks\n    # np.array_split handles non-divisible lengths correctly\n    split_indices = np.arange(L, N, L)\n    blocks = np.array_split(r_t, split_indices)\n\n    # Perform bootstrap resampling\n    bootstrap_means = np.zeros(R)\n    num_blocks_to_sample = int(np.ceil(N / L))\n    \n    for i in range(R):\n        # Sample block indices with replacement\n        block_indices = rng.choice(len(blocks), size=num_blocks_to_sample, replace=True)\n        \n        # Concatenate sampled blocks and truncate to length N\n        bootstrap_series = np.concatenate([blocks[j] for j in block_indices])[:N]\n        \n        # Compute mean of the bootstrap replicate\n        bootstrap_means[i] = np.mean(bootstrap_series)\n\n    # 3. Calculate p-value and determine significance\n    # p = proportion of bootstrap means >= observed mean\n    p_value = np.mean(bootstrap_means >= d_bar)\n    \n    return p_value < alpha\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        # (N, phi, sigma, Delta, L, R, alpha, seed, heteroskedastic)\n        (180, 0.6, 0.2, 0.15, 10, 5000, 0.05, 123, False),\n        (180, 0.0, 0.25, 0.0, 1, 3000, 0.05, 456, False),\n        (23, 0.7, 0.3, 0.10, 10, 5000, 0.05, 321, False),\n        (120, 0.5, 0.25, -0.10, 7, 4000, 0.05, 789, False),\n        (240, 0.6, 0.1, 0.12, 12, 6000, 0.05, 2024, True)\n    ]\n\n    results = []\n    for case in test_cases:\n        result = perform_bootstrap_test(case)\n        results.append(str(result))\n\n    # Print the final output in the required format\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}