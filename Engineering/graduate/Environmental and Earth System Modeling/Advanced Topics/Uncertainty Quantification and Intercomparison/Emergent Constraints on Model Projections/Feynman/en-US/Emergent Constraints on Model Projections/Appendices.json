{
    "hands_on_practices": [
        {
            "introduction": "At the heart of an emergent constraint lies a process of Bayesian inference, where information from observations is used to update knowledge derived from a model ensemble. This exercise isolates the foundational statistical step: using a noisy observation of a present-day predictor, $x_{\\mathrm{obs}}$, to refine our estimate of its true but unobserved value, $X^{\\ast}$. By working through this errors-in-variables model from first principles, you will derive the posterior mean of the predictor, which is the essential first step in constraining a future projection, $Y$ .",
            "id": "3878124",
            "problem": "Consider an emergent-constraint problem in environmental and earth system modeling. A set of Earth System Models (ESMs) reveals a physically motivated linear relationship between a future projection $Y$ (global-mean surface air temperature change for a late-century period relative to a historical baseline) and a present-day predictor $X^{\\ast}$ (a dimensionless metric of interannual variability amplitude). The relationship is modeled as a linear regression across the ESM ensemble with additive Gaussian scatter,\n$$\nY \\mid X^{\\ast} \\sim \\mathcal{N}\\!\\left(a + b\\,X^{\\ast},\\,\\sigma^{2}\\right),\n$$\nwhere $a$ and $b$ are regression parameters estimated from the ensemble and $\\sigma^{2}$ quantifies model-to-model scatter about the line due to structural differences and internal variability. The observational estimate of the predictor is subject to errors-in-variables: the measured value $x_{\\mathrm{obs}}$ deviates from the latent $X^{\\ast}$ due to measurement error and sampling variability. The observational data model is\n$$\nx_{\\mathrm{obs}} \\mid X^{\\ast} \\sim \\mathcal{N}\\!\\left(X^{\\ast},\\,s_{x}^{2}\\right).\n$$\nTo close the Bayesian hierarchy, the latent predictor $X^{\\ast}$ is assigned a prior based on the ensemble distribution of the predictor,\n$$\nX^{\\ast} \\sim \\mathcal{N}\\!\\left(\\mu_{x},\\,\\tau_{x}^{2}\\right).\n$$\nAssume the following scientifically plausible parameter values, obtained from the ESM ensemble and observational characterization:\n- $a = 1.0$ (kelvin),\n- $b = 1.5$ (kelvin per unit of the dimensionless predictor),\n- $\\sigma = 0.2$ (kelvin),\n- $\\mu_{x} = 0.1$ (dimensionless),\n- $\\tau_{x}^{2} = 0.36$,\n- $s_{x}^{2} = 0.04$,\n- $x_{\\mathrm{obs}} = 0.8$ (dimensionless).\n\nStarting from Bayesâ€™ theorem and properties of the normal distribution, derive an analytic expression for the posterior mean $\\mathbb{E}\\!\\left[Y \\mid x_{\\mathrm{obs}}\\right]$ under this hierarchical errors-in-variables regression model, making all conditioning and marginalization steps explicit. Then evaluate the expression numerically using the given parameter values. Round your final numerical answer to four significant figures and express it in kelvin.",
            "solution": "The objective is to derive an analytic expression for the posterior mean of the future projection $Y$ given the observational data $x_{\\mathrm{obs}}$, denoted as $\\mathbb{E}\\!\\left[Y \\mid x_{\\mathrm{obs}}\\right]$, and then to evaluate this expression numerically. The derivation begins from the fundamental principles of probability theory, specifically the law of total expectation and Bayes' theorem.\n\nThe quantity of interest is the posterior expectation $\\mathbb{E}\\!\\left[Y \\mid x_{\\mathrm{obs}}\\right]$. We can express this by marginalizing over the latent (unobserved) predictor $X^{\\ast}$. Using the law of total expectation, we write:\n$$\n\\mathbb{E}\\!\\left[Y \\mid x_{\\mathrm{obs}}\\right] = \\mathbb{E}_{X^{\\ast} \\mid x_{\\mathrm{obs}}}\\!\\left[\\mathbb{E}\\!\\left[Y \\mid X^{\\ast}, x_{\\mathrm{obs}}\\right]\\right]\n$$\nThe inner expectation is the expected value of $Y$ conditioned on both the latent variable $X^{\\ast}$ and the observation $x_{\\mathrm{obs}}$. From the problem statement, the data-generating process for $Y$ is given by $Y \\mid X^{\\ast} \\sim \\mathcal{N}\\!\\left(a + b\\,X^{\\ast},\\,\\sigma^{2}\\right)$. This expression indicates that the distribution of $Y$ depends only on $X^{\\ast}$. The variables $Y$ and $x_{\\mathrm{obs}}$ are conditionally independent given $X^{\\ast}$. Consequently, conditioning on $x_{\\mathrm{obs}}$ provides no additional information about $Y$ once $X^{\\ast}$ is known. Therefore:\n$$\n\\mathbb{E}\\!\\left[Y \\mid X^{\\ast}, x_{\\mathrm{obs}}\\right] = \\mathbb{E}\\!\\left[Y \\mid X^{\\ast}\\right] = a + b\\,X^{\\ast}\n$$\nSubstituting this result back into the law of total expectation yields:\n$$\n\\mathbb{E}\\!\\left[Y \\mid x_{\\mathrm{obs}}\\right] = \\mathbb{E}_{X^{\\ast} \\mid x_{\\mathrm{obs}}}\\!\\left[a + b\\,X^{\\ast}\\right]\n$$\nBy the linearity of expectation, this simplifies to:\n$$\n\\mathbb{E}\\!\\left[Y \\mid x_{\\mathrm{obs}}\\right] = a + b\\,\\mathbb{E}\\!\\left[X^{\\ast} \\mid x_{\\mathrm{obs}}\\right]\n$$\nThe problem is thus reduced to finding the posterior mean of the latent predictor $X^{\\ast}$ given the observation $x_{\\mathrm{obs}}$. This requires determining the posterior probability distribution $p(X^{\\ast} \\mid x_{\\mathrm{obs}})$.\n\nWe apply Bayes' theorem to find the posterior distribution of $X^{\\ast}$:\n$$\np(X^{\\ast} \\mid x_{\\mathrm{obs}}) = \\frac{p(x_{\\mathrm{obs}} \\mid X^{\\ast}) \\, p(X^{\\ast})}{p(x_{\\mathrm{obs}})}\n$$\nThe term $p(x_{\\mathrm{obs}} \\mid X^{\\ast})$ is the likelihood of the observation given the latent variable, and $p(X^{\\ast})$ is the prior distribution of the latent variable. The denominator $p(x_{\\mathrm{obs}})$ is the marginal likelihood, which serves as a normalization constant. The posterior distribution is therefore proportional to the product of the likelihood and the prior:\n$$\np(X^{\\ast} \\mid x_{\\mathrm{obs}}) \\propto p(x_{\\mathrm{obs}} \\mid X^{\\ast}) \\, p(X^{\\ast})\n$$\nThe problem provides the functional forms for the likelihood and the prior:\n\\begin{itemize}\n    \\item Likelihood: $x_{\\mathrm{obs}} \\mid X^{\\ast} \\sim \\mathcal{N}\\!\\left(X^{\\ast},\\,s_{x}^{2}\\right)$, so $p(x_{\\mathrm{obs}} \\mid X^{\\ast}) \\propto \\exp\\left(-\\frac{1}{2s_{x}^{2}}(x_{\\mathrm{obs}} - X^{\\ast})^2\\right)$\n    \\item Prior: $X^{\\ast} \\sim \\mathcal{N}\\!\\left(\\mu_{x},\\,\\tau_{x}^{2}\\right)$, so $p(X^{\\ast}) \\propto \\exp\\left(-\\frac{1}{2\\tau_{x}^{2}}(X^{\\ast} - \\mu_x)^2\\right)$\n\\end{itemize}\nThe posterior is proportional to the product of these two Gaussian functions. The product of two Gaussian probability density functions is proportional to another Gaussian PDF. We can find its parameters by analyzing the exponent of the product:\n$$\n\\ln p(X^{\\ast} \\mid x_{\\mathrm{obs}}) \\propto -\\frac{1}{2s_{x}^{2}}(X^{\\ast} - x_{\\mathrm{obs}})^2 - \\frac{1}{2\\tau_{x}^{2}}(X^{\\ast} - \\mu_{x})^2\n$$\nExpanding the quadratic terms in $X^{\\ast}$:\n$$\n\\ln p(X^{\\ast} \\mid x_{\\mathrm{obs}}) \\propto -\\frac{1}{2} \\left[ \\frac{(X^{\\ast})^2 - 2X^{\\ast}x_{\\mathrm{obs}} + x_{\\mathrm{obs}}^2}{s_{x}^{2}} + \\frac{(X^{\\ast})^2 - 2X^{\\ast}\\mu_x + \\mu_x^2}{\\tau_{x}^{2}} \\right]\n$$\nCollecting terms in powers of $X^{\\ast}$:\n$$\n\\ln p(X^{\\ast} \\mid x_{\\mathrm{obs}}) \\propto -\\frac{1}{2} \\left[ (X^{\\ast})^2 \\left(\\frac{1}{s_{x}^{2}} + \\frac{1}{\\tau_{x}^{2}}\\right) - 2X^{\\ast} \\left(\\frac{x_{\\mathrm{obs}}}{s_{x}^{2}} + \\frac{\\mu_x}{\\tau_{x}^{2}}\\right) \\right] + C\n$$\nwhere $C$ contains terms not involving $X^{\\ast}$. This expression is a quadratic in $X^{\\ast}$, confirming the posterior is Gaussian, say $X^{\\ast} \\mid x_{\\mathrm{obs}} \\sim \\mathcal{N}(\\mu_{\\text{post}}, \\sigma_{\\text{post}}^2)$. The exponent of its PDF is $-\\frac{1}{2\\sigma_{\\text{post}}^2}(X^{\\ast} - \\mu_{\\text{post}})^2 \\propto -\\frac{1}{2}\\left[\\frac{(X^{\\ast})^2}{\\sigma_{\\text{post}}^2} - \\frac{2X^{\\ast}\\mu_{\\text{post}}}{\\sigma_{\\text{post}}^2}\\right]$.\nBy comparing the coefficients of the $(X^{\\ast})^2$ term, we identify the inverse posterior variance (precision):\n$$\n\\frac{1}{\\sigma_{\\text{post}}^2} = \\frac{1}{s_{x}^{2}} + \\frac{1}{\\tau_{x}^{2}} = \\frac{s_{x}^{2} + \\tau_{x}^{2}}{s_{x}^{2}\\tau_{x}^{2}}\n$$\nBy comparing the coefficients of the $X^{\\ast}$ term, we find the posterior mean $\\mu_{\\text{post}} = \\mathbb{E}\\!\\left[X^{\\ast} \\mid x_{\\mathrm{obs}}\\right]$:\n$$\n\\frac{\\mu_{\\text{post}}}{\\sigma_{\\text{post}}^2} = \\frac{x_{\\mathrm{obs}}}{s_{x}^{2}} + \\frac{\\mu_x}{\\tau_{x}^{2}}\n$$\n$$\n\\mu_{\\text{post}} = \\sigma_{\\text{post}}^2 \\left(\\frac{x_{\\mathrm{obs}}}{s_{x}^{2}} + \\frac{\\mu_x}{\\tau_{x}^{2}}\\right) = \\left(\\frac{s_{x}^{2}\\tau_{x}^{2}}{s_{x}^{2} + \\tau_{x}^{2}}\\right) \\left(\\frac{x_{\\mathrm{obs}}\\tau_{x}^{2} + \\mu_x s_{x}^{2}}{s_{x}^{2}\\tau_{x}^{2}}\\right) = \\frac{x_{\\mathrm{obs}}\\tau_{x}^{2} + \\mu_x s_{x}^{2}}{s_{x}^{2} + \\tau_{x}^{2}}\n$$\nThis posterior mean is a precision-weighted average of the observation $x_{\\mathrm{obs}}$ and the prior mean $\\mu_x$.\n\nThe final analytic expression for the posterior mean of $Y$ is therefore:\n$$\n\\mathbb{E}\\!\\left[Y \\mid x_{\\mathrm{obs}}\\right] = a + b\\,\\left(\\frac{\\tau_{x}^{2}}{s_{x}^{2} + \\tau_{x}^{2}}x_{\\mathrm{obs}} + \\frac{s_{x}^{2}}{s_{x}^{2} + \\tau_{x}^{2}}\\mu_x\\right)\n$$\nWe now evaluate this expression using the provided parameter values: $a = 1.0$, $b = 1.5$, $\\sigma = 0.2$ (implying $\\sigma^2 = 0.04$), $\\mu_{x} = 0.1$, $\\tau_{x}^{2} = 0.36$, $s_{x}^{2} = 0.04$, and $x_{\\mathrm{obs}} = 0.8$. Note that $\\sigma^2$ is not required for the posterior mean calculation.\n\nFirst, we calculate the posterior mean of $X^{\\ast}$:\n$$\n\\mathbb{E}\\!\\left[X^{\\ast} \\mid x_{\\mathrm{obs}}\\right] = \\frac{0.36}{0.04 + 0.36} (0.8) + \\frac{0.04}{0.04 + 0.36} (0.1)\n$$\n$$\n\\mathbb{E}\\!\\left[X^{\\ast} \\mid x_{\\mathrm{obs}}\\right] = \\frac{0.36}{0.40} (0.8) + \\frac{0.04}{0.40} (0.1)\n$$\n$$\n\\mathbb{E}\\!\\left[X^{\\ast} \\mid x_{\\mathrm{obs}}\\right] = (0.9)(0.8) + (0.1)(0.1) = 0.72 + 0.01 = 0.73\n$$\nNext, we substitute this value into the expression for $\\mathbb{E}\\!\\left[Y \\mid x_{\\mathrm{obs}}\\right]$:\n$$\n\\mathbb{E}\\!\\left[Y \\mid x_{\\mathrm{obs}}\\right] = 1.0 + 1.5 \\times 0.73\n$$\n$$\n\\mathbb{E}\\!\\left[Y \\mid x_{\\mathrm{obs}}\\right] = 1.0 + 1.095 = 2.095\n$$\nThe problem requires the final numerical answer to be rounded to four significant figures. The calculated value of $2.095$ already has exactly four significant figures. The units are kelvin.",
            "answer": "$$\\boxed{2.095}$$"
        },
        {
            "introduction": "Building on the core Bayesian update, this exercise guides you through the complete workflow of constructing a linear emergent constraint. Starting with paired data from a model ensemble, you will first estimate the linear relationship using ordinary least squares (OLS) regression. You will then derive the full predictive distribution for the future quantity, $Y^\\star$, by correctly propagating all relevant sources of uncertainty, including the uncertainty in the observation itself . This practice introduces the \"reduction factor,\" a crucial metric used to quantify the effectiveness of a constraint by measuring the fractional decrease in projection uncertainty.",
            "id": "4047342",
            "problem": "You are given a conceptual setup for an emergent constraint in climate modeling: across multiple Global Climate Models (GCMs), a present-day observable $X$ (dimensionless, e.g., an albedo-related metric) and a future-response variable $Y$ (e.g., snow-albedo feedback strength) are paired as $(X_i, Y_i)$, for $i = 1, \\dots, n$. The emergent constraint posits that systematic inter-model differences in $X$ explain inter-model differences in $Y$ through a physically motivated and statistically testable relationship. Assume a linear model across models, $Y_i = a + b\\,X_i + \\varepsilon_i$, where $\\varepsilon_i$ is Gaussian noise with zero mean and common variance, and $a$ and $b$ are unknown coefficients. You observe the real-world present-day quantity with measurement uncertainty, $X^\\star \\sim \\mathcal{N}(x_{\\mathrm{obs}}, s_x^2)$, where $x_{\\mathrm{obs}}$ is the observed value and $s_x$ is the standard deviation of its uncertainty. The task is to quantify the constrained projection uncertainty of the real-world future quantity $Y^\\star$ using the emergent constraint implied by the model ensemble and the observation.\n\nStarting from first principles in statistical inference (Bayes' theorem for Gaussian likelihoods and noninformative priors), ordinary least squares (OLS) estimation as the maximum likelihood estimator for linear-Gaussian models, and the law of total expectation and the law of total variance, derive and implement a method to:\n- Estimate the linear relationship between $X$ and $Y$ across models.\n- Form the posterior predictive distribution for $Y^\\star$ conditioned on the uncertain real-world $X^\\star \\sim \\mathcal{N}(x_{\\mathrm{obs}}, s_x^2)$.\n- Quantify the constrained projection uncertainty through the predictive mean and predictive standard deviation of $Y^\\star$.\n- Quantify the unconstrained projection uncertainty through the sample standard deviation of $Y_i$ across models, and report the reduction factor defined as the ratio of constrained predictive standard deviation to unconstrained ensemble standard deviation.\n\nAll outputs involving $Y$ must be expressed in $\\mathrm{W\\,m^{-2}\\,K^{-1}}$ and be floating-point numbers. The reduction factor must be expressed as a floating-point decimal (unitless). Angles do not appear in this problem.\n\nImplement a complete Python program that computes, for each test case, the constrained predictive mean of $Y^\\star$, the constrained predictive standard deviation of $Y^\\star$, and the reduction factor (constrained standard deviation divided by the unconstrained ensemble standard deviation). The program should not read any input and must run as-is.\n\nUse the following test suite of parameter values covering a range of scenarios, including a high-correlation case, a weak-correlation case, a boundary case with zero measurement uncertainty, and a large measurement uncertainty case. In each case, $X$ is dimensionless and $Y$ is in $\\mathrm{W\\,m^{-2}\\,K^{-1}}$:\n\n- Test case $1$ (high correlation, small measurement uncertainty):\n  - $X = [\\,0.2,\\,0.4,\\,0.6,\\,0.8,\\,1.0,\\,0.3,\\,0.7,\\,0.9\\,]$\n  - $Y = [\\,0.98,\\,1.25,\\,1.73,\\,2.08,\\,2.51,\\,1.10,\\,1.86,\\,2.32\\,]$ in $\\mathrm{W\\,m^{-2}\\,K^{-1}}$\n  - $x_{\\mathrm{obs}} = 0.65$\n  - $s_x = 0.05$\n\n- Test case $2$ (weak correlation, comparable noise):\n  - $X = [\\,0.1,\\,0.2,\\,0.5,\\,0.6,\\,0.8,\\,0.3,\\,0.4,\\,0.7,\\,0.9,\\,0.55\\,]$\n  - $Y = [\\,0.71,\\,1.27,\\,0.95,\\,1.26,\\,0.93,\\,1.08,\\,0.99,\\,1.17,\\,0.89,\\,1.205\\,]$ in $\\mathrm{W\\,m^{-2}\\,K^{-1}}$\n  - $x_{\\mathrm{obs}} = 0.50$\n  - $s_x = 0.05$\n\n- Test case $3$ (boundary: zero measurement uncertainty and observation at the ensemble mean):\n  - $X = [\\,0.2,\\,0.3,\\,0.4,\\,0.5,\\,0.6\\,]$\n  - $Y = [\\,0.0,\\,0.05,\\,0.22,\\,0.27,\\,0.41\\,]$ in $\\mathrm{W\\,m^{-2}\\,K^{-1}}$\n  - $x_{\\mathrm{obs}} = 0.40$\n  - $s_x = 0.00$\n\n- Test case $4$ (moderate correlation, large measurement uncertainty):\n  - $X = [\\,0.2,\\,0.5,\\,0.7,\\,1.0,\\,0.3,\\,0.9\\,]$\n  - $Y = [\\,0.70,\\,0.85,\\,1.40,\\,1.65,\\,0.65,\\,1.65\\,]$ in $\\mathrm{W\\,m^{-2}\\,K^{-1}}$\n  - $x_{\\mathrm{obs}} = 0.60$\n  - $s_x = 0.20$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case result is itself a three-element list of floats in the order $[\\,\\text{predictive mean of }Y^\\star,\\,\\text{predictive standard deviation of }Y^\\star,\\,\\text{reduction factor}\\,]$, all formatted to six decimal places. For example, in the required format: $[[\\mu_1,\\sigma_1,r_1],[\\mu_2,\\sigma_2,r_2],[\\mu_3,\\sigma_3,r_3],[\\mu_4,\\sigma_4,r_4]]$.",
            "solution": "The problem requires the derivation and implementation of a method to calculate the constrained projection uncertainty for a future climate variable, $Y^\\star$, using an emergent constraint derived from an ensemble of model simulations. The constraint is a linear relationship between a present-day observable, $X$, and the future variable, $Y$. The real-world observation of the present-day quantity, $X^\\star$, is uncertain and described by a Gaussian distribution.\n\nThe derivation proceeds in three main stages: first, estimating the linear relationship from the model ensemble using Ordinary Least Squares (OLS); second, deriving the predictive mean and variance for $Y^\\star$ by propagating the uncertainty from the observation of $X^\\star$ through the linear model; and third, quantifying the reduction in uncertainty.\n\n**1. Estimation of the Linear Relationship via Ordinary Least Squares (OLS)**\n\nWe are given $n$ pairs of data points $(X_i, Y_i)$ from an ensemble of climate models. The assumed linear model is:\n$$\nY_i = a + b\\,X_i + \\varepsilon_i\n$$\nwhere $a$ and $b$ are the intercept and slope coefficients, respectively, and $\\varepsilon_i$ are independent and identically distributed random errors from a Gaussian distribution with zero mean and variance $\\sigma^2$, i.e., $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$.\n\nThe OLS method finds the coefficient estimates, $\\hat{a}$ and $\\hat{b}$, that minimize the sum of squared residuals. These estimates are also the maximum likelihood estimates under the Gaussian error assumption. They are given by:\n$$\n\\hat{b} = \\frac{\\sum_{i=1}^n (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum_{i=1}^n (X_i - \\bar{X})^2} = \\frac{S_{XY}}{S_{XX}}\n$$\n$$\n\\hat{a} = \\bar{Y} - \\hat{b}\\,\\bar{X}\n$$\nwhere $\\bar{X} = \\frac{1}{n}\\sum_{i=1}^n X_i$ and $\\bar{Y} = \\frac{1}{n}\\sum_{i=1}^n Y_i$ are the sample means.\n\nThe variance of the residuals, $\\sigma^2$, which represents the scatter of the models around the regression line (a form of model structural uncertainty), is estimated by its unbiased estimator, $\\hat{\\sigma}^2$:\n$$\n\\hat{\\sigma}^2 = \\frac{1}{n-2} \\sum_{i=1}^n (Y_i - (\\hat{a} + \\hat{b}\\,X_i))^2\n$$\nThe denominator is $n-2$ because two degrees of freedom are used to estimate the two parameters, $\\hat{a}$ and $\\hat{b}$.\n\n**2. Derivation of the Constrained Predictive Distribution**\n\nWe are tasked with predicting the real-world future quantity, $Y^\\star$, based on an uncertain observation of the real-world present-day quantity, $X^\\star$. The observation is given as a distribution: $X^\\star \\sim \\mathcal{N}(x_{\\mathrm{obs}}, s_x^2)$. The quantity $Y^\\star$ is related to $X^\\star$ through the model $Y^\\star = \\hat{a} + \\hat{b}\\,X^\\star$, but this prediction itself is uncertain.\n\nThe total uncertainty in the prediction of $Y^\\star$ stems from three sources: (1) the intrinsic 'structural' uncertainty of the model relationship, captured by $\\hat{\\sigma}^2$; (2) the uncertainty in the estimated regression parameters $\\hat{a}$ and $\\hat{b}$; and (3) the uncertainty in the predictor variable $X^\\star$, captured by $s_x^2$.\n\nTo find the mean and variance of the posterior predictive distribution for $Y^\\star$, we use the law of total expectation and the law of total variance.\n\n**Predictive Mean of $Y^\\star$**\nThe law of total expectation states $\\mathbb{E}[Y^\\star] = \\mathbb{E}_{X^\\star}[\\mathbb{E}[Y^\\star | X^\\star]]$.\nThe inner expectation is the predicted value of $Y^\\star$ for a given value of $X^\\star$:\n$$\n\\mathbb{E}[Y^\\star | X^\\star = x] = \\hat{a} + \\hat{b}\\,x\n$$\nTaking the outer expectation over the distribution of $X^\\star$:\n$$\n\\mu_{Y^\\star} = \\mathbb{E}[Y^\\star] = \\mathbb{E}_{X^\\star}[\\hat{a} + \\hat{b}\\,X^\\star] = \\hat{a} + \\hat{b}\\,\\mathbb{E}[X^\\star]\n$$\nSince $\\mathbb{E}[X^\\star] = x_{\\mathrm{obs}}$, the predictive mean is:\n$$\n\\mu_{Y^\\star} = \\hat{a} + \\hat{b}\\,x_{\\mathrm{obs}}\n$$\n\n**Predictive Variance of $Y^\\star$**\nThe law of total variance states $\\mathrm{Var}[Y^\\star] = \\mathbb{E}_{X^\\star}[\\mathrm{Var}[Y^\\star | X^\\star]] + \\mathrm{Var}_{X^\\star}[\\mathbb{E}[Y^\\star | X^\\star]]$.\n\nThe first term, $\\mathbb{E}_{X^\\star}[\\mathrm{Var}[Y^\\star | X^\\star]]$, represents the average prediction uncertainty. The variance of a prediction for a *single new observation* at a *known* point $x$ is given by:\n$$\n\\mathrm{Var}[Y^\\star | X^\\star=x] = \\hat{\\sigma}^2 \\left( 1 + \\frac{1}{n} + \\frac{(x - \\bar{X})^2}{S_{XX}} \\right)\n$$\nThis expression accounts for both the residual variance ($\\hat{\\sigma}^2$) and the uncertainty in the estimated regression line itself (the terms with $1/n$ and $(x - \\bar{X})^2/S_{XX}$). We now average this over the distribution of $X^\\star$:\n$$\n\\mathbb{E}_{X^\\star}[\\mathrm{Var}[Y^\\star | X^\\star]] = \\mathbb{E}_{X^\\star} \\left[ \\hat{\\sigma}^2 \\left( 1 + \\frac{1}{n} + \\frac{(X^\\star - \\bar{X})^2}{S_{XX}} \\right) \\right]\n$$\n$$\n= \\hat{\\sigma}^2 \\left( 1 + \\frac{1}{n} \\right) + \\frac{\\hat{\\sigma}^2}{S_{XX}} \\mathbb{E}_{X^\\star}[(X^\\star - \\bar{X})^2]\n$$\nThe expectation $\\mathbb{E}[(X^\\star - \\bar{X})^2]$ can be expanded as $\\mathbb{E}[(X^\\star - x_{\\mathrm{obs}} + x_{\\mathrm{obs}} - \\bar{X})^2] = \\mathbb{E}[(X^\\star-x_{\\mathrm{obs}})^2] + (x_{\\mathrm{obs}}-\\bar{X})^2 = s_x^2 + (x_{\\mathrm{obs}}-\\bar{X})^2$.\nThus, the first term of the total variance is:\n$$\n\\mathbb{E}_{X^\\star}[\\mathrm{Var}[Y^\\star | X^\\star]] = \\hat{\\sigma}^2 \\left( 1 + \\frac{1}{n} + \\frac{s_x^2 + (x_{\\mathrm{obs}} - \\bar{X})^2}{S_{XX}} \\right)\n$$\n\nThe second term, $\\mathrm{Var}_{X^\\star}[\\mathbb{E}[Y^\\star | X^\\star]]$, represents the uncertainty in the mean prediction due to the uncertainty in the predictor $X^\\star$.\n$$\n\\mathrm{Var}_{X^\\star}[\\mathbb{E}[Y^\\star | X^\\star]] = \\mathrm{Var}_{X^\\star}[\\hat{a} + \\hat{b}\\,X^\\star] = \\hat{b}^2\\,\\mathrm{Var}[X^\\star]\n$$\nSince $\\mathrm{Var}[X^\\star] = s_x^2$, this term is:\n$$\n\\mathrm{Var}_{X^\\star}[\\mathbb{E}[Y^\\star | X^\\star]] = \\hat{b}^2 s_x^2\n$$\n\nCombining both terms, the total predictive variance, $\\sigma^2_{Y^\\star}$, is:\n$$\n\\sigma^2_{Y^\\star} = \\hat{\\sigma}^2 \\left( 1 + \\frac{1}{n} + \\frac{(x_{\\mathrm{obs}} - \\bar{X})^2 + s_x^2}{S_{XX}} \\right) + \\hat{b}^2 s_x^2\n$$\nThe constrained predictive standard deviation is $\\sigma_{Y^\\star} = \\sqrt{\\sigma^2_{Y^\\star}}$.\n\n**3. Unconstrained Uncertainty and Reduction Factor**\n\nThe unconstrained projection uncertainty is defined as the spread of the future response variable $Y$ across the raw model ensemble, quantified by the sample standard deviation:\n$$\ns_Y = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n (Y_i - \\bar{Y})^2}\n$$\nThe reduction factor, $R$, quantifies the benefit of the emergent constraint. It is the ratio of the constrained predictive standard deviation to the unconstrained ensemble standard deviation:\n$$\nR = \\frac{\\sigma_{Y^\\star}}{s_Y}\n$$\nA value of $R < 1$ indicates that the constraint has successfully reduced the projection uncertainty.\n\nThe implementation will follow these derived formulas to calculate $(\\mu_{Y^\\star}, \\sigma_{Y^\\star}, R)$ for each provided test case.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes constrained projections and uncertainty reduction for a series of test cases\n    based on the emergent constraint methodology.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"X\": [0.2, 0.4, 0.6, 0.8, 1.0, 0.3, 0.7, 0.9],\n            \"Y\": [0.98, 1.25, 1.73, 2.08, 2.51, 1.10, 1.86, 2.32],\n            \"x_obs\": 0.65,\n            \"s_x\": 0.05,\n        },\n        {\n            \"X\": [0.1, 0.2, 0.5, 0.6, 0.8, 0.3, 0.4, 0.7, 0.9, 0.55],\n            \"Y\": [0.71, 1.27, 0.95, 1.26, 0.93, 1.08, 0.99, 1.17, 0.89, 1.205],\n            \"x_obs\": 0.50,\n            \"s_x\": 0.05,\n        },\n        {\n            \"X\": [0.2, 0.3, 0.4, 0.5, 0.6],\n            \"Y\": [0.0, 0.05, 0.22, 0.27, 0.41],\n            \"x_obs\": 0.40,\n            \"s_x\": 0.00,\n        },\n        {\n            \"X\": [0.2, 0.5, 0.7, 1.0, 0.3, 0.9],\n            \"Y\": [0.70, 0.85, 1.40, 1.65, 0.65, 1.65],\n            \"x_obs\": 0.60,\n            \"s_x\": 0.20,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        # Main logic to calculate the result for one case goes here.\n        X_data = np.array(case[\"X\"])\n        Y_data = np.array(case[\"Y\"])\n        x_obs = case[\"x_obs\"]\n        s_x = case[\"s_x\"]\n\n        n = len(X_data)\n        \n        # Step 1: OLS Estimation\n        X_mean = np.mean(X_data)\n        Y_mean = np.mean(Y_data)\n\n        S_xy = np.sum((X_data - X_mean) * (Y_data - Y_mean))\n        S_xx = np.sum((X_data - X_mean)**2)\n        \n        if S_xx == 0:\n            # Handle degenerate case where all X values are the same.\n            # This should not occur in the given test cases.\n            # In this scenario, b_hat is undefined and correlation is meaningless.\n            # This simple handling prevents division by zero.\n            b_hat = 0\n            a_hat = Y_mean\n            sigma_hat_sq = np.var(Y_data, ddof=1) if n > 1 else 0\n        else:\n            b_hat = S_xy / S_xx\n            a_hat = Y_mean - b_hat * X_mean\n\n            Y_pred_model = a_hat + b_hat * X_data\n            residuals = Y_data - Y_pred_model\n            SSR = np.sum(residuals**2)\n            # Degrees of freedom are n-2 for simple linear regression\n            sigma_hat_sq = SSR / (n - 2)\n            \n        # Step 2: Calculate constrained predictive mean and variance\n        pred_mean_Y = a_hat + b_hat * x_obs\n        \n        # The total predictive variance calculation requires S_xx > 0\n        if S_xx > 0:\n            term1 = sigma_hat_sq * (1 + 1/n + ((x_obs - X_mean)**2 + s_x**2) / S_xx)\n            term2 = b_hat**2 * s_x**2\n            pred_var_Y = term1 + term2\n        else: # If S_xx=0, we cannot use the constraint. Uncertainty is just the sample variance.\n            pred_var_Y = np.var(Y_data, ddof=1) if n > 1 else 0\n\n        pred_std_Y = np.sqrt(pred_var_Y)\n        \n        # Step 3: Quantify uncertainty reduction\n        unconstrained_std_Y = np.std(Y_data, ddof=1)\n        \n        if unconstrained_std_Y > 0:\n            reduction_factor = pred_std_Y / unconstrained_std_Y\n        else:\n            # If original data has no spread, the concept of reduction is ill-defined.\n            reduction_factor = 1.0 if pred_std_Y == 0 else float('inf')\n\n        results.append([pred_mean_Y, pred_std_Y, reduction_factor])\n\n    # Final print statement in the exact required format.\n    # Format each float to 6 decimal places and then join.\n    formatted_results = []\n    for res_list in results:\n        formatted_list_str = f\"[{','.join([f'{val:.6f}' for val in res_list])}]\"\n        formatted_results.append(formatted_list_str)\n    \n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "While linear relationships are a common starting point, the true link between a predictor and a projected variable may be nonlinear. This advanced practice tackles the critical, real-world challenge of model selection, where we must determine the most appropriate functional form for the emergent constraint. You will fit linear, quadratic, and saturating exponential models to the data and use a formal statistical metric, the Akaike Information Criterion (AIC), to select the most plausible relationship . This exercise will equip you with a more robust and flexible framework for developing emergent constraints by teaching you how to propagate uncertainty through nonlinear functions.",
            "id": "3878138",
            "problem": "Let an Emergent Constraint (EC) be constructed between a dimensionless predictor $X$ (e.g., a historical variability metric) and a dimensionless target $Y$ (e.g., a future change metric) from an ensemble of $N$ Earth system models. You are given an ensemble of paired samples $\\{(x_i,y_i)\\}_{i=1}^N$ and an observational distribution for the predictor $X_{\\text{obs}} \\sim \\mathcal{N}(\\mu_x,\\sigma_x^2)$, where $\\mathcal{N}$ denotes the normal distribution with mean $\\mu_x$ and variance $\\sigma_x^2$. Assume additive Gaussian residuals $\\epsilon \\sim \\mathcal{N}(0,\\sigma_\\epsilon^2)$ and independence between $X$ and $\\epsilon$.\n\nThe candidate EC functional forms are:\n- Linear: $f_1(x) = a_1 + b_1 x$.\n- Quadratic: $f_2(x) = a_2 + b_2 x + c_2 x^2$.\n- Saturating exponential: $f_3(x) = \\alpha_3 + \\beta_3 \\left(1 - e^{-\\gamma_3 x}\\right)$.\n\nFor each functional form $f_j$, estimate its parameters by maximum likelihood under the Gaussian residual assumption. Under this assumption, the maximum likelihood estimate for $\\sigma_\\epsilon^2$ is $\\hat{\\sigma}_\\epsilon^2 = \\frac{1}{N}\\sum_{i=1}^N r_i^2$, where $r_i = y_i - f_j(x_i)$ are residuals. The maximized log-likelihood is $\\ln(\\hat{L}) = -\\frac{N}{2}\\left(\\ln\\left(2\\pi\\hat{\\sigma}_\\epsilon^2\\right) + 1\\right)$. Use the Akaike Information Criterion (AIC) defined by $\\mathrm{AIC} = 2k - 2\\ln(\\hat{L})$, where $k$ is the number of estimated parameters including $\\sigma_\\epsilon^2$.\n\nSelect the functional form with the lowest AIC. Then, for the selected form, compute the EC-constrained predictive mean $m = \\mathbb{E}[Y \\mid X_{\\text{obs}}]$ and standard deviation $s = \\sqrt{\\operatorname{Var}(Y \\mid X_{\\text{obs}})}$ by marginalizing over $X_{\\text{obs}} \\sim \\mathcal{N}(\\mu_x,\\sigma_x^2)$ and the additive residual $\\epsilon \\sim \\mathcal{N}(0,\\hat{\\sigma}_\\epsilon^2)$. Define the extrapolation indicator $E$ to be $\\mathrm{True}$ if $\\mu_x$ lies outside the closed interval $[\\min_i x_i,\\max_i x_i]$, and $\\mathrm{False}$ otherwise.\n\nYour program must implement the above procedure and return, for each test case, a list $[m, s, i, E]$, where $m$ is a float, $s$ is a float, $i$ is an integer index of the selected functional form ($i=0$ for linear, $i=1$ for quadratic, $i=2$ for saturating exponential), and $E$ is a boolean extrapolation flag. All quantities are dimensionless.\n\nUse the following test suite. In each case, $\\{x_i\\}$ and $\\{y_i\\}$ are given explicitly, along with $(\\mu_x,\\sigma_x)$:\n\n- Test Case $1$ (approximately linear EC):\n  - Predictor ensemble: $x = [\\,0,\\;0.5,\\;1.0,\\;1.5,\\;2.0\\,]$.\n  - Target ensemble: $y = [\\,2.1,\\;3.45,\\;5.08,\\;6.48,\\;8.05\\,]$.\n  - Observation: $\\mu_x = 1.2$, $\\sigma_x = 0.1$.\n\n- Test Case $2$ (approximately quadratic EC):\n  - Predictor ensemble: $x = [\\,0,\\;0.5,\\;1.0,\\;1.5,\\;2.0,\\;2.5\\,]$.\n  - Target ensemble: $y = [\\,1.02,\\;0.595,\\;0.51,\\;0.675,\\;0.98,\\;1.665\\,]$.\n  - Observation: $\\mu_x = 1.8$, $\\sigma_x = 0.2$.\n\n- Test Case $3$ (approximately saturating exponential EC):\n  - Predictor ensemble: $x = [\\,0,\\;0.5,\\;1.0,\\;1.5,\\;2.0,\\;3.0\\,]$.\n  - Target ensemble: $y = [\\,0.1,\\;2.45594182,\\;3.5440289405,\\;4.353505559,\\;4.6264102335,\\;4.99338139\\,]$.\n  - Observation: $\\mu_x = 1.7$, $\\sigma_x = 0.3$.\n\n- Test Case $4$ (extrapolation check):\n  - Predictor ensemble: $x = [\\,0.0,\\;0.2,\\;0.4,\\;0.6,\\;0.8,\\;1.0\\,]$.\n  - Target ensemble: $y = [\\,0.98,\\;1.41,\\;1.8,\\;2.19,\\;2.62,\\;2.98\\,]$.\n  - Observation: $\\mu_x = 2.0$, $\\sigma_x = 0.1$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, namely $[\\,[m_1,s_1,i_1,E_1],\\,[m_2,s_2,i_2,E_2],\\,[m_3,s_3,i_3,E_3],\\,[m_4,s_4,i_4,E_4]\\,]$. All values $m_j$ and $s_j$ must be floats, $i_j$ must be integers, and $E_j$ must be booleans. No units conversion is required because all quantities are dimensionless.",
            "solution": "We formalize emergent constraints by positing a functional relationship $Y = f(X) + \\epsilon$ with $\\epsilon \\sim \\mathcal{N}(0,\\sigma_\\epsilon^2)$ and $X$ treated as known for model outputs and as a random variable for observations. The ensemble $\\{(x_i,y_i)\\}_{i=1}^N$ provides samples to estimate $f$ and $\\sigma_\\epsilon^2$ by maximum likelihood under Gaussian residuals.\n\nGiven a parametric form $f_\\theta(x)$ with parameter vector $\\theta$, the Gaussian likelihood is\n$$\nL(\\theta,\\sigma_\\epsilon^2) = \\prod_{i=1}^N \\frac{1}{\\sqrt{2\\pi\\sigma_\\epsilon^2}} \\exp\\left(-\\frac{\\left(y_i - f_\\theta(x_i)\\right)^2}{2\\sigma_\\epsilon^2}\\right).\n$$\nThe log-likelihood is\n$$\n\\ln L(\\theta,\\sigma_\\epsilon^2) = -\\frac{N}{2}\\ln(2\\pi\\sigma_\\epsilon^2) - \\frac{1}{2\\sigma_\\epsilon^2}\\sum_{i=1}^N \\left(y_i - f_\\theta(x_i)\\right)^2.\n$$\nMaximizing with respect to $\\sigma_\\epsilon^2$ for a fixed $\\theta$ yields the maximum likelihood estimate\n$$\n\\hat{\\sigma}_\\epsilon^2 = \\frac{1}{N}\\sum_{i=1}^N r_i^2,\\quad r_i = y_i - f_{\\hat{\\theta}}(x_i),\n$$\nand the maximized log-likelihood evaluates to\n$$\n\\ln(\\hat{L}) = -\\frac{N}{2}\\left(\\ln\\left(2\\pi\\hat{\\sigma}_\\epsilon^2\\right) + 1\\right).\n$$\nTo balance goodness-of-fit and parsimony, select the functional form by minimizing the Akaike Information Criterion,\n$$\n\\mathrm{AIC} = 2k - 2\\ln(\\hat{L}),\n$$\nwhere $k$ counts the number of free parameters including $\\sigma_\\epsilon^2$. For the candidate forms, $k$ equals $3$ for the linear model ($a_1$, $b_1$, $\\sigma_\\epsilon^2$), $4$ for the quadratic model ($a_2$, $b_2$, $c_2$, $\\sigma_\\epsilon^2$), and $4$ for the saturating exponential model ($\\alpha_3$, $\\beta_3$, $\\gamma_3$, $\\sigma_\\epsilon^2$).\n\nOnce the form is selected, we derive the predictive moments for the observed predictor $X_{\\text{obs}} \\sim \\mathcal{N}(\\mu_x,\\sigma_x^2)$ under independence between $X$ and residual noise $\\epsilon$:\n$$\nY = f(X) + \\epsilon,\\quad \\epsilon \\sim \\mathcal{N}(0,\\hat{\\sigma}_\\epsilon^2).\n$$\nThus,\n$$\n\\mathbb{E}[Y \\mid X_{\\text{obs}}] = \\mathbb{E}[f(X)],\\quad \\operatorname{Var}(Y \\mid X_{\\text{obs}}) = \\operatorname{Var}(f(X)) + \\hat{\\sigma}_\\epsilon^2.\n$$\nWe compute these moments using known properties of the normal distribution and moment generating functions.\n\nFor the linear model $f_1(x) = a_1 + b_1 x$,\n$$\n\\mathbb{E}[Y \\mid X_{\\text{obs}}] = a_1 + b_1 \\mu_x,\\quad \\operatorname{Var}(Y \\mid X_{\\text{obs}}) = b_1^2 \\sigma_x^2 + \\hat{\\sigma}_\\epsilon^2.\n$$\n\nFor the quadratic model $f_2(x) = a_2 + b_2 x + c_2 x^2$, using $\\mathbb{E}[X^2] = \\mu_x^2 + \\sigma_x^2$, $\\operatorname{Var}(X) = \\sigma_x^2$, $\\operatorname{Var}(X^2) = 2\\sigma_x^4 + 4\\mu_x^2\\sigma_x^2$, and $\\operatorname{Cov}(X,X^2) = 2\\mu_x\\sigma_x^2$ for $X \\sim \\mathcal{N}(\\mu_x,\\sigma_x^2)$,\n$$\n\\mathbb{E}[Y \\mid X_{\\text{obs}}] = a_2 + b_2 \\mu_x + c_2\\left(\\mu_x^2 + \\sigma_x^2\\right),\n$$\n$$\n\\operatorname{Var}(Y \\mid X_{\\text{obs}}) = b_2^2 \\sigma_x^2 + c_2^2\\left(2\\sigma_x^4 + 4\\mu_x^2\\sigma_x^2\\right) + 2b_2c_2\\left(2\\mu_x\\sigma_x^2\\right) + \\hat{\\sigma}_\\epsilon^2.\n$$\n\nFor the saturating exponential model $f_3(x) = \\alpha_3 + \\beta_3 \\left(1 - e^{-\\gamma_3 x}\\right)$, denote $Z = e^{-\\gamma_3 X}$. For $X \\sim \\mathcal{N}(\\mu_x,\\sigma_x^2)$, using the moment generating function of a normal random variable, we have\n$$\n\\mathbb{E}[Z] = \\exp\\left(-\\gamma_3 \\mu_x + \\frac{1}{2}\\gamma_3^2 \\sigma_x^2\\right),\n$$\nand\n$$\n\\operatorname{Var}(Z) = \\exp\\left(-2\\gamma_3 \\mu_x\\right)\\left[\\exp\\left(2\\gamma_3^2 \\sigma_x^2\\right) - \\exp\\left(\\gamma_3^2 \\sigma_x^2\\right)\\right].\n$$\nConsequently,\n$$\n\\mathbb{E}[Y \\mid X_{\\text{obs}}] = \\alpha_3 + \\beta_3\\left(1 - \\mathbb{E}[Z]\\right),\n$$\n$$\n\\operatorname{Var}(Y \\mid X_{\\text{obs}}) = \\beta_3^2 \\operatorname{Var}(Z) + \\hat{\\sigma}_\\epsilon^2.\n$$\n\nThe extrapolation indicator is defined by comparing the observed predictor mean to the ensemble support:\n$$\nE = \\begin{cases}\n\\mathrm{True}, & \\text{if } \\mu_x < \\min_i x_i \\text{ or } \\mu_x > \\max_i x_i,\\\\\n\\mathrm{False}, & \\text{otherwise}.\n\\end{cases}\n$$\n\nAlgorithmic steps for each test case:\n$1.$ Fit all three candidate models to $\\{(x_i,y_i)\\}_{i=1}^N$ by minimizing $\\sum_i r_i^2$ (equivalently maximizing the Gaussian likelihood), obtain parameter estimates and compute $\\hat{\\sigma}_\\epsilon^2 = \\frac{1}{N}\\sum_i r_i^2$ and $\\ln(\\hat{L}) = -\\frac{N}{2}\\left(\\ln\\left(2\\pi\\hat{\\sigma}_\\epsilon^2\\right) + 1\\right)$.\n$2.$ Compute $\\mathrm{AIC} = 2k - 2\\ln(\\hat{L})$ for each candidate, with $k$ equal to $3$ for linear, $4$ for quadratic, and $4$ for saturating exponential.\n$3.$ Select the candidate with minimum AIC, and compute $m = \\mathbb{E}[Y \\mid X_{\\text{obs}}]$ and $s = \\sqrt{\\operatorname{Var}(Y \\mid X_{\\text{obs}})}$ using the formulas above.\n$4.$ Compute $E$ as specified.\n$5.$ Return $[m,s,i,E]$ with $i \\in \\{0,1,2\\}$ mapping to linear, quadratic, and saturating exponential respectively.\n\nThe final program applies this procedure to the four provided test cases and prints a single line containing the aggregated list of results $[\\,[m_1,s_1,i_1,E_1],\\,[m_2,s_2,i_2,E_2],\\,[m_3,s_3,i_3,E_3],\\,[m_4,s_4,i_4,E_4]\\,]$. All quantities are dimensionless, and no units conversion is required. The approach leverages fundamental properties of the normal distribution and maximum likelihood estimation without relying on shortcut formulas beyond standard derivations.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import curve_fit\n\ndef fit_linear(x, y):\n    # Fit y = a + b x\n    # np.polyfit returns [b, a] for deg=1\n    b, a = np.polyfit(x, y, deg=1)\n    y_hat = a + b * x\n    residuals = y - y_hat\n    N = len(y)\n    sigma_eps2 = float(np.sum(residuals**2) / N)\n    logL = -0.5 * N * (np.log(2 * np.pi * sigma_eps2) + 1.0)\n    k = 3  # a, b, sigma^2\n    AIC = 2 * k - 2 * logL\n    return {'name': 'linear', 'params': (a, b), 'sigma_eps2': sigma_eps2, 'AIC': AIC}\n\ndef fit_quadratic(x, y):\n    # Fit y = a + b x + c x^2\n    # np.polyfit returns [c, b, a] for deg=2\n    c, b, a = np.polyfit(x, y, deg=2)\n    y_hat = a + b * x + c * x**2\n    residuals = y - y_hat\n    N = len(y)\n    sigma_eps2 = float(np.sum(residuals**2) / N)\n    logL = -0.5 * N * (np.log(2 * np.pi * sigma_eps2) + 1.0)\n    k = 4  # a, b, c, sigma^2\n    AIC = 2 * k - 2 * logL\n    return {'name': 'quadratic', 'params': (a, b, c), 'sigma_eps2': sigma_eps2, 'AIC': AIC}\n\ndef exp_saturating(x, alpha, beta, gamma):\n    return alpha + beta * (1.0 - np.exp(-gamma * x))\n\ndef fit_exponential(x, y):\n    # Fit y = alpha + beta (1 - exp(-gamma x))\n    # Provide reasonable initial guesses and bounds\n    alpha0 = float(np.min(y))\n    beta0 = float(np.max(y) - alpha0)\n    x_range = float(np.max(x) - np.min(x))\n    gamma0 = 1.0 / (x_range + 1e-6)\n    p0 = (alpha0, beta0 if beta0 > 0 else 1.0, gamma0)\n    bounds = ([-1e3, 0.0, 1e-6], [1e3, 1e3, 1e3])\n    try:\n        popt, _ = curve_fit(exp_saturating, x, y, p0=p0, bounds=bounds, maxfev=10000)\n    except Exception:\n        # Fallback to initial guesses if optimization fails\n        popt = p0\n    alpha, beta, gamma = popt\n    y_hat = exp_saturating(x, alpha, beta, gamma)\n    residuals = y - y_hat\n    N = len(y)\n    sigma_eps2 = float(np.sum(residuals**2) / N)\n    logL = -0.5 * N * (np.log(2 * np.pi * sigma_eps2) + 1.0)\n    k = 4  # alpha, beta, gamma, sigma^2\n    AIC = 2 * k - 2 * logL\n    return {'name': 'exponential', 'params': (alpha, beta, gamma), 'sigma_eps2': sigma_eps2, 'AIC': AIC}\n\ndef moments_linear(params, mu_x, sigma_x, sigma_eps2):\n    a, b = params\n    mean = a + b * mu_x\n    var = (b ** 2) * (sigma_x ** 2) + sigma_eps2\n    return mean, var\n\ndef moments_quadratic(params, mu_x, sigma_x, sigma_eps2):\n    a, b, c = params\n    Ex2 = mu_x**2 + sigma_x**2\n    VarX = sigma_x**2\n    VarX2 = 2.0 * (sigma_x**4) + 4.0 * (mu_x**2) * (sigma_x**2)\n    CovXX2 = 2.0 * mu_x * (sigma_x**2)\n    mean = a + b * mu_x + c * Ex2\n    var = (b**2) * VarX + (c**2) * VarX2 + 2.0 * b * c * CovXX2 + sigma_eps2\n    return mean, var\n\ndef moments_exponential(params, mu_x, sigma_x, sigma_eps2):\n    alpha, beta, gamma = params\n    Ez = np.exp(-gamma * mu_x + 0.5 * (gamma**2) * (sigma_x**2))\n    var_z = np.exp(-2.0 * gamma * mu_x) * (np.exp(2.0 * (gamma**2) * (sigma_x**2)) - np.exp((gamma**2) * (sigma_x**2)))\n    mean = alpha + beta * (1.0 - Ez)\n    var = (beta**2) * var_z + sigma_eps2\n    return mean, var\n\ndef select_model_and_predict(x, y, mu_x, sigma_x):\n    # Fit all models\n    models = [\n        ('linear', fit_linear(x, y)),\n        ('quadratic', fit_quadratic(x, y)),\n        ('exponential', fit_exponential(x, y))\n    ]\n    # Select by lowest AIC\n    AICs = [m[1]['AIC'] for m in models]\n    best_idx = int(np.argmin(AICs))\n    best_name, best = models[best_idx]\n    # Compute predictive moments based on selected model\n    if best_name == 'linear':\n        mean, var = moments_linear(best['params'], mu_x, sigma_x, best['sigma_eps2'])\n        model_index = 0\n    elif best_name == 'quadratic':\n        mean, var = moments_quadratic(best['params'], mu_x, sigma_x, best['sigma_eps2'])\n        model_index = 1\n    else:\n        mean, var = moments_exponential(best['params'], mu_x, sigma_x, best['sigma_eps2'])\n        model_index = 2\n    std = float(np.sqrt(var))\n    # Extrapolation flag\n    extrap = bool((mu_x  float(np.min(x))) or (mu_x > float(np.max(x))))\n    return [float(mean), float(std), int(model_index), extrap]\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test Case 1\n        {\n            'x': np.array([0.0, 0.5, 1.0, 1.5, 2.0]),\n            'y': np.array([2.1, 3.45, 5.08, 6.48, 8.05]),\n            'mu_x': 1.2,\n            'sigma_x': 0.1\n        },\n        # Test Case 2\n        {\n            'x': np.array([0.0, 0.5, 1.0, 1.5, 2.0, 2.5]),\n            'y': np.array([1.02, 0.595, 0.51, 0.675, 0.98, 1.665]),\n            'mu_x': 1.8,\n            'sigma_x': 0.2\n        },\n        # Test Case 3\n        {\n            'x': np.array([0.0, 0.5, 1.0, 1.5, 2.0, 3.0]),\n            'y': np.array([0.1, 2.45594182, 3.5440289405, 4.353505559, 4.6264102335, 4.99338139]),\n            'mu_x': 1.7,\n            'sigma_x': 0.3\n        },\n        # Test Case 4\n        {\n            'x': np.array([0.0, 0.2, 0.4, 0.6, 0.8, 1.0]),\n            'y': np.array([0.98, 1.41, 1.8, 2.19, 2.62, 2.98]),\n            'mu_x': 2.0,\n            'sigma_x': 0.1\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        x = case['x']\n        y = case['y']\n        mu_x = case['mu_x']\n        sigma_x = case['sigma_x']\n        res = select_model_and_predict(x, y, mu_x, sigma_x)\n        results.append(res)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}