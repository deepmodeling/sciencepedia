{
    "hands_on_practices": [
        {
            "introduction": "A common goal in model intercomparison studies is to synthesize multiple model outputs into a single, more accurate prediction. This practice delves into the foundational technique of creating a weighted ensemble mean. By minimizing the mean squared error, we can derive optimal weights that account not just for individual model skill but, crucially, for the error correlations between models. Mastering this technique provides a powerful tool for forecast improvement and a deeper intuition for how model interdependence shapes ensemble performance. ",
            "id": "3895017",
            "problem": "In a model intercomparison study of a Multi-Model Ensemble (MME), $3$ independent modeling centers provide annual mean predictions $\\{Y_{k}\\}_{k=1}^{3}$ for the same geophysical quantity $Y$ (for example, a global-mean surface temperature anomaly). Assume each model is individually unbiased for $Y$, in the sense that the model error $e_{k} \\equiv Y_{k}-Y$ satisfies $\\mathbb{E}[e_{k}] = 0$ for each $k$, and let the model error vector be $e \\equiv (e_{1},e_{2},e_{3})^{\\top}$ with covariance matrix $\\Sigma \\equiv \\mathrm{Cov}(e)$ known from a long, quasi-stationary verification dataset. You wish to construct a linear ensemble estimator $\\hat{Y} \\equiv \\sum_{k=1}^{3} w_{k} Y_{k}$ with weights $w \\equiv (w_{1},w_{2},w_{3})^{\\top}$ that minimizes the expected squared error $\\mathbb{E}\\!\\left[\\left(\\sum_{k=1}^{3} w_{k} Y_{k} - Y\\right)^{2}\\right]$, subject to the constraint $\\sum_{k=1}^{3} w_{k} = 1$.\n\nStarting only from the definitions of expectation, variance, and covariance, and using the method of Lagrange multipliers from multivariate calculus, derive the constrained minimizer $w$ in symbolic form. Then, for the scientifically plausible error covariance matrix\n$$\n\\Sigma \\;=\\;\n\\begin{pmatrix}\n4 & 1 & 1 \\\\\n1 & 3 & 0 \\\\\n1 & 0 & 2\n\\end{pmatrix},\n$$\ncompute the exact ensemble weights $w$. Express the final weight vector using exact rational numbers. No rounding is required. The weights are dimensionless.",
            "solution": "### Symbolic Derivation of Optimal Weights\nThe objective is to minimize the expected squared error of the linear ensemble estimator, $J(w) = \\mathbb{E}[(\\hat{Y} - Y)^2]$, subject to the constraint that the weights sum to one, $\\sum_{k=1}^{3} w_k = 1$. First, we express the estimator's error in terms of the individual model errors, $e_k = Y_k - Y$:\n$$\n\\hat{Y} - Y = \\sum_{k=1}^{3} w_k Y_k - Y = \\sum_{k=1}^{3} w_k Y_k - \\left(\\sum_{k=1}^{3} w_k\\right) Y = \\sum_{k=1}^{3} w_k (Y_k - Y) = \\sum_{k=1}^{3} w_k e_k\n$$\nIn vector notation, this is $w^\\top e$. The objective function, which is the variance of the ensemble error (since $\\mathbb{E}[e]=\\mathbf{0}$), becomes:\n$$\nJ(w) = \\mathbb{E}[(w^\\top e)^2] = \\mathbb{E}[w^\\top e e^\\top w] = w^\\top \\mathbb{E}[e e^\\top] w = w^\\top \\Sigma w\n$$\nwhere $\\Sigma = \\mathrm{Cov}(e)$ is the error covariance matrix.\n\nWe minimize $J(w)$ subject to the constraint $\\mathbf{1}^\\top w = 1$ using the method of Lagrange multipliers. The Lagrangian is:\n$$\n\\mathcal{L}(w, \\lambda) = w^\\top \\Sigma w - \\lambda (\\mathbf{1}^\\top w - 1)\n$$\nSetting the gradient with respect to $w$ to zero yields:\n$$\n\\nabla_w \\mathcal{L} = 2 \\Sigma w - \\lambda \\mathbf{1} = \\mathbf{0} \\implies w = \\frac{\\lambda}{2} \\Sigma^{-1} \\mathbf{1}\n$$\nWe find the Lagrange multiplier $\\lambda$ by applying the constraint:\n$$\n\\mathbf{1}^\\top w = 1 \\implies \\mathbf{1}^\\top \\left( \\frac{\\lambda}{2} \\Sigma^{-1} \\mathbf{1} \\right) = 1 \\implies \\frac{\\lambda}{2} = \\frac{1}{\\mathbf{1}^\\top \\Sigma^{-1} \\mathbf{1}}\n$$\nSubstituting this back gives the symbolic solution for the optimal weights:\n$$\nw = \\frac{\\Sigma^{-1} \\mathbf{1}}{\\mathbf{1}^\\top \\Sigma^{-1} \\mathbf{1}}\n$$\n\n### Numerical Calculation for the Given Covariance Matrix\nWe are given the covariance matrix:\n$$\n\\Sigma =\n\\begin{pmatrix}\n4 & 1 & 1 \\\\\n1 & 3 & 0 \\\\\n1 & 0 & 2\n\\end{pmatrix}\n$$\nFirst, we compute the inverse of $\\Sigma$. The determinant is $\\det(\\Sigma) = 4(3 \\cdot 2 - 0 \\cdot 0) - 1(1 \\cdot 2 - 0 \\cdot 1) + 1(1 \\cdot 0 - 3 \\cdot 1) = 24 - 2 - 3 = 19$. The inverse is $\\Sigma^{-1} = \\frac{1}{\\det(\\Sigma)} \\text{adj}(\\Sigma)$. The adjugate matrix, which is the transpose of the cofactor matrix, is:\n$$\n\\text{adj}(\\Sigma) = \n\\begin{pmatrix}\n\\det\\begin{pmatrix} 3 & 0 \\\\ 0 & 2 \\end{pmatrix} & -\\det\\begin{pmatrix} 1 & 0 \\\\ 1 & 2 \\end{pmatrix} & \\det\\begin{pmatrix} 1 & 3 \\\\ 1 & 0 \\end{pmatrix} \\\\\n-\\det\\begin{pmatrix} 1 & 1 \\\\ 0 & 2 \\end{pmatrix} & \\det\\begin{pmatrix} 4 & 1 \\\\ 1 & 2 \\end{pmatrix} & -\\det\\begin{pmatrix} 4 & 1 \\\\ 1 & 0 \\end{pmatrix} \\\\\n\\det\\begin{pmatrix} 1 & 1 \\\\ 3 & 0 \\end{pmatrix} & -\\det\\begin{pmatrix} 4 & 1 \\\\ 1 & 0 \\end{pmatrix} & \\det\\begin{pmatrix} 4 & 1 \\\\ 1 & 3 \\end{pmatrix}\n\\end{pmatrix}^\\top\n=\n\\begin{pmatrix}\n6 & -2 & -3 \\\\\n-2 & 7 & 1 \\\\\n-3 & 1 & 11\n\\end{pmatrix}^\\top\n=\n\\begin{pmatrix}\n6 & -2 & -3 \\\\\n-2 & 7 & 1 \\\\\n-3 & 1 & 11\n\\end{pmatrix}\n$$\nThus, the inverse matrix is:\n$$\n\\Sigma^{-1} = \\frac{1}{19}\n\\begin{pmatrix}\n6 & -2 & -3 \\\\\n-2 & 7 & 1 \\\\\n-3 & 1 & 11\n\\end{pmatrix}\n$$\nNext, we calculate the numerator of the weight formula, $\\Sigma^{-1} \\mathbf{1}$:\n$$\n\\Sigma^{-1} \\mathbf{1} = \\frac{1}{19}\n\\begin{pmatrix}\n6 & -2 & -3 \\\\\n-2 & 7 & 1 \\\\\n-3 & 1 & 11\n\\end{pmatrix}\n\\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}\n= \\frac{1}{19}\n\\begin{pmatrix}\n6 - 2 - 3 \\\\\n-2 + 7 + 1 \\\\\n-3 + 1 + 11\n\\end{pmatrix}\n= \\frac{1}{19}\n\\begin{pmatrix}\n1 \\\\\n6 \\\\\n9\n\\end{pmatrix}\n$$\nNext, we calculate the denominator, $\\mathbf{1}^\\top \\Sigma^{-1} \\mathbf{1}$. This is the sum of the components of the vector we just calculated:\n$$\n\\mathbf{1}^\\top \\Sigma^{-1} \\mathbf{1} = \\frac{1}{19} (1 + 6 + 9) = \\frac{16}{19}\n$$\nFinally, we compute the weight vector $w$:\n$$\nw = \\frac{\\Sigma^{-1} \\mathbf{1}}{\\mathbf{1}^\\top \\Sigma^{-1} \\mathbf{1}} = \\frac{\\frac{1}{19} \\begin{pmatrix} 1 \\\\ 6 \\\\ 9 \\end{pmatrix}}{\\frac{16}{19}} = \\frac{1}{16} \\begin{pmatrix} 1 \\\\ 6 \\\\ 9 \\end{pmatrix}\n$$\nSo the individual weights are:\n$w_1 = \\frac{1}{16}$, $w_2 = \\frac{6}{16} = \\frac{3}{8}$, and $w_3 = \\frac{9}{16}$.\nThe final weight vector is $w = (1/16, 3/8, 9/16)^\\top$.",
            "answer": "$$\\boxed{\\begin{pmatrix} \\frac{1}{16} & \\frac{3}{8} & \\frac{9}{16} \\end{pmatrix}}$$"
        },
        {
            "introduction": "Simply counting the number of models in an ensemble can be misleading, as models often share code, parameterizations, and biases, making them non-independent. This exercise introduces the concept of the effective sample size, $N_{\\text{eff}}$, which quantifies the true number of independent pieces of information in a correlated ensemble. You will derive this metric from first principles, starting with the variance of an ensemble mean, to understand the information content of a multi-model ensemble. Calculating $N_{\\text{eff}}$ is a crucial skill for realistically assessing the confidence in an ensemble-mean estimate and for appreciating the true value of adding a new, potentially related, model to an existing collection. ",
            "id": "3895076",
            "problem": "Consider a multi-model ensemble in the Coupled Model Intercomparison Project (CMIP), consisting of $N=6$ climate models grouped into two structural families of size $3$ each, based on shared dynamical cores and parameterization choices. Let $X_i$ denote the standardized annual-mean near-surface temperature anomaly for model $i$, where standardization ensures $\\operatorname{Var}(X_i)=1$ for all $i$. Assume the following scientifically motivated factor structure: for models in family $A$,\n$$\nX_i = \\sqrt{0.3}\\,C + \\sqrt{0.5}\\,G_A + \\sqrt{0.2}\\,\\varepsilon_i,\n$$\nand for models in family $B$,\n$$\nX_j = \\sqrt{0.3}\\,C + \\sqrt{0.5}\\,G_B + \\sqrt{0.2}\\,\\varepsilon_j,\n$$\nwhere $C$, $G_A$, $G_B$, and all $\\varepsilon_i$ are mutually independent, each with zero mean and unit variance, and families $A$ and $B$ are disjoint index sets of size $3$ each. This construction implies the correlation matrix $\\boldsymbol{R}=\\{\\rho_{ij}\\}$ across models,\n$$\n\\boldsymbol{R} = \\begin{pmatrix}\n1 & 0.8 & 0.8 & 0.3 & 0.3 & 0.3 \\\\\n0.8 & 1 & 0.8 & 0.3 & 0.3 & 0.3 \\\\\n0.8 & 0.8 & 1 & 0.3 & 0.3 & 0.3 \\\\\n0.3 & 0.3 & 0.3 & 1 & 0.8 & 0.8 \\\\\n0.3 & 0.3 & 0.3 & 0.8 & 1 & 0.8 \\\\\n0.3 & 0.3 & 0.3 & 0.8 & 0.8 & 1\n\\end{pmatrix}.\n$$\nStarting from first principles—namely, the definition of the variance of a mean of correlated random variables in terms of covariances and correlations—define the effective sample size $N_{\\text{eff}}$ as the unique number satisfying that the variance of the ensemble mean under correlation equals the variance of the mean of $N_{\\text{eff}}$ independent, unit-variance draws. Derive a closed-form analytic expression for $N_{\\text{eff}}$ in terms of $\\{\\rho_{ij}\\}$ and $N$, and then compute $N_{\\text{eff}}$ for the given $\\boldsymbol{R}$. Round your final numerical answer to four significant figures. Report a dimensionless number. In your reasoning, interpret the value of $N_{\\text{eff}}$ relative to the actual ensemble size $N$ in the context of model intercomparison studies and ensemble averaging.",
            "solution": "### Derivation of the Effective Sample Size Formula\nLet $\\bar{X}$ be the ensemble mean of the $N$ model outputs $X_i$:\n$$\n\\bar{X} = \\frac{1}{N} \\sum_{i=1}^{N} X_i\n$$\nThe variance of the ensemble mean, $\\operatorname{Var}(\\bar{X})$, is given by:\n$$\n\\operatorname{Var}(\\bar{X}) = \\operatorname{Var}\\left(\\frac{1}{N} \\sum_{i=1}^{N} X_i\\right) = \\frac{1}{N^2} \\operatorname{Var}\\left(\\sum_{i=1}^{N} X_i\\right)\n$$\nThe variance of a sum of random variables is the sum of all elements in their covariance matrix:\n$$\n\\operatorname{Var}\\left(\\sum_{i=1}^{N} X_i\\right) = \\sum_{i=1}^{N} \\sum_{j=1}^{N} \\operatorname{Cov}(X_i, X_j)\n$$\nSince each $X_i$ is standardized to have unit variance, $\\operatorname{Var}(X_i) = 1$. The covariance is related to the correlation $\\rho_{ij}$ by $\\operatorname{Cov}(X_i, X_j) = \\rho_{ij} \\sqrt{\\operatorname{Var}(X_i)\\operatorname{Var}(X_j)}$. Given unit variances, we have $\\operatorname{Cov}(X_i, X_j) = \\rho_{ij}$.\nSubstituting this into the expression for the variance of the mean:\n$$\n\\operatorname{Var}(\\bar{X}) = \\frac{1}{N^2} \\sum_{i=1}^{N} \\sum_{j=1}^{N} \\rho_{ij}\n$$\nThe problem defines the effective sample size $N_{\\text{eff}}$ by equating this variance to the variance of the mean of $N_{\\text{eff}}$ independent, unit-variance draws. For a sample of size $N_{\\text{eff}}$ of i.i.d. variables $Y_k$ with $\\operatorname{Var}(Y_k)=1$, the variance of their mean $\\bar{Y}$ is $\\operatorname{Var}(\\bar{Y}) = \\frac{1}{N_{\\text{eff}}}$.\nEquating the two expressions for the variance of the mean:\n$$\n\\frac{1}{N_{\\text{eff}}} = \\frac{1}{N^2} \\sum_{i=1}^{N} \\sum_{j=1}^{N} \\rho_{ij}\n$$\nSolving for $N_{\\text{eff}}$ yields the general closed-form expression:\n$$\nN_{\\text{eff}} = \\frac{N^2}{\\sum_{i=1}^{N} \\sum_{j=1}^{N} \\rho_{ij}}\n$$\nThis expression relates the effective sample size to the nominal sample size $N$ and the sum of all elements in the correlation matrix $\\boldsymbol{R}$.\n\n### Calculation for the Given Ensemble\nWe are given $N=6$ and the correlation matrix $\\boldsymbol{R}$. The first step is to compute the sum of all its elements, $\\sum_{i=1}^{6} \\sum_{j=1}^{6} \\rho_{ij}$.\nThe matrix has a block structure. Let's sum the elements in each block.\nThe top-left $3 \\times 3$ block corresponding to family $A$ has $3$ diagonal elements equal to $1$ and $3 \\times 2 = 6$ off-diagonal elements equal to $0.8$. The sum of its elements is:\n$$\n\\text{Sum}_A = 3 \\times 1 + 6 \\times 0.8 = 3 + 4.8 = 7.8\n$$\nThe bottom-right $3 \\times 3$ block for family $B$ is identical in structure and sum:\n$$\n\\text{Sum}_B = 3 \\times 1 + 6 \\times 0.8 = 3 + 4.8 = 7.8\n$$\nThe top-right $3 \\times 3$ off-diagonal block represents the correlations between family $A$ and family $B$. All $3 \\times 3 = 9$ elements are equal to $0.3$. The sum of its elements is:\n$$\n\\text{Sum}_{AB} = 9 \\times 0.3 = 2.7\n$$\nThe bottom-left block is the transpose and has the same sum:\n$$\n\\text{Sum}_{BA} = 9 \\times 0.3 = 2.7\n$$\nThe total sum of all elements in $\\boldsymbol{R}$ is the sum of the sums of these four blocks:\n$$\n\\sum_{i=1}^{6} \\sum_{j=1}^{6} \\rho_{ij} = \\text{Sum}_A + \\text{Sum}_B + \\text{Sum}_{AB} + \\text{Sum}_{BA} = 7.8 + 7.8 + 2.7 + 2.7 = 15.6 + 5.4 = 21.0\n$$\nNow we can compute $N_{\\text{eff}}$ using the derived formula with $N=6$:\n$$\nN_{\\text{eff}} = \\frac{N^2}{\\sum_{i,j} \\rho_{ij}} = \\frac{6^2}{21.0} = \\frac{36}{21} = \\frac{12}{7}\n$$\nTo obtain the numerical value rounded to four significant figures:\n$$\nN_{\\text{eff}} = \\frac{12}{7} \\approx 1.7142857... \\approx 1.714\n$$\n\n### Interpretation\nThe actual size of the model ensemble is $N=6$. The calculated effective sample size is $N_{\\text{eff}} \\approx 1.714$. The fact that $N_{\\text{eff}} < N$ is a direct consequence of the positive correlations among the model outputs. The interpretation is that, in terms of reducing the variance of the ensemble mean, this ensemble of $6$ correlated models provides the same amount of information as a much smaller hypothetical ensemble of approximately $1.714$ independent models.\nThis substantial reduction from $6$ to $1.714$ highlights the strong redundancy within the ensemble, driven by the high intra-family correlations ($\\rho=0.8$) and the non-zero inter-family correlation ($\\rho=0.3$). In the context of model intercomparison studies like CMIP, this result demonstrates that simply increasing the number of models in an ensemble does not linearly increase the confidence in the ensemble mean if the models are not independent. Shared development history, code, and physical parameterizations lead to correlated errors and biases, which the concept of effective sample size quantifies. It underscores the importance of model diversity over mere model numerosity for robust ensemble forecasting and uncertainty quantification.",
            "answer": "$$\n\\boxed{1.714}\n$$"
        },
        {
            "introduction": "The differences, or \"spread,\" among models in an ensemble arise from multiple sources. A key task in model intercomparison is to attribute this uncertainty to its underlying causes. This practice applies a hierarchical Analysis of Variance (ANOVA) framework to decompose the total variance in model outputs into contributions from model structure, parameter choices, and internal variability. This powerful diagnostic approach allows modelers to move beyond simply noting that models disagree and instead quantify *why* they disagree, providing critical insights for future model development and for understanding the limits of predictability. ",
            "id": "3895089",
            "problem": "In a model intercomparison study for annual mean near-surface air temperature bias, each structural model (distinct dynamical core and physics suite) is paired with multiple calibration parameter sets, and each parameter set is run as an ensemble over multiple initial conditions. Let $Y_{ijk}$ denote the bias (in kelvin) from structure (model) $i \\in \\{1,\\dots,a\\}$, parameter set $j \\in \\{1,\\dots,b\\}$ nested in structure $i$, and ensemble member $k \\in \\{1,\\dots,r\\}$. Assume the hierarchical linear mixed-effects model\n$$\nY_{ijk} \\;=\\; \\mu \\;+\\; S_{i} \\;+\\; P_{j(i)} \\;+\\; \\varepsilon_{k(ij)},\n$$\nwith independent random effects $S_{i} \\sim \\mathcal{N}(0,\\sigma_{\\text{struct}}^{2})$, $P_{j(i)} \\sim \\mathcal{N}(0,\\sigma_{\\text{param}}^{2})$, and $\\varepsilon_{k(ij)} \\sim \\mathcal{N}(0,\\sigma_{\\varepsilon}^{2})$. The design is balanced with $a$ structures, each with $b$ parameter sets and $r$ ensemble members per parameter set. The Analysis of Variance (ANOVA) decomposition is nested: structures, then parameters within structures, then residual (internal variability across initial conditions).\n\nStarting only from the model definition, the additivity and independence of the random effects, and the definition of ANOVA sums of squares and mean squares for balanced nested designs, derive expressions for the expectations of the mean squares at each level and use the method of moments to obtain unbiased estimators for $\\sigma_{\\text{struct}}^{2}$ and $\\sigma_{\\text{param}}^{2}$ in terms of observed mean squares.\n\nThen, for a balanced experiment with $a=4$, $b=3$, and $r=5$, the observed ANOVA sums of squares are:\n- Between-structures sum of squares $SS_{\\text{struct}} = 67.5$ (kelvin squared, K$^{2}$),\n- Between-parameters-within-structure sum of squares $SS_{\\text{param}(\\text{struct})} = 36.0$ (K$^{2}$),\n- Residual (within-parameter-set) sum of squares $SS_{\\varepsilon} = 96.0$ (K$^{2}$).\n\nCompute the unbiased estimates $\\hat{\\sigma}_{\\text{struct}}^{2}$ and $\\hat{\\sigma}_{\\text{param}}^{2}$ using your derived expressions. Round each estimate to three significant figures. Express your final estimates in kelvin squared (K$^{2}$). Report your final answer as the ordered pair $\\left(\\hat{\\sigma}_{\\text{struct}}^{2}, \\hat{\\sigma}_{\\text{param}}^{2}\\right)$.",
            "solution": "**Part 1: Derivation of Estimators**\n\nThe specified hierarchical linear mixed-effects model is:\n$$\nY_{ijk} \\;=\\; \\mu \\;+\\; S_{i} \\;+\\; P_{j(i)} \\;+\\; \\varepsilon_{k(ij)}\n$$\nfor structure $i \\in \\{1,\\dots,a\\}$, parameter set $j \\in \\{1,\\dots,b\\}$ nested in structure $i$, and ensemble member $k \\in \\{1,\\dots,r\\}$. The random effects are independent with variances $\\sigma_{\\text{struct}}^{2}$, $\\sigma_{\\text{param}}^{2}$, and $\\sigma_{\\varepsilon}^{2}$.\n\nThe mean squares (MS) for a balanced nested ANOVA are defined as $MS = SS/df$, where $SS$ are the sums of squares and $df$ are the degrees of freedom. To derive the estimators, we find the expectation of each mean square (EMS). For a balanced nested design, these expectations are standard results:\n$$\n\\begin{aligned}\nE[MS_{\\text{struct}}] &= \\sigma_{\\varepsilon}^2 + r\\sigma_{\\text{param}}^2 + br\\sigma_{\\text{struct}}^2 \\\\\nE[MS_{\\text{param}(\\text{struct})}] &= \\sigma_{\\varepsilon}^2 + r\\sigma_{\\text{param}}^2 \\\\\nE[MS_{\\varepsilon}] &= \\sigma_{\\varepsilon}^2\n\\end{aligned}\n$$\nwhere $MS_{\\text{struct}}$, $MS_{\\text{param}(\\text{struct})}$, and $MS_{\\varepsilon}$ are the mean squares for structures, parameters-within-structures, and the residual error, respectively.\n\nUsing the method of moments, we set the observed mean squares equal to their expected values to obtain a system of equations for the variance component estimators ($\\hat{\\sigma}^2$):\n1) $MS_{\\varepsilon} = \\hat{\\sigma}_{\\varepsilon}^2$\n2) $MS_{\\text{param}(\\text{struct})} = \\hat{\\sigma}_{\\varepsilon}^2 + r\\hat{\\sigma}_{\\text{param}}^2$\n3) $MS_{\\text{struct}} = \\hat{\\sigma}_{\\varepsilon}^2 + r\\hat{\\sigma}_{\\text{param}}^2 + br\\hat{\\sigma}_{\\text{struct}}^2$\n\nSolving this system by back-substitution yields the unbiased estimators.\nFrom (2), substituting (1):\n$$\nMS_{\\text{param}(\\text{struct})} = MS_{\\varepsilon} + r\\hat{\\sigma}_{\\text{param}}^2 \\implies \\hat{\\sigma}_{\\text{param}}^2 = \\frac{MS_{\\text{param}(\\text{struct})} - MS_{\\varepsilon}}{r}\n$$\nFrom (3), substituting (2):\n$$\nMS_{\\text{struct}} = MS_{\\text{param}(\\text{struct})} + br\\hat{\\sigma}_{\\text{struct}}^2 \\implies \\hat{\\sigma}_{\\text{struct}}^2 = \\frac{MS_{\\text{struct}} - MS_{\\text{param}(\\text{struct})}}{br}\n$$\nThese are the required unbiased estimators.\n\n**Part 2: Numerical Calculation**\n\nWe are given the following values for a balanced experiment:\n- Number of structures: $a=4$\n- Number of parameter sets per structure: $b=3$\n- Number of ensemble members per parameter set: $r=5$\n- Sums of Squares: $SS_{\\text{struct}} = 67.5 \\, \\text{K}^2$, $SS_{\\text{param}(\\text{struct})} = 36.0 \\, \\text{K}^2$, $SS_{\\varepsilon} = 96.0 \\, \\text{K}^2$\n\nFirst, calculate the degrees of freedom:\n- $df_{\\text{struct}} = a - 1 = 4 - 1 = 3$\n- $df_{\\text{param}(\\text{struct})} = a(b - 1) = 4(3 - 1) = 8$\n- $df_{\\varepsilon} = ab(r - 1) = 4 \\times 3 \\times (5 - 1) = 48$\n\nNext, calculate the observed mean squares:\n- $MS_{\\text{struct}} = \\frac{SS_{\\text{struct}}}{df_{\\text{struct}}} = \\frac{67.5}{3} = 22.5 \\, \\text{K}^2$\n- $MS_{\\text{param}(\\text{struct})} = \\frac{SS_{\\text{param}(\\text{struct})}}{df_{\\text{param}(\\text{struct})}} = \\frac{36.0}{8} = 4.5 \\, \\text{K}^2$\n- $MS_{\\varepsilon} = \\frac{SS_{\\varepsilon}}{df_{\\varepsilon}} = \\frac{96.0}{48} = 2.0 \\, \\text{K}^2$\n\nNow, use the derived estimator formulas to compute the variance components:\n- For the structural variance, $\\sigma_{\\text{struct}}^{2}$:\n$$\n\\hat{\\sigma}_{\\text{struct}}^{2} = \\frac{MS_{\\text{struct}} - MS_{\\text{param}(\\text{struct})}}{br} = \\frac{22.5 - 4.5}{3 \\times 5} = \\frac{18.0}{15} = 1.2 \\, \\text{K}^2\n$$\n- For the parametric variance, $\\sigma_{\\text{param}}^{2}$:\n$$\n\\hat{\\sigma}_{\\text{param}}^{2} = \\frac{MS_{\\text{param}(\\text{struct})} - MS_{\\varepsilon}}{r} = \\frac{4.5 - 2.0}{5} = \\frac{2.5}{5} = 0.5 \\, \\text{K}^2\n$$\nRounding each estimate to three significant figures gives:\n- $\\hat{\\sigma}_{\\text{struct}}^{2} = 1.20 \\, \\text{K}^2$\n- $\\hat{\\sigma}_{\\text{param}}^{2} = 0.500 \\, \\text{K}^2$\n\nThe final answer is the ordered pair $(\\hat{\\sigma}_{\\text{struct}}^{2}, \\hat{\\sigma}_{\\text{param}}^{2})$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1.20 & 0.500\n\\end{pmatrix}\n}\n$$"
        }
    ]
}