{
    "hands_on_practices": [
        {
            "introduction": "In advanced data assimilation, the tangent linear model is a cornerstone for propagating uncertainty and for building the adjoint. Before it can be trusted, it must be rigorously verified against the original nonlinear model. This practice guides you through the analytical derivation of a Taylor remainder test, a fundamental procedure for confirming that your tangent linear code is a correct first-order approximation of the nonlinear dynamics .",
            "id": "3872862",
            "problem": "Consider the verification of a tangent linear operator in the context of data assimilation for environmental and earth system modeling. You are assimilating a satellite measurement of Top of Atmosphere (TOA) outgoing longwave radiation. The nonlinear observation operator maps a $2$-variable atmospheric state $x = (T, W)$, consisting of surface temperature $T$ and column-integrated water vapor $W$, to TOA longwave flux through absorption by water vapor and emission by the surface. Assume the operator is\n$$\nH(T, W) = \\epsilon\\,\\sigma\\,T^{4}\\,\\exp(-\\kappa\\,W),\n$$\nwhere $\\epsilon$ is surface emissivity, $\\sigma$ is the Stefan–Boltzmann constant, and $\\kappa$ is an effective absorption coefficient for water vapor. Treat $\\epsilon$, $\\sigma$, and $\\kappa$ as positive constants and assume $H$ is twice continuously differentiable.\n\nIn order to define and perform a Taylor remainder test for the tangent linear of the nonlinear observation operator, proceed as follows. Let $\\delta = (a, b)$ be a fixed direction in the state space with $a \\in \\mathbb{R}$ and $b \\in \\mathbb{R}$, and consider the scalar step size $h \\in \\mathbb{R}$. Define $R(h)$ by\n$$\nR(h) = \\frac{H\\big(x + h\\,\\delta\\big) - H(x) - H'(x)\\big(h\\,\\delta\\big)}{h^{2}},\n$$\nwhere $H'(x)$ denotes the Jacobian (tangent linear) of $H$ at $x$ acting on the perturbation $h\\,\\delta$.\n\nStarting from the basic definition of differentiability and the second-order Taylor expansion for multivariate functions, derive the analytic expression for the limit\n$$\nR = \\lim_{h \\to 0} R(h)\n$$\nin terms of $\\epsilon$, $\\sigma$, $\\kappa$, $T$, $W$, $a$, and $b$. Your derivation must be from first principles, beginning with the definition of the Jacobian and the Hessian and using the second-order Taylor theorem, without invoking any pre-derived shortcut formulas.\n\nExpress your final result as a closed-form analytic expression. The final expression has physical units of watts per square meter; you do not need to include units in the boxed answer. No numerical evaluation is required.",
            "solution": "Let the state vector be $x = (T, W)$ and the nonlinear operator be $H(x) = H(T, W)$. The problem asks for the evaluation of the limit:\n$$\nR = \\lim_{h \\to 0} R(h) = \\lim_{h \\to 0} \\frac{H\\big(x + h\\,\\delta\\big) - H(x) - H'(x)\\big(h\\,\\delta\\big)}{h^{2}}\n$$\nwhere $\\delta = (a, b)$ is a perturbation direction and $H'(x)$ is the Jacobian of $H$ at $x$.\n\nThe derivation proceeds from the second-order Taylor expansion of a multivariate function $H(x)$ around a point $x$. Given that $H$ is twice continuously differentiable, for a perturbation $\\Delta x$, Taylor's theorem states:\n$$\nH(x + \\Delta x) = H(x) + dH_x(\\Delta x) + \\frac{1}{2} d^{2}H_x(\\Delta x, \\Delta x) + o\\left(\\|\\Delta x\\|^{2}\\right)\n$$\nHere, $dH_x(\\Delta x)$ is the first differential (the action of the Jacobian on the perturbation) and $d^{2}H_x(\\Delta x, \\Delta x)$ is the second differential (a quadratic form involving the Hessian matrix).\n\nIn our specific case, the perturbation is $\\Delta x = h\\,\\delta$. The first differential is $dH_x(h\\,\\delta)$, which is precisely the term denoted as $H'(x)(h\\,\\delta)$ in the problem statement. This can be expressed using the Jacobian matrix $H'(x) = \\nabla H(x)$ as:\n$$\nH'(x)(h\\,\\delta) = h \\, \\left( \\nabla H(x) \\cdot \\delta \\right)\n$$\nThe second differential is a quadratic form involving the Hessian matrix $H''(x)$:\n$$\nd^{2}H_x(h\\,\\delta, h\\,\\delta) = (h\\,\\delta)^{T} H''(x) (h\\,\\delta) = h^{2} \\delta^{T} H''(x) \\delta\n$$\nSubstituting $\\Delta x = h\\,\\delta$ into the Taylor expansion gives:\n$$\nH(x + h\\,\\delta) = H(x) + H'(x)(h\\,\\delta) + \\frac{1}{2} h^{2} \\delta^{T} H''(x) \\delta + o(h^{2})\n$$\nThe little-o term $o\\left(\\|h\\,\\delta\\|^{2}\\right)$ simplifies to $o(h^{2})$ because $\\|\\delta\\|$ is a fixed constant.\n\nNow, we rearrange this equation to match the numerator in the definition of $R(h)$:\n$$\nH(x + h\\,\\delta) - H(x) - H'(x)(h\\,\\delta) = \\frac{1}{2} h^{2} \\delta^{T} H''(x) \\delta + o(h^{2})\n$$\nSubstituting this into the expression for $R(h)$:\n$$\nR(h) = \\frac{\\frac{1}{2} h^{2} \\delta^{T} H''(x) \\delta + o(h^{2})}{h^{2}} = \\frac{1}{2} \\delta^{T} H''(x) \\delta + \\frac{o(h^{2})}{h^{2}}\n$$\nTaking the limit as $h \\to 0$:\n$$\nR = \\lim_{h \\to 0} R(h) = \\lim_{h \\to 0} \\left( \\frac{1}{2} \\delta^{T} H''(x) \\delta + \\frac{o(h^{2})}{h^{2}} \\right) = \\frac{1}{2} \\delta^{T} H''(x) \\delta\n$$\nThis is because, by definition, $\\lim_{h \\to 0} \\frac{o(h^{2})}{h^{2}} = 0$.\n\nThe next step is to compute the Hessian matrix $H''(x)$ for the given function $H(T, W) = \\epsilon\\sigma T^{4}\\exp(-\\kappa W)$. The state vector is $x = (T, W)$.\n\nFirst, we find the first partial derivatives, which form the components of the Jacobian (gradient) $\\nabla H$:\n$$\n\\frac{\\partial H}{\\partial T} = \\frac{\\partial}{\\partial T} \\left( \\epsilon\\sigma T^{4}\\exp(-\\kappa W) \\right) = 4\\epsilon\\sigma T^{3}\\exp(-\\kappa W)\n$$\n$$\n\\frac{\\partial H}{\\partial W} = \\frac{\\partial}{\\partial W} \\left( \\epsilon\\sigma T^{4}\\exp(-\\kappa W) \\right) = \\epsilon\\sigma T^{4} \\left( -\\kappa\\exp(-\\kappa W) \\right) = -\\kappa\\epsilon\\sigma T^{4}\\exp(-\\kappa W)\n$$\nNext, we find the second partial derivatives, which are the components of the Hessian matrix $H''(x) = \\begin{pmatrix} \\frac{\\partial^2 H}{\\partial T^2} & \\frac{\\partial^2 H}{\\partial W \\partial T} \\\\ \\frac{\\partial^2 H}{\\partial T \\partial W} & \\frac{\\partial^2 H}{\\partial W^2} \\end{pmatrix}$:\n$$\n\\frac{\\partial^2 H}{\\partial T^2} = \\frac{\\partial}{\\partial T} \\left( 4\\epsilon\\sigma T^{3}\\exp(-\\kappa W) \\right) = 12\\epsilon\\sigma T^{2}\\exp(-\\kappa W)\n$$\n$$\n\\frac{\\partial^2 H}{\\partial W^2} = \\frac{\\partial}{\\partial W} \\left( -\\kappa\\epsilon\\sigma T^{4}\\exp(-\\kappa W) \\right) = -\\kappa\\epsilon\\sigma T^{4} \\left( -\\kappa\\exp(-\\kappa W) \\right) = \\kappa^{2}\\epsilon\\sigma T^{4}\\exp(-\\kappa W)\n$$\n$$\n\\frac{\\partial^2 H}{\\partial T \\partial W} = \\frac{\\partial}{\\partial T} \\left( -\\kappa\\epsilon\\sigma T^{4}\\exp(-\\kappa W) \\right) = -4\\kappa\\epsilon\\sigma T^{3}\\exp(-\\kappa W)\n$$\nSince $H$ is $C^2$, Clairaut's theorem guarantees that $\\frac{\\partial^2 H}{\\partial W \\partial T} = \\frac{\\partial^2 H}{\\partial T \\partial W}$.\n\nNow we evaluate the quadratic form $\\delta^{T} H''(x) \\delta$ with $\\delta = (a, b)$:\n$$\n\\delta^{T} H''(x) \\delta = \\begin{pmatrix} a & b \\end{pmatrix} \\begin{pmatrix} \\frac{\\partial^2 H}{\\partial T^2} & \\frac{\\partial^2 H}{\\partial T \\partial W} \\\\ \\frac{\\partial^2 H}{\\partial T \\partial W} & \\frac{\\partial^2 H}{\\partial W^2} \\end{pmatrix} \\begin{pmatrix} a \\\\ b \\end{pmatrix} = a^{2}\\frac{\\partial^2 H}{\\partial T^2} + 2ab\\frac{\\partial^2 H}{\\partial T \\partial W} + b^{2}\\frac{\\partial^2 H}{\\partial W^2}\n$$\nSubstituting the calculated second derivatives:\n\\begin{align*}\n\\delta^{T} H''(x) \\delta &= a^{2} \\left( 12\\epsilon\\sigma T^{2}\\exp(-\\kappa W) \\right) + 2ab \\left( -4\\kappa\\epsilon\\sigma T^{3}\\exp(-\\kappa W) \\right) + b^{2} \\left( \\kappa^{2}\\epsilon\\sigma T^{4}\\exp(-\\kappa W) \\right) \\\\\n&= \\epsilon\\sigma\\exp(-\\kappa W) \\left( 12a^{2}T^{2} - 8ab\\kappa T^{3} + b^{2}\\kappa^{2}T^{4} \\right)\n\\end{align*}\nFinally, we compute $R = \\frac{1}{2} \\delta^{T} H''(x) \\delta$:\n$$\nR = \\frac{1}{2} \\epsilon\\sigma\\exp(-\\kappa W) \\left( 12a^{2}T^{2} - 8ab\\kappa T^{3} + b^{2}\\kappa^{2}T^{4} \\right)\n$$\nDistributing the factor of $\\frac{1}{2}$ yields the final expression:\n$$\nR = \\epsilon\\sigma\\exp(-\\kappa W) \\left( 6a^{2}T^{2} - 4ab\\kappa T^{3} + \\frac{1}{2}b^{2}\\kappa^{2}T^{4} \\right)\n$$\nThis is the closed-form analytic expression for the limit $R$.",
            "answer": "$$\n\\boxed{\\epsilon\\sigma\\exp(-\\kappa W) \\left( 6a^{2}T^{2} - 4ab\\kappa T^{3} + \\frac{1}{2}b^{2}\\kappa^{2}T^{4} \\right)}\n$$"
        },
        {
            "introduction": "Variational data assimilation methods rely on the adjoint model to efficiently compute the gradient of a cost function, and its correctness is paramount. This practice delves into the 'dot-product test,' the definitive method for ensuring a coded adjoint operator is the true mathematical adjoint of its tangent linear counterpart . You will explore the nuances of this test, from the choice of inner product to the interpretation of its results in finite-precision arithmetic.",
            "id": "3872884",
            "problem": "In variational data assimilation for environmental and Earth system models governed by discretized Partial Differential Equations (PDEs), consider a discrete tangent linear operator $A$ that maps a state perturbation at one time to a state perturbation at a later time in a gridded model. Let the domain and codomain be finite-dimensional real vector spaces $X$ and $Y$, each equipped with a symmetric positive definite weight defining an inner product: for $u, v \\in X$, $\\langle u, v\\rangle_{X} = u^{\\top} W_{X} v$, and for $p, q \\in Y$, $\\langle p, q\\rangle_{Y} = p^{\\top} W_{Y} q$, where $W_{X}$ and $W_{Y}$ are symmetric positive definite matrices that represent, for example, cell volumes or mass matrices in a consistent discretization of an $L^{2}$ inner product. A code is available for the action of the tangent linear operator $A$ and a separate code claims to implement its adjoint $A^{\\ast}$ with respect to the given inner products. You will verify the adjoint implementation using the standard dot-product test by probing the operators with random vectors and comparing bilinear pairings.\n\nWhich of the following statements about how to correctly formulate, implement, and interpret such an adjoint verification test are valid? Select all that apply.\n\nA. If the inner products on $X$ and $Y$ are defined by symmetric positive definite matrices $W_{X}$ and $W_{Y}$, then the matrix representing the adjoint must satisfy $A^{\\ast} = W_{X}^{-1} A^{\\top} W_{Y}$, and therefore the verification must compute bilinear pairings using these same weights.\n\nB. In floating-point arithmetic with double precision machine epsilon $\\epsilon_{\\mathrm{mach}}$, a correctly implemented tangent linear and adjoint pair will yield a relative discrepancy of order $O(\\epsilon_{\\mathrm{mach}})$ when comparing the two bilinear pairings across a range of randomly sampled vectors, after appropriate scaling; significant discrepancies often indicate mismatched inner products or inconsistencies in boundary conditions or time discretization.\n\nC. For a time-dependent nonlinear model advanced by a sequence of discrete updates, the discrete adjoint compatible with the bilinear pairing must be integrated backward in time using the transposes (with respect to the chosen inner products) of the linearized update operators evaluated along the same base trajectory, implying that trajectory information must be available at the same sequence of times.\n\nD. The Euclidean inner product $\\langle u, v\\rangle = u^{\\top} v$ is always sufficient for an adjoint verification test, even when the state is represented in a non-orthonormal basis such as a finite element basis with a nontrivial mass matrix; using any other weight tests a nonphysical adjoint.\n\nE. It is sufficient for verification to test only vectors of the form $y = A x$ for randomly chosen $x$; if the bilinear pairings match to within a tight tolerance for all such tests, then the adjoint is guaranteed to be correct.\n\nF. Passing the dot-product test demonstrates consistency between the tangent linear and adjoint codes with respect to the same bilinear form, but it does not by itself certify that the tangent linear code is a correct linearization of the nonlinear model; separate Taylor remainder tests are required to assess linearization accuracy.\n\nG. Normalizing the test vectors $x$ and $y$ to unit norm guarantees that the dot-product discrepancy will be negligibly small independently of the conditioning of $A$, so normalization alone suffices to ensure a pass in practice.",
            "solution": "The problem statement describes the verification of an adjoint operator code `A*` against a tangent linear operator code `A` in the context of variational data assimilation for discretized PDEs. The correctness of the statements must be evaluated based on the definition of an adjoint operator in finite-dimensional inner product spaces.\n\nThe fundamental definition of the adjoint operator $A^{\\ast}: Y \\to X$ of a linear operator $A: X \\to Y$ with respect to the inner products $\\langle \\cdot, \\cdot \\rangle_{X}$ and $\\langle \\cdot, \\cdot \\rangle_{Y}$ is the unique linear operator that satisfies the following relation for all $x \\in X$ and $y \\in Y$:\n$$\n\\langle Ax, y \\rangle_{Y} = \\langle x, A^{\\ast}y \\rangle_{X}\n$$\nThe problem specifies the inner products in terms of symmetric positive definite (SPD) weight matrices $W_{X}$ and $W_{Y}$:\n$$\n\\langle u, v \\rangle_{X} = u^{\\top} W_{X} v\n$$\n$$\n\\langle p, q \\rangle_{Y} = p^{\\top} W_{Y} q\n$$\nSubstituting these definitions into the adjoint relation gives:\n$$\n(Ax)^{\\top} W_{Y} y = x^{\\top} W_{X} (A^{\\ast}y)\n$$\nUsing the property $(Bv)^{\\top} = v^{\\top}B^{\\top}$ for any matrix $B$ and vector $v$, we get:\n$$\nx^{\\top} A^{\\top} W_{Y} y = x^{\\top} (W_{X} A^{\\ast}) y\n$$\nSince this equality must hold for all vectors $x \\in X$ and $y \\in Y$, the matrices themselves must be equal:\n$$\nA^{\\top} W_{Y} = W_{X} A^{\\ast}\n$$\nBecause $W_{X}$ is SPD, its inverse $W_{X}^{-1}$ exists. We can solve for the matrix representation of the adjoint operator $A^{\\ast}$:\n$$\nA^{\\ast} = W_{X}^{-1} A^{\\top} W_{Y}\n$$\nThe \"dot-product test\" is a numerical verification of the fundamental adjoint relation $\\langle Ax, y \\rangle_{Y} = \\langle x, A^{\\ast}y \\rangle_{X}$ by computing both sides for randomly chosen vectors $x$ and $y$ and checking if they are equal to within floating-point precision.\n\nWith this foundation, we evaluate each statement.\n\n**A. If the inner products on $X$ and $Y$ are defined by symmetric positive definite matrices $W_{X}$ and $W_{Y}$, then the matrix representing the adjoint must satisfy $A^{\\ast} = W_{X}^{-1} A^{\\top} W_{Y}$, and therefore the verification must compute bilinear pairings using these same weights.**\nAs derived above, the matrix representation of the adjoint operator is indeed $A^{\\ast} = W_{X}^{-1} A^{\\top} W_{Y}$. The verification test, known as the dot-product test, is a direct implementation of the defining identity $\\langle Ax, y \\rangle_{Y} = \\langle x, A^{\\ast}y \\rangle_{X}$. This identity is only valid for the specific inner products with which the adjoint is defined. To perform the test, one must compute the left-hand side as $(Ax)^{\\top} W_{Y} y$ and the right-hand side as $x^{\\top} W_{X} (A^{\\ast}y)$ and compare the results. Therefore, the verification absolutely must use the weighting matrices $W_X$ and $W_Y$.\n**Verdict: Correct.**\n\n**B. In floating-point arithmetic with double precision machine epsilon $\\epsilon_{\\mathrm{mach}}$, a correctly implemented tangent linear and adjoint pair will yield a relative discrepancy of order $O(\\epsilon_{\\mathrm{mach}})$ when comparing the two bilinear pairings across a range of randomly sampled vectors, after appropriate scaling; significant discrepancies often indicate mismatched inner products or inconsistencies in boundary conditions or time discretization.**\nIn exact arithmetic, the two bilinear pairings are identical. In finite-precision floating-point arithmetic, the computed values will differ due to round-off error. For a well-conditioned problem and a numerically stable implementation, the accumulated error in computing the two scalar products, $s_1 = \\langle Ax, y \\rangle_{Y}$ and $s_2 = \\langle x, A^{\\ast}y \\rangle_{X}$, will be small. The relative difference, e.g., $|s_1 - s_2| / |s_1|$, is expected to be on the order of the machine epsilon, possibly multiplied by factors related to the dimensions of the vectors and condition numbers of the operators. A discrepancy significantly larger than this typically points to an error in the implementation. The statement correctly lists common and subtle sources of such errors: using the wrong inner product (e.g., identity matrices instead of $W_X, W_Y$), or errors in implementing complex components like boundary conditions, which must also be correctly transposed.\n**Verdict: Correct.**\n\n**C. For a time-dependent nonlinear model advanced by a sequence of discrete updates, the discrete adjoint compatible with the bilinear pairing must be integrated backward in time using the transposes (with respect to the chosen inner products) of the linearized update operators evaluated along the same base trajectory, implying that trajectory information must be available at the same sequence of times.**\nLet a nonlinear model evolve a state $x$ through a sequence of steps: $x_{k+1} = \\mathcal{M}_k(x_k)$. The tangent linear model propagates a perturbation $\\delta x$ via the chain rule: $\\delta x_N = (L_{N-1} \\circ \\dots \\circ L_1 \\circ L_0) \\delta x_0$, where $L_k = \\frac{\\partial \\mathcal{M}_k}{\\partial x_k}$ is the tangent linear operator for step $k$, evaluated on the base trajectory. The full tangent linear operator is $A = L_{N-1} \\dots L_1 L_0$. The adjoint of a composition of operators is the composition of the adjoints in reverse order: $A^{\\ast} = (L_{N-1} \\dots L_0)^{\\ast} = L_0^{\\ast} L_1^{\\ast} \\dots L_{N-1}^{\\ast}$. This reverse ordering means the adjoint model must be applied sequentially backwards in time. Each $L_k$ depends on the state $x_k$ from the forward \"base\" trajectory, and thus its adjoint $L_k^{\\ast}$ (the \"transpose with respect to the chosen inner products\") will also depend on $x_k$. Consequently, computing the full adjoint action requires access to the sequence of states from the forward model run.\n**Verdict: Correct.**\n\n**D. The Euclidean inner product $\\langle u, v\\rangle = u^{\\top} v$ is always sufficient for an adjoint verification test, even when the state is represented in a non-orthonormal basis such as a finite element basis with a nontrivial mass matrix; using any other weight tests a nonphysical adjoint.**\nThis statement is incorrect. The adjoint operator is defined *relative to* specific inner products on its domain and codomain. An adjoint code is written to be the adjoint with respect to a particular pair of inner products. Testing it with a different inner product (like the Euclidean one, where $W_X=I, W_Y=I$) is testing a different mathematical property. If the code was designed for non-identity weights $W_X, W_Y$, it will fail the test with Euclidean weights. Furthermore, the claim that non-Euclidean weights are \"nonphysical\" is false. For many discretizations of PDEs (e.g., Finite Element Method, Finite Volume Method on non-uniform grids), the discrete analogue of the canonical $L^2$ inner product, $\\int f(z)g(z) d\\Omega$, is a weighted inner product $u^{\\top}W v$, where $W$ is a mass matrix or a diagonal matrix of grid cell volumes. In these cases, it is the $L^2$-based inner product that carries the physical meaning (e.g., energy, mass), not the Euclidean inner product of the coefficient vectors.\n**Verdict: Incorrect.**\n\n**E. It is sufficient for verification to test only vectors of the form $y = A x$ for randomly chosen $x$; if the bilinear pairings match to within a tight tolerance for all such tests, then the adjoint is guaranteed to be correct.**\nThe adjoint identity must hold for *all* vectors $y$ in the codomain $Y$. Restricting the test vectors $y$ to the range of $A$, i.e., $y \\in \\mathrm{Ran}(A)$, is insufficient. This test checks the identity $\\langle Ax, Ax' \\rangle_Y = \\langle x, A^{\\ast}(Ax') \\rangle_X$. This verifies the action of the composite operator $A^{\\ast}A$ but provides no information about the action of $A^{\\ast}$ on vectors outside the range of $A$. Specifically, if $Y = \\mathrm{Ran}(A) \\oplus (\\mathrm{Ran}(A))^{\\perp}$, this test fails to check how $A^{\\ast}$ acts on the subspace $(\\mathrm{Ran}(A))^{\\perp}$. The fundamental theorem of linear algebra states that $(\\mathrm{Ran}(A))^{\\perp} = \\mathrm{Ker}(A^{\\ast})$. An incorrect implementation might map a vector $y \\in \\mathrm{Ker}(A^{\\ast})$ to a nonzero vector, but this error would never be detected by the proposed restricted test, as such a $y$ would not be selected. To ensure correctness, $y$ must be chosen randomly from the entire space $Y$.\n**Verdict: Incorrect.**\n\n**F. Passing the dot-product test demonstrates consistency between the tangent linear and adjoint codes with respect to the same bilinear form, but it does not by itself certify that the tangent linear code is a correct linearization of the nonlinear model; separate Taylor remainder tests are required to assess linearization accuracy.**\nThis statement correctly distinguishes between two different, essential verification steps. The dot-product test is a self-consistency check for a pair of linear codes purporting to be an operator and its adjoint. It verifies the mathematical relationship $\\langle Ax, y \\rangle_Y = \\langle x, A^{\\ast}y \\rangle_X$. It presumes the code for $A$ is given, and verifies the code for $A^{\\ast}$. It provides no information as to whether the operator $A$ itself is a correct representation of the derivative (linearization) of the underlying nonlinear model, $\\mathcal{M}$. To verify the tangent linear code $A$ itself, one must perform a Taylor test, which checks if the remainder term $\\mathcal{R}(\\delta x) = \\mathcal{M}(x_0 + \\delta x) - \\mathcal{M}(x_0) - A(x_0)\\delta x$ satisfies $\\|\\mathcal{R}(\\delta x)\\| = O(\\|\\delta x\\|^2)$.\n**Verdict: Correct.**\n\n**G. Normalizing the test vectors $x$ and $y$ to unit norm guarantees that the dot-product discrepancy will be negligibly small independently of the conditioning of $A$, so normalization alone suffices to ensure a pass in practice.**\nThis statement is false. While normalizing the input vectors $x$ and $y$ (with respect to the correct norms, i.e., $\\|x\\|_X=1$ and $\\|y\\|_Y=1$) is good practice for standardizing the test, it does not eliminate the effect of the operator's conditioning. The condition number of an operator (or matrix) characterizes the amplification of input errors to output errors during numerical computation. If $A$ is ill-conditioned, the computation of $Ax$ can suffer from a significant loss of relative precision, regardless of the norm of $x$. This increased numerical error in the computed value of $Ax$ will then propagate into the bilinear pairing $\\langle Ax, y \\rangle_Y$, potentially leading to a large discrepancy when compared to $\\langle x, A^{\\ast}y \\rangle_X$. The discrepancy is therefore not independent of the conditioning of $A$.\n**Verdict: Incorrect.**",
            "answer": "$$\\boxed{ABCF}$$"
        },
        {
            "introduction": "Once the core operators of a data assimilation system are verified, the focus shifts to statistical tuning, where performance hinges on realistic specifications for the background ($B$) and observation ($R$) error covariances. This practice introduces a powerful diagnostic method that uses innovation statistics—specifically the covariances of observation-minus-forecast (O–F) and observation-minus-analysis (O–A) residuals—to assess the consistency of $B$ and $R$ . Understanding these relationships is essential for tuning and monitoring any operational data assimilation system.",
            "id": "3872861",
            "problem": "In a linearized data assimilation setting for atmospheric state estimation, consider a state vector $x \\in \\mathbb{R}^n$ and an observation vector $y \\in \\mathbb{R}^p$ related by a linear observation operator $H \\in \\mathbb{R}^{p \\times n}$ as $y = H x_t + v$, where $x_t$ is the (unknown) true state and $v$ is the observation error with zero mean and covariance $R \\in \\mathbb{R}^{p \\times p}$. The background (prior) state is $x_b = x_t + b$, where $b$ is the background error with zero mean and covariance $B \\in \\mathbb{R}^{n \\times n}$. Assume $b$ and $v$ are uncorrelated, that is, $\\mathbb{E}[b] = 0$, $\\mathbb{E}[v] = 0$, and $\\mathbb{E}[b v^\\top] = 0$. Define the analysis as $x_a = x_b + K \\left(y - H x_b\\right)$, where $K \\in \\mathbb{R}^{n \\times p}$ is the gain used by the system.\n\nDefine the observation-minus-forecast (O–F, also called the innovation) as $d \\equiv y - H x_b$ and the observation-minus-analysis (O–A, also called the analysis residual in observation space) as $e \\equiv y - H x_a$.\n\nFrom first principles and under the stated assumptions:\n- Derive the expressions for the second-order moments $\\mathbb{E}[d d^\\top]$, $\\mathbb{E}[e d^\\top]$, and $\\mathbb{E}[e e^\\top]$ in terms of $H$, $B$, $R$, and $K$.\n- Then, specialize to the case where $K$ is the optimal Kalman gain for this linear-Gaussian setting. Use this to state what each of these moments reduces to and explain how these identities can be used in practice to diagnose misspecification of the observation error covariance $R$ and the background error covariance $B$.\n\nSelect the option that is fully consistent with the above derivations and provides a correct diagnostic interpretation:\n\nA. Under the assumptions stated and with the optimal Kalman gain, $\\mathbb{E}[d d^\\top] = H B H^\\top + R$ and $\\mathbb{E}[e d^\\top] = R$. Therefore, if sample estimates of the O–A/O–F cross-covariance depart from the specified $R$, this indicates misspecified $R$. Likewise, the difference between the O–F covariance and the specified $R$ estimates $H B H^\\top$, so a mismatch with $H B H^\\top$ computed from the specified $B$ indicates misspecified $B$.\n\nB. With any linear gain $K$, $\\mathbb{E}[e d^\\top] = H B H^\\top$, so comparing the O–A/O–F cross-covariance to $H B H^\\top$ diagnoses $R$ misspecification. The O–F covariance alone cannot diagnose $B$ because it does not depend on $B$.\n\nC. If $B$ and $R$ are correctly specified, then $\\mathbb{E}[e d^\\top] = 0$, so any nonzero sample O–A/O–F cross-covariance implies model bias. Meanwhile, $\\mathbb{E}[d d^\\top] = R$ regardless of $B$.\n\nD. The O–A covariance satisfies $\\mathbb{E}[e e^\\top] = R$ when $K$ is optimal, so matching the O–A covariance to the specified $R$ is the most robust way to calibrate $R$. The O–F covariance provides no information on $B$ because $H B H^\\top$ cancels out.\n\nE. Even for a nonlinear $H$, as long as a first-order linearization is used around the true state, one has $\\mathbb{E}[e d^\\top] = R$ regardless of the gain $K$. Therefore, the O–A/O–F cross-covariance always estimates $R$, and the O–F covariance always estimates $H B H^\\top + R$ without any requirement on $K$.",
            "solution": "The core task is to derive the second-order moments $\\mathbb{E}[d d^{\\top}]$, $\\mathbb{E}[e d^{\\top}]$, and $\\mathbb{E}[e e^{\\top}]$ and interpret them in the context of diagnostics, particularly when the gain $K$ is optimal.\n\nLet us first express the innovation $d$ and the analysis residual $e$ in terms of the fundamental error quantities, the background error $b$ and the observation error $v$.\n\nThe innovation (or Observation-minus-Forecast, O–F) is defined as $d \\equiv y - H x_b$.\nSubstituting the given relations $y = H x_t + v$ and $x_b = x_t + b$:\n$$d = (H x_t + v) - H (x_t + b) = H x_t + v - H x_t - H b = v - H b$$\nThe problem states that the errors have zero mean, i.e., $\\mathbb{E}[b]=0$ and $\\mathbb{E}[v]=0$. The mean of the innovation is therefore:\n$$\\mathbb{E}[d] = \\mathbb{E}[v - H b] = \\mathbb{E}[v] - H \\mathbb{E}[b] = 0 - H \\cdot 0 = 0$$\n\nThe analysis residual (or Observation-minus-Analysis, O–A) is defined as $e \\equiv y - H x_a$.\nUsing the definition of the analysis state $x_a = x_b + K(y - H x_b)$, we can write:\n$$e = y - H (x_b + K(y - H x_b)) = (y - H x_b) - H K (y - H x_b)$$\nRecognizing that $d = y - H x_b$, this simplifies to:\n$$e = d - H K d = (I - H K) d$$\nwhere $I$ is the identity matrix of size $p \\times p$.\nThe mean of the analysis residual is also zero:\n$$\\mathbb{E}[e] = \\mathbb{E}[(I - H K)d] = (I - H K)\\mathbb{E}[d] = (I - H K) \\cdot 0 = 0$$\n\nNow, we derive the requested second-order moments.\n\n**Derivation of $\\mathbb{E}[d d^{\\top}]$**\nUsing the expression $d = v - H b$:\n$$\\mathbb{E}[d d^{\\top}] = \\mathbb{E}[(v - H b)(v - H b)^{\\top}] = \\mathbb{E}[(v - H b)(v^{\\top} - b^{\\top} H^{\\top})]$$\nExpanding the product:\n$$\\mathbb{E}[d d^{\\top}] = \\mathbb{E}[v v^{\\top} - v b^{\\top} H^{\\top} - H b v^{\\top} + H b b^{\\top} H^{\\top}]$$\nUsing the linearity of the expectation operator:\n$$\\mathbb{E}[d d^{\\top}] = \\mathbb{E}[v v^{\\top}] - \\mathbb{E}[v b^{\\top}] H^{\\top} - H \\mathbb{E}[b v^{\\top}] + H \\mathbb{E}[b b^{\\top}] H^{\\top}$$\nWe are given the definitions of the error covariances, $\\mathbb{E}[b b^{\\top}] = B$ and $\\mathbb{E}[v v^{\\top}] = R$. We are also given that the errors are uncorrelated, meaning $\\mathbb{E}[b v^{\\top}] = 0$. Since $\\mathbb{E}[v b^{\\top}] = (\\mathbb{E}[b v^{\\top}])^{\\top}$, we also have $\\mathbb{E}[v b^{\\top}] = 0$. Substituting these into the equation:\n$$\\mathbb{E}[d d^{\\top}] = R - 0 \\cdot H^{\\top} - H \\cdot 0 + H B H^{\\top}$$\n$$\\mathbb{E}[d d^{\\top}] = H B H^{\\top} + R$$\nThis expression for the innovation covariance holds for any linear gain $K$, as it does not depend on $K$.\n\n**Derivation of $\\mathbb{E}[e d^{\\top}]$**\nUsing the relationship $e = (I - H K) d$:\n$$\\mathbb{E}[e d^{\\top}] = \\mathbb{E}[((I - H K) d) d^{\\top}] = (I - H K) \\mathbb{E}[d d^{\\top}]$$\nSubstituting the result for $\\mathbb{E}[d d^{\\top}]$:\n$$\\mathbb{E}[e d^{\\top}] = (I - H K)(H B H^{\\top} + R)$$\nThis is the general expression for any gain $K$.\n\nNow, we specialize to the case where $K$ is the optimal Kalman gain, which minimizes the analysis error variance. The formula for the optimal gain is:\n$$K = B H^{\\top} (H B H^{\\top} + R)^{-1}$$\nSubstituting this optimal $K$ into the expression for $\\mathbb{E}[e d^{\\top}]$:\n$$\\mathbb{E}[e d^{\\top}] = (I - H [B H^{\\top} (H B H^{\\top} + R)^{-1}]) (H B H^{\\top} + R)$$\nDistributing the $(H B H^{\\top} + R)$ term:\n$$\\mathbb{E}[e d^{\\top}] = I(H B H^{\\top} + R) - H B H^{\\top} (H B H^{\\top} + R)^{-1} (H B H^{\\top} + R)$$\n$$\\mathbb{E}[e d^{\\top}] = (H B H^{\\top} + R) - H B H^{\\top} = R$$\nThus, for the optimal Kalman gain, we have the identity $\\mathbb{E}[e d^{\\top}] = R$.\n\n**Derivation of $\\mathbb{E}[e e^{\\top}]$**\nUsing the relationship $e = (I - H K) d$:\n$$\\mathbb{E}[e e^{\\top}] = \\mathbb{E}[((I - H K) d) ((I - H K) d)^{\\top}] = (I - H K) \\mathbb{E}[d d^{\\top}] (I - H K)^{\\top}$$\nSubstituting the expressions for $\\mathbb{E}[d d^{\\top}]$ and the optimal $K$:\n$$\\mathbb{E}[e e^{\\top}] = (I - H K) (H B H^{\\top} + R) (I - K^{\\top} H^{\\top})$$\nFrom the previous derivation, we know that for the optimal $K$, the term $(I - H K)(H B H^{\\top} + R)$ equals $R$. Therefore:\n$$\\mathbb{E}[e e^{\\top}] = R(I - K^{\\top} H^{\\top}) = R - R K^{\\top} H^{\\top}$$\nThis is generally not equal to $R$.\n\n**Diagnostic Interpretation**\nThe derived identities, assuming the system uses the theoretically optimal gain (i.e., the specified $B$ and $R$ are correct), are:\n1.  $\\mathbb{E}[d d^{\\top}] = H B H^{\\top} + R$\n2.  $\\mathbb{E}[e d^{\\top}] = R$\n\nIn practice, a data assimilation system uses specified covariance matrices, let's call them $B_{spec}$ and $R_{spec}$, to compute its gain $K$. We can then compute sample statistics (averages over many analysis cycles) of the innovation $d$ and residual $e$.\n\n-   **Diagnosing $R$:** The identity $\\mathbb{E}[e d^{\\top}] = R$ is a powerful diagnostic tool proposed by Desroziers et al. If the specified $B_{spec}$ and $R_{spec}$ are correct, then the computed gain is optimal, and the sample cross-covariance between O-A and O-F, denoted $\\hat{\\mathbb{E}}[e d^{\\top}]$, should be close to the specified $R_{spec}$. If $\\hat{\\mathbb{E}}[e d^{\\top}] \\neq R_{spec}$, it indicates that the optimality assumption is violated, implying that either $B_{spec}$ or $R_{spec}$ (or both) are misspecified. This check is particularly sensitive to $R_{spec}$.\n\n-   **Diagnosing $B$:** The identity $\\mathbb{E}[d d^{\\top}] = H B H^{\\top} + R$ holds for any gain. It can be rearranged as $\\mathbb{E}[d d^{\\top}] - R = H B H^{\\top}$. To diagnose $B$, one can compute the sample innovation covariance $\\hat{\\mathbb{E}}[d d^{\\top}]$. Assuming $R_{spec}$ is reasonably well-known (perhaps tuned using the first diagnostic), one can compare the quantity $\\hat{\\mathbb{E}}[d d^{\\top}] - R_{spec}$ to the theoretically expected value $H B_{spec} H^{\\top}$. A significant discrepancy suggests that $B_{spec}$ is misspecified.\n\nNow we evaluate the given options.\n\n**A. Under the assumptions stated and with the optimal Kalman gain, $\\mathbb{E}[d d^\\top] = H B H^\\top + R$ and $\\mathbb{E}[e d^\\top] = R$. Therefore, if sample estimates of the O–A/O–F cross-covariance depart from the specified $R$, this indicates misspecified $R$. Likewise, the difference between the O–F covariance and the specified $R$ estimates $H B H^\\top$, so a mismatch with $H B H^\\top$ computed from the specified $B$ indicates misspecified $B$.**\nThis option correctly states the two key identities, $\\mathbb{E}[d d^{\\top}] = H B H^{\\top} + R$ and $\\mathbb{E}[e d^{\\top}] = R$. It also correctly interprets their diagnostic use: the O-A/O-F cross-covariance $(\\mathbb{E}[e d^{\\top}])$ is used to check $R$, and the O-F covariance $(\\mathbb{E}[d d^{\\top}])$ is used in conjunction with $R$ to check $H B H^{\\top}$. This statement is fully consistent with our derivation.\n**Verdict: Correct**\n\n**B. With any linear gain $K$, $\\mathbb{E}[e d^\\top] = H B H^\\top$, so comparing the O–A/O–F cross-covariance to $H B H^\\top$ diagnoses $R$ misspecification. The O–F covariance alone cannot diagnose $B$ because it does not depend on $B$.**\nThe first claim, $\\mathbb{E}[e d^{\\top}] = H B H^{\\top}$ for any $K$, is false. We derived $\\mathbb{E}[e d^{\\top}] = (I - H K)(H B H^{\\top} + R)$. The second claim, that the O-F covariance does not depend on $B$, is also false. We derived $\\mathbb{E}[d d^{\\top}] = H B H^{\\top} + R$, which explicitly depends on $B$.\n**Verdict: Incorrect**\n\n**C. If $B$ and $R$ are correctly specified, then $\\mathbb{E}[e d^\\top] = 0$, so any nonzero sample O–A/O–F cross-covariance implies model bias. Meanwhile, $\\mathbb{E}[d d^\\top] = R$ regardless of $B$.**\nThe first claim, $\\mathbb{E}[e d^{\\top}] = 0$, is false. For an optimal system, $\\mathbb{E}[e d^{\\top}] = R$, which is non-zero in general. The second claim, $\\mathbb{E}[d d^{\\top}] = R$, is also false. The correct expression is $\\mathbb{E}[d d^{\\top}] = H B H^{\\top} + R$.\n**Verdict: Incorrect**\n\n**D. The O–A covariance satisfies $\\mathbb{E}[e e^\\top] = R$ when $K$ is optimal, so matching the O–A covariance to the specified $R$ is the most robust way to calibrate $R$. The O–F covariance provides no information on $B$ because $H B H^\\top$ cancels out.**\nThe first claim, $\\mathbb{E}[e e^{\\top}] = R$ for optimal $K$, is false. As derived, $\\mathbb{E}[e e^{\\top}] = R (I - K^{\\top} H^{\\top})$, which is not generally $R$. The second claim, that the O-F covariance provides no information on $B$, is false, as explained for option B.\n**Verdict: Incorrect**\n\n**E. Even for a nonlinear $H$, as long as a first-order linearization is used around the true state, one has $\\mathbb{E}[e d^\\top] = R$ regardless of the gain $K$. Therefore, the O–A/O–F cross-covariance always estimates $R$, and the O–F covariance always estimates $H B H^\\top + R$ without any requirement on $K$.**\nThe premise that the identities hold for a linearized non-linear $H$ is reasonable, as the problem is in a linearized setting. However, the claim that $\\mathbb{E}[e d^{\\top}] = R$ holds \"regardless of the gain $K$\" is false. This identity is a special property of the optimal gain. The O-F covariance part is correct in that $\\mathbb{E}[d d^{\\top}] = H B H^{\\top} + R$ does not depend on K, but the first part of the statement renders the entire option incorrect.\n**Verdict: Incorrect**\n\nOnly option A is fully correct in its statement of the mathematical identities and their practical diagnostic interpretation.",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}