{
    "hands_on_practices": [
        {
            "introduction": "The performance of a particle filter critically depends on the diversity of its weighted ensemble. A common failure mode is weight degeneracy, where a few particles dominate the posterior approximation. This practice  addresses the essential task of monitoring ensemble health by computing the Effective Sample Size ($ESS$) and implementing the numerically stable \"log-sum-exp\" technique to handle weights that span many orders of magnitude, a vital skill for any robust implementation.",
            "id": "3906073",
            "problem": "Consider a sequential importance sampling scenario used in environmental and earth system modeling, where a probability distribution over a latent environmental state is approximated by a weighted ensemble of particles. Let there be $N$ particles, indexed by $i \\in \\{1, \\dots, N\\}$, with unnormalized importance weights $\\tilde{w}_i \\geq 0$. The normalized weights $w_i$ satisfy $\\sum_{i=1}^{N} w_i = 1$ and $w_i \\geq 0$ for all $i$. In practical implementations, the values provided are often the logarithms of the unnormalized weights, denoted $\\ell_i = \\log \\tilde{w}_i$, because $\\tilde{w}_i$ may span many orders of magnitude. A central concern in particle filters is degeneracy, where only a few particles have substantial weight. An established measure of degeneracy is the effective sample size, which depends on the second-order moment of the normalized weights.\n\nStarting from core definitions of importance sampling and normalization, derive and implement numerically stable procedures to:\n- Transform a vector of $N$ log-weights $\\{\\ell_i\\}_{i=1}^N$ into normalized weights $\\{w_i\\}_{i=1}^N$ that sum to $1$ and avoid numerical underflow or overflow during computation.\n- Compute the effective sample size using a principled definition based on the normalized weights.\n\nYour implementation must handle extreme values of $\\ell_i$, including very large negative values and negative infinity, in a way consistent with the mathematical properties of the normalization and the effective sample size.\n\nYou will test your procedures using $N=1000$ particles under five scenarios that emulate different degrees of degeneracy and numerical stress. For reproducibility, each scenario specifies a pseudo-random generator seed and a rule for constructing the log-weights $\\{\\ell_i\\}_{i=1}^N$:\n\n- Test Case 1 (balanced variability): $N = 1000$. Use a pseudo-random generator with seed $s_1 = 42$, and let $\\ell_i$ be independently drawn from a normal distribution with mean $0$ and standard deviation $1$.\n- Test Case 2 (uniform weights): $N = 1000$. Set $\\ell_i = 0$ for all $i$.\n- Test Case 3 (near-total degeneracy): $N = 1000$. Set $\\ell_1 = 0$ and $\\ell_i = -100$ for all $i \\in \\{2, \\dots, 1000\\}$.\n- Test Case 4 (extremely small weights): $N = 1000$. Use a pseudo-random generator with seed $s_4 = 123$, and let $\\ell_i$ be independently drawn from a normal distribution with mean $-1000$ and standard deviation $10$.\n- Test Case 5 (structural zeros): $N = 1000$. Set $\\ell_i = 0$ for $i \\in \\{1, \\dots, 500\\}$ and $\\ell_i = -\\infty$ for $i \\in \\{501, \\dots, 1000\\}$, where $-\\infty$ denotes an exact negative infinity value.\n\nYour program must:\n1. For each test case, compute the normalized weights $\\{w_i\\}_{i=1}^N$ from the provided log-weights $\\{\\ell_i\\}_{i=1}^N$ using a numerically stable method that preserves $\\sum_{i=1}^{N} w_i = 1$.\n2. Compute the effective sample size for each test case using the normalized weights.\n\nThe outputs for each test case must be real numbers. Your program should produce a single line of output containing the five effective sample size values in a comma-separated list enclosed in square brackets, in the order of the test cases described above (i.e., $[\\text{ESS}_1,\\text{ESS}_2,\\text{ESS}_3,\\text{ESS}_4,\\text{ESS}_5]$). No physical units are involved, and no angles or percentages are required. The result type for each entry must be a float.",
            "solution": "The problem requires the derivation and implementation of numerically stable methods to compute normalized importance weights and the effective sample size (ESS) from a given set of log-weights. This is a foundational task in sequential importance sampling methods, such as particle filters, which are widely used in environmental and earth system modeling to approximate posterior distributions.\n\nThe solution is presented in two parts. First, we derive a numerically stable procedure for weight normalization. Second, we define the effective sample size and outline its computation.\n\n**1. Numerically Stable Normalization of Log-Weights**\n\nLet $\\{\\ell_i\\}_{i=1}^N$ be the vector of logarithms of the unnormalized importance weights, where $\\ell_i = \\log \\tilde{w}_i$ for $N$ particles. The unnormalized weights are therefore $\\tilde{w}_i = \\exp(\\ell_i)$.\n\nThe normalized weight for the $i$-th particle, $w_i$, is defined as its unnormalized weight divided by the sum of all unnormalized weights:\n$$\nw_i = \\frac{\\tilde{w}_i}{\\sum_{j=1}^{N} \\tilde{w}_j} = \\frac{\\exp(\\ell_i)}{\\sum_{j=1}^{N} \\exp(\\ell_j)}\n$$\nA naive computation of this expression is prone to severe numerical errors. If any $\\ell_j$ is a large positive number (e.g., $\\ell_j > 709$), $\\exp(\\ell_j)$ will exceed the maximum value representable by a standard $64$-bit float, causing an overflow. Conversely, if $\\ell_j$ is a large negative number (e.g., $\\ell_j < -709$), $\\exp(\\ell_j)$ will underflow to $0$, potentially leading to a loss of precision or division by zero if all weights underflow.\n\nTo circumvent this, we employ a standard numerical stabilization technique often called the \"log-sum-exp\" trick. Let $\\ell_{\\max} = \\max_{j=1}^N \\{\\ell_j\\}$. We can factor out $\\exp(\\ell_{\\max})$ from the expression for $w_i$:\n$$\nw_i = \\frac{\\exp(\\ell_i)}{\\exp(\\ell_{\\max}) \\sum_{j=1}^{N} \\frac{\\exp(\\ell_j)}{\\exp(\\ell_{\\max})}} = \\frac{\\exp(\\ell_i - \\ell_{\\max})}{\\sum_{j=1}^{N} \\exp(\\ell_j - \\ell_{\\max})}\n$$\nThis revised formula is numerically stable. The term in the exponent of the numerator, $\\ell_i - \\ell_{\\max}$, is always less than or equal to $0$, so $\\exp(\\ell_i - \\ell_{\\max})$ will evaluate to a value between $0$ and $1$, preventing overflow. Similarly, all terms in the summation in the denominator are also between $0$ and $1$. The term corresponding to the maximum log-weight, where $\\ell_j = \\ell_{\\max}$, will be $\\exp(0) = 1$, ensuring that the sum in the denominator is at least $1$ (unless all $\\ell_i$ are $-\\infty$) and thus preventing underflow of the sum.\n\nThis formulation also correctly handles log-weights of $-\\infty$. If $\\ell_i = -\\infty$, this physically corresponds to an unnormalized weight of $\\tilde{w}_i = 0$. In our stable formula, if $\\ell_i = -\\infty$ and $\\ell_{\\max}$ is finite, then $\\ell_i - \\ell_{\\max} = -\\infty$, and $\\exp(-\\infty)$ evaluates to $0$. The resulting normalized weight $w_i$ will be correctly computed as $0$. In the special case where all $\\ell_i = -\\infty$, then $\\ell_{\\max} = -\\infty$. This implies all particles have zero weight, the distribution is undefined, and the effective sample size is zero.\n\n**2. Effective Sample Size (ESS)**\n\nThe effective sample size, $ESS$, is a metric used to quantify the degeneracy of the particle set. A particle set is considered degenerate if a small number of particles have weights close to $1$ while the rest have weights close to $0$. An ideal, non-degenerate sample of size $N$ would have uniform weights, $w_i = 1/N$ for all $i$. The $ESS$ provides an estimate of the equivalent number of uniformly weighted particles that would have the same statistical variance as the current weighted particle set.\n\nThe problem statement specifies that the $ESS$ depends on the second-order moment of the normalized weights. The standard and principled definition, which we adopt here, is:\n$$\nESS = \\frac{1}{\\sum_{i=1}^{N} w_i^2}\n$$\nThis definition aligns with the qualitative understanding of degeneracy.\n- For a non-degenerate sample with uniform weights $w_i = 1/N$, the sum of squares is $\\sum_{i=1}^{N} (1/N)^2 = N \\cdot (1/N^2) = 1/N$. The ESS is therefore $ESS = 1 / (1/N) = N$, which is the maximum possible value.\n- For a maximally degenerate sample where one particle $k$ has $w_k = 1$ and all other particles have $w_j = 0$ for $j \\neq k$, the sum of squares is $\\sum_{i=1}^{N} w_i^2 = 1^2 = 1$. The ESS is therefore $ESS = 1/1 = 1$, the minimum possible value for a valid weight distribution.\n\nThe value of $ESS$ ranges from $1$ to $N$, providing a continuous measure of sample quality. A low $ESS$ relative to $N$ is an indicator that resampling is necessary in a particle filter algorithm.\n\n**Summary of the Complete Algorithm**\n\nThe combined procedure to compute the $ESS$ from a vector of log-weights $\\{\\ell_i\\}_{i=1}^N$ is as follows:\n1. Given the input vector of log-weights $\\boldsymbol{\\ell} = [\\ell_1, \\dots, \\ell_N]$.\n2. Find the maximum log-weight: $\\ell_{\\max} = \\max_{i} \\{\\ell_i\\}$.\n3. If $\\ell_{\\max} = -\\infty$ (i.e., all log-weights are $-\\infty$), the effective sample size is $0$.\n4. Otherwise, compute shifted log-weights: $\\boldsymbol{\\ell'} = \\boldsymbol{\\ell} - \\ell_{\\max}$.\n5. Exponentiate the shifted log-weights to get intermediate values: $u_i = \\exp(\\ell'_i)$.\n6. Sum these values to get the normalization constant: $S = \\sum_{i=1}^{N} u_i$.\n7. Compute the normalized weights: $w_i = u_i / S$.\n8. Compute the sum of the squares of the normalized weights: $V = \\sum_{i=1}^{N} w_i^2$.\n9. The effective sample size is the reciprocal of this sum: $ESS = 1/V$.\nThis algorithm is numerically robust and handles the extreme values specified in the problem statement correctly.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by generating test cases, computing the effective\n    sample size for each, and printing the results in the specified format.\n    \"\"\"\n\n    def compute_ess_from_log_weights(log_weights: np.ndarray) -> float:\n        \"\"\"\n        Computes the effective sample size (ESS) from a vector of log-weights\n        using a numerically stable method.\n\n        Args:\n            log_weights: A 1D NumPy array of log-weights.\n\n        Returns:\n            The effective sample size as a float.\n        \"\"\"\n        # Find the maximum log-weight. np.max correctly handles -np.inf.\n        l_max = np.max(log_weights)\n\n        # If all log-weights are -inf, the maximum will be -inf.\n        # This implies all weights are zero, so the effective sample size is 0.\n        if l_max == -np.inf:\n            return 0.0\n\n        # Shift the log-weights by subtracting the maximum value.\n        # This prevents overflow/underflow in the exp calculation (log-sum-exp trick).\n        # log_weights - l_max results in values <= 0.\n        shifted_logs = log_weights - l_max\n\n        # Exponentiate the shifted log-weights.\n        # np.exp(-np.inf) correctly evaluates to 0.0.\n        unnormalized_weights = np.exp(shifted_logs)\n\n        # Calculate the sum for normalization.\n        sum_weights = np.sum(unnormalized_weights)\n\n        # Normalize the weights.\n        # This check is for the unlikely case that sum_weights is zero\n        # despite l_max not being -inf (e.g., due to catastrophic cancellation,\n        # though unlikely here).\n        if sum_weights == 0:\n            return 0.0\n            \n        normalized_weights = unnormalized_weights / sum_weights\n\n        # Compute the sum of the squares of the normalized weights.\n        # This is the second-order moment.\n        sum_sq_weights = np.sum(normalized_weights**2)\n        \n        # The sum of squared weights for a valid probability distribution cannot be zero.\n        # A check for zero is good practice to prevent division by zero errors.\n        if sum_sq_weights == 0:\n            return 0.0 # Should not be reached with valid normalized_weights\n\n        # The effective sample size is the reciprocal of the sum of squared weights.\n        ess = 1.0 / sum_sq_weights\n        \n        return ess\n\n    # --- Test Case Generation and Execution ---\n\n    N = 1000\n    test_cases = [\n        # Test Case 1: Balanced variability\n        {\"name\": \"balanced\", \"seed\": 42, \"type\": \"normal\", \"params\": {\"loc\": 0, \"scale\": 1}},\n        # Test Case 2: Uniform weights\n        {\"name\": \"uniform\", \"type\": \"constant\", \"value\": 0.0},\n        # Test Case 3: Near-total degeneracy\n        {\"name\": \"degenerate\", \"type\": \"specific\"},\n        # Test Case 4: Extremely small weights\n        {\"name\": \"small\", \"seed\": 123, \"type\": \"normal\", \"params\": {\"loc\": -1000, \"scale\": 10}},\n        # Test Case 5: Structural zeros\n        {\"name\": \"zeros\", \"type\": \"structural\"}\n    ]\n\n    results = []\n    for case in test_cases:\n        log_weights = np.zeros(N, dtype=float)\n        \n        if case[\"type\"] == \"normal\":\n            rng = np.random.default_rng(seed=case[\"seed\"])\n            log_weights = rng.normal(loc=case[\"params\"][\"loc\"], scale=case[\"params\"][\"scale\"], size=N)\n        elif case[\"type\"] == \"constant\":\n            log_weights = np.full(N, case[\"value\"])\n        elif case[\"type\"] == \"specific\": # Case 3\n            log_weights = np.full(N, -100.0)\n            log_weights[0] = 0.0\n        elif case[\"type\"] == \"structural\": # Case 5\n            # These indices correspond to mathematical i in {1, ..., 500}\n            log_weights[0:500] = 0.0\n            # These indices correspond to mathematical i in {501, ..., 1000}\n            log_weights[500:1000] = -np.inf\n            \n        ess = compute_ess_from_log_weights(log_weights)\n        results.append(ess)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.10f}' for r in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "When the Effective Sample Size reveals significant particle degeneracy, resampling is necessary to rejuvenate the ensemble. This procedure discards particles with low weights and duplicates those with high weights, focusing the filter on more probable regions of the state space. This exercise  provides a step-by-step application of stratified resampling, a low-variance scheme that is often superior to simpler methods, and solidifies understanding by guiding you through a proof of its unbiasedness property.",
            "id": "3906053",
            "problem": "A data assimilation step in a Sequential Monte Carlo (SMC) particle filter for a one-dimensional greenhouse gas tracer uses stratified resampling to combat weight degeneracy. You have $N=8$ particles with normalized importance weights $\\tilde{w}_{1:8}$ given by\n$$\n\\tilde{w}_{1:8} = \\big(0.05,\\; 0.10,\\; 0.20,\\; 0.15,\\; 0.25,\\; 0.05,\\; 0.10,\\; 0.10\\big),\n$$\nbased on a single satellite retrieval at the current time step. Let the cumulative sums be $c_{j} = \\sum_{k=1}^{j} \\tilde{w}_{k}$ for $j=1,\\dots,8$, and define $c_{0}=0$. In stratified resampling, the resampled ancestor indices are defined by $a_{i} = \\min\\{j : c_{j} \\geq U_{i}\\}$ for $i=1,\\dots,N$, where $U_{i}$ are independent draws uniformly distributed over the $N$ equal-length strata on $[0,1]$:\n$$\nU_{i} = \\frac{i-1 + \\epsilon_{i}}{N}, \\quad \\epsilon_{i} \\sim \\text{Uniform}(0,1], \\quad \\text{independent for } i=1,\\dots,N.\n$$\nFor this realization, suppose the random offsets are\n$$\n\\epsilon_{1:8} = \\big(0.12,\\; 0.87,\\; 0.44,\\; 0.03,\\; 0.59,\\; 0.77,\\; 0.21,\\; 0.66\\big).\n$$\nTask 1. Compute the resampled ancestor index vector $a_{1:8}$ under stratified resampling for the given $\\tilde{w}_{1:8}$ and $\\epsilon_{1:8}$. Report your answer as an ordered row of indices using the convention that indices start at $1$.\n\nTask 2. Starting from the definition of stratified resampling and properties of the Uniform distribution on an interval, derive from first principles the unbiasedness property that the expected resampling counts satisfy\n$$\n\\mathbb{E}[N_{j}] = N \\tilde{w}_{j}, \\quad j=1,\\dots,8,\n$$\nwhere $N_{j} = \\sum_{i=1}^{N} \\mathbf{1}\\{a_{i}=j\\}$ is the number of times particle $j$ is selected. Your derivation must rely only on the definitions given and standard facts about lengths of intervals under Uniform distributions.\n\nAnswer specification: Provide the ancestor index row for Task 1 as the final answer. No rounding is required. Do not include any units. The derivation for Task 2 should appear in your solution but does not need to be reflected in the final answer.",
            "solution": "The problem is well-posed, scientifically grounded, and provides all necessary information for a unique solution. The tasks involve a direct application of the stratified resampling algorithm and a standard derivation of its unbiasedness property. We will address each task in sequence.\n\n### Task 1: Computation of the Ancestor Index Vector\n\nThe first task is to compute the resampled ancestor index vector, $a_{1:8}$, for a particle filter with $N=8$ particles.\n\nThe given normalized importance weights are:\n$$\n\\tilde{w}_{1:8} = \\big(0.05,\\; 0.10,\\; 0.20,\\; 0.15,\\; 0.25,\\; 0.05,\\; 0.10,\\; 0.10\\big)\n$$\nFirst, we compute the cumulative sums $c_j = \\sum_{k=1}^{j} \\tilde{w}_k$, with $c_0 = 0$.\n\\begin{align*}\nc_0 &= 0 \\\\\nc_1 &= 0.05 \\\\\nc_2 &= 0.05 + 0.10 = 0.15 \\\\\nc_3 &= 0.15 + 0.20 = 0.35 \\\\\nc_4 &= 0.35 + 0.15 = 0.50 \\\\\nc_5 &= 0.50 + 0.25 = 0.75 \\\\\nc_6 &= 0.75 + 0.05 = 0.80 \\\\\nc_7 &= 0.80 + 0.10 = 0.90 \\\\\nc_8 &= 0.90 + 0.10 = 1.00\n\\end{align*}\nThese cumulative sums define the boundaries of the intervals on $[0,1]$ that correspond to each particle index.\n\nNext, we calculate the stratified random numbers $U_i$ for $i=1, \\dots, 8$. The formula is $U_i = \\frac{i-1 + \\epsilon_i}{N}$, with $N=8$ and the given random offsets $\\epsilon_{1:8} = \\big(0.12,\\; 0.87,\\; 0.44,\\; 0.03,\\; 0.59,\\; 0.77,\\; 0.21,\\; 0.66\\big)$.\n\\begin{align*}\nU_1 &= \\frac{1-1 + 0.12}{8} = \\frac{0.12}{8} = 0.015 \\\\\nU_2 &= \\frac{2-1 + 0.87}{8} = \\frac{1.87}{8} = 0.23375 \\\\\nU_3 &= \\frac{3-1 + 0.44}{8} = \\frac{2.44}{8} = 0.305 \\\\\nU_4 &= \\frac{4-1 + 0.03}{8} = \\frac{3.03}{8} = 0.37875 \\\\\nU_5 &= \\frac{5-1 + 0.59}{8} = \\frac{4.59}{8} = 0.57375 \\\\\nU_6 &= \\frac{6-1 + 0.77}{8} = \\frac{5.77}{8} = 0.72125 \\\\\nU_7 &= \\frac{7-1 + 0.21}{8} = \\frac{6.21}{8} = 0.77625 \\\\\nU_8 &= \\frac{8-1 + 0.66}{8} = \\frac{7.66}{8} = 0.9575\n\\end{align*}\n\nThe ancestor index $a_i$ is determined by finding the smallest index $j$ such that $c_j \\geq U_i$. This is equivalent to finding which interval $(c_{j-1}, c_j]$ contains $U_i$.\n\\begin{itemize}\n    \\item For $U_1 = 0.015$: $c_0=0 < 0.015 \\leq c_1=0.05$, so $a_1=1$.\n    \\item For $U_2 = 0.23375$: $c_2=0.15 < 0.23375 \\leq c_3=0.35$, so $a_2=3$.\n    \\item For $U_3 = 0.305$: $c_2=0.15 < 0.305 \\leq c_3=0.35$, so $a_3=3$.\n    \\item For $U_4 = 0.37875$: $c_3=0.35 < 0.37875 \\leq c_4=0.50$, so $a_4=4$.\n    \\item For $U_5 = 0.57375$: $c_4=0.50 < 0.57375 \\leq c_5=0.75$, so $a_5=5$.\n    \\item For $U_6 = 0.72125$: $c_4=0.50 < 0.72125 \\leq c_5=0.75$, so $a_6=5$.\n    \\item For $U_7 = 0.77625$: $c_5=0.75 < 0.77625 \\leq c_6=0.80$, so $a_7=6$.\n    \\item For $U_8 = 0.9575$: $c_7=0.90 < 0.9575 \\leq c_8=1.00$, so $a_8=8$.\n\\end{itemize}\nThe resulting ancestor index vector is $a_{1:8} = (1, 3, 3, 4, 5, 5, 6, 8)$.\n\n### Task 2: Derivation of the Unbiasedness Property\n\nThe second task is to derive the unbiasedness property of stratified resampling, which states that the expected number of times particle $j$ is selected, $\\mathbb{E}[N_j]$, is equal to $N \\tilde{w}_j$.\n\nThe number of times particle $j$ is selected, $N_j$, is given by the sum of indicator functions:\n$$\nN_j = \\sum_{i=1}^{N} \\mathbf{1}\\{a_i = j\\}\n$$\nwhere $\\mathbf{1}\\{E\\}$ is $1$ if event $E$ is true, and $0$ otherwise.\n\nBy the linearity of expectation, the expected value of $N_j$ is:\n$$\n\\mathbb{E}[N_j] = \\mathbb{E}\\left[\\sum_{i=1}^{N} \\mathbf{1}\\{a_i = j\\}\\right] = \\sum_{i=1}^{N} \\mathbb{E}[\\mathbf{1}\\{a_i = j\\}]\n$$\nThe expectation of an indicator function is the probability of the event it indicates.\n$$\n\\mathbb{E}[\\mathbf{1}\\{a_i = j\\}] = P(a_i = j)\n$$\nThus, we have:\n$$\n\\mathbb{E}[N_j] = \\sum_{i=1}^{N} P(a_i = j)\n$$\nThe ancestor index $a_i$ is defined as $a_i = \\min\\{k : c_k \\geq U_i\\}$. The condition $a_i = j$ is therefore equivalent to $U_i$ falling into the interval corresponding to particle $j$, which is $(c_{j-1}, c_j]$. So,\n$$\nP(a_i = j) = P(c_{j-1} < U_i \\leq c_j)\n$$\nThe random variable $U_i$ is defined as $U_i = \\frac{i-1+\\epsilon_i}{N}$, where $\\epsilon_i$ is drawn from a uniform distribution on $(0, 1]$. This implies that $i-1 < i-1+\\epsilon_i \\leq i$, and therefore $\\frac{i-1}{N} < U_i \\leq \\frac{i}{N}$. This means $U_i$ is a random variable uniformly distributed on the $i$-th stratum, $S_i = (\\frac{i-1}{N}, \\frac{i}{N}]$, which has length $\\frac{1}{N}$.\n\nThe probability $P(c_{j-1} < U_i \\leq c_j)$ is the length of the intersection of the event interval $E_j = (c_{j-1}, c_j]$ with the support of $U_i$, $S_i$, divided by the length of the support of $U_i$.\n$$\nP(a_i = j) = \\frac{\\text{length}(E_j \\cap S_i)}{\\text{length}(S_i)} = \\frac{\\text{length}\\left((c_{j-1}, c_j] \\cap \\left(\\frac{i-1}{N}, \\frac{i}{N}\\right]\\right)}{1/N} = N \\cdot \\text{length}\\left((c_{j-1}, c_j] \\cap \\left(\\frac{i-1}{N}, \\frac{i}{N}\\right]\\right)\n$$\nNow, we substitute this back into the expression for $\\mathbb{E}[N_j]$:\n$$\n\\mathbb{E}[N_j] = \\sum_{i=1}^{N} N \\cdot \\text{length}\\left((c_{j-1}, c_j] \\cap \\left(\\frac{i-1}{N}, \\frac{i}{N}\\right]\\right)\n$$\nWe can factor out $N$:\n$$\n\\mathbb{E}[N_j] = N \\sum_{i=1}^{N} \\text{length}\\left((c_{j-1}, c_j] \\cap \\left(\\frac{i-1}{N}, \\frac{i}{N}\\right]\\right)\n$$\nThe strata $S_i = (\\frac{i-1}{N}, \\frac{i}{N}]$ for $i=1, \\dots, N$ are disjoint and their union covers the entire interval $(0, 1]$. That is, $\\bigcup_{i=1}^{N} S_i = (0, 1]$ and $S_i \\cap S_k = \\emptyset$ for $i \\neq k$.\nBecause of this partitioning property, the sum of the lengths of the intersections is equal to the length of the intersection of $E_j$ with the entire union of strata:\n$$\n\\sum_{i=1}^{N} \\text{length}(E_j \\cap S_i) = \\text{length}\\left(E_j \\cap \\left(\\bigcup_{i=1}^{N} S_i\\right)\\right) = \\text{length}((c_{j-1}, c_j] \\cap (0, 1])\n$$\nSince the weights are normalized, we have $0 \\leq c_{j-1} < c_j \\leq 1$ for all $j=1,\\dots,N$. Thus, the intersection $(c_{j-1}, c_j] \\cap (0, 1]$ is simply the interval $(c_{j-1}, c_j]$ itself. The length of this interval is $c_j - c_{j-1}$.\nBy definition of the cumulative sums, $c_j - c_{j-1} = \\tilde{w}_j$.\nTherefore,\n$$\n\\sum_{i=1}^{N} \\text{length}(E_j \\cap S_i) = c_j - c_{j-1} = \\tilde{w}_j\n$$\nSubstituting this result back into the expression for $\\mathbb{E}[N_j]$ yields the final result:\n$$\n\\mathbb{E}[N_j] = N \\tilde{w}_j\n$$\nThis completes the derivation of the unbiasedness property of stratified resampling.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1 & 3 & 3 & 4 & 5 & 5 & 6 & 8\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Beyond resampling, the choice of proposal distribution is paramount for an efficient particle filter. A well-designed proposal incorporates prior knowledge, such as physical constraints on the state variables. In many environmental models, quantities like pollutant concentrations must be non-negative. This advanced practice  demonstrates how to enforce such constraints by sampling in a transformed (log) space and then deriving the correct importance weight, a process that requires a crucial Jacobian adjustment from the change-of-variables theorem.",
            "id": "3906030",
            "problem": "Consider a Sequential Importance Resampling Particle Filter (PF) applied to an atmospheric chemical transport model in which the state is a vector of nonnegative pollutant concentrations $c \\in \\mathbb{R}_{+}^{d}$ across $d$ grid cells. Observations $y \\in \\mathbb{R}^{m}$ are related to the concentrations through a linear measurement operator $H \\in \\mathbb{R}^{m \\times d}$ and additive Gaussian measurement noise, so that the observation model is $y \\mid c \\sim \\mathcal{N}(H c, R)$ with a known, positive-definite covariance matrix $R \\in \\mathbb{R}^{m \\times m}$. The forecast step of the model yields a prior density $p(c)$ over $c$.\n\nTo enforce nonnegativity of concentrations in the proposal design, consider sampling in log-space as follows: define $u = \\ln c$ componentwise (so $u_{i} = \\ln c_{i}$ for $i = 1, \\dots, d$). Suppose the proposal in log-space is chosen as a multivariate Gaussian $q_{u}(u \\mid y) = \\mathcal{N}(u; \\mu, \\Sigma)$, with mean vector $\\mu \\in \\mathbb{R}^{d}$ and positive-definite covariance matrix $\\Sigma \\in \\mathbb{R}^{d \\times d}$, and then map back to concentration space via $c = \\exp(u)$ componentwise.\n\nStarting from the importance sampling identity for PF weights and the change-of-variables theorem for probability densities, derive the explicit, closed-form expression for the unnormalized importance weight as a function of $c$, corresponding to the target posterior density $p(y \\mid c) \\, p(c)$ and the proposal induced by $q_{u}(u \\mid y)$ under the transformation $c = \\exp(u)$. Your derivation must include the correct Jacobian adjustment required by the transformation that enforces nonnegativity.\n\nUse the standard, explicit probability density functions for the multivariate normal in both the likelihood and the proposal:\n$$\np(y \\mid c) = (2\\pi)^{-m/2} \\, |R|^{-1/2} \\, \\exp\\!\\left( -\\frac{1}{2} (y - H c)^{\\top} R^{-1} (y - H c) \\right),\n$$\n$$\nq_{u}(u \\mid y) = (2\\pi)^{-d/2} \\, |\\Sigma|^{-1/2} \\, \\exp\\!\\left( -\\frac{1}{2} (u - \\mu)^{\\top} \\Sigma^{-1} (u - \\mu) \\right).\n$$\n\nExpress your final answer as a single closed-form analytic expression for the unnormalized importance weight $w(c)$ as a function of $c$, $y$, $H$, $R$, $\\mu$, and $\\Sigma$, and the prior $p(c)$. Do not use proportionality notation; include all multiplicative factors required by the change-of-variables theorem. No numerical evaluation or rounding is required, and no physical units should appear in the final expression.",
            "solution": "The problem is well-posed, scientifically grounded, and contains all necessary information to derive the requested expression.\n\nThe unnormalized importance weight, $w(c)$, for a particle (a sample state vector) $c$ in a Sequential Importance Resampling particle filter is given by the ratio of the target density to the proposal density, evaluated at that particle. The target density is the unnormalized posterior probability density of the state given the observations, which is proportional to the likelihood multiplied by the prior, i.e., $p(c \\mid y) \\propto p(y \\mid c) p(c)$.\n\nThus, the importance weight $w(c)$ is defined as:\n$$\nw(c) = \\frac{p(y \\mid c) p(c)}{q_c(c \\mid y)}\n$$\nwhere $p(y \\mid c)$ is the likelihood of observing $y$ given state $c$, $p(c)$ is the prior density of the state, and $q_c(c \\mid y)$ is the proposal density from which the particle $c$ is drawn.\n\nThe problem provides the explicit form for the likelihood $p(y \\mid c)$ and the prior $p(c)$ is a given function. The main task is to find the expression for the proposal density $q_c(c \\mid y)$ in the space of concentrations $c$. We are given the proposal density in log-space, $q_u(u \\mid y)$, where $u = \\ln c$ componentwise. We must use the change of variables theorem for probability densities to transform $q_u(u \\mid y)$ into $q_c(c \\mid y)$.\n\nThe relationship between the densities of two random variable vectors $U$ and $C$ related by an invertible, differentiable transformation $C = g(U)$ (or equivalently $U = g^{-1}(C)$) is given by:\n$$\np_C(c) = p_U(g^{-1}(c)) \\left| \\det\\left( J_{g^{-1}}(c) \\right) \\right|\n$$\nwhere $J_{g^{-1}}(c)$ is the Jacobian matrix of the inverse transformation $g^{-1}$.\n\nIn our case, the state variable is $c \\in \\mathbb{R}_{+}^{d}$ and the transformed variable is $u \\in \\mathbb{R}^{d}$.\nThe transformation is from $u$ to $c$: $c = \\exp(u)$, where the exponential is applied componentwise, so $c_i = \\exp(u_i)$ for $i=1, \\dots, d$.\nThe inverse transformation is from $c$ to $u$: $u = \\ln c$, where the logarithm is applied componentwise, so $u_i = \\ln(c_i)$ for $i=1, \\dots, d$. Here, $g^{-1}(c) = \\ln(c)$.\n\nWe need to compute the Jacobian matrix of this inverse transformation, $J_{\\ln(c)} = \\frac{\\partial u}{\\partial c}$. The entries of this matrix are $J_{ij} = \\frac{\\partial u_i}{\\partial c_j}$.\n$$\nJ_{ij} = \\frac{\\partial (\\ln c_i)}{\\partial c_j} =\n\\begin{cases}\n    \\frac{1}{c_i} & \\text{if } i=j \\\\\n    0 & \\text{if } i \\neq j\n\\end{cases}\n$$\nThis shows that the Jacobian matrix is a diagonal matrix:\n$$\nJ_{\\ln(c)} = \\text{diag}\\left(\\frac{1}{c_1}, \\frac{1}{c_2}, \\dots, \\frac{1}{c_d}\\right)\n$$\nThe determinant of a diagonal matrix is the product of its diagonal elements:\n$$\n\\det\\left( J_{\\ln(c)} \\right) = \\prod_{i=1}^{d} \\frac{1}{c_i}\n$$\nSince the concentrations $c_i$ are strictly positive ($c \\in \\mathbb{R}_{+}^{d}$), the absolute value of the determinant is simply the determinant itself:\n$$\n\\left| \\det\\left( J_{\\ln(c)} \\right) \\right| = \\prod_{i=1}^{d} \\frac{1}{c_i} = \\left(\\prod_{i=1}^{d} c_i\\right)^{-1}\n$$\nThis is the Jacobian adjustment factor.\n\nNow we can write the proposal density in $c$-space, $q_c(c \\mid y)$, using the given proposal in $u$-space, $q_u(u \\mid y) = \\mathcal{N}(u; \\mu, \\Sigma)$:\n$$\nq_c(c \\mid y) = q_u(\\ln c \\mid y) \\left| \\det\\left( J_{\\ln(c)} \\right) \\right|\n$$\nSubstituting the explicit form for $q_u$ and the Jacobian determinant:\n$$\nq_c(c \\mid y) = \\left( (2\\pi)^{-d/2} |\\Sigma|^{-1/2} \\exp\\left( -\\frac{1}{2} (\\ln c - \\mu)^{\\top} \\Sigma^{-1} (\\ln c - \\mu) \\right) \\right) \\left( \\prod_{i=1}^{d} \\frac{1}{c_i} \\right)\n$$\nHere, $\\ln c$ is the vector $(\\ln c_1, \\dots, \\ln c_d)^{\\top}$.\n\nFinally, we substitute all the components back into the formula for the importance weight $w(c)$:\n$$\nw(c) = \\frac{p(y \\mid c) p(c)}{q_c(c \\mid y)} = \\frac{ \\left( (2\\pi)^{-m/2} |R|^{-1/2} \\exp\\left( -\\frac{1}{2} (y - H c)^{\\top} R^{-1} (y - H c) \\right) \\right) p(c) }{ \\left( (2\\pi)^{-d/2} |\\Sigma|^{-1/2} \\exp\\left( -\\frac{1}{2} (\\ln c - \\mu)^{\\top} \\Sigma^{-1} (\\ln c - \\mu) \\right) \\right) \\left( \\prod_{i=1}^{d} c_i^{-1} \\right) }\n$$\nTo simplify, we can rearrange the terms. The term $\\left( \\prod_{i=1}^{d} c_i^{-1} \\right)$ in the denominator moves to the numerator. The exponential term in the denominator also moves to the numerator with its sign flipped in the argument.\n$$\nw(c) = \\frac{(2\\pi)^{-m/2} |R|^{-1/2}}{(2\\pi)^{-d/2} |\\Sigma|^{-1/2}} \\left( \\prod_{i=1}^{d} c_i \\right) p(c) \\exp\\left( -\\frac{1}{2} (y - H c)^{\\top} R^{-1} (y - H c) - \\left(-\\frac{1}{2} (\\ln c - \\mu)^{\\top} \\Sigma^{-1} (\\ln c - \\mu) \\right) \\right)\n$$\nGrouping the constant prefactors and combining the exponents:\n$$\nw(c) = (2\\pi)^{(d-m)/2} \\frac{|\\Sigma|^{1/2}}{|R|^{1/2}} \\left( \\prod_{i=1}^{d} c_i \\right) p(c) \\exp\\left( -\\frac{1}{2} (y - H c)^{\\top} R^{-1} (y - H c) + \\frac{1}{2} (\\ln c - \\mu)^{\\top} \\Sigma^{-1} (\\ln c - \\mu) \\right)\n$$\nThis is the final, explicit, closed-form expression for the unnormalized importance weight $w(c)$.",
            "answer": "$$\n\\boxed{(2\\pi)^{\\frac{d-m}{2}} \\left( \\frac{|\\Sigma|}{|R|} \\right)^{\\frac{1}{2}} \\left( \\prod_{i=1}^{d} c_i \\right) p(c) \\exp\\left(-\\frac{1}{2} (y - H c)^{\\top} R^{-1} (y - H c) + \\frac{1}{2} (\\ln c - \\mu)^{\\top} \\Sigma^{-1} (\\ln c - \\mu)\\right)}\n$$"
        }
    ]
}