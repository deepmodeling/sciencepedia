{
    "hands_on_practices": [
        {
            "introduction": "This first practice is designed to solidify your understanding of the core calculation in Optimal Interpolation. You will compute the posterior analysis error variance for a simple two-observation scenario, directly applying the formula for a Best Linear Unbiased Estimator (BLUE). This exercise provides a concrete numerical example of how OI quantifies the reduction in uncertainty after assimilating data. ",
            "id": "3805220",
            "problem": "Consider a zero-mean, stationary Gaussian random field representing sea surface temperature anomaly, denoted by $x(\\mathbf{x})$, with isotropic covariance function $C(r) = \\sigma^{2}\\exp(-r/L)$, where $r$ is the horizontal separation, $\\sigma^{2}$ is the prior variance, and $L$ is the correlation length scale. You wish to estimate $x(\\mathbf{x}_0)$ at an analysis location $\\mathbf{x}_0$ using linear Objective Analysis (Optimal Interpolation), which is the Best Linear Unbiased Estimator of $x(\\mathbf{x}_0)$ constructed from a set of point observations.\n\nTwo independent scalar observations $y_1$ and $y_2$ of the same anomaly field are available at locations $\\mathbf{x}_1$ and $\\mathbf{x}_2$, respectively. Each observation is contaminated by additive, independent, zero-mean measurement noise with known variances $\\mathrm{Var}(\\epsilon_1) = r_1$ and $\\mathrm{Var}(\\epsilon_2) = r_2$, and no cross-covariance between measurement errors. Assume the errors are uncorrelated with the signal $x(\\mathbf{x})$.\n\nYou are given the following scientifically reasonable parameters:\n- Prior variance $\\sigma^{2} = 1.2$ in $\\,^\\circ\\text{C}^2$.\n- Correlation length $L = 60\\,\\mathrm{km}$.\n- Distances from the analysis point: $r_1 = 30\\,\\mathrm{km}$ and $r_2 = 60\\,\\mathrm{km}$.\n- Separation between the two observation locations: $d_{12} = 90\\,\\mathrm{km}$.\n- Measurement error variances: $r_1 = r_2 = 0.09$ in $\\,^\\circ\\text{C}^2$.\n\nStarting from the definition of a Best Linear Unbiased Estimator for a Gaussian random field with additive measurement noise, derive the expression for the posterior analysis error variance at $\\mathbf{x}_0$ produced by Optimal Interpolation in terms of the prior covariance and the data covariance. Then, evaluate this posterior variance numerically for the configuration given above. Clearly indicate all intermediate quantities you compute (such as the signalâ€“data covariance vector and the data covariance matrix) based on the stated covariance model and distances.\n\nExpress your final answer as a single real number in $\\,^\\circ\\text{C}^2$ and round your result to four significant figures. Briefly interpret this quantity physically as a posterior uncertainty at the analysis location after assimilating the two observations.",
            "solution": "The problem is scientifically grounded, well-posed, objective, and contains sufficient information for a unique solution. The notational ambiguity wherein the symbols $r_1$ and $r_2$ are used to denote both distances and error variances is noted; however, the intended meaning is clear from the physical units and context. To prevent confusion in the derivation, the distances from the analysis point $\\mathbf{x}_0$ to the observation points $\\mathbf{x}_1$ and $\\mathbf{x}_2$ will be denoted by $d_{01}$ and $d_{02}$, respectively. The measurement error variances will be denoted as per the problem statement by $r_1$ and $r_2$.\n\nThe objective is to find the posterior analysis error variance, denoted $\\sigma_a^2$, at the location $\\mathbf{x}_0$. For a Best Linear Unbiased Estimator (BLUE), which is the basis of Optimal Interpolation (OI), the analysis error variance is given by the expression:\n$$\n\\sigma_a^2 = \\sigma_b^2 - \\mathbf{k}^\\top \\mathbf{C}_{yy}^{-1} \\mathbf{k}\n$$\nHere, $\\sigma_b^2$ is the prior (background) error variance at the analysis location, $\\mathbf{k}$ is the covariance vector between the true state at the analysis location and the observations, and $\\mathbf{C}_{yy}$ is the total covariance matrix of the observations.\n\nWe will now determine each component of this equation.\n\n1.  **Prior Variance ($\\sigma_b^2$)**\n    The prior variance at the analysis location $\\mathbf{x}_0$ is the variance of the field itself, evaluated at zero separation, as there is no other prior information.\n    $$\n    \\sigma_b^2 = \\mathrm{Var}(x(\\mathbf{x}_0)) = C(0) = \\sigma^2 \\exp(-0/L) = \\sigma^2\n    $$\n    Given $\\sigma^2 = 1.2\\,^\\circ\\text{C}^2$, we have $\\sigma_b^2 = 1.2$.\n\n2.  **Signal-Data Covariance Vector ($\\mathbf{k}$)**\n    This vector contains the covariances between the true field value at the analysis point $x(\\mathbf{x}_0)$ and each of the observations, $y_1$ and $y_2$. The $i$-th element of $\\mathbf{k}$ is $k_i = \\mathrm{Cov}(x(\\mathbf{x}_0), y_i)$.\n    Since $y_i = x(\\mathbf{x}_i) + \\epsilon_i$ and the measurement noise $\\epsilon_i$ is uncorrelated with the signal $x$, this simplifies to:\n    $$\n    k_i = \\mathrm{Cov}(x(\\mathbf{x}_0), x(\\mathbf{x}_i) + \\epsilon_i) = \\mathrm{Cov}(x(\\mathbf{x}_0), x(\\mathbf{x}_i)) + \\mathrm{Cov}(x(\\mathbf{x}_0), \\epsilon_i) = C(d_{0i}) + 0\n    $$\n    where $d_{0i}$ is the distance between $\\mathbf{x}_0$ and $\\mathbf{x}_i$. The vector $\\mathbf{k}$ is thus:\n    $$\n    \\mathbf{k} = \\begin{pmatrix} C(d_{01}) \\\\ C(d_{02}) \\end{pmatrix} = \\begin{pmatrix} \\sigma^2 \\exp(-d_{01}/L) \\\\ \\sigma^2 \\exp(-d_{02}/L) \\end{pmatrix}\n    $$\n    Using the given values $d_{01} = 30\\,\\mathrm{km}$, $d_{02} = 60\\,\\mathrm{km}$, $L = 60\\,\\mathrm{km}$, and $\\sigma^2 = 1.2$:\n    $$\n    k_1 = 1.2 \\exp(-30/60) = 1.2 \\exp(-0.5)\n    $$\n    $$\n    k_2 = 1.2 \\exp(-60/60) = 1.2 \\exp(-1)\n    $$\n    Numerically, $k_1 \\approx 0.727837\\,^\\circ\\text{C}^2$ and $k_2 \\approx 0.441455\\,^\\circ\\text{C}^2$.\n\n3.  **Data Covariance Matrix ($\\mathbf{C}_{yy}$)**\n    This matrix represents the total covariance of the observation vector $\\mathbf{y} = (y_1, y_2)^\\top$. It is the sum of the signal covariance matrix $\\mathbf{C}_{xx}$ and the measurement error covariance matrix $\\mathbf{R}$:\n    $$\n    \\mathbf{C}_{yy} = \\mathbf{C}_{xx} + \\mathbf{R}\n    $$\n    The elements of the signal covariance matrix are $(\\mathbf{C}_{xx})_{ij} = \\mathrm{Cov}(x(\\mathbf{x}_i), x(\\mathbf{x}_j)) = C(d_{ij})$, where $d_{ij}$ is the distance between observation points $\\mathbf{x}_i$ and $\\mathbf{x}_j$.\n    $$\n    \\mathbf{C}_{xx} = \\begin{pmatrix} C(0) & C(d_{12}) \\\\ C(d_{21}) & C(0) \\end{pmatrix} = \\begin{pmatrix} \\sigma^2 & \\sigma^2 \\exp(-d_{12}/L) \\\\ \\sigma^2 \\exp(-d_{12}/L) & \\sigma^2 \\end{pmatrix}\n    $$\n    The measurement errors are independent, so their covariance matrix $\\mathbf{R}$ is diagonal:\n    $$\n    \\mathbf{R} = \\begin{pmatrix} \\mathrm{Var}(\\epsilon_1) & 0 \\\\ 0 & \\mathrm{Var}(\\epsilon_2) \\end{pmatrix} = \\begin{pmatrix} r_1 & 0 \\\\ 0 & r_2 \\end{pmatrix}\n    $$\n    Combining these gives:\n    $$\n    \\mathbf{C}_{yy} = \\begin{pmatrix} \\sigma^2 + r_1 & \\sigma^2 \\exp(-d_{12}/L) \\\\ \\sigma^2 \\exp(-d_{12}/L) & \\sigma^2 + r_2 \\end{pmatrix}\n    $$\n    Using the given values $d_{12} = 90\\,\\mathrm{km}$, $L=60\\,\\mathrm{km}$, $\\sigma^2=1.2$, and $r_1 = r_2 = 0.09$:\n    - Diagonal elements: $\\sigma^2 + r_1 = 1.2 + 0.09 = 1.29$.\n    - Off-diagonal elements: $\\sigma^2 \\exp(-90/60) = 1.2 \\exp(-1.5)$.\n    So, the matrix is:\n    $$\n    \\mathbf{C}_{yy} = \\begin{pmatrix} 1.29 & 1.2 \\exp(-1.5) \\\\ 1.2 \\exp(-1.5) & 1.29 \\end{pmatrix}\n    $$\n    Numerically, $1.2 \\exp(-1.5) \\approx 0.267756$, so $\\mathbf{C}_{yy} \\approx \\begin{pmatrix} 1.29 & 0.267756 \\\\ 0.267756 & 1.29 \\end{pmatrix}$.\n\n4.  **Final Calculation**\n    We need to compute the variance reduction term, $\\Delta\\sigma^2 = \\mathbf{k}^\\top \\mathbf{C}_{yy}^{-1} \\mathbf{k}$.\n    First, we find the inverse of the $2 \\times 2$ matrix $\\mathbf{C}_{yy}$.\n    The determinant is $\\det(\\mathbf{C}_{yy}) = (1.29)^2 - (1.2 \\exp(-1.5))^2 = 1.6641 - (1.44 \\exp(-3))$.\n    $\\det(\\mathbf{C}_{yy}) \\approx 1.6641 - 1.44 \\times 0.049787 = 1.6641 - 0.071693 = 1.592407$.\n    The inverse is:\n    $$\n    \\mathbf{C}_{yy}^{-1} = \\frac{1}{\\det(\\mathbf{C}_{yy})} \\begin{pmatrix} 1.29 & -1.2 \\exp(-1.5) \\\\ -1.2 \\exp(-1.5) & 1.29 \\end{pmatrix}\n    $$\n    The quadratic form $\\Delta\\sigma^2 = \\mathbf{k}^\\top \\mathbf{C}_{yy}^{-1} \\mathbf{k}$ is:\n    $$\n    \\Delta\\sigma^2 = \\frac{1}{\\det(\\mathbf{C}_{yy})} \\left[ 1.29(k_1^2 + k_2^2) - 2 \\cdot (1.2 \\exp(-1.5)) \\cdot k_1 k_2 \\right]\n    $$\n    Substituting the expressions for $k_1$ and $k_2$:\n    $k_1^2 = (1.2 \\exp(-0.5))^2 = 1.44 \\exp(-1)$\n    $k_2^2 = (1.2 \\exp(-1))^2 = 1.44 \\exp(-2)$\n    $k_1 k_2 = (1.2 \\exp(-0.5)) (1.2 \\exp(-1)) = 1.44 \\exp(-1.5)$\n    Numerator of $\\Delta\\sigma^2$:\n    $1.29 \\left( 1.44 \\exp(-1) + 1.44 \\exp(-2) \\right) - 2 \\cdot (1.2 \\exp(-1.5)) \\cdot (1.44 \\exp(-1.5))$\n    $= 1.29 \\cdot 1.44 (\\exp(-1) + \\exp(-2)) - 2 \\cdot 1.2 \\cdot 1.44 \\cdot \\exp(-3)$\n    $= 1.8576 (\\exp(-1) + \\exp(-2)) - 3.456 \\exp(-3)$\n    Using numerical values:\n    $k_1 \\approx 0.727837$, $k_2 \\approx 0.441455$, $1.2 \\exp(-1.5) \\approx 0.267756$.\n    $k_1^2 \\approx 0.529727$, $k_2^2 \\approx 0.194883$.\n    $k_1 k_2 \\approx 0.321303$.\n    Numerator $\\approx 1.29(0.529727 + 0.194883) - 2(0.267756)(0.321303)$\n    $\\approx 1.29(0.724610) - 0.172061 \\approx 0.934747 - 0.172061 = 0.762686$.\n    $\\Delta\\sigma^2 \\approx \\frac{0.762686}{1.592407} \\approx 0.479002 \\,^\\circ\\text{C}^2$.\n\n    Finally, the posterior analysis error variance is:\n    $$\n    \\sigma_a^2 = \\sigma_b^2 - \\Delta\\sigma^2 \\approx 1.2 - 0.479002 = 0.720998 \\,^\\circ\\text{C}^2\n    $$\n    Rounding to four significant figures, we get $\\sigma_a^2 = 0.7210 \\,^\\circ\\text{C}^2$.\n\nThe physical interpretation of this result is that the initial uncertainty in the sea surface temperature anomaly at the analysis location, represented by the prior variance $\\sigma_b^2 = 1.2\\,^\\circ\\text{C}^2$, has been reduced by assimilating the two observations. The posterior variance $\\sigma_a^2 \\approx 0.7210\\,^\\circ\\text{C}^2$ is the expected squared error of the optimal estimate. It quantifies the remaining uncertainty after the information from the data has been incorporated. The square root of this value, $\\sqrt{\\sigma_a^2} \\approx 0.849\\,^\\circ\\text{C}$, is the expected root-mean-square error of the final analysis.",
            "answer": "$$\n\\boxed{0.7210}\n$$"
        },
        {
            "introduction": "The \"optimality\" in OI is contingent upon accurate statistical assumptions. This exercise challenges you to explore what happens when these assumptions are violated, specifically by underestimating the observation error variance. By deriving the resulting sub-optimal analysis error, you will gain a deeper appreciation for the concept of \"overfitting\" and the importance of correctly specifying error covariances in any data assimilation system. ",
            "id": "3903515",
            "problem": "Consider a one-dimensional data assimilation setting used in environmental and earth system modeling, in which Optimal Interpolation (OI) is applied to estimate a geophysical scalar state $x$. Let the background (prior) state $x_{b}$ be an unbiased estimator of $x$ with background error $x - x_{b}$ modeled as a zero-mean Gaussian random variable with variance $\\sigma_{b}^{2}$. A single observation $y$ is obtained from a linear measurement model $y = x + \\varepsilon$, where the true observation error $\\varepsilon$ is zero-mean Gaussian with variance $\\sigma_{r}^{2}$, independent of the background error. The OI analysis estimator takes the linear form $x_{a} = x_{b} + K\\,(y - x_{b})$, where $K$ is the gain chosen to minimize the mean-square analysis error under the assumed error statistics.\n\nSuppose that the data assimilation system underestimates the observation error variance and uses an assumed value $R_{\\text{assumed}} = \\alpha\\,\\sigma_{r}^{2}$ with $0 < \\alpha < 1$ instead of the true value $R_{\\text{true}} = \\sigma_{r}^{2}$ when constructing the gain. Starting from the Gaussian conditioning and linear minimum-variance estimation principles, do the following:\n\n1. Derive the OI gain $K(\\alpha)$ that minimizes the mean-square analysis error under the assumption $R_{\\text{assumed}} = \\alpha\\,\\sigma_{r}^{2}$, and show that $K(\\alpha)$ increases as $\\alpha$ decreases. Use this to demonstrate that the expected squared magnitude of the analysis increment $d \\equiv x_{a} - x_{b}$ is larger when $R$ is underestimated than when $R$ is correct.\n\n2. Compute the true variance of the analysis error $A(\\alpha) \\equiv \\operatorname{Var}(x_{a} - x)$ when the gain is computed with the underestimated $R_{\\text{assumed}} = \\alpha\\,\\sigma_{r}^{2}$ but the world obeys the true observation error variance $R_{\\text{true}} = \\sigma_{r}^{2}$. Express $A(\\alpha)$ in terms of $\\sigma_{b}^{2}$, $\\sigma_{r}^{2}$, and $\\alpha$.\n\n3. Compute the true posterior variance $A_{\\text{true}} \\equiv \\operatorname{Var}(x \\mid y)$ under the correct model $R_{\\text{true}} = \\sigma_{r}^{2}$.\n\n4. Define the overfitting factor as the ratio of the trace of the analysis error covariance under underestimated $R$ to the true posterior variance, which in this scalar setting reduces to $\\gamma(\\alpha) \\equiv \\dfrac{A(\\alpha)}{A_{\\text{true}}}$. Provide a closed-form analytic expression for $\\gamma(\\alpha)$ as a function of $\\sigma_{b}^{2}$, $\\sigma_{r}^{2}$, and $\\alpha$, and show that it exceeds $1$ for $0 < \\alpha < 1$.\n\nYour final answer must be the single closed-form expression for $\\gamma(\\alpha)$. No numerical values are required; no rounding is necessary because the final answer is symbolic and dimensionless.",
            "solution": "The problem statement has been validated and is deemed sound, well-posed, and scientifically grounded within the standard framework of linear estimation theory and data assimilation. We may proceed with the solution.\n\nThe analysis state $x_{a}$ is a linear combination of the background state $x_{b}$ and the observation $y$. The analysis error is defined as $e_{a} \\equiv x_{a} - x$, where $x$ is the true state. We are given the analysis equation $x_{a} = x_{b} + K(y - x_{b})$. We can express the analysis error in terms of the background error $e_{b} \\equiv x_{b} - x$ and the observation error $\\varepsilon \\equiv y - x$.\nFirst, rewrite the innovation term $y - x_{b}$:\n$$y - x_{b} = (x + \\varepsilon) - x_{b} = \\varepsilon - (x_{b} - x) = \\varepsilon - e_{b}$$\nNow substitute this into the expression for $x_{a}$:\n$$x_{a} = x_{b} + K(\\varepsilon - e_{b})$$\nThe analysis error is then:\n$$e_{a} = x_{a} - x = (x_{b} - x) + K(\\varepsilon - e_{b}) = e_{b} + K(\\varepsilon - e_{b}) = (1-K)e_{b} + K\\varepsilon$$\nThe background and observation errors are given as zero-mean random variables, $E[e_{b}] = 0$ and $E[\\varepsilon] = 0$. The expected analysis error is:\n$$E[e_{a}] = E[(1-K)e_{b} + K\\varepsilon] = (1-K)E[e_{b}] + K E[\\varepsilon] = 0$$\nSince the analysis is unbiased, the mean-square analysis error is equal to its variance, $\\operatorname{Var}(e_{a}) = E[e_{a}^{2}]$. As the errors $e_{b}$ and $\\varepsilon$ are independent, $E[e_{b}\\varepsilon] = E[e_{b}]E[\\varepsilon]=0$. The variance is:\n$$E[e_{a}^{2}] = E[((1-K)e_{b} + K\\varepsilon)^{2}] = (1-K)^{2}E[e_{b}^{2}] + K^{2}E[\\varepsilon^{2}] + 2K(1-K)E[e_{b}\\varepsilon]$$\n$$E[e_{a}^{2}] = (1-K)^{2}\\sigma_{b}^{2} + K^{2}\\sigma_{r}^{2}$$\nwhere $\\sigma_{b}^{2} = \\operatorname{Var}(e_{b})$ and $\\sigma_{r}^{2} = \\operatorname{Var}(\\varepsilon)$.\n\n### Part 1: Derivation of the Gain $K(\\alpha)$ and Analysis Increment\n\nThe OI gain is computed by minimizing the mean-square analysis error under the *assumed* error statistics. The system assumes an observation error variance of $R_{\\text{assumed}} = \\alpha\\,\\sigma_{r}^{2}$. Therefore, the gain $K(\\alpha)$ is chosen to minimize the cost function $J_{\\text{assumed}}(K)$:\n$$J_{\\text{assumed}}(K) = (1-K)^{2}\\sigma_{b}^{2} + K^{2}(\\alpha\\sigma_{r}^{2})$$\nTo find the minimum, we differentiate with respect to $K$ and set the result to zero:\n$$\\frac{dJ_{\\text{assumed}}}{dK} = -2(1-K)\\sigma_{b}^{2} + 2K(\\alpha\\sigma_{r}^{2}) = 0$$\n$$- \\sigma_{b}^{2} + K\\sigma_{b}^{2} + K\\alpha\\sigma_{r}^{2} = 0$$\n$$K(\\sigma_{b}^{2} + \\alpha\\sigma_{r}^{2}) = \\sigma_{b}^{2}$$\nThis gives the expression for the gain $K(\\alpha)$:\n$$K(\\alpha) = \\frac{\\sigma_{b}^{2}}{\\sigma_{b}^{2} + \\alpha\\sigma_{r}^{2}}$$\nTo show that $K(\\alpha)$ increases as $\\alpha$ decreases, we examine its derivative with respect to $\\alpha$:\n$$\\frac{dK(\\alpha)}{d\\alpha} = \\frac{d}{d\\alpha} \\left( \\sigma_{b}^{2} (\\sigma_{b}^{2} + \\alpha\\sigma_{r}^{2})^{-1} \\right) = -\\sigma_{b}^{2}(\\sigma_{b}^{2} + \\alpha\\sigma_{r}^{2})^{-2}(\\sigma_{r}^{2}) = -\\frac{\\sigma_{b}^{2}\\sigma_{r}^{2}}{(\\sigma_{b}^{2} + \\alpha\\sigma_{r}^{2})^{2}}$$\nSince $\\sigma_{b}^{2} > 0$ and $\\sigma_{r}^{2} > 0$, the derivative $\\frac{dK(\\alpha)}{d\\alpha}$ is always negative for $\\alpha>0$. Thus, $K(\\alpha)$ is a strictly decreasing function of $\\alpha$. As $\\alpha$ decreases (i.e., the observation is trusted more), the gain $K(\\alpha)$ increases.\n\nThe analysis increment is $d \\equiv x_{a} - x_{b} = K(\\alpha)(y - x_{b})$. The expected squared magnitude of the increment is $E[d^2]$. The innovation $y - x_{b}$ has a mean of zero and its variance is calculated using the *true* statistics:\n$$\\operatorname{Var}(y-x_{b}) = \\operatorname{Var}(\\varepsilon - e_{b}) = \\operatorname{Var}(\\varepsilon) + \\operatorname{Var}(e_{b}) = \\sigma_{r}^{2} + \\sigma_{b}^{2}$$\nThe expected squared increment is:\n$$E[d^{2}] = E[(K(\\alpha)(y-x_{b}))^{2}] = K(\\alpha)^{2}E[(y-x_{b})^{2}] = K(\\alpha)^{2}(\\sigma_{r}^{2} + \\sigma_{b}^{2})$$\nSince $K(\\alpha)$ is a decreasing function of $\\alpha$, $K(\\alpha)^{2}$ is also a decreasing function of $\\alpha$. Therefore, $E[d^{2}]$ is a decreasing function of $\\alpha$. Underestimation of $R$ corresponds to using an $\\alpha < 1$. Because $E[d^2]$ is a decreasing function of $\\alpha$, the value of $E[d^2]$ for $\\alpha < 1$ is larger than its value for $\\alpha=1$ (the correct case). This demonstrates that the expected magnitude of the analysis increment is larger when the observation error variance is underestimated.\n\n### Part 2: True Variance of the Analysis Error $A(\\alpha)$\n\nThe true variance of the analysis error, $A(\\alpha) = \\operatorname{Var}(x_a - x)$, is calculated using the suboptimal gain $K(\\alpha)$ but with the *true* error variances $\\sigma_{b}^{2}$ and $\\sigma_{r}^{2}$.\nFrom our initial derivation, the general form of the analysis error variance is $E[e_{a}^{2}] = (1-K)^{2}\\sigma_{b}^{2} + K^{2}\\sigma_{r}^{2}$. We substitute $K=K(\\alpha)$:\n$$A(\\alpha) = (1-K(\\alpha))^{2}\\sigma_{b}^{2} + K(\\alpha)^{2}\\sigma_{r}^{2}$$\nWe have $K(\\alpha) = \\frac{\\sigma_{b}^{2}}{\\sigma_{b}^{2} + \\alpha\\sigma_{r}^{2}}$ and $1 - K(\\alpha) = \\frac{\\alpha\\sigma_{r}^{2}}{\\sigma_{b}^{2} + \\alpha\\sigma_{r}^{2}}$. Substituting these into the expression for $A(\\alpha)$:\n$$A(\\alpha) = \\left(\\frac{\\alpha\\sigma_{r}^{2}}{\\sigma_{b}^{2} + \\alpha\\sigma_{r}^{2}}\\right)^{2}\\sigma_{b}^{2} + \\left(\\frac{\\sigma_{b}^{2}}{\\sigma_{b}^{2} + \\alpha\\sigma_{r}^{2}}\\right)^{2}\\sigma_{r}^{2}$$\n$$A(\\alpha) = \\frac{\\alpha^{2}\\sigma_{r}^{4}\\sigma_{b}^{2} + \\sigma_{b}^{4}\\sigma_{r}^{2}}{(\\sigma_{b}^{2} + \\alpha\\sigma_{r}^{2})^{2}} = \\frac{\\sigma_{b}^{2}\\sigma_{r}^{2}(\\alpha^{2}\\sigma_{r}^{2} + \\sigma_{b}^{2})}{(\\sigma_{b}^{2} + \\alpha\\sigma_{r}^{2})^{2}}$$\n\n### Part 3: True Posterior Variance $A_{\\text{true}}$\n\nThe true posterior variance $A_{\\text{true}} = \\operatorname{Var}(x \\mid y)$ is the minimum possible analysis error variance, achieved when the gain is computed with the correct observation error variance, i.e., with $\\alpha=1$. We can find $A_{\\text{true}}$ by evaluating $A(\\alpha)$ at $\\alpha=1$.\n$$A_{\\text{true}} = A(1) = \\frac{\\sigma_{b}^{2}\\sigma_{r}^{2}(1^{2}\\sigma_{r}^{2} + \\sigma_{b}^{2})}{(\\sigma_{b}^{2} + 1\\cdot\\sigma_{r}^{2})^{2}} = \\frac{\\sigma_{b}^{2}\\sigma_{r}^{2}(\\sigma_{r}^{2} + \\sigma_{b}^{2})}{(\\sigma_{b}^{2} + \\sigma_{r}^{2})^{2}}$$\n$$A_{\\text{true}} = \\frac{\\sigma_{b}^{2}\\sigma_{r}^{2}}{\\sigma_{b}^{2} + \\sigma_{r}^{2}}$$\nThis is the standard formula for the analysis error variance in scalar Optimal Interpolation.\n\n### Part 4: Overfitting Factor $\\gamma(\\alpha)$\n\nThe overfitting factor is defined as the ratio $\\gamma(\\alpha) \\equiv \\frac{A(\\alpha)}{A_{\\text{true}}}$. Using the expressions derived above:\n$$\\gamma(\\alpha) = \\frac{\\frac{\\sigma_{b}^{2}\\sigma_{r}^{2}(\\sigma_{b}^{2} + \\alpha^{2}\\sigma_{r}^{2})}{(\\sigma_{b}^{2} + \\alpha\\sigma_{r}^{2})^{2}}}{\\frac{\\sigma_{b}^{2}\\sigma_{r}^{2}}{\\sigma_{b}^{2} + \\sigma_{r}^{2}}}$$\nCanceling the common term $\\sigma_{b}^{2}\\sigma_{r}^{2}$:\n$$\\gamma(\\alpha) = \\frac{(\\sigma_{b}^{2} + \\alpha^{2}\\sigma_{r}^{2})(\\sigma_{b}^{2} + \\sigma_{r}^{2})}{(\\sigma_{b}^{2} + \\alpha\\sigma_{r}^{2})^{2}}$$\nThis is the required closed-form expression for $\\gamma(\\alpha)$.\n\nTo show that $\\gamma(\\alpha) > 1$ for $0 < \\alpha < 1$, we must prove the inequality:\n$$\\frac{(\\sigma_{b}^{2} + \\alpha^{2}\\sigma_{r}^{2})(\\sigma_{b}^{2} + \\sigma_{r}^{2})}{(\\sigma_{b}^{2} + \\alpha\\sigma_{r}^{2})^{2}} > 1$$\nLet's define a dimensionless ratio of variances $S = \\frac{\\sigma_{b}^{2}}{\\sigma_{r}^{2}}$. Since variances are positive, $S>0$. We can rewrite the inequality by dividing the numerator and denominator by $\\sigma_r^4$:\n$$\\frac{(\\frac{\\sigma_{b}^{2}}{\\sigma_{r}^{2}} + \\alpha^{2})(\\frac{\\sigma_{b}^{2}}{\\sigma_{r}^{2}} + 1)}{(\\frac{\\sigma_{b}^{2}}{\\sigma_{r}^{2}} + \\alpha)^{2}} = \\frac{(S + \\alpha^{2})(S+1)}{(S+\\alpha)^{2}} > 1$$\nSince the denominator $(S+\\alpha)^{2}$ is positive, we can multiply both sides by it without changing the inequality's direction:\n$$(S + \\alpha^{2})(S+1) > (S+\\alpha)^{2}$$\nExpanding both sides:\n$$S^{2} + S + \\alpha^{2}S + \\alpha^{2} > S^{2} + 2\\alpha S + \\alpha^{2}$$\nSubtracting $S^{2}+\\alpha^{2}$ from both sides yields:\n$$S + \\alpha^{2}S > 2\\alpha S$$\nSince $S > 0$, we can divide by $S$:\n$$1 + \\alpha^{2} > 2\\alpha$$\nRearranging the terms gives:\n$$1 - 2\\alpha + \\alpha^{2} > 0$$\n$$(1-\\alpha)^{2} > 0$$\nThis inequality is true for all real numbers $\\alpha$ except $\\alpha=1$. The problem specifies that $0 < \\alpha < 1$, a range for which the inequality $(1-\\alpha)^{2} > 0$ is strictly satisfied. Therefore, $\\gamma(\\alpha) > 1$ for $0 < \\alpha < 1$. This confirms that underestimating the observation error variance leads to an analysis that is truly less accurate than the optimal analysis, a phenomenon termed overfitting.",
            "answer": "$$\\boxed{\\frac{(\\sigma_{b}^{2} + \\sigma_{r}^{2})(\\sigma_{b}^{2} + \\alpha^{2}\\sigma_{r}^{2})}{(\\sigma_{b}^{2} + \\alpha\\sigma_{r}^{2})^{2}}}$$"
        },
        {
            "introduction": "This final practice transitions from theoretical derivations to a practical, computational application common in the Earth sciences. You will implement the full OI algorithm to generate a gridded map of a geophysical field from sparse, irregular observations, a task known as objective analysis. This exercise involves constructing the full covariance matrices and computing the spatial map of the analysis error, illustrating how data coverage and covariance parameters shape the final uncertainty. ",
            "id": "3805211",
            "problem": "You are provided with a scenario in computational oceanography involving Objective Analysis and Optimal Interpolation (OI). A set of irregular sea surface temperature (SST) observations are collected along two ship tracks within a square domain. Assume the SST anomalies form a second-order stationary Gaussian random field with isotropic covariance specified by $C(r)=\\sigma^2\\exp\\left(-\\dfrac{r}{L}\\right)$, where $r$ is the Euclidean separation in $\\mathrm{km}$, $\\sigma^2$ is the prior variance in $\\,^\\circ\\text{C}^2$, and $L$ is the covariance length scale in $\\mathrm{km}$. The measurement error is independent, Gaussian, and spatially uncorrelated with known variance $\\epsilon^2$ in $\\,^\\circ\\text{C}^2$.\n\nYou must implement Optimal Interpolation (OI) to map SST onto a regular grid from the irregular ship-track observations and compute the corresponding analysis error variance map. The Objective Analysis (OA) and Optimal Interpolation (OI) should be derived and implemented from first principles, beginning with a Gaussian random field assumption and the definition of the linear unbiased estimator that minimizes mean squared error.\n\nThe computational domain is a square with coordinates in $\\mathrm{km}$: $x\\in[0,200]$ and $y\\in[0,200]$. The grid for mapping consists of $21\\times 21$ points with $10\\,\\mathrm{km}$ spacing in both $x$ and $y$. The ship tracks are defined by the following observation locations (in $\\mathrm{km}$):\n- Track A: $(10,50)$, $(40,50)$, $(70,50)$, $(100,50)$, $(130,50)$, $(160,50)$, $(190,50)$.\n- Track B: $(120,20)$, $(120,60)$, $(120,100)$, $(120,140)$, $(120,180)$.\nThese yield $12$ observations. The corresponding observed SST values in $^\\circ\\mathrm{C}$ must be treated as given constants in your program, constructed by evaluating a smooth, physically plausible true SST anomaly field and adding small, fixed measurement noise for reproducibility. The analysis error variance mapping must depend only on the geometry and the covariance parameters $L$, $\\sigma^2$, and $\\epsilon^2$.\n\nFrom the Gaussian random field fundamentals and the Best Linear Unbiased Estimator (BLUE) principles, derive the OI weights and the analysis error variance as functions of the covariance model and the observation network. Implement the OI mapping on the specified grid and compute the analysis error variance map across the grid in $\\,^\\circ\\text{C}^2$.\n\nYour program must evaluate the following test suite of parameter sets, each specified by $(L,\\sigma^2,\\epsilon^2)$ with units $L$ in $\\mathrm{km}$ and $\\sigma^2,\\epsilon^2$ in $\\,^\\circ\\text{C}^2$:\n1. $(50,1.0,0.04)$: a typical mesoscale-dominant case with moderate measurement error.\n2. $(10,1.0,0.01)$: a short length-scale case where observations have highly localized influence.\n3. $(200,1.0,0.25)$: a long length-scale case with relatively high measurement error.\n4. $(80,1.0,4.0)$: an edge case with very high measurement error approaching prior uncertainty.\n\nFor each parameter set, compute the domain-mean analysis error variance across the grid in $\\,^\\circ\\text{C}^2$. Report each domain-mean value as a float rounded to six decimal places.\n\nAngle units do not apply in this problem. All distances must be in $\\mathrm{km}$, and all variances must be in $\\,^\\circ\\text{C}^2$. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with no spaces, in the order of the parameter sets listed above, for example: $[v_1,v_2,v_3,v_4]$, where each $v_i$ is the domain-mean analysis error variance in $\\,^\\circ\\text{C}^2$ rounded to six decimals.\n\nEnsure scientific realism and internal consistency. The mapping and analysis error must be computed using the OI equations derived from the stated covariance and measurement error assumptions, without using shortcut formulas that bypass those derivations.",
            "solution": "The problem requires the implementation of Optimal Interpolation (OI) to estimate a sea surface temperature (SST) anomaly field and its corresponding analysis error variance. The derivation must proceed from first principles, assuming the SST anomalies constitute a second-order stationary Gaussian random field.\n\nLet the true SST anomaly at a location $\\mathbf{x}$ be denoted by the random variable $t(\\mathbf{x})$. We are given a set of $N=12$ observations at locations $\\mathbf{x}_i$ for $i=1, \\dots, N$. The vector of observations is $\\mathbf{d}^o$, where each observation $d^o_i$ is the sum of the true field value and a measurement error $\\eta_i$:\n$$d^o_i = t(\\mathbf{x}_i) + \\eta_i$$\nIn vector form, this is $\\mathbf{d}^o = \\mathbf{t}^o + \\boldsymbol{\\eta}$, where $\\mathbf{t}^o$ is the vector of true SST anomalies at the observation locations.\n\nThe statistical properties of the field and errors are specified:\n1.  The field $t(\\mathbf{x})$ and error $\\boldsymbol{\\eta}$ are zero-mean Gaussian random variables: $E[t(\\mathbf{x})] = 0$ and $E[\\boldsymbol{\\eta}] = \\mathbf{0}$.\n2.  The covariance of the true field between two points $\\mathbf{x}_i$ and $\\mathbf{x}_j$ is given by the isotropic function $C(r_{ij}) = \\sigma^2\\exp\\left(-\\frac{r_{ij}}{L}\\right)$, where $r_{ij} = ||\\mathbf{x}_i - \\mathbf{x}_j||$ is the Euclidean distance. The matrix of these covariances between all pairs of observation points is the prior signal covariance matrix, $\\mathbf{P}$. Its elements are $P_{ij} = E[t(\\mathbf{x}_i) t(\\mathbf{x}_j)] = C(r_{ij})$.\n3.  The measurement errors are spatially uncorrelated and independent of the signal, with a constant variance $\\epsilon^2$. The measurement error covariance matrix, $\\mathbf{R}$, is therefore diagonal: $R_{ij} = E[\\eta_i \\eta_j] = \\epsilon^2 \\delta_{ij}$, where $\\delta_{ij}$ is the Kronecker delta.\n\nThe goal is to find an estimate, or analysis, $t_g^a$ of the true value $t_g = t(\\mathbf{x}_g)$ at a grid point $\\mathbf{x}_g$. We seek the Best Linear Unbiased Estimator (BLUE).\n\nThe estimator is defined as a linear combination of the observations:\n$$t_g^a = \\mathbf{w}^\\top \\mathbf{d}^o = \\sum_{i=1}^{N} w_i d^o_i$$\nwhere $\\mathbf{w}$ is a vector of weights to be determined.\n\nThe estimator must be unbiased, meaning its expected value equals the expected value of the true quantity: $E[t_g^a] = E[t_g]$.\nGiven the zero-mean assumption for both the field and the observations, we have $E[t_g] = 0$ and $E[t_g^a] = E[\\mathbf{w}^\\top \\mathbf{d}^o] = \\mathbf{w}^\\top E[\\mathbf{d}^o] = \\mathbf{0}$. The unbiased condition is thus satisfied for any choice of weights $\\mathbf{w}$.\n\nThe \"best\" estimator is the one that minimizes the mean squared error, which is the analysis error variance $\\sigma_a^2$:\n$$\\sigma_a^2 = J(\\mathbf{w}) = E[(t_g^a - t_g)^2]$$\nSubstituting the expressions for $t_g^a$ and $\\mathbf{d}^o$:\n$$\\sigma_a^2 = E\\left[ \\left(\\mathbf{w}^\\top (\\mathbf{t}^o + \\boldsymbol{\\eta}) - t_g\\right)^2 \\right]$$\nExpanding and taking the expectation, we use the fact that the signal and noise are uncorrelated ($E[\\mathbf{t}^o \\boldsymbol{\\eta}^\\top]=\\mathbf{0}$, $E[t_g \\boldsymbol{\\eta}^\\top]=\\mathbf{0}$):\n$$\\sigma_a^2 = E\\left[ \\mathbf{w}^\\top \\mathbf{t}^o (\\mathbf{t}^o)^\\top \\mathbf{w} + \\mathbf{w}^\\top \\boldsymbol{\\eta} \\boldsymbol{\\eta}^\\top \\mathbf{w} - 2\\mathbf{w}^\\top \\mathbf{t}^o t_g + t_g^2 \\right]$$\n$$\\sigma_a^2 = \\mathbf{w}^\\top E[\\mathbf{t}^o (\\mathbf{t}^o)^\\top] \\mathbf{w} + \\mathbf{w}^\\top E[\\boldsymbol{\\eta} \\boldsymbol{\\eta}^\\top] \\mathbf{w} - 2\\mathbf{w}^\\top E[\\mathbf{t}^o t_g] + E[t_g^2]$$\n\nWe identify the expectation terms based on the covariance definitions:\n- $E[\\mathbf{t}^o (\\mathbf{t}^o)^\\top] = \\mathbf{P}$, the $N \\times N$ prior signal covariance matrix at observation points.\n- $E[\\boldsymbol{\\eta} \\boldsymbol{\\eta}^\\top] = \\mathbf{R}$, the $N \\times N$ measurement error covariance matrix.\n- $E[t_g^2] = C(0) = \\sigma^2\\exp(0) = \\sigma^2$, the prior variance of the field.\n- $E[\\mathbf{t}^o t_g]$ is an $N \\times 1$ column vector whose $i$-th element is $E[t(\\mathbf{x}_i) t(\\mathbf{x}_g)] = C(\\mathbf{x}_i, \\mathbf{x}_g)$. We denote this vector by $\\mathbf{p}_g$.\n\nSubstituting these into the expression for $\\sigma_a^2$ yields the cost function:\n$$\\sigma_a^2(\\mathbf{w}) = \\mathbf{w}^\\top (\\mathbf{P} + \\mathbf{R}) \\mathbf{w} - 2\\mathbf{w}^\\top \\mathbf{p}_g + \\sigma^2$$\nTo minimize this quadratic function with respect to $\\mathbf{w}$, we compute its gradient and set it to zero:\n$$\\nabla_{\\mathbf{w}} \\sigma_a^2 = 2(\\mathbf{P} + \\mathbf{R})\\mathbf{w} - 2\\mathbf{p}_g = \\mathbf{0}$$\nThis gives the normal equations for the optimal weights:\n$$(\\mathbf{P} + \\mathbf{R})\\mathbf{w} = \\mathbf{p}_g$$\nThe solution for the optimal weight vector $\\mathbf{w}$ is:\n$$\\mathbf{w} = (\\mathbf{P} + \\mathbf{R})^{-1} \\mathbf{p}_g$$\nThe matrix $\\mathbf{P} + \\mathbf{R}$ is the observation covariance matrix, which is invertible as long as $\\mathbf{R}$ has positive diagonal elements (i.e., $\\epsilon^2 > 0$), which is true here.\n\nWith the optimal weights, we can write the final OI equations.\nThe analysis field at grid point $\\mathbf{x}_g$ is:\n$$t_g^a = \\mathbf{w}^\\top \\mathbf{d}^o = \\mathbf{p}_g^\\top (\\mathbf{P} + \\mathbf{R})^{-1} \\mathbf{d}^o$$\nThe analysis error variance is found by substituting the optimal $\\mathbf{w}$ back into the cost function. A simpler form is obtained by noting that $\\mathbf{p}_g^\\top = \\mathbf{w}^\\top (\\mathbf{P}+\\mathbf{R})$:\n$$\\sigma_a^2 = \\sigma^2 - \\mathbf{p}_g^\\top \\mathbf{w} = \\sigma^2 - \\mathbf{p}_g^\\top (\\mathbf{P} + \\mathbf{R})^{-1} \\mathbf{p}_g$$\nThis final equation is central to the problem. It shows that the analysis error variance $\\sigma_a^2$ at any point $\\mathbf{x}_g$ is the prior variance $\\sigma^2$ reduced by an amount that depends on the geometry of the observations relative to the grid point and the statistical parameters $(L, \\sigma^2, \\epsilon^2)$. Crucially, it does not depend on the specific observation values in $\\mathbf{d}^o$.\n\nTo solve the problem, we will implement this formula for each parameter set. The algorithm is as follows:\n1.  Define the fixed observation locations $\\mathbf{x}_i$ and generate the set of grid point locations $\\mathbf{x}_g$.\n2.  For each parameter set $(L, \\sigma^2, \\epsilon^2)$:\n    a. Construct the $N \\times N$ observation covariance matrix $\\mathbf{M} = \\mathbf{P} + \\mathbf{R}$. The elements are $M_{ij} = \\sigma^2\\exp(-||\\mathbf{x}_i - \\mathbf{x}_j||/L) + \\epsilon^2\\delta_{ij}$.\n    b. Compute the inverse $\\mathbf{M}^{-1}$. This is done once per parameter set.\n    c. For each of the $21 \\times 21 = 441$ grid points $\\mathbf{x}_g$:\n        i. Construct the $N \\times 1$ grid-to-observation covariance vector $\\mathbf{p}_g$, with elements $p_{g,i} = \\sigma^2\\exp(-||\\mathbf{x}_g - \\mathbf{x}_i||/L)$.\n        ii. Calculate the analysis error variance: $\\sigma_{a,g}^2 = \\sigma^2 - \\mathbf{p}_g^\\top \\mathbf{M}^{-1} \\mathbf{p}_g$.\n    d. Compute the arithmetic mean of all $441$ values of $\\sigma_{a,g}^2$ to obtain the domain-mean analysis error variance.\n3.  Report the results for all parameter sets, formatted as requested.",
            "answer": "```python\nimport numpy as np\nfrom scipy.spatial.distance import cdist\n\ndef solve():\n    \"\"\"\n    Solves the Optimal Interpolation problem for the given test cases.\n    \"\"\"\n\n    # --- Define problem geometry and constants ---\n    # Observation locations in km\n    obs_locs = np.array([\n        [10., 50.], [40., 50.], [70., 50.], [100., 50.], [130., 50.], [160., 50.], [190., 50.],\n        [120., 20.], [120., 60.], [120., 100.], [120., 140.], [120., 180.]\n    ])\n    num_obs = obs_locs.shape[0]\n\n    # Grid definition in km\n    grid_dim = 21\n    grid_coords = np.linspace(0., 200., grid_dim)\n    gx, gy = np.meshgrid(grid_coords, grid_coords)\n    grid_locs = np.vstack([gx.ravel(), gy.ravel()]).T\n\n    # Test cases: (L, sigma^2, epsilon^2)\n    # L in km, sigma^2 and epsilon^2 in (degC)^2\n    test_cases = [\n        (50.0, 1.0, 0.04),\n        (10.0, 1.0, 0.01),\n        (200.0, 1.0, 0.25),\n        (80.0, 1.0, 4.0),\n    ]\n\n    # As per the problem, observed SST values are constants, though not used\n    # for error variance calculation. We define them here for completeness.\n    def true_field(x, y):\n        # A simple, physically plausible smooth field for anomalies\n        return 1.5 * np.exp(-((x - 100.)**2 + (y - 100.)**2) / (2. * 60.**2))\n\n    true_values_at_obs = np.array([true_field(x, y) for x, y in obs_locs])\n    # Reproducible noise with a small, fixed standard deviation\n    rng = np.random.RandomState(seed=123)\n    noise = rng.normal(0, np.sqrt(0.0225), size=num_obs)\n    # This d_obs vector is a constant, as required by the problem statement.\n    d_obs = true_values_at_obs + noise\n\n    def compute_mean_error_variance(L, sigma_sq, epsilon_sq, obs_locs, grid_locs):\n        \"\"\"\n        Computes the domain-mean analysis error variance for a given set of parameters.\n        \n        Args:\n            L (float): Covariance length scale in km.\n            sigma_sq (float): Prior variance in (degC)^2.\n            epsilon_sq (float): Measurement error variance in (degC)^2.\n            obs_locs (np.ndarray): Array of observation locations (N_obs, 2).\n            grid_locs (np.ndarray): Array of grid locations (N_grid, 2).\n\n        Returns:\n            float: The domain-mean analysis error variance.\n        \"\"\"\n        n_obs = obs_locs.shape[0]\n\n        # 1. Construct observation covariance matrix M = P + R\n        # P: prior signal covariance matrix between observations\n        dist_obs_obs = cdist(obs_locs, obs_locs, 'euclidean')\n        P = sigma_sq * np.exp(-dist_obs_obs / L)\n        \n        # R: measurement error covariance matrix (diagonal)\n        R = np.eye(n_obs) * epsilon_sq\n        \n        M = P + R\n        \n        # 2. Invert the matrix M\n        try:\n            M_inv = np.linalg.inv(M)\n        except np.linalg.LinAlgError:\n            # Add a small nugget for stability if matrix is singular, though\n            # epsilon_sq > 0 should prevent this.\n            M += np.eye(n_obs) * 1e-9\n            M_inv = np.linalg.inv(M)\n\n        # 3. Compute analysis error variance for each grid point\n        # p_g: grid-to-observation covariance vector\n        dist_grid_obs = cdist(grid_locs, obs_locs, 'euclidean')\n        P_go = sigma_sq * np.exp(-dist_grid_obs / L) # Shape: (n_grid, n_obs)\n        \n        # Variance reduction term: p_g^T * M^-1 * p_g for each grid point\n        # This can be vectorized efficiently.\n        # einsum 'ik,kl,lj->i' does (P_go @ M_inv @ P_go.T) and takes the diagonal.\n        # A more direct way is sum( (P_go @ M_inv) * P_go, axis=1)\n        variance_reduction = np.sum((P_go @ M_inv) * P_go, axis=1)\n        \n        # Analysis error variance map: sigma_a^2 = sigma^2 - reduction\n        analysis_error_variance_map = sigma_sq - variance_reduction\n        \n        # 4. Compute domain-mean of the analysis error variance\n        mean_error_variance = np.mean(analysis_error_variance_map)\n        \n        return mean_error_variance\n\n    results = []\n    for case in test_cases:\n        L, sigma_sq, epsilon_sq = case\n        mean_var = compute_mean_error_variance(L, sigma_sq, epsilon_sq, obs_locs, grid_locs)\n        results.append(f\"{mean_var:.6f}\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}