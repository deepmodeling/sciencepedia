{
    "hands_on_practices": [
        {
            "introduction": "我们从一个基础练习开始，它将变分数据同化问题简化至其核心。通过在一个理想化的线性高斯框架下推导最优分析解，您将亲手揭示系统如何根据背景和观测各自的不确定性来对它们进行最优加权。这个练习 () 是理解所有更复杂的数据同化方案的基石。",
            "id": "3929900",
            "problem": "考虑地球系统状态变分数据同化中的一个分析步骤。设状态向量为 $\\mathbf{x} \\in \\mathbb{R}^n$，背景（先验）状态为 $\\mathbf{x}_b \\in \\mathbb{R}^n$，观测为 $\\mathbf{y} \\in \\mathbb{R}^n$。假设线性观测算子 $\\mathbf{H} = \\mathbf{I}$，背景误差协方差 $\\mathbf{B} = \\sigma_b^2 \\mathbf{I}$，以及观测误差协方差 $\\mathbf{R} = \\sigma_o^2 \\mathbf{I}$，其中 $\\sigma_b^2 > 0$ 和 $\\sigma_o^2 > 0$，$I$ 表示适当维度的单位矩阵。假设背景误差和观测误差是独立的、零均值的高斯分布，其协方差如上所述。使用最大后验（MAP）估计，推导分析状态 $\\mathbf{x}_a$ 的闭式表达式，该表达式最小化负对数后验（等效于标准的二次变分数据同化目标函数），并用 $\\mathbf{x}_b$、$\\mathbf{y}$、$\\sigma_b^2$ 和 $\\sigma_o^2$ 表示。此外，在您的推导中，请指明 $\\mathbf{H}$ 的伴随算子在一阶最优性条件中的作用，并解释当 $\\sigma_b^2 \\to 0$ 和 $\\sigma_o^2 \\to 0$ 时的极限行为。请以单一闭式解析表达式的形式提供最终的 MAP 分析 $\\mathbf{x}_a$。不需要进行数值舍入，且 $\\mathbf{x}$、$\\mathbf{x}_b$ 或 $\\mathbf{y}$ 均不关联物理单位。",
            "solution": "问题要求在一个简化的变分数据同化场景中，使用最大后验（MAP）估计推导分析状态 $\\mathbf{x}_a$。状态向量为 $\\mathbf{x} \\in \\mathbb{R}^n$。给定背景（先验）状态 $\\mathbf{x}_b \\in \\mathbb{R}^n$，观测 $\\mathbf{y} \\in \\mathbb{R}^n$，线性观测算子 $\\mathbf{H} = \\mathbf{I}$，背景误差协方差矩阵 $\\mathbf{B} = \\sigma_b^2 \\mathbf{I}$，以及观测误差协方差矩阵 $\\mathbf{R} = \\sigma_o^2 \\mathbf{I}$。方差 $\\sigma_b^2$ 和 $\\sigma_o^2$ 是正标量，$\\mathbf{I}$ 是单位矩阵。\n\n状态 $\\mathbf{x}$ 的 MAP 估计是使后验概率密度函数 $p(\\mathbf{x}|\\mathbf{y})$ 最大化的状态。根据贝叶斯定理，后验概率与似然和先验的乘积成正比：\n$$p(\\mathbf{x}|\\mathbf{y}) \\propto p(\\mathbf{y}|\\mathbf{x}) p(\\mathbf{x})$$\n问题陈述背景误差和观测误差是独立的、零均值的高斯分布。\n基于背景信息，状态 $\\mathbf{x}$ 的先验分布由 $\\mathbf{x} \\sim \\mathcal{N}(\\mathbf{x}_b, \\mathbf{B})$ 给出。其概率密度函数为：\n$$p(\\mathbf{x}) \\propto \\exp\\left(-\\frac{1}{2}(\\mathbf{x} - \\mathbf{x}_b)^\\mathsf{T} \\mathbf{B}^{-1} (\\mathbf{x} - \\mathbf{x}_b)\\right)$$\n似然，即给定状态 $\\mathbf{x}$ 时观测 $\\mathbf{y}$ 的概率，基于模型 $\\mathbf{y} = \\mathbf{H}\\mathbf{x} + \\epsilon$，其中误差 $\\epsilon \\sim \\mathcal{N}(0, \\mathbf{R})$。当 $\\mathbf{H}=\\mathbf{I}$ 时，模型变为 $\\mathbf{y} = \\mathbf{x} + \\epsilon$，因此 $\\mathbf{y}|\\mathbf{x} \\sim \\mathcal{N}(\\mathbf{x}, \\mathbf{R})$。其概率密度函数为：\n$$p(\\mathbf{y}|\\mathbf{x}) \\propto \\exp\\left(-\\frac{1}{2}(\\mathbf{y} - \\mathbf{H}\\mathbf{x})^\\mathsf{T} \\mathbf{R}^{-1} (\\mathbf{y} - \\mathbf{H}\\mathbf{x})\\right)$$\n最大化后验概率 $p(\\mathbf{x}|\\mathbf{y})$ 等价于最大化其对数 $\\ln(p(\\mathbf{x}|\\mathbf{y}))$，而这又等价于最小化其负对数。这就产生了变分代价函数，通常表示为 $J(\\mathbf{x})$。\n$$J(\\mathbf{x}) = \\frac{1}{2}(\\mathbf{x} - \\mathbf{x}_b)^\\mathsf{T} \\mathbf{B}^{-1} (\\mathbf{x} - \\mathbf{x}_b) + \\frac{1}{2}(\\mathbf{y} - \\mathbf{H}\\mathbf{x})^\\mathsf{T} \\mathbf{R}^{-1} (\\mathbf{y} - \\mathbf{H}\\mathbf{x})$$\n分析状态 $\\mathbf{x}_a$ 是使该代价函数最小化的 $\\mathbf{x}$ 的值。\n\n给定 $\\mathbf{H}=\\mathbf{I}$，$\\mathbf{B} = \\sigma_b^2 \\mathbf{I}$ 和 $\\mathbf{R} = \\sigma_o^2 \\mathbf{I}$。协方差矩阵的逆分别为 $\\mathbf{B}^{-1} = (\\sigma_b^2 \\mathbf{I})^{-1} = \\frac{1}{\\sigma_b^2}\\mathbf{I}$ 和 $\\mathbf{R}^{-1} = (\\sigma_o^2 \\mathbf{I})^{-1} = \\frac{1}{\\sigma_o^2}\\mathbf{I}$。将这些代入代价函数得到：\n$$J(\\mathbf{x}) = \\frac{1}{2}(\\mathbf{x} - \\mathbf{x}_b)^\\mathsf{T} \\left(\\frac{1}{\\sigma_b^2}\\mathbf{I}\\right) (\\mathbf{x} - \\mathbf{x}_b) + \\frac{1}{2}(\\mathbf{y} - \\mathbf{I}\\mathbf{x})^\\mathsf{T} \\left(\\frac{1}{\\sigma_o^2}\\mathbf{I}\\right) (\\mathbf{y} - \\mathbf{I}\\mathbf{x})$$\n$$J(\\mathbf{x}) = \\frac{1}{2\\sigma_b^2}(\\mathbf{x} - \\mathbf{x}_b)^\\mathsf{T}(\\mathbf{x} - \\mathbf{x}_b) + \\frac{1}{2\\sigma_o^2}(\\mathbf{y} - \\mathbf{x})^\\mathsf{T}(\\mathbf{y} - \\mathbf{x})$$\n为了找到这个二次凸代价函数的最小值，我们计算它关于 $\\mathbf{x}$ 的梯度并将其设为零。这是一阶最优性条件。一般代价函数的梯度为：\n$$\\nabla_\\mathbf{x} J(\\mathbf{x}) = \\mathbf{B}^{-1}(\\mathbf{x} - \\mathbf{x}_b) - \\mathbf{H}^\\mathsf{T} \\mathbf{R}^{-1}(\\mathbf{y} - \\mathbf{H}\\mathbf{x})$$\n这里，算子 $\\mathbf{H}^\\mathsf{T}$ 是观测算子 $\\mathbf{H}$ 的伴随算子。它的作用至关重要，因为它将观测空间残差向量 $(\\mathbf{y}-\\mathbf{H}\\mathbf{x})$ 映射回状态空间，在那里可以与状态空间背景残差 $(\\mathbf{x}-\\mathbf{x}_b)$ 相结合。对于这个问题，$\\mathbf{H}=\\mathbf{I}$，所以其伴随算子 $\\mathbf{H}^\\mathsf{T}$ 也是单位矩阵 $\\mathbf{I}$。因此，梯度变为：\n$$\\nabla_\\mathbf{x} J(\\mathbf{x}) = \\left(\\frac{1}{\\sigma_b^2}\\mathbf{I}\\right)(\\mathbf{x} - \\mathbf{x}_b) - \\mathbf{I}^\\mathsf{T} \\left(\\frac{1}{\\sigma_o^2}\\mathbf{I}\\right)(\\mathbf{y} - \\mathbf{I}\\mathbf{x})$$\n$$\\nabla_\\mathbf{x} J(\\mathbf{x}) = \\frac{1}{\\sigma_b^2}(\\mathbf{x} - \\mathbf{x}_b) - \\frac{1}{\\sigma_o^2}(\\mathbf{y} - \\mathbf{x}) = \\frac{1}{\\sigma_b^2}(\\mathbf{x} - \\mathbf{x}_b) + \\frac{1}{\\sigma_o^2}(\\mathbf{x} - \\mathbf{y})$$\n将分析状态 $\\mathbf{x} = \\mathbf{x}_a$ 处的梯度设为零：\n$$\\nabla_\\mathbf{x} J(\\mathbf{x}_a) = \\frac{1}{\\sigma_b^2}(\\mathbf{x}_a - \\mathbf{x}_b) + \\frac{1}{\\sigma_o^2}(\\mathbf{x}_a - \\mathbf{y}) = 0$$\n现在我们求解 $\\mathbf{x}_a$：\n$$\\mathbf{x}_a \\left(\\frac{1}{\\sigma_b^2}\\right) - \\frac{\\mathbf{x}_b}{\\sigma_b^2} + \\mathbf{x}_a \\left(\\frac{1}{\\sigma_o^2}\\right) - \\frac{\\mathbf{y}}{\\sigma_o^2} = 0$$\n$$\\mathbf{x}_a \\left(\\frac{1}{\\sigma_b^2} + \\frac{1}{\\sigma_o^2}\\right) = \\frac{\\mathbf{x}_b}{\\sigma_b^2} + \\frac{\\mathbf{y}}{\\sigma_o^2}$$\n$$\\mathbf{x}_a \\left(\\frac{\\sigma_o^2 + \\sigma_b^2}{\\sigma_b^2 \\sigma_o^2}\\right) = \\frac{\\sigma_o^2 \\mathbf{x}_b + \\sigma_b^2 \\mathbf{y}}{\\sigma_b^2 \\sigma_o^2}$$\n两边同乘以 $\\sigma_b^2 \\sigma_o^2$ 并除以 $(\\sigma_o^2 + \\sigma_b^2)$，得到分析状态 $\\mathbf{x}_a$ 的闭式表达式：\n$$\\mathbf{x}_a = \\frac{\\sigma_o^2 \\mathbf{x}_b + \\sigma_b^2 \\mathbf{y}}{\\sigma_o^2 + \\sigma_b^2}$$\n这个表达式表明，分析是背景状态 $\\mathbf{x}_b$ 和观测 $\\mathbf{y}$ 的加权平均。权重与其各自的误差方差成反比。\n\n接下来，我们分析极限行为。\n1.  当对背景的置信度变得完美时，其误差方差趋近于零：$\\sigma_b^2 \\to 0$。\n    $$\\lim_{\\sigma_b^2 \\to 0} \\mathbf{x}_a = \\lim_{\\sigma_b^2 \\to 0} \\frac{\\sigma_o^2 \\mathbf{x}_b + \\sigma_b^2 \\mathbf{y}}{\\sigma_o^2 + \\sigma_b^2} = \\frac{\\sigma_o^2 \\mathbf{x}_b + (0) \\mathbf{y}}{\\sigma_o^2 + 0} = \\frac{\\sigma_o^2 \\mathbf{x}_b}{\\sigma_o^2} = \\mathbf{x}_b$$\n    在此极限下，分析状态收敛于背景状态。观测被忽略，因为背景被认为是完全准确的。\n\n2.  当对观测的置信度变得完美时，其误差方差趋近于零：$\\sigma_o^2 \\to 0$。\n    $$\\lim_{\\sigma_o^2 \\to 0} \\mathbf{x}_a = \\lim_{\\sigma_o^2 \\to 0} \\frac{\\sigma_o^2 \\mathbf{x}_b + \\sigma_b^2 \\mathbf{y}}{\\sigma_o^2 + \\sigma_b^2} = \\frac{(0) \\mathbf{x}_b + \\sigma_b^2 \\mathbf{y}}{0 + \\sigma_b^2} = \\frac{\\sigma_b^2 \\mathbf{y}}{\\sigma_b^2} = \\mathbf{y}$$\n    在此极限下，分析状态收敛于观测。背景被忽略，因为观测被认为是完全准确的。\n\n这些极限行为与最优估计的原理一致，即最终估计由不确定性最低的信息源主导。",
            "answer": "$$\n\\boxed{\\mathbf{x}_a = \\frac{\\sigma_o^2 \\mathbf{x}_b + \\sigma_b^2 \\mathbf{y}}{\\sigma_b^2 + \\sigma_o^2}}\n$$"
        },
        {
            "introduction": "在现实世界中，地球系统模型和观测算子通常是高度非线性的，这使得我们无法直接求得代价函数的解析最小值。我们必须依赖迭代优化算法，而这些算法的核心需求是代价函数的梯度。这个练习 () 让您通过反向模式自动微分（即伴随方法）的原理，亲手计算一个非线性函数的梯度，从而掌握高效计算复杂模型梯度的关键技术。",
            "id": "3929915",
            "problem": "在用于环境与地球系统建模的变分数据同化框架中，考虑一个标量观测算子 $f(x,y)$，它将两个无量纲控制变量 $x$ 和 $y$ （例如，标准化的温度和湿度）映射到一个用于四维变分同化（4D-Var）代价函数中的合成观测值。该算子定义为 $f(x,y) = \\exp(xy + \\sin x)$。伴随方法和自动微分（AD）被用来获取变分算法所需的梯度。\n\n从基本原理出发，即微分学中的链式法则和反向模式伴随范式（将输出伴随的初值设为 $1$ 并沿计算图向后传播），计算 $f$ 关于 $x$ 和 $y$ 的反向模式伴随导数，即分量 $\\partial f / \\partial x$ 和 $\\partial f / \\partial y$，并将其表示为 $x$ 和 $y$ 的解析函数。将输入的反向模式伴随解释为在输出伴随初值为 $1$ 的情况下计算得到的 $f$ 的梯度。\n\n以闭合形式给出 $\\partial f / \\partial x$ 和 $\\partial f / \\partial y$ 的最终表达式。将最终答案表示为一个包含两个解析表达式的单行矩阵。无需进行数值计算。无需单位。",
            "solution": "所述问题是有效的。其科学基础是微分学和反向模式自动微分（AD）的原理，这也是变分数据同化中使用的伴随方法的数学基础。该问题是适定的、客观的，并包含了推导唯一解析解所需的所有信息。\n\n目标是计算标量函数 $f(x, y)$ 关于其输入变量 $x$ 和 $y$ 的偏导数。函数由下式给出：\n$$f(x, y) = \\exp(xy + \\sin x)$$\n反向模式伴随方法为计算函数梯度提供了一种高效的方式。其核心原理是微积分的链式法则，通过定义函数的一系列运算反向应用。在 AD 的背景下，这个运算序列由一个计算图表示。一个变量的“伴随”是它相对于最终输出的偏导数。对于输入 $x$ 和 $y$，它们的伴随 $\\bar{x}$ 和 $\\bar{y}$ 定义为：\n$$\\bar{x} \\equiv \\frac{\\partial f}{\\partial x} \\quad \\text{and} \\quad \\bar{y} \\equiv \\frac{\\partial f}{\\partial y}$$\n反向模式过程始于将输出变量的伴随初值设为 $1$，即 $\\bar{f} = \\frac{\\partial f}{\\partial f} = 1$，然后向后传播敏感度。\n\n首先，我们将函数 $f(x, y)$ 分解为一系列基本运算（“前向传播”）：\n1. $v_1 = \\sin x$\n2. $v_2 = x \\cdot y$\n3. $v_3 = v_1 + v_2$\n4. $f = \\exp(v_3)$\n\n接下来，我们执行“反向传播”来计算伴随。我们从输出 $f$ 开始，反向推导至输入 $x$ 和 $y$。变量 $u$ 的伴随表示为 $\\bar{u}$。对于一个运算 $w = g(u_1, u_2, \\dots)$，其基本法则是输入的伴随计算方式为 $\\bar{u}_i = \\bar{u}_i + \\bar{w} \\frac{\\partial g}{\\partial u_i}$。对于出现在多个运算中的变量，累加（加法）至关重要。\n\n**步骤a：初始化输出伴随。**\n过程始于将最终输出的伴随设为 $1$。\n$$\\bar{f} = 1$$\n\n**步骤b：计算 $v_3$ 的伴随。**\n运算为 $f = \\exp(v_3)$。应用链式法则：\n$$\\bar{v}_3 = \\frac{\\partial f}{\\partial v_3} \\bar{f} = \\frac{d}{dv_3}(\\exp(v_3)) \\cdot \\bar{f} = \\exp(v_3) \\cdot 1 = \\exp(v_3)$$\n代入 $v_3$ 的表达式：\n$$\\bar{v}_3 = \\exp(xy + \\sin x)$$\n\n**步骤c：计算 $v_1$ 和 $v_2$ 的伴随。**\n运算为 $v_3 = v_1 + v_2$。伴随 $\\bar{v}_3$ 向后传播到 $\\bar{v}_1$ 和 $\\bar{v}_2$。\n$$\\bar{v}_1 = \\frac{\\partial v_3}{\\partial v_1} \\bar{v}_3 = 1 \\cdot \\bar{v}_3 = \\exp(xy + \\sin x)$$\n$$\\bar{v}_2 = \\frac{\\partial v_3}{\\partial v_2} \\bar{v}_3 = 1 \\cdot \\bar{v}_3 = \\exp(xy + \\sin x)$$\n\n**步骤d：计算 $y$ 的伴随。**\n变量 $y$ 是运算 $v_2 = x \\cdot y$ 的一个输入。其伴随 $\\bar{y}$ 由 $\\bar{v}_2$ 计算得出。\n$$\\bar{y} = \\frac{\\partial v_2}{\\partial y} \\bar{v}_2 = x \\cdot \\bar{v}_2 = x \\exp(xy + \\sin x)$$\n这就是偏导数 $\\frac{\\partial f}{\\partial y}$。\n\n**步骤e：计算 $x$ 的伴随。**\n变量 $x$ 是两个运算的输入：$v_1 = \\sin x$ 和 $v_2 = x \\cdot y$。它的总伴随 $\\bar{x}$ 是来自两条路径贡献的总和。\n来自 $v_1$ 的贡献：\n$$ \\left( \\frac{\\partial v_1}{\\partial x} \\right) \\bar{v}_1 = \\left( \\frac{d}{dx}(\\sin x) \\right) \\bar{v}_1 = (\\cos x) \\cdot \\bar{v}_1 = (\\cos x) \\exp(xy + \\sin x) $$\n来自 $v_2$ 的贡献：\n$$ \\left( \\frac{\\partial v_2}{\\partial x} \\right) \\bar{v}_2 = y \\cdot \\bar{v}_2 = y \\exp(xy + \\sin x) $$\n$x$ 的总伴随是这两个贡献的总和：\n$$\\bar{x} = (\\cos x) \\exp(xy + \\sin x) + y \\exp(xy + \\sin x)$$\n提出指数项：\n$$\\bar{x} = (y + \\cos x) \\exp(xy + \\sin x)$$\n这就是偏导数 $\\frac{\\partial f}{\\partial x}$。\n\n输入的最终伴随即为 $f$ 梯度的分量：\n$$\\frac{\\partial f}{\\partial x} = (y + \\cos x) \\exp(xy + \\sin x)$$\n$$\\frac{\\partial f}{\\partial y} = x \\exp(xy + \\sin x)$$\n这些表达式就是所求的反向模式伴随导数，以闭合解析形式给出。它们表示输出 $f$ 对输入 $x$ 和 $y$ 无穷小变化的敏感度。",
            "answer": "$$\n\\boxed{\\begin{pmatrix} (y + \\cos x) \\exp(xy + \\sin x)  x \\exp(xy + \\sin x) \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "这个综合性练习将前面的概念——代价函数及其梯度——付诸实践。您将为一个现实的（尽管是小规模的）非线性三维变分（3D-Var）问题，实现迭代优化算法中的一个关键部分：满足沃尔夫（Wolfe）条件的线搜索。通过这个编码练习 ()，您将把理论知识转化为实际的计算实现，填补理论与数据同化系统实际运作之间的鸿沟。",
            "id": "3929916",
            "problem": "你的任务是实现一个稳健的线搜索过程，该过程须满足 Wolfe 条件，用于环境与地球系统建模中常用的非线性三维变分 (3D-Var) 代价泛函。此问题必须通过变分资料同化和优化的基本定义推导来解决。最终目标是使用 Wolfe 条件计算沿给定搜索方向的可接受步长，并基于方向导数和近似 Hessian 矩阵，采用有原则的初始步长选择方法。\n\n给定三维变分 (3D-Var) 代价泛函\n$$\nJ(\\mathbf{x}) = \\tfrac{1}{2}\\,(\\mathbf{x} - \\mathbf{x}_b)^\\top \\mathbf{B}^{-1} (\\mathbf{x} - \\mathbf{x}_b) + \\tfrac{1}{2}\\,(\\mathbf{y} - H(\\mathbf{x}))^\\top \\mathbf{R}^{-1} (\\mathbf{y} - H(\\mathbf{x})),\n$$\n其中 $\\mathbf{x} \\in \\mathbb{R}^n$ 是状态 ($n=3$)，$\\mathbf{x}_b \\in \\mathbb{R}^n$ 是背景场状态，$\\mathbf{B} \\in \\mathbb{R}^{n \\times n}$ 是背景场误差协方差矩阵，$\\mathbf{y} \\in \\mathbb{R}^m$ 是观测向量 ($m=3$)，$\\mathbf{R} \\in \\mathbb{R}^{m \\times m}$ 是观测误差协方差矩阵，以及 $H: \\mathbb{R}^n \\to \\mathbb{R}^m$ 是一个非线性观测算子，请实现以下内容：\n\n1. 使用非线性观测算子\n$$\nH(\\mathbf{x}) =\n\\begin{bmatrix}\nx_1^2 + \\sin(x_2) \\\\\nx_2 \\exp(x_1) \\\\\n\\tanh(x_3) + x_1 x_3\n\\end{bmatrix},\n$$\n及其雅可比矩阵\n$$\nH'(\\mathbf{x}) =\n\\begin{bmatrix}\n2x_1  \\cos(x_2)  0 \\\\\nx_2 \\exp(x_1)  \\exp(x_1)  0 \\\\\nx_3  0  \\operatorname{sech}^2(x_3) + x_1\n\\end{bmatrix}.\n$$\n\n2. 使用复合函数的基本求导法则计算代价泛函的梯度：\n$$\n\\nabla J(\\mathbf{x}) = \\mathbf{B}^{-1}(\\mathbf{x} - \\mathbf{x}_b) - H'(\\mathbf{x})^\\top \\mathbf{R}^{-1} \\left(\\mathbf{y} - H(\\mathbf{x})\\right).\n$$\n\n3. 对于给定的搜索方向 $\\mathbf{p} \\in \\mathbb{R}^n$，定义沿该线的单变量函数：\n$$\n\\phi(\\alpha) = J(\\mathbf{x} + \\alpha \\mathbf{p}), \\quad \\phi'(\\alpha) = \\nabla J(\\mathbf{x} + \\alpha \\mathbf{p})^\\top \\mathbf{p},\n$$\n并实现一个强 Wolfe 线搜索，找到满足强 Wolfe 条件的 $\\alpha > 0$：\n- 充分下降 (Armijo) 条件：\n$$\n\\phi(\\alpha) \\le \\phi(0) + c_1 \\alpha \\phi'(0),\n$$\n- 曲率条件：\n$$\n\\left|\\phi'(\\alpha)\\right| \\le c_2 \\left|\\phi'(0)\\right|,\n$$\n其中 $c_1 \\in (0,1)$ 且 $c_2 \\in (c_1,1)$。\n\n4. 基于方向导数和 Hessian 矩阵的 Gauss-Newton 近似所构成的二次模型，选择初始步长 $\\alpha_0$。使用近似\n$$\n\\nabla^2 J(\\mathbf{x}) \\approx \\mathbf{B}^{-1} + H'(\\mathbf{x})^\\top \\mathbf{R}^{-1} H'(\\mathbf{x}),\n$$\n选择\n$$\n\\alpha_0 = -\\frac{\\phi'(0)}{\\mathbf{p}^\\top\\left(\\mathbf{B}^{-1} + H'(\\mathbf{x})^\\top \\mathbf{R}^{-1} H'(\\mathbf{x})\\right)\\mathbf{p}},\n$$\n如果分母非正或过小，则需有稳健的备用方案。此选择必须从最小化 $\\phi(\\alpha)$ 的局部二次近似推导得出。\n\n所有矩阵 $\\mathbf{B}$ 和 $\\mathbf{R}$ 都是对称正定的；具体来说，使用\n$$\n\\mathbf{B} = \\operatorname{diag}(2.0,\\,1.0,\\,3.0), \\quad \\mathbf{R} = \\operatorname{diag}(0.7,\\,0.5,\\,0.9),\n$$\n以及它们的逆矩阵 $\\mathbf{B}^{-1}$ 和 $\\mathbf{R}^{-1}$。背景场状态和观测值为\n$$\n\\mathbf{x}_b = \\begin{bmatrix} 0.8 \\\\ -0.5 \\\\ 0.3 \\end{bmatrix}, \\quad\n\\mathbf{y} = \\begin{bmatrix} 1.0 \\\\ -0.2 \\\\ 0.7 \\end{bmatrix}.\n$$\n\n使用区间限定与缩放 (bracketing and zoom) 策略实现强 Wolfe 线搜索，参数为 $c_1 = 10^{-4}$，$c_2 = 0.9$，最大步长界限为 $\\alpha_{\\max} = 10$，以及合理的迭代次数限制。如果该方向不是下降方向，即如果 $\\phi'(0) \\ge 0$，则返回 $\\alpha = 0$。\n\n测试套件：\n你必须对以下四个测试用例运行线搜索，每个用例都指定了初始状态 $\\mathbf{x}_0$ 和搜索方向 $\\mathbf{p}$。\n\n- 用例 1 (正常路径)：$\\mathbf{x}_0 = [0.5,\\,-0.2,\\,0.1]^\\top$，$\\mathbf{p} = -\\nabla J(\\mathbf{x}_0)$。\n- 用例 2 (缩放下降方向)：$\\mathbf{x}_0 = [1.5,\\,1.0,\\,-0.8]^\\top$，$\\mathbf{p} = -5\\,\\nabla J(\\mathbf{x}_0)$。\n- 用例 3 (大曲率区域)：$\\mathbf{x}_0 = [3.0,\\,3.0,\\,3.0]^\\top$，$\\mathbf{p} = -\\nabla J(\\mathbf{x}_0)$。\n- 用例 4 (非下降方向边缘情况)：$\\mathbf{x}_0 = [0.2,\\,0.1,\\,-0.1]^\\top$，$\\mathbf{p} = +\\nabla J(\\mathbf{x}_0)$。\n\n对于每个用例，计算在上述设置下满足强 Wolfe 条件的可接受步长 $\\alpha$，并返回该标量。\n\n最终输出格式：\n你的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果（例如，“[result1,result2,result3,result4]”）。结果必须是按上述顺序排列的四个测试用例的可接受步长 $\\alpha$。所有值均为不带单位的实数。",
            "solution": "该问题是良态的、有科学依据且自包含的。解决方案所需的所有组件都已提供：一个明确定义的代价泛函 $J(\\mathbf{x})$、其梯度 $\\nabla J(\\mathbf{x})$、非线性算子 $H(\\mathbf{x})$ 及其雅可比矩阵 $H'(\\mathbf{x})$、所有常数和向量（$\\mathbf{B}$、$\\mathbf{R}$、$\\mathbf{x}_b$、$\\mathbf{y}$）的具体值，以及一组明确的测试用例。任务是实现一个满足强 Wolfe 条件的线搜索算法，这是非线性优化中的一个标准和基本过程。3D-Var 公式、梯度计算、Gauss-Newton Hessian 近似以及 Wolfe 条件的理论基础都是正确的，并且基于数值优化和资料同化中的既定原则。因此，该问题是有效的。\n\n目标是从一个点 $\\mathbf{x} \\in \\mathbb{R}^n$ 开始，沿着给定的搜索方向 $\\mathbf{p} \\in \\mathbb{R}^n$ 找到一个合适的步长 $\\alpha > 0$。该步长必须满足单变量函数 $\\phi(\\alpha) = J(\\mathbf{x} + \\alpha \\mathbf{p})$ 的强 Wolfe 条件。\n\n我们给定了 3D-Var 代价泛函：\n$$\nJ(\\mathbf{x}) = \\tfrac{1}{2}\\,(\\mathbf{x} - \\mathbf{x}_b)^\\top \\mathbf{B}^{-1} (\\mathbf{x} - \\mathbf{x}_b) + \\tfrac{1}{2}\\,(\\mathbf{y} - H(\\mathbf{x}))^\\top \\mathbf{R}^{-1} (\\mathbf{y} - H(\\mathbf{x}))\n$$\n使用标准向量微积分法则和链式法则推导，此泛函的梯度为：\n$$\n\\nabla J(\\mathbf{x}) = \\mathbf{B}^{-1}(\\mathbf{x} - \\mathbf{x}_b) - H'(\\mathbf{x})^\\top \\mathbf{R}^{-1} \\left(\\mathbf{y} - H(\\mathbf{x})\\right)\n$$\n其中 $H'(\\mathbf{x})$ 是非线性观测算子 $H(\\mathbf{x})$ 的雅可比矩阵。\n\n线搜索是在单变量函数 $\\phi(\\alpha) = J(\\mathbf{x} + \\alpha \\mathbf{p})$ 上执行的，其关于 $\\alpha$ 的导数由链式法则给出：\n$$\n\\phi'(\\alpha) = \\frac{d}{d\\alpha} J(\\mathbf{x} + \\alpha \\mathbf{p}) = \\nabla J(\\mathbf{x} + \\alpha \\mathbf{p})^\\top \\frac{d}{d\\alpha}(\\mathbf{x} + \\alpha \\mathbf{p}) = \\nabla J(\\mathbf{x} + \\alpha \\mathbf{p})^\\top \\mathbf{p}\n$$\n起点的方向导数是 $\\phi'(0) = \\nabla J(\\mathbf{x})^\\top \\mathbf{p}$。为使步长 $\\alpha$ 对应于代价函数的减小，搜索方向 $\\mathbf{p}$ 必须是下降方向，即 $\\phi'(0)  0$。如果不满足此条件，则任何正步长都不能保证减小，唯一正确的操作是返回 $\\alpha = 0$。\n\n可接受的步长 $\\alpha$ 必须满足强 Wolfe 条件：\n1.  **充分下降条件 (Armijo 条件):** 这确保步长能导致代价泛函有意义的减小。\n    $$\n    \\phi(\\alpha) \\le \\phi(0) + c_1 \\alpha \\phi'(0)\n    $$\n2.  **强曲率条件:** 这确保新点的斜率得到充分减小，防止步长过短。\n    $$\n    |\\phi'(\\alpha)| \\le c_2 |\\phi'(0)|\n    $$\n参数 $c_1$ 和 $c_2$ 是满足 $0  c_1  c_2  1$ 的常数。对于此问题，我们使用 $c_1 = 10^{-4}$ 和 $c_2 = 0.9$。\n\n找到这样的 $\\alpha$ 需要一个系统的搜索策略。一个稳健且常用的方法，如题目所指定，是一个涉及区间限定和缩放的两阶段过程。在开始搜索之前，需要对步长 $\\alpha_0$ 进行一个好的初始猜测。一个有原则的 $\\alpha_0$ 选择可以通过最小化 $\\phi(\\alpha)$ 在 $\\alpha=0$ 附近的局部二次模型来推导：\n$$\n\\phi(\\alpha) \\approx \\phi(0) + \\alpha\\phi'(0) + \\tfrac{1}{2}\\alpha^2\\phi''(0)\n$$\n这个二次函数的最小值出现在其导数为零的地方，从而得到 $\\alpha = -\\phi'(0)/\\phi''(0)$。二阶导数是 $\\phi''(0) = \\mathbf{p}^\\top \\nabla^2 J(\\mathbf{x}) \\mathbf{p}$。我们使用代价泛函 Hessian 矩阵的 Gauss-Newton 近似：\n$$\n\\nabla^2 J(\\mathbf{x}) \\approx \\tilde{\\mathbf{B}} = \\mathbf{B}^{-1} + H'(\\mathbf{x})^\\top \\mathbf{R}^{-1} H'(\\mathbf{x})\n$$\n这导致了指定的初始步长：\n$$\n\\alpha_0 = -\\frac{\\phi'(0)}{\\mathbf{p}^\\top \\tilde{\\mathbf{B}} \\mathbf{p}}\n$$\n为使该估计有用，分母必须为正。如果 $\\mathbf{p}^\\top \\tilde{\\mathbf{B}} \\mathbf{p} \\le 0$，则二次模型没有正的最小值，必须使用一个备用的默认值（例如 $\\alpha_0 = 1.0$）。\n\n所实现的线搜索算法按以下步骤进行：\n1.  通过检查 $\\phi'(0)  0$ 来验证搜索方向 $\\mathbf{p}$ 是否为下降方向。如果不是，则返回 $\\alpha = 0$。\n2.  使用二次模型近似计算初始步长 $\\alpha_0$。如果分母不够大（非正），则使用默认初始步长 $\\alpha_0 = 1.0$。初始猜测值会被最大允许步长 $\\alpha_{\\max} = 10$ 截断。\n3.  **区间限定阶段：** 从 $\\alpha_1 = \\alpha_0$ 开始，迭代寻找一个保证包含满足 Wolfe 条件的点的区间 $[\\alpha_{lo}, \\alpha_{hi}]$。\n    -   在每次迭代 $i$ 中，评估 $\\phi(\\alpha_i)$。如果 $\\alpha_i$ 违反了 Armijo 条件或 $\\phi(\\alpha_i) \\ge \\phi(\\alpha_{i-1})$，则找到了一个区间，我们进入缩放阶段。\n    -   如果满足强曲率条件，$\\alpha_i$ 是一个可接受的步长并被返回。\n    -   如果导数 $\\phi'(\\alpha_i)$ 变为非负，则找到了一个区间，我们进入缩放阶段。\n    -   如果以上情况均未发生，则步长太短。选择一个新的、更大的试验步长 $\\alpha_{i+1}$（在 $\\alpha_i$ 和 $\\alpha_{\\max}$ 之间），并重复该过程。\n4.  **缩放阶段：** 此阶段细化区间 $[\\alpha_{lo}, \\alpha_{hi}]$ 以找到一个可接受的 $\\alpha$。\n    -   在 $(\\alpha_{lo}, \\alpha_{hi})$ 内选择一个试验步长 $\\alpha_j$。由于其稳健性，使用二分法（$\\alpha_j = (\\alpha_{lo} + \\alpha_{hi})/2$）。\n    -   如果 $\\phi(\\alpha_j)$ 未通过 Armijo 条件或大于 $\\phi(\\alpha_{lo})$，则更新区间的上界：$\\alpha_{hi} = \\alpha_j$。\n    -   否则，$\\alpha_j$ 满足 Armijo 条件。然后我们检查曲率。如果满足，则返回 $\\alpha_j$。如果不满足，则根据 $\\phi'(\\alpha_j)$ 的符号缩小区间。如果 $\\phi'(\\alpha_j)  0$，更新下界：$\\alpha_{lo} = \\alpha_j$。如果 $\\phi'(\\alpha_j) \\ge 0$，更新上界：$\\alpha_{hi} = \\alpha_j$。\n    -   此过程重复固定次数的迭代，或直到区间足够小。\n\n这个完整的算法将被实现，用以为四个指定的测试用例求解步长 $\\alpha$。",
            "answer": "[1.0,0.01953125,1.0,0.0]"
        }
    ]
}