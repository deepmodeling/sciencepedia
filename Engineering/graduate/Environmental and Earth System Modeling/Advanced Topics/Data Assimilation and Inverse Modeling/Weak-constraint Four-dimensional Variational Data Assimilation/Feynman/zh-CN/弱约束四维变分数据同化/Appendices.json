{
    "hands_on_practices": [
        {
            "introduction": "弱约束四维变分同化通过允许模式存在误差，解决了强约束假设下的一个根本限制。这个练习  提供了一个清晰的实例，展示了当模式存在系统性偏差时，强约束方法为何会失效。通过求解一个简化的常数偏差，你将亲身体验弱约束方法如何灵活地将模式与观测之间的不匹配归因于模式误差，从而找到一个更合理的分析状态。",
            "id": "3431076",
            "problem": "考虑一个一维离散时间动力系统，其时间窗由 $k \\in \\{0,1,2\\}$ 索引，状态为 $x_k \\in \\mathbb{R}$，观测算子 $H$ 由 $H x_k = x_k$ 给出。可用于同化的数据为观测值 $y_0 = 0$、$y_1 = 1$ 和 $y_2 = 2$。同化中使用的预报模式是持续性模式 $M_k(x_k) = x_k$，因此在不允许模式误差的情况下，预报约束为 $x_{k+1} = x_k$。初始条件的背景（先验）为 $x_0 \\sim \\mathcal{N}(x_b, \\sigma_b^2)$，其中 $x_b = 0$ 且 $\\sigma_b^2 = 1$。每个时间点的观测误差是独立的，且服从方差为 $\\sigma_o^2 = 1$ 的高斯分布。\n\n1. 解释为什么在强约束四维变分同化（4D-Var）下，不存在任何 $x_0$ 的选择可以精确拟合所有观测值 $y_0, y_1, y_2$ 而不违反模式约束。强约束4D-Var假设模式是完美的（$x_{k+1} = x_k$）并且只估计 $x_0$。\n\n2. 现在采用弱约束四维变分同化（4D-Var），通过引入一个加性模式误差序列，其简化参数形式为 $w_k \\equiv b$ (对于 $k \\in \\{0,1\\}$)，其中 $b \\in \\mathbb{R}$ 是在该时间窗内的一个常数偏差。假设每个 $w_k$ 的先验分布为零均值高斯分布，方差为 $\\sigma_q^2 = 1$，且在时间上是独立的。使用高斯先验、高斯似然以及误差源之间相互独立的假设，建立相应的关于 $(x_0, b)$ 的最大后验估计问题，将其简化为关于 $(x_0, b)$ 的二次最小化问题，并求解其唯一极小值点。计算该特定数据集的最优常数模式误差偏差 $b^{\\star}$ 的值。不要对结果进行四舍五入；以精确形式给出。\n\n你的最终答案应该是 $b^{\\star}$ 的单个值，不带单位，也无额外评论。",
            "solution": "该问题要求在两种不同的框架下分析一个简单的数据同化情景：强约束和弱约束四维变分同化（4D-Var）。\n\n第1部分：强约束4D-Var\n\n在强约束4D-Var的表述中，预报模式被假设为完美的。给定的预报模式是持续性模式，$M_k(x_k) = x_k$，这意味着模式约束为 $x_{k+1} = x_k$ (对于 $k \\in \\{0, 1\\}$)。在这种设置中，控制变量仅仅是初始状态 $x_0$。\n\n在这个完美模式假设下，整个同化窗内的系统状态由初始状态决定：\n$$x_1 = M_0(x_0) = x_0$$\n$$x_2 = M_1(x_1) = x_1 = x_0$$\n因此，模式动力学施加了严格的约束 $x_0 = x_1 = x_2$。\n\n问题指出，对观测值的精确拟合要求在每个时间点 $k$ 的模式状态都等于相应的观测值 $y_k$。给定的观测值为 $y_0 = 0$、$y_1 = 1$ 和 $y_2 = 2$。因此，精确拟合将意味着：\n$$x_0 = y_0 = 0$$\n$$x_1 = y_1 = 1$$\n$$x_2 = y_2 = 2$$\n\n这三个条件 $x_0 = 0$、$x_1 = 1$ 和 $x_2 = 2$ 与模式约束 $x_0 = x_1 = x_2$ 直接矛盾。单个 $x_0$ 值不可能同时满足 $x_0 = 0$、$x_0 = 1$ 和 $x_0 = 2$。因此，初始状态 $x_0$ 的任何选择都无法在不违反完美模式约束的情况下完美拟合所有三个观测值。强约束4D-Var将通过最小化一个平衡了观测失配和背景失配的代价函数来找到一个最优的 $x_0$，但在这种情况下，它永远无法实现对所有观测值的零失配。\n\n第2部分：弱约束4D-Var\n\n在弱约束4D-Var的表述中，模式被允许是不完美的。这是通过引入一个模式误差项 $w_k$ 来实现的。现在的模式动力学由下式给出：\n$$x_{k+1} = M_k(x_k) + w_k = x_k + w_k$$\n问题为模式误差指定了一个简化的参数化形式，即它是在该时间窗内的一个常数偏差：$w_k \\equiv b$ (对于 $k \\in \\{0, 1\\}$)。现在用于同化的控制变量是初始状态 $x_0$ 和常数模式误差偏差 $b$。\n\n目标是找到使后验概率密度最大化的 $(x_0, b)$ 的值，这在高斯误差的假设下，等价于最小化一个二次代价函数 $J(x_0, b)$。该代价函数是三项之和：背景项（$J_b$）、模式误差项（$J_q$）和观测项（$J_o$）。\n$$J(x_0, b) = J_b(x_0) + J_q(b) + J_o(x_0, b)$$\n\n1.  背景项惩罚初始状态 $x_0$ 与背景估计 $x_b$ 的偏差：\n    $$J_b(x_0) = \\frac{1}{2\\sigma_b^2}(x_0 - x_b)^2$$\n    当 $x_b = 0$ 且 $\\sigma_b^2 = 1$ 时，此项变为 $J_b(x_0) = \\frac{1}{2}x_0^2$。\n\n2.  模式误差项惩罚模式误差对其先验估计（均值为零）的偏离。由于模式误差序列为 $w_0 = b$ 和 $w_1 = b$，且每个 $w_k$ 的先验是独立的，方差为 $\\sigma_q^2$，因此该项为：\n    $$J_q(b) = \\sum_{k=0}^{1} \\frac{1}{2\\sigma_q^2}w_k^2 = \\frac{1}{2\\sigma_q^2}b^2 + \\frac{1}{2\\sigma_q^2}b^2 = \\frac{b^2}{\\sigma_q^2}$$\n    当 $\\sigma_q^2 = 1$ 时，此项变为 $J_q(b) = b^2$。\n\n3.  观测项惩罚模式轨迹与观测值之间的失配。首先，我们用控制变量 $x_0, b$ 来表示轨迹 $x_1, x_2$：\n    $$x_1 = x_0 + w_0 = x_0 + b$$\n    $$x_2 = x_1 + w_1 = (x_0 + b) + b = x_0 + 2b$$\n    观测项为：\n    $$J_o(x_0, b) = \\sum_{k=0}^{2} \\frac{1}{2\\sigma_o^2}(y_k - Hx_k)^2$$\n    当观测算子为 $H x_k = x_k$，观测值为 $y_0 = 0, y_1 = 1, y_2 = 2$，方差为 $\\sigma_o^2 = 1$ 时，此项变为：\n    $$J_o(x_0, b) = \\frac{1}{2} \\left[ (y_0 - x_0)^2 + (y_1 - x_1)^2 + (y_2 - x_2)^2 \\right]$$\n    $$J_o(x_0, b) = \\frac{1}{2} \\left[ (0 - x_0)^2 + (1 - (x_0 + b))^2 + (2 - (x_0 + 2b))^2 \\right]$$\n\n结合这些项，总代价函数为：\n$$J(x_0, b) = \\frac{1}{2}x_0^2 + b^2 + \\frac{1}{2} \\left[ x_0^2 + (1 - x_0 - b)^2 + (2 - x_0 - 2b)^2 \\right]$$\n为了找到最小化 $J$ 的最优值 $(x_0^\\star, b^\\star)$，我们计算 $J$ 关于 $x_0$ 和 $b$ 的梯度，并将其设为零。\n\n关于 $x_0$ 的偏导数：\n$$\\frac{\\partial J}{\\partial x_0} = x_0 + \\frac{1}{2} \\left[ 2x_0 + 2(1 - x_0 - b)(-1) + 2(2 - x_0 - 2b)(-1) \\right]$$\n$$\\frac{\\partial J}{\\partial x_0} = x_0 + x_0 - (1 - x_0 - b) - (2 - x_0 - 2b)$$\n$$\\frac{\\partial J}{\\partial x_0} = 2x_0 - 1 + x_0 + b - 2 + x_0 + 2b = 4x_0 + 3b - 3$$\n\n关于 $b$ 的偏导数：\n$$\\frac{\\partial J}{\\partial b} = 2b + \\frac{1}{2} \\left[ 2(1 - x_0 - b)(-1) + 2(2 - x_0 - 2b)(-2) \\right]$$\n$$\\frac{\\partial J}{\\partial b} = 2b - (1 - x_0 - b) - 2(2 - x_0 - 2b)$$\n$$\\frac{\\partial J}{\\partial b} = 2b - 1 + x_0 + b - 4 + 2x_0 + 4b = 3x_0 + 7b - 5$$\n\n将偏导数设为零，得到一个关于 $(x_0, b)$ 的线性方程组：\n1) $4x_0 + 3b = 3$\n2) $3x_0 + 7b = 5$\n\n我们可以解这个方程组。从方程（1）中，我们将 $x_0$ 用 $b$ 表示：\n$$4x_0 = 3 - 3b \\implies x_0 = \\frac{3 - 3b}{4}$$\n将这个 $x_0$ 的表达式代入方程（2）：\n$$3\\left(\\frac{3 - 3b}{4}\\right) + 7b = 5$$\n将整个方程乘以 $4$ 以消去分数：\n$$3(3 - 3b) + 28b = 20$$\n$$9 - 9b + 28b = 20$$\n$$19b = 11$$\n$$b^\\star = \\frac{11}{19}$$\n这就是最优的常数模式误差偏差。问题要求的就是这个值。Hessian矩阵的正定性保证了这是一个唯一的极小值。",
            "answer": "$$\\boxed{\\frac{11}{19}}$$"
        },
        {
            "introduction": "求解弱约束四维变分问题需要在巨大的控制变量空间上进行优化，这要求高效地计算代价函数的梯度。这个练习  将引导你使用拉格朗日乘子法，从第一性原理出发推导出控制整个优化过程的一阶最优性条件。掌握这个推导过程对于理解伴随模式的核心作用以及四维变分同化的内在机制至关重要。",
            "id": "3426006",
            "problem": "考虑一个表述为弱约束四维变分(4D-Var)数据同化的离散时间、有限时域数据同化问题。设在离散时间 $k = 0, 1, \\dots, N$ 的状态为 $x_k \\in \\mathbb{R}^{n}$，并假设动力学由一个带有加性模式误差的可能非线性的模型给出，\n$$\nx_{k+1} = m_k(x_k) + \\eta_k,\n$$\n其中 $m_k: \\mathbb{R}^{n} \\to \\mathbb{R}^{n}$ 是连续可微的，$\\eta_k \\in \\mathbb{R}^{n}$ 是在时间 $k$ 的模式误差。观测值 $y_k \\in \\mathbb{R}^{p_k}$ 通过一个假设可微的观测算子 $H_k: \\mathbb{R}^{n} \\to \\mathbb{R}^{p_k}$ 与状态相关联，并带有加性零均值高斯观测误差，其协方差为对称正定的 $R_k \\in \\mathbb{R}^{p_k \\times p_k}$。模式误差被假设为零均值高斯分布，其协方差为对称正定的 $Q_k \\in \\mathbb{R}^{n \\times n}$。先验（背景）状态 $x_b \\in \\mathbb{R}^{n}$ 的误差协方差为对称正定的 $B \\in \\mathbb{R}^{n \\times n}$。\n\n定义弱约束4D-Var代价函数\n$$\nJ(x_0, \\{\\eta_k\\}_{k=0}^{N-1}) = \\tfrac{1}{2} (x_0 - x_b)^{\\top} B^{-1} (x_0 - x_b) + \\tfrac{1}{2} \\sum_{k=0}^{N} \\left(H_k(x_k) - y_k\\right)^{\\top} R_k^{-1} \\left(H_k(x_k) - y_k\\right) + \\tfrac{1}{2} \\sum_{k=0}^{N-1} \\eta_k^{\\top} Q_k^{-1} \\eta_k,\n$$\n其中状态序列 $\\{x_k\\}$ 受模型 $x_{k+1} = m_k(x_k) + \\eta_k$ 的约束。\n\n从这些定义出发，利用基于拉格朗日乘子和变分法的约束优化第一性原理，推导弱约束4D-Var关于 $x_0$ 和 $\\{\\eta_k\\}$ 的一阶最优性条件，包括：\n- 以模型和观测算子的雅可比矩阵表示的伴随递归，\n- 边界（终端）伴随条件，\n- 以及关于模式误差控制量 $\\{\\eta_k\\}$ 的平稳性条件。\n\n将模型的雅可比矩阵表示为 $M_k(x_k) = \\nabla m_k(x_k) \\in \\mathbb{R}^{n \\times n}$，观测算子的雅可比矩阵表示为 $H_k'(x_k) = \\nabla H_k(x_k) \\in \\mathbb{R}^{p_k \\times n}$，并定义新息 $d_k = H_k(x_k) - y_k$。最后，提供最优控制更新 $\\eta_k$ 关于 $Q_k$ 和时间 $k+1$ 处伴随变量的闭式解析表达式。\n\n你的最终输出必须是关于控制更新的单个符号数学表达式。不需要进行数值计算或四舍五入。",
            "solution": "该问题要求使用拉格朗日乘子法推导弱约束四维变分(4D-Var)数据同化问题的一阶最优性条件。目标是在模型动力学施加的约束条件下，找到使给定代价函数 $J$ 最小化的初始状态 $x_0$ 和模式误差序列 $\\{\\eta_k\\}_{k=0}^{N-1}$。\n\n需要最小化的代价函数是：\n$$\nJ(x_0, \\{\\eta_k\\}_{k=0}^{N-1}) = \\tfrac{1}{2} (x_0 - x_b)^{\\top} B^{-1} (x_0 - x_b) + \\tfrac{1}{2} \\sum_{k=0}^{N} \\left(H_k(x_k) - y_k\\right)^{\\top} R_k^{-1} \\left(H_k(x_k) - y_k\\right) + \\tfrac{1}{2} \\sum_{k=0}^{N-1} \\eta_k^{\\top} Q_k^{-1} \\eta_k\n$$\n这个最小化问题受制于作为约束条件的模型动力学：\n$$\nx_{k+1} = m_k(x_k) + \\eta_k, \\quad \\text{for } k = 0, 1, \\dots, N-1\n$$\n为解决此约束优化问题，我们通过将代价函数与约束条件相结合来构造拉格朗日函数 $\\mathcal{L}$，其中约束条件由一系列拉格朗日乘子（或伴随变量）$\\lambda_{k+1} \\in \\mathbb{R}^n$（对于 $k = 0, \\dots, N-1$）加权。在构建拉格朗日函数时，我们将状态轨迹 $\\{x_k\\}_{k=0}^N$ 和模式误差 $\\{\\eta_k\\}_{k=0}^{N-1}$ 视为独立变量。拉格朗日乘子项的符号选择是一种惯例；我们采用在控制理论和数据同化中通行的符号。\n$$\n\\mathcal{L} \\left(\\{x_k\\}_{k=0}^N, \\{\\eta_k\\}_{k=0}^{N-1}, \\{\\lambda_k\\}_{k=1}^N\\right) = J - \\sum_{k=0}^{N-1} \\lambda_{k+1}^{\\top} \\left( x_{k+1} - m_k(x_k) - \\eta_k \\right)\n$$\n取得极值的一阶必要条件是拉格朗日函数对其所有自变量的偏导数必须为零。我们通过计算这些导数来继续。\n\n1.  **关于拉格朗日乘子 $\\lambda_{k+1}$ 的导数（对于 $k=0, \\dots, N-1$）：**\n    求 $\\mathcal{L}$ 关于 $\\lambda_{k+1}$ 的梯度并令其为零，可以恢复模型约束方程：\n    $$\n    \\nabla_{\\lambda_{k+1}} \\mathcal{L} = -\\left( x_{k+1} - m_k(x_k) - \\eta_k \\right) = 0\n    $$\n    $$\n    \\implies x_{k+1} = m_k(x_k) + \\eta_k\n    $$\n    这证实了在最优点，状态轨迹必须满足模型动力学。\n\n2.  **关于控制变量 $\\eta_k$ 的导数（对于 $k=0, \\dots, N-1$）：**\n    我们求 $\\mathcal{L}$ 关于每个模式误差向量 $\\eta_k$ 的梯度。$\\mathcal{L}$ 中依赖于特定 $\\eta_k$ 的项是 $\\tfrac{1}{2}\\eta_k^{\\top} Q_k^{-1} \\eta_k$ 和 $\\lambda_{k+1}^{\\top}\\eta_k$。\n    $$\n    \\nabla_{\\eta_k} \\mathcal{L} = \\nabla_{\\eta_k} \\left(\\tfrac{1}{2}\\eta_k^{\\top} Q_k^{-1} \\eta_k\\right) + \\nabla_{\\eta_k} \\left(\\lambda_{k+1}^{\\top}\\eta_k\\right) = Q_k^{-1} \\eta_k + \\lambda_{k+1} = 0\n    $$\n    这给出了**关于模式误差控制量 $\\{\\eta_k\\}$ 的平稳性条件**。求解 $\\eta_k$ 可得到最优模式误差关于伴随变量的闭式表达式：\n    $$\n    \\eta_k = -Q_k \\lambda_{k+1}\n    $$\n\n3.  **关于终端状态 $x_N$ 的导数：**\n    $\\mathcal{L}$ 中依赖于 $x_N$ 的项是最终的观测项 $\\tfrac{1}{2}(H_N(x_N) - y_N)^{\\top} R_N^{-1} (H_N(x_N) - y_N)$ 和最终的约束项 $-\\lambda_N^{\\top}x_N$。使用记号 $d_k = H_k(x_k) - y_k$ 和 $H_k' = \\nabla H_k(x_k)$：\n    $$\n    \\nabla_{x_N} \\mathcal{L} = \\nabla_{x_N} \\left(\\tfrac{1}{2}d_N^{\\top}R_N^{-1}d_N\\right) - \\nabla_{x_N}\\left(\\lambda_N^{\\top}x_N\\right) = (H_N'(x_N))^{\\top} R_N^{-1} d_N - \\lambda_N = 0\n    $$\n    这就得到了**边界（终端）伴随条件**：\n    $$\n    \\lambda_N = (H_N'(x_N))^{\\top} R_N^{-1} d_N\n    $$\n\n4.  **关于中间状态 $x_k$ 的导数（对于 $k=1, \\dots, N-1$）：**\n    对于轨迹中间的一个状态 $x_k$，$\\mathcal{L}$ 中的相关项是时间 $k$ 的观测项、连接 $x_k$ 与 $x_{k-1}$ 的约束项（即 $-\\lambda_k^{\\top}x_k$），以及连接 $x_{k+1}$ 与 $x_k$ 的约束项（即 $\\lambda_{k+1}^{\\top}m_k(x_k)$）。\n    $$\n    \\nabla_{x_k} \\mathcal{L} = \\nabla_{x_k}\\left(\\tfrac{1}{2}d_k^{\\top}R_k^{-1}d_k\\right) - \\nabla_{x_k}\\left(\\lambda_k^{\\top}x_k\\right) + \\nabla_{x_k}\\left(\\lambda_{k+1}^{\\top}m_k(x_k)\\right) = 0\n    $$\n    使用记号 $M_k = \\nabla m_k(x_k)$：\n    $$\n    (H_k'(x_k))^{\\top} R_k^{-1} d_k - \\lambda_k + (M_k(x_k))^{\\top} \\lambda_{k+1} = 0\n    $$\n    重排该方程可得到**伴随递归**：\n    $$\n    \\lambda_k = (M_k(x_k))^{\\top} \\lambda_{k+1} + (H_k'(x_k))^{\\top} R_k^{-1} d_k, \\quad \\text{for } k=N-1, \\dots, 1\n    $$\n    此关系表明，伴随变量 $\\lambda_k$ 是从 $k=N$ 的终端条件开始向后（在时间上）传播的。\n\n5.  **关于初始状态 $x_0$ 的导数**：\n    最后，我们计算关于控制变量 $x_0$ 的梯度。相关项是背景项、在 $k=0$ 时的观测项，以及涉及 $m_0(x_0)$ 的第一个约束项。\n    $$\n    \\nabla_{x_0} \\mathcal{L} = \\nabla_{x_0}\\left(\\tfrac{1}{2}(x_0-x_b)^{\\top}B^{-1}(x_0-x_b)\\right) + \\nabla_{x_0}\\left(\\tfrac{1}{2}d_0^{\\top}R_0^{-1}d_0\\right) + \\nabla_{x_0}\\left(\\lambda_1^{\\top}m_0(x_0)\\right) = 0\n    $$\n    $$\n    B^{-1}(x_0 - x_b) + (H_0'(x_0))^{\\top} R_0^{-1} d_0 + (M_0(x_0))^{\\top} \\lambda_1 = 0\n    $$\n    这个方程是 $x_0$ 的平稳性条件，它代表了代价函数 $J$ 关于 $x_0$ 的梯度。如果我们通过扩展递归来定义一个初始伴随变量 $\\lambda_0 = (M_0(x_0))^{\\top} \\lambda_1 + (H_0'(x_0))^{\\top} R_0^{-1} d_0$，则平稳性条件简化为 $B^{-1}(x_0 - x_b) + \\lambda_0 = 0$。\n\n总而言之，一阶最优性条件包括正向模型积分、反向伴随模型积分以及控制量的解析表达式。问题明确要求给出最优控制更新 $\\eta_k$ 的表达式。如步骤2中所推导，这由关于模式误差的平稳性条件给出。\n\n所需的最优控制更新 $\\eta_k$ 的表达式是平稳性条件 $\\nabla_{\\eta_k} \\mathcal{L} = 0$ 的直接结果。该条件 $Q_k^{-1} \\eta_k + \\lambda_{k+1} = 0$，可以通过左乘模式误差协方差矩阵 $Q_k$ 来求解 $\\eta_k$。",
            "answer": "$$\n\\boxed{\\eta_k = -Q_k \\lambda_{k+1}}\n$$"
        },
        {
            "introduction": "将变分同化的理论转化为可靠的数值代码，关键一步是确保代价函数的梯度计算完全正确。这个练习  介绍了一种被称为“梯度检验”的标准验证技术，通过将解析梯度与有限差分近似进行比较来完成。完成此练习将为你提供一个必不可少的实践技能，以确保你的伴随模式及梯度代码的正确性，这是开发任何优化算法的基石。",
            "id": "3931478",
            "problem": "考虑弱约束四维变分数据同化（weak-constraint Four-Dimensional Variational Data Assimilation, 4D-Var），其中模型动力学包含一个随时间变化的加性模型误差强迫项。在一维设定中，离散时间预报模型由线性递推关系 $x_{k+1} = a \\, x_k + w_k$ 给出，其中 $k \\in \\{0,1,\\dots,N-1\\}$，$x_k \\in \\mathbb{R}$ 是模型状态，$a \\in \\mathbb{R}$ 是一个已知的标量模型算子，$w_k \\in \\mathbb{R}$ 是在时间索引 $k$ 处的模型误差强迫。观测模型为 $y_k = H \\, x_k + \\eta_k$，其中 $H \\in \\mathbb{R}$ 是一个已知的标量观测算子，$y_k \\in \\mathbb{R}$ 是观测数据，$\\eta_k \\in \\mathbb{R}$ 是观测噪声。假设误差模型为高斯模型，具有已知的正方差且误差独立，因此标准的变分代价泛函由初始条件的背景项惩罚、整个时间窗内的观测失配惩罚以及模型误差惩罚构成。\n\n从以下基本依据出发：\n- 线性离散模型演化律 $x_{k+1} = a \\, x_k + w_k$。\n- 线性观测模型 $y_k = H \\, x_k + \\eta_k$。\n- 具有独立分量的高斯误差模型，意味着变分代价函数中包含二次惩罚项。\n\n定义控制向量 $v = (x_0, w_0, w_1, \\dots, w_{N-1}) \\in \\mathbb{R}^{N+1}$ 的弱约束 4D-Var 代价函数为\n$$\nJ(v) = \\tfrac{1}{2}\\,\\frac{(x_0 - x_b)^2}{B} + \\tfrac{1}{2}\\,\\sum_{k=0}^{N} \\frac{(H\\,x_k(v) - y_k)^2}{R} + \\tfrac{1}{2}\\,\\sum_{k=0}^{N-1} \\frac{w_k^2}{Q},\n$$\n其中 $x_b \\in \\mathbb{R}$ 是背景初始条件，$B > 0$ 是背景方差标量，$R > 0$ 是观测方差标量，$Q > 0$ 是模型误差方差标量，$x_k(v)$ 表示在模型误差序列 $\\{w_k\\}$ 驱动下，从 $x_0$ 开始通过模型正向迭代得到的时间点 $k$ 的状态。\n\n你的任务是使用随机扰动和中心有限差分方法，为 $J(v)$ 实现一个数值梯度正确性检验。该检验必须将解析推导的梯度 $\\nabla J(v)$ 与一个随机方向 $p$ 的内积所给出的方向导数，同中心有限差分近似进行比较：\n$$\nD_{\\text{FD}}(v;p,\\varepsilon) = \\frac{J(v+\\varepsilon p) - J(v-\\varepsilon p)}{2\\,\\varepsilon}.\n$$\n对于给定的 $\\varepsilon$，相对误差为\n$$\nE(v;p,\\varepsilon) = \\frac{\\left|\\langle \\nabla J(v), p \\rangle - D_{\\text{FD}}(v;p,\\varepsilon)\\right|}{\\max\\left(1,\\left|\\langle \\nabla J(v), p \\rangle\\right|,\\left|D_{\\text{FD}}(v;p,\\varepsilon)\\right|\\right)}.\n$$\n\n你必须通过从基本原理出发，推导并实现一维线性模型的离散伴随模型，来构建解析梯度 $\\nabla J(v)$。不要使用自动微分或外部算法微分工具。使用归一化为单位欧几里得范数的随机扰动方向 $p$。对于每个测试用例，在一组预设的 $\\varepsilon$ 值上计算 $E(v;p,\\varepsilon)$，并报告该组中的最大相对误差。\n\n程序必须是自包含的，并通过首先构建一个参考真实轨迹 $x_k^{\\text{true}}$ 来生成合成观测 $y_k$。该真实轨迹遵循相同的模型 $x_{k+1}^{\\text{true}} = a\\,x_k^{\\text{true}} + w_k^{\\text{true}}$，其中对于所有 $k$ 都有 $w_k^{\\text{true}} = 0$，且初始状态 $x_0^{\\text{true}}$ 已指定。然后，通过抽取方差为 $R$ 的独立高斯观测噪声 $\\eta_k$，得到 $y_k = H\\,x_k^{\\text{true}} + \\eta_k$。为了可复现性，对 $v$ 和 $p$ 的随机抽样以及观测噪声的生成必须使用所提供的种子。\n\n实现以下测试套件，每一项指定了 $(a, N, H, B, R, Q, x_b, x_0^{\\text{true}}, \\text{seed}, \\text{epsilons})$：\n- 情况 1: $(a = 0.9, N = 20, H = 1.0, B = 1.0, R = 0.5, Q = 0.1, x_b = 0.0, x_0^{\\text{true}} = 1.5, \\text{seed} = 42, \\text{epsilons} = [10^{-2}, 10^{-4}, 10^{-6}])$。\n- 情况 2: $(a = 1.0, N = 30, H = 1.0, B = 0.2, R = 0.8, Q = 0.05, x_b = 0.0, x_0^{\\text{true}} = 1.0, \\text{seed} = 7, \\text{epsilons} = [10^{-2}, 10^{-4}, 10^{-6}])$。\n- 情况 3: $(a = 0.5, N = 10, H = 1.0, B = 2.0, R = 0.3, Q = 10^{-3}, x_b = 0.0, x_0^{\\text{true}} = 0.5, \\text{seed} = 99, \\text{epsilons} = [10^{-2}, 10^{-4}, 10^{-6}])$。\n- 情况 4: $(a = 0.9, N = 15, H = 0.0, B = 1.5, R = 1.0, Q = 0.2, x_b = 0.0, x_0^{\\text{true}} = 2.0, \\text{seed} = 5, \\text{epsilons} = [10^{-2}, 10^{-4}, 10^{-6}])$。\n- 情况 5: $(a = 0.99, N = 40, H = 1.0, B = 0.5, R = 10^{-4}, Q = 0.2, x_b = 0.0, x_0^{\\text{true}} = 1.2, \\text{seed} = 123, \\text{epsilons} = [10^{-2}, 10^{-4}, 10^{-6}])$。\n\n对于每种情况，使用指定的种子从标准正态分布中独立抽样，构建基准控制向量 $v$ 和随机方向 $p$，并将 $p$ 归一化使其欧几里得范数为1。计算并返回最大相对误差 $\\max_{\\varepsilon \\in \\text{epsilons}} E(v;p,\\varepsilon)$，结果为浮点数。\n\n你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔的结果列表，按情况的顺序排列，例如 $[\\text{result}_1,\\text{result}_2,\\dots,\\text{result}_5]$。不涉及物理单位或角度，因此不需要进行单位转换。每个测试用例的答案必须是一个浮点数。",
            "solution": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the gradient correctness test for all specified cases.\n    \"\"\"\n\n    test_cases = [\n        # (a, N, H, B, R, Q, x_b, x_0_true, seed, epsilons)\n        (0.9, 20, 1.0, 1.0, 0.5, 0.1, 0.0, 1.5, 42, [1e-2, 1e-4, 1e-6]),\n        (1.0, 30, 1.0, 0.2, 0.8, 0.05, 0.0, 1.0, 7, [1e-2, 1e-4, 1e-6]),\n        (0.5, 10, 1.0, 2.0, 0.3, 1e-3, 0.0, 0.5, 99, [1e-2, 1e-4, 1e-6]),\n        (0.9, 15, 0.0, 1.5, 1.0, 0.2, 0.0, 2.0, 5, [1e-2, 1e-4, 1e-6]),\n        (0.99, 40, 1.0, 0.5, 1e-4, 0.2, 0.0, 1.2, 123, [1e-2, 1e-4, 1e-6]),\n    ]\n\n    results = []\n    for case in test_cases:\n        max_error = run_gradient_test(*case)\n        results.append(max_error)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef run_forward_model(v, a, N):\n    \"\"\"\n    Runs the forward prognostic model to get the state trajectory.\n\n    Args:\n        v (np.ndarray): Control vector (x_0, w_0, ..., w_{N-1}).\n        a (float): Model operator.\n        N (int): Number of time steps.\n\n    Returns:\n        np.ndarray: State trajectory x_0, ..., x_N.\n    \"\"\"\n    x = np.zeros(N + 1)\n    x0 = v[0]\n    w = v[1:]\n    x[0] = x0\n    for k in range(N):\n        x[k + 1] = a * x[k] + w[k]\n    return x\n\ndef compute_J(v, a, N, H, B, R, Q, x_b, y):\n    \"\"\"\n    Computes the weak-constraint 4D-Var cost function J(v).\n\n    Args:\n        v, a, N, H, B, R, Q, x_b: As defined in the problem.\n        y (np.ndarray): Observation vector.\n\n    Returns:\n        float: The value of the cost function J(v).\n    \"\"\"\n    x0 = v[0]\n    w = v[1:]\n\n    # Forward model run\n    x = run_forward_model(v, a, N)\n\n    # Background penalty\n    J_b = 0.5 * ((x0 - x_b)**2) / B\n\n    # Observation penalty\n    J_o = 0.5 * np.sum((H * x - y)**2) / R\n    \n    # Model error penalty\n    J_q = 0.5 * np.sum(w**2) / Q\n\n    return J_b + J_o + J_q\n\ndef compute_grad_J(v, a, N, H, B, R, Q, x_b, y):\n    \"\"\"\n    Computes the gradient of the cost function, grad J(v), using the adjoint method.\n\n    Args:\n        v, a, N, H, B, R, Q, x_b: As defined in the problem.\n        y (np.ndarray): Observation vector.\n\n    Returns:\n        np.ndarray: The gradient vector nabla J(v).\n    \"\"\"\n    x0 = v[0]\n    w = v[1:]\n    grad_v = np.zeros(N + 1)\n\n    # 1. Forward pass: Get state trajectory x\n    x = run_forward_model(v, a, N)\n\n    # 2. Backward pass: Compute adjoint variables lambda\n    lambdas = np.zeros(N + 1)\n    \n    # Terminal condition for adjoint\n    lambdas[N] = (H / R) * (H * x[N] - y[N])\n\n    # Backward recurrence\n    for k in range(N - 1, -1, -1):\n        forcing = (H / R) * (H * x[k] - y[k])\n        lambdas[k] = a * lambdas[k + 1] + forcing\n\n    # 3. Gradient assembly\n    # Gradient component for x_0\n    grad_v[0] = (x0 - x_b) / B + lambdas[0]\n\n    # Gradient components for w_k\n    grad_v[1:] = w / Q + lambdas[1:]\n\n    return grad_v\n\ndef run_gradient_test(a, N, H, B, R, Q, x_b, x0_true, seed, epsilons):\n    \"\"\"\n    Performs the gradient correctness test for a single case.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n\n    # Generate synthetic observations\n    x_true = np.zeros(N + 1)\n    x_true[0] = x0_true\n    for k in range(N):\n        x_true[k+1] = a * x_true[k] # w_true is zero\n    \n    noise = rng.normal(loc=0.0, scale=np.sqrt(R), size=N + 1)\n    y = H * x_true + noise\n\n    # Generate random control vector v and perturbation p\n    v = rng.normal(loc=0.0, scale=1.0, size=N + 1)\n    p = rng.normal(loc=0.0, scale=1.0, size=N + 1)\n    p /= np.linalg.norm(p) # Normalize perturbation\n\n    # Compute analytical directional derivative\n    grad_J = compute_grad_J(v, a, N, H, B, R, Q, x_b, y)\n    dir_deriv_analytic = np.dot(grad_J, p)\n\n    errors = []\n    for eps in epsilons:\n        # Compute finite difference approximation\n        v_plus = v + eps * p\n        v_minus = v - eps * p\n        \n        J_plus = compute_J(v_plus, a, N, H, B, R, Q, x_b, y)\n        J_minus = compute_J(v_minus, a, N, H, B, R, Q, x_b, y)\n        \n        dir_deriv_fd = (J_plus - J_minus) / (2 * eps)\n\n        # Compute relative error\n        numerator = np.abs(dir_deriv_analytic - dir_deriv_fd)\n        denominator = np.max([1.0, np.abs(dir_deriv_analytic), np.abs(dir_deriv_fd)])\n        error = numerator / denominator\n        errors.append(error)\n        \n    return np.max(errors)\n\nif __name__ == '__main__':\n    solve()\n```",
            "answer": "[1.761895697223001e-12,2.378931122847259e-12,5.204170425337223e-13,0.0,6.014237194605156e-11]"
        }
    ]
}