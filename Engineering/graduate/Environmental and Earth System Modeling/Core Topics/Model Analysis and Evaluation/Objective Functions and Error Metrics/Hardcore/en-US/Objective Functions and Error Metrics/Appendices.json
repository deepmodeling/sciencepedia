{
    "hands_on_practices": [
        {
            "introduction": "The choice of an objective function is one of the most fundamental decisions in model calibration. This exercise delves into the practical consequences of selecting between two of the most common metrics: the Root Mean Square Error (RMSE) and the Mean Absolute Error (MAE). By calculating the optimal model bias parameter under each metric for a set of precipitation data, you will gain first-hand insight into how their differing mathematical properties—particularly their sensitivity to outliers—shape the calibration result and define what \"best fit\" means .",
            "id": "3899972",
            "problem": "A catchment-scale hydrological model is being calibrated for daily precipitation using an additive bias parameter. Let the raw model precipitation for day $i$ be $P_{i}^{\\mathrm{raw}}$, the observed precipitation be $P_{i}^{\\mathrm{obs}}$, and the model with an additive bias parameter $b$ be $P_{i}^{\\mathrm{mod}}(b) = P_{i}^{\\mathrm{raw}} + b$. Define the residuals as $e_{i}(b) = P_{i}^{\\mathrm{mod}}(b) - P_{i}^{\\mathrm{obs}}$. For a $12$-day training period, the residuals at $b=0$ are given (in millimeters per day) by the set\n$$\n\\{ -6.2,\\ 3.9,\\ -1.1,\\ 0.0,\\ 5.3,\\ -14.8,\\ 7.5,\\ 1.6,\\ -2.9,\\ 2.1,\\ -0.7,\\ 4.0 \\}.\n$$\nStarting only from the definitions of Root Mean Square Error (RMSE) and Mean Absolute Error (MAE), perform the following:\n\n1) Using the provided residuals at $b=0$, compute the Root Mean Square Error (RMSE) and the Mean Absolute Error (MAE). Express both in millimeters per day.\n\n2) Treating the additive bias $b$ as the sole calibration parameter, write down the objective functions $J_{\\mathrm{RMSE}}(b)$ and $J_{\\mathrm{MAE}}(b)$ that correspond to the Root Mean Square Error (RMSE) and the Mean Absolute Error (MAE), respectively, in terms of the residuals $e_{i}(b)$. From first principles, derive the values of $b$ that minimize each objective, and evaluate these minimizing values numerically for the provided data.\n\n3) Using your derivations, discuss qualitatively how each metric shapes the calibration landscape for $b$ (for example, smoothness, curvature, and sensitivity to outliers), and relate these properties to the minimizers you obtained.\n\nFor grading, report as a single scalar the ratio of RMSE to MAE computed in part $1$ at $b=0$. Round your reported ratio to four significant figures. Report the ratio as a pure number with no units. If intermediate quantities are stated, express them in millimeters per day.",
            "solution": "The problem is assessed to be valid as it is scientifically grounded, well-posed, and objective. It presents a standard calibration task from environmental modeling that is solvable with the provided information.\n\nThe total number of data points for the training period is $N=12$. The set of residuals at a bias of $b=0$ is given as $E_0 = \\{ -6.2,\\ 3.9,\\ -1.1,\\ 0.0,\\ 5.3,\\ -14.8,\\ 7.5,\\ 1.6,\\ -2.9,\\ 2.1,\\ -0.7,\\ 4.0 \\}$. The units are millimeters per day (mm/day).\n\nFirst, we compute the Root Mean Square Error (RMSE) and Mean Absolute Error (MAE) for the case where the additive bias $b=0$.\nThe definition of RMSE for a set of $N$ residuals $e_i$ is:\n$$\n\\mathrm{RMSE} = \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} e_{i}^{2}}\n$$\nFor the given residuals $e_i(0)$, we first calculate the sum of squares:\n$$\n\\sum_{i=1}^{12} (e_{i}(0))^{2} = (-6.2)^2 + (3.9)^2 + (-1.1)^2 + (0.0)^2 + (5.3)^2 + (-14.8)^2 + (7.5)^2 + (1.6)^2 + (-2.9)^2 + (2.1)^2 + (-0.7)^2 + (4.0)^2\n$$\n$$\n\\sum_{i=1}^{12} (e_{i}(0))^{2} = 38.44 + 15.21 + 1.21 + 0.0 + 28.09 + 219.04 + 56.25 + 2.56 + 8.41 + 4.41 + 0.49 + 16.0 = 390.11\n$$\nThe RMSE at $b=0$ is therefore:\n$$\n\\mathrm{RMSE}(0) = \\sqrt{\\frac{390.11}{12}} \\approx 5.7017 \\text{ mm/day}\n$$\nThe definition of MAE for a set of $N$ residuals $e_i$ is:\n$$\n\\mathrm{MAE} = \\frac{1}{N} \\sum_{i=1}^{N} |e_i|\n$$\nFor the given residuals, we calculate the sum of absolute values:\n$$\n\\sum_{i=1}^{12} |e_{i}(0)| = |-6.2| + |3.9| + |-1.1| + |0.0| + |5.3| + |-14.8| + |7.5| + |1.6| + |-2.9| + |2.1| + |-0.7| + |4.0|\n$$\n$$\n\\sum_{i=1}^{12} |e_{i}(0)| = 6.2 + 3.9 + 1.1 + 0.0 + 5.3 + 14.8 + 7.5 + 1.6 + 2.9 + 2.1 + 0.7 + 4.0 = 50.1\n$$\nThe MAE at $b=0$ is therefore:\n$$\n\\mathrm{MAE}(0) = \\frac{50.1}{12} = 4.175 \\text{ mm/day}\n$$\n\nSecond, we determine the values of the additive bias parameter $b$ that minimize the RMSE and MAE. The residuals as a function of $b$ are $e_i(b) = P_{i}^{\\mathrm{mod}}(b) - P_{i}^{\\mathrm{obs}} = (P_{i}^{\\mathrm{raw}} + b) - P_{i}^{\\mathrm{obs}} = (P_{i}^{\\mathrm{raw}} - P_{i}^{\\mathrm{obs}}) + b = e_{i}(0) + b$.\n\nTo minimize RMSE, we define the objective function $J_{\\mathrm{RMSE}}(b)$:\n$$\nJ_{\\mathrm{RMSE}}(b) = \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} (e_{i}(b))^{2}} = \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} (e_{i}(0) + b)^{2}}\n$$\nMinimizing $J_{\\mathrm{RMSE}}(b)$ is equivalent to minimizing its square, the Mean Squared Error (MSE), which we denote $L(b) = (J_{\\mathrm{RMSE}}(b))^2$. To find the minimum, we compute the derivative of $L(b)$ with respect to $b$ and set it to $0$:\n$$\n\\frac{dL}{db} = \\frac{d}{db} \\left[ \\frac{1}{N} \\sum_{i=1}^{N} (e_{i}(0) + b)^{2} \\right] = \\frac{1}{N} \\sum_{i=1}^{N} 2(e_{i}(0) + b) = \\frac{2}{N} \\left( \\sum_{i=1}^{N} e_{i}(0) + Nb \\right)\n$$\nSetting the derivative to zero gives:\n$$\n\\sum_{i=1}^{N} e_{i}(0) + Nb = 0 \\implies b = -\\frac{1}{N} \\sum_{i=1}^{N} e_{i}(0) = -\\overline{e(0)}\n$$\nThe optimal bias $b_{\\mathrm{RMSE}}$ is the negative of the mean of the initial residuals. The second derivative $\\frac{d^{2}L}{db^2} = 2 > 0$, confirming this is a minimum.\nWe calculate the sum of the initial residuals:\n$$\n\\sum_{i=1}^{12} e_{i}(0) = -6.2 + 3.9 - 1.1 + 0.0 + 5.3 - 14.8 + 7.5 + 1.6 - 2.9 + 2.1 - 0.7 + 4.0 = -1.3\n$$\nThe optimal bias for RMSE is:\n$$\nb_{\\mathrm{RMSE}} = -\\frac{-1.3}{12} = \\frac{1.3}{12} \\approx 0.1083 \\text{ mm/day}\n$$\n\nTo minimize MAE, we define the objective function $J_{\\mathrm{MAE}}(b)$:\n$$\nJ_{\\mathrm{MAE}}(b) = \\frac{1}{N} \\sum_{i=1}^{N} |e_{i}(b)| = \\frac{1}{N} \\sum_{i=1}^{N} |e_{i}(0) + b|\n$$\nMinimizing $J_{\\mathrm{MAE}}(b)$ is equivalent to minimizing the sum of absolute deviations $\\sum_{i=1}^{N} |e_{i}(0) - (-b)|$. From first principles of statistics, this sum is minimized when the point $-b$ is the median of the set of values $\\{e_{i}(0)\\}$. Therefore, the optimal bias is $b_{\\mathrm{MAE}} = -\\mathrm{median}\\{e_{i}(0)\\}$.\nTo find the median, we sort the initial residuals:\n$$\n\\{ -14.8, -6.2, -2.9, -1.1, -0.7, 0.0, 1.6, 2.1, 3.9, 4.0, 5.3, 7.5 \\}\n$$\nSince $N=12$ is an even number, the median is the average of the two central values, the $6$-th and $7$-th elements:\n$$\n\\mathrm{median}\\{e_{i}(0)\\} = \\frac{0.0 + 1.6}{2} = 0.8\n$$\nThe optimal bias for MAE is:\n$$\nb_{\\mathrm{MAE}} = -0.8 \\text{ mm/day}\n$$\n\nThird, we provide a qualitative discussion of the calibration landscapes.\nThe RMSE objective function, being based on squared errors, results in a calibration landscape for $b$ that is a smooth, convex parabola. Its objective function, $L(b) = (J_{\\mathrm{RMSE}}(b))^2$, is quadratic in $b$, guaranteeing a unique, well-defined minimum that can be found analytically. The key property of the squared term is its sensitivity to outliers. Large residuals are weighted much more heavily than small residuals. The value $b_{\\mathrm{RMSE}} \\approx 0.1083$ is a small positive bias. It is influenced significantly by the large negative outlier $e_6(0) = -14.8$. This single large error pulls the mean of the residuals down to $\\overline{e(0)} \\approx -0.1083$, so the calibration adjusts with a positive bias to counteract this.\n\nThe MAE objective function, based on absolute errors, results in a calibration landscape that is also convex but is not smooth. It is piecewise linear, with points of non-differentiability (\"kinks\") at each value of $b$ such that $b = -e_i(0)$. Its minimizer, the median, is robust to outliers. The magnitude of an extreme residual does not influence the median's position, only its rank. For our data, the median of the residuals is $0.8$. This indicates that half of the time the model over-predicts and half the time it under-predicts relative to this value. The large outlier $-14.8$ has no more influence on the median than the value $-6.2$ does. The resulting optimal bias, $b_{\\mathrm{MAE}} = -0.8$, suggests a correction based on the central tendency of the bulk of the data, effectively ignoring the extreme outlier and concluding that the model generally over-predicts precipitation. The substantial difference between $b_{\\mathrm{RMSE}} \\approx 0.1083$ and $b_{\\mathrm{MAE}} = -0.8$ is a direct consequence of how each metric handles the outlier.\n\nFinally, for the purpose of grading, we report the ratio of the RMSE to the MAE at $b=0$.\n$$\n\\text{Ratio} = \\frac{\\mathrm{RMSE}(0)}{\\mathrm{MAE}(0)} = \\frac{\\sqrt{390.11/12}}{4.175} \\approx \\frac{5.70168}{4.175} \\approx 1.36567\n$$\nRounding to four significant figures, the ratio is $1.366$.",
            "answer": "$$\n\\boxed{1.366}\n$$"
        },
        {
            "introduction": "Standard aggregate metrics like RMSE can sometimes provide a misleading picture of a model's true utility, especially when performance during rare, high-impact events is critical. This practice presents a thought experiment where a model with a superior overall RMSE performs catastrophically in the tail of the distribution, a common challenge in fields like flood forecasting. This exercise will demonstrate the need for and the design of composite objective functions that explicitly and separately weigh performance in the bulk of the distribution versus the extremes, allowing for a more nuanced and application-specific model evaluation .",
            "id": "3899989",
            "problem": "Consider an environmental hydrology model that predicts daily river discharge $\\hat{Q}_t$ for day $t$, with observations $Q_t$. Let the prediction error be $e_t = \\hat{Q}_t - Q_t$. Define the Root Mean Square Error (RMSE) as $\\mathrm{RMSE} = \\sqrt{\\mathbb{E}(e_t^2)}$, where $\\mathbb{E}(\\cdot)$ denotes expectation over the daily distribution. Suppose we are interested in both the bulk flow regime and the high-flow tail regime. Define a tail indicator $I_t = \\mathbf{1}\\{Q_t > T\\}$, where $T$ is the $99.9\\%$ empirical quantile of $Q_t$, so that the probability of tail exceedance is $p = \\mathbb{P}(I_t = 1) = 0.001$. Consider two competing models, $\\mathcal{M}_1$ and $\\mathcal{M}_2$, whose error distributions are conditionally Gaussian with mean zero and regime-dependent variances:\n- Bulk regime $(I_t = 0)$: Under $\\mathcal{M}_1$, $e_t \\mid (I_t = 0) \\sim \\mathcal{N}(0, \\sigma_{b,1}^2)$ with $\\sigma_{b,1} = 0.5$, and under $\\mathcal{M}_2$, $e_t \\mid (I_t = 0) \\sim \\mathcal{N}(0, \\sigma_{b,2}^2)$ with $\\sigma_{b,2} = 0.8$.\n- Tail regime $(I_t = 1)$: Under $\\mathcal{M}_1$, $e_t \\mid (I_t = 1) \\sim \\mathcal{N}(0, \\sigma_{t,1}^2)$ with $\\sigma_{t,1} = 20$, and under $\\mathcal{M}_2$, $e_t \\mid (I_t = 1) \\sim \\mathcal{N}(0, \\sigma_{t,2}^2)$ with $\\sigma_{t,2} = 5$.\n\nAssume the mixture $(I_t)$ and conditional errors are independent and identically distributed over $t$, and that the mixture probability $p$ and conditional variances are stationary.\n\nYour tasks are:\n1. Using only the fundamental definitions of expectation for mixtures and the definition of RMSE, derive the unconditional RMSE for $\\mathcal{M}_1$ and $\\mathcal{M}_2$, and the conditional tail RMSE (restricted to $I_t = 1$) for both models. Explain how the unconditional RMSE may prefer a model that underperforms substantially in the tail due to the small probability $p$.\n2. Propose a mathematically coherent composite objective $J(x)$ that explicitly balances bulk and tail fit for a generic model with parameters $x$, using conditional terms that isolate bulk and tail performance and a tunable weight that reflects the decision-maker’s emphasis on extreme events. The objective must avoid double counting the bulk contribution and must be well-defined under the mixture structure described.\n\nSelect the option that both correctly establishes the RMSE and tail-RMSE ordering for $\\mathcal{M}_1$ versus $\\mathcal{M}_2$ and proposes a valid composite objective as specified in task 2.\n\nA. The unconditional RMSEs satisfy $\\mathrm{RMSE}(\\mathcal{M}_1)  \\mathrm{RMSE}(\\mathcal{M}_2)$, while the conditional tail RMSEs satisfy $\\mathrm{RMSE}_{\\mathrm{tail}}(\\mathcal{M}_1) \\gg \\mathrm{RMSE}_{\\mathrm{tail}}(\\mathcal{M}_2)$. A valid composite objective is $J(x) = (1 - \\alpha)\\,\\mathrm{RMSE}_{\\mathrm{bulk}}(x) + \\alpha\\,\\mathrm{RMSE}_{\\mathrm{tail}}(x)$, where $\\mathrm{RMSE}_{\\mathrm{bulk}}(x) = \\sqrt{\\mathbb{E}(e_x^2 \\mid I_t = 0)}$ and $\\mathrm{RMSE}_{\\mathrm{tail}}(x) = \\sqrt{\\mathbb{E}(e_x^2 \\mid I_t = 1)}$, and $\\alpha \\in [0,1]$ is a user-chosen weight.\n\nB. The unconditional RMSEs satisfy $\\mathrm{RMSE}(\\mathcal{M}_1) > \\mathrm{RMSE}(\\mathcal{M}_2)$. To correct tail issues, use $J(x) = \\sqrt{\\mathbb{E}(e_x^2)} + \\alpha\\,\\sqrt{\\mathbb{E}(e_x^2 \\mid I_t = 1)}$, which adds a tail penalty to the standard RMSE.\n\nC. Squaring magnifies rare extremes, so $\\mathrm{RMSE}(\\mathcal{M}_1) > \\mathrm{RMSE}(\\mathcal{M}_2)$ is guaranteed. A tail-focused objective $J(x) = \\max_t |e_{x,t}|$ is sufficient to balance bulk and tail performance.\n\nD. Because discharge is positive and skewed, log-transforming fixes tail underfit; use $J(x) = \\sqrt{\\mathbb{E}((\\log \\hat{Q}_x - \\log Q)^2)}$ in place of RMSE to balance bulk and tail performance without separate tail terms.",
            "solution": "The user wants a critical validation of the problem statement, followed by a full derivation and evaluation of the options.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n-   Model prediction: $\\hat{Q}_t$ (daily river discharge)\n-   Observation: $Q_t$\n-   Prediction error: $e_t = \\hat{Q}_t - Q_t$\n-   Root Mean Square Error (RMSE) definition: $\\mathrm{RMSE} = \\sqrt{\\mathbb{E}(e_t^2)}$\n-   Tail indicator: $I_t = \\mathbf{1}\\{Q_t > T\\}$, where $T$ is the $99.9\\%$ empirical quantile of $Q_t$.\n-   Probability of tail exceedance: $p = \\mathbb{P}(I_t = 1) = 0.001$.\n-   Probability of bulk regime: $\\mathbb{P}(I_t = 0) = 1 - p = 0.999$.\n-   Two models, $\\mathcal{M}_1$ and $\\mathcal{M}_2$, with conditionally Gaussian errors of mean zero.\n-   Model $\\mathcal{M}_1$:\n    -   Bulk regime ($I_t = 0$): $e_t \\mid (I_t = 0) \\sim \\mathcal{N}(0, \\sigma_{b,1}^2)$ with $\\sigma_{b,1} = 0.5$.\n    -   Tail regime ($I_t = 1$): $e_t \\mid (I_t = 1) \\sim \\mathcal{N}(0, \\sigma_{t,1}^2)$ with $\\sigma_{t,1} = 20$.\n-   Model $\\mathcal{M}_2$:\n    -   Bulk regime ($I_t = 0$): $e_t \\mid (I_t = 0) \\sim \\mathcal{N}(0, \\sigma_{b,2}^2)$ with $\\sigma_{b,2} = 0.8$.\n    -   Tail regime ($I_t = 1$): $e_t \\mid (I_t = 1) \\sim \\mathcal{N}(0, \\sigma_{t,2}^2)$ with $\\sigma_{t,2} = 5$.\n-   Assumptions: $I_t$ and conditional errors are i.i.d. over $t$; $p$ and conditional variances are stationary.\n\n**Step 2: Validate Using Extracted Givens**\n\nThe problem statement is a well-defined exercise in probability and statistics, applied to the common and relevant field of environmental model evaluation.\n1.  **Scientifically Grounded**: The problem is grounded in standard statistical theory (mixture models, expectation, variance, Normal distribution) and its application to hydrological modeling, a key area of environmental science. The setup of evaluating models based on their performance in different flow regimes (bulk vs. tail/extreme) is a standard and critical task in the field. All concepts are factual and mainstream.\n2.  **Well-Posed**: The problem is well-posed. All necessary parameters ($p$, $\\sigma$ values for both models and regimes) are provided to perform the required calculations. The tasks are clearly stated, and the conditions for a valid composite objective are specified, allowing for a structured evaluation. A unique numerical answer exists for the RMSE calculations.\n3.  **Objective**: The language is formal, precise, and free of any subjective or ambiguous terminology.\n4.  **No Flaws**: The problem does not violate any of the invalidity criteria. It is complete, consistent, realistic (as a simplified model), and requires substantive reasoning about the properties of statistical metrics.\n\n**Step 3: Verdict and Action**\n\nThe problem is **valid**. The solution process will proceed.\n\n### Derivation and Solution\n\n**Task 1: RMSE Calculation and Analysis**\n\nThe unconditional Mean Squared Error (MSE), $\\mathbb{E}(e_t^2)$, can be calculated using the law of total expectation over the mixture defined by the indicator $I_t$.\n$$\n\\mathbb{E}(e_t^2) = \\mathbb{E}(e_t^2 \\mid I_t = 0) \\mathbb{P}(I_t = 0) + \\mathbb{E}(e_t^2 \\mid I_t = 1) \\mathbb{P}(I_t = 1)\n$$\nFor a random variable $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$, the second moment is given by $\\mathbb{E}(X^2) = \\mathrm{Var}(X) + (\\mathbb{E}(X))^2$. In this problem, the conditional errors have a mean of $0$, so $\\mathbb{E}(e_t \\mid I_t) = 0$. Therefore, the conditional second moment is equal to the conditional variance:\n-   $\\mathbb{E}(e_t^2 \\mid I_t = 0) = \\sigma_b^2$ (bulk regime)\n-   $\\mathbb{E}(e_t^2 \\mid I_t = 1) = \\sigma_t^2$ (tail regime)\n\nThe unconditional MSE is thus:\n$$\n\\mathrm{MSE} = (1-p) \\sigma_b^2 + p \\sigma_t^2\n$$\nThe RMSE is the square root of the MSE.\n\n**For Model $\\mathcal{M}_1$:**\n-   Given: $\\sigma_{b,1} = 0.5$, $\\sigma_{t,1} = 20$, $p = 0.001$.\n-   $\\mathrm{MSE}(\\mathcal{M}_1) = (1 - 0.001) \\times (0.5)^2 + 0.001 \\times (20)^2$\n-   $\\mathrm{MSE}(\\mathcal{M}_1) = 0.999 \\times 0.25 + 0.001 \\times 400$\n-   $\\mathrm{MSE}(\\mathcal{M}_1) = 0.24975 + 0.4 = 0.64975$\n-   $\\mathrm{RMSE}(\\mathcal{M}_1) = \\sqrt{0.64975} \\approx 0.80607$\n\n**For Model $\\mathcal{M}_2$:**\n-   Given: $\\sigma_{b,2} = 0.8$, $\\sigma_{t,2} = 5$, $p = 0.001$.\n-   $\\mathrm{MSE}(\\mathcal{M}_2) = (1 - 0.001) \\times (0.8)^2 + 0.001 \\times (5)^2$\n-   $\\mathrm{MSE}(\\mathcal{M}_2) = 0.999 \\times 0.64 + 0.001 \\times 25$\n-   $\\mathrm{MSE}(\\mathcal{M}_2) = 0.63936 + 0.025 = 0.66436$\n-   $\\mathrm{RMSE}(\\mathcal{M}_2) = \\sqrt{0.66436} \\approx 0.81514$\n\nComparing the unconditional RMSEs: $\\mathrm{RMSE}(\\mathcal{M}_1) \\approx 0.806  0.815 \\approx \\mathrm{RMSE}(\\mathcal{M}_2)$. Thus, $\\mathrm{RMSE}(\\mathcal{M}_1)  \\mathrm{RMSE}(\\mathcal{M}_2)$.\n\n**Conditional Tail RMSE:**\nThe conditional tail RMSE is the RMSE evaluated only for events in the tail regime ($I_t = 1$).\n$$\n\\mathrm{RMSE}_{\\mathrm{tail}} = \\sqrt{\\mathbb{E}(e_t^2 \\mid I_t = 1)}\n$$\nAs established, $\\mathbb{E}(e_t^2 \\mid I_t = 1) = \\sigma_t^2$. Therefore, $\\mathrm{RMSE}_{\\mathrm{tail}} = \\sqrt{\\sigma_t^2} = \\sigma_t$.\n\n-   For $\\mathcal{M}_1$: $\\mathrm{RMSE}_{\\mathrm{tail}}(\\mathcal{M}_1) = \\sigma_{t,1} = 20$.\n-   For $\\mathcal{M}_2$: $\\mathrm{RMSE}_{\\mathrm{tail}}(\\mathcal{M}_2) = \\sigma_{t,2} = 5$.\n\nComparing the conditional tail RMSEs: $\\mathrm{RMSE}_{\\mathrm{tail}}(\\mathcal{M}_1) = 20$ and $\\mathrm{RMSE}_{\\mathrm{tail}}(\\mathcal{M}_2) = 5$. Clearly, $\\mathrm{RMSE}_{\\mathrm{tail}}(\\mathcal{M}_1) > \\mathrm{RMSE}_{\\mathrm{tail}}(\\mathcal{M}_2)$. The ratio is $4:1$, so the phrasing $\\mathrm{RMSE}_{\\mathrm{tail}}(\\mathcal{M}_1) \\gg \\mathrm{RMSE}_{\\mathrm{tail}}(\\mathcal{M}_2)$ is justified.\n\n**Explanation of Misleading Unconditional RMSE:**\nThe unconditional RMSE gives preference to model $\\mathcal{M}_1$ despite its catastrophic failure in the high-flow tail regime (an error standard deviation of $20$ vs. $5$ for $\\mathcal{M}_2$). This occurs because the unconditional RMSE is a probability-weighted average. The bulk regime, where $\\mathcal{M}_1$ excels ($\\sigma_{b,1} = 0.5$ vs. $\\sigma_{b,2} = 0.8$), occurs with probability $0.999$, while the tail regime occurs with probability $0.001$. The superior performance of $\\mathcal{M}_1$ across $99.9\\%$ of events is sufficient to overcome its extremely poor performance in the rare $0.1\\%$ of events, resulting in a lower overall RMSE. For applications where tail performance is critical (e.g., flood forecasting), this highlights the danger of relying solely on aggregate metrics like RMSE.\n\n**Task 2: Proposing a Composite Objective Function**\n\nThe task requires a mathematically coherent objective $J(x)$ that:\n1.  Explicitly balances bulk and tail fit.\n2.  Uses conditional terms that isolate bulk and tail performance.\n3.  Includes a tunable weight for the decision-maker's emphasis.\n4.  Avoids double counting.\n\nA natural way to construct such an objective is to define metrics for each disjoint regime and combine them using a user-specified weight. The conditional RMSEs are perfect for this. Let $x$ represent the parameters of a generic model.\n-   Bulk performance metric: $\\mathrm{RMSE}_{\\mathrm{bulk}}(x) = \\sqrt{\\mathbb{E}(e_x^2 \\mid I_t = 0)}$\n-   Tail performance metric: $\\mathrm{RMSE}_{\\mathrm{tail}}(x) = \\sqrt{\\mathbb{E}(e_x^2 \\mid I_t = 1)}$\n\nA composite objective function can be a weighted sum of these two metrics. Let $\\alpha \\in [0,1]$ be the tunable weight representing the relative importance of tail performance.\n$$\nJ(x) = (1 - \\alpha)\\,\\mathrm{RMSE}_{\\mathrm{bulk}}(x) + \\alpha\\,\\mathrm{RMSE}_{\\mathrm{tail}}(x)\n$$\nThis structure satisfies all requirements:\n1.  It balances bulk and tail fit via the weight $\\alpha$. If $\\alpha=0$, we only consider bulk performance. If $\\alpha=1$, we only consider tail performance. If $\\alpha=0.5$, we weight them equally.\n2.  It uses the conditional RMSEs, which by definition isolate the performance in the two regimes.\n3.  $\\alpha$ is a tunable weight.\n4.  Since the events $I_t=0$ and $I_t=1$ form a partition of the event space, the metrics $\\mathrm{RMSE}_{\\mathrm{bulk}}$ and $\\mathrm{RMSE}_{\\mathrm{tail}}$ are calculated on disjoint sets of data. Combining them does not constitute double counting. It is a re-weighting of the importance of the two partitions, from the \"natural\" weights ($p, 1-p$) to user-defined weights ($\\alpha, 1-\\alpha$).\n\n### Option-by-Option Analysis\n\n**A. The unconditional RMSEs satisfy $\\mathrm{RMSE}(\\mathcal{M}_1)  \\mathrm{RMSE}(\\mathcal{M}_2)$, while the conditional tail RMSEs satisfy $\\mathrm{RMSE}_{\\mathrm{tail}}(\\mathcal{M}_1) \\gg \\mathrm{RMSE}_{\\mathrm{tail}}(\\mathcal{M}_2)$. A valid composite objective is $J(x) = (1 - \\alpha)\\,\\mathrm{RMSE}_{\\mathrm{bulk}}(x) + \\alpha\\,\\mathrm{RMSE}_{\\mathrm{tail}}(x)$, where $\\mathrm{RMSE}_{\\mathrm{bulk}}(x) = \\sqrt{\\mathbb{E}(e_x^2 \\mid I_t = 0)}$ and $\\mathrm{RMSE}_{\\mathrm{tail}}(x) = \\sqrt{\\mathbb{E}(e_x^2 \\mid I_t = 1)}$, and $\\alpha \\in [0,1]$ is a user-chosen weight.**\n\n-   The statement $\\mathrm{RMSE}(\\mathcal{M}_1)  \\mathrm{RMSE}(\\mathcal{M}_2)$ is consistent with our calculation ($0.806  0.815$).\n-   The statement $\\mathrm{RMSE}_{\\mathrm{tail}}(\\mathcal{M}_1) \\gg \\mathrm{RMSE}_{\\mathrm{tail}}(\\mathcal{M}_2)$ is consistent with our calculation ($20 \\gg 5$).\n-   The proposed objective function $J(x)$ is exactly the valid form derived above, which meets all criteria of the problem.\n-   Verdict: **Correct**\n\n**B. The unconditional RMSEs satisfy $\\mathrm{RMSE}(\\mathcalM}_1) > \\mathrm{RMSE}(\\mathcalM}_2)$. To correct tail issues, use $J(x) = \\sqrt{\\mathbb{E}(e_x^2)} + \\alpha\\,\\sqrt{\\mathbb{E}(e_x^2 \\mid I_t = 1)}$, which adds a tail penalty to the standard RMSE.**\n\n-   The statement $\\mathrm{RMSE}(\\mathcal{M}_1) > \\mathrm{RMSE}(\\mathcal{M}_2)$ contradicts our calculation.\n-   The proposed objective $J(x) = \\mathrm{RMSE}(x) + \\alpha\\,\\mathrm{RMSE}_{\\mathrm{tail}}(x)$ does not isolate bulk and tail performance and explicitly double-counts the contribution from tail events. The tail errors are part of the first term, $\\mathrm{RMSE}(x)$, and are then added again in the second term. This violates the problem's implicit requirement for a clean separation and balancing.\n-   Verdict: **Incorrect**\n\n**C. Squaring magnifies rare extremes, so $\\mathrm{RMSE}(\\mathcal{M}_1) > \\mathrm{RMSE}(\\mathcal{M}_2)$ is guaranteed. A tail-focused objective $J(x) = \\max_t |e_{x,t}|$ is sufficient to balance bulk and tail performance.**\n\n-   The premise $\\mathrm{RMSE}(\\mathcal{M}_1) > \\mathrm{RMSE}(\\mathcal{M}_2)$ is false, as shown by direct calculation. While squaring magnifies large errors, their contribution to the final expectation is attenuated by their small probability of occurrence.\n-   The proposed objective $J(x) = \\max_t |e_{x,t}|$ (the $L_\\infty$ error) focuses exclusively on the single worst prediction, completely ignoring performance across the rest of the distribution. This is an extreme objective, not one that *balances* bulk and tail performance as requested.\n-   Verdict: **Incorrect**\n\n**D. Because discharge is positive and skewed, log-transforming fixes tail underfit; use $J(x) = \\sqrt{\\mathbb{E}((\\log \\hat{Q}_x - \\log Q)^2)}$ in place of RMSE to balance bulk and tail performance without separate tail terms.**\n\n-   While log-transformation is a valid technique in hydrology, it is offered here as a replacement for an explicit, tunable, composite objective. The problem specifically asks for an objective with \"conditional terms that isolate bulk and tail performance and a tunable weight\". This proposal, the Root Mean Squared Logarithmic Error (RMSLE), is a single, non-composite metric and lacks any tunable weight to balance explicit tail vs. bulk performance. It does not fulfill the requirements of Task 2.\n-   Verdict: **Incorrect**",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "Objective functions are not limited to post-hoc model evaluation; they are a cornerstone of data assimilation, the science of optimally blending model predictions with incoming observations. This exercise introduces the 3D-Var (Three-Dimensional Variational) framework in a simplified scalar context, tasking you with minimizing a cost function to find the best estimate of a system's state. You will derive how the optimal solution, or 'analysis,' creates a statistically informed balance between the model forecast and the observation, weighted by their respective error variances, providing a powerful example of an objective function used for data fusion .",
            "id": "3899992",
            "problem": "Consider a single-state, single-observation Three-Dimensional Variational (3D-Var) data assimilation for near-surface air temperature at a single grid point. Let the state be the scalar temperature $x$ expressed in $\\mathrm{K}$, with a prior (background) estimate $x_b$ and a single observation $y$. Assume that background and observation errors are unbiased, Gaussian, and uncorrelated, with background error variance $B$ and observation error variance $R$. The observation operator is linear and, for this scalar case, equal to the identity, so $H = 1$. The 3D-Var objective function is defined as\n$$\nJ(x) = \\tfrac{1}{2}\\,(x - x_b)\\,B^{-1}\\,(x - x_b) + \\tfrac{1}{2}\\,(y - Hx)\\,R^{-1}\\,(y - Hx).\n$$\nStarting from this definition and first principles of least-squares optimality, derive the stationarity condition by taking the derivative of $J(x)$ with respect to $x$ and setting it to zero, then solve for the analysis $x_a$ and the analysis increment $\\delta x \\equiv x_a - x_b$ in closed form. Use your derivation to explicitly show how $B$ and $R$ determine the balance between the background and the observation in the scalar increment formula.\n\nFinally, evaluate the analysis increment for the following physically plausible case: $x_b = 290\\,\\mathrm{K}$, $y = 293.7\\,\\mathrm{K}$, $B = 5\\,\\mathrm{K}^2$, $R = 2\\,\\mathrm{K}^2$, and $H = 1$. Express the analysis increment in $\\mathrm{K}$ and round your answer to $4$ significant figures. The final reported quantity must be the single real value of $\\delta x$.",
            "solution": "The problem is first validated to ensure it is scientifically sound, well-posed, and complete.\n\n### Step 1: Extract Givens\n-   State variable: scalar temperature $x$ in $\\mathrm{K}$.\n-   Prior (background) estimate: $x_b$.\n-   Single observation: $y$.\n-   Background error variance: $B$.\n-   Observation error variance: $R$.\n-   Error assumptions: background and observation errors are unbiased, Gaussian, and uncorrelated.\n-   Observation operator: $H = 1$.\n-   Objective function: $J(x) = \\tfrac{1}{2}\\,(x - x_b)\\,B^{-1}\\,(x - x_b) + \\tfrac{1}{2}\\,(y - Hx)\\,R^{-1}\\,(y - Hx)$.\n-   Definition of analysis increment: $\\delta x \\equiv x_a - x_b$.\n-   Numerical values for evaluation: $x_b = 290\\,\\mathrm{K}$, $y = 293.7\\,\\mathrm{K}$, $B = 5\\,\\mathrm{K}^2$, $R = 2\\,\\mathrm{K}^2$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, presenting a standard formulation of Three-Dimensional Variational (3D-Var) data assimilation, a fundamental technique in meteorology and oceanography. The objective function is the correct negative log-likelihood for the given Gaussian error assumptions. The problem is well-posed; the objective function $J(x)$ is a sum of quadratic terms with positive coefficients (variances $B$ and $R$ are positive), making it a strictly convex function with a unique minimum. The problem is objective, complete, and consistent, with all necessary definitions and values provided for a unique solution. The provided numerical values are physically plausible for near-surface air temperature.\n\n### Step 3: Verdict and Action\nThe problem is valid. A full derivation and solution will be provided.\n\n### Derivation\nThe objective function to be minimized is given as:\n$$\nJ(x) = \\tfrac{1}{2}\\,(x - x_b)\\,B^{-1}\\,(x - x_b) + \\tfrac{1}{2}\\,(y - Hx)\\,R^{-1}\\,(y - Hx)\n$$\nIn this scalar case, the variables $x$, $x_b$, and $y$ are scalars. The error variances $B$ and $R$ are also scalars, so their inverses are simply $B^{-1} = \\frac{1}{B}$ and $R^{-1} = \\frac{1}{R}$. The observation operator is the identity, $H = 1$. The objective function simplifies to:\n$$\nJ(x) = \\frac{1}{2B}(x - x_b)^2 + \\frac{1}{2R}(y - x)^2\n$$\nThe minimum of the objective function corresponds to the optimal state estimate, known as the analysis, $x_a$. This minimum is found at the stationary point where the first derivative of $J(x)$ with respect to $x$ is zero. We compute the derivative:\n$$\n\\frac{dJ}{dx} = \\frac{d}{dx} \\left( \\frac{1}{2B}(x - x_b)^2 \\right) + \\frac{d}{dx} \\left( \\frac{1}{2R}(y - x)^2 \\right)\n$$\nUsing the chain rule, we have:\n$$\n\\frac{dJ}{dx} = \\frac{1}{2B} \\cdot 2(x - x_b) \\cdot 1 + \\frac{1}{2R} \\cdot 2(y - x) \\cdot (-1)\n$$\n$$\n\\frac{dJ}{dx} = \\frac{1}{B}(x - x_b) - \\frac{1}{R}(y - x)\n$$\nSetting the derivative to zero to find the analysis $x_a$:\n$$\n\\frac{1}{B}(x_a - x_b) - \\frac{1}{R}(y - x_a) = 0\n$$\nNow, we solve for $x_a$:\n$$\n\\frac{x_a}{B} - \\frac{x_b}{B} = \\frac{y}{R} - \\frac{x_a}{R}\n$$\n$$\nx_a \\left( \\frac{1}{B} + \\frac{1}{R} \\right) = \\frac{x_b}{B} + \\frac{y}{R}\n$$\n$$\nx_a \\left( \\frac{R + B}{BR} \\right) = \\frac{x_b R + y B}{BR}\n$$\nMultiplying both sides by $BR$ yields the expression for the analysis $x_a$:\n$$\nx_a(R+B) = x_b R + y B\n$$\n$$\nx_a = \\frac{R}{B+R}x_b + \\frac{B}{B+R}y\n$$\nThis equation shows that the analysis $x_a$ is a weighted average of the background $x_b$ and the observation $y$. The weights are determined by the respective error variances.\n\nNext, we derive the analysis increment, $\\delta x$, defined as $\\delta x = x_a - x_b$:\n$$\n\\delta x = \\left( \\frac{R}{B+R}x_b + \\frac{B}{B+R}y \\right) - x_b\n$$\n$$\n\\delta x = \\left( \\frac{R}{B+R} - 1 \\right)x_b + \\frac{B}{B+R}y\n$$\n$$\n\\delta x = \\left( \\frac{R - (B+R)}{B+R} \\right)x_b + \\frac{B}{B+R}y\n$$\n$$\n\\delta x = \\left( \\frac{-B}{B+R} \\right)x_b + \\frac{B}{B+R}y\n$$\nFactoring out the common term $\\frac{B}{B+R}$:\n$$\n\\delta x = \\frac{B}{B+R} (y - x_b)\n$$\nThis is the closed-form expression for the analysis increment. The term $(y - x_b)$ is the innovation, and the coefficient $K = \\frac{B}{B+R}$ is the Kalman gain for this scalar case.\n\nThe balance between the background and the observation is explicitly determined by the ratio of their error variances, $B$ and $R$, as captured by the gain $K$.\n-   If the background is considered highly accurate (i.e., its error variance $B$ is small, $B \\to 0$), then the gain $K \\to 0$. The increment $\\delta x \\to 0$, and the analysis $x_a = x_b + \\delta x \\approx x_b$. The system trusts the background.\n-   If the observation is considered highly accurate (i.e., its error variance $R$ is small, $R \\to 0$), then the gain $K = \\frac{B}{B+R} \\to \\frac{B}{B} = 1$. The increment $\\delta x \\to y - x_b$, and the analysis $x_a = x_b + (y - x_b) = y$. The system trusts the observation.\nThus, the increment corrects the background by a fraction of the innovation, where that fraction is determined by the relative uncertainty of the background compared to the total uncertainty ($B+R$).\n\n### Numerical Evaluation\nWe are given the following values:\n-   $x_b = 290\\,\\mathrm{K}$\n-   $y = 293.7\\,\\mathrm{K}$\n-   $B = 5\\,\\mathrm{K}^2$\n-   $R = 2\\,\\mathrm{K}^2$\n\nFirst, calculate the innovation:\n$$\ny - x_b = 293.7 - 290 = 3.7\\,\\mathrm{K}\n$$\nNext, calculate the analysis increment $\\delta x$ using the derived formula:\n$$\n\\delta x = \\frac{B}{B+R} (y - x_b)\n$$\n$$\n\\delta x = \\frac{5}{5+2} (3.7) = \\frac{5}{7} (3.7)\n$$\n$$\n\\delta x = \\frac{18.5}{7} \\approx 2.642857... \\,\\mathrm{K}\n$$\nThe problem requires rounding the answer to $4$ significant figures.\n$$\n\\delta x \\approx 2.643\\,\\mathrm{K}\n$$\nThe final requested quantity is the single real value of $\\delta x$.",
            "answer": "$$\n\\boxed{2.643}\n$$"
        }
    ]
}