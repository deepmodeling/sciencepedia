{
    "hands_on_practices": [
        {
            "introduction": "在开始复杂的模型校准之前，关键的第一步是确定模型的参数是否可以从可用数据中被唯一地识别出来。本练习将介绍“参数含糊性”（sloppiness）的概念，在这种情况下，某些参数组合受到很弱的约束，导致校准结果不可靠。通过分析费雪信息矩阵（Fisher Information Matrix, FIM）的特征值，你将学习一种诊断参数可识别性问题的强大技术，并理解模型结构和实验设计如何影响参数估计的可靠性。",
            "id": "3894183",
            "problem": "考虑在一个简单的环境箱式模型设置中，在加性独立高斯噪声下的参数可识别性问题。假设一个模型预测在时间 $t_{i}$（其中 $i \\in \\{1,\\ldots,n\\}$）的观测值为 $y_{i}(\\boldsymbol{\\theta})$，参数向量为 $\\boldsymbol{\\theta} \\in \\mathbb{R}^{p}$。在标准差为 $\\sigma_{i}$ 的独立高斯误差下，负对数似然与加权最小二乘目标成正比。在 $\\boldsymbol{\\theta}$ 处的费雪信息矩阵 (FIM) 等于该目标的 Hessian 矩阵的高斯-牛顿近似，由下式给出：\n$$\n\\mathbf{F}(\\boldsymbol{\\theta}) \\equiv \\mathbf{J}(\\boldsymbol{\\theta})^{\\top}\\,\\mathbf{W}\\,\\mathbf{J}(\\boldsymbol{\\theta}),\n$$\n其中 $\\mathbf{J}(\\boldsymbol{\\theta}) \\in \\mathbb{R}^{n \\times p}$ 是灵敏度（雅可比）矩阵，其元素为 $J_{ij}(\\boldsymbol{\\theta}) = \\frac{\\partial y_{i}(\\boldsymbol{\\theta})}{\\partial \\theta_{j}}$，且 $\\mathbf{W} = \\mathrm{diag}(w_{1},\\ldots,w_{n})$，其中 $w_{i} = \\sigma_{i}^{-2}$。当 $\\mathbf{J}(\\boldsymbol{\\theta})$ 的列近似共线时，$\\mathbf{F}(\\boldsymbol{\\theta})$ 会展现出一个特征值谱，其特征值的幅度差异巨大；小的特征值对应于弱可识别性的方向（即所谓的“草率性”）。为了进行数值评估，定义条件数\n$$\n\\kappa(\\mathbf{F}) \\equiv \\frac{\\lambda_{\\max}(\\mathbf{F})}{\\max\\{\\lambda_{\\min}(\\mathbf{F}), \\delta\\}},\n$$\n其中 $\\lambda_{\\max}$ 和 $\\lambda_{\\min}$ 分别是 $\\mathbf{F}$ 的最大和最小特征值，而 $\\delta$ 是一个小的正数，仅用于在数值计算中避免除以零（但不改变秩的判定）。如果对于一个指定的阈值 $\\tau$，有 $\\kappa(\\mathbf{F}) > \\tau$，则认为模型在 $\\boldsymbol{\\theta}$ 处是“草率的”。对于数值秩，使用\n$$\n\\mathrm{rank}_{\\epsilon}(\\mathbf{F}) \\equiv \\#\\{\\lambda_{j}(\\mathbf{F}) : \\lambda_{j}(\\mathbf{F}) \\ge \\epsilon \\cdot \\lambda_{\\max}(\\mathbf{F})\\},\n$$\n容差为 $\\epsilon$。\n\n本问题中所有量均为无量纲。\n\n您必须实现一个程序，对于以下每个测试用例，在指定的 $\\boldsymbol{\\theta}$ 处构造 $\\mathbf{J}(\\boldsymbol{\\theta})$，形成 $\\mathbf{F}(\\boldsymbol{\\theta})$，计算最小和最大特征值、条件数、使用阈值 $\\tau$ 的布尔型草率性分类，以及使用下面指定的容差 $\\epsilon$ 的数值秩。然后将所有测试用例的结果汇总成最终的输出格式。\n\n待使用的模型及其灵敏度：\n- 模型 $\\mathcal{M}_{1}$（“带振幅的单室指数衰减”）：$y(t; a, k) = a\\,\\exp(-k\\,t)$，参数为 $\\boldsymbol{\\theta} = [a, k]^{\\top}$。灵敏度为\n$$\n\\frac{\\partial y}{\\partial a}(t; a, k) = \\exp(-k\\,t), \\quad \\frac{\\partial y}{\\partial k}(t; a, k) = -a\\,t\\,\\exp(-k\\,t).\n$$\n- 模型 $\\mathcal{M}_{2}$（“乘积模糊性”）：$y(t; \\alpha, \\beta) = (\\alpha\\,\\beta)\\,\\exp(-t)$，参数为 $\\boldsymbol{\\theta} = [\\alpha, \\beta]^{\\top}$。灵敏度为\n$$\n\\frac{\\partial y}{\\partial \\alpha}(t; \\alpha, \\beta) = \\beta\\,\\exp(-t), \\quad \\frac{\\partial y}{\\partial \\beta}(t; \\alpha, \\beta) = \\alpha\\,\\exp(-t).\n$$\n\n在所有情况下使用以下全局数值常量：$\\delta = 10^{-18}$（用于条件数分母保护），$\\epsilon = 10^{-12}$（用于数值秩），以及 $\\tau = 10^{6}$（用于草率性分类）。\n\n测试套件（每个案例指定了模型、参数向量 $\\boldsymbol{\\theta}$、观测时间 $\\{t_{i}\\}$ 和标准差 $\\{\\sigma_{i}\\}$）：\n- 案例1（理想路径，灵敏度条件中等良好）：\n  - 模型 $\\mathcal{M}_{1}$，$\\boldsymbol{\\theta} = [1.0, 0.4]^{\\top}$。\n  - 时间：$t \\in \\{0, 1, 2, 5, 10\\}$。\n  - 标准差：所有 $i$ 的 $\\sigma_{i} = 0.05$。\n- 案例2（由于时间跨度有限，灵敏度出现近似共线性，导致草率性）：\n  - 模型 $\\mathcal{M}_{1}$，$\\boldsymbol{\\theta} = [1.0, 0.5]^{\\top}$。\n  - 时间：$t \\in \\{0, 0.02, 0.04, 0.06, 0.08\\}$。\n  - 标准差：所有 $i$ 的 $\\sigma_{i} = 0.05$。\n- 案例3（由于乘积模糊性导致的结构不可识别性；精确秩亏）：\n  - 模型 $\\mathcal{M}_{2}$，$\\boldsymbol{\\theta} = [2.0, 0.5]^{\\top}$。\n  - 时间：$t \\in \\{0, 1, 2, 3\\}$。\n  - 标准差：所有 $i$ 的 $\\sigma_{i} = 0.1$。\n- 案例4（边界加权：在 $t=0$ 处的压倒性精确观测放大了某个方向，增加了 $\\kappa$）：\n  - 模型 $\\mathcal{M}_{1}$，$\\boldsymbol{\\theta} = [1.0, 0.4]^{\\top}$。\n  - 时间：$t \\in \\{0, 1, 2, 3, 4\\}$。\n  - 标准差：$\\sigma = [10^{-6}, 0.1, 0.1, 0.1, 0.1]$，与所列时间对齐。\n\n对于每个案例，计算：\n- $\\mathbf{F}(\\boldsymbol{\\theta})$ 的最小特征值 $\\lambda_{\\min}$。\n- $\\mathbf{F}(\\boldsymbol{\\theta})$ 的最大特征值 $\\lambda_{\\max}$。\n- 按上述定义计算的条件数 $\\kappa(\\mathbf{F})$，在分母保护中使用 $\\delta = 10^{-18}$。\n- 由 $\\kappa(\\mathbf{F}) > \\tau$ 定义的草率性布尔值，其中 $\\tau = 10^{6}$。\n- 数值秩 $\\mathrm{rank}_{\\epsilon}(\\mathbf{F})$，其中 $\\epsilon = 10^{-12}$。\n\n您的程序应生成单行输出，其中包含所有案例的结果，形式为方括号内的一个逗号分隔列表。每个案例的结果本身应该是一个列表，顺序为 $[\\lambda_{\\min}, \\lambda_{\\max}, \\kappa, \\text{是否草率}, \\mathrm{rank}_{\\epsilon}]$。例如，一个包含两个假设案例的有效输出格式看起来像 $[[\\ldots],[\\ldots]]$。实际数值应由您的程序计算并以这种精确的嵌套列表格式打印。不允许外部输入；所有值均如上所述。",
            "solution": "用户提供了一个关于使用费雪信息矩阵 (FIM) 对环境模型中参数可识别性进行数值评估的问题。我将首先验证问题的有效性，然后进行详细的解答。\n\n### 步骤1：提取给定信息\n- **费雪信息矩阵 (FIM):** $\\mathbf{F}(\\boldsymbol{\\theta}) \\equiv \\mathbf{J}(\\boldsymbol{\\theta})^{\\top}\\,\\mathbf{W}\\,\\mathbf{J}(\\boldsymbol{\\theta})$，其中 $\\mathbf{J}(\\boldsymbol{\\theta})$ 是灵敏度矩阵，其元素为 $J_{ij} = \\frac{\\partial y_{i}(\\boldsymbol{\\theta})}{\\partial \\theta_{j}}$，$\\mathbf{W} = \\mathrm{diag}(\\sigma_{1}^{-2}, \\ldots, \\sigma_{n}^{-2})$ 是权重矩阵。\n- **条件数：** $\\kappa(\\mathbf{F}) \\equiv \\frac{\\lambda_{\\max}(\\mathbf{F})}{\\max\\{\\lambda_{\\min}(\\mathbf{F}), \\delta\\}}$，其中 $\\lambda_{\\max}$ 和 $\\lambda_{\\min}$ 是 $\\mathbf{F}$ 的最大和最小特征值。\n- **数值秩：** $\\mathrm{rank}_{\\epsilon}(\\mathbf{F}) \\equiv \\#\\{\\lambda_{j}(\\mathbf{F}) : \\lambda_{j}(\\mathbf{F}) \\ge \\epsilon \\cdot \\lambda_{\\max}(\\mathbf{F})\\}$。\n- **草率性标准：** 如果 $\\kappa(\\mathbf{F}) > \\tau$，则认为模型是“草率的”。\n- **全局常量：**\n    - 条件数保护措施：$\\delta = 10^{-18}$。\n    - 数值秩容差：$\\epsilon = 10^{-12}$。\n    - 草率性阈值：$\\tau = 10^{6}$。\n- **模型 $\\mathcal{M}_{1}$:**\n    - 函数形式：$y(t; a, k) = a\\,\\exp(-k\\,t)$。\n    - 参数：$\\boldsymbol{\\theta} = [a, k]^{\\top}$。\n    - 灵敏度：$\\frac{\\partial y}{\\partial a} = \\exp(-k\\,t)$, $\\frac{\\partial y}{\\partial k} = -a\\,t\\,\\exp(-k\\,t)$。\n- **模型 $\\mathcal{M}_{2}$:**\n    - 函数形式：$y(t; \\alpha, \\beta) = (\\alpha\\,\\beta)\\,\\exp(-t)$。\n    - 参数：$\\boldsymbol{\\theta} = [\\alpha, \\beta]^{\\top}$。\n    - 灵敏度：$\\frac{\\partial y}{\\partial \\alpha} = \\beta\\,\\exp(-t)$, $\\frac{\\partial y}{\\partial \\beta} = \\alpha\\,\\exp(-t)$。\n- **测试用例：**\n    - **案例1：** 模型 $\\mathcal{M}_{1}$，$\\boldsymbol{\\theta}=[1.0, 0.4]^{\\top}$，时间 $t \\in \\{0, 1, 2, 5, 10\\}$，所有 $\\sigma_i = 0.05$。\n    - **案例2：** 模型 $\\mathcal{M}_{1}$，$\\boldsymbol{\\theta}=[1.0, 0.5]^{\\top}$，时间 $t \\in \\{0, 0.02, 0.04, 0.06, 0.08\\}$，所有 $\\sigma_i = 0.05$。\n    - **案例3：** 模型 $\\mathcal{M}_{2}$，$\\boldsymbol{\\theta}=[2.0, 0.5]^{\\top}$，时间 $t \\in \\{0, 1, 2, 3\\}$，所有 $\\sigma_i = 0.1$。\n    - **案例4：** 模型 $\\mathcal{M}_{1}$，$\\boldsymbol{\\theta}=[1.0, 0.4]^{\\top}$，时间 $t \\in \\{0, 1, 2, 3, 4\\}$，$\\sigma = [10^{-6}, 0.1, 0.1, 0.1, 0.1]$。\n- **每个案例的所需输出：** 一个包含 $[\\lambda_{\\min}, \\lambda_{\\max}, \\kappa, \\text{是否草率}, \\mathrm{rank}_{\\epsilon}]$ 的列表。\n\n### 步骤2：使用提取的信息进行验证\n1.  **科学依据：** 该问题牢固地植根于非线性模型参数估计和可识别性分析的既定统计理论。费雪信息矩阵、其特征值、条件数和数值秩的使用是该领域的标准技术。所用的模型和情景是用于阐释这些概念的原型。该问题在科学上和数学上都是合理的。\n2.  **良定性：** 该问题是良定的。对于每个案例，所有必要的组成部分（模型、参数、观测时间表、误差结构）都已明确定义，从而可以明确地构建雅可比矩阵 $\\mathbf{J}$、权重矩阵 $\\mathbf{W}$，并因此构建 FIM $\\mathbf{F}$。需要从 $\\mathbf{F}$ 计算的度量标准也已精确定义。\n3.  **客观性：** 该问题是客观陈述的，使用了精确的数学定义和数值。它没有歧义、主观性或个人观点。\n\n### 步骤3：结论与行动\n该问题是有效的。它是一个基于可靠科学原理、定义明确的计算任务。我现在将着手解决。\n\n### 算法设计\n解决方案需要实现一个计算过程，处理每个测试用例以确定其可识别性特征。对于每个案例，必须执行以下步骤序列：\n\n1.  **识别案例详情：** 提取模型类型（$\\mathcal{M}_{1}$ 或 $\\mathcal{M}_{2}$）、参数向量 $\\boldsymbol{\\theta}$、观测时间向量 $\\{t_i\\}_{i=1}^n$ 以及标准差向量 $\\{\\sigma_i\\}_{i=1}^n$。\n\n2.  **构建灵敏度矩阵 $\\mathbf{J}$：** 该矩阵的维度为 $n \\times p$，其中 $n$ 是时间点的数量，$p$ 是参数的数量（两个模型中均为 $p=2$）。$\\mathbf{J}$ 的每一列是模型输出相对于一个参数在所有时间点上评估的灵敏度向量。\n    - 对于模型 $\\mathcal{M}_1$ 及其参数 $\\boldsymbol{\\theta}=[a,k]^\\top$，使用提供的灵敏度方程计算各列：\n      - 第1列：$\\frac{\\partial y}{\\partial a}(t_i; a, k) = \\exp(-k\\,t_i)$，其中 $i=1, \\dots, n$。\n      - 第2列：$\\frac{\\partial y}{\\partial k}(t_i; a, k) = -a\\,t_i\\,\\exp(-k\\,t_i)$，其中 $i=1, \\dots, n$。\n    - 对于模型 $\\mathcal{M}_2$ 及其参数 $\\boldsymbol{\\theta}=[\\alpha,\\beta]^\\top$：\n      - 第1列：$\\frac{\\partial y}{\\partial \\alpha}(t_i; \\alpha, \\beta) = \\beta\\,\\exp(-t_i)$，其中 $i=1, \\dots, n$。\n      - 第2列：$\\frac{\\partial y}{\\partial \\beta}(t_i; \\alpha, \\beta) = \\alpha\\,\\exp(-t_i)$，其中 $i=1, \\dots, n$。\n\n3.  **构建权重矩阵 $\\mathbf{W}$：** 这是一个 $n \\times n$ 的对角矩阵。对角元素是方差的倒数，$W_{ii} = w_i = \\sigma_i^{-2}$。\n\n4.  **计算费雪信息矩阵 $\\mathbf{F}$：** 这涉及到根据公式 $\\mathbf{F} = \\mathbf{J}^{\\top}\\mathbf{W}\\mathbf{J}$ 进行矩阵乘法。得到的矩阵 $\\mathbf{F}$ 将是一个 $p \\times p$ 的对称半正定矩阵。对于本问题，$p=2$。\n\n5.  **特征值分解：** 计算矩阵 $\\mathbf{F}$ 的特征值。由于 $\\mathbf{F}$ 是实对称矩阵，其特征值为实数且非负。设计算出的特征值按非递减顺序排序为 $\\lambda_1, \\dots, \\lambda_p$。则 $\\lambda_{\\min} = \\lambda_1$ 且 $\\lambda_{\\max} = \\lambda_p$。\n\n6.  **计算可识别性度量：**\n    - **最小特征值：** $\\lambda_{\\min}$。\n    - **最大特征值：** $\\lambda_{\\max}$。\n    - **条件数：** 使用给定的常量 $\\delta = 10^{-18}$ 计算 $\\kappa(\\mathbf{F}) = \\frac{\\lambda_{\\max}}{\\max\\{\\lambda_{\\min}, \\delta\\}}$。\n    - **草率性分类：** 评估布尔表达式 $\\kappa(\\mathbf{F}) > \\tau$，其中 $\\tau = 10^{6}$。\n    - **数值秩：** 计算满足条件 $\\lambda_j \\ge \\epsilon \\cdot \\lambda_{\\max}$ 的特征值 $\\lambda_j$ 的数量，使用 $\\epsilon = 10^{-12}$。\n\n7.  **汇总结果：** 存储当前案例的五个计算值——$[\\lambda_{\\min}, \\lambda_{\\max}, \\kappa, \\text{是否草率}, \\mathrm{rank}_{\\epsilon}]$。对所有测试用例重复此过程，并编译最终的结果列表。最终输出必须格式化为列表的列表的单行字符串表示。\n我现在将用 Python 实现此过程。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the parameter identifiability problem for the given test cases.\n    \"\"\"\n    \n    # Global numerical constants\n    DELTA = 1e-18   # Safeguard for condition number denominator\n    EPSILON = 1e-12 # Tolerance for numerical rank\n    TAU = 1e6       # Threshold for sloppiness classification\n\n    def get_jacobian_m1(theta, times):\n        \"\"\"Computes the Jacobian for Model M1.\"\"\"\n        a, k = theta\n        n = len(times)\n        p = 2\n        J = np.zeros((n, p))\n        exp_term = np.exp(-k * times)\n        J[:, 0] = exp_term  # d(y)/d(a)\n        J[:, 1] = -a * times * exp_term  # d(y)/d(k)\n        return J\n\n    def get_jacobian_m2(theta, times):\n        \"\"\"Computes the Jacobian for Model M2.\"\"\"\n        alpha, beta = theta\n        n = len(times)\n        p = 2\n        J = np.zeros((n, p))\n        exp_term = np.exp(-times)\n        J[:, 0] = beta * exp_term  # d(y)/d(alpha)\n        J[:, 1] = alpha * exp_term  # d(y)/d(beta)\n        return J\n\n    # Test suite definition\n    test_cases = [\n        {\n            \"id\": 1,\n            \"model_func\": get_jacobian_m1,\n            \"theta\": np.array([1.0, 0.4]),\n            \"times\": np.array([0, 1, 2, 5, 10], dtype=float),\n            \"sigmas\": np.array([0.05, 0.05, 0.05, 0.05, 0.05], dtype=float),\n        },\n        {\n            \"id\": 2,\n            \"model_func\": get_jacobian_m1,\n            \"theta\": np.array([1.0, 0.5]),\n            \"times\": np.array([0, 0.02, 0.04, 0.06, 0.08], dtype=float),\n            \"sigmas\": np.array([0.05, 0.05, 0.05, 0.05, 0.05], dtype=float),\n        },\n        {\n            \"id\": 3,\n            \"model_func\": get_jacobian_m2,\n            \"theta\": np.array([2.0, 0.5]),\n            \"times\": np.array([0, 1, 2, 3], dtype=float),\n            \"sigmas\": np.array([0.1, 0.1, 0.1, 0.1, 0.1], dtype=float),\n        },\n        {\n            \"id\": 4,\n            \"model_func\": get_jacobian_m1,\n            \"theta\": np.array([1.0, 0.4]),\n            \"times\": np.array([0, 1, 2, 3, 4], dtype=float),\n            \"sigmas\": np.array([1e-6, 0.1, 0.1, 0.1, 0.1], dtype=float),\n        },\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        # Step 1: Construct Jacobian J\n        J = case[\"model_func\"](case[\"theta\"], case[\"times\"])\n        \n        # Step 2: Construct Weight Matrix W\n        weights = 1.0 / (case[\"sigmas\"] ** 2)\n        W = np.diag(weights)\n        \n        # Step 3: Compute Fisher Information Matrix F\n        F = J.T @ W @ J\n        \n        # Step 4: Eigenvalue decomposition\n        # eigvalsh returns eigenvalues in ascending order for symmetric matrices\n        eigenvalues = np.linalg.eigvalsh(F)\n        \n        lambda_min = eigenvalues[0]\n        lambda_max = eigenvalues[-1]\n        \n        # Step 5: Calculate identifiability metrics\n        \n        # Condition number\n        kappa = lambda_max / max(lambda_min, DELTA)\n        \n        # Sloppiness boolean\n        is_sloppy = kappa > TAU\n        \n        # Numerical rank\n        rank_threshold = EPSILON * lambda_max\n        # Note: for a singular matrix, lambda_min can be slightly negative due to numerics,\n        # but the threshold is non-negative, so the comparison is fine.\n        numerical_rank = np.sum(eigenvalues >= rank_threshold)\n        \n        # Assemble results for the case\n        case_results = [\n            lambda_min, \n            lambda_max, \n            kappa, \n            is_sloppy, \n            int(numerical_rank)\n        ]\n        all_results.append(case_results)\n\n    # Format the final output string\n    # str() on a list produces a string representation with brackets, e.g., '[...]'\n    # str() on a boolean produces 'True' or 'False'\n    result_strings = [str(res) for res in all_results]\n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "当参数高度相关时，许多优化算法的效率会变得低下，这表现为一个病态的优化问题。本练习探讨了一种称为“白化变换”（whitening transform）的强大参数重整技术，它通过重新缩放和旋转参数空间来消除这些相关性。你将从第一性原理出发推导这种变换，并分析它如何显著改善海森矩阵（Hessian matrix）的条件数，从而将一个困难的优化问题转化为一个更简单的问题，并加速校准算法的收敛。",
            "id": "3894221",
            "problem": "一个集总式降雨-径流模型的水文校准问题，旨在通过最小化一个确定性最小二乘目标函数来估计一个双参数向量 $\\boldsymbol{\\theta} \\in \\mathbb{R}^{2}$。在当前估计值 $\\boldsymbol{\\theta}_{\\ast}$ 附近，假设具有高斯观测误差的模型-数据失配可以进行线性化。在此情况下，目标函数关于参数的海森矩阵的高斯-牛顿近似是一个对称正定矩阵 $\\mathbf{H} \\in \\mathbb{R}^{2 \\times 2}$，而来自一组校准的经验参数协方差是一个对称正定矩阵 $\\mathbf{P} \\in \\mathbb{R}^{2 \\times 2}$。考虑以下具体的、经验上一致的矩阵：\n- $\\mathbf{H} = \\begin{pmatrix} 50  20 \\\\[4pt] 20  10 \\end{pmatrix}$，\n- $\\mathbf{P} = \\begin{pmatrix} 0.1  -0.2 \\\\[4pt] -0.2  0.5 \\end{pmatrix}$。\n仅使用基本定义，完成以下任务：\n- 构造一个对称白化变换 $\\mathbf{S} \\in \\mathbb{R}^{2 \\times 2}$，使得重参数化变量 $\\boldsymbol{z} = \\mathbf{S}\\left(\\boldsymbol{\\theta} - \\boldsymbol{\\mu}\\right)$ 具有单位协方差，其中 $\\boldsymbol{\\mu}$ 是 $\\boldsymbol{\\theta}$ 的集成均值。您的构造必须从协方差和随机向量线性变换的定义出发，并根据 $\\mathbf{P}$ 的谱分解来证明 $\\mathbf{S}$ 选择的合理性。\n- 使用优化中变量替换的链式法则，推导关于 $\\boldsymbol{z}$ 的变换后的高斯-牛顿海森矩阵 $\\mathbf{H}_{z}$，并用 $\\mathbf{H}$ 和 $\\mathbf{P}$ 表示。\n- 将对称正定矩阵 $\\mathbf{A}$ 的条件数定义为 $\\kappa(\\mathbf{A}) = \\lambda_{\\max}(\\mathbf{A}) / \\lambda_{\\min}(\\mathbf{A})$，其中 $\\lambda_{\\max}$ 和 $\\lambda_{\\min}$ 分别是其最大和最小特征值。计算因白化带来的条件数改善因子 $r = \\kappa(\\mathbf{H}) / \\kappa(\\mathbf{H}_{z})$ 的精确封闭形式表达式。\n\n以精确解析表达式的形式给出 $r$ 的最终结果。不要进行近似或四舍五入。最终答案是无量纲的，不需要单位。",
            "solution": "我们从协方差的定义及其在线性映射下的变换开始。如果随机向量 $\\boldsymbol{\\theta}$ 的协方差为 $\\mathbf{P}$，那么对于任意确定性矩阵 $\\mathbf{S}$，变换后的随机向量 $\\boldsymbol{z} = \\mathbf{S}\\left(\\boldsymbol{\\theta} - \\boldsymbol{\\mu}\\right)$ 的协方差为\n$$\n\\operatorname{Cov}(\\boldsymbol{z}) = \\mathbf{S}\\,\\operatorname{Cov}(\\boldsymbol{\\theta})\\,\\mathbf{S}^{\\top} = \\mathbf{S}\\,\\mathbf{P}\\,\\mathbf{S}^{\\top}.\n$$\n为使协方差为单位矩阵，我们需要\n$$\n\\mathbf{S}\\,\\mathbf{P}\\,\\mathbf{S}^{\\top} = \\mathbf{I}.\n$$\n因为 $\\mathbf{P}$ 是对称正定矩阵，所以它可以进行谱分解\n$$\n\\mathbf{P} = \\mathbf{Q}\\,\\mathbf{D}\\,\\mathbf{Q}^{\\top},\n$$\n其中 $\\mathbf{Q}$ 是正交矩阵，$\\mathbf{D} = \\operatorname{diag}(d_{1}, d_{2})$ 且 $d_{i}  0$。定义对称矩阵\n$$\n\\mathbf{P}^{-1/2} = \\mathbf{Q}\\,\\mathbf{D}^{-1/2}\\,\\mathbf{Q}^{\\top}, \\quad \\text{其中} \\quad \\mathbf{D}^{-1/2} = \\operatorname{diag}\\!\\left(d_{1}^{-1/2}, d_{2}^{-1/2}\\right).\n$$\n则\n$$\n\\mathbf{P}^{-1/2}\\,\\mathbf{P}\\,\\mathbf{P}^{-1/2} = \\mathbf{Q}\\,\\mathbf{D}^{-1/2}\\,\\mathbf{Q}^{\\top}\\,\\mathbf{Q}\\,\\mathbf{D}\\,\\mathbf{Q}^{\\top}\\,\\mathbf{Q}\\,\\mathbf{D}^{-1/2}\\,\\mathbf{Q}^{\\top} = \\mathbf{Q}\\,\\mathbf{D}^{-1/2}\\,\\mathbf{D}\\,\\mathbf{D}^{-1/2}\\,\\mathbf{Q}^{\\top} = \\mathbf{I}.\n$$\n因此，一个有效的对称白化变换是\n$$\n\\mathbf{S} = \\mathbf{P}^{-1/2}.\n$$\n\n接下来，我们分析这种重参数化对高斯-牛顿海森矩阵的影响。设原始参数写作\n$$\n\\boldsymbol{\\theta} = \\boldsymbol{\\mu} + \\mathbf{P}^{1/2}\\,\\boldsymbol{z}, \\quad \\text{等价地} \\quad \\boldsymbol{z} = \\mathbf{P}^{-1/2}\\,(\\boldsymbol{\\theta} - \\boldsymbol{\\mu}).\n$$\n设 $J(\\boldsymbol{\\theta})$ 表示最小二乘目标函数。根据链式法则，梯度变换如下\n$$\n\\nabla_{\\boldsymbol{z}} J = \\left(\\frac{\\partial \\boldsymbol{\\theta}}{\\partial \\boldsymbol{z}}\\right)^{\\top} \\nabla_{\\boldsymbol{\\theta}} J = \\left(\\mathbf{P}^{1/2}\\right)^{\\top} \\nabla_{\\boldsymbol{\\theta}} J.\n$$\n假设采用高斯-牛顿近似，关于 $\\boldsymbol{\\theta}$ 的海森矩阵是 $\\mathbf{H}$，我们将其在 $\\boldsymbol{\\theta}_{\\ast}$ 附近局部地视为常数。那么，关于 $\\boldsymbol{z}$ 的海森矩阵变为\n$$\n\\mathbf{H}_{z} = \\left(\\mathbf{P}^{1/2}\\right)^{\\top} \\mathbf{H}\\, \\mathbf{P}^{1/2} = \\mathbf{P}^{1/2}\\, \\mathbf{H}\\, \\mathbf{P}^{1/2},\n$$\n因为 $\\mathbf{P}^{1/2}$ 是对称的。\n\n对于给定的矩阵，\n$$\n\\mathbf{H} = \\begin{pmatrix} 50  20 \\\\[4pt] 20  10 \\end{pmatrix}, \\qquad \\mathbf{P} = \\begin{pmatrix} 0.1  -0.2 \\\\[4pt] -0.2  0.5 \\end{pmatrix}.\n$$\n我们首先验证 $\\mathbf{P} = \\mathbf{H}^{-1}$。$\\mathbf{H}$ 的行列式是\n$$\n\\det(\\mathbf{H}) = 50 \\cdot 10 - 20 \\cdot 20 = 500 - 400 = 100,\n$$\n其逆矩阵是\n$$\n\\mathbf{H}^{-1} = \\frac{1}{\\det(\\mathbf{H})} \\begin{pmatrix} 10  -20 \\\\ -20  50 \\end{pmatrix} = \\begin{pmatrix} 0.1  -0.2 \\\\ -0.2  0.5 \\end{pmatrix} = \\mathbf{P}.\n$$\n因此 $\\mathbf{P} = \\mathbf{H}^{-1}$，这与线性高斯情况下的结论一致，即经验参数协方差等于高斯-牛顿海森矩阵的逆。将 $\\mathbf{P} = \\mathbf{H}^{-1}$ 代入变换后的海森矩阵，\n$$\n\\mathbf{H}_{z} = \\mathbf{P}^{1/2}\\, \\mathbf{H}\\, \\mathbf{P}^{1/2} = \\left(\\mathbf{H}^{-1}\\right)^{1/2}\\, \\mathbf{H}\\, \\left(\\mathbf{H}^{-1}\\right)^{1/2} = \\mathbf{I},\n$$\n因此白化在 $\\boldsymbol{z}$ 坐标下产生了一个单位海森矩阵。\n\n现在我们计算条件数。对于 $\\mathbf{H}$，由于它是对称正定矩阵，其特征值是以下方程的根\n$$\n\\det\\!\\left(\\mathbf{H} - \\lambda \\mathbf{I}\\right) = 0 \\;\\; \\Longleftrightarrow \\;\\; \\left(50 - \\lambda\\right)\\left(10 - \\lambda\\right) - 20^{2} = 0.\n$$\n展开得到\n$$\n\\lambda^{2} - 60 \\lambda + 100 = 0,\n$$\n其解为\n$$\n\\lambda_{\\pm} = \\frac{60 \\pm \\sqrt{60^{2} - 4 \\cdot 100}}{2} = \\frac{60 \\pm \\sqrt{3600 - 400}}{2} = \\frac{60 \\pm \\sqrt{3200}}{2} = 30 \\pm 20 \\sqrt{2}.\n$$\n因此\n$$\n\\kappa(\\mathbf{H}) = \\frac{\\lambda_{\\max}(\\mathbf{H})}{\\lambda_{\\min}(\\mathbf{H})} = \\frac{30 + 20 \\sqrt{2}}{30 - 20 \\sqrt{2}}.\n$$\n这可以通过分母有理化来简化：\n$$\n\\kappa(\\mathbf{H}) = \\frac{30 + 20 \\sqrt{2}}{30 - 20 \\sqrt{2}} \\cdot \\frac{30 + 20 \\sqrt{2}}{30 + 20 \\sqrt{2}} = \\frac{(30 + 20 \\sqrt{2})^{2}}{30^{2} - (20 \\sqrt{2})^{2}} = \\frac{900 + 1200 \\sqrt{2} + 800}{900 - 800} = \\frac{1700 + 1200 \\sqrt{2}}{100} = 17 + 12 \\sqrt{2}.\n$$\n对于白化后的海森矩阵 $\\mathbf{H}_{z} = \\mathbf{I}$，其特征值均为 $1$，因此\n$$\n\\kappa(\\mathbf{H}_{z}) = \\frac{1}{1} = 1.\n$$\n因此，因白化带来的条件数改善因子为\n$$\nr = \\frac{\\kappa(\\mathbf{H})}{\\kappa(\\mathbf{H}_{z})} = \\frac{17 + 12 \\sqrt{2}}{1} = 17 + 12 \\sqrt{2}.\n$$\n这是一个封闭形式的精确表达式，无需四舍五入。",
            "answer": "$$\\boxed{17 + 12\\sqrt{2}}$$"
        },
        {
            "introduction": "理论算法最终必须在代码中实现，而在代码中它们会遇到浮点运算的实际限制。本练习将演示不当缩放的参数，尤其是在环境模型中常见的指数函数内，如何导致数值上溢或下溢，从而使梯度计算失败。你将诊断这些不稳定性，并通过在对数空间中重新构建问题来实施一个稳健的重缩放补救措施，这项技术即使在极端参数值下也能确保梯度评估的稳定性和可靠性。",
            "id": "3894248",
            "problem": "考虑一个常用于环境与地球系统建模中的简单指数敏感性模型的校准，该模型用于表示一个具有乘性强迫的过程。设外源强迫为序列 $\\{x_t\\}_{t=1}^T$，测量量为 $\\{d_t\\}_{t=1}^T$，模型预测为 $y_t(\\alpha,\\beta) = \\alpha \\exp(\\beta x_t)$，其中 $\\alpha > 0$ 和 $\\beta \\in \\mathbb{R}$ 是待校准的参数。\n\n假设测量误差为独立的、均值为零、方差为 $\\{\\sigma_t^2\\}_{t=1}^T$ 的高斯误差。相应的最大似然估计 (MLE) 准则是加权最小二乘目标\n$$\nJ(\\alpha,\\beta) = \\frac{1}{2} \\sum_{t=1}^T w_t \\big(y_t(\\alpha,\\beta) - d_t\\big)^2,\n$$\n其中 $w_t = \\frac{1}{\\sigma_t^2}$。\n\n任务 A (从第一性原理推导)：仅从上述定义出发，使用链式法则，推导梯度分量 $\\frac{\\partial J}{\\partial \\alpha}$ 和 $\\frac{\\partial J}{\\partial \\beta}$，将其表示为 $\\alpha$、$\\beta$、$\\{x_t\\}$、$\\{d_t\\}$ 和 $\\{w_t\\}$ 的函数。\n\n任务 B (数值不稳定性分析)：尺度不当的参数可能导致 $\\nabla J(\\alpha,\\beta)$ 的计算在数值上不稳定。根据电气和电子工程师协会 754 (IEEE 754) 双精度标准中的浮点运算法则，当 $\\exp(\\cdot)$ 的求值参数大于 $L_{\\max} = \\log(\\text{max float})$ 时会发生上溢，当参数小于 $L_{\\min} = \\log(\\text{tiny})$ 时会发生下溢。从你的编程环境的浮点数信息中计算 $L_{\\max}$ 和 $L_{\\min}$，并通过检查以下条件来检测梯度计算中的上溢或下溢：\n- 是否存在任何 $\\beta x_t > L_{\\max}$ (上溢风险) 或 $\\beta x_t  L_{\\min}$ (下溢风险)，\n- 是否存在任何对 $\\exp(\\beta x_t)$ 的调用返回非有限值或零。\n\n对每个测试用例报告一个布尔值，指示朴素梯度计算是否既是有限的又没有发生上溢/下溢。此条件定义为两个梯度分量均为有限值，并且不存在上述风险的合取。\n\n任务 C (重标度修正与稳定梯度)：一个稳健的重标度修正是拟合对数残差，定义为\n$$\nJ_{\\log}(\\alpha,\\beta) = \\frac{1}{2} \\sum_{t=1}^T w_t \\Big(\\log y_t(\\alpha,\\beta) - \\log d_t\\Big)^2,\n$$\n当 $d_t > 0$ 时，该式是无量纲且尺度良好的。从基本定义明确推导 $\\frac{\\partial J_{\\log}}{\\partial \\alpha}$ 和 $\\frac{\\partial J_{\\log}}{\\partial \\beta}$，并实现其计算。这种修正方法避免了在梯度中计算 $\\exp(\\cdot)$。本问题中的所有量都是无单位的；以无量纲的浮点数或布尔值形式生成答案。\n\n测试套件数据：\n- 强迫序列：$\\{x_t\\} = [-20.0,\\,-10.0,\\,0.0,\\,10.0,\\,20.0]$。\n- 观测值：$\\{d_t\\} = [0.1,\\,1.0,\\,10.0,\\,100.0,\\,1000.0]$ (严格为正，适用于对数运算)。\n- 权重：$\\{w_t\\} = [1.0,\\,1.0,\\,1.0,\\,1.0,\\,1.0]$ (等价地，对所有 $t$，$\\sigma_t^2 = 1.0$)。\n\n参数测试用例：\n- 用例 1 (理想情况)：$(\\alpha,\\beta) = (2.0,\\,0.01)$。\n- 用例 2 (上溢)：$(\\alpha,\\beta) = (1.0,\\,100.0)$。\n- 用例 3 (下溢)：$(\\alpha,\\beta) = (10^{-300},\\,-100.0)$。\n- 用例 4 (混合极端情况)：$(\\alpha,\\beta) = (1.0,\\,50.0)$。\n\n你的程序必须：\n1. 实现任务 A 中的朴素梯度 $\\nabla J(\\alpha,\\beta)$，并为每个测试用例计算其值。\n2. 按照任务 B 中的规定，使用 $L_{\\max}$ 和 $L_{\\min}$ 检测上溢/下溢风险，并将这些检查与梯度分量的有限性相结合，为每个测试用例生成一个布尔值，以指示朴素梯度是否有限且稳定。\n3. 通过为每个测试用例计算任务 C 中的稳定梯度 $\\nabla J_{\\log}(\\alpha,\\beta)$ 来实现重标度修正方法。\n\n最终输出格式：\n- 生成单行输出，包含一个顶层列表，其中每个测试用例对应一个子列表。\n- 每个子列表的形式必须为 $[\\text{boolean},\\,g_{\\log,\\alpha},\\,g_{\\log,\\beta}]$，其中布尔值如上文所定义，$g_{\\log,\\alpha}$ 和 $g_{\\log,\\beta}$ 分别是 $\\frac{\\partial J_{\\log}}{\\partial \\alpha}$ 和 $\\frac{\\partial J_{\\log}}{\\partial \\beta}$ 的浮点数值。\n- 例如，输出必须在单行上呈现为 $[[\\text{bool}_1,\\,g_{1,\\log,\\alpha},\\,g_{1,\\log,\\beta}],\\,[\\text{bool}_2,\\,g_{2,\\log,\\alpha},\\,g_{2,\\log,\\beta}],\\,[\\text{bool}_3,\\,g_{3,\\log,\\alpha},\\,g_{3,\\log,\\beta}],\\,[\\text{bool}_4,\\,g_{4,\\log,\\alpha},\\,g_{4,\\log,\\beta}]]$ 的形式。\n\n所有计算和输出都是无单位的。不涉及角度，也不允许使用百分号；如果需要，请将任何比例表示为小数或分数。",
            "solution": "该问题在科学和数学上是适定的，为科学建模中的数值优化问题提供了一套自洽且一致的定义、数据和任务。所有指定操作所需的条件均已满足。因此，我们着手解决，解决方案包括三个主要部分：推导两种不同的梯度公式，并分析其数值稳定性。\n\n### 任务 A：朴素梯度 $\\nabla J(\\alpha,\\beta)$ 的推导\n\n主要目标函数是加权残差平方和，由下式给出：\n$$\nJ(\\alpha,\\beta) = \\frac{1}{2} \\sum_{t=1}^T w_t \\big(y_t(\\alpha,\\beta) - d_t\\big)^2\n$$\n其中模型预测为 $y_t(\\alpha,\\beta) = \\alpha \\exp(\\beta x_t)$。权重为 $w_t = 1/\\sigma_t^2$。\n\n为了求得梯度 $\\nabla J = \\left[ \\frac{\\partial J}{\\partial \\alpha}, \\frac{\\partial J}{\\partial \\beta} \\right]^T$，我们计算关于 $\\alpha$ 和 $\\beta$ 的偏导数。\n\n1.  **关于 $\\alpha$ 的偏导数**：\n    我们应用链式法则。外部函数 $(\\cdot)^2$ 的导数得到 $2(y_t - d_t)$，内部函数 $y_t - d_t$ 关于 $\\alpha$ 的导数是 $\\frac{\\partial y_t}{\\partial \\alpha}$。\n    $$\n    \\frac{\\partial J}{\\partial \\alpha} = \\frac{1}{2} \\sum_{t=1}^T w_t \\cdot 2 \\big(y_t(\\alpha,\\beta) - d_t\\big) \\cdot \\frac{\\partial y_t(\\alpha,\\beta)}{\\partial \\alpha}\n    $$\n    模型函数 $y_t$ 关于 $\\alpha$ 的偏导数是：\n    $$\n    \\frac{\\partial y_t}{\\partial \\alpha} = \\frac{\\partial}{\\partial \\alpha} \\big(\\alpha \\exp(\\beta x_t)\\big) = \\exp(\\beta x_t)\n    $$\n    将此代回，我们得到梯度的第一个分量：\n    $$\n    \\frac{\\partial J}{\\partial \\alpha} = \\sum_{t=1}^T w_t \\big(\\alpha \\exp(\\beta x_t) - d_t\\big) \\exp(\\beta x_t)\n    $$\n\n2.  **关于 $\\beta$ 的偏导数**：\n    类似地，我们应用关于 $\\beta$ 的链式法则。\n    $$\n    \\frac{\\partial J}{\\partial \\beta} = \\frac{1}{2} \\sum_{t=1}^T w_t \\cdot 2 \\big(y_t(\\alpha,\\beta) - d_t\\big) \\cdot \\frac{\\partial y_t(\\alpha,\\beta)}{\\partial \\beta}\n    $$\n    模型函数 $y_t$ 关于 $\\beta$ 的偏导数是：\n    $$\n    \\frac{\\partial y_t}{\\partial \\beta} = \\frac{\\partial}{\\partial \\beta} \\big(\\alpha \\exp(\\beta x_t)\\big) = \\alpha \\exp(\\beta x_t) \\cdot x_t = y_t(\\alpha,\\beta) x_t\n    $$\n    将此代回，我们得到梯度的第二个分量：\n    $$\n    \\frac{\\partial J}{\\partial \\beta} = \\sum_{t=1}^T w_t \\big(\\alpha \\exp(\\beta x_t) - d_t\\big) \\alpha \\exp(\\beta x_t) x_t\n    $$\n\n这两个表达式构成了朴素梯度 $\\nabla J(\\alpha,\\beta)$。它们的计算需要求解 $\\exp(\\beta x_t)$，这可能是数值不稳定性的来源。\n\n### 任务 B：数值不稳定性分析\n\n朴素梯度 $\\nabla J(\\alpha,\\beta)$ 的计算容易受到数值上溢和下溢的影响，特别是来自 $\\exp(\\beta x_t)$ 项。在 IEEE 754 双精度浮点运算中，浮点数具有有限的范围。\n-   **上溢**：当计算产生的结果大于可表示的最大有限数（表示为 `max float`）时发生。对于指数函数，如果其参数 $\\beta x_t$ 太大，就会发生这种情况。因此，$\\exp(\\beta x_t)$ 变为 `inf`。\n-   **下溢**：当结果的绝对值小于最小的正规格化数时发生，通常会解析为零。对于指数函数，如果其参数 $\\beta x_t$ 是非常大的负数，就会发生这种情况。因此，$\\exp(\\beta x_t)$ 变为 $0$。\n\n为了形式化地检测这些问题的风险，我们根据浮点表示的限制定义了两个阈值：\n-   $L_{\\max} = \\log(\\text{max float})$，即不会导致上溢的 $\\exp(\\cdot)$ 函数的最大参数。\n-   $L_{\\min} = \\log(\\text{tiny})$，其中 `tiny` 是最小的正（非规格化）浮点数。低于此阈值的参数几乎肯定会下溢到 $0$。\n\n对于给定的参数集 $(\\alpha, \\beta)$ 和数据 $\\{x_t\\}$，稳定性检查涉及几个条件：\n1. 预计算检查：验证对所有 $t$，参数 $\\beta x_t$ 都在安全范围内，即 $L_{\\min}  \\beta x_t  L_{\\max}$。\n2. 后计算检查：验证 $\\exp(\\beta x_t)$ 的所有计算值都是有限且非零的。\n3. 最终梯度检查：验证 $\\frac{\\partial J}{\\partial \\alpha}$ 和 $\\frac{\\partial J}{\\partial \\beta}$ 的最终计算值为有限数（不是 `inf`、`-inf` 或 `NaN`）。\n\n将报告一个布尔标志，当且仅当最终的梯度分量是有限的，并且所有上述风险（参数或结果级别的上溢/下溢）都不存在时，该标志为 `True`。\n\n### 任务 C：稳定梯度 $\\nabla J_{\\log}(\\alpha,\\beta)$ 的推导\n\n为了规避与指数函数相关的数值问题，可以通过最小化对数空间中的误差平方和来重构该问题。这通常等同于假设测量误差服从对数正态分布。重标度的目标函数是：\n$$\nJ_{\\log}(\\alpha,\\beta) = \\frac{1}{2} \\sum_{t=1}^T w_t \\Big(\\log y_t(\\alpha,\\beta) - \\log d_t\\Big)^2\n$$\n这要求对所有 $t$ 都有 $d_t > 0$，所提供的数据满足此条件。一个关键的简化来自于模型函数的对数：\n$$\n\\log y_t(\\alpha,\\beta) = \\log\\big(\\alpha \\exp(\\beta x_t)\\big) = \\log \\alpha + \\log(\\exp(\\beta x_t)) = \\log \\alpha + \\beta x_t\n$$\n将此代入 $J_{\\log}$，我们得到：\n$$\nJ_{\\log}(\\alpha,\\beta) = \\frac{1}{2} \\sum_{t=1}^T w_t \\big(\\log \\alpha + \\beta x_t - \\log d_t\\big)^2\n$$\n这是一个关于参数 $\\theta_1 = \\log \\alpha$ 和 $\\theta_2 = \\beta$ 的线性最小二乘问题。其梯度可以在不计算任何指数函数的情况下求得，使其在数值上是稳健的。\n\n1.  **关于 $\\alpha$ 的偏导数**：\n    我们使用链式法则，对 $\\alpha$ 求导。内部函数是 $\\log \\alpha$。\n    $$\n    \\frac{\\partial J_{\\log}}{\\partial \\alpha} = \\frac{1}{2} \\sum_{t=1}^T w_t \\cdot 2 \\big(\\log \\alpha + \\beta x_t - \\log d_t\\big) \\cdot \\frac{\\partial}{\\partial \\alpha}(\\log \\alpha + \\beta x_t - \\log d_t)\n    $$\n    括号内项关于 $\\alpha$ 的导数是 $\\frac{1}{\\alpha}$。\n    $$\n    \\frac{\\partial J_{\\log}}{\\partial \\alpha} = \\sum_{t=1}^T w_t \\big(\\log \\alpha + \\beta x_t - \\log d_t\\big) \\cdot \\frac{1}{\\alpha} = \\frac{1}{\\alpha} \\sum_{t=1}^T w_t (\\log \\alpha + \\beta x_t - \\log d_t)\n    $$\n\n2.  **关于 $\\beta$ 的偏导数**：\n    我们对 $\\beta$ 求导。\n    $$\n    \\frac{\\partial J_{\\log}}{\\partial \\beta} = \\frac{1}{2} \\sum_{t=1}^T w_t \\cdot 2 \\big(\\log \\alpha + \\beta x_t - \\log d_t\\big) \\cdot \\frac{\\partial}{\\partial \\beta}(\\log \\alpha + \\beta x_t - \\log d_t)\n    $$\n    括号内项关于 $\\beta$ 的导数是 $x_t$。\n    $$\n    \\frac{\\partial J_{\\log}}{\\partial \\beta} = \\sum_{t=1}^T w_t \\big(\\log \\alpha + \\beta x_t - \\log d_t\\big) x_t\n    $$\n\n$\\nabla J_{\\log}(\\alpha,\\beta)$ 的这些表达式对于困扰朴素公式的上溢和下溢问题是稳健的，因为它们只涉及对数和基本算术运算。参数 $\\alpha$ 必须为正，这是已知的。实现将为每个提供的测试用例计算这些推导出的量。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes gradients for an exponential sensitivity model, checks for numerical\n    instability in the naive formulation, and computes a stable gradient using\n    a log-transformed objective function.\n    \"\"\"\n    # Define problem data\n    x_t = np.array([-20.0, -10.0, 0.0, 10.0, 20.0])\n    d_t = np.array([0.1, 1.0, 10.0, 100.0, 1000.0])\n    w_t = np.array([1.0, 1.0, 1.0, 1.0, 1.0])\n\n    # Define parameter test cases\n    test_cases = [\n        (2.0, 0.01),      # Case 1 (happy path)\n        (1.0, 100.0),     # Case 2 (overflow)\n        (10**-300, -100.0), # Case 3 (underflow)\n        (1.0, 50.0)       # Case 4 (mixed extremes)\n    ]\n\n    # Get IEEE 754 double precision limits for exp() arguments\n    finfo = np.finfo(float)\n    L_max = np.log(finfo.max)\n    L_min = np.log(finfo.tiny)\n\n    results = []\n\n    for alpha, beta in test_cases:\n        # --- Task A  B: Naive gradient and stability check ---\n\n        # Calculate arguments for exp()\n        beta_x = beta * x_t\n\n        # Define risks as per problem statement\n        risk_arg_overflow = np.any(beta_x > L_max)\n        risk_arg_underflow = np.any(beta_x  L_min)\n\n        # Evaluate exp() while suppressing runtime warnings for over/underflow\n        with np.errstate(over='ignore', under='ignore'):\n            exp_beta_x = np.exp(beta_x)\n\n        risk_exp_nonfinite = not np.all(np.isfinite(exp_beta_x))\n        risk_exp_zero = np.any(exp_beta_x == 0.0)\n\n        absence_of_risks = not (risk_arg_overflow or risk_arg_underflow or risk_exp_nonfinite or risk_exp_zero)\n\n        # Calculate naive gradient components, suppressing all calculation warnings\n        with np.errstate(all='ignore'):\n            y_t = alpha * exp_beta_x\n            residuals = y_t - d_t\n            grad_J_alpha = np.sum(w_t * residuals * exp_beta_x)\n            grad_J_beta = np.sum(w_t * residuals * y_t * x_t)\n\n        # Check for finiteness of the final gradient components\n        finiteness = np.isfinite(grad_J_alpha) and np.isfinite(grad_J_beta)\n\n        # The final stability boolean is the conjunction of finiteness and absence of risks\n        is_stable = finiteness and absence_of_risks\n\n        # --- Task C: Stable gradient calculation ---\n\n        log_alpha = np.log(alpha)\n        log_d_t = np.log(d_t)\n        \n        log_residuals = log_alpha + beta * x_t - log_d_t\n        \n        grad_J_log_alpha = (1.0 / alpha) * np.sum(w_t * log_residuals)\n        grad_J_log_beta = np.sum(w_t * log_residuals * x_t)\n\n        results.append([is_stable, grad_J_log_alpha, grad_J_log_beta])\n\n    # Final print statement in the exact required format.\n    # The format is a string representation of a list of lists.\n    # The provided print statement template correctly constructs this.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}