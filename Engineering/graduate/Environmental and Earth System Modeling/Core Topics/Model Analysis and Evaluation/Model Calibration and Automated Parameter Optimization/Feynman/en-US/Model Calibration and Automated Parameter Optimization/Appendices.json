{
    "hands_on_practices": [
        {
            "introduction": "The efficiency of many optimization algorithms is highly dependent on the geometry of the cost function, which is characterized by the Hessian matrix. This exercise demonstrates a powerful preconditioning technique called whitening . By transforming parameters based on their covariance structure, you will see how an ill-conditioned problem can be converted into an ideally conditioned one, providing insight into methods that accelerate convergence.",
            "id": "3894221",
            "problem": "A hydrologic calibration problem for a lumped rainfall–runoff model seeks to estimate a two-parameter vector $\\boldsymbol{\\theta} \\in \\mathbb{R}^{2}$ by minimizing a deterministic least-squares objective. Near the current estimate $\\boldsymbol{\\theta}_{\\ast}$, assume a linearization of the model–data misfit with Gaussian observational errors. In this regime, the Gauss–Newton approximation to the Hessian of the objective with respect to parameters is a symmetric positive definite matrix $\\mathbf{H} \\in \\mathbb{R}^{2 \\times 2}$, and an empirical parameter covariance from an ensemble of calibrations is the symmetric positive definite matrix $\\mathbf{P} \\in \\mathbb{R}^{2 \\times 2}$. Consider the following specific, empirically consistent matrices:\n- $\\mathbf{H} = \\begin{pmatrix} 50  20 \\\\ 20  10 \\end{pmatrix}$,\n- $\\mathbf{P} = \\begin{pmatrix} 0.1  -0.2 \\\\ -0.2  0.5 \\end{pmatrix}$.\nUsing only fundamental definitions, do the following:\n- Construct a symmetric whitening transform $\\mathbf{S} \\in \\mathbb{R}^{2 \\times 2}$ such that the reparameterized variable $\\boldsymbol{z} = \\mathbf{S}\\left(\\boldsymbol{\\theta} - \\boldsymbol{\\mu}\\right)$ has unit covariance, where $\\boldsymbol{\\mu}$ is the ensemble mean of $\\boldsymbol{\\theta}$. Your construction must start from the definition of covariance and linear transformations of random vectors and justify the choice of $\\mathbf{S}$ in terms of the spectral decomposition of $\\mathbf{P}$.\n- Using the chain rule for variable changes in optimization, derive the transformed Gauss–Newton Hessian $\\mathbf{H}_{z}$ with respect to $\\boldsymbol{z}$ in terms of $\\mathbf{H}$ and $\\mathbf{P}$.\n- Define the condition number for a symmetric positive definite matrix $\\mathbf{A}$ as $\\kappa(\\mathbf{A}) = \\lambda_{\\max}(\\mathbf{A}) / \\lambda_{\\min}(\\mathbf{A})$, where $\\lambda_{\\max}$ and $\\lambda_{\\min}$ are its largest and smallest eigenvalues, respectively. Compute the exact, closed-form expression for the improvement factor in conditioning due to whitening, defined as $r = \\kappa(\\mathbf{H}) / \\kappa(\\mathbf{H}_{z})$.\n\nProvide your final result for $r$ as an exact analytic expression. Do not approximate or round. The final answer is dimensionless; no units are required.",
            "solution": "We begin from the definition of covariance and its transformation under linear mappings. If a random vector $\\boldsymbol{\\theta}$ has covariance $\\mathbf{P}$, then for any deterministic matrix $\\mathbf{S}$, the transformed random vector $\\boldsymbol{z} = \\mathbf{S}\\left(\\boldsymbol{\\theta} - \\boldsymbol{\\mu}\\right)$ has covariance\n$$\n\\operatorname{Cov}(\\boldsymbol{z}) = \\mathbf{S}\\,\\operatorname{Cov}(\\boldsymbol{\\theta})\\,\\mathbf{S}^{\\top} = \\mathbf{S}\\,\\mathbf{P}\\,\\mathbf{S}^{\\top}.\n$$\nTo enforce unit covariance, we require\n$$\n\\mathbf{S}\\,\\mathbf{P}\\,\\mathbf{S}^{\\top} = \\mathbf{I}.\n$$\nBecause $\\mathbf{P}$ is symmetric positive definite, it admits the spectral decomposition\n$$\n\\mathbf{P} = \\mathbf{Q}\\,\\mathbf{D}\\,\\mathbf{Q}^{\\top},\n$$\nwhere $\\mathbf{Q}$ is orthogonal and $\\mathbf{D} = \\operatorname{diag}(d_{1}, d_{2})$ with $d_{i}  0$. Define the symmetric matrix\n$$\n\\mathbf{P}^{-1/2} = \\mathbf{Q}\\,\\mathbf{D}^{-1/2}\\,\\mathbf{Q}^{\\top}, \\quad \\text{where} \\quad \\mathbf{D}^{-1/2} = \\operatorname{diag}\\!\\left(d_{1}^{-1/2}, d_{2}^{-1/2}\\right).\n$$\nThen\n$$\n\\mathbf{P}^{-1/2}\\,\\mathbf{P}\\,\\mathbf{P}^{-1/2} = \\mathbf{Q}\\,\\mathbf{D}^{-1/2}\\,\\mathbf{Q}^{\\top}\\,\\mathbf{Q}\\,\\mathbf{D}\\,\\mathbf{Q}^{\\top}\\,\\mathbf{Q}\\,\\mathbf{D}^{-1/2}\\,\\mathbf{Q}^{\\top} = \\mathbf{Q}\\,\\mathbf{D}^{-1/2}\\,\\mathbf{D}\\,\\mathbf{D}^{-1/2}\\,\\mathbf{Q}^{\\top} = \\mathbf{I}.\n$$\nThus a valid symmetric whitening transform is\n$$\n\\mathbf{S} = \\mathbf{P}^{-1/2}.\n$$\n\nNext, we analyze the effect of this reparameterization on the Gauss–Newton Hessian. Let the original parameter be written as\n$$\n\\boldsymbol{\\theta} = \\boldsymbol{\\mu} + \\mathbf{P}^{1/2}\\,\\boldsymbol{z}, \\quad \\text{equivalently} \\quad \\boldsymbol{z} = \\mathbf{P}^{-1/2}\\,(\\boldsymbol{\\theta} - \\boldsymbol{\\mu}).\n$$\nLet $J(\\boldsymbol{\\theta})$ denote the least-squares objective. The gradient transforms by the chain rule as\n$$\n\\nabla_{\\boldsymbol{z}} J = \\left(\\frac{\\partial \\boldsymbol{\\theta}}{\\partial \\boldsymbol{z}}\\right)^{\\top} \\nabla_{\\boldsymbol{\\theta}} J = \\left(\\mathbf{P}^{1/2}\\right)^{\\top} \\nabla_{\\boldsymbol{\\theta}} J.\n$$\nAssuming the Gauss–Newton approximation, the Hessian with respect to $\\boldsymbol{\\theta}$ is $\\mathbf{H}$, which we treat locally as constant near $\\boldsymbol{\\theta}_{\\ast}$. Then the Hessian with respect to $\\boldsymbol{z}$ becomes\n$$\n\\mathbf{H}_{z} = \\left(\\mathbf{P}^{1/2}\\right)^{\\top} \\mathbf{H}\\, \\mathbf{P}^{1/2} = \\mathbf{P}^{1/2}\\, \\mathbf{H}\\, \\mathbf{P}^{1/2},\n$$\nsince $\\mathbf{P}^{1/2}$ is symmetric.\n\nFor the given matrices,\n$$\n\\mathbf{H} = \\begin{pmatrix} 50  20 \\\\ 20  10 \\end{pmatrix}, \\qquad \\mathbf{P} = \\begin{pmatrix} 0.1  -0.2 \\\\ -0.2  0.5 \\end{pmatrix}.\n$$\nWe first verify that $\\mathbf{P} = \\mathbf{H}^{-1}$. The determinant of $\\mathbf{H}$ is\n$$\n\\det(\\mathbf{H}) = 50 \\cdot 10 - 20 \\cdot 20 = 500 - 400 = 100,\n$$\nand the inverse is\n$$\n\\mathbf{H}^{-1} = \\frac{1}{\\det(\\mathbf{H})} \\begin{pmatrix} 10  -20 \\\\ -20  50 \\end{pmatrix} = \\begin{pmatrix} 0.1  -0.2 \\\\ -0.2  0.5 \\end{pmatrix} = \\mathbf{P}.\n$$\nTherefore $\\mathbf{P} = \\mathbf{H}^{-1}$, which is consistent with the linear-Gaussian case where the empirical parameter covariance equals the inverse Gauss–Newton Hessian. Substituting $\\mathbf{P} = \\mathbf{H}^{-1}$ into the transformed Hessian,\n$$\n\\mathbf{H}_{z} = \\mathbf{P}^{1/2}\\, \\mathbf{H}\\, \\mathbf{P}^{1/2} = \\left(\\mathbf{H}^{-1}\\right)^{1/2}\\, \\mathbf{H}\\, \\left(\\mathbf{H}^{-1}\\right)^{1/2} = \\mathbf{I},\n$$\nso whitening yields an identity Hessian in $\\boldsymbol{z}$-coordinates.\n\nWe now compute the condition numbers. For $\\mathbf{H}$, since it is symmetric positive definite, its eigenvalues are the roots of\n$$\n\\det\\!\\left(\\mathbf{H} - \\lambda \\mathbf{I}\\right) = 0 \\;\\; \\Longleftrightarrow \\;\\; \\left(50 - \\lambda\\right)\\left(10 - \\lambda\\right) - 20^{2} = 0.\n$$\nExpanding gives\n$$\n\\lambda^{2} - 60 \\lambda + 100 = 0,\n$$\nwhose solutions are\n$$\n\\lambda_{\\pm} = \\frac{60 \\pm \\sqrt{60^{2} - 4 \\cdot 100}}{2} = \\frac{60 \\pm \\sqrt{3600 - 400}}{2} = \\frac{60 \\pm \\sqrt{3200}}{2} = 30 \\pm 20 \\sqrt{2}.\n$$\nThus\n$$\n\\kappa(\\mathbf{H}) = \\frac{\\lambda_{\\max}(\\mathbf{H})}{\\lambda_{\\min}(\\mathbf{H})} = \\frac{30 + 20 \\sqrt{2}}{30 - 20 \\sqrt{2}}.\n$$\nThis can be simplified by rationalizing the denominator:\n$$\n\\kappa(\\mathbf{H}) = \\frac{30 + 20 \\sqrt{2}}{30 - 20 \\sqrt{2}} \\cdot \\frac{30 + 20 \\sqrt{2}}{30 + 20 \\sqrt{2}} = \\frac{(30 + 20 \\sqrt{2})^{2}}{30^{2} - (20 \\sqrt{2})^{2}} = \\frac{900 + 1200 \\sqrt{2} + 800}{900 - 800} = \\frac{1700 + 1200 \\sqrt{2}}{100} = 17 + 12 \\sqrt{2}.\n$$\nFor the whitened Hessian $\\mathbf{H}_{z} = \\mathbf{I}$, its eigenvalues are both $1$, hence\n$$\n\\kappa(\\mathbf{H}_{z}) = \\frac{1}{1} = 1.\n$$\nTherefore, the improvement factor in conditioning due to whitening is\n$$\nr = \\frac{\\kappa(\\mathbf{H})}{\\kappa(\\mathbf{H}_{z})} = \\frac{17 + 12 \\sqrt{2}}{1} = 17 + 12 \\sqrt{2}.\n$$\nThis is a closed-form exact expression, requiring no rounding.",
            "answer": "$$\\boxed{17 + 12\\sqrt{2}}$$"
        },
        {
            "introduction": "Theoretical algorithms meet practical limits when implemented on digital computers, especially in models with exponential functions where extreme values can cause numerical overflow or underflow. This practice addresses the critical issue of numerical stability in gradient-based optimization . You will investigate how poorly scaled parameters can lead to floating-point errors and implement a robust remedy by reformulating the objective function in logarithmic space.",
            "id": "3894248",
            "problem": "Consider the calibration of a simple exponential sensitivity model often used in environmental and earth system modeling to represent a process with multiplicative forcing. Let the exogenous forcing be the sequence $\\{x_t\\}_{t=1}^T$, the measured quantity be $\\{d_t\\}_{t=1}^T$, and the model prediction be $y_t(\\alpha,\\beta) = \\alpha \\exp(\\beta x_t)$, where $\\alpha  0$ and $\\beta \\in \\mathbb{R}$ are parameters to be calibrated.\n\nAssume independent Gaussian measurement errors with zero mean and variances $\\{\\sigma_t^2\\}_{t=1}^T$. The corresponding Maximum Likelihood Estimation (MLE) criterion is the weighted least-squares objective\n$$\nJ(\\alpha,\\beta) = \\frac{1}{2} \\sum_{t=1}^T w_t \\big(y_t(\\alpha,\\beta) - d_t\\big)^2,\n$$\nwhere $w_t = \\frac{1}{\\sigma_t^2}$.\n\nTask A (derivation from first principles): Starting solely from the definitions above and using the chain rule, derive the gradient components $\\frac{\\partial J}{\\partial \\alpha}$ and $\\frac{\\partial J}{\\partial \\beta}$ as functions of $\\alpha$, $\\beta$, $\\{x_t\\}$, $\\{d_t\\}$, and $\\{w_t\\}$.\n\nTask B (numerical instability analysis): Ill-scaled parameters can make the evaluation of $\\nabla J(\\alpha,\\beta)$ numerically unstable. Using the laws of floating-point arithmetic in Institute of Electrical and Electronics Engineers 754 (IEEE 754) double precision, overflow occurs when $\\exp(\\cdot)$ is evaluated at an argument larger than $L_{\\max} = \\log(\\text{max float})$, and underflow when the argument is smaller than $L_{\\min} = \\log(\\text{tiny})$. Compute $L_{\\max}$ and $L_{\\min}$ from your programming environment’s floating-point information, and detect overflow or underflow in the gradient evaluation by checking:\n- whether any $\\beta x_t  L_{\\max}$ (overflow risk) or $\\beta x_t  L_{\\min}$ (underflow risk),\n- whether any call to $\\exp(\\beta x_t)$ returns a non-finite value or zero.\n\nReport a boolean per test case indicating whether the naive gradient evaluation is both finite and free of overflow/underflow, defined as the conjunction of finiteness of both gradient components and the absence of the risks above.\n\nTask C (rescaling remedy and stable gradient): A robust rescaling remedy is to fit in logarithmic residuals, defining\n$$\nJ_{\\log}(\\alpha,\\beta) = \\frac{1}{2} \\sum_{t=1}^T w_t \\Big(\\log y_t(\\alpha,\\beta) - \\log d_t\\Big)^2,\n$$\nwhich is dimensionless and well-scaled when $d_t  0$. Derive $\\frac{\\partial J_{\\log}}{\\partial \\alpha}$ and $\\frac{\\partial J_{\\log}}{\\partial \\beta}$ explicitly from fundamental definitions, and implement their evaluation. This remedy avoids evaluating $\\exp(\\cdot)$ in the gradient. All quantities in this problem are unitless; produce answers as dimensionless floats or booleans.\n\nData for the test suite:\n- Forcing sequence: $\\{x_t\\} = [-20.0,\\,-10.0,\\,0.0,\\,10.0,\\,20.0]$.\n- Observations: $\\{d_t\\} = [0.1,\\,1.0,\\,10.0,\\,100.0,\\,1000.0]$ (strictly positive, suitable for logarithms).\n- Weights: $\\{w_t\\} = [1.0,\\,1.0,\\,1.0,\\,1.0,\\,1.0]$ (equivalently, $\\sigma_t^2 = 1.0$ for all $t$).\n\nParameter test cases:\n- Case $1$ (happy path): $(\\alpha,\\beta) = (2.0,\\,0.01)$.\n- Case $2$ (overflow): $(\\alpha,\\beta) = (1.0,\\,100.0)$.\n- Case $3$ (underflow): $(\\alpha,\\beta) = (10^{-300},\\,-100.0)$.\n- Case $4$ (mixed extremes): $(\\alpha,\\beta) = (1.0,\\,50.0)$.\n\nYour program must:\n1. Implement the naive gradient $\\nabla J(\\alpha,\\beta)$ from Task A and evaluate it for each test case.\n2. Detect overflow/underflow risks using $L_{\\max}$ and $L_{\\min}$ as specified in Task B, and combine those checks with finiteness of the gradient components to produce one boolean per test case indicating whether the naive gradient is finite and stable.\n3. Implement the rescaled remedy by evaluating the stable gradient $\\nabla J_{\\log}(\\alpha,\\beta)$ from Task C for each test case.\n\nFinal output format:\n- Produce a single line containing a top-level list with one sublist per test case.\n- Each sublist must be of the form $[\\text{boolean},\\,g_{\\log,\\alpha},\\,g_{\\log,\\beta}]$, where the boolean is as defined above, and $g_{\\log,\\alpha}$ and $g_{\\log,\\beta}$ are the floats for $\\frac{\\partial J_{\\log}}{\\partial \\alpha}$ and $\\frac{\\partial J_{\\log}}{\\partial \\beta}$, respectively.\n- For example, the output must look like $[[\\text{bool}_1,\\,g_{1,\\log,\\alpha},\\,g_{1,\\log,\\beta}],\\,[\\text{bool}_2,\\,g_{2,\\log,\\alpha},\\,g_{2,\\log,\\beta}],\\,[\\text{bool}_3,\\,g_{3,\\log,\\alpha},\\,g_{3,\\log,\\beta}],\\,[\\text{bool}_4,\\,g_{4,\\log,\\alpha},\\,g_{4,\\log,\\beta}]]$ on a single line.\n\nAll computations and outputs are unitless. Angles are not involved, and no percentage signs are permitted; express any proportions as decimals or fractions if needed.",
            "solution": "The problem is scientifically and mathematically well-posed, providing a self-contained and consistent set of definitions, data, and tasks relevant to numerical optimization in scientific modeling. All required conditions for the specified operations are met. We therefore proceed with the solution, which involves three main parts: the derivation of two different gradient formulations and an analysis of their numerical stability.\n\n### Task A: Derivation of the Naive Gradient $\\nabla J(\\alpha,\\beta)$\n\nThe primary objective function is the weighted sum of squared residuals, given by:\n$$\nJ(\\alpha,\\beta) = \\frac{1}{2} \\sum_{t=1}^T w_t \\big(y_t(\\alpha,\\beta) - d_t\\big)^2\n$$\nwhere the model prediction is $y_t(\\alpha,\\beta) = \\alpha \\exp(\\beta x_t)$. The weights are $w_t = 1/\\sigma_t^2$.\n\nTo find the gradient $\\nabla J = \\left[ \\frac{\\partial J}{\\partial \\alpha}, \\frac{\\partial J}{\\partial \\beta} \\right]^T$, we compute the partial derivatives with respect to $\\alpha$ and $\\beta$.\n\n1.  **Partial derivative with respect to $\\alpha$**:\n    We apply the chain rule. The derivative of the outer function $(\\cdot)^2$ gives $2(y_t - d_t)$, and the derivative of the inner function $y_t - d_t$ with respect to $\\alpha$ is $\\frac{\\partial y_t}{\\partial \\alpha}$.\n    $$\n    \\frac{\\partial J}{\\partial \\alpha} = \\frac{1}{2} \\sum_{t=1}^T w_t \\cdot 2 \\big(y_t(\\alpha,\\beta) - d_t\\big) \\cdot \\frac{\\partial y_t(\\alpha,\\beta)}{\\partial \\alpha}\n    $$\n    The partial derivative of the model function $y_t$ with respect to $\\alpha$ is:\n    $$\n    \\frac{\\partial y_t}{\\partial \\alpha} = \\frac{\\partial}{\\partial \\alpha} \\big(\\alpha \\exp(\\beta x_t)\\big) = \\exp(\\beta x_t)\n    $$\n    Substituting this back, we obtain the first component of the gradient:\n    $$\n    \\frac{\\partial J}{\\partial \\alpha} = \\sum_{t=1}^T w_t \\big(\\alpha \\exp(\\beta x_t) - d_t\\big) \\exp(\\beta x_t)\n    $$\n\n2.  **Partial derivative with respect to $\\beta$**:\n    Similarly, we apply the chain rule with respect to $\\beta$.\n    $$\n    \\frac{\\partial J}{\\partial \\beta} = \\frac{1}{2} \\sum_{t=1}^T w_t \\cdot 2 \\big(y_t(\\alpha,\\beta) - d_t\\big) \\cdot \\frac{\\partial y_t(\\alpha,\\beta)}{\\partial \\beta}\n    $$\n    The partial derivative of the model function $y_t$ with respect to $\\beta$ is:\n    $$\n    \\frac{\\partial y_t}{\\partial \\beta} = \\frac{\\partial}{\\partial \\beta} \\big(\\alpha \\exp(\\beta x_t)\\big) = \\alpha \\exp(\\beta x_t) \\cdot x_t = y_t(\\alpha,\\beta) x_t\n    $$\n    Substituting this back, we obtain the second component of the gradient:\n    $$\n    \\frac{\\partial J}{\\partial \\beta} = \\sum_{t=1}^T w_t \\big(\\alpha \\exp(\\beta x_t) - d_t\\big) \\alpha \\exp(\\beta x_t) x_t\n    $$\n\nThese two expressions constitute the naive gradient $\\nabla J(\\alpha,\\beta)$. Their evaluation requires computing $\\exp(\\beta x_t)$, which can be a source of numerical instability.\n\n### Task B: Numerical Instability Analysis\n\nThe evaluation of the naive gradient $\\nabla J(\\alpha,\\beta)$ is susceptible to numerical overflow and underflow, particularly from the term $\\exp(\\beta x_t)$. In IEEE 754 double-precision arithmetic, floating-point numbers have a finite range.\n-   **Overflow**: Occurs when a calculation produces a result larger than the maximum representable finite number, denoted `max float`. For the exponential function, this happens if its argument $\\beta x_t$ is too large. Consequently, $\\exp(\\beta x_t)$ becomes `inf`.\n-   **Underflow**: Occurs when a result is smaller in magnitude than the smallest positive normalized number, often resolving to zero. For the exponential function, this happens if its argument $\\beta x_t$ is very negative. Consequently, $\\exp(\\beta x_t)$ becomes $0$.\n\nTo formally detect the risk of these issues, we define two thresholds based on the limits of the floating-point representation:\n-   $L_{\\max} = \\log(\\text{max float})$, the largest argument to $\\exp(\\cdot)$ that does not overflow.\n-   $L_{\\min} = \\log(\\text{tiny})$, where `tiny` is the smallest positive (subnormal) floating-point number. An argument below this threshold will almost certainly underflow to $0$.\n\nThe stability check involves several conditions for a given parameter set $(\\alpha, \\beta)$ and data $\\{x_t\\}$:\n$1$. Pre-computation check: Verify that for all $t$, the argument $\\beta x_t$ is within a safe range, i.e., $L_{\\min}  \\beta x_t  L_{\\max}$.\n$2$. Post-computation check: Verify that all computed values of $\\exp(\\beta x_t)$ are finite and non-zero.\n$3$. Final gradient check: Verify that the final computed values for $\\frac{\\partial J}{\\partial \\alpha}$ and $\\frac{\\partial J}{\\partial \\beta}$ are finite numbers (not `inf`, `-inf`, or `NaN`).\n\nA boolean flag will be reported, which is `True` if and only if the final gradient components are finite and all of the aforementioned risks ( overflow/underflow at argument or result level) are absent.\n\n### Task C: Derivation of the Stable Gradient $\\nabla J_{\\log}(\\alpha,\\beta)$\n\nTo circumvent the numerical issues associated with the exponential function, the problem can be reformulated by minimizing the sum of squared errors in logarithmic space. This is often equivalent to assuming a log-normal distribution for the measurement errors. The rescaled objective function is:\n$$\nJ_{\\log}(\\alpha,\\beta) = \\frac{1}{2} \\sum_{t=1}^T w_t \\Big(\\log y_t(\\alpha,\\beta) - \\log d_t\\Big)^2\n$$\nThis requires $d_t  0$ for all $t$, a condition satisfied by the provided data. A key simplification arises from the logarithm of the model function:\n$$\n\\log y_t(\\alpha,\\beta) = \\log\\big(\\alpha \\exp(\\beta x_t)\\big) = \\log \\alpha + \\log(\\exp(\\beta x_t)) = \\log \\alpha + \\beta x_t\n$$\nSubstituting this into $J_{\\log}$, we get:\n$$\nJ_{\\log}(\\alpha,\\beta) = \\frac{1}{2} \\sum_{t=1}^T w_t \\big(\\log \\alpha + \\beta x_t - \\log d_t\\big)^2\n$$\nThis is a linear least-squares problem in the parameters $\\theta_1 = \\log \\alpha$ and $\\theta_2 = \\beta$. The gradient can be computed without evaluating any exponentials, making it numerically robust.\n\n1.  **Partial derivative with respect to $\\alpha$**:\n    We use the chain rule, differentiating with respect to $\\alpha$. The inner function is $\\log \\alpha$.\n    $$\n    \\frac{\\partial J_{\\log}}{\\partial \\alpha} = \\frac{1}{2} \\sum_{t=1}^T w_t \\cdot 2 \\big(\\log \\alpha + \\beta x_t - \\log d_t\\big) \\cdot \\frac{\\partial}{\\partial \\alpha}(\\log \\alpha + \\beta x_t - \\log d_t)\n    $$\n    The derivative of the term in parentheses with respect to $\\alpha$ is $\\frac{1}{\\alpha}$.\n    $$\n    \\frac{\\partial J_{\\log}}{\\partial \\alpha} = \\sum_{t=1}^T w_t \\big(\\log \\alpha + \\beta x_t - \\log d_t\\big) \\cdot \\frac{1}{\\alpha} = \\frac{1}{\\alpha} \\sum_{t=1}^T w_t (\\log \\alpha + \\beta x_t - \\log d_t)\n    $$\n\n2.  **Partial derivative with respect to $\\beta$**:\n    We differentiate with respect to $\\beta$.\n    $$\n    \\frac{\\partial J_{\\log}}{\\partial \\beta} = \\frac{1}{2} \\sum_{t=1}^T w_t \\cdot 2 \\big(\\log \\alpha + \\beta x_t - \\log d_t\\big) \\cdot \\frac{\\partial}{\\partial \\beta}(\\log \\alpha + \\beta x_t - \\log d_t)\n    $$\n    The derivative of the term in parentheses with respect to $\\beta$ is $x_t$.\n    $$\n    \\frac{\\partial J_{\\log}}{\\partial \\beta} = \\sum_{t=1}^T w_t \\big(\\log \\alpha + \\beta x_t - \\log d_t\\big) x_t\n    $$\n\nThese expressions for $\\nabla J_{\\log}(\\alpha,\\beta)$ are robust against the overflow and underflow issues that plague the naive formulation, as they only involve logarithms and basic arithmetic operations. The parameter $\\alpha$ must be positive, which is given. The implementation will compute these derived quantities for each of the provided test cases.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes gradients for an exponential sensitivity model, checks for numerical\n    instability in the naive formulation, and computes a stable gradient using\n    a log-transformed objective function.\n    \"\"\"\n    # Define problem data\n    x_t = np.array([-20.0, -10.0, 0.0, 10.0, 20.0])\n    d_t = np.array([0.1, 1.0, 10.0, 100.0, 1000.0])\n    w_t = np.array([1.0, 1.0, 1.0, 1.0, 1.0])\n\n    # Define parameter test cases\n    test_cases = [\n        (2.0, 0.01),      # Case 1 (happy path)\n        (1.0, 100.0),     # Case 2 (overflow)\n        (10**-300, -100.0), # Case 3 (underflow)\n        (1.0, 50.0)       # Case 4 (mixed extremes)\n    ]\n\n    # Get IEEE 754 double precision limits for exp() arguments\n    finfo = np.finfo(float)\n    L_max = np.log(finfo.max)\n    L_min = np.log(finfo.tiny)\n\n    results = []\n\n    for alpha, beta in test_cases:\n        # --- Task A  B: Naive gradient and stability check ---\n\n        # Calculate arguments for exp()\n        beta_x = beta * x_t\n\n        # Define risks as per problem statement\n        risk_arg_overflow = np.any(beta_x > L_max)\n        risk_arg_underflow = np.any(beta_x  L_min)\n\n        # Evaluate exp() while suppressing runtime warnings for over/underflow\n        with np.errstate(over='ignore', under='ignore'):\n            exp_beta_x = np.exp(beta_x)\n\n        risk_exp_nonfinite = not np.all(np.isfinite(exp_beta_x))\n        risk_exp_zero = np.any(exp_beta_x == 0.0)\n\n        absence_of_risks = not (risk_arg_overflow or risk_arg_underflow or risk_exp_nonfinite or risk_exp_zero)\n\n        # Calculate naive gradient components, suppressing all calculation warnings\n        with np.errstate(all='ignore'):\n            y_t = alpha * exp_beta_x\n            residuals = y_t - d_t\n            grad_J_alpha = np.sum(w_t * residuals * exp_beta_x)\n            grad_J_beta = np.sum(w_t * residuals * y_t * x_t)\n\n        # Check for finiteness of the final gradient components\n        finiteness = np.isfinite(grad_J_alpha) and np.isfinite(grad_J_beta)\n\n        # The final stability boolean is the conjunction of finiteness and absence of risks\n        is_stable = finiteness and absence_of_risks\n\n        # --- Task C: Stable gradient calculation ---\n\n        log_alpha = np.log(alpha)\n        log_d_t = np.log(d_t)\n        \n        log_residuals = log_alpha + beta * x_t - log_d_t\n        \n        grad_J_log_alpha = (1.0 / alpha) * np.sum(w_t * log_residuals)\n        grad_J_log_beta = np.sum(w_t * log_residuals * x_t)\n\n        results.append([is_stable, grad_J_log_alpha, grad_J_log_beta])\n\n    # Final print statement in the exact required format.\n    # The format is a string representation of a list of lists.\n    # The provided print statement template correctly constructs this.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A successful calibration that minimizes a cost function does not guarantee that the parameter values are meaningful or well-constrained by the data. This exercise introduces the concept of \"sloppiness\" and provides a quantitative method for diagnosing parameter identifiability using the Fisher Information Matrix (FIM) . By analyzing the FIM's eigenvalue spectrum, you will learn to distinguish well-posed models from those where parameter trade-offs hinder unique estimation.",
            "id": "3894183",
            "problem": "Consider parameter identifiability in a simple environmental box-model setting under additive independent Gaussian noise. Suppose a model predicts observations $y_{i}(\\boldsymbol{\\theta})$ at times $t_{i}$ for $i \\in \\{1,\\ldots,n\\}$, with parameter vector $\\boldsymbol{\\theta} \\in \\mathbb{R}^{p}$. Under independent Gaussian errors with standard deviations $\\sigma_{i}$, the negative log-likelihood is proportional to a weighted least squares objective. The Fisher Information Matrix (FIM) at $\\boldsymbol{\\theta}$ equals the Gauss–Newton approximation of the Hessian of this objective and is given by\n$$\n\\mathbf{F}(\\boldsymbol{\\theta}) \\equiv \\mathbf{J}(\\boldsymbol{\\theta})^{\\top}\\,\\mathbf{W}\\,\\mathbf{J}(\\boldsymbol{\\theta}),\n$$\nwhere $\\mathbf{J}(\\boldsymbol{\\theta}) \\in \\mathbb{R}^{n \\times p}$ is the sensitivity (Jacobian) matrix with entries $J_{ij}(\\boldsymbol{\\theta}) = \\frac{\\partial y_{i}(\\boldsymbol{\\theta})}{\\partial \\theta_{j}}$, and $\\mathbf{W} = \\mathrm{diag}(w_{1},\\ldots,w_{n})$ with $w_{i} = \\sigma_{i}^{-2}$. When columns of $\\mathbf{J}(\\boldsymbol{\\theta})$ are nearly collinear, $\\mathbf{F}(\\boldsymbol{\\theta})$ exhibits a spectrum of eigenvalues with widely separated magnitudes; small eigenvalues correspond to directions of weak identifiability (so-called sloppiness). For numerical assessment, define the condition number\n$$\n\\kappa(\\mathbf{F}) \\equiv \\frac{\\lambda_{\\max}(\\mathbf{F})}{\\max\\{\\lambda_{\\min}(\\mathbf{F}), \\delta\\}},\n$$\nwhere $\\lambda_{\\max}$ and $\\lambda_{\\min}$ are the largest and smallest eigenvalues of $\\mathbf{F}$, respectively, and $\\delta$ is a small positive number used only to avoid division by zero in numerical computations (but not to alter rank decisions). A model is deemed “sloppy” at $\\boldsymbol{\\theta}$ if $\\kappa(\\mathbf{F})  \\tau$ for a prescribed threshold $\\tau$. For numerical rank, use\n$$\n\\mathrm{rank}_{\\epsilon}(\\mathbf{F}) \\equiv \\#\\{\\lambda_{j}(\\mathbf{F}) : \\lambda_{j}(\\mathbf{F}) \\ge \\epsilon \\cdot \\lambda_{\\max}(\\mathbf{F})\\},\n$$\nwith tolerance $\\epsilon$.\n\nAll quantities in this problem are dimensionless.\n\nYou must implement a program that, for each of the following test cases, constructs $\\mathbf{J}(\\boldsymbol{\\theta})$ at the specified $\\boldsymbol{\\theta}$, forms $\\mathbf{F}(\\boldsymbol{\\theta})$, computes the smallest and largest eigenvalues, the condition number, a boolean sloppiness classification using threshold $\\tau$, and the numerical rank using tolerance $\\epsilon$ as specified below. Then aggregate the results across all test cases into the final output format.\n\nModels and their sensitivities to be used:\n- Model $\\mathcal{M}_{1}$ (“single-compartment exponential decay with amplitude”): $y(t; a, k) = a\\,\\exp(-k\\,t)$ with parameters $\\boldsymbol{\\theta} = [a, k]^{\\top}$. Sensitivities are\n$$\n\\frac{\\partial y}{\\partial a}(t; a, k) = \\exp(-k\\,t), \\quad \\frac{\\partial y}{\\partial k}(t; a, k) = -a\\,t\\,\\exp(-k\\,t).\n$$\n- Model $\\mathcal{M}_{2}$ (“product ambiguity”): $y(t; \\alpha, \\beta) = (\\alpha\\,\\beta)\\,\\exp(-t)$ with parameters $\\boldsymbol{\\theta} = [\\alpha, \\beta]^{\\top}$. Sensitivities are\n$$\n\\frac{\\partial y}{\\partial \\alpha}(t; \\alpha, \\beta) = \\beta\\,\\exp(-t), \\quad \\frac{\\partial y}{\\partial \\beta}(t; \\alpha, \\beta) = \\alpha\\,\\exp(-t).\n$$\n\nUse the following global numerical constants in all cases: $\\delta = 10^{-18}$ (for the condition number denominator safeguard), $\\epsilon = 10^{-12}$ (for the numerical rank), and $\\tau = 10^{6}$ (for the sloppiness classification).\n\nTest suite (each case specifies the model, parameter vector $\\boldsymbol{\\theta}$, observation times $\\{t_{i}\\}$, and standard deviations $\\{\\sigma_{i}\\}$):\n- Case $1$ (happy path, moderately well-conditioned sensitivities):\n  - Model $\\mathcal{M}_{1}$, $\\boldsymbol{\\theta} = [1.0, 0.4]^{\\top}$.\n  - Times: $t \\in \\{0, 1, 2, 5, 10\\}$.\n  - Standard deviations: $\\sigma_{i} = 0.05$ for all $i$.\n- Case $2$ (near-collinearity in sensitivities due to limited time span, inducing sloppiness):\n  - Model $\\mathcal{M}_{1}$, $\\boldsymbol{\\theta} = [1.0, 0.5]^{\\top}$.\n  - Times: $t \\in \\{0, 0.02, 0.04, 0.06, 0.08\\}$.\n  - Standard deviations: $\\sigma_{i} = 0.05$ for all $i$.\n- Case $3$ (structural non-identifiability due to product ambiguity; exact rank deficiency):\n  - Model $\\mathcal{M}_{2}$, $\\boldsymbol{\\theta} = [2.0, 0.5]^{\\top}$.\n  - Times: $t \\in \\{0, 1, 2, 3\\}$.\n  - Standard deviations: $\\sigma_{i} = 0.1$ for all $i$.\n- Case $4$ (boundary weighting: overwhelmingly precise observation at $t = 0$ amplifies one direction, increasing $\\kappa$):\n  - Model $\\mathcal{M}_{1}$, $\\boldsymbol{\\theta} = [1.0, 0.4]^{\\top}$.\n  - Times: $t \\in \\{0, 1, 2, 3, 4\\}$.\n  - Standard deviations: $\\sigma = [10^{-6}, 0.1, 0.1, 0.1, 0.1]$ aligned with the listed times.\n\nFor each case, compute:\n- The smallest eigenvalue $\\lambda_{\\min}$ of $\\mathbf{F}(\\boldsymbol{\\theta})$.\n- The largest eigenvalue $\\lambda_{\\max}$ of $\\mathbf{F}(\\boldsymbol{\\theta})$.\n- The condition number $\\kappa(\\mathbf{F})$ as defined above, using $\\delta = 10^{-18}$ in the denominator safeguard.\n- The sloppiness boolean defined by $\\kappa(\\mathbf{F})  \\tau$ with $\\tau = 10^{6}$.\n- The numerical rank $\\mathrm{rank}_{\\epsilon}(\\mathbf{F})$ with $\\epsilon = 10^{-12}$.\n\nYour program should produce a single line of output containing the results for all cases as a comma-separated list enclosed in square brackets. Each case’s result should itself be a list in the order $[\\lambda_{\\min}, \\lambda_{\\max}, \\kappa, \\text{sloppy}, \\mathrm{rank}_{\\epsilon}]$. For example, a valid output format with two hypothetical cases would look like $[[\\ldots],[\\ldots]]$. The actual numerical values should be computed by your program and printed in this exact nested list format. No external inputs are permitted; all values are as specified above.",
            "solution": "The user has provided a problem concerning the numerical assessment of parameter identifiability in environmental models using the Fisher Information Matrix (FIM). I will first verify the problem's validity and then proceed to a detailed solution.\n\n### Step 1: Extract Givens\n- **Fisher Information Matrix (FIM):** $\\mathbf{F}(\\boldsymbol{\\theta}) \\equiv \\mathbf{J}(\\boldsymbol{\\theta})^{\\top}\\,\\mathbf{W}\\,\\mathbf{J}(\\boldsymbol{\\theta})$, where $\\mathbf{J}(\\boldsymbol{\\theta})$ is the sensitivity matrix with entries $J_{ij} = \\frac{\\partial y_{i}(\\boldsymbol{\\theta})}{\\partial \\theta_{j}}$ and $\\mathbf{W} = \\mathrm{diag}(\\sigma_{1}^{-2}, \\ldots, \\sigma_{n}^{-2})$ is the weight matrix.\n- **Condition Number:** $\\kappa(\\mathbf{F}) \\equiv \\frac{\\lambda_{\\max}(\\mathbf{F})}{\\max\\{\\lambda_{\\min}(\\mathbf{F}), \\delta\\}}$, where $\\lambda_{\\max}$ and $\\lambda_{\\min}$ are the maximum and minimum eigenvalues of $\\mathbf{F}$.\n- **Numerical Rank:** $\\mathrm{rank}_{\\epsilon}(\\mathbf{F}) \\equiv \\#\\{\\lambda_{j}(\\mathbf{F}) : \\lambda_{j}(\\mathbf{F}) \\ge \\epsilon \\cdot \\lambda_{\\max}(\\mathbf{F})\\}$.\n- **Sloppiness Criterion:** A model is deemed \"sloppy\" if $\\kappa(\\mathbf{F})  \\tau$.\n- **Global Constants:**\n    - Safeguard for condition number: $\\delta = 10^{-18}$.\n    - Tolerance for numerical rank: $\\epsilon = 10^{-12}$.\n    - Threshold for sloppiness: $\\tau = 10^{6}$.\n- **Model $\\mathcal{M}_{1}$:**\n    - Functional form: $y(t; a, k) = a\\,\\exp(-k\\,t)$.\n    - Parameters: $\\boldsymbol{\\theta} = [a, k]^{\\top}$.\n    - Sensitivities: $\\frac{\\partial y}{\\partial a} = \\exp(-k\\,t)$, $\\frac{\\partial y}{\\partial k} = -a\\,t\\,\\exp(-k\\,t)$.\n- **Model $\\mathcal{M}_{2}$:**\n    - Functional form: $y(t; \\alpha, \\beta) = (\\alpha\\,\\beta)\\,\\exp(-t)$.\n    - Parameters: $\\boldsymbol{\\theta} = [\\alpha, \\beta]^{\\top}$.\n    - Sensitivities: $\\frac{\\partial y}{\\partial \\alpha} = \\beta\\,\\exp(-t)$, $\\frac{\\partial y}{\\partial \\beta} = \\alpha\\,\\exp(-t)$.\n- **Test Cases:**\n    - **Case 1:** Model $\\mathcal{M}_{1}$, $\\boldsymbol{\\theta}=[1.0, 0.4]^{\\top}$, Times $t \\in \\{0, 1, 2, 5, 10\\}$, All $\\sigma_i = 0.05$.\n    - **Case 2:** Model $\\mathcal{M}_{1}$, $\\boldsymbol{\\theta}=[1.0, 0.5]^{\\top}$, Times $t \\in \\{0, 0.02, 0.04, 0.06, 0.08\\}$, All $\\sigma_i = 0.05$.\n    - **Case 3:** Model $\\mathcal{M}_{2}$, $\\boldsymbol{\\theta}=[2.0, 0.5]^{\\top}$, Times $t \\in \\{0, 1, 2, 3\\}$, All $\\sigma_i = 0.1$.\n    - **Case 4:** Model $\\mathcal{M}_{1}$, $\\boldsymbol{\\theta}=[1.0, 0.4]^{\\top}$, Times $t \\in \\{0, 1, 2, 3, 4\\}$, $\\sigma = [10^{-6}, 0.1, 0.1, 0.1, 0.1]$.\n- **Required Outputs per Case:** A list containing $[\\lambda_{\\min}, \\lambda_{\\max}, \\kappa, \\text{sloppy}, \\mathrm{rank}_{\\epsilon}]$.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientific Grounding:** The problem is firmly rooted in the established statistical theory of parameter estimation and identifiability analysis for nonlinear models. The use of the Fisher Information Matrix, its eigenvalues, condition number, and numerical rank are standard techniques in this field. The models and scenarios are archetypes used to illustrate these concepts. The problem is scientifically and mathematically sound.\n2.  **Well-Posedness:** The problem is well-posed. For each case, all necessary components (model, parameters, observation schedule, error structure) are explicitly defined, allowing for the unambiguous construction of the Jacobian $\\mathbf{J}$, the weight matrix $\\mathbf{W}$, and consequently the FIM $\\mathbf{F}$. The metrics to be computed from $\\mathbf{F}$ are also precisely defined.\n3.  **Objectivity:** The problem is stated objectively, using precise mathematical definitions and numerical values. It is free of ambiguity, subjectivity, or opinion.\n\n### Step 3: Verdict and Action\nThe problem is valid. It is a well-defined computational task based on sound scientific principles. I will now proceed with the solution.\n\n### Algorithm Design\nThe solution requires implementing a computational procedure that processes each test case to determine its identifiability characteristics. For each case, the following sequence of steps must be performed:\n\n1.  **Identify Case Specifics:** Extract the model type ($\\mathcal{M}_{1}$ or $\\mathcal{M}_{2}$), the parameter vector $\\boldsymbol{\\theta}$, the vector of observation times $\\{t_i\\}_{i=1}^n$, and the vector of standard deviations $\\{\\sigma_i\\}_{i=1}^n$.\n\n2.  **Construct the Sensitivity Matrix $\\mathbf{J}$:** This matrix has dimensions $n \\times p$, where $n$ is the number of time points and $p$ is the number of parameters ($p=2$ for both models). Each column of $\\mathbf{J}$ is the vector of sensitivities of the model output with respect to one parameter, evaluated at all time points.\n    - For model $\\mathcal{M}_1$ with $\\boldsymbol{\\theta}=[a,k]^\\top$, the columns are computed using the provided sensitivity equations:\n      - Column $1$: $\\frac{\\partial y}{\\partial a}(t_i; a, k) = \\exp(-k\\,t_i)$ for $i=1, \\dots, n$.\n      - Column $2$: $\\frac{\\partial y}{\\partial k}(t_i; a, k) = -a\\,t_i\\,\\exp(-k\\,t_i)$ for $i=1, \\dots, n$.\n    - For model $\\mathcal{M}_2$ with $\\boldsymbol{\\theta}=[\\alpha,\\beta]^\\top$:\n      - Column $1$: $\\frac{\\partial y}{\\partial \\alpha}(t_i; \\alpha, \\beta) = \\beta\\,\\exp(-t_i)$ for $i=1, \\dots, n$.\n      - Column $2$: $\\frac{\\partial y}{\\partial \\beta}(t_i; \\alpha, \\beta) = \\alpha\\,\\exp(-t_i)$ for $i=1, \\dots, n$.\n\n3.  **Construct the Weight Matrix $\\mathbf{W}$:** This is an $n \\times n$ diagonal matrix. The diagonal elements are the inverse variances, $W_{ii} = w_i = \\sigma_i^{-2}$.\n\n4.  **Compute the Fisher Information Matrix $\\mathbf{F}$:** This involves matrix multiplication according to the formula $\\mathbf{F} = \\mathbf{J}^{\\top}\\mathbf{W}\\mathbf{J}$. The resulting matrix $\\mathbf{F}$ will be a $p \\times p$ symmetric, positive semi-definite matrix. For this problem, $p=2$.\n\n5.  **Eigenvalue Decomposition:** Compute the eigenvalues of the matrix $\\mathbf{F}$. Since $\\mathbf{F}$ is real and symmetric, its eigenvalues are real and non-negative. Let the computed eigenvalues, sorted in non-decreasing order, be $\\lambda_1, \\dots, \\lambda_p$. Then, $\\lambda_{\\min} = \\lambda_1$ and $\\lambda_{\\max} = \\lambda_p$.\n\n6.  **Calculate Identifiability Metrics:**\n    - **Smallest Eigenvalue:** $\\lambda_{\\min}$.\n    - **Largest Eigenvalue:** $\\lambda_{\\max}$.\n    - **Condition Number:** Compute $\\kappa(\\mathbf{F}) = \\frac{\\lambda_{\\max}}{\\max\\{\\lambda_{\\min}, \\delta\\}}$ using the given constant $\\delta = 10^{-18}$.\n    - **Sloppiness Classification:** Evaluate the boolean expression $\\kappa(\\mathbf{F})  \\tau$, where $\\tau = 10^{6}$.\n    - **Numerical Rank:** Count the number of eigenvalues $\\lambda_j$ that satisfy the condition $\\lambda_j \\ge \\epsilon \\cdot \\lambda_{\\max}$, using $\\epsilon = 10^{-12}$.\n\n7.  **Aggregate Results:** Store the five computed values—$[\\lambda_{\\min}, \\lambda_{\\max}, \\kappa, \\text{sloppy}, \\mathrm{rank}_{\\epsilon}]$—for the current case. Repeat for all test cases and compile the final list of results. The final output must be formatted as a single-line string representation of a list of lists.\nI will now implement this procedure in Python.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the parameter identifiability problem for the given test cases.\n    \"\"\"\n    \n    # Global numerical constants\n    DELTA = 1e-18   # Safeguard for condition number denominator\n    EPSILON = 1e-12 # Tolerance for numerical rank\n    TAU = 1e6       # Threshold for sloppiness classification\n\n    def get_jacobian_m1(theta, times):\n        \"\"\"Computes the Jacobian for Model M1.\"\"\"\n        a, k = theta\n        n = len(times)\n        p = 2\n        J = np.zeros((n, p))\n        exp_term = np.exp(-k * times)\n        J[:, 0] = exp_term  # d(y)/d(a)\n        J[:, 1] = -a * times * exp_term  # d(y)/d(k)\n        return J\n\n    def get_jacobian_m2(theta, times):\n        \"\"\"Computes the Jacobian for Model M2.\"\"\"\n        alpha, beta = theta\n        n = len(times)\n        p = 2\n        J = np.zeros((n, p))\n        exp_term = np.exp(-times)\n        J[:, 0] = beta * exp_term  # d(y)/d(alpha)\n        J[:, 1] = alpha * exp_term  # d(y)/d(beta)\n        return J\n\n    # Test suite definition\n    test_cases = [\n        {\n            \"id\": 1,\n            \"model_func\": get_jacobian_m1,\n            \"theta\": np.array([1.0, 0.4]),\n            \"times\": np.array([0, 1, 2, 5, 10], dtype=float),\n            \"sigmas\": np.array([0.05, 0.05, 0.05, 0.05, 0.05], dtype=float),\n        },\n        {\n            \"id\": 2,\n            \"model_func\": get_jacobian_m1,\n            \"theta\": np.array([1.0, 0.5]),\n            \"times\": np.array([0, 0.02, 0.04, 0.06, 0.08], dtype=float),\n            \"sigmas\": np.array([0.05, 0.05, 0.05, 0.05, 0.05], dtype=float),\n        },\n        {\n            \"id\": 3,\n            \"model_func\": get_jacobian_m2,\n            \"theta\": np.array([2.0, 0.5]),\n            \"times\": np.array([0, 1, 2, 3], dtype=float),\n            \"sigmas\": np.array([0.1, 0.1, 0.1, 0.1], dtype=float),\n        },\n        {\n            \"id\": 4,\n            \"model_func\": get_jacobian_m1,\n            \"theta\": np.array([1.0, 0.4]),\n            \"times\": np.array([0, 1, 2, 3, 4], dtype=float),\n            \"sigmas\": np.array([1e-6, 0.1, 0.1, 0.1, 0.1], dtype=float),\n        },\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        # Step 1: Construct Jacobian J\n        J = case[\"model_func\"](case[\"theta\"], case[\"times\"])\n        \n        # Step 2: Construct Weight Matrix W\n        weights = 1.0 / (case[\"sigmas\"] ** 2)\n        W = np.diag(weights)\n        \n        # Step 3: Compute Fisher Information Matrix F\n        F = J.T @ W @ J\n        \n        # Step 4: Eigenvalue decomposition\n        # eigvalsh returns eigenvalues in ascending order for symmetric matrices\n        eigenvalues = np.linalg.eigvalsh(F)\n        \n        lambda_min = eigenvalues[0]\n        lambda_max = eigenvalues[-1]\n        \n        # Step 5: Calculate identifiability metrics\n        \n        # Condition number\n        kappa = lambda_max / max(lambda_min, DELTA)\n        \n        # Sloppiness boolean\n        is_sloppy = kappa > TAU\n        \n        # Numerical rank\n        rank_threshold = EPSILON * lambda_max\n        # Note: for a singular matrix, lambda_min can be slightly negative due to numerics,\n        # but the threshold is non-negative, so the comparison is fine.\n        numerical_rank = np.sum(eigenvalues >= rank_threshold)\n        \n        # Assemble results for the case\n        case_results = [\n            lambda_min, \n            lambda_max, \n            kappa, \n            is_sloppy, \n            int(numerical_rank)\n        ]\n        all_results.append(case_results)\n\n    # Format the final output string\n    # str() on a list produces a string representation with brackets, e.g., '[...]'\n    # str() on a boolean produces 'True' or 'False'\n    result_strings = [str(res) for res in all_results]\n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n```"
        }
    ]
}