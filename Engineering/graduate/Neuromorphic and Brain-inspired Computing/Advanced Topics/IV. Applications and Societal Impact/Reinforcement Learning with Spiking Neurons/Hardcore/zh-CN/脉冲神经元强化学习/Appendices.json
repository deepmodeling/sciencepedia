{
    "hands_on_practices": [
        {
            "introduction": "要使用脉冲神经元构建强化学习智能体，我们必须首先理解单个神经元如何处理信息。这个基础练习将引导您推导漏积分放电（LIF）模型在恒定输入电流下，其输入与产生的脉冲时序之间的关系。掌握这种联系是理解脉冲神经网络（SNNs）如何编码和处理信息以做出决策的关键。",
            "id": "4057344",
            "problem": "在用脉冲神经元实现的强化学习（RL）策略网络中，一个动作通道由单个漏积分放电（LIF）神经元发出，其峰间间隔（ISI）编码了动作的强度。在恒定注入电流 $I(t)=I_0$ 的作用下，膜电位 $V(t)$ 遵循标准的LIF动力学，其膜时间常数为 $\\tau_m$，膜电阻为 $R$，即\n$$\n\\tau_m \\frac{dV}{dt} = - V + R I_0.\n$$\n当 $V(t)$ 达到阈值 $\\vartheta$ 时，会发出一个脉冲，之后膜电位瞬间重置为 $V_\\mathrm{reset}$。假设没有不应期或其他突触动力学。在 $t=0$ 时刻的脉冲之后，膜电位立即满足 $V(0^+) = V_\\mathrm{reset}$。假设参数满足 $V_\\mathrm{reset}  \\vartheta$ 和 $R I_0 > \\vartheta$，从而保证后续脉冲能在有限时间内发生。\n\n从给定的微分方程和这些条件出发，基于第一性原理推导峰间间隔 $T_{\\mathrm{ISI}}$（从 $t=0$ 时刻重置到下一次穿越阈值的时间）的闭式解析表达式，该表达式是关于 $\\tau_m$、$R$、$I_0$、$\\vartheta$ 和 $V_\\mathrm{reset}$ 的函数。将你的最终答案以秒为单位，表示为单个闭式表达式。不需要进行数值计算或四舍五入。",
            "solution": "所述问题是有效的。它在科学上基于已建立的漏积分放电（LIF）神经元模型，在数学上是适定的，具有一个特定的常微分方程和充分的初始条件，并以客观、正式的语言表述。因此，我们可以直接进行推导。\n\n膜电位 $V(t)$ 的动力学由以下微分方程决定：\n$$\n\\tau_m \\frac{dV}{dt} = - V + R I_0\n$$\n这是一个一阶线性非齐次常微分方程。我们可以将其整理成标准形式 $\\frac{dV}{dt} + p(t)V = q(t)$:\n$$\n\\frac{dV}{dt} + \\frac{1}{\\tau_m} V(t) = \\frac{R I_0}{\\tau_m}\n$$\n该方程的通解是齐次解 $V_h(t)$ 和一个特解 $V_p(t)$ 的和。\n\n齐次方程为 $\\frac{dV_h}{dt} + \\frac{1}{\\tau_m} V_h = 0$，其解为 $V_h(t) = A \\exp(-t/\\tau_m)$，其中 $A$ 是一个积分常数。\n\n对于特解，由于输入电流 $I_0$ 是恒定的，常微分方程的右侧也是一个常数。因此，我们可以寻找一个常数稳态解 $V_p(t) = V_{\\infty}$。将其代入完整的常微分方程，导数项变为零：\n$$\n\\tau_m \\frac{d(V_{\\infty})}{dt} = 0 = -V_{\\infty} + R I_0\n$$\n这得到稳态电位 $V_{\\infty} = R I_0$。\n\n$V(t)$ 的完整通解是齐次解和特解的和：\n$$\nV(t) = V_h(t) + V_p(t) = V_{\\infty} + A \\exp(-t/\\tau_m) = R I_0 + A \\exp(-t/\\tau_m)\n$$\n为了确定常数 $A$，我们应用初始条件。问题指明，在 $t=0$ 时刻的脉冲之后，膜电位被重置。因此，$V(0) = V_\\mathrm{reset}$。将此代入通解中：\n$$\nV(0) = V_\\mathrm{reset} = R I_0 + A \\exp(-0/\\tau_m) = R I_0 + A\n$$\n求解 $A$，我们得到 $A = V_\\mathrm{reset} - R I_0$。\n\n将这个常数代回通解，得到从重置值开始的膜电位的显式轨迹：\n$$\nV(t) = R I_0 + (V_\\mathrm{reset} - R I_0) \\exp(-t/\\tau_m)\n$$\n峰间间隔 $T_{\\mathrm{ISI}}$ 是电位从 $V_\\mathrm{reset}$ 上升到放电阈值 $\\vartheta$ 所需的时间。我们通过设置 $t = T_{\\mathrm{ISI}}$ 和 $V(T_{\\mathrm{ISI}}) = \\vartheta$ 来求得这个时间：\n$$\n\\vartheta = R I_0 + (V_\\mathrm{reset} - R I_0) \\exp(-T_{\\mathrm{ISI}}/\\tau_m)\n$$\n现在我们求解这个方程以得到 $T_{\\mathrm{ISI}}$。首先，我们分离出指数项：\n$$\n\\vartheta - R I_0 = (V_\\mathrm{reset} - R I_0) \\exp(-T_{\\mathrm{ISI}}/\\tau_m)\n$$\n$$\n\\exp(-T_{\\mathrm{ISI}}/\\tau_m) = \\frac{\\vartheta - R I_0}{V_\\mathrm{reset} - R I_0}\n$$\n通过将分数的分子和分母同乘以 $-1$，我们可以更方便地表示右侧：\n$$\n\\exp(-T_{\\mathrm{ISI}}/\\tau_m) = \\frac{R I_0 - \\vartheta}{R I_0 - V_\\mathrm{reset}}\n$$\n为了求解 $T_{\\mathrm{ISI}}$，我们对两边取自然对数：\n$$\n\\ln\\left( \\exp(-T_{\\mathrm{ISI}}/\\tau_m) \\right) = \\ln\\left( \\frac{R I_0 - \\vartheta}{R I_0 - V_\\mathrm{reset}} \\right)\n$$\n$$\n-\\frac{T_{\\mathrm{ISI}}}{\\tau_m} = \\ln\\left( \\frac{R I_0 - \\vartheta}{R I_0 - V_\\mathrm{reset}} \\right)\n$$\n最后，乘以 $-\\tau_m$ 得到 $T_{\\mathrm{ISI}}$ 的表达式：\n$$\nT_{\\mathrm{ISI}} = -\\tau_m \\ln\\left( \\frac{R I_0 - \\vartheta}{R I_0 - V_\\mathrm{reset}} \\right)\n$$\n利用对数恒等式 $-\\ln(x/y) = \\ln(y/x)$，我们得到最终的闭式表达式：\n$$\nT_{\\mathrm{ISI}} = \\tau_m \\ln\\left( \\frac{R I_0 - V_\\mathrm{reset}}{R I_0 - \\vartheta} \\right)\n$$\n条件 $R I_0 > \\vartheta$ 和 $V_\\mathrm{reset}  \\vartheta$ 确保了 $R I_0 - V_\\mathrm{reset} > R I_0 - \\vartheta > 0$。因此，对数的参数大于 1，这保证了 $T_{\\mathrm{ISI}}$ 是一个正的实数值时间间隔，这与物理要求相符。",
            "answer": "$$\n\\boxed{\\tau_m \\ln\\left( \\frac{R I_0 - V_\\mathrm{reset}}{R I_0 - \\vartheta} \\right)}\n$$"
        },
        {
            "introduction": "在理解了单个神经元的动力学之后，我们现在来探讨神经元之间的突触连接是如何学习的。本练习将解析三因子学习规则的机制，这是脉冲神经网络中强化学习的基石，其中局部脉冲时序会产生一个突触“资格迹”。您将计算该迹如何演变，以及一个延迟的奖励信号如何将其转化为持久的权重更新，从而将微观的神经事件与宏观的行为成功联系起来。",
            "id": "4057394",
            "problem": "在一个三因子强化学习（RL）框架中，考虑一个连接一个突触前脉冲神经元和一个突触后脉冲神经元的单一可塑性突触。该突触维持一个资格迹 $e(t)$，它由局部脉冲时序通过脉冲时序依赖可塑性（STDP）机制塑造，并随时间衰减。突触前脉冲序列为 $s_{\\text{pre}}(t) = \\sum_{i}\\delta(t - t_{i}^{\\text{pre}})$，突触后脉冲序列为 $s_{\\text{post}}(t) = \\sum_{j}\\delta(t - t_{j}^{\\text{post}})$，其中 $\\delta(\\cdot)$ 表示狄拉克δ函数，脉冲分别在时间 $t_{i}^{\\text{pre}}$ 和 $t_{j}^{\\text{post}}$ 发生。通过微分方程 $\\frac{d x(t)}{d t} = -\\frac{x(t)}{\\tau_{+}} + s_{\\text{pre}}(t)$ 和 $\\frac{d y(t)}{d t} = -\\frac{y(t)}{\\tau_{-}} + s_{\\text{post}}(t)$ 定义指数衰减的脉冲迹 $x(t)$ 和 $y(t)$，其中 $x(t)$ 和 $y(t)$ 分别在突触前和突触后脉冲时增加1。资格迹 $e(t)$ 遵循泄漏累积规则 $\\frac{d e(t)}{d t} = -\\frac{e(t)}{\\tau_{e}} + A_{+}\\,x(t)\\,s_{\\text{post}}(t) - A_{-}\\,y(t)\\,s_{\\text{pre}}(t)$，且 $e(0)=0$。与时间差为 $\\Delta t = t^{\\text{post}} - t^{\\text{pre}}$ 的突触前-后脉冲对相关的成对STDP更新核 $g(\\Delta t)$ 由 $g(\\Delta t) = A_{+}\\exp\\!\\left(-\\frac{\\Delta t}{\\tau_{+}}\\right)$（当 $\\Delta t > 0$ 时），$g(\\Delta t) = -A_{-}\\exp\\!\\left(\\frac{\\Delta t}{\\tau_{-}}\\right)$（当 $\\Delta t  0$ 时）给出，且 $g(0)=0$。假设以下参数和脉冲时间：$\\tau_{e} = 100\\,\\text{ms}$，$\\tau_{+} = 20\\,\\text{ms}$，$\\tau_{-} = 30\\,\\text{ms}$，$A_{+} = 0.8$，$A_{-} = 0.5$，$t_{i}^{\\text{pre}} \\in \\{10\\,\\text{ms},\\,70\\,\\text{ms}\\}$，以及 $t_{j}^{\\text{post}} \\in \\{40\\,\\text{ms},\\,90\\,\\text{ms}\\}$。一个标量奖励 $r$ 在时间 $t_{r} = 150\\,\\text{ms}$ 到达，其大小为 $r = 0.4$，学习率为 $\\eta = 0.05$。从所提供的定义出发，不引入任何额外的唯象简化规则，计算由该突触前-后序列产生的资格迹 $e(t_{r})$，然后计算净权重更新 $\\Delta w = \\eta\\,r\\,e(t_{r})$。将你对 $\\Delta w$ 的最终数值答案四舍五入到六位有效数字。最终答案不带单位表示。",
            "solution": "该问题要求计算净权重更新 $\\Delta w = \\eta\\,r\\,e(t_r)$。核心任务是计算奖励到达时刻 $t_r=150\\,\\text{ms}$ 的资格迹值 $e(t_r)$。\n\n资格迹 $e(t)$ 的动力学由一个线性常微分方程描述，其解可以表示为对过去驱动项的指数衰减求和。在本问题中，资格迹的更新可以被理解为一个简化的模型，即每个突触前-后脉冲对 $(t_i^{\\text{pre}}, t_j^{\\text{post}})$ 都会在突触后脉冲发放的时刻 $t_j^{\\text{post}}$ 对资格迹产生一个瞬时更新，更新量为 $g(\\Delta t_{ij})$，其中 $\\Delta t_{ij} = t_j^{\\text{post}} - t_i^{\\text{pre}}$。然后，这个贡献会以时间常数 $\\tau_e$ 指数衰减。\n\n因此，在奖励时刻 $t_r$ 的资格迹值是所有脉冲对产生的、并衰减到 $t_r$ 的贡献之和：\n$$ e(t_r) = \\sum_{i,j} \\exp\\left(-\\frac{t_r - t_j^{\\text{post}}}{\\tau_e}\\right) g(t_j^{\\text{post}} - t_i^{\\text{pre}}) $$\n其中求和遍历所有可能的突触前脉冲 $i$ 和突触后脉冲 $j$。\n\n给定的脉冲时间为：\n- 突触前: $t_1^{\\text{pre}} = 10\\,\\text{ms}$, $t_2^{\\text{pre}} = 70\\,\\text{ms}$\n- 突触后: $t_1^{\\text{post}} = 40\\,\\text{ms}$, $t_2^{\\text{post}} = 90\\,\\text{ms}$\n\n我们有四个脉冲对需要考虑：\n1.  **$(t_1^{\\text{pre}}, t_1^{\\text{post}}) = (10, 40)$**:\n    $\\Delta t = 40 - 10 = 30\\,\\text{ms} > 0$ (因果)。\n    $g(30) = A_+ \\exp(-30/\\tau_+) = 0.8 \\exp(-30/20) = 0.8 \\exp(-1.5)$。\n2.  **$(t_2^{\\text{pre}}, t_1^{\\text{post}}) = (70, 40)$**:\n    $\\Delta t = 40 - 70 = -30\\,\\text{ms}  0$ (反因果)。\n    $g(-30) = -A_- \\exp(-30/\\tau_-) = -0.5 \\exp(-30/30) = -0.5 \\exp(-1)$。\n3.  **$(t_1^{\\text{pre}}, t_2^{\\text{post}}) = (10, 90)$**:\n    $\\Delta t = 90 - 10 = 80\\,\\text{ms} > 0$ (因果)。\n    $g(80) = A_+ \\exp(-80/\\tau_+) = 0.8 \\exp(-80/20) = 0.8 \\exp(-4)$。\n4.  **$(t_2^{\\text{pre}}, t_2^{\\text{post}}) = (70, 90)$**:\n    $\\Delta t = 90 - 70 = 20\\,\\text{ms} > 0$ (因果)。\n    $g(20) = A_+ \\exp(-20/\\tau_+) = 0.8 \\exp(-20/20) = 0.8 \\exp(-1)$。\n\n现在，我们将这些贡献分组并应用衰减。\n\n- **在 $t_1^{\\text{post}} = 40\\,\\text{ms}$ 发生的更新**:\n  贡献来自对(1)和对(2)。从 $t=40$ 到 $t_r=150$ 的衰减时间为 $110\\,\\text{ms}$。\n  $e_1(t_r) = \\exp(-110/\\tau_e) [g(30) + g(-30)] = \\exp(-110/100) [0.8 e^{-1.5} - 0.5 e^{-1}]$。\n  $e_1(t_r) = e^{-1.1} [0.8 \\times 0.223130 - 0.5 \\times 0.367879] = e^{-1.1} [0.178504 - 0.183940] = e^{-1.1} [-0.005436] \\approx -0.0018079$。\n\n- **在 $t_2^{\\text{post}} = 90\\,\\text{ms}$ 发生的更新**:\n  贡献来自对(3)和对(4)。从 $t=90$ 到 $t_r=150$ 的衰减时间为 $60\\,\\text{ms}$。\n  $e_2(t_r) = \\exp(-60/\\tau_e) [g(80) + g(20)] = \\exp(-60/100) [0.8 e^{-4} + 0.8 e^{-1}]$。\n  $e_2(t_r) = e^{-0.6} \\cdot 0.8 [e^{-4} + e^{-1}] = e^{-0.6} \\cdot 0.8 [0.018316 + 0.367879] = e^{-0.6} \\cdot 0.8 [0.386195] \\approx 0.548812 \\times 0.308956 \\approx 0.1695536$。\n\n总的资格迹值为：\n$e(t_r) = e_1(t_r) + e_2(t_r) \\approx -0.0018079 + 0.1695536 = 0.1677457$。\n\n最后，我们计算权重更新 $\\Delta w$：\n$\\Delta w = \\eta \\cdot r \\cdot e(t_r) = 0.05 \\times 0.4 \\times 0.1677457 = 0.02 \\times 0.1677457 \\approx 0.003354914$。\n\n四舍五入到六位有效数字，我们得到 $0.00335491$。",
            "answer": "$$\n\\boxed{0.00335491}\n$$"
        },
        {
            "introduction": "有效的学习需要的不仅仅是可塑性机制；智能体还必须正确地分配信贷。本问题呈现了一个场景，其中一个简单的学习规则会被环境中的相关性所误导，导致有偏见的、不正确的更新。通过计算这种偏差并使用基线（baseline）提出一个原则性的解决方案，您将解决一个处于计算神经科学和现代强化学习理论交叉点的复杂挑战。",
            "id": "4057313",
            "problem": "考虑强化学习（RL）中一个由尖峰广义线性模型（GLM）实现的单神经元“行动者”（actor）。环境是一个单步上下文老虎机（contextual bandit），其二元状态 $S \\in \\{0,1\\}$ 在每次试验开始时独立抽取，满足 $\\mathbb{P}(S=1)=\\frac{1}{2}$ 和 $\\mathbb{P}(S=0)=\\frac{1}{2}$。在固定的试验时长 $[0,\\Delta]$ 内，该神经元接收一个恒定的输入 $x(S)$，其中 $x(0)=0$ 且 $x(1)=1$。以状态 $S$ 为条件，神经元发出一个尖峰序列 $y(t)$，该序列被建模为一个具有恒定条件强度的泊松过程\n$$\n\\lambda(S) = \\exp\\!\\big(w\\,x(S)\\big),\n$$\n其中 $w \\in \\mathbb{R}$ 是待学习的突触权重。令 $N=\\int_{0}^{\\Delta} y(t)\\,dt$ 表示试验中的总尖峰数。奖励 $R$ 在试验结束时根据 $R = r(S) + \\xi$ 生成，其中 $r(0)=0$，$r(1)=1$，$\\xi$ 是一个均值为零的随机变量，独立于 $S$ 和 $y(t)$，代表具有有限方差的外源性奖励噪声。\n\n一个常用的朴素三因子法则在每次试验结束时通过以下方式更新权重\n$$\n\\Delta w = \\eta\\,R \\int_{0}^{\\Delta} y(t)\\,x(S)\\,dt = \\eta\\,R\\,x(S)\\,N,\n$$\n其中 $\\eta0$ 是一个小的学习率。由于环境的奖励在因果上不依赖于神经元的尖峰发放（在给定 $S$ 的情况下，动作对 $R$ 没有影响），一个无偏的更新应具有零期望。然而，由于 $R$ 和 $y(t)$ 都依赖于 $S$，可能会出现混淆。\n\n从上述定义出发，计算朴素更新期望值 $\\mathbb{E}[\\Delta w]$ 的闭式解析表达式，该表达式是关于 $\\eta$、$\\Delta$ 和 $w$ 的函数。您的表达式必须是精确的，而非经验性的。然后，从概念上提出一个有原则的三因子更新修改方案，通过对奖励和发放率中依赖于状态的分量进行条件化来消除混淆偏差，并简要论证为何在当前假设下其期望值为零。将最终的朴素更新期望值表示为单个闭式解析表达式。无需四舍五入，也无需报告物理单位。",
            "solution": "该问题要求做两件事：首先，计算朴素三因子学习法则的期望值 $\\mathbb{E}[\\Delta w]$；其次，提出一个修正的、无偏的学习法则，并论证其期望为何为零。\n\n首先，我们计算朴素权重更新 $\\Delta w = \\eta\\,R\\,x(S)\\,N$ 的期望。该期望记为 $\\mathbb{E}[\\Delta w]$。\n$$\n\\mathbb{E}[\\Delta w] = \\mathbb{E}[\\eta\\,R\\,x(S)\\,N] = \\eta\\,\\mathbb{E}[R\\,x(S)\\,N]\n$$\n为了计算期望 $\\mathbb{E}[R\\,x(S)\\,N]$，我们通过对状态 $S$ 进行条件化来使用全期望定律。状态 $S$ 可以是 $0$ 或 $1$，每个的概率为 $\\frac{1}{2}$。\n$$\n\\mathbb{E}[R\\,x(S)\\,N] = \\mathbb{P}(S=0)\\,\\mathbb{E}[R\\,x(S)\\,N | S=0] + \\mathbb{P}(S=1)\\,\\mathbb{E}[R\\,x(S)\\,N | S=1]\n$$\n我们对每种情况分别评估条件期望。\n\n情况1：$S=0$\n给定 $S=0$，输入为 $x(S) = x(0) = 0$。\n期望内的项为 $R\\,x(0)\\,N = R \\cdot 0 \\cdot N = 0$。\n因此，条件期望为：\n$$\n\\mathbb{E}[R\\,x(S)\\,N | S=0] = \\mathbb{E}[0] = 0\n$$\n\n情况2：$S=1$\n给定 $S=1$，输入为 $x(S) = x(1) = 1$。奖励为 $R = r(1) + \\xi = 1 + \\xi$。\n期望内的项为 $R\\,x(1)\\,N = (1+\\xi) \\cdot 1 \\cdot N = (1+\\xi)N$。\n我们需要计算条件期望 $\\mathbb{E}[(1+\\xi)N | S=1]$。\n根据问题描述，奖励噪声 $\\xi$ 独立于状态 $S$ 和尖峰序列 $y(t)$。由于 $N = \\int_{0}^{\\Delta} y(t)\\,dt$，所以 $\\xi$ 也独立于 $N$。\n在 $S=1$ 的条件下，$N$ 和 $\\xi$ 是独立的随机变量。因此，我们可以将它们乘积的期望分开计算：\n$$\n\\mathbb{E}[(1+\\xi)N | S=1] = \\mathbb{E}[1+\\xi | S=1] \\cdot \\mathbb{E}[N | S=1]\n$$\n由于 $\\xi$ 独立于 $S$，$\\mathbb{E}[1+\\xi | S=1] = \\mathbb{E}[1+\\xi] = 1 + \\mathbb{E}[\\xi]$。我们已知 $\\xi$ 是一个均值为零的随机变量，所以 $\\mathbb{E}[\\xi] = 0$。\n$$\n\\mathbb{E}[1+\\xi | S=1] = 1 + 0 = 1\n$$\n接下来，我们求 $\\mathbb{E}[N | S=1]$。尖峰序列 $y(t)$ 被建模为具有恒定条件强度 $\\lambda(S)$ 的泊松过程。在时长 $\\Delta$ 内的总尖峰数 $N$ 是一个泊松分布的随机变量，其均值为 $\\lambda(S)\\Delta$。\n对于 $S=1$，强度为 $\\lambda(1) = \\exp(w\\,x(1)) = \\exp(w)$。\n所以，$N$ 的条件期望是：\n$$\n\\mathbb{E}[N | S=1] = \\lambda(1)\\Delta = \\Delta\\exp(w)\n$$\n结合 $S=1$ 情况下的这些结果：\n$$\n\\mathbb{E}[(1+\\xi)N | S=1] = 1 \\cdot \\Delta\\exp(w) = \\Delta\\exp(w)\n$$\n\n现在，我们将两种情况的结果代回到全期望定律中：\n$$\n\\mathbb{E}[R\\,x(S)\\,N] = \\left(\\frac{1}{2}\\right) \\cdot (0) + \\left(\\frac{1}{2}\\right) \\cdot (\\Delta\\exp(w)) = \\frac{\\Delta}{2}\\exp(w)\n$$\n最后，我们求出期望的权重更新：\n$$\n\\mathbb{E}[\\Delta w] = \\eta \\cdot \\frac{\\Delta}{2}\\exp(w) = \\frac{\\eta\\Delta}{2}\\exp(w)\n$$\n这就是朴素更新期望的闭式表达式。这个非零结果证实了该更新法则是带偏的。这种偏差的产生是因为状态 $S$ 充当了一个混淆变量，它通过 $r(S)$ 同时影响奖励，并通过 $x(S)$ 影响神经元活动，从而在奖励和尖峰发放之间产生了虚假的相关性。\n\n其次，我们提出了一个有原则的修改方案来消除这种偏差。在强化学习中，这通常通过从奖励项中减去一个依赖于状态的基线 $b(S)$ 来实现。更新则由奖励预测误差（RPE）$R - b(S)$ 驱动。修改后的更新法则是：\n$$\n\\Delta w' = \\eta (R - b(S)) x(S) N\n$$\n能够消除混淆并通常能减少方差的最优基线是在给定状态下的期望奖励，即 $b(S) = \\mathbb{E}[R|S]$。我们计算这个基线：\n$$\n\\mathbb{E}[R|S] = \\mathbb{E}[r(S) + \\xi | S] = r(S) + \\mathbb{E}[\\xi|S]\n$$\n由于 $\\xi$ 独立于 $S$ 且均值为零，所以 $\\mathbb{E}[\\xi|S] = \\mathbb{E}[\\xi] = 0$。因此，基线为 $b(S) = r(S)$。\n修改后的无偏更新法则是：\n$$\n\\Delta w' = \\eta (R - r(S)) x(S) N\n$$\n代入 $R = r(S) + \\xi$，RPE 项变为 $R - r(S) = (r(S) + \\xi) - r(S) = \\xi$。该法则简化为：\n$$\n\\Delta w' = \\eta\\,\\xi\\,x(S)\\,N\n$$\n为了证明该法则是无偏的，我们计算它的期望：\n$$\n\\mathbb{E}[\\Delta w'] = \\mathbb{E}[\\eta\\,\\xi\\,x(S)\\,N] = \\eta\\,\\mathbb{E}[\\xi\\,x(S)\\,N]\n$$\n问题陈述指出 $\\xi$ 独立于 $S$ 和 $y(t)$，因此也独立于 $N = \\int_{0}^{\\Delta} y(t)\\,dt$。这意味着 $\\xi$ 独立于乘积 $x(S)N$。因此我们可以将期望分开：\n$$\n\\mathbb{E}[\\xi\\,x(S)\\,N] = \\mathbb{E}[\\xi] \\cdot \\mathbb{E}[x(S)\\,N]\n$$\n由于 $\\mathbb{E}[\\xi] = 0$，整个表达式变为零：\n$$\n\\mathbb{E}[\\Delta w'] = \\eta \\cdot 0 \\cdot \\mathbb{E}[x(S)\\,N] = 0\n$$\n修改后更新的期望为零，这正确地反映了在该环境中神经元的尖峰发放动作对奖励没有因果影响的事实。通过对奖励中依赖于状态的分量 $r(S)$ 进行条件化，我们消除了混淆偏差。",
            "answer": "$$\n\\boxed{\\frac{\\eta\\Delta}{2}\\exp(w)}\n$$"
        }
    ]
}