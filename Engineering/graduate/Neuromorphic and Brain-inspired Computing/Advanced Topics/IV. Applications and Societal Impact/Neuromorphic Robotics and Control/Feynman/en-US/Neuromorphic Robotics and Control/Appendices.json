{
    "hands_on_practices": [
        {
            "introduction": "A primary motivation for neuromorphic engineering is the pursuit of extreme energy efficiency, mirroring the low power consumption of the biological brain. To design and optimize such systems, it is essential to understand the energetic cost of their most fundamental operation: the generation of a spike. This practice grounds the abstract concept of a spiking neuron in fundamental physics by guiding you to derive, from first principles, the energy required to charge the membrane capacitance to its firing threshold. By working through this exercise , you will connect the well-known formula for capacitor energy, $E = \\frac{1}{2} C V^{2}$, to the operational cost of a hardware neuron, providing a physical basis for analyzing power consumption in neuromorphic controllers.",
            "id": "4052832",
            "problem": "A legged robot uses an on-chip neuromorphic controller implemented with spiking neurons of the Leaky Integrate-and-Fire (LIF) type. Each neuron’s membrane is modeled as an ideal linear capacitor of capacitance $C_{\\mathrm{m}}$ from the membrane node to a reference ground. A refractory reset circuit fully discharges the membrane to ground between spikes, so immediately before each new upstroke the membrane voltage is $v(0)=0$. The upstroke that leads to threshold is produced by an ideal high-compliance transconductance stage that sources current into the membrane node from an energy reservoir, such that during the upstroke there are no resistive or leakage losses and all delivered electrical energy is stored in the electric field of the membrane capacitor. The threshold crossing occurs at membrane voltage $v(T)=V_{\\mathrm{p}}$, where $V_{\\mathrm{p}}$ is the spike’s peak (relative to ground) at the end of the upstroke.\n\nUsing only the capacitor constitutive law $q=C v$, the definition of current $i=\\mathrm{d}q/\\mathrm{d}t$, and the definition of instantaneous power $p=v\\,i$, derive from first principles a closed-form analytic expression for the energy drawn from the source during the upstroke from $v(0)=0$ to $v(T)=V_{\\mathrm{p}}$. Then evaluate this energy for a hardware neuron with $C_{\\mathrm{m}}=8\\times 10^{-12}$ and $V_{\\mathrm{p}}=0.12$. Express the final energy in Joules and round your answer to four significant figures.",
            "solution": "The problem is deemed valid as it is scientifically grounded in fundamental principles of circuit theory, well-posed with sufficient information for a unique solution, and stated using objective, unambiguous language. The given values are physically realistic for a neuromorphic hardware implementation.\n\nThe objective is to derive an expression for the total energy, $E$, drawn from the source during the charging of the neuron's membrane capacitance, $C_{\\mathrm{m}}$, from an initial voltage $v(0)=0$ to a final peak voltage $v(T)=V_{\\mathrm{p}}$. We are instructed to use only the fundamental relations for charge, current, and power. The problem states that during the upstroke, there are no resistive or leakage losses, and all delivered electrical energy is stored in the electric field of the membrane capacitor. This means the energy drawn from the source is equal to the energy stored in the capacitor.\n\nTotal energy $E$ is the time integral of instantaneous power $p(t)$ over the duration of the event, from $t=0$ to $t=T$.\n$$\nE = \\int_{0}^{T} p(t) \\, \\mathrm{d}t\n$$\nThe problem provides the definition of instantaneous power as $p(t) = v(t) i(t)$, where $v(t)$ is the instantaneous membrane voltage and $i(t)$ is the instantaneous current flowing into the membrane. Substituting this into the energy integral gives:\n$$\nE = \\int_{0}^{T} v(t) i(t) \\, \\mathrm{d}t\n$$\nNext, we must express the current $i(t)$ in terms of the voltage $v(t)$ and capacitance $C_{\\mathrm{m}}$. We are given the definition of current, $i(t) = \\frac{\\mathrm{d}q}{\\mathrm{d}t}$, and the capacitor's constitutive law, $q(t) = C_{\\mathrm{m}} v(t)$, where $C_{\\mathrm{m}}$ is a constant. By differentiating the constitutive law with respect to time, we find the current:\n$$\ni(t) = \\frac{\\mathrm{d}}{\\mathrm{d}t} \\left( C_{\\mathrm{m}} v(t) \\right) = C_{\\mathrm{m}} \\frac{\\mathrm{d}v}{\\mathrm{d}t}\n$$\nNow, substitute this expression for $i(t)$ back into the integral for energy:\n$$\nE = \\int_{0}^{T} v(t) \\left( C_{\\mathrm{m}} \\frac{\\mathrm{d}v}{\\mathrm{d}t} \\right) \\, \\mathrm{d}t\n$$\nSince $C_{\\mathrm{m}}$ is a constant, it can be moved outside the integral. We can also rearrange the terms within the integrand:\n$$\nE = C_{\\mathrm{m}} \\int_{0}^{T} v(t) \\frac{\\mathrm{d}v}{\\mathrm{d}t} \\, \\mathrm{d}t\n$$\nTo solve this integral, we perform a change of variables from time $t$ to voltage $v$. The differential element $\\mathrm{d}v$ is related to $\\mathrm{d}t$ by $\\mathrm{d}v = \\frac{\\mathrm{d}v}{\\mathrm{d}t} \\mathrm{d}t$. We must also change the limits of integration. At the lower limit $t=0$, the voltage is $v(0)=0$. At the upper limit $t=T$, the voltage is $v(T)=V_{\\mathrm{p}}$. The integral transforms as follows:\n$$\nE = C_{\\mathrm{m}} \\int_{v(0)}^{v(T)} v \\, \\mathrm{d}v = C_{\\mathrm{m}} \\int_{0}^{V_{\\mathrm{p}}} v \\, \\mathrm{d}v\n$$\nThis is a standard integral. The antiderivative of $v$ with respect to $v$ is $\\frac{1}{2}v^2$. Evaluating this antiderivative at the new limits:\n$$\nE = C_{\\mathrm{m}} \\left[ \\frac{1}{2} v^2 \\right]_{0}^{V_{\\mathrm{p}}}\n$$\n$$\nE = C_{\\mathrm{m}} \\left( \\frac{1}{2} V_{\\mathrm{p}}^2 - \\frac{1}{2} (0)^2 \\right)\n$$\nThis simplifies to the final closed-form analytic expression for the energy:\n$$\nE = \\frac{1}{2} C_{\\mathrm{m}} V_{\\mathrm{p}}^2\n$$\nThis is the well-known expression for the energy stored in an ideal linear capacitor charged to a voltage $V_{\\mathrm{p}}$.\n\nThe problem then asks to evaluate this energy for a hardware neuron with capacitance $C_{\\mathrm{m}} = 8 \\times 10^{-12}$ Farads and peak voltage $V_{\\mathrm{p}} = 0.12$ Volts. Substituting these values into our derived expression:\n$$\nE = \\frac{1}{2} (8 \\times 10^{-12} \\, \\text{F}) (0.12 \\, \\text{V})^2\n$$\n$$\nE = (4 \\times 10^{-12}) (0.0144) \\, \\text{J}\n$$\n$$\nE = 5.76 \\times 10^{-14} \\, \\text{J}\n$$\nThe problem requires the answer to be expressed in Joules and rounded to four significant figures. The calculated value $5.76 \\times 10^{-14}$ has three significant figures. To express this with four significant figures, we append a zero.\n$$\nE = 5.760 \\times 10^{-14} \\, \\text{J}\n$$",
            "answer": "$$\n\\boxed{5.760 \\times 10^{-14}}\n$$"
        },
        {
            "introduction": "While individual spiking neurons are the building blocks, the true power of neuromorphic computing emerges from the collective, learned behavior of large networks. A significant challenge in training Spiking Neural Networks (SNNs) is that the spiking mechanism, an all-or-none event, is described by a non-differentiable function like the Heaviside step function, precluding the direct use of standard gradient-based optimization. This exercise  introduces the powerful technique of surrogate gradients, which replaces the true derivative with a smooth approximation, enabling effective learning. You will derive a complete gradient expression for a synaptic weight using Backpropagation Through Time (BPTT), tackling a cornerstone problem in the field of learning in SNNs.",
            "id": "4052813",
            "problem": "A neuromorphic robot employs a single-neuron controller implemented as a Leaky Integrate-and-Fire (LIF) unit to regulate a motor command from event-based sensory input. The membrane potential $u[t]$ evolves in discrete time according to\n$$\nu[t+1] = \\lambda\\,u[t] + w\\,x[t] - V_{\\mathrm{th}}\\,z[t],\n$$\nwhere $t \\in \\{1,2,\\dots,T\\}$ is the discrete time index, $\\lambda \\in (0,1)$ is the leak factor, $w \\in \\mathbb{R}$ is a synaptic weight multiplying the presynaptic input $x[t] \\in \\mathbb{R}$, $V_{\\mathrm{th}} \\in \\mathbb{R}$ is the firing threshold, and $z[t] \\in \\{0,1\\}$ is the spike at time $t$ generated by the thresholding nonlinearity $z[t] = H(u[t] - V_{\\mathrm{th}})$, where $H(\\cdot)$ denotes the Heaviside step function. The initial condition $u[1] = u_{0}$ is independent of $w$. The control objective induces a differentiable loss $L(z[1],\\dots,z[T])$ that depends on the spike train $\\{z[t]\\}_{t=1}^{T}$.\n\nBecause the threshold nonlinearity $H(\\cdot)$ is not differentiable, a surrogate gradient is used. Assume a smooth pseudo-derivative\n$$\ng(u) = \\frac{1}{\\delta\\sqrt{\\pi}}\\,\\exp\\!\\left(-\\left(\\frac{u - V_{\\mathrm{th}}}{\\delta}\\right)^{2}\\right),\n$$\nwith smoothing parameter $\\delta > 0$, to approximate $\\frac{d}{du}H(u - V_{\\mathrm{th}})$ in Backpropagation Through Time (BPTT). For stability, adopt the detached-reset assumption in the state dynamics: when differentiating $u[t+1]$ with respect to $w$, treat $z[t]$ as a constant so that gradients do not flow through the reset term $-V_{\\mathrm{th}}\\,z[t]$.\n\nStarting from these definitions and assumptions, derive from first principles a closed-form analytic expression for the gradient $\\frac{dL}{dw}$ in terms of $\\lambda$, $x[1],\\dots,x[T]$, $u[1],\\dots,u[T]$, $V_{\\mathrm{th}}$, and $\\delta$. Your final answer must be a single analytical expression. No numerical evaluation is required.",
            "solution": "The objective is to derive a closed-form analytic expression for the gradient of the loss function $L$ with respect to the synaptic weight $w$, denoted as $\\frac{dL}{dw}$. The loss $L$ is a function of the spike train $\\{z[t]\\}_{t=1}^{T}$, where each spike $z[t]$ is generated from the membrane potential $u[t]$. The weight $w$ influences $u[t]$ for all $t > 1$.\n\nWe begin by applying the chain rule for total derivatives. The loss $L$ is an explicit function of $z[1], \\dots, z[T]$. A change in $w$ affects all $u[t]$ (for $t > 1$) and consequently all $z[t]$ (for $t>1$). The total derivative $\\frac{dL}{dw}$ is the sum of the influences of $w$ on $L$ through each spike $z[t]$:\n$$\n\\frac{dL}{dw} = \\sum_{t=1}^{T} \\frac{\\partial L}{\\partial z[t]} \\frac{dz[t]}{dw}\n$$\nThe term $\\frac{\\partial L}{\\partial z[t]}$ represents the error signal backpropagated from the loss function to the spike output at time $t$. We treat this as a known quantity.\n\nNext, we find the derivative of the spike train, $\\frac{dz[t]}{dw}$. Since $z[t]$ is a function of $u[t]$, which in turn depends on $w$, we apply the chain rule again:\n$$\n\\frac{dz[t]}{dw} = \\frac{dz[t]}{du[t]} \\frac{du[t]}{dw}\n$$\nThe term $\\frac{dz[t]}{du[t]}$ is the derivative of the Heaviside step function $z[t] = H(u[t] - V_{\\mathrm{th}})$. The problem specifies using the surrogate gradient $g(u[t])$ for this derivative:\n$$\n\\frac{dz[t]}{du[t]} \\approx g(u[t]) = \\frac{1}{\\delta\\sqrt{\\pi}}\\exp\\left(-\\left(\\frac{u[t] - V_{\\mathrm{th}}}{\\delta}\\right)^{2}\\right)\n$$\nThe term $\\frac{du[t]}{dw}$ is the total derivative of the membrane potential at time $t$ with respect to $w$. This term accounts for the recurrent dependencies in the network dynamics. Let us define $h[t] \\equiv \\frac{du[t]}{dw}$.\n\nSubstituting these into our main expression for $\\frac{dL}{dw}$, we get:\n$$\n\\frac{dL}{dw} = \\sum_{t=1}^{T} \\frac{\\partial L}{\\partial z[t]} g(u[t]) h[t]\n$$\nTo complete the derivation, we must find an expression for $h[t]$. We start with the state update equation for the membrane potential:\n$$\nu[t+1] = \\lambda u[t] + w x[t] - V_{\\mathrm{th}} z[t]\n$$\nWe differentiate this entire equation with respect to $w$ to find a recurrence relation for $h[t] = \\frac{du[t]}{dw}$:\n$$\n\\frac{d}{dw} u[t+1] = \\frac{d}{dw} \\left( \\lambda u[t] + w x[t] - V_{\\mathrm{th}} z[t] \\right)\n$$\n$$\nh[t+1] = \\lambda \\frac{du[t]}{dw} + \\frac{d(w x[t])}{dw} - V_{\\mathrm{th}} \\frac{dz[t]}{dw}\n$$\nThe problem provides the \"detached-reset\" assumption: \"when differentiating $u[t+1]$ with respect to $w$, treat $z[t]$ as a constant.\" Applying this assumption means we set the gradient path through the reset term to zero, i.e., $\\frac{d}{dw}(-V_{\\mathrm{th}} z[t]) = 0$. The derivative of the input term is $\\frac{d(w x[t])}{dw} = x[t]$, as $x[t]$ is independent of $w$. The derivative of the leak term is $\\lambda \\frac{du[t]}{dw} = \\lambda h[t]$.\nThis simplifies the recurrence for $h[t]$ to:\n$$\nh[t+1] = \\lambda h[t] + x[t]\n$$\nThis is a linear first-order non-homogeneous recurrence relation. To solve it, we need an initial condition. The problem states that the initial potential, $u[1] = u_0$, is independent of $w$. Therefore, its derivative with respect to $w$ is zero:\n$$\nh[1] = \\frac{du[1]}{dw} = 0\n$$\nWe can now unroll the recurrence to find a closed-form solution for $h[t]$:\n$h[1] = 0$\n$h[2] = \\lambda h[1] + x[1] = x[1]$\n$h[3] = \\lambda h[2] + x[2] = \\lambda x[1] + x[2]$\n$h[4] = \\lambda h[3] + x[3] = \\lambda(\\lambda x[1] + x[2]) + x[3] = \\lambda^2 x[1] + \\lambda x[2] + x[3]$\n\nBy induction, the general solution for $t \\geq 1$ is:\n$$\nh[t] = \\sum_{k=1}^{t-1} \\lambda^{t-1-k} x[k]\n$$\nNote that for $t=1$, this is an empty sum, which is correctly evaluated as $0$.\n\nFinally, we substitute the expression for $h[t]$ and $g(u[t])$ back into our equation for $\\frac{dL}{dw}$:\n$$\n\\frac{dL}{dw} = \\sum_{t=1}^{T} \\left( \\frac{\\partial L}{\\partial z[t]} \\cdot g(u[t]) \\cdot h[t] \\right)\n$$\n$$\n\\frac{dL}{dw} = \\sum_{t=1}^{T} \\left( \\frac{\\partial L}{\\partial z[t]} \\cdot \\frac{1}{\\delta\\sqrt{\\pi}}\\exp\\left(-\\left(\\frac{u[t] - V_{\\mathrm{th}}}{\\delta}\\right)^{2}\\right) \\cdot \\left( \\sum_{k=1}^{t-1} \\lambda^{t-1-k}x[k] \\right) \\right)\n$$\nThis is the final closed-form analytic expression for the gradient $\\frac{dL}{dw}$ in terms of the specified quantities.",
            "answer": "$$\\boxed{\\sum_{t=1}^{T} \\left( \\frac{\\partial L}{\\partial z[t]} \\frac{1}{\\delta\\sqrt{\\pi}}\\exp\\left(-\\left(\\frac{u[t] - V_{\\mathrm{th}}}{\\delta}\\right)^{2}\\right) \\sum_{k=1}^{t-1} \\lambda^{t-1-k}x[k] \\right)}$$"
        },
        {
            "introduction": "The ultimate test of a neuromorphic controller is its performance and robustness in a closed-loop system. Real-world neuromorphic hardware is subject to various sources of noise, with spike-time jitter—small random variations in the timing of spikes—being a critical factor affecting precision. This comprehensive practice  provides a full-stack analysis of this challenge, from the low-level physics of noise to its high-level impact on robotic control. You will derive how jitter in a temporal code translates into command error, and then propagate this error through the dynamics of a linear plant to quantify the resulting degradation in steady-state control performance, bridging the gap between neuron-level imperfections and system-level behavior.",
            "id": "4052833",
            "problem": "You are asked to rigorously analyze and simulate the effect of spike-time jitter on temporal code decoding error and propagate this error through a closed-loop neuromorphic control system for a simple linear plant. The encoding scheme is a scalar temporal code using time-to-first-spike, the plant is scalar and linear, and the controller is static state feedback. Your program must compute the steady-state closed-loop performance metric for a set of test cases.\n\nAssume the following foundations, which you must use as the basis for your derivation:\n\n- Temporal encoding uses a monotonic affine mapping from a scalar command $y$ (dimensionless) to a spike time $T(y)$ defined by $T(y) = T_0 - k y$, where $T_0$ is a constant offset in seconds and $k$ is a slope in seconds per unit command. You may set $T_0$ to an arbitrary constant because it cancels out in the decoding error for this mapping.\n- Observed spike times are corrupted by additive spike-time jitter $\\varepsilon$ with zero mean and variance $\\sigma_t^2$ (in $\\text{s}^2$), independently across neurons and control cycles. The jitter is modeled as an independent, identically distributed random variable with finite second moment.\n- Decoding uses unbiased inversion of the encoder for each spike time to estimate the command, and optionally averages across $N$ independent neurons that share the same encoder.\n- The plant is a continuous-time scalar Linear Time-Invariant (LTI) system with dynamics $\\dot{x}(t) = a x(t) + b u(t)$, where $x$ is the state (dimensionless), $u$ is the control input (dimensionless), $a$ has units of $\\text{s}^{-1}$, and $b$ is dimensionless. The control law is $u_k = -K x_k + r + \\tilde{e}_k$ at discrete sampling instants $t_k$, where $K$ is the feedback gain (dimensionless), $r$ is a constant reference (dimensionless), and $\\tilde{e}_k$ is the decoded command error injected by the temporal code at each control cycle.\n- The control loop operates at sampling period $h$ (in seconds) with zero-order hold, yielding a discrete-time system for analysis.\n\nYour task is to:\n\n1. Derive from first principles the variance of the decoded command error $\\tilde{e}_k$ as a function of the spike-time jitter variance $\\sigma_t^2$, the encoder slope $k$, and the number of neurons $N$ used for averaging.\n2. Discretize the continuous-time plant with zero-order hold to obtain discrete-time coefficients for the state update over sampling period $h$.\n3. Using the discrete-time closed-loop system under the static feedback control law, derive the steady-state variance of the state $x_k$ due solely to the injected decoded command error $\\tilde{e}_k$ when the closed-loop is mean-square stable.\n4. Implement a program that computes the steady-state standard deviation of $x_k$ (the square root of the steady-state variance) for each test case below, rounding the final standard deviation for each case to six decimal places.\n\nImportant details and requirements:\n\n- Angles are not involved; no angle unit specification is required.\n- All times must be treated in seconds, and all variances must be in the square of the unit of the variable they pertain to (e.g., $\\sigma_t^2$ in $\\text{s}^2$). The state $x$ and command $y$ are dimensionless.\n- You must express the final outputs as dimensionless standard deviations.\n- You must not assume any shortcut formulas not derived from the above foundations; your derivations must originate from the stated encoder, noise model, plant, and controller.\n- If the closed-loop system is not mean-square stable (that is, if the magnitude of the closed-loop scalar coefficient is not strictly less than one), the steady-state variance does not exist. For such a case, your program must output a Not-a-Number value for that test case. However, the provided test suite avoids instability.\n\nTest Suite:\n\nFor each test case, the parameters are $(a, b, h, K, k, \\sigma_t^2, N)$ in the specified units.\n\n- Case 1 (general stable case): $a = -2.0$ $\\text{s}^{-1}$, $b = 1.0$, $h = 0.02$ $\\text{s}$, $K = 1.0$, $k = 0.01$ $\\text{s}$ per unit command, $\\sigma_t^2 = 1.0 \\times 10^{-6}$ $\\text{s}^2$, $N = 1$.\n- Case 2 (neutral continuous-time plant): $a = 0.0$ $\\text{s}^{-1}$, $b = 1.0$, $h = 0.01$ $\\text{s}$, $K = 50.0$, $k = 0.01$ $\\text{s}$ per unit command, $\\sigma_t^2 = 1.0 \\times 10^{-6}$ $\\text{s}^2$, $N = 1$.\n- Case 3 (near-stability boundary): $a = 0.5$ $\\text{s}^{-1}$, $b = 1.0$, $h = 0.1$ $\\text{s}$, $K = 0.51$, $k = 0.02$ $\\text{s}$ per unit command, $\\sigma_t^2 = 1.0 \\times 10^{-6}$ $\\text{s}^2$, $N = 1$.\n- Case 4 (multi-neuron averaging): $a = -2.0$ $\\text{s}^{-1}$, $b = 1.0$, $h = 0.02$ $\\text{s}$, $K = 1.0$, $k = 0.01$ $\\text{s}$ per unit command, $\\sigma_t^2 = 1.0 \\times 10^{-6}$ $\\text{s}^2$, $N = 4$.\n\nFinal Output Format:\n\nYour program should produce a single line of output containing the steady-state standard deviations for the four cases, rounded to six decimal places, as a comma-separated list enclosed in square brackets, for example, \"[s1,s2,s3,s4]\". Each entry must be a float.",
            "solution": "We begin with the temporal encoding and decoding. The encoder maps a dimensionless command $y$ to a spike time by $T(y) = T_0 - k y$, with $T_0 \\in \\mathbb{R}$ in seconds and $k > 0$ in seconds per unit command. The observed spike time is corrupted by jitter modeled as an additive random variable $\\varepsilon$ with zero mean and variance $\\sigma_t^2$ in $\\text{s}^2$. Thus, the observed spike time is $t_{\\text{obs}} = T(y) + \\varepsilon = T_0 - k y + \\varepsilon$.\n\nThe decoder inverts the encoder affine map. For a single spike, the decoded command estimate is\n$$\n\\hat{y} = \\frac{T_0 - t_{\\text{obs}}}{k} = \\frac{T_0 - (T_0 - k y + \\varepsilon)}{k} = \\frac{k y - \\varepsilon}{k} = y - \\frac{\\varepsilon}{k}.\n$$\nTherefore, the decoding error for a single spike is\n$$\n\\tilde{e} = \\hat{y} - y = -\\frac{\\varepsilon}{k}.\n$$\nSince $\\varepsilon$ has zero mean and variance $\\sigma_t^2$, it follows that $\\tilde{e}$ has zero mean and variance\n$$\n\\sigma_y^2 = \\operatorname{Var}[\\tilde{e}] = \\frac{\\sigma_t^2}{k^2}.\n$$\n\nIf there are $N$ independent neurons emitting one spike each per control cycle, with independent jitter $\\varepsilon_i$ of the same distribution, and the decoder averages their estimates, the averaged decoded command estimate is\n$$\n\\hat{y}_{\\text{avg}} = \\frac{1}{N} \\sum_{i=1}^N \\left( y - \\frac{\\varepsilon_i}{k} \\right) = y - \\frac{1}{k} \\cdot \\frac{1}{N} \\sum_{i=1}^N \\varepsilon_i.\n$$\nThe averaged decoding error is\n$$\n\\tilde{e}_{\\text{avg}} = \\hat{y}_{\\text{avg}} - y = - \\frac{1}{k} \\cdot \\frac{1}{N} \\sum_{i=1}^N \\varepsilon_i.\n$$\nBy independence, the variance of the sum is the sum of variances, hence\n$$\n\\operatorname{Var}\\left[ \\frac{1}{N} \\sum_{i=1}^N \\varepsilon_i \\right] = \\frac{1}{N^2} \\cdot N \\cdot \\sigma_t^2 = \\frac{\\sigma_t^2}{N}.\n$$\nTherefore, the variance of the averaged decoding error is\n$$\n\\sigma_{y,\\text{avg}}^2 = \\operatorname{Var}[\\tilde{e}_{\\text{avg}}] = \\frac{1}{k^2} \\cdot \\frac{\\sigma_t^2}{N} = \\frac{\\sigma_t^2}{k^2 N}.\n$$\n\nWe now propagate this decoding error through the closed-loop system. The continuous-time plant is\n$$\n\\dot{x}(t) = a x(t) + b u(t),\n$$\nwith $a \\in \\mathbb{R}$ in $\\text{s}^{-1}$ and $b \\in \\mathbb{R}$ dimensionless. Under zero-order hold control with sampling period $h > 0$ seconds, the discrete-time equivalent over one sampling interval maps $x_k = x(t_k)$ to $x_{k+1} = x(t_{k+1})$ by\n$$\nx_{k+1} = A_d x_k + B_d u_k,\n$$\nwhere $A_d = e^{a h}$ and\n$$\nB_d =\n\\begin{cases}\nb \\dfrac{e^{a h} - 1}{a}, & \\text{if } a \\neq 0, \\\\\nb h, & \\text{if } a = 0.\n\\end{cases}\n$$\nThese expressions follow from solving the linear differential equation with constant input over an interval and integrating the impulse response $\\exp(a \\tau)$ from $0$ to $h$.\n\nThe static state feedback control law at sample $k$ is\n$$\nu_k = - K x_k + r + \\tilde{e}_k,\n$$\nwhere $K$ is a feedback gain (dimensionless), $r$ is a constant reference (dimensionless), and $\\tilde{e}_k$ is the decoded command error injected additively (dimensionless). Substituting, the closed-loop state update becomes\n$$\nx_{k+1} = (A_d - B_d K) x_k + B_d r + B_d \\tilde{e}_k.\n$$\nLet $A_{\\text{cl}} = A_d - B_d K$ denote the scalar closed-loop coefficient. The term $B_d r$ contributes to the mean state but not to the variance induced by $\\tilde{e}_k$. Assuming the sequence $\\{ \\tilde{e}_k \\}$ is independent across $k$, zero mean, with variance $\\sigma_y^2$ (or $\\sigma_{y,\\text{avg}}^2$ when averaging across $N$ neurons), and independent of $x_k$, the scalar variance propagation for an AutoRegressive model of order $1$ (AR(1)) with input noise is\n$$\n\\operatorname{Var}[x_{k+1}] = A_{\\text{cl}}^2 \\operatorname{Var}[x_k] + B_d^2 \\sigma_y^2.\n$$\nIn steady state for a mean-square stable system, the variance satisfies the discrete Lyapunov equation\n$$\n\\Sigma = A_{\\text{cl}}^2 \\Sigma + B_d^2 \\sigma_y^2,\n$$\nwith solution\n$$\n\\Sigma = \\frac{B_d^2 \\sigma_y^2}{1 - A_{\\text{cl}}^2},\n$$\nprovided $\\lvert A_{\\text{cl}} \\rvert < 1$. The steady-state standard deviation is $\\sqrt{\\Sigma}$.\n\nCombining the earlier decoding error variance with this propagation:\n\n- For $N = 1$, $\\sigma_y^2 = \\dfrac{\\sigma_t^2}{k^2}$.\n- For general $N$, $\\sigma_y^2 = \\dfrac{\\sigma_t^2}{k^2 N}$.\n\nHence, the steady-state variance is\n$$\n\\Sigma = \\frac{B_d^2}{1 - (A_d - B_d K)^2} \\cdot \\frac{\\sigma_t^2}{k^2 N},\n$$\nand the closed-loop performance metric requested is the steady-state standard deviation\n$$\n\\sqrt{\\Sigma} = \\sqrt{ \\frac{B_d^2}{1 - (A_d - B_d K)^2} \\cdot \\frac{\\sigma_t^2}{k^2 N} }.\n$$\n\nAlgorithmic steps for each test case with parameters $(a, b, h, K, k, \\sigma_t^2, N)$:\n\n1. Compute $A_d = e^{a h}$.\n2. Compute $B_d$ using $B_d = b \\dfrac{e^{a h} - 1}{a}$ if $a \\neq 0$, else $B_d = b h$.\n3. Compute $A_{\\text{cl}} = A_d - B_d K$.\n4. Compute the decoded command error variance $\\sigma_y^2 = \\dfrac{\\sigma_t^2}{k^2 N}$.\n5. If $\\lvert A_{\\text{cl}} \\rvert \\geq 1$, the steady-state variance does not exist; return a Not-a-Number for this case. Otherwise, compute\n$$\n\\Sigma = \\frac{B_d^2 \\sigma_y^2}{1 - A_{\\text{cl}}^2},\n$$\nand output $\\sqrt{\\Sigma}$ rounded to six decimal places.\n\nApplying these steps to the provided test suite will produce four floats representing the steady-state standard deviation of the state $x_k$ due to spike-time jitter propagated through the temporal decoder and the closed-loop dynamics. The final program aggregates these into a single output line in the specified format.",
            "answer": "```python\nimport numpy as np\n\ndef steady_state_std(a, b, h, K, k, sigma_t2, N):\n    # Discrete-time coefficients under zero-order hold\n    Ad = np.exp(a * h)\n    if a != 0.0:\n        Bd = b * (Ad - 1.0) / a\n    else:\n        Bd = b * h\n\n    Acl = Ad - Bd * K\n\n    # Decoded command error variance with N-neuron averaging\n    sigma_y2 = sigma_t2 / (k ** 2 * N)\n\n    # Check mean-square stability\n    if abs(Acl) >= 1.0:\n        return float('nan')\n\n    denom = 1.0 - (Acl ** 2)\n    Sigma = (Bd ** 2) * sigma_y2 / denom\n    std = np.sqrt(Sigma)\n    return std\n\ndef solve():\n    # Define the test cases from the problem statement:\n    # Each case is (a, b, h, K, k, sigma_t^2, N)\n    test_cases = [\n        (-2.0, 1.0, 0.02, 1.0, 0.01, 1.0e-6, 1),\n        ( 0.0, 1.0, 0.01, 50.0, 0.01, 1.0e-6, 1),\n        ( 0.5, 1.0, 0.10, 0.51, 0.02, 1.0e-6, 1),\n        (-2.0, 1.0, 0.02, 1.0, 0.01, 1.0e-6, 4),\n    ]\n\n    results = []\n    for case in test_cases:\n        a, b, h, K, k, sigma_t2, N = case\n        std = steady_state_std(a, b, h, K, k, sigma_t2, N)\n        # Round to six decimal places as required\n        if np.isnan(std):\n            results.append(\"nan\")\n        else:\n            results.append(f\"{std:.6f}\")\n\n    # Final print statement in the exact required format: single line, comma-separated, enclosed in brackets\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}