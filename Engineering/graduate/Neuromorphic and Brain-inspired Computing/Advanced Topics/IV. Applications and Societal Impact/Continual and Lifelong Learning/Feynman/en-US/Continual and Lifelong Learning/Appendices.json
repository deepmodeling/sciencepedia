{
    "hands_on_practices": [
        {
            "introduction": "A foundational challenge in continual learning is catastrophic forgetting, where a model's performance on previously learned tasks degrades severely upon learning a new one. This exercise provides a hands-on analytical demonstration of this phenomenon. By deriving the performance of a simple linear model under both a sequential (\"blocked\") and a joint (\"interleaved\") training schedule, you will quantify the precise benefit of rehearsal-based strategies in mitigating interference between tasks . This practice builds a crucial intuition for why naive sequential training fails and serves as a baseline for evaluating more sophisticated methods.",
            "id": "4041098",
            "problem": "Consider a continual learning setup in neuromorphic and brain-inspired computing in which a single synaptic parameter models a pair of tasks ($2$ tasks) as linear regressions with shared inputs. Let the input $x$ be drawn independently and identically distributed from a zero-mean Gaussian with variance $\\sigma_{x}^{2}$, that is $x \\sim \\mathcal{N}(0,\\sigma_{x}^{2})$. Task $t \\in \\{1,2\\}$ is governed by a generative linear model $y_{t} = w_{t} x + \\epsilon_{t}$, where the task-specific true parameter $w_{t} \\in \\mathbb{R}$ is fixed, and the noise is independent and zero-mean Gaussian with variance $\\sigma_{\\epsilon}^{2}$, that is $\\epsilon_{t} \\sim \\mathcal{N}(0,\\sigma_{\\epsilon}^{2})$, independent of $x$ and of $\\epsilon_{t'}$ for $t' \\neq t$. The learner is a one-parameter linear regressor $f(x; w) = w x$ trained with the squared error loss using infinite data and sufficiently small learning rate so that training converges to the minimizer of the expected loss.\n\nDefine the blocked schedule as training to convergence on task $1$ and then training to convergence on task $2$ (thereby overwriting the parameter to the minimizer for task $2$). Define the interleaved schedule as training to convergence on a mixture that presents examples from task $1$ and task $2$ with equal probability (thereby minimizing the equally weighted average expected loss).\n\nTo measure performance under continual learning, define the per-task Mean Squared Error (MSE) of parameter $w$ on task $t$ as $\\mathrm{MSE}_{t}(w) = \\mathbb{E}\\!\\left[(w x - y_{t})^{2}\\right]$, and define a normalized accuracy for regression as $a_{t}(w) = 1 - \\frac{\\mathrm{MSE}_{t}(w)}{\\mathrm{MSE}_{t}(0)}$, where $\\mathrm{MSE}_{t}(0) = \\mathbb{E}\\!\\left[(0 - y_{t})^{2}\\right]$ is the MSE of the zero predictor and serves as a baseline. Let the average accuracy across tasks be $A(w) = \\frac{1}{2}\\left(a_{1}(w) + a_{2}(w)\\right)$.\n\nStarting only from these definitions and standard properties of Gaussian random variables and least squares regression, derive the closed-form expressions of the converged parameters under the blocked and interleaved schedules, and compute the benefit of interleaving over blocking, defined as $B = A(w_{\\mathrm{I}}) - A(w_{\\mathrm{B}})$, where $w_{\\mathrm{B}}$ and $w_{\\mathrm{I}}$ denote the respective converged parameters. Express your final answer for $B$ as a single closed-form analytic expression in terms of $w_{1}$, $w_{2}$, $\\sigma_{x}^{2}$, and $\\sigma_{\\epsilon}^{2}$. No numerical evaluation is required, and no rounding is needed.",
            "solution": "The problem asks for the benefit of an interleaved training schedule over a blocked training schedule, $B = A(w_{\\mathrm{I}}) - A(w_{\\mathrm{B}})$, in a continual learning context with two linear regression tasks. To find this, we must first derive the final parameters under each schedule, $w_{\\mathrm{I}}$ and $w_{\\mathrm{B}}$, and then compute the average accuracies $A(w_{\\mathrm{I}})$ and $A(w_{\\mathrm{B}})$.\n\nFirst, we derive a general expression for the per-task Mean Squared Error, $\\mathrm{MSE}_{t}(w)$.\nThe generative model for task $t$ is $y_{t} = w_{t} x + \\epsilon_{t}$. The learner's prediction is $w x$. The MSE for task $t$ is the expected squared error:\n$$\n\\mathrm{MSE}_{t}(w) = \\mathbb{E}\\!\\left[(w x - y_{t})^{2}\\right]\n$$\nSubstituting the expression for $y_t$:\n$$\n\\mathrm{MSE}_{t}(w) = \\mathbb{E}\\!\\left[(w x - (w_{t} x + \\epsilon_{t}))^{2}\\right] = \\mathbb{E}\\!\\left[((w - w_{t})x - \\epsilon_{t})^{2}\\right]\n$$\nExpanding the square:\n$$\n\\mathrm{MSE}_{t}(w) = \\mathbb{E}\\!\\left[(w - w_{t})^{2}x^{2} - 2(w - w_{t})x\\epsilon_{t} + \\epsilon_{t}^{2}\\right]\n$$\nUsing the linearity of expectation, we get:\n$$\n\\mathrm{MSE}_{t}(w) = (w - w_{t})^{2}\\mathbb{E}[x^{2}] - 2(w - w_{t})\\mathbb{E}[x\\epsilon_{t}] + \\mathbb{E}[\\epsilon_{t}^{2}]\n$$\nWe are given the properties of the random variables:\n1.  $x \\sim \\mathcal{N}(0, \\sigma_{x}^{2})$, so $\\mathbb{E}[x] = 0$ and $\\mathrm{Var}(x) = \\mathbb{E}[x^{2}] - (\\mathbb{E}[x])^{2} = \\mathbb{E}[x^{2}] = \\sigma_{x}^{2}$.\n2.  $\\epsilon_{t} \\sim \\mathcal{N}(0, \\sigma_{\\epsilon}^{2})$, so $\\mathbb{E}[\\epsilon_{t}] = 0$ and $\\mathbb{E}[\\epsilon_{t}^{2}] = \\sigma_{\\epsilon}^{2}$.\n3.  $x$ and $\\epsilon_t$ are independent, so $\\mathbb{E}[x\\epsilon_{t}] = \\mathbb{E}[x]\\mathbb{E}[\\epsilon_{t}] = 0 \\cdot 0 = 0$.\n\nSubstituting these into the expression for $\\mathrm{MSE}_{t}(w)$:\n$$\n\\mathrm{MSE}_{t}(w) = (w - w_{t})^{2}\\sigma_{x}^{2} + \\sigma_{\\epsilon}^{2}\n$$\nThis is the general expression for the MSE of a parameter $w$ on task $t$.\n\nNext, we find the parameters that minimize the relevant loss functions.\nThe parameter $w_t^*$ that minimizes the loss for a single task $t$ is found by minimizing $\\mathrm{MSE}_{t}(w)$. Since $\\mathrm{MSE}_{t}(w)$ is a quadratic in $w$ with a positive leading coefficient, its minimum occurs at the vertex. We can find this by setting its derivative to zero:\n$$\n\\frac{d}{dw} \\mathrm{MSE}_{t}(w) = \\frac{d}{dw} \\left[(w - w_{t})^{2}\\sigma_{x}^{2} + \\sigma_{\\epsilon}^{2}\\right] = 2(w - w_{t})\\sigma_{x}^{2} = 0\n$$\nAssuming $\\sigma_{x}^{2} \\neq 0$, the minimizer is $w = w_{t}$. So, the optimal parameter for task $t$ is $w_{t}^{*} = w_{t}$.\n\nNow, we determine the converged parameters for each schedule.\nFor the **blocked schedule**, the learner first converges on task $1$, yielding the parameter $w_1^* = w_1$. Then, it converges on task $2$, overwriting the previous parameter. The final parameter is the one that minimizes the loss for task $2$, which is $w_2^* = w_2$. Therefore:\n$$\nw_{\\mathrm{B}} = w_{2}\n$$\nFor the **interleaved schedule**, the learner minimizes the equally weighted average expected loss across both tasks:\n$$\nL_{\\mathrm{I}}(w) = \\frac{1}{2}\\left(\\mathrm{MSE}_{1}(w) + \\mathrm{MSE}_{2}(w)\\right) = \\frac{1}{2}\\left[((w - w_{1})^{2}\\sigma_{x}^{2} + \\sigma_{\\epsilon}^{2}) + ((w - w_{2})^{2}\\sigma_{x}^{2} + \\sigma_{\\epsilon}^{2})\\right]\n$$\n$$\nL_{\\mathrm{I}}(w) = \\frac{1}{2}\\sigma_{x}^{2}\\left((w - w_{1})^{2} + (w - w_{2})^{2}\\right) + \\sigma_{\\epsilon}^{2}\n$$\nTo find the minimizer $w_{\\mathrm{I}}$, we set the derivative of $L_{\\mathrm{I}}(w)$ to zero:\n$$\n\\frac{d L_{\\mathrm{I}}(w)}{dw} = \\frac{1}{2}\\sigma_{x}^{2}\\left(2(w - w_{1}) + 2(w - w_{2})\\right) = \\sigma_{x}^{2}(2w - w_{1} - w_{2}) = 0\n$$\nThis gives the converged parameter for the interleaved schedule:\n$$\nw_{\\mathrm{I}} = \\frac{w_{1} + w_{2}}{2}\n$$\n\nNext, we derive the expression for the normalized accuracy $a_{t}(w)$.\n$$\na_{t}(w) = 1 - \\frac{\\mathrm{MSE}_{t}(w)}{\\mathrm{MSE}_{t}(0)}\n$$\nThe baseline MSE is $\\mathrm{MSE}_{t}(0) = (0 - w_{t})^{2}\\sigma_{x}^{2} + \\sigma_{\\epsilon}^{2} = w_{t}^{2}\\sigma_{x}^{2} + \\sigma_{\\epsilon}^{2}$.\nSubstituting the expressions for $\\mathrm{MSE}_{t}(w)$ and $\\mathrm{MSE}_{t}(0)$:\n$$\na_{t}(w) = 1 - \\frac{(w - w_{t})^{2}\\sigma_{x}^{2} + \\sigma_{\\epsilon}^{2}}{w_{t}^{2}\\sigma_{x}^{2} + \\sigma_{\\epsilon}^{2}} = \\frac{(w_{t}^{2}\\sigma_{x}^{2} + \\sigma_{\\epsilon}^{2}) - ((w - w_{t})^{2}\\sigma_{x}^{2} + \\sigma_{\\epsilon}^{2})}{w_{t}^{2}\\sigma_{x}^{2} + \\sigma_{\\epsilon}^{2}}\n$$\n$$\na_{t}(w) = \\frac{\\sigma_{x}^{2}\\left(w_{t}^{2} - (w - w_{t})^{2}\\right)}{w_{t}^{2}\\sigma_{x}^{2} + \\sigma_{\\epsilon}^{2}} = \\frac{\\sigma_{x}^{2}(w_t - (w-w_t))(w_t + (w-w_t))}{w_{t}^{2}\\sigma_{x}^{2} + \\sigma_{\\epsilon}^{2}} = \\frac{\\sigma_{x}^{2}(2w_t-w)w}{w_{t}^{2}\\sigma_{x}^{2} + \\sigma_{\\epsilon}^{2}}\n$$\n\nNow we compute the average accuracies $A(w_{\\mathrm{B}})$ and $A(w_{\\mathrm{I}})$.\nThe average accuracy is $A(w) = \\frac{1}{2}(a_{1}(w) + a_{2}(w))$.\nFor the **blocked schedule**, $w_{\\mathrm{B}} = w_{2}$:\n$$\nA(w_{\\mathrm{B}}) = \\frac{1}{2}\\left(a_{1}(w_{2}) + a_{2}(w_{2})\\right)\n$$\n$$\na_{1}(w_{2}) = \\frac{\\sigma_{x}^{2}w_{2}(2w_{1} - w_{2})}{w_{1}^{2}\\sigma_{x}^{2} + \\sigma_{\\epsilon}^{2}}\n$$\n$$\na_{2}(w_{2}) = \\frac{\\sigma_{x}^{2}w_{2}(2w_{2} - w_{2})}{w_{2}^{2}\\sigma_{x}^{2} + \\sigma_{\\epsilon}^{2}} = \\frac{\\sigma_{x}^{2}w_{2}^{2}}{w_{2}^{2}\\sigma_{x}^{2} + \\sigma_{\\epsilon}^{2}}\n$$\n\nFor the **interleaved schedule**, $w_{\\mathrm{I}} = \\frac{w_1+w_2}{2}$:\n$$\nA(w_{\\mathrm{I}}) = \\frac{1}{2}\\left(a_{1}(w_{\\mathrm{I}}) + a_{2}(w_{\\mathrm{I}})\\right)\n$$\n$$\na_{1}(w_{\\mathrm{I}}) = \\frac{\\sigma_{x}^{2}w_{\\mathrm{I}}(2w_{1} - w_{\\mathrm{I}})}{w_{1}^{2}\\sigma_{x}^{2} + \\sigma_{\\epsilon}^{2}} = \\frac{\\sigma_{x}^{2}(\\frac{w_{1}+w_{2}}{2})(2w_{1} - \\frac{w_{1}+w_{2}}{2})}{w_{1}^{2}\\sigma_{x}^{2} + \\sigma_{\\epsilon}^{2}} = \\frac{\\sigma_{x}^{2}(\\frac{w_{1}+w_{2}}{2})(\\frac{3w_{1}-w_{2}}{2})}{w_{1}^{2}\\sigma_{x}^{2} + \\sigma_{\\epsilon}^{2}} = \\frac{\\sigma_{x}^{2}(3w_{1}^{2} + 2w_{1}w_{2} - w_{2}^{2})}{4(w_{1}^{2}\\sigma_{x}^{2} + \\sigma_{\\epsilon}^{2})}\n$$\n$$\na_{2}(w_{\\mathrm{I}}) = \\frac{\\sigma_{x}^{2}w_{\\mathrm{I}}(2w_{2} - w_{\\mathrm{I}})}{w_{2}^{2}\\sigma_{x}^{2} + \\sigma_{\\epsilon}^{2}} = \\frac{\\sigma_{x}^{2}(\\frac{w_{1}+w_{2}}{2})(2w_{2} - \\frac{w_{1}+w_{2}}{2})}{w_{2}^{2}\\sigma_{x}^{2} + \\sigma_{\\epsilon}^{2}} = \\frac{\\sigma_{x}^{2}(\\frac{w_{1}+w_{2}}{2})(\\frac{3w_{2}-w_{1}}{2})}{w_{2}^{2}\\sigma_{x}^{2} + \\sigma_{\\epsilon}^{2}} = \\frac{\\sigma_{x}^{2}(3w_{2}^{2} + 2w_{1}w_{2} - w_{1}^{2})}{4(w_{2}^{2}\\sigma_{x}^{2} + \\sigma_{\\epsilon}^{2})}\n$$\n\nFinally, we calculate the benefit $B = A(w_{\\mathrm{I}}) - A(w_{\\mathrm{B}}) = \\frac{1}{2}\\left[ (a_{1}(w_{\\mathrm{I}}) - a_{1}(w_{\\mathrm{B}})) + (a_{2}(w_{\\mathrm{I}}) - a_{2}(w_{\\mathrm{B}})) \\right]$.\nLet's compute the differences for each task.\nFor task $1$:\n$$\na_{1}(w_{\\mathrm{I}}) - a_{1}(w_{B}) = \\frac{\\sigma_{x}^{2}}{w_{1}^{2}\\sigma_{x}^{2} + \\sigma_{\\epsilon}^{2}} \\left[ \\frac{3w_{1}^{2} + 2w_{1}w_{2} - w_{2}^{2}}{4} - w_{2}(2w_{1} - w_{2}) \\right]\n$$\n$$\n= \\frac{\\sigma_{x}^{2}}{w_{1}^{2}\\sigma_{x}^{2} + \\sigma_{\\epsilon}^{2}} \\left[ \\frac{3w_{1}^{2} + 2w_{1}w_{2} - w_{2}^{2} - 8w_{1}w_{2} + 4w_{2}^{2}}{4} \\right] = \\frac{\\sigma_{x}^{2}(3w_{1}^{2} - 6w_{1}w_{2} + 3w_{2}^{2})}{4(w_{1}^{2}\\sigma_{x}^{2} + \\sigma_{\\epsilon}^{2})} = \\frac{3\\sigma_{x}^{2}(w_{1} - w_{2})^{2}}{4(w_{1}^{2}\\sigma_{x}^{2} + \\sigma_{\\epsilon}^{2})}\n$$\nFor task $2$:\n$$\na_{2}(w_{\\mathrm{I}}) - a_{2}(w_{B}) = \\frac{\\sigma_{x}^{2}}{w_{2}^{2}\\sigma_{x}^{2} + \\sigma_{\\epsilon}^{2}} \\left[ \\frac{3w_{2}^{2} + 2w_{1}w_{2} - w_{1}^{2}}{4} - w_{2}^{2} \\right]\n$$\n$$\n= \\frac{\\sigma_{x}^{2}}{w_{2}^{2}\\sigma_{x}^{2} + \\sigma_{\\epsilon}^{2}} \\left[ \\frac{3w_{2}^{2} + 2w_{1}w_{2} - w_{1}^{2} - 4w_{2}^{2}}{4} \\right] = \\frac{\\sigma_{x}^{2}(-w_{2}^{2} + 2w_{1}w_{2} - w_{1}^{2})}{4(w_{2}^{2}\\sigma_{x}^{2} + \\sigma_{\\epsilon}^{2})} = \\frac{-\\sigma_{x}^{2}(w_{1} - w_{2})^{2}}{4(w_{2}^{2}\\sigma_{x}^{2} + \\sigma_{\\epsilon}^{2})}\n$$\nNow, we compute $B$:\n$$\nB = \\frac{1}{2} \\left[ \\frac{3\\sigma_{x}^{2}(w_{1} - w_{2})^{2}}{4(w_{1}^{2}\\sigma_{x}^{2} + \\sigma_{\\epsilon}^{2})} - \\frac{\\sigma_{x}^{2}(w_{1} - w_{2})^{2}}{4(w_{2}^{2}\\sigma_{x}^{2} + \\sigma_{\\epsilon}^{2})} \\right]\n$$\nFactoring out common terms:\n$$\nB = \\frac{\\sigma_{x}^{2}(w_{1} - w_{2})^{2}}{8} \\left[ \\frac{3}{w_{1}^{2}\\sigma_{x}^{2} + \\sigma_{\\epsilon}^{2}} - \\frac{1}{w_{2}^{2}\\sigma_{x}^{2} + \\sigma_{\\epsilon}^{2}} \\right]\n$$\nCombining the terms in the bracket over a common denominator:\n$$\nB = \\frac{\\sigma_{x}^{2}(w_{1} - w_{2})^{2}}{8} \\left[ \\frac{3(w_{2}^{2}\\sigma_{x}^{2} + \\sigma_{\\epsilon}^{2}) - (w_{1}^{2}\\sigma_{x}^{2} + \\sigma_{\\epsilon}^{2})}{(w_{1}^{2}\\sigma_{x}^{2} + \\sigma_{\\epsilon}^{2})(w_{2}^{2}\\sigma_{x}^{2} + \\sigma_{\\epsilon}^{2})} \\right]\n$$\nSimplifying the numerator in the bracket:\n$$\n3w_{2}^{2}\\sigma_{x}^{2} + 3\\sigma_{\\epsilon}^{2} - w_{1}^{2}\\sigma_{x}^{2} - \\sigma_{\\epsilon}^{2} = (3w_{2}^{2} - w_{1}^{2})\\sigma_{x}^{2} + 2\\sigma_{\\epsilon}^{2}\n$$\nThus, the final expression for the benefit $B$ is:\n$$\nB = \\frac{\\sigma_{x}^{2}(w_{1} - w_{2})^{2} \\left( (3w_{2}^{2} - w_{1}^{2})\\sigma_{x}^{2} + 2\\sigma_{\\epsilon}^{2} \\right)}{8(w_{1}^{2}\\sigma_{x}^{2} + \\sigma_{\\epsilon}^{2})(w_{2}^{2}\\sigma_{x}^{2} + \\sigma_{\\epsilon}^{2})}\n$$",
            "answer": "$$ \\boxed{ \\frac{\\sigma_{x}^{2}(w_{1} - w_{2})^{2} \\left( (3w_{2}^{2} - w_{1}^{2})\\sigma_{x}^{2} + 2\\sigma_{\\epsilon}^{2} \\right)}{8(w_{1}^{2}\\sigma_{x}^{2} + \\sigma_{\\epsilon}^{2})(w_{2}^{2}\\sigma_{x}^{2} + \\sigma_{\\epsilon}^{2})} } $$"
        },
        {
            "introduction": "Having established the problem of catastrophic forgetting, we now explore a powerful, brain-inspired solution: Elastic Weight Consolidation (EWC). EWC protects knowledge by penalizing changes to synaptic parameters deemed important for previous tasks. This exercise delves into the core mechanism of EWC by asking you to find the optimal balance between learning a new task and preserving an old one . By calculating the ideal trade-off parameter $\\lambda$ in a well-defined quadratic loss landscape, you will gain a deeper understanding of how regularization-based methods manage the stability-plasticity dilemma at the synaptic level.",
            "id": "4041100",
            "problem": "Consider a two-parameter neuromorphic model with parameter vector $\\boldsymbol{\\theta} \\in \\mathbb{R}^{2}$ trained sequentially on two tasks. Assume synaptic consolidation is enforced by Elastic Weight Consolidation (EWC), where the penalty is scaled by a trade-off parameter $\\lambda \\geq 0$. Near each task’s optimum, approximate the expected negative log-likelihood by a quadratic form whose curvature is given by the empirical Fisher information matrix. Concretely, for task $i \\in \\{1,2\\}$ with optimum $\\boldsymbol{\\theta}_{i}$ and Fisher information matrix $\\mathbf{F}_{i}$, define the local quadratic loss\n$$\nL_{i}(\\boldsymbol{\\theta}) = \\frac{1}{2}(\\boldsymbol{\\theta} - \\boldsymbol{\\theta}_{i})^{\\top}\\mathbf{F}_{i}(\\boldsymbol{\\theta} - \\boldsymbol{\\theta}_{i}).\n$$\nDuring training on task $2$, the EWC objective is\n$$\nJ(\\boldsymbol{\\theta};\\lambda) = \\frac{1}{2}(\\boldsymbol{\\theta} - \\boldsymbol{\\theta}_{2})^{\\top}\\mathbf{F}_{2}(\\boldsymbol{\\theta} - \\boldsymbol{\\theta}_{2}) + \\frac{\\lambda}{2}(\\boldsymbol{\\theta} - \\boldsymbol{\\theta}_{1})^{\\top}\\mathbf{F}_{1}(\\boldsymbol{\\theta} - \\boldsymbol{\\theta}_{1}),\n$$\nwhich yields the consolidated parameter $\\boldsymbol{\\theta}^{\\star}(\\lambda) = \\arg\\min_{\\boldsymbol{\\theta}} J(\\boldsymbol{\\theta};\\lambda)$. Assume accuracies for the tasks are equally important and, to leading order in the quadratic approximation, the average normalized accuracy is maximized by minimizing the average quadratic loss\n$$\nL_{\\mathrm{avg}}(\\lambda) = \\frac{1}{2}L_{1}(\\boldsymbol{\\theta}^{\\star}(\\lambda)) + \\frac{1}{2}L_{2}(\\boldsymbol{\\theta}^{\\star}(\\lambda)).\n$$\nLet the optima and Fisher matrices be\n$$\n\\boldsymbol{\\theta}_{1} = \\begin{pmatrix}1 \\\\ 0\\end{pmatrix}, \\quad \\boldsymbol{\\theta}_{2} = \\begin{pmatrix}0 \\\\ 2\\end{pmatrix}, \\quad \\mathbf{F}_{1} = \\begin{pmatrix}4 & 0 \\\\ 0 & 1\\end{pmatrix}, \\quad \\mathbf{F}_{2} = \\begin{pmatrix}1 & 0 \\\\ 0 & 9\\end{pmatrix}.\n$$\nUnder these assumptions, determine the value of the trade-off parameter $\\lambda$ that maximizes the average normalized accuracy across the two tasks. Express your final answer as a dimensionless number. No rounding is required; provide the exact value.",
            "solution": "To find the optimal trade-off parameter $\\lambda$, we must first find the consolidated parameter $\\boldsymbol{\\theta}^{\\star}(\\lambda)$ by minimizing the EWC objective $J(\\boldsymbol{\\theta};\\lambda)$. Then, we express the average loss $L_{\\mathrm{avg}}(\\lambda)$ as a function of $\\lambda$ and find the value of $\\lambda$ that minimizes it.\n\n1.  **Find the consolidated parameter $\\boldsymbol{\\theta}^{\\star}(\\lambda)$**\n    The objective $J(\\boldsymbol{\\theta};\\lambda)$ is a quadratic function of $\\boldsymbol{\\theta}$. Its minimum can be found by setting its gradient with respect to $\\boldsymbol{\\theta}$ to zero:\n    $$\n    \\nabla_{\\boldsymbol{\\theta}} J(\\boldsymbol{\\theta};\\lambda) = \\mathbf{F}_{2}(\\boldsymbol{\\theta} - \\boldsymbol{\\theta}_{2}) + \\lambda \\mathbf{F}_{1}(\\boldsymbol{\\theta} - \\boldsymbol{\\theta}_{1}) = \\mathbf{0}\n    $$\n    Solving for $\\boldsymbol{\\theta}$:\n    $$\n    (\\mathbf{F}_{2} + \\lambda \\mathbf{F}_{1})\\boldsymbol{\\theta} = \\mathbf{F}_{2}\\boldsymbol{\\theta}_{2} + \\lambda \\mathbf{F}_{1}\\boldsymbol{\\theta}_{1}\n    $$\n    $$\n    \\boldsymbol{\\theta}^{\\star}(\\lambda) = (\\mathbf{F}_{2} + \\lambda \\mathbf{F}_{1})^{-1}(\\mathbf{F}_{2}\\boldsymbol{\\theta}_{2} + \\lambda \\mathbf{F}_{1}\\boldsymbol{\\theta}_{1})\n    $$\n    Given that $\\mathbf{F}_{1}$ and $\\mathbf{F}_{2}$ are diagonal, all matrix operations are simplified.\n    $$\n    \\mathbf{F}_{2} + \\lambda \\mathbf{F}_{1} = \\begin{pmatrix}1 & 0 \\\\ 0 & 9\\end{pmatrix} + \\lambda\\begin{pmatrix}4 & 0 \\\\ 0 & 1\\end{pmatrix} = \\begin{pmatrix}1+4\\lambda & 0 \\\\ 0 & 9+\\lambda\\end{pmatrix}\n    $$\n    $$\n    \\mathbf{F}_{2}\\boldsymbol{\\theta}_{2} + \\lambda \\mathbf{F}_{1}\\boldsymbol{\\theta}_{1} = \\begin{pmatrix}1 & 0 \\\\ 0 & 9\\end{pmatrix}\\begin{pmatrix}0 \\\\ 2\\end{pmatrix} + \\lambda\\begin{pmatrix}4 & 0 \\\\ 0 & 1\\end{pmatrix}\\begin{pmatrix}1 \\\\ 0\\end{pmatrix} = \\begin{pmatrix}0 \\\\ 18\\end{pmatrix} + \\begin{pmatrix}4\\lambda \\\\ 0\\end{pmatrix} = \\begin{pmatrix}4\\lambda \\\\ 18\\end{pmatrix}\n    $$\n    So, $\\boldsymbol{\\theta}^{\\star}(\\lambda)$ is:\n    $$\n    \\boldsymbol{\\theta}^{\\star}(\\lambda) = \\begin{pmatrix}\\frac{1}{1+4\\lambda} & 0 \\\\ 0 & \\frac{1}{9+\\lambda}\\end{pmatrix} \\begin{pmatrix}4\\lambda \\\\ 18\\end{pmatrix} = \\begin{pmatrix}\\frac{4\\lambda}{1+4\\lambda} \\\\ \\frac{18}{9+\\lambda}\\end{pmatrix}\n    $$\n\n2.  **Calculate the average loss $L_{\\mathrm{avg}}(\\lambda)$**\n    We need to compute $L_{1}(\\boldsymbol{\\theta}^{\\star}(\\lambda))$ and $L_{2}(\\boldsymbol{\\theta}^{\\star}(\\lambda))$.\n    $$\n    L_{1}(\\boldsymbol{\\theta}^{\\star}(\\lambda)) = \\frac{1}{2}(\\boldsymbol{\\theta}^{\\star}(\\lambda) - \\boldsymbol{\\theta}_{1})^{\\top}\\mathbf{F}_{1}(\\boldsymbol{\\theta}^{\\star}(\\lambda) - \\boldsymbol{\\theta}_{1})\n    $$\n    $$\n    \\boldsymbol{\\theta}^{\\star}(\\lambda) - \\boldsymbol{\\theta}_{1} = \\begin{pmatrix}\\frac{4\\lambda}{1+4\\lambda} - 1 \\\\ \\frac{18}{9+\\lambda} - 0\\end{pmatrix} = \\begin{pmatrix}\\frac{-1}{1+4\\lambda} \\\\ \\frac{18}{9+\\lambda}\\end{pmatrix}\n    $$\n    $$\n    L_{1}(\\boldsymbol{\\theta}^{\\star}(\\lambda)) = \\frac{1}{2} \\left[ 4\\left(\\frac{-1}{1+4\\lambda}\\right)^2 + 1\\left(\\frac{18}{9+\\lambda}\\right)^2 \\right] = \\frac{2}{(1+4\\lambda)^2} + \\frac{162}{(9+\\lambda)^2}\n    $$\n    Next, for task 2:\n    $$\n    L_{2}(\\boldsymbol{\\theta}^{\\star}(\\lambda)) = \\frac{1}{2}(\\boldsymbol{\\theta}^{\\star}(\\lambda) - \\boldsymbol{\\theta}_{2})^{\\top}\\mathbf{F}_{2}(\\boldsymbol{\\theta}^{\\star}(\\lambda) - \\boldsymbol{\\theta}_{2})\n    $$\n    $$\n    \\boldsymbol{\\theta}^{\\star}(\\lambda) - \\boldsymbol{\\theta}_{2} = \\begin{pmatrix}\\frac{4\\lambda}{1+4\\lambda} - 0 \\\\ \\frac{18}{9+\\lambda} - 2\\end{pmatrix} = \\begin{pmatrix}\\frac{4\\lambda}{1+4\\lambda} \\\\ \\frac{18 - 2(9+\\lambda)}{9+\\lambda}\\end{pmatrix} = \\begin{pmatrix}\\frac{4\\lambda}{1+4\\lambda} \\\\ \\frac{-2\\lambda}{9+\\lambda}\\end{pmatrix}\n    $$\n    $$\n    L_{2}(\\boldsymbol{\\theta}^{\\star}(\\lambda)) = \\frac{1}{2} \\left[ 1\\left(\\frac{4\\lambda}{1+4\\lambda}\\right)^2 + 9\\left(\\frac{-2\\lambda}{9+\\lambda}\\right)^2 \\right] = \\frac{8\\lambda^2}{(1+4\\lambda)^2} + \\frac{18\\lambda^2}{(9+\\lambda)^2}\n    $$\n    The average loss is $L_{\\mathrm{avg}}(\\lambda) = \\frac{1}{2}(L_{1} + L_{2})$:\n    $$\n    L_{\\mathrm{avg}}(\\lambda) = \\frac{1}{2} \\left[ \\frac{2+8\\lambda^2}{(1+4\\lambda)^2} + \\frac{162+18\\lambda^2}{(9+\\lambda)^2} \\right] = \\frac{1+4\\lambda^2}{(1+4\\lambda)^2} + \\frac{81+9\\lambda^2}{(9+\\lambda)^2}\n    $$\n\n3.  **Minimize $L_{\\mathrm{avg}}(\\lambda)$**\n    To find the minimum, we set the derivative with respect to $\\lambda$ to zero. Let's differentiate each term.\n    For the first term, $g_1(\\lambda) = \\frac{1+4\\lambda^2}{(1+4\\lambda)^2}$:\n    $$\n    \\frac{d g_1}{d\\lambda} = \\frac{8\\lambda(1+4\\lambda)^2 - (1+4\\lambda^2) \\cdot 2(1+4\\lambda) \\cdot 4}{(1+4\\lambda)^4} = \\frac{8\\lambda(1+4\\lambda) - 8(1+4\\lambda^2)}{(1+4\\lambda)^3} = \\frac{8\\lambda+32\\lambda^2 - 8-32\\lambda^2}{(1+4\\lambda)^3} = \\frac{8(\\lambda-1)}{(1+4\\lambda)^3}\n    $$\n    For the second term, $g_2(\\lambda) = \\frac{81+9\\lambda^2}{(9+\\lambda)^2}$:\n    $$\n    \\frac{d g_2}{d\\lambda} = \\frac{18\\lambda(9+\\lambda)^2 - (81+9\\lambda^2) \\cdot 2(9+\\lambda)}{(9+\\lambda)^4} = \\frac{18\\lambda(9+\\lambda) - 2(81+9\\lambda^2)}{(9+\\lambda)^3} = \\frac{162\\lambda+18\\lambda^2 - 162-18\\lambda^2}{(9+\\lambda)^3} = \\frac{162(\\lambda-1)}{(9+\\lambda)^3}\n    $$\n    Setting the derivative of $L_{\\mathrm{avg}}(\\lambda)$ to zero:\n    $$\n    \\frac{d L_{\\mathrm{avg}}}{d\\lambda} = \\frac{8(\\lambda-1)}{(1+4\\lambda)^3} + \\frac{162(\\lambda-1)}{(9+\\lambda)^3} = (\\lambda-1) \\left[ \\frac{8}{(1+4\\lambda)^3} + \\frac{162}{(9+\\lambda)^3} \\right] = 0\n    $$\n    For $\\lambda \\geq 0$, the term in the square brackets is strictly positive. Therefore, the only solution is $\\lambda - 1 = 0$, which gives $\\lambda=1$.\n    The second derivative test would confirm this is a minimum. Thus, the optimal trade-off parameter is $1$.",
            "answer": "$$\n\\boxed{1}\n$$"
        },
        {
            "introduction": "Beyond regularization, another major approach to continual learning involves projecting gradients to avoid interference. These methods aim to find update directions for a new task that lie in the null space of gradients from previous tasks, theoretically preventing any forgetting. This practice shifts the focus to the practical, computational side of such an algorithm . By analyzing the number of floating-point operations (flops) required for the core projection step, you will connect the abstract mathematical concept of orthogonal projection to its concrete computational cost, a critical consideration for implementing scalable lifelong learning systems.",
            "id": "4041143",
            "problem": "Consider a continual learning setup in neuromorphic and brain-inspired computing where a parameter vector in $\\mathbb{R}^{d}$ is updated by stochastic gradient-based rules across tasks while avoiding interference with previously learned tasks. Suppose $k$ previous tasks have produced $k$ linearly independent gradient directions, collected as columns of a matrix $G \\in \\mathbb{R}^{d \\times k}$ with $k \\leq d$. Let an orthonormal basis $Q = [q_{1}, \\ldots, q_{k}] \\in \\mathbb{R}^{d \\times k}$ for the column space of $G$ be obtained once per task-switch using the Gram–Schmidt orthogonalization procedure. On each training step within the current task, a fresh gradient $g \\in \\mathbb{R}^{d}$ is projected onto the null space of $G^{\\top}$ (the orthogonal complement of the span of the columns of $G$) using the Gram–Schmidt recurrence\n$$\ng \\leftarrow g - \\sum_{i=1}^{k} \\langle q_{i}, g \\rangle q_{i},\n$$\nso that the update direction is $g_{\\perp} = g - \\sum_{i=1}^{k} \\langle q_{i}, g \\rangle q_{i}$, which belongs to the null space of $G^{\\top}$.\n\nAdopt the standard floating-point operation (flop) counting model: each scalar multiplication costs $1$ flop and each scalar addition or subtraction costs $1$ flop. Under this model, a dot product between two vectors in $\\mathbb{R}^{d}$ costs $2d - 1$ flops, a matrix–vector product with a $d \\times r$ matrix costs $d(2r - 1)$ flops, and subtracting two vectors in $\\mathbb{R}^{d}$ costs $d$ flops.\n\nAssume $Q$ has already been computed and stored and that $G$ has full column rank $k$. Derive, from first principles, the exact number of flops required per training step to compute $g_{\\perp}$ using the Gram–Schmidt iterate described above, as an explicit closed-form function of $d$ and $k$. Your final answer must be a single analytic expression in terms of $d$ and $k$ only. Do not simplify by using asymptotic notation. No rounding is needed.",
            "solution": "The problem requires us to find the total number of floating-point operations (flops) to compute the projected gradient $g_{\\perp}$ on each training step, using the formula:\n$$\ng_{\\perp} = g - \\sum_{i=1}^{k} \\langle q_{i}, g \\rangle q_{i}\n$$\nwhere $g, q_i \\in \\mathbb{R}^{d}$. We must derive the exact flop count from first principles based on the provided cost model.\n\nThe calculation can be broken down into three main stages:\n\n1.  **Compute all scalar coefficients**: First, we must compute the $k$ scalar coefficients, $c_i = \\langle q_{i}, g \\rangle$, for $i=1, \\ldots, k$. Each coefficient is the dot product of two vectors, $q_i$ and $g$, which are both in $\\mathbb{R}^{d}$. According to the problem's cost model, a single dot product costs $2d-1$ flops. Since we need to compute $k$ such dot products, the total cost for this stage is:\n    $$\n    \\text{Flops}_{\\text{coeffs}} = k \\times (2d - 1) = 2kd - k\n    $$\n\n2.  **Compute the projection sum**: Next, we compute the vector sum $S = \\sum_{i=1}^{k} c_i q_i$. This operation consists of $k$ scalar-vector multiplications (one for each $c_i q_i$) and $k-1$ vector additions to sum them up.\n    *   Each scalar-vector multiplication $c_i q_i$ involves multiplying a $d$-dimensional vector by a scalar, which costs $d$ flops. For $k$ such operations, the cost is $k \\times d$.\n    *   Summing these $k$ vectors requires $k-1$ vector additions. Each vector addition costs $d$ flops. The total cost for additions is $(k-1) \\times d$.\n    The total cost for computing the sum $S$ is:\n    $$\n    \\text{Flops}_{\\text{sum}} = (k \\times d) + ((k-1) \\times d) = kd + kd - d = 2kd - d\n    $$\n\n3.  **Compute the final subtraction**: Finally, we compute the projected vector $g_{\\perp} = g - S$. This is a subtraction of two vectors in $\\mathbb{R}^{d}$, which costs $d$ flops.\n    $$\n    \\text{Flops}_{\\text{sub}} = d\n    $$\n\nThe total number of flops is the sum of the costs from these three stages:\n$$\n\\text{Total Flops} = \\text{Flops}_{\\text{coeffs}} + \\text{Flops}_{\\text{sum}} + \\text{Flops}_{\\text{sub}}\n$$\n$$\n\\text{Total Flops} = (2kd - k) + (2kd - d) + d\n$$\n$$\n\\text{Total Flops} = 4kd - k\n$$\nThus, the total number of flops required per training step to compute $g_{\\perp}$ is $4kd - k$.",
            "answer": "$$\\boxed{4kd - k}$$"
        }
    ]
}