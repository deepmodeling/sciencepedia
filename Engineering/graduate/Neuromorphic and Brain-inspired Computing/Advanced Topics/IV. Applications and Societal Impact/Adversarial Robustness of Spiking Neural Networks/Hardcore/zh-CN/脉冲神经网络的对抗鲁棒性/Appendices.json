{
    "hands_on_practices": [
        {
            "introduction": "要使用基于梯度的方法生成对抗性样本，核心在于评估网络的鲁棒性。第一个练习聚焦于该过程在脉冲神经网络（SNN）中的核心机制：计算损失函数相对于输入脉冲序列的梯度。通过在一个简单的SNN模型中，利用代理梯度进行时间反向传播（backpropagation through time）的推导，你将对如何在时间域中构造对抗性扰动建立起基础性的理解 。",
            "id": "4034865",
            "problem": "考虑一个单层脉冲神经网络（SNN），用作对抗性鲁棒性分析的特征提取器。存在 $N$ 个输入通道，索引为 $i \\in \\{1,\\dots,N\\}$，以及 $T$ 个离散时间步，索引为 $t \\in \\{1,\\dots,T\\}$。输入脉冲序列为 $x_{i,t} \\in \\{0,1\\}$，但为了进行基于梯度的对抗性扰动分析，将 $x_{i,t}$ 视为实值变量。突触后神经元的膜电位为 $u_t \\in \\mathbb{R}$，其随离散时间进行无重置的泄露积分：\n$$\nu_t \\;=\\; \\lambda\\,u_{t-1} \\;+\\; \\sum_{i=1}^{N} w_i\\, x_{i,t}, \\quad u_0 \\;=\\; 0,\n$$\n其中 $\\lambda \\in (0,1)$ 是泄露因子，$w_i \\in \\mathbb{R}$ 是突触权重。神经元通过在 $\\vartheta \\in \\mathbb{R}$ 处的硬阈值化发出一个脉冲 $s_t \\in \\{0,1\\}$：\n$$\ns_t \\;=\\; H\\!\\left(u_t - \\vartheta\\right),\n$$\n其中 $H(\\cdot)$ 是 Heaviside 阶跃函数。对于反向传播，使用宽度参数为 $\\beta > 0$ 的三角形代理导数来处理阈值非线性：\n$$\n\\psi_{\\beta}(z) \\;=\\; \\begin{cases}\n\\dfrac{1}{\\beta}\\left(1 - \\dfrac{|z|}{\\beta}\\right), & \\text{if } |z| \\le \\beta,\\\\[6pt]\n0, & \\text{if } |z| > \\beta,\n\\end{cases}\n$$\n并用 $\\psi_{\\beta}\\!\\left(u_t - \\vartheta\\right)$ 来近似 $\\dfrac{\\partial s_t}{\\partial u_t}$。\n\n一个线性读出层将脉冲聚合为标量计数 $S \\;=\\; \\sum_{t=1}^{T} s_t$，然后生成 $K$ 个类别 logits\n$$\nz_k \\;=\\; \\alpha_k\\, S \\;+\\; b_k, \\quad k \\in \\{1,\\dots,K\\},\n$$\n其中 $\\alpha_k, b_k \\in \\mathbb{R}$。预测的类别概率由 softmax 函数给出\n$$\np_k \\;=\\; \\dfrac{\\exp(z_k)}{\\sum_{\\ell=1}^{K} \\exp(z_{\\ell})},\n$$\n训练目标是针对独热标签向量 $y \\in \\{0,1\\}^{K}$ 的单样本交叉熵损失，\n$$\n\\mathcal{L} \\;=\\; - \\sum_{k=1}^{K} y_k \\,\\ln p_k.\n$$\n\n在此设置下，对于任意固定的 $j \\in \\{1,\\dots,N\\}$ 和 $\\tau \\in \\{1,\\dots,T\\}$，计算损失函数关于输入分量 $x_{j,\\tau}$ 的梯度的精确解析表达式（闭式解）：\n$$\n\\dfrac{\\partial \\mathcal{L}}{\\partial x_{j,\\tau}} \\quad \\text{作为 } \\left(\\lambda, \\{w_i\\}_{i=1}^{N}, \\vartheta, \\beta, \\{\\alpha_k,b_k\\}_{k=1}^{K}, \\{x_{i,t}\\}, \\{u_t\\}\\right) \\text{ 的函数}.\n$$\n\n您的最终答案必须是单一的闭式符号表达式。不需要数值近似或四舍五入。答案中不带任何单位。避免使用任何不等式或方程作为最终答案；只提供所要求的单一表达式。",
            "solution": "该问题是有效的。它提出了一个定义清晰、自成体系且具有科学依据的简单脉冲神经网络（SNN）模型，并要求计算一个特定的梯度。问题提供了必要的方程、参数和初始条件，任务是应用微分学原理，特别是链式法则，来推导一个解析表达式。对于不可微的脉冲机制使用代理梯度是训练SNN领域的一项标准技术。该问题是适定的，并允许一个唯一的、精确的解。\n\n我们的目标是计算损失函数 $\\mathcal{L}$ 相对于特定时间步 $\\tau$ 和输入通道 $j$ 上的单个输入分量 $x_{j,\\tau}$ 的梯度。这需要沿着网络的计算图，从损失函数反向到输入应用链式法则。计算路径如下：$\\mathcal{L} \\rightarrow \\{p_k\\} \\rightarrow \\{z_k\\} \\rightarrow S \\rightarrow \\{s_t\\} \\rightarrow \\{u_t\\} \\rightarrow x_{j,\\tau}$。\n\n首先，我们计算交叉熵损失 $\\mathcal{L} = - \\sum_{k=1}^{K} y_k \\ln p_k$ 相对于 logits $z_m$ 的梯度。概率 $p_k$ 由 softmax 函数给出：$p_k = \\frac{\\exp(z_k)}{\\sum_{\\ell=1}^{K} \\exp(z_{\\ell})}$。\n损失函数相对于 logit $z_m$ 的导数是一个标准结果：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial z_m} = p_m - y_m.\n$$\n\n接下来，我们求损失函数相对于总脉冲数 $S$ 的梯度。logits 定义为 $z_k = \\alpha_k S + b_k$。使用链式法则，我们对所有依赖于 $S$ 的 logits $z_k$ 进行求和：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial S} = \\sum_{k=1}^{K} \\frac{\\partial \\mathcal{L}}{\\partial z_k} \\frac{\\partial z_k}{\\partial S}.\n$$\n由于 $\\frac{\\partial z_k}{\\partial S} = \\alpha_k$，我们代入前面的结果：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial S} = \\sum_{k=1}^{K} \\alpha_k (p_k - y_k).\n$$\n该项相对于时间索引 $t$ 是一个常数。\n\n现在，我们将梯度传播到单个脉冲 $s_t$。总脉冲数是 $S = \\sum_{t=1}^{T} s_t$。损失函数相对于单个脉冲 $s_t$ 的梯度是：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial s_t} = \\frac{\\partial \\mathcal{L}}{\\partial S} \\frac{\\partial S}{\\partial s_t}.\n$$\n已知 $\\frac{\\partial S}{\\partial s_t} = 1$，我们有：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial s_t} = \\frac{\\partial \\mathcal{L}}{\\partial S} = \\sum_{k=1}^{K} \\alpha_k (p_k - y_k).\n$$\n\n下一步是将梯度传播到膜电位 $u_t$。脉冲 $s_t$ 由 $s_t = H(u_t - \\vartheta)$ 生成，其中 $H(\\cdot)$ 是 Heaviside 阶跃函数。由于 $H(\\cdot)$ 的导数几乎处处为零且在原点未定义，我们使用所提供的代理梯度 $\\psi_{\\beta}(z)$。我们近似 $\\frac{\\partial s_t}{\\partial u_t} \\approx \\psi_{\\beta}(u_t - \\vartheta)$。\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial u_t} = \\frac{\\partial \\mathcal{L}}{\\partial s_t} \\frac{\\partial s_t}{\\partial u_t} \\approx \\left( \\sum_{k=1}^{K} \\alpha_k (p_k - y_k) \\right) \\psi_{\\beta}(u_t - \\vartheta).\n$$\n\n最后，我们计算损失函数相对于输入 $x_{j,\\tau}$ 的梯度。在时间 $\\tau$ 的输入会影响所有后续时间 $t \\ge \\tau$ 的膜电位。因此，我们必须将 $x_{j,\\tau}$ 通过所有受影响的电位 $\\{u_t\\}_{t=\\tau}^T$ 的贡献加起来。\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial x_{j,\\tau}} = \\sum_{t=1}^{T} \\frac{\\partial \\mathcal{L}}{\\partial u_t} \\frac{\\partial u_t}{\\partial x_{j,\\tau}}.\n$$\n我们需要确定雅可比项 $\\frac{\\partial u_t}{\\partial x_{j,\\tau}}$。膜电位根据 $u_t = \\lambda u_{t-1} + \\sum_{i=1}^{N} w_i x_{i,t}$ 演化，其中 $u_0 = 0$。\n如果 $t  \\tau$，$u_t$ 不依赖于 $x_{j,\\tau}$，所以 $\\frac{\\partial u_t}{\\partial x_{j,\\tau}} = 0$。\n如果 $t = \\tau$，我们有 $u_\\tau = \\lambda u_{\\tau-1} + \\sum_{i=1}^{N} w_i x_{i,\\tau}$。项 $u_{\\tau-1}$ 不依赖于 $x_{j,\\tau}$，所以：\n$$\n\\frac{\\partial u_\\tau}{\\partial x_{j,\\tau}} = w_j.\n$$\n如果 $t  \\tau$，我们对递推关系应用链式法则：\n$$\n\\frac{\\partial u_t}{\\partial x_{j,\\tau}} = \\frac{\\partial}{\\partial x_{j,\\tau}} \\left( \\lambda u_{t-1} + \\sum_{i=1}^{N} w_i x_{i,t} \\right) = \\lambda \\frac{\\partial u_{t-1}}{\\partial x_{j,\\tau}}.\n$$\n这定义了一个简单的递推关系。我们可以将其展开，为 $t \\ge \\tau$ 找到一个闭式表达式：\n$$\n\\frac{\\partial u_t}{\\partial x_{j,\\tau}} = \\lambda \\frac{\\partial u_{t-1}}{\\partial x_{j,\\tau}} = \\lambda^2 \\frac{\\partial u_{t-2}}{\\partial x_{j,\\tau}} = \\dots = \\lambda^{t-\\tau} \\frac{\\partial u_{\\tau}}{\\partial x_{j,\\tau}} = \\lambda^{t-\\tau} w_j.\n$$\n现在我们可以整合梯度的最终表达式。对 $t$ 的求和从 $\\tau$ 开始：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial x_{j,\\tau}} = \\sum_{t=\\tau}^{T} \\frac{\\partial \\mathcal{L}}{\\partial u_t} \\frac{\\partial u_t}{\\partial x_{j,\\tau}} = \\sum_{t=\\tau}^{T} \\left[ \\left( \\sum_{k=1}^{K} \\alpha_k (p_k - y_k) \\right) \\psi_{\\beta}(u_t - \\vartheta) \\right] \\left( \\lambda^{t-\\tau} w_j \\right).\n$$\n我们可以将不依赖于求和索引 $t$ 的项提取出来：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial x_{j,\\tau}} = w_j \\left( \\sum_{k=1}^{K} \\alpha_k (p_k - y_k) \\right) \\left( \\sum_{t=\\tau}^{T} \\lambda^{t-\\tau} \\psi_{\\beta}(u_t - \\vartheta) \\right).\n$$\n这就是损失函数相对于输入分量 $x_{j,\\tau}$ 的梯度的最终闭式表达式。",
            "answer": "$$\n\\boxed{w_j \\left( \\sum_{k=1}^{K} \\alpha_k (p_k - y_k) \\right) \\left( \\sum_{t=\\tau}^{T} \\lambda^{t-\\tau} \\psi_{\\beta}(u_t - \\vartheta) \\right)}\n$$"
        },
        {
            "introduction": "除了攻击机制本身，网络架构在其固有的鲁棒性中也扮演着关键角色。本练习提供了一个思想实验，在SNN的背景下比较了两种常见的神经操作：最大池化（max pooling）和平均池化（average pooling）。通过分析确定哪种池化策略对稀疏的对抗性脉冲注入更具弹性，你将深入理解架构设计选择如何可能制造漏洞或构建防御 。",
            "id": "4034876",
            "problem": "一个脉冲神经网络（SNN）层在固定的观测窗口 $t \\in [0,T]$ 内产生突触前脉冲序列 $\\{s_i(t)\\}_{i=1}^n$，其中每个 $s_i(t)$ 是代表脉冲的狄拉克脉冲之和。定义单元 $i$ 的脉冲计数 $c_i$ 为 $c_i = \\int_0^T s_i(t)\\,\\mathrm{d}t$，因此每个脉冲对 $c_i$ 的贡献为 $1$。考虑一个大小为 $n$ 的池化区域，以及应用于 $\\{c_i\\}_{i=1}^n$ 的两个（2个）脉冲形式的池化函数：\n- 最大池化：$y_{\\max} = \\max_{1 \\le i \\le n} c_i$。\n- 平均池化：$y_{\\mathrm{avg}} = \\frac{1}{n} \\sum_{i=1}^n c_i$。\n\n假设存在一个最坏情况下的攻击者，他可以通过向计数中添加一个非负整数向量 $a \\in \\mathbb{Z}_{\\ge 0}^n$ 来插入稀疏事件，从而产生扰动后的计数 $c_i' = c_i + a_i$。此操作受 $\\ell_1$ 预算 $\\|a\\|_1 = \\sum_{i=1}^n a_i \\le s$ 的限制，其中 $s$ 是在 $[0,T]$ 期间整个池化区域内插入的脉冲总数。在窗口内，$c_i$ 没有上限饱和。\n\n下游的决策阶段将池化值 $y$ 与一个固定的阈值 $\\theta$ 进行比较：如果 $y \\ge \\theta$，则宣告检测到事件。对于一个未受扰动的池化值 $y$，其裕量 $\\Delta = \\theta - y  0$，攻击者试图通过插入脉冲，以最小的代价将 $y$ 提升到至少为 $\\theta$。在此模型和所述定义下，哪个选项最能描述最大池化与平均池化在单个池化区域内对稀疏事件插入的相对对抗鲁棒性？\n\nA. 在最坏情况下，平均池化需要比最大池化多至少 $n$ 倍的插入脉冲才能越过相同的下游阈值，这意味着平均池化对稀疏插入更具鲁棒性。\n\nB. 最大池化更具鲁棒性，因为它丢弃了除最大计数值之外的所有值；稀疏攻击者在不影响大多数单元的情况下无法显著改变最大计数值，因此需要更多插入的脉冲。\n\nC. 在脉冲插入下，最大池化和平均池化的鲁棒性相同，因为两种池化函数在脉冲计数上都是单调的。\n\nD. 对稀疏脉冲插入的鲁棒性仅由突触后泄漏和不应期动力学决定，而与使用最大池化还是平均池化无关。",
            "solution": "对问题陈述进行严格评估后，认定其是有效的。它在简化神经网络模型的标准理论框架内具有科学依据，问题设定良好，目标明确，语言客观，并且内部一致。为进行形式化分析提供了所有必要的定义。\n\n该问题要求在脉冲神经网络（SNN）背景下，比较最大池化与平均池化的对抗鲁棒性。鲁棒性由导致错误分类所需的最少对抗性插入脉冲数来隐式量化，即，使池化输出达到阈值 $\\theta$。\n\n设大小为 $n$ 的池化区域中的初始脉冲计数为向量 $c = (c_1, c_2, \\dots, c_n)$，其中 $c_i \\in \\mathbb{Z}_{\\ge 0}$ 是整数。攻击者添加一个扰动向量 $a = (a_1, a_2, \\dots, a_n)$，其中 $a_i \\in \\mathbb{Z}_{\\ge 0}$，得到扰动后的计数 $c' = c + a$。插入的脉冲总数是 $a$ 的 $\\ell_1$ 范数，记为 $S = \\|a\\|_1 = \\sum_{i=1}^n a_i$。\n\n初始状态下，池化输出 $y$ 低于阈值 $\\theta$。也就是说，存在一个裕量 $\\Delta = \\theta - y  0$。攻击者的目标是找到最小的整数 $S$，使得新的池化输出 $y'$ 满足 $y' \\ge \\theta$。为了使分析易于处理并与问题的整数定义（$c_i, a_i$）保持一致，我们假设阈值 $\\theta$ 也是一个整数。\n\n**情况1：最大池化**\n\n输出由 $y_{\\max} = \\max_{1 \\le i \\le n} c_i$ 给出。初始状态满足 $y_{\\max}  \\theta$。\n扰动后的输出为 $y'_{\\max} = \\max_{1 \\le i \\le n} (c_i + a_i)$。\n攻击者的目标是实现 $y'_{\\max} \\ge \\theta$。这要求至少对于一个神经元 $j$，扰动后的计数满足 $c_j + a_j \\ge \\theta$。\n\n为了最小化插入脉冲的总数 $S = \\sum a_i$，攻击者应采用最高效的策略。这包括识别出达到阈值所需最少附加脉冲的神经元，并将所有扰动集中在该单个神经元上。将神经元 $j$ 推至阈值所需的脉冲数为 $a_j = \\theta - c_j$（因为 $c_j  \\theta$，且所有量都是整数）。为了最小化此成本，攻击者应选择具有最高初始脉冲计数的神经元 $k$，即 $c_k = \\max_{i} c_i = y_{\\max}$。\n\n通过设置 $a_k = \\theta - c_k$ 并对所有 $i \\ne k$ 设置 $a_i = 0$，攻击者可以用最小可能的脉冲总数实现目标。设此最小值为 $S_{\\max}^{\\min}$。\n$$S_{\\max}^{\\min} = \\theta - c_k = \\theta - \\max_{1 \\le i \\le n} c_i = \\theta - y_{\\max}$$\n\n**情况2：平均池化**\n\n输出由 $y_{\\mathrm{avg}} = \\frac{1}{n} \\sum_{i=1}^n c_i$ 给出。初始状态满足 $y_{\\mathrm{avg}}  \\theta$。\n扰动后的输出为 $y'_{\\mathrm{avg}} = \\frac{1}{n} \\sum_{i=1}^n (c_i + a_i)$。\n攻击者的目标是实现 $y'_{\\mathrm{avg}} \\ge \\theta$。\n$$ \\frac{1}{n} \\sum_{i=1}^n (c_i + a_i) \\ge \\theta $$\n整理不等式：\n$$ \\sum_{i=1}^n c_i + \\sum_{i=1}^n a_i \\ge n \\theta $$\n$$ \\sum_{i=1}^n a_i \\ge n \\theta - \\sum_{i=1}^n c_i $$\n左边的量是脉冲总数 $S$。右边的量 $n \\theta - \\sum c_i$ 是一个整数，因为 $n$、$\\theta$ 和所有 $c_i$ 都是整数。满足此不等式的最小整数 $S$ 是右侧的值。设此最小值为 $S_{\\mathrm{avg}}^{\\min}$。\n$$ S_{\\mathrm{avg}}^{\\min} = n \\theta - \\sum_{i=1}^n c_i $$\n这也可以用初始输出 $y_{\\mathrm{avg}}$ 来表示：\n$$ S_{\\mathrm{avg}}^{\\min} = n(\\theta - \\frac{1}{n}\\sum_{i=1}^n c_i) = n(\\theta - y_{\\mathrm{avg}}) $$\n\n**鲁棒性比较**\n\n为了比较鲁棒性，我们比较每种情况下所需的最小脉冲数，$S_{\\max}^{\\min}$ 和 $S_{\\mathrm{avg}}^{\\min}$。我们将评估比率 $S_{\\mathrm{avg}}^{\\min} / S_{\\max}^{\\min}$。\n$$ \\frac{S_{\\mathrm{avg}}^{\\min}}{S_{\\max}^{\\min}} = \\frac{n \\theta - \\sum_{i=1}^n c_i}{\\theta - \\max_{1 \\le i \\le n} c_i} $$\n对于任何非负数集合 $\\{c_i\\}_{i=1}^n$，一个基本性质是其总和小于或等于最大值的 $n$ 倍：\n$$ \\sum_{i=1}^n c_i \\le n \\cdot \\max_{1 \\le i \\le n} c_i $$\n乘以 $-1$ 会反转不等式：\n$$ -\\sum_{i=1}^n c_i \\ge -n \\cdot \\max_{1 \\le i \\le n} c_i $$\n两边同时加上 $n\\theta$ 得出：\n$$ n \\theta - \\sum_{i=1}^n c_i \\ge n \\theta - n \\cdot \\max_{1 \\le i \\le n} c_i $$\n$$ n \\theta - \\sum_{i=1}^n c_i \\ge n \\left(\\theta - \\max_{1 \\le i \\le n} c_i\\right) $$\n代入最小脉冲数的表达式：\n$$ S_{\\mathrm{avg}}^{\\min} \\ge n \\cdot S_{\\max}^{\\min} $$\n由于我们有初始裕量 $\\Delta  0$，所以 $S_{\\max}^{\\min} = \\theta - y_{\\max} \\ge 1$，因此比较是有意义的。等式 $S_{\\mathrm{avg}}^{\\min} = n \\cdot S_{\\max}^{\\min}$ 成立的充要条件是 $\\sum c_i = n \\cdot \\max c_i$，这又等价于所有 $c_i$ 都相等。如果计数 $c_i$ 不一致，则不等式是严格的：$S_{\\mathrm{avg}}^{\\min}  n \\cdot S_{\\max}^{\\min}$。\n\n这一结果表明，与最大池化相比，平均池化需要至少 $n$ 倍的脉冲才能被这种类型的攻击攻破。因此，平均池化的鲁棒性要强得多。\n\n**逐项分析**\n\nA. 在最坏情况下，平均池化需要比最大池化多至少 $n$ 倍的插入脉冲才能越过相同的下游阈值，这意味着平均池化对稀疏插入更具鲁棒性。\n- 这个陈述准确地反映了我们推导出的不等式 $S_{\\mathrm{avg}}^{\\min} \\ge n \\cdot S_{\\max}^{\\min}$。平均池化更具鲁棒性的结论直接源于这一数学事实，因为鲁棒性是通过导致失败所需的扰动幅度来衡量的。\n- **结论：正确。**\n\nB. 最大池化更具鲁棒性，因为它丢弃了除最大计数值之外的所有值；稀疏攻击者在不影响大多数单元的情况下无法显著改变最大计数值，因此需要更多插入的脉冲。\n- “最大池化更具鲁棒性”的结论与我们的发现相反。其提供的理由是有缺陷的：稀疏攻击者可以通过*仅*针对具有最大计数的单个神经元来高效地攻击最大池化，这是一种效率最高的单点攻击。它不需要影响大多数单元。\n- **结论：错误。**\n\nC. 在脉冲插入下，最大池化和平均池化的鲁棒性相同，因为两种池化函数在脉冲计数上都是单调的。\n- 虽然两种函数确实都是单调不减的，但仅此属性并不能决定鲁棒性的程度。输出对输入变化的敏感度，即与导数（或离散差分）相关的量，才是关键。对于最大池化，单个脉冲可使输出增加 $1$，而对于平均池化，单个脉冲仅使输出增加 $1/n$。它们的鲁棒性显然是不相等的。\n- **结论：错误。**\n\nD. 对稀疏脉冲插入的鲁棒性仅由突触后泄漏和不应期动力学决定，而与使用最大池化还是平均池化无关。\n- 这个陈述引入了问题明确定义的数学模型之外的概念（泄漏、不应期动力学）。分析必须在给定的框架内进行。我们的推导表明，在此模型中，池化函数的选择是决定鲁棒性的决定性因素。\n- **结论：错误。**",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "平滑代理梯度的理论世界与离散、不可微的脉冲的现实世界并非总是完全一致。这最后一个综合性练习将挑战你通过实现一个完整的攻击与评估流程，来量化这种“可迁移性差距”。你将使用代理模型生成一个对抗性样本，然后在一个具有精确脉冲动力学的SNN上测量其真实影响，从而让你深刻体会到基于梯度的攻击在实际应用中的局限性与有效性 。",
            "id": "4034858",
            "problem": "你的任务是实现一个完整的、可运行的程序，用于量化使用代理梯度制作的对抗样本与其在脉冲神经网络（SNN）上的真实影响之间的可迁移性差距。实验协议必须经过数学上的指定，然后在代码中实例化。目标系统是一个双输出、单层的漏电积分发放（LIF）神经元网络，接收时间序列输入电流。其中，对抗性扰动使用代理梯度模型生成，并在精确的脉冲动力学下进行评估。\n\n基本原理和定义：\n- 脉冲神经网络（SNN）是一个神经元的动力学系统，其中每个神经元在其膜电位超过阈值时会发出脉冲。每个神经元被建模为一个漏电积分发放（LIF）单元，其在每个时间步 $t$ 的离散时间更新如下：\n  - 重置前膜电位为 $v^{\\text{hat}}_{t,k} = \\lambda v_{t,k} + \\sum_{i=1}^{N_{\\text{in}}} W_{k,i} x_{t,i}$，其中 $k$ 是输出神经元索引，$\\lambda \\in (0,1)$ 是泄漏因子，权重矩阵为 $W \\in \\mathbb{R}^{N_{\\text{out}} \\times N_{\\text{in}}}$，输入为 $x_t \\in \\mathbb{R}^{N_{\\text{in}}}$。\n  - 精确脉冲 $z_{t,k}$ 为 $z_{t,k} = H(v^{\\text{hat}}_{t,k} - V_{\\text{th}})$，其中 $H(\\cdot)$ 是亥维赛阶跃函数，$V_{\\text{th}}$ 是阈值。\n  - 重置后膜电位为 $v_{t+1,k} = v^{\\text{hat}}_{t,k} - z_{t,k} V_{\\text{reset}}$，其中 $V_{\\text{reset}}$ 是发放脉冲时应用的重置减量。\n  - 在 $T$ 个时间步的范围内，输出 $k$ 的脉冲计数为 $S_k = \\sum_{t=0}^{T-1} z_{t,k}$。\n- 分类被定义为具有最大脉冲计数 $S_k$ 的索引 $k$，即在输出上的 $\\operatorname{argmax}$。对于真实类别索引 $y$，损失是脉冲计数的 softmax 与独热分布之间的交叉熵：$L = -\\log\\left(\\exp(S_y) / \\sum_{j=1}^{N_{\\text{out}}} \\exp(S_j)\\right)$。\n- 代理梯度建模将不可微的 $H(\\cdot)$ 替换为一个平滑函数。我们定义代理脉冲为 $z^{\\text{sur}}_{t,k} = \\sigma(\\beta (v^{\\text{hat}}_{t,k} - V_{\\text{th}}))$，其中 $\\sigma(\\cdot)$ 是逻辑斯谛函数，$\\beta  0$ 是一个控制代理转换锐度的斜率参数。\n- 对抗性扰动是使用在代理动力学下计算的快速梯度符号法（FGSM）生成的。对于一个扰动预算 $\\epsilon$，输入序列 $x$ 通过 $\\tilde{x}_{t,i} = \\Pi_{[0,1]}(x_{t,i} + \\epsilon \\cdot \\operatorname{sign}(\\nabla_{x_{t,i}} L))$ 被扰动为 $\\tilde{x}$，其中 $\\Pi_{[0,1]}(\\cdot)$ 将元素逐一投影到有效的输入范围 $[0,1]$ 内。\n\n可迁移性差距度量：\n- 攻击后的代理边际定义为 $\\tilde{m}^{\\text{sur}} = \\tilde{S}^{\\text{sur}}_{y} - \\max_{k \\neq y} \\tilde{S}^{\\text{sur}}_{k}$，其中 $\\tilde{S}^{\\text{sur}}_{k}$ 是在对抗性输入 $\\tilde{x}$ 上应用代理动力学得到的代理脉冲计数。\n- 攻击后的精确边际定义为 $\\tilde{m}^{\\text{exact}} = \\tilde{S}^{\\text{exact}}_{y} - \\max_{k \\neq y} \\tilde{S}^{\\text{exact}}_{k}$，其中 $\\tilde{S}^{\\text{exact}}_{k}$ 是在对抗性输入 $\\tilde{x}$ 上应用亥维赛动力学得到的精确脉冲计数。\n- 可迁移性差距为 $g = \\tilde{m}^{\\text{sur}} - \\tilde{m}^{\\text{exact}}$。一个正的 $g$ 值表示，相对于精确的脉冲动力学，代理动力学高估了攻击的效果。\n\n用于可复现性的固定模型和数据：\n- 输入数量为 $N_{\\text{in}} = 4$，输出数量为 $N_{\\text{out}} = 2$。\n- 权重矩阵固定为 $W = \\begin{bmatrix} 0.9  0.9  -0.9  -0.9 \\\\ -0.9  -0.9  0.9  0.9 \\end{bmatrix}$。\n- 泄漏因子为 $\\lambda = 0.9$，阈值为 $V_{\\text{th}} = 1.0$，重置值为 $V_{\\text{reset}} = 1.0$，真实类别索引为 $y = 0$。\n- 干净的输入序列在每个时间步都是一个常数向量：对于每个 $t$，$x_t = [0.6, 0.6, 0.4, 0.4]$。\n- 输入被逐元素限制在 $[0,1]$ 范围内。\n\n你的程序必须：\n- 为指定的 LIF 模型实现精确脉冲动力学和代理动力学。\n- 在代理动力学下，使用脉冲计数作为 logits 计算交叉熵损失，然后使用代理导数通过时间反向传播以获得 $\\nabla_x L$。\n- 在代理动力学下，使用预算 $\\epsilon$ 构建一个单步 FGSM 对抗样本，并投影到 $[0,1]$。\n- 在对抗性输入上评估 $\\tilde{m}^{\\text{sur}}$ 和 $\\tilde{m}^{\\text{exact}}$，并报告每个测试用例的可迁移性差距 $g$。\n\n测试套件：\n- 每个测试用例是一个元组 $(\\epsilon,\\beta,T)$，其中 $\\epsilon$ 是 FGSM 预算，$\\beta$ 是代理斜率， $T$ 是时间步数。\n- 使用以下测试用例：\n  - $(0.0, 2.0, 10)$ 表示无攻击。\n  - $(0.05, 2.0, 10)$ 表示小预算和中等锐度。\n  - $(0.3, 2.0, 10)$ 表示大预算。\n  - $(0.15, 10.0, 10)$ 表示陡峭的代理。\n  - $(0.15, 0.5, 10)$ 表示平滑的代理。\n  - $(0.15, 2.0, 1)$ 表示单步边界条件。\n\n最终输出规范：\n- 你的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表的结果。该列表的顺序必须与上面的测试套件相同，每个元素必须是可迁移性差距 $g$（一个浮点数）。例如，形如 $[g_1,g_2,g_3,g_4,g_5,g_6]$ 的输出行，其中每个 $g_i$ 对应于按所列顺序的测试用例。",
            "solution": "该问题是有效的。这是一个定义明确、具有科学依据的计算任务，属于神经拟态系统对抗性机器学习领域。所有参数、模型和目标都经过了充分的数学严谨性定义，可以得出唯一解。\n\n该问题的核心是量化用于基于梯度的对抗性攻击的代理模型与脉冲神经网络（SNN）真实的、不可微的动力学之间的差异。这种差异被称为“可迁移性差距”。解决方案涉及一个多步骤的计算过程，其基础是神经动力学、自动微分和对抗性机器学习的原理。\n\n**1. 漏电积分发放（LIF）神经元动力学**\nSNN 的基本组成部分是漏电积分发放（LIF）神经元。其行为由一组离散时间动力学方程控制。对于每个输出神经元 $k \\in \\{0, ..., N_{\\text{out}}-1\\}$，在时间步 $t \\in \\{0, ..., T-1\\}$，膜电位的演化如下：\n- 重置前电位 $v^{\\text{hat}}_{t,k}$ 整合了上一步的衰减电位和加权输入：\n$$v^{\\text{hat}}_{t,k} = \\lambda v_{t,k} + \\sum_{i=1}^{N_{\\text{in}}} W_{k,i} x_{t,i}$$\n这里，$\\lambda \\in (0,1)$ 是泄漏因子，$v_{t,k}$ 是在 $t-1$ 步可能重置后的膜电位（其中 $v_{0,k} = 0$），$W$ 是权重矩阵，$x_t$ 是在时间 $t$ 的输入向量。\n\n- 如果电位超过阈值 $V_{\\text{th}}$，则会发放一个脉冲。这是一个不可微的事件，由亥维赛阶跃函数 $H(\\cdot)$ 建模：\n$$z_{t,k} = H(v^{\\text{hat}}_{t,k} - V_{\\text{th}})$$\n其中 $z_{t,k} \\in \\{0, 1\\}$。\n\n- 发放脉冲后，电位会减少 $V_{\\text{reset}}$（减法重置机制）：\n$$v_{t+1,k} = v^{\\text{hat}}_{t,k} - z_{t,k} V_{\\text{reset}}$$\n\n- 在时间范围 $T$ 内，神经元 $k$ 的总脉冲计数为 $S_k = \\sum_{t=0}^{T-1} z_{t,k}$。\n\n**2. 代理梯度原理**\n像快速梯度符号法（FGSM）这样的对抗性攻击需要损失函数相对于输入的梯度 $\\nabla_x L$。精确脉冲生成中的亥维赛函数是不可微的，这使得梯度计算无法进行。为了克服这个问题，我们采用一个代理模型，其中亥维赛函数被一个平滑的近似函数所取代。问题指定了逻辑斯谛函数 $\\sigma(\\cdot)$：\n$$z^{\\text{sur}}_{t,k} = \\sigma(\\beta (v^{\\text{hat}}_{t,k} - V_{\\text{th}})) = \\frac{1}{1 + \\exp(-\\beta (v^{\\text{hat}}_{t,k} - V_{\\text{th}}))}$$\n参数 $\\beta  0$ 控制代理的陡峭程度，当 $\\beta \\to \\infty$ 时，它更接近亥维赛函数。所有后续的动力学，包括重置机制和脉冲计数，都使用这个连续值的“代理脉冲” $z^{\\text{sur}}_{t,k}$ 来保持可微性。\n\n**3. 通过随时间反向传播（BPTT）计算梯度**\n为了计算 $\\nabla_{x_t} L$，我们必须通过代理 SNN 展开的时间动力学来微分交叉熵损失。损失为 $L = -\\log\\left(\\exp(S^{\\text{sur}}_y) / \\sum_{j} \\exp(S^{\\text{sur}}_j)\\right)$，其中 $y$ 是真实类别索引。损失相对于代理脉冲计数 $S^{\\text{sur}}_k$ 的梯度是 $\\frac{\\partial L}{\\partial S^{\\text{sur}}_k} = p_k - \\delta_{ky}$，其中 $p_k$ 是 softmax 概率，$\\delta_{ky}$ 是克罗内克 delta。\n\nBPTT 算法将误差信号向后传播。令 $\\delta v^{\\text{hat}}_{t,k} = \\frac{\\partial L}{\\partial v^{\\text{hat}}_{t,k}}$ 和 $\\delta v_{t,k} = \\frac{\\partial L}{\\partial v_{t,k}}$ 为损失相对于电位的梯度。递推关系为：\n- $\\delta v^{\\text{hat}}_{t,k} = \\left(\\frac{\\partial L}{\\partial S^{\\text{sur}}_k} \\psi_{t,k}\\right) + \\delta v_{t+1,k} (1 - \\psi_{t,k} V_{\\text{reset}})$\n- $\\delta v_{t,k} = \\lambda \\delta v^{\\text{hat}}_{t,k}$\n其中 $\\psi_{t,k} = \\frac{\\partial z^{\\text{sur}}_{t,k}}{\\partial v^{\\text{hat}}_{t,k}} = \\beta \\sigma'(\\beta(v^{\\text{hat}}_{t,k} - V_{\\text{th}}))$。反向传播从 $\\delta v_{T,k} = 0$ 开始。\n然后通过链式法则找到相对于时间 $t$ 的输入的梯度：\n$$\\nabla_{x_{t}} L = W^T \\delta v^{\\text{hat}}_{t}$$\n\n**4. 对抗性攻击生成（FGSM）**\n利用从代理模型计算出的梯度 $\\nabla_x L$，我们可以使用单步 FGSM 制作一个对抗性输入 $\\tilde{x}$：\n$$\\tilde{x}_{t,i} = \\Pi_{[0,1]}\\left(x_{t,i} + \\epsilon \\cdot \\text{sign}((\\nabla_x L)_{t,i})\\right)$$\n其中 $\\epsilon$ 是扰动预算，$\\Pi_{[0,1]}(\\cdot)$ 是一个裁剪函数，将输入投影回有效范围 $[0, 1]$ 内。\n\n**5. 评估可迁移性差距**\n最后一步是衡量攻击的有效性。我们计算决策边际，定义为正确类别的脉冲计数减去任何不正确类别的最大脉冲计数。由于 $N_{\\text{out}}=2$ 且 $y=0$，这简化为 $S_0 - S_1$。我们在代理和精确动力学下，为对抗性输入 $\\tilde{x}$ 计算此边际：\n- 代理边际：$\\tilde{m}^{\\text{sur}} = \\tilde{S}^{\\text{sur}}_{0} - \\tilde{S}^{\\text{sur}}_{1}$\n- 精确边际：$\\tilde{m}^{\\text{exact}} = \\tilde{S}^{\\text{exact}}_{0} - \\tilde{S}^{\\text{exact}}_{1}$\n\n可迁移性差距 $g = \\tilde{m}^{\\text{sur}} - \\tilde{m}^{\\text{exact}}$ 量化了代理模型对攻击成功率的预测与真实 SNN 中实际结果的偏离程度。一个正的 $g$ 值表示代理模型高估了攻击的有效性。\n\n实现将为每个测试用例系统地遵循这些步骤，将动力学和梯度计算封装在专用函数中。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the transferability gap for adversarial examples in a Spiking Neural Network.\n    \"\"\"\n\n    # --- Fixed model and data for reproducibility ---\n    N_IN = 4\n    N_OUT = 2\n    W = np.array([\n        [0.9, 0.9, -0.9, -0.9],\n        [-0.9, -0.9, 0.9, 0.9]\n    ], dtype=np.float64)\n    LAMBDA = 0.9\n    V_TH = 1.0\n    V_RESET = 1.0\n    Y_TRUE_CLASS = 0\n    X_CLEAN = np.array([0.6, 0.6, 0.4, 0.4], dtype=np.float64)\n    \n    # --- Test suite ---\n    test_cases = [\n        (0.0, 2.0, 10),      # No attack\n        (0.05, 2.0, 10),     # Small budget, moderate sharpness\n        (0.3, 2.0, 10),      # Large budget\n        (0.15, 10.0, 10),    # Steep surrogate\n        (0.15, 0.5, 10),     # Smooth surrogate\n        (0.15, 2.0, 1),      # One-step boundary condition\n    ]\n\n    def sigmoid(x):\n        \"\"\"Numerically stable logistic sigmoid function.\"\"\"\n        return 1.0 / (1.0 + np.exp(-x))\n\n    def lif_simulation(x_seq, T, beta, mode, store_history=False):\n        \"\"\"\n        Simulates the LIF SNN for a given input sequence.\n        \n        Args:\n            x_seq (np.ndarray): Input sequence of shape (T, N_in).\n            T (int): Number of time steps.\n            beta (float): Surrogate slope parameter.\n            mode (str): 'exact' for Heaviside spikes, 'surrogate' for sigmoid spikes.\n            store_history (bool): If True, returns intermediate states for BPTT.\n\n        Returns:\n            Tuple[np.ndarray, dict]: Spike counts and history dictionary.\n        \"\"\"\n        v_mem = np.zeros(N_OUT, dtype=np.float64)\n        spike_counts = np.zeros(N_OUT, dtype=np.float64)\n        \n        history = {'v_hat_seq': [], 'z_sur_seq': []} if store_history else None\n\n        for t in range(T):\n            # Input current\n            i_in = W @ x_seq[t]\n            \n            # Pre-reset membrane potential\n            v_hat = LAMBDA * v_mem + i_in\n            \n            # Spike generation\n            if mode == 'exact':\n                spikes = (v_hat >= V_TH).astype(np.float64)\n            elif mode == 'surrogate':\n                spikes = sigmoid(beta * (v_hat - V_TH))\n            else:\n                raise ValueError(\"Invalid mode specified. Must be 'exact' or 'surrogate'.\")\n\n            # Post-reset membrane potential\n            v_mem = v_hat - spikes * V_RESET\n            \n            spike_counts += spikes\n\n            if store_history:\n                history['v_hat_seq'].append(v_hat)\n                history['z_sur_seq'].append(spikes) # Surrogate spikes\n        \n        return spike_counts, history\n\n    def compute_gradients(history, S_sur, T, beta):\n        \"\"\"\n        Computes gradients of the loss w.r.t. the input sequence using BPTT.\n        \n        Args:\n            history (dict): Stored states from the surrogate forward pass.\n            S_sur (np.ndarray): Total surrogate spike counts.\n            T (int): Number of time steps.\n            beta (float): Surrogate slope parameter.\n\n        Returns:\n            np.ndarray: Gradient tensor of shape (T, N_in).\n        \"\"\"\n        v_hat_seq = history['v_hat_seq']\n        z_sur_seq = history['z_sur_seq']\n\n        # Gradient of loss w.r.t. spike counts (logits)\n        exp_S = np.exp(S_sur - np.max(S_sur)) # Stabilized softmax\n        p = exp_S / np.sum(exp_S)\n        y_one_hot = np.zeros(N_OUT, dtype=np.float64)\n        y_one_hot[Y_TRUE_CLASS] = 1.0\n        dLdS = p - y_one_hot\n        \n        grad_x = np.zeros((T, N_IN), dtype=np.float64)\n        \n        # BPTT initialization\n        delta_v_next = np.zeros(N_OUT, dtype=np.float64)\n\n        for t in reversed(range(T)):\n            # Gradient of surrogate spike function sigma'(...)\n            psi_t = beta * z_sur_seq[t] * (1.0 - z_sur_seq[t])\n            \n            # Propagate error back to pre-reset potential\n            term1 = dLdS * psi_t\n            term2 = delta_v_next * (1.0 - psi_t * V_RESET)\n            delta_v_hat_t = term1 + term2\n            \n            # Gradient w.r.t. input x_t\n            grad_x[t] = W.T @ delta_v_hat_t\n            \n            # Propagate error to previous membrane potential for next BPTT step\n            delta_v_next = LAMBDA * delta_v_hat_t\n            \n        return grad_x\n\n    results = []\n    for epsilon, beta, T in test_cases:\n        x_clean_seq = np.tile(X_CLEAN, (T, 1))\n\n        if epsilon == 0.0:\n            x_adv = x_clean_seq\n        else:\n            # 1. Run surrogate forward pass on clean input to get history for BPTT\n            S_sur_clean, history = lif_simulation(x_clean_seq, T, beta, 'surrogate', store_history=True)\n\n            # 2. Compute gradients via BPTT\n            grad_x = compute_gradients(history, S_sur_clean, T, beta)\n\n            # 3. Construct adversarial example using FGSM\n            x_adv = x_clean_seq + epsilon * np.sign(grad_x)\n            x_adv = np.clip(x_adv, 0.0, 1.0)\n\n        # 4. Evaluate margins on the adversarial example\n        # Surrogate margin\n        S_sur_adv, _ = lif_simulation(x_adv, T, beta, 'surrogate')\n        m_sur_adv = S_sur_adv[Y_TRUE_CLASS] - S_sur_adv[1 - Y_TRUE_CLASS]\n\n        # Exact margin\n        S_exact_adv, _ = lif_simulation(x_adv, T, beta, 'exact')\n        m_exact_adv = S_exact_adv[Y_TRUE_CLASS] - S_exact_adv[1 - Y_TRUE_CLASS]\n\n        # 5. Compute the transferability gap\n        gap = m_sur_adv - m_exact_adv\n        results.append(gap)\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}