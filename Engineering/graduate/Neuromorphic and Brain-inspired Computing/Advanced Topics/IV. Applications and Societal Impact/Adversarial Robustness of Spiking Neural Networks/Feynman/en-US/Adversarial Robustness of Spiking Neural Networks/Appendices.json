{
    "hands_on_practices": [
        {
            "introduction": "To craft effective gradient-based adversarial attacks, one must first compute the gradient of a loss function with respect to the network's input. In Spiking Neural Networks (SNNs), this is complicated by the non-differentiable nature of the spike generation mechanism. This exercise  walks you through the essential process of calculating this gradient for a simple SNN by applying the chain rule through time and employing a surrogate gradient, a cornerstone technique in training and analyzing SNNs.",
            "id": "4034865",
            "problem": "Consider a single-layer Spiking Neural Network (SNN) used as a feature extractor for adversarial robustness analysis. There are $N$ input channels indexed by $i \\in \\{1,\\dots,N\\}$ and $T$ discrete time steps indexed by $t \\in \\{1,\\dots,T\\}$. The input spike train is $x_{i,t} \\in \\{0,1\\}$, but for gradient-based analysis of adversarial perturbations, treat $x_{i,t}$ as a real-valued variable. The postsynaptic neuron has membrane potential $u_t \\in \\mathbb{R}$ with discrete-time leaky integration without reset:\n$$\nu_t \\;=\\; \\lambda\\,u_{t-1} \\;+\\; \\sum_{i=1}^{N} w_i\\, x_{i,t}, \\quad u_0 \\;=\\; 0,\n$$\nwhere $\\lambda \\in (0,1)$ is the leak factor and $w_i \\in \\mathbb{R}$ are synaptic weights. The neuron emits a spike $s_t \\in \\{0,1\\}$ by hard thresholding at $\\vartheta \\in \\mathbb{R}$:\n$$\ns_t \\;=\\; H\\!\\left(u_t - \\vartheta\\right),\n$$\nwhere $H(\\cdot)$ is the Heaviside step function. For backpropagation, use a triangular surrogate derivative for the thresholding nonlinearity with width parameter $\\beta > 0$:\n$$\n\\psi_{\\beta}(z) \\;=\\; \\begin{cases}\n\\dfrac{1}{\\beta}\\left(1 - \\dfrac{|z|}{\\beta}\\right), & \\text{if } |z| \\le \\beta,\\\\[6pt]\n0, & \\text{if } |z| > \\beta,\n\\end{cases}\n$$\nand approximate $\\dfrac{\\partial s_t}{\\partial u_t}$ by $\\psi_{\\beta}\\!\\left(u_t - \\vartheta\\right)$.\n\nA linear readout aggregates spikes into a scalar count $S \\;=\\; \\sum_{t=1}^{T} s_t$, then produces $K$ class logits\n$$\nz_k \\;=\\; \\alpha_k\\, S \\;+\\; b_k, \\quad k \\in \\{1,\\dots,K\\},\n$$\nwith $\\alpha_k, b_k \\in \\mathbb{R}$. The predicted class probabilities are given by the softmax\n$$\np_k \\;=\\; \\dfrac{\\exp(z_k)}{\\sum_{\\ell=1}^{K} \\exp(z_{\\ell})},\n$$\nand the training objective is the single-sample cross-entropy loss for the one-hot label vector $y \\in \\{0,1\\}^{K}$,\n$$\n\\mathcal{L} \\;=\\; - \\sum_{k=1}^{K} y_k \\,\\ln p_k.\n$$\n\nWithin this setting, compute the exact analytical expression, in closed form, for the gradient of the loss with respect to an input component $x_{j,\\tau}$ for any fixed $j \\in \\{1,\\dots,N\\}$ and $\\tau \\in \\{1,\\dots,T\\}$:\n$$\n\\dfrac{\\partial \\mathcal{L}}{\\partial x_{j,\\tau}} \\quad \\text{as a function of } \\left(\\lambda, \\{w_i\\}_{i=1}^{N}, \\vartheta, \\beta, \\{\\alpha_k,b_k\\}_{k=1}^{K}, \\{x_{i,t}\\}, \\{u_t\\}\\right).\n$$\n\nYour final answer must be a single closed-form symbolic expression. No numerical approximation or rounding is required. Express your answer without any unit. Avoid any inequalities or equations as the final answer; provide the single requested expression only.",
            "solution": "The problem is valid. It presents a clearly defined, self-contained, and scientifically grounded model of a simple Spiking Neural Network (SNN) and asks for the computation of a specific gradient. The necessary equations, parameters, and initial conditions are provided, and the task is to apply the principles of differential calculus, specifically the chain rule, to derive an analytical expression. The use of a surrogate gradient for the non-differentiable spiking mechanism is a standard technique in the field of training SNNs. The problem is well-posed and allows for a unique, exact solution.\n\nOur objective is to compute the gradient of the loss function $\\mathcal{L}$ with respect to a single input component $x_{j,\\tau}$ at a specific time step $\\tau$ and input channel $j$. This requires applying the chain rule through the computational graph of the network, from the loss back to the input. The computational path is as follows: $\\mathcal{L} \\rightarrow \\{p_k\\} \\rightarrow \\{z_k\\} \\rightarrow S \\rightarrow \\{s_t\\} \\rightarrow \\{u_t\\} \\rightarrow x_{j,\\tau}$.\n\nFirst, we compute the gradient of the cross-entropy loss $\\mathcal{L} = - \\sum_{k=1}^{K} y_k \\ln p_k$ with respect to the logits $z_m$. The probabilities $p_k$ are given by the softmax function $p_k = \\frac{\\exp(z_k)}{\\sum_{\\ell=1}^{K} \\exp(z_{\\ell})}$.\nThe derivative of the loss with respect to a logit $z_m$ is a standard result:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial z_m} = p_m - y_m.\n$$\n\nNext, we find the gradient of the loss with respect to the total spike count $S$. The logits are defined as $z_k = \\alpha_k S + b_k$. Using the chain rule, we sum over all logits $z_k$ that depend on $S$:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial S} = \\sum_{k=1}^{K} \\frac{\\partial \\mathcal{L}}{\\partial z_k} \\frac{\\partial z_k}{\\partial S}.\n$$\nSince $\\frac{\\partial z_k}{\\partial S} = \\alpha_k$, we substitute the previous result:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial S} = \\sum_{k=1}^{K} \\alpha_k (p_k - y_k).\n$$\nThis term is constant with respect to the time index $t$.\n\nNow, we propagate the gradient to the individual spikes $s_t$. The total spike count is $S = \\sum_{t=1}^{T} s_t$. The gradient of the loss with respect to a single spike $s_t$ is:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial s_t} = \\frac{\\partial \\mathcal{L}}{\\partial S} \\frac{\\partial S}{\\partial s_t}.\n$$\nGiven that $\\frac{\\partial S}{\\partial s_t} = 1$, we have:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial s_t} = \\frac{\\partial \\mathcal{L}}{\\partial S} = \\sum_{k=1}^{K} \\alpha_k (p_k - y_k).\n$$\n\nThe next step is to propagate the gradient to the membrane potential $u_t$. The spike $s_t$ is generated by $s_t = H(u_t - \\vartheta)$, where $H(\\cdot)$ is the Heaviside step function. Since $H(\\cdot)$ has a derivative that is zero almost everywhere and undefined at the origin, we use the provided surrogate gradient $\\psi_{\\beta}(z)$. We approximate $\\frac{\\partial s_t}{\\partial u_t} \\approx \\psi_{\\beta}(u_t - \\vartheta)$.\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial u_t} = \\frac{\\partial \\mathcal{L}}{\\partial s_t} \\frac{\\partial s_t}{\\partial u_t} \\approx \\left( \\sum_{k=1}^{K} \\alpha_k (p_k - y_k) \\right) \\psi_{\\beta}(u_t - \\vartheta).\n$$\n\nFinally, we compute the gradient of the loss with respect to the input $x_{j,\\tau}$. An input at time $\\tau$ influences the membrane potential at all subsequent times $t \\ge \\tau$. Therefore, we must sum the contributions of $x_{j,\\tau}$ through all affected potentials $\\{u_t\\}_{t=\\tau}^T$.\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial x_{j,\\tau}} = \\sum_{t=1}^{T} \\frac{\\partial \\mathcal{L}}{\\partial u_t} \\frac{\\partial u_t}{\\partial x_{j,\\tau}}.\n$$\nWe need to determine the Jacobian term $\\frac{\\partial u_t}{\\partial x_{j,\\tau}}$. The membrane potential evolves according to $u_t = \\lambda u_{t-1} + \\sum_{i=1}^{N} w_i x_{i,t}$, with $u_0 = 0$.\nIf $t < \\tau$, $u_t$ does not depend on $x_{j,\\tau}$, so $\\frac{\\partial u_t}{\\partial x_{j,\\tau}} = 0$.\nIf $t = \\tau$, we have $u_\\tau = \\lambda u_{\\tau-1} + \\sum_{i=1}^{N} w_i x_{i,\\tau}$. The term $u_{\\tau-1}$ does not depend on $x_{j,\\tau}$, so:\n$$\n\\frac{\\partial u_\\tau}{\\partial x_{j,\\tau}} = w_j.\n$$\nIf $t > \\tau$, we apply the chain rule to the recurrence relation:\n$$\n\\frac{\\partial u_t}{\\partial x_{j,\\tau}} = \\frac{\\partial}{\\partial x_{j,\\tau}} \\left( \\lambda u_{t-1} + \\sum_{i=1}^{N} w_i x_{i,t} \\right) = \\lambda \\frac{\\partial u_{t-1}}{\\partial x_{j,\\tau}}.\n$$\nThis defines a simple recurrence. We can unroll it to find a closed-form expression for $t \\ge \\tau$:\n$$\n\\frac{\\partial u_t}{\\partial x_{j,\\tau}} = \\lambda \\frac{\\partial u_{t-1}}{\\partial x_{j,\\tau}} = \\lambda^2 \\frac{\\partial u_{t-2}}{\\partial x_{j,\\tau}} = \\dots = \\lambda^{t-\\tau} \\frac{\\partial u_{\\tau}}{\\partial x_{j,\\tau}} = \\lambda^{t-\\tau} w_j.\n$$\nNow we can assemble the final expression for the gradient. The sum over $t$ starts from $\\tau$:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial x_{j,\\tau}} = \\sum_{t=\\tau}^{T} \\frac{\\partial \\mathcal{L}}{\\partial u_t} \\frac{\\partial u_t}{\\partial x_{j,\\tau}} = \\sum_{t=\\tau}^{T} \\left[ \\left( \\sum_{k=1}^{K} \\alpha_k (p_k - y_k) \\right) \\psi_{\\beta}(u_t - \\vartheta) \\right] \\left( \\lambda^{t-\\tau} w_j \\right).\n$$\nWe can factor out the terms that do not depend on the summation index $t$:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial x_{j,\\tau}} = w_j \\left( \\sum_{k=1}^{K} \\alpha_k (p_k - y_k) \\right) \\left( \\sum_{t=\\tau}^{T} \\lambda^{t-\\tau} \\psi_{\\beta}(u_t - \\vartheta) \\right).\n$$\nThis is the final closed-form expression for the gradient of the loss with respect to the input component $x_{j,\\tau}$.",
            "answer": "$$\n\\boxed{w_j \\left( \\sum_{k=1}^{K} \\alpha_k (p_k - y_k) \\right) \\left( \\sum_{t=\\tau}^{T} \\lambda^{t-\\tau} \\psi_{\\beta}(u_t - \\vartheta) \\right)}\n$$"
        },
        {
            "introduction": "While surrogate gradients enable the creation of adversarial examples, these attacks are optimized for a differentiable approximation of the true network dynamics. A critical question is how well these attacks transfer to the exact, non-differentiable Spiking Neural Network (SNN). This hands-on coding practice  guides you through a complete experimental protocol to generate an attack using the Fast Gradient Sign Method on a surrogate model and then measure the resulting \"transferability gap\" on the true SNN, offering a direct look at the practical challenges of attacking spiking systems.",
            "id": "4034858",
            "problem": "You are tasked with implementing a complete, runnable program that quantifies the transferability gap between adversarial examples crafted using surrogate gradients and their true impact on a Spiking Neural Network (SNN). The experiment protocol must be specified mathematically and then instantiated in code. The target system is a two-output, single-layer network of Leaky Integrate-and-Fire (LIF) neurons receiving a time series of input currents, where adversarial perturbations are generated using a surrogate gradient model and evaluated under exact spike dynamics.\n\nFundamental basis and definitions:\n- A Spiking Neural Network (SNN) is a dynamical system of neurons in which each neuron emits spikes when its membrane potential crosses a threshold. Each neuron is modeled as a Leaky Integrate-and-Fire (LIF) unit with the following discrete-time update per time step $t$:\n  - The pre-reset membrane potential is $v^{\\text{hat}}_{t,k} = \\lambda v_{t,k} + \\sum_{i=1}^{N_{\\text{in}}} W_{k,i} x_{t,i}$ for output neuron index $k$, leak factor $\\lambda \\in (0,1)$, weight matrix $W \\in \\mathbb{R}^{N_{\\text{out}} \\times N_{\\text{in}}}$, and input $x_t \\in \\mathbb{R}^{N_{\\text{in}}}$.\n  - The exact spike $z_{t,k}$ is $z_{t,k} = H(v^{\\text{hat}}_{t,k} - V_{\\text{th}})$, where $H(\\cdot)$ is the Heaviside step function and $V_{\\text{th}}$ is the threshold.\n  - The post-reset membrane potential is $v_{t+1,k} = v^{\\text{hat}}_{t,k} - z_{t,k} V_{\\text{reset}}$, where $V_{\\text{reset}}$ is the reset decrement applied upon spiking.\n  - The spike count over a horizon of $T$ steps for output $k$ is $S_k = \\sum_{t=0}^{T-1} z_{t,k}$.\n- Classification is defined as the index $k$ with the largest spike count $S_k$, i.e., the $\\operatorname{argmax}$ over outputs. The loss for a true class index $y$ is the cross-entropy between the softmax of counts and a one-hot distribution: $L = -\\log\\left(\\exp(S_y) / \\sum_{j=1}^{N_{\\text{out}}} \\exp(S_j)\\right)$.\n- Surrogate gradient modeling replaces the non-differentiable $H(\\cdot)$ by a smooth function. We define the surrogate spike as $z^{\\text{sur}}_{t,k} = \\sigma(\\beta (v^{\\text{hat}}_{t,k} - V_{\\text{th}}))$ where $\\sigma(\\cdot)$ is the logistic function and $\\beta > 0$ is a slope parameter controlling the sharpness of the surrogate transition.\n- The adversarial perturbation is generated using the Fast Gradient Sign Method (FGSM) computed under the surrogate dynamics. For a perturbation budget $\\epsilon$, the input sequence $x$ is perturbed to $\\tilde{x}$ by $\\tilde{x}_{t,i} = \\Pi_{[0,1]}(x_{t,i} + \\epsilon \\cdot \\operatorname{sign}(\\nabla_{x_{t,i}} L))$, where $\\Pi_{[0,1]}(\\cdot)$ projects to the valid input box $[0,1]$ elementwise.\n\nTransferability gap metric:\n- The surrogate margin after attack is defined as $\\tilde{m}^{\\text{sur}} = \\tilde{S}^{\\text{sur}}_{y} - \\max_{k \\neq y} \\tilde{S}^{\\text{sur}}_{k}$, where $\\tilde{S}^{\\text{sur}}_{k}$ is the surrogate spike count under the surrogate dynamics applied to the adversarial input $\\tilde{x}$.\n- The exact margin after attack is defined as $\\tilde{m}^{\\text{exact}} = \\tilde{S}^{\\text{exact}}_{y} - \\max_{k \\neq y} \\tilde{S}^{\\text{exact}}_{k}$, where $\\tilde{S}^{\\text{exact}}_{k}$ is the exact spike count under Heaviside dynamics applied to the adversarial input $\\tilde{x}$.\n- The transferability gap is $g = \\tilde{m}^{\\text{sur}} - \\tilde{m}^{\\text{exact}}$. A positive $g$ indicates that the surrogate dynamics overestimate the attack effect relative to exact spike dynamics.\n\nFixed model and data for reproducibility:\n- Number of inputs is $N_{\\text{in}} = 4$, number of outputs is $N_{\\text{out}} = 2$.\n- The weight matrix is fixed to $W = \\begin{bmatrix} 0.9 & 0.9 & -0.9 & -0.9 \\\\ -0.9 & -0.9 & 0.9 & 0.9 \\end{bmatrix}$.\n- Leak factor is $\\lambda = 0.9$, threshold is $V_{\\text{th}} = 1.0$, reset is $V_{\\text{reset}} = 1.0$, true class index is $y = 0$.\n- The clean input sequence is a constant vector per time step: for each $t$, $x_t = [0.6, 0.6, 0.4, 0.4]$.\n- Inputs are elementwise constrained to $[0,1]$.\n\nYour program must:\n- Implement exact spike dynamics and surrogate dynamics for the specified LIF model.\n- Compute the cross-entropy loss under surrogate dynamics using spike counts as logits, then backpropagate through time using the surrogate derivative to obtain $\\nabla_x L$.\n- Construct a single-step FGSM adversarial example under surrogate dynamics with budget $\\epsilon$ and project to $[0,1]$.\n- Evaluate $\\tilde{m}^{\\text{sur}}$ and $\\tilde{m}^{\\text{exact}}$ on the adversarial input and report the transferability gap $g$ for each test case.\n\nTest suite:\n- Each test case is a tuple $(\\epsilon,\\beta,T)$ where $\\epsilon$ is the FGSM budget, $\\beta$ is the surrogate slope, and $T$ is the number of time steps.\n- Use the following test cases:\n  - $(0.0, 2.0, 10)$ representing no attack.\n  - $(0.05, 2.0, 10)$ representing a small budget with moderate sharpness.\n  - $(0.3, 2.0, 10)$ representing a large budget.\n  - $(0.15, 10.0, 10)$ representing a steep surrogate.\n  - $(0.15, 0.5, 10)$ representing a smooth surrogate.\n  - $(0.15, 2.0, 1)$ representing a one-step boundary condition.\n\nFinal output specification:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must be in the same order as the test suite above, and each element must be the transferability gap $g$ as a floating-point number. For example, an output line like $[g_1,g_2,g_3,g_4,g_5,g_6]$ where each $g_i$ corresponds to the test case in the same order as listed.",
            "solution": "The problem is valid. It is a well-posed, scientifically grounded computational task within the domain of adversarial machine learning for neuromorphic systems. All parameters, models, and objectives are defined with sufficient mathematical rigor to permit a unique solution.\n\nThe core of this problem is to quantify the discrepancy between a surrogate model used for gradient-based adversarial attacks and the true, non-differentiable dynamics of a Spiking Neural Network (SNN). This discrepancy is termed the \"transferability gap.\" The solution involves a multi-step computational process grounded in the principles of neural dynamics, automatic differentiation, and adversarial machine learning.\n\n**1. Leaky Integrate-and-Fire (LIF) Neuron Dynamics**\nThe fundamental component of the SNN is the Leaky Integrate-and-Fire (LIF) neuron. Its behavior is governed by a set of discrete-time dynamical equations. For each output neuron $k \\in \\{0, ..., N_{\\text{out}}-1\\}$ at time step $t \\in \\{0, ..., T-1\\}$, the membrane potential evolves as follows:\n- The pre-reset potential $v^{\\text{hat}}_{t,k}$ integrates the decayed potential from the previous step and the weighted inputs:\n$$v^{\\text{hat}}_{t,k} = \\lambda v_{t,k} + \\sum_{i=1}^{N_{\\text{in}}} W_{k,i} x_{t,i}$$\nHere, $\\lambda \\in (0,1)$ is the leak factor, $v_{t,k}$ is the membrane potential after a potential reset at step $t-1$ (with $v_{0,k} = 0$), $W$ is the weight matrix, and $x_t$ is the input vector at time $t$.\n\n- A spike is emitted if the potential crosses a threshold $V_{\\text{th}}$. This is a non-differentiable event, modeled by the Heaviside step function $H(\\cdot)$:\n$$z_{t,k} = H(v^{\\text{hat}}_{t,k} - V_{\\text{th}})$$\nwhere $z_{t,k} \\in \\{0, 1\\}$.\n\n- Upon spiking, the potential is reduced by $V_{\\text{reset}}$ (reset-by-subtraction mechanism):\n$$v_{t+1,k} = v^{\\text{hat}}_{t,k} - z_{t,k} V_{\\text{reset}}$$\n\n- The total spike count for neuron $k$ over the time horizon $T$ is $S_k = \\sum_{t=0}^{T-1} z_{t,k}$.\n\n**2. Surrogate Gradient Principle**\nAdversarial attacks like the Fast Gradient Sign Method (FGSM) require the gradient of a loss function with respect to the input, $\\nabla_x L$. The Heaviside function in the exact spike generation is non-differentiable, making gradient computation impossible. To overcome this, we employ a surrogate model where the Heaviside function is replaced by a smooth approximation. The problem specifies the logistic function $\\sigma(\\cdot)$:\n$$z^{\\text{sur}}_{t,k} = \\sigma(\\beta (v^{\\text{hat}}_{t,k} - V_{\\text{th}})) = \\frac{1}{1 + \\exp(-\\beta (v^{\\text{hat}}_{t,k} - V_{\\text{th}}))}$$\nThe parameter $\\beta > 0$ controls the steepness of the surrogate, approximating the Heaviside function more closely as $\\beta \\to \\infty$. All subsequent dynamics, including the reset mechanism and spike counting, use this continuous-valued \"surrogate spike\" $z^{\\text{sur}}_{t,k}$ to maintain differentiability.\n\n**3. Gradient Computation via Backpropagation Through Time (BPTT)**\nTo compute $\\nabla_{x_t} L$, we must differentiate the cross-entropy loss through the unrolled temporal dynamics of the surrogate SNN. The loss is $L = -\\log\\left(\\exp(S^{\\text{sur}}_y) / \\sum_{j} \\exp(S^{\\text{sur}}_j)\\right)$, where $y$ is the true class index. The gradient of the loss with respect to the surrogate spike counts $S^{\\text{sur}}_k$ is $\\frac{\\partial L}{\\partial S^{\\text{sur}}_k} = p_k - \\delta_{ky}$, where $p_k$ are the softmax probabilities and $\\delta_{ky}$ is the Kronecker delta.\n\nThe BPTT algorithm propagates error signals backward in time. Let $\\delta v^{\\text{hat}}_{t,k} = \\frac{\\partial L}{\\partial v^{\\text{hat}}_{t,k}}$ and $\\delta v_{t,k} = \\frac{\\partial L}{\\partial v_{t,k}}$ be the gradients of the loss with respect to the potentials. The recurrence relations are:\n- $\\delta v^{\\text{hat}}_{t,k} = \\left(\\frac{\\partial L}{\\partial S^{\\text{sur}}_k} \\psi_{t,k}\\right) + \\delta v_{t+1,k} (1 - \\psi_{t,k} V_{\\text{reset}})$\n- $\\delta v_{t,k} = \\lambda \\delta v^{\\text{hat}}_{t,k}$\nwhere $\\psi_{t,k} = \\frac{\\partial z^{\\text{sur}}_{t,k}}{\\partial v^{\\text{hat}}_{t,k}} = \\beta \\sigma'(\\beta(v^{\\text{hat}}_{t,k} - V_{\\text{th}}))$. The backward pass starts with $\\delta v_{T,k} = 0$.\nThe gradient with respect to the input at time $t$ is then found via the chain rule:\n$$\\nabla_{x_{t}} L = W^T \\delta v^{\\text{hat}}_{t}$$\n\n**4. Adversarial Attack Generation (FGSM)**\nWith the gradient $\\nabla_x L$ computed from the surrogate model, we can craft an adversarial input $\\tilde{x}$ using the single-step FGSM:\n$$\\tilde{x}_{t,i} = \\Pi_{[0,1]}\\left(x_{t,i} + \\epsilon \\cdot \\text{sign}((\\nabla_x L)_{t,i})\\right)$$\nwhere $\\epsilon$ is the perturbation budget and $\\Pi_{[0,1]}(\\cdot)$ is a clipping function that projects the input back into the valid range $[0, 1]$.\n\n**5. Evaluating the Transferability Gap**\nThe final step is to measure the effectiveness of the attack. We calculate the decision margin, defined as the spike count of the correct class minus the maximum spike count of any incorrect class. Since $N_{\\text{out}}=2$ and $y=0$, this simplifies to $S_0 - S_1$. We compute this margin for the adversarial input $\\tilde{x}$ under both the surrogate and exact dynamics:\n- Surrogate margin: $\\tilde{m}^{\\text{sur}} = \\tilde{S}^{\\text{sur}}_{0} - \\tilde{S}^{\\text{sur}}_{1}$\n- Exact margin: $\\tilde{m}^{\\text{exact}} = \\tilde{S}^{\\text{exact}}_{0} - \\tilde{S}^{\\text{exact}}_{1}$\n\nThe transferability gap $g = \\tilde{m}^{\\text{sur}} - \\tilde{m}^{\\text{exact}}$ quantifies the degree to which the surrogate model's prediction of the attack's success deviates from the actual outcome in the true SNN. A positive $g$ indicates that the surrogate model overestimates the attack's effectiveness.\n\nThe implementation will systematically follow these steps for each test case, encapsulating the dynamics and gradient calculations in dedicated functions.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the transferability gap for adversarial examples in a Spiking Neural Network.\n    \"\"\"\n\n    # --- Fixed model and data for reproducibility ---\n    N_IN = 4\n    N_OUT = 2\n    W = np.array([\n        [0.9, 0.9, -0.9, -0.9],\n        [-0.9, -0.9, 0.9, 0.9]\n    ], dtype=np.float64)\n    LAMBDA = 0.9\n    V_TH = 1.0\n    V_RESET = 1.0\n    Y_TRUE_CLASS = 0\n    X_CLEAN = np.array([0.6, 0.6, 0.4, 0.4], dtype=np.float64)\n    \n    # --- Test suite ---\n    test_cases = [\n        (0.0, 2.0, 10),      # No attack\n        (0.05, 2.0, 10),     # Small budget, moderate sharpness\n        (0.3, 2.0, 10),      # Large budget\n        (0.15, 10.0, 10),    # Steep surrogate\n        (0.15, 0.5, 10),     # Smooth surrogate\n        (0.15, 2.0, 1),      # One-step boundary condition\n    ]\n\n    def sigmoid(x):\n        \"\"\"Numerically stable logistic sigmoid function.\"\"\"\n        return 1.0 / (1.0 + np.exp(-x))\n\n    def lif_simulation(x_seq, T, beta, mode, store_history=False):\n        \"\"\"\n        Simulates the LIF SNN for a given input sequence.\n        \n        Args:\n            x_seq (np.ndarray): Input sequence of shape (T, N_in).\n            T (int): Number of time steps.\n            beta (float): Surrogate slope parameter.\n            mode (str): 'exact' for Heaviside spikes, 'surrogate' for sigmoid spikes.\n            store_history (bool): If True, returns intermediate states for BPTT.\n\n        Returns:\n            Tuple[np.ndarray, dict]: Spike counts and history dictionary.\n        \"\"\"\n        v_mem = np.zeros(N_OUT, dtype=np.float64)\n        spike_counts = np.zeros(N_OUT, dtype=np.float64)\n        \n        history = {'v_hat_seq': [], 'z_sur_seq': []} if store_history else None\n\n        for t in range(T):\n            # Input current\n            i_in = W @ x_seq[t]\n            \n            # Pre-reset membrane potential\n            v_hat = LAMBDA * v_mem + i_in\n            \n            # Spike generation\n            if mode == 'exact':\n                spikes = (v_hat >= V_TH).astype(np.float64)\n            elif mode == 'surrogate':\n                spikes = sigmoid(beta * (v_hat - V_TH))\n            else:\n                raise ValueError(\"Invalid mode specified. Must be 'exact' or 'surrogate'.\")\n\n            # Post-reset membrane potential\n            v_mem = v_hat - spikes * V_RESET\n            \n            spike_counts += spikes\n\n            if store_history:\n                history['v_hat_seq'].append(v_hat)\n                history['z_sur_seq'].append(spikes) # Surrogate spikes\n        \n        return spike_counts, history\n\n    def compute_gradients(history, S_sur, T, beta):\n        \"\"\"\n        Computes gradients of the loss w.r.t. the input sequence using BPTT.\n        \n        Args:\n            history (dict): Stored states from the surrogate forward pass.\n            S_sur (np.ndarray): Total surrogate spike counts.\n            T (int): Number of time steps.\n            beta (float): Surrogate slope parameter.\n\n        Returns:\n            np.ndarray: Gradient tensor of shape (T, N_in).\n        \"\"\"\n        v_hat_seq = history['v_hat_seq']\n        z_sur_seq = history['z_sur_seq']\n\n        # Gradient of loss w.r.t. spike counts (logits)\n        exp_S = np.exp(S_sur - np.max(S_sur)) # Stabilized softmax\n        p = exp_S / np.sum(exp_S)\n        y_one_hot = np.zeros(N_OUT, dtype=np.float64)\n        y_one_hot[Y_TRUE_CLASS] = 1.0\n        dLdS = p - y_one_hot\n        \n        grad_x = np.zeros((T, N_IN), dtype=np.float64)\n        \n        # BPTT initialization\n        delta_v_next = np.zeros(N_OUT, dtype=np.float64)\n\n        for t in reversed(range(T)):\n            # Gradient of surrogate spike function sigma'(...)\n            psi_t = beta * z_sur_seq[t] * (1.0 - z_sur_seq[t])\n            \n            # Propagate error back to pre-reset potential\n            term1 = dLdS * psi_t\n            term2 = delta_v_next * (1.0 - psi_t * V_RESET)\n            delta_v_hat_t = term1 + term2\n            \n            # Gradient w.r.t. input x_t\n            grad_x[t] = W.T @ delta_v_hat_t\n            \n            # Propagate error to previous membrane potential for next BPTT step\n            delta_v_next = LAMBDA * delta_v_hat_t\n            \n        return grad_x\n\n    results = []\n    for epsilon, beta, T in test_cases:\n        x_clean_seq = np.tile(X_CLEAN, (T, 1))\n\n        if epsilon == 0.0:\n            x_adv = x_clean_seq\n        else:\n            # 1. Run surrogate forward pass on clean input to get history for BPTT\n            S_sur_clean, history = lif_simulation(x_clean_seq, T, beta, 'surrogate', store_history=True)\n\n            # 2. Compute gradients via BPTT\n            grad_x = compute_gradients(history, S_sur_clean, T, beta)\n\n            # 3. Construct adversarial example using FGSM\n            x_adv = x_clean_seq + epsilon * np.sign(grad_x)\n            x_adv = np.clip(x_adv, 0.0, 1.0)\n\n        # 4. Evaluate margins on the adversarial example\n        # Surrogate margin\n        S_sur_adv, _ = lif_simulation(x_adv, T, beta, 'surrogate')\n        m_sur_adv = S_sur_adv[Y_TRUE_CLASS] - S_sur_adv[1 - Y_TRUE_CLASS]\n\n        # Exact margin\n        S_exact_adv, _ = lif_simulation(x_adv, T, beta, 'exact')\n        m_exact_adv = S_exact_adv[Y_TRUE_CLASS] - S_exact_adv[1 - Y_TRUE_CLASS]\n\n        # 5. Compute the transferability gap\n        gap = m_sur_adv - m_exact_adv\n        results.append(gap)\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Beyond understanding attack mechanisms, a key goal in adversarial robustness is to design networks that are inherently more resilient. Architectural choices play a crucial role in this endeavor, as different operations can either amplify or dampen the effect of perturbations. This exercise  presents a focused analysis comparing two common pooling strategies, max pooling and average pooling, to determine which is more robust against sparse adversarial spike insertions, highlighting how fundamental design principles can form a first line of defense.",
            "id": "4034876",
            "problem": "A Spiking Neural Network (SNN) layer produces presynaptic spike trains $\\{s_i(t)\\}_{i=1}^n$ over a fixed observation window $t \\in [0,T]$, where each $s_i(t)$ is a sum of Dirac impulses representing spikes. Define the spike count $c_i$ of unit $i$ by $c_i = \\int_0^T s_i(t)\\,\\mathrm{d}t$, so that each spike contributes $1$ to $c_i$. Consider a pooling region of size $n$, and two ($2$) spiking-form pooling functions applied to $\\{c_i\\}_{i=1}^n$:\n- Max pooling: $y_{\\max} = \\max_{1 \\le i \\le n} c_i$.\n- Average pooling: $y_{\\mathrm{avg}} = \\frac{1}{n} \\sum_{i=1}^n c_i$.\n\nAssume a worst-case adversary who can insert sparse events by adding a nonnegative integer vector $a \\in \\mathbb{Z}_{\\ge 0}^n$ to the counts, yielding perturbed counts $c_i' = c_i + a_i$, subject to an $\\ell_1$ budget $\\|a\\|_1 = \\sum_{i=1}^n a_i \\le s$, where $s$ is the total number of inserted spikes across the pooling region during $[0,T]$. There is no upper-bound saturation on $c_i$ within the window.\n\nThe downstream decision stage compares the pooled value $y$ to a fixed threshold $\\theta$: a detection event is declared if $y \\ge \\theta$. For an unperturbed pooled value $y$ with margin $\\Delta = \\theta - y > 0$, the adversary seeks to minimally raise $y$ to at least $\\theta$ by spike insertions. Under this model and the stated definitions, which option best characterizes the relative adversarial robustness of max pooling versus average pooling to sparse event insertions within a single pooling region?\n\nA. In the worst case, average pooling requires at least $n$ times more inserted spikes than max pooling to cross the same downstream threshold, implying average pooling is more robust to sparse insertions.\n\nB. Max pooling is more robust because it discards all but the largest count; a sparse adversary cannot significantly change the largest count without affecting most units, so more inserted spikes are needed.\n\nC. Max pooling and average pooling are equally robust under spike insertions because both pooling functions are monotone in the spike counts.\n\nD. Robustness to sparse spike insertions is determined only by postsynaptic leak and refractory dynamics and does not depend on whether max or average pooling is used.",
            "solution": "The problem statement is critically evaluated and found to be valid. It is scientifically grounded within the standard theoretical framework of simplified neural network models, well-posed with a clear objective, objective in its language, and internally consistent. All necessary definitions for a formal analysis are provided.\n\nThe problem asks for a comparison of the adversarial robustness of max pooling versus average pooling in a Spiking Neural Network (SNN) context. Robustness is implicitly quantified by the minimum number of adversarially inserted spikes required to cause a misclassification, i.e., to make the pooled output reach a threshold $\\theta$.\n\nLet the initial spike counts in the pooling region of size $n$ be the vector $c = (c_1, c_2, \\dots, c_n)$, where $c_i \\in \\mathbb{Z}_{\\ge 0}$ are integers. The adversary adds a perturbation vector $a = (a_1, a_2, \\dots, a_n)$, where $a_i \\in \\mathbb{Z}_{\\ge 0}$, resulting in perturbed counts $c' = c + a$. The total number of inserted spikes is the $\\ell_1$ norm of $a$, denoted by $S = \\|a\\|_1 = \\sum_{i=1}^n a_i$.\n\nThe initial state is such that the pooled output $y$ is below the threshold $\\theta$. That is, there is a margin $\\Delta = \\theta - y > 0$. The adversary's goal is to find the minimum integer $S$ such that the new pooled output $y'$ satisfies $y' \\ge \\theta$. For the analysis to be tractable and consistent with the problem's integer-based definitions ($c_i, a_i$), we assume the threshold $\\theta$ is also an integer.\n\n**Case 1: Max Pooling**\n\nThe output is given by $y_{\\max} = \\max_{1 \\le i \\le n} c_i$. The initial state satisfies $y_{\\max} < \\theta$.\nThe perturbed output is $y'_{\\max} = \\max_{1 \\le i \\le n} (c_i + a_i)$.\nThe adversary's goal is to achieve $y'_{\\max} \\ge \\theta$. This requires that for at least one neuron $j$, the perturbed count satisfies $c_j + a_j \\ge \\theta$.\n\nTo minimize the total number of inserted spikes $S = \\sum a_i$, the adversary should employ the most efficient strategy. This involves identifying the neuron that requires the fewest added spikes to reach the threshold and concentrating all perturbations on that single neuron. The number of spikes required to push neuron $j$ to the threshold is $a_j = \\theta - c_j$ (since $c_j < \\theta$, and all quantities are integers). To minimize this cost, the adversary should choose the neuron $k$ with the highest initial spike count, i.e., $c_k = \\max_{i} c_i = y_{\\max}$.\n\nBy setting $a_k = \\theta - c_k$ and $a_i = 0$ for all $i \\ne k$, the adversary achieves the goal with the minimum possible total number of spikes. Let this minimum be $S_{\\max}^{\\min}$.\n$$S_{\\max}^{\\min} = \\theta - c_k = \\theta - \\max_{1 \\le i \\le n} c_i = \\theta - y_{\\max}$$\n\n**Case 2: Average Pooling**\n\nThe output is given by $y_{\\mathrm{avg}} = \\frac{1}{n} \\sum_{i=1}^n c_i$. The initial state satisfies $y_{\\mathrm{avg}} < \\theta$.\nThe perturbed output is $y'_{\\mathrm{avg}} = \\frac{1}{n} \\sum_{i=1}^n (c_i + a_i)$.\nThe adversary's goal is to achieve $y'_{\\mathrm{avg}} \\ge \\theta$.\n$$ \\frac{1}{n} \\sum_{i=1}^n (c_i + a_i) \\ge \\theta $$\nRearranging the inequality:\n$$ \\sum_{i=1}^n c_i + \\sum_{i=1}^n a_i \\ge n \\theta $$\n$$ \\sum_{i=1}^n a_i \\ge n \\theta - \\sum_{i=1}^n c_i $$\nThe quantity on the left is the total number of spikes, $S$. The quantity on the right, $n \\theta - \\sum c_i$, is an integer because $n$, $\\theta$, and all $c_i$ are integers. The minimum integer $S$ that satisfies this inequality is the value on the right-hand side. Let this minimum be $S_{\\mathrm{avg}}^{\\min}$.\n$$ S_{\\mathrm{avg}}^{\\min} = n \\theta - \\sum_{i=1}^n c_i $$\nThis can also be written in terms of the initial output $y_{\\mathrm{avg}}$:\n$$ S_{\\mathrm{avg}}^{\\min} = n(\\theta - \\frac{1}{n}\\sum_{i=1}^n c_i) = n(\\theta - y_{\\mathrm{avg}}) $$\n\n**Comparison of Robustness**\n\nTo compare the robustness, we compare the minimum number of spikes required in each case, $S_{\\max}^{\\min}$ and $S_{\\mathrm{avg}}^{\\min}$. We will evaluate the ratio $S_{\\mathrm{avg}}^{\\min} / S_{\\max}^{\\min}$.\n$$ \\frac{S_{\\mathrm{avg}}^{\\min}}{S_{\\max}^{\\min}} = \\frac{n \\theta - \\sum_{i=1}^n c_i}{\\theta - \\max_{1 \\le i \\le n} c_i} $$\nFor any set of non-negative numbers $\\{c_i\\}_{i=1}^n$, it is a fundamental property that the sum is less than or equal to $n$ times the maximum value:\n$$ \\sum_{i=1}^n c_i \\le n \\cdot \\max_{1 \\le i \\le n} c_i $$\nMultiplying by $-1$ reverses the inequality:\n$$ -\\sum_{i=1}^n c_i \\ge -n \\cdot \\max_{1 \\le i \\le n} c_i $$\nAdding $n\\theta$ to both sides yields:\n$$ n \\theta - \\sum_{i=1}^n c_i \\ge n \\theta - n \\cdot \\max_{1 \\le i \\le n} c_i $$\n$$ n \\theta - \\sum_{i=1}^n c_i \\ge n \\left(\\theta - \\max_{1 \\le i \\le n} c_i\\right) $$\nSubstituting the expressions for the minimum spikes:\n$$ S_{\\mathrm{avg}}^{\\min} \\ge n \\cdot S_{\\max}^{\\min} $$\nSince we have an initial margin $\\Delta > 0$, $S_{\\max}^{\\min} = \\theta - y_{\\max} \\ge 1$, so the comparison is meaningful. The equality $S_{\\mathrm{avg}}^{\\min} = n \\cdot S_{\\max}^{\\min}$ holds if and only if $\\sum c_i = n \\cdot \\max c_i$, which occurs if and only if all $c_i$ are equal. If the counts $c_i$ are non-uniform, the inequality is strict: $S_{\\mathrm{avg}}^{\\min} > n \\cdot S_{\\max}^{\\min}$.\n\nThis result demonstrates that average pooling requires at least $n$ times more spikes to be compromised by this type of attack compared to max pooling. Therefore, average pooling is significantly more robust.\n\n**Option-by-Option Analysis**\n\nA. In the worst case, average pooling requires at least $n$ times more inserted spikes than max pooling to cross the same downstream threshold, implying average pooling is more robust to sparse insertions.\n- This statement accurately reflects our derived inequality $S_{\\mathrm{avg}}^{\\min} \\ge n \\cdot S_{\\max}^{\\min}$. The conclusion that average pooling is more robust directly follows from this mathematical fact, as robustness is measured by the magnitude of the perturbation needed to cause failure.\n- **Verdict: Correct.**\n\nB. Max pooling is more robust because it discards all but the largest count; a sparse adversary cannot significantly change the largest count without affecting most units, so more inserted spikes are needed.\n- The conclusion that \"Max pooling is more robust\" is the opposite of our finding. The reasoning provided is flawed: a sparse adversary can be highly effective against max pooling by targeting *only* the single neuron with the maximum count, which is a maximally efficient, single-point attack. It does not require affecting most units.\n- **Verdict: Incorrect.**\n\nC. Max pooling and average pooling are equally robust under spike insertions because both pooling functions are monotone in the spike counts.\n- While both functions are indeed monotone non-decreasing, this property alone does not determine the degree of robustness. The sensitivity of the output to an input change, which is related to the derivative (or discrete difference), is what matters. For max pooling, a single spike can increase the output by $1$, whereas for average pooling, a single spike only increases the output by $1/n$. Their robustness is demonstrably unequal.\n- **Verdict: Incorrect.**\n\nD. Robustness to sparse spike insertions is determined only by postsynaptic leak and refractory dynamics and does not depend on whether max or average pooling is used.\n- This statement introduces concepts (leak, refractory dynamics) that are external to the explicitly defined mathematical model of the problem. The analysis must be conducted within the given framework. Our derivation shows that, within this model, the choice of pooling function is a decisive factor in determining robustness.\n- **Verdict: Incorrect.**",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}