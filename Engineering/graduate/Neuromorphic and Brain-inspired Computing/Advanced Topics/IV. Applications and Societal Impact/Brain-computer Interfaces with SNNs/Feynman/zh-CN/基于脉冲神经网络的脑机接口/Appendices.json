{
    "hands_on_practices": [
        {
            "introduction": "准确地为神经元的放电模式建模是理解神经编码和构建脑机接口的基础。本练习将引导你推导一个基于泊松广义线性模型（GLM）的脉冲序列的对数似然函数，这是统计神经科学中的一个基石。掌握这一推导对于构建和训练能够预测神经活动的模型至关重要，如练习  所示。",
            "id": "4038724",
            "problem": "一个用尖峰神经网络（SNN）构建的脑机接口（BCI）解码器，将在观测窗口 $[0,T]$ 内记录的单个神经元尖峰序列建模为非齐次泊松过程。条件强度函数被假定遵循具有对数连接的广义线性模型（GLM），其中对数强度由 $\\log \\lambda(t) = \\mathbf{w}^{\\top}\\mathbf{x}(t)$ 给出，其中 $\\mathbf{x}(t) \\in \\mathbb{R}^{d}$ 是一个时变协变量向量，源自 SNN 状态和与任务相关的特征，$\\mathbf{w} \\in \\mathbb{R}^{d}$ 是未知的参数向量。设 $[0,T]$ 内的尖峰时间为 $\\{t_{k}\\}_{k=1}^{K}$，并假设非齐次泊松过程的假设成立：事件是单个发生的，在区间 $[t,t+dt)$ 内发生的无穷小概率为 $\\lambda(t)\\,dt$，并且在不相交的区间上是独立的。\n\n从非齐次泊松过程的定义以及在 $[0,T]$ 内观测到事件发生在时间点 $\\{t_{k}\\}$ 的实现的概率由条件强度 $\\lambda(t)$ 决定的基本事实出发，推导在 GLM 模型下观测到的尖峰序列的完整对数似然 $\\mathcal{L}(\\mathbf{w})$。然后，计算最大似然估计所需的梯度 $\\nabla_{\\mathbf{w}} \\mathcal{L}(\\mathbf{w})$。\n\n你的最终答案必须是关于对数似然和相对于 $\\mathbf{w}$ 的梯度的闭式解析表达式。不需要数值近似或四舍五入。",
            "solution": "该问题要求推导一个神经尖峰序列的对数似然函数及其梯度，该序列被建模为具有特定条件强度函数的非齐次泊松过程。\n\n### 步骤 1：问题验证\n\n**提取已知条件：**\n- **过程：** 单个神经元尖峰序列被建模为非齐次泊松过程。\n- **观测窗口：** 在区间 $[0, T]$ 上观测该过程。\n- **尖峰时间：** 观测到的离散尖峰时间为 $\\{t_{k}\\}_{k=1}^{K}$，其中每个 $t_k \\in [0, T]$。\n- **模型：** 使用带有对数连接函数的广义线性模型（GLM）来表示条件强度。\n- **条件强度函数：** 瞬时发放率，或称条件强度，为 $\\lambda(t)$。其对数由 $\\log \\lambda(t) = \\mathbf{w}^{\\top}\\mathbf{x}(t)$ 给出，这意味着 $\\lambda(t) = \\exp(\\mathbf{w}^{\\top}\\mathbf{x}(t))$。\n- **协变量和参数：** $\\mathbf{x}(t) \\in \\mathbb{R}^{d}$ 是一个时变协变量向量，$\\mathbf{w} \\in \\mathbb{R}^{d}$ 是待估计的未知参数向量。\n- **泊松过程假设：**\n    1. 在无穷小区间 $[t, t+dt)$ 内发生单个事件（尖峰）的概率是 $\\lambda(t)\\,dt$。\n    2. 在 $[t, t+dt)$ 内发生多于一个事件的概率是可忽略的，即 $o(dt)$。\n    3. 在不相交的时间区间内，事件发生的数量是独立的。\n\n**根据标准进行验证：**\n- **科学依据：** 该问题牢固地植根于点过程理论和统计建模，这些是分析神经尖峰序列数据的标准且完善的框架。用于尖峰序列的 GLM 是现代计算神经科学的基石。该问题在科学上是合理的。\n- **适定性：** 该问题要求从一套完整的定义和假设中推导出两个特定的数学对象（对数似然及其梯度）。这是一个定义明确且有唯一解的数学问题。\n- **客观性：** 该问题以精确、客观的数学语言陈述。\n- **完整性与一致性：** 所有必要的信息——模型形式、基础过程、数据结构和目标——都已提供。没有矛盾之处。\n- **真实性与可行性：** 该模型是复杂神经动力学的简化，但它是一个科学上合理且被广泛使用的近似。没有物理上不可能的条件。\n\n**结论：** 该问题是有效的。这是神经数据分析领域一个标准且基础的推导。我将继续进行解答。\n\n### 步骤 2：对数似然及其梯度的推导\n\n对于在区间 $[0, T]$ 上观测到的、具有一组 $K$ 个事件时间 $\\{t_{k}\\}_{k=1}^{K}$ 的非齐次泊松过程，其似然函数是点过程理论中的一个标准结果。它由两个部分构成：在指定时间 $\\{t_k\\}$ 观测到事件的概率，以及在所有其他时间没有观测到事件的概率。\n\n在时间 $t$ 发生单个事件的概率密度与强度 $\\lambda(t)$ 成正比。对于 $K$ 个独立事件，联合概率密度与事件发生时间的强度乘积成正比。在某个区间内不发生事件的概率与该区间上强度的积分有关。综合这些事实，给定参数向量 $\\mathbf{w}$，观测到尖峰序列 $\\{t_k\\}_{k=1}^K$ 的似然函数 $L$ 为：\n$$\nL(\\mathbf{w}) = \\left( \\prod_{k=1}^{K} \\lambda(t_k | \\mathbf{w}) \\right) \\exp\\left( - \\int_{0}^{T} \\lambda(t | \\mathbf{w}) \\, dt \\right)\n$$\n这里，乘积项解释了已发生的尖峰，指数项解释了尖峰之间、第一个尖峰之前以及最后一个尖峰之后的静默期。\n\n对数似然函数，记为 $\\mathcal{L}(\\mathbf{w})$，是似然函数 $L(\\mathbf{w})$ 的自然对数。取对数将乘积简化为求和，这对于优化更为方便。\n$$\n\\mathcal{L}(\\mathbf{w}) = \\ln(L(\\mathbf{w})) = \\ln\\left( \\left( \\prod_{k=1}^{K} \\lambda(t_k | \\mathbf{w}) \\right) \\exp\\left( - \\int_{0}^{T} \\lambda(t | \\mathbf{w}) \\, dt \\right) \\right)\n$$\n使用对数的性质 $\\ln(ab) = \\ln(a) + \\ln(b)$ 和 $\\ln(e^x) = x$，我们得到：\n$$\n\\mathcal{L}(\\mathbf{w}) = \\sum_{k=1}^{K} \\ln(\\lambda(t_k | \\mathbf{w})) - \\int_{0}^{T} \\lambda(t | \\mathbf{w}) \\, dt\n$$\n现在，我们将问题陈述中提供的特定 GLM 形式代入条件强度。给定 $\\log \\lambda(t) = \\mathbf{w}^{\\top}\\mathbf{x}(t)$，这意味着 $\\lambda(t) = \\exp(\\mathbf{w}^{\\top}\\mathbf{x}(t))$。将此代入通用对数似然表达式中：\n$$\n\\mathcal{L}(\\mathbf{w}) = \\sum_{k=1}^{K} \\ln\\left( \\exp(\\mathbf{w}^{\\top}\\mathbf{x}(t_k)) \\right) - \\int_{0}^{T} \\exp(\\mathbf{w}^{\\top}\\mathbf{x}(t)) \\, dt\n$$\n这简化为对数似然的最终表达式：\n$$\n\\mathcal{L}(\\mathbf{w}) = \\sum_{k=1}^{K} \\mathbf{w}^{\\top}\\mathbf{x}(t_k) - \\int_{0}^{T} \\exp(\\mathbf{w}^{\\top}\\mathbf{x}(t)) \\, dt\n$$\n这是所要求答案的第一部分。\n\n接下来，我们计算对数似然相对于参数向量 $\\mathbf{w}$ 的梯度，记为 $\\nabla_{\\mathbf{w}} \\mathcal{L}(\\mathbf{w})$。梯度是一个偏导数向量，$\\left[ \\frac{\\partial \\mathcal{L}}{\\partial w_1}, \\dots, \\frac{\\partial \\mathcal{L}}{\\partial w_d} \\right]^{\\top}$。我们可以通过对 $\\mathcal{L}(\\mathbf{w})$ 关于向量 $\\mathbf{w}$ 求导来找到它。\n$$\n\\nabla_{\\mathbf{w}} \\mathcal{L}(\\mathbf{w}) = \\nabla_{\\mathbf{w}} \\left( \\sum_{k=1}^{K} \\mathbf{w}^{\\top}\\mathbf{x}(t_k) - \\int_{0}^{T} \\exp(\\mathbf{w}^{\\top}\\mathbf{x}(t)) \\, dt \\right)\n$$\n由于微分的线性性质，我们可以将梯度算子分别应用于每一项。\n\n对于第一项：\n$$\n\\nabla_{\\mathbf{w}} \\left( \\sum_{k=1}^{K} \\mathbf{w}^{\\top}\\mathbf{x}(t_k) \\right) = \\sum_{k=1}^{K} \\nabla_{\\mathbf{w}} (\\mathbf{w}^{\\top}\\mathbf{x}(t_k))\n$$\n线性形式 $\\mathbf{w}^{\\top}\\mathbf{a}$ 相对于 $\\mathbf{w}$ 的梯度就是向量 $\\mathbf{a}$。因此，$\\nabla_{\\mathbf{w}} (\\mathbf{w}^{\\top}\\mathbf{x}(t_k)) = \\mathbf{x}(t_k)$。\n所以，第一项的梯度是：\n$$\n\\sum_{k=1}^{K} \\mathbf{x}(t_k)\n$$\n对于第二项，我们需要对一个积分关于参数求导。由于积分的上下限（$0$ 和 $T$）不依赖于 $\\mathbf{w}$，我们可以使用莱布尼茨积分法则将梯度算子移到积分内部：\n$$\n\\nabla_{\\mathbf{w}} \\left( \\int_{0}^{T} \\exp(\\mathbf{w}^{\\top}\\mathbf{x}(t)) \\, dt \\right) = \\int_{0}^{T} \\nabla_{\\mathbf{w}} \\left( \\exp(\\mathbf{w}^{\\top}\\mathbf{x}(t)) \\right) \\, dt\n$$\n我们使用链式法则计算内部梯度。设 $u(t, \\mathbf{w}) = \\mathbf{w}^{\\top}\\mathbf{x}(t)$。那么我们需要 $\\nabla_{\\mathbf{w}} \\exp(u)$。梯度是 $\\exp(u) \\cdot \\nabla_{\\mathbf{w}} u$。\n$$\n\\nabla_{\\mathbf{w}} u = \\nabla_{\\mathbf{w}} (\\mathbf{w}^{\\top}\\mathbf{x}(t)) = \\mathbf{x}(t)\n$$\n因此，\n$$\n\\nabla_{\\mathbf{w}} \\left( \\exp(\\mathbf{w}^{\\top}\\mathbf{x}(t)) \\right) = \\exp(\\mathbf{w}^{\\top}\\mathbf{x}(t)) \\mathbf{x}(t)\n$$\n将此代回积分：\n$$\n\\nabla_{\\mathbf{w}} \\left( \\int_{0}^{T} \\exp(\\mathbf{w}^{\\top}\\mathbf{x}(t)) \\, dt \\right) = \\int_{0}^{T} \\exp(\\mathbf{w}^{\\top}\\mathbf{x}(t)) \\mathbf{x}(t) \\, dt\n$$\n结合两项的梯度，我们得到对数似然梯度的最终表达式：\n$$\n\\nabla_{\\mathbf{w}} \\mathcal{L}(\\mathbf{w}) = \\sum_{k=1}^{K} \\mathbf{x}(t_k) - \\int_{0}^{T} \\exp(\\mathbf{w}^{\\top}\\mathbf{x}(t)) \\mathbf{x}(t) \\, dt\n$$\n这是所要求答案的第二部分。该表达式也可以用条件强度 $\\lambda(t | \\mathbf{w})$ 来写：\n$$\n\\nabla_{\\mathbf{w}} \\mathcal{L}(\\mathbf{w}) = \\sum_{k=1}^{K} \\mathbf{x}(t_k) - \\int_{0}^{T} \\lambda(t | \\mathbf{w}) \\mathbf{x}(t) \\, dt\n$$\n这个结果有一个直观的解释：梯度通过加上每个尖峰时间的协变量贡献，并减去在整个观测窗口上积分的协变量期望贡献来更新参数 $\\mathbf{w}$。最大似然估计旨在找到一个使该梯度为零的 $\\mathbf{w}$。",
            "answer": "$$\n\\boxed{\n\\begin{aligned}\n\\mathcal{L}(\\mathbf{w}) = \\sum_{k=1}^{K} \\mathbf{w}^{\\top}\\mathbf{x}(t_k) - \\int_{0}^{T} \\exp(\\mathbf{w}^{\\top}\\mathbf{x}(t)) \\, dt \\\\\n\\nabla_{\\mathbf{w}} \\mathcal{L}(\\mathbf{w}) = \\sum_{k=1}^{K} \\mathbf{x}(t_k) - \\int_{0}^{T} \\exp(\\mathbf{w}^{\\top}\\mathbf{x}(t)) \\mathbf{x}(t) \\, dt\n\\end{aligned}\n}\n$$"
        },
        {
            "introduction": "在理想模型之外，真实神经元的放电规律更为复杂。本练习  将探讨如何使用变异系数（$\\mathrm{CV}$）和法诺因子（Fano factor）等标准指标来量化脉冲序列的规律性。通过分析一个包含不应期的神经元模型，你将理解生物物理约束如何影响脉冲的统计特性，并最终影响 BCI 解码的可靠性。",
            "id": "4038749",
            "problem": "一个用脉冲神经网络（SNN）实现的运动脑机接口（BCI）解码器，使用单个神经元在观测窗口内的脉冲计数来估计预期的运动速度。假设该神经元的脉冲序列是一个平稳更新过程，其脉冲间期（ISI）是独立同分布的。脉冲间期随机变量用 $T$ 表示，其均值为 $\\mu_{T}$，标准差为 $\\sigma_{T}$。在持续时间为 $T_{\\mathrm{obs}}$ 的观测窗口内的脉冲计数为 $N(T_{\\mathrm{obs}})$。考虑一个生物物理上合理的绝对不应期机制，其脉冲间期被建模为移位指数分布：\n- 每次脉冲后，存在一个持续时间为 $\\delta$ 的绝对不应期，在此期间不能产生脉冲。\n- 不应期结束后，到下一次脉冲的时间服从速率为 $\\lambda$ 的指数分布。\n\n因此，脉冲间期可以写成 $T = \\delta + X$，其中 $X \\sim \\mathrm{Exp}(\\lambda)$ 在各个间期之间是独立的。对于所研究的BCI，假设参数为 $\\delta = 5$ 毫秒 和 $\\lambda = 40$ 赫兹，并且解码器在长度为 $T_{\\mathrm{obs}} = 100$ 秒的窗口上整合脉冲。仅使用经过充分检验的更新过程理论和概率论的基本原理，计算：\n- 在 $T_{\\mathrm{obs}}$ 很大的渐近状态下的法诺因子 $F = \\mathrm{Var}[N(T_{\\mathrm{obs}})] / \\mathbb{E}[N(T_{\\mathrm{obs}})]$。\n- 脉冲间期分布的变异系数 $\\mathrm{CV} = \\sigma_{T} / \\mu_{T}$。\n\n然后，简要解释您计算出的值与理想泊松情况的偏差对于BCI解码的变异性意味着什么。\n\n使用给定的参数 $\\delta = 5$ 毫秒 和 $\\lambda = 40$ 赫兹，以及 $T_{\\mathrm{obs}} = 100$ 秒来证明任何渐近近似的合理性。将 $(F,\\ \\mathrm{CV})$ 的最终数值答案按该顺序以行矩阵的形式报告，并四舍五入到四位有效数字。$F$ 和 $\\mathrm{CV}$ 都是无量纲的；在最终报告的值中不要包含单位。在适用情况下，用适当的单位表示任何中间量，但最终报告的 $(F,\\ \\mathrm{CV})$ 必须是无单位的。",
            "solution": "首先将对问题陈述的科学合理性、自洽性和完整性进行验证。\n\n### 问题验证\n\n**步骤 1：提取已知条件**\n- 该系统是一个运动脑机接口（BCI）解码器。\n- 解码器使用一个脉冲神经网络（SNN）。\n- 输入信号是来自单个神经元的脉冲序列。\n- 脉冲序列被建模为一个平稳更新过程。\n- 脉冲间期（ISI）是独立同分布（i.i.d.）的。\n- 脉冲间期随机变量是 $T$。\n- 脉冲间期的均值为 $\\mu_T$，其标准差为 $\\sigma_T$。\n- 持续时间为 $T_{\\mathrm{obs}}$ 的观测窗口内的脉冲计数用 $N(T_{\\mathrm{obs}})$ 表示。\n- 脉冲间期分布是移位指数分布：$T = \\delta + X$，其中 $X \\sim \\mathrm{Exp}(\\lambda)$。\n- 参数 $\\delta$ 是绝对不应期。\n- 参数 $\\lambda$ 是指数分量的速率。\n- 随机变量 $X$ 在各个间期之间是独立的。\n- 给定参数值：$\\delta = 5$ 毫秒，$\\lambda = 40$ 赫兹， $T_{\\mathrm{obs}} = 100$ 秒。\n- 要求计算：\n    1.  在 $T_{\\mathrm{obs}}$ 很大的渐近状态下的法诺因子 $F = \\mathrm{Var}[N(T_{\\mathrm{obs}})] / \\mathbb{E}[N(T_{\\mathrm{obs}})]$。\n    2.  脉冲间期分布的变异系数 $\\mathrm{CV} = \\sigma_{T} / \\mu_{T}$。\n- 要求解释：计算出的 $F$ 和 $\\mathrm{CV}$ 值相对于理想泊松情况对于BCI解码变异性的意义。\n- 要求输出格式：一个行矩阵 $(F, \\mathrm{CV})$，数值四舍五入到四位有效数字。\n\n**步骤 2：使用提取的已知条件进行验证**\n- **科学依据：** 该问题在计算神经科学和概率论方面有坚实的基础。更新过程是神经元脉冲序列的标准模型。移位指数分布（也称为死时间修正的泊松过程）是一个生物物理上合理的模型，它包含了一个绝对不应期，这是神经元的一个基本属性。法诺因子和变异系数是用于描述脉冲序列变异性的标准统计量度。参数值在生物物理上是现实的。\n- **适定性：** 该问题提供了一个完整的概率模型和所有必要的参数，以唯一地确定所关注的量。法诺因子的渐近公式的使用，因 $T_{\\mathrm{obs}}$ 的值相对于神经动力学的时间尺度很大而被明确证明是合理的。\n- **客观性：** 问题以精确、客观的数学和科学语言陈述，没有歧义或主观论断。\n\n**步骤 3：结论与行动**\n该问题被认为是有效的，因为它是科学合理的、适定的、客观的和完整的。将提供完整的解答。\n\n### 解答\n\n解答过程首先确定脉冲间期（ISI）分布的统计特性，即其均值和方差。然后使用这些量来计算变异系数（$\\mathrm{CV}$）和渐近法诺因子（$F$）。\n\n**1. 单位一致性**\n首先，将所有参数转换为国际单位制（SI）基本单位，以确保计算的一致性。时间单位将是秒（s），速率单位将是赫兹（Hz），相当于 $s^{-1}$。\n- 绝对不应期：$\\delta = 5 \\text{ ms} = 5 \\times 10^{-3} \\text{ s}$。\n- 指数速率：$\\lambda = 40 \\text{ Hz} = 40 \\text{ s}^{-1}$。\n- 观测窗口：$T_{\\mathrm{obs}} = 100 \\text{ s}$。\n\n**2. 脉冲间期（ISI）分布分析**\n脉冲间期由随机变量 $T = \\delta + X$ 给出，其中 $X$ 是一个速率为 $\\lambda$ 的指数分布随机变量。$X$ 的概率密度函数为 $f_X(x) = \\lambda \\exp(-\\lambda x)$，对于 $x \\ge 0$。\n\n指数分量 $X$ 的均值和方差为：\n$$ \\mathbb{E}[X] = \\frac{1}{\\lambda} $$\n$$ \\mathrm{Var}[X] = \\frac{1}{\\lambda^2} $$\n\n利用期望的线性性质，平均脉冲间期 $\\mu_T$ 为：\n$$ \\mu_T = \\mathbb{E}[T] = \\mathbb{E}[\\delta + X] = \\delta + \\mathbb{E}[X] = \\delta + \\frac{1}{\\lambda} $$\n\n脉冲间期的方差 $\\sigma_T^2$ 计算如下：\n$$ \\sigma_T^2 = \\mathrm{Var}[T] = \\mathrm{Var}[\\delta + X] = \\mathrm{Var}[X] = \\frac{1}{\\lambda^2} $$\n常数 $\\delta$ 使分布发生平移，但不影响其方差。因此，脉冲间期的标准差 $\\sigma_T$ 为：\n$$ \\sigma_T = \\sqrt{\\mathrm{Var}[T]} = \\sqrt{\\frac{1}{\\lambda^2}} = \\frac{1}{\\lambda} $$\n\n**3. 变异系数（CV）的计算**\n脉冲间期的变异系数定义为其标准差与其均值的比值：\n$$ \\mathrm{CV} = \\frac{\\sigma_T}{\\mu_T} $$\n代入上面推导出的表达式：\n$$ \\mathrm{CV} = \\frac{1/\\lambda}{\\delta + 1/\\lambda} = \\frac{1}{\\lambda\\delta + 1} $$\n现在，我们代入给定的数值：\n$$ \\lambda\\delta = (40 \\text{ s}^{-1}) \\times (5 \\times 10^{-3} \\text{ s}) = 0.2 $$\n因此，变异系数为：\n$$ \\mathrm{CV} = \\frac{1}{0.2 + 1} = \\frac{1}{1.2} = \\frac{5}{6} \\approx 0.83333... $$\n四舍五入到四位有效数字，$\\mathrm{CV} = 0.8333$。\n\n**4. 渐近法诺因子（F）的计算**\n对于一个平稳更新过程，脉冲计数 $N(t)$ 的法诺因子定义为 $F(t) = \\mathrm{Var}[N(t)] / \\mathbb{E}[N(t)]$，在观测时间很长（$t \\to \\infty$）的渐近极限下，它会趋于一个常数值。这个渐近法诺因子 $F$ 由更新理论的一个基本结果给出：\n$$ F = \\lim_{t \\to \\infty} \\frac{\\mathrm{Var}[N(t)]}{\\mathbb{E}[N(t)]} = \\frac{\\sigma_T^2}{\\mu_T^2} = \\left(\\frac{\\sigma_T}{\\mu_T}\\right)^2 = \\mathrm{CV}^2 $$\n问题要求使用渐近状态，这是合理的，因为观测窗口 $T_{\\mathrm{obs}} = 100 \\text{ s}$ 远大于平均脉冲间期。我们可以验证这一点：\n$$ \\mu_T = \\delta + \\frac{1}{\\lambda} = 5 \\times 10^{-3} \\text{ s} + \\frac{1}{40} \\text{ s} = 0.005 \\text{ s} + 0.025 \\text{ s} = 0.030 \\text{ s} $$\n比值 $T_{\\mathrm{obs}}/\\mu_T = 100 / 0.030 \\approx 3333$，这证实了观测窗口包含大量的脉冲间期，使得渐近近似非常精确。\n\n使用关系式 $F = \\mathrm{CV}^2$：\n$$ F = \\left(\\frac{5}{6}\\right)^2 = \\frac{25}{36} \\approx 0.69444... $$\n四舍五入到四位有效数字， $F = 0.6944$。\n\n**5. 解释**\n理想泊松过程是随机点过程的一个基准。对于泊松过程，脉冲间期服从指数分布（在给定模型中对应于 $\\delta=0$），得出 $\\mathrm{CV} = 1$。泊松过程的脉冲计数 $N(t)$ 服从泊松分布，其方差等于均值，因此对于所有 $t$，法诺因子 $F=1$。\n\n我们计算出的值为 $\\mathrm{CV} \\approx 0.8333$ 和 $F \\approx 0.6944$。\n- **CV的偏差：** $\\mathrm{CV}  1$ 的值表明脉冲序列比泊松过程更规则（其时间上的变异性更小）。绝对不应期 $\\delta$ 阻止了非常短的脉冲间期的出现，从而降低了脉冲间期分布相对于其均值的整体变异性。\n- **F的偏差：** $F  1$ 的值表示脉冲计数过程是“亚泊松的”。这意味着在长窗口内脉冲计数的方差小于具有相同平均发放率的泊松过程的方差。其方差为 $\\mathrm{Var}[N(T_{\\mathrm{obs}})] = F \\cdot \\mathbb{E}[N(T_{\\mathrm{obs}})]  \\mathbb{E}[N(T_{\\mathrm{obs}})]$。\n\n**对BCI解码变异性的意义：** BCI解码器的目标是从离散、带噪声的信号（脉冲计数）中估计一个连续变量（运动速度）。脉冲计数的变异性或“噪声”直接导致解码速度的变异性和误差。$F  1$ 这一事实对BCI性能有利。这意味着神经信号内在地比泊松过程基线所暗示的更可靠（噪声更小）。对于给定的平均脉冲数，方差更低。神经编码中这种较低的内在变异性应能转化为BCI解码器对预期运动的更稳定和精确的估计，从而提高脑机接口的整体性能和可靠性。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.6944  0.8333\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "脑机接口的最后一步是从神经活动中解码出用户的意图。本练习  聚焦于读出机制，特别是如何使用岭回归（ridge regression）来训练一个线性解码器。你将亲手推导其解，并直观地看到正则化如何通过防止过拟合来创建更鲁棒的解码器，这对于构建能够良好泛化的实用 BCI 系统是一个至关重要的概念。",
            "id": "4038766",
            "problem": "一个使用脉冲神经网络（SNN）储备池的脑机接口（BCI）被训练用于使用线性读出从储备池状态中解码一维运动学变量。设储备池状态矩阵为 $X \\in \\mathbb{R}^{4 \\times 2}$，其行向量由下式给出\n$$\nX = \\begin{pmatrix}\n1  0 \\\\\n0  1 \\\\\n1  1 \\\\\n2  1\n\\end{pmatrix},\n$$\n目标输出向量为\n$$\nY = \\begin{pmatrix}\n1 \\\\\n-1 \\\\\n0 \\\\\n2\n\\end{pmatrix}.\n$$\n线性读出使用参数 $\\mathbf{w} \\in \\mathbb{R}^{2}$ 并预测 $\\hat{y}_t = \\mathbf{w}^{\\top} \\mathbf{x}_t$。参数通过最小化岭正则化经验风险来学习\n$$\nJ(\\mathbf{w}) = \\sum_{t=1}^{4} \\left(y_t - \\mathbf{w}^{\\top} \\mathbf{x}_t\\right)^{2} + \\lambda \\|\\mathbf{w}\\|_{2}^{2}.\n$$\n从这个目标函数和第一性原理出发，推导出 $\\mathbf{w}$ 的最优性条件，求解当 $\\lambda = 3$ 时的岭正则化解，并将其与非正则化最小二乘解（对应于 $\\lambda = 0$）进行比较。然后，计算标量比率\n$$\nR = \\frac{\\|\\mathbf{w}_{\\lambda=3}\\|_{2}^{2}}{\\|\\mathbf{w}_{\\lambda=0}\\|_{2}^{2}}.\n$$\n以单个精确数值的形式给出 $R$ 的值。无需四舍五入。使用你执行的计算来解释，在此 BCI SNN 储备池的背景下，正则化如何影响读出层的幅度。",
            "solution": "问题要求推导基于脉冲神经网络（SNN）的脑机接口（BCI）的最优线性读出权重，包括有和没有岭正则化两种情况，并计算它们二范数平方的比率。验证过程确定该问题在科学上是合理的、适定的和完整的。因此，我们可以继续进行求解。\n\n需要最小化的目标函数是岭正则化经验风险，由下式给出：\n$$\nJ(\\mathbf{w}) = \\sum_{t=1}^{4} \\left(y_t - \\mathbf{w}^{\\top} \\mathbf{x}_t\\right)^{2} + \\lambda \\|\\mathbf{w}\\|_{2}^{2}\n$$\n其中 $\\mathbf{w} \\in \\mathbb{R}^{2}$ 是权重向量，$\\mathbf{x}_t$ 是状态矩阵 $X$ 的行向量，$y_t$ 是目标向量 $Y$ 的元素，$\\lambda$ 是正则化参数。\n\n首先，我们用矩阵表示法表示目标函数。求和项是误差向量 $Y - X\\mathbf{w}$ 的欧几里得范数的平方。正则化项是权重向量 $\\mathbf{w}$ 的欧几里得范数的平方。\n$$\nJ(\\mathbf{w}) = \\|Y - X\\mathbf{w}\\|_{2}^{2} + \\lambda \\|\\mathbf{w}\\|_{2}^{2}\n$$\n展开这些项，我们得到：\n$$\nJ(\\mathbf{w}) = (Y - X\\mathbf{w})^{\\top}(Y - X\\mathbf{w}) + \\lambda \\mathbf{w}^{\\top}\\mathbf{w}\n$$\n$$\nJ(\\mathbf{w}) = (Y^{\\top} - \\mathbf{w}^{\\top}X^{\\top})(Y - X\\mathbf{w}) + \\lambda \\mathbf{w}^{\\top}\\mathbf{w}\n$$\n$$\nJ(\\mathbf{w}) = Y^{\\top}Y - Y^{\\top}X\\mathbf{w} - \\mathbf{w}^{\\top}X^{\\top}Y + \\mathbf{w}^{\\top}X^{\\top}X\\mathbf{w} + \\lambda \\mathbf{w}^{\\top}\\mathbf{w}\n$$\n由于 $\\mathbf{w}^{\\top}X^{\\top}Y$ 是一个标量，它等于其转置 $Y^{\\top}X\\mathbf{w}$。因此，表达式简化为：\n$$\nJ(\\mathbf{w}) = Y^{\\top}Y - 2\\mathbf{w}^{\\top}X^{\\top}Y + \\mathbf{w}^{\\top}X^{\\top}X\\mathbf{w} + \\lambda \\mathbf{w}^{\\top}\\mathbf{w}\n$$\n\n为了找到最小化 $J(\\mathbf{w})$ 的最优权重向量 $\\mathbf{w}$，我们必须找到 $J(\\mathbf{w})$ 相对于 $\\mathbf{w}$ 的梯度为零向量的点。最优性条件是 $\\nabla_{\\mathbf{w}} J(\\mathbf{w}) = \\mathbf{0}$。\n使用标准矩阵微积分恒等式 $\\nabla_{\\mathbf{w}}(\\mathbf{a}^{\\top}\\mathbf{w}) = \\mathbf{a}$ 和对于对称矩阵 $A$ 的 $\\nabla_{\\mathbf{w}}(\\mathbf{w}^{\\top}A\\mathbf{w}) = 2A\\mathbf{w}$，我们计算梯度：\n$$\n\\nabla_{\\mathbf{w}} J(\\mathbf{w}) = \\nabla_{\\mathbf{w}}(Y^{\\top}Y) - \\nabla_{\\mathbf{w}}(2\\mathbf{w}^{\\top}X^{\\top}Y) + \\nabla_{\\mathbf{w}}(\\mathbf{w}^{\\top}X^{\\top}X\\mathbf{w}) + \\nabla_{\\mathbf{w}}(\\lambda \\mathbf{w}^{\\top}I\\mathbf{w})\n$$\n$$\n\\nabla_{\\mathbf{w}} J(\\mathbf{w}) = \\mathbf{0} - 2X^{\\top}Y + 2X^{\\top}X\\mathbf{w} + 2\\lambda I\\mathbf{w}\n$$\n将梯度设为零，得到最优性条件：\n$$\n-2X^{\\top}Y + 2X^{\\top}X\\mathbf{w} + 2\\lambda I\\mathbf{w} = \\mathbf{0}\n$$\n$$\n(X^{\\top}X + \\lambda I)\\mathbf{w} = X^{\\top}Y\n$$\n这是岭回归的正规方程的一般形式。$\\mathbf{w}$ 的解是：\n$$\n\\mathbf{w}_{\\lambda} = (X^{\\top}X + \\lambda I)^{-1} X^{\\top}Y\n$$\n\n我们给定的矩阵 $X$ 和 $Y$ 如下：\n$$\nX = \\begin{pmatrix} 1  0 \\\\ 0  1 \\\\ 1  1 \\\\ 2  1 \\end{pmatrix}, \\quad Y = \\begin{pmatrix} 1 \\\\ -1 \\\\ 0 \\\\ 2 \\end{pmatrix}\n$$\n首先，我们计算所需的矩阵乘积 $X^{\\top}X$ 和 $X^{\\top}Y$：\n$$\nX^{\\top}X = \\begin{pmatrix} 1  0  1  2 \\\\ 0  1  1  1 \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 0  1 \\\\ 1  1 \\\\ 2  1 \\end{pmatrix} = \\begin{pmatrix} 1+0+1+4  0+0+1+2 \\\\ 0+0+1+2  0+1+1+1 \\end{pmatrix} = \\begin{pmatrix} 6  3 \\\\ 3  3 \\end{pmatrix}\n$$\n$$\nX^{\\top}Y = \\begin{pmatrix} 1  0  1  2 \\\\ 0  1  1  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\\\ 0 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 1-0+0+4 \\\\ 0-1+0+2 \\end{pmatrix} = \\begin{pmatrix} 5 \\\\ 1 \\end{pmatrix}\n$$\n\n情况1：非正则化最小二乘解（$\\lambda=0$）。\n解 $\\mathbf{w}_{\\lambda=0}$ 由 $\\mathbf{w}_{\\lambda=0} = (X^{\\top}X)^{-1}X^{\\top}Y$ 给出。\n我们求 $X^{\\top}X$ 的逆矩阵：\n$$\n(X^{\\top}X)^{-1} = \\begin{pmatrix} 6  3 \\\\ 3  3 \\end{pmatrix}^{-1} = \\frac{1}{6(3) - 3(3)} \\begin{pmatrix} 3  -3 \\\\ -3  6 \\end{pmatrix} = \\frac{1}{9} \\begin{pmatrix} 3  -3 \\\\ -3  6 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{3}  -\\frac{1}{3} \\\\ -\\frac{1}{3}  \\frac{2}{3} \\end{pmatrix}\n$$\n现在我们求解 $\\mathbf{w}_{\\lambda=0}$：\n$$\n\\mathbf{w}_{\\lambda=0} = \\begin{pmatrix} \\frac{1}{3}  -\\frac{1}{3} \\\\ -\\frac{1}{3}  \\frac{2}{3} \\end{pmatrix} \\begin{pmatrix} 5 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} \\frac{5}{3} - \\frac{1}{3} \\\\ -\\frac{5}{3} + \\frac{2}{3} \\end{pmatrix} = \\begin{pmatrix} \\frac{4}{3} \\\\ -1 \\end{pmatrix}\n$$\n其L2范数的平方为 $\\|\\mathbf{w}_{\\lambda=0}\\|_{2}^{2} = (\\frac{4}{3})^2 + (-1)^2 = \\frac{16}{9} + 1 = \\frac{25}{9}$。\n\n情况2：岭正则化解（$\\lambda=3$）。\n解 $\\mathbf{w}_{\\lambda=3}$ 由 $\\mathbf{w}_{\\lambda=3} = (X^{\\top}X + 3I)^{-1}X^{\\top}Y$ 给出。\n首先，我们计算待求逆的矩阵：\n$$\nX^{\\top}X + 3I = \\begin{pmatrix} 6  3 \\\\ 3  3 \\end{pmatrix} + 3\\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} 6  3 \\\\ 3  3 \\end{pmatrix} + \\begin{pmatrix} 3  0 \\\\ 0  3 \\end{pmatrix} = \\begin{pmatrix} 9  3 \\\\ 3  6 \\end{pmatrix}\n$$\n现在，我们求其逆矩阵：\n$$\n(X^{\\top}X + 3I)^{-1} = \\begin{pmatrix} 9  3 \\\\ 3  6 \\end{pmatrix}^{-1} = \\frac{1}{9(6) - 3(3)} \\begin{pmatrix} 6  -3 \\\\ -3  9 \\end{pmatrix} = \\frac{1}{45} \\begin{pmatrix} 6  -3 \\\\ -3  9 \\end{pmatrix}\n$$\n现在我们求解 $\\mathbf{w}_{\\lambda=3}$：\n$$\n\\mathbf{w}_{\\lambda=3} = \\frac{1}{45} \\begin{pmatrix} 6  -3 \\\\ -3  9 \\end{pmatrix} \\begin{pmatrix} 5 \\\\ 1 \\end{pmatrix} = \\frac{1}{45} \\begin{pmatrix} 30-3 \\\\ -15+9 \\end{pmatrix} = \\frac{1}{45} \\begin{pmatrix} 27 \\\\ -6 \\end{pmatrix} = \\begin{pmatrix} \\frac{27}{45} \\\\ -\\frac{6}{45} \\end{pmatrix} = \\begin{pmatrix} \\frac{3}{5} \\\\ -\\frac{2}{15} \\end{pmatrix}\n$$\n其L2范数的平方为 $\\|\\mathbf{w}_{\\lambda=3}\\|_{2}^{2} = (\\frac{3}{5})^2 + (-\\frac{2}{15})^2 = \\frac{9}{25} + \\frac{4}{225} = \\frac{81}{225} + \\frac{4}{225} = \\frac{85}{225} = \\frac{17}{45}$。\n\n最后，我们计算标量比率 $R$：\n$$\nR = \\frac{\\|\\mathbf{w}_{\\lambda=3}\\|_{2}^{2}}{\\|\\mathbf{w}_{\\lambda=0}\\|_{2}^{2}} = \\frac{\\frac{17}{45}}{\\frac{25}{9}} = \\frac{17}{45} \\cdot \\frac{9}{25} = \\frac{17 \\cdot 9}{45 \\cdot 25} = \\frac{17}{5 \\cdot 25} = \\frac{17}{125}\n$$\n从这个计算中可以明显看出正则化对读出层幅度的影响。正则化权重向量的范数平方 $\\|\\mathbf{w}_{\\lambda=3}\\|_{2}^{2} = \\frac{17}{45} \\approx 0.378$，显著小于非正则化权重向量的范数平方 $\\|\\mathbf{w}_{\\lambda=0}\\|_{2}^{2} = \\frac{25}{9} \\approx 2.778$。比率 $R = \\frac{17}{125} = 0.136$ 量化了这种减小。目标函数中的正则化项 $\\lambda \\|\\mathbf{w}\\|_{2}^{2}$ 惩罚大的权重值。因此，优化过程在拟合数据（最小化误差平方和）和保持权重较小之间找到了一个折衷。这种技术，也称为权重衰减，通过使模型对输入数据（储备池状态）中的噪声不那么敏感，有助于防止过拟合。在BCI的背景下，这可以得到一个更鲁棒的解码模型，该模型能更好地泛化到新的神经数据上。计算明确地表明，引入 $\\lambda=3$ 的惩罚项成功地减小了读出权重的幅度。",
            "answer": "$$\n\\boxed{\\frac{17}{125}}\n$$"
        }
    ]
}