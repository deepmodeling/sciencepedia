{
    "hands_on_practices": [
        {
            "introduction": "At its most fundamental level, attention allows us to discern important information from a noisy and cluttered sensory environment. This exercise models this core function as the selective amplification of a task-relevant signal amidst background noise. By calculating the improvement in the Signal-to-Noise Ratio (SNR), you will quantify one of the most basic yet powerful benefits of attentional gain, providing a foundational understanding of how attention enhances perception .",
            "id": "4051873",
            "problem": "A neuromorphic attention module applies a multiplicative gain to specific feature channels to prioritize task-relevant inputs. Consider a single-channel linear readout modeled as follows. Let the latent stimulus-aligned signal be a zero-mean, second-order stationary process with power $P_{s} = \\mathbb{E}[s^{2}(t)]$ that is independent of noise. The attention mechanism applies a static, nonnegative gain $g \\geq 0$ to the signal path only, producing an output\n$$\ny_{g}(t) = g\\,s(t) + n(t),\n$$\nwhere $n(t)$ is additive white Gaussian noise (AWGN) with zero mean and variance $\\sigma^{2}$ that is independent of $s(t)$ and is not modulated by the gain. The baseline (no-attention) condition corresponds to $g=1$.\n\nUsing the standard definition of signal-to-noise ratio (SNR) as the ratio of signal power to noise power at the output, derive from first principles an exact, closed-form expression for the SNR improvement factor $R(g)$ defined as\n$$\nR(g) \\triangleq \\frac{\\mathrm{SNR}_{\\text{with gain } g}}{\\mathrm{SNR}_{\\text{baseline}}},\n$$\nexpressed solely in terms of $g$. Then, determine the minimal gain threshold $g_{\\text{th}}$ for which the SNR strictly improves relative to baseline.\n\nExpress your final answer as a single row matrix $\\begin{pmatrix} R(g) & g_{\\text{th}} \\end{pmatrix}$. No numerical approximation is required, and no units are needed for the final expression.",
            "solution": "The task is to derive the signal-to-noise ratio (SNR) improvement factor $R(g)$ and the minimal gain threshold $g_{\\text{th}}$ for which the SNR strictly improves. We begin from the first principles laid out in the problem statement.\n\nThe output of the neuromorphic attention module is given by the linear model:\n$$y_{g}(t) = g\\,s(t) + n(t)$$\nwhere $s(t)$ is the stimulus-aligned signal, $n(t)$ is the additive noise, and $g$ is the multiplicative gain applied only to the signal path. The gain is nonnegative, so $g \\geq 0$.\n\nThe problem specifies that the SNR is defined as the ratio of signal power to noise power at the output.\nFirst, let's identify the signal and noise components at the output. Based on the structure of the model, the signal component is $g\\,s(t)$ and the noise component is $n(t)$.\n\nThe power of the signal component at the output, which we denote $P_{\\text{signal}, g}$, is the expected value of its square. Since $g$ is a deterministic constant:\n$$P_{\\text{signal}, g} = \\mathbb{E}\\left[ (g\\,s(t))^{2} \\right] = \\mathbb{E}\\left[ g^{2}s^{2}(t) \\right] = g^{2}\\mathbb{E}\\left[ s^{2}(t) \\right]$$\nThe problem states that the latent signal $s(t)$ is a zero-mean process with power $P_{s} = \\mathbb{E}[s^{2}(t)]$. Substituting this into our expression, we get:\n$$P_{\\text{signal}, g} = g^{2}P_{s}$$\n\nThe power of the noise component at the output, $P_{\\text{noise}, g}$, is the power of the additive noise process $n(t)$. The problem states that $n(t)$ is additive white Gaussian noise (AWGN) with zero mean and variance $\\sigma^{2}$. The power of a zero-mean process is equal to its variance. Therefore:\n$$P_{\\text{noise}, g} = \\mathbb{E}\\left[ n^{2}(t) \\right] = \\sigma^{2}$$\nAs specified, the noise power is not modulated by the gain $g$.\n\nNow, we can express the SNR for a given gain $g$, denoted $\\mathrm{SNR}_{\\text{with gain } g}$, as the ratio of the signal power to the noise power at the output:\n$$\\mathrm{SNR}_{\\text{with gain } g} = \\frac{P_{\\text{signal}, g}}{P_{\\text{noise}, g}} = \\frac{g^{2}P_{s}}{\\sigma^{2}}$$\n\nNext, we establish the baseline SNR. The baseline condition is defined for a gain of $g=1$. Substituting $g=1$ into the general SNR expression yields the baseline SNR, $\\mathrm{SNR}_{\\text{baseline}}$:\n$$\\mathrm{SNR}_{\\text{baseline}} = \\frac{1^{2}P_{s}}{\\sigma^{2}} = \\frac{P_{s}}{\\sigma^{2}}$$\n\nThe SNR improvement factor, $R(g)$, is defined as the ratio of the SNR with gain $g$ to the baseline SNR:\n$$R(g) \\triangleq \\frac{\\mathrm{SNR}_{\\text{with gain } g}}{\\mathrm{SNR}_{\\text{baseline}}}$$\nSubstituting the expressions derived above:\n$$R(g) = \\frac{\\frac{g^{2}P_{s}}{\\sigma^{2}}}{\\frac{P_{s}}{\\sigma^{2}}}$$\nAssuming that the signal power $P_{s}$ and noise power $\\sigma^{2}$ are both greater than zero (which is implicit for a meaningful SNR calculation), we can cancel the term $\\frac{P_{s}}{\\sigma^{2}}$:\n$$R(g) = g^{2}$$\nThis is the required closed-form expression for the SNR improvement factor expressed solely in terms of $g$.\n\nFor the second part of the problem, we must find the minimal gain threshold $g_{\\text{th}}$ for which the SNR strictly improves relative to the baseline. A strict improvement occurs when the SNR with gain $g$ is strictly greater than the baseline SNR, which is equivalent to the condition $R(g) > 1$.\nUsing our derived expression for $R(g)$, we have the inequality:\n$$g^{2} > 1$$\nWe are given that the gain $g$ is a nonnegative value, i.e., $g \\geq 0$. The solution to the inequality $g^{2} > 1$ under the constraint $g \\geq 0$ is:\n$$g > 1$$\nThe problem asks for the minimal gain threshold, $g_{\\text{th}}$, which is the boundary point of the region of strict improvement. For any gain $g$ strictly greater than this threshold, the SNR must improve. The condition $g>1$ implies that this threshold value is:\n$$g_{\\text{th}} = 1$$\nAt $g=1$, $R(1)=1$, representing no change from baseline. For any $g>1$, $R(g)>1$, representing a strict improvement. For any $0 \\leq g < 1$, $R(g)<1$, representing a degradation. Thus, $g_{\\text{th}}=1$ is the threshold.\n\nThe final answer requires expressing $R(g)$ and $g_{\\text{th}}$ as a single row matrix.\nThe two results are $R(g) = g^{2}$ and $g_{\\text{th}} = 1$.\nThe row matrix is $\\begin{pmatrix} g^{2} & 1 \\end{pmatrix}$.",
            "answer": "$$\\boxed{\\begin{pmatrix} g^{2} & 1 \\end{pmatrix}}$$"
        },
        {
            "introduction": "Moving beyond simple signal amplification, a more sophisticated view frames attention as a mechanism for refining the brain's probabilistic beliefs about the world. This practice adopts a Bayesian inference perspective, where attention functions by increasing the *precision* of incoming sensory data. You will derive how this attentional modulation reduces the uncertainty (posterior variance) of a latent state, illustrating how attention sharpens our internal representations and leads to more reliable perception .",
            "id": "4051876",
            "problem": "A neuromorphic observer implements Bayesian inference over a single latent scalar state $x$ that represents the instantaneous activation level in a putative global workspace model of attention and consciousness. The observer has a Gaussian prior $p(x) = \\mathcal{N}(\\mu_{0}, \\sigma_{0}^{2})$ reflecting slow dynamics and homeostatic constraints of cortical activity. A sensory measurement $y$ is generated by a linear Gaussian observation model $y = x + \\eta$, where the measurement noise $\\eta \\sim \\mathcal{N}(0, \\sigma_{y}^{2})$ arises from stochastic spiking and synaptic transmission variability.\n\nAttention is hypothesized to modulate the measurement precision by a multiplicative factor $k \\geq 1$, consistent with gain control and stochastic resonance mechanisms in neuromorphic circuits, so that under attention the measurement noise variance becomes $\\sigma_{y}^{2}/k$. Define the prior precision $\\tau_{0} = 1/\\sigma_{0}^{2}$ and the baseline (pre-attention) measurement precision $\\tau_{y} = 1/\\sigma_{y}^{2}$.\n\nStarting only from Bayesâ€™ theorem and the probability density function of the Gaussian distribution, derive the posterior precision as a function of $k$ and subsequently derive the posterior variance $V(k)$ of $x$ given $y$ under attention. Express your final answer as a single closed-form analytic expression for $V(k)$ in terms of $\\tau_{0}$, $\\tau_{y}$, and $k$. No numerical evaluation is required, and no units are needed for the final expression.",
            "solution": "The goal is to derive the posterior variance, $V(k)$, of a latent state $x$ given a sensory measurement $y$, under an attentional modulation factor $k$. The derivation begins with Bayes' theorem and the probability density functions (PDFs) of the specified Gaussian distributions.\n\nFirst, let's establish the prior and the likelihood distributions.\n\nThe prior distribution of the latent state $x$ is given as a Gaussian:\n$$p(x) = \\mathcal{N}(\\mu_{0}, \\sigma_{0}^{2})$$\nThe PDF for this distribution is proportional to:\n$$p(x) \\propto \\exp\\left(-\\frac{(x - \\mu_{0})^{2}}{2\\sigma_{0}^{2}}\\right)$$\n\nThe sensory measurement $y$ is generated by the linear Gaussian observation model $y = x + \\eta$. The measurement noise $\\eta$ is a Gaussian random variable. Under attentional modulation, the noise distribution is given by $\\eta \\sim \\mathcal{N}(0, \\sigma_{y}^{2}/k)$. From the observation model, for a given state $x$, the measurement $y$ is distributed as a Gaussian centered at $x$ with variance $\\sigma_{y}^{2}/k$. This defines the likelihood function $p(y|x)$:\n$$p(y|x) = \\mathcal{N}(x, \\sigma_{y}^{2}/k)$$\nAs a function of $x$ for a fixed measurement $y$, the likelihood is:\n$$p(y|x) \\propto \\exp\\left(-\\frac{(y - x)^{2}}{2(\\sigma_{y}^{2}/k)}\\right) = \\exp\\left(-\\frac{k(x - y)^{2}}{2\\sigma_{y}^{2}}\\right)$$\n\nAccording to Bayes' theorem, the posterior probability distribution of $x$ given $y$, $p(x|y)$, is proportional to the product of the likelihood and the prior:\n$$p(x|y) \\propto p(y|x) p(x)$$\nSubstituting the expressions for the prior and the likelihood:\n$$p(x|y) \\propto \\exp\\left(-\\frac{k(x - y)^{2}}{2\\sigma_{y}^{2}}\\right) \\exp\\left(-\\frac{(x - \\mu_{0})^{2}}{2\\sigma_{0}^{2}}\\right)$$\nWe can combine the exponents:\n$$p(x|y) \\propto \\exp\\left( -\\left[ \\frac{k(x - y)^{2}}{2\\sigma_{y}^{2}} + \\frac{(x - \\mu_{0})^{2}}{2\\sigma_{0}^{2}} \\right] \\right)$$\n\nThe problem defines precision as the reciprocal of variance. The prior precision is $\\tau_{0} = 1/\\sigma_{0}^{2}$, and the baseline measurement precision is $\\tau_{y} = 1/\\sigma_{y}^{2}$. We can define the effective measurement precision under attention as $\\tau_{y,k} = 1/(\\sigma_{y}^{2}/k) = k/\\sigma_{y}^{2} = k\\tau_{y}$.\n\nSubstituting these precision terms into the exponent of the posterior distribution:\n$$p(x|y) \\propto \\exp\\left( -\\frac{1}{2} \\left[ k\\tau_{y}(x - y)^{2} + \\tau_{0}(x - \\mu_{0})^{2} \\right] \\right)$$\nThe product of two Gaussian distributions is another Gaussian. Therefore, the posterior distribution $p(x|y)$ is a Gaussian, which we can denote as $\\mathcal{N}(\\mu_{\\text{post}}, \\sigma_{\\text{post}}^{2})$. The PDF of this posterior is proportional to:\n$$p(x|y) \\propto \\exp\\left(-\\frac{(x - \\mu_{\\text{post}})^{2}}{2\\sigma_{\\text{post}}^{2}}\\right) = \\exp\\left(-\\frac{1}{2}\\tau_{\\text{post}}(x - \\mu_{\\text{post}})^{2}\\right)$$\nwhere $\\tau_{\\text{post}} = 1/\\sigma_{\\text{post}}^{2}$ is the posterior precision.\n\nTo find the posterior variance (or precision), we can analyze the terms in the exponent of our derived posterior expression that are quadratic in $x$. Let's expand the terms inside the exponent's brackets:\n$$k\\tau_{y}(x^{2} - 2xy + y^{2}) + \\tau_{0}(x^{2} - 2x\\mu_{0} + \\mu_{0}^{2})$$\nCollecting the terms with $x^{2}$:\n$$(k\\tau_{y} + \\tau_{0})x^{2} - 2(k\\tau_{y}y + \\tau_{0}\\mu_{0})x + (\\text{terms independent of } x)$$\nThe general form of the exponent of a Gaussian PDF in terms of its precision $\\tau_{\\text{post}}$ is $-\\frac{1}{2}(\\tau_{\\text{post}}x^2 - 2\\tau_{\\text{post}}\\mu_{\\text{post}}x + \\dots)$. By comparing the coefficient of the $x^{2}$ term in our derived posterior with this general form, we can identify the posterior precision.\nThe coefficient of $x^2$ in the exponent is $-\\frac{1}{2}(k\\tau_{y} + \\tau_{0})$.\nBy comparison, this must be equal to $-\\frac{1}{2}\\tau_{\\text{post}}$.\nTherefore, the posterior precision, as a function of the attentional gain $k$, is:\n$$\\tau_{\\text{post}}(k) = \\tau_{0} + k\\tau_{y}$$\nThis result illustrates a fundamental principle of Bayesian fusion for Gaussian variables: the precision of the posterior is the sum of the precisions of the prior and the likelihood.\n\nThe problem asks for the posterior variance, $V(k)$. The variance is the reciprocal of the precision:\n$$V(k) = \\sigma_{\\text{post}}^{2}(k) = \\frac{1}{\\tau_{\\text{post}}(k)}$$\nSubstituting the expression for $\\tau_{\\text{post}}(k)$:\n$$V(k) = \\frac{1}{\\tau_{0} + k\\tau_{y}}$$\nThis is the final closed-form analytic expression for the posterior variance $V(k)$ in terms of the prior precision $\\tau_{0}$, the baseline measurement precision $\\tau_{y}$, and the attentional modulation factor $k$.",
            "answer": "$$\\boxed{\\frac{1}{\\tau_{0} + k\\tau_{y}}}$$"
        },
        {
            "introduction": "A key feature of attention is its limited capacity; we cannot process everything at once, forcing the brain to make strategic decisions. This final exercise models attention as a finite cognitive resource that must be distributed optimally across multiple concurrent tasks to maximize overall performance, which is quantified here as total information gain. By solving this constrained optimization problem, you will derive an optimal attentional policy, bridging the gap between low-level neural mechanisms and the high-level cognitive challenge of multitasking .",
            "id": "4051839",
            "problem": "Consider a neuromorphic multi-sensor system performing $m$ concurrent perceptual tasks indexed by $j \\in \\{1,\\dots,m\\}$, where each task involves inferring a latent variable $X_{j}$ from a corresponding measurement stream $Z_{j}$. Let the expected information gain for task $j$ from allocating an attentional time $t_{j} \\ge 0$ be quantified by the expected mutual information $\\mathbb{E}[I(X_{j};Z_{j}\\,|\\,t_{j})]$, where $I(X;Z)$ denotes mutual information in the sense of Shannon information theory. Assume tasks are conditionally independent given their attentional allocations, so that the total expected information gain is additive across tasks.\n\nAs a resource-limited brain-inspired model, assume that the encoding fidelity of each stream exhibits diminishing returns with additional attentional time due to noise and redundancy, and that near an operating point of interest the expected information gain can be approximated by a second-order concave model derived from a Taylor expansion around $t_{j}=0$:\n$$\n\\mathbb{E}[I(X_{j};Z_{j}\\,|\\,t_{j})] \\approx \\beta_{j}\\,t_{j} - \\frac{1}{2}\\alpha_{j}\\,t_{j}^{2},\n$$\nwhere $\\beta_{j} > 0$ is the first-order marginal information gain at $t_{j}=0$ and $\\alpha_{j} > 0$ is the curvature capturing diminishing returns. Let the total attentional time budget be $T > 0$ with the hard constraint $\\sum_{j=1}^{m} t_{j} = T$ and $t_{j} \\ge 0$ for all $j$.\n\nFormulate and solve the optimization problem of allocating attentional time $\\{t_{j}\\}_{j=1}^{m}$ to maximize the total expected information gain\n$$\n\\sum_{j=1}^{m}\\left(\\beta_{j}\\,t_{j} - \\frac{1}{2}\\alpha_{j}\\,t_{j}^{2}\\right)\n$$\nsubject to $\\sum_{j=1}^{m} t_{j} = T$ and $t_{j} \\ge 0$. Assume that the time budget $T$ satisfies\n$$\n\\sum_{j=1}^{m}\\frac{\\beta_{j}}{\\alpha_{j}} - \\left(\\min_{1\\le j \\le m}\\beta_{j}\\right)\\sum_{j=1}^{m}\\frac{1}{\\alpha_{j}} < T < \\sum_{j=1}^{m}\\frac{\\beta_{j}}{\\alpha_{j}},\n$$\nwhich ensures an interior optimum with $t_{j}^{\\star} > 0$ for all $j$.\n\nDerive the optimal allocation policy and present it as a single closed-form analytic expression for the vector $\\big(t_{1}^{\\star},\\dots,t_{m}^{\\star}\\big)$ explicitly in terms of $\\{\\alpha_{j}\\}$, $\\{\\beta_{j}\\}$, and $T$. The final answer must be a single closed-form analytic expression. No rounding is required.",
            "solution": "This problem requires solving a constrained optimization to find the optimal allocation of attentional time $\\{t_{j}\\}_{j=1}^{m}$. The goal is to maximize the total expected information gain, subject to a fixed total time budget.\n\nThe optimization problem can be stated as:\n$$\n\\text{maximize} \\quad F(t_1, \\dots, t_m) = \\sum_{j=1}^{m}\\left(\\beta_{j}\\,t_{j} - \\frac{1}{2}\\alpha_{j}\\,t_{j}^{2}\\right)\n$$\n$$\n\\text{subject to} \\quad \\sum_{j=1}^{m} t_{j} = T \\quad \\text{and} \\quad t_j \\ge 0 \\quad \\text{for } j=1, \\dots, m.\n$$\nSince the objective function is strictly concave and the constraints define a convex set, a unique global maximum exists. The problem states that the conditions on $T$ ensure an interior solution where $t_j^\\star > 0$ for all $j$. This allows us to solve the problem using the method of Lagrange multipliers for the equality constraint, as the non-negativity constraints will not be active.\n\nThe Lagrangian function $\\mathcal{L}$ is:\n$$\n\\mathcal{L}(t_1, \\dots, t_m, \\lambda) = \\sum_{j=1}^{m}\\left(\\beta_{j}\\,t_{j} - \\frac{1}{2}\\alpha_{j}\\,t_{j}^{2}\\right) - \\lambda\\left(\\sum_{j=1}^{m} t_{j} - T\\right)\n$$\nwhere $\\lambda$ is the Lagrange multiplier. To find the optimal allocation, we set the partial derivative of $\\mathcal{L}$ with respect to each $t_j$ to zero:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial t_{j}} = \\beta_{j} - \\alpha_{j} t_{j} - \\lambda = 0\n$$\nSolving for $t_j$ gives the optimal time allocation for task $j$ in terms of $\\lambda$:\n$$\nt_{j}^{\\star} = \\frac{\\beta_{j} - \\lambda}{\\alpha_{j}}\n$$\nThis result shows that at the optimum, the marginal gain ($\\beta_{j} - \\alpha_{j} t_{j}$) is equalized across all tasks and is equal to the shadow price $\\lambda$.\n\nTo determine $\\lambda$, we substitute the expression for $t_{j}^{\\star}$ into the budget constraint $\\sum_{j=1}^{m} t_{j}^{\\star} = T$:\n$$\n\\sum_{j=1}^{m} \\frac{\\beta_{j} - \\lambda}{\\alpha_{j}} = T\n$$\nSolving for $\\lambda$:\n$$\n\\sum_{j=1}^{m} \\frac{\\beta_{j}}{\\alpha_{j}} - \\lambda \\sum_{j=1}^{m} \\frac{1}{\\alpha_{j}} = T\n$$\n$$\n\\lambda = \\frac{\\sum_{k=1}^{m} \\frac{\\beta_{k}}{\\alpha_{k}} - T}{\\sum_{k=1}^{m} \\frac{1}{\\alpha_{k}}}\n$$\nFinally, we substitute this expression for $\\lambda$ back into the equation for $t_{j}^{\\star}$:\n$$\nt_{j}^{\\star} = \\frac{1}{\\alpha_j} \\left( \\beta_{j} - \\frac{\\sum_{k=1}^{m} \\frac{\\beta_{k}}{\\alpha_{k}} - T}{\\sum_{k=1}^{m} \\frac{1}{\\alpha_{k}}} \\right)\n$$\nThis can be simplified by combining terms over a common denominator:\n$$\nt_{j}^{\\star} = \\frac{1}{\\alpha_j} \\left( \\frac{\\beta_{j} \\left(\\sum_{k=1}^{m} \\frac{1}{\\alpha_{k}}\\right) - \\left(\\sum_{k=1}^{m} \\frac{\\beta_{k}}{\\alpha_{k}} - T\\right)}{\\sum_{k=1}^{m} \\frac{1}{\\alpha_{k}}} \\right)\n$$\n$$\nt_{j}^{\\star} = \\frac{ T + \\beta_{j} \\sum_{k=1}^{m} \\frac{1}{\\alpha_{k}} - \\sum_{k=1}^{m} \\frac{\\beta_{k}}{\\alpha_{k}} }{ \\alpha_{j} \\sum_{k=1}^{m} \\frac{1}{\\alpha_{k}} }\n$$\nThis expression gives the optimal allocation $t_j^\\star$ for any task $j$, thereby defining the complete optimal allocation vector $\\big(t_{1}^{\\star},\\dots,t_{m}^{\\star}\\big)$.",
            "answer": "$$\n\\boxed{\nt_{j}^{\\star} = \\frac{T + \\beta_{j} \\sum_{k=1}^{m} \\frac{1}{\\alpha_{k}} - \\sum_{k=1}^{m} \\frac{\\beta_{k}}{\\alpha_{k}}}{\\alpha_{j} \\sum_{k=1}^{m} \\frac{1}{\\alpha_{k}}}\n}\n$$"
        }
    ]
}