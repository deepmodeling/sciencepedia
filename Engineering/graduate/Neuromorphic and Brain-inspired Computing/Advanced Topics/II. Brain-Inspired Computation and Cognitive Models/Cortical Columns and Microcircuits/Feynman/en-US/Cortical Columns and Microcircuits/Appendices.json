{
    "hands_on_practices": [
        {
            "introduction": "The journey into understanding cortical microcircuits begins with its fundamental processing unit: the neuron. The Leaky Integrate-and-Fire (LIF) model serves as a cornerstone abstraction, capturing the essential dynamics of membrane potential integration and spiking. This first exercise  provides hands-on practice in deriving the neuron's frequency-current ($f-I$) curve, which describes its firing rate as a function of input current. Mastering this analytical derivation is a crucial first step, as it reveals the basic computational transfer function of the cell.",
            "id": "4041431",
            "problem": "In a canonical cortical column microcircuit, the pooled synaptic drive to a representative excitatory neuron can be approximated as a constant current during sustained activity. Model a single layer $2/3$ pyramidal neuron as a Leaky Integrate-and-Fire (LIF) unit with membrane capacitance $C$, membrane time constant $\\tau_m$, resting potential $v_{rest}$, spike threshold $v_{th}$, and reset potential $v_{reset}$. The membrane potential $v(t)$ evolves according to the current-balance equation\n$$\nC \\frac{dv}{dt} = -\\frac{C}{\\tau_m}\\bigl(v - v_{rest}\\bigr) + I\n$$\nwhere $I$ is a constant input current representing the net effect of many presynaptic neurons within the microcircuit. Upon reaching $v_{th}$, the neuron emits a spike, is instantly reset to $v_{reset}$, and then undergoes an absolute refractory period of duration $\\tau_{ref}$ during which it does not integrate input.\n\nUsing first-passage time reasoning in the deterministic limit (no noise), derive the steady-state firing rate $r(I)$, expressed in $\\mathrm{s}^{-1}$, as a closed-form analytic expression in terms of $I$, $C$, $\\tau_m$, $v_{rest}$, $v_{th}$, $v_{reset}$, and $\\tau_{ref}$. Assume the constant input is sufficiently strong to drive repetitive spiking, i.e., the effective steady-state potential under constant drive satisfies $v_{rest} + \\tau_m I / C > v_{th}$. Provide your final answer as a single explicit expression for $r(I)$ with no inequalities. Do not perform any numerical substitution or rounding; give the exact symbolic expression.",
            "solution": "The firing rate $r$ is the inverse of the inter-spike interval, $T_{ISI}$. The inter-spike interval is the sum of two periods: the absolute refractory period, $\\tau_{ref}$, during which the neuron is inactive, and the charging time, $T_{charge}$, which is the time it takes for the membrane potential $v(t)$ to rise from the reset potential $v_{reset}$ to the threshold potential $v_{th}$.\n$$\nr(I) = \\frac{1}{T_{ISI}} = \\frac{1}{T_{charge} + \\tau_{ref}}\n$$\nOur primary task is to find an expression for $T_{charge}$. This time is determined by the dynamics of the membrane potential, which are governed by the given ordinary differential equation (ODE):\n$$\nC \\frac{dv}{dt} = -\\frac{C}{\\tau_m}\\bigl(v - v_{rest}\\bigr) + I\n$$\nTo solve this equation, we can first rearrange it into a more standard linear first-order form. Dividing by the capacitance $C$, we get:\n$$\n\\frac{dv}{dt} = -\\frac{1}{\\tau_m}\\bigl(v - v_{rest}\\bigr) + \\frac{I}{C}\n$$\nRearranging the terms to isolate the derivative:\n$$\n\\frac{dv}{dt} = -\\frac{1}{\\tau_m} v + \\frac{v_{rest}}{\\tau_m} + \\frac{I}{C}\n$$\nMultiplying by $\\tau_m$ provides a cleaner structure:\n$$\n\\tau_m \\frac{dv}{dt} = -v + v_{rest} + \\frac{\\tau_m I}{C}\n$$\nWe can define an effective steady-state potential, $v_{\\infty}$, which is the potential the membrane would approach if there were no firing threshold:\n$$\nv_{\\infty} = v_{rest} + \\frac{\\tau_m I}{C}\n$$\nThe problem states that $v_{\\infty} > v_{th}$, which guarantees that the neuron will always reach the threshold and fire repetitively. Substituting $v_{\\infty}$ into the ODE yields:\n$$\n\\tau_m \\frac{dv}{dt} = -(v - v_{\\infty})\n$$\nThis is a separable differential equation. We can solve it to find the evolution of $v(t)$ during the charging period. We set the initial condition for this period at time $t=0$ to be the reset potential, $v(0) = v_{reset}$. We want to find the time $T_{charge}$ at which the potential reaches the threshold, $v(T_{charge}) = v_{th}$.\nSeparating variables gives:\n$$\n\\frac{dv}{v - v_{\\infty}} = -\\frac{dt}{\\tau_m}\n$$\nWe integrate this equation from the start of the charging period (time $t=0$, potential $v=v_{reset}$) to a general time $t$ where the potential is $v(t)$:\n$$\n\\int_{v_{reset}}^{v(t)} \\frac{1}{v' - v_{\\infty}} dv' = \\int_{0}^{t} -\\frac{1}{\\tau_m} dt'\n$$\nPerforming the integration gives:\n$$\n\\Bigl[ \\ln|v' - v_{\\infty}| \\Bigr]_{v_{reset}}^{v(t)} = -\\frac{t}{\\tau_m}\n$$\n$$\n\\ln|v(t) - v_{\\infty}| - \\ln|v_{reset} - v_{\\infty}| = -\\frac{t}{\\tau_m}\n$$\nGiven the condition $v_{\\infty} > v_{th}$ and the fact that for an LIF neuron $v_{th} > v_{reset}$, the potential $v(t)$ during the charging phase is always less than $v_{\\infty}$. Therefore, the terms inside the absolute value are always negative, so we can write $|x| = -x$.\n$$\n\\ln(v_{\\infty} - v(t)) - \\ln(v_{\\infty} - v_{reset}) = -\\frac{t}{\\tau_m}\n$$\nUsing the properties of logarithms:\n$$\n\\ln\\left(\\frac{v_{\\infty} - v(t)}{v_{\\infty} - v_{reset}}\\right) = -\\frac{t}{\\tau_m}\n$$\nNow, we solve for the specific time $t = T_{charge}$ when the potential reaches the threshold, $v(T_{charge}) = v_{th}$:\n$$\n\\ln\\left(\\frac{v_{\\infty} - v_{th}}{v_{\\infty} - v_{reset}}\\right) = -\\frac{T_{charge}}{\\tau_m}\n$$\nSolving for $T_{charge}$:\n$$\nT_{charge} = -\\tau_m \\ln\\left(\\frac{v_{\\infty} - v_{th}}{v_{\\infty} - v_{reset}}\\right)\n$$\nUsing the logarithmic identity $-\\ln(a/b) = \\ln(b/a)$, we can write the expression for $T_{charge}$ in a form where the argument of the logarithm is greater than $1$:\n$$\nT_{charge} = \\tau_m \\ln\\left(\\frac{v_{\\infty} - v_{reset}}{v_{\\infty} - v_{th}}\\right)\n$$\nThe condition $v_{\\infty} > v_{th} > v_{reset}$ ensures that the argument of the logarithm is a real number greater than $1$, so $T_{charge}$ is a positive real value, as expected.\n\nNext, we substitute the definition of $v_{\\infty}$ back into the expression for $T_{charge}$:\n$$\nT_{charge} = \\tau_m \\ln\\left(\\frac{(v_{rest} + \\frac{\\tau_m I}{C}) - v_{reset}}{(v_{rest} + \\frac{\\tau_m I}{C}) - v_{th}}\\right)\n$$\nNow we can express the total inter-spike interval $T_{ISI}$:\n$$\nT_{ISI} = \\tau_{ref} + T_{charge} = \\tau_{ref} + \\tau_m \\ln\\left(\\frac{v_{rest} - v_{reset} + \\frac{\\tau_m I}{C}}{v_{rest} - v_{th} + \\frac{\\tau_m I}{C}}\\right)\n$$\nFinally, the steady-state firing rate $r(I)$ is the reciprocal of $T_{ISI}$:\n$$\nr(I) = \\frac{1}{T_{ISI}} = \\frac{1}{\\tau_{ref} + \\tau_m \\ln\\left(\\frac{v_{rest} - v_{reset} + \\frac{\\tau_m I}{C}}{v_{rest} - v_{th} + \\frac{\\tau_m I}{C}}\\right)}\n$$\nThis is the closed-form analytic expression for the firing rate as a function of the input current $I$ and the neuron's parameters.",
            "answer": "$$\n\\boxed{\\frac{1}{\\tau_{ref} + \\tau_m \\ln\\left(\\frac{v_{rest} - v_{reset} + \\frac{\\tau_m I}{C}}{v_{rest} - v_{th} + \\frac{\\tau_m I}{C}}\\right)}}\n$$"
        },
        {
            "introduction": "Building upon the single-neuron model, this practice scales up to the microcircuit level, where a neuron is embedded within a network of thousands of connections. In the cortex, a neuron receives a continuous bombardment of both excitatory and inhibitory synaptic inputs, which are inherently unreliable. This exercise  guides you through calculating the average net input current under these realistic conditions, accounting for synaptic failure probabilities. You will then apply the mean-drive approximation to predict the neuron's resulting firing rate, directly connecting the statistical properties of the microcircuit to observable neural activity.",
            "id": "4041468",
            "problem": "Consider a simplified microcircuit within a cortical column that contains a single postsynaptic excitatory neuron modeled as a Leaky Integrate-and-Fire (LIF) neuron. The neuron receives inputs from two presynaptic populations: an excitatory population and an inhibitory population. Presynaptic spike trains are modeled as independent homogeneous Poisson processes. Each presynaptic spike attempts a synaptic release that independently fails with probability $p_f$ and succeeds with probability $1 - p_f$. Successful events evoke postsynaptic currents described by exponential kernels of unit area. Specifically, for each presynaptic spike that successfully releases neurotransmitter, the excitatory postsynaptic current is $I_e(t) = A_e \\frac{1}{\\tau_{se}} \\exp\\!\\left(-\\frac{t}{\\tau_{se}}\\right) u(t)$ and the inhibitory postsynaptic current is $I_i(t) = -A_i \\frac{1}{\\tau_{si}} \\exp\\!\\left(-\\frac{t}{\\tau_{si}}\\right) u(t)$, where $u(t)$ is the unit step function and $A_e$ and $A_i$ are the areas (in Ampere-second) of the excitatory and inhibitory postsynaptic currents per successful event, respectively. The LIF membrane equation is $\\tau_m \\frac{dV}{dt} = -(V - V_{\\text{rest}}) + R_m I(t)$, with $V_{\\text{rest}}$ the resting potential, membrane resistance $R_m$, membrane time constant $\\tau_m$, and $I(t)$ the total synaptic input current. After reaching threshold $V_{\\text{th}}$, the membrane potential resets to $V_{\\text{reset}}$ and the neuron remains refractory for $\\tau_{\\text{ref}}$.\n\nUse the following parameters, which are physiologically plausible for a cortical microcircuit:\n- Number of excitatory synapses: $N_e = 1000$.\n- Number of inhibitory synapses: $N_i = 800$.\n- Excitatory presynaptic rate: $\\nu_e = 5 \\,\\text{Hz}$.\n- Inhibitory presynaptic rate: $\\nu_i = 5 \\,\\text{Hz}$.\n- Synaptic failure probability: $p_f = 0.2$.\n- Excitatory event area: $A_e = 0.06 \\,\\text{pA}\\cdot\\text{s}$.\n- Inhibitory event area magnitude: $A_i = 0.02 \\,\\text{pA}\\cdot\\text{s}$.\n- Membrane resistance: $R_m = 100 \\,\\text{M}\\Omega$.\n- Membrane time constant: $\\tau_m = 20 \\,\\text{ms}$.\n- Resting and reset potentials: $V_{\\text{rest}} = V_{\\text{reset}} = -65 \\,\\text{mV}$.\n- Threshold potential: $V_{\\text{th}} = -50 \\,\\text{mV}$.\n- Absolute refractory period: $\\tau_{\\text{ref}} = 2 \\,\\text{ms}$.\n\nStarting from first principles and well-tested facts about Poisson processes and the LIF dynamics, compute the expected effective mean synaptic input current under synaptic failures and, under a mean-drive approximation (replace $I(t)$ by its time-average), derive the resulting steady firing rate of the LIF neuron. Provide the final firing rate as a single number in $\\text{Hz}$. Round your answer to three significant figures. Express your final result in $\\text{Hz}$.",
            "solution": "The solution is derived in two stages: first, we compute the mean synaptic input current, and second, we use this mean current to determine the firing rate of the LIF neuron.\n\n**1. Mean Synaptic Input Current**\nThe total input current $I(t)$ is the sum of currents from all excitatory and inhibitory synapses. Due to the linearity of expectation, the mean total current $\\langle I \\rangle$ is the sum of the mean excitatory and mean inhibitory currents:\n$$ \\langle I \\rangle = \\langle I_e \\rangle + \\langle I_i \\rangle $$\nThe presynaptic spike trains are modeled as homogeneous Poisson processes. The property of superposition for independent Poisson processes states that the sum of these processes is also a Poisson process with a rate equal to the sum of the individual rates.\n\nFor the excitatory population, there are $N_e$ synapses, each receiving spikes at a rate $\\nu_e$. Each spike triggers a transmitter release with a success probability of $1-p_f$. Thus, the rate of successful excitatory synaptic events at a single synapse is $\\nu_e (1-p_f)$. The total rate of successful excitatory events across all synapses is:\n$$ \\Lambda_e = N_e \\nu_e (1 - p_f) $$\nAccording to Campbell's theorem for shot noise, the time-averaged current generated by a Poisson process of events is the product of the event rate and the total charge transferred per event. Each successful excitatory event delivers a total charge of $A_e = \\int_0^\\infty I_e(t) dt$. Therefore, the mean excitatory current is:\n$$ \\langle I_e \\rangle = \\Lambda_e A_e = N_e \\nu_e (1 - p_f) A_e $$\nSimilarly, for the inhibitory population, the total rate of successful inhibitory events is:\n$$ \\Lambda_i = N_i \\nu_i (1 - p_f) $$\nEach successful inhibitory event delivers a charge of $-A_i = \\int_0^\\infty I_i(t) dt$. The mean inhibitory current is:\n$$ \\langle I_i \\rangle = \\Lambda_i (-A_i) = - N_i \\nu_i (1 - p_f) A_i $$\nThe total mean synaptic input current is:\n$$ \\langle I \\rangle = N_e \\nu_e (1 - p_f) A_e - N_i \\nu_i (1 - p_f) A_i = (1 - p_f) (N_e \\nu_e A_e - N_i \\nu_i A_i) $$\nSubstituting the given numerical values:\n- $N_e = 1000$, $\\nu_e = 5 \\,\\text{s}^{-1}$, $A_e = 0.06 \\times 10^{-12} \\,\\text{A}\\cdot\\text{s}$\n- $N_i = 800$, $\\nu_i = 5 \\,\\text{s}^{-1}$, $A_i = 0.02 \\times 10^{-12} \\,\\text{A}\\cdot\\text{s}$\n- $p_f = 0.2$\n\n$$ \\langle I_e \\rangle = 1000 \\times (5 \\,\\text{s}^{-1}) \\times (1 - 0.2) \\times (0.06 \\times 10^{-12} \\,\\text{A}\\cdot\\text{s}) = 240 \\times 10^{-12} \\,\\text{A} = 240 \\,\\text{pA} $$\n$$ \\langle I_i \\rangle = -800 \\times (5 \\,\\text{s}^{-1}) \\times (1 - 0.2) \\times (0.02 \\times 10^{-12} \\,\\text{A}\\cdot\\text{s}) = -64 \\times 10^{-12} \\,\\text{A} = -64 \\,\\text{pA} $$\n$$ \\langle I \\rangle = 240 \\,\\text{pA} - 64 \\,\\text{pA} = 176 \\,\\text{pA} = 176 \\times 10^{-12} \\,\\text{A} $$\n\n**2. LIF Neuron Firing Rate**\nIn the mean-drive approximation, the stochastic input current $I(t)$ is replaced by its constant mean value $\\langle I \\rangle$. The LIF membrane equation becomes a deterministic first-order linear ordinary differential equation:\n$$ \\tau_m \\frac{dV}{dt} = -(V - V_{\\text{rest}}) + R_m \\langle I \\rangle $$\nThis equation can be rewritten as:\n$$ \\tau_m \\frac{dV}{dt} = -(V - (V_{\\text{rest}} + R_m \\langle I \\rangle)) $$\nLet $V_{\\infty} = V_{\\text{rest}} + R_m \\langle I \\rangle$ be the steady-state membrane potential the neuron would approach if it never fired.\nUsing the given parameters:\n- $R_m = 100 \\,\\text{M}\\Omega = 100 \\times 10^6 \\,\\Omega$\n- $V_{\\text{rest}} = -65 \\,\\text{mV} = -65 \\times 10^{-3} \\,\\text{V}$\n- $\\langle I \\rangle = 176 \\times 10^{-12} \\,\\text{A}$\n\nThe steady-state input voltage is $R_m \\langle I \\rangle = (100 \\times 10^6 \\,\\Omega) \\times (176 \\times 10^{-12} \\,\\text{A}) = 17.6 \\times 10^{-3} \\,\\text{V} = 17.6 \\,\\text{mV}$.\nThus, $V_{\\infty} = -65 \\,\\text{mV} + 17.6 \\,\\text{mV} = -47.4 \\,\\text{mV}$.\n\nThe neuron will fire if and only if $V_{\\infty} > V_{\\text{th}}$. Here, $V_{\\text{th}} = -50 \\,\\text{mV}$, so with $V_{\\infty} = -47.4 \\,\\text{mV}$, the condition is met and the neuron will fire repetitively.\n\nTo find the firing rate, we must calculate the time $T_{\\text{charge}}$ it takes for the membrane potential to rise from the reset potential $V_{\\text{reset}}$ to the threshold potential $V_{\\text{th}}$. We solve the ODE with the initial condition $V(0) = V_{\\text{reset}}$:\n$$ \\int_{V_{\\text{reset}}}^{V_{\\text{th}}} \\frac{dV}{V - V_{\\infty}} = -\\int_0^{T_{\\text{charge}}} \\frac{dt}{\\tau_m} $$\n$$ \\ln|V_{\\text{th}} - V_{\\infty}| - \\ln|V_{\\text{reset}} - V_{\\infty}| = -\\frac{T_{\\text{charge}}}{\\tau_m} $$\nSince $V$ remains between $V_{\\text{reset}}$ and $V_{\\text{th}}$, and both are less than $V_{\\infty}$, the arguments of the logarithms are negative. We can write this as:\n$$ \\ln(V_{\\infty} - V_{\\text{th}}) - \\ln(V_{\\infty} - V_{\\text{reset}}) = -\\frac{T_{\\text{charge}}}{\\tau_m} $$\n$$ T_{\\text{charge}} = \\tau_m \\ln\\left(\\frac{V_{\\infty} - V_{\\text{reset}}}{V_{\\infty} - V_{\\text{th}}}\\right) $$\nThe total time for one inter-spike interval (ISI) is the sum of this charging time and the absolute refractory period $\\tau_{\\text{ref}}$:\n$$ T_{\\text{ISI}} = T_{\\text{charge}} + \\tau_{\\text{ref}} $$\nThe firing rate $f$ is the reciprocal of the ISI: $f = 1 / T_{\\text{ISI}}$.\nSubstituting the known values:\n- $\\tau_m = 20 \\,\\text{ms} = 0.020 \\,\\text{s}$\n- $\\tau_{\\text{ref}} = 2 \\,\\text{ms} = 0.002 \\,\\text{s}$\n- $V_{\\infty} = -47.4 \\,\\text{mV}$\n- $V_{\\text{reset}} = -65 \\,\\text{mV}$\n- $V_{\\text{th}} = -50 \\,\\text{mV}$\n\n$$ T_{\\text{charge}} = (20 \\,\\text{ms}) \\times \\ln\\left(\\frac{-47.4 - (-65)}{-47.4 - (-50)}\\right) = (20 \\,\\text{ms}) \\times \\ln\\left(\\frac{17.6}{2.6}\\right) $$\n$$ T_{\\text{charge}} \\approx (20 \\,\\text{ms}) \\times \\ln(6.76923) \\approx (20 \\,\\text{ms}) \\times 1.9124 \\approx 38.248 \\,\\text{ms} $$\nThe total inter-spike interval is:\n$$ T_{\\text{ISI}} = 38.248 \\,\\text{ms} + 2 \\,\\text{ms} = 40.248 \\,\\text{ms} = 0.040248 \\,\\text{s} $$\nThe final firing rate is:\n$$ f = \\frac{1}{T_{\\text{ISI}}} = \\frac{1}{0.040248 \\,\\text{s}} \\approx 24.846 \\,\\text{Hz} $$\nRounding to three significant figures, the firing rate is $24.8 \\,\\text{Hz}$.",
            "answer": "$$\\boxed{24.8}$$"
        },
        {
            "introduction": "Our final practice demonstrates how the rich, recurrent dynamics of cortical microcircuits can be harnessed for powerful computation. We will explore the paradigm of reservoir computing, where a fixed, complex network transforms input signals into high-dimensional state trajectories, making complex patterns linearly separable. This exercise  involves building a computational model of a microcircuit that respects biological constraints like Dale's principle and sparse connectivity. By training a simple linear readout, you will see how this \"reservoir\" can perform a challenging temporal classification task, providing a concrete example of brain-inspired information processing.",
            "id": "4041506",
            "problem": "You are to implement a computational model inspired by cortical column microcircuits to perform temporal classification using a reservoir computing approach and ridge regression for the readout. The reservoir represents a simplified, rate-based microcircuit with excitatory and inhibitory neurons obeying Dale’s principle. Your program must construct the reservoir, process input streams, compute readout weights via ridge regression, and evaluate classification performance on specified test cases. All angles are to be treated in radians. No physical units are involved in this problem.\n\nFundamental base and definitions:\n- A cortical column microcircuit can be abstracted as a recurrent network of neurons with sparse connectivity, with approximately a fraction of neurons being excitatory and the remainder inhibitory. Dale’s principle states that the outgoing synaptic weights of each neuron are either all nonnegative (excitatory) or all nonpositive (inhibitory).\n- A rate-based reservoir model uses a leaky update equation. For reservoir state vector $x_t \\in \\mathbb{R}^{N}$ at discrete time $t$, input vector $u_t \\in \\mathbb{R}^{d}$, recurrent weight matrix $W \\in \\mathbb{R}^{N \\times N}$, input weight matrix $W_{\\text{in}} \\in \\mathbb{R}^{N \\times d}$, and leak rate $0 < \\alpha \\leq 1$, the update is\n$$\nx_{t+1} = (1 - \\alpha)x_t + \\alpha \\tanh\\!\\left(W x_t + W_{\\text{in}} u_t\\right).\n$$\n- The spectral radius of a matrix is the largest absolute value of its eigenvalues, denoted $\\rho(W)$. Reservoirs are commonly scaled to a target spectral radius to control dynamics.\n- Ridge regression solves for readout weights $W_{\\text{out}}$ by minimizing the penalized least squares objective\n$$\n\\min_{W_{\\text{out}}} \\|X W_{\\text{out}} - Y\\|_F^2 + \\lambda \\|W_{\\text{out}}\\|_F^2\n$$\nwhere $X \\in \\mathbb{R}^{M \\times (N+1)}$ are feature vectors constructed from reservoir states appended with a bias term, $Y \\in \\mathbb{R}^{M \\times C}$ are one-hot class targets, $M$ is the number of sequences, $C$ is the number of classes, and $\\lambda \\ge 0$ is the regularization coefficient. The closed-form solution is\n$$\nW_{\\text{out}} = (X^\\top X + \\lambda I)^{-1} X^\\top Y.\n$$\n\nProblem setup:\n- Reservoir construction:\n  - The reservoir has $N$ neurons with a fraction $r_{\\text{exc}}$ designated as excitatory and the remainder inhibitory, enforcing that the entire row of outgoing weights from an excitatory neuron are nonnegative and from an inhibitory neuron are nonpositive.\n  - Connectivity is sparse with connection density $p \\in (0,1)$; absent connections are set to $0$.\n  - The recurrent weight matrix $W$ is scaled so that $\\rho(W)$ equals a specified target spectral radius.\n  - The input weight matrix $W_{\\text{in}}$ is drawn randomly and scaled by an input scaling factor $s_{\\text{in}}$.\n- Dynamics:\n  - For each input sequence, initialize $x_0 = \\mathbf{0}$ and update using the leaky reservoir equation above.\n  - After processing the full sequence of length $T$, construct a feature vector $s = [x_T^\\top, 1]^\\top \\in \\mathbb{R}^{N+1}$ by appending a bias term $1$.\n- Readout training and inference:\n  - Construct a training set of $M_{\\text{train}}$ sequences balanced across $C = 3$ classes, and a test set of $M_{\\text{test}}$ sequences similarly balanced.\n  - Train the readout $W_{\\text{out}} \\in \\mathbb{R}^{(N+1) \\times C}$ using ridge regression with regularization parameter $\\lambda$. If $\\lambda = 0$ and the system is singular, use the Moore–Penrose pseudoinverse to obtain the Least Squares (LS) solution.\n  - For a test sequence feature vector $s$, compute output scores $o = s^\\top W_{\\text{out}} \\in \\mathbb{R}^{C}$ and predict the class as the index of the largest component of $o$.\n- Input generation:\n  - There are $d = 2$ input channels.\n  - For class $0$: channel $0$ is a sinusoid with frequency $f_{\\text{high}}$ cycles per sample and channel $1$ is a sinusoid with frequency $f_{\\text{low}}$ cycles per sample for all times $t \\in \\{0,1,\\ldots,T-1\\}$.\n  - For class $1$: channel $0$ uses $f_{\\text{low}}$ and channel $1$ uses $f_{\\text{high}}$ for all times $t$.\n  - For class $2$: both channels use $f_{\\text{low}}$ for $t < T/2$ and $f_{\\text{high}}$ for $t \\ge T/2$.\n  - Each sinusoid is $\\sin(2\\pi f t)$ with angles in radians, and independent Gaussian noise with standard deviation $\\sigma$ is added to each channel at each time step.\n- Frequencies:\n  - Use $f_{\\text{high}} = 0.15$ and $f_{\\text{low}} = 0.03$ cycles per sample. The angular frequency is $\\omega = 2\\pi f$.\n\nYour task is to implement this model and evaluate test accuracies for the following test suite of parameter sets. Each parameter set is a tuple $(N, p, r_{\\text{exc}}, \\rho_{\\text{target}}, \\alpha, s_{\\text{in}}, \\lambda, \\sigma, T, M_{\\text{train}}, M_{\\text{test}}, \\text{seed})$:\n- Case $1$ (general case): $(200, 0.10, 0.80, 0.90, 0.30, 1.00, 0.01, 0.05, 100, 300, 120, 41)$.\n- Case $2$ (boundary $\\lambda = 0$): $(200, 0.10, 0.80, 0.90, 0.30, 1.00, 0.00, 0.05, 100, 300, 120, 42)$.\n- Case $3$ (edge input scale $s_{\\text{in}} = 0$): $(200, 0.10, 0.80, 0.90, 0.30, 0.00, 0.10, 0.05, 100, 300, 120, 43)$.\n- Case $4$ (near-instability $\\rho_{\\text{target}} = 1.20$ with smaller leak, heavy regularization): $(200, 0.10, 0.80, 1.20, 0.10, 1.00, 1.00, 0.05, 100, 300, 120, 44)$.\n- Case $5$ (short sequences): $(200, 0.10, 0.80, 0.90, 0.50, 1.00, 0.01, 0.05, 20, 300, 120, 45)$.\n\nDesign for coverage:\n- Case $1$ is the happy path for a well-conditioned reservoir.\n- Case $2$ tests the boundary condition with no regularization, requiring a robust handling of possible singularities.\n- Case $3$ tests the degenerate case with no input drive, where the reservoir cannot distinguish classes and performance should degrade toward chance level.\n- Case $4$ tests high spectral radius with small leak and strong regularization.\n- Case $5$ tests reduced temporal memory due to short sequence length.\n\nFinal output specification:\n- Your program should produce a single line of output containing the test accuracies as a comma-separated list of floats rounded to six decimal places and enclosed in square brackets. For example, the output format must be like $[a_1,a_2,a_3,a_4,a_5]$, where each $a_i$ is the accuracy for case $i$ expressed as a decimal fraction rounded to six decimal places.",
            "solution": "The solution involves a step-by-step implementation and evaluation of this model. The core components are: input signal generation, reservoir construction adhering to Dale's principle, simulation of reservoir dynamics, training of a linear readout using ridge regression, and performance evaluation.\n\n### 1. Input Data Generation\n\nThe model must classify temporal patterns defined across $d=2$ input channels and $C=3$ classes. For each class $c \\in \\{0, 1, 2\\}$, we generate input sequences $\\mathbf{u}^{(c)} = \\{u_0, u_1, \\ldots, u_{T-1}\\}$ of length $T$, where $u_t \\in \\mathbb{R}^2$. The signals are based on sinusoids with two distinct frequencies, a high frequency $f_{\\text{high}} = 0.15$ cycles/sample and a low frequency $f_{\\text{low}} = 0.03$ cycles/sample. The corresponding angular frequencies are $\\omega_{\\text{high}} = 2\\pi f_{\\text{high}}$ and $\\omega_{\\text{low}} = 2\\pi f_{\\text{low}}$. Independent Gaussian noise, $n_t \\sim \\mathcal{N}(0, \\sigma^2 I)$, is added at each time step.\n\nThe input vector $u_t$ for each class is defined as follows:\n- **Class 0:** $u_t = [\\sin(\\omega_{\\text{high}} t), \\sin(\\omega_{\\text{low}} t)]^\\top + n_t$\n- **Class 1:** $u_t = [\\sin(\\omega_{\\text{low}} t), \\sin(\\omega_{\\text{high}} t)]^\\top + n_t$\n- **Class 2:** $u_t = \\begin{cases} [\\sin(\\omega_{\\text{low}} t), \\sin(\\omega_{\\text{low}} t)]^\\top + n_t & \\text{if } t < T/2 \\\\ [\\sin(\\omega_{\\text{high}} t), \\sin(\\omega_{\\text{high}} t)]^\\top + n_t & \\text{if } t \\ge T/2 \\end{cases}$\n\nFor each test case, we generate $M_{\\text{train}}$ training sequences and $M_{\\text{test}}$ testing sequences, balanced across the three classes.\n\n### 2. Reservoir Construction\n\nThe reservoir is a recurrent neural network of $N$ neurons. Its structure is defined by the recurrent weight matrix $W \\in \\mathbb{R}^{N \\times N}$ and the input weight matrix $W_{\\text{in}} \\in \\mathbb{R}^{N \\times d}$.\n\n**Recurrent Weight Matrix ($W$):**\n1.  **Initialization:** A dense matrix $W_{\\text{rand}} \\in \\mathbb{R}^{N \\times N}$ is initialized with values drawn from a standard normal distribution $\\mathcal{N}(0, 1)$.\n2.  **Sparsity:** A binary mask $M_{\\text{sparse}} \\in \\{0,1\\}^{N \\times N}$ is created, where each element is $1$ with probability $p$ (the connection density) and $0$ otherwise. The sparse matrix is $W_{\\text{sparse}} = W_{\\text{rand}} \\odot M_{\\text{sparse}}$, where $\\odot$ is the Hadamard (element-wise) product.\n3.  **Dale's Principle:** The $N$ neurons are partitioned into $N_{\\text{exc}} = \\lfloor N \\cdot r_{\\text{exc}} \\rfloor$ excitatory and $N_{\\text{inh}} = N - N_{\\text{exc}}$ inhibitory neurons. For each neuron $i$, its outgoing weights (row $i$ of $W$) are made either all non-negative (if excitatory) or all non-positive (if inhibitory). This is achieved by taking the absolute value of the weights in $W_{\\text{sparse}}$ and then multiplying each row by $+1$ or $-1$ corresponding to the neuron type.\n4.  **Spectral Radius Scaling:** The spectral radius $\\rho(W)$, which is the maximum absolute eigenvalue of the resulting matrix, governs the network's dynamics. To ensure desired properties (e.g., the echo state property), the matrix is scaled to a target spectral radius $\\rho_{\\text{target}}$. The final recurrent matrix is $W = \\rho_{\\text{target}} \\frac{W_{\\text{dale}}}{\\rho(W_{\\text{dale}})}$, assuming $\\rho(W_{\\text{dale}}) \\neq 0$.\n\n**Input Weight Matrix ($W_{\\text{in}}$):**\nThe input matrix $W_{\\text{in}}$ is constructed by drawing elements from a uniform distribution $U(-1, 1)$ and scaling the entire matrix by the input scaling factor $s_{\\text{in}}$.\n\n### 3. Reservoir Dynamics and Feature Extraction\n\nThe state of the reservoir, $x_t \\in \\mathbb{R}^{N}$, evolves over time according to the leaky rate-based update equation:\n$$\nx_{t+1} = (1 - \\alpha)x_t + \\alpha \\tanh(W x_t + W_{\\text{in}} u_t)\n$$\nwhere $\\alpha$ is the leak rate. For each input sequence, the reservoir is initialized with a zero state vector, $x_0 = \\mathbf{0}$, and driven by the input for $T$ time steps. The final state of the reservoir, $x_T$, captures a non-linear projection of the input history. This state is augmented with a bias term to form the feature vector for classification: $s = [x_T^\\top, 1]^\\top \\in \\mathbb{R}^{N+1}$.\n\n### 4. Readout Training via Ridge Regression\n\nThe readout mechanism is a linear classifier trained to map the reservoir's feature vectors to class labels. We collect the feature vectors from all $M_{\\text{train}}$ training sequences into a feature matrix $X \\in \\mathbb{R}^{M_{\\text{train}} \\times (N+1)}$. The corresponding class labels are represented in a one-hot encoded target matrix $Y \\in \\mathbb{R}^{M_{\\text{train}} \\times C}$.\n\nThe readout weights $W_{\\text{out}} \\in \\mathbb{R}^{(N+1) \\times C}$ are found by solving the ridge regression problem, which minimizes a combination of the squared error and a penalty on the magnitude of the weights:\n$$\n\\min_{W_{\\text{out}}} \\|X W_{\\text{out}} - Y\\|_F^2 + \\lambda \\|W_{\\text{out}}\\|_F^2\n$$\nThe regularization parameter $\\lambda \\ge 0$ controls the trade-off. The closed-form solution is:\n$$\nW_{\\text{out}} = (X^\\top X + \\lambda I)^{-1} X^\\top Y\n$$\nFor the case where $\\lambda = 0$, the problem reduces to ordinary least squares. If $X^\\top X$ is singular, the inverse does not exist. As specified, the solution is then found using the Moore-Penrose pseudoinverse, $X^\\dagger$. The solution for the weights becomes $W_{\\text{out}} = X^\\dagger Y$. This provides the minimum-norm solution to the least-squares problem.\n\n### 5. Inference and Evaluation\n\nOnce $W_{\\text{out}}$ is trained, the model's performance is evaluated on the $M_{\\text{test}}$ test sequences. For each test sequence, we compute its feature vector $s_{\\text{test}}$ and calculate the output scores $o = s_{\\text{test}}^\\top W_{\\text{out}}$. The predicted class is the one corresponding to the highest score: $\\hat{y} = \\text{argmax}_{i} (o_i)$.\n\nThe final metric is the test accuracy, defined as the fraction of test sequences that are correctly classified. This procedure is repeated for each parameter set provided in the test suite.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and evaluates a reservoir computing model for temporal classification.\n    \"\"\"\n    test_cases = [\n        # (N, p, r_exc, rho_target, alpha, s_in, lambda, sigma, T, M_train, M_test, seed)\n        (200, 0.10, 0.80, 0.90, 0.30, 1.00, 0.01, 0.05, 100, 300, 120, 41),\n        (200, 0.10, 0.80, 0.90, 0.30, 1.00, 0.00, 0.05, 100, 300, 120, 42),\n        (200, 0.10, 0.80, 0.90, 0.30, 0.00, 0.10, 0.05, 100, 300, 120, 43),\n        (200, 0.10, 0.80, 1.20, 0.10, 1.00, 1.00, 0.05, 100, 300, 120, 44),\n        (200, 0.10, 0.80, 0.90, 0.50, 1.00, 0.01, 0.05, 20, 300, 120, 45),\n    ]\n\n    results = []\n\n    f_high = 0.15\n    f_low = 0.03\n    d = 2\n    C = 3\n\n    for params in test_cases:\n        N, p, r_exc, rho_target, alpha, s_in, lam, sigma, T, M_train, M_test, seed = params\n        rng = np.random.default_rng(seed)\n\n        # 1. Reservoir Construction\n        N_exc = int(N * r_exc)\n        \n        # Initialize W\n        W = rng.standard_normal(size=(N, N))\n        \n        # Sparsity\n        sparsity_mask = rng.random(size=(N, N)) < p\n        W *= sparsity_mask\n\n        # Dale's Principle\n        exc_indices = rng.choice(N, size=N_exc, replace=False)\n        inh_indices = np.setdiff1d(np.arange(N), exc_indices)\n        dale_mask = np.ones((N, 1))\n        dale_mask[inh_indices] = -1\n        W = np.abs(W) * dale_mask\n        \n        # Scale spectral radius\n        try:\n            eigenvalues = np.linalg.eigvals(W)\n            rho_W = np.max(np.abs(eigenvalues))\n            if rho_W > 1e-9: # Avoid division by zero\n                W = W * (rho_target / rho_W)\n        except np.linalg.LinAlgError:\n            # This can happen for pathological matrices, though unlikely here.\n            # If so, we proceed with the unscaled matrix.\n            pass\n\n        # Input weights\n        W_in = rng.uniform(-1, 1, size=(N, d)) * s_in\n\n        # 2. Data Generation & Feature Extraction\n        def generate_data_and_features(M):\n            X_features = np.zeros((M, N + 1))\n            Y_labels = np.zeros((M,), dtype=int)\n            \n            samples_per_class = M // C\n            time_vec = np.arange(T)\n            s_high = np.sin(2 * np.pi * f_high * time_vec)\n            s_low = np.sin(2 * np.pi * f_low * time_vec)\n            \n            for i in range(M):\n                class_idx = i // samples_per_class\n                Y_labels[i] = class_idx\n                \n                u = np.zeros((T, d))\n                if class_idx == 0:\n                    u[:, 0] = s_high\n                    u[:, 1] = s_low\n                elif class_idx == 1:\n                    u[:, 0] = s_low\n                    u[:, 1] = s_high\n                else: # class_idx == 2\n                    split_idx = T // 2\n                    u[:split_idx, :] = np.vstack([s_low[:split_idx], s_low[:split_idx]]).T\n                    u[split_idx:, :] = np.vstack([s_high[split_idx:], s_high[split_idx:]]).T\n                \n                noise = rng.normal(0, sigma, size=(T, d))\n                u += noise\n                \n                # Run reservoir dynamics\n                x = np.zeros(N)\n                for t in range(T):\n                    x = (1 - alpha) * x + alpha * np.tanh(W @ x + W_in @ u[t])\n                \n                # Store final state with bias\n                X_features[i, :N] = x\n                X_features[i, N] = 1.0 # Bias term\n            \n            return X_features, Y_labels\n\n        X_train, y_train_labels = generate_data_and_features(M_train)\n        X_test, y_test_labels = generate_data_and_features(M_test)\n        \n        # One-hot encode training labels\n        Y_train_onehot = np.zeros((M_train, C))\n        Y_train_onehot[np.arange(M_train), y_train_labels] = 1\n\n        # 3. Readout Training\n        if lam == 0.0:\n            # Use Moore-Penrose pseudoinverse for Ordinary Least Squares\n            try:\n                X_pinv = np.linalg.pinv(X_train)\n                W_out = X_pinv @ Y_train_onehot\n            except np.linalg.LinAlgError:\n                # Should not be reached with pinv\n                W_out = np.zeros((N + 1, C))\n        else:\n            # Ridge Regression\n            I = np.identity(N + 1)\n            term1 = np.linalg.inv(X_train.T @ X_train + lam * I)\n            term2 = X_train.T @ Y_train_onehot\n            W_out = term1 @ term2\n        \n        # 4. Inference and Evaluation\n        output_scores = X_test @ W_out\n        predicted_classes = np.argmax(output_scores, axis=1)\n        \n        accuracy = np.mean(predicted_classes == y_test_labels)\n        results.append(accuracy)\n\n    # Final print statement\n    print(f\"[{','.join(f'{acc:.6f}' for acc in results)}]\")\n\nsolve()\n```"
        }
    ]
}