{
    "hands_on_practices": [
        {
            "introduction": "At the heart of the Bayesian brain is the mathematical process of integrating prior knowledge with sensory evidence. This exercise provides a foundational look at this process by deriving the posterior distribution for a simple, yet powerful, linear-Gaussian model. By working through the algebra, you will see precisely how the brain's updated belief can be interpreted as a precision-weighted average of its initial prediction and the incoming sensory data .",
            "id": "4063569",
            "problem": "In the Bayesian brain hypothesis, cortical computation is often modeled as probabilistic inference that combines internally generated predictions (priors) with incoming sensory evidence (likelihoods), with the relative influence modulated by precision (inverse variance). Consider a neuromorphic sensory estimation module for a single latent cause $s$ with prior $p(s)=\\mathcal{N}(0,1)$, and a scalar observation $x$ generated conditionally by $p(x\\mid s)=\\mathcal{N}(s,0.25)$. Starting only from Bayes' rule $p(s\\mid x)\\propto p(x\\mid s)\\,p(s)$ and the definition of precision as the inverse of variance, derive the posterior distribution $p(s\\mid x)$ by explicitly completing the square in the exponent of the product of Gaussians. Then, compute the posterior mean and posterior variance of $s$ given $x$, and interpret the posterior mean as a precision-weighted average of the prior mean and the observed value. \n\nExpress your final answer as a single row matrix containing the posterior mean and posterior variance, in exact symbolic form as functions of $x$. No rounding is required. No physical units are involved.",
            "solution": "The problem requires the derivation of the posterior distribution $p(s \\mid x)$ for a latent variable $s$ given an observation $x$, based on a specified prior $p(s)$ and likelihood $p(x \\mid s)$. The derivation must proceed from Bayes' rule by completing the square in the exponent of the product of Gaussian densities. Subsequently, the posterior mean and variance are to be computed and interpreted.\n\nThe provided distributions are:\n1.  Prior distribution for $s$: $p(s) = \\mathcal{N}(\\mu_{prior}, \\sigma_{prior}^2)$, where the mean is $\\mu_{prior} = 0$ and the variance is $\\sigma_{prior}^2 = 1$.\n2.  Likelihood of observation $x$ given $s$: $p(x \\mid s) = \\mathcal{N}(s, \\sigma_{like}^2)$, where the variance is $\\sigma_{like}^2 = 0.25$. Note that the mean of this distribution is the latent variable $s$.\n\nThe probability density function (PDF) for a general Gaussian distribution $\\mathcal{N}(\\mu, \\sigma^2)$ is given by $f(y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y-\\mu)^2}{2\\sigma^2}\\right)$.\nThus, the specific PDFs for the prior and likelihood are:\n$p(s) = \\frac{1}{\\sqrt{2\\pi(1)}} \\exp\\left(-\\frac{(s-0)^2}{2(1)}\\right) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{s^2}{2}\\right)$\n$p(x \\mid s) = \\frac{1}{\\sqrt{2\\pi(0.25)}} \\exp\\left(-\\frac{(x-s)^2}{2(0.25)}\\right) = \\frac{1}{\\sqrt{0.5\\pi}} \\exp\\left(-\\frac{(x-s)^2}{0.5}\\right)$\n\nAccording to Bayes' rule, the posterior distribution $p(s \\mid x)$ is proportional to the product of the likelihood and the prior:\n$p(s \\mid x) \\propto p(x \\mid s) p(s)$\n\nSubstituting the expressions for the PDFs, we get:\n$p(s \\mid x) \\propto \\left[ \\frac{1}{\\sqrt{0.5\\pi}} \\exp\\left(-\\frac{(x-s)^2}{0.5}\\right) \\right] \\left[ \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{s^2}{2}\\right) \\right]$\n\nSince we are working with proportionality, the constant normalization factors can be ignored. The posterior is proportional to the exponential part:\n$p(s \\mid x) \\propto \\exp\\left(-\\frac{(x-s)^2}{0.5}\\right) \\exp\\left(-\\frac{s^2}{2}\\right) = \\exp\\left(-\\frac{(x-s)^2}{0.5} - \\frac{s^2}{2}\\right)$\n\nLet's analyze the exponent, which we denote as $\\Phi(s)$:\n$\\Phi(s) = -\\left(\\frac{(s-x)^2}{0.5} + \\frac{s^2}{2}\\right) = -\\left(2(s-x)^2 + \\frac{s^2}{2}\\right)$\n\nTo identify the form of the posterior distribution, we expand the terms in the exponent and collect powers of $s$. A posterior of Gaussian form $\\mathcal{N}(\\mu_{post}, \\sigma_{post}^2)$ will have an exponent of the form $-\\frac{(s-\\mu_{post})^2}{2\\sigma_{post}^2} + C$, where $C$ is a constant with respect to $s$.\n$\\Phi(s) = -\\left(2(s^2 - 2sx + x^2) + \\frac{s^2}{2}\\right)$\n$\\Phi(s) = -\\left(2s^2 - 4sx + 2x^2 + \\frac{s^2}{2}\\right)$\n$\\Phi(s) = -\\left(\\left(2 + \\frac{1}{2}\\right)s^2 - 4sx + 2x^2\\right)$\n$\\Phi(s) = -\\left(\\frac{5}{2}s^2 - 4sx + 2x^2\\right)$\n\nNow, we complete the square for the terms involving $s$. The general form is $As^2+Bs+C$. We factor out $A$ from the terms involving $s$: $A(s^2 + \\frac{B}{A}s) + C = A\\left(s+\\frac{B}{2A}\\right)^2 + C - \\frac{B^2}{4A}$.\nIn our expression, the term in the parenthesis is $\\frac{5}{2}s^2 - 4sx + 2x^2$. Here, $A = \\frac{5}{2}$ and $B = -4x$.\n$\\frac{5}{2}s^2 - 4sx + 2x^2 = \\frac{5}{2}\\left(s^2 - \\frac{2}{5}(4x)s\\right) + 2x^2$\n$= \\frac{5}{2}\\left(s^2 - \\frac{8x}{5}s\\right) + 2x^2$\n$= \\frac{5}{2}\\left[\\left(s - \\frac{4x}{5}\\right)^2 - \\left(\\frac{4x}{5}\\right)^2\\right] + 2x^2$\n$= \\frac{5}{2}\\left(s - \\frac{4x}{5}\\right)^2 - \\frac{5}{2}\\left(\\frac{16x^2}{25}\\right) + 2x^2$\n$= \\frac{5}{2}\\left(s - \\frac{4x}{5}\\right)^2 - \\frac{8x^2}{5} + 2x^2$\n$= \\frac{5}{2}\\left(s - \\frac{4x}{5}\\right)^2 - \\frac{8x^2}{5} + \\frac{10x^2}{5}$\n$= \\frac{5}{2}\\left(s - \\frac{4x}{5}\\right)^2 + \\frac{2x^2}{5}$\n\nSubstituting this back into the expression for $\\Phi(s)$:\n$\\Phi(s) = -\\left(\\frac{5}{2}\\left(s - \\frac{4x}{5}\\right)^2 + \\frac{2x^2}{5}\\right) = -\\frac{5}{2}\\left(s - \\frac{4x}{5}\\right)^2 - \\frac{2x^2}{5}$\n\nTherefore, the posterior distribution is:\n$p(s \\mid x) \\propto \\exp\\left(-\\frac{1}{2} \\cdot 5 \\left(s - \\frac{4x}{5}\\right)^2\\right) \\exp\\left(-\\frac{2x^2}{5}\\right)$\n\nThe term $\\exp(-2x^2/5)$ is constant with respect to $s$ and can be absorbed into the normalization constant. The remaining expression has the form of a Gaussian PDF for $s$:\n$p(s \\mid x) \\propto \\exp\\left(-\\frac{(s - \\mu_{post})^2}{2\\sigma_{post}^2}\\right)$\n\nBy comparing the derived exponent with the general form, we can identify the posterior mean $\\mu_{post}$ and posterior variance $\\sigma_{post}^2$:\n$\\mu_{post} = \\frac{4x}{5}$\n$\\frac{1}{2\\sigma_{post}^2} = \\frac{5}{2} \\implies \\sigma_{post}^2 = \\frac{1}{5}$\n\nSo, the posterior distribution is a Gaussian: $p(s \\mid x) = \\mathcal{N}\\left(\\frac{4x}{5}, \\frac{1}{5}\\right)$.\nThe posterior mean is $\\mu_{post} = \\frac{4x}{5}$.\nThe posterior variance is $\\sigma_{post}^2 = \\frac{1}{5}$.\n\nTo interpret the posterior mean as a precision-weighted average, we first define precisions. Precision $\\lambda$ is the inverse of variance, $\\lambda = 1/\\sigma^2$.\nPrior precision: $\\lambda_{prior} = 1/\\sigma_{prior}^2 = 1/1 = 1$.\nLikelihood precision: $\\lambda_{like} = 1/\\sigma_{like}^2 = 1/0.25 = 4$.\n\nThe general formula for the posterior mean $\\mu_{post}$ in the case of a Gaussian prior and Gaussian likelihood is a precision-weighted average of the prior mean $\\mu_{prior}$ and the data $x$ (which is the mean of the likelihood function evaluated at $s=x$):\n$\\mu_{post} = \\frac{\\lambda_{prior}\\mu_{prior} + \\lambda_{like}x}{\\lambda_{prior} + \\lambda_{like}}$\nThe posterior precision is the sum of the prior and likelihood precisions:\n$\\lambda_{post} = \\lambda_{prior} + \\lambda_{like}$\nAnd the posterior variance is the inverse of the posterior precision:\n$\\sigma_{post}^2 = \\frac{1}{\\lambda_{post}} = \\frac{1}{\\lambda_{prior} + \\lambda_{like}}$\n\nLet's verify our results using these general formulas with the given values:\n$\\mu_{prior} = 0$, $\\lambda_{prior} = 1$, $\\lambda_{like} = 4$.\n$\\mu_{post} = \\frac{(1)(0) + (4)x}{1 + 4} = \\frac{4x}{5}$\n$\\sigma_{post}^2 = \\frac{1}{1 + 4} = \\frac{1}{5}$\n\nThe results match perfectly. The interpretation is that the posterior mean $\\mu_{post}$ is a weighted average of the prior's belief about the mean ($\\mu_{prior}=0$) and the sensory evidence ($x$). The weights are the respective precisions. The data has a precision of $4$ while the prior has a precision of $1$, so the sensory data has four times the influence on the final estimate as the prior belief. This can be seen by writing the posterior mean as:\n$\\mu_{post} = \\left(\\frac{\\lambda_{prior}}{\\lambda_{prior}+\\lambda_{like}}\\right)\\mu_{prior} + \\left(\\frac{\\lambda_{like}}{\\lambda_{prior}+\\lambda_{like}}\\right)x = \\left(\\frac{1}{5}\\right)(0) + \\left(\\frac{4}{5}\\right)x = \\frac{4x}{5}$.\n\nThe final answer requires the posterior mean and posterior variance in a single row matrix.\nPosterior mean: $\\frac{4x}{5}$\nPosterior variance: $\\frac{1}{5}$",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{4x}{5} & \\frac{1}{5}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "While exact Bayesian inference is elegant, its computational demands often make it intractable for the complex generative models the brain might use. This is where approximate methods like variational inference become crucial, reframing inference as an optimization problem. This practice involves deriving the Evidence Lower Bound (ELBO), the objective function that neural circuits could plausibly optimize to find an efficient approximation of the true posterior distribution .",
            "id": "4063529",
            "problem": "Consider a scalar latent cause $s$ that generates a scalar sensory datum $x$ in a brain-inspired generative model consistent with the Bayesian brain hypothesis. The brain’s internal prior over latent causes is Gaussian, $p(s)=\\mathcal{N}(0,\\sigma_{s}^{2})$, and the sensory likelihood is Gaussian, $p(x \\mid s)=\\mathcal{N}(s,\\sigma_{n}^{2})$, where $\\sigma_{s}^{2}>0$ and $\\sigma_{n}^{2}>0$ are fixed variances representing prior uncertainty and sensory noise, respectively. The brain maintains a variational posterior $q(s)=\\mathcal{N}(\\mu_{q},\\sigma_{q}^{2})$ with free parameters $\\mu_{q}\\in\\mathbb{R}$ and $\\sigma_{q}>0$.\n\nStarting from Bayes’ rule $p(s \\mid x)\\propto p(x \\mid s)p(s)$ and the definition of the Evidence Lower Bound (ELBO), namely $\\mathcal{L}(q)=\\mathbb{E}_{q(s)}\\left[\\ln p(x,s)-\\ln q(s)\\right]$, derive an analytic expression for $\\mathcal{L}(q)$ as a function of $\\mu_{q}$, $\\sigma_{q}$, $x$, $\\sigma_{s}$, and $\\sigma_{n}$. Then, compute the gradient of $\\mathcal{L}(q)$ with respect to $\\mu_{q}$ and $\\sigma_{q}$, holding the observed $x$ fixed.\n\nAssume all expectations are taken exactly under $q(s)=\\mathcal{N}(\\mu_{q},\\sigma_{q}^{2})$ and the logarithms are natural logarithms. Define any acronyms you use upon first appearance; for example, Evidence Lower Bound (ELBO) and Kullback–Leibler divergence (KLD). Express your final gradient vector as a single closed-form analytic expression in the form of a row matrix using the $\\mathrm{pmatrix}$ environment. No rounding is required, and no physical units are involved.",
            "solution": "The task is to derive an analytic expression for the Evidence Lower Bound (ELBO), denoted $\\mathcal{L}(q)$, and then compute its gradient with respect to the variational parameters $\\mu_{q}$ and $\\sigma_{q}$.\n\nThe given quantities are:\n- Prior distribution over the latent cause $s$: $p(s) = \\mathcal{N}(s; 0, \\sigma_{s}^{2})$\n- Sensory likelihood: $p(x \\mid s) = \\mathcal{N}(x; s, \\sigma_{n}^{2})$\n- Variational posterior distribution: $q(s) = \\mathcal{N}(s; \\mu_{q}, \\sigma_{q}^{2})$\n\nThe ELBO is defined as:\n$$\n\\mathcal{L}(q) = \\mathbb{E}_{q(s)}\\left[\\ln p(x, s) - \\ln q(s)\\right]\n$$\nUsing the product rule of probability, $p(x,s) = p(x \\mid s)p(s)$, we can rewrite the ELBO as:\n$$\n\\mathcal{L}(q) = \\mathbb{E}_{q(s)}\\left[\\ln p(x \\mid s) + \\ln p(s) - \\ln q(s)\\right]\n$$\nThis expression can be separated into two parts: an expected log-likelihood term and a term related to the Kullback-Leibler divergence (KLD).\n$$\n\\mathcal{L}(q) = \\mathbb{E}_{q(s)}\\left[\\ln p(x \\mid s)\\right] - \\mathbb{E}_{q(s)}\\left[\\ln q(s) - \\ln p(s)\\right]\n$$\nThe second term is the KLD between $q(s)$ and $p(s)$:\n$$\n\\mathrm{KLD}\\left(q(s) \\parallel p(s)\\right) = \\mathbb{E}_{q(s)}\\left[\\ln\\frac{q(s)}{p(s)}\\right]\n$$\nTherefore, the ELBO can be expressed as:\n$$\n\\mathcal{L}(q) = \\mathbb{E}_{q(s)}\\left[\\ln p(x \\mid s)\\right] - \\mathrm{KLD}\\left(q(s) \\parallel p(s)\\right)\n$$\nWe will now compute each term separately.\n\nFirst, we compute the expected log-likelihood, $\\mathbb{E}_{q(s)}\\left[\\ln p(x \\mid s)\\right]$.\nThe probability density function (PDF) for the likelihood is:\n$$\np(x \\mid s) = \\frac{1}{\\sqrt{2\\pi\\sigma_{n}^{2}}} \\exp\\left(-\\frac{(x - s)^{2}}{2\\sigma_{n}^{2}}\\right)\n$$\nTaking the natural logarithm, we get:\n$$\n\\ln p(x \\mid s) = -\\frac{1}{2}\\ln(2\\pi\\sigma_{n}^{2}) - \\frac{(x - s)^{2}}{2\\sigma_{n}^{2}}\n$$\nNow, we take the expectation with respect to $s \\sim q(s) = \\mathcal{N}(\\mu_{q}, \\sigma_{q}^{2})$:\n$$\n\\mathbb{E}_{q(s)}\\left[\\ln p(x \\mid s)\\right] = \\mathbb{E}_{q(s)}\\left[-\\frac{1}{2}\\ln(2\\pi\\sigma_{n}^{2}) - \\frac{x^{2} - 2xs + s^{2}}{2\\sigma_{n}^{2}}\\right]\n$$\n$$\n= -\\frac{1}{2}\\ln(2\\pi\\sigma_{n}^{2}) - \\frac{1}{2\\sigma_{n}^{2}}\\mathbb{E}_{q(s)}\\left[x^{2} - 2xs + s^{2}\\right]\n$$\nUsing the properties of the expectation $\\mathbb{E}_{q(s)}[s] = \\mu_{q}$ and $\\mathbb{E}_{q(s)}[s^{2}] = \\mathrm{Var}_{q(s)}[s] + (\\mathbb{E}_{q(s)}[s])^{2} = \\sigma_{q}^{2} + \\mu_{q}^{2}$:\n$$\n\\mathbb{E}_{q(s)}\\left[\\ln p(x \\mid s)\\right] = -\\frac{1}{2}\\ln(2\\pi\\sigma_{n}^{2}) - \\frac{1}{2\\sigma_{n}^{2}}\\left(x^{2} - 2x\\mu_{q} + (\\sigma_{q}^{2} + \\mu_{q}^{2})\\right)\n$$\n$$\n= -\\frac{1}{2}\\ln(2\\pi\\sigma_{n}^{2}) - \\frac{(x - \\mu_{q})^{2} + \\sigma_{q}^{2}}{2\\sigma_{n}^{2}}\n$$\n\nSecond, we compute the KLD between two univariate Gaussian distributions, $q(s) = \\mathcal{N}(\\mu_{q}, \\sigma_{q}^{2})$ and $p(s) = \\mathcal{N}(0, \\sigma_{s}^{2})$. The general formula is:\n$$\n\\mathrm{KLD}(\\mathcal{N}(\\mu_1, \\sigma_1^2) \\parallel \\mathcal{N}(\\mu_2, \\sigma_2^2)) = \\ln\\frac{\\sigma_2}{\\sigma_1} + \\frac{\\sigma_1^2 + (\\mu_1 - \\mu_2)^2}{2\\sigma_2^2} - \\frac{1}{2}\n$$\nSubstituting our parameters ($\\mu_1=\\mu_q, \\sigma_1=\\sigma_q, \\mu_2=0, \\sigma_2=\\sigma_s$):\n$$\n\\mathrm{KLD}\\left(q(s) \\parallel p(s)\\right) = \\ln\\frac{\\sigma_{s}}{\\sigma_{q}} + \\frac{\\sigma_{q}^{2} + (\\mu_{q} - 0)^{2}}{2\\sigma_{s}^{2}} - \\frac{1}{2}\n$$\n$$\n= \\frac{1}{2}\\ln\\left(\\frac{\\sigma_{s}^{2}}{\\sigma_{q}^{2}}\\right) + \\frac{\\sigma_{q}^{2} + \\mu_{q}^{2}}{2\\sigma_{s}^{2}} - \\frac{1}{2}\n$$\n\nNow, we combine the terms to get the full expression for the ELBO, $\\mathcal{L}(q)$:\n$$\n\\mathcal{L}(q) = \\left(-\\frac{1}{2}\\ln(2\\pi\\sigma_{n}^{2}) - \\frac{(x - \\mu_{q})^{2} + \\sigma_{q}^{2}}{2\\sigma_{n}^{2}}\\right) - \\left(\\frac{1}{2}\\ln\\sigma_{s}^{2} - \\frac{1}{2}\\ln\\sigma_{q}^{2} + \\frac{\\sigma_{q}^{2} + \\mu_{q}^{2}}{2\\sigma_{s}^{2}} - \\frac{1}{2}\\right)\n$$\n$$\n\\mathcal{L}(q) = -\\frac{1}{2}\\ln(2\\pi) - \\frac{1}{2}\\ln(\\sigma_{n}^{2}) - \\frac{(x - \\mu_{q})^{2}}{2\\sigma_{n}^{2}} - \\frac{\\sigma_{q}^{2}}{2\\sigma_{n}^{2}} - \\frac{1}{2}\\ln(\\sigma_{s}^{2}) + \\frac{1}{2}\\ln(\\sigma_{q}^{2}) - \\frac{\\sigma_{q}^{2}}{2\\sigma_{s}^{2}} - \\frac{\\mu_{q}^{2}}{2\\sigma_{s}^{2}} + \\frac{1}{2}\n$$\nThis is the analytic expression for $\\mathcal{L}(q)$. Next, we compute the partial derivatives with respect to $\\mu_{q}$ and $\\sigma_{q}$.\n\nThe gradient with respect to $\\mu_q$:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\mu_{q}} = \\frac{\\partial}{\\partial \\mu_{q}}\\left[-\\frac{(x - \\mu_{q})^{2}}{2\\sigma_{n}^{2}} - \\frac{\\mu_{q}^{2}}{2\\sigma_{s}^{2}}\\right]\n$$\n$$\n= -\\frac{2(x - \\mu_{q})(-1)}{2\\sigma_{n}^{2}} - \\frac{2\\mu_{q}}{2\\sigma_{s}^{2}}\n$$\n$$\n= \\frac{x - \\mu_{q}}{\\sigma_{n}^{2}} - \\frac{\\mu_{q}}{\\sigma_{s}^{2}}\n$$\nThis can also be written as $\\frac{x}{\\sigma_{n}^{2}} - \\mu_{q}\\left(\\frac{1}{\\sigma_{n}^{2}} + \\frac{1}{\\sigma_{s}^{2}}\\right)$.\n\nThe gradient with respect to $\\sigma_q$:\nNoting that $\\frac{1}{2}\\ln(\\sigma_{q}^{2}) = \\ln(\\sigma_{q})$, the terms involving $\\sigma_q$ are:\n$$\n-\\frac{\\sigma_{q}^{2}}{2\\sigma_{n}^{2}} + \\ln(\\sigma_{q}) - \\frac{\\sigma_{q}^{2}}{2\\sigma_{s}^{2}}\n$$\nThe partial derivative is:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\sigma_{q}} = \\frac{\\partial}{\\partial \\sigma_{q}}\\left[-\\frac{\\sigma_{q}^{2}}{2\\sigma_{n}^{2}} + \\ln(\\sigma_{q}) - \\frac{\\sigma_{q}^{2}}{2\\sigma_{s}^{2}}\\right]\n$$\n$$\n= -\\frac{2\\sigma_{q}}{2\\sigma_{n}^{2}} + \\frac{1}{\\sigma_{q}} - \\frac{2\\sigma_{q}}{2\\sigma_{s}^{2}}\n$$\n$$\n= \\frac{1}{\\sigma_{q}} - \\frac{\\sigma_{q}}{\\sigma_{n}^{2}} - \\frac{\\sigma_{q}}{\\sigma_{s}^{2}}\n$$\nThis can also be written as $\\frac{1}{\\sigma_{q}} - \\sigma_{q}\\left(\\frac{1}{\\sigma_{n}^{2}} + \\frac{1}{\\sigma_{s}^{2}}\\right)$.\n\nThe gradient vector of $\\mathcal{L}(q)$ with respect to its parameters $(\\mu_{q}, \\sigma_{q})$ is $\\nabla \\mathcal{L}(q) = \\begin{pmatrix} \\frac{\\partial \\mathcal{L}}{\\partial \\mu_{q}} & \\frac{\\partial \\mathcal{L}}{\\partial \\sigma_{q}} \\end{pmatrix}$.\n\nCombining the results, the gradient is given by the row matrix:\n$$\n\\nabla_{\\mu_{q}, \\sigma_{q}} \\mathcal{L}(q) = \\begin{pmatrix} \\frac{x - \\mu_{q}}{\\sigma_{n}^{2}} - \\frac{\\mu_{q}}{\\sigma_{s}^{2}} & \\frac{1}{\\sigma_{q}} - \\frac{\\sigma_{q}}{\\sigma_{n}^{2}} - \\frac{\\sigma_{q}}{\\sigma_{s}^{2}} \\end{pmatrix}\n$$",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{x - \\mu_{q}}{\\sigma_{n}^{2}} - \\frac{\\mu_{q}}{\\sigma_{s}^{2}} & \\frac{1}{\\sigma_{q}} - \\frac{\\sigma_{q}}{\\sigma_{n}^{2}} - \\frac{\\sigma_{q}}{\\sigma_{s}^{2}} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "The Bayesian brain hypothesis is not just an abstract computational theory; it makes concrete proposals about how inference is implemented in neural circuits. Predictive coding is a leading framework that maps these computations onto the hierarchical architecture of the cortex. This exercise invites you to model a minimal predictive coding circuit as a continuous-time dynamical system, allowing you to explore how neural activity evolves to minimize prediction error and how the stability of this process depends on feedback between cortical layers .",
            "id": "4063576",
            "problem": "In the Bayesian brain hypothesis, cortical circuits are often modeled as implementing predictive coding, in which deep-layer representation units predict sensory inputs while superficial-layer error units signal mismatches. Consider a minimal laminar predictive coding circuit for a single latent cause, grounded in a linear-Gaussian generative model and gradient flow on Variational Free Energy (VFE). Let the generative model be $y = w x + \\varepsilon$, with $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^{2})$ and a Gaussian prior $x \\sim \\mathcal{N}(\\mu,\\pi^{-1})$. Define the sensory precision $\\rho = \\sigma^{-2}$ and the prior precision $\\pi > 0$. In predictive coding, error units compute an estimate of the precision-weighted prediction error and send feedback to deep-layer representation units.\n\nStarting from the above model and the definition of VFE as a quadratic energy in prediction error and deviation from the prior mean, derive a minimal continuous-time laminar dynamical system in which:\n- Superficial-layer error activity $e$ is a leaky integrator that relaxes toward the precision-weighted prediction error at rate $\\kappa > 0$.\n- Deep-layer representation activity $x$ is driven by prior precision and laminar feedback from error units with feedback gain $g > 0$.\nAssume time has been rescaled so that the deep-layer time constant equals $1$, and analyze the internal stability around the fixed point $x^{\\star} = \\mu$, $y^{\\star} = w \\mu$ by linearizing the dynamics in the absence of external drive ($\\delta y = 0$).\n\nFrom first principles, construct the linearized two-dimensional dynamical system in terms of deviations $\\delta x$ and $\\delta e$, and derive its Jacobian. Compute the eigenvalues of this Jacobian as functions of the feedback gain $g$ and the parameters $\\pi$, $\\rho$, $\\kappa$, and $w$. Determine the critical feedback gain $g_{\\star}$ at which the eigenvalues transition from real to complex (i.e., the discriminant of the characteristic polynomial equals zero). Provide your final answer as a single closed-form analytic expression for $g_{\\star}$ in terms of $\\pi$, $\\rho$, $\\kappa$, and $w$. No numerical evaluation is required, and no units are to be included.",
            "solution": "The problem asks for the derivation of a critical feedback gain $g_{\\star}$ in a minimal predictive coding circuit model. The derivation proceeds by first establishing the continuous-time dynamics of the system, linearizing it around a specific fixed point, and then analyzing the eigenvalues of the resulting Jacobian matrix.\n\nFirst, we formalize the dynamical system based on the provided description. The system consists of a superficial-layer error unit with activity $e$ and a deep-layer representation unit with activity $x$.\n\nThe error unit activity $e$ is described as a leaky integrator relaxing towards the precision-weighted prediction error, $\\rho(y-wx)$, at a rate $\\kappa > 0$. This translates to the following ordinary differential equation (ODE):\n$$\n\\frac{de}{dt} = \\kappa \\left( \\rho(y-wx) - e \\right)\n$$\n\nThe representation unit activity $x$ is described as being driven by two terms: feedback from the error unit and the influence of the prior. The feedback from the error unit $e$ has a gain $g > 0$ and is mediated by the synaptic weight $w$. The prior's influence pulls $x$ towards its mean $\\mu$ with a strength determined by the prior precision $\\pi > 0$. The problem states that the time constant for this unit is rescaled to $1$. This leads to the ODE:\n$$\n\\frac{dx}{dt} = gwe - \\pi(x-\\mu)\n$$\nHere, the term $gwe$ represents the feedback drive, and the term $-\\pi(x-\\mu)$ represents the drive from the prior, consistent with gradient descent on the Variational Free Energy $F = \\frac{1}{2}\\rho(y-wx)^2 + \\frac{1}{2}\\pi(x-\\mu)^2$.\n\nThe complete dynamical system is thus:\n$$\n\\begin{cases}\n\\frac{dx}{dt} = gwe - \\pi(x-\\mu) \\\\\n\\frac{de}{dt} = \\kappa(\\rho(y-wx) - e)\n\\end{cases}\n$$\n\nNext, we identify the fixed point $(x^{\\star}, e^{\\star})$ around which to linearize. The problem specifies this point as corresponding to the prior mean, $x^{\\star} = \\mu$, which occurs when the sensory input is $y^{\\star} = w\\mu$. We verify that $(x, e) = (\\mu, 0)$ is indeed a fixed point for $y = y^{\\star}$:\nSetting $\\frac{dx}{dt} = 0$ and $\\frac{de}{dt} = 0$:\n$$\ngwe^{\\star} - \\pi(x^{\\star}-\\mu) = 0\n$$\n$$\n\\kappa(\\rho(y^{\\star}-wx^{\\star}) - e^{\\star}) = 0\n$$\nSubstituting $x^{\\star} = \\mu$ and $y^{\\star} = w\\mu$ into the second equation gives:\n$$\n\\kappa(\\rho(w\\mu - w\\mu) - e^{\\star}) = 0 \\implies \\kappa(-e^{\\star}) = 0 \\implies e^{\\star} = 0\n$$\nSubstituting $x^{\\star} = \\mu$ and $e^{\\star} = 0$ into the first equation gives:\n$$\ngw(0) - \\pi(\\mu-\\mu) = 0 \\implies 0 = 0\n$$\nThe conditions are satisfied, so $(x^{\\star}, e^{\\star}) = (\\mu, 0)$ is the correct fixed point.\n\nNow, we linearize the system around this fixed point. We consider small deviations $\\delta x = x - x^{\\star}$ and $\\delta e = e - e^{\\star}$. The analysis is performed for \"internal stability\", which implies no change in the external input, so $\\delta y = 0$, meaning $y$ is held constant at $y^{\\star} = w\\mu$.\nThe system for the deviations is:\n$$ \\frac{d}{dt} \\begin{pmatrix} \\delta x \\\\ \\delta e \\end{pmatrix} = J \\begin{pmatrix} \\delta x \\\\ \\delta e \\end{pmatrix} $$\nwhere $J$ is the Jacobian matrix evaluated at the fixed point $(x^{\\star}, e^{\\star}) = (\\mu, 0)$:\n$$\nJ = \\begin{pmatrix} \\frac{\\partial}{\\partial x}(\\frac{dx}{dt}) & \\frac{\\partial}{\\partial e}(\\frac{dx}{dt}) \\\\ \\frac{\\partial}{\\partial x}(\\frac{de}{dt}) & \\frac{\\partial}{\\partial e}(\\frac{de}{dt}) \\end{pmatrix}_{x=\\mu, e=0}\n$$\nThe partial derivatives are:\n$$\n\\frac{\\partial}{\\partial x}(gwe - \\pi(x-\\mu)) = -\\pi\n$$\n$$\n\\frac{\\partial}{\\partial e}(gwe - \\pi(x-\\mu)) = gw\n$$\n$$\n\\frac{\\partial}{\\partial x}(\\kappa(\\rho(y-wx) - e)) = -\\kappa \\rho w\n$$\n$$\n\\frac{\\partial}{\\partial e}(\\kappa(\\rho(y-wx) - e)) = -\\kappa\n$$\nThese derivatives are constant, so the Jacobian matrix is:\n$$\nJ = \\begin{pmatrix} -\\pi & gw \\\\ -\\kappa \\rho w & -\\kappa \\end{pmatrix}\n$$\n\nTo determine the stability and nature of the fixed point, we compute the eigenvalues $\\lambda$ of the Jacobian by solving the characteristic equation $\\det(J - \\lambda I) = 0$:\n$$\n\\det \\begin{pmatrix} -\\pi-\\lambda & gw \\\\ -\\kappa \\rho w & -\\kappa-\\lambda \\end{pmatrix} = 0\n$$\n$$\n(-\\pi-\\lambda)(-\\kappa-\\lambda) - (gw)(-\\kappa \\rho w) = 0\n$$\n$$\n(\\pi+\\lambda)(\\kappa+\\lambda) + g\\kappa\\rho w^2 = 0\n$$\n$$\n\\lambda^2 + (\\pi+\\kappa)\\lambda + (\\pi\\kappa + g\\kappa\\rho w^2) = 0\n$$\nThis is a quadratic equation for $\\lambda$. The solutions are given by the quadratic formula:\n$$\n\\lambda = \\frac{-(\\pi+\\kappa) \\pm \\sqrt{(\\pi+\\kappa)^2 - 4(\\pi\\kappa + g\\kappa\\rho w^2)}}{2}\n$$\nThe eigenvalues transition from being real to complex when the discriminant of the characteristic polynomial, $\\Delta = (\\pi+\\kappa)^2 - 4(\\pi\\kappa + g\\kappa\\rho w^2)$, changes sign from positive to negative. The critical point occurs when the discriminant is zero. We solve for the gain $g = g_{\\star}$ at which $\\Delta = 0$:\n$$\n(\\pi+\\kappa)^2 - 4(\\pi\\kappa + g_{\\star}\\kappa\\rho w^2) = 0\n$$\nExpanding the squared term:\n$$\n\\pi^2 + 2\\pi\\kappa + \\kappa^2 - 4\\pi\\kappa - 4g_{\\star}\\kappa\\rho w^2 = 0\n$$\nSimplifying the polynomial in $\\pi$ and $\\kappa$:\n$$\n\\pi^2 - 2\\pi\\kappa + \\kappa^2 - 4g_{\\star}\\kappa\\rho w^2 = 0\n$$\nRecognizing the perfect square:\n$$\n(\\pi-\\kappa)^2 - 4g_{\\star}\\kappa\\rho w^2 = 0\n$$\nNow, we isolate $g_{\\star}$:\n$$\n4g_{\\star}\\kappa\\rho w^2 = (\\pi - \\kappa)^2\n$$\nAssuming $\\kappa, \\rho, w$ are non-zero, we can solve for $g_{\\star}$:\n$$\ng_{\\star} = \\frac{(\\pi - \\kappa)^2}{4\\kappa\\rho w^2}\n$$\nThis expression provides the critical feedback gain at which the system's eigenvalues shift from real to complex, marking a transition in the dynamics from a stable node to a stable spiral (damped oscillations).",
            "answer": "$$\\boxed{\\frac{(\\pi - \\kappa)^{2}}{4\\kappa\\rho w^{2}}}$$"
        }
    ]
}