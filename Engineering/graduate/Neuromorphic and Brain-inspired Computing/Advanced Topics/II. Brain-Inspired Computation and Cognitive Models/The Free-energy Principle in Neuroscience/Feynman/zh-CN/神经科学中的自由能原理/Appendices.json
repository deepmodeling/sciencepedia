{
    "hands_on_practices": [
        {
            "introduction": "自由能原理不仅是一个理论框架，也是一个强大的计算工具。要真正掌握它，动手实践至关重要。本章将引导您通过一系列具体的计算和编程练习，将抽象的数学概念转化为可操作的技能。我们将从自由能原理的核心机制——预测编码（predictive coding）开始。这个练习将让您亲手实现感知推断过程中的信念更新，这是大脑最小化“惊奇”或预测误差的基础动力学过程。通过这个练习，您将深入理解大脑如何利用自上而下的预测和自下而上的感觉信号来不断修正其对外部世界成因的表征。",
            "id": "4028586",
            "problem": "考虑一个适用于自由能原理下预测编码的线性高斯生成模型。设潜状态向量为 $\\mathbf{s} \\in \\mathbb{R}^m$，其先验分布为零均值高斯分布 $\\mathbf{s} \\sim \\mathcal{N}(\\mathbf{0}, I)$，其中 $I$ 是 $m \\times m$ 的单位矩阵。设观测向量为 $\\mathbf{y} \\in \\mathbb{R}^n$，根据 $\\mathbf{y} \\mid \\mathbf{s} \\sim \\mathcal{N}(W \\mathbf{s}, \\Sigma)$ 生成，其中 $W \\in \\mathbb{R}^{n \\times m}$ 是一个已知的生成映射，$\\Sigma \\in \\mathbb{R}^{n \\times n}$ 是一个已知的正定观测噪声协方差。考虑一个高斯变分后验 $q(\\mathbf{s}) = \\mathcal{N}(\\mu_q, \\Sigma_q)$，其均值为 $\\mu_q \\in \\mathbb{R}^m$，协方差为正定的 $\\Sigma_q \\in \\mathbb{R}^{m \\times m}$。\n\n从第一性原理出发，仅依赖于线性高斯模型的贝叶斯法则、作为对数模型证据下界的负变分自由能的定义，以及拉普拉斯近似（众数周围的高斯近似），推导在由 $\\Sigma_q$ 进行自然梯度预处理和单位先验协方差 $I$ 的条件下，均值参数 $\\mu_q$ 的连续时间预测编码动力学。使用这些动力学来定义一个对应于步长 $\\eta=1$ 的单个显式欧拉步的离散更新，并计算应用于上述模型的一次迭代中均值参数的变化，记为 $\\Delta \\mu_q$。您的程序必须实现给定 $(W, \\Sigma, \\mu_q, \\Sigma_q, \\mathbf{y})$ 时 $\\Delta \\mu_q$ 的计算，并且不得依赖任何无法从所述基本定律和定义中推导出的公式。\n\n为以下测试套件实现该计算。每个案例提供维度为 $n = 2$ 和 $m = 2$ 的 $(W, \\Sigma, \\mu_q, \\Sigma_q, \\mathbf{y})$：\n\n- 案例 $1$（一般良态情况）：\n  - $W = \\begin{bmatrix} 1.0  0.2 \\\\ 0.0  1.0 \\end{bmatrix}$,\n  - $\\Sigma = \\begin{bmatrix} 0.5  0.0 \\\\ 0.0  0.5 \\end{bmatrix}$,\n  - $\\mu_q = \\begin{bmatrix} 0.1 \\\\ -0.2 \\end{bmatrix}$,\n  - $\\Sigma_q = \\begin{bmatrix} 0.8  0.0 \\\\ 0.0  0.6 \\end{bmatrix}$,\n  - $\\mathbf{y} = \\begin{bmatrix} 1.5 \\\\ -0.5 \\end{bmatrix}$。\n\n- 案例 $2$（高观测噪声）：\n  - $W = \\begin{bmatrix} 1.0  -0.5 \\\\ 0.3  1.2 \\end{bmatrix}$,\n  - $\\Sigma = \\begin{bmatrix} 5.0  0.0 \\\\ 0.0  5.0 \\end{bmatrix}$,\n  - $\\mu_q = \\begin{bmatrix} -0.3 \\\\ 0.4 \\end{bmatrix}$,\n  - $\\Sigma_q = \\begin{bmatrix} 1.1  0.0 \\\\ 0.0  0.9 \\end{bmatrix}$,\n  - $\\mathbf{y} = \\begin{bmatrix} 0.2 \\\\ -1.0 \\end{bmatrix}$。\n\n- 案例 $3$（相关的后验协方差）：\n  - $W = \\begin{bmatrix} 0.7  0.4 \\\\ -0.2  1.3 \\end{bmatrix}$,\n  - $\\Sigma = \\begin{bmatrix} 0.6  0.0 \\\\ 0.0  0.4 \\end{bmatrix}$,\n  - $\\mu_q = \\begin{bmatrix} 0.5 \\\\ 0.1 \\end{bmatrix}$,\n  - $\\Sigma_q = \\begin{bmatrix} 0.7  0.3 \\\\ 0.3  0.9 \\end{bmatrix}$,\n  - $\\mathbf{y} = \\begin{bmatrix} 1.0 \\\\ 0.0 \\end{bmatrix}$。\n\n- 案例 $4$（秩亏生成映射）：\n  - $W = \\begin{bmatrix} 1.0  0.0 \\\\ 0.0  0.0 \\end{bmatrix}$,\n  - $\\Sigma = \\begin{bmatrix} 0.1  0.0 \\\\ 0.0  0.1 \\end{bmatrix}$,\n  - $\\mu_q = \\begin{bmatrix} 2.0 \\\\ -1.0 \\end{bmatrix}$,\n  - $\\Sigma_q = \\begin{bmatrix} 1.0  0.0 \\\\ 0.0  1.0 \\end{bmatrix}$,\n  - $\\mathbf{y} = \\begin{bmatrix} 1.0 \\\\ 0.0 \\end{bmatrix}$。\n\n- 案例 $5$（强感知精度）：\n  - $W = \\begin{bmatrix} 0.9  -0.1 \\\\ 0.2  0.8 \\end{bmatrix}$,\n  - $\\Sigma = \\begin{bmatrix} 0.01  0.0 \\\\ 0.0  0.02 \\end{bmatrix}$,\n  - $\\mu_q = \\begin{bmatrix} -0.1 \\\\ 0.2 \\end{bmatrix}$,\n  - $\\Sigma_q = \\begin{bmatrix} 0.5  0.0 \\\\ 0.0  0.5 \\end{bmatrix}$,\n  - $\\mathbf{y} = \\begin{bmatrix} 0.0 \\\\ 1.0 \\end{bmatrix}$。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，其中每个条目是针对一个测试案例的二维变化向量 $\\Delta \\mu_q$，四舍五入到六位小数，并表示为 Python 风格的列表。例如，格式应为 $[\\,[\\delta\\mu_{1,1}, \\delta\\mu_{1,2}], [\\delta\\mu_{2,1}, \\delta\\mu_{2,2}], \\dots\\,]$。不涉及物理单位。不使用角度。所有数值输出必须是实值浮点数。该算法必须根据给定的基本定义实现，不得调用任何预先指定的快捷公式。",
            "solution": "我们从一个线性高斯生成模型和一个高斯变分后验开始。潜状态为 $\\mathbf{s} \\in \\mathbb{R}^m$，其先验为 $p(\\mathbf{s}) = \\mathcal{N}(\\mathbf{0}, I)$，其中 $I$ 是 $m \\times m$ 的单位矩阵。似然为 $p(\\mathbf{y} \\mid \\mathbf{s}) = \\mathcal{N}(W \\mathbf{s}, \\Sigma)$，其中 $W \\in \\mathbb{R}^{n \\times m}$，$\\Sigma \\in \\mathbb{R}^{n \\times n}$ 是一个正定协方差。变分后验为 $q(\\mathbf{s}) = \\mathcal{N}(\\mu_q, \\Sigma_q)$，其中 $\\mu_q \\in \\mathbb{R}^m$，$\\Sigma_q \\in \\mathbb{R}^{m \\times m}$ 是正定的。\n\n自由能原理采用变分自由能来作为对数证据的下界。负变分自由能（即证据下界）由下式给出：\n$$\n\\mathcal{L}(\\mu_q, \\Sigma_q) = \\mathbb{E}_{q(\\mathbf{s})}[\\log p(\\mathbf{y}, \\mathbf{s})] - \\mathbb{E}_{q(\\mathbf{s})}[\\log q(\\mathbf{s})].\n$$\n在拉普拉斯近似和自然梯度动力学下，通过对变分参数在 $\\mathcal{L}$ 上执行梯度上升，预测编码得以涌现。此过程通常由高斯族的费雪信息度量进行预处理，在此，对于均值参数，该度量由 $\\Sigma_q$ 表示。为推导更新规则，我们从基本的概率定义入手。\n\n首先，写出联合密度：\n$$\n\\log p(\\mathbf{y}, \\mathbf{s}) = \\log p(\\mathbf{y}\\mid \\mathbf{s}) + \\log p(\\mathbf{s}).\n$$\n对于高斯项，\n$$\n\\log p(\\mathbf{y}\\mid \\mathbf{s}) = -\\tfrac{1}{2}(\\mathbf{y} - W \\mathbf{s})^\\top \\Sigma^{-1}(\\mathbf{y} - W \\mathbf{s}) + \\text{const},\n$$\n$$\n\\log p(\\mathbf{s}) = -\\tfrac{1}{2}\\mathbf{s}^\\top I \\mathbf{s} + \\text{const} = -\\tfrac{1}{2}\\mathbf{s}^\\top \\mathbf{s} + \\text{const}.\n$$\n变分后验为\n$$\n\\log q(\\mathbf{s}) = -\\tfrac{1}{2}(\\mathbf{s} - \\mu_q)^\\top \\Sigma_q^{-1}(\\mathbf{s} - \\mu_q) + \\text{const}.\n$$\n我们关注均值的更新。在对均值使用拉普拉斯近似下，我们将负自由能（其在相差一个常数的情况下等价于期望负对数后验）视为 $\\mu_q$ 的函数，通过在均值处求值来近似期望（这在拉普拉斯变分方案中是标准做法）。定义能量函数\n$$\nF(\\mu_q) = \\tfrac{1}{2}(\\mathbf{y} - W \\mu_q)^\\top \\Sigma^{-1}(\\mathbf{y} - W \\mu_q) + \\tfrac{1}{2}\\mu_q^\\top I \\mu_q,\n$$\n这是在 $\\mu_q$ 处求值的负对数后验（第一项对应于似然，第二项对应于先验，其中 $I$ 是先验精度）。最小化 $F(\\mu_q)$ 对应于最大化后验概率，并等价于在拉普拉斯近似下关于 $\\mu_q$ 最大化证据下界。\n\n计算 $F(\\mu_q)$ 关于 $\\mu_q$ 的梯度。使用标准矩阵微积分，\n$$\n\\nabla_{\\mu_q} F(\\mu_q) = - W^\\top \\Sigma^{-1}(\\mathbf{y} - W \\mu_q) + I \\mu_q.\n$$\n预测编码可以表示为在 $F(\\mu_q)$ 上的梯度下降，或在 $\\mathcal{L}$ 上并由 $\\Sigma_q$ 进行自然梯度预处理的梯度上升。指数族高斯分布中均值参数的自然梯度步可以通过将普通欧几里得梯度左乘 $\\Sigma_q$ 来近似，从而产生一个考虑了后验协方差所隐含的局部曲率的方向。因此，在 $\\mathcal{L}$ 上进行自然梯度上升（等价于在 $F$ 上进行自然梯度下降）时，$\\mu_q$ 的连续时间动力学变为\n$$\n\\frac{d\\mu_q}{dt} = \\Sigma_q\\left[ W^\\top \\Sigma^{-1}(\\mathbf{y} - W \\mu_q) - I \\mu_q \\right].\n$$\n取步长 $\\eta = 1$ 的单个显式欧拉步，可得到离散变化\n$$\n\\Delta \\mu_q = \\Sigma_q\\left[ W^\\top \\Sigma^{-1}(\\mathbf{y} - W \\mu_q) - I \\mu_q \\right].\n$$\n这个 $\\Delta \\mu_q$ 就是在规定假设（线性高斯模型、单位先验协方差、拉普拉斯近似以及由 $\\Sigma_q$ 进行的自然梯度预处理）下，一次预测编码迭代中后验均值的所求变化。\n\n算法设计：\n- 输入：矩阵 $W$、$\\Sigma$、$\\Sigma_q$ 和向量 $\\mu_q$、$\\mathbf{y}$。\n- 验证维度：$W$ 是 $n \\times m$，$\\Sigma$ 是 $n \\times n$，$\\Sigma_q$ 是 $m \\times m$，$\\mu_q \\in \\mathbb{R}^m$，$\\mathbf{y} \\in \\mathbb{R}^n$。\n- 计算感知预测误差 $\\mathbf{e}_y = \\mathbf{y} - W \\mu_q$。\n- 计算精度加权误差 $\\mathbf{r}_y = \\Sigma^{-1} \\mathbf{e}_y$。\n- 计算反向传播到潜层的精度加权误差 $\\mathbf{g} = W^\\top \\mathbf{r}_y$。\n- 计算先验贡献 $\\mathbf{p} = I \\mu_q = \\mu_q$。\n- 组合成欧几里得梯度方向 $\\mathbf{d} = \\mathbf{g} - \\mathbf{p}$。\n- 由 $\\Sigma_q$ 预处理以获得自然梯度步 $\\Delta \\mu_q = \\Sigma_q \\mathbf{d}$。\n- 按规定对 $\\Delta \\mu_q$ 进行数值四舍五入后输出。\n\n测试套件中包含了边界情况：\n- 案例 1 验证一个一般的良态场景。\n- 案例 2 使用大的观测噪声 $\\Sigma$，抑制感知项相对于先验项的作用。\n- 案例 3 使用相关的 $\\Sigma_q$，在不同维度上运用自然梯度预处理。\n- 案例 4 使用秩亏的 $W$，将感知信息限制在一个子空间内。\n- 案例 5 使用非常小的观测噪声（高精度），强调感知项并导致大的修正。\n\n最终程序为每个测试案例实现上述步骤，并以所要求的单行格式打印结果，即一个由列表组成的逗号分隔列表，其中每个分量都四舍五入到六位小数。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef predictive_coding_delta_mu(W, Sigma, mu_q, Sigma_q, y):\n    \"\"\"\n    Compute one predictive coding iteration change in posterior mean (Delta mu_q)\n    under a linear-Gaussian generative model with identity prior covariance,\n    Laplace approximation, and natural gradient preconditioning by Sigma_q.\n\n    Delta mu_q = Sigma_q * [ W^T * Sigma^{-1} * (y - W * mu_q) - mu_q ]\n    \"\"\"\n    # Sensory prediction error\n    e_y = y - W @ mu_q\n    # Precision-weighted error\n    r_y = np.linalg.solve(Sigma, e_y)\n    # Backpropagated precision-weighted error to latent space\n    g = W.T @ r_y\n    # Prior term (identity prior precision)\n    p = mu_q\n    # Euclidean gradient direction\n    d = g - p\n    # Natural gradient step preconditioned by Sigma_q\n    delta_mu = Sigma_q @ d\n    return delta_mu\n\ndef format_vector(vec):\n    # Round to six decimals and format as Python-style list\n    return \"[\" + \",\".join(f\"{x:.6f}\" for x in vec.tolist()) + \"]\"\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1\n        (\n            np.array([[1.0, 0.2],\n                      [0.0, 1.0]]),\n            np.array([[0.5, 0.0],\n                      [0.0, 0.5]]),\n            np.array([0.1, -0.2]),\n            np.array([[0.8, 0.0],\n                      [0.0, 0.6]]),\n            np.array([1.5, -0.5])\n        ),\n        # Case 2\n        (\n            np.array([[1.0, -0.5],\n                      [0.3,  1.2]]),\n            np.array([[5.0, 0.0],\n                      [0.0, 5.0]]),\n            np.array([-0.3, 0.4]),\n            np.array([[1.1, 0.0],\n                      [0.0, 0.9]]),\n            np.array([0.2, -1.0])\n        ),\n        # Case 3\n        (\n            np.array([[ 0.7, 0.4],\n                      [-0.2, 1.3]]),\n            np.array([[0.6, 0.0],\n                      [0.0, 0.4]]),\n            np.array([0.5, 0.1]),\n            np.array([[0.7, 0.3],\n                      [0.3, 0.9]]),\n            np.array([1.0, 0.0])\n        ),\n        # Case 4\n        (\n            np.array([[1.0, 0.0],\n                      [0.0, 0.0]]),\n            np.array([[0.1, 0.0],\n                      [0.0, 0.1]]),\n            np.array([2.0, -1.0]),\n            np.array([[1.0, 0.0],\n                      [0.0, 1.0]]),\n            np.array([1.0, 0.0])\n        ),\n        # Case 5\n        (\n            np.array([[0.9, -0.1],\n                      [0.2,  0.8]]),\n            np.array([[0.01, 0.0],\n                      [0.0,  0.02]]),\n            np.array([-0.1, 0.2]),\n            np.array([[0.5, 0.0],\n                      [0.0, 0.5]]),\n            np.array([0.0, 1.0])\n        ),\n    ]\n\n    results = []\n    for W, Sigma, mu_q, Sigma_q, y in test_cases:\n        delta_mu = predictive_coding_delta_mu(W, Sigma, mu_q, Sigma_q, y)\n        results.append(format_vector(delta_mu))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "掌握了感知推断的基础后，我们将进入主动推断（active inference）的领域，即大脑如何选择行动。智能体并非被动地接收信息，而是主动地探索环境以达成目标并减少不确定性。这个练习将指导您计算不同行动策略的期望自由能（expected free energy），该期望自由能巧妙地平衡了两种驱动力：实现偏好结果的“实用价值”（extrinsic value）和减少对世界状态不确定性的“认知价值”（epistemic value）。通过这个练习，您将学会如何量化决策过程，并理解为何一个理想的智能体必须同时是目标导向的执行者和好奇的信息搜集者。",
            "id": "4028595",
            "problem": "考虑一个根据脑建模和计算神经科学中的自由能原理（FEP）制定的单步决策问题。隐藏状态是二元的，$s \\in \\{s_{0}, s_{1}\\}$，当前的信念（识别密度）$Q(s)$ 由 $Q(s_{0}) = 0.6$ 和 $Q(s_{1}) = 0.4$ 给出。观测是二元的，$o \\in \\{0,1\\}$，存在两个候选的单步策略 $\\pi_{1}$ 和 $\\pi_{2}$，分别对应两个行动 $a_{1}$ 和 $a_{2}$，它们如下调节似然映射 $P(o \\mid s, a)$：\n- 在行动 $a_{1}$ 下：$P(o=1 \\mid s_{0}, a_{1}) = 0.8$， $P(o=0 \\mid s_{0}, a_{1}) = 0.2$， $P(o=1 \\mid s_{1}, a_{1}) = 0.3$， $P(o=0 \\mid s_{1}, a_{1}) = 0.7$。\n- 在行动 $a_{2}$ 下：$P(o=1 \\mid s_{0}, a_{2}) = 0.6$， $P(o=0 \\mid s_{0}, a_{2}) = 0.4$， $P(o=1 \\mid s_{1}, a_{2}) = 0.1$， $P(o=0 \\mid s_{1}, a_{2}) = 0.9$。\n\n偏好的结果由先验偏好分布 $P^{\\ast}(o)$ 编码，其中 $P^{\\ast}(1) = 0.9$ 和 $P^{\\ast}(0) = 0.1$。假设只有一个时间步，并且用于评分预期自由能的隐藏状态先验与当前信念匹配，即 $P(s) = Q(s)$。使用自然对数 $\\ln$。\n\n从单个观测的变分自由能的定义出发（该定义用识别分布 $Q(s \\mid o, \\pi)$ 和带有结果偏好 $P^{\\ast}(o)$ 的生成模型来表示），并且仅使用变分贝叶斯推断的核心定义（例如，Kullback-Leibler散度（KLD）、互信息和全期望定律），推导策略 $\\pi$ 的单步预期自由能 $\\mathcal{G}(\\pi)$ 的表达式，该表达式可分为 (i) 一个依赖于 $P^{\\ast}(o)$ 和预测分布 $Q(o \\mid \\pi)$ 的外在项，以及 (ii) 一个在策略 $\\pi$ 下隐藏状态和结果之间的互信息的认知项。然后，数值计算 $\\mathcal{G}(\\pi_{1})$ 和 $\\mathcal{G}(\\pi_{2})$，并确定哪个策略能最小化 $\\mathcal{G}(\\pi)$。\n\n作为最终答案，报告最小化策略 $\\pi_{i}$ 的索引 $i \\in \\{1,2\\}$。最终索引无需四舍五入。如果需要中间数值计算，请使用精确算术和 $\\ln$ 求值；任何近似值都应在推导过程中明确指出。",
            "solution": "用户希望我解决一个基于自由能原理的问题。\n\n### 第一步：提取已知条件\n-   **隐藏状态：** $s \\in \\{s_{0}, s_{1}\\}$。\n-   **识别密度（当前信念）：** $Q(s_{0}) = 0.6$， $Q(s_{1}) = 0.4$。\n-   **观测：** $o \\in \\{0, 1\\}$。\n-   **策略：** $\\pi_1$（行动 $a_1$）和 $\\pi_2$（行动 $a_2$）。\n-   **行动 $a_1$ 的似然映射：**\n    -   $P(o=1 \\mid s_{0}, a_{1}) = 0.8$, $P(o=0 \\mid s_{0}, a_{1}) = 0.2$。\n    -   $P(o=1 \\mid s_{1}, a_{1}) = 0.3$, $P(o=0 \\mid s_{1}, a_{1}) = 0.7$。\n-   **行动 $a_2$ 的似然映射：**\n    -   $P(o=1 \\mid s_{0}, a_{2}) = 0.6$, $P(o=0 \\mid s_{0}, a_{2}) = 0.4$。\n    -   $P(o=1 \\mid s_{1}, a_{2}) = 0.1$, $P(o=0 \\mid s_{1}, a_{2}) = 0.9$。\n-   **偏好的结果（先验偏好）：** $P^{\\ast}(1) = 0.9$， $P^{\\ast}(0) = 0.1$。\n-   **假设：** 用于评分预期自由能的隐藏状态先验是当前信念：$P(s) = Q(s)$。\n-   **对数：** 使用自然对数 $\\ln$。\n\n### 第二步：使用提取的已知条件进行验证\n该问题在主动推断和自由能原理的既定框架内具有科学依据。术语和表述在计算神经科学中是标准的。问题定义明确，提供了所有必要的数值和清晰无歧义的目标。所有条件分布的给定概率之和为1，且没有内部矛盾。数据是一致的。问题是客观且可形式化的。\n\n### 第三步：结论与行动\n问题有效。将提供完整解答。\n\n### 预期自由能表达式的推导\n\n一个策略 $\\pi$ 的预期自由能 $\\mathcal{G}(\\pi)$ 是在该策略下预期的自由能，对该策略预测的结果 $o$ 进行平均。这可以写作：\n$$ \\mathcal{G}(\\pi) = \\mathbb{E}_{Q(o|\\pi)}[F(o|\\pi)] $$\n其中 $F(o|\\pi)$ 是与特定未来结果 $o$ 相关的变分自由能。问题陈述我们应该从一个涉及识别分布 $Q(s|o,\\pi)$ 和带有结果偏好 $P^\\ast(o)$ 的生成模型的自由能定义出发。自由能是识别密度 $Q$ 和生成模型 $P$ 的一个泛函，定义为 $F = \\mathbb{E}_{Q}[\\ln Q - \\ln P]$。对于主动推断，生成模型体现了智能体的目标。我们定义一个简单的生成模型，其中状态和结果是独立的，结果遵循偏好分布 $P^\\ast(o)$，状态遵循先验分布 $P(s)$。因此，联合概率为 $P(s,o) = P(s)P^\\ast(o)$。\n\n对于未来结果 $o$ 的自由能是近似后验信念 $Q(s|o,\\pi)$ 与这个目标导向的生成模型 $P(s,o)$ 之间的Kullback-Leibler（KL）散度：\n$$ F(o|\\pi) = \\mathbb{E}_{Q(s|o,\\pi)}[\\ln Q(s|o,\\pi) - \\ln P(s,o)] $$\n代入 $P(s,o) = P(s)P^\\ast(o)$：\n$$ F(o|\\pi) = \\mathbb{E}_{Q(s|o,\\pi)}[\\ln Q(s|o,\\pi) - \\ln(P(s)P^\\ast(o))] $$\n$$ F(o|\\pi) = \\mathbb{E}_{Q(s|o,\\pi)}[\\ln \\frac{Q(s|o,\\pi)}{P(s)}] - \\ln P^\\ast(o) $$\n第一项是KL散度 $D_{KL}[Q(s|o,\\pi) || P(s)]$。所以：\n$$ F(o|\\pi) = D_{KL}[Q(s|o,\\pi) || P(s)] - \\ln P^\\ast(o) $$\n现在，我们通过对结果的预测分布 $Q(o|\\pi) = \\sum_s P(o|s,\\pi)Q(s)$ 取 $F(o|\\pi)$ 的期望来计算预期自由能 $\\mathcal{G}(\\pi)$：\n$$ \\mathcal{G}(\\pi) = \\mathbb{E}_{Q(o|\\pi)}[D_{KL}[Q(s|o,\\pi) || P(s)] - \\ln P^\\ast(o)] $$\n$$ \\mathcal{G}(\\pi) = \\mathbb{E}_{Q(o|\\pi)}[D_{KL}[Q(s|o,\\pi) || P(s)]] + \\mathbb{E}_{Q(o|\\pi)}[-\\ln P^\\ast(o)] $$\n这个表达式分为两项。\n\n第一项是**认知项**，它量化了关于隐藏状态的预期信息增益：\n$$ \\mathcal{G}_I(\\pi) = \\mathbb{E}_{Q(o|\\pi)}[D_{KL}[Q(s|o,\\pi) || P(s)]] $$\n根据问题条件，状态的先验是当前信念，$P(s) = Q(s)$，该项变为：\n$$ \\mathcal{G}_I(\\pi) = \\mathbb{E}_{Q(o|\\pi)}[D_{KL}[Q(s|o,\\pi) || Q(s)]] = \\sum_o Q(o|\\pi) \\sum_s Q(s|o,\\pi) \\ln\\frac{Q(s|o,\\pi)}{Q(s)} $$\n根据定义，这是在策略 $\\pi$ 下隐藏状态 $s$ 和结果 $o$ 之间的互信息 $I(s;o|\\pi)$。\n\n第二项是**外在项**（或实用价值），它量化了预期结果与偏好结果的偏离程度：\n$$ \\mathcal{G}_E(\\pi) = \\mathbb{E}_{Q(o|\\pi)}[-\\ln P^\\ast(o)] = \\sum_o Q(o|\\pi) (-\\ln P^\\ast(o)) $$\n这是预测分布 $Q(o|\\pi)$ 和偏好分布 $P^\\ast(o)$ 之间的交叉熵。该项如要求的那样依赖于 $P^{\\ast}(o)$ 和 $Q(o|\\pi)$。\n\n因此，需要最小化的单步预期自由能是：\n$$ \\mathcal{G}(\\pi) = \\mathcal{G}_E(\\pi) + \\mathcal{G}_I(\\pi) $$\n\n### 策略 $\\pi_1$（行动 $a_1$）的计算\n\n当前信念为 $Q(s_0) = 0.6$ 和 $Q(s_1) = 0.4$。\n\n1.  **预测结果分布 $Q(o|\\pi_1)$**：\n    $Q(o=1|\\pi_1) = \\sum_s P(o=1|s, a_1)Q(s) = (0.8)(0.6) + (0.3)(0.4) = 0.48 + 0.12 = 0.6$。\n    $Q(o=0|\\pi_1) = \\sum_s P(o=0|s, a_1)Q(s) = (0.2)(0.6) + (0.7)(0.4) = 0.12 + 0.28 = 0.4$。\n\n2.  **外在项 $\\mathcal{G}_E(\\pi_1)$**：\n    $\\mathcal{G}_E(\\pi_1) = Q(o=0|\\pi_1)(-\\ln P^\\ast(0)) + Q(o=1|\\pi_1)(-\\ln P^\\ast(1))$\n    $\\mathcal{G}_E(\\pi_1) = 0.4(-\\ln 0.1) + 0.6(-\\ln 0.9) = 0.4\\ln(10) - 0.6\\ln(0.9)$。\n\n3.  **认知项 $\\mathcal{G}_I(\\pi_1)$**：\n    首先，我们找到后验信念 $Q(s|o, \\pi_1) = \\frac{P(o|s,a_1)Q(s)}{Q(o|\\pi_1)}$。\n    对于 $o=1$：\n    $Q(s_0|o=1, \\pi_1) = \\frac{0.8 \\times 0.6}{0.6} = 0.8$。\n    $Q(s_1|o=1, \\pi_1) = \\frac{0.3 \\times 0.4}{0.6} = 0.2$。\n    对于 $o=0$：\n    $Q(s_0|o=0, \\pi_1) = \\frac{0.2 \\times 0.6}{0.4} = 0.3$。\n    $Q(s_1|o=0, \\pi_1) = \\frac{0.7 \\times 0.4}{0.4} = 0.7$。\n\n    接下来，我们计算KL散度：\n    $D_{KL}[Q(s|o=1,\\pi_1) || Q(s)] = 0.8\\ln(\\frac{0.8}{0.6}) + 0.2\\ln(\\frac{0.2}{0.4}) = 0.8\\ln(\\frac{4}{3}) + 0.2\\ln(\\frac{1}{2})$。\n    $D_{KL}[Q(s|o=0,\\pi_1) || Q(s)] = 0.3\\ln(\\frac{0.3}{0.6}) + 0.7\\ln(\\frac{0.7}{0.4}) = 0.3\\ln(\\frac{1}{2}) + 0.7\\ln(\\frac{7}{4})$。\n\n    $\\mathcal{G}_I(\\pi_1) = Q(o=1|\\pi_1) D_{KL}[Q(s|o=1,\\pi_1) || Q(s)] + Q(o=0|\\pi_1) D_{KL}[Q(s|o=0,\\pi_1) || Q(s)]$\n    $\\mathcal{G}_I(\\pi_1) = 0.6(0.8\\ln(\\frac{4}{3}) + 0.2\\ln(\\frac{1}{2})) + 0.4(0.3\\ln(\\frac{1}{2}) + 0.7\\ln(\\frac{7}{4}))$\n    $\\mathcal{G}_I(\\pi_1) = 0.48\\ln(\\frac{4}{3}) + 0.12\\ln(\\frac{1}{2}) + 0.12\\ln(\\frac{1}{2}) + 0.28\\ln(\\frac{7}{4})$\n    $\\mathcal{G}_I(\\pi_1) = 0.48\\ln(\\frac{4}{3}) + 0.24\\ln(\\frac{1}{2}) + 0.28\\ln(\\frac{7}{4})$。\n\n4.  **总预期自由能 $\\mathcal{G}(\\pi_1)$**：\n    $\\mathcal{G}(\\pi_1) = \\mathcal{G}_E(\\pi_1) + \\mathcal{G}_I(\\pi_1)$\n    $\\mathcal{G}(\\pi_1) = (0.4\\ln(10) - 0.6\\ln(0.9)) + (0.48\\ln(\\frac{4}{3}) - 0.24\\ln(2) + 0.28\\ln(\\frac{7}{4}))$\n    使用数值：$\\ln(10) \\approx 2.3026$, $\\ln(0.9) \\approx -0.1054$, $\\ln(4/3) \\approx 0.2877$, $\\ln(2) \\approx 0.6931$, $\\ln(7/4) \\approx 0.5596$。\n    $\\mathcal{G}_E(\\pi_1) \\approx 0.4(2.3026) + 0.6(0.1054) = 0.92104 + 0.06324 = 0.98428$。\n    $\\mathcal{G}_I(\\pi_1) \\approx 0.48(0.2877) - 0.24(0.6931) + 0.28(0.5596) = 0.13810 - 0.16634 + 0.15669 = 0.12845$。\n    $\\mathcal{G}(\\pi_1) \\approx 0.98428 + 0.12845 = 1.11273$。\n\n### 策略 $\\pi_2$（行动 $a_2$）的计算\n\n1.  **预测结果分布 $Q(o|\\pi_2)$**：\n    $Q(o=1|\\pi_2) = \\sum_s P(o=1|s, a_2)Q(s) = (0.6)(0.6) + (0.1)(0.4) = 0.36 + 0.04 = 0.4$。\n    $Q(o=0|\\pi_2) = \\sum_s P(o=0|s, a_2)Q(s) = (0.4)(0.6) + (0.9)(0.4) = 0.24 + 0.36 = 0.6$。\n\n2.  **外在项 $\\mathcal{G}_E(\\pi_2)$**：\n    $\\mathcal{G}_E(\\pi_2) = Q(o=0|\\pi_2)(-\\ln P^\\ast(0)) + Q(o=1|\\pi_2)(-\\ln P^\\ast(1))$\n    $\\mathcal{G}_E(\\pi_2) = 0.6(-\\ln 0.1) + 0.4(-\\ln 0.9) = 0.6\\ln(10) - 0.4\\ln(0.9)$。\n\n3.  **认知项 $\\mathcal{G}_I(\\pi_2)$**：\n    首先，我们找到后验信念 $Q(s|o, \\pi_2) = \\frac{P(o|s,a_2)Q(s)}{Q(o|\\pi_2)}$。\n    对于 $o=1$：\n    $Q(s_0|o=1, \\pi_2) = \\frac{0.6 \\times 0.6}{0.4} = 0.9$。\n    $Q(s_1|o=1, \\pi_2) = \\frac{0.1 \\times 0.4}{0.4} = 0.1$。\n    对于 $o=0$：\n    $Q(s_0|o=0, \\pi_2) = \\frac{0.4 \\times 0.6}{0.6} = 0.4$。\n    $Q(s_1|o=0, \\pi_2) = \\frac{0.9 \\times 0.4}{0.6} = 0.6$。\n\n    接下来，我们计算KL散度：\n    $D_{KL}[Q(s|o=1,\\pi_2) || Q(s)] = 0.9\\ln(\\frac{0.9}{0.6}) + 0.1\\ln(\\frac{0.1}{0.4}) = 0.9\\ln(1.5) + 0.1\\ln(0.25)$。\n    $D_{KL}[Q(s|o=0,\\pi_2) || Q(s)] = 0.4\\ln(\\frac{0.4}{0.6}) + 0.6\\ln(\\frac{0.6}{0.4}) = 0.4\\ln(\\frac{2}{3}) + 0.6\\ln(1.5) = 0.2\\ln(1.5)$。\n\n    $\\mathcal{G}_I(\\pi_2) = Q(o=1|\\pi_2) D_{KL}[Q(s|o=1,\\pi_2) || Q(s)] + Q(o=0|\\pi_2) D_{KL}[Q(s|o=0,\\pi_2) || Q(s)]$\n    $\\mathcal{G}_I(\\pi_2) = 0.4(0.9\\ln(1.5) + 0.1\\ln(0.25)) + 0.6(0.2\\ln(1.5))$\n    $\\mathcal{G}_I(\\pi_2) = 0.36\\ln(1.5) + 0.04\\ln(0.25) + 0.12\\ln(1.5) = 0.48\\ln(1.5) + 0.04\\ln(0.25)$。\n\n4.  **总预期自由能 $\\mathcal{G}(\\pi_2)$**：\n    $\\mathcal{G}(\\pi_2) = \\mathcal{G}_E(\\pi_2) + \\mathcal{G}_I(\\pi_2)$\n    $\\mathcal{G}(\\pi_2) = (0.6\\ln(10) - 0.4\\ln(0.9)) + (0.48\\ln(1.5) + 0.04\\ln(0.25))$\n    使用数值：$\\ln(1.5) \\approx 0.4055$, $\\ln(0.25) \\approx -1.3863$。\n    $\\mathcal{G}_E(\\pi_2) \\approx 0.6(2.3026) + 0.4(0.1054) = 1.38156 + 0.04216 = 1.42372$。\n    $\\mathcal{G}_I(\\pi_2) \\approx 0.48(0.4055) + 0.04(-1.3863) = 0.19464 - 0.05545 = 0.13919$。\n    $\\mathcal{G}(\\pi_2) \\approx 1.42372 + 0.13919 = 1.56291$。\n\n### 结论\n\n比较两种策略的总预期自由能：\n$$ \\mathcal{G}(\\pi_1) \\approx 1.11273 $$\n$$ \\mathcal{G}(\\pi_2) \\approx 1.56291 $$\n由于遵循自由能原理的智能体选择最小化预期自由能的策略，我们比较这两个值。我们发现 $\\mathcal{G}(\\pi_1)  \\mathcal{G}(\\pi_2)$。\n因此，策略 $\\pi_1$ 是最优策略。最小化策略的索引是 $i=1$。",
            "answer": "$$\\boxed{1}$$"
        },
        {
            "introduction": "最后，我们将实践的范围从在给定模型内进行推断和决策，扩展到更高层次的学习问题——模型选择（model selection）。大脑不仅需要推断世界的当前状态，还必须学习和更新其关于世界如何运作的生成模型。这个练习将展示如何利用变分自由能作为贝叶斯模型证据的近似，来在多个候选模型中进行选择。您将实现一个权衡模型准确性（accuracy）与复杂性（complexity）的计算过程，这体现了奥卡姆剃刀原则：在解释力相当的情况下，更简单的模型更优。这揭示了自由能原理如何为学习和适应提供一个统一的计算框架。",
            "id": "4063890",
            "problem": "您将在线性高斯生成模型中实现和分析基于自由能原理的模型选择，重点关注准确性与复杂度之间的权衡。考虑一个单变量潜变量 $s \\in \\mathbb{R}$ 和一个观测变量 $o \\in \\mathbb{R}$。生成模型 $M$ 由一个高斯先验和一个高斯似然指定：先验 $p(s \\mid M) = \\mathcal{N}(s ; \\mu_0, v_0)$，其均值为 $\\mu_0 \\in \\mathbb{R}$，方差为 $v_0 \\in \\mathbb{R}_{0}$；似然 $p(o \\mid s, M) = \\mathcal{N}(o ; s, v_s)$，其方差为 $v_s \\in \\mathbb{R}_{0}$。近似后验 $q(s \\mid o, M)$ 被约束为高斯分布。在共轭高斯情况下，精确后验 $p(s \\mid o, M)$ 是高斯分布并且属于这个分布族。\n\n您的任务是：\n- 从 Bayes 法则 $p(s \\mid o, M) \\propto p(o \\mid s, M) \\, p(s \\mid M)$ 和高斯密度的归一化出发，以闭合形式推导出精确后验 $p(s \\mid o, M)$，包括其均值和方差，表示为 $\\mu_0$、$v_0$、$v_s$ 和 $o$ 的函数。\n- 使用期望值和 Kullback–Leibler 散度的定义，定义 $q(s \\mid o, M)$ 的两个泛函：准确性 $\\mathcal{A}(M, o)$，即期望对数似然 $\\mathbb{E}_{q}[\\log p(o \\mid s, M)]$；以及复杂度 $\\mathcal{C}(M, o)$，即散度 $\\mathrm{KL}(q(s \\mid o, M) \\, \\| \\, p(s \\mid M))$。当 $q(s \\mid o, M)$ 为高斯分布时，以闭合形式表示 $\\mathcal{A}(M, o)$ 和 $\\mathcal{C}(M, o)$。所有对数均为自然对数，所有信息论量均以奈特 (nats) 为单位。\n- 将变分自由能 $\\mathcal{F}(M, o)$ 定义为差值 $\\mathcal{C}(M, o) - \\mathcal{A}(M, o)$。实现一个程序，对于每个观测值 $o$，评估给定集合中每个模型 $M$ 的 $\\mathcal{F}(M, o)$，并选择使 $\\mathcal{F}(M, o)$ 最小化的模型索引。如果多个模型的 $\\mathcal{F}$ 值完全相等，则选择最小的索引来打破平局。\n\n仅使用以下固定的模型和观测值测试套件：\n- 模型（由 $i \\in \\{1, 2, 3, 4\\}$ 索引），每个模型带参数 $(\\mu_0, v_0, v_s)$：\n  - $M_1$: $(\\mu_0, v_0, v_s) = (0, 1, 4)$。\n  - $M_2$: $(\\mu_0, v_0, v_s) = (0, 1, 0.25)$。\n  - $M_3$: $(\\mu_0, v_0, v_s) = (0, 10, 0.25)$。\n  - $M_4$: $(\\mu_0, v_0, v_s) = (0, 0.1, 0.25)$。\n- 观测值（按顺序）：$o \\in \\{0, 2.5, -1.5, 8\\}$。\n\n您的程序必须：\n- 对于列表中的每个观测值 $o$，使用您从第一性原理为高斯情况（其中 $q(s \\mid o, M) = p(s \\mid o, M)$）推导出的闭合形式表达式，计算每个 $i \\in \\{1, 2, 3, 4\\}$ 的 $\\mathcal{F}(M_i, o)$。\n- 生成单行输出，其中包含所选模型索引的逗号分隔列表，并用方括号括起来（例如，$[1,2,3,4]$）。输出必须与上述给定顺序的观测值相对应，并且每个观测值恰好包含一个整数。\n\n不涉及物理单位。不出现角度。所有返回值都是无量纲的。通过保证方差 $v_0$ 和 $v_s$ 的正性并使用自然对数来确保数值稳定性。最终输出必须是整数。程序不得要求任何输入。评估必须以自然单位（奈特）进行。",
            "solution": "该问题有效且适定，基于标准的贝叶斯推断和信息论。我们逐步推导所需的量和随后的计算方法。\n\n### 1. 精确后验分布的推导\n\n生成模型 $M$ 由高斯先验和高斯似然定义：\n- 先验: $p(s \\mid M) = \\mathcal{N}(s ; \\mu_0, v_0)$\n- 似然: $p(o \\mid s, M) = \\mathcal{N}(o ; s, v_s)$\n\n根据 Bayes 法则，给定观测值 $o$ 时，潜变量 $s$ 的后验分布正比于似然与先验的乘积：\n$$p(s \\mid o, M) \\propto p(o \\mid s, M) p(s \\mid M)$$\n由于先验和似然都是高斯分布，它们的乘积也与一个高斯函数成正比，这意味着后验分布也是高斯分布。我们可以通过检查所得分布的指数部分来确定其参数。对数后验由下式给出：\n$$\\log p(s \\mid o, M) = \\log p(o \\mid s, M) + \\log p(s \\mid M) + \\text{const.}$$\n代入高斯分布的概率密度函数（并忽略归一化常数）：\n$$\\log p(s \\mid o, M) = -\\frac{(o-s)^2}{2v_s} - \\frac{(s-\\mu_0)^2}{2v_0} + \\text{const.}$$\n为了找到后验均值和方差，我们展开二次项并合并关于 $s^2$ 和 $s$ 的项：\n$$\\log p(s \\mid o, M) = -\\frac{1}{2v_s}(o^2 - 2os + s^2) - \\frac{1}{2v_0}(s^2 - 2s\\mu_0 + \\mu_0^2) + \\text{const.}$$\n$$= s^2 \\left(-\\frac{1}{2v_s} - \\frac{1}{2v_0}\\right) + s \\left(\\frac{o}{v_s} + \\frac{\\mu_0}{v_0}\\right) + \\text{const.}$$\n这是关于 $s$ 的一个二次型，是高斯对数密度的特征。一个一般的高斯后验 $p(s \\mid o, M) = \\mathcal{N}(s; \\mu_p, v_p)$ 具有以下形式的对数密度：\n$$\\log \\mathcal{N}(s; \\mu_p, v_p) = -\\frac{(s-\\mu_p)^2}{2v_p} + \\text{const.} = s^2\\left(-\\frac{1}{2v_p}\\right) + s\\left(\\frac{\\mu_p}{v_p}\\right) + \\text{const.}$$\n通过比较 $s^2$ 项的系数，我们找到后验精度（方差的倒数）：\n$$\\frac{1}{v_p} = \\frac{1}{v_0} + \\frac{1}{v_s} = \\frac{v_0 + v_s}{v_0 v_s}$$\n因此，后验方差 $v_p$ 是：\n$$v_p = \\frac{v_0 v_s}{v_0 + v_s}$$\n通过比较 $s$ 项的系数，我们找到后验均值 $\\mu_p$：\n$$\\frac{\\mu_p}{v_p} = \\frac{\\mu_0}{v_0} + \\frac{o}{v_s}$$\n$$\\mu_p = v_p \\left( \\frac{\\mu_0}{v_0} + \\frac{o}{v_s} \\right) = \\frac{v_0 v_s}{v_0 + v_s} \\left( \\frac{\\mu_0 v_s + o v_0}{v_0 v_s} \\right)$$\n因此，后验均值 $\\mu_p$ 是先验均值和观测值的精度加权平均：\n$$\\mu_p = \\frac{v_s \\mu_0 + v_0 o}{v_0 + v_s}$$\n问题指定使用一个近似后验 $q(s \\mid o, M)$，在这个共轭情况下它就是精确后验。设 $q(s \\mid o, M) = \\mathcal{N}(s; \\mu_q, v_q)$，其中 $\\mu_q = \\mu_p$ 且 $v_q = v_p$。\n\n### 2. 准确性和复杂度的推导\n\n**准确性 $\\mathcal{A}(M, o)$** 是在近似后验 $q$ 下的期望对数似然：\n$$\\mathcal{A}(M, o) = \\mathbb{E}_{q}[\\log p(o \\mid s, M)]$$\n对数似然是：\n$$\\log p(o \\mid s, M) = \\log\\left(\\frac{1}{\\sqrt{2\\pi v_s}}\\exp\\left(-\\frac{(o-s)^2}{2v_s}\\right)\\right) = -\\frac{1}{2}\\log(2\\pi v_s) - \\frac{(o-s)^2}{2v_s}$$\n对 $q(s \\mid o, M) = \\mathcal{N}(s; \\mu_q, v_q)$ 求期望：\n$$\\mathcal{A}(M, o) = \\mathbb{E}_{q}\\left[-\\frac{1}{2}\\log(2\\pi v_s) - \\frac{(o-s)^2}{2v_s}\\right] = -\\frac{1}{2}\\log(2\\pi v_s) - \\frac{1}{2v_s}\\mathbb{E}_{q}[(o-s)^2]$$\n均方误差的期望值为：\n$$\\mathbb{E}_{q}[(o-s)^2] = \\mathbb{E}_{q}[(s-o)^2] = \\mathbb{E}_{q}[(s-\\mu_q + \\mu_q-o)^2]$$\n$$= \\mathbb{E}_{q}[(s-\\mu_q)^2 + 2(s-\\mu_q)(\\mu_q-o) + (\\mu_q-o)^2]$$\n$$= \\mathbb{E}_{q}[(s-\\mu_q)^2] + 2(\\mu_q-o)\\mathbb{E}_{q}[s-\\mu_q] + (\\mu_q-o)^2$$\n由于 $\\mathbb{E}_{q}[s-\\mu_q] = 0$ 且 $\\mathbb{E}_{q}[(s-\\mu_q)^2] = v_q$，上式简化为：\n$$\\mathbb{E}_{q}[(o-s)^2] = v_q + (\\mu_q-o)^2$$\n将此代回，准确性的闭合形式表达式为：\n$$\\mathcal{A}(M, o) = -\\frac{1}{2}\\log(2\\pi v_s) - \\frac{v_q + (o-\\mu_q)^2}{2v_s}$$\n\n**复杂度 $\\mathcal{C}(M, o)$** 是从先验到后验的 Kullback-Leibler (KL) 散度：\n$$\\mathcal{C}(M, o) = \\mathrm{KL}(q(s \\mid o, M) \\, \\| \\, p(s \\mid M))$$\n对于两个单变量高斯分布 $q(s) = \\mathcal{N}(s; \\mu_q, v_q)$ 和 $p(s) = \\mathcal{N}(s; \\mu_0, v_0)$，KL 散度由标准公式给出：\n$$\\mathrm{KL}(q \\| p) = \\frac{1}{2} \\left( \\log\\frac{v_0}{v_q} + \\frac{v_q}{v_0} + \\frac{(\\mu_q - \\mu_0)^2}{v_0} - 1 \\right)$$\n因此，复杂度的闭合形式表达式为：\n$$\\mathcal{C}(M, o) = \\frac{1}{2} \\left( \\log\\left(\\frac{v_0}{v_q}\\right) + \\frac{v_q + (\\mu_q - \\mu_0)^2}{v_0} - 1 \\right)$$\n\n### 3. 变分自由能与计算策略\n\n变分自由能 $\\mathcal{F}(M, o)$ 定义为：\n$$\\mathcal{F}(M, o) = \\mathcal{C}(M, o) - \\mathcal{A}(M, o)$$\n这个量是负对数证据 $-\\log p(o \\mid M)$ 的一个上界。当近似后验是精确后验时，即 $q(s \\mid o, M) = p(s \\mid o, M)$，这个界变为等式：\n$$\\mathcal{F}(M, o) = -\\log p(o \\mid M)$$\n这为计算提供了一个更简单的表达式。边际似然或模型证据 $p(o \\mid M)$ 是通过对联合分布 $p(o,s \\mid M) = p(o \\mid s, M)p(s \\mid M)$ 对潜变量 $s$ 进行边际化得到的：\n$$p(o \\mid M) = \\int p(o \\mid s, M) p(s \\mid M) \\,ds$$\n这个积分代表了两个高斯分布的卷积：$\\mathcal{N}(s; \\mu_0, v_0)$ 和一个给定 $s$ 时 $o$ 的核，对应于 $\\mathcal{N}(o; s, v_s)$，这可以看作是 $o = s+\\epsilon$，其中 $\\epsilon \\sim \\mathcal{N}(0,v_s)$。$o$ 的结果分布也是高斯分布。其均值是均值之和（$\\mu_0 + 0 = \\mu_0$），其方差是方差之和（$v_0 + v_s$），因为 $s$ 和 $\\epsilon$ 是独立的。\n$$p(o \\mid M) = \\mathcal{N}(o; \\mu_0, v_0+v_s)$$\n因此，变分自由能就是这个边际似然的负对数：\n$$\\mathcal{F}(M, o) = -\\log \\left( \\frac{1}{\\sqrt{2\\pi(v_0+v_s)}} \\exp\\left(-\\frac{(o-\\mu_0)^2}{2(v_0+v_s)}\\right) \\right)$$\n$$\\mathcal{F}(M, o) = \\frac{(o-\\mu_0)^2}{2(v_0+v_s)} + \\frac{1}{2}\\log(2\\pi(v_0+v_s))$$\n这个表达式代表了一种权衡。第一项 $\\frac{(o-\\mu_0)^2}{2(v_0+v_s)}$ 是一个准确性或预测误差项，惩罚那些先验预测均值 $\\mu_0$ 远离观测值 $o$ 的模型，并受预测不确定性 $v_0+v_s$ 的调节。第二项 $\\frac{1}{2}\\log(2\\pi(v_0+v_s))$ 作为一个复杂度惩罚项，惩罚具有较大预测不确定性的模型。最小化 $\\mathcal{F}$ 实现了准确性与复杂度之间的最佳平衡，这就是贝叶斯模型选择的原理。\n\n计算过程如下：\n1.  对于集合 $\\{0, 2.5, -1.5, 8\\}$ 中的每个观测值 $o$。\n2.  对于集合 $\\{M_1, M_2, M_3, M_4\\}$ 中的每个模型 $M_i$，及其各自的参数 $(\\mu_0, v_0, v_s)$。\n3.  使用上面推导的简化表达式计算 $\\mathcal{F}(M_i, o)$。\n4.  对于给定的 $o$，选择产生 $\\mathcal{F}(M_i, o)$ 最小值的模型索引 $i$。\n5.  如果多个模型产生相同的最小值，则通过选择索引最小的模型来打破平局（例如，选择 $M_1$ 而不是 $M_2$）。\n6.  按指定顺序收集每个观测值的获胜模型索引。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements model selection under the free-energy principle for a set of\n    linear Gaussian models and observations.\n    \"\"\"\n    # Define the models as a list of tuples (mu_0, v_0, v_s).\n    # Indices are 1-based, so models[0] corresponds to M_1.\n    models = [\n        (0.0, 1.0, 4.0),     # M1\n        (0.0, 1.0, 0.25),    # M2\n        (0.0, 10.0, 0.25),   # M3\n        (0.0, 0.1, 0.25),    # M4\n    ]\n\n    # Define the list of observations.\n    observations = [0.0, 2.5, -1.5, 8.0]\n\n    def calculate_free_energy(o, mu0, v0, vs):\n        \"\"\"\n        Calculates the variational free energy F(M, o), which for an exact\n        posterior equals the negative log model evidence -log p(o|M).\n\n        The formula is F = (o-mu0)^2 / (2*(v0+vs)) + 0.5*log(2*pi*(v0+vs)).\n\n        Args:\n            o (float): The observation.\n            mu0 (float): The prior mean.\n            v0 (float): The prior variance.\n            vs (float): The likelihood variance.\n\n        Returns:\n            float: The variational free energy.\n        \"\"\"\n        predictive_variance = v0 + vs\n        \n        # Accuracy term (or prediction error)\n        accuracy_term = (o - mu0)**2 / (2 * predictive_variance)\n        \n        # Complexity term\n        complexity_term = 0.5 * np.log(2 * np.pi * predictive_variance)\n        \n        return accuracy_term + complexity_term\n\n    selected_model_indices = []\n    \n    # Iterate through each observation\n    for o in observations:\n        free_energies = []\n        \n        # Evaluate free energy for each model given the observation\n        for mu0, v0, vs in models:\n            f = calculate_free_energy(o, mu0, v0, vs)\n            free_energies.append(f)\n        \n        # Find the index of the model that minimizes the free energy.\n        # np.argmin returns the index of the first minimum, which handles the tie-breaking rule.\n        # Add 1 to convert from 0-based index to 1-based model index.\n        best_model_index = np.argmin(free_energies) + 1\n        selected_model_indices.append(best_model_index)\n\n    # Format the output as a comma-separated list in square brackets.\n    output_str = f\"[{','.join(map(str, selected_model_indices))}]\"\n    print(output_str)\n\nsolve()\n```"
        }
    ]
}