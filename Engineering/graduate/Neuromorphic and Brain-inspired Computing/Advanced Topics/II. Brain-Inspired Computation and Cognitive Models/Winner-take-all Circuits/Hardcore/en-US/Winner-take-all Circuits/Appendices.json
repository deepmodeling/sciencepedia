{
    "hands_on_practices": [
        {
            "introduction": "To truly grasp the essence of a Winner-Take-All (WTA) circuit, we must first understand its dynamics. This practice explores the foundational principle of how competition arises from lateral inhibition. By analyzing a simple two-neuron model using the tools of dynamical systems, you will determine the critical point at which the network's behavior fundamentally changes, transitioning from a state where both neurons can be active to one where only a single winner can emerge. This analysis reveals the mathematical origin of the \"winner-take-all\" functionality and provides a deep intuition for how network parameters shape its computational behavior .",
            "id": "4067988",
            "problem": "You are given a minimal continuous-time model of a two-unit Winner-Take-All (WTA) circuit, a central motif in neuromorphic and brain-inspired computing. The network comprises two nonnegative rate variables $x_1$ and $x_2$ that evolve according to a threshold-linear (rectified linear) dynamical system. The equations are\n$$\n\\frac{dx_i}{dt} = -x_i + \\phi(w x_i - g x_j + b),\n\\quad \\text{for } i \\in \\{1,2\\},\\; j \\neq i,\n$$\nwhere $\\phi(u) = \\max\\{0,u\\}$ is the threshold-linear activation, $w \\in \\mathbb{R}$ is a self-excitation weight, $g \\in \\mathbb{R}_{\\ge 0}$ is a global inhibition gain that couples the units competitively, and $b \\in \\mathbb{R}$ is a constant bias common to both units. The phase space is restricted to $x_i \\in \\mathbb{R}_{\\ge 0}$ for $i \\in \\{1,2\\}$.\n\nUse the following standard base concepts from dynamical systems:\n- A fixed point is any $(x_1^\\star,x_2^\\star)$ such that $\\frac{dx_1}{dt}=\\frac{dx_2}{dt}=0$.\n- For any differentiable regime, the local stability of a fixed point is determined by the eigenvalues of the Jacobian matrix $J$ of the right-hand side of the dynamical system evaluated at that fixed point; a fixed point is locally asymptotically stable if and only if all eigenvalues of $J$ have negative real parts.\n\nDefine the critical inhibition gain $g^\\star$ as the smallest value of $g$ for which the network transitions from having a stable multi-active equilibrium (both $x_1$ and $x_2$ strictly positive in steady state) to exhibiting only single-winner attractors (steady states in which exactly one of $x_1$ or $x_2$ is strictly positive and the other is $0$). Assume $w \\in (0,1)$ and $b  0$, so that the leaky dynamics are well-defined and the rectifying nonlinearity is operative.\n\nTask:\n- Starting from the fundamental definitions above, derive the condition for the loss of stability of the symmetric multi-active fixed point as $g$ increases, and use it to determine $g^\\star$ as a function of $w$ and $b$.\n- Implement this reasoning in a program that, for each test case listed below, computes $g^\\star$ as a floating-point number. Express each output value as a decimal rounded to $6$ decimal places.\n\nTest suite (each test case is a pair $(w,b)$):\n- Case $1$: $(w,b)=(0.5,\\,1.0)$\n- Case $2$: $(w,b)=(0.0,\\,0.7)$\n- Case $3$: $(w,b)=(0.8,\\,0.3)$\n- Case $4$: $(w,b)=(0.99,\\,1.0)$\n- Case $5$: $(w,b)=(0.2,\\,5.0)$\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,r_3,r_4,r_5]$) where $r_k$ is the computed $g^\\star$ for test case $k$. Each $r_k$ must be a decimal rounded to $6$ decimal places.",
            "solution": "The problem is assessed to be valid as it is scientifically grounded, well-posed, objective, complete, and internally consistent. The derivation of the critical inhibition gain $g^\\star$ proceeds in four steps.\n\nFirst, we locate the symmetric multi-active fixed point $(x_1^\\star, x_2^\\star)$ where $x_1^\\star = x_2^\\star = x^\\star  0$. At a fixed point, $\\frac{dx_i}{dt} = 0$, so the governing equation becomes $x_i^\\star = \\phi(w x_i^\\star - g x_j^\\star + b)$. For a non-zero fixed point $x^\\star  0$, the argument of the activation function $\\phi(u) = \\max\\{0,u\\}$ must be positive. This allows us to treat $\\phi$ as the identity function in this regime. Substituting $x_1^\\star = x_2^\\star = x^\\star$ gives:\n$$x^\\star = w x^\\star - g x^\\star + b$$\nSolving for $x^\\star$, we find:\n$$x^\\star(1 - w + g) = b \\quad \\implies \\quad x^\\star = \\frac{b}{1 - w + g}$$\nSince we assume $b  0$, $w  1$, and $g \\ge 0$, the denominator $1 - w + g$ is always positive, ensuring $x^\\star  0$. The condition for the argument of $\\phi$ to be positive, $(w-g)x^\\star + b  0$, is also always satisfied under these constraints. Thus, a symmetric multi-active fixed point exists for all valid parameters.\n\nSecond, we analyze the stability of this fixed point by linearizing the system around $(x^\\star, x^\\star)$. The Jacobian matrix $J$ of the system is computed from the right-hand sides of the ODEs, $F_i = -x_i + \\phi(w x_i - g x_j + b)$. In the linear regime around the fixed point, the derivative of $\\phi(u)$ is 1. The partial derivatives are:\n$$\n\\frac{\\partial F_i}{\\partial x_i} = -1 + w, \\quad \\frac{\\partial F_i}{\\partial x_j} = -g \\quad (\\text{for } j \\neq i)\n$$\nThis gives the Jacobian matrix:\n$$\nJ = \\begin{pmatrix} w-1  -g \\\\ -g  w-1 \\end{pmatrix}\n$$\n\nThird, we find the eigenvalues of the Jacobian matrix. The characteristic equation is $\\det(J - \\lambda I) = 0$:\n$$\n(w-1-\\lambda)^2 - (-g)^2 = 0 \\quad \\implies \\quad (w-1-\\lambda)^2 = g^2\n$$\nTaking the square root of both sides yields $w-1-\\lambda = \\pm g$. The two eigenvalues are:\n$$\n\\lambda_1 = w - 1 - g, \\quad \\lambda_2 = w - 1 + g\n$$\nFor the fixed point to be locally asymptotically stable, both eigenvalues must be negative.\n\nFourth, we determine the critical gain $g^\\star$. We analyze the sign of each eigenvalue:\n1.  $\\lambda_1 = w - 1 - g$: Since $w  1$ and $g \\ge 0$, $w-1$ is negative and $-g$ is non-positive. Thus, $\\lambda_1$ is always negative.\n2.  $\\lambda_2 = w - 1 + g$: This eigenvalue's sign depends on the value of $g$. For the fixed point to be stable, we need $\\lambda_2  0$, which implies $g  1 - w$.\n\nThe network transitions from having a stable symmetric fixed point to an unstable one when $\\lambda_2$ crosses zero. This loss of stability occurs precisely when $\\lambda_2 = 0$. The critical inhibition gain $g^\\star$ is the value of $g$ at this boundary:\n$$w - 1 + g^\\star = 0 \\quad \\implies \\quad g^\\star = 1 - w$$\nThis result gives the smallest value of $g$ for which the symmetric, multi-active equilibrium loses stability, triggering the winner-take-all behavior. Note that $g^\\star$ depends only on the self-excitation weight $w$ and not on the bias $b$, as the bias term is a constant that disappears during the differentiation to compute the Jacobian.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ...\n\ndef solve():\n    \"\"\"\n    Computes the critical inhibition gain g* for a set of test cases.\n    The derivation shows that g* = 1 - w, independent of b.\n    \"\"\"\n    # Define the test cases from the problem statement as pairs of (w, b).\n    test_cases = [\n        (0.5, 1.0),\n        (0.0, 0.7),\n        (0.8, 0.3),\n        (0.99, 1.0),\n        (0.2, 5.0),\n    ]\n\n    results = []\n    for w, b in test_cases:\n        # According to the derived formula, the critical inhibition gain g_star\n        # is given by 1 - w.\n        g_star = 1.0 - w\n        \n        # Format the result to 6 decimal places as a string.\n        # This formatting handles rounding and ensures the correct number of decimal places.\n        formatted_result = f\"{g_star:.6f}\"\n        results.append(formatted_result)\n\n    # Final print statement in the exact required format: [r_1,r_2,...]\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Moving from dynamics to function, this exercise addresses a practical design challenge: how to configure a WTA circuit to perform a specific selection task. Here, we shift our perspective from the internal competitive dynamics to the circuit's input-output relationship. By leveraging probability theory, you will derive the optimal setting for a global threshold to select a predetermined number of winners ($k$) from a larger pool of inputs. This practice demonstrates how to translate statistical properties of the input signals into concrete design parameters, a crucial skill for applications like sparse coding and feature detection .",
            "id": "4067927",
            "problem": "Consider a parallel array of $N$ input channels feeding a $k$-winner-take-all (WTA) circuit, where $k \\in \\{1,2,\\dots,N-1\\}$. The WTA module is implemented by global inhibitory thresholding: the output of channel $i$ is active if and only if its input $x_i$ exceeds a fixed threshold $\\theta$, that is, $y_i = H(x_i - \\theta)$ where $H(\\cdot)$ is the Heaviside step function. Assume the inputs $\\{x_i\\}_{i=1}^{N}$ are independent and identically distributed (i.i.d.) draws from a continuous distribution with cumulative distribution function $F_X(x)$ and probability density function $f_X(x)$, so that ties occur with probability zero. Let $S(\\theta)$ denote the random number of active outputs produced by the thresholded WTA module.\n\nStarting from the basic probabilistic definitions of indicator random variables and the binomial distribution, derive a threshold $\\theta^{\\star}$ that optimizes the probability that exactly $k$ outputs are active, i.e., that maximizes $\\mathbb{P}\\{S(\\theta) = k\\}$ over all fixed $\\theta \\in \\mathbb{R}$. Express $\\theta^{\\star}$ in closed form in terms of $F_X$, $N$, and $k$. Provide the exact analytic expression; no rounding is required, and no physical units need to be reported.",
            "solution": "The problem asks for the optimal threshold $\\theta^{\\star}$ that maximizes the probability of obtaining exactly $k$ active outputs from a winner-take-all (WTA) circuit with $N$ input channels. The inputs $\\{x_i\\}_{i=1}^{N}$ are independent and identically distributed (i.i.d.) random variables with a continuous cumulative distribution function (CDF) $F_X(x)$ and probability density function (PDF) $f_X(x)$.\n\nFirst, we formalize the behavior of a single channel. The output of channel $i$, denoted by $y_i$, is active if its input $x_i$ exceeds the threshold $\\theta$. This is described by the Heaviside step function $y_i = H(x_i - \\theta)$, where $y_i=1$ if $x_i  \\theta$ and $y_i=0$ if $x_i \\le \\theta$. We can treat $y_i$ as an indicator random variable for the event that channel $i$ is active.\n\nThe probability that a single, arbitrary channel $i$ is active is given by:\n$$p(\\theta) = \\mathbb{P}\\{y_i = 1\\} = \\mathbb{P}\\{x_i  \\theta\\}$$\nUsing the definition of the cumulative distribution function, $F_X(\\theta) = \\mathbb{P}\\{x_i \\le \\theta\\}$, we can express $p(\\theta)$ as:\n$$p(\\theta) = 1 - \\mathbb{P}\\{x_i \\le \\theta\\} = 1 - F_X(\\theta)$$\nSince the inputs $\\{x_i\\}_{i=1}^{N}$ are i.i.d., the events of each channel being active are independent Bernoulli trials, each with a success probability of $p(\\theta)$.\n\nThe total number of active outputs, $S(\\theta)$, is the sum of these indicator random variables:\n$$S(\\theta) = \\sum_{i=1}^{N} y_i$$\nAs $S(\\theta)$ is the sum of $N$ i.i.d. Bernoulli trials, it follows a binomial distribution with parameters $N$ and $p(\\theta)$. We denote this as $S(\\theta) \\sim \\text{Binomial}(N, p(\\theta))$.\n\nThe probability of having exactly $k$ active outputs is given by the probability mass function (PMF) of the binomial distribution:\n$$L(\\theta) \\equiv \\mathbb{P}\\{S(\\theta) = k\\} = \\binom{N}{k} [p(\\theta)]^k [1 - p(\\theta)]^{N-k}$$\nOur objective is to find the value of $\\theta$, denoted $\\theta^{\\star}$, that maximizes this probability $L(\\theta)$. The term $\\binom{N}{k}$ is a positive constant with respect to $\\theta$, so we only need to maximize the part that depends on $\\theta$.\n\nMaximizing $L(\\theta)$ is equivalent to maximizing its natural logarithm, $\\ln(L(\\theta))$, since the logarithm is a monotonically increasing function. This simplifies the differentiation process.\n$$\\ln(L(\\theta)) = \\ln\\binom{N}{k} + k \\ln(p(\\theta)) + (N-k) \\ln(1 - p(\\theta))$$\nTo find the maximum, we take the derivative of $\\ln(L(\\theta))$ with respect to $\\theta$ and set it to zero. We apply the chain rule, noting that $p(\\theta)$ is a function of $\\theta$.\n$$\\frac{d}{d\\theta} \\ln(L(\\theta)) = \\frac{d}{dp} \\left[ \\ln\\binom{N}{k} + k \\ln(p) + (N-k) \\ln(1 - p) \\right] \\frac{dp}{d\\theta}$$\nThe derivative of $p(\\theta) = 1 - F_X(\\theta)$ with respect to $\\theta$ is:\n$$\\frac{dp}{d\\theta} = -\\frac{dF_X(\\theta)}{d\\theta} = -f_X(\\theta)$$\nThe derivative of the log-probability term with respect to $p$ is:\n$$\\frac{d}{dp}(\\dots) = \\frac{k}{p} - \\frac{N-k}{1-p}$$\nCombining these, we get:\n$$\\frac{d}{d\\theta} \\ln(L(\\theta)) = \\left( \\frac{k}{p(\\theta)} - \\frac{N-k}{1-p(\\theta)} \\right) (-f_X(\\theta))$$\nFor the derivative to be zero, one of the factors must be zero. We assume that the PDF $f_X(\\theta)$ is non-zero in the region of interest; otherwise, small changes in $\\theta$ would not affect the probability, which is inconsistent with finding a unique maximum for a continuous distribution. Thus, we set the first factor to zero:\n$$\\frac{k}{p(\\theta)} - \\frac{N-k}{1-p(\\theta)} = 0$$\n$$\\frac{k}{p(\\theta)} = \\frac{N-k}{1-p(\\theta)}$$\nSolving for $p(\\theta)$, we get:\n$$k(1 - p(\\theta)) = (N-k)p(\\theta)$$\n$$k - k p(\\theta) = N p(\\theta) - k p(\\theta)$$\n$$k = N p(\\theta)$$\nThis gives the optimal probability of success for a single trial:\n$$p(\\theta^{\\star}) = \\frac{k}{N}$$\nThis result is intuitive: the probability of exactly $k$ successes in a binomial experiment is maximized when the single-trial success probability equals the desired proportion of successes, $k/N$.\n\nNow, we substitute the definition of $p(\\theta)$ back into this equation to solve for the optimal threshold $\\theta^{\\star}$:\n$$1 - F_X(\\theta^{\\star}) = \\frac{k}{N}$$\n$$F_X(\\theta^{\\star}) = 1 - \\frac{k}{N} = \\frac{N-k}{N}$$\nTo isolate $\\theta^{\\star}$, we apply the inverse of the cumulative distribution function, which is the quantile function, denoted $F_X^{-1}$. Since $k \\in \\{1, 2, \\dots, N-1\\}$, the argument $\\frac{N-k}{N}$ is in the interval $(0, 1)$, for which the inverse CDF of a continuous distribution is well-defined.\n$$\\theta^{\\star} = F_X^{-1}\\left(\\frac{N-k}{N}\\right)$$\nThis expression provides the optimal threshold $\\theta^{\\star}$ in closed form in terms of the given quantities $F_X$, $N$, and $k$.",
            "answer": "$$\n\\boxed{F_{X}^{-1}\\left(\\frac{N-k}{N}\\right)}\n$$"
        },
        {
            "introduction": "The final step in our practical exploration is to bridge the gap between idealized models and physical reality. Real-world neuromorphic hardware is subject to manufacturing variations, or device mismatch, which introduces noise and can disrupt computation. This exercise tackles this challenge by modeling these imperfections as random offsets in neuron thresholds. Your task is to calculate the minimum inhibitory strength required to guarantee that the correct winner is chosen with high probability, despite this inherent noise. This problem provides hands-on experience in designing robust circuits that can function reliably in the face of physical non-idealities, a cornerstone of practical neuromorphic engineering .",
            "id": "4067903",
            "problem": "Consider a pool of $N$ identical linear-threshold neurons implementing a winner-take-all (WTA) circuit, modeled at steady state by a single active neuron (the winner) inhibiting all others equally. Let the deterministic input currents be ordered as $I_{1}  I_{2} \\geq I_{3} \\geq \\cdots \\geq I_{N}  0$. Each neuron has an effective threshold uncertainty due to device mismatch modeled as an independent, identically distributed random offset $\\delta V_{T,i} \\sim \\mathcal{N}(0,\\sigma_{V_{T}}^{2})$, which linearly perturbs the effective input current by $-\\alpha \\delta V_{T,i}$, where $\\alpha  0$ converts threshold offset in volts to an equivalent current shift in amperes.\n\nAssume the following simplified steady-state interaction: if neuron $w$ with input $I_{1}$ is the winner, then its output equals its effective input, so its steady-state activity is $x_{w} = I_{1} - \\alpha \\delta V_{T,1}$, and each loser $k \\neq w$ receives a net input $I_{k} - \\alpha \\delta V_{T,k} - g\\,x_{w}$, where $g  0$ is the uniform inhibitory gain. Correct winner selection means that all losers are suppressed, i.e., $I_{k} - \\alpha \\delta V_{T,k} - g(I_{1} - \\alpha \\delta V_{T,1})  0$ for all $k \\geq 2$.\n\nTo guarantee correct winner selection with confidence $1-\\epsilon$ (that is, with probability at least $1-\\epsilon$ over the random threshold offsets), it suffices to enforce suppression of the strongest competitor $k=2$. Using only:\n- the independence and Gaussianity of $\\delta V_{T,1}$ and $\\delta V_{T,2}$,\n- the definition that if $X \\sim \\mathcal{N}(\\mu,\\sigma^{2})$ then $\\mathbb{P}[X  0] = \\Phi(\\mu/\\sigma)$ where $\\Phi(\\cdot)$ is the standard normal cumulative distribution function,\n\nderive a closed-form analytic expression for the minimal inhibitory gain $g_{\\min}$ as a function of $I_{1}$, $I_{2}$, $\\alpha$, $\\sigma_{V_{T}}$, and $\\epsilon$ that achieves $\\mathbb{P}[\\text{correct winner}] \\geq 1-\\epsilon$. Express your final answer in terms of the standard normal quantile $z_{1-\\epsilon}$ defined by $z_{1-\\epsilon} = \\Phi^{-1}(1-\\epsilon)$.\n\nYour final answer must be a single closed-form analytic expression for $g_{\\min}$ in terms of the given symbols. Do not introduce any numerical approximations or rounding. No units are required in the final answer. Ensure all symbols are clearly defined as above, and assume parameter values such that the expression is real and finite.",
            "solution": "The problem is assessed to be valid. It is scientifically grounded in the theory of neuromorphic circuits, well-posed with a clear objective and sufficient information, and free of contradictions or ambiguities.\n\nThe goal is to find the minimal inhibitory gain $g_{\\min}$ that ensures correct winner selection with a probability of at least $1-\\epsilon$. The problem states that neuron $1$ is the intended winner, as it receives the largest input current $I_{1}$. Correct winner selection requires all other neurons $k \\geq 2$ to be suppressed. The problem simplifies this by stating it is sufficient to enforce suppression for the strongest competitor, which is neuron $k=2$ with input current $I_{2}$.\n\nThe condition for the suppression of neuron $k=2$ is given as:\n$$I_{2} - \\alpha \\delta V_{T,2} - g(I_{1} - \\alpha \\delta V_{T,1})  0$$\nwhere $g$ is the inhibitory gain, $\\alpha  0$ is a conversion factor, and $\\delta V_{T,1}$ and $\\delta V_{T,2}$ are independent random variables representing threshold voltage offsets, drawn from a normal distribution $\\mathcal{N}(0, \\sigma_{V_{T}}^2)$.\n\nTo analyze this probabilistically, we define a random variable $Y$ representing the margin of suppression for neuron $2$. The condition for suppression is equivalent to $Y  0$:\n$$Y = g(I_{1} - \\alpha \\delta V_{T,1}) - (I_{2} - \\alpha \\delta V_{T,2})$$\nWe can rearrange $Y$ to separate the deterministic and random components:\n$$Y = (gI_{1} - I_{2}) + (\\alpha \\delta V_{T,2} - g\\alpha \\delta V_{T,1})$$\nThe variable $Y$ is a linear transformation of Gaussian random variables, and is therefore itself a Gaussian random variable. We find its mean $\\mu_Y$ and variance $\\sigma_Y^2$.\n\nThe mean of $Y$ is:\n$$\\mu_Y = \\mathbb{E}[Y] = \\mathbb{E}[(gI_{1} - I_{2}) + (\\alpha \\delta V_{T,2} - g\\alpha \\delta V_{T,1})]$$\nSince $\\mathbb{E}[\\delta V_{T,1}] = \\mathbb{E}[\\delta V_{T,2}] = 0$, the mean is:\n$$\\mu_Y = gI_{1} - I_{2}$$\nThe variance of $Y$ is:\n$$\\sigma_Y^2 = \\text{Var}[Y] = \\text{Var}[(gI_{1} - I_{2}) + (\\alpha \\delta V_{T,2} - g\\alpha \\delta V_{T,1})]$$\nThe deterministic part $(gI_{1} - I_{2})$ does not contribute to the variance. Due to the independence of $\\delta V_{T,1}$ and $\\delta V_{T,2}$, the variance of the sum is the sum of the variances:\n$$\\sigma_Y^2 = \\text{Var}[\\alpha \\delta V_{T,2}] + \\text{Var}[-g\\alpha \\delta V_{T,1}]$$\nUsing the property $\\text{Var}[aX] = a^2\\text{Var}[X]$:\n$$\\sigma_Y^2 = \\alpha^2\\text{Var}[\\delta V_{T,2}] + (-g\\alpha)^2\\text{Var}[\\delta V_{T,1}]$$\nGiven that $\\text{Var}[\\delta V_{T,1}] = \\text{Var}[\\delta V_{T,2}] = \\sigma_{V_{T}}^2$, we have:\n$$\\sigma_Y^2 = \\alpha^2\\sigma_{V_{T}}^2 + g^2\\alpha^2\\sigma_{V_{T}}^2 = \\alpha^2\\sigma_{V_{T}}^2(1+g^2)$$\nSo, the random variable $Y$ follows the distribution $Y \\sim \\mathcal{N}\\left(gI_{1} - I_{2}, \\alpha^2\\sigma_{V_{T}}^2(1+g^2)\\right)$.\n\nThe problem requires the probability of correct suppression to be at least $1-\\epsilon$:\n$$\\mathbb{P}[Y  0] \\geq 1-\\epsilon$$\nUsing the provided formula for a Gaussian variable $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$, $\\mathbb{P}[X  0] = \\Phi(\\mu/\\sigma)$, where $\\Phi(\\cdot)$ is the standard normal cumulative distribution function (CDF), we can write:\n$$\\mathbb{P}[Y  0] = \\Phi\\left(\\frac{\\mu_Y}{\\sigma_Y}\\right) = \\Phi\\left(\\frac{gI_{1} - I_{2}}{\\sqrt{\\alpha^2\\sigma_{V_{T}}^2(1+g^2)}}\\right) = \\Phi\\left(\\frac{gI_{1} - I_{2}}{\\alpha\\sigma_{V_{T}}\\sqrt{1+g^2}}\\right)$$\nThe condition becomes:\n$$\\Phi\\left(\\frac{gI_{1} - I_{2}}{\\alpha\\sigma_{V_{T}}\\sqrt{1+g^2}}\\right) \\geq 1-\\epsilon$$\nSince $\\Phi(\\cdot)$ is a strictly increasing function, we can apply its inverse, the quantile function $\\Phi^{-1}(\\cdot)$, to both sides of the inequality:\n$$\\frac{gI_{1} - I_{2}}{\\alpha\\sigma_{V_{T}}\\sqrt{1+g^2}} \\geq \\Phi^{-1}(1-\\epsilon)$$\nThe problem defines the standard normal quantile $z_{1-\\epsilon} = \\Phi^{-1}(1-\\epsilon)$. Thus:\n$$\\frac{gI_{1} - I_{2}}{\\alpha\\sigma_{V_{T}}\\sqrt{1+g^2}} \\geq z_{1-\\epsilon}$$\nTo find the minimal gain $g_{\\min}$, we solve for $g$ in the equality corresponding to the boundary of this condition. Let $C = \\alpha\\sigma_{V_{T}}z_{1-\\epsilon}$. The inequality is $gI_{1} - I_{2} \\geq C\\sqrt{1+g^2}$. For a non-trivial solution with high confidence (e.g., $\\epsilon  0.5$), we have $z_{1-\\epsilon}  0$ and thus $C  0$. The right side is positive, which implies the left side must also be positive, $gI_{1} - I_{2}  0$. Under this condition, we can square both sides:\n$$(gI_{1} - I_{2})^2 \\geq C^2(1+g^2)$$\n$$g^2I_{1}^2 - 2gI_{1}I_{2} + I_{2}^2 \\geq C^2 + C^2g^2$$\nRearranging the terms to form a quadratic inequality in $g$:\n$$(I_{1}^2 - C^2)g^2 - (2I_{1}I_{2})g + (I_{2}^2 - C^2) \\geq 0$$\nFor a minimal gain to exist, this quadratic in $g$ (which opens upwards, assuming $I_1^2  C^2$) must be greater than or equal to $0$. The minimal gain $g_{\\min}$ will be the larger of the two roots of the corresponding equation $(I_{1}^2 - C^2)g^2 - (2I_{1}I_{2})g + (I_{2}^2 - C^2) = 0$.\nUsing the quadratic formula $g = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$:\nThe discriminant is $\\Delta = (-2I_{1}I_{2})^2 - 4(I_{1}^2 - C^2)(I_{2}^2 - C^2)$:\n$$\\Delta = 4I_{1}^2I_{2}^2 - 4(I_{1}^2I_{2}^2 - I_{1}^2C^2 - I_{2}^2C^2 + C^4)$$\n$$\\Delta = 4I_{1}^2C^2 + 4I_{2}^2C^2 - 4C^4 = 4C^2(I_{1}^2 + I_{2}^2 - C^2)$$\nThe roots are:\n$$g = \\frac{2I_{1}I_{2} \\pm \\sqrt{4C^2(I_{1}^2 + I_{2}^2 - C^2)}}{2(I_{1}^2 - C^2)} = \\frac{I_{1}I_{2} \\pm C\\sqrt{I_{1}^2 + I_{2}^2 - C^2}}{I_{1}^2 - C^2}$$\nThe minimal gain $g_{\\min}$ corresponds to the larger root (the one with the plus sign):\n$$g_{\\min} = \\frac{I_{1}I_{2} + C\\sqrt{I_{1}^2 + I_{2}^2 - C^2}}{I_{1}^2 - C^2}$$\nSubstituting back $C = \\alpha\\sigma_{V_{T}}z_{1-\\epsilon}$, we obtain the final expression for the minimal inhibitory gain:\n$$g_{\\min} = \\frac{I_{1}I_{2} + (\\alpha\\sigma_{V_{T}}z_{1-\\epsilon})\\sqrt{I_{1}^2 + I_{2}^2 - (\\alpha\\sigma_{V_{T}}z_{1-\\epsilon})^2}}{I_{1}^2 - (\\alpha\\sigma_{V_{T}}z_{1-\\epsilon})^2}$$\nThis expression is valid under the assumptions that the denominator and the term inside the square root are positive, which corresponds to the signal-to-noise ratio being sufficiently high for the desired confidence level. The problem statement guarantees this by asking to assume parameters such that the expression is real and finite.",
            "answer": "$$\\boxed{\\frac{I_{1}I_{2} + (\\alpha\\sigma_{V_{T}}z_{1-\\epsilon})\\sqrt{I_{1}^{2} + I_{2}^{2} - (\\alpha\\sigma_{V_{T}}z_{1-\\epsilon})^{2}}}{I_{1}^{2} - (\\alpha\\sigma_{V_{T}}z_{1-\\epsilon})^{2}}}$$"
        }
    ]
}