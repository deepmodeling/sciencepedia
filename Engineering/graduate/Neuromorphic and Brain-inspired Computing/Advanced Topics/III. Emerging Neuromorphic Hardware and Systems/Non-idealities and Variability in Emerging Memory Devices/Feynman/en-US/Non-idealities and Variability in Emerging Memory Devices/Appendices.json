{
    "hands_on_practices": [
        {
            "introduction": "A fundamental challenge in analog memory technologies like Phase-Change Memory (PCM) is conductance drift, where the stored state changes over time. This phenomenon arises from the physical process of structural relaxation in the amorphous material, and its rate is highly sensitive to temperature. This exercise provides practice in connecting a physical model—an Arrhenius temperature dependence—to an observable device non-ideality, allowing you to derive the expected change in drift rate across an operating temperature range and build an intuition for its impact on device reliability .",
            "id": "4053917",
            "problem": "A nanoscale phase-change memory (PCM) cell exhibits conductance drift due to structural relaxation of the amorphous phase. Over intermediate timescales that are long compared to local vibrational relaxation but short compared to crystallization, the conductance is empirically observed to follow a scale-invariant power law with temperature-dependent drift exponent, modeled as\n$$\nG(t;T) = G_0 \\left( \\frac{t}{t_0} \\right)^{-\\nu(T)},\n$$\nwhere $G_0$ and $t_0$ are constants set by the programming condition and readout protocol, $t$ is the elapsed time after programming, and $\\nu(T)$ is a positive drift exponent. Assume that the temperature dependence of the drift exponent follows an Arrhenius law for an activation-limited attempt rate,\n$$\n\\nu(T) = \\nu_0 \\exp\\!\\left(-\\frac{E_\\nu}{k_B T}\\right),\n$$\nwhere $\\nu_0$ is a constant prefactor, $E_\\nu$ is an effective activation energy, and $k_B$ is the Boltzmann constant. Define the dimensionless drift rate per logarithmic time decade as\n$$\nR(T) := -\\frac{d \\ln G(t;T)}{d \\ln t}.\n$$\nStarting from the above definitions and without assuming any additional shortcut formulas, derive an analytic expression for the ratio $R(T_2)/R(T_1)$ in terms of $E_\\nu$, $k_B$, $T_1$, and $T_2$. Then evaluate this ratio numerically for $T_1 = 300\\,\\mathrm{K}$, $T_2 = 350\\,\\mathrm{K}$, $E_\\nu = 0.25\\,\\mathrm{eV}$, and $k_B = 8.617333262 \\times 10^{-5}\\,\\mathrm{eV/K}$. Express your final answer as a dimensionless decimal rounded to four significant figures.",
            "solution": "The conductance relaxation model encodes scale invariance in time, a widely observed feature of disordered relaxations in amorphous materials that underpins drift in phase-change memory. We begin from the given power-law form\n$$\nG(t;T) = G_0 \\left( \\frac{t}{t_0} \\right)^{-\\nu(T)}.\n$$\nTaking the natural logarithm of both sides gives\n$$\n\\ln G(t;T) = \\ln G_0 - \\nu(T)\\,\\ln\\!\\left(\\frac{t}{t_0}\\right).\n$$\nBy definition, the drift rate per logarithmic time decade is the negative logarithmic derivative with respect to time,\n$$\nR(T) := -\\frac{d \\ln G(t;T)}{d \\ln t}.\n$$\nUsing the chain rule and the logarithm of the conductance, we compute\n$$\n\\frac{d \\ln G(t;T)}{d \\ln t} = \\frac{d \\ln G(t;T)}{d t} \\cdot \\frac{d t}{d \\ln t}.\n$$\nHowever, it is more direct to note that $\\ln G(t;T)$ depends on $\\ln t$ through the term $-\\nu(T)\\,\\ln(t/t_0)$, and $T$ is fixed when evaluating $R(T)$. Therefore,\n$$\n\\frac{d \\ln G(t;T)}{d \\ln t} = -\\nu(T)\\,\\frac{d \\ln(t/t_0)}{d \\ln t} = -\\nu(T),\n$$\nsince $d \\ln(t/t_0)/d \\ln t = 1$. Substituting into the definition of $R(T)$ yields\n$$\nR(T) = \\nu(T).\n$$\nThus, the drift rate per logarithmic decade equals the drift exponent. With the Arrhenius temperature dependence\n$$\n\\nu(T) = \\nu_0 \\exp\\!\\left(-\\frac{E_\\nu}{k_B T}\\right),\n$$\nthe ratio of drift rates at two temperatures $T_1$ and $T_2$ is\n$$\n\\frac{R(T_2)}{R(T_1)} = \\frac{\\nu(T_2)}{\\nu(T_1)} = \\frac{\\nu_0 \\exp\\!\\left(-\\frac{E_\\nu}{k_B T_2}\\right)}{\\nu_0 \\exp\\!\\left(-\\frac{E_\\nu}{k_B T_1}\\right)}.\n$$\nThe prefactor $\\nu_0$ cancels, giving\n$$\n\\frac{R(T_2)}{R(T_1)} = \\exp\\!\\left[-\\frac{E_\\nu}{k_B}\\left(\\frac{1}{T_2} - \\frac{1}{T_1}\\right)\\right].\n$$\nThis is the required analytic expression.\n\nNext, we evaluate numerically for $T_1 = 300\\,\\mathrm{K}$, $T_2 = 350\\,\\mathrm{K}$, $E_\\nu = 0.25\\,\\mathrm{eV}$, and $k_B = 8.617333262 \\times 10^{-5}\\,\\mathrm{eV/K}$. First compute the reciprocal temperature difference,\n$$\n\\frac{1}{T_2} - \\frac{1}{T_1} = \\frac{1}{350\\,\\mathrm{K}} - \\frac{1}{300\\,\\mathrm{K}} = \\frac{300 - 350}{300 \\times 350}\\,\\mathrm{K}^{-1} = -\\frac{50}{105000}\\,\\mathrm{K}^{-1} = -4.7619047619 \\times 10^{-4}\\,\\mathrm{K}^{-1}.\n$$\nCompute the factor $E_\\nu/k_B$,\n$$\n\\frac{E_\\nu}{k_B} = \\frac{0.25\\,\\mathrm{eV}}{8.617333262 \\times 10^{-5}\\,\\mathrm{eV/K}} \\approx 2.90112953 \\times 10^{3}\\,\\mathrm{K}.\n$$\nTherefore, the exponent becomes\n$$\n-\\frac{E_\\nu}{k_B}\\left(\\frac{1}{T_2} - \\frac{1}{T_1}\\right) = -\\left(2.90112953 \\times 10^{3}\\,\\mathrm{K}\\right)\\left(-4.7619047619 \\times 10^{-4}\\,\\mathrm{K}^{-1}\\right) \\approx 1.38149025.\n$$\nFinally, the ratio is\n$$\n\\frac{R(T_2)}{R(T_1)} = \\exp(1.38149025) \\approx 3.981.\n$$\nThis ratio is dimensionless. Rounding to four significant figures, the result is $3.981$.",
            "answer": "$$\\boxed{3.981}$$"
        },
        {
            "introduction": "For neuromorphic systems to scale, we must distinguish between variability that is fixed for each device (device-to-device) and variability that occurs with each new operation (cycle-to-cycle). A hierarchical statistical model provides a powerful framework for disentangling these components from experimental data. This practice guides you through the process of not only estimating these variance components, $\\sigma_D^2$ and $\\sigma_C^2$, but also designing an optimal experiment to do so, teaching you how to maximize statistical power under a fixed measurement budget .",
            "id": "4053973",
            "problem": "Emerging memory devices such as Phase-Change Memory (PCM) and Resistive Random Access Memory (RRAM) are used as analog synapses in neuromorphic and brain-inspired computing systems. Non-idealities arise from variability at multiple scales: device-to-device variability and cycle-to-cycle variability. Consider measuring the programmed conductance in siemens (S) after a standardized write operation. A balanced experiment chooses a number of devices and a number of programming cycles per device, then records one conductance read per cycle. Suppose the measurement noise standard deviation is known and controlled by instrumentation.\n\nAssume a hierarchical Gaussian model for the recorded conductance $y_{d,c}$ for device $d$ in cycle $c$:\n$$\ny_{d,c} = \\mu + u_d + v_{d,c} + \\epsilon_{d,c},\n$$\nwhere $u_d \\sim \\mathcal{N}(0,\\sigma_D^2)$ captures device-to-device variability, $v_{d,c} \\sim \\mathcal{N}(0,\\sigma_C^2)$ captures cycle-to-cycle variability within a device, and $\\epsilon_{d,c} \\sim \\mathcal{N}(0,\\sigma_M^2)$ models measurement noise with known variance $\\sigma_M^2$. All random variables are mutually independent. The experiment is balanced with $D$ devices and $C$ cycles per device, yielding $N = D \\cdot C$ total measurements.\n\nStarting from the fundamental properties of the normal distribution and the definitions of unbiased sample mean and variance, do the following:\n1. Define the unbiased within-device sample variance for each device and the unbiased sample variance of the device means across devices. Based on their expectations under the hierarchical Gaussian model, derive unbiased method-of-moments estimators for $\\sigma_C^2$ and $\\sigma_D^2$.\n2. Using the well-tested facts that, for independent and identically distributed Gaussian samples, the unbiased sample variance follows a scaled chi-square distribution and that the sample mean and sample variance are independent, derive closed-form expressions for the variances of the unbiased method-of-moments estimators of $\\sigma_C^2$ and $\\sigma_D^2$ as functions of $D$, $C$, $\\sigma_D^2$, $\\sigma_C^2$, and $\\sigma_M^2$.\n3. For a given total measurement budget $N$ under the balanced design constraint $D \\cdot C = N$ with $D \\ge 2$ and $C \\ge 2$, choose integer $D$ and $C$ that minimize the sum of squared relative standard errors of the variance estimators, defined as\n$$\nJ(D,C) = \\left(\\frac{\\mathrm{SE}[\\widehat{\\sigma_D^2}]}{\\sigma_D^2}\\right)^2 + \\left(\\frac{\\mathrm{SE}[\\widehat{\\sigma_C^2}]}{\\sigma_C^2}\\right)^2,\n$$\nwhere $\\mathrm{SE}[\\cdot]$ denotes the standard error (the square root of the variance). In the event of ties in $J(D,C)$ up to numerical precision, prefer the design with larger $D$.\n4. Specify the final output format and implement the computation.\n\nAll conductance-related quantities must be expressed in siemens (S), and all variances and standard errors must be expressed in square siemens ($\\mathrm{S}^2$). Angles are not part of this problem. The final output of your program must be a single line containing a comma-separated list of lists, with each inner list formatted as $[D,C,\\mathrm{SE}[\\widehat{\\sigma_D^2}],\\mathrm{SE}[\\widehat{\\sigma_C^2}]]$ and no spaces.\n\nTest suite and answer specification:\n- Use the following parameter sets, each given as $(N,\\sigma_D,\\sigma_C,\\sigma_M)$ with all $\\sigma$ values in siemens (S).\n    1. $(48, $3.0 \\times 10^{-7}$, $2.0 \\times 10^{-7}$, $5.0 \\times 10^{-8})$.\n    2. $(36, $1.0 \\times 10^{-7}$, $5.0 \\times 10^{-8}$, $2.0 \\times 10^{-7})$.\n    3. $(8, $5.0 \\times 10^{-7}$, $1.0 \\times 10^{-8}$, $1.0 \\times 10^{-8})$.\n- For each parameter set, search all integer factorizations of $N$ into $D \\cdot C$ with $D \\ge 2$ and $C \\ge 2$ to find the optimal $(D,C)$ according to the criterion above. Compute the standard errors $\\mathrm{SE}[\\widehat{\\sigma_D^2}]$ and $\\mathrm{SE}[\\widehat{\\sigma_C^2}]$ for the chosen $(D,C)$ and report them as floating-point numbers in $\\mathrm{S}^2$.\n- Your program should produce a single line of output containing the results as a comma-separated list of inner lists without spaces, for example: $[[D_1,C_1,SE_{D,1},SE_{C,1}],[D_2,C_2,SE_{D,2},SE_{C,2}],[D_3,C_3,SE_{D,3},SE_{C,3}]]$.",
            "solution": "The problem requires the derivation of estimators for variance components in a hierarchical Gaussian model, the calculation of the variances of these estimators, and the application of these results to an optimal experimental design problem. The solution is presented in a step-by-step derivation from fundamental principles.\n\nThe hierarchical model for the measured conductance $y_{d,c}$ of device $d$ in cycle $c$ is given by:\n$$\ny_{d,c} = \\mu + u_d + v_{d,c} + \\epsilon_{d,c}\n$$\nwhere $\\mu$ is the global mean conductance, $u_d \\sim \\mathcal{N}(0, \\sigma_D^2)$ represents the device-to-device variability, $v_{d,c} \\sim \\mathcal{N}(0, \\sigma_C^2)$ represents the cycle-to-cycle variability, and $\\epsilon_{d,c} \\sim \\mathcal{N}(0, \\sigma_M^2)$ is the measurement noise. All random variables are mutually independent. The experimental design is balanced with $D$ devices and $C$ cycles per device. The measurement noise variance $\\sigma_M^2$ is known.\n\nFirst, we define two key sample statistics. The sample mean for each device $d$ is:\n$$\n\\bar{y}_{d \\cdot} = \\frac{1}{C} \\sum_{c=1}^{C} y_{d,c}\n$$\nAnd the overall sample mean is:\n$$\n\\bar{y}_{\\cdot \\cdot} = \\frac{1}{D} \\sum_{d=1}^{D} \\bar{y}_{d \\cdot} = \\frac{1}{DC} \\sum_{d=1}^{D} \\sum_{c=1}^{C} y_{d,c}\n$$\n\nSubstituting the model into the device mean expression, we get:\n$$\n\\bar{y}_{d \\cdot} = \\frac{1}{C} \\sum_{c=1}^{C} (\\mu + u_d + v_{d,c} + \\epsilon_{d,c}) = \\mu + u_d + \\bar{v}_{d \\cdot} + \\bar{\\epsilon}_{d \\cdot}\n$$\nwhere $\\bar{v}_{d \\cdot} = \\frac{1}{C}\\sum_c v_{d,c}$ and $\\bar{\\epsilon}_{d \\cdot} = \\frac{1}{C}\\sum_c \\epsilon_{d,c}$. Due to the properties of the normal distribution, $\\bar{v}_{d \\cdot} \\sim \\mathcal{N}(0, \\sigma_C^2/C)$ and $\\bar{\\epsilon}_{d \\cdot} \\sim \\mathcal{N}(0, \\sigma_M^2/C)$. The terms $\\bar{y}_{d \\cdot}$ for $d=1,\\dots,D$ are independent and identically distributed (i.i.d.) normal random variables with mean $\\mathrm{E}[\\bar{y}_{d \\cdot}] = \\mu$ and variance:\n$$\n\\mathrm{Var}[\\bar{y}_{d \\cdot}] = \\mathrm{Var}[u_d] + \\mathrm{Var}[\\bar{v}_{d \\cdot}] + \\mathrm{Var}[\\bar{\\epsilon}_{d \\cdot}] = \\sigma_D^2 + \\frac{\\sigma_C^2}{C} + \\frac{\\sigma_M^2}{C} = \\sigma_D^2 + \\frac{\\sigma_C^2 + \\sigma_M^2}{C}\n$$\n\nDerivation of the Method-of-Moments Estimators\n\nWe proceed to derive estimators for the variance components $\\sigma_C^2$ and $\\sigma_D^2$.\n\nThe unbiased within-device sample variance for each device $d$ is defined as:\n$$\nS_{W,d}^2 = \\frac{1}{C-1} \\sum_{c=1}^{C} (y_{d,c} - \\bar{y}_{d \\cdot})^2\n$$\nFor a fixed device $d$, the random variables $y_{d,c}$ for $c=1,\\dots,C$ can be seen as samples from a normal distribution $\\mathcal{N}(\\mu+u_d, \\sigma_C^2 + \\sigma_M^2)$. This is because, for a given $u_d$, the term $y_{d,c} - (\\mu+u_d) = v_{d,c} + \\epsilon_{d,c}$ is a sum of two independent, zero-mean normal variables, thus $v_{d,c} + \\epsilon_{d,c} \\sim \\mathcal{N}(0, \\sigma_C^2 + \\sigma_M^2)$. The expectation of an unbiased sample variance is the true variance of the population. Therefore:\n$$\n\\mathrm{E}[S_{W,d}^2] = \\sigma_C^2 + \\sigma_M^2\n$$\nWe can average these variances over all devices to obtain a single, more stable statistic, the mean squared error within devices:\n$$\nS_W^2 = \\frac{1}{D} \\sum_{d=1}^{D} S_{W,d}^2 = \\frac{1}{D(C-1)} \\sum_{d=1}^{D} \\sum_{c=1}^{C} (y_{d,c} - \\bar{y}_{d \\cdot})^2\n$$\nBy linearity of expectation, $\\mathrm{E}[S_W^2] = \\sigma_C^2 + \\sigma_M^2$. Since $\\sigma_M^2$ is known, we can construct an unbiased estimator for $\\sigma_C^2$:\n$$\n\\widehat{\\sigma_C^2} = S_W^2 - \\sigma_M^2\n$$\nIts expectation is $\\mathrm{E}[\\widehat{\\sigma_C^2}] = \\mathrm{E}[S_W^2] - \\sigma_M^2 = (\\sigma_C^2 + \\sigma_M^2) - \\sigma_M^2 = \\sigma_C^2$.\n\nNext, we define the unbiased sample variance of the device means:\n$$\nS_B^2 = \\frac{1}{D-1} \\sum_{d=1}^{D} (\\bar{y}_{d \\cdot} - \\bar{y}_{\\cdot \\cdot})^2\n$$\nThe terms $\\bar{y}_{d \\cdot}$ are $D$ i.i.d. samples from a distribution with variance $\\mathrm{Var}[\\bar{y}_{d \\cdot}]$. The expectation of $S_B^2$ is this variance:\n$$\n\\mathrm{E}[S_B^2] = \\mathrm{Var}[\\bar{y}_{d \\cdot}] = \\sigma_D^2 + \\frac{\\sigma_C^2 + \\sigma_M^2}{C}\n$$\nUsing the method of moments, we replace the expected values with their sample counterparts. We replace $\\mathrm{E}[S_B^2]$ with $S_B^2$ and $(\\sigma_C^2 + \\sigma_M^2)$ with its unbiased estimator $S_W^2$.\n$$\nS_B^2 \\approx \\sigma_D^2 + \\frac{S_W^2}{C}\n$$\nThis suggests the following unbiased estimator for $\\sigma_D^2$:\n$$\n\\widehat{\\sigma_D^2} = S_B^2 - \\frac{S_W^2}{C}\n$$\nIts expectation is $\\mathrm{E}[\\widehat{\\sigma_D^2}] = \\mathrm{E}[S_B^2] - \\frac{\\mathrm{E}[S_W^2]}{C} = (\\sigma_D^2 + \\frac{\\sigma_C^2 + \\sigma_M^2}{C}) - \\frac{\\sigma_C^2 + \\sigma_M^2}{C} = \\sigma_D^2$.\n\nDerivation of the Variances of the Estimators\n\nTo derive the variances of $\\widehat{\\sigma_C^2}$ and $\\widehat{\\sigma_D^2}$, we use the property that for $n$ i.i.d. samples from $\\mathcal{N}(\\mu, \\sigma^2)$, the unbiased sample variance $s^2$ has a distribution related to the chi-square distribution: $\\frac{(n-1)s^2}{\\sigma^2} \\sim \\chi^2_{n-1}$. The variance of a $\\chi^2_k$ distribution is $2k$, so $\\mathrm{Var}[s^2] = \\frac{2\\sigma^4}{n-1}$.\n\nFor the estimator $\\widehat{\\sigma_C^2}$:\n$$\n\\mathrm{Var}[\\widehat{\\sigma_C^2}] = \\mathrm{Var}[S_W^2 - \\sigma_M^2] = \\mathrm{Var}[S_W^2]\n$$\n$S_W^2 = \\frac{1}{D}\\sum_{d=1}^D S_{W,d}^2$. The terms $S_{W,d}^2$ are independent across devices $d$.\n$$\n\\mathrm{Var}[S_W^2] = \\frac{1}{D^2} \\sum_{d=1}^D \\mathrm{Var}[S_{W,d}^2]\n$$\nEach $S_{W,d}^2$ is a sample variance of $C$ samples from a population with variance $\\sigma_C^2 + \\sigma_M^2$. Thus, using the formula with $n=C$ and $\\sigma^2 = \\sigma_C^2 + \\sigma_M^2$:\n$$\n\\mathrm{Var}[S_{W,d}^2] = \\frac{2(\\sigma_C^2 + \\sigma_M^2)^2}{C-1}\n$$\nSubstituting this back, we get:\n$$\n\\mathrm{Var}[\\widehat{\\sigma_C^2}] = \\mathrm{Var}[S_W^2] = \\frac{1}{D^2} \\cdot D \\cdot \\frac{2(\\sigma_C^2 + \\sigma_M^2)^2}{C-1} = \\frac{2(\\sigma_C^2 + \\sigma_M^2)^2}{D(C-1)}\n$$\n\nFor the estimator $\\widehat{\\sigma_D^2}$:\n$$\n\\mathrm{Var}[\\widehat{\\sigma_D^2}] = \\mathrm{Var}\\left[S_B^2 - \\frac{S_W^2}{C}\\right]\n$$\nIn a balanced one-way random effects model, the between-group sum of squares (related to $S_B^2$) and the within-group sum of squares (related to $S_W^2$) are independent. Therefore, $S_B^2$ and $S_W^2$ are independent, and their covariance is zero.\n$$\n\\mathrm{Var}[\\widehat{\\sigma_D^2}] = \\mathrm{Var}[S_B^2] + \\mathrm{Var}\\left[\\frac{S_W^2}{C}\\right] = \\mathrm{Var}[S_B^2] + \\frac{1}{C^2}\\mathrm{Var}[S_W^2]\n$$\n$S_B^2$ is the sample variance of $D$ i.i.d. samples $\\bar{y}_{d \\cdot}$ from a population with variance $\\mathrm{Var}[\\bar{y}_{d \\cdot}] = \\sigma_D^2 + \\frac{\\sigma_C^2 + \\sigma_M^2}{C}$. Applying the sample variance formula with $n=D$ and $\\sigma^2 = \\mathrm{Var}[\\bar{y}_{d \\cdot}]$:\n$$\n\\mathrm{Var}[S_B^2] = \\frac{2(\\mathrm{Var}[\\bar{y}_{d \\cdot}])^2}{D-1} = \\frac{2}{D-1}\\left(\\sigma_D^2 + \\frac{\\sigma_C^2 + \\sigma_M^2}{C}\\right)^2\n$$\nSubstituting the expressions for $\\mathrm{Var}[S_B^2]$ and $\\mathrm{Var}[S_W^2]$:\n$$\n\\mathrm{Var}[\\widehat{\\sigma_D^2}] = \\frac{2}{D-1}\\left(\\sigma_D^2 + \\frac{\\sigma_C^2 + \\sigma_M^2}{C}\\right)^2 + \\frac{1}{C^2} \\frac{2(\\sigma_C^2 + \\sigma_M^2)^2}{D(C-1)}\n$$\n\nOptimal Experimental Design\n\nThe objective is to minimize the sum of the squared relative standard errors of the variance estimators:\n$$\nJ(D,C) = \\left(\\frac{\\mathrm{SE}[\\widehat{\\sigma_D^2}]}{\\sigma_D^2}\\right)^2 + \\left(\\frac{\\mathrm{SE}[\\widehat{\\sigma_C^2}]}{\\sigma_C^2}\\right)^2 = \\frac{\\mathrm{Var}[\\widehat{\\sigma_D^2}]}{(\\sigma_D^2)^2} + \\frac{\\mathrm{Var}[\\widehat{\\sigma_C^2}]}{(\\sigma_C^2)^2}\n$$\nThe minimization is subject to the constraints $D \\cdot C = N$, and $D, C$ are integers $\\ge 2$. For each test case $(N, \\sigma_D, \\sigma_C, \\sigma_M)$, we enumerate all valid integer factor pairs $(D, C)$ of $N$. For each pair, we compute $J(D,C)$ using the derived variance expressions. The optimal pair $(D^*, C^*)$ is the one that yields the minimum value of $J(D,C)$. In case of a tie, the design with the larger value of $D$ is chosen.\n\nFinally, for the optimal pair $(D^*, C^*)$, the corresponding standard errors $\\mathrm{SE}[\\widehat{\\sigma_D^2}] = \\sqrt{\\mathrm{Var}[\\widehat{\\sigma_D^2}]}$ and $\\mathrm{SE}[\\widehat{\\sigma_C^2}] = \\sqrt{\\mathrm{Var}[\\widehat{\\sigma_C^2}]}$ are computed and reported. The variances are evaluated using the optimal $(D^*, C^*)$ and the given true variance components.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the optimal experimental design problem for estimating variance components\n    in a hierarchical Gaussian model for emerging memory devices.\n    \"\"\"\n    # Test cases: (N, sigma_D, sigma_C, sigma_M)\n    test_cases = [\n        (48, 3.0e-7, 2.0e-7, 5.0e-8),\n        (36, 1.0e-7, 5.0e-8, 2.0e-7),\n        (8, 5.0e-7, 1.0e-8, 1.0e-8),\n    ]\n\n    results = []\n    for N, sigma_D, sigma_C, sigma_M in test_cases:\n        result = find_optimal_design(N, sigma_D, sigma_C, sigma_M)\n        results.append(result)\n\n    # Format the output string as per the requirements.\n    # The string representation of each inner list must not contain spaces.\n    results_str = [f\"[{r[0]},{r[1]},{r[2]},{r[3]}]\" for r in results]\n    print(f\"[{','.join(results_str)}]\")\n\ndef find_optimal_design(N, sigma_D, sigma_C, sigma_M):\n    \"\"\"\n    Finds the optimal (D, C) pair for a given set of parameters.\n\n    Args:\n        N (int): Total number of measurements.\n        sigma_D (float): Standard deviation for device-to-device variability.\n        sigma_C (float): Standard deviation for cycle-to-cycle variability.\n        sigma_M (float): Standard deviation for measurement noise.\n\n    Returns:\n        list: A list containing [D_opt, C_opt, SE_D_opt, SE_C_opt].\n    \"\"\"\n    K_D = sigma_D**2\n    K_C = sigma_C**2\n    K_M = sigma_M**2\n\n    # Generate all valid integer factor pairs (D, C) of N, with D, C >= 2.\n    factors = []\n    for d_cand in range(2, int(N / 2) + 1):\n        if N % d_cand == 0:\n            c_cand = N // d_cand\n            if c_cand >= 2:\n                factors.append((d_cand, c_cand))\n    \n    # Add perfect square case if missed.\n    sqrt_N = int(np.sqrt(N))\n    if sqrt_N * sqrt_N == N and sqrt_N >= 2:\n      if (sqrt_N, sqrt_N) not in factors:\n        factors.append((sqrt_N, sqrt_N))\n\n    # Sort by D descending to handle the tie-breaking rule efficiently.\n    # The first pair found to minimize J will have the largest D among any ties.\n    factors.sort(key=lambda x: x[0], reverse=True)\n\n    min_J = float('inf')\n    optimal_pair = None\n\n    for D, C in factors:\n        var_K_D, var_K_C = calculate_estimator_variances(D, C, K_D, K_C, K_M)\n        \n        # The problem statement's parameters ensure K_D and K_C are non-zero.\n        J = (var_K_D / (K_D**2)) + (var_K_C / (K_C**2))\n        \n        if J  min_J:\n            min_J = J\n            optimal_pair = (D, C)\n    \n    if optimal_pair is None:\n        # This case should not be reached with the given test cases.\n        raise ValueError(f\"No valid (D, C) factorization found for N={N} with D, C >= 2.\")\n        \n    D_opt, C_opt = optimal_pair\n    var_K_D_opt, var_K_C_opt = calculate_estimator_variances(D_opt, C_opt, K_D, K_C, K_M)\n    \n    se_K_D = np.sqrt(var_K_D_opt)\n    se_K_C = np.sqrt(var_K_C_opt)\n    \n    return [D_opt, C_opt, se_K_D, se_K_C]\n\ndef calculate_estimator_variances(D, C, K_D, K_C, K_M):\n    \"\"\"\n    Calculates the theoretical variances of the variance component estimators.\n    K_D, K_C, K_M are the variances (sigma^2).\n    \"\"\"\n    if D = 1 or C = 1:\n        return float('inf'), float('inf')\n\n    K_CM = K_C + K_M\n\n    # Variance of the cycle-to-cycle variance estimator\n    var_K_C = (2 * (K_CM**2)) / (D * (C - 1))\n\n    # Variance of the device-to-device variance estimator\n    term1_var_K_D = (2 / (D - 1)) * (K_D + K_CM / C)**2\n    term2_var_K_D = (2 * (K_CM**2)) / (D * (C**2) * (C - 1))\n    var_K_D = term1_var_K_D + term2_var_K_D\n\n    return var_K_D, var_K_C\n\nif __name__ == '__main__':\n    solve()\n```"
        },
        {
            "introduction": "Achieving high programming precision in analog memory arrays requires compensating for the unique, non-ideal response of each individual device. A common and effective strategy is to perform a closed-loop calibration, where each device's local update characteristics—its gain ($\\beta$) and offset ($\\alpha$)—are estimated using linear regression. This hands-on problem challenges you to determine the number of measurements needed to estimate these parameters to a specified precision, linking the statistical theory of confidence intervals to the practical design of an efficient calibration routine .",
            "id": "4053981",
            "problem": "Consider a neuromorphic synaptic array composed of emerging memory devices whose conductance response to small voltage pulses exhibits device-dependent non-idealities. Each calibration measurement applies a small write voltage $v_i \\in \\mathbb{R}$ and observes an incremental conductance change $y_i \\in \\mathbb{R}$. Assume a locally linear response model around a chosen operating point, defined by the offset $\\alpha$ (in Siemens) and the local update gain $\\beta = \\partial G / \\partial V$ (in Siemens per Volt), such that the measurement model is\n$$\ny_i = \\alpha + \\beta v_i + \\eta_i,\n$$\nwhere $\\eta_i$ are independent noise terms satisfying a Gaussian model $\\eta_i \\sim \\mathcal{N}(0,\\sigma^2)$ with known standard deviation $\\sigma$ (in Siemens) that aggregates read noise and cycle-to-cycle programming variability. You will design a closed-loop calibration routine that repeatedly applies write voltages to estimate $\\alpha$ and $\\beta$, and then derive the number of measurements needed to guarantee a prescribed confidence interval half-width for both parameters.\n\nClosed-loop routine specification:\n- At iteration $i$, select $v_i \\in \\{+A, -A\\}$ with fixed amplitude $A > 0$ (in Volts).\n- The selection is closed-loop and aims to keep the design balanced, enforcing $\\sum_{i=1}^n v_i = 0$ by alternating the sign adaptively based on the running estimate to minimize estimator coupling. This enforces $\\bar{v} = \\frac{1}{n}\\sum_i v_i = 0$ and $\\sum_i v_i^2 = nA^2$.\n- After $n$ measurements, estimate $(\\alpha,\\beta)$ using Ordinary Least Squares (OLS) regression with a design matrix having columns $[1, v_i]$.\n\nUsing the balanced design property, the OLS estimator covariance is determined by the fundamental result\n$$\n\\operatorname{Cov}\\begin{pmatrix}\\hat{\\alpha} \\\\ \\hat{\\beta}\\end{pmatrix}\n= \\sigma^2 \\left(X^\\top X\\right)^{-1},\n$$\nand under $\\bar{v}=0$ and $v_i^2=A^2$ for all $i$, the marginal variances satisfy\n$$\n\\operatorname{Var}(\\hat{\\alpha}) = \\frac{\\sigma^2}{n}, \\quad\n\\operatorname{Var}(\\hat{\\beta}) = \\frac{\\sigma^2}{nA^2}.\n$$\nAssuming Gaussian noise and unknown parameters, the sampling distributions of the standardized OLS estimators follow the Student's $t$ distribution with $n-2$ degrees of freedom. Therefore, the two-sided $(1-\\delta)$ confidence interval half-widths for $\\hat{\\alpha}$ and $\\hat{\\beta}$ are\n$$\nh_\\alpha(n) = t_{1-\\delta/2}(n-2)\\sqrt{\\frac{\\sigma^2}{n}}, \\quad\nh_\\beta(n) = t_{1-\\delta/2}(n-2)\\sqrt{\\frac{\\sigma^2}{nA^2}},\n$$\nwhere $t_{1-\\delta/2}(n-2)$ denotes the $(1-\\delta/2)$ quantile of the Student's $t$ distribution with $n-2$ degrees of freedom. The calibration goal is to choose the smallest even integer $n \\geq 4$ such that both $h_\\alpha(n) \\le \\varepsilon_\\alpha$ and $h_\\beta(n) \\le \\varepsilon_\\beta$, where $\\varepsilon_\\alpha$ (in Siemens) and $\\varepsilon_\\beta$ (in Siemens per Volt) are prescribed half-width tolerances.\n\nTask:\n- Implement a program that, given ($\\sigma$, $A$, $\\varepsilon_\\alpha$, $\\varepsilon_\\beta$, $\\delta$), computes the smallest even integer $n \\ge 4$ that satisfies the two inequalities above. If $\\sigma = 0$, return $n=4$.\n- Use a search procedure over even $n$ that evaluates $t_{1-\\delta/2}(n-2)$ and the half-widths until both constraints are met.\n\nPhysical units:\n- Conductance offset $\\alpha$ must be considered in Siemens ($\\mathrm{S}$).\n- Local update gain $\\beta$ must be considered in Siemens per Volt ($\\mathrm{S}/\\mathrm{V}$).\n- Voltage amplitude $A$ must be considered in Volts ($\\mathrm{V}$).\n- Noise standard deviation $\\sigma$ must be considered in Siemens ($\\mathrm{S}$).\n\nAngle units do not apply. All fraction or ratio outputs must be expressed as decimals.\n\nTest suite:\nProvide results for the following parameter sets, expressed in the units above:\n1. $\\sigma = 5 \\times 10^{-6}$, $A = 0.2$, $\\varepsilon_\\alpha = 5 \\times 10^{-6}$, $\\varepsilon_\\beta = 1 \\times 10^{-5}$, $\\delta = 0.05$.\n2. $\\sigma = 1 \\times 10^{-6}$, $A = 0.3$, $\\varepsilon_\\alpha = 2 \\times 10^{-6}$, $\\varepsilon_\\beta = 5 \\times 10^{-6}$, $\\delta = 0.10$.\n3. $\\sigma = 5 \\times 10^{-6}$, $A = 0.05$, $\\varepsilon_\\alpha = 5 \\times 10^{-6}$, $\\varepsilon_\\beta = 5 \\times 10^{-6}$, $\\delta = 0.01$.\n4. $\\sigma = 0$, $A = 0.1$, $\\varepsilon_\\alpha = 1 \\times 10^{-6}$, $\\varepsilon_\\beta = 1 \\times 10^{-6}$, $\\delta = 0.05$.\n\nFinal output format:\nYour program should produce a single line of output containing the resulting measurement counts as a comma-separated list of integers enclosed in square brackets, for example, \"[n1,n2,n3,n4]\".",
            "solution": "We start from the principle that a locally linear response model around an operating point is justified by a first-order Taylor expansion of the conductance update with respect to the applied voltage. Denote the observed incremental conductance change by $y_i$ and the applied voltage pulse by $v_i$. The local model is\n$$\ny_i = \\alpha + \\beta v_i + \\eta_i,\n$$\nwhere $\\alpha$ is the local offset in Siemens and $\\beta = \\partial G/\\partial V$ is the local update gain in Siemens per Volt. The noise term $\\eta_i$ captures device non-idealities including read noise and cycle-to-cycle variability, and is modeled as independent Gaussian with zero mean and variance $\\sigma^2$.\n\nThe closed-loop calibration design chooses $v_i \\in \\{+A,-A\\}$ and enforces balancing $\\sum_i v_i = 0$ by adaptively alternating the sign based on current estimates or the running residual so as to maintain a zero mean for the applied voltages. This balancing is important because it minimizes estimator covariance and simplifies the variance of the Ordinary Least Squares (OLS) estimators. Let $X$ be the $n \\times 2$ design matrix whose first column is all ones and second column is $v_i$. The OLS estimator is given by\n$$\n\\begin{pmatrix}\\hat{\\alpha} \\\\ \\hat{\\beta}\\end{pmatrix}\n= \\left(X^\\top X\\right)^{-1} X^\\top \\mathbf{y},\n$$\nwhere $\\mathbf{y} = (y_1,\\dots,y_n)^\\top$.\n\nUnder the balanced design, we have $\\bar{v} = \\frac{1}{n}\\sum_{i=1}^n v_i = 0$ and $\\sum_{i=1}^n v_i^2 = nA^2$. The well-tested formula for the covariance of the OLS estimator in the homoscedastic Gaussian model is\n$$\n\\operatorname{Cov}\\left(\\begin{pmatrix}\\hat{\\alpha} \\\\ \\hat{\\beta}\\end{pmatrix}\\right)\n= \\sigma^2 \\left(X^\\top X\\right)^{-1}.\n$$\nUsing the structure\n$$\nX^\\top X = \\begin{pmatrix}\nn  \\sum_i v_i \\\\\n\\sum_i v_i  \\sum_i v_i^2\n\\end{pmatrix}\n= \\begin{pmatrix}\nn  0 \\\\\n0  nA^2\n\\end{pmatrix},\n$$\nwe obtain\n$$\n\\left(X^\\top X\\right)^{-1}\n= \\begin{pmatrix}\n\\frac{1}{n}  0 \\\\\n0  \\frac{1}{nA^2}\n\\end{pmatrix},\n$$\nwhich leads directly to the marginal variances\n$$\n\\operatorname{Var}(\\hat{\\alpha}) = \\frac{\\sigma^2}{n}, \\quad\n\\operatorname{Var}(\\hat{\\beta}) = \\frac{\\sigma^2}{nA^2}.\n$$\n\nWhen $\\sigma^2$ is unknown but the Gaussian assumption holds, the standardized estimators follow the Student's $t$ distribution with $n-2$ degrees of freedom due to the use of an unbiased variance estimate and the fact that two parameters are estimated. The $(1-\\delta)$ two-sided confidence interval half-widths then take the form\n$$\nh_\\alpha(n) = t_{1-\\delta/2}(n-2)\\sqrt{\\frac{\\sigma^2}{n}}, \\quad\nh_\\beta(n) = t_{1-\\delta/2}(n-2)\\sqrt{\\frac{\\sigma^2}{nA^2}},\n$$\nwhere $t_{1-\\delta/2}(n-2)$ is the $(1-\\delta/2)$ quantile of the Student's $t$ distribution with $n-2$ degrees of freedom.\n\nThe calibration requirement is that $h_\\alpha(n) \\le \\varepsilon_\\alpha$ and $h_\\beta(n) \\le \\varepsilon_\\beta$ for given tolerances $\\varepsilon_\\alpha$ and $\\varepsilon_\\beta$. Substituting the formulas yields two inequalities:\n$$\nt_{1-\\delta/2}(n-2)\\frac{\\sigma}{\\sqrt{n}} \\le \\varepsilon_\\alpha, \\quad\nt_{1-\\delta/2}(n-2)\\frac{\\sigma}{\\sqrt{n}A} \\le \\varepsilon_\\beta.\n$$\nBecause $t_{1-\\delta/2}(n-2)$ depends on $n$, a closed-form solution for $n$ is not available. However, the monotonicity of both sides with respect to $n$ allows a simple search over even integers $n \\ge 4$ (to keep the design perfectly balanced) until both constraints hold.\n\nAlgorithmic procedure used in the program:\n1. For each test case, if $\\sigma = 0$, return $n=4$ because noiseless measurements collapse the confidence interval to zero and the minimal even $n$ with at least one degree of freedom beyond the two parameters is $4$.\n2. Otherwise, initialize $n=4$ and iteratively increase $n$ by $2$.\n3. At each $n$, compute $q = t_{1-\\delta/2}(n-2)$ from the Student's $t$ distribution.\n4. Compute $h_\\alpha(n) = q \\,\\sigma / \\sqrt{n}$ and $h_\\beta(n) = q \\,\\sigma / (\\sqrt{n}A)$.\n5. If both $h_\\alpha(n) \\le \\varepsilon_\\alpha$ and $h_\\beta(n) \\le \\varepsilon_\\beta$, stop and return $n$; else increment $n$ and continue.\n\nThis procedure ensures the smallest even $n$ satisfying both confidence interval constraints is found. The test suite parameters are provided with clear units, and the program outputs the list of required measurement counts in the specified single-line format.",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import t\n\ndef required_measurements(sigma, A, eps_alpha, eps_beta, delta, max_n=1000000):\n    \"\"\"\n    Compute the smallest even integer n >= 4 such that:\n      t_{1-delta/2}(n-2) * sigma / sqrt(n)      = eps_alpha\n      t_{1-delta/2}(n-2) * sigma / (sqrt(n)*A)  = eps_beta\n    If sigma == 0, return 4.\n    \"\"\"\n    # Handle noiseless case: minimal balanced design with df = n-2 >= 2 -> n >= 4\n    if sigma == 0.0:\n        return 4\n\n    # Start from the smallest even n with df >= 2\n    n = 4\n    # Sanity checks for parameters\n    if A = 0 or eps_alpha = 0 or eps_beta = 0 or not (0  delta  1):\n        # Invalid parameters; in a robust system we might raise an error.\n        # Here, return -1 to indicate failure.\n        return -1\n\n    while n = max_n:\n        df = n - 2\n        # Compute the t quantile for two-sided (1 - delta) CI\n        q = t.ppf(1 - delta / 2.0, df)\n        # Half-widths for alpha and beta\n        h_alpha = q * sigma / np.sqrt(n)\n        h_beta = q * sigma / (np.sqrt(n) * A)\n        if (h_alpha = eps_alpha) and (h_beta = eps_beta):\n            return n\n        n += 2  # maintain even n for balanced design\n\n    # If not found within max_n, indicate failure\n    return -1\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case: (sigma [S], A [V], eps_alpha [S], eps_beta [S/V], delta)\n    test_cases = [\n        (5e-6, 0.2, 5e-6, 1e-5, 0.05),\n        (1e-6, 0.3, 2e-6, 5e-6, 0.10),\n        (5e-6, 0.05, 5e-6, 5e-6, 0.01),\n        (0.0, 0.1, 1e-6, 1e-6, 0.05),\n    ]\n\n    results = []\n    for case in test_cases:\n        sigma, A, eps_alpha, eps_beta, delta = case\n        n = required_measurements(sigma, A, eps_alpha, eps_beta, delta)\n        results.append(n)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}