{
    "hands_on_practices": [
        {
            "introduction": "This practice delves into the fundamental dynamics of neural populations within an organoid. We use the classic Wilson-Cowan model to understand how the interaction between excitatory ($E$) and inhibitory ($I$) neuron groups can give rise to collective behaviors like oscillations. By performing a linear stability analysis, you will learn to predict the conditions under which these network rhythms emerge, a crucial skill for linking theoretical models to observable data from MEA recordings .",
            "id": "4037921",
            "problem": "Consider a cortical organoid network interfaced as a bio-hybrid system with a Multi-Electrode Array (MEA). A coarse-grained excitatory-inhibitory population description is modeled by the Wilsonâ€“Cowan equations\n$$\\tau_{e} \\,\\dot{E}(t) = -E(t) + S\\!\\left(w_{ee}\\,E(t) - w_{ei}\\,I(t) + P\\right)$$\n$$\\tau_{i} \\,\\dot{I}(t) = -I(t) + S\\!\\left(w_{ie}\\,E(t) - w_{ii}\\,I(t)\\right),$$\nwhere $E(t)$ and $I(t)$ are dimensionless population firing rates, $\\tau_{e}$ and $\\tau_{i}$ are positive time constants, $w_{ee}, w_{ei}, w_{ie}, w_{ii}$ are dimensionless effective synaptic couplings, $P$ is a constant excitatory drive, and $S(x)$ is a differentiable sigmoidal activation function representing the population response curve. In an operating regime where the organoid exhibits stationary activity with small-amplitude oscillations observable in the MEA power spectrum, the local gains (slopes) of $S$ at the steady-state inputs are empirically estimated to be $s_{e} = S'\\!\\left(w_{ee}\\,E^{*} - w_{ei}\\,I^{*} + P\\right)$ and $s_{i} = S'\\!\\left(w_{ie}\\,E^{*} - w_{ii}\\,I^{*}\\right)$, with $E^{*}$ and $I^{*}$ denoting the steady-state rates.\n\nStarting from the model equations and fundamental linear systems analysis, derive the condition under which the linearized dynamics around $(E^{*}, I^{*})$ exhibits oscillatory modes and obtain an expression for the small-oscillation angular frequency in terms of the linearization parameters. Then, for the parameter set\n- $\\tau_{e} = 0.02 \\ \\mathrm{s}$, $\\tau_{i} = 0.01 \\ \\mathrm{s}$,\n- $w_{ee} = 1.5$, $w_{ei} = 2.0$, $w_{ie} = 3.0$, $w_{ii} = 0.5$,\n- $s_{e} = 0.8$, $s_{i} = 0.7$,\ncompute the predicted oscillation frequency observed in the MEA power spectrum, assuming the system is sufficiently close to the steady operating point for the linear prediction to be valid. Round your answer to $4$ significant figures and express it in $\\mathrm{Hz}$. You may treat $P$ and the steady state $(E^{*}, I^{*})$ as fixed quantities that only determine $s_{e}$ and $s_{i}$ and do not enter explicitly into the linearization beyond those slopes.",
            "solution": "The user-provided problem is assessed to be valid. It is scientifically grounded in the well-established Wilson-Cowan model of neural population dynamics, a standard framework in computational neuroscience. The problem is well-posed, providing a complete set of equations and parameters for a standard linear stability analysis. The language is objective and precise, and the setup is self-contained and internally consistent. No flaws that would invalidate the problem have been identified. We may therefore proceed with the solution.\n\nThe problem asks for the derivation of the condition for oscillatory behavior and the corresponding oscillation frequency for a system described by the Wilson-Cowan equations, followed by a numerical calculation for a given parameter set. The analysis begins by linearizing the system around its steady-state point $(E^{*}, I^{*})$.\n\nThe given system of equations is:\n$$\n\\tau_{e} \\,\\dot{E}(t) = -E(t) + S(u_e(t))\n$$\n$$\n\\tau_{i} \\,\\dot{I}(t) = -I(t) + S(u_i(t))\n$$\nwhere $u_e(t) = w_{ee}\\,E(t) - w_{ei}\\,I(t) + P$ and $u_i(t) = w_{ie}\\,E(t) - w_{ii}\\,I(t)$.\n\nAt the steady state, denoted by $(E^{*}, I^{*})$, the time derivatives are zero:\n$$\n0 = -E^{*} + S(u_e^{*})\n$$\n$$\n0 = -I^{*} + S(u_i^{*})\n$$\nwhere $u_e^{*} = w_{ee}\\,E^{*} - w_{ei}\\,I^{*} + P$ and $u_i^{*} = w_{ie}\\,E^{*} - w_{ii}\\,I^{*}$.\n\nTo analyze the dynamics near this steady state, we introduce small perturbations $\\delta_E(t) = E(t) - E^{*}$ and $\\delta_I(t) = I(t) - I^{*}$. The time evolution of these perturbations is found by substituting $E(t) = E^{*} + \\delta_E(t)$ and $I(t) = I^{*} + \\delta_I(t)$ into the original equations and performing a first-order Taylor expansion of the sigmoid function $S(x)$ around the steady-state inputs $u_e^{*}$ and $u_i^{*}$.\n\nFor the excitatory population:\n$$\n\\tau_e \\dot{\\delta}_E(t) = -(E^{*} + \\delta_E(t)) + S(u_e^{*} + w_{ee}\\delta_E(t) - w_{ei}\\delta_I(t))\n$$\nUsing the Taylor expansion $S(x_0 + \\Delta x) \\approx S(x_0) + S'(x_0)\\Delta x$:\n$$\n\\tau_e \\dot{\\delta}_E(t) \\approx -E^{*} - \\delta_E(t) + S(u_e^{*}) + S'(u_e^{*})(w_{ee}\\delta_E(t) - w_{ei}\\delta_I(t))\n$$\nSince $-E^{*} + S(u_e^{*}) = 0$ at steady state, and using the given definition $s_e = S'(u_e^{*})$, the equation simplifies to:\n$$\n\\tau_e \\dot{\\delta}_E(t) = -\\delta_E(t) + s_e(w_{ee}\\delta_E(t) - w_{ei}\\delta_I(t))\n$$\n$$\n\\dot{\\delta}_E(t) = \\frac{1}{\\tau_e}[(s_e w_{ee} - 1)\\delta_E(t) - s_e w_{ei}\\delta_I(t)]\n$$\n\nSimilarly, for the inhibitory population:\n$$\n\\tau_i \\dot{\\delta}_I(t) = -(I^{*} + \\delta_I(t)) + S(u_i^{*} + w_{ie}\\delta_E(t) - w_{ii}\\delta_I(t))\n$$\nExpanding and using the steady-state condition $-I^{*} + S(u_i^{*}) = 0$ and the definition $s_i = S'(u_i^{*})$:\n$$\n\\tau_i \\dot{\\delta}_I(t) \\approx -\\delta_I(t) + s_i(w_{ie}\\delta_E(t) - w_{ii}\\delta_I(t))\n$$\n$$\n\\dot{\\delta}_I(t) = \\frac{1}{\\tau_i}[s_i w_{ie}\\delta_E(t) - (1 + s_i w_{ii})\\delta_I(t)]\n$$\n\nThis linearized system can be written in matrix form, $\\dot{\\mathbf{x}} = J\\mathbf{x}$, where $\\mathbf{x} = \\begin{pmatrix} \\delta_E \\\\ \\delta_I \\end{pmatrix}$ and $J$ is the Jacobian matrix evaluated at the steady state:\n$$\nJ = \\begin{pmatrix}\n\\frac{s_e w_{ee} - 1}{\\tau_e}  -\\frac{s_e w_{ei}}{\\tau_e} \\\\\n\\frac{s_i w_{ie}}{\\tau_i}  -\\frac{1 + s_i w_{ii}}{\\tau_i}\n\\end{pmatrix}\n$$\nThe dynamics of the linearized system are determined by the eigenvalues $\\lambda$ of $J$, which are the roots of the characteristic equation $\\det(J - \\lambda I) = 0$. This equation is $\\lambda^2 - \\text{Tr}(J)\\lambda + \\det(J) = 0$, where $\\text{Tr}(J)$ is the trace and $\\det(J)$ is the determinant of the Jacobian.\n\nThe eigenvalues are given by:\n$$\n\\lambda = \\frac{\\text{Tr}(J) \\pm \\sqrt{(\\text{Tr}(J))^2 - 4\\det(J)}}{2}\n$$\nThe system exhibits oscillatory modes when the eigenvalues are a complex conjugate pair. This occurs when the discriminant of the characteristic equation is negative.\nCondition for oscillatory modes:\n$$\n(\\text{Tr}(J))^2 - 4\\det(J)  0\n$$\nWhen this condition is met, the eigenvalues are of the form $\\lambda = \\alpha \\pm i\\omega$, where $\\alpha = \\frac{\\text{Tr}(J)}{2}$ is the real part determining the stability (growth or decay of oscillations) and $\\omega$ is the imaginary part, representing the angular frequency of the small oscillations.\nThe expression for the angular frequency is:\n$$\n\\omega = \\frac{\\sqrt{4\\det(J) - (\\text{Tr}(J))^2}}{2}\n$$\nThis completes the derivation part of the problem.\n\nNow, we compute the numerical value for the given parameters:\n$\\tau_{e} = 0.02 \\ \\mathrm{s}$, $\\tau_{i} = 0.01 \\ \\mathrm{s}$\n$w_{ee} = 1.5$, $w_{ei} = 2.0$, $w_{ie} = 3.0$, $w_{ii} = 0.5$\n$s_{e} = 0.8$, $s_{i} = 0.7$\n\nFirst, calculate the elements of the Jacobian matrix $J$:\n$$\nJ_{11} = \\frac{s_e w_{ee} - 1}{\\tau_e} = \\frac{(0.8)(1.5) - 1}{0.02} = \\frac{1.2 - 1}{0.02} = \\frac{0.2}{0.02} = 10 \\ \\mathrm{s}^{-1}\n$$\n$$\nJ_{12} = -\\frac{s_e w_{ei}}{\\tau_e} = -\\frac{(0.8)(2.0)}{0.02} = -\\frac{1.6}{0.02} = -80 \\ \\mathrm{s}^{-1}\n$$\n$$\nJ_{21} = \\frac{s_i w_{ie}}{\\tau_i} = \\frac{(0.7)(3.0)}{0.01} = \\frac{2.1}{0.01} = 210 \\ \\mathrm{s}^{-1}\n$$\n$$\nJ_{22} = -\\frac{1 + s_i w_{ii}}{\\tau_i} = -\\frac{1 + (0.7)(0.5)}{0.01} = -\\frac{1 + 0.35}{0.01} = -\\frac{1.35}{0.01} = -135 \\ \\mathrm{s}^{-1}\n$$\nThe Jacobian matrix is\n$$\nJ = \\begin{pmatrix} 10  -80 \\\\ 210  -135 \\end{pmatrix}\n$$\n\nNext, we calculate the trace and determinant of $J$:\n$$\n\\text{Tr}(J) = J_{11} + J_{22} = 10 - 135 = -125 \\ \\mathrm{s}^{-1}\n$$\n$$\n\\det(J) = J_{11}J_{22} - J_{12}J_{21} = (10)(-135) - (-80)(210) = -1350 + 16800 = 15450 \\ \\mathrm{s}^{-2}\n$$\n\nNow, we check the condition for oscillations:\n$$\n(\\text{Tr}(J))^2 - 4\\det(J) = (-125)^2 - 4(15450) = 15625 - 61800 = -46175\n$$\nSince the discriminant is negative ($-46175  0$), the system exhibits oscillatory behavior. Since $\\text{Tr}(J)  0$, the oscillations are damped, and the steady state is a stable spiral point, consistent with the problem's description of \"stationary activity with small-amplitude oscillations\".\n\nFinally, we compute the angular frequency $\\omega$ and the frequency $f$ in Hertz.\n$$\n\\omega = \\frac{\\sqrt{4\\det(J) - (\\text{Tr}(J))^2}}{2} = \\frac{\\sqrt{4(15450) - (-125)^2}}{2} = \\frac{\\sqrt{61800 - 15625}}{2} = \\frac{\\sqrt{46175}}{2} \\ \\mathrm{rad/s}\n$$\nThe frequency in Hertz is $f = \\frac{\\omega}{2\\pi}$:\n$$\nf = \\frac{\\sqrt{46175}}{4\\pi} \\approx \\frac{214.88369}{4\\pi} \\approx 17.09999 \\ \\mathrm{Hz}\n$$\nRounding to $4$ significant figures, the predicted oscillation frequency is $17.10 \\ \\mathrm{Hz}$.",
            "answer": "$$\n\\boxed{17.10}\n$$"
        },
        {
            "introduction": "To evaluate an organoid's potential as a computational device, we must quantify its ability to process information. This exercise introduces a core tool from information theory, mutual information ($I(S;Y)$), to measure how much a system's output ($Y$) reveals about its input ($S$). You will calculate this value from hypothetical experimental data, gaining hands-on experience in assessing the information-encoding capacity of a bio-hybrid system and understanding practical limits like response saturation .",
            "id": "4037904",
            "problem": "A human cortical organoid interfaced with a Micro-Electrode Array (MEA) is driven with discrete extracellular stimulation amplitudes, producing spike count responses in short observation windows. Let the discrete stimulus set be $S \\in \\{s_{1}, s_{2}, s_{3}\\}$, representing low, medium, and high amplitudes, respectively. In a fixed observation window of $T = 50\\,\\mathrm{ms}$, the spike count response is discretized as $Y \\in \\{0, 1, 2, 3\\}$. Assume stationarity and that repeated trials at each stimulus produce the following empirically observed conditional response distributions:\n- For $s_{1}$: $p(y \\mid s_{1}) = [0.5, 0.3, 0.15, 0.05]$ for $y = 0, 1, 2, 3$.\n- For $s_{2}$: $p(y \\mid s_{2}) = [0.2, 0.4, 0.3, 0.1]$ for $y = 0, 1, 2, 3$.\n- For $s_{3}$: $p(y \\mid s_{3}) = [0.05, 0.15, 0.35, 0.45]$ for $y = 0, 1, 2, 3$.\n\nAssume the prior stimulus distribution is uniform, $p(s_{1}) = p(s_{2}) = p(s_{3}) = 1/3$. Using the definition of mutual information from Shannon information theory, compute $I(S;Y)$ between stimuli $S$ and spike-count responses $Y$. Express your answer in bits (use base-$2$ logarithms) and round your answer to four significant figures. In addition, based on the computed value and the provided distributions, briefly interpret what saturation of spike counts at high stimulus intensities implies for $I(S;Y)$ as intensities increase beyond $s_{3}$.",
            "solution": "The user has provided a well-defined problem in computational neuroscience and information theory. The problem is scientifically grounded, self-contained, and all provided probability distributions are valid (i.e., they sum to $1$ for each condition). Therefore, the problem is deemed valid and a solution will be provided.\n\nThe primary task is to compute the mutual information $I(S;Y)$ between the stimulus set $S = \\{s_1, s_2, s_3\\}$ and the spike-count response set $Y = \\{0, 1, 2, 3\\}$. The mutual information quantifies the reduction in uncertainty about one random variable from observing another. It is defined as:\n$$I(S;Y) = H(Y) - H(Y|S)$$\nwhere $H(Y)$ is the entropy of the response distribution and $H(Y|S)$ is the conditional entropy of the response given the stimulus. The calculations will use the base-2 logarithm to express the answer in bits.\n\nFirst, we calculate the conditional entropy $H(Y|S)$, which is the average of the response entropies for each stimulus condition:\n$$H(Y|S) = \\sum_{s \\in S} p(s) H(Y|S=s)$$\nwhere $H(Y|S=s) = -\\sum_{y \\in Y} p(y|s) \\log_2(p(y|s))$.\nGiven the uniform stimulus prior $p(s) = 1/3$:\n- $H(Y|S=s_1) = -[0.5\\log_2(0.5) + 0.3\\log_2(0.3) + 0.15\\log_2(0.15) + 0.05\\log_2(0.05)] \\approx 1.6459$ bits.\n- $H(Y|S=s_2) = -[0.2\\log_2(0.2) + 0.4\\log_2(0.4) + 0.3\\log_2(0.3) + 0.1\\log_2(0.1)] \\approx 1.8464$ bits.\n- $H(Y|S=s_3) = -[0.05\\log_2(0.05) + 0.15\\log_2(0.15) + 0.35\\log_2(0.35) + 0.45\\log_2(0.45)] \\approx 1.6734$ bits.\n\nThe total conditional entropy is:\n$$H(Y|S) = \\frac{1}{3} (1.6459 + 1.8464 + 1.6734) \\approx 1.7219 \\text{ bits.}$$\n\nNext, we calculate the marginal entropy of the response, $H(Y)$. We first need the marginal probability distribution $p(y) = \\sum_{s \\in S} p(y|s)p(s)$:\n- $p(y=0) = \\frac{1}{3} (0.5 + 0.2 + 0.05) = 0.25$\n- $p(y=1) = \\frac{1}{3} (0.3 + 0.4 + 0.15) = \\frac{0.85}{3} \\approx 0.2833$\n- $p(y=2) = \\frac{1}{3} (0.15 + 0.3 + 0.35) = \\frac{0.80}{3} \\approx 0.2667$\n- $p(y=3) = \\frac{1}{3} (0.05 + 0.1 + 0.45) = 0.20$\n\nThe marginal entropy $H(Y) = -\\sum_{y \\in Y} p(y)\\log_2(p(y))$ is:\n$$H(Y) = -[0.25\\log_2(0.25) + \\frac{0.85}{3}\\log_2(\\frac{0.85}{3}) + \\frac{0.80}{3}\\log_2(\\frac{0.80}{3}) + 0.20\\log_2(0.20)] \\approx 1.9884 \\text{ bits.}$$\n\nFinally, we compute the mutual information:\n$$I(S;Y) = H(Y) - H(Y|S) \\approx 1.9884 - 1.7219 = 0.2665 \\text{ bits.}$$\nUsing higher precision for all intermediate steps gives $I(S;Y) \\approx 1.98836 - 1.72192 = 0.26644$. Rounding to four significant figures, we get $0.2664$ bits.\n\nThe second part of the problem asks for an interpretation of what saturation of spike counts implies for $I(S;Y)$ as stimulus intensities increase beyond $s_{3}$.\nNeural response saturation means that for stimulus intensities $s$ greater than some level (in this case, beyond $s_3$), the spike count response distribution $p(y|s)$ ceases to change significantly. For example, we would have $p(y|s_4) \\approx p(y|s_3)$ for a stimulus $s_4 > s_3$. Mutual information, $I(S;Y)$, measures how well the response $Y$ can be used to distinguish between different stimuli in $S$. If multiple, distinct, high-intensity stimuli (e.g., $s_3, s_4, s_5, \\dots$) all produce the same statistical response, it becomes impossible to distinguish between them based on observing the spike count $Y$. While increasing the number of stimulus levels increases the total entropy of the stimulus set, $H(S)$, the information transmitted, $I(S;Y)$, will not increase proportionally. The inability to discriminate between the saturated stimuli means that the uncertainty about the stimulus given the response, $H(S|Y)$, grows along with $H(S)$, causing their difference, $I(S;Y)$, to plateau. The information channel formed by the organoid saturates, indicating a limit to the amount of information the system can encode about stimulus intensity. Essentially, distinct inputs are mapped to statistically indistinguishable outputs, leading to a bottleneck in information transmission.",
            "answer": "$$\n\\boxed{0.2664}\n$$"
        },
        {
            "introduction": "Beyond understanding input-output relationships, a deeper challenge is to reverse-engineer the functional circuitry of a neural network from its activity. This practice guides you through implementing Granger causality, a powerful time-series analysis technique used to infer directed influence between different recording sites. By building a Vector Autoregressive (VAR) model and accounting for confounding variables, you will develop a practical tool for mapping the effective connectivity within an organoid, a key step towards understanding its internal computations .",
            "id": "4037977",
            "problem": "You are given a directive to design and implement an algorithm that uses Granger causality (GC) within a Vector Autoregressive (VAR) framework to infer directed connectivity between channels recorded from a bio-hybrid system, such as an organoid network instrumented with multiple electrodes. Begin from the accepted definition of Granger causality: a time series $x_t$ is said to Granger-cause a time series $y_t$ if past values of $x_t$ contain unique predictive information about $y_t$ beyond what is contained in the past values of $y_t$ alone. Use a VAR model of appropriate order and the principle of nested hypothesis testing via Ordinary Least Squares (OLS) to derive and implement a test for directed connectivity. Explicitly incorporate the case of common inputs by computing both bivariate GC (ignoring the common input) and conditional GC (including a recorded common input channel to control for confounding). Argue, from first principles and standard assumptions, conditions under which causal identifiability is achievable in the presence of a common input.\n\nYou must produce a complete, runnable program that:\n- Simulates multi-channel time series according to specified test cases using a stable Vector Autoregressive (VAR) model of order $p$:\n$$\n\\mathbf{y}_t = \\sum_{k=1}^{p} \\mathbf{A}_k \\mathbf{y}_{t-k} + \\mathbf{e}_t,\n$$\nwhere $\\mathbf{y}_t \\in \\mathbb{R}^n$ is the channel vector at time $t$, $\\mathbf{A}_k \\in \\mathbb{R}^{n \\times n}$ are lag coefficient matrices, and $\\mathbf{e}_t \\sim \\mathcal{N}(\\mathbf{0}, \\boldsymbol{\\Sigma})$ is zero-mean Gaussian innovation noise with covariance $\\boldsymbol{\\Sigma}$. Stability must be ensured by choosing coefficients such that the spectral radius of the companion matrix is less than $1$.\n\n- Fits, for each directed pair $(x \\to y)$, both an unrestricted OLS regression that includes lags of all included channels and a restricted OLS regression that excludes the lags of the driver channel. Uses the nested model framework to compute the Granger causality $F$-statistic and its significance level at a user-defined significance threshold $\\alpha$ expressed as a decimal.\n\n- Selects the VAR order $p$ from candidates using the Bayesian Information Criterion (BIC) computed from the unrestricted regression for the target channel, for each test. Consider the candidate set $p \\in \\{1, 2\\}$, and choose the $p$ that minimizes the BIC.\n\n- Computes, for each test case, four boolean outputs indicating significant directed connectivity at level $\\alpha$: \n    $[\\text{X}\\to\\text{Y (bivariate)},\\ \\text{Y}\\to\\text{X (bivariate)},\\ \\text{X}\\to\\text{Y (conditional)},\\ \\text{Y}\\to\\text{X (conditional)}]$.\n\nScientific bases and assumptions to use:\n- Granger causality (GC) definition in the time domain: $x_t$ Granger-causes $y_t$ if including past $x_t$ reduces the prediction error variance of $y_t$ compared to a model excluding past $x_t$, conditional on included variables.\n- Ordinary Least Squares (OLS) under Gaussian innovations provides maximum likelihood estimators for linear VAR models, enabling valid $F$-tests under standard regularity conditions.\n- Bayesian Information Criterion (BIC) for model order selection: for an OLS regression with $n_{\\text{eff}}$ samples, residual sum of squares $\\text{RSS}$, and $k$ parameters, the BIC is\n$$\n\\text{BIC} = n_{\\text{eff}} \\cdot \\ln\\left(\\frac{\\text{RSS}}{n_{\\text{eff}}}\\right) + k \\cdot \\ln(n_{\\text{eff}}).\n$$\n\nTesting framework and significance computation:\n- For the directed test $x \\to y$ with a chosen lag order $p$ and included channel set of size $d$, define the unrestricted regression with $k_u = d \\cdot p$ parameters (no intercept) predicting $y_t$ from lagged values of all included channels, and the restricted regression that removes the $p$ lagged terms of $x_t$ from the predictors, yielding $k_r = (d-1) \\cdot p$ parameters. With $n_{\\text{eff}} = T - p$ effective samples and residual sums of squares $\\text{RSS}_u$ and $\\text{RSS}_r$, compute the $F$-statistic:\n$$\nF = \\frac{\\left(\\text{RSS}_r - \\text{RSS}_u\\right) / m}{\\text{RSS}_u / (n_{\\text{eff}} - k_u)},\n$$\nwhere $m = p$ is the number of linear restrictions. Under the null hypothesis that $x_t$ does not Granger-cause $y_t$, $F$ follows an $F$-distribution with $(m, n_{\\text{eff}} - k_u)$ degrees of freedom. Compute the $p$-value and declare significance if the $p$-value is less than $\\alpha$ and $\\text{RSS}_r  \\text{RSS}_u$.\n\nCausal identifiability conditions in the presence of common inputs:\n- To mitigate spurious causality from a common input $z_t$ that drives both $x_t$ and $y_t$, conditional GC must include the recorded $z_t$ in the VAR. If the common input is recorded and included, and if innovations are uncorrelated across channels, the conditional GC test asymptotically identifies directed connectivity under stationarity and correct model order.\n- If there exists a latent unrecorded common input, identifiability may fail; conditions that help include sufficiency of observed variables that block all backdoor paths, independence and whiteness of innovations, absence of instantaneous (zero-lag) coupling, and correct lag order to capture the true dynamics.\n\nUse the following test suite to simulate data and evaluate the algorithm. In all cases, set the significance threshold to $\\alpha = 0.01$, the candidate lag orders to $p \\in \\{1, 2\\}$, and the random seed to $0$. Channels are indexed as $X \\equiv 0$, $Y \\equiv 1$, $Z \\equiv 2$.\n\n- Test case $1$ (happy path with common input and true directed edge): $T = 1024$, $n = 3$, $\\mathbf{A}_1$ and $\\mathbf{A}_2$ given by\n$$\n\\mathbf{A}_1 = \\begin{bmatrix}\n0.2  0.0  0.3 \\\\\n0.5  0.2  0.4 \\\\\n0.0  0.0  0.3\n\\end{bmatrix}, \\quad\n\\mathbf{A}_2 = \\begin{bmatrix}\n0.0  0.0  0.0 \\\\\n0.0  0.0  0.0 \\\\\n0.0  0.0  0.0\n\\end{bmatrix},\n$$\nwith $\\boldsymbol{\\Sigma} = \\sigma^2 \\mathbf{I}$, $\\sigma = 0.5$.\n\n- Test case $2$ (pure common input, no direct edge between $X$ and $Y$): $T = 1024$, $n = 3$,\n$$\n\\mathbf{A}_1 = \\begin{bmatrix}\n0.2  0.0  0.5 \\\\\n0.0  0.2  0.5 \\\\\n0.0  0.0  0.3\n\\end{bmatrix}, \\quad\n\\mathbf{A}_2 = \\mathbf{0}_{3 \\times 3},\n$$\nwith $\\sigma = 0.5$.\n\n- Test case $3$ (weak coupling and low signal-to-noise): $T = 1024$, $n = 3$,\n$$\n\\mathbf{A}_1 = \\begin{bmatrix}\n0.2  0.0  0.0 \\\\\n0.15  0.2  0.0 \\\\\n0.0  0.0  0.3\n\\end{bmatrix}, \\quad\n\\mathbf{A}_2 = \\mathbf{0}_{3 \\times 3},\n$$\nwith $\\sigma = 1.0$.\n\n- Test case $4$ (bidirectional coupling without common input): $T = 1024$, $n = 3$,\n$$\n\\mathbf{A}_1 = \\begin{bmatrix}\n0.2  0.25  0.0 \\\\\n0.35  0.2  0.0 \\\\\n0.0  0.0  0.3\n\\end{bmatrix}, \\quad\n\\mathbf{A}_2 = \\mathbf{0}_{3 \\times 3},\n$$\nwith $\\sigma = 0.5$.\n\n- Test case $5$ (short record length boundary case): $T = 64$, $n = 3$,\n$$\n\\mathbf{A}_1 = \\begin{bmatrix}\n0.1  0.0  0.3 \\\\\n0.3  0.1  0.2 \\\\\n0.0  0.0  0.2\n\\end{bmatrix}, \\quad\n\\mathbf{A}_2 = \\mathbf{0}_{3 \\times 3},\n$$\nwith $\\sigma = 0.5$.\n\nProgram requirements:\n- Implement simulation using the specified $\\mathbf{A}_k$ and $\\boldsymbol{\\Sigma}$ for each case.\n- For each test case, compute the four boolean GC significance indicators: bivariate $X \\to Y$, bivariate $Y \\to X$, conditional $X \\to Y$ including $Z$, conditional $Y \\to X$ including $Z$, each at level $\\alpha = 0.01$.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each test case reported as a list of four booleans in the order specified. For example:\n$[\\,[\\text{true},\\text{false},\\text{true},\\text{false}],\\ldots\\,]$.",
            "solution": "The problem of inferring directed connectivity from multichannel time series is fundamental to understanding complex systems, particularly in neuroscience and bio-hybrid computing where electrode arrays record the activity of neural networks or organoids. The proposed task is to implement Granger causality (GC) analysis, a widely-used statistical method for this purpose, grounded in a Vector Autoregressive (VAR) framework. The problem is scientifically valid, well-posed, and provides a complete specification for a robust computational implementation.\n\nThe core principle of Granger causality is predictive. A time series $x_t$ is said to Granger-cause another time series $y_t$ if past values of $x_t$ contain information that helps predict future values of $y_t$ better than using only past values of $y_t$. This concept is operationalized through a hypothesis test comparing two nested linear regression models.\n\nThe system's dynamics are modeled by a VAR process of order $p$. For an $n$-channel system with time series data $\\mathbf{y}_t \\in \\mathbb{R}^n$, the model is:\n$$\n\\mathbf{y}_t = \\sum_{k=1}^{p} \\mathbf{A}_k \\mathbf{y}_{t-k} + \\mathbf{e}_t\n$$\nHere, $\\mathbf{A}_k \\in \\mathbb{R}^{n \\times n}$ are the coefficient matrices for lag $k$, and $\\mathbf{e}_t$ is a vector of zero-mean Gaussian innovation noise, $\\mathbf{e}_t \\sim \\mathcal{N}(\\mathbf{0}, \\boldsymbol{\\Sigma})$. The off-diagonal elements of $\\mathbf{A}_k$ matrices represent the linear influence of one channel on another at that lag. A non-zero entry $A_k[i, j]$ implies that channel $j$ influences channel $i$ at lag $k$. Granger causality from channel $j$ to channel $i$ exists if at least one coefficient $A_k[i, j]$ for $k=1, \\dots, p$ is non-zero.\n\nThe algorithm proceeds in several distinct steps for each directed pair of channels to be tested (e.g., $X \\to Y$).\n\nFirst, we must simulate the data for each test case. A stable VAR process is generated according to the specified equation. We generate $T + T_{\\text{burn-in}}$ samples and discard the first $T_{\\text{burn-in}}$ to allow the process to reach its stationary distribution, ensuring the statistical properties of the analyzed data are time-invariant. The stability of the process, guaranteed by ensuring the spectral radius of the VAR's companion matrix is less than $1$, is a prerequisite for valid GC analysis.\n\nSecond, for each specific test (e.g., bivariate $X \\to Y$, or conditional $X \\to Y$ given $Z$), we must select an appropriate model order $p$. An order that is too low may fail to capture the true dynamics, while one that is too high may overfit the data. The Bayesian Information Criterion (BIC) provides a principled way to balance model fit with parsimony. For each candidate lag order $p \\in \\{1, 2\\}$, we fit an *unrestricted* linear model predicting the target channel from the past $p$ values of all channels included in the test. The BIC is calculated as:\n$$\n\\text{BIC} = n_{\\text{eff}} \\cdot \\ln\\left(\\frac{\\text{RSS}}{n_{\\text{eff}}}\\right) + k \\cdot \\ln(n_{\\text{eff}})\n$$\nwhere $n_{\\text{eff}} = T - p$ is the effective number of samples, $\\text{RSS}$ is the residual sum of squares from the Ordinary Least Squares (OLS) fit, and $k$ is the number of parameters in the model ($k = d \\cdot p$ for $d$ included channels). The lag order $p$ that yields the minimum BIC is selected for the hypothesis test. This selection is performed independently for each of the four tests in a case (bivariate $X \\to Y$, bivariate $Y \\to X$, conditional $X \\to Y$, conditional $Y \\to X$), as the set of included variables, and thus the BIC, may differ.\n\nThird, with the selected order $p$, we perform the hypothesis test. To test for $X \\to Y$, we compare two models:\n1.  The **unrestricted model** ($M_u$), which predicts $Y_t$ using $p$ lags of all included channels (e.g., $X$ and $Y$ for bivariate; $X, Y, Z$ for conditional). This model has $k_u = d \\cdot p$ parameters and yields a residual sum of squares $\\text{RSS}_u$.\n2.  The **restricted model** ($M_r$), which predicts $Y_t$ using $p$ lags of all included channels *except* for the source channel $X$. This model has $k_r = (d-1) \\cdot p$ parameters and yields $\\text{RSS}_r$.\n\nThe null hypothesis ($H_0$) is that $X$ does not Granger-cause $Y$, which corresponds to the $p$ coefficients for the lags of $X$ being jointly zero. If $H_0$ is true, there should be no statistically significant increase in the RSS when these predictors are removed, i.e., $\\text{RSS}_r$ should not be significantly larger than $\\text{RSS}_u$. We quantify this with the $F$-statistic:\n$$\nF = \\frac{\\left(\\text{RSS}_r - \\text{RSS}_u\\right) / m}{\\text{RSS}_u / (n_{\\text{eff}} - k_u)}\n$$\nwhere $m = p$ is the number of restrictions (the number of omitted parameters). Under $H_0$, this statistic follows an $F$-distribution with $(m, n_{\\text{eff}} - k_u)$ degrees of freedom. We compute the $p$-value from this distribution. If the $p$-value is less than the significance level $\\alpha$ (and $\\text{RSS}_r > \\text{RSS}_u$), we reject $H_0$ and conclude that significant Granger causality exists.\n\nA critical aspect of causal inference is addressing confounding variables. In the context of our system, a common input $Z$ driving both $X$ and $Y$ can induce a spurious correlation, which a bivariate GC test (considering only $X$ and $Y$) might falsely interpret as a direct causal link. **Conditional Granger causality** addresses this by including the potential confounder $Z$ in both the unrestricted and restricted models. This way, we test for the predictive power of $X$'s past *after* accounting for the predictive information from both $Y$'s own past and $Z$'s past. If channel $Z$ is indeed the only common driver and is successfully recorded and included in the model, conditional GC can correctly distinguish between direct causality ($X \\to Y$) and spurious correlation due to the common driver. This highlights a key condition for causal identifiability: all significant common drivers (i.e., confounders that create \"backdoor paths\") must be observed and conditioned upon.\n\nThe provided implementation follows these principles. A function `simulate_var` generates the time series. A helper function `prepare_regression_mats` constructs the necessary data matrices for the OLS models. The central function, `granger_causality_test`, orchestrates the BIC-based model selection and the subsequent F-test to deliver a boolean significance result. The main `solve` function iterates through the specified test cases, applies the GC tests, and formats the results as required.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import f as f_dist\n\ndef solve():\n    \"\"\"\n    Main function to run the Granger causality analysis on all test cases.\n    \"\"\"\n    # Test cases defined in the problem statement\n    test_cases = [\n        {\n            \"T\": 1024, \"n\": 3, \"sigma\": 0.5,\n            \"A_mats\": [\n                np.array([[0.2, 0.0, 0.3], [0.5, 0.2, 0.4], [0.0, 0.0, 0.3]]),\n                np.array([[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]])\n            ]\n        },\n        {\n            \"T\": 1024, \"n\": 3, \"sigma\": 0.5,\n            \"A_mats\": [\n                np.array([[0.2, 0.0, 0.5], [0.0, 0.2, 0.5], [0.0, 0.0, 0.3]]),\n                np.array([[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]])\n            ]\n        },\n        {\n            \"T\": 1024, \"n\": 3, \"sigma\": 1.0,\n            \"A_mats\": [\n                np.array([[0.2, 0.0, 0.0], [0.15, 0.2, 0.0], [0.0, 0.0, 0.3]]),\n                np.array([[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]])\n            ]\n        },\n        {\n            \"T\": 1024, \"n\": 3, \"sigma\": 0.5,\n            \"A_mats\": [\n                np.array([[0.2, 0.25, 0.0], [0.35, 0.2, 0.0], [0.0, 0.0, 0.3]]),\n                np.array([[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]])\n            ]\n        },\n        {\n            \"T\": 64, \"n\": 3, \"sigma\": 0.5,\n            \"A_mats\": [\n                np.array([[0.1, 0.0, 0.3], [0.3, 0.1, 0.2], [0.0, 0.0, 0.2]]),\n                np.array([[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]])\n            ]\n        }\n    ]\n\n    alpha = 0.01\n    candidate_lags = [1, 2]\n    np.random.seed(0)\n    \n    all_results = []\n    \n    for case_params in test_cases:\n        data = simulate_var(\n            T=case_params[\"T\"],\n            n=case_params[\"n\"],\n            A_mats=case_params[\"A_mats\"],\n            sigma=case_params[\"sigma\"]\n        )\n        \n        # Channel indices\n        X, Y, Z = 0, 1, 2\n        \n        case_results = []\n        \n        # Bivariate X -> Y\n        res_bv_xy = granger_causality_test(data, X, Y, [], candidate_lags, alpha)\n        case_results.append(res_bv_xy)\n        \n        # Bivariate Y -> X\n        res_bv_yx = granger_causality_test(data, Y, X, [], candidate_lags, alpha)\n        case_results.append(res_bv_yx)\n        \n        # Conditional X -> Y | Z\n        res_cond_xy = granger_causality_test(data, X, Y, [Z], candidate_lags, alpha)\n        case_results.append(res_cond_xy)\n        \n        # Conditional Y -> X | Z\n        res_cond_yx = granger_causality_test(data, Y, X, [Z], candidate_lags, alpha)\n        case_results.append(res_cond_yx)\n        \n        all_results.append(case_results)\n\n    # Format the final output string\n    str_results = [f\"[{','.join(str(b).lower() for b in res)}]\" for res in all_results]\n    final_output = f\"[{','.join(str_results)}]\"\n    print(final_output)\n\ndef simulate_var(T, n, A_mats, sigma):\n    \"\"\"\n    Simulates a stable Vector Autoregressive (VAR) process.\n    \"\"\"\n    burn_in = 100\n    p_gen = len(A_mats)\n    total_T = T + burn_in\n    \n    Y = np.zeros((total_T, n))\n    noise = np.random.randn(total_T, n) * sigma\n    \n    for t in range(p_gen, total_T):\n        y_t = np.zeros(n)\n        for k in range(p_gen):\n            y_t += A_mats[k] @ Y[t - k - 1, :]\n        Y[t, :] = y_t + noise[t, :]\n        \n    return Y[burn_in:, :]\n\ndef prepare_regression_mats(data, p, regressor_indices, target_idx):\n    \"\"\"\n    Constructs the response vector (y_vec) and predictor matrix (X_mat) for OLS.\n    \"\"\"\n    T, _ = data.shape\n    n_eff = T - p\n    \n    y_vec = data[p:, target_idx]\n    \n    num_regressors = len(regressor_indices)\n    X_mat = np.zeros((n_eff, num_regressors * p))\n    \n    col_idx = 0\n    for idx in regressor_indices:\n        for lag in range(1, p + 1):\n            X_mat[:, col_idx] = data[p - lag : T - lag, idx]\n            col_idx += 1\n            \n    return y_vec, X_mat\n\ndef granger_causality_test(data, source_idx, target_idx, conditioning_indices, candidate_lags, alpha):\n    \"\"\"\n    Performs a Granger causality test from a source to a target channel.\n    \"\"\"\n    T = data.shape[0]\n    all_indices = sorted(list(set([source_idx, target_idx] + conditioning_indices)))\n    \n    # 1. Model order selection using BIC on the unrestricted model\n    best_p = -1\n    min_bic = np.inf\n\n    for p in candidate_lags:\n        if T = p: continue\n        \n        y_vec, X_u = prepare_regression_mats(data, p, all_indices, target_idx)\n        n_eff = len(y_vec)\n        k_u = X_u.shape[1]\n\n        if k_u = n_eff: continue\n\n        # lstsq returns sum of squared residuals in the second element of the tuple\n        residuals = np.linalg.lstsq(X_u, y_vec, rcond=None)[1]\n        \n        if not residuals: # Perfect fit, RSS is 0\n            rss_u = 1e-10 # Use a small number to avoid log(0)\n        else:\n            rss_u = residuals[0]\n\n        bic = n_eff * np.log(rss_u / n_eff) + k_u * np.log(n_eff)\n        \n        if bic  min_bic:\n            min_bic = bic\n            best_p = p\n\n    if best_p == -1:\n        return False # Not enough data to select a model\n\n    # 2. Perform F-test with the selected lag order 'best_p'\n    p = best_p\n    \n    # Unrestricted model\n    y_vec, X_u = prepare_regression_mats(data, p, all_indices, target_idx)\n    \n    # Restricted model (omitting the source channel's lags)\n    restricted_indices = [idx for idx in all_indices if idx != source_idx]\n    _, X_r = prepare_regression_mats(data, p, restricted_indices, target_idx)\n    \n    n_eff = len(y_vec)\n    k_u = X_u.shape[1]\n\n    # Check if the unrestricted model is solvable\n    if k_u = n_eff:\n        return False\n\n    rss_u_val = np.linalg.lstsq(X_u, y_vec, rcond=None)[1]\n    rss_r_val = np.linalg.lstsq(X_r, y_vec, rcond=None)[1]\n    \n    rss_u = rss_u_val[0] if rss_u_val else 0.0\n    rss_r = rss_r_val[0] if rss_r_val else 0.0\n\n    if rss_r = rss_u or rss_u == 0:\n        # If restriction doesn't worsen the fit or the fit is already perfect, H0 is not rejected.\n        return False\n        \n    m = p  # number of restrictions\n    df1 = m\n    df2 = n_eff - k_u\n\n    if df2 = 0:\n        return False # Cannot compute the F-statistic\n\n    F_stat = ((rss_r - rss_u) / df1) / (rss_u / df2)\n    p_value = f_dist.sf(F_stat, df1, df2)\n\n    return p_value  alpha\n\nif __name__ == '__main__':\n    solve()\n```"
        }
    ]
}