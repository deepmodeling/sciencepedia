{
    "hands_on_practices": [
        {
            "introduction": "The very existence of precisely timed neural codes depends on the brain's ability to manage and compensate for conduction delays. This first practice invites you to explore a fundamental biophysical mechanism for achieving synchrony at the postsynaptic neuron. By treating dendritic pathways as adjustable delay lines, you can calculate whether it's possible to compensate for the variability in axonal path lengths, thus ensuring that inputs arrive together in time. This exercise  provides a quantitative foundation for understanding how the morphology of a neuron directly supports temporal computation.",
            "id": "4062855",
            "problem": "Consider a single layer in a synfire chain in which two presynaptic neurons project to a common postsynaptic neuron. Let the axonal path lengths from the presynaptic neurons to the synaptic contact sites be $ \\ell_{a,1} $ and $ \\ell_{a,2} $, respectively, and let the axonal conduction velocity be $ v_{a} $. After a spike arrives at the synapse, it must propagate from the synaptic contact site to the soma along a dendritic pathway whose effective conduction velocity is $ v_{d} $. Assume the postsynaptic spike initiation is sensitive to the arrival-time difference of the two inputs, and synchrony is functionally defined as an absolute arrival-time difference less than a tolerance $ \\varepsilon_{\\mathrm{syn}} $.\n\nYou may exploit dendritic routing to compensate axonal delay differences by choosing dendritic path lengths $ \\ell_{d,1} $ and $ \\ell_{d,2} $ from each synapse to the soma, subject to a morphological bound $ 0 \\leq \\ell_{d,k} \\leq L_{\\max} $ for $ k \\in \\{1,2\\} $.\n\nUse only the following foundations:\n- Conduction delay is distance divided by velocity.\n- The alignment error is the absolute difference in total conduction times from each presynaptic neuron to the soma.\n- Synchrony is restored if the alignment error does not exceed the tolerance.\n\nStarting from these principles, derive the minimal possible alignment error under the dendritic length constraint, and evaluate the synchrony margin defined as $ S = \\varepsilon_{\\mathrm{syn}} - E_{\\min} $, where $ E_{\\min} $ is the minimal achievable alignment error.\n\nGiven the measured and morphologically plausible parameters\n- $ v_{a} = 2.0 \\,\\mathrm{m/s} $,\n- $ v_{d} = 0.35 \\,\\mathrm{m/s} $,\n- $ \\ell_{a,1} = 0.062 \\,\\mathrm{m} $,\n- $ \\ell_{a,2} = 0.059 \\,\\mathrm{m} $,\n- $ L_{\\max} = 7.0 \\times 10^{-4} \\,\\mathrm{m} $,\n- $ \\varepsilon_{\\mathrm{syn}} = 5.0 \\times 10^{-4} \\,\\mathrm{s} $,\n\ncompute the synchrony margin $ S $ after optimally compensating axonal delay differences with dendritic routing. Round your final answer to three significant figures and express it in milliseconds. The final answer must be a single real number.",
            "solution": "The problem asks for the minimal alignment error achievable through dendritic routing and the subsequent synchrony margin. We begin by formalizing the given principles.\n\nLet the two presynaptic neurons be indexed by $k \\in \\{1, 2\\}$. The total conduction time, $T_k$, for a spike from presynaptic neuron $k$ to reach the postsynaptic soma is the sum of its axonal conduction delay and its dendritic conduction delay.\nThe axonal delay is $\\tau_{a,k} = \\frac{\\ell_{a,k}}{v_a}$, where $\\ell_{a,k}$ is the axonal path length and $v_a$ is the axonal conduction velocity.\nThe dendritic delay is $\\tau_{d,k} = \\frac{\\ell_{d,k}}{v_d}$, where $\\ell_{d,k}$ is the dendritic path length and $v_d$ is the dendritic conduction velocity.\nThe total time for path $k$ is therefore:\n$$T_k = \\tau_{a,k} + \\tau_{d,k} = \\frac{\\ell_{a,k}}{v_a} + \\frac{\\ell_{d,k}}{v_d}$$\nThe alignment error, $E$, is defined as the absolute difference in the total conduction times of the two paths:\n$$E = |T_1 - T_2| = \\left| \\left( \\frac{\\ell_{a,1}}{v_a} + \\frac{\\ell_{d,1}}{v_d} \\right) - \\left( \\frac{\\ell_{a,2}}{v_a} + \\frac{\\ell_{d,2}}{v_d} \\right) \\right|$$\nWe can rearrange this expression by grouping the axonal and dendritic contributions:\n$$E = \\left| \\left( \\frac{\\ell_{a,1}}{v_a} - \\frac{\\ell_{a,2}}{v_a} \\right) + \\left( \\frac{\\ell_{d,1}}{v_d} - \\frac{\\ell_{d,2}}{v_d} \\right) \\right|$$\nLet $\\Delta\\tau_a = \\frac{\\ell_{a,1} - \\ell_{a,2}}{v_a}$ be the fixed axonal delay difference, which is determined by the given anatomical parameters.\nLet $\\Delta\\tau_d = \\frac{\\ell_{d,1} - \\ell_{d,2}}{v_d}$ be the adjustable dendritic delay difference, which can be controlled by choosing the dendritic path lengths $\\ell_{d,1}$ and $\\ell_{d,2}$.\nThe alignment error can now be written as:\n$$E = |\\Delta\\tau_a + \\Delta\\tau_d|$$\nOur goal is to minimize $E$ by choosing appropriate values for $\\ell_{d,1}$ and $\\ell_{d,2}$. These choices are subject to the morphological bound $0 \\leq \\ell_{d,k} \\leq L_{\\max}$ for $k \\in \\{1, 2\\}$. This constraint limits the possible range of the dendritic length difference, $\\ell_{d,1} - \\ell_{d,2}$.\nThe maximum value of $\\ell_{d,1} - \\ell_{d,2}$ is $L_{\\max} - 0 = L_{\\max}$ (when $\\ell_{d,1} = L_{\\max}$ and $\\ell_{d,2} = 0$).\nThe minimum value is $0 - L_{\\max} = -L_{\\max}$ (when $\\ell_{d,1} = 0$ and $\\ell_{d,2} = L_{\\max}$).\nThus, the achievable range for the dendritic length difference is $-L_{\\max} \\leq \\ell_{d,1} - \\ell_{d,2} \\leq L_{\\max}$.\nThis, in turn, constrains the range of the adjustable dendritic delay difference $\\Delta\\tau_d$:\n$$\\frac{-L_{\\max}}{v_d} \\leq \\Delta\\tau_d \\leq \\frac{L_{\\max}}{v_d}$$\nLet us define the maximum dendritic compensation capacity as $\\tau_{d, \\text{comp}} = \\frac{L_{\\max}}{v_d}$. The achievable range for $\\Delta\\tau_d$ is therefore $[-\\tau_{d, \\text{comp}}, \\tau_{d, \\text{comp}}]$.\nTo minimize $E = |\\Delta\\tau_a + \\Delta\\tau_d|$, we need to choose a $\\Delta\\tau_d$ from its allowed interval that makes the sum $\\Delta\\tau_a + \\Delta\\tau_d$ as close to zero as possible. This is equivalent to choosing $\\Delta\\tau_d$ to be as close as possible to $-\\Delta\\tau_a$.\nThere are two cases:\n$1$. If the required compensation $-\\Delta\\tau_a$ lies within the achievable range for $\\Delta\\tau_d$, i.e., $|-\\Delta\\tau_a| \\leq \\tau_{d, \\text{comp}}$, or simply $|\\Delta\\tau_a| \\leq \\tau_{d, \\text{comp}}$, then we can choose $\\ell_{d,1}$ and $\\ell_{d,2}$ such that $\\Delta\\tau_d = -\\Delta\\tau_a$. This perfectly cancels the axonal delay difference, resulting in a minimal alignment error of $E_{\\min} = 0$.\n$2$. If the required compensation is outside the achievable range, i.e., $|\\Delta\\tau_a| > \\tau_{d, \\text{comp}}$, then the best we can do is to choose $\\Delta\\tau_d$ to be the endpoint of the interval $[-\\tau_{d, \\text{comp}}, \\tau_{d, \\text{comp}}]$ that is closest to $-\\Delta\\tau_a$. The magnitude of the residual error will be the difference between the magnitude of the required compensation and the maximum possible compensation: $E_{\\min} = |\\Delta\\tau_a| - \\tau_{d, \\text{comp}}$.\nThese two cases can be summarized by the expression:\n$$E_{\\min} = \\max(0, |\\Delta\\tau_a| - \\tau_{d, \\text{comp}})$$\nWe now substitute the given numerical parameters:\n$v_a = 2.0 \\,\\mathrm{m/s}$, $v_d = 0.35 \\,\\mathrm{m/s}$, $\\ell_{a,1} = 0.062 \\,\\mathrm{m}$, $\\ell_{a,2} = 0.059 \\,\\mathrm{m}$, $L_{\\max} = 7.0 \\times 10^{-4} \\,\\mathrm{m}$, and $\\varepsilon_{\\mathrm{syn}} = 5.0 \\times 10^{-4} \\,\\mathrm{s}$.\n\nFirst, we compute the fixed axonal delay difference $\\Delta\\tau_a$:\n$$\\Delta\\tau_a = \\frac{\\ell_{a,1} - \\ell_{a,2}}{v_a} = \\frac{0.062\\,\\mathrm{m} - 0.059\\,\\mathrm{m}}{2.0\\,\\mathrm{m/s}} = \\frac{0.003\\,\\mathrm{m}}{2.0\\,\\mathrm{m/s}} = 0.0015\\,\\mathrm{s} = 1.5 \\times 10^{-3}\\,\\mathrm{s}$$\nNext, we compute the maximum dendritic compensation capacity $\\tau_{d, \\text{comp}}$:\n$$\\tau_{d, \\text{comp}} = \\frac{L_{\\max}}{v_d} = \\frac{7.0 \\times 10^{-4}\\,\\mathrm{m}}{0.35\\,\\mathrm{m/s}} = 2.0 \\times 10^{-3}\\,\\mathrm{s}$$\nNow we compare $|\\Delta\\tau_a|$ with $\\tau_{d, \\text{comp}}$:\n$$|\\Delta\\tau_a| = 1.5 \\times 10^{-3}\\,\\mathrm{s}$$\n$$\\tau_{d, \\text{comp}} = 2.0 \\times 10^{-3}\\,\\mathrm{s}$$\nSince $1.5 \\times 10^{-3}\\,\\mathrm{s} \\leq 2.0 \\times 10^{-3}\\,\\mathrm{s}$, we find that $|\\Delta\\tau_a| \\leq \\tau_{d, \\text{comp}}$. This corresponds to the first case, where the axonal delay difference can be fully compensated by dendritic routing.\nTherefore, the minimal achievable alignment error is:\n$$E_{\\min} = 0\\,\\mathrm{s}$$\nFinally, we compute the synchrony margin $S = \\varepsilon_{\\mathrm{syn}} - E_{\\min}$:\n$$S = 5.0 \\times 10^{-4}\\,\\mathrm{s} - 0\\,\\mathrm{s} = 5.0 \\times 10^{-4}\\,\\mathrm{s}$$\nThe problem requires the answer to be expressed in milliseconds (ms) and rounded to three significant figures.\n$$S = 5.0 \\times 10^{-4}\\,\\mathrm{s} \\times \\frac{1000\\,\\mathrm{ms}}{1\\,\\mathrm{s}} = 0.5\\,\\mathrm{ms}$$\nRounding to three significant figures gives $0.500\\,\\mathrm{ms}$.",
            "answer": "$$\\boxed{0.500}$$"
        },
        {
            "introduction": "Precise temporal patterns are not just static features; they are dynamically learned and maintained. This practice delves into the crucial role of synaptic plasticity in forming polychronous groups, the computational backbone of many temporal coding theories. You will model the interplay between Spike-Timing-Dependent Plasticity (STDP), which reinforces causal pre-post firing, and heterosynaptic competition, which ensures a stable distribution of synaptic resources. By deriving the equilibrium state of the synaptic weights , you will uncover how local learning rules enable a neuron to selectively strengthen inputs that form a coherent and predictive temporal sequence.",
            "id": "4062867",
            "problem": "Consider a single postsynaptic neuron participating in a synfire chain and receiving excitatory inputs from $M$ presynaptic neurons indexed by $k \\in \\{1,\\dots,M\\}$. In each cycle of a repeated stimulus, presynaptic neuron $k$ emits a spike at a fixed time $s_k$ relative to cycle onset. The conduction from presynaptic neuron $k$ to the postsynaptic neuron has a fixed axonal delay $d_k$. The postsynaptic neuron emits a polychronous spike sequence consisting of $P$ spikes per cycle at times $\\{t_n\\}_{n=1}^{P}$, with these postsynaptic spike times exogenously determined by upstream network dynamics and independent of synaptic weights in the slow-learning limit. All synaptic weights are nonnegative and are updated after each cycle by spike-timing-dependent plasticity (STDP), followed immediately by heterosynaptic competition implemented through weight normalization.\n\nThe spike-timing-dependent plasticity rule is defined by a standard pair-based kernel: for each presynaptic spike time $s_k$ and postsynaptic spike time $t_n$, the time difference is $\\Delta t_{k,n} = t_n - (s_k + d_k)$. The STDP contribution for synapse $k$ from this pre-post pair is given by\n$$\nW(\\Delta t_{k,n}) =\n\\begin{cases}\nA_{+} \\exp\\!\\left(-\\frac{\\Delta t_{k,n}}{\\tau_{+}}\\right), & \\Delta t_{k,n} > 0, \\\\\n- A_{-} \\exp\\!\\left(\\frac{\\Delta t_{k,n}}{\\tau_{-}}\\right), & \\Delta t_{k,n} < 0, \\\\\n0, & \\Delta t_{k,n} = 0,\n\\end{cases}\n$$\nwhere $A_{+} > 0$, $A_{-} > 0$, $\\tau_{+} > 0$, and $\\tau_{-} > 0$ are constants. Let the per-cycle additive update for synapse $k$ be $\\Delta w_k = \\eta \\sum_{n=1}^{P} W(\\Delta t_{k,n})$, with learning rate $\\eta > 0$ sufficiently small to justify the slow-learning, time-scale separation approximation. After applying the additive update, enforce heterosynaptic competition via normalization:\n$$\nw_k \\leftarrow \\frac{w_k}{\\sum_{j=1}^{M} w_j},\n$$\nand enforce the nonnegativity constraint $w_k \\ge 0$ for all $k$.\n\nStarting from this setup and these definitions, derive, in closed form, the fixed-point normalized weight vector $w^{\\ast} = (w_1^{\\ast}, \\dots, w_M^{\\ast})$ attained in the slow-learning limit, expressed explicitly in terms of $\\{s_k\\}_{k=1}^{M}$, $\\{d_k\\}_{k=1}^{M}$, $\\{t_n\\}_{n=1}^{P}$, $A_{+}$, $A_{-}$, $\\tau_{+}$, and $\\tau_{-}$. Your final expression must be valid under the stated assumptions and must show how heterosynaptic normalization stabilizes a set of delays that produce the polychronous firing sequence. Express the final vector as a single analytic expression. No numerical evaluation is required.",
            "solution": "### Derivation of the Fixed-Point Solution\n\nThe core of the problem is to find the stable fixed point of a set of synaptic weights under the combined influence of additive STDP and multiplicative normalization.\n\nLet $F_k$ be the total STDP-induced change for synapse $k$ over a single cycle, aggregated over all $P$ postsynaptic spikes. This term represents the \"fitness\" or \"drive\" for synapse $k$.\n$$\nF_k = \\sum_{n=1}^{P} W(\\Delta t_{k,n}) = \\sum_{n=1}^{P} W(t_n - (s_k + d_k))\n$$\nThe problem states a discrete update rule per cycle: an additive update followed by nonnegativity enforcement and normalization. Let $w_k(i)$ be the weight of synapse $k$ after cycle $i$. The update rule can be written as:\n$1$. Tentative update: $\\hat{w}_k = w_k(i) + \\eta F_k$.\n$2$. Nonnegativity: $\\tilde{w}_k = \\max(0, \\hat{w}_k)$.\n$3$. Normalization: $w_k(i+1) = \\frac{\\tilde{w}_k}{\\sum_{j=1}^{M} \\tilde{w}_j}$.\n\nIn the slow-learning limit ($\\eta \\to 0$), the change per cycle is very small. We can analyze the evolution of the weights over a very long time period $T$, corresponding to $N = T/\\Delta T_{\\text{cycle}}$ cycles.\nLet's consider the evolution of an unnormalized weight vector $\\tilde{w}_k$, starting from some initial condition $\\tilde{w}_k(0) > 0$. After $N$ cycles, the unnormalized weight, without considering the nonnegativity constraint for a moment, would be approximately:\n$$\n\\tilde{w}_k(N) \\approx \\tilde{w}_k(0) + N \\eta F_k\n$$\nThe term $N\\eta$ represents the total learning over the period. The nonnegativity constraint implies that if a weight hits zero, it can only increase if its driving term $F_k$ is positive. Over a long time, a synapse with a consistently negative drive ($F_k < 0$) will have its weight driven to zero and held there. A synapse with a positive drive ($F_k > 0$) will tend to be potentiated.\nTherefore, the long-term behavior of the unnormalized weight, including the nonnegativity constraint, is:\n$$\n\\tilde{w}_k(N) \\approx \\max(0, \\tilde{w}_k(0) + N \\eta F_k)\n$$\nThe fixed-point normalized weights $w_k^{\\ast}$ are the values obtained after this dynamic has run for a very long time, i.e., in the limit $N \\to \\infty$.\n$$\nw_k^{\\ast} = \\lim_{N\\to\\infty} \\frac{\\tilde{w}_k(N)}{\\sum_{j=1}^{M} \\tilde{w}_j(N)} = \\lim_{N\\to\\infty} \\frac{\\max(0, \\tilde{w}_k(0) + N \\eta F_k)}{\\sum_{j=1}^{M} \\max(0, \\tilde{w}_j(0) + N \\eta F_j)}\n$$\nIn this limit, the term $N \\eta F_k$ dominates the initial weight $\\tilde{w}_k(0)$. We can factor out the growing term $N\\eta$:\n$$\nw_k^{\\ast} = \\lim_{N\\to\\infty} \\frac{\\max(0, \\frac{\\tilde{w}_k(0)}{N\\eta} + F_k)}{\\sum_{j=1}^{M} \\max(0, \\frac{\\tilde{w}_j(0)}{N\\eta} + F_j)}\n$$\nAs $N \\to \\infty$, the term $\\frac{\\tilde{w}_k(0)}{N\\eta} \\to 0$. The expression simplifies to:\n$$\nw_k^{\\ast} = \\frac{\\max(0, F_k)}{\\sum_{j=1}^{M} \\max(0, F_j)}\n$$\nThis is the fixed-point solution. It indicates that synapses with a net positive STDP drive ($F_k > 0$) survive and share the total synaptic weight in proportion to their respective drives. Synapses with a net non-positive drive ($F_k \\le 0$) are pruned away, having their weights set to zero. This is a classic result of competitive learning with additive updates and multiplicative normalization.\n\nNow, we substitute the full expression for $F_k$.\n$$\nF_k = \\sum_{\\substack{n=1 \\\\ t_n > s_k+d_k}}^{P} A_{+} \\exp\\left(-\\frac{t_n - s_k - d_k}{\\tau_{+}}\\right) - \\sum_{\\substack{n=1 \\\\ t_n < s_k+d_k}}^{P} A_{-} \\exp\\left(\\frac{t_n - s_k - d_k}{\\tau_{-}}\\right)\n$$\nThe term for $t_n = s_k+d_k$ is zero as per the definition of $W(\\cdot)$, so it does not contribute to either sum. Combining these results gives the final expression for the $k$-th component of the fixed-point weight vector $w^{\\ast}$. This expression shows that the learning rule selectively strengthens synapses whose delayed spike arrivals are predictive of the postsynaptic firing pattern, which is the mechanism underlying the formation of polychronous groups.",
            "answer": "$$\n\\boxed{\\frac{\\max\\left(0, \\sum_{\\substack{n=1 \\\\ t_n > s_k+d_k}}^{P} A_{+} \\exp\\left(-\\frac{t_n - s_k - d_k}{\\tau_{+}}\\right) - \\sum_{\\substack{n=1 \\\\ t_n < s_k+d_k}}^{P} A_{-} \\exp\\left(\\frac{t_n - s_k - d_k}{\\tau_{-}}\\right)\\right)}{\\sum_{j=1}^{M} \\max\\left(0, \\sum_{\\substack{n=1 \\\\ t_n > s_j+d_j}}^{P} A_{+} \\exp\\left(-\\frac{t_n - s_j - d_j}{\\tau_{+}}\\right) - \\sum_{\\substack{n=1 \\\\ t_n < s_j+d_j}}^{P} A_{-} \\exp\\left(\\frac{t_n - s_j - d_j}{\\tau_{-}}\\right)\\right)}}\n$$"
        },
        {
            "introduction": "Theories of polychronization are compelling, but their validation requires robust methods for detecting these complex patterns in noisy spike train data. This final practice moves from analytical derivation to algorithmic implementation and statistical analysis . You will develop a procedure to search for a specific, predefined temporal pattern within simulated spike recordings and then evaluate its statistical significance. This exercise provides critical, hands-on experience in the data-analytic techniques required to bridge the gap between theoretical models and experimental observation in neuroscience.",
            "id": "4062902",
            "problem": "Consider a candidate polychronous group defined on a finite set of neurons. Let there be $M$ labeled neurons indexed by $i \\in \\{1,\\dots,M\\}$, and let their expected relative spike times be $\\{\\tau_i\\}_{i=1}^M$, where each $\\tau_i \\in \\mathbb{R}$ is measured relative to an unknown anchor time $a \\in [0,L]$ on a trial of duration $L$. For each trial $t \\in \\{1,\\dots,T\\}$, the observed spike times for neuron $i$ are given as a finite set $S_{t,i} \\subset [0,L]$. A pattern is said to be realized within tolerance $\\epsilon > 0$ if there exists an anchor time $a \\in \\mathbb{R}$ such that, for all $i \\in \\{1,\\dots,M\\}$, there exists a spike $s \\in S_{t,i}$ satisfying $|(s - a) - \\tau_i| \\le \\epsilon$, and additionally $a + \\tau_i \\in [0,L]$ for all $i$. Define the reproducibility metric across trials by\n$$\nR \\triangleq \\frac{1}{T} \\sum_{t=1}^{T} \\mathbb{I}\\left[\\text{pattern realized within tolerance } \\epsilon \\text{ on trial } t\\right],\n$$\nwhere $\\mathbb{I}[\\cdot]$ is the indicator function.\n\nUnder a null model in which each neuron $i$ emits spikes as an independent homogeneous Poisson process of rate $\\lambda_i$ (in $\\text{s}^{-1}$) on $[0,L]$, assume anchors are considered on a discrete grid with spacing $\\Delta > 0$ over the allowable anchor interval so that windows $[a+\\tau_i-\\epsilon, a+\\tau_i+\\epsilon]$ lie within $[0,L]$. Let $W \\triangleq \\max_i \\tau_i - \\min_i \\tau_i$, and define\n$$\nK \\triangleq \\left\\lfloor \\frac{L - W}{\\Delta} \\right\\rfloor + 1 \\quad \\text{if } L \\ge W, \\quad \\text{and } K \\triangleq 0 \\text{ otherwise.}\n$$\nFor each neuron $i$, the probability of at least one spike falling within a window of width $2\\epsilon$ is $q_i \\triangleq 1 - e^{-\\lambda_i (2\\epsilon)}$. Using independence across neurons and a union bound across anchors, an upper bound on the per-trial null probability of a pattern realization is\n$$\np_0^{\\text{upper}} \\triangleq \\min\\left(1, \\; K \\cdot \\prod_{i=1}^M q_i \\right).\n$$\nGiven an observed number of realizations $K_{\\text{obs}} \\in \\{0,1,\\dots,T\\}$ (so that $R_{\\text{obs}} = K_{\\text{obs}}/T$), a tail bound on the $p$-value under the null for observing at least $K_{\\text{obs}}$ realizations is provided by the Hoeffding inequality:\n$$\np_{\\text{Hoeff}}^{\\text{upper}} \\triangleq\n\\begin{cases}\n\\exp\\left(-2 T \\left(R_{\\text{obs}} - p_0^{\\text{upper}}\\right)^2\\right), & \\text{if } R_{\\text{obs}} > p_0^{\\text{upper}},\\\\\n1, & \\text{otherwise.}\n\\end{cases}\n$$\n\nYour task is to:\n- Implement an algorithm to test, for each trial $t$, whether the pattern is realized within tolerance $\\epsilon$. The algorithm must be grounded in the formal definition above and must not assume any special structure beyond the given sets. It should search over candidate anchor times generated from observed spikes by $a = s - \\tau_i$ and verify the realization condition.\n- Compute $R_{\\text{obs}}$, the upper bound $p_0^{\\text{upper}}$, and the Hoeffding upper bound $p_{\\text{Hoeff}}^{\\text{upper}}$ for each provided test case.\n\nAll times must be treated in seconds, and all rates in inverse seconds. The final outputs are dimensionless real numbers.\n\nTest Suite:\nFor each test case, you are given $L$, $T$, $M$, $\\epsilon$, $\\Delta$, the vector $(\\tau_1,\\dots,\\tau_M)$, the rate vector $(\\lambda_1,\\dots,\\lambda_M)$, and the observed spike sets $\\{S_{t,i}\\}$.\n\nCase $1$ (general case):\n- $L = 0.1$.\n- $T = 3$.\n- $M = 3$.\n- $\\epsilon = 0.001$.\n- $\\Delta = 0.001$.\n- $(\\tau_1,\\tau_2,\\tau_3) = (0.010, 0.015, 0.026)$.\n- $(\\lambda_1,\\lambda_2,\\lambda_3) = (20.0, 30.0, 25.0)$.\n- Observed spikes per trial:\n  - Trial $1$: $S_{1,1} = \\{0.012, 0.060, 0.090\\}$, $S_{1,2} = \\{0.065, 0.070\\}$, $S_{1,3} = \\{0.076, 0.081\\}$.\n  - Trial $2$: $S_{2,1} = \\{0.050, 0.061\\}$, $S_{2,2} = \\{0.066\\}$, $S_{2,3} = \\{0.077, 0.084\\}$.\n  - Trial $3$: $S_{3,1} = \\{0.030, 0.080\\}$, $S_{3,2} = \\{0.063\\}$, $S_{3,3} = \\{0.079\\}$.\n\nCase $2$ (boundary anchor and tighter tolerance):\n- $L = 0.05$.\n- $T = 4$.\n- $M = 4$.\n- $\\epsilon = 0.0005$.\n- $\\Delta = 0.0005$.\n- $(\\tau_1,\\tau_2,\\tau_3,\\tau_4) = (0.000, 0.007, 0.012, 0.020)$.\n- $(\\lambda_1,\\lambda_2,\\lambda_3,\\lambda_4) = (10.0, 12.0, 15.0, 8.0)$.\n- Observed spikes per trial:\n  - Trial $1$: $S_{1,1} = \\{0.0002, 0.030\\}$, $S_{1,2} = \\{0.0072\\}$, $S_{1,3} = \\{0.0126\\}$, $S_{1,4} = \\{0.0206\\}$.\n  - Trial $2$: $S_{2,1} = \\{0.010\\}$, $S_{2,2} = \\{0.016\\}$, $S_{2,3} = \\{0.022\\}$, $S_{2,4} = \\{0.030\\}$.\n  - Trial $3$: $S_{3,1} = \\{0.0053\\}$, $S_{3,2} = \\{0.0124\\}$, $S_{3,3} = \\{0.0170\\}$, $S_{3,4} = \\{0.0252\\}$.\n  - Trial $4$: $S_{4,1} = \\{0.002\\}$, $S_{4,2} = \\{0.009\\}$, $S_{4,3} = \\{0.011\\}$, $S_{4,4} = \\{0.018\\}$.\n\nCase $3$ (no realization with very small tolerance):\n- $L = 0.2$.\n- $T = 2$.\n- $M = 5$.\n- $\\epsilon = 0.0002$.\n- $\\Delta = 0.0002$.\n- $(\\tau_1,\\tau_2,\\tau_3,\\tau_4,\\tau_5) = (0.005, 0.013, 0.021, 0.034, 0.055)$.\n- $(\\lambda_1,\\lambda_2,\\lambda_3,\\lambda_4,\\lambda_5) = (40.0, 40.0, 40.0, 40.0, 40.0)$.\n- Observed spikes per trial:\n  - Trial $1$: $S_{1,1} = \\{0.010\\}$, $S_{1,2} = \\{0.020\\}$, $S_{1,3} = \\{0.030\\}$, $S_{1,4} = \\{0.040\\}$, $S_{1,5} = \\{0.060\\}$.\n  - Trial $2$: $S_{2,1} = \\{0.006\\}$, $S_{2,2} = \\{0.025\\}$, $S_{2,3} = \\{0.033\\}$, $S_{2,4} = \\{0.050\\}$, $S_{2,5} = \\{0.070\\}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each test case, output the triple $[R_{\\text{obs}}, p_0^{\\text{upper}}, p_{\\text{Hoeff}}^{\\text{upper}}]$ flattened into a single list across cases, i.e., $[R_1, p_{0,1}^{\\text{upper}}, p_{\\text{Hoeff},1}^{\\text{upper}}, R_2, p_{0,2}^{\\text{upper}}, p_{\\text{Hoeff},2}^{\\text{upper}}, R_3, p_{0,3}^{\\text{upper}}, p_{\\text{Hoeff},3}^{\\text{upper}}]$. All numeric outputs must be real numbers.",
            "solution": "The problem requires the evaluation of the reproducibility of a specific spatiotemporal spike pattern, known as a polychronous group, across multiple trials and an assessment of its statistical significance against a null hypothesis of random, independent spiking. The solution is structured in two main parts: first, an algorithmic procedure to determine if the pattern is realized on any given trial, and second, the calculation of statistical metrics based on the trial outcomes.\n\n### Part 1: Algorithmic Detection of Pattern Realization\n\nThe core of the problem is to determine, for each trial $t \\in \\{1,\\dots,T\\}$, whether the specified pattern is realized. A pattern is defined by a set of $M$ neurons and their expected relative spike times $\\{\\tau_i\\}_{i=1}^M$. A realization occurs if there exists an `anchor time` $a$ such that for every neuron $i$, an observed spike $s \\in S_{t,i}$ falls close to the expected time $a + \\tau_i$, and all expected spike times lie within the trial duration $[0,L]$.\n\nFormally, a pattern is realized on trial $t$ if there exists an $a \\in \\mathbb{R}$ satisfying two conditions:\n1.  **Anchor Validity**: For all $i \\in \\{1,\\dots,M\\}$, the expected spike time $a+\\tau_i$ must be within the trial interval, i.e., $a + \\tau_i \\in [0,L]$. This constrains $a$ to the interval $[-\\min_i \\tau_i, L - \\max_i \\tau_i]$.\n2.  **Spike Matching**: For all $i \\in \\{1,\\dots,M\\}$, there must exist an observed spike $s \\in S_{t,i}$ such that $|(s-a) - \\tau_i| \\le \\epsilon$. This is equivalent to finding a spike $s$ in the window $[a+\\tau_i-\\epsilon, a+\\tau_i+\\epsilon]$.\n\nA brute-force search for a suitable $a$ over a continuous range is computationally infeasible. However, the problem specifies a more efficient approach: to search over a finite set of candidate anchor times. If a pattern is realized for some anchor $a$, then for each neuron $i$, there's a spike $s_i \\in S_{t,i}$ such that $s_i \\approx a + \\tau_i$. Rearranging this, we get $a \\approx s_i - \\tau_i$. This insight motivates the search strategy: any valid anchor time must be \"close\" to one of the values derived by shifting an observed spike time $s$ by its corresponding relative delay $-\\tau_i$. We therefore construct a finite set of candidate anchors from the observed data.\n\nThe algorithm for a single trial $t$ proceeds as follows:\n\n1.  **Generate Candidate Anchors**: Construct a set of candidate anchors, $\\mathcal{A}_t$, by pairing every observed spike with its neuron's relative timing.\n    $$\n    \\mathcal{A}_t = \\bigcup_{i=1}^M \\{ s - \\tau_i \\mid s \\in S_{t,i} \\}\n    $$\n    Using a set automatically discards duplicate candidate values.\n\n2.  **Verify Candidate Anchors**: Iterate through each candidate anchor $a \\in \\mathcal{A}_t$. For an anchor to be valid, it must satisfy both the anchor validity and spike matching conditions.\n    a.  **Check Anchor Validity**: First, check if the anchor is plausible by ensuring all expected spike times fall within the trial duration. Let $\\tau_{\\min} = \\min_{i} \\tau_i$ and $\\tau_{\\max} = \\max_{i} \\tau_i$. The condition is $a \\in [-\\tau_{\\min}, L - \\tau_{\\max}]$. If $a$ is outside this range, it is discarded.\n    b.  **Check Spike Matching**: If the anchor is valid, verify if it aligns the full pattern. For this anchor $a$, we must check that for *every* neuron $i \\in \\{1,\\dots,M\\}$, there exists at least one spike $s \\in S_{t,i}$ satisfying $|(s - a) - \\tau_i| \\le \\epsilon$.\n    c.  If an anchor $a$ satisfies both conditions, the pattern is considered realized for trial $t$. The search for this trial can be terminated, and we record a success.\n\n3.  **Determine Trial Outcome**: If the loop over all candidate anchors completes without finding a suitable anchor, the pattern is not realized for trial $t$.\n\nThis procedure is repeated for all $T$ trials. The total number of successful trials is denoted $K_{\\text{obs}}$. The observed reproducibility is then computed as $R_{\\text{obs}} = K_{\\text{obs}}/T$.\n\n### Part 2: Statistical Significance Testing\n\nThe second part of the task is to evaluate the statistical significance of the observed reproducibility $R_{\\text{obs}}$ under a null model. The null model assumes that each neuron $i$ fires as an independent homogeneous Poisson process with a given rate $\\lambda_i$.\n\n1.  **Upper Bound on Null Probability ($p_0^{\\text{upper}}$)**: We first calculate an upper bound on the probability of observing a pattern by chance in a single trial.\n    - The probability that a given neuron $i$ fires at least one spike in a small time window of width $2\\epsilon$ is $q_i = 1 - e^{-2\\epsilon\\lambda_i}$.\n    - For a *single* fixed anchor time $a$, the probability of a chance realization (i.e., every neuron $i$ happens to have a spike in its respective window $[a+\\tau_i-\\epsilon, a+\\tau_i+\\epsilon]$) is $\\prod_{i=1}^M q_i$, due to the independence assumption.\n    - To account for the fact that we can search for the anchor $a$, we use a union bound. The number of effectively independent anchor positions, $K$, is estimated based on the allowed range for $a$ and a discretization step $\\Delta$. Let $W = \\tau_{\\max} - \\tau_{\\min}$ be the temporal span of the pattern. The valid interval for $a$ has length $L-W$. The number of discrete anchor positions is $K = \\lfloor(L-W)/\\Delta\\rfloor + 1$ (if $L \\ge W$, otherwise $K=0$).\n    - The union bound gives an upper limit on the per-trial probability of a chance realization: $p_0^{\\text{upper}} = \\min(1, K \\cdot \\prod_{i=1}^M q_i)$.\n\n2.  **Hoeffding P-Value Bound ($p_{\\text{Hoeff}}^{\\text{upper}}$)**: The number of observed realizations $K_{\\text{obs}}$ over $T$ trials can be modeled as a sum of $T$ Bernoulli random variables, where the success probability for each is bounded by $p_0^{\\text{upper}}$. We want to find the probability of observing at least $K_{\\text{obs}}$ successes by chance. Hoeffding's inequality provides an upper bound on this tail probability (the $p$-value).\n    - If the observed rate $R_{\\text{obs}} = K_{\\text{obs}}/T$ is not greater than the chance probability bound $p_0^{\\text{upper}}$, the observation is not surprising, and the $p$-value is trivially $1$.\n    - If $R_{\\text{obs}} > p_0^{\\text{upper}}$, Hoeffding's inequality gives the bound:\n        $$\n        p_{\\text{Hoeff}}^{\\text{upper}} = \\exp\\left(-2 T (R_{\\text{obs}} - p_0^{\\text{upper}})^2\\right)\n        $$\nThis value represents an upper bound on the probability of observing a reproducibility of $R_{\\text{obs}}$ or higher under the null hypothesis, providing a conservative measure of statistical significance.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to define, solve, and print results for all test cases.\n    \"\"\"\n    \n    test_cases = [\n        {\n            \"L\": 0.1, \"T\": 3, \"M\": 3, \"epsilon\": 0.001, \"Delta\": 0.001,\n            \"tau\": np.array([0.010, 0.015, 0.026]),\n            \"lambdas\": np.array([20.0, 30.0, 25.0]),\n            \"S\": [\n                [[0.012, 0.060, 0.090], [0.065, 0.070], [0.076, 0.081]],  # Trial 1\n                [[0.050, 0.061], [0.066], [0.077, 0.084]],               # Trial 2\n                [[0.030, 0.080], [0.063], [0.079]],                      # Trial 3\n            ],\n        },\n        {\n            \"L\": 0.05, \"T\": 4, \"M\": 4, \"epsilon\": 0.0005, \"Delta\": 0.0005,\n            \"tau\": np.array([0.000, 0.007, 0.012, 0.020]),\n            \"lambdas\": np.array([10.0, 12.0, 15.0, 8.0]),\n            \"S\": [\n                [[0.0002, 0.030], [0.0072], [0.0126], [0.0206]],         # Trial 1\n                [[0.010], [0.016], [0.022], [0.030]],                    # Trial 2\n                [[0.0053], [0.0124], [0.0170], [0.0252]],                # Trial 3\n                [[0.002], [0.009], [0.011], [0.018]],                    # Trial 4\n            ],\n        },\n        {\n            \"L\": 0.2, \"T\": 2, \"M\": 5, \"epsilon\": 0.0002, \"Delta\": 0.0002,\n            \"tau\": np.array([0.005, 0.013, 0.021, 0.034, 0.055]),\n            \"lambdas\": np.array([40.0, 40.0, 40.0, 40.0, 40.0]),\n            \"S\": [\n                [[0.010], [0.020], [0.030], [0.040], [0.060]],           # Trial 1\n                [[0.006], [0.025], [0.033], [0.050], [0.070]],           # Trial 2\n            ],\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        res_tuple = solve_case(\n            L=case[\"L\"], T=case[\"T\"], M=case[\"M\"],\n            epsilon=case[\"epsilon\"], Delta=case[\"Delta\"],\n            tau=case[\"tau\"], lambdas=case[\"lambdas\"], S_all_trials=case[\"S\"]\n        )\n        results.extend(res_tuple)\n        \n    print(f\"[{','.join(map(str, results))}]\")\n\n\ndef _is_pattern_realized_in_trial(L, M, epsilon, tau, S_trial):\n    \"\"\"\n    Checks if the pattern is realized in a single trial.\n    S_trial is a list of spike sets for each neuron for this trial.\n    \"\"\"\n    candidate_anchors = set()\n    for i in range(M):\n        for s in S_trial[i]:\n            candidate_anchors.add(s - tau[i])\n    \n    if not candidate_anchors:\n        return False\n        \n    tau_min = np.min(tau)\n    tau_max = np.max(tau)\n    \n    valid_anchor_min = -tau_min\n    valid_anchor_max = L - tau_max\n\n    for a in candidate_anchors:\n        # 1. Check anchor validity\n        if not (valid_anchor_min <= a <= valid_anchor_max):\n            continue\n\n        # 2. Check spike matching for all neurons for this anchor\n        is_realized_for_a = True\n        for i in range(M):\n            found_spike_for_neuron = False\n            for s in S_trial[i]:\n                if abs((s - a) - tau[i]) <= epsilon:\n                    found_spike_for_neuron = True\n                    break \n            if not found_spike_for_neuron:\n                is_realized_for_a = False\n                break \n        \n        if is_realized_for_a:\n            return True\n\n    return False\n\n\ndef solve_case(L, T, M, epsilon, Delta, tau, lambdas, S_all_trials):\n    \"\"\"\n    Solves a single test case.\n    \"\"\"\n    # Part 1: Compute observed reproducibility R_obs\n    K_obs = 0\n    for t in range(T):\n        S_trial = S_all_trials[t]\n        if _is_pattern_realized_in_trial(L, M, epsilon, tau, S_trial):\n            K_obs += 1\n    \n    R_obs = K_obs / T\n\n    # Part 2: Compute null hypothesis metrics\n    # p0_upper\n    W = np.max(tau) - np.min(tau)\n    \n    if L >= W:\n        K = np.floor((L - W) / Delta) + 1\n    else:\n        K = 0\n    \n    q_i = 1 - np.exp(-lambdas * (2 * epsilon))\n    prod_q_i = np.prod(q_i)\n    \n    p0_upper = min(1.0, K * prod_q_i)\n\n    # p_Hoeff_upper\n    if R_obs > p0_upper:\n        p_Hoeff_upper = np.exp(-2 * T * (R_obs - p0_upper)**2)\n    else:\n        p_Hoeff_upper = 1.0\n\n    return [R_obs, p0_upper, p_Hoeff_upper]\n\nsolve()\n```"
        }
    ]
}