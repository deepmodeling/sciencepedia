{
    "hands_on_practices": [
        {
            "introduction": "To build effective Spiking Convolutional Neural Networks, we must first master the dynamics of a single synapse. This exercise explores the alpha function, a standard model for the postsynaptic current that follows a presynaptic spike. By analyzing its key features, you will gain a concrete understanding of how synaptic parameters like time constants and weights shape the temporal response of a neuron, which is the fundamental building block of computation in SNNs .",
            "id": "4060537",
            "problem": "Consider a single synaptic connection inside a Spiking Convolutional Neural Network (SCNN), where a presynaptic spike at time $t=0$ evokes a postsynaptic current kernel used for time-domain convolution. The postsynaptic current is modeled by an $\\alpha$-function arising from the cascade of $2$ identical first-order synaptic filters with time constant $\\tau0$, and is given by\n$$\n\\alpha(t) \\;=\\; A \\,\\frac{t}{\\tau}\\,\\exp\\!\\left(-\\frac{t}{\\tau}\\right)\\,H(t),\n$$\nwhere $A0$ is an amplitude scaling constant, and $H(t)$ is the Heaviside step function ($H(t)=0$ for $t0$ and $H(t)=1$ for $t\\ge 0$). In this setup, the synaptic efficacy or weight $w$ is operationally defined as the total charge transfer elicited by a single spike, namely $w = \\int_{0}^{\\infty} \\alpha(t)\\,dt$.\n\nStarting from the definitions of peak time and total charge transfer, and using only fundamental calculus and linear time-invariant system principles, do the following:\n- Determine the peak time $t_{\\mathrm{peak}}$ of $\\alpha(t)$ for $t\\ge 0$.\n- Determine the peak amplitude $\\alpha(t_{\\mathrm{peak}})$.\n- Evaluate the integral $\\int_{0}^{\\infty} \\alpha(t)\\,dt$ and use the operational definition $w = \\int_{0}^{\\infty} \\alpha(t)\\,dt$ to express the amplitude scaling constant $A$ in terms of $w$ and $\\tau$, and then express $\\alpha(t_{\\mathrm{peak}})$ completely in terms of $w$ and $\\tau$.\n\nExpress your final answer as a single row matrix containing, in order, the expressions for $t_{\\mathrm{peak}}$, $\\alpha(t_{\\mathrm{peak}})$, and $\\int_{0}^{\\infty} \\alpha(t)\\,dt$. No numerical approximation is required. Do not include units in your final answer.",
            "solution": "The problem statement is evaluated to be valid as it is scientifically grounded in the standard principles of computational neuroscience and linear systems theory, is well-posed with a unique and derivable solution, and is formulated using objective, unambiguous mathematical language. All necessary data and definitions are provided, and there are no internal contradictions.\n\nWe are given the postsynaptic current kernel as the $\\alpha$-function:\n$$\n\\alpha(t) \\;=\\; A \\,\\frac{t}{\\tau}\\,\\exp\\!\\left(-\\frac{t}{\\tau}\\right)\\,H(t)\n$$\nwith parameters $A > 0$ and $\\tau > 0$. For $t \\ge 0$, $H(t)=1$, so the function simplifies to:\n$$\n\\alpha(t) \\;=\\; \\frac{A}{\\tau}\\,t\\,\\exp\\!\\left(-\\frac{t}{\\tau}\\right)\n$$\nThe synaptic weight $w$ is defined as the total charge transfer:\n$$\nw = \\int_{0}^{\\infty} \\alpha(t)\\,dt\n$$\n\nFirst, we determine the peak time $t_{\\mathrm{peak}}$ by finding the maximum of $\\alpha(t)$ for $t \\ge 0$. This occurs where the first derivative of $\\alpha(t)$ with respect to $t$ is zero. We use the product rule for differentiation, $\\frac{d}{dt}(uv) = u'v + uv'$, with $u=t$ and $v=\\exp(-t/\\tau)$.\n\n$$\n\\frac{d\\alpha}{dt} = \\frac{A}{\\tau} \\frac{d}{dt} \\left[ t \\exp\\left(-\\frac{t}{\\tau}\\right) \\right] = \\frac{A}{\\tau} \\left[ \\frac{d(t)}{dt} \\exp\\left(-\\frac{t}{\\tau}\\right) + t \\frac{d}{dt}\\left(\\exp\\left(-\\frac{t}{\\tau}\\right)\\right) \\right]\n$$\n$$\n\\frac{d\\alpha}{dt} = \\frac{A}{\\tau} \\left[ (1) \\exp\\left(-\\frac{t}{\\tau}\\right) + t \\left(-\\frac{1}{\\tau}\\right)\\exp\\left(-\\frac{t}{\\tau}\\right) \\right]\n$$\n$$\n\\frac{d\\alpha}{dt} = \\frac{A}{\\tau} \\exp\\left(-\\frac{t}{\\tau}\\right) \\left( 1 - \\frac{t}{\\tau} \\right)\n$$\nSetting the derivative to $0$ to find the critical point:\n$$\n\\frac{A}{\\tau} \\exp\\left(-\\frac{t}{\\tau}\\right) \\left( 1 - \\frac{t}{\\tau} \\right) = 0\n$$\nSince $A > 0$, $\\tau > 0$, and $\\exp(-t/\\tau)$ is always positive for finite $t$, the equation is satisfied only when the term in the parentheses is zero:\n$$\n1 - \\frac{t}{\\tau} = 0 \\quad \\implies \\quad t = \\tau\n$$\nTo confirm this is a maximum, we could check the second derivative, but it is clear from the function's behavior ($\\alpha(0)=0$ and $\\alpha(t) \\to 0$ as $t \\to \\infty$) that this single critical point for $t>0$ must be the global maximum.\nTherefore, the peak time is:\n$$\nt_{\\mathrm{peak}} = \\tau\n$$\n\nNext, we determine the peak amplitude, $\\alpha(t_{\\mathrm{peak}})$, by substituting $t_{\\mathrm{peak}} = \\tau$ into the expression for $\\alpha(t)$:\n$$\n\\alpha(t_{\\mathrm{peak}}) = \\alpha(\\tau) = \\frac{A}{\\tau} (\\tau) \\exp\\left(-\\frac{\\tau}{\\tau}\\right) = A \\cdot 1 \\cdot \\exp(-1) = \\frac{A}{e}\n$$\n\nNow, we evaluate the integral $\\int_{0}^{\\infty} \\alpha(t)\\,dt$, which defines the synaptic weight $w$.\n$$\nw = \\int_{0}^{\\infty} \\frac{A}{\\tau} t \\exp\\left(-\\frac{t}{\\tau}\\right) dt = \\frac{A}{\\tau} \\int_{0}^{\\infty} t \\exp\\left(-\\frac{t}{\\tau}\\right) dt\n$$\nWe use integration by parts, $\\int u \\,dv = uv - \\int v \\,du$. Let:\n- $u = t \\implies du = dt$\n- $dv = \\exp(-t/\\tau) dt \\implies v = -\\tau \\exp(-t/\\tau)$\n\n$$\n\\int t \\exp\\left(-\\frac{t}{\\tau}\\right) dt = t \\left(-\\tau \\exp\\left(-\\frac{t}{\\tau}\\right)\\right) - \\int \\left(-\\tau \\exp\\left(-\\frac{t}{\\tau}\\right)\\right) dt\n$$\n$$\n= -t\\tau \\exp\\left(-\\frac{t}{\\tau}\\right) + \\tau \\int \\exp\\left(-\\frac{t}{\\tau}\\right) dt\n$$\n$$\n= -t\\tau \\exp\\left(-\\frac{t}{\\tau}\\right) + \\tau \\left(-\\tau \\exp\\left(-\\frac{t}{\\tau}\\right)\\right) = -\\exp\\left(-\\frac{t}{\\tau}\\right)(t\\tau + \\tau^2)\n$$\nNow we apply the definite integral limits from $0$ to $\\infty$:\n$$\n\\int_{0}^{\\infty} t \\exp\\left(-\\frac{t}{\\tau}\\right) dt = \\left[ -\\exp\\left(-\\frac{t}{\\tau}\\right)(t\\tau + \\tau^2) \\right]_{0}^{\\infty}\n$$\nEvaluating at the upper limit $t \\to \\infty$: $\\lim_{t\\to\\infty} \\left[ -\\exp\\left(-\\frac{t}{\\tau}\\right)(t\\tau + \\tau^2) \\right] = 0$, since the exponential decay to zero is faster than the polynomial growth.\nEvaluating at the lower limit $t=0$: $-\\exp(0)(0 \\cdot \\tau + \\tau^2) = -1(0 + \\tau^2) = -\\tau^2$.\nThe value of the definite integral is $(0) - (-\\tau^2) = \\tau^2$.\n\nSubstituting this result back into the expression for $w$:\n$$\nw = \\frac{A}{\\tau} (\\tau^2) = A\\tau\n$$\nSo, the value of the integral $\\int_{0}^{\\infty} \\alpha(t)\\,dt$ is $A\\tau$. According to the problem's definition, this value is denoted by $w$.\n\nUsing the relation $w = A\\tau$, we express the amplitude scaling constant $A$ in terms of $w$ and $\\tau$:\n$$\nA = \\frac{w}{\\tau}\n$$\n\nFinally, we express the peak amplitude $\\alpha(t_{\\mathrm{peak}})$ in terms of $w$ and $\\tau$. We previously found $\\alpha(t_{\\mathrm{peak}}) = A/e$. Substituting the expression for $A$:\n$$\n\\alpha(t_{\\mathrm{peak}}) = \\frac{1}{e} \\left(\\frac{w}{\\tau}\\right) = \\frac{w}{e\\tau}\n$$\n\nThe three quantities to be reported are $t_{\\mathrm{peak}}$, $\\alpha(t_{\\mathrm{peak}})$ in terms of $w$ and $\\tau$, and the evaluated integral $\\int_{0}^{\\infty} \\alpha(t) \\,dt$. The problem defines this integral as $w$, which is therefore its evaluated symbolic value.\nThe results are:\n- $t_{\\mathrm{peak}} = \\tau$\n- $\\alpha(t_{\\mathrm{peak}}) = \\frac{w}{e\\tau}$\n- $\\int_{0}^{\\infty} \\alpha(t) \\,dt = w$",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\tau  \\frac{w}{e\\tau}  w \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "One of the primary motivations for using SCNNs is their potential for high computational efficiency when processing sparse, event-based data. This practice moves from the single synapse to the network layer, focusing on the mechanics of event-driven convolution. By deriving the expected number of computations per input spike , you will quantify the performance benefits of sparse processing, a core concept that distinguishes SCNNs from traditional, dense CNNs.",
            "id": "4060527",
            "problem": "Consider a two-dimensional spiking input map of size $H \\times W$ with exactly $S$ nonzero spike events located at integer coordinates $\\{(i_{\\ell}, j_{\\ell})\\}_{\\ell=1}^{S}$ on a regular grid. A single-input, single-output-channel, two-dimensional strided convolution with kernel support of size $k \\times k$ and stride $s$ is implemented in an event-driven (scatter) manner: for each input spike event at position $(i, j)$ with value $x[i,j] \\neq 0$, contributions are propagated only to those output neurons whose receptive fields include $(i,j)$ under the strided convolution. Assume zero padding sufficient to realize the standard “same” convolution shape, and neglect boundary truncation so that any kernel offset within $\\{0,1,\\dots,k-1\\}$ is admissible. Also assume that spike locations are uniformly distributed over stride residue classes modulo $s$ in each dimension and that there is no thresholding or nonlinearity during this counting step.\n\nStarting only from the definition of discrete two-dimensional strided convolution and the described event-driven scatter rule, derive the expected number, per input spike event, of:\n1) synaptic event computations (i.e., multiply-accumulate operations with kernel weights that are actually performed), and\n2) distinct output event candidates (i.e., distinct output lattice sites that receive a nonzero contribution from the input event).\n\nExpress both answers as closed-form analytic expressions depending only on $k$ and $s$. Report the final result as a two-entry row vector, where the first entry is the expected synaptic event computations per input event and the second entry is the expected number of output event candidates per input event. No numerical rounding is required, and no physical units apply.",
            "solution": "The problem requires the derivation of two expected values related to an event-driven, two-dimensional strided convolution. Let's first validate the problem and then proceed with a rigorous derivation.\n\n### Problem Validation\n\n1.  **Extract Givens**:\n    *   Input: A $H \\times W$ spiking input map with $S$ nonzero spike events at integer coordinates $\\{(i_{\\ell}, j_{\\ell})\\}_{\\ell=1}^{S}$. Each spike at $(i, j)$ has a value $x[i,j] \\neq 0$.\n    *   Convolution Parameters: Kernel support size is $k \\times k$, stride is $s$.\n    *   Implementation: Event-driven scatter rule, where each input spike at $(i, j)$ propagates contributions to all output neurons whose receptive fields include $(i,j)$.\n    *   Padding and Boundaries: \"Same\" convolution shape with sufficient zero padding. Boundary truncation is neglected.\n    *   Spike Distribution: Spike locations are uniformly distributed over stride residue classes modulo $s$ in each dimension. This means for a spike at $(i,j)$, $i \\pmod s$ and $j \\pmod s$ are independent random variables, each uniformly distributed on $\\{0, 1, \\dots, s-1\\}$.\n    *   Neuron Model: No thresholding or nonlinearity is considered. The operations are simple accumulations.\n\n2.  **Validate**:\n    *   The problem is **scientifically grounded** in the domain of computational neuroscience and deep learning, specifically concerning efficient implementations of convolutions for sparse, event-based data (spiking neural networks).\n    *   It is **well-posed**, as it asks for expected values based on a clearly defined probabilistic model and a precise operational rule (strided convolution).\n    *   It is **objective**, using formal and unambiguous technical language.\n    *   The setup is **complete and consistent**. The simplifying assumptions (neglecting boundaries, uniform distribution) are standard for theoretical analysis and make the problem tractable.\n    *   The problem is **valid** and can be solved.\n\n### Derivation\n\nLet the input grid be indexed by $(i,j)$ and the output grid by $(p,q)$. In a two-dimensional convolution with kernel size $k \\times k$ and stride $s$, the receptive field of an output neuron at location $(p,q)$ covers a $k \\times k$ patch of the input. The top-left corner of this patch is at input coordinate $(s \\cdot p, s \\cdot q)$, assuming a coordinate system where padding has been appropriately handled. Thus, the receptive field for output neuron $(p,q)$ spans the input coordinates $(i', j')$ where:\n$$s \\cdot p \\leq i' \\leq s \\cdot p + k - 1$$\n$$s \\cdot q \\leq j' \\leq s \\cdot q + k - 1$$\n\nThe problem describes a scatter-based implementation. For a single input spike at location $(i,j)$, we must identify all output neurons $(p,q)$ that are affected. This occurs if and only if the input location $(i,j)$ falls within the receptive field of the output neuron $(p,q)$. Using the inequalities above with $(i', j') = (i, j)$, we get:\n$$s \\cdot p \\leq i \\leq s \\cdot p + k - 1$$\n$$s \\cdot q \\leq j \\leq s \\cdot q + k - 1$$\n\nTo find the range of affected integer output coordinates $(p,q)$, we rearrange these inequalities:\nFor the first dimension:\n$$i - (k - 1) \\leq s \\cdot p \\leq i$$\n$$\\frac{i - k + 1}{s} \\leq p \\leq \\frac{i}{s}$$\nSince $p$ must be an integer, the range of possible values for $p$ is given by:\n$$p \\in \\left[ \\lceil \\frac{i - k + 1}{s} \\rceil, \\lfloor \\frac{i}{s} \\rfloor \\right]$$\nSimilarly, for the second dimension:\n$$q \\in \\left[ \\lceil \\frac{j - k + 1}{s} \\rceil, \\lfloor \\frac{j}{s} \\rfloor \\right]$$\n\nThe number of distinct output neurons affected by a spike at $(i,j)$ is the total number of integer pairs $(p,q)$ satisfying these conditions. Let $N_p(i)$ be the number of possible integer values for $p$, and $N_q(j)$ be the number of possible integer values for $q$. The total number of affected output neurons is $N_{out}(i,j) = N_p(i) \\cdot N_q(j)$.\n\nLet's first analyze the two quantities requested:\n1.  **Distinct output event candidates**: This is, by definition, the number of distinct output lattice sites $(p,q)$ that receive a nonzero contribution from the input event at $(i,j)$. This quantity is precisely $N_{out}(i,j)$.\n2.  **Synaptic event computations**: The scatter rule states that a contribution is propagated to each affected output neuron. Each such contribution involves one kernel weight and the input spike's value, constituting a single multiply-accumulate (MAC) operation. There is a one-to-one correspondence between an affected output neuron and the MAC operation it receives from the input spike. Therefore, the number of synaptic event computations is also equal to $N_{out}(i,j)$.\n\nOur task reduces to finding the expected value of $N_{out}(i,j)$.\n$$E[N_{out}(i,j)] = E[N_p(i) \\cdot N_q(j)]$$\nThe problem states that the spike locations are uniformly distributed over stride residue classes. This means $i \\pmod s$ and $j \\pmod s$ are independent and identically distributed. As we will see, $N_p(i)$ depends only on $i \\pmod s$, and $N_q(j)$ depends only on $j \\pmod s$. Due to independence, the expectation of the product is the product of the expectations:\n$$E[N_{out}(i,j)] = E[N_p(i)] \\cdot E[N_q(j)]$$\n\nLet's calculate $E[N_p(i)]$. The number of integers in an interval $[\\alpha, \\beta]$ is $\\lfloor\\beta\\rfloor - \\lceil\\alpha\\rceil + 1$.\n$$N_p(i) = \\left\\lfloor \\frac{i}{s} \\right\\rfloor - \\left\\lceil \\frac{i - k + 1}{s} \\right\\rceil + 1$$\nLet $i = z_i s + r_i$, where $r_i = i \\pmod s$ and $z_i = \\lfloor i/s \\rfloor$ is an integer. $r_i$ is uniformly distributed on $\\{0, 1, \\dots, s-1\\}$.\n$$N_p(i) = \\left\\lfloor \\frac{z_i s + r_i}{s} \\right\\rfloor - \\left\\lceil \\frac{z_i s + r_i - k + 1}{s} \\right\\rceil + 1$$\n$$N_p(i) = z_i - \\left( z_i + \\left\\lceil \\frac{r_i - k + 1}{s} \\right\\rceil \\right) + 1 = 1 - \\left\\lceil \\frac{r_i - k + 1}{s} \\right\\rceil$$\nUsing the identity $\\lceil x \\rceil = -\\lfloor -x \\rfloor$:\n$$N_p(i) = 1 + \\left\\lfloor -\\frac{r_i - k + 1}{s} \\right\\rfloor = 1 + \\left\\lfloor \\frac{k - 1 - r_i}{s} \\right\\rfloor$$\nAs shown, $N_p(i)$ depends only on $r_i = i \\pmod s$. Let's denote this function as $N_p(r_i)$.\nThe expected value $E[N_p]$ is calculated by averaging over all possible values of $r_i$:\n$$E[N_p] = \\frac{1}{s} \\sum_{r_i=0}^{s-1} N_p(r_i) = \\frac{1}{s} \\sum_{r_i=0}^{s-1} \\left( 1 + \\left\\lfloor \\frac{k - 1 - r_i}{s} \\right\\rfloor \\right)$$\n$$E[N_p] = \\frac{1}{s} \\left( \\sum_{r_i=0}^{s-1} 1 + \\sum_{r_i=0}^{s-1} \\left\\lfloor \\frac{k - 1 - r_i}{s} \\right\\rfloor \\right) = \\frac{1}{s} \\left( s + \\sum_{r_i=0}^{s-1} \\left\\lfloor \\frac{k - 1 - r_i}{s} \\right\\rfloor \\right)$$\nTo evaluate the sum, let $A = k - 1$. The sum is $\\sum_{r=0}^{s-1} \\lfloor \\frac{A - r}{s} \\rfloor$. We can use the property $\\sum_{i=0}^{n-1} \\lfloor x + i/n \\rfloor = \\lfloor nx \\rfloor$ by setting $r = s-1-j$, which means $j$ also goes from $s-1$ to $0$. Let's use a more direct method. Let $A = z_A s + r_A$ where $z_A = \\lfloor A/s \\rfloor$ and $r_A = A \\pmod s$.\n$$\\sum_{r=0}^{s-1} \\left\\lfloor \\frac{z_A s + r_A - r}{s} \\right\\rfloor = \\sum_{r=0}^{s-1} \\left( z_A + \\left\\lfloor \\frac{r_A - r}{s} \\right\\rfloor \\right) = s \\cdot z_A + \\sum_{r=0}^{s-1} \\left\\lfloor \\frac{r_A - r}{s} \\right\\rfloor$$\nThe term $\\lfloor \\frac{r_A - r}{s} \\rfloor$ is $0$ for $r \\in \\{0, \\dots, r_A\\}$ (since $0 \\le r_A-r  s$) and $-1$ for $r \\in \\{r_A+1, \\dots, s-1\\}$ (since $-s  r_A-r  0$).\nThere are $r_A+1$ terms that are $0$ and $(s-1) - (r_A+1) + 1 = s - 1 - r_A$ terms that are $-1$.\n$$\\sum_{r=0}^{s-1} \\left\\lfloor \\frac{r_A - r}{s} \\right\\rfloor = (r_A+1) \\cdot 0 + (s - 1 - r_A) \\cdot (-1) = r_A - s + 1$$\nThe total sum is $s \\cdot z_A + r_A - s + 1 = (s \\cdot z_A + r_A) - s + 1 = A - s + 1$.\nSubstituting $A=k-1$, the sum is $(k-1) - s + 1 = k-s$.\nNow, we substitute this back into the expression for $E[N_p]$:\n$$E[N_p] = \\frac{1}{s} (s + k - s) = \\frac{k}{s}$$\nBy symmetry, the calculation for the second dimension is identical:\n$$E[N_q] = \\frac{k}{s}$$\nFinally, we can compute the expected number of affected output neurons:\n$$E[N_{out}] = E[N_p] \\cdot E[N_q] = \\frac{k}{s} \\cdot \\frac{k}{s} = \\frac{k^2}{s^2}$$\nAs established earlier, the expected number of synaptic computations is equal to the expected number of distinct output event candidates. Therefore, both requested quantities are equal to $\\frac{k^2}{s^2}$.\n\nThe final answer should be presented as a two-entry row vector.\n1.  Expected number of synaptic event computations per input event: $\\frac{k^2}{s^2}$\n2.  Expected number of distinct output event candidates per input event: $\\frac{k^2}{s^2}$",
            "answer": "$$\\boxed{\\begin{pmatrix} \\frac{k^2}{s^2}  \\frac{k^2}{s^2} \\end{pmatrix}}$$"
        },
        {
            "introduction": "After understanding synaptic dynamics and layer-wise computation, the final step is to enable learning. This practice delves into the heart of training SNNs by deriving the gradient for a convolutional weight using Backpropagation Through Time (BPTT). You will learn how to handle the temporal dependencies of leaky-integrate-and-fire neurons and navigate the challenge of non-differentiable spike events using a surrogate gradient, a cornerstone technique for training modern SNNs directly .",
            "id": "4060550",
            "problem": "Consider a single-output-channel Spiking Convolutional Neural Network (SCNN) layer operating in discrete time. The layer computes a pre-synaptic current by convolving input spike trains with a spatial kernel and then updates neuron membrane potentials via leaky integration. Formally, for spatial coordinates indexed by integers $(i,j)$, time index $t \\in \\{1,\\dots,T\\}$, and input channels indexed by $c$, define:\n- The pre-synaptic current at time $t$ and position $(i,j)$ as\n$$\na_{t}(i,j) = \\sum_{u} \\sum_{v} \\sum_{c} w_{u,v,c} \\, x_{t}(i+u, j+v, c),\n$$\nwhere $x_{t}(p,q,c) \\in \\{0,1\\}$ denotes the binary input spike at time $t$, spatial position $(p,q)$, and channel $c$, and $w_{u,v,c}$ are the shared convolutional kernel parameters.\n- The membrane potential as a linear leaky integration without reset,\n$$\nm_{t}(i,j) = \\alpha \\, m_{t-1}(i,j) + a_{t}(i,j),\n$$\nwith leak coefficient $0 \\leq \\alpha  1$ and given initial condition $m_{0}(i,j)$.\n- The spike output via a hard threshold,\n$$\ns_{t}(i,j) = H\\big(m_{t}(i,j) - \\theta\\big),\n$$\nwhere $H(\\cdot)$ is the Heaviside step function and $\\theta$ is the firing threshold.\n\nAssume the total loss $L$ is a differentiable functional of the spike outputs $\\{s_{t}(i,j)\\}_{t,i,j}$, and that Backpropagation Through Time (BPTT) is performed by replacing the non-differentiable derivative of the Heaviside function with a smooth surrogate derivative $\\sigma'\\!\\big(u\\big)$ evaluated at the thresholded membrane, that is, using $\\sigma'\\!\\big(m_{t}(i,j)-\\theta\\big)$ wherever $\\frac{\\partial s_{t}(i,j)}{\\partial m_{t}(i,j)}$ would appear.\n\nStarting from core definitions of discrete-time convolution, linear leaky integration, the chain rule of differentiation, and time-unfolded BPTT, derive a closed-form analytic expression for the gradient $\\frac{\\partial L}{\\partial w_{u,v,c}}$ that explicitly sums over time and space. Your final expression must be in terms of the leak $\\alpha$, the surrogate derivative $\\sigma'(\\cdot)$, the threshold $\\theta$, the membrane potentials $m_{t}(i,j)$, the input spikes $x_{t}(\\cdot)$, and the partial derivatives $\\frac{\\partial L}{\\partial s_{t}(i,j)}$. Express the answer as a single analytic expression. No numerical approximation is required.",
            "solution": "The objective is to derive a closed-form analytic expression for the gradient of the total loss $L$ with respect to a convolutional kernel weight $w_{u,v,c}$. This requires the application of the chain rule of differentiation through the computational graph of the Spiking Convolutional Neural Network (SCNN) layer, unfolded over time, a procedure known as Backpropagation Through Time (BPTT).\n\nLet us begin by expressing the total gradient $\\frac{\\partial L}{\\partial w_{u,v,c}}$ as the sum of its influences on the loss $L$ via all intermediate variables it directly affects. The weight $w_{u,v,c}$ directly contributes to the pre-synaptic current $a_{t}(i,j)$ at every time step $t$ and for every spatial position $(i,j)$. Using the chain rule, we can write:\n$$\n\\frac{\\partial L}{\\partial w_{u,v,c}} = \\sum_{t=1}^{T} \\sum_{i} \\sum_{j} \\frac{\\partial L}{\\partial a_{t}(i,j)} \\frac{\\partial a_{t}(i,j)}{\\partial w_{u,v,c}}\n$$\nThe summation is over all time steps $t$ from $1$ to $T$ and all spatial positions $(i,j)$ of the output map where the kernel is applied.\n\nOur derivation proceeds in three main parts:\n1.  Compute the partial derivative $\\frac{\\partial a_{t}(i,j)}{\\partial w_{u,v,c}}$.\n2.  Find an expression for the term $\\frac{\\partial L}{\\partial a_{t}(i,j)}$ by tracing its influence through the network dynamics.\n3.  Combine these results to form the final expression.\n\nFirst, we compute the partial derivative of the pre-synaptic current $a_{t}(i,j)$ with respect to the weight $w_{u,v,c}$. The definition of $a_{t}(i,j)$ is given as:\n$$\na_{t}(i,j) = \\sum_{u'} \\sum_{v'} \\sum_{c'} w_{u',v',c'} \\, x_{t}(i+u', j+v', c')\n$$\nDifferentiating with respect to a specific weight $w_{u,v,c}$ yields a non-zero term only when the indices $(u',v',c')$ match $(u,v,c)$. Thus, we have:\n$$\n\\frac{\\partial a_{t}(i,j)}{\\partial w_{u,v,c}} = x_{t}(i+u, j+v, c)\n$$\nThis term represents the input spike that was multiplied by the weight $w_{u,v,c}$ to contribute to the current $a_{t}(i,j)$.\n\nSecond, we determine the gradient of the loss with respect to the pre-synaptic current, $\\frac{\\partial L}{\\partial a_{t}(i,j)}$. The current $a_{t}(i,j)$ influences the loss $L$ solely through its effect on the membrane potential $m_{t}(i,j)$, as defined by the leaky integrator equation:\n$$\nm_{t}(i,j) = \\alpha \\, m_{t-1}(i,j) + a_{t}(i,j)\n$$\nApplying the chain rule again:\n$$\n\\frac{\\partial L}{\\partial a_{t}(i,j)} = \\frac{\\partial L}{\\partial m_{t}(i,j)} \\frac{\\partial m_{t}(i,j)}{\\partial a_{t}(i,j)}\n$$\nFrom the membrane potential equation, $\\frac{\\partial m_{t}(i,j)}{\\partial a_{t}(i,j)} = 1$. Therefore:\n$$\n\\frac{\\partial L}{\\partial a_{t}(i,j)} = \\frac{\\partial L}{\\partial m_{t}(i,j)}\n$$\nLet us define the error signal backpropagated to the membrane potential at time $t$ and position $(i,j)$ as $\\delta_t^m(i,j) \\equiv \\frac{\\partial L}{\\partial m_t(i,j)}$.\n\nThe core of the problem is to find an expression for $\\delta_t^m(i,j)$. The membrane potential $m_t(i,j)$ influences the total loss $L$ through two paths in the time-unfolded graph:\n1.  Directly, by determining the output spike $s_t(i,j)$ at the same time step.\n2.  Indirectly, by contributing to the membrane potential at the next time step, $m_{t+1}(i,j)$.\n\nThe total gradient $\\delta_t^m(i,j)$ is the sum of the gradients from these two paths:\n$$\n\\delta_t^m(i,j) = \\frac{\\partial L}{\\partial s_t(i,j)} \\frac{\\partial s_t(i,j)}{\\partial m_t(i,j)} + \\frac{\\partial L}{\\partial m_{t+1}(i,j)} \\frac{\\partial m_{t+1}(i,j)}{\\partial m_t(i,j)}\n$$\nWe evaluate the partial derivatives using the provided definitions:\n- The problem specifies using a surrogate derivative $\\sigma'(\\cdot)$ for the Heaviside spike function: $\\frac{\\partial s_t(i,j)}{\\partial m_t(i,j)} = \\sigma'(m_t(i,j) - \\theta)$.\n- From the membrane potential update rule, $m_{t+1}(i,j) = \\alpha \\, m_t(i,j) + a_{t+1}(i,j)$, we find $\\frac{\\partial m_{t+1}(i,j)}{\\partial m_t(i,j)} = \\alpha$.\n\nSubstituting these into the equation for $\\delta_t^m(i,j)$ yields a backward recurrence relation:\n$$\n\\delta_t^m(i,j) = \\frac{\\partial L}{\\partial s_t(i,j)} \\sigma'(m_t(i,j) - \\theta) + \\alpha \\, \\delta_{t+1}^m(i,j)\n$$\nThis relation holds for $t \\in \\{1, \\dots, T-1\\}$. At the final time step $t=T$, $m_T(i,j)$ has no influence on future potentials, so the second term is zero (or, formally, $\\delta_{T+1}^m(i,j) = 0$). The boundary condition is:\n$$\n\\delta_T^m(i,j) = \\frac{\\partial L}{\\partial s_T(i,j)} \\sigma'(m_T(i,j) - \\theta)\n$$\nThis linear recurrence can be solved by unrolling it from $t=T$ backward to $t$:\n\\begin{align*}\n\\delta_{T-1}^m(i,j) = \\left(\\frac{\\partial L}{\\partial s_{T-1}(i,j)}\\sigma'(m_{T-1}(i,j) - \\theta)\\right) + \\alpha \\left(\\frac{\\partial L}{\\partial s_T(i,j)}\\sigma'(m_T(i,j) - \\theta)\\right) \\\\\n\\delta_{T-2}^m(i,j) = \\left(\\frac{\\partial L}{\\partial s_{T-2}(i,j)}\\sigma'(m_{T-2}(i,j) - \\theta)\\right) + \\alpha \\delta_{T-1}^m(i,j) \\\\\n\\end{align*}\nThe general closed-form solution is a sum of future error injections, discounted by powers of the leak factor $\\alpha$:\n$$\n\\delta_t^m(i,j) = \\sum_{k=t}^{T} \\alpha^{k-t} \\frac{\\partial L}{\\partial s_k(i,j)} \\sigma'(m_k(i,j) - \\theta)\n$$\n\nFinally, we assemble the complete expression for the gradient $\\frac{\\partial L}{\\partial w_{u,v,c}}$. We substitute the derived expressions for $\\frac{\\partial a_{t}(i,j)}{\\partial w_{u,v,c}}$ and $\\frac{\\partial L}{\\partial a_{t}(i,j)} = \\delta_t^m(i,j)$ into our initial formula:\n$$\n\\frac{\\partial L}{\\partial w_{u,v,c}} = \\sum_{t=1}^{T} \\sum_{i} \\sum_{j} \\delta_t^m(i,j) \\, x_t(i+u, j+v, c)\n$$\nSubstituting the closed-form expression for $\\delta_t^m(i,j)$ gives the final analytic result:\n$$\n\\frac{\\partial L}{\\partial w_{u,v,c}} = \\sum_{t=1}^{T} \\sum_{i} \\sum_{j} \\left( \\sum_{k=t}^{T} \\alpha^{k-t} \\frac{\\partial L}{\\partial s_k(i,j)} \\sigma'(m_k(i,j) - \\theta) \\right) x_t(i+u, j+v, c)\n$$\nThis expression represents a spatio-temporal correlation. The term in parentheses is the total error signal at position $(i,j)$ and time $t$, accumulated from all future time steps $k \\ge t$ and backpropagated through time with decay $\\alpha$. The outer sums then correlate this error with every input spike $x_t(i+u, j+v, c)$ that contributed to it via the weight $w_{u,v,c}$, thereby calculating the weight's total contribution to the loss.",
            "answer": "$$\n\\boxed{\\frac{\\partial L}{\\partial w_{u,v,c}} = \\sum_{t=1}^{T} \\sum_{i} \\sum_{j} \\left( \\sum_{k=t}^{T} \\alpha^{k-t} \\frac{\\partial L}{\\partial s_k(i,j)} \\sigma'(m_k(i,j) - \\theta) \\right) x_t(i+u, j+v, c)}\n$$"
        }
    ]
}