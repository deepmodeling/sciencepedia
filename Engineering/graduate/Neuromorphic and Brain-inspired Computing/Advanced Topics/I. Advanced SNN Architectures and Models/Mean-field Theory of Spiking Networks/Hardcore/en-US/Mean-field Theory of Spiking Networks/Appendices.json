{
    "hands_on_practices": [
        {
            "introduction": "Before we can model an entire network, we must first characterize the synaptic input received by a single neuron. This foundational exercise guides you through calculating the mean and variance of synaptic currents and conductances generated by random, Poisson-distributed spike trains. Mastering this calculation is essential, as these first two moments form the basis of the diffusion approximation used in many mean-field theories. ",
            "id": "4051240",
            "problem": "Consider a stationary presynaptic spike train modeled as a homogeneous Poisson point process with rate $\\nu$ (events per unit time). Let the postsynaptic response to a single spike be governed by the causal single-exponential synaptic kernel $\\alpha(t) = \\exp(-t/\\tau_s) H(t)$, where $H(t)$ is the Heaviside step function and $\\tau_s > 0$ is the synaptic time constant. Two synaptic models are considered:\n\n1. A current-based synapse, in which the postsynaptic current is given by $I(t) = J \\sum_{k} \\alpha(t - t_k)$, where $J$ is a constant current increment per spike and $\\{t_k\\}$ are the presynaptic spike times.\n\n2. A conductance-based synapse, in which the postsynaptic conductance is given by $g(t) = g_0 \\sum_{k} \\alpha(t - t_k)$, where $g_0$ is a constant conductance increment per spike.\n\nStarting only from:\n- The definition of a homogeneous Poisson point process and its stationarity.\n- Well-tested properties of sums over Poisson point processes (e.g., the expectation of sums and second moments over independent points), and the fact that $\\alpha(t)$ is integrable and square-integrable on $[0,\\infty)$.\n\nDerive the stationary mean and variance of the synaptic current $I(t)$ for the current-based synapse, and the stationary mean and variance of the synaptic conductance $g(t)$ for the conductance-based synapse. Express your final answer as closed-form analytic expressions in terms of $\\nu$, $\\tau_s$, $J$, and $g_0$. Do not substitute any numerical values. Provide the final answer as a row matrix with entries in the order $\\langle I \\rangle$, $\\mathrm{Var}(I)$, $\\langle g \\rangle$, $\\mathrm{Var}(g)$. No rounding is required. Express the answer symbolically only.",
            "solution": "The problem asks for the stationary mean and variance of the synaptic current $I(t)$ and synaptic conductance $g(t)$, which are driven by a homogeneous Poisson spike train with rate $\\nu$. The problem statement is validated as scientifically grounded, well-posed, and objective. It is a standard problem in theoretical and computational neuroscience.\n\nBoth the current $I(t)$ and the conductance $g(t)$ can be represented as linear filtered Poisson processes, also known as shot noise. A general shot noise process $X(t)$ is defined as a sum of responses to events occurring at random times $\\{t_k\\}$:\n$$X(t) = \\sum_{k} h(t - t_k)$$\nwhere $\\{t_k\\}$ are the event times of a Poisson process with rate $\\nu$, and $h(t)$ is the response function or kernel.\n\nFor a stationary, homogeneous Poisson process, the stationary mean $\\langle X \\rangle$ and variance $\\mathrm{Var}(X)$ of the process $X(t)$ are given by Campbell's theorems. The problem statement permits the use of these well-tested properties. The formulas are:\n$$ \\langle X \\rangle = \\nu \\int_{-\\infty}^{\\infty} h(s) ds $$\n$$ \\mathrm{Var}(X) = \\nu \\int_{-\\infty}^{\\infty} h(s)^2 ds $$\nThese formulas are valid under the condition that the integrals converge, which is satisfied here as the kernel $\\alpha(t)$ is stated to be integrable and square-integrable.\n\nThe synaptic kernel is given as $\\alpha(t) = \\exp(-t/\\tau_s) H(t)$, where $H(t)$ is the Heaviside step function. This kernel is causal, meaning $\\alpha(t) = 0$ for $t < 0$. We will apply these formulas to both the current-based and conductance-based synaptic models.\n\nFirst, we calculate the necessary integrals of the kernel $\\alpha(t)$.\nThe integral of $\\alpha(t)$ is:\n$$ \\int_{-\\infty}^{\\infty} \\alpha(s) ds = \\int_0^{\\infty} \\exp(-s/\\tau_s) ds = \\left[ -\\tau_s \\exp(-s/\\tau_s) \\right]_0^{\\infty} = 0 - (-\\tau_s \\exp(0)) = \\tau_s $$\n\nThe integral of $\\alpha(t)^2$ is:\n$$ \\int_{-\\infty}^{\\infty} \\alpha(s)^2 ds = \\int_0^{\\infty} \\left( \\exp(-s/\\tau_s) \\right)^2 ds = \\int_0^{\\infty} \\exp(-2s/\\tau_s) ds = \\left[ -\\frac{\\tau_s}{2} \\exp(-2s/\\tau_s) \\right]_0^{\\infty} = 0 - \\left(-\\frac{\\tau_s}{2} \\exp(0)\\right) = \\frac{\\tau_s}{2} $$\n\nNow we can proceed to calculate the moments for each model.\n\n1.  Current-Based Synapse\nThe postsynaptic current is given by $I(t) = J \\sum_{k} \\alpha(t - t_k)$.\nThis is a shot noise process where the response kernel is $h_I(t) = J \\alpha(t)$.\n\nThe mean current, $\\langle I \\rangle$, is:\n$$ \\langle I \\rangle = \\nu \\int_{-\\infty}^{\\infty} h_I(s) ds = \\nu \\int_{-\\infty}^{\\infty} J \\alpha(s) ds $$\n$$ \\langle I \\rangle = \\nu J \\int_{-\\infty}^{\\infty} \\alpha(s) ds = \\nu J \\tau_s $$\n\nThe variance of the current, $\\mathrm{Var}(I)$, is:\n$$ \\mathrm{Var}(I) = \\nu \\int_{-\\infty}^{\\infty} h_I(s)^2 ds = \\nu \\int_{-\\infty}^{\\infty} (J \\alpha(s))^2 ds $$\n$$ \\mathrm{Var}(I) = \\nu J^2 \\int_{-\\infty}^{\\infty} \\alpha(s)^2 ds = \\nu J^2 \\left( \\frac{\\tau_s}{2} \\right) = \\frac{\\nu J^2 \\tau_s}{2} $$\n\n2.  Conductance-Based Synapse\nThe postsynaptic conductance is given by $g(t) = g_0 \\sum_{k} \\alpha(t - t_k)$.\nThis is also a shot noise process, but with the response kernel $h_g(t) = g_0 \\alpha(t)$. The structure of the calculation is identical to the current-based case, with the constant $J$ being replaced by $g_0$.\n\nThe mean conductance, $\\langle g \\rangle$, is:\n$$ \\langle g \\rangle = \\nu \\int_{-\\infty}^{\\infty} h_g(s) ds = \\nu \\int_{-\\infty}^{\\infty} g_0 \\alpha(s) ds $$\n$$ \\langle g \\rangle = \\nu g_0 \\int_{-\\infty}^{\\infty} \\alpha(s) ds = \\nu g_0 \\tau_s $$\n\nThe variance of the conductance, $\\mathrm{Var}(g)$, is:\n$$ \\mathrm{Var}(g) = \\nu \\int_{-\\infty}^{\\infty} h_g(s)^2 ds = \\nu \\int_{-\\infty}^{\\infty} (g_0 \\alpha(s))^2 ds $$\n$$ \\mathrm{Var}(g) = \\nu g_0^2 \\int_{-\\infty}^{\\infty} \\alpha(s)^2 ds = \\nu g_0^2 \\left( \\frac{\\tau_s}{2} \\right) = \\frac{\\nu g_0^2 \\tau_s}{2} $$\n\nThe four requested quantities are:\n- Mean current: $\\langle I \\rangle = \\nu J \\tau_s$\n- Variance of current: $\\mathrm{Var}(I) = \\frac{\\nu J^2 \\tau_s}{2}$\n- Mean conductance: $\\langle g \\rangle = \\nu g_0 \\tau_s$\n- Variance of conductance: $\\mathrm{Var}(g) = \\frac{\\nu g_0^2 \\tau_s}{2}$\n\nThe final answer is to be presented as a row matrix with these four values in order.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\nu J \\tau_s & \\frac{\\nu J^2 \\tau_s}{2} & \\nu g_0 \\tau_s & \\frac{\\nu g_0^2 \\tau_s}{2}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "This practice represents the core application of mean-field theory: predicting the collective behavior of a recurrently connected network. Using the statistical description of synaptic input, you will formulate and numerically solve the self-consistent equations for a network composed of excitatory and inhibitory neurons. This powerful technique allows you to find the stable firing rates—the fixed points of the network dynamics—that emerge from the balance of excitation and inhibition. ",
            "id": "3996524",
            "problem": "Consider a large, randomly connected, two-population network of spiking neurons comprising an excitatory population labeled $e$ and an inhibitory population labeled $i$. Each neuron receives $K_{pq}$ presynaptic connections from population $q \\in \\{e,i\\}$, and each presynaptic spike from population $q$ produces an instantaneous jump of amplitude $J_{pq}$ in the postsynaptic membrane potential of a neuron in population $p \\in \\{e,i\\}$. Assume the network operates in the high in-degree limit so that incoming spike trains can be approximated as independent Poisson processes and the aggregate synaptic input to a neuron can be treated under a diffusion approximation.\n\nStarting from first principles appropriate to the diffusion approximation for synaptic shot noise and the Leaky Integrate-and-Fire (LIF) model with additive Gaussian white noise:\n\n- The synaptic shot noise is approximated by a Gaussian process characterized by a drift (mean) and diffusion (variance) that are linear functions of the presynaptic firing rates.\n- The membrane potential evolves according to the LIF dynamics with membrane time constant $\\tau_m$ and an absolute refractory period $\\tau_{\\mathrm{ref}}$. Threshold and reset are denoted by $V_{\\theta,p}$ and $V_{r}$, respectively. We assume $V_{r}$ is shared between populations and that all voltage quantities are expressed in units normalized to the threshold scale so that jump amplitudes $J_{pq}$ are dimensionless in those units.\n- In the diffusion limit for large $K_{pq}$ with presynaptic Poisson inputs of rates $\\nu_e$ and $\\nu_i$, the effective drift and diffusion for population $p$ are\n$$\n\\mu_p(\\nu_e,\\nu_i) \\;=\\; \\mu_{0,p} \\;+\\; K_{pe}\\,J_{pe}\\,\\nu_e \\;+\\; K_{pi}\\,J_{pi}\\,\\nu_i,\n$$\n$$\n\\sigma_p^2(\\nu_e,\\nu_i) \\;=\\; \\sigma_{0,p}^2 \\;+\\; K_{pe}\\,J_{pe}^2\\,\\nu_e \\;+\\; K_{pi}\\,J_{pi}^2\\,\\nu_i,\n$$\nwhere $(\\mu_{0,p},\\sigma_{0,p})$ represent external baseline mean and standard deviation contributions that are independent of network activity.\n\n- The single-neuron transfer function for population $p$ under the LIF model with additive white noise is given by the Siegert formula:\n$$\n\\nu_p \\;=\\; \\left[ \\tau_{\\mathrm{ref}} \\;+\\; \\tau_m \\,\\sqrt{\\pi}\\;\\int_{u_{r,p}}^{u_{\\theta,p}} \\exp(u^2)\\,\\bigl(1+\\mathrm{erf}(u)\\bigr)\\,du \\right]^{-1},\n$$\nwith $u_{r,p} = \\dfrac{V_r - \\mu_p}{\\sigma_p}$ and $u_{\\theta,p} = \\dfrac{V_{\\theta,p} - \\mu_p}{\\sigma_p}$, and where $\\mathrm{erf}(\\cdot)$ denotes the error function. For numerical stability, note that $\\exp(u^2)\\bigl(1+\\mathrm{erf}(u)\\bigr) = \\mathrm{erfcx}(-u)$, where $\\mathrm{erfcx}$ is the scaled complementary error function.\n\nYour tasks are:\n1. Derive the self-consistent mean-field equations that define the fixed point firing rates $(\\nu_e,\\nu_i)$ by combining the expressions for $\\mu_p(\\nu_e,\\nu_i)$, $\\sigma_p(\\nu_e,\\nu_i)$, and the single-neuron transfer functions $\\nu_p = F_p\\bigl(\\mu_p(\\nu_e,\\nu_i),\\sigma_p(\\nu_e,\\nu_i)\\bigr)$ for $p \\in \\{e,i\\}$.\n2. Implement a program that solves the resulting two-dimensional fixed point problem for $(\\nu_e,\\nu_i)$ using a robust numerical method.\n3. Use the following test suite of parameter sets. In all cases, express $\\tau_m$ and $\\tau_{\\mathrm{ref}}$ in seconds, and report firing rates in Hertz (spikes per second). Voltage-related quantities are dimensionless in the threshold-normalized units as described above.\n\nTest case A (balanced regime):\n- $\\tau_m = 0.02$, $\\tau_{\\mathrm{ref}} = 0.002$, $V_r = 0$, $V_{\\theta,e} = 1.0$, $V_{\\theta,i} = 1.0$.\n- $K_{ee} = 800$, $K_{ei} = 200$, $K_{ie} = 800$, $K_{ii} = 200$.\n- $J_{ee} = 0.001$, $J_{ei} = -0.004$, $J_{ie} = 0.001$, $J_{ii} = -0.004$.\n- $\\mu_{0,e} = 0.6$, $\\sigma_{0,e} = 0.3$, $\\mu_{0,i} = 0.6$, $\\sigma_{0,i} = 0.3$.\n\nTest case B (weak inhibition, higher activity):\n- Same as A except $J_{ei} = -0.001$, $J_{ii} = -0.001$.\n\nTest case C (strong inhibition, low baseline; near-silence edge case):\n- $\\tau_m = 0.02$, $\\tau_{\\mathrm{ref}} = 0.002$, $V_r = 0$, $V_{\\theta,e} = 1.0$, $V_{\\theta,i} = 1.0$.\n- $K_{ee} = 800$, $K_{ei} = 200$, $K_{ie} = 800$, $K_{ii} = 200$.\n- $J_{ee} = 0.001$, $J_{ei} = -0.008$, $J_{ie} = 0.001$, $J_{ii} = -0.008$.\n- $\\mu_{0,e} = 0.3$, $\\sigma_{0,e} = 0.15$, $\\mu_{0,i} = 0.3$, $\\sigma_{0,i} = 0.15$.\n\nAlgorithmic requirements:\n- Construct the fixed point equations from the provided components without introducing simplifications that bypass fundamental definitions.\n- Solve for $(\\nu_e,\\nu_i)$ using a method that includes a fallback strategy in case a generic root solver fails to converge.\n- Ensure numerical stability of the integral defining the transfer function by using the scaled complementary error function when appropriate.\n\nOutput specification:\n- Your program should produce a single line of output containing the results as a comma-separated Python-style list of lists, one per test case, in the order A, B, C.\n- Each inner list must be the two floats $[\\nu_e,\\nu_i]$ in Hertz, rounded to three decimal places.\n- For example, the output format must be exactly of the form $[[\\nu_{e,A},\\nu_{i,A}],[\\nu_{e,B},\\nu_{i,B}],[\\nu_{e,C},\\nu_{i,C}]]$ on a single line with no additional text.",
            "solution": "The user has provided a problem from computational neuroscience, asking for the derivation and numerical solution of mean-field equations for a two-population network of spiking neurons.\n\n### Problem Validation\n\nThe problem is evaluated against the specified criteria:\n1.  **Givens**: All parameters, equations, and boundary conditions for a two-population Leaky Integrate-and-Fire (LIF) network under the diffusion approximation are explicitly provided. This includes the expressions for mean synaptic input ($\\mu_p$), input variance ($\\sigma_p^2$), and the single-neuron transfer function ($\\nu_p = F_p(\\mu_p, \\sigma_p)$ via the Siegert formula). Three specific parameter sets are given for testing.\n2.  **Scientific Grounding**: The problem is fundamentally sound. The diffusion approximation for synaptic currents, the LIF neuron model, and the Siegert formula for the first-passage time are all canonical components of mean-field theory in computational neuroscience. The setup is a standard, well-established theoretical model.\n3.  **Well-Posedness and Objectivity**: The problem is to find a fixed point of a system of non-linear equations. This is a well-defined mathematical task. The problem statement is objective, using precise, standard terminology. It correctly anticipates numerical challenges by requiring a robust solution method, indicating a well-considered structure. The provided parameters and equations are self-contained, consistent, and sufficient to proceed.\n\nThe problem is deemed **valid**. We may proceed with the solution.\n\n### Part 1: Derivation of Self-Consistent Mean-Field Equations\n\nThe network's state at a fixed point is described by the stationary firing rates of the excitatory and inhibitory populations, $(\\nu_e, \\nu_i)$. A self-consistent solution is one where these firing rates, when fed back into the network, produce the very same rates. This constitutes a fixed point of the network dynamics mapping.\n\nLet the transfer function of population $p \\in \\{e,i\\}$ be denoted by $F_p$. This function calculates the output firing rate $\\nu_p$ given the mean $\\mu_p$ and standard deviation $\\sigma_p$ of the total synaptic input.\n$$\n\\nu_p = F_p(\\mu_p, \\sigma_p)\n$$\nThe function $F_p$ is given by the Siegert formula:\n$$\nF_p(\\mu_p, \\sigma_p) = \\left[ \\tau_{\\mathrm{ref}} + \\tau_m \\sqrt{\\pi} \\int_{u_{r,p}}^{u_{\\theta,p}} \\exp(u^2)\\bigl(1+\\mathrm{erf}(u)\\bigr)\\,du \\right]^{-1}\n$$\nwhere the integration limits are functions of the input statistics:\n$$\nu_{r,p} = \\frac{V_r - \\mu_p}{\\sigma_p}, \\quad u_{\\theta,p} = \\frac{V_{\\theta,p} - \\mu_p}{\\sigma_p}\n$$\nIn turn, the input statistics $\\mu_p$ and $\\sigma_p$ are themselves functions of the network firing rates $(\\nu_e, \\nu_i)$, as specified by the diffusion approximation:\n$$\n\\mu_p(\\nu_e, \\nu_i) = \\mu_{0,p} + K_{pe} J_{pe} \\nu_e + K_{pi} J_{pi} \\nu_i\n$$\n$$\n\\sigma_p(\\nu_e, \\nu_i) = \\sqrt{\\sigma_{0,p}^2 + K_{pe} J_{pe}^2 \\nu_e + K_{pi} J_{pi}^2 \\nu_i}\n$$\nThe self-consistent mean-field equations are obtained by requiring that the output rates match the input rates that generate them. This is achieved by substituting the expressions for $\\mu_p(\\nu_e, \\nu_i)$ and $\\sigma_p(\\nu_e, \\nu_i)$ into the transfer functions, leading to a coupled system of two non-linear equations for the two unknown rates $\\nu_e$ and $\\nu_i$:\n$$\n\\begin{cases}\n\\nu_e = F_e\\Big(\\mu_e(\\nu_e, \\nu_i), \\sigma_e(\\nu_e, \\nu_i)\\Big) \\\\\n\\nu_i = F_i\\Big(\\mu_i(\\nu_e, \\nu_i), \\sigma_i(\\nu_e, \\nu_i)\\Big)\n\\end{cases}\n$$\nThese equations define the fixed points of the system. A solution $(\\nu_e^*, \\nu_i^*)$ represents the stable (or unstable) firing rates of the network in the mean-field limit.\n\n### Part 2 & 3: Numerical Implementation and Solution\n\nTo solve this system, we seek the root of the vector function $\\mathbf{G}(\\boldsymbol{\\nu}) = \\mathbf{F}(\\boldsymbol{\\nu}) - \\boldsymbol{\\nu} = \\mathbf{0}$, where $\\boldsymbol{\\nu} = (\\nu_e, \\nu_i)^T$ and $\\mathbf{F}(\\boldsymbol{\\nu})$ is the vector of transfer functions.\n\n**Numerical Method:**\n1.  **Transfer Function:** The integral in the Siegert formula is numerically computed using `scipy.integrate.quad`. For numerical stability, the integrand $\\exp(u^2)(1+\\mathrm{erf}(u))$ is replaced by the scaled complementary error function, $\\mathrm{erfcx}(-u)$, as suggested. This avoids floating-point overflow for large positive values of $u$, which can occur when the neuron is far from its threshold.\n2.  **Root Finding:** We will use `scipy.optimize.root` with the `hybr` method, a robust algorithm for finding roots of non-linear systems. An initial guess is generated by calculating the firing rates based solely on the external inputs $(\\mu_{0,p}, \\sigma_{0,p})$.\n3.  **Fallback Strategy:** As required, a fallback method is implemented in case the primary root-finder fails. A damped fixed-point iteration scheme is used:\n    $$\n    \\boldsymbol{\\nu}^{(k+1)} = (1-\\alpha)\\boldsymbol{\\nu}^{(k)} + \\alpha \\mathbf{F}(\\boldsymbol{\\nu}^{(k)})\n    $$\n    with a small damping factor $\\alpha > 0$. This iterative method is often stable for this class of problems, though it may converge more slowly than gradient-based methods. Physically, firing rates must be non-negative, $\\nu_p \\ge 0$. The transfer function $F_p$ inherently produces positive values, so the fixed-point iteration preserves this constraint. The primary solver's solution is also checked for positivity.\n\nThe following Python code implements this strategy to solve for the fixed-point firing rates for the three given test cases.",
            "answer": "```python\nimport numpy as np\nfrom scipy.special import erfcx, erf\nfrom scipy.integrate import quad\nfrom scipy.optimize import root\n\ndef solve():\n    \"\"\"\n    Solves the mean-field equations for a two-population spiking network for three test cases.\n    \"\"\"\n\n    def transfer_function(mu, sigma, V_theta, V_r, tau_m, tau_ref):\n        \"\"\"\n        Calculates the firing rate of an LIF neuron given input statistics using the Siegert formula.\n        \n        This is the function F_p(mu_p, sigma_p). The numerically stable form of the integrand is used.\n        \"\"\"\n        # Handle the near-deterministic case where sigma is very small to avoid division by zero.\n        # The diffusion approximation is not valid for sigma -> 0, but this provides numerical stability.\n        if sigma < 1e-9:\n            if mu > V_theta:\n                # In the deterministic limit, firing occurs if mu is above threshold.\n                # The inter-spike interval is the time to drift from V_r to V_theta.\n                # This check prevents log(negative) if mu < V_r.\n                if mu > V_r:\n                    t_isi = tau_ref + tau_m * np.log((mu - V_r) / (mu - V_theta))\n                    return 1.0 / t_isi if t_isi > 1e-9 else 1e9\n                else:\n                    return 0.0 # mu is between V_r and V_theta, so it never reaches threshold.\n            else:\n                return 0.0 # Subthreshold drive.\n\n        # Calculate normalized voltage bounds for the integral\n        u_r = (V_r - mu) / sigma\n        u_theta = (V_theta - mu) / sigma\n        \n        # The problem defines the integral of exp(u^2)*(1+erf(u)), which is numerically\n        # unstable. The hint is to use erfcx(-u).\n        # integrand = lambda u: np.exp(u**2) * (1 + erf(u)) is equivalent to\n        integrand = lambda u: erfcx(-u)\n\n        # The integral is guaranteed to be positive as erfcx(-u) > 0 for all u, and u_theta > u_r.\n        integral_val, _ = quad(integrand, u_r, u_theta)\n\n        # Calculate the mean inter-spike interval\n        tau_isi = tau_ref + tau_m * np.sqrt(np.pi) * integral_val\n\n        # Firing rate is the inverse of the mean ISI.\n        # tau_isi is guaranteed to be positive.\n        return 1.0 / tau_isi\n\n    def fixed_point_system(nu, params_case):\n        \"\"\"\n        Calculates the new firing rates F(nu) based on the current rates nu.\n        \n        Args:\n            nu (array-like): A list or array [nu_e, nu_i] of current firing rates in Hz.\n            params_case (dict): Dictionary containing all parameters for the network.\n        \n        Returns:\n            np.array: A numpy array [nu_e_new, nu_i_new] of calculated firing rates.\n        \"\"\"\n        nu_e, nu_i = max(0, nu[0]), max(0, nu[1]) # Rates must be non-negative\n\n        p_e = params_case['e']\n        p_i = params_case['i']\n\n        # Calculate input mean (drift) for each population\n        mu_e = p_e['mu0'] + p_e['K_ee'] * p_e['J_ee'] * nu_e + p_e['K_ei'] * p_e['J_ei'] * nu_i\n        mu_i = p_i['mu0'] + p_i['K_ie'] * p_i['J_ie'] * nu_e + p_i['K_ii'] * p_i['J_ii'] * nu_i\n        \n        # Calculate input variance (diffusion^2) for each population\n        sigma2_e = p_e['sigma0']**2 + p_e['K_ee'] * p_e['J_ee']**2 * nu_e + p_e['K_ei'] * p_e['J_ei']**2 * nu_i\n        sigma2_i = p_i['sigma0']**2 + p_i['K_ie'] * p_i['J_ie']**2 * nu_e + p_i['K_ii'] * p_i['J_ii']**2 * nu_i\n\n        # Variances must be non-negative. A small floor ensures this.\n        sigma_e = np.sqrt(max(1e-12, sigma2_e))\n        sigma_i = np.sqrt(max(1e-12, sigma2_i))\n\n        # Calculate the new firing rates using the transfer functions\n        nu_e_new = transfer_function(mu_e, sigma_e, p_e['V_theta'], p_e['V_r'], p_e['tau_m'], p_e['tau_ref'])\n        nu_i_new = transfer_function(mu_i, sigma_i, p_i['V_theta'], p_i['V_r'], p_i['tau_m'], p_i['tau_ref'])\n\n        return np.array([nu_e_new, nu_i_new])\n\n    def objective_function(nu, params_case):\n        \"\"\"Objective function for the root finder: G(nu) = F(nu) - nu.\"\"\"\n        return fixed_point_system(nu, params_case) - nu\n\n    def solve_for_case(params_case):\n        \"\"\"\n        Solves the fixed-point problem for a single parameter set.\n        Uses a primary root-finder and a fallback iterative method.\n        \"\"\"\n        # --- Initial Guess ---\n        # Calculate rates based on external input only\n        p_e = params_case['e']\n        p_i = params_case['i']\n        nu_e_0 = transfer_function(p_e['mu0'], p_e['sigma0'], p_e['V_theta'], p_e['V_r'], p_e['tau_m'], p_e['tau_ref'])\n        nu_i_0 = transfer_function(p_i['mu0'], p_i['sigma0'], p_i['V_theta'], p_i['V_r'], p_i['tau_m'], p_i['tau_ref'])\n        initial_guess = np.array([nu_e_0, nu_i_0])\n\n        # --- Primary Solver: scipy.optimize.root ---\n        sol = root(objective_function, initial_guess, args=(params_case,), method='hybr', tol=1e-9)\n\n        if sol.success and np.all(sol.x >= 0):\n            return sol.x\n        \n        # --- Fallback Solver: Damped Fixed-Point Iteration ---\n        nu_k = initial_guess\n        max_iter = 1000\n        tolerance = 1e-9\n        alpha = 0.1  # Damping factor for stability\n        for _ in range(max_iter):\n            nu_k_plus_1 = fixed_point_system(nu_k, params_case)\n            change = np.linalg.norm(nu_k_plus_1 - nu_k)\n            nu_k = (1 - alpha) * nu_k + alpha * nu_k_plus_1\n            if change < tolerance:\n                return nu_k\n        \n        # If the primary solver failed but the fallback also failed to converge,\n        # return the result of the primary solver as a last resort, as it might be close.\n        return sol.x\n    \n    # Define test cases from the problem statement.\n    test_cases_params = [\n        # Test Case A: Balanced regime\n        {\n            'e': {'tau_m': 0.02, 'tau_ref': 0.002, 'V_r': 0.0, 'V_theta': 1.0, 'K_ee': 800, 'K_ei': 200, 'J_ee': 0.001, 'J_ei': -0.004, 'mu0': 0.6, 'sigma0': 0.3},\n            'i': {'tau_m': 0.02, 'tau_ref': 0.002, 'V_r': 0.0, 'V_theta': 1.0, 'K_ie': 800, 'K_ii': 200, 'J_ie': 0.001, 'J_ii': -0.004, 'mu0': 0.6, 'sigma0': 0.3}\n        },\n        # Test Case B: Weak inhibition\n        {\n            'e': {'tau_m': 0.02, 'tau_ref': 0.002, 'V_r': 0.0, 'V_theta': 1.0, 'K_ee': 800, 'K_ei': 200, 'J_ee': 0.001, 'J_ei': -0.001, 'mu0': 0.6, 'sigma0': 0.3},\n            'i': {'tau_m': 0.02, 'tau_ref': 0.002, 'V_r': 0.0, 'V_theta': 1.0, 'K_ie': 800, 'K_ii': 200, 'J_ie': 0.001, 'J_ii': -0.001, 'mu0': 0.6, 'sigma0': 0.3}\n        },\n        # Test Case C: Strong inhibition, low baseline\n        {\n            'e': {'tau_m': 0.02, 'tau_ref': 0.002, 'V_r': 0.0, 'V_theta': 1.0, 'K_ee': 800, 'K_ei': 200, 'J_ee': 0.001, 'J_ei': -0.008, 'mu0': 0.3, 'sigma0': 0.15},\n            'i': {'tau_m': 0.02, 'tau_ref': 0.002, 'V_r': 0.0, 'V_theta': 1.0, 'K_ie': 800, 'K_ii': 200, 'J_ie': 0.001, 'J_ii': -0.008, 'mu0': 0.3, 'sigma0': 0.15}\n        }\n    ]\n\n    results = []\n    for params in test_cases_params:\n        solution = solve_for_case(params)\n        results.append(solution)\n\n    # Format the final output string exactly as specified.\n    # Each inner list is [nu_e, nu_i], rounded to three decimal places.\n    # The final string is a list of these lists, with no extra spaces.\n    formatted_results = [f\"[{res[0]:.3f},{res[1]:.3f}]\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Scientific models are built on approximations, and a key part of theoretical work is understanding their limits. The diffusion approximation simplifies complex synaptic shot noise into a simpler Gaussian process defined only by a mean and variance. This exercise provides a rigorous method to quantify the error of this fundamental assumption by deriving higher-order Kramers-Moyal coefficients and calculating the input's skewness, helping you determine when the diffusion approximation is a valid and accurate tool. ",
            "id": "4051248",
            "problem": "Consider a single-compartment neuron receiving a large number of statistically independent synaptic inputs that can be aggregated into a single homogeneous Poisson point process (PPP) of rate $\\nu$. Each presynaptic spike causes a postsynaptic current jump of amplitude $J$ followed by an exponential decay with synaptic time constant $\\tau_s$. The total synaptic current $I(t)$ obeys the linear shot-noise dynamics\n$$\n\\frac{d I(t)}{dt} \\;=\\; -\\frac{I(t)}{\\tau_s} \\;+\\; J \\sum_k \\delta\\!\\left(t-t_k\\right),\n$$\nwhere $\\{t_k\\}$ are the Poisson event times. Equivalently, in differential form with respect to the counting process $N_t$ of the PPP,\n$$\ndI(t) \\;=\\; -\\frac{I(t)}{\\tau_s}\\,dt \\;+\\; J\\,dN_t,\n$$\nwith $\\mathbb{E}[dN_t] = \\nu\\,dt$.\n\nStarting from the definition of the Kramers–Moyal expansion (KME), the $n$-th Kramers–Moyal coefficient $a_n(I)$ is\n$$\na_n(I) \\;=\\; \\lim_{dt\\to 0} \\frac{1}{dt}\\,\\mathbb{E}\\!\\left[ \\big(\\Delta I\\big)^{n} \\,\\big|\\, I(t)=I \\right],\n$$\nwhere $\\Delta I \\equiv I(t+dt)-I(t)$.\n\n1) Using only the dynamics above and the properties of a homogeneous Poisson process in an infinitesimal interval, derive the first three Kramers–Moyal coefficients $a_1(I)$, $a_2(I)$, and $a_3(I)$.\n\n2) In the stationary regime, the synaptic current can be represented as the linear superposition\n$$\nI(t) \\;=\\; \\sum_{t_k < t} J \\exp\\!\\left(-\\frac{t-t_k}{\\tau_s}\\right).\n$$\nUsing well-tested results for Poisson shot noise, compute the first three equal-time cumulants $\\kappa_1$, $\\kappa_2$, and $\\kappa_3$ of $I(t)$.\n\n3) The diffusion approximation neglects third and higher Kramers–Moyal terms, hence an error indicator is the standardized third cumulant (skewness magnitude)\n$$\n\\gamma \\;\\equiv\\; \\frac{|\\kappa_3|}{\\kappa_2^{3/2}}.\n$$\nExpress $\\gamma$ as a closed-form analytic expression in terms of $J$, $\\nu$, and $\\tau_s$. Provide your final answer as a single symbolic expression. No numerical evaluation or rounding is required, and no units should be included in the final expression.",
            "solution": "We begin from the stochastic differential form\n$$\ndI(t) \\;=\\; -\\frac{I(t)}{\\tau_s}\\,dt \\;+\\; J\\,dN_t,\n$$\nwhere $N_t$ is the counting process of a homogeneous Poisson point process (PPP) with rate $\\nu$. Over an infinitesimal interval $[t,t+dt)$, the increment is\n$$\n\\Delta I \\;=\\; -\\frac{I}{\\tau_s}\\,dt \\;+\\; J\\,\\Delta N,\n$$\nwith $\\Delta N \\equiv N_{t+dt}-N_t$. For a homogeneous PPP, $\\Delta N$ has distribution $\\mathbb{P}(\\Delta N=1)=\\nu\\,dt + o(dt)$, $\\mathbb{P}(\\Delta N=0)=1-\\nu\\,dt+o(dt)$, and $\\mathbb{P}(\\Delta N \\ge 2)=o(dt)$.\n\n1) Computation of the first three Kramers–Moyal coefficients.\n\nBy definition,\n$$\na_n(I) \\;=\\; \\lim_{dt\\to 0}\\frac{1}{dt}\\,\\mathbb{E}\\!\\left[ \\big(\\Delta I\\big)^n \\,\\big|\\, I(t)=I \\right].\n$$\nWe evaluate these up to terms $O(dt)$.\n\nFor $n=1$,\n$$\n\\mathbb{E}[\\Delta I \\mid I] \\;=\\; -\\frac{I}{\\tau_s}\\,dt \\;+\\; J\\,\\mathbb{E}[\\Delta N] \\;=\\; -\\frac{I}{\\tau_s}\\,dt \\;+\\; J\\,\\nu\\,dt,\n$$\nthus\n$$\na_1(I) \\;=\\; -\\frac{I}{\\tau_s} \\;+\\; J\\,\\nu.\n$$\n\nFor $n=2$, expand $\\Delta I = -\\frac{I}{\\tau_s}dt + J \\Delta N$. Products involving $dt$ and $\\Delta N$ are $O(dt^2)$ in expectation and can be neglected at leading order. The $O(dt)$ contribution arises from $(J\\Delta N)^2$ because $(dt)^2$ terms vanish in the $dt\\to 0$ limit. Since $\\Delta N^2 = \\Delta N$ for $\\Delta N \\in \\{0,1\\}$ to $O(dt)$,\n$$\n\\mathbb{E}\\!\\left[(\\Delta I)^2 \\mid I\\right] \\;=\\; J^2\\,\\mathbb{E}[\\Delta N] \\;+\\; o(dt) \\;=\\; J^2 \\nu\\,dt \\;+\\; o(dt),\n$$\nhence\n$$\na_2(I) \\;=\\; J^2 \\nu.\n$$\n\nFor $n=3$, by the same logic the leading $O(dt)$ contribution is $(J\\Delta N)^3$, and since $\\Delta N^3=\\Delta N$ for $\\Delta N\\in\\{0,1\\}$ to $O(dt)$,\n$$\n\\mathbb{E}\\!\\left[(\\Delta I)^3 \\mid I\\right] \\;=\\; J^3\\,\\mathbb{E}[\\Delta N] \\;+\\; o(dt) \\;=\\; J^3 \\nu\\,dt \\;+\\; o(dt),\n$$\nso\n$$\na_3(I) \\;=\\; J^3 \\nu.\n$$\n\nThus, up to third order,\n$$\na_1(I) \\;=\\; -\\frac{I}{\\tau_s} + J\\nu,\\qquad a_2(I) \\;=\\; J^2 \\nu,\\qquad a_3(I) \\;=\\; J^3 \\nu.\n$$\n\n2) Stationary equal-time cumulants of $I(t)$.\n\nIn steady state, $I(t)$ can be written as a Poisson shot noise\n$$\nI(t) \\;=\\; \\sum_{t_k < t} h(t-t_k),\\qquad h(s) \\;=\\; J \\exp\\!\\left(-\\frac{s}{\\tau_s}\\right)\\,\\mathbf{1}_{\\{s\\ge 0\\}}.\n$$\nFor a homogeneous PPP with rate $\\nu$ and deterministic kernel $h$, the $n$-th equal-time cumulant $\\kappa_n$ of $I(t)$ is given by the well-tested Campbell theorem for cumulants of Poisson shot noise:\n$$\n\\kappa_n \\;=\\; \\nu \\int_{0}^{\\infty} \\big(h(s)\\big)^n\\,ds.\n$$\nFor the exponential kernel,\n$$\n\\int_{0}^{\\infty} \\big(h(s)\\big)^n\\,ds \\;=\\; \\int_{0}^{\\infty} J^n \\exp\\!\\left(-\\frac{n s}{\\tau_s}\\right)\\,ds \\;=\\; J^n \\frac{\\tau_s}{n}.\n$$\nTherefore,\n$$\n\\kappa_n \\;=\\; \\nu\\,J^n\\,\\frac{\\tau_s}{n}.\n$$\nIn particular,\n$$\n\\kappa_1 \\;=\\; \\nu J \\tau_s,\\qquad \\kappa_2 \\;=\\; \\nu J^2 \\frac{\\tau_s}{2},\\qquad \\kappa_3 \\;=\\; \\nu J^3 \\frac{\\tau_s}{3}.\n$$\n\n3) Standardized third cumulant (skewness magnitude) as an error indicator.\n\nDefine\n$$\n\\gamma \\;\\equiv\\; \\frac{|\\kappa_3|}{\\kappa_2^{3/2}}.\n$$\nSubstituting the expressions above,\n$$\n\\gamma \\;=\\; \\frac{\\left|\\nu J^3 \\frac{\\tau_s}{3}\\right|}{\\left(\\nu J^2 \\frac{\\tau_s}{2}\\right)^{3/2}}\n\\;=\\; \\frac{|J|^3 \\nu \\tau_s / 3}{|J|^3 (\\nu \\tau_s)^{3/2} / 2^{3/2}}\n\\;=\\; \\frac{2^{3/2}}{3}\\,\\frac{1}{\\sqrt{\\nu \\tau_s}}.\n$$\nThus the diffusion approximation error indicator based on the smallness of the third cumulant is\n$$\n\\gamma \\;=\\; \\frac{2^{3/2}}{3}\\,(\\nu \\tau_s)^{-1/2}.\n$$\nNotably, the dependence on $J$ cancels in the standardized measure; only the product $\\nu \\tau_s$ controls the magnitude, with larger $\\nu \\tau_s$ yielding a smaller skewness and hence a better diffusion approximation.",
            "answer": "$$\\boxed{\\frac{2^{3/2}}{3}\\,(\\nu\\,\\tau_s)^{-1/2}}$$"
        }
    ]
}