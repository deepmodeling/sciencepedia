{
    "hands_on_practices": [
        {
            "introduction": "在机器学习中，许多优化问题都可以从概率性的贝叶斯视角得到优雅的解释，稀疏编码也不例外。本练习将引导您通过构建一个最大后验概率（MAP）估计问题，来推导出广泛使用的 $L_1$ 正则化最小二乘目标函数。这个过程将确定性的代价函数与一个生成模型联系起来，该模型对噪声（高斯分布）和信号结构（拉普拉斯先验）做出了特定假设 。",
            "id": "4058323",
            "problem": "在一个神经形态稀疏编码前端中，输入向量 $\\mathbf{x} \\in \\mathbb{R}^{n}$ 由一个带有加性噪声的线性生成过程建模，即 $\\mathbf{x} = \\mathbf{D}\\mathbf{a} + \\boldsymbol{\\varepsilon}$，其中 $\\mathbf{D} \\in \\mathbb{R}^{n \\times m}$ 是一个固定的字典，$\\mathbf{a} \\in \\mathbb{R}^{m}$ 是一个系数向量，$\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^{2}\\mathbf{I}_{n})$ 是方差为 $\\sigma^{2}  0$ 的零均值高斯噪声。假设系数服从一个促进稀疏性的先验分布，其中 $\\mathbf{a}$ 的分量是独立同分布的，服从由 $\\beta  0$ 参数化的拉普拉斯（双指数）分布，即 $p(\\mathbf{a}) \\propto \\exp\\!\\big(-\\beta \\lVert \\mathbf{a} \\rVert_{1}\\big)$。使用 Bayes 法则以及似然和先验的基本定义，推导给定 $\\mathbf{x}$ 时 $\\mathbf{a}$ 的最大后验（MAP）估计量，并证明它在不改变最小化器的前提下，在相加一个常数和乘以一个正常数的范围内，等价于最小化一个形式如下的 $\\ell_{1}$-正则化重构误差\n$$\n\\frac{1}{2}\\lVert \\mathbf{x} - \\mathbf{D}\\mathbf{a} \\rVert_{2}^{2} + \\lambda \\lVert \\mathbf{a} \\rVert_{1}.\n$$\n产生这种等价性的正则化权重 $\\lambda$ 的封闭形式解析表达式是什么（用 $\\sigma$ 和 $\\beta$ 表示）？你的最终答案必须是单个解析表达式，并且不得包含单位。",
            "solution": "目标是求出给定输入向量 $\\mathbf{x}$ 时系数向量 $\\mathbf{a}$ 的最大后验（MAP）估计量，并从此推导中确定正则化权重 $\\lambda$ 的表达式。\n\n$\\mathbf{a}$ 的 MAP 估计，记为 $\\hat{\\mathbf{a}}_{\\text{MAP}}$，是使后验概率分布 $p(\\mathbf{a} | \\mathbf{x})$ 最大化的 $\\mathbf{a}$ 的值。根据 Bayes 法则，后验概率由下式给出：\n$$\np(\\mathbf{a} | \\mathbf{x}) = \\frac{p(\\mathbf{x} | \\mathbf{a}) p(\\mathbf{a})}{p(\\mathbf{x})}\n$$\n项 $p(\\mathbf{x})$ 是数据的边际概率，它不依赖于 $\\mathbf{a}$。因此，关于 $\\mathbf{a}$ 最大化后验概率 $p(\\mathbf{a} | \\mathbf{x})$ 等价于最大化分子 $p(\\mathbf{x} | \\mathbf{a}) p(\\mathbf{a})$。\n$$\n\\hat{\\mathbf{a}}_{\\text{MAP}} = \\arg\\max_{\\mathbf{a}} p(\\mathbf{a} | \\mathbf{x}) = \\arg\\max_{\\mathbf{a}} p(\\mathbf{x} | \\mathbf{a}) p(\\mathbf{a})\n$$\n由于自然对数 $\\ln(\\cdot)$ 是一个严格单调递增的函数，最大化一个函数等价于最大化其对数。这将概率的乘积简化为对数概率的和：\n$$\n\\hat{\\mathbf{a}}_{\\text{MAP}} = \\arg\\max_{\\mathbf{a}} \\ln\\big(p(\\mathbf{x} | \\mathbf{a}) p(\\mathbf{a})\\big) = \\arg\\max_{\\mathbf{a}} \\big(\\ln p(\\mathbf{x} | \\mathbf{a}) + \\ln p(\\mathbf{a})\\big)\n$$\n此外，最大化一个函数等价于最小化其负值。因此，MAP 估计问题可以表示为一个最小化问题：\n$$\n\\hat{\\mathbf{a}}_{\\text{MAP}} = \\arg\\min_{\\mathbf{a}} \\big(-\\ln p(\\mathbf{x} | \\mathbf{a}) - \\ln p(\\mathbf{a})\\big)\n$$\n我们现在推导这个目标函数中的两项，$-\\ln p(\\mathbf{x} | \\mathbf{a})$ 和 $-\\ln p(\\mathbf{a})$。\n\n首先，我们考虑似然项 $p(\\mathbf{x} | \\mathbf{a})$。问题陈述了生成模型为 $\\mathbf{x} = \\mathbf{D}\\mathbf{a} + \\boldsymbol{\\varepsilon}$，其中噪声 $\\boldsymbol{\\varepsilon}$ 从一个零均值、对角协方差矩阵的多元高斯分布中抽取，即 $\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^{2}\\mathbf{I}_{n})$。这意味着给定 $\\mathbf{a}$，观测值 $\\mathbf{x}$ 服从均值为 $\\mathbf{D}\\mathbf{a}$、协方差为 $\\sigma^{2}\\mathbf{I}_{n}$ 的高斯分布。似然的概率密度函数（PDF）是：\n$$\np(\\mathbf{x} | \\mathbf{a}) = \\frac{1}{(2\\pi \\sigma^{2})^{n/2}} \\exp\\left( -\\frac{1}{2\\sigma^{2}} (\\mathbf{x} - \\mathbf{D}\\mathbf{a})^{T}(\\mathbf{x} - \\mathbf{D}\\mathbf{a}) \\right)\n$$\n项 $(\\mathbf{x} - \\mathbf{D}\\mathbf{a})^{T}(\\mathbf{x} - \\mathbf{D}\\mathbf{a})$ 是重构误差的平方欧几里得范数（或 $\\ell_2$-范数），即 $\\lVert \\mathbf{x} - \\mathbf{D}\\mathbf{a} \\rVert_{2}^{2}$。所以，似然是：\n$$\np(\\mathbf{x} | \\mathbf{a}) = \\frac{1}{(2\\pi \\sigma^{2})^{n/2}} \\exp\\left( -\\frac{1}{2\\sigma^{2}} \\lVert \\mathbf{x} - \\mathbf{D}\\mathbf{a} \\rVert_{2}^{2} \\right)\n$$\n因此，负对数似然是：\n$$\n-\\ln p(\\mathbf{x} | \\mathbf{a}) = -\\ln\\left( \\frac{1}{(2\\pi \\sigma^{2})^{n/2}} \\right) - \\left( -\\frac{1}{2\\sigma^{2}} \\lVert \\mathbf{x} - \\mathbf{D}\\mathbf{a} \\rVert_{2}^{2} \\right) = \\frac{n}{2}\\ln(2\\pi \\sigma^{2}) + \\frac{1}{2\\sigma^{2}} \\lVert \\mathbf{x} - \\mathbf{D}\\mathbf{a} \\rVert_{2}^{2}\n$$\n\n接下来，我们考虑先验项 $p(\\mathbf{a})$。问题陈述了系数的先验分布与 $\\exp(-\\beta \\lVert \\mathbf{a} \\rVert_{1})$ 成正比，其中 $\\lVert \\mathbf{a} \\rVert_{1} = \\sum_{i=1}^{m} |a_{i}|$ 是 $\\ell_1$-范数。我们可以将其写为 $p(\\mathbf{a}) = C \\exp(-\\beta \\lVert \\mathbf{a} \\rVert_{1})$，其中 $C$ 是一个不依赖于 $\\mathbf{a}$ 的归一化常数。负对数先验是：\n$$\n-\\ln p(\\mathbf{a}) = -\\ln\\left( C \\exp(-\\beta \\lVert \\mathbf{a} \\rVert_{1}) \\right) = -\\ln(C) + \\beta \\lVert \\mathbf{a} \\rVert_{1}\n$$\n\n结合负对数似然和负对数先验，我们得到要最小化的 MAP 目标函数 $J(\\mathbf{a})$：\n$$\nJ(\\mathbf{a}) = -\\ln p(\\mathbf{x} | \\mathbf{a}) - \\ln p(\\mathbf{a}) = \\left(\\frac{1}{2\\sigma^{2}} \\lVert \\mathbf{x} - \\mathbf{D}\\mathbf{a} \\rVert_{2}^{2} + \\frac{n}{2}\\ln(2\\pi \\sigma^{2})\\right) + \\left(\\beta \\lVert \\mathbf{a} \\rVert_{1} - \\ln(C)\\right)\n$$\n为了找到最小化器 $\\hat{\\mathbf{a}}_{\\text{MAP}}$，我们可以舍去任何相对于 $\\mathbf{a}$ 是常数的项。项 $\\frac{n}{2}\\ln(2\\pi \\sigma^{2})$ 和 $-\\ln(C)$ 就是这样的常数。因此，最小化问题等价于：\n$$\n\\hat{\\mathbf{a}}_{\\text{MAP}} = \\arg\\min_{\\mathbf{a}} \\left( \\frac{1}{2\\sigma^{2}} \\lVert \\mathbf{x} - \\mathbf{D}\\mathbf{a} \\rVert_{2}^{2} + \\beta \\lVert \\mathbf{a} \\rVert_{1} \\right)\n$$\n问题要求证明这等价于最小化一个形式为 $\\frac{1}{2}\\lVert \\mathbf{x} - \\mathbf{D}\\mathbf{a} \\rVert_{2}^{2} + \\lambda \\lVert \\mathbf{a} \\rVert_{1}$ 的目标函数。我们可以将我们推导出的目标函数乘以一个正常数，而不会改变最小值的位置。由于给定 $\\sigma^{2}  0$，我们可以乘以 $\\sigma^{2}$：\n$$\n\\hat{\\mathbf{a}}_{\\text{MAP}} = \\arg\\min_{\\mathbf{a}} \\left[ \\sigma^{2} \\left( \\frac{1}{2\\sigma^{2}} \\lVert \\mathbf{x} - \\mathbf{D}\\mathbf{a} \\rVert_{2}^{2} + \\beta \\lVert \\mathbf{a} \\rVert_{1} \\right) \\right]\n$$\n$$\n\\hat{\\mathbf{a}}_{\\text{MAP}} = \\arg\\min_{\\mathbf{a}} \\left( \\frac{1}{2} \\lVert \\mathbf{x} - \\mathbf{D}\\mathbf{a} \\rVert_{2}^{2} + \\sigma^{2}\\beta \\lVert \\mathbf{a} \\rVert_{1} \\right)\n$$\n这个表达式现在是所要求的形式了。通过将其与目标函数 $\\frac{1}{2}\\lVert \\mathbf{x} - \\mathbf{D}\\mathbf{a} \\rVert_{2}^{2} + \\lambda \\lVert \\mathbf{a} \\rVert_{1}$ 进行比较，我们可以确定正则化权重 $\\lambda$。\n项 $\\frac{1}{2}\\lVert \\mathbf{x} - \\mathbf{D}\\mathbf{a} \\rVert_{2}^{2}$ 是重构误差（数据保真项），项 $\\lambda \\lVert \\mathbf{a} \\rVert_{1}$ 是 $\\ell_1$-正则化（促进稀疏性）项。\n通过直接比较，我们发现 $\\lambda$ 和概率模型参数之间的关系：\n$$\n\\lambda = \\sigma^{2}\\beta\n$$\n此推导表明，具有高斯噪声和拉普拉斯先验的线性生成模型的 MAP 估计量等价于求解一个 $\\ell_1$-正则化最小二乘问题，该问题也被称为 LASSO（最小绝对收缩和选择算子）或基追踪降噪。正则化参数 $\\lambda$ 将噪声的方差 $\\sigma^2$ 与促进稀疏性的先验的尺度参数 $\\beta$ 直接联系起来。",
            "answer": "$$\n\\boxed{\\sigma^{2}\\beta}\n$$"
        },
        {
            "introduction": "在找到一个稀疏表示后，拥有一个有原则的方法来衡量其“稀疏程度”至关重要。本练习介绍了一种标准的稀疏性指数，并要求您从基本的范数属性推导出它。通过将其应用于一个样本活动向量，您将具体理解该度量如何捕捉神经群体中活动的集中程度 。",
            "id": "4058335",
            "problem": "考虑一个神经形态编码器，它为 $m$ 个并行神经元生成一个非负的群体活动向量 $a \\in \\mathbb{R}^{m}$。在群体稀疏编码中，需要一个稀疏性泛函 $s(a)$，该泛函需遵循以下约束，这些约束植根于标准的范数属性和广为接受的群体稀疏性准则：(i) $s(a)$ 对正向缩放不变，即对于所有 $\\alpha  0$ 都有 $s(\\alpha a) = s(a)$；(ii) $s(a)$ 是连续的，并且仅通过捕捉群体激活幅度的范数依赖于 $a$；(iii) 当 $a$ 的所有分量相等（最大分布活动）时，$s(a)$ 达到其最小值；当 $a$ 中只有一个分量非零（最大集中活动）时，$s(a)$ 达到其最大值。从 $\\ell_{1}$ 范数 $\\lVert a \\rVert_{1} = \\sum_{i=1}^{m} |a_{i}|$ 和欧几里得 $\\ell_{2}$ 范数 $\\lVert a \\rVert_{2} = \\sqrt{\\sum_{i=1}^{m} a_{i}^{2}}$ 的核心定义以及 Cauchy–Schwarz 不等式出发，推导出一个尺度不变的稀疏性指数 $s(a)$，该指数将比率 $\\lVert a \\rVert_{1} / \\lVert a \\rVert_{2}$ 仿射地映射到单位区间上，且与 (i)–(iii) 一致。然后，对于 $m = 5$ 的特定情况以及\n$$\na = \\begin{pmatrix}\n\\frac{3}{5} \\\\\n0 \\\\\n0 \\\\\n\\frac{4}{5} \\\\\n0\n\\end{pmatrix},\n$$\n计算 $s(a)$ 的精确解析表达式。请用单一的封闭形式解析表达式表示您的最终答案，不要进行近似。此外，请仅使用属性 (i)–(iii) 和范数不等式，而不进行数值近似，简要解释该值在此编码器中群体稀疏性方面的意义。",
            "solution": "我们的任务是为一个非负的群体活动向量 $a \\in \\mathbb{R}^{m}$（其中对所有 $i=1, \\dots, m$ 都有 $a_i \\ge 0$）推导一个尺度不变的稀疏性指数 $s(a)$。该指数必须将比率 $x(a) = \\frac{\\lVert a \\rVert_{1}}{\\lVert a \\rVert_{2}}$ 仿射地映射到单位区间 $[0, 1]$ 上，并与给定的准则一致。\n\n首先，我们确定比率 $x(a) = \\frac{\\lVert a \\rVert_{1}}{\\lVert a \\rVert_{2}}$ 的范围。由于所有 $a_i \\ge 0$，$\\ell_1$ 范数为 $\\lVert a \\rVert_{1} = \\sum_{i=1}^{m} a_i$。$\\ell_2$ 范数为 $\\lVert a \\rVert_{2} = \\sqrt{\\sum_{i=1}^{m} a_{i}^{2}}$。比率 $x(a)$ 对正向缩放是不变的：对于任何 $\\alpha  0$，$x(\\alpha a) = \\frac{\\lVert \\alpha a \\rVert_{1}}{\\lVert \\alpha a \\rVert_{2}} = \\frac{\\alpha \\lVert a \\rVert_{1}}{\\alpha \\lVert a \\rVert_{2}} = x(a)$，这满足约束 (i)。\n\n为了找到 $x(a)$ 的界限，我们使用 Cauchy–Schwarz 不等式。我们定义两个向量 $u = (a_1, a_2, \\dots, a_m)$ 和 $v = (1, 1, \\dots, 1)$。Cauchy-Schwarz 不等式表明 $|\\langle u, v \\rangle| \\le \\lVert u \\rVert_2 \\lVert v \\rVert_2$。\n在我们的情况下，$\\langle u, v \\rangle = \\sum_{i=1}^{m} a_i \\cdot 1 = \\sum_{i=1}^{m} a_i = \\lVert a \\rVert_1$。\n$u$ 的范数是 $\\lVert u \\rVert_2 = \\lVert a \\rVert_2$。\n$v$ 的范数是 $\\lVert v \\rVert_2 = \\sqrt{\\sum_{i=1}^{m} 1^2} = \\sqrt{m}$。\n将这些代入不等式，我们得到 $\\lVert a \\rVert_1 \\le \\lVert a \\rVert_2 \\sqrt{m}$。\n两边除以 $\\lVert a \\rVert_2$（假设 $a$ 不是零向量），我们找到比率的上限：\n$$\n\\frac{\\lVert a \\rVert_1}{\\lVert a \\rVert_2} \\le \\sqrt{m}\n$$\n等号成立当且仅当一个向量是另一个向量的标量倍，即对于某个标量 $c$ 有 $u = c v$。这意味着对于所有 $i$ 都有 $a_i = c \\cdot 1$，所以 $a$ 的所有分量都相等。根据约束 (iii)，这对应于最小稀疏性。因此，比率的最大值 $x_{max} = \\sqrt{m}$ 对应于稀疏性指数 $s(a)$ 的最小值。\n\n接下来，我们求比率 $x(a)$ 的最小值。考虑 $\\ell_1$ 范数的平方：$(\\lVert a \\rVert_1)^2 = (\\sum_{i=1}^{m} a_i)^2 = \\sum_{i=1}^{m} a_i^2 + 2\\sum_{i  j} a_i a_j$。由于 $a_i \\ge 0$，交叉项 $a_i a_j$ 是非负的。\n因此，$(\\lVert a \\rVert_1)^2 \\ge \\sum_{i=1}^{m} a_i^2 = (\\lVert a \\rVert_2)^2$。\n两边取平方根，我们得到 $\\lVert a \\rVert_1 \\ge \\lVert a \\rVert_2$。\n这意味着比率的最小值为：\n$$\n\\frac{\\lVert a \\rVert_1}{\\lVert a \\rVert_2} \\ge 1\n$$\n等号成立当且仅当所有交叉项均为零，即对于所有 $i \\ne j$ 都有 $a_i a_j = 0$。这要求至多只有一个分量 $a_i$ 非零。根据约束 (iii)，这种最大集中活动的情况对应于最大稀疏性。因此，比率的最小值 $x_{min} = 1$ 对应于稀疏性指数 $s(a)$ 的最大值。\n\n比率 $x(a)$ 的范围是 $[1, \\sqrt{m}]$。我们需要定义一个仿射映射 $s(x) = A x + B$，将这个区间映射到单位区间 $[0, 1]$，使得该映射与最小和最大稀疏性的定义一致。\n最大稀疏性 ($s(a)=1$) 发生在 $x(a)=1$ 时。\n最小稀疏性 ($s(a)=0$) 发生在 $x(a)=\\sqrt{m}$ 时。\n我们为系数 $A$ 和 $B$ 建立一个二元线性方程组：\n$$\n\\begin{cases}\nA \\cdot 1 + B = 1 \\\\\nA \\cdot \\sqrt{m} + B = 0\n\\end{cases}\n$$\n用第一个方程减去第二个方程，得到 $A(1 - \\sqrt{m}) = 1$，所以 $A = \\frac{1}{1 - \\sqrt{m}}$。\n将 $A$ 代入第一个方程：$B = 1 - A = 1 - \\frac{1}{1 - \\sqrt{m}} = \\frac{1 - \\sqrt{m} - 1}{1 - \\sqrt{m}} = \\frac{-\\sqrt{m}}{1 - \\sqrt{m}} = \\frac{\\sqrt{m}}{\\sqrt{m} - 1}$。\n因此，稀疏性指数为：\n$$\ns(a) = \\left(\\frac{1}{1 - \\sqrt{m}}\\right) \\frac{\\lVert a \\rVert_1}{\\lVert a \\rVert_2} + \\frac{\\sqrt{m}}{\\sqrt{m} - 1} = \\frac{-\\frac{\\lVert a \\rVert_1}{\\lVert a \\rVert_2} + \\sqrt{m}}{\\sqrt{m} - 1}\n$$\n$$\ns(a) = \\frac{\\sqrt{m} - \\frac{\\lVert a \\rVert_{1}}{\\lVert a \\rVert_{2}}}{\\sqrt{m} - 1}\n$$\n这个公式满足所有给定的约束。\n\n现在，我们计算在 $m=5$ 且 $a = (\\frac{3}{5}, 0, 0, \\frac{4}{5}, 0)^T$ 的特定情况下的 $s(a)$ 值。\n首先，我们计算 $a$ 的范数：\n$$\n\\lVert a \\rVert_{1} = \\frac{3}{5} + 0 + 0 + \\frac{4}{5} + 0 = \\frac{7}{5}\n$$\n$$\n\\lVert a \\rVert_{2} = \\sqrt{\\left(\\frac{3}{5}\\right)^2 + 0^2 + 0^2 + \\left(\\frac{4}{5}\\right)^2 + 0^2} = \\sqrt{\\frac{9}{25} + \\frac{16}{25}} = \\sqrt{\\frac{25}{25}} = \\sqrt{1} = 1\n$$\n比率为 $\\frac{\\lVert a \\rVert_1}{\\lVert a \\rVert_2} = \\frac{7/5}{1} = \\frac{7}{5}$。\n将 $m=5$ 和这个比率代入 $s(a)$ 的公式：\n$$\ns(a) = \\frac{\\sqrt{5} - \\frac{7}{5}}{\\sqrt{5} - 1}\n$$\n为了将其表示为单一的封闭形式表达式，我们可以将分母有理化：\n$$\ns(a) = \\frac{\\sqrt{5} - \\frac{7}{5}}{\\sqrt{5} - 1} \\times \\frac{\\sqrt{5} + 1}{\\sqrt{5} + 1} = \\frac{(\\sqrt{5})(\\sqrt{5}) + \\sqrt{5} - \\frac{7}{5}\\sqrt{5} - \\frac{7}{5}}{(\\sqrt{5})^2 - 1^2}\n$$\n$$\ns(a) = \\frac{5 + \\sqrt{5}\\left(1 - \\frac{7}{5}\\right) - \\frac{7}{5}}{5 - 1} = \\frac{\\frac{25}{5} - \\frac{7}{5} + \\sqrt{5}\\left(\\frac{5}{5} - \\frac{7}{5}\\right)}{4}\n$$\n$$\ns(a) = \\frac{\\frac{18}{5} - \\frac{2}{5}\\sqrt{5}}{4} = \\frac{18 - 2\\sqrt{5}}{20} = \\frac{9 - \\sqrt{5}}{10}\n$$\n关于解释，值 $s(a)$ 表示群体稀疏度的程度。$s(a)=1$ 表示最大稀疏性（只有一个神经元活跃），而 $s(a)=0$ 表示最小稀疏性或最大密集活动（所有神经元同样活跃）。给定的向量 $a$ 在五个神经元中有两个是活跃的。这是一个中间情况。比率 $\\frac{\\lVert a \\rVert_1}{\\lVert a \\rVert_2} = \\frac{7}{5}$ 远比其可能的最大值 $\\sqrt{5}$ 更接近其可能的最小值 $1$，这表明活动模式远比最大密集情况更接近最大稀疏情况。因此，得到的稀疏性指数 $s(a) = \\frac{9-\\sqrt{5}}{10}$ 是一个介于 $0$ 和 $1$ 之间且显著更接近 $1$ 的值，反映了编码器输出中高度的群体稀疏性。",
            "answer": "$$\n\\boxed{\\frac{9 - \\sqrt{5}}{10}}\n$$"
        },
        {
            "introduction": "为了以一种类似大脑的方式解决稀疏编码的优化问题，需要一个动态的、分布式的算法。局部竞争算法（LCA）提供了这样一个神经元上可信的动态模型。本实践将让您执行LCA的单个更新步骤，亲身感受膜电位如何通过竞争性相互作用进行演化，从而产生稀疏的活动模式 。",
            "id": "4058373",
            "problem": "考虑一个用于稀疏编码的局部竞争算法（LCA），该算法基于一个线性生成模型，其字典为 $D \\in \\mathbb{R}^{m \\times n}$，输入为 $x \\in \\mathbb{R}^{m}$。LCA 的连续时间膜电位动力学由以下常微分方程定义\n$$\n\\tau \\,\\frac{d u}{d t} \\;=\\; -\\,u \\;+\\; D^{\\top} x \\;-\\; \\big(D^{\\top} D - I\\big)\\,a,\n$$\n其中 $u \\in \\mathbb{R}^{n}$ 是膜电位向量，$a \\in \\mathbb{R}^{n}$ 是活动向量，由分量的软阈值非线性函数 $a_i \\,=\\, \\operatorname{sign}(u_i)\\max\\{|u_i|-\\lambda,\\,0\\}$ 给出，阈值为 $\\lambda  0$，$I$ 是 $n \\times n$ 的单位矩阵。使用时间步长为 $\\Delta t$ 的前向欧拉离散化来模拟从时间 $t$ 到 $t+\\Delta t$ 的单步更新：\n$$\nu^{t+1} \\;=\\; u^{t} \\;+\\; \\frac{\\Delta t}{\\tau}\\left(-\\,u^{t} \\;+\\; D^{\\top} x \\;-\\; \\big(D^{\\top} D - I\\big)\\,a^{t}\\right), \\quad a^{t} \\,=\\, \\operatorname{sign}(u^{t})\\max\\{|u^{t}|-\\lambda,\\,0\\}\\;\\text{(逐分量)}.\n$$\n设字典的维度为 $m=2$ 和 $n=3$，其列向量为\n$$\nd_{1} \\,=\\, \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix},\\quad\nd_{2} \\,=\\, \\begin{pmatrix} 0.8 \\\\ 0.6 \\end{pmatrix},\\quad\nd_{3} \\,=\\, \\begin{pmatrix} 0.6 \\\\ 0.8 \\end{pmatrix},\n$$\n因此 $D \\,=\\, \\begin{pmatrix} 1  0.8  0.6 \\\\ 0  0.6  0.8 \\end{pmatrix}$。设输入为 $x \\,=\\, \\begin{pmatrix} 1.1 \\\\ 0.7 \\end{pmatrix}$，当前膜电位为 $u^{t} \\,=\\, \\begin{pmatrix} 0.9 \\\\ 0.2 \\\\ 0.4 \\end{pmatrix}$，阈值为 $\\lambda \\,=\\, 0.3$，膜时间常数为 $\\tau \\,=\\, 2$，步长为 $\\Delta t \\,=\\, 0.5$。使用以上定义，计算单步更新后的 $u^{t+1}$ 和得到的活动 $a^{t+1}$。将最终答案表示为一个单行矩阵，按顺序 $(u^{t+1}_{1},\\,u^{t+1}_{2},\\,u^{t+1}_{3},\\,a^{t+1}_{1},\\,a^{t+1}_{2},\\,a^{t+1}_{3})$ 列出这六个条目。提供精确的小数值，不要四舍五入。",
            "solution": "问题要求对膜电位向量 $u$ 和相应的活动向量 $a$ 进行单步更新。LCA 动力学的前向欧拉离散化由下式给出：\n$$u^{t+1} = u^{t} + \\frac{\\Delta t}{\\tau}\\left(-u^{t} + D^{\\top} x - \\left(D^{\\top} D - I\\right)a^{t}\\right)$$\n活动通过一个软阈值函数与电位相关联：\n$$a_i = \\operatorname{sign}(u_i)\\max\\{|u_i|-\\lambda, 0\\}$$\n该函数逐分量应用。\n\n给定的参数如下：\n- 字典 $D = \\begin{pmatrix} 1  0.8  0.6 \\\\ 0  0.6  0.8 \\end{pmatrix}$\n- 输入向量 $x = \\begin{pmatrix} 1.1 \\\\ 0.7 \\end{pmatrix}$\n- 当前膜电位 $u^{t} = \\begin{pmatrix} 0.9 \\\\ 0.2 \\\\ 0.4 \\end{pmatrix}$\n- 阈值 $\\lambda = 0.3$\n- 时间常数 $\\tau = 2$\n- 时间步长 $\\Delta t = 0.5$\n\n我们将逐步计算所需的量。\n\n**第 1 步：计算当前活动向量 $a^t$**\n使用逐分量的软阈值函数，其中 $u^t$ 且 $\\lambda = 0.3$：\n$$a_1^t = \\operatorname{sign}(u_1^t)\\max\\{|u_1^t|-\\lambda, 0\\} = \\operatorname{sign}(0.9)\\max\\{|0.9|-0.3, 0\\} = 1 \\cdot \\max\\{0.6, 0\\} = 0.6$$\n$$a_2^t = \\operatorname{sign}(u_2^t)\\max\\{|u_2^t|-\\lambda, 0\\} = \\operatorname{sign}(0.2)\\max\\{|0.2|-0.3, 0\\} = 1 \\cdot \\max\\{-0.1, 0\\} = 0$$\n$$a_3^t = \\operatorname{sign}(u_3^t)\\max\\{|u_3^t|-\\lambda, 0\\} = \\operatorname{sign}(0.4)\\max\\{|0.4|-0.3, 0\\} = 1 \\cdot \\max\\{0.1, 0\\} = 0.1$$\n因此，当前活动向量为 $a^t = \\begin{pmatrix} 0.6 \\\\ 0 \\\\ 0.1 \\end{pmatrix}$。\n\n**第 2 步：计算驱动输入项 $D^\\top x$**\n字典的转置是 $D^\\top = \\begin{pmatrix} 1  0 \\\\ 0.8  0.6 \\\\ 0.6  0.8 \\end{pmatrix}$。\n$$D^\\top x = \\begin{pmatrix} 1  0 \\\\ 0.8  0.6 \\\\ 0.6  0.8 \\end{pmatrix} \\begin{pmatrix} 1.1 \\\\ 0.7 \\end{pmatrix} = \\begin{pmatrix} (1)(1.1) + (0)(0.7) \\\\ (0.8)(1.1) + (0.6)(0.7) \\\\ (0.6)(1.1) + (0.8)(0.7) \\end{pmatrix} = \\begin{pmatrix} 1.1 \\\\ 0.88 + 0.42 \\\\ 0.66 + 0.56 \\end{pmatrix} = \\begin{pmatrix} 1.1 \\\\ 1.3 \\\\ 1.22 \\end{pmatrix}$$\n\n**第 3 步：计算抑制矩阵项 $(D^\\top D - I)a^t$**\n首先，我们计算格拉姆矩阵 $D^\\top D$：\n$$D^\\top D = \\begin{pmatrix} 1  0 \\\\ 0.8  0.6 \\\\ 0.6  0.8 \\end{pmatrix} \\begin{pmatrix} 1  0.8  0.6 \\\\ 0  0.6  0.8 \\end{pmatrix} = \\begin{pmatrix} 1  0.8  0.6 \\\\ 0.8  0.64+0.36  0.48+0.48 \\\\ 0.6  0.48+0.48  0.36+0.64 \\end{pmatrix} = \\begin{pmatrix} 1  0.8  0.6 \\\\ 0.8  1  0.96 \\\\ 0.6  0.96  1 \\end{pmatrix}$$\n抑制矩阵是 $G = D^\\top D - I$，其中 $I$ 是 $3 \\times 3$ 的单位矩阵：\n$$G = D^\\top D - I = \\begin{pmatrix} 1  0.8  0.6 \\\\ 0.8  1  0.96 \\\\ 0.6  0.96  1 \\end{pmatrix} - \\begin{pmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 0  0  1 \\end{pmatrix} = \\begin{pmatrix} 0  0.8  0.6 \\\\ 0.8  0  0.96 \\\\ 0.6  0.96  0 \\end{pmatrix}$$\n现在，我们计算抑制项 $Ga^t$：\n$$(D^\\top D - I)a^t = \\begin{pmatrix} 0  0.8  0.6 \\\\ 0.8  0  0.96 \\\\ 0.6  0.96  0 \\end{pmatrix} \\begin{pmatrix} 0.6 \\\\ 0 \\\\ 0.1 \\end{pmatrix} = \\begin{pmatrix} (0)(0.6) + (0.8)(0) + (0.6)(0.1) \\\\ (0.8)(0.6) + (0)(0) + (0.96)(0.1) \\\\ (0.6)(0.6) + (0.96)(0) + (0)(0.1) \\end{pmatrix} = \\begin{pmatrix} 0.06 \\\\ 0.48 + 0.096 \\\\ 0.36 \\end{pmatrix} = \\begin{pmatrix} 0.06 \\\\ 0.576 \\\\ 0.36 \\end{pmatrix}$$\n\n**第 4 步：计算更新后的膜电位 $u^{t+1}$**\n我们组合更新规则的各项。缩放因子为 $\\frac{\\Delta t}{\\tau} = \\frac{0.5}{2} = 0.25$。\n括号中的表达式为：\n$$-u^{t} + D^{\\top} x - (D^{\\top} D - I)a^{t} = -\\begin{pmatrix} 0.9 \\\\ 0.2 \\\\ 0.4 \\end{pmatrix} + \\begin{pmatrix} 1.1 \\\\ 1.3 \\\\ 1.22 \\end{pmatrix} - \\begin{pmatrix} 0.06 \\\\ 0.576 \\\\ 0.36 \\end{pmatrix} = \\begin{pmatrix} -0.9 + 1.1 - 0.06 \\\\ -0.2 + 1.3 - 0.576 \\\\ -0.4 + 1.22 - 0.36 \\end{pmatrix} = \\begin{pmatrix} 0.14 \\\\ 0.524 \\\\ 0.46 \\end{pmatrix}$$\n现在我们执行更新：\n$$u^{t+1} = u^{t} + \\frac{\\Delta t}{\\tau} \\begin{pmatrix} 0.14 \\\\ 0.524 \\\\ 0.46 \\end{pmatrix} = \\begin{pmatrix} 0.9 \\\\ 0.2 \\\\ 0.4 \\end{pmatrix} + 0.25 \\begin{pmatrix} 0.14 \\\\ 0.524 \\\\ 0.46 \\end{pmatrix} = \\begin{pmatrix} 0.9 \\\\ 0.2 \\\\ 0.4 \\end{pmatrix} + \\begin{pmatrix} 0.035 \\\\ 0.131 \\\\ 0.115 \\end{pmatrix} = \\begin{pmatrix} 0.935 \\\\ 0.331 \\\\ 0.515 \\end{pmatrix}$$\n\n**第 5 步：计算新的活动向量 $a^{t+1}$**\n使用更新后的电位 $u^{t+1} = \\begin{pmatrix} 0.935 \\\\ 0.331 \\\\ 0.515 \\end{pmatrix}$ 和阈值 $\\lambda = 0.3$：\n$$a_1^{t+1} = \\operatorname{sign}(0.935)\\max\\{|0.935|-0.3, 0\\} = 1 \\cdot \\max\\{0.635, 0\\} = 0.635$$\n$$a_2^{t+1} = \\operatorname{sign}(0.331)\\max\\{|0.331|-0.3, 0\\} = 1 \\cdot \\max\\{0.031, 0\\} = 0.031$$\n$$a_3^{t+1} = \\operatorname{sign}(0.515)\\max\\{|0.515|-0.3, 0\\} = 1 \\cdot \\max\\{0.215, 0\\} = 0.215$$\n新的活动向量是 $a^{t+1} = \\begin{pmatrix} 0.635 \\\\ 0.031 \\\\ 0.215 \\end{pmatrix}$。\n\n最终答案所需的六个值是：\n$u^{t+1}_{1} = 0.935$\n$u^{t+1}_{2} = 0.331$\n$u^{t+1}_{3} = 0.515$\n$a^{t+1}_{1} = 0.635$\n$a^{t+1}_{2} = 0.031$\n$a^{t+1}_{3} = 0.215$\n这些值需要以单行矩阵的形式呈现。",
            "answer": "$$\\boxed{\\begin{pmatrix} 0.935  0.331  0.515  0.635  0.031  0.215 \\end{pmatrix}}$$"
        }
    ]
}