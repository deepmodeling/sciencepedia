{
    "hands_on_practices": [
        {
            "introduction": "The standard sparse coding objective function, with its squared error and $\\ell_{1}$ penalty, is not an arbitrary construct. This foundational exercise guides you through a derivation from first principles, revealing its identity as a Maximum a Posteriori (MAP) estimator under a Bayesian framework . By completing this practice, you will understand how assuming Gaussian noise and a Laplace prior on neural activities naturally leads to this widely used cost function, providing a powerful probabilistic justification for its form.",
            "id": "4058323",
            "problem": "In a neuromorphic sparse coding front-end, an input vector $\\mathbf{x} \\in \\mathbb{R}^{n}$ is modeled by a linear generative process with additive noise, $\\mathbf{x} = \\mathbf{D}\\mathbf{a} + \\boldsymbol{\\varepsilon}$, where $\\mathbf{D} \\in \\mathbb{R}^{n \\times m}$ is a fixed dictionary, $\\mathbf{a} \\in \\mathbb{R}^{m}$ is a coefficient vector, and $\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^{2}\\mathbf{I}_{n})$ is zero-mean Gaussian noise with variance $\\sigma^{2} > 0$. Assume a sparsity-promoting prior over coefficients in which components of $\\mathbf{a}$ are independent and identically distributed with a Laplace (double-exponential) distribution parameterized by $\\beta > 0$, that is $p(\\mathbf{a}) \\propto \\exp\\!\\big(-\\beta \\lVert \\mathbf{a} \\rVert_{1}\\big)$. Using Bayes’ rule and only fundamental definitions of likelihoods and priors, derive the Maximum a Posteriori (MAP) estimator of $\\mathbf{a}$ given $\\mathbf{x}$ and show that it is equivalent, up to additive constants and multiplication by a positive scalar that does not change the minimizer, to minimizing an $\\ell_{1}$-regularized reconstruction error of the form\n$$\n\\frac{1}{2}\\lVert \\mathbf{x} - \\mathbf{D}\\mathbf{a} \\rVert_{2}^{2} + \\lambda \\lVert \\mathbf{a} \\rVert_{1}.\n$$\nWhat is the closed-form analytic expression, in terms of $\\sigma$ and $\\beta$, for the regularization weight $\\lambda$ that yields this equivalence? Your final answer must be a single analytic expression and must not include units.",
            "solution": "The objective is to find the Maximum a Posteriori (MAP) estimator for the coefficient vector $\\mathbf{a}$ given the input vector $\\mathbf{x}$, and from this derivation, determine the expression for the regularization weight $\\lambda$.\n\nThe MAP estimate of $\\mathbf{a}$, denoted $\\hat{\\mathbf{a}}_{\\text{MAP}}$, is the value of $\\mathbf{a}$ that maximizes the posterior probability distribution $p(\\mathbf{a} | \\mathbf{x})$. According to Bayes’ rule, the posterior is given by:\n$$\np(\\mathbf{a} | \\mathbf{x}) = \\frac{p(\\mathbf{x} | \\mathbf{a}) p(\\mathbf{a})}{p(\\mathbf{x})}\n$$\nThe term $p(\\mathbf{x})$ is the marginal probability of the data, which does not depend on $\\mathbf{a}$. Therefore, maximizing the posterior $p(\\mathbf{a} | \\mathbf{x})$ with respect to $\\mathbf{a}$ is equivalent to maximizing the numerator, $p(\\mathbf{x} | \\mathbf{a}) p(\\mathbf{a})$.\n$$\n\\hat{\\mathbf{a}}_{\\text{MAP}} = \\arg\\max_{\\mathbf{a}} p(\\mathbf{a} | \\mathbf{x}) = \\arg\\max_{\\mathbf{a}} p(\\mathbf{x} | \\mathbf{a}) p(\\mathbf{a})\n$$\nSince the natural logarithm, $\\ln(\\cdot)$, is a strictly monotonically increasing function, maximizing a function is equivalent to maximizing its logarithm. This simplifies the product of probabilities into a sum of log-probabilities:\n$$\n\\hat{\\mathbf{a}}_{\\text{MAP}} = \\arg\\max_{\\mathbf{a}} \\ln\\big(p(\\mathbf{x} | \\mathbf{a}) p(\\mathbf{a})\\big) = \\arg\\max_{\\mathbf{a}} \\big(\\ln p(\\mathbf{x} | \\mathbf{a}) + \\ln p(\\mathbf{a})\\big)\n$$\nFurthermore, maximizing a function is equivalent to minimizing its negative. Thus, the MAP estimation problem can be expressed as a minimization problem:\n$$\n\\hat{\\mathbf{a}}_{\\text{MAP}} = \\arg\\min_{\\mathbf{a}} \\big(-\\ln p(\\mathbf{x} | \\mathbf{a}) - \\ln p(\\mathbf{a})\\big)\n$$\nWe now derive the two terms in this objective function, $-\\ln p(\\mathbf{x} | \\mathbf{a})$ and $-\\ln p(\\mathbf{a})$.\n\nFirst, we consider the likelihood term, $p(\\mathbf{x} | \\mathbf{a})$. The problem states the generative model is $\\mathbf{x} = \\mathbf{D}\\mathbf{a} + \\boldsymbol{\\varepsilon}$, where the noise $\\boldsymbol{\\varepsilon}$ is drawn from a zero-mean multivariate Gaussian distribution with a diagonal covariance matrix, $\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^{2}\\mathbf{I}_{n})$. This implies that given $\\mathbf{a}$, the observation $\\mathbf{x}$ is Gaussian distributed with mean $\\mathbf{D}\\mathbf{a}$ and covariance $\\sigma^{2}\\mathbf{I}_{n}$. The probability density function (PDF) of the likelihood is:\n$$\np(\\mathbf{x} | \\mathbf{a}) = \\frac{1}{(2\\pi \\sigma^{2})^{n/2}} \\exp\\left( -\\frac{1}{2\\sigma^{2}} (\\mathbf{x} - \\mathbf{D}\\mathbf{a})^{T}(\\mathbf{x} - \\mathbf{D}\\mathbf{a}) \\right)\n$$\nThe term $(\\mathbf{x} - \\mathbf{D}\\mathbf{a})^{T}(\\mathbf{x} - \\mathbf{D}\\mathbf{a})$ is the squared Euclidean norm (or $\\ell_2$-norm) of the reconstruction error, $\\lVert \\mathbf{x} - \\mathbf{D}\\mathbf{a} \\rVert_{2}^{2}$. So, the likelihood is:\n$$\np(\\mathbf{x} | \\mathbf{a}) = \\frac{1}{(2\\pi \\sigma^{2})^{n/2}} \\exp\\left( -\\frac{1}{2\\sigma^{2}} \\lVert \\mathbf{x} - \\mathbf{D}\\mathbf{a} \\rVert_{2}^{2} \\right)\n$$\nThe negative log-likelihood is therefore:\n$$\n-\\ln p(\\mathbf{x} | \\mathbf{a}) = -\\ln\\left( \\frac{1}{(2\\pi \\sigma^{2})^{n/2}} \\right) - \\left( -\\frac{1}{2\\sigma^{2}} \\lVert \\mathbf{x} - \\mathbf{D}\\mathbf{a} \\rVert_{2}^{2} \\right) = \\frac{n}{2}\\ln(2\\pi \\sigma^{2}) + \\frac{1}{2\\sigma^{2}} \\lVert \\mathbf{x} - \\mathbf{D}\\mathbf{a} \\rVert_{2}^{2}\n$$\n\nNext, we consider the prior term, $p(\\mathbf{a})$. The problem states that the prior distribution over the coefficients is proportional to $\\exp(-\\beta \\lVert \\mathbf{a} \\rVert_{1})$, where $\\lVert \\mathbf{a} \\rVert_{1} = \\sum_{i=1}^{m} |a_{i}|$ is the $\\ell_1$-norm. We can write this as $p(\\mathbf{a}) = C \\exp(-\\beta \\lVert \\mathbf{a} \\rVert_{1})$, where $C$ is a normalization constant that does not depend on $\\mathbf{a}$. The negative log-prior is:\n$$\n-\\ln p(\\mathbf{a}) = -\\ln\\left( C \\exp(-\\beta \\lVert \\mathbf{a} \\rVert_{1}) \\right) = -\\ln(C) + \\beta \\lVert \\mathbf{a} \\rVert_{1}\n$$\n\nCombining the negative log-likelihood and negative log-prior, we obtain the MAP objective function to be minimized, $J(\\mathbf{a})$:\n$$\nJ(\\mathbf{a}) = -\\ln p(\\mathbf{x} | \\mathbf{a}) - \\ln p(\\mathbf{a}) = \\left(\\frac{1}{2\\sigma^{2}} \\lVert \\mathbf{x} - \\mathbf{D}\\mathbf{a} \\rVert_{2}^{2} + \\frac{n}{2}\\ln(2\\pi \\sigma^{2})\\right) + \\left(\\beta \\lVert \\mathbf{a} \\rVert_{1} - \\ln(C)\\right)\n$$\nTo find the minimizer $\\hat{\\mathbf{a}}_{\\text{MAP}}$, we can drop any terms that are constant with respect to $\\mathbf{a}$. The terms $\\frac{n}{2}\\ln(2\\pi \\sigma^{2})$ and $-\\ln(C)$ are such constants. Thus, the minimization problem is equivalent to:\n$$\n\\hat{\\mathbf{a}}_{\\text{MAP}} = \\arg\\min_{\\mathbf{a}} \\left( \\frac{1}{2\\sigma^{2}} \\lVert \\mathbf{x} - \\mathbf{D}\\mathbf{a} \\rVert_{2}^{2} + \\beta \\lVert \\mathbf{a} \\rVert_{1} \\right)\n$$\nThe problem asks to show that this is equivalent to minimizing an objective of the form $\\frac{1}{2}\\lVert \\mathbf{x} - \\mathbf{D}\\mathbf{a} \\rVert_{2}^{2} + \\lambda \\lVert \\mathbf{a} \\rVert_{1}$. We can multiply our derived objective function by a positive scalar without changing the location of the minimum. Since $\\sigma^{2} > 0$ is given, we can multiply by $\\sigma^{2}$:\n$$\n\\hat{\\mathbf{a}}_{\\text{MAP}} = \\arg\\min_{\\mathbf{a}} \\left[ \\sigma^{2} \\left( \\frac{1}{2\\sigma^{2}} \\lVert \\mathbf{x} - \\mathbf{D}\\mathbf{a} \\rVert_{2}^{2} + \\beta \\lVert \\mathbf{a} \\rVert_{1} \\right) \\right]\n$$\n$$\n\\hat{\\mathbf{a}}_{\\text{MAP}} = \\arg\\min_{\\mathbf{a}} \\left( \\frac{1}{2} \\lVert \\mathbf{x} - \\mathbf{D}\\mathbf{a} \\rVert_{2}^{2} + \\sigma^{2}\\beta \\lVert \\mathbf{a} \\rVert_{1} \\right)\n$$\nThis expression is now in the desired form. By comparing this to the target objective function $\\frac{1}{2}\\lVert \\mathbf{x} - \\mathbf{D}\\mathbf{a} \\rVert_{2}^{2} + \\lambda \\lVert \\mathbf{a} \\rVert_{1}$, we can identify the regularization weight $\\lambda$.\nThe term $\\frac{1}{2}\\lVert \\mathbf{x} - \\mathbf{D}\\mathbf{a} \\rVert_{2}^{2}$ is the reconstruction error (data fidelity term), and the term $\\lambda \\lVert \\mathbf{a} \\rVert_{1}$ is the $\\ell_1$-regularization (sparsity-promoting) term.\nBy direct comparison, we find the relationship between $\\lambda$ and the parameters of the probabilistic model:\n$$\n\\lambda = \\sigma^{2}\\beta\n$$\nThis derivation shows that the MAP estimator for a linear generative model with Gaussian noise and a Laplace prior is equivalent to solving an $\\ell_1$-regularized least squares problem, which is also known as LASSO (Least Absolute Shrinkage and Selection Operator) or Basis Pursuit Denoising. The regularization parameter $\\lambda$ directly relates the variance of the noise, $\\sigma^2$, to the scale parameter of the sparsity-promoting prior, $\\beta$.",
            "answer": "$$\n\\boxed{\\sigma^{2}\\beta}\n$$"
        },
        {
            "introduction": "To be a useful scientific concept, \"sparsity\" must be quantifiable. This practice introduces a formal measure of population sparsity, demonstrating how it can be rigorously derived from the mathematical properties of vector norms . By deriving this index and applying it to a sample neural activity vector, you will gain a concrete tool for evaluating how efficiently a representation distributes information across a population of neurons.",
            "id": "4058335",
            "problem": "Consider a neuromorphic encoder that produces a nonnegative population activity vector $a \\in \\mathbb{R}^{m}$ for $m$ parallel neurons. In population sparse coding, a sparsity functional $s(a)$ is desired that obeys the following constraints, rooted in standard norm properties and widely accepted desiderata for population sparsity: (i) $s(a)$ is invariant to positive scaling, so $s(\\alpha a) = s(a)$ for all $\\alpha > 0$; (ii) $s(a)$ is continuous and depends on $a$ only through norms that capture population activation magnitudes; (iii) $s(a)$ attains its minimum value when all entries of $a$ are equal (maximally distributed activity), and its maximum value when exactly one entry of $a$ is nonzero (maximally concentrated activity). Begin from the core definitions of the $\\ell_{1}$ norm $\\lVert a \\rVert_{1} = \\sum_{i=1}^{m} |a_{i}|$ and the Euclidean $\\ell_{2}$ norm $\\lVert a \\rVert_{2} = \\sqrt{\\sum_{i=1}^{m} a_{i}^{2}}$ together with the Cauchy–Schwarz inequality, and derive a scale-invariant sparsity index $s(a)$ that maps the ratio $\\lVert a \\rVert_{1} / \\lVert a \\rVert_{2}$ affinely onto the unit interval, consistent with (i)–(iii). Then, for the specific case $m = 5$ and\n$$\na = \\begin{pmatrix}\n\\frac{3}{5} \\\\\n0 \\\\\n0 \\\\\n\\frac{4}{5} \\\\\n0\n\\end{pmatrix},\n$$\ncompute the exact analytic expression for $s(a)$. Express your final answer in a single closed-form analytic expression, and do not approximate. Also, briefly interpret the value in terms of population sparsity in this encoder, using only the properties (i)–(iii) and norm inequalities without numerical approximation.",
            "solution": "We are tasked with deriving a scale-invariant sparsity index $s(a)$ for a nonnegative population activity vector $a \\in \\mathbb{R}^{m}$, where $a_i \\ge 0$ for all $i=1, \\dots, m$. This index must affinely map the ratio $x(a) = \\frac{\\lVert a \\rVert_{1}}{\\lVert a \\rVert_{2}}$ onto the unit interval $[0, 1]$, consistent with the given desiderata.\n\nFirst, we determine the range of the ratio $x(a) = \\frac{\\lVert a \\rVert_{1}}{\\lVert a \\rVert_{2}}$. Since all $a_i \\ge 0$, the $\\ell_1$ norm is $\\lVert a \\rVert_{1} = \\sum_{i=1}^{m} a_i$. The $\\ell_2$ norm is $\\lVert a \\rVert_{2} = \\sqrt{\\sum_{i=1}^{m} a_{i}^{2}}$. The ratio $x(a)$ is invariant to positive scaling: for any $\\alpha > 0$, $x(\\alpha a) = \\frac{\\lVert \\alpha a \\rVert_{1}}{\\lVert \\alpha a \\rVert_{2}} = \\frac{\\alpha \\lVert a \\rVert_{1}}{\\alpha \\lVert a \\rVert_{2}} = x(a)$, which satisfies constraint (i).\n\nTo find the bounds of $x(a)$, we use the Cauchy–Schwarz inequality. Let us define two vectors $u = (a_1, a_2, \\dots, a_m)$ and $v = (1, 1, \\dots, 1)$. The Cauchy-Schwarz inequality states $|\\langle u, v \\rangle| \\le \\lVert u \\rVert_2 \\lVert v \\rVert_2$.\nIn our case, $\\langle u, v \\rangle = \\sum_{i=1}^{m} a_i \\cdot 1 = \\sum_{i=1}^{m} a_i = \\lVert a \\rVert_1$.\nThe norm of $u$ is $\\lVert u \\rVert_2 = \\lVert a \\rVert_2$.\nThe norm of $v$ is $\\lVert v \\rVert_2 = \\sqrt{\\sum_{i=1}^{m} 1^2} = \\sqrt{m}$.\nSubstituting these into the inequality, we get $\\lVert a \\rVert_1 \\le \\lVert a \\rVert_2 \\sqrt{m}$.\nDividing by $\\lVert a \\rVert_2$ (assuming $a$ is not the zero vector), we find the upper bound for the ratio:\n$$\n\\frac{\\lVert a \\rVert_1}{\\lVert a \\rVert_2} \\le \\sqrt{m}\n$$\nEquality holds if and only if one vector is a scalar multiple of the other, i.e., $u = c v$ for some scalar $c$. This means $a_i = c \\cdot 1$ for all $i$, so all entries of $a$ are equal. According to constraint (iii), this corresponds to the minimum sparsity. Therefore, the maximum value of the ratio, $x_{max} = \\sqrt{m}$, corresponds to the minimum value of the sparsity index $s(a)$.\n\nNext, we find the minimum value of the ratio $x(a)$. Consider the square of the $\\ell_1$ norm: $(\\lVert a \\rVert_1)^2 = (\\sum_{i=1}^{m} a_i)^2 = \\sum_{i=1}^{m} a_i^2 + 2\\sum_{i < j} a_i a_j$. Since $a_i \\ge 0$, the cross-terms $a_i a_j$ are non-negative.\nThus, $(\\lVert a \\rVert_1)^2 \\ge \\sum_{i=1}^{m} a_i^2 = (\\lVert a \\rVert_2)^2$.\nTaking the square root of both sides, we get $\\lVert a \\rVert_1 \\ge \\lVert a \\rVert_2$.\nThis implies the minimum value of the ratio is:\n$$\n\\frac{\\lVert a \\rVert_1}{\\lVert a \\rVert_2} \\ge 1\n$$\nEquality holds if and only if all cross-terms are zero, i.e., $a_i a_j = 0$ for all $i \\ne j$. This requires that at most one component $a_i$ is non-zero. According to constraint (iii), this case of maximally concentrated activity corresponds to maximum sparsity. Therefore, the minimum value of the ratio, $x_{min} = 1$, corresponds to the maximum value of the sparsity index $s(a)$.\n\nThe range of the ratio $x(a)$ is $[1, \\sqrt{m}]$. We need to define an affine map $s(x) = A x + B$ that maps this interval to the unit interval $[0, 1]$ such that the mapping is consistent with the definitions of minimum and maximum sparsity.\nMaximum sparsity ($s(a)=1$) occurs when $x(a)=1$.\nMinimum sparsity ($s(a)=0$) occurs when $x(a)=\\sqrt{m}$.\nWe set up a system of two linear equations for the coefficients $A$ and $B$:\n$$\n\\begin{cases}\nA \\cdot 1 + B = 1 \\\\\nA \\cdot \\sqrt{m} + B = 0\n\\end{cases}\n$$\nSubtracting the second equation from the first gives $A(1 - \\sqrt{m}) = 1$, so $A = \\frac{1}{1 - \\sqrt{m}}$.\nSubstituting $A$ into the first equation: $B = 1 - A = 1 - \\frac{1}{1 - \\sqrt{m}} = \\frac{1 - \\sqrt{m} - 1}{1 - \\sqrt{m}} = \\frac{-\\sqrt{m}}{1 - \\sqrt{m}} = \\frac{\\sqrt{m}}{\\sqrt{m} - 1}$.\nThe sparsity index is therefore:\n$$\ns(a) = \\left(\\frac{1}{1 - \\sqrt{m}}\\right) \\frac{\\lVert a \\rVert_1}{\\lVert a \\rVert_2} + \\frac{\\sqrt{m}}{\\sqrt{m} - 1} = \\frac{-\\frac{\\lVert a \\rVert_1}{\\lVert a \\rVert_2} + \\sqrt{m}}{\\sqrt{m} - 1}\n$$\n$$\ns(a) = \\frac{\\sqrt{m} - \\frac{\\lVert a \\rVert_{1}}{\\lVert a \\rVert_{2}}}{\\sqrt{m} - 1}\n$$\nThis formula satisfies all the given constraints.\n\nNow, we compute the value of $s(a)$ for the specific case $m=5$ and $a = (\\frac{3}{5}, 0, 0, \\frac{4}{5}, 0)^T$.\nFirst, we compute the norms of $a$:\n$$\n\\lVert a \\rVert_{1} = \\frac{3}{5} + 0 + 0 + \\frac{4}{5} + 0 = \\frac{7}{5}\n$$\n$$\n\\lVert a \\rVert_{2} = \\sqrt{\\left(\\frac{3}{5}\\right)^2 + 0^2 + 0^2 + \\left(\\frac{4}{5}\\right)^2 + 0^2} = \\sqrt{\\frac{9}{25} + \\frac{16}{25}} = \\sqrt{\\frac{25}{25}} = \\sqrt{1} = 1\n$$\nThe ratio is $\\frac{\\lVert a \\rVert_1}{\\lVert a \\rVert_2} = \\frac{7/5}{1} = \\frac{7}{5}$.\nSubstituting $m=5$ and this ratio into the formula for $s(a)$:\n$$\ns(a) = \\frac{\\sqrt{5} - \\frac{7}{5}}{\\sqrt{5} - 1}\n$$\nTo express this in a single closed-form expression, we can rationalize the denominator:\n$$\ns(a) = \\frac{\\sqrt{5} - \\frac{7}{5}}{\\sqrt{5} - 1} \\times \\frac{\\sqrt{5} + 1}{\\sqrt{5} + 1} = \\frac{(\\sqrt{5})(\\sqrt{5}) + \\sqrt{5} - \\frac{7}{5}\\sqrt{5} - \\frac{7}{5}}{(\\sqrt{5})^2 - 1^2}\n$$\n$$\ns(a) = \\frac{5 + \\sqrt{5}\\left(1 - \\frac{7}{5}\\right) - \\frac{7}{5}}{5 - 1} = \\frac{\\frac{25}{5} - \\frac{7}{5} + \\sqrt{5}\\left(\\frac{5}{5} - \\frac{7}{5}\\right)}{4}\n$$\n$$\ns(a) = \\frac{\\frac{18}{5} - \\frac{2}{5}\\sqrt{5}}{4} = \\frac{18 - 2\\sqrt{5}}{20} = \\frac{9 - \\sqrt{5}}{10}\n$$\nFor the interpretation, the value $s(a)$ indicates the degree of population sparsity. A value of $s(a)=1$ represents maximal sparsity (only one neuron active), and $s(a)=0$ represents minimal sparsity or maximally dense activity (all neurons equally active). The given vector $a$ has two active neurons out of five. This is an intermediate case. The fact that the ratio $\\frac{\\lVert a \\rVert_1}{\\lVert a \\rVert_2} = \\frac{7}{5}$ is much closer to its minimum possible value of $1$ than its maximum possible value of $\\sqrt{5}$ indicates that the activity pattern is much closer to the maximally sparse case than the maximally dense case. Consequently, the resulting sparsity index $s(a) = \\frac{9-\\sqrt{5}}{10}$ is a value between $0$ and $1$ that is significantly closer to $1$, reflecting a high degree of population sparsity in the encoder's output.",
            "answer": "$$\n\\boxed{\\frac{9 - \\sqrt{5}}{10}}\n$$"
        },
        {
            "introduction": "Minimizing the sparse coding cost function requires a practical algorithm, and for brain-inspired computing, one that is neurally plausible. This exercise provides a hands-on simulation of the Locally Competitive Algorithm (LCA), a dynamical system that solves the $\\ell_{1}$-regularized problem through recurrent inhibition and thresholding . By calculating a single update step, you will gain a tangible understanding of how the interplay between feedforward drive, lateral inhibition, and a nonlinear activation function works to find a sparse code over time.",
            "id": "4058373",
            "problem": "Consider the Locally Competitive Algorithm (LCA) in sparse coding for a linear generative model with dictionary $D \\in \\mathbb{R}^{m \\times n}$ and input $x \\in \\mathbb{R}^{m}$. The LCA continuous-time membrane potential dynamics are defined by the ordinary differential equation\n$$\n\\tau \\,\\frac{d u}{d t} \\;=\\; -\\,u \\;+\\; D^{\\top} x \\;-\\; \\big(D^{\\top} D - I\\big)\\,a,\n$$\nwhere $u \\in \\mathbb{R}^{n}$ is the membrane potential vector, $a \\in \\mathbb{R}^{n}$ is the activity vector given by a component-wise soft-threshold nonlinearity $a_i \\,=\\, \\operatorname{sign}(u_i)\\max\\{|u_i|-\\lambda,\\,0\\}$ with threshold $\\lambda > 0$, and $I$ is the $n \\times n$ identity matrix. Use a forward Euler discretization with time step $\\Delta t$ to simulate a single update from time $t$ to time $t+\\Delta t$:\n$$\nu^{t+1} \\;=\\; u^{t} \\;+\\; \\frac{\\Delta t}{\\tau}\\left(-\\,u^{t} \\;+\\; D^{\\top} x \\;-\\; \\big(D^{\\top} D - I\\big)\\,a^{t}\\right), \\quad a^{t} \\,=\\, \\operatorname{sign}(u^{t})\\max\\{|u^{t}|-\\lambda,\\,0\\}\\;\\text{(component-wise)}.\n$$\nLet the dictionary have $m=2$ and $n=3$ with columns\n$$\nd_{1} \\,=\\, \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix},\\quad\nd_{2} \\,=\\, \\begin{pmatrix} 0.8 \\\\ 0.6 \\end{pmatrix},\\quad\nd_{3} \\,=\\, \\begin{pmatrix} 0.6 \\\\ 0.8 \\end{pmatrix},\n$$\nso that $D \\,=\\, \\begin{pmatrix} 1 & 0.8 & 0.6 \\\\ 0 & 0.6 & 0.8 \\end{pmatrix}$. Let the input be $x \\,=\\, \\begin{pmatrix} 1.1 \\\\ 0.7 \\end{pmatrix}$, the current membrane potential be $u^{t} \\,=\\, \\begin{pmatrix} 0.9 \\\\ 0.2 \\\\ 0.4 \\end{pmatrix}$, the threshold be $\\lambda \\,=\\, 0.3$, the membrane time constant be $\\tau \\,=\\, 2$, and the step size be $\\Delta t \\,=\\, 0.5$. Compute the one-step update $u^{t+1}$ and the resulting activity $a^{t+1}$ using the definitions above. Express your final answer as a single row matrix listing the six entries in the order $(u^{t+1}_{1},\\,u^{t+1}_{2},\\,u^{t+1}_{3},\\,a^{t+1}_{1},\\,a^{t+1}_{2},\\,a^{t+1}_{3})$. Provide exact decimal values without rounding.",
            "solution": "The problem asks for a one-step update of the membrane potential vector $u$ and the corresponding activity vector $a$. The forward Euler discretization of the LCA dynamics is given by:\n$$u^{t+1} = u^{t} + \\frac{\\Delta t}{\\tau}\\left(-u^{t} + D^{\\top} x - \\left(D^{\\top} D - I\\right)a^{t}\\right)$$\nThe activities are related to the potentials via a soft-thresholding function:\n$$a_i = \\operatorname{sign}(u_i)\\max\\{|u_i|-\\lambda, 0\\}$$\nwhich is applied component-wise.\n\nThe given parameters are:\n- Dictionary $D = \\begin{pmatrix} 1 & 0.8 & 0.6 \\\\ 0 & 0.6 & 0.8 \\end{pmatrix}$\n- Input vector $x = \\begin{pmatrix} 1.1 \\\\ 0.7 \\end{pmatrix}$\n- Current membrane potential $u^{t} = \\begin{pmatrix} 0.9 \\\\ 0.2 \\\\ 0.4 \\end{pmatrix}$\n- Threshold $\\lambda = 0.3$\n- Time constant $\\tau = 2$\n- Time step $\\Delta t = 0.5$\n\nWe will compute the required quantities step by step.\n\n**Step 1: Compute the current activity vector $a^t$**\nUsing the component-wise soft-thresholding function with $u^t$ and $\\lambda = 0.3$:\n$$a_1^t = \\operatorname{sign}(u_1^t)\\max\\{|u_1^t|-\\lambda, 0\\} = \\operatorname{sign}(0.9)\\max\\{|0.9|-0.3, 0\\} = 1 \\cdot \\max\\{0.6, 0\\} = 0.6$$\n$$a_2^t = \\operatorname{sign}(u_2^t)\\max\\{|u_2^t|-\\lambda, 0\\} = \\operatorname{sign}(0.2)\\max\\{|0.2|-0.3, 0\\} = 1 \\cdot \\max\\{-0.1, 0\\} = 0$$\n$$a_3^t = \\operatorname{sign}(u_3^t)\\max\\{|u_3^t|-\\lambda, 0\\} = \\operatorname{sign}(0.4)\\max\\{|0.4|-0.3, 0\\} = 1 \\cdot \\max\\{0.1, 0\\} = 0.1$$\nThus, the current activity vector is $a^t = \\begin{pmatrix} 0.6 \\\\ 0 \\\\ 0.1 \\end{pmatrix}$.\n\n**Step 2: Compute the driving input term $D^\\top x$**\nThe transpose of the dictionary is $D^\\top = \\begin{pmatrix} 1 & 0 \\\\ 0.8 & 0.6 \\\\ 0.6 & 0.8 \\end{pmatrix}$.\n$$D^\\top x = \\begin{pmatrix} 1 & 0 \\\\ 0.8 & 0.6 \\\\ 0.6 & 0.8 \\end{pmatrix} \\begin{pmatrix} 1.1 \\\\ 0.7 \\end{pmatrix} = \\begin{pmatrix} (1)(1.1) + (0)(0.7) \\\\ (0.8)(1.1) + (0.6)(0.7) \\\\ (0.6)(1.1) + (0.8)(0.7) \\end{pmatrix} = \\begin{pmatrix} 1.1 \\\\ 0.88 + 0.42 \\\\ 0.66 + 0.56 \\end{pmatrix} = \\begin{pmatrix} 1.1 \\\\ 1.3 \\\\ 1.22 \\end{pmatrix}$$\n\n**Step 3: Compute the inhibitory matrix term $(D^\\top D - I)a^t$**\nFirst, we compute the Gram matrix $D^\\top D$:\n$$D^\\top D = \\begin{pmatrix} 1 & 0 \\\\ 0.8 & 0.6 \\\\ 0.6 & 0.8 \\end{pmatrix} \\begin{pmatrix} 1 & 0.8 & 0.6 \\\\ 0 & 0.6 & 0.8 \\end{pmatrix} = \\begin{pmatrix} 1 & 0.8 & 0.6 \\\\ 0.8 & 0.64+0.36 & 0.48+0.48 \\\\ 0.6 & 0.48+0.48 & 0.36+0.64 \\end{pmatrix} = \\begin{pmatrix} 1 & 0.8 & 0.6 \\\\ 0.8 & 1 & 0.96 \\\\ 0.6 & 0.96 & 1 \\end{pmatrix}$$\nThe inhibitory matrix is $G = D^\\top D - I$, where $I$ is the $3 \\times 3$ identity matrix:\n$$G = D^\\top D - I = \\begin{pmatrix} 1 & 0.8 & 0.6 \\\\ 0.8 & 1 & 0.96 \\\\ 0.6 & 0.96 & 1 \\end{pmatrix} - \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 0 & 0.8 & 0.6 \\\\ 0.8 & 0 & 0.96 \\\\ 0.6 & 0.96 & 0 \\end{pmatrix}$$\nNow, we compute the inhibition term $Ga^t$:\n$$(D^\\top D - I)a^t = \\begin{pmatrix} 0 & 0.8 & 0.6 \\\\ 0.8 & 0 & 0.96 \\\\ 0.6 & 0.96 & 0 \\end{pmatrix} \\begin{pmatrix} 0.6 \\\\ 0 \\\\ 0.1 \\end{pmatrix} = \\begin{pmatrix} (0)(0.6) + (0.8)(0) + (0.6)(0.1) \\\\ (0.8)(0.6) + (0)(0) + (0.96)(0.1) \\\\ (0.6)(0.6) + (0.96)(0) + (0)(0.1) \\end{pmatrix} = \\begin{pmatrix} 0.06 \\\\ 0.48 + 0.096 \\\\ 0.36 \\end{pmatrix} = \\begin{pmatrix} 0.06 \\\\ 0.576 \\\\ 0.36 \\end{pmatrix}$$\n\n**Step 4: Compute the updated membrane potential $u^{t+1}$**\nWe assemble the terms for the update rule. The scaling factor is $\\frac{\\Delta t}{\\tau} = \\frac{0.5}{2} = 0.25$.\nThe expression in the parentheses is:\n$$-u^{t} + D^{\\top} x - (D^{\\top} D - I)a^{t} = -\\begin{pmatrix} 0.9 \\\\ 0.2 \\\\ 0.4 \\end{pmatrix} + \\begin{pmatrix} 1.1 \\\\ 1.3 \\\\ 1.22 \\end{pmatrix} - \\begin{pmatrix} 0.06 \\\\ 0.576 \\\\ 0.36 \\end{pmatrix} = \\begin{pmatrix} -0.9 + 1.1 - 0.06 \\\\ -0.2 + 1.3 - 0.576 \\\\ -0.4 + 1.22 - 0.36 \\end{pmatrix} = \\begin{pmatrix} 0.14 \\\\ 0.524 \\\\ 0.46 \\end{pmatrix}$$\nNow we perform the update:\n$$u^{t+1} = u^{t} + \\frac{\\Delta t}{\\tau} \\begin{pmatrix} 0.14 \\\\ 0.524 \\\\ 0.46 \\end{pmatrix} = \\begin{pmatrix} 0.9 \\\\ 0.2 \\\\ 0.4 \\end{pmatrix} + 0.25 \\begin{pmatrix} 0.14 \\\\ 0.524 \\\\ 0.46 \\end{pmatrix} = \\begin{pmatrix} 0.9 \\\\ 0.2 \\\\ 0.4 \\end{pmatrix} + \\begin{pmatrix} 0.035 \\\\ 0.131 \\\\ 0.115 \\end{pmatrix} = \\begin{pmatrix} 0.935 \\\\ 0.331 \\\\ 0.515 \\end{pmatrix}$$\n\n**Step 5: Compute the new activity vector $a^{t+1}$**\nUsing the updated potential $u^{t+1} = \\begin{pmatrix} 0.935 \\\\ 0.331 \\\\ 0.515 \\end{pmatrix}$ and the threshold $\\lambda = 0.3$:\n$$a_1^{t+1} = \\operatorname{sign}(0.935)\\max\\{|0.935|-0.3, 0\\} = 1 \\cdot \\max\\{0.635, 0\\} = 0.635$$\n$$a_2^{t+1} = \\operatorname{sign}(0.331)\\max\\{|0.331|-0.3, 0\\} = 1 \\cdot \\max\\{0.031, 0\\} = 0.031$$\n$$a_3^{t+1} = \\operatorname{sign(0.515)}\\max\\{|0.515|-0.3, 0\\} = 1 \\cdot \\max\\{0.215, 0\\} = 0.215$$\nThe new activity vector is $a^{t+1} = \\begin{pmatrix} 0.635 \\\\ 0.031 \\\\ 0.215 \\end{pmatrix}$.\n\nThe six values required for the final answer are:\n$u^{t+1}_{1} = 0.935$\n$u^{t+1}_{2} = 0.331$\n$u^{t+1}_{3} = 0.515$\n$a^{t+1}_{1} = 0.635$\n$a^{t+1}_{2} = 0.031$\n$a^{t+1}_{3} = 0.215$\nThese are to be presented as a single row matrix.",
            "answer": "$$\\boxed{\\begin{pmatrix} 0.935 & 0.331 & 0.515 & 0.635 & 0.031 & 0.215 \\end{pmatrix}}$$"
        }
    ]
}