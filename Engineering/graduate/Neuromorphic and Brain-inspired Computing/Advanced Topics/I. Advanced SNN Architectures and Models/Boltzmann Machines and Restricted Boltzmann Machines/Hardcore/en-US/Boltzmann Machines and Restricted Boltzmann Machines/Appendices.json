{
    "hands_on_practices": [
        {
            "introduction": "To build a solid foundation, we begin with the fully connected Boltzmann Machine. This exercise challenges you to derive the number of independent parameters that define the model, showing that constraints like weight matrix symmetry and zero diagonals are not limitations but conventions that emerge from the model's mathematical structure. Engaging with this problem  provides a crucial insight into parameter identifiability and the true expressive capacity of energy-based models.",
            "id": "4038503",
            "problem": "Consider a fully connected Boltzmann machine with $n$ binary units $x_{i} \\in \\{0,1\\}$, pairwise interaction weights collected in a real matrix $W \\in \\mathbb{R}^{n \\times n}$, and biases $b \\in \\mathbb{R}^{n}$. The probability of a state $x \\in \\{0,1\\}^{n}$ is given by a Boltzmann distribution $p(x) \\propto \\exp\\!\\big(-E(x)\\big)$, where $E(x)$ is an energy function derived from pairwise interactions and biases. Assume only pairwise interactions and unit-wise biases are present (no higher-order terms). Impose the physical consistency requirement that the pairwise coupling energy is invariant to relabeling of unit indices and that self-interaction terms that are not identifiable from the state space are excluded. \n\nUsing only these foundational assumptions, determine the number of independent parameters in the model when one enforces $W$ to be symmetric and to have zero diagonal. Explicitly count the independent interaction weights and the independent biases, and give a single closed-form expression in terms of $n$ for the total number of independent parameters. Explain, in your reasoning, why imposing $W = W^{\\top}$ and $\\operatorname{diag}(W)=0$ does not reduce the expressive family of models relative to allowing arbitrary real $W$ and $b$. \n\nProvide your final answer as a single closed-form analytic expression in terms of $n$. No numerical substitution is required and no rounding is needed.",
            "solution": "The problem asks for the total number of independent parameters in a fully connected Boltzmann machine with $n$ binary units, under the constraints that the weight matrix $W$ is symmetric with a zero diagonal. It also requires an explanation for why these constraints do not reduce the expressive power of the model.\n\nLet the state of the system be a vector $x \\in \\{0, 1\\}^n$, where each unit $x_i$ for $i \\in \\{1, 2, \\dots, n\\}$ is a binary variable, i.e., $x_i \\in \\{0, 1\\}$. The most general energy function $E'(x)$ for a model with only pairwise interactions and unit-wise biases can be formulated using a general real-valued weight matrix $W' \\in \\mathbb{R}^{n \\times n}$ and a bias vector $b' \\in \\mathbb{R}^n$. This energy is given by:\n$$E'(x) = - \\sum_{i=1}^n \\sum_{j=1}^n x_i W'_{ij} x_j - \\sum_{i=1}^n b'_i x_i$$\nThe probability of a state $x$ is then given by the Boltzmann distribution $p(x) = \\frac{1}{Z} \\exp(-E(x))$, where $Z$ is the partition function. Since $p(x)$ depends on $E(x)$ only through the exponential function, any two energy functions that differ by a constant offset for all states $x$ will yield the same probability distribution. Furthermore, any parameters in the energy function that can be combined or eliminated without altering the energy value for any state $x$ are not independent. We will now show that an arbitrary model defined by $(W', b')$ can be reparametrized into an equivalent model $(W, b)$ where $W$ is symmetric ($W=W^\\top$) and has a zero diagonal ($\\operatorname{diag}(W)=0$), without any loss of expressive power.\n\nFirst, let us analyze the diagonal terms of the weight matrix $W'$. We can separate the double summation in the energy function into diagonal and off-diagonal parts:\n$$\\sum_{i=1}^n \\sum_{j=1}^n x_i W'_{ij} x_j = \\sum_{i \\neq j} x_i W'_{ij} x_j + \\sum_{i=1}^n x_i W'_{ii} x_i$$\nA crucial property of the binary units $x_i \\in \\{0, 1\\}$ is that $x_i^2 = x_i$. Applying this to the diagonal sum, we get:\n$$\\sum_{i=1}^n x_i W'_{ii} x_i = \\sum_{i=1}^n W'_{ii} x_i^2 = \\sum_{i=1}^n W'_{ii} x_i$$\nSubstituting this back into the expression for $E'(x)$:\n$$E'(x) = - \\sum_{i \\neq j} x_i W'_{ij} x_j - \\sum_{i=1}^n W'_{ii} x_i - \\sum_{i=1}^n b'_i x_i$$\nWe can combine the two linear terms:\n$$E'(x) = - \\sum_{i \\neq j} x_i W'_{ij} x_j - \\sum_{i=1}^n (b'_i + W'_{ii}) x_i$$\nThis demonstrates that the effect of the diagonal weight $W'_{ii}$ is indistinguishable from the effect of the bias $b'_i$. The terms are not separately identifiable. We can therefore define a new set of parameters: a weight matrix $W''$ with a zero diagonal, and a new bias vector $b$. Let $W''_{ij} = W'_{ij}$ for $i \\neq j$, $W''_{ii} = 0$, and $b_i = b'_i + W'_{ii}$. The energy function remains identical for every state $x$:\n$$E(x) = - \\sum_{i \\neq j} x_i W''_{ij} x_j - \\sum_{i=1}^n b_i x_i$$\nThus, we can enforce the constraint $\\operatorname{diag}(W) = 0$ without any loss of generality or reduction in the model's expressive capacity. This corresponds to the problem's stated requirement to exclude \"self-interaction terms that are not identifiable from the state space\".\n\nNext, we address the symmetry of the weight matrix. Consider the off-diagonal summation term $\\sum_{i \\neq j} x_i W''_{ij} x_j$. Since the indices $i$ and $j$ are symmetric in the product $x_i x_j$ (i.e., $x_i x_j = x_j x_i$), we can group the terms for each pair of indices $(i, j)$ with $i \\neq j$:\n$$x_i W''_{ij} x_j + x_j W''_{ji} x_i = (W''_{ij} + W''_{ji}) x_i x_j$$\nThis shows that the energy function only depends on the sum $W''_{ij} + W''_{ji}$, not on the individual values of $W''_{ij}$ and $W''_{ji}$. We can define a new, symmetric weight matrix $W$ by setting its elements to be the average of the corresponding elements in $W''$ and its transpose:\n$$W_{ij} = \\frac{W''_{ij} + W''_{ji}}{2}$$\nFor this new matrix $W$, we have $W_{ij} = W_{ji}$, so it is symmetric. Also, since $W''_{ii} = 0$, it follows that $W_{ii} = \\frac{W''_{ii} + W''_{ii}}{2} = 0$, so $W$ also has a zero diagonal. The energy component for the pair $(i, j)$ using the new matrix $W$ is:\n$$\\sum_{k,l \\in \\{i,j\\}, k \\neq l} x_k W_{kl} x_l = x_i W_{ij} x_j + x_j W_{ji} x_i = 2 W_{ij} x_i x_j = (W''_{ij} + W''_{ji}) x_i x_j$$\nThis is precisely the same as the original contribution. Thus, replacing the potentially non-symmetric matrix $W''$ with the symmetric matrix $W$ does not change the energy function for any state $x$. This justifies enforcing the constraint $W = W^\\top$. This corresponds to the physical intuition that the coupling energy between two units should be described by a single parameter.\n\nHaving established that the constraints of a symmetric weight matrix with a zero diagonal do not reduce the family of representable probability distributions, we can now count the number of independent parameters for such a model.\n\n1.  **Independent interaction weights:** The parameters are the elements of the $n \\times n$ matrix $W$. The constraint $\\operatorname{diag}(W) = 0$ implies that all $n$ diagonal elements are fixed at $0$ and are not free parameters. The symmetry constraint $W = W^\\top$ means $W_{ij} = W_{ji}$ for all $i, j$. Therefore, we only need to specify the elements in the upper (or lower) triangle of the matrix, as the other elements are then determined. The number of elements in the strict upper triangle (i.e., with $i  j$) corresponds to the number of ways to choose $2$ distinct indices from a set of $n$, which is given by the binomial coefficient $\\binom{n}{2}$.\n    Number of independent weights = $\\binom{n}{2} = \\frac{n(n-1)}{2}$.\n\n2.  **Independent biases:** The bias parameters are the components of the vector $b \\in \\mathbb{R}^n$. There is one bias parameter $b_i$ for each of the $n$ units. These are all independent.\n    Number of independent biases = $n$.\n\n3.  **Total number of independent parameters:** The total number of independent parameters in the model is the sum of the number of independent weights and independent biases.\n    Total parameters = (Number of independent weights) + (Number of independent biases)\n    $$N_{\\text{params}} = \\frac{n(n-1)}{2} + n$$\n    Simplifying this expression:\n    $$N_{\\text{params}} = \\frac{n^2 - n}{2} + \\frac{2n}{2} = \\frac{n^2 - n + 2n}{2} = \\frac{n^2 + n}{2}$$\n    This can be written in a more compact closed form:\n    $$N_{\\text{params}} = \\frac{n(n+1)}{2}$$\n\nThis expression represents the total number of free parameters required to specify a general Boltzmann machine with $n$ binary units and pairwise interactions.",
            "answer": "$$\n\\boxed{\\frac{n(n+1)}{2}}\n$$"
        },
        {
            "introduction": "Having examined the general Boltzmann Machine, we now turn to its more structured and widely used variant, the Restricted Boltzmann Machine (RBM). The defining feature of an RBM is its bipartite structure, which gives rise to a powerful property: the conditional independence of its hidden units given a visible state. This hands-on problem  demystifies this abstract principle by guiding you through a direct verification on a small-scale RBM, building a concrete understanding from first principles.",
            "id": "4038508",
            "problem": "Consider a binary-valued Restricted Boltzmann Machine (RBM) with $n_v=2$ visible units and $n_h=2$ hidden units. The visible units are $v=(v_1,v_2)\\in\\{0,1\\}^2$, the hidden units are $h=(h_1,h_2)\\in\\{0,1\\}^2$, and the energy function is defined by\n$$\nE(v,h)=-\\sum_{i=1}^{2} a_i v_i-\\sum_{j=1}^{2} b_j h_j-\\sum_{i=1}^{2}\\sum_{j=1}^{2} v_i W_{ij} h_j.\n$$\nThe joint distribution over $(v,h)$ is given by the Boltzmann distribution $P(v,h)=\\exp(-E(v,h))/Z$, where $Z$ is the partition function.\n\nLet the parameters be\n$$\na=\\begin{pmatrix}0.3\\\\-0.1\\end{pmatrix},\\quad b=\\begin{pmatrix}0.2\\\\0.4\\end{pmatrix},\\quad W=\\begin{pmatrix}0.7  -0.5\\\\ -0.3  0.6\\end{pmatrix}.\n$$\n\nTasks:\n1. Starting from the Boltzmann distribution and the given energy function, derive an explicit expression for the conditional distribution $P(h\\mid v)$ for arbitrary $v\\in\\{0,1\\}^2$, without assuming any special factorization a priori.\n2. Using only the bipartite structure encoded in the energy function, prove from first principles that the hidden units are conditionally independent given $v$, and identify the parameterization of the resulting Bernoulli product form.\n3. Enumerate all $16$ configurations $(v,h)\\in\\{0,1\\}^2\\times\\{0,1\\}^2$ and compute the corresponding energies $E(v,h)$ and the unnormalized weights $\\exp(-E(v,h))$. For each fixed $v$, explicitly normalize over the four hidden states $h$ to obtain $P(h\\mid v)$ and verify that it equals the Bernoulli product form obtained in Task $2$.\n4. For the specific visible configuration $v=(1,0)$, compute the exact value of $P(h_1=1\\mid v)$.\n\nReport as your final answer the exact expression for $P(h_1=1\\mid v=(1,0))$ in terms of the exponential function. Do not numerically approximate. No units are required.",
            "solution": "The problem statement is first validated against the specified criteria.\n\n**Problem Validation**\n\n1.  **Extract Givens**:\n    -   A Restricted Boltzmann Machine (RBM) with $n_v=2$ visible units $v=(v_1,v_2)\\in\\{0,1\\}^2$ and $n_h=2$ hidden units $h=(h_1,h_2)\\in\\{0,1\\}^2$.\n    -   Energy function: $E(v,h)=-\\sum_{i=1}^{2} a_i v_i-\\sum_{j=1}^{2} b_j h_j-\\sum_{i=1}^{2}\\sum_{j=1}^{2} v_i W_{ij} h_j$.\n    -   Joint distribution: $P(v,h)=\\exp(-E(v,h))/Z$.\n    -   Parameters: $a=\\begin{pmatrix}0.3\\\\-0.1\\end{pmatrix}$, $b=\\begin{pmatrix}0.2\\\\0.4\\end{pmatrix}$, $W=\\begin{pmatrix}0.7  -0.5\\\\ -0.3  0.6\\end{pmatrix}$.\n    -   The tasks are to derive $P(h\\mid v)$, prove conditional independence of hidden units, verify this property by enumerating all $16$ states, and compute a specific conditional probability.\n\n2.  **Validate Using Extracted Givens**:\n    -   **Scientifically Grounded**: The problem is a standard exercise in the theory of RBMs, a topic central to machine learning and computational neuroscience. The definitions and model are standard.\n    -   **Well-Posed**: The problem is fully specified with all necessary parameters and definitions. Each task is clearly stated and leads to a unique, verifiable solution.\n    -   **Objective**: The problem is expressed using precise mathematical language and contains no subjective or ambiguous statements.\n    -   All other criteria for a valid problem are met. There are no contradictions, missing data, or unrealistic assumptions.\n\n3.  **Verdict and Action**: The problem is deemed valid. A complete solution will be provided.\n\n**Solution**\n\n**Task 1: Derivation of the conditional distribution $P(h\\mid v)$**\n\nThe conditional distribution $P(h\\mid v)$ is defined as $P(h\\mid v) = \\frac{P(v,h)}{P(v)}$. The joint probability is $P(v,h) = \\frac{1}{Z}\\exp(-E(v,h))$, and the marginal probability of the visible units is $P(v) = \\sum_{h'} P(v,h') = \\frac{1}{Z}\\sum_{h'} \\exp(-E(v,h'))$.\nSubstituting these into the definition gives:\n$$\nP(h\\mid v) = \\frac{\\frac{1}{Z}\\exp(-E(v,h))}{\\frac{1}{Z}\\sum_{h'} \\exp(-E(v,h'))} = \\frac{\\exp(-E(v,h))}{\\sum_{h'} \\exp(-E(v,h'))}\n$$\nThe energy function is $E(v,h) = -\\sum_{i} a_i v_i - \\sum_{j} b_j h_j - \\sum_{i,j} v_i W_{ij} h_j$. We can write this as $E(v,h) = E_v(v) + E_h(h) + E_{vh}(v,h)$, where the terms depend on $v$ only, $h$ only, and both, respectively. However, a more useful partition for this calculation is to separate terms that depend on $h$ from those that do not.\nLet's substitute the energy function into the expression for $P(h\\mid v)$:\n$$\nP(h\\mid v) = \\frac{\\exp\\left(\\sum_{i} a_i v_i + \\sum_{j} b_j h_j + \\sum_{i,j} v_i W_{ij} h_j\\right)}{\\sum_{h'} \\exp\\left(\\sum_{i} a_i v_i + \\sum_{j} b_j h'_j + \\sum_{i,j} v_i W_{ij} h'_j\\right)}\n$$\nThe term $\\exp(\\sum_{i} a_i v_i)$ is constant with respect to the summation variable $h'$ in the denominator. Thus, it can be factored out and canceled from the numerator and denominator:\n$$\nP(h\\mid v) = \\frac{\\exp(\\sum_{i} a_i v_i) \\exp\\left(\\sum_{j} b_j h_j + \\sum_{i,j} v_i W_{ij} h_j\\right)}{\\exp(\\sum_{i} a_i v_i) \\sum_{h'} \\exp\\left(\\sum_{j} b_j h'_j + \\sum_{i,j} v_i W_{ij} h'_j\\right)}\n$$\nThis simplifies to the final expression for the conditional distribution:\n$$\nP(h\\mid v) = \\frac{\\exp\\left(\\sum_{j} b_j h_j + \\sum_{j} \\left(\\sum_{i} v_i W_{ij}\\right) h_j\\right)}{\\sum_{h'} \\exp\\left(\\sum_{j} b_j h'_j + \\sum_{j} \\left(\\sum_{i} v_i W_{ij}\\right) h'_j\\right)}\n$$\n\n**Task 2: Proof of Conditional Independence of Hidden Units**\n\nThe structure of the energy function is key. It lacks any terms of the form $h_j h_k$ for $j \\neq k$, which represents the bipartite graph structure of an RBM. We can leverage this to prove conditional independence. Starting from the result of Task $1$, let's analyze the exponent in the numerator:\n$$\n\\sum_{j} b_j h_j + \\sum_{j} \\left(\\sum_{i} v_i W_{ij}\\right) h_j = \\sum_{j} \\left(b_j + \\sum_{i} v_i W_{ij}\\right) h_j\n$$\nThis expression is a sum over the hidden units $j$, where each term only involves a single $h_j$. Let's define an effective bias for each hidden unit $j$, which depends on the state of the visible layer $v$:\n$$\n\\theta_j(v) = b_j + \\sum_{i=1}^{2} v_i W_{ij}\n$$\nThe expression in the exponent becomes $\\sum_{j=1}^{2} \\theta_j(v) h_j$. The conditional distribution is:\n$$\nP(h\\mid v) = \\frac{\\exp\\left(\\sum_{j=1}^{2} \\theta_j(v) h_j\\right)}{\\sum_{h' \\in \\{0,1\\}^2} \\exp\\left(\\sum_{j=1}^{2} \\theta_j(v) h'_j\\right)}\n$$\nThe numerator can be factorized as a product over $j$: $\\exp\\left(\\sum_{j} \\theta_j(v) h_j\\right) = \\prod_{j} \\exp(\\theta_j(v) h_j)$.\nThe denominator is a sum over all $2^2=4$ states of $h'=(h'_1, h'_2)$. This sum can also be factorized:\n$$\n\\sum_{h'_1 \\in \\{0,1\\}} \\sum_{h'_2 \\in \\{0,1\\}} \\exp(\\theta_1(v) h'_1 + \\theta_2(v) h'_2) = \\sum_{h'_1, h'_2} \\exp(\\theta_1(v) h'_1) \\exp(\\theta_2(v) h'_2)\n$$\n$$\n= \\left( \\sum_{h'_1 \\in \\{0,1\\}} \\exp(\\theta_1(v) h'_1) \\right) \\left( \\sum_{h'_2 \\in \\{0,1\\}} \\exp(\\theta_2(v) h'_2) \\right) = \\prod_{j=1}^{2} \\left(\\sum_{h'_j \\in \\{0,1\\}} \\exp(\\theta_j(v) h'_j)\\right)\n$$\nEach term in this product is $\\exp(\\theta_j(v) \\cdot 0) + \\exp(\\theta_j(v) \\cdot 1) = 1 + \\exp(\\theta_j(v))$.\nSo the denominator is $\\prod_{j=1}^{2} (1 + \\exp(\\theta_j(v)))$.\nCombining the factorized numerator and denominator:\n$$\nP(h\\mid v) = \\frac{\\prod_{j=1}^{2} \\exp(\\theta_j(v) h_j)}{\\prod_{j=1}^{2} (1 + \\exp(\\theta_j(v)))} = \\prod_{j=1}^{2} \\frac{\\exp(\\theta_j(v) h_j)}{1 + \\exp(\\theta_j(v))}\n$$\nThis shows that $P(h\\mid v)$ is a product of distributions for each hidden unit, i.e., $P(h\\mid v) = P(h_1\\mid v) P(h_2\\mid v)$. This is the definition of conditional independence.\nThe term for a single hidden unit $h_j$ is:\n$$\nP(h_j\\mid v) = \\frac{\\exp(\\theta_j(v) h_j)}{1 + \\exp(\\theta_j(v))}\n$$\nFor $h_j=1$, we get $P(h_j=1\\mid v) = \\frac{\\exp(\\theta_j(v))}{1 + \\exp(\\theta_j(v))} = \\frac{1}{1 + \\exp(-\\theta_j(v))}$, which is the logistic sigmoid function, $\\sigma(\\theta_j(v))$.\nFor $h_j=0$, we get $P(h_j=0\\mid v) = \\frac{1}{1 + \\exp(\\theta_j(v))} = 1 - \\sigma(\\theta_j(v))$.\nThus, each hidden unit $h_j$ follows a Bernoulli distribution with parameter $p_j = \\sigma(\\theta_j(v))$. The distribution $P(h\\mid v)$ is a product of these Bernoulli distributions. The parameterization is given by $p_j = \\sigma(b_j + \\sum_i v_i W_{ij})$ for $j=1,2$.\n\n**Task 3: Enumeration and Verification**\n\nWe compute the energy $E(v,h) = -(a_1 v_1 + a_2 v_2) - (b_1 h_1 + b_2 h_2) - (v_1W_{11}h_1 + v_1W_{12}h_2 + v_2W_{21}h_1 + v_2W_{22}h_2)$ and the unnormalized probability mass $\\exp(-E(v,h))$ for all $16$ configurations of $(v,h)$.\n\nFirst, let's calculate the effective biases $\\theta_j(v) = b_j + v_1 W_{1j} + v_2 W_{2j}$ for each $v$.\n-   $v=(0,0)$: $\\theta_1=b_1=0.2$, $\\theta_2=b_2=0.4$.\n-   $v=(0,1)$: $\\theta_1=b_1+W_{21}=0.2-0.3=-0.1$, $\\theta_2=b_2+W_{22}=0.4+0.6=1.0$.\n-   $v=(1,0)$: $\\theta_1=b_1+W_{11}=0.2+0.7=0.9$, $\\theta_2=b_2+W_{12}=0.4-0.5=-0.1$.\n-   $v=(1,1)$: $\\theta_1=b_1+W_{11}+W_{21}=0.2+0.7-0.3=0.6$, $\\theta_2=b_2+W_{12}+W_{22}=0.4-0.5+0.6=0.5$.\n\nNow, we enumerate all $16$ states. The term $-E(v,h)$ is equal to $a^T v + \\theta_1(v) h_1 + \\theta_2(v) h_2$.\n\n| $v_1$ | $v_2$ | $h_1$ | $h_2$ | $a^Tv$ | $\\theta_1 h_1+\\theta_2 h_2$ | $-E(v,h)$ | $E(v,h)$ | $\\exp(-E(v,h))$ |\n|---|---|---|---|---|---|---|---|---|\n| $0$ | $0$ | $0$ | $0$ | $0.0$ | $0.0$ | $0.0$ | $0.0$ | $\\exp(0.0) = 1$ |\n| $0$ | $0$ | $0$ | $1$ | $0.0$ | $0.4$ | $0.4$ | $-0.4$ | $\\exp(0.4)$ |\n| $0$ | $0$ | $1$ | $0$ | $0.0$ | $0.2$ | $0.2$ | $-0.2$ | $\\exp(0.2)$ |\n| $0$ | $0$ | $1$ | $1$ | $0.0$ | $0.6$ | $0.6$ | $-0.6$ | $\\exp(0.6)$ |\n| $0$ | $1$ | $0$ | $0$ | $-0.1$ | $0.0$ | $-0.1$ | $0.1$ | $\\exp(-0.1)$ |\n| $0$ | $1$ | $0$ | $1$ | $-0.1$ | $1.0$ | $0.9$ | $-0.9$ | $\\exp(0.9)$ |\n| $0$ | $1$ | $1$ | $0$ | $-0.1$ | $-0.1$ | $-0.2$ | $0.2$ | $\\exp(-0.2)$ |\n| $0$ | $1$ | $1$ | $1$ | $-0.1$ | $0.9$ | $0.8$ | $-0.8$ | $\\exp(0.8)$ |\n| $1$ | $0$ | $0$ | $0$ | $0.3$ | $0.0$ | $0.3$ | $-0.3$ | $\\exp(0.3)$ |\n| $1$ | $0$ | $0$ | $1$ | $0.3$ | $-0.1$ | $0.2$ | $-0.2$ | $\\exp(0.2)$ |\n| $1$ | $0$ | $1$ | $0$ | $0.3$ | $0.9$ | $1.2$ | $-1.2$ | $\\exp(1.2)$ |\n| $1$ | $0$ | $1$ | $1$ | $0.3$ | $0.8$ | $1.1$ | $-1.1$ | $\\exp(1.1)$ |\n| $1$ | $1$ | $0$ | $0$ | $0.2$ | $0.0$ | $0.2$ | $-0.2$ | $\\exp(0.2)$ |\n| $1$ | $1$ | $0$ | $1$ | $0.2$ | $0.5$ | $0.7$ | $-0.7$ | $\\exp(0.7)$ |\n| $1$ | $1$ | $1$ | $0$ | $0.2$ | $0.6$ | $0.8$ | $-0.8$ | $\\exp(0.8)$ |\n| $1$ | $1$ | $1$ | $1$ | $0.2$ | $1.1$ | $1.3$ | $-1.3$ | $\\exp(1.3)$ |\n\nVerification for each fixed $v$:\nFor a fixed $v$, $P(h\\mid v) = \\frac{\\exp(-E(v,h))}{\\sum_{h'}\\exp(-E(v,h'))}$. Let's verify for $v=(1,0)$.\nFrom the table, for $v=(1,0)$, the unnormalized probabilities are $\\exp(0.3)$, $\\exp(0.2)$, $\\exp(1.2)$, $\\exp(1.1)$.\nThe normalization constant is $Z_{(1,0)} = \\exp(0.3) + \\exp(0.2) + \\exp(1.2) + \\exp(1.1)$.\n$P(h=(1,1)\\mid v=(1,0)) = \\frac{\\exp(1.1)}{\\exp(0.3) + \\exp(0.2) + \\exp(1.2) + \\exp(1.1)}$.\nFrom Task $2$, $P(h\\mid v) = P(h_1\\mid v)P(h_2\\mid v)$ where $P(h_j=1\\mid v) = \\sigma(\\theta_j(v))$. For $v=(1,0)$, $\\theta_1=0.9, \\theta_2=-0.1$.\nSo, $P(h_1=1\\mid v)=\\sigma(0.9)$, $P(h_2=1\\mid v)=\\sigma(-0.1)$.\n$P(h_1=0\\mid v)=1-\\sigma(0.9)$, $P(h_2=0\\mid v)=1-\\sigma(-0.1)$.\n$P(h=(1,1)\\mid v=(1,0)) = \\sigma(0.9)\\sigma(-0.1) = \\left(\\frac{\\exp(0.9)}{1+\\exp(0.9)}\\right)\\left(\\frac{\\exp(-0.1)}{1+\\exp(-0.1)}\\right) = \\frac{\\exp(0.8)}{(1+\\exp(0.9))(1+\\exp(-0.1))}$.\nThe denominator from the explicit summation is $Z_{(1,0)} = \\exp(0.3)[1 + \\exp(-0.1) + \\exp(0.9) + \\exp(0.8)] = \\exp(0.3)[(1+\\exp(0.9))(1+\\exp(-0.1))]$.\nThe numerator is $\\exp(1.1) = \\exp(0.3)\\exp(0.8)$.\nSo, $P(h=(1,1)\\mid v=(1,0)) = \\frac{\\exp(0.3)\\exp(0.8)}{\\exp(0.3)(1+\\exp(0.9))(1+\\exp(-0.1))} = \\frac{\\exp(0.8)}{(1+\\exp(0.9))(1+\\exp(-0.1))}$.\nThe results match. The verification holds for all other combinations of $(v,h)$ by analogous calculations, confirming the Bernoulli product form.\n\n**Task 4: Computation of $P(h_1=1\\mid v=(1,0))$**\n\nWe need to compute $P(h_1=1\\mid v)$ for the specific visible configuration $v=(1,0)$.\nUsing the formula derived in Task $2$:\n$$\nP(h_1=1\\mid v) = \\sigma\\left(b_1 + \\sum_{i=1}^{2} v_i W_{i1}\\right)\n$$\nFor $v=(1,0)$, we have $v_1=1$ and $v_2=0$.\nThe argument of the sigmoid function is:\n$$\n\\theta_1(1,0) = b_1 + v_1 W_{11} + v_2 W_{21} = 0.2 + (1)(0.7) + (0)(-0.3) = 0.2 + 0.7 = 0.9\n$$\nSubstituting this value into the sigmoid function:\n$$\nP(h_1=1\\mid v=(1,0)) = \\sigma(0.9) = \\frac{1}{1 + \\exp(-0.9)}\n$$\nThis is the exact final expression.",
            "answer": "$$\n\\boxed{\\frac{1}{1 + \\exp(-0.9)}}\n$$"
        },
        {
            "introduction": "Understanding a model's theoretical properties is only the first step; effectively training it presents its own set of challenges. This practice shifts our focus to the dynamics of learning, specifically the pervasive issue of overfitting. By analyzing simulated training logs , you will learn to connect a model's capacity, controlled by the number of hidden units $H$, to its generalization performance and diagnose overfitting by observing the gap between training and test log-likelihoods. This exercise hones the essential skill of interpreting empirical results to guide model selection and regularization strategies.",
            "id": "4038496",
            "problem": "A binary Restricted Boltzmann Machine (RBM) with $D$ visible units and $H$ hidden units defines a model distribution $p_{\\theta}(v)$ over visible configurations $v \\in \\{0,1\\}^{D}$ through an energy-based parametrization and a partition function. Maximum likelihood learning seeks parameters $\\theta$ that maximize the expected log-likelihood of the data under $p_{\\theta}(v)$. Consider a dataset sampled from an unknown distribution $p^{\\star}(v)$, and suppose training uses Contrastive Divergence with a fixed number of Gibbs steps, while model evaluation of $\\log p_{\\theta}(v)$ uses Annealed Importance Sampling with sufficiently long chains to yield negligible bias relative to variance.\n\nDefine the empirical training and test objectives\n$$\nJ_{\\text{train}}(\\theta) = \\mathbb{E}_{\\hat{p}_{\\text{train}}}[\\log p_{\\theta}(v)], \\quad J_{\\text{test}}(\\theta) = \\mathbb{E}_{\\hat{p}_{\\text{test}}}[\\log p_{\\theta}(v)],\n$$\nand the generalization gap\n$$\n\\Delta(\\theta) = J_{\\text{train}}(\\theta) - J_{\\text{test}}(\\theta).\n$$\nYou monitor $J_{\\text{train}}(\\theta_t)$ and $J_{\\text{test}}(\\theta_t)$ over epochs $t$ during training for several configurations. The following empirically observed behaviors are reported:\n\n- Configuration $\\mathcal{S}_1$: $H = 100$, no explicit regularization ($\\lambda = 0$). $J_{\\text{train}}(\\theta_t)$ increases steadily with $t$, while $J_{\\text{test}}(\\theta_t)$ initially increases but later decreases. $\\Delta(\\theta_t)$ grows from approximately $0.5$ nats at epoch $t = 10$ to approximately $5.0$ nats at epoch $t = 200$.\n- Configuration $\\mathcal{S}_2$: $H = 100$, with $\\ell_{2}$ weight decay of strength $\\lambda = 10^{-4}$. $J_{\\text{train}}(\\theta_t)$ increases moderately with $t$, $J_{\\text{test}}(\\theta_t)$ increases and stabilizes, and $\\Delta(\\theta_t)$ peaks near approximately $1.2$ nats before decreasing to approximately $0.8$ nats. Early stopping at the epoch where $J_{\\text{test}}$ is maximized yields the best held-out performance.\n- Configuration $\\mathcal{S}_3$: $H = 400$, $\\lambda = 0$. $J_{\\text{train}}(\\theta_t)$ increases rapidly with $t$, $J_{\\text{test}}(\\theta_t)$ increases briefly then decreases more sharply, and $\\Delta(\\theta_t)$ grows from approximately $0.6$ nats at epoch $t = 10$ to approximately $9.0$ nats at epoch $t = 200$.\n- Configuration $\\mathcal{S}_4$: $H = 400$, $\\lambda = 10^{-4}$. $J_{\\text{train}}(\\theta_t)$ increases, $J_{\\text{test}}(\\theta_t)$ increases then stabilizes, and $\\Delta(\\theta_t)$ remains moderate, near approximately $2.0$ nats by epoch $t = 200$.\n\nAssume the dataset size is fixed and moderate relative to the number of parameters ($D \\cdot H$), and that optimization noise (e.g., learning rate stochasticity) and log-likelihood estimator variance do not dominate the observed trends. Starting from the principles that maximum likelihood learning minimizes the Kullbackâ€“Leibler divergence $\\mathrm{KL}(p^{\\star} \\Vert p_{\\theta})$ via empirical risk minimization, and that model capacity influences uniform convergence between empirical and population risks, analyze the reported behavior of $\\Delta(\\theta_t)$ and its dependence on $H$ and $\\lambda$.\n\nWhich statement best explains the observed gaps and prescribes an effective intervention to improve generalization?\n\n- A. The growth in $\\Delta(\\theta_t)$ in $\\mathcal{S}_1$ and $\\mathcal{S}_3$ is primarily due to biased gradient estimates in Contrastive Divergence; increasing $H$ reduces this bias, so larger models will close the gap even without regularization.\n- B. The increase of $\\Delta(\\theta_t)$ with $H$ reflects weakened uniform convergence from higher capacity; adding $\\ell_{2}$ weight decay reduces effective capacity, thereby shrinking $\\Delta(\\theta_t)$, and early stopping selects parameters before overfitting fully manifests.\n- C. Because the RBM is an energy-based model, the partition function scaling with $H$ leaves the generalization gap invariant; thus $\\Delta(\\theta_t)$ is unrelated to capacity and cannot be mitigated by weight decay or early stopping.\n- D. $\\ell_{2}$ weight decay increases $J_{\\text{train}}(\\theta_t)$ faster than $J_{\\text{test}}(\\theta_t)$, which aggravates overfitting; therefore regularization should be removed to reduce $\\Delta(\\theta_t)$.",
            "solution": "This problem tests the understanding of model capacity, overfitting, and regularization in the context of training RBMs. We analyze the provided empirical observations in light of fundamental principles of statistical learning theory.\n\n1.  **Analyze the Core Phenomenon: Overfitting and the Generalization Gap**\n    The central observation is the behavior of the generalization gap, $\\Delta(\\theta_t) = J_{\\text{train}}(\\theta_t) - J_{\\text{test}}(\\theta_t)$. A large and growing gap indicates that the model is overfitting: it is learning features specific to the training set that do not generalize to the unseen test set. This leads to an increasing training log-likelihood ($J_{\\text{train}}$) but a stagnating or decreasing test log-likelihood ($J_{\\text{test}}$).\n\n2.  **Analyze the Effect of Model Capacity ($H$)**\n    -   In configuration $\\mathcal{S}_1$ ($H=100, \\lambda=0$), the model shows clear overfitting, with $\\Delta(\\theta_t)$ growing to $5.0$ nats.\n    -   In configuration $\\mathcal{S}_3$ ($H=400, \\lambda=0$), the model has a much higher capacity. The overfitting is more severe: $J_{\\text{train}}$ increases more rapidly, $J_{\\text{test}}$ decreases more sharply, and the gap $\\Delta(\\theta_t)$ grows to a much larger $9.0$ nats.\n    -   This comparison directly demonstrates a core principle: increasing model capacity (larger $H$) increases the risk of overfitting and leads to a larger generalization gap for a finite dataset. The model becomes too flexible and \"memorizes\" the training data instead of learning the underlying distribution. This is related to the concept of **uniform convergence**, where the convergence of the empirical risk to the population risk is weaker for higher-capacity model classes.\n\n3.  **Analyze the Effect of Regularization ($\\lambda$ and Early Stopping)**\n    -   In configuration $\\mathcal{S}_2$ ($H=100, \\lambda=10^{-4}$), adding $\\ell_{2}$ weight decay to the model from $\\mathcal{S}_1$ significantly mitigates overfitting. The test log-likelihood stabilizes, and the final gap is much smaller ($\\approx 0.8$ nats). $\\ell_2$ regularization penalizes large weights, effectively reducing the model's complexity or \"effective capacity\".\n    -   Similarly, comparing $\\mathcal{S}_3$ and $\\mathcal{S}_4$ shows that the same regularization successfully controls overfitting even in the much higher-capacity model.\n    -   The mention that **early stopping** is effective in $\\mathcal{S}_2$ is another key piece of evidence. Early stopping is a form of regularization that halts training at the point of maximum test performance, explicitly preventing the model from entering the later stages of training where overfitting becomes severe.\n\n4.  **Evaluate the Options based on the Analysis**\n    -   **A:** Incorrect. The generalization gap is a statistical phenomenon of overfitting, not a direct consequence of the CD algorithm's gradient bias. The empirical data also shows that larger models have a *larger* gap, contradicting the claim that they \"close the gap\".\n    -   **B:** Correct. This statement perfectly aligns with our analysis. It correctly identifies that the increasing gap with $H$ is due to higher capacity weakening uniform convergence. It also correctly states that $\\ell_2$ weight decay and early stopping are effective regularization techniques that reduce effective capacity and mitigate overfitting.\n    -   **C:** Incorrect. While the partition function $Z(\\theta)$ cancels out when calculating $\\Delta(\\theta)$, this does not make the gap invariant to capacity. The capacity ($H$) determines the flexibility of the energy function $E_{\\theta}(v)$. A higher capacity allows $E_{\\theta}(v)$ to be fine-tuned to the training set, leading to a larger difference between the average energy on the train and test sets, and thus a larger gap. The claim that regularization is ineffective is also directly contradicted by the data.\n    -   **D:** Incorrect. This statement mischaracterizes the effect of regularization. $\\ell_2$ weight decay *reduces* overfitting and *decreases* the generalization gap, typically at the cost of a slightly lower training likelihood. The suggestion to remove regularization to reduce the gap is the opposite of the correct action.\n\nTherefore, statement B provides the most accurate and comprehensive explanation of the observed phenomena.",
            "answer": "$$\n\\boxed{B}\n$$"
        }
    ]
}