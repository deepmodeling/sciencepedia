## 引言
[玻尔兹曼机](@entry_id:1121742)（Boltzmann Machines）是源于统计力学思想的一类强大的随机[生成模型](@entry_id:177561)，它们通过为系统[状态分配](@entry_id:172668)“能量”来学习复杂的数据分布。然而，通用的[玻尔兹曼机](@entry_id:1121742)由于其内部连接复杂，面临着巨大的计算挑战，这构成了其广泛应用的主要障碍。本文旨在系统性地介绍[玻尔兹曼机](@entry_id:1121742)，特别是其在计算上更易于处理且影响深远的变体——[受限玻尔兹曼机](@entry_id:636627)（RBM），从而填补理论与实用之间的知识鸿沟。

在接下来的章节中，读者将踏上一段从理论到实践的旅程。第一章“**原理与机制**”将奠定理论基石，从[基于能量的模型](@entry_id:636419)和玻尔兹曼分布出发，详细剖析RBM的结构、关键的[条件独立性](@entry_id:262650)，并推导其核心学习算法——对比散度。第二章“**应用与跨学科联系**”将视野拓展至实际应用，展示RBM如何作为[特征学习](@entry_id:749268)器驱动深度网络、赋能[推荐系统](@entry_id:172804)，并探讨其与计算神经科学、物理学等领域的深刻联系。最后，在“**动手实践**”部分，我们将通过具体问题引导读者巩固所学，将抽象的数学概念转化为可操作的见解。

## 原理与机制

本章深入探讨[玻尔兹曼机](@entry_id:1121742)及其重要变体的基本原理和核心机制。我们将从[基于能量的模型](@entry_id:636419)的普适概念出发，建立起玻尔兹曼分布的数学框架。随后，我们将详细定义通用[玻尔兹曼机](@entry_id:1121742)（BM）的结构，并重点剖析其在计算上更易于处理的变体——[受限玻尔兹曼机](@entry_id:636627)（RBM）。我们将推导 RBM 的关键数学性质，阐释其学习算法（特别是对比散度）的机制，并探讨其与神经形态计算及生物学习规则的深刻联系。最后，我们将简要展望深度[玻尔兹曼机](@entry_id:1121742)（DBM），以揭示模型架构复杂化所带来的新挑战。

### [基于能量的模型](@entry_id:636419)与玻尔兹曼分布

在许多生成式建模任务中，我们希望学习一个能够描述数据复杂分布的[概率模型](@entry_id:265150)。一种强大而灵活的方法是**[基于能量的模型](@entry_id:636419)（Energy-Based Models, EBMs）**。其核心思想并非直接定义[概率密度函数](@entry_id:140610)，而是为一个系统的每个可能状态 $\mathbf{x}$ 分配一个标量**能量** $E(\mathbf{x})$。该能量函数由模型参数 $\theta$ 决定，记作 $E_\theta(\mathbf{x})$。能量的直观含义是：能量越低的状态，其出现的可能性越高。

这种可能性与能量之间的定量关系由**玻尔兹曼-吉布斯分布（Boltzmann-Gibbs distribution）**给出，该分布源于统计力学：

$$
p(\mathbf{x}) = \frac{1}{Z} \exp(-E(\mathbf{x}))
$$

在此方程中，[指数函数](@entry_id:161417)确保了概率值非负，且能量越低，$\exp(-E(\mathbf{x}))$ 的值越大。$Z$ 是**[配分函数](@entry_id:140048)（partition function）**，定义为对所有可能状态的[玻尔兹曼因子](@entry_id:141054)（Boltzmann factor）求和：

$$
Z = \sum_{\mathbf{x}'} \exp(-E(\mathbf{x}'))
$$

[配分函数](@entry_id:140048) $Z$ 的作用是确保概率分布的总和为 1，即 $\sum_{\mathbf{x}'} p(\mathbf{x}') = 1$。然而，这也正是[基于能量的模型](@entry_id:636419)在计算上面临的核心挑战。对于一个具有 $N$ 个二元单元的系统，其[状态空间](@entry_id:160914)的大小为 $2^N$。因此，对所有状态进行求和通常是**计算上不可行的（computationally intractable）**。精确计算[配分函数](@entry_id:140048)的问题在复杂[度理论](@entry_id:636058)中被归类为 **$\#\mathrm{P}$-困难**，这意味着即使对于中等规模的模型，直接计算也绝无可能  。这一根本性的困难催生了后续章节中将要讨论的近似学习与推断算法。

在某些物理模型中，分布写作 $p(\mathbf{x}) \propto \exp(-\beta E(\mathbf{x}))$，其中 $\beta$ 是[逆温](@entry_id:140086)度参数。在机器学习的语境下，这个 $\beta$ 参数通常被吸收到能量函数的定义中，因此我们默认 $\beta=1$ 而不失[一般性](@entry_id:161765)。

### 通用[玻尔兹曼机](@entry_id:1121742) (General Boltzmann Machine)

**[玻尔兹曼机](@entry_id:1121742)（Boltzmann Machine, BM）** 是一种将[基于能量的模型](@entry_id:636419)与神经[网络结构](@entry_id:265673)相结合的随机生成模型。一个通用的[玻尔兹曼机](@entry_id:1121742)由一组二元随机单元组成，这些单元通常被划分为**可见单元（visible units）** $v \in \{0,1\}^{n_v}$ 和**隐藏单元（hidden units）** $h \in \{0,1\}^{n_h}$。可见单元直接与数据交互，而隐藏单元则用于捕捉数据中的高阶相关性。

通用[玻尔兹曼机](@entry_id:1121742)的图结构是一个[无向图](@entry_id:270905)，其中每个单元都被视为一个节点。其能量函数是网络状态的二次函数，包含了对每个单元的偏置（bias）项和对每对连接单元的相互作用（interaction）项。假设可见单元内部、隐藏单元内部以及两层之间都存在连接，其能量函数可以被精确地定义。

设可见层内部的对称权重矩阵为 $W_{vv} \in \mathbb{R}^{n_v \times n_v}$，隐藏层内部的对称权重矩阵为 $W_{hh} \in \mathbb{R}^{n_h \times n_h}$，连接可见层与隐藏层的权重矩阵为 $W_{vh} \in \mathbb{R}^{n_v \times n_h}$。可见单元的偏置为 $b \in \mathbb{R}^{n_v}$，隐藏单元的偏置为 $c \in \mathbb{R}^{n_h}$。为避免重复计算对称连接的能量（例如，从单元 $i$到 $j$ 的连接与从 $j$到 $i$ 的连接是同一个），层内[相互作用项](@entry_id:637283)需要一个 $\frac{1}{2}$ 的系数。因此，一个通用[玻尔兹曼机](@entry_id:1121742)的能量函数可以写为 ：

$$
E(v,h) = -\frac{1}{2}v^{\top}W_{vv}v - \frac{1}{2}h^{\top}W_{hh}h - v^{\top}W_{vh}h - b^{\top}v - c^{\top}h
$$

负号的约定是为了让正权重表示单元间的兴奋性（合作）关系，即当两个相连的单元同时激活时，系统的总能量会降低，从而使得该状态的概率更高。通用[玻尔兹曼机](@entry_id:1121742)强大的[表达能力](@entry_id:149863)源于其密集的连接，但这也使其训练和推断变得极其困难，因为网络中充满了短循环（例如，由任意三个相互连接的单元构成的长度为3的环），导致单元之间存在复杂的依赖关系。

### [受限玻尔兹曼机](@entry_id:636627) (Restricted Boltzmann Machine, RBM)

为了克服通用[玻尔兹曼机](@entry_id:1121742)在计算上的挑战，研究者提出了一种简化结构，即**[受限玻尔兹曼机](@entry_id:636627)（Restricted Boltzmann Machine, RBM）**。其核心“限制”在于**禁止层内连接**。这意味着可见单元之间没有连接，隐藏单元之间也没有连接。所有的连接都存在于可见层和隐藏层之间。

这个限制将模型的图结构变成了一个**二分图（bipartite graph）**。这一结构上的改变带来了显著的计算优势。由于层内连接被移除（即 $W_{vv}=0$ 和 $W_{hh}=0$），RBM的能量函数简化为 ：

$$
E(v,h) = -v^{\top}Wh - b^{\top}v - c^{\top}h
$$

其中，$W \in \mathbb{R}^{n_v \times n_h}$ 是连接可见层与隐藏层的权重矩阵。

从图论的角度看，这一限制消除了所有奇数长度的环。一个全连接的[玻尔兹曼机](@entry_id:1121742)中普遍存在长度为3的环（三角形），其**图 girth（[最短环](@entry_id:276378)路长度）** 为3。而在RBM中，由于其二分图的性质，任何环路都必须在可见层和隐藏层之间交替行进，因此最短的环路长度为4（例如，一个 $v_1 - h_1 - v_2 - h_2 - v_1$ 的路径）。这种结构上的差异对于采样算法的效率有着深远的影响 。

### RBM 的关键性质

RBM的[二分图](@entry_id:262451)结构赋予了它一组优雅且极具计算价值的性质，使其成为[深度学习](@entry_id:142022)中最成功的构建模块之一。

#### [条件独立性](@entry_id:262650)

RBM最重要的性质是其**[条件独立性](@entry_id:262650)（conditional independence）**。具体来说：
1.  **给定可见层的状态，所有隐藏单元是条件独立的。**
2.  **给定隐藏层的状态，所有可见单元是条件独立的。**

这个性质可以直接从能量函数推导出来。例如，我们来考察给定 $v$ 时 $h$ 的[条件概率分布](@entry_id:163069) $p(h|v)$：

$$
p(h|v) = \frac{p(v,h)}{p(v)} = \frac{\exp(-E(v,h))}{\sum_{h'}\exp(-E(v,h'))} \propto \exp(-E(v,h)) = \exp(v^{\top}Wh + b^{\top}v + c^{\top}h)
$$

由于 $b^{\top}v$ 项在给定 $v$ 时是常数，它可以被吸收到[归一化常数](@entry_id:752675)中。因此：

$$
p(h|v) \propto \exp(v^{\top}Wh + c^{\top}h) = \exp\left( \sum_{j=1}^{n_h} \left( \sum_{i=1}^{n_v} v_i W_{ij} + c_j \right) h_j \right) = \prod_{j=1}^{n_h} \exp\left( \left( \sum_{i=1}^{n_v} v_i W_{ij} + c_j \right) h_j \right)
$$

这个表达式可以分解为 $n_h$ 个独立项的乘积，每一项只依赖于一个隐藏单元 $h_j$。这意味着联合条件概率 $p(h|v)$ 可以分解为各个隐藏单元的独立条件概率的乘积  ：

$$
p(h|v) = \prod_{j=1}^{n_h} p(h_j|v)
$$

通过完全对称的推导，我们同样可以得到 $p(v|h) = \prod_{i=1}^{n_v} p(v_i|h)$。这种[条件独立性](@entry_id:262650)是RBM计算效率的关键所在。它允许我们并行地、精确地对一整层的单元进行采样，而无需考虑该层内部单元之间的相互作用。

#### 条件激活概率

基于[条件独立性](@entry_id:262650)，我们可以推导出单个单元的**激活概率（activation probability）**。对于一个隐藏单元 $h_j$，其激活（即 $h_j=1$）的条件概率为：

$$
p(h_j=1|v) = \frac{\exp\left( \sum_{i} v_i W_{ij} + c_j \right)}{ \exp(0) + \exp\left( \sum_{i} v_i W_{ij} + c_j \right) } = \frac{1}{1 + \exp\left( -(\sum_{i} v_i W_{ij} + c_j) \right)}
$$

这正是**逻辑斯蒂函数（logistic sigmoid function）** $\sigma(x) = 1/(1+e^{-x})$。因此，我们得到一个简洁的表达式  ：

$$
p(h_j=1|v) = \sigma\left( c_j + \sum_i W_{ij}v_i \right)
$$

同样地，可见单元 $v_i$ 的激活概率为：

$$
p(v_i=1|h) = \sigma\left( b_i + \sum_j W_{ij}h_j \right)
$$

这些激活概率的形式非常直观：一个单元的激活概率取决于其偏置以及来自另一层单元的加权输入之和，并通过一个[非线性](@entry_id:637147)函数进行转换。

#### 自由能

为了分析RBM的性质，引入**自由能（free energy）**的概念非常有用。一个可见向量 $v$ 的自由能 $F(v)$ 被定义为：

$$
F(v) = -\log \sum_h \exp(-E(v,h))
$$

自由能可以被看作是可见层构型 $v$ 的“[有效能](@entry_id:139794)量”。通过 marginalizing out ([边缘化](@entry_id:264637))隐藏单元，可见向量 $v$ 的边缘概率可以简洁地写为 $p(v) = \frac{\exp(-F(v))}{Z}$。对于RBM，由于其特殊的结构，自由能有一个解析的[闭式](@entry_id:271343)解 ：

$$
F(v) = -b^{\top}v - \sum_{j=1}^{n_h} \log\left(1 + \exp\left(c_j + \sum_i W_{ij}v_i\right)\right)
$$

这个表达式的推导利用了隐藏单元的[条件独立性](@entry_id:262650)，它将对 $2^{n_h}$ 个[隐藏状态](@entry_id:634361)的求和转化为了 $n_h$ 个独立项的乘积。这个解析形式在推导RBM的学习规则时至关重要。

### RBM 的学习：最大似然与对比散度

#### 对数似然梯度

RBM的训练目标是调整模型参数 $\theta = (W, b, c)$，以最大化训练数据（假设为一个可见向量 $v$）的对数似然 $\log p(v)$。利用自由能的表达式 $\log p(v) = -F(v) - \log Z$，我们可以求其关于某个权重 $W_{ij}$ 的梯度：

$$
\frac{\partial \log p(v)}{\partial W_{ij}} = -\frac{\partial F(v)}{\partial W_{ij}} - \frac{\partial \log Z}{\partial W_{ij}}
$$

通过细致的推导，可以证明这两个项分别对应于两个[期望值](@entry_id:150961) ：

$$
\frac{\partial \log p(v)}{\partial W_{ij}} = \mathbb{E}_{p(h|v)}[v_i h_j] - \mathbb{E}_{p(v,h)}[v_i h_j]
$$

这个梯度具有一个非常优美的“Hebbian-like”形式，即两个相关性项的差异。
-   **第一项 $\mathbb{E}_{p(h|v)}[v_i h_j]$**，通常被称为**正相（positive phase）**。它表示在可见单元被“钳位（clamped）”到数据样本 $v$ 的条件下，可见单元 $v_i$ 和隐藏单元 $h_j$ 之间相关性的[期望值](@entry_id:150961)。这个期望是易于计算的，因为 $v_i$ 是固定的，而 $h_j$ 的期望就是其激活概率 $p(h_j=1|v)$。
-   **第二项 $\mathbb{E}_{p(v,h)}[v_i h_j]$**，被称为**负相（negative phase）**。它表示在模型自身的[联合分布](@entry_id:263960) $p(v,h)$ 下（即模型处于“自由运行”的平衡状态时），同样的相关性的[期望值](@entry_id:150961)。这个期望的计算是**不可行的**，因为它需要在整个模型[状态空间](@entry_id:160914)上求平均，这又回到了计算[配分函数](@entry_id:140048) $Z$ 的难题 。

学习规则的直观解释是：增加在数据中观察到的相关性（正相），同时减少模型自身生成的相关性（负相）。

#### 通过[马尔可夫链蒙特卡洛](@entry_id:138779)进行采样

由于负相的计算不可行，我们需要借助近似方法。标准的方法是使用**[马尔可夫链蒙特卡洛](@entry_id:138779)（Markov Chain Monte Carlo, MCMC）**方法从模型分布 $p(v,h)$ 中抽取样本，然后用这些样本的平均值来估计负相的期望。

**[吉布斯采样](@entry_id:139152)（Gibbs sampling）** 是一种广泛使用的[MCMC算法](@entry_id:751788)。它通过迭代地从每个变量的[条件分布](@entry_id:138367)中采样来更新系统状态。对于通用[玻尔兹曼机](@entry_id:1121742)，由于所有单元都相互依赖，只能进行逐个单元的更新（single-site Gibbs sampling），这个过程可能非常缓慢。

然而，RBM的[条件独立性](@entry_id:262650)结构使得[吉布斯采样](@entry_id:139152)异常高效。我们可以执行**块[吉布斯采样](@entry_id:139152)（block Gibbs sampling）**：
1.  给定可见层 $v^{(t)}$，并行地从 $p(h_j | v^{(t)})$ 中为所有隐藏单元 $h_j$ 采样，得到 $h^{(t+1)}$。这需要 $n_h$ 次独立的标量更新。
2.  给定隐藏层 $h^{(t+1)}$，并行地从 $p(v_i | h^{(t+1)})$ 中为所有可见单元 $v_i$ 采样，得到 $v^{(t+1)}$。这需要 $n_v$ 次独立的标量更新。

一次完整的吉布斯步骤（$v \to h \to v$）总共需要 $n_v + n_h$ 次标量条件更新 。这种并行更新的能力大大提高了[采样效率](@entry_id:754496)，是RBM相比于通用BM的一大优势 。

为了确保[MCMC采样](@entry_id:751801)能够收敛到[目标分布](@entry_id:634522) $p(v,h)$，采样过程（即马尔可夫链的转移核 $K(x \to x')$）必须满足**[细致平衡条件](@entry_id:265158)（detailed balance condition）**：$\pi(x) K(x \to x') = \pi(x') K(x' \to x)$，其中 $\pi$ 是目标[平稳分布](@entry_id:194199)。细致平衡保证了 $\pi$ 是马尔可夫链的一个[平稳分布](@entry_id:194199)。此外，如果链是**不可约的（irreducible）**（任何状态都可以从任何其他状态到达）和**非周期的（aperiodic）**（不会陷入固定循环），那么链就是遍历的，保证从任何初始状态出发，链的分布都会收敛到唯一的[平稳分布](@entry_id:194199) $\pi$ 。对于RBM的[吉布斯采样](@entry_id:139152)，这些条件通常都满足。

#### 对比散度 (Contrastive Divergence, CD)

尽管[吉布斯采样](@entry_id:139152)最终会收敛到模型分布，但收敛过程可能需要很多步。**对比散度（Contrastive Divergence, CD）**算法是一种实用的近似方法，它通过运行[吉布斯采样](@entry_id:139152)链仅 $k$ 个步骤来近似负相。

CD-$k$ 算法的流程如下：
1.  将可见单元 $v^{(0)}$ 初始化为训练数据样本。
2.  执行 $k$ 步块[吉布斯采样](@entry_id:139152)，从 $v^{(0)}$ 开始，交替采样隐藏层和可见层：$v^{(0)} \to h^{(0)} \to v^{(1)} \to h^{(1)} \to \dots \to v^{(k)} \to h^{(k)}$。
3.  梯度更新规则近似为：

$$
\Delta W_{ij} \propto \langle v_i h_j \rangle_{\text{data}} - \langle v_i h_j \rangle_{k}
$$

其中，$\langle v_i h_j \rangle_{\text{data}}$ 是由初始数据 $v^{(0)}$ 和采样得到的 $h^{(0)}$ 计算出的正相相关性，而 $\langle v_i h_j \rangle_{k}$ 是由 $k$ 步采样后得到的 $(v^{(k)}, h^{(k)})$ 计算出的负相相关性。

当 $k=1$ 时（即CD-1），这个算法特别高效。尽管CD-$k$（对于有限的 $k$）提供的梯度是有偏的，但实践证明它在训练RBM时非常有效。CD可以被看作是最小化数据分布与 $k$ 步后模型分布之间的散度，而非数据分布与模型真实平衡分布之间的散度。为了减轻CD-1的偏差，**持续性对比散度（Persistent CD, PCD）**被提出，它在多[次梯度](@entry_id:142710)更新之间维持一个持续的[马尔可夫链](@entry_id:150828)，而不是每次都从数据点重新开始，从而让采样链有更多时间探索[模型空间](@entry_id:635763) 。对于具有多模态、[崎岖能量景观](@entry_id:137117)的模型，更先进的采样技术如**并行[回火](@entry_id:182408)（Parallel Tempering）**或称**[副本交换蒙特卡洛](@entry_id:142860)（Replica Exchange [Monte Carlo](@entry_id:144354)）**可以用来改善采样器在不同能量谷之间的混合，从而得到更准确的负相估计 。

### RBM 的表征能力与应用

#### 高效的表征能力

RBM的一个显著优点是其强大的表征能力。一个具有 $n$ 个可见单元和 $m$ 个隐藏单元的RBM，其参数数量为 $nm + n + m$。然而，它可以精确地表示一个由 $2^m$ 个不同概率分布混合而成的复杂分布。具体来说，RBM的可见层边缘概率 $p(v)$可以被证明等价于一个[指数族](@entry_id:263444)分布的[混合模型](@entry_id:266571) ：

$$
p(v) \propto \exp(b^{\top}v) \prod_{j=1}^{m} (1 + \exp(c_j + w_j^{\top}v))
$$

这个公式表明，每个隐藏单元 $h_j$ 都为可见层的分布贡献了一个“专家”或“特征”。通过激活或不激活隐藏单元，RBM能够以组合的方式组合这些特征，从而以指数级的效率捕捉数据中的高阶相关性。相比之下，一个朴素的表格模型需要 $2^n - 1$ 个参数来描述 $n$ 个[二元变量](@entry_id:162761)的任意分布。当 $n$ 很大时，RBM的参数效率优势是压倒性的 。

#### 与神经形态和生物系统的联系

RBM的数学形式与生物神经元的随机行为有着有趣的对应关系。我们可以将RBM的二元随机单元映射到一个**随机脉冲神经元（stochastic spiking neuron）**模型上 。假设一个神经形态单元的膜电位 $U_i$ 由其偏置、突触输入和内在噪声 $\eta_i$ 决定：$U_i = \beta_i + \sum_{j} J_{ij} v_j + \eta_i$。如果该神经元在 $U_i$ 超过阈值 $\theta_i$ 时发放脉冲，并且噪声 $\eta_i$ 服从**逻辑斯蒂分布**，其[累积分布函数](@entry_id:143135)为 $F_{\eta}(x) = 1/(1 + \exp(-x/T))$，其中 $T$ 是一个[尺度参数](@entry_id:268705)（类似于温度）。

那么，该神经元的放电概率 $P(U_i \ge \theta_i)$ 可以推导为：

$$
P(\text{fire}) = \sigma\left( \frac{\beta_i - \theta_i}{T} + \sum_j \frac{J_{ij}}{T} v_j \right)
$$

将此表达式与RBM的隐藏单元激活概率 $p(h_i=1|v) = \sigma(c_i + \sum_j W_{ij} v_j)$ 进行比较，我们可以建立起RBM抽象参数与神经形态硬件物理参数之间的直接映射关系 ：

$$
W_{ij} = \frac{J_{ij}}{T} \quad \text{and} \quad c_i = \frac{\beta_i - \theta_i}{T}
$$

这表明，RBM的权重和偏置可以分别对应于神经形态电路中的突触权重和有效阈值（偏置与物理阈值的差），并由系统噪声水平（温度 $T$）进行缩放。

此外，RBM的CD学习规则，作为一种**两阶段（two-phase）**、基于相关性差异的学习范式，与一些生物学上合理的[学习理论](@entry_id:634752)，如**对比[赫布学习](@entry_id:156080)（Contrastive Hebbian Learning, CHL）**，有着惊人的相似之处 。CHL假设学习发生在两个阶段之间：一个是被外部输入“钳位”的阶段，另一个是网络“自由”活动的阶段。突触权重的更新正比于这两个阶段[神经元活动](@entry_id:174309)相关性的差异，即 $\Delta W_{ij} \propto \langle x_i x_j \rangle_{\text{clamped}} - \langle x_i x_j \rangle_{\text{free}}$。CD学习规则本质上也是这种形式，正相对应于“钳位”阶段，负相对应于“自由”阶段。尽管在具体实现上存在差异（例如，CHL通常需要对称权重 $W_{ij}=W_{ji}$，这是一个生物学上较强的约束），但这种形式上的共性暗示了自然和人工学习系统可能共享某些基本计算原理 。

### 超越 RBM：深度[玻尔兹曼机](@entry_id:1121742) (DBM)

将RBM的成功经验自然地推广，便得到了**深度[玻尔兹曼机](@entry_id:1121742)（Deep Boltzmann Machine, DBM）**。一个DBM由多个堆叠的隐藏层构成，连接只存在于相邻层之间，形成一个多层二分图的结构。例如，一个两层隐藏层的DBM的能量函数为 ：

$$
E(v,h^{(1)},h^{(2)}) = - v^{\top} W^{(1)} h^{(1)} - (h^{(1)})^{\top} W^{(2)} h^{(2)} - b^{\top} v - (c^{(1)})^{\top} h^{(1)} - (c^{(2)})^{\top} h^{(2)}
$$

然而，DBM的“深度”带来了一个关键的计算挑战，使得它比RBM更难处理。在DBM中，即使给定可见层 $v$，两个隐藏层 $h^{(1)}$ 和 $h^{(2)}$ 之间仍然通过权重 $W^{(2)}$ 相互耦合。这意味着RBM的关键性质——给定可见层时隐藏单元的[条件独立性](@entry_id:262650)——**在DBM中不再成立**。$p(h^{(1)}, h^{(2)}|v)$ 不能再分解为 $p(h^{(1)}|v)p(h^{(2)}|v)$。

这种现象被称为“**[解释消除](@entry_id:203703)（explaining away）**”：当两个隐藏层共同解释可见数据时，它们之间会产生依赖。其后果是，DBM中的精确后验推断（即计算 $p(h^{(1)}, h^{(2)}|v)$）变得不可行，因为它需要对一个具有复杂依赖关系的通用[伊辛模型](@entry_id:139066)（Ising model）进行推断，这是一个 $\#\mathrm{P}$-困难问题 。因此，DBM的训练和推断不仅需要像RBM那样近似负相，还需要近似计算正相（即[后验分布](@entry_id:145605)），这通常通过**平均场[变分推断](@entry_id:634275)（mean-field variational inference）**等更复杂的近似技术来完成。