{
    "hands_on_practices": [
        {
            "introduction": "Understanding a model begins with its fundamental parameters. This exercise challenges you to determine the true number of independent parameters in a general Boltzmann Machine by analyzing its energy function. By exploring why we can enforce weight matrix symmetry and a zero diagonal without loss of generality, you will gain a deeper appreciation for which parameters are truly identifiable and essential to the model's expressive power.",
            "id": "4038503",
            "problem": "Consider a fully connected Boltzmann machine with $n$ binary units $x_{i} \\in \\{0,1\\}$, pairwise interaction weights collected in a real matrix $W \\in \\mathbb{R}^{n \\times n}$, and biases $b \\in \\mathbb{R}^{n}$. The probability of a state $x \\in \\{0,1\\}^{n}$ is given by a Boltzmann distribution $p(x) \\propto \\exp\\!\\big(-E(x)\\big)$, where $E(x)$ is an energy function derived from pairwise interactions and biases. Assume only pairwise interactions and unit-wise biases are present (no higher-order terms). Impose the physical consistency requirement that the pairwise coupling energy is invariant to relabeling of unit indices and that self-interaction terms that are not identifiable from the state space are excluded. \n\nUsing only these foundational assumptions, determine the number of independent parameters in the model when one enforces $W$ to be symmetric and to have zero diagonal. Explicitly count the independent interaction weights and the independent biases, and give a single closed-form expression in terms of $n$ for the total number of independent parameters. Explain, in your reasoning, why imposing $W = W^{\\top}$ and $\\operatorname{diag}(W)=0$ does not reduce the expressive family of models relative to allowing arbitrary real $W$ and $b$. \n\nProvide your final answer as a single closed-form analytic expression in terms of $n$. No numerical substitution is required and no rounding is needed.",
            "solution": "The problem asks for the total number of independent parameters in a fully connected Boltzmann machine with $n$ binary units, under the constraints that the weight matrix $W$ is symmetric with a zero diagonal. It also requires an explanation for why these constraints do not reduce the expressive power of the model.\n\nLet the state of the system be a vector $x \\in \\{0, 1\\}^n$. The most general energy function $E'(x)$ for a model with only pairwise interactions and unit-wise biases can be formulated using a general real-valued weight matrix $W' \\in \\mathbb{R}^{n \\times n}$ and a bias vector $b' \\in \\mathbb{R}^n$:\n$$E'(x) = - \\sum_{i=1}^n \\sum_{j=1}^n x_i W'_{ij} x_j - \\sum_{i=1}^n b'_i x_i$$\nThe probability of a state $x$ depends on $E(x)$ only through the exponential function, so any parameters in the energy function that can be combined or eliminated without altering the energy value for any state $x$ are not independent. We will now show that an arbitrary model defined by $(W', b')$ can be reparametrized into an equivalent model $(W, b)$ where $W$ is symmetric ($W=W^\\top$) and has a zero diagonal ($\\operatorname{diag}(W)=0$), without any loss of expressive power.\n\nFirst, let us analyze the diagonal terms of the weight matrix $W'$. We can separate the double summation in the energy function into diagonal and off-diagonal parts:\n$$\\sum_{i=1}^n \\sum_{j=1}^n x_i W'_{ij} x_j = \\sum_{i \\neq j} x_i W'_{ij} x_j + \\sum_{i=1}^n x_i W'_{ii} x_i$$\nA crucial property of the binary units $x_i \\in \\{0, 1\\}$ is that $x_i^2 = x_i$. Applying this to the diagonal sum, we get:\n$$\\sum_{i=1}^n x_i W'_{ii} x_i = \\sum_{i=1}^n W'_{ii} x_i^2 = \\sum_{i=1}^n W'_{ii} x_i$$\nSubstituting this back into the expression for $E'(x)$:\n$$E'(x) = - \\sum_{i \\neq j} x_i W'_{ij} x_j - \\sum_{i=1}^n W'_{ii} x_i - \\sum_{i=1}^n b'_i x_i$$\nWe can combine the two linear terms:\n$$E'(x) = - \\sum_{i \\neq j} x_i W'_{ij} x_j - \\sum_{i=1}^n (b'_i + W'_{ii}) x_i$$\nThis demonstrates that the effect of the diagonal weight $W'_{ii}$ is indistinguishable from the effect of the bias $b'_i$. The terms are not separately identifiable. We can therefore define a new set of parameters: a weight matrix with a zero diagonal and a new bias vector. By setting the diagonal elements of the weight matrix to zero and defining new biases $b_i = b'_i + W'_{ii}$, the energy function remains identical for every state $x$. Thus, we can enforce the constraint $\\operatorname{diag}(W) = 0$ without any loss of generality.\n\nNext, we address the symmetry of the weight matrix. Consider the off-diagonal summation term $\\sum_{i \\neq j} x_i W'_{ij} x_j$. Since the indices $i$ and $j$ are symmetric in the product $x_i x_j$ (i.e., $x_i x_j = x_j x_i$), we can group the terms for each pair of indices $(i, j)$ with $i \\neq j$:\n$$x_i W'_{ij} x_j + x_j W'_{ji} x_i = (W'_{ij} + W'_{ji}) x_i x_j$$\nThis shows that the energy function only depends on the sum $W'_{ij} + W'_{ji}$, not on the individual values of $W'_{ij}$ and $W'_{ji}$. We can define a new, symmetric weight matrix $W$ by setting its elements to be the average of the corresponding elements in the off-diagonal part of $W'$ and its transpose:\n$$W_{ij} = \\frac{W'_{ij} + W'_{ji}}{2} \\text{ for } i \\neq j, \\text{ and } W_{ii} = 0$$\nFor this new matrix $W$, we have $W_{ij} = W_{ji}$, so it is symmetric. The energy component for the pair $(i, j)$ using the new matrix $W$ is:\n$$x_i W_{ij} x_j + x_j W_{ji} x_i = 2 W_{ij} x_i x_j = (W'_{ij} + W'_{ji}) x_i x_j$$\nThis is precisely the same as the original contribution. Thus, replacing the potentially non-symmetric off-diagonal weights with symmetric ones does not change the energy function for any state $x$. This justifies enforcing the constraint $W = W^\\top$.\n\nHaving established that the constraints of a symmetric weight matrix with a zero diagonal do not reduce the family of representable probability distributions, we can now count the number of independent parameters for such a model.\n\n1.  **Independent interaction weights:** The parameters are the elements of the $n \\times n$ matrix $W$. The constraint $\\operatorname{diag}(W) = 0$ implies that all $n$ diagonal elements are fixed at $0$. The symmetry constraint $W = W^\\top$ means $W_{ij} = W_{ji}$. Therefore, we only need to specify the elements in the strict upper triangle (i.e., with $i  j$), as the other elements are then determined. The number of elements in the strict upper triangle is the number of ways to choose $2$ distinct indices from a set of $n$, which is given by the binomial coefficient $\\binom{n}{2}$.\n    Number of independent weights = $\\binom{n}{2} = \\frac{n(n-1)}{2}$.\n\n2.  **Independent biases:** The bias parameters are the components of the vector $b \\in \\mathbb{R}^n$. There is one bias parameter $b_i$ for each of the $n$ units. These are all independent.\n    Number of independent biases = $n$.\n\n3.  **Total number of independent parameters:** The total number of independent parameters in the model is the sum of the number of independent weights and independent biases.\n    Total parameters = (Number of independent weights) + (Number of independent biases)\n    $$N_{\\text{params}} = \\frac{n(n-1)}{2} + n$$\n    Simplifying this expression:\n    $$N_{\\text{params}} = \\frac{n^2 - n}{2} + \\frac{2n}{2} = \\frac{n^2 - n + 2n}{2} = \\frac{n^2 + n}{2}$$\n    This can be written in a more compact closed form:\n    $$N_{\\text{params}} = \\frac{n(n+1)}{2}$$\n\nThis expression represents the total number of free parameters required to specify a general Boltzmann machine with $n$ binary units and pairwise interactions.",
            "answer": "$$\n\\boxed{\\frac{n(n+1)}{2}}\n$$"
        },
        {
            "introduction": "The 'restricted' nature of an RBM is the source of its computational advantages over a general Boltzmann Machine. This practice exercise guides you through a concrete example to reveal the RBM's cornerstone property: the conditional independence of its hidden units given a visible state. By enumerating all possible states and computing probabilities from first principles, you will build a solid, intuitive understanding of how the RBM's structure simplifies inference and learning.",
            "id": "4038508",
            "problem": "Consider a binary-valued Restricted Boltzmann Machine (RBM) with $n_v=2$ visible units and $n_h=2$ hidden units. The visible units are $v=(v_1,v_2)\\in\\{0,1\\}^2$, the hidden units are $h=(h_1,h_2)\\in\\{0,1\\}^2$, and the energy function is defined by\n$$\nE(v,h)=-\\sum_{i=1}^{2} a_i v_i-\\sum_{j=1}^{2} b_j h_j-\\sum_{i=1}^{2}\\sum_{j=1}^{2} v_i W_{ij} h_j.\n$$\nThe joint distribution over $(v,h)$ is given by the Boltzmann distribution $P(v,h)=\\exp(-E(v,h))/Z$, where $Z$ is the partition function.\n\nLet the parameters be\n$$\na=\\begin{pmatrix}0.3\\\\-0.1\\end{pmatrix},\\quad b=\\begin{pmatrix}0.2\\\\0.4\\end{pmatrix},\\quad W=\\begin{pmatrix}0.7  -0.5\\\\ -0.3  0.6\\end{pmatrix}.\n$$\n\nTasks:\n1. Starting from the Boltzmann distribution and the given energy function, derive an explicit expression for the conditional distribution $P(h\\mid v)$ for arbitrary $v\\in\\{0,1\\}^2$, without assuming any special factorization a priori.\n2. Using only the bipartite structure encoded in the energy function, prove from first principles that the hidden units are conditionally independent given $v$, and identify the parameterization of the resulting Bernoulli product form.\n3. Enumerate all $16$ configurations $(v,h)\\in\\{0,1\\}^2\\times\\{0,1\\}^2$ and compute the corresponding energies $E(v,h)$ and the unnormalized weights $\\exp(-E(v,h))$. For each fixed $v$, explicitly normalize over the four hidden states $h$ to obtain $P(h\\mid v)$ and verify that it equals the Bernoulli product form obtained in Task $2$.\n4. For the specific visible configuration $v=(1,0)$, compute the exact value of $P(h_1=1\\mid v)$.\n\nReport as your final answer the exact expression for $P(h_1=1\\mid v=(1,0))$ in terms of the exponential function. Do not numerically approximate. No units are required.",
            "solution": "This problem involves deriving and verifying the properties of a Restricted Boltzmann Machine.\n\n**Task 1: Derivation of the conditional distribution $P(h\\mid v)$**\n\nThe conditional distribution $P(h\\mid v)$ is $P(h\\mid v) = \\frac{P(v,h)}{P(v)}$. The joint probability is $P(v,h) = \\frac{1}{Z}\\exp(-E(v,h))$, and the marginal $P(v) = \\sum_{h'} P(v,h') = \\frac{1}{Z}\\sum_{h'} \\exp(-E(v,h'))$.\nSubstituting these gives:\n$$\nP(h\\mid v) = \\frac{\\frac{1}{Z}\\exp(-E(v,h))}{\\frac{1}{Z}\\sum_{h'} \\exp(-E(v,h'))} = \\frac{\\exp(-E(v,h))}{\\sum_{h'} \\exp(-E(v,h'))}\n$$\nThe energy function is $E(v,h) = -\\sum_{i} a_i v_i - \\sum_{j} b_j h_j - \\sum_{i,j} v_i W_{ij} h_j$. The term $-\\sum_{i} a_i v_i$ depends only on $v$ and is constant with respect to $h$. Thus, $\\exp(\\sum_{i} a_i v_i)$ can be factored out and canceled from the numerator and denominator:\n$$\nP(h\\mid v) = \\frac{\\exp\\left(\\sum_{j} b_j h_j + \\sum_{i,j} v_i W_{ij} h_j\\right)}{\\sum_{h'} \\exp\\left(\\sum_{j} b_j h'_j + \\sum_{i,j} v_i W_{ij} h'_j\\right)}\n$$\nWe can regroup the terms in the exponent:\n$$\nP(h\\mid v) = \\frac{\\exp\\left(\\sum_{j} \\left(b_j + \\sum_{i} v_i W_{ij}\\right) h_j\\right)}{\\sum_{h'} \\exp\\left(\\sum_{j} \\left(b_j + \\sum_{i} v_i W_{ij}\\right) h'_j\\right)}\n$$\n\n**Task 2: Proof of Conditional Independence of Hidden Units**\n\nThe structure of the energy function lacks any terms of the form $h_j h_k$ for $j \\neq k$. This is the key to conditional independence. Let's analyze the expression from Task 1. Define an effective bias $\\theta_j(v) = b_j + \\sum_{i} v_i W_{ij}$. The exponent becomes a sum over independent terms for each $h_j$: $\\sum_{j} \\theta_j(v) h_j$.\nThe numerator of $P(h|v)$ can be factorized as $\\exp\\left(\\sum_{j} \\theta_j(v) h_j\\right) = \\prod_{j} \\exp(\\theta_j(v) h_j)$.\nThe denominator (the conditional partition function) also factorizes:\n$$\n\\sum_{h' \\in \\{0,1\\}^2} \\exp\\left(\\sum_{j=1}^{2} \\theta_j(v) h'_j\\right) = \\sum_{h'_1, h'_2} \\prod_{j=1}^{2} \\exp(\\theta_j(v) h'_j) = \\left(\\sum_{h'_1 \\in \\{0,1\\}} \\exp(\\theta_1(v) h'_1)\\right) \\left(\\sum_{h'_2 \\in \\{0,1\\}} \\exp(\\theta_2(v) h'_2)\\right)\n$$\nEach term in the product is $(1 + \\exp(\\theta_j(v)))$. So the denominator is $\\prod_{j=1}^{2} (1 + \\exp(\\theta_j(v)))$.\nCombining numerator and denominator:\n$$\nP(h\\mid v) = \\frac{\\prod_{j=1}^{2} \\exp(\\theta_j(v) h_j)}{\\prod_{j=1}^{2} (1 + \\exp(\\theta_j(v)))} = \\prod_{j=1}^{2} \\frac{\\exp(\\theta_j(v) h_j)}{1 + \\exp(\\theta_j(v))}\n$$\nThis shows that $P(h\\mid v)$ is a product of distributions for each hidden unit, i.e., $P(h\\mid v) = P(h_1\\mid v) P(h_2\\mid v)$. This is the definition of conditional independence. Each $P(h_j \\mid v)$ is a Bernoulli distribution where the probability of $h_j=1$ is $P(h_j=1\\mid v) = \\frac{\\exp(\\theta_j(v))}{1 + \\exp(\\theta_j(v))} = \\sigma(\\theta_j(v))$, the logistic sigmoid function.\n\n**Task 3: Enumeration and Verification**\n\nFirst, compute the effective biases $\\theta_j(v) = b_j + v_1 W_{1j} + v_2 W_{2j}$ for each $v$:\n-   $v=(0,0)$: $\\theta_1=0.2$, $\\theta_2=0.4$.\n-   $v=(0,1)$: $\\theta_1=0.2-0.3=-0.1$, $\\theta_2=0.4+0.6=1.0$.\n-   $v=(1,0)$: $\\theta_1=0.2+0.7=0.9$, $\\theta_2=0.4-0.5=-0.1$.\n-   $v=(1,1)$: $\\theta_1=0.2+0.7-0.3=0.6$, $\\theta_2=0.4-0.5+0.6=0.5$.\n\nWe will verify for the case $v=(1,0)$.\n**Method 1: From the joint distribution.** The conditional probability is $P(h|v) = \\frac{\\exp(-E(v,h))}{\\sum_{h'} \\exp(-E(v,h'))}$. For $v=(1,0)$, $-E(v,h) = 0.3 + 0.9h_1 - 0.1h_2$. The unnormalized joint probabilities $\\exp(-E((1,0),h))$ for $h \\in \\{(0,0), (0,1), (1,0), (1,1)\\}$ are $\\exp(0.3)$, $\\exp(0.2)$, $\\exp(1.2)$, and $\\exp(1.1)$ respectively.\nLet's check $P(h=(1,1)|v=(1,0))$. From direct normalization:\n$$P(h=(1,1)|v=(1,0)) = \\frac{\\exp(-E((1,0),(1,1)))}{\\sum_{h'}\\exp(-E((1,0),h'))} = \\frac{\\exp(1.1)}{\\exp(0.3)+\\exp(0.2)+\\exp(1.2)+\\exp(1.1)}$$\nWe can factor $\\exp(0.3)$ from the denominator: $\\exp(0.3)[1+\\exp(-0.1)+\\exp(0.9)+\\exp(0.8)]$. The numerator is $\\exp(0.3)\\exp(0.8)$.\nThe probability simplifies to $\\frac{\\exp(0.8)}{1+\\exp(-0.1)+\\exp(0.9)+\\exp(0.8)}$.\nThe denominator can be further factored: $1+\\exp(-0.1)+\\exp(0.9)+\\exp(0.8) = (1+\\exp(0.9))(1+\\exp(-0.1))$.\nThus, from direct normalization, $P(h=(1,1)|v=(1,0)) = \\frac{\\exp(0.8)}{(1+\\exp(0.9))(1+\\exp(-0.1))}$.\n\n**Method 2: From the product form.** For $v=(1,0)$, we have $\\theta_1=0.9, \\theta_2=-0.1$.\n$$P(h=(1,1)|v=(1,0)) = \\sigma(0.9)\\sigma(-0.1) = \\left(\\frac{\\exp(0.9)}{1+\\exp(0.9)}\\right)\\left(\\frac{\\exp(-0.1)}{1+\\exp(-0.1)}\\right) = \\frac{\\exp(0.8)}{(1+\\exp(0.9))(1+\\exp(-0.1))}$$\nThe results from both methods match perfectly. The verification holds for all other combinations of $(v,h)$ by analogous calculations.\n\n**Task 4: Computation of $P(h_1=1\\mid v=(1,0))$**\n\nWe need to compute $P(h_1=1\\mid v)$ for the specific visible configuration $v=(1,0)$. Using the formula derived in Task 2:\n$$\nP(h_1=1\\mid v) = \\sigma\\left(b_1 + \\sum_{i=1}^{2} v_i W_{i1}\\right)\n$$\nFor $v=(1,0)$, we have $v_1=1$ and $v_2=0$. The argument of the sigmoid function is:\n$$\n\\theta_1(1,0) = b_1 + v_1 W_{11} + v_2 W_{21} = 0.2 + (1)(0.7) + (0)(-0.3) = 0.9\n$$\nSubstituting this value into the sigmoid function:\n$$\nP(h_1=1\\mid v=(1,0)) = \\sigma(0.9) = \\frac{1}{1 + \\exp(-0.9)}\n$$\nThis is the exact final expression.",
            "answer": "$$\n\\boxed{\\frac{1}{1 + \\exp(-0.9)}}\n$$"
        },
        {
            "introduction": "A theoretical model is only as good as its ability to generalize from training data to unseen examples. This exercise places you in the role of a machine learning practitioner, analyzing training and test performance metrics to diagnose a classic problem: overfitting. By examining how model capacity and regularization affect the generalization gap, you will develop critical skills for training robust models that perform well in practice.",
            "id": "4038496",
            "problem": "A binary Restricted Boltzmann Machine (RBM) with $D$ visible units and $H$ hidden units defines a model distribution $p_{\\theta}(v)$ over visible configurations $v \\in \\{0,1\\}^{D}$ through an energy-based parametrization and a partition function. Maximum likelihood learning seeks parameters $\\theta$ that maximize the expected log-likelihood of the data under $p_{\\theta}(v)$. Consider a dataset sampled from an unknown distribution $p^{\\star}(v)$, and suppose training uses Contrastive Divergence with a fixed number of Gibbs steps, while model evaluation of $\\log p_{\\theta}(v)$ uses Annealed Importance Sampling with sufficiently long chains to yield negligible bias relative to variance.\n\nDefine the empirical training and test objectives\n$$\nJ_{\\text{train}}(\\theta) = \\mathbb{E}_{\\hat{p}_{\\text{train}}}[\\log p_{\\theta}(v)], \\quad J_{\\text{test}}(\\theta) = \\mathbb{E}_{\\hat{p}_{\\text{test}}}[\\log p_{\\theta}(v)],\n$$\nand the generalization gap\n$$\n\\Delta(\\theta) = J_{\\text{train}}(\\theta) - J_{\\text{test}}(\\theta).\n$$\nYou monitor $J_{\\text{train}}(\\theta_t)$ and $J_{\\text{test}}(\\theta_t)$ over epochs $t$ during training for several configurations. The following empirically observed behaviors are reported:\n\n- Configuration $\\mathcal{S}_1$: $H = 100$, no explicit regularization ($\\lambda = 0$). $J_{\\text{train}}(\\theta_t)$ increases steadily with $t$, while $J_{\\text{test}}(\\theta_t)$ initially increases but later decreases. $\\Delta(\\theta_t)$ grows from approximately $0.5$ nats at epoch $t = 10$ to approximately $5.0$ nats at epoch $t = 200$.\n\n- Configuration $\\mathcal{S}_2$: $H = 100$, with $\\ell_{2}$ weight decay of strength $\\lambda = 10^{-4}$. $J_{\\text{train}}(\\theta_t)$ increases moderately with $t$, $J_{\\text{test}}(\\theta_t)$ increases and stabilizes, and $\\Delta(\\theta_t)$ peaks near approximately $1.2$ nats before decreasing to approximately $0.8$ nats. Early stopping at the epoch where $J_{\\text{test}}$ is maximized yields the best held-out performance.\n\n- Configuration $\\mathcal{S}_3$: $H = 400$, $\\lambda = 0$. $J_{\\text{train}}(\\theta_t)$ increases rapidly with $t$, $J_{\\text{test}}(\\theta_t)$ increases briefly then decreases more sharply, and $\\Delta(\\theta_t)$ grows from approximately $0.6$ nats at epoch $t = 10$ to approximately $9.0$ nats at epoch $t = 200$.\n\n- Configuration $\\mathcal{S}_4$: $H = 400$, $\\lambda = 10^{-4}$. $J_{\\text{train}}(\\theta_t)$ increases, $J_{\\text{test}}(\\theta_t)$ increases then stabilizes, and $\\Delta(\\theta_t)$ remains moderate, near approximately $2.0$ nats by epoch $t = 200$.\n\nAssume the dataset size is fixed and moderate relative to the number of parameters ($D \\cdot H$), and that optimization noise (e.g., learning rate stochasticity) and log-likelihood estimator variance do not dominate the observed trends. Starting from the principles that maximum likelihood learning minimizes the Kullbackâ€“Leibler divergence $\\mathrm{KL}(p^{\\star} \\Vert p_{\\theta})$ via empirical risk minimization, and that model capacity influences uniform convergence between empirical and population risks, analyze the reported behavior of $\\Delta(\\theta_t)$ and its dependence on $H$ and $\\lambda$.\n\nWhich statement best explains the observed gaps and prescribes an effective intervention to improve generalization?\n\n- A. The growth in $\\Delta(\\theta_t)$ in $\\mathcal{S}_1$ and $\\mathcal{S}_3$ is primarily due to biased gradient estimates in Contrastive Divergence; increasing $H$ reduces this bias, so larger models will close the gap even without regularization.\n\n- B. The increase of $\\Delta(\\theta_t)$ with $H$ reflects weakened uniform convergence from higher capacity; adding $\\ell_{2}$ weight decay reduces effective capacity, thereby shrinking $\\Delta(\\theta_t)$, and early stopping selects parameters before overfitting fully manifests.\n\n- C. Because the RBM is an energy-based model, the partition function scaling with $H$ leaves the generalization gap invariant; thus $\\Delta(\\theta_t)$ is unrelated to capacity and cannot be mitigated by weight decay or early stopping.\n\n- D. $\\ell_{2}$ weight decay increases $J_{\\text{train}}(\\theta_t)$ faster than $J_{\\text{test}}(\\theta_t)$, which aggravates overfitting; therefore regularization should be removed to reduce $\\Delta(\\theta_t)$.",
            "solution": "The problem asks for the best explanation for a set of empirical observations related to training a Restricted Boltzmann Machine (RBM), focusing on the generalization gap $\\Delta(\\theta) = J_{\\text{train}}(\\theta) - J_{\\text{test}}(\\theta)$.\n\n**Core Concepts**\n1.  **Overfitting and Model Capacity:** Overfitting occurs when a model learns features specific to the training set that do not generalize to unseen data. This typically manifests as the training performance metric ($J_{\\text{train}}$) continuing to improve while the test performance metric ($J_{\\text{test}}$) stagnates or degrades. The model's capacity, primarily determined by the number of hidden units $H$ in an RBM, is a key factor. A higher capacity model is more flexible and can fit the training data more closely, but is also more prone to overfitting. The generalization gap, $\\Delta(\\theta)$, is a direct measure of this phenomenon. Statistical learning theory (e.g., via the principle of uniform convergence) predicts that for a fixed dataset size, higher capacity models are expected to have a larger potential generalization gap.\n2.  **Regularization:** Techniques like $\\ell_2$ weight decay (controlled by parameter $\\lambda$) and early stopping are used to combat overfitting. Weight decay penalizes large parameter values, effectively reducing the model's complexity or \"effective capacity\" and encouraging it to find simpler solutions that generalize better. Early stopping halts training at the point where test performance is maximal, preventing the model from further specializing to the training set at the expense of generalization.\n\n**Analysis of Empirical Observations**\n-   **Effect of Capacity ($H$):** Comparing $\\mathcal{S}_1$ ($H=100, \\lambda=0$) and $\\mathcal{S}_3$ ($H=400, \\lambda=0$), we see that the model with higher capacity ($\\mathcal{S}_3$) exhibits a much larger generalization gap ($\\Delta \\approx 9.0$ nats) than the lower-capacity model ($\\mathcal{S}_1$, $\\Delta \\approx 5.0$ nats). This directly supports the theory that higher capacity leads to more severe overfitting.\n-   **Effect of Regularization ($\\lambda$):** Comparing the unregularized scenarios ($\\mathcal{S}_1, \\mathcal{S}_3$) with their regularized counterparts ($\\mathcal{S}_2, \\mathcal{S}_4$), we observe that adding $\\ell_2$ weight decay consistently improves test performance (prevents it from decreasing) and significantly shrinks the generalization gap. For instance, in the $H=400$ case, $\\Delta$ is reduced from $\\approx 9.0$ nats to $\\approx 2.0$ nats. This demonstrates the effectiveness of regularization in reducing overfitting.\n-   **Early Stopping:** The note in $\\mathcal{S}_2$ that early stopping is effective is another classic sign of managing overfitting.\n\n**Evaluation of Options**\n-   **A:** This option incorrectly attributes the generalization gap to the bias in the Contrastive Divergence (CD) algorithm. The generalization gap is a statistical phenomenon related to model capacity and data size, not an artifact of the training algorithm's gradient approximation. The claim that increasing $H$ reduces CD bias is unsubstantiated and the conclusion that larger models close the gap is directly contradicted by the data.\n-   **B:** This statement provides a perfect explanation. It correctly links the growing gap $\\Delta(\\theta_t)$ to higher capacity ($H$) through the lens of weakened uniform convergence. It correctly identifies that $\\ell_2$ decay works by reducing the effective capacity, which shrinks the gap. Finally, it correctly interprets early stopping as a method to select the model at the point of peak generalization before overfitting becomes severe. This aligns perfectly with both theory and the provided data.\n-   **C:** This option makes a flawed argument. While the partition function term $\\log Z(\\theta)$ does cancel out when calculating the gap $\\Delta(\\theta)$, this does not make the gap invariant to capacity. The capacity ($H$) determines the flexibility of the energy function $E_{\\theta}(v)$. A higher capacity allows $E_{\\theta}(v)$ to fit the training data more closely (lower average energy on training samples), which can lead to a larger difference between average energies on train and test sets, thus increasing the gap. The conclusion is also directly falsified by the data.\n-   **D:** This option is the opposite of correct. Regularization's purpose is to *reduce* overfitting, not aggravate it. It typically leads to a lower (or equal) final training score $J_{\\text{train}}$ but a higher test score $J_{\\text{test}}$, thereby *reducing* the gap $\\Delta(\\theta)$. The provided data clearly shows that regularization helps.\n\n**Conclusion**\nOption B provides the most accurate and comprehensive explanation for the observed phenomena, correctly applying fundamental principles of machine learning concerning model capacity, overfitting, and regularization.",
            "answer": "$$\n\\boxed{B}\n$$"
        }
    ]
}