{
    "hands_on_practices": [
        {
            "introduction": "The Bienenstock-Cooper-Munro (BCM) learning rule elegantly unifies synaptic potentiation and depression within a single mathematical framework. To build intuition, this first exercise simplifies the rule by considering a fixed activity threshold, $\\theta$. By analyzing the weight dynamics under this condition, you will demonstrate how the BCM rule functions as a \"thresholded Hebbian\" mechanism, where synapses potentiate for high postsynaptic activity and depress for low activity, and you will derive the critical weight at which this transition occurs .",
            "id": "4037134",
            "problem": "Consider a single-rate neuron driven by one presynaptic channel with activity $x(t)$ and one synaptic weight $w(t)$. Assume a rate-coded postsynaptic response $y(t)$ that is linearly proportional to the net input, $y(t)=w(t)\\,x(t)$, and that the synaptic weight is constrained by a projection to the nonnegative orthant, $w(t)\\geq 0$, as is common in rectifying neuromorphic circuits. Let $x(t)\\equiv x0$ be constant over the time scale of synaptic changes. The Bienenstock-Cooper-Munro (BCM) learning rule (Bienenstock-Cooper-Munro (BCM)) for the synaptic weight is defined by an activity-gated update of the form $\\dot{w}(t)=\\eta\\,x(t)\\,y(t)\\big(y(t)-\\theta\\big)$, where $\\eta0$ is a learning-rate constant and $\\theta0$ is a fixed, small threshold parameter with the same units as $y(t)$. Starting from the assumptions of rate coding, nonnegativity of $w(t)$, and the BCM update structure above, derive the autonomous differential equation for $w(t)$ and perform a sign analysis of its right-hand side to identify the unique nonzero critical weight $w_{\\mathrm{c}}$ at which the plasticity switches from depression to potentiation. Use this sign analysis to justify that, for a fixed small $\\theta$, the BCM rule approximates a thresholded Hebbian rule that depresses synapses when the postsynaptic activity is low (that is, when $y(t)\\theta$) and potentiates when $y(t)\\theta$. Provide the closed-form analytic expression for the switching weight $w_{\\mathrm{c}}$ in terms of $x$ and $\\theta$ as your final answer. No numerical approximation is required.",
            "solution": "The problem provides the Bienenstock-Cooper-Munro (BCM) learning rule for a synaptic weight $w(t)$ and asks for the derivation and analysis of its dynamics under specific conditions. We begin by stating the given equations and constraints.\n\nThe postsynaptic response $y(t)$ is linearly proportional to the presynaptic input $x(t)$ and the synaptic weight $w(t)$:\n$$y(t) = w(t) x(t)$$\nThe synaptic weight is constrained to be non-negative:\n$$w(t) \\ge 0$$\nThe presynaptic input is assumed to be constant and positive:\n$$x(t) \\equiv x, \\quad \\text{with } x > 0$$\nUnder this condition, the postsynaptic response simplifies to:\n$$y(t) = w(t) x$$\nThe BCM learning rule is given by:\n$$\\dot{w}(t) = \\eta \\, x(t) \\, y(t) \\big(y(t) - \\theta\\big)$$\nwhere $\\dot{w}(t)$ is the time derivative of the weight, $\\eta > 0$ is the learning rate, and $\\theta > 0$ is a fixed activity threshold.\n\nThe first step is to derive the autonomous differential equation for $w(t)$. We substitute the expressions for $x(t)$ and $y(t)$ into the BCM rule:\n$$\\dot{w}(t) = \\eta \\, x \\, (w(t)x) \\big(w(t)x - \\theta\\big)$$\nBy rearranging terms, we obtain the autonomous ordinary differential equation (ODE) for $w(t)$, as the right-hand side depends only on $w(t)$ and not explicitly on time $t$:\n$$\\dot{w}(t) = \\eta x^2 w(t) \\big(w(t)x - \\theta\\big)$$\nLet's define the right-hand side function as $F(w) = \\eta x^2 w (wx - \\theta)$. The sign of $\\dot{w}(t) = F(w(t))$ determines whether the synapse undergoes potentiation ($\\dot{w} > 0$) or depression ($\\dot{w}  0$).\n\nNext, we perform a sign analysis of $F(w)$. The critical points (or fixed points) of the dynamics occur when $\\dot{w}(t) = 0$, which means $F(w) = 0$.\n$$\\eta x^2 w (wx - \\theta) = 0$$\nSince we are given that $\\eta > 0$ and $x > 0$, the term $\\eta x^2$ is strictly positive. Therefore, the equation is satisfied if either $w = 0$ or $(wx - \\theta) = 0$.\nThis gives two critical points:\n1. $w_1 = 0$\n2. $wx - \\theta = 0 \\implies w_2 = \\frac{\\theta}{x}$\n\nThe problem asks for the unique nonzero critical weight $w_{\\mathrm{c}}$ where plasticity switches from depression to potentiation. This corresponds to $w_2$.\n$$w_{\\mathrm{c}} = \\frac{\\theta}{x}$$\nSince $\\theta > 0$ and $x > 0$, $w_{\\mathrm{c}}$ is a positive weight, consistent with the constraint $w(t) \\ge 0$.\n\nNow we analyze the sign of $\\dot{w}$ in the regions defined by these critical points for $w > 0$. The sign of $\\dot{w} = \\eta x^2 w (wx - \\theta)$ is determined by the sign of the term $(wx - \\theta)$, because $\\eta x^2 > 0$ and we are considering $w > 0$.\n\nCase 1: Depression ($\\dot{w}  0$).\nThis occurs when $(wx - \\theta)  0$, which implies $wx  \\theta$. Solving for $w$ gives:\n$$w  \\frac{\\theta}{x} \\implies w  w_{\\mathrm{c}}$$\nSo, for any weight $w$ in the interval $0  w  w_{\\mathrm{c}}$, the synapse will weaken ($\\dot{w}  0$).\n\nCase 2: Potentiation ($\\dot{w} > 0$).\nThis occurs when $(wx - \\theta) > 0$, which implies $wx > \\theta$. Solving for $w$ gives:\n$$w > \\frac{\\theta}{x} \\implies w > w_{\\mathrm{c}}$$\nFor any weight $w > w_{\\mathrm{c}}$, the synapse will strengthen ($\\dot{w} > 0$).\n\nThis sign analysis justifies the interpretation of the BCM rule as a thresholded Hebbian rule. The post-synaptic activity is $y = wx$. The condition for switching from depression to potentiation, $w = w_{\\mathrm{c}} = \\frac{\\theta}{x}$, is equivalent to the postsynaptic activity reaching the threshold:\n$$y = w_{\\mathrm{c}}x = \\left(\\frac{\\theta}{x}\\right)x = \\theta$$\nThus, the BCM rule implements the following logic:\n- If postsynaptic activity is low ($y  \\theta$), the synapse depresses. This corresponds to Long-Term Depression (LTD).\n- If postsynaptic activity is high ($y > \\theta$), the synapse potentiates. This corresponds to Long-Term Potentiation (LTP).\n\nThe Hebbian character of the rule is evident from the multiplicative dependence on presynaptic ($x$) and postsynaptic ($y$) activity in the term $\\eta x y$. The factor $(y - \\theta)$ introduces the threshold that governs the direction of plasticity, making it a \"thresholded Hebbian rule\". The critical weight $w_{\\mathrm{c}}$ is the specific weight value at which, for a given input $x$, the postsynaptic response $y$ equals the threshold $\\theta$, marking the boundary between depression and potentiation.\n\nThe final answer required is the closed-form analytic expression for this switching weight $w_{\\mathrm{c}}$. As derived from the analysis of the critical points of the autonomous ODE, this expression is:\n$$w_{\\mathrm{c}} = \\frac{\\theta}{x}$$",
            "answer": "$$\\boxed{\\frac{\\theta}{x}}$$"
        },
        {
            "introduction": "A key innovation of the BCM rule is that the modification threshold, $\\theta$, is not fixed but dynamically adapts to the neuron's recent history of activation, providing a powerful homeostatic mechanism that stabilizes learning. In this practice problem, you will analyze the dynamics of this sliding threshold. This exercise will allow you to establish the fundamental relationship between its steady-state expected value and the statistical properties of the neuron's output firing rate, demonstrating how the neuron maintains stability .",
            "id": "4037170",
            "problem": "Consider a single-neuron model under the Bienenstock-Cooper-Munro (BCM) learning rule, where the sliding threshold evolves according to a first-order low-pass dynamics. Specifically, let the threshold variable $\\theta(t)$ obey the linear ordinary differential equation $ \\tau_{\\theta} \\dot{\\theta}(t) = y(t)^{2} - \\theta(t)$, where $\\tau_{\\theta}  0$ is a constant time scale and $y(t)$ is the neuronâ€™s output. Assume that $y(t)$ is a stationary ergodic process with finite second moment, mean $\\mu$, and variance $\\sigma^{2}$. Starting from the given dynamical equation and the definitions of stationarity and ergodicity, do the following:\n- Derive the steady-state solution for $\\theta(t)$ as a causal functional of $y(\\cdot)$, and use this to characterize the steady-state distribution of $\\theta(t)$ in terms of the distribution of the process $y(t)^{2}$.\n- Using only the fundamental properties of linear time-invariant filtering of stationary processes and the ergodic theorem, establish a relationship between the steady-state expectation $\\mathbb{E}[\\theta]$ and $\\mathbb{E}[y^{2}]$.\n- Express the final answer for the steady-state expectation $\\mathbb{E}[\\theta]$ solely in terms of $\\mu$ and $\\sigma^{2}$.\n\nYour final answer must be a single closed-form analytic expression for $\\mathbb{E}[\\theta]$. No numerical approximation or rounding is required.",
            "solution": "First, we derive the steady-state solution for the threshold variable $\\theta(t)$. The dynamics of $\\theta(t)$ are governed by the first-order linear ordinary differential equation (ODE):\n$$ \\tau_{\\theta} \\dot{\\theta}(t) = y(t)^{2} - \\theta(t) $$\nwhere $\\dot{\\theta}(t)$ denotes the derivative of $\\theta$ with respect to time $t$, and $\\tau_{\\theta}  0$ is a constant. We can rewrite this equation in the standard form:\n$$ \\frac{d\\theta}{dt} + \\frac{1}{\\tau_{\\theta}}\\theta(t) = \\frac{1}{\\tau_{\\theta}}y(t)^{2} $$\nThis is a non-homogeneous linear first-order ODE. We can solve it using an integrating factor, $I(t)$, defined as:\n$$ I(t) = \\exp\\left(\\int \\frac{1}{\\tau_{\\theta}} dt\\right) = \\exp\\left(\\frac{t}{\\tau_{\\theta}}\\right) $$\nMultiplying the ODE by $I(t)$ yields:\n$$ \\exp\\left(\\frac{t}{\\tau_{\\theta}}\\right)\\frac{d\\theta}{dt} + \\frac{1}{\\tau_{\\theta}}\\exp\\left(\\frac{t}{\\tau_{\\theta}}\\right)\\theta(t) = \\frac{1}{\\tau_{\\theta}}\\exp\\left(\\frac{t}{\\tau_{\\theta}}\\right)y(t)^{2} $$\nThe left-hand side is the derivative of the product $\\theta(t)I(t)$:\n$$ \\frac{d}{dt}\\left[\\theta(t)\\exp\\left(\\frac{t}{\\tau_{\\theta}}\\right)\\right] = \\frac{1}{\\tau_{\\theta}}\\exp\\left(\\frac{t}{\\tau_{\\theta}}\\right)y(t)^{2} $$\nIntegrating both sides from a past time $t_0$ to the current time $t$:\n$$ \\theta(t)\\exp\\left(\\frac{t}{\\tau_{\\theta}}\\right) - \\theta(t_0)\\exp\\left(\\frac{t_0}{\\tau_{\\theta}}\\right) = \\frac{1}{\\tau_{\\theta}}\\int_{t_0}^{t} \\exp\\left(\\frac{s}{\\tau_{\\theta}}\\right)y(s)^{2} ds $$\nSolving for $\\theta(t)$:\n$$ \\theta(t) = \\theta(t_0)\\exp\\left(-\\frac{t-t_0}{\\tau_{\\theta}}\\right) + \\frac{1}{\\tau_{\\theta}}\\int_{t_0}^{t} \\exp\\left(-\\frac{t-s}{\\tau_{\\theta}}\\right)y(s)^{2} ds $$\nThe steady-state solution is obtained by considering the system's evolution from the distant past, i.e., by taking the limit $t_0 \\to -\\infty$. Since $\\tau_{\\theta}  0$, the first term containing the initial condition $\\theta(t_0)$ decays to zero, assuming $\\theta(t_0)$ is finite. The steady-state solution $\\theta_{ss}(t)$ is therefore:\n$$ \\theta_{ss}(t) = \\frac{1}{\\tau_{\\theta}}\\int_{-\\infty}^{t} \\exp\\left(-\\frac{t-s}{\\tau_{\\theta}}\\right)y(s)^{2} ds $$\nThis expression represents $\\theta(t)$ as a causal functional of the process $y(\\cdot)$. Specifically, it shows that $\\theta(t)$ is the output of a linear time-invariant (LTI) filter with an impulse response $h(\\tau) = \\frac{1}{\\tau_{\\theta}}\\exp(-\\tau/\\tau_{\\theta})$ for $\\tau \\ge 0$ (and $h(\\tau)=0$ for $\\tau  0$), driven by the input signal $y(t)^2$. The distribution of $\\theta(t)$ is thus the distribution of this weighted moving average of the past values of the process $y(t)^2$.\n\nSecond, we establish the relationship between the steady-state expectation $\\mathbb{E}[\\theta]$ and $\\mathbb{E}[y^{2}]$. Since $y(t)$ is a stationary process, the process $y(t)^2$ is also stationary. This means the expectation $\\mathbb{E}[y(s)^2]$ is constant for all $s$. Let us denote this constant value by $\\mathbb{E}[y^2]$. Taking the expectation of the steady-state solution $\\theta_{ss}(t)$:\n$$ \\mathbb{E}[\\theta_{ss}(t)] = \\mathbb{E}\\left[ \\frac{1}{\\tau_{\\theta}}\\int_{-\\infty}^{t} \\exp\\left(-\\frac{t-s}{\\tau_{\\theta}}\\right)y(s)^{2} ds \\right] $$\nBy the linearity of expectation and Fubini's theorem, we can interchange the expectation and integral operators:\n$$ \\mathbb{E}[\\theta_{ss}(t)] = \\frac{1}{\\tau_{\\theta}}\\int_{-\\infty}^{t} \\exp\\left(-\\frac{t-s}{\\tau_{\\theta}}\\right)\\mathbb{E}[y(s)^{2}] ds $$\nSubstituting the constant expectation $\\mathbb{E}[y^2]$:\n$$ \\mathbb{E}[\\theta_{ss}(t)] = \\mathbb{E}[y^2] \\left( \\frac{1}{\\tau_{\\theta}}\\int_{-\\infty}^{t} \\exp\\left(-\\frac{t-s}{\\tau_{\\theta}}\\right) ds \\right) $$\nLet's evaluate the integral. Let $u = t-s$, so $du = -ds$. The limits of integration become $u=\\infty$ for $s=-\\infty$ and $u=0$ for $s=t$.\n$$ \\frac{1}{\\tau_{\\theta}}\\int_{\\infty}^{0} \\exp\\left(-\\frac{u}{\\tau_{\\theta}}\\right) (-du) = \\frac{1}{\\tau_{\\theta}}\\int_{0}^{\\infty} \\exp\\left(-\\frac{u}{\\tau_{\\theta}}\\right) du = \\left[ -\\exp\\left(-\\frac{u}{\\tau_{\\theta}}\\right) \\right]_{0}^{\\infty} = -0 - (-1) = 1 $$\nThe integral of the filter's impulse response is $1$. In steady-state, the statistics of $\\theta(t)$ are time-invariant, so we denote its expectation as $\\mathbb{E}[\\theta]$. Thus, we find:\n$$ \\mathbb{E}[\\theta] = \\mathbb{E}[y^2] \\cdot 1 = \\mathbb{E}[y^2] $$\nAlternatively, as requested, we can use the ergodic theorem. Taking the time average of the original ODE over an interval $[0, T]$:\n$$ \\frac{1}{T}\\int_{0}^{T} \\tau_{\\theta} \\dot{\\theta}(t) dt = \\frac{1}{T}\\int_{0}^{T} y(t)^2 dt - \\frac{1}{T}\\int_{0}^{T} \\theta(t) dt $$\n$$ \\frac{\\tau_{\\theta}}{T}(\\theta(T) - \\theta(0)) = \\langle y^2 \\rangle_T - \\langle \\theta \\rangle_T $$\nwhere $\\langle \\cdot \\rangle_T$ denotes the time average. As $T \\to \\infty$, for a stable system where $\\theta(t)$ is bounded, the term $\\frac{\\theta(T) - \\theta(0)}{T} \\to 0$. This gives $\\lim_{T \\to \\infty} \\langle \\theta \\rangle_T = \\lim_{T \\to \\infty} \\langle y^2 \\rangle_T$. Since $y(t)$ is ergodic, the output $\\theta(t)$ of the stable LTI filter is also an ergodic process. By the ergodic theorem, time averages converge to ensemble averages, so we conclude $\\mathbb{E}[\\theta] = \\mathbb{E}[y^2]$.\n\nThird, we express $\\mathbb{E}[\\theta]$ in terms of the given mean $\\mu$ and variance $\\sigma^2$ of the process $y(t)$. The variance is defined as:\n$$ \\sigma^2 = \\mathbb{E}[(y(t) - \\mathbb{E}[y(t)])^2] $$\nGiven $\\mathbb{E}[y(t)] = \\mu$:\n$$ \\sigma^2 = \\mathbb{E}[(y - \\mu)^2] = \\mathbb{E}[y^2 - 2\\mu y + \\mu^2] $$\nBy linearity of expectation:\n$$ \\sigma^2 = \\mathbb{E}[y^2] - 2\\mu\\mathbb{E}[y] + \\mathbb{E}[\\mu^2] = \\mathbb{E}[y^2] - 2\\mu(\\mu) + \\mu^2 = \\mathbb{E}[y^2] - 2\\mu^2 + \\mu^2 $$\n$$ \\sigma^2 = \\mathbb{E}[y^2] - \\mu^2 $$\nThis relation provides an expression for the second moment, $\\mathbb{E}[y^2]$:\n$$ \\mathbb{E}[y^2] = \\mu^2 + \\sigma^2 $$\nFinally, substituting this into our result for the steady-state expectation of the threshold:\n$$ \\mathbb{E}[\\theta] = \\mu^2 + \\sigma^2 $$\nThis is the final expression for the steady-state expectation of the threshold variable, expressed solely in terms of the mean and variance of the neuron's output.",
            "answer": "$$\\boxed{\\mu^{2} + \\sigma^{2}}$$"
        },
        {
            "introduction": "Having established the principles of thresholded plasticity and homeostatic regulation, we now apply the full BCM rule to see its functional consequence: the development of neuronal selectivity. This problem presents a neuron with a choice between two different input patterns. By calculating the expected weight changes, you will predict which pattern the neuron learns to prefer, thereby demonstrating how BCM dynamics enable a neuron to become a selective feature detector in its environment .",
            "id": "4037209",
            "problem": "Consider a single linear neuron with synaptic weight vector $w \\in \\mathbb{R}^{2}$ receiving presynaptic input patterns $x \\in \\mathbb{R}^{2}$. The output of the neuron is $y = w^{\\top} x$. Two distinct, fixed input patterns $x^{(1)}$ and $x^{(2)}$ are presented with equal frequency, so that the input $x$ is drawn independently from $\\{x^{(1)}, x^{(2)}\\}$ with probability $1/2$ each. Assume synaptic plasticity follows the Bienenstock-Cooper-Munro (BCM) learning rule: synaptic change is proportional to presynaptic activity multiplied by a modification function that depends on postsynaptic activity relative to a sliding threshold, and the sliding threshold on a slower timescale equilibrates to the long-term average of the squared postsynaptic activity under the current distribution of inputs. The learning rate constant is $1$.\n\nLet the initial synaptic weight be $w = \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{1}{2} \\end{pmatrix}$, and the two input patterns be\n$$\nx^{(1)} = \\begin{pmatrix} 2 \\\\ \\frac{2}{5} \\end{pmatrix}, \\qquad x^{(2)} = \\begin{pmatrix} \\frac{1}{10} \\\\ \\frac{1}{10} \\end{pmatrix}.\n$$\nTreat the threshold as equilibrated to the current expected squared output computed with this $w$ and the given input distribution. Using the BCM framework and the equal-frequency input distribution, compute:\n\n1. The expected instantaneous weight change vector $\\mathbb{E}\\!\\left[\\frac{dw}{dt}\\right]$ at the stated $w$ and inputs.\n2. Which of the two patterns the neuron will become selective for under BCM dynamics, encoded as $k = 1$ if selective for $x^{(1)}$ and $k = 2$ if selective for $x^{(2)}$.\n\nReport your final answer as a single row matrix containing the two components of $\\mathbb{E}\\!\\left[\\frac{dw}{dt}\\right]$ followed by $k$, in the form $\\begin{pmatrix} \\Delta w_{1}  \\Delta w_{2}  k \\end{pmatrix}$. Provide exact values; do not round.",
            "solution": "The BCM learning rule is given by $\\frac{dw}{dt} = \\eta y (y - \\theta_M) x$, with learning rate $\\eta=1$. The sliding threshold $\\theta_M$ is equilibrated to the expected squared postsynaptic activity, $\\theta_M = \\mathbb{E}[y^2]$. The neuron's output is $y = w^{\\top} x$.\n\nFirst, we compute the postsynaptic activity $y$ for each input pattern using the initial weight vector $w = \\begin{pmatrix} 1/2 \\\\ 1/2 \\end{pmatrix}$:\n$$\ny^{(1)} = w^{\\top} x^{(1)} = \\begin{pmatrix} \\frac{1}{2}  \\frac{1}{2} \\end{pmatrix} \\begin{pmatrix} 2 \\\\ \\frac{2}{5} \\end{pmatrix} = 1 + \\frac{1}{5} = \\frac{6}{5}\n$$\n$$\ny^{(2)} = w^{\\top} x^{(2)} = \\begin{pmatrix} \\frac{1}{2}  \\frac{1}{2} \\end{pmatrix} \\begin{pmatrix} \\frac{1}{10} \\\\ \\frac{1}{10} \\end{pmatrix} = \\frac{1}{20} + \\frac{1}{20} = \\frac{1}{10}\n$$\n\nNext, we compute the threshold $\\theta_M$. Since the patterns are presented with equal probability ($P=1/2$), the expectation is:\n$$\n\\theta_M = \\mathbb{E}[y^2] = \\frac{1}{2} (y^{(1)})^2 + \\frac{1}{2} (y^{(2)})^2 = \\frac{1}{2} \\left(\\frac{6}{5}\\right)^2 + \\frac{1}{2} \\left(\\frac{1}{10}\\right)^2 = \\frac{1}{2} \\left(\\frac{36}{25}\\right) + \\frac{1}{2} \\left(\\frac{1}{100}\\right) = \\frac{18}{25} + \\frac{1}{200}\n$$\n$$\n\\theta_M = \\frac{144}{200} + \\frac{1}{200} = \\frac{145}{200} = \\frac{29}{40}\n$$\n\nNow, we can compute the expected instantaneous weight change vector $\\mathbb{E}\\left[\\frac{dw}{dt}\\right]$. This is the average of the instantaneous changes for each pattern:\n$$\n\\mathbb{E}\\left[\\frac{dw}{dt}\\right] = \\frac{1}{2} \\left( y^{(1)}(y^{(1)} - \\theta_M) x^{(1)} \\right) + \\frac{1}{2} \\left( y^{(2)}(y^{(2)} - \\theta_M) x^{(2)} \\right)\n$$\nWe compute the modification terms $(y - \\theta_M)$:\n$$\ny^{(1)} - \\theta_M = \\frac{6}{5} - \\frac{29}{40} = \\frac{48 - 29}{40} = \\frac{19}{40}\n$$\n$$\ny^{(2)} - \\theta_M = \\frac{1}{10} - \\frac{29}{40} = \\frac{4 - 29}{40} = -\\frac{25}{40} = -\\frac{5}{8}\n$$\nSubstituting these back into the expectation:\n$$\n\\mathbb{E}\\left[\\frac{dw}{dt}\\right] = \\frac{1}{2} \\left( \\frac{6}{5} \\cdot \\frac{19}{40} \\right) x^{(1)} + \\frac{1}{2} \\left( \\frac{1}{10} \\cdot \\left(-\\frac{5}{8}\\right) \\right) x^{(2)} = \\left( \\frac{57}{200} \\right) x^{(1)} - \\left( \\frac{1}{32} \\right) x^{(2)}\n$$\nNow we substitute the vectors for $x^{(1)}$ and $x^{(2)}$:\n$$\n\\mathbb{E}\\left[\\frac{dw}{dt}\\right] = \\frac{57}{200} \\begin{pmatrix} 2 \\\\ \\frac{2}{5} \\end{pmatrix} - \\frac{1}{32} \\begin{pmatrix} \\frac{1}{10} \\\\ \\frac{1}{10} \\end{pmatrix} = \\begin{pmatrix} \\frac{114}{200} \\\\ \\frac{114}{1000} \\end{pmatrix} - \\begin{pmatrix} \\frac{1}{320} \\\\ \\frac{1}{320} \\end{pmatrix} = \\begin{pmatrix} \\frac{57}{100} \\\\ \\frac{57}{500} \\end{pmatrix} - \\begin{pmatrix} \\frac{1}{320} \\\\ \\frac{1}{320} \\end{pmatrix}\n$$\nWe compute each component of the resulting vector:\n$$\n\\Delta w_1 = \\frac{57}{100} - \\frac{1}{320} = \\frac{57 \\cdot 16 - 5}{1600} = \\frac{912 - 5}{1600} = \\frac{907}{1600}\n$$\n$$\n\\Delta w_2 = \\frac{57}{500} - \\frac{1}{320} = \\frac{57 \\cdot 16 - 25}{8000} = \\frac{912 - 25}{8000} = \\frac{887}{8000}\n$$\nSo, the expected instantaneous weight change is $\\mathbb{E}\\left[\\frac{dw}{dt}\\right] = \\begin{pmatrix} \\frac{907}{1600} \\\\ \\frac{887}{8000} \\end{pmatrix}$.\n\nTo determine neuronal selectivity, we examine the sign of the modification term for each pattern.\n- For pattern $x^{(1)}$: $y^{(1)} - \\theta_M = \\frac{19}{40} > 0$. This leads to Long-Term Potentiation (LTP), increasing the neuron's response to $x^{(1)}$.\n- For pattern $x^{(2)}$: $y^{(2)} - \\theta_M = -\\frac{5}{8}  0$. This leads to Long-Term Depression (LTD), decreasing the neuron's response to $x^{(2)}$.\nSince the dynamics reinforce the response to $x^{(1)}$ and suppress the response to $x^{(2)}$, the neuron will become selective for pattern $x^{(1)}$. This corresponds to $k=1$.\n\nThe final answer is a row matrix containing the two components of the expected weight change vector followed by the selectivity indicator $k$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{907}{1600}  \\frac{887}{8000}  1 \\end{pmatrix}}\n$$"
        }
    ]
}