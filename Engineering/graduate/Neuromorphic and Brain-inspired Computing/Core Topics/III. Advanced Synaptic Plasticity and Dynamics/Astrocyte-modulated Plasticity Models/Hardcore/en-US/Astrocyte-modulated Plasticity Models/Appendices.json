{
    "hands_on_practices": [
        {
            "introduction": "To understand how astrocytes shape neural circuits, we must first quantify their most direct influence: the release of gliotransmitters. This practice challenges you to model the effect of a transient glutamate pulse from an astrocyte on the postsynaptic current, mediated by extrasynaptic NMDA receptors. By integrating the current over time, you will calculate the total charge transfer, providing a concrete measure of the astrocytic signal's impact on the neuron . This exercise is fundamental for translating a biophysical event into a quantifiable electrical signal.",
            "id": "4035731",
            "problem": "Astrocytes modulate synaptic and extrasynaptic signaling by releasing glutamate into the extracellular space. Consider a neuromorphic model of astrocyte-modulated plasticity in which activation of extrasynaptic N-Methyl-D-Aspartate (NMDA) receptors contributes to postsynaptic current as a saturating function of the instantaneous extracellular glutamate concentration. Assume a voltage-clamped postsynaptic membrane so that the NMDA conductance is a fixed function $g_{N}(V)$ of the clamp voltage $V$, and assume quasi-steady-state receptor occupancy relative to glutamate clearance. Let the NMDA current be modeled by the standard Michaelis–Menten form\n$$\nI_{NMDA}(t) \\;=\\; g_{N}(V)\\,\\frac{[G_{e}](t)}{K_{N} + [G_{e}](t)},\n$$\nwhere $[G_{e}](t)$ is the extracellular glutamate concentration, $K_{N} > 0$ is the effective half-saturation constant of the extrasynaptic NMDA receptor binding under the given co-agonist and magnesium conditions, and $g_{N}(V) > 0$ is constant in time. Prior to astrocyte release, the baseline concentration is $[G_{e}](t) = G_{0}$ with $G_{0} \\ge 0$. At time $t=0$, the astrocyte releases a transient glutamate pulse into the extracellular space that adds to the baseline and decays monoexponentially due to uptake and diffusion, so that for $t \\ge 0$,\n$$\n[G_{e}](t) \\;=\\; G_{0} + A\\,\\exp\\!\\left(-\\frac{t}{\\tau_{g}}\\right),\n$$\nwith $A > 0$ the pulse amplitude and $\\tau_{g} > 0$ the decay time constant.\n\nStarting from mass-action receptor occupancy and assuming the above dynamics and voltage-clamp conditions, derive a closed-form analytic expression for the total time-integrated increase in NMDA current relative to baseline induced by the pulse,\n$$\n\\Delta Q \\;=\\; \\int_{0}^{\\infty}\\big(I_{NMDA}(t) - I_{NMDA}^{\\text{base}}\\big)\\,dt,\n$$\nwhere\n$$\nI_{NMDA}^{\\text{base}} \\;=\\; g_{N}(V)\\,\\frac{G_{0}}{K_{N} + G_{0}}.\n$$\nExpress your final answer as a single closed-form analytic expression in terms of $g_{N}(V)$, $K_{N}$, $G_{0}$, $A$, and $\\tau_{g}$. No numerical evaluation is required, and no units should be included in the final boxed answer.",
            "solution": "The problem as stated is subjected to validation against the established criteria.\n\n### Step 1: Extract Givens\nThe following quantities and relations are provided verbatim:\n-   NMDA current model: $I_{NMDA}(t) = g_{N}(V)\\,\\frac{[G_{e}](t)}{K_{N} + [G_{e}](t)}$\n-   Extracellular glutamate concentration for $t \\ge 0$: $[G_{e}](t) = G_{0} + A\\,\\exp(-\\frac{t}{\\tau_{g}})$\n-   Baseline NMDA current: $I_{NMDA}^{\\text{base}} = g_{N}(V)\\,\\frac{G_{0}}{K_{N} + G_{0}}$\n-   Total time-integrated current increase to be derived: $\\Delta Q = \\int_{0}^{\\infty}\\big(I_{NMDA}(t) - I_{NMDA}^{\\text{base}}\\big)\\,dt$\n-   Parameter constraints: $K_{N} > 0$, $g_{N}(V) > 0$, $G_{0} \\ge 0$, $A > 0$, $\\tau_{g} > 0$.\n-   Contextual conditions: Voltage-clamped postsynaptic membrane, quasi-steady-state receptor occupancy.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed for validity:\n-   **Scientifically Grounded**: The problem utilizes a standard Michaelis–Menten formulation for receptor kinetics and a monoexponential decay for a neurotransmitter pulse. These are common and well-accepted simplifications in computational neuroscience and biophysical modeling. The setup is scientifically plausible within this modeling context.\n-   **Well-Posed**: The problem is mathematically well-defined. It requests the evaluation of a definite integral of a function constructed from well-behaved components. The integrand approaches zero as $t \\to \\infty$, ensuring the integral converges. All necessary parameters are provided, and a unique analytical solution is expected to exist.\n-   **Objective**: The problem is stated using precise mathematical definitions and objective language, free of ambiguity or subjective claims.\n\nThe problem does not exhibit any of the enumerated invalidity flaws. It is scientifically sound, formally stated, self-contained, and solvable.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A solution will be derived.\n\nThe objective is to compute the total-time integrated increase in NMDA current, $\\Delta Q$, relative to its baseline value. The definition is given as:\n$$\n\\Delta Q = \\int_{0}^{\\infty} \\left( I_{NMDA}(t) - I_{NMDA}^{\\text{base}} \\right) dt\n$$\nFirst, we substitute the expressions for $I_{NMDA}(t)$ and $I_{NMDA}^{\\text{base}}$. The term $g_{N}(V)$ is a common factor and can be moved outside the integral.\n$$\n\\Delta Q = g_{N}(V) \\int_{0}^{\\infty} \\left( \\frac{[G_{e}](t)}{K_{N} + [G_{e}](t)} - \\frac{G_{0}}{K_{N} + G_{0}} \\right) dt\n$$\nNext, we substitute the expression for the time-dependent glutamate concentration, $[G_{e}](t) = G_{0} + A\\,\\exp(-t/\\tau_{g})$, into the integrand for $t \\ge 0$.\n$$\n\\Delta Q = g_{N}(V) \\int_{0}^{\\infty} \\left( \\frac{G_{0} + A\\,\\exp(-t/\\tau_{g})}{K_{N} + G_{0} + A\\,\\exp(-t/\\tau_{g})} - \\frac{G_{0}}{K_{N} + G_{0}} \\right) dt\n$$\nTo simplify the expression inside the integral, we combine the two fractions by finding a common denominator, which is $(K_{N} + G_{0})(K_{N} + G_{0} + A\\,\\exp(-t/\\tau_{g}))$.\nLet's denote $x(t) = A\\,\\exp(-t/\\tau_{g})$ for brevity. The integrand becomes:\n$$\n\\frac{G_{0} + x(t)}{K_{N} + G_{0} + x(t)} - \\frac{G_{0}}{K_{N} + G_{0}} = \\frac{(G_{0} + x(t))(K_{N} + G_{0}) - G_{0}(K_{N} + G_{0} + x(t))}{(K_{N} + G_{0})(K_{N} + G_{0} + x(t))}\n$$\nExpanding the numerator:\n$$\n(G_{0}K_{N} + G_{0}^2 + x(t)K_{N} + x(t)G_{0}) - (G_{0}K_{N} + G_{0}^2 + G_{0}x(t))\n$$\nSimplifying the numerator yields:\n$$\nG_{0}K_{N} + G_{0}^2 + K_{N}x(t) + G_{0}x(t) - G_{0}K_{N} - G_{0}^2 - G_{0}x(t) = K_{N}x(t)\n$$\nSubstituting $x(t) = A\\,\\exp(-t/\\tau_{g})$ back, the simplified integrand is:\n$$\n\\frac{K_{N}A\\,\\exp(-t/\\tau_{g})}{(K_{N} + G_{0})(K_{N} + G_{0} + A\\,\\exp(-t/\\tau_{g}))}\n$$\nThe integral for $\\Delta Q$ is now:\n$$\n\\Delta Q = g_{N}(V) \\int_{0}^{\\infty} \\frac{K_{N}A\\,\\exp(-t/\\tau_{g})}{(K_{N} + G_{0})(K_{N} + G_{0} + A\\,\\exp(-t/\\tau_{g}))} dt\n$$\nWe can factor out the terms that are not dependent on the integration variable $t$:\n$$\n\\Delta Q = \\frac{g_{N}(V) K_{N} A}{K_{N} + G_{0}} \\int_{0}^{\\infty} \\frac{\\exp(-t/\\tau_{g})}{K_{N} + G_{0} + A\\,\\exp(-t/\\tau_{g})} dt\n$$\nTo solve the remaining integral, we perform a change of variables. Let $u = K_{N} + G_{0} + A\\,\\exp(-t/\\tau_{g})$.\nThe differential $du$ is then:\n$$\ndu = A \\left(-\\frac{1}{\\tau_{g}}\\right) \\exp(-t/\\tau_{g}) dt\n$$\nThis can be rearranged to express the numerator of the integrand:\n$$\n\\exp(-t/\\tau_{g}) dt = -\\frac{\\tau_{g}}{A} du\n$$\nWe must also transform the limits of integration from $t$ to $u$:\n-   At the lower limit, $t=0$, we have $u(0) = K_{N} + G_{0} + A\\,\\exp(0) = K_{N} + G_{0} + A$.\n-   At the upper limit, $t \\to \\infty$, we have $u(\\infty) = K_{N} + G_{0} + A\\,\\lim_{t \\to \\infty}\\exp(-t/\\tau_{g}) = K_{N} + G_{0}$.\nSubstituting these into the integral:\n$$\n\\int_{0}^{\\infty} \\frac{\\exp(-t/\\tau_{g})}{K_{N} + G_{0} + A\\,\\exp(-t/\\tau_{g})} dt = \\int_{K_{N} + G_{0} + A}^{K_{N} + G_{0}} \\frac{1}{u} \\left(-\\frac{\\tau_{g}}{A}\\right) du\n$$\nWe can reverse the limits of integration, which removes the negative sign:\n$$\n= \\frac{\\tau_{g}}{A} \\int_{K_{N} + G_{0}}^{K_{N} + G_{0} + A} \\frac{1}{u} du\n$$\nThe integral of $1/u$ is $\\ln|u|$. Since all parameters ($K_{N}, A, \\tau_{g}$) are positive and $G_{0} \\ge 0$, the argument $u$ is always positive over the integration interval. Thus, the absolute value is not necessary.\n$$\n= \\frac{\\tau_{g}}{A} \\left[ \\ln(u) \\right]_{K_{N} + G_{0}}^{K_{N} + G_{0} + A}\n$$\nEvaluating the definite integral:\n$$\n= \\frac{\\tau_{g}}{A} \\left( \\ln(K_{N} + G_{0} + A) - \\ln(K_{N} + G_{0}) \\right)\n$$\nUsing the property of logarithms $\\ln(a) - \\ln(b) = \\ln(a/b)$, we get:\n$$\n= \\frac{\\tau_{g}}{A} \\ln\\left(\\frac{K_{N} + G_{0} + A}{K_{N} + G_{0}}\\right)\n$$\nFinally, we substitute this result back into the expression for $\\Delta Q$:\n$$\n\\Delta Q = \\frac{g_{N}(V) K_{N} A}{K_{N} + G_{0}} \\left[ \\frac{\\tau_{g}}{A} \\ln\\left(\\frac{K_{N} + G_{0} + A}{K_{N} + G_{0}}\\right) \\right]\n$$\nThe term $A$ in the prefactor cancels with the $A$ in the denominator of the integrated expression:\n$$\n\\Delta Q = \\frac{g_{N}(V) K_{N} \\tau_{g}}{K_{N} + G_{0}} \\ln\\left(\\frac{K_{N} + G_{0} + A}{K_{N} + G_{0}}\\right)\n$$\nThis is the final closed-form analytic expression for the total time-integrated increase in NMDA current.",
            "answer": "$$\n\\boxed{\\frac{g_{N}(V) K_{N} \\tau_{g}}{K_{N} + G_{0}} \\ln\\left(\\frac{K_{N} + G_{0} + A}{K_{N} + G_{0}}\\right)}\n$$"
        },
        {
            "introduction": "Gliotransmitter release is not a deterministic clockwork but a stochastic process, contributing to the variability observed in synaptic function. This practice moves from a single pulse to a continuous stream of events, modeling the release mechanism as a simple two-state Markov process . You will apply principles from renewal theory to derive the mean rate and variance of these events, and then directly link these statistics to fluctuations in synaptic weight. This exercise provides a powerful framework for understanding how microscopic, random events at the tripartite synapse scale up to influence a key macroscopic property like synaptic strength.",
            "id": "4035744",
            "problem": "Consider a gliotransmitter release mechanism from an astrocyte modeled as a $2$-state continuous-time Markov process with states $U$ (unprimed) and $P$ (primed). Transitions are as follows: $U \\to P$ occurs with rate $\\alpha > 0$ (priming), and $P \\to U$ occurs with rate $\\beta > 0$ (release), where each $P \\to U$ transition emits a single gliotransmitter event. Assume priming and release waiting times are independent and exponentially distributed. Let $N_T$ denote the number of gliotransmitter events emitted in a time window of duration $T$, with the process initialized in its stationary regime.\n\nStarting from first principles of renewal theory and exponential waiting-time properties, derive:\n- the long-time mean event rate $\\bar{r} = \\lim_{T \\to \\infty} \\mathbb{E}[N_T]/T$, and\n- the long-time event-count variance rate $\\sigma_N^2 = \\lim_{T \\to \\infty} \\operatorname{Var}(N_T)/T$.\n\nThen assume that the synaptic weight fluctuation $\\Delta w_T$ over the same window is related to the event count by a linear response of gain $\\gamma > 0$, namely $\\Delta w_T = \\gamma \\left(N_T - \\mathbb{E}[N_T]\\right)$. Using your derived statistics of $N_T$, propagate them to obtain the long-time variance rate of synaptic weight fluctuations,\n$$\n\\Sigma_w \\equiv \\lim_{T \\to \\infty} \\frac{\\operatorname{Var}(\\Delta w_T)}{T}.\n$$\n\nProvide your final answer as a single closed-form analytic expression for $\\Sigma_w$ in terms of $\\alpha$, $\\beta$, and $\\gamma$. Since this is a symbolic expression, no rounding is required. Express the variance rate in units of squared-weight per second, but do not include units in your final boxed answer.",
            "solution": "The problem is valid. It is a well-posed problem in the theory of stochastic processes, specifically continuous-time Markov chains and renewal theory, with a direct application in computational neuroscience. The premises are scientifically sound and all necessary information is provided.\n\nThe problem asks for the long-time variance rate of synaptic weight fluctuations, $\\Sigma_w$, which depends on the statistical properties of a gliotransmitter release process. This process is modeled as a $2$-state continuous-time Markov chain. The emission of a gliotransmitter is an event that marks a renewal point. Thus, the sequence of emissions forms a renewal process. We will solve this by deriving the properties of the inter-event intervals (IEIs) and then using standard results from renewal theory.\n\nFirst, we characterize the inter-event interval, $I$. An event is defined as the transition from state $P$ (primed) to state $U$ (unprimed). A full cycle between two consecutive events consists of two stages:\n1.  The system is in state $U$ and waits for a transition to state $P$. The waiting time, let's call it $\\tau_U$, is exponentially distributed with rate $\\alpha$.\n2.  The system is then in state $P$ and waits for a transition back to state $U$, which triggers the event. The waiting time for this, let's call it $\\tau_P$, is exponentially distributed with rate $\\beta$.\n\nThe total time between two consecutive events, the IEI, is $I = \\tau_U + \\tau_P$. Since the waiting times are stated to be independent, we can find the mean and variance of $I$.\n\nThe mean and variance of an exponentially distributed random variable with rate $\\lambda$ are $\\mathbb{E}[\\tau] = 1/\\lambda$ and $\\operatorname{Var}(\\tau) = 1/\\lambda^2$, respectively.\nThe mean of the inter-event interval, $\\mu$, is:\n$$\n\\mu = \\mathbb{E}[I] = \\mathbb{E}[\\tau_U + \\tau_P] = \\mathbb{E}[\\tau_U] + \\mathbb{E}[\\tau_P] = \\frac{1}{\\alpha} + \\frac{1}{\\beta} = \\frac{\\alpha + \\beta}{\\alpha\\beta}\n$$\nThe variance of the inter-event interval, $\\sigma_I^2$, due to the independence of $\\tau_U$ and $\\tau_P$, is:\n$$\n\\sigma_I^2 = \\operatorname{Var}(I) = \\operatorname{Var}(\\tau_U + \\tau_P) = \\operatorname{Var}(\\tau_U) + \\operatorname{Var}(\\tau_P) = \\frac{1}{\\alpha^2} + \\frac{1}{\\beta^2} = \\frac{\\beta^2 + \\alpha^2}{\\alpha^2\\beta^2}\n$$\nNow, we use results from renewal theory to find the long-time statistics of the event count $N_T$. The problem states the process is initialized in its stationary regime, which justifies the use of asymptotic theorems.\n\nThe long-time mean event rate, $\\bar{r}$, is given by the elementary renewal theorem as the reciprocal of the mean inter-event interval:\n$$\n\\bar{r} = \\lim_{T \\to \\infty} \\frac{\\mathbb{E}[N_T]}{T} = \\frac{1}{\\mu} = \\frac{\\alpha\\beta}{\\alpha + \\beta}\n$$\nThe long-time event-count variance rate, $\\sigma_N^2$, is a standard result from the central limit theorem for renewal processes:\n$$\n\\sigma_N^2 = \\lim_{T \\to \\infty} \\frac{\\operatorname{Var}(N_T)}{T} = \\frac{\\sigma_I^2}{\\mu^3}\n$$\nSubstituting the expressions we derived for $\\mu$ and $\\sigma_I^2$:\n$$\n\\sigma_N^2 = \\frac{\\frac{\\alpha^2 + \\beta^2}{\\alpha^2\\beta^2}}{\\left(\\frac{\\alpha + \\beta}{\\alpha\\beta}\\right)^3} = \\frac{\\alpha^2 + \\beta^2}{\\alpha^2\\beta^2} \\cdot \\frac{(\\alpha\\beta)^3}{(\\alpha + \\beta)^3} = \\frac{\\alpha\\beta(\\alpha^2 + \\beta^2)}{(\\alpha + \\beta)^3}\n$$\nThis gives the first two requested quantities. Now we proceed to the final step, which is to find the long-time variance rate of the synaptic weight fluctuations, $\\Sigma_w$.\n\nThe synaptic weight fluctuation $\\Delta w_T$ is given by the linear response model:\n$$\n\\Delta w_T = \\gamma \\left(N_T - \\mathbb{E}[N_T]\\right)\n$$\nwhere $\\gamma > 0$ is the gain. We need to find the variance of $\\Delta w_T$. Using the properties of variance, $\\operatorname{Var}(aX+b) = a^2\\operatorname{Var}(X)$, where $a$ and $b$ are constants:\n$$\n\\operatorname{Var}(\\Delta w_T) = \\operatorname{Var}\\left(\\gamma (N_T - \\mathbb{E}[N_T])\\right) = \\gamma^2 \\operatorname{Var}(N_T - \\mathbb{E}[N_T])\n$$\nSince $\\mathbb{E}[N_T]$ is a constant (for a given $T$), $\\operatorname{Var}(N_T - \\mathbb{E}[N_T]) = \\operatorname{Var}(N_T)$. Thus:\n$$\n\\operatorname{Var}(\\Delta w_T) = \\gamma^2 \\operatorname{Var}(N_T)\n$$\nThe quantity to be derived is the long-time variance rate of the synaptic weight, $\\Sigma_w$:\n$$\n\\Sigma_w = \\lim_{T \\to \\infty} \\frac{\\operatorname{Var}(\\Delta w_T)}{T} = \\lim_{T \\to \\infty} \\frac{\\gamma^2 \\operatorname{Var}(N_T)}{T}\n$$\nWe can pull the constant $\\gamma^2$ out of the limit:\n$$\n\\Sigma_w = \\gamma^2 \\left( \\lim_{T \\to \\infty} \\frac{\\operatorname{Var}(N_T)}{T} \\right)\n$$\nThe term in the parenthesis is precisely the event-count variance rate, $\\sigma_N^2$, which we have already calculated. Therefore:\n$$\n\\Sigma_w = \\gamma^2 \\sigma_N^2\n$$\nSubstituting our expression for $\\sigma_N^2$:\n$$\n\\Sigma_w = \\gamma^2 \\frac{\\alpha\\beta(\\alpha^2 + \\beta^2)}{(\\alpha + \\beta)^3}\n$$\nThis is the final closed-form expression for the long-time variance rate of synaptic weight fluctuations in terms of the given parameters $\\alpha$, $\\beta$, and $\\gamma$.",
            "answer": "$$\n\\boxed{\\gamma^2 \\frac{\\alpha\\beta(\\alpha^2 + \\beta^2)}{(\\alpha + \\beta)^3}}\n$$"
        },
        {
            "introduction": "Astrocytes are not just passive signalers; they are integral components of a feedback loop that regulates synaptic plasticity. In this culminating practice, you will analyze a model where an astrocyte dynamically sets the sliding threshold $\\theta_m$ for a BCM-like plasticity rule, a key mechanism for homeostasis . By performing a linear stability analysis, you will determine how the astrocyte's integration timescale, $T$, dictates the conditions under which this feedback loop ensures stable learning, revealing its essential role in maintaining a functional and stable learning system.",
            "id": "4035704",
            "problem": "You will formalize and analyze a single-synapse plasticity system in which an astrocyte acts as an activity integrator over a finite window to set a metaplastic sliding threshold in a Bienenstock–Cooper–Munro (BCM)-like rule. Start from a valid base that is suitable for neuromorphic and brain-inspired computing and astrocyte-modulated plasticity. Use the following fundamental bases and core definitions: (i) Hebbian plasticity principle with rate-based variables; (ii) a linear postsynaptic response model; and (iii) a first-order low-pass integrator to represent finite-window astrocytic integration.\n\nDefine a single synapse with presynaptic firing rate $x \\ge 0$, synaptic weight $w$, postsynaptic rate $y$, plasticity learning rate $\\eta > 0$, and astrocyte-driven metaplastic threshold $\\theta_m$. Model the postsynaptic response by $y = w x$. Let the BCM-like plasticity follow\n$$\\dot{w} = \\eta \\, x \\, y \\, (y - \\theta_m),$$\nand let the astrocyte integrate cumulative activity over a window of duration $T > 0$ to set the sliding threshold via a first-order differential equation\n$$\\dot{\\theta}_m = \\frac{-\\theta_m + \\alpha \\, y^2}{T},$$\nwhere $\\alpha > 0$ is a proportionality constant setting the scale of the activity-dependent threshold. The window $T$ determines how quickly the threshold tracks integrated activity. Assume all variables are dimensionless rates and weights for the purpose of this mathematical analysis.\n\nTasks:\n1. Derive the fixed points $(w^\\star,\\theta_m^\\star)$ of the coupled system from first principles without using any shortcut formulas beyond the base definitions above.\n2. Perform a linear stability analysis of the fixed points by constructing the Jacobian matrix of the system at the fixed points and computing its eigenvalues, determining when trajectories are asymptotically stable (both eigenvalues have strictly negative real parts) versus unstable or neutrally stable.\n3. Implement the analysis as a program that, given $(\\alpha,\\eta,x,T)$, returns:\n   - the nontrivial fixed point $(w^\\star,\\theta_m^\\star)$ when $x>0$, or $(0,0)$ by convention when $x=0$;\n   - a boolean $s$ indicating asymptotic stability at that fixed point, where $s$ is true if and only if both eigenvalues have strictly negative real parts;\n   - round all reported floating-point results to six decimal places.\n\nYour program must not simulate time-domain dynamics; it must compute the fixed point and the linearized stability analytically or via exact algebraic expressions and numeric eigenvalue computation of the Jacobian. The final output format must be a single line of text containing a comma-separated list enclosed in square brackets, where each test case contributes one sublist of the form $[w^\\star,\\theta_m^\\star,s]$.\n\nTest suite:\nProvide results for the following parameter sets $(\\alpha,\\eta,x,T)$, chosen to test a happy path, boundary, large-window instability, zero presynaptic input, and very fast astrocytic tracking:\n- Case A (happy path): $(\\alpha,\\eta,x,T) = (1.0, 0.05, 1.0, 10.0)$\n- Case B (boundary condition): $(\\alpha,\\eta,x,T) = (1.0, 0.05, 1.0, 20.0)$\n- Case C (slow astrocyte, instability): $(\\alpha,\\eta,x,T) = (1.0, 0.05, 1.0, 50.0)$\n- Case D (edge case, no presynaptic drive): $(\\alpha,\\eta,x,T) = (1.0, 0.05, 0.0, 10.0)$\n- Case E (very fast astrocyte): $(\\alpha,\\eta,x,T) = (1.0, 0.05, 2.0, 4.0)$\n\nAnswer specification:\n- For each case, compute $[w^\\star,\\theta_m^\\star,s]$ where $w^\\star$ and $\\theta_m^\\star$ are floats rounded to six decimals, and $s$ is a boolean.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[[0.123456,0.654321,True],[\\dots]]$).",
            "solution": "We formalize the system based on foundational principles in neuromorphic plasticity. Hebbian plasticity states that synaptic changes are driven by the correlation between presynaptic and postsynaptic activity. For rate-based units, a canonical continuous-time formulation is the Bienenstock–Cooper–Munro (BCM)-like scalar rule. We specify a linear postsynaptic response and a first-order astrocytic integrator to realize metaplasticity.\n\nModel:\n- Presynaptic firing rate: $x \\ge 0$.\n- Synaptic weight: $w$.\n- Postsynaptic rate: $y = w x$.\n- Plasticity learning rate: $\\eta > 0$.\n- Metaplastic sliding threshold: $\\theta_m$ governed by the astrocyte with window $T > 0$ and scale $\\alpha > 0$.\n- Dynamics:\n$$\\dot{w} = \\eta \\, x \\, y \\, (y - \\theta_m) = \\eta \\, x^2 \\, w \\, (w x - \\theta_m),$$\n$$\\dot{\\theta}_m = \\frac{-\\theta_m + \\alpha y^2}{T} = \\frac{-\\theta_m + \\alpha w^2 x^2}{T}.$$\n\nFixed points:\nFixed points $(w^\\star,\\theta_m^\\star)$ satisfy $\\dot{w}=0$ and $\\dot{\\theta}_m=0$ simultaneously. From $\\dot{w}=0$,\n$$\\eta x^2 w^\\star (w^\\star x - \\theta_m^\\star) = 0.$$\nFor $x \\ge 0$ and $\\eta>0$, this yields two cases:\n- Case 1: $w^\\star = 0$.\n- Case 2: $w^\\star x - \\theta_m^\\star = 0 \\Rightarrow \\theta_m^\\star = w^\\star x$.\n\nFrom $\\dot{\\theta}_m=0$,\n$$\\frac{-\\theta_m^\\star + \\alpha w^{\\star 2} x^2}{T} = 0 \\Rightarrow \\theta_m^\\star = \\alpha w^{\\star 2} x^2.$$\n\nCombine with Case 1: $w^\\star=0$ implies $y^\\star=0$, then $\\theta_m^\\star=0$ from the threshold equation. Thus $(w^\\star,\\theta_m^\\star)=(0,0)$ is a fixed point.\n\nCombine with Case 2 for $x>0$:\n$$\\theta_m^\\star = w^\\star x = \\alpha w^{\\star 2} x^2.$$\nAssuming $w^\\star \\ne 0$ and dividing both sides by $x$,\n$$w^\\star = \\alpha w^{\\star 2} x \\Rightarrow 1 = \\alpha w^\\star x \\Rightarrow w^\\star = \\frac{1}{\\alpha x}.$$\nThen,\n$$\\theta_m^\\star = w^\\star x = \\frac{1}{\\alpha}.$$\nHence for $x>0$, there is a nontrivial fixed point $(w^\\star,\\theta_m^\\star)=\\left(\\frac{1}{\\alpha x}, \\frac{1}{\\alpha}\\right)$.\n\nLinear stability analysis:\nDefine the vector field $\\mathbf{f}(w,\\theta_m) = (f_w, f_{\\theta})$ with\n$$f_w = \\eta x^2 w (w x - \\theta_m) = \\eta x^3 w^2 - \\eta x^2 w \\theta_m,$$\n$$f_{\\theta} = \\frac{-\\theta_m + \\alpha w^2 x^2}{T}.$$\nThe Jacobian matrix $J$ is\n$$J = \\begin{bmatrix}\n\\frac{\\partial f_w}{\\partial w} & \\frac{\\partial f_w}{\\partial \\theta_m} \\\\\n\\frac{\\partial f_{\\theta}}{\\partial w} & \\frac{\\partial f_{\\theta}}{\\partial \\theta_m}\n\\end{bmatrix} = \\begin{bmatrix}\n\\eta x^2 (2 w x - \\theta_m) & -\\eta x^2 w \\\\\n\\frac{2 \\alpha w x^2}{T} & -\\frac{1}{T}\n\\end{bmatrix}.$$\n\nEvaluate $J$ at the nontrivial fixed point for $x>0$ with $(w^\\star,\\theta_m^\\star)=\\left(\\frac{1}{\\alpha x}, \\frac{1}{\\alpha}\\right)$:\nCompute $2 w^\\star x - \\theta_m^\\star = 2 \\frac{1}{\\alpha} - \\frac{1}{\\alpha} = \\frac{1}{\\alpha}$, and $w^\\star = \\frac{1}{\\alpha x}$.\nThus,\n$$J^\\star = \\begin{bmatrix}\n\\frac{\\eta x^2}{\\alpha} & -\\frac{\\eta x}{\\alpha} \\\\\n\\frac{2 x}{T} & -\\frac{1}{T}\n\\end{bmatrix}.$$\nThe characteristic polynomial is\n$$\\lambda^2 - \\tau \\lambda + \\Delta = 0,$$\nwith trace\n$$\\tau = \\frac{\\eta x^2}{\\alpha} - \\frac{1}{T},$$\nand determinant\n$$\\Delta = \\frac{\\eta x^2}{\\alpha T}.$$\nAsymptotic stability in two dimensions requires both eigenvalues to have strictly negative real parts. For real matrices with positive determinant, the necessary and sufficient conditions for asymptotic stability are $\\tau < 0$ and $\\Delta > 0$. Here $\\Delta > 0$ is guaranteed for $\\eta>0$, $x\\ge 0$, $\\alpha>0$, and $T>0$. Therefore, stability reduces to\n$$\\tau < 0 \\quad \\Leftrightarrow \\quad \\frac{\\eta x^2}{\\alpha} - \\frac{1}{T} < 0 \\quad \\Leftrightarrow \\quad T < \\frac{\\alpha}{\\eta x^2}.$$\nThis gives a clear metaplastic stability condition: the astrocytic integration must be sufficiently fast (small $T$) relative to the synaptic drive $x$ and plasticity gain $\\eta$, scaled by $\\alpha$, to ensure stability. At the boundary $T = \\frac{\\alpha}{\\eta x^2}$, $\\tau = 0$ and the eigenvalues are purely imaginary (neutrally stable), not asymptotically stable. For $T > \\frac{\\alpha}{\\eta x^2}$, the trace becomes positive and the fixed point is unstable.\n\nFor the trivial fixed point $(0,0)$, evaluating $J$ yields\n$$J_0 = \\begin{bmatrix}\n0 & 0 \\\\\n0 & -\\frac{1}{T}\n\\end{bmatrix},$$\nwhose eigenvalues are $0$ and $-\\frac{1}{T}$. The zero eigenvalue indicates neutral stability along the $w$ direction; the dynamics are not asymptotically stable at the origin. Indeed, near $w=0$ and $\\theta_m=0$, one has $\\dot{w} \\approx \\eta x^3 w^2$ for $x>0$, which causes $w$ to increase for any $w>0$; thus the origin is unstable for $x>0$ and neutrally stable only when $x=0$ since $\\dot{w}=0$ for all $w$ if $x=0$.\n\nAlgorithmic design for the program:\n- Input: $(\\alpha,\\eta,x,T)$ for each test case.\n- Compute the fixed point:\n  - If $x>0$, set $w^\\star = \\frac{1}{\\alpha x}$ and $\\theta_m^\\star = \\frac{1}{\\alpha}$.\n  - If $x=0$, set $(w^\\star,\\theta_m^\\star)=(0,0)$ by convention.\n- Construct the Jacobian matrix at $(w^\\star,\\theta_m^\\star)$ using the exact partial derivatives.\n- Compute eigenvalues $\\lambda_1,\\lambda_2$ and determine asymptotic stability $s$ by checking if $\\Re(\\lambda_1) < 0$ and $\\Re(\\lambda_2) < 0$.\n- Round $w^\\star$ and $\\theta_m^\\star$ to six decimals and output $[w^\\star,\\theta_m^\\star,s]$ per test case.\n- Aggregate all results into a single line of output as a comma-separated list enclosed in square brackets.\n\nTest suite expectations:\n- Case A: $(\\alpha,\\eta,x,T) = (1.0, 0.05, 1.0, 10.0)$ yields $T < \\frac{\\alpha}{\\eta x^2} = \\frac{1.0}{0.05 \\cdot 1.0} = 20.0$ so asymptotically stable at the nontrivial fixed point.\n- Case B: $(1.0, 0.05, 1.0, 20.0)$ yields the boundary $T = 20.0$, neutrally stable, not asymptotically stable.\n- Case C: $(1.0, 0.05, 1.0, 50.0)$ yields $T > 20.0$, unstable.\n- Case D: $(1.0, 0.05, 0.0, 10.0)$ yields $(w^\\star,\\theta_m^\\star)=(0,0)$ and one eigenvalue equals $0$, not asymptotically stable.\n- Case E: $(1.0, 0.05, 2.0, 4.0)$ yields $\\frac{\\alpha}{\\eta x^2} = \\frac{1.0}{0.05 \\cdot 4.0} = 5.0$ and $T=4.0 < 5.0$ so asymptotically stable.\n\nThe program will implement this logic, compute the Jacobian, eigenvalues, and the final stability boolean for each case, and print the aggregated list in the specified format.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef fixed_point(alpha, eta, x, T):\n    \"\"\"\n    Compute the fixed point (w*, theta*) for the given parameters.\n    For x > 0: nontrivial fixed point w* = 1/(alpha*x), theta* = 1/alpha.\n    For x = 0: by convention, return (0.0, 0.0).\n    \"\"\"\n    if x > 0:\n        w_star = 1.0 / (alpha * x)\n        theta_star = 1.0 / alpha\n    else:\n        w_star = 0.0\n        theta_star = 0.0\n    return w_star, theta_star\n\ndef jacobian(alpha, eta, x, T, w, theta):\n    \"\"\"\n    Construct the Jacobian matrix at (w, theta) for given parameters.\n    J = [[ eta*x^2*(2*w*x - theta), -eta*x^2*w ],\n         [ 2*alpha*w*x**2 / T,      -1/T        ]]\n    \"\"\"\n    j11 = eta * x**2 * (2.0 * w * x - theta)\n    j12 = -eta * x**2 * w\n    j21 = (2.0 * alpha * w * x**2) / T if T > 0 else np.inf\n    j22 = -1.0 / T if T > 0 else -np.inf\n    return np.array([[j11, j12],\n                     [j21, j22]], dtype=float)\n\ndef stability_from_jacobian(J):\n    \"\"\"\n    Compute eigenvalues and decide asymptotic stability:\n    stable if and only if both eigenvalues have strictly negative real parts.\n    \"\"\"\n    eigvals = np.linalg.eigvals(J)\n    real_parts = np.real(eigvals)\n    stable = np.all(real_parts  0.0)\n    return stable, eigvals\n\ndef analyze_case(alpha, eta, x, T):\n    w_star, theta_star = fixed_point(alpha, eta, x, T)\n    J = jacobian(alpha, eta, x, T, w_star, theta_star)\n    stable, _ = stability_from_jacobian(J)\n    # Round floats to 6 decimals as specified\n    return [round(w_star, 6), round(theta_star, 6), bool(stable)]\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case is (alpha, eta, x, T)\n    test_cases = [\n        (1.0, 0.05, 1.0, 10.0),  # Case A: happy path, stable\n        (1.0, 0.05, 1.0, 20.0),  # Case B: boundary, neutrally stable (not asymptotically stable)\n        (1.0, 0.05, 1.0, 50.0),  # Case C: unstable due to large T\n        (1.0, 0.05, 0.0, 10.0),  # Case D: x=0, neutral along w (not asymptotically stable)\n        (1.0, 0.05, 2.0, 4.0),   # Case E: fast astrocyte, stable\n    ]\n\n    results = []\n    for (alpha, eta, x, T) in test_cases:\n        result = analyze_case(alpha, eta, x, T)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(str(results))\n\nsolve()\n```"
        }
    ]
}