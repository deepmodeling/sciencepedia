{
    "hands_on_practices": [
        {
            "introduction": "To build robust models of astrocyte-neuron interactions, we must first understand the fundamental biophysical consequences of gliotransmission. This exercise provides a foundational calculation, guiding you to quantify the direct impact of a transient glutamate pulse from an astrocyte on a postsynaptic neuron. By deriving the total charge influx through extrasynaptic NMDA receptors, you will gain a concrete understanding of how a single astrocytic signaling event translates into a measurable electrical effect, a crucial building block for more complex network models .",
            "id": "4035731",
            "problem": "Astrocytes modulate synaptic and extrasynaptic signaling by releasing glutamate into the extracellular space. Consider a neuromorphic model of astrocyte-modulated plasticity in which activation of extrasynaptic N-Methyl-D-Aspartate (NMDA) receptors contributes to postsynaptic current as a saturating function of the instantaneous extracellular glutamate concentration. Assume a voltage-clamped postsynaptic membrane so that the NMDA conductance is a fixed function $g_{N}(V)$ of the clamp voltage $V$, and assume quasi-steady-state receptor occupancy relative to glutamate clearance. Let the NMDA current be modeled by the standard Michaelis–Menten form\n$$\nI_{NMDA}(t) \\;=\\; g_{N}(V)\\,\\frac{[G_{e}](t)}{K_{N} + [G_{e}](t)},\n$$\nwhere $[G_{e}](t)$ is the extracellular glutamate concentration, $K_{N} > 0$ is the effective half-saturation constant of the extrasynaptic NMDA receptor binding under the given co-agonist and magnesium conditions, and $g_{N}(V) > 0$ is constant in time. Prior to astrocyte release, the baseline concentration is $[G_{e}](t) = G_{0}$ with $G_{0} \\ge 0$. At time $t=0$, the astrocyte releases a transient glutamate pulse into the extracellular space that adds to the baseline and decays monoexponentially due to uptake and diffusion, so that for $t \\ge 0$,\n$$\n[G_{e}](t) \\;=\\; G_{0} + A\\,\\exp\\!\\left(-\\frac{t}{\\tau_{g}}\\right),\n$$\nwith $A > 0$ the pulse amplitude and $\\tau_{g} > 0$ the decay time constant.\n\nStarting from mass-action receptor occupancy and assuming the above dynamics and voltage-clamp conditions, derive a closed-form analytic expression for the total time-integrated increase in NMDA current relative to baseline induced by the pulse,\n$$\n\\Delta Q \\;=\\; \\int_{0}^{\\infty}\\big(I_{NMDA}(t) - I_{NMDA}^{\\text{base}}\\big)\\,dt,\n$$\nwhere\n$$\nI_{NMDA}^{\\text{base}} \\;=\\; g_{N}(V)\\,\\frac{G_{0}}{K_{N} + G_{0}}.\n$$\nExpress your final answer as a single closed-form analytic expression in terms of $g_{N}(V)$, $K_{N}$, $G_{0}$, $A$, and $\\tau_{g}$. No numerical evaluation is required, and no units should be included in the final boxed answer.",
            "solution": "The problem as stated is subjected to validation against the established criteria.\n\n### Step 1: Extract Givens\nThe following quantities and relations are provided verbatim:\n-   NMDA current model: $I_{NMDA}(t) = g_{N}(V)\\,\\frac{[G_{e}](t)}{K_{N} + [G_{e}](t)}$\n-   Extracellular glutamate concentration for $t \\ge 0$: $[G_{e}](t) = G_{0} + A\\,\\exp(-\\frac{t}{\\tau_{g}})$\n-   Baseline NMDA current: $I_{NMDA}^{\\text{base}} = g_{N}(V)\\,\\frac{G_{0}}{K_{N} + G_{0}}$\n-   Total time-integrated current increase to be derived: $\\Delta Q = \\int_{0}^{\\infty}\\big(I_{NMDA}(t) - I_{NMDA}^{\\text{base}}\\big)\\,dt$\n-   Parameter constraints: $K_{N} > 0$, $g_{N}(V) > 0$, $G_{0} \\ge 0$, $A > 0$, $\\tau_{g} > 0$.\n-   Contextual conditions: Voltage-clamped postsynaptic membrane, quasi-steady-state receptor occupancy.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed for validity:\n-   **Scientifically Grounded**: The problem utilizes a standard Michaelis–Menten formulation for receptor kinetics and a monoexponential decay for a neurotransmitter pulse. These are common and well-accepted simplifications in computational neuroscience and biophysical modeling. The setup is scientifically plausible within this modeling context.\n-   **Well-Posed**: The problem is mathematically well-defined. It requests the evaluation of a definite integral of a function constructed from well-behaved components. The integrand approaches zero as $t \\to \\infty$, ensuring the integral converges. All necessary parameters are provided, and a unique analytical solution is expected to exist.\n-   **Objective**: The problem is stated using precise mathematical definitions and objective language, free of ambiguity or subjective claims.\n\nThe problem does not exhibit any of the enumerated invalidity flaws. It is scientifically sound, formally stated, self-contained, and solvable.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A solution will be derived.\n\nThe objective is to compute the total-time integrated increase in NMDA current, $\\Delta Q$, relative to its baseline value. The definition is given as:\n$$\n\\Delta Q = \\int_{0}^{\\infty} \\left( I_{NMDA}(t) - I_{NMDA}^{\\text{base}} \\right) dt\n$$\nFirst, we substitute the expressions for $I_{NMDA}(t)$ and $I_{NMDA}^{\\text{base}}$. The term $g_{N}(V)$ is a common factor and can be moved outside the integral.\n$$\n\\Delta Q = g_{N}(V) \\int_{0}^{\\infty} \\left( \\frac{[G_{e}](t)}{K_{N} + [G_{e}](t)} - \\frac{G_{0}}{K_{N} + G_{0}} \\right) dt\n$$\nNext, we substitute the expression for the time-dependent glutamate concentration, $[G_{e}](t) = G_{0} + A\\,\\exp(-t/\\tau_{g})$, into the integrand for $t \\ge 0$.\n$$\n\\Delta Q = g_{N}(V) \\int_{0}^{\\infty} \\left( \\frac{G_{0} + A\\,\\exp(-t/\\tau_{g})}{K_{N} + G_{0} + A\\,\\exp(-t/\\tau_{g})} - \\frac{G_{0}}{K_{N} + G_{0}} \\right) dt\n$$\nTo simplify the expression inside the integral, we combine the two fractions by finding a common denominator, which is $(K_{N} + G_{0})(K_{N} + G_{0} + A\\,\\exp(-t/\\tau_{g}))$.\nLet's denote $x(t) = A\\,\\exp(-t/\\tau_{g})$ for brevity. The integrand becomes:\n$$\n\\frac{G_{0} + x(t)}{K_{N} + G_{0} + x(t)} - \\frac{G_{0}}{K_{N} + G_{0}} = \\frac{(G_{0} + x(t))(K_{N} + G_{0}) - G_{0}(K_{N} + G_{0} + x(t))}{(K_{N} + G_{0})(K_{N} + G_{0} + x(t))}\n$$\nExpanding the numerator:\n$$\n(G_{0}K_{N} + G_{0}^2 + x(t)K_{N} + x(t)G_{0}) - (G_{0}K_{N} + G_{0}^2 + G_{0}x(t))\n$$\nSimplifying the numerator yields:\n$$\nG_{0}K_{N} + G_{0}^2 + K_{N}x(t) + G_{0}x(t) - G_{0}K_{N} - G_{0}^2 - G_{0}x(t) = K_{N}x(t)\n$$\nSubstituting $x(t) = A\\,\\exp(-t/\\tau_{g})$ back, the simplified integrand is:\n$$\n\\frac{K_{N}A\\,\\exp(-t/\\tau_{g})}{(K_{N} + G_{0})(K_{N} + G_{0} + A\\,\\exp(-t/\\tau_{g}))}\n$$\nThe integral for $\\Delta Q$ is now:\n$$\n\\Delta Q = g_{N}(V) \\int_{0}^{\\infty} \\frac{K_{N}A\\,\\exp(-t/\\tau_{g})}{(K_{N} + G_{0})(K_{N} + G_{0} + A\\,\\exp(-t/\\tau_{g}))} dt\n$$\nWe can factor out the terms that are not dependent on the integration variable $t$:\n$$\n\\Delta Q = \\frac{g_{N}(V) K_{N} A}{K_{N} + G_{0}} \\int_{0}^{\\infty} \\frac{\\exp(-t/\\tau_{g})}{K_{N} + G_{0} + A\\,\\exp(-t/\\tau_{g})} dt\n$$\nTo solve the remaining integral, we perform a change of variables. Let $u = K_{N} + G_{0} + A\\,\\exp(-t/\\tau_{g})$.\nThe differential $du$ is then:\n$$\ndu = A \\left(-\\frac{1}{\\tau_{g}}\\right) \\exp(-t/\\tau_{g}) dt\n$$\nThis can be rearranged to express the numerator of the integrand:\n$$\n\\exp(-t/\\tau_{g}) dt = -\\frac{\\tau_{g}}{A} du\n$$\nWe must also transform the limits of integration from $t$ to $u$:\n-   At the lower limit, $t=0$, we have $u(0) = K_{N} + G_{0} + A\\,\\exp(0) = K_{N} + G_{0} + A$.\n-   At the upper limit, $t \\to \\infty$, we have $u(\\infty) = K_{N} + G_{0} + A\\,\\lim_{t \\to \\infty}\\exp(-t/\\tau_{g}) = K_{N} + G_{0}$.\nSubstituting these into the integral:\n$$\n\\int_{0}^{\\infty} \\frac{\\exp(-t/\\tau_{g})}{K_{N} + G_{0} + A\\,\\exp(-t/\\tau_{g})} dt = \\int_{K_{N} + G_{0} + A}^{K_{N} + G_{0}} \\frac{1}{u} \\left(-\\frac{\\tau_{g}}{A}\\right) du\n$$\nWe can reverse the limits of integration, which removes the negative sign:\n$$\n= \\frac{\\tau_{g}}{A} \\int_{K_{N} + G_{0}}^{K_{N} + G_{0} + A} \\frac{1}{u} du\n$$\nThe integral of $1/u$ is $\\ln|u|$. Since all parameters ($K_{N}, A, \\tau_{g}$) are positive and $G_{0} \\ge 0$, the argument $u$ is always positive over the integration interval. Thus, the absolute value is not necessary.\n$$\n= \\frac{\\tau_{g}}{A} \\left[ \\ln(u) \\right]_{K_{N} + G_{0}}^{K_{N} + G_{0} + A}\n$$\nEvaluating the definite integral:\n$$\n= \\frac{\\tau_{g}}{A} \\left( \\ln(K_{N} + G_{0} + A) - \\ln(K_{N} + G_{0}) \\right)\n$$\nUsing the property of logarithms $\\ln(a) - \\ln(b) = \\ln(a/b)$, we get:\n$$\n= \\frac{\\tau_{g}}{A} \\ln\\left(\\frac{K_{N} + G_{0} + A}{K_{N} + G_{0}}\\right)\n$$\nFinally, we substitute this result back into the expression for $\\Delta Q$:\n$$\n\\Delta Q = \\frac{g_{N}(V) K_{N} A}{K_{N} + G_{0}} \\left[ \\frac{\\tau_{g}}{A} \\ln\\left(\\frac{K_{N} + G_{0} + A}{K_{N} + G_{0}}\\right) \\right]\n$$\nThe term $A$ in the prefactor cancels with the $A$ in the denominator of the integrated expression:\n$$\n\\Delta Q = \\frac{g_{N}(V) K_{N} \\tau_{g}}{K_{N} + G_{0}} \\ln\\left(\\frac{K_{N} + G_{0} + A}{K_{N} + G_{0}}\\right)\n$$\nThis is the final closed-form analytic expression for the total time-integrated increase in NMDA current.",
            "answer": "$$\n\\boxed{\\frac{g_{N}(V) K_{N} \\tau_{g}}{K_{N} + G_{0}} \\ln\\left(\\frac{K_{N} + G_{0} + A}{K_{N} + G_{0}}\\right)}\n$$"
        },
        {
            "introduction": "Beyond single events, astrocytes play a higher-level functional role by integrating synaptic activity over time to regulate learning rules, a phenomenon known as metaplasticity. In this practice, you will model this process by constructing a coupled dynamical system where an astrocyte sets a \"sliding threshold\" for a BCM-like plasticity rule . Performing a stability analysis will allow you to determine the conditions under which this astrocytic feedback successfully homeostatically regulates the synapse, preventing runaway potentiation or depression and ensuring the stability of the learning system.",
            "id": "4035704",
            "problem": "You will formalize and analyze a single-synapse plasticity system in which an astrocyte acts as an activity integrator over a finite window to set a metaplastic sliding threshold in a Bienenstock–Cooper–Munro (BCM)-like rule. Start from a valid base that is suitable for neuromorphic and brain-inspired computing and astrocyte-modulated plasticity. Use the following fundamental bases and core definitions: (i) Hebbian plasticity principle with rate-based variables; (ii) a linear postsynaptic response model; and (iii) a first-order low-pass integrator to represent finite-window astrocytic integration.\n\nDefine a single synapse with presynaptic firing rate $x \\ge 0$, synaptic weight $w$, postsynaptic rate $y$, plasticity learning rate $\\eta > 0$, and astrocyte-driven metaplastic threshold $\\theta_m$. Model the postsynaptic response by $y = w x$. Let the BCM-like plasticity follow\n$$\\dot{w} = \\eta \\, x \\, y \\, (y - \\theta_m),$$\nand let the astrocyte integrate cumulative activity over a window of duration $T > 0$ to set the sliding threshold via a first-order differential equation\n$$\\dot{\\theta}_m = \\frac{-\\theta_m + \\alpha \\, y^2}{T},$$\nwhere $\\alpha > 0$ is a proportionality constant setting the scale of the activity-dependent threshold. The window $T$ determines how quickly the threshold tracks integrated activity. Assume all variables are dimensionless rates and weights for the purpose of this mathematical analysis.\n\nTasks:\n1. Derive the fixed points $(w^\\star,\\theta_m^\\star)$ of the coupled system from first principles without using any shortcut formulas beyond the base definitions above.\n2. Perform a linear stability analysis of the fixed points by constructing the Jacobian matrix of the system at the fixed points and computing its eigenvalues, determining when trajectories are asymptotically stable (both eigenvalues have strictly negative real parts) versus unstable or neutrally stable.\n3. Implement the analysis as a program that, given $(\\alpha,\\eta,x,T)$, returns:\n   - the nontrivial fixed point $(w^\\star,\\theta_m^\\star)$ when $x>0$, or $(0,0)$ by convention when $x=0$;\n   - a boolean $s$ indicating asymptotic stability at that fixed point, where $s$ is true if and only if both eigenvalues have strictly negative real parts;\n   - round all reported floating-point results to six decimal places.\n\nYour program must not simulate time-domain dynamics; it must compute the fixed point and the linearized stability analytically or via exact algebraic expressions and numeric eigenvalue computation of the Jacobian. The final output format must be a single line of text containing a comma-separated list enclosed in square brackets, where each test case contributes one sublist of the form $[w^\\star,\\theta_m^\\star,s]$.\n\nTest suite:\nProvide results for the following parameter sets $(\\alpha,\\eta,x,T)$, chosen to test a happy path, boundary, large-window instability, zero presynaptic input, and very fast astrocytic tracking:\n- Case A (happy path): $(\\alpha,\\eta,x,T) = (1.0, 0.05, 1.0, 10.0)$\n- Case B (boundary condition): $(\\alpha,\\eta,x,T) = (1.0, 0.05, 1.0, 20.0)$\n- Case C (slow astrocyte, instability): $(\\alpha,\\eta,x,T) = (1.0, 0.05, 1.0, 50.0)$\n- Case D (edge case, no presynaptic drive): $(\\alpha,\\eta,x,T) = (1.0, 0.05, 0.0, 10.0)$\n- Case E (very fast astrocyte): $(\\alpha,\\eta,x,T) = (1.0, 0.05, 2.0, 4.0)$\n\nAnswer specification:\n- For each case, compute $[w^\\star,\\theta_m^\\star,s]$ where $w^\\star$ and $\\theta_m^\\star$ are floats rounded to six decimals, and $s$ is a boolean.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[[0.123456,0.654321,True],[\\dots]]$).",
            "solution": "We formalize the system based on foundational principles in neuromorphic plasticity. Hebbian plasticity states that synaptic changes are driven by the correlation between presynaptic and postsynaptic activity. For rate-based units, a canonical continuous-time formulation is the Bienenstock–Cooper–Munro (BCM)-like scalar rule. We specify a linear postsynaptic response and a first-order astrocytic integrator to realize metaplasticity.\n\nModel:\n- Presynaptic firing rate: $x \\ge 0$.\n- Synaptic weight: $w$.\n- Postsynaptic rate: $y = w x$.\n- Plasticity learning rate: $\\eta > 0$.\n- Metaplastic sliding threshold: $\\theta_m$ governed by the astrocyte with window $T > 0$ and scale $\\alpha > 0$.\n- Dynamics:\n$$\\dot{w} = \\eta \\, x \\, y \\, (y - \\theta_m) = \\eta \\, x^2 \\, w \\, (w x - \\theta_m),$$\n$$\\dot{\\theta}_m = \\frac{-\\theta_m + \\alpha y^2}{T} = \\frac{-\\theta_m + \\alpha w^2 x^2}{T}.$$\n\nFixed points:\nFixed points $(w^\\star,\\theta_m^\\star)$ satisfy $\\dot{w}=0$ and $\\dot{\\theta}_m=0$ simultaneously. From $\\dot{w}=0$,\n$$\\eta x^2 w^\\star (w^\\star x - \\theta_m^\\star) = 0.$$\nFor $x \\ge 0$ and $\\eta>0$, this yields two cases:\n- Case 1: $w^\\star = 0$.\n- Case 2: $w^\\star x - \\theta_m^\\star = 0 \\Rightarrow \\theta_m^\\star = w^\\star x$.\n\nFrom $\\dot{\\theta}_m=0$,\n$$\\frac{-\\theta_m^\\star + \\alpha w^{\\star 2} x^2}{T} = 0 \\Rightarrow \\theta_m^\\star = \\alpha w^{\\star 2} x^2.$$\n\nCombine with Case 1: $w^\\star=0$ implies $y^\\star=0$, then $\\theta_m^\\star=0$ from the threshold equation. Thus $(w^\\star,\\theta_m^\\star)=(0,0)$ is a fixed point.\n\nCombine with Case 2 for $x>0$:\n$$\\theta_m^\\star = w^\\star x = \\alpha w^{\\star 2} x^2.$$\nAssuming $w^\\star \\ne 0$ and dividing both sides by $x$,\n$$w^\\star = \\alpha w^{\\star 2} x \\Rightarrow 1 = \\alpha w^\\star x \\Rightarrow w^\\star = \\frac{1}{\\alpha x}.$$\nThen,\n$$\\theta_m^\\star = w^\\star x = \\frac{1}{\\alpha}.$$\nHence for $x>0$, there is a nontrivial fixed point $(w^\\star,\\theta_m^\\star)=\\left(\\frac{1}{\\alpha x}, \\frac{1}{\\alpha}\\right)$.\n\nLinear stability analysis:\nDefine the vector field $\\mathbf{f}(w,\\theta_m) = (f_w, f_{\\theta})$ with\n$$f_w = \\eta x^2 w (w x - \\theta_m) = \\eta x^3 w^2 - \\eta x^2 w \\theta_m,$$\n$$f_{\\theta} = \\frac{-\\theta_m + \\alpha w^2 x^2}{T}.$$\nThe Jacobian matrix $J$ is\n$$J = \\begin{bmatrix}\n\\frac{\\partial f_w}{\\partial w} & \\frac{\\partial f_w}{\\partial \\theta_m} \\\\\n\\frac{\\partial f_{\\theta}}{\\partial w} & \\frac{\\partial f_{\\theta}}{\\partial \\theta_m}\n\\end{bmatrix} = \\begin{bmatrix}\n\\eta x^2 (2 w x - \\theta_m) & -\\eta x^2 w \\\\\n\\frac{2 \\alpha w x^2}{T} & -\\frac{1}{T}\n\\end{bmatrix}.$$\n\nEvaluate $J$ at the nontrivial fixed point for $x>0$ with $(w^\\star,\\theta_m^\\star)=\\left(\\frac{1}{\\alpha x}, \\frac{1}{\\alpha}\\right)$:\nCompute $2 w^\\star x - \\theta_m^\\star = 2 \\frac{1}{\\alpha} - \\frac{1}{\\alpha} = \\frac{1}{\\alpha}$, and $w^\\star = \\frac{1}{\\alpha x}$.\nThus,\n$$J^\\star = \\begin{bmatrix}\n\\frac{\\eta x^2}{\\alpha} & -\\frac{\\eta x}{\\alpha} \\\\\n\\frac{2 x}{T} & -\\frac{1}{T}\n\\end{bmatrix}.$$\nThe characteristic polynomial is\n$$\\lambda^2 - \\tau \\lambda + \\Delta = 0,$$\nwith trace\n$$\\tau = \\frac{\\eta x^2}{\\alpha} - \\frac{1}{T},$$\nand determinant\n$$\\Delta = \\frac{\\eta x^2}{\\alpha T}.$$\nAsymptotic stability in two dimensions requires both eigenvalues to have strictly negative real parts. For real matrices with positive determinant, the necessary and sufficient conditions for asymptotic stability are $\\tau < 0$ and $\\Delta > 0$. Here $\\Delta > 0$ is guaranteed for $\\eta>0$, $x\\ge 0$, $\\alpha>0$, and $T>0$. Therefore, stability reduces to\n$$\\tau < 0 \\quad \\Leftrightarrow \\quad \\frac{\\eta x^2}{\\alpha} - \\frac{1}{T} < 0 \\quad \\Leftrightarrow \\quad T < \\frac{\\alpha}{\\eta x^2}.$$\nThis gives a clear metaplastic stability condition: the astrocytic integration must be sufficiently fast (small $T$) relative to the synaptic drive $x$ and plasticity gain $\\eta$, scaled by $\\alpha$, to ensure stability. At the boundary $T = \\frac{\\alpha}{\\eta x^2}$, $\\tau = 0$ and the eigenvalues are purely imaginary (neutrally stable), not asymptotically stable. For $T > \\frac{\\alpha}{\\eta x^2}$, the trace becomes positive and the fixed point is unstable.\n\nFor the trivial fixed point $(0,0)$, evaluating $J$ yields\n$$J_0 = \\begin{bmatrix}\n0 & 0 \\\\\n0 & -\\frac{1}{T}\n\\end{bmatrix},$$\nwhose eigenvalues are $0$ and $-\\frac{1}{T}$. The zero eigenvalue indicates neutral stability along the $w$ direction; the dynamics are not asymptotically stable at the origin. Indeed, near $w=0$ and $\\theta_m=0$, one has $\\dot{w} \\approx \\eta x^3 w^2$ for $x>0$, which causes $w$ to increase for any $w>0$; thus the origin is unstable for $x>0$ and neutrally stable only when $x=0$ since $\\dot{w}=0$ for all $w$ if $x=0$.\n\nAlgorithmic design for the program:\n- Input: $(\\alpha,\\eta,x,T)$ for each test case.\n- Compute the fixed point:\n  - If $x>0$, set $w^\\star = \\frac{1}{\\alpha x}$ and $\\theta_m^\\star = \\frac{1}{\\alpha}$.\n  - If $x=0$, set $(w^\\star,\\theta_m^\\star)=(0,0)$ by convention.\n- Construct the Jacobian matrix at $(w^\\star,\\theta_m^\\star)$ using the exact partial derivatives.\n- Compute eigenvalues $\\lambda_1,\\lambda_2$ and determine asymptotic stability $s$ by checking if $\\Re(\\lambda_1) < 0$ and $\\Re(\\lambda_2) < 0$.\n- Round $w^\\star$ and $\\theta_m^\\star$ to six decimals and output $[w^\\star,\\theta_m^\\star,s]$ per test case.\n- Aggregate all results into a single line of output as a comma-separated list enclosed in square brackets.\n\nTest suite expectations:\n- Case A: $(\\alpha,\\eta,x,T) = (1.0, 0.05, 1.0, 10.0)$ yields $T < \\frac{\\alpha}{\\eta x^2} = \\frac{1.0}{0.05 \\cdot 1.0} = 20.0$ so asymptotically stable at the nontrivial fixed point.\n- Case B: $(1.0, 0.05, 1.0, 20.0)$ yields the boundary $T = 20.0$, neutrally stable, not asymptotically stable.\n- Case C: $(1.0, 0.05, 1.0, 50.0)$ yields $T > 20.0$, unstable.\n- Case D: $(1.0, 0.05, 0.0, 10.0)$ yields $(w^\\star,\\theta_m^\\star)=(0,0)$ and one eigenvalue equals $0$, not asymptotically stable.\n- Case E: $(1.0, 0.05, 2.0, 4.0)$ yields $\\frac{\\alpha}{\\eta x^2} = \\frac{1.0}{0.05 \\cdot 4.0} = 5.0$ and $T=4.0 < 5.0$ so asymptotically stable.\n\nThe program will implement this logic, compute the Jacobian, eigenvalues, and the final stability boolean for each case, and print the aggregated list in the specified format.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef fixed_point(alpha, eta, x, T):\n    \"\"\"\n    Compute the fixed point (w*, theta*) for the given parameters.\n    For x > 0: nontrivial fixed point w* = 1/(alpha*x), theta* = 1/alpha.\n    For x = 0: by convention, return (0.0, 0.0).\n    \"\"\"\n    if x > 0:\n        w_star = 1.0 / (alpha * x)\n        theta_star = 1.0 / alpha\n    else:\n        w_star = 0.0\n        theta_star = 0.0\n    return w_star, theta_star\n\ndef jacobian(alpha, eta, x, T, w, theta):\n    \"\"\"\n    Construct the Jacobian matrix at (w, theta) for given parameters.\n    J = [[ eta*x^2*(2*w*x - theta), -eta*x^2*w ],\n         [ 2*alpha*w*x**2 / T,      -1/T        ]]\n    \"\"\"\n    j11 = eta * x**2 * (2.0 * w * x - theta)\n    j12 = -eta * x**2 * w\n    j21 = (2.0 * alpha * w * x**2) / T if T > 0 else np.inf\n    j22 = -1.0 / T if T > 0 else -np.inf\n    return np.array([[j11, j12],\n                     [j21, j22]], dtype=float)\n\ndef stability_from_jacobian(J):\n    \"\"\"\n    Compute eigenvalues and decide asymptotic stability:\n    stable if and only if both eigenvalues have strictly negative real parts.\n    \"\"\"\n    eigvals = np.linalg.eigvals(J)\n    real_parts = np.real(eigvals)\n    stable = np.all(real_parts < 0.0)\n    return stable, eigvals\n\ndef analyze_case(alpha, eta, x, T):\n    w_star, theta_star = fixed_point(alpha, eta, x, T)\n    J = jacobian(alpha, eta, x, T, w_star, theta_star)\n    stable, _ = stability_from_jacobian(J)\n    # Round floats to 6 decimals as specified\n    return [round(w_star, 6), round(theta_star, 6), bool(stable)]\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case is (alpha, eta, x, T)\n    test_cases = [\n        (1.0, 0.05, 1.0, 10.0),  # Case A: happy path, stable\n        (1.0, 0.05, 1.0, 20.0),  # Case B: boundary, neutrally stable (not asymptotically stable)\n        (1.0, 0.05, 1.0, 50.0),  # Case C: unstable due to large T\n        (1.0, 0.05, 0.0, 10.0),  # Case D: x=0, neutral along w (not asymptotically stable)\n        (1.0, 0.05, 2.0, 4.0),   # Case E: fast astrocyte, stable\n    ]\n\n    results = []\n    for (alpha, eta, x, T) in test_cases:\n        result = analyze_case(alpha, eta, x, T)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(str(results))\n\nsolve()\n```"
        },
        {
            "introduction": "A key challenge in computational neuroscience is confronting theoretical models with experimental data. This practice bridges that gap by placing you in the role of a data scientist tasked with parameterizing a model of astrocyte-modulated synaptic transmission . Using the powerful technique of maximum likelihood estimation, you will analyze simulated postsynaptic current recordings to disentangle and quantify the separate contributions of direct synaptic transmission and astrocytic modulation, a crucial skill for validating models and generating testable predictions.",
            "id": "4035724",
            "problem": "You are given an astrocyte-modulated synaptic transmission model at discrete times that follows the linear-response superposition principle and independent Gaussian measurement noise. In each time bin indexed by $t \\in \\{1,\\dots,T\\}$, the observed postsynaptic current $y_t$ (in picoamperes) is modeled as\n$$\ny_t \\;=\\; \\mu_t \\;+\\; \\varepsilon_t, \\quad \\varepsilon_t \\sim \\mathcal{N}(0,\\sigma^2),\n$$\nwith a mean that is the sum of two physiologically interpretable components\n$$\n\\mu_t \\;=\\; \\alpha\\, x_t \\;+\\; \\beta\\, z_t.\n$$\nHere $x_t$ (in picoampere-second) is a known effective synaptic regressor derived from presynaptic activity convolved with a synaptic response kernel, and $z_t$ (in picoampere-second) is a known effective astrocytic regressor derived from astrocytic calcium activity convolved with a gliotransmission kernel. The parameters $\\alpha$ and $\\beta$ (both in per second, i.e., $\\text{s}^{-1}$) are nonnegative release-rate scale parameters that you must estimate by maximum likelihood. Assume $\\sigma^2$ is unknown but constant across time, and that $(\\varepsilon_t)$ are independent across $t$.\n\nYour task is to design and implement a maximum likelihood estimator of $\\alpha$ and $\\beta$ under the nonnegativity constraints $\\alpha \\ge 0$, $\\beta \\ge 0$, using the Gaussian likelihood implied by the model. The estimator should handle potential collinearity between $x_t$ and $z_t$ and should correctly return boundary values when one pathway is absent.\n\nYou will be given four independent test datasets. For each dataset $i \\in \\{1,2,3,4\\}$, you are provided with sequences $\\{x_t^{(i)}\\}_{t=1}^{T_i}$, $\\{z_t^{(i)}\\}_{t=1}^{T_i}$, and observed currents $\\{y_t^{(i)}\\}_{t=1}^{T_i}$. Estimate $(\\alpha^{(i)}, \\beta^{(i)})$ from each dataset by maximizing the Gaussian log-likelihood subject to $\\alpha \\ge 0$ and $\\beta \\ge 0$. Report your estimates for $\\alpha$ and $\\beta$ for each dataset in per second, rounded to four decimal places.\n\nFoundational modeling base:\n- Linear-response superposition of synaptic and astrocytic contributions to current: $\\mu_t = \\alpha x_t + \\beta z_t$.\n- Independent, identically distributed Gaussian measurement noise: $\\varepsilon_t \\sim \\mathcal{N}(0,\\sigma^2)$, with $\\sigma^2$ constant and unknown.\n\nDatasets:\n\nDataset 1 (length $T_1 = 20$):\n- $x^{(1)} = [3.0, 2.8, 2.6, 2.5, 2.3, 2.1, 1.9, 1.8, 1.7, 1.6, 1.5, 1.45, 1.4, 1.35, 1.3, 1.25, 1.2, 1.15, 1.1, 1.05]$ (picoampere-second)\n- $z^{(1)} = [0.5, 0.7, 0.9, 1.0, 1.1, 1.05, 1.0, 0.95, 0.9, 0.85, 0.8, 0.78, 0.76, 0.74, 0.72, 0.70, 0.68, 0.66, 0.64, 0.62]$ (picoampere-second)\n- $y^{(1)} = [39.2, 36.8, 36.8, 33.8, 33.6, 30.05, 28.0, 25.45, 25.5, 22.95, 22.3, 21.1, 21.0, 19.3, 19.4, 18.4, 18.3, 16.8, 16.5, 15.3]$ (picoampere)\n\nDataset 2 (length $T_2 = 15$):\n- $x^{(2)} = [0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 2.8, 3.0, 2.6, 2.2, 1.8, 1.4, 1.0, 0.7, 0.5]$ (picoampere-second)\n- $z^{(2)} = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]$ (picoampere-second)\n- $y^{(2)} = [0.2, 3.9, 8.3, 11.5, 16.6, 19.6, 22.5, 23.8, 21.1, 17.3, 14.6, 11.1, 8.1, 5.4, 4.0]$ (picoampere)\n\nDataset 3 (length $T_3 = 10$):\n- $x^{(3)} = [1.0, 1.1, 1.15, 1.2, 1.18, 1.16, 1.14, 1.12, 1.1, 1.08]$ (picoampere-second)\n- $z^{(3)} = [0.91, 0.97, 1.055, 1.07, 1.062, 1.054, 1.016, 1.008, 1.01, 0.952]$ (picoampere-second)\n- $y^{(3)} = [6.78, 7.29, 7.805, 7.98, 7.906, 7.822, 7.598, 7.514, 7.41, 7.206]$ (picoampere)\n\nDataset 4 (length $T_4 = 8$):\n- $x^{(4)} = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]$ (picoampere-second)\n- $z^{(4)} = [0.2, 0.5, 1.0, 1.2, 1.1, 0.9, 0.6, 0.3]$ (picoampere-second)\n- $y^{(4)} = [2.1, 4.8, 10.3, 11.5, 11.4, 8.7, 6.2, 2.9]$ (picoampere)\n\nScientific realism and units:\n- Report $\\alpha$ and $\\beta$ in per second ($\\text{s}^{-1}$).\n- The observed currents $y_t$ are in picoampere ($\\text{pA}$). The regressors $x_t$ and $z_t$ have units of picoampere-second so that $\\alpha x_t$ and $\\beta z_t$ yield picoampere.\n- The Gaussian noise variance $\\sigma^2$ is unknown and constant; it does not need to be reported.\n\nAlgorithmic requirement:\n- Use maximum likelihood under the Gaussian model with nonnegativity constraints $\\alpha \\ge 0$, $\\beta \\ge 0$. The resulting estimator is equivalent to solving a constrained least-squares problem.\n\nAngle units are irrelevant to this problem.\n\nYour program must read no input and must compute, for each dataset, the maximum likelihood estimates of $\\alpha$ and $\\beta$ in $\\text{s}^{-1}$, rounded to four decimal places. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is a two-element list $[\\alpha,\\beta]$ for the corresponding dataset in order $1$ through $4$. For example, the output format must be exactly like\n$$\n[\\,[\\alpha_1,\\beta_1],[\\alpha_2,\\beta_2],[\\alpha_3,\\beta_3],[\\alpha_4,\\beta_4]\\,],\n$$\nwith all values printed as decimal numbers rounded to four decimal places and no extra spaces.\n\nTest suite coverage:\n- Dataset $1$: general case with both pathways active and moderate noise.\n- Dataset $2$: boundary case with absent astrocytic pathway ($z_t \\equiv 0$), which should yield $\\beta$ at the boundary $0$.\n- Dataset $3$: near-collinearity between $x_t$ and $z_t$, testing numerical stability of the estimator.\n- Dataset $4$: boundary case with absent synaptic pathway ($x_t \\equiv 0$), which should yield $\\alpha$ at the boundary $0$.",
            "solution": "The problem statement has been evaluated and is determined to be valid. It is scientifically grounded, well-posed, objective, and contains all necessary information to derive a unique and meaningful solution.\n\nThe problem requires the estimation of parameters $\\alpha$ and $\\beta$ for the linear model\n$$\ny_t = \\alpha x_t + \\beta z_t + \\varepsilon_t\n$$\nwhere $\\varepsilon_t$ are independent and identically distributed random variables from a normal distribution with mean $0$ and unknown variance $\\sigma^2$, i.e., $\\varepsilon_t \\sim \\mathcal{N}(0, \\sigma^2)$. The estimation is to be performed using the method of maximum likelihood, subject to the nonnegativity constraints $\\alpha \\ge 0$ and $\\beta \\ge 0$.\n\nLet us formulate the problem in vector notation for a dataset of length $T$. Let $\\mathbf{y} = [y_1, y_2, \\dots, y_T]^T$ be the vector of observations, $\\mathbf{x} = [x_1, x_2, \\dots, x_T]^T$ and $\\mathbf{z} = [z_1, z_2, \\dots, z_T]^T$ be the vectors of regressors. We can define a design matrix $\\mathbf{X} = [\\mathbf{x}, \\mathbf{z}]$, which is a $T \\times 2$ matrix, and a parameter vector $\\boldsymbol{\\theta} = [\\alpha, \\beta]^T$. The model for the entire dataset can then be written as:\n$$\n\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\theta} + \\boldsymbol{\\varepsilon}\n$$\nwhere $\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2\\mathbf{I})$, with $\\mathbf{0}$ being a $T \\times 1$ zero vector and $\\mathbf{I}$ being the $T \\times T$ identity matrix.\n\nThe probability density function for a single observation $y_t$ given $\\alpha, \\beta,$ and $\\sigma^2$ is:\n$$\np(y_t | \\alpha, \\beta, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_t - (\\alpha x_t + \\beta z_t))^2}{2\\sigma^2}\\right)\n$$\nDue to the independence of the noise terms $\\varepsilon_t$, the likelihood function for the entire observation vector $\\mathbf{y}$ is the product of the individual probabilities:\n$$\nL(\\alpha, \\beta, \\sigma^2 | \\mathbf{y}, \\mathbf{X}) = \\prod_{t=1}^T p(y_t | \\alpha, \\beta, \\sigma^2) = \\left(\\frac{1}{2\\pi\\sigma^2}\\right)^{T/2} \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{t=1}^T (y_t - \\alpha x_t - \\beta z_t)^2\\right)\n$$\nThe log-likelihood function, $\\mathcal{L} = \\ln L$, is:\n$$\n\\mathcal{L}(\\alpha, \\beta, \\sigma^2) = -\\frac{T}{2}\\ln(2\\pi) - \\frac{T}{2}\\ln(\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{t=1}^T (y_t - \\alpha x_t - \\beta z_t)^2\n$$\nTo find the maximum likelihood estimates (MLE) of $\\alpha$ and $\\beta$, we need to maximize $\\mathcal{L}$ with respect to these parameters. Observing the structure of $\\mathcal{L}$, we can see that for any given value of $\\sigma^2 > 0$, maximizing $\\mathcal{L}$ with respect to $\\alpha$ and $\\beta$ is equivalent to minimizing the sum of squared errors (SSE) term:\n$$\n\\text{SSE}(\\alpha, \\beta) = \\sum_{t=1}^T (y_t - \\alpha x_t - \\beta z_t)^2 = ||\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\theta}||_2^2\n$$\nwhere $||\\cdot||_2^2$ denotes the squared Euclidean norm.\n\nThis equivalence demonstrates that for a linear model with additive Gaussian noise, maximum likelihood estimation is identical to ordinary least squares (OLS) estimation. The problem, however, imposes the constraints $\\alpha \\ge 0$ and $\\beta \\ge 0$. Therefore, the task reduces to solving the following constrained optimization problem:\n$$\n\\underset{\\alpha, \\beta}{\\text{minimize}} \\quad ||\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\theta}||_2^2 \\quad \\text{subject to} \\quad \\boldsymbol{\\theta} \\ge \\mathbf{0}\n$$\nThis is a classical problem in numerical optimization known as Non-Negative Least Squares (NNLS).\n\nThe NNLS problem does not have a simple closed-form solution like unconstrained OLS. It is typically solved using iterative active-set methods, such as the algorithm developed by Lawson and Hanson. This algorithm is robust and is implemented in standard scientific computing libraries. We will use the `nnls` function from the `scipy.optimize` library, which provides an efficient and numerically stable implementation of this algorithm.\n\nFor each of the four provided datasets, the procedure is as follows:\n1.  Construct the observation vector $\\mathbf{y}^{(i)}$ from the given $y_t^{(i)}$ values.\n2.  Construct the $T_i \\times 2$ design matrix $\\mathbf{X}^{(i)}$ by combining the regressor vectors $\\mathbf{x}^{(i)}$ and $\\mathbf{z}^{(i)}$ as its columns.\n3.  Solve the NNLS problem $\\min ||\\mathbf{y}^{(i)} - \\mathbf{X}^{(i)}\\boldsymbol{\\theta}^{(i)}||_2^2$ subject to $\\boldsymbol{\\theta}^{(i)} = [\\alpha^{(i)}, \\beta^{(i)}]^T \\ge \\mathbf{0}$ using `scipy.optimize.nnls`.\n4.  The solution vector provides the estimated parameters $(\\hat{\\alpha}^{(i)}, \\hat{\\beta}^{(i)})$.\n5.  The results are then rounded to four decimal places as required.\n\nThis approach naturally handles the specific test cases:\n- For Dataset 2, where $z_t \\equiv 0$, the second column of $\\mathbf{X}^{(2)}$ is zero. The NNLS algorithm will correctly find that any non-zero $\\beta$ would increase the residual sum of squares, thus forcing $\\hat{\\beta}=0$.\n- For Dataset 4, where $x_t \\equiv 0$, the first column of $\\mathbf{X}^{(4)}$ is zero, and by symmetry, the algorithm will correctly estimate $\\hat{\\alpha}=0$.\n- For Dataset 3, where the regressors are nearly collinear, the active-set method used by `nnls` is more numerically stable than computing the unconstrained OLS solution via the matrix inverse $(\\mathbf{X}^T\\mathbf{X})^{-1}$, which would be ill-conditioned.\n\nThe implementation will apply this procedure to each of the four datasets to obtain the final estimates.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import nnls\n\ndef solve():\n    \"\"\"\n    Solves for the maximum likelihood estimates of alpha and beta for four datasets\n    under nonnegativity constraints using a Non-Negative Least Squares (NNLS) solver.\n    \"\"\"\n\n    # Define the datasets provided in the problem statement.\n    datasets = [\n        {\n            \"x\": [3.0, 2.8, 2.6, 2.5, 2.3, 2.1, 1.9, 1.8, 1.7, 1.6, 1.5, 1.45, 1.4, 1.35, 1.3, 1.25, 1.2, 1.15, 1.1, 1.05],\n            \"z\": [0.5, 0.7, 0.9, 1.0, 1.1, 1.05, 1.0, 0.95, 0.9, 0.85, 0.8, 0.78, 0.76, 0.74, 0.72, 0.70, 0.68, 0.66, 0.64, 0.62],\n            \"y\": [39.2, 36.8, 36.8, 33.8, 33.6, 30.05, 28.0, 25.45, 25.5, 22.95, 22.3, 21.1, 21.0, 19.3, 19.4, 18.4, 18.3, 16.8, 16.5, 15.3]\n        },\n        {\n            \"x\": [0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 2.8, 3.0, 2.6, 2.2, 1.8, 1.4, 1.0, 0.7, 0.5],\n            \"z\": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n            \"y\": [0.2, 3.9, 8.3, 11.5, 16.6, 19.6, 22.5, 23.8, 21.1, 17.3, 14.6, 11.1, 8.1, 5.4, 4.0]\n        },\n        {\n            \"x\": [1.0, 1.1, 1.15, 1.2, 1.18, 1.16, 1.14, 1.12, 1.1, 1.08],\n            \"z\": [0.91, 0.97, 1.055, 1.07, 1.062, 1.054, 1.016, 1.008, 1.01, 0.952],\n            \"y\": [6.78, 7.29, 7.805, 7.98, 7.906, 7.822, 7.598, 7.514, 7.41, 7.206]\n        },\n        {\n            \"x\": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n            \"z\": [0.2, 0.5, 1.0, 1.2, 1.1, 0.9, 0.6, 0.3],\n            \"y\": [2.1, 4.8, 10.3, 11.5, 11.4, 8.7, 6.2, 2.9]\n        }\n    ]\n\n    results = []\n    for data in datasets:\n        # Convert lists to NumPy arrays\n        x_vec = np.array(data[\"x\"])\n        z_vec = np.array(data[\"z\"])\n        y_vec = np.array(data[\"y\"])\n\n        # Construct the design matrix A (denoted as X in the solution text)\n        A = np.stack([x_vec, z_vec], axis=1)\n        \n        # The vector b is the observation vector y\n        b = y_vec\n\n        # Solve the Non-Negative Least Squares problem: min ||Ax - b||_2 for x >= 0\n        # The solution 'x' corresponds to the parameter vector [alpha, beta]\n        params, _ = nnls(A, b)\n        \n        alpha_est = params[0]\n        beta_est = params[1]\n        \n        # Append the estimated, rounded parameters to the list of results\n        results.append([round(alpha_est, 4), round(beta_est, 4)])\n\n    # Format the final output string exactly as required, with no extra spaces.\n    formatted_results = [f\"[{res[0]:.4f},{res[1]:.4f}]\" for res in results]\n    final_output = f\"[{','.join(formatted_results)}]\"\n    \n    print(final_output)\n\nsolve()\n```"
        }
    ]
}