{
    "hands_on_practices": [
        {
            "introduction": "奖励调制STDP的一个核心洞见是，它与强化学习中的时序差分（TD）算法在功能上具有等价性。这项练习将通过一个简单的离散时间任务来具体展示这一联系，揭示一个受生物学启发的神经形态规则如何能够实现强大的学习算法，从而解决信用分配问题。",
            "id": "4057751",
            "problem": "考虑一个最小化的分幕式任务，其模型为一个双状态马尔可夫链，包含状态 $A$ 和 $B$ 以及一个终止吸收状态 $\\perp$。该分幕在时间 $t=0$ 时从状态 $A$ 开始，在 $t=1$ 时确定性地转移到状态 $B$，并在 $t=2$ 时于状态 $\\perp$ 终止。一个标量奖励 $R$ 仅在终止时给出；即，$r_{0}=0$，$r_{1}=R$，而 $r_{2}$ 未定义，因为分幕在 $t=2$ 进入 $\\perp$ 时结束。该任务用于训练一个单参数线性价值函数 $V(s;w)=w\\,x(s)$，其中 $w\\in\\mathbb{R}$ 是待调整的突触权重，$x(s)\\in\\mathbb{R}$ 是状态 $s$ 的一个固定特征。假设 $x(A)=x_{0}$ 和 $x(B)=x_{1}$ 是给定的常数。折扣因子为 $\\gamma\\in(0,1)$，资格迹参数为 $\\lambda\\in[0,1]$。学习率为 $\\eta0$。\n\n您将使用零阶时序差分（TD）学习（也称为时序差分 TD(0)）与资格迹来计算单次分幕的突触权重更新，然后验证其与一个奖励调控的三因子脉冲时间依赖可塑性（STDP）规则的等价性。使用以下基本定义：\n\n- 时刻 $t$ 的时序差分误差为 $\\delta_{t}=r_{t}+\\gamma\\,V(s_{t+1};w)-V(s_{t};w)$，约定 $V(\\perp;w)=0$。\n- 资格迹遵循 $e_{t}=\\gamma\\,\\lambda\\,e_{t-1}+x(s_{t})$，且 $e_{-1}=0$。\n\n在奖励调控的 STDP 的神经形态表述中，突触更新是一个三因子乘积，其中全局调制信号 $M_{t}$ 门控一个由突触前和突触后活动累积的局部资格 $e_{t}$；在本任务中，将 $M_{t}$ 等同于时序差分误差，即 $M_{t}\\equiv\\delta_{t}$，这与奖励预测误差信号一致。\n\n从这些定义和上述马尔可夫链结构出发，推导由带资格迹的 TD(0) 产生的单次分幕突触权重增量 $\\Delta w$，并明确证明，当 $M_{t}=\\delta_{t}$ 时，三因子奖励调控的 STDP 规则会产生相同的 $\\Delta w$。将您的最终答案表示为一个由符号 $\\eta$、$\\gamma$、$\\lambda$、$R$、$w$、$x_{0}$ 和 $x_{1}$ 组成的封闭形式解析表达式。无需进行数值评估，最终表达式也无需附加单位。最终答案必须是单个封闭形式的表达式；不要将其呈现为待解方程。",
            "solution": "在进行求解之前，对问题陈述进行验证。\n\n### 步骤 1：提取已知条件\n- **任务结构**：一个双状态马尔可夫链，包含状态 $A$ 和 $B$ 以及一个终止吸收状态 $\\perp$。\n- **分幕动态**：分幕在时间 $t=0$ 时从状态 $s_0=A$ 开始，在 $t=1$ 时转移到 $s_1=B$，并在 $t=2$ 时于 $s_2=\\perp$ 终止。\n- **奖励安排**：$r_0=0$，$r_1=R$。\n- **价值函数**：一个单参数线性价值函数 $V(s;w)=w\\,x(s)$，其中 $w \\in \\mathbb{R}$ 是权重，$x(s) \\in \\mathbb{R}$ 是状态特征。\n- **状态特征**：$x(A)=x_0$ 和 $x(B)=x_1$。\n- **学习参数**：折扣因子 $\\gamma \\in (0,1)$，资格迹参数 $\\lambda \\in [0,1]$，学习率 $\\eta  0$。\n- **时序差分（TD）误差**：$\\delta_{t}=r_{t}+\\gamma\\,V(s_{t+1};w)-V(s_{t};w)$，约定 $V(\\perp;w)=0$。\n- **资格迹**：$e_{t}=\\gamma\\,\\lambda\\,e_{t-1}+x(s_{t})$，且 $e_{-1}=0$。\n- **三因子规则表述**：突触更新被描述为全局调制信号 $M_t$ 和局部资格 $e_t$ 的乘积。\n- **等同关系**：调制信号被等同于 TD 误差，即 $M_t \\equiv \\delta_t$。\n- **目标**：使用带资格迹的 TD 学习推导单次分幕的权重增量 $\\Delta w$，并证明其与指定的三因子规则的等价性。结果必须是一个由 $\\eta, \\gamma, \\lambda, R, w, x_0, x_1$ 组成的封闭形式表达式。\n\n### 步骤 2：使用提取的已知条件进行验证\n该问题具有科学依据，是强化学习理论（TD($\\lambda$)）中的一个标准练习，并且是其在计算神经科学中作为奖励调控的突触可塑性模型的应用。所有术语都有正式定义，且各组成部分（马尔可夫链、线性价值函数、学习规则）在文献中都是标准的。该问题是适定的、自洽的、客观的，为求得唯一解提供了所有必要的参数和方程。定义中没有科学或事实上的不健全之处，没有矛盾，也没有歧义。该问题是一个可形式化且具有实质内容的数学推导。\n\n### 步骤 3：结论与行动\n问题被判定为有效。下面是一个完整的、附有推理的解法。\n\n### 突触权重更新的推导\n单个分幕中突触权重 $w$ 的总变化由每个时间步的更新总和给出。学习规则是带资格迹的时序差分学习，通常称为 TD($\\lambda$)。更新规则是：\n$$\n\\Delta w = \\eta \\sum_{t=0}^{T-1} \\delta_t e_t\n$$\n在此问题中，分幕包含从 $t=0$到 $t=1$ 的转移，并在 $t=2$ 终止。因此，求和的范围是 $t=0, 1$。\n$$\n\\Delta w = \\eta (\\delta_0 e_0 + \\delta_1 e_1)\n$$\n为了计算 $\\Delta w$，我们必须首先确定资格迹 $e_0$ 和 $e_1$ 以及 TD 误差 $\\delta_0$ 和 $\\delta_1$ 的值。\n\n**1. 资格迹 ($e_t$) 的计算**\n资格迹根据递推关系 $e_{t}=\\gamma\\,\\lambda\\,e_{t-1}+x(s_{t})$ 演化，初始条件为 $e_{-1}=0$。\n\n对于 $t=0$：\n状态为 $s_0=A$，因此特征为 $x(s_0)=x_0$。\n$$\ne_0 = \\gamma\\,\\lambda\\,e_{-1} + x(s_0) = \\gamma\\,\\lambda\\,(0) + x_0 = x_0\n$$\n\n对于 $t=1$：\n状态为 $s_1=B$，因此特征为 $x(s_1)=x_1$。\n$$\ne_1 = \\gamma\\,\\lambda\\,e_{0} + x(s_1) = \\gamma\\,\\lambda\\,x_0 + x_1\n$$\n\n**2. TD 误差 ($\\delta_t$) 的计算**\nTD 误差定义为 $\\delta_{t}=r_{t}+\\gamma\\,V(s_{t+1};w)-V(s_{t};w)$。价值函数为 $V(s;w)=w\\,x(s)$。\n\n对于 $t=0$：\n状态为 $s_0=A$ 和 $s_1=B$。奖励为 $r_0=0$。\n价值函数为 $V(s_0;w) = V(A;w) = w\\,x_0$ 和 $V(s_1;w) = V(B;w) = w\\,x_1$。\n$$\n\\delta_0 = r_0 + \\gamma\\,V(s_1;w) - V(s_0;w) = 0 + \\gamma\\,(w\\,x_1) - w\\,x_0 = w(\\gamma\\,x_1 - x_0)\n$$\n\n对于 $t=1$：\n状态为 $s_1=B$ 和终止状态 $s_2=\\perp$。奖励为 $r_1=R$。\n价值函数为 $V(s_1;w) = V(B;w) = w\\,x_1$，且根据约定，$V(s_2;w) = V(\\perp;w) = 0$。\n$$\n\\delta_1 = r_1 + \\gamma\\,V(s_2;w) - V(s_1;w) = R + \\gamma\\,(0) - w\\,x_1 = R - w\\,x_1\n$$\n\n**3. 总权重更新 ($\\Delta w$) 的计算**\n现在我们将 $e_0, e_1, \\delta_0, \\delta_1$ 的表达式代入 $\\Delta w$ 的方程中：\n$$\n\\Delta w = \\eta (\\delta_0 e_0 + \\delta_1 e_1)\n$$\n$$\n\\Delta w = \\eta \\left[ \\left( w(\\gamma\\,x_1 - x_0) \\right) (x_0) + \\left( R - w\\,x_1 \\right) (\\gamma\\,\\lambda\\,x_0 + x_1) \\right]\n$$\n我们展开括号内的项：\n$$\n\\Delta w = \\eta \\left[ (w\\,\\gamma\\,x_0\\,x_1 - w\\,x_0^2) + (R(\\gamma\\,\\lambda\\,x_0 + x_1) - w\\,x_1(\\gamma\\,\\lambda\\,x_0 + x_1)) \\right]\n$$\n$$\n\\Delta w = \\eta \\left[ w\\,\\gamma\\,x_0\\,x_1 - w\\,x_0^2 + R\\,\\gamma\\,\\lambda\\,x_0 + R\\,x_1 - w\\,\\gamma\\,\\lambda\\,x_0\\,x_1 - w\\,x_1^2 \\right]\n$$\n为了得到最终的封闭形式表达式，我们将涉及奖励 $R$ 的项和涉及当前权重 $w$ 的项分组：\n$$\n\\Delta w = \\eta \\left[ (R\\,\\gamma\\,\\lambda\\,x_0 + R\\,x_1) + (w\\,\\gamma\\,x_0\\,x_1 - w\\,x_0^2 - w\\,\\gamma\\,\\lambda\\,x_0\\,x_1 - w\\,x_1^2) \\right]\n$$\n将 $R$ 和 $w$ 提出公因子：\n$$\n\\Delta w = \\eta \\left[ R(\\gamma\\,\\lambda\\,x_0 + x_1) + w(\\gamma\\,x_0\\,x_1 - x_0^2 - \\gamma\\,\\lambda\\,x_0\\,x_1 - x_1^2) \\right]\n$$\n乘以 $w$ 的项可以为了清晰而重新整理：\n$$\n\\gamma\\,x_0\\,x_1 - x_0^2 - \\gamma\\,\\lambda\\,x_0\\,x_1 - x_1^2 = -(x_0^2 + x_1^2 - \\gamma\\,x_0\\,x_1 + \\gamma\\,\\lambda\\,x_0\\,x_1) = -(x_0^2 + x_1^2 - \\gamma(1-\\lambda)x_0\\,x_1)\n$$\n将此代回，得到最终表达式：\n$$\n\\Delta w = \\eta \\left[ R(\\gamma\\,\\lambda\\,x_0 + x_1) - w(x_0^2 + x_1^2 - \\gamma(1-\\lambda)x_0\\,x_1) \\right]\n$$\n\n**4. 与三因子规则的等价性**\n问题将奖励调控的三因子 STDP 规则定义为全局调制信号 $M_t$ 门控局部资格 $e_t$ 的乘积。总更新是这些乘积随时间累加的总和，并由学习率 $\\eta$ 缩放：\n$$\n\\Delta w = \\eta \\sum_t M_t e_t\n$$\n问题明确给出了等同关系 $M_t \\equiv \\delta_t$。将其代入三因子规则，得到：\n$$\n\\Delta w = \\eta \\sum_t \\delta_t e_t\n$$\n这正是作为推导起点的 TD($\\lambda$) 更新规则的定义。因此，等价性由问题陈述中提供的定义确立。根据此等同关系，上述为 TD($\\lambda$) 更新所做的计算，同时也是为指定的三因子 STDP 规则所做的计算。问题简化为在 TD($\\lambda$) 框架下推导单次分幕的权重更新，此任务已经完成。",
            "answer": "$$\n\\boxed{\\eta \\left[ R(\\gamma\\lambda x_0 + x_1) - w(x_0^2 + x_1^2 - \\gamma(1-\\lambda)x_0 x_1) \\right]}\n$$"
        },
        {
            "introduction": "尽管奖励调制STDP可以驱动学习，但其学习效率至关重要，尤其是在处理随机奖励信号时。这项练习旨在解决奖励信号高方差的问题，通过引入“基线”这一强化学习中的常用技巧。你将推导出能够最小化更新信号方差的最优基线，这是构建稳健学习系统的关键一步。",
            "id": "4057771",
            "problem": "在奖励调制的脉冲时间依赖可塑性（STDP）规则中，单个突触根据 $\\Delta w = \\eta\\, e\\,(R - b)$ 更新其权重，其中 $\\eta$ 是一个固定的学习率，$e$ 是一个二元资格标记，表示在预设窗口内是否存在一个因果性的突触前-后脉冲时间事件（因此 $e \\in \\{0,1\\}$ 且 $e^{2}=e$），$R$ 是事件发生后给出的一个有界标量奖励，$b$ 是一个恒定的奖励基线。在多次试验中，$(e,R)$ 是来自一个具有有限二阶矩的平稳联合分布的独立同分布样本，且 $\\mathbb{E}[e] \\neq 0$。基线 $b$ 被约束为一个不依赖于试验结果的常数。\n\n从方差和期望的定义出发，在不假设联合分布具有超出所述条件的任何附加结构的情况下，推导出使方差 $\\mathrm{Var}\\!\\big(e\\,(R - b)\\big)$ 最小化的恒定基线 $b^{*}$ 的值。将你的最终答案表示为一个仅包含 $e$ 和 $R$ 的期望的闭式符号表达式。\n\n然后，简要论证如何使用一种不需要存储整个历史记录的稳定递归方案，从数据中在线估计 $b^{*}$，并说明当 $\\mathbb{E}[e]$ 很小时如何防止除以零的情况。\n\n将 $b^{*}$ 的最终答案以单个解析表达式的形式给出。不需要数值近似，也不需要单位。",
            "solution": "该问题要求推导出使突触权重更新项的方差 $\\mathrm{Var}\\!\\big(e\\,(R - b)\\big)$ 最小化的恒定奖励基线 $b$。我们已知学习率 $\\eta$ 是一个常数，因此最小化 $\\mathrm{Var}\\!\\big(\\eta\\,e\\,(R - b)\\big)$ 等价于最小化 $\\mathrm{Var}\\!\\big(e\\,(R - b)\\big)$，因为 $\\mathrm{Var}(cX) = c^2\\mathrm{Var}(X)$。\n\n设更新项的随机变量为 $Y(b) = e(R - b)$。我们希望找到使 $V(b) = \\mathrm{Var}(Y(b))$ 最小化的常数 $b$ 的值，记为 $b^*$。\n我们可以将 $Y(b)$ 写为 $eR - be$。$eR$ 项是一个随机变量，$be$ 项是一个常数 $b$ 乘以随机变量 $e$。我们可以使用随机变量线性组合的方差性质。对于随机变量 $X_1$ 和 $X_2$ 以及常数 $c$，有 $\\mathrm{Var}(X_1 - cX_2) = \\mathrm{Var}(X_1) - 2c\\mathrm{Cov}(X_1, X_2) + c^2\\mathrm{Var}(X_2)$。\n\n我们令 $X_1 = eR$，$X_2 = e$，常数为 $b$。我们想要最小化的方差是：\n$$V(b) = \\mathrm{Var}(eR - be) = \\mathrm{Var}(eR) - 2b\\mathrm{Cov}(eR, e) + b^2\\mathrm{Var}(e)$$\n这个 $V(b)$ 的表达式是关于 $b$ 的二次函数。为了找到使 $V(b)$ 最小化的 $b$ 的值，我们可以对 $b$ 求导并令其为零。\n$$\\frac{d V(b)}{db} = -2\\mathrm{Cov}(eR, e) + 2b\\mathrm{Var}(e)$$\n将导数设为零以求最小值：\n$$ -2\\mathrm{Cov}(eR, e) + 2b^*\\mathrm{Var}(e) = 0 $$\n$$ 2b^*\\mathrm{Var}(e) = 2\\mathrm{Cov}(eR, e) $$\n在 $\\mathrm{Var}(e) \\neq 0$ 的条件下，我们可以解出 $b^*$：\n$$ b^* = \\frac{\\mathrm{Cov}(eR, e)}{\\mathrm{Var}(e)} $$\n二阶导数是 $\\frac{d^2 V(b)}{db^2} = 2\\mathrm{Var}(e)$。由于 $e$ 是一个二元随机变量，$e \\in \\{0, 1\\}$，其方差为 $\\mathrm{Var}(e) = \\mathbb{E}[e](1 - \\mathbb{E}[e])$。问题陈述 $\\mathbb{E}[e] \\neq 0$。如果 $\\mathbb{E}[e]=1$，$e$ 就是一个常数（几乎必然为 $e=1$），所以 $\\mathrm{Var}(e)=0$。在这种情况下，方差 $V(b) = \\mathrm{Var}(R-b) = \\mathrm{Var}(R)$，不依赖于 $b$，任何 $b$ 都是最小值点。题干中的“该值”暗示了唯一解，所以我们必须假设 $\\mathbb{E}[e] \\in (0, 1)$，这使得 $\\mathrm{Var}(e)  0$。因此，二阶导数为正，我们求得的 $b^*$ 对应一个唯一的最小值。\n\n现在，我们必须用 $e$ 和 $R$ 的期望来表示 $b^*$。我们使用方差和协方差的定义。\n$e$ 的方差是：\n$$ \\mathrm{Var}(e) = \\mathbb{E}[e^2] - (\\mathbb{E}[e])^2 $$\n由于 $e$ 是一个二元标记，满足 $e \\in \\{0, 1\\}$，我们有 $e^2 = e$。因此，\n$$ \\mathrm{Var}(e) = \\mathbb{E}[e] - (\\mathbb{E}[e])^2 = \\mathbb{E}[e](1 - \\mathbb{E}[e]) $$\n$eR$ 和 $e$ 之间的协方差是：\n$$ \\mathrm{Cov}(eR, e) = \\mathbb{E}[(eR)e] - \\mathbb{E}[eR]\\mathbb{E}[e] $$\n再次使用 $e^2 = e$，这可以简化为：\n$$ \\mathrm{Cov}(eR, e) = \\mathbb{E}[e^2R] - \\mathbb{E}[eR]\\mathbb{E}[e] = \\mathbb{E}[eR] - \\mathbb{E}[eR]\\mathbb{E}[e] = \\mathbb{E}[eR](1 - \\mathbb{E}[e]) $$\n将这些表达式代回关于 $b^*$ 的方程中：\n$$ b^* = \\frac{\\mathbb{E}[eR](1 - \\mathbb{E}[e])}{\\mathbb{E}[e](1 - \\mathbb{E}[e])} $$\n因为我们已经确定 $\\mathbb{E}[e] \\in (0, 1)$，所以 $(1 - \\mathbb{E}[e])$ 项不为零，我们可以从分子和分母中消去它。这就得到了最优基线的最终表达式：\n$$ b^* = \\frac{\\mathbb{E}[eR]}{\\mathbb{E}[e]} $$\n这个表达式有一个直观的解释，即在因果性脉冲时间事件发生的条件下，奖励 $R$ 的条件期望，即 $b^* = \\mathbb{E}[R|e=1]$。\n\n对于问题的第二部分，我们被要求论证如何在线估计 $b^*$。最优基线 $b^*$ 是两个期望的比值，即 $\\mathbb{E}[eR]$ 和 $\\mathbb{E}[e]$。给定来自试验 $t=1, 2, \\dots$ 的数据样本流 $(e_t, R_t)$，我们可以使用指数加权移动平均（EWMA）来递归地估计每个期望。设 $\\hat{\\mu}_{eR, t}$ 和 $\\hat{\\mu}_{e, t}$ 为在试验 $t$ 时的估计值，$\\alpha$ 为一个小的学习率：\n$$ \\hat{\\mu}_{eR, t} = \\hat{\\mu}_{eR, t-1} + \\alpha(e_t R_t - \\hat{\\mu}_{eR, t-1}) $$\n$$ \\hat{\\mu}_{e, t} = \\hat{\\mu}_{e, t-1} + \\alpha(e_t - \\hat{\\mu}_{e, t-1}) $$\n然后，基线被估计为比值 $\\hat{b}_t = \\frac{\\hat{\\mu}_{eR, t}}{\\hat{\\mu}_{e, t}}$。这个方案是递归的、稳定的，并且不需要存储数据历史。\n\n为了防止除以零或除以一个非常小的数（当 $\\mathbb{E}[e]$ 很小时，尤其是在估计的早期阶段，这种情况可能发生），我们可以通过加上一个小的正常数 $\\epsilon$ 来对分母进行正则化：$\\hat{b}_t = \\frac{\\hat{\\mu}_{eR, t}}{\\hat{\\mu}_{e, t} + \\epsilon}$。\n另外，一个更巧妙的在线方案源于注意到，推导出的 $b^*$ 也是确保平均突触更新为零的值：$\\mathbb{E}[\\Delta w] \\propto \\mathbb{E}[e(R-b)] = 0$。这为 $b^*$ 本身启发了一个直接的随机近似算法：\n$$ \\hat{b}_t = \\hat{b}_{t-1} + \\alpha_b \\, e_t (R_t - \\hat{b}_{t-1}) $$\n其中 $\\alpha_b$ 是基线的学习率。此更新的不动点发生在 $\\mathbb{E}[e(R-b)]=0$ 时，其解为 $b = \\frac{\\mathbb{E}[eR]}{\\mathbb{E}[e]} = b^*$。这个更新法则是递归的，内在地避免了除法问题，并且即使 $\\mathbb{E}[e]$ 很小也能保持数值稳定，尽管在这种情况下收敛会更慢。",
            "answer": "$$\\boxed{\\frac{\\mathbb{E}[eR]}{\\mathbb{E}[e]}}$$"
        },
        {
            "introduction": "现在，让我们将理论付诸实践，构建一个完整的计算模型。这项练习要求你搭建一个脉冲神经网络，该网络利用奖励调制STDP来学习一个简单的决策任务。这是一个将前面练习中的概念（特别是基线扣除的作用）应用于实际的机会，让你能够亲眼观察理论概念如何转化为学习智能体的性能和收敛速度。",
            "id": "4057793",
            "problem": "考虑一个由奖励调制的脉冲时间依赖可塑性 (STDP) 控制的双臂赌博机式脉冲网络。有 $N_{\\mathrm{pre}}$ 个独立的泊松输入神经元，为两个相互竞争的输出（动作）神经元提供输入。在每次试验中，输入神经元发放脉冲，动作神经元最多产生一个脉冲；最早产生脉冲的动作被选择。获得的标量奖励通过一个基线扣除因子来调制突触可塑性。\n\n使用以下基本原理和定义来构建和分析其动力学：\n\n- 脉冲时间依赖可塑性 (STDP)：对于一个在时间 $t_{\\mathrm{pre}}$ 发生的突触前脉冲和一个在时间 $t_{\\mathrm{post}}$ 发生的突触后脉冲，定义 STDP 时间窗函数 $F(\\Delta t)$，其中 $\\Delta t = t_{\\mathrm{post}} - t_{\\mathrm{pre}}$，如下：\n$$\nF(\\Delta t) = \n\\begin{cases}\nA_{+} \\exp\\left(-\\frac{\\Delta t}{\\tau_{+}}\\right),  \\Delta t  0 \\\\\n- A_{-} \\exp\\left(\\frac{\\Delta t}{\\tau_{-}}\\right),  \\Delta t  0\n\\end{cases}\n$$\n其中 $A_{+}  0$，$A_{-}  0$，$ \\tau_{+}  0$，$ \\tau_{-}  0$。\n\n- 奖励调制的可塑性：令 $R \\in \\{0,1\\}$ 表示一次试验中所选动作的奖励结果，令 $b$ 表示一个旨在减少方差的基线估计。从突触前神经元 $i$ 到动作神经元 $j$ 的突触权重 $w_{ji}$ 的更新定义为：\n$$\n\\Delta w_{ji} = \\eta \\, (R - b) \\, e_{ji},\n$$\n其中 $\\eta  0$ 是学习率，$e_{ji}$ 是由脉冲对构建的资格迹，\n$$\ne_{ji} = \\sum_{t_{\\mathrm{pre}} \\in \\mathcal{S}_i} F(t_{\\mathrm{post}} - t_{\\mathrm{pre}}),\n$$\n其中 $\\mathcal{S}_i$ 是该次试验中来自神经元 $i$ 的突触前脉冲时间集合，$t_{\\mathrm{post}}$ 是所选动作神经元的突触后脉冲时间。如果在试验期间没有发生突触后脉冲，则取 $e_{ji} = 0$。\n\n- 通过竞争风险进行动作选择：对于每个动作神经元 $j \\in \\{0,1\\}$，将其风险率 $\\lambda_j$ 定义为总突触前驱动的增函数，\n$$\n\\lambda_j = \\alpha \\max\\!\\left( \\sum_{i=1}^{N_{\\mathrm{pre}}} w_{ji} x_i, \\, \\varepsilon \\right),\n$$\n其中 $\\alpha  0$ 是一个增益，$\\varepsilon  0$ 确保风险率为严格正值，$x_i$ 是突触前神经元 $i$ 在该次试验中发放的脉冲数量。从速率为 $\\lambda_j$ 的指数分布中抽取一个候选脉冲时间 $t_j$。所选的动作是具有不超过试验时长 $T$ 的最小 $t_j$ 的那一个。如果两个候选时间都不在 $[0,T]$ 范围内，则按与 $\\lambda_j$ 成正比的概率进行随机选择，并且如果没有发生突触后脉冲，则设 $e_{ji} = 0$。\n\n- 赌博机环境：动作 $j=0$ （最优）以概率 $p_{\\mathrm{opt}}$ 返回 $R=1$，否则返回 $R=0$。动作 $j=1$ 以概率 $p_{\\mathrm{non}}$ 返回 $R=1$，否则返回 $R=0$，其中 $p_{\\mathrm{opt}}  p_{\\mathrm{non}}$。在选定动作的条件下，各次试验的奖励是独立的。\n\n- 基线扣除：每次试验的基线 $b$ 定义为 $b = \\mu + \\xi$，其中 $\\mu$ 是通过指数平均更新的期望奖励的移动估计，\n$$\n\\mu \\leftarrow (1 - \\gamma) \\mu + \\gamma R,\n$$\n其中 $0  \\gamma  1$，$\\xi$ 是独立于 $R$ 和脉冲时间的零均值高斯噪声，其方差为 $\\sigma_b^2$。量 $\\sigma_b^2$ 参数化了基线方差。\n\n您必须为每个基线方差参数模拟 $M$ 次独立的学习试验，并测量学习时间，即选择最优动作的移动比例（在最近 $W$ 次试验上计算）首次超过阈值 $\\theta$ 所需的试验次数。如果在 $K$ 次试验的上限内未达到该阈值，则报告 $K$。\n\n使用的模拟参数如下：\n- 突触前神经元数量：$N_{\\mathrm{pre}} = 30$。\n- 试验时长：$T = 0.2$。\n- 每个神经元的突触前发放率：$r_{\\mathrm{in}} = 25$。\n- STDP 参数：$A_{+} = 1.0$, $A_{-} = 0.5$, $\\tau_{+} = 0.02$, $\\tau_{-} = 0.02$。\n- 风险增益和下限：$\\alpha = 30.0$, $\\varepsilon = 10^{-12}$。\n- 学习率和衰减：$\\eta = 2 \\times 10^{-4}$，并实现一个小的权重衰减，每次试验为 $- \\lambda_w w_{ji}$，其中 $\\lambda_w = 10^{-4}$。\n- 奖励概率：$p_{\\mathrm{opt}} = 0.8$, $p_{\\mathrm{non}} = 0.2$。\n- 基线平均率：$\\gamma = 0.01$。\n- 滑动窗口和阈值：$W = 100$, $\\theta = 0.9$。\n- 权重边界：用小的随机正值初始化 $w_{ji}$，并在每次更新后将所有权重裁剪到 $[0, 0.05]$ 范围内。\n- 试验上限：$K = 2000$。\n\n测试套件：\n- 要测试的基线方差值：$\\sigma_b^2 \\in \\{0.0, 0.0025, 0.04, 0.25\\}$。\n\n对于测试套件中的每个 $\\sigma_b^2$ 值，模拟学习过程并计算学习时间（以整数试验次数表示）。您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表（例如，“[n1,n2,n3,n4]”），其中每个 $n_i$ 是对应于第 $i$ 个基线方差测试用例的学习时间。不允许有其他输出。",
            "solution": "该问题要求模拟一个由脉冲神经网络执行的双动作强化学习任务。网络的突触权重通过奖励调制的脉冲时间依赖可塑性 (R-STDP) 规则进行更新。目标是确定在调制性基线信号具有不同方差水平时的学习时间。该问题具有科学依据，提法明确，并包含构建模拟所需的所有信息。\n\n模拟的结构是一系列试验，对每个指定的基线方差 $\\sigma_b^2$ 重复进行。对于每个 $\\sigma_b^2$ 值，系统被初始化，然后演化最多 $K=2000$ 次试验。为确保公平比较，在每次模拟运行开始时，随机数生成器都会重置为相同的状态，从而隔离 $\\sigma_b^2$ 的影响。\n\n模拟中的单次试验过程如下：\n\n1.  **突触前脉冲生成**：对于 $N_{\\mathrm{pre}}=30$ 个输入神经元中的每一个，在时长为 $T=0.2$ s 的试验中将要发放的脉冲数 $x_i$ 从均值为 $\\lambda = r_{\\mathrm{in}} T = 25 \\times 0.2 = 5$ 的泊松分布中抽取。这些 $x_i$ 个脉冲的时间（由集合 $\\mathcal{S}_i$ 表示）从区间 $[0, T]$ 上的均匀分布中抽取。\n\n2.  **动作选择**：两个输出神经元（索引为 $j \\in \\{0, 1\\}$）竞争首先发放脉冲。每个神经元 $j$ 的瞬时发放概率由一个风险率 $\\lambda_j$ 控制。该速率是其接收到的总加权输入驱动的函数：\n    $$\n    \\lambda_j = \\alpha \\max\\left( \\sum_{i=1}^{N_{\\mathrm{pre}}} w_{ji} x_i, \\, \\varepsilon \\right)\n    $$\n    其中 $w_{ji}$ 是从输入神经元 $i$ 到输出神经元 $j$ 的突触权重，$\\alpha=30.0$ 是一个增益因子，$\\varepsilon=10^{-12}$ 是一个小的正常数，以防止速率为零。然后，为每个神经元从速率为 $\\lambda_j$ 的指数分布中抽取一个候选脉冲时间 $t_j$。获胜的动作 $j_{\\text{chosen}}$ 对应于具有最小候选时间的神经元，$t_{\\text{post}} = \\min(t_0, t_1)$。\n    如果 $t_{\\text{post}}  T$，则在试验期间内没有脉冲发生。在这种情况下，会以与风险率成正比的概率随机选择一个动作，$P(\\text{选择 } j) = \\lambda_j / (\\lambda_0 + \\lambda_1)$。\n\n3.  **资格迹计算**：资格迹 $e_{ji}$ 量化了突触前脉冲和突触后脉冲之间的因果联系，作为后续权重更新的记忆。它仅对获胜神经元的突触非零，并且仅当该神经元在试验时长内（$t_{\\text{post}} \\le T$）发放了脉冲时才非零。对于获胜的神经元 $j_{\\text{chosen}}$，每个输入 $i$ 的资格迹为：\n    $$\n    e_{j_{\\text{chosen}}, i} = \\sum_{t_{\\mathrm{pre}} \\in \\mathcal{S}_i} F(t_{\\text{post}} - t_{\\mathrm{pre}})\n    $$\n    其中 $F(\\Delta t)$ 是用参数 $A_{+}=1.0$，$A_{-}=0.5$ 和 $\\tau_{+}=\\tau_{-}=0.02$ 定义的 STDP 时间窗函数。对于失败的神经元，或者如果没有脉冲发生，所有资格迹均为 $0$。\n\n4.  **奖励与学习信号**：采取动作 $j_{\\text{chosen}}$ 后，环境提供一个随机奖励 $R \\in \\{0, 1\\}$。动作 $j=0$ 是最优的，以概率 $p_{\\mathrm{opt}}=0.8$ 产生 $R=1$。动作 $j=1$ 是次优的，概率为 $p_{\\mathrm{non}}=0.2$。通过将奖励 $R$ 与基线 $b = \\mu + \\xi$ 进行比较来形成学习信号。这里，$\\mu$ 是过去奖励的指数移动平均，$\\xi$ 是方差为 $\\sigma_b^2$ 的零均值高斯噪声。方差 $\\sigma_b^2$ 是我们研究的参数。\n\n5.  **权重与状态更新**：突触权重根据奖励预测误差 $(R-b)$ 和资格迹 $e_{ji}$ 进行更新：\n    $$\n    w_{ji}(t+1) = (1 - \\lambda_w) w_{ji}(t) + \\eta (R - b) e_{ji}(t)\n    $$\n    此更新结合了 R-STDP 规则（学习率 $\\eta = 2 \\times 10^{-4}$）和一个权重衰减项（$\\lambda_w = 10^{-4}$）。更新后，所有权重被裁剪到 $[0, 0.05]$ 范围内。移动平均奖励 $\\mu$ 也被更新：$\\mu \\leftarrow (1-\\gamma)\\mu + \\gamma R$，其中 $\\gamma=0.01$。\n\n6.  **性能评估**：通过计算在最近 $W=100$ 次试验的滑动窗口内选择最优动作（$j=0$）的比例来监控学习过程。对于给定的 $\\sigma_b^2$，当该比例首次超过阈值 $\\theta=0.9$ 时，模拟终止。发生这种情况时的试验次数即为学习时间。如果在 $K=2000$ 次试验内未达到阈值，则学习时间记录为 $K$。\n\n整个过程在一个 Python 脚本中实现。对测试套件 $\\{0.0, 0.0025, 0.04, 0.25\\}$ 中的每个 $\\sigma_b^2$ 值运行模拟，并将得到的整数学习时间收集并格式化为最终输出。",
            "answer": "```python\nimport numpy as np\n\ndef run_simulation(sigma_b_sq, params, rng):\n    \"\"\"\n    Runs a single full simulation for a given baseline variance.\n\n    Args:\n        sigma_b_sq (float): The variance of the baseline noise.\n        params (tuple): A tuple containing all simulation parameters.\n        rng (np.random.Generator): The random number generator instance.\n\n    Returns:\n        int: The learning time in number of trials.\n    \"\"\"\n    # Unpack parameters\n    (N_pre, T, r_in, A_plus, A_minus, tau_plus, tau_minus,\n     alpha, epsilon, eta, lambda_w, p_opt, p_non,\n     gamma, W, theta, K, w_max) = params\n\n    sigma_b = np.sqrt(sigma_b_sq)\n\n    # Initialization\n    w = rng.uniform(0, 0.01, size=(2, N_pre))\n    mu = (p_opt + p_non) / 2.0\n    choice_history = np.zeros(W, dtype=int)\n\n    for trial in range(1, K + 1):\n        # 1. Generate presynaptic spikes\n        spike_counts = rng.poisson(r_in * T, size=N_pre)\n        presynaptic_spikes = [sorted(rng.uniform(0, T, size=n)) for n in spike_counts]\n\n        # 2. Action selection\n        drive = w @ spike_counts\n        lambdas = alpha * np.maximum(drive, epsilon)\n        candidate_times = rng.exponential(1.0 / lambdas)\n        t_post = np.min(candidate_times)\n        chosen_action = np.argmin(candidate_times)\n\n        eligibility_traces = np.zeros_like(w)\n\n        if t_post = T:\n            # 3. Calculate eligibility traces for the winning neuron\n            e_ji = np.zeros(N_pre)\n            for i in range(N_pre):\n                temp_e = 0.0\n                for t_pre in presynaptic_spikes[i]:\n                    delta_t = t_post - t_pre\n                    if delta_t  0:\n                        temp_e += A_plus * np.exp(-delta_t / tau_plus)\n                    elif delta_t  0:\n                        temp_e += -A_minus * np.exp(delta_t / tau_minus)\n                e_ji[i] = temp_e\n            eligibility_traces[chosen_action, :] = e_ji\n        else:\n            # No spike occurred: stochastic choice, eligibility remains zero\n            probs = lambdas / (np.sum(lambdas) + 1e-20)\n            chosen_action = rng.choice([0, 1], p=probs)\n\n        # 4. Determine Reward and Baseline\n        reward_prob = p_opt if chosen_action == 0 else p_non\n        R = 1 if rng.random()  reward_prob else 0\n        xi = rng.normal(0, sigma_b)\n        b = mu + xi\n\n        # 5. Update Weights\n        w = (1 - lambda_w) * w + eta * (R - b) * eligibility_traces\n        w = np.clip(w, 0, w_max)\n\n        # 6. Update Mean Reward\n        mu = (1 - gamma) * mu + gamma * R\n\n        # 7. Check Learning Criterion\n        choice_history[(trial - 1) % W] = 1 if chosen_action == 0 else 0\n        if trial = W:\n            performance = np.mean(choice_history)\n            if performance  theta:\n                return trial\n\n    return K\n\ndef solve():\n    \"\"\"\n    Main function to set up and run the simulations for the test suite.\n    \"\"\"\n    # Simulation Parameters\n    params = (\n        30,      # N_pre: Number of presynaptic neurons\n        0.2,     # T: Trial duration\n        25.0,    # r_in: Presynaptic rate per neuron\n        1.0,     # A_plus: STDP parameter\n        0.5,     # A_minus: STDP parameter\n        0.02,    # tau_plus: STDP parameter\n        0.02,    # tau_minus: STDP parameter\n        30.0,    # alpha: Hazard gain\n        1e-12,   # epsilon: Hazard floor\n        2e-4,    # eta: Learning rate\n        1e-4,    # lambda_w: Weight decay\n        0.8,     # p_opt: Reward probability for optimal action\n        0.2,     # p_non: Reward probability for non-optimal action\n        0.01,    # gamma: Baseline averaging rate\n        100,     # W: Sliding window size\n        0.9,     # theta: Performance threshold\n        2000,    # K: Trial cap\n        0.05     # w_max: Maximum weight\n    )\n\n    # Test Suite\n    test_cases = [0.0, 0.0025, 0.04, 0.25]\n    results = []\n\n    # Using the same seed for each run ensures that the only difference\n    # between simulations is the parameter being tested (sigma_b^2),\n    # allowing for a fair comparison.\n    seed = 42\n    \n    for sigma_b_sq in test_cases:\n        rng = np.random.default_rng(seed=seed)\n        learning_time = run_simulation(sigma_b_sq, params, rng)\n        results.append(learning_time)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}