{
    "hands_on_practices": [
        {
            "introduction": "To understand how reward can shape synaptic plasticity, we must first grasp the baseline dynamics of Spike-Timing-Dependent Plasticity (STDP) itself. This foundational exercise asks you to derive the expected long-term drift of a synaptic weight when subjected to uncorrelated presynaptic and postsynaptic spike trains . By performing this calculation, you will see how the microscopic details of the STDP time window translate into a macroscopic rate of change, revealing the intrinsic tendencies of a synapse to strengthen or weaken based on firing statistics alone.",
            "id": "4057782",
            "problem": "Consider a single synapse undergoing reward-modulated Spike-Timing-Dependent Plasticity (STDP). The presynaptic and postsynaptic neurons emit independent, homogeneous Poisson spike trains with rates $\\lambda_{\\text{pre}}$ and $\\lambda_{\\text{post}}$, respectively. Plasticity is all-to-all and pair-based: each ordered pre-post spike pair $(t_{\\text{pre}}, t_{\\text{post}})$ contributes an eligibility increment specified by an STDP window $F(\\Delta t)$, where $\\Delta t = t_{\\text{post}} - t_{\\text{pre}}$, and $F$ is absolutely integrable. The plasticity is reward-modulated by a tonic, stationary scalar reward signal that can be treated as a constant $R_{0}$ on the timescale of synaptic changes. The learning rate is $\\eta$. Assume no additional constraints on the weight dynamics (no hard bounds and no explicit weight dependence).\n\nStarting from the definitions of homogeneous Poisson processes and pair-based STDP, and using only fundamental properties of independent Poisson processes and linearity of expectation, derive the expected synaptic weight change per unit time in the long-time limit in terms of $\\lambda_{\\text{pre}}$, $\\lambda_{\\text{post}}$, $\\eta$, $R_{0}$, and $F$. Then, state the condition under which the synaptic weight diverges to $+\\infty$ (and to $-\\infty$) in the absence of stabilizing mechanisms. Your final numerical or analytical answer must be the expected synaptic weight change per unit time; do not include any units in the final answer box.",
            "solution": "The problem requires the derivation of the expected rate of synaptic weight change, $\\frac{d\\langle w \\rangle}{dt}$, for a single synapse under a specific reward-modulated Spike-Timing-Dependent Plasticity (STDP) rule. The core assumptions are that the presynaptic and postsynaptic neurons fire as independent, homogeneous Poisson processes, and the plasticity rule is pair-based.\n\nLet the presynaptic and postsynaptic spike trains be represented by sets of spike times $\\{t_i^{\\text{pre}}\\}$ and $\\{t_j^{\\text{post}}\\}$, respectively. The corresponding firing rates are constant and given by $\\lambda_{\\text{pre}}$ and $\\lambda_{\\text{post}}$. The plasticity rule states that for each ordered pair of a presynaptic spike at time $t_{\\text{pre}}$ and a postsynaptic spike at time $t_{\\text{post}}$, the synaptic weight $w$ is updated by an amount $\\Delta w$. This update is given by:\n$$ \\Delta w = \\eta R_{0} F(\\Delta t) $$\nwhere $\\Delta t = t_{\\text{post}} - t_{\\text{pre}}$ is the time difference between the spikes, $\\eta$ is a constant learning rate, $R_{0}$ is a constant scalar reward signal, and $F(\\Delta t)$ is the STDP window function which determines the update based on the spike timing.\n\nWe aim to calculate the expected rate of change of the weight, $\\frac{d\\langle w \\rangle}{dt}$, in the long-time limit. Given that the underlying spike train statistics are stationary (homogeneous Poisson processes) and the parameters $\\eta$ and $R_{0}$ are constant, the expected rate of change will also be a constant. We can calculate this rate by integrating the expected contributions from all possible spike pairings over all possible time differences.\n\nA fundamental property of two independent homogeneous Poisson processes with rates $\\lambda_{1}$ and $\\lambda_{2}$ is that the rate of occurrence of pairs of events, one from each process, with a time separation that lies in the infinitesimal interval $[\\tau, \\tau + d\\tau]$, is given by $\\lambda_{1} \\lambda_{2} d\\tau$. This result stems directly from the independence of the processes. The probability of an event from the first process in a small time window $dt$ is $\\lambda_{1} dt$. The probability of an event from the second process in a window $d\\tau$ at a time lag $\\tau$ later is $\\lambda_{2} d\\tau$. The joint probability of this specific pair is $(\\lambda_{1} dt)(\\lambda_{2} d\\tau)$. The rate density of such pairs is obtained by dividing by $dt$ and the interval $d\\tau$, giving $\\lambda_{1}\\lambda_{2}$.\n\nApplying this principle to the given synapse, the rate of (presynaptic, postsynaptic) spike pairs with a time difference $\\Delta t = t_{\\text{post}} - t_{\\text{pre}}$ falling within the interval $[\\tau, \\tau + d\\tau]$ is $\\lambda_{\\text{pre}} \\lambda_{\\text{post}} d\\tau$.\n\nEach such pair contributes an amount $\\eta R_{0} F(\\tau)$ to the total weight change. The expected differential contribution to the rate of weight change, arising from pairs with time lags in $[\\tau, \\tau + d\\tau]$, is the product of the rate of these pairs and the change per pair:\n$$ d\\left(\\frac{d\\langle w \\rangle}{dt}\\right) = (\\eta R_{0} F(\\tau)) \\times (\\lambda_{\\text{pre}} \\lambda_{\\text{post}} d\\tau) $$\nTo obtain the total expected rate of change, $\\frac{d\\langle w \\rangle}{dt}$, we must integrate this contribution over all possible time lags $\\tau$. The \"all-to-all\" nature of the plasticity rule implies the integration range is from $-\\infty$ to $\\infty$:\n$$ \\frac{d\\langle w \\rangle}{dt} = \\int_{-\\infty}^{\\infty} \\eta R_{0} \\lambda_{\\text{pre}} \\lambda_{\\text{post}} F(\\tau) d\\tau $$\nSince $\\eta$, $R_{0}$, $\\lambda_{\\text{pre}}$, and $\\lambda_{\\text{post}}$ are constants, they can be factored out of the integral, yielding the final expression for the expected rate of change:\n$$ \\frac{d\\langle w \\rangle}{dt} = \\eta R_{0} \\lambda_{\\text{pre}} \\lambda_{\\text{post}} \\int_{-\\infty}^{\\infty} F(\\tau) d\\tau $$\nThe problem states that $F(\\tau)$ is absolutely integrable, which guarantees that the integral $\\int_{-\\infty}^{\\infty} F(\\tau) d\\tau$ converges to a finite value, making the rate well-defined.\n\nThe second part of the problem asks for the condition under which the synaptic weight diverges. In the absence of any stabilizing mechanisms (such as weight dependence or hard bounds), the expected weight evolves linearly with time: $\\langle w(t) \\rangle = \\langle w(0) \\rangle + t \\cdot \\frac{d\\langle w \\rangle}{dt}$. The weight will diverge to $+\\infty$ if the rate $\\frac{d\\langle w \\rangle}{dt}$ is positive and to $-\\infty$ if the rate is negative.\n\nLet $C = \\eta R_{0} \\lambda_{\\text{pre}} \\lambda_{\\text{post}} \\int_{-\\infty}^{\\infty} F(\\tau) d\\tau$.\nThe learning rate $\\eta$ and the firing rates $\\lambda_{\\text{pre}}$ and $\\lambda_{\\text{post}}$ are non-negative. We assume they are strictly positive for any change to occur. Thus, the sign of $C$ is determined by the sign of the product of the reward $R_{0}$ and the integral of the STDP window function.\n\nThe condition for divergence to $+\\infty$ is $C  0$, which implies:\n$$ R_{0} \\int_{-\\infty}^{\\infty} F(\\tau) d\\tau  0 $$\nThis is satisfied if ($R_{0}  0$ and $\\int_{-\\infty}^{\\infty} F(\\tau) d\\tau  0$) or if ($R_{0}  0$ and $\\int_{-\\infty}^{\\infty} F(\\tau) d\\tau  0$).\n\nThe condition for divergence to $-\\infty$ is $C  0$, which implies:\n$$ R_{0} \\int_{-\\infty}^{\\infty} F(\\tau) d\\tau  0 $$\nThis is satisfied if ($R_{0}  0$ and $\\int_{-\\infty}^{\\infty} F(\\tau) d\\tau  0$) or if ($R_{0}  0$ and $\\int_{-\\infty}^{\\infty} F(\\tau) d\\tau  0$).\nIf the product is zero, the expected weight is stable.\nThe question asks for the expected synaptic weight change per unit time as the final answer.",
            "answer": "$$\n\\boxed{\\eta R_{0} \\lambda_{\\text{pre}} \\lambda_{\\text{post}} \\int_{-\\infty}^{\\infty} F(\\tau) d\\tau}\n$$"
        },
        {
            "introduction": "A key insight in modern computational neuroscience is the deep connection between biologically plausible learning rules and formal algorithms from reinforcement learning. This practice provides a concrete example of this link by showing the equivalence between a three-factor synaptic plasticity rule and the Temporal-Difference (TD) learning algorithm . By solving this problem, you will demonstrate how a global reward prediction error signal can modulate local synaptic eligibility traces to precisely implement a TD($\\lambda$) update, providing a powerful framework for understanding how the brain might solve credit assignment problems.",
            "id": "4057751",
            "problem": "Consider a minimal episodic task modeled as a two-state Markov chain with states $A$ and $B$ and a terminal absorbing state $\\perp$. The episode begins at time $t=0$ in state $A$, transitions deterministically to state $B$ at $t=1$, and terminates at $t=2$ in state $\\perp$. A scalar reward $R$ is delivered only at termination; that is, $r_{0}=0$, $r_{1}=R$, and $r_{2}$ is undefined because the episode ends at $t=2$ upon entering $\\perp$. This task is used to train a single-parameter linear value function $V(s;w)=w\\,x(s)$, where $w\\in\\mathbb{R}$ is the synaptic weight to be adapted, and $x(s)\\in\\mathbb{R}$ is a fixed feature of state $s$. Assume $x(A)=x_{0}$ and $x(B)=x_{1}$ are given constants. The discount factor is $\\gamma\\in(0,1)$, and the eligibility trace parameter is $\\lambda\\in[0,1]$. The learning rate is $\\eta0$.\n\nYou will compute the one-episode synaptic weight update using Temporal-Difference (TD) learning of order zero, also known as Temporal-Difference (TD(0)), with eligibility traces, and then verify its equivalence to a reward-modulated three-factor Spike-Timing-Dependent Plasticity (STDP) rule. Use the following fundamental definitions:\n\n- The Temporal-Difference error at time $t$ is $\\delta_{t}=r_{t}+\\gamma\\,V(s_{t+1};w)-V(s_{t};w)$, with the convention $V(\\perp;w)=0$.\n- The eligibility trace obeys $e_{t}=\\gamma\\,\\lambda\\,e_{t-1}+x(s_{t})$, with $e_{-1}=0$.\n\nIn a neuromorphic formulation of reward-modulated STDP, the synaptic update is a three-factor product, where the global modulatory signal $M_{t}$ gates a local eligibility $e_{t}$ accrued from pre- and post-synaptic activity; in this task, identify $M_{t}$ with the Temporal-Difference error, $M_{t}\\equiv\\delta_{t}$, consistent with reward prediction error signaling.\n\nStarting from these definitions and the Markov chain structure described above, derive the single-episode synaptic weight increment $\\Delta w$ produced by TD(0) with eligibility traces and explicitly show that the same $\\Delta w$ results from the three-factor reward-modulated STDP rule with $M_{t}=\\delta_{t}$. Express your final answer as a closed-form analytic expression in terms of the symbols $\\eta$, $\\gamma$, $\\lambda$, $R$, $w$, $x_{0}$, and $x_{1}$. No numerical evaluation is required, and no units need be attached to the final expression. The final answer must be a single closed-form expression; do not present it as an equation to be solved.",
            "solution": "The total change in the synaptic weight $w$ over a single episode is given by the sum of updates at each time step. The learning rule is Temporal-Difference learning with eligibility traces, commonly known as TD($\\lambda$). The update rule is:\n$$\n\\Delta w = \\eta \\sum_{t=0}^{T-1} \\delta_t e_t\n$$\nIn this problem, the episode consists of transitions from $t=0$ to $t=1$, with termination at $t=2$. Thus, the summation is over $t=0, 1$.\n$$\n\\Delta w = \\eta (\\delta_0 e_0 + \\delta_1 e_1)\n$$\nTo calculate $\\Delta w$, we must first determine the values of the eligibility traces $e_0$ and $e_1$, and the TD errors $\\delta_0$ and $\\delta_1$.\n\n**1. Calculation of Eligibility Traces ($e_t$)**\nThe eligibility trace evolves according to the recurrence relation $e_{t}=\\gamma\\,\\lambda\\,e_{t-1}+x(s_{t})$, with the initial condition $e_{-1}=0$.\n\nFor $t=0$:\nThe state is $s_0=A$, so the feature is $x(s_0)=x_0$.\n$$\ne_0 = \\gamma\\,\\lambda\\,e_{-1} + x(s_0) = \\gamma\\,\\lambda\\,(0) + x_0 = x_0\n$$\n\nFor $t=1$:\nThe state is $s_1=B$, so the feature is $x(s_1)=x_1$.\n$$\ne_1 = \\gamma\\,\\lambda\\,e_{0} + x(s_1) = \\gamma\\,\\lambda\\,x_0 + x_1\n$$\n\n**2. Calculation of TD Errors ($\\delta_t$)**\nThe TD error is defined as $\\delta_{t}=r_{t}+\\gamma\\,V(s_{t+1};w)-V(s_{t};w)$. The value function is $V(s;w)=w\\,x(s)$.\n\nFor $t=0$:\nThe states are $s_0=A$ and $s_1=B$. The reward is $r_0=0$.\nThe value functions are $V(s_0;w) = V(A;w) = w\\,x_0$ and $V(s_1;w) = V(B;w) = w\\,x_1$.\n$$\n\\delta_0 = r_0 + \\gamma\\,V(s_1;w) - V(s_0;w) = 0 + \\gamma\\,(w\\,x_1) - w\\,x_0 = w(\\gamma\\,x_1 - x_0)\n$$\n\nFor $t=1$:\nThe states are $s_1=B$ and the terminal state $s_2=\\perp$. The reward is $r_1=R$.\nThe value functions are $V(s_1;w) = V(B;w) = w\\,x_1$ and by convention, $V(s_2;w) = V(\\perp;w) = 0$.\n$$\n\\delta_1 = r_1 + \\gamma\\,V(s_2;w) - V(s_1;w) = R + \\gamma\\,(0) - w\\,x_1 = R - w\\,x_1\n$$\n\n**3. Calculation of the Total Weight Update ($\\Delta w$)**\nNow we substitute the expressions for $e_0, e_1, \\delta_0, \\delta_1$ into the equation for $\\Delta w$:\n$$\n\\Delta w = \\eta (\\delta_0 e_0 + \\delta_1 e_1)\n$$\n$$\n\\Delta w = \\eta \\left[ \\left( w(\\gamma\\,x_1 - x_0) \\right) (x_0) + \\left( R - w\\,x_1 \\right) (\\gamma\\,\\lambda\\,x_0 + x_1) \\right]\n$$\nWe expand the terms inside the brackets:\n$$\n\\Delta w = \\eta \\left[ (w\\,\\gamma\\,x_0\\,x_1 - w\\,x_0^2) + (R(\\gamma\\,\\lambda\\,x_0 + x_1) - w\\,x_1(\\gamma\\,\\lambda\\,x_0 + x_1)) \\right]\n$$\n$$\n\\Delta w = \\eta \\left[ w\\,\\gamma\\,x_0\\,x_1 - w\\,x_0^2 + R\\,\\gamma\\,\\lambda\\,x_0 + R\\,x_1 - w\\,\\gamma\\,\\lambda\\,x_0\\,x_1 - w\\,x_1^2 \\right]\n$$\nTo obtain the final closed-form expression, we group terms involving the reward $R$ and terms involving the current weight $w$:\n$$\n\\Delta w = \\eta \\left[ (R\\,\\gamma\\,\\lambda\\,x_0 + R\\,x_1) + (w\\,\\gamma\\,x_0\\,x_1 - w\\,x_0^2 - w\\,\\gamma\\,\\lambda\\,x_0\\,x_1 - w\\,x_1^2) \\right]\n$$\nFactoring out $R$ and $w$:\n$$\n\\Delta w = \\eta \\left[ R(\\gamma\\,\\lambda\\,x_0 + x_1) + w(\\gamma\\,x_0\\,x_1 - x_0^2 - \\gamma\\,\\lambda\\,x_0\\,x_1 - x_1^2) \\right]\n$$\nThe term multiplying $w$ can be rearranged for clarity:\n$$\n\\gamma\\,x_0\\,x_1 - x_0^2 - \\gamma\\,\\lambda\\,x_0\\,x_1 - x_1^2 = -(x_0^2 + x_1^2 - \\gamma\\,x_0\\,x_1 + \\gamma\\,\\lambda\\,x_0\\,x_1) = -(x_0^2 + x_1^2 - \\gamma(1-\\lambda)x_0\\,x_1)\n$$\nSubstituting this back gives the final expression:\n$$\n\\Delta w = \\eta \\left[ R(\\gamma\\,\\lambda\\,x_0 + x_1) - w(x_0^2 + x_1^2 - \\gamma(1-\\lambda)x_0\\,x_1) \\right]\n$$\n\n**4. Equivalence with the Three-Factor Rule**\nThe problem defines the reward-modulated three-factor STDP rule as a product of a global modulatory signal $M_t$ gating a local eligibility $e_t$. The total update is the sum of these products over time, scaled by a learning rate $\\eta$:\n$$\n\\Delta w = \\eta \\sum_t M_t e_t\n$$\nThe problem explicitly provides the identification $M_t \\equiv \\delta_t$. Substituting this into the three-factor rule gives:\n$$\n\\Delta w = \\eta \\sum_t \\delta_t e_t\n$$\nThis is precisely the definition of the update rule for TD($\\lambda$) that was used as the starting point of the derivation. Therefore, the equivalence is established by the definitions provided in the problem statement. The calculation performed above for the TD($\\lambda$) update is, by this identification, simultaneously the calculation for the specified three-factor STDP rule. The problem reduces to deriving the single-episode weight update under the TD($\\lambda$) framework, which has been completed.",
            "answer": "$$\n\\boxed{\\eta \\left[ R(\\gamma\\lambda x_0 + x_1) - w(x_0^2 + x_1^2 - \\gamma(1-\\lambda)x_0 x_1) \\right]}\n$$"
        },
        {
            "introduction": "This final practice brings together the theoretical concepts into a functional, learning system. You are tasked with building a spiking neural network that solves a classic reinforcement learning problem—the multi-armed bandit—using reward-modulated STDP . This simulation will not only require you to implement the core components of a spiking agent but also to investigate a crucial mechanism for efficient learning: variance reduction via baseline subtraction. Completing this exercise will provide a tangible understanding of how synaptic rules, action selection mechanisms, and reward signaling interact to produce adaptive behavior.",
            "id": "4057793",
            "problem": "Consider a two-action bandit-like spiking network controlled by Reward-Modulated Spike-Timing-Dependent Plasticity (STDP). There are $N_{\\mathrm{pre}}$ independent Poisson input neurons feeding two competing output (action) neurons. On each trial, input neurons emit spikes and the action neurons produce at most one spike; the action with the earliest spike is taken. The obtained scalar reward modulates synaptic plasticity via a baseline-subtracted factor.\n\nUse the following fundamental bases and definitions to construct and analyze the dynamics:\n\n- Spike-Timing-Dependent Plasticity (STDP): For a presynaptic spike at time $t_{\\mathrm{pre}}$ and a postsynaptic spike at time $t_{\\mathrm{post}}$, define the STDP window function $F(\\Delta t)$ with $\\Delta t = t_{\\mathrm{post}} - t_{\\mathrm{pre}}$ as\n$$\nF(\\Delta t) = \n\\begin{cases}\nA_{+} \\exp\\left(-\\frac{\\Delta t}{\\tau_{+}}\\right),  \\Delta t  0 \\\\\n- A_{-} \\exp\\left(\\frac{\\Delta t}{\\tau_{-}}\\right),  \\Delta t  0\n\\end{cases}\n$$\nwhere $A_{+}  0$, $A_{-}  0$, $\\tau_{+}  0$, and $\\tau_{-}  0$.\n\n- Reward-Modulated Plasticity: Let $R \\in \\{0,1\\}$ denote the reward outcome of the chosen action on a trial, and let $b$ denote a baseline estimate intended to reduce variance. The synaptic update for weight $w_{ji}$ from presynaptic neuron $i$ to action neuron $j$ is defined as\n$$\n\\Delta w_{ji} = \\eta \\, (R - b) \\, e_{ji},\n$$\nwhere $\\eta  0$ is the learning rate and $e_{ji}$ is the eligibility signal constructed from spike pairs,\n$$\ne_{ji} = \\sum_{t_{\\mathrm{pre}} \\in \\mathcal{S}_i} F(t_{\\mathrm{post}} - t_{\\mathrm{pre}}),\n$$\nwith $\\mathcal{S}_i$ the set of presynaptic spike times from neuron $i$ on that trial and $t_{\\mathrm{post}}$ the postsynaptic spike time of the chosen action neuron. If no postsynaptic spike occurs during the trial, take $e_{ji} = 0$.\n\n- Action Selection by Competing Hazards: For each action neuron $j \\in \\{0,1\\}$, define its hazard rate $\\lambda_j$ as an increasing function of aggregate presynaptic drive,\n$$\n\\lambda_j = \\alpha \\max\\!\\left( \\sum_{i=1}^{N_{\\mathrm{pre}}} w_{ji} x_i, \\, \\varepsilon \\right),\n$$\nwhere $\\alpha  0$ is a gain, $\\varepsilon  0$ ensures strictly positive rate, and $x_i$ is the number of spikes emitted by presynaptic neuron $i$ on that trial. Draw a candidate spike time $t_j$ from an exponential distribution with rate $\\lambda_j$. The chosen action is the one with the smallest $t_j$ not exceeding the trial duration $T$. If neither candidate time falls within $[0,T]$, select stochastically with probability proportional to $\\lambda_j$, and set $e_{ji} = 0$ if no postsynaptic spike occurs.\n\n- Bandit Environment: Action $j=0$ (optimal) returns $R=1$ with probability $p_{\\mathrm{opt}}$ and $R=0$ otherwise. Action $j=1$ returns $R=1$ with probability $p_{\\mathrm{non}}$ and $R=0$ otherwise, with $p_{\\mathrm{opt}}  p_{\\mathrm{non}}$. Rewards are independent across trials conditioned on the chosen action.\n\n- Baseline Subtraction: The baseline $b$ on each trial is defined as $b = \\mu + \\xi$, where $\\mu$ is a running estimate of expected reward updated by exponential averaging\n$$\n\\mu \\leftarrow (1 - \\gamma) \\mu + \\gamma R,\n$$\nwith $0  \\gamma  1$, and $\\xi$ is zero-mean Gaussian noise independent of $R$ and spike timing, with variance $\\sigma_b^2$. The quantity $\\sigma_b^2$ parameterizes baseline variance.\n\nYou must simulate $M$ independent learning trials for each baseline variance parameter and measure the learning time as the number of trials until the moving fraction of selecting the optimal action, computed over the last $W$ trials, first exceeds a threshold $\\theta$. If the threshold is not reached within a cap of $K$ trials, report $K$.\n\nThe simulation parameters to use are:\n- Number of presynaptic neurons: $N_{\\mathrm{pre}} = 30$.\n- Trial duration: $T = 0.2$.\n- Presynaptic rate per neuron: $r_{\\mathrm{in}} = 25$.\n- STDP parameters: $A_{+} = 1.0$, $A_{-} = 0.5$, $\\tau_{+} = 0.02$, $\\tau_{-} = 0.02$.\n- Hazard gain and floor: $\\alpha = 30.0$, $\\varepsilon = 10^{-12}$.\n- Learning rate and decay: $\\eta = 2 \\times 10^{-4}$, with a small weight decay implemented as $- \\lambda_w w_{ji}$ per trial where $\\lambda_w = 10^{-4}$.\n- Reward probabilities: $p_{\\mathrm{opt}} = 0.8$, $p_{\\mathrm{non}} = 0.2$.\n- Baseline averaging rate: $\\gamma = 0.01$.\n- Sliding window and threshold: $W = 100$, $\\theta = 0.9$.\n- Weight bounds: initialize $w_{ji}$ with small random positive values and clip all weights to $[0, 0.05]$ after each update.\n- Trial cap: $K = 2000$.\n\nTest Suite:\n- Baseline variance values to test: $\\sigma_b^2 \\in \\{0.0, 0.0025, 0.04, 0.25\\}$.\n\nFor each value of $\\sigma_b^2$ in the test suite, simulate the learning process and compute the learning time as an integer number of trials. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[n1,n2,n3,n4]\"), where each $n_i$ is the learning time corresponding to the $i$-th baseline variance test case. No other output is permitted.",
            "solution": "The problem asks for a simulation of a two-action reinforcement learning task performed by a spiking neural network. The synaptic weights of the network are updated via a reward-modulated spike-timing-dependent plasticity (R-STDP) rule. The objective is to determine the learning time for different levels of variance in a modulatory baseline signal. The problem is scientifically grounded, well-posed, and contains all necessary information to construct a simulation.\n\nThe simulation is structured as a series of trials, repeated for each specified baseline variance $\\sigma_b^2$. For each value of $\\sigma_b^2$, the system is initialized and then evolved for a maximum of $K=2000$ trials. To ensure a fair comparison, the random number generator is reset to the same state at the beginning of each simulation run, isolating the effect of $\\sigma_b^2$.\n\nA single trial within the simulation proceeds as follows:\n\n1.  **Presynaptic Spike Generation**: For each of the $N_{\\mathrm{pre}}=30$ input neurons, the number of spikes, $x_i$, to be fired during the trial of duration $T=0.2$ s is drawn from a Poisson distribution with mean $\\lambda = r_{\\mathrm{in}} T = 25 \\times 0.2 = 5$. The timings of these $x_i$ spikes, denoted by the set $\\mathcal{S}_i$, are drawn from a uniform distribution over the interval $[0, T]$.\n\n2.  **Action Selection**: The two output neurons, indexed by $j \\in \\{0, 1\\}$, compete to fire first. The instantaneous probability of firing for each neuron $j$ is governed by a hazard rate, $\\lambda_j$. This rate is a function of the total weighted input drive it receives:\n    $$\n    \\lambda_j = \\alpha \\max\\left( \\sum_{i=1}^{N_{\\mathrm{pre}}} w_{ji} x_i, \\, \\varepsilon \\right)\n    $$\n    where $w_{ji}$ is the synaptic weight from input neuron $i$ to output neuron $j$, $\\alpha=30.0$ is a gain factor, and $\\varepsilon=10^{-12}$ is a small positive constant to prevent rates from being zero. A candidate spike time $t_j$ for each neuron is then drawn from an exponential distribution with rate $\\lambda_j$. The winning action, $j_{\\text{chosen}}$, corresponds to the neuron with the minimum candidate time, $t_{\\text{post}} = \\min(t_0, t_1)$.\n    If $t_{\\text{post}}  T$, no spike occurs within the trial period. In this case, an action is chosen stochastically with probabilities proportional to the hazard rates, $P(\\text{choose } j) = \\lambda_j / (\\lambda_0 + \\lambda_1)$.\n\n3.  **Eligibility Trace Calculation**: An eligibility trace, $e_{ji}$, quantifies the causal link between presynaptic spikes and a postsynaptic spike, serving as a memory for later weight updates. It is non-zero only for the synapses of the winning neuron, and only if it fired within the trial duration ($t_{\\text{post}} \\le T$). For the winning neuron $j_{\\text{chosen}}$, the eligibility trace for each input $i$ is:\n    $$\n    e_{j_{\\text{chosen}}, i} = \\sum_{t_{\\mathrm{pre}} \\in \\mathcal{S}_i} F(t_{\\text{post}} - t_{\\mathrm{pre}})\n    $$\n    where $F(\\Delta t)$ is the STDP window function defined with parameters $A_{+}=1.0$, $A_{-}=0.5$, and $\\tau_{+}=\\tau_{-}=0.02$. For the losing neuron, or if no spike occurred, all eligibility traces are $0$.\n\n4.  **Reward and Learning Signal**: After an action $j_{\\text{chosen}}$ is taken, the environment provides a stochastic reward $R \\in \\{0, 1\\}$. Action $j=0$ is optimal, yielding $R=1$ with probability $p_{\\mathrm{opt}}=0.8$. Action $j=1$ is suboptimal, with $p_{\\mathrm{non}}=0.2$. A learning signal is formed by comparing the reward $R$ to a baseline $b = \\mu + \\xi$. Here, $\\mu$ is an exponential moving average of past rewards, and $\\xi$ is zero-mean Gaussian noise with variance $\\sigma_b^2$. The variance $\\sigma_b^2$ is the parameter we investigate.\n\n5.  **Weight and State Update**: The synaptic weights are updated based on the reward prediction error $(R-b)$ and the eligibility trace $e_{ji}$:\n    $$\n    w_{ji}(t+1) = (1 - \\lambda_w) w_{ji}(t) + \\eta (R - b) e_{ji}(t)\n    $$\n    This update combines the R-STDP rule (with learning rate $\\eta = 2 \\times 10^{-4}$) and a weight decay term ($\\lambda_w = 10^{-4}$). After the update, all weights are clipped to the range $[0, 0.05]$. The running average reward $\\mu$ is also updated: $\\mu \\leftarrow (1-\\gamma)\\mu + \\gamma R$ with $\\gamma=0.01$.\n\n6.  **Performance Evaluation**: The learning process is monitored by calculating the fraction of optimal actions ($j=0$) chosen over a sliding window of the last $W=100$ trials. The simulation for a given $\\sigma_b^2$ terminates when this fraction first exceeds the threshold $\\theta=0.9$. The trial number at which this occurs is the learning time. If the threshold is not reached within $K=2000$ trials, the learning time is recorded as $K$.\n\nThis entire procedure is implemented in a Python script. The simulation is run for each value of $\\sigma_b^2$ in the test suite $\\{0.0, 0.0025, 0.04, 0.25\\}$, and the resulting integer learning times are collected and formatted as the final output.",
            "answer": "```python\nimport numpy as np\n\ndef run_simulation(sigma_b_sq, params, rng):\n    \"\"\"\n    Runs a single full simulation for a given baseline variance.\n\n    Args:\n        sigma_b_sq (float): The variance of the baseline noise.\n        params (tuple): A tuple containing all simulation parameters.\n        rng (np.random.Generator): The random number generator instance.\n\n    Returns:\n        int: The learning time in number of trials.\n    \"\"\"\n    # Unpack parameters\n    (N_pre, T, r_in, A_plus, A_minus, tau_plus, tau_minus,\n     alpha, epsilon, eta, lambda_w, p_opt, p_non,\n     gamma, W, theta, K, w_max) = params\n\n    sigma_b = np.sqrt(sigma_b_sq)\n\n    # Initialization\n    w = rng.uniform(0, 0.01, size=(2, N_pre))\n    mu = (p_opt + p_non) / 2.0\n    choice_history = np.zeros(W, dtype=int)\n\n    for trial in range(1, K + 1):\n        # 1. Generate presynaptic spikes\n        spike_counts = rng.poisson(r_in * T, size=N_pre)\n        presynaptic_spikes = [sorted(rng.uniform(0, T, size=n)) for n in spike_counts]\n\n        # 2. Action selection\n        drive = w @ spike_counts\n        lambdas = alpha * np.maximum(drive, epsilon)\n        candidate_times = rng.exponential(1.0 / lambdas)\n        t_post = np.min(candidate_times)\n        chosen_action = np.argmin(candidate_times)\n\n        eligibility_traces = np.zeros_like(w)\n\n        if t_post = T:\n            # 3. Calculate eligibility traces for the winning neuron\n            e_ji = np.zeros(N_pre)\n            for i in range(N_pre):\n                temp_e = 0.0\n                for t_pre in presynaptic_spikes[i]:\n                    delta_t = t_post - t_pre\n                    if delta_t  0:\n                        temp_e += A_plus * np.exp(-delta_t / tau_plus)\n                    elif delta_t  0:\n                        temp_e += -A_minus * np.exp(delta_t / tau_minus)\n                e_ji[i] = temp_e\n            eligibility_traces[chosen_action, :] = e_ji\n        else:\n            # No spike occurred: stochastic choice, eligibility remains zero\n            probs = lambdas / (np.sum(lambdas) + 1e-20)\n            chosen_action = rng.choice([0, 1], p=probs)\n\n        # 4. Determine Reward and Baseline\n        reward_prob = p_opt if chosen_action == 0 else p_non\n        R = 1 if rng.random()  reward_prob else 0\n        xi = rng.normal(0, sigma_b)\n        b = mu + xi\n\n        # 5. Update Weights\n        w = (1 - lambda_w) * w + eta * (R - b) * eligibility_traces\n        w = np.clip(w, 0, w_max)\n\n        # 6. Update Mean Reward\n        mu = (1 - gamma) * mu + gamma * R\n\n        # 7. Check Learning Criterion\n        choice_history[(trial - 1) % W] = 1 if chosen_action == 0 else 0\n        if trial = W:\n            performance = np.mean(choice_history)\n            if performance  theta:\n                return trial\n\n    return K\n\ndef solve():\n    \"\"\"\n    Main function to set up and run the simulations for the test suite.\n    \"\"\"\n    # Simulation Parameters\n    params = (\n        30,      # N_pre: Number of presynaptic neurons\n        0.2,     # T: Trial duration\n        25.0,    # r_in: Presynaptic rate per neuron\n        1.0,     # A_plus: STDP parameter\n        0.5,     # A_minus: STDP parameter\n        0.02,    # tau_plus: STDP parameter\n        0.02,    # tau_minus: STDP parameter\n        30.0,    # alpha: Hazard gain\n        1e-12,   # epsilon: Hazard floor\n        2e-4,    # eta: Learning rate\n        1e-4,    # lambda_w: Weight decay\n        0.8,     # p_opt: Reward probability for optimal action\n        0.2,     # p_non: Reward probability for non-optimal action\n        0.01,    # gamma: Baseline averaging rate\n        100,     # W: Sliding window size\n        0.9,     # theta: Performance threshold\n        2000,    # K: Trial cap\n        0.05     # w_max: Maximum weight\n    )\n\n    # Test Suite\n    test_cases = [0.0, 0.0025, 0.04, 0.25]\n    results = []\n\n    # Using the same seed for each run ensures that the only difference\n    # between simulations is the parameter being tested (sigma_b^2),\n    # allowing for a fair comparison.\n    seed = 42\n    \n    for sigma_b_sq in test_cases:\n        rng = np.random.default_rng(seed=seed)\n        learning_time = run_simulation(sigma_b_sq, params, rng)\n        results.append(learning_time)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}