{
    "hands_on_practices": [
        {
            "introduction": "在我们深入研究如何通过时间反向传播（BPTT）训练脉冲神经网络（SNN）之前，我们必须首先掌握其基本构建单元——单个神经元——的前向动态。本实践练习旨在通过精确模拟一个离散时间的“漏积分-发放”（Leaky Integrate-and-Fire, LIF）神经元，为后续的梯度计算打下坚实基础。通过亲手实现神经元状态随时间的演化，您将构建出BPTT算法所依赖的计算图，并深入理解脉冲活动是如何从输入和神经元参数中产生的 。",
            "id": "4036252",
            "problem": "考虑一个在已知输入电流序列下，在离散时间内演化的单个漏积分放电（Leaky Integrate-and-Fire, LIF）神经元。在连续时间内，膜电位 $v(t)$ 遵循常微分方程 (ODE) $dv/dt = -\\frac{1}{\\tau} v(t) + k I(t)$，其中 $\\tau$ 是膜时间常数，$k$ 是一个增益（可以包含膜电阻等生物物理常数），$I(t)$ 是输入电流。当膜电位达到或超过一个阈值时，会发出一个脉冲。脉冲之后，膜电位被重置到一个指定的重置值。针对脉冲神经网络（Spiking Neural Networks, SNNs）的时间反向传播（Backpropagation Through Time, BPTT）算法需要一个随时间精确展开的前向状态序列来定义计算图。\n\n假设使用均匀的离散时间步长，并采用指数欧拉（exponential-Euler）方法对LIF动力学进行离散化。令 $\\alpha \\in (0,1)$ 表示对应于一个时间步长内连续时间衰减的离散时间泄漏因子，$\\theta$ 表示放电阈值，$V_{\\text{reset}}$ 表示硬重置电位。令 $I_t$ 表示在时间步 $t$ 的离散时间驱动，它已经被缩放，以使其贡献如所选离散化方法定义的那样线性地进入阈值前膜电位。定义赫维赛德阶跃函数 (Heaviside step function) $H(x)$，当 $x \\ge 0$ 时返回 $1$，否则返回 $0$。在每个离散步骤中，神经元的演化如下：从时间索引 $t$ 的膜电位 $v_t$ 开始，形成一个阈值前电位，根据是否跨越阈值来确定一个脉冲，并应用硬重置以获得下一步的膜电位 $v_{t+1}$。确切的顺序必须是：泄漏与输入积分，阈值判断，然后重置。时间反向传播（BPTT）在每个步骤 $t$ 所需的展开状态必须记录积分前状态 $v_t$、阈值前积分电位、脉冲指示符 $s_t \\in \\{0,1\\}$ 和重置后状态 $v_{t+1}$。\n\n你的任务是：\n- 从连续时间LIF ODE $dv/dt = -\\frac{1}{\\tau} v(t) + k I(t)$ 出发，根据所述的指数欧拉离散化和硬重置规则，推导出阈值前电位的离散时间递推公式和脉冲重置规则，用给定的参数 $\\alpha$、$\\theta$、$V_{\\text{reset}}$ 以及序列 $I_t$ 来表示。\n- 实现一个程序，给定 $(\\alpha,\\theta,V_{\\text{reset}})$、初始膜电位 $v_0$ 以及一个长度为 $T+1$（索引为 $t=0,\\dots,T$）的序列 $I_t$，为每个 $t=0,\\dots,T$ 计算并记录：积分前膜电位 $v_t$、阈值前电位、脉冲指示符 $s_t \\in \\{0,1\\}$ 和重置后膜电位 $v_{t+1}$，并遵循精确的顺序：积分、阈值判断、重置。当阈值前电位恰好等于 $\\theta$ 时，使用临界情况处理规则 $s_t = 1$。\n- 对于下面的每个测试用例，报告BPTT所需的确切展开状态列表，该列表为列表的列表，其中第 $t$ 个条目是 $[t, v_t, \\text{pre\\_threshold}, s_t, v_{t+1}]$，其中 $t$ 为整数，$v_t$ 和阈值前电位为浮点值，$s_t$ 为整数 $0$ 或 $1$，$v_{t+1}$ 为浮点值。\n\n使用以下具有科学上合理且自洽的参数值的测试套件：\n1. 典型脉冲情况：\n   - $\\alpha = 0.9$，$\\theta = 1.0$， $V_{\\text{reset}} = 0.0$， $v_0 = 0.0$， $T = 5$， $I_t$ 对于 $t=0,\\dots,5$ 等于 $[0.5, 0.6, 0.0, 0.0, 0.0, 0.0]$。\n2. 边界阈值相等情况（测试临界处理规则 $H(0)=1$）：\n   - $\\alpha = 0.5$，$\\theta = 1.0$， $V_{\\text{reset}} = 0.0$， $v_0 = 2.0$， $T = 2$， $I_t$ 对于 $t=0,\\dots,2$ 等于 $[0.0, 0.0, 0.0]$。\n3. 有泄漏情况下的无脉冲累积：\n   - $\\alpha = 0.95$，$\\theta = 1.2$， $V_{\\text{reset}} = 0.0$，$v_0 = 0.1$， $T = 4$， $I_t$ 对于 $t=0,\\dots,4$ 等于 $[0.1, 0.1, 0.1, 0.1, 0.1]$。\n4. 非零重置下的多次脉冲：\n   - $\\alpha = 0.8$，$\\theta = 1.0$，$V_{\\text{reset}} = 0.2$，$v_0 = 0.0$，$T = 6$， $I_t$ 对于 $t=0,\\dots,6$ 等于 $[1.0, 0.9, 0.9, 0.9, 0.0, 1.2, 0.0]$。\n\n最终输出格式要求：\n- 你的程序应生成单行输出，其中包含一个用方括号括起来的、以逗号分隔的结果列表。这个顶层列表中的每个元素对应一个测试用例，并且它本身必须是一个包含 $T+1$ 个形如 $[t,v_t,\\text{pre\\_threshold},s_t,v_{t+1}]$ 条目的、用方括号括起来的列表。例如，外层列表必须形如 $[ \\text{case1}, \\text{case2}, \\dots ]$，不含任何额外文本。本问题不使用角度，也无需物理单位。所有报告的值必须是指定类型的数值。",
            "solution": "该问题已经过验证，被认为是可靠、适定且具有科学依据的。它提出了计算神经科学中的一个标准任务：在离散时间内模拟漏积分放电（LIF）神经元模型，这是诸如时间反向传播（BPTT）等算法的基础步骤。\n\n### 离散时间递推公式的推导\n\n膜电位 $v(t)$ 的动力学由以下连续时间常微分方程（ODE）给出：\n$$ \\frac{dv}{dt} = -\\frac{1}{\\tau} v(t) + k I(t) $$\n这是一个一阶线性常微分方程。为了在时长为 $\\Delta t$ 的时间步长上对其进行离散化，我们使用指数欧拉方法。该方法在输入电流 $I(t)$ 在该区间内恒定的假设下，为 ODE 在区间 $[t, t+\\Delta t]$ 上提供了精确解。\n\n齐次方程 $\\frac{dv}{dt} = -\\frac{1}{\\tau} v(t)$ 的解是 $v(t) = v(t_0) e^{-(t-t_0)/\\tau}$。在一个时间步长 $\\Delta t$ 内，电位会衰减一个因子 $e^{-\\Delta t/\\tau}$。问题将这个离散时间泄漏因子定义为 $\\alpha$：\n$$ \\alpha = e^{-\\Delta t/\\tau} $$\n对于完整的非齐次方程，在单个时间步长从 $t$ 到 $t+\\Delta t$ 的精确解（假设在此期间 $I(t)$ 是一个常数 $I_t$）是：\n$$ v(t+\\Delta t) = v(t) e^{-\\Delta t/\\tau} + k\\tau(1 - e^{-\\Delta t/\\tau}) I_t $$\n代入 $\\alpha$ 得到：\n$$ v(t+\\Delta t) = \\alpha v(t) + k\\tau(1 - \\alpha) I_t $$\n问题指出，离散时间输入驱动 $I_t$ 已经“被缩放，以使其贡献线性地进入”。这意味着项 $k\\tau(1 - \\alpha)$ 已被吸收到所提供的输入序列 $I_t$ 中。因此，在考虑脉冲和重置之前，膜电位的离散时间更新规则就是泄漏后的先前电位与缩放后输入的总和。\n\n设 $v_t$ 为时间步 $t$ 开始时的膜电位。遵循指定的操作顺序（积分、阈值判断、重置），我们首先计算阈值前电位，我们将其表示为 $u_t$：\n$$ u_t = \\alpha v_t + I_t $$\n此步骤代表泄漏和输入电流的积分。\n\n接下来，我们应用阈值判断规则。如果阈值前电位 $u_t$ 达到或超过阈值 $\\theta$，则发出一个脉冲 $s_t$。使用赫维赛德阶跃函数 $H(x)$（定义为当 $x \\ge 0$ 时为 $1$，否则为 $0$），脉冲指示符 $s_t \\in \\{0, 1\\}$ 为：\n$$ s_t = H(u_t - \\theta) $$\n这个公式正确地实现了当 $u_t = \\theta$ 时生成脉冲的临界情况处理规则。\n\n最后，我们应用硬重置机制来确定下一个时间步开始时的膜电位 $v_{t+1}$。如果生成了脉冲（$s_t = 1$），电位将重置为 $V_{\\text{reset}}$。如果没有发生脉冲（$s_t = 0$），电位将从其阈值前的值 $u_t$ 继承而来。这可以表示为单个方程：\n$$ v_{t+1} = (1 - s_t) u_t + s_t V_{\\text{reset}} $$\n\n### 模拟算法\n\n模拟从初始状态 $v_0$ 开始，针对时间步 $t = 0, 1, \\dots, T$ 迭代进行。对于每个时间步 $t$：\n\n1.  **获取状态和输入**：该步骤开始时的状态是 $v_t$。此步骤的输入是 $I_t$。\n2.  **积分**：计算阈值前电位 $u_t = \\alpha v_t + I_t$。\n3.  **阈值判断**：如果 $u_t \\ge \\theta$，确定脉冲输出 $s_t = 1$；否则 $s_t = 0$。\n4.  **重置**：计算下一个时间步的重置后电位 $v_{t+1} = (1 - s_t) u_t + s_t V_{\\text{reset}}$。\n5.  **为BPTT记录状态**：存储元组 $[t, v_t, u_t, s_t, v_{t+1}]$。\n6.  **推进时间**：下一个步骤的电位 $v_{t+1}$ 成为 $t+1$ 迭代的起始电位。\n\n此过程生成BPTT所需的完整展开状态序列。实现将对每个提供的测试用例执行此算法。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# No other libraries are required for this problem.\n\ndef solve():\n    \"\"\"\n    Main function to run the LIF neuron simulations for all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        # 1. Typical spiking case\n        {\n            \"alpha\": 0.9, \"theta\": 1.0, \"v_reset\": 0.0, \"v0\": 0.0, \"T\": 5,\n            \"I\": [0.5, 0.6, 0.0, 0.0, 0.0, 0.0]\n        },\n        # 2. Boundary threshold equality case\n        {\n            \"alpha\": 0.5, \"theta\": 1.0, \"v_reset\": 0.0, \"v0\": 2.0, \"T\": 2,\n            \"I\": [0.0, 0.0, 0.0]\n        },\n        # 3. No-spike accumulation under leak\n        {\n            \"alpha\": 0.95, \"theta\": 1.2, \"v_reset\": 0.0, \"v0\": 0.1, \"T\": 4,\n            \"I\": [0.1, 0.1, 0.1, 0.1, 0.1]\n        },\n        # 4. Multiple spikes with nonzero reset\n        {\n            \"alpha\": 0.8, \"theta\": 1.0, \"v_reset\": 0.2, \"v0\": 0.0, \"T\": 6,\n            \"I\": [1.0, 0.9, 0.9, 0.9, 0.0, 1.2, 0.0]\n        }\n    ]\n\n    results = []\n    for case_params in test_cases:\n        alpha = case_params[\"alpha\"]\n        theta = case_params[\"theta\"]\n        v_reset = case_params[\"v_reset\"]\n        v0 = case_params[\"v0\"]\n        I_seq = case_params[\"I\"]\n        T = case_params[\"T\"]\n        \n        case_result = simulate_lif(alpha, theta, v_reset, v0, I_seq, T)\n        results.append(case_result)\n\n    # Final print statement in the exact required format.\n    # The default str() representation of lists is compliant with the required output format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef simulate_lif(alpha, theta, v_reset, v0, I_seq, T):\n    \"\"\"\n    Simulates a discrete-time Leaky Integrate-and-Fire (LIF) neuron.\n\n    Args:\n        alpha (float): Discrete-time leak factor.\n        theta (float): Firing threshold.\n        v_reset (float): Reset potential after a spike.\n        v0 (float): Initial membrane potential.\n        I_seq (list of float): Sequence of input currents of length T+1.\n        T (int): The final time index for the simulation (total steps is T+1).\n\n    Returns:\n        list of lists: The unrolled state history required for BPTT. Each inner list\n                       is of the form [t, v_t, pre_threshold_potential, s_t, v_{t+1}].\n    \"\"\"\n    recorded_states = []\n    v_current = float(v0)\n    \n    # The simulation runs for t from 0 to T, inclusive.\n    for t in range(T + 1):\n        # Input for the current time step\n        I_t = float(I_seq[t])\n        \n        # v_t is the potential at the beginning of the step.\n        v_t = v_current\n        \n        # 1. Integrate: leak and input to get pre-threshold potential.\n        # This is denoted as u_t in the derivation.\n        pre_threshold_potential = alpha * v_t + I_t\n        \n        # 2. Threshold: check for spike. s_t is the spike indicator.\n        # The condition includes equality, implementing H(x) where H(0)=1.\n        s_t = 1 if pre_threshold_potential >= theta else 0\n        \n        # 3. Reset: determine the next state v_{t+1}.\n        if s_t == 1:\n            v_next = v_reset\n        else:\n            v_next = pre_threshold_potential\n        \n        # Record the BPTT state for this step.\n        # Types are: int, float, float, int, float.\n        recorded_states.append([t, v_t, pre_threshold_potential, s_t, float(v_next)])\n        \n        # Update the current potential for the next iteration.\n        v_current = float(v_next)\n        \n    return recorded_states\n\nsolve()\n```"
        },
        {
            "introduction": "掌握了SNN的前向模拟后，我们面临着训练过程中的核心挑战：脉冲发放事件的不可微性，这使得传统的梯度下降法无法直接应用。本实践练习将带您直面这一问题，通过引入“替代梯度”（surrogate gradients）的概念来解决。您将亲手实现两种主流的替代梯度方法——直通估计器（Straight-Through Estimator, STE）和一个平滑的S型函数近似——并应用BPTT计算权重梯度，通过比较这两种方法的差异，您将对替代梯度在SNN训练中的作用和特性有更深刻的理解 。",
            "id": "4036243",
            "problem": "考虑一个单神经元脉冲神经网络 (SNN)，其包含一个离散时间的漏积分发放 (LIF) 神经元。膜电位 $v_t$ 根据差分方程演化\n$$v_{t+1} = \\beta v_t + w x_t - s_t \\theta,$$\n其中 $t \\in \\{0,1,\\dots,T-1\\}$，$v_0 = 0$，$w$ 是一个标量突触权重，$x_t$ 是一个已知的输入序列，$\\beta \\in (0,1)$ 是泄漏因子，$\\theta > 0$ 是脉冲阈值，$s_t \\in \\{0,1\\}$ 是在时间 $t$ 的脉冲，由赫维赛德阶跃函数 (Heaviside step function) $s_t = H(v_t - \\theta)$ 给出。损失定义为\n$$L = \\frac{1}{2}\\sum_{t=0}^{T-1} \\left(s_t - y_t\\right)^2,$$\n其中 $y_t \\in \\{0,1\\}$ 是一个预设的目标脉冲序列。任务是通过时间反向传播 (BPTT) 计算梯度 $\\partial L / \\partial w$，并使用两种不同的代理导数来替代 $\\partial s_t / \\partial v_t$：\n1. 直通估计器 (Straight-Through Estimator, STE): $$\\frac{\\partial s_t}{\\partial v_t} \\approx \\mathbf{1}_{|v_t - \\theta| < \\delta},$$ 其中 $\\delta > 0$ 定义了指示函数窗口。\n2. 真实脉冲函数的光滑近似：定义 $h_k(v) = \\sigma(k(v - \\theta))$，其中 $\\sigma(u) = \\frac{1}{1+e^{-u}}$，并使用 $$\\frac{d h_k(v)}{dv} = k \\sigma(k(v - \\theta))\\left(1 - \\sigma(k(v - \\theta))\\right)$$ 作为 $\\frac{\\partial s_t}{\\partial v_t}$ 的代理，其中陡峭度参数 $k > 0$。\n\n仅从给定的神经元动力学、赫维赛德定义和损失函数出发，推导用于计算 $\\partial v_t / \\partial w$ 的 BPTT 递推关系，以及用所选代理导数表示的 $\\partial L / \\partial w$ 的表达式。实现这两种梯度计算，并通过报告每个测试案例的单个标量来分析引起的梯度不匹配：\n$$r = \\frac{\\left|\\left(\\partial L / \\partial w\\right)_{\\text{STE}} - \\left(\\partial L / \\partial w\\right)_{k}\\right|}{\\left|\\left(\\partial L / \\partial w\\right)_{k}\\right| + \\varepsilon},$$\n其中 $\\varepsilon = 10^{-12}$ 是一个数值稳定器。\n\n在所有测试案例中使用以下固定的 SNN 参数：$T = 12$，$\\beta = 0.9$，$\\theta = 1.0$，$v_0 = 0$，以及一个固定的目标脉冲序列 $y_t$，在时间 $t = 4$ 和 $t = 9$ (零基索引) 处有单位脉冲，其他地方 $y_t = 0$。\n\n对于输入序列，使用基于类型标签和振幅 $A$ 的确定性、时间索引的定义：\n- 类型 \"pulses2\": 当 $t \\in \\{3,7\\}$ 时 $x_t = A$，否则 $x_t = 0$。\n- 类型 \"pulses4\": 当 $t \\in \\{2,4,6,8\\}$ 时 $x_t = A$，否则 $x_t = 0$。\n- 类型 \"single\": 当 $t = 4$ 时 $x_t = A$，否则 $x_t = 0$。\n- 类型 \"zero\": 对所有 $t$，$x_t = 0$。\n- 类型 \"constant\": 对所有 $t$，$x_t = A$。\n\n您的程序必须为以下每个测试案例计算 $r$，每个案例指定为 $(\\delta, k, w, \\text{type}, A)$：\n- 案例 1: $(0.05, 100.0, 0.8, \\text{\"pulses2\"}, 1.2)$。\n- 案例 2: $(0.2, 100.0, 0.8, \\text{\"pulses2\"}, 1.2)$。\n- 案例 3: $(0.05, 20.0, 1.2, \\text{\"pulses4\"}, 1.5)$。\n- 案例 4: $(0.5, 10.0, 0.3, \\text{\"single\"}, 2.0)$。\n- 案例 5: $(0.01, 200.0, 0.8, \\text{\"zero\"}, 0.0)$。\n- 案例 6: $(0.2, 50.0, 1.0, \\text{\"constant\"}, 0.5)$。\n\n您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果 (例如, $[r_1,r_2,\\dots,r_6]$)。每个 $r_i$ 都必须是浮点数。不应打印任何额外文本。",
            "solution": "该问题已经过验证，被认为是可靠、适定且具有科学依据的。它提出了神经形态计算领域的一个标准任务：使用基于梯度的方法训练脉冲神经网络 (SNN)，这需要解决脉冲生成机制的不可微性问题。所有参数、模型和目标都已明确定义。\n\n问题的核心是计算损失函数 $L$ 相对于突触权重 $w$ 的梯度，表示为 $\\frac{\\partial L}{\\partial w}$。损失函数定义在脉冲序列上：\n$$L = \\frac{1}{2}\\sum_{t=0}^{T-1} (s_t - y_t)^2$$\n其中 $s_t$ 是神经元在时间 $t$ 的输出脉冲，$y_t$ 是目标脉冲。\n\n使用链式法则，梯度可以表示为：\n$$\\frac{\\partial L}{\\partial w} = \\sum_{t=0}^{T-1} \\frac{\\partial L}{\\partial s_t} \\frac{d s_t}{d w}$$\n第一项可以从损失函数的定义中直接计算：\n$$\\frac{\\partial L}{\\partial s_t} = s_t - y_t$$\n第二项 $\\frac{d s_t}{d w}$ 需要进一步展开。输出脉冲 $s_t$ 是膜电位 $v_t$ 的函数，而膜电位 $v_t$ 又是权重 $w$ 的函数。再次应用链式法则：\n$$\\frac{d s_t}{d w} = \\frac{d s_t}{d v_t} \\frac{d v_t}{d w}$$\n项 $\\frac{d s_t}{d v_t}$ 表示脉冲生成函数 $s_t = H(v_t - \\theta)$ 的导数，其中 $H$ 是赫维赛德阶跃函数 (Heaviside step function)。其数学导数几乎处处为零，在阈值处是一个未定义的狄拉克δ函数 (Dirac delta function)，不适用于基于梯度的学习。为解决此问题，我们用一个连续的代理函数替换它，记为 $\\psi(v_t)$：\n$$\\frac{d s_t}{d v_t} \\approx \\psi(v_t)$$\n该问题要求为 $\\psi(v_t)$ 使用两种不同的代理：\n1.  **直通估计器 (STE)**: $\\psi(v_t) = \\mathbf{1}_{|v_t - \\theta| < \\delta}$，其中 $\\mathbf{1}$ 是指示函数。这将导数近似为在阈值 $\\theta$ 周围宽度为 $2\\delta$ 的小窗口内为常数值 $1$，在窗口外为 $0$。\n2.  **平滑 Sigmoid 导数**: $\\psi(v_t) = \\frac{d h_k(v_t)}{d v_t} = k \\sigma(k(v_t - \\theta))(1 - \\sigma(k(v_t - \\theta)))$，其中 $\\sigma(u) = (1+e^{-u})^{-1}$ 是 logistic sigmoid 函数。这使用了阶跃函数的光滑近似的导数。\n\n剩下需要确定的项是 $\\frac{d v_t}{d w}$，即膜电位在时间 $t$ 对权重 $w$ 的全导数。我们可以通过对神经元的动力学方程求导来推导该项的递推关系：\n$$v_{t+1} = \\beta v_t + w x_t - s_t \\theta$$\n让我们定义 $g_t := \\frac{d v_t}{d w}$。对动力学方程关于 $w$ 求导可得：\n$$\\frac{d v_{t+1}}{d w} = \\frac{d}{d w} (\\beta v_t + w x_t - s_t \\theta)$$\n$$g_{t+1} = \\beta \\frac{d v_t}{d w} + x_t - \\theta \\frac{d s_t}{d w}$$\n代入 $\\frac{d s_t}{d w} = \\psi(v_t) \\frac{d v_t}{d w} = \\psi(v_t) g_t$：\n$$g_{t+1} = \\beta g_t + x_t - \\theta \\psi(v_t) g_t$$\n这给出了 $g_t$ 的前向递推关系：\n$$g_{t+1} = (\\beta - \\theta \\psi(v_t)) g_t + x_t$$\n初始条件基于 $v_0 = 0$，它是一个常数，不依赖于 $w$。因此，递推的初始条件是 $g_0 = \\frac{d v_0}{d w} = 0$。\n\n将这些部分组合起来，总梯度的最终表达式为：\n$$\\frac{\\partial L}{\\partial w} \\approx \\sum_{t=0}^{T-1} (s_t - y_t) \\psi(v_t) g_t$$\n\n计算梯度的总体算法如下：\n\n1.  **前向传播**: 模拟神经元动力学，以获得在 $t \\in \\{0, 1, \\dots, T-1\\}$ 期间的膜电位 $v_t$ 和脉冲 $s_t$ 的历史记录。\n    - 初始化 $v_0 = 0$。\n    - 对于 $t = 0, \\dots, T-1$:\n        - 如果 $v_t \\ge \\theta$，$s_t = 1$，否则 $s_t = 0$。\n        - $v_{t+1} = \\beta v_t + w x_t - s_t \\theta$。\n    - 存储序列 $v_0, \\dots, v_{T-1}$ 和 $s_0, \\dots, s_{T-1}$。\n\n2.  **梯度计算**: 计算每个时间步的梯度贡献并累加。这涉及到第二次前向传播来计算序列 $g_t$。\n    - 初始化总梯度 $\\frac{\\partial L}{\\partial w} = 0$。\n    - 初始化电位的梯度 $g_0 = 0$。\n    - 对于 $t = 0, \\dots, T-1$:\n        - 根据所选方法 (STE 或 Smoothed) 计算代理导数 $\\psi(v_t)$。\n        - 更新总梯度：$\\frac{\\partial L}{\\partial w} \\leftarrow \\frac{\\partial L}{\\partial w} + (s_t - y_t) \\psi(v_t) g_t$。\n        - 更新下一个时间步的电位梯度：$g_{t+1} = (\\beta - \\theta \\psi(v_t)) g_t + x_t$。\n\n对每种代理梯度方法执行一次此过程，以获得 $(\\partial L / \\partial w)_{\\text{STE}}$ 和 $(\\partial L / \\partial w)_{k}$。然后按规定计算相对差异度量 $r$。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import expit\n\ndef generate_input_sequence(seq_type, A, T):\n    \"\"\"Generates the input sequence x_t.\"\"\"\n    x = np.zeros(T)\n    if seq_type == \"pulses2\":\n        if 3 < T: x[3] = A\n        if 7 < T: x[7] = A\n    elif seq_type == \"pulses4\":\n        indices = [2, 4, 6, 8]\n        for i in indices:\n            if i < T:\n                x[i] = A\n    elif seq_type == \"single\":\n        if 4 < T: x[4] = A\n    elif seq_type == \"zero\":\n        pass  # x is already all zeros\n    elif seq_type == \"constant\":\n        x[:] = A\n    else:\n        raise ValueError(\"Unknown sequence type\")\n    return x\n\ndef calculate_gradient(w, seq_type, A, surrogate_choice, surrogate_param):\n    \"\"\"\n    Calculates the gradient dL/dw for a single-neuron SNN.\n    \n    Args:\n        w (float): The synaptic weight.\n        seq_type (str): The type of input sequence.\n        A (float): The amplitude of the input sequence.\n        surrogate_choice (str): 'STE' or 'Smoothed'.\n        surrogate_param (float): Delta for STE or k for Smoothed.\n\n    Returns:\n        float: The calculated gradient dL/dw.\n    \"\"\"\n    # Fixed SNN parameters\n    T = 12\n    beta = 0.9\n    theta = 1.0\n    v0 = 0.0\n\n    # Target spike train\n    y = np.zeros(T)\n    y[4] = 1.0\n    y[9] = 1.0\n\n    # Input sequence\n    x = generate_input_sequence(seq_type, A, T)\n\n    # --- 1. Forward Pass: Simulate neuron dynamics ---\n    v_hist = np.zeros(T)\n    s_hist = np.zeros(T)\n    v = v0\n\n    for t in range(T):\n        v_hist[t] = v\n        s = 1.0 if v >= theta else 0.0\n        s_hist[t] = s\n        v = beta * v + w * x[t] - s * theta\n\n    # --- 2. Gradient Calculation (Forward Mode) ---\n    grad_L_w = 0.0\n    g = 0.0  # g_0 = d(v_0)/dw = 0\n\n    for t in range(T):\n        v_t = v_hist[t]\n        \n        # Calculate surrogate derivative psi(v_t)\n        if surrogate_choice == 'STE':\n            delta = surrogate_param\n            psi = 1.0 if np.abs(v_t - theta) < delta else 0.0\n        elif surrogate_choice == 'Smoothed':\n            k = surrogate_param\n            u = k * (v_t - theta)\n            sigma_u = expit(u)\n            psi = k * sigma_u * (1.0 - sigma_u)\n        else:\n            raise ValueError(\"Invalid surrogate choice\")\n            \n        # Accumulate gradient\n        grad_L_w += (s_hist[t] - y[t]) * psi * g\n        \n        # Update g for the next time step\n        g = (beta - theta * psi) * g + x[t]\n        \n    return grad_L_w\n    \n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and compute the gradient mismatch ratio r.\n    \"\"\"\n    test_cases = [\n        # (delta, k, w, type, A)\n        (0.05, 100.0, 0.8, \"pulses2\", 1.2),\n        (0.2, 100.0, 0.8, \"pulses2\", 1.2),\n        (0.05, 20.0, 1.2, \"pulses4\", 1.5),\n        (0.5, 10.0, 0.3, \"single\", 2.0),\n        (0.01, 200.0, 0.8, \"zero\", 0.0),\n        (0.2, 50.0, 1.0, \"constant\", 0.5),\n    ]\n\n    results = []\n    epsilon = 1e-12\n\n    for case in test_cases:\n        delta, k, w, seq_type, A = case\n        \n        grad_ste = calculate_gradient(w, seq_type, A, 'STE', delta)\n        grad_k = calculate_gradient(w, seq_type, A, 'Smoothed', k)\n        \n        r = np.abs(grad_ste - grad_k) / (np.abs(grad_k) + epsilon)\n        results.append(r)\n\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "实现像BPTT这样复杂的算法时，确保其正确性至关重要。一个微小的错误就可能导致训练失败且难以察觉。本实践练习将向您介绍“梯度检验”（gradient checking），这是一种通过数值方法验证解析梯度计算正确性的强大技术。您将为一个使用了“软重置”（soft reset）和可微激活函数的SNN模型实现BPTT，并将其计算出的梯度与通过“有限差分法”（finite-difference method）得到的数值估计进行比较，从而验证您BPTT实现的正确性 。",
            "id": "4036196",
            "problem": "考虑一个单神经元离散时间漏式整合发放（Leaky Integrate-and-Fire, LIF）脉冲神经网络（Spiking Neural Network, SNN），该网络具有单一突触权重 $w$，并由外部输入序列驱动。使用随时间反向传播（Backpropagation Through Time, BPTT）算法，并为脉冲生成的非线性部分引入一个可微代理函数，以计算关于 $w$ 的解析梯度，并将其与同一梯度的有限差分估计值进行比较。然后，量化这两种梯度估计值之间的相对误差。\n\n基本原理和定义：LIF 神经元的状态根据以下在时间步 $t$ 的离散时间更新规则进行演化：\n1. 重置前膜电位为 $V_t^{\\mathrm{pre}} = \\alpha V_{t-1} + w x_t$，其中 $V_{t-1}$ 是上一步重置后的膜电位，$\\alpha \\in (0,1)$ 是一个泄漏因子，$x_t$ 是时间 $t$ 的输入。初始膜电位为 $V_0 = 0$。\n2. 脉冲激活为 $z_t = s(u_t)$，其中 $u_t = V_t^{\\mathrm{pre}} - V_{\\mathrm{th}}$，$V_{\\mathrm{th}}$ 是一个固定的阈值，$s(u)$ 是亥维赛阶跃函数的一个可微代理函数。使用标准的 logistic 函数 $s(u) = \\frac{1}{1 + \\exp(-\\beta u)}$，其斜率参数为 $\\beta > 0$；其导数为 $s'(u) = \\beta s(u)(1 - s(u))$。\n3. 通过使用代理脉冲激活对膜电位进行门控，实现软重置：$V_t = (1 - z_t) V_t^{\\mathrm{pre}}$。\n\n在长度为 $T$ 的时间范围内，损失是脉冲激活与目标序列 $y_t$ 之间的二次误差：\n$$\n\\mathcal{L}(w) = \\frac{1}{2} \\sum_{t=1}^T \\left(z_t - y_t\\right)^2.\n$$\n\n任务：\n1. 使用斜率为 $\\beta$ 的可微代理函数 $s(u)$，实现上述前向动力学过程。\n2. 推导并实现随时间反向传播（BPTT）算法，通过在由 $V_t^{\\mathrm{pre}}$、$z_t$ 和 $V_t$ 定义的计算图上应用链式法则，计算解析梯度 $\\frac{\\partial \\mathcal{L}}{\\partial w}$。\n3. 使用扰动 $\\varepsilon > 0$ 计算梯度的中心有限差分估计值：\n$$\ng_{\\mathrm{fd}}(w) = \\frac{\\mathcal{L}(w + \\varepsilon) - \\mathcal{L}(w - \\varepsilon)}{2 \\varepsilon}.\n$$\n4. 计算解析梯度 $g_{\\mathrm{bptt}}(w)$ 和有限差分估计值 $g_{\\mathrm{fd}}(w)$ 之间的相对误差，公式如下：\n$$\n\\mathrm{err} = \\frac{\\left|g_{\\mathrm{bptt}}(w) - g_{\\mathrm{fd}}(w)\\right|}{\\left|g_{\\mathrm{fd}}(w)\\right| + 10^{-12}}.\n$$\n5. 对每个测试用例，输出相对误差，结果为浮点数。\n\n科学真实性：使用对于所述模型而言合理且自洽的固定序列和参数。请注意，在前向传播中使用可微代理函数对于有意义的有限差分梯度检验至关重要。\n\n您的程序应生成单行输出，其中包含用方括号括起来的、以逗号分隔的结果列表（例如，“[result1,result2,result3]”）。\n\n测试套件：\n使用以下参数集来测试不同的机制和边界情况。在所有情况下，除非另有说明，初始膜电位均为 $V_0 = 0$，泄漏因子为 $\\alpha = 0.9$，阈值为 $V_{\\mathrm{th}} = 1.0$，时间范围长度为 $T = 6$。时间 $t = 1, \\ldots, T$ 的输入和目标序列为：\n$$\nx_{1:T} = [0, 1, 0.5, 0, 1, 0], \\quad y_{1:T} = [0, 1, 0, 0, 1, 0].\n$$\n\n- 情况1（理想情况，中等斜率和有限差分步长）：$w = 0.8$，$\\beta = 4.0$，$\\varepsilon = 10^{-4}$。\n- 情况2（使用非常小扰动的有限差分边界）：$w = 0.8$，$\\beta = 4.0$，$\\varepsilon = 10^{-8}$。\n- 情况3（陡峭的代理函数，接近硬阈值）：$w = 0.8$，$\\beta = 50.0$，$\\varepsilon = 10^{-4}$。\n- 情况4（接近阈值的权重以引发敏感性）：$w = 1.2$，$\\beta = 4.0$，$\\varepsilon = 10^{-4}$。\n\n答案规格：\n- 对于每种情况，计算如上定义的单个浮点数值 $\\mathrm{err}$。\n- 最终输出必须是单行文本，包含一个按上述情况顺序排列的四个误差的列表，格式为“[e1,e2,e3,e4]”。",
            "solution": "该问题被评估为有效。它在计算神经科学领域，特别是在脉冲神经网络的训练方面，具有科学依据。该问题定义明确，提供了所有必要的数学定义、参数和输入，以获得唯一且可验证的解。语言客观、正式。\n\n问题的核心是在单神经元漏式整合发放（LIF）模型中，计算损失函数 $\\mathcal{L}$ 关于突触权重 $w$ 的梯度。这将通过使用随时间反向传播（BPTT）进行解析计算，并使用有限差分法进行数值计算来完成。\n\n首先，我们建立模型动力学和损失函数。神经元的状态在离散时间步 $t=1, \\dots, T$ 上演化。\n\n时间 $t$ 的重置前膜电位由下式给出：\n$$\nV_t^{\\mathrm{pre}} = \\alpha V_{t-1} + w x_t\n$$\n其中 $V_{t-1}$ 是上一步重置后的膜电位（$V_0 = 0$），$\\alpha$ 是泄漏因子，$w$ 是权重，$x_t$ 是外部输入。\n\n代理脉冲激活 $z_t$ 是 $V_t^{\\mathrm{pre}}$ 的一个可微函数：\n$$\nz_t = s(u_t) = s(V_t^{\\mathrm{pre}} - V_{\\mathrm{th}}) = \\frac{1}{1 + \\exp(-\\beta (V_t^{\\mathrm{pre}} - V_{\\mathrm{th}}))}\n$$\nsigmoid 函数 $s(u)$ 的导数是 $s'(u) = \\beta s(u)(1 - s(u))$，用 $z_t$ 表示为 $s'(u_t) = \\beta z_t(1 - z_t)$。\n\n膜电位通过软重置机制进行更新：\n$$\nV_t = (1 - z_t) V_t^{\\mathrm{pre}}\n$$\n\n在时间范围 $T$ 内的总损失 $\\mathcal{L}$ 是脉冲激活 $z_t$ 和目标序列 $y_t$ 之间平方误差的总和：\n$$\n\\mathcal{L}(w) = \\sum_{t=1}^T \\mathcal{L}_t = \\frac{1}{2} \\sum_{t=1}^T (z_t - y_t)^2\n$$\n\n我们的目标是计算梯度 $\\frac{\\partial \\mathcal{L}}{\\partial w}$。由于损失是随时间累加的，其梯度也是一个和：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial w} = \\sum_{t=1}^T \\frac{\\partial \\mathcal{L}}{\\partial w} (\\text{at time } t)\n$$\n我们使用链式法则来追踪 $w$ 对损失的影响。在每个时间步 $t$，$w$ 通过重置前膜电位 $V_t^{\\mathrm{pre}}$ 来影响 $\\mathcal{L}$。\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial w} = \\sum_{t=1}^T \\frac{\\partial \\mathcal{L}}{\\partial V_t^{\\mathrm{pre}}} \\frac{\\partial V_t^{\\mathrm{pre}}}{\\partial w}\n$$\n根据 $V_t^{\\mathrm{pre}}$ 的定义，偏导数 $\\frac{\\partial V_t^{\\mathrm{pre}}}{\\partial w}$ 就是 $x_t$。这是局部偏导数，而不是全导数，因为我们正在展开计算图。\n因此，总梯度可以写为：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial w} = \\sum_{t=1}^T \\delta_t^{V^{\\mathrm{pre}}} x_t\n$$\n其中我们定义 $\\delta_t^{V^{\\mathrm{pre}}} := \\frac{\\partial \\mathcal{L}}{\\partial V_t^{\\mathrm{pre}}}$ 为总损失关于时间 $t$ 的重置前电位的梯度。BPTT 算法提供了一种通过在时间上反向传播梯度来高效计算这些 $\\delta_t^{V^{\\mathrm{pre}}}$ 项的方法。\n\n让我们来推导梯度的递推关系。总损失 $\\mathcal{L}$ 通过两条路径依赖于 $V_t^{\\mathrm{pre}}$：\n1.  直接通过 $z_t$，它影响损失项 $\\mathcal{L}_t$。\n2.  间接通过 $V_t$，它影响所有未来的损失项 $\\mathcal{L}_{t+1}, \\dots, \\mathcal{L}_T$。\n\n所以，我们可以将 $\\delta_t^{V^{\\mathrm{pre}}}$ 写为：\n$$\n\\delta_t^{V^{\\mathrm{pre}}} = \\frac{\\partial \\mathcal{L}}{\\partial V_t^{\\mathrm{pre}}} = \\frac{\\partial \\mathcal{L}_t}{\\partial V_t^{\\mathrm{pre}}} + \\frac{\\partial \\left( \\sum_{k=t+1}^T \\mathcal{L}_k \\right)}{\\partial V_t^{\\mathrm{pre}}}\n$$\n第一项是来自当前时间步损失的梯度：\n$$\n\\frac{\\partial \\mathcal{L}_t}{\\partial V_t^{\\mathrm{pre}}} = \\frac{\\partial \\mathcal{L}_t}{\\partial z_t} \\frac{\\partial z_t}{\\partial u_t} \\frac{\\partial u_t}{\\partial V_t^{\\mathrm{pre}}} = (z_t - y_t) \\cdot s'(u_t) \\cdot 1 = (z_t - y_t) s'(u_t)\n$$\n第二项代表从未来反向流动的梯度。这个梯度通过重置后电位 $V_t$ 进行传播。我们定义 $\\delta_t^V := \\frac{\\partial \\mathcal{L}}{\\partial V_t}$ 为总损失关于 $V_t$ 的梯度。链式法则给出：\n$$\n\\frac{\\partial \\left( \\sum_{k=t+1}^T \\mathcal{L}_k \\right)}{\\partial V_t^{\\mathrm{pre}}} = \\frac{\\partial \\mathcal{L}}{\\partial V_t} \\frac{\\partial V_t}{\\partial V_t^{\\mathrm{pre}}} = \\delta_t^V \\frac{\\partial V_t}{\\partial V_t^{\\mathrm{pre}}}\n$$\n我们计算重置机制的偏导数：\n$$\n\\frac{\\partial V_t}{\\partial V_t^{\\mathrm{pre}}} = \\frac{\\partial}{\\partial V_t^{\\mathrm{pre}}} \\left[ (1 - z_t) V_t^{\\mathrm{pre}} \\right] = (1 - z_t) \\cdot 1 + V_t^{\\mathrm{pre}} \\cdot \\left( -\\frac{\\partial z_t}{\\partial V_t^{\\mathrm{pre}}} \\right) = (1 - z_t) - V_t^{\\mathrm{pre}} s'(u_t)\n$$\n将这些项结合起来，得到 $\\delta_t^{V^{\\mathrm{pre}}}$ 的表达式：\n$$\n\\delta_t^{V^{\\mathrm{pre}}} = (z_t - y_t) s'(u_t) + \\delta_t^V \\left[ (1 - z_t) - V_t^{\\mathrm{pre}} s'(u_t) \\right]\n$$\n为了完成递推，我们需要将 $\\delta_t^V$ 与时间 $t+1$ 的梯度联系起来。电位 $V_t$ 仅通过 $V_{t+1}^{\\mathrm{pre}}$ 影响未来。\n$$\n\\delta_t^V = \\frac{\\partial \\mathcal{L}}{\\partial V_t} = \\frac{\\partial \\mathcal{L}}{\\partial V_{t+1}^{\\mathrm{pre}}} \\frac{\\partial V_{t+1}^{\\mathrm{pre}}}{\\partial V_t} = \\delta_{t+1}^{V^{\\mathrm{pre}}} \\cdot \\alpha\n$$\n这就为我们提供了一种方法，可以从反向传播上一步计算出的 $\\delta_{t+1}^{V^{\\mathrm{pre}}}$ 来计算 $\\delta_t^V$。\n\nBPTT 算法的步骤如下：\n1.  **前向传播**：对于 $t = 1, \\dots, T$，使用模型的动力学方程计算并存储序列 $V_t^{\\mathrm{pre}}$、$u_t$、$z_t$ 和 $V_t$。初始状态为 $V_0 = 0$。\n2.  **反向传播**：初始化总梯度 $\\frac{\\partial \\mathcal{L}}{\\partial w} = 0$ 和传入的电位梯度 $\\delta_T^V = \\frac{\\partial \\mathcal{L}}{\\partial V_T} = 0$。从 $t = T$ 反向迭代到 $1$：\n    a. 使用上面推导的公式计算 $\\delta_t^{V^{\\mathrm{pre}}}$，其中 $\\delta_t^V$ 是从步骤 $t+1$ 传播来的梯度。\n    b. 更新总权重梯度：$\\frac{\\partial \\mathcal{L}}{\\partial w} \\leftarrow \\frac{\\partial \\mathcal{L}}{\\partial w} + \\delta_t^{V^{\\mathrm{pre}}} x_t$。\n    c. 计算要传递到上一步的梯度：$\\delta_{t-1}^V = \\delta_t^{V^{\\mathrm{pre}}} \\alpha$。这成为循环下一次迭代（在时间 $t-1$）的新的 $\\delta_t^V$。\n\n得到的 $\\frac{\\partial \\mathcal{L}}{\\partial w}$ 的值就是解析梯度 $g_{\\mathrm{bptt}}(w)$。\n\n为了验证，我们使用中心有限差分公式计算数值梯度：\n$$\ng_{\\mathrm{fd}}(w) = \\frac{\\mathcal{L}(w + \\varepsilon) - \\mathcal{L}(w - \\varepsilon)}{2 \\varepsilon}\n$$\n这涉及到使用扰动后的权重 $w+\\varepsilon$ 和 $w-\\varepsilon$ 运行两次前向模拟，以找到相应的损失。\n\n最后，解析估计和数值估计之间的相对误差计算如下：\n$$\n\\mathrm{err} = \\frac{\\left|g_{\\mathrm{bptt}}(w) - g_{\\mathrm{fd}}(w)\\right|}{\\left|g_{\\mathrm{fd}}(w)\\right| + 10^{-12}}\n$$\n分母中的小常数 $10^{-12}$ 确保了当 $g_{\\mathrm{fd}}(w)$ 接近零时的数值稳定性。实现将对每个指定的测试用例执行这整个过程。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the BPTT for LIF neuron problem for a suite of test cases.\n    \"\"\"\n    \n    # Fixed parameters for all test cases as per the problem statement\n    ALPHA = 0.9\n    V_TH = 1.0\n    T = 6\n    # Sequences are padded with a 0 at index 0 to align with 1-based time indexing t=1..T\n    X_SEQ = np.array([0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0])\n    Y_SEQ = np.array([0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0])\n    V0 = 0.0\n\n    def sigmoid(u, beta):\n        \"\"\"Computes the logistic sigmoid function.\"\"\"\n        # Note: Using np.exp is essential for vectorized operations with numpy arrays.\n        return 1.0 / (1.0 + np.exp(-beta * u))\n\n    def run_simulation(w, beta):\n        \"\"\"\n        Performs the forward pass of the LIF neuron simulation.\n        \n        Args:\n            w (float): The synaptic weight.\n            beta (float): The slope parameter of the surrogate spike function.\n\n        Returns:\n            tuple: A tuple containing:\n                - loss (float): The total loss over the time horizon.\n                - history (dict): A dictionary of stored states (V, V_pre, u, z).\n        \"\"\"\n        V_hist = np.zeros(T + 1)\n        V_pre_hist = np.zeros(T + 1)\n        u_hist = np.zeros(T + 1)\n        z_hist = np.zeros(T + 1)\n\n        V_hist[0] = V0\n\n        # Forward pass from t=1 to T\n        for t in range(1, T + 1):\n            V_pre_hist[t] = ALPHA * V_hist[t - 1] + w * X_SEQ[t]\n            u_hist[t] = V_pre_hist[t] - V_TH\n            z_hist[t] = sigmoid(u_hist[t], beta)\n            V_hist[t] = (1 - z_hist[t]) * V_pre_hist[t]\n\n        # Calculate loss. Slicing with [1:] ignores the dummy 0th element.\n        loss = 0.5 * np.sum((z_hist[1:] - Y_SEQ[1:])**2)\n\n        history = {\n            \"V_pre\": V_pre_hist,\n            \"u\": u_hist,\n            \"z\": z_hist,\n        }\n        \n        return loss, history\n\n    def compute_bptt_gradient(w, beta):\n        \"\"\"\n        Computes the analytic gradient dL/dw using Backpropagation Through Time.\n        \n        Args:\n            w (float): The synaptic weight.\n            beta (float): The slope parameter.\n\n        Returns:\n            float: The analytic gradient g_bptt(w).\n        \"\"\"\n        _, history = run_simulation(w, beta)\n        \n        V_pre = history[\"V_pre\"]\n        z = history[\"z\"]\n\n        dL_dw = 0.0\n        # dL_dV is the gradient dL/dV_t propagated from the future (t+1) to the present (t).\n        # We start with dL/dV_T = 0, as V_T has no future influence.\n        dL_dV = 0.0\n        \n        # Backward pass from t=T down to 1\n        for t in range(T, 0, -1):\n            s_prime = beta * z[t] * (1.0 - z[t])\n            \n            # Gradient of loss w.r.t. V_pre_t.\n            # This combines the local gradient from z_t and the propagated gradient from V_t.\n            dL_dV_pre = (z[t] - Y_SEQ[t]) * s_prime + dL_dV * ((1.0 - z[t]) - V_pre[t] * s_prime)\n\n            # Accumulate gradient dL/dw from the current time step\n            dL_dw += dL_dV_pre * X_SEQ[t]\n            \n            # Propagate gradient to V_{t-1} for the next iteration.\n            # This becomes the new dL_dV for time step t-1.\n            dL_dV = dL_dV_pre * ALPHA\n            \n        return dL_dw\n\n    def compute_fd_gradient(w, beta, epsilon):\n        \"\"\"\n        Computes the numerical gradient dL/dw using the central finite-difference method.\n        \n        Args:\n            w (float): The synaptic weight.\n            beta (float): The slope parameter.\n            epsilon (float): The perturbation for the finite difference.\n\n        Returns:\n            float: The numerical gradient g_fd(w).\n        \"\"\"\n        loss_plus, _ = run_simulation(w + epsilon, beta)\n        loss_minus, _ = run_simulation(w - epsilon, beta)\n        \n        g_fd = (loss_plus - loss_minus) / (2.0 * epsilon)\n        return g_fd\n\n    # Test suite from the problem statement\n    test_cases = [\n        # (w, beta, epsilon)\n        (0.8, 4.0, 1e-4),  # Case 1\n        (0.8, 4.0, 1e-8),  # Case 2\n        (0.8, 50.0, 1e-4), # Case 3\n        (1.2, 4.0, 1e-4),  # Case 4\n    ]\n\n    results = []\n    \n    for w, beta, epsilon in test_cases:\n        g_bptt = compute_bptt_gradient(w, beta)\n        g_fd = compute_fd_gradient(w, beta, epsilon)\n        \n        # Compute relative error\n        err = np.abs(g_bptt - g_fd) / (np.abs(g_fd) + 1e-12)\n        results.append(err)\n        \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}