{
    "hands_on_practices": [
        {
            "introduction": "Before we can apply backpropagation, we must first master the forward propagation of information through a spiking neuron. This exercise challenges you to implement the discrete-time dynamics of a Leaky Integrate-and-Fire (LIF) neuron, the fundamental building block of many SNNs. By meticulously tracking the neuron's state at each time step, you will construct the computational graph that is essential for the subsequent backward pass of BPTT .",
            "id": "4036252",
            "problem": "Consider a single Leaky Integrate-and-Fire (LIF) neuron evolving in discrete time under a known input current sequence. The membrane potential $v(t)$ in continuous time obeys the ordinary differential equation (ODE) $dv/dt = -\\frac{1}{\\tau} v(t) + k I(t)$, where $\\tau$ is a membrane time constant, $k$ is a gain (which can absorb biophysical constants such as membrane resistance), and $I(t)$ is the input current. A spike is emitted when the membrane potential reaches or exceeds a threshold. After a spike, the membrane potential is reset to a specified reset value. Backpropagation Through Time (BPTT) for Spiking Neural Networks (SNNs) requires an exact unrolled sequence of forward states over time to define the computational graph.\n\nAssume a uniform discrete-time step with an exponential-Euler discretization of the LIF dynamics. Let $\\alpha \\in (0,1)$ denote the discrete-time leak factor corresponding to the continuous-time decay over one step, $\\theta$ denote the firing threshold, and $V_{\\text{reset}}$ denote the hard reset potential. Let $I_t$ denote the discrete-time drive at step $t$, already scaled so that its contribution enters linearly in the pre-threshold membrane as defined by the chosen discretization. Define the Heaviside step function $H(x)$ to return $1$ when $x \\ge 0$ and $0$ otherwise. The neuron evolves as follows each discrete step: starting from the membrane potential $v_t$ at time index $t$, form a pre-threshold potential, determine a spike according to threshold crossing, and apply a hard reset to obtain the next-step membrane $v_{t+1}$. The exact ordering must be leak-and-input integration, thresholding, then reset. The unrolled state required by Backpropagation Through Time (BPTT) at each step $t$ must record the pre-integration state $v_t$, the pre-threshold integrated potential, the spike indicator $s_t$, and the post-reset state $v_{t+1}$.\n\nYour task is:\n- From the continuous-time LIF ODE $dv/dt = -\\frac{1}{\\tau} v(t) + k I(t)$, derive the discrete-time recurrence for the pre-threshold potential and the spike-reset rule under the stated exponential-Euler discretization and hard reset, expressed in terms of the given parameters $\\alpha$, $\\theta$, and $V_{\\text{reset}}$, and the sequence $I_t$.\n- Implement a program that, given $(\\alpha,\\theta,V_{\\text{reset}})$, an initial membrane potential $v_0$, and a sequence $I_t$ of length $T+1$ indexed by $t=0,\\dots,T$, computes and records for each $t=0,\\dots,T$: the pre-integration membrane $v_t$, the pre-threshold potential, the spike indicator $s_t \\in \\{0,1\\}$, and the post-reset membrane $v_{t+1}$, following the exact ordering: integrate, threshold, reset. Use the tie-breaking rule $s_t = 1$ when the pre-threshold potential equals $\\theta$ exactly.\n- Report, for each test case below, the exact unrolled state list required by BPTT as a list of lists, where the $t$-th entry is $[t, v_t, \\text{pre\\_threshold}, s_t, v_{t+1}]$ with $t$ as an integer, $v_t$ and the pre-threshold potential as floating-point values, $s_t$ as an integer $0$ or $1$, and $v_{t+1}$ as a floating-point value.\n\nUse the following test suite with scientifically plausible and self-consistent parameter values:\n1. Typical spiking case:\n   - $\\alpha = 0.9$, $\\theta = 1.0$, $V_{\\text{reset}} = 0.0$, $v_0 = 0.0$, $T = 5$, $I_t$ for $t=0,\\dots,5$ equals $[0.5, 0.6, 0.0, 0.0, 0.0, 0.0]$.\n2. Boundary threshold equality case (tests tie-breaking $H(0)=1$):\n   - $\\alpha = 0.5$, $\\theta = 1.0$, $V_{\\text{reset}} = 0.0$, $v_0 = 2.0$, $T = 2$, $I_t$ for $t=0,\\dots,2$ equals $[0.0, 0.0, 0.0]$.\n3. No-spike accumulation under leak:\n   - $\\alpha = 0.95$, $\\theta = 1.2$, $V_{\\text{reset}} = 0.0$, $v_0 = 0.1$, $T = 4$, $I_t$ for $t=0,\\dots,4$ equals $[0.1, 0.1, 0.1, 0.1, 0.1]$.\n4. Multiple spikes with nonzero reset:\n   - $\\alpha = 0.8$, $\\theta = 1.0$, $V_{\\text{reset}} = 0.2$, $v_0 = 0.0$, $T = 6$, $I_t$ for $t=0,\\dots,6$ equals $[1.0, 0.9, 0.9, 0.9, 0.0, 1.2, 0.0]$.\n\nFinal output format requirement:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element in this top-level list corresponds to one test case and must itself be a bracketed list containing $T+1$ entries of the form $[t,v_t,\\text{pre\\_threshold},s_t,v_{t+1}]$ in that order. For example, the outer list must look like $[ \\text{case1}, \\text{case2}, \\dots ]$ with no additional text. Angles are not used in this problem, and no physical units are required. All reported values must be numerical in the specified types.",
            "solution": "The problem has been validated and is deemed sound, well-posed, and scientifically grounded. It presents a standard task in computational neuroscience: the simulation of a Leaky Integrate-and-Fire (LIF) neuron model in discrete time, which is a fundamental step for algorithms like Backpropagation Through Time (BPTT).\n\n### Derivation of the Discrete-Time Recurrence\n\nThe dynamics of the membrane potential $v(t)$ are given by the continuous-time ordinary differential equation (ODE):\n$$ \\frac{dv}{dt} = -\\frac{1}{\\tau} v(t) + k I(t) $$\nThis is a first-order linear ODE. To discretize it over a time step of duration $\\Delta t$, we use the exponential-Euler method. This method provides an exact solution to the ODE over the interval $[t, t+\\Delta t]$ under the assumption that the input current $I(t)$ is constant over that interval.\n\nThe solution to the homogeneous equation $\\frac{dv}{dt} = -\\frac{1}{\\tau} v(t)$ is $v(t) = v(t_0) e^{-(t-t_0)/\\tau}$. Over one time step $\\Delta t$, the potential decays by a factor of $e^{-\\Delta t/\\tau}$. The problem defines this discrete-time leak factor as $\\alpha$:\n$$ \\alpha = e^{-\\Delta t/\\tau} $$\nFor the full non-homogeneous equation, the exact solution over a single time step from $t$ to $t+\\Delta t$, assuming $I(t)$ is a constant $I_t$ during this interval, is:\n$$ v(t+\\Delta t) = v(t) e^{-\\Delta t/\\tau} + k\\tau(1 - e^{-\\Delta t/\\tau}) I_t $$\nSubstituting $\\alpha$ gives:\n$$ v(t+\\Delta t) = \\alpha v(t) + k\\tau(1 - \\alpha) I_t $$\nThe problem states that the discrete-time input drive $I_t$ is \"already scaled so that its contribution enters linearly\". This implies that the term $k\\tau(1 - \\alpha)$ is absorbed into the provided input sequence $I_t$. Thus, the discrete-time update rule for the membrane potential before considering spiking and reset is simply the sum of the leaked prior potential and the scaled input.\n\nLet $v_t$ be the membrane potential at the beginning of time step $t$. Following the specified order of operations (integrate, threshold, reset), we first calculate the pre-threshold potential, which we will denote as $u_t$:\n$$ u_t = \\alpha v_t + I_t $$\nThis step represents the leak and integration of input current.\n\nNext, we apply the thresholding rule. A spike $s_t$ is emitted if the pre-threshold potential $u_t$ reaches or exceeds the threshold $\\theta$. Using the Heaviside step function $H(x)$, defined as $1$ for $x \\ge 0$ and $0$ otherwise, the spike indicator $s_t \\in \\{0, 1\\}$ is:\n$$ s_t = H(u_t - \\theta) $$\nThis formulation correctly implements the tie-breaking rule that a spike is generated if $u_t = \\theta$.\n\nFinally, we apply the hard reset mechanism to determine the membrane potential $v_{t+1}$ for the beginning of the next time step. If a spike was generated ($s_t = 1$), the potential is reset to $V_{\\text{reset}}$. If no spike occurred ($s_t = 0$), the potential carries over from its pre-threshold value $u_t$. This can be expressed as a single equation:\n$$ v_{t+1} = (1 - s_t) u_t + s_t V_{\\text{reset}} $$\n\n### Simulation Algorithm\n\nThe simulation proceeds iteratively from an initial state $v_0$ for time steps $t = 0, 1, \\dots, T$. For each time step $t$:\n\n1.  **Retrieve State and Input**: The state at the start of the step is $v_t$. The input for this step is $I_t$.\n2.  **Integrate**: Calculate the pre-threshold potential $u_t = \\alpha v_t + I_t$.\n3.  **Threshold**: Determine the spike output $s_t = 1$ if $u_t \\ge \\theta$, and $s_t = 0$ otherwise.\n4.  **Reset**: Calculate the post-reset potential for the next time step, $v_{t+1} = (1 - s_t) u_t + s_t V_{\\text{reset}}$.\n5.  **Record State for BPTT**: Store the tuple $[t, v_t, u_t, s_t, v_{t+1}]$.\n6.  **Advance Time**: The potential for the next step, $v_{t+1}$, becomes the starting potential for the iteration at $t+1$.\n\nThis procedure generates the complete unrolled sequence of states required for BPTT. The implementation will execute this algorithm for each of the provided test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# No other libraries are required for this problem.\n\ndef solve():\n    \"\"\"\n    Main function to run the LIF neuron simulations for all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        # 1. Typical spiking case\n        {\n            \"alpha\": 0.9, \"theta\": 1.0, \"v_reset\": 0.0, \"v0\": 0.0, \"T\": 5,\n            \"I\": [0.5, 0.6, 0.0, 0.0, 0.0, 0.0]\n        },\n        # 2. Boundary threshold equality case\n        {\n            \"alpha\": 0.5, \"theta\": 1.0, \"v_reset\": 0.0, \"v0\": 2.0, \"T\": 2,\n            \"I\": [0.0, 0.0, 0.0]\n        },\n        # 3. No-spike accumulation under leak\n        {\n            \"alpha\": 0.95, \"theta\": 1.2, \"v_reset\": 0.0, \"v0\": 0.1, \"T\": 4,\n            \"I\": [0.1, 0.1, 0.1, 0.1, 0.1]\n        },\n        # 4. Multiple spikes with nonzero reset\n        {\n            \"alpha\": 0.8, \"theta\": 1.0, \"v_reset\": 0.2, \"v0\": 0.0, \"T\": 6,\n            \"I\": [1.0, 0.9, 0.9, 0.9, 0.0, 1.2, 0.0]\n        }\n    ]\n\n    results = []\n    for case_params in test_cases:\n        alpha = case_params[\"alpha\"]\n        theta = case_params[\"theta\"]\n        v_reset = case_params[\"v_reset\"]\n        v0 = case_params[\"v0\"]\n        I_seq = case_params[\"I\"]\n        T = case_params[\"T\"]\n        \n        case_result = simulate_lif(alpha, theta, v_reset, v0, I_seq, T)\n        results.append(case_result)\n\n    # Final print statement in the exact required format.\n    # The default str() representation of lists is compliant with the required output format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef simulate_lif(alpha, theta, v_reset, v0, I_seq, T):\n    \"\"\"\n    Simulates a discrete-time Leaky Integrate-and-Fire (LIF) neuron.\n\n    Args:\n        alpha (float): Discrete-time leak factor.\n        theta (float): Firing threshold.\n        v_reset (float): Reset potential after a spike.\n        v0 (float): Initial membrane potential.\n        I_seq (list of float): Sequence of input currents of length T+1.\n        T (int): The final time index for the simulation (total steps is T+1).\n\n    Returns:\n        list of lists: The unrolled state history required for BPTT. Each inner list\n                       is of the form [t, v_t, pre_threshold_potential, s_t, v_{t+1}].\n    \"\"\"\n    recorded_states = []\n    v_current = float(v0)\n    \n    # The simulation runs for t from 0 to T, inclusive.\n    for t in range(T + 1):\n        # Input for the current time step\n        I_t = float(I_seq[t])\n        \n        # v_t is the potential at the beginning of the step.\n        v_t = v_current\n        \n        # 1. Integrate: leak and input to get pre-threshold potential.\n        # This is denoted as u_t in the derivation.\n        pre_threshold_potential = alpha * v_t + I_t\n        \n        # 2. Threshold: check for spike. s_t is the spike indicator.\n        # The condition includes equality, implementing H(x) where H(0)=1.\n        s_t = 1 if pre_threshold_potential >= theta else 0\n        \n        # 3. Reset: determine the next state v_{t+1}.\n        if s_t == 1:\n            v_next = v_reset\n        else:\n            v_next = pre_threshold_potential\n        \n        # Record the BPTT state for this step.\n        # Types are: int, float, float, int, float.\n        recorded_states.append([t, v_t, pre_threshold_potential, s_t, float(v_next)])\n        \n        # Update the current potential for the next iteration.\n        v_current = float(v_next)\n        \n    return recorded_states\n\nsolve()\n```"
        },
        {
            "introduction": "The primary obstacle to training SNNs with gradient descent is the non-differentiable nature of the spike generation event. This practice tackles this challenge head-on by having you implement Backpropagation Through Time (BPTT) using surrogate gradients, which create a \"pseudo-gradient\" to guide learning. You will compare two popular surrogates, the Straight-Through Estimator (STE) and a smooth sigmoid-based function, to understand how these approximations enable credit assignment in SNNs .",
            "id": "4036243",
            "problem": "Consider a single-neuron Spiking Neural Network (SNN) with a discrete-time Leaky Integrate-and-Fire (LIF) neuron. The membrane potential $v_t$ evolves according to the difference equation\n$$v_{t+1} = \\beta v_t + w x_t - s_t \\theta,$$\nwhere $t \\in \\{0,1,\\dots,T-1\\}$, $v_0 = 0$, $w$ is a scalar synaptic weight, $x_t$ is a known input sequence, $\\beta \\in (0,1)$ is the leak factor, $\\theta > 0$ is the spike threshold, and $s_t \\in \\{0,1\\}$ is the spike at time $t$ given by the Heaviside step function $s_t = H(v_t - \\theta)$. The loss is defined as\n$$L = \\frac{1}{2}\\sum_{t=0}^{T-1} \\left(s_t - y_t\\right)^2,$$\nwhere $y_t \\in \\{0,1\\}$ is a prescribed target spike train. The task is to compute the gradient $\\partial L / \\partial w$ via Backpropagation Through Time (BPTT) using two different surrogate derivatives for $\\partial s_t / \\partial v_t$:\n1. The Straight-Through Estimator (STE): $$\\frac{\\partial s_t}{\\partial v_t} \\approx \\mathbf{1}_{|v_t - \\theta| < \\delta},$$ where $\\delta > 0$ defines the indicator window.\n2. A smoothed approximation to the true spike function: define $h_k(v) = \\sigma(k(v - \\theta))$ with $\\sigma(u) = \\frac{1}{1+e^{-u}}$ and use $$\\frac{d h_k(v)}{dv} = k \\sigma(k(v - \\theta))\\left(1 - \\sigma(k(v - \\theta))\\right)$$ as the surrogate for $\\frac{\\partial s_t}{\\partial v_t}$, for steepness parameter $k > 0$.\n\nStarting only from the given neuron dynamics, the Heaviside definition, and the loss function, derive the BPTT recursion for $\\partial v_t / \\partial w$ and the expression for $\\partial L / \\partial w$ in terms of the chosen surrogate derivative. Implement both gradient computations and analyze the induced gradient mismatch by reporting a single scalar per test case:\n$$r = \\frac{\\left|\\left(\\partial L / \\partial w\\right)_{\\text{STE}} - \\left(\\partial L / \\partial w\\right)_{k}\\right|}{\\left|\\left(\\partial L / \\partial w\\right)_{k}\\right| + \\varepsilon},$$\nwhere $\\varepsilon = 10^{-12}$ is a numerical stabilizer.\n\nUse the following fixed SNN parameters across all test cases: $T = 12$, $\\beta = 0.9$, $\\theta = 1.0$, $v_0 = 0$, and a fixed target spike train $y_t$ with unit spikes at times $t = 4$ and $t = 9$ (zero-based indexing), and $y_t = 0$ elsewhere.\n\nFor the input sequences, use deterministic, time-indexed definitions based on a type label and amplitude $A$:\n- Type \"pulses2\": $x_t = A$ for $t \\in \\{3,7\\}$ and $x_t = 0$ otherwise.\n- Type \"pulses4\": $x_t = A$ for $t \\in \\{2,4,6,8\\}$ and $x_t = 0$ otherwise.\n- Type \"single\": $x_t = A$ for $t = 4$ and $x_t = 0$ otherwise.\n- Type \"zero\": $x_t = 0$ for all $t$.\n- Type \"constant\": $x_t = A$ for all $t$.\n\nYour program must compute $r$ for each of the following test cases, each specified as $(\\delta, k, w, \\text{type}, A)$:\n- Case $1$: $(0.05, 100.0, 0.8, \\text{\"pulses2\"}, 1.2)$.\n- Case $2$: $(0.2, 100.0, 0.8, \\text{\"pulses2\"}, 1.2)$.\n- Case $3$: $(0.05, 20.0, 1.2, \\text{\"pulses4\"}, 1.5)$.\n- Case $4$: $(0.5, 10.0, 0.3, \\text{\"single\"}, 2.0)$.\n- Case $5$: $(0.01, 200.0, 0.8, \\text{\"zero\"}, 0.0)$.\n- Case $6$: $(0.2, 50.0, 1.0, \\text{\"constant\"}, 0.5)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[r_1,r_2,\\dots,r_6]$). Each $r_i$ must be a floating-point number. No additional text should be printed.",
            "solution": "The problem has been validated and is deemed sound, well-posed, and scientifically grounded. It presents a standard task in the field of neuromorphic computing: training a Spiking Neural Network (SNN) using gradient-based methods, which requires addressing the non-differentiability of the spike generation mechanism. All parameters, models, and objectives are clearly defined.\n\nThe core of the problem is to compute the gradient of a loss function $L$ with respect to a synaptic weight $w$, denoted as $\\frac{\\partial L}{\\partial w}$. The loss function is defined over a sequence of spikes:\n$$L = \\frac{1}{2}\\sum_{t=0}^{T-1} (s_t - y_t)^2$$\nwhere $s_t$ is the neuron's output spike at time $t$ and $y_t$ is the target spike.\n\nUsing the chain rule, the gradient can be expressed as:\n$$\\frac{\\partial L}{\\partial w} = \\sum_{t=0}^{T-1} \\frac{\\partial L}{\\partial s_t} \\frac{d s_t}{d w}$$\nThe first term is straightforward to compute from the loss function definition:\n$$\\frac{\\partial L}{\\partial s_t} = s_t - y_t$$\nThe second term, $\\frac{d s_t}{d w}$, requires further expansion. The output spike $s_t$ is a function of the membrane potential $v_t$, which in turn is a function of the weight $w$. Applying the chain rule again:\n$$\\frac{d s_t}{d w} = \\frac{d s_t}{d v_t} \\frac{d v_t}{d w}$$\nThe term $\\frac{d s_t}{d v_t}$ represents the derivative of the spike generation function, $s_t = H(v_t - \\theta)$, where $H$ is the Heaviside step function. The mathematical derivative is zero almost everywhere and is an undefined Dirac delta function at the threshold, which is unsuitable for gradient-based learning. To resolve this, we replace it with a continuous surrogate function, which we denote as $\\psi(v_t)$:\n$$\\frac{d s_t}{d v_t} \\approx \\psi(v_t)$$\nThe problem requires using two distinct surrogates for $\\psi(v_t)$:\n1.  **Straight-Through Estimator (STE)**: $\\psi(v_t) = \\mathbf{1}_{|v_t - \\theta| < \\delta}$, where $\\mathbf{1}$ is the indicator function. This approximates the derivative as a constant value of $1$ within a small window of width $2\\delta$ around the threshold $\\theta$, and $0$ otherwise.\n2.  **Smoothed Sigmoid Derivative**: $\\psi(v_t) = \\frac{d h_k(v_t)}{d v_t} = k \\sigma(k(v_t - \\theta))(1 - \\sigma(k(v_t - \\theta)))$, where $\\sigma(u) = (1+e^{-u})^{-1}$ is the logistic sigmoid function. This uses the derivative of a smooth approximation of the step function.\n\nThe remaining term to be determined is $\\frac{d v_t}{d w}$, which is the total derivative of the membrane potential at time $t$ with respect to the weight $w$. We can derive a recurrence relation for this term by differentiating the neuron's dynamic equation:\n$$v_{t+1} = \\beta v_t + w x_t - s_t \\theta$$\nLet us define $g_t := \\frac{d v_t}{d w}$. Differentiating the dynamics with respect to $w$ yields:\n$$\\frac{d v_{t+1}}{d w} = \\frac{d}{d w} (\\beta v_t + w x_t - s_t \\theta)$$\n$$g_{t+1} = \\beta \\frac{d v_t}{d w} + x_t - \\theta \\frac{d s_t}{d w}$$\nSubstituting $\\frac{d s_t}{d w} = \\psi(v_t) \\frac{d v_t}{d w} = \\psi(v_t) g_t$:\n$$g_{t+1} = \\beta g_t + x_t - \\theta \\psi(v_t) g_t$$\nThis gives the forward recurrence relation for $g_t$:\n$$g_{t+1} = (\\beta - \\theta \\psi(v_t)) g_t + x_t$$\nThe initial condition is based on $v_0 = 0$, which is a constant and does not depend on $w$. Therefore, the initial condition for the recurrence is $g_0 = \\frac{d v_0}{d w} = 0$.\n\nCombining these pieces, the final expression for the total gradient is:\n$$\\frac{\\partial L}{\\partial w} \\approx \\sum_{t=0}^{T-1} (s_t - y_t) \\psi(v_t) g_t$$\n\nThe overall algorithm for computing the gradient is as follows:\n\n1.  **Forward Pass**: Simulate the neuron dynamics to obtain the history of membrane potentials $v_t$ and spikes $s_t$ for $t \\in \\{0, 1, \\dots, T-1\\}$.\n    - Initialize $v_0 = 0$.\n    - For $t = 0, \\dots, T-1$:\n        - $s_t = 1$ if $v_t \\ge \\theta$, otherwise $s_t = 0$.\n        - $v_{t+1} = \\beta v_t + w x_t - s_t \\theta$.\n    - Store the sequences $v_0, \\dots, v_{T-1}$ and $s_0, \\dots, s_{T-1}$.\n\n2.  **Gradient Calculation**: Compute the gradient contribution at each time step and accumulate it. This involves a second forward pass to compute the sequence $g_t$.\n    - Initialize total gradient $\\frac{\\partial L}{\\partial w} = 0$.\n    - Initialize the potential's gradient $g_0 = 0$.\n    - For $t = 0, \\dots, T-1$:\n        - Compute the surrogate derivative $\\psi(v_t)$ based on the chosen method (STE or Smoothed).\n        - Update the total gradient: $\\frac{\\partial L}{\\partial w} \\leftarrow \\frac{\\partial L}{\\partial w} + (s_t - y_t) \\psi(v_t) g_t$.\n        - Update the potential's gradient for the next step: $g_{t+1} = (\\beta - \\theta \\psi(v_t)) g_t + x_t$.\n\nThis procedure is executed once for each surrogate gradient method to obtain $(\\partial L / \\partial w)_{\\text{STE}}$ and $(\\partial L / \\partial w)_{k}$. The relative difference metric $r$ is then computed as specified.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import expit\n\ndef generate_input_sequence(seq_type, A, T):\n    \"\"\"Generates the input sequence x_t.\"\"\"\n    x = np.zeros(T)\n    if seq_type == \"pulses2\":\n        if 3  T: x[3] = A\n        if 7  T: x[7] = A\n    elif seq_type == \"pulses4\":\n        indices = [2, 4, 6, 8]\n        for i in indices:\n            if i  T:\n                x[i] = A\n    elif seq_type == \"single\":\n        if 4  T: x[4] = A\n    elif seq_type == \"zero\":\n        pass  # x is already all zeros\n    elif seq_type == \"constant\":\n        x[:] = A\n    else:\n        raise ValueError(\"Unknown sequence type\")\n    return x\n\ndef calculate_gradient(w, seq_type, A, surrogate_choice, surrogate_param):\n    \"\"\"\n    Calculates the gradient dL/dw for a single-neuron SNN.\n    \n    Args:\n        w (float): The synaptic weight.\n        seq_type (str): The type of input sequence.\n        A (float): The amplitude of the input sequence.\n        surrogate_choice (str): 'STE' or 'Smoothed'.\n        surrogate_param (float): Delta for STE or k for Smoothed.\n\n    Returns:\n        float: The calculated gradient dL/dw.\n    \"\"\"\n    # Fixed SNN parameters\n    T = 12\n    beta = 0.9\n    theta = 1.0\n    v0 = 0.0\n\n    # Target spike train\n    y = np.zeros(T)\n    y[4] = 1.0\n    y[9] = 1.0\n\n    # Input sequence\n    x = generate_input_sequence(seq_type, A, T)\n\n    # --- 1. Forward Pass: Simulate neuron dynamics ---\n    v_hist = np.zeros(T)\n    s_hist = np.zeros(T)\n    v = v0\n\n    for t in range(T):\n        v_hist[t] = v\n        s = 1.0 if v >= theta else 0.0\n        s_hist[t] = s\n        v = beta * v + w * x[t] - s * theta\n\n    # --- 2. Gradient Calculation (Forward Mode) ---\n    grad_L_w = 0.0\n    g = 0.0  # g_0 = d(v_0)/dw = 0\n\n    for t in range(T):\n        v_t = v_hist[t]\n        \n        # Calculate surrogate derivative psi(v_t)\n        if surrogate_choice == 'STE':\n            delta = surrogate_param\n            psi = 1.0 if np.abs(v_t - theta)  delta else 0.0\n        elif surrogate_choice == 'Smoothed':\n            k = surrogate_param\n            u = k * (v_t - theta)\n            sigma_u = expit(u)\n            psi = k * sigma_u * (1.0 - sigma_u)\n        else:\n            raise ValueError(\"Invalid surrogate choice\")\n            \n        # Accumulate gradient\n        grad_L_w += (s_hist[t] - y[t]) * psi * g\n        \n        # Update g for the next time step\n        g = (beta - theta * psi) * g + x[t]\n        \n    return grad_L_w\n    \n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and compute the gradient mismatch ratio r.\n    \"\"\"\n    test_cases = [\n        # (delta, k, w, type, A)\n        (0.05, 100.0, 0.8, \"pulses2\", 1.2),\n        (0.2, 100.0, 0.8, \"pulses2\", 1.2),\n        (0.05, 20.0, 1.2, \"pulses4\", 1.5),\n        (0.5, 10.0, 0.3, \"single\", 2.0),\n        (0.01, 200.0, 0.8, \"zero\", 0.0),\n        (0.2, 50.0, 1.0, \"constant\", 0.5),\n    ]\n\n    results = []\n    epsilon = 1e-12\n\n    for case in test_cases:\n        delta, k, w, seq_type, A = case\n        \n        grad_ste = calculate_gradient(w, seq_type, A, 'STE', delta)\n        grad_k = calculate_gradient(w, seq_type, A, 'Smoothed', k)\n        \n        r = np.abs(grad_ste - grad_k) / (np.abs(grad_k) + epsilon)\n        results.append(r)\n\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Any complex numerical algorithm requires rigorous verification, and BPTT is no exception. This advanced practice illuminates the deep connection between BPTT and the broader principle of reverse-mode automatic differentiation (AD), the cornerstone of modern deep learning. By building a simple AD engine from scratch and using it to verify your own BPTT implementation, you will not only gain confidence in your code but also a fundamental understanding of how gradients are computed in any computational graph .",
            "id": "4036240",
            "problem": "Design and implement a complete, runnable program that constructs a single-neuron discrete-time Spiking Neural Network (SNN) with a differentiable surrogate for the spike generation mechanism and verifies the backward recursion for the adjoint variables using Backpropagation Through Time (BPTT). The verification must be performed by comparing the adjoint sequence computed from first principles against the gradients produced by a generic reverse-mode Automatic Differentiation (AD) engine operating on the same differentiable surrogate model.\n\nFundamental base and core definitions:\n- Consider a single neuron with membrane potential sequence $\\{v_t\\}_{t=0}^{T}$ governed by a discrete-time leaky-integration-and-reset recurrence. The neuron is driven by an external input sequence $\\{x_t\\}_{t=0}^{T-1}$.\n- The spike generation is modeled by a differentiable surrogate function replacing the Heaviside step. Let $\\sigma_\\beta(u) = \\frac{1}{1 + e^{-\\beta u}}$ be the logistic function with slope parameter $\\beta$, which is differentiable with derivative $\\sigma'_\\beta(u) = \\beta \\sigma_\\beta(u)\\left(1 - \\sigma_\\beta(u)\\right)$.\n- The discrete-time membrane dynamics are defined by\n$$\nv_{t+1} = \\alpha \\, v_t + w \\, x_t - r \\, \\sigma_\\beta\\!\\left(v_t - \\theta\\right),\n$$\nwhere $t \\in \\{0,1,\\dots,T-1\\}$, $\\alpha \\in (0,1)$ is the leak factor, $w \\in \\mathbb{R}$ is the synaptic weight, $r \\in \\mathbb{R}_{\\ge 0}$ is the reset magnitude, and $\\theta \\in \\mathbb{R}$ is the threshold.\n- The scalar loss is defined as\n$$\nL = \\sum_{t=0}^{T-1} \\frac{\\gamma}{2} \\left(\\sigma_\\beta\\!\\left(v_t - \\theta\\right) - y_t\\right)^2 \\;+\\; \\frac{\\rho}{2} \\, v_T^2,\n$$\nwith $\\gamma \\in \\mathbb{R}_{0}$ and $\\rho \\in \\mathbb{R}_{\\ge 0}$, and a reference sequence $\\{y_t\\}_{t=0}^{T-1}$.\n\nTask requirements:\n1. Implement a generic reverse-mode Automatic Differentiation engine that operates on a computational graph composed of elementary operations (addition, subtraction, multiplication by scalars, squaring, and the logistic function $\\sigma_\\beta$). Use it to compute the gradients $\\left\\{\\frac{\\partial L}{\\partial v_t}\\right\\}_{t=0}^{T}$ for the differentiable surrogate model defined above.\n2. Independently derive, implement, and compute the adjoint sequence $\\{\\delta^v_t\\}_{t=0}^{T}$ via Backpropagation Through Time (BPTT) starting strictly from the chain rule and the provided recurrence. The adjoint $\\delta^v_t$ is defined as $\\delta^v_t = \\frac{\\partial L}{\\partial v_t}$. You must not use any pre-packaged gradient functions or rely on any external automatic differentiation tools; the BPTT recursion must be obtained from first principles.\n3. Design a unit test that, for each provided test case, verifies that the adjoint sequence computed via BPTT matches the gradient sequence produced by the AD engine to within a specified numerical tolerance. The comparison metric must be the maximum absolute difference across all time indices $t \\in \\{0,\\dots,T\\}$.\n4. Your program must output, for all test cases, a single line containing a comma-separated list enclosed in square brackets, where each entry is a boolean indicating whether the verification passed (true) or failed (false) for that test case.\n\nTest suite specification:\nUse the following four test cases. For each case, the membrane potential initial condition is $v_0 = 0$.\n\n- Case $1$ (general happy path):\n  - $T = 8$\n  - $\\alpha = 0.9$\n  - $w = 0.5$\n  - $\\theta = 0.2$\n  - $r = 0.2$\n  - $\\beta = 8.0$\n  - $\\gamma = 1.3$\n  - $\\rho = 0.7$\n  - $x = [0.1, 0.0, 0.4, 0.0, 0.3, 0.5, 0.2, 0.0]$\n  - $y = [0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0]$\n\n- Case $2$ (boundary with effectively no spikes due to high threshold):\n  - $T = 6$\n  - $\\alpha = 0.8$\n  - $w = 0.3$\n  - $\\theta = 5.0$\n  - $r = 0.5$\n  - $\\beta = 8.0$\n  - $\\gamma = 1.0$\n  - $\\rho = 0.0$\n  - $x = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1]$\n  - $y = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]$\n\n- Case $3$ (edge with zero leak):\n  - $T = 5$\n  - $\\alpha = 0.0$\n  - $w = 1.0$\n  - $\\theta = 0.1$\n  - $r = 0.1$\n  - $\\beta = 5.0$\n  - $\\gamma = 0.7$\n  - $\\rho = 0.2$\n  - $x = [0.5, 0.5, 0.5, 0.5, 0.5]$\n  - $y = [1.0, 1.0, 0.0, 1.0, 0.0]$\n\n- Case $4$ (inhibitory input with negative weight):\n  - $T = 7$\n  - $\\alpha = 0.95$\n  - $w = -0.7$\n  - $\\theta = 0.25$\n  - $r = 0.25$\n  - $\\beta = 6.0$\n  - $\\gamma = 0.9$\n  - $\\rho = 0.4$\n  - $x = [0.2, 0.2, 0.0, 0.3, 0.1, 0.0, 0.2]$\n  - $y = [0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0]$\n\nNumerical tolerance:\n- Let the maximum absolute difference between the adjoint sequence from BPTT and the gradient sequence from AD be denoted by $\\Delta = \\max_{t \\in \\{0,\\dots,T\\}} \\left| \\delta^v_t - \\frac{\\partial L}{\\partial v_t} \\right|$.\n- A test case passes if $\\Delta \\le 10^{-6}$; otherwise, it fails.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[\\text{true},\\text{false},\\text{true},\\text{true}]$), with each entry being a boolean for the corresponding test case, in the order of the cases specified above. No additional text or lines should be printed.",
            "solution": "The problem requires the design and implementation of a verification framework for Backpropagation Through Time (BPTT) as applied to a single-neuron Spiking Neural Network (SNN). The verification is performed by comparing the adjoint sequence computed from a first-principles derivation of BPTT against gradients computed by a generic, from-scratch reverse-mode Automatic Differentiation (AD) engine.\n\n### 1. Mathematical Model Formulation\n\nThe system under consideration is a discrete-time, single-neuron model governed by a set of recurrence relations.\n\nThe neuron's membrane potential, denoted by the sequence $\\{v_t\\}_{t=0}^{T}$, evolves according to the leaky-integration-and-reset dynamics:\n$$\nv_{t+1} = \\alpha \\, v_t + w \\, x_t - r \\, \\sigma_\\beta(v_t - \\theta), \\quad \\text{for } t \\in \\{0, 1, \\dots, T-1\\}\n$$\nwhere:\n- $v_t \\in \\mathbb{R}$: membrane potential at time $t$.\n- $x_t \\in \\mathbb{R}$: external input at time $t$.\n- $\\alpha \\in (0,1)$: membrane potential leak factor.\n- $w \\in \\mathbb{R}$: synaptic weight.\n- $r \\in \\mathbb{R}_{\\ge 0}$: reset magnitude upon spiking.\n- $\\theta \\in \\mathbb{R}$: membrane potential threshold.\n- $v_0$: initial membrane potential, specified as $v_0 = 0$.\n\nThe non-differentiable nature of an ideal spike is addressed by using a differentiable surrogate, the logistic function $\\sigma_\\beta(u)$:\n$$\ns_t = \\sigma_\\beta(v_t - \\theta) = \\frac{1}{1 + e^{-\\beta (v_t - \\theta)}}\n$$\nwhere $s_t$ represents the neuron's \"firing rate\" or spike probability at time $t$, and $\\beta$ is a slope parameter. The derivative of the logistic function is essential for gradient-based methods and is given by:\n$$\n\\sigma'_\\beta(u) = \\frac{d\\sigma_\\beta(u)}{du} = \\beta \\, \\sigma_\\beta(u) \\left(1 - \\sigma_\\beta(u)\\right)\n$$\n\nThe performance of the neuron is quantified by a scalar loss function $L$, which is a sum of a spike-matching objective over time and a regularization term on the final membrane potential:\n$$\nL = \\sum_{t=0}^{T-1} \\frac{\\gamma}{2} \\left(\\sigma_\\beta(v_t - \\theta) - y_t\\right)^2 + \\frac{\\rho}{2} \\, v_T^2\n$$\nwhere:\n- $\\{y_t\\}_{t=0}^{T-1}$: target spike sequence.\n- $\\gamma \\in \\mathbb{R}_{0}$: weight of the spike-matching error.\n- $\\rho \\in \\mathbb{R}_{\\ge 0}$: weight of the final potential regularization.\n\nOur primary goal is to compute the sequence of gradients $\\left\\{\\frac{\\partial L}{\\partial v_t}\\right\\}_{t=0}^{T}$.\n\n### 2. Derivation of the BPTT Adjoint Equations\n\nBPTT is an application of the chain rule to a temporal computational graph. We define the adjoint variable $\\delta^v_t$ as the total derivative of the loss $L$ with respect to the state variable $v_t$:\n$$\n\\delta^v_t = \\frac{dL}{dv_t}\n$$\nThe value of $L$ depends on $v_t$ both directly through the loss term at time $t$, and indirectly through the influence of $v_t$ on all subsequent states $v_{t+1}, \\dots, v_T$. The chain rule provides a recursive relationship for the adjoints:\n$$\n\\delta^v_t = \\frac{\\partial L}{\\partial v_t} + \\frac{dL}{dv_{t+1}}\\frac{\\partial v_{t+1}}{\\partial v_t} = \\frac{\\partial L}{\\partial v_t} + \\delta^v_{t+1} \\frac{\\partial v_{t+1}}{\\partial v_t}\n$$\nThis recurrence relation operates backwards in time, from $t=T$ down to $t=0$.\n\nTo formulate the BPTT algorithm, we must derive expressions for the two partial derivatives in the recurrence.\n\n**Terminal Condition (at $t=T$)**:\nThe recursion is initialized at the final time step $T$. The only term in the loss $L$ that explicitly depends on $v_T$ is $\\frac{\\rho}{2}v_T^2$.\n$$\n\\delta^v_T = \\frac{dL}{dv_T} = \\frac{\\partial}{\\partial v_T} \\left( \\frac{\\rho}{2} v_T^2 \\right) = \\rho \\, v_T\n$$\n\n**Local Gradient Term ($\\frac{\\partial L}{\\partial v_t}$ for $t  T$)**:\nFor any time step $t \\in \\{0, \\dots, T-1\\}$, the explicit dependence of $L$ on $v_t$ comes from the spike-matching term $\\frac{\\gamma}{2} (\\sigma_\\beta(v_t - \\theta) - y_t)^2$.\n$$\n\\frac{\\partial L}{\\partial v_t} = \\frac{\\partial}{\\partial v_t} \\left[ \\frac{\\gamma}{2} \\left(\\sigma_\\beta(v_t - \\theta) - y_t\\right)^2 \\right]\n$$\nApplying the chain rule:\n$$\n\\frac{\\partial L}{\\partial v_t} = \\gamma \\left(\\sigma_\\beta(v_t - \\theta) - y_t\\right) \\cdot \\frac{\\partial}{\\partial v_t} \\left(\\sigma_\\beta(v_t - \\theta)\\right)\n$$\n$$\n\\frac{\\partial L}{\\partial v_t} = \\gamma \\left(s_t - y_t\\right) \\cdot \\sigma'_\\beta(v_t - \\theta) \\cdot \\frac{\\partial}{\\partial v_t}(v_t - \\theta)\n$$\n$$\n\\frac{\\partial L}{\\partial v_t} = \\gamma \\left(s_t - y_t\\right) \\sigma'_\\beta(v_t - \\theta)\n$$\nFor notational convenience, let $s'_t = \\sigma'_\\beta(v_t - \\theta)$. Then, $\\frac{\\partial L}{\\partial v_t} = \\gamma (s_t - y_t) s'_t$.\n\n**State Transition Jacobian ($\\frac{\\partial v_{t+1}}{\\partial v_t}$)**:\nThis term captures how a change in $v_t$ propagates to $v_{t+1}$. It is derived from the membrane dynamics equation:\n$$\n\\frac{\\partial v_{t+1}}{\\partial v_t} = \\frac{\\partial}{\\partial v_t} \\left( \\alpha v_t + w x_t - r \\sigma_\\beta(v_t - \\theta) \\right)\n$$\n$$\n\\frac{\\partial v_{t+1}}{\\partial v_t} = \\alpha - r \\cdot \\frac{\\partial}{\\partial v_t} \\left(\\sigma_\\beta(v_t - \\theta)\\right)\n$$\n$$\n\\frac{\\partial v_{t+1}}{\\partial v_t} = \\alpha - r \\, \\sigma'_\\beta(v_t - \\theta) = \\alpha - r s'_t\n$$\n\n**BPTT Recurrence Summary**:\nCombining these components, we obtain the full BPTT algorithm:\n1.  **Forward Pass**: Compute and store the state and spike sequences for $t \\in \\{0, \\dots, T\\}$ by iterating the dynamics equation.\n    - $v_0 = 0$\n    - For $t = 0, \\dots, T-1$:\n        - $s_t = \\sigma_\\beta(v_t - \\theta)$\n        - $s'_t = \\beta s_t (1 - s_t)$\n        - $v_{t+1} = \\alpha v_t + w x_t - r s_t$\n2.  **Backward Pass**: Compute the adjoint sequence $\\{\\delta^v_t\\}_{t=0}^{T}$ by iterating backwards in time.\n    - Initialize: $\\delta^v_T = \\rho v_T$\n    - For $t = T-1, \\dots, 0$:\n        - $\\delta^v_t = \\gamma (s_t - y_t) s'_t + \\delta^v_{t+1} (\\alpha - r s'_t)$\n\n### 3. Automatic Differentiation Framework for Verification\n\nTo verify the correctness of the BPTT derivation and implementation, we construct an independent method for computing the same gradients. A generic reverse-mode Automatic Differentiation (AD) engine serves this purpose. The principle of reverse-mode AD is to first build a computational graph during a forward pass, recording all elementary operations and their inputs. Then, a backward pass traverses this graph in reverse topological order, propagating derivatives from the output back to the inputs using the chain rule.\n\nOur implementation consists of:\n- **`ADVar` Class**: A data structure to represent a node in the computational graph. Each `ADVar` object stores its scalar `value` and its `grad` (the derivative of the final loss with respect to its value), which is initialized to $0$.\n- **Computational Tape**: A global list that acts as a \"tape\". During the forward pass, for each operation performed, a corresponding backward function (a closure) is appended to this tape. This function encapsulates the knowledge of how to propagate gradients back to the operation's inputs.\n- **Elementary Operations**: A set of functions (`ad_add`, `ad_sub`, `ad_mul`, `ad_square`, `ad_sigma`) that operate on `ADVar` objects. Each function computes the output value, wraps it in a new `ADVar`, and adds the corresponding gradient propagation rule to the tape. For instance, for an addition $z = x + y$, the rule is $\\frac{\\partial L}{\\partial x} \\mathrel{+}= \\frac{\\partial L}{\\partial z}$ and $\\frac{\\partial L}{\\partial y} \\mathrel{+}= \\frac{\\partial L}{\\partial z}$.\n- **Backward Execution**: After the forward pass constructs the entire computational graph and computes the final loss $L_{AD}$, its gradient is seeded to $1$. The backward pass then iterates through the tape in reverse order, executing each stored function. This process systematically accumulates the correct gradients in the `.grad` attribute of every `ADVar` that participated in the computation.\n\n### 4. Verification Methodology\n\nThe verification process rigorously compares the outputs of the two independent gradient computation methods.\n\n1.  **AD Computation**: The SNN dynamics and loss function are constructed step-by-step using the elementary operations of the AD framework. This automatically builds the full computational graph. A backward pass is then initiated from the final loss variable, populating the `.grad` attribute for each $v_t$ variable, yielding the sequence $\\{\\frac{\\partial L}{\\partial v_t}_{\\text{AD}}\\}_{t=0}^{T}$.\n2.  **BPTT Computation**: The BPTT algorithm, as derived from first principles, is executed. This involves a standard numerical forward pass to record states, followed by the BPTT backward recurrence to compute the adjoint sequence $\\{\\delta^v_t\\}_{t=0}^{T}$.\n3.  **Comparison**: The two resulting gradient sequences are compared using the maximum absolute difference as the error metric:\n    $$\n    \\Delta = \\max_{t \\in \\{0,\\dots,T\\}} \\left| \\delta^v_t - \\frac{\\partial L}{\\partial v_t}_{\\text{AD}} \\right|\n    $$\n    A test case is considered \"passed\" if this error is within a specified numerical tolerance, $\\Delta \\le 10^{-6}$. This tolerance accounts for potential minor floating-point discrepancies between the two computational paths. A successful verification provides high confidence in the correctness of the BPTT derivation and its implementation.",
            "answer": "```python\nimport numpy as np\n\n# A simple global tape for the AD engine\n_AD_TAPE = []\n\nclass ADVar:\n    \"\"\"Represents a variable in the computational graph for Automatic Differentiation.\"\"\"\n    def __init__(self, value):\n        self.value = np.float64(value)\n        self.grad = np.float64(0.0)\n\ndef _reset_ad():\n    \"\"\"Clears the global tape and resets gradients.\"\"\"\n    global _AD_TAPE\n    _AD_TAPE = []\n\ndef _backward_ad(root_var):\n    \"\"\"Executes the backward pass of the AD engine.\"\"\"\n    root_var.grad = np.float64(1.0)\n    for backward_fn in reversed(_AD_TAPE):\n        backward_fn()\n\n# --- Elementary operations for the AD engine ---\n\ndef ad_add(a: ADVar, b: ADVar) -> ADVar:\n    \"\"\"AD operation for addition.\"\"\"\n    out = ADVar(a.value + b.value)\n    def _backward_fn():\n        a.grad += out.grad\n        b.grad += out.grad\n    _AD_TAPE.append(_backward_fn)\n    return out\n\ndef ad_sub(a: ADVar, b: ADVar) -> ADVar:\n    \"\"\"AD operation for subtraction.\"\"\"\n    out = ADVar(a.value - b.value)\n    def _backward_fn():\n        a.grad += out.grad\n        b.grad -= out.grad\n    _AD_TAPE.append(_backward_fn)\n    return out\n\ndef ad_mul(a: ADVar, b: ADVar) -> ADVar:\n    \"\"\"AD operation for multiplication.\"\"\"\n    out = ADVar(a.value * b.value)\n    def _backward_fn():\n        a.grad += b.value * out.grad\n        b.grad += a.value * out.grad\n    _AD_TAPE.append(_backward_fn)\n    return out\n\ndef ad_square(a: ADVar) -> ADVar:\n    \"\"\"AD operation for squaring.\"\"\"\n    out = ADVar(a.value ** 2)\n    def _backward_fn():\n        a.grad += 2 * a.value * out.grad\n    _AD_TAPE.append(_backward_fn)\n    return out\n\ndef _logistic(u, beta):\n    \"\"\"The logistic sigmoid function.\"\"\"\n    return 1.0 / (1.0 + np.exp(-beta * u))\n\ndef ad_sigma(u_var: ADVar, beta: float) -> ADVar:\n    \"\"\"AD operation for the logistic surrogate spike function.\"\"\"\n    s_val = _logistic(u_var.value, beta)\n    out = ADVar(s_val)\n    def _backward_fn():\n        s_prime = beta * s_val * (1.0 - s_val)\n        u_var.grad += s_prime * out.grad\n    _AD_TAPE.append(_backward_fn)\n    return out\n\n# --- Verification Implementations ---\n\ndef get_grads_with_ad(params: dict) -> np.ndarray:\n    \"\"\"Computes gradients using the generic AD engine.\"\"\"\n    _reset_ad()\n    T = params['T']\n    alpha, w, theta, r = params['alpha'], params['w'], params['theta'], params['r']\n    beta, gamma, rho = params['beta'], params['gamma'], params['rho']\n    x, y = params['x'], params['y']\n\n    # --- AD Forward Pass ---\n    v_ad = [None] * (T + 1)\n    v_ad[0] = ADVar(0.0)\n    s_ad = [None] * T\n\n    # Neuron dynamics\n    for t in range(T):\n        u_t = ad_sub(v_ad[t], ADVar(theta))\n        s_ad[t] = ad_sigma(u_t, beta)\n        \n        term1 = ad_mul(ADVar(alpha), v_ad[t])\n        term2 = ad_mul(ADVar(w), ADVar(x[t]))\n        term3 = ad_mul(ADVar(r), s_ad[t])\n        \n        v_ad[t+1] = ad_sub(ad_add(term1, term2), term3)\n\n    # Loss calculation\n    total_loss = ADVar(0.0)\n    \n    # Spike-matching loss component\n    if gamma > 0:\n        spike_loss = ADVar(0.0)\n        for t in range(T):\n            err_t = ad_sub(s_ad[t], ADVar(y[t]))\n            loss_t = ad_square(err_t)\n            spike_loss = ad_add(spike_loss, loss_t)\n        scaled_spike_loss = ad_mul(ADVar(gamma / 2.0), spike_loss)\n        total_loss = ad_add(total_loss, scaled_spike_loss)\n        \n    # Final potential regularization component\n    if rho > 0:\n        final_loss_term = ad_mul(ADVar(rho / 2.0), ad_square(v_ad[T]))\n        total_loss = ad_add(total_loss, final_loss_term)\n        \n    # --- AD Backward Pass ---\n    _backward_ad(total_loss)\n    \n    return np.array([v.grad for v in v_ad], dtype=np.float64)\n\ndef get_grads_with_bptt(params: dict) -> np.ndarray:\n    \"\"\"Computes gradients using the derived BPTT algorithm.\"\"\"\n    T = params['T']\n    alpha, w, theta, r = params['alpha'], params['w'], params['theta'], params['r']\n    beta, gamma, rho = params['beta'], params['gamma'], params['rho']\n    x, y = params['x'], params['y']\n\n    # --- BPTT Forward Pass (to record states) ---\n    v = np.zeros(T + 1, dtype=np.float64)\n    s = np.zeros(T, dtype=np.float64)\n    s_prime = np.zeros(T, dtype=np.float64)\n    v[0] = 0.0\n\n    for t in range(T):\n        u = v[t] - theta\n        s_val = _logistic(u, beta)\n        s[t] = s_val\n        s_prime[t] = beta * s_val * (1.0 - s_val)\n        v[t+1] = alpha * v[t] + w * x[t] - r * s[t]\n\n    # --- BPTT Backward Pass (computing adjoints) ---\n    delta_v = np.zeros(T + 1, dtype=np.float64)\n    delta_v[T] = rho * v[T]\n\n    for t in range(T - 1, -1, -1):\n        local_grad = gamma * (s[t] - y[t]) * s_prime[t]\n        propagated_grad = delta_v[t+1] * (alpha - r * s_prime[t])\n        delta_v[t] = local_grad + propagated_grad\n\n    return delta_v\n\ndef solve():\n    \"\"\"Main function to run test cases and produce the final output.\"\"\"\n    test_cases = [\n        {\n            \"T\": 8, \"alpha\": 0.9, \"w\": 0.5, \"theta\": 0.2, \"r\": 0.2, \"beta\": 8.0,\n            \"gamma\": 1.3, \"rho\": 0.7,\n            \"x\": [0.1, 0.0, 0.4, 0.0, 0.3, 0.5, 0.2, 0.0],\n            \"y\": [0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0]\n        },\n        {\n            \"T\": 6, \"alpha\": 0.8, \"w\": 0.3, \"theta\": 5.0, \"r\": 0.5, \"beta\": 8.0,\n            \"gamma\": 1.0, \"rho\": 0.0,\n            \"x\": [0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n            \"y\": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n        },\n        {\n            \"T\": 5, \"alpha\": 0.0, \"w\": 1.0, \"theta\": 0.1, \"r\": 0.1, \"beta\": 5.0,\n            \"gamma\": 0.7, \"rho\": 0.2,\n            \"x\": [0.5, 0.5, 0.5, 0.5, 0.5],\n            \"y\": [1.0, 1.0, 0.0, 1.0, 0.0]\n        },\n        {\n            \"T\": 7, \"alpha\": 0.95, \"w\": -0.7, \"theta\": 0.25, \"r\": 0.25, \"beta\": 6.0,\n            \"gamma\": 0.9, \"rho\": 0.4,\n            \"x\": [0.2, 0.2, 0.0, 0.3, 0.1, 0.0, 0.2],\n            \"y\": [0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0]\n        }\n    ]\n\n    results = []\n    tolerance = 1e-6\n\n    for case_params in test_cases:\n        grads_ad = get_grads_with_ad(case_params)\n        grads_bptt = get_grads_with_bptt(case_params)\n        \n        max_abs_diff = np.max(np.abs(grads_ad - grads_bptt))\n        \n        passed = max_abs_diff = tolerance\n        results.append(str(passed).lower())\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}