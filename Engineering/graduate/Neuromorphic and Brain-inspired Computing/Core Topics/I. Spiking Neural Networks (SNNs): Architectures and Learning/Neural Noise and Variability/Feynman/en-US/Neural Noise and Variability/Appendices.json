{
    "hands_on_practices": [
        {
            "introduction": "Understanding neural variability begins with quantifying it. This exercise focuses on two key metrics: the coefficient of variation (CV) of interspike intervals and the Fano factor of spike counts. By working through the gamma renewal process—a versatile model for neuronal firing—you will derive the fundamental relationship between these two measures, gaining insight into how short-term timing precision shapes long-term count statistics. ",
            "id": "4052251",
            "problem": "Consider a stationary renewal point process modeling a neuromorphic spiking neuron, where interspike intervals (ISIs) are independent and identically distributed according to the gamma distribution with shape parameter $k0$ and rate parameter $\\beta0$, having density $p(\\tau)=\\frac{\\beta^{k}}{\\Gamma(k)}\\tau^{k-1}\\exp(-\\beta \\tau)$. Let $N(T)$ denote the spike count in a window of duration $T0$. The Fano factor $F(T)$ of the spike count is defined as $F(T)=\\frac{\\mathrm{Var}[N(T)]}{\\mathbb{E}[N(T)]}$. Starting from the definitions of a renewal process and using well-tested limit theorems for renewal processes together with properties of the gamma distribution, derive the asymptotic Fano factor $F=\\lim_{T\\to\\infty}F(T)$ as a function of $k$ and $\\beta$. Then compute the coefficient of variation (CV) of the ISI distribution, defined as $\\mathrm{CV}=\\frac{\\sqrt{\\mathrm{Var}[\\tau]}}{\\mathbb{E}[\\tau]}$, and express the relationship between the asymptotic Fano factor and the ISI coefficient of variation. The final answer must consist of two closed-form analytic expressions: the asymptotic Fano factor $F$ in terms of $k$ and $\\beta$, and the ISI coefficient of variation $\\mathrm{CV}$ in terms of $k$ and $\\beta$. No rounding is required. Since both quantities are dimensionless, do not include any units.",
            "solution": "The problem is well-posed and scientifically grounded, resting on established principles of renewal theory and probability distributions commonly used in computational neuroscience. We can therefore proceed with the derivation.\n\nThe problem asks for the asymptotic Fano factor $F = \\lim_{T\\to\\infty} F(T)$ and the coefficient of variation $\\mathrm{CV}$ of the interspike intervals (ISIs) for a stationary renewal point process.\n\nFirst, let's establish the general relationship between these two quantities for any stationary renewal process.\nLet $\\tau$ be the random variable representing the ISI, with mean $\\mu = \\mathbb{E}[\\tau]$ and variance $\\sigma^2 = \\mathrm{Var}[\\tau]$. Let $N(T)$ be the number of spikes (events) in a time window of duration $T$.\n\nFor a stationary renewal process, the expected number of events in an interval of length $T$ is given by:\n$$ \\mathbb{E}[N(T)] = \\frac{T}{\\mu} $$\n\nA fundamental result from renewal theory states that for large $T$, the variance of the number of events is asymptotically proportional to $T$:\n$$ \\mathrm{Var}[N(T)] \\approx T \\frac{\\sigma^2}{\\mu^3} \\quad \\text{for } T \\to \\infty $$\nMore formally, $\\lim_{T \\to \\infty} \\frac{\\mathrm{Var}[N(T)]}{T} = \\frac{\\sigma^2}{\\mu^3}$.\n\nThe Fano factor of the spike count is defined as $F(T) = \\frac{\\mathrm{Var}[N(T)]}{\\mathbb{E}[N(T)]}$. The asymptotic Fano factor, $F$, is the limit of this quantity as $T \\to \\infty$:\n$$ F = \\lim_{T\\to\\infty} F(T) = \\lim_{T\\to\\infty} \\frac{\\mathrm{Var}[N(T)]}{\\mathbb{E}[N(T)]} $$\nSubstituting the asymptotic forms for the mean and variance:\n$$ F = \\lim_{T\\to\\infty} \\frac{T \\frac{\\sigma^2}{\\mu^3}}{\\frac{T}{\\mu}} = \\frac{\\sigma^2}{\\mu^2} $$\nThe quantity $\\frac{\\sigma}{\\mu}$ is defined as the coefficient of variation, $\\mathrm{CV}$, of the ISI distribution. Therefore, we have the general relationship:\n$$ F = \\left(\\frac{\\sigma}{\\mu}\\right)^2 = (\\mathrm{CV})^2 $$\nThis equation shows that the asymptotic Fano factor of the spike count is equal to the squared coefficient of variation of the ISI distribution.\n\nNow, we apply this general result to the specific ISI distribution given in the problem. The ISIs, $\\tau$, are independent and identically distributed according to a gamma distribution with shape parameter $k0$ and rate parameter $\\beta0$. The probability density function is:\n$$ p(\\tau) = \\frac{\\beta^{k}}{\\Gamma(k)}\\tau^{k-1}\\exp(-\\beta \\tau), \\quad \\tau \\ge 0 $$\nThe mean and variance of a gamma-distributed random variable are well-known properties:\nThe mean (expected value) is:\n$$ \\mu = \\mathbb{E}[\\tau] = \\frac{k}{\\beta} $$\nThe variance is:\n$$ \\sigma^2 = \\mathrm{Var}[\\tau] = \\frac{k}{\\beta^2} $$\n\nWith these moments, we can now compute the two quantities requested.\n\nFirst, we compute the coefficient of variation (CV) of the ISI distribution:\n$$ \\mathrm{CV} = \\frac{\\sqrt{\\mathrm{Var}[\\tau]}}{\\mathbb{E}[\\tau]} = \\frac{\\sigma}{\\mu} $$\nSubstituting the expressions for $\\mu$ and $\\sigma = \\sqrt{\\sigma^2}$:\n$$ \\mathrm{CV} = \\frac{\\sqrt{\\frac{k}{\\beta^2}}}{\\frac{k}{\\beta}} = \\frac{\\frac{\\sqrt{k}}{\\beta}}{\\frac{k}{\\beta}} = \\frac{\\sqrt{k}}{k} = \\frac{1}{\\sqrt{k}} $$\nSo, the ISI coefficient of variation is $\\mathrm{CV} = \\frac{1}{\\sqrt{k}}$. This expression depends only on the shape parameter $k$, not the rate parameter $\\beta$, as expected for a dimensionless ratio.\n\nSecond, we compute the asymptotic Fano factor, $F$. Using the relationship $F = (\\mathrm{CV})^2$ that we derived:\n$$ F = \\left(\\frac{1}{\\sqrt{k}}\\right)^2 = \\frac{1}{k} $$\nAlternatively, we can compute $F$ directly from $F = \\sigma^2 / \\mu^2$:\n$$ F = \\frac{\\frac{k}{\\beta^2}}{\\left(\\frac{k}{\\beta}\\right)^2} = \\frac{\\frac{k}{\\beta^2}}{\\frac{k^2}{\\beta^2}} = \\frac{k}{k^2} = \\frac{1}{k} $$\nBoth methods yield the same result. The asymptotic Fano factor is $F = \\frac{1}{k}$. Like the CV, this dimensionless quantity depends only on the shape parameter $k$.\n\nThe relationship between the two quantities is $F = (\\mathrm{CV})^2$, as shown in the general derivation.\n\nThe problem requires the two closed-form expressions for $F$ and $\\mathrm{CV}$ in terms of $k$ and $\\beta$. These are:\n1. Asymptotic Fano factor: $F = \\frac{1}{k}$\n2. ISI Coefficient of Variation: $\\mathrm{CV} = \\frac{1}{\\sqrt{k}}$",
            "answer": "$$\\boxed{\\begin{pmatrix} \\frac{1}{k}  \\frac{1}{\\sqrt{k}} \\end{pmatrix}}$$"
        },
        {
            "introduction": "Where does neural variability come from? This practice moves from statistical description to a mechanistic model, exploring how a neuron's membrane potential fluctuates due to noisy inputs. You will solve the Fokker-Planck equation for a simplified leaky integrate-and-fire (LIF) neuron to find its stationary voltage distribution, explicitly linking biophysical parameters like drift and diffusion to emergent statistical properties and the firing rate. ",
            "id": "4052243",
            "problem": "Consider a one-dimensional subthreshold membrane voltage dynamics for a leaky integrate-and-fire (LIF) neuron that, over the operating range between reset and threshold, is well-approximated by constant drift and additive Gaussian white noise. Model the subthreshold voltage by the Itô stochastic differential equation\n$$\n\\mathrm{d}V_t \\;=\\; \\mu \\,\\mathrm{d}t \\;+\\; \\sqrt{2D}\\, \\mathrm{d}W_t,\n$$\nwith constant drift $\\mu0$ and diffusion coefficient $D0$. The voltage is confined to the interval $[V_r,\\theta]$ with $V_r\\theta$. When $V_t$ reaches the threshold $\\theta$, a spike is said to occur instantaneously, the subthreshold probability density is absorbed at $V=\\theta$, and the voltage is instantaneously reset to $V=V_r$ with no refractory period. Assume stationarity of the subthreshold dynamics with a constant stationary firing flux.\n\nStarting from standard Fokker–Planck conservation principles and definitions of probability flux, derive the stationary subthreshold voltage distribution $p(V)$ for $V\\in[V_r,\\theta]$ under these assumptions. Impose the absorbing boundary at $V=\\theta$, the instantaneous reinjection at $V=V_r$ implied by conservation of flux in steady state, and the normalization condition\n$$\n\\int_{V_r}^{\\theta} p(V)\\,\\mathrm{d}V \\;=\\; 1.\n$$\nExpress your final result as a single closed-form analytic expression $p(V)$ in terms of $\\mu$, $D$, $V_r$, $\\theta$, and $V$. Verify explicitly that your solution satisfies the absorbing boundary at $V=\\theta$ and the steady-state flux consistency at $V=V_r$, and that it is properly normalized on $[V_r,\\theta]$. No numerical evaluation is required. Your final answer must be a single analytic expression.",
            "solution": "The problem asks for the stationary subthreshold voltage distribution $p(V)$ for a particle described by the Itô stochastic differential equation (SDE):\n$$\n\\mathrm{d}V_t = \\mu \\,\\mathrm{d}t + \\sqrt{2D}\\, \\mathrm{d}W_t\n$$\nwhere $\\mu  0$ is the constant drift, $D  0$ is the diffusion coefficient, and $W_t$ is a standard Wiener process. The voltage $V_t$ is defined on the interval $[V_r, \\theta]$. The boundary at $V=\\theta$ is absorbing, and particles absorbed at $\\theta$ are instantaneously reinjected at $V=V_r$.\n\nThe dynamics of the probability density function $p(V, t)$ are governed by the Fokker-Planck equation. For a general SDE $\\mathrm{d}X_t = A(X_t)\\mathrm{d}t + B(X_t)\\mathrm{d}W_t$, the Fokker-Planck equation is:\n$$\n\\frac{\\partial p(V,t)}{\\partial t} = -\\frac{\\partial}{\\partial V} \\left[ A(V)p(V,t) \\right] + \\frac{1}{2}\\frac{\\partial^2}{\\partial V^2} \\left[ B(V)^2 p(V,t) \\right]\n$$\nIn our case, the drift term is $A(V) = \\mu$ and the diffusion term is $B(V) = \\sqrt{2D}$, so $B(V)^2 = 2D$. The Fokker-Planck equation becomes:\n$$\n\\frac{\\partial p(V,t)}{\\partial t} = -\\frac{\\partial}{\\partial V} \\left[ \\mu p(V,t) \\right] + \\frac{\\partial^2}{\\partial V^2} \\left[ D p(V,t) \\right]\n$$\nThis can be written in the form of a conservation equation, $\\frac{\\partial p}{\\partial t} = -\\frac{\\partial J}{\\partial V}$, where $J(V,t)$ is the probability flux:\n$$\nJ(V,t) = \\mu p(V,t) - D \\frac{\\partial p(V,t)}{\\partial V}\n$$\nThe problem specifies that the system is in a stationary state. Therefore, $\\frac{\\partial p}{\\partial t} = 0$, which implies that the stationary probability flux $J(V)$ must be constant with respect to $V$, i.e., $\\frac{\\mathrm{d}J}{\\mathrm{d}V} = 0$. Let this constant flux be $J_0$. This constant $J_0$ represents the stationary firing rate of the neuron. The governing equation for the stationary probability distribution $p(V)$ is a first-order ordinary differential equation (ODE):\n$$\n\\mu p(V) - D \\frac{\\mathrm{d} p(V)}{\\mathrm{d} V} = J_0\n$$\nWe can rearrange this ODE into standard linear form:\n$$\n\\frac{\\mathrm{d}p}{\\mathrm{d}V} - \\frac{\\mu}{D} p(V) = -\\frac{J_0}{D}\n$$\nThe integrating factor is $I(V) = \\exp\\left(\\int -\\frac{\\mu}{D} \\mathrm{d}V\\right) = \\exp\\left(-\\frac{\\mu}{D}V\\right)$. Multiplying the ODE by $I(V)$ yields:\n$$\n\\frac{\\mathrm{d}}{\\mathrm{d}V} \\left( p(V) \\exp\\left(-\\frac{\\mu}{D}V\\right) \\right) = -\\frac{J_0}{D} \\exp\\left(-\\frac{\\mu}{D}V\\right)\n$$\nIntegrating both sides with respect to $V$:\n$$\np(V) \\exp\\left(-\\frac{\\mu}{D}V\\right) = \\int -\\frac{J_0}{D} \\exp\\left(-\\frac{\\mu}{D}V\\right) \\mathrm{d}V = -\\frac{J_0}{D} \\left(-\\frac{D}{\\mu}\\right) \\exp\\left(-\\frac{\\mu}{D}V\\right) + C\n$$\nwhere $C$ is the constant of integration.\n$$\np(V) \\exp\\left(-\\frac{\\mu}{D}V\\right) = \\frac{J_0}{\\mu} \\exp\\left(-\\frac{\\mu}{D}V\\right) + C\n$$\nSolving for $p(V)$ gives the general solution:\n$$\np(V) = \\frac{J_0}{\\mu} + C \\exp\\left(\\frac{\\mu}{D}V\\right)\n$$\nTo find the constants $C$ and $J_0$, we apply the boundary conditions and the normalization requirement.\n\n1.  **Absorbing boundary at $V=\\theta$**: An absorbing boundary condition implies that the probability density at that point is zero. Thus, $p(\\theta) = 0$.\n    $$\n    0 = \\frac{J_0}{\\mu} + C \\exp\\left(\\frac{\\mu}{D}\\theta\\right)\n    $$\n    Solving for $C$:\n    $$\n    C = -\\frac{J_0}{\\mu} \\exp\\left(-\\frac{\\mu}{D}\\theta\\right)\n    $$\n2.  **Substitute $C$ back into $p(V)$**:\n    $$\n    p(V) = \\frac{J_0}{\\mu} - \\frac{J_0}{\\mu} \\exp\\left(-\\frac{\\mu}{D}\\theta\\right) \\exp\\left(\\frac{\\mu}{D}V\\right) = \\frac{J_0}{\\mu} \\left( 1 - \\exp\\left(\\frac{\\mu}{D}(V-\\theta)\\right) \\right)\n    $$\n3.  **Normalization condition**: The total probability of finding the particle in the interval $[V_r, \\theta]$ must be $1$.\n    $$\n    \\int_{V_r}^{\\theta} p(V)\\,\\mathrm{d}V = 1\n    $$\n    $$\n    \\frac{J_0}{\\mu} \\int_{V_r}^{\\theta} \\left( 1 - \\exp\\left(\\frac{\\mu}{D}(V-\\theta)\\right) \\right) \\mathrm{d}V = 1\n    $$\n    We evaluate the integral:\n    $$\n    \\int_{V_r}^{\\theta} \\left( 1 - \\exp\\left(\\frac{\\mu}{D}(V-\\theta)\\right) \\right) \\mathrm{d}V = \\left[ V - \\frac{D}{\\mu} \\exp\\left(\\frac{\\mu}{D}(V-\\theta)\\right) \\right]_{V_r}^{\\theta}\n    $$\n    $$\n    = \\left( \\theta - \\frac{D}{\\mu} \\exp(0) \\right) - \\left( V_r - \\frac{D}{\\mu} \\exp\\left(\\frac{\\mu}{D}(V_r-\\theta)\\right) \\right)\n    $$\n    $$\n    = \\theta - V_r - \\frac{D}{\\mu} \\left( 1 - \\exp\\left(-\\frac{\\mu}{D}(\\theta-V_r)\\right) \\right)\n    $$\n    Substituting this back into the normalization equation:\n    $$\n    \\frac{J_0}{\\mu} \\left[ (\\theta - V_r) - \\frac{D}{\\mu} \\left( 1 - \\exp\\left(-\\frac{\\mu}{D}(\\theta-V_r)\\right) \\right) \\right] = 1\n    $$\n    Solving for $J_0$:\n    $$\n    J_0 = \\frac{\\mu}{(\\theta - V_r) - \\frac{D}{\\mu} \\left( 1 - \\exp\\left(-\\frac{\\mu}{D}(\\theta-V_r)\\right) \\right)}\n    $$\n    Now, we substitute this expression for $J_0$ into the equation for $p(V)$:\n    $$\n    p(V) = \\frac{1}{\\mu} \\left( \\frac{\\mu}{(\\theta - V_r) - \\frac{D}{\\mu} \\left( 1 - \\exp\\left(-\\frac{\\mu}{D}(\\theta-V_r)\\right) \\right)} \\right) \\left( 1 - \\exp\\left(\\frac{\\mu}{D}(V-\\theta)\\right) \\right)\n    $$\n    $$\n    p(V) = \\frac{1 - \\exp\\left(\\frac{\\mu}{D}(V-\\theta)\\right)}{(\\theta - V_r) - \\frac{D}{\\mu} \\left( 1 - \\exp\\left(-\\frac{\\mu}{D}(\\theta-V_r)\\right) \\right)}\n    $$\n    To present the final answer as a single fraction without nested fractions, we multiply the numerator and denominator by $\\mu$:\n    $$\n    p(V) = \\frac{\\mu \\left(1 - \\exp\\left(\\frac{\\mu}{D}(V-\\theta)\\right)\\right)}{\\mu(\\theta - V_r) - D \\left(1 - \\exp\\left(-\\frac{\\mu}{D}(\\theta-V_r)\\right)\\right)}\n    $$\n    $$\n    p(V) = \\frac{\\mu \\left(1 - \\exp\\left(\\frac{\\mu}{D}(V-\\theta)\\right)\\right)}{\\mu(\\theta - V_r) - D + D\\exp\\left(-\\frac{\\mu}{D}(\\theta - V_r)\\right)}\n    $$\n    This is the final closed-form expression for the stationary subthreshold voltage distribution.\n\nWe now verify the solution as requested.\n\n**Verification 1: Absorbing boundary at $V=\\theta$**\nWe must check if $p(\\theta)=0$.\n$$\np(\\theta) = \\frac{\\mu \\left(1 - \\exp\\left(\\frac{\\mu}{D}(\\theta-\\theta)\\right)\\right)}{\\mu(\\theta - V_r) - D + D\\exp\\left(-\\frac{\\mu}{D}(\\theta - V_r)\\right)} = \\frac{\\mu \\left(1 - \\exp(0)\\right)}{\\text{denominator}} = \\frac{\\mu(1-1)}{\\text{denominator}} = 0\n$$\nThe condition is satisfied.\n\n**Verification 2: Steady-state flux consistency**\nWe must verify that our solution $p(V)$ yields a constant flux $J_0$ for all $V \\in [V_r, \\theta]$. We calculate the flux $J(V) = \\mu p(V) - D \\frac{\\mathrm{d}p}{\\mathrm{d}V}$.\nFirst, we find the derivative of $p(V)$:\n$$\n\\frac{\\mathrm{d}p}{\\mathrm{d}V} = \\frac{\\mu}{\\text{denominator}} \\left( -\\frac{\\mu}{D} \\exp\\left(\\frac{\\mu}{D}(V-\\theta)\\right) \\right) = -\\frac{\\mu^2}{D \\cdot \\text{denominator}} \\exp\\left(\\frac{\\mu}{D}(V-\\theta)\\right)\n$$\nNow, substitute $p(V)$ and $\\frac{\\mathrm{d}p}{\\mathrm{d}V}$ into the expression for the flux:\n$$\nJ(V) = \\mu \\left( \\frac{\\mu \\left(1 - \\exp\\left(\\frac{\\mu}{D}(V-\\theta)\\right)\\right)}{\\text{denominator}} \\right) - D \\left( -\\frac{\\mu^2}{D \\cdot \\text{denominator}} \\exp\\left(\\frac{\\mu}{D}(V-\\theta)\\right) \\right)\n$$\n$$\nJ(V) = \\frac{\\mu^2}{\\text{denominator}} \\left(1 - \\exp\\left(\\frac{\\mu}{D}(V-\\theta)\\right)\\right) + \\frac{\\mu^2}{\\text{denominator}} \\exp\\left(\\frac{\\mu}{D}(V-\\theta)\\right)\n$$\n$$\nJ(V) = \\frac{\\mu^2}{\\text{denominator}} \\left[ 1 - \\exp\\left(\\frac{\\mu}{D}(V-\\theta)\\right) + \\exp\\left(\\frac{\\mu}{D}(V-\\theta)\\right) \\right] = \\frac{\\mu^2}{\\text{denominator}}\n$$\nSubstituting the expression for the denominator:\n$$\nJ(V) = \\frac{\\mu^2}{\\mu(\\theta - V_r) - D + D\\exp\\left(-\\frac{\\mu}{D}(\\theta - V_r)\\right)}\n$$\nThis expression is independent of $V$, so the flux is indeed constant, $J(V) = J_0$. This verifies the consistency of the steady-state assumption. The reinjection at $V=V_r$ is what supports this non-zero flux across the domain.\n\n**Verification 3: Normalization**\nWe determined the constant $J_0$ (which is embedded in our final expression for $p(V)$) by enforcing that $\\int_{V_r}^{\\theta} p(V)\\,\\mathrm{d}V = 1$. The calculation proceeded as:\n$$\n1 = \\int_{V_r}^{\\theta} p(V)\\,\\mathrm{d}V = \\int_{V_r}^{\\theta} \\frac{J_0}{\\mu} \\left( 1 - \\exp\\left(\\frac{\\mu}{D}(V-\\theta)\\right) \\right) \\mathrm{d}V\n$$\n$$\n1 = \\frac{J_0}{\\mu} \\times \\left( (\\theta - V_r) - \\frac{D}{\\mu} \\left( 1 - \\exp\\left(-\\frac{\\mu}{D}(\\theta-V_r)\\right) \\right) \\right)\n$$\nThis equation is satisfied by our definition of $J_0$:\n$$\nJ_0 = \\frac{\\mu}{(\\theta - V_r) - \\frac{D}{\\mu} \\left( 1 - \\exp\\left(-\\frac{\\mu}{D}(\\theta-V_r)\\right) \\right)}\n$$\nThus, the solution is properly normalized by construction.",
            "answer": "$$\n\\boxed{\\frac{\\mu \\left(1 - \\exp\\left(\\frac{\\mu}{D}(V-\\theta)\\right)\\right)}{\\mu(\\theta - V_r) - D + D\\exp\\left(-\\frac{\\mu}{D}(\\theta - V_r)\\right)}}\n$$"
        },
        {
            "introduction": "The ultimate test of a model is its ability to explain real data. This exercise introduces the Generalized Linear Model (GLM), a cornerstone of statistical neuroscience for connecting stimuli and spike history to firing probability. You will derive the gradient of the log-likelihood function, a crucial step in the optimization process used to fit GLMs to experimental recordings and uncover the features that drive a neuron's response. ",
            "id": "4052224",
            "problem": "In neuromorphic and brain-inspired computing, spiking variability is often modeled using a point-process Generalized Linear Model (GLM). Consider a single neuron observed over discrete time bins of width $\\Delta  0$. Let $y_t \\in \\{0,1,2,\\ldots\\}$ denote the spike count in bin $t$, $x_t \\in \\mathbb{R}^{d}$ denote the exogenous stimulus vector in bin $t$, and let $r_t \\in \\mathbb{R}^{p}$ denote a spike-history vector constructed from the neuron's recent past, for example $r_t = [y_{t-1}, y_{t-2}, \\ldots, y_{t-p}]^{\\top}$. Let $k \\in \\mathbb{R}^{d}$ be a stimulus filter, $h \\in \\mathbb{R}^{p}$ be a spike-history filter, and $b \\in \\mathbb{R}$ be a scalar bias. Define the linear drive\n$$\n\\eta_t = k^{\\top} x_t + h^{\\top} r_t + b,\n$$\nand the conditional intensity per unit time\n$$\n\\lambda_t = f(\\eta_t),\n$$\nwhere $f:\\mathbb{R}\\to\\mathbb{R}_{+}$ is a differentiable nonlinearity that maps the linear drive to a nonnegative rate. Assume that conditioned on past observations, the spike counts are independent across bins and follow a Poisson distribution with mean $\\Delta \\lambda_t$, i.e., $y_t \\sim \\mathrm{Poisson}(\\Delta \\lambda_t)$.\n\n1. Starting from the definition of the Poisson likelihood and the chain rule from multivariable calculus, write the log-likelihood of the observed spike train $\\{y_t\\}_{t=1}^{T}$ given parameters $(k,h,b)$ and derive the gradient of the log-likelihood with respect to $k$ and $h$ in terms of $x_t$, $r_t$, $y_t$, $f(\\cdot)$, and $f'(\\cdot)$. You may treat $b$ as known and fixed for this derivation.\n\n2. Express your final result as a single closed-form analytic expression for the pair of gradients with respect to $(k,h)$, written as a two-entry row where the first entry is the gradient with respect to $k$ and the second entry is the gradient with respect to $h$.\n\nNo numerical evaluation is required. Express the final answer symbolically; do not round. Units are not required for the final expression.",
            "solution": "The user wants to find the gradient of the log-likelihood for a Poisson Generalized Linear Model (GLM) of a neuron's spike train. The parameters of interest are the stimulus filter $k$ and the spike-history filter $h$.\n\nThe log-likelihood function, $\\mathcal{L}(k,h)$, must first be constructed. The problem states that the spike count $y_t$ in time bin $t$ follows a Poisson distribution, $y_t \\sim \\mathrm{Poisson}(\\mu_t)$, with mean $\\mu_t = \\Delta \\lambda_t$. The rate $\\lambda_t$ is given by $\\lambda_t = f(\\eta_t)$, where $\\eta_t = k^{\\top}x_t + h^{\\top}r_t + b$ is the linear drive.\n\nThe probability mass function (PMF) for a single observation $y_t$ is:\n$$\nP(y_t | k,h) = \\frac{(\\mu_t)^{y_t} \\exp(-\\mu_t)}{y_t!} = \\frac{(\\Delta f(\\eta_t))^{y_t} \\exp(-\\Delta f(\\eta_t))}{y_t!}\n$$\nThe problem states that conditioned on past observations, the spike counts are independent across bins. Therefore, the total likelihood for an observed spike train $\\{y_t\\}_{t=1}^{T}$ is the product of the individual probabilities:\n$$\nL(k,h) = \\prod_{t=1}^{T} P(y_t | k,h) = \\prod_{t=1}^{T} \\frac{(\\Delta f(\\eta_t))^{y_t} \\exp(-\\Delta f(\\eta_t))}{y_t!}\n$$\nThe log-likelihood, $\\mathcal{L}(k,h) = \\ln(L(k,h))$, is the sum of the individual log-probabilities:\n$$\n\\mathcal{L}(k,h) = \\sum_{t=1}^{T} \\ln \\left( \\frac{(\\Delta f(\\eta_t))^{y_t} \\exp(-\\Delta f(\\eta_t))}{y_t!} \\right)\n$$\nUsing the properties of logarithms, we can simplify this expression:\n$$\n\\mathcal{L}(k,h) = \\sum_{t=1}^{T} \\left( \\ln((\\Delta f(\\eta_t))^{y_t}) - \\Delta f(\\eta_t) - \\ln(y_t!) \\right)\n$$\n$$\n\\mathcal{L}(k,h) = \\sum_{t=1}^{T} \\left( y_t \\ln(\\Delta f(\\eta_t)) - \\Delta f(\\eta_t) - \\ln(y_t!) \\right)\n$$\nThe parameters $k$ and $h$ influence $\\mathcal{L}$ only through the linear drive $\\eta_t$. We can find the gradients $\\nabla_k\\mathcal{L}$ and $\\nabla_h\\mathcal{L}$ using the multivariable chain rule. The gradient of the log-likelihood is the sum of the gradients of the per-timestep contributions:\n$$\n\\nabla \\mathcal{L} = \\sum_{t=1}^{T} \\nabla \\mathcal{L}_t\n$$\nwhere $\\mathcal{L}_t = y_t \\ln(\\Delta f(\\eta_t)) - \\Delta f(\\eta_t) - \\ln(y_t!)$.\n\nLet's apply the chain rule. For any parameter vector $\\theta$ (which will be $k$ or $h$), the gradient is:\n$$\n\\nabla_{\\theta} \\mathcal{L} = \\sum_{t=1}^{T} \\frac{\\partial \\mathcal{L}_t}{\\partial \\eta_t} \\nabla_{\\theta} \\eta_t\n$$\nFirst, we compute the scalar derivative $\\frac{\\partial \\mathcal{L}_t}{\\partial \\eta_t}$. The term $\\ln(y_t!)$ is constant with respect to $\\eta_t$.\n$$\n\\frac{\\partial \\mathcal{L}_t}{\\partial \\eta_t} = \\frac{\\partial}{\\partial \\eta_t} \\left( y_t \\ln(\\Delta) + y_t \\ln(f(\\eta_t)) - \\Delta f(\\eta_t) \\right)\n$$\n$$\n\\frac{\\partial \\mathcal{L}_t}{\\partial \\eta_t} = 0 + y_t \\frac{1}{f(\\eta_t)} f'(\\eta_t) - \\Delta f'(\\eta_t)\n$$\nwhere $f'(\\eta_t)$ is the derivative of $f$ with respect to its argument. Factoring out $f'(\\eta_t)$, we get:\n$$\n\\frac{\\partial \\mathcal{L}_t}{\\partial \\eta_t} = \\left( \\frac{y_t}{f(\\eta_t)} - \\Delta \\right) f'(\\eta_t)\n$$\nNext, we compute the gradient of $\\eta_t$ with respect to $k$ and $h$.\nThe linear drive is $\\eta_t = k^{\\top}x_t + h^{\\top}r_t + b$.\nThe gradient with respect to $k$ is:\n$$\n\\nabla_k \\eta_t = \\nabla_k (k^{\\top}x_t + h^{\\top}r_t + b) = x_t\n$$\nThe gradient with respect to $h$ is:\n$$\n\\nabla_h \\eta_t = \\nabla_h (k^{\\top}x_t + h^{\\top}r_t + b) = r_t\n$$\nNow, we can assemble the final expressions for the gradients.\n\nFor the stimulus filter $k$:\n$$\n\\nabla_k \\mathcal{L}(k,h) = \\sum_{t=1}^{T} \\frac{\\partial \\mathcal{L}_t}{\\partial \\eta_t} \\nabla_k \\eta_t = \\sum_{t=1}^{T} \\left( \\frac{y_t}{f(\\eta_t)} - \\Delta \\right) f'(\\eta_t) x_t\n$$\nSubstituting $\\eta_t = k^{\\top}x_t + h^{\\top}r_t + b$:\n$$\n\\nabla_k \\mathcal{L}(k,h) = \\sum_{t=1}^{T} \\left( \\frac{y_t}{f(k^{\\top}x_t + h^{\\top}r_t + b)} - \\Delta \\right) f'(k^{\\top}x_t + h^{\\top}r_t + b) x_t\n$$\nFor the spike-history filter $h$:\n$$\n\\nabla_h \\mathcal{L}(k,h) = \\sum_{t=1}^{T} \\frac{\\partial \\mathcal{L}_t}{\\partial \\eta_t} \\nabla_h \\eta_t = \\sum_{t=1}^{T} \\left( \\frac{y_t}{f(\\eta_t)} - \\Delta \\right) f'(\\eta_t) r_t\n$$\nSubstituting $\\eta_t = k^{\\top}x_t + h^{\\top}r_t + b$:\n$$\n\\nabla_h \\mathcal{L}(k,h) = \\sum_{t=1}^{T} \\left( \\frac{y_t}{f(k^{\\top}x_t + h^{\\top}r_t + b)} - \\Delta \\right) f'(k^{\\top}x_t + h^{\\top}r_t + b) r_t\n$$\nThe problem asks for the result as a two-entry row, which we construct with the two derived gradient expressions.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\sum_{t=1}^{T} \\left( \\frac{y_t}{f(k^{\\top}x_t + h^{\\top}r_t + b)} - \\Delta \\right) f'(k^{\\top}x_t + h^{\\top}r_t + b) x_t  \\sum_{t=1}^{T} \\left( \\frac{y_t}{f(k^{\\top}x_t + h^{\\top}r_t + b)} - \\Delta \\right) f'(k^{\\top}x_t + h^{\\top}r_t + b) r_t\n\\end{pmatrix}\n}\n$$"
        }
    ]
}