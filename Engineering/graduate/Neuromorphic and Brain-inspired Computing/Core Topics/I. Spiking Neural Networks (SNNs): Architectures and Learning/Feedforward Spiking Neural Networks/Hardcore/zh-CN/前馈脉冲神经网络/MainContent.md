## 引言
脉冲神经网络（Spiking Neural Networks, SNNs）作为第三代神经网络模型，通过模仿生物大脑中神经元通[过离散](@entry_id:263748)脉冲进行信息传递与处理的方式，为构建更高效、更具生物真实性的智能计算系统提供了可能。在前馈[脉冲神经网络](@entry_id:1132168)（Feedforward SNNs）中，信息沿着一个无环路的方向流动，构成了许多复杂网络的基础。然而，要充分利用其潜力，必须深入理解其独特的[时空动力学](@entry_id:1132003)如何转化为强大的计算能力，并将这些理论原理与实际应用联系起来，这正是当前领域面临的一个关键知识挑战。

本篇文章旨在系统性地阐述前馈[脉冲神经网络](@entry_id:1132168)的核心理论、学习机制及其在多个前沿领域的应用。通过逐章深入的探讨，读者将能够建立一个从底层机制到高层应用的完整知识框架。

- 在第一章**“原理与机制”**中，我们将剖析前馈SNN的结构基础——[有向无环图](@entry_id:164045)，并深入探讨构成网络的基本单元——脉冲神经元（特别是LIF模型）的动力学行为。本章还将详细介绍信息如何在时间域中被编码、传递和计算，以及网络层面的动态特性和学习的基本原理，如[代理梯度法](@entry_id:1132706)。

- 随后的**“应用与跨学科联系”**一章将理论与实践相结合，展示前馈SNN如何在不同领域大放异彩。我们将探讨其在神经形态工程中如何实现事件驱动的超[低功耗计算](@entry_id:1127486)，在机器学习中如何处理复杂的[时空模式](@entry_id:203673)，以及在[计算神经科学](@entry_id:274500)中如何作为模型来揭示大脑的工作奥秘。

- 最后，为了将理论知识转化为实践技能，**“动手实践”**部分提供了一系列精心设计的问题，引导读者通过计算和分析，亲手解决与[神经元动力学](@entry_id:1128649)、[网络设计](@entry_id:267673)和学习规则相关的具体问题，从而加深对核心概念的理解。

## 原理与机制

本章在前文介绍的基础上，深入探讨了前馈[脉冲神经网络](@entry_id:1132168)（SNN）的核心工作原理与基本机制。我们将从网络的结构基础出发，逐步解析单个神经元的动力学行为、信息在网络中的编码与计算方式，直至网络层面的动态特性与学习法则。通过本章的学习，读者将对前馈SNN如何利用时间动态进行计算建立一个系统而严谨的认知框架。

### 前馈[脉冲网络](@entry_id:1132166)的结构基础

[脉冲神经网络](@entry_id:1132168)在宏观上可以被描述为一个[有向图](@entry_id:920596) $G=(V, E)$，其中顶点 $V$ 代表神经元，有向边 $E$ 代表突触连接。网络的一个关键拓扑特性在于其连接模式是否包含环路。**前馈脉冲神经网络（Feedforward Spiking Neural Network）** 的核心结构特征在于其底层连接图是一个**有向无环图（Directed Acyclic Graph, DAG）**。这意味着信息流在网络中是单向的，从输入层开始，逐层传递，直至输出层，不存在任何形式的反馈回路。

这种无环结构具有深远的计算意义。有向无环图的一个基本性质是其顶点可以被**[拓扑排序](@entry_id:156507)（topological ordering）**。[拓扑排序](@entry_id:156507)为网络中的所有神经元 $v \in V$ 分配一个唯一的序号 $\pi(v) \in \{1, \dots, |V|\}$，使得对于网络中的任意一条突触连接 $(j \to i)$，其始端神经元 $j$ 的序号总是小于末端神经元 $i$ 的序号，即 $\pi(j)  \pi(i)$。

[拓扑排序](@entry_id:156507)的存在意味着前馈SNN的计算过程是天然有序且无歧义的。我们可以按照拓扑顺序逐一计算每个神经元的状态和输出。当计算神经元 $i$ 的状态时，所有为其提供输入的上游神经元（即其父节点）的状态都已经被确定。因此，整个网络在时间窗口 $[0, T]$ 内的完整时空动态，可以通过对神经元的一次性**单遍评估（single-pass evaluation）** 来精确构建，无需回溯或迭代求解。 这一特性与包含环路的循环神经网络（Recurrent SNN）形成鲜明对比。在循环网络中，尤其是当环路的总传导延迟为零时，神经元的当前状态可能瞬时地依赖于环路中其他神经元的当前状态，从而导致需要求解复杂的[非线性](@entry_id:637147)[代数方程](@entry_id:272665)组，这可能引发解的不存在或不唯一等“非[适定性](@entry_id:148590)”（non-well-posedness）问题。 因此，[前馈网络](@entry_id:1124893)的DAG结构保证了其动力学解的[适定性](@entry_id:148590)与计算上的易处理性。

值得注意的是，只要网络的连接图保持为DAG且所有神经元和突触的动态行为都是因果的（即输出不依赖于未来输入），这种单遍评估的有效性就得以保证。即使网络中存在跨越多层的“[跳跃连接](@entry_id:637548)”（skip connections），只要它们不破坏图的无环性，计算流程的有效性依然不变。

### 脉冲活动的数学表示

为了精确地描述SNN中的计算，我们首先需要为脉冲活动建立一个严格的数学模型。一个神经元在一段时间内的输出，即一个**[脉冲序列](@entry_id:1132157)（spike train）**，本质上是在连续时间轴上的一系列离散事件。由于神经元存在[不应期](@entry_id:152190)，任何有限时间窗口 $[0, T]$ 内的脉冲数量都是有限的。

我们可以通过至少两种等价的方式来形式化地表示一个[脉冲序列](@entry_id:1132157)：

1.  **有序时间戳序列**：将[脉冲序列](@entry_id:1132157)表示为一个有限、严格递增的时间戳序列 $(t_1, t_2, \dots, t_K)$，其中 $0 \le t_1  t_2  \dots  t_K \le T$。$K$ 是脉冲总数，可以为零（对应静息神经元）。如果一个SNN有 $N_{\text{in}}$ 个输入通道和 $N_{\text{out}}$ 个输出通道，那么整个网络就可以被看作一个映射 $F_T: (\mathcal{S}_T)^{N_{\text{in}}} \to (\mathcal{S}_T)^{N_{\text{out}}}$，其中 $\mathcal{S}_T$ 是在 $[0, T]$ 上所有可能的有序时间戳序列的集合。

2.  **简单[计数测度](@entry_id:188748)**：在[点过程](@entry_id:1129862)理论的框架下，一个[脉冲序列](@entry_id:1132157)可以被表示为一个定义在 $[0, T]$ 上的简单[计数测度](@entry_id:188748)（simple counting measure）$\nu = \sum_{k=1}^{K} \delta_{t_k}$，其中 $\delta_{t}$ 是在时间点 $t$ 处的狄拉克单位质量测度。“简单”意味着在任意单个时间点最多只有一个脉冲，这与神经元的[绝对不应期](@entry_id:151661)特性相符。使用这种表示，网络被形式化为一个从输入[测度空间](@entry_id:191702)到输出[测度空间](@entry_id:191702)的映射 $F_T: (\mathcal{N}_T^{\text{sf}})^{N_{\text{in}}} \to (\mathcal{N}_T^{\text{sf}})^{N_{\text{out}}}$，其中 $\mathcal{N}_T^{\text{sf}}$ 是 $[0, T]$ 上所有简单有限[计数测度](@entry_id:188748)的空间。

无论采用哪种表示，一个核心属性必须被精确刻画，那就是**因果性（causality）**，或称**非预期性（non-anticipativity）**。这意味着网络在任意时刻 $t$ 的输出，只能依赖于该时刻及之前（$t' \le t$）的输入。假设网络在 $t=0$ 时从静息状态启动，该性质可以被形式化地表达为：
$$
\mathsf{R}_t\big(F_T(S_{\text{in}})\big) = F_t\big(\mathsf{R}_t(S_{\text{in}})\big)
$$
这里，$S_{\text{in}}$ 是完整的输入[脉冲序列](@entry_id:1132157)，$\mathsf{R}_t$ 是一个[限制算子](@entry_id:754316)，它将一个[脉冲序列](@entry_id:1132157)截断，只保留 $[0, t]$ 区间内的脉冲。$F_t$ 则表示在更短的时间窗口 $[0, t]$ 上运行的同一网络。这个等式优雅地表明，要计算到 $t$ 时刻为止的输出，我们只需要知道到 $t$ 时刻为止的输入即可。

### 脉冲神经元：动力学与整合机制

前馈SNN的基本计算单元是脉冲神经元。**漏电积分-发放（Leaky Integrate-and-Fire, LIF）** 模型是其中最经典和广泛使用的模型之一。

LIF模型将神经元[细胞膜](@entry_id:146704)抽象为一个简单的并行[RC电路](@entry_id:275926)，由一个[膜电容](@entry_id:171929) $C$ 和一个漏电阻（其电导为 $g_L$）并联构成。根据基尔霍夫电流定律，注入神经元的总电流 $I(t)$ 分为两部分：一部分为电容器充电（$I_C = C \frac{dV}{dt}$），另一部分通过漏电通路流失（$I_L = g_L(V(t)-E_L)$）。由此，我们可以推导出描述膜电位 $V(t)$ 在阈下（subthreshold）演化的核心[微分](@entry_id:158422)方程：
$$
C \frac{dV(t)}{dt} = -g_L\big(V(t) - E_L\big) + I(t)
$$
这个方程中的各个参数具有明确的物理和计算意义：
-   **$C$ ([膜电容](@entry_id:171929))**：代表膜[存储电荷](@entry_id:1132461)的能力。$C$ 越大，膜电位对电流的响应越慢，体现了对输入的**积分**作用。
-   **$g_L$ (漏电导)**：量化了膜的“漏电”程度。这个项使得在没有输入时，膜电位会指数衰减回[静息电位](@entry_id:176014)，构成了模型的“遗忘”机制。
-   **$E_L$ (漏电[反转电位](@entry_id:177450))**：是漏电通路的平衡电位，通常也就是神经元的**静息电位**。
-   **$\tau_m = C/g_L$ ([膜时间常数](@entry_id:168069))**：是膜电位指数衰减的时间尺度，决定了神经元的“记忆窗口”长度。

当膜电位 $V(t)$ 从下方达到一个预设的**[发放阈值](@entry_id:198849) $\theta$** 时，[LIF神经元](@entry_id:1127215)会“发放”一个脉冲。紧接着，其膜电位被瞬时重置到一个**复位电位 $V_r$**（通常 $V_r  \theta$），并可能进入一个短暂的不应期。这个“事件驱动”的脉冲发放与复位机制，构成了LIF模型的[非线性](@entry_id:637147)计算核心。

注入神经元的突触电流 $I(t)$ 本身也有不同的建模方式，这深刻影响着神经元的计算特性。两种主流模型是：

1.  **[基于电流的突触](@entry_id:1123292)（Current-Based, CUBA）**：[突触电流](@entry_id:1132766)被建模为 $I_{\text{syn}}(t) = \sum_i w_i \kappa(t-t_i)$，其中 $w_i$ 是突触权重，$\kappa(t)$ 是一个标准的突触后电流波形。在这种模型中，来自不同突触的输入电流**线性叠加**，并且与突触后神经元的膜电位 $V(t)$ 无关。这意味着神经元的有效[膜时间常数](@entry_id:168069) $\tau_m$ 是一个固定值，不随输入活动强度而改变。

2.  **[基于电导的突触](@entry_id:1122856)（Conductance-Based, COBA）**：突触电流被建模为 $I_{\text{syn}}(t) = \sum_i g_i(t) (E_{\text{rev},i}-V(t))$，其中 $g_i(t)$ 是时变的[突触电导](@entry_id:193384)，而 $E_{\text{rev},i}$ 是该突触对应的[反转电位](@entry_id:177450)。这种模型更加符合生物物理现实。关键区别在于，[突触电流](@entry_id:1132766)的大小现在依赖于突触后膜电位 $V(t)$ 和[反转电位](@entry_id:177450)之间的差值（即驱动力）。

将COBA模型代入LIF方程后，膜[动力学方程](@entry_id:751029)变为：
$$
C \frac{dV}{dt} = -\left(g_L + \sum_i g_i(t)\right)V(t) + \dots
$$
可以看出，总的有效电导 $g_{\text{total}}(t) = g_L + \sum_i g_i(t)$ 是随输入活动动态变化的。这导致**有效[膜时间常数](@entry_id:168069) $\tau_{\text{eff}}(t) = C/g_{\text{total}}(t)$ 也是动态变化的**。当大量突触被激活时（无论是兴奋性还是抑制性），$g_{\text{total}}(t)$ 增大，$\tau_{\text{eff}}(t)$ 减小，使得神经元对输入的响应更快，膜也变得“更漏”。这种效应，特别是当抑制性输入的反转电位接近静息电位时，被称为**分流抑制（shunting inhibition）**。它通过增大分母 $g_{\text{total}}(t)$ 来[非线性](@entry_id:637147)地、除法式地缩放其他兴奋性输入的效果，被认为是实现**[除法归一化](@entry_id:894527)（divisive normalization）**——一种在生物视觉等系统中广泛存在的规范化计算——的关键生物物理机制。

### 网络层面的动力学与稳定性

我们将单个神经元的动力学特性与网络的宏观结构相结合，可以揭示前馈SNN整体的动态行为。

首先，[前馈网络](@entry_id:1124893)是**固有稳定**的。一个系统的稳定性意味着有界输入产生有界输出（BIBO稳定）。在前馈SNN中，由于不存在反馈环路来放大或循环传递活动，信号只能[单向传播](@entry_id:174820)。每一层神经元的活动都由其上游有限的输入和自身的漏电、复位机制所约束。因此，只要输入活动的速率有界，网络中每一层神经元的输出速率也必然有界。 我们可以通过一个更形式化的角度来理解这一点：如果我们将网络连接的有效强度[矩阵表示](@entry_id:146025)为 $W$，那么对于一个根据[拓扑排序](@entry_id:156507)编号的神经元，这个矩阵 $W$ 是严格上（或下）三角的。这样的矩阵是幂零的（nilpotent），其谱半径（最大特征值的模）为零。由于稳定性（特别是在[线性系统理论](@entry_id:172825)中）的充分条件是谱半径小于1，[前馈网络](@entry_id:1124893)以 $\rho(W)=0$ 的形式天然地满足了这个条件。

这种固有的稳定性和无环结构导致了一个重要的推论：**在没有随时间变化的外部输入时，前馈SNN无法产生自持性的内部活动或振荡**。假设在时刻 $t_0$ 之后，所有外部输入都变为恒定。网络中在此之后发生的任何脉冲活动，都必须是 $t_0$ 之前发生的瞬态活动的“回响”。由于网络是有限深度的DAG，并且每个神经元都具有衰减记忆（由漏电项和有限时程的突触响应保证），这股活动的“波”将逐层传播，其影响最终会完全消散。活动无法被反射回来以形成持续的循环，因此网络最终必然会进入一个静息或[稳态](@entry_id:139253)的非振荡状态。

然而，尽管[前馈网络](@entry_id:1124893)是稳定的，但在深度网络中却面临着**活动消失（vanishing activity）**的风险。想象一个很深的网络，如果每一层的总输入驱动（突触输入加上偏置电流）不足以克服神经元的漏电和[发放阈值](@entry_id:198849)，那么脉冲活动在逐层传递时可能会逐渐减弱，最终在到达网络深层之前就完全消失。 为了维持信息的持续流动，必须确保每一层的净输入足以支持脉冲发放。通过对[LIF模型](@entry_id:1127214)的[稳态分析](@entry_id:271474)可以推导出，为了保证第 $l$ 层的发放率 $r_l > 0$，一个充分条件是其总平均输入电流必须超过一个与漏电和阈值相关的临界值。这可以转化为对该层偏置电流 $b_l$ 的一个约束，例如：
$$
b_l > \frac{\theta_l}{\tau_{m,l}} - w_l r_{l-1}
$$
其中 $w_l r_{l-1}$ 是来自上一层的平均突触输入。这个不等式保证了即使在最坏情况下（例如，膜电位刚好在阈值处复位，漏电损失最大），输入电流也足以驱动神经元再次发放脉冲，从而防止了活动流的中断。

### 时间域中的信息编码与计算

SNN的核心魅力在于其直接利用脉冲的精确时间进行计算。信息不是编码在静态的激活值中，而是编码在脉冲的时间结构里。

#### 速率编码与[延迟编码](@entry_id:1127087)

让我们通过一个简单的二元通信任务来比较两种基本的编码策略。假设发送符号 "1" 对应于一个速率为 $\lambda$ 的泊松脉冲过程，而发送 "0" 则对应于静默。

-   **速率编码（Rate Coding）**：解码器在一个固定的时间窗口 $T_R$ [内积](@entry_id:750660)分脉冲。如果在窗口内观测到至少一个脉冲，则判定为 "1"，否则为 "0"。决策总是在窗口结束时刻 $T_R$ 做出。
-   **[延迟编码](@entry_id:1127087)（Latency Coding）**：解码器等待第一个脉冲的到来。如果在某个截止时间 $\tau_L$ 之前接收到脉冲，则立即判定为 "1"；如果直到 $\tau_L$ 仍无脉冲，则判定为 "0"。决策时间是可变的，取决于第一个脉冲的到达时间。

在保证两种方案具有相同平均错误率 $\epsilon$ 的前提下，我们可以推导出它们的平均决策延迟。分析表明，为了达到相同的错误率，必须有 $T_R = \tau_L = \frac{1}{\lambda}\ln(\frac{p}{\epsilon})$，其中 $p$ 是符号 "1" 的先验概率。然而，[延迟编码](@entry_id:1127087)的平均决策时间总是严格小于速率编码的决策时间 $T_R$。其差值为 $\Delta = \frac{1}{\lambda}(p - \epsilon - p\ln(\frac{p}{\epsilon})) > 0$。这是因为[延迟编码](@entry_id:1127087)利用了“提前终止”的优势：一旦信息（第一个脉冲）到达，决策即可做出，无需等到固定的窗口结束。这个例子有力地证明了**[时间编码](@entry_id:1132912)（如[延迟编码](@entry_id:1127087)）在信息传输效率上相比于纯粹的速率编码具有根本优势**。

#### 排序编码

另一种强大的[时间编码](@entry_id:1132912)策略是**排序编码（Rank-Order Coding）**，其中信息被编码在一组输入神经元发放其第一个脉冲的相对顺序中，而非[绝对时间](@entry_id:265046)。 一个设计良好的前馈层应该能够检测并传递这种排序信息。

考虑一个简单的线性积分模型，输出神经元 $j$ 的膜电位为 $V_j(t) = \sum_{i=1}^{N} w_{ji} K(t - t_i)$，其中 $t_1  t_2  \dots  t_N$ 是输入[脉冲时间](@entry_id:1132155)， $K(t)$ 是一个指数衰减的突触核。为了让输出神经元 $j$ 恰好在其对应的输入脉冲 $t_j$ 到达时发放第一个脉冲（即“保持排序”），其突触权重 $w_{ji}$ 必须满足一系列约束。具体来说，在 $t_j$ 到达前的任何时刻，其膜电位都必须低于阈值 $\Theta$，而在 $t_j$ 到达的瞬间，膜电位必须跨过阈值。这为“主”突触权重 $w_{jj}$ 设定了一个必要的下界，该下界取决于所有在 $t_j$ 之前到达的脉冲的贡献。例如，对于输出神经元3，其权重 $w_{33}$ 必须足够大，以在 $t_1$ 和 $t_2$ 的衰减贡献之上，将膜电位推过阈值。通过求解这些约束，我们可以精确地设计出能够进行排序检测的突触权重矩阵。

#### 时间阈值逻辑

我们可以将神经元的计算功能抽象为一个更普适的框架，即**时间阈值逻辑（temporal threshold logic）**。 在这个视图下，神经元被看作一个计算[判别函数](@entry_id:637860) $G(\mathbf{t})$ 的装置，其中 $\mathbf{t}=(t_1, \dots, t_N)$ 是输入脉冲时间向量。网络发放脉冲的条件是 $G(\mathbf{t}) \ge 0$。

对于一个没有漏电的线性积分神经元，其膜电位 $V(t)$ 是多个指数衰减函数的和。对该函数求导可以发现，$V'(t) = -V(t)/\tau$，这意味着在任意两个脉冲之间，膜电位总是单调地朝向零衰减。因此，$V(t)$ 的最大值必然在其定义域的[边界点](@entry_id:176493)达到，即在各个输入脉冲的到达时刻 $t_k$ 或 $t=0$。这意味着，要判断膜电位是否曾越过阈值 $\theta$，我们只需检查在所有输入脉冲到达时刻的电位值即可。因此，[判别函数](@entry_id:637860)可以被写为一个简洁的[封闭形式](@entry_id:272960)：
$$
G(\mathbf{t}) = \max\left(0, \max_{k \in \{1,\dots,N\}} V(t_k) \right) - \theta
$$
这个表达式明确地将神经元的发放决策与输入[脉冲时间](@entry_id:1132155)向量 $\mathbf{t}$ 联系起来，其[决策边界](@entry_id:146073) $G(\mathbf{t}) = 0$ 在高维的时间空间中定义了一个复杂的曲面。这形式化地表明，单个脉冲神经元就是一个强大的[时空模式](@entry_id:203673)检测器。

### 前馈SNN的学习原理

由于脉冲发放是一个非连续、非可微的事件（通常用[阶跃函数](@entry_id:159192) $s = H(u-\theta)$ 建模），将基于梯度的学习算法（如[反向传播](@entry_id:199535)）应用于SNN面临着根本性的挑战。

为了解决这个问题，研究人员引入了**代理梯度（surrogate gradients）** 的概念。 其核心思想是在反向传播计算梯度时，用一个光滑、可微的函数 $g_\beta(z)$ 来替代[阶跃函数](@entry_id:159192)那在几乎所有点都为零的导数。这个代理函数通常是一个钟形曲线（如高斯或其变体），其形状由一个陡峭度参数 $\beta$ 控制。

然而，即使使用了[代理梯度](@entry_id:1132703)，SNN的训练依然会面临**梯度消失**的问题，这通常被称为“死亡神经元问题”。当一个神经元的膜电位由于权重和偏置的设置，长期稳定在远高于或远低于其[发放阈值](@entry_id:198849)的区域时，它的膜电位-阈值差 $z=u-\theta$ 的分布将远离[代理梯度](@entry_id:1132703)函数 $g_\beta(z)$ 的有效区域（通常是 $z=0$ 附近）。此时，无论下游的[误差信号](@entry_id:271594)有多大，反向传播回来的梯度都会因乘以一个接近于零的代理梯度值而消失，导致该神经元的权重无法更新。

我们可以通过分析代理梯度的[期望值](@entry_id:150961)来量化这个问题。假设膜电位-阈值差 $z$ 服从均值为 $\mu_z = -\theta$、方差为 $\sigma_z^2 = \|w\|^2$ 的高斯分布，使用高斯型的[代理梯度](@entry_id:1132703)，可以推导出[代理梯度](@entry_id:1132703)的[期望值](@entry_id:150961)为：
$$
\mathbb{E}[g_\beta(z)] = \frac{\beta}{\sqrt{\pi\left(1 + 2 \beta^2 \sigma_z^2\right)}} \exp\left(-\frac{\beta^2 \mu_z^2}{1 + 2 \beta^2 \sigma_z^2}\right)
$$
这个表达式清晰地揭示了梯度消失的两个主要原因：
1.  **大的 $|\mu_z|$**：即神经元的平均膜电位远离其阈值。
2.  **小的 $\sigma_z^2$**：即膜电位变化范围过小，导致其被“卡”在一个远离阈值的稳定状态。

为了维持有效的[梯度流](@entry_id:635964)并成功训练SNN，研究者们提出了一系列策略：
-   **阈值[退火](@entry_id:159359)/平衡（Threshold Annealing/Balancing）**：在训练过程中动态调整神经元的阈值 $\theta$，使其保持在神经元平均膜电位附近，从而驱动 $\mu_z \to 0$。
-   **[代理梯度](@entry_id:1132703)陡峭度退火（Steepness Annealing）**：训练初期使用一个较小的 $\beta$ 值，这对应一个较“宽”的[代理梯度](@entry_id:1132703)，能够捕捉到更大范围内的膜电位波动，从而提供初始的梯度信号。随着训练的进行，逐渐增大 $\beta$，使神经元的行为更接近理想的[阶跃函数](@entry_id:159192)。
-   **噪声注入（Noise Injection）**：向神经元的膜电位中注入少量噪声。这能有效增加膜电位的方差 $\sigma_z^2$，帮助“卡住”的神经元探索更广阔的[状态空间](@entry_id:160914)，从而有机会进入[代理梯度](@entry_id:1132703)的有效区域并开始学习。

这些原理和技术构成了现代[SNN训练](@entry_id:1131801)方法的基础，使得在复杂的任务上训练深度前馈脉冲神经网络成为可能。