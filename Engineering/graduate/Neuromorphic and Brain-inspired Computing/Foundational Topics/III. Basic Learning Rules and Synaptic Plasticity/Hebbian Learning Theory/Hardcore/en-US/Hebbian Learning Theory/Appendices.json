{
    "hands_on_practices": [
        {
            "introduction": "The foundational Hebbian postulate, \"cells that fire together, wire together,\" can lead to unstable, runaway synaptic growth if applied naively. This first exercise provides a concrete, hands-on calculation to explore a common solution: incorporating a homeostatic mechanism. By computing a single weight update, you will gain an intuition for the critical balance between associative potentiation and a stabilizing depression term that counteracts hyperactivity, a core principle in modern plasticity models .",
            "id": "4054261",
            "problem": "Consider an on-chip learning circuit in a neuromorphic synapse that implements rate-based Hebbian plasticity with a quadratic homeostatic stabilization term. The pre-synaptic activity is encoded as a firing rate $r_{\\text{pre}}$ and the post-synaptic activity is encoded as a firing rate $r_{\\text{post}}$, both represented by current-mode signals in an analog multiplier and squaring stage. The circuit computes a weight update increment $\\Delta w$ per learning cycle according to the following rate-based Hebbian rule derived from Hebb’s postulate of learning and homeostatic control: \n$$\n\\Delta w = \\eta \\left(r_{\\text{pre}}\\,r_{\\text{post}} - \\beta\\,r_{\\text{post}}^{2}\\right),\n$$\nwhere $\\eta$ is a learning rate constant and $\\beta$ is a homeostatic scaling parameter. Assume that $r_{\\text{pre}}$ and $r_{\\text{post}}$ are measured over a sufficiently long window to be treated as stationary rates and that $\\eta$ has been chosen so that $\\Delta w$ is dimensionless. For a particular learning cycle, the measured values are $r_{\\text{pre}} = 20\\ \\text{Hz}$, $r_{\\text{post}} = 15\\ \\text{Hz}$, $\\eta = 10^{-4}$, and $\\beta = 0.5$. Compute the numerical value of the dimensionless update increment $\\Delta w$ for this cycle. Provide the final answer as a single real number. No rounding is required.",
            "solution": "The problem requires the computation of the dimensionless weight update increment, $\\Delta w$, for an on-chip learning circuit. The behavior of the circuit is governed by a specified rate-based Hebbian plasticity rule that includes a quadratic homeostatic stabilization term.\n\nThe provided equation for the weight update is:\n$$\n\\Delta w = \\eta \\left(r_{\\text{pre}}\\,r_{\\text{post}} - \\beta\\,r_{\\text{post}}^{2}\\right)\n$$\nThe parameters for a specific learning cycle are given as:\n- The pre-synaptic firing rate, $r_{\\text{pre}} = 20\\ \\text{Hz}$.\n- The post-synaptic firing rate, $r_{\\text{post}} = 15\\ \\text{Hz}$.\n- The learning rate constant, $\\eta = 10^{-4}$.\n- The homeostatic scaling parameter, $\\beta = 0.5$.\n\nThe problem states that $\\Delta w$ is a dimensionless quantity, which implies that the units of $\\text{Hz}^2$ from the rate terms are absorbed by the learning rate constant $\\eta$. We proceed by substituting the given numerical values directly into the equation.\n\nSubstitution of the given values yields:\n$$\n\\Delta w = 10^{-4} \\left( (20)(15) - (0.5)(15)^{2} \\right)\n$$\nWe first evaluate the two terms inside the parentheses. The first term corresponds to the standard Hebbian product:\n$$\nr_{\\text{pre}}\\,r_{\\text{post}} = (20)(15) = 300\n$$\nThe second term corresponds to the homeostatic stabilization component:\n$$\n\\beta\\,r_{\\text{post}}^{2} = (0.5)(15)^{2}\n$$\nCalculating the square of $r_{\\text{post}}$:\n$$\n15^{2} = 225\n$$\nNow, multiplying by $\\beta$:\n$$\n\\beta\\,r_{\\text{post}}^{2} = (0.5)(225) = 112.5\n$$\nNext, we compute the difference between these two terms, as specified by the expression inside the parentheses:\n$$\n(r_{\\text{pre}}\\,r_{\\text{post}} - \\beta\\,r_{\\text{post}}^{2}) = 300 - 112.5 = 187.5\n$$\nFinally, we multiply this result by the learning rate $\\eta$ to obtain the final weight update increment $\\Delta w$:\n$$\n\\Delta w = 10^{-4} \\times 187.5\n$$\nThis calculation is equivalent to shifting the decimal point four places to the left:\n$$\n\\Delta w = 0.01875\n$$\nThus, the numerical value of the dimensionless update increment for the given learning cycle is $0.01875$.",
            "answer": "$$\n\\boxed{0.01875}\n$$"
        },
        {
            "introduction": "Moving beyond simple stability, this practice explores how a local learning rule can give rise to powerful, network-level computation. You will analyze Oja's rule, a classic and elegant model of stabilized Hebbian learning. The analysis reveals a profound result: the synaptic weights of the neuron spontaneously learn to extract the principal component of the input data . This exercise provides a crucial link between low-level synaptic mechanisms and high-level information processing, demonstrating how unsupervised learning extracts meaningful structure from sensory inputs.",
            "id": "4046811",
            "problem": "Consider a single linear neuron in a neuromorphic system whose synaptic weight vector $w(t) \\in \\mathbb{R}^{2}$ is adapted from streaming inputs according to a local activity-dependent rule motivated by the Hebbian learning principle (“cells that fire together wire together”) and stabilized by a local normalizer. The neuron receives inputs $x(t) \\in \\mathbb{R}^{2}$ drawn as an Independent and Identically Distributed (i.i.d.) zero-mean stationary random vector with covariance matrix $C = \\begin{pmatrix} a  b \\\\ b  d \\end{pmatrix}$, where $a  0$, $d  0$, and $ad - b^{2}  0$. The instantaneous output is $y(t) = w(t)^{\\top} x(t)$, and the online adaptation at time $t$ is given by $\\Delta w(t) = \\eta \\, y(t) \\big(x(t) - y(t) w(t)\\big)$ for a small learning rate $\\eta  0$.\n\nStarting only from the Hebbian learning principle and the provided adaptation rule, treat the learning process in the mean-field continuous-time limit under the i.i.d. and stationarity assumptions. Derive the expected dynamical system for $w(t)$ in terms of the moments of $x(t)$, analyze its fixed points and their stability, and determine the limiting expected output power $\\mathbb{E}[y(t)^{2}]$ at convergence. Express your final answer as a single closed-form analytic expression in terms of $a$, $b$, and $d$. No units are required.",
            "solution": "The problem asks for the limiting expected output power of a single linear neuron whose synaptic weight vector $w(t)$ adapts according to a specific Hebbian-type rule. The solution requires analyzing the dynamics of the weight vector in the mean-field continuous-time limit.\n\nFirst, we are given the discrete-time update rule for the weight vector $w(t) \\in \\mathbb{R}^{2}$:\n$$ \\Delta w(t) = w(t+1) - w(t) = \\eta \\, y(t) \\big(x(t) - y(t) w(t)\\big) $$\nwhere $y(t) = w(t)^{\\top} x(t)$ is the neuron's output, $x(t) \\in \\mathbb{R}^{2}$ is the input vector, and $\\eta  0$ is a small learning rate.\n\nTo derive the expected dynamical system in the continuous-time limit, we treat $w$ as a slowly varying variable (due to small $\\eta$) and average the update rule over the stationary distribution of the fast-varying input $x(t)$. The change $\\Delta w(t)$ is approximated by a differential $\\frac{dw}{dt}$, scaled by $\\eta$.\n$$ \\frac{dw}{dt} = \\mathbb{E}_{x} \\left[ y(x) \\big(x - y(x) w \\big) \\right] $$\nwhere the expectation $\\mathbb{E}_{x}[\\cdot]$ is taken over the distribution of the input vector $x$. The weight vector $w$ is held constant inside the expectation. Substituting $y(x) = w^{\\top}x$:\n$$ \\frac{dw}{dt} = \\mathbb{E}_{x} \\left[ (w^{\\top}x) \\left(x - (w^{\\top}x)w \\right) \\right] $$\nUsing the linearity of the expectation operator, we can separate the terms:\n$$ \\frac{dw}{dt} = \\mathbb{E}_{x} \\left[ (w^{\\top}x)x \\right] - \\mathbb{E}_{x} \\left[ (w^{\\top}x)^2 w \\right] $$\nLet's analyze each term. For the first term:\n$$ \\mathbb{E}_{x} \\left[ (w^{\\top}x)x \\right] = \\mathbb{E}_{x} \\left[ x(x^{\\top}w) \\right] = \\mathbb{E}_{x} \\left[ xx^{\\top} \\right] w $$\nThe problem states that the input $x$ is a zero-mean random vector with covariance matrix $C = \\begin{pmatrix} a  b \\\\ b  d \\end{pmatrix}$. By definition, $C = \\mathbb{E}[ (x-\\mathbb{E}[x])(x-\\mathbb{E}[x])^{\\top} ]$. Since $\\mathbb{E}[x] = 0$, the covariance matrix is $C = \\mathbb{E}[xx^{\\top}]$. Therefore, the first term is $Cw$.\n\nFor the second term:\n$$ \\mathbb{E}_{x} \\left[ (w^{\\top}x)^2 w \\right] = w \\, \\mathbb{E}_{x} \\left[ (w^{\\top}x)^2 \\right] = w \\, \\mathbb{E}_{x} \\left[ (w^{\\top}x)(x^{\\top}w) \\right] $$\n$$ = w \\, w^{\\top} \\mathbb{E}_{x} \\left[ xx^{\\top} \\right] w = w (w^{\\top}Cw) $$\nCombining the terms, we obtain the mean-field continuous-time dynamical system for $w(t)$:\n$$ \\frac{dw}{dt} = \\eta \\left( Cw - (w^{\\top}Cw)w \\right) $$\nThis is a form of Oja's rule for a single neuron.\n\nNext, we analyze the fixed points $w^*$ of this system by setting $\\frac{dw}{dt} = 0$:\n$$ Cw^* - (w^{*\\top}Cw^*)w^* = 0 $$\nThis equation implies that $Cw^*$ must be collinear with $w^*$. This means that any non-zero fixed point $w^*$ must be an eigenvector of the matrix $C$. Let $w^*$ be an eigenvector of $C$ with a corresponding eigenvalue $\\lambda$, so that $Cw^* = \\lambda w^*$. Substituting this into the fixed point equation:\n$$ \\lambda w^* - (w^{*\\top}(\\lambda w^*)) w^* = 0 $$\n$$ \\lambda w^* - \\lambda (w^{*\\top}w^*) w^* = 0 $$\n$$ \\lambda w^* (1 - \\|w^*\\|^2) = 0 $$\nThis equation admits two kinds of solutions:\n$1$. $w^* = 0$, the trivial fixed point.\n$2$. If $\\lambda \\neq 0$, then $\\|w^*\\|^2 = 1$. The non-trivial fixed points are the unit-norm eigenvectors of the covariance matrix $C$.\n\nThe stability of these fixed points determines the long-term behavior of $w(t)$.\nFor the trivial fixed point $w^*=0$, we linearize the system around $w=0$. The term $(w^{\\top}Cw)w$ is of order $\\|w\\|^3$ and can be neglected for small $\\|w\\|$. The linearized system is $\\frac{dw}{dt} \\approx \\eta Cw$. The given conditions $a0, d0$, and $ad-b^20$ ensure that $C$ is a positive definite matrix. All its eigenvalues are strictly positive. Thus, any small perturbation from $w=0$ will grow, making the fixed point $w^*=0$ unstable.\n\nFor the non-trivial fixed points $w^*=e_i$, where $e_i$ are the normalized eigenvectors of $C$, the weight vector converges to the direction of the eigenvector associated with the largest eigenvalue, $\\lambda_1$. This is a standard result for Oja's rule. The dynamics project the vector $w$ onto the principal subspace of $C$ while normalizing its length to $1$. The stable fixed points are $w_{ss} = \\pm e_1$, where $e_1$ is the normalized eigenvector corresponding to the largest eigenvalue $\\lambda_1$ of $C$. All other eigenvector fixed points are unstable (saddle points).\n\nThe problem asks for the limiting expected output power $\\mathbb{E}[y(t)^2]$ at convergence. At convergence ($t \\to \\infty$), the weight vector $w(t)$ approaches a stable fixed point, $w_{ss}$. The expected output power is calculated by averaging over the input distribution, with the weight vector fixed at its steady-state value $w_{ss}$:\n$$ \\lim_{t\\to\\infty} \\mathbb{E}_{x}[y(t)^2] = \\mathbb{E}_{x}[(w_{ss}^{\\top}x)^2] = \\mathbb{E}_{x}[w_{ss}^{\\top}xx^{\\top}w_{ss}] $$\nPulling the constant vector $w_{ss}$ out of the expectation:\n$$ = w_{ss}^{\\top} \\mathbb{E}_{x}[xx^{\\top}] w_{ss} = w_{ss}^{\\top} C w_{ss} $$\nSince $w_{ss}$ is a normalized eigenvector $e_1$ (or $-e_1$) corresponding to the largest eigenvalue $\\lambda_1$, we have $Ce_1 = \\lambda_1 e_1$ and $\\|e_1\\|^2 = 1$.\n$$ w_{ss}^{\\top} C w_{ss} = (\\pm e_1)^{\\top} C (\\pm e_1) = e_1^{\\top} C e_1 = e_1^{\\top}(\\lambda_1 e_1) = \\lambda_1(e_1^{\\top}e_1) = \\lambda_1 $$\nThus, the limiting expected output power is the largest eigenvalue of the input covariance matrix $C$.\n\nFinally, we calculate the eigenvalues of $C = \\begin{pmatrix} a  b \\\\ b  d \\end{pmatrix}$. The eigenvalues $\\lambda$ are the roots of the characteristic equation $\\det(C - \\lambda I) = 0$:\n$$ \\det \\begin{pmatrix} a-\\lambda  b \\\\ b  d-\\lambda \\end{pmatrix} = (a-\\lambda)(d-\\lambda) - b^2 = 0 $$\n$$ \\lambda^2 - (a+d)\\lambda + ad - b^2 = 0 $$\nUsing the quadratic formula to solve for $\\lambda$:\n$$ \\lambda = \\frac{-(-(a+d)) \\pm \\sqrt{(-(a+d))^2 - 4(1)(ad-b^2)}}{2(1)} $$\n$$ \\lambda = \\frac{a+d \\pm \\sqrt{a^2 + 2ad + d^2 - 4ad + 4b^2}}{2} $$\n$$ \\lambda = \\frac{a+d \\pm \\sqrt{a^2 - 2ad + d^2 + 4b^2}}{2} $$\n$$ \\lambda = \\frac{a+d \\pm \\sqrt{(a-d)^2 + 4b^2}}{2} $$\nThe two eigenvalues are $\\lambda_1 = \\frac{a+d + \\sqrt{(a-d)^2 + 4b^2}}{2}$ and $\\lambda_2 = \\frac{a+d - \\sqrt{(a-d)^2 + 4b^2}}{2}$.\nThe term $\\sqrt{(a-d)^2 + 4b^2}$ is always non-negative, so $\\lambda_1$ is the larger of the two eigenvalues.\nThe limiting expected output power is $\\lambda_1$.",
            "answer": "$$ \\boxed{\\frac{a+d + \\sqrt{(a-d)^2 + 4b^2}}{2}} $$"
        },
        {
            "introduction": "Theoretical neuroscience offers a diverse \"zoo\" of plasticity models, including the simple Hebbian rule, Oja's rule, and the Bienenstock–Cooper–Munroe (BCM) model. This final practice moves from theory to application, challenging you to act as a computational neuroscientist. By generating synthetic data and using statistical model selection, you will learn to quantitatively determine which learning rule best explains a given set of observations . This exercise develops essential skills in model fitting and comparison, which are central to validating theories against experimental data in neuromorphic engineering and brain-inspired computing.",
            "id": "4046831",
            "problem": "A program must map experimentally observed synaptic plasticity phenomenology to candidate Hebbian learning models by quantitatively fitting parameters and performing model selection using an information criterion. The educational context is neuromorphic and brain-inspired computing with focus on Hebbian learning theory. The task must be solved from first principles, beginning from core definitions and widely accepted facts about correlation-based learning, normalization, and sliding-threshold homeostasis.\n\nA synapse links a presynaptic neuron with activity rate $x$ (in Hz) to a postsynaptic neuron with activity rate $y$ (in Hz). A synaptic weight $w$ modulates postsynaptic impact and changes by an amount $\\Delta w$ after a trial that lasts for duration $T$ seconds. The observed phenomenology is provided as synthetic, reproducible datasets generated under controlled conditions. The program must classify which of the three candidate Hebbian models best explains each dataset:\n\n- Model $0$ (Simple Hebbian rate rule): weight changes are proportional to the correlation of presynaptic and postsynaptic rates.\n- Model $1$ (Oja normalization rule): weight changes consist of a correlation term and a normalization term that depends on the squared postsynaptic rate and the current weight.\n- Model $2$ (Bienenstock–Cooper–Munroe (BCM) rule): weight changes are correlation-driven with a sliding modification threshold that depends on a slow average of the squared postsynaptic rate.\n\nThe candidate models are instantiated as follows:\n\n1. Simple Hebbian rate rule:\n$$\n\\Delta w_i = \\eta \\, x_i \\, y_i + \\varepsilon_i,\n$$\nwhere $i$ indexes trials, $x_i$ and $y_i$ are measured in Hz, $\\eta$ is a scalar learning rate parameter to be fit, and $\\varepsilon_i$ is zero-mean Gaussian observation noise.\n\n2. Oja normalization rule:\n$$\n\\Delta w_i = \\eta \\, x_i \\, y_i - \\eta \\lambda \\, y_i^2 \\, w_i + \\varepsilon_i,\n$$\nwhere $\\eta$ and $\\lambda$ are scalar parameters to be fit.\n\n3. Bienenstock–Cooper–Munroe rule:\n$$\n\\Delta w_i = \\eta \\, x_i \\, y_i \\, (y_i - \\theta_i) + \\varepsilon_i,\n$$\nwhere $\\eta$ is a scalar parameter to be fit and $\\theta_i$ is a sliding modification threshold computed as an exponential moving average of the squared postsynaptic rate,\n$$\n\\theta_i = (1 - \\rho)\\,\\theta_{i-1} + \\rho \\, y_i^2, \\quad \\rho \\in (0,1), \\quad \\theta_0 \\text{ specified}.\n$$\n\nFundamental base to use in the derivation and algorithm design:\n- Hebbian principle: synaptic changes increase with positive correlation between presynaptic and postsynaptic activities.\n- Normalization/homeostasis: biological systems often regulate synaptic growth by normalization terms that prevent divergence of weights.\n- Sliding-threshold homeostasis: long-term covariance-sensitive plasticity exhibits a threshold that adapts to the activity statistics.\n- Gaussian observation noise and maximum likelihood under the assumption of independent trials lead to least squares parameter estimation and the Akaike Information Criterion (AIC) for model selection.\n\nThe program must perform the following steps for each dataset:\n1. Generate the trial data $(x_i, y_i, w_i, \\Delta w_i)$ using the precise seeds and parameters provided below. The generation procedure ensures scientific realism by using positively supported, correlated log-normal rate distributions in Hz, independent weight samples across observed synapses in dimensionless units, and additive zero-mean Gaussian observation noise in the same units as $\\Delta w$.\n2. Fit the parameters of each candidate model by minimizing the sum of squared residuals under the Gaussian noise assumption. For the Simple Hebbian and BCM rules, this is a one-parameter linear least squares. For Oja, this is a two-parameter linear least squares where the basis functions are $x_i y_i$ and $y_i^2 w_i$.\n3. Compute the Akaike Information Criterion (AIC) for each fitted model,\n$$\n\\mathrm{AIC} = 2k + n \\ln\\!\\left(\\frac{\\mathrm{RSS}}{n}\\right),\n$$\nwhere $k$ is the number of fitted parameters and $n$ is the number of trials with residual sum of squares $\\mathrm{RSS} = \\sum_i (\\Delta w_i - \\widehat{\\Delta w}_i)^2$.\n4. Select the model with the smallest AIC. Encode the choice as an integer: $0$ for Simple Hebbian, $1$ for Oja, and $2$ for BCM.\n\nPhysical and numerical units must be respected:\n- Activity rates $x_i$ and $y_i$ are in Hz.\n- Trial duration $T$ is in seconds.\n- Synaptic weights $w_i$ and weight changes $\\Delta w_i$ are dimensionless.\n- No angles or percentages are involved.\n\nData generation details for reproducibility:\n- For each dataset, sample $n$ trials. For trial $i$, draw a bivariate normal vector $z_i \\sim \\mathcal{N}(\\mu, \\Sigma)$ with correlation coefficient $\\rho_c \\in (-1,1)$ and log-standard deviations $\\sigma_x  0$, $\\sigma_y  0$. Set $x_i = \\exp(z_{i,1})$ and $y_i = \\exp(z_{i,2})$, yielding log-normal rates in Hz. Independently sample $w_i$ from a uniform distribution on $[w_{\\min}, w_{\\max}]$. Compute $\\Delta w_i$ via the specified model equation and add independent Gaussian noise $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma_{\\varepsilon}^2)$.\n\n- Trial duration is $T = 1$ s for all datasets.\n\n- For the BCM rule, compute $\\theta_i$ sequentially with $\\theta_0$ equal to the empirical mean of $y_i^2$ over the dataset and with a specified $\\rho$.\n\nTest suite:\n- Dataset $1$ (seeded Hebbian phenomenology):\n  - Seed: $s = 12345$.\n  - Number of trials: $n = 120$.\n  - Log-normal parameters: $\\mu = [\\ln(5), \\ln(7)]$, $\\sigma_x = 0.4$, $\\sigma_y = 0.4$, correlation $\\rho_c = 0.6$.\n  - Weight range: $w_{\\min} = 0.05$, $w_{\\max} = 0.35$.\n  - Model parameters: use Simple Hebbian with $\\eta = 0.008$.\n  - Noise standard deviation: $\\sigma_{\\varepsilon} = 0.002$.\n\n- Dataset $2$ (seeded Oja phenomenology):\n  - Seed: $s = 54321$.\n  - Number of trials: $n = 150$.\n  - Log-normal parameters: $\\mu = [\\ln(6), \\ln(6)]$, $\\sigma_x = 0.5$, $\\sigma_y = 0.5$, correlation $\\rho_c = 0.5$.\n  - Weight range: $w_{\\min} = 0.05$, $w_{\\max} = 0.90$.\n  - Model parameters: use Oja with $\\eta = 0.020$, $\\lambda = 1.2$.\n  - Noise standard deviation: $\\sigma_{\\varepsilon} = 0.003$.\n\n- Dataset $3$ (seeded BCM phenomenology with sliding threshold):\n  - Seed: $s = 24680$.\n  - Number of trials: $n = 140$.\n  - Log-normal parameters: $\\mu = [\\ln(5), \\ln(10)]$, $\\sigma_x = 0.6$, $\\sigma_y = 0.6$, correlation $\\rho_c = 0.2$.\n  - Weight range: $w_{\\min} = 0.02$, $w_{\\max} = 0.50$.\n  - Model parameters: use BCM with $\\eta = 0.025$, sliding threshold rate $\\rho = 0.05$, initial threshold $\\theta_0$ equal to the empirical mean of $y_i^2$ over the dataset.\n  - Noise standard deviation: $\\sigma_{\\varepsilon} = 0.003$.\n\n- Dataset $4$ (boundary case with weak normalization):\n  - Seed: $s = 13579$.\n  - Number of trials: $n = 130$.\n  - Log-normal parameters: $\\mu = [\\ln(5.5), \\ln(6.5)]$, $\\sigma_x = 0.4$, $\\sigma_y = 0.4$, correlation $\\rho_c = 0.4$.\n  - Weight range: $w_{\\min} = 0.05$, $w_{\\max} = 0.80$.\n  - Model parameters: use Oja with $\\eta = 0.020$, $\\lambda = 0.10$.\n  - Noise standard deviation: $\\sigma_{\\varepsilon} = 0.003$.\n\nAnswer specification:\n- For each dataset, output the integer code of the selected model: $0$ for Simple Hebbian, $1$ for Oja, $2$ for BCM.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[0,1,2,1]\").\n\nNo external input is permitted; all data must be generated internally from the provided specifications. All computations must use consistent units (Hz for rates and seconds for $T$; weight and weight change are dimensionless). The final outputs are integers with no associated units.",
            "solution": "The problem requires the development of a program to classify synthetic datasets of synaptic plasticity according to three candidate Hebbian learning models. The classification is achieved by fitting each model's parameters to a given dataset and using the Akaike Information Criterion (AIC) for model selection. The entire process, from data generation to model selection, is performed from first principles as outlined below.\n\nFirst, we address the generation of synthetic data, which is designed to emulate experimentally observed synaptic activity. For each dataset, we generate $n$ trials. The activity rates of the presynaptic neuron, $x_i$, and the postsynaptic neuron, $y_i$, for each trial $i$ are sampled from a correlated log-normal distribution. This is achieved by first drawing a bivariate normal vector $\\mathbf{z}_i = [z_{i,1}, z_{i,2}]$ from a distribution $\\mathcal{N}(\\boldsymbol{\\mu}, \\mathbf{\\Sigma})$, where the mean vector is $\\boldsymbol{\\mu} = [\\mu_x, \\mu_y]$ and the covariance matrix is $\\mathbf{\\Sigma} = \\begin{pmatrix} \\sigma_x^2  \\rho_c \\sigma_x \\sigma_y \\\\ \\rho_c \\sigma_x \\sigma_y  \\sigma_y^2 \\end{pmatrix}$. The rates, in units of Hz, are then given by $x_i = \\exp(z_{i,1})$ and $y_i = \\exp(z_{i,2})$. The initial synaptic weight for each trial, $w_i$, is sampled independently from a uniform distribution $U(w_{\\min}, w_{\\max})$. The observed change in synaptic weight, $\\Delta w_i$, is then calculated using the equation of the true underlying model specified for that dataset, to which we add zero-mean Gaussian noise, $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma_{\\varepsilon}^2)$. The trial duration $T=1$ s is implicitly absorbed into the model parameters. This procedure generates a set of observations $(\\{x_i\\}, \\{y_i\\}, \\{w_i\\}, \\{\\Delta w_i\\})$ for each dataset, indexed from $i=1, \\dots, n$.\n\nNext, we fit the parameters of the three candidate models to the generated data. The problem states that the observation noise $\\varepsilon_i$ is Gaussian. Under this assumption, the principle of maximum likelihood estimation (MLE) dictates that we should choose the model parameters that maximize the probability of observing the given data. For an additive Gaussian noise model, this is equivalent to minimizing the sum of squared residuals (RSS), which is the core idea of linear least squares. Each of the three candidate models can be cast into a linear regression problem of the general form $\\mathbf{b} = \\mathbf{A}\\mathbf{p}$, where $\\mathbf{b}$ is the $n \\times 1$ vector of observed weight changes $\\Delta w_i$, $\\mathbf{A}$ is the $n \\times k$ design matrix whose columns are the predictors, and $\\mathbf{p}$ is the $k \\times 1$ vector of parameters to be estimated.\n\nThe three models are formulated for least-squares fitting as follows:\n\n1.  **Model 0 (Simple Hebbian rate rule):** The model is $\\Delta w_i = \\eta \\, (x_i y_i) + \\varepsilon_i$. This is a linear model with one parameter, $p_1 = \\eta$. The design matrix $\\mathbf{A}_0$ is an $n \\times 1$ matrix with entries $(x_i y_i)$. The number of fitted parameters is $k_0=1$.\n\n2.  **Model 1 (Oja normalization rule):** The model is $\\Delta w_i = \\eta \\, x_i y_i - \\eta \\lambda \\, y_i^2 w_i + \\varepsilon_i$. We can rewrite this as $\\Delta w_i = p_1(x_i y_i) + p_2(y_i^2 w_i) + \\varepsilon_i$, where the fitted parameters are $p_1 = \\eta$ and $p_2 = -\\eta\\lambda$. This is a two-parameter linear model. The design matrix $\\mathbf{A}_1$ is an $n \\times 2$ matrix with the first column being $(x_i y_i)$ and the second column being $(y_i^2 w_i)$. The number of fitted parameters is $k_1=2$.\n\n3.  **Model 2 (Bienenstock–Cooper–Munroe rule):** The model is $\\Delta w_i = \\eta \\, x_i y_i (y_i - \\theta_i) + \\varepsilon_i$. The sliding threshold $\\theta_i$ is not a free parameter to be fit but is computed from the postsynaptic activity history. For any given dataset, we first compute the sequence of thresholds $\\{\\theta_i\\}$. The initial threshold $\\theta_0$ is set to the empirical mean of the squared postsynaptic rates, $\\theta_0 = \\frac{1}{n} \\sum_{j=1}^n y_j^2$. The subsequent thresholds are computed recursively as $\\theta_i = (1 - \\rho)\\theta_{i-1} + \\rho y_i^2$ for $i=1, \\dots, n$, using the given hyperparameter $\\rho=0.05$. Once the vector of thresholds is computed, the model becomes a linear regression problem $\\Delta w_i = \\eta_2 (x_i y_i (y_i - \\theta_i)) + \\varepsilon_i$. Here, we fit one parameter, $p_1 = \\eta_2$. The design matrix $\\mathbf{A}_2$ is an $n \\times 1$ matrix with entries $(x_i y_i (y_i - \\theta_i))$. The number of fitted parameters is $k_2=1$.\n\nFor each model, we solve the linear least-squares problem to find the parameter vector $\\mathbf{p}$ that minimizes the Residual Sum of Squares, $\\mathrm{RSS} = \\sum_{i=1}^n (\\Delta w_i - \\widehat{\\Delta w}_i)^2$, where $\\widehat{\\Delta w}_i$ is the model's prediction for trial $i$.\n\nFinally, we perform model selection using the Akaike Information Criterion (AIC). The AIC provides a means to compare models by balancing their goodness of fit (measured by RSS) against their complexity (measured by the number of fitted parameters $k$). A model with more parameters can achieve a better fit (lower RSS) simply due to its increased flexibility, which can lead to overfitting. The AIC penalizes this complexity. The formula is:\n$$\n\\mathrm{AIC} = 2k + n \\ln\\left(\\frac{\\mathrm{RSS}}{n}\\right)\n$$\nFor each dataset, we compute the AIC value for each of the three fitted models. The model that yields the lowest AIC value is selected as the most plausible explanation for the observed data, balancing explanatory power with parsimony. The final output is an integer code representing the selected model ($0$, $1$, or $2$). This entire procedure is repeated for each of the four specified datasets.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the model selection process for all test cases.\n    \"\"\"\n    test_cases = [\n        {\n            \"name\": \"Dataset 1 (Simple Hebbian)\",\n            \"seed\": 12345, \"n\": 120,\n            \"log_normal_mu\": [np.log(5), np.log(7)],\n            \"sigma_x\": 0.4, \"sigma_y\": 0.4, \"rho_c\": 0.6,\n            \"w_min\": 0.05, \"w_max\": 0.35,\n            \"true_model\": \"hebbian\",\n            \"model_params\": {\"eta\": 0.008},\n            \"sigma_eps\": 0.002\n        },\n        {\n            \"name\": \"Dataset 2 (Oja)\",\n            \"seed\": 54321, \"n\": 150,\n            \"log_normal_mu\": [np.log(6), np.log(6)],\n            \"sigma_x\": 0.5, \"sigma_y\": 0.5, \"rho_c\": 0.5,\n            \"w_min\": 0.05, \"w_max\": 0.90,\n            \"true_model\": \"oja\",\n            \"model_params\": {\"eta\": 0.020, \"lambda\": 1.2},\n            \"sigma_eps\": 0.003\n        },\n        {\n            \"name\": \"Dataset 3 (BCM)\",\n            \"seed\": 24680, \"n\": 140,\n            \"log_normal_mu\": [np.log(5), np.log(10)],\n            \"sigma_x\": 0.6, \"sigma_y\": 0.6, \"rho_c\": 0.2,\n            \"w_min\": 0.02, \"w_max\": 0.50,\n            \"true_model\": \"bcm\",\n            \"model_params\": {\"eta\": 0.025, \"rho\": 0.05},\n            \"sigma_eps\": 0.003\n        },\n        {\n            \"name\": \"Dataset 4 (Weak Oja)\",\n            \"seed\": 13579, \"n\": 130,\n            \"log_normal_mu\": [np.log(5.5), np.log(6.5)],\n            \"sigma_x\": 0.4, \"sigma_y\": 0.4, \"rho_c\": 0.4,\n            \"w_min\": 0.05, \"w_max\": 0.80,\n            \"true_model\": \"oja\",\n            \"model_params\": {\"eta\": 0.020, \"lambda\": 0.10},\n            \"sigma_eps\": 0.003\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        selected_model = process_dataset(case)\n        results.append(selected_model)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef generate_data(params):\n    \"\"\"\n    Generates synthetic synaptic plasticity data based on provided parameters.\n    \"\"\"\n    rng = np.random.default_rng(params[\"seed\"])\n    n = params[\"n\"]\n\n    # Generate correlated log-normal firing rates x and y\n    mu = params[\"log_normal_mu\"]\n    cov = [[params[\"sigma_x\"]**2, params[\"rho_c\"] * params[\"sigma_x\"] * params[\"sigma_y\"]],\n           [params[\"rho_c\"] * params[\"sigma_x\"] * params[\"sigma_y\"], params[\"sigma_y\"]**2]]\n    z = rng.multivariate_normal(mu, cov, size=n)\n    x, y = np.exp(z[:, 0]), np.exp(z[:, 1])\n\n    # Generate initial weights w\n    w = rng.uniform(params[\"w_min\"], params[\"w_max\"], size=n)\n\n    # Generate noise\n    noise = rng.normal(0, params[\"sigma_eps\"], size=n)\n\n    # Generate delta_w based on the true model\n    model_type = params[\"true_model\"]\n    mp = params[\"model_params\"]\n    delta_w_true = np.zeros(n)\n\n    if model_type == \"hebbian\":\n        delta_w_true = mp[\"eta\"] * x * y\n    elif model_type == \"oja\":\n        delta_w_true = mp[\"eta\"] * x * y - mp[\"eta\"] * mp[\"lambda\"] * y**2 * w\n    elif model_type == \"bcm\":\n        theta0 = np.mean(y**2)\n        theta = np.zeros(n)\n        theta_prev = theta0\n        for i in range(n):\n            theta[i] = (1 - mp[\"rho\"]) * theta_prev + mp[\"rho\"] * y[i]**2\n            theta_prev = theta[i]\n        delta_w_true = mp[\"eta\"] * x * y * (y - theta)\n    \n    delta_w_obs = delta_w_true + noise\n    return x, y, w, delta_w_obs\n\ndef fit_and_select(x, y, w, delta_w):\n    \"\"\"\n    Fits all three models to the data and selects the best one using AIC.\n    \"\"\"\n    n = len(x)\n    aics = []\n    \n    # --- Model 0: Simple Hebbian ---\n    k0 = 1\n    A0 = (x * y).reshape(-1, 1)\n    p0, rss0_arr, _, _ = np.linalg.lstsq(A0, delta_w, rcond=None)\n    rss0 = rss0_arr[0]\n    aic0 = 2 * k0 + n * np.log(rss0 / n)\n    aics.append(aic0)\n\n    # --- Model 1: Oja Rule ---\n    k1 = 2\n    A1 = np.vstack((x * y, -y**2 * w)).T # Note: fitting p2 = eta * lambda, not -eta*lambda\n    p1, rss1_arr, _, _ = np.linalg.lstsq(A1, delta_w, rcond=None)\n    rss1 = rss1_arr[0]\n    aic1 = 2 * k1 + n * np.log(rss1 / n)\n    aics.append(aic1)\n\n    # --- Model 2: BCM Rule ---\n    k2 = 1\n    rho_bcm = 0.05\n    theta0 = np.mean(y**2)\n    theta = np.zeros(n)\n    theta_prev = theta0\n    for i in range(n):\n        theta[i] = (1 - rho_bcm) * theta_prev + rho_bcm * y[i]**2\n        theta_prev = theta[i]\n    \n    A2 = (x * y * (y - theta)).reshape(-1, 1)\n    p2, rss2_arr, _, _ = np.linalg.lstsq(A2, delta_w, rcond=None)\n    rss2 = rss2_arr[0]\n    aic2 = 2 * k2 + n * np.log(rss2 / n)\n    aics.append(aic2)\n\n    return np.argmin(aics)\n\ndef process_dataset(params):\n    \"\"\"\n    Orchestrates data generation and model selection for a single dataset.\n    \"\"\"\n    x, y, w, delta_w = generate_data(params)\n    # The solution text described fitting p2 = -eta*lambda with a design matrix column of (y^2*w).\n    # This code implements fitting p2=eta*lambda with a column of (-y^2*w), which is equivalent.\n    # The key part is that there are two predictors for Oja.\n    best_model_idx = fit_and_select(x, y, w, delta_w)\n    return best_model_idx\n\n# Execute the main function when the script is run.\nif __name__ == \"__main__\":\n    solve()\n\n```"
        }
    ]
}