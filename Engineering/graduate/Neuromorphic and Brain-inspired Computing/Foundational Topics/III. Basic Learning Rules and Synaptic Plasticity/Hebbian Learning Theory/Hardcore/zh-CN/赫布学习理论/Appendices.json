{
    "hands_on_practices": [
        {
            "introduction": "赫布学习理论的核心在于其“一起发放的神经元连接在一起”的原则，但纯粹的赫布可塑性本质上是不稳定的。为了构建功能性的学习系统，必须引入稳定化机制。这项练习  引导我们通过平均场分析，为一个包含线性衰减和饱和项的突触权重推导其动力学方程，从而探索其长期行为。通过求解该系统的平衡点并分析其稳定性，你将掌握将抽象的学习规则转化为可预测的动力学模型，并理解突触权重如何达到稳定状态的核心技能。",
            "id": "4046815",
            "problem": "考虑一个神经形态系统中的单个线性突触后神经元，该神经元具有一个突触前输入流。突触前活动被建模为一个具有有限二阶矩的零均值、平稳、遍历的随机过程。将突触前信号表示为 $x(t)$，突触效能表示为 $w(t)$。假设存在时间尺度分离，使得突触动力学的演化时间尺度慢于 $x(t)$ 的涨落。在学习所探索的 $w(t)$ 范围内，突触后活动可以由线性响应模型很好地近似，其中 $y(t)$ 满足 $y(t) \\approx w(t)\\,x(t)$。\n\n采用核心的赫布学习原则，即瞬时突触修饰与突触前和突触后活动的乘积成正比，并包含受生物物理学启发的稳态正则化项：由权重泄漏引起的线性衰减项和代表资源限制及突触巩固中非线性的三次饱和项。设赫布驱动的比例常数为 $\\eta0$，泄漏率为 $\\lambda0$，饱和系数为 $\\mu0$。通过 $\\sigma_x^{2} = \\lim_{T \\to \\infty} \\frac{1}{T}\\int_{0}^{T} x^{2}(t)\\,dt$ 定义输入方差，根据平稳性和遍历性，该方差存在。\n\n从这些原则和假设出发，进行平均场分析，以 $\\eta$、$\\lambda$、$\\mu$ 和 $\\sigma_x^{2}$ 为参数，推导出一个决定 $w(t)$ 演化的确定性常微分方程(ODE)。确定该常微分方程的平衡态（不动点）并对其局部稳定性进行分类。然后，在存在非零稳定平衡的参数体系下，以 $\\eta$、$\\lambda$、$\\mu$ 和 $\\sigma_x^{2}$ 的闭式表达式计算 $w(t)$ 的非零稳定不动点的大小。将你的最终答案表示为单个简化的解析表达式。不需要数值近似。在首次使用时定义所有缩略语；例如，在首次提及时使用“常微分方程(ODE)”。最终答案必须是单个无单位的闭式解析表达式。",
            "solution": "该问题要求在一种特定形式的赫布学习下，推导并分析突触权重动力学的平均场模型。分析将分三个阶段进行：首先，构建突触效能 $w(t)$ 的完整随机微分方程；其次，应用平均场近似推导出一个确定性常微分方程(ODE)；第三，求出该常微分方程的不动点，分析其稳定性，并确定稳定非零平衡点的大小。\n\n首先，我们构建突触效能变化率 $\\frac{dw}{dt}$ 的方程。问题陈述了三个贡献因素：\n1. 一个赫布学习项，其中突触修饰与突触前和突触后活动 $x(t)y(t)$ 的乘积成正比。比例常数为 $\\eta$。突触后活动由线性响应模型 $y(t) \\approx w(t)x(t)$ 给出。因此，赫布项为 $\\eta x(t) y(t) = \\eta x(t) (w(t)x(t)) = \\eta w(t) x^{2}(t)$。由于 $\\eta  0$，该项驱动增强。\n\n2. 一个代表权重泄漏的线性衰减项，速率常数为 $\\lambda  0$。该项与权重本身成正比，作用是减小其大小。因此，表示为 $-\\lambda w(t)$。\n\n3. 一个代表资源限制的三次饱和项，系数为 $\\mu  0$。该项防止权重的无界增长，表示为 $-\\mu w^{3}(t)$。在突触可塑性中，三次形式是建模非线性饱和效应的常见选择。\n\n结合这些项，我们得到控制 $w(t)$ 演化的随机微分方程：\n$$\n\\frac{dw(t)}{dt} = \\eta w(t) x^{2}(t) - \\lambda w(t) - \\mu w^{3}(t)\n$$\n由于存在波动的突触前信号 $x(t)$，该方程是随机的。\n\n接下来，我们进行平均场分析。问题陈述了时间尺度分离，即突触权重 $w(t)$ 的演化速度远慢于突触前信号 $x(t)$。这一假设允许我们对随机方程在 $x(t)$ 的快速涨落上进行平均，从而得到一个描述 $w(t)$ 缓慢演化的确定性常微分方程(ODE)。在此平均过程中，慢变量 $w(t)$ 被视为常数。我们用 $\\langle \\cdot \\rangle$ 表示平均操作。将其应用于我们的方程：\n$$\n\\frac{dw}{dt} = \\left\\langle \\eta w x^{2} - \\lambda w - \\mu w^{3} \\right\\rangle\n$$\n利用平均算子的线性性质，并将 $w$ 视为准静态：\n$$\n\\frac{dw}{dt} = \\eta w \\langle x^{2}(t) \\rangle - \\lambda w \\langle 1 \\rangle - \\mu w^{3} \\langle 1 \\rangle\n$$\n$$\n\\frac{dw}{dt} = \\eta w \\langle x^{2}(t) \\rangle - \\lambda w - \\mu w^{3}\n$$\n突触前信号 $x(t)$ 是一个零均值、平稳、遍历的随机过程。其方差定义为 $\\sigma_x^{2} = \\lim_{T \\to \\infty} \\frac{1}{T}\\int_{0}^{T} x^{2}(t)\\,dt$。由于遍历性，时间平均 $\\langle x^{2}(t) \\rangle$ 等于系综平均（或期望）$\\mathbb{E}[x^{2}(t)]$。由于均值为零，方差等于二阶矩：$\\sigma_x^{2} = \\mathbb{E}[x^{2}(t)] - (\\mathbb{E}[x(t)])^{2} = \\mathbb{E}[x^{2}(t)] - 0^{2} = \\mathbb{E}[x^{2}(t)]$。因此，$\\langle x^{2}(t) \\rangle = \\sigma_x^{2}$。\n\n将此代入方程，我们得到描述 $w(t)$ 演化的确定性平均场常微分方程：\n$$\n\\frac{dw}{dt} = \\eta w \\sigma_x^{2} - \\lambda w - \\mu w^{3}\n$$\n$$\n\\frac{dw}{dt} = (\\eta \\sigma_x^{2} - \\lambda) w - \\mu w^{3}\n$$\n我们将右侧定义为 $f(w) = (\\eta \\sigma_x^{2} - \\lambda) w - \\mu w^{3}$。\n\n通过设置 $\\frac{dw}{dt} = 0$，可以找到该常微分方程的平衡态，或称不动点 $w^*$：\n$$\nf(w^*) = (\\eta \\sigma_x^{2} - \\lambda) w^* - \\mu (w^*)^{3} = 0\n$$\n将 $w^*$ 因子提出：\n$$\nw^* \\left[ (\\eta \\sigma_x^{2} - \\lambda) - \\mu (w^*)^{2} \\right] = 0\n$$\n该方程为 $w^*$ 提供了以下解：\n1. $w_{0}^* = 0$\n2. $(\\eta \\sigma_x^{2} - \\lambda) - \\mu (w^*)^{2} = 0 \\implies (w^*)^{2} = \\frac{\\eta \\sigma_x^{2} - \\lambda}{\\mu}$\n\n第二种情况仅在右侧为正时给出实的、非零的不动点，即当 $\\eta \\sigma_x^{2} - \\lambda  0$ 时。如果此条件成立，则存在两个对称的非零不动点：\n$$\nw_{\\pm}^* = \\pm \\sqrt{\\frac{\\eta \\sigma_x^{2} - \\lambda}{\\mu}}\n$$\n为了对这些不动点的局部稳定性进行分类，我们分析每个不动点处导数 $f'(w) = \\frac{df}{dw}$ 的符号。如果 $f'(w^*)  0$，不动点 $w^*$ 是局部稳定的；如果 $f'(w^*)  0$，则是不稳定的。\n导数为：\n$$\nf'(w) = \\frac{d}{dw} \\left[ (\\eta \\sigma_x^{2} - \\lambda) w - \\mu w^{3} \\right] = (\\eta \\sigma_x^{2} - \\lambda) - 3\\mu w^{2}\n$$\n现在，我们在不动点处计算 $f'(w)$ 的值。\n- 对于 $w_{0}^* = 0$：\n  $f'(0) = (\\eta \\sigma_x^{2} - \\lambda) - 3\\mu (0)^{2} = \\eta \\sigma_x^{2} - \\lambda$。\n  原点的稳定性取决于该项的符号。如果 $\\eta \\sigma_x^{2}  \\lambda$，$f'(0)  0$，原点是稳定的。如果 $\\eta \\sigma_x^{2}  \\lambda$，$f'(0)  0$，原点是不稳定的。\n\n- 对于 $w_{\\pm}^* = \\pm \\sqrt{\\frac{\\eta \\sigma_x^{2} - \\lambda}{\\mu}}$：\n  这些不动点仅在 $\\eta \\sigma_x^{2}  \\lambda$ 的体系中存在。让我们在此体系中分析它们的稳定性。注意，$f'(w)$ 依赖于 $w^{2}$，因此 $w_{+}^*$ 和 $w_{-}^*$ 的稳定性是相同的。\n  将 $(w_{\\pm}^*)^{2} = \\frac{\\eta \\sigma_x^{2} - \\lambda}{\\mu}$ 代入 $f'(w)$ 的表达式中：\n  $$\n  f'(w_{\\pm}^*) = (\\eta \\sigma_x^{2} - \\lambda) - 3\\mu \\left( \\frac{\\eta \\sigma_x^{2} - \\lambda}{\\mu} \\right)\n  $$\n  $$\n  f'(w_{\\pm}^*) = (\\eta \\sigma_x^{2} - \\lambda) - 3(\\eta \\sigma_x^{2} - \\lambda) = -2(\\eta \\sigma_x^{2} - \\lambda)\n  $$\n  在这些非零不动点存在的体系中，我们有 $\\eta \\sigma_x^{2} - \\lambda  0$。因此，$f'(w_{\\pm}^*)  0$。这表明两个非零不动点 $w_{+}^*$ 和 $w_{-}^*$ 在其存在时都是局部稳定的。\n\n问题要求在存在非零稳定不动点的参数体系下，计算其大小。该体系为 $\\eta \\sigma_x^{2}  \\lambda$。非零稳定不动点为 $w_{\\pm}^* = \\pm \\sqrt{\\frac{\\eta \\sigma_x^{2} - \\lambda}{\\mu}}$。\n其大小是绝对值：\n$$\n|w_{\\pm}^*| = \\left| \\pm \\sqrt{\\frac{\\eta \\sigma_x^{2} - \\lambda}{\\mu}} \\right| = \\sqrt{\\frac{\\eta \\sigma_x^{2} - \\lambda}{\\mu}}\n$$\n这就是最终的闭式表达式。",
            "answer": "$$\n\\boxed{\\sqrt{\\frac{\\eta \\sigma_x^{2} - \\lambda}{\\mu}}}\n$$"
        },
        {
            "introduction": "当我们将赫布学习从单个突触扩展到处理多个输入的神经元时，一个关键问题出现了：神经元如何学习从高维输入数据中提取有意义的特征？这项练习  介绍了 Oja 提出的一个著名学习规则，该规则通过一个归一化项来稳定多维权重向量的赫布式增长。通过分析该系统的动力学，你将揭示一个惊人的结果：一个简单的局部学习规则如何使神经元能够自动提取输入数据中最重要的统计特征，即主成分，这展示了赫布式学习在计算神经科学中的强大功能。",
            "id": "4046811",
            "problem": "考虑一个神经拟态系统中的单个线性神经元，其突触权重向量 $w(t) \\in \\mathbb{R}^{2}$ 根据流式输入进行调整。该调整遵循一种受赫布学习原则（“共同激活的细胞连接在一起”）启发的局部活动依赖规则，并由一个局部归一化器进行稳定。该神经元接收的输入 $x(t) \\in \\mathbb{R}^{2}$ 是一个独立同分布 (i.i.d.) 的零均值平稳随机向量，其协方差矩阵为 $C = \\begin{pmatrix} a  b \\\\ b  d \\end{pmatrix}$，其中 $a  0$, $d  0$ 且 $ad - b^{2}  0$。瞬时输出为 $y(t) = w(t)^{\\top} x(t)$，在时间 $t$ 的在线自适应由 $\\Delta w(t) = \\eta \\, y(t) \\big(x(t) - y(t) w(t)\\big)$ 给出，其中 $\\eta  0$ 是一个很小的学习率。\n\n仅从赫布学习原则和所提供的自适应规则出发，在独立同分布和平稳性假设下，于平均场连续时间极限中处理该学习过程。推导 $w(t)$ 关于 $x(t)$ 的矩的期望动力学系统，分析其不动点及其稳定性，并确定收敛时的极限期望输出功率 $\\mathbb{E}[y(t)^{2}]$。请用 $a$、$b$ 和 $d$ 将您的最终答案表示为单个闭式解析表达式。无需单位。",
            "solution": "该问题要求计算单个线性神经元的极限期望输出功率，该神经元的突触权重向量 $w(t)$ 根据一种特定的赫布型规则进行自适应。解决方案需要分析在平均场连续时间极限下权重向量的动力学。\n\n首先，给定权重向量 $w(t) \\in \\mathbb{R}^{2}$ 的离散时间更新规则：\n$$ \\Delta w(t) = w(t+1) - w(t) = \\eta \\, y(t) \\big(x(t) - y(t) w(t)\\big) $$\n其中 $y(t) = w(t)^{\\top} x(t)$ 是神经元的输出，$x(t) \\in \\mathbb{R}^{2}$ 是输入向量，$\\eta  0$ 是一个很小的学习率。\n\n为了推导连续时间极限下的期望动力学系统，我们将 $w$ 视为一个慢变变量（因为 $\\eta$ 很小），并在快变输入 $x(t)$ 的平稳分布上对更新规则取平均。变化量 $\\Delta w(t)$ 可由一个微分 $\\frac{dw}{dt}$ 近似，并按 $\\eta$ 缩放。\n$$ \\frac{dw}{dt} = \\mathbb{E}_{x} \\left[ y(x) \\big(x - y(x) w \\big) \\right] $$\n其中期望 $\\mathbb{E}_{x}[\\cdot]$ 是对输入向量 $x$ 的分布求得的。在期望内部，权重向量 $w$ 被视为常数。代入 $y(x) = w^{\\top}x$：\n$$ \\frac{dw}{dt} = \\mathbb{E}_{x} \\left[ (w^{\\top}x) \\left(x - (w^{\\top}x)w \\right) \\right] $$\n利用期望算子的线性性，我们可以将各项分开：\n$$ \\frac{dw}{dt} = \\mathbb{E}_{x} \\left[ (w^{\\top}x)x \\right] - \\mathbb{E}_{x} \\left[ (w^{\\top}x)^2 w \\right] $$\n我们来分析每一项。对于第一项：\n$$ \\mathbb{E}_{x} \\left[ (w^{\\top}x)x \\right] = \\mathbb{E}_{x} \\left[ x(x^{\\top}w) \\right] = \\mathbb{E}_{x} \\left[ xx^{\\top} \\right] w $$\n题目说明输入 $x$ 是一个零均值随机向量，其协方差矩阵为 $C = \\begin{pmatrix} a  b \\\\ b  d \\end{pmatrix}$。根据定义，$C = \\mathbb{E}[ (x-\\mathbb{E}[x])(x-\\mathbb{E}[x])^{\\top} ]$。由于 $\\mathbb{E}[x] = 0$，协方差矩阵为 $C = \\mathbb{E}[xx^{\\top}]$。因此，第一项是 $Cw$。\n\n对于第二项：\n$$ \\mathbb{E}_{x} \\left[ (w^{\\top}x)^2 w \\right] = w \\, \\mathbb{E}_{x} \\left[ (w^{\\top}x)^2 \\right] = w \\, \\mathbb{E}_{x} \\left[ (w^{\\top}x)(x^{\\top}w) \\right] $$\n$$ = w \\, w^{\\top} \\mathbb{E}_{x} \\left[ xx^{\\top} \\right] w = w (w^{\\top}Cw) $$\n合并各项，我们得到 $w(t)$ 的平均场连续时间动力学系统：\n$$ \\frac{dw}{dt} = \\eta \\left( Cw - (w^{\\top}Cw)w \\right) $$\n这是单个神经元的 Oja 法则的一种形式。\n\n接下来，我们通过设置 $\\frac{dw}{dt} = 0$ 来分析该系统的不动点 $w^*$：\n$$ Cw^* - (w^{*\\top}Cw^*)w^* = 0 $$\n该方程意味着 $Cw^*$ 必须与 $w^*$ 共线。这意味着任何非零不动点 $w^*$ 必须是矩阵 $C$ 的一个特征向量。设 $w^*$ 是 $C$ 的一个特征向量，其对应的特征值为 $\\lambda$，因此有 $Cw^* = \\lambda w^*$。将此代入不动点方程：\n$$ \\lambda w^* - (w^{*\\top}(\\lambda w^*)) w^* = 0 $$\n$$ \\lambda w^* - \\lambda (w^{*\\top}w^*) w^* = 0 $$\n$$ \\lambda w^* (1 - \\|w^*\\|^2) = 0 $$\n该方程有两种解：\n1. $w^* = 0$，即平凡不动点。\n2. 如果 $\\lambda \\neq 0$，则 $\\|w^*\\|^2 = 1$。非平凡不动点是协方差矩阵 $C$ 的单位范数特征向量。\n\n这些不动点的稳定性决定了 $w(t)$ 的长期行为。\n对于平凡不动点 $w^*=0$，我们在 $w=0$ 附近对系统进行线性化。项 $(w^{\\top}Cw)w$ 是 $\\|w\\|^3$ 阶的，对于小的 $\\|w\\|$ 可以忽略。线性化系统为 $\\frac{dw}{dt} \\approx \\eta Cw$。给定条件 $a0, d0$ 和 $ad-b^20$ 确保 $C$ 是一个正定矩阵。其所有特征值都严格为正。因此，任何来自 $w=0$ 的小扰动都会增长，使得不动点 $w^*=0$ 不稳定。\n\n对于非平凡不动点 $w^*=e_i$（其中 $e_i$ 是 $C$ 的归一化特征向量），权重向量会收敛到与最大特征值 $\\lambda_1$ 相关联的特征向量的方向。这是 Oja 法则的一个标准结果。动力学将向量 $w$ 投影到 $C$ 的主子空间上，同时将其长度归一化为 1。稳定不动点是 $w_{ss} = \\pm e_1$，其中 $e_1$ 是对应于 $C$ 的最大特征值 $\\lambda_1$ 的归一化特征向量。所有其他特征向量不动点都是不稳定的（鞍点）。\n\n问题要求的是收敛时的极限期望输出功率 $\\mathbb{E}[y(t)^2]$。在收敛时（$t \\to \\infty$），权重向量 $w(t)$ 趋近于一个稳定不动点 $w_{ss}$。期望输出功率是通过在输入分布上取平均来计算的，其中权重向量固定在其稳态值 $w_{ss}$：\n$$ \\lim_{t\\to\\infty} \\mathbb{E}_{x}[y(t)^2] = \\mathbb{E}_{x}[(w_{ss}^{\\top}x)^2] = \\mathbb{E}_{x}[w_{ss}^{\\top}xx^{\\top}w_{ss}] $$\n将常数向量 $w_{ss}$ 从期望中提出：\n$$ = w_{ss}^{\\top} \\mathbb{E}_{x}[xx^{\\top}] w_{ss} = w_{ss}^{\\top} C w_{ss} $$\n由于 $w_{ss}$ 是对应于最大特征值 $\\lambda_1$ 的归一化特征向量 $e_1$（或 $-e_1$），我们有 $Ce_1 = \\lambda_1 e_1$ 和 $\\|e_1\\|^2 = 1$。\n$$ w_{ss}^{\\top} C w_{ss} = (\\pm e_1)^{\\top} C (\\pm e_1) = e_1^{\\top} C e_1 = e_1^{\\top}(\\lambda_1 e_1) = \\lambda_1(e_1^{\\top}e_1) = \\lambda_1 $$\n因此，极限期望输出功率是输入协方差矩阵 $C$ 的最大特征值。\n\n最后，我们计算 $C = \\begin{pmatrix} a  b \\\\ b  d \\end{pmatrix}$ 的特征值。特征值 $\\lambda$ 是特征方程 $\\det(C - \\lambda I) = 0$ 的根：\n$$ \\det \\begin{pmatrix} a-\\lambda  b \\\\ b  d-\\lambda \\end{pmatrix} = (a-\\lambda)(d-\\lambda) - b^2 = 0 $$\n$$ \\lambda^2 - (a+d)\\lambda + ad - b^2 = 0 $$\n使用二次公式求解 $\\lambda$：\n$$ \\lambda = \\frac{-(-(a+d)) \\pm \\sqrt{(-(a+d))^2 - 4(1)(ad-b^2)}}{2(1)} $$\n$$ \\lambda = \\frac{a+d \\pm \\sqrt{a^2 + 2ad + d^2 - 4ad + 4b^2}}{2} $$\n$$ \\lambda = \\frac{a+d \\pm \\sqrt{a^2 - 2ad + d^2 + 4b^2}}{2} $$\n$$ \\lambda = \\frac{a+d \\pm \\sqrt{(a-d)^2 + 4b^2}}{2} $$\n两个特征值是 $\\lambda_1 = \\frac{a+d + \\sqrt{(a-d)^2 + 4b^2}}{2}$ 和 $\\lambda_2 = \\frac{a+d - \\sqrt{(a-d)^2 + 4b^2}}{2}$。\n项 $\\sqrt{(a-d)^2 + 4b^2}$ 总是非负的，所以 $\\lambda_1$ 是两个特征值中较大的一个。\n极限期望输出功率是 $\\lambda_1$。",
            "answer": "$$ \\boxed{\\frac{a+d + \\sqrt{(a-d)^2 + 4b^2}}{2}} $$"
        },
        {
            "introduction": "理论神经科学家提出了多种学习规则来解释突触可塑性现象，例如经典的赫布模型、Oja 规则和 BCM 规则。但在面对真实的实验数据时，我们如何判断哪一个模型是最好的解释呢？这项动手编程实践  模拟了计算神经科学家的工作流程：你将生成由不同理论模型产生的“模拟实验”数据，然后通过拟合模型参数和应用统计模型选择标准来反向推断出最能解释该数据的原始模型。这项练习将理论知识与数据分析实践相结合，让你掌握通过定量方法验证和比较科学模型的关键技能。",
            "id": "4046831",
            "problem": "一个程序必须将实验观测到的突触可塑性现象映射到候选的赫布学习模型，通过定量拟合参数并使用信息准则进行模型选择。其教育背景是神经形态和类脑计算，重点是赫布学习理论。该任务必须从第一性原理出发解决，从基于相关的学习、归一化和滑动阈值稳态的核心定义和广泛接受的事实开始。\n\n一个突触将活动速率为 $x$ (单位Hz) 的突触前神经元连接到活动速率为 $y$ (单位Hz) 的突触后神经元。突触权重 $w$ 调节突触后影响，并在持续时间为 $T$ 秒的试验后，变化量为 $\\Delta w$。观测到的现象以在受控条件下生成的合成、可复现的数据集形式提供。程序必须对每个数据集进行分类，判断以下三个候选赫布模型中哪一个能最好地解释该数据集：\n\n- 模型 $0$ (简单赫布速率规则)：权重变化与突触前和突触后速率的相关性成正比。\n- 模型 $1$ (Oja 归一化规则)：权重变化包括一个相关项和一个取决于突触后速率平方及当前权重的归一化项。\n- 模型 $2$ (Bienenstock–Cooper–Munroe (BCM) 规则)：权重变化由相关性驱动，并带有一个滑动修改阈值，该阈值取决于突触后速率平方的缓慢平均值。\n\n候选模型实例如下：\n\n1. 简单赫布速率规则：\n$$\n\\Delta w_i = \\eta \\, x_i \\, y_i + \\varepsilon_i,\n$$\n其中 $i$ 是试验的索引，$x_i$ 和 $y_i$ 以Hz为单位测量，$\\eta$ 是一个待拟合的标量学习率参数，$\\varepsilon_i$ 是零均值高斯观测噪声。\n\n2. Oja 归一化规则：\n$$\n\\Delta w_i = \\eta \\, x_i \\, y_i - \\eta \\lambda \\, y_i^2 \\, w_i + \\varepsilon_i,\n$$\n其中 $\\eta$ 和 $\\lambda$ 是待拟合的标量参数。\n\n3. Bienenstock–Cooper–Munroe 规则：\n$$\n\\Delta w_i = \\eta \\, x_i \\, y_i \\, (y_i - \\theta_i) + \\varepsilon_i,\n$$\n其中 $\\eta$ 是一个待拟合的标量参数，$\\theta_i$ 是一个滑动修改阈值，计算为突触后速率平方的指数移动平均值，\n$$\n\\theta_i = (1 - \\rho)\\,\\theta_{i-1} + \\rho \\, y_i^2, \\quad \\rho \\in (0,1), \\quad \\theta_0 \\text{ 已指定}。\n$$\n\n在推导和算法设计中使用的基本基础：\n- 赫布原则：突触变化随突触前和突触后活动之间的正相关性而增加。\n- 归一化/稳态：生物系统通常通过防止权重发散的归一化项来调节突触生长。\n- 滑动阈值稳态：长期的协方差敏感可塑性表现出一个适应活动统计特性的阈值。\n- 在独立试验的假设下，高斯观测噪声和最大似然法导出最小二乘参数估计和用于模型选择的赤池信息准则 (AIC)。\n\n程序必须为每个数据集执行以下步骤：\n1. 使用下面提供的精确种子和参数生成试验数据 $(x_i, y_i, w_i, \\Delta w_i)$。生成过程通过使用正值、相关的对数正态速率分布（单位Hz）、跨观测突触的独立权重样本（无量纲单位）以及 $\\Delta w$ 中加性的零均值高斯观测噪声（与 $\\Delta w$ 单位相同）来确保科学真实性。\n2. 在高斯噪声假设下，通过最小化残差平方和来拟合每个候选模型的参数。对于简单赫布规则和 BCM 规则，这是一个单参数线性最小二乘法。对于 Oja 规则，这是一个双参数线性最小二乘法，其中基函数是 $x_i y_i$ 和 $y_i^2 w_i$。\n3. 计算每个拟合模型的赤池信息准则 (AIC)，\n$$\n\\mathrm{AIC} = 2k + n \\ln\\!\\left(\\frac{\\mathrm{RSS}}{n}\\right),\n$$\n其中 $k$ 是拟合参数的数量，$n$ 是试验次数，残差平方和为 $\\mathrm{RSS} = \\sum_i (\\Delta w_i - \\widehat{\\Delta w}_i)^2$。\n4. 选择 AIC 最小的模型。将选择编码为整数：简单赫布为 $0$，Oja 为 $1$，BCM 为 $2$。\n\n必须遵守物理和数值单位：\n- 活动速率 $x_i$ 和 $y_i$ 的单位是 Hz。\n- 试验持续时间 $T$ 的单位是秒。\n- 突触权重 $w_i$ 和权重变化 $\\Delta w_i$ 是无量纲的。\n- 不涉及角度或百分比。\n\n用于可复现性的数据生成细节：\n- 对每个数据集，采样 $n$ 次试验。对于试验 $i$，从一个相关系数为 $\\rho_c \\in (-1,1)$、对数标准差为 $\\sigma_x  0$ 和 $\\sigma_y  0$ 的二元正态分布 $\\mathcal{N}(\\mu, \\Sigma)$ 中抽取向量 $z_i$。设置 $x_i = \\exp(z_{i,1})$ 和 $y_i = \\exp(z_{i,2})$，得到单位为Hz的对数正态速率。从 $[w_{\\min}, w_{\\max}]$ 上的均匀分布中独立采样 $w_i$。通过指定的模型方程计算 $\\Delta w_i$，并添加独立的高斯噪声 $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma_{\\varepsilon}^2)$。\n\n- 所有数据集的试验持续时间为 $T = 1$ 秒。\n\n- 对于 BCM 规则，使用指定的 $\\rho$ 顺序计算 $\\theta_i$，其中 $\\theta_0$ 等于数据集中 $y_i^2$ 的经验均值。\n\n测试套件：\n- 数据集 $1$ (有种子的赫布现象)：\n  - 种子: $s = 12345$。\n  - 试验次数: $n = 120$。\n  - 对数正态参数: $\\mu = [\\ln(5), \\ln(7)]$, $\\sigma_x = 0.4$, $\\sigma_y = 0.4$, 相关性 $\\rho_c = 0.6$。\n  - 权重范围: $w_{\\min} = 0.05$, $w_{\\max} = 0.35$。\n  - 模型参数: 使用简单赫布规则，$\\eta = 0.008$。\n  - 噪声标准差: $\\sigma_{\\varepsilon} = 0.002$。\n\n- 数据集 $2$ (有种子的 Oja 现象)：\n  - 种子: $s = 54321$。\n  - 试验次数: $n = 150$。\n  - 对数正态参数: $\\mu = [\\ln(6), \\ln(6)]$, $\\sigma_x = 0.5$, $\\sigma_y = 0.5$, 相关性 $\\rho_c = 0.5$。\n  - 权重范围: $w_{\\min} = 0.05$, $w_{\\max} = 0.90$。\n  - 模型参数: 使用 Oja 规则，$\\eta = 0.020$, $\\lambda = 1.2$。\n  - 噪声标准差: $\\sigma_{\\varepsilon} = 0.003$。\n\n- 数据集 $3$ (有种子的 BCM 现象，带滑动阈值)：\n  - 种子: $s = 24680$。\n  - 试验次数: $n = 140$。\n  - 对数正态参数: $\\mu = [\\ln(5), \\ln(10)]$, $\\sigma_x = 0.6$, $\\sigma_y = 0.6$, 相关性 $\\rho_c = 0.2$。\n  - 权重范围: $w_{\\min} = 0.02$, $w_{\\max} = 0.50$。\n  - 模型参数: 使用 BCM 规则，$\\eta = 0.025$，滑动阈值速率 $\\rho = 0.05$，初始阈值 $\\theta_0$ 等于数据集中 $y_i^2$ 的经验均值。\n  - 噪声标准差: $\\sigma_{\\varepsilon} = 0.003$。\n\n- 数据集 $4$ (弱归一化的边界情况)：\n  - 种子: $s = 13579$。\n  - 试验次数: $n = 130$。\n  - 对数正态参数: $\\mu = [\\ln(5.5), \\ln(6.5)]$, $\\sigma_x = 0.4$, $\\sigma_y = 0.4$, 相关性 $\\rho_c = 0.4$。\n  - 权重范围: $w_{\\min} = 0.05$, $w_{\\max} = 0.80$。\n  - 模型参数: 使用 Oja 规则，$\\eta = 0.020$, $\\lambda = 0.10$。\n  - 噪声标准差: $\\sigma_{\\varepsilon} = 0.003$。\n\n答案规格：\n- 对于每个数据集，输出所选模型的整数代码：简单赫布为 $0$，Oja 为 $1$，BCM 为 $2$。\n- 你的程序应生成一行输出，其中包含用方括号括起来的逗号分隔列表形式的结果（例如，“[0,1,2,1]”）。\n\n不允许外部输入；所有数据必须根据提供的规格在内部生成。所有计算必须使用一致的单位（速率用Hz，T用秒；权重和权重变化是无量纲的）。最终输出是整数，没有相关单位。",
            "solution": "该问题要求开发一个程序，根据三个候选的赫布学习模型对突触可塑性的合成数据集进行分类。分类是通过对给定数据集拟合每个模型的参数，并使用赤池信息准则 (AIC) 进行模型选择来实现的。从数据生成到模型选择的整个过程都如下文所述，从第一性原理出发执行。\n\n首先，我们解决合成数据的生成问题，这些数据旨在模拟实验观测到的突触活动。对于每个数据集，我们生成 $n$ 次试验。每次试验 $i$ 的突触前神经元活动速率 $x_i$ 和突触后神经元活动速率 $y_i$ 从相关的对数正态分布中采样。这是通过首先从分布 $\\mathcal{N}(\\boldsymbol{\\mu}, \\mathbf{\\Sigma})$ 中抽取一个二元正态向量 $\\mathbf{z}_i = [z_{i,1}, z_{i,2}]$ 来实现的，其中均值向量是 $\\boldsymbol{\\mu} = [\\mu_x, \\mu_y]$，协方差矩阵是 $\\mathbf{\\Sigma} = \\begin{pmatrix} \\sigma_x^2  \\rho_c \\sigma_x \\sigma_y \\\\ \\rho_c \\sigma_x \\sigma_y  \\sigma_y^2 \\end{pmatrix}$。然后，以Hz为单位的速率由 $x_i = \\exp(z_{i,1})$ 和 $y_i = \\exp(z_{i,2})$ 给出。每次试验的初始突触权重 $w_i$ 从均匀分布 $U(w_{\\min}, w_{\\max})$ 中独立采样。然后，使用为该数据集指定的真实底层模型的方程计算观测到的突触权重变化 $\\Delta w_i$，并向其添加零均值高斯噪声 $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma_{\\varepsilon}^2)$。试验持续时间 $T=1$ 秒被隐式地吸收到模型参数中。此过程为每个数据集生成一组观测值 $(\\{x_i\\}, \\{y_i\\}, \\{w_i\\}, \\{\\Delta w_i\\})$，索引从 $i=1, \\dots, n$。\n\n接下来，我们将三个候选模型的参数拟合到生成的数据中。问题陈述观测噪声 $\\varepsilon_i$ 是高斯分布的。在此假设下，最大似然估计 (MLE) 原理要求我们选择使观测到给定数据的概率最大化的模型参数。对于加性高斯噪声模型，这等同于最小化残差平方和 (RSS)，这是线性最小二乘法的核心思想。三个候选模型中的每一个都可以转化为通用形式为 $\\mathbf{b} = \\mathbf{A}\\mathbf{p}$ 的线性回归问题，其中 $\\mathbf{b}$ 是观测到的权重变化 $\\Delta w_i$ 的 $n \\times 1$ 向量，$\\mathbf{A}$ 是 $n \\times k$ 的设计矩阵，其列是预测变量，$\\mathbf{p}$ 是待估计参数的 $k \\times 1$ 向量。\n\n三个模型为进行最小二乘拟合的公式如下：\n\n1.  **模型 0 (简单赫布速率规则):** 模型为 $\\Delta w_i = \\eta \\, (x_i y_i) + \\varepsilon_i$。这是一个带有一个参数 $p_1 = \\eta$ 的线性模型。设计矩阵 $\\mathbf{A}_0$ 是一个 $n \\times 1$ 矩阵，其元素为 $(x_i y_i)$。拟合参数的数量为 $k_0=1$。\n\n2.  **模型 1 (Oja 归一化规则):** 模型为 $\\Delta w_i = \\eta \\, x_i y_i - \\eta \\lambda \\, y_i^2 w_i + \\varepsilon_i$。我们可以将其重写为 $\\Delta w_i = p_1(x_i y_i) + p_2(y_i^2 w_i) + \\varepsilon_i$，其中拟合的参数是 $p_1 = \\eta$ 和 $p_2 = -\\eta\\lambda$。这是一个双参数线性模型。设计矩阵 $\\mathbf{A}_1$ 是一个 $n \\times 2$ 矩阵，第一列是 $(x_i y_i)$，第二列是 $(y_i^2 w_i)$。拟合参数的数量为 $k_1=2$。\n\n3.  **模型 2 (Bienenstock–Cooper–Munroe 规则):** 模型为 $\\Delta w_i = \\eta \\, x_i y_i (y_i - \\theta_i) + \\varepsilon_i$。滑动阈值 $\\theta_i$ 不是一个要拟合的自由参数，而是从突触后活动历史中计算得出。对于任何给定的数据集，我们首先计算阈值序列 $\\{\\theta_i\\}$。初始阈值 $\\theta_0$ 设置为突触后速率平方的经验均值，即 $\\theta_0 = \\frac{1}{n} \\sum_{j=1}^n y_j^2$。随后的阈值使用给定的超参数 $\\rho=0.05$ 递归计算为 $\\theta_i = (1 - \\rho)\\theta_{i-1} + \\rho y_i^2$，$i=1, \\dots, n$。一旦计算出阈值向量，模型就变成一个线性回归问题 $\\Delta w_i = \\eta_2 (x_i y_i (y_i - \\theta_i)) + \\varepsilon_i$。在这里，我们拟合一个参数 $p_1 = \\eta_2$。设计矩阵 $\\mathbf{A}_2$ 是一个 $n \\times 1$ 矩阵，其元素为 $(x_i y_i (y_i - \\theta_i))$。拟合参数的数量为 $k_2=1$。\n\n对于每个模型，我们解决线性最小二乘问题，以找到最小化残差平方和 $\\mathrm{RSS} = \\sum_{i=1}^n (\\Delta w_i - \\widehat{\\Delta w}_i)^2$ 的参数向量 $\\mathbf{p}$，其中 $\\widehat{\\Delta w}_i$ 是模型对试验 $i$ 的预测。\n\n最后，我们使用赤池信息准则 (AIC) 进行模型选择。AIC 通过平衡模型的拟合优度（由RSS衡量）和其复杂性（由拟合参数的数量 $k$ 衡量）来提供比较模型的方法。参数更多的模型可能仅仅因为其灵活性增加而获得更好的拟合（更低的RSS），这可能导致过拟合。AIC 对这种复杂性进行惩罚。其公式为：\n$$\n\\mathrm{AIC} = 2k + n \\ln\\left(\\frac{\\mathrm{RSS}}{n}\\right)\n$$\n对于每个数据集，我们计算三个拟合模型中每个模型的 AIC 值。产生最低 AIC 值的模型被选为对观测数据最合理的解释，它平衡了解释能力和简约性。最终输出是代表所选模型（$0$、$1$ 或 $2$）的整数代码。对于四个指定的每个数据集，重复此整个过程。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the model selection process for all test cases.\n    \"\"\"\n    test_cases = [\n        {\n            \"name\": \"Dataset 1 (Simple Hebbian)\",\n            \"seed\": 12345, \"n\": 120,\n            \"log_normal_mu\": [np.log(5), np.log(7)],\n            \"sigma_x\": 0.4, \"sigma_y\": 0.4, \"rho_c\": 0.6,\n            \"w_min\": 0.05, \"w_max\": 0.35,\n            \"true_model\": \"hebbian\",\n            \"model_params\": {\"eta\": 0.008},\n            \"sigma_eps\": 0.002\n        },\n        {\n            \"name\": \"Dataset 2 (Oja)\",\n            \"seed\": 54321, \"n\": 150,\n            \"log_normal_mu\": [np.log(6), np.log(6)],\n            \"sigma_x\": 0.5, \"sigma_y\": 0.5, \"rho_c\": 0.5,\n            \"w_min\": 0.05, \"w_max\": 0.90,\n            \"true_model\": \"oja\",\n            \"model_params\": {\"eta\": 0.020, \"lambda\": 1.2},\n            \"sigma_eps\": 0.003\n        },\n        {\n            \"name\": \"Dataset 3 (BCM)\",\n            \"seed\": 24680, \"n\": 140,\n            \"log_normal_mu\": [np.log(5), np.log(10)],\n            \"sigma_x\": 0.6, \"sigma_y\": 0.6, \"rho_c\": 0.2,\n            \"w_min\": 0.02, \"w_max\": 0.50,\n            \"true_model\": \"bcm\",\n            \"model_params\": {\"eta\": 0.025, \"rho\": 0.05},\n            \"sigma_eps\": 0.003\n        },\n        {\n            \"name\": \"Dataset 4 (Weak Oja)\",\n            \"seed\": 13579, \"n\": 130,\n            \"log_normal_mu\": [np.log(5.5), np.log(6.5)],\n            \"sigma_x\": 0.4, \"sigma_y\": 0.4, \"rho_c\": 0.4,\n            \"w_min\": 0.05, \"w_max\": 0.80,\n            \"true_model\": \"oja\",\n            \"model_params\": {\"eta\": 0.020, \"lambda\": 0.10},\n            \"sigma_eps\": 0.003\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        selected_model = process_dataset(case)\n        results.append(selected_model)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef generate_data(params):\n    \"\"\"\n    Generates synthetic synaptic plasticity data based on provided parameters.\n    \"\"\"\n    rng = np.random.default_rng(params[\"seed\"])\n    n = params[\"n\"]\n\n    # Generate correlated log-normal firing rates x and y\n    mu = params[\"log_normal_mu\"]\n    cov = [[params[\"sigma_x\"]**2, params[\"rho_c\"] * params[\"sigma_x\"] * params[\"sigma_y\"]],\n           [params[\"rho_c\"] * params[\"sigma_x\"] * params[\"sigma_y\"], params[\"sigma_y\"]**2]]\n    z = rng.multivariate_normal(mu, cov, size=n)\n    x, y = np.exp(z[:, 0]), np.exp(z[:, 1])\n\n    # Generate initial weights w\n    w = rng.uniform(params[\"w_min\"], params[\"w_max\"], size=n)\n\n    # Generate noise\n    noise = rng.normal(0, params[\"sigma_eps\"], size=n)\n\n    # Generate delta_w based on the true model\n    model_type = params[\"true_model\"]\n    mp = params[\"model_params\"]\n    delta_w_true = np.zeros(n)\n\n    if model_type == \"hebbian\":\n        delta_w_true = mp[\"eta\"] * x * y\n    elif model_type == \"oja\":\n        delta_w_true = mp[\"eta\"] * x * y - mp[\"eta\"] * mp[\"lambda\"] * y**2 * w\n    elif model_type == \"bcm\":\n        theta0 = np.mean(y**2)\n        theta = np.zeros(n)\n        theta_prev = theta0\n        for i in range(n):\n            theta[i] = (1 - mp[\"rho\"]) * theta_prev + mp[\"rho\"] * y[i]**2\n            theta_prev = theta[i]\n        delta_w_true = mp[\"eta\"] * x * y * (y - theta)\n    \n    delta_w_obs = delta_w_true + noise\n    return x, y, w, delta_w_obs\n\ndef fit_and_select(x, y, w, delta_w):\n    \"\"\"\n    Fits all three models to the data and selects the best one using AIC.\n    \"\"\"\n    n = len(x)\n    aics = []\n    \n    # --- Model 0: Simple Hebbian ---\n    k0 = 1\n    A0 = (x * y).reshape(-1, 1)\n    p0, rss0_arr, _, _ = np.linalg.lstsq(A0, delta_w, rcond=None)\n    rss0 = rss0_arr[0] if len(rss0_arr) > 0 else np.sum((delta_w - A0 @ p0)**2)\n    aic0 = 2 * k0 + n * np.log(rss0 / n)\n    aics.append(aic0)\n\n    # --- Model 1: Oja Rule ---\n    k1 = 2\n    A1 = np.vstack((x * y, -(y**2 * w))).T\n    p1, rss1_arr, _, _ = np.linalg.lstsq(A1, delta_w, rcond=None)\n    rss1 = rss1_arr[0] if len(rss1_arr) > 0 else np.sum((delta_w - A1 @ p1)**2)\n    aic1 = 2 * k1 + n * np.log(rss1 / n)\n    aics.append(aic1)\n\n    # --- Model 2: BCM Rule ---\n    k2 = 1\n    rho_bcm = 0.05\n    theta0 = np.mean(y**2)\n    theta = np.zeros(n)\n    theta_prev = theta0\n    for i in range(n):\n        theta[i] = (1 - rho_bcm) * theta_prev + rho_bcm * y[i]**2\n        theta_prev = theta[i]\n    \n    A2 = (x * y * (y - theta)).reshape(-1, 1)\n    p2, rss2_arr, _, _ = np.linalg.lstsq(A2, delta_w, rcond=None)\n    rss2 = rss2_arr[0] if len(rss2_arr) > 0 else np.sum((delta_w - A2 @ p2)**2)\n    aic2 = 2 * k2 + n * np.log(rss2 / n)\n    aics.append(aic2)\n\n    return np.argmin(aics)\n\ndef process_dataset(params):\n    \"\"\"\n    Orchestrates data generation and model selection for a single dataset.\n    \"\"\"\n    x, y, w, delta_w = generate_data(params)\n    # The logic in the problem description for Oja regression is slightly off.\n    # It says to fit against (x*y) and (y^2*w).\n    # The model is Δw = η(xy) - ηλ(y²w).\n    # This is Δw = p1*(xy) + p2*(y²w) where p1=η, p2=-ηλ.\n    # The original Python code had A1 = np.vstack((x * y, y**2 * w)).T\n    # This would fit p1 and p2 correctly. Let's revert to a safer implementation.\n    n = len(x)\n    aics = []\n    \n    # Model 0\n    k0 = 1\n    A0 = (x * y).reshape(-1, 1)\n    p0, rss0_arr, _, _ = np.linalg.lstsq(A0, delta_w, rcond=None)\n    rss0 = rss0_arr[0]\n    aic0 = 2 * k0 + n * np.log(rss0 / n)\n    aics.append(aic0)\n\n    # Model 1\n    k1 = 2\n    A1 = np.vstack([x * y, y**2 * w]).T\n    p1, rss1_arr, _, _ = np.linalg.lstsq(A1, delta_w, rcond=None)\n    rss1 = rss1_arr[0]\n    aic1 = 2 * k1 + n * np.log(rss1 / n)\n    aics.append(aic1)\n    \n    # Model 2\n    k2 = 1\n    rho_bcm = 0.05 # This is a fixed hyperparameter for the model, not fitted.\n    theta0 = np.mean(y**2)\n    theta = np.zeros(n)\n    theta_prev = theta0\n    for i in range(n):\n        theta[i] = (1 - rho_bcm) * theta_prev + rho_bcm * y[i]**2\n        theta_prev = theta[i]\n    \n    A2 = (x * y * (y - theta)).reshape(-1, 1)\n    p2, rss2_arr, _, _ = np.linalg.lstsq(A2, delta_w, rcond=None)\n    rss2 = rss2_arr[0]\n    aic2 = 2 * k2 + n * np.log(rss2 / n)\n    aics.append(aic2)\n\n    return np.argmin(aics)\n\n\n# Execute the main function when the script is run.\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}