{
    "hands_on_practices": [
        {
            "introduction": "Before exploring how a single-layer perceptron learns, it is essential to master its fundamental decision-making mechanism. This first exercise focuses on the forward pass, where an input vector $x$ is processed to produce a classification. By computing the pre-activation $a(x) = w^{\\top} x + b$ and applying the threshold function, you will gain a concrete understanding of how the parameters $w$ and $b$ define a linear decision boundary that partitions the input space .",
            "id": "4065769",
            "problem": "Consider a single-layer perceptron realized as a linear threshold unit, a canonical model in neuromorphic and brain-inspired computing that maps an input vector $x \\in \\mathbb{R}^2$ to a scalar pre-activation $a(x) = w^{\\top} x + b$, followed by a sign-threshold output $y(x) = \\operatorname{sgn}(a(x))$, where $\\operatorname{sgn}(z) = 1$ if $z  0$, $\\operatorname{sgn}(z) = -1$ if $z  0$, and $\\operatorname{sgn}(z) = 0$ if $z = 0$ to explicitly encode points lying exactly on the decision boundary. This definition is consistent with the well-tested formulation of linear threshold classification and its decision boundary $w^{\\top} x + b = 0$.\n\nLet the perceptron parameters be $w = \\begin{pmatrix} 3 \\\\ -4 \\end{pmatrix}$ and $b = 5$. Consider the input set $X = \\{x_1, x_2, x_3, x_4\\}$ with\n$$\nx_1 = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}, \\quad\nx_2 = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix}, \\quad\nx_3 = \\begin{pmatrix} 0 \\\\ 3 \\end{pmatrix}, \\quad\nx_4 = \\begin{pmatrix} \\frac{10}{3} \\\\ \\frac{5}{2} \\end{pmatrix}.\n$$\n\nUsing only the fundamental definitions stated above, compute the classification outputs $y(x_i)$ for $i \\in \\{1,2,3,4\\}$ and identify which $x_i$ lie exactly on the decision boundary. Then compute the scalar\n$$\nS = \\sum_{i=1}^{4} y(x_i).\n$$\n\nReport only the value of $S$ as your final answer. No rounding is required.",
            "solution": "The problem statement is subjected to validation prior to any attempt at a solution.\n\n**Step 1: Extract Givens**\n- Model: Single-layer perceptron (linear threshold unit).\n- Input vector: $x \\in \\mathbb{R}^2$.\n- Pre-activation function: $a(x) = w^{\\top} x + b$.\n- Output function: $y(x) = \\operatorname{sgn}(a(x))$.\n- Sign function definition: $\\operatorname{sgn}(z) = 1$ if $z  0$, $\\operatorname{sgn}(z) = -1$ if $z  0$, and $\\operatorname{sgn}(z) = 0$ if $z = 0$.\n- Decision boundary condition: $w^{\\top} x + b = 0$.\n- Perceptron weight vector: $w = \\begin{pmatrix} 3 \\\\ -4 \\end{pmatrix}$.\n- Perceptron bias: $b = 5$.\n- Input set: $X = \\{x_1, x_2, x_3, x_4\\}$.\n- Input vectors:\n$x_1 = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$, $x_2 = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix}$, $x_3 = \\begin{pmatrix} 0 \\\\ 3 \\end{pmatrix}$, $x_4 = \\begin{pmatrix} \\frac{10}{3} \\\\ \\frac{5}{2} \\end{pmatrix}$.\n- Target quantity: $S = \\sum_{i=1}^{4} y(x_i)$.\n\n**Step 2: Validate Using Extracted Givens**\n- The problem is **scientifically grounded** and **objective**. The single-layer perceptron is a fundamental concept in machine learning and computational neuroscience. The definitions provided for the pre-activation, output function, and decision boundary are standard and mathematically precise.\n- The problem is **well-posed**. It provides all the necessary components—the model parameters ($w, b$) and the input data ($x_i$)—to compute the required outputs ($y(x_i)$) and the final sum ($S$). The calculation leads to a unique, deterministic answer.\n- The problem setup is **complete and consistent**. There is no missing information, nor are there any contradictory constraints. The explicit definition of $\\operatorname{sgn}(0)=0$ is crucial and is used consistently with the task of identifying points on the decision boundary.\n\n**Step 3: Verdict and Action**\n- The problem is deemed **valid**. A solution will be derived.\n\nThe task is to compute the sum $S = \\sum_{i=1}^{4} y(x_i)$. This requires calculating the classification output $y(x_i)$ for each input vector $x_i$, which in turn depends on the pre-activation $a(x_i)$. The general formula for the pre-activation is $a(x) = w^{\\top} x + b$.\n\nThe given parameters are $w = \\begin{pmatrix} 3 \\\\ -4 \\end{pmatrix}$ and $b = 5$.\n\nWe proceed by computing the pre-activation $a(x_i)$ and the corresponding output $y(x_i) = \\operatorname{sgn}(a(x_i))$ for each $i \\in \\{1, 2, 3, 4\\}$.\n\nFor $i=1$:\nThe input vector is $x_1 = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$.\nThe pre-activation is:\n$$\na(x_1) = w^{\\top} x_1 + b = \\begin{pmatrix} 3  -4 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} + 5 = (3)(1) + (-4)(2) + 5 = 3 - 8 + 5 = 0\n$$\nSince $a(x_1) = 0$, the output is $y(x_1) = \\operatorname{sgn}(0) = 0$.\n\nFor $i=2$:\nThe input vector is $x_2 = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix}$.\nThe pre-activation is:\n$$\na(x_2) = w^{\\top} x_2 + b = \\begin{pmatrix} 3  -4 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix} + 5 = (3)(2) + (-4)(0) + 5 = 6 + 0 + 5 = 11\n$$\nSince $a(x_2) = 11  0$, the output is $y(x_2) = \\operatorname{sgn}(11) = 1$.\n\nFor $i=3$:\nThe input vector is $x_3 = \\begin{pmatrix} 0 \\\\ 3 \\end{pmatrix}$.\nThe pre-activation is:\n$$\na(x_3) = w^{\\top} x_3 + b = \\begin{pmatrix} 3  -4 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 3 \\end{pmatrix} + 5 = (3)(0) + (-4)(3) + 5 = 0 - 12 + 5 = -7\n$$\nSince $a(x_3) = -7  0$, the output is $y(x_3) = \\operatorname{sgn}(-7) = -1$.\n\nFor $i=4$:\nThe input vector is $x_4 = \\begin{pmatrix} \\frac{10}{3} \\\\ \\frac{5}{2} \\end{pmatrix}$.\nThe pre-activation is:\n$$\na(x_4) = w^{\\top} x_4 + b = \\begin{pmatrix} 3  -4 \\end{pmatrix} \\begin{pmatrix} \\frac{10}{3} \\\\ \\frac{5}{2} \\end{pmatrix} + 5 = (3)\\left(\\frac{10}{3}\\right) + (-4)\\left(\\frac{5}{2}\\right) + 5 = 10 - 10 + 5 = 5\n$$\nSince $a(x_4) = 5  0$, the output is $y(x_4) = \\operatorname{sgn}(5) = 1$.\n\nAn input point $x_i$ lies exactly on the decision boundary if its pre-activation is zero, i.e., $a(x_i) = w^{\\top} x_i + b = 0$. From our calculations, only $a(x_1) = 0$. Therefore, $x_1$ is the only input point that lies exactly on the decision boundary.\n\nFinally, we compute the scalar sum $S$:\n$$\nS = \\sum_{i=1}^{4} y(x_i) = y(x_1) + y(x_2) + y(x_3) + y(x_4)\n$$\nSubstituting the computed values:\n$$\nS = 0 + 1 + (-1) + 1 = 1\n$$\nThe value of the sum is $1$.",
            "answer": "$$\\boxed{1}$$"
        },
        {
            "introduction": "Building upon the static classification process, we now turn to the dynamic nature of learning in a perceptron. This practice simulates the classic mistake-driven learning algorithm, where the model's parameters are updated sequentially in response to misclassified examples. By manually tracing the evolution of the weight vector $w$ and bias $b$, you will develop an intuition for how the decision boundary iteratively adjusts its position and orientation to correctly separate the data, demonstrating the core principle of convergence .",
            "id": "4065810",
            "problem": "Consider a single-layer perceptron operating as a hard-threshold classifier in a neuromorphic event-driven stream. The classifier has weight vector $w \\in \\mathbb{R}^{2}$ and bias $b \\in \\mathbb{R}$, and predicts the label of an input $x \\in \\mathbb{R}^{2}$ as $\\operatorname{sign}(w \\cdot x + b)$ with labels $y \\in \\{ -1, +1 \\}$. The learning is mistake-driven with learning rate $\\eta = 1$: upon receiving the $t$-th example $(x^{(t)}, y^{(t)})$, compute the activation $a^{(t)} = w^{(t-1)} \\cdot x^{(t)} + b^{(t-1)}$. If $y^{(t)} a^{(t)} \\le 0$, then update according to\n$$\nw^{(t)} = w^{(t-1)} + y^{(t)} x^{(t)}, \n\\quad\nb^{(t)} = b^{(t-1)} + y^{(t)},\n$$\notherwise leave the parameters unchanged, that is, $w^{(t)} = w^{(t-1)}$ and $b^{(t)} = b^{(t-1)}$. The initial state is $w^{(0)} = (0, 0)$ and $b^{(0)} = 0$.\n\nA fixed, finite set of labeled inputs in $\\mathbb{R}^{2}$ is given by\n- $x_{1} = (1, 0)$ with $y_{1} = +1$,\n- $x_{2} = (-0.1, 0)$ with $y_{2} = -1$,\n- $x_{3} = (0, 1)$ with $y_{3} = +1$.\n\nThese inputs are presented as an infinite ordered stream that repeats the cycle $(x_{1}, y_{1}), (x_{2}, y_{2}), (x_{3}, y_{3})$ ad infinitum. Presentation time $t \\in \\mathbb{N}$ counts the total number of examples processed, starting at $t = 1$.\n\nDefine the classifier to be consistent with the data at time $t$ if, immediately after processing the example at time $t$, the current parameters $(w^{(t)}, b^{(t)})$ classify all three points $\\{x_{1}, x_{2}, x_{3}\\}$ correctly (i.e., for each $i \\in \\{1,2,3\\}$, $y_{i}\\,(w^{(t)} \\cdot x_{i} + b^{(t)})  0$).\n\nCompute explicitly the sequence of parameter states $(w^{(t)}, b^{(t)})$ for consecutive presentation times $t$ until the earliest presentation time $T$ at which the classifier becomes consistent with the data (as defined above). Report the minimal such presentation index $T$. Your final answer must be a single integer with no units. No rounding is required.",
            "solution": "The user wants me to solve a problem for a given problem statement.\n\nThe problem asks for the earliest presentation time $T$ at which a single-layer perceptron classifier becomes consistent with a given dataset. The learning process is mistake-driven and proceeds by iterating through a fixed cycle of three data points.\n\nFirst, I will validate the problem statement.\n\n### Step 1: Extract Givens\n- **Model**: Single-layer perceptron.\n- **Input space**: $x \\in \\mathbb{R}^{2}$.\n- **Label space**: $y \\in \\{ -1, +1 \\}$.\n- **Classifier function**: $\\operatorname{sign}(w \\cdot x + b)$, where $w \\in \\mathbb{R}^{2}$ and $b \\in \\mathbb{R}$.\n- **Learning rule**: Mistake-driven (Perceptron algorithm) with learning rate $\\eta = 1$.\n- **Update condition**: An update occurs at step $t$ if $y^{(t)} a^{(t)} \\le 0$, where $a^{(t)} = w^{(t-1)} \\cdot x^{(t)} + b^{(t-1)}$.\n- **Update equations**:\n    - $w^{(t)} = w^{(t-1)} + y^{(t)} x^{(t)}$\n    - $b^{(t)} = b^{(t-1)} + y^{(t)}$\n- **No-update condition**: If $y^{(t)} a^{(t)}  0$, then $w^{(t)} = w^{(t-1)}$ and $b^{(t)} = b^{(t-1)}$.\n- **Initial state**: $w^{(0)} = (0, 0)$ and $b^{(0)} = 0$.\n- **Data set**:\n    - $x_{1} = (1, 0)$ with $y_{1} = +1$.\n    - $x_{2} = (-0.1, 0)$ with $y_{2} = -1$.\n    - $x_{3} = (0, 1)$ with $y_{3} = +1$.\n- **Input stream**: An infinite sequence repeating the cycle $(x_{1}, y_{1}), (x_{2}, y_{2}), (x_{3}, y_{3})$.\n- **Presentation time**: $t \\in \\mathbb{N}$ starts at $t=1$.\n- **Consistency definition**: The classifier is consistent at time $t$ if the parameters $(w^{(t)}, b^{(t)})$ correctly classify all three points $\\{x_{1}, x_{2}, x_{3}\\}$. Correct classification means $y_{i}\\,(w^{(t)} \\cdot x_{i} + b^{(t)})  0$ for $i \\in \\{1, 2, 3\\}$.\n- **Goal**: Find the smallest presentation time $T$ at which the classifier becomes consistent.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, well-posed, and objective. It provides a standard formulation of the Perceptron learning algorithm, a cornerstone of machine learning. All initial conditions, data, update rules, and the stopping criterion are precisely defined. The dataset is linearly separable, which guarantees that the algorithm will converge to a solution in a finite number of steps. The problem asks for a verifiable quantity (the number of steps to convergence) under a specific data presentation order. There are no contradictions, ambiguities, or factual errors.\n\n### Step 3: Verdict and Action\nThe problem is valid. I will now proceed with the solution.\n\nTo find the earliest presentation time $T$, we must simulate the learning process step-by-step, starting from the initial state and updating the parameters upon each mistake. After each step $t$, we check if the resulting classifier $(w^{(t)}, b^{(t)})$ is consistent with the entire dataset.\n\nThe initial state of the parameters at $t=0$ is $w^{(0)} = (0, 0)$ and $b^{(0)} = 0$.\n\n**Step $t=1$**:\nThe first example presented is $(x^{(1)}, y^{(1)}) = (x_1, y_1) = ((1, 0), +1)$.\nThe parameters before this step are $(w^{(0)}, b^{(0)}) = ((0, 0), 0)$.\nThe activation is $a^{(1)} = w^{(0)} \\cdot x^{(1)} + b^{(0)} = (0, 0) \\cdot (1, 0) + 0 = 0$.\nWe check the mistake condition: $y^{(1)}a^{(1)} = (+1)(0) = 0$. Since $0 \\le 0$, a mistake has occurred.\nWe update the parameters:\n$w^{(1)} = w^{(0)} + y^{(1)}x^{(1)} = (0, 0) + (+1)(1, 0) = (1, 0)$.\n$b^{(1)} = b^{(0)} + y^{(1)} = 0 + 1 = 1$.\nThe state after step $1$ is $(w^{(1)}, b^{(1)}) = ((1, 0), 1)$.\nWe check for consistency:\n1. For $(x_1, y_1)$: $y_1(w^{(1)} \\cdot x_1 + b^{(1)}) = +1((1, 0) \\cdot (1, 0) + 1) = 2  0$. (Correct)\n2. For $(x_2, y_2)$: $y_2(w^{(1)} \\cdot x_2 + b^{(1)}) = -1((1, 0) \\cdot (-0.1, 0) + 1) = -1(-0.1 + 1) = -0.9 \\le 0$. (Incorrect)\nThe classifier is not consistent.\n\n**Step $t=2$**:\nThe second example is $(x^{(2)}, y^{(2)}) = (x_2, y_2) = ((-0.1, 0), -1)$.\nThe parameters are $(w^{(1)}, b^{(1)}) = ((1, 0), 1)$.\nThe activation is $a^{(2)} = w^{(1)} \\cdot x^{(2)} + b^{(1)} = (1, 0) \\cdot (-0.1, 0) + 1 = -0.1 + 1 = 0.9$.\nMistake check: $y^{(2)}a^{(2)} = (-1)(0.9) = -0.9 \\le 0$. A mistake has occurred.\nUpdate:\n$w^{(2)} = w^{(1)} + y^{(2)}x^{(2)} = (1, 0) + (-1)(-0.1, 0) = (1, 0) + (0.1, 0) = (1.1, 0)$.\n$b^{(2)} = b^{(1)} + y^{(2)} = 1 + (-1) = 0$.\nState after step $2$: $(w^{(2)}, b^{(2)}) = ((1.1, 0), 0)$.\nConsistency check:\n1. For $(x_1, y_1)$: $y_1(w^{(2)} \\cdot x_1 + b^{(2)}) = +1((1.1, 0) \\cdot (1, 0) + 0) = 1.1  0$. (Correct)\n2. For $(x_2, y_2)$: $y_2(w^{(2)} \\cdot x_2 + b^{(2)}) = -1((1.1, 0) \\cdot (-0.1, 0) + 0) = -1(-0.11) = 0.11  0$. (Correct)\n3. For $(x_3, y_3)$: $y_3(w^{(2)} \\cdot x_3 + b^{(2)}) = +1((1.1, 0) \\cdot (0, 1) + 0) = 0 \\le 0$. (Incorrect)\nThe classifier is not consistent.\n\n**Step $t=3$**:\nThe third example is $(x^{(3)}, y^{(3)}) = (x_3, y_3) = ((0, 1), +1)$.\nThe parameters are $(w^{(2)}, b^{(2)}) = ((1.1, 0), 0)$.\nThe activation is $a^{(3)} = w^{(2)} \\cdot x^{(3)} + b^{(2)} = (1.1, 0) \\cdot (0, 1) + 0 = 0$.\nMistake check: $y^{(3)}a^{(3)} = (+1)(0) = 0 \\le 0$. A mistake has occurred.\nUpdate:\n$w^{(3)} = w^{(2)} + y^{(3)}x^{(3)} = (1.1, 0) + (+1)(0, 1) = (1.1, 1)$.\n$b^{(3)} = b^{(2)} + y^{(3)} = 0 + 1 = 1$.\nState after step $3$: $(w^{(3)}, b^{(3)}) = ((1.1, 1), 1)$.\nConsistency check:\n1. For $(x_1, y_1)$: $y_1(w^{(3)} \\cdot x_1 + b^{(3)}) = +1((1.1, 1) \\cdot (1, 0) + 1) = 2.1  0$. (Correct)\n2. For $(x_2, y_2)$: $y_2(w^{(3)} \\cdot x_2 + b^{(3)}) = -1((1.1, 1) \\cdot (-0.1, 0) + 1) = -1(-0.11 + 1) = -0.89 \\le 0$. (Incorrect)\nThe classifier is not consistent.\n\n**Step $t=4$**:\nThe stream repeats; the fourth example is $(x^{(4)}, y^{(4)}) = (x_1, y_1) = ((1, 0), +1)$.\nThe parameters are $(w^{(3)}, b^{(3)}) = ((1.1, 1), 1)$.\nThe activation is $a^{(4)} = w^{(3)} \\cdot x^{(4)} + b^{(3)} = (1.1, 1) \\cdot (1, 0) + 1 = 1.1 + 1 = 2.1$.\nMistake check: $y^{(4)}a^{(4)} = (+1)(2.1) = 2.1  0$. No mistake.\nThe parameters are not updated: $(w^{(4)}, b^{(4)}) = (w^{(3)}, b^{(3)}) = ((1.1, 1), 1)$.\nSince the parameters have not changed from step $t=3$, the classifier is still not consistent.\n\n**Step $t=5$**:\nThe fifth example is $(x^{(5)}, y^{(5)}) = (x_2, y_2) = ((-0.1, 0), -1)$.\nThe parameters are $(w^{(4)}, b^{(4)}) = ((1.1, 1), 1)$.\nThe activation is $a^{(5)} = w^{(4)} \\cdot x^{(5)} + b^{(4)} = (1.1, 1) \\cdot (-0.1, 0) + 1 = -0.11 + 1 = 0.89$.\nMistake check: $y^{(5)}a^{(5)} = (-1)(0.89) = -0.89 \\le 0$. A mistake has occurred.\nUpdate:\n$w^{(5)} = w^{(4)} + y^{(5)}x^{(5)} = (1.1, 1) + (-1)(-0.1, 0) = (1.1, 1) + (0.1, 0) = (1.2, 1)$.\n$b^{(5)} = b^{(4)} + y^{(5)} = 1 + (-1) = 0$.\nState after step $5$: $(w^{(5)}, b^{(5)}) = ((1.2, 1), 0)$.\nConsistency check:\n1. For $(x_1, y_1)$: $y_1(w^{(5)} \\cdot x_1 + b^{(5)}) = +1((1.2, 1) \\cdot (1, 0) + 0) = 1.2  0$. (Correct)\n2. For $(x_2, y_2)$: $y_2(w^{(5)} \\cdot x_2 + b^{(5)}) = -1((1.2, 1) \\cdot (-0.1, 0) + 0) = -1(-0.12) = 0.12  0$. (Correct)\n3. For $(x_3, y_3)$: $y_3(w^{(5)} \\cdot x_3 + b^{(5)}) = +1((1.2, 1) \\cdot (0, 1) + 0) = 1  0$. (Correct)\nAfter processing the example at $t=5$, the classifier correctly classifies all three points in the dataset. Therefore, the classifier has become consistent.\n\nThe earliest presentation time $T$ at which the classifier becomes consistent is $5$.",
            "answer": "$$\\boxed{5}$$"
        },
        {
            "introduction": "Once a perceptron has learned a separating hyperplane, a crucial next step is to evaluate the quality and robustness of the solution. This practice introduces the geometric margin, a powerful concept that quantifies the 'confidence' of the classification by measuring the shortest distance from any data point to the decision boundary. Calculating the margin provides a tangible measure of the classifier's tolerance to noise and input perturbations, a vital consideration in the design of reliable neuromorphic systems .",
            "id": "4065844",
            "problem": "Consider a single-layer perceptron implemented on a resistive crossbar array within a neuromorphic processor. The perceptron computes a decision by applying a linear threshold: a data vector $x \\in \\mathbb{R}^{3}$ is classified according to the sign of the affine functional determined by the weight vector $w \\in \\mathbb{R}^{3}$ and bias $b \\in \\mathbb{R}$. The geometric margin, defined as the smallest signed distance of labeled samples to the decision hyperplane under the labeling convention $y \\in \\{-1,+1\\}$, quantifies the robustness of the decision boundary to perturbations in the input and hardware noise.\n\nA calibration procedure yields the candidate hyperplane parameters $w = (3,-1,2)$ and $b = -1$. You are given the labeled dataset $\\{(x_i,y_i)\\}_{i=1}^{6}$ in $\\mathbb{R}^{3}$ with\n- $x_{1} = (1,0,1)$, $y_{1} = +1$,\n- $x_{2} = (2,-1,0)$, $y_{2} = +1$,\n- $x_{3} = (0,1,-1)$, $y_{3} = -1$,\n- $x_{4} = (1,2,-2)$, $y_{4} = -1$,\n- $x_{5} = (3,0,1)$, $y_{5} = +1$,\n- $x_{6} = (-1,1,0)$, $y_{6} = -1$.\n\nStarting from the core definitions of the single-layer perceptron decision rule and the signed distance of a point to a hyperplane in Euclidean space, derive the expression for the geometric margin of this hyperplane with respect to the given dataset, and compute its exact value. Express your final answer as a single simplified exact number. Do not round.",
            "solution": "The problem statement is evaluated and found to be valid. It is scientifically grounded in the principles of machine learning, specifically the single-layer perceptron model. The problem is well-posed, objective, and self-contained, providing all necessary data and definitions for a unique solution.\n\nThe core task is to compute the geometric margin of a dataset with respect to a given decision hyperplane. The decision hyperplane is defined by the equation $w \\cdot x + b = 0$, where $w$ is the weight vector and $b$ is the bias term.\n\nThe signed distance of a point $x_i$ from this hyperplane is given by the formula:\n$$ d_i = \\frac{w \\cdot x_i + b}{\\|w\\|} $$\nwhere $\\|w\\|$ is the Euclidean norm of the weight vector.\n\nThe geometric margin for a single correctly classified data point $(x_i, y_i)$, where $y_i \\in \\{-1, +1\\}$ is the label, is the perpendicular distance from the point to the hyperplane. This is calculated as:\n$$ \\gamma_i = y_i \\frac{w \\cdot x_i + b}{\\|w\\|} $$\nFor a correctly classified point, $y_i$ has the same sign as $w \\cdot x_i + b$, so $\\gamma_i$ is always positive. The problem defines the geometric margin of the entire dataset as the smallest of these individual margins. Therefore, we seek to compute:\n$$ \\gamma = \\min_{i} \\left( y_i \\frac{w \\cdot x_i + b}{\\|w\\|} \\right) $$\n\nFirst, we identify the given parameters:\nWeight vector: $w = (3, -1, 2)$\nBias: $b = -1$\n\nNext, we calculate the Euclidean norm of the weight vector $w$:\n$$ \\|w\\| = \\sqrt{3^2 + (-1)^2 + 2^2} = \\sqrt{9 + 1 + 4} = \\sqrt{14} $$\n\nNow, we evaluate the affine functional $f(x_i) = w \\cdot x_i + b$ for each data point $(x_i, y_i)$ in the dataset.\n\nFor $(x_1, y_1) = ((1, 0, 1), +1)$:\n$$ f(x_1) = (3)(1) + (-1)(0) + (2)(1) + (-1) = 3 - 0 + 2 - 1 = 4 $$\nThe sign of $f(x_1)$ is positive, which matches $y_1 = +1$. The point is correctly classified.\nThe margin for this point is $\\gamma_1 = y_1 \\frac{f(x_1)}{\\|w\\|} = (+1) \\frac{4}{\\sqrt{14}} = \\frac{4}{\\sqrt{14}}$.\n\nFor $(x_2, y_2) = ((2, -1, 0), +1)$:\n$$ f(x_2) = (3)(2) + (-1)(-1) + (2)(0) + (-1) = 6 + 1 + 0 - 1 = 6 $$\nThe sign of $f(x_2)$ is positive, matching $y_2 = +1$. The point is correctly classified.\nThe margin is $\\gamma_2 = (+1) \\frac{6}{\\sqrt{14}} = \\frac{6}{\\sqrt{14}}$.\n\nFor $(x_3, y_3) = ((0, 1, -1), -1)$:\n$$ f(x_3) = (3)(0) + (-1)(1) + (2)(-1) + (-1) = 0 - 1 - 2 - 1 = -4 $$\nThe sign of $f(x_3)$ is negative, matching $y_3 = -1$. The point is correctly classified.\nThe margin is $\\gamma_3 = (-1) \\frac{-4}{\\sqrt{14}} = \\frac{4}{\\sqrt{14}}$.\n\nFor $(x_4, y_4) = ((1, 2, -2), -1)$:\n$$ f(x_4) = (3)(1) + (-1)(2) + (2)(-2) + (-1) = 3 - 2 - 4 - 1 = -4 $$\nThe sign of $f(x_4)$ is negative, matching $y_4 = -1$. The point is correctly classified.\nThe margin is $\\gamma_4 = (-1) \\frac{-4}{\\sqrt{14}} = \\frac{4}{\\sqrt{14}}$.\n\nFor $(x_5, y_5) = ((3, 0, 1), +1)$:\n$$ f(x_5) = (3)(3) + (-1)(0) + (2)(1) + (-1) = 9 - 0 + 2 - 1 = 10 $$\nThe sign of $f(x_5)$ is positive, matching $y_5 = +1$. The point is correctly classified.\nThe margin is $\\gamma_5 = (+1) \\frac{10}{\\sqrt{14}} = \\frac{10}{\\sqrt{14}}$.\n\nFor $(x_6, y_6) = ((-1, 1, 0), -1)$:\n$$ f(x_6) = (3)(-1) + (-1)(1) + (2)(0) + (-1) = -3 - 1 + 0 - 1 = -5 $$\nThe sign of $f(x_6)$ is negative, matching $y_6 = -1$. The point is correctly classified.\nThe margin is $\\gamma_6 = (-1) \\frac{-5}{\\sqrt{14}} = \\frac{5}{\\sqrt{14}}$.\n\nAll points in the dataset are correctly classified by the given hyperplane. The geometric margin for the dataset is the minimum of the individual margins:\n$$ \\gamma = \\min\\{\\gamma_1, \\gamma_2, \\gamma_3, \\gamma_4, \\gamma_5, \\gamma_6\\} $$\n$$ \\gamma = \\min\\left\\{\\frac{4}{\\sqrt{14}}, \\frac{6}{\\sqrt{14}}, \\frac{4}{\\sqrt{14}}, \\frac{4}{\\sqrt{14}}, \\frac{10}{\\sqrt{14}}, \\frac{5}{\\sqrt{14}}\\right\\} $$\nThe minimum value in this set is $\\frac{4}{\\sqrt{14}}$.\n\nTo provide the answer in a simplified exact form, we rationalize the denominator:\n$$ \\gamma = \\frac{4}{\\sqrt{14}} = \\frac{4 \\cdot \\sqrt{14}}{\\sqrt{14} \\cdot \\sqrt{14}} = \\frac{4\\sqrt{14}}{14} = \\frac{2\\sqrt{14}}{7} $$\n\nThus, the geometric margin of the hyperplane with respect to the given dataset is $\\frac{2\\sqrt{14}}{7}$. This value represents the smallest distance from any data point in the set to the decision boundary, quantifying the robustness of the classification.",
            "answer": "$$\\boxed{\\frac{2\\sqrt{14}}{7}}$$"
        }
    ]
}