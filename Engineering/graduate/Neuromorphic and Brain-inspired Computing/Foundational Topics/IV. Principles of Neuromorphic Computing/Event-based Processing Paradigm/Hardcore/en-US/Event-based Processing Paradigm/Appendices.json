{
    "hands_on_practices": [
        {
            "introduction": "The primary advantage of event-based sensing over traditional frame-based methods lies in its inherent data efficiency. Instead of capturing entire scenes at fixed intervals, which often contain vast amounts of redundant information, event-based sensors transmit data only when and where changes occur. This exercise provides a first-principles approach to quantify this benefit, guiding you to derive a direct comparison of the data rates produced by each paradigm . Mastering this calculation is crucial for understanding the fundamental trade-offs and identifying scenarios where event-based vision offers a compelling advantage.",
            "id": "4043693",
            "problem": "Consider a neuromorphic vision sensor that follows the event-based processing paradigm. In this paradigm, pixels generate asynchronous events when their local signal changes sufficiently. Assume the following scenario is stationary over the observation interval: a fraction $p$ of the $N_x \\times N_y$ pixels are active, and each active pixel produces events according to a memoryless process with average rate $\\lambda$ events per second. Each event carries an address with $b_a$ bits and a timestamp with $b_t$ bits. Ignore all additional protocol overhead and assume addresses and timestamps are encoded independently per event. For comparison, consider a conventional frame-based camera with the same spatial resolution $N_x \\times N_y$, operating at $f$ frames per second (Frames Per Second (FPS)), where each pixel intensity is quantized with $b_p$ bits per pixel, and assume negligible overhead beyond the pixel data.\n\nStarting from first principles and well-tested facts about rate processes and information representation, derive an expression for the expected event-stream data rate in bits per second and, based on that, derive a closed-form expression for the dimensionless ratio $\\Gamma$ of the event-stream data rate to the frame-based data rate. Provide your final answer as a single analytical expression for $\\Gamma$ in terms of $p$, $\\lambda$, $f$, $b_a$, $b_t$, and $b_p$.",
            "solution": "The event-based sensor can be modeled by a collection of independent point processes, one per pixel. We assume stationarity and memorylessness for the active pixels, which is consistent with a Poisson process model. For a Poisson process with rate $\\lambda$ (events per second), the expected number of events in a time interval of length $T$ is $\\lambda T$. By linearity of expectation, the expected number of events produced by $M$ independent processes, each with rate $\\lambda$, over duration $T$ is $M \\lambda T$.\n\nIn the given scenario, a fraction $p$ of the $N_x \\times N_y$ pixels are active, so the expected number of active pixels is\n$$\nM \\;=\\; p \\, N_x N_y.\n$$\nTherefore, the expected number of events per second generated by the sensor is\n$$\n\\text{events per second} \\;=\\; M \\lambda \\;=\\; p \\, N_x N_y \\, \\lambda.\n$$\nEach event carries an address of $b_a$ bits and a timestamp of $b_t$ bits. Assuming both are included per event and neglecting other overhead, the number of bits per event is\n$$\nb_{\\text{event}} \\;=\\; b_a + b_t.\n$$\nHence, the expected event-stream data rate $R_{\\text{event}}$ in bits per second is\n$$\nR_{\\text{event}} \\;=\\; \\Big(p \\, N_x N_y \\, \\lambda\\Big) \\, \\Big(b_a + b_t\\Big).\n$$\n\nFor the frame-based camera, each frame encodes $N_x N_y$ pixel intensities, with $b_p$ bits per pixel. The number of bits per frame is\n$$\nb_{\\text{frame}} \\;=\\; N_x N_y \\, b_p.\n$$\nAt $f$ frames per second, the frame-based data rate $R_{\\text{frame}}$ is\n$$\nR_{\\text{frame}} \\;=\\; f \\, b_{\\text{frame}} \\;=\\; f \\, N_x N_y \\, b_p.\n$$\n\nWe are asked to compare the event-stream data rate to the frame-based data rate using a dimensionless ratio $\\Gamma$ defined as\n$$\n\\Gamma \\;=\\; \\frac{R_{\\text{event}}}{R_{\\text{frame}}}.\n$$\nSubstituting the expressions derived above,\n$$\n\\Gamma \\;=\\; \\frac{\\Big(p \\, N_x N_y \\, \\lambda\\Big)\\, \\Big(b_a + b_t\\Big)}{f \\, N_x N_y \\, b_p}.\n$$\nThe common factor $N_x N_y$ cancels, yielding the closed-form expression\n$$\n\\Gamma \\;=\\; \\frac{p \\, \\lambda \\, \\Big(b_a + b_t\\Big)}{f \\, b_p}.\n$$\nThis expression depends only on the active fraction $p$, the per-active-pixel event rate $\\lambda$, the frame rate $f$, and the per-sample bit allocations $b_a$, $b_t$, and $b_p$. It encapsulates the comparison requested and is dimensionless, as required.",
            "answer": "$$\\boxed{\\frac{p\\,\\lambda\\,(b_a+b_t)}{f\\,b_p}}$$"
        },
        {
            "introduction": "The efficiency of the event-based paradigm extends beyond sensing into computation, particularly in the realm of neural networks. Sparse input events from a sensor can be propagated as sparse activations through a network, drastically reducing the number of required computations compared to dense, frame-by-frame processing. This practice explores the concept of event-driven updates in a feedforward network, where neurons communicate only when their state changes significantly . By calculating the total operations saved, you will gain a tangible understanding of how event-based processing translates to significant computational savings.",
            "id": "4043669",
            "problem": "Consider a feedforward network operated under an event-based processing paradigm. The network has an input layer of size $N_{0}$, a hidden layer of size $N_{1}$, and an output layer of size $N_{2}$. The hidden and output layers use the Rectified Linear Unit (ReLU) nonlinearity, which is known to be $1$-Lipschitz, that is, for any inputs $u$ and $v$, $|\\mathrm{ReLU}(u) - \\mathrm{ReLU}(v)| \\leq |u - v|$. In the dense update regime, at each discrete time step $t$, each neuron computes its preactivation $a_{j}^{(\\ell)}(t) = \\sum_{i} w_{ji}^{(\\ell)} x_{i}^{(\\ell-1)}(t) + b_{j}^{(\\ell)}$ and applies the nonlinearity to yield $y_{j}^{(\\ell)}(t) = \\phi(a_{j}^{(\\ell)}(t))$, with $\\phi(u) = \\max\\{0, u\\}$. In the event-driven regime, each neuron maintains a locally stored last-transmitted preactivation $\\hat{a}_{j}^{(\\ell)}$ and transmits an event only when the deviation of its current preactivation from this stored value exceeds a threshold $\\tau_{\\ell}$. An event at a neuron triggers a Multiply-Accumulate (MAC) operation along each of its outgoing synapses.\n\nPart A (derivation): Starting from the $L$-Lipschitz continuity of the nonlinearity and an intra-layer error tolerance $\\varepsilon_{\\ell}$ on the deviation of the transmitted activation, derive a sufficient condition for propagating an event in layer $\\ell$ that guarantees $|\\phi(a_{j}^{(\\ell)}(t)) - \\phi(\\hat{a}_{j}^{(\\ell)})| \\leq \\varepsilon_{\\ell}$ is violated only when the threshold is crossed. Express your condition as an inequality involving $|a_{j}^{(\\ell)}(t) - \\hat{a}_{j}^{(\\ell)}|$ and $\\tau_{\\ell}$, and relate $\\tau_{\\ell}$ to $\\varepsilon_{\\ell}$ and the Lipschitz constant $L$ of $\\phi$.\n\nPart B (calculation): Consider the specific network sizes $N_{0} = 4096$, $N_{1} = 1024$, $N_{2} = 256$. Assume $M = 10000$ discrete time steps. In the dense regime, count one MAC per synapse per time step (ignore costs of applying the nonlinearity). In the event-driven regime, suppose the input layer produces a total of $E_{0} = 2.0 \\times 10^{6}$ events over the entire time window, and each input event triggers MACs along all outgoing synapses to the hidden layer. The hidden layer produces events according to an asynchronous delta encoding model: its aggregate total variation of preactivations across all hidden neurons over the window is $V_{1} = 1.024 \\times 10^{9}$, and the hidden-layer tolerance is $\\varepsilon_{1} = 128$. Use the $1$-Lipschitz property of ReLU to set $\\tau_{1}$, and assume the number of hidden-layer events satisfies $E_{1} = V_{1} / \\tau_{1}$, each of which triggers MACs along all outgoing synapses to the output layer. Compute the total number of MAC operations saved by the event-driven regime relative to the dense regime over the full window. Report your final answer as a single real-valued number. Do not include units and do not round; provide the exact value.",
            "solution": "For Part A, we start from the definition of $L$-Lipschitz continuity. A function $\\phi$ is $L$-Lipschitz if for any $u$ and $v$,\n$$\n|\\phi(u) - \\phi(v)| \\leq L |u - v|.\n$$\nFor the Rectified Linear Unit (ReLU), it is known that $L = 1$. In an event-based regime, a neuron stores its last transmitted preactivation $\\hat{a}_{j}^{(\\ell)}$ and decides whether to transmit based on the deviation of its current preactivation $a_{j}^{(\\ell)}(t)$. To guarantee that the deviation in the transmitted activation $|\\phi(a_{j}^{(\\ell)}(t)) - \\phi(\\hat{a}_{j}^{(\\ell)})|$ does not exceed an intra-layer tolerance $\\varepsilon_{\\ell}$, it suffices to ensure\n$$\n|\\phi(a_{j}^{(\\ell)}(t)) - \\phi(\\hat{a}_{j}^{(\\ell)})| \\leq \\varepsilon_{\\ell}.\n$$\nBy Lipschitz continuity,\n$$\n|\\phi(a_{j}^{(\\ell)}(t)) - \\phi(\\hat{a}_{j}^{(\\ell)})| \\leq L |a_{j}^{(\\ell)}(t) - \\hat{a}_{j}^{(\\ell)}|.\n$$\nThus, if we enforce $|a_{j}^{(\\ell)}(t) - \\hat{a}_{j}^{(\\ell)}|  \\tau_{\\ell}$ with $\\tau_{\\ell} = \\varepsilon_{\\ell} / L$, then the activation deviation is bounded by $\\varepsilon_{\\ell}$. Consequently, a sufficient and standard event propagation condition is\n$$\n|a_{j}^{(\\ell)}(t) - \\hat{a}_{j}^{(\\ell)}| \\geq \\tau_{\\ell},\n$$\nwith\n$$\n\\tau_{\\ell} = \\frac{\\varepsilon_{\\ell}}{L}.\n$$\nFor ReLU, $L = 1$, so $\\tau_{\\ell} = \\varepsilon_{\\ell}$. This rule ensures that events are propagated only when the stored preactivation and the current preactivation differ by at least the threshold corresponding to the allowed activation deviation, and therefore respects the sparse update principle by transmitting updates only when needed to maintain the prescribed accuracy.\n\nFor Part B, we quantify MAC operations in both regimes.\n\nDense regime: Count one MAC per synapse per time step. The number of synapses between input and hidden is $N_{0} N_{1}$, and between hidden and output is $N_{1} N_{2}$. Therefore, the total dense MACs over $M$ steps is\n$$\n\\mathrm{MAC}_{\\mathrm{dense}} = M \\left( N_{0} N_{1} + N_{1} N_{2} \\right).\n$$\nSubstitute the given values $N_{0} = 4096$, $N_{1} = 1024$, $N_{2} = 256$, and $M = 10000$:\n$$\nN_{0} N_{1} = 4096 \\times 1024 = 4{,}194{,}304,\n$$\n$$\nN_{1} N_{2} = 1024 \\times 256 = 262{,}144,\n$$\n$$\nN_{0} N_{1} + N_{1} N_{2} = 4{,}194{,}304 + 262{,}144 = 4{,}456{,}448,\n$$\n$$\n\\mathrm{MAC}_{\\mathrm{dense}} = 10000 \\times 4{,}456{,}448 = 44{,}564{,}480{,}000.\n$$\n\nEvent-driven regime: Each event triggers MACs along all outgoing synapses. Input events: There are $E_{0} = 2.0 \\times 10^{6}$ input events, each fan out to $N_{1}$ synapses, so\n$$\n\\mathrm{MAC}_{0 \\to 1} = E_{0} N_{1} = (2.0 \\times 10^{6}) \\times 1024 = 2{,}048{,}000{,}000.\n$$\nHidden-layer events: The hidden layer uses asynchronous delta encoding. Its total variation is $V_{1} = 1.024 \\times 10^{9}$, and with ReLU being $1$-Lipschitz and hidden-layer tolerance $\\varepsilon_{1} = 128$, we set\n$$\n\\tau_{1} = \\frac{\\varepsilon_{1}}{L} = \\frac{128}{1} = 128.\n$$\nUnder this encoding, the number of hidden events is\n$$\nE_{1} = \\frac{V_{1}}{\\tau_{1}} = \\frac{1.024 \\times 10^{9}}{128} = 8.0 \\times 10^{6}.\n$$\nEach hidden-layer event fans out to $N_{2}$ synapses, hence\n$$\n\\mathrm{MAC}_{1 \\to 2} = E_{1} N_{2} = (8.0 \\times 10^{6}) \\times 256 = 2{,}048{,}000{,}000.\n$$\nTotal event-driven MACs are\n$$\n\\mathrm{MAC}_{\\mathrm{event}} = \\mathrm{MAC}_{0 \\to 1} + \\mathrm{MAC}_{1 \\to 2} = 2{,}048{,}000{,}000 + 2{,}048{,}000{,}000 = 4{,}096{,}000{,}000.\n$$\n\nOperations saved relative to dense updates are\n$$\nS = \\mathrm{MAC}_{\\mathrm{dense}} - \\mathrm{MAC}_{\\mathrm{event}} = 44{,}564{,}480{,}000 - 4{,}096{,}000{,}000 = 40{,}468{,}480{,}000.\n$$\nExpressed in scientific notation,\n$$\nS = 4.046848 \\times 10^{10}.\n$$\n\nTherefore, the sufficient condition for event propagation is $|a_{j}^{(\\ell)}(t) - \\hat{a}_{j}^{(\\ell)}| \\geq \\tau_{\\ell}$ with $\\tau_{\\ell} = \\varepsilon_{\\ell} / L$, and the exact number of MAC operations saved under the given parameters is $4.046848 \\times 10^{10}$.",
            "answer": "$$\\boxed{4.046848 \\times 10^{10}}$$"
        },
        {
            "introduction": "To fully harness the power of event-based systems, we must adopt simulation methods that mirror their asynchronous, discrete-event nature. Standard time-stepped simulators are ill-suited, as they waste computational cycles on moments of inactivity. This practice challenges you to design and analyze an event-driven simulator for a Leaky Integrate-and-Fire (LIF) neuron, the canonical model in spiking neural networks . By contrasting its performance with a traditional time-stepped approach, you will develop a deep appreciation for why the complexity of an event-driven simulation scales with the number of events, not the passage of time.",
            "id": "4043696",
            "problem": "Design and implement an event-driven simulator for a single Leaky Integrate-and-Fire (LIF) neuron that advances the state only at synaptic event times and at threshold crossings (spike times). Derive the simulator behavior from fundamental definitions of LIF dynamics and prove, by first principles and asymptotic analysis, that its time complexity scales with the number of events rather than the number of uniform time steps. Additionally, implement a reference time-stepped simulator for comparison. Your program must compute specified outputs for several test cases and print them in the required format.\n\nFundamental base and definitions:\n- The Leaky Integrate-and-Fire (LIF) neuron has membrane potential $V(t)$ governed by the ordinary differential equation \n$$\\frac{dV}{dt} = -\\frac{V(t) - V_{\\mathrm{leak}}}{\\tau} + I_{\\mathrm{syn}}(t),$$\nwhere $V_{\\mathrm{leak}}$ is the leak reversal potential, $\\tau$ is the membrane time constant, and $I_{\\mathrm{syn}}(t)$ is the synaptic input current. We model synaptic input as a sum of instantaneous impulses (Dirac delta functions) that cause instantaneous jumps in $V(t)$:\n$$I_{\\mathrm{syn}}(t) = \\sum_{k=1}^{K} w_k \\,\\delta(t - t_k),$$\nwhere $t_k$ are event times and $w_k$ are event weights measured in volts (interpreted as instantaneous jumps in membrane potential due to integrated impulse current).\n- Between synaptic events and outside refractory periods, the homogeneous solution yields exponential decay:\n$$V(t) = V_{\\mathrm{leak}} + \\big(V(t_0) - V_{\\mathrm{leak}}\\big)\\, e^{-\\frac{t - t_0}{\\tau}} \\quad \\text{for } t \\in (t_0, t_{\\mathrm{next}}),$$\nwith $t_0$ the last state update time. At an event time $t_k$, the membrane potential undergoes an instantaneous jump:\n$$V(t_k^+) = V(t_k^-) + w_k.$$\n- Threshold crossing rule: if $V(t_k^+) \\ge V_{\\mathrm{th}}$, a spike occurs at time $t_k$, $V$ is reset to $V_{\\mathrm{reset}}$, and the neuron enters an absolute refractory period of duration $t_{\\mathrm{ref}}$ during which $V(t) = V_{\\mathrm{reset}}$ is clamped and synaptic inputs are ignored. After the refractory period ends, dynamics resume from $V_{\\mathrm{reset}}$.\n- Initial condition: $V(0) = V_{\\mathrm{leak}}$. All times are measured in seconds. All potentials and weights are measured in volts. Threshold comparisons use the rule $V \\ge V_{\\mathrm{th}}$ at event instants.\n\nAlgorithmic goals:\n- Event-driven simulator: Advance the state only at synaptic event times and threshold crossings. For simultaneous events at identical time $t$, aggregate their weights by summation into a single jump of total weight $\\sum w$ at that time. When an event time occurs during the refractory period, ignore it. Complexity is measured by counting the number of state advancements, defined as the number of processed aggregated event times plus the number of spikes.\n- Reference time-stepped simulator: Use forward Euler updates on a uniform time grid with step size $\\Delta t$, applying events at the start of the step that contains their time. Ignore events during refractory. Complexity is measured by counting the number of time steps $N = \\lceil T_{\\mathrm{end}}/\\Delta t \\rceil$.\n\nMathematical and complexity requirements:\n- Derive the event-driven update from the homogeneous exponential decay between events and instantaneous jumps at events. Prove that the number of state advancements performed by the event-driven simulator is $K_{\\mathrm{agg}} + S$, where $K_{\\mathrm{agg}}$ is the number of aggregated event times and $S$ is the number of spikes, establishing $O(K_{\\mathrm{agg}} + S)$ complexity. In contrast, the time-stepped simulator performs $N$ updates regardless of event sparsity, establishing $O(N)$ complexity. Provide clear reasoning that the event-driven simulator does not perform any updates between events or spikes.\n- The program must compute, for each test case, the integer spike count produced by the event-driven simulator, the integer spike count produced by the time-stepped simulator, the integer count of event-driven state advancements, and the integer count of time-stepped updates. Additionally, it must output the list of event-driven spike times in seconds as floats.\n\nTest suite and parameters:\nUse the following five test cases, all with units explicitly stated:\n- Case $1$ (general case with mixed excitation and inhibition):\n    - $\\tau = 0.02$ s, $V_{\\mathrm{leak}} = -0.065$ V, $V_{\\mathrm{th}} = -0.050$ V, $V_{\\mathrm{reset}} = -0.065$ V, $t_{\\mathrm{ref}} = 0.005$ s, $T_{\\mathrm{end}} = 0.1$ s, $\\Delta t = 0.0001$ s.\n    - Events: $(t,w)$ pairs in seconds and volts: $(0.010, 0.012)$, $(0.030, 0.015)$, $(0.032, -0.010)$, $(0.070, 0.020)$.\n- Case $2$ (no synaptic events):\n    - Same parameters as Case $1$ except events list is empty and $T_{\\mathrm{end}} = 0.1$ s, $\\Delta t = 0.0001$ s.\n- Case $3$ (simultaneous events aggregated at one time):\n    - Same parameters as Case $1$.\n    - Events: $(0.050, 0.010)$, $(0.050, 0.011)$, $(0.050, 0.010)$.\n- Case $4$ (net inhibition suppresses spiking):\n    - Same parameters as Case $1$.\n    - Events: $(0.010, 0.010)$, $(0.015, -0.015)$, $(0.040, 0.008)$.\n- Case $5$ (event inside refractory is ignored):\n    - Same parameters as Case $1$.\n    - Events: $(0.010, 0.020)$, $(0.012, 0.005)$, $(0.020, 0.020)$.\n\nAnswer specification:\n- For each test case, compute a list of the form $[S_{\\mathrm{event}}, S_{\\mathrm{time}}, C_{\\mathrm{event}}, C_{\\mathrm{time}}, \\text{spike\\_times\\_event}]$, where $S_{\\mathrm{event}}$ is the integer number of spikes in the event-driven simulator, $S_{\\mathrm{time}}$ is the integer number of spikes in the time-stepped simulator, $C_{\\mathrm{event}} = K_{\\mathrm{agg}} + S_{\\mathrm{event}}$ is the integer count of event-driven state advancements, $C_{\\mathrm{time}} = N$ is the integer count of time-stepped updates, and $\\text{spike\\_times\\_event}$ is the list of event-driven spike times in seconds as floats.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for all five test cases in order, for example: $[[\\cdots],[\\cdots],[\\cdots],[\\cdots],[\\cdots]]$.",
            "solution": "The problem requires the design, analysis, and implementation of two simulators for a Leaky Integrate-and-Fire (LIF) neuron: an event-driven simulator and a time-stepped simulator. This analysis will culminate in a computational comparison based on a provided test suite.\n\nThe dynamics of the LIF neuron's membrane potential, $V(t)$, are governed by the ordinary differential equation:\n$$\n\\frac{dV}{dt} = -\\frac{V(t) - V_{\\mathrm{leak}}}{\\tau} + I_{\\mathrm{syn}}(t)\n$$\nHere, $\\tau$ is the membrane time constant, $V_{\\mathrm{leak}}$ is the leak reversal potential, and $I_{\\mathrm{syn}}(t)$ represents the synaptic input current. The input is modeled as a series of instantaneous impulses arriving at times $t_k$ with weights $w_k$:\n$$\nI_{\\mathrm{syn}}(t) = \\sum_{k=1}^{K} w_k \\,\\delta(t - t_k)\n$$\nIntegrating this current over an infinitesimally small time interval around $t_k$ results in an instantaneous jump in membrane potential of magnitude $w_k$. The neuron's state is initialized at $V(0) = V_{\\mathrm{leak}}$.\n\nA spike is generated at time $t$ if $V(t)$ reaches or exceeds the threshold $V_{\\mathrm{th}}$. Upon spiking, the potential is reset to $V_{\\mathrm{reset}}$ and clamped at this value for an absolute refractory period of duration $t_{\\mathrm{ref}}$. During this period, all synaptic inputs are ignored.\n\n**Event-Driven Simulator Design**\n\nThe core principle of the event-driven simulator is to leverage the analytical solution of the LIF dynamics in the absence of input. Between any two consecutive events (synaptic inputs or spikes), the input current term $I_{\\mathrm{syn}}(t)$ is zero. The governing equation becomes a homogeneous linear first-order ODE:\n$$\n\\frac{dV}{dt} = -\\frac{V(t) - V_{\\mathrm{leak}}}{\\tau}\n$$\nGiven the potential $V(t_0)$ at a time $t_0$, the solution for any subsequent time $t  t_0$ (before the next event) is:\n$$\nV(t) = V_{\\mathrm{leak}} + \\big(V(t_0) - V_{\\mathrm{leak}}\\big)\\, e^{-\\frac{t - t_0}{\\tau}}\n$$\nThis analytical solution allows the simulator to \"jump\" in time from one event to the next, calculating the state precisely at the next event time without simulating the intervening interval. State updates are only necessary at times when the system's dynamics change discontinuously, which are the synaptic event times.\n\nThe algorithm is as follows:\n1.  **Initialization**: Set the current time $t_{\\mathrm{curr}} = 0$, membrane potential $V_{\\mathrm{curr}} = V_{\\mathrm{leak}}$, and the time until which the neuron is refractory $t_{\\mathrm{ref\\_end}} = -\\infty$. Initialize an empty list for spike times.\n2.  **Event Aggregation**: Pre-process the list of synaptic events $(t_k, w_k)$. For all events occurring at the exact same time, their weights are summed. This yields a sorted list of unique, aggregated event times and their corresponding total weights. Let this list have $K_{\\mathrm{agg}}$ entries.\n3.  **Event Loop**: Iterate through each aggregated event $(t_{\\mathrm{event}}, w_{\\mathrm{event}})$ in chronological order.\n    a. **Refractory Check**: If $t_{\\mathrm{event}}  t_{\\mathrm{ref\\_end}}$, the neuron is in its absolute refractory period. The event is ignored.\n    b. **State Advancement**: If the event is not ignored, the neuron's potential must be advanced from the time of the last update to $t_{\\mathrm{event}}$. The period of decay starts at $t_{\\mathrm{decay\\_start}} = \\max(t_{\\mathrm{curr}}, t_{\\mathrm{ref\\_end}})$. The potential at the start of decay is $V_{\\mathrm{at\\_decay\\_start}}$, which is $V_{\\mathrm{reset}}$ if the neuron just exited a refractory period, or $V_{\\mathrm{curr}}$ otherwise. The potential just before the synaptic jump at $t_{\\mathrm{event}}$ is:\n       $$\n       V(t_{\\mathrm{event}}^-) = V_{\\mathrm{leak}} + \\big(V_{\\mathrm{at\\_decay\\_start}} - V_{\\mathrm{leak}}\\big)\\, e^{-\\frac{t_{\\mathrm{event}} - t_{\\mathrm{decay\\_start}}}{\\tau}}\n       $$\n    c. **Synaptic Integration**: The synaptic weight is added instantaneously: $V(t_{\\mathrm{event}}^+) = V(t_{\\mathrm{event}}^-) + w_{\\mathrm{event}}$.\n    d. **Spike Check**: If $V(t_{\\mathrm{event}}^+) \\ge V_{\\mathrm{th}}$, a spike is recorded at $t_{\\mathrm{event}}$. The potential is reset to $V_{\\mathrm{curr}} = V_{\\mathrm{reset}}$, and the refractory end time is updated to $t_{\\mathrm{ref\\_end}} = t_{\\mathrm{event}} + t_{\\mathrm{ref}}$.\n    e. **No-Spike Update**: If no spike occurs, the new potential is set to $V_{\\mathrmcurr}} = V(t_{\\mathrm{event}}^+)$.\n    f. **Time Update**: The current simulation time is updated to $t_{\\mathrm{curr}} = t_{\\mathrm{event}}$.\n\n**Time-Stepped Simulator Design**\n\nThe time-stepped simulator approximates the continuous dynamics using a numerical integration method on a discrete time grid. Here, we use the Forward Euler method with a fixed time step $\\Delta t$.\nThe discretized form of the ODE is:\n$$\nV_{i+1} = V_i + \\Delta t \\left( -\\frac{V_i - V_{\\mathrm{leak}}}{\\tau} \\right)\n$$\nwhere $V_i$ is the potential at time $t_i = i \\cdot \\Delta t$.\n\nThe algorithm is as follows:\n1.  **Initialization**: Set $t = 0$, $V = V_{\\mathrm{leak}}$, $t_{\\mathrm{ref\\_end}} = -\\infty$. Determine the total number of steps $N = \\lceil T_{\\mathrm{end}} / \\Delta t \\rceil$. Pre-process synaptic events by binning them into the time step intervals they belong to.\n2.  **Time Loop**: Iterate for $i$ from $0$ to $N-1$. Let the current time be $t = i \\cdot \\Delta t$.\n    a. **Refractory Check**: If $t  t_{\\mathrm{ref\\_end}}$, set $V = V_{\\mathrm{reset}}$ and proceed to the next time step.\n    b. **Synaptic Integration**: Sum the weights of all events that occur in the interval $[t, t+\\Delta t)$ and add this sum to $V$.\n    c. **Spike Check**: If $V \\ge V_{\\mathrm{th}}$, record a spike at time $t$, reset $V = V_{\\mathrm{reset}}$, update $t_{\\mathrm{ref\\_end}} = t + t_{\\mathrm{ref}}$, and proceed to the next time step.\n    d. **Euler Update**: If no spike occurred, update the potential using the Euler formula: $V \\leftarrow V + \\Delta t \\cdot \\left(-(V - V_{\\mathrm{leak}})/\\tau\\right)$.\n\n**Complexity Analysis**\n\nThe computational complexity is measured by the number of fundamental state advancement operations.\n\n-   **Event-Driven Simulator**: The algorithm's main loop iterates once for each of the $K_{\\mathrm{agg}}$ aggregated synaptic events. Inside the loop, a constant number of arithmetic operations are performed. The total number of spikes is $S$. According to the problem's definition, the complexity measure is the count of processed aggregated events plus the number of spikes, $C_{\\mathrm{event}} = K_{\\mathrm{agg}} + S$. The runtime of the simulation loop is therefore proportional to $K_{\\mathrm{agg}}$. Thus, the time complexity is $O(K_{\\mathrm{agg}} + S)$. The pre-sorting of $K$ initial events takes $O(K \\log K)$, but the simulation complexity itself scales with the number of unique event times, not the simulation duration or time-step resolution. This method performs no updates between events, as it analytically bridges the time gaps.\n\n-   **Time-Stepped Simulator**: The algorithm executes a loop a fixed number of times, $N = \\lceil T_{\\mathrm{end}}/\\Delta t \\rceil$. In each iteration, it performs a constant number of operations. The computational cost is independent of the number of synaptic events. The total number of state advancements is defined as the number of time steps, $C_{\\mathrm{time}} = N$. Therefore, the time complexity is $O(N)$.\n\n-   **Comparison**: When synaptic input is sparse, the number of events $K_{\\mathrm{agg}}$ is typically much smaller than the number of time steps $N$ required for an accurate simulation (i.e., $K_{\\mathrm{agg}} \\ll T_{\\mathrm{end}}/\\Delta t$). In this regime, the event-driven simulator is significantly more computationally efficient than the time-stepped simulator.",
            "answer": "[[1,1,5,1000,[0.07]],[0,0,0,1000,[]],[1,1,2,1000,[0.05]],[0,0,3,1000,[]],[2,2,5,1000,[0.01, 0.02]]]"
        }
    ]
}