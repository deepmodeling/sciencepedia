{
    "hands_on_practices": [
        {
            "introduction": "The McCulloch-Pitts neuron's defining feature is its all-or-none, hard-threshold activation. This practice delves into a critical and practical consequence of this design: the neuron's \"brittleness\" in the presence of noise. By deriving the precise conditions under which a single noisy input bit can flip the neuron's output, you will develop a quantitative understanding of the decision boundary and the stability of its computations .",
            "id": "4065167",
            "problem": "Consider a McCulloch-Pitts neuron with binary inputs, fixed synaptic weights, and a hard-threshold activation. Let the inputs be $x_{1}, x_{2}, \\dots, x_{n} \\in \\{0,1\\}$, the synaptic weights be $w_{1}, w_{2}, \\dots, w_{n} \\in \\mathbb{R}$, and the threshold be $\\theta \\in \\mathbb{R}$. Define the neuronâ€™s output as $y = H\\!\\left(\\sum_{i=1}^{n} w_{i} x_{i} - \\theta\\right)$, where $H(\\cdot)$ is the Heaviside step function with $H(z) = 1$ for $z \\ge 0$ and $H(z) = 0$ for $z  0$. In a single-bit noise model, one input coordinate $j \\in \\{1, \\dots, n\\}$ is selected uniformly at random and toggled $x_{j} \\mapsto 1 - x_{j}$.\n\nStarting only from these core definitions, explain conceptually why hard thresholding makes the neuron brittle with respect to small input perturbations near the threshold. Then, derive necessary and sufficient conditions, expressed in terms of the signed margin $m = \\sum_{i=1}^{n} w_{i} x_{i} - \\theta$, the weight $w_{j}$, and the bit value $x_{j}$, under which toggling a single bit $x_{j}$ flips the output $y$. Your derivation should not assume any particular values of $n$, $\\theta$, $w_{i}$, or $x_{i}$.\n\nFinally, apply your conditions to the specific instance with $n = 7$, weights $(w_{1}, \\dots, w_{7}) = (0.9, 1.6, -0.7, 1.1, 0.5, -1.2, 0.8)$, threshold $\\theta = 1.9$, and input $(x_{1}, \\dots, x_{7}) = (1, 1, 0, 1, 0, 1, 0)$. Under the single-bit noise model that toggles one uniformly random input coordinate, compute the exact probability that the output flips. Express your final probability as a reduced fraction. No rounding is required, and no units are involved.",
            "solution": "The problem requires a three-part analysis of a McCulloch-Pitts neuron: a conceptual explanation of its brittleness, a general derivation of conditions for an output flip under single-bit noise, and a specific calculation of the flip probability for a given instance.\n\nFirst, we address the conceptual question. The McCulloch-Pitts neuron's output is determined by the Heaviside step function, $y = H(m)$, where $m = \\sum_{i=1}^{n} w_{i} x_{i} - \\theta$ is the signed margin. The defining characteristic of the Heaviside function $H(z)$ is its discontinuity at $z=0$, where its value jumps from $0$ for $z  0$ to $1$ for $z \\ge 0$. This means that an infinitesimally small change in the argument $m$ that crosses the value $0$ (for instance, from an arbitrarily small negative value $-\\epsilon$ to an arbitrarily small non-negative value $\\delta$) results in a maximal change in the neuron's output, from $y=0$ to $y=1$. An input perturbation, such as toggling a single bit $x_j$, induces a change in the weighted sum $\\sum w_i x_i$ and thus a change in the margin $m$. If the neuron's initial state is such that its margin $m$ is very close to the threshold $0$ (i.e., $|m|$ is small), even a small input perturbation that produces a change in $m$ of magnitude greater than $|m|$ can flip the sign of the margin. This sign change forces the output to jump between $0$ and $1$. This high sensitivity of the output to small input perturbations when the neuron is operating near its threshold ($m \\approx 0$) is precisely what is meant by brittleness.\n\nNext, we derive the necessary and sufficient conditions for an output flip. Let the initial input vector be $\\mathbf{x} = (x_1, \\dots, x_n)$, the initial weighted sum be $S = \\sum_{i=1}^{n} w_{i} x_{i}$, and the initial margin be $m = S - \\theta$. The initial output is $y = H(m)$. When a single input bit $x_j$ is toggled to $1-x_j$, the new weighted sum $S'$ is calculated.\nIf $x_j=0$ is toggled to $1$, the new sum is $S' = S + w_j$. The new margin is $m' = m+w_j$.\nIf $x_j=1$ is toggled to $0$, the new sum is $S' = S - w_j$. The new margin is $m' = m-w_j$.\nAn output flip occurs if and only if the new output $y' = H(m')$ is different from the initial output $y = H(m)$. There are two possibilities for a flip:\n1.  The output flips from $y=0$ to $y'=1$. This requires the initial margin to be negative ($m  0$) and the new margin to be non-negative ($m' \\ge 0$).\n2.  The output flips from $y=1$ to $y'=0$. This requires the initial margin to be non-negative ($m \\ge 0$) and the new margin to be negative ($m'  0$).\n\nWe analyze these possibilities based on the initial value of the toggled bit $x_j$.\n\nCase 1: Toggling $x_j=0$. The new margin is $m' = m+w_j$.\nA flip occurs if ($m0$ and $m+w_j \\ge 0$) or ($m \\ge 0$ and $m+w_j  0$).\nFor the first condition ($y=0 \\to y'=1$), we have $-w_j \\le m  0$. This is only possible if $w_j0$.\nFor the second condition ($y=1 \\to y'=0$), we have $0 \\le m  -w_j$. This is only possible if $w_j0$.\n\nCase 2: Toggling $x_j=1$. The new margin is $m' = m-w_j$.\nA flip occurs if ($m0$ and $m-w_j \\ge 0$) or ($m \\ge 0$ and $m-w_j  0$).\nFor the first condition ($y=0 \\to y'=1$), we have $w_j \\le m  0$. This is only possible if $w_j0$.\nFor the second condition ($y=1 \\to y'=0$), we have $0 \\le m  w_j$. This is only possible if $w_j0$.\n\nThese four conditions are the necessary and sufficient conditions for an output flip when toggling bit $x_j$.\n\nFinally, we apply these conditions to the specific instance provided and compute the probability of a flip.\nThe parameters are: $n=7$, weights $(w_{1}, \\dots, w_{7}) = (0.9, 1.6, -0.7, 1.1, 0.5, -1.2, 0.8)$, threshold $\\theta = 1.9$, and input vector $\\mathbf{x} = (x_{1}, \\dots, x_{7}) = (1, 1, 0, 1, 0, 1, 0)$.\n\nFirst, we compute the initial state of the neuron.\nThe weighted sum is $S = \\sum_{i=1}^{7} w_{i} x_{i}$:\n$S = w_1 x_1 + w_2 x_2 + w_3 x_3 + w_4 x_4 + w_5 x_5 + w_6 x_6 + w_7 x_7$\n$S = (0.9)(1) + (1.6)(1) + (-0.7)(0) + (1.1)(1) + (0.5)(0) + (-1.2)(1) + (0.8)(0)$\n$S = 0.9 + 1.6 + 0 + 1.1 + 0 - 1.2 + 0 = 2.4$\nThe initial margin is $m = S - \\theta = 2.4 - 1.9 = 0.5$.\nSince $m = 0.5 \\ge 0$, the initial output is $y = H(0.5) = 1$.\n\nAn output flip can only occur if the new output is $y'=0$. This is the $y=1 \\to y'=0$ case. We now examine each input coordinate $j \\in \\{1, \\dots, 7\\}$ to determine if toggling it causes a flip.\n\nFor $j$ where $x_j=0$ (i.e., $j=3, 5, 7$):\nThe condition for a flip is $0 \\le m  -w_j$, which requires $w_j  0$.\n- For $j=3$: $x_3=0$, $w_3 = -0.7$. Here $w_3  0$. We check the condition: $0 \\le 0.5  -(-0.7) = 0.7$. This is true. Toggling $x_3$ causes a flip.\n- For $j=5$: $x_5=0$, $w_5 = 0.5$. Here $w_5$ is not negative, so the condition for a flip is not met. No flip.\n- For $j=7$: $x_7=0$, $w_7 = 0.8$. Here $w_7$ is not negative, so the condition for a flip is not met. No flip.\n\nFor $j$ where $x_j=1$ (i.e., $j=1, 2, 4, 6$):\nThe condition for a flip is $0 \\le m  w_j$, which requires $w_j  0$.\n- For $j=1$: $x_1=1$, $w_1 = 0.9$. Here $w_1  0$. We check the condition: $0 \\le 0.5  0.9$. This is true. Toggling $x_1$ causes a flip.\n- For $j=2$: $x_2=1$, $w_2 = 1.6$. Here $w_2  0$. We check the condition: $0 \\le 0.5  1.6$. This is true. Toggling $x_2$ causes a flip.\n- For $j=4$: $x_4=1$, $w_4 = 1.1$. Here $w_4  0$. We check the condition: $0 \\le 0.5  1.1$. This is true. Toggling $x_4$ causes a flip.\n- For $j=6$: $x_6=1$, $w_6 = -1.2$. Here $w_6$ is not positive, so the condition for a flip is not met. No flip.\n\nIn summary, toggling the input bit causes the output to flip for the indices $j \\in \\{1, 2, 3, 4\\}$. There are $4$ such indices.\nThe single-bit noise model selects one input coordinate $j \\in \\{1, \\dots, 7\\}$ uniformly at random. The total number of possible choices is $n=7$. The number of choices that result in an output flip is $4$.\nThe probability of a flip is the ratio of the number of flip-inducing outcomes to the total number of outcomes:\n$P(\\text{flip}) = \\frac{\\text{Number of coordinates that cause a flip}}{\\text{Total number of coordinates}} = \\frac{4}{7}$.\nThis fraction is already in its most reduced form.",
            "answer": "$$\\boxed{\\frac{4}{7}}$$"
        },
        {
            "introduction": "While powerful, a single McCulloch-Pitts neuron is fundamentally limited to computing linearly separable functions. This exercise confronts this limitation directly by tasking you with the design of a network for the classic non-linearly separable parity function. In doing so, you will first construct a modular two-layer XOR gate and then assemble these modules into a larger hierarchical network, providing concrete experience with the principles of layered composition and the analysis of network complexity measures like depth and size .",
            "id": "4065070",
            "problem": "Consider the McCulloch-Pitts neuron model, in which a neuron computes a Boolean-valued output on Boolean inputs by applying a linear threshold rule: for inputs $x_{1},\\dots,x_{m} \\in \\{0,1\\}$, weights $w_{1},\\dots,w_{m} \\in \\mathbb{R}$, and threshold $\\theta \\in \\mathbb{R}$, the neuron outputs $1$ if and only if $\\sum_{i=1}^{m} w_{i} x_{i} \\ge \\theta$, and outputs $0$ otherwise. Negative weights (representing inhibition) are allowed. The exclusive OR (XOR) of two bits, denoted $\\mathrm{XOR}(x,y)$, is defined by $\\mathrm{XOR}(x,y)=1$ if exactly one of $x$ or $y$ is $1$, and $0$ otherwise. The $n$-bit parity function is defined by $\\mathrm{PARITY}_{n}(x_{1},\\dots,x_{n})=\\mathrm{XOR}(x_{1},\\mathrm{XOR}(x_{2},\\dots,\\mathrm{XOR}(x_{n-1},x_{n})\\dots))$, equivalently the sum $\\sum_{i=1}^{n} x_{i}$ modulo $2$.\n\nDesign a realization of $\\mathrm{XOR}(x,y)$ using a two-layer McCulloch-Pitts subnetwork composed of a hidden layer and an output layer, where the hidden layer may contain multiple neurons and the output layer contains a single neuron. Then, using a hierarchical balanced binary tree of these $\\mathrm{XOR}$ subnetworks to compute $\\mathrm{PARITY}_{n}$, define the network depth as the maximum number of neuron layers along any path from an input bit to the final output neuron, and define the network size as the total number of McCulloch-Pitts neurons used (counting all hidden and output neurons across the entire tree, and excluding input nodes). From first principles, derive closed-form expressions in terms of $n$ for the depth and the size of this parity network.\n\nProvide your final answer as two expressions arranged in a single row, with the first entry equal to the depth and the second entry equal to the size. No numerical approximation or rounding is required.",
            "solution": "The problem asks for the derivation of closed-form expressions for the network depth and size of a network that computes the $n$-bit parity function, $\\mathrm{PARITY}_{n}$. The network is to be constructed as a hierarchical balanced binary tree of two-layer McCulloch-Pitts (MP) subnetworks, each implementing the two-input exclusive OR ($\\mathrm{XOR}$) function.\n\nFirst, we must design a two-layer subnetwork for $\\mathrm{XOR}(x_1, x_2)$ using MP neurons. An MP neuron with inputs $x_1, \\dots, x_m \\in \\{0, 1\\}$, real-valued weights $w_1, \\dots, w_m$, and a real-valued threshold $\\theta$ produces an output $y=1$ if the weighted sum of its inputs meets or exceeds the threshold, i.e., $\\sum_{i=1}^{m} w_i x_i \\ge \\theta$, and $y=0$ otherwise.\n\nThe $\\mathrm{XOR}$ function is not linearly separable, meaning a single MP neuron cannot implement it. Therefore, a multi-layer network is necessary. The problem specifies a two-layer network, consisting of a hidden layer and an output layer. We can construct an $\\mathrm{XOR}$ gate by decomposing the logical function. One such decomposition is $\\mathrm{XOR}(x_1, x_2) = (x_1 \\lor x_2) \\land \\neg(x_1 \\land x_2)$. We can assign one hidden neuron to compute the OR clause and another to compute the NAND clause ($\\neg(x_1 \\land x_2)$). The output neuron will then compute the AND of the outputs of these two hidden neurons.\n\nLet the inputs be $x_1, x_2 \\in \\{0, 1\\}$.\nLet the hidden layer consist of two neurons, $H_1$ and $H_2$, and the output layer consist of a single neuron, $O$.\n\n**1. Hidden Neuron $H_1$: Computes $x_1 \\lor x_2$.**\nThe function $x_1 \\lor x_2$ is $1$ if at least one input is $1$, and $0$ otherwise.\nWe need to find weights $w_{11}, w_{12}$ and a threshold $\\theta_1$ such that $w_{11}x_1 + w_{12}x_2 \\ge \\theta_1$ if and only if $x_1=1$ or $x_2=1$.\nLet's choose $w_{11}=1$ and $w_{12}=1$. The weighted sum is $x_1+x_2$.\n- If $(x_1, x_2)=(0,0)$, the sum is $0$.\n- If $(x_1, x_2)=(0,1)$ or $(1,0)$, the sum is $1$.\n- If $(x_1, x_2)=(1,1)$, the sum is $2$.\nTo separate the $(0,0)$ case from the others, the threshold $\\theta_1$ must satisfy $0  \\theta_1 \\le 1$. Let's choose $\\theta_1=1$.\nThus, $H_1$ is defined by weights $w_{11}=1, w_{12}=1$ and threshold $\\theta_1=1$.\n\n**2. Hidden Neuron $H_2$: Computes $\\mathrm{NAND}(x_1, x_2) = \\neg(x_1 \\land x_2)$.**\nThe function $\\mathrm{NAND}(x_1, x_2)$ is $0$ only if both inputs are $1$, and $1$ otherwise.\nUsing inhibitory (negative) weights is convenient here. Let's choose $w_{21}=-1$ and $w_{22}=-1$. The weighted sum is $-x_1-x_2$.\n- If $(x_1, x_2)=(0,0)$, the sum is $0$.\n- If $(x_1, x_2)=(0,1)$ or $(1,0)$, the sum is $-1$.\n- If $(x_1, x_2)=(1,1)$, the sum is $-2$.\nWe want the neuron to fire for all cases except $(1,1)$. So, the condition $w_{21}x_1 + w_{22}x_2 \\ge \\theta_2$ must be true for sums of $0$ and $-1$, but false for a sum of $-2$. This requires the threshold $\\theta_2$ to satisfy $-2  \\theta_2 \\le -1$. Let's choose $\\theta_2=-1$.\nThus, $H_2$ is defined by weights $w_{21}=-1, w_{22}=-1$ and threshold $\\theta_2=-1$.\n\n**3. Output Neuron $O$: Computes $h_1 \\land h_2$.**\nThe output neuron $O$ takes the outputs of the hidden neurons, $h_1$ and $h_2$, as its inputs. It must compute $h_1 \\land h_2$ to realize the final $\\mathrm{XOR}$ function.\nLet's analyze the inputs to $O$:\n- If $(x_1, x_2)=(0,0)$, then $h_1=0$ (OR is false), $h_2=1$ (NAND is true). Final output should be $0$.\n- If $(x_1, x_2)=(0,1)$, then $h_1=1$, $h_2=1$. Final output should be $1$.\n- If $(x_1, x_2)=(1,0)$, then $h_1=1$, $h_2=1$. Final output should be $1$.\n- If $(x_1, x_2)=(1,1)$, then $h_1=1$, $h_2=0$. Final output should be $0$.\nNeuron $O$ must output $1$ only when its inputs $(h_1, h_2)$ are $(1,1)$.\nLet the weights for $O$ be $w_{o1}=1, w_{o2}=1$. The weighted sum is $h_1+h_2$.\n- If $(h_1, h_2)=(0,1)$ or $(1,0)$, the sum is $1$.\n- If $(h_1, h_2)=(1,1)$, the sum is $2$.\nTo fire only for $(1,1)$, the threshold $\\theta_o$ must satisfy $1  \\theta_o \\le 2$. Let's choose $\\theta_o=2$.\nThus, $O$ is defined by weights $w_{o1}=1, w_{o2}=1$ and threshold $\\theta_o=2$.\n\nThis completes the design of the $\\mathrm{XOR}$ subnetwork.\n- **Size of an $\\mathrm{XOR}$ subnetwork**: It contains $2$ hidden neurons and $1$ output neuron, for a total of $3$ neurons.\n- **Depth of an $\\mathrm{XOR}$ subnetwork**: The signal path flows from inputs through the hidden layer (first layer of neurons) to the output layer (second layer of neurons). Thus, the depth is $2$.\n\nNext, we analyze the entire $\\mathrm{PARITY}_{n}$ network, built as a \"hierarchical balanced binary tree\" of these $\\mathrm{XOR}$ subnetworks.\n\n**Derivation of Network Size:**\nThe function $\\mathrm{PARITY}_{n}(x_1, \\dots, x_n)$ is equivalent to $x_1 \\oplus x_2 \\oplus \\dots \\oplus x_n$, where $\\oplus$ denotes the $\\mathrm{XOR}$ operation. Computing an expression with $n-1$ binary operators requires exactly $n-1$ applications of the operator. A balanced binary tree is one way to arrange these operations. For example, for $n=4$, we compute $\\mathrm{XOR}(\\mathrm{XOR}(x_1, x_2), \\mathrm{XOR}(x_3, x_4))$. This uses $3$ (which is $4-1$) $\\mathrm{XOR}$ operations. For any $n$, such a tree structure will require $n-1$ two-input $\\mathrm{XOR}$ functions.\nSince each $\\mathrm{XOR}$ subnetwork is composed of $3$ neurons, the total number of neurons in the $\\mathrm{PARITY}_{n}$ network (its size) is:\n$$ \\text{Size} = (\\text{Number of XOR subnetworks}) \\times (\\text{Neurons per subnetwork}) = (n-1) \\times 3 = 3(n-1) $$\n\n**Derivation of Network Depth:**\nThe depth is the maximum number of neuron layers along any path from an input to the final output. The network is a tree of $\\mathrm{XOR}$ blocks. We first need to find the number of levels of these blocks in the tree.\nThe first level of $\\mathrm{XOR}$ blocks takes $n$ inputs and produces $\\lceil n/2 \\rceil$ outputs. The second level takes these $\\lceil n/2 \\rceil$ signals as inputs and produces $\\lceil (\\lceil n/2 \\rceil)/2 \\rceil$ outputs, and so on. This process continues until a single output remains.\nThe number of levels in this tree of blocks, let's call it $L$, is the number of times we must apply the function $f(k)=\\lceil k/2 \\rceil$ starting from $k=n$ until we reach $1$. This is equivalent to the definition of the ceiling of the base-$2$ logarithm.\nFor example, if $n=8$, inputs are reduced as $8 \\to 4 \\to 2 \\to 1$ ($3$ levels). $\\lceil \\log_2 8 \\rceil = 3$.\nIf $n=7$, inputs are reduced as $7 \\to 4 \\to 2 \\to 1$ ($3$ levels). $\\lceil \\log_2 7 \\rceil = 3$.\nSo, the number of levels of $\\mathrm{XOR}$ subnetworks is $L = \\lceil \\log_2 n \\rceil$.\nThe longest path from an initial input bit to the final output will pass through one $\\mathrm{XOR}$ subnetwork at each of these $L$ levels. Since each $\\mathrm{XOR}$ subnetwork has a depth of $2$ neuron layers, the total depth of the $\\mathrm{PARITY}_{n}$ network is:\n$$ \\text{Depth} = (\\text{Number of block levels}) \\times (\\text{Depth per block}) = \\lceil \\log_2 n \\rceil \\times 2 = 2 \\lceil \\log_2 n \\rceil $$\nNote that for the edge case $n=1$, $\\mathrm{PARITY}_1(x_1)=x_1$, which requires $0$ neurons and has a depth of $0$. Our formulas give Size$=3(1-1)=0$ and Depth$=2\\lceil\\log_2 1\\rceil=0$, which are consistent.\n\nThe derived closed-form expressions are $2 \\lceil \\log_2 n \\rceil$ for the depth and $3(n-1)$ for the size.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n2 \\lceil \\log_2 n \\rceil  3(n-1)\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "When neurons are interconnected to form recurrent networks, their behavior unfolds over time, creating complex dynamics. This practice explores how these emergent dynamics are not just a function of the network's wiring diagram but are also critically dependent on the temporal rules governing neuron updates. By comparing synchronous and asynchronous update schemes in a simple recurrent circuit, you will analyze how different timing assumptions can lead to dramatically different stable states and basins of attraction, offering a foundational insight into the behavior of dynamic neural systems .",
            "id": "4065112",
            "problem": "Consider the McCulloch-Pitts neuron model, where each neuron has a binary state $x_i(t) \\in \\{0,1\\}$ and updates according to a weighted sum followed by a thresholding nonlinearity. The state update for neuron $i$ is given by $x_i(t+1) = H\\left(\\sum_{j} w_{ij} x_j(t) - \\theta_i\\right)$, where $H(\\cdot)$ is the Heaviside step function returning $1$ if its argument is greater than or equal to $0$ and $0$ otherwise, $w_{ij}$ is the weight from neuron $j$ to neuron $i$, and $\\theta_i$ is the threshold of neuron $i$. Construct a recurrent network of $n = 3$ McCulloch-Pitts neurons labeled $A$, $B$, and $C$ with a ring topology and copy dynamics in the following sense: neuron $A$ copies neuron $C$, neuron $B$ copies neuron $A$, and neuron $C$ copies neuron $B$. Formally specify the weight matrix $W$ and threshold vector $\\boldsymbol{\\theta}$ as $W = \\begin{bmatrix} 0  0  1 \\\\ 1  0  0 \\\\ 0  1  0 \\end{bmatrix}$ and $\\boldsymbol{\\theta} = \\begin{bmatrix} 0.5 \\\\ 0.5 \\\\ 0.5 \\end{bmatrix}$. This ensures that the update rule reduces to logical copying from the designated predecessor in the ring.\n\nDefine two update regimes:\n- Synchronous updates: all neurons update simultaneously according to the current network state $\\mathbf{x}(t) = (x_A(t), x_B(t), x_C(t))$, producing $\\mathbf{x}(t+1)$ in one step.\n- Asynchronous updates with a fixed sequential schedule: in one macro-step, perform three micro-steps in the fixed order $(A, B, C)$. At each micro-step, update the targeted neuron using the current state vector, immediately committing the new value before proceeding to the next neuron. The macro-step mapping $\\mathbf{x}(t) \\mapsto \\mathbf{x}(t+1)$ is the composition of these three micro-steps.\n\nFor each regime, consider the state graph whose vertices are all $2^3 = 8$ binary states $\\{0,1\\}^3$ and whose directed edges represent the single-step (for the synchronous regime) or single macro-step (for the asynchronous sequential regime) transitions induced by the network dynamics.\n\nStarting from the foundational definitions stated above, analyze how asynchronous updates lead to different limit cycles than synchronous updates in this network. Compute the limit cycles for both regimes and analyze their basins of attraction by constructing the full state transition function. Represent states as $3$-bit binary vectors $(x_A,x_B,x_C)$ encoded as integers in $[0,7]$ via $x_A \\cdot 4 + x_B \\cdot 2 + x_C \\cdot 1$.\n\nYour program must implement the exact specified network and update rules and produce the following outputs derived from the state graphs:\n- For each initial state in the test suite defined below, compute the length of the limit cycle reached under synchronous updates and under asynchronous sequential updates. The length of a limit cycle is the number of distinct states in the cycle. If a fixed point is reached, the length is $1$.\n- Compute the number of distinct limit cycles in the synchronous state graph and in the asynchronous sequential state graph, across all $8$ states.\n- Compute the multiset of basin sizes for the synchronous regime and for the asynchronous sequential regime. The basin size of a limit cycle is the number of states that eventually reach that cycle under repeated application of the respective update mapping.\n\nTest suite:\n- Use the initial states $\\mathbf{x}(0)$ equal to $(0,0,1)$, $(1,0,0)$, $(1,1,1)$, $(0,0,0)$, and $(0,1,1)$, encoded respectively as integers $1$, $4$, $7$, $0$, and $3$.\n- For each of these initial states, compute the synchronous and asynchronous sequential limit cycle lengths as described.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The line must contain the following items in order:\n- A list of $5$ integers giving the synchronous limit cycle lengths for the initial states in the order $[1,4,7,0,3]$.\n- A list of $5$ integers giving the asynchronous sequential limit cycle lengths for the initial states in the order $[1,4,7,0,3]$.\n- A boolean indicating whether the set of limit cycles differs between the two regimes across all $8$ states.\n- An integer giving the number of distinct synchronous limit cycles across all $8$ states.\n- An integer giving the number of distinct asynchronous sequential limit cycles across all $8$ states.\n- A list of integers giving the basin sizes of all distinct synchronous limit cycles, sorted in nondecreasing order.\n- A list of integers giving the basin sizes of all distinct asynchronous sequential limit cycles, sorted in nondecreasing order.\n\nYour program must not read any input and must not print any other text beyond the single specified line.",
            "solution": "The user wants to analyze a 3-neuron McCulloch-Pitts network and compare its dynamics under synchronous and asynchronous update regimes.\n\n### Step 1: Problem Validation\n\n**1.1. Extract Givens**\n- **Model**: McCulloch-Pitts neuron with state $x_i(t) \\in \\{0,1\\}$ and update rule $x_i(t+1) = H\\left(\\sum_{j} w_{ij} x_j(t) - \\theta_i\\right)$. $H(\\cdot)$ is the Heaviside step function: $H(z) = 1$ for $z \\ge 0$ and $H(z) = 0$ for $z  0$.\n- **Network**: $n=3$ neurons labeled $A$, $B$, $C$.\n- **Topology**: Ring with copy dynamics: $A$ copies $C$, $B$ copies $A$, $C$ copies $B$.\n- **Parameters**: Weight matrix $W = \\begin{bmatrix} 0  0  1 \\\\ 1  0  0 \\\\ 0  1  0 \\end{bmatrix}$ and threshold vector $\\boldsymbol{\\theta} = \\begin{bmatrix} 0.5 \\\\ 0.5 \\\\ 0.5 \\end{bmatrix}$.\n- **State Representation**: A state $\\mathbf{x} = (x_A, x_B, x_C)$ is encoded as an integer $k = x_A \\cdot 4 + x_B \\cdot 2 + x_C \\cdot 1$. The state space is $\\{0, 1, \\dots, 7\\}$.\n- **Update Regimes**:\n    1.  **Synchronous**: All neurons update simultaneously.\n    2.  **Asynchronous Sequential**: Neurons update in the fixed order $(A, B, C)$, with each update taking effect immediately.\n- **Tasks**:\n    1.  For each update regime, construct the state transition graph over the $8$ possible states.\n    2.  For a given test suite of initial states, compute the length of the limit cycle reached under each regime.\n    3.  Compute the number of distinct limit cycles for each regime.\n    4.  Compute the multiset of basin sizes for each regime.\n- **Test Suite**: Initial states encoded as integers $1, 4, 7, 0, 3$.\n- **Output Format**: A single line, comma-separated list enclosed in brackets, containing:\n    - List of synchronous cycle lengths for the test suite.\n    - List of asynchronous cycle lengths for the test suite.\n    - Boolean indicating if the sets of limit cycles differ.\n    - Number of synchronous limit cycles.\n    - Number of asynchronous limit cycles.\n    - Sorted list of synchronous basin sizes.\n    - Sorted list of asynchronous basin sizes.\n\n**1.2. Validate Using Extracted Givens**\n- **Scientific Grounding**: The problem is grounded in the well-established theory of artificial neural networks, specifically the McCulloch-Pitts model. The analysis of network dynamics under different update schemes (synchronous vs. asynchronous) is a fundamental topic in the study of recurrent networks and discrete dynamical systems.\n- **Well-Posedness**: The problem is fully specified. The neuron model, network parameters ($W, \\boldsymbol{\\theta}$), state space, and update rules are all defined unambiguously. The state space is finite, guaranteeing that the dynamics will eventually enter a limit cycle (which includes fixed points). All requested outputs are deterministic computations based on the specified dynamics.\n- **Objectivity**: The problem is stated in precise, mathematical terms, free of subjective or ambiguous language.\n\n**1.3. Verdict and Action**\nThe problem is scientifically sound, well-posed, and objective. It is **valid**. We proceed to the solution.\n\n### Step 2: Derivation and Solution\n\nThe state vector is $\\mathbf{x}(t) = (x_A(t), x_B(t), x_C(t))$. The update rule for a neuron $i$ is $x_i(t+1) = H(\\mathbf{w}_i \\cdot \\mathbf{x}(t) - \\theta_i)$, where $\\mathbf{w}_i$ is the $i$-th row of $W$.\n\nLet's verify the \"copy\" dynamic with $\\theta_i = 0.5$ for all $i$.\n- Neuron $A$ (index $1$): The input is $\\sum_j w_{Aj} x_j - \\theta_A = 1 \\cdot x_C - 0.5$. The output $x_A(t+1) = H(x_C(t) - 0.5)$ is $1$ if $x_C(t)=1$ and $0$ if $x_C(t)=0$. Thus, $x_A(t+1) = x_C(t)$.\n- Neuron $B$ (index $2$): The input is $\\sum_j w_{Bj} x_j - \\theta_B = 1 \\cdot x_A - 0.5$. The output $x_B(t+1) = H(x_A(t) - 0.5)$ is $1$ if $x_A(t)=1$ and $0$ if $x_A(t)=0$. Thus, $x_B(t+1) = x_A(t)$.\n- Neuron $C$ (index $3$): The input is $\\sum_j w_{Cj} x_j - \\theta_C = 1 \\cdot x_B - 0.5$. The output $x_C(t+1) = H(x_B(t) - 0.5)$ is $1$ if $x_B(t)=1$ and $0$ if $x_B(t)=0$. Thus, $x_C(t+1) = x_B(t)$.\n\nThe parameters correctly implement the described copy dynamics. We now analyze the two update regimes.\n\n**2.1. Synchronous Update Regime**\nIn this regime, all neurons update simultaneously based on the state at time $t$. The new state $\\mathbf{x}(t+1) = (x_A(t+1), x_B(t+1), x_C(t+1))$ is given by:\n$$ x_A(t+1) = x_C(t) $$\n$$ x_B(t+1) = x_A(t) $$\n$$ x_C(t+1) = x_B(t) $$\nSo, the state vector transformation is $\\mathbf{x}(t+1) = (x_C(t), x_A(t), x_B(t))$. This is a permutation of the components of $\\mathbf{x}(t)$. We construct the state transition graph for all $2^3=8$ states, using the integer encoding $k = 4x_A + 2x_B + 1x_C$.\n\n- $0=(0,0,0) \\to (0,0,0)=0$\n- $1=(0,0,1) \\to (1,0,0)=4$\n- $2=(0,1,0) \\to (0,0,1)=1$\n- $3=(0,1,1) \\to (1,0,1)=5$\n- $4=(1,0,0) \\to (0,1,0)=2$\n- $5=(1,0,1) \\to (1,1,0)=6$\n- $6=(1,1,0) \\to (0,1,1)=3$\n- $7=(1,1,1) \\to (1,1,1)=7$\n\nThe full state transition mapping is $T_{sync}: \\{0 \\to 0, 1 \\to 4, 2 \\to 1, 3 \\to 5, 4 \\to 2, 5 \\to 6, 6 \\to 3, 7 \\to 7\\}$.\nThe dynamics partition the state space into the following disjoint sets:\n- **Fixed point:** $\\{0\\}$. This is a limit cycle of length $1$. Its basin of attraction is $\\{0\\}$. Size: $1$.\n- **Fixed point:** $\\{7\\}$. This is a limit cycle of length $1$. Its basin of attraction is $\\{7\\}$. Size: $1$.\n- **Cycle:** $1 \\to 4 \\to 2 \\to 1$. This is a limit cycle of length $3$. Its basin of attraction is $\\{1, 2, 4\\}$. Size: $3$.\n- **Cycle:** $3 \\to 5 \\to 6 \\to 3$. This is a limit cycle of length $3$. Its basin of attraction is $\\{3, 5, 6\\}$. Size: $3$.\n\n**2.2. Asynchronous Sequential Update Regime**\nIn this regime, neurons are updated in the fixed order $(A, B, C)$. Let the state at time $t$ be $(x_A, x_B, x_C)$. The macro-step consists of three micro-steps:\n1.  Update $A$: $x'_A = x_C$. The state becomes $(x'_A, x_B, x_C)$.\n2.  Update $B$: $x'_B = x'_A$. The state becomes $(x'_A, x'_B, x_C)$.\n3.  Update $C$: $x'_C = x'_B$. The state becomes $(x'_A, x'_B, x'_C)$.\n\nThe final state after one macro-step is $\\mathbf{x}(t+1) = (x'_A, x'_B, x'_C)$. By substitution:\n$$ x_A(t+1) = x_C(t) $$\n$$ x_B(t+1) = x_A(t+1) = x_C(t) $$\n$$ x_C(t+1) = x_B(t+1) = x_C(t) $$\nThe transformation is $\\mathbf{x}(t+1) = (x_C(t), x_C(t), x_C(t))$. The next state is determined entirely by the current state of neuron $C$. If $x_C(t)=0$, the next state is $(0,0,0)=0$. If $x_C(t)=1$, the next state is $(1,1,1)=7$.\n\nThe state transition graph is:\n- States with $x_C=0$: $\\{0, 2, 4, 6\\}$. All transition to state $0$.\n  - $0=(0,0,0) \\to (0,0,0)=0$\n  - $2=(0,1,0) \\to (0,0,0)=0$\n  - $4=(1,0,0) \\to (0,0,0)=0$\n  - $6=(1,1,0) \\to (0,0,0)=0$\n- States with $x_C=1$: $\\{1, 3, 5, 7\\}$. All transition to state $7$.\n  - $1=(0,0,1) \\to (1,1,1)=7$\n  - $3=(0,1,1) \\to (1,1,1)=7$\n  - $5=(1,0,1) \\to (1,1,1)=7$\n  - $7=(1,1,1) \\to (1,1,1)=7$\n\nThe full state transition mapping is $T_{async}: \\{0 \\to 0, 1 \\to 7, 2 \\to 0, 3 \\to 7, 4 \\to 0, 5 \\to 7, 6 \\to 0, 7 \\to 7\\}$.\nThe dynamics partition the state space into two sets:\n- **Fixed point:** $\\{0\\}$. This is a limit cycle of length $1$. Its basin of attraction is $\\{0, 2, 4, 6\\}$. Size: $4$.\n- **Fixed point:** $\\{7\\}$. This is a limit cycle of length $1$. Its basin of attraction is $\\{1, 3, 5, 7\\}$. Size: $4$.\n\n**2.3. Calculation of Final Results**\nWe now compute the specific quantities required by the problem statement based on the analysis above.\nThe test suite consists of initial states $1, 4, 7, 0, 3$.\n\n- **Synchronous Limit Cycle Lengths**:\n  - State $1$: Is in cycle $\\{1, 4, 2\\}$. Length is $3$.\n  - State $4$: Is in cycle $\\{1, 4, 2\\}$. Length is $3$.\n  - State $7$: Is in cycle $\\{7\\}$. Length is $1$.\n  - State $0$: Is in cycle $\\{0\\}$. Length is $1$.\n  - State $3$: Is in cycle $\\{3, 5, 6\\}$. Length is $3$.\n  - Result: `[3, 3, 1, 1, 3]`\n\n- **Asynchronous Limit Cycle Lengths**:\n  - State $1$: Transitions to $7$, which is a fixed point. Length is $1$.\n  - State $4$: Transitions to $0$, which is a fixed point. Length is $1$.\n  - State $7$: Is the fixed point $\\{7\\}$. Length is $1$.\n  - State $0$: Is the fixed point $\\{0\\}$. Length is $1$.\n  - State $3$: Transitions to $7$, which is a fixed point. Length is $1$.\n  - Result: `[1, 1, 1, 1, 1]`\n\n- **Limit Cycles Differ**: The set of synchronous cycles is $\\{\\{0\\}, \\{7\\}, \\{1, 2, 4\\}, \\{3, 5, 6\\}\\}$. The set of asynchronous cycles is $\\{\\{0\\}, \\{7\\}\\}$. These sets are different. Result: `True`.\n\n- **Number of Distinct Limit Cycles**:\n  - Synchronous: $4$ cycles.\n  - Asynchronous: $2$ cycles.\n\n- **Basin Sizes (sorted)**:\n  - Synchronous: The basins have sizes $1, 1, 3, 3$. Result: `[1, 1, 3, 3]`.\n  - Asynchronous: The basins have sizes $4, 4$. Result: `[4, 4]`.\n\nThese results will be computed and formatted by the program.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ...\n\ndef solve():\n    \"\"\"\n    Analyzes the dynamics of a 3-neuron McCulloch-Pitts network\n    under synchronous and asynchronous update regimes.\n    \"\"\"\n    \n    num_states = 8\n    \n    # --- Compute state transition maps ---\n    \n    # Synchronous updates: x(t+1) = (xC(t), xA(t), xB(t))\n    next_state_sync = np.zeros(num_states, dtype=int)\n    for i in range(num_states):\n        x_a = (i >> 2)  1\n        x_b = (i >> 1)  1\n        x_c = i  1\n        \n        next_x_a = x_c\n        next_x_b = x_a\n        next_x_c = x_b\n        \n        next_state_sync[i] = (next_x_a  2) | (next_x_b  1) | next_x_c\n\n    # Asynchronous sequential updates (A, B, C): x(t+1) = (xC(t), xC(t), xC(t))\n    next_state_async = np.zeros(num_states, dtype=int)\n    for i in range(num_states):\n        x_c = i  1\n        \n        # The next state is (xC, xC, xC)\n        if x_c == 1:\n            next_state_async[i] = 7  # (1, 1, 1)\n        else:\n            next_state_async[i] = 0  # (0, 0, 0)\n            \n    def find_dynamics(transitions):\n        \"\"\"\n        Analyzes a state transition map to find limit cycles and basins of attraction.\n        \n        Args:\n            transitions (np.ndarray): An array where transitions[i] is the next state from i.\n\n        Returns:\n            tuple: A tuple containing:\n                - state_to_cycle (dict): Maps each state to the frozenset representing its limit cycle.\n                - basins (dict): Maps each cycle (frozenset) to its basin size.\n        \"\"\"\n        n_states = len(transitions)\n        visited = [False] * n_states\n        state_to_cycle = {}\n        basins = {}\n\n        for i in range(n_states):\n            if visited[i]:\n                continue\n\n            path = []\n            path_indices = {}\n            current_state = i\n            \n            while current_state not in path_indices:\n                if visited[current_state]: # Path has merged into an already found basin\n                    # Find the cycle this path leads to\n                    cycle_set = state_to_cycle[current_state]\n                    break\n                \n                path.append(current_state)\n                path_indices[current_state] = len(path) - 1\n                current_state = transitions[current_state]\n            else: # Loop completed without break, new cycle found\n                cycle_start_index = path_indices[current_state]\n                cycle_states = path[cycle_start_index:]\n                cycle_set = frozenset(cycle_states)\n\n            # Update basin info for all states in the traced path\n            if cycle_set not in basins:\n                basins[cycle_set] = 0\n\n            for state_in_path in path:\n                if not visited[state_in_path]:\n                    visited[state_in_path] = True\n                    state_to_cycle[state_in_path] = cycle_set\n                    basins[cycle_set] += 1\n        \n        return state_to_cycle, basins\n\n    # --- Analyze both regimes ---\n    state_to_cycle_sync, basins_sync = find_dynamics(next_state_sync)\n    state_to_cycle_async, basins_async = find_dynamics(next_state_async)\n    \n    # --- Assemble results for output ---\n    test_suite = [1, 4, 7, 0, 3]\n\n    # 1. Synchronous limit cycle lengths for test suite\n    sync_cycle_lengths = [len(state_to_cycle_sync[s]) for s in test_suite]\n\n    # 2. Asynchronous limit cycle lengths for test suite\n    async_cycle_lengths = [len(state_to_cycle_async[s]) for s in test_suite]\n\n    # 3. Boolean indicating if the set of limit cycles differs\n    sync_cycles_set = set(basins_sync.keys())\n    async_cycles_set = set(basins_async.keys())\n    cycles_differ = sync_cycles_set != async_cycles_set\n\n    # 4. Number of distinct synchronous limit cycles\n    num_sync_cycles = len(sync_cycles_set)\n\n    # 5. Number of distinct asynchronous sequential limit cycles\n    num_async_cycles = len(async_cycles_set)\n    \n    # 6. Sorted list of synchronous basin sizes\n    sync_basin_sizes = sorted(list(basins_sync.values()))\n\n    # 7. Sorted list of asynchronous sequential limit cycles\n    async_basin_sizes = sorted(list(basins_async.values()))\n\n    # --- Format final output ---\n    final_results = [\n        sync_cycle_lengths,\n        async_cycle_lengths,\n        cycles_differ,\n        num_sync_cycles,\n        num_async_cycles,\n        sync_basin_sizes,\n        async_basin_sizes\n    ]\n    \n    # Format to a comma-separated list of items enclosed in brackets, with no spaces\n    output_str = str(final_results).replace(\" \", \"\")\n    \n    print(output_str)\n\nsolve()\n```"
        }
    ]
}