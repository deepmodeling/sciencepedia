{
    "hands_on_practices": [
        {
            "introduction": "理论神经科学的一个核心问题是，神经元群体的物理特性如何决定其编码信息的精确度。通过计算费雪信息（Fisher Information），我们可以为任何无偏解码器的性能设定一个理论上的下限，即克拉默-拉奥下限（Cramér-Rao Lower Bound）。这个练习  旨在通过一个理想化的模型——具有高斯调谐曲线和独立泊松噪声的神经元群体——来推导解码精度的基本极限。完成这个练习将帮助你掌握如何将信息论概念应用于具体的神经编码模型，并揭示神经元数量、发放率和调谐宽度等参数如何共同影响群体编码的效率。",
            "id": "4055130",
            "problem": "考虑一个一维刺激 $s \\in \\mathbb{R}$，在一个神经形态系统中由一个包含 $N$ 个神经元的群体进行编码，每个神经元都具有独立的泊松尖峰发放和相同的高斯调谐特性。在一个固定的观测窗口内，神经元 $i$ 的平均尖峰计数为 $\\lambda_i(s) = \\alpha \\exp\\!\\big(-\\frac{(s - \\mu_i)^{2}}{2 \\sigma^{2}}\\big)$，其中 $\\alpha  0$ 是峰值平均计数，$\\sigma  0$ 是调谐宽度，$\\mu_i \\in \\mathbb{R}$ 是神经元 $i$ 的偏好刺激。假设偏好刺激是密集平铺的：集合 $\\{\\mu_i\\}_{i=1}^{N}$ 在一个长度为 $L  0$ 的区间上均匀分布，间距为 $\\Delta = \\frac{L}{N}$，因此密度为 $\\rho = \\frac{N}{L}$。尖峰计数 $n_i$ 是独立的，且 $n_i \\sim \\text{Poisson}(\\lambda_i(s))$。\n\n重点关注使用最优线性估计器 (OLE) 对固定工作点 $s_0 \\in \\mathbb{R}$ 周围 $s$ 的微小扰动进行解码。OLE 定义为观测计数的线性函数，它在 $s_0$ 处局部无偏，并且在泊松噪声模型下，在 $(s - s_0)$ 的一阶上最小化均方误差。从独立泊松可变性的基本原理和调谐曲线在 $s_0$ 周围的一阶（局部）线性化出发，在密集平铺假设下，推导当 $N \\to \\infty$ 时 $s$ 的OLE估计的渐近方差。您的最终表达式必须用 $\\alpha$、$\\sigma$、$L$ 和 $N$ 来表示。\n\n将最终方差表示为单一的闭式解析表达式。无需进行数值四舍五入。将您的最终结果表示为以刺激变量 $s$ 为单位的方差，但不要在最终答案的方框中包含单位。",
            "solution": "该问题要求推导刺激 $s$ 的最优线性估计器 (OLE) 的渐近方差。局部无偏且方差最小的 OLE 的方差由 Cramér-Rao 下界 (CRLB) 给出，该下界是费雪信息 (Fisher Information) $J(s)$ 的倒数。我们将计算指定神经元群体的费雪信息，然后求其倒数以找到方差。\n\n该模型由 $N$ 个具有独立泊松尖峰发放的神经元组成。神经元 $i$ 的平均尖峰计数由高斯调谐曲线给出：\n$$\n\\lambda_i(s) = \\alpha \\exp\\left(-\\frac{(s - \\mu_i)^{2}}{2 \\sigma^{2}}\\right)\n$$\n其中 $\\alpha$ 是峰值平均计数，$\\sigma$ 是调谐宽度，$\\mu_i$ 是偏好刺激。尖峰计数 $n_i$ 是独立的泊松变量，$n_i \\sim \\text{Poisson}(\\lambda_i(s))$。\n\n对于一个独立的泊松神经元群体，费雪信息 $J(s)$ 可以计算如下：\n$$\nJ(s) = \\sum_{i=1}^{N} \\frac{\\left(\\frac{d\\lambda_i(s)}{ds}\\right)^2}{\\lambda_i(s)}\n$$\n让我们将 $\\lambda_i(s)$ 关于 $s$ 的导数表示为 $\\lambda'_i(s)$。首先，我们计算这个导数：\n$$\n\\lambda'_i(s) = \\frac{d}{ds} \\left[ \\alpha \\exp\\left(-\\frac{(s - \\mu_i)^{2}}{2 \\sigma^{2}}\\right) \\right]\n$$\n使用链式法则，我们得到：\n$$\n\\lambda'_i(s) = \\alpha \\exp\\left(-\\frac{(s - \\mu_i)^{2}}{2 \\sigma^{2}}\\right) \\cdot \\left( -\\frac{2(s - \\mu_i)}{2\\sigma^2} \\right) = -\\frac{s - \\mu_i}{\\sigma^2} \\lambda_i(s)\n$$\n现在，我们可以计算费雪信息求和中的项：\n$$\n\\frac{(\\lambda'_i(s))^2}{\\lambda_i(s)} = \\frac{\\left( -\\frac{s - \\mu_i}{\\sigma^2} \\lambda_i(s) \\right)^2}{\\lambda_i(s)} = \\frac{(s - \\mu_i)^2}{\\sigma^4} \\frac{\\lambda_i(s)^2}{\\lambda_i(s)} = \\frac{(s - \\mu_i)^2}{\\sigma^4} \\lambda_i(s)\n$$\n代入 $\\lambda_i(s)$ 的表达式：\n$$\n\\frac{(\\lambda'_i(s))^2}{\\lambda_i(s)} = \\frac{(s - \\mu_i)^2}{\\sigma^4} \\alpha \\exp\\left(-\\frac{(s - \\mu_i)^{2}}{2 \\sigma^{2}}\\right)\n$$\n在工作点 $s_0$ 的总费雪信息是所有 $N$ 个神经元的总和：\n$$\nJ(s_0) = \\sum_{i=1}^{N} \\frac{(s_0 - \\mu_i)^2}{\\sigma^4} \\alpha \\exp\\left(-\\frac{(s_0 - \\mu_i)^{2}}{2 \\sigma^{2}}\\right)\n$$\n问题指定了当 $N \\to \\infty$ 时的密集平铺假设。偏好刺激 $\\{\\mu_i\\}$ 在长度为 $L$ 的区间上均匀分布，密度为 $\\rho = \\frac{N}{L}$。在此极限下，离散和可以用变量 $\\mu$ 上的积分代替：\n$$\n\\sum_{i=1}^{N} f(\\mu_i) \\to \\int f(\\mu) \\rho \\, d\\mu\n$$\n将此应用于 $J(s_0)$ 的表达式：\n$$\nJ(s_0) = \\int \\rho \\, \\frac{(s_0 - \\mu)^2}{\\sigma^4} \\alpha \\exp\\left(-\\frac{(s_0 - \\mu)^{2}}{2 \\sigma^{2}}\\right) d\\mu\n$$\n积分范围是偏好刺激的范围，即一个长度为 $L$ 的区间。然而，由于当 $|\\mu - s_0| \\gg \\sigma$ 时高斯项会迅速衰减，如果我们假设刺激范围 $L$ 远大于调谐宽度 $\\sigma$，我们可以通过将积分限扩展到 $(-\\infty, \\infty)$ 来近似该积分，而不会产生显著误差。这是在这种情况下的一种标准做法。\n$$\nJ(s_0) \\approx \\rho \\alpha \\int_{-\\infty}^{\\infty} \\frac{(s_0 - \\mu)^2}{\\sigma^4} \\exp\\left(-\\frac{(s_0 - \\mu)^{2}}{2 \\sigma^{2}}\\right) d\\mu\n$$\n为了求解这个积分，我们进行变量替换。令 $x = \\frac{\\mu - s_0}{\\sigma}$。这意味着 $d\\mu = \\sigma dx$ 且 $(s_0 - \\mu)^2 = (-\\sigma x)^2 = \\sigma^2 x^2$。积分限保持不变。\n$$\nJ(s_0) \\approx \\rho \\alpha \\int_{-\\infty}^{\\infty} \\frac{\\sigma^2 x^2}{\\sigma^4} \\exp\\left(-\\frac{x^2}{2}\\right) (\\sigma \\, dx)\n$$\n$$\nJ(s_0) \\approx \\frac{\\rho \\alpha}{\\sigma} \\int_{-\\infty}^{\\infty} x^2 \\exp\\left(-\\frac{x^2}{2}\\right) dx\n$$\n积分 $\\int_{-\\infty}^{\\infty} x^2 \\exp\\left(-\\frac{x^2}{2}\\right) dx$ 是一个标准结果。它可以与标准正态分布 $N(0,1)$ 的方差相关联，该分布的概率密度函数为 $\\phi(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^2}{2}\\right)$。其方差为 $1$，由 $E[X^2] - (E[X])^2 = \\int_{-\\infty}^{\\infty} x^2 \\phi(x) dx - 0^2 = 1$ 给出。因此：\n$$\n\\int_{-\\infty}^{\\infty} x^2 \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^2}{2}\\right) dx = 1\n$$\n这意味着：\n$$\n\\int_{-\\infty}^{\\infty} x^2 \\exp\\left(-\\frac{x^2}{2}\\right) dx = \\sqrt{2\\pi}\n$$\n将此结果代回 $J(s_0)$ 的表达式中：\n$$\nJ(s_0) \\approx \\frac{\\rho \\alpha}{\\sigma} \\sqrt{2\\pi}\n$$\n问题要求最终表达式用 $N$ 和 $L$ 来表示。使用密度 $\\rho = \\frac{N}{L}$，我们有：\n$$\nJ(s_0) \\approx \\frac{N}{L} \\frac{\\alpha \\sqrt{2\\pi}}{\\sigma} = \\frac{\\sqrt{2\\pi} \\alpha N}{L \\sigma}\n$$\nOLE 的方差 $\\text{Var}(\\hat{s})$ 是费雪信息的倒数：\n$$\n\\text{Var}(\\hat{s}) = J(s_0)^{-1} \\approx \\left( \\frac{\\sqrt{2\\pi} \\alpha N}{L \\sigma} \\right)^{-1}\n$$\n这就给出了渐近方差的最终表达式：\n$$\n\\text{Var}(\\hat{s}) = \\frac{L \\sigma}{\\sqrt{2\\pi} \\alpha N}\n$$\n这个结果与工作点 $s_0$ 无关，这是偏好刺激的均匀密度以及在无穷域上对积分进行近似的结果。",
            "answer": "$$ \\boxed{ \\frac{L \\sigma}{\\sqrt{2\\pi} \\alpha N} } $$"
        },
        {
            "introduction": "在许多理论模型中，我们常常假设神经元噪声是独立的，以简化分析。然而，在真实的生物网络中，由于共享输入或局部回路连接，神经元之间的噪声往往存在相关性。这个练习  探讨了在这种更现实的场景下，如何构建一个最优的线性解码器。你将推导出在存在相关高斯噪声时，最优解码器如何利用噪声协方差矩阵的逆来“白化”响应，从而有效地整合来自不同神经元的信息。这项实践有助于你从理想化模型过渡到处理更复杂的、具有生物学合理性的统计结构，这是分析真实神经数据的一项关键技能。",
            "id": "4055126",
            "problem": "考虑一个由 $n$ 个神经元组成的群体编码，其对标量刺激 $s \\in \\mathbb{R}$ 的平均响应由调谐曲线 $\\mathbf{f}(s) \\in \\mathbb{R}^{n}$ 给出。观测到的群体响应 $\\mathbf{r} \\in \\mathbb{R}^{n}$ 服从概率模型 $\\mathbf{r} \\sim \\mathcal{N}(\\mathbf{f}(s),\\Sigma)$，其中 $\\Sigma \\in \\mathbb{R}^{n \\times n}$ 是一个与刺激无关的、正定的噪声协方差，神经元之间的噪声可能存在相关性。固定一个参考刺激 $s_{0}$，并假设在一个局部线性区间内，$\\mathbf{f}(s)$ 是可微的，并且可以用其在 $s_{0}$ 附近的一阶泰勒展开来近似，其梯度为 $\\mathbf{f}'(s_{0}) = \\frac{d\\mathbf{f}}{ds}\\big|_{s_{0}} \\in \\mathbb{R}^{n}$。考虑形式为 $\\hat{s} = a + \\mathbf{w}^{\\top}\\mathbf{r}$ 的线性估计器，其中 $a \\in \\mathbb{R}$ 且 $\\mathbf{w} \\in \\mathbb{R}^{n}$。在 $s_{0}$ 处施加局部无偏约束：$\\mathbb{E}[\\hat{s}\\mid s_{0}] = s_{0}$ 以及 $\\frac{d}{ds}\\mathbb{E}[\\hat{s}\\mid s]\\big|_{s_{0}} = 1$。在所有此类局部无偏线性估计器中，推导在相关高斯噪声 $\\mathcal{N}(\\mathbf{f}(s),\\Sigma)$ 下最小化 $\\mathrm{Var}(\\hat{s}\\mid s_{0})$ 的估计器。然后，将您的表达式特化到不相关情况 $\\Sigma = \\mathrm{diag}(\\sigma_{1}^{2},\\ldots,\\sigma_{n}^{2})$，其中对于 $i=1,\\ldots,n$，有 $\\sigma_{i}0$。将两种情况下的最优估计器在 $s_{0}$ 处的闭式解析表达式作为最终答案，用 $\\mathbf{f}(s_{0})$、$\\mathbf{f}'(s_{0})$、$\\Sigma$ 和 $\\mathbf{r}$ 表示。不需要进行数值计算或四舍五入。除已陈述的假设外，不要引入任何其他假设。为确保术语清晰，在提及该估计器时，请使用其显式表达式，而不要直接使用其名称“最佳线性无偏估计器”(BLUE)。",
            "solution": "用户希望基于神经元群体响应 $\\mathbf{r}$，找到标量刺激 $s$ 的最优线性估计器。该问题将通过首先将优化问题形式化，然后使用拉格朗日乘数法来解决。\n\n问题定义了一个形式为 $\\hat{s} = a + \\mathbf{w}^{\\top}\\mathbf{r}$ 的线性估计器，其中 $a \\in \\mathbb{R}$ 是一个标量偏移量，$\\mathbf{w} \\in \\mathbb{R}^{n}$ 是一个权重向量。群体响应 $\\mathbf{r}$ 是从一个多元高斯分布中抽取的随机向量，$\\mathbf{r} \\sim \\mathcal{N}(\\mathbf{f}(s),\\Sigma)$，其中 $\\mathbf{f}(s)$ 是平均神经响应（调谐曲线）的向量，$\\Sigma$ 是与刺激无关的噪声协方差矩阵。\n\n第一步是用估计器参数 $a$ 和 $\\mathbf{w}$ 来表示两个局部无偏约束。给定刺激 $s$ 时，估计器的期望值为：\n$$\n\\mathbb{E}[\\hat{s}\\mid s] = \\mathbb{E}[a + \\mathbf{w}^{\\top}\\mathbf{r}\\mid s] = a + \\mathbf{w}^{\\top}\\mathbb{E}[\\mathbf{r}\\mid s] = a + \\mathbf{w}^{\\top}\\mathbf{f}(s)\n$$\n第一个约束，即在参考刺激 $s_{0}$ 处的局部无偏性，为 $\\mathbb{E}[\\hat{s}\\mid s_{0}] = s_{0}$。将此应用于期望值的表达式，得到：\n$$\na + \\mathbf{w}^{\\top}\\mathbf{f}(s_{0}) = s_{0}\n$$\n这个约束使我们能够用 $\\mathbf{w}$ 表示 $a$：\n$$\na = s_{0} - \\mathbf{w}^{\\top}\\mathbf{f}(s_{0})\n$$\n第二个约束涉及期望值对刺激的导数，在 $s_0$ 处求值：$\\frac{d}{ds}\\mathbb{E}[\\hat{s}\\mid s]\\big|_{s_{0}} = 1$。我们首先计算导数：\n$$\n\\frac{d}{ds}\\mathbb{E}[\\hat{s}\\mid s] = \\frac{d}{ds} \\left( a + \\mathbf{w}^{\\top}\\mathbf{f}(s) \\right) = \\mathbf{w}^{\\top}\\frac{d\\mathbf{f}(s)}{ds} = \\mathbf{w}^{\\top}\\mathbf{f}'(s)\n$$\n在 $s=s_{0}$ 处求值并应用该约束，得到：\n$$\n\\mathbf{w}^{\\top}\\mathbf{f}'(s_{0}) = 1\n$$\n这是对权重向量 $\\mathbf{w}$ 的一个约束。\n\n接下来，我们确定要最小化的目标函数，即估计器在 $s_{0}$ 处的方差 $\\mathrm{Var}(\\hat{s}\\mid s_{0})$。\n$$\n\\mathrm{Var}(\\hat{s}\\mid s_{0}) = \\mathrm{Var}(a + \\mathbf{w}^{\\top}\\mathbf{r}\\mid s_{0}) = \\mathrm{Var}(\\mathbf{w}^{\\top}\\mathbf{r}\\mid s_{0})\n$$\n随机向量的线性变换的方差由 $\\mathbf{w}^{\\top}\\mathrm{Cov}(\\mathbf{r})\\mathbf{w}$ 给出。由于 $\\mathrm{Cov}(\\mathbf{r} \\mid s_{0}) = \\Sigma$，目标函数为：\n$$\n\\mathrm{Var}(\\hat{s}\\mid s_{0}) = \\mathbf{w}^{\\top}\\Sigma\\mathbf{w}\n$$\n现在问题是在线性约束 $\\mathbf{w}^{\\top}\\mathbf{f}'(s_{0}) = 1$ 下最小化 $\\mathbf{w}^{\\top}\\Sigma\\mathbf{w}$。在找到最优的 $\\mathbf{w}$ 之后，可以确定参数 $a$。这是一个约束优化问题，可以使用拉格朗日乘数 $\\lambda$ 来解决。我们定义拉格朗日函数 $\\mathcal{L}$：\n$$\n\\mathcal{L}(\\mathbf{w}, \\lambda) = \\mathbf{w}^{\\top}\\Sigma\\mathbf{w} - \\lambda(\\mathbf{w}^{\\top}\\mathbf{f}'(s_{0}) - 1)\n$$\n为了找到最小值，我们对 $\\mathcal{L}$ 关于 $\\mathbf{w}$ 求梯度，并将其设为零向量。协方差矩阵 $\\Sigma$ 是对称的。\n$$\n\\nabla_{\\mathbf{w}}\\mathcal{L} = 2\\Sigma\\mathbf{w} - \\lambda\\mathbf{f}'(s_{0}) = \\mathbf{0}\n$$\n这得到 $2\\Sigma\\mathbf{w} = \\lambda\\mathbf{f}'(s_{0})$。由于给定的 $\\Sigma$ 是正定的，所以它是可逆的。我们可以解出 $\\mathbf{w}$：\n$$\n\\mathbf{w} = \\frac{\\lambda}{2}\\Sigma^{-1}\\mathbf{f}'(s_{0})\n$$\n为了确定拉格朗日乘数 $\\lambda$，我们将这个关于 $\\mathbf{w}$ 的表达式代入约束方程 $\\mathbf{w}^{\\top}\\mathbf{f}'(s_{0}) = 1$ 中：\n$$\n\\left(\\frac{\\lambda}{2}\\Sigma^{-1}\\mathbf{f}'(s_{0})\\right)^{\\top}\\mathbf{f}'(s_{0}) = 1\n$$\n$$\n\\frac{\\lambda}{2}(\\mathbf{f}'(s_{0}))^{\\top}(\\Sigma^{-1})^{\\top}\\mathbf{f}'(s_{0}) = 1\n$$\n由于 $\\Sigma$ 是对称的，其逆矩阵 $\\Sigma^{-1}$ 也是对称的，即 $(\\Sigma^{-1})^{\\top} = \\Sigma^{-1}$。方程简化为：\n$$\n\\frac{\\lambda}{2}(\\mathbf{f}'(s_{0}))^{\\top}\\Sigma^{-1}\\mathbf{f}'(s_{0}) = 1\n$$\n解出 $\\lambda$：\n$$\n\\lambda = \\frac{2}{(\\mathbf{f}'(s_{0}))^{\\top}\\Sigma^{-1}\\mathbf{f}'(s_{0})}\n$$\n分母 $(\\mathbf{f}'(s_{0}))^{\\top}\\Sigma^{-1}\\mathbf{f}'(s_{0})$ 是刺激 $s$ 在 $s_{0}$ 处的费雪信息 (Fisher information)。由于 $\\Sigma^{-1}$ 是正定的，并且假设 $\\mathbf{f}'(s_{0}) \\neq \\mathbf{0}$，费雪信息为正，因此 $\\lambda$ 是良定义的。\n现在我们将 $\\lambda$ 代回 $\\mathbf{w}$ 的表达式中，以求得最优权重向量 $\\mathbf{w}_{\\text{opt}}$：\n$$\n\\mathbf{w}_{\\text{opt}} = \\frac{1}{2}\\left(\\frac{2}{(\\mathbf{f}'(s_{0}))^{\\top}\\Sigma^{-1}\\mathbf{f}'(s_{0})}\\right)\\Sigma^{-1}\\mathbf{f}'(s_{0}) = \\frac{\\Sigma^{-1}\\mathbf{f}'(s_{0})}{(\\mathbf{f}'(s_{0}))^{\\top}\\Sigma^{-1}\\mathbf{f}'(s_{0})}\n$$\n利用最优权重向量 $\\mathbf{w}_{\\text{opt}}$，我们求得最优偏移量 $a_{\\text{opt}}$：\n$$\na_{\\text{opt}} = s_{0} - \\mathbf{w}_{\\text{opt}}^{\\top}\\mathbf{f}(s_{0}) = s_{0} - \\frac{(\\Sigma^{-1}\\mathbf{f}'(s_{0}))^{\\top}\\mathbf{f}(s_{0})}{(\\mathbf{f}'(s_{0}))^{\\top}\\Sigma^{-1}\\mathbf{f}'(s_{0})} = s_{0} - \\frac{(\\mathbf{f}'(s_{0}))^{\\top}\\Sigma^{-1}\\mathbf{f}(s_{0})}{(\\mathbf{f}'(s_{0}))^{\\top}\\Sigma^{-1}\\mathbf{f}'(s_{0})}\n$$\n最后，我们构造最优估计器 $\\hat{s}_{\\text{opt}} = a_{\\text{opt}} + \\mathbf{w}_{\\text{opt}}^{\\top}\\mathbf{r}$：\n$$\n\\hat{s}_{\\text{opt}} = \\left(s_{0} - \\frac{(\\mathbf{f}'(s_{0}))^{\\top}\\Sigma^{-1}\\mathbf{f}(s_{0})}{(\\mathbf{f}'(s_{0}))^{\\top}\\Sigma^{-1}\\mathbf{f}'(s_{0})}\\right) + \\left(\\frac{\\Sigma^{-1}\\mathbf{f}'(s_{0})}{(\\mathbf{f}'(s_{0}))^{\\top}\\Sigma^{-1}\\mathbf{f}'(s_{0})}\\right)^{\\top}\\mathbf{r}\n$$\n$$\n\\hat{s}_{\\text{opt}} = s_{0} - \\frac{(\\mathbf{f}'(s_{0}))^{\\top}\\Sigma^{-1}\\mathbf{f}(s_{0})}{(\\mathbf{f}'(s_{0}))^{\\top}\\Sigma^{-1}\\mathbf{f}'(s_{0})} + \\frac{(\\mathbf{f}'(s_{0}))^{\\top}\\Sigma^{-1}\\mathbf{r}}{(\\mathbf{f}'(s_{0}))^{\\top}\\Sigma^{-1}\\mathbf{f}'(s_{0})}\n$$\n合并各项，得到一般相关情况下的最优估计器为：\n$$\n\\hat{s}_{\\text{corr}} = s_{0} + \\frac{(\\mathbf{f}'(s_{0}))^{\\top}\\Sigma^{-1}(\\mathbf{r} - \\mathbf{f}(s_{0}))}{(\\mathbf{f}'(s_{0}))^{\\top}\\Sigma^{-1}\\mathbf{f}'(s_{0})}\n$$\n现在，我们将此结果特化到不相关的情况，其中 $\\Sigma = \\mathrm{diag}(\\sigma_{1}^{2}, \\ldots, \\sigma_{n}^{2})$。逆协方差矩阵为 $\\Sigma^{-1} = \\mathrm{diag}(\\frac{1}{\\sigma_{1}^{2}}, \\ldots, \\frac{1}{\\sigma_{n}^{2}})$。设 $f_{i}(s_{0})$、$f'_{i}(s_{0})$ 和 $r_{i}$ 分别是 $\\mathbf{f}(s_{0})$、$\\mathbf{f}'(s_{0})$ 和 $\\mathbf{r}$ 的第 $i$ 个分量。\n分母（费雪信息）变为一个和式：\n$$\n(\\mathbf{f}'(s_{0}))^{\\top}\\Sigma^{-1}\\mathbf{f}'(s_{0}) = \\sum_{i=1}^{n} f'_{i}(s_{0}) \\left(\\frac{1}{\\sigma_{i}^{2}}\\right) f'_{i}(s_{0}) = \\sum_{i=1}^{n} \\frac{(f'_{i}(s_{0}))^{2}}{\\sigma_{i}^{2}}\n$$\n分子变为：\n$$\n(\\mathbf{f}'(s_{0}))^{\\top}\\Sigma^{-1}(\\mathbf{r} - \\mathbf{f}(s_{0})) = \\sum_{i=1}^{n} f'_{i}(s_{0}) \\left(\\frac{1}{\\sigma_{i}^{2}}\\right) (r_{i} - f_{i}(s_{0})) = \\sum_{i=1}^{n} \\frac{f'_{i}(s_{0})(r_{i} - f_{i}(s_{0}))}{\\sigma_{i}^{2}}\n$$\n将这些和式代回一般表达式，得到不相关情况下的最优估计器：\n$$\n\\hat{s}_{\\text{uncorr}} = s_{0} + \\frac{\\sum_{i=1}^{n} \\frac{f'_{i}(s_{0})(r_{i} - f_{i}(s_{0}))}{\\sigma_{i}^{2}}}{\\sum_{i=1}^{n} \\frac{(f'_{i}(s_{0}))^{2}}{\\sigma_{i}^{2}}}\n$$\n至此，两种要求情况的推导均已完成。",
            "answer": "$$\n\\boxed{\\begin{pmatrix} s_{0} + \\frac{(\\mathbf{f}'(s_{0}))^{\\top}\\Sigma^{-1}(\\mathbf{r} - \\mathbf{f}(s_{0}))}{(\\mathbf{f}'(s_{0}))^{\\top}\\Sigma^{-1}\\mathbf{f}'(s_{0})} \\\\\\\\ s_{0} + \\frac{\\sum_{i=1}^{n} \\frac{f'_{i}(s_{0})(r_{i} - f_{i}(s_{0}))}{\\sigma_{i}^{2}}}{\\sum_{i=1}^{n} \\frac{(f'_{i}(s_{0}))^{2}}{\\sigma_{i}^{2}}} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "神经计算的一个高级观点是，大脑不仅仅是估计一个刺激的单一数值，而是在表征关于该刺激的整个概率分布，从而处理不确定性。概率群体编码（Probabilistic Population Codes, PPCs）正是这样一个强大的理论框架。这个练习  引导你探索PPC模型的核心机制，即如何通过神经元发放的脉冲来更新对一个潜在变量的贝叶斯后验分布。你将看到，在一个设计巧妙的编码方案中，复杂的贝叶斯推断可以简化为对脉冲计数的简单线性求和操作，这为大脑如何进行概率计算提供了一种优雅且计算上可行的解释。",
            "id": "4055165",
            "problem": "考虑一个二维潜变量 $\\mathbf{x} \\in \\mathbb{R}^{2}$，它由一个概率群体编码（PPC）表示。一个由 $M$ 个神经元组成的群体在固定的观察窗口内发出脉冲计数 $\\{n_{i}\\}_{i=1}^{M}$，这些计数在给定 $\\mathbf{x}$ 的条件下是条件独立的。假设对于每个神经元 $i$，每次脉冲对刺激的对数似然贡献由以下形式的基函数近似\n$$\n\\phi_{i}(\\mathbf{x}) \\;=\\; \\alpha_{i} \\;+\\; \\mathbf{u}_{i}^{\\top}\\mathbf{x} \\;-\\; \\frac{1}{2}\\,\\mathbf{x}^{\\top}\\mathbf{U}_{i}\\,\\mathbf{x},\n$$\n其中 $\\alpha_{i} \\in \\mathbb{R}$，$\\mathbf{u}_{i} \\in \\mathbb{R}^{2}$，$\\mathbf{U}_{i} \\in \\mathbb{R}^{2 \\times 2}$ 是对称半正定矩阵。总对数似然被建模为这些贡献在所有脉冲和神经元上的总和，因此（在不考虑一个与 $\\mathbf{x}$ 无关的加性项的情况下）观测似然遵循\n$$\n\\ln p\\!\\left(\\{n_{i}\\}_{i=1}^{M}\\mid \\mathbf{x}\\right) \\;=\\; \\sum_{i=1}^{M} n_{i}\\,\\phi_{i}(\\mathbf{x}) \\;+\\; \\text{const}(\\{n_{i}\\}),\n$$\n其中 $\\text{const}(\\{n_{i}\\})$ 不依赖于 $\\mathbf{x}$。设 $\\mathbf{x}$ 的先验为高斯分布，其均值为 $\\boldsymbol{\\mu}_{0} \\in \\mathbb{R}^{2}$，协方差为 $\\boldsymbol{\\Sigma}_{0} \\in \\mathbb{R}^{2\\times 2}$，其自然（正则）参数定义为\n$$\n\\boldsymbol{\\Lambda}_{0} \\;=\\; \\boldsymbol{\\Sigma}_{0}^{-1}, \\qquad \\mathbf{h}_{0} \\;=\\; \\boldsymbol{\\Lambda}_{0}\\,\\boldsymbol{\\mu}_{0}.\n$$\n正则形式的高斯分布为\n$$\n\\ln p(\\mathbf{x}) \\;=\\; -\\frac{1}{2}\\,\\mathbf{x}^{\\top}\\boldsymbol{\\Lambda}_{0}\\,\\mathbf{x} \\;+\\; \\mathbf{h}_{0}^{\\top}\\mathbf{x} \\;+\\; \\text{const}.\n$$\n从这些组成部分和将独立似然与先验相结合的标准规则出发，构建能够产生关于 $\\mathbf{x}$ 的高斯后验的 PPC，并推导从脉冲计数 $\\{n_{i}\\}_{i=1}^{M}$ 到后验高斯自然参数 $(\\mathbf{h}, \\boldsymbol{\\Lambda})$ 的显式映射。将你的最终答案表示为关于 $\\{n_{i}\\}$、$\\{\\mathbf{u}_{i}\\}$、$\\{\\mathbf{U}_{i}\\}$ 和 $(\\mathbf{h}_{0}, \\boldsymbol{\\Lambda}_{0})$ 的闭式解。不需要数值近似；仅提供解析表达式。",
            "solution": "用户希望在一个特定的概率群体编码模型下，推导从脉冲计数到潜变量后验自然参数的映射。\n\n### 步骤 1：提取已知条件\n- 潜变量：$\\mathbf{x} \\in \\mathbb{R}^{2}$。\n- 神经元数量：$M$。\n- 脉冲计数：$\\{n_{i}\\}_{i=1}^{M}$。\n- 脉冲计数在给定 $\\mathbf{x}$ 下的条件独立性。\n- 神经元 $i$ 的单次脉冲对数似然贡献：$\\phi_{i}(\\mathbf{x}) = \\alpha_{i} + \\mathbf{u}_{i}^{\\top}\\mathbf{x} - \\frac{1}{2}\\mathbf{x}^{\\top}\\mathbf{U}_{i}\\mathbf{x}$，其中 $\\alpha_{i} \\in \\mathbb{R}$，$\\mathbf{u}_{i} \\in \\mathbb{R}^{2}$，且 $\\mathbf{U}_{i} \\in \\mathbb{R}^{2 \\times 2}$ 是对称半正定矩阵。\n- 总对数似然：$\\ln p(\\{n_{i}\\}_{i=1}^{M} \\mid \\mathbf{x}) = \\sum_{i=1}^{M} n_{i}\\phi_{i}(\\mathbf{x}) + \\text{const}(\\{n_{i}\\})$。\n- 先验分布：$p(\\mathbf{x})$ 是均值为 $\\boldsymbol{\\mu}_{0}$、协方差为 $\\boldsymbol{\\Sigma}_{0}$ 的高斯分布。\n- 先验正则参数：$\\boldsymbol{\\Lambda}_{0} = \\boldsymbol{\\Sigma}_{0}^{-1}$ 和 $\\mathbf{h}_{0} = \\boldsymbol{\\Lambda}_{0}\\boldsymbol{\\mu}_{0}$。\n- 正则形式的对数先验：$\\ln p(\\mathbf{x}) = -\\frac{1}{2}\\mathbf{x}^{\\top}\\boldsymbol{\\Lambda}_{0}\\mathbf{x} + \\mathbf{h}_{0}^{\\top}\\mathbf{x} + \\text{const}$。\n\n### 步骤 2：使用提取的已知条件进行验证\n- **科学基础**：该问题描述了一个在理论神经科学和机器学习中常见的贝叶斯推断场景。该模型是概率群体编码（PPC）的一个具体实例，其中似然和先验都属于指数族（具体来说，它们都具有高斯形式）。高斯似然和高斯先验的乘积产生高斯后验，这是一个涉及共轭先验的标准结果。该模型在科学上是一致且成熟的。\n- **适定性**：问题陈述清晰，要求进行特定的推导：寻找后验分布的参数。给定的信息足以执行此推导。假设（例如，$\\mathbf{U}_i$ 是半正定的，$\\boldsymbol{\\Sigma}_0$ 是协方差矩阵）确保了最终的分布在数学上是良态的。\n- **客观性**：问题是用精确的数学语言表述的，没有歧义或主观成分。\n\n所有其他验证标准均已满足。该问题在理论建模的背景下是完整的、一致的、现实的，并且可以通过直接的数学推导来验证。\n\n### 步骤 3：结论与行动\n问题是**有效的**。开始求解。\n\n为了找到潜变量 $\\mathbf{x}$ 的后验分布，我们使用贝叶斯定理。后验概率密度正比于似然和先验的乘积：\n$$\np(\\mathbf{x} \\mid \\{n_{i}\\}_{i=1}^{M}) \\;\\propto\\; p(\\{n_{i}\\}_{i=1}^{M} \\mid \\mathbf{x})\\,p(\\mathbf{x})\n$$\n由于似然和先验都是以对数形式给出的，处理后验的对数会更方便。对数后验是对数似然和对数先验之和，相差一个不依赖于 $\\mathbf{x}$ 的加性常数：\n$$\n\\ln p(\\mathbf{x} \\mid \\{n_{i}\\}_{i=1}^{M}) \\;=\\; \\ln p(\\{n_{i}\\}_{i=1}^{M} \\mid \\mathbf{x}) \\;+\\; \\ln p(\\mathbf{x}) \\;+\\; \\text{const}\n$$\n我们已知对数似然和对数先验的表达式。让我们将它们代入上述方程。\n$$\n\\ln p(\\mathbf{x} \\mid \\{n_{i}\\}_{i=1}^{M}) \\;=\\; \\left(\\sum_{i=1}^{M} n_{i}\\,\\phi_{i}(\\mathbf{x}) + \\text{const}_1\\right) \\;+\\; \\left(-\\frac{1}{2}\\,\\mathbf{x}^{\\top}\\boldsymbol{\\Lambda}_{0}\\,\\mathbf{x} + \\mathbf{h}_{0}^{\\top}\\mathbf{x} + \\text{const}_2\\right) \\;+\\; \\text{const}\n$$\n现在，代入 $\\phi_{i}(\\mathbf{x})$ 的定义：\n$$\n\\ln p(\\mathbf{x} \\mid \\{n_{i}\\}) \\;=\\; \\sum_{i=1}^{M} n_{i}\\left(\\alpha_{i} + \\mathbf{u}_{i}^{\\top}\\mathbf{x} - \\frac{1}{2}\\,\\mathbf{x}^{\\top}\\mathbf{U}_{i}\\,\\mathbf{x}\\right) \\;-\\; \\frac{1}{2}\\,\\mathbf{x}^{\\top}\\boldsymbol{\\Lambda}_{0}\\,\\mathbf{x} \\;+\\; \\mathbf{h}_{0}^{\\top}\\mathbf{x} \\;+\\; \\text{const}'\n$$\n其中所有不依赖于 $\\mathbf{x}$ 的常数都被收集到一个单独的项 $\\text{const}'$ 中。我们的目标是将此表达式重排为高斯对数密度的正则形式，即 $\\ln p(\\mathbf{x} \\mid \\{n_{i}\\}) = -\\frac{1}{2}\\mathbf{x}^{\\top}\\boldsymbol{\\Lambda}\\mathbf{x} + \\mathbf{h}^{\\top}\\mathbf{x} + \\text{const}''$。为此，我们对关于 $\\mathbf{x}$ 的二次项和线性项进行分组。\n\n首先，我们展开求和项：\n$$\n\\ln p(\\mathbf{x} \\mid \\{n_{i}\\}) \\;=\\; \\sum_{i=1}^{M} n_{i}\\alpha_{i} \\;+\\; \\sum_{i=1}^{M} n_{i}\\mathbf{u}_{i}^{\\top}\\mathbf{x} \\;-\\; \\sum_{i=1}^{M} n_{i}\\frac{1}{2}\\,\\mathbf{x}^{\\top}\\mathbf{U}_{i}\\,\\mathbf{x} \\;-\\; \\frac{1}{2}\\,\\mathbf{x}^{\\top}\\boldsymbol{\\Lambda}_{0}\\,\\mathbf{x} \\;+\\; \\mathbf{h}_{0}^{\\top}\\mathbf{x} \\;+\\; \\text{const}'\n$$\n项 $\\sum_{i=1}^{M} n_{i}\\alpha_{i}$ 不依赖于 $\\mathbf{x}$，可以被吸收到总的常数项中。让我们根据对 $\\mathbf{x}$ 的依赖性对剩余的项进行分组。\n\n关于 $\\mathbf{x}$ 的线性项是：\n$$\n\\left(\\sum_{i=1}^{M} n_{i}\\mathbf{u}_{i}^{\\top}\\mathbf{x}\\right) \\;+\\; \\mathbf{h}_{0}^{\\top}\\mathbf{x} \\;=\\; \\left(\\sum_{i=1}^{M} n_{i}\\mathbf{u}_{i}\\right)^{\\top}\\mathbf{x} \\;+\\; \\mathbf{h}_{0}^{\\top}\\mathbf{x} \\;=\\; \\left(\\mathbf{h}_{0} + \\sum_{i=1}^{M} n_{i}\\mathbf{u}_{i}\\right)^{\\top}\\mathbf{x}\n$$\n关于 $\\mathbf{x}$ 的二次项是：\n$$\n-\\frac{1}{2}\\left(\\sum_{i=1}^{M} n_{i}\\,\\mathbf{x}^{\\top}\\mathbf{U}_{i}\\,\\mathbf{x}\\right) \\;-\\; \\frac{1}{2}\\,\\mathbf{x}^{\\top}\\boldsymbol{\\Lambda}_{0}\\,\\mathbf{x} \\;=\\; -\\frac{1}{2}\\,\\mathbf{x}^{\\top}\\left(\\sum_{i=1}^{M} n_{i}\\mathbf{U}_{i}\\right)\\mathbf{x} \\;-\\; \\frac{1}{2}\\,\\mathbf{x}^{\\top}\\boldsymbol{\\Lambda}_{0}\\,\\mathbf{x} \\;=\\; -\\frac{1}{2}\\,\\mathbf{x}^{\\top}\\left(\\boldsymbol{\\Lambda}_{0} + \\sum_{i=1}^{M} n_{i}\\mathbf{U}_{i}\\right)\\mathbf{x}\n$$\n结合这些结果，对数后验可以写为：\n$$\n\\ln p(\\mathbf{x} \\mid \\{n_{i}\\}) \\;=\\; -\\frac{1}{2}\\,\\mathbf{x}^{\\top}\\left(\\boldsymbol{\\Lambda}_{0} + \\sum_{i=1}^{M} n_{i}\\mathbf{U}_{i}\\right)\\mathbf{x} \\;+\\; \\left(\\mathbf{h}_{0} + \\sum_{i=1}^{M} n_{i}\\mathbf{u}_{i}\\right)^{\\top}\\mathbf{x} \\;+\\; \\text{const}''\n$$\n该表达式是正则形式下高斯分布的对数密度。通过将其与一般形式 $\\ln p(\\mathbf{x}) = -\\frac{1}{2}\\mathbf{x}^{\\top}\\boldsymbol{\\Lambda}\\mathbf{x} + \\mathbf{h}^{\\top}\\mathbf{x} + \\text{const}$ 进行比较，我们可以直接确定后验分布的自然参数 $(\\mathbf{h}, \\boldsymbol{\\Lambda})$。\n\n后验自然参数 $\\mathbf{h}$（势向量）是线性项的系数：\n$$\n\\mathbf{h} \\;=\\; \\mathbf{h}_{0} + \\sum_{i=1}^{M} n_{i}\\mathbf{u}_{i}\n$$\n后验自然参数 $\\boldsymbol{\\Lambda}$（精度矩阵）是二次项中的矩阵：\n$$\n\\boldsymbol{\\Lambda} \\;=\\; \\boldsymbol{\\Lambda}_{0} + \\sum_{i=1}^{M} n_{i}\\mathbf{U}_{i}\n$$\n这些表达式给出了在给定先验参数和神经元基函数的情况下，从脉冲计数 $\\{n_i\\}$ 到后验自然参数 $(\\mathbf{h}, \\boldsymbol{\\Lambda})$ 的显式映射。这种更新规则在正则参数空间中是加性的，这是指数族与共轭先验进行推断的一个典型特征。",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\mathbf{h}_{0} + \\sum_{i=1}^{M} n_{i} \\mathbf{u}_{i} \\\\\\\\ \\boldsymbol{\\Lambda}_{0} + \\sum_{i=1}^{M} n_{i} \\mathbf{U}_{i} \\end{pmatrix}}\n$$"
        }
    ]
}