{
    "hands_on_practices": [
        {
            "introduction": "理解大脑如何从神经活动中提取信息是计算神经科学的核心目标。本练习将引导你亲手推导最优线性解码器——一个完成此项任务的基础工具。通过实践无偏估计和方差最小化的原则，你将学会如何构建一个能够最优地处理神经元之间噪声相关性的解码器，而这种相关性是生物神经回路中的一个普遍特征。 ",
            "id": "4055126",
            "problem": "考虑一个由$n$个神经元组成的群体编码，其对标量刺激$s \\in \\mathbb{R}$的平均响应由调谐曲线$\\mathbf{f}(s) \\in \\mathbb{R}^{n}$给出。观测到的群体响应$\\mathbf{r} \\in \\mathbb{R}^{n}$遵循概率模型$\\mathbf{r} \\sim \\mathcal{N}(\\mathbf{f}(s),\\Sigma)$，其中$\\Sigma \\in \\mathbb{R}^{n \\times n}$是一个与刺激无关、正定的噪声协方差，在神经元之间可能存在相关性。固定一个参考刺激$s_{0}$，并假设一个局部线性区域，其中$\\mathbf{f}(s)$是可微的，并且可以用其在$s_0$附近的一阶泰勒展开来近似，其梯度为$\\mathbf{f}'(s_{0}) = \\frac{d\\mathbf{f}}{ds}\\big|_{s_{0}} \\in \\mathbb{R}^{n}$。考虑形式为$\\hat{s} = a + \\mathbf{w}^{\\top}\\mathbf{r}$的线性估计器，其中$a \\in \\mathbb{R}$且$\\mathbf{w} \\in \\mathbb{R}^{n}$。在$s_{0}$处施加局部无偏约束：$\\mathbb{E}[\\hat{s}\\mid s_{0}] = s_{0}$和$\\frac{d}{ds}\\mathbb{E}[\\hat{s}\\mid s]\\big|_{s_{0}} = 1$。在所有此类局部无偏线性估计器中，推导在相关的$\\mathcal{N}(\\mathbf{f}(s),\\Sigma)$高斯噪声下使$\\mathrm{Var}(\\hat{s}\\mid s_{0})$最小化的估计器。然后将您的表达式特化到不相关的情况$\\Sigma = \\mathrm{diag}(\\sigma_{1}^{2},\\ldots,\\sigma_{n}^{2})$，其中对$i=1,\\ldots,n$有$\\sigma_{i}0$。将您的最终答案表示为两种情况下$s_0$处最优估计器的闭式解析表达式，用$\\mathbf{f}(s_{0})$、$\\mathbf{f}'(s_{0})$、$\\Sigma$和$\\mathbf{r}$写出。不需要进行数值计算或四舍五入。除已声明的假设外，不要引入任何其他假设。为清晰起见，在提及该估计器时，请使用其显式表达式，而不要使用其名称“最佳线性无偏估计器”(BLUE)。",
            "solution": "问题定义了一个形式为$\\hat{s} = a + \\mathbf{w}^{\\top}\\mathbf{r}$的线性估计器，其中$a \\in \\mathbb{R}$是一个标量偏移，$\\mathbf{w} \\in \\mathbb{R}^{n}$是一个权重向量。群体响应$\\mathbf{r}$是从一个多元高斯分布$\\mathbf{r} \\sim \\mathcal{N}(\\mathbf{f}(s),\\Sigma)$中抽取的随机向量，其中$\\mathbf{f}(s)$是平均神经响应（调谐曲线）的向量，$\\Sigma$是与刺激无关的噪声协方差矩阵。\n\n第一步是根据估计器参数$a$和$\\mathbf{w}$来表达两个局部无偏约束。给定刺激$s$，估计器的期望值为：\n$$\n\\mathbb{E}[\\hat{s}\\mid s] = \\mathbb{E}[a + \\mathbf{w}^{\\top}\\mathbf{r}\\mid s] = a + \\mathbf{w}^{\\top}\\mathbb{E}[\\mathbf{r}\\mid s] = a + \\mathbf{w}^{\\top}\\mathbf{f}(s)\n$$\n第一个约束，即在参考刺激$s_{0}$处的局部无偏性，是$\\mathbb{E}[\\hat{s}\\mid s_{0}] = s_{0}$。将此应用于期望值的表达式，得到：\n$$\na + \\mathbf{w}^{\\top}\\mathbf{f}(s_{0}) = s_{0}\n$$\n这个约束使我们能够用$\\mathbf{w}$表示$a$：\n$$\na = s_{0} - \\mathbf{w}^{\\top}\\mathbf{f}(s_{0})\n$$\n第二个约束涉及期望值对刺激的导数，并在$s_0$处求值：$\\frac{d}{ds}\\mathbb{E}[\\hat{s}\\mid s]\\big|_{s_{0}} = 1$。我们首先计算导数：\n$$\n\\frac{d}{ds}\\mathbb{E}[\\hat{s}\\mid s] = \\frac{d}{ds} \\left( a + \\mathbf{w}^{\\top}\\mathbf{f}(s) \\right) = \\mathbf{w}^{\\top}\\frac{d\\mathbf{f}(s)}{ds} = \\mathbf{w}^{\\top}\\mathbf{f}'(s)\n$$\n在$s=s_{0}$处求值并应用该约束，得到：\n$$\n\\mathbf{w}^{\\top}\\mathbf{f}'(s_{0}) = 1\n$$\n这是对权重向量$\\mathbf{w}$的一个约束。\n\n接下来，我们确定要最小化的目标函数，即估计器在$s_{0}$处的方差$\\mathrm{Var}(\\hat{s}\\mid s_{0})$。\n$$\n\\mathrm{Var}(\\hat{s}\\mid s_{0}) = \\mathrm{Var}(a + \\mathbf{w}^{\\top}\\mathbf{r}\\mid s_{0}) = \\mathrm{Var}(\\mathbf{w}^{\\top}\\mathbf{r}\\mid s_{0})\n$$\n随机向量的线性变换的方差由$\\mathbf{w}^{\\top}\\mathrm{Cov}(\\mathbf{r})\\mathbf{w}$给出。由于$\\mathrm{Cov}(\\mathbf{r} \\mid s_{0}) = \\Sigma$，目标函数为：\n$$\n\\mathrm{Var}(\\hat{s}\\mid s_{0}) = \\mathbf{w}^{\\top}\\Sigma\\mathbf{w}\n$$\n现在问题是在线性约束$\\mathbf{w}^{\\top}\\mathbf{f}'(s_{0}) = 1$下最小化$\\mathbf{w}^{\\top}\\Sigma\\mathbf{w}$。参数$a$可以在找到最优的$\\mathbf{w}$之后确定。这是一个可以使用拉格朗日乘子$\\lambda$解决的约束优化问题。我们定义拉格朗日函数$\\mathcal{L}$：\n$$\n\\mathcal{L}(\\mathbf{w}, \\lambda) = \\mathbf{w}^{\\top}\\Sigma\\mathbf{w} - \\lambda(\\mathbf{w}^{\\top}\\mathbf{f}'(s_{0}) - 1)\n$$\n为了找到最小值，我们对$\\mathcal{L}$关于$\\mathbf{w}$求梯度，并将其设为零向量。协方差矩阵$\\Sigma$是对称的。\n$$\n\\nabla_{\\mathbf{w}}\\mathcal{L} = 2\\Sigma\\mathbf{w} - \\lambda\\mathbf{f}'(s_{0}) = \\mathbf{0}\n$$\n这得到$2\\Sigma\\mathbf{w} = \\lambda\\mathbf{f}'(s_{0})$。由于$\\Sigma$被指定为正定矩阵，因此它是可逆的。我们可以解出$\\mathbf{w}$：\n$$\n\\mathbf{w} = \\frac{\\lambda}{2}\\Sigma^{-1}\\mathbf{f}'(s_{0})\n$$\n为了确定拉格朗日乘子$\\lambda$，我们将$\\mathbf{w}$的这个表达式代回约束方程$\\mathbf{w}^{\\top}\\mathbf{f}'(s_{0}) = 1$：\n$$\n\\left(\\frac{\\lambda}{2}\\Sigma^{-1}\\mathbf{f}'(s_{0})\\right)^{\\top}\\mathbf{f}'(s_{0}) = 1\n$$\n$$\n\\frac{\\lambda}{2}(\\mathbf{f}'(s_{0}))^{\\top}(\\Sigma^{-1})^{\\top}\\mathbf{f}'(s_{0}) = 1\n$$\n由于$\\Sigma$是对称的，其逆矩阵$\\Sigma^{-1}$也是对称的，即$(\\Sigma^{-1})^{\\top} = \\Sigma^{-1}$。方程简化为：\n$$\n\\frac{\\lambda}{2}(\\mathbf{f}'(s_{0}))^{\\top}\\Sigma^{-1}\\mathbf{f}'(s_{0}) = 1\n$$\n解出$\\lambda$：\n$$\n\\lambda = \\frac{2}{(\\mathbf{f}'(s_{0}))^{\\top}\\Sigma^{-1}\\mathbf{f}'(s_{0})}\n$$\n分母$(\\mathbf{f}'(s_{0}))^{\\top}\\Sigma^{-1}\\mathbf{f}'(s_{0})$是在$s_0$处刺激$s$的Fisher信息。由于$\\Sigma^{-1}$是正定的，并且假设$\\mathbf{f}'(s_{0}) \\neq \\mathbf{0}$，Fisher信息为正，因此$\\lambda$是良定义的。\n现在我们将$\\lambda$代回$\\mathbf{w}$的表达式中，以找到最优权重向量$\\mathbf{w}_{\\text{opt}}$：\n$$\n\\mathbf{w}_{\\text{opt}} = \\frac{1}{2}\\left(\\frac{2}{(\\mathbf{f}'(s_{0}))^{\\top}\\Sigma^{-1}\\mathbf{f}'(s_{0})}\\right)\\Sigma^{-1}\\mathbf{f}'(s_{0}) = \\frac{\\Sigma^{-1}\\mathbf{f}'(s_{0})}{(\\mathbf{f}'(s_{0}))^{\\top}\\Sigma^{-1}\\mathbf{f}'(s_{0})}\n$$\n有了最优权重向量$\\mathbf{w}_{\\text{opt}}$，我们找到最优偏移量$a_{\\text{opt}}$：\n$$\na_{\\text{opt}} = s_{0} - \\mathbf{w}_{\\text{opt}}^{\\top}\\mathbf{f}(s_{0}) = s_{0} - \\frac{(\\Sigma^{-1}\\mathbf{f}'(s_{0}))^{\\top}\\mathbf{f}(s_{0})}{(\\mathbf{f}'(s_{0}))^{\\top}\\Sigma^{-1}\\mathbf{f}'(s_{0})} = s_{0} - \\frac{(\\mathbf{f}'(s_{0}))^{\\top}\\Sigma^{-1}\\mathbf{f}(s_{0})}{(\\mathbf{f}'(s_{0}))^{\\top}\\Sigma^{-1}\\mathbf{f}'(s_{0})}\n$$\n最后，我们构建最优估计器$\\hat{s}_{\\text{opt}} = a_{\\text{opt}} + \\mathbf{w}_{\\text{opt}}^{\\top}\\mathbf{r}$：\n$$\n\\hat{s}_{\\text{opt}} = \\left(s_{0} - \\frac{(\\mathbf{f}'(s_{0}))^{\\top}\\Sigma^{-1}\\mathbf{f}(s_{0})}{(\\mathbf{f}'(s_{0}))^{\\top}\\Sigma^{-1}\\mathbf{f}'(s_{0})}\\right) + \\left(\\frac{\\Sigma^{-1}\\mathbf{f}'(s_{0})}{(\\mathbf{f}'(s_{0}))^{\\top}\\Sigma^{-1}\\mathbf{f}'(s_{0})}\\right)^{\\top}\\mathbf{r}\n$$\n$$\n\\hat{s}_{\\text{opt}} = s_{0} - \\frac{(\\mathbf{f}'(s_{0}))^{\\top}\\Sigma^{-1}\\mathbf{f}(s_{0})}{(\\mathbf{f}'(s_{0}))^{\\top}\\Sigma^{-1}\\mathbf{f}'(s_{0})} + \\frac{(\\mathbf{f}'(s_{0}))^{\\top}\\Sigma^{-1}\\mathbf{r}}{(\\mathbf{f}'(s_{0}))^{\\top}\\Sigma^{-1}\\mathbf{f}'(s_{0})}\n$$\n合并各项，一般相关情况下的最优估计器为：\n$$\n\\hat{s}_{\\text{corr}} = s_{0} + \\frac{(\\mathbf{f}'(s_{0}))^{\\top}\\Sigma^{-1}(\\mathbf{r} - \\mathbf{f}(s_{0}))}{(\\mathbf{f}'(s_{0}))^{\\top}\\Sigma^{-1}\\mathbf{f}'(s_{0})}\n$$\n现在，我们将此结果特化到不相关的情况，其中$\\Sigma = \\mathrm{diag}(\\sigma_{1}^{2}, \\ldots, \\sigma_{n}^{2})$。逆协方差矩阵为$\\Sigma^{-1} = \\mathrm{diag}(\\frac{1}{\\sigma_{1}^{2}}, \\ldots, \\frac{1}{\\sigma_{n}^{2}})$。设$f_{i}(s_{0})$、$f'_{i}(s_{0})$和$r_{i}$分别是$\\mathbf{f}(s_{0})$、$\\mathbf{f}'(s_{0})$和$\\mathbf{r}$的第$i$个分量。\n分母（Fisher信息）变为一个和式：\n$$\n(\\mathbf{f}'(s_{0}))^{\\top}\\Sigma^{-1}\\mathbf{f}'(s_{0}) = \\sum_{i=1}^{n} f'_{i}(s_{0}) \\left(\\frac{1}{\\sigma_{i}^{2}}\\right) f'_{i}(s_{0}) = \\sum_{i=1}^{n} \\frac{(f'_{i}(s_{0}))^{2}}{\\sigma_{i}^{2}}\n$$\n分子变为：\n$$\n(\\mathbf{f}'(s_{0}))^{\\top}\\Sigma^{-1}(\\mathbf{r} - \\mathbf{f}(s_{0})) = \\sum_{i=1}^{n} f'_{i}(s_{0}) \\left(\\frac{1}{\\sigma_{i}^{2}}\\right) (r_{i} - f_{i}(s_{0})) = \\sum_{i=1}^{n} \\frac{f'_{i}(s_{0})(r_{i} - f_{i}(s_{0}))}{\\sigma_{i}^{2}}\n$$\n将这些和式代回一般表达式中，得到不相关情况下的最优估计器：\n$$\n\\hat{s}_{\\text{uncorr}} = s_{0} + \\frac{\\sum_{i=1}^{n} \\frac{f'_{i}(s_{0})(r_{i} - f_{i}(s_{0}))}{\\sigma_{i}^{2}}}{\\sum_{i=1}^{n} \\frac{(f'_{i}(s_{0}))^{2}}{\\sigma_{i}^{2}}}\n$$\n这就完成了对两种要求情况的推导。",
            "answer": "$$\n\\boxed{\\begin{pmatrix} s_{0} + \\frac{(\\mathbf{f}'(s_{0}))^{\\top}\\Sigma^{-1}(\\mathbf{r} - \\mathbf{f}(s_{0}))}{(\\mathbf{f}'(s_{0}))^{\\top}\\Sigma^{-1}\\mathbf{f}'(s_{0})}  s_{0} + \\frac{\\sum_{i=1}^{n} \\frac{f'_{i}(s_{0})(r_{i} - f_{i}(s_{0}))}{\\sigma_{i}^{2}}}{\\sum_{i=1}^{n} \\frac{(f'_{i}(s_{0}))^{2}}{\\sigma_{i}^{2}}} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "构建解码器后，我们必须评估其性能。本练习将通过计算一个经典群体编码模型的费雪信息（Fisher Information），深入探讨解码精度的理论极限。你将为一个具有高斯调谐曲线和泊松噪声的神经元群体推导解码方差，从而揭示神经群体物理特性与所编码信息精度之间的直接联系。 ",
            "id": "4055130",
            "problem": "考虑一个神经形态系统中的一维刺激 $s \\in \\mathbb{R}$，它由一个包含 $N$ 个神经元的群体编码，每个神经元都具有独立的泊松尖峰发放和相同的高斯调谐特性。在一个固定的观察窗口内，神经元 $i$ 的平均尖峰计数为 $\\lambda_i(s) = \\alpha \\exp\\!\\big(-\\frac{(s - \\mu_i)^{2}}{2 \\sigma^{2}}\\big)$，其中 $\\alpha  0$ 是峰值平均计数，$\\sigma  0$ 是调谐宽度，$\\mu_i \\in \\mathbb{R}$ 是神经元 $i$ 的偏好刺激。假设偏好刺激是密集平铺的：集合 $\\{\\mu_i\\}_{i=1}^{N}$ 在一个长度为 $L  0$ 的区间上均匀分布，间距为 $\\Delta = \\frac{L}{N}$，因此密度为 $\\rho = \\frac{N}{L}$。尖峰计数 $n_i$ 是独立的，且满足 $n_i \\sim \\text{Poisson}(\\lambda_i(s))$。\n\n重点是使用最优线性估计器 (OLE) 解码在一个固定工作点 $s_0 \\in \\mathbb{R}$ 周围的 $s$ 的微小扰动。该 OLE 被定义为观测计数的线性函数，其在 $s_0$ 处局部无偏，并在泊松噪声模型下，最小化关于 $(s - s_0)$ 的一阶均方误差。从独立的泊松变异性以及调谐曲线在 $s_0$ 周围的一阶（局部）线性化的基本原理出发，在密集平铺的假设下，推导当 $N \\to \\infty$ 时，$s$ 的 OLE 估计的渐近方差。最终表达式必须用 $\\alpha$、$\\sigma$、$L$ 和 $N$ 表示。\n\n将最终方差表示为单个闭合形式的解析表达式。无需进行数值四舍五入。将最终结果表述为以刺激变量 $s$ 的单位计量的方差，但在加框的最终答案中不包括单位。",
            "solution": "该问题要求推导刺激 $s$ 的最优线性估计器 (OLE) 的渐近方差。一个局部无偏且方差最小的 OLE 的方差由克拉美-罗下界 (CRLB) 给出，它是费雪信息 $J(s)$ 的倒数。我们将计算指定神经元群体的费雪信息，然后将其求逆以找到方差。\n\n该模型由 $N$ 个具有独立泊松尖峰发放的神经元组成。神经元 $i$ 的平均尖峰计数由一个高斯调谐曲线给出：\n$$\n\\lambda_i(s) = \\alpha \\exp\\left(-\\frac{(s - \\mu_i)^{2}}{2 \\sigma^{2}}\\right)\n$$\n其中 $\\alpha$ 是峰值平均计数，$\\sigma$ 是调谐宽度，$\\mu_i$ 是偏好刺激。尖峰计数 $n_i$ 是独立的泊松变量，$n_i \\sim \\text{Poisson}(\\lambda_i(s))$。\n\n对于一个独立的泊松神经元群体，其费雪信息 $J(s)$ 可以计算为：\n$$\nJ(s) = \\sum_{i=1}^{N} \\frac{\\left(\\frac{d\\lambda_i(s)}{ds}\\right)^2}{\\lambda_i(s)}\n$$\n让我们将 $\\lambda_i(s)$ 对 $s$ 的导数表示为 $\\lambda'_i(s)$。首先，我们计算这个导数：\n$$\n\\lambda'_i(s) = \\frac{d}{ds} \\left[ \\alpha \\exp\\left(-\\frac{(s - \\mu_i)^{2}}{2 \\sigma^{2}}\\right) \\right]\n$$\n使用链式法则，我们得到：\n$$\n\\lambda'_i(s) = \\alpha \\exp\\left(-\\frac{(s - \\mu_i)^{2}}{2 \\sigma^{2}}\\right) \\cdot \\left( -\\frac{2(s - \\mu_i)}{2\\sigma^2} \\right) = -\\frac{s - \\mu_i}{\\sigma^2} \\lambda_i(s)\n$$\n现在，我们可以计算费雪信息求和中的项：\n$$\n\\frac{(\\lambda'_i(s))^2}{\\lambda_i(s)} = \\frac{\\left( -\\frac{s - \\mu_i}{\\sigma^2} \\lambda_i(s) \\right)^2}{\\lambda_i(s)} = \\frac{(s - \\mu_i)^2}{\\sigma^4} \\frac{\\lambda_i(s)^2}{\\lambda_i(s)} = \\frac{(s - \\mu_i)^2}{\\sigma^4} \\lambda_i(s)\n$$\n代入 $\\lambda_i(s)$ 的表达式：\n$$\n\\frac{(\\lambda'_i(s))^2}{\\lambda_i(s)} = \\frac{(s - \\mu_i)^2}{\\sigma^4} \\alpha \\exp\\left(-\\frac{(s - \\mu_i)^{2}}{2 \\sigma^{2}}\\right)\n$$\n在工作点 $s_0$ 的总费雪信息是对所有 $N$ 个神经元的求和：\n$$\nJ(s_0) = \\sum_{i=1}^{N} \\frac{(s_0 - \\mu_i)^2}{\\sigma^4} \\alpha \\exp\\left(-\\frac{(s_0 - \\mu_i)^{2}}{2 \\sigma^{2}}\\right)\n$$\n该问题指定了当 $N \\to \\infty$ 时的密集平铺假设。偏好刺激 $\\{\\mu_i\\}$ 在长度为 $L$ 的区间上均匀分布，密度为 $\\rho = \\frac{N}{L}$。在此极限下，离散求和可以被替换为对变量 $\\mu$ 的积分：\n$$\n\\sum_{i=1}^{N} f(\\mu_i) \\to \\int f(\\mu) \\rho \\, d\\mu\n$$\n将此应用于 $J(s_0)$ 的表达式：\n$$\nJ(s_0) = \\int \\rho \\, \\frac{(s_0 - \\mu)^2}{\\sigma^4} \\alpha \\exp\\left(-\\frac{(s_0 - \\mu)^{2}}{2 \\sigma^{2}}\\right) d\\mu\n$$\n积分是在偏好刺激的范围内进行的，这是一个长度为 $L$ 的区间。然而，由于当 $|\\mu - s_0| \\gg \\sigma$ 时高斯项会迅速衰减，如果我们假设刺激范围 $L$ 远大于调谐宽度 $\\sigma$，我们可以通过将积分限扩展到 $(-\\infty, \\infty)$ 来近似该积分，而不会产生显著误差。在这种情况下，这是一个标准步骤。\n$$\nJ(s_0) \\approx \\rho \\alpha \\int_{-\\infty}^{\\infty} \\frac{(s_0 - \\mu)^2}{\\sigma^4} \\exp\\left(-\\frac{(s_0 - \\mu)^{2}}{2 \\sigma^{2}}\\right) d\\mu\n$$\n为了求解这个积分，我们进行变量替换。令 $x = \\frac{\\mu - s_0}{\\sigma}$。这意味着 $d\\mu = \\sigma dx$ 且 $(s_0 - \\mu)^2 = (-\\sigma x)^2 = \\sigma^2 x^2$。积分限保持不变。\n$$\nJ(s_0) \\approx \\rho \\alpha \\int_{-\\infty}^{\\infty} \\frac{\\sigma^2 x^2}{\\sigma^4} \\exp\\left(-\\frac{x^2}{2}\\right) (\\sigma \\, dx)\n$$\n$$\nJ(s_0) \\approx \\frac{\\rho \\alpha}{\\sigma} \\int_{-\\infty}^{\\infty} x^2 \\exp\\left(-\\frac{x^2}{2}\\right) dx\n$$\n积分 $\\int_{-\\infty}^{\\infty} x^2 \\exp\\left(-\\frac{x^2}{2}\\right) dx$ 是一个标准结果。它可以与标准正态分布 $N(0,1)$ 的方差相关联，该分布的概率密度函数为 $\\phi(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^2}{2}\\right)$。方差为 $1$，由 $E[X^2] - (E[X])^2 = \\int_{-\\infty}^{\\infty} x^2 \\phi(x) dx - 0^2 = 1$ 给出。因此：\n$$\n\\int_{-\\infty}^{\\infty} x^2 \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^2}{2}\\right) dx = 1\n$$\n这意味着：\n$$\n\\int_{-\\infty}^{\\infty} x^2 \\exp\\left(-\\frac{x^2}{2}\\right) dx = \\sqrt{2\\pi}\n$$\n将此代回 $J(s_0)$ 的表达式中：\n$$\nJ(s_0) \\approx \\frac{\\rho \\alpha}{\\sigma} \\sqrt{2\\pi}\n$$\n问题要求最终表达式用 $N$ 和 $L$ 表示。使用密度 $\\rho = \\frac{N}{L}$，我们有：\n$$\nJ(s_0) \\approx \\frac{N}{L} \\frac{\\alpha \\sqrt{2\\pi}}{\\sigma} = \\frac{\\sqrt{2\\pi} \\alpha N}{L \\sigma}\n$$\nOLE 的方差 $\\text{Var}(\\hat{s})$ 是费雪信息的倒数：\n$$\n\\text{Var}(\\hat{s}) = J(s_0)^{-1} \\approx \\left( \\frac{\\sqrt{2\\pi} \\alpha N}{L \\sigma} \\right)^{-1}\n$$\n这就给出了渐近方差的最终表达式：\n$$\n\\text{Var}(\\hat{s}) = \\frac{L \\sigma}{\\sqrt{2\\pi} \\alpha N}\n$$\n这个结果与工作点 $s_0$ 无关，这是偏好刺激的均匀密度以及在无限域上近似积分的结果。",
            "answer": "$$ \\boxed{ \\frac{L \\sigma}{\\sqrt{2\\pi} \\alpha N} } $$"
        },
        {
            "introduction": "超越单点估计，现代神经科学理论提出大脑以概率方式表征信息。本练习将介绍概率群体编码（Probabilistic Population Codes, PPCs）这一强大框架，在该框架下，神经活动编码了关于外部刺激的一个完整概率分布。你将推导一个简洁的数学法则，阐明传入的神经脉冲如何更新该分布的参数，从而展示神经群体如何实现完全的贝叶斯推断。 ",
            "id": "4055165",
            "problem": "考虑一个由概率群体编码 (PPC) 表示的二维潜变量 $\\mathbf{x} \\in \\mathbb{R}^{2}$。一个由 $M$ 个神经元组成的群体在一个固定的观测窗口内发出脉冲计数 $\\{n_{i}\\}_{i=1}^{M}$，并且这些计数在给定 $\\mathbf{x}$ 的条件下是条件独立的。假设对于每个神经元 $i$，其单个脉冲的对数似然贡献由以下形式的基函数近似：\n$$\n\\phi_{i}(\\mathbf{x}) \\;=\\; \\alpha_{i} \\;+\\; \\mathbf{u}_{i}^{\\top}\\mathbf{x} \\;-\\; \\frac{1}{2}\\,\\mathbf{x}^{\\top}\\mathbf{U}_{i}\\,\\mathbf{x},\n$$\n其中 $\\alpha_{i} \\in \\mathbb{R}$，$\\mathbf{u}_{i} \\in \\mathbb{R}^{2}$，且 $\\mathbf{U}_{i} \\in \\mathbb{R}^{2 \\times 2}$ 是对称半正定矩阵。总对数似然被建模为这些贡献在所有脉冲和神经元上的总和，因此（在不考虑一个与 $\\mathbf{x}$ 无关的加性项的情况下）观测似然服从\n$$\n\\ln p\\!\\left(\\{n_{i}\\}_{i=1}^{M}\\mid \\mathbf{x}\\right) \\;=\\; \\sum_{i=1}^{M} n_{i}\\,\\phi_{i}(\\mathbf{x}) \\;+\\; \\text{const}(\\{n_{i}\\}),\n$$\n其中 $\\text{const}(\\{n_{i}\\})$ 不依赖于 $\\mathbf{x}$。设 $\\mathbf{x}$ 的先验分布为高斯分布，其均值为 $\\boldsymbol{\\mu}_{0} \\in \\mathbb{R}^{2}$，协方差为 $\\boldsymbol{\\Sigma}_{0} \\in \\mathbb{R}^{2\\times 2}$，其自然（正则）参数定义为\n$$\n\\boldsymbol{\\Lambda}_{0} \\;=\\; \\boldsymbol{\\Sigma}_{0}^{-1}, \\qquad \\mathbf{h}_{0} \\;=\\; \\boldsymbol{\\Lambda}_{0}\\,\\boldsymbol{\\mu}_{0}.\n$$\n高斯分布的正则形式为\n$$\n\\ln p(\\mathbf{x}) \\;=\\; -\\frac{1}{2}\\,\\mathbf{x}^{\\top}\\boldsymbol{\\Lambda}_{0}\\,\\mathbf{x} \\;+\\; \\mathbf{h}_{0}^{\\top}\\mathbf{x} \\;+\\; \\text{const}.\n$$\n从这些组成部分以及将独立似然与先验结合的标准规则出发，构建能够产生关于 $\\mathbf{x}$ 的高斯后验分布的PPC，并推导出从脉冲计数 $\\{n_{i}\\}_{i=1}^{M}$ 到后验高斯自然参数 $(\\mathbf{h}, \\boldsymbol{\\Lambda})$ 的显式映射。将你的最终答案表示为 $\\{n_{i}\\}$、$\\{\\mathbf{u}_{i}\\}$、$\\{\\mathbf{U}_{i}\\}$ 和 $(\\mathbf{h}_{0}, \\boldsymbol{\\Lambda}_{0})$ 的闭式函数。不需要数值近似；仅提供解析表达式。",
            "solution": "为了找到潜变量 $\\mathbf{x}$ 的后验分布，我们使用贝叶斯定理。后验概率密度正比于似然和先验的乘积：\n$$\np(\\mathbf{x} \\mid \\{n_{i}\\}_{i=1}^{M}) \\;\\propto\\; p(\\{n_{i}\\}_{i=1}^{M} \\mid \\mathbf{x})\\,p(\\mathbf{x})\n$$\n由于似然和先验都以对数形式给出，处理对数后验更为方便。对数后验是对数似然和对数先验的和，再加上一个不依赖于 $\\mathbf{x}$ 的加性常数：\n$$\n\\ln p(\\mathbf{x} \\mid \\{n_{i}\\}_{i=1}^{M}) \\;=\\; \\ln p(\\{n_{i}\\}_{i=1}^{M} \\mid \\mathbf{x}) \\;+\\; \\ln p(\\mathbf{x}) \\;+\\; \\text{const}\n$$\n我们已知对数似然和对数先验的表达式。让我们将它们代入上述方程。\n$$\n\\ln p(\\mathbf{x} \\mid \\{n_{i}\\}_{i=1}^{M}) \\;=\\; \\left(\\sum_{i=1}^{M} n_{i}\\,\\phi_{i}(\\mathbf{x}) + \\text{const}_1\\right) \\;+\\; \\left(-\\frac{1}{2}\\,\\mathbf{x}^{\\top}\\boldsymbol{\\Lambda}_{0}\\,\\mathbf{x} + \\mathbf{h}_{0}^{\\top}\\mathbf{x} + \\text{const}_2\\right) \\;+\\; \\text{const}\n$$\n现在，代入 $\\phi_{i}(\\mathbf{x})$ 的定义：\n$$\n\\ln p(\\mathbf{x} \\mid \\{n_{i}\\}) \\;=\\; \\sum_{i=1}^{M} n_{i}\\left(\\alpha_{i} + \\mathbf{u}_{i}^{\\top}\\mathbf{x} - \\frac{1}{2}\\,\\mathbf{x}^{\\top}\\mathbf{U}_{i}\\,\\mathbf{x}\\right) \\;-\\; \\frac{1}{2}\\,\\mathbf{x}^{\\top}\\boldsymbol{\\Lambda}_{0}\\,\\mathbf{x} \\;+\\; \\mathbf{h}_{0}^{\\top}\\mathbf{x} \\;+\\; \\text{const}'\n$$\n其中所有不依赖于 $\\mathbf{x}$ 的常数都集中到单个项 $\\text{const}'$ 中。我们的目标是将此表达式重新排列成高斯对数密度的正则形式，即 $\\ln p(\\mathbf{x} \\mid \\{n_{i}\\}) = -\\frac{1}{2}\\mathbf{x}^{\\top}\\boldsymbol{\\Lambda}\\mathbf{x} + \\mathbf{h}^{\\top}\\mathbf{x} + \\text{const}''$。为此，我们将关于 $\\mathbf{x}$ 的二次项和一次项进行分组。\n\n首先，让我们展开这个和式：\n$$\n\\ln p(\\mathbf{x} \\mid \\{n_{i}\\}) \\;=\\; \\sum_{i=1}^{M} n_{i}\\alpha_{i} \\;+\\; \\sum_{i=1}^{M} n_{i}\\mathbf{u}_{i}^{\\top}\\mathbf{x} \\;-\\; \\sum_{i=1}^{M} n_{i}\\frac{1}{2}\\,\\mathbf{x}^{\\top}\\mathbf{U}_{i}\\,\\mathbf{x} \\;-\\; \\frac{1}{2}\\,\\mathbf{x}^{\\top}\\boldsymbol{\\Lambda}_{0}\\,\\mathbf{x} \\;+\\; \\mathbf{h}_{0}^{\\top}\\mathbf{x} \\;+\\; \\text{const}'\n$$\n项 $\\sum_{i=1}^{M} n_{i}\\alpha_{i}$ 不依赖于 $\\mathbf{x}$，可以并入总的常数项中。让我们根据对 $\\mathbf{x}$ 的依赖性对余下的项进行分组。\n\n关于 $\\mathbf{x}$ 的一次项是：\n$$\n\\left(\\sum_{i=1}^{M} n_{i}\\mathbf{u}_{i}^{\\top}\\mathbf{x}\\right) \\;+\\; \\mathbf{h}_{0}^{\\top}\\mathbf{x} \\;=\\; \\left(\\sum_{i=1}^{M} n_{i}\\mathbf{u}_{i}\\right)^{\\top}\\mathbf{x} \\;+\\; \\mathbf{h}_{0}^{\\top}\\mathbf{x} \\;=\\; \\left(\\mathbf{h}_{0} + \\sum_{i=1}^{M} n_{i}\\mathbf{u}_{i}\\right)^{\\top}\\mathbf{x}\n$$\n关于 $\\mathbf{x}$ 的二次项是：\n$$\n-\\frac{1}{2}\\left(\\sum_{i=1}^{M} n_{i}\\,\\mathbf{x}^{\\top}\\mathbf{U}_{i}\\,\\mathbf{x}\\right) \\;-\\; \\frac{1}{2}\\,\\mathbf{x}^{\\top}\\boldsymbol{\\Lambda}_{0}\\,\\mathbf{x} \\;=\\; -\\frac{1}{2}\\,\\mathbf{x}^{\\top}\\left(\\sum_{i=1}^{M} n_{i}\\mathbf{U}_{i}\\right)\\mathbf{x} \\;-\\; \\frac{1}{2}\\,\\mathbf{x}^{\\top}\\boldsymbol{\\Lambda}_{0}\\,\\mathbf{x} \\;=\\; -\\frac{1}{2}\\,\\mathbf{x}^{\\top}\\left(\\boldsymbol{\\Lambda}_{0} + \\sum_{i=1}^{M} n_{i}\\mathbf{U}_{i}\\right)\\mathbf{x}\n$$\n结合这些结果，对数后验可以写为：\n$$\n\\ln p(\\mathbf{x} \\mid \\{n_{i}\\}) \\;=\\; -\\frac{1}{2}\\,\\mathbf{x}^{\\top}\\left(\\boldsymbol{\\Lambda}_{0} + \\sum_{i=1}^{M} n_{i}\\mathbf{U}_{i}\\right)\\mathbf{x} \\;+\\; \\left(\\mathbf{h}_{0} + \\sum_{i=1}^{M} n_{i}\\mathbf{u}_{i}\\right)^{\\top}\\mathbf{x} \\;+\\; \\text{const}''\n$$\n这个表达式是正则形式的高斯分布的对数密度。通过将其与一般形式 $\\ln p(\\mathbf{x}) = -\\frac{1}{2}\\mathbf{x}^{\\top}\\boldsymbol{\\Lambda}\\mathbf{x} + \\mathbf{h}^{\\top}\\mathbf{x} + \\text{const}$ 进行比较，我们可以直接确定后验分布的自然参数 $(\\mathbf{h}, \\boldsymbol{\\Lambda})$。\n\n后验自然参数 $\\mathbf{h}$（势向量）是一次项的系数：\n$$\n\\mathbf{h} \\;=\\; \\mathbf{h}_{0} + \\sum_{i=1}^{M} n_{i}\\mathbf{u}_{i}\n$$\n后验自然参数 $\\boldsymbol{\\Lambda}$（精度矩阵）是二次项中的矩阵：\n$$\n\\boldsymbol{\\Lambda} \\;=\\; \\boldsymbol{\\Lambda}_{0} + \\sum_{i=1}^{M} n_{i}\\mathbf{U}_{i}\n$$\n这些表达式提供了在给定先验参数和神经元基函数的情况下，从脉冲计数 $\\{n_i\\}$ 到后验自然参数 $(\\mathbf{h}, \\boldsymbol{\\Lambda})$ 的显式映射。此更新规则在正则参数空间中是加性的，这是使用共轭先验在指数族中进行推断的一个典型特征。",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\mathbf{h}_{0} + \\sum_{i=1}^{M} n_{i} \\mathbf{u}_{i}  \\boldsymbol{\\Lambda}_{0} + \\sum_{i=1}^{M} n_{i} \\mathbf{U}_{i} \\end{pmatrix}}\n$$"
        }
    ]
}