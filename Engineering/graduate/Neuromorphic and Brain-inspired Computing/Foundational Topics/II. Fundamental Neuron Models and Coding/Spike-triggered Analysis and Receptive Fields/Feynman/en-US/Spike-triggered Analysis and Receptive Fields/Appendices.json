{
    "hands_on_practices": [
        {
            "introduction": "This first exercise provides a practical foundation for interpreting the results of a spike-triggered analysis. You will implement algorithms to extract key quantitative features—such as latency, duration, and effective integration time—from a given Spike-Triggered Average (STA) trace. This practice will solidify your understanding of how the shape of the STA relates to a neuron's temporal processing properties. ",
            "id": "4060247",
            "problem": "You are given discrete spike-triggered average (STA) traces across time lags and are asked to compute temporal latency and duration of the dominant temporal receptive field lobe by locating peaks in the STA, and to relate these quantities to synaptic integration properties. Work strictly in discrete time. Let the discrete time lags be defined by $ \\tau_i = (i - i_0)\\,\\Delta t $, where $ i \\in \\{0,1,\\dots,N-1\\} $, $ i_0 $ is the zero-lag index, and $ \\Delta t $ is the uniform sampling interval in milliseconds. Let the STA samples be given by a real-valued sequence $ s[i] $ of length $ N $. Use the following definitions and requirements to compute the requested quantities.\n\nFundamental base and definitions:\n- The spike-triggered average (STA) is the conditional expectation of the stimulus preceding a spike, $ \\mathrm{STA}(\\tau) = \\mathbb{E}[x(t-\\tau)\\,|\\,\\text{spike at }t] $, for a stimulus $ x(t) $. Under a linear filter with additive noise and a monotonic static nonlinearity, the STA is proportional to the linear temporal filter (temporal receptive field) when the stimulus is temporally white, hence its dominant lobe encodes the temporal sensitivity profile that most strongly drives spiking.\n- The temporal latency of the dominant lobe is the time lag at which the magnitude of the STA attains its largest absolute value, $ t_{\\mathrm{lat}} = \\tau_{i_{\\max}} $, where $ i_{\\max} = \\arg\\max_i |s[i]| $.\n- The duration of the dominant lobe is quantified by the full-width at half-maximum (FWHM) of that lobe. Let $ \\sigma = \\mathrm{sign}(s[i_{\\max}]) \\in \\{-1,+1\\} $ and let the dominant lobe be the maximal contiguous index set $ \\mathcal{L} = \\{ \\ell,\\ell+1,\\dots,r \\} $ containing $ i_{\\max} $ such that $ s[j]\\cdot\\sigma > 0 $ for all $ j \\in \\mathcal{L} $, bounded on each side by the first zero-crossing or the array boundary. Let $ a[j] = \\sigma\\, s[j] \\ge 0 $ on $ \\mathcal{L} $, and $ h = \\tfrac{1}{2}\\,a[i_{\\max}] $ be the half-maximum. Define the left and right half-maximum crossing times by linear interpolation on the discrete grid if they exist:\n  - Find the smallest $ k \\in \\{\\ell+1,\\dots,i_{\\max}\\} $ such that $ a[k-1] < h \\le a[k] $, then\n    $$ \\tau_{\\mathrm{L}} = \\tau_{k-1} + \\frac{h - a[k-1]}{a[k] - a[k-1]}\\,\\Delta t. $$\n  - Find the largest $ k \\in \\{ i_{\\max},\\dots,r-1 \\} $ such that $ a[k] \\ge h > a[k+1] $, then\n    $$ \\tau_{\\mathrm{R}} = \\tau_{k} + \\frac{h - a[k]}{a[k+1] - a[k]}\\,\\Delta t. $$\n  The FWHM duration is $ t_{\\mathrm{dur}} = \\tau_{\\mathrm{R}} - \\tau_{\\mathrm{L}} $ if both crossings exist. If exactly one side crossing exists (left or right), define $ t_{\\mathrm{dur}} $ as twice the one-sided half-width around the peak. If neither side exists, define $ t_{\\mathrm{dur}} = (r-\\ell+1)\\,\\Delta t $.\n- The effective synaptic integration window is quantified by the area-to-peak ratio over the dominant lobe,\n  $$ T_{\\mathrm{eff}} = \\frac{\\Delta t \\sum_{j=\\ell}^{r} a[j]}{a[i_{\\max}]}, $$\n  which has units of milliseconds and equals the effective time scale over which inputs contribute relative to the peak response, independent of the exact lobe shape. This measure arises because, for a nonnegative kernel $ k(t) $, the ratio $ \\int k(t)\\,dt / \\max_t k(t) $ is a width-like scale reflecting integration.\n- The sign of the dominant lobe encodes whether increases in the stimulus preceding a spike are excitatory-like or inhibitory-like in the linear regime: output an integer flag $ u \\in \\{-1,+1\\} $ where $ u=+1 $ if $ \\sigma=+1 $ and $ u=-1 $ if $ \\sigma=-1 $.\n\nComputational tasks:\nGiven $ \\Delta t $ in milliseconds, the zero-lag index $ i_0 $, and the STA samples $ s[i] $, compute the quadruple $ [t_{\\mathrm{lat}},\\, t_{\\mathrm{dur}},\\, T_{\\mathrm{eff}},\\, u] $ according to the definitions above. All time quantities must be expressed in milliseconds and rounded to three decimal places. The sign flag $ u $ must be an integer.\n\nTest suite:\nImplement your program to compute results for the following four cases. In each case, define $ \\tau_i = (i - i_0)\\,\\Delta t $ and construct $ s[i] $ as specified.\n\n- Case A (unimodal Gaussian lobe):\n  - $ N = 201 $, $ \\Delta t = 1.0 $, $ i_0 = 100 $.\n  - $ s[i] = \\exp\\!\\left( -\\tfrac{1}{2}\\,\\big(\\tfrac{\\tau_i - 25.0}{5.0}\\big)^2 \\right) $.\n\n- Case B (biphasic difference of Gaussians, dominant negative lobe):\n  - $ N = 181 $, $ \\Delta t = 1.0 $, $ i_0 = 90 $.\n  - $ s[i] = 0.8\\,\\exp\\!\\left( -\\tfrac{1}{2}\\,\\big(\\tfrac{\\tau_i - 35.0}{6.0}\\big)^2 \\right) - 1.2\\,\\exp\\!\\left( -\\tfrac{1}{2}\\,\\big(\\tfrac{\\tau_i - 10.0}{4.0}\\big)^2 \\right) $.\n\n- Case C (alpha-function synaptic kernel with delay):\n  - $ N = 151 $, $ \\Delta t = 1.0 $, $ i_0 = 50 $.\n  - Define $ d = 30.0 $ and $ \\tau_s = 8.0 $. Let\n    $$ s[i] =\n      \\begin{cases}\n        \\left(\\dfrac{\\tau_i - d}{\\tau_s}\\right)\\exp\\!\\left(1 - \\dfrac{\\tau_i - d}{\\tau_s}\\right), & \\text{if } \\tau_i \\ge d,\\\\\n        0, & \\text{if } \\tau_i < d.\n      \\end{cases}\n    $$\n\n- Case D (boundary case: single-sample lobe):\n  - $ N = 51 $, $ \\Delta t = 1.0 $, $ i_0 = 25 $.\n  - $ s[i] = 0 $ for all $ i $ except $ s[i_0 + 5] = 1.0 $.\n\nFinal output format:\nYour program should produce a single line of output containing a comma-separated list enclosed in square brackets, where each element corresponds to one test case in order A, B, C, D, and is itself a four-element list $ [t_{\\mathrm{lat}}, t_{\\mathrm{dur}}, T_{\\mathrm{eff}}, u] $. For example, the printed structure must look like $ [[\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot],\\dots] $ with all time quantities rounded to three decimal places and $ u $ as an integer. No additional text should be printed.",
            "solution": "The problem is scientifically valid, well-posed, and self-contained. It presents a standard task in computational neuroscience: the characterization of a temporal receptive field, approximated by a spike-triggered average (STA), using common metrics. The definitions for latency, duration, and integration time are precise and computationally tractable. The test cases provided are based on canonical models of neural response kernels (Gaussian, difference-of-Gaussians, alpha function), and include a boundary case to test the robustness of the algorithm. We will proceed with a solution.\n\nThe core task is to implement a computational procedure that, given a discrete STA trace $s[i]$, extracts a set of four characteristic features: the latency $t_{\\mathrm{lat}}$, the duration $t_{\\mathrm{dur}}$, the effective integration time $T_{\\mathrm{eff}}$, and the sign $u$ of the dominant lobe. The overall procedure can be broken down into the following steps, which directly follow the definitions provided.\n\n**Step 1: STA Signal Generation**\nFor each test case, we are given the number of samples $N$, the sampling interval $\\Delta t$, and the zero-lag index $i_0$. The first step is to construct the discrete time lag array $\\tau$ and the corresponding STA signal $s$.\nThe time lags are defined as:\n$$ \\tau_i = (i - i_0)\\,\\Delta t \\quad \\text{for } i \\in \\{0, 1, \\dots, N-1\\} $$\nUsing these time lags, the STA signal $s[i]$ is generated according to the specific function provided for each case (Gaussian, difference-of-Gaussians, etc.).\n\n**Step 2: Dominant Lobe Peak and Sign Identification**\nThe dominant lobe is defined by the peak of the STA's absolute magnitude.\nFirst, we find the index $i_{\\max}$ where $|s[i]|$ is maximal:\n$$ i_{\\max} = \\arg\\max_i |s[i]| $$\nThe temporal latency, $t_{\\mathrm{lat}}$, is the time lag corresponding to this peak:\n$$ t_{\\mathrm{lat}} = \\tau_{i_{\\max}} = (i_{\\max} - i_0)\\,\\Delta t $$\nThe sign of the dominant lobe, $\\sigma$, indicates whether it is excitatory-like (positive) or inhibitory-like (negative). It is determined by the sign of the STA at the peak, and the output flag $u$ is set accordingly:\n$$ \\sigma = \\mathrm{sign}(s[i_{\\max}]) $$\n$$ u = \\sigma $$\n\n**Step 3: Dominant Lobe Boundary Detection**\nThe dominant lobe is the contiguous region around the peak $i_{\\max}$ that has the same sign as the peak. We must find the maximal contiguous index set $\\mathcal{L} = \\{\\ell, \\ell+1, \\dots, r\\}$ containing $i_{\\max}$ such that $s[j] \\cdot \\sigma > 0$ for all $j \\in \\mathcal{L}$.\nThe left boundary $\\ell$ is found by starting at $i_{\\max}$ and searching backwards (decreasing index $j$) for the first index where $s[j] \\cdot \\sigma \\le 0$ or the array boundary at $j=0$ is reached. The index of the first point inside the lobe is $\\ell$.\nSimilarly, the right boundary $r$ is found by searching forwards (increasing index $j$) from $i_{\\max}$ for the first index where $s[j] \\cdot \\sigma \\le 0$ or the array boundary at $j=N-1$ is reached. The index of the last point inside the lobe is $r$.\n\n**Step 4: Effective Integration Time ($T_{\\mathrm{eff}}$)**\nThis metric quantifies the effective duration of the integration window. It is calculated as the ratio of the lobe's area to its peak height. To compute this, we first define a positive-valued version of the lobe, $a[j] = \\sigma \\cdot s[j]$ for $j \\in \\mathcal{L}$. The peak of this rectified lobe is $a[i_{\\max}] = |s[i_{\\max}]|$.\nThe area under the lobe is approximated by a discrete sum:\n$$ A_{\\mathrm{lobe}} = \\Delta t \\sum_{j=\\ell}^{r} a[j] = \\Delta t \\sum_{j=\\ell}^{r} \\sigma \\cdot s[j] $$\nThe effective integration time is then:\n$$ T_{\\mathrm{eff}} = \\frac{A_{\\mathrm{lobe}}}{a[i_{\\max}]} = \\frac{\\Delta t \\sum_{j=\\ell}^{r} a[j]}{a[i_{\\max}]} $$\n\n**Step 5: Lobe Duration ($t_{\\mathrm{dur}}$) via Full-Width at Half-Maximum (FWHM)**\nThe FWHM measures the width of the lobe at half of its maximum amplitude.\nFirst, we calculate the half-maximum height:\n$$ h = \\frac{1}{2} a[i_{\\max}] $$\nNext, we find the time points $\\tau_{\\mathrm{L}}$ and $\\tau_{\\mathrm{R}}$ where the rectified lobe $a[j]$ crosses this height $h$. Because the signal is discrete, these points are found by linear interpolation between adjacent samples.\n\nTo find the left crossing time $\\tau_{\\mathrm{L}}$, we search for the smallest index $k \\in \\{\\ell+1, \\dots, i_{\\max}\\}$ such that $a[k-1] < h \\le a[k]$. If such a $k$ is found, $\\tau_{\\mathrm{L}}$ is interpolated as:\n$$ \\tau_{\\mathrm{L}} = \\tau_{k-1} + \\frac{h - a[k-1]}{a[k] - a[k-1]}\\,\\Delta t $$\nTo find the right crossing time $\\tau_{\\mathrm{R}}$, we search for the largest index $k \\in \\{i_{\\max}, \\dots, r-1\\}$ such that $a[k] \\ge h > a[k+1]$. If such a $k$ is found, $\\tau_{\\mathrm{R}}$ is interpolated as:\n$$ \\tau_{\\mathrm{R}} = \\tau_{k} + \\frac{h - a[k]}{a[k+1] - a[k]}\\,\\Delta t $$\n\nThe duration $t_{\\mathrm{dur}}$ is then determined based on which crossings were found:\n1.  If both $\\tau_{\\mathrm{L}}$ and $\\tau_{\\mathrm{R}}$ exist, $t_{\\mathrm{dur}} = \\tau_{\\mathrm{R}} - \\tau_{\\mathrm{L}}$.\n2.  If only $\\tau_{\\mathrm{L}}$ exists, $t_{\\mathrm{dur}} = 2 \\cdot (\\tau_{i_{\\max}} - \\tau_{\\mathrm{L}})$.\n3.  If only $\\tau_{\\mathrm{R}}$ exists, $t_{\\mathrm{dur}} = 2 \\cdot (\\tau_{\\mathrm{R}} - \\tau_{i_{\\max}})$.\n4.  If neither crossing exists (e.g., a very narrow, single-sample peak), $t_{\\mathrm{dur}} = (r - \\ell + 1) \\cdot \\Delta t$.\n\nThis comprehensive procedure computes the four required quantities for any given STA trace. It is implemented for each of the four test cases specified. The resulting time-based quantities ($t_{\\mathrm{lat}}, t_{\\mathrm{dur}}, T_{\\mathrm{eff}}$) are rounded to three decimal places as required.",
            "answer": "```python\nimport numpy as np\n\ndef compute_sta_features(N, dt, i0, s_func):\n    \"\"\"\n    Computes temporal features of a spike-triggered average (STA) trace.\n    \"\"\"\n    # Step 1: STA Signal Generation\n    indices = np.arange(N)\n    tau = (indices - i0) * dt\n    s = s_func(tau)\n\n    # Handle all-zero STA gracefully, though not expected in tests.\n    if not np.any(s):\n        return [0.0, 0.0, 0.0, 1]\n\n    # Step 2: Dominant Lobe Peak and Sign Identification\n    i_max = np.argmax(np.abs(s))\n    t_lat = (i_max - i0) * dt\n    s_peak = s[i_max]\n    sigma = np.sign(s_peak)\n    if sigma == 0: sigma = 1  # Convention for zero-peak case\n    u = int(sigma)\n\n    # Step 3: Dominant Lobe Boundary Detection\n    l, r = i_max, i_max\n    # Find left boundary\n    if l > 0:\n        for i in range(i_max - 1, -1, -1):\n            if s[i] * sigma > 0:\n                l = i\n            else:\n                break\n    # Find right boundary\n    if r  N - 1:\n        for i in range(i_max + 1, N):\n            if s[i] * sigma > 0:\n                r = i\n            else:\n                break\n\n    # Step 4: Effective Integration Time (T_eff)\n    lobe_indices = np.arange(l, r + 1)\n    a_lobe_values = s[lobe_indices] * sigma\n    a_max = s_peak * sigma\n    \n    if a_max == 0:\n        T_eff = 0.0\n    else:\n        T_eff = dt * np.sum(a_lobe_values) / a_max\n    \n    # Step 5: Lobe Duration (t_dur) via FWHM\n    h = 0.5 * a_max\n    \n    a_full = s * sigma  # Rectified full signal\n    tau_L, tau_R = None, None\n\n    # Find left crossing (search from lobe start up to the peak)\n    for k in range(l + 1, i_max + 1):\n        if a_full[k-1]  h = a_full[k]:\n            if a_full[k] - a_full[k-1] != 0:\n                tau_L = tau[k-1] + (h - a_full[k-1]) / (a_full[k] - a_full[k-1]) * dt\n            else:  # Flat region at half-max\n                tau_L = tau[k]\n            break\n\n    # Find right crossing (search from lobe end down to the peak)\n    # The problem asks for the largest k, so searching backwards and stopping works.\n    for k in range(r - 1, i_max - 1, -1):\n        if a_full[k] >= h > a_full[k+1]:\n            if a_full[k+1] - a_full[k] != 0:\n                tau_R = tau[k] + (h - a_full[k]) / (a_full[k+1] - a_full[k]) * dt\n            else:\n                tau_R = tau[k]\n            break\n\n    # Determine t_dur based on which crossings were found\n    t_peak = tau[i_max]\n    if tau_L is not None and tau_R is not None:\n        t_dur = tau_R - tau_L\n    elif tau_L is not None:\n        t_dur = 2 * (t_peak - tau_L)\n    elif tau_R is not None:\n        t_dur = 2 * (tau_R - t_peak)\n    else: # Neither crossing exists (e.g., single-point lobe)\n        t_dur = (r - l + 1) * dt\n\n    return [\n        round(t_lat, 3),\n        round(t_dur, 3),\n        round(T_eff, 3),\n        u\n    ]\n\ndef solve():\n    \"\"\"\n    Main function to run the test cases and print the results.\n    \"\"\"\n    test_cases = [\n        # Case A: unimodal Gaussian lobe\n        {\n            \"N\": 201, \"dt\": 1.0, \"i0\": 100,\n            \"s_func\": lambda tau: np.exp(-0.5 * ((tau - 25.0) / 5.0)**2)\n        },\n        # Case B: biphasic difference of Gaussians, dominant negative lobe\n        {\n            \"N\": 181, \"dt\": 1.0, \"i0\": 90,\n            \"s_func\": lambda tau: 0.8 * np.exp(-0.5 * ((tau - 35.0) / 6.0)**2) - 1.2 * np.exp(-0.5 * ((tau - 10.0) / 4.0)**2)\n        },\n        # Case C: alpha-function synaptic kernel with delay\n        {\n            \"N\": 151, \"dt\": 1.0, \"i0\": 50,\n            \"s_func\": lambda tau, d=30.0, tau_s=8.0: np.piecewise(\n                tau, [tau  d], [0.0, lambda t: ((t - d) / tau_s) * np.exp(1 - (t - d) / tau_s)]\n            )\n        },\n        # Case D: boundary case: single-sample lobe\n        {\n            \"N\": 51, \"dt\": 1.0, \"i0\": 25,\n            \"s_func\": lambda tau: np.where(tau == 5.0, 1.0, 0.0)\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = compute_sta_features(case[\"N\"], case[\"dt\"], case[\"i0\"], case[\"s_func\"])\n        results.append(result)\n\n    # Format the output string as specified in the problem\n    result_str = \"[\" + \",\".join([f\"[{r[0]},{r[1]},{r[2]},{r[3]}]\" for r in results]) + \"]\"\n\n    print(result_str)\n\nsolve()\n```"
        },
        {
            "introduction": "The validity of the STA as an estimator for a neuron's receptive field relies on specific assumptions about the stimulus, particularly its statistical stationarity. This simulation-based exercise explores what happens when these assumptions are violated, specifically in the context of changing stimulus contrast. By comparing STA estimation with and without divisive normalization, you will gain insight into why this form of gain control is crucial for robust neural coding and analysis. ",
            "id": "4060228",
            "problem": "You are given a discrete-time simulation framework to study spike-triggered analysis and receptive field estimation under contrast normalization. Consider a linear–nonlinear–Poisson model for a single neuron. In each time bin $t \\in \\{1,\\ldots,T\\}$, a stimulus vector $x_t \\in \\mathbb{R}^d$ is drawn from a zero-mean Gaussian distribution with time-varying per-frame variance. The neuron’s synaptic filter (the ground-truth receptive field) is $w \\in \\mathbb{R}^d$ with $\\|w\\|_2 = 1$. The instantaneous drive is $u_t = w^\\top x_t$, and the conditional spike count $s_t \\in \\{0,1,2,\\ldots\\}$ is sampled from a Poisson distribution with rate\n$$\n\\lambda_t = r_0 \\exp\\left(\\beta u_t\\right),\n$$\nwhere $r_0  0$ is a baseline rate and $\\beta  0$ is the nonlinearity slope. The spike-triggered average (STA) estimator is defined for any stimulus representation $z_t \\in \\mathbb{R}^d$ as\n$$\n\\widehat{w}_{\\text{STA}}(z) = \\frac{\\sum_{t=1}^T s_t z_t}{\\sum_{t=1}^T s_t},\n$$\nprovided $\\sum_{t=1}^T s_t  0$; otherwise, define $\\widehat{w}_{\\text{STA}}(z) = 0\\cdot \\mathbf{1}_d$.\n\nDivisive contrast normalization is applied per frame via\n$$\nz_t^{\\text{norm}} = \\frac{x_t}{\\sigma_t + \\varepsilon},\n$$\nwhere $\\sigma_t$ is the empirical standard deviation of the coordinates of $x_t$ (computed over its $d$ entries) and $\\varepsilon  0$ is a small constant to prevent division by values near zero. The unnormalized representation is $z_t^{\\text{raw}} = x_t$.\n\nYour task is to simulate stimuli with alternating-contrast segments and quantify the effect of divisive normalization on receptive field estimation by comparing $\\widehat{w}_{\\text{STA}}(z^{\\text{raw}})$ and $\\widehat{w}_{\\text{STA}}(z^{\\text{norm}})$ to the ground-truth $w$. To ensure scientific realism and a principled derivation base, use the following:\n\n- Fundamental definition of the spike-triggered average: $\\widehat{w}_{\\text{STA}}(z)$ as given above.\n- Poisson spiking with rate $\\lambda_t$ as specified, a well-tested generative model in neural encoding.\n- Gaussian stimulus generation with heteroscedastic per-frame variance, alternating between a “low-contrast” variance $v_{\\text{low}}$ and a “high-contrast” variance $v_{\\text{high}}$ in contiguous segments.\n\nFor each test case, generate the following:\n\n- Draw $w \\sim \\mathcal{N}(0, I_d)$ and then set $w \\leftarrow w/\\|w\\|_2$.\n- For each time bin $t$, sample $x_t \\sim \\mathcal{N}(0, v_t I_d)$, where $v_t \\in \\{v_{\\text{low}}, v_{\\text{high}}\\}$ alternates every segment of length $L$ bins, starting with $v_{\\text{low}}$.\n- Compute $u_t = w^\\top x_t$ and sample $s_t \\sim \\text{Poisson}(\\lambda_t)$ with $\\lambda_t = r_0 \\exp(\\beta u_t)$.\n- Compute the spike-triggered averages $\\widehat{w}_{\\text{STA}}(z^{\\text{raw}})$ and $\\widehat{w}_{\\text{STA}}(z^{\\text{norm}})$.\n- Quantify estimation quality by two metrics:\n  1. Cosine similarity with $w$:\n     $$\n     \\text{cos}(w,\\widehat{w}) = \\frac{w^\\top \\widehat{w}}{\\|w\\|_2 \\|\\widehat{w}\\|_2},\n     $$\n     with the convention that if $\\|\\widehat{w}\\|_2 = 0$ then $\\text{cos}(w,\\widehat{w}) = 0$.\n  2. Scale-invariant normalized mean squared error (NMSE):\n     $$\n     c^\\star = \\arg\\min_c \\left\\| \\widehat{w} - c w \\right\\|_2^2 = \\frac{w^\\top \\widehat{w}}{\\|w\\|_2^2}, \\quad\n     \\text{NMSE}(\\widehat{w},w) = \\frac{\\left\\| \\widehat{w} - c^\\star w \\right\\|_2}{\\|w\\|_2}.\n     $$\n     Since $\\|w\\|_2 = 1$ by construction, this reduces to $\\text{NMSE}(\\widehat{w},w) = \\sqrt{\\|\\widehat{w}\\|_2^2 - (w^\\top \\widehat{w})^2}$.\n\nFor each test case, report two floats:\n- $\\Delta_{\\text{cos}} = \\text{cos}(w,\\widehat{w}_{\\text{STA}}(z^{\\text{norm}})) - \\text{cos}(w,\\widehat{w}_{\\text{STA}}(z^{\\text{raw}}))$,\n- $\\Delta_{\\text{NMSE}} = \\text{NMSE}(\\widehat{w}_{\\text{STA}}(z^{\\text{raw}}),w) - \\text{NMSE}(\\widehat{w}_{\\text{STA}}(z^{\\text{norm}}),w)$.\nPositive values indicate improvement from normalization.\n\nTest Suite:\nUse the following parameter sets, where all scalar quantities are in arbitrary dimensionless units:\n\n- Case $1$: $d=32$, $T=20000$, $L=1000$, $v_{\\text{low}}=0.25$, $v_{\\text{high}}=4.0$, $\\beta=0.15$, $r_0=0.5$, $\\varepsilon=0.05$, seed $=0$.\n- Case $2$: $d=64$, $T=30000$, $L=1000$, $v_{\\text{low}}=0.10$, $v_{\\text{high}}=6.0$, $\\beta=0.15$, $r_0=0.5$, $\\varepsilon=0.001$, seed $=1$.\n- Case $3$: $d=32$, $T=20000$, $L=1000$, $v_{\\text{low}}=1.0$, $v_{\\text{high}}=1.0$, $\\beta=0.15$, $r_0=0.5$, $\\varepsilon=0.05$, seed $=2$.\n- Case $4$: $d=32$, $T=15000$, $L=750$, $v_{\\text{low}}=0.01$, $v_{\\text{high}}=0.5$, $\\beta=0.15$, $r_0=0.5$, $\\varepsilon=0.20$, seed $=3$.\n- Case $5$: $d=16$, $T=4000$, $L=200$, $v_{\\text{low}}=0.20$, $v_{\\text{high}}=5.0$, $\\beta=0.15$, $r_0=0.5$, $\\varepsilon=0.05$, seed $=4$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each entry corresponds to one test case and must itself be a two-element list $[\\Delta_{\\text{cos}}, \\Delta_{\\text{NMSE}}]$. For example, an output line with three cases would look like $[[0.123,-0.045],[0.002,0.010],[0.000,0.000]]$.",
            "solution": "The user has provided a problem statement that is valid. It is scientifically grounded in the principles of computational neuroscience, specifically using the Linear-Nonlinear-Poisson (LNP) model for neural encoding and spike-triggered average (STA) for receptive field estimation. The problem is well-posed, with all parameters, models, and procedures clearly and unambiguously defined. The use of a random seed for each test case ensures that the simulation is deterministic and reproducible. The setup is mathematically and computationally sound. Therefore, I will proceed with a full solution.\n\nThe problem asks to quantify the effect of divisive contrast normalization on the accuracy of receptive field estimation using the spike-triggered average. The simulation framework is based on a single neuron whose spiking activity is described by an LNP model. The core of the problem lies in comparing the STA computed from raw stimuli to the STA computed from stimuli that have been normalized frame-by-frame.\n\nThe theoretical foundation for this analysis rests on how the STA behaves under different stimulus statistics. The STA estimator for a stimulus representation $z$ is given by:\n$$\n\\widehat{w}_{\\text{STA}}(z) = \\frac{\\sum_{t=1}^T s_t z_t}{\\sum_{t=1}^T s_t}\n$$\nwhere $s_t$ is the spike count at time $t$. The expectation of the numerator is $E[\\sum_t s_t z_t] = \\sum_t E[s_t z_t]$. Since $s_t \\sim \\text{Poisson}(\\lambda_t)$, we have $E[s_t | z_t] = \\lambda_t$. Therefore, $E[s_t z_t] = E[E[s_t z_t | z_t]] = E[\\lambda_t z_t]$.\n\nThe spiking rate is $\\lambda_t = r_0 \\exp(\\beta u_t) = r_0 \\exp(\\beta w^\\top x_t)$, where $w$ is the true receptive field. The stimulus $x_t$ is drawn from a Gaussian distribution with a time-varying covariance, $x_t \\sim \\mathcal{N}(0, v_t I_d)$. The variance $v_t$ alternates between a low value, $v_{\\text{low}}$, and a high value, $v_{\\text{high}}$, creating non-stationary stimulus conditions (alternating contrast).\n\nFor the unnormalized (raw) case, $z_t^{\\text{raw}} = x_t$. The expectation term becomes $E[\\lambda_t x_t] = r_0 E[x_t \\exp(\\beta w^\\top x_t)]$. A key result for Gaussian variables, known as Stein's Lemma or Price's Theorem, states that for $x \\sim \\mathcal{N}(0, C)$, $E[x g(x)] = C E[\\nabla g(x)]$. Applying a related identity, it can be shown that $E[x_t \\exp(\\beta w^\\top x_t)]$ is proportional to $v_t w$. Because $v_t$ is not constant, the contributions from high-contrast segments (large $v_t$), where stimuli have larger magnitudes, will dominate the sum $\\sum_t s_t x_t$. This effectively biases the STA estimate towards the statistics of the high-contrast stimuli, potentially corrupting the estimate of $w$.\n\nDivisive normalization is a mechanism to counteract such effects. The normalized stimulus is $z_t^{\\text{norm}} = \\frac{x_t}{\\sigma_t + \\varepsilon}$, where $\\sigma_t$ is the empirical standard deviation of the components of the stimulus vector $x_t$ at time $t$. Since the components of $x_t$ are i.i.d. draws from $\\mathcal{N}(0, v_t)$, the expected value of $\\sigma_t^2$ is proportional to the variance $v_t$. Thus, dividing by $\\sigma_t$ approximately cancels the effect of the fluctuating variance, making the statistics of the normalized stimulus $z_t^{\\text{norm}}$ more stationary over time. This stabilization of stimulus statistics is hypothesized to lead to a more accurate STA estimate, particularly in its direction.\n\nThe simulation proceeds as follows for each test case:\n1.  **Initialization**: A random number generator is seeded for reproducibility. The ground-truth receptive field $w \\in \\mathbb{R}^d$ is generated by drawing from an isotropic Gaussian $\\mathcal{N}(0, I_d)$ and normalizing its $L_2$-norm to $1$.\n2.  **Stimulus Generation**: A time series of variances, $\\{v_t\\}_{t=1}^T$, is created, alternating between $v_{\\text{low}}$ and $v_{\\text{high}}$ in segments of length $L$. Then, for each time $t$, a stimulus vector $x_t$ is drawn from $\\mathcal{N}(0, v_t I_d)$.\n3.  **Spike Generation**: For each stimulus $x_t$, the linear drive $u_t = w^\\top x_t$ is computed. This drive modulates the rate of a Poisson process via the exponential nonlinearity $\\lambda_t = r_0 \\exp(\\beta u_t)$. The spike count $s_t$ is then sampled from $\\text{Poisson}(\\lambda_t)$.\n4.  **STA Calculation**: If the total number of spikes $\\sum_t s_t$ is positive, two STA estimates are computed:\n    -   $\\widehat{w}_{\\text{STA}}(z^{\\text{raw}}) = \\frac{\\sum_t s_t x_t}{\\sum_t s_t}$\n    -   $\\widehat{w}_{\\text{STA}}(z^{\\text{norm}}) = \\frac{\\sum_t s_t z_t^{\\text{norm}}}{\\sum_t s_t}$, where $z_t^{\\text{norm}} = x_t / (\\text{std}(x_t) + \\varepsilon)$.\n5.  **Performance Evaluation**: The quality of each estimate $\\widehat{w}$ is assessed relative to the true filter $w$ using two metrics:\n    -   Cosine Similarity: $\\text{cos}(w, \\widehat{w}) = \\frac{w^\\top \\widehat{w}}{\\|\\widehat{w}\\|_2}$, since $\\|w\\|_2=1$. This measures the alignment of the estimated and true filters.\n    -   Scale-Invariant Normalized Mean Squared Error (NMSE): $\\text{NMSE}(\\widehat{w},w) = \\sqrt{\\|\\widehat{w}\\|_2^2 - (w^\\top \\widehat{w})^2}$. This measures the magnitude of the component of $\\widehat{w}$ that is orthogonal to $w$.\n6.  **Result Reporting**: The final outputs for each case are the differences in these metrics, $\\Delta_{\\text{cos}} = \\text{cos}_{\\text{norm}} - \\text{cos}_{\\text{raw}}$ and $\\Delta_{\\text{NMSE}} = \\text{NMSE}_{\\text{raw}} - \\text{NMSE}_{\\text{norm}}$. Positive values for these quantities indicate that divisive normalization improved the receptive field estimate. For the control case where $v_{\\text{low}} = v_{\\text{high}}$, we expect these differences to be small and possibly negative, as normalization is unnecessary and may introduce minor distortions.\n\nThe implementation will use the `numpy` library for all vector and matrix operations, including random number generation, to ensure efficiency.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation for all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        # (d, T, L, v_low, v_high, beta, r0, epsilon, seed)\n        (32, 20000, 1000, 0.25, 4.0, 0.15, 0.5, 0.05, 0),\n        (64, 30000, 1000, 0.10, 6.0, 0.15, 0.5, 0.001, 1),\n        (32, 20000, 1000, 1.0, 1.0, 0.15, 0.5, 0.05, 2),\n        (32, 15000, 750, 0.01, 0.5, 0.15, 0.5, 0.20, 3),\n        (16, 4000, 200, 0.20, 5.0, 0.15, 0.5, 0.05, 4),\n    ]\n\n    results = []\n    for params in test_cases:\n        delta_cos, delta_nmse = run_simulation(*params)\n        results.append([delta_cos, delta_nmse])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef run_simulation(d, T, L, v_low, v_high, beta, r0, epsilon, seed):\n    \"\"\"\n    Runs a single simulation case for spike-triggered analysis.\n\n    Args:\n        d (int): Dimensionality of the stimulus space.\n        T (int): Total number of time bins.\n        L (int): Length of each contrast segment.\n        v_low (float): Variance for low-contrast segments.\n        v_high (float): Variance for high-contrast segments.\n        beta (float): Slope of the exponential nonlinearity.\n        r0 (float): Baseline spike rate.\n        epsilon (float): Regularization constant for normalization.\n        seed (int): Seed for the random number generator.\n\n    Returns:\n        tuple[float, float]: A tuple containing (delta_cos, delta_nmse).\n    \"\"\"\n    rng = np.random.default_rng(seed)\n\n    # 1. Generate ground-truth receptive field w\n    w = rng.normal(size=d)\n    w /= np.linalg.norm(w)\n\n    # 2. Generate stimulus X with alternating contrast\n    v_t = np.ones(T)\n    for i in range(0, T, L):\n        is_low_contrast = (i // L) % 2 == 0\n        v_t[i:i+L] = v_low if is_low_contrast else v_high\n    \n    # Generate stimuli X ~ N(0, v_t * I_d)\n    # This is done by scaling standard normal samples by sqrt(v_t)\n    sqrt_v_t = np.sqrt(v_t)\n    X = rng.normal(size=(T, d)) * sqrt_v_t[:, np.newaxis]\n\n    # 3. Generate spikes s\n    u_t = X @ w\n    lambda_t = r0 * np.exp(beta * u_t)\n    s_t = rng.poisson(lambda_t)\n    \n    total_spikes = np.sum(s_t)\n\n    if total_spikes == 0:\n        return 0.0, 0.0\n\n    # 4. Compute Spike-Triggered Averages (STAs)\n    # Raw STA\n    w_hat_raw_num = s_t @ X\n    w_hat_raw = w_hat_raw_num / total_spikes\n    \n    # Normalized STA\n    # The problem asks for empirical stddev of coordinates of x_t.\n    # np.std computes this along an axis. ddof=0 is the default in older numpy\n    # and corresponds to the direct definition.\n    sigma_t = np.std(X, axis=1, ddof=0)\n    Z_norm = X / (sigma_t[:, np.newaxis] + epsilon)\n    \n    w_hat_norm_num = s_t @ Z_norm\n    w_hat_norm = w_hat_norm_num / total_spikes\n\n    # 5. Calculate performance metrics\n    cos_raw = calculate_cosine_similarity(w, w_hat_raw)\n    nmse_raw = calculate_nmse(w, w_hat_raw)\n    \n    cos_norm = calculate_cosine_similarity(w, w_hat_norm)\n    nmse_norm = calculate_nmse(w, w_hat_norm)\n\n    # 6. Calculate the differences\n    delta_cos = cos_norm - cos_raw\n    delta_nmse = nmse_raw - nmse_norm\n    \n    return delta_cos, delta_nmse\n\ndef calculate_cosine_similarity(w, w_hat):\n    \"\"\"\n    Computes the cosine similarity between two vectors.\n    Assumes norm(w) = 1.\n    \"\"\"\n    norm_w_hat = np.linalg.norm(w_hat)\n    if norm_w_hat == 0:\n        return 0.0\n    \n    # Since norm(w) = 1, cos = (w.T @ w_hat) / norm(w_hat)\n    return (w @ w_hat) / norm_w_hat\n\ndef calculate_nmse(w, w_hat):\n    \"\"\"\n    Computes the scale-invariant normalized mean squared error.\n    Assumes norm(w) = 1.\n    \"\"\"\n    norm_w_hat_sq = np.linalg.norm(w_hat)**2\n    w_dot_w_hat_sq = (w @ w_hat)**2\n    \n    # NMSE^2 = ||w_hat||^2 - (w.T @ w_hat)^2\n    # The term inside the sqrt can be negative due to float precision.\n    val = norm_w_hat_sq - w_dot_w_hat_sq\n    return np.sqrt(max(0, val))\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "The Spike-Triggered Average is not just a statistical tool; it can also be understood as an outcome of biological learning processes. This practice connects STA to synaptic plasticity by simulating a simple, correlation-based learning rule. You will observe how a synaptic weight vector dynamically evolves and converges to a filter proportional to the spike-triggered mean, providing a powerful mechanistic interpretation for the receptive fields we measure. ",
            "id": "4060251",
            "problem": "Consider a discrete-time synaptic learning scenario in neuromorphic and brain-inspired computing, where both pre-synaptic and post-synaptic spike trains are modeled as Poisson processes in small time bins. Let the pre-synaptic input at time index $t$ be a $d$-dimensional nonnegative integer vector $x_t \\in \\mathbb{N}^d$, where each coordinate $x_{t,i}$ is an independent draw from a Poisson distribution with rate $\\lambda_i  0$. Define the centered pre-synaptic activity $z_t = x_t - \\lambda$, where $\\lambda \\in \\mathbb{R}^d$ is the vector of rates. Let the post-synaptic spike indicator be $s_t \\in \\{0,1\\}$, where $s_t = 1$ indicates a spike in bin $t$, with a Bernoulli probability $p_t$ given by a logistic nonlinearity\n$$\np_t = \\sigma\\!\\left(b + g\\, w_\\ast^\\top z_t \\right),\n$$\nwhere $w_\\ast \\in \\mathbb{R}^d$ is a fixed but unknown target filter direction, $b \\in \\mathbb{R}$ is a bias, $g0$ is a gain, and $\\sigma(u) = \\frac{1}{1 + e^{-u}}$ is the logistic function. This Bernoulli spiking in small bins is consistent with an inhomogeneous Poisson process model for post-synaptic spiking.\n\nA synaptic weight vector $w_t \\in \\mathbb{R}^d$ evolves according to a correlation-based update with homeostatic decay\n$$\nw_{t+1} = w_t + \\eta \\left( s_t z_t - \\delta\\, w_t \\right),\n$$\nwhere $\\eta  0$ is a learning rate and $\\delta  0$ is a decay coefficient. Define the spike-triggered mean (STM) of the centered input as\n$$\nm = \\mathbb{E}\\left[ z_t \\mid s_t = 1 \\right],\n$$\nand its empirical estimator\n$$\n\\hat{m} = \\frac{1}{N_{\\text{spike}}} \\sum_{t: s_t = 1} z_t,\n$$\nwith $N_{\\text{spike}}$ the number of time bins where $s_t = 1$.\n\nStarting from the fundamental definitions of the Poisson and Bernoulli processes and the specified learning rule, derive from first principles why the expected update aligns the synaptic weight direction with the spike-triggered mean $\\hat{m}$, up to a proportionality constant set by the decay $\\delta$. Then implement a simulation that estimates $w_T$ and $\\hat{m}$ for a large number of time steps $T$, and quantifies convergence by the cosine similarity\n$$\n\\mathrm{cos\\_sim}(w_T, \\hat{m}) = \\frac{w_T^\\top \\hat{m}}{\\|w_T\\|_2 \\, \\|\\hat{m}\\|_2},\n$$\nwhenever both norms are nonzero. In the case that $\\|\\hat{m}\\|_2 = 0$, the STM is ill-defined in direction; in that case quantify convergence by the final norm $\\|w_T\\|_2$ instead.\n\nYour program must implement the simulation for the three test cases below, using the specified random seed for reproducibility, and produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case’s result is a single float computed as specified.\n\nDefinitions and assumptions to use in the derivation and program:\n- The pre-synaptic inputs $x_{t,i}$ are independent across dimensions and time, with $x_{t,i} \\sim \\mathrm{Poisson}(\\lambda_i)$.\n- The centered inputs are $z_t = x_t - \\lambda$ so that $\\mathbb{E}[z_t] = 0$.\n- The post-synaptic spike $s_t \\sim \\mathrm{Bernoulli}(p_t)$ with $p_t = \\sigma(b + g\\, w_\\ast^\\top z_t)$.\n- The learning rule is $w_{t+1} = w_t + \\eta ( s_t z_t - \\delta w_t )$.\n\nTest suite:\n1. General case (happy path): $d = 32$, $T = 120{,}000$, $\\lambda_i = 5$ for all $i$, $b = -0.75$, $g = 0.15$, $\\eta = 0.01$, $\\delta = 0.01$, random seed $123$, and $w_\\ast$ a random unit-norm vector drawn from a standard normal distribution and normalized to unit length. Output the cosine similarity $\\mathrm{cos\\_sim}(w_T, \\hat{m})$ as a float.\n2. Independence edge case: $d = 32$, $T = 120{,}000$, $\\lambda_i = 5$ for all $i$, $b = -0.75$, $g = 0.0$ (making post-synaptic spikes independent of $z_t$), $\\eta = 0.01$, $\\delta = 0.01$, random seed $456$, and $w_\\ast = 0$ (the zero vector). Output the final weight norm $\\|w_T\\|_2$ as a float.\n3. Low-rate boundary case: $d = 32$, $T = 200{,}000$, $\\lambda_i = 5$ for all $i$, $b = -3.0$, $g = 0.5$, $\\eta = 0.005$, $\\delta = 0.01$, random seed $789$, and $w_\\ast$ a random unit-norm vector drawn from a standard normal distribution and normalized to unit length. Output the cosine similarity $\\mathrm{cos\\_sim}(w_T, \\hat{m})$ as a float.\n\nFinal output format requirement:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3]\"), in the order of the three test cases listed above.",
            "solution": "The problem statement is validated as scientifically grounded, well-posed, objective, and complete. It describes a standard scenario in computational neuroscience and neuromorphic engineering. All parameters and conditions are specified, allowing for both a theoretical derivation and a reproducible numerical simulation.\n\n### Theoretical Derivation\n\nThe objective is to demonstrate from first principles that the learning rule aligns the synaptic weight vector $w_t$ with the spike-triggered mean (STM) of the centered input, $m = \\mathbb{E}[ z_t \\mid s_t = 1 ]$. The empirical estimator of the STM is $\\hat{m}$, and for a sufficiently large number of time steps $T$, we expect $\\hat{m} \\approx m$. Therefore, showing alignment with $m$ implies eventual alignment with $\\hat{m}$.\n\nThe synaptic weight vector $w_t \\in \\mathbb{R}^d$ evolves according to the correlation-based update rule with a homeostatic decay term:\n$$\nw_{t+1} = w_t + \\eta \\left( s_t z_t - \\delta\\, w_t \\right)\n$$\nwhere $\\eta  0$ is the learning rate, $\\delta  0$ is the decay coefficient, $s_t \\in \\{0, 1\\}$ is the post-synaptic spike indicator, and $z_t \\in \\mathbb{R}^d$ is the centered pre-synaptic activity.\n\nTo understand the long-term behavior of $w_t$, we analyze the expected change in the weights over one time step. We take the expectation of the update, $\\Delta w_t = w_{t+1} - w_t$, with respect to the distributions of the stochastic variables $z_t$ and $s_t$. The expectation is conditioned on the current weight state $w_t$.\n$$\n\\mathbb{E}[\\Delta w_t \\mid w_t] = \\mathbb{E}[w_{t+1} - w_t \\mid w_t] = \\mathbb{E}\\left[ \\eta (s_t z_t - \\delta w_t) \\mid w_t \\right]\n$$\nBy linearity of expectation, and noting that $w_t$ is constant within the expectation at time $t$:\n$$\n\\mathbb{E}[\\Delta w_t \\mid w_t] = \\eta \\left( \\mathbb{E}[s_t z_t] - \\delta \\mathbb{E}[w_t] \\right) = \\eta \\left( \\mathbb{E}[s_t z_t] - \\delta w_t \\right)\n$$\nThe dependency of the distributions of $s_t$ and $z_t$ on $w_t$ is not present, as $p_t$ depends on $w_\\ast$, not $w_t$.\n\nThe core of the analysis lies in evaluating the correlation term $\\mathbb{E}[s_t z_t]$. We can evaluate this using the law of total expectation, by conditioning on the value of the spike indicator $s_t$:\n$$\n\\mathbb{E}[s_t z_t] = \\mathbb{E}[s_t z_t \\mid s_t=1] P(s_t=1) + \\mathbb{E}[s_t z_t \\mid s_t=0] P(s_t=0)\n$$\nWhen $s_t=1$, the expression becomes $\\mathbb{E}[1 \\cdot z_t \\mid s_t=1] P(s_t=1)$. The term $\\mathbb{E}[z_t \\mid s_t=1]$ is, by definition, the spike-triggered mean $m$.\nWhen $s_t=0$, the expression becomes $\\mathbb{E}[0 \\cdot z_t \\mid s_t=0] P(s_t=0) = 0$.\n\nLet $P_{\\text{spike}} = P(s_t=1)$ denote the average probability of a post-synaptic spike. Substituting these into the equation for $\\mathbb{E}[s_t z_t]$ gives:\n$$\n\\mathbb{E}[s_t z_t] = m \\cdot P_{\\text{spike}} + 0 = P_{\\text{spike}} \\, m\n$$\nThis result shows that the expected correlation between the spike and the centered input is the spike-triggered mean, scaled by the average firing rate.\n\nNow, we substitute this back into the equation for the expected weight update:\n$$\n\\mathbb{E}[\\Delta w_t \\mid w_t] = \\eta \\left( P_{\\text{spike}} \\, m - \\delta w_t \\right)\n$$\nThe system reaches a stable point, or equilibrium, when the expected change in the weights is zero, i.e., $\\mathbb{E}[\\Delta w_t \\mid w_t] = 0$. Let $w_{\\text{eq}}$ be the weight vector at this equilibrium.\n$$\n\\eta \\left( P_{\\text{spike}} \\, m - \\delta w_{\\text{eq}} \\right) = 0\n$$\nSince $\\eta  0$ and $\\delta  0$, we can solve for $w_{\\text{eq}}$:\n$$\n\\delta w_{\\text{eq}} = P_{\\text{spike}} \\, m\n$$\n$$\nw_{\\text{eq}} = \\frac{P_{\\text{spike}}}{\\delta} m\n$$\nThis derivation demonstrates that the fixed point of the learning rule's expectation, $w_{\\text{eq}}$, is directly proportional to the true spike-triggered mean, $m$. The constant of proportionality is given by the ratio of the average firing rate $P_{\\text{spike}}$ to the decay coefficient $\\delta$.\n\nThe learning rule effectively performs stochastic gradient ascent on an underlying objective function whose maximum is located at $w_{\\text{eq}}$. Over many time steps, the synaptic weight vector $w_t$ will fluctuate around this expected equilibrium value. The empirical spike-triggered mean, $\\hat{m} = \\frac{1}{N_{\\text{spike}}} \\sum_{t: s_t = 1} z_t$, is a statistical estimator for the true STM, $m$. For a large number of time steps $T$ and a sufficient number of spikes $N_{\\text{spike}}$, the Law of Large Numbers ensures that $\\hat{m}$ converges to $m$. Consequently, the final weight vector $w_T$ is expected to be aligned with $\\hat{m}$, as both are estimators (up to a scaling factor) of the same underlying directional quantity rooted in the statistics of the neural activity.\n\n### Simulation Implementation\n\nThe provided Python code will simulate the described process for the three given test cases. It implements the discrete-time simulation of the pre-synaptic Poisson inputs, the probabilistic post-synaptic spiking, and the synaptic weight updates. After running the simulation for $T$ time steps, it calculates the final weight vector $w_T$ and the empirical STM $\\hat{m}$. Finally, it computes the metric specified for each test case—cosine similarity or vector norm—and prints the results in the required format.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by running three distinct simulations of a synaptic learning rule\n    and computes the specified metric for each case.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (d, T, lam_val, b, g, eta, delta, seed, w_star_mode, metric)\n        (32, 120_000, 5.0, -0.75, 0.15, 0.01, 0.01, 123, 'random', 'cos_sim'),\n        (32, 120_000, 5.0, -0.75, 0.0, 0.01, 0.01, 456, 'zero', 'norm_wT'),\n        (32, 200_000, 5.0, -3.0, 0.5, 0.005, 0.01, 789, 'random', 'cos_sim'),\n    ]\n\n    results = []\n\n    def sigma(u):\n        \"\"\"\n        Computes the logistic sigmoid function with clipping to prevent overflow.\n        \"\"\"\n        # Clip input to a safe range for np.exp to avoid overflow.\n        u_clipped = np.clip(u, -700, 700)\n        return 1.0 / (1.0 + np.exp(-u_clipped))\n\n    for case in test_cases:\n        d, T, lam_val, b, g, eta, delta, seed, w_star_mode, metric = case\n        \n        rng = np.random.default_rng(seed)\n\n        if w_star_mode == 'random':\n            w_star_raw = rng.standard_normal(d)\n            norm_w_star = np.linalg.norm(w_star_raw)\n            # Normalize to unit length, handling the unlikely case of a zero vector.\n            w_star = w_star_raw if norm_w_star == 0 else w_star_raw / norm_w_star\n        elif w_star_mode == 'zero':\n            w_star = np.zeros(d)\n\n        lam_vec = np.full(d, lam_val)\n        w = np.zeros(d)\n        \n        z_sum_spiked = np.zeros(d)\n        n_spike = 0\n        \n        # Pre-compute the decay factor for the weight update\n        decay_factor = 1.0 - eta * delta\n        \n        for _ in range(T):\n            # 1. Generate pre-synaptic Poisson spikes\n            x_t = rng.poisson(lam_vec)\n            \n            # 2. Compute centered input\n            z_t = x_t - lam_vec\n            \n            # 3. Compute post-synaptic spiking probability\n            p_t = sigma(b + g * np.dot(w_star, z_t))\n            \n            # 4. Generate post-synaptic spike\n            s_t = 1 if rng.random()  p_t else 0\n            \n            # 5. Accumulate spike-triggered inputs\n            if s_t == 1:\n                z_sum_spiked += z_t\n                n_spike += 1\n            \n            # 6. Apply learning rule\n            w = decay_factor * w + eta * s_t * z_t\n\n        w_T = w\n        \n        if n_spike > 0:\n            m_hat = z_sum_spiked / n_spike\n        else:\n            m_hat = np.zeros(d)\n\n        # Calculate the result based on the specified metric for the test case\n        if metric == 'cos_sim':\n            norm_wT = np.linalg.norm(w_T)\n            norm_m_hat = np.linalg.norm(m_hat)\n            \n            # Per problem instructions, if m_hat is a zero vector, use norm of w_T.\n            if norm_m_hat == 0:\n                result = norm_wT\n            # Otherwise, if w_T is zero, the similarity is zero.\n            elif norm_wT == 0:\n                result = 0.0\n            # Standard cosine similarity calculation\n            else:\n                result = np.dot(w_T, m_hat) / (norm_wT * norm_m_hat)\n        \n        elif metric == 'norm_wT':\n            # This metric is explicitly requested for Test Case 2\n            result = np.linalg.norm(w_T)\n        \n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        }
    ]
}