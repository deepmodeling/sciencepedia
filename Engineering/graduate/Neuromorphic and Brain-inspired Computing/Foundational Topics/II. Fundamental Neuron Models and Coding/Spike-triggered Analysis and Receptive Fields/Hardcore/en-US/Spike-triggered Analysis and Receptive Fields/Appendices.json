{
    "hands_on_practices": [
        {
            "introduction": "Before we can confidently interpret a spike-triggered average (STA), we must understand its reliability as a statistical estimator. The STA is computed from a finite number of spikes, so it is subject to sampling error. This exercise provides a foundational, hands-on derivation of the STA's statistical properties, guiding you to calculate its variance and construct confidence intervals from first principles . Mastering this will allow you to quantify the uncertainty in your receptive field estimates and understand how their accuracy scales with the amount of data collected.",
            "id": "4060196",
            "problem": "Consider a neuron driven by a temporally white, zero-mean, independent Gaussian stimulus with per-component variance $\\,\\sigma^{2}\\,$. Let $\\,\\mathbf{s}_{t}\\in\\mathbb{R}^{d}\\,$ denote the stimulus vector at discrete time $\\,t\\,$, and suppose spikes are recorded at times $\\,t_{1},t_{2},\\dots,t_{N}\\,$. Define the spike-triggered average (STA) at a fixed lag $\\,\\tau\\,$ as the empirical mean of the stimulus vectors preceding spikes:\n$$\n\\widehat{\\mathbf{m}}(\\tau)\\;=\\;\\frac{1}{N}\\sum_{j=1}^{N}\\mathbf{s}_{t_{j}-\\tau}.\n$$\nFocus on any single STA component $\\,\\widehat{m}_{i}(\\tau)\\,$, which is the average of the $\\,i$-th stimulus coordinate over the $\\,N\\,$ spike-triggered samples. Assume the spike-triggered samples $\\,\\{s_{t_{j}-\\tau,i}\\}_{j=1}^{N}\\,$ are independent and identically distributed with mean $\\,\\mu_{i}(\\tau)\\,$ and variance $\\,\\sigma^{2}\\,$, and further assume each sample is sub-Gaussian with proxy variance parameter $\\,\\sigma^{2}\\,$ (i.e., it has a moment generating function bounded by that of a centered Gaussian of variance $\\,\\sigma^{2}\\,$).\n\nStarting from these premises and without invoking any specialized shortcut formulas, derive:\n- The variance of the STA component $\\,\\widehat{m}_{i}(\\tau)\\,$ as a function of the spike count $\\,N\\,$ and stimulus variance $\\,\\sigma^{2}\\,$.\n- A two-sided confidence interval half-width $\\,r(N,\\sigma,\\delta)\\,$ such that\n$$\n\\mathbb{P}\\Big(|\\widehat{m}_{i}(\\tau)-\\mu_{i}(\\tau)|\\le r(N,\\sigma,\\delta)\\Big)\\;\\ge\\;1-\\delta,\n$$\nfor a prescribed confidence level $\\,1-\\delta\\,$ with $\\,\\delta\\in(0,1)\\,$, using a concentration bound justified by the sub-Gaussian assumption.\n\nExpress your final answer as a single row matrix containing the two quantities $\\,\\mathrm{Var}[\\widehat{m}_{i}(\\tau)]\\,$ and $\\,r(N,\\sigma,\\delta)\\,$, in that order. No numerical evaluation or rounding is required; provide closed-form analytic expressions.",
            "solution": "The problem statement is subjected to validation.\n\n### Step 1: Extract Givens\n- Stimulus: Temporally white, zero-mean, independent Gaussian stimulus with per-component variance $\\sigma^{2}$.\n- Stimulus vector at discrete time $t$: $\\mathbf{s}_{t}\\in\\mathbb{R}^{d}$.\n- Spike times: $t_{1},t_{2},\\dots,t_{N}$.\n- Spike-triggered average (STA) definition: $\\widehat{\\mathbf{m}}(\\tau) = \\frac{1}{N}\\sum_{j=1}^{N}\\mathbf{s}_{t_{j}-\\tau}$.\n- Single STA component: $\\widehat{m}_{i}(\\tau)$.\n- Spike-triggered samples for component $i$: $\\{s_{t_{j}-\\tau,i}\\}_{j=1}^{N}$.\n- Assumption 1: The samples $\\{s_{t_{j}-\\tau,i}\\}_{j=1}^{N}$ are independent and identically distributed (i.i.d.).\n- Assumption 2: The mean of each sample is $\\mu_{i}(\\tau) = \\mathbb{E}[s_{t_{j}-\\tau,i}]$.\n- Assumption 3: The variance of each sample is $\\mathrm{Var}[s_{t_{j}-\\tau,i}] = \\sigma^{2}$.\n- Assumption 4: Each sample $s_{t_{j}-\\tau,i}$ is sub-Gaussian with proxy variance parameter $\\sigma^{2}$.\n- Goal 1: Derive $\\mathrm{Var}[\\widehat{m}_{i}(\\tau)]$ as a function of $N$ and $\\sigma^{2}$.\n- Goal 2: Derive a two-sided confidence interval half-width $r(N,\\sigma,\\delta)$ such that $\\mathbb{P}(|\\widehat{m}_{i}(\\tau)-\\mu_{i}(\\tau)|\\le r(N,\\sigma,\\delta)) \\ge 1-\\delta$, for $\\delta\\in(0,1)$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, well-posed, and objective. It presents a standard theoretical problem in computational neuroscience, specifically concerning the statistical properties of the spike-triggered average, a fundamental tool for receptive field estimation.\n- **Scientific Grounding**: The concepts of spike-triggered analysis, Gaussian stimuli, i.i.d. samples, variance, and concentration inequalities (like Hoeffding's, which is applicable to sub-Gaussian variables) are all standard and well-established in statistics, machine learning, and computational neuroscience. The problem is a direct application of these principles.\n- **Well-Posedness**: The problem provides a clear definition of the quantity to be analyzed ($\\widehat{m}_{i}(\\tau)$) and all necessary statistical assumptions (i.i.d. samples, known variance, sub-Gaussian property) to derive the two requested quantities. The goals are specific and a unique analytical solution exists.\n- **Objectivity**: The problem is stated in precise mathematical language, free of ambiguity or subjective claims.\n- **Consistency**: The assumptions are internally consistent for the purpose of a theoretical derivation. While the assumption that the variance of the spike-triggered stimulus is the same as the raw stimulus variance ($\\sigma^2$) is a simplification, it is a valid premise within a self-contained mathematical problem and does not create a logical contradiction.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. A full derivation will be provided.\n\n### Derivation\nLet us denote a single spike-triggered stimulus sample for the $i$-th component as $X_j = s_{t_{j}-\\tau,i}$. The problem states that the set of samples $\\{X_j\\}_{j=1}^{N}$ is i.i.d. with mean $\\mathbb{E}[X_j] = \\mu_{i}(\\tau)$ and variance $\\mathrm{Var}[X_j] = \\sigma^2$. The STA component is the sample mean of these variables:\n$$\n\\widehat{m}_{i}(\\tau) = \\frac{1}{N}\\sum_{j=1}^{N} X_j\n$$\n\n**Part 1: Variance of the STA component**\n\nWe seek to compute the variance of $\\widehat{m}_{i}(\\tau)$.  Using the properties of variance, we can first factor out the constant term $\\frac{1}{N}$:\n$$\n\\mathrm{Var}[\\widehat{m}_{i}(\\tau)] = \\mathrm{Var}\\left[\\frac{1}{N}\\sum_{j=1}^{N} X_j\\right] = \\frac{1}{N^2}\\mathrm{Var}\\left[\\sum_{j=1}^{N} X_j\\right]\n$$\nBecause the random variables $X_j$ are assumed to be independent, the variance of their sum is the sum of their variances:\n$$\n\\mathrm{Var}\\left[\\sum_{j=1}^{N} X_j\\right] = \\sum_{j=1}^{N} \\mathrm{Var}[X_j]\n$$\nWe are given that the variance of each individual sample is $\\mathrm{Var}[X_j] = \\sigma^2$. Therefore, the sum of the variances is:\n$$\n\\sum_{j=1}^{N} \\mathrm{Var}[X_j] = \\sum_{j=1}^{N} \\sigma^2 = N\\sigma^2\n$$\nSubstituting this back into the expression for the variance of the STA component, we get:\n$$\n\\mathrm{Var}[\\widehat{m}_{i}(\\tau)] = \\frac{1}{N^2} (N\\sigma^2) = \\frac{\\sigma^2}{N}\n$$\nThis is the first required quantity.\n\n**Part 2: Confidence Interval Half-Width**\n\nWe need to find a half-width $r = r(N,\\sigma,\\delta)$ for a confidence interval around the true mean $\\mu_i(\\tau)$. The problem specifies using a concentration bound justified by the sub-Gaussian assumption.\n\nThe quantity of interest is the deviation of the sample mean from the true mean: $\\widehat{m}_{i}(\\tau) - \\mu_{i}(\\tau)$. This can be written as:\n$$\n\\widehat{m}_{i}(\\tau) - \\mu_{i}(\\tau) = \\frac{1}{N}\\sum_{j=1}^{N} X_j - \\mu_i(\\tau) = \\frac{1}{N}\\sum_{j=1}^{N} (X_j - \\mu_i(\\tau))\n$$\nThis is the average of $N$ i.i.d. zero-mean random variables $Z_j = X_j - \\mu_i(\\tau)$. The problem states that each $X_j$ is sub-Gaussian with proxy variance parameter $\\sigma^2$. This property implies that for any $\\lambda \\in \\mathbb{R}$:\n$$\n\\mathbb{E}[\\exp(\\lambda Z_j)] = \\mathbb{E}[\\exp(\\lambda (X_j - \\mu_i(\\tau)))] \\le \\exp\\left(\\frac{\\lambda^2 \\sigma^2}{2}\\right)\n$$\nFor an average of $N$ such i.i.d. variables, Hoeffding's inequality provides a tight concentration bound. The inequality states that for any $\\epsilon > 0$:\n$$\n\\mathbb{P}\\left(\\left|\\frac{1}{N}\\sum_{j=1}^{N} Z_j\\right| \\ge \\epsilon\\right) \\le 2 \\exp\\left(-\\frac{N \\epsilon^2}{2\\sigma^2}\\right)\n$$\nIn our notation, this is:\n$$\n\\mathbb{P}\\Big(|\\widehat{m}_{i}(\\tau) - \\mu_{i}(\\tau)| \\ge \\epsilon\\Big) \\le 2 \\exp\\left(-\\frac{N \\epsilon^2}{2\\sigma^2}\\right)\n$$\nWe are looking for a half-width $r$ such that $\\mathbb{P}(|\\widehat{m}_{i}(\\tau)-\\mu_{i}(\\tau)|\\le r) \\ge 1-\\delta$. This is equivalent to ensuring the probability of the contrary event is bounded by $\\delta$:\n$$\n\\mathbb{P}\\Big(|\\widehat{m}_{i}(\\tau) - \\mu_{i}(\\tau)| > r\\Big) \\le \\delta\n$$\nWe can achieve this by setting the upper bound from Hoeffding's inequality equal to $\\delta$ and solving for $\\epsilon$, which will be our half-width $r$.\n$$\n\\delta = 2 \\exp\\left(-\\frac{N r^2}{2\\sigma^2}\\right)\n$$\nNow, we solve for $r(N,\\sigma,\\delta)$:\n$$\n\\frac{\\delta}{2} = \\exp\\left(-\\frac{N r^2}{2\\sigma^2}\\right)\n$$\nTaking the natural logarithm of both sides:\n$$\n\\ln\\left(\\frac{\\delta}{2}\\right) = -\\frac{N r^2}{2\\sigma^2}\n$$\nMultiplying by $-1$ and using the property $\\ln(1/x) = -\\ln(x)$:\n$$\n\\ln\\left(\\frac{2}{\\delta}\\right) = \\frac{N r^2}{2\\sigma^2}\n$$\nIsolating $r^2$:\n$$\nr^2 = \\frac{2\\sigma^2}{N} \\ln\\left(\\frac{2}{\\delta}\\right)\n$$\nFinally, taking the square root gives the expression for the half-width $r$:\n$$\nr(N,\\sigma,\\delta) = \\sqrt{\\frac{2\\sigma^2}{N} \\ln\\left(\\frac{2}{\\delta}\\right)} = \\sigma \\sqrt{\\frac{2}{N} \\ln\\left(\\frac{2}{\\delta}\\right)}\n$$\nThis is the second required quantity.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{\\sigma^2}{N}  \\sigma \\sqrt{\\frac{2}{N} \\ln\\left(\\frac{2}{\\delta}\\right)}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Once a reliable spike-triggered average has been computed, it exists as a vector or matrix of numerical values. To gain neuroscientific insight, we must translate these raw numbers into physiologically meaningful parameters. This practice focuses on the essential task of characterizing a temporal receptive field by extracting key features from its STA trace, such as response latency and integration duration . By implementing algorithms to quantify these properties, you will develop the practical skills needed to describe and compare the response dynamics of different neurons.",
            "id": "4060247",
            "problem": "You are given discrete spike-triggered average (STA) traces across time lags and are asked to compute temporal latency and duration of the dominant temporal receptive field lobe by locating peaks in the STA, and to relate these quantities to synaptic integration properties. Work strictly in discrete time. Let the discrete time lags be defined by $ \\tau_i = (i - i_0)\\,\\Delta t $, where $ i \\in \\{0,1,\\dots,N-1\\} $, $ i_0 $ is the zero-lag index, and $ \\Delta t $ is the uniform sampling interval in milliseconds. Let the STA samples be given by a real-valued sequence $ s[i] $ of length $ N $. Use the following definitions and requirements to compute the requested quantities.\n\nFundamental base and definitions:\n- The spike-triggered average (STA) is the conditional expectation of the stimulus preceding a spike, $ \\mathrm{STA}(\\tau) = \\mathbb{E}[x(t-\\tau)\\,|\\,\\text{spike at }t] $, for a stimulus $ x(t) $. Under a linear filter with additive noise and a monotonic static nonlinearity, the STA is proportional to the linear temporal filter (temporal receptive field) when the stimulus is temporally white, hence its dominant lobe encodes the temporal sensitivity profile that most strongly drives spiking.\n- The temporal latency of the dominant lobe is the time lag at which the magnitude of the STA attains its largest absolute value, $ t_{\\mathrm{lat}} = \\tau_{i_{\\max}} $, where $ i_{\\max} = \\arg\\max_i |s[i]| $.\n- The duration of the dominant lobe is quantified by the full-width at half-maximum (FWHM) of that lobe. Let $ \\sigma = \\mathrm{sign}(s[i_{\\max}]) \\in \\{-1,+1\\} $ and let the dominant lobe be the maximal contiguous index set $ \\mathcal{L} = \\{ \\ell,\\ell+1,\\dots,r \\} $ containing $ i_{\\max} $ such that $ s[j]\\cdot\\sigma  0 $ for all $ j \\in \\mathcal{L} $, bounded on each side by the first zero-crossing or the array boundary. Let $ a[j] = \\sigma\\, s[j] \\ge 0 $ on $ \\mathcal{L} $, and $ h = \\tfrac{1}{2}\\,a[i_{\\max}] $ be the half-maximum. Define the left and right half-maximum crossing times by linear interpolation on the discrete grid if they exist:\n  - Find the smallest $ k \\in \\{\\ell+1,\\dots,i_{\\max}\\} $ such that $ a[k-1]  h \\le a[k] $, then\n    $$ \\tau_{\\mathrm{L}} = \\tau_{k-1} + \\frac{h - a[k-1]}{a[k] - a[k-1]}\\,\\Delta t. $$\n  - Find the largest $ k \\in \\{ i_{\\max},\\dots,r-1 \\} $ such that $ a[k] \\ge h  a[k+1] $, then\n    $$ \\tau_{\\mathrm{R}} = \\tau_{k} + \\frac{h - a[k]}{a[k+1] - a[k]}\\,\\Delta t. $$\n  The FWHM duration is $ t_{\\mathrm{dur}} = \\tau_{\\mathrm{R}} - \\tau_{\\mathrm{L}} $ if both crossings exist. If exactly one side crossing exists (left or right), define $ t_{\\mathrm{dur}} $ as twice the one-sided half-width around the peak. If neither side exists, define $ t_{\\mathrm{dur}} = (r-\\ell+1)\\,\\Delta t $.\n- The effective synaptic integration window is quantified by the area-to-peak ratio over the dominant lobe,\n  $$ T_{\\mathrm{eff}} = \\frac{\\Delta t \\sum_{j=\\ell}^{r} a[j]}{a[i_{\\max}]}, $$\n  which has units of milliseconds and equals the effective time scale over which inputs contribute relative to the peak response, independent of the exact lobe shape. This measure arises because, for a nonnegative kernel $ k(t) $, the ratio $ \\int k(t)\\,dt / \\max_t k(t) $ is a width-like scale reflecting integration.\n- The sign of the dominant lobe encodes whether increases in the stimulus preceding a spike are excitatory-like or inhibitory-like in the linear regime: output an integer flag $ u \\in \\{-1,+1\\} $ where $ u=+1 $ if $ \\sigma=+1 $ and $ u=-1 $ if $ \\sigma=-1 $.\n\nComputational tasks:\nGiven $ \\Delta t $ in milliseconds, the zero-lag index $ i_0 $, and the STA samples $ s[i] $, compute the quadruple $ [t_{\\mathrm{lat}},\\, t_{\\mathrm{dur}},\\, T_{\\mathrm{eff}},\\, u] $ according to the definitions above. All time quantities must be expressed in milliseconds and rounded to three decimal places. The sign flag $ u $ must be an integer.\n\nTest suite:\nImplement your program to compute results for the following four cases. In each case, define $ \\tau_i = (i - i_0)\\,\\Delta t $ and construct $ s[i] $ as specified.\n\n- Case A (unimodal Gaussian lobe):\n  - $ N = 201 $, $ \\Delta t = 1.0 $, $ i_0 = 100 $.\n  - $ s[i] = \\exp\\!\\left( -\\tfrac{1}{2}\\,\\big(\\tfrac{\\tau_i - 25.0}{5.0}\\big)^2 \\right) $.\n\n- Case B (biphasic difference of Gaussians, dominant negative lobe):\n  - $ N = 181 $, $ \\Delta t = 1.0 $, $ i_0 = 90 $.\n  - $ s[i] = 0.8\\,\\exp\\!\\left( -\\tfrac{1}{2}\\,\\big(\\tfrac{\\tau_i - 35.0}{6.0}\\big)^2 \\right) - 1.2\\,\\exp\\!\\left( -\\tfrac{1}{2}\\,\\big(\\tfrac{\\tau_i - 10.0}{4.0}\\big)^2 \\right) $.\n\n- Case C (alpha-function synaptic kernel with delay):\n  - $ N = 151 $, $ \\Delta t = 1.0 $, $ i_0 = 50 $.\n  - Define $ d = 30.0 $ and $ \\tau_s = 8.0 $. Let\n    $$ s[i] =\n      \\begin{cases}\n        \\left(\\dfrac{\\tau_i - d}{\\tau_s}\\right)\\exp\\!\\left(1 - \\dfrac{\\tau_i - d}{\\tau_s}\\right),  \\text{if } \\tau_i \\ge d,\\\\\n        0,  \\text{if } \\tau_i  d.\n      \\end{cases}\n    $$\n\n- Case D (boundary case: single-sample lobe):\n  - $ N = 51 $, $ \\Delta t = 1.0 $, $ i_0 = 25 $.\n  - $ s[i] = 0 $ for all $ i $ except $ s[i_0 + 5] = 1.0 $.\n\nFinal output format:\nYour program should produce a single line of output containing a comma-separated list enclosed in square brackets, where each element corresponds to one test case in order A, B, C, D, and is itself a four-element list $ [t_{\\mathrm{lat}}, t_{\\mathrm{dur}}, T_{\\mathrm{eff}}, u] $. For example, the printed structure must look like $ [[\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot],\\dots] $ with all time quantities rounded to three decimal places and $ u $ as an integer. No additional text should be printed.",
            "solution": "The problem is scientifically valid, well-posed, and self-contained. It presents a standard task in computational neuroscience: the characterization of a temporal receptive field, approximated by a spike-triggered average (STA), using common metrics. The definitions for latency, duration, and integration time are precise and computationally tractable. The test cases provided are based on canonical models of neural response kernels (Gaussian, difference-of-Gaussians, alpha function), and include a boundary case to test the robustness of the algorithm. We will proceed with a solution.\n\nThe core task is to implement a computational procedure that, given a discrete STA trace $s[i]$, extracts a set of four characteristic features: the latency $t_{\\mathrm{lat}}$, the duration $t_{\\mathrm{dur}}$, the effective integration time $T_{\\mathrm{eff}}$, and the sign $u$ of the dominant lobe. The overall procedure can be broken down into the following steps, which directly follow the definitions provided.\n\n**Step 1: STA Signal Generation**\nFor each test case, we are given the number of samples $N$, the sampling interval $\\Delta t$, and the zero-lag index $i_0$. The first step is to construct the discrete time lag array $\\tau$ and the corresponding STA signal $s$.\nThe time lags are defined as:\n$$ \\tau_i = (i - i_0)\\,\\Delta t \\quad \\text{for } i \\in \\{0, 1, \\dots, N-1\\} $$\nUsing these time lags, the STA signal $s[i]$ is generated according to the specific function provided for each case (Gaussian, difference-of-Gaussians, etc.).\n\n**Step 2: Dominant Lobe Peak and Sign Identification**\nThe dominant lobe is defined by the peak of the STA's absolute magnitude.\nFirst, we find the index $i_{\\max}$ where $|s[i]|$ is maximal:\n$$ i_{\\max} = \\arg\\max_i |s[i]| $$\nThe temporal latency, $t_{\\mathrm{lat}}$, is the time lag corresponding to this peak:\n$$ t_{\\mathrm{lat}} = \\tau_{i_{\\max}} = (i_{\\max} - i_0)\\,\\Delta t $$\nThe sign of the dominant lobe, $\\sigma$, indicates whether it is excitatory-like (positive) or inhibitory-like (negative). It is determined by the sign of the STA at the peak, and the output flag $u$ is set accordingly:\n$$ \\sigma = \\mathrm{sign}(s[i_{\\max}]) $$\n$$ u = \\sigma $$\n\n**Step 3: Dominant Lobe Boundary Detection**\nThe dominant lobe is the contiguous region around the peak $i_{\\max}$ that has the same sign as the peak. We must find the maximal contiguous index set $\\mathcal{L} = \\{\\ell, \\ell+1, \\dots, r\\}$ containing $i_{\\max}$ such that $s[j] \\cdot \\sigma > 0$ for all $j \\in \\mathcal{L}$.\nThe left boundary $\\ell$ is found by starting at $i_{\\max}$ and searching backwards (decreasing index $j$) for the first index where $s[j] \\cdot \\sigma \\le 0$ or the array boundary at $j=0$ is reached. The index of the first point inside the lobe is $\\ell$.\nSimilarly, the right boundary $r$ is found by searching forwards (increasing index $j$) from $i_{\\max}$ for the first index where $s[j] \\cdot \\sigma \\le 0$ or the array boundary at $j=N-1$ is reached. The index of the last point inside the lobe is $r$.\n\n**Step 4: Effective Integration Time ($T_{\\mathrm{eff}}$)**\nThis metric quantifies the effective duration of the integration window. It is calculated as the ratio of the lobe's area to its peak height. To compute this, we first define a positive-valued version of the lobe, $a[j] = \\sigma \\cdot s[j]$ for $j \\in \\mathcal{L}$. The peak of this rectified lobe is $a[i_{\\max}] = |s[i_{\\max}]|$.\nThe area under the lobe is approximated by a discrete sum:\n$$ A_{\\mathrm{lobe}} = \\Delta t \\sum_{j=\\ell}^{r} a[j] = \\Delta t \\sum_{j=\\ell}^{r} \\sigma \\cdot s[j] $$\nThe effective integration time is then:\n$$ T_{\\mathrm{eff}} = \\frac{A_{\\mathrm{lobe}}}{a[i_{\\max}]} = \\frac{\\Delta t \\sum_{j=\\ell}^{r} a[j]}{a[i_{\\max}]} $$\n\n**Step 5: Lobe Duration ($t_{\\mathrm{dur}}$) via Full-Width at Half-Maximum (FWHM)**\nThe FWHM measures the width of the lobe at half of its maximum amplitude.\nFirst, we calculate the half-maximum height:\n$$ h = \\frac{1}{2} a[i_{\\max}] $$\nNext, we find the time points $\\tau_{\\mathrm{L}}$ and $\\tau_{\\mathrm{R}}$ where the rectified lobe $a[j]$ crosses this height $h$. Because the signal is discrete, these points are found by linear interpolation between adjacent samples.\n\nTo find the left crossing time $\\tau_{\\mathrm{L}}$, we search for the smallest index $k \\in \\{\\ell+1, \\dots, i_{\\max}\\}$ such that $a[k-1]  h \\le a[k]$. If such a $k$ is found, $\\tau_{\\mathrm{L}}$ is interpolated as:\n$$ \\tau_{\\mathrm{L}} = \\tau_{k-1} + \\frac{h - a[k-1]}{a[k] - a[k-1]}\\,\\Delta t $$\nTo find the right crossing time $\\tau_{\\mathrm{R}}$, we search for the largest index $k \\in \\{i_{\\max}, \\dots, r-1\\}$ such that $a[k] \\ge h  a[k+1]$. If such a $k$ is found, $\\tau_{\\mathrm{R}}$ is interpolated as:\n$$ \\tau_{\\mathrm{R}} = \\tau_{k} + \\frac{h - a[k]}{a[k+1] - a[k]}\\,\\Delta t $$\n\nThe duration $t_{\\mathrm{dur}}$ is then determined based on which crossings were found:\n1.  If both $\\tau_{\\mathrm{L}}$ and $\\tau_{\\mathrm{R}}$ exist, $t_{\\mathrm{dur}} = \\tau_{\\mathrm{R}} - \\tau_{\\mathrm{L}}$.\n2.  If only $\\tau_{\\mathrm{L}}$ exists, $t_{\\mathrm{dur}} = 2 \\cdot (\\tau_{i_{\\max}} - \\tau_{\\mathrm{L}})$.\n3.  If only $\\tau_{\\mathrm{R}}$ exists, $t_{\\mathrm{dur}} = 2 \\cdot (\\tau_{\\mathrm{R}} - \\tau_{i_{\\max}})$.\n4.  If neither crossing exists (e.g., a very narrow, single-sample peak), $t_{\\mathrm{dur}} = (r - \\ell + 1) \\cdot \\Delta t$.\n\nThis comprehensive procedure computes the four required quantities for any given STA trace. It is implemented for each of the four test cases specified. The resulting time-based quantities ($t_{\\mathrm{lat}}, t_{\\mathrm{dur}}, T_{\\mathrm{eff}}$) are rounded to three decimal places as required.",
            "answer": "```python\nimport numpy as np\n\ndef compute_sta_features(N, dt, i0, s_func):\n    \"\"\"\n    Computes temporal features of a spike-triggered average (STA) trace.\n    \"\"\"\n    # Step 1: STA Signal Generation\n    indices = np.arange(N)\n    tau = (indices - i0) * dt\n    s = s_func(tau)\n\n    # Handle all-zero STA gracefully, though not expected in tests.\n    if not np.any(s):\n        return [0.0, 0.0, 0.0, 1]\n\n    # Step 2: Dominant Lobe Peak and Sign Identification\n    i_max = np.argmax(np.abs(s))\n    t_lat = (i_max - i0) * dt\n    s_peak = s[i_max]\n    sigma = np.sign(s_peak)\n    if sigma == 0: sigma = 1  # Convention for zero-peak case\n    u = int(sigma)\n\n    # Step 3: Dominant Lobe Boundary Detection\n    l, r = i_max, i_max\n    # Find left boundary\n    if l > 0:\n        for i in range(i_max - 1, -1, -1):\n            if s[i] * sigma > 0:\n                l = i\n            else:\n                break\n    # Find right boundary\n    if r  N - 1:\n        for i in range(i_max + 1, N):\n            if s[i] * sigma > 0:\n                r = i\n            else:\n                break\n\n    # Step 4: Effective Integration Time (T_eff)\n    lobe_indices = np.arange(l, r + 1)\n    a_lobe_values = s[lobe_indices] * sigma\n    a_max = s_peak * sigma\n    \n    if a_max == 0:\n        T_eff = 0.0\n    else:\n        T_eff = dt * np.sum(a_lobe_values) / a_max\n    \n    # Step 5: Lobe Duration (t_dur) via FWHM\n    h = 0.5 * a_max\n    \n    a_full = s * sigma  # Rectified full signal\n    tau_L, tau_R = None, None\n\n    # Find left crossing (search from lobe start up to the peak)\n    for k in range(l + 1, i_max + 1):\n        if a_full[k-1]  h = a_full[k]:\n            if a_full[k] - a_full[k-1] != 0:\n                tau_L = tau[k-1] + (h - a_full[k-1]) / (a_full[k] - a_full[k-1]) * dt\n            else:  # Flat region at half-max\n                tau_L = tau[k]\n            break\n\n    # Find right crossing (search from lobe end down to the peak)\n    # The problem asks for the largest k, so searching backwards and stopping works.\n    for k in range(r - 1, i_max - 1, -1):\n        if a_full[k] >= h > a_full[k+1]:\n            if a_full[k+1] - a_full[k] != 0:\n                tau_R = tau[k] + (h - a_full[k]) / (a_full[k+1] - a_full[k]) * dt\n            else:\n                tau_R = tau[k]\n            break\n\n    # Determine t_dur based on which crossings were found\n    t_peak = tau[i_max]\n    if tau_L is not None and tau_R is not None:\n        t_dur = tau_R - tau_L\n    elif tau_L is not None:\n        t_dur = 2 * (t_peak - tau_L)\n    elif tau_R is not None:\n        t_dur = 2 * (tau_R - t_peak)\n    else: # Neither crossing exists (e.g., single-point lobe)\n        t_dur = (r - l + 1) * dt\n\n    return [\n        round(t_lat, 3),\n        round(t_dur, 3),\n        round(T_eff, 3),\n        u\n    ]\n\ndef solve():\n    \"\"\"\n    Main function to run the test cases and print the results.\n    \"\"\"\n    test_cases = [\n        # Case A: unimodal Gaussian lobe\n        {\n            \"N\": 201, \"dt\": 1.0, \"i0\": 100,\n            \"s_func\": lambda tau: np.exp(-0.5 * ((tau - 25.0) / 5.0)**2)\n        },\n        # Case B: biphasic difference of Gaussians, dominant negative lobe\n        {\n            \"N\": 181, \"dt\": 1.0, \"i0\": 90,\n            \"s_func\": lambda tau: 0.8 * np.exp(-0.5 * ((tau - 35.0) / 6.0)**2) - 1.2 * np.exp(-0.5 * ((tau - 10.0) / 4.0)**2)\n        },\n        # Case C: alpha-function synaptic kernel with delay\n        {\n            \"N\": 151, \"dt\": 1.0, \"i0\": 50,\n            \"s_func\": lambda tau, d=30.0, tau_s=8.0: np.piecewise(\n                tau, [tau  d], [0.0, lambda t: ((t - d) / tau_s) * np.exp(1 - (t - d) / tau_s)]\n            )\n        },\n        # Case D: boundary case: single-sample lobe\n        {\n            \"N\": 51, \"dt\": 1.0, \"i0\": 25,\n            \"s_func\": lambda tau: np.where(tau == 5.0, 1.0, 0.0)\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = compute_sta_features(case[\"N\"], case[\"dt\"], case[\"i0\"], case[\"s_func\"])\n        results.append(result)\n\n    # Format the output string as specified in the problem\n    result_str = \"[\" + \",\".join([f\"[{r[0]},{r[1]},{r[2]},{r[3]}]\" for r in results]) + \"]\"\n\n    print(result_str)\n\nsolve()\n```"
        },
        {
            "introduction": "Having learned how to estimate and characterize receptive fields, a deeper question emerges: how do such specialized neural filters arise in the first place? This practice explores the powerful hypothesis that receptive fields are sculpted by synaptic plasticity. You will simulate a biologically plausible, correlation-based learning rule and discover that the synaptic weights naturally converge to a filter proportional to the spike-triggered mean of the input . This exercise provides a profound link between the descriptive power of STA and a generative, mechanistic model of learning and development in the brain.",
            "id": "4060251",
            "problem": "Consider a discrete-time synaptic learning scenario in neuromorphic and brain-inspired computing, where both pre-synaptic and post-synaptic spike trains are modeled as Poisson processes in small time bins. Let the pre-synaptic input at time index $t$ be a $d$-dimensional nonnegative integer vector $x_t \\in \\mathbb{N}^d$, where each coordinate $x_{t,i}$ is an independent draw from a Poisson distribution with rate $\\lambda_i  0$. Define the centered pre-synaptic activity $z_t = x_t - \\lambda$, where $\\lambda \\in \\mathbb{R}^d$ is the vector of rates. Let the post-synaptic spike indicator be $s_t \\in \\{0,1\\}$, where $s_t = 1$ indicates a spike in bin $t$, with a Bernoulli probability $p_t$ given by a logistic nonlinearity\n$$\np_t = \\sigma\\!\\left(b + g\\, w_\\ast^\\top z_t \\right),\n$$\nwhere $w_\\ast \\in \\mathbb{R}^d$ is a fixed but unknown target filter direction, $b \\in \\mathbb{R}$ is a bias, $g0$ is a gain, and $\\sigma(u) = \\frac{1}{1 + e^{-u}}$ is the logistic function. This Bernoulli spiking in small bins is consistent with an inhomogeneous Poisson process model for post-synaptic spiking.\n\nA synaptic weight vector $w_t \\in \\mathbb{R}^d$ evolves according to a correlation-based update with homeostatic decay\n$$\nw_{t+1} = w_t + \\eta \\left( s_t z_t - \\delta\\, w_t \\right),\n$$\nwhere $\\eta  0$ is a learning rate and $\\delta  0$ is a decay coefficient. Define the spike-triggered mean (STM) of the centered input as\n$$\nm = \\mathbb{E}\\left[ z_t \\mid s_t = 1 \\right],\n$$\nand its empirical estimator\n$$\n\\hat{m} = \\frac{1}{N_{\\text{spike}}} \\sum_{t: s_t = 1} z_t,\n$$\nwith $N_{\\text{spike}}$ the number of time bins where $s_t = 1$.\n\nStarting from the fundamental definitions of the Poisson and Bernoulli processes and the specified learning rule, derive from first principles why the expected update aligns the synaptic weight direction with the spike-triggered mean $\\hat{m}$, up to a proportionality constant set by the decay $\\delta$. Then implement a simulation that estimates $w_T$ and $\\hat{m}$ for a large number of time steps $T$, and quantifies convergence by the cosine similarity\n$$\n\\mathrm{cos\\_sim}(w_T, \\hat{m}) = \\frac{w_T^\\top \\hat{m}}{\\|w_T\\|_2 \\, \\|\\hat{m}\\|_2},\n$$\nwhenever both norms are nonzero. In the case that $\\|\\hat{m}\\|_2 = 0$, the STM is ill-defined in direction; in that case quantify convergence by the final norm $\\|w_T\\|_2$ instead.\n\nYour program must implement the simulation for the three test cases below, using the specified random seed for reproducibility, and produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case’s result is a single float computed as specified.\n\nDefinitions and assumptions to use in the derivation and program:\n- The pre-synaptic inputs $x_{t,i}$ are independent across dimensions and time, with $x_{t,i} \\sim \\mathrm{Poisson}(\\lambda_i)$.\n- The centered inputs are $z_t = x_t - \\lambda$ so that $\\mathbb{E}[z_t] = 0$.\n- The post-synaptic spike $s_t \\sim \\mathrm{Bernoulli}(p_t)$ with $p_t = \\sigma(b + g\\, w_\\ast^\\top z_t)$.\n- The learning rule is $w_{t+1} = w_t + \\eta ( s_t z_t - \\delta w_t )$.\n\nTest suite:\n1. General case (happy path): $d = 32$, $T = 120{,}000$, $\\lambda_i = 5$ for all $i$, $b = -0.75$, $g = 0.15$, $\\eta = 0.01$, $\\delta = 0.01$, random seed $123$, and $w_\\ast$ a random unit-norm vector drawn from a standard normal distribution and normalized to unit length. Output the cosine similarity $\\mathrm{cos\\_sim}(w_T, \\hat{m})$ as a float.\n2. Independence edge case: $d = 32$, $T = 120{,}000$, $\\lambda_i = 5$ for all $i$, $b = -0.75$, $g = 0.0$ (making post-synaptic spikes independent of $z_t$), $\\eta = 0.01$, $\\delta = 0.01$, random seed $456$, and $w_\\ast = 0$ (the zero vector). Output the final weight norm $\\|w_T\\|_2$ as a float.\n3. Low-rate boundary case: $d = 32$, $T = 200{,}000$, $\\lambda_i = 5$ for all $i$, $b = -3.0$, $g = 0.5$, $\\eta = 0.005$, $\\delta = 0.01$, random seed $789$, and $w_\\ast$ a random unit-norm vector drawn from a standard normal distribution and normalized to unit length. Output the cosine similarity $\\mathrm{cos\\_sim}(w_T, \\hat{m})$ as a float.\n\nFinal output format requirement:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3]\"), in the order of the three test cases listed above.",
            "solution": "The problem statement is validated as scientifically grounded, well-posed, objective, and complete. It describes a standard scenario in computational neuroscience and neuromorphic engineering. All parameters and conditions are specified, allowing for both a theoretical derivation and a reproducible numerical simulation.\n\n### Theoretical Derivation\n\nThe objective is to demonstrate from first principles that the learning rule aligns the synaptic weight vector $w_t$ with the spike-triggered mean (STM) of the centered input, $m = \\mathbb{E}[ z_t \\mid s_t = 1 ]$. The empirical estimator of the STM is $\\hat{m}$, and for a sufficiently large number of time steps $T$, we expect $\\hat{m} \\approx m$. Therefore, showing alignment with $m$ implies eventual alignment with $\\hat{m}$.\n\nThe synaptic weight vector $w_t \\in \\mathbb{R}^d$ evolves according to the correlation-based update rule with a homeostatic decay term:\n$$\nw_{t+1} = w_t + \\eta \\left( s_t z_t - \\delta\\, w_t \\right)\n$$\nwhere $\\eta  0$ is the learning rate, $\\delta  0$ is the decay coefficient, $s_t \\in \\{0, 1\\}$ is the post-synaptic spike indicator, and $z_t \\in \\mathbb{R}^d$ is the centered pre-synaptic activity.\n\nTo understand the long-term behavior of $w_t$, we analyze the expected change in the weights over one time step. We take the expectation of the update, $\\Delta w_t = w_{t+1} - w_t$, with respect to the distributions of the stochastic variables $z_t$ and $s_t$. The expectation is conditioned on the current weight state $w_t$.\n$$\n\\mathbb{E}[\\Delta w_t \\mid w_t] = \\mathbb{E}[w_{t+1} - w_t \\mid w_t] = \\mathbb{E}\\left[ \\eta (s_t z_t - \\delta w_t) \\mid w_t \\right]\n$$\nBy linearity of expectation, and noting that $w_t$ is constant within the expectation at time $t$:\n$$\n\\mathbb{E}[\\Delta w_t \\mid w_t] = \\eta \\left( \\mathbb{E}[s_t z_t] - \\delta \\mathbb{E}[w_t] \\right) = \\eta \\left( \\mathbb{E}[s_t z_t] - \\delta w_t \\right)\n$$\nThe dependency of the distributions of $s_t$ and $z_t$ on $w_t$ is not present, as $p_t$ depends on $w_\\ast$, not $w_t$.\n\nThe core of the analysis lies in evaluating the correlation term $\\mathbb{E}[s_t z_t]$. We can evaluate this using the law of total expectation, by conditioning on the value of the spike indicator $s_t$:\n$$\n\\mathbb{E}[s_t z_t] = \\mathbb{E}[s_t z_t \\mid s_t=1] P(s_t=1) + \\mathbb{E}[s_t z_t \\mid s_t=0] P(s_t=0)\n$$\nWhen $s_t=1$, the expression becomes $\\mathbb{E}[1 \\cdot z_t \\mid s_t=1] P(s_t=1)$. The term $\\mathbb{E}[z_t \\mid s_t=1]$ is, by definition, the spike-triggered mean $m$.\nWhen $s_t=0$, the expression becomes $\\mathbb{E}[0 \\cdot z_t \\mid s_t=0] P(s_t=0) = 0$.\n\nLet $P_{\\text{spike}} = P(s_t=1)$ denote the average probability of a post-synaptic spike. Substituting these into the equation for $\\mathbb{E}[s_t z_t]$ gives:\n$$\n\\mathbb{E}[s_t z_t] = m \\cdot P_{\\text{spike}} + 0 = P_{\\text{spike}} \\, m\n$$\nThis result shows that the expected correlation between the spike and the centered input is the spike-triggered mean, scaled by the average firing rate.\n\nNow, we substitute this back into the equation for the expected weight update:\n$$\n\\mathbb{E}[\\Delta w_t \\mid w_t] = \\eta \\left( P_{\\text{spike}} \\, m - \\delta w_t \\right)\n$$\nThe system reaches a stable point, or equilibrium, when the expected change in the weights is zero, i.e., $\\mathbb{E}[\\Delta w_t \\mid w_t] = 0$. Let $w_{\\text{eq}}$ be the weight vector at this equilibrium.\n$$\n\\eta \\left( P_{\\text{spike}} \\, m - \\delta w_{\\text{eq}} \\right) = 0\n$$\nSince $\\eta  0$ and $\\delta  0$, we can solve for $w_{\\text{eq}}$:\n$$\n\\delta w_{\\text{eq}} = P_{\\text{spike}} \\, m\n$$\n$$\nw_{\\text{eq}} = \\frac{P_{\\text{spike}}}{\\delta} m\n$$\nThis derivation demonstrates that the fixed point of the learning rule's expectation, $w_{\\text{eq}}$, is directly proportional to the true spike-triggered mean, $m$. The constant of proportionality is given by the ratio of the average firing rate $P_{\\text{spike}}$ to the decay coefficient $\\delta$.\n\nThe learning rule effectively performs stochastic gradient ascent on an underlying objective function whose maximum is located at $w_{\\text{eq}}$. Over many time steps, the synaptic weight vector $w_t$ will fluctuate around this expected equilibrium value. The empirical spike-triggered mean, $\\hat{m} = \\frac{1}{N_{\\text{spike}}} \\sum_{t: s_t = 1} z_t$, is a statistical estimator for the true STM, $m$. For a large number of time steps $T$ and a sufficient number of spikes $N_{\\text{spike}}$, the Law of Large Numbers ensures that $\\hat{m}$ converges to $m$. Consequently, the final weight vector $w_T$ is expected to be aligned with $\\hat{m}$, as both are estimators (up to a scaling factor) of the same underlying directional quantity rooted in the statistics of the neural activity.\n\n### Simulation Implementation\n\nThe provided Python code will simulate the described process for the three given test cases. It implements the discrete-time simulation of the pre-synaptic Poisson inputs, the probabilistic post-synaptic spiking, and the synaptic weight updates. After running the simulation for $T$ time steps, it calculates the final weight vector $w_T$ and the empirical STM $\\hat{m}$. Finally, it computes the metric specified for each test case—cosine similarity or vector norm—and prints the results in the required format.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by running three distinct simulations of a synaptic learning rule\n    and computes the specified metric for each case.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (d, T, lam_val, b, g, eta, delta, seed, w_star_mode, metric)\n        (32, 120_000, 5.0, -0.75, 0.15, 0.01, 0.01, 123, 'random', 'cos_sim'),\n        (32, 120_000, 5.0, -0.75, 0.0, 0.01, 0.01, 456, 'zero', 'norm_wT'),\n        (32, 200_000, 5.0, -3.0, 0.5, 0.005, 0.01, 789, 'random', 'cos_sim'),\n    ]\n\n    results = []\n\n    def sigma(u):\n        \"\"\"\n        Computes the logistic sigmoid function with clipping to prevent overflow.\n        \"\"\"\n        # Clip input to a safe range for np.exp to avoid overflow.\n        u_clipped = np.clip(u, -700, 700)\n        return 1.0 / (1.0 + np.exp(-u_clipped))\n\n    for case in test_cases:\n        d, T, lam_val, b, g, eta, delta, seed, w_star_mode, metric = case\n        \n        rng = np.random.default_rng(seed)\n\n        if w_star_mode == 'random':\n            w_star_raw = rng.standard_normal(d)\n            norm_w_star = np.linalg.norm(w_star_raw)\n            # Normalize to unit length, handling the unlikely case of a zero vector.\n            w_star = w_star_raw if norm_w_star == 0 else w_star_raw / norm_w_star\n        elif w_star_mode == 'zero':\n            w_star = np.zeros(d)\n\n        lam_vec = np.full(d, lam_val)\n        w = np.zeros(d)\n        \n        z_sum_spiked = np.zeros(d)\n        n_spike = 0\n        \n        # Pre-compute the decay factor for the weight update\n        decay_factor = 1.0 - eta * delta\n        \n        for _ in range(T):\n            # 1. Generate pre-synaptic Poisson spikes\n            x_t = rng.poisson(lam_vec)\n            \n            # 2. Compute centered input\n            z_t = x_t - lam_vec\n            \n            # 3. Compute post-synaptic spiking probability\n            p_t = sigma(b + g * np.dot(w_star, z_t))\n            \n            # 4. Generate post-synaptic spike\n            s_t = 1 if rng.random()  p_t else 0\n            \n            # 5. Accumulate spike-triggered inputs\n            if s_t == 1:\n                z_sum_spiked += z_t\n                n_spike += 1\n            \n            # 6. Apply learning rule\n            w = decay_factor * w + eta * s_t * z_t\n\n        w_T = w\n        \n        if n_spike > 0:\n            m_hat = z_sum_spiked / n_spike\n        else:\n            m_hat = np.zeros(d)\n\n        # Calculate the result based on the specified metric for the test case\n        if metric == 'cos_sim':\n            norm_wT = np.linalg.norm(w_T)\n            norm_m_hat = np.linalg.norm(m_hat)\n            \n            # Per problem instructions, if m_hat is a zero vector, use norm of w_T.\n            if norm_m_hat == 0:\n                result = norm_wT\n            # Otherwise, if w_T is zero, the similarity is zero.\n            elif norm_wT == 0:\n                result = 0.0\n            # Standard cosine similarity calculation\n            else:\n                result = np.dot(w_T, m_hat) / (norm_wT * norm_m_hat)\n        \n        elif metric == 'norm_wT':\n            # This metric is explicitly requested for Test Case 2\n            result = np.linalg.norm(w_T)\n        \n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        }
    ]
}