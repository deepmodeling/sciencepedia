{
    "hands_on_practices": [
        {
            "introduction": "The reliability of a rate code depends critically on the stability of the spike count over a given observation window. This practice explores the fundamental link between the microscopic regularity of a spike train, characterized by its inter-spike intervals (ISIs), and the macroscopic stability of the resulting spike count. By deriving the relationship between the ISI's coefficient of variation ($CV$) and the spike count's asymptotic Fano factor ($F_{\\infty}$), you will gain a deep understanding of the conditions that favor stable rate-based information transmission .",
            "id": "4056661",
            "problem": "Consider a stationary spike train modeled as a renewal point process with independent and identically distributed inter-spike intervals (ISI) $\\{X_i\\}_{i \\ge 1}$, where $X_i$ has finite mean $\\mu$ and variance $\\sigma_{I}^{2}$. Let $N_T$ denote the spike count in an observation window of duration $T$, and let the empirical rate be $R_T = N_T/T$. Define the coefficient of variation (CV) of the ISI as $\\mathrm{CV} = \\sigma_{I}/\\mu$, and define the Fano factor of the count as $F_T = \\mathrm{Var}(N_T)/\\mathbb{E}[N_T]$. Starting from the definitions of renewal processes and basic limit theorems for sums of independent and identically distributed random variables, derive the asymptotic large-window limit $F_{\\infty} = \\lim_{T \\to \\infty} F_T$ in terms of $\\mu$ and $\\sigma_{I}^{2}$. Then, for each of the following ISI models, compute the expressions for $\\mathrm{CV}$ and $F_{\\infty}$ in terms of the model parameters:\n- Poisson process with exponential ISI of rate $\\lambda0$.\n- Gamma-renewal with shape $k0$ and scale $b0$.\n- Lognormal ISI with $\\ln X \\sim \\mathcal{N}(m,s^{2})$ where $s0$.\n\nAssume an operational rate-coding stability criterion that requires sub-Poisson variability in the large-window limit, namely $F_{\\infty} \\le \\theta$ for a fixed target variability level $0\\theta1$. Using your derived expressions, determine, for the lognormal ISI model, the maximal log-standard deviation $s_{\\star}$ (as a function of $\\theta$) that guarantees the stability criterion $F_{\\infty} \\le \\theta$. Provide $s_{\\star}$ as a single exact symbolic expression in terms of $\\theta$. No rounding is required, and no units are involved. In your reasoning, justify which parameter regimes for the gamma-renewal and lognormal ISI models favor rate-coding stability under the stated criterion.",
            "solution": "The problem is well-posed and scientifically grounded in the theory of point processes, specifically renewal processes, which are a standard model for neuronal spike trains. All definitions are standard and the tasks are mathematically tractable. I will proceed with the solution.\n\nThe problem asks for several derivations and calculations related to the statistical properties of a stationary spike train modeled as a renewal process.\n\nFirst, we derive the asymptotic Fano factor, $F_{\\infty}$, in terms of the mean $\\mu$ and variance $\\sigma_{I}^{2}$ of the inter-spike interval (ISI) distribution. A renewal process is defined by a sequence of independent and identically distributed (i.i.d.) positive random variables $\\{X_i\\}_{i \\ge 1}$, representing the ISIs. The time of the $n$-th event (spike) is given by the sum $S_n = \\sum_{i=1}^{n} X_i$. The number of events in the interval $[0, T]$ is $N_T = \\max\\{n \\ge 0 : S_n \\le T\\}$.\n\nBy the Elementary Renewal Theorem, the long-term average rate of events is the reciprocal of the mean ISI:\n$$ \\lim_{T \\to \\infty} \\frac{\\mathbb{E}[N_T]}{T} = \\frac{1}{\\mu} $$\nThus, for large $T$, the expected number of spikes is $\\mathbb{E}[N_T] \\approx \\frac{T}{\\mu}$.\n\nThe Central Limit Theorem for renewal processes states that for large $T$, the distribution of $N_T$ approaches a normal distribution with mean $T/\\mu$ and variance $T\\sigma_I^2/\\mu^3$.\n$$ \\mathrm{Var}(N_T) \\approx T \\frac{\\sigma_{I}^{2}}{\\mu^3} $$\nThe Fano factor is defined as $F_T = \\mathrm{Var}(N_T) / \\mathbb{E}[N_T]$. We seek its asymptotic limit as $T \\to \\infty$:\n$$ F_{\\infty} = \\lim_{T \\to \\infty} F_T = \\lim_{T \\to \\infty} \\frac{\\mathrm{Var}(N_T)}{\\mathbb{E}[N_T]} $$\nSubstituting the asymptotic expressions for the mean and variance:\n$$ F_{\\infty} = \\lim_{T \\to \\infty} \\frac{T \\frac{\\sigma_{I}^{2}}{\\mu^3}}{T \\frac{1}{\\mu}} = \\frac{\\sigma_{I}^{2}/\\mu^3}{1/\\mu} = \\frac{\\sigma_{I}^{2}}{\\mu^2} $$\nThis expression is the square of the coefficient of variation (CV) of the ISI, where $\\mathrm{CV} = \\sigma_{I}/\\mu$.\n$$ F_{\\infty} = \\left(\\frac{\\sigma_{I}}{\\mu}\\right)^2 = \\mathrm{CV}^2 $$\nThis is a fundamental result in renewal theory: the asymptotic Fano factor of the spike count equals the squared coefficient of variation of the ISI.\n\nNext, we compute $\\mathrm{CV}$ and $F_{\\infty}$ for the three specified ISI models.\n\n1.  **Poisson process with exponential ISI:**\n    The ISI distribution is the exponential distribution with rate $\\lambda  0$, having a probability density function $f(x) = \\lambda \\exp(-\\lambda x)$ for $x \\ge 0$.\n    The mean is $\\mu = \\mathbb{E}[X] = 1/\\lambda$.\n    The variance is $\\sigma_{I}^{2} = \\mathrm{Var}(X) = 1/\\lambda^2$.\n    The coefficient of variation is $\\mathrm{CV} = \\sigma_{I}/\\mu = (1/\\lambda) / (1/\\lambda) = 1$.\n    The asymptotic Fano factor is $F_{\\infty} = \\mathrm{CV}^2 = 1^2 = 1$.\n    A Poisson process is characterized by $F_T=1$ for all $T$, so the asymptotic result is consistent.\n\n2.  **Gamma-renewal with Gamma ISI:**\n    The ISI is drawn from a Gamma distribution with shape parameter $k  0$ and scale parameter $b  0$.\n    The mean is $\\mu = \\mathbb{E}[X] = kb$.\n    The variance is $\\sigma_{I}^{2} = \\mathrm{Var}(X) = kb^2$.\n    The coefficient of variation is $\\mathrm{CV} = \\sigma_{I}/\\mu = \\sqrt{kb^2} / (kb) = \\sqrt{k}b / (kb) = 1/\\sqrt{k}$.\n    The asymptotic Fano factor is $F_{\\infty} = \\mathrm{CV}^2 = (1/\\sqrt{k})^2 = 1/k$.\n\n3.  **Lognormal ISI:**\n    The ISI $X$ is such that its natural logarithm, $Y = \\ln X$, follows a normal distribution $\\mathcal{N}(m, s^2)$ with $s  0$.\n    The mean of the lognormal distribution is $\\mu = \\mathbb{E}[X] = \\exp(m + s^2/2)$.\n    The variance is $\\sigma_{I}^{2} = \\mathrm{Var}(X) = (\\exp(s^2) - 1)\\exp(2m + s^2)$.\n    We compute the squared coefficient of variation:\n    $$ \\mathrm{CV}^2 = \\frac{\\sigma_{I}^{2}}{\\mu^2} = \\frac{(\\exp(s^2) - 1)\\exp(2m + s^2)}{(\\exp(m + s^2/2))^2} = \\frac{(\\exp(s^2) - 1)\\exp(2m + s^2)}{\\exp(2m + s^2)} = \\exp(s^2) - 1 $$\n    The coefficient of variation is $\\mathrm{CV} = \\sqrt{\\exp(s^2) - 1}$.\n    The asymptotic Fano factor is $F_{\\infty} = \\mathrm{CV}^2 = \\exp(s^2) - 1$.\n\nNow, we analyze the rate-coding stability criterion, $F_{\\infty} \\le \\theta$ for $0  \\theta  1$. This criterion requires sub-Poisson variability.\nFor the **Gamma-renewal model**, the criterion is $F_{\\infty} = 1/k \\le \\theta$. This is equivalent to $k \\ge 1/\\theta$. Since $0  \\theta  1$, this necessitates $k  1$. A larger shape parameter $k$ makes the Gamma distribution less skewed and more concentrated around its mean (for $k  1$), leading to more regular spiking. Thus, the regime of large $k$ ($k  1/\\theta$) favors rate-coding stability under this criterion.\n\nFor the **Lognormal ISI model**, the criterion is $F_{\\infty} = \\exp(s^2) - 1 \\le \\theta$.\nThis inequality can be solved for the parameter $s$:\n$$ \\exp(s^2) \\le 1 + \\theta $$\nTaking the natural logarithm of both sides (which is a monotonic function):\n$$ s^2 \\le \\ln(1 + \\theta) $$\nSince we are given $s  0$, we can take the square root:\n$$ s \\le \\sqrt{\\ln(1 + \\theta)} $$\nNote that since $0  \\theta  1$, we have $1  1 + \\theta  2$, so $\\ln(1 + \\theta)$ is positive and well-defined.\nThe maximal value of the log-standard deviation, $s_{\\star}$, that satisfies the stability criterion is obtained at the boundary of this inequality:\n$$ s_{\\star} = \\sqrt{\\ln(1 + \\theta)} $$\nFor the lognormal model, lower values of the parameter $s$ lead to a smaller $F_{\\infty}$, as $F_{\\infty} \\to 0$ when $s \\to 0$. A smaller $s$ corresponds to an ISI distribution that is more sharply peaked and less variable, indicating more regular spiking. Therefore, the regime of small $s$ ($0  s \\le s_{\\star}$) favors rate-coding stability.\n\nIn summary, for both the gamma and lognormal models, rate-coding stability (as defined by sub-Poisson variability) is achieved when the ISI distribution becomes more regular, i.e., has a lower coefficient of variation. This corresponds to large $k$ for the gamma model and small $s$ for the lognormal model. The final task is to provide the expression for $s_{\\star}$.",
            "answer": "$$\\boxed{\\sqrt{\\ln(1 + \\theta)}}$$"
        },
        {
            "introduction": "Having established the foundations of rate code stability, we now explore its limitations. This exercise presents a classic scenario where a pure rate code is ambiguous: two distinct stimuli elicit the same mean firing rate. You will apply the Neyman-Pearson principle to construct an optimal decoder that distinguishes the stimuli by leveraging the full temporal structure of the spike train—specifically, the shape of the ISI distribution—thereby demonstrating the power of temporal coding .",
            "id": "4056642",
            "problem": "A single-compartment sensory neuron is observed under two alternative stimuli, denoted $S_A$ and $S_B$. Under either stimulus, the spike train is well modeled as a renewal process: interspike intervals (ISIs) are independent and identically distributed (i.i.d.) and fully characterize the spike statistics. The neuromorphic readout must decide which stimulus was presented based only on a sequence of observed ISIs, exploiting temporal information beyond a mean-rate code.\n\nAssume:\n- Under $S_A$, each ISI $\\Delta t$ follows a Gamma distribution with shape $k_A$ and scale $\\theta_A$, with parameters $k_A = 2$ and $\\theta_A = 10\\,\\mathrm{ms}$.\n- Under $S_B$, each ISI $\\Delta t$ follows a Gamma distribution with shape $k_B$ and scale $\\theta_B$, with parameters $k_B = 5$ and $\\theta_B = 4\\,\\mathrm{ms}$.\n\nNote that the mean ISI is $k \\theta$ for a Gamma distribution, so both stimuli induce the same mean ISI $\\mu = 20\\,\\mathrm{ms}$ and hence the same mean firing rate $r = 1/\\mu$, even though the ISI distributions differ. This captures a setting where a pure rate code is ambiguous but temporal statistics differ.\n\nYou observe $N=8$ consecutive, complete ISIs (in milliseconds): $\\{10, 15, 15, 20, 20, 25, 25, 30\\}$. Starting only from the renewal assumption (i.i.d. ISIs), standard properties of the Gamma distribution, and the Neyman–Pearson principle for hypothesis testing, derive the form of the optimal likelihood ratio test that distinguishes $S_A$ from $S_B$ using the observed ISIs $\\{\\Delta t_i\\}_{i=1}^{N}$, and then evaluate the natural logarithm of the likelihood ratio, $\\ln \\Lambda(\\{\\Delta t_i\\})$, for the given data.\n\nReport the final numerical value of $\\ln \\Lambda$ rounded to four significant figures. The natural logarithm of the likelihood ratio is dimensionless; therefore, no physical units should be included in your final answer.",
            "solution": "The appropriate modeling base consists of: (i) the renewal process definition implying independent and identically distributed (i.i.d.) interspike intervals (ISIs), (ii) the Gamma distribution as the parametric model for ISIs under each stimulus, and (iii) the Neyman–Pearson lemma, which states that the likelihood ratio test (LRT) is optimal for distinguishing two simple hypotheses.\n\nFirst, under stimulus $S \\in \\{S_A,S_B\\}$, the ISIs $\\{\\Delta t_i\\}_{i=1}^{N}$ are i.i.d. with density $f_S(t)$ given by the Gamma probability density function with shape $k_S$ and scale $\\theta_S$:\n$$\nf_S(t) \\;=\\; \\frac{t^{k_S - 1} \\exp\\!\\left(-\\frac{t}{\\theta_S}\\right)}{\\Gamma(k_S)\\,\\theta_S^{k_S}}, \\quad t  0.\n$$\nBy the renewal (i.i.d.) assumption, the joint likelihood for the $N$ observed ISIs under stimulus $S$ is\n$$\n\\mathcal{L}_S(\\{\\Delta t_i\\}_{i=1}^{N}) \\;=\\; \\prod_{i=1}^{N} f_S(\\Delta t_i).\n$$\n\nThe likelihood ratio test compares\n$$\n\\Lambda(\\{\\Delta t_i\\}) \\;=\\; \\frac{\\mathcal{L}_A(\\{\\Delta t_i\\})}{\\mathcal{L}_B(\\{\\Delta t_i\\})} \\;=\\; \\prod_{i=1}^{N} \\frac{f_A(\\Delta t_i)}{f_B(\\Delta t_i)}.\n$$\nIt is customary and numerically stable to work with the natural logarithm:\n$$\n\\ln \\Lambda(\\{\\Delta t_i\\}) \\;=\\; \\sum_{i=1}^{N} \\left[ \\ln f_A(\\Delta t_i) - \\ln f_B(\\Delta t_i) \\right].\n$$\nUsing the Gamma log-density,\n$$\n\\ln f_S(t) \\;=\\; (k_S - 1)\\ln t \\;-\\; \\frac{t}{\\theta_S} \\;-\\; \\ln\\Gamma(k_S) \\;-\\; k_S \\ln \\theta_S,\n$$\nwe obtain\n$$\n\\ln \\Lambda(\\{\\Delta t_i\\})\n\\;=\\;\n\\sum_{i=1}^{N} \\Big[ (k_A - k_B)\\,\\ln \\Delta t_i \\;-\\; \\Delta t_i \\left(\\frac{1}{\\theta_A} - \\frac{1}{\\theta_B}\\right) \\Big]\n\\;+\\;\nN \\Big[ -\\ln \\Gamma(k_A) - k_A \\ln \\theta_A + \\ln \\Gamma(k_B) + k_B \\ln \\theta_B \\Big].\n$$\n\nThis expression shows explicitly how temporal structure beyond the mean rate contributes to decoding: although $k_A \\theta_A = k_B \\theta_B$ implies identical mean ISIs (and hence the same mean firing rate), the shape parameters $k_A \\neq k_B$ lead to different weightings of the $\\ln \\Delta t_i$ terms as well as different exponential decay rates via $\\theta_A$ and $\\theta_B$. A rate-only decoder that compresses the spike train to a single average rate would discard these terms and be unable to distinguish the stimuli in expectation, whereas the renewal-based LRT leverages the ISI distributional differences.\n\nWe now evaluate $\\ln \\Lambda$ for the given parameters and data. The parameters are $k_A = 2$, $\\theta_A = 10\\,\\mathrm{ms}$, $k_B = 5$, $\\theta_B = 4\\,\\mathrm{ms}$, and $N = 8$. The observed ISIs (in milliseconds) are $\\{\\Delta t_i\\} = \\{10, 15, 15, 20, 20, 25, 25, 30\\}$.\n\nFirst compute the sufficient summaries appearing in the log-likelihood ratio:\n- The sum of the ISIs is\n$$\n\\sum_{i=1}^{N} \\Delta t_i \\;=\\; 10 + 15 + 15 + 20 + 20 + 25 + 25 + 30 \\;=\\; 160.\n$$\n- The sum of the logarithms of the ISIs is\n$$\n\\sum_{i=1}^{N} \\ln \\Delta t_i \\;=\\; \\ln 10 + 2 \\ln 15 + 2 \\ln 20 + 2 \\ln 25 + \\ln 30.\n$$\nUsing standard values,\n$$\n\\ln 10 \\approx 2.302585093,\\quad\n\\ln 15 \\approx 2.708050201,\\quad\n\\ln 20 \\approx 2.995732274,\\quad\n\\ln 25 \\approx 3.218875825,\\quad\n\\ln 30 \\approx 3.401197382,\n$$\nwe obtain\n$$\n\\sum_{i=1}^{N} \\ln \\Delta t_i \\;\\approx\\; 2.302585093 + 2(2.708050201) + 2(2.995732274) + 2(3.218875825) + 3.401197382 \\;\\approx\\; 23.549099075.\n$$\n\nNext compute the parameter-dependent constants:\n$$\nk_A - k_B \\;=\\; 2 - 5 \\;=\\; -3,\\qquad\n\\frac{1}{\\theta_A} - \\frac{1}{\\theta_B} \\;=\\; \\frac{1}{10} - \\frac{1}{4} \\;=\\; 0.1 - 0.25 \\;=\\; -0.15.\n$$\nTherefore,\n$$\n\\sum_{i=1}^{N} \\Big[ (k_A - k_B)\\,\\ln \\Delta t_i \\;-\\; \\Delta t_i \\left(\\frac{1}{\\theta_A} - \\frac{1}{\\theta_B}\\right) \\Big]\n\\;=\\;\n(-3)\\sum_{i=1}^{N} \\ln \\Delta t_i \\;+\\; 0.15 \\sum_{i=1}^{N} \\Delta t_i\n\\;\\approx\\;\n(-3)(23.549099075) \\;+\\; 0.15(160).\n$$\nNumerically,\n$$\n(-3)(23.549099075) \\approx -70.647297225,\\qquad 0.15(160) = 24,\n$$\nso this sum is\n$$\n-70.647297225 + 24 \\;=\\; -46.647297225.\n$$\n\nNow compute the constant term\n$$\nC \\;\\equiv\\; -\\ln \\Gamma(k_A) - k_A \\ln \\theta_A + \\ln \\Gamma(k_B) + k_B \\ln \\theta_B.\n$$\nWe have $\\Gamma(2) = 1! = 1$ so $\\ln \\Gamma(2) = 0$, and $\\Gamma(5) = 4! = 24$ so $\\ln \\Gamma(5) = \\ln 24 \\approx 3.178053830$. Also $\\ln \\theta_A = \\ln 10 \\approx 2.302585093$ and $\\ln \\theta_B = \\ln 4 \\approx 1.386294361$. Thus,\n$$\nC \\;\\approx\\; -0 - 2 \\cdot 2.302585093 + 3.178053830 + 5 \\cdot 1.386294361\n\\;\\approx\\; -4.605170186 + 3.178053830 + 6.931471805 \\;\\approx\\; 5.504355450.\n$$\nMultiplying by $N = 8$ gives\n$$\nN C \\;\\approx\\; 8 \\cdot 5.504355450 \\;\\approx\\; 44.034843600.\n$$\n\nFinally, assemble the log-likelihood ratio:\n$$\n\\ln \\Lambda(\\{\\Delta t_i\\}) \\;\\approx\\; \\left[-46.647297225\\right] + \\left[44.034843600\\right] \\;=\\; -2.612453625.\n$$\nRounded to four significant figures, this is approximately $-2.612$.\n\nInterpretation: since $\\ln \\Lambda  0$, the likelihood ratio $\\Lambda$ favors $S_B$ over $S_A$ for this sample, demonstrating that temporal coding via renewal memory can disambiguate stimuli with identical mean firing rates by exploiting differences in the ISI distribution.",
            "answer": "$$\\boxed{-2.612}$$"
        },
        {
            "introduction": "When both rate and temporal information are available, how can we formally compare their effectiveness for encoding a stimulus? This problem introduces a powerful framework for quantifying and comparing the precision of different neural codes using Fisher information. By deriving the mean squared error ($MSE$) for an optimal estimator that reconstructs a stimulus from either population firing rates or first-spike latencies, you will perform a rigorous quantitative comparison and gain insight into the trade-offs governing neural coding strategies .",
            "id": "4056669",
            "problem": "Consider a one-dimensional stimulus parameter $\\theta$ encoded by a heterogeneous population of $N=3$ neurons using two coding schemes: rate coding (mean firing rates) and temporal coding (first-spike latencies). Assume a local operating point at $\\theta_0 = 0.5$ radians. The known encoding maps are:\n- Rate maps (in $\\mathrm{Hz}$): $r_1(\\theta) = 20 + 5\\,\\theta$, $r_2(\\theta) = 15 + 10\\,\\theta$, $r_3(\\theta) = 30 - 4\\,\\theta$.\n- First-spike latency maps (in $\\mathrm{s}$): $t_{1,1}(\\theta) = 0.150 - 0.030\\,\\theta$, $t_{1,2}(\\theta) = 0.200 - 0.050\\,\\theta$, $t_{1,3}(\\theta) = 0.120 - 0.020\\,\\theta$.\n\nYou measure rates by counting spikes in a window of duration $T = 0.25\\,\\mathrm{s}$, and you assume independent Poisson spiking across neurons. For the empirical rate estimate $y_i^{(r)} = k_i/T$ with $k_i \\sim \\mathrm{Poisson}(r_i(\\theta)\\,T)$, the variance at $\\theta_0$ is $\\mathrm{Var}(y_i^{(r)} \\mid \\theta_0) = r_i(\\theta_0)/T$. You measure first-spike latencies with independent Gaussian jitter: $y_i^{(t)} = t_{1,i}(\\theta) + \\varepsilon_i$, where $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma_{t,i}^2)$ are independent and satisfy $\\sigma_{t,1} = 0.008\\,\\mathrm{s}$, $\\sigma_{t,2} = 0.010\\,\\mathrm{s}$, and $\\sigma_{t,3} = 0.006\\,\\mathrm{s}$.\n\nUsing an optimal linear unbiased estimator constructed from first principles at $\\theta_0$ for each coding scheme, compare the mean squared error (MSE) of reconstructing $\\theta$ from the rate vector versus from the latency vector. Specifically, form the locally optimal linear estimator for $\\theta$ in each modality under the stated noise models and the independence assumptions, and compute the ratio\n$$\n\\rho \\equiv \\frac{\\mathrm{MSE}_{\\text{rate}}}{\\mathrm{MSE}_{\\text{time}}}.\n$$\nExpress $\\rho$ as a dimensionless number and round your answer to four significant figures.",
            "solution": "The user-supplied problem is a well-posed and scientifically grounded question in theoretical neuroscience, specifically concerning the precision of neural codes. All necessary parameters and models are provided, the assumptions are clearly stated, and the objective is mathematically precise. The problem is therefore deemed valid.\n\nThe core of the problem is to compare the precision of two different neural coding schemes for a stimulus parameter $\\theta$. The precision of an unbiased estimator $\\hat{\\theta}$ is quantified by its mean squared error, $\\mathrm{MSE} = E[(\\hat{\\theta} - \\theta)^2]$, which for an unbiased estimator equals its variance, $\\mathrm{Var}(\\hat{\\theta})$. The Cramér-Rao Lower Bound (CRLB) provides a lower bound on the variance of any unbiased estimator, and the variance of an optimal (or efficient) estimator achieves this bound. The CRLB is given by the reciprocal of the Fisher Information, $\\mathcal{I}(\\theta)$.\n$$\n\\mathrm{MSE} = \\mathrm{Var}(\\hat{\\theta}) \\ge \\frac{1}{\\mathcal{I}(\\theta)}\n$$\nFor an optimal estimator, we have $\\mathrm{MSE} = 1/\\mathcal{I}(\\theta)$. The problem asks for the MSE of a locally optimal linear estimator at $\\theta_0$, which corresponds to this value.\n\nSince the neurons are assumed to be independent, the total Fisher Information for the population is the sum of the Fisher Informations from each neuron:\n$$\n\\mathcal{I}_{\\text{total}}(\\theta) = \\sum_{i=1}^{N} \\mathcal{I}_i(\\theta)\n$$\nFor a single neuron whose observation $y_i$ has a mean $\\mu_i(\\theta)$ and variance $\\sigma_i^2(\\theta)$, the Fisher information is generally given by $\\mathcal{I}_i(\\theta) = \\frac{(\\mu'_i(\\theta))^2}{\\sigma_i^2(\\theta)}$, provided the noise distribution meets certain conditions (e.g., Gaussian with constant variance, or from a one-parameter exponential family like Poisson), which are met here.\n\nWe will calculate the Fisher Information for each coding scheme at the operating point $\\theta_0 = 0.5$.\n\n**1. Rate Coding Scheme**\n\nFor the rate code, the observation for neuron $i$ is the empirical firing rate $y_i^{(r)}$. The mean of this observation is $E[y_i^{(r)}] = r_i(\\theta)$, and its variance is given as $\\mathrm{Var}(y_i^{(r)} \\mid \\theta) = r_i(\\theta)/T$.\nThe Fisher Information for neuron $i$ is:\n$$\n\\mathcal{I}_{i, \\text{rate}}(\\theta) = \\frac{\\left(\\frac{d}{d\\theta} r_i(\\theta)\\right)^2}{\\mathrm{Var}(y_i^{(r)} \\mid \\theta)} = \\frac{(r'_i(\\theta))^2}{r_i(\\theta)/T} = \\frac{(r'_i(\\theta))^2 T}{r_i(\\theta)}\n$$\nWe evaluate this at $\\theta_0 = 0.5$ for $T = 0.25\\,\\mathrm{s}$.\n\n- **Neuron 1:**\n  $r_1(\\theta) = 20 + 5\\theta \\implies r'_1(\\theta) = 5$.\n  $r_1(\\theta_0) = 20 + 5(0.5) = 22.5\\,\\mathrm{Hz}$.\n  $\\mathcal{I}_{1, \\text{rate}}(\\theta_0) = \\frac{5^2 \\times 0.25}{22.5} = \\frac{25 \\times 0.25}{22.5} = \\frac{6.25}{22.5} = \\frac{5}{18}$.\n\n- **Neuron 2:**\n  $r_2(\\theta) = 15 + 10\\theta \\implies r'_2(\\theta) = 10$.\n  $r_2(\\theta_0) = 15 + 10(0.5) = 20\\,\\mathrm{Hz}$.\n  $\\mathcal{I}_{2, \\text{rate}}(\\theta_0) = \\frac{10^2 \\times 0.25}{20} = \\frac{100 \\times 0.25}{20} = \\frac{25}{20} = \\frac{5}{4}$.\n\n- **Neuron 3:**\n  $r_3(\\theta) = 30 - 4\\theta \\implies r'_3(\\theta) = -4$.\n  $r_3(\\theta_0) = 30 - 4(0.5) = 28\\,\\mathrm{Hz}$.\n  $\\mathcal{I}_{3, \\text{rate}}(\\theta_0) = \\frac{(-4)^2 \\times 0.25}{28} = \\frac{16 \\times 0.25}{28} = \\frac{4}{28} = \\frac{1}{7}$.\n\nThe total Fisher Information for the rate code is:\n$$\n\\mathcal{I}_{\\text{rate}}(\\theta_0) = \\sum_{i=1}^{3} \\mathcal{I}_{i, \\text{rate}}(\\theta_0) = \\frac{5}{18} + \\frac{5}{4} + \\frac{1}{7}\n$$\nUsing a common denominator of $252$:\n$$\n\\mathcal{I}_{\\text{rate}}(\\theta_0) = \\frac{5 \\times 14}{252} + \\frac{5 \\times 63}{252} + \\frac{1 \\times 36}{252} = \\frac{70 + 315 + 36}{252} = \\frac{421}{252}\n$$\nThe MSE for the rate code is the reciprocal of the total Fisher Information:\n$$\n\\mathrm{MSE}_{\\text{rate}} = \\frac{1}{\\mathcal{I}_{\\text{rate}}(\\theta_0)} = \\frac{252}{421}\n$$\n\n**2. Temporal Coding Scheme**\n\nFor the temporal code, the observation for neuron $i$ is the first-spike latency $y_i^{(t)} = t_{1,i}(\\theta) + \\varepsilon_i$, where $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma_{t,i}^2)$.\nThe mean of this observation is $E[y_i^{(t)}] = t_{1,i}(\\theta)$, and its variance is $\\mathrm{Var}(y_i^{(t)}) = \\sigma_{t,i}^2$.\nThe Fisher Information for neuron $i$ is:\n$$\n\\mathcal{I}_{i, \\text{time}}(\\theta) = \\frac{\\left(\\frac{d}{d\\theta} t_{1,i}(\\theta)\\right)^2}{\\mathrm{Var}(y_i^{(t)})} = \\frac{(t'_{1,i}(\\theta))^2}{\\sigma_{t,i}^2}\n$$\nSince the tuning curves are linear and the noise variances are constant, this value is independent of $\\theta$.\n\n- **Neuron 1:**\n  $t_{1,1}(\\theta) = 0.150 - 0.030\\theta \\implies t'_{1,1}(\\theta) = -0.030$.\n  $\\sigma_{t,1} = 0.008\\,\\mathrm{s}$.\n  $\\mathcal{I}_{1, \\text{time}} = \\frac{(-0.030)^2}{(0.008)^2} = \\left(\\frac{0.030}{0.008}\\right)^2 = \\left(\\frac{30}{8}\\right)^2 = \\left(\\frac{15}{4}\\right)^2 = \\frac{225}{16}$.\n\n- **Neuron 2:**\n  $t_{1,2}(\\theta) = 0.200 - 0.050\\theta \\implies t'_{1,2}(\\theta) = -0.050$.\n  $\\sigma_{t,2} = 0.010\\,\\mathrm{s}$.\n  $\\mathcal{I}_{2, \\text{time}} = \\frac{(-0.050)^2}{(0.010)^2} = \\left(\\frac{0.050}{0.010}\\right)^2 = 5^2 = 25$.\n\n- **Neuron 3:**\n  $t_{1,3}(\\theta) = 0.120 - 0.020\\theta \\implies t'_{1,3}(\\theta) = -0.020$.\n  $\\sigma_{t,3} = 0.006\\,\\mathrm{s}$.\n  $\\mathcal{I}_{3, \\text{time}} = \\frac{(-0.020)^2}{(0.006)^2} = \\left(\\frac{0.020}{0.006}\\right)^2 = \\left(\\frac{20}{6}\\right)^2 = \\left(\\frac{10}{3}\\right)^2 = \\frac{100}{9}$.\n\nThe total Fisher Information for the temporal code is:\n$$\n\\mathcal{I}_{\\text{time}} = \\sum_{i=1}^{3} \\mathcal{I}_{i, \\text{time}} = \\frac{225}{16} + 25 + \\frac{100}{9}\n$$\nUsing a common denominator of $144$:\n$$\n\\mathcal{I}_{\\text{time}} = \\frac{225 \\times 9}{144} + \\frac{25 \\times 144}{144} + \\frac{100 \\times 16}{144} = \\frac{2025 + 3600 + 1600}{144} = \\frac{7225}{144}\n$$\nThe MSE for the temporal code is:\n$$\n\\mathrm{MSE}_{\\text{time}} = \\frac{1}{\\mathcal{I}_{\\text{time}}} = \\frac{144}{7225}\n$$\n\n**3. Ratio of MSEs**\n\nFinally, we compute the ratio $\\rho = \\frac{\\mathrm{MSE}_{\\text{rate}}}{\\mathrm{MSE}_{\\text{time}}}$.\n$$\n\\rho = \\frac{252/421}{144/7225} = \\frac{252}{421} \\times \\frac{7225}{144}\n$$\nWe can simplify the fraction $\\frac{252}{144}$:\n$$\n\\frac{252}{144} = \\frac{126}{72} = \\frac{63}{36} = \\frac{7}{4}\n$$\nSubstituting this back into the expression for $\\rho$:\n$$\n\\rho = \\frac{7}{4} \\times \\frac{7225}{421} = \\frac{50575}{1684}\n$$\nPerforming the division gives:\n$$\n\\rho \\approx 30.03265...\n$$\nRounding to four significant figures, we get $\\rho = 30.03$. This indicates that under the given conditions, the temporal code is approximately $30$ times more precise than the rate code for estimating the stimulus parameter $\\theta$.",
            "answer": "$$\\boxed{30.03}$$"
        }
    ]
}