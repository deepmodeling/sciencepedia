{
    "hands_on_practices": [
        {
            "introduction": "To truly understand neuronal computation, we must ground our models in biophysics. Neuronal refractoriness is not merely an abstract rule but a direct consequence of the intricate dynamics of voltage-gated ion channels. This first practice takes you to the heart of the mechanism, focusing on the sodium channel inactivation gate modeled by the Hodgkin-Huxley formalism. By deriving the recovery time constant $\\tau_h(V)$, you will mathematically demonstrate how the neuron's membrane potential influences its readiness to fire another action potential, providing a biophysical basis for the refractory period .",
            "id": "4053164",
            "problem": "A neuromorphic sodium inactivation gate is modeled by the Hodgkinâ€“Huxley (HH) formalism, where the inactivation variable $h(t)$ evolves under a voltage clamp at fixed membrane potential $V$ according to the kinetic equation\n$$\n\\frac{dh}{dt}=\\alpha_{h}(V)\\big(1-h\\big)-\\beta_{h}(V)\\,h,\n$$\nwith voltage-dependent rates $\\alpha_{h}(V)$ and $\\beta_{h}(V)$ that are nonnegative and bounded on any finite voltage interval. In neuromorphic and brain-inspired computing, the refractory recovery of sodium channels after a spike is governed by the relaxation dynamics of $h(t)$ back to its steady state $h_{\\infty}(V)$ when the membrane is held at a given $V$. The recovery time constant $\\tau_{h}(V)$ is defined as the characteristic time scale of the exponential approach of $h(t)$ to $h_{\\infty}(V)$ after a voltage step, assuming $V$ is held constant and $h$ is initially displaced from $h_{\\infty}(V)$.\n\n1. Starting from the kinetic equation above and the definition of exponential relaxation to steady state for a linear first-order system at fixed $V$, derive an expression for the recovery time constant $\\tau_{h}(V)$ in terms of $\\alpha_{h}(V)$ and $\\beta_{h}(V)$, without invoking any prederived gating formulas.\n\n2. To quantify the effect of hyperpolarization on recovery in a canonical HH-based neuromorphic design, use the classical rate functions\n$$\n\\alpha_{h}(V)=0.07\\,\\exp\\!\\left(-\\frac{V+65}{20}\\right),\\qquad\n\\beta_{h}(V)=\\frac{1}{\\exp\\!\\left(-\\frac{V+35}{10}\\right)+1},\n$$\nwhere $V$ is in millivolts and time is in milliseconds. Compute the ratio\n$$\nR=\\frac{\\tau_{h}(-95\\ \\text{mV})}{\\tau_{h}(-65\\ \\text{mV})}\n$$\nto demonstrate how hyperpolarization influences recovery speed. Express the final ratio as a pure number (unitless) and round your answer to four significant figures.",
            "solution": "The problem is valid as it is scientifically grounded in the Hodgkin-Huxley formalism, well-posed, and objective. All necessary information is provided, and the task involves a standard derivation and calculation within computational neuroscience.\n\n**Part 1: Derivation of the Recovery Time Constant $\\tau_{h}(V)$**\n\nWe begin with the kinetic equation for the sodium inactivation variable $h(t)$ at a fixed membrane potential $V$:\n$$\n\\frac{dh}{dt}=\\alpha_{h}(V)\\big(1-h\\big)-\\beta_{h}(V)\\,h\n$$\nSince the voltage $V$ is held constant (voltage clamp), the rate functions $\\alpha_{h}(V)$ and $\\beta_{h}(V)$ are constants with respect to time $t$. For notational simplicity during the derivation, let us denote them as $\\alpha_h$ and $\\beta_h$. We can expand and rearrange the differential equation:\n$$\n\\frac{dh}{dt} = \\alpha_h - \\alpha_h h - \\beta_h h\n$$\n$$\n\\frac{dh}{dt} = \\alpha_h - (\\alpha_h + \\beta_h)h\n$$\nThis is a first-order linear ordinary differential equation. To find the time constant, we can arrange it into the standard form $\\tau \\frac{dy}{dt} + y = y_{\\infty}$, where $\\tau$ is the time constant and $y_{\\infty}$ is the steady-state value.\n$$\n\\frac{dh}{dt} + (\\alpha_h + \\beta_h)h = \\alpha_h\n$$\nTo match the standard form, we divide the entire equation by the term $(\\alpha_h + \\beta_h)$:\n$$\n\\frac{1}{\\alpha_h + \\beta_h} \\frac{dh}{dt} + h = \\frac{\\alpha_h}{\\alpha_h + \\beta_h}\n$$\nThe steady-state value of $h$, denoted $h_{\\infty}$, is reached when the system is at equilibrium, i.e., when $\\frac{dh}{dt} = 0$. From the original rearranged equation:\n$$\n0 = \\alpha_h - (\\alpha_h + \\beta_h)h_{\\infty}\n$$\nSolving for $h_{\\infty}$ gives:\n$$\nh_{\\infty} = \\frac{\\alpha_h}{\\alpha_h + \\beta_h}\n$$\nSubstituting this back into our rearranged ODE, we get:\n$$\n\\frac{1}{\\alpha_h + \\beta_h} \\frac{dh}{dt} + h = h_{\\infty}\n$$\nBy comparing this to the canonical form $\\tau_h \\frac{dh}{dt} + h = h_{\\infty}$, we can directly identify the time constant $\\tau_h$. Reintroducing the explicit dependence on voltage $V$, we find:\n$$\n\\tau_h(V) = \\frac{1}{\\alpha_{h}(V) + \\beta_{h}(V)}\n$$\nThis is the expression for the recovery time constant.\n\n**Part 2: Calculation of the Ratio $R$**\n\nWe are asked to compute the ratio $R = \\frac{\\tau_{h}(-95\\ \\text{mV})}{\\tau_{h}(-65\\ \\text{mV})}$. Using the expression for $\\tau_h(V)$ derived in Part 1, we can write the ratio as:\n$$\nR = \\frac{\\frac{1}{\\alpha_{h}(-95) + \\beta_{h}(-95)}}{\\frac{1}{\\alpha_{h}(-65) + \\beta_{h}(-65)}} = \\frac{\\alpha_{h}(-65) + \\beta_{h}(-65)}{\\alpha_{h}(-95) + \\beta_{h}(-95)}\n$$\nThe given rate functions are:\n$$\n\\alpha_{h}(V)=0.07\\,\\exp\\!\\left(-\\frac{V+65}{20}\\right)\n$$\n$$\n\\beta_{h}(V)=\\frac{1}{\\exp\\!\\left(-\\frac{V+35}{10}\\right)+1}\n$$\nFirst, we evaluate the rates at $V = -65\\ \\text{mV}$:\n$$\n\\alpha_{h}(-65) = 0.07\\,\\exp\\!\\left(-\\frac{-65+65}{20}\\right) = 0.07\\,\\exp(0) = 0.07\n$$\n$$\n\\beta_{h}(-65) = \\frac{1}{\\exp\\!\\left(-\\frac{-65+35}{10}\\right)+1} = \\frac{1}{\\exp\\!\\left(-\\frac{-30}{10}\\right)+1} = \\frac{1}{\\exp(3)+1}\n$$\nThe sum in the numerator of $R$ is:\n$$\n\\alpha_{h}(-65) + \\beta_{h}(-65) = 0.07 + \\frac{1}{\\exp(3)+1}\n$$\nNext, we evaluate the rates at $V = -95\\ \\text{mV}$:\n$$\n\\alpha_{h}(-95) = 0.07\\,\\exp\\!\\left(-\\frac{-95+65}{20}\\right) = 0.07\\,\\exp\\!\\left(-\\frac{-30}{20}\\right) = 0.07\\,\\exp(1.5)\n$$\n$$\n\\beta_{h}(-95) = \\frac{1}{\\exp\\!\\left(-\\frac{-95+35}{10}\\right)+1} = \\frac{1}{\\exp\\!\\left(-\\frac{-60}{10}\\right)+1} = \\frac{1}{\\exp(6)+1}\n$$\nThe sum in the denominator of $R$ is:\n$$\n\\alpha_{h}(-95) + \\beta_{h}(-95) = 0.07\\,\\exp(1.5) + \\frac{1}{\\exp(6)+1}\n$$\nNow, we compute the numerical values.\nThe numerator is:\n$$\n0.07 + \\frac{1}{\\exp(3)+1} \\approx 0.07 + \\frac{1}{20.085537+1} = 0.07 + \\frac{1}{21.085537} \\approx 0.07 + 0.047426 = 0.117426\n$$\nThe denominator is:\n$$\n0.07\\,\\exp(1.5) + \\frac{1}{\\exp(6)+1} \\approx 0.07 \\times 4.481689 + \\frac{1}{403.428793+1} \\approx 0.313718 + \\frac{1}{404.428793} \\approx 0.313718 + 0.002473 = 0.316191\n$$\nFinally, we compute the ratio $R$:\n$$\nR = \\frac{0.117426}{0.316191} \\approx 0.37137508\n$$\nRounding the result to four significant figures, we get:\n$$\nR \\approx 0.3714\n$$\nThis result, being less than $1$, indicates that the recovery time constant is smaller at the hyperpolarized potential ($-95\\ \\text{mV}$) than at the reference potential ($-65\\ \\text{mV}$), meaning recovery from inactivation is faster.",
            "answer": "$$\\boxed{0.3714}$$"
        },
        {
            "introduction": "Building on the biophysical foundation of refractoriness, we now shift our focus to its functional impact on the neuron's output spike train. This exercise employs a classic and powerful abstraction: modeling the neuron as a renewal process with a fixed \"dead-time.\" Your task is to derive the relationship between the intrinsic drive to the neuron, its absolute refractory period $\\Delta$, and its resulting steady-state firing rate. This analysis will reveal a fundamental principle: the refractory period imposes a hard limit on the maximum firing rate, a critical constraint for neural coding and neuromorphic hardware design .",
            "id": "4053200",
            "problem": "In a neuromorphic spike generator emulating neuronal refractoriness, the output spike train is modeled as a renewal process driven by a constant hazard that is state dependent: after each output spike, the system enters an absolute refractory state of fixed duration $ \\Delta  0 $ during which no spikes can be emitted, and immediately upon leaving this state the system behaves as a memoryless emitter with constant hazard (instantaneous spike probability rate) $ \\lambda  0 $ as long as it is not refractory. Assume that the refractory state is non-paralyzable, meaning putative input events during the refractory interval do not extend its duration, and that the drive is stationary and constant in time so that stationarity of the output renewal process can be assumed.\n\nFrom first principles appropriate to renewal processes and point processes with state-dependent hazard, derive the steady-state firing rate $ \\nu $ of the output spike train under this constant drive. Your derivation should explicitly identify the interspike interval distribution implied by the specified state-dependent hazard, compute its mean, and invoke only foundational renewal arguments to connect this mean to the steady-state firing rate. Then, interpret the limiting behavior of $ \\nu $ as $ \\lambda \\to \\infty $ in terms of the refractory parameter $ \\Delta $ and the neuromorphic spiking mechanism described.\n\nProvide the final answer as a single closed-form analytic expression for $ \\nu $ in terms of $ \\lambda $ and $ \\Delta $. Do not include units in the final expression. You may discuss the limiting interpretation in your solution, but it must not alter the required form of the final answer.",
            "solution": "The problem asks for the steady-state firing rate, $ \\nu $, of a spike generator that operates as a renewal process. A key property of a stationary renewal process is that its long-term average rate is the reciprocal of the mean interspike interval (ISI). Let $ T $ be the random variable representing the duration of an ISI. The steady-state firing rate is then given by the elementary renewal theorem:\n$$ \\nu = \\frac{1}{\\mathbb{E}[T]} $$\nwhere $ \\mathbb{E}[T] $ is the mean of the ISI distribution. Our first task is to determine the probability distribution of $ T $.\n\nThe process of generating an interval $ T $ is described in two stages:\n1. An absolute refractory period of fixed duration $ \\Delta $. During this time, the probability of firing is zero.\n2. After this period, the system becomes a memoryless emitter with a constant hazard rate $ \\lambda $.\n\nLet's formalize this. The interspike interval $ T $ can be expressed as the sum of the deterministic refractory duration $ \\Delta $ and a random variable, let's call it $ T_{wait} $, which represents the waiting time for a spike *after* the refractory period has ended.\n$$ T = \\Delta + T_{wait} $$\nThe problem states that after time $ \\Delta $, the system has a constant hazard rate $ \\lambda $. A process with a constant hazard rate is a Poisson process. The waiting time for the next event in a Poisson process with rate $ \\lambda $ follows an exponential distribution with the same rate parameter.\nThus, the random variable $ T_{wait} $ is exponentially distributed with rate $ \\lambda $. The probability density function (PDF) of $ T_{wait} $, let's denote it as $ f_{T_{wait}}(t') $, is given by:\n$$ f_{T_{wait}}(t') = \\lambda \\exp(-\\lambda t') \\quad \\text{for } t' \\ge 0 $$\nand $ f_{T_{wait}}(t') = 0 $ for $ t'  0 $.\n\nNow we can find the PDF of the total interspike interval $ T $. Let this PDF be $ p(t) $.\nSince no spike can occur before the refractory period is over, $ p(t) = 0 $ for $ t  \\Delta $.\nFor $ t \\ge \\Delta $, an interval of duration $ t $ occurs if the waiting time after the refractory period, $ T_{wait} $, is exactly $ t - \\Delta $. Therefore, the probability density $ p(t) $ is given by the density of $ T_{wait} $ evaluated at $ t' = t - \\Delta $:\n$$ p(t) = f_{T_{wait}}(t - \\Delta) = \\lambda \\exp(-\\lambda (t - \\Delta)) \\quad \\text{for } t \\ge \\Delta $$\nThis distribution is a shifted exponential distribution.\n\nThe next step is to compute the mean interspike interval, $ \\mathbb{E}[T] $. We can use the linearity of expectation:\n$$ \\mathbb{E}[T] = \\mathbb{E}[\\Delta + T_{wait}] = \\mathbb{E}[\\Delta] + \\mathbb{E}[T_{wait}] $$\nSince $ \\Delta $ is a constant, its expectation is simply $ \\Delta $. The mean of an exponential distribution with rate parameter $ \\lambda $ is $ \\frac{1}{\\lambda} $. Therefore:\n$$ \\mathbb{E}[T_{wait}] = \\frac{1}{\\lambda} $$\nSubstituting these into the expression for $ \\mathbb{E}[T] $:\n$$ \\mathbb{E}[T] = \\Delta + \\frac{1}{\\lambda} $$\nThis is the mean interspike interval.\n\nFinally, we use the renewal theorem to find the steady-state firing rate $ \\nu $:\n$$ \\nu = \\frac{1}{\\mathbb{E}[T]} = \\frac{1}{\\Delta + \\frac{1}{\\lambda}} $$\nTo obtain a single closed-form expression, we can simplify this fraction:\n$$ \\nu = \\frac{1}{\\frac{\\lambda\\Delta + 1}{\\lambda}} = \\frac{\\lambda}{1 + \\lambda\\Delta} $$\nThis is the derived steady-state firing rate.\n\nThe problem also asks for an interpretation of the limiting behavior as $ \\lambda \\to \\infty $. Let's examine the limit of our expression for $ \\nu $:\n$$ \\lim_{\\lambda \\to \\infty} \\nu = \\lim_{\\lambda \\to \\infty} \\frac{\\lambda}{1 + \\lambda\\Delta} $$\nTo evaluate this limit, we can divide the numerator and the denominator by $ \\lambda $:\n$$ \\lim_{\\lambda \\to \\infty} \\frac{1}{\\frac{1}{\\lambda} + \\Delta} $$\nAs $ \\lambda \\to \\infty $, the term $ \\frac{1}{\\lambda} \\to 0 $. The limit then becomes:\n$$ \\lim_{\\lambda \\to \\infty} \\nu = \\frac{1}{0 + \\Delta} = \\frac{1}{\\Delta} $$\nThis limiting behavior has a clear physical interpretation. As the intrinsic hazard rate $ \\lambda $ becomes infinitely large, the waiting time for a spike after the refractory period, $ T_{wait} $, approaches zero in expectation ($ \\mathbb{E}[T_{wait}] = \\frac{1}{\\lambda} \\to 0 $). This means that a new spike is guaranteed to occur almost instantaneously after the refractory period $ \\Delta $ concludes. In this limit, the interspike intervals are no longer random but become fixed at the duration of the absolute refractory period, $ \\Delta $. The spike train becomes perfectly periodic with period $ \\Delta $. The firing rate of such a deterministic, periodic process is simply the reciprocal of its period, which is $ \\frac{1}{\\Delta} $. This confirms that the maximum possible firing rate of this neuromorphic spike generator is fundamentally constrained by its refractory mechanism. No matter how strong the input drive (represented by $ \\lambda $), the neuron cannot fire faster than $ \\frac{1}{\\Delta} $.",
            "answer": "$$ \\boxed{\\frac{\\lambda}{1 + \\lambda \\Delta}} $$"
        },
        {
            "introduction": "This final practice expands our scope from refractoriness to the more general phenomenon of spike-frequency adaptation and introduces a powerful tool from systems engineering to analyze its computational role. Using small-signal analysis and the Laplace transform, you will derive the transfer function for a linearized model of an adaptive neuron. This exercise reveals that adaptation is not just a limitation but a sophisticated signal processing feature, causing the neuron to act as a high-pass filter that emphasizes changes in its input while suppressing steady signals. This property is fundamental to efficient information processing in the brain .",
            "id": "4053276",
            "problem": "Consider a continuous-time neuromorphic adaptive neuron operated in the small-signal regime around a steady operating point. Let the input be a current perturbation $u(t)$ around a baseline, the state of the adaptation branch be $w(t)$, and the output firing-rate perturbation be $r(t)$. The static input-output characteristic of the instantaneous neuron nonlinearity is $r(t)=f(x(t))$ with $x(t)=u(t)-w(t)$. Assume that, near the operating point $x_0$, the static nonlinearity can be linearized with gain $g_0=f^{\\prime}(x_0)$ so that small perturbations satisfy $\\delta r(t)=g_0\\,\\delta x(t)=g_0\\big(\\delta u(t)-\\delta w(t)\\big)$. The adaptation branch is implemented physically as a leaky integrator that low-pass filters a unity-gain copy of the input current with time constant $\\tau_w0$, i.e., $\\tau_w\\,\\frac{d}{dt}\\,\\delta w(t)=-\\delta w(t)+\\delta u(t)$. This structure models spike-frequency adaptation and refractoriness implicitly through the small-signal slope $g_0$ of the static characteristic, while the adaptation dynamics are explicitly captured by the leaky integrator driven by the input.\n\nUsing only small-signal linearization, the capacitor law for a leaky integrator, and the definition of the unilateral Laplace transform with complex frequency variable $s$, derive the transfer function $H(s)=\\frac{R(s)}{U(s)}$ from input perturbation $u(t)$ to firing-rate perturbation $r(t)$, where $R(s)$ and $U(s)$ are the Laplace transforms of $\\delta r(t)$ and $\\delta u(t)$, respectively. Show from first principles that adaptation produces zero gain at direct current (DC), i.e., $H(0)=0$, and places a real pole at $s=-\\frac{1}{\\tau_w}$.\n\nProvide your final result as a single closed-form analytic expression for $H(s)$ in terms of $g_0$, $\\tau_w$, and $s$. No numerical approximation is required, and no units need be reported.",
            "solution": "The problem requires the derivation of the transfer function $H(s) = \\frac{R(s)}{U(s)}$ for a linearized model of an adaptive neuron. The derivation will proceed from the governing differential and algebraic equations provided for the small-signal perturbations.\n\nThe system is described by two relationships in the time domain. First, the relationship between the output firing-rate perturbation $\\delta r(t)$, the input current perturbation $\\delta u(t)$, and the adaptation state perturbation $\\delta w(t)$ is given by the linearized static characteristic:\n$$\n\\delta r(t) = g_0 \\big(\\delta u(t) - \\delta w(t)\\big)\n\\quad \\quad (1)\n$$\nwhere $g_0$ is the gain of the neuron's instantaneous nonlinearity at its operating point.\n\nSecond, the dynamics of the adaptation state $\\delta w(t)$ are governed by a leaky integrator model, which is a first-order linear ordinary differential equation:\n$$\n\\tau_w \\frac{d}{dt} \\delta w(t) = -\\delta w(t) + \\delta u(t)\n\\quad \\quad (2)\n$$\nwhere $\\tau_w  0$ is the adaptation time constant.\n\nTo derive the transfer function $H(s)$, we apply the unilateral Laplace transform to equations $(1)$ and $(2)$. Let $R(s) = \\mathcal{L}\\{\\delta r(t)\\}$, $U(s) = \\mathcal{L}\\{\\delta u(t)\\}$, and $W(s) = \\mathcal{L}\\{\\delta w(t)\\}$. For the derivation of a transfer function, it is standard practice to assume the system is initially at rest, meaning all initial conditions are zero. Specifically, $\\delta w(0) = 0$. Using the differentiation property of the Laplace transform, $\\mathcal{L}\\{\\frac{df(t)}{dt}\\} = sF(s) - f(0)$, the assumption of zero initial conditions simplifies this to $\\mathcal{L}\\{\\frac{df(t)}{dt}\\} = sF(s)$.\n\nApplying the Laplace transform to equation $(1)$ yields:\n$$\nR(s) = g_0 \\big(U(s) - W(s)\\big)\n\\quad \\quad (3)\n$$\n\nApplying the Laplace transform to equation $(2)$ yields:\n$$\n\\tau_w \\big(s W(s)\\big) = -W(s) + U(s)\n$$\nWe now solve this equation for $W(s)$ in terms of $U(s)$. Rearranging the terms to isolate $W(s)$:\n$$\n\\tau_w s W(s) + W(s) = U(s)\n$$\n$$\nW(s) (\\tau_w s + 1) = U(s)\n$$\nThis gives the transfer function from the input perturbation $U(s)$ to the adaptation state $W(s)$:\n$$\nW(s) = \\frac{1}{\\tau_w s + 1} U(s)\n\\quad \\quad (4)\n$$\nThis is the characteristic transfer function of a first-order low-pass filter with time constant $\\tau_w$.\n\nNext, we substitute the expression for $W(s)$ from equation $(4)$ into equation $(3)$ to eliminate the internal state variable $W(s)$ and find the direct relationship between the input $U(s)$ and the output $R(s)$:\n$$\nR(s) = g_0 \\left( U(s) - \\frac{1}{\\tau_w s + 1} U(s) \\right)\n$$\nFactoring out $U(s)$ from the terms on the right-hand side:\n$$\nR(s) = g_0 \\left( 1 - \\frac{1}{\\tau_w s + 1} \\right) U(s)\n$$\nTo simplify the expression in the parentheses, we find a common denominator:\n$$\nR(s) = g_0 \\left( \\frac{(\\tau_w s + 1) - 1}{\\tau_w s + 1} \\right) U(s)\n$$\n$$\nR(s) = g_0 \\left( \\frac{\\tau_w s}{\\tau_w s + 1} \\right) U(s)\n$$\nThe transfer function $H(s)$ is defined as the ratio $\\frac{R(s)}{U(s)}$. Dividing both sides by $U(s)$, we obtain the final expression for the transfer function:\n$$\nH(s) = \\frac{g_0 \\tau_w s}{\\tau_w s + 1}\n$$\nThis transfer function represents a first-order high-pass filter.\n\nThe problem statement further requires showing from first principles that this model exhibits two key properties.\n\nFirst, we must show that adaptation produces zero gain at direct current (DC). The DC gain of a system is given by its transfer function evaluated at $s=0$. Substituting $s=0$ into the expression for $H(s)$:\n$$\nH(0) = \\frac{g_0 \\tau_w (0)}{\\tau_w (0) + 1} = \\frac{0}{0 + 1} = 0\n$$\nThis confirms that the DC gain is precisely zero. This is the mathematical signature of adaptation: for a constant (DC) input perturbation, the neuron's firing rate perturbation eventually returns to zero as the adaptation mechanism fully compensates for the input.\n\nSecond, we must show that the system has a real pole at $s = -\\frac{1}{\\tau_w}$. Poles of a transfer function are the roots of the denominator polynomial, i.e., the values of $s$ for which the denominator equals zero. The denominator of $H(s)$ is $\\tau_w s + 1$. Setting this to zero to find the pole's location:\n$$\n\\tau_w s + 1 = 0\n$$\n$$\n\\tau_w s = -1\n$$\n$$\ns = -\\frac{1}{\\tau_w}\n$$\nSince $\\tau_w$ is given to be a positive real number ($\\tau_w  0$), the pole is located on the negative real axis at $s = -1/\\tau_w$. This confirms the pole location and, because the pole lies in the left-half of the complex plane, it indicates that the adaptation process is stable. The derivation from the governing equations is complete.",
            "answer": "$$\n\\boxed{\\frac{g_0 \\tau_w s}{\\tau_w s + 1}}\n$$"
        }
    ]
}