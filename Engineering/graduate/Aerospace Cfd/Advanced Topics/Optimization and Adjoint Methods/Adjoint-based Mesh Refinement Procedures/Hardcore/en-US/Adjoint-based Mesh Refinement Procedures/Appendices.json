{
    "hands_on_practices": [
        {
            "introduction": "Before applying adjoint methods to complex problems, it is crucial to master the fundamentals of implementation and verification. This first exercise provides a hands-on guide to building and validating a discrete adjoint solver for a simple one-dimensional advection-diffusion equation. By comparing the adjoint-derived sensitivities against finite-difference approximations, you will perform the essential \"adjoint check,\" a cornerstone of robust code development in computational science and engineering .",
            "id": "3941772",
            "problem": "Consider a one-dimensional, steady, linear advection–diffusion model that is commonly used in Computational Fluid Dynamics (CFD) as a simplified setting for studying adjoint-based mesh refinement procedures. The continuous model on a unit interval domain is given by the differential operator acting on the state variable $u(x)$,\n$$\n-\\frac{d}{dx}\\left(\\kappa \\frac{du}{dx}\\right) + a \\frac{du}{dx} = s(x), \\quad x \\in (0,1),\n$$\nwith Dirichlet boundary conditions $u(0)=u_L$ and $u(1)=u_R$, where $\\kappa>0$ is the diffusion coefficient, $a$ is a constant advection speed, and $s(x)$ is a source term. In the discrete setting, consider a uniform mesh of $N$ interior nodes with spacing $\\Delta x = \\frac{1}{N+1}$, and let the vector of unknowns be $\\mathbf{u} \\in \\mathbb{R}^N$ representing the state at interior nodes $x_i = i \\Delta x$, for $i=1,2,\\dots,N$.\n\nUse a second-order central difference for the diffusion term and a first-order upwind difference for the advection term (assume $a>0$ for all test cases). The discrete residual for interior node $i$ is\n$$\nR_i(\\mathbf{u}) = -\\kappa \\frac{u_{i+1} - 2u_i + u_{i-1}}{\\Delta x^2} + a \\frac{u_i - u_{i-1}}{\\Delta x} - s_i,\n$$\nwith the understanding that $u_0 = u_L$ and $u_{N+1} = u_R$ are known boundary values. This yields a linear system\n$$\n\\mathbf{A}\\,\\mathbf{u} = \\mathbf{s}_{\\text{eff}},\n$$\nwhere $\\mathbf{A} \\in \\mathbb{R}^{N \\times N}$ is the Jacobian matrix of the residual with respect to $\\mathbf{u}$, and $\\mathbf{s}_{\\text{eff}} \\in \\mathbb{R}^N$ is the effective source incorporating the physical source vector $\\mathbf{s}$ and boundary contributions.\n\nDefine a goal functional\n$$\nJ(\\mathbf{u}) = \\frac{1}{2} \\sum_{i=1}^{N} \\alpha_i \\, u_i^2 \\, \\Delta x,\n$$\nwhere $\\alpha_i = \\alpha(x_i)$ is a specified nonnegative weight. The discrete adjoint state $\\boldsymbol{\\lambda} \\in \\mathbb{R}^N$ is defined by the linear system\n$$\n\\mathbf{A}^\\top \\boldsymbol{\\lambda} = \\frac{\\partial J}{\\partial \\mathbf{u}},\n$$\nwhere the gradient $\\frac{\\partial J}{\\partial \\mathbf{u}}$ has components $\\left(\\frac{\\partial J}{\\partial u_i}\\right) = \\alpha_i u_i \\Delta x$. For the parameterized source $\\mathbf{s}$ entering the residual as $-\\mathbf{s}$, the sensitivity of $J$ with respect to $\\mathbf{s}$ is\n$$\n\\frac{dJ}{d\\mathbf{s}} = \\boldsymbol{\\lambda},\n$$\nby the discrete adjoint identity and the chain rule, since $\\frac{\\partial \\mathbf{R}}{\\partial \\mathbf{s}} = -\\mathbf{I}$.\n\nYour tasks are as follows:\n- Construct $\\mathbf{A}$ for the advection–diffusion discretization described above with positive advection speed $a>0$, and assemble $\\mathbf{s}_{\\text{eff}}$ including Dirichlet boundary conditions $u_L$ and $u_R$.\n- Solve the primal system $\\mathbf{A}\\,\\mathbf{u} = \\mathbf{s}_{\\text{eff}}$ for $\\mathbf{u}$.\n- Compute the gradient $\\frac{\\partial J}{\\partial \\mathbf{u}}$ analytically from the definition of $J$.\n- Validate the implementation of $\\frac{\\partial J}{\\partial \\mathbf{u}}$ via a finite difference approximation: for a small perturbation $\\varepsilon$ applied componentwise to $\\mathbf{u}$, approximate $\\left(\\frac{\\partial J}{\\partial u_i}\\right)$ by\n$$\n\\left(\\frac{\\partial J}{\\partial u_i}\\right)_{\\text{FD}} \\approx \\frac{J(\\mathbf{u} + \\varepsilon \\mathbf{e}_i) - J(\\mathbf{u})}{\\varepsilon},\n$$\nwhere $\\mathbf{e}_i$ is the $i$-th canonical basis vector in $\\mathbb{R}^N$.\n- Solve the discrete adjoint system $\\mathbf{A}^\\top \\boldsymbol{\\lambda} = \\frac{\\partial J}{\\partial \\mathbf{u}}$ for $\\boldsymbol{\\lambda}$.\n- Validate the adjoint-derived sensitivity of $J$ with respect to the source $\\mathbf{s}$ via finite differences: for a small perturbation $\\delta$ applied componentwise to $\\mathbf{s}_{\\text{eff}}$, approximate $\\left(\\frac{dJ}{ds_j}\\right)$ by\n$$\n\\left(\\frac{dJ}{ds_j}\\right)_{\\text{FD}} \\approx \\frac{J(\\mathbf{u}^{(j,+)}) - J(\\mathbf{u})}{\\delta},\n$$\nwhere $\\mathbf{u}^{(j,+)}$ is the solution of $\\mathbf{A}\\,\\mathbf{u}^{(j,+)} = \\mathbf{s}_{\\text{eff}} + \\delta\\,\\mathbf{e}_j$.\n\nReport, for each test case, a single scalar result defined as the maximum of two absolute error metrics:\n$$\nE_{\\text{grad}} = \\max_{i} \\left| \\left(\\frac{\\partial J}{\\partial u_i}\\right)_{\\text{FD}} - \\left(\\frac{\\partial J}{\\partial u_i}\\right) \\right|, \\quad\nE_{\\text{sens}} = \\max_{j} \\left| \\left(\\frac{dJ}{ds_j}\\right)_{\\text{FD}} - \\lambda_j \\right|,\n$$\nand the final reported value is $\\max\\{E_{\\text{grad}}, E_{\\text{sens}}\\}$.\n\nAll quantities are dimensionless. Angles do not appear. Your program must be a complete, runnable program in the specified environment and must produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The numerical perturbations must be chosen to be sufficiently small to approximate derivatives accurately but sufficiently large to avoid catastrophic round-off; use the same $\\varepsilon$ and $\\delta$ for all test cases.\n\nUse the following test suite:\n- Test case $1$: $N=20$, $a=1.0$, $\\kappa=0.05$, $u_L=1.0$, $u_R=0.0$, $\\alpha(x) = \\exp\\left(-\\left(\\frac{x-0.5}{0.2}\\right)^2\\right)$, and $s_i=0$ for all $i$.\n- Test case $2$: $N=10$, $a=2.0$, $\\kappa=0.01$, $u_L=0.0$, $u_R=1.0$, $\\alpha(x) = 1$, and $s_i=0$ for all $i$.\n- Test case $3$: $N=50$, $a=0.5$, $\\kappa=0.10$, $u_L=0.2$, $u_R=0.2$, $\\alpha(x) = \\sin^2(\\pi x)$, and $s_i=0$ for all $i$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test cases as they are listed: for example, $[E_1,E_2,E_3]$, where each $E_k$ is the scalar $\\max\\{E_{\\text{grad}}, E_{\\text{sens}}\\}$ computed for test case $k$.",
            "solution": "The problem is valid. It is scientifically grounded in the well-established fields of numerical analysis for partial differential equations and adjoint-based sensitivity analysis. The problem is well-posed, self-contained, and provides all necessary information to construct a unique numerical solution. We proceed with a detailed derivation and description of the computational procedure.\n\n### 1. Primal Problem: Discretization and System Assembly\n\nThe governing equation is a one-dimensional, steady, linear advection-diffusion equation on the domain $x \\in (0,1)$:\n$$\n-\\frac{d}{dx}\\left(\\kappa \\frac{du}{dx}\\right) + a \\frac{du}{dx} = s(x)\n$$\nwith diffusion coefficient $\\kappa > 0$, advection speed $a > 0$, and source term $s(x)$. The domain is subject to Dirichlet boundary conditions $u(0) = u_L$ and $u(1) = u_R$.\n\nWe discretize this equation on a uniform mesh with $N$ interior nodes $x_i = i \\Delta x$ for $i=1, \\dots, N$, where the mesh spacing is $\\Delta x = \\frac{1}{N+1}$. Let $u_i \\approx u(x_i)$ be the discrete solution at node $i$.\n\nThe diffusion term is discretized using a second-order central difference scheme:\n$$\n-\\frac{d}{dx}\\left(\\kappa \\frac{du}{dx}\\right) \\bigg|_{x_i} \\approx -\\kappa \\frac{u_{i+1} - 2u_i + u_{i-1}}{\\Delta x^2}\n$$\n\nThe advection term is discretized using a first-order upwind scheme. Since $a > 0$, the information flows from left to right, so we use a backward difference:\n$$\na \\frac{du}{dx} \\bigg|_{x_i} \\approx a \\frac{u_i - u_{i-1}}{\\Delta x}\n$$\n\nCombining these discretizations, the discrete residual equation for an interior node $i$ is $R_i(\\mathbf{u}) = 0$, where:\n$$\nR_i(\\mathbf{u}) = -\\kappa \\frac{u_{i+1} - 2u_i + u_{i-1}}{\\Delta x^2} + a \\frac{u_i - u_{i-1}}{\\Delta x} - s_i\n$$\nHere, $s_i = s(x_i)$. Rearranging terms to group the unknowns $u_j$, we obtain:\n$$\n\\left(-\\frac{\\kappa}{\\Delta x^2} - \\frac{a}{\\Delta x}\\right)u_{i-1} + \\left(\\frac{2\\kappa}{\\Delta x^2} + \\frac{a}{\\Delta x}\\right)u_i + \\left(-\\frac{\\kappa}{\\Delta x^2}\\right)u_{i+1} = s_i\n$$\nThis equation holds for $i = 1, \\dots, N$. The boundary values $u_0 = u_L$ and $u_{N+1} = u_R$ are known. We move the terms involving these known values to the right-hand side. This leads to a linear system of equations $\\mathbf{A} \\mathbf{u} = \\mathbf{s}_{\\text{eff}}$.\n\nThe matrix $\\mathbf{A} \\in \\mathbb{R}^{N \\times N}$ is a tridiagonal matrix with the following non-zero entries:\n- Main diagonal ($j=i$):\n$$A_{ii} = \\frac{2\\kappa}{\\Delta x^2} + \\frac{a}{\\Delta x}$$\n- Lower diagonal ($j=i-1$):\n$$A_{i,i-1} = -\\frac{\\kappa}{\\Delta x^2} - \\frac{a}{\\Delta x}$$\n- Upper diagonal ($j=i+1$):\n$$A_{i,i+1} = -\\frac{\\kappa}{\\Delta x^2}$$\n\nThe effective source vector $\\mathbf{s}_{\\text{eff}} \\in \\mathbb{R}^N$ incorporates the physical source $s_i$ and the boundary contributions:\n- For the first node ($i=1$):\n$$(\\mathbf{s}_{\\text{eff}})_1 = s_1 - \\left(-\\frac{\\kappa}{\\Delta x^2} - \\frac{a}{\\Delta x}\\right)u_0 = s_1 + \\left(\\frac{\\kappa}{\\Delta x^2} + \\frac{a}{\\Delta x}\\right)u_L$$\n- For intermediate nodes ($1 < i < N$):\n$$(\\mathbf{s}_{\\text{eff}})_i = s_i$$\n- For the last node ($i=N$):\n$$(\\mathbf{s}_{\\text{eff}})_N = s_N - \\left(-\\frac{\\kappa}{\\Delta x^2}\\right)u_{N+1} = s_N + \\frac{\\kappa}{\\Delta x^2}u_R$$\n\nThe primal problem is solved for the state vector $\\mathbf{u}$ by solving this linear system.\n\n### 2. Adjoint Problem and Sensitivity Analysis\n\nThe goal functional is defined as:\n$$\nJ(\\mathbf{u}) = \\frac{1}{2} \\sum_{i=1}^{N} \\alpha_i u_i^2 \\Delta x\n$$\nwhere $\\alpha_i = \\alpha(x_i)$ is a non-negative weight. The gradient of $J$ with respect to the components of the state vector $\\mathbf{u}$ is computed directly:\n$$\n\\frac{\\partial J}{\\partial u_i} = \\frac{1}{2} \\alpha_i (2u_i) \\Delta x = \\alpha_i u_i \\Delta x\n$$\n\nThe discrete adjoint state $\\boldsymbol{\\lambda} \\in \\mathbb{R}^N$ is the solution to the linear system:\n$$\n\\mathbf{A}^\\top \\boldsymbol{\\lambda} = \\frac{\\partial J}{\\partial \\mathbf{u}}\n$$\nwhere $\\mathbf{A}^\\top$ is the transpose of the primal system matrix and the right-hand side is the vector of gradient components derived above.\n\nA key result of the discrete adjoint method is that the sensitivity of the functional $J$ with respect to the physical source term vector $\\mathbf{s}$ is equal to the adjoint state vector $\\boldsymbol{\\lambda}$. This holds because the residual is defined to contain the term $-\\mathbf{s}$, which means $\\frac{\\partial \\mathbf{R}}{\\partial \\mathbf{s}} = -\\mathbf{I}$. The derivation shows that $\\frac{dJ}{d\\mathbf{s}} = \\boldsymbol{\\lambda}$.\n\n### 3. Validation via Finite Differences\n\nTo ensure the correctness of our implementation, we validate both the analytical gradient and the adjoint-derived sensitivity using finite difference approximations.\n\n**Gradient Validation:** The analytical gradient $\\frac{\\partial J}{\\partial \\mathbf{u}}$ is compared against a forward finite difference approximation. For each component $i$, we compute:\n$$\n\\left(\\frac{\\partial J}{\\partial u_i}\\right)_{\\text{FD}} = \\frac{J(\\mathbf{u} + \\varepsilon \\mathbf{e}_i) - J(\\mathbf{u})}{\\varepsilon}\n$$\nwhere $\\mathbf{e}_i$ is the $i$-th canonical basis vector and $\\varepsilon$ is a small perturbation (e.g., $\\varepsilon=10^{-7}$). The error is quantified by the maximum absolute difference over all components:\n$$\nE_{\\text{grad}} = \\max_{i} \\left| \\left(\\frac{\\partial J}{\\partial u_i}\\right)_{\\text{FD}} - \\left(\\frac{\\partial J}{\\partial u_i}\\right) \\right|\n$$\n\n**Sensitivity Validation:** The adjoint-derived sensitivity $\\boldsymbol{\\lambda}$ is compared against a finite difference approximation of the sensitivity of $J$ with respect to the source term $\\mathbf{s}$. For each component $j$, we compute:\n$$\n\\left(\\frac{dJ}{ds_j}\\right)_{\\text{FD}} = \\frac{J(\\mathbf{u}^{(j,+)}) - J(\\mathbf{u})}{\\delta}\n$$\nHere, $\\delta$ is a small perturbation (e.g., $\\delta=10^{-7}$) and $\\mathbf{u}^{(j,+)}$ is the solution of the primal system with a perturbed source: $\\mathbf{A}\\,\\mathbf{u}^{(j,+)} = \\mathbf{s}_{\\text{eff}} + \\delta\\,\\mathbf{e}_j$. The error is then:\n$$\nE_{\\text{sens}} = \\max_{j} \\left| \\left(\\frac{dJ}{ds_j}\\right)_{\\text{FD}} - \\lambda_j \\right|\n$$\n\nThe final reported value for each test case is the maximum of these two error metrics, $\\max\\{E_{\\text{grad}}, E_{\\text{sens}}\\}$. Small values for this result indicate a correct implementation of both the primal and adjoint systems.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import solve as solve_linear_system\n\ndef solve():\n    \"\"\"\n    Main function to orchestrate the solution for the given test cases.\n    It sets up the test cases, iterates through them, calls a helper\n    function to process each one, and prints the final results.\n    \"\"\"\n\n    def process_case(params):\n        \"\"\"\n        Processes a single test case for the 1D advection-diffusion problem.\n        It assembles and solves the primal system, computes the goal functional\n        and its gradient, solves the adjoint system, and validates the results\n        using finite differences to compute the final error metric.\n        \"\"\"\n        N, a, kappa, u_L, u_R, alpha_func = params\n        s_vec = np.zeros(N)\n\n        # 0. Setup mesh and perturbation parameters\n        delta_x = 1.0 / (N + 1)\n        x_nodes = np.linspace(delta_x, 1.0 - delta_x, N)\n        \n        epsilon = 1e-7  # Perturbation for gradient validation\n        delta = 1e-7    # Perturbation for sensitivity validation\n\n        # 1. Assemble the primal system matrix A and effective source s_eff\n        diag_main = (2.0 * kappa / delta_x**2 + a / delta_x) * np.ones(N)\n        diag_upper = (-kappa / delta_x**2) * np.ones(N - 1)\n        diag_lower = (-kappa / delta_x**2 - a / delta_x) * np.ones(N - 1)\n        \n        A = np.diag(diag_main) + np.diag(diag_upper, k=1) + np.diag(diag_lower, k=-1)\n        \n        s_eff = np.copy(s_vec)\n        s_eff[0] += (kappa / delta_x**2 + a / delta_x) * u_L\n        s_eff[-1] += (kappa / delta_x**2) * u_R\n\n        # 2. Solve the primal system for the state vector u\n        u = solve_linear_system(A, s_eff)\n\n        # 3. Compute goal functional and its analytical gradient\n        alpha_vec = alpha_func(x_nodes)\n\n        def J_func(u_vec):\n            return 0.5 * np.sum(alpha_vec * u_vec**2 * delta_x)\n\n        grad_J_analytic = alpha_vec * u * delta_x\n        J_base = J_func(u)\n\n        # 4. Validate the gradient via finite differences\n        grad_J_fd = np.zeros(N)\n        for i in range(N):\n            u_pert = np.copy(u)\n            u_pert[i] += epsilon\n            J_pert = J_func(u_pert)\n            grad_J_fd[i] = (J_pert - J_base) / epsilon\n\n        E_grad = np.max(np.abs(grad_J_fd - grad_J_analytic))\n\n        # 5. Solve the discrete adjoint system for the adjoint vector\n        A_T = A.T\n        adjoint_vec = solve_linear_system(A_T, grad_J_analytic)\n\n        # 6. Validate the adjoint sensitivity via finite differences\n        sens_J_fd = np.zeros(N)\n        for j in range(N):\n            s_eff_pert = np.copy(s_eff)\n            s_eff_pert[j] += delta\n            u_pert_sens = solve_linear_system(A, s_eff_pert)\n            J_pert_sens = J_func(u_pert_sens)\n            sens_J_fd[j] = (J_pert_sens - J_base) / delta\n        \n        E_sens = np.max(np.abs(sens_J_fd - adjoint_vec))\n\n        # 7. Compute the final result for the test case\n        return max(E_grad, E_sens)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (N, a, kappa, u_L, u_R, alpha_func)\n        (20, 1.0, 0.05, 1.0, 0.0, lambda x: np.exp(-((x - 0.5) / 0.2)**2)),\n        (10, 2.0, 0.01, 0.0, 1.0, lambda x: np.ones_like(x)),\n        (50, 0.5, 0.10, 0.2, 0.2, lambda x: np.sin(np.pi * x)**2)\n    ]\n\n    results = []\n    for case in test_cases:\n        result = process_case(case)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.12e}' for r in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "With a validated adjoint solver, the next step is to understand its role in error estimation. This practice demonstrates the fundamental error representation formula, which shows that the adjoint-weighted residual can provide a precise measure of the error in a functional of interest. Using the Method of Manufactured Solutions for a 2D convection-diffusion problem, you will numerically verify that the adjoint-based error indicator is not merely an estimate, but for this linear system, is identical to the actual output error .",
            "id": "3941740",
            "problem": "Consider the steady, linear, two-dimensional convection–diffusion equation on the unit square domain $\\Omega = [0,1]\\times[0,1]$ with Dirichlet boundary conditions given by a manufactured exact solution. The governing equation is\n$$\n-\\nabla\\cdot\\left(\\kappa \\nabla u\\right) + \\mathbf{a}\\cdot\\nabla u = q \\quad \\text{in } \\Omega,\n$$\nwith boundary condition $u = u^{\\star}$ on $\\partial \\Omega$, where $\\kappa > 0$ is the diffusion coefficient, $\\mathbf{a} = (a_x, a_y)$ is the constant convection velocity, and $q$ is a source term chosen to make a prescribed smooth function $u^{\\star}$ an exact solution of the continuous partial differential equation. Let the manufactured exact solution be\n$$\nu^{\\star}(x,y) = \\sin(\\pi x)\\sin(\\pi y).\n$$\nFrom the fundamental definitions of gradient, divergence, and the Laplacian, compute the source term $q(x,y)$ such that $u^{\\star}$ satisfies the continuous equation. Use a uniform Cartesian mesh with $N\\times N$ interior points, grid spacing $h=\\frac{1}{N+1}$, and coordinates $x_i=(i+1)h$, $y_j=(j+1)h$ for $i,j\\in\\{0,\\dots,N-1\\}$.\n\nDiscretize the operator with second-order centered differences for the diffusion term and first-order upwind differences for the convection term. Assemble the resulting linear system\n$$\nA_h \\mathbf{u}_h = \\mathbf{b}_h,\n$$\nwhere $A_h$ is the discrete convection–diffusion operator, $\\mathbf{u}_h$ is the vector of unknowns at interior nodes, and $\\mathbf{b}_h$ is the discrete right-hand side consisting of the manufactured source evaluated at interior nodes plus contributions from the Dirichlet boundary values induced by the discrete stencil. Define the scalar output functional as the domain integral of the field,\n$$\nJ(u) = \\int_{\\Omega} u(x,y)\\,\\mathrm{d}x\\,\\mathrm{d}y.\n$$\nApproximate $J$ by the cell-centered Riemann sum on the interior grid,\n$$\nJ_h(\\mathbf{u}_h) = \\mathbf{g}_h^\\top \\mathbf{u}_h, \\quad \\text{where} \\quad \\mathbf{g}_h = h^2\\,\\mathbf{1},\n$$\nand $\\mathbf{1}$ is the vector of ones of the same size as $\\mathbf{u}_h$.\n\nDefine the discrete adjoint $\\boldsymbol{\\psi}_h$ by solving the transpose system\n$$\nA_h^\\top \\boldsymbol{\\psi}_h = \\mathbf{g}_h.\n$$\nLet the interior-grid restriction of the exact solution be $\\mathbf{u}^{\\star}_h$, obtained by sampling $u^{\\star}$ at the interior grid points. Compute the discrete truncation residual\n$$\n\\boldsymbol{\\tau}_h = A_h \\mathbf{u}^{\\star}_h - \\mathbf{b}_h,\n$$\nand the adjoint-based output error indicator\n$$\n\\eta_h = \\boldsymbol{\\psi}_h^\\top \\boldsymbol{\\tau}_h.\n$$\nCompute the actual discrete output error\n$$\n\\varepsilon_h = J_h(\\mathbf{u}^{\\star}_h) - J_h(\\mathbf{u}_h) = \\mathbf{g}_h^\\top (\\mathbf{u}^{\\star}_h - \\mathbf{u}_h).\n$$\nStarting from the linear algebra representation of the discrete primal and adjoint problems and the definition of the output functional, derive, from first principles, the relationship between $\\eta_h$ and $\\varepsilon_h$ for this linear setting. Then, implement the described discretization and computations to numerically verify that the adjoint-based indicator scales linearly with the actual output error by reporting the ratio\n$$\nr = \\frac{\\eta_h}{\\varepsilon_h}\n$$\nfor each case in the following test suite and demonstrating that it is approximately constant across refinements and parameter variations.\n\nUse the following test suite of parameter values $(N,\\kappa,a_x,a_y)$:\n- Case 1 (general convection–diffusion, moderate grid): $(16,\\,0.05,\\,1.0,\\,0.4)$.\n- Case 2 (mixed-sign convection, moderate grid): $(16,\\,0.05,\\,-0.7,\\,1.3)$.\n- Case 3 (stronger convection, finer grid): $(32,\\,0.01,\\,2.0,\\,-1.0)$.\n- Case 4 (purely vertical convection, coarse grid): $(8,\\,0.1,\\,0.0,\\,1.0)$.\n\nAll computations are non-dimensional, so no physical units are required. Your program should produce a single line of output containing the ratios $r$ for the four cases as a comma-separated list enclosed in square brackets (e.g., \"[r1,r2,r3,r4]\"). The outputs must be reported as floating-point numbers.",
            "solution": "The problem statement is assessed to be valid. It is a well-posed and self-contained problem from the field of numerical analysis for partial differential equations. It is scientifically grounded in the theory of finite difference methods and adjoint-based error estimation, using the standard method of manufactured solutions for verification. All parameters, equations, and numerical methods are clearly and unambiguously defined.\n\nThe solution proceeds in three stages:\n1.  Derivation of the source term $q(x,y)$ required for the manufactured solution.\n2.  Analytical derivation of the relationship between the actual output error $\\varepsilon_h$ and the adjoint-based error indicator $\\eta_h$.\n3.  Specification of the numerical discretization scheme to be implemented.\n\n**1. Source Term Derivation**\n\nThe governing continuous equation is $-\\nabla\\cdot\\left(\\kappa \\nabla u\\right) + \\mathbf{a}\\cdot\\nabla u = q$. We are given the constant diffusion coefficient $\\kappa > 0$, the constant convection velocity vector $\\mathbf{a} = (a_x, a_y)$, and a manufactured solution $u^{\\star}(x,y) = \\sin(\\pi x)\\sin(\\pi y)$ on the domain $\\Omega = [0,1]\\times[0,1]$. We must find the source term $q(x,y)$ that makes $u^{\\star}$ an exact solution.\n\nFirst, we compute the necessary derivatives of $u^{\\star}$:\nThe gradient of $u^{\\star}$ is:\n$$\n\\nabla u^{\\star}(x,y) = \\begin{pmatrix} \\frac{\\partial u^{\\star}}{\\partial x} \\\\ \\frac{\\partial u^{\\star}}{\\partial y} \\end{pmatrix} = \\begin{pmatrix} \\pi \\cos(\\pi x)\\sin(\\pi y) \\\\ \\pi \\sin(\\pi x)\\cos(\\pi y) \\end{pmatrix}\n$$\nThe convection term is the dot product of $\\mathbf{a}$ and $\\nabla u^{\\star}$:\n$$\n\\mathbf{a} \\cdot \\nabla u^{\\star} = a_x \\pi \\cos(\\pi x)\\sin(\\pi y) + a_y \\pi \\sin(\\pi x)\\cos(\\pi y)\n$$\nThe diffusion term involves the Laplacian, $\\nabla^2 u^{\\star} = \\nabla \\cdot \\nabla u^{\\star}$. For constant $\\kappa$, the term is $-\\nabla\\cdot\\left(\\kappa \\nabla u^{\\star}\\right) = -\\kappa \\nabla^2 u^{\\star}$. The second partial derivatives are:\n$$\n\\frac{\\partial^2 u^{\\star}}{\\partial x^2} = -\\pi^2 \\sin(\\pi x)\\sin(\\pi y)\n$$\n$$\n\\frac{\\partial^2 u^{\\star}}{\\partial y^2} = -\\pi^2 \\sin(\\pi x)\\sin(\\pi y)\n$$\nThe Laplacian is the sum of these:\n$$\n\\nabla^2 u^{\\star} = \\frac{\\partial^2 u^{\\star}}{\\partial x^2} + \\frac{\\partial^2 u^{\\star}}{\\partial y^2} = -2\\pi^2 \\sin(\\pi x)\\sin(\\pi y) = -2\\pi^2 u^{\\star}(x,y)\n$$\nThus, the diffusion term is:\n$$\n-\\kappa \\nabla^2 u^{\\star} = 2\\kappa\\pi^2 \\sin(\\pi x)\\sin(\\pi y)\n$$\nSubstituting these into the governing equation, we solve for $q$:\n$$\nq(x,y) = -\\kappa \\nabla^2 u^{\\star} + \\mathbf{a}\\cdot\\nabla u^{\\star} = 2\\kappa\\pi^2 \\sin(\\pi x)\\sin(\\pi y) + a_x \\pi \\cos(\\pi x)\\sin(\\pi y) + a_y \\pi \\sin(\\pi x)\\cos(\\pi y)\n$$\nThis is the required source term.\n\n**2. Analytical Relationship between $\\varepsilon_h$ and $\\eta_h$**\n\nThe core of this problem is to derive the relationship between the actual output error $\\varepsilon_h$ and the adjoint-based error indicator $\\eta_h$ from first principles.\n\nThe discrete primal problem is given by the linear system:\n$$\nA_h \\mathbf{u}_h = \\mathbf{b}_h\n$$\nwhere $\\mathbf{u}_h$ is the numerical solution vector.\n\nThe actual discrete output error, $\\varepsilon_h$, is the difference between the functional evaluated with the exact solution projected onto the grid, $\\mathbf{u}^{\\star}_h$, and the numerical solution $\\mathbf{u}_h$:\n$$\n\\varepsilon_h = J_h(\\mathbf{u}^{\\star}_h) - J_h(\\mathbf{u}_h) = \\mathbf{g}_h^\\top \\mathbf{u}^{\\star}_h - \\mathbf{g}_h^\\top \\mathbf{u}_h = \\mathbf{g}_h^\\top (\\mathbf{u}^{\\star}_h - \\mathbf{u}_h)\n$$\nThe discrete adjoint problem is defined by the transposed system:\n$$\nA_h^\\top \\boldsymbol{\\psi}_h = \\mathbf{g}_h\n$$\nBy taking the transpose of this equation, we can express the functional's gradient $\\mathbf{g}_h^\\top$ in terms of the adjoint solution $\\boldsymbol{\\psi}_h$ and the system matrix $A_h$:\n$$\n\\mathbf{g}_h^\\top = (A_h^\\top \\boldsymbol{\\psi}_h)^\\top = \\boldsymbol{\\psi}_h^\\top A_h\n$$\nNow, substitute this expression for $\\mathbf{g}_h^\\top$ into the equation for the error $\\varepsilon_h$:\n$$\n\\varepsilon_h = (\\boldsymbol{\\psi}_h^\\top A_h) (\\mathbf{u}^{\\star}_h - \\mathbf{u}_h)\n$$\nUsing the associativity of matrix-vector multiplication, we regroup the terms:\n$$\n\\varepsilon_h = \\boldsymbol{\\psi}_h^\\top [A_h (\\mathbf{u}^{\\star}_h - \\mathbf{u}_h)] = \\boldsymbol{\\psi}_h^\\top (A_h \\mathbf{u}^{\\star}_h - A_h \\mathbf{u}_h)\n$$\nWe know that $A_h \\mathbf{u}_h = \\mathbf{b}_h$, so we can substitute this into the expression:\n$$\n\\varepsilon_h = \\boldsymbol{\\psi}_h^\\top (A_h \\mathbf{u}^{\\star}_h - \\mathbf{b}_h)\n$$\nThe term in the parenthesis, $A_h \\mathbf{u}^{\\star}_h - \\mathbf{b}_h$, is precisely the definition of the discrete truncation residual, $\\boldsymbol{\\tau}_h$. This residual measures how well the exact solution satisfies the discrete equations.\n$$\n\\boldsymbol{\\tau}_h = A_h \\mathbf{u}^{\\star}_h - \\mathbf{b}_h\n$$\nTherefore, we find:\n$$\n\\varepsilon_h = \\boldsymbol{\\psi}_h^\\top \\boldsymbol{\\tau}_h\n$$\nThe adjoint-based output error indicator is defined as $\\eta_h = \\boldsymbol{\\psi}_h^\\top \\boldsymbol{\\tau}_h$. Consequently, we have proven that for this linear problem with the specified definitions:\n$$\n\\varepsilon_h = \\eta_h\n$$\nThis is a fundamental result in adjoint-based error analysis, often called the \"error representation formula\". It shows that the adjoint-weighted residual provides an exact measure of the error in the linear functional of interest. The ratio $r = \\frac{\\eta_h}{\\varepsilon_h}$ must therefore be equal to $1$, up to floating-point precision errors.\n\n**3. Discretization Scheme**\n\nThe numerical implementation requires constructing the matrix $A_h$ and the right-hand side vector $\\mathbf{b}_h$. The $N \\times N$ interior grid points are mapped to a single vector of size $N^2$. A common mapping is the row-major or column-major lexicographical ordering. Let a grid point be indexed by $(i,j)$ for $i,j \\in \\{0, \\dots, N-1\\}$. A mapping to a single index $k$ can be $k = i + jN$.\n\nThe discrete operator at an interior node $(i,j)$ is formed by summing the contributions from the diffusion and convection terms.\nThe second-order centered difference stencil for the diffusion term $-\\kappa \\nabla^2 u$ at $(x_i, y_j)$ is:\n$$\n-\\kappa \\left( \\frac{u_{i+1,j} - 2u_{i,j} + u_{i-1,j}}{h^2} + \\frac{u_{i,j+1} - 2u_{i,j} + u_{i,j-1}}{h^2} \\right)\n$$\nThe first-order upwind difference for the convection term $\\mathbf{a}\\cdot\\nabla u$ at $(x_i, y_j)$ depends on the sign of the velocity components $a_x$ and $a_y$. Let $a_x^+ = \\max(a_x, 0)$, $a_x^- = \\min(a_x, 0)$, $a_y^+ = \\max(a_y, 0)$, and $a_y^- = \\min(a_y, 0)$. The discretization is:\n$$\na_x \\frac{\\partial u}{\\partial x} \\approx a_x^+ \\frac{u_{i,j} - u_{i-1,j}}{h} + a_x^- \\frac{u_{i+1,j} - u_{i,j}}{h}\n$$\n$$\na_y \\frac{\\partial u}{\\partial y} \\approx a_y^+ \\frac{u_{i,j} - u_{i,j-1}}{h} + a_y^- \\frac{u_{i,j+1} - u_{i,j}}{h}\n$$\n\nCombining these stencils, we can determine the entries of the matrix $A_h$. For a row corresponding to node $k=i+jN$, the non-zero entries are:\n- Diagonal entry (coefficient of $u_{i,j}$): $A_{k,k} = \\frac{4\\kappa}{h^2} + \\frac{a_x^+ - a_x^-}{h} + \\frac{a_y^+ - a_y^-}{h} = \\frac{4\\kappa}{h^2} + \\frac{|a_x|}{h} + \\frac{|a_y|}{h}$\n- Off-diagonal for $u_{i-1,j}$ (if $i>0$): $A_{k,k-1} = -\\frac{\\kappa}{h^2} - \\frac{a_x^+}{h}$\n- Off-diagonal for $u_{i+1,j}$ (if $i<N-1$): $A_{k,k+1} = -\\frac{\\kappa}{h^2} + \\frac{a_x^-}{h}$\n- Off-diagonal for $u_{i,j-1}$ (if $j>0$): $A_{k,k-N} = -\\frac{\\kappa}{h^2} - \\frac{a_y^+}{h}$\n- Off-diagonal for $u_{i,j+1}$ (if $j<N-1$): $A_{k,k+N} = -\\frac{\\kappa}{h^2} + \\frac{a_y^-}{h}$\n\nThe right-hand side vector $\\mathbf{b}_h$ consists of the source term $q(x_i, y_j)$ plus contributions from the boundary conditions. When a stencil at an interior node (e.g., $i=0$) references a boundary node (e.g., $u_{-1,j}$), that term is known ($u^{\\star}(0, y_j)$) and moved to the right-hand side. However, for the given manufactured solution $u^{\\star}(x,y) = \\sin(\\pi x)\\sin(\\pi y)$, the value is zero on the entire boundary $\\partial\\Omega$. Therefore, all boundary contributions to $\\mathbf{b}_h$ are zero, and the vector is simply the source term evaluated at the interior grid points: $b_k = q(x_i, y_j)$ for $k=i+jN$.\n\nThe numerical portion of the problem involves implementing this discretization, solving the primal and adjoint linear systems, computing $\\varepsilon_h$ and $\\eta_h$, and verifying that their ratio $r$ is indeed $1.0$ for all test cases.",
            "answer": "```python\nimport numpy as np\nfrom scipy.sparse import lil_matrix, csc_matrix\nfrom scipy.sparse.linalg import spsolve\n\ndef compute_ratio(N, kappa, ax, ay):\n    \"\"\"\n    Computes the ratio of the adjoint-based error indicator to the actual output error\n    for the 2D convection-diffusion problem.\n\n    Args:\n        N (int): The number of interior grid points in each dimension.\n        kappa (float): The diffusion coefficient.\n        ax (float): The convection velocity in the x-direction.\n        ay (float): The convection velocity in the y-direction.\n\n    Returns:\n        float: The ratio eta_h / eps_h.\n    \"\"\"\n    # 1. Grid and problem setup\n    h = 1.0 / (N + 1)\n    n_nodes = N * N\n\n    # Create coordinate arrays for interior points\n    i = np.arange(N)\n    x = (i + 1) * h\n    y = (i + 1) * h\n    X, Y = np.meshgrid(x, y)\n\n    # Define manufactured solution and source term functions\n    def u_star(x_val, y_val):\n        return np.sin(np.pi * x_val) * np.sin(np.pi * y_val)\n\n    def q_source(x_val, y_val, k, a_x, a_y):\n        term1 = 2 * k * np.pi**2 * np.sin(np.pi * x_val) * np.sin(np.pi * y_val)\n        term2 = a_x * np.pi * np.cos(np.pi * x_val) * np.sin(np.pi * y_val)\n        term3 = a_y * np.pi * np.sin(np.pi * x_val) * np.cos(np.pi * y_val)\n        return term1 + term2 + term3\n\n    # 2. Assemble the discrete system A_h * u_h = b_h\n    # Using LIL format for efficient assembly\n    A_h = lil_matrix((n_nodes, n_nodes), dtype=np.float64)\n    b_h = np.zeros(n_nodes, dtype=np.float64)\n    \n    # Pre-calculate upwind coefficients\n    ax_pos = max(ax, 0)\n    ax_neg = min(ax, 0)\n    ay_pos = max(ay, 0)\n    ay_neg = min(ay, 0)\n\n    # Loop over all interior nodes (i, j)\n    for j in range(N):\n        for i in range(N):\n            # Map 2D (i, j) index to 1D k index\n            k = i + j * N\n            \n            # Source term at node (i, j)\n            b_h[k] = q_source(x[i], y[j], kappa, ax, ay)\n\n            # Assemble matrix A_h row by row\n            # Diagonal coefficient\n            diag_coeff = (4 * kappa / h**2) + (abs(ax) / h) + (abs(ay) / h)\n            A_h[k, k] = diag_coeff\n\n            # Off-diagonal coefficients for neighbors\n            # Left neighbor (i-1, j)\n            if i > 0:\n                A_h[k, k - 1] = -kappa / h**2 - ax_pos / h\n            # Right neighbor (i+1, j)\n            if i  N - 1:\n                A_h[k, k + 1] = -kappa / h**2 + ax_neg / h\n            # Bottom neighbor (i, j-1)\n            if j > 0:\n                A_h[k, k - N] = -kappa / h**2 - ay_pos / h\n            # Top neighbor (i, j+1)\n            if j  N - 1:\n                A_h[k, k + N] = -kappa / h**2 + ay_neg / h\n\n    # Boundary condition contributions to b_h are zero because u_star = 0 on the boundary.\n    # Convert matrix to CSC format for efficient solves\n    A_h_csc = A_h.tocsc()\n\n    # 3. Solve primal and adjoint systems\n    u_h = spsolve(A_h_csc, b_h)\n    \n    g_h = h**2 * np.ones(n_nodes)\n    psi_h = spsolve(A_h_csc.T, g_h)\n\n    # 4. Compute errors and indicators\n    # Exact solution on the grid (flattened)\n    u_star_h = u_star(X, Y).flatten()\n\n    # Truncation residual: tau_h = A_h * u_star_h - b_h\n    tau_h = A_h_csc @ u_star_h - b_h\n    \n    # Adjoint-based error indicator: eta_h = psi_h^T * tau_h\n    eta_h = psi_h.dot(tau_h)\n\n    # Actual discrete output error: eps_h = g_h^T * (u_star_h - u_h)\n    eps_h = g_h.dot(u_star_h - u_h)\n    \n    # 5. Compute the ratio\n    # The ratio should be 1.0 up to machine precision.\n    # Avoid division by zero if error is extremely small.\n    if abs(eps_h)  1e-15:\n        return 1.0 if abs(eta_h)  1e-15 else np.nan\n    \n    r = eta_h / eps_h\n    return r\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (N, kappa, a_x, a_y)\n        (16, 0.05, 1.0, 0.4),\n        (16, 0.05, -0.7, 1.3),\n        (32, 0.01, 2.0, -1.0),\n        (8, 0.1, 0.0, 1.0),\n    ]\n\n    results = []\n    for case in test_cases:\n        N, kappa, ax, ay = case\n        result = compute_ratio(N, kappa, ax, ay)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Ultimately, adjoint-based indicators are tools for making intelligent decisions about mesh refinement. This final exercise moves from theory to strategy, challenging you to implement a greedy algorithm that uses adjoint information to optimally allocate a fixed computational budget between different refinement types—local mesh splitting ($h$-refinement) and polynomial enrichment ($p$-refinement). By maximizing the error reduction per unit cost, you will explore the practical trade-offs involved in designing an efficient, goal-oriented adaptive procedure .",
            "id": "3941845",
            "problem": "Consider a two-dimensional inviscid, compressible flow over a transonic airfoil modeled by the Euler equations, and a scalar functional corresponding to drag coefficient, denoted by $J(\\boldsymbol{u}) = C_D$, where $\\boldsymbol{u}$ denotes the state vector (density, momentum components, energy). In an adjoint-based error estimation framework, the error in $J$ due to discretization can be approximated by an adjoint-weighted residual representation based on the Fréchet derivative of $J$ evaluated at the discrete solution. This yields an estimate of the form\n$$\n\\Delta J \\approx \\sum_{i=1}^{N_{\\text{el}}} w_i R_i,\n$$\nwhere $N_{\\text{el}}$ is the number of elements, $w_i$ is a nonnegative scalar adjoint weight for element $i$ indicating the sensitivity of $J$ to local perturbations, and $R_i$ is a nonnegative scalar residual magnitude for element $i$ obtained from the primal discretization.\n\nAdjoint-based mesh refinement procedures attempt to reduce $\\Delta J$ by allocating refinement according to the adjoint weights $w_i$ and local solution regularity. Two refinement types are considered:\n\n- $h$-refinement: replacing an element by smaller elements (reducing local size), which we model as halving the characteristic element size. Let the initial size be normalized, and a single $h$ action halves the size. For element $i$, define a smoothness parameter $s_i \\in [0,1]$, where $s_i = 1$ indicates locally analytic/smooth flow and $s_i = 0$ indicates a strong non-smooth feature (e.g., shock or contact discontinuity). The modeled residual reduction factor for one $h$ action is\n$$\n\\phi_h(s_i) = 2^{-\\left(r_s s_i + r_n (1 - s_i)\\right)},\n$$\nwith $r_s > 0$ and $r_n > 0$ representing effective convergence rates in smooth and non-smooth regions, respectively. The corresponding predicted drag error reduction from one $h$ action at element $i$ is\n$$\n\\Delta e_{i,h} = w_i R_i \\left(1 - \\phi_h(s_i)\\right).\n$$\n\n- $p$-refinement: increasing the local polynomial degree by one. For element $i$ with current degree $p_i \\in \\mathbb{N}$, the modeled residual reduction factor for one $p$ action is\n$$\n\\phi_p(s_i, p_i) = s_i e^{-\\alpha} + (1 - s_i) \\left(1 - \\frac{\\mu}{p_i + 1}\\right),\n$$\nwith $\\alpha > 0$ parameterizing exponential convergence in smooth regions and $\\mu \\in (0,1)$ capturing the limited effectiveness of $p$-refinement in non-smooth regions. The corresponding predicted drag error reduction from one $p$ action at element $i$ is\n$$\n\\Delta e_{i,p} = w_i R_i \\left(1 - \\phi_p(s_i, p_i)\\right).\n$$\n\nWe model the computational cost of each refinement action in terms of the increase in degrees of freedom. In two dimensions, let the cost of one $h$ action at element $i$ be\n$$\nc_{i,h} = \\eta_h (p_i + 1)^2,\n$$\nand the cost of one $p$ action at element $i$ be\n$$\nc_{i,p} = \\eta_p \\left( (p_i + 2)^2 - (p_i + 1)^2 \\right) = \\eta_p \\left( 2 p_i + 3 \\right),\n$$\nwhere $\\eta_h > 0$ and $\\eta_p > 0$ are scaling coefficients. Given a total budget $B$ (measured in degrees-of-freedom units), we seek the optimal mix of $h$ and $p$ actions, under adjoint guidance, to minimize the predicted drag error\n$$\nE_{\\text{final}} = \\sum_{i=1}^{N_{\\text{el}}} w_i R_i \\psi_i,\n$$\nwhere $\\psi_i = \\phi_h(s_i)$ if element $i$ receives an $h$ action, $\\psi_i = \\phi_p(s_i, p_i)$ if element $i$ receives a $p$ action, and $\\psi_i = 1$ if element $i$ receives no action. For this problem, restrict to selecting at most one action per element.\n\nYour task is to implement a program that, for each test case, uses a greedy strategy to select actions that maximize the marginal adjoint-weighted residual reduction per unit cost, subject to the budget constraint and the one-action-per-element restriction:\n- For each element $i$, compute the two candidate actions ($h$ and $p$), their predicted reductions $\\Delta e_{i,h}$, $\\Delta e_{i,p}$, and costs $c_{i,h}$, $c_{i,p}$.\n- Form a list of all candidate actions across all elements, each with its benefit-cost ratio $\\rho = \\Delta e / c$.\n- Sort actions by descending $\\rho$ and select actions greedily, skipping any action for an element already acted upon, until the total cost would exceed the budget $B$ if the next action were taken.\n- Report the final predicted drag error $E_{\\text{final}}$ (dimensionless) and the fraction of selected actions that are $p$-refinements, defined as $f_p = N_p / (N_p + N_h)$, where $N_p$ and $N_h$ are the counts of selected $p$ and $h$ actions, respectively, with $f_p = 0$ if no actions are selected.\n\nExpress $E_{\\text{final}}$ as a dimensionless float and $f_p$ as a float in $[0,1]$. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case is reported as a two-element list $[E_{\\text{final}}, f_p]$, and each float must be rounded to six decimal places.\n\nUse the following parameters for all test cases:\n- $r_s = 2.0$, $r_n = 1.0$, $\\alpha = 1.0$, $\\mu = 0.5$, $\\eta_h = 1.0$, $\\eta_p = 1.0$.\n\nTest Suite:\n- Case A (mixed transonic features): $N_{\\text{el}} = 8$, budget $B = 60$, element data\n    - $s = [\\,0.1,\\,0.2,\\,0.9,\\,0.8,\\,0.5,\\,0.1,\\,0.95,\\,0.6\\,]$\n    - $w = [\\,1.5,\\,1.0,\\,0.7,\\,0.6,\\,0.8,\\,1.2,\\,0.4,\\,0.5\\,]$\n    - $R = [\\,0.08,\\,0.06,\\,0.03,\\,0.02,\\,0.025,\\,0.07,\\,0.015,\\,0.02\\,]$\n    - $p = [\\,3,\\,3,\\,4,\\,3,\\,2,\\,3,\\,4,\\,2\\,]$\n- Case B (zero budget): $N_{\\text{el}} = 5$, budget $B = 0$, element data\n    - $s = [\\,0.4,\\,0.6,\\,0.5,\\,0.7,\\,0.3\\,]$\n    - $w = [\\,0.9,\\,0.7,\\,1.1,\\,0.8,\\,1.0\\,]$\n    - $R = [\\,0.04,\\,0.03,\\,0.05,\\,0.02,\\,0.06\\,]$\n    - $p = [\\,3,\\,2,\\,3,\\,4,\\,2\\,]$\n- Case C (predominantly smooth flow): $N_{\\text{el}} = 6$, budget $B = 50$, element data\n    - $s = [\\,0.95,\\,0.9,\\,0.85,\\,0.92,\\,0.88,\\,0.93\\,]$\n    - $w = [\\,0.6,\\,0.7,\\,0.5,\\,0.8,\\,0.65,\\,0.55\\,]$\n    - $R = [\\,0.025,\\,0.03,\\,0.02,\\,0.028,\\,0.026,\\,0.022\\,]$\n    - $p = [\\,3,\\,3,\\,2,\\,4,\\,3,\\,2\\,]$\n- Case D (predominantly non-smooth, shock-dominated): $N_{\\text{el}} = 6$, budget $B = 50$, element data\n    - $s = [\\,0.05,\\,0.1,\\,0.08,\\,0.12,\\,0.02,\\,0.15\\,]$\n    - $w = [\\,1.2,\\,1.0,\\,0.9,\\,1.1,\\,1.3,\\,0.95\\,]$\n    - $R = [\\,0.06,\\,0.055,\\,0.05,\\,0.065,\\,0.07,\\,0.045\\,]$\n    - $p = [\\,3,\\,2,\\,3,\\,4,\\,3,\\,2\\,]$\n\nFinal Output Format:\nYour program should produce a single line of output containing the four results for Cases A–D as a comma-separated list enclosed in square brackets, where each result is a two-element list $[E_{\\text{final}}, f_p]$ with both floats rounded to six decimal places. For example, the format should be\n$$\n[\\,[E_A, f_{p,A}],\\,[E_B, f_{p,B}],\\,[E_C, f_{p,C}],\\,[E_D, f_{p,D}]\\,].\n$$\nAll reported $E_{\\text{final}}$ values are pure dimensionless quantities and all $f_p$ values are dimensionless fractions in $[0,1]$.",
            "solution": "The user has provided a well-defined computational problem concerning adjoint-based mesh refinement in computational fluid dynamics. The task is to implement a specified greedy algorithm to select mesh refinement actions that optimally reduce a predicted error functional within a given computational budget.\n\nThe problem is structured as a constrained optimization task. We aim to maximize the total predicted error reduction, $\\Delta E_{total} = \\sum_{i \\in \\text{selected}} \\Delta e_i$, subject to a total cost constraint, $\\sum_{i \\in \\text{selected}} c_i \\le B$, and a structural constraint that at most one refinement action can be applied to any given element. This is a variant of the knapsack problem, specifically the multiple-choice knapsack problem, where for each element (\"item group\"), we can choose one of three options: no action, $h$-refinement, or $p$-refinement.\n\nThe problem prescribes a greedy heuristic to find an approximate solution. This approach is computationally efficient and often effective for knapsack-type problems. The core of the strategy is to prioritize actions that offer the most \"bang for the buck,\" quantified by the benefit-to-cost ratio.\n\nThe step-by-step implementation of this greedy algorithm is as follows:\n\n1.  **Candidate Generation**: For each of the $N_{\\text{el}}$ elements in the mesh, two potential refinement actions are considered: one $h$-refinement and one $p$-refinement. For each of these $2 \\times N_{\\text{el}}$ candidate actions, we calculate three key metrics based on the provided formulae:\n    *   **Benefit**: The predicted error reduction, $\\Delta e_i$. This is calculated for both $h$- and $p$-refinement types as $\\Delta e = w_i R_i (1 - \\phi)$, where $\\phi$ is the respective residual reduction factor ($\\phi_h$ or $\\phi_p$).\n    *   **Cost**: The computational cost, $c_i$, measured in degrees of freedom. The costs $c_{i,h}$ and $c_{i,p}$ are determined by the element's current polynomial degree $p_i$.\n    *   **Efficiency**: The benefit-to-cost ratio, $\\rho = \\Delta e / c$. This metric normalizes the error reduction by the resources required to achieve it, making it the central criterion for the greedy selection.\n\n2.  **Greedy Selection**: The generated list of $2 \\times N_{\\text{el}}$ candidate actions is sorted in descending order of their efficiency ratio, $\\rho$. The algorithm then iterates through this sorted list, considering the most efficient actions first. For each action in the list, two conditions are checked:\n    *   **Budget Constraint**: The action's cost must not cause the cumulative cost of all selected actions to exceed the total budget $B$.\n    *   **Uniqueness Constraint**: The element corresponding to the action must not have been previously selected for any other refinement action.\n\n    If both conditions are met, the action is selected. Its cost is added to the running total, its benefit is accumulated, and the element is marked as \"refined\" to prevent further actions on it. This process continues until the entire sorted list of candidates has been evaluated.\n\n3.  **Final Output Calculation**: After the selection process is complete, the total initial error is calculated as $E_{\\text{initial}} = \\sum_{i=1}^{N_{\\text{el}}} w_i R_i$. The total error reduction achieved by the selected actions, $\\Delta E_{\\text{total}}$, is subtracted from this initial error to yield the final predicted error, $E_{\\text{final}} = E_{\\text{initial}} - \\Delta E_{\\text{total}}$. Additionally, the fraction of selected actions that are of $p$-type, $f_p = N_p / (N_p + N_h)$, is computed, where $N_p$ and $N_h$ are the total counts of selected $p$- and $h$-refinements, respectively. If no actions are selected (e.g., due to a zero budget), $f_p$ is defined to be $0$.\n\nThis procedure is applied independently to each test case provided, and the final results, $E_{\\text{final}}$ and $f_p$, are reported with the specified formatting. The use of NumPy allows for efficient, vectorized calculations of the element-wise properties.\n\nThe parameters provided in the model, such as convergence rates ($r_s, r_n$) and cost factors ($\\eta_h, \\eta_p$), are crucial as they instantiate the trade-off between the effectiveness and cost of $h$- and $p$-refinement strategies in different flow regimes (smooth vs. non-smooth), a central theme in advanced numerical methods for PDEs.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements a greedy algorithm for adjoint-based mesh refinement.\n    \"\"\"\n    # Global parameters for all test cases\n    r_s = 2.0\n    r_n = 1.0\n    alpha = 1.0\n    mu = 0.5\n    eta_h = 1.0\n    eta_p = 1.0\n\n    # Test suite data\n    test_cases = [\n        {\n            \"name\": \"Case A\",\n            \"B\": 60.0,\n            \"s\": np.array([0.1, 0.2, 0.9, 0.8, 0.5, 0.1, 0.95, 0.6]),\n            \"w\": np.array([1.5, 1.0, 0.7, 0.6, 0.8, 1.2, 0.4, 0.5]),\n            \"R\": np.array([0.08, 0.06, 0.03, 0.02, 0.025, 0.07, 0.015, 0.02]),\n            \"p\": np.array([3, 3, 4, 3, 2, 3, 4, 2]),\n        },\n        {\n            \"name\": \"Case B\",\n            \"B\": 0.0,\n            \"s\": np.array([0.4, 0.6, 0.5, 0.7, 0.3]),\n            \"w\": np.array([0.9, 0.7, 1.1, 0.8, 1.0]),\n            \"R\": np.array([0.04, 0.03, 0.05, 0.02, 0.06]),\n            \"p\": np.array([3, 2, 3, 4, 2]),\n        },\n        {\n            \"name\": \"Case C\",\n            \"B\": 50.0,\n            \"s\": np.array([0.95, 0.9, 0.85, 0.92, 0.88, 0.93]),\n            \"w\": np.array([0.6, 0.7, 0.5, 0.8, 0.65, 0.55]),\n            \"R\": np.array([0.025, 0.03, 0.02, 0.028, 0.026, 0.022]),\n            \"p\": np.array([3, 3, 2, 4, 3, 2]),\n        },\n        {\n            \"name\": \"Case D\",\n            \"B\": 50.0,\n            \"s\": np.array([0.05, 0.1, 0.08, 0.12, 0.02, 0.15]),\n            \"w\": np.array([1.2, 1.0, 0.9, 1.1, 1.3, 0.95]),\n            \"R\": np.array([0.06, 0.055, 0.05, 0.065, 0.07, 0.045]),\n            \"p\": np.array([3, 2, 3, 4, 3, 2]),\n        },\n    ]\n\n    all_results = []\n    for case in test_cases:\n        s, w, R, p, B = case[\"s\"], case[\"w\"], case[\"R\"], case[\"p\"], case[\"B\"]\n        N_el = len(s)\n        \n        # Calculate initial total error\n        wR = w * R\n        E_initial = np.sum(wR)\n\n        # Generate all candidate actions\n        candidate_actions = []\n        \n        # h-refinement actions\n        phi_h = 2**(-(r_s * s + r_n * (1 - s)))\n        delta_e_h = wR * (1 - phi_h)\n        c_h = eta_h * (p + 1)**2\n        # Avoid division by zero, although c_h > 0 for p >= 0\n        rho_h = np.divide(delta_e_h, c_h, out=np.zeros_like(delta_e_h), where=c_h!=0)\n\n        # p-refinement actions\n        phi_p = s * np.exp(-alpha) + (1 - s) * (1 - mu / (p + 1))\n        delta_e_p = wR * (1 - phi_p)\n        c_p = eta_p * (2 * p + 3)\n        # Avoid division by zero, although c_p > 0 for p >= 0\n        rho_p = np.divide(delta_e_p, c_p, out=np.zeros_like(delta_e_p), where=c_p!=0)\n\n        for i in range(N_el):\n            candidate_actions.append({\n                \"rho\": rho_h[i], \"benefit\": delta_e_h[i], \"cost\": c_h[i], \n                \"index\": i, \"type\": 'h'\n            })\n            candidate_actions.append({\n                \"rho\": rho_p[i], \"benefit\": delta_e_p[i], \"cost\": c_p[i], \n                \"index\": i, \"type\": 'p'\n            })\n            \n        # Sort actions by benefit-cost ratio in descending order\n        candidate_actions.sort(key=lambda x: x[\"rho\"], reverse=True)\n\n        # Greedy selection process\n        total_cost_used = 0.0\n        total_benefit = 0.0\n        N_h, N_p = 0, 0\n        element_refined = np.zeros(N_el, dtype=bool)\n\n        for action in candidate_actions:\n            idx = action[\"index\"]\n            cost = action[\"cost\"]\n            \n            if not element_refined[idx] and total_cost_used + cost = B:\n                total_cost_used += cost\n                total_benefit += action[\"benefit\"]\n                element_refined[idx] = True\n                if action[\"type\"] == 'h':\n                    N_h += 1\n                else: # 'p'\n                    N_p += 1\n        \n        # Calculate final results\n        E_final = E_initial - total_benefit\n        \n        if N_p + N_h > 0:\n            f_p = N_p / (N_p + N_h)\n        else:\n            f_p = 0.0\n            \n        all_results.append((E_final, f_p))\n\n    # Format the final output string\n    formatted_results = []\n    for e_final, fp in all_results:\n        formatted_results.append(f\"[{e_final:.6f},{fp:.6f}]\")\n    \n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}