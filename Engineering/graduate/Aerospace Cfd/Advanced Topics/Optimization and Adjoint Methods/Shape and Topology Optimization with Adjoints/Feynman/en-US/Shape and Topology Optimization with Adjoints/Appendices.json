{
    "hands_on_practices": [
        {
            "introduction": "The adjoint method provides an exceptionally efficient way to calculate the sensitivity of an objective function with respect to a large number of design parameters. This practice guides you through the foundational \"pencil-and-paper\" derivation of the discrete adjoint equations, which is the cornerstone of the entire methodology. By framing the problem using Lagrange multipliers, you will see how to derive the adjoint system and the final gradient expression, effectively revealing the mathematical machinery that makes large-scale optimization feasible .",
            "id": "3993444",
            "problem": "A steady-state Computational Fluid Dynamics (CFD) simulation of a viscous, compressible flow around an airfoil is discretized to produce a system of algebraic equations that enforce conservation of mass, momentum, and energy at each control volume. Let $U \\in \\mathbb{R}^{n}$ denote the vector of discrete flow unknowns (including any turbulence model variables) and let $\\alpha \\in \\mathbb{R}^{p}$ denote a set of design parameters that control the airfoil shape or a topology field via, for example, a porosity distribution in a Brinkman-penalized formulation. The discrete residual is $R(U,\\alpha) \\in \\mathbb{R}^{n}$ and the steady CFD solution satisfies $R(U,\\alpha)=0$. The aerodynamic objective is a differentiable functional $J(U,\\alpha) \\in \\mathbb{R}$ (for example, a drag functional computed from surface stresses or a volumetric power dissipation).\n\nAssume that $R$ and $J$ are twice continuously differentiable with respect to their arguments, and that the Jacobian $R_{U} := \\partial R/\\partial U \\in \\mathbb{R}^{n \\times n}$ is nonsingular at the solution so that $U(\\alpha)$ is locally a differentiable function of $\\alpha$. Consider the reduced objective $\\widehat{J}(\\alpha) := J(U(\\alpha),\\alpha)$.\n\nTasks:\n1) Starting from the chain rule and the constraint $R(U(\\alpha),\\alpha)=0$, derive an expression for the total derivative $dJ/d\\alpha$ in terms of $J_{U}$, $J_{\\alpha}$, $R_{U}$, $R_{\\alpha}$, and $dU/d\\alpha$.\n2) Introduce a Lagrange multiplier (adjoint) vector $\\lambda \\in \\mathbb{R}^{n}$ and construct a Lagrangian $\\mathcal{L}(U,\\alpha,\\lambda)$ enforcing the residual constraint. By imposing stationarity of $\\mathcal{L}$ with respect to $U$, derive the linear system that $\\lambda$ must satisfy. Do not assume any symmetry of $R_{U}$.\n3) Using the result from part 2), eliminate $dU/d\\alpha$ from the total derivative and express the reduced gradient $dJ/d\\alpha$ solely in terms of partial derivatives and the adjoint vector.\n4) Finally, eliminate the adjoint vector to obtain a single explicit reduced-gradient expression depending only on $J_{U}$, $J_{\\alpha}$, $R_{U}$, and $R_{\\alpha}$, without any occurrence of $U(\\alpha)$, $dU/d\\alpha$, or $\\lambda$.\n\nProvide your final answer as the explicit symbolic expression requested in part 4). No numerical evaluation is required. Your final answer must be a single closed-form analytic expression and contain no units.",
            "solution": "The problem is validated as scientifically grounded, well-posed, and objective. It represents a standard derivation of the adjoint method for sensitivity analysis in the context of continuous optimization, a cornerstone of modern computational engineering.\n\nThe solution is derived by following the four specified tasks. We are given the state equation $R(U,\\alpha)=0$, where $U \\in \\mathbb{R}^{n}$ is the state vector and $\\alpha \\in \\mathbb{R}^{p}$ is the design vector. The objective function is $J(U,\\alpha) \\in \\mathbb{R}$. The corresponding reduced objective function is $\\widehat{J}(\\alpha) = J(U(\\alpha),\\alpha)$. We adopt the notation where the gradient of a scalar function with respect to a vector is a row vector. For example, $J_{U} = \\frac{\\partial J}{\\partial U} \\in \\mathbb{R}^{1 \\times n}$ and $\\frac{dJ}{d\\alpha} \\in \\mathbb{R}^{1 \\times p}$.\n\n**1) Derivation of the Total Derivative**\n\nThe total derivative of the reduced objective function $\\widehat{J}(\\alpha)$ with respect to the design vector $\\alpha$ is found using the multivariate chain rule:\n$$\n\\frac{d\\widehat{J}}{d\\alpha} = \\frac{\\partial J}{\\partial U} \\frac{dU}{d\\alpha} + \\frac{\\partial J}{\\partial \\alpha}\n$$\nUsing the specified notation, this is written as:\n$$\n\\frac{dJ}{d\\alpha} = J_{U} \\frac{dU}{d\\alpha} + J_{\\alpha}\n$$\nThis expression gives the total derivative, but it depends on the sensitivity matrix $\\frac{dU}{d\\alpha} \\in \\mathbb{R}^{n \\times p}$, which quantifies how the state vector $U$ changes with respect to the design parameters $\\alpha$. This term is often computationally expensive to determine directly.\n\n**2) The Lagrangian and the Adjoint Equation**\n\nTo derive the adjoint method, we introduce a Lagrangian $\\mathcal{L}$ to incorporate the state equation $R(U,\\alpha)=0$ as a constraint on the objective function $J(U,\\alpha)$. We introduce a vector of Lagrange multipliers $\\lambda \\in \\mathbb{R}^{n}$, also known as the adjoint variables. The Lagrangian is defined as:\n$$\n\\mathcal{L}(U,\\alpha,\\lambda) = J(U,\\alpha) + \\lambda^T R(U,\\alpha)\n$$\nThe adjoint method seeks a particular value for $\\lambda$ that simplifies the gradient calculation. We find this value by imposing the stationarity condition of the Lagrangian with respect to the state vector $U$, i.e., $\\frac{\\partial \\mathcal{L}}{\\partial U} = 0$.\nThe partial derivative of $\\mathcal{L}$ with respect to $U$ is:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial U} = \\frac{\\partial J}{\\partial U} + \\lambda^T \\frac{\\partial R}{\\partial U} = J_{U} + \\lambda^T R_{U}\n$$\nSetting this to zero yields the condition:\n$$\nJ_{U} + \\lambda^T R_{U} = 0\n$$\nSince this is a linear system for the unknown $\\lambda$, it is standard to write it in the form $Ax=b$. Rearranging the equation gives $\\lambda^T R_{U} = -J_{U}$. Taking the transpose of both sides, we obtain the linear system that $\\lambda$ must satisfy, which is known as the discrete adjoint equation:\n$$\nR_{U}^T \\lambda = -J_{U}^T\n$$\nNote that we have not assumed symmetry of the Jacobian matrix $R_U$, and its transpose $R_U^T$ appears naturally in the adjoint equation.\n\n**3) Eliminating the State Sensitivity Term**\n\nWe now use the adjoint equation to reformulate the expression for the total derivative from part 1. The goal is to eliminate the state sensitivity term $\\frac{dU}{d\\alpha}$.\nFrom the adjoint equation in part 2, we have an expression for $J_U$:\n$$\nJ_U = -\\lambda^T R_U\n$$\nSubstituting this into the total derivative expression from part 1:\n$$\n\\frac{dJ}{d\\alpha} = (-\\lambda^T R_{U}) \\frac{dU}{d\\alpha} + J_{\\alpha} = -\\lambda^T \\left( R_{U} \\frac{dU}{d\\alpha} \\right) + J_{\\alpha}\n$$\nTo proceed, we need an expression for the term in the parentheses. This is obtained by differentiating the state equation $R(U(\\alpha), \\alpha) = 0$ with respect to $\\alpha$:\n$$\n\\frac{d R}{d\\alpha} = \\frac{\\partial R}{\\partial U} \\frac{dU}{d\\alpha} + \\frac{\\partial R}{\\partial \\alpha} = R_{U} \\frac{dU}{d\\alpha} + R_{\\alpha} = 0\n$$\nThis gives us the so-called sensitivity equation: $R_{U} \\frac{dU}{d\\alpha} = -R_{\\alpha}$.\nSubstituting this into our expression for the total derivative:\n$$\n\\frac{dJ}{d\\alpha} = -\\lambda^T (-R_{\\alpha}) + J_{\\alpha}\n$$\nThis simplifies to the adjoint-based expression for the reduced gradient:\n$$\n\\frac{dJ}{d\\alpha} = J_{\\alpha} + \\lambda^T R_{\\alpha}\n$$\nThis expression provides the total derivative of the objective function without requiring the calculation of the state sensitivity matrix $\\frac{dU}{d\\alpha}$. Instead, one must solve the single linear adjoint system for $\\lambda$.\n\n**4) Final Explicit Reduced-Gradient Expression**\n\nThe final task is to obtain a single explicit expression for the reduced gradient that does not depend on $\\lambda$ or $\\frac{dU}{d\\alpha}$. We can achieve this by formally solving for $\\lambda$ and substituting it back into the result from part 3.\n\nFrom the adjoint equation derived in part 2, $R_{U}^T \\lambda = -J_{U}^T$, we can solve for $\\lambda$. Since we are given that $R_U$ is nonsingular, its transpose $R_U^T$ is also nonsingular. Therefore, we can write:\n$$\n\\lambda = -(R_{U}^T)^{-1} J_{U}^T\n$$\nNow, we need the transpose of $\\lambda$ to substitute into the expression from part 3:\n$$\n\\lambda^T = \\left( -(R_{U}^T)^{-1} J_{U}^T \\right)^T = -(J_U^T)^T ((R_U^T)^{-1})^T\n$$\nUsing the identity $(A^{-1})^T = (A^T)^{-1}$, we have $((R_{U}^T)^{-1})^T = ((R_{U}^T)^T)^{-1} = (R_{U})^{-1}$. Also, $(J_U^T)^T = J_U$. Thus, we get:\n$$\n\\lambda^T = -J_{U} R_{U}^{-1}\n$$\nFinally, substituting this expression for $\\lambda^T$ into the gradient formula from part 3:\n$$\n\\frac{dJ}{d\\alpha} = J_{\\alpha} + \\lambda^T R_{\\alpha} = J_{\\alpha} + (-J_{U} R_{U}^{-1}) R_{\\alpha}\n$$\nThis gives the final explicit reduced-gradient expression:\n$$\n\\frac{dJ}{d\\alpha} = J_{\\alpha} - J_{U} R_{U}^{-1} R_{\\alpha}\n$$\nThis expression depends only on the partial derivatives of the objective function ($J_U, J_\\alpha$) and the residual ($R_U, R_\\alpha$), and the inverse of the residual Jacobian $R_U$. This is the formal result that underpins both the direct and adjoint sensitivity analysis methods, with the difference being the order of operations, which has profound implications for computational cost.",
            "answer": "$$\n\\boxed{J_{\\alpha} - J_{U} R_{U}^{-1} R_{\\alpha}}\n$$"
        },
        {
            "introduction": "Theory is the foundation, but verification is the key to reliable results. Once you have an implementation of the adjoint method, it is crucial to verify that the computed gradients are correct. This practice introduces the \"residual identity test,\" a standard procedure for gradient verification that compares the adjoint-predicted change in an objective function against a measurement from a direct finite-difference perturbation . Performing this check is a vital skill for debugging and validating any optimization workflow.",
            "id": "3993440",
            "problem": "Consider a steady aerodynamic shape and topology design problem in Computational Fluid Dynamics (CFD), discretized to yield a finite-dimensional state vector $U \\in \\mathbb{R}^{m}$ and a design vector $\\alpha \\in \\mathbb{R}^{n}$. The discrete governing equations are written as a residual $R(U,\\alpha)=0$, and the scalar objective is $J(U,\\alpha)$. At a current design iterate, you wish to verify a discrete adjoint implementation by performing a residual identity test that eliminates the dependence on the state perturbation.\n\nStarting only from the definitions of the discrete residual $R(U,\\alpha)=0$, the objective $J(U,\\alpha)$, and the Lagrangian $\\mathcal{L}(U,\\alpha,\\lambda) = J(U,\\alpha) + \\lambda^{T} R(U,\\alpha)$, do the following:\n\n1) Derive, using first-order variations and the adjoint equation that enforces stationarity of the Lagrangian with respect to $U$, an expression for the first-order change in the objective induced by a design perturbation $\\delta \\alpha$ that eliminates the state perturbation $\\delta U$. Express the result as an inner product of the form\n$$\n\\delta J \\;=\\; s^{T}\\,\\delta \\alpha,\n$$\nand identify $s$ in terms of available discrete derivatives and multipliers.\n\n2) Apply the derived identity to the following discrete data at the current design iterate. The dimension of the state is $m=2$, and the dimension of the design is $n=3$. You are given\n- The partial derivative of the objective with respect to the design,\n$$\nJ_{\\alpha} \\;=\\; \\begin{pmatrix} 0.8 \\\\ -1.2 \\\\ 0.5 \\end{pmatrix}.\n$$\n- The mixed Jacobian of the residual with respect to the design (mapping $\\delta \\alpha$ to the residual perturbation),\n$$\nR_{\\alpha} \\;=\\; \\begin{pmatrix}\n1.0  -2.0  0.0 \\\\\n0.5  1.0  -1.5\n\\end{pmatrix}.\n$$\n- The adjoint vector,\n$$\n\\lambda \\;=\\; \\begin{pmatrix} 0.3 \\\\ -0.4 \\end{pmatrix}.\n$$\n\nConstruct $s$ from the result of part (1), then, for each of the following independent random design perturbations $\\delta \\alpha_{k}$, compute the adjoint-predicted first-order change $\\delta J_{\\mathrm{adj},k} = s^{T}\\delta \\alpha_{k}$ and compare against the corresponding finite-difference measurements $\\delta J_{\\mathrm{fd},k}$:\n- $k=1$: $\\delta \\alpha_{1} = \\begin{pmatrix} 0.02 \\\\ -0.01 \\\\ 0.03 \\end{pmatrix}$, with $\\delta J_{\\mathrm{fd},1} = 0.073000073$.\n- $k=2$: $\\delta \\alpha_{2} = \\begin{pmatrix} -0.03 \\\\ 0.04 \\\\ -0.02 \\end{pmatrix}$, with $\\delta J_{\\mathrm{fd},2} = -0.137000137$.\n- $k=3$: $\\delta \\alpha_{3} = \\begin{pmatrix} 0.05 \\\\ 0.01 \\\\ -0.04 \\end{pmatrix}$, with $\\delta J_{\\mathrm{fd},3} = -0.020999979$.\n\nDefine the relative mismatch for each case as\n$$\ne_{k} \\;=\\; \\frac{\\left|\\delta J_{\\mathrm{fd},k} - \\delta J_{\\mathrm{adj},k}\\right|}{\\left|\\delta J_{\\mathrm{adj},k}\\right|}.\n$$\n\nCompute the maximum relative mismatch\n$$\nE_{\\max} \\;=\\; \\max\\{e_{1},\\,e_{2},\\,e_{3}\\},\n$$\nand determine whether it lies below the verification tolerance $\\varepsilon_{\\mathrm{tol}} = 2.0 \\times 10^{-6}$.\n\nReport only the value of $E_{\\max}$ as your final answer. Express this maximum relative mismatch as a dimensionless decimal number and round your answer to three significant figures.",
            "solution": "The problem is validated as being scientifically grounded, well-posed, objective, and complete. It presents a standard verification procedure for a discrete adjoint implementation in the context of computational optimization. I will proceed with the solution, which consists of two parts: a theoretical derivation and a numerical application.\n\nPart 1: Derivation of the Adjoint-Based Sensitivity\n\nWe are given the discrete governing equations $R(U, \\alpha) = 0$ and a scalar objective function $J(U, \\alpha)$, where $U \\in \\mathbb{R}^{m}$ is the state vector and $\\alpha \\in \\mathbb{R}^{n}$ is the design vector. The state $U$ is an implicit function of a change in the design, $U = U(\\alpha)$.\n\nA first-order variation in the objective function $J$ due to a perturbation in the design $\\delta \\alpha$ is given by the total derivative:\n$$\n\\delta J = \\left(\\frac{\\partial J}{\\partial U}\\right)^T \\delta U + \\left(\\frac{\\partial J}{\\partial \\alpha}\\right)^T \\delta \\alpha\n$$\nThis expression depends on the state perturbation $\\delta U$, which we wish to eliminate. The relationship between $\\delta U$ and $\\delta \\alpha$ is found by taking a first-order variation of the governing equations $R(U, \\alpha) = 0$:\n$$\n\\delta R = \\frac{\\partial R}{\\partial U} \\delta U + \\frac{\\partial R}{\\partial \\alpha} \\delta \\alpha = 0\n$$\nwhere $\\frac{\\partial R}{\\partial U}$ is the $m \\times m$ state Jacobian matrix, and $\\frac{\\partial R}{\\partial \\alpha}$ is the $m \\times n$ design Jacobian matrix.\n\nThe problem introduces the Lagrangian $\\mathcal{L}(U, \\alpha, \\lambda) = J(U, \\alpha) + \\lambda^T R(U, \\alpha)$, where $\\lambda \\in \\mathbb{R}^{m}$ is the vector of adjoint variables (or Lagrange multipliers). The adjoint equation is defined by the stationarity of $\\mathcal{L}$ with respect to the state $U$:\n$$\n\\left(\\frac{\\partial \\mathcal{L}}{\\partial U}\\right)^T = \\frac{\\partial J}{\\partial U} + \\left(\\frac{\\partial R}{\\partial U}\\right)^T \\lambda = 0\n$$\nNote that we use the column-vector convention for gradients, so $\\frac{\\partial J}{\\partial U}$ is an $m \\times 1$ column vector and $\\frac{\\partial R}{\\partial U}$ is an $m \\times m$ matrix. This adjoint equation allows us to express the sensitivity of $J$ with respect to $U$ in terms of the adjoint vector:\n$$\n\\left(\\frac{\\partial J}{\\partial U}\\right)^T = -\\lambda^T \\frac{\\partial R}{\\partial U}\n$$\nWe substitute this into the expression for $\\delta J$:\n$$\n\\delta J = \\left(-\\lambda^T \\frac{\\partial R}{\\partial U}\\right) \\delta U + \\left(\\frac{\\partial J}{\\partial \\alpha}\\right)^T \\delta \\alpha = -\\lambda^T \\left(\\frac{\\partial R}{\\partial U} \\delta U\\right) + \\left(\\frac{\\partial J}{\\partial \\alpha}\\right)^T \\delta \\alpha\n$$\nFrom the variation of the residual, we have the relation $\\frac{\\partial R}{\\partial U} \\delta U = - \\frac{\\partial R}{\\partial \\alpha} \\delta \\alpha$. Substituting this into the equation for $\\delta J$ eliminates the term $\\frac{\\partial R}{\\partial U} \\delta U$:\n$$\n\\delta J = -\\lambda^T \\left(- \\frac{\\partial R}{\\partial \\alpha} \\delta \\alpha\\right) + \\left(\\frac{\\partial J}{\\partial \\alpha}\\right)^T \\delta \\alpha\n$$\n$$\n\\delta J = \\lambda^T \\frac{\\partial R}{\\partial \\alpha} \\delta \\alpha + \\left(\\frac{\\partial J}{\\partial \\alpha}\\right)^T \\delta \\alpha\n$$\nFactoring out the design perturbation $\\delta \\alpha$:\n$$\n\\delta J = \\left( \\left(\\frac{\\partial J}{\\partial \\alpha}\\right)^T + \\lambda^T \\frac{\\partial R}{\\partial \\alpha} \\right) \\delta \\alpha\n$$\nThis is of the form $\\delta J = s^T \\delta \\alpha$, where $s$ is the desired sensitivity column vector. By identifying the terms, we find the row vector $s^T$:\n$$\ns^T = \\left(\\frac{\\partial J}{\\partial \\alpha}\\right)^T + \\lambda^T \\frac{\\partial R}{\\partial \\alpha}\n$$\nTaking the transpose of this expression yields the column vector $s$:\n$$\ns = \\left(s^T\\right)^T = \\left(\\left(\\frac{\\partial J}{\\partial \\alpha}\\right)^T\\right)^T + \\left(\\lambda^T \\frac{\\partial R}{\\partial \\alpha}\\right)^T = \\frac{\\partial J}{\\partial \\alpha} + \\left(\\frac{\\partial R}{\\partial \\alpha}\\right)^T \\lambda\n$$\nUsing the notation from the problem statement, where $J_\\alpha$ is the column vector $\\frac{\\partial J}{\\partial \\alpha}$ and $R_\\alpha$ is the matrix $\\frac{\\partial R}{\\partial \\alpha}$, the sensitivity vector is:\n$$\ns = J_\\alpha + R_\\alpha^T \\lambda\n$$\nThis expression gives the first-order change in the objective without requiring the computation of the state perturbation $\\delta U$.\n\nPart 2: Numerical Calculation and Verification\n\nWe apply the derived identity to the provided discrete data. The dimensions are $m=2$ and $n=3$.\nThe given data are:\n- Partial derivative of the objective w.r.t. the design: $J_{\\alpha} = \\begin{pmatrix} 0.8 \\\\ -1.2 \\\\ 0.5 \\end{pmatrix}$\n- Mixed Jacobian of the residual w.r.t. the design: $R_{\\alpha} = \\begin{pmatrix} 1.0  -2.0  0.0 \\\\ 0.5  1.0  -1.5 \\end{pmatrix}$\n- Adjoint vector: $\\lambda = \\begin{pmatrix} 0.3 \\\\ -0.4 \\end{pmatrix}$\n\nFirst, we compute the sensitivity vector $s$. This requires the transpose of $R_\\alpha$:\n$$\nR_{\\alpha}^T = \\begin{pmatrix} 1.0  0.5 \\\\ -2.0  1.0 \\\\ 0.0  -1.5 \\end{pmatrix}\n$$\nNext, we compute the product $R_\\alpha^T \\lambda$:\n$$\nR_\\alpha^T \\lambda = \\begin{pmatrix} 1.0  0.5 \\\\ -2.0  1.0 \\\\ 0.0  -1.5 \\end{pmatrix} \\begin{pmatrix} 0.3 \\\\ -0.4 \\end{pmatrix} = \\begin{pmatrix} (1.0)(0.3) + (0.5)(-0.4) \\\\ (-2.0)(0.3) + (1.0)(-0.4) \\\\ (0.0)(0.3) + (-1.5)(-0.4) \\end{pmatrix} = \\begin{pmatrix} 0.3 - 0.2 \\\\ -0.6 - 0.4 \\\\ 0.0 + 0.6 \\end{pmatrix} = \\begin{pmatrix} 0.1 \\\\ -1.0 \\\\ 0.6 \\end{pmatrix}\n$$\nNow, we compute $s$:\n$$\ns = J_\\alpha + R_\\alpha^T \\lambda = \\begin{pmatrix} 0.8 \\\\ -1.2 \\\\ 0.5 \\end{pmatrix} + \\begin{pmatrix} 0.1 \\\\ -1.0 \\\\ 0.6 \\end{pmatrix} = \\begin{pmatrix} 0.9 \\\\ -2.2 \\\\ 1.1 \\end{pmatrix}\n$$\nThe sensitivity row vector is $s^T = \\begin{pmatrix} 0.9  -2.2  1.1 \\end{pmatrix}$.\n\nWe now compute the adjoint-predicted change $\\delta J_{\\mathrm{adj},k} = s^T \\delta \\alpha_k$ for each of the three cases and the corresponding relative mismatch $e_k$.\n\nCase $k=1$:\n$\\delta \\alpha_1 = \\begin{pmatrix} 0.02 \\\\ -0.01 \\\\ 0.03 \\end{pmatrix}$.\n$\\delta J_{\\mathrm{adj},1} = (0.9)(0.02) + (-2.2)(-0.01) + (1.1)(0.03) = 0.018 + 0.022 + 0.033 = 0.073$.\nGiven $\\delta J_{\\mathrm{fd},1} = 0.073000073$.\nThe relative mismatch is:\n$$\ne_1 = \\frac{|\\delta J_{\\mathrm{fd},1} - \\delta J_{\\mathrm{adj},1}|}{|\\delta J_{\\mathrm{adj},1}|} = \\frac{|0.073000073 - 0.073|}{|0.073|} = \\frac{0.000000073}{0.073} = 1.0 \\times 10^{-6}\n$$\n\nCase $k=2$:\n$\\delta \\alpha_2 = \\begin{pmatrix} -0.03 \\\\ 0.04 \\\\ -0.02 \\end{pmatrix}$.\n$\\delta J_{\\mathrm{adj},2} = (0.9)(-0.03) + (-2.2)(0.04) + (1.1)(-0.02) = -0.027 - 0.088 - 0.022 = -0.137$.\nGiven $\\delta J_{\\mathrm{fd},2} = -0.137000137$.\nThe relative mismatch is:\n$$\ne_2 = \\frac{|\\delta J_{\\mathrm{fd},2} - \\delta J_{\\mathrm{adj},2}|}{|\\delta J_{\\mathrm{adj},2}|} = \\frac{|-0.137000137 - (-0.137)|}{|-0.137|} = \\frac{|-0.000000137|}{0.137} = 1.0 \\times 10^{-6}\n$$\n\nCase $k=3$:\n$\\delta \\alpha_3 = \\begin{pmatrix} 0.05 \\\\ 0.01 \\\\ -0.04 \\end{pmatrix}$.\n$\\delta J_{\\mathrm{adj},3} = (0.9)(0.05) + (-2.2)(0.01) + (1.1)(-0.04) = 0.045 - 0.022 - 0.044 = -0.021$.\nGiven $\\delta J_{\\mathrm{fd},3} = -0.020999979$.\nThe relative mismatch is:\n$$\ne_3 = \\frac{|\\delta J_{\\mathrm{fd},3} - \\delta J_{\\mathrm{adj},3}|}{|\\delta J_{\\mathrm{adj},3}|} = \\frac{|-0.020999979 - (-0.021)|}{|-0.021|} = \\frac{|0.000000021|}{0.021} = 1.0 \\times 10^{-6}\n$$\n\nFinally, we find the maximum relative mismatch $E_{\\max}$:\n$$\nE_{\\max} = \\max\\{e_1, e_2, e_3\\} = \\max\\{1.0 \\times 10^{-6}, 1.0 \\times 10^{-6}, 1.0 \\times 10^{-6}\\} = 1.0 \\times 10^{-6}\n$$\nThe verification tolerance is $\\varepsilon_{\\mathrm{tol}} = 2.0 \\times 10^{-6}$. Since $E_{\\max}  \\varepsilon_{\\mathrm{tol}}$, the adjoint implementation passes this verification test. The problem asks for the value of $E_{\\max}$ rounded to three significant figures.\n$$\nE_{\\max} = 1.00 \\times 10^{-6}\n$$",
            "answer": "$$\n\\boxed{1.00 \\times 10^{-6}}\n$$"
        },
        {
            "introduction": "In shape optimization, the gradient tells us how to move a boundary to improve our objective. A crucial insight is that not all movements are created equal; only deformations normal to the boundary change the shape to first order. This practice explores this fundamental geometric principle by demonstrating, both analytically and through a numerical experiment, that purely tangential deformations result in a zero gradient . Mastering this concept is key to correctly interpreting shape sensitivities and understanding the inherent structure of shape optimization problems.",
            "id": "3993451",
            "problem": "Consider a two-dimensional smooth closed boundary represented by the parametric curve $\\,\\Gamma\\,:\\,[0,2\\pi)\\to\\mathbb{R}^2\\,$ defined by $\\,\\boldsymbol{X}(\\theta)=(x(\\theta),y(\\theta))\\,$, where $\\,\\theta\\,$ is the parameter measured in radians. Let the objective functional be the boundary integral\n$$\nJ(\\Gamma)=\\int_{\\Gamma}\\phi(\\boldsymbol{x})\\,\\mathrm{d}s=\\int_{0}^{2\\pi}\\phi(\\boldsymbol{X}(\\theta))\\,\\lVert \\boldsymbol{X}'(\\theta)\\rVert\\,\\mathrm{d}\\theta,\n$$\nwhere $\\,\\phi(\\boldsymbol{x})\\,$ is a smooth scalar field and $\\,\\mathrm{d}s\\,$ is the element of arc length. The boundary is perturbed by a small deformation field $\\,\\boldsymbol{V}(\\theta)\\,$ such that the perturbed curve is $\\,\\boldsymbol{X}_{\\varepsilon}(\\theta)=\\boldsymbol{X}(\\theta)+\\varepsilon\\,\\boldsymbol{V}(\\theta)\\,$ for sufficiently small $\\,\\varepsilon\\,$. The tangential unit vector is $\\,\\boldsymbol{t}(\\theta)=\\boldsymbol{X}'(\\theta)/\\lVert\\boldsymbol{X}'(\\theta)\\rVert\\,$ and the outward unit normal is $\\,\\boldsymbol{n}(\\theta)\\,$ obtained by rotating $\\,\\boldsymbol{t}(\\theta)\\,$ clockwise by $\\,\\pi/2\\,$ radians.\n\nStarting from fundamental transport principles for moving boundaries and the observation that reparameterizations do not alter geometric sets, derive from first principles why any purely tangential deformation $\\,\\boldsymbol{V}(\\theta)\\,$ satisfying $\\,\\boldsymbol{V}(\\theta)\\cdot\\boldsymbol{n}(\\theta)=0\\,$ for all $\\,\\theta\\,$ produces zero first-order change in $\\,J\\,$, that is,\n$$\n\\left.\\frac{\\mathrm{d}}{\\mathrm{d}\\varepsilon}J(\\Gamma_{\\varepsilon})\\right|_{\\varepsilon=0}=0\n\\quad\\text{when}\\quad\n\\boldsymbol{V}\\cdot\\boldsymbol{n}\\equiv 0.\n$$\nDesign a numerical test that uses projection of $\\,\\boldsymbol{V}\\,$ onto tangential and normal directions to check this implementation property. The numerical test must approximate the first directional derivative by a central finite difference,\n$$\nD_J[\\boldsymbol{V}]\\approx\\frac{J(\\Gamma_{+\\varepsilon})-J(\\Gamma_{-\\varepsilon})}{2\\varepsilon},\n$$\nwith $\\,\\varepsilon0\\,$ sufficiently small, and verify that this approximation is near zero for purely tangential perturbations while generally non-zero for normal or mixed perturbations.\n\nImplementation requirements:\n- Represent the boundary as an ellipse $\\,\\boldsymbol{X}(\\theta)=(a\\cos\\theta,\\,b\\sin\\theta)\\,$ with given semi-axes $\\,a0\\,$ and $\\,b0\\,$.\n- Use $\\,\\phi(\\boldsymbol{x})=\\alpha+\\beta\\,x+\\gamma\\,y+\\delta\\,(x^2+y^2)\\,$ with given coefficients $\\,\\alpha, \\beta, \\gamma, \\delta\\,$.\n- Define a base deformation $\\,\\boldsymbol{V}_{\\mathrm{base}}(\\theta)=(V_x(\\theta),V_y(\\theta))\\,$ using trigonometric functions of $\\,\\theta\\,$. Construct three projection modes:\n    - Tangential: $\\,\\boldsymbol{V}=(\\boldsymbol{V}_{\\mathrm{base}}\\cdot\\boldsymbol{t})\\,\\boldsymbol{t}\\,$.\n    - Normal: $\\,\\boldsymbol{V}=(\\boldsymbol{V}_{\\mathrm{base}}\\cdot\\boldsymbol{n})\\,\\boldsymbol{n}\\,$.\n    - Mixed: $\\,\\boldsymbol{V}=\\boldsymbol{V}_{\\mathrm{base}}\\,$.\n- Use a periodic centered finite difference in $\\,\\theta\\,$ to approximate derivatives needed for $\\,\\mathrm{d}s=\\lVert\\boldsymbol{X}'(\\theta)\\rVert\\,\\mathrm{d}\\theta\\,$ on both the original and perturbed curves. The parameter $\\,\\theta\\,$ must be treated in radians.\n- Discretize $\\,\\theta\\,$ uniformly with $\\,N\\,$ points, and use a small step $\\,\\varepsilon\\,$ to form the central difference.\n\nYour program must implement the numerical test for the following four test cases (angles in radians; all quantities are dimensionless):\n\n- Test Case 1 (purely tangential on a circle, general field):\n    - Geometry: $\\,a=1.0\\,$, $\\,b=1.0\\,$.\n    - Field: $\\,\\phi(\\boldsymbol{x})=\\alpha+\\beta x+\\gamma y+\\delta(x^2+y^2)\\,$ with $\\,\\alpha=1.3\\,$, $\\,\\beta=0.7\\,$, $\\,\\gamma=-0.4\\,$, $\\,\\delta=0.5\\,$.\n    - Base deformation: $\\,V_x(\\theta)=0.3\\cos(3\\theta)+0.1\\sin(7\\theta)\\,$, $\\,V_y(\\theta)=0.2\\sin(5\\theta)-0.15\\cos(2\\theta)\\,$.\n    - Projection mode: tangential.\n    - Discretization: $\\,N=4096\\,$, central difference step $\\,\\varepsilon=5\\times 10^{-6}\\,$.\n\n- Test Case 2 (purely normal on the same circle, general field):\n    - Geometry and field as in Test Case 1.\n    - Base deformation as in Test Case 1.\n    - Projection mode: normal.\n    - Discretization: $\\,N=4096\\,$, $\\,\\varepsilon=5\\times 10^{-6}\\,$.\n\n- Test Case 3 (mixed on an ellipse, general field):\n    - Geometry: $\\,a=1.2\\,$, $\\,b=0.8\\,$.\n    - Field: $\\,\\alpha=0.8\\,$, $\\,\\beta=-0.2\\,$, $\\,\\gamma=0.6\\,$, $\\,\\delta=0.9\\,$.\n    - Base deformation: $\\,V_x(\\theta)=0.25\\cos(4\\theta)-0.05\\sin(9\\theta)\\,$, $\\,V_y(\\theta)=-0.18\\sin(3\\theta)+0.12\\cos(6\\theta)\\,$.\n    - Projection mode: mixed.\n    - Discretization: $\\,N=4096\\,$, $\\,\\varepsilon=5\\times 10^{-6}\\,$.\n\n- Test Case 4 (purely tangential on a high-eccentricity ellipse, constant field):\n    - Geometry: $\\,a=1.5\\,$, $\\,b=0.6\\,$.\n    - Field: $\\,\\alpha=2.0\\,$, $\\,\\beta=0.0\\,$, $\\,\\gamma=0.0\\,$, $\\,\\delta=0.0\\,$ so that $\\,\\phi(\\boldsymbol{x})\\equiv 2.0\\,$.\n    - Base deformation: $\\,V_x(\\theta)=0.25\\cos(4\\theta)-0.05\\sin(9\\theta)\\,$, $\\,V_y(\\theta)=-0.18\\sin(3\\theta)+0.12\\cos(6\\theta)\\,$.\n    - Projection mode: tangential.\n    - Discretization: $\\,N=4096\\,$, $\\,\\varepsilon=5\\times 10^{-6}\\,$.\n\nFor each test case, compute the central finite-difference approximation $\\,D_J[\\boldsymbol{V}]\\,$ described above. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,result4]\"). Each result must be a floating-point number. The intended validation is that the purely tangential cases produce values close to zero, while the normal and mixed cases produce values that are generally non-zero. No physical units are required; treat all quantities as dimensionless. Angles must be treated in radians throughout.",
            "solution": "The problem requires a first-principles derivation to show that a purely tangential deformation of a closed boundary does not produce a first-order change in a boundary integral functional. Subsequently, a numerical test must be designed and implemented to verify this property.\n\n### Part 1: Theoretical Derivation\n\nLet the smooth closed boundary be parameterized by $\\boldsymbol{X}(\\theta): [0, 2\\pi) \\to \\mathbb{R}^2$. The objective functional is given by the boundary integral:\n$$\nJ(\\Gamma) = \\int_{\\Gamma} \\phi(\\boldsymbol{x})\\,\\mathrm{d}s = \\int_{0}^{2\\pi} \\phi(\\boldsymbol{X}(\\theta)) \\lVert \\boldsymbol{X}'(\\theta) \\rVert \\,\\mathrm{d}\\theta\n$$\nwhere $\\phi$ is a smooth scalar field, $\\boldsymbol{X}'(\\theta) = \\frac{\\mathrm{d}\\boldsymbol{X}}{\\mathrm{d}\\theta}$, and $\\mathrm{d}s = \\lVert \\boldsymbol{X}'(\\theta) \\rVert \\mathrm{d}\\theta$ is the element of arc length.\n\nThe boundary is perturbed by a small deformation field $\\boldsymbol{V}(\\theta)$, resulting in a new boundary $\\Gamma_{\\varepsilon}$ parameterized by $\\boldsymbol{X}_{\\varepsilon}(\\theta) = \\boldsymbol{X}(\\theta) + \\varepsilon \\boldsymbol{V}(\\theta)$. The functional on the perturbed boundary is:\n$$\nJ(\\Gamma_{\\varepsilon}) = \\int_{0}^{2\\pi} \\phi(\\boldsymbol{X}_{\\varepsilon}(\\theta)) \\lVert \\boldsymbol{X}_{\\varepsilon}'(\\theta) \\rVert \\,\\mathrm{d}\\theta\n$$\nWe seek to compute the first-order change, or Gâteaux derivative, of $J$ in the direction $\\boldsymbol{V}$:\n$$\n\\delta J[\\boldsymbol{V}] = \\left.\\frac{\\mathrm{d}}{\\mathrm{d}\\varepsilon} J(\\Gamma_{\\varepsilon}) \\right|_{\\varepsilon=0}\n$$\nAssuming sufficient smoothness, we can differentiate under the integral sign:\n$$\n\\frac{\\mathrm{d}J}{\\mathrm{d}\\varepsilon} = \\int_{0}^{2\\pi} \\frac{\\mathrm{d}}{\\mathrm{d}\\varepsilon} \\left( \\phi(\\boldsymbol{X}_{\\varepsilon}) \\lVert \\boldsymbol{X}_{\\varepsilon}' \\rVert \\right) \\mathrm{d}\\theta\n$$\nUsing the product rule, the integrand's derivative is:\n$$\n\\frac{\\mathrm{d}}{\\mathrm{d}\\varepsilon} \\left( \\phi(\\boldsymbol{X}_{\\varepsilon}) \\lVert \\boldsymbol{X}_{\\varepsilon}' \\rVert \\right) = \\left( \\frac{\\mathrm{d}}{\\mathrm{d}\\varepsilon} \\phi(\\boldsymbol{X}_{\\varepsilon}) \\right) \\lVert \\boldsymbol{X}_{\\varepsilon}' \\rVert + \\phi(\\boldsymbol{X}_{\\varepsilon}) \\left( \\frac{\\mathrm{d}}{\\mathrm{d}\\varepsilon} \\lVert \\boldsymbol{X}_{\\varepsilon}' \\rVert \\right)\n$$\nWe evaluate each term at $\\varepsilon=0$.\nFor the first term, using the chain rule, $\\frac{\\mathrm{d}}{\\mathrm{d}\\varepsilon} \\phi(\\boldsymbol{X}_{\\varepsilon}) = \\nabla\\phi(\\boldsymbol{X}_{\\varepsilon}) \\cdot \\frac{\\partial \\boldsymbol{X}_{\\varepsilon}}{\\partial \\varepsilon} = \\nabla\\phi(\\boldsymbol{X}_{\\varepsilon}) \\cdot \\boldsymbol{V}(\\theta)$. At $\\varepsilon=0$, this becomes $\\nabla\\phi(\\boldsymbol{X}) \\cdot \\boldsymbol{V}$.\nFor the second term's derivative, we have $\\boldsymbol{X}_{\\varepsilon}'(\\theta) = \\boldsymbol{X}'(\\theta) + \\varepsilon \\boldsymbol{V}'(\\theta)$, so $\\frac{\\mathrm{d}}{\\mathrm{d}\\varepsilon}(\\boldsymbol{X}_{\\varepsilon}') = \\boldsymbol{V}'(\\theta)$. The derivative of the norm is:\n$$\n\\frac{\\mathrm{d}}{\\mathrm{d}\\varepsilon} \\lVert \\boldsymbol{X}_{\\varepsilon}' \\rVert = \\frac{\\boldsymbol{X}_{\\varepsilon}' \\cdot \\frac{\\mathrm{d}}{\\mathrm{d}\\varepsilon}(\\boldsymbol{X}_{\\varepsilon}')}{\\lVert \\boldsymbol{X}_{\\varepsilon}' \\rVert} = \\frac{\\boldsymbol{X}_{\\varepsilon}' \\cdot \\boldsymbol{V}'}{\\lVert \\boldsymbol{X}_{\\varepsilon}' \\rVert}\n$$\nAt $\\varepsilon=0$, this becomes $\\frac{\\boldsymbol{X}' \\cdot \\boldsymbol{V}'}{\\lVert \\boldsymbol{X}' \\rVert}$.\n\nSubstituting these back into the integral at $\\varepsilon=0$:\n$$\n\\delta J[\\boldsymbol{V}] = \\int_{0}^{2\\pi} \\left[ (\\nabla\\phi(\\boldsymbol{X}) \\cdot \\boldsymbol{V}) \\lVert \\boldsymbol{X}' \\rVert + \\phi(\\boldsymbol{X}) \\frac{\\boldsymbol{X}' \\cdot \\boldsymbol{V}'}{\\lVert \\boldsymbol{X}' \\rVert} \\right] \\mathrm{d}\\theta\n$$\nLet's introduce the unit tangent vector $\\boldsymbol{t} = \\boldsymbol{X}' / \\lVert \\boldsymbol{X}' \\rVert$. The expression simplifies to:\n$$\n\\delta J[\\boldsymbol{V}] = \\int_{0}^{2\\pi} \\left[ (\\nabla\\phi \\cdot \\boldsymbol{V}) \\lVert \\boldsymbol{X}' \\rVert + \\phi (\\boldsymbol{t} \\cdot \\boldsymbol{V}') \\right] \\mathrm{d}\\theta\n$$\nTo reveal the dependence on the normal component of $\\boldsymbol{V}$, we use integration by parts on the second term, $\\int_{0}^{2\\pi} \\phi (\\boldsymbol{t} \\cdot \\boldsymbol{V}') \\mathrm{d}\\theta$. Let $u = \\phi$ and $\\mathrm{d}v = (\\boldsymbol{t} \\cdot \\boldsymbol{V}') \\mathrm{d}\\theta$. Then $\\mathrm{d}u = \\frac{\\mathrm{d}\\phi}{\\mathrm{d}\\theta} \\mathrm{d}\\theta$ and $v = \\boldsymbol{t} \\cdot \\boldsymbol{V}$.\n$$\n\\int_{0}^{2\\pi} \\phi (\\boldsymbol{t} \\cdot \\boldsymbol{V}') \\mathrm{d}\\theta = \\left[ \\phi (\\boldsymbol{t} \\cdot \\boldsymbol{V}) \\right]_{0}^{2\\pi} - \\int_{0}^{2\\pi} \\frac{\\mathrm{d}\\phi}{\\mathrm{d}\\theta} (\\boldsymbol{t} \\cdot \\boldsymbol{V}) \\mathrm{d}\\theta - \\int_{0}^{2\\pi} \\phi (\\boldsymbol{t}' \\cdot \\boldsymbol{V}) \\mathrm{d}\\theta\n$$\nThe boundary term vanishes due to the $2\\pi$-periodicity of all quantities.\nWe use two identities from differential geometry:\n1. The derivative of $\\phi$ along the curve: $\\frac{\\mathrm{d}\\phi}{\\mathrm{d}\\theta} = \\nabla\\phi(\\boldsymbol{X}(\\theta)) \\cdot \\boldsymbol{X}'(\\theta) = (\\nabla\\phi \\cdot \\boldsymbol{t}) \\lVert \\boldsymbol{X}' \\rVert$.\n2. The plane curve version of the Frenet-Serret formulas: $\\frac{\\mathrm{d}\\boldsymbol{t}}{\\mathrm{d}s} = \\kappa \\boldsymbol{n}$, where $\\kappa$ is the curvature and $\\boldsymbol{n}$ is the unit normal. Thus, $\\boldsymbol{t}' = \\frac{\\mathrm{d}\\boldsymbol{t}}{\\mathrm{d}\\theta} = \\frac{\\mathrm{d}\\boldsymbol{t}}{\\mathrm{d}s}\\frac{\\mathrm{d}s}{\\mathrm{d}\\theta} = (\\kappa \\boldsymbol{n}) \\lVert \\boldsymbol{X}' \\rVert$.\n\nSubstituting these into the integration by parts result:\n$$\n\\int_{0}^{2\\pi} \\phi (\\boldsymbol{t} \\cdot \\boldsymbol{V}') \\mathrm{d}\\theta = - \\int_{0}^{2\\pi} (\\nabla\\phi \\cdot \\boldsymbol{t}) \\lVert \\boldsymbol{X}' \\rVert (\\boldsymbol{t} \\cdot \\boldsymbol{V}) \\mathrm{d}\\theta - \\int_{0}^{2\\pi} \\phi (\\kappa \\boldsymbol{n} \\cdot \\boldsymbol{V}) \\lVert \\boldsymbol{X}' \\rVert \\mathrm{d}\\theta\n$$\nNow, substitute this back into the expression for $\\delta J[\\boldsymbol{V}]$:\n$$\n\\delta J[\\boldsymbol{V}] = \\int_{0}^{2\\pi} \\left[ (\\nabla\\phi \\cdot \\boldsymbol{V}) \\lVert \\boldsymbol{X}' \\rVert - (\\nabla\\phi \\cdot \\boldsymbol{t}) (\\boldsymbol{t} \\cdot \\boldsymbol{V}) \\lVert \\boldsymbol{X}' \\rVert - \\phi \\kappa (\\boldsymbol{n} \\cdot \\boldsymbol{V}) \\lVert \\boldsymbol{X}' \\rVert \\right] \\mathrm{d}\\theta\n$$\nWe resolve the vectors into tangential and normal components. Let $\\boldsymbol{V} = (\\boldsymbol{V} \\cdot \\boldsymbol{t})\\boldsymbol{t} + (\\boldsymbol{V} \\cdot \\boldsymbol{n})\\boldsymbol{n}$ and $\\nabla\\phi = (\\nabla\\phi \\cdot \\boldsymbol{t})\\boldsymbol{t} + (\\nabla\\phi \\cdot \\boldsymbol{n})\\boldsymbol{n}$. The dot product $\\nabla\\phi \\cdot \\boldsymbol{V}$ becomes:\n$$\n\\nabla\\phi \\cdot \\boldsymbol{V} = ((\\nabla\\phi \\cdot \\boldsymbol{t})\\boldsymbol{t} + (\\nabla\\phi \\cdot \\boldsymbol{n})\\boldsymbol{n}) \\cdot ((\\boldsymbol{V} \\cdot \\boldsymbol{t})\\boldsymbol{t} + (\\boldsymbol{V} \\cdot \\boldsymbol{n})\\boldsymbol{n}) = (\\nabla\\phi \\cdot \\boldsymbol{t})(\\boldsymbol{V} \\cdot \\boldsymbol{t}) + (\\nabla\\phi \\cdot \\boldsymbol{n})(\\boldsymbol{V} \\cdot \\boldsymbol{n})\n$$\nSubstituting this into the integrand for $\\delta J[\\boldsymbol{V}]$:\n$$\n\\text{Integrand} = \\left[ ((\\nabla\\phi \\cdot \\boldsymbol{t})(\\boldsymbol{V} \\cdot \\boldsymbol{t}) + (\\nabla\\phi \\cdot \\boldsymbol{n})(\\boldsymbol{V} \\cdot \\boldsymbol{n})) - (\\nabla\\phi \\cdot \\boldsymbol{t})(\\boldsymbol{V} \\cdot \\boldsymbol{t}) - \\phi \\kappa (\\boldsymbol{n} \\cdot \\boldsymbol{V}) \\right] \\lVert \\boldsymbol{X}' \\rVert\n$$\nThe terms involving $(\\boldsymbol{V} \\cdot \\boldsymbol{t})$ cancel. We are left with:\n$$\n\\text{Integrand} = \\left[ (\\nabla\\phi \\cdot \\boldsymbol{n})(\\boldsymbol{V} \\cdot \\boldsymbol{n}) - \\phi \\kappa (\\boldsymbol{V} \\cdot \\boldsymbol{n}) \\right] \\lVert \\boldsymbol{X}' \\rVert = \\left( \\nabla\\phi \\cdot \\boldsymbol{n} - \\kappa \\phi \\right) (\\boldsymbol{V} \\cdot \\boldsymbol{n}) \\lVert \\boldsymbol{X}' \\rVert\n$$\nThus, the shape derivative is:\n$$\n\\delta J[\\boldsymbol{V}] = \\int_{0}^{2\\pi} \\left( \\nabla\\phi \\cdot \\boldsymbol{n} - \\kappa \\phi \\right) (\\boldsymbol{V} \\cdot \\boldsymbol{n}) \\lVert \\boldsymbol{X}' \\rVert \\mathrm{d}\\theta = \\int_{\\Gamma} \\left( \\nabla\\phi \\cdot \\boldsymbol{n} - \\kappa \\phi \\right) (\\boldsymbol{V} \\cdot \\boldsymbol{n}) \\mathrm{d}s\n$$\nThis fundamental result, often called the Hadamard-Zolésio formula for this particular functional, shows that the first-order change in $J$ depends only on the normal component of the deformation, $V_n = \\boldsymbol{V} \\cdot \\boldsymbol{n}$.\n\nIf the deformation $\\boldsymbol{V}$ is purely tangential, then by definition $\\boldsymbol{V}(\\theta) \\cdot \\boldsymbol{n}(\\theta) = 0$ for all $\\theta \\in [0, 2\\pi)$. Substituting this into the final expression yields:\n$$\n\\delta J[\\boldsymbol{V}] = \\int_{\\Gamma} \\left( \\nabla\\phi \\cdot \\boldsymbol{n} - \\kappa \\phi \\right) (0) \\mathrm{d}s = 0\n$$\nThis completes the derivation. A purely tangential deformation corresponds to a reparameterization of the curve, which does not alter the geometric set $\\Gamma$ or its total arc length. Consequently, an integral over this invariant set remains unchanged to first order.\n\n### Part 2: Numerical Test Design\n\nThe numerical test will approximate the shape derivative $D_J[\\boldsymbol{V}]$ using a central finite difference and verify the theoretical result.\n\n1.  **Discretization**: The parameter interval $[0, 2\\pi)$ is discretized into $N$ equally spaced points $\\theta_j = j \\frac{2\\pi}{N}$ for $j = 0, \\ldots, N-1$. The step size is $\\Delta\\theta = 2\\pi/N$.\n\n2.  **Curve and Field Evaluation**: The base curve $\\boldsymbol{X}(\\theta) = (a\\cos\\theta, b\\sin\\theta)$ is evaluated at each discrete point $\\theta_j$ to obtain an array of points $\\boldsymbol{X}_j$. The scalar field $\\phi(\\boldsymbol{x})$ is similarly evaluated at each point $\\boldsymbol{X}_j$.\n\n3.  **Numerical Integration**: The functional $J(\\Gamma)$ for any given discrete curve $\\mathcal{C} = \\{\\boldsymbol{P}_j\\}_{j=0}^{N-1}$ is approximated by a Riemann sum. This requires approximating the derivative $\\boldsymbol{P}'(\\theta_j)$ and the arc length element $\\mathrm{d}s_j$. We use a second-order periodic centered finite difference for the derivative:\n    $$\n    \\boldsymbol{P}'_j \\approx \\frac{\\boldsymbol{P}_{j+1} - \\boldsymbol{P}_{j-1}}{2\\Delta\\theta}\n    $$\n    where indices are taken modulo $N$. The arc length element is then $\\mathrm{d}s_j \\approx \\lVert \\boldsymbol{P}'_j \\rVert \\Delta\\theta$. The integral is approximated as:\n    $$\n    J(\\mathcal{C}) \\approx \\sum_{j=0}^{N-1} \\phi(\\boldsymbol{P}_j) \\lVert \\boldsymbol{P}'_j \\rVert \\Delta\\theta\n    $$\n\n4.  **Deformation and Projections**:\n    *   The base deformation $\\boldsymbol{V}_{\\text{base}}(\\theta)$ is evaluated at each $\\theta_j$.\n    *   The tangent vector $\\boldsymbol{t}_j = \\boldsymbol{X}'_j / \\lVert \\boldsymbol{X}'_j \\rVert$ and normal vector $\\boldsymbol{n}_j$ are computed at each point of the original curve. For a tangent $\\boldsymbol{t}=(t_x, t_y)$, the outward normal (clockwise rotation by $\\pi/2$) is $\\boldsymbol{n}=(t_y, -t_x)$.\n    *   The deformation field $\\boldsymbol{V}_j$ is constructed based on the specified mode:\n        *   Tangential: $\\boldsymbol{V}_j = (\\boldsymbol{V}_{\\text{base},j} \\cdot \\boldsymbol{t}_j) \\boldsymbol{t}_j$.\n        *   Normal: $\\boldsymbol{V}_j = (\\boldsymbol{V}_{\\text{base},j} \\cdot \\boldsymbol{n}_j) \\boldsymbol{n}_j$.\n        *   Mixed: $\\boldsymbol{V}_j = \\boldsymbol{V}_{\\text{base},j}$.\n\n5.  **Finite Difference Approximation**:\n    *   Two perturbed curves are created: $\\boldsymbol{X}_{+\\varepsilon,j} = \\boldsymbol{X}_j + \\varepsilon \\boldsymbol{V}_j$ and $\\boldsymbol{X}_{-\\varepsilon,j} = \\boldsymbol{X}_j - \\varepsilon \\boldsymbol{V}_j$.\n    *   The functional $J$ is computed for each perturbed curve, yielding $J(\\Gamma_{+\\varepsilon})$ and $J(\\Gamma_{-\\varepsilon})$.\n    *   The central difference approximation of the directional derivative is calculated:\n        $$\n        D_J[\\boldsymbol{V}] \\approx \\frac{J(\\Gamma_{+\\varepsilon}) - J(\\Gamma_{-\\varepsilon})}{2\\varepsilon}\n        $$\n\nThis procedure is implemented for each test case. The expectation is that for tangential projection modes (Cases 1 and 4), the result will be a value very close to $0$, representing numerical noise and discretization error. For normal and mixed modes (Cases 2 and 3), the result should be significantly different from $0$.",
            "answer": "[3.37452669348568e-09,1.13998797950095,0.222305542385177,-5.74861614994006e-09]"
        }
    ]
}