{
    "hands_on_practices": [
        {
            "introduction": "To work with grids that are not simple Cartesian squares, we need a robust mathematical language. This is the role of tensor calculus, where metric tensors allow us to measure distances, angles, and areas within the distorted computational space. This foundational exercise  guides you through deriving the relationship between the covariant and contravariant representations of the grid, which is essential for transforming physical laws into the computational domain.",
            "id": "3966302",
            "problem": "In two-dimensional curvilinear coordinates $\\xi$ and $\\eta$, a hyperbolic grid generation method in Computational Fluid Dynamics (CFD) constructs a body-fitted mapping $\\mathbf{x}(\\xi,\\eta)$ that marches outward from a boundary using a well-posed first-order system, producing covariant basis vectors $\\mathbf{x}_{1}=\\partial\\mathbf{x}/\\partial\\xi$ and $\\mathbf{x}_{2}=\\partial\\mathbf{x}/\\partial\\eta$. The covariant metric tensor is defined by $g_{ij}=\\mathbf{x}_{i}\\cdot\\mathbf{x}_{j}$, where $i,j\\in\\{1,2\\}$ and $\\cdot$ denotes the Euclidean inner product in physical space. The contravariant basis vectors $\\mathbf{x}^{i}$ are defined as the dual basis to $\\{\\mathbf{x}_{j}\\}$ via the Kronecker-delta orthogonality condition $\\mathbf{x}^{i}\\cdot\\mathbf{x}_{j}=\\delta^{i}_{j}$.\n\nStarting from these fundamental definitions, and assuming a nondegenerate grid with a positive Jacobian and a positive-definite metric, derive the relationship between $\\mathbf{x}^{i}$ and $\\mathbf{x}_{j}$ through the metric tensor. Then, for a general two-dimensional non-orthogonal curvilinear grid with $g_{11}=\\mathbf{x}_{1}\\cdot\\mathbf{x}_{1}$, $g_{22}=\\mathbf{x}_{2}\\cdot\\mathbf{x}_{2}$, and $g_{12}=\\mathbf{x}_{1}\\cdot\\mathbf{x}_{2}=g_{21}$, obtain a closed-form expression for the contravariant metric components $g^{ij}$ in terms of $g_{ij}$.\n\nExpress your final result for $g^{ij}$ as a single analytic $2\\times 2$ matrix in terms of $g_{11}$, $g_{22}$, and $g_{12}$. No numerical evaluation is required, and no rounding is necessary. The answer must be given as a matrix without units.",
            "solution": "The problem requires the derivation of two fundamental results in the theory of curvilinear coordinates, which underpins computational fluid dynamics grid generation methods. First, we must derive the relationship between the contravariant basis vectors $\\mathbf{x}^{i}$ and the covariant basis vectors $\\mathbf{x}_{j}$. Second, we must find the explicit matrix expression for the contravariant metric components $g^{ij}$ in terms of the covariant components $g_{ij}$.\n\nLet us begin with the first part. The problem defines the covariant basis vectors as $\\mathbf{x}_{1} = \\frac{\\partial\\mathbf{x}}{\\partial\\xi}$ and $\\mathbf{x}_{2} = \\frac{\\partial\\mathbf{x}}{\\partial\\eta}$. Since the grid is assumed to be nondegenerate, these two vectors are linearly independent and form a basis for the local tangent plane. Consequently, any vector in this plane, including the contravariant basis vectors $\\mathbf{x}^{i}$ for $i \\in \\{1, 2\\}$, can be written as a linear combination of the covariant basis vectors:\n$$ \\mathbf{x}^{i} = \\sum_{k=1}^{2} C^{ik} \\mathbf{x}_{k} $$\nwhere $C^{ik}$ are the coefficients we need to determine.\n\nThe contravariant basis vectors are defined by the duality condition with the covariant basis vectors, given by the Kronecker-delta orthogonality relation:\n$$ \\mathbf{x}^{i} \\cdot \\mathbf{x}_{j} = \\delta^{i}_{j} $$\nwhere $\\delta^{i}_{j}$ is the Kronecker delta, equal to $1$ if $i=j$ and $0$ if $i \\neq j$.\n\nSubstituting the linear combination for $\\mathbf{x}^{i}$ into the duality condition yields:\n$$ \\left( \\sum_{k=1}^{2} C^{ik} \\mathbf{x}_{k} \\right) \\cdot \\mathbf{x}_{j} = \\delta^{i}_{j} $$\nBy the linearity of the dot product, we can write:\n$$ \\sum_{k=1}^{2} C^{ik} (\\mathbf{x}_{k} \\cdot \\mathbf{x}_{j}) = \\delta^{i}_{j} $$\nThe problem defines the components of the covariant metric tensor as $g_{kj} = \\mathbf{x}_{k} \\cdot \\mathbf{x}_{j}$. Substituting this definition into the equation, we get:\n$$ \\sum_{k=1}^{2} C^{ik} g_{kj} = \\delta^{i}_{j} $$\nThis equation represents matrix multiplication. If we let $C$ be the matrix with entries $C^{ij}$ and $G$ be the matrix with entries $g_{ij}$, this equation is equivalent to $C G^T = I$, where $I$ is the $2 \\times 2$ identity matrix. However, the metric tensor is symmetric, meaning $g_{kj} = \\mathbf{x}_{k} \\cdot \\mathbf{x}_{j} = \\mathbf{x}_{j} \\cdot \\mathbf{x}_{k} = g_{jk}$, so its matrix representation $G$ is a symmetric matrix, i.e., $G = G^T$. Therefore, the equation simplifies to $C G = I$. This implies that the matrix $C$ is the inverse of the matrix $G$.\n\nBy definition, the components of the inverse of the covariant metric tensor $(g_{ij})$ are the components of the contravariant metric tensor, denoted by $g^{ij}$. Thus, the coefficients $C^{ik}$ must be the contravariant metric components $g^{ik}$:\n$$ C^{ik} = g^{ik} $$\nSubstituting this result back into our initial expression for $\\mathbf{x}^{i}$ gives the desired relationship:\n$$ \\mathbf{x}^{i} = \\sum_{k=1}^{2} g^{ik} \\mathbf{x}_{k} $$\nThis shows that the contravariant metric tensor components provide the transformation from the covariant basis to the contravariant basis.\n\nNow, we proceed to the second part of the problem: finding the explicit expression for the contravariant metric components $g^{ij}$. As we have just established, the matrix of contravariant components, which we denote as $G^{-1} = (g^{ij})$, is the inverse of the matrix of covariant components, $G = (g_{ij})$.\n\nFor a two-dimensional system, the covariant metric tensor is given by the $2 \\times 2$ matrix:\n$$ G = \\begin{pmatrix} g_{11} & g_{12} \\\\ g_{21} & g_{22} \\end{pmatrix} $$\nGiven the symmetry $g_{12} = g_{21}$, the matrix is:\n$$ G = \\begin{pmatrix} g_{11} & g_{12} \\\\ g_{12} & g_{22} \\end{pmatrix} $$\nThe inverse of a generic invertible $2 \\times 2$ matrix $M = \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}$ is given by the formula:\n$$ M^{-1} = \\frac{1}{ad - bc} \\begin{pmatrix} d & -b \\\\ -c & a \\end{pmatrix} $$\nThe determinant of the metric tensor $G$, denoted by $g$, is:\n$$ g = \\det(G) = g_{11}g_{22} - g_{12}g_{12} = g_{11}g_{22} - g_{12}^{2} $$\nThe problem states that the metric is positive-definite, which ensures that its determinant $g$ is positive, so $g > 0$ and the inverse exists.\n\nApplying the matrix inversion formula to $G$, we find the matrix of contravariant components $g^{ij}$:\n$$ G^{-1} = (g^{ij}) = \\frac{1}{g_{11}g_{22} - g_{12}^{2}} \\begin{pmatrix} g_{22} & -g_{12} \\\\ -g_{12} & g_{11} \\end{pmatrix} $$\nThis provides the closed-form expression for the contravariant metric components $g^{ij}$ as a function of the covariant components $g_{ij}$, as required.",
            "answer": "$$ \\boxed{ \\frac{1}{g_{11}g_{22} - g_{12}^{2}} \\begin{pmatrix} g_{22} & -g_{12} \\\\ -g_{12} & g_{11} \\end{pmatrix} } $$"
        },
        {
            "introduction": "The primary purpose of a computational grid is to provide a structured domain for solving partial differential equations (PDEs). However, a PDE written in physical coordinates $(x,y)$ must be systematically transformed to the computational coordinates $(\\xi,\\eta)$. This practice  demonstrates this crucial process, showing how the chain rule and Jacobian matrix facilitate the conversion and revealing how the PDE's characteristics manifest in the new coordinate system.",
            "id": "3966300",
            "problem": "Consider a smooth, one-to-one curvilinear mapping from computational space to physical space, $(\\xi,\\eta) \\mapsto (x(\\xi,\\eta), y(\\xi,\\eta))$, used in hyperbolic grid generation methods in Computational Fluid Dynamics (CFD). Assume the Jacobian determinant $J = x_{\\xi} y_{\\eta} - x_{\\eta} y_{\\xi}$ is strictly positive everywhere so that the mapping is orientation-preserving and invertible.\n\nA scalar field $u(x,y)$ satisfies the first-order Partial Differential Equation (PDE) $u_{x} + u_{y} = 0$ in physical space. Using only the chain rule for multivariable calculus and the definition of the Jacobian and its inverse, transform this PDE into computational space to obtain a first-order linear PDE in the form $A(\\xi,\\eta)\\,u_{\\xi} + B(\\xi,\\eta)\\,u_{\\eta} = 0$ with $A$ and $B$ expressed solely in terms of $x_{\\xi}$, $x_{\\eta}$, $y_{\\xi}$, $y_{\\eta}$.\n\nThen, using the method of characteristics for first-order linear PDEs, determine the slope $d\\eta/d\\xi$ of the characteristic curves in the $(\\xi,\\eta)$-plane along which $u$ is constant, expressing your result purely in terms of $x_{\\xi}$, $x_{\\eta}$, $y_{\\xi}$, $y_{\\eta}$.\n\nProvide as your final answer the single closed-form analytic expression for the characteristic slope $d\\eta/d\\xi$ in terms of $x_{\\xi}$, $x_{\\eta}$, $y_{\\xi}$, and $y_{\\eta}$ only. No numerical evaluation is required. Do not include any units. If you choose to simplify, ensure your expression is algebraically exact.",
            "solution": "The task is to transform the physical-space Partial Differential Equation (PDE) $u_x + u_y = 0$ into the computational space $(\\xi, \\eta)$, and then find the slope of the characteristic curves for the transformed PDE.\n\nThe mapping from computational space to physical space is given by $(x,y) = (x(\\xi,\\eta), y(\\xi,\\eta))$. The scalar field $u$ can be considered a function of either $(x,y)$ or $(\\xi,\\eta)$. To transform the derivatives of $u$ from physical coordinates to computational coordinates, we employ the chain rule.\n\nThe partial derivatives of $u$ with respect to $\\xi$ and $\\eta$ are:\n$$\n\\frac{\\partial u}{\\partial \\xi} = \\frac{\\partial u}{\\partial x} \\frac{\\partial x}{\\partial \\xi} + \\frac{\\partial u}{\\partial y} \\frac{\\partial y}{\\partial \\xi} \\quad \\implies \\quad u_{\\xi} = u_x x_{\\xi} + u_y y_{\\xi}\n$$\n$$\n\\frac{\\partial u}{\\partial \\eta} = \\frac{\\partial u}{\\partial x} \\frac{\\partial x}{\\partial \\eta} + \\frac{\\partial u}{\\partial y} \\frac{\\partial y}{\\partial \\eta} \\quad \\implies \\quad u_{\\eta} = u_x x_{\\eta} + u_y y_{\\eta}\n$$\n\nThis forms a system of linear equations for the physical-space derivatives $u_x$ and $u_y$, with the computational-space derivatives $u_{\\xi}$ and $u_{\\eta}$ being knowns in this context. In matrix form, the system is:\n$$\n\\begin{pmatrix} x_{\\xi} & y_{\\xi} \\\\ x_{\\eta} & y_{\\eta} \\end{pmatrix}\n\\begin{pmatrix} u_x \\\\ u_y \\end{pmatrix}\n=\n\\begin{pmatrix} u_{\\xi} \\\\ u_{\\eta} \\end{pmatrix}\n$$\nThe matrix on the left is the Jacobian matrix of the transformation $(\\xi,\\eta) \\mapsto (x,y)$. Its determinant is the Jacobian determinant $J = x_{\\xi} y_{\\eta} - x_{\\eta} y_{\\xi}$. The problem states that $J$ is strictly positive, which guarantees that the matrix is invertible.\n\nWe solve for $u_x$ and $u_y$ by inverting the matrix:\n$$\n\\begin{pmatrix} u_x \\\\ u_y \\end{pmatrix}\n=\n\\begin{pmatrix} x_{\\xi} & y_{\\xi} \\\\ x_{\\eta} & y_{\\eta} \\end{pmatrix}^{-1}\n\\begin{pmatrix} u_{\\xi} \\\\ u_{\\eta} \\end{pmatrix}\n=\n\\frac{1}{x_{\\xi} y_{\\eta} - x_{\\eta} y_{\\xi}}\n\\begin{pmatrix} y_{\\eta} & -y_{\\xi} \\\\ -x_{\\eta} & x_{\\xi} \\end{pmatrix}\n\\begin{pmatrix} u_{\\xi} \\\\ u_{\\eta} \\end{pmatrix}\n$$\nThis gives the expressions for $u_x$ and $u_y$ in terms of the computational-space derivatives:\n$$\nu_x = \\frac{1}{J} (y_{\\eta} u_{\\xi} - y_{\\xi} u_{\\eta})\n$$\n$$\nu_y = \\frac{1}{J} (-x_{\\eta} u_{\\xi} + x_{\\xi} u_{\\eta})\n$$\n\nNow, we substitute these expressions into the original PDE, $u_x + u_y = 0$:\n$$\n\\frac{1}{J} (y_{\\eta} u_{\\xi} - y_{\\xi} u_{\\eta}) + \\frac{1}{J} (-x_{\\eta} u_{\\xi} + x_{\\xi} u_{\\eta}) = 0\n$$\nSince $J \\neq 0$, we can multiply both sides by $J$:\n$$\n(y_{\\eta} u_{\\xi} - y_{\\xi} u_{\\eta}) + (-x_{\\eta} u_{\\xi} + x_{\\xi} u_{\\eta}) = 0\n$$\nGrouping terms by $u_{\\xi}$ and $u_{\\eta}$, we obtain the transformed PDE in the required form $A(\\xi,\\eta)\\,u_{\\xi} + B(\\xi,\\eta)\\,u_{\\eta} = 0$:\n$$\n(y_{\\eta} - x_{\\eta}) u_{\\xi} + (x_{\\xi} - y_{\\xi}) u_{\\eta} = 0\n$$\nFrom this, we identify the coefficients $A$ and $B$:\n$$\nA(\\xi,\\eta) = y_{\\eta} - x_{\\eta}\n$$\n$$\nB(\\xi,\\eta) = x_{\\xi} - y_{\\xi}\n$$\n\nNext, we use the method of characteristics to find the slope of the curves in the $(\\xi,\\eta)$-plane along which $u$ is constant. For a first-order linear PDE of the form $A u_{\\xi} + B u_{\\eta} = 0$, the characteristic curves are defined by the ordinary differential equation:\n$$\n\\frac{d\\xi}{A} = \\frac{d\\eta}{B}\n$$\nThis equation describes the direction field of the characteristic curves. The slope of these curves, $\\frac{d\\eta}{d\\xi}$, is therefore given by the ratio of the coefficients:\n$$\n\\frac{d\\eta}{d\\xi} = \\frac{B}{A}\n$$\nSubstituting the expressions we found for $A$ and $B$:\n$$\n\\frac{d\\eta}{d\\xi} = \\frac{x_{\\xi} - y_{\\xi}}{y_{\\eta} - x_{\\eta}}\n$$\nThis expression gives the slope of the characteristic curves in the computational $(\\xi,\\eta)$-plane purely in terms of the partial derivatives of the mapping functions $x(\\xi,\\eta)$ and $y(\\xi,\\eta)$, as required.",
            "answer": "$$\n\\boxed{\\frac{x_{\\xi} - y_{\\xi}}{y_{\\eta} - x_{\\eta}}}\n$$"
        },
        {
            "introduction": "Hyperbolic grid generation is a \"marching\" method, where the grid is constructed step-by-step away from an initial boundary. A key challenge in any such numerical marching scheme is ensuring stability; a poorly chosen step size can lead to catastrophic grid folding, where cells become inverted. This exercise  tackles this practical issue head-on by having you derive a stability constraint that governs the maximum allowable step size, linking the local grid geometry, via the Jacobian determinant, directly to the robustness of the algorithm.",
            "id": "3966307",
            "problem": "A two-dimensional hyperbolic grid generation method is used in Aerospace Computational Fluid Dynamics (CFD) to march a structured grid from a boundary along a marching coordinate. Let the computational coordinates be $\\xi \\in \\mathbb{R}$ (the marching coordinate) and $\\eta \\in [0,1]$ (the tangential coordinate), and let the physical mapping be $(x(\\xi,\\eta), y(\\xi,\\eta))$. The local area scaling is given by the Jacobian determinant $J(\\xi,\\eta) = x_{\\xi}(\\xi,\\eta)\\,y_{\\eta}(\\xi,\\eta) - x_{\\eta}(\\xi,\\eta)\\,y_{\\xi}(\\xi,\\eta)$, where subscripts denote partial derivatives.\n\nTo prevent grid folding during the forward Euler marching update from $\\xi$ to $\\xi + \\Delta \\xi$, the relative change in the Jacobian magnitude per step is constrained by a dimensionless parameter $\\alpha \\in (0,1)$ uniformly in $\\eta$. Starting from smoothness of the mapping and the first-order Taylor expansion in $\\Delta \\xi$, derive an upper bound on $\\Delta \\xi$ that ensures, for all $\\eta \\in [0,1]$, that the magnitude of the change in $J$ over a single step does not exceed the fraction $\\alpha$ of its local value prior to the step. Express this bound in terms of the extrema of $J(\\xi,\\eta)$ and $\\partial_{\\xi} J(\\xi,\\eta)$ over $\\eta$, using the essential supremum norm of $\\partial_{\\xi} J(\\xi,\\eta)$ over $\\eta$, defined by $\\|\\partial_{\\xi} J(\\xi,\\cdot)\\|_{\\infty} := \\max_{\\eta \\in [0,1]} |\\partial_{\\xi} J(\\xi,\\eta)|$.\n\nThen, evaluate your bound at a specific marching station $\\xi$ where the field distributions along $\\eta$ are\n$$\nJ(\\xi,\\eta) = J_{0} + \\beta \\cos(2\\pi \\eta), \\quad \\partial_{\\xi} J(\\xi,\\eta) = A \\cos(2\\pi \\eta) + B \\sin(2\\pi \\eta),\n$$\nwith constants $J_{0} > 0$, $\\beta \\in \\mathbb{R}$ satisfying $J_{0} > |\\beta|$, and $A,B \\in \\mathbb{R}$. Provide the maximum admissible step size $\\Delta \\xi_{\\max}$ as a single closed-form analytic expression in terms of $J_{0}$, $\\beta$, $A$, $B$, and $\\alpha$. No numerical approximation is required; express the final answer exactly without units.",
            "solution": "The problem asks for the derivation of an upper bound on the marching step size, $\\Delta \\xi$, in a hyperbolic grid generation scheme. The bound must ensure that the magnitude of the change in the Jacobian determinant, $J$, over a single step does not exceed a fraction $\\alpha$ of the local Jacobian magnitude. This constraint must hold uniformly for all values of the tangential coordinate $\\eta \\in [0,1]$. After deriving a general expression for this bound, we are asked to evaluate it for specific functional forms of $J$ and a specific marching station $\\xi$.\n\nFirst, we derive the general upper bound for $\\Delta \\xi$.\nThe constraint is given as:\n$$\n|J(\\xi + \\Delta \\xi, \\eta) - J(\\xi, \\eta)| \\leq \\alpha |J(\\xi, \\eta)|\n$$\nThis must be satisfied for all $\\eta \\in [0,1]$. The problem specifies using a first-order Taylor expansion for $J(\\xi + \\Delta \\xi, \\eta)$ about $\\xi$. This is equivalent to a forward Euler marching update.\n$$\nJ(\\xi + \\Delta \\xi, \\eta) \\approx J(\\xi, \\eta) + \\frac{\\partial J}{\\partial \\xi}(\\xi, \\eta) \\Delta \\xi\n$$\nThe change in $J$ over the step $\\Delta \\xi$ is therefore approximated as:\n$$\n\\Delta J(\\xi, \\eta) = J(\\xi + \\Delta \\xi, \\eta) - J(\\xi, \\eta) \\approx \\frac{\\partial J}{\\partial \\xi}(\\xi, \\eta) \\Delta \\xi\n$$\nSubstituting this approximation into the constraint inequality, we obtain:\n$$\n\\left| \\frac{\\partial J}{\\partial \\xi}(\\xi, \\eta) \\Delta \\xi \\right| \\leq \\alpha |J(\\xi, \\eta)|\n$$\nSince the step size $\\Delta \\xi$ is a non-negative scalar, we can write:\n$$\n\\Delta \\xi \\left| \\frac{\\partial J}{\\partial \\xi}(\\xi, \\eta) \\right| \\leq \\alpha |J(\\xi, \\eta)|\n$$\nThis inequality must hold for all $\\eta \\in [0,1]$. To find a single value for $\\Delta \\xi$ that guarantees this for the entire domain of $\\eta$, we must consider the most restrictive case. A sufficient, robust condition can be established by ensuring that the maximum possible value of the left-hand side is less than or equal to the minimum possible value of the right-hand side.\n\nThe maximum value of the left-hand side over $\\eta \\in [0,1]$ is:\n$$\n\\max_{\\eta \\in [0,1]} \\left( \\Delta \\xi \\left| \\frac{\\partial J}{\\partial \\xi}(\\xi, \\eta) \\right| \\right) = \\Delta \\xi \\max_{\\eta \\in [0,1]} \\left| \\frac{\\partial J}{\\partial \\xi}(\\xi, \\eta) \\right| = \\Delta \\xi \\left\\|\\frac{\\partial J}{\\partial \\xi}(\\xi, \\cdot)\\right\\|_{\\infty}\n$$\nThe minimum value of the right-hand side over $\\eta \\in [0,1]$ is:\n$$\n\\min_{\\eta \\in [0,1]} \\left( \\alpha |J(\\xi, \\eta)| \\right) = \\alpha \\min_{\\eta \\in [0,1]} |J(\\xi, \\eta)|\n$$\nThus, a sufficient condition for the constraint to hold over all $\\eta$ is:\n$$\n\\Delta \\xi \\left\\|\\frac{\\partial J}{\\partial \\xi}(\\xi, \\cdot)\\right\\|_{\\infty} \\leq \\alpha \\min_{\\eta \\in [0,1]} |J(\\xi, \\eta)|\n$$\nSolving for $\\Delta \\xi$ gives the desired upper bound. If $\\left\\|\\frac{\\partial J}{\\partial \\xi}(\\xi, \\cdot)\\right\\|_{\\infty} > 0$, we have:\n$$\n\\Delta \\xi \\leq \\alpha \\frac{\\min_{\\eta \\in [0,1]} |J(\\xi, \\eta)|}{\\left\\|\\frac{\\partial J}{\\partial \\xi}(\\xi, \\cdot)\\right\\|_{\\infty}}\n$$\nThe maximum admissible step size, $\\Delta \\xi_{\\max}$, is the value on the right-hand side of this inequality. This expression for the bound is formulated in terms of the extrema of $|J|$ and $|\\partial_{\\xi}J|$ over $\\eta$, as requested.\n\nNext, we evaluate this bound for the given field distributions at a specific marching station $\\xi$:\n$$\nJ(\\xi,\\eta) = J_{0} + \\beta \\cos(2\\pi \\eta)\n$$\n$$\n\\frac{\\partial J}{\\partial \\xi}(\\xi,\\eta) = A \\cos(2\\pi \\eta) + B \\sin(2\\pi \\eta)\n$$\nWe need to find the minimum of $|J(\\xi,\\eta)|$ and the maximum of $|\\partial_{\\xi} J(\\xi,\\eta)|$ over $\\eta \\in [0,1]$.\n\nLet's first analyze $J(\\xi,\\eta)$. We are given that $J_0 > 0$ and $J_0 > |\\beta|$. The term $\\cos(2\\pi \\eta)$ ranges from $-1$ to $1$ as $\\eta$ ranges from $0$ to $1$.\nThe minimum value of $\\beta \\cos(2\\pi \\eta)$ is $-|\\beta|$ and its maximum value is $+|\\beta|$.\nTherefore, the minimum value of $J(\\xi,\\eta)$ is $J_0 - |\\beta|$ and the maximum is $J_0 + |\\beta|$.\nSince we are given $J_0 > |\\beta|$, it follows that $J_0 - |\\beta| > 0$. This means $J(\\xi,\\eta)$ is strictly positive for all $\\eta$. Consequently, $|J(\\xi,\\eta)| = J(\\xi,\\eta)$.\nThe minimum value is:\n$$\n\\min_{\\eta \\in [0,1]} |J(\\xi, \\eta)| = \\min_{\\eta \\in [0,1]} (J_0 + \\beta \\cos(2\\pi \\eta)) = J_0 - |\\beta|\n$$\n\nNow, let's analyze $\\partial_{\\xi} J(\\xi,\\eta)$. This expression is of the form $A \\cos \\theta + B \\sin \\theta$, which can be rewritten using the R-formula (harmonic addition theorem). Let $\\theta = 2\\pi \\eta$.\n$$\nA \\cos \\theta + B \\sin \\theta = R \\cos(\\theta - \\delta)\n$$\nwhere $R = \\sqrt{A^2 + B^2}$ and $\\delta$ is an appropriate phase angle.\nThe amplitude of this sinusoidal function is $R = \\sqrt{A^2 + B^2}$. As $\\eta$ varies over $[0,1]$, the argument $\\theta - \\delta$ covers a full period (or more), so $\\cos(\\theta-\\delta)$ will take all values between $-1$ and $1$.\nTherefore, the maximum absolute value of $\\partial_{\\xi} J(\\xi,\\eta)$ is the amplitude of the oscillation.\nThis is the definition of the essential supremum norm for this function:\n$$\n\\left\\|\\frac{\\partial J}{\\partial \\xi}(\\xi, \\cdot)\\right\\|_{\\infty} = \\max_{\\eta \\in [0,1]} |A \\cos(2\\pi \\eta) + B \\sin(2\\pi \\eta)| = \\sqrt{A^2 + B^2}\n$$\nThis holds assuming $A$ and $B$ are not both zero. If $A=B=0$, then $\\partial_{\\xi} J = 0$, and the bound on $\\Delta \\xi$ would be infinite, which is physically correct as there would be no change in $J$ with $\\xi$.\n\nFinally, we substitute these extrema into the expression for the maximum admissible step size:\n$$\n\\Delta \\xi_{\\max} = \\alpha \\frac{\\min_{\\eta \\in [0,1]} |J(\\xi, \\eta)|}{\\left\\|\\frac{\\partial J}{\\partial \\xi}(\\xi, \\cdot)\\right\\|_{\\infty}} = \\alpha \\frac{J_0 - |\\beta|}{\\sqrt{A^2 + B^2}}\n$$\nThis is the single closed-form analytic expression for the maximum admissible step size in terms of the given parameters.",
            "answer": "$$\n\\boxed{\\alpha \\frac{J_{0} - |\\beta|}{\\sqrt{A^{2} + B^{2}}}}\n$$"
        }
    ]
}