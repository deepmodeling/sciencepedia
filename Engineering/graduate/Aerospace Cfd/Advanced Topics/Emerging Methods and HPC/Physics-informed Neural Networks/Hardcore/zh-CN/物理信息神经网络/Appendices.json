{
    "hands_on_practices": [
        {
            "introduction": "物理信息神经网络（PINN）的核心在于其损失函数，该函数将控制物理定律编码到神经网络的训练过程中。本练习旨在为经典的泊松方程构建损失函数，这是一个在静电学、流体力学和热传导等领域中无处不在的偏微分方程。通过这个基础实践，您将掌握将微分方程及其边界条件转化为可优化目标的核心技能，这是应用PINN解决更复杂问题的第一步。",
            "id": "2126324",
            "problem": "一位研究人员正在构建一个物理信息神经网络（PINN），以寻找二维方形区域内静电势 $V(x,y)$ 的近似解。电势的物理行为由泊松方程描述：\n$$\n\\nabla^2 V(x,y) = -f(x,y)\n$$\n其中 $f(x,y)$ 表示给定的电荷分布密度，$\\nabla^2 = \\frac{\\partial^2}{\\partial x^2} + \\frac{\\partial^2}{\\partial y^2}$ 是拉普拉斯算子。电势定义在域 $D = \\{(x,y) \\mid -L \\le x \\le L, -L \\le y \\le L\\}$ 上。该域的边界 $\\partial D$ 保持在零电势（接地），这施加了边界条件 $V(x,y) = 0$ 对所有 $(x,y) \\in \\partial D$ 成立。\n\nPINN 模型，记为 $\\hat{V}(x,y; \\theta)$，通过最小化一个包含问题物理信息的损失函数 $L(\\theta)$ 来学习近似 $V(x,y)$。这里，$\\theta$ 代表神经网络的所有可训练参数。损失函数使用两组离散点计算：\n1.  一组 $N_{pde}$ 个配置点，$S_{pde} = \\{(x_i, y_i) \\mid i=1, \\dots, N_{pde}\\}$，位于域 $D$ 的内部。\n2.  一组 $N_{bc}$ 个边界点，$S_{bc} = \\{(x_j, y_j) \\mid j=1, \\dots, N_{bc}\\}$，位于边界 $\\partial D$ 上。\n\n总损失函数 $L(\\theta)$ 是两个均方误差项之和：一项用于控制偏微分方程 ($L_{pde}$)，另一项用于边界条件 ($L_{bc}$)。\n\n构建总损失函数 $L(\\theta) = L_{pde} + L_{bc}$ 的数学表达式。您的表达式应使用网络输出 $\\hat{V}$、其二阶偏导数、函数 $f$、给定的点集及其各自的大小 $N_{pde}$ 和 $N_{bc}$ 来表示。",
            "solution": "我们从控制泊松方程和边界条件开始：\n$$\n\\nabla^{2}V(x,y)=-f(x,y), \\quad V(x,y)=0 \\text{ for } (x,y)\\in \\partial D.\n$$\n物理信息神经网络通过 $\\hat{V}(x,y;\\theta)$ 来近似 $V$。在内部配置点 $(x_{i},y_{i})\\in S_{pde}$ 处的偏微分方程残差是通过将泊松方程施加于 $\\hat{V}$ 来定义的：\n$$\nr_{i}(\\theta)=\\nabla^{2}\\hat{V}(x_{i},y_{i};\\theta)+f(x_{i},y_{i}).\n$$\n使用拉普拉斯算子在二维中的定义，这等价于\n$$\nr_{i}(\\theta)=\\frac{\\partial^{2}\\hat{V}}{\\partial x^{2}}(x_{i},y_{i};\\theta)+\\frac{\\partial^{2}\\hat{V}}{\\partial y^{2}}(x_{i},y_{i};\\theta)+f(x_{i},y_{i}).\n$$\n在 $S_{pde}$ 上强制执行偏微分方程的均方误差则为\n$$\nL_{pde}(\\theta)=\\frac{1}{N_{pde}}\\sum_{i=1}^{N_{pde}}\\left(r_{i}(\\theta)\\right)^{2}=\\frac{1}{N_{pde}}\\sum_{i=1}^{N_{pde}}\\left(\\frac{\\partial^{2}\\hat{V}}{\\partial x^{2}}(x_{i},y_{i};\\theta)+\\frac{\\partial^{2}\\hat{V}}{\\partial y^{2}}(x_{i},y_{i};\\theta)+f(x_{i},y_{i})\\right)^{2}.\n$$\n边界条件 $V=0$ on $\\partial D$ 是通过惩罚在边界点 $(x_{j},y_{j})\\in S_{bc}$ 处 $\\hat{V}$ 与零的偏差来强制执行的：\n$$\nL_{bc}(\\theta)=\\frac{1}{N_{bc}}\\sum_{j=1}^{N_{bc}}\\left(\\hat{V}(x_{j},y_{j};\\theta)-0\\right)^{2}=\\frac{1}{N_{bc}}\\sum_{j=1}^{N_{bc}}\\left(\\hat{V}(x_{j},y_{j};\\theta)\\right)^{2}.\n$$\n因此，总损失是两个均方误差项的和：\n$$\nL(\\theta)=L_{pde}(\\theta)+L_{bc}(\\theta)=\\frac{1}{N_{pde}}\\sum_{i=1}^{N_{pde}}\\left(\\frac{\\partial^{2}\\hat{V}}{\\partial x^{2}}(x_{i},y_{i};\\theta)+\\frac{\\partial^{2}\\hat{V}}{\\partial y^{2}}(x_{i},y_{i};\\theta)+f(x_{i},y_{i})\\right)^{2}+\\frac{1}{N_{bc}}\\sum_{j=1}^{N_{bc}}\\left(\\hat{V}(x_{j},y_{j};\\theta)\\right)^{2}.\n$$",
            "answer": "$$\\boxed{\\frac{1}{N_{pde}}\\sum_{i=1}^{N_{pde}}\\left(\\frac{\\partial^{2}\\hat{V}}{\\partial x^{2}}(x_{i},y_{i};\\theta)+\\frac{\\partial^{2}\\hat{V}}{\\partial y^{2}}(x_{i},y_{i};\\theta)+f(x_{i},y_{i})\\right)^{2}+\\frac{1}{N_{bc}}\\sum_{j=1}^{N_{bc}}\\left(\\hat{V}(x_{j},y_{j};\\theta)\\right)^{2}}$$"
        },
        {
            "introduction": "PINN在解决反问题方面表现出色，但这类问题通常是不适定的（ill-posed），对噪声数据极其敏感，容易导致数值不稳定。本练习将带您深入研究具有挑战性的反向热方程问题，并演示如何通过在损失函数中引入正则化项来稳定求解过程。掌握这项技术对于从含噪或稀疏的测量数据中恢复物理上有意义的解至关重要。",
            "id": "2126308",
            "problem": "一个研究团队正在开发一种物理信息神经网络 (PINN)，以解决著名的一维不适定逆向热方程。该方程模拟了时间反演的扩散过程，对最终状态数据中的噪声高度敏感，常导致非物理的、爆炸性的解。\n\n其控制偏微分方程 (PDE) 为：\n$$ \\frac{\\partial u}{\\partial t} + \\alpha \\frac{\\partial^2 u}{\\partial x^2} = 0 $$\n对于时空域 $(x, t) \\in [-L, L] \\times [0, T]$ 上的函数 $u(x, t)$。其中，$\\alpha > 0$ 是热扩散系数。该问题是“逆向”的，因为我们给定的是最终时刻 $T$ 的数据，并且必须推断出所有 $t < T$ 时的解。\n\nPINN 使用神经网络 $\\hat{u}(x, t; \\theta)$ 来近似解，其中 $\\theta$ 代表可训练的网络参数。训练过程旨在最小化一个复合损失函数 $L(\\theta)$。该损失函数是四个不同项的加权和：\n$L(\\theta) = \\lambda_d L_{data} + \\lambda_f L_{pde} + \\lambda_b L_{bc} + \\lambda_r L_{reg}$。\n\n您的任务是根据以下规范构建 $L(\\theta)$ 的完整解析表达式：\n\n1.  **数据保真度损失 ($L_{data}$):** 这一项衡量了与可用数据之间的不匹配程度。给定一组包含 $N_d$ 个最终状态的带噪测量值 $\\{ (x_i, u_i) \\}_{i=1}^{N_d}$，其中 $u_i \\approx u(x_i, T)$。该损失分量使用均方误差 (MSE)。\n\n2.  **PDE 残差损失 ($L_{pde}$):** 这一项强制执行控制物理定律。它是 PDE 残差的均方误差，在一组从定义域内部 $[-L, L] \\times [0, T]$ 采样的 $N_f$ 个配置点 $\\{ (x_j^{(f)}, t_j^{(f)}) \\}_{j=1}^{N_f}$ 上进行评估。\n\n3.  **边界条件损失 ($L_{bc}$):** 系统具有周期性边界条件：$u(-L, t) = u(L, t)$ 和 $\\frac{\\partial u}{\\partial x}(-L, t) = \\frac{\\partial u}{\\partial x}(L, t)$。该损失是两个周期性条件残差的累积均方误差，在一组位于边界区间 $[0, T]$ 上的 $N_b$ 个时间点 $\\{ t_k^{(b)} \\}_{k=1}^{N_b}$ 上进行评估。\n\n4.  **正则化损失 ($L_{reg}$):** 为了抵消逆向问题中的爆炸性不稳定性，添加了一个正则化项。这一项惩罚解在整个时空域上的总体大小。它被定义为场的总“能量”，即能量密度 $\\hat{u}(x, t; \\theta)^2$ 在整个定义域 $(x, t) \\in [-L, L] \\times [0, T]$ 上的积分。\n\n使用已定义的参数和点集，写出总损失函数 $L(\\theta)$ 的完整数学表达式。您的表达式应明确显示所有涉及的和与积分。",
            "solution": "我们已知逆向热方程为\n$$\n\\frac{\\partial u}{\\partial t}(x,t)+\\alpha \\frac{\\partial^{2} u}{\\partial x^{2}}(x,t)=0,\n$$\n在 $(x,t)\\in[-L,L]\\times[0,T]$ 上，我们用神经网络 $\\hat{u}(x,t;\\theta)$ 来近似其解。总损失是一个加权和\n$$\nL(\\theta)=\\lambda_{d}L_{data}+\\lambda_{f}L_{pde}+\\lambda_{b}L_{bc}+\\lambda_{r}L_{reg}.\n$$\n我们现在明确推导每一项。\n\n在最终时刻 $T$ 处，对于 $N_{d}$ 个测量值 $\\{(x_{i},u_{i})\\}_{i=1}^{N_{d}}$，数据保真度损失使用均方误差：\n$$\nL_{data}=\\frac{1}{N_{d}}\\sum_{i=1}^{N_{d}}\\left(\\hat{u}(x_{i},T;\\theta)-u_{i}\\right)^{2}.\n$$\n\n热方程的 PDE 残差为\n$$\nr(x,t;\\theta)=\\frac{\\partial \\hat{u}}{\\partial t}(x,t;\\theta)+\\alpha \\frac{\\partial^{2} \\hat{u}}{\\partial x^{2}}(x,t;\\theta).\n$$\n在 $N_{f}$ 个内部配置点 $\\{(x_{j}^{(f)},t_{j}^{(f)})\\}_{j=1}^{N_{f}}$ 上进行评估，残差损失是均方误差\n$$\nL_{pde}=\\frac{1}{N_{f}}\\sum_{j=1}^{N_{f}}\\left(r(x_{j}^{(f)},t_{j}^{(f)};\\theta)\\right)^{2}\n=\\frac{1}{N_{f}}\\sum_{j=1}^{N_{f}}\\left(\\frac{\\partial \\hat{u}}{\\partial t}(x_{j}^{(f)},t_{j}^{(f)};\\theta)+\\alpha \\frac{\\partial^{2} \\hat{u}}{\\partial x^{2}}(x_{j}^{(f)},t_{j}^{(f)};\\theta)\\right)^{2}.\n$$\n\n周期性边界条件为 $u(-L,t)=u(L,t)$ 和 $\\frac{\\partial u}{\\partial x}(-L,t)=\\frac{\\partial u}{\\partial x}(L,t)$。在 $N_{b}$ 个时间点 $\\{t_{k}^{(b)}\\}_{k=1}^{N_{b}}$ 上，我们构建两个残差的累积均方误差：\n$$\nL_{bc}=\\frac{1}{N_{b}}\\sum_{k=1}^{N_{b}}\\left[\\left(\\hat{u}(-L,t_{k}^{(b)};\\theta)-\\hat{u}(L,t_{k}^{(b)};\\theta)\\right)^{2}\n+\\left(\\frac{\\partial \\hat{u}}{\\partial x}(-L,t_{k}^{(b)};\\theta)-\\frac{\\partial \\hat{u}}{\\partial x}(L,t_{k}^{(b)};\\theta)\\right)^{2}\\right].\n$$\n\n正则化项惩罚场在整个定义域上的总能量，定义为 $\\hat{u}^{2}$ 的时空积分：\n$$\nL_{reg}=\\int_{0}^{T}\\int_{-L}^{L}\\left(\\hat{u}(x,t;\\theta)\\right)^{2}\\,\\mathrm{d}x\\,\\mathrm{d}t.\n$$\n\n将这四项组合起来，得到总损失的完整解析表达式：\n$$\nL(\\theta)=\\lambda_{d}\\frac{1}{N_{d}}\\sum_{i=1}^{N_{d}}\\left(\\hat{u}(x_{i},T;\\theta)-u_{i}\\right)^{2}\n+\\lambda_{f}\\frac{1}{N_{f}}\\sum_{j=1}^{N_{f}}\\left(\\frac{\\partial \\hat{u}}{\\partial t}(x_{j}^{(f)},t_{j}^{(f)};\\theta)+\\alpha \\frac{\\partial^{2} \\hat{u}}{\\partial x^{2}}(x_{j}^{(f)},t_{j}^{(f)};\\theta)\\right)^{2}\n+\\lambda_{b}\\frac{1}{N_{b}}\\sum_{k=1}^{N_{b}}\\left[\\left(\\hat{u}(-L,t_{k}^{(b)};\\theta)-\\hat{u}(L,t_{k}^{(b)};\\theta)\\right)^{2}\n+\\left(\\frac{\\partial \\hat{u}}{\\partial x}(-L,t_{k}^{(b)};\\theta)-\\frac{\\partial \\hat{u}}{\\partial x}(L,t_{k}^{(b)};\\theta)\\right)^{2}\\right]\n+\\lambda_{r}\\int_{0}^{T}\\int_{-L}^{L}\\left(\\hat{u}(x,t;\\theta)\\right)^{2}\\,\\mathrm{d}x\\,\\mathrm{d}t.\n$$\n该表达式按要求明确显示了所有的和与积分，每一项分别对应于指定的数据保真度、PDE残差、周期性边界条件和能量正则化。",
            "answer": "$$\\boxed{\\lambda_{d}\\frac{1}{N_{d}}\\sum_{i=1}^{N_{d}}\\left(\\hat{u}(x_{i},T;\\theta)-u_{i}\\right)^{2}+\\lambda_{f}\\frac{1}{N_{f}}\\sum_{j=1}^{N_{f}}\\left(\\frac{\\partial \\hat{u}}{\\partial t}(x_{j}^{(f)},t_{j}^{(f)};\\theta)+\\alpha \\frac{\\partial^{2} \\hat{u}}{\\partial x^{2}}(x_{j}^{(f)},t_{j}^{(f)};\\theta)\\right)^{2}+\\lambda_{b}\\frac{1}{N_{b}}\\sum_{k=1}^{N_{b}}\\left[\\left(\\hat{u}(-L,t_{k}^{(b)};\\theta)-\\hat{u}(L,t_{k}^{(b)};\\theta)\\right)^{2}+\\left(\\frac{\\partial \\hat{u}}{\\partial x}(-L,t_{k}^{(b)};\\theta)-\\frac{\\partial \\hat{u}}{\\partial x}(L,t_{k}^{(b)};\\theta)\\right)^{2}\\right]+\\lambda_{r}\\int_{0}^{T}\\int_{-L}^{L}\\left(\\hat{u}(x,t;\\theta)\\right)^{2}\\,\\mathrm{d}x\\,\\mathrm{d}t}$$"
        },
        {
            "introduction": "深入理解任何数值方法的优势与局限性，是成为一名优秀研究者的关键。这个动手编码练习旨在探索“谱偏差”（spectral bias），这是神经网络（包括PINN）的一个固有特性，即它们倾向于优先学习解的低频分量。通过训练一个网络去拟合一个包含多种频率成分的函数，您将能够亲手实现并定量地观察到这种现象，从而更深刻地理解PINN在处理多尺度问题时可能面临的挑战。",
            "id": "2427229",
            "problem": "您将实现一个完整、可运行的程序，该程序演示物理信息神经网络（PINN）的谱偏置（spectral bias）。其核心思想是训练一个PINN来求解一个一维边值问题，该问题的已知解是低频正弦和高频正弦的叠加，即 $u(x) = \\sin(x) + \\sin(25x)$，并定量观察在训练过程中哪个频率分量被首先学习。整个过程中角度必须使用弧度制。\n\n从以下具有周期性边界条件的物理一致常微分方程（ODE）开始：\n给定域 $x \\in [0, 2\\pi]$，考虑方程\n$$\nu''(x) + u(x) = -624 \\sin(25x),\n$$\n其周期性边界条件为\n$$\nu(0) = u(2\\pi), \\quad u'(0) = u'(2\\pi).\n$$\n一个经过充分检验的事实是，如果 $u(x) = \\sin(x) + \\sin(25x)$，那么 $u''(x) + u(x) = -624 \\sin(25x)$ 并且周期性边界条件成立。除了边界条件外，您不得使用任何关于 $u(x)$ 的标记训练数据；相反，应在损失函数中使用ODE残差和边界残差，这是物理信息神经网络（PINN）的标准做法。\n\n构建一个具有 $H$ 个隐藏单元和双曲正切激活函数的单隐藏层神经网络 $u_{\\theta}(x)$ 作为试探解。将隐藏层预激活定义为 $z_i(x) = w_i x + b_i$（其中 $i \\in \\{1,\\dots,H\\}$），隐藏层激活定义为 $h_i(x) = \\tanh(z_i(x))$，输出定义为\n$$\nu_{\\theta}(x) = \\sum_{i=1}^{H} a_i h_i(x) + c.\n$$\n使用链式法则和乘积法则，以闭合形式计算 $u_{\\theta}(x)$ 相对于 $x$ 的一阶和二阶导数。回想一下双曲正切及其导数的标准恒等式：\n$$\n\\tanh'(z) = \\operatorname{sech}^2(z), \\quad \\frac{d}{dz}\\operatorname{sech}^2(z) = -2\\operatorname{sech}^2(z)\\tanh(z), \\quad \\operatorname{sech}^2(z) = 1 - \\tanh^2(z).\n$$\n将配置点 $\\{x_n\\}_{n=1}^{N}$ 的逐点物理残差定义为\n$$\nr_{\\text{phys}}(x_n;\\theta) = u_{\\theta}''(x_n) + u_{\\theta}(x_n) - \\left(-624 \\sin(25 x_n)\\right),\n$$\n将周期性边界残差定义为\n$$\nr_{\\text{bc},1}(\\theta) = u_{\\theta}(0) - u_{\\theta}(2\\pi), \\quad r_{\\text{bc},2}(\\theta) = u_{\\theta}'(0) - u_{\\theta}'(2\\pi).\n$$\n使用带有边界权重 $\\lambda_{\\text{bc}}$ 的均方残差损失：\n$$\n\\mathcal{L}(\\theta) = \\frac{1}{N}\\sum_{n=1}^{N} r_{\\text{phys}}(x_n;\\theta)^2 + \\lambda_{\\text{bc}}\\left(r_{\\text{bc},1}(\\theta)^2 + r_{\\text{bc},2}(\\theta)^2\\right).\n$$\n通过基于梯度的优化方法，从随机初始化开始训练参数 $\\theta = \\{a_i, w_i, b_i, c\\}_{i=1}^{H}$。为了定量评估谱偏置，在短暂的训练预算结束后，通过最小二乘法将学习到的函数 $u_{\\theta}(x)$ 投影到 $[0, 2\\pi)$ 上的密集均匀网格上的两个基函数 $\\sin(x)$ 和 $\\sin(25x)$ 上。也就是说，找到系数 $(\\hat{\\alpha}_1, \\hat{\\alpha}_{25})$ 以最小化\n$$\n\\sum_{m=1}^{M}\\left(u_{\\theta}(x_m) - \\hat{\\alpha}_1 \\sin(x_m) - \\hat{\\alpha}_{25}\\sin(25 x_m)\\right)^2,\n$$\n其中 $x_m$ 在 $[0, 2\\pi)$ 中均匀分布。将学习到的振幅定义为 $A_1 = |\\hat{\\alpha}_1|$ 和 $A_{25} = |\\hat{\\alpha}_{25}|$。如果在早期训练中 $A_1 > A_{25}$，则认为存在谱偏置。\n\n使用完全向量化的训练循环和关于所有网络参数的闭合形式梯度来实现程序，仅使用ODE残差和边界残差。不要使用任何外部自动微分库。\n\n测试套件和输出规范：\n- 使用以下三个测试用例来检验不同情况。每个用例指定 $(H, N, K, \\eta)$，其中 $H$ 是隐藏单元的数量， $N$ 是配置点的数量， $K$ 是梯度步数， $\\eta$ 是学习率。在所有用例中均使用 $\\lambda_{\\text{bc}} = 1$。角度以弧度为单位。\n  1. 用例1： $(H, N, K, \\eta) = (20, 128, 60, 0.01)$。\n  2. 用例2： $(H, N, K, \\eta) = (10, 64, 80, 0.01)$。\n  3. 用例3： $(H, N, K, \\eta) = (5, 128, 120, 0.01)$。\n- 对于每个用例，使用固定的种子初始化参数，以确保结果是确定性的。训练 $K$ 步后，通过在包含 $M$ 个点（$M=4096$）的密集网格上进行最小二乘投影来计算 $A_1$ 和 $A_{25}$。记录该用例的布尔结果，定义如下：\n$$\n\\text{result} = \\begin{cases}\n\\text{True},  \\text{if } A_1 > A_{25},\\\\\n\\text{False},  \\text{otherwise.}\n\\end{cases}\n$$\n- 最终输出格式：您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔的结果列表（例如，“[True,True,False]”）。\n\n您的程序必须是自包含的，不接收任何输入，并且可以直接运行。角度必须使用弧度制。所有数值答案都是无量纲的，最终输出是布尔值。训练和投影必须使用上述公式以纯线性代数方式实现，不使用任何外部机器学习框架。其目标是通过这些测试用例证明，物理信息神经网络（PINN）学习低频分量 $\\sin(x)$ 的速度要早于高频分量 $\\sin(25x)$，这与谱偏置的现象一致。",
            "solution": "所提出的问题是计算物理学中一个有效且适定的练习，旨在具体展示物理信息神经网络（PINN）中的谱偏置现象。它具有科学依据，给出了陈述正确的微分方程及其解析解。所有参数和过程都已指定，从而可以得到唯一且可验证的计算结果。我将继续提供解决方案。\n\n目标是训练一个神经网络 $u_{\\theta}(x)$ 来逼近一维常微分方程（ODE）\n$$\nu''(x) + u(x) = -624 \\sin(25x)\n$$\n在域 $x \\in [0, 2\\pi]$ 上，带有周期性边界条件 $u(0) = u(2\\pi)$ 和 $u'(0) = u'(2\\pi)$ 的解。其解析解 $u(x) = \\sin(x) + \\sin(25x)$ 是一个低频分量和一个高频分量的叠加。我们将证明，对PINN损失进行基于梯度的优化，会使网络学习低频分量 $\\sin(x)$ 的速度快于高频分量 $\\sin(25x)$。\n\n首先，我们定义神经网络拟设（ansatz），一个具有 $H$ 个神经元和 $\\tanh$ 激活函数的单隐藏层感知器：\n$$\nu_{\\theta}(x) = \\sum_{i=1}^{H} a_i \\tanh(w_i x + b_i) + c\n$$\n网络的参数为 $\\theta = \\{a_i, w_i, b_i, c\\}_{i=1}^{H}$。为了强制满足ODE，我们必须计算 $u_{\\theta}(x)$ 相对于 $x$ 的一阶和二阶导数。使用链式法则和恒等式 $\\frac{d}{dz}\\tanh(z) = \\operatorname{sech}^2(z)$ 及 $\\frac{d}{dz}\\operatorname{sech}^2(z) = -2\\operatorname{sech}^2(z)\\tanh(z)$，我们得到：\n$$\nu'_{\\theta}(x) = \\frac{d u_{\\theta}}{dx} = \\sum_{i=1}^{H} a_i w_i \\operatorname{sech}^2(w_i x + b_i)\n$$\n$$\nu''_{\\theta}(x) = \\frac{d^2 u_{\\theta}}{dx^2} = -2 \\sum_{i=1}^{H} a_i w_i^2 \\operatorname{sech}^2(w_i x + b_i) \\tanh(w_i x + b_i)\n$$\n\n该网络通过最小化一个由ODE残差和边界条件残差的均方误差组成的损失函数来进行训练。在一组 $N$ 个配置点 $\\{x_n\\}$ 上的物理残差为：\n$$\nr_{\\text{phys}}(x_n;\\theta) = u_{\\theta}''(x_n) + u_{\\theta}(x_n) + 624 \\sin(25 x_n)\n$$\n周期性边界条件残差为：\n$$\nr_{\\text{bc},1}(\\theta) = u_{\\theta}(0) - u_{\\theta}(2\\pi)\n$$\n$$\nr_{\\text{bc},2}(\\theta) = u_{\\theta}'(0) - u_{\\theta}'(2\\pi)\n$$\n总损失函数是一个加权和：\n$$\n\\mathcal{L}(\\theta) = \\mathcal{L}_{\\text{phys}} + \\lambda_{\\text{bc}} \\mathcal{L}_{\\text{bc}} = \\frac{1}{N}\\sum_{n=1}^{N} r_{\\text{phys}}(x_n;\\theta)^2 + \\lambda_{\\text{bc}}\\left(r_{\\text{bc},1}(\\theta)^2 + r_{\\text{bc},2}(\\theta)^2\\right)\n$$\n其中 $\\lambda_{\\text{bc}}$ 是一个用于平衡各项的超参数，给定为 $\\lambda_{\\text{bc}} = 1$。\n\n训练使用梯度下降法进行。参数根据 $\\theta \\leftarrow \\theta - \\eta \\nabla_{\\theta} \\mathcal{L}(\\theta)$ 进行更新，其中 $\\eta$ 是学习率。我们必须推导解析梯度 $\\nabla_{\\theta} \\mathcal{L}(\\theta)$。损失函数相对于任意参数 $p \\in \\theta$ 的梯度为：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial p} = \\frac{2}{N}\\sum_{n=1}^{N} r_{\\text{phys}}(x_n) \\left(\\frac{\\partial u''_{\\theta}(x_n)}{\\partial p} + \\frac{\\partial u_{\\theta}(x_n)}{\\partial p}\\right) + 2\\lambda_{\\text{bc}}r_{\\text{bc},1}\\left(\\frac{\\partial u_{\\theta}(0)}{\\partial p} - \\frac{\\partial u_{\\theta}(2\\pi)}{\\partial p}\\right) + 2\\lambda_{\\text{bc}}r_{\\text{bc},2}\\left(\\frac{\\partial u'_{\\theta}(0)}{\\partial p} - \\frac{\\partial u'_{\\theta}(2\\pi)}{\\partial p}\\right)\n$$\n网络输出及其空间导数相对于参数 $\\{a_k, w_k, b_k, c\\}$ 的导数通过链式法则计算。这些推导虽然繁琐但系统，并以向量化形式实现以提高计算效率。例如，相对于输出权重 $a_k$ 的梯度涉及 $\\frac{\\partial u_{\\theta}(x)}{\\partial a_k} = \\tanh(w_k x + b_k)$ 这样的项。所有梯度的完整表达式都在代码中实现。\n\n在训练指定步数后，我们量化学习到的频率分量。我们在 $[0, 2\\pi)$ 内的 $M$ 个点 $\\{x_m\\}$ 组成的密集网格上评估训练好的网络 $u_{\\theta}(x)$。然后，我们通过求解一个线性最小二乘问题，将这个学习到的函数投影到基函数 $\\sin(x)$ 和 $\\sin(25x)$ 上，以找到最小化以下表达式的系数 $(\\hat{\\alpha}_1, \\hat{\\alpha}_{25})$：\n$$\n\\sum_{m=1}^{M}\\left(u_{\\theta}(x_m) - \\hat{\\alpha}_1 \\sin(x_m) - \\hat{\\alpha}_{25}\\sin(25 x_m)\\right)^2\n$$\n这个问题的解由 $\\hat{\\boldsymbol{\\alpha}} = (\\mathbf{B}^T\\mathbf{B})^{-1}\\mathbf{B}^T\\mathbf{y}$ 给出，其中 $\\mathbf{y}$ 是网络预测值 $u_{\\theta}(x_m)$ 的向量，$\\mathbf{B}$ 是以 $\\sin(x_m)$ 和 $\\sin(25x_m)$ 为列的设计矩阵。学习到的振幅为 $A_1 = |\\hat{\\alpha}_1|$ 和 $A_{25} = |\\hat{\\alpha}_{25}|$。我们得出结论，如果 $A_1 > A_{25}$，则观察到谱偏置。\n\n该实现将遵循这些原则，使用 `numpy` 进行向量化的数值计算，包括完全解析的梯度计算和标准的梯度下降循环。参数初始化将使用固定的随机种子和 Glorot/Xavier 缩放，以保证可复现性和训练稳定性。",
            "answer": "```python\nimport numpy as np\n\nclass PINN:\n    \"\"\"\n    A Physics-Informed Neural Network to demonstrate spectral bias.\n    The implementation is fully vectorized and uses analytical gradients.\n    \"\"\"\n    def __init__(self, H, N, seed):\n        \"\"\"\n        Initializes the PINN.\n        H: number of hidden units\n        N: number of collocation points\n        seed: random seed for parameter initialization\n        \"\"\"\n        self.H = H\n        self.N = N\n        self.lambda_bc = 1.0\n        self.rng = np.random.default_rng(seed)\n\n        # Xavier/Glorot initialization\n        # For weights w, n_in=1, n_out=1 (conceptual). limit = sqrt(6 / (1+1)) = sqrt(3)\n        limit_w = np.sqrt(3.0)\n        self.w = self.rng.uniform(-limit_w, limit_w, size=(1, self.H))\n        \n        # For weights a, n_in=H, n_out=1. limit = sqrt(6 / (H+1))\n        limit_a = np.sqrt(6.0 / (self.H + 1.0))\n        self.a = self.rng.uniform(-limit_a, limit_a, size=(self.H, 1))\n\n        self.b = np.zeros((1, self.H))\n        self.c = np.zeros((1, 1))\n\n        self.x_colloc = np.linspace(0, 2 * np.pi, self.N, endpoint=False).reshape(-1, 1)\n\n    def forward(self, x):\n        \"\"\"\n        Computes the network output u and its derivatives u', u'' w.r.t. x.\n        x: input points, shape (num_points, 1)\n        \"\"\"\n        z = x @ self.w + self.b\n        h = np.tanh(z)\n        s = 1.0 - h**2  # sech^2(z)\n\n        u = h @ self.a + self.c\n        u_prime = (s * self.w) @ self.a\n        u_double_prime = (-2.0 * s * h * (self.w**2)) @ self.a\n\n        return u, u_prime, u_double_prime, h, s\n\n    def _compute_gradients(self):\n        \"\"\"\n        Computes the loss and the gradients of the loss w.r.t. all parameters.\n        All calculations are vectorized.\n        \"\"\"\n        # --- Physics Loss and Gradients ---\n        u, _, u_pp, H_c, S_c = self.forward(self.x_colloc)\n        \n        f_term = -624.0 * np.sin(25.0 * self.x_colloc)\n        r_phys = u_pp + u - f_term\n        loss_phys = np.mean(r_phys**2)\n\n        # Common factor for physics gradients\n        grad_common_phys = (2.0 / self.N) * r_phys\n        \n        # Gradient w.r.t. a\n        d_u_da = H_c\n        d_u_pp_da = -2.0 * S_c * H_c * self.w**2\n        grad_a_phys = (d_u_da + d_u_pp_da).T @ grad_common_phys\n\n        # Gradient w.r.t. b\n        d_u_db = self.a.T * S_c\n        d_u_pp_db = self.a.T * (4.0 * self.w**2 * S_c * H_c**2 - 2.0 * self.w**2 * S_c**2)\n        grad_b_phys = np.sum(grad_common_phys * (d_u_db + d_u_pp_db), axis=0)\n\n        # Gradient w.r.t. w\n        d_u_dw = self.a.T * self.x_colloc * S_c\n        d_u_pp_dw = self.a.T * (\n            -4.0 * self.w * S_c * H_c + \n            self.x_colloc * (4.0 * self.w**2 * S_c * H_c**2 - 2.0 * self.w**2 * S_c**2)\n        )\n        grad_w_phys = np.sum(grad_common_phys * (d_u_dw + d_u_pp_dw), axis=0)\n        \n        # Gradient w.r.t. c\n        grad_c_phys = np.sum(grad_common_phys)\n\n        # --- Boundary Loss and Gradients ---\n        x_bc = np.array([[0.0], [2 * np.pi]])\n        u_bc, u_p_bc, _, H_bc, S_bc = self.forward(x_bc)\n        \n        u0, u2pi = u_bc[0], u_bc[1]\n        u0_p, u2pi_p = u_p_bc[0], u_p_bc[1]\n\n        r_bc1 = u0 - u2pi\n        r_bc2 = u0_p - u2pi_p\n        loss_bc = r_bc1**2 + r_bc2**2\n        \n        H0, H2pi = H_bc[0:1, :], H_bc[1:2, :]\n        S0, S2pi = S_bc[0:1, :], S_bc[1:2, :]\n        \n        # Common factors for BC gradients\n        common1 = 2.0 * self.lambda_bc * r_bc1\n        common2 = 2.0 * self.lambda_bc * r_bc2\n\n        # Gradient w.r.t. a\n        delta_u_da = (H0 - H2pi).T\n        delta_u_p_da = (self.w * (S0 - S2pi)).T\n        grad_a_bc = common1 * delta_u_da + common2 * delta_u_p_da\n        \n        # Gradient w.r.t. b\n        delta_u_db = self.a.T * (S0 - S2pi)\n        delta_u_p_db = -2.0 * self.a.T * self.w * (S0 * H0 - S2pi * H2pi)\n        grad_b_bc = common1 * delta_u_db + common2 * delta_u_p_db\n\n        # Gradient w.r.t. w\n        delta_u_dw = -2.0 * np.pi * self.a.T * S2pi\n        delta_u_p_dw = self.a.T * (S0 - S2pi) + 4.0 * np.pi * self.a.T * self.w * S2pi * H2pi\n        grad_w_bc = common1 * delta_u_dw + common2 * delta_u_p_dw\n\n        # Gradient w.r.t. c (is zero)\n        grad_c_bc = 0.0\n\n        # --- Total Loss and Gradients ---\n        loss = loss_phys + self.lambda_bc * loss_bc\n        grad_a = grad_a_phys + grad_a_bc\n        grad_w = grad_w_phys.reshape(1, -1) + grad_w_bc\n        grad_b = grad_b_phys.reshape(1, -1) + grad_b_bc\n        grad_c = grad_c_phys + grad_c_bc\n\n        return loss, grad_a, grad_w, grad_b, grad_c\n\n    def train(self, K, eta):\n        \"\"\"\n        Trains the network using gradient descent.\n        K: number of training steps\n        eta: learning rate\n        \"\"\"\n        for _ in range(K):\n            loss, grad_a, grad_w, grad_b, grad_c = self._compute_gradients()\n            \n            self.a -= eta * grad_a\n            self.w -= eta * grad_w\n            self.b -= eta * grad_b\n            self.c -= eta * grad_c\n\n    def project_and_analyze(self):\n        \"\"\"\n        Projects the learned function onto sin(x) and sin(25x) and checks for spectral bias.\n        \"\"\"\n        M = 4096\n        x_dense = np.linspace(0, 2 * np.pi, M, endpoint=False).reshape(-1, 1)\n        \n        u_pred, _, _, _, _ = self.forward(x_dense)\n        u_pred = u_pred.flatten()\n        \n        # Create design matrix for least squares\n        B = np.zeros((M, 2))\n        B[:, 0] = np.sin(x_dense.flatten())\n        B[:, 1] = np.sin(25.0 * x_dense.flatten())\n        \n        # Solve least squares problem: B * alpha = u_pred\n        alpha, _, _, _ = np.linalg.lstsq(B, u_pred, rcond=None)\n        \n        A1 = np.abs(alpha[0])\n        A25 = np.abs(alpha[1])\n        \n        return A1 > A25\n\ndef solve():\n    \"\"\"\n    Main function to run the test cases and produce the final output.\n    \"\"\"\n    test_cases = [\n        (20, 128, 60, 0.01),  # Case 1: (H, N, K, eta)\n        (10, 64, 80, 0.01),   # Case 2\n        (5, 128, 120, 0.01),  # Case 3\n    ]\n\n    results = []\n    base_seed = 42\n\n    for i, (H, N, K, eta) in enumerate(test_cases):\n        seed = base_seed + i\n        pinn = PINN(H=H, N=N, seed=seed)\n        pinn.train(K=K, eta=eta)\n        result = pinn.project_and_analyze()\n        results.append(result)\n\n    # Format the final output as specified\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}