{
    "hands_on_practices": [
        {
            "introduction": "This first exercise tackles one of the most practical questions in Monte Carlo simulation: how many samples are needed to achieve a desired precision? By working through the estimation of a simple integral, you will derive the relationship between the standard error of a Monte Carlo estimator and the sample size $N$. This practice  provides a concrete foundation for understanding the characteristic $1/\\sqrt{N}$ convergence rate that governs the efficiency of many Monte Carlo methods.",
            "id": "2653261",
            "problem": "In equilibrium statistical mechanics, configurational averages of observables can be expressed as expectations with respect to a target probability density function (PDF). Consider the observable $f(x)=x^{2}$ defined on the configuration space $\\Omega=[0,1]$, and suppose the target PDF $\\pi(x)$ is uniform on $\\Omega$, that is, $\\pi(x)=1$ for $x \\in [0,1]$. You wish to estimate the integral $I=\\int_{0}^{1} f(x)\\,dx$ using a plain Monte Carlo (MC) scheme based on independent sampling from $\\pi(x)$.\n\nTask: \n1. Construct the plain MC estimator for $I$ using independent and identically distributed samples $X_{1},\\dots,X_{N}$ drawn from the uniform distribution on $[0,1]$, and give its standard error in terms of $N$.\n2. Using only first principles of expectation and variance under independence, determine the minimal integer sample size $N$ required so that the standard error of the estimator is strictly less than $10^{-3}$.\n\nProvide as your final answer only the minimal integer $N$ (dimensionless). No derivations should appear in the final answer. The final answer must be a single number. If rounding is needed, report the minimal integer satisfying the requirement (do not apply a significant-figures rule to this integer).",
            "solution": "The problem requires the determination of a minimal sample size for a Monte Carlo estimation based on a specified precision. We begin by formally defining the components of the problem.\n\nThe integral to be estimated is $I = \\int_{0}^{1} f(x) \\, dx$, with the observable function given as $f(x) = x^{2}$ over the configuration space $\\Omega = [0,1]$. The target probability density function is uniform on this interval, $\\pi(x) = 1$ for $x \\in [0,1]$. The integral $I$ is therefore equivalent to the expectation of $f(X)$, where $X$ is a random variable drawn from the uniform distribution $U(0,1)$.\n$$\nI = \\int_{0}^{1} x^{2} \\cdot \\pi(x) \\, dx = \\mathbb{E}_{\\pi}[f(X)] = \\mathbb{E}[X^{2}]\n$$\nThis establishes the first principle: the integral is an expectation.\n\nThe plain Monte Carlo estimator for $I$, which we denote as $\\hat{I}_{N}$, is constructed as the sample mean of the observable $f(x)$ evaluated over $N$ independent and identically distributed (i.i.d.) samples $X_{1}, \\dots, X_{N}$, drawn from the distribution $\\pi(x)$.\n$$\n\\hat{I}_{N} = \\frac{1}{N} \\sum_{i=1}^{N} f(X_{i}) = \\frac{1}{N} \\sum_{i=1}^{N} X_{i}^{2}\n$$\nThis is the estimator for the first part of the task.\n\nThe second part of the first task is to determine the standard error of this estimator, $\\text{SE}(\\hat{I}_{N})$. The standard error is the square root of the variance of the estimator, $\\text{SE}(\\hat{I}_{N}) = \\sqrt{\\text{Var}(\\hat{I}_{N})}$. Due to the i.i.d. nature of the samples, the variance of the sample mean is the population variance of the observable divided by the sample size $N$.\n$$\n\\text{Var}(\\hat{I}_{N}) = \\text{Var}\\left(\\frac{1}{N} \\sum_{i=1}^{N} X_{i}^{2}\\right) = \\frac{1}{N^{2}} \\sum_{i=1}^{N} \\text{Var}(X_{i}^{2}) = \\frac{N \\cdot \\text{Var}(X^{2})}{N^{2}} = \\frac{\\text{Var}(X^{2})}{N}\n$$\nHere, $\\text{Var}(X^{2})$ is the variance of the observable $f(X) = X^{2}$. The variance of a random variable $Y$ is given by $\\text{Var}(Y) = \\mathbb{E}[Y^{2}] - (\\mathbb{E}[Y])^{2}$. Applying this to our observable $X^{2}$:\n$$\n\\text{Var}(X^{2}) = \\mathbb{E}\\left[(X^{2})^{2}\\right] - \\left(\\mathbb{E}[X^{2}]\\right)^{2} = \\mathbb{E}[X^{4}] - \\left(\\mathbb{E}[X^{2}]\\right)^{2}\n$$\nTo proceed, we must calculate the necessary moments of the uniform distribution $U(0,1)$. The $k$-th moment of a random variable $X \\sim U(0,1)$ is:\n$$\n\\mathbb{E}[X^{k}] = \\int_{0}^{1} x^{k} \\pi(x) \\, dx = \\int_{0}^{1} x^{k} \\, dx = \\left[ \\frac{x^{k+1}}{k+1} \\right]_{0}^{1} = \\frac{1}{k+1}\n$$\nWe require the second and fourth moments:\n$$\n\\mathbb{E}[X^{2}] = \\frac{1}{2+1} = \\frac{1}{3}\n$$\n$$\n\\mathbb{E}[X^{4}] = \\frac{1}{4+1} = \\frac{1}{5}\n$$\nSubstituting these results back into the expression for the variance of the observable:\n$$\n\\text{Var}(X^{2}) = \\frac{1}{5} - \\left(\\frac{1}{3}\\right)^{2} = \\frac{1}{5} - \\frac{1}{9} = \\frac{9 - 5}{45} = \\frac{4}{45}\n$$\nWith this result, the standard error of the estimator $\\hat{I}_{N}$ is:\n$$\n\\text{SE}(\\hat{I}_{N}) = \\sqrt{\\frac{\\text{Var}(X^{2})}{N}} = \\sqrt{\\frac{4/45}{N}} = \\frac{2}{\\sqrt{45N}}\n$$\nThis completes the derivation for the first task.\n\nThe second task is to find the minimal integer sample size $N$ such that the standard error is strictly less than $10^{-3}$. We establish the inequality:\n$$\n\\text{SE}(\\hat{I}_{N}) < 10^{-3}\n$$\n$$\n\\frac{2}{\\sqrt{45N}} < 10^{-3}\n$$\nWe now solve this inequality for $N$.\n$$\n\\sqrt{45N} > \\frac{2}{10^{-3}} = 2 \\times 10^{3} = 2000\n$$\nSquaring both sides yields:\n$$\n45N > (2000)^{2} = 4 \\times 10^{6}\n$$\n$$\nN > \\frac{4 \\times 10^{6}}{45}\n$$\nTo determine the limit on $N$, we evaluate the fraction:\n$$\n\\frac{4000000}{45} = \\frac{800000}{9} = 88888.888\\dots = 88888.\\overline{8}\n$$\nThe condition is thus $N > 88888.\\overline{8}$. Since the sample size $N$ must be an integer, the minimal integer value that satisfies this strict inequality is the smallest integer greater than $88888.\\overline{8}$.\n$$\nN_{\\min} = 88889\n$$\nThis is the minimal required sample size.",
            "answer": "$$\\boxed{88889}$$"
        },
        {
            "introduction": "Real-world simulations in aerospace CFD often require modeling stochastic processes that do not follow a uniform distribution. This practice  addresses the critical task of generating random variates from specified physical laws, such as the exponential distribution for eddy lifetimes. You will derive and implement two foundational algorithms—inverse transform sampling and the Box-Muller transform—to translate uniform random numbers into samples that correctly represent the underlying physics of a model.",
            "id": "3977015",
            "problem": "You are modeling stochastic eddy lifetimes and velocity fluctuations in Aerospace Computational Fluid Dynamics (CFD). In many turbulence models, eddy termination is treated as a memoryless event, and the waiting time until termination is modeled by an exponential random variable, while small-scale velocity fluctuations are well-approximated by independent standard normal random variables in two dimensions. Your tasks are to derive appropriate sampling methods from first principles, implement them, and compute specified Monte Carlo metrics.\n\nStarting from foundational probability definitions, derive an algorithm to sample from an exponential distribution using inverse transform sampling. Specifically, take a random variable $U$ that is uniformly distributed on the open interval $(0,1)$, derive the cumulative distribution function of the exponential distribution and its inverse, and obtain a sampling expression for an exponential random variable $X$ with rate parameter $\\lambda$ measured in $\\mathrm{s}^{-1}$. Ensure the derivation starts from the definition of the cumulative distribution function and does not skip intermediate logical steps.\n\nThen, starting from the joint probability density function of a pair of independent standard normal random variables $(Z_1,Z_2)$, derive a method (Box–Muller transform) to generate $(Z_1,Z_2)$ from two independent uniform random variables $(U_1,U_2)$ on $(0,1)$ by an appropriate change of variables to polar coordinates. Your derivation must include the Jacobian determinant and show why the resulting angles must be uniformly distributed on $[0,2\\pi)$ (angles measured in radians), and why the radial component corresponds to an exponential law in the squared radius.\n\nImplementation requirements:\n- Implement the inverse transform sampling algorithm for the exponential random variable $X$ with parameter $\\lambda$ using uniform random numbers $U$ on $(0,1)$.\n- Implement the Box–Muller transform to generate $(Z_1,Z_2)$ from $(U_1,U_2)$, paying attention to numerical stability for $U_1$ values extremely close to $0$ (the logarithm singularity).\n- Use deterministic pseudorandom number generation by setting fixed seeds for each test case.\n\nCompute the following Monte Carlo metrics for the given test suite. All outputs must be pure numbers (no units appended), and angles must be handled in radians. Where physical units apply, interpret the number in the stated unit.\n\nTest suite:\n- Case $1$: Exponential sampling for eddy lifetimes with $\\lambda=5.0\\,\\mathrm{s}^{-1}$, sample size $N=200000$, seed $20231011$. Compute the sample mean of $X$ and report it as a float in $\\mathrm{s}$.\n- Case $2$: Exponential survival probability with $\\lambda=5.0\\,\\mathrm{s}^{-1}$, threshold $T=0.05\\,\\mathrm{s}$, sample size $N=200000$, seed $20231012$. Compute the Monte Carlo estimate of $\\mathbb{P}(X>T)$ and report the absolute difference between this estimate and the theoretical value derived from first principles as a float (dimensionless).\n- Case $3$: Box–Muller sampling quality with sample size $N=200000$, seed $5555$. Compute the maximum absolute deviation among the following five quantities: the sample means of $Z_1$ and $Z_2$, the differences between the sample variances of $Z_1$ and $Z_2$ and the theoretical value $1$, and the sample correlation between $Z_1$ and $Z_2$. Report this maximum absolute deviation as a single float (dimensionless).\n- Case $4$: Exponential sampling under scaling extremes with $\\lambda=10^{-4}\\,\\mathrm{s}^{-1}$, sample size $N=100000$, seed $7$. Compute the ratio of the sample mean of $X$ to the theoretical mean and report this ratio as a single float (dimensionless).\n- Case $5$: Box–Muller behavior near the logarithm singularity with sample size $N=50000$, seed $8888$. Override one uniform draw to be very close to $0$ (set $U_1=10^{-12}$ for a single pair) and then perform the transform. Report the maximum absolute value across all generated $Z_1$ and $Z_2$ as a single float (dimensionless).\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets in the order of Cases $1$ through $5$ (e.g., \"[$r_1,r_2,r_3,r_4,r_5]$\"). The answers must be numerically computed by your code and must be floats. For quantities involving time, interpret the numerical values in $\\mathrm{s}$ as specified; for all others, the values are dimensionless. Angles must be in radians throughout.",
            "solution": "The problem is assessed to be valid as it is scientifically grounded in probability theory and computational statistics, well-posed with clear objectives and constraints, objective in its language, and internally consistent. It presents a standard but non-trivial set of tasks relevant to Monte Carlo methods in scientific computing.\n\nThe solution proceeds with the derivations requested, followed by the implementation of algorithms to compute the specified metrics for each test case.\n\n**Part 1: Derivation of the Inverse Transform Sampling for the Exponential Distribution**\n\nThe goal is to generate a random variate $X$ from an exponential distribution with rate parameter $\\lambda > 0$, given a uniform random variate $U$ from the interval $(0,1)$.\n\nThe probability density function (PDF) of an exponential random variable $X$ is given by:\n$$ f_X(x; \\lambda) = \\begin{cases} \\lambda e^{-\\lambda x} & \\text{if } x \\ge 0 \\\\ 0 & \\text{if } x < 0 \\end{cases} $$\nThe cumulative distribution function (CDF), $F_X(x)$, is the probability $\\mathbb{P}(X \\le x)$. For $x \\ge 0$, the CDF is calculated by integrating the PDF from $0$ to $x$:\n$$ F_X(x) = \\mathbb{P}(X \\le x) = \\int_0^x f_X(t; \\lambda) dt = \\int_0^x \\lambda e^{-\\lambda t} dt $$\nPerforming the integration yields:\n$$ F_X(x) = \\lambda \\left[ -\\frac{1}{\\lambda} e^{-\\lambda t} \\right]_0^x = -[e^{-\\lambda t}]_0^x = -(e^{-\\lambda x} - e^0) = 1 - e^{-\\lambda x} $$\nSo, the CDF is $F_X(x) = 1 - e^{-\\lambda x}$ for $x \\ge 0$.\n\nThe inverse transform sampling method is based on the principle that if $U$ is a random variable uniformly distributed on $(0,1)$, then the random variable $X = F_X^{-1}(U)$ has the CDF $F_X(x)$. To apply this, we must find the inverse of the CDF, $F_X^{-1}(u)$.\n\nLet $u = F_X(x)$, where $u \\in (0,1)$. We solve for $x$ in terms of $u$:\n$$ u = 1 - e^{-\\lambda x} $$\n$$ e^{-\\lambda x} = 1 - u $$\nTaking the natural logarithm of both sides:\n$$ -\\lambda x = \\ln(1 - u) $$\nFinally, solving for $x$ gives the inverse function:\n$$ x = F_X^{-1}(u) = -\\frac{1}{\\lambda} \\ln(1 - u) $$\nThis expression provides the algorithm for sampling from the exponential distribution.\n\nA common simplification relies on the fact that if $U \\sim U(0,1)$, then the random variable $V = 1 - U$ is also distributed as $U(0,1)$. Therefore, we can substitute $1 - u$ with $u$ in the formula, yielding the computationally equivalent and more direct sampling formula:\n$$ X = -\\frac{1}{\\lambda} \\ln(U) $$\nwhere $U$ is a sample from the uniform distribution on $(0,1)$. The problem statement specifies $U$ on the open interval $(0,1)$, which correctly avoids the singularity of the logarithm at $0$.\n\n**Part 2: Derivation of the Box-Muller Transform**\n\nThe objective is to generate a pair of independent standard normal random variables, $(Z_1, Z_2)$, from two independent uniform random variables, $(U_1, U_2)$, from the interval $(0,1)$.\n\nThe PDF of a standard normal random variable $Z$ is $\\phi(z) = \\frac{1}{\\sqrt{2\\pi}} e^{-z^2/2}$. Since $Z_1$ and $Z_2$ are independent, their joint PDF is the product of their individual PDFs:\n$$ f_{Z_1, Z_2}(z_1, z_2) = \\phi(z_1) \\phi(z_2) = \\left( \\frac{1}{\\sqrt{2\\pi}} e^{-z_1^2/2} \\right) \\left( \\frac{1}{\\sqrt{2\\pi}} e^{-z_2^2/2} \\right) = \\frac{1}{2\\pi} e^{-\\frac{1}{2}(z_1^2 + z_2^2)} $$\nWe now perform a change of variables from the Cartesian coordinates $(z_1, z_2)$ to a polar coordinate system $(r, \\theta)$, where:\n$$ z_1 = r \\cos(\\theta) $$\n$$ z_2 = r \\sin(\\theta) $$\nwith $r \\ge 0$ and $\\theta \\in [0, 2\\pi)$. The term $z_1^2 + z_2^2$ becomes $r^2$.\n\nTo transform the joint PDF, we need the Jacobian determinant of the transformation from $(r, \\theta)$ to $(z_1, z_2)$:\n$$ J = \\det \\begin{pmatrix} \\frac{\\partial z_1}{\\partial r} & \\frac{\\partial z_1}{\\partial \\theta} \\\\ \\frac{\\partial z_2}{\\partial r} & \\frac{\\partial z_2}{\\partial \\theta} \\end{pmatrix} = \\det \\begin{pmatrix} \\cos(\\theta) & -r \\sin(\\theta) \\\\ \\sin(\\theta) & r \\cos(\\theta) \\end{pmatrix} = r \\cos^2(\\theta) - (-r \\sin^2(\\theta)) = r(\\cos^2(\\theta) + \\sin^2(\\theta)) = r $$\nThe absolute value of the Jacobian is $|J| = r$.\n\nThe joint PDF in polar coordinates, $f_{R, \\Theta}(r, \\theta)$, is related to the Cartesian PDF by $f_{R, \\Theta}(r, \\theta) = f_{Z_1, Z_2}(z_1, z_2) |J|$. Substituting $z_1^2 + z_2^2 = r^2$ and $|J| = r$, we get:\n$$ f_{R, \\Theta}(r, \\theta) = \\frac{1}{2\\pi} e^{-r^2/2} \\cdot r $$\nThis joint PDF for $r \\in [0, \\infty)$ and $\\theta \\in [0, 2\\pi)$ can be factored into two independent parts:\n$$ f_{R, \\Theta}(r, \\theta) = \\left( \\frac{1}{2\\pi} \\right) \\cdot \\left( r e^{-r^2/2} \\right) = f_{\\Theta}(\\theta) \\cdot f_R(r) $$\nThis factorization shows that the random variables $R$ and $\\Theta$ are independent.\n\nThe marginal PDF for $\\Theta$ is $f_{\\Theta}(\\theta) = \\frac{1}{2\\pi}$ for $\\theta \\in [0, 2\\pi)$. This is the PDF of a uniform distribution on $[0, 2\\pi)$. Thus, we can generate $\\Theta$ from a uniform random variable $U_2 \\sim U(0,1)$ using the transformation $\\Theta = 2\\pi U_2$. This directly answers why the angle must be uniformly distributed.\n\nThe marginal PDF for $R$ is $f_R(r) = r e^{-r^2/2}$ for $r \\ge 0$. This is a Rayleigh distribution. We can use inverse transform sampling to generate $R$. First, we find its CDF, $F_R(s) = \\mathbb{P}(R \\le s)$:\n$$ F_R(s) = \\int_0^s r e^{-r^2/2} dr $$\nUsing the substitution $v = r^2/2$, which implies $dv = r dr$, the integral becomes:\n$$ F_R(s) = \\int_0^{s^2/2} e^{-v} dv = [-e^{-v}]_0^{s^2/2} = 1 - e^{-s^2/2} $$\nTo find the sampling formula, we set $u_1 = F_R(r) = 1 - e^{-r^2/2}$ and solve for $r$:\n$$ e^{-r^2/2} = 1 - u_1 \\implies -\\frac{r^2}{2} = \\ln(1 - u_1) \\implies r^2 = -2 \\ln(1-u_1) $$\nUsing the same logic as for the exponential distribution, we can replace $1-U_1$ with another uniform random variable $U_1 \\sim U(0,1)$, giving $r^2 = -2 \\ln(U_1)$. This shows that the squared radius, $R^2$, follows an exponential distribution with rate parameter $\\lambda = 1/2$. Taking the square root gives the sampling formula for $R$:\n$$ R = \\sqrt{-2 \\ln(U_1)} $$\nHere, $U_1$ must be in $(0,1)$ to avoid the logarithm of zero.\n\nCombining these results, the Box-Muller transformation algorithm is:\n1.  Generate two independent uniform random variables $U_1, U_2 \\sim U(0,1)$.\n2.  Compute $R = \\sqrt{-2 \\ln(U_1)}$ and $\\Theta = 2\\pi U_2$.\n3.  Transform back to Cartesian coordinates to obtain the standard normal variates:\n    $$ Z_1 = R \\cos(\\Theta) = \\sqrt{-2 \\ln(U_1)} \\cos(2\\pi U_2) $$\n    $$ Z_2 = R \\sin(\\Theta) = \\sqrt{-2 \\ln(U_1)} \\sin(2\\pi U_2) $$\n\nThis completes the required derivations from first principles.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the five test cases specified in the problem statement.\n    \"\"\"\n    results = []\n\n    # --- Case 1: Exponential Sample Mean ---\n    lambda_1 = 5.0\n    N_1 = 200000\n    seed_1 = 20231011\n    rng_1 = np.random.default_rng(seed_1)\n    u_1 = rng_1.uniform(low=0.0, high=1.0, size=N_1)\n    # Ensure u_1 is in (0, 1) by replacing any exact 0.0, though highly unlikely.\n    u_1[u_1 == 0] = np.finfo(float).eps\n    x_1 = -1.0 / lambda_1 * np.log(u_1)\n    sample_mean_1 = np.mean(x_1)\n    results.append(sample_mean_1)\n\n    # --- Case 2: Exponential Survival Probability ---\n    lambda_2 = 5.0\n    T_2 = 0.05\n    N_2 = 200000\n    seed_2 = 20231012\n    rng_2 = np.random.default_rng(seed_2)\n    u_2 = rng_2.uniform(low=0.0, high=1.0, size=N_2)\n    u_2[u_2 == 0] = np.finfo(float).eps\n    x_2 = -1.0 / lambda_2 * np.log(u_2)\n    mc_prob_2 = np.sum(x_2 > T_2) / N_2\n    theoretical_prob_2 = np.exp(-lambda_2 * T_2)\n    abs_diff_2 = np.abs(mc_prob_2 - theoretical_prob_2)\n    results.append(abs_diff_2)\n\n    # --- Case 3: Box-Muller Sampling Quality ---\n    N_3 = 200000\n    seed_3 = 5555\n    rng_3 = np.random.default_rng(seed_3)\n    u1_3 = rng_3.uniform(low=0.0, high=1.0, size=N_3)\n    u2_3 = rng_3.uniform(low=0.0, high=1.0, size=N_3)\n    # Handle potential log(0)\n    u1_3[u1_3 == 0] = np.finfo(float).eps\n    \n    R_3 = np.sqrt(-2.0 * np.log(u1_3))\n    Theta_3 = 2.0 * np.pi * u2_3\n    \n    z1_3 = R_3 * np.cos(Theta_3)\n    z2_3 = R_3 * np.sin(Theta_3)\n    \n    mean_z1 = np.mean(z1_3)\n    mean_z2 = np.mean(z2_3)\n    # Use ddof=1 for sample variance, an unbiased estimator of population variance.\n    var_z1 = np.var(z1_3, ddof=1)\n    var_z2 = np.var(z2_3, ddof=1)\n    corr_z1_z2 = np.corrcoef(z1_3, z2_3)[0, 1]\n    \n    devs_3 = [\n        np.abs(mean_z1),\n        np.abs(mean_z2),\n        np.abs(var_z1 - 1.0),\n        np.abs(var_z2 - 1.0),\n        np.abs(corr_z1_z2)\n    ]\n    max_dev_3 = np.max(devs_3)\n    results.append(max_dev_3)\n\n    # --- Case 4: Exponential Sampling Under Scaling Extremes ---\n    lambda_4 = 1e-4\n    N_4 = 100000\n    seed_4 = 7\n    rng_4 = np.random.default_rng(seed_4)\n    u_4 = rng_4.uniform(low=0.0, high=1.0, size=N_4)\n    u_4[u_4 == 0] = np.finfo(float).eps\n    x_4 = -1.0 / lambda_4 * np.log(u_4)\n    sample_mean_4 = np.mean(x_4)\n    theoretical_mean_4 = 1.0 / lambda_4\n    ratio_4 = sample_mean_4 / theoretical_mean_4\n    results.append(ratio_4)\n    \n    # --- Case 5: Box-Muller Behavior Near Singularity ---\n    N_5 = 50000\n    seed_5 = 8888\n    rng_5 = np.random.default_rng(seed_5)\n    u1_5 = rng_5.uniform(low=0.0, high=1.0, size=N_5)\n    u2_5 = rng_5.uniform(low=0.0, high=1.0, size=N_5)\n    \n    # Override one draw to be very close to 0\n    u1_5[0] = 1e-12\n    # Ensure no other values are exactly 0\n    u1_5[u1_5 == 0] = np.finfo(float).eps\n\n    R_5 = np.sqrt(-2.0 * np.log(u1_5))\n    Theta_5 = 2.0 * np.pi * u2_5\n    \n    z1_5 = R_5 * np.cos(Theta_5)\n    z2_5 = R_5 * np.sin(Theta_5)\n    \n    max_abs_z = np.max([np.max(np.abs(z1_5)), np.max(np.abs(z2_5))])\n    results.append(max_abs_z)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Markov Chain Monte Carlo (MCMC) methods are powerful but depend on subtle theoretical conditions for their validity. This exercise  explores the critical importance of the detailed balance condition by analyzing an algorithm with a non-symmetric proposal distribution that omits the necessary Hastings correction. By dissecting why this seemingly plausible implementation produces a biased result, you will gain a deeper appreciation for the microscopic reversibility that underpins the Metropolis-Hastings algorithm and ensures convergence to the correct target distribution.",
            "id": "2458820",
            "problem": "A canonical target distribution for a one-dimensional system at inverse temperature $\\beta$ is given by $\\pi(x) \\propto \\exp\\!\\left(-\\beta\\,U(x)\\right)$, where the potential energy is $U(x) = \\tfrac{k}{2}\\,x^{2}$ with $k>0$. To sample $\\pi(x)$, an algorithm implements a Markov Chain Monte Carlo (MCMC) scheme with a non-symmetric Gaussian proposal probability density\n$$\ng(x \\to x') \\;=\\; \\frac{1}{\\sqrt{2\\pi}\\,s}\\,\\exp\\!\\left(-\\frac{(x' - x - \\mu)^{2}}{2 s^{2}}\\right),\n$$\nwhere $\\mu \\neq 0$ and $s>0$ are fixed constants. The implemented acceptance probability is\n$$\na(x \\to x') \\;=\\; \\min\\!\\left\\{\\,1,\\; \\exp\\!\\left(-\\beta\\,[\\,U(x') - U(x)\\,]\\right)\\right\\},\n$$\nand no additional correction for the non-symmetric proposal is used.\n\nWhich of the following statements about the long-time behavior of this algorithm are correct?\n\nA. Because the acceptance depends only on the energy difference, the Markov chain satisfies detailed balance with respect to $\\pi(x)$ for any $\\mu$, and therefore samples the exact Boltzmann distribution.\n\nB. For $\\mu \\neq 0$, detailed balance with respect to $\\pi(x)$ is violated; the chain can converge to a biased stationary distribution different from $\\pi(x)$, determined by $\\mu$ and $s$.\n\nC. The violation of detailed balance implies that no stationary distribution can exist for the chain.\n\nD. The algorithm will sample $\\pi(x)$ exactly only in the special case $\\mu = 0$, or if the acceptance probability is modified to include the Hastings factor $g(x' \\to x)/g(x \\to x')$.\n\nE. In steady state with the naive acceptance and $\\mu \\neq 0$, there is a nonzero probability current $J(x,x') = \\pi_{\\mathrm{ss}}(x)\\,P(x \\to x') - \\pi_{\\mathrm{ss}}(x')\\,P(x' \\to x)$ for some pairs $(x,x')$, indicating broken microscopic reversibility.\n\nF. Because $\\mu > 0$, the chain necessarily drifts to $+\\infty$ and fails to be ergodic for any values of $k$, $\\beta$, and $s$.",
            "solution": "The problem statement is subjected to validation and is found to be valid. It describes a well-defined Markov Chain Monte Carlo (MCMC) algorithm with a specified target distribution, proposal density, and acceptance rule. The question asks for an analysis of the algorithm's long-time behavior, which is a standard problem in computational physics.\n\nThe core of the MCMC method is to construct a Markov chain whose stationary distribution is the desired target distribution, $\\pi(x)$. A sufficient condition for this is the satisfaction of the detailed balance condition:\n$$\n\\pi(x) P(x \\to x') = \\pi(x') P(x' \\to x)\n$$\nfor all states $x$ and $x'$. Here, $P(x \\to x')$ is the transition probability from state $x$ to state $x'$. For a Metropolis-Hastings type algorithm, the transition probability for a move from $x$ to $x' \\neq x$ is given by the product of the proposal probability density $g(x \\to x')$ and the acceptance probability $a(x \\to x')$.\n$$\nP(x \\to x') = g(x \\to x') a(x \\to x')\n$$\nThe general form of the acceptance probability that ensures detailed balance is the Metropolis-Hastings rule:\n$$\na_{\\text{MH}}(x \\to x') = \\min\\left\\{1, \\frac{\\pi(x') g(x' \\to x)}{\\pi(x) g(x \\to x')}\\right\\}\n$$\nBy construction, using this acceptance probability in the expression for $P(x \\to x')$ satisfies the detailed balance equation with respect to $\\pi(x)$.\n\nThe problem states that the implemented algorithm uses a \"naive\" acceptance probability, which corresponds to the simpler Metropolis rule:\n$$\na(x \\to x') = \\min\\left\\{1, \\frac{\\pi(x')}{\\pi(x)}\\right\\} = \\min\\left\\{1, \\exp(-\\beta [U(x') - U(x)])\\right\\}\n$$\nLet us test if this choice satisfies detailed balance with the given non-symmetric proposal density. Substituting the implemented $P(x \\to x')$ into the detailed balance condition:\n$$\n\\pi(x) g(x \\to x') a(x \\to x') \\stackrel{?}{=} \\pi(x') g(x' \\to x) a(x' \\to x)\n$$\n$$\n\\pi(x) g(x \\to x') \\min\\left\\{1, \\frac{\\pi(x')}{\\pi(x)}\\right\\} \\stackrel{?}{=} \\pi(x') g(x' \\to x) \\min\\left\\{1, \\frac{\\pi(x)}{\\pi(x')}\\right\\}\n$$\nWe can use the identity $\\min\\{1, A\\} = A \\min\\{1, 1/A\\}$. Let $A = \\pi(x')/\\pi(x)$. The equation becomes:\n$$\n\\pi(x) g(x \\to x') \\frac{\\pi(x')}{\\pi(x)} \\min\\left\\{1, \\frac{\\pi(x)}{\\pi(x')}\\right\\} \\stackrel{?}{=} \\pi(x') g(x' \\to x) \\min\\left\\{1, \\frac{\\pi(x)}{\\pi(x')}\\right\\}\n$$\nDividing both sides by $\\pi(x') \\min\\{1, \\pi(x)/\\pi(x')\\}$, which is non-zero, we find that the condition simplifies to:\n$$\ng(x \\to x') \\stackrel{?}{=} g(x' \\to x)\n$$\nThis demonstrates that the implemented algorithm satisfies detailed balance if and only if the proposal density $g(x \\to x')$ is symmetric.\n\nLet us check the symmetry of the given proposal density:\n$$\ng(x \\to x') = \\frac{1}{\\sqrt{2\\pi}\\,s}\\,\\exp\\left(-\\frac{(x' - x - \\mu)^{2}}{2 s^{2}}\\right)\n$$\n$$\ng(x' \\to x) = \\frac{1}{\\sqrt{2\\pi}\\,s}\\,\\exp\\left(-\\frac{(x - x' - \\mu)^{2}}{2 s^{2}}\\right) = \\frac{1}{\\sqrt{2\\pi}\\,s}\\,\\exp\\left(-\\frac{(-(x' - x) - \\mu)^{2}}{2 s^{2}}\\right) = \\frac{1}{\\sqrt{2\\pi}\\,s}\\,\\exp\\left(-\\frac{(x' - x + \\mu)^{2}}{2 s^{2}}\\right)\n$$\nFor $g(x \\to x') = g(x' \\to x)$, the exponents must be equal:\n$$\n(x' - x - \\mu)^2 = (x' - x + \\mu)^2\n$$\nThis implies $x' - x - \\mu = \\pm(x' - x + \\mu)$. The positive sign leads to $-\\mu = \\mu$, which means $\\mu = 0$. The negative sign leads to $x' - x - \\mu = -(x' - x) - \\mu$, which implies $x' = x$, a trivial case.\nTherefore, the proposal is symmetric only if $\\mu=0$. Since the problem specifies $\\mu \\neq 0$, the proposal is non-symmetric, and the detailed balance condition with respect to $\\pi(x)$ is violated.\n\nNow we evaluate each statement:\n\nA. Because the acceptance depends only on the energy difference, the Markov chain satisfies detailed balance with respect to $\\pi(x)$ for any $\\mu$, and therefore samples the exact Boltzmann distribution.\nThis statement is **Incorrect**. As derived above, the Metropolis acceptance rule (depending only on the energy/probability ratio) ensures detailed balance only for symmetric proposal distributions. Since $g(x \\to x')$ is not symmetric for $\\mu \\neq 0$, detailed balance is violated.\n\nB. For $\\mu \\neq 0$, detailed balance with respect to $\\pi(x)$ is violated; the chain can converge to a biased stationary distribution different from $\\pi(x)$, determined by $\\mu$ and $s$.\nThis statement is **Correct**. The first part, that detailed balance is violated for $\\mu \\neq 0$, is true as demonstrated. When detailed balance with respect to the target $\\pi(x)$ is not satisfied, the chain, if it converges, will converge to a different stationary distribution $\\pi_{\\mathrm{ss}}(x)$. The existence of a stationary distribution is guaranteed because the harmonic potential $U(x) = \\frac{k}{2}x^2$ is confining, ensuring the chain is positive recurrent and thus ergodic. The resulting distribution $\\pi_{\\mathrm{ss}}(x)$ is \"biased\" in the sense that it is not the intended target $\\pi(x)$. The nature of this bias is determined by the parameters of the proposal distribution, namely $\\mu$ and $s$.\n\nC. The violation of detailed balance implies that no stationary distribution can exist for the chain.\nThis statement is **Incorrect**. Violation of detailed balance only implies that the chain is not reversible. A stationary distribution is guaranteed by the weaker condition of global balance, $\\pi_{\\mathrm{ss}}(y) = \\int \\pi_{\\mathrm{ss}}(x) P(x \\to y) dx$. For an ergodic Markov chain, a unique stationary distribution is guaranteed to exist. As argued in B, the confining nature of the potential ensures ergodicity.\n\nD. The algorithm will sample $\\pi(x)$ exactly only in the special case $\\mu = 0$, or if the acceptance probability is modified to include the Hastings factor $g(x' \\to x)/g(x \\to x')$.\nThis statement is **Correct**. It accurately describes the two remedies for the flawed algorithm.\n1. If $\\mu = 0$, the proposal becomes symmetric, and the implemented Metropolis acceptance rule is sufficient to satisfy detailed balance. The algorithm correctly samples $\\pi(x)$.\n2. If the acceptance probability is modified to the full Metropolis-Hastings form, $a(x \\to x') = \\min\\{1, \\frac{\\pi(x')}{\\pi(x)} \\frac{g(x' \\to x)}{g(x \\to x')}\\}$, it correctly accounts for the proposal asymmetry. This corrected algorithm satisfies detailed balance by construction and will sample $\\pi(x)$ for any $\\mu$. The statement correctly identifies these as the conditions for exact sampling.\n\nE. In steady state with the naive acceptance and $\\mu \\neq 0$, there is a nonzero probability current $J(x,x') = \\pi_{\\mathrm{ss}}(x)\\,P(x \\to x') - \\pi_{\\mathrm{ss}}(x')\\,P(x' \\to x)$ for some pairs $(x,x')$, indicating broken microscopic reversibility.\nThis statement is **Correct**. The \"steady state\" refers to the state where the system samples from its stationary distribution, $\\pi_{\\mathrm{ss}}(x)$. Microscopic reversibility is equivalent to the detailed balance condition holding for the stationary distribution, i.e., $J(x,x') = 0$ for all pairs $(x,x')$. The use of a non-symmetric proposal without the proper Hastings correction is analogous to subjecting the system to a non-conservative driving force. Such driven systems typically settle into a non-equilibrium steady state (NESS), which is fundamentally non-reversible and is characterized by the presence of persistent probability currents. The chain does not satisfy detailed balance with respect to its own stationary distribution $\\pi_{\\mathrm{ss}}(x)$, and thus $J(x,x')$ must be non-zero for some pairs $(x,x')$.\n\nF. Because $\\mu > 0$, the chain necessarily drifts to $+\\infty$ and fails to be ergodic for any values of $k$, $\\beta$, and $s$.\nThis statement is **Incorrect**. The proposal mechanism introduces a constant drift bias $\\mu$. However, the target distribution has a quadratic potential $U(x) = \\frac{k}{2}x^2$, which provides a restoring force, $-dU/dx = -kx$, that grows linearly with the displacement from the origin. For large $|x|$, moves further away from the origin are strongly suppressed by the acceptance probability, which depends on $\\exp(-\\beta [U(x') - U(x)])$. This confining potential prevents the walker from escaping to infinity, ensuring the chain is recurrent (specifically, positive recurrent) and thus ergodic. The stationary distribution will have a shifted mean due to the drift, but it will be a proper, normalizable distribution.",
            "answer": "$$\\boxed{BDE}$$"
        }
    ]
}