{
    "hands_on_practices": [
        {
            "introduction": "A primary goal of any turbulence closure model is to satisfy the Reynolds-Averaged Navier-Stokes (RANS) equations. A perfect model would yield a zero residual, signifying a complete momentum balance. This exercise  provides a concrete way to measure a model's physical consistency by calculating the RANS momentum residual for a hypothetical learned Reynolds stress tensor in a 2D boundary layer. Quantifying this residual is a critical step in a posteriori model evaluation and understanding the sources of error in CFD simulations.",
            "id": "3974989",
            "problem": "Consider a steady, two-dimensional, incompressible turbulent boundary layer over a flat plate in the streamwise coordinate $x$ and wall-normal coordinate $y$, with constant density $\\rho$ and dynamic viscosity $\\mu$. The mean velocity field is given by $\\overline{\\boldsymbol{u}}(x,y) = (\\overline{u}(y), \\overline{v}(y))$ with $\\overline{u}(y) = U_{\\infty}\\left(1 - \\exp(-\\beta y)\\right)$ and $\\overline{v}(y) = 0$. The mean pressure gradient is known and given by $\\partial_{x}\\overline{p} = 0$ and $\\partial_{y}\\overline{p} = 0$. A learned turbulence closure model provides a modeled Reynolds stress tensor $\\tau_{ij}^{(m)}(x,y)$ with only the shear component nonzero, $\\tau_{xy}^{(m)}(y) = -\\rho\\,C\\,U_{\\infty}^{2}\\exp(-\\beta y)$, and $\\tau_{xx}^{(m)}(x,y) = 0$, $\\tau_{yy}^{(m)}(x,y) = 0$. Here, $U_{\\infty}$, $\\beta$, and $C$ are positive constants. The domain is the rectangle $\\Omega = \\{(x,y): 0 \\le x \\le L,\\;0 \\le y \\le H\\}$.\n\nStarting from the Reynolds-Averaged Navier-Stokes (RANS) momentum equations for incompressible flow,\n$$\n\\rho\\left(\\overline{u}\\,\\partial_{x}\\overline{u} + \\overline{v}\\,\\partial_{y}\\overline{u}\\right) = -\\partial_{x}\\overline{p} + \\mu\\left(\\partial_{xx}\\overline{u} + \\partial_{yy}\\overline{u}\\right) - \\partial_{x}\\tau_{xx}^{(m)} - \\partial_{y}\\tau_{xy}^{(m)},\n$$\n$$\n\\rho\\left(\\overline{u}\\,\\partial_{x}\\overline{v} + \\overline{v}\\,\\partial_{y}\\overline{v}\\right) = -\\partial_{y}\\overline{p} + \\mu\\left(\\partial_{xx}\\overline{v} + \\partial_{yy}\\overline{v}\\right) - \\partial_{x}\\tau_{xy}^{(m)} - \\partial_{y}\\tau_{yy}^{(m)},\n$$\nassemble the residual vector of the RANS momentum balance,\n$$\n\\boldsymbol{R}(x,y) = \\left(R_{x}(x,y),\\,R_{y}(x,y)\\right),\n$$\ndefined by moving all terms to the left-hand side, that is,\n$$\nR_{x} = -\\partial_{x}\\overline{p} + \\mu\\left(\\partial_{xx}\\overline{u} + \\partial_{yy}\\overline{u}\\right) - \\partial_{x}\\tau_{xx}^{(m)} - \\partial_{y}\\tau_{xy}^{(m)} - \\rho\\left(\\overline{u}\\,\\partial_{x}\\overline{u} + \\overline{v}\\,\\partial_{y}\\overline{u}\\right),\n$$\n$$\nR_{y} = -\\partial_{y}\\overline{p} + \\mu\\left(\\partial_{xx}\\overline{v} + \\partial_{yy}\\overline{v}\\right) - \\partial_{x}\\tau_{xy}^{(m)} - \\partial_{y}\\tau_{yy}^{(m)} - \\rho\\left(\\overline{u}\\,\\partial_{x}\\overline{v} + \\overline{v}\\,\\partial_{y}\\overline{v}\\right).\n$$\nQuantify the residual imbalance by the area-normalized root-mean-square (RMS) norm over the domain $\\Omega$,\n$$\n\\|\\boldsymbol{R}\\|_{2} = \\left(\\frac{1}{|\\Omega|}\\int_{0}^{L}\\int_{0}^{H}\\left(R_{x}^{2}(x,y) + R_{y}^{2}(x,y)\\right)\\,\\mathrm{d}y\\,\\mathrm{d}x\\right)^{\\frac{1}{2}},\n$$\nwhere $|\\Omega| = L H$.\n\nUse the provided parameter values: $\\rho = 1.2$ $\\mathrm{kg/m^{3}}$, $\\mu = 1.8\\times 10^{-5}$ $\\mathrm{Pa\\cdot s}$, $U_{\\infty} = 50$ $\\mathrm{m/s}$, $\\beta = 100$ $\\mathrm{m^{-1}}$, $C = 0.002$, $L = 1.0$ $\\mathrm{m}$, and $H = 0.05$ $\\mathrm{m}$. Round your final answer to four significant figures and express the residual norm in $\\mathrm{N/m^{3}}$.",
            "solution": "The problem requires the calculation of the area-normalized root-mean-square (RMS) norm of the residual of the Reynolds-Averaged Navier-Stokes (RANS) momentum equations for a given set of flow fields and a modeled Reynolds stress tensor.\n\nThe given mean velocity field is $\\overline{\\boldsymbol{u}}(x,y) = (\\overline{u}(y), \\overline{v}(y))$, with components:\n$$\n\\overline{u}(y) = U_{\\infty}\\left(1 - \\exp(-\\beta y)\\right)\n$$\n$$\n\\overline{v}(y) = 0\n$$\nThe mean pressure gradients are given as zero:\n$$\n\\partial_{x}\\overline{p} = 0, \\quad \\partial_{y}\\overline{p} = 0\n$$\nThe modeled Reynolds stress tensor $\\tau_{ij}^{(m)}$ has only one non-zero component:\n$$\n\\tau_{xy}^{(m)}(y) = -\\rho\\,C\\,U_{\\infty}^{2}\\exp(-\\beta y)\n$$\nwith $\\tau_{xx}^{(m)} = 0$ and $\\tau_{yy}^{(m)} = 0$.\n\nThe residuals of the $x$- and $y$-momentum RANS equations are defined as:\n$$\nR_{x} = -\\partial_{x}\\overline{p} + \\mu\\left(\\partial_{xx}\\overline{u} + \\partial_{yy}\\overline{u}\\right) - \\partial_{x}\\tau_{xx}^{(m)} - \\partial_{y}\\tau_{xy}^{(m)} - \\rho\\left(\\overline{u}\\,\\partial_{x}\\overline{u} + \\overline{v}\\,\\partial_{y}\\overline{u}\\right)\n$$\n$$\nR_{y} = -\\partial_{y}\\overline{p} + \\mu\\left(\\partial_{xx}\\overline{v} + \\partial_{yy}\\overline{v}\\right) - \\partial_{x}\\tau_{xy}^{(m)} - \\partial_{y}\\tau_{yy}^{(m)} - \\rho\\left(\\overline{u}\\,\\partial_{x}\\overline{v} + \\overline{v}\\,\\partial_{y}\\overline{v}\\right)\n$$\nOur first step is to compute each term in these residual expressions.\n\nFor the $x$-momentum residual, $R_x$:\nThe convective terms are: Since $\\overline{u}$ depends only on $y$, $\\partial_{x}\\overline{u} = 0$. Also, $\\overline{v}=0$. Therefore, the convective acceleration is zero:\n$$\n\\rho\\left(\\overline{u}\\,\\partial_{x}\\overline{u} + \\overline{v}\\,\\partial_{y}\\overline{u}\\right) = \\rho\\left(\\overline{u}(y) \\cdot 0 + 0 \\cdot \\partial_{y}\\overline{u}(y)\\right) = 0\n$$\nThe pressure gradient term is given: $\\partial_{x}\\overline{p} = 0$.\nThe viscous terms are: Since $\\overline{u}$ depends only on $y$, $\\partial_{xx}\\overline{u} = 0$. We compute the second derivative with respect to $y$:\n$$\n\\partial_{y}\\overline{u} = \\frac{d}{dy}\\left[U_{\\infty}\\left(1 - \\exp(-\\beta y)\\right)\\right] = U_{\\infty}\\beta\\exp(-\\beta y)\n$$\n$$\n\\partial_{yy}\\overline{u} = \\frac{d}{dy}\\left[U_{\\infty}\\beta\\exp(-\\beta y)\\right] = -U_{\\infty}\\beta^2\\exp(-\\beta y)\n$$\nSo the viscous term is $\\mu\\left(\\partial_{xx}\\overline{u} + \\partial_{yy}\\overline{u}\\right) = -\\mu U_{\\infty}\\beta^2\\exp(-\\beta y)$.\nThe Reynolds stress terms are: $\\tau_{xx}^{(m)} = 0$, so $\\partial_{x}\\tau_{xx}^{(m)} = 0$. For $\\tau_{xy}^{(m)}(y)$, we compute the derivative with respect to $y$:\n$$\n\\partial_{y}\\tau_{xy}^{(m)} = \\frac{d}{dy}\\left[-\\rho\\,C\\,U_{\\infty}^{2}\\exp(-\\beta y)\\right] = \\rho\\,C\\,U_{\\infty}^{2}\\beta\\exp(-\\beta y)\n$$\nAssembling all non-zero terms for $R_x$:\n$$\nR_{x}(y) = 0 + \\left(-\\mu U_{\\infty}\\beta^2\\exp(-\\beta y)\\right) - 0 - \\left(\\rho\\,C\\,U_{\\infty}^{2}\\beta\\exp(-\\beta y)\\right) - 0\n$$\n$$\nR_{x}(y) = -U_{\\infty}\\beta\\exp(-\\beta y)\\left(\\mu\\beta + \\rho C U_{\\infty}\\right)\n$$\nNote that $R_x$ is a function of $y$ only.\n\nFor the $y$-momentum residual, $R_y$:\nThe convective terms are: Since $\\overline{v}=0$, all its derivatives are zero ($\\partial_x\\overline{v}=0, \\partial_y\\overline{v}=0, \\partial_{xx}\\overline{v}=0, \\partial_{yy}\\overline{v}=0$).\n$$\n\\rho\\left(\\overline{u}\\,\\partial_{x}\\overline{v} + \\overline{v}\\,\\partial_{y}\\overline{v}\\right) = 0\n$$\nThe pressure gradient term is given: $\\partial_{y}\\overline{p} = 0$.\nThe viscous terms are zero: $\\mu\\left(\\partial_{xx}\\overline{v} + \\partial_{yy}\\overline{v}\\right) = \\mu(0+0) = 0$.\nThe Reynolds stress terms are: $\\tau_{xy}^{(m)}$ depends only on $y$, so $\\partial_{x}\\tau_{xy}^{(m)} = 0$. And $\\tau_{yy}^{(m)}=0$, so $\\partial_{y}\\tau_{yy}^{(m)}=0$.\nTherefore, all terms in the $R_y$ expression are zero:\n$$\nR_{y}(x,y) = 0\n$$\n\nNow we compute the area-normalized RMS norm of the residual vector $\\boldsymbol{R} = (R_x, R_y)$:\n$$\n\\|\\boldsymbol{R}\\|_{2} = \\left(\\frac{1}{|\\Omega|}\\int_{0}^{L}\\int_{0}^{H}\\left(R_{x}^{2}(y) + R_{y}^{2}(y)\\right)\\,\\mathrm{d}y\\,\\mathrm{d}x\\right)^{\\frac{1}{2}}\n$$\nSince $R_y=0$ and $|\\Omega| = LH$, the expression simplifies to:\n$$\n\\|\\boldsymbol{R}\\|_{2} = \\left(\\frac{1}{LH}\\int_{0}^{L}\\int_{0}^{H} R_{x}^{2}(y)\\,\\mathrm{d}y\\,\\mathrm{d}x\\right)^{\\frac{1}{2}}\n$$\nSince $R_x(y)$ does not depend on $x$, the integral over $x$ from $0$ to $L$ is simply multiplication by $L$:\n$$\n\\|\\boldsymbol{R}\\|_{2} = \\left(\\frac{L}{LH}\\int_{0}^{H} R_{x}^{2}(y)\\,\\mathrm{d}y\\right)^{\\frac{1}{2}} = \\left(\\frac{1}{H}\\int_{0}^{H} R_{x}^{2}(y)\\,\\mathrm{d}y\\right)^{\\frac{1}{2}}\n$$\nLet's compute the integral of $R_x^2(y)$:\n$$\nR_{x}^{2}(y) = \\left[-U_{\\infty}\\beta\\exp(-\\beta y)\\left(\\mu\\beta + \\rho C U_{\\infty}\\right)\\right]^2 = U_{\\infty}^2\\beta^2\\left(\\mu\\beta + \\rho C U_{\\infty}\\right)^2 \\exp(-2\\beta y)\n$$\nThe term $A = U_{\\infty}^2\\beta^2\\left(\\mu\\beta + \\rho C U_{\\infty}\\right)^2$ is a constant.\n$$\n\\int_{0}^{H} R_{x}^{2}(y)\\,\\mathrm{d}y = A \\int_{0}^{H} \\exp(-2\\beta y)\\,\\mathrm{d}y = A \\left[\\frac{\\exp(-2\\beta y)}{-2\\beta}\\right]_{0}^{H} = A \\left(\\frac{\\exp(-2\\beta H) - \\exp(0)}{-2\\beta}\\right) = A \\frac{1 - \\exp(-2\\beta H)}{2\\beta}\n$$\nSubstituting this back into the expression for $\\|\\boldsymbol{R}\\|_{2}^2$:\n$$\n\\|\\boldsymbol{R}\\|_{2}^2 = \\frac{1}{H} \\left(U_{\\infty}^2\\beta^2\\left(\\mu\\beta + \\rho C U_{\\infty}\\right)^2 \\frac{1 - \\exp(-2\\beta H)}{2\\beta}\\right)\n$$\n$$\n\\|\\boldsymbol{R}\\|_{2}^2 = \\frac{U_{\\infty}^2\\beta\\left(\\mu\\beta + \\rho C U_{\\infty}\\right)^2}{2H} \\left(1 - \\exp(-2\\beta H)\\right)\n$$\nTaking the square root, and noting that all constants are positive:\n$$\n\\|\\boldsymbol{R}\\|_{2} = U_{\\infty}\\left(\\mu\\beta + \\rho C U_{\\infty}\\right) \\sqrt{\\frac{\\beta}{2H}\\left(1 - \\exp(-2\\beta H)\\right)}\n$$\nNow, we substitute the numerical values:\n$\\rho = 1.2\\,\\mathrm{kg/m^{3}}$, $\\mu = 1.8\\times 10^{-5}\\,\\mathrm{Pa\\cdot s}$, $U_{\\infty} = 50\\,\\mathrm{m/s}$, $\\beta = 100\\,\\mathrm{m^{-1}}$, $C = 0.002$, $H = 0.05\\,\\mathrm{m}$.\n\nFirst, we calculate the terms in the parentheses:\n$$\n\\mu\\beta = (1.8 \\times 10^{-5}) \\times 100 = 1.8 \\times 10^{-3}\n$$\n$$\n\\rho C U_{\\infty} = 1.2 \\times 0.002 \\times 50 = 0.12\n$$\n$$\n\\mu\\beta + \\rho C U_{\\infty} = 0.0018 + 0.12 = 0.1218\n$$\nNext, we calculate the term under the square root. The exponent is:\n$$\n-2\\beta H = -2 \\times 100 \\times 0.05 = -10\n$$\nThe term under the square root is:\n$$\n\\frac{\\beta}{2H}\\left(1 - \\exp(-2\\beta H)\\right) = \\frac{100}{2 \\times 0.05}\\left(1 - \\exp(-10)\\right) = 1000\\left(1 - \\exp(-10)\\right)\n$$\nNow, we combine all parts:\n$$\n\\|\\boldsymbol{R}\\|_{2} = 50 \\times 0.1218 \\times \\sqrt{1000\\left(1 - \\exp(-10)\\right)}\n$$\n$$\n\\|\\boldsymbol{R}\\|_{2} = 6.09 \\times \\sqrt{1000(1 - 4.53999 \\times 10^{-5})}\n$$\n$$\n\\|\\boldsymbol{R}\\|_{2} = 6.09 \\times \\sqrt{999.954600}\n$$\n$$\n\\|\\boldsymbol{R}\\|_{2} = 6.09 \\times 31.622058756\n$$\n$$\n\\|\\boldsymbol{R}\\|_{2} \\approx 192.5783378 \\,\\mathrm{N/m^3}\n$$\nRounding to four significant figures, we get $192.6$.\nThe units were verified during the derivation. The RANS equations represent a force balance per unit volume, so the residual has units of $\\mathrm{N/m^3}$. The RMS norm retains these units.",
            "answer": "$$\n\\boxed{192.6}\n$$"
        },
        {
            "introduction": "Modern machine learning closures are often designed with built-in physical principles, such as Galilean invariance. This is commonly achieved by constructing the Reynolds stress anisotropy tensor, $b_{ij}$, from a basis of tensors derived from the mean flow gradients. This practice problem  invites you to step into the role of the model itself, assembling the anisotropy tensor from its constituent basis tensors and learned coefficients for a canonical shear flow. This process illuminates how abstract model outputs are grounded in the flow's kinematic state and provides insight into the predicted turbulence structure.",
            "id": "3974965",
            "problem": "Consider a steady, incompressible, plane shear flow representative of an aerospace boundary layer subregion, with mean velocity field specified by $U_{1}(x_{j}) = \\dot{\\gamma}\\, y$, $U_{2}(x_{j}) = 0$, and $U_{3}(x_{j}) = 0$, where $x_{1} = x$, $x_{2} = y$, and $x_{3} = z$. The Reynolds stress anisotropy tensor $b_{ij}$ is defined via $b_{ij} = \\tau_{ij}/(2k) - \\delta_{ij}/3$, where $\\tau_{ij}$ are the specific Reynolds stresses $\\overline{u'_i u'_j}$ and $k$ is the turbulent kinetic energy. In a tensor-basis machine-learning closure, the anisotropy is modeled as a contraction of invariant tensor bases constructed from the mean strain-rate and rotation-rate tensors, with\n$$\nb_{ij} = \\sum_{m=1}^{3} \\theta_{m}\\, T^{(m)}_{ij},\n$$\nwhere the first three basis tensors are\n$$\nT^{(1)}_{ij} = \\tilde{S}_{ij}, \\quad T^{(2)}_{ij} = \\tilde{S}_{ik}\\tilde{R}_{kj} - \\tilde{R}_{ik}\\tilde{S}_{kj}, \\quad T^{(3)}_{ij} = \\tilde{S}_{ik}\\tilde{S}_{kj} - \\frac{1}{3}\\,\\operatorname{tr}(\\tilde{S}^{2})\\,\\delta_{ij}.\n$$\nHere, $\\tilde{S}_{ij} = T\\, S_{ij}$ and $\\tilde{R}_{ij} = T\\, R_{ij}$ are the non-dimensionalized mean strain-rate and rotation-rate tensors, respectively, using the turbulent time scale $T = k/\\epsilon$, with $\\epsilon$ the dissipation rate. The kinematic tensors are defined by\n$$\nS_{ij} = \\frac{1}{2}\\left(\\frac{\\partial U_{i}}{\\partial x_{j}} + \\frac{\\partial U_{j}}{\\partial x_{i}}\\right), \\quad R_{ij} = \\frac{1}{2}\\left(\\frac{\\partial U_{i}}{\\partial x_{j}} - \\frac{\\partial U_{j}}{\\partial x_{i}}\\right).\n$$\n\nLet the shear rate, turbulent kinetic energy, and dissipation be given by $\\dot{\\gamma} = 500\\,\\mathrm{s}^{-1}$, $k = 0.5\\,\\mathrm{m}^{2}\\,\\mathrm{s}^{-2}$, and $\\epsilon = 100\\,\\mathrm{m}^{2}\\,\\mathrm{s}^{-3}$, so that $T = k/\\epsilon$. Let the machine-learned scalar coefficients be fixed at the state of interest as $\\theta_{1} = -\\frac{2}{25}$, $\\theta_{2} = \\frac{1}{50}$, and $\\theta_{3} = \\frac{11}{100}$.\n\nUsing these definitions and values, compute the predicted $b_{ij}$, and then determine the largest eigenvalue $\\lambda_{\\max}$ of $b_{ij}$ as a closed-form analytical expression. Provide the final answer as an exact expression; do not round.",
            "solution": "The first step is to compute the mean velocity gradient tensor, $\\frac{\\partial U_i}{\\partial x_j}$, from the given mean velocity field $U_1 = \\dot{\\gamma} y$, $U_2 = 0$, $U_3 = 0$. In index notation, this is $U_1(x_2) = \\dot{\\gamma} x_2$. The only non-zero component of the gradient is $\\frac{\\partial U_1}{\\partial x_2} = \\dot{\\gamma}$. The velocity gradient tensor, $\\nabla\\mathbf{U}$, is therefore:\n$$\n\\frac{\\partial U_i}{\\partial x_j} = \\begin{pmatrix} 0 & \\dot{\\gamma} & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix}\n$$\n\nNext, we calculate the mean strain-rate tensor, $S_{ij}$, and the mean rotation-rate tensor, $R_{ij}$:\n$$\nS_{ij} = \\frac{1}{2}\\left(\\frac{\\partial U_{i}}{\\partial x_{j}} + \\frac{\\partial U_{j}}{\\partial x_{i}}\\right)\n\\quad \\implies \\quad\nS = \\frac{1}{2} \\begin{pmatrix} 0 & \\dot{\\gamma} & 0 \\\\ \\dot{\\gamma} & 0 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 0 & \\frac{\\dot{\\gamma}}{2} & 0 \\\\ \\frac{\\dot{\\gamma}}{2} & 0 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix}\n$$\n$$\nR_{ij} = \\frac{1}{2}\\left(\\frac{\\partial U_{i}}{\\partial x_{j}} - \\frac{\\partial U_{j}}{\\partial x_{i}}\\right)\n\\quad \\implies \\quad\nR = \\frac{1}{2} \\begin{pmatrix} 0 & \\dot{\\gamma} & 0 \\\\ -\\dot{\\gamma} & 0 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 0 & \\frac{\\dot{\\gamma}}{2} & 0 \\\\ -\\frac{\\dot{\\gamma}}{2} & 0 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix}\n$$\n\nThe tensors are non-dimensionalized using the turbulent time scale, $T = k/\\epsilon$. Using the given values $\\dot{\\gamma} = 500\\,\\mathrm{s}^{-1}$, $k = 0.5\\,\\mathrm{m}^{2}\\,\\mathrm{s}^{-2}$, and $\\epsilon = 100\\,\\mathrm{m}^{2}\\,\\mathrm{s}^{-3}$:\n$$\nT = \\frac{k}{\\epsilon} = \\frac{0.5}{100} = 0.005\\,\\mathrm{s}\n$$\nThe non-dimensional tensors are $\\tilde{S}_{ij} = T S_{ij}$ and $\\tilde{R}_{ij} = T R_{ij}$. Let's define a parameter $\\alpha = \\frac{T\\dot{\\gamma}}{2}$ for convenience.\n$$\n\\alpha = \\frac{(0.005\\,\\mathrm{s})(500\\,\\mathrm{s}^{-1})}{2} = \\frac{2.5}{2} = 1.25 = \\frac{5}{4}\n$$\nThe non-dimensional tensors can be written in terms of $\\alpha$:\n$$\n\\tilde{S} = \\begin{pmatrix} 0 & \\alpha & 0 \\\\ \\alpha & 0 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix}, \\quad \\tilde{R} = \\begin{pmatrix} 0 & \\alpha & 0 \\\\ -\\alpha & 0 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix}\n$$\n\nNow, we construct the three tensor bases $T^{(m)}_{ij}$:\n1.  $T^{(1)}_{ij} = \\tilde{S}_{ij}$:\n    $$\n    T^{(1)} = \\begin{pmatrix} 0 & \\alpha & 0 \\\\ \\alpha & 0 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix}\n    $$\n2.  $T^{(2)}_{ij} = \\tilde{S}_{ik}\\tilde{R}_{kj} - \\tilde{R}_{ik}\\tilde{S}_{kj}$:\n    $$\n    \\tilde{S}\\tilde{R} = \\begin{pmatrix} 0 & \\alpha & 0 \\\\ \\alpha & 0 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} 0 & \\alpha & 0 \\\\ -\\alpha & 0 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix} = \\begin{pmatrix} -\\alpha^2 & 0 & 0 \\\\ 0 & \\alpha^2 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix}\n    $$\n    $$\n    \\tilde{R}\\tilde{S} = \\begin{pmatrix} 0 & \\alpha & 0 \\\\ -\\alpha & 0 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} 0 & \\alpha & 0 \\\\ \\alpha & 0 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix} = \\begin{pmatrix} \\alpha^2 & 0 & 0 \\\\ 0 & -\\alpha^2 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix}\n    $$\n    $$\n    T^{(2)} = \\tilde{S}\\tilde{R} - \\tilde{R}\\tilde{S} = \\begin{pmatrix} -2\\alpha^2 & 0 & 0 \\\\ 0 & 2\\alpha^2 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix}\n    $$\n3.  $T^{(3)}_{ij} = \\tilde{S}_{ik}\\tilde{S}_{kj} - \\frac{1}{3}\\operatorname{tr}(\\tilde{S}^2)\\delta_{ij}$:\n    $$\n    \\tilde{S}^2 = \\tilde{S}\\tilde{S} = \\begin{pmatrix} 0 & \\alpha & 0 \\\\ \\alpha & 0 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} 0 & \\alpha & 0 \\\\ \\alpha & 0 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix} = \\begin{pmatrix} \\alpha^2 & 0 & 0 \\\\ 0 & \\alpha^2 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix}\n    $$\n    The trace is $\\operatorname{tr}(\\tilde{S}^2) = \\alpha^2 + \\alpha^2 + 0 = 2\\alpha^2$.\n    $$\n    T^{(3)} = \\begin{pmatrix} \\alpha^2 & 0 & 0 \\\\ 0 & \\alpha^2 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix} - \\frac{2\\alpha^2}{3}\\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} = \\begin{pmatrix} \\frac{\\alpha^2}{3} & 0 & 0 \\\\ 0 & \\frac{\\alpha^2}{3} & 0 \\\\ 0 & 0 & -\\frac{2\\alpha^2}{3} \\end{pmatrix}\n    $$\n\nNext, we assemble the anisotropy tensor $b_{ij} = \\sum_{m=1}^{3} \\theta_{m}\\, T^{(m)}_{ij}$ using the given coefficients $\\theta_{1} = -2/25$, $\\theta_{2} = 1/50$, and $\\theta_{3} = 11/100$.\n$$\nb = \\theta_{1}T^{(1)} + \\theta_{2}T^{(2)} + \\theta_{3}T^{(3)}\n$$\n$$\nb = \\theta_1 \\begin{pmatrix} 0 & \\alpha & 0 \\\\ \\alpha & 0 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix} + \\theta_2 \\begin{pmatrix} -2\\alpha^2 & 0 & 0 \\\\ 0 & 2\\alpha^2 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix} + \\theta_3 \\begin{pmatrix} \\frac{\\alpha^2}{3} & 0 & 0 \\\\ 0 & \\frac{\\alpha^2}{3} & 0 \\\\ 0 & 0 & -\\frac{2\\alpha^2}{3} \\end{pmatrix}\n$$\nCombining these gives the matrix for $b_{ij}$:\n$$\nb = \\begin{pmatrix} -2\\theta_2\\alpha^2 + \\frac{\\theta_3\\alpha^2}{3} & \\theta_1\\alpha & 0 \\\\ \\theta_1\\alpha & 2\\theta_2\\alpha^2 + \\frac{\\theta_3\\alpha^2}{3} & 0 \\\\ 0 & 0 & -\\frac{2\\theta_3\\alpha^2}{3} \\end{pmatrix}\n$$\nThe tensor $b_{ij}$ is block-diagonal. One eigenvalue is immediately apparent from the $(3,3)$ component:\n$$\n\\lambda_3 = -\\frac{2\\theta_3\\alpha^2}{3}\n$$\nThe other two eigenvalues, $\\lambda_1$ and $\\lambda_2$, are the eigenvalues of the upper-left $2\\times2$ block matrix. For a general $2\\times2$ symmetric matrix $\\begin{pmatrix} a & c \\\\ c & b \\end{pmatrix}$, the eigenvalues are $\\lambda = \\frac{a+b}{2} \\pm \\sqrt{\\left(\\frac{a-b}{2}\\right)^2 + c^2}$.\nLet $a = (-2\\theta_2 + \\frac{\\theta_3}{3})\\alpha^2$, $b = (2\\theta_2 + \\frac{\\theta_3}{3})\\alpha^2$, and $c = \\theta_1\\alpha$.\n$$\n\\frac{a+b}{2} = \\frac{1}{2} \\left( \\frac{2\\theta_3\\alpha^2}{3} \\right) = \\frac{\\theta_3\\alpha^2}{3}\n$$\n$$\n\\frac{a-b}{2} = \\frac{1}{2} \\left( -4\\theta_2\\alpha^2 \\right) = -2\\theta_2\\alpha^2\n$$\nSo the other two eigenvalues are:\n$$\n\\lambda_{1,2} = \\frac{\\theta_3\\alpha^2}{3} \\pm \\sqrt{(-2\\theta_2\\alpha^2)^2 + (\\theta_1\\alpha)^2} = \\frac{\\theta_3\\alpha^2}{3} \\pm \\sqrt{4\\theta_2^2\\alpha^4 + \\theta_1^2\\alpha^2}\n$$\nSince $\\alpha > 0$, we can factor out $\\alpha$ from the square root:\n$$\n\\lambda_{1,2} = \\frac{\\theta_3\\alpha^2}{3} \\pm \\alpha\\sqrt{4\\theta_2^2\\alpha^2 + \\theta_1^2}\n$$\nThe three eigenvalues are:\n$$\n\\lambda_1 = \\frac{\\theta_3\\alpha^2}{3} + \\alpha\\sqrt{4\\theta_2^2\\alpha^2 + \\theta_1^2}\n$$\n$$\n\\lambda_2 = \\frac{\\theta_3\\alpha^2}{3} - \\alpha\\sqrt{4\\theta_2^2\\alpha^2 + \\theta_1^2}\n$$\n$$\n\\lambda_3 = -\\frac{2\\theta_3\\alpha^2}{3}\n$$\nNow, substitute the numerical values: $\\alpha = 5/4$, $\\alpha^2 = 25/16$, $\\theta_1 = -2/25$, $\\theta_2 = 1/50$, $\\theta_3 = 11/100$.\nThe term under the square root is:\n$$\n4\\theta_2^2\\alpha^2 + \\theta_1^2 = 4\\left(\\frac{1}{50}\\right)^2\\left(\\frac{25}{16}\\right) + \\left(-\\frac{2}{25}\\right)^2 = 4\\left(\\frac{1}{2500}\\right)\\left(\\frac{25}{16}\\right) + \\frac{4}{625} = \\frac{100}{40000} + \\frac{4}{625} = \\frac{1}{400} + \\frac{4}{625}\n$$\nThe least common denominator of $400=16\\times25$ and $625=25\\times25$ is $16\\times625=10000$.\n$$\n\\frac{1}{400} + \\frac{4}{625} = \\frac{25}{10000} + \\frac{64}{10000} = \\frac{89}{10000}\n$$\nThe radical term is:\n$$\n\\alpha\\sqrt{4\\theta_2^2\\alpha^2 + \\theta_1^2} = \\frac{5}{4}\\sqrt{\\frac{89}{10000}} = \\frac{5}{4}\\frac{\\sqrt{89}}{100} = \\frac{\\sqrt{89}}{80}\n$$\nThe first term in the expression for $\\lambda_{1,2}$ is:\n$$\n\\frac{\\theta_3\\alpha^2}{3} = \\frac{(11/100)(25/16)}{3} = \\frac{11/(4\\times16)}{3} = \\frac{11}{64 \\times 3} = \\frac{11}{192}\n$$\nThe eigenvalues are:\n$$\n\\lambda_1 = \\frac{11}{192} + \\frac{\\sqrt{89}}{80}\n$$\n$$\n\\lambda_2 = \\frac{11}{192} - \\frac{\\sqrt{89}}{80}\n$$\n$$\n\\lambda_3 = -2\\left(\\frac{\\theta_3\\alpha^2}{3}\\right) = -2\\left(\\frac{11}{192}\\right) = -\\frac{22}{192} = -\\frac{11}{96}\n$$\nTo find the largest eigenvalue, we compare them.\n$\\lambda_1$ is positive.\nFor $\\lambda_2$, we compare $11/192$ and $\\sqrt{89}/80$. Since $9 < \\sqrt{89} < 10$, $\\sqrt{89}/80 \\approx 9.4/80 \\approx 0.1175$. And $11/192 \\approx 11/200 = 0.055$. Since $\\sqrt{89}/80 > 11/192$, $\\lambda_2$ is negative.\n$\\lambda_3 = -11/96 = -0.1145...$ is also negative.\nTherefore, the largest eigenvalue is $\\lambda_{\\max} = \\lambda_1$.\nTo write the final answer in the required form, we find a common denominator for $192 = 3 \\times 64$ and $80 = 5 \\times 16$. The least common multiple is $3 \\times 5 \\times 64 = 960$.\n$$\n\\lambda_{\\max} = \\frac{11}{192} + \\frac{\\sqrt{89}}{80} = \\frac{11 \\times 5}{192 \\times 5} + \\frac{\\sqrt{89} \\times 12}{80 \\times 12} = \\frac{55}{960} + \\frac{12\\sqrt{89}}{960} = \\frac{55 + 12\\sqrt{89}}{960}\n$$",
            "answer": "$$\\boxed{\\frac{55 + 12\\sqrt{89}}{960}}$$"
        },
        {
            "introduction": "A key challenge in developing ML-based physical models is enforcing fundamental constraints, such as the symmetry of the Reynolds stress tensor. Naive model architectures may produce non-symmetric outputs, violating physical law. This exercise  guides you through a powerful technique to address this: projecting the model's output onto the subspace of symmetric matrices. You will not only derive this projection but also analyze its profound effect on the training process, demonstrating how enforcing physics can stabilize learning by reducing the variance of the loss function's gradient.",
            "id": "3975008",
            "problem": "In Reynolds-Averaged Navierâ€“Stokes (RANS) closure modeling within Computational Fluid Dynamics (CFD), the Reynolds stress anisotropy tensor, denoted by $b_{ij}$, is physically constrained to be symmetric. Consider a Machine Learning (ML) model that outputs a raw matrix $\\mathbf{B} \\in \\mathbb{R}^{3 \\times 3}$ intended to approximate a symmetric target $\\mathbf{b}^{\\star}$, but due to numerical error slightly violates symmetry so that $b_{ij} \\neq b_{ji}$. One way to enforce the physical constraint during training is to project the raw output $\\mathbf{B}$ onto the subspace of symmetric matrices using the orthogonal projection induced by the Frobenius inner product.\n\nAssume the following setup:\n- The Frobenius inner product between matrices $\\mathbf{X}, \\mathbf{Y} \\in \\mathbb{R}^{3 \\times 3}$ is $\\langle \\mathbf{X}, \\mathbf{Y} \\rangle_{F} \\equiv \\mathrm{tr}(\\mathbf{X}^{\\top} \\mathbf{Y})$, and the Frobenius norm is $\\|\\mathbf{X}\\|_{F}^{2} \\equiv \\langle \\mathbf{X}, \\mathbf{X} \\rangle_{F}$.\n- The symmetric target $\\mathbf{b}^{\\star}$ is deterministic and satisfies $(\\mathbf{b}^{\\star})^{\\top} = \\mathbf{b}^{\\star}$.\n- Training uses the mean-squared error with and without projection. Without projection, the loss is $L_{0}(\\mathbf{B}) \\equiv \\tfrac{1}{2} \\|\\mathbf{B} - \\mathbf{b}^{\\star}\\|_{F}^{2}$. With projection, the loss is $L(\\mathbf{B}) \\equiv \\tfrac{1}{2} \\|\\mathcal{P}(\\mathbf{B}) - \\mathbf{b}^{\\star}\\|_{F}^{2}$, where $\\mathcal{P}$ is the orthogonal projection onto the subspace of symmetric matrices under $\\langle \\cdot, \\cdot \\rangle_{F}$.\n- Define the training stability metric at the network output as the expected squared Frobenius norm of the gradient with respect to the raw output $\\mathbf{B}$. Assume the raw prediction error $\\mathbf{E} \\equiv \\mathbf{B} - \\mathbf{b}^{\\star}$ has independent, zero-mean entries with variance $\\sigma^{2}$, identical for all $9$ entries, and is independent of $\\mathbf{b}^{\\star}$.\n\nTasks:\n- Starting from the definition of orthogonal projection in an inner product space, derive the explicit form of $\\mathcal{P}$ as the minimizer of $\\min_{\\mathbf{S} = \\mathbf{S}^{\\top}} \\|\\mathbf{B} - \\mathbf{S}\\|_{F}^{2}$.\n- Using matrix calculus and the adjoint of linear operators under the Frobenius inner product, derive $\\nabla_{\\mathbf{B}} L_{0}(\\mathbf{B})$ and $\\nabla_{\\mathbf{B}} L(\\mathbf{B})$.\n- Under the stated statistical assumptions on $\\mathbf{E}$, compute the ratio\n$$\n\\mathcal{R} \\equiv \\frac{\\mathbb{E}\\!\\left[\\|\\nabla_{\\mathbf{B}} L(\\mathbf{B})\\|_{F}^{2}\\right]}{\\mathbb{E}\\!\\left[\\|\\nabla_{\\mathbf{B}} L_{0}(\\mathbf{B})\\|_{F}^{2}\\right]} \\, ,\n$$\nwhich quantifies the factor by which the expected squared gradient norm is reduced when enforcing symmetry by orthogonal projection.\n\nProvide your final answer for $\\mathcal{R}$ as an exact value, with no units. If you use any numerical simplification, do not round; give the exact fraction or radical form.",
            "solution": "### Part 1: Derivation of the Projection Operator $\\mathcal{P}$\n\nThe operator $\\mathcal{P}(\\mathbf{B})$ is the orthogonal projection of a matrix $\\mathbf{B} \\in \\mathbb{R}^{3 \\times 3}$ onto the subspace of symmetric matrices, which we denote by $\\mathcal{S}$. By definition, this projection $\\mathcal{P}(\\mathbf{B})$ is the unique symmetric matrix $\\mathbf{S}$ that minimizes the squared Frobenius norm of the difference, $\\|\\mathbf{B} - \\mathbf{S}\\|_{F}^{2}$, over all $\\mathbf{S} \\in \\mathcal{S}$.\nLet $f(\\mathbf{S}) = \\|\\mathbf{B} - \\mathbf{S}\\|_{F}^{2} = \\langle \\mathbf{B} - \\mathbf{S}, \\mathbf{B} - \\mathbf{S} \\rangle_{F}$.\nFor $\\mathbf{S}_{opt} = \\mathcal{P}(\\mathbf{B})$ to be the minimizer, the directional derivative of $f$ at $\\mathbf{S}_{opt}$ in any direction $\\mathbf{H} \\in \\mathcal{S}$ must be zero. Since $\\mathcal{S}$ is a linear subspace, this is equivalent to the condition that the gradient of $f$ with respect to $\\mathbf{S}$ is orthogonal to the subspace $\\mathcal{S}$. This is the same as stating that the residual, $\\mathbf{B} - \\mathbf{S}_{opt}$, must lie in the orthogonal complement of $\\mathcal{S}$, denoted $\\mathcal{S}^{\\perp}$.\n\nThe space of $3 \\times 3$ matrices $\\mathbb{R}^{3 \\times 3}$ can be decomposed into the direct sum of the subspace of symmetric matrices $\\mathcal{S}$ and the subspace of skew-symmetric matrices $\\mathcal{A} = \\{\\mathbf{A} \\in \\mathbb{R}^{3 \\times 3} \\mid \\mathbf{A}^{\\top} = -\\mathbf{A}\\}$. These two subspaces are orthogonal with respect to the Frobenius inner product. To verify this, let $\\mathbf{S} \\in \\mathcal{S}$ and $\\mathbf{A} \\in \\mathcal{A}$.\n$$\n\\langle \\mathbf{S}, \\mathbf{A} \\rangle_{F} = \\mathrm{tr}(\\mathbf{S}^{\\top}\\mathbf{A}) = \\mathrm{tr}(\\mathbf{S}\\mathbf{A})\n$$\nUsing the property $\\mathrm{tr}(\\mathbf{X}) = \\mathrm{tr}(\\mathbf{X}^{\\top})$,\n$$\n\\mathrm{tr}(\\mathbf{S}\\mathbf{A}) = \\mathrm{tr}((\\mathbf{S}\\mathbf{A})^{\\top}) = \\mathrm{tr}(\\mathbf{A}^{\\top}\\mathbf{S}^{\\top}) = \\mathrm{tr}((-\\mathbf{A})\\mathbf{S}) = -\\mathrm{tr}(\\mathbf{A}\\mathbf{S}) = -\\mathrm{tr}(\\mathbf{S}\\mathbf{A})\n$$\nThe only scalar equal to its negative is $0$, so $\\langle \\mathbf{S}, \\mathbf{A} \\rangle_{F} = 0$. Thus, $\\mathcal{S}^{\\perp} = \\mathcal{A}$.\n\nThe optimality condition requires that $\\mathbf{B} - \\mathbf{S}_{opt} \\in \\mathcal{A}$, meaning it must be skew-symmetric.\n$$\n(\\mathbf{B} - \\mathbf{S}_{opt})^{\\top} = -(\\mathbf{B} - \\mathbf{S}_{opt})\n$$\n$$\n\\mathbf{B}^{\\top} - \\mathbf{S}_{opt}^{\\top} = -\\mathbf{B} + \\mathbf{S}_{opt}\n$$\nSince $\\mathbf{S}_{opt}$ is symmetric, $\\mathbf{S}_{opt}^{\\top} = \\mathbf{S}_{opt}$.\n$$\n\\mathbf{B}^{\\top} - \\mathbf{S}_{opt} = -\\mathbf{B} + \\mathbf{S}_{opt}\n$$\n$$\n2\\mathbf{S}_{opt} = \\mathbf{B} + \\mathbf{B}^{\\top}\n$$\n$$\n\\mathbf{S}_{opt} = \\frac{1}{2}(\\mathbf{B} + \\mathbf{B}^{\\top})\n$$\nThus, the projection operator is given by $\\mathcal{P}(\\mathbf{B}) = \\frac{1}{2}(\\mathbf{B} + \\mathbf{B}^{\\top})$.\n\n### Part 2: Derivation of the Gradients\n\nWe use the definition of the gradient $\\nabla_{\\mathbf{X}} f$ through the directional derivative: $df = \\langle \\nabla_{\\mathbf{X}} f, d\\mathbf{X} \\rangle_{F}$.\n\nFor the loss function without projection, $L_{0}(\\mathbf{B}) = \\frac{1}{2} \\|\\mathbf{B} - \\mathbf{b}^{\\star}\\|_{F}^{2} = \\frac{1}{2} \\langle \\mathbf{B} - \\mathbf{b}^{\\star}, \\mathbf{B} - \\mathbf{b}^{\\star} \\rangle_{F}$.\n$$\ndL_{0} = \\frac{1}{2} \\cdot 2 \\langle d\\mathbf{B}, \\mathbf{B} - \\mathbf{b}^{\\star} \\rangle_{F} = \\langle \\mathbf{B} - \\mathbf{b}^{\\star}, d\\mathbf{B} \\rangle_{F}\n$$\nBy inspection, the gradient is:\n$$\n\\nabla_{\\mathbf{B}} L_{0}(\\mathbf{B}) = \\mathbf{B} - \\mathbf{b}^{\\star}\n$$\n\nFor the loss function with projection, $L(\\mathbf{B}) = \\frac{1}{2} \\|\\mathcal{P}(\\mathbf{B}) - \\mathbf{b}^{\\star}\\|_{F}^{2} = \\frac{1}{2} \\langle \\mathcal{P}(\\mathbf{B}) - \\mathbf{b}^{\\star}, \\mathcal{P}(\\mathbf{B}) - \\mathbf{b}^{\\star} \\rangle_{F}$.\n$$\ndL = \\langle d(\\mathcal{P}(\\mathbf{B})), \\mathcal{P}(\\mathbf{B}) - \\mathbf{b}^{\\star} \\rangle_{F}\n$$\nSince $\\mathcal{P}$ is a linear operator, $d(\\mathcal{P}(\\mathbf{B})) = \\mathcal{P}(d\\mathbf{B})$.\n$$\ndL = \\langle \\mathcal{P}(d\\mathbf{B}), \\mathcal{P}(\\mathbf{B}) - \\mathbf{b}^{\\star} \\rangle_{F}\n$$\nTo find the gradient, we must express $dL$ in the form $\\langle d\\mathbf{B}, \\nabla_{\\mathbf{B}} L \\rangle_{F}$. This requires the adjoint of $\\mathcal{P}$, denoted $\\mathcal{P}^{\\dagger}$, which satisfies $\\langle \\mathcal{P}(\\mathbf{X}), \\mathbf{Y} \\rangle_{F} = \\langle \\mathbf{X}, \\mathcal{P}^{\\dagger}(\\mathbf{Y}) \\rangle_{F}$. As shown in Part 1, the subspace of symmetric matrices is orthogonal to its complement, which makes the orthogonal projection self-adjoint, i.e., $\\mathcal{P}^{\\dagger} = \\mathcal{P}$.\n$$\ndL = \\langle d\\mathbf{B}, \\mathcal{P}^{\\dagger}(\\mathcal{P}(\\mathbf{B}) - \\mathbf{b}^{\\star}) \\rangle_{F} = \\langle d\\mathbf{B}, \\mathcal{P}(\\mathcal{P}(\\mathbf{B}) - \\mathbf{b}^{\\star}) \\rangle_{F}\n$$\nSo, $\\nabla_{\\mathbf{B}} L(\\mathbf{B}) = \\mathcal{P}(\\mathcal{P}(\\mathbf{B}) - \\mathbf{b}^{\\star})$. Since $\\mathcal{P}$ is a projection, it is idempotent ($\\mathcal{P} \\circ \\mathcal{P} = \\mathcal{P}$).\n$$\n\\nabla_{\\mathbf{B}} L(\\mathbf{B}) = \\mathcal{P}(\\mathcal{P}(\\mathbf{B})) - \\mathcal{P}(\\mathbf{b}^{\\star}) = \\mathcal{P}(\\mathbf{B}) - \\mathcal{P}(\\mathbfb^{\\star})\n$$\nThe target $\\mathbf{b}^{\\star}$ is symmetric, so it lies within the subspace onto which we are projecting. Therefore, $\\mathcal{P}(\\mathbf{b}^{\\star}) = \\mathbf{b}^{\\star}$.\n$$\n\\nabla_{\\mathbf{B}} L(\\mathbf{B}) = \\mathcal{P}(\\mathbf{B}) - \\mathbf{b}^{\\star}\n$$\n\n### Part 3: Computation of the Ratio $\\mathcal{R}$\n\nThe ratio to compute is $\\mathcal{R} \\equiv \\frac{\\mathbb{E}\\!\\left[\\|\\nabla_{\\mathbf{B}} L(\\mathbf{B})\\|_{F}^{2}\\right]}{\\mathbb{E}\\!\\left[\\|\\nabla_{\\mathbf{B}} L_{0}(\\mathbf{B})\\|_{F}^{2}\\right]}$. Let $\\mathbf{E} \\equiv \\mathbf{B} - \\mathbf{b}^{\\star}$ be the raw error matrix. Its entries $E_{ij}$ are independent with $\\mathbb{E}[E_{ij}]=0$ and $\\mathbb{E}[E_{ij}^{2}] = \\mathrm{Var}(E_{ij}) = \\sigma^2$.\n\nFirst, we compute the denominator:\n$$\n\\nabla_{\\mathbf{B}} L_{0}(\\mathbf{B}) = \\mathbf{B} - \\mathbf{b}^{\\star} = \\mathbf{E}\n$$\n$$\n\\mathbb{E}\\!\\left[\\|\\nabla_{\\mathbf{B}} L_{0}(\\mathbf{B})\\|_{F}^{2}\\right] = \\mathbb{E}\\!\\left[\\|\\mathbf{E}\\|_{F}^{2}\\right] = \\mathbb{E}\\!\\left[\\sum_{i=1}^{3}\\sum_{j=1}^{3} E_{ij}^2\\right]\n$$\nBy linearity of expectation,\n$$\n\\mathbb{E}\\!\\left[\\sum_{i,j} E_{ij}^2\\right] = \\sum_{i,j} \\mathbb{E}[E_{ij}^2] = \\sum_{i,j} \\sigma^2 = 9 \\sigma^2\n$$\n\nNext, we compute the numerator:\n$$\n\\nabla_{\\mathbf{B}} L(\\mathbf{B}) = \\mathcal{P}(\\mathbf{B}) - \\mathbf{b}^{\\star}\n$$\nSubstituting $\\mathbf{B} = \\mathbf{E} + \\mathbf{b}^{\\star}$:\n$$\n\\nabla_{\\mathbf{B}} L(\\mathbf{B}) = \\mathcal{P}(\\mathbf{E} + \\mathbf{b}^{\\star}) - \\mathbf{b}^{\\star} = \\mathcal{P}(\\mathbf{E}) + \\mathcal{P}(\\mathbf{b}^{\\star}) - \\mathbf{b}^{\\star} = \\mathcal{P}(\\mathbf{E}) + \\mathbf{b}^{\\star} - \\mathbf{b}^{\\star} = \\mathcal{P}(\\mathbf{E})\n$$\nSo we need to compute $\\mathbb{E}[\\|\\mathcal{P}(\\mathbf{E})\\|_{F}^{2}]$. We decompose the error matrix $\\mathbf{E}$ into its symmetric part $\\mathbf{S_E} = \\mathcal{P}(\\mathbf{E}) = \\frac{1}{2}(\\mathbf{E} + \\mathbf{E}^{\\top})$ and its skew-symmetric part $\\mathbf{A_E} = \\frac{1}{2}(\\mathbf{E} - \\mathbf{E}^{\\top})$.\nSince $\\mathbf{S_E}$ and $\\mathbf{A_E}$ are orthogonal, $\\|\\mathbf{E}\\|_{F}^{2} = \\|\\mathbf{S_E}\\|_{F}^{2} + \\|\\mathbf{A_E}\\|_{F}^{2}$. Taking the expectation, $\\mathbb{E}[\\|\\mathbf{E}\\|_{F}^{2}] = \\mathbb{E}[\\|\\mathbf{S_E}\\|_{F}^{2}] + \\mathbb{E}[\\|\\mathbf{A_E}\\|_{F}^{2}]$.\nWe already know $\\mathbb{E}[\\|\\mathbf{E}\\|_{F}^{2}] = 9\\sigma^2$. Let's compute $\\mathbb{E}[\\|\\mathbf{A_E}\\|_{F}^{2}]$. The components of $\\mathbf{A_E}$ are $(A_E)_{ij} = \\frac{1}{2}(E_{ij} - E_{ji})$.\n$$\n\\|\\mathbf{A_E}\\|_{F}^{2} = \\sum_{i,j} \\left( \\frac{1}{2}(E_{ij} - E_{ji}) \\right)^2 = \\frac{1}{4} \\sum_{i,j} (E_{ij}^2 - 2E_{ij}E_{ji} + E_{ji}^2)\n$$\nTaking the expectation:\n$$\n\\mathbb{E}[\\|\\mathbf{A_E}\\|_{F}^{2}] = \\frac{1}{4} \\sum_{i,j} \\left(\\mathbb{E}[E_{ij}^2] - 2\\mathbb{E}[E_{ij}E_{ji}] + \\mathbb{E}[E_{ji}^2]\\right)\n$$\nWe have $\\mathbb{E}[E_{ij}^2] = \\sigma^2$. For the cross-term, if $i \\neq j$, $E_{ij}$ and $E_{ji}$ are independent, so $\\mathbb{E}[E_{ij}E_{ji}] = \\mathbb{E}[E_{ij}]\\mathbb{E}[E_{ji}] = 0 \\cdot 0 = 0$. If $i=j$, $\\mathbb{E}[E_{ii}E_{ii}] = \\mathbb{E}[E_{ii}^2] = \\sigma^2$.\nThe sum $\\sum_{i,j} \\mathbb{E}[E_{ij}^2]$ is $9\\sigma^2$. The sum $\\sum_{i,j} \\mathbb{E}[E_{ji}^2]$ is also $9\\sigma^2$.\nThe sum $\\sum_{i,j} \\mathbb{E}[E_{ij}E_{ji}]$ contains $3$ diagonal terms ($i=j$) and $6$ off-diagonal terms ($i \\neq j$):\n$$\n\\sum_{i,j} \\mathbb{E}[E_{ij}E_{ji}] = \\sum_{i=1}^3 \\mathbb{E}[E_{ii}^2] + \\sum_{i \\neq j} \\mathbb{E}[E_{ij}E_{ji}] = 3\\sigma^2 + 6 \\cdot 0 = 3\\sigma^2\n$$\nSubstituting these into the expression for $\\mathbb{E}[\\|\\mathbf{A_E}\\|_{F}^{2}]$:\n$$\n\\mathbb{E}[\\|\\mathbf{A_E}\\|_{F}^{2}] = \\frac{1}{4}(9\\sigma^2 - 2(3\\sigma^2) + 9\\sigma^2) = \\frac{1}{4}(18\\sigma^2 - 6\\sigma^2) = \\frac{12\\sigma^2}{4} = 3\\sigma^2\n$$\nNow we find the numerator of our ratio, which is $\\mathbb{E}[\\|\\mathbf{S_E}\\|_{F}^{2}] = \\mathbb{E}[\\|\\mathcal{P}(\\mathbf{E})\\|_{F}^{2}]$.\n$$\n\\mathbb{E}[\\|\\mathbf{S_E}\\|_{F}^{2}] = \\mathbb{E}[\\|\\mathbf{E}\\|_{F}^{2}] - \\mathbb{E}[\\|\\mathbf{A_E}\\|_{F}^{2}] = 9\\sigma^2 - 3\\sigma^2 = 6\\sigma^2\n$$\nFinally, we compute the ratio $\\mathcal{R}$:\n$$\n\\mathcal{R} = \\frac{\\mathbb{E}\\!\\left[\\|\\nabla_{\\mathbf{B}} L(\\mathbf{B})\\|_{F}^{2}\\right]}{\\mathbb{E}\\!\\left[\\|\\nabla_{\\mathbf{B}} L_{0}(\\mathbf{B})\\|_{F}^{2}\\right]} = \\frac{6\\sigma^2}{9\\sigma^2} = \\frac{6}{9} = \\frac{2}{3}\n$$\nEnforcing the symmetry constraint via orthogonal projection reduces the expected squared norm of the training gradient by a factor of $2/3$ under the given statistical assumptions.",
            "answer": "$$\\boxed{\\frac{2}{3}}$$"
        }
    ]
}