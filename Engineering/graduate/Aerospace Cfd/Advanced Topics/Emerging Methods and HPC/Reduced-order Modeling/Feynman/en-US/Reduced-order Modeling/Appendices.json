{
    "hands_on_practices": [
        {
            "introduction": "A central task in reduced-order modeling is to decide how much information to discard. Proper Orthogonal Decomposition (POD) provides an elegant way to do this through an energy-based criterion. This practice () will guide you through the fundamental process of analyzing singular values to select a model rank $r$ that retains a desired percentage of the system's total energy, a crucial trade-off between model fidelity and computational efficiency.",
            "id": "3990098",
            "problem": "Consider reduced-order modeling for aerospace computational fluid dynamics based on Proper Orthogonal Decomposition (POD). Let the snapshot matrix be denoted by $X \\in \\mathbb{R}^{m \\times n}$, and assume it admits a Singular Value Decomposition (SVD) $X = U \\Sigma V^\\top$, where the diagonal entries of $\\Sigma$ are the singular values $\\sigma_1, \\sigma_2, \\dots, \\sigma_k$ with $k = \\min(m,n)$, and $U$ and $V$ are orthonormal matrices. In POD, the energetic content associated with mode $i$ is proportional to $\\sigma_i^2$, and the total energy is proportional to $\\sum_{i=1}^{k} \\sigma_i^2$. The cumulative energy retained by a rank-$r$ approximation is defined as the fraction of total energy captured by the leading $r$ singular values. The neglected energy fraction is defined as the complement of the retained fraction.\n\nStarting from these definitions and the SVD properties, do the following for each provided test case. You must treat any list of singular values as an unordered multiset of nonnegative real numbers and first sort them in nonincreasing order before any computation.\n\n1. Given a list of singular values $\\{\\sigma_i\\}_{i=1}^k$, compute the cumulative energy retained curve $\\{E_r\\}_{r=0}^k$ defined by\n$$\nE_r = \\frac{\\sum_{i=1}^{r} \\sigma_i^2}{\\sum_{i=1}^{k} \\sigma_i^2},\n$$\nwith the convention $E_0 = 0$. If $\\sum_{i=1}^{k} \\sigma_i^2 = 0$, define $E_r = 0$ for all $r$ and, for the purpose of the tolerance criterion below, define the neglected energy fraction to be $0$.\n2. Given a tolerance $\\epsilon \\in [0,1]$ (expressed as a decimal, not a percentage), select the smallest integer $r \\in \\{0,1,\\dots,k\\}$ such that the neglected energy fraction $1 - E_r \\le \\epsilon$.\n3. For each test case, output a list of the form $[r, [E_0, E_1, \\dots, E_k], \\text{ok}]$, where $r$ is the selected rank, $[E_0, E_1, \\dots, E_k]$ is the cumulative energy retained curve, and $\\text{ok}$ is a boolean value indicating whether the inequality $1 - E_r \\le \\epsilon$ holds for the selected $r$.\n\nUse the following test suite of singular value arrays and tolerances, designed to cover typical behavior, boundary conditions, and edge cases that probe sensitivity to the tolerance:\n- Test case $1$: singular values $[4.0, 2.0, 1.0, 0.5, 0.25]$, tolerance $\\epsilon = 0.05$.\n- Test case $2$: singular values $[3.0, 0.0, 0.0]$, tolerance $\\epsilon = 0$.\n- Test case $3$: singular values $[0.5, 2.0, 1.0]$, tolerance $\\epsilon = 0.2$.\n- Test case $4$: singular values $[10, 9.9, 0.1, 0.1, 0.1]$, tolerance $\\epsilon = 0.0002$.\n- Test case $5$: singular values $[10, 9.9, 0.1, 0.1, 0.1]$, tolerance $\\epsilon = 0.00015$.\n- Test case $6$: singular values $[0.0, 0.0]$, tolerance $\\epsilon = 0.5$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each element corresponding to one test case in order. For example, the output format must be exactly of the form\n$[[r_1,[E_{0,1},\\dots,E_{k_1,1}],\\text{ok}_1],[r_2,[E_{0,2},\\dots,E_{k_2,2}],\\text{ok}_2],\\dots]$.",
            "solution": "The problem requires the implementation of an algorithm based on Proper Orthogonal Decomposition (POD) to determine the optimal reduced-order model rank for a given energy tolerance. The process involves analyzing a set of singular values derived from a snapshot matrix. The solution is structured into a sequence of well-defined computational steps for each test case.\n\nFirst, let the given list of singular values be denoted as a multiset $\\{\\sigma_i\\}_{i=1}^k$. As per the problem statement, these must be sorted in non-increasing order to properly represent the energy hierarchy of the POD modes. Let the sorted singular values be $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_k \\ge 0$. Here, $k$ is the total number of singular values.\n\nThe energetic content, or simply \"energy\", associated with the $i$-th POD mode is proportional to the square of its corresponding singular value, $\\sigma_i^2$. The total energy of the system captured in the snapshots is proportional to the sum of the energies of all modes, which is $E_{\\text{total}} = \\sum_{i=1}^{k} \\sigma_i^2$.\n\nThe core of the problem is to compute the cumulative energy retained by a rank-$r$ approximation, defined as:\n$$\nE_r = \\frac{\\sum_{i=1}^{r} \\sigma_i^2}{\\sum_{i=1}^{k} \\sigma_i^2} \\quad \\text{for } r \\in \\{1, 2, \\dots, k\\}\n$$\nBy convention, for a rank-$0$ model (which contains no modes), the retained energy is zero, so $E_0 = 0$. The sequence $\\{E_r\\}_{r=0}^k$ constitutes the cumulative energy retained curve. This sequence is, by definition, monotonically non-decreasing, starting at $E_0 = 0$ and ending at $E_k = 1$. A special case arises if the total energy is zero ($E_{\\text{total}} = 0$), which occurs if and only if all singular values are zero. In this scenario, the problem specifies that $E_r = 0$ for all $r \\in \\{0, 1, \\dots, k\\}$.\n\nThe algorithm proceeds as follows for each test case, which provides a list of singular values and a tolerance $\\epsilon \\in [0,1]$:\n\n1.  **Initialization and Sorting**: The input list of singular values is converted into a numerical array and sorted in non-increasing order to obtain the sequence $\\sigma_1, \\sigma_2, \\dots, \\sigma_k$.\n\n2.  **Energy Calculation**: The square of each singular value, $\\sigma_i^2$, is computed. The total energy, $E_{\\text{total}} = \\sum_{i=1}^{k} \\sigma_i^2$, is then calculated.\n\n3.  **Cumulative Energy Curve Construction**:\n    - If $E_{\\text{total}}$ is zero, the cumulative energy curve $\\{E_r\\}_{r=0}^k$ is a sequence of $k+1$ zeros.\n    - If $E_{\\text{total}}$ is positive, the partial sums of the squared singular values, $S_r = \\sum_{i=1}^{r} \\sigma_i^2$, are computed for $r=1, \\dots, k$. The cumulative energy for each rank is then $E_r = S_r / E_{\\text{total}}$. The final curve is the sequence $[E_0, E_1, \\dots, E_k]$.\n\n4.  **Rank Selection**: The primary task is to find the smallest integer rank $r \\in \\{0, 1, \\dots, k\\}$ such that the neglected energy fraction is no more than the specified tolerance $\\epsilon$. This condition is expressed by the inequality:\n    $$\n    1 - E_r \\le \\epsilon\n    $$\n    This is mathematically equivalent to finding the smallest $r$ that satisfies:\n    $$\n    E_r \\ge 1 - \\epsilon\n    $$\n    Since the sequence $\\{E_r\\}$ is monotonically non-decreasing, we can efficiently find the smallest $r$ that meets this criterion by searching through the sequence, starting from $r=0$. The search terminates at the first rank where the condition is met. For the special case where $E_{\\text{total}} = 0$, the problem defines the neglected energy fraction as $0$. The condition becomes $0 \\le \\epsilon$, which is always true for $\\epsilon \\in [0,1]$. Thus, the smallest rank satisfying the condition is $r=0$.\n\n5.  **Verification**: The problem requires a boolean value, $\\text{ok}$, confirming that the selected rank $r$ indeed satisfies the inequality $1 - E_r \\le \\epsilon$. By the construction of our rank selection method, this condition will always be satisfied. This step serves as a formal verification of the result.\n\n6.  **Output Assembly**: For each test case, the final result is assembled into a list containing three elements: the selected rank $r$, the complete cumulative energy curve $[E_0, E_1, \\dots, E_k]$, and the boolean verification flag $\\text{ok}$.\n\nThis procedure provides a complete and deterministic solution to the problem, correctly handling the specified definitions, conventions, and edge cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef process_case(sigma_values, epsilon):\n    \"\"\"\n    Processes a single test case of singular values and a tolerance.\n\n    Args:\n        sigma_values (list): A list of non-negative real numbers representing singular values.\n        epsilon (float): The tolerance for neglected energy, in the range [0, 1].\n\n    Returns:\n        list: A list of the form [r, E_curve, ok], where r is the selected rank,\n              E_curve is the cumulative energy list, and ok is the verification boolean.\n    \"\"\"\n    # Use float64 for better precision in calculations.\n    sv = np.array(sigma_values, dtype=np.float64)\n\n    # 1. Sort singular values in non-increasing order.\n    sv_sorted = np.sort(sv)[::-1]\n    k = len(sv_sorted)\n\n    # 2. Calculate squared values and total energy.\n    sv_squared = np.square(sv_sorted)\n    total_energy = np.sum(sv_squared)\n\n    # 3. Handle the zero total energy edge case.\n    if np.isclose(total_energy, 0.0):\n        E_curve = [0.0] * (k + 1)\n        # As per the problem: neglected energy fraction is defined as 0.\n        # The condition is 0 <= epsilon, which is always true for valid epsilon.\n        # The smallest rank r is therefore 0.\n        r = 0\n        ok = True  # The condition 0 <= epsilon holds.\n        return [r, E_curve, ok]\n\n    # 4. Calculate the cumulative energy retained curve {E_r}.\n    cumulative_energies = np.cumsum(sv_squared) / total_energy\n    # Prepend E_0 = 0 to form the full curve {E_r}_{r=0 to k}.\n    E_curve = np.insert(cumulative_energies, 0, 0.0)\n\n    # 5. Find the smallest rank r such that 1 - E_r <= epsilon,\n    # which is equivalent to E_r >= 1 - epsilon.\n    threshold = 1.0 - epsilon\n\n    # Since E_curve is monotonically non-decreasing, we can find the\n    # first index where the condition is met. np.searchsorted is efficient for this.\n    # It finds the first index `i` such that E_curve[i] >= threshold.\n    r = np.searchsorted(E_curve, threshold, side='left')\n\n    # The rank r must be at most k. This is guaranteed because E_curve[k] = 1.0\n    # and the threshold is at most 1.0.\n\n    # 6. Verify that the selected rank r satisfies the condition.\n    # By construction of the search for r, this should always be true.\n    neglected_energy = 1.0 - E_curve[r]\n    ok = neglected_energy <= epsilon\n    \n    # Python's floating-point precision can sometimes make `ok` False if\n    # `neglected_energy` is infinitesimally larger than `epsilon`.\n    # To be robust, one might use `np.isclose` or a small tolerance.\n    # However, the problem tests sensitivity, so direct comparison is used.\n    # Re-checking with a close-to-equal check to handle precision artifacts.\n    if not ok and np.isclose(neglected_energy, epsilon):\n        ok = True\n\n    return [int(r), E_curve.tolist(), ok]\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1: Typical decay\n        ([4.0, 2.0, 1.0, 0.5, 0.25], 0.05),\n        # Test case 2: Rank-deficient, zero tolerance\n        ([3.0, 0.0, 0.0], 0.0),\n        # Test case 3: Unordered input\n        ([0.5, 2.0, 1.0], 0.2),\n        # Test case 4: Tolerance sensitivity 1\n        ([10.0, 9.9, 0.1, 0.1, 0.1], 0.0002),\n        # Test case 5: Tolerance sensitivity 2\n        ([10.0, 9.9, 0.1, 0.1, 0.1], 0.00015),\n        # Test case 6: Zero energy\n        ([0.0, 0.0], 0.5),\n    ]\n\n    all_results = []\n    for sv, eps in test_cases:\n        result = process_case(sv, eps)\n        all_results.append(result)\n\n    # Format the final output string to match the required format exactly.\n    # [[r1,[E...],ok1],[r2,[E...],ok2],...]\n    # Example format: [3,[0.0,0.75...,1.0],true]\n    result_strings = []\n    for res in all_results:\n        r, E_curve, ok = res\n        E_curve_str = f\"[{','.join(map(str, E_curve))}]\"\n        ok_str = str(ok).lower()\n        result_strings.append(f\"[{r},{E_curve_str},{ok_str}]\")\n    \n    final_output = f\"[{','.join(result_strings)}]\"\n    \n    # Final print statement in the exact required format.\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "Having mastered the art of rank selection, we now apply these concepts to a practical engineering problem: modeling heat transfer. This exercise () demonstrates how to build a single reduced basis from solutions corresponding to a few training parameters ($\\{\\kappa_i\\}$) that can accurately represent the system's behavior for new, unseen parameters. You will see firsthand how POD excels for diffusion-dominated problems, creating a compact and generalizable model from a small set of high-fidelity simulations.",
            "id": "3184782",
            "problem": "You are asked to implement a complete, runnable program that constructs a Proper Orthogonal Decomposition (POD) basis from training snapshots of the one-dimensional heat equation and then evaluates how the orthogonal projection error decays as the number of retained POD modes increases for several unseen thermal diffusivity parameters within a specified range. All quantities must be treated in purely mathematical terms, without physical units.\n\nConsider the one-dimensional heat equation with zero Dirichlet boundary conditions on a unit interval:\n- Governing law: $u_t = \\kappa\\,u_{xx}$ for $x \\in [0,1]$ and $t \\ge 0$.\n- Boundary conditions: $u(0,t) = 0$ and $u(1,t) = 0$ for all $t \\ge 0$.\n- Initial condition: $u(x,0) = \\sum_{n=1}^{8} a_n \\sin(n\\pi x)$, where the coefficients are given by $a_1=1.0$, $a_2=0.5$, $a_3=-0.3$, $a_4=0.2$, $a_5=0.1$, $a_6=-0.05$, $a_7=0.04$, $a_8=-0.02$.\n\nFundamental basis for solution construction: It is a well-tested fact that the solution to the above initial-boundary-value problem, under these boundary conditions, admits a Fourier sine series representation with time-dependent modal amplitudes that decay exponentially in time. You must use this fact to compute snapshots analytically, without numerically discretizing the partial differential equation.\n\nDefinitions and required computations:\n- Define the analytical solution as $u(x,t;\\kappa) = \\sum_{n=1}^{8} a_n \\exp\\!\\big(-\\kappa (n\\pi)^2 t\\big)\\,\\sin(n\\pi x)$.\n- Let the spatial grid be uniform on $[0,1]$ with $N_x = 200$ points, including the endpoints.\n- Let the training parameter values be $\\kappa_{\\text{train}} \\in \\{0.06, 0.14, 0.22\\}$ and the training snapshot times be $t \\in \\{0.00, 0.05, 0.10, 0.20, 0.50\\}$.\n- Construct the training snapshot matrix $M_{\\text{train}} \\in \\mathbb{R}^{N_x \\times N_{\\text{snap}}}$ by concatenating, as columns, all spatial snapshots $u(\\cdot, t; \\kappa)$ for every $t$ in the specified set and every $\\kappa$ in the specified training set, in any fixed deterministic order.\n- Compute a POD basis as the columns of $U$ obtained from the Singular Value Decomposition (SVD) $M_{\\text{train}} = U \\Sigma V^\\top$, where $U \\in \\mathbb{R}^{N_x \\times r_{\\max}}$ has orthonormal columns and $r_{\\max} \\le \\min(N_x, N_{\\text{snap}})$. Use these $U$ columns in order as the POD modes; do not subtract a mean (i.e., perform POD directly on $M_{\\text{train}}$).\n- For evaluation, use unseen test parameters $\\kappa_{\\text{test}} \\in \\{0.05, 0.10, 0.17, 0.25\\}$ and the same time set $t \\in \\{0.00, 0.05, 0.10, 0.20, 0.50\\}$ to build $M_{\\text{test}}(\\kappa) \\in \\mathbb{R}^{N_x \\times N_{\\text{test}}}$ for each test parameter.\n- For a given number of retained modes $r$, define the orthogonal projection operator $P_r = U_r U_r^\\top$, where $U_r \\in \\mathbb{R}^{N_x \\times r}$ contains the first $r$ POD modes. For each test parameter $\\kappa$, compute the relative projection error\n$$\nE(\\kappa,r) = \\frac{\\lVert M_{\\text{test}}(\\kappa) - P_r M_{\\text{test}}(\\kappa) \\rVert_F}{\\lVert M_{\\text{test}}(\\kappa) \\rVert_F},\n$$\nwhere $\\lVert \\cdot \\rVert_F$ denotes the Frobenius norm. Take $E(\\kappa,0)$ to be the relative error of the zero approximation (i.e., $P_0 = 0$).\n- Evaluate $E(\\kappa,r)$ for $r \\in \\{0,1,2,3,5,8,10\\}$ and for each $\\kappa \\in \\{0.05, 0.10, 0.17, 0.25\\}$.\n\nTest suite and coverage rationale:\n- The choice $r=0$ tests the boundary case of no retained modes.\n- The choice $r=10$ exceeds the number of nonzero initial Fourier components, probing the expected saturation to zero error.\n- The test parameters include both interval endpoints $0.05$ and $0.25$ and interior values $0.10$ and $0.17$, none of which are used in training, thus exercising generalization across the parameter range.\n\nFinal output requirements:\n- Your program should produce a single line of output containing the results as a nested list of floats with no spaces. The outer list must be ordered by the test parameters in ascending order $\\kappa \\in [0.05, 0.10, 0.17, 0.25]$. For each test parameter, the inner list must contain the errors $[E(\\kappa,0), E(\\kappa,1), E(\\kappa,2), E(\\kappa,3), E(\\kappa,5), E(\\kappa,8), E(\\kappa,10)]$ in this exact $r$ order. Express each float rounded to eight decimal places. For example, the printed shape must look like $[[e_{11},e_{12},\\dots],[e_{21},\\dots],\\dots]$ with numeric entries in place of the $e_{ij}$.",
            "solution": "The problem requires the implementation of a Proper Orthogonal Decomposition (POD) based reduced-order model for the one-dimensional heat equation. The process involves generating training data, constructing a low-dimensional basis, and evaluating the basis's effectiveness on unseen test data. The entire procedure is based on an analytical solution, avoiding discretization errors of the governing partial differential equation (PDE).\n\nStep 1: Analytical Solution and Data Generation\n\nThe problem concerns the heat equation $u_t = \\kappa\\,u_{xx}$ on the domain $x \\in [0,1]$ with zero Dirichlet boundary conditions, $u(0,t) = u(1,t) = 0$. The initial condition is a finite Fourier series, $u(x,0) = \\sum_{n=1}^{8} a_n \\sin(n\\pi x)$, with given coefficients $\\{a_n\\}$. The analytical solution, which is derived from the method of separation of variables, is provided as:\n$$\nu(x,t;\\kappa) = \\sum_{n=1}^{8} a_n \\exp\\!\\big(-\\kappa (n\\pi)^2 t\\big)\\,\\sin(n\\pi x)\n$$\nThis formula allows us to generate \"snapshots\" of the solution field, which are vectors representing the temperature distribution $u(\\cdot, t; \\kappa)$ at a specific time $t$ and for a given thermal diffusivity $\\kappa$. These snapshots are evaluated on a uniform spatial grid of $N_x = 200$ points on $[0,1]$.\n\nStep 2: Training Snapshot Matrix Construction\n\nTo build a reduced-order model, we first collect a set of representative solutions. The training data is generated using the training parameter set $\\kappa_{\\text{train}} \\in \\{0.06, 0.14, 0.22\\}$ and snapshot times $t \\in \\{0.00, 0.05, 0.10, 0.20, 0.50\\}$. For each pair $(\\kappa, t)$ in the training set, we compute the corresponding solution snapshot vector. These snapshot vectors are then concatenated as columns to form the training snapshot matrix $M_{\\text{train}} \\in \\mathbb{R}^{N_x \\times N_{\\text{snap}}}$. Here, $N_x = 200$ and the total number of snapshots is $N_{\\text{snap}} = |\\kappa_{\\text{train}}| \\times |t| = 3 \\times 5 = 15$.\n\nStep 3: Proper Orthogonal Decomposition (POD)\n\nThe core of POD is to find an orthonormal basis that is optimal for representing the training data in a least-squares sense. This is achieved by performing a Singular Value Decomposition (SVD) on the snapshot matrix:\n$$\nM_{\\text{train}} = U \\Sigma V^\\top\n$$\nThe columns of the matrix $U \\in \\mathbb{R}^{N_x \\times N_{\\text{snap}}}$ (from a thin SVD, since $N_x > N_{\\text{snap}}$) are the left singular vectors. These vectors form the POD basis. They are ordered by the magnitude of their corresponding singular values in the diagonal matrix $\\Sigma \\in \\mathbb{R}^{N_{\\text{snap}} \\times N_{\\text{snap}}}$, with the first column of $U$ being the most energetic mode.\nAn important observation is that all snapshots, for any $t$ and $\\kappa$, lie within the $8$-dimensional subspace spanned by the functions $\\{\\sin(n\\pi x)\\}_{n=1}^8$. Consequently, the rank of the snapshot matrix $M_{\\text{train}}$ cannot exceed $8$. This implies that only the first $8$ singular values will be significantly non-zero, and the POD basis will effectively have a dimension of $8$.\n\nStep 4: Projection and Error Evaluation\n\nThe POD basis allows for the approximation of any solution snapshot (both within and outside the training set) by its orthogonal projection onto the subspace spanned by the first $r$ POD modes. We denote the truncated basis matrix as $U_r \\in \\mathbb{R}^{N_x \\times r}$, which contains the first $r$ columns of $U$. The orthogonal projection operator onto this subspace is $P_r = U_r U_r^\\top$.\n\nTo evaluate the quality of the basis, we generate test data for a set of unseen parameters $\\kappa_{\\text{test}} \\in \\{0.05, 0.10, 0.17, 0.25\\}$ using the same set of time instances. For each $\\kappa \\in \\kappa_{\\text{test}}$, we form a test snapshot matrix $M_{\\text{test}}(\\kappa)$. The approximation of this test matrix using $r$ modes is given by $P_r M_{\\text{test}}(\\kappa)$. The error of this approximation is quantified using the relative Frobenius norm:\n$$\nE(\\kappa,r) = \\frac{\\lVert M_{\\text{test}}(\\kappa) - P_r M_{\\text{test}}(\\kappa) \\rVert_F}{\\lVert M_{\\text{test}}(\\kappa) \\rVert_F}\n$$\nFor computational efficiency, the projection $P_r M_{\\text{test}}(\\kappa)$ is calculated as $U_r (U_r^\\top M_{\\text{test}}(\\kappa))$. The case $r=0$ corresponds to approximating the solution by zero, yielding an error $E(\\kappa,0)=1$. We compute this error for each $\\kappa \\in \\kappa_{\\text{test}}$ and for a specified set of retained modes $r \\in \\{0, 1, 2, 3, 5, 8, 10\\}$. The results are aggregated into a nested list for the final output. Since the underlying solution space is $8$-dimensional, we expect the error to drop to nearly zero for $r \\ge 8$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import linalg\n\ndef solve():\n    \"\"\"\n    Constructs a POD basis for the 1D heat equation and evaluates projection error.\n    \"\"\"\n    # Define problem parameters\n    A_COEFFS = np.array([1.0, 0.5, -0.3, 0.2, 0.1, -0.05, 0.04, -0.02])\n    NX = 200\n    KAPPA_TRAIN = [0.06, 0.14, 0.22]\n    T_POINTS = [0.00, 0.05, 0.10, 0.20, 0.50]\n    KAPPA_TEST = [0.05, 0.10, 0.17, 0.25]\n    R_VALUES = [0, 1, 2, 3, 5, 8, 10]\n\n    # Spatial grid\n    x = np.linspace(0, 1, NX)\n\n    def analytical_u(x_grid, t, kappa, coeffs):\n        \"\"\"Computes the analytical solution u(x, t; kappa).\"\"\"\n        sol = np.zeros_like(x_grid)\n        for n_idx, a_n in enumerate(coeffs):\n            n = n_idx + 1\n            mode_component = a_n * np.exp(-kappa * (n * np.pi)**2 * t) * np.sin(n * np.pi * x_grid)\n            sol += mode_component\n        return sol\n\n    # --- Step 1: Construct Training Snapshot Matrix ---\n    train_snapshots = []\n    for kappa_train_val in KAPPA_TRAIN:\n        for t_val in T_POINTS:\n            snapshot = analytical_u(x, t_val, kappa_train_val, A_COEFFS)\n            train_snapshots.append(snapshot)\n    \n    M_train = np.stack(train_snapshots, axis=1)\n\n    # --- Step 2: Compute POD Basis via SVD ---\n    # Use thin SVD since Nx > N_snap\n    U, s, vh = linalg.svd(M_train, full_matrices=False)\n\n    # --- Step 3: Evaluate Projection Error for Test Cases ---\n    all_results = []\n    for kappa_test_val in KAPPA_TEST:\n        # Build the test snapshot matrix for the current kappa\n        test_snapshots = []\n        for t_val in T_POINTS:\n            snapshot = analytical_u(x, t_val, kappa_test_val, A_COEFFS)\n            test_snapshots.append(snapshot)\n        \n        M_test = np.stack(test_snapshots, axis=1)\n\n        norm_M_test = linalg.norm(M_test, 'fro')\n        \n        kappa_errors = []\n        for r in R_VALUES:\n            if norm_M_test == 0:\n                 # Should not happen in this problem, but for robustness\n                 # If M_test is zero, projection is perfect (error 0), except for r=0\n                 error = 1.0 if r == 0 else 0.0\n            elif r == 0:\n                # Per problem spec, P_0 = 0. Error is ||M - 0|| / ||M|| = 1.\n                error = 1.0\n            else:\n                # Truncate the POD basis\n                Ur = U[:, :r]\n                \n                # Project M_test onto the r-dimensional subspace\n                # P_r * M_test = Ur * Ur.T * M_test\n                projected_M_test = Ur @ (Ur.T @ M_test)\n                \n                # Compute the relative Frobenius norm of the error\n                error_matrix = M_test - projected_M_test\n                norm_error = linalg.norm(error_matrix, 'fro')\n                error = norm_error / norm_M_test\n\n            kappa_errors.append(error)\n        \n        all_results.append(kappa_errors)\n        \n    # --- Step 4: Format output string ---\n    outer_parts = []\n    for inner_list in all_results:\n        inner_parts = [f\"{x:.8f}\" for x in inner_list]\n        outer_parts.append(f\"[{','.join(inner_parts)}]\")\n    final_string = f\"[{','.join(outer_parts)}]\"\n\n    print(final_string)\n\nsolve()\n```"
        },
        {
            "introduction": "A low reconstruction error is a necessary, but not sufficient, condition for a good reduced-order model. The ultimate test is whether the ROM can accurately predict the system's evolution in time. This advanced practice () confronts a critical challenge where a perfectly stable high-fidelity system can produce an unstable, diverging ROM upon Galerkin projection, a phenomenon often linked to non-normal system dynamics. By working through this example, you will gain a crucial understanding of why ROM stability must be carefully assessed and cannot be taken for granted.",
            "id": "2432128",
            "problem": "You are asked to implement a complete numerical experiment in reduced-order modeling that demonstrates the following phenomenon: a Proper Orthogonal Decomposition (POD) basis can be excellent for reconstructing training snapshots of a stable full-order linear time-invariant system, yet the Galerkin-projected reduced-order model (ROM) can produce unstable dynamics that blow up when integrated in time.\n\nYour implementation must start from the full-order ordinary differential equation\n$$\n\\frac{d\\mathbf{x}}{dt} = \\mathbf{A}\\mathbf{x} + \\mathbf{b},\n$$\nwhere $\\mathbf{A}\\in\\mathbb{R}^{n\\times n}$ and $\\mathbf{b}\\in\\mathbb{R}^{n}$ are constant, and $\\mathbf{x}(t)\\in\\mathbb{R}^{n}$ is the state. All computations are over the real numbers with the standard Euclidean inner product. You will use $n=2$ throughout.\n\nFundamental definitions and requirements:\n- Proper Orthogonal Decomposition (POD) basis: Given a snapshot matrix\n$$\n\\mathbf{X} = \\begin{bmatrix}\\mathbf{x}(t_1) & \\mathbf{x}(t_2) & \\cdots & \\mathbf{x}(t_m)\\end{bmatrix}\\in\\mathbb{R}^{n\\times m},\n$$\ncompute its singular value decomposition (SVD) $\\mathbf{X}=\\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V}^\\top$. The rank-$r$ POD basis $\\mathbf{Q}\\in\\mathbb{R}^{n\\times r}$ is taken as the first $r$ columns of $\\mathbf{U}$.\n- Galerkin projection: The reduced operator and reduced forcing are\n$$\n\\mathbf{A}_r = \\mathbf{Q}^\\top\\mathbf{A}\\mathbf{Q}\\in\\mathbb{R}^{r\\times r},\\qquad \\mathbf{b}_r=\\mathbf{Q}^\\top\\mathbf{b}\\in\\mathbb{R}^{r}.\n$$\nThe reduced state $\\mathbf{z}(t)\\in\\mathbb{R}^{r}$ evolves as\n$$\n\\frac{d\\mathbf{z}}{dt} = \\mathbf{A}_r \\mathbf{z} + \\mathbf{b}_r,\\qquad \\mathbf{x}_r(t)=\\mathbf{Q}\\mathbf{z}(t).\n$$\n- Time integration: Use the classical fourth-order Runge–Kutta method with a fixed time step $h>0$ for both the full-order model and the ROM. Set the initial condition to $\\mathbf{x}(0)=\\mathbf{0}$ and $\\mathbf{z}(0)=\\mathbf{Q}^\\top\\mathbf{x}(0)=\\mathbf{0}$.\n- Snapshot collection: Integrate the full-order model over a training horizon $[0,T_{\\text{train}}]$ with a constant time step $h$, sampling the state at every step to form $\\mathbf{X}$.\n- Reconstruction error: Measure the relative POD reconstruction error of the training snapshots as\n$$\n\\varepsilon_{\\text{rec}} = \\frac{\\lVert \\mathbf{X} - \\mathbf{Q}\\mathbf{Q}^\\top \\mathbf{X}\\rVert_F}{\\lVert \\mathbf{X}\\rVert_F},\n$$\nwhere $\\lVert\\cdot\\rVert_F$ denotes the Frobenius norm.\n- Blow-up detection: Evolve both the full-order model and the ROM over a test horizon $[0,T_{\\text{test}}]$ with the same $h$. Declare a solution “blown up” if at any time step the Euclidean norm of the current state exceeds a threshold $M$, or if any component becomes not-a-number or infinite. Use the threshold $M=10^6$.\n\nConstructed forcing to target instability under ROM:\n- For each test, you must construct the constant forcing $\\mathbf{b}$ as follows. Compute the symmetric part $\\mathbf{S}=\\frac{1}{2}(\\mathbf{A}+\\mathbf{A}^\\top)$ and its dominant unit eigenvector $\\mathbf{q}\\in\\mathbb{R}^{n}$ associated with the largest eigenvalue of $\\mathbf{S}$ (break ties arbitrarily but deterministically). Set\n$$\n\\mathbf{b}=-\\mathbf{A}\\mathbf{q}.\n$$\nThis construction ensures that the full-order steady state is $\\mathbf{x}_\\infty = -\\mathbf{A}^{-1}\\mathbf{b}=\\mathbf{q}$. When $\\mathbf{A}$ is highly non-normal and the largest eigenvalue of $\\mathbf{S}$ is positive, the scalar ROM obtained with $r=1$ and $\\mathbf{Q}=\\mathbf{q}$ has reduced dynamics $\\frac{dz}{dt} = a_r z + b_r$ with $a_r=\\mathbf{q}^\\top\\mathbf{A}\\mathbf{q}>0$ and $b_r=-a_r$, which is unstable and diverges from $z(0)=0$.\n\nNumerical specification common to all tests:\n- Use $n=2$.\n- Use $h=10^{-3}$.\n- Use classical fourth-order Runge–Kutta.\n- Use the Euclidean norm for all vector norms.\n- Use $\\mathbf{x}(0)=\\mathbf{0}$.\n\nTest suite:\nImplement the above for the following parameter sets. In each case, define $\\mathbf{A}$, compute $\\mathbf{q}$ and $\\mathbf{b}$ as specified, collect snapshots over $[0,T_{\\text{train}}]$ to form $\\mathbf{Q}$, then form the ROM and run both models over $[0,T_{\\text{test}}]$.\n\n- Test $1$ (highly non-normal, rank-$1$ POD):\n  - $\\mathbf{A}=\\begin{bmatrix}-0.1 & \\alpha \\\\ 0 & -1.0\\end{bmatrix}$ with $\\alpha=50.0$,\n  - $r=1$,\n  - $T_{\\text{train}}=4.0$,\n  - $T_{\\text{test}}=1.0$.\n- Test $2$ (highly non-normal, rank-$2$ POD):\n  - $\\mathbf{A}=\\begin{bmatrix}-0.1 & \\alpha \\\\ 0 & -1.0\\end{bmatrix}$ with $\\alpha=50.0$,\n  - $r=2$,\n  - $T_{\\text{train}}=4.0$,\n  - $T_{\\text{test}}=1.0$.\n- Test $3$ (symmetric negative definite, rank-$1$ POD):\n  - $\\mathbf{A}=\\begin{bmatrix}-1.0 & 0.0 \\\\ 0.0 & -2.0\\end{bmatrix}$,\n  - $r=1$,\n  - $T_{\\text{train}}=4.0$,\n  - $T_{\\text{test}}=1.0$.\n- Test $4$ (more highly non-normal, rank-$1$ POD):\n  - $\\mathbf{A}=\\begin{bmatrix}-0.1 & \\alpha \\\\ 0 & -1.0\\end{bmatrix}$ with $\\alpha=120.0$,\n  - $r=1$,\n  - $T_{\\text{train}}=4.0$,\n  - $T_{\\text{test}}=1.0$.\n\nRequired outputs:\n- For each test, output a list of three entries:\n  - the scalar $\\varepsilon_{\\text{rec}}$ rounded to six decimal places,\n  - a boolean indicating whether the ROM blew up on $[0,T_{\\text{test}}]$,\n  - a boolean indicating whether the full-order model blew up on $[0,T_{\\text{test}}]$.\n- Aggregate the results from all tests into a single line as a comma-separated list enclosed in square brackets, in the same order as the tests. Example format:\n$[\\,[\\varepsilon_{\\text{rec}}^{(1)},\\,\\text{ROM}^{(1)}\\_\\text{blowup},\\,\\text{FOM}^{(1)}\\_\\text{blowup}],\\,[\\varepsilon_{\\text{rec}}^{(2)},\\,\\text{ROM}^{(2)}\\_\\text{blowup},\\,\\text{FOM}^{(2)}\\_\\text{blowup}],\\,\\ldots\\,]$.",
            "solution": "The problem requires a numerical implementation to demonstrate a critical potential pitfall in reduced-order modeling: the creation of an unstable reduced-order model (ROM) from a perfectly stable full-order model (FOM). The experiment is designed around a linear time-invariant (LTI) system, where stability is linked to the properties of the system matrix $\\mathbf{A}$.\n\nThe core of the phenomenon lies in the non-normal nature of the matrix $\\mathbf{A}$. While the stability of the FOM, $\\frac{d\\mathbf{x}}{dt} = \\mathbf{A}\\mathbf{x} + \\mathbf{b}$, is governed by the eigenvalues of $\\mathbf{A}$ (all of which have negative real parts in the specified test cases), the stability of the Galerkin-projected ROM is governed by the eigenvalues of the reduced matrix $\\mathbf{A}_r = \\mathbf{Q}^\\top\\mathbf{A}\\mathbf{Q}$. For a non-normal matrix $\\mathbf{A}$, it is possible for $\\mathbf{A}_r$ to have eigenvalues with positive real parts, leading to instability, even if the FOM is stable. This often occurs when the projection basis $\\mathbf{Q}$ captures directions of transient energy growth in the FOM.\n\nThe implementation follows these key steps for each test case:\n\n1.  **System and Forcing Term Setup**: The matrix $\\mathbf{A}$ is defined for the specific test. A special forcing vector $\\mathbf{b}$ is constructed to steer the FOM dynamics toward the direction of maximum transient growth. This is done by finding the eigenvector $\\mathbf{q}$ associated with the largest eigenvalue of the symmetric part of $\\mathbf{A}$, $\\mathbf{S}=\\frac{1}{2}(\\mathbf{A}+\\mathbf{A}^\\top)$, and setting $\\mathbf{b}=-\\mathbf{A}\\mathbf{q}$. This ensures the FOM's steady state is precisely this direction of interest.\n\n2.  **Snapshot Generation**: The FOM is integrated over a training time horizon $[0, T_{\\text{train}}]$ starting from $\\mathbf{x}(0)=\\mathbf{0}$, using the classical fourth-order Runge-Kutta (RK4) method. The state vector at each time step is stored as a column in the snapshot matrix $\\mathbf{X}$.\n\n3.  **POD Basis and ROM Construction**: A Singular Value Decomposition (SVD) is performed on the snapshot matrix $\\mathbf{X}$. The first $r$ left singular vectors are selected to form the orthonormal POD basis $\\mathbf{Q} \\in \\mathbb{R}^{n \\times r}$. The Galerkin projection is then applied to obtain the reduced system matrices: $\\mathbf{A}_r = \\mathbf{Q}^\\top\\mathbf{A}\\mathbf{Q}$ and $\\mathbf{b}_r = \\mathbf{Q}^\\top\\mathbf{b}$.\n\n4.  **Reconstruction Error**: The quality of the POD basis for representing the training data is quantified by the relative reconstruction error $\\varepsilon_{\\text{rec}} = \\lVert \\mathbf{X} - \\mathbf{Q}\\mathbf{Q}^\\top \\mathbf{X}\\rVert_F / \\lVert \\mathbf{X}\\rVert_F$.\n\n5.  **Comparative Simulation**: Both the FOM and the rank-$r$ ROM are simulated over a test horizon $[0, T_{\\text{test}}]$, again using the RK4 method and starting from zero initial conditions. During this simulation, the norm of the state vector for each model is monitored at every time step. If the norm exceeds a large threshold $M=10^6$, the model is flagged as having \"blown up\".\n\n6.  **Analysis of Test Cases**:\n    - **Non-normal cases (Tests 1, 4)**: With $r=1$, the POD basis is expected to align with the direction of transient growth, causing the scalar reduced operator $A_r$ to be positive and leading to an unstable ROM.\n    - **Full-rank case (Test 2)**: With $r=n=2$, the POD basis is complete. The ROM is dynamically identical to the FOM and therefore stable. The reconstruction error is zero.\n    - **Symmetric case (Test 3)**: For a normal (symmetric) matrix $\\mathbf{A}$, the reduced operator $\\mathbf{A}_r$ is guaranteed to be stable. No blow-up is expected.\n\nBy executing these steps, the program will produce the required outputs ($\\varepsilon_{\\text{rec}}$, ROM blow-up status, FOM blow-up status) for each test case, empirically demonstrating the conditions under which a projection-based ROM can fail to preserve the stability of the original system.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import eigh\n\ndef solve():\n    \"\"\"\n    Implements the full numerical experiment to demonstrate ROM instability\n    for a stable FOM.\n    \"\"\"\n\n    def rk4_step(f, y, h, A, b):\n        \"\"\"A single step of the classical fourth-order Runge-Kutta method.\"\"\"\n        k1 = f(y, A, b)\n        k2 = f(y + h / 2 * k1, A, b)\n        k3 = f(y + h / 2 * k2, A, b)\n        k4 = f(y + h * k3, A, b)\n        return y + h / 6 * (k1 + 2 * k2 + 2 * k3 + k4)\n\n    def lti_rhs(y, A, b):\n        \"\"\"RHS of the LTI system dy/dt = Ay + b.\"\"\"\n        return A @ y + b\n\n    def simulate(A, b, y0, T, h, M):\n        \"\"\"\n        Simulates an LTI system and returns snapshots and blow-up status.\n        \"\"\"\n        num_steps = int(T / h)\n        y = y0.copy()\n        snapshots = [y0.copy()]\n        blew_up = False\n        \n        for _ in range(num_steps):\n            y = rk4_step(lti_rhs, y, h, A, b)\n            if np.linalg.norm(y) > M or not np.all(np.isfinite(y)):\n                blew_up = True\n                # Continue collecting snapshots to see the blow-up, if needed for X\n                # But stop checking once blown up.\n                while len(snapshots) < num_steps + 1:\n                    snapshots.append(y.copy()) # Append the diverging state\n                    y = rk4_step(lti_rhs, y, h, A, b) # Could become inf/nan\n                return np.array(snapshots).T, True\n\n            snapshots.append(y.copy())\n            \n        return np.array(snapshots).T, blew_up\n\n    # General parameters\n    n = 2\n    h = 1e-3\n    M = 1e6\n    x0 = np.zeros(n)\n\n    # Test cases from the problem statement.\n    test_cases = [\n        # (A_params, r, T_train, T_test)\n        ({\"alpha\": 50.0}, 1, 4.0, 1.0),\n        ({\"alpha\": 50.0}, 2, 4.0, 1.0),\n        ({\"alpha\": None}, 1, 4.0, 1.0), # Symmetric case\n        ({\"alpha\": 120.0}, 1, 4.0, 1.0),\n    ]\n\n    results = []\n    \n    for i, (params, r, T_train, T_test) in enumerate(test_cases):\n        # 1. Define A\n        if i == 2: # Test 3: Symmetric case\n            A = np.array([[-1.0, 0.0], [0.0, -2.0]])\n        else: # Tests 1, 2, 4: Non-normal case\n            alpha = params[\"alpha\"]\n            A = np.array([[-0.1, alpha], [0.0, -1.0]])\n\n        # 2. Construct b\n        S = 0.5 * (A + A.T)\n        eigvals, eigvecs = eigh(S)\n        q = eigvecs[:, -1] # Dominant eigenvector (eigh sorts eigenvalues)\n        b = -A @ q\n\n        # 3. Generate FOM snapshots for training\n        X, _ = simulate(A, b, x0, T_train, h, M)\n\n        # 4. Compute POD basis Q\n        U, s, _ = np.linalg.svd(X, full_matrices=False)\n        Q = U[:, :r]\n\n        # 5. Calculate reconstruction error\n        # eps_rec = norm(X - Q @ Q.T @ X) / norm(X)\n        # Using singular values is more direct: sqrt(sum(s_i^2 for i>r)) / sqrt(sum(s_i^2))\n        if X.shape[1]>1:\n         norm_X_sq = np.sum(s**2)\n         if norm_X_sq > 0:\n            norm_err_sq = np.sum(s[r:]**2)\n            eps_rec = np.sqrt(norm_err_sq / norm_X_sq)\n         else:\n            eps_rec = 0.0\n        else:\n            eps_rec = 0.0\n\n\n        # 6. Form the ROM\n        Ar = Q.T @ A @ Q\n        br = Q.T @ b\n        z0 = np.zeros(r)\n\n        # 7. Simulate FOM and ROM for testing, check blow-up\n        _, fom_blew_up = simulate(A, b, x0, T_test, h, M)\n        _, rom_blew_up = simulate(Ar, br, z0, T_test, h, M)\n\n        # 8. Record results\n        results.append([round(eps_rec, 6), rom_blew_up, fom_blew_up])\n\n    # Final print statement in the exact required format.\n    # Convert bools to lowercase 'true'/'false' for JS-like format\n    formatted_results = []\n    for res in results:\n        eps_str = f\"{res[0]:.6f}\"\n        rom_bool_str = str(res[1]).lower()\n        fom_bool_str = str(res[2]).lower()\n        formatted_results.append(f\"[{eps_str},{rom_bool_str},{fom_bool_str}]\")\n\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}