{
    "hands_on_practices": [
        {
            "introduction": "The performance of parallel CFD solvers is often limited by inter-process communication, with halo exchange being the dominant pattern in structured-grid methods. This cost can be effectively estimated using a latency-bandwidth model, which is a cornerstone of performance analysis. This exercise  provides foundational practice in building such a quantitative model, allowing you to derive the total communication time and understand how it scales with the problem size, processor topology, and network characteristics.",
            "id": "3983349",
            "problem": "Consider a three-dimensional structured finite-volume Computational Fluid Dynamics (CFD) solver of the compressible Navier–Stokes equations on a uniform Cartesian mesh with $N_{x} \\times N_{y} \\times N_{z}$ control volumes. The parallelization uses a block domain decomposition across $P$ Message Passing Interface (MPI) ranks, arranged as a logical processor grid of size $P_{x} \\times P_{y} \\times P_{z}$ with $P = P_{x} P_{y} P_{z}$, and assume $N_{x}$, $N_{y}$, and $N_{z}$ are divisible by $P_{x}$, $P_{y}$, and $P_{z}$, respectively. Each rank owns a subdomain of size $n_{x} \\times n_{y} \\times n_{z}$ cells, where $n_{x} = N_{x}/P_{x}$, $n_{y} = N_{y}/P_{y}$, and $n_{z} = N_{z}/P_{z}$. A stencil of width $w$ is used, meaning halo layers of thickness $w$ must be exchanged with face-adjacent neighbors along each of the three coordinate directions. Assume that:\n- Each halo exchange along a subdomain face is performed as a single message containing the $w$ layers of that face (no separate messages for edges or corners are sent).\n- A conservative state vector of $q$ scalar components per cell is exchanged for each halo cell.\n- The communication time for a single message is modeled by the standard latency–bandwidth relation $T_{\\text{msg}} = \\alpha + \\beta m$, where $\\alpha$ is the latency, $\\beta$ is the inverse bandwidth (time per scalar data item), and $m$ is the number of scalar data items in the message.\n- Only inter-rank exchanges are counted; physical boundary conditions do not incur MPI communications.\n\nUsing the model $T = \\sum_{\\text{faces}} \\left( \\alpha + \\beta m_{\\text{face}} \\right)$ over all directed inter-rank face messages in one complete halo exchange step (i.e., all three directions), derive a closed-form expression for the total halo exchange time $T_{\\text{total}}$ in terms of $N_{x}$, $N_{y}$, $N_{z}$, $P_{x}$, $P_{y}$, $P_{z}$, $w$, $q$, $\\alpha$, and $\\beta$. Express the final result in seconds as a single analytic expression. No numerical evaluation is required.",
            "solution": "We begin from the standard latency–bandwidth communication model for Message Passing Interface (MPI), which states that the time to send a message containing $m$ scalar data items is $T_{\\text{msg}} = \\alpha + \\beta m$, where $\\alpha$ is the latency and $\\beta$ is the time per scalar data item. For a structured block decomposition, halo exchange is executed between face-adjacent subdomains, sending $w$ layers of cells across each shared face in both directions.\n\nThe total halo exchange time $T_{\\text{total}}$ across the entire processor grid during one complete exchange over all three coordinate directions follows from summing the message time $T_{\\text{msg}}$ over all directed face messages. We must therefore determine:\n1. The number of directed inter-rank face messages per direction.\n2. The size $m_{\\text{face}}$ of each such message in terms of the mesh and decomposition parameters.\n\nFirst, consider the processor grid of size $P_{x} \\times P_{y} \\times P_{z}$. Along the $x$-direction, there are $P_{y} P_{z}$ processor lines, each containing $P_{x}$ ranks. The number of inter-rank interfaces along $x$ is the number of internal boundaries in these lines, which is $\\left( P_{x} - 1 \\right)$ per line. Thus, the total number of undirected interfaces along $x$ is $\\left( P_{x} - 1 \\right) P_{y} P_{z}$. Since halo exchanges are bidirectional (each interface generates a send in both directions), the number of directed messages along $x$ is $2 \\left( P_{x} - 1 \\right) P_{y} P_{z}$.\n\nAnalogous reasoning applies to the $y$- and $z$-directions, yielding $2 \\left( P_{y} - 1 \\right) P_{x} P_{z}$ and $2 \\left( P_{z} - 1 \\right) P_{x} P_{y}$ directed messages, respectively.\n\nSecond, we determine the message size $m_{\\text{face}}$ for a face in each direction. Each subdomain has dimensions $n_{x} \\times n_{y} \\times n_{z}$ cells, where $n_{x} = N_{x}/P_{x}$, $n_{y} = N_{y}/P_{y}$, and $n_{z} = N_{z}/P_{z}$. A face normal to the $x$-direction has area $n_{y} \\times n_{z}$ in cells, and $w$ layers of thickness in the $x$-direction are transferred as a single message. The number of halo cells contributed by this face per message is therefore $w \\, n_{y} \\, n_{z}$. Since each cell contains $q$ scalar components, the number of scalar items in the message is\n$$\nm_{x} = w \\, n_{y} \\, n_{z} \\, q = w \\, q \\, \\frac{N_{y}}{P_{y}} \\, \\frac{N_{z}}{P_{z}}.\n$$\nBy symmetry, for a face normal to the $y$-direction,\n$$\nm_{y} = w \\, n_{x} \\, n_{z} \\, q = w \\, q \\, \\frac{N_{x}}{P_{x}} \\, \\frac{N_{z}}{P_{z}},\n$$\nand for a face normal to the $z$-direction,\n$$\nm_{z} = w \\, n_{x} \\, n_{y} \\, q = w \\, q \\, \\frac{N_{x}}{P_{x}} \\, \\frac{N_{y}}{P_{y}}.\n$$\n\nUsing the latency–bandwidth model, the time for a single message on an $x$-face is $\\alpha + \\beta m_{x}$, and similarly for the $y$- and $z$-faces. Summing over all directed inter-rank face messages, the total time is\n$$\nT_{\\text{total}} = 2 \\left( P_{x} - 1 \\right) P_{y} P_{z} \\left( \\alpha + \\beta m_{x} \\right) + 2 \\left( P_{y} - 1 \\right) P_{x} P_{z} \\left( \\alpha + \\beta m_{y} \\right) + 2 \\left( P_{z} - 1 \\right) P_{x} P_{y} \\left( \\alpha + \\beta m_{z} \\right).\n$$\nSubstituting the expressions for $m_{x}$, $m_{y}$, and $m_{z}$, we obtain the closed-form analytic expression:\n$$\nT_{\\text{total}} = 2 \\left( P_{x} - 1 \\right) P_{y} P_{z} \\left( \\alpha + \\beta \\, w \\, q \\, \\frac{N_{y}}{P_{y}} \\, \\frac{N_{z}}{P_{z}} \\right) + 2 \\left( P_{y} - 1 \\right) P_{x} P_{z} \\left( \\alpha + \\beta \\, w \\, q \\, \\frac{N_{x}}{P_{x}} \\, \\frac{N_{z}}{P_{z}} \\right) + 2 \\left( P_{z} - 1 \\right) P_{x} P_{y} \\left( \\alpha + \\beta \\, w \\, q \\, \\frac{N_{x}}{P_{x}} \\, \\frac{N_{y}}{P_{y}} \\right).\n$$\nThis expression is in seconds provided $\\alpha$ is in seconds and $\\beta$ is in seconds per scalar data item. It counts all directed inter-rank face messages in one complete halo exchange step over the three coordinate directions and assumes that halo exchanges are aggregated per face, edges and corners are not sent separately, and only interior interfaces contribute to MPI communication time.",
            "answer": "$$\\boxed{2\\left(P_{x}-1\\right)P_{y}P_{z}\\left(\\alpha+\\beta\\,w\\,q\\,\\frac{N_{y}}{P_{y}}\\,\\frac{N_{z}}{P_{z}}\\right)+2\\left(P_{y}-1\\right)P_{x}P_{z}\\left(\\alpha+\\beta\\,w\\,q\\,\\frac{N_{x}}{P_{x}}\\,\\frac{N_{z}}{P_{z}}\\right)+2\\left(P_{z}-1\\right)P_{x}P_{y}\\left(\\alpha+\\beta\\,w\\,q\\,\\frac{N_{x}}{P_{x}}\\,\\frac{N_{y}}{P_{y}}\\right)}$$"
        },
        {
            "introduction": "Beyond modeling the cost of individual messages, the choice of domain decomposition strategy fundamentally alters the overall communication graph. Different partitioning schemes, such as 1D slab, 2D pencil, or 3D block decompositions, yield distinct subdomain surface-to-volume ratios and communication patterns. This practice  delves into the topological consequences of these choices, helping you develop an intuition for why higher-dimensional decompositions are crucial for achieving strong scaling in large-scale simulations.",
            "id": "3983415",
            "problem": "Consider a compressible Navier–Stokes solver implemented with finite volume discretization on a uniform Cartesian mesh with periodic boundary conditions in all three directions. The global domain has $N_{x} \\times N_{y} \\times N_{z}$ control volumes and is distributed over $P$ ranks using Message Passing Interface (MPI). The solver performs explicit time integration and at each substep exchanges halo data of width $w=1$ cell across all subdomain faces; edge and corner exchanges are not performed. Three domain decomposition strategies are considered:\n\n- Slab decomposition: one-dimensional partitioning along a single coordinate, with $(P_{x},P_{y},P_{z})=(1,1,P)$.\n- Pencil decomposition: two-dimensional partitioning along two coordinates, with $(P_{x},P_{y},P_{z})=(1,P_{y},P_{z})$ and $P_{y}P_{z}=P$.\n- Block decomposition: three-dimensional partitioning along all coordinates, with $(P_{x},P_{y},P_{z})$ satisfying $P_{x}P_{y}P_{z}=P$.\n\nDefine the neighbor count per rank $d$ as the number of distinct ranks that share a face with a given rank’s subdomain (under periodic boundaries). Define the communication pattern complexity metric $C$ as the total number of unique undirected face-adjacency pairs among all $P$ ranks, i.e., the number of distinct rank pairs that exchange halo messages per substep due to face sharing.\n\nStarting from the definitions above and fundamental properties of Cartesian partitioning and periodic adjacency, compute:\n\n1. The neighbor count per rank $d$ for slab, pencil, and block decompositions.\n2. The communication pattern complexity $C$ for slab, pencil, and block decompositions, expressed in terms of $P$ (and, where appropriate, $(P_{x},P_{y},P_{z})$ satisfying $P_{x}P_{y}P_{z}=P$).\n3. The normalized complexity $C/P$ for slab, pencil, and block decompositions.\n\nProvide your final answer as a single row matrix using the $\\mathrm{pmatrix}$ environment containing, in order, $d_{\\mathrm{slab}}$, $d_{\\mathrm{pencil}}$, $d_{\\mathrm{block}}$, $C_{\\mathrm{slab}}$, $C_{\\mathrm{pencil}}$, $C_{\\mathrm{block}}$, $C_{\\mathrm{slab}}/P$, $C_{\\mathrm{pencil}}/P$, and $C_{\\mathrm{block}}/P$. No rounding is required.",
            "solution": "The problem requires analyzing the topological properties of the communication graph where the $P$ MPI ranks are vertices and an edge represents a face-adjacency. With periodic boundaries, the processor topology is a Cartesian product of cycle graphs, $C_{P_x} \\times C_{P_y} \\times C_{P_z}$. We assume that for any partitioned dimension $i$ (i.e., $P_i > 1$), the number of partitions is greater than two ($P_i > 2$).\n\n**1. Derivation of Communication Complexity ($C$)**\nThe total number of unique adjacencies (edges), $C$, is the sum of adjacencies in each partitioned dimension. In a partitioned dimension $i$ with $P_i > 2$ partitions, there are $P_i$ adjacencies forming a cycle. This cycle is repeated across the other dimensions.\n- Number of adjacencies in $x$-direction: $C_x = P_x \\times P_y \\times P_z = P$, if $P_x > 1$. Otherwise $C_x=0$.\n- Number of adjacencies in $y$-direction: $C_y = P_x \\times P_y \\times P_z = P$, if $P_y > 1$. Otherwise $C_y=0$.\n- Number of adjacencies in $z$-direction: $C_z = P_x \\times P_y \\times P_z = P$, if $P_z > 1$. Otherwise $C_z=0$.\n\n- **Slab decomposition**: $(1, 1, P)$. Only the $z$-dimension is partitioned. $C_{\\mathrm{slab}} = 0 + 0 + P = P$.\n- **Pencil decomposition**: $(1, P_y, P_z)$. The $y$ and $z$ dimensions are partitioned. $C_{\\mathrm{pencil}} = 0 + P + P = 2P$.\n- **Block decomposition**: $(P_x, P_y, P_z)$. All three dimensions are partitioned. $C_{\\mathrm{block}} = P + P + P = 3P$.\n\n**2. Derivation of Neighbor Count per Rank ($d$)**\nThe neighbor count per rank, $d$, is the degree of each vertex. For a regular graph with $P$ vertices and $C$ edges, the handshaking lemma gives $P \\times d = 2C$, so $d = 2C/P$.\n- **Slab decomposition**: $d_{\\mathrm{slab}} = 2 C_{\\mathrm{slab}} / P = 2P/P = 2$.\n- **Pencil decomposition**: $d_{\\mathrm{pencil}} = 2 C_{\\mathrm{pencil}} / P = 2(2P)/P = 4$.\n- **Block decomposition**: $d_{\\mathrm{block}} = 2 C_{\\mathrm{block}} / P = 2(3P)/P = 6$.\n\n**3. Derivation of Normalized Complexity ($C/P$)**\nThis is found by dividing the expressions for $C$ by $P$.\n- **Slab decomposition**: $C_{\\mathrm{slab}}/P = P/P = 1$.\n- **Pencil decomposition**: $C_{\\mathrm{pencil}}/P = 2P/P = 2$.\n- **Block decomposition**: $C_{\\mathrm{block}}/P = 3P/P = 3$.\n\nThese nine results are arranged in a single row matrix as requested.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n2 & 4 & 6 & P & 2P & 3P & 1 & 2 & 3\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Our performance models become more realistic when we account for non-uniform workloads, a common feature in CFD where phenomena like shock waves or boundary layers create \"hot spots\". In explicit methods, these regions can severely constrain the global timestep via the Courant–Friedrichs–Lewy (CFL) condition, leading to significant load imbalance. This advanced exercise  challenges you to quantify the performance gains of a multi-rate timestepping strategy, a key technique for mitigating this issue and improving the efficiency of solvers for multiscale problems.",
            "id": "3983432",
            "problem": "An explicit finite-volume solver for the compressible Euler equations is run for a physical duration $T_{\\text{phys}}$ on a mesh with $N$ control volumes, partitioned evenly by cell count across $P$ ranks of the Message Passing Interface (MPI). The time integration is stable if it satisfies the Courant–Friedrichs–Lewy (CFL) restriction, namely that the local timestep in control volume $j$ obeys $\\Delta t_{j} \\leq \\tau_{j}$ with $\\tau_{j} = C \\, h_{j}/(|u_{j}| + a_{j})$, where $C \\in (0,1)$ is a fixed Courant number, $h_{j}$ is a local length scale, $|u_{j}|$ is the local flow speed, and $a_{j}$ is the local speed of sound. Assume that the cost per local explicit update per control volume is a constant $c$ and communication overheads are negligible.\n\nSuppose the field $\\{\\tau_{j}\\}$ has a bimodal distribution: a fraction $f \\in (0,1)$ of the control volumes belong to a “straggler” set with $\\tau_{j} = \\tau_{s}$, and the remaining fraction $1-f$ have $\\tau_{j} = \\tau_{\\ell}$, with $\\tau_{\\ell} = r \\, \\tau_{s}$ and $r > 1$. Assume that after load balancing the straggler cells are distributed uniformly across ranks by count, and that each rank owns exactly $N/P$ cells.\n\n1. Starting only from the CFL restriction and the stated cost model, derive the wall-clock time $T_{\\text{sync}}$ to reach $T_{\\text{phys}}$ under a synchronous adaptive scheme that selects the global timestep as the minimum over all local stability limits at each step.\n\n2. To avoid over-constraining the global timestep by the small straggler set, consider a multi-rate strategy: select a macro-timestep equal to a high quantile of the distribution of $\\{\\tau_{j}\\}$, and locally subcycle cells with $\\tau_{j}$ below that macro-timestep. Assume the quantile is chosen high enough that the macro-timestep equals $\\tau_{\\ell}$, so that each “slow” cell (with $\\tau_{j} = \\tau_{\\ell}$) takes one local substep per macro-timestep, while each “fast” cell (with $\\tau_{j} = \\tau_{s}$) takes $r = \\tau_{\\ell}/\\tau_{s}$ local substeps per macro-timestep. Derive the wall-clock time $T_{\\text{mr}}$ to reach $T_{\\text{phys}}$ under this strategy.\n\n3. Using your results, derive a closed-form analytic expression for the speedup $S(r,f) = T_{\\text{sync}}/T_{\\text{mr}}$ as a function of $r$ and $f$, simplified as far as possible. Provide your final answer as a single analytic expression in terms of $r$ and $f$ only. No numerical substitution is required, and no units are needed for $S(r,f)$.",
            "solution": "The solution is derived in three parts: the wall-clock time for a synchronous scheme, $T_{\\text{sync}}$; the wall-clock time for a multi-rate scheme, $T_{\\text{mr}}$; and the speedup $S(r,f) = T_{\\text{sync}}/T_{\\text{mr}}$.\n\n**Part 1: Derivation of $T_{\\text{sync}}$**\n\nThe synchronous adaptive scheme uses a single global timestep, $\\Delta t$, which must be the minimum of all local stability limits to ensure stability everywhere: $\\Delta t_{\\text{sync}} = \\min_{j} \\{\\tau_{j}\\}$. Given the bimodal distribution with $\\tau_{s}  \\tau_{\\ell}$, the global timestep is constrained by the straggler cells, so $\\Delta t_{\\text{sync}} = \\tau_{s}$.\n\nThe total number of timesteps required to reach $T_{\\text{phys}}$ is $N_{\\text{steps}} = T_{\\text{phys}} / \\Delta t_{\\text{sync}} = T_{\\text{phys}} / \\tau_{s}$.\n\nAt each timestep, all $N$ cells are updated once. With a cost of $c$ per cell update, the work per step is $N \\times c$. Since work is balanced across $P$ ranks, the wall-clock time per step is proportional to $(N/P) \\times c$. Let's set the time per step per rank as $t_{\\text{step}} = (N/P)c$. The total wall-clock time is:\n$$\nT_{\\text{sync}} = N_{\\text{steps}} \\times t_{\\text{step}} = \\left(\\frac{T_{\\text{phys}}}{\\tau_{s}}\\right) \\left(\\frac{N}{P} c\\right)\n$$\n\n**Part 2: Derivation of $T_{\\text{mr}}$**\n\nThe multi-rate scheme uses a macro-timestep, $\\Delta T_{\\text{macro}} = \\tau_{\\ell}$. The number of macro-steps needed is $N_{\\text{macro}} = T_{\\text{phys}} / \\Delta T_{\\text{macro}} = T_{\\text{phys}} / \\tau_{\\ell}$.\n\nWithin each macro-step:\n-   \"Slow\" cells (fraction $1-f$) with $\\tau_j = \\tau_{\\ell}$ are updated once.\n-   \"Fast\" cells (fraction $f$) with $\\tau_j = \\tau_{s}$ are subcycled $n_s = \\Delta T_{\\text{macro}} / \\tau_s = \\tau_{\\ell} / \\tau_s = r$ times.\n\nThe work is balanced, so we consider the work on a single rank, which owns $N/P$ cells.\n-   Number of fast cells per rank: $f \\times (N/P)$\n-   Number of slow cells per rank: $(1-f) \\times (N/P)$\n\nThe total work per rank per macro-step is the sum of the work on its fast and slow cells. The wall-clock time for one macro-step, $t_{\\text{macro}}$, is:\n$$\nt_{\\text{macro}} = \\left( (\\text{fast cells per rank} \\times n_{s}) + (\\text{slow cells per rank} \\times 1) \\right) \\times c\n$$\n$$\nt_{\\text{macro}} = \\left( \\left(f \\frac{N}{P}\\right) r + \\left((1-f) \\frac{N}{P}\\right) \\cdot 1 \\right) c = \\frac{N}{P} c \\left( fr + 1-f \\right)\n$$\nThe total wall-clock time, $T_{\\text{mr}}$, is the number of macro-steps multiplied by the time per macro-step:\n$$\nT_{\\text{mr}} = N_{\\text{macro}} \\times t_{\\text{macro}} = \\left(\\frac{T_{\\text{phys}}}{\\tau_{\\ell}}\\right) \\left(\\frac{N}{P} c (fr + 1 - f)\\right)\n$$\nSubstituting $\\tau_{\\ell} = r \\tau_{s}$:\n$$\nT_{\\text{mr}} = \\left(\\frac{T_{\\text{phys}}}{r \\tau_{s}}\\right) \\left(\\frac{N}{P} c (fr + 1 - f)\\right)\n$$\n\n**Part 3: Derivation of the Speedup $S(r, f)$**\n\nThe speedup $S(r, f)$ is the ratio $T_{\\text{sync}} / T_{\\text{mr}}$:\n$$\nS(r, f) = \\frac{T_{\\text{sync}}}{T_{\\text{mr}}} = \\frac{\\left(\\frac{T_{\\text{phys}}}{\\tau_{s}}\\right) \\left(\\frac{N}{P} c\\right)}{\\left(\\frac{T_{\\text{phys}}}{r \\tau_{s}}\\right) \\left(\\frac{N}{P} c (fr + 1 - f)\\right)}\n$$\nThe common factor of $\\frac{T_{\\text{phys}}}{\\tau_{s}} \\frac{N}{P} c$ cancels from the numerator and the denominator:\n$$\nS(r, f) = \\frac{1}{\\frac{1}{r} (fr + 1 - f)}\n$$\nMultiplying the numerator and denominator by $r$ gives the simplified expression:\n$$\nS(r, f) = \\frac{r}{fr + 1 - f} = \\frac{r}{1 + f(r - 1)}\n$$\nThis is the final simplified expression for the speedup.",
            "answer": "$$\\boxed{\\frac{r}{1 + f(r - 1)}}$$"
        }
    ]
}