{
    "hands_on_practices": [
        {
            "introduction": "The foundation of least-squares gradient reconstruction lies in approximating a smooth field with a linear function derived from a Taylor series expansion. This first exercise guides you through the process of building the reconstruction algorithm from first principles. By applying it to a known manufactured solution, you will quantitatively explore how the accuracy is influenced by different neighbor stencils and weighting strategies, providing crucial insights into the method's sensitivity to its core parameters. ",
            "id": "3972721",
            "problem": "Consider a two-dimensional, cell-centered finite-volume reconstruction problem typical of Aerospace Computational Fluid Dynamics (CFD), where the goal is to estimate the gradient of a smooth scalar field at a given cell centroid using neighboring cell values. Let the scalar field be defined by a manufactured solution that is sufficiently smooth and whose exact gradient can be evaluated analytically. The estimation must proceed from first principles using the following base facts: the definition of the gradient as the linear term in a Taylor expansion of a smooth function, and the principle of least squares from calculus and linear algebra for fitting parameters by minimizing a weighted sum of squared residuals.\n\nLet the manufactured solution be the scalar field given by\n$$\\phi(x,y) = \\sin(\\alpha x) + \\beta \\cos(\\gamma y) + \\delta x y,$$\nwith constants set to $\\,\\alpha = 1.6\\,$, $\\,\\beta = 0.5\\,$, $\\,\\gamma = 0.9\\,$, and $\\,\\delta = 0.3\\,$. The reconstruction point (the cell centroid) is at $(x_0,y_0)$ with $\\,x_0 = 0.27\\,$ and $\\,y_0 = -0.19\\,$. Let $\\,h = 0.05\\,$ be the characteristic neighbor offset scale. All quantities are dimensionless, and there are no physical units involved. Angles must be interpreted in radians.\n\nTo estimate the gradient at $(x_0,y_0)$, approximate the local behavior of $\\,\\phi\\,$ by a linear function centered at $(x_0,y_0)$ and determine the gradient vector $\\,\\nabla \\phi(x_0,y_0)\\,$ by minimizing a weighted sum of squared residuals over a given stencil of neighbor offsets. For each neighbor offset $\\,(d_{x,i}, d_{y,i})\\,$, the neighbor coordinate is $\\,(x_0 + d_{x,i}, y_0 + d_{y,i})\\,$ and the corresponding scalar value is $\\,\\phi(x_0 + d_{x,i}, y_0 + d_{y,i})\\,$. The residual for a candidate gradient $\\,\\mathbf{g} = (g_x,g_y)\\,$ is the difference between $\\,\\phi(x_0 + d_{x,i}, y_0 + d_{y,i})\\,$ and the linear prediction $\\,\\phi(x_0,y_0) + g_x d_{x,i} + g_y d_{y,i}\\,$. The weighted least squares estimate of $\\,\\mathbf{g}\\,$ is defined as the minimizer of the sum of residuals squared, with weights $\\,w_i \\ge 0\\,$ assigned to each neighbor. You must treat the problem generically without assuming any special structure beyond the given offsets and weights.\n\nYou must compare the estimation error of the least-squares gradient across different stencils and weightings while using the same manufactured solution. The error must be quantified as the Euclidean norm of the difference between the numerical gradient and the exact analytical gradient at $(x_0,y_0)$, that is,\n$$E = \\left\\| \\nabla \\phi_{\\text{numerical}}(x_0,y_0) - \\nabla \\phi_{\\text{exact}}(x_0,y_0) \\right\\|_2.$$\n\nUse the following three stencils of neighbor offsets, which represent different geometric configurations encountered in practice:\n\nStencil $\\,\\mathcal{S}_1\\,$ (approximately isotropic ring of eight neighbors):\n$\\{(h,0),(-h,0),(0,h),(0,-h),(h,h),(-h,h),(h,-h),(-h,-h)\\}.$\n\nStencil $\\,\\mathcal{S}_2\\,$ (strongly $\\,x$-biased with slight $\\,y$-jitter to avoid exact collinearity, eight neighbors):\n$\\{(-3h,0.002h),(-2h,-0.002h),(-h,0.001h),(h,-0.001h),(2h,0.002h),(3h,-0.002h),(-0.5h,0.0015h),(0.5h,-0.0015h)\\}.$\n\nStencil $\\,\\mathcal{S}_3\\,$ (nearly collinear neighbors along an oblique direction with small angular perturbations, eight neighbors):\nLet $\\,\\theta = \\pi/6\\,$ and perturbations $\\,\\{\\varepsilon_k\\} = \\{0, 0.001, -0.001, 0.002, -0.002, 0.003, -0.003, 0.004\\}\\,$. Let radii $\\,\\{r_k\\} = \\{h,1.25h,1.5h,1.75h,2h,2.25h,2.5h,2.75h\\}\\,$. The offsets are defined by\n$$d_{x,k} = r_k \\cos(\\theta + \\varepsilon_k), \\quad d_{y,k} = r_k \\sin(\\theta + \\varepsilon_k), \\quad k = 1,\\dots,8.$$\n\nFor each stencil, compute the least-squares gradient using each of the following three weighting strategies that act on the residuals:\n\nWeighting $\\,\\mathcal{W}_1\\,$ (uniform weights): $\\,w_i = 1\\,$ for all neighbors.\n\nWeighting $\\,\\mathcal{W}_2\\,$ (inverse-distance weights): $\\,w_i = 1 / \\sqrt{d_{x,i}^2 + d_{y,i}^2}\\,$.\n\nWeighting $\\,\\mathcal{W}_3\\,$ (inverse-square-distance weights): $\\,w_i = 1 / (d_{x,i}^2 + d_{y,i}^2)\\,$.\n\nFor each stencil-weighting pair, evaluate the numerical gradient through the weighted least-squares reconstruction and compute the error $\\,E\\,$ as defined above. The exact gradient of the manufactured solution at the centroid must be evaluated analytically from the definition of $\\,\\phi\\,$.\n\nTest Suite and Answer Specification:\nPerform the computations for the three stencils $\\,\\mathcal{S}_1\\,$, $\\,\\mathcal{S}_2\\,$, $\\,\\mathcal{S}_3\\,$ and the three weighting strategies $\\,\\mathcal{W}_1\\,$, $\\,\\mathcal{W}_2\\,$, $\\,\\mathcal{W}_3\\,$, yielding $\\,9\\,$ error values. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets and grouped by stencil. Each inner list corresponds to a stencil and must contain the three error values for the weightings in the order $\\,\\mathcal{W}_1\\,$, $\\,\\mathcal{W}_2\\,$, $\\,\\mathcal{W}_3\\,$. The final output format must be\n`[[E_11,E_12,E_13],[E_21,E_22,E_23],[E_31,E_32,E_33]]`,\nwhere $\\,E_{s,w}\\,$ denotes the error for stencil $\\,s\\,$ with weighting $\\,w\\,$. No additional text should be printed.",
            "solution": "The user has provided a problem statement that requires validation.\n\n### Step 1: Extract Givens\n- **Scalar Field:** $\\phi(x,y) = \\sin(\\alpha x) + \\beta \\cos(\\gamma y) + \\delta x y$\n- **Constants:** $\\alpha = 1.6$, $\\beta = 0.5$, $\\gamma = 0.9$, $\\delta = 0.3$.\n- **Reconstruction Point:** $(x_0,y_0) = (0.27, -0.19)$.\n- **Characteristic Scale:** $h = 0.05$.\n- **Objective:** Estimate the gradient $\\nabla \\phi(x_0,y_0)$ using a weighted least-squares method.\n- **Approximation:** A linear function $\\phi(x_0 + d_{x,i}, y_0 + d_{y,i}) \\approx \\phi(x_0,y_0) + g_x d_{x,i} + g_y d_{y,i}$.\n- **Residual:** $R_i = \\phi(x_0 + d_{x,i}, y_0 + d_{y,i}) - (\\phi(x_0,y_0) + g_x d_{x,i} + g_y d_{y,i})$.\n- **Minimization Target:** Sum of weighted squared residuals, $\\sum_i w_i R_i^2$.\n- **Error Metric:** Euclidean norm $E = \\left\\| \\nabla \\phi_{\\text{numerical}}(x_0,y_0) - \\nabla \\phi_{\\text{exact}}(x_0,y_0) \\right\\|_2$.\n- **Stencils:**\n    - $\\mathcal{S}_1$: $\\{(h,0),(-h,0),(0,h),(0,-h),(h,h),(-h,h),(h,-h),(-h,-h)\\}$.\n    - $\\mathcal{S}_2$: $\\{(-3h,0.002h),(-2h,-0.002h),(-h,0.001h),(h,-0.001h),(2h,0.002h),(3h,-0.002h),(-0.5h,0.0015h),(0.5h,-0.0015h)\\}$.\n    - $\\mathcal{S}_3$: Offsets $(d_{x,k}, d_{y,k})$ defined by $d_{x,k} = r_k \\cos(\\theta + \\varepsilon_k), d_{y,k} = r_k \\sin(\\theta + \\varepsilon_k)$ for $k = 1,\\dots,8$, with $\\theta = \\pi/6$, radii $\\{r_k\\} = h \\cdot \\{1,1.25,1.5,1.75,2,2.25,2.5,2.75\\}$, and perturbations $\\{\\varepsilon_k\\} = \\{0, 0.001, -0.001, 0.002, -0.002, 0.003, -0.003, 0.004\\}$.\n- **Weighting Strategies:**\n    - $\\mathcal{W}_1$: $w_i = 1$.\n    - $\\mathcal{W}_2$: $w_i = 1 / \\sqrt{d_{x,i}^2 + d_{y,i}^2}$.\n    - $\\mathcal{W}_3$: $w_i = 1 / (d_{x,i}^2 + d_{y,i}^2)$.\n- **Required Output:** A $3 \\times 3$ set of error values corresponding to each stencil-weighting pair, formatted as a list of lists.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, being a standard numerical method (least-squares gradient reconstruction) applied in computational science and engineering, particularly in the finite volume method for CFD. It is well-posed; the given stencils are designed to prevent the collinearity issues that would make the resulting linear system singular, ensuring a unique solution exists. The problem is objective, with all terms, constants, and procedures defined mathematically and unambiguously. It is self-contained, providing all necessary information. It is not trivial, as it requires the derivation and implementation of a well-known numerical algorithm and an understanding of how stencil geometry and weighting affect accuracy. The problem is verifiable through direct computation.\n\n### Step 3: Verdict and Action\nThe problem is valid. A detailed solution will be provided.\n\n---\n\nThe problem requires the estimation of the gradient of a scalar field, $\\nabla\\phi$, at a cell centroid $(x_0, y_0)$ from the values of the field at a set of neighboring locations. This is a classic reconstruction problem in finite volume methods. The chosen methodology is the principle of least squares, which seeks to find the gradient components that best fit the data from the neighbors in a linear sense.\n\nLet the unknown gradient vector at $(x_0, y_0)$ be denoted by $\\mathbf{g} = (g_x, g_y)$. According to Taylor's theorem, for a smooth function $\\phi(x,y)$, the value at a nearby point $(x_i, y_i)$ can be approximated by a linear expansion around $(x_0, y_0)$:\n$$ \\phi(x_i, y_i) \\approx \\phi(x_0, y_0) + \\left. \\frac{\\partial\\phi}{\\partial x} \\right|_{(x_0,y_0)} (x_i - x_0) + \\left. \\frac{\\partial\\phi}{\\partial y} \\right|_{(x_0,y_0)} (y_i - y_0) $$\nLet $\\mathbf{d}_i = (d_{x,i}, d_{y,i}) = (x_i-x_0, y_i-y_0)$ be the offset vector from the centroid to the $i$-th neighbor. The approximation can be written using the unknown gradient $\\mathbf{g}$ as:\n$$ \\phi(x_i, y_i) \\approx \\phi(x_0, y_0) + g_x d_{x,i} + g_y d_{y,i} $$\nLetting $\\Delta\\phi_i = \\phi(x_i, y_i) - \\phi(x_0, y_0)$, the linear model is $\\Delta\\phi_i \\approx g_x d_{x,i} + g_y d_{y,i}$. The residual, or error, for the $i$-th neighbor for a given candidate gradient $\\mathbf{g}$ is:\n$$ R_i(\\mathbf{g}) = \\Delta\\phi_i - (g_x d_{x,i} + g_y d_{y,i}) $$\nThe method of weighted least squares aims to find the gradient $\\mathbf{g}$ that minimizes the sum of the weighted squared residuals over all $N$ neighbors in the stencil:\n$$ J(\\mathbf{g}) = J(g_x, g_y) = \\sum_{i=1}^{N} w_i R_i^2 = \\sum_{i=1}^{N} w_i \\left( \\Delta\\phi_i - g_x d_{x,i} - g_y d_{y,i} \\right)^2 $$\nTo find the minimum, we take the partial derivatives of $J$ with respect to $g_x$ and $g_y$ and set them to zero.\n$$ \\frac{\\partial J}{\\partial g_x} = \\sum_{i=1}^{N} 2 w_i (\\Delta\\phi_i - g_x d_{x,i} - g_y d_{y,i})(-d_{x,i}) = 0 $$\n$$ \\frac{\\partial J}{\\partial g_y} = \\sum_{i=1}^{N} 2 w_i (\\Delta\\phi_i - g_x d_{x,i} - g_y d_{y,i})(-d_{y,i}) = 0 $$\nRearranging these two equations, we obtain a system of two linear equations, known as the normal equations:\n$$ \\left( \\sum_{i=1}^{N} w_i d_{x,i}^2 \\right) g_x + \\left( \\sum_{i=1}^{N} w_i d_{x,i} d_{y,i} \\right) g_y = \\sum_{i=1}^{N} w_i \\Delta\\phi_i d_{x,i} $$\n$$ \\left( \\sum_{i=1}^{N} w_i d_{x,i} d_{y,i} \\right) g_x + \\left( \\sum_{i=1}^{N} w_i d_{y,i}^2 \\right) g_y = \\sum_{i=1}^{N} w_i \\Delta\\phi_i d_{y,i} $$\nThis system can be written in matrix form $A\\mathbf{g} = \\mathbf{b}$, where $\\mathbf{g} = \\begin{pmatrix} g_x \\\\ g_y \\end{pmatrix}$, and\n$$ A = \\begin{pmatrix} \\sum w_i d_{x,i}^2 & \\sum w_i d_{x,i} d_{y,i} \\\\ \\sum w_i d_{x,i} d_{y,i} & \\sum w_i d_{y,i}^2 \\end{pmatrix}, \\quad \\mathbf{b} = \\begin{pmatrix} \\sum w_i \\Delta\\phi_i d_{x,i} \\\\ \\sum w_i \\Delta\\phi_i d_{y,i} \\end{pmatrix} $$\nThe matrix $A$ is a symmetric $2 \\times 2$ matrix. As long as the neighbor points are not all collinear, $A$ is invertible, and the least-squares estimate of the gradient is given by $\\mathbf{g}_{\\text{numerical}} = A^{-1}\\mathbf{b}$.\n\nTo assess the accuracy of this numerical gradient, we compare it to the exact gradient derived from the manufactured solution $\\phi(x,y) = \\sin(\\alpha x) + \\beta \\cos(\\gamma y) + \\delta xy$. The exact partial derivatives are:\n$$ \\frac{\\partial \\phi}{\\partial x} = \\alpha \\cos(\\alpha x) + \\delta y $$\n$$ \\frac{\\partial \\phi}{\\partial y} = -\\beta\\gamma \\sin(\\gamma y) + \\delta x $$\nWe evaluate these expressions at the centroid $(x_0, y_0) = (0.27, -0.19)$ using the specified constants $\\alpha = 1.6$, $\\beta = 0.5$, $\\gamma = 0.9$, $\\delta = 0.3$ to obtain $\\nabla\\phi_{\\text{exact}}(x_0, y_0)$.\n\nThe procedure for each of the $9$ test cases (3 stencils $\\times$ 3 weightings) is as follows:\n1.  Define the set of neighbor offset vectors $\\{ \\mathbf{d}_i \\}$ for the given stencil.\n2.  Calculate the scalar value $\\phi_0 = \\phi(x_0, y_0)$ at the centroid.\n3.  For each neighbor $i$, calculate its coordinate $(x_i, y_i) = (x_0+d_{x,i}, y_0+d_{y,i})$ and the scalar value $\\phi_i = \\phi(x_i, y_i)$, then find $\\Delta\\phi_i = \\phi_i - \\phi_0$.\n4.  Determine the set of weights $\\{ w_i \\}$ based on the specified weighting strategy.\n5.  Construct the elements of matrix $A$ and vector $\\mathbf{b}$ by summing over all neighbors.\n6.  Solve the linear system $A\\mathbf{g} = \\mathbf{b}$ to find the numerical gradient $\\mathbf{g}_{\\text{numerical}}$.\n7.  Calculate the error $E$ using the Euclidean norm of the difference: $E = \\left\\| \\mathbf{g}_{\\text{numerical}} - \\nabla\\phi_{\\text{exact}}(x_0, y_0) \\right\\|_2$.\n\nThis systematic process is implemented for each stencil and weighting combination to produce the required set of error values.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the least-squares gradient estimation error for a manufactured solution\n    across different stencils and weighting strategies.\n    \"\"\"\n    # Define constants and problem parameters\n    alpha = 1.6\n    beta = 0.5\n    gamma = 0.9\n    delta = 0.3\n    x0, y0 = 0.27, -0.19\n    h = 0.05\n\n    def phi(x, y):\n        \"\"\"The manufactured scalar field solution.\"\"\"\n        return np.sin(alpha * x) + beta * np.cos(gamma * y) + delta * x * y\n\n    def grad_phi_exact(x, y):\n        \"\"\"The exact analytical gradient of the scalar field.\"\"\"\n        gx = alpha * np.cos(alpha * x) + delta * y\n        gy = -beta * gamma * np.sin(gamma * y) + delta * x\n        return np.array([gx, gy])\n\n    def define_stencils(h_val):\n        \"\"\"Defines the three neighbor offset stencils.\"\"\"\n        # Stencil S1: Isotropic ring\n        s1 = np.array([\n            [h_val, 0], [-h_val, 0], [0, h_val], [0, -h_val],\n            [h_val, h_val], [-h_val, h_val], [h_val, -h_val], [-h_val, -h_val]\n        ])\n\n        # Stencil S2: Strongly x-biased\n        s2 = h_val * np.array([\n            [-3.0, 0.002], [-2.0, -0.002], [-1.0, 0.001],\n            [1.0, -0.001], [2.0, 0.002], [3.0, -0.002],\n            [-0.5, 0.0015], [0.5, -0.0015]\n        ])\n\n        # Stencil S3: Nearly collinear along an oblique direction\n        theta = np.pi / 6.0\n        epsilons = np.array([0, 0.001, -0.001, 0.002, -0.002, 0.003, -0.003, 0.004])\n        radii = h_val * np.array([1.0, 1.25, 1.5, 1.75, 2.0, 2.25, 2.5, 2.75])\n        angles = theta + epsilons\n        dx_s3 = radii * np.cos(angles)\n        dy_s3 = radii * np.sin(angles)\n        s3 = np.column_stack((dx_s3, dy_s3))\n        \n        return [s1, s2, s3]\n\n    def get_weights(offsets, strategy_id):\n        \"\"\"Computes weights based on the specified strategy.\"\"\"\n        if strategy_id == 1:  # W1: Uniform\n            return np.ones(len(offsets))\n        \n        distances_sq = np.sum(offsets**2, axis=1)\n        \n        if strategy_id == 2:  # W2: Inverse-distance\n            return 1.0 / np.sqrt(distances_sq)\n        elif strategy_id == 3:  # W3: Inverse-square-distance\n            return 1.0 / distances_sq\n        else:\n            raise ValueError(\"Invalid weighting strategy ID.\")\n\n    def compute_gradient_lsq(offsets, weights, phi_0):\n        \"\"\"\n        Computes the numerical gradient using weighted least squares.\n        \n        Args:\n            offsets (np.ndarray): Array of neighbor offset vectors.\n            weights (np.ndarray): Array of weights for each neighbor.\n            phi_0 (float): Scalar value at the central point (x0, y0).\n\n        Returns:\n            np.ndarray: The computed numerical gradient vector [gx, gy].\n        \"\"\"\n        neighbor_coords = np.array([x0, y0]) + offsets\n        phi_neighbors = phi(neighbor_coords[:, 0], neighbor_coords[:, 1])\n        delta_phi = phi_neighbors - phi_0\n        \n        dx = offsets[:, 0]\n        dy = offsets[:, 1]\n        \n        # Assemble the normal equation matrix A\n        A11 = np.sum(weights * dx**2)\n        A12 = np.sum(weights * dx * dy)\n        A22 = np.sum(weights * dy**2)\n        A = np.array([[A11, A12], [A12, A22]])\n        \n        # Assemble the right-hand side vector b\n        b1 = np.sum(weights * delta_phi * dx)\n        b2 = np.sum(weights * delta_phi * dy)\n        b = np.array([b1, b2])\n\n        # Solve the linear system A*g = b for the gradient g\n        grad_num = np.linalg.solve(A, b)\n        \n        return grad_num\n\n    # Main execution logic\n    stencils = define_stencils(h)\n    exact_grad = grad_phi_exact(x0, y0)\n    phi0_val = phi(x0, y0)\n    \n    all_errors = []\n    \n    for stencil in stencils:\n        stencil_errors = []\n        for weight_strategy_id in [1, 2, 3]:\n            # Calculate weights for the current strategy\n            current_weights = get_weights(stencil, weight_strategy_id)\n            \n            # Compute numerical gradient\n            numerical_grad = compute_gradient_lsq(stencil, current_weights, phi0_val)\n            \n            # Compute and store the error\n            error = np.linalg.norm(numerical_grad - exact_grad)\n            stencil_errors.append(error)\n        \n        all_errors.append(stencil_errors)\n\n    # Format the final output string as specified\n    inner_strings = [f\"[{','.join(f'{err:.16e}' for err in sublist)}]\" for sublist in all_errors]\n    final_output = f\"[{','.join(inner_strings)}]\"\n    \n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "After implementing a numerical scheme, verification is a critical next step to ensure its correctness. This practice introduces the 'patch test,' a fundamental benchmark in computational mechanics that verifies if your gradient reconstruction can exactly reproduce a simple linear (affine) field, a necessary property for convergence. You will also learn to diagnose a key failure mode—rank deficiency—which arises from poorly configured stencils and compromises the uniqueness of the solution. ",
            "id": "3972768",
            "problem": "You are tasked with designing and implementing a least-squares gradient reconstruction patch test for an affine scalar field within the context of Aerospace Computational Fluid Dynamics (Aerospace CFD). The goal is to verify that the reconstruction exactly reproduces the gradient of an affine field on a cell-centered patch when the geometric sampling is sufficiently rich, and to diagnose failure due to rank deficiency or implementation errors. The test must be constructed and executed entirely in purely mathematical terms without any physical units.\n\nBackground and fundamental base: An affine scalar field in $d$ spatial dimensions is defined as $u(\\mathbf{x}) = \\alpha + \\mathbf{g}^{\\top} \\mathbf{x}$, where $\\alpha \\in \\mathbb{R}$ and $\\mathbf{g} \\in \\mathbb{R}^{d}$ are constant coefficients, and $\\mathbf{x} \\in \\mathbb{R}^{d}$ denotes spatial coordinates. For a central point $\\mathbf{x}_{0}$, the exact affine relation implies $u(\\mathbf{x}_{j}) - u(\\mathbf{x}_{0}) = \\mathbf{g}^{\\top} (\\mathbf{x}_{j} - \\mathbf{x}_{0})$ for any neighbor point $\\mathbf{x}_{j}$. In a general patch with more neighbors than unknown gradient components, one may determine $\\mathbf{g}$ by minimizing a weighted sum of squared residuals between the difference values and their linear predictions, ensuring an unbiased reconstruction under full-rank sampling.\n\nTask: Write a complete program that, for each test case described below, performs the following steps:\n1. Construct the design offsets $\\Delta \\mathbf{x}_{j} = \\mathbf{x}_{j} - \\mathbf{x}_{0}$ and response differences $\\Delta u_{j} = u(\\mathbf{x}_{j}) - u(\\mathbf{x}_{0})$ from the provided affine field and geometry.\n2. Formulate and solve the weighted least-squares problem that minimizes the objective $\\sum_{j=1}^{m} w_{j} \\left(\\mathbf{g}^{\\top} \\Delta \\mathbf{x}_{j} - \\Delta u_{j}\\right)^{2}$ over $\\mathbf{g} \\in \\mathbb{R}^{d}$ using a numerically stable linear algebra method. Do not assume a priori any specialized formula; the method must follow from the least-squares minimization principle.\n3. Determine the numerical rank of the weighted design matrix using a singular value decomposition with a relative tolerance $r_{\\mathrm{tol}} = 10^{-12}$. If the numerical rank is strictly less than $d$, declare the system rank-deficient for the purposes of this test case.\n4. Compute the Euclidean error norm $\\lVert \\mathbf{g}_{\\mathrm{rec}} - \\mathbf{g}_{\\mathrm{true}} \\rVert_{2}$ between the reconstructed gradient $\\mathbf{g}_{\\mathrm{rec}}$ and the true gradient $\\mathbf{g}_{\\mathrm{true}}$.\n5. Produce a single-line output that aggregates the results for all test cases in the format specified below.\n\nScientific realism requirement: For an exact affine field and a patch with full column rank, the reconstruction must yield zero error in exact arithmetic, and a near-zero error bounded by rounding in floating-point arithmetic. Failure to achieve near-zero error indicates either rank deficiency or an implementation bug.\n\nAngle units are not involved in this problem. No physical units are involved; all quantities are nondimensional.\n\nTest suite: Implement the five test cases below. In each case, $d$ denotes the spatial dimension, $\\mathbf{x}_{0}$ is the central point, the neighbor coordinates are listed, the affine field coefficients $(\\alpha, \\mathbf{g}_{\\mathrm{true}})$ are provided so that $u(\\mathbf{x}) = \\alpha + \\mathbf{g}_{\\mathrm{true}}^{\\top} \\mathbf{x}$, and positive weights $w_{j}$ are specified. All points and coefficients are exact real numbers.\n\n- Case $1$ (happy path, $2$-D, uniform weights): $d = 2$, $\\mathbf{x}_{0} = (0, 0)$, neighbors $\\{(1, 0), (0, 1), (-1, 0), (0, -1), (1, 1)\\}$, affine field coefficients $\\alpha = 2$, $\\mathbf{g}_{\\mathrm{true}} = (3, -4)$, weights $w_{j} = 1$ for all $j$.\n- Case $2$ (rank-deficient geometry, $2$-D, collinear neighbors): $d = 2$, $\\mathbf{x}_{0} = (0, 0)$, neighbors $\\{(1, 0), (2, 0), (-1, 0)\\}$, affine field coefficients $\\alpha = 2$, $\\mathbf{g}_{\\mathrm{true}} = (3, -4)$, weights $w_{j} = 1$ for all $j$.\n- Case $3$ (happy path, $3$-D, uniform weights): $d = 3$, $\\mathbf{x}_{0} = (0, 0, 0)$, neighbors $\\{(1, 0, 0), (0, 1, 0), (0, 0, 1), (-1, 1, 1)\\}$, affine field coefficients $\\alpha = -1$, $\\mathbf{g}_{\\mathrm{true}} = (0.5, 1.5, -2)$, weights $w_{j} = 1$ for all $j$.\n- Case $4$ (nearly singular geometry, $2$-D, uniform weights): $d = 2$, $\\mathbf{x}_{0} = (0, 0)$, neighbors $\\{(1, 0), (2, 10^{-8}), (-1, -10^{-8})\\}$, affine field coefficients $\\alpha = 2$, $\\mathbf{g}_{\\mathrm{true}} = (3, -4)$, weights $w_{j} = 1$ for all $j$.\n- Case $5$ (happy path, $2$-D, nonuniform positive weights): $d = 2$, $\\mathbf{x}_{0} = (0, 0)$, neighbors $\\{(1, 0), (0, 1), (-1, 0), (0, -1), (1, 1)\\}$, affine field coefficients $\\alpha = 2$, $\\mathbf{g}_{\\mathrm{true}} = (3, -4)$, weights $(1, 2, 3, 4, 5)$ matching the neighbor order.\n\nNumerical rank detection: Let $\\sigma_{\\max}$ and $\\sigma_{\\min}$ denote the largest and smallest singular values of the weighted design matrix, respectively. Compute the numerical rank as the number of singular values $\\sigma$ satisfying $\\sigma \\ge r_{\\mathrm{tol}} \\, \\sigma_{\\max}$ with $r_{\\mathrm{tol}} = 10^{-12}$. Declare rank deficiency if this numerical rank is less than $d$.\n\nFinal output specification: For each test case in order $1$ through $5$, append two items to the output list: first the floating-point error norm $\\lVert \\mathbf{g}_{\\mathrm{rec}} - \\mathbf{g}_{\\mathrm{true}} \\rVert_{2}$, then a boolean indicating rank deficiency (use the language-native boolean values for true and false). Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, `[$e_1,b_1,e_2,b_2,...,e_5,b_5$]`, where $e_{k}$ is a floating-point value and $b_{k}$ is a boolean. No other output is permitted.",
            "solution": "The problem is to create a patch test for a least-squares gradient reconstruction algorithm. This involves verifying that for a given affine scalar field, the algorithm can exactly reconstruct the field's gradient, provided the geometric arrangement of sample points is adequate. The task requires deriving the solution from first principles, implementing it numerically, and diagnosing cases of rank deficiency.\n\nAn affine scalar field in a $d$-dimensional space $\\mathbb{R}^d$ is given by the function:\n$$\nu(\\mathbf{x}) = \\alpha + \\mathbf{g}_{\\mathrm{true}}^{\\top} \\mathbf{x}\n$$\nwhere $\\alpha \\in \\mathbb{R}$ is a scalar offset, $\\mathbf{g}_{\\mathrm{true}} \\in \\mathbb{R}^d$ is the constant true gradient vector, and $\\mathbf{x} \\in \\mathbb{R}^d$ is the position vector.\n\nFor a central point $\\mathbf{x}_{0}$ and a set of $m$ neighboring points $\\{\\mathbf{x}_{j}\\}_{j=1}^{m}$, the difference in the scalar field value is linearly related to the difference in position:\n$$\n\\Delta u_{j} = u(\\mathbf{x}_{j}) - u(\\mathbf{x}_{0}) = (\\alpha + \\mathbf{g}_{\\mathrm{true}}^{\\top} \\mathbf{x}_{j}) - (\\alpha + \\mathbf{g}_{\\mathrm{true}}^{\\top} \\mathbf{x}_{0}) = \\mathbf{g}_{\\mathrm{true}}^{\\top} (\\mathbf{x}_{j} - \\mathbf{x}_{0})\n$$\nLet $\\Delta \\mathbf{x}_{j} = \\mathbf{x}_{j} - \\mathbf{x}_{0}$. The relationship is $\\Delta u_{j} = \\mathbf{g}_{\\mathrm{true}}^{\\top} \\Delta \\mathbf{x}_{j}$.\n\nThe goal is to reconstruct an approximation of the gradient, $\\mathbf{g}_{\\mathrm{rec}}$, using only the known values of $\\Delta u_{j}$ and $\\Delta \\mathbf{x}_{j}$. This is formulated as a weighted least-squares problem, where we seek the vector $\\mathbf{g}$ that minimizes the sum of weighted squared residuals:\n$$\nJ(\\mathbf{g}) = \\sum_{j=1}^{m} w_{j} \\left( (\\Delta \\mathbf{x}_{j})^{\\top} \\mathbf{g} - \\Delta u_{j} \\right)^{2}\n$$\nHere, $w_{j} > 0$ are given weights for each neighbor $j$.\n\nTo solve this minimization problem, we first express it in matrix notation. Let $A$ be the $m \\times d$ design matrix, where the $j$-th row is $(\\Delta \\mathbf{x}_{j})^{\\top}$. Let $\\mathbf{b}$ be the $m \\times 1$ vector of responses, where the $j$-th element is $\\Delta u_{j}$. The system of equations for all neighbors is ideally $A\\mathbf{g} = \\mathbf{b}$. The objective function can be written as the squared norm of a weighted residual vector. Let $W$ be an $m \\times m$ diagonal matrix with $W_{jj} = \\sqrt{w_j}$. The objective function is:\n$$\nJ(\\mathbf{g}) = \\lVert W(A\\mathbf{g} - \\mathbf{b}) \\rVert_2^2\n$$\nLet $\\tilde{A} = WA$ and $\\tilde{\\mathbf{b}} = W\\mathbf{b}$. The problem is to minimize $J(\\mathbf{g}) = \\lVert \\tilde{A}\\mathbf{g} - \\tilde{\\mathbf{b}} \\rVert_2^2$.\nWe expand the squared norm:\n$$\nJ(\\mathbf{g}) = (\\tilde{A}\\mathbf{g} - \\tilde{\\mathbf{b}})^{\\top}(\\tilde{A}\\mathbf{g} - \\tilde{\\mathbf{b}}) = \\mathbf{g}^{\\top}\\tilde{A}^{\\top}\\tilde{A}\\mathbf{g} - 2\\mathbf{g}^{\\top}\\tilde{A}^{\\top}\\tilde{\\mathbf{b}} + \\tilde{\\mathbf{b}}^{\\top}\\tilde{\\mathbf{b}}\n$$\nTo find the minimum, we take the gradient of $J(\\mathbf{g})$ with respect to $\\mathbf{g}$ and set it to the zero vector:\n$$\n\\nabla_{\\mathbf{g}} J(\\mathbf{g}) = 2\\tilde{A}^{\\top}\\tilde{A}\\mathbf{g} - 2\\tilde{A}^{\\top}\\tilde{\\mathbf{b}} = \\mathbf{0}\n$$\nThis yields the normal equations for the weighted least-squares problem:\n$$\n(\\tilde{A}^{\\top}\\tilde{A})\\mathbf{g} = \\tilde{A}^{\\top}\\tilde{\\mathbf{b}}\n$$\nIf the matrix $\\tilde{A}^{\\top}\\tilde{A}$ is invertible (i.e., if $\\tilde{A}$ has full column rank), there is a unique solution for the reconstructed gradient $\\mathbf{g}_{\\mathrm{rec}}$. The rank of $\\tilde{A} = WA$ is the same as the rank of $A$ because $W$ is invertible. Therefore, a unique solution exists if and only if the set of displacement vectors $\\{\\Delta \\mathbf{x}_{j}\\}$ spans $\\mathbb{R}^d$.\n\nDirectly solving the normal equations can be numerically unstable if $\\tilde{A}$ is ill-conditioned. A more robust approach is to use the Singular Value Decomposition (SVD) of the weighted design matrix $\\tilde{A}$. Let the SVD of $\\tilde{A}$ be $\\tilde{A} = U\\Sigma V^{\\top}$, where $U$ is an $m \\times m$ orthogonal matrix, $\\Sigma$ is an $m \\times d$ diagonal matrix of singular values $\\sigma_i$, and $V$ is a $d \\times d$ orthogonal matrix. The least-squares solution is given by:\n$$\n\\mathbf{g}_{\\mathrm{rec}} = V \\Sigma^{+} U^{\\top} \\tilde{\\mathbf{b}}\n$$\nwhere $\\Sigma^{+}$ is the Moore-Penrose pseudoinverse of $\\Sigma$. This approach is numerically stable even for ill-conditioned or rank-deficient systems.\n\nThe numerical rank of $\\tilde{A}$ is determined by its singular values. Given a relative tolerance $r_{\\mathrm{tol}} = 10^{-12}$, the rank is the count of singular values $\\sigma_i$ such that $\\sigma_i \\geq r_{\\mathrm{tol}} \\sigma_{\\max}$, where $\\sigma_{\\max}$ is the largest singular value. If this numerical rank is less than the spatial dimension $d$, the system is declared rank-deficient. For such systems, the geometric sampling is insufficient to uniquely determine all components of the gradient.\n\nThe implementation will proceed by iterating through the five test cases. For each case:\n1.  The displacement vectors $\\Delta \\mathbf{x}_{j}$ and scalar differences $\\Delta u_j = \\mathbf{g}_{\\mathrm{true}}^{\\top} \\Delta \\mathbf{x}_j$ are constructed.\n2.  The design matrix $A$, response vector $\\mathbf{b}$, and weighting matrix $W$ are formed.\n3.  The weighted matrix $\\tilde{A} = WA$ and vector $\\tilde{\\mathbf{b}} = W\\mathbf{b}$ are computed.\n4.  The SVD of $\\tilde{A}$ is computed to determine its singular values and numerical rank. A boolean flag is set if the rank is less than $d$.\n5.  A stable least-squares solver, which internally uses SVD, is employed to find $\\mathbf{g}_{\\mathrm{rec}}$ by solving $\\tilde{A}\\mathbf{g} = \\tilde{\\mathbf{b}}$.\n6.  The Euclidean error norm $\\lVert\\mathbf{g}_{\\mathrm{rec}} - \\mathbf{g}_{\\mathrm{true}}\\rVert_2$ is calculated.\n\nFor test cases with full rank, since the response data $\\Delta u_j$ are derived exactly from the affine field, the reconstructed gradient $\\mathbf{g}_{\\mathrm{rec}}$ should be equal to $\\mathbf{g}_{\\mathrm{true}}$ up to floating-point precision. For rank-deficient cases, the reconstruction will fail to recover $\\mathbf{g}_{\\mathrm{true}}$, resulting in a non-zero error. For nearly singular cases, the system is technically full-rank, but numerical round-off errors may be amplified, leading to a small but non-zero reconstruction error.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ...\n\ndef solve():\n    \"\"\"\n    Main function to execute the least-squares gradient reconstruction patch test\n    for all specified test cases.\n    \"\"\"\n    # Define the test cases as a list of dictionaries.\n    test_cases = [\n        {\n            \"d\": 2, \"x0\": np.array([0., 0.]),\n            \"neighbors\": np.array([[1., 0.], [0., 1.], [-1., 0.], [0., -1.], [1., 1.]]),\n            \"alpha\": 2., \"g_true\": np.array([3., -4.]),\n            \"weights\": np.array([1., 1., 1., 1., 1.])\n        },\n        {\n            \"d\": 2, \"x0\": np.array([0., 0.]),\n            \"neighbors\": np.array([[1., 0.], [2., 0.], [-1., 0.]]),\n            \"alpha\": 2., \"g_true\": np.array([3., -4.]),\n            \"weights\": np.array([1., 1., 1.])\n        },\n        {\n            \"d\": 3, \"x0\": np.array([0., 0., 0.]),\n            \"neighbors\": np.array([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.], [-1., 1., 1.]]),\n            \"alpha\": -1., \"g_true\": np.array([0.5, 1.5, -2.]),\n            \"weights\": np.array([1., 1., 1., 1.])\n        },\n        {\n            \"d\": 2, \"x0\": np.array([0., 0.]),\n            \"neighbors\": np.array([[1., 0.], [2., 1e-8], [-1., -1e-8]]),\n            \"alpha\": 2., \"g_true\": np.array([3., -4.]),\n            \"weights\": np.array([1., 1., 1.])\n        },\n        {\n            \"d\": 2, \"x0\": np.array([0., 0.]),\n            \"neighbors\": np.array([[1., 0.], [0., 1.], [-1., 0.], [0., -1.], [1., 1.]]),\n            \"alpha\": 2., \"g_true\": np.array([3., -4.]),\n            \"weights\": np.array([1., 2., 3., 4., 5.])\n        }\n    ]\n\n    r_tol = 1e-12\n    results = []\n\n    for case in test_cases:\n        d = case[\"d\"]\n        x0 = case[\"x0\"]\n        neighbors = case[\"neighbors\"]\n        g_true = case[\"g_true\"]\n        weights = case[\"weights\"]\n        # alpha is not needed for gradient reconstruction, as it cancels out.\n        # u(x) = alpha + g_true.T @ x\n\n        # 1. Construct design offsets and response differences.\n        # delta_x_j = x_j - x_0\n        # A is the design matrix, with rows being delta_x_j.T\n        A = neighbors - x0\n        \n        # delta_u_j = u(x_j) - u(x_0) = g_true.T @ (x_j - x_0)\n        # b is the response vector with elements delta_u_j\n        b = A @ g_true\n\n        # 2. Formulate the weighted least-squares problem.\n        # Create diagonal weight matrix W with sqrt(w_j) on diagonal\n        W = np.diag(np.sqrt(weights))\n        \n        # Weighted design matrix and response vector\n        A_tilde = W @ A\n        b_tilde = W @ b\n\n        # 3. Determine the numerical rank of the weighted design matrix.\n        # Compute singular values of A_tilde\n        try:\n            singular_values = np.linalg.svd(A_tilde, compute_uv=False)\n            s_max = singular_values[0] if len(singular_values) > 0 else 0\n            \n            # Compute numerical rank based on the tolerance\n            numerical_rank = np.sum(singular_values >= r_tol * s_max)\n            is_rank_deficient = numerical_rank < d\n        except np.linalg.LinAlgError:\n            # In an empty or otherwise invalid case\n            numerical_rank = 0\n            is_rank_deficient = True\n            \n        # 4. Solve for the reconstructed gradient and compute the error.\n        # Use a numerically stable least-squares solver\n        g_rec, _, _, _ = np.linalg.lstsq(A_tilde, b_tilde, rcond=None)\n        \n        # Compute the Euclidean error norm\n        error_norm = np.linalg.norm(g_rec - g_true)\n        \n        # 5. Append results for this case to the list.\n        results.append(error_norm)\n        results.append(is_rank_deficient)\n\n    # Final print statement in the exact required format.\n    # str(True) -> 'True', str(False) -> 'False'\n    # The problem asks for \"language-native boolean values\", which these are.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "In practical CFD applications with complex geometries, meshes are often skewed, leading to non-orthogonality errors where cell-center connection lines are misaligned with face normals. This practice moves beyond basic implementation to design, challenging you to engineer a custom weighting strategy that actively mitigates this source of error. You will test your design on a synthetic skewed mesh and quantify its performance against a standard weighting scheme, demonstrating a key technique for improving accuracy on realistic grids. ",
            "id": "3972776",
            "problem": "Consider a two-dimensional cell-centered finite-volume setting in Computational Fluid Dynamics (CFD) where the gradient of a scalar field is reconstructed at each cell center using Least-Squares (LS) with neighbor cell values. The reconstruction uses neighboring cells that share a face with the target cell. Non-orthogonality error arises when the line connecting cell centers is not aligned with the face normal, which biases the estimation of the normal component of the gradient needed for fluxes. Your task is to design and implement a weighting strategy in the LS formulation that prioritizes neighbors aligned with face normals, and to validate it on synthetic skewed meshes generated by affine transformations of a Cartesian grid.\n\nStart from the following fundamental definitions:\n- The scalar field is a smooth function $u(x,y)$ defined on the physical space. Its gradient at a point is $\\nabla u(x,y) = \\left[\\partial u/\\partial x, \\ \\partial u/\\partial y\\right]$.\n- For a target cell with center at $(x_i,y_i)$ and a neighbor $j$ with center $(x_j,y_j)$, define the displacement $\\Delta \\boldsymbol{x}_{ij} = \\boldsymbol{x}_j - \\boldsymbol{x}_i$ and the value difference $\\Delta u_{ij} = u(\\boldsymbol{x}_j) - u(\\boldsymbol{x}_i)$.\n- The LS gradient reconstruction seeks the vector $\\boldsymbol{g}_i$ that minimizes the weighted sum of squared residuals $\\sum_j w_{ij} \\left(\\Delta u_{ij} - \\boldsymbol{g}_i \\cdot \\Delta \\boldsymbol{x}_{ij} \\right)^2$, where the sum extends over face-sharing neighbors $j$ and $w_{ij} > 0$ are weights.\n\nTo prioritize alignment with face normals, design weights $w_{ij}$ that depend on an alignment factor with the face normal $\\boldsymbol{n}_{ij}$, where $\\boldsymbol{n}_{ij}$ is the unit normal of the shared face between cells $i$ and $j$, oriented consistently from the first cell to the second. The alignment should reward neighbors whose center-to-center vector $\\Delta \\boldsymbol{x}_{ij}$ is more aligned with $\\boldsymbol{n}_{ij}$. In addition, the weights must reflect distance in a physically reasonable way to maintain numerical stability.\n\nConstruct synthetic meshes by applying an affine transformation to an axis-aligned vertex grid:\n1. Start from a uniform vertex grid on the square domain with integer coordinates and dimensions $N_x \\times N_y$ vertices.\n2. Apply a shear transformation with parameter $s$ followed by a rotation by angle $\\theta$ (in radians) to each vertex to obtain the physical coordinates.\n3. Define each cell as the quadrilateral formed by four adjacent transformed vertices, and its center as the average of its four vertices.\n4. Define the face normal of each shared face as the unit vector perpendicular to the face segment, oriented from the \"left/below\" cell to the \"right/above\" cell.\n\nUse the following scalar field:\n- $u(x,y) = \\sin(x) + \\frac{1}{2} y^2 + 0.3 x y$.\n- Its exact gradient is $\\nabla u(x,y) = \\left[\\cos(x) + 0.3 y, \\ y + 0.3 x \\right]$.\n\nImplement two LS reconstructions at interior cells (those having all four face-sharing neighbors):\n- Baseline weights: $w^{\\mathrm{base}}_{ij} = 1/\\lVert\\Delta \\boldsymbol{x}_{ij}\\rVert^2$.\n- Proposed alignment-aware weights: $w^{\\mathrm{align}}_{ij}$ that increase with the alignment factor $a_{ij} = \\frac{|\\Delta \\boldsymbol{x}_{ij} \\cdot \\boldsymbol{n}_{ij}|}{\\lVert\\Delta \\boldsymbol{x}_{ij}\\rVert}$ and include a distance scaling. Choose a smooth, strictly positive functional form that reflects these principles and is suitable for LS.\n\nFor each mesh, quantify performance by the root-mean-square (RMS) error of the normal derivative at interior faces. For a face between cells $i$ and $j$ with unit normal $\\boldsymbol{n}_{ij}$ and face center $\\boldsymbol{x}_f$, define:\n- The reconstructed normal derivative using cell gradients as $\\partial_n u|_{\\mathrm{rec}} = \\left( \\frac{\\boldsymbol{g}_i + \\boldsymbol{g}_j}{2} \\right) \\cdot \\boldsymbol{n}_{ij}$.\n- The exact normal derivative as $\\partial_n u|_{\\mathrm{exact}} = \\nabla u(\\boldsymbol{x}_f) \\cdot \\boldsymbol{n}_{ij}$.\nCompute the RMS of the difference $\\partial_n u|_{\\mathrm{rec}} - \\partial_n u|_{\\mathrm{exact}}$ over interior faces. Report the ratio $R = E_{\\mathrm{align}} / E_{\\mathrm{base}}$, where $E_{\\mathrm{align}}$ and $E_{\\mathrm{base}}$ are RMS errors from the alignment-aware and baseline reconstructions, respectively.\n\nAngle unit specification: all angles $\\theta$ must be in radians.\n\nTest suite and parameters:\n- Use $N_x = 30$, $N_y = 30$ vertices for the base grid in all tests.\n- Test cases:\n  1. Moderate skew: shear $s = 0.3$, rotation $\\theta = 0.2$.\n  2. Strong skew: shear $s = 0.8$, rotation $\\theta = 0.5$.\n  3. Orthogonal reference: shear $s = 0.0$, rotation $\\theta = 0.0$.\n\nYour program should compute $R$ for each test case and produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[$r_1,r_2,r_3]$\"). Each $r_k$ must be a floating-point number. No units are required because all quantities are dimensionless by construction.",
            "solution": "The posed problem is scientifically well-grounded, self-contained, and algorithmically specified. It constitutes a valid and interesting numerical experiment in computational fluid dynamics. I shall therefore proceed with a complete solution.\n\nThe core of the problem lies in the least-squares (LS) reconstruction of a scalar field's gradient on a computational grid. For a target cell `$i$`, we seek a gradient vector `$\\boldsymbol{g}_i$` that best explains the differences in the scalar value, `$u$`, between cell `$i$` and its face-sharing neighbors `$j$`. The first-order Taylor series expansion `$\\Delta u_{ij} = u(\\boldsymbol{x}_j) - u(\\boldsymbol{x}_i) \\approx \\nabla u(\\boldsymbol{x}_i) \\cdot (\\boldsymbol{x}_j - \\boldsymbol{x}_i) = \\boldsymbol{g}_i \\cdot \\Delta \\boldsymbol{x}_{ij}$` forms the basis of this reconstruction.\n\nTo find `$\\boldsymbol{g}_i = [g_x, g_y]^T$`, we minimize the weighted sum of squared residuals for all `$N$` neighbors:\n$$\n\\text{minimize} \\sum_{j=1}^{N} w_{ij} \\left( \\Delta u_{ij} - \\boldsymbol{g}_i \\cdot \\Delta \\boldsymbol{x}_{ij} \\right)^2\n$$\nThis is a standard weighted linear least-squares problem. The solution can be expressed in matrix form. Let `$A$` be an `$N \\times 2$` matrix whose rows are the displacement vectors `$\\Delta \\boldsymbol{x}_{ij}^T$`, let `$\\boldsymbol{b}$` be an `$N \\times 1$` vector of the scalar differences `$\\Delta u_{ij}$`, and let `$W$` be an `$N \\times N$` diagonal matrix of weights `$w_{ij}$`. The problem becomes finding `$\\boldsymbol{g}_i$` that minimizes `$\\lVert A \\boldsymbol{g}_i - \\boldsymbol{b} \\rVert_W^2$`. The normal equations provide the solution:\n$$\n(A^T W A) \\boldsymbol{g}_i = A^T W \\boldsymbol{b}\n$$\nThe gradient is then found by solving this `$2 \\times 2$` linear system:\n$$\n\\boldsymbol{g}_i = (A^T W A)^{-1} A^T W \\boldsymbol{b}\n$$\n\nThe problem requires constructing a synthetic skewed mesh by applying an affine transformation to a uniform Cartesian grid of vertices. The transformation consists of a shear followed by a rotation. A vertex `$\\boldsymbol{v} = [x, y]^T$` on the base grid is mapped to a physical coordinate `$\\boldsymbol{v}'$` by the transformation matrix `$T$`:\n$$\n\\boldsymbol{v}' = T \\boldsymbol{v} = R(\\theta) S(s) \\boldsymbol{v} = \\begin{pmatrix} \\cos\\theta & -\\sin\\theta \\\\ \\sin\\theta & \\cos\\theta \\end{pmatrix} \\begin{pmatrix} 1 & s \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\end{pmatrix}\n$$\nwhere `$s$` is the shear parameter and `$\\theta$` is the rotation angle in radians. Cell centers are defined as the arithmetic mean of their four vertex coordinates. Face normals `$\\boldsymbol{n}_{ij}$` are unit vectors perpendicular to the face segment, and their orientation is consistently defined from the cell with a lower index `$(i,j)$` to the one with a higher index (e.g., from `$(i,j)$` to `$(i+1,j)$`).\n\nTwo weighting strategies are to be compared.\n1.  **Baseline Weights**: `$w^{\\mathrm{base}}_{ij} = 1/\\lVert\\Delta \\boldsymbol{x}_{ij}\\rVert^2$`. This is a common inverse-distance squared weighting, which gives more influence to closer neighbors.\n\n2.  **Alignment-Aware Weights**: The problem requires designing weights `$w^{\\mathrm{align}}_{ij}$` that are strictly positive, incorporate distance scaling, and prioritize neighbors whose center-to-center vector `$\\Delta \\boldsymbol{x}_{ij}$` is well-aligned with the corresponding face normal `$\\boldsymbol{n}_{ij}$`. The alignment is quantified by the factor `$a_{ij} = \\frac{|\\Delta \\boldsymbol{x}_{ij} \\cdot \\boldsymbol{n}_{ij}|}{\\lVert\\Delta \\boldsymbol{x}_{ij}\\rVert} = |\\cos(\\phi)|$`, where `$\\phi$` is the angle between the two vectors. To satisfy these requirements, we can modify the baseline weight by an exponential factor that rewards alignment:\n    $$\n    w^{\\mathrm{align}}_{ij} = \\frac{\\exp(a_{ij})}{\\lVert\\Delta \\boldsymbol{x}_{ij}\\rVert^2}\n    $$\n    This form is strictly positive since `$\\exp(a_{ij}) > 0$`. It retains the inverse-square distance scaling. The exponential function smoothly and strongly increases the weight as the alignment factor `$a_{ij}$` approaches its maximum value of `$1$` (perfect alignment) and reduces it for poor alignment (`$a_{ij} \\to 0$`). For an orthogonal grid, `$a_{ij}=1$` for all neighbors. The alignment-aware weights become `$w^{\\mathrm{align}}_{ij} = \\exp(1) \\cdot w^{\\mathrm{base}}_{ij}$`. Since multiplying all weights by a constant does not change the LS solution, both methods will yield identical gradients, as expected.\n\nThe performance of each method is evaluated by the root-mean-square (RMS) error of the normal derivative on interior faces. An interior face is one that separates two interior cells (cells not on the domain boundary). For each such face, with center `$\\boldsymbol{x}_f$` and unit normal `$\\boldsymbol{n}_{ij}$` pointing from cell `$i$` to cell `$j$`, we compute:\n-   The reconstructed normal derivative: `$\\partial_n u|_{\\mathrm{rec}} = \\left( \\frac{\\boldsymbol{g}_i + \\boldsymbol{g}_j}{2} \\right) \\cdot \\boldsymbol{n}_{ij}$`, where `$\\boldsymbol{g}_i$` and `$\\boldsymbol{g}_j$` are the reconstructed gradients in the adjacent cells.\n-   The exact normal derivative: `$\\partial_n u|_{\\mathrm{exact}} = \\nabla u(\\boldsymbol{x}_f) \\cdot \\boldsymbol{n}_{ij}$`, using the given analytical gradient of `$u(x,y)$`.\n\nThe RMS error `$E$` is calculated over all `$N_f$` interior faces:\n$$\nE = \\sqrt{\\frac{1}{N_f} \\sum_{f=1}^{N_f} \\left(\\partial_n u|_{\\mathrm{rec}} - \\partial_n u|_{\\mathrm{exact}}\\right)^2}\n$$\nThis calculation is performed for both the baseline (`$E_{\\mathrm{base}}$`) and alignment-aware (`$E_{\\mathrm{align}}$`) weighting schemes. The final output is the ratio `$R = E_{\\mathrm{align}} / E_{\\mathrm{base}}$`. A ratio `$R  1$` indicates that the alignment-aware weighting strategy improves the accuracy of the normal derivative reconstruction.\n\nThe scalar field and its exact gradient are given by:\n- `$u(x,y) = \\sin(x) + \\frac{1}{2} y^2 + 0.3 x y$`\n- `$\\nabla u(x,y) = \\left[\\cos(x) + 0.3 y, \\ y + 0.3 x \\right]$`\n\nThe computational procedure involves generating the specified mesh, identifying interior cells and faces, computing gradients at each interior cell for both weighting schemes by solving the `$2 \\times 2$` LS system, and then calculating the respective RMS errors to find the ratio `$R$`.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef u_func(p: np.ndarray) - np.ndarray:\n    \"\"\"Computes the scalar field u at given points.\"\"\"\n    x, y = p[:, 0], p[:, 1]\n    return np.sin(x) + 0.5 * y**2 + 0.3 * x * y\n\ndef grad_u_func(p: np.ndarray) - np.ndarray:\n    \"\"\"Computes the exact gradient of u at given points.\"\"\"\n    x, y = p[:, 0], p[:, 1]\n    gx = np.cos(x) + 0.3 * y\n    gy = y + 0.3 * x\n    return np.stack([gx, gy], axis=-1)\n\ndef compute_gradients(vertices: np.ndarray, centers: np.ndarray, weight_type: str) - np.ndarray:\n    \"\"\"Computes gradients at interior cell centers using LS.\"\"\"\n    Nx_v, Ny_v, _ = vertices.shape\n    Ncx, Ncy, _ = centers.shape # Ncx = Nx_v - 1, Ncy = Ny_v - 1\n    \n    gradients = np.zeros_like(centers)\n\n    for i in range(1, Ncx - 1):\n        for j in range(1, Ncy - 1): # Iterate over interior cells\n            center_i = centers[i, j]\n            val_i = u_func(center_i.reshape(1, 2))[0]\n            \n            neighbor_indices = [(i - 1, j), (i + 1, j), (i, j - 1), (i, j + 1)]\n            \n            A = np.zeros((4, 2))\n            b = np.zeros(4)\n            W_diag = np.zeros(4)\n\n            for k, (ni, nj) in enumerate(neighbor_indices):\n                center_j = centers[ni, nj]\n                delta_x_vec = center_j - center_i\n                \n                A[k, :] = delta_x_vec\n                b[k] = u_func(center_j.reshape(1, 2))[0] - val_i\n\n                dist_sq = np.dot(delta_x_vec, delta_x_vec)\n                if dist_sq  1e-12: dist_sq = 1e-12\n\n                if weight_type == 'baseline':\n                    W_diag[k] = 1.0 / dist_sq\n                elif weight_type == 'aligned':\n                    # Determine face vertices\n                    if ni == i - 1: # Left neighbor\n                        v1, v2 = vertices[i, j], vertices[i, j + 1]\n                    elif ni == i + 1: # Right neighbor\n                        v1, v2 = vertices[i + 1, j], vertices[i + 1, j + 1]\n                    elif nj == j - 1: # Bottom neighbor\n                        v1, v2 = vertices[i, j], vertices[i + 1, j]\n                    else: # Top neighbor\n                        v1, v2 = vertices[i, j + 1], vertices[i + 1, j + 1]\n\n                    face_vec = v2 - v1\n                    n_vec = np.array([face_vec[1], -face_vec[0]])\n                    n_norm = np.linalg.norm(n_vec)\n                    if n_norm  1e-12: n_norm = 1e-12\n                    n_unit = n_vec / n_norm\n\n                    if np.dot(n_unit, delta_x_vec)  0.0:\n                        n_unit = -n_unit\n                    \n                    dist = np.sqrt(dist_sq)\n                    alignment_factor = np.abs(np.dot(delta_x_vec, n_unit)) / dist\n                    W_diag[k] = np.exp(alignment_factor) / dist_sq\n            \n            M = A.T @ np.diag(W_diag) @ A\n            rhs = A.T @ (W_diag * b)\n            \n            try:\n                grad = np.linalg.solve(M, rhs)\n            except np.linalg.LinAlgError:\n                # Fallback to unweighted if weighted system is singular\n                grad = np.linalg.lstsq(A, b, rcond=None)[0]\n                \n            gradients[i, j, :] = grad\n            \n    return gradients\n\ndef compute_rms_error(gradients: np.ndarray, vertices: np.ndarray, centers: np.ndarray) - float:\n    \"\"\"Computes RMS error of the normal derivative on interior faces.\"\"\"\n    Ncx, Ncy, _ = centers.shape\n    total_sq_error = 0.0\n    num_faces = 0\n\n    # Interior vertical faces (between (i,j) and (i+1,j))\n    for i in range(1, Ncx - 2):\n        for j in range(1, Ncy - 1):\n            cell1_idx, cell2_idx = (i, j), (i + 1, j)\n            v1, v2 = vertices[i + 1, j], vertices[i + 1, j + 1]\n            \n            face_center = 0.5 * (v1 + v2)\n            face_vec = v2 - v1\n            n_vec = np.array([face_vec[1], -face_vec[0]])\n            n_norm = np.linalg.norm(n_vec)\n            if n_norm  1e-12: n_norm = 1e-12\n            n_unit = n_vec / n_norm\n\n            disp_vec = centers[cell2_idx] - centers[cell1_idx]\n            if np.dot(n_unit, disp_vec)  0.0:\n                n_unit = -n_unit\n\n            avg_grad = 0.5 * (gradients[cell1_idx] + gradients[cell2_idx])\n            deriv_rec = np.dot(avg_grad, n_unit)\n            \n            exact_grad = grad_u_func(face_center.reshape(1, 2))[0]\n            deriv_exact = np.dot(exact_grad, n_unit)\n\n            total_sq_error += (deriv_rec - deriv_exact)**2\n            num_faces += 1\n\n    # Interior horizontal faces (between (i,j) and (i,j+1))\n    for i in range(1, Ncx - 1):\n        for j in range(1, Ncy - 2):\n            cell1_idx, cell2_idx = (i, j), (i, j + 1)\n            v1, v2 = vertices[i, j + 1], vertices[i + 1, j + 1]\n\n            face_center = 0.5 * (v1 + v2)\n            face_vec = v2 - v1\n            n_vec = np.array([face_vec[1], -face_vec[0]])\n            n_norm = np.linalg.norm(n_vec)\n            if n_norm  1e-12: n_norm = 1e-12\n            n_unit = n_vec / n_norm\n\n            disp_vec = centers[cell2_idx] - centers[cell1_idx]\n            if np.dot(n_unit, disp_vec)  0.0:\n                n_unit = -n_unit\n\n            avg_grad = 0.5 * (gradients[cell1_idx] + gradients[cell2_idx])\n            deriv_rec = np.dot(avg_grad, n_unit)\n\n            exact_grad = grad_u_func(face_center.reshape(1, 2))[0]\n            deriv_exact = np.dot(exact_grad, n_unit)\n            \n            total_sq_error += (deriv_rec - deriv_exact)**2\n            num_faces += 1\n\n    return np.sqrt(total_sq_error / num_faces) if num_faces  0 else 0.0\n\ndef run_case(Nx, Ny, s, theta):\n    \"\"\"Generates mesh, computes gradients, errors, and the ratio R.\"\"\"\n    # 1. Generate Mesh\n    x_v, y_v = np.arange(Nx), np.arange(Ny)\n    xv, yv = np.meshgrid(x_v, y_v, indexing='ij')\n    base_vertices = np.stack([xv, yv], axis=-1)\n\n    rot_matrix = np.array([[np.cos(theta), -np.sin(theta)], [np.sin(theta), np.cos(theta)]])\n    shear_matrix = np.array([[1, s], [0, 1]])\n    trans_matrix = rot_matrix @ shear_matrix\n    \n    physical_vertices = (base_vertices.reshape(-1, 2) @ trans_matrix.T).reshape(Nx, Ny, 2)\n    \n    Ncx, Ncy = Nx - 1, Ny - 1\n    cell_centers = 0.25 * (physical_vertices[:Ncx, :Ncy] +\n                           physical_vertices[1:Nx, :Ncy] +\n                           physical_vertices[1:Nx, 1:Ny] +\n                           physical_vertices[:Ncx, 1:Ny])\n\n    # 2. Compute Gradients and Errors\n    grad_base = compute_gradients(physical_vertices, cell_centers, 'baseline')\n    error_base = compute_rms_error(grad_base, physical_vertices, cell_centers)\n\n    grad_align = compute_gradients(physical_vertices, cell_centers, 'aligned')\n    error_align = compute_rms_error(grad_align, physical_vertices, cell_centers)\n\n    if error_base  1e-12:\n        return 1.0\n\n    return error_align / error_base\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (30, 30, 0.3, 0.2), # Moderate skew\n        (30, 30, 0.8, 0.5), # Strong skew\n        (30, 30, 0.0, 0.0), # Orthogonal reference\n    ]\n\n    results = []\n    for Nx, Ny, s, theta in test_cases:\n        ratio = run_case(Nx, Ny, s, theta)\n        results.append(ratio)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}