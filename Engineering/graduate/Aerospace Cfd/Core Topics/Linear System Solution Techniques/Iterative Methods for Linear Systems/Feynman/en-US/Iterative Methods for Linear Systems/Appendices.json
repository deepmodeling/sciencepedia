{
    "hands_on_practices": [
        {
            "introduction": "The convergence of any iterative method is not guaranteed. Before employing a solver, we must determine if the sequence of approximations will actually approach the true solution. This exercise  serves as a fundamental check on this concept, focusing on the Jacobi method. By calculating the spectral radius $\\rho(T_J)$ of the Jacobi iteration matrix $T_J$, you will directly apply the core theorem of iterative method convergence, reinforcing the essential link between matrix properties and algorithmic behavior.",
            "id": "2182298",
            "problem": "Consider the linear system of equations $A\\mathbf{x} = \\mathbf{b}$, where the matrix $A$ and the vector $\\mathbf{b}$ are given by:\n$$\nA = \\begin{pmatrix} 5 & 2 \\\\ 1 & -4 \\end{pmatrix}, \\quad \\mathbf{x} = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}, \\quad \\mathbf{b} = \\begin{pmatrix} 9 \\\\ -2 \\end{pmatrix}\n$$\nThe Jacobi method is an iterative algorithm for approximately solving this system. The convergence of this method depends on the spectral radius of its associated iteration matrix, $T_J$. Calculate the spectral radius, $\\rho(T_J)$, for the given matrix $A$.\n\nExpress your final answer as an exact value in the form $\\frac{\\sqrt{N}}{M}$ where $N$ and $M$ are positive integers.",
            "solution": "For the Jacobi method, write the splitting $A = D + R$ where $D$ is the diagonal of $A$ and $R = A - D$. The Jacobi iteration matrix is\n$$\nT_{J} = -D^{-1}R = I - D^{-1}A.\n$$\nFor\n$$\nA = \\begin{pmatrix} 5 & 2 \\\\ 1 & -4 \\end{pmatrix},\n$$\nwe have\n$$\nD = \\begin{pmatrix} 5 & 0 \\\\ 0 & -4 \\end{pmatrix}, \\quad D^{-1} = \\begin{pmatrix} \\frac{1}{5} & 0 \\\\ 0 & -\\frac{1}{4} \\end{pmatrix}.\n$$\nCompute\n$$\nD^{-1}A = \\begin{pmatrix} \\frac{1}{5} & 0 \\\\ 0 & -\\frac{1}{4} \\end{pmatrix} \\begin{pmatrix} 5 & 2 \\\\ 1 & -4 \\end{pmatrix} = \\begin{pmatrix} 1 & \\frac{2}{5} \\\\ -\\frac{1}{4} & 1 \\end{pmatrix},\n$$\nso\n$$\nT_{J} = I - D^{-1}A = \\begin{pmatrix} 0 & -\\frac{2}{5} \\\\ \\frac{1}{4} & 0 \\end{pmatrix}.\n$$\nFor a matrix of the form $\\begin{pmatrix} 0 & a \\\\ b & 0 \\end{pmatrix}$, the characteristic polynomial is $\\lambda^{2} - ab$, hence the eigenvalues are $\\lambda = \\pm \\sqrt{ab}$. Here $a = -\\frac{2}{5}$ and $b = \\frac{1}{4}$, so\n$$\nab = -\\frac{1}{10}, \\quad \\lambda^{2} = -\\frac{1}{10}, \\quad \\lambda = \\pm i \\frac{1}{\\sqrt{10}}.\n$$\nTherefore, the spectral radius is the maximum modulus of the eigenvalues,\n$$\n\\rho(T_{J}) = \\frac{1}{\\sqrt{10}} = \\frac{\\sqrt{10}}{10}.\n$$",
            "answer": "$$\\boxed{\\frac{\\sqrt{10}}{10}}$$"
        },
        {
            "introduction": "The Conjugate Gradient (CG) method is a cornerstone of numerical linear algebra, prized for its efficiency in solving large, sparse, symmetric positive-definite (SPD) systems. However, its power is tied to these specific matrix properties. This exercise  challenges you to explore what happens when this fundamental requirement is violated. By applying CG to a symmetric but indefinite system, you will witness the breakdown of the algorithm firsthand and connect it directly to the loss of convexity in the underlying optimization problem, providing a deep understanding of why the SPD condition is not merely a theoretical footnote but a practical necessity.",
            "id": "3245204",
            "problem": "Consider applying the Conjugate Gradient (CG) method to the linear system $A x = b$ where $A$ is symmetric but not positive definite. The CG method is classically defined by minimizing the quadratic functional $\\phi(x) = \\tfrac{1}{2} x^{\\mathsf{T}} A x - b^{\\mathsf{T}} x$ along search directions $p_k$, with residuals $r_k = b - A x_k$, and the initial search direction $p_0 = r_0$. From first principles, the step length $\\alpha_k$ is determined by minimizing $\\phi(x_k + \\alpha p_k)$ with respect to the scalar $\\alpha$.\n\nWork with the explicit example\n$$\nA = \\begin{pmatrix} 1 & 0 \\\\ 0 & -1 \\end{pmatrix}, \n\\quad \nb = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}, \n\\quad \nx_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}.\n$$\nStarting from the definition of $\\phi(x)$ and the directional derivative condition for optimality along $p_k$, derive the expression for the step length $\\alpha_0$ in terms of $p_0$ and $r_0$ and identify the denominator that must be strictly positive when $A$ is positive definite. Then, compute the scalar $p_0^{\\mathsf{T}} A p_0$ for the given $A$, $b$, and $x_0$, and use this value to explain whether the CG step is well-defined in this case and why the method breaks down for this non-positive-definite $A$.\n\nYour final reported answer should be the numerical value of $p_0^{\\mathsf{T}} A p_0$. No rounding is required.",
            "solution": "The problem asks for an analysis of the Conjugate Gradient (CG) method applied to a linear system $A x = b$ where the matrix $A$ is symmetric but not positive definite. We will first derive the general formula for the step length $\\alpha_k$ from first principles, then apply it to the specific case provided to demonstrate the method's breakdown.\n\nThe CG method aims to find the solution $x$ by minimizing the quadratic functional $\\phi(x) = \\frac{1}{2} x^{\\mathsf{T}} A x - b^{\\mathsf{T}} x$. The gradient of this functional is $\\nabla \\phi(x) = A x - b$, which is the negative of the residual, $r = b - A x$. The method proceeds iteratively by generating a sequence of approximations $x_{k+1} = x_k + \\alpha_k p_k$, where $p_k$ is a search direction and $\\alpha_k$ is a step length chosen to minimize $\\phi(x_k + \\alpha_k p_k)$.\n\nTo derive $\\alpha_k$, we define a function of a single variable $\\alpha$:\n$$f(\\alpha) = \\phi(x_k + \\alpha p_k) = \\frac{1}{2} (x_k + \\alpha p_k)^{\\mathsf{T}} A (x_k + \\alpha p_k) - b^{\\mathsf{T}} (x_k + \\alpha p_k)$$\nExpanding the terms, we get:\n$$f(\\alpha) = \\frac{1}{2} (x_k^{\\mathsf{T}} A x_k + \\alpha x_k^{\\mathsf{T}} A p_k + \\alpha p_k^{\\mathsf{T}} A x_k + \\alpha^2 p_k^{\\mathsf{T}} A p_k) - b^{\\mathsf{T}} x_k - \\alpha b^{\\mathsf{T}} p_k$$\nSince $A$ is symmetric, $A = A^{\\mathsf{T}}$, the scalar quantity $p_k^{\\mathsf{T}} A x_k = (x_k^{\\mathsf{T}} A^{\\mathsf{T}} p_k)^{\\mathsf{T}} = (x_k^{\\mathsf{T}} A p_k)^{\\mathsf{T}} = x_k^{\\mathsf{T}} A p_k$. We can group terms by powers of $\\alpha$:\n$$f(\\alpha) = \\left(\\frac{1}{2} x_k^{\\mathsf{T}} A x_k - b^{\\mathsf{T}} x_k\\right) + \\alpha (x_k^{\\mathsf{T}} A p_k - b^{\\mathsf{T}} p_k) + \\frac{1}{2} \\alpha^2 (p_k^{\\mathsf{T}} A p_k)$$\nThe first term is $\\phi(x_k)$. The term linear in $\\alpha$ can be rewritten using the residual $r_k = b - A x_k$.\n$$x_k^{\\mathsf{T}} A p_k - b^{\\mathsf{T}} p_k = -(b - A x_k)^{\\mathsf{T}} p_k = -r_k^{\\mathsf{T}} p_k$$\nSo, the function to minimize is:\n$$f(\\alpha) = \\phi(x_k) - \\alpha r_k^{\\mathsf{T}} p_k + \\frac{1}{2} \\alpha^2 p_k^{\\mathsf{T}} A p_k$$\nTo find the minimum, we take the derivative with respect to $\\alpha$ and set it to zero, as dictated by the \"directional derivative condition for optimality\":\n$$\\frac{df}{d\\alpha} = -r_k^{\\mathsf{T}} p_k + \\alpha p_k^{\\mathsf{T}} A p_k = 0$$\nSolving for $\\alpha$ yields the step length $\\alpha_k$:\n$$\\alpha_k = \\frac{r_k^{\\mathsf{T}} p_k}{p_k^{\\mathsf{T}} A p_k}$$\nFor the first iteration ($k=0$), where the search direction $p_0$ is set to be the initial residual $r_0$, this becomes:\n$$\\alpha_0 = \\frac{r_0^{\\mathsf{T}} r_0}{r_0^{\\mathsf{T}} A r_0}$$\nThe denominator in this expression is $p_k^{\\mathsf{T}} A p_k$. For the CG method to be well-defined and for the step to correspond to a minimum of $\\phi$ along the search direction, the second derivative of $f(\\alpha)$ must be positive:\n$$\\frac{d^2f}{d\\alpha^2} = p_k^{\\mathsf{T}} A p_k > 0$$\nIf $A$ is symmetric positive definite (SPD), then by definition, $v^{\\mathsf{T}} A v > 0$ for any non-zero vector $v$. Since the search directions $p_k$ generated by the CG algorithm are non-zero (unless the exact solution has been found), the denominator $p_k^{\\mathsf{T}} A p_k$ is guaranteed to be strictly positive when $A$ is SPD.\n\nNow we analyze the specific case provided:\n$$A = \\begin{pmatrix} 1 & 0 \\\\ 0 & -1 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}, \\quad x_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$$\nThe matrix $A$ is symmetric, but its eigenvalues are $1$ and $-1$, so it is not positive definite.\nFirst, we compute the initial residual $r_0$:\n$$r_0 = b - A x_0 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 1 & 0 \\\\ 0 & -1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$$\nThe initial search direction is $p_0 = r_0$, so $p_0 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$.\nThe problem asks to compute the scalar $p_0^{\\mathsf{T}} A p_0$, which is the denominator required to find the step length $\\alpha_0$.\n$$p_0^{\\mathsf{T}} A p_0 = \\begin{pmatrix} 1 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 0 & -1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$$\nFirst, we compute the product $A p_0$:\n$$A p_0 = \\begin{pmatrix} 1 & 0 \\\\ 0 & -1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot 1 + 0 \\cdot 1 \\\\ 0 \\cdot 1 + (-1) \\cdot 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$$\nNow we compute the final inner product:\n$$p_0^{\\mathsf{T}} (A p_0) = \\begin{pmatrix} 1 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = (1)(1) + (1)(-1) = 1 - 1 = 0$$\nThus, the value of the denominator is $p_0^{\\mathsf{T}} A p_0 = 0$.\n\nThe CG step is not well-defined because calculating the step length $\\alpha_0$ requires division by this quantity. The numerator is $r_0^{\\mathsf{T}} p_0 = r_0^{\\mathsf{T}} r_0 = 1^2 + 1^2 = 2$. Therefore, $\\alpha_0 = \\frac{2}{0}$, which is undefined. The algorithm breaks down at the first iteration.\n\nThis breakdown occurs because the condition $p_0^{\\mathsf{T}} A p_0 > 0$ is violated. When $p_0^{\\mathsf{T}} A p_0 = 0$, the function $f(\\alpha) = \\phi(x_0 + \\alpha p_0)$ is no longer a convex quadratic. It becomes a linear function:\n$$f(\\alpha) = \\phi(x_0) - \\alpha (r_0^{\\mathsf{T}} p_0) = \\phi(x_0) - 2\\alpha$$\nA non-constant linear function has no minimum on the real line. The CG method fails because its core assumption—that it can find a minimum of the quadratic functional along the search direction—is not met. This demonstrates a fundamental reason why the standard CG method is restricted to systems with symmetric positive definite matrices.\nThe numerical value of $p_0^{\\mathsf{T}} A p_0$ is $0$.",
            "answer": "$$\\boxed{0}$$"
        },
        {
            "introduction": "In computational fluid dynamics (CFD), the sheer scale of linear systems arising from discretized PDEs makes direct solvers infeasible and basic iterative methods too slow. The key to practical solutions lies in preconditioning. This exercise  brings you to the forefront of this practice by analyzing the Incomplete LU factorization with zero fill-in, $\\text{ILU}(0)$, for a matrix derived from a standard five-point stencil. You will move beyond simple numerical calculation to perform a structural analysis of the factors $\\mathbf{L}$ and $\\mathbf{U}$, a critical skill for understanding memory costs and the computational efficiency of preconditioners in real-world CFD codes.",
            "id": "3969374",
            "problem": "Consider the pressure Poisson equation arising in a projection method for incompressible flow in aerospace Computational Fluid Dynamics (CFD), discretized on a rectangular domain using a uniform Cartesian grid with $N_x$ interior points in the $x$-direction and $N_y$ interior points in the $y$-direction, and Dirichlet boundary conditions on the perimeter. The second-order central finite-volume or finite-difference discretization leads to a sparse linear system $\\mathbf{A}\\mathbf{x}=\\mathbf{b}$, where $\\mathbf{A}$ corresponds to the standard two-dimensional five-point stencil with constant coefficients. Assume natural lexicographic row-wise ordering of the unknowns, i.e., indices increase first along $x$ within a row and then in $y$ between rows.\n\nStarting from core definitions of the five-point stencil arising from the discrete Laplacian and the Gaussian elimination procedure, construct the Incomplete LU factorization with zero fill, known as Incomplete LU (ILU) with level zero, denoted $\\text{ILU}(0)$, for $\\mathbf{A}$. The $\\text{ILU}(0)$ is defined by performing the standard LU factorization while enforcing that no fill-in is retained beyond the original sparsity pattern of $\\mathbf{A}$. Take $\\mathbf{L}$ to have a unit diagonal and $\\mathbf{U}$ to carry the diagonal.\n\nDerive the nonzero structure of $\\mathbf{L}$ and $\\mathbf{U}$ implied by the five-point stencil and the specified ordering, carefully justifying which neighbor couplings appear in the strictly lower part of $\\mathbf{L}$ and strictly upper part of $\\mathbf{U}$ under the no-fill constraint. Then, compute the exact total number of nonzero entries in $\\mathbf{L}$ plus $\\mathbf{U}$, denoted $\\mathrm{nnz}(\\mathbf{L})+\\mathrm{nnz}(\\mathbf{U})$, as a closed-form expression in terms of $N_x$ and $N_y$.\n\nProvide your final answer as a single closed-form analytic expression. No rounding is required. No physical units are associated with this count.",
            "solution": "The system of linear equations $\\mathbf{A}\\mathbf{x}=\\mathbf{b}$ is defined on a grid with $N_x \\times N_y$ interior points. The total number of unknowns, and thus the dimension of the square matrix $\\mathbf{A}$, is $N = N_x N_y$. The unknowns are ordered using a natural lexicographic scheme, where the index $k$ for a grid point $(i,j)$ with $i \\in \\{1, \\dots, N_x\\}$ and $j \\in \\{1, \\dots, N_y\\}$ is given by $k = (j-1)N_x + i$.\n\nThe matrix $\\mathbf{A}$ represents the discrete negative Laplacian operator using a five-point stencil. For a given point $(i,j)$ with global index $k$, it is coupled to its immediate neighbors:\n- West: $(i-1, j)$, with global index $k-1$ (if $i>1$).\n- East: $(i+1, j)$, with global index $k+1$ (if $i<N_x$).\n- South: $(i, j-1)$, with global index $k-N_x$ (if $j>1$).\n- North: $(i, j+1)$, with global index $k+N_x$ (if $j<N_y$).\n\nThis structure implies that the matrix $\\mathbf{A}$ has nonzero entries only on its main diagonal and on four off-diagonals at offsets $\\pm 1$ and $\\pm N_x$. The set of index pairs $(k,l)$ for which $A_{k,l}$ is potentially nonzero is called the sparsity pattern of $\\mathbf{A}$, denoted $S(\\mathbf{A})$.\n\nThe Incomplete LU factorization with zero fill-in, $\\text{ILU}(0)$, produces a lower triangular matrix $\\mathbf{L}$ and an upper triangular matrix $\\mathbf{U}$ such that $\\mathbf{A} \\approx \\mathbf{L}\\mathbf{U}$. The defining constraint of $\\text{ILU}(0)$ is that the sparsity patterns of $\\mathbf{L}$ and $\\mathbf{U}$, denoted $S(\\mathbf{L})$ and $S(\\mathbf{U})$, must be subsets of the original matrix's sparsity pattern $S(\\mathbf{A})$. Formally, $S(\\mathbf{L}) \\cup S(\\mathbf{U}) \\subseteq S(\\mathbf{A})$.\n\nThe problem specifies that $\\mathbf{L}$ has a unit diagonal ($L_{k,k}=1$ for all $k$) and $\\mathbf{U}$ carries the main diagonal entries of the factorization.\nThe nonzero structure of $\\mathbf{L}$ is thus defined by the strictly lower part of $\\mathbf{A}$'s sparsity pattern, plus its own unit diagonal. A nonzero entry $L_{k,l}$ for $k > l$ exists only if $A_{k,l}$ is nonzero. Based on the lexicographic ordering:\n- $A_{k, k-1}$ is nonzero, corresponding to the West neighbor. This populates the first subdiagonal of $\\mathbf{L}$.\n- $A_{k, k-N_x}$ is nonzero, corresponding to the South neighbor. This populates the $N_x$-th subdiagonal of $\\mathbf{L}$.\nTherefore, $\\mathbf{L}$ is a unit lower triangular matrix with nonzeros restricted to the main diagonal, the first subdiagonal, and the $N_x$-th subdiagonal.\n\nThe nonzero structure of $\\mathbf{U}$ is defined by the upper triangular part (including the diagonal) of $\\mathbf{A}$'s sparsity pattern. A nonzero entry $U_{k,l}$ for $k \\le l$ exists only if $A_{k,l}$ is nonzero.\n- $A_{k,k}$ is nonzero. This populates the main diagonal of $\\mathbf{U}$.\n- $A_{k, k+1}$ is nonzero, corresponding to the East neighbor. This populates the first superdiagonal of $\\mathbf{U}$.\n- $A_{k, k+N_x}$ is nonzero, corresponding to the North neighbor. This populates the $N_x$-th superdiagonal of $\\mathbf{U}$.\nTherefore, $\\mathbf{U}$ is an upper triangular matrix with nonzeros restricted to the main diagonal, the first superdiagonal, and the $N_x$-th superdiagonal.\n\nWith the nonzero structures of $\\mathbf{L}$ and $\\mathbf{U}$ established, we can calculate the total number of nonzero entries, $\\mathrm{nnz}(\\mathbf{L}) + \\mathrm{nnz}(\\mathbf{U})$.\nLet $N = N_x N_y$ be the total number of interior points.\n\nFirst, let's calculate $\\mathrm{nnz}(\\mathbf{L})$:\n1.  Main diagonal: By definition, $\\mathbf{L}$ is unit triangular, so there are $N$ entries, all equal to $1$.\n2.  First subdiagonal (West connections): Nonzero entries $L_{k,k-1}$ exist for points not on the western boundary of the grid, i.e., for $i \\in \\{2, \\dots, N_x\\}$. There are $N_x-1$ such points in each of the $N_y$ rows. Total: $N_y(N_x-1)$.\n3.  $N_x$-th subdiagonal (South connections): Nonzero entries $L_{k,k-N_x}$ exist for points not in the first row of the grid, i.e., for $j \\in \\{2, \\dots, N_y\\}$. There are $N_y-1$ such rows, each with $N_x$ points. Total: $N_x(N_y-1)$.\nSo, the total number of nonzeros in $\\mathbf{L}$ is:\n$$ \\mathrm{nnz}(\\mathbf{L}) = N + N_y(N_x-1) + N_x(N_y-1) $$\n$$ \\mathrm{nnz}(\\mathbf{L}) = N_x N_y + N_x N_y - N_y + N_x N_y - N_x = 3N_x N_y - N_x - N_y $$\nNext, we calculate $\\mathrm{nnz}(\\mathbf{U})$:\n1.  Main diagonal: For M-matrices like the discrete Laplacian, the diagonal entries of the LU factors are guaranteed to be nonzero. There are $N$ such entries.\n2.  First superdiagonal (East connections): Nonzero entries $U_{k,k+1}$ exist for points not on the eastern boundary, i.e., for $i \\in \\{1, \\dots, N_x-1\\}$. There are $N_x-1$ such points in each of the $N_y$ rows. Total: $N_y(N_x-1)$.\n3.  $N_x$-th superdiagonal (North connections): Nonzero entries $U_{k,k+N_x}$ exist for points not in the last row, i.e., for $j \\in \\{1, \\dots, N_y-1\\}$. There are $N_y-1$ such rows, each with $N_x$ points. Total: $N_x(N_y-1)$.\nThe structure of nonzeros in the strictly upper part of $\\mathbf{U}$ is symmetric to the strictly lower part of $\\mathbf{L}$. Thus, the count is the same:\n$$ \\mathrm{nnz}(\\mathbf{U}) = N + N_y(N_x-1) + N_x(N_y-1) = 3N_x N_y - N_x - N_y $$\nThe total number of nonzeros is the sum $\\mathrm{nnz}(\\mathbf{L}) + \\mathrm{nnz}(\\mathbf{U})$:\n$$ \\mathrm{nnz}(\\mathbf{L}) + \\mathrm{nnz}(\\mathbf{U}) = (3N_x N_y - N_x - N_y) + (3N_x N_y - N_x - N_y) $$\n$$ \\mathrm{nnz}(\\mathbf{L}) + \\mathrm{nnz}(\\mathbf{U}) = 6N_x N_y - 2N_x - 2N_y $$\nAlternatively, we can reason that the total number of nonzero entries in the $\\text{ILU}(0)$ factors is the number of nonzeros in the original matrix $\\mathbf{A}$, plus an additional count for the main diagonal, which is stored in both $\\mathbf{L}$ (as ones) and $\\mathbf{U}$ (as computed values).\nThe number of nonzeros in $\\mathbf{A}$ is:\n$$ \\mathrm{nnz}(\\mathbf{A}) = N (\\text{diagonal}) + 2 \\times N_y(N_x-1) (\\text{horiz.}) + 2 \\times N_x(N_y-1) (\\text{vert.}) $$\n$$ \\mathrm{nnz}(\\mathbf{A}) = N_x N_y + 2(N_x N_y - N_y) + 2(N_x N_y - N_x) = 5N_x N_y - 2N_x - 2N_y $$\nThe total count is $\\mathrm{nnz}(\\mathbf{L}) + \\mathrm{nnz}(\\mathbf{U}) = \\mathrm{nnz}(\\mathbf{A}) + N$:\n$$ \\mathrm{nnz}(\\mathbf{L}) + \\mathrm{nnz}(\\mathbf{U}) = (5N_x N_y - 2N_x - 2N_y) + N_x N_y = 6N_x N_y - 2N_x - 2N_y $$\nBoth methods yield the same result.",
            "answer": "$$\n\\boxed{6N_x N_y - 2N_x - 2N_y}\n$$"
        }
    ]
}