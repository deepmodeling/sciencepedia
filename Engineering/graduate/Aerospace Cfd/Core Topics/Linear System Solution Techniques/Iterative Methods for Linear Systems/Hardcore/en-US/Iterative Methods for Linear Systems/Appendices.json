{
    "hands_on_practices": [
        {
            "introduction": "Before tackling the complex systems found in CFD, it is essential to master the fundamentals of convergence for simple iterative schemes. This first exercise focuses on the Jacobi method and its convergence criterion, which is governed by the spectral radius of the iteration matrix. By analyzing a small system, you will gain a concrete understanding of why an iterative method may or may not succeed, a principle that underlies all iterative techniques.",
            "id": "2182298",
            "problem": "Consider the linear system of equations $A\\mathbf{x} = \\mathbf{b}$, where the matrix $A$ and the vector $\\mathbf{b}$ are given by:\n$$\nA = \\begin{pmatrix} 5  2 \\\\ 1  -4 \\end{pmatrix}, \\quad \\mathbf{x} = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}, \\quad \\mathbf{b} = \\begin{pmatrix} 9 \\\\ -2 \\end{pmatrix}\n$$\nThe Jacobi method is an iterative algorithm for approximately solving this system. The convergence of this method depends on the spectral radius of its associated iteration matrix, $T_J$. Calculate the spectral radius, $\\rho(T_J)$, for the given matrix $A$.\n\nExpress your final answer as an exact value in the form $\\frac{\\sqrt{N}}{M}$ where $N$ and $M$ are positive integers.",
            "solution": "For the Jacobi method, we write the splitting $A = D + L + U$, where $D$ is the diagonal of $A$, and $L$ and $U$ are the strictly lower and upper triangular parts, respectively. The Jacobi iteration matrix is\n$$\nT_J = -D^{-1}(L+U).\n$$\nFor\n$$\nA = \\begin{pmatrix} 5  2 \\\\ 1  -4 \\end{pmatrix},\n$$\nwe have\n$$\nD = \\begin{pmatrix} 5  0 \\\\ 0  -4 \\end{pmatrix}, \\quad D^{-1} = \\begin{pmatrix} \\frac{1}{5}  0 \\\\ 0  -\\frac{1}{4} \\end{pmatrix}.\n$$\nThe off-diagonal part is $L+U = \\begin{pmatrix} 0  2 \\\\ 1  0 \\end{pmatrix}$. We compute\n$$\nT_J = -D^{-1}(L+U) = - \\begin{pmatrix} \\frac{1}{5}  0 \\\\ 0  -\\frac{1}{4} \\end{pmatrix} \\begin{pmatrix} 0  2 \\\\ 1  0 \\end{pmatrix} = - \\begin{pmatrix} 0  \\frac{2}{5} \\\\ -\\frac{1}{4}  0 \\end{pmatrix} = \\begin{pmatrix} 0  -\\frac{2}{5} \\\\ \\frac{1}{4}  0 \\end{pmatrix}.\n$$\nFor a matrix of the form $\\begin{pmatrix} 0  a \\\\ b  0 \\end{pmatrix}$, the characteristic polynomial is $\\lambda^{2} - ab = 0$, hence the eigenvalues are $\\lambda = \\pm \\sqrt{ab}$. Here $a = -\\frac{2}{5}$ and $b = \\frac{1}{4}$, so\n$$\nab = -\\frac{1}{10}, \\quad \\lambda^{2} = -\\frac{1}{10}, \\quad \\lambda = \\pm i \\frac{1}{\\sqrt{10}}.\n$$\nTherefore, the spectral radius is the maximum modulus of the eigenvalues,\n$$\n\\rho(T_J) = \\left| \\pm i \\frac{1}{\\sqrt{10}} \\right| = \\frac{1}{\\sqrt{10}} = \\frac{\\sqrt{10}}{10}.\n$$",
            "answer": "$$\\boxed{\\frac{\\sqrt{10}}{10}}$$"
        },
        {
            "introduction": "While stationary methods like Jacobi have clear convergence criteria based on the spectral radius , more advanced Krylov subspace methods like the Conjugate Gradient (CG) rely on different matrix properties. This exercise explores the theoretical underpinnings of the CG method, specifically its requirement for a symmetric positive-definite (SPD) matrix, by demonstrating how the algorithm breaks down when this condition is not met. Understanding these limitations is critical for correctly applying advanced solvers in practice.",
            "id": "3245204",
            "problem": "Consider applying the Conjugate Gradient (CG) method to the linear system $A \\mathbf{x} = \\mathbf{b}$ where $A$ is symmetric but not positive definite. The CG method is classically defined by minimizing the quadratic functional $\\phi(\\mathbf{x}) = \\tfrac{1}{2} \\mathbf{x}^{\\mathsf{T}} A \\mathbf{x} - \\mathbf{b}^{\\mathsf{T}} \\mathbf{x}$ along search directions $\\mathbf{p}_k$, with residuals $\\mathbf{r}_k = \\mathbf{b} - A \\mathbf{x}_k$, and the initial search direction $\\mathbf{p}_0 = \\mathbf{r}_0$. From first principles, the step length $\\alpha_k$ is determined by minimizing $\\phi(\\mathbf{x}_k + \\alpha \\mathbf{p}_k)$ with respect to the scalar $\\alpha$.\n\nWork with the explicit example\n$$\nA = \\begin{pmatrix} 1  0 \\\\ 0  -1 \\end{pmatrix}, \n\\quad \n\\mathbf{b} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}, \n\\quad \n\\mathbf{x}_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}.\n$$\nStarting from the definition of $\\phi(\\mathbf{x})$ and the directional derivative condition for optimality along $\\mathbf{p}_k$, derive the expression for the step length $\\alpha_0$ in terms of $\\mathbf{p}_0$ and $\\mathbf{r}_0$ and identify the denominator that must be strictly positive when $A$ is positive definite. Then, compute the scalar $\\mathbf{p}_0^{\\mathsf{T}} A \\mathbf{p}_0$ for the given $A$, $\\mathbf{b}$, and $\\mathbf{x}_0$, and use this value to explain whether the CG step is well-defined in this case and why the method breaks down for this non-positive-definite $A$.\n\nYour final reported answer should be the numerical value of $\\mathbf{p}_0^{\\mathsf{T}} A \\mathbf{p}_0$. No rounding is required.",
            "solution": "The problem asks for an analysis of the Conjugate Gradient (CG) method applied to a linear system $A \\mathbf{x} = \\mathbf{b}$ where the matrix $A$ is symmetric but not positive definite. We will first derive the general formula for the step length $\\alpha_k$ from first principles, then apply it to the specific case provided to demonstrate the method's breakdown.\n\nThe CG method aims to find the solution $\\mathbf{x}$ by minimizing the quadratic functional $\\phi(\\mathbf{x}) = \\frac{1}{2} \\mathbf{x}^{\\mathsf{T}} A \\mathbf{x} - \\mathbf{b}^{\\mathsf{T}} \\mathbf{x}$. The gradient of this functional is $\\nabla \\phi(\\mathbf{x}) = A \\mathbf{x} - \\mathbf{b}$, which is the negative of the residual, $\\mathbf{r} = \\mathbf{b} - A \\mathbf{x}$. The method proceeds iteratively by generating a sequence of approximations $\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k \\mathbf{p}_k$, where $\\mathbf{p}_k$ is a search direction and $\\alpha_k$ is a step length chosen to minimize $\\phi(\\mathbf{x}_k + \\alpha_k \\mathbf{p}_k)$.\n\nTo derive $\\alpha_k$, we define a function of a single variable $\\alpha$:\n$$f(\\alpha) = \\phi(\\mathbf{x}_k + \\alpha \\mathbf{p}_k) = \\frac{1}{2} (\\mathbf{x}_k + \\alpha \\mathbf{p}_k)^{\\mathsf{T}} A (\\mathbf{x}_k + \\alpha \\mathbf{p}_k) - \\mathbf{b}^{\\mathsf{T}} (\\mathbf{x}_k + \\alpha \\mathbf{p}_k)$$\nExpanding the terms and using the symmetry of $A$ ($A = A^{\\mathsf{T}}$), we get:\n$$f(\\alpha) = \\left(\\frac{1}{2} \\mathbf{x}_k^{\\mathsf{T}} A \\mathbf{x}_k - \\mathbf{b}^{\\mathsf{T}} \\mathbf{x}_k\\right) + \\alpha (\\mathbf{p}_k^{\\mathsf{T}} A \\mathbf{x}_k - \\mathbf{p}_k^{\\mathsf{T}} \\mathbf{b}) + \\frac{1}{2} \\alpha^2 (\\mathbf{p}_k^{\\mathsf{T}} A \\mathbf{p}_k)$$\nThe first term is $\\phi(\\mathbf{x}_k)$. The term linear in $\\alpha$ can be rewritten using the residual $\\mathbf{r}_k = \\mathbf{b} - A \\mathbf{x}_k$.\n$$ \\mathbf{p}_k^{\\mathsf{T}} A \\mathbf{x}_k - \\mathbf{p}_k^{\\mathsf{T}} \\mathbf{b} = - \\mathbf{p}_k^{\\mathsf{T}} (\\mathbf{b} - A \\mathbf{x}_k) = - \\mathbf{p}_k^{\\mathsf{T}} \\mathbf{r}_k$$\nSo, the function to minimize is:\n$$f(\\alpha) = \\phi(\\mathbf{x}_k) - \\alpha \\mathbf{p}_k^{\\mathsf{T}} \\mathbf{r}_k + \\frac{1}{2} \\alpha^2 (\\mathbf{p}_k^{\\mathsf{T}} A \\mathbf{p}_k)$$\nTo find the minimum, we take the derivative with respect to $\\alpha$ and set it to zero:\n$$\\frac{df}{d\\alpha} = -\\mathbf{p}_k^{\\mathsf{T}} \\mathbf{r}_k + \\alpha (\\mathbf{p}_k^{\\mathsf{T}} A \\mathbf{p}_k) = 0$$\nSolving for $\\alpha$ yields the step length $\\alpha_k$:\n$$\\alpha_k = \\frac{\\mathbf{p}_k^{\\mathsf{T}} \\mathbf{r}_k}{\\mathbf{p}_k^{\\mathsf{T}} A \\mathbf{p}_k}$$\nFor the first iteration ($k=0$), the search direction $\\mathbf{p}_0$ is set to be the initial residual $\\mathbf{r}_0$, so this becomes:\n$$\\alpha_0 = \\frac{\\mathbf{r}_0^{\\mathsf{T}} \\mathbf{r}_0}{\\mathbf{r}_0^{\\mathsf{T}} A \\mathbf{r}_0}$$\nThe denominator in this expression is $\\mathbf{p}_k^{\\mathsf{T}} A \\mathbf{p}_k$. For the CG method to be well-defined, this denominator must be non-zero. For the step to correspond to a minimum, the second derivative $\\frac{d^2f}{d\\alpha^2} = \\mathbf{p}_k^{\\mathsf{T}} A \\mathbf{p}_k$ must be positive. If $A$ is symmetric positive definite (SPD), then by definition, $\\mathbf{v}^{\\mathsf{T}} A \\mathbf{v} > 0$ for any non-zero vector $\\mathbf{v}$. Since the search directions $\\mathbf{p}_k$ are non-zero (unless the solution is found), the denominator is guaranteed to be strictly positive.\n\nNow we analyze the specific case:\n$$A = \\begin{pmatrix} 1  0 \\\\ 0  -1 \\end{pmatrix}, \\quad \\mathbf{b} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}, \\quad \\mathbf{x}_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$$\nThe matrix $A$ is symmetric, but its eigenvalues are $1$ and $-1$, so it is not positive definite. First, we compute the initial residual $\\mathbf{r}_0$:\n$$\\mathbf{r}_0 = \\mathbf{b} - A \\mathbf{x}_0 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 1  0 \\\\ 0  -1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$$\nThe initial search direction is $\\mathbf{p}_0 = \\mathbf{r}_0 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$. We compute the scalar $\\mathbf{p}_0^{\\mathsf{T}} A \\mathbf{p}_0$, which is the denominator for $\\alpha_0$:\n$$\\mathbf{p}_0^{\\mathsf{T}} A \\mathbf{p}_0 = \\begin{pmatrix} 1  1 \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 0  -1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$$\nFirst, we compute the product $A \\mathbf{p}_0$:\n$$A \\mathbf{p}_0 = \\begin{pmatrix} 1  0 \\\\ 0  -1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$$\nNow we compute the final inner product:\n$$\\mathbf{p}_0^{\\mathsf{T}} (A \\mathbf{p}_0) = \\begin{pmatrix} 1  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = (1)(1) + (1)(-1) = 1 - 1 = 0$$\nThe value of the denominator is $\\mathbf{p}_0^{\\mathsf{T}} A \\mathbf{p}_0 = 0$.\n\nThe CG step is not well-defined because calculating the step length $\\alpha_0$ requires division by zero. The numerator is $\\mathbf{r}_0^{\\mathsf{T}} \\mathbf{r}_0 = 1^2 + 1^2 = 2$. Therefore, $\\alpha_0 = \\frac{2}{0}$, which is undefined. The algorithm breaks down at the first iteration. This breakdown occurs because the condition $\\mathbf{p}_0^{\\mathsf{T}} A \\mathbf{p}_0 > 0$ is violated, demonstrating a fundamental reason why the standard CG method is restricted to systems with symmetric positive definite matrices.",
            "answer": "$$\\boxed{0}$$"
        },
        {
            "introduction": "Advanced solvers are powerful, but their performance on the large, sparse, and often ill-conditioned systems from CFD discretizations can be poor . To accelerate convergence, we use preconditioners. This final practice delves into constructing one of the most common types of preconditioners, the Incomplete LU (ILU) factorization, for a matrix representing the pressure Poisson equationâ€”a cornerstone of incompressible flow solvers. Analyzing the structure and cost of this preconditioner provides direct insight into the practical engineering of efficient CFD codes.",
            "id": "3969374",
            "problem": "Consider the pressure Poisson equation arising in a projection method for incompressible flow in aerospace Computational Fluid Dynamics (CFD), discretized on a rectangular domain using a uniform Cartesian grid with $N_x$ interior points in the $x$-direction and $N_y$ interior points in the $y$-direction, and Dirichlet boundary conditions on the perimeter. The second-order central finite-volume or finite-difference discretization leads to a sparse linear system $\\mathbf{A}\\mathbf{x}=\\mathbf{b}$, where $\\mathbf{A}$ corresponds to the standard two-dimensional five-point stencil with constant coefficients. Assume natural lexicographic row-wise ordering of the unknowns, i.e., indices increase first along $x$ within a row and then in $y$ between rows.\n\nStarting from core definitions of the five-point stencil arising from the discrete Laplacian and the Gaussian elimination procedure, construct the Incomplete LU factorization with zero fill, known as Incomplete LU (ILU) with level zero, denoted $\\text{ILU}(0)$, for $\\mathbf{A}$. The $\\text{ILU}(0)$ is defined by performing the standard LU factorization while enforcing that no fill-in is retained beyond the original sparsity pattern of $\\mathbf{A}$. Take $\\mathbf{L}$ to have a unit diagonal and $\\mathbf{U}$ to carry the diagonal.\n\nDerive the nonzero structure of $\\mathbf{L}$ and $\\mathbf{U}$ implied by the five-point stencil and the specified ordering, carefully justifying which neighbor couplings appear in the strictly lower part of $\\mathbf{L}$ and strictly upper part of $\\mathbf{U}$ under the no-fill constraint. Then, compute the exact total number of nonzero entries in $\\mathbf{L}$ plus $\\mathbf{U}$, denoted $\\mathrm{nnz}(\\mathbf{L})+\\mathrm{nnz}(\\mathbf{U})$, as a closed-form expression in terms of $N_x$ and $N_y$.\n\nProvide your final answer as a single closed-form analytic expression. No rounding is required. No physical units are associated with this count.",
            "solution": "The system of linear equations $\\mathbf{A}\\mathbf{x}=\\mathbf{b}$ is defined on a grid with $N_x \\times N_y$ interior points. The total number of unknowns, and thus the dimension of the square matrix $\\mathbf{A}$, is $N = N_x N_y$. The unknowns are ordered using a natural lexicographic scheme, where the index $k$ for a grid point $(i,j)$ with $i \\in \\{1, \\dots, N_x\\}$ and $j \\in \\{1, \\dots, N_y\\}$ is given by $k = (j-1)N_x + i$.\n\nThe matrix $\\mathbf{A}$ represents a discrete operator based on a five-point stencil. For a given point $(i,j)$ with global index $k$, it is coupled to its immediate neighbors:\n- West: $(i-1, j)$, with global index $k-1$ (if $i>1$).\n- East: $(i+1, j)$, with global index $k+1$ (if $iN_x$).\n- South: $(i, j-1)$, with global index $k-N_x$ (if $j>1$).\n- North: $(i, j+1)$, with global index $k+N_x$ (if $jN_y$).\n\nThis structure implies that the matrix $\\mathbf{A}$ has nonzero entries only on its main diagonal and on four off-diagonals at offsets $\\pm 1$ and $\\pm N_x$. The set of index pairs $(k,l)$ for which $A_{k,l}$ is potentially nonzero is called the sparsity pattern of $\\mathbf{A}$, denoted $S(\\mathbf{A})$.\n\nThe Incomplete LU factorization with zero fill-in, $\\text{ILU}(0)$, produces a lower triangular matrix $\\mathbf{L}$ and an upper triangular matrix $\\mathbf{U}$ such that $\\mathbf{A} \\approx \\mathbf{L}\\mathbf{U}$. The defining constraint of $\\text{ILU}(0)$ is that the sparsity patterns of $\\mathbf{L}$ and $\\mathbf{U}$ must be subsets of the original matrix's sparsity pattern. For $\\text{ILU}(0)$, the sparsity pattern of the factors is taken to be exactly the corresponding parts of $S(\\mathbf{A})$.\n\nThe problem specifies that $\\mathbf{L}$ has a unit diagonal ($L_{k,k}=1$ for all $k$) and $\\mathbf{U}$ carries the main diagonal entries of the factorization.\nThe nonzero structure of $\\mathbf{L}$ is thus defined by the strictly lower part of $\\mathbf{A}$'s sparsity pattern, plus its own unit diagonal. A nonzero entry $L_{k,l}$ for $k > l$ exists only if $A_{k,l}$ is nonzero. Based on the lexicographic ordering:\n- $A_{k, k-1}$ is nonzero (West neighbor), populating the first subdiagonal of $\\mathbf{L}$.\n- $A_{k, k-N_x}$ is nonzero (South neighbor), populating the $N_x$-th subdiagonal of $\\mathbf{L}$.\nTherefore, $\\mathbf{L}$ is a unit lower triangular matrix with nonzeros restricted to the main diagonal, the first subdiagonal, and the $N_x$-th subdiagonal.\n\nThe nonzero structure of $\\mathbf{U}$ is defined by the upper triangular part (including the diagonal) of $\\mathbf{A}$'s sparsity pattern. A nonzero entry $U_{k,l}$ for $k \\le l$ exists only if $A_{k,l}$ is nonzero.\n- $A_{k,k}$ is nonzero, populating the main diagonal of $\\mathbf{U}$.\n- $A_{k, k+1}$ is nonzero (East neighbor), populating the first superdiagonal of $\\mathbf{U}$.\n- $A_{k, k+N_x}$ is nonzero (North neighbor), populating the $N_x$-th superdiagonal of $\\mathbf{U}$.\nTherefore, $\\mathbf{U}$ is an upper triangular matrix with nonzeros restricted to the main diagonal, the first superdiagonal, and the $N_x$-th superdiagonal.\n\nWith the nonzero structures established, we can calculate the total number of nonzero entries, $\\mathrm{nnz}(\\mathbf{L}) + \\mathrm{nnz}(\\mathbf{U})$. Let $N = N_x N_y$.\n\nFirst, we calculate $\\mathrm{nnz}(\\mathbf{L})$:\n1.  Main diagonal: There are $N$ entries, all equal to $1$.\n2.  First subdiagonal (West connections): Nonzero entries $L_{k,k-1}$ exist for points not on the western boundary of the grid ($i > 1$). This occurs for $N_x-1$ points in each of the $N_y$ rows. Total: $N_y(N_x-1)$.\n3.  $N_x$-th subdiagonal (South connections): Nonzero entries $L_{k,k-N_x}$ exist for points not in the first row of the grid ($j > 1$). This occurs for all $N_x$ points in each of the $N_y-1$ upper rows. Total: $N_x(N_y-1)$.\nSo, $\\mathrm{nnz}(\\mathbf{L}) = N + N_y(N_x-1) + N_x(N_y-1) = N_x N_y + (N_x N_y - N_y) + (N_x N_y - N_x) = 3N_x N_y - N_x - N_y$.\n\nNext, we calculate $\\mathrm{nnz}(\\mathbf{U})$:\n1.  Main diagonal: There are $N$ entries.\n2.  First superdiagonal (East connections): The number of these connections is the same as the West connections: $N_y(N_x-1)$.\n3.  $N_x$-th superdiagonal (North connections): The number of these connections is the same as the South connections: $N_x(N_y-1)$.\nSo, $\\mathrm{nnz}(\\mathbf{U}) = N + N_y(N_x-1) + N_x(N_y-1) = 3N_x N_y - N_x - N_y$.\n\nThe total number of nonzeros is the sum $\\mathrm{nnz}(\\mathbf{L}) + \\mathrm{nnz}(\\mathbf{U})$:\n$$ \\mathrm{nnz}(\\mathbf{L}) + \\mathrm{nnz}(\\mathbf{U}) = (3N_x N_y - N_x - N_y) + (3N_x N_y - N_x - N_y) = 6N_x N_y - 2N_x - 2N_y $$",
            "answer": "$$\n\\boxed{6N_x N_y - 2N_x - 2N_y}\n$$"
        }
    ]
}