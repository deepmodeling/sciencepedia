{
    "hands_on_practices": [
        {
            "introduction": "The first step in mastering any numerical method is to implement it directly from its definition. This exercise provides a foundational coding challenge to implement both the Jacobi and Gauss-Seidel iterations. By applying these methods to a series of well-behaved linear systems, you will gain firsthand experience with their mechanics and observe the typical convergence advantage of Gauss-Seidel, which incorporates updated information more rapidly. ",
            "id": "2406968",
            "problem": "Let $A \\in \\mathbb{R}^{n \\times n}$ be decomposed as $A = D + L + U$, where $D$ is the diagonal of $A$, $L$ is the strict lower triangular part of $A$, and $U$ is the strict upper triangular part of $A$. For a given right-hand side $b \\in \\mathbb{R}^n$ and initial iterate $x^{(0)} \\in \\mathbb{R}^n$, define the Jacobi iteration by\n$$\nx^{(k+1)} = D^{-1}\\bigl(b - (L + U)x^{(k)}\\bigr),\n$$\nand the Gauss–Seidel iteration by\n$$\nx^{(k+1)} = (D + L)^{-1}\\bigl(b - U x^{(k)}\\bigr).\n$$\nFor each linear system below, use the same initial vector $x^{(0)} = 0$ (the zero vector), and perform exactly $10$ iterations of each method to obtain $x_{\\mathrm{J}}^{(10)}$ and $x_{\\mathrm{GS}}^{(10)}$. Let $x^\\star$ denote the exact solution of $A x = b$. For each test case, compute the following two quantities:\n1. The Euclidean norm of the difference between the two iterates, $d = \\lVert x_{\\mathrm{J}}^{(10)} - x_{\\mathrm{GS}}^{(10)} \\rVert_2$, rounded to six decimal places.\n2. The boolean value of whether the Gauss–Seidel iterate is strictly closer to the exact solution in the Euclidean norm than the Jacobi iterate, that is, whether $\\lVert x_{\\mathrm{GS}}^{(10)} - x^\\star \\rVert_2  \\lVert x_{\\mathrm{J}}^{(10)} - x^\\star \\rVert_2$ holds.\n\nAll arithmetic is over the real numbers. No physical units are involved.\n\nTest suite (each test case is a pair $\\{A, b\\}$):\n- Test case $1$ ($n=3$):\n$$\nA = \\begin{bmatrix}\n4  -1  0\\\\\n-1  4  -1\\\\\n0  -1  3\n\\end{bmatrix},\\quad\nb = \\begin{bmatrix}\n15\\\\\n10\\\\\n10\n\\end{bmatrix}.\n$$\n- Test case $2$ ($n=2$):\n$$\nA = \\begin{bmatrix}\n1.001  1.0\\\\\n1.0  1.001\n\\end{bmatrix},\\quad\nb = \\begin{bmatrix}\n2.0\\\\\n2.0\n\\end{bmatrix}.\n$$\n- Test case $3$ ($n=3$):\n$$\nA = \\begin{bmatrix}\n5  -2  1\\\\\n1  6  -2\\\\\n0.5  -1  4\n\\end{bmatrix},\\quad\nb = \\begin{bmatrix}\n12\\\\\n-1\\\\\n3\n\\end{bmatrix}.\n$$\n- Test case $4$ ($n=4$):\n$$\nA = \\begin{bmatrix}\n2  -1  0  0\\\\\n-1  2  -1  0\\\\\n0  -1  2  -1\\\\\n0  0  -1  2\n\\end{bmatrix},\\quad\nb = \\begin{bmatrix}\n1\\\\\n0\\\\\n0\\\\\n1\n\\end{bmatrix}.\n$$\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list of per-test-case pairs, enclosed in square brackets. Each pair must be a two-element list of the form $[d,\\ t]$, where $d$ is the rounded float defined above and $t$ is the boolean defined above. For example, the overall format must be\n$$\n\\bigl[[d_1,\\ t_1],[d_2,\\ t_2],[d_3,\\ t_3],[d_4,\\ t_4]\\bigr].\n$$",
            "solution": "The problem statement is critically examined and found to be valid. It is scientifically grounded in the field of numerical linear algebra, well-posed with sufficient and consistent data, and objectively formulated. All matrices provided are non-singular (specifically, they are strictly or irreducibly diagonally dominant), which guarantees the convergence of both the Jacobi and Gauss–Seidel methods to a unique solution. The task is to implement these iterative methods, carry out a specified number of iterations, and compute well-defined comparison metrics.\n\nThe objective is to solve the linear system $Ax = b$ using two iterative schemes: the Jacobi method and the Gauss–Seidel method. For each of the $4$ provided test cases, we must start with an initial guess of the zero vector, $x^{(0)} = 0$, and perform exactly $k_{\\max} = 10$ iterations. Subsequently, we compute the Euclidean norm of the difference between the resulting iterates, $d = \\lVert x_{\\mathrm{J}}^{(10)} - x_{\\mathrm{GS}}^{(10)} \\rVert_2$, and determine if the Gauss–Seidel iterate is strictly closer to the exact solution $x^\\star$, i.e., if $\\lVert x_{\\mathrm{GS}}^{(10)} - x^\\star \\rVert_2  \\lVert x_{\\mathrm{J}}^{(10)} - x^\\star \\rVert_2$.\n\nThe core of both methods lies in the decomposition of the matrix $A$ into its diagonal ($D$), strict lower triangular ($L$), and strict upper triangular ($U$) components, such that $A = D + L + U$. The system $Ax=b$ can be rewritten as $(D + L + U)x = b$.\n\n**Jacobi Method**\n\nThe Jacobi iteration is derived by rearranging the system as $Dx = b - (L+U)x$. Assuming $D$ is invertible (i.e., no zero diagonal entries), the iterative formula is:\n$$\nx^{(k+1)} = D^{-1}\\bigl(b - (L + U)x^{(k)}\\bigr)\n$$\nThis update rule is explicit: each component of the new iterate $x^{(k+1)}$ can be computed simultaneously using only the components of the previous iterate $x^{(k)}$. The implementation will first compute the matrices $D^{-1}$ and $L+U$. Then, a loop will run for $10$ iterations, applying the above matrix-vector formula to update the solution vector $x_{\\mathrm{J}}$.\n\n**Gauss–Seidel Method**\n\nThe Gauss–Seidel method modifies the update by using the most recently computed information available. The system is rearranged as $(D+L)x = b - Ux$, leading to the iterative formula:\n$$\nx^{(k+1)} = (D + L)^{-1}\\bigl(b - U x^{(k)}\\bigr)\n$$\nThis is equivalent to solving the lower triangular system $(D + L)x^{(k+1)} = b - U x^{(k)}$ for $x^{(k+1)}$ at each iteration $k$. This system can be solved efficiently using forward substitution. The implementation will compute the matrices $D+L$ and $U$. In each of the $10$ iterations, the right-hand side vector $v^{(k)} = b - U x_{\\mathrm{GS}}^{(k)}$ is first computed. Then, the lower triangular system $(D+L)x_{\\mathrm{GS}}^{(k+1)} = v^{(k)}$ is solved to find the next iterate $x_{\\mathrm{GS}}^{(k+1)}$. This is accomplished using a specialized triangular solver, `scipy.linalg.solve_triangular`.\n\n**Calculation of Required Quantities**\n\nFor each test case, after obtaining the tenth iterates $x_{\\mathrm{J}}^{(10)}$ and $x_{\\mathrm{GS}}^{(10)}$, we proceed as follows:\n1.  The exact solution $x^\\star$ is computed by solving $Ax=b$ directly using a standard, high-precision linear solver, specifically `numpy.linalg.solve(A, b)`.\n2.  The quantity $d$ is calculated as the Euclidean norm of the difference vector: $d = \\lVert x_{\\mathrm{J}}^{(10)} - x_{\\mathrm{GS}}^{(10)} \\rVert_2$. The result is rounded to $6$ decimal places as required.\n3.  The boolean value $t$ is determined by comparing the Euclidean norms of the error vectors for each method. Let $e_{\\mathrm{J}} = \\lVert x_{\\mathrm{J}}^{(10)} - x^\\star \\rVert_2$ and $e_{\\mathrm{GS}} = \\lVert x_{\\mathrm{GS}}^{(10)} - x^\\star \\rVert_2$. The boolean $t$ is the result of the comparison $e_{\\mathrm{GS}}  e_{\\mathrm{J}}$.\n\nThis procedure will be systematically applied to each of the $4$ test cases defined in the problem statement. The final output will be constructed by collecting the pair $[d, t]$ for each test case into a list.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import solve_triangular\n\ndef solve():\n    \"\"\"\n    Solves the given problem by implementing Jacobi and Gauss-Seidel iterations\n    for a suite of test cases and computing specified comparison metrics.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (\n            np.array([[4, -1, 0], [-1, 4, -1], [0, -1, 3]], dtype=float),\n            np.array([15, 10, 10], dtype=float)\n        ),\n        (\n            np.array([[1.001, 1.0], [1.0, 1.001]], dtype=float),\n            np.array([2.0, 2.0], dtype=float)\n        ),\n        (\n            np.array([[5, -2, 1], [1, 6, -2], [0.5, -1, 4]], dtype=float),\n            np.array([12, -1, 3], dtype=float)\n        ),\n        (\n            np.array([[2, -1, 0, 0], [-1, 2, -1, 0], [0, -1, 2, -1], [0, 0, -1, 2]], dtype=float),\n            np.array([1, 0, 0, 1], dtype=float)\n        )\n    ]\n\n    results = []\n    num_iterations = 10\n\n    for A, b in test_cases:\n        n = A.shape[0]\n\n        # Initialize solution vectors\n        x_J = np.zeros(n, dtype=float)\n        x_GS = np.zeros(n, dtype=float)\n\n        # Decompose matrix A\n        D = np.diag(np.diag(A))\n        L = np.tril(A, k=-1)\n        U = np.triu(A, k=1)\n\n        # --- Jacobi Iteration ---\n        D_inv = np.diag(1 / np.diag(A))\n        L_plus_U = L + U\n        for _ in range(num_iterations):\n            x_J = D_inv @ (b - L_plus_U @ x_J)\n\n        # --- Gauss-Seidel Iteration ---\n        D_plus_L = D + L\n        for _ in range(num_iterations):\n            v = b - U @ x_GS\n            x_GS = solve_triangular(D_plus_L, v, lower=True)\n            \n        # --- Compute Exact Solution ---\n        x_star = np.linalg.solve(A, b)\n\n        # --- Calculate required quantities ---\n        # 1. Norm of the difference between iterates\n        d = np.linalg.norm(x_J - x_GS)\n        d_rounded = round(d, 6)\n        \n        # 2. Boolean comparison of errors\n        error_J = np.linalg.norm(x_J - x_star)\n        error_GS = np.linalg.norm(x_GS - x_star)\n        is_GS_closer = error_GS  error_J\n        \n        results.append([d_rounded, is_GS_closer])\n\n    # Final print statement in the exact required format.\n    # Format: [[d1, t1], [d2, t2], ...] with lowercase booleans\n    output_parts = []\n    for d_val, t_val in results:\n        t_str = 'true' if t_val else 'false'\n        output_parts.append(f\"[{d_val},{t_str}]\")\n    \n    print(f\"[{','.join(output_parts)}]\")\n\n\nsolve()\n```"
        },
        {
            "introduction": "The convergence of iterative methods is critically dependent on the properties of the system matrix, particularly diagonal dominance. This practice demonstrates that a seemingly non-convergent system is not always a lost cause. You will analyze how a simple row permutation can restructure the matrix to achieve strict diagonal dominance, thereby transforming a divergent Jacobi method into a convergent one, which highlights the importance of pre-processing in computational engineering. ",
            "id": "2406931",
            "problem": "In computational engineering, the convergence of the Jacobi iterative method for solving a linear system depends on the iteration matrix derived from the system matrix. Consider the parametric family of linear systems $A(t)\\,x=b$ with\n$$\nA(t) \\;=\\; \\begin{pmatrix}\nt  5  0\\\\\n0  t  5\\\\\n5  0  t\n\\end{pmatrix}, \n\\qquad\nb \\;=\\; \\begin{pmatrix}\n1\\\\\n1\\\\\n1\n\\end{pmatrix},\n$$\nwhere $t$ is a positive real parameter. Let the row-permutation matrix\n$$\nP \\;=\\; \\begin{pmatrix}\n0  0  1\\\\\n1  0  0\\\\\n0  1  0\n\\end{pmatrix}\n$$\nbe applied to obtain the permuted system $P\\,A(t)\\,x = P\\,b$. Take $t=4$. Analyze the Jacobi method for both the original and the permuted systems in terms of their iteration matrices and convergence properties based on first principles.\n\nCompute the spectral radius of the Jacobi iteration matrix for the permuted system at $t=4$. Round your answer to four significant figures.",
            "solution": "The validation of the problem statement is the first and most critical step.\n\n**Step 1: Extracted Givens**\n- A parametric family of linear systems is defined as $A(t)\\,x=b$.\n- The system matrix is $A(t) = \\begin{pmatrix} t  5  0\\\\ 0  t  5\\\\ 5  0  t \\end{pmatrix}$.\n- The right-hand side vector is $b = \\begin{pmatrix} 1\\\\ 1\\\\ 1 \\end{pmatrix}$.\n- The parameter $t$ is a positive real number.\n- A row-permutation matrix is given: $P = \\begin{pmatrix} 0  0  1\\\\ 1  0  0\\\\ 0  1  0 \\end{pmatrix}$.\n- The permuted system is $P\\,A(t)\\,x = P\\,b$.\n- The specific parameter value is $t=4$.\n- The task is to analyze the Jacobi method for both the original and permuted systems at $t=4$ and to compute the spectral radius of the Jacobi iteration matrix for the permuted system.\n- The numerical result must be rounded to four significant figures.\n\n**Step 2: Validation Verdict**\nThe problem is scientifically grounded, concerning standard topics in numerical linear algebra—specifically, the Jacobi iterative method, convergence criteria, and the effect of matrix properties like diagonal dominance. It is well-posed, with all necessary data provided for a unique solution. The language is objective and unambiguous. The problem is therefore deemed **valid**.\n\nProceeding to solution.\n\nThe Jacobi method is an iterative procedure for solving a system of linear equations $M x = c$. The matrix $M$ is decomposed as $M = D - L - U$, where $D$ is the diagonal part of $M$, $-L$ is the strictly lower-triangular part, and $-U$ is the strictly upper-triangular part. The iteration is defined by the formula:\n$$ x^{(k+1)} = D^{-1}(L+U) x^{(k)} + D^{-1}c $$\nThe matrix $T_J = D^{-1}(L+U)$ is the Jacobi iteration matrix. The iterative process converges for any initial vector $x^{(0)}$ if and only if the spectral radius of $T_J$, denoted $\\rho(T_J)$, is strictly less than $1$. The spectral radius is the maximum absolute value among the eigenvalues of $T_J$.\n\n**Analysis of the Original System**\nFor the parameter value $t=4$, the original system matrix is:\n$$ A(4) = \\begin{pmatrix} 4  5  0\\\\ 0  4  5\\\\ 5  0  4 \\end{pmatrix} $$\nThis matrix is not strictly diagonally dominant. For example, in the first row, the absolute value of the diagonal element, $|4|$, is not greater than the sum of the absolute values of the off-diagonal elements, $|5| + |0| = 5$. This lack of diagonal dominance suggests that the Jacobi method may not converge.\n\nWe decompose $A(4)$ into its components $D$, $L$, and $U$:\n$$ D = \\begin{pmatrix} 4  0  0\\\\ 0  4  0\\\\ 0  0  4 \\end{pmatrix}, \\quad L = \\begin{pmatrix} 0  0  0\\\\ 0  0  0\\\\ -5  0  0 \\end{pmatrix}, \\quad U = \\begin{pmatrix} 0  -5  0\\\\ 0  0  -5\\\\ 0  0  0 \\end{pmatrix} $$\nThe Jacobi iteration matrix $T_{J,A}$ for the original system is $T_{J,A} = D^{-1}(L+U)$.\n$$ T_{J,A} = \\begin{pmatrix} \\frac{1}{4}  0  0\\\\ 0  \\frac{1}{4}  0\\\\ 0  0  \\frac{1}{4} \\end{pmatrix} \\begin{pmatrix} 0  -5  0\\\\ 0  0  -5\\\\ -5  0  0 \\end{pmatrix} = \\begin{pmatrix} 0  -\\frac{5}{4}  0\\\\ 0  0  -\\frac{5}{4}\\\\ -\\frac{5}{4}  0  0 \\end{pmatrix} $$\nThe eigenvalues $\\lambda$ of $T_{J,A}$ are found from the characteristic equation $\\det(T_{J,A} - \\lambda I) = 0$:\n$$ \\det \\begin{pmatrix} -\\lambda  -\\frac{5}{4}  0\\\\ 0  -\\lambda  -\\frac{5}{4}\\\\ -\\frac{5}{4}  0  -\\lambda \\end{pmatrix} = 0 $$\nExpanding the determinant yields:\n$$ -\\lambda^3 - \\left(\\frac{5}{4}\\right)^3 = 0 \\implies \\lambda^3 = -\\left(\\frac{5}{4}\\right)^3 $$\nThe magnitude of each eigenvalue $\\lambda$ is given by $|\\lambda|^3 = |-\\left(\\frac{5}{4}\\right)^3| = \\left(\\frac{5}{4}\\right)^3$, which implies $|\\lambda| = \\frac{5}{4}$.\nThe spectral radius is therefore $\\rho(T_{J,A}) = \\frac{5}{4} = 1.25$. Since $\\rho(T_{J,A})  1$, the Jacobi method for the original system is divergent.\n\n**Analysis of the Permuted System**\nThe permuted system is characterized by the matrix $A' = P A(4)$:\n$$ A' = P A(4) = \\begin{pmatrix} 0  0  1\\\\ 1  0  0\\\\ 0  1  0 \\end{pmatrix} \\begin{pmatrix} 4  5  0\\\\ 0  4  5\\\\ 5  0  4 \\end{pmatrix} = \\begin{pmatrix} 5  0  4\\\\ 4  5  0\\\\ 0  4  5 \\end{pmatrix} $$\nWe inspect the diagonal dominance of $A'$. For each row $i$, we check if $|a'_{ii}|  \\sum_{j \\neq i} |a'_{ij}|$.\n- Row $1$: $|5|  |0| + |4| \\implies 5  4$.\n- Row $2$: $|5|  |4| + |0| \\implies 5  4$.\n- Row $3$: $|5|  |0| + |4| \\implies 5  4$.\nThe matrix $A'$ is strictly diagonally dominant. This is a sufficient condition for the convergence of the Jacobi method. We will now compute the spectral radius to confirm convergence and quantify the rate.\n\nWe decompose $A'$ as $A' = D' - L' - U'$:\n$$ D' = \\begin{pmatrix} 5  0  0\\\\ 0  5  0\\\\ 0  0  5 \\end{pmatrix}, \\quad L' = \\begin{pmatrix} 0  0  0\\\\ -4  0  0\\\\ 0  -4  0 \\end{pmatrix}, \\quad U' = \\begin{pmatrix} 0  0  -4\\\\ 0  0  0\\\\ 0  0  0 \\end{pmatrix} $$\nThe Jacobi iteration matrix $T_{J,A'}$ for the permuted system is:\n$$ T_{J,A'} = (D')^{-1}(L'+U') = \\begin{pmatrix} \\frac{1}{5}  0  0\\\\ 0  \\frac{1}{5}  0\\\\ 0  0  \\frac{1}{5} \\end{pmatrix} \\begin{pmatrix} 0  0  -4\\\\ -4  0  0\\\\ 0  -4  0 \\end{pmatrix} = \\begin{pmatrix} 0  0  -\\frac{4}{5}\\\\ -\\frac{4}{5}  0  0\\\\ 0  -\\frac{4}{5}  0 \\end{pmatrix} $$\nThe eigenvalues $\\lambda'$ of $T_{J,A'}$ are the roots of the characteristic equation $\\det(T_{J,A'} - \\lambda' I) = 0$:\n$$ \\det \\begin{pmatrix} -\\lambda'  0  -\\frac{4}{5}\\\\ -\\frac{4}{5}  -\\lambda'  0\\\\ 0  -\\frac{4}{5}  -\\lambda' \\end{pmatrix} = 0 $$\nExpanding this determinant gives:\n$$ -(\\lambda')^3 - \\left(\\frac{4}{5}\\right)^3 = 0 \\implies (\\lambda')^3 = -\\left(\\frac{4}{5}\\right)^3 $$\nThe magnitude of each eigenvalue $\\lambda'$ is $|\\lambda'|^3 = |-\\left(\\frac{4}{5}\\right)^3| = \\left(\\frac{4}{5}\\right)^3$, which implies $|\\lambda'| = \\frac{4}{5}$.\nThe spectral radius of the Jacobi iteration matrix for the permuted system is $\\rho(T_{J,A'}) = \\max_i |\\lambda'_i| = \\frac{4}{5} = 0.8$.\nSince $\\rho(T_{J,A'})  1$, the Jacobi method converges for the permuted system, which is consistent with the property of strict diagonal dominance.\n\nThe problem requires the numerical value of this spectral radius rounded to four significant figures.\nThe value is $0.8$. Expressed with four significant figures, this is $0.8000$.",
            "answer": "$$\\boxed{0.8000}$$"
        },
        {
            "introduction": "We now apply our understanding of iterative solvers to a classic problem in fluid dynamics and heat transfer: the Laplace equation. This capstone exercise involves discretizing the PDE and solving the resulting large, sparse linear system. You will compare the performance of Jacobi and Gauss-Seidel and also implement the red-black ordering for Gauss-Seidel, a key technique for parallelizing the algorithm on modern hardware architectures. ",
            "id": "2406990",
            "problem": "You are to implement iterative solvers for the discrete two-dimensional Laplace equation on the unit square using three methods and to instrument the implementation to evaluate the effect of a red-black checkerboard ordering for parallelizing the Gauss–Seidel method.\n\nThe mathematical base is as follows. Consider the Laplace equation on the unit square with Dirichlet boundary conditions, namely the partial differential equation $\\nabla^2 u(x,y) = 0$ on $(x,y) \\in (0,1)\\times(0,1)$ together with prescribed values of $u$ on the boundary. Discretize this equation using a uniform Cartesian grid with $N_x$ interior points in the $x$-direction and $N_y$ interior points in the $y$-direction. The grid spacings are $h_x = 1/(N_x+1)$ and $h_y = 1/(N_y+1)$. Index interior points by integers $i = 1,\\dots,N_y$ and $j = 1,\\dots,N_x$, and include a one-point-thick boundary layer on all sides. Assume homogeneous Dirichlet boundary conditions on the left, right, and bottom sides, namely $u(0,y)=0$, $u(1,y)=0$, and $u(x,0)=0$, and a nonhomogeneous Dirichlet boundary condition on the top side given by $u(x,1)=g(x)$. The function $g(x)$ is specified below; angles must be interpreted in radians.\n\nFrom the standard second-order finite-difference discretization of the Laplacian, the discrete equation at every interior point $(i,j)$ is obtained by enforcing that the five-point stencil approximation of $\\nabla^2 u$ equals zero, leading to a linear system $A \\mathbf{u} = \\mathbf{b}$ with a sparse, symmetric positive definite matrix $A$. You may assume this fundamental construction and derive any iterative fixed-point updates you require from it. Do not introduce shortcuts that bypass this derivation.\n\nYour tasks are:\n\n- Implement the Jacobi method applied to the discrete Laplace equation.\n- Implement the lexicographic Gauss–Seidel method applied to the same equation.\n- Implement the red-black Gauss–Seidel method that applies a two-color checkerboard ordering to the grid, which is valid because each interior point couples only to its four axis-aligned neighbors, thereby forming a bipartite graph. In this ordering, all points of one color can be updated simultaneously from the opposite color, followed by the other color, to emulate data-parallel updates.\n- Use the infinity norm of the discrete residual as the stopping criterion. At an interior point $(i,j)$, the discrete residual is the value of the five-point finite-difference Laplacian applied to the current approximation. Compute the infinity norm as the maximum absolute residual over all interior points. Stop an iterative method when this infinity norm is less than a given tolerance $tol$.\n- For each method, count the number of full iterations. For Jacobi, a full iteration is one global sweep producing a new iterate. For lexicographic Gauss–Seidel, a full iteration is one sweep over all interior points. For red-black Gauss–Seidel, a full iteration consists of one red sweep followed by one black sweep.\n\nBoundary data specification is as follows. Use $g(x) = \\sin(\\pi x)$, so that $g(0)=0$ and $g(1)=0$. This choice ensures compatibility at the corners with the homogeneous boundary values on the left and right sides.\n\nNumerical details:\n\n- Initialize the interior values to zero.\n- Keep boundary values fixed at all times.\n- Use the discrete residual infinity norm as defined above to test for convergence.\n- Use a maximum number of iterations $k_{\\max}$; declare nonconvergence if $k_{\\max}$ is reached without satisfying the tolerance.\n\nDesign a test suite comprising four cases to exercise different aspects of the implementation:\n\n- Case $1$ (square grid, even counts): $N_x = 8$, $N_y = 8$, $tol = 10^{-6}$, $k_{\\max} = 50000$.\n- Case $2$ (square grid, odd counts): $N_x = 7$, $N_y = 7$, $tol = 10^{-6}$, $k_{\\max} = 50000$.\n- Case $3$ (rectangular grid): $N_x = 8$, $N_y = 12$, $tol = 10^{-6}$, $k_{\\max} = 50000$.\n- Case $4$ (minimal interior): $N_x = 1$, $N_y = 1$, $tol = 10^{-12}$, $k_{\\max} = 50000$.\n\nFor each test case, your program must compute the following four values:\n\n- The integer number of iterations required by Jacobi to satisfy the tolerance, or $k_{\\max}$ if it does not converge within the limit.\n- The integer number of iterations required by red-black Gauss–Seidel to satisfy the tolerance, or $k_{\\max}$ if it does not converge within the limit.\n- The integer number of iterations required by lexicographic Gauss–Seidel to satisfy the tolerance, or $k_{\\max}$ if it does not converge within the limit.\n- A boolean indicating whether the infinity norm of the difference between the converged red-black Gauss–Seidel solution and the converged lexicographic Gauss–Seidel solution is less than $10^{-10}$.\n\nFinal output format:\n\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list should contain four sublists, one per test case, in the order of cases $1$ through $4$. Each sublist must be of the form $[n_J, n_{RBGS}, n_{GS}, ok]$, where $n_J$, $n_{RBGS}$, and $n_{GS}$ are integers and $ok$ is a boolean. For example: $[[12,8,7,true],[...],...]$.\n\nAngles must be in radians. There are no physical units required for this problem. All numerical values in your derivation and implementation should be handled as real numbers.",
            "solution": "The problem presented is a valid, well-posed exercise in computational engineering. It requires the implementation and comparison of three classical iterative methods for solving the linear system arising from the finite-difference discretization of the two-dimensional Laplace equation. The problem statement is scientifically sound, self-contained, and objective. I will proceed with the derivation and solution.\n\nThe governing partial differential equation is the Laplace equation on the unit square:\n$$ \\nabla^2 u(x,y) = \\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2} = 0 \\quad \\text{for} \\quad (x,y) \\in (0,1) \\times (0,1) $$\nwith specified Dirichlet boundary conditions:\n$$ u(x,0) = 0, \\quad u(x,1) = g(x) = \\sin(\\pi x), \\quad u(0,y) = 0, \\quad u(1,y) = 0 $$\nThe domain is discretized using a uniform Cartesian grid with $N_x$ interior points in the $x$-direction and $N_y$ interior points in the $y$-direction. The grid spacings are $h_x = 1/(N_x+1)$ and $h_y = 1/(N_y+1)$. The solution $u(x,y)$ is approximated at grid points $(x_j, y_i)$ by the discrete values $u_{i,j}$, where $x_j = j h_x$ and $y_i = i h_y$. The indices for interior points are $i \\in \\{1, \\dots, N_y\\}$ and $j \\in \\{1, \\dots, N_x\\}$. The full grid includes boundary points, with $i \\in \\{0, \\dots, N_y+1\\}$ and $j \\in \\{0, \\dots, N_x+1\\}$.\n\nThe second-order partial derivatives are approximated using central finite differences:\n$$ \\frac{\\partial^2 u}{\\partial x^2}\\bigg|_{(i,j)} \\approx \\frac{u_{i,j-1} - 2u_{i,j} + u_{i,j+1}}{h_x^2} $$\n$$ \\frac{\\partial^2 u}{\\partial y^2}\\bigg|_{(i,j)} \\approx \\frac{u_{i-1,j} - 2u_{i,j} + u_{i+1,j}}{h_y^2} $$\nSubstituting these into the Laplace equation yields the five-point stencil equation for each interior grid point $(i,j)$:\n$$ \\frac{u_{i,j-1} - 2u_{i,j} + u_{i,j+1}}{h_x^2} + \\frac{u_{i-1,j} - 2u_{i,j} + u_{i+1,j}}{h_y^2} = 0 $$\nTo form a fixed-point iteration, we solve for $u_{i,j}$:\n$$ 2 \\left( \\frac{1}{h_x^2} + \\frac{1}{h_y^2} \\right) u_{i,j} = \\frac{u_{i,j-1} + u_{i,j+1}}{h_x^2} + \\frac{u_{i-1,j} + u_{i+1,j}}{h_y^2} $$\nThis can be rearranged into the iterative update form:\n$$ u_{i,j} = C_x (u_{i,j-1} + u_{i,j+1}) + C_y (u_{i-1,j} + u_{i+1,j}) $$\nwhere the coefficients are defined as:\n$$ C_x = \\frac{h_y^2}{2(h_x^2 + h_y^2)} \\quad \\text{and} \\quad C_y = \\frac{h_x^2}{2(h_x^2 + h_y^2)} $$\nThis single update formula is the basis for all three required iterative methods.\n\nLet $U^{(k)}$ be the matrix of solution values at iteration $k$.\n\n1.  **Jacobi Method**: The Jacobi method computes the next state $U^{(k+1)}$ for all interior points based entirely on the values in the current state $U^{(k)}$. A temporary copy of the grid is required.\n    $$ u_{i,j}^{(k+1)} = C_x (u_{i,j-1}^{(k)} + u_{i,j+1}^{(k)}) + C_y (u_{i-1,j}^{(k)} + u_{i+1,j}^{(k)}) $$\n\n2.  **Lexicographic Gauss–Seidel Method**: This method updates the grid values in-place, using the most recently computed values within the same iteration. The \"lexicographic\" ordering refers to a sequential sweep, for instance, row-by-row then column-by-column. For a sweep where $i$ and $j$ are increasing:\n    $$ u_{i,j}^{(k+1)} = C_x (u_{i,j-1}^{(k+1)} + u_{i,j+1}^{(k)}) + C_y (u_{i-1,j}^{(k+1)} + u_{i+1,j}^{(k)}) $$\n\n3.  **Red-Black Gauss–Seidel Method**: This method partitions the grid points into two sets, \"red\" and \"black,\" akin to a checkerboard pattern. A point $(i,j)$ is red if $i+j$ is even and black if $i+j$ is odd. The five-point stencil ensures that all neighbors of a red point are black, and vice versa. An iteration consists of two stages:\n    a.  **Red Sweep**: Update all red points. Since their updates depend only on black points, all red points can be computed simultaneously using values from the start of the iteration.\n    b.  **Black Sweep**: Update all black points. Their updates use the newly computed values of their red neighbors. All black points can be computed simultaneously.\n    This structure is particularly amenable to parallelization. One full iteration comprises one red sweep followed by one black sweep.\n\nThe stopping criterion is based on the infinity norm of the discrete residual vector, $\\|r\\|_\\infty$. The residual $r_{i,j}$ at an interior point $(i,j)$ is the evaluation of the discrete Laplacian on the current solution approximation $\\tilde{U}$:\n$$ r_{i,j} = \\frac{\\tilde{u}_{i,j-1} - 2\\tilde{u}_{i,j} + \\tilde{u}_{i,j+1}}{h_x^2} + \\frac{\\tilde{u}_{i-1,j} - 2\\tilde{u}_{i,j} + \\tilde{u}_{i+1,j}}{h_y^2} $$\nThe iteration is terminated when $\\max_{i,j} |r_{i,j}|  tol$ for a given tolerance $tol$, or when the number of iterations reaches a maximum limit $k_{\\max}$.\n\nThe numerical implementation proceeds as follows:\nFor each test case defined by $N_x$, $N_y$, $tol$, and $k_{\\max}$:\n1.  An $(N_y+2) \\times (N_x+2)$ grid is initialized to $0$.\n2.  The Dirichlet boundary conditions are set. The top boundary $u(x,1)$ at $i=N_y+1$ is populated with values $g(x_j) = \\sin(\\pi x_j)$ for $j \\in \\{1, \\dots, N_x\\}$. The other boundaries are $0$.\n3.  Each of the three iterative solvers is executed, starting from the zero-initialized interior grid.\n4.  Each solver loops, performing updates and checking the residual norm, until convergence or $k_{\\max}$ is reached. The number of iterations is recorded.\n5.  After the Gauss-Seidel variants have converged, the infinity norm of the difference between their final solution grids, $\\|U_{\\text{RBGS}} - U_{\\text{GS}}\\|_\\infty$, is computed and compared to a threshold of $10^{-10}$ to verify they produce consistent results.\nThe results from the four specified test cases are then consolidated into the required output format.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef setup_grid(Nx, Ny):\n    \"\"\"\n    Initializes the grid and applies boundary conditions.\n    Uses 'standard' indexing where grid row i corresponds to y-coordinate.\n    i=0 is bottom (y=0), i=Ny+1 is top (y=1).\n    \"\"\"\n    U = np.zeros((Ny + 2, Nx + 2))\n    hx = 1.0 / (Nx + 1)\n    \n    # Left, right, bottom boundaries are already zero.\n    \n    # Top boundary: u(x,1) = g(x) = sin(pi*x)\n    # This corresponds to row index Ny+1\n    x = np.linspace(0, 1, Nx + 2)\n    g = np.sin(np.pi * x)\n    U[Ny + 1, :] = g\n    return U\n\ndef calculate_residual_norm(U, hx, hy):\n    \"\"\"Calculates the infinity norm of the discrete residual.\"\"\"\n    Nx = U.shape[1] - 2\n    Ny = U.shape[0] - 2\n    if Nx = 0 or Ny = 0:\n        return 0.0\n    \n    # Slices for interior and neighbor points\n    U_int = U[1:-1, 1:-1]\n    U_left = U[1:-1, :-2]\n    U_right = U[1:-1, 2:]\n    U_down = U[:-2, 1:-1]\n    U_up = U[2:, 1:-1]\n    \n    # Discrete Laplacian operator\n    res_grid = (U_left - 2 * U_int + U_right) / (hx**2) + \\\n               (U_down - 2 * U_int + U_up) / (hy**2)\n               \n    return np.max(np.abs(res_grid))\n\ndef jacobi_solver(Nx, Ny, tol, k_max):\n    \"\"\"Solves the Laplace equation using the Jacobi method.\"\"\"\n    hx = 1.0 / (Nx + 1)\n    hy = 1.0 / (Ny + 1)\n    U = setup_grid(Nx, Ny)\n    U_old = U.copy()\n\n    Cx = hy**2 / (2 * (hx**2 + hy**2))\n    Cy = hx**2 / (2 * (hx**2 + hy**2))\n\n    for k in range(1, k_max + 1):\n        U_old[1:-1, 1:-1] = U[1:-1, 1:-1]\n\n        # Vectorized update using values from U_old\n        U[1:-1, 1:-1] = Cx * (U_old[1:-1, :-2] + U_old[1:-1, 2:]) + \\\n                        Cy * (U_old[:-2, 1:-1] + U_old[2:, 1:-1])\n\n        residual_norm = calculate_residual_norm(U, hx, hy)\n        if residual_norm  tol:\n            return k, U\n    return k_max, U\n\ndef gs_solver(Nx, Ny, tol, k_max):\n    \"\"\"Solves the Laplace equation using the lexicographic Gauss-Seidel method.\"\"\"\n    hx = 1.0 / (Nx + 1)\n    hy = 1.0 / (Ny + 1)\n    U = setup_grid(Nx, Ny)\n\n    Cx = hy**2 / (2 * (hx**2 + hy**2))\n    Cy = hx**2 / (2 * (hx**2 + hy**2))\n\n    for k in range(1, k_max + 1):\n        # In-place update with lexicographic sweep\n        for i in range(1, Ny + 1):\n            for j in range(1, Nx + 1):\n                U[i, j] = Cx * (U[i, j-1] + U[i, j+1]) + \\\n                          Cy * (U[i-1, j] + U[i+1, j])\n\n        residual_norm = calculate_residual_norm(U, hx, hy)\n        if residual_norm  tol:\n            return k, U\n    return k_max, U\n\ndef rbgs_solver(Nx, Ny, tol, k_max):\n    \"\"\"Solves the Laplace equation using the red-black Gauss-Seidel method.\"\"\"\n    hx = 1.0 / (Nx + 1)\n    hy = 1.0 / (Ny + 1)\n    U = setup_grid(Nx, Ny)\n\n    Cx = hy**2 / (2 * (hx**2 + hy**2))\n    Cy = hx**2 / (2 * (hx**2 + hy**2))\n    \n    for k in range(1, k_max + 1):\n        # A full iteration consists of one red sweep and one black sweep.\n        # Red sweep: update points where (i+j) is even\n        for i in range(1, Ny + 1):\n            for j in range(1, Nx + 1):\n                if (i + j) % 2 == 0:\n                    U[i, j] = Cx * (U[i, j-1] + U[i, j+1]) + \\\n                              Cy * (U[i-1, j] + U[i+1, j])\n        # Black sweep: update points where (i+j) is odd\n        for i in range(1, Ny + 1):\n            for j in range(1, Nx + 1):\n                if (i + j) % 2 != 0:\n                    U[i, j] = Cx * (U[i, j-1] + U[i, j+1]) + \\\n                              Cy * (U[i-1, j] + U[i+1, j])\n\n        residual_norm = calculate_residual_norm(U, hx, hy)\n        if residual_norm  tol:\n            return k, U\n    return k_max, U\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (Nx, Ny, tol, k_max)\n        (8, 8, 1e-6, 50000),\n        (7, 7, 1e-6, 50000),\n        (8, 12, 1e-6, 50000),\n        (1, 1, 1e-12, 50000),\n    ]\n\n    all_results = []\n    for case in test_cases:\n        Nx, Ny, tol, k_max = case\n        \n        n_J, U_J = jacobi_solver(Nx, Ny, tol, k_max)\n        n_RBGS, U_RBGS = rbgs_solver(Nx, Ny, tol, k_max)\n        n_GS, U_GS = gs_solver(Nx, Ny, tol, k_max)\n        \n        # Check if the solutions from GS and RBGS are close\n        diff_norm = np.max(np.abs(U_RBGS - U_GS))\n        ok = bool(diff_norm  1e-10)\n        \n        all_results.append([n_J, n_RBGS, n_GS, ok])\n\n    # Final print statement in the exact required format with lowercase booleans.\n    output_parts = []\n    for res in all_results:\n        n_j, n_rbgs, n_gs, ok_val = res\n        ok_str = 'true' if ok_val else 'false'\n        output_parts.append(f\"[{n_j},{n_rbgs},{n_gs},{ok_str}]\")\n    \n    print(f\"[{','.join(output_parts)}]\")\n\nsolve()\n```"
        }
    ]
}