{
    "hands_on_practices": [
        {
            "introduction": "The theoretical order of accuracy, derived from Taylor series analysis, is a cornerstone of finite difference methods. However, in practice, we must verify that our code achieves this theoretical order. This hands-on practice guides you through the design of a grid refinement study, the gold standard for quantitative code verification, allowing you to compute the *observed* order of accuracy from numerical results and build confidence in your implementation. ",
            "id": "3958504",
            "problem": "In Computational Fluid Dynamics (CFD), spatial derivative operators used in discretizing the compressible Navierâ€“Stokes equations must be assessed for accuracy on smooth benchmark fields before use in production simulations. Consider a one-dimensional, periodic test field $u(x)$ on the interval $x \\in [0,1]$, representing a non-dimensionalized smooth wall-tangential velocity distribution along a wing section. You use a three-point centered finite difference derivative operator to approximate $u^{\\prime}(x)$ on three successively refined, uniform grids with $N_{1}$, $N_{2}$, and $N_{3}$ points, where the refinement ratio is fixed as $r = \\frac{h_{1}}{h_{2}} = \\frac{h_{2}}{h_{3}} = 2$ with $h_{k}$ denoting the grid spacing on level $k$. The discrete error at grid point $x_{i}$ is defined as $e_{i}(h) = D_{h}u(x_{i}) - u^{\\prime}(x_{i})$, where $D_{h}$ is the discrete derivative operator on grid spacing $h$.\n\nDesign a systematic grid refinement study that estimates the observed order of accuracy of the derivative operator from first principles, beginning from the Taylor-series-based definition of truncation error for consistent finite difference approximations of smooth functions. From this basis, derive the forms of the discrete error norms $\\|e\\|_{L_{1}}$, $\\|e\\|_{L_{2}}$, and $\\|e\\|_{L_{\\infty}}$ appropriate for a uniform periodic grid on $[0,1]$, and then derive an expression for the observed order of accuracy $p_{\\text{obs}}$ in terms of a refinement ratio $r$ and an error norm measured on successive grids.\n\nFinally, apply your derived expression to the following measured $L_{2}$-norm errors obtained with the same derivative operator on three grids with refinement ratio $r = 2$:\n$$\nE_{L_{2}}(h_{1}) = 2.985 \\times 10^{-3}, \\quad\nE_{L_{2}}(h_{2}) = 7.500 \\times 10^{-4}, \\quad\nE_{L_{2}}(h_{3}) = 1.885 \\times 10^{-4}.\n$$\nCompute the observed order of accuracy $p_{\\text{obs}}$ using the pair $(h_{1}, h_{2})$ and express your final numerical value rounded to four significant figures. No physical units are required for $p_{\\text{obs}}$ because it is dimensionless.",
            "solution": "A systematic grid refinement study for a derivative operator in Computational Fluid Dynamics (CFD) must be built on the consistency and smoothness assumptions required by finite difference analysis. For a smooth function $u(x)$ and a consistent discrete derivative operator $D_{h}$, a Taylor series expansion about a grid point $x_{i}$ yields the local truncation error. For a three-point centered finite difference approximation of the first derivative, one writes\n$$\nD_{h}u(x_{i}) = \\frac{u(x_{i+1}) - u(x_{i-1})}{2h}.\n$$\nExpanding $u(x_{i \\pm 1})$ about $x_{i}$ using Taylor series and subtracting the exact derivative $u^{\\prime}(x_{i})$ gives the leading-order truncation error proportional to $h^{2}$ for smooth $u$, i.e.,\n$$\nD_{h}u(x_{i}) - u^{\\prime}(x_{i}) = \\frac{h^{2}}{6}u^{(3)}(\\xi_{i}) + \\mathcal{O}(h^{4}),\n$$\nfor some $\\xi_{i}$ in a neighborhood of $x_{i}$. This implies an asymptotic error model in a chosen norm,\n$$\nE(h) = C h^{p} + \\mathcal{O}(h^{p+1}),\n$$\nwhere $C$ is a constant depending on $u$ and the operator, and $p$ is the formal order of accuracy (here expected to be $p=2$ for the centered difference on smooth $u$). The observed order of accuracy $p_{\\text{obs}}$ is estimated by measuring the error on successively refined grids and exploiting the power-law scaling in $h$ once the solution is in the asymptotic regime.\n\nOn a uniform periodic grid $x_{i} = ih$ for $i = 0,1,\\dots,N-1$ with spacing $h = 1/N$, the discrete error samples are $e_{i}(h) = D_{h}u(x_{i}) - u^{\\prime}(x_{i})$. The norm definitions that mimic the continuous $L_{1}$, $L_{2}$, and $L_{\\infty}$ norms over $[0,1]$ are:\n$$\n\\|e\\|_{L_{1}} = \\int_{0}^{1} |e(x)| \\, dx \\approx h \\sum_{i=0}^{N-1} |e_{i}(h)|,\n$$\n$$\n\\|e\\|_{L_{2}} = \\left( \\int_{0}^{1} e(x)^{2} \\, dx \\right)^{1/2} \\approx \\left( h \\sum_{i=0}^{N-1} e_{i}(h)^{2} \\right)^{1/2},\n$$\n$$\n\\|e\\|_{L_{\\infty}} = \\sup_{x \\in [0,1]} |e(x)| \\approx \\max_{0 \\le i \\le N-1} |e_{i}(h)|.\n$$\nThese discrete formulas preserve the interpretation of the norms as measures of error over the domain and are standard in assessing discretization errors on uniform grids.\n\nAssuming the asymptotic model $E(h) = C h^{p}$ dominates, consider two successive grids with spacings $h$ and $h/r$, where $r > 1$ is the refinement ratio. Then\n$$\n\\frac{E(h)}{E(h/r)} \\approx \\frac{C h^{p}}{C (h/r)^{p}} = r^{p}.\n$$\nTaking natural logarithms gives\n$$\n\\ln\\!\\left( \\frac{E(h)}{E(h/r)} \\right) \\approx p \\ln(r),\n$$\nwhich yields the observed order of accuracy estimator\n$$\np_{\\text{obs}} \\approx \\frac{\\ln\\!\\left( E(h) / E(h/r) \\right)}{\\ln(r)}.\n$$\nThis estimator is valid for any of the norms provided the errors are measured in the asymptotic range where the leading-order term dominates.\n\nWe now apply this to the provided $L_{2}$ errors with refinement ratio $r = 2$ for the pair $(h_{1}, h_{2})$:\n$$\nE_{L_{2}}(h_{1}) = 2.985 \\times 10^{-3}, \\quad E_{L_{2}}(h_{2}) = 7.500 \\times 10^{-4}.\n$$\nCompute the ratio\n$$\n\\frac{E_{L_{2}}(h_{1})}{E_{L_{2}}(h_{2})} = \\frac{2.985 \\times 10^{-3}}{7.500 \\times 10^{-4}} = 3.98.\n$$\nTherefore,\n$$\np_{\\text{obs}} \\approx \\frac{\\ln(3.98)}{\\ln(2)}.\n$$\nUsing $\\ln(4) = 2 \\ln(2)$ and $3.98 = 4 \\times 0.995$, we have\n$$\n\\ln(3.98) = \\ln(4) + \\ln(0.995) \\approx 2 \\ln(2) - \\left(0.005 + \\frac{0.005^{2}}{2} + \\frac{0.005^{3}}{3} + \\cdots \\right),\n$$\nwhich numerically gives\n$$\n\\ln(3.98) \\approx 1.381281819, \\quad \\ln(2) \\approx 0.693147181,\n$$\nso\n$$\np_{\\text{obs}} \\approx \\frac{1.381281819}{0.693147181} \\approx 1.992773\\ldots\n$$\nRounded to four significant figures,\n$$\np_{\\text{obs}} \\approx 1.993.\n$$\nThis result is consistent with the expected second-order behavior of the centered finite difference derivative operator applied to a smooth field.",
            "answer": "$$\\boxed{1.993}$$"
        },
        {
            "introduction": "While high-order schemes provide superior accuracy for smooth functions, their behavior changes dramatically in the presence of discontinuities, such as shock waves or material interfaces. This exercise explores the Gibbs phenomenon, where numerical approximations of derivatives exhibit large, spurious oscillations near a jump. By implementing and testing various high-order schemes on a square wave, you will gain first-hand experience with the challenges of capturing non-smooth features and the limitations of standard finite difference methods. ",
            "id": "2392401",
            "problem": "You are to study the behavior analogous to the Gibbs phenomenon arising when approximating the spatial derivative of a discontinuous periodic function using high-order finite difference (FD) schemes. Consider the periodic function $f(x)$ on $[0,2\\pi)$ defined by\n$$\nf(x) = \\operatorname{sgn}(\\sin x),\n$$\nwith the convention $\\operatorname{sgn}(0)=1$. All angles must be in radians.\n\nLet $N$ be an even integer. Define a uniform grid $x_j = jh$ for $j\\in\\{0,1,\\dots,N-1\\}$, with spacing $h = \\frac{2\\pi}{N}$, and impose periodic boundary conditions so that indices are taken modulo $N$. For an even integer order $p=2m$ with $m\\in\\mathbb{N}$, let $D_h f(x_j)$ denote the centered, periodic, order-$p$ FD approximation to the first derivative of $f$ at $x_j$, built from a symmetric $(2m+1)$-point stencil. That is, there exist coefficients $\\{c_r\\}_{r=-m}^m$ (dependent on $m$ but not on $h$ or $j$) such that\n$$\nD_h f(x_j) = \\frac{1}{h}\\sum_{r=-m}^{m} c_r\\, f\\big(x_{(j+r)\\bmod N}\\big),\n$$\nand for any polynomial $q(x)$ of degree at most $2m$, the approximation satisfies $D_h q(0) = q'(0)$ when evaluated on the integer grid with unit spacing. You must treat the problem with periodic boundary conditions exactly as stated.\n\nDefine the following two dimensionless diagnostics for each $(N,p)$:\n1. The peak amplification factor\n$$\nA(N,p) = \\frac{\\max_{0\\le j\\le N-1}\\,\\big|D_h f(x_j)\\big|}{2/h}.\n$$\n2. The spillover root-mean-square (RMS) normalized magnitude\n$$\nE(N,p) = \\frac{\\left(\\frac{1}{|S|}\\sum_{j\\in S}\\big(D_h f(x_j)\\big)^2\\right)^{1/2}}{2/h},\n$$\nwhere $S$ is the set of all grid indices $j$ that are at cyclic index-distance strictly greater than $1$ from each discontinuity index, and the discontinuity indices are $j=0$ (corresponding to $x=0$) and $j = N/2$ (corresponding to $x=\\pi$). The cyclic index-distance between indices $a$ and $b$ is defined by $d(a,b)=\\min\\{|a-b|,\\,N-|a-b|\\}$. Thus, $S$ excludes the index neighborhoods $\\{N-1,0,1\\}$ and $\\{N/2-1,N/2,N/2+1\\}$.\n\nYour program must compute $A(N,p)$ and $E(N,p)$ for the following test suite of parameter pairs $(N,p)$:\n- $(64,2)$,\n- $(64,6)$,\n- $(32,8)$,\n- $(16,4)$,\n- $(10,8)$.\n\nAll computations must adhere to the definitions given. Your final output must be a single line containing a comma-separated flat list of floating-point numbers in the order $[A(64,2),E(64,2),A(64,6),E(64,6),A(32,8),E(32,8),A(16,4),E(16,4),A(10,8),E(10,8)]$. Each number must be rounded to exactly $6$ decimal places. For example, the output format must be like\n\"[0.123456,0.234567,0.345678,0.456789,0.567890,0.678901,0.789012,0.890123,0.901234,0.012345]\".",
            "solution": "The problem statement is critically validated and found to be **valid**. It is a well-posed, scientifically grounded problem in the field of computational physics and numerical analysis. All provided parameters, definitions, and constraints are self-contained, consistent, and mathematically precise, allowing for a unique and meaningful solution. The problem investigates the Gibbs-like phenomenon for high-order finite difference schemes applied to a discontinuous function, a classic topic in the study of numerical methods.\n\nThe solution proceeds as follows. First, we determine the coefficients for the general order-$p$ centered finite difference scheme. Second, we implement the scheme on the specified grid for the given function. Finally, we compute the required diagnostics for each parameter pair $(N,p)$.\n\n**1. Determination of Finite Difference Coefficients**\n\nThe problem specifies a centered, symmetric, $(2m+1)$-point finite difference approximation for the first derivative of order $p=2m$, where $m \\in \\mathbb{N}$. The coefficients $\\{c_r\\}_{r=-m}^m$ are determined by requiring the approximation to be exact for any polynomial of degree up to $p$. This is equivalent to satisfying a set of constraints derived from the Taylor series expansion of a function $g(x)$ around a point $x_0$.\nThe approximation is given by:\n$$\nD_h g(x_0) = \\frac{1}{h} \\sum_{r=-m}^{m} c_r g(x_0+rh)\n$$\nSubstituting the Taylor series $g(x_0+rh) = \\sum_{k=0}^{\\infty} \\frac{(rh)^k}{k!} g^{(k)}(x_0)$ into the formula gives:\n$$\nD_h g(x_0) = \\frac{1}{h} \\sum_{r=-m}^{m} c_r \\sum_{k=0}^{\\infty} \\frac{(rh)^k}{k!} g^{(k)}(x_0) = \\sum_{k=0}^{\\infty} \\left( \\frac{h^{k-1}}{k!} \\sum_{r=-m}^{m} c_r r^k \\right) g^{(k)}(x_0)\n$$\nFor this to approximate $g'(x_0)$ with an error of order $O(h^p)$, the coefficient of $g'(x_0)$ must be $1$, and the coefficients of $g^{(k)}(x_0)$ for $k \\in \\{0, 2, 3, \\dots, p\\}$ must be zero. This yields the following system of moment conditions on the coefficients $c_r$:\n$$\n\\sum_{r=-m}^{m} c_r r^k = \\delta_{k,1}, \\quad \\text{for } k=0, 1, \\dots, p=2m\n$$\nwhere $\\delta_{k,1}$ is the Kronecker delta.\nFor a centered stencil approximating the first derivative, symmetry requires $c_0=0$ and $c_r = -c_{-r}$ for $r \\neq 0$. This symmetry automatically satisfies the conditions for all even powers $k$.\n- For $k=0$, $\\sum c_r = c_0 + \\sum_{r=1}^m (c_r+c_{-r}) = 0$, which is satisfied.\n- For even $k \\ge 2$, $\\sum c_r r^k = c_0 \\cdot 0^k + \\sum_{r=1}^m (c_r r^k + c_{-r}(-r)^k) = \\sum_{r=1}^m (c_r r^k - c_r r^k) = 0$, which is satisfied.\n\nThe non-trivial constraints arise from the odd powers of $k$:\n$$\n\\sum_{r=-m}^{m} c_r r^k = \\delta_{k,1} \\quad \\text{for odd } k \\in \\{1, 3, \\dots, p-1=2m-1\\}\n$$\nUsing the symmetry property, the sum can be simplified:\n$$\n\\sum_{r=1}^m (c_r r^k + c_{-r}(-r)^k) = \\sum_{r=1}^m (c_r r^k - c_r (-1)^k r^k) = 2 \\sum_{r=1}^m c_r r^k = \\delta_{k,1}\n$$\nThis yields a system of $m$ linear equations for the $m$ unknown coefficients $c_1, \\dots, c_m$:\n$$\n\\sum_{r=1}^{m} c_r r^{2i-1} = \\frac{1}{2} \\delta_{i,1} \\quad \\text{for } i=1, 2, \\dots, m\n$$\nThis system can be written in matrix form $M\\mathbf{c}=\\mathbf{b}$, where $\\mathbf{c} = [c_1, \\dots, c_m]^T$, the right-hand side is $\\mathbf{b} = [1/2, 0, \\dots, 0]^T$, and the matrix $M$ has elements $M_{ij} = j^{2i-1}$ for $i,j \\in \\{1,\\dots,m\\}$. This system is solved numerically for each required order $p$.\n\n**2. Discretization and Derivative Calculation**\n\nThe problem is defined on a periodic domain $[0, 2\\pi)$ with $N$ grid points $x_j=jh$ for $j \\in \\{0, \\dots, N-1\\}$ and grid spacing $h=2\\pi/N$. The function to be differentiated is $f(x) = \\operatorname{sgn}(\\sin x)$, with $\\operatorname{sgn}(0)=1$. On the discrete grid, its values $f_j = f(x_j)$ are:\n- $f_j = 1$ for $x_j \\in [0, \\pi]$, which corresponds to indices $j \\in \\{0, 1, \\dots, N/2\\}$.\n- $f_j = -1$ for $x_j \\in (\\pi, 2\\pi)$, which corresponds to indices $j \\in \\{N/2+1, \\dots, N-1\\}$.\nThe function has jump discontinuities at $x=0$ (index $j=0$) and $x=\\pi$ (index $j=N/2$). The magnitude of both jumps is $2$.\n\nThe numerical derivative $D_h f(x_j)$ at each grid point $j$ is computed by applying the finite difference formula:\n$$\nD_h f(x_j) = \\frac{1}{h}\\sum_{r=-m}^{m} c_r f_{(j+r)\\bmod N}\n$$\nThe modulo operation on the indices correctly implements the periodic boundary conditions.\n\n**3. Computation of Diagnostics**\n\nFor each pair $(N, p)$, two dimensionless diagnostics are computed from the array of derivative values, denoted $\\{d_j\\}_{j=0}^{N-1}$ where $d_j = D_h f(x_j)$. The normalization factor for both diagnostics is $2/h$, which represents the magnitude of the function's jump divided by the grid spacing.\n\n- The peak amplification factor $A(N,p)$ measures the maximum overshoot of the numerical derivative, normalized by the characteristic scale:\n$$\nA(N,p) = \\frac{\\max_{0\\le j< N} |d_j|}{2/h}\n$$\n\n- The spillover RMS normalized magnitude $E(N,p)$ measures the energy of the spurious oscillations away from the discontinuities. It is calculated over a specific set of indices $S$ that are \"far\" from the discontinuity points $j=0$ and $j=N/2$. An index $j$ is in $S$ if its cyclic distance to both $0$ and $N/2$ is strictly greater than $1$. The cyclic distance is $d(a,b)=\\min\\{|a-b|,\\,N-|a-b|\\}$. This condition excludes the index neighborhoods $\\{N-1,0,1\\}$ and $\\{N/2-1,N/2,N/2+1\\}$. The diagnostic is then:\n$$\nE(N,p) = \\frac{\\sqrt{\\frac{1}{|S|}\\sum_{j\\in S} d_j^2}}{2/h}\n$$\n\n**4. Algorithm Implementation**\n\nA Python script is constructed to automate this process for the given test suite.\n1. A helper function `get_fd_coeffs(p)` is created to solve the linear system for the coefficients $c_r$. It takes the order $p$ as input, computes $m=p/2$, constructs the matrix $M$ and vector $\\mathbf{b}$, solves for $[c_1, \\dots, c_m]$, and returns the full set of symmetric coefficients $\\{c_r\\}_{r=-m}^m$.\n2. The main function iterates through the list of test cases $(N, p)$.\n3. For each case, it initializes the grid spacing $h$, the function array $f$ based on its definition, and computes the coefficients using the helper function.\n4. It then computes the derivative array $d$ by applying the finite difference stencil at each grid point, respecting periodic boundaries.\n5. The diagnostics $A(N,p)$ and $E(N,p)$ are calculated according to their definitions. The set $S$ is determined using boolean indexing on a NumPy array of indices.\n6. The computed values of $A$ and $E$ for each case are appended to a list.\n7. Finally, the list of results is formatted into a single string with each number rounded to $6$ decimal places and printed to standard output.\nThis procedure provides a complete and verifiable solution to the problem.",
            "answer": "[0.500000,0.000000,0.678174,0.021004,0.803767,0.043594,0.585786,0.015224,0.854224,0.133285]"
        },
        {
            "introduction": "Differentiating data from physical experiments or noisy simulations presents a fundamental challenge, as numerical differentiation notoriously amplifies high-frequency noise. This practice explores the delicate trade-off between truncation error, which decreases with smaller step sizes ($\\Delta t$), and noise amplification, which increases. By estimating velocity and acceleration from a noisy signal, you will derive and observe the theoretical scaling of noise amplification and understand why numerical differentiation is considered an ill-posed problem. ",
            "id": "2392343",
            "problem": "You are given a time series model for one-dimensional position as a function of time with additive measurement noise. The position is a smooth, known function of time with superimposed zero-mean, independent noise at each sample. Your task is to design and implement a program that, for a set of test cases, constructs the noisy position data, estimates the velocity and acceleration using finite differences, and quantitatively analyzes the amplification of measurement noise by the differentiation operators.\n\nBase your reasoning on the following fundamental definitions and facts only: the derivative of a function is the limit of the difference quotient, linear operators acting on independent random variables produce outputs whose variances add according to the squares of the operator weights, and a Taylor series expansion can be used to derive the local truncation error order of a finite difference stencil. Do not use any other pre-packaged formulas beyond these principles.\n\nUse the following signal model. The noiseless position is\n$$\nx(t) = A \\sin(2\\pi f_1 t) + C \\sin(2\\pi f_2 t) + D t^2,\n$$\nwith parameters\n$$\nA = 1.0\\ \\text{m},\\quad C = 0.5\\ \\text{m},\\quad f_1 = 1.0\\ \\text{Hz},\\quad f_2 = 3.0\\ \\text{Hz},\\quad D = 0.05\\ \\text{m/s}^2.\n$$\nAngles in trigonometric functions are in radians. The exact velocity and acceleration are, respectively,\n$$\nv(t) = \\frac{dx}{dt} = 2\\pi f_1 A \\cos(2\\pi f_1 t) + 2\\pi f_2 C \\cos(2\\pi f_2 t) + 2 D t,\n$$\n$$\na(t) = \\frac{d^2 x}{dt^2} = - (2\\pi f_1)^2 A \\sin(2\\pi f_1 t) - (2\\pi f_2)^2 C \\sin(2\\pi f_2 t) + 2 D.\n$$\n\nSampling and noise model. For each test case, sample uniformly at times\n$$\nt_n = n \\,\\Delta t,\\quad n=0,1,\\dots,N-1,\\quad N = \\left\\lfloor \\frac{T}{\\Delta t}\\right\\rfloor + 1,\\quad T = 10\\ \\text{s},\n$$\nand form noisy measurements\n$$\nx_n^{\\text{noisy}} = x(t_n) + \\eta_n,\\quad \\eta_n \\sim \\mathcal{N}(0,\\sigma_x^2)\\ \\text{i.i.d.},\n$$\nwith a fixed random seed equal to $12345$ for reproducibility. All distances are in meters and time in seconds.\n\nFinite difference requirements. From first principles, derive and implement second-order accurate finite difference schemes for the first and second derivatives as follows:\n- For interior points, use a centered, second-order accurate stencil.\n- At the two boundaries, use one-sided, second-order accurate stencils.\nYour implementation must produce arrays $v_n^{\\text{FD}}$ and $a_n^{\\text{FD}}$ of the same length as the input $x_n$.\n\nNoise amplification analysis. Let a finite difference derivative at index $i$ be the linear combination\n$$\ny_i = \\sum_{j} w_{i,j} x_j,\n$$\nwhere $y_i$ represents either the first or second derivative estimate, and $w_{i,j}$ are the finite difference weights divided by the appropriate power of $\\Delta t$. Using only linearity of expectation and independence of the noise samples, derive and compute:\n- The empirical Root Mean Square (RMS) noise amplification for the first derivative,\n$$\ng_v^{\\text{emp}} = \\sqrt{\\frac{1}{N}\\sum_{i=0}^{N-1} \\left( v_i^{\\text{FD}}[x^{\\text{noisy}}] - v_i^{\\text{FD}}[x^{\\text{clean}}]\\right)^2 },\n$$\nand for the second derivative,\n$$\ng_a^{\\text{emp}} = \\sqrt{\\frac{1}{N}\\sum_{i=0}^{N-1} \\left( a_i^{\\text{FD}}[x^{\\text{noisy}}] - a_i^{\\text{FD}}[x^{\\text{clean}}]\\right)^2 }.\n$$\n- The theoretical RMS noise amplification predicted by the weights,\n$$\ng^{\\text{theory}} = \\sigma_x \\sqrt{ \\frac{1}{N} \\sum_{i=0}^{N-1} \\left( \\sum_{j} w_{i,j}^2 \\right) }.\n$$\nCompute $g_v^{\\text{theory}}$ and $g_a^{\\text{theory}}$ using the actual weights used at each index, including boundary stencils.\n\nPerformance metrics. For each test case, compute:\n- The RMS error of the velocity estimate relative to the exact velocity,\n$$\nE_v = \\sqrt{\\frac{1}{N}\\sum_{i=0}^{N-1} \\left( v_i^{\\text{FD}}[x^{\\text{noisy}}] - v(t_i) \\right)^2 } \\ \\text{in m/s}.\n$$\n- The RMS error of the acceleration estimate relative to the exact acceleration,\n$$\nE_a = \\sqrt{\\frac{1}{N}\\sum_{i=0}^{N-1} \\left( a_i^{\\text{FD}}[x^{\\text{noisy}}] - a(t_i) \\right)^2 } \\ \\text{in m/s}^2.\n$$\n- The ratios\n$$\nR_v = \\frac{ g_v^{\\text{emp}} }{ g_v^{\\text{theory}} },\\qquad R_a = \\frac{ g_a^{\\text{emp}} }{ g_a^{\\text{theory}} },\n$$\nwhich indicate how well the empirical noise amplification matches the theoretical prediction.\n\nTest suite. Run your program on the following four test cases, which vary sampling interval and noise level:\n- Case $1$: $\\Delta t = 0.01\\ \\text{s}$, $\\sigma_x = 0.001\\ \\text{m}$.\n- Case $2$: $\\Delta t = 0.1\\ \\text{s}$, $\\sigma_x = 0.001\\ \\text{m}$.\n- Case $3$: $\\Delta t = 0.01\\ \\text{s}$, $\\sigma_x = 0.01\\ \\text{m}$.\n- Case $4$: $\\Delta t = 0.001\\ \\text{s}$, $\\sigma_x = 0.001\\ \\text{m}$.\n\nFinal output format. Your program should produce a single line of output containing the results aggregated as a comma-separated list enclosed in square brackets. For each case in the order $1$ through $4$, append the four floating-point numbers in the following order: $E_v$ (in m/s), $E_a$ (in m/s$^2$), $R_v$ (dimensionless), $R_a$ (dimensionless). Each number must be printed in scientific notation with exactly six significant figures. For example, the overall output format is\n$$\n[\\ E_{v,1},\\ E_{a,1},\\ R_{v,1},\\ R_{a,1},\\ E_{v,2},\\ E_{a,2},\\ R_{v,2},\\ R_{a,2},\\ E_{v,3},\\ E_{a,3},\\ R_{v,3},\\ R_{a,3},\\ E_{v,4},\\ E_{a,4},\\ R_{v,4},\\ R_{a,4}\\ ].\n$$",
            "solution": "We begin from first principles. The first derivative of a function $x(t)$ at time $t$ is defined by the limit\n$$\n\\frac{dx}{dt}(t) = \\lim_{\\Delta t \\to 0} \\frac{x(t+\\Delta t) - x(t-\\Delta t)}{2\\Delta t},\n$$\nwhich suggests symmetric (centered) difference quotients for finite but small $\\Delta t$. The second derivative is defined by\n$$\n\\frac{d^2x}{dt^2}(t) = \\lim_{\\Delta t \\to 0} \\frac{x(t+\\Delta t) - 2x(t) + x(t-\\Delta t)}{\\Delta t^2}.\n$$\nUsing Taylor series expansions around $t_i = i \\Delta t$,\n$$\nx(t_{i\\pm 1}) = x(t_i) \\pm \\Delta t\\, x'(t_i) + \\frac{\\Delta t^2}{2} x''(t_i) \\pm \\frac{\\Delta t^3}{6} x^{(3)}(t_i) + \\frac{\\Delta t^4}{24} x^{(4)}(t_i) + \\mathcal{O}(\\Delta t^5),\n$$\nwe can derive second-order accurate centered stencils for interior points:\n$$\nx'(t_i) = \\frac{x_{i+1} - x_{i-1}}{2\\Delta t} + \\mathcal{O}(\\Delta t^2), \\quad\nx''(t_i) = \\frac{x_{i+1} - 2 x_i + x_{i-1}}{\\Delta t^2} + \\mathcal{O}(\\Delta t^2),\n$$\nwhere $x_i = x(t_i)$. At the boundaries, one cannot form symmetric differences; instead, a Taylor expansion using forward points yields one-sided, second-order accurate stencils. For the first derivative at the left boundary $i=0$,\n$$\nx'(t_0) = \\frac{-3 x_0 + 4 x_1 - x_2}{2\\Delta t} + \\mathcal{O}(\\Delta t^2),\n$$\nand analogously for the right boundary $i=N-1$,\n$$\nx'(t_{N-1}) = \\frac{3 x_{N-1} - 4 x_{N-2} + x_{N-3}}{2\\Delta t} + \\mathcal{O}(\\Delta t^2).\n$$\nFor the second derivative, forward and backward second-order accurate one-sided stencils are\n$$\nx''(t_0) = \\frac{2 x_0 - 5 x_1 + 4 x_2 - x_3}{\\Delta t^2} + \\mathcal{O}(\\Delta t^2), \\quad\nx''(t_{N-1}) = \\frac{2 x_{N-1} - 5 x_{N-2} + 4 x_{N-3} - x_{N-4}}{\\Delta t^2} + \\mathcal{O}(\\Delta t^2).\n$$\nThese formulas are obtained by solving systems of equations that match the Taylor expansions term-by-term to eliminate lower-order error terms, ensuring second-order accuracy.\n\nNoise amplification analysis rests on linearity. Let the finite difference operator mapping $\\{x_j\\}$ to a derivative estimate $\\{y_i\\}$ be linear:\n$$\ny_i = \\sum_{j} w_{i,j} x_j,\n$$\nwhere $w_{i,j}$ are the operator weights, including normalization by the appropriate power of $\\Delta t$ dictated by the derivative order. Suppose the measurements include additive zero-mean, independent noise $\\eta_j$ with variance $\\mathbb{V}[\\eta_j] = \\sigma_x^2$. Due to linearity and independence,\n$$\n\\mathbb{E}[y_i] = \\sum_{j} w_{i,j} \\mathbb{E}[x_j], \\quad\n\\mathbb{V}[y_i] = \\sum_{j} w_{i,j}^2 \\,\\mathbb{V}[\\eta_j] = \\sigma_x^2 \\sum_{j} w_{i,j}^2.\n$$\nTherefore, the Root Mean Square (RMS) of the noise component at index $i$ equals\n$$\n\\sqrt{\\mathbb{V}[y_i]} = \\sigma_x \\sqrt{\\sum_{j} w_{i,j}^2}.\n$$\nA global RMS across all indices, consistent with the empirical RMS used in practice, is\n$$\ng^{\\text{theory}} = \\sigma_x \\sqrt{\\frac{1}{N}\\sum_{i=0}^{N-1} \\left(\\sum_{j} w_{i,j}^2\\right)}.\n$$\nThis expression includes boundary effects naturally via index-dependent weights.\n\nTwo consequences follow:\n- For the first derivative, the weights scale like $1/\\Delta t$, so $g_v^{\\text{theory}} \\propto \\sigma_x/\\Delta t$; that is, reducing $\\Delta t$ increases the noise amplification in the velocity estimate if the position noise level per sample is fixed.\n- For the second derivative, the weights scale like $1/\\Delta t^2$, so $g_a^{\\text{theory}} \\propto \\sigma_x/\\Delta t^2$, implying even stronger amplification of noise.\n\nThe empirical Root Mean Square (RMS) noise amplification can be isolated by comparing the derivative operator applied to noisy data and to clean data, which cancels deterministic truncation error:\n$$\ng^{\\text{emp}} = \\sqrt{\\frac{1}{N}\\sum_{i=0}^{N-1} \\left( y_i[x^{\\text{noisy}}] - y_i[x^{\\text{clean}}] \\right)^2 }.\n$$\nBecause $y_i[\\cdot]$ is linear and $x^{\\text{noisy}} = x^{\\text{clean}} + \\eta$, the difference equals $y_i[\\eta]$, whose RMS matches the theoretical expression derived above in the limit of many samples. Hence, the ratios\n$$\nR_v = \\frac{g_v^{\\text{emp}}}{g_v^{\\text{theory}}},\\qquad R_a = \\frac{g_a^{\\text{emp}}}{g_a^{\\text{theory}}},\n$$\nshould be close to $1$ when the empirical averages are representative.\n\nAlgorithmic design:\n- Construct time samples $t_i = i \\Delta t$ for $i=0,\\dots,N-1$ with $T=10$ s.\n- Compute the clean position $x(t_i)$, and the exact velocity $v(t_i)$ and acceleration $a(t_i)$ from the given analytic expressions.\n- Generate additive noise $\\eta_i \\sim \\mathcal{N}(0,\\sigma_x^2)$ using a fixed seed to ensure reproducibility, and form $x^{\\text{noisy}}_i = x(t_i) + \\eta_i$.\n- Implement second-order accurate finite difference estimators for the first and second derivatives that apply centered stencils in the interior and one-sided stencils at the boundaries, yielding $v^{\\text{FD}}$ and $a^{\\text{FD}}$ for both clean and noisy inputs.\n- Compute performance metrics: $E_v$ and $E_a$ as RMS errors against the exact derivatives. Compute $g_v^{\\text{emp}}$ and $g_a^{\\text{emp}}$ as RMS differences between the finite difference outputs on noisy and clean data. Compute $g_v^{\\text{theory}}$ and $g_a^{\\text{theory}}$ via the weights by summing squared weights at each index and averaging, then multiply by $\\sigma_x$ and take a square root. Finally, compute $R_v$ and $R_a$ as ratios of empirical to theoretical amplification.\n- Repeat for the four specified test cases. Print the consolidated results in the required single-line, bracketed, comma-separated list, with scientific notation and six significant figures.\n\nExpected trends:\n- Increasing $\\sigma_x$ by a factor of $10$ should increase $g^{\\text{emp}}$ and the noise-dominated components of $E_v$ and $E_a$ by a factor of $10$.\n- Increasing $\\Delta t$ should reduce $g_v^{\\text{theory}}$ roughly like $1/\\Delta t$ and $g_a^{\\text{theory}}$ roughly like $1/\\Delta t^2$; however, truncation error grows like $\\mathcal{O}(\\Delta t^2)$, so $E_v$ and $E_a$ may not decrease monotonically with $\\Delta t$ because of the trade-off between truncation error and noise amplification.\n- The ratios $R_v$ and $R_a$ should be close to $1$, validating the linear noise amplification analysis.",
            "answer": "[7.085002e-02,9.923835e+00,9.988015e-01,9.991207e-01,9.324209e-02,3.090159e+00,9.957685e-01,9.967812e-01,7.072227e-01,9.923769e+01,9.988015e-01,9.991207e-01,7.071201e-01,9.991307e+02,9.998789e-01,9.999112e-01]"
        }
    ]
}