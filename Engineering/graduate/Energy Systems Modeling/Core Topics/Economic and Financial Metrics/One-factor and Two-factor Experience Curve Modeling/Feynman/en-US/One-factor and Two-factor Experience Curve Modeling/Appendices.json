{
    "hands_on_practices": [
        {
            "introduction": "A foundational task in modeling is choosing the right level of complexity. While a two-factor model can potentially capture more dynamics than a one-factor model, it also runs the risk of overfitting the data, leading to poor predictive performance. This exercise  introduces a robust, standard procedure for model selection: blocked $K$-fold cross-validation. By implementing this method, you will learn how to empirically compare the predictive power of competing models while respecting the temporal structure of energy technology data.",
            "id": "4109570",
            "problem": "You are tasked with designing and implementing a programmatic $K$-fold Cross-Validation (CV) procedure to compare the predictive performance of one-factor versus two-factor experience curve models in energy systems modeling. The dependent variable is the natural logarithm of unit costs, and the loss function must operate in log-cost space. The CV must respect temporal structure by assigning contiguous blocks to folds. The comparison must be performed on three synthetic test cases constructed from scientifically plausible, self-consistent generative processes.\n\nStarting point and fundamental base: Assume that unit costs of an energy technology are governed by constant elasticities with respect to scale and an additional driver. Constant elasticity with multiplicative separability implies that, after taking natural logarithms, models become linear in parameters. Model training must therefore use Ordinary Least Squares (OLS), and model comparison must be performed under a clearly specified loss in log-cost space.\n\nRequirements:\n- Use blocked $K$-fold Cross-Validation (CV), defined as partitioning the time-ordered data into $K$ contiguous blocks, using each block once as a validation set and the remaining blocks jointly as the training set.\n- Define the loss on log-costs as the mean squared error (Mean Squared Error (MSE)) over all validation points across the $K$ folds. If $\\hat{y}_i$ denotes the predicted log-cost and $y_i$ denotes the observed log-cost, and there are $m$ total validation points across all folds, the loss must be\n$$\n\\mathcal{L} = \\frac{1}{m} \\sum_{i=1}^{m} \\left(y_i - \\hat{y}_i\\right)^2.\n$$\n- Costs are measured in United States Dollars per kilowatt ($USD/kW$). Before taking logarithms, normalize by dividing by $1$ $USD/kW$, so that the natural logarithm is applied to a dimensionless ratio.\n- One-factor model: regress log-costs on a single explanatory variable derived from scale (log-transformed).\n- Two-factor model: regress log-costs on two explanatory variables derived from scale and an additional driver (both log-transformed).\n- Use OLS to estimate linear-in-parameters models on the training folds and generate validation predictions on the held-out fold.\n\nTest suite and data generation:\nConstruct three test cases with deterministic generative processes. In all cases, let $t \\in \\{1,2,\\dots,N\\}$ denote time index, $S_t$ denote cumulative scale, $D_t$ denote an additional driver, and $C_t$ denote unit cost. Define $S_t$ and $D_t$ using the specified sequences, then define the unit cost as\n$$\nC_t = A \\cdot S_t^{-b} \\cdot D_t^{-g} \\cdot e^{\\varepsilon_t},\n$$\nwhere $A$ is a positive scale parameter in $USD/kW$, and $\\varepsilon_t$ is a deterministic disturbance. The observed dependent variable is $y_t = \\ln\\left(C_t / (1\\ \\text{USD}/\\text{kW})\\right)$.\n\n- Case A (general “happy path” two-factor):\n  - $N = 30$, $K = 5$, $A = 1000$, $b = 0.3$, $g = 0.2$.\n  - Production increments: $s_t = 0.5 + 0.02 t + 0.1 \\sin(0.1 t)$, cumulative scale: $S_t = \\sum_{i=1}^{t} s_i$.\n  - Knowledge increments: $d_t = 0.2 + 0.01 t$, knowledge stock: $D_t = \\sum_{i=1}^{t} d_i$.\n  - Disturbance: $\\varepsilon_t = 0.05 \\sin(0.3 t)$.\n\n- Case B (edge case dominated by one-factor):\n  - $N = 8$, $K = 4$, $A = 800$, $b = 0.35$, $g = 0$.\n  - Production increments: $s_t = 0.6 + 0.03 t$, cumulative scale: $S_t = \\sum_{i=1}^{t} s_i$.\n  - Additional driver constant: $D_t = 1$ for all $t$.\n  - Disturbance: $\\varepsilon_t = 0.02 \\cos(0.5 t)$.\n\n- Case C (boundary case with strong collinearity and leave-one-out):\n  - $N = 12$, $K = 12$, $A = 900$, $b = 0.25$, $g = 0.1$.\n  - Production increments: $s_t = 0.7 + 0.01 t$, cumulative scale: $S_t = \\sum_{i=1}^{t} s_i$.\n  - Additional driver: $D_t = 1.05 S_t + 0.5$.\n  - Disturbance: $\\varepsilon_t = 0.04 \\sin(0.2 t)$.\n\nImplementation details:\n- For each case, construct $S_t$, $D_t$, and $C_t$ using the above definitions and compute $y_t$ as the natural logarithm of the normalized costs.\n- For the one-factor model, fit $y_t$ on an intercept and $\\ln(S_t)$.\n- For the two-factor model, fit $y_t$ on an intercept, $\\ln(S_t)$, and $\\ln(D_t)$.\n- Use blocked $K$-fold CV: partition indices $\\{1,\\dots,N\\}$ into $K$ contiguous folds whose sizes differ by at most $1$. In each fold, the validation set is the current block, and the training set is the complement.\n- Compute $\\mathcal{L}_{1}$ as the average validation MSE across all folds for the one-factor model and $\\mathcal{L}_{2}$ analogously for the two-factor model.\n\nRequired final output:\n- For each test case, compute the scalar difference $\\Delta = \\mathcal{L}_{1} - \\mathcal{L}_{2}$ in log-cost units (dimensionless). Report these three values as plain decimal numbers.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[$\\text{result1}$,$\\text{result2}$,$\\text{result3}$]\"), without additional text.\n\nAll computations must be expressed in radians for any trigonometric functions, and all numeric outputs must be decimals without percentage signs.",
            "solution": "The problem statement has been validated and is deemed sound. It is scientifically grounded in the principles of experience curve modeling and statistical model validation, is well-posed with a clear and complete setup, and is objective in its formulation. The provided test cases are deterministic, ensuring a unique and verifiable solution. We may therefore proceed with the solution.\n\nThe task is to programmatically implement a blocked $K$-fold cross-validation (CV) procedure to compare a one-factor experience curve model against a two-factor model for three distinct, synthetically generated test cases. The comparison metric is the difference in mean squared error (MSE) on a held-out validation set, aggregated across all folds.\n\nThe generative process for unit costs, $C_t$, at time $t$ is given by a multiplicative model with constant elasticities:\n$$\nC_t = A \\cdot S_t^{-b} \\cdot D_t^{-g} \\cdot e^{\\varepsilon_t}\n$$\nHere, $S_t$ is the cumulative scale of production, $D_t$ is an additional explanatory driver (e.g., knowledge stock), $A$ is a baseline cost parameter, $b$ and $g$ are the learning elasticities with respect to scale and the additional driver, respectively, and $\\varepsilon_t$ is a deterministic disturbance term.\n\nBy taking the natural logarithm and normalizing the cost by $1 \\text{ USD/kW}$ to create a dimensionless quantity, we obtain a model that is linear in its parameters:\n$$\ny_t = \\ln\\left(\\frac{C_t}{1 \\text{ USD/kW}}\\right) = \\ln(A) - b\\ln(S_t) - g\\ln(D_t) + \\varepsilon_t\n$$\nThis log-linear form is the basis for our estimation procedure. We will compare two models fitted using Ordinary Least Squares (OLS) on this log-transformed data.\n\nModel 1 (One-factor): This model relates the log-cost, $y_t$, solely to the log of cumulative scale, $\\ln(S_t)$. It includes an intercept term. The model equation is:\n$$\ny_t = \\beta_0 + \\beta_1 \\ln(S_t) + u_t\n$$\nwhere $\\beta_0$ and $\\beta_1$ are the coefficients to be estimated, and $u_t$ is the error term.\n\nModel 2 (Two-factor): This model extends the one-factor model by including the log of the additional driver, $\\ln(D_t)$, as a second explanatory variable. The model equation is:\n$$\ny_t = \\gamma_0 + \\gamma_1 \\ln(S_t) + \\gamma_2 \\ln(D_t) + v_t\n$$\nwhere $\\gamma_0$, $\\gamma_1$, and $\\gamma_2$ are the coefficients to be estimated, and $v_t$ is the error term.\n\nThe core of the analysis is a blocked $K$-fold cross-validation procedure. This method is chosen to respect the temporal ordering of the data, which is crucial in time-series contexts to prevent lookahead bias. The dataset of $N$ observations is partitioned into $K$ contiguous, non-overlapping blocks (folds). The procedure iterates $K$ times. In each iteration $k \\in \\{1, \\dots, K\\}$, the $k$-th block serves as the validation set, and the remaining $K-1$ blocks are combined to form the training set.\n\nFor each of the $K$ iterations, we perform the following steps:\n1.  The model parameters ($\\beta$s and $\\gamma$s) for both models are estimated using OLS on the training data. For a general linear model $y = X\\beta + \\epsilon$, the OLS estimate is $\\hat{\\beta} = (X^T X)^{-1} X^T y$. Numerically stable algorithms, such as those provided by `numpy.linalg.lstsq`, are used for this computation.\n2.  The fitted models are then used to predict log-costs, $\\hat{y}_i$, for the observations $i$ in the validation set.\n3.  The true observed log-costs, $y_i$, and the predicted log-costs, $\\hat{y}_i$, for the validation set are stored.\n\nAfter completing all $K$ folds, all stored validation predictions are aggregated. The overall performance of each model is quantified by the mean squared error (MSE) across all $m$ validation points, where $m=N$. The loss function is defined as:\n$$\n\\mathcal{L} = \\frac{1}{m} \\sum_{i=1}^{m} (y_i - \\hat{y}_i)^2\n$$\nLet $\\mathcal{L}_1$ be the loss for the one-factor model and $\\mathcal{L}_2$ be the loss for the two-factor model. The final comparison is performed by computing the difference $\\Delta = \\mathcal{L}_1 - \\mathcal{L}_2$. A positive value of $\\Delta$ indicates that the two-factor model has a lower MSE and thus superior predictive performance under this evaluation framework.\n\nThis entire procedure is applied to three distinct test cases, each defined by a specific set of parameters ($A, b, g, N, K$) and functional forms for $S_t$, $D_t$, and $\\varepsilon_t$. The logic is encapsulated in a Python program that first generates the data for each case, then executes the described cross-validation workflow to compute $\\mathcal{L}_1$ and $\\mathcal{L}_2$, and finally calculates their difference, $\\Delta$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the cross-validation comparison for all test cases.\n    \"\"\"\n\n    def perform_kfold_cv(X, y, K):\n        \"\"\"\n        Performs blocked K-fold cross-validation for a given model.\n\n        Args:\n            X (np.ndarray): The design matrix, including an intercept column.\n            y (np.ndarray): The dependent variable vector.\n            K (int): The number of folds.\n\n        Returns:\n            float: The mean squared error over all validation predictions.\n        \"\"\"\n        n_samples = len(y)\n        indices = np.arange(n_samples)\n\n        # Determine the size of each contiguous block for the folds.\n        # This ensures fold sizes differ by at most 1.\n        base_fold_size = n_samples // K\n        num_larger_folds = n_samples % K\n        \n        fold_start_indices = [0]\n        current_idx = 0\n        for i in range(K):\n            fold_size = base_fold_size + 1 if i < num_larger_folds else base_fold_size\n            current_idx += fold_size\n            if i < K - 1:\n                fold_start_indices.append(current_idx)\n\n        all_y_true = []\n        all_y_pred = []\n\n        for i in range(K):\n            # Define validation and training indices for the current fold\n            val_start = fold_start_indices[i]\n            val_end = fold_start_indices[i+1] if i + 1 < K else n_samples\n            val_idx = indices[val_start:val_end]\n            \n            train_mask = np.ones(n_samples, dtype=bool)\n            train_mask[val_idx] = False\n            train_idx = indices[train_mask]\n\n            X_train, y_train = X[train_idx], y[train_idx]\n            X_val, y_val = X[val_idx], y[val_idx]\n\n            # Fit model using OLS on the training set\n            # np.linalg.lstsq is numerically robust and handles (multi)collinearity\n            try:\n                coeffs = np.linalg.lstsq(X_train, y_train, rcond=None)[0]\n            except np.linalg.LinAlgError:\n                # This should not occur with the given test cases, but is a safeguard.\n                # If it occurs, prediction quality is zero.\n                coeffs = np.zeros(X_train.shape[1])\n\n            # Predict on the validation set\n            y_pred = X_val @ coeffs\n            \n            all_y_true.append(y_val)\n            all_y_pred.append(y_pred)\n\n        # Concatenate results from all folds\n        y_true_flat = np.concatenate(all_y_true)\n        y_pred_flat = np.concatenate(all_y_pred)\n        \n        # Calculate overall Mean Squared Error\n        mse = np.mean((y_true_flat - y_pred_flat)**2)\n        return mse\n\n    def run_case(params):\n        \"\"\"\n        Generates data and runs the CV comparison for a single test case.\n        \"\"\"\n        case_id, N, K, A, b, g, s_func, d_spec, eps_func = params\n\n        # Generate data based on the case specification\n        t = np.arange(1, N + 1)\n        \n        s_t = s_func(t)\n        S_t = np.cumsum(s_t)\n\n        if case_id == 'A':\n            d_t = d_spec(t)\n            D_t = np.cumsum(d_t)\n        elif case_id == 'B':\n            D_t = np.full(N, d_spec)\n        elif case_id == 'C':\n            D_t = d_spec(S_t)\n\n        epsilon_t = eps_func(t)\n        \n        # Compute cost and the log-transformed dependent variable y_t\n        C_t = A * (S_t ** -b) * (D_t ** -g) * np.exp(epsilon_t)\n        y = np.log(C_t)\n\n        # Prepare regressor data\n        log_S = np.log(S_t)\n        log_D = np.log(D_t)\n        intercept = np.ones_like(y)\n\n        # Create design matrices for the two models\n        X1 = np.stack([intercept, log_S], axis=1) # One-factor model\n        X2 = np.stack([intercept, log_S, log_D], axis=1) # Two-factor model\n\n        # Perform CV and get MSE for each model\n        loss1 = perform_kfold_cv(X1, y, K)\n        loss2 = perform_kfold_cv(X2, y, K)\n\n        # Return the difference in loss\n        return loss1 - loss2\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        ('A', 30, 5, 1000, 0.3, 0.2, \n         lambda t: 0.5 + 0.02 * t + 0.1 * np.sin(0.1 * t), \n         lambda t: 0.2 + 0.01 * t,\n         lambda t: 0.05 * np.sin(0.3 * t)),\n        ('B', 8, 4, 800, 0.35, 0,\n         lambda t: 0.6 + 0.03 * t,\n         1.0,\n         lambda t: 0.02 * np.cos(0.5 * t)),\n        ('C', 12, 12, 900, 0.25, 0.1,\n         lambda t: 0.7 + 0.01 * t,\n         lambda S: 1.05 * S + 0.5,\n         lambda t: 0.04 * np.sin(0.2 * t))\n    ]\n    \n    results = [run_case(case) for case in test_cases]\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Once a multi-factor model is specified, it is critical to diagnose potential econometric problems that can undermine the reliability of its parameter estimates. In two-factor experience curves, the explanatory variables—such as cumulative production and a knowledge stock—are often highly correlated, leading to multicollinearity. This practice  guides you through implementing the Variance Inflation Factor (VIF) from first principles, providing a direct, hands-on method to quantify the severity of multicollinearity and assess the stability of your model's results.",
            "id": "4109606",
            "problem": "Consider the two-factor experience curve for energy technology unit cost modeled by the logarithmic specification $ \\ln C = \\alpha - b \\ln Q - c \\ln S $, where $ C $ denotes unit cost measured in a fixed monetary unit per energy unit, $ Q $ denotes cumulative output in energy units, $ S $ denotes cumulative spillovers or knowledge stock in a dimensionless index, and $ \\alpha $, $ b $, $ c $ are unknown parameters to be estimated. In practical estimation using Ordinary Least Squares (OLS), strong linear dependence among predictors inflates the uncertainty of parameter estimates, a phenomenon termed multicollinearity. A standard diagnostic grounded in OLS and the coefficient of determination is the Variance Inflation Factor (VIF), applied to each predictor in the regression. Your task is to implement, from first principles, a program to diagnose multicollinearity in the predictor pair $ (\\ln Q, \\ln S) $ by computing the VIF for each predictor via auxiliary regressions that regress each predictor on the other predictor and an intercept, relying only on the definitions of OLS and the coefficient of determination. Angles in trigonometric functions must be treated in radians.\n\nUse the following test suite of three synthetic datasets that are scientifically plausible for energy systems modeling. For each dataset, construct the sequences $ \\{Q_i\\} $ and $ \\{S_i\\} $ for the specified index range, then transform to $ x_{1,i} = \\ln Q_i $ and $ x_{2,i} = \\ln S_i $ before performing the diagnostics.\n\n- Dataset A (general case, moderate correlation): For $ i = 1,2,\\dots,20 $,\n  $$ Q_i = 3.0 \\times 10^{5} + 1.8 \\times 10^{4} \\cdot i^{1.1}, $$\n  $$ S_i = 2.5 \\times 10^{4} + 7.0 \\times 10^{3} \\cdot \\sin(0.2 \\, i) + 1.4 \\times 10^{3} \\cdot i. $$\n  Use radians for $ \\sin(\\cdot) $.\n\n- Dataset B (near-collinearity edge case): For $ i = 1,2,\\dots,40 $,\n  $$ Q_i = 1.0 \\times 10^{5} + 1.0 \\times 10^{4} \\cdot i, $$\n  $$ S_i = 1.0 \\times 10^{2} \\cdot Q_i^{0.97} \\cdot \\left(1 + 10^{-4} \\cdot \\cos(i)\\right). $$\n  Use radians for $ \\cos(\\cdot) $.\n\n- Dataset C (small-sample boundary case): For $ i = 1,2,\\dots,6 $,\n  $$ u = [0, 2, 1, 3, 0.5, 2.5], \\quad v = [2, 0, -1, 1.5, 2.2, 0.5], $$\n  $$ Q_i = 1.2 \\times 10^{5} + 2.0 \\times 10^{4} \\cdot u_i, $$\n  $$ S_i = 4.5 \\times 10^{4} + 2.0 \\times 10^{3} \\cdot v_i. $$\n\nImplement the following steps for each dataset:\n1. Compute $ x_{1,i} = \\ln Q_i $ and $ x_{2,i} = \\ln S_i $.\n2. For each predictor $ x_j $ in $ \\{x_1, x_2\\} $, estimate the auxiliary OLS regression of $ x_j $ on an intercept and the other predictor $ x_k $ (with $ j \\neq k $). From this auxiliary regression, compute the coefficient of determination $ R_j^2 $ using its definition in terms of sums of squares.\n3. Quantify multicollinearity for each predictor using the variance inflation framework based on $ R_j^2 $ obtained from the auxiliary regression, and report the resulting VIF values.\n4. Interpret thresholds for concern in energy cost datasets using two decision rules. Define two booleans per dataset: one indicating whether any predictor’s VIF strictly exceeds $ 5 $ (moderate concern) and another indicating whether any predictor’s VIF strictly exceeds $ 10 $ (strong concern). Express all comparisons using decimals, not percentage signs.\n\nOutput specification:\n- For each dataset, produce a list with four entries: the VIF for $ \\ln Q $ rounded to $ 3 $ decimal places, the VIF for $ \\ln S $ rounded to $ 3 $ decimal places, a boolean indicating whether any VIF $ > 5 $, and a boolean indicating whether any VIF $ > 10 $.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each dataset’s results are enclosed in their own brackets, in the order A, B, C. For example, the output structure must be $ [[\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot]] $.\n- No external input is permitted; all computations must be self-contained.",
            "solution": "The problem requires the implementation of a multicollinearity diagnostic for a two-factor experience curve model from first principles. The model for the unit cost $C$ is given by the logarithmic specification:\n$$ \\ln C = \\alpha - b \\ln Q - c \\ln S $$\nwhere $Q$ is the cumulative output, $S$ is the cumulative knowledge stock, and $\\alpha$, $b$, and $c$ are parameters. The predictors in this linear model are $x_1 = \\ln Q$ and $x_2 = \\ln S$. Multicollinearity arises when these predictors are highly linearly correlated, which can inflate the variance of the Ordinary Least Squares (OLS) estimates of the parameters $b$ and $c$, making them unreliable.\n\nThe chosen diagnostic tool is the Variance Inflation Factor (VIF). For a given predictor $x_j$ in a multiple regression model, its VIF quantifies how much the variance of its estimated coefficient is increased due to its linear dependence on the other predictors. The VIF for predictor $x_j$ is defined as:\n$$ \\text{VIF}_j = \\frac{1}{1 - R_j^2} $$\nHere, $R_j^2$ is the coefficient of determination from an auxiliary OLS regression where $x_j$ is treated as the dependent variable and all other predictors (in this case, just $x_k$ where $k \\neq j$) and an intercept are the independent variables.\n\nThe task is to compute $\\text{VIF}_1$ and $\\text{VIF}_2$ for the predictors $x_1 = \\ln Q$ and $x_2 = \\ln S$ for three given datasets. This requires performing two auxiliary simple linear regressions from first principles:\n1.  Regress $x_1$ on $x_2$: $x_{1,i} = \\beta_{10} + \\beta_{11} x_{2,i} + \\epsilon_{1,i}$\n2.  Regress $x_2$ on $x_1$: $x_{2,i} = \\beta_{20} + \\beta_{21} x_{1,i} + \\epsilon_{2,i}$\n\nFor a generic simple linear regression of a dependent variable $y$ on an independent variable $x$ for a set of $n$ observations $\\{(x_i, y_i)\\}_{i=1}^n$, the model is $y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i$. The OLS estimators $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ that minimize the sum of squared residuals are given by:\n$$ \\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2} $$\n$$ \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x} $$\nwhere $\\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i$ and $\\bar{y} = \\frac{1}{n}\\sum_{i=1}^n y_i$ are the sample means.\n\nOnce the coefficients are estimated, the coefficient of determination, $R^2$, is calculated. $R^2$ measures the proportion of the variance in the dependent variable that is predictable from the independent variable(s). It is defined as:\n$$ R^2 = 1 - \\frac{\\text{SSR}}{\\text{SST}} $$\nwhere:\n-   The Total Sum of Squares, $\\text{SST} = \\sum_{i=1}^n (y_i - \\bar{y})^2$, measures the total sample variance of the dependent variable.\n-   The Residual Sum of Squares, $\\text{SSR} = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2$, measures the variance not explained by the model, where $\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i$ are the predicted values.\n\nIn the specific case of two predictors, $x_1$ and $x_2$, the coefficient of determination from regressing $x_1$ on $x_2$ ($R_1^2$) is identical to that from regressing $x_2$ on $x_1$ ($R_2^2$). Both are equal to the square of the Pearson correlation coefficient between $x_1$ and $x_2$, i.e., $R_1^2 = R_2^2 = r_{x_1, x_2}^2$. Consequently, their VIF values must be identical: $\\text{VIF}_1 = \\text{VIF}_2$. This provides a useful consistency check.\n\nThe computational procedure for each dataset is as follows:\n1.  Generate the time series data for cumulative output, $\\{Q_i\\}$, and knowledge stock, $\\{S_i\\}$, according to the provided formulas. For Dataset A with $i = 1, 2, \\dots, 20$:\n    $Q_i = 3.0 \\times 10^{5} + 1.8 \\times 10^{4} \\cdot i^{1.1}$\n    $S_i = 2.5 \\times 10^{4} + 7.0 \\times 10^{3} \\cdot \\sin(0.2 \\, i) + 1.4 \\times 10^{3} \\cdot i$\n    For Dataset B with $i = 1, 2, \\dots, 40$:\n    $Q_i = 1.0 \\times 10^{5} + 1.0 \\times 10^{4} \\cdot i$\n    $S_i = 1.0 \\times 10^{2} \\cdot Q_i^{0.97} \\cdot (1 + 10^{-4} \\cdot \\cos(i))$\n    For Dataset C with $i = 1, 2, \\dots, 6$:\n    $u = [0, 2, 1, 3, 0.5, 2.5]$, $v = [2, 0, -1, 1.5, 2.2, 0.5]$\n    $Q_i = 1.2 \\times 10^{5} + 2.0 \\times 10^{4} \\cdot u_i$\n    $S_i = 4.5 \\times 10^{4} + 2.0 \\times 10^{3} \\cdot v_i$\n2.  Transform the data using the natural logarithm: $x_{1,i} = \\ln Q_i$ and $x_{2,i} = \\ln S_i$.\n3.  To compute $\\text{VIF}_1$ for $x_1 = \\ln Q$:\n    a.  Perform an OLS regression with $y=x_1$ and $x=x_2$ to find $\\hat{\\beta}_{10}, \\hat{\\beta}_{11}$.\n    b.  Calculate $R_1^2$ using the SST and SSR from this regression.\n    c.  Compute $\\text{VIF}_1 = 1 / (1 - R_1^2)$.\n4.  To compute $\\text{VIF}_2$ for $x_2 = \\ln S$:\n    a.  Perform an OLS regression with $y=x_2$ and $x=x_1$ to find $\\hat{\\beta}_{20}, \\hat{\\beta}_{21}$.\n    b.  Calculate $R_2^2$.\n    c.  Compute $\\text{VIF}_2 = 1 / (1 - R_2^2)$.\n5.  Apply the decision rules: determine if any VIF value exceeds the thresholds of $5$ (moderate concern) and $10$ (strong concern).\n6.  The final result for each dataset is a list containing the VIF for $\\ln Q$ rounded to $3$ decimal places, the VIF for $\\ln S$ rounded to $3$ decimal places, and two boolean values corresponding to the decision rules.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the multicollinearity diagnosis problem for three datasets.\n    \"\"\"\n\n    def calculate_vif(y_vec, x_vec):\n        \"\"\"\n        Computes the Variance Inflation Factor (VIF) for a predictor by regressing it\n        on another predictor from first principles (OLS and R-squared).\n\n        Args:\n            y_vec (np.ndarray): The dependent variable vector (the predictor for which VIF is calculated).\n            x_vec (np.ndarray): The independent variable vector (the other predictor).\n\n        Returns:\n            float: The calculated VIF. Returns np.inf for perfect multicollinearity.\n        \"\"\"\n        n = len(y_vec)\n        if n < 2:\n            return np.nan \n\n        # Calculate means\n        y_mean = np.mean(y_vec)\n        x_mean = np.mean(x_vec)\n\n        # Calculate OLS coefficient beta_1 for y = beta_0 + beta_1*x\n        numerator = np.sum((x_vec - x_mean) * (y_vec - y_mean))\n        denominator = np.sum((x_vec - x_mean)**2)\n\n        if denominator == 0:\n            # The independent variable has zero variance, regression is ill-defined.\n            return np.nan\n\n        beta_1 = numerator / denominator\n        beta_0 = y_mean - beta_1 * x_mean\n\n        # Calculate predicted values and sums of squares\n        y_pred = beta_0 + beta_1 * x_vec\n        sst = np.sum((y_vec - y_mean)**2)\n        ssr = np.sum((y_vec - y_pred)**2)\n\n        if sst == 0:\n            # The dependent variable has zero variance.\n            return np.nan\n\n        # Calculate R-squared\n        r_squared = 1.0 - (ssr / sst)\n\n        # Handle perfect multicollinearity case (R^2 = 1)\n        if r_squared >= 1.0 - 1e-12: # Use a tolerance for floating point comparison\n            return np.inf\n\n        # Calculate VIF\n        vif = 1.0 / (1.0 - r_squared)\n        return vif\n\n    # Define the datasets\n    # Dataset A\n    i_A = np.arange(1, 21)\n    Q_A = 3.0e5 + 1.8e4 * np.power(i_A, 1.1)\n    S_A = 2.5e4 + 7.0e3 * np.sin(0.2 * i_A) + 1.4e3 * i_A\n\n    # Dataset B\n    i_B = np.arange(1, 41)\n    Q_B = 1.0e5 + 1.0e4 * i_B\n    S_B = 1.0e2 * np.power(Q_B, 0.97) * (1.0 + 1e-4 * np.cos(i_B))\n\n    # Dataset C\n    u_C = np.array([0.0, 2.0, 1.0, 3.0, 0.5, 2.5])\n    v_C = np.array([2.0, 0.0, -1.0, 1.5, 2.2, 0.5])\n    Q_C = 1.2e5 + 2.0e4 * u_C\n    S_C = 4.5e4 + 2.0e3 * v_C\n\n    test_cases = [\n        {\"name\": \"A\", \"Q\": Q_A, \"S\": S_A},\n        {\"name\": \"B\", \"Q\": Q_B, \"S\": S_B},\n        {\"name\": \"C\", \"Q\": Q_C, \"S\": S_C},\n    ]\n\n    all_results_str = []\n    \n    for case in test_cases:\n        # Step 1: Compute transformed predictors\n        x1 = np.log(case[\"Q\"])  # x1 corresponds to ln(Q)\n        x2 = np.log(case[\"S\"])  # x2 corresponds to ln(S)\n\n        # Step 2 & 3: Compute VIF for each predictor\n        # VIF for ln(Q) is based on regression of ln(Q) on ln(S)\n        vif_lnQ = calculate_vif(y_vec=x1, x_vec=x2)\n        \n        # VIF for ln(S) is based on regression of ln(S) on ln(Q)\n        vif_lnS = calculate_vif(y_vec=x2, x_vec=x1)\n        \n        # Step 4: Apply decision rules\n        # Since VIFs must be equal for 2 predictors, we can check one\n        moderate_concern = vif_lnQ > 5.0\n        strong_concern = vif_lnQ > 10.0\n        \n        # Round VIFs for output\n        vif_lnQ_rounded = round(vif_lnQ, 3)\n        vif_lnS_rounded = round(vif_lnS, 3)\n        \n        # Format the result string for this case to avoid spaces\n        # Python's str(bool) gives 'True'/'False' literals\n        result_str = (\n            f\"[{vif_lnQ_rounded},\"\n            f\"{vif_lnS_rounded},\"\n            f\"{str(moderate_concern)},\"\n            f\"{str(strong_concern)}]\"\n        )\n        all_results_str.append(result_str)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(all_results_str)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The primary output of an experience curve analysis is often the learning rate, $LR$, a metric with direct economic and policy implications. However, models estimate the experience exponent $b$, not the $LR$ directly. This final exercise  bridges this gap by demonstrating how to translate the statistical uncertainty in an estimate of $b$ into a confidence interval for the $LR$. You will see how the non-linear relationship $LR = 1 - 2^{-b}$ creates an asymmetric confidence interval, a crucial insight for accurately reporting and interpreting model uncertainty.",
            "id": "4109590",
            "problem": "Consider a technology whose unit cost follows a one-factor experience relationship embedded in a two-factor specification frequently used in energy systems modeling: the logarithm of unit cost $C_t$ is modeled as $\\ln(C_t) = \\beta_0 - b \\ln(Q_t) + \\gamma Z_t + \\varepsilon_t$, where $Q_t$ is cumulative production, $Z_t$ is an auxiliary driver (for example, calendar time or research and development intensity), $b$ is the experience exponent associated with cumulative production, and $\\varepsilon_t$ is a mean-zero disturbance. Under the canonical Wright-type one-factor experience curve, a doubling of cumulative production ($Q_t \\mapsto 2 Q_t$) implies a progress ratio $PR = 2^{-b}$ and a learning rate $LR = 1 - PR = 1 - 2^{-b}$. Suppose a large-sample, heteroskedasticity-robust estimator of $b$ delivers $\\hat{b} = 0.265$ with standard error $s_b = 0.022959$, and the $(1 - \\alpha)$ confidence interval for $b$ with $\\alpha = 0.05$ is $[0.22, 0.31]$.\n\nStarting from the fundamental definitions above and using only well-tested statistical facts about confidence intervals and monotone transformations, construct the $(1 - \\alpha)$ confidence interval for the learning rate $LR$ by transforming the confidence interval endpoints for $b$ through the mapping $LR(b) = 1 - 2^{-b}$. Then, explain why the resulting interval for $LR$ is generally asymmetric around the point estimate $LR(\\hat{b})$ and contrast it with a first-order (delta method) symmetric approximation around $LR(\\hat{b})$ obtained from a linearization of $LR(b)$ at $b = \\hat{b}$.\n\nExpress your final numerical interval for $LR$ as decimal fractions and round each endpoint to four significant figures.",
            "solution": "The problem will first be validated against the specified criteria.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n- Model for unit cost $C_t$: $\\ln(C_t) = \\beta_0 - b \\ln(Q_t) + \\gamma Z_t + \\varepsilon_t$\n- $Q_t$: cumulative production\n- $Z_t$: auxiliary driver\n- $b$: experience exponent\n- $\\varepsilon_t$: mean-zero disturbance\n- Progress Ratio ($PR$) definition: $PR = 2^{-b}$\n- Learning Rate ($LR$) definition: $LR = 1 - PR = 1 - 2^{-b}$\n- Point estimate for $b$: $\\hat{b} = 0.265$\n- Standard error of $\\hat{b}$: $s_b = 0.022959$\n- Confidence level: $(1 - \\alpha)$ with $\\alpha = 0.05$\n- Confidence interval for $b$: $[0.22, 0.31]$\n\n**Step 2: Validate Using Extracted Givens**\n\n- **Scientifically Grounded**: The problem is based on the well-established concept of experience curves (or learning curves) widely used in industrial engineering, technology forecasting, and energy systems modeling. The specific model form is a standard two-factor extension of the one-factor Wright's Law. The statistical procedure requested—transforming a confidence interval and analyzing the properties of the transformation—is a fundamental and correct application of statistical theory.\n- **Well-Posed**: The problem is well-posed. It provides all necessary information (the confidence interval for the parameter $b$ and the functional form of the transformation to $LR$) to compute a unique and meaningful confidence interval for the learning rate. The request for an explanation of asymmetry is a standard conceptual question in statistics.\n- **Objective**: The problem is stated in precise, objective language. All terms are defined mathematically, and no subjective or opinion-based statements are present.\n\n**Step 3: Verdict and Action**\n\nThe problem is valid as it is scientifically grounded, well-posed, objective, and self-contained. The solution process may proceed.\n\n### Solution\n\nThe problem requires the construction of a $(1-\\alpha)$ confidence interval for the learning rate, $LR$, given the confidence interval for the experience exponent, $b$. The relationship between these two quantities is given by the function:\n$$LR(b) = 1 - 2^{-b}$$\nThis function can also be written using the natural exponential function as $LR(b) = 1 - \\exp(-b \\ln(2))$.\n\nA confidence interval for a transformed parameter can be found by applying the transformation function to the endpoints of the original parameter's confidence interval, provided the function is strictly monotonic over that interval. To verify this, we compute the first derivative of $LR(b)$ with respect to $b$:\n$$\\frac{d(LR)}{db} = \\frac{d}{db}(1 - \\exp(-b \\ln(2))) = - \\exp(-b \\ln(2)) \\cdot (-\\ln(2)) = (\\ln(2)) 2^{-b}$$\nSince $\\ln(2) > 0$ and $2^{-b} > 0$ for all finite $b$, the derivative $\\frac{d(LR)}{db}$ is strictly positive. This confirms that $LR(b)$ is a strictly increasing function of $b$.\n\nGiven that the $(1 - \\alpha)$ confidence interval for $b$ is $[b_{lower}, b_{upper}]$, and the transformation $LR(b)$ is strictly increasing, the corresponding $(1 - \\alpha)$ confidence interval for $LR$ is $[LR(b_{lower}), LR(b_{upper})]$.\n\nThe problem provides the $95\\%$ confidence interval for $b$ as $[0.22, 0.31]$. Therefore, we have:\n- $b_{lower} = 0.22$\n- $b_{upper} = 0.31$\n\nThe lower bound of the confidence interval for $LR$ is:\n$$LR_{lower} = LR(b_{lower}) = 1 - 2^{-0.22} \\approx 1 - 0.858564 = 0.141436$$\nThe upper bound of the confidence interval for $LR$ is:\n$$LR_{upper} = LR(b_{upper}) = 1 - 2^{-0.31} \\approx 1 - 0.806639 = 0.193361$$\n\nRounding each endpoint to four significant figures as requested, we get:\n- $LR_{lower} \\approx 0.1414$\n- $LR_{upper} \\approx 0.1934$\nThe resulting confidence interval for $LR$ is $[0.1414, 0.1934]$.\n\nThe next part of the task is to explain the asymmetry of this interval. The confidence interval for $b$, $[0.22, 0.31]$, is symmetric around the point estimate $\\hat{b} = 0.265$, since $0.265 - 0.22 = 0.045$ and $0.31 - 0.265 = 0.045$. The point estimate for the learning rate is:\n$$LR(\\hat{b}) = LR(0.265) = 1 - 2^{-0.265} \\approx 1 - 0.832203 = 0.167797$$\nAfter rounding, $LR(\\hat{b}) \\approx 0.1678$.\n\nThe asymmetry of the interval $[LR_{lower}, LR_{upper}]$ around the point estimate $LR(\\hat{b})$ is determined by comparing the distances from the point estimate to the endpoints:\n- Upper interval width: $LR_{upper} - LR(\\hat{b}) \\approx 0.193361 - 0.167797 = 0.025564$\n- Lower interval width: $LR(\\hat{b}) - LR_{lower} \\approx 0.167797 - 0.141436 = 0.026361$\nSince $0.025564 \\neq 0.026361$, the confidence interval for $LR$ is asymmetric. Specifically, the point estimate $LR(\\hat{b})$ is closer to the upper bound than to the lower bound.\n\nThis asymmetry arises because the transformation $LR(b) = 1 - 2^{-b}$ is non-linear. The nature of the non-linearity is revealed by the second derivative of the function:\n$$\\frac{d^2(LR)}{db^2} = \\frac{d}{db}\\left((\\ln(2)) 2^{-b}\\right) = (\\ln(2)) \\cdot (-\\ln(2) 2^{-b}) = -(\\ln(2))^2 2^{-b}$$\nSince $-(\\ln(2))^2 < 0$ and $2^{-b} > 0$, the second derivative $\\frac{d^2(LR)}{db^2}$ is strictly negative. This indicates that the function $LR(b)$ is strictly concave. For a strictly concave function, the slope decreases as the input variable increases. Consequently, for a fixed step size $\\delta > 0$, the change in the function value is smaller for larger inputs, i.e., $LR(\\hat{b} + \\delta) - LR(\\hat{b}) < LR(\\hat{b}) - LR(\\hat{b} - \\delta)$. This is precisely the source of the asymmetry observed, where the upper part of the interval is narrower than the lower part.\n\nIn contrast, a first-order (delta method) approximation constructs a symmetric confidence interval. The delta method approximates the standard error of the transformed parameter $LR$ using a linearization of the function $LR(b)$ around the point estimate $\\hat{b}$:\n$$s_{LR} \\approx \\left| \\frac{d(LR)}{db} \\right|_{b=\\hat{b}} | \\cdot s_b = \\left| (\\ln 2) 2^{-\\hat{b}} \\right| \\cdot s_b$$\nThe resulting confidence interval is symmetric by construction:\n$$[LR(\\hat{b}) - z_{\\alpha/2} s_{LR}, \\quad LR(\\hat{b}) + z_{\\alpha/2} s_{LR}]$$\nThis method effectively approximates the curved function $LR(b)$ with its tangent line at $\\hat{b}$. By ignoring the curvature (i.e., the second derivative), the delta method fails to capture the asymmetry that arises from the non-linear nature of the transformation. The exact transformation of the interval endpoints, as performed here, is superior because it fully accounts for this non-linearity.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 0.1414 & 0.1934 \\end{pmatrix}}\n$$"
        }
    ]
}