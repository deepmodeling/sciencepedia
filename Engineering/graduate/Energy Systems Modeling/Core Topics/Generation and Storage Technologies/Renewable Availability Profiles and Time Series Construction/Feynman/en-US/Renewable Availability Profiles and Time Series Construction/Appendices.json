{
    "hands_on_practices": [
        {
            "introduction": "Before performing any complex calculations, it is crucial to ensure the integrity of your input data. In energy systems modeling, a subtle but common source of error arises from the misinterpretation of time series conventions. This practice challenges you to think critically about the difference between interval-averaged measurements and instantaneous point samples, and how timestamps relate to these values . Mastering this concept is the first step toward building robust models, as it allows you to avoid significant, hidden biases when calculating key performance metrics like the capacity factor.",
            "id": "4115881",
            "problem": "An energy systems modeler needs to compute the annual capacity factor of a wind or Photovoltaic (PV) plant from an hourly availability time series over a horizon of length $T$, partitioned into $N$ equal intervals of width $\\Delta t = T/N$. Let $a(t) \\in [0,1]$ denote the underlying continuous availability fraction and $P_{\\mathrm{rated}}$ the rated power, so that the physical power is $P(t) = P_{\\mathrm{rated}} a(t)$. Three data providers supply time series with different timestamp conventions for the same underlying process and the same set of disjoint, consecutive intervals that exactly cover the horizon:\n- Provider $A$: values labeled with end-of-interval times $t_k$; each reported value equals the interval-average availability over $(t_{k-1}, t_k]$.\n- Provider $B$: values labeled with beginning-of-interval times $t_{k-1}$; each reported value equals the interval-average availability over $[t_{k-1}, t_k)$.\n- Provider $C$: values labeled with centered times $(t_{k-1} + t_k)/2$; each reported value equals the instantaneous availability at the center of $[t_{k-1}, t_k)$.\n\nThe modeler forms an estimate of capacity factor by discrete summation over the reported series and normalizing by $P_{\\mathrm{rated}}$ and $T$. Assume $a(t)$ is continuous with bounded derivative on $[0,T]$.\n\nWhich statement best characterizes the impact of the timestamp convention on the computed capacity factor and prescribes a consistent convention that avoids bias?\n\nA. If reported values are interval averages that tile $[0,T]$ as half-open intervals and no edge intervals are dropped or duplicated, the capacity factor estimate is invariant to whether timestamps denote beginnings, ends, or centers. Bias arises primarily when interval averages are misinterpreted as point samples or when horizon edges are mishandled. A consistent, unbiased convention is to store interval-averaged availability for half-open intervals $[t_k, t_{k+1})$, time-stamp at either $t_k$ or $t_{k+1}$ with explicit interval semantics, and compute capacity factor from the sum of interval energies divided by $P_{\\mathrm{rated}} T$. If only point samples are available, use midpoint or trapezoidal quadrature on $[0,T]$ with aligned boundaries.\n\nB. Beginning-of-interval timestamps systematically inflate capacity factors for wind and PV because availability typically increases within each hour; end-of-interval timestamps eliminate this effect. Therefore a consistent, unbiased convention is to use end-of-interval timestamps and interpret values as instantaneous.\n\nC. Centered timestamps guarantee zero bias for any smooth availability profile when capacity factor is computed by a rectangle rule on point samples; beginning and end timestamps produce equal-magnitude, opposite-sign first-order biases proportional to the local time derivative. Therefore one should always time-stamp at interval centers to eliminate bias.\n\nD. To avoid double counting, with end-of-interval timestamps one should drop the first reported value and sum the remaining $N-1$ values; this removes edge effects and ensures an unbiased capacity factor.\n\nE. Timestamp choice is irrelevant under any interpretation because the sum over a fixed set of $N$ values is invariant to relabeling; therefore capacity factor cannot depend on beginning versus end versus centered conventions, and no special handling of edges is needed.",
            "solution": "The problem requires an analysis of how different time series timestamping and data representation conventions affect the calculation of an annual capacity factor.\n\nThe true capacity factor, $CF$, is defined as the total actual energy generated over a period $T$ divided by the maximum possible energy that could have been generated in that period. Mathematically, it is the time-average of the availability fraction $a(t)$:\n$$CF = \\frac{\\int_0^T P(t) \\, dt}{P_{\\mathrm{rated}} T} = \\frac{\\int_0^T P_{\\mathrm{rated}} a(t) \\, dt}{P_{\\mathrm{rated}} T} = \\frac{1}{T} \\int_0^T a(t) \\, dt$$\n\nThe time horizon $[0, T]$ is partitioned into $N$ equal intervals, each of width $\\Delta t = T/N$. The $k$-th interval can be denoted as $[t_{k-1}, t_k)$ or $(t_{k-1}, t_k]$, where $t_k = k \\Delta t$ for $k=0, 1, \\dots, N$. The integral can be expressed as a sum over these intervals:\n$$CF = \\frac{1}{T} \\sum_{k=1}^N \\int_{t_{k-1}}^{t_k} a(t) \\, dt$$\n\nThe modeler's estimate of the capacity factor, $\\widehat{CF}$, is formed by a discrete summation of the provided time series values, let's call them $\\{v_k\\}_{k=1}^N$:\n$$\\widehat{CF} = \\frac{\\sum_{k=1}^N (v_k P_{\\mathrm{rated}}) \\Delta t}{P_{\\mathrm{rated}} T} = \\frac{\\Delta t}{T} \\sum_{k=1}^N v_k = \\frac{1}{N} \\sum_{k=1}^N v_k$$\n\nWe will now analyze the estimate $\\widehat{CF}$ for each data provider.\n\n**Analysis for Provider A (End-of-interval average) and Provider B (Beginning-of-interval average):**\n\nBoth providers supply values that are interval averages.\nFor Provider A, the value $A_k$ is timestamped at $t_k$ and represents the average over $(t_{k-1}, t_k]$.\nFor Provider B, the value $B_k$ is timestamped at $t_{k-1}$ and represents the average over $[t_{k-1}, t_k)$.\n\nIn both cases, the reported value for the $k$-th interval is the true average availability over that interval:\n$$v_k = \\frac{1}{\\Delta t} \\int_{t_{k-1}}^{t_k} a(\\tau) \\, d\\tau$$\nThe set of reported values $\\{v_k\\}_{k=1}^N$ is identical for both Provider A and Provider B; only the time labels attached to these values differ.\n\nThe estimated capacity factor is computed as:\n$$\\widehat{CF} = \\frac{1}{N} \\sum_{k=1}^N v_k = \\frac{1}{N} \\sum_{k=1}^N \\left( \\frac{1}{\\Delta t} \\int_{t_{k-1}}^{t_k} a(\\tau) \\, d\\tau \\right)$$\nSubstituting $N = T / \\Delta t$:\n$$\\widehat{CF} = \\frac{\\Delta t}{T} \\frac{1}{\\Delta t} \\sum_{k=1}^N \\int_{t_{k-1}}^{t_k} a(\\tau) \\, d\\tau = \\frac{1}{T} \\sum_{k=1}^N \\int_{t_{k-1}}^{t_k} a(\\tau) \\, d\\tau$$\nBy the additivity of integrals, the sum of integrals over the disjoint intervals $[t_{k-1}, t_k]$ that tile $[0, T]$ is the integral over the entire horizon:\n$$\\widehat{CF} = \\frac{1}{T} \\int_0^T a(\\tau) \\, d\\tau = CF$$\n\nConclusion: If the data consist of a complete set of non-overlapping interval averages that perfectly tile the time horizon, the discrete summation correctly and exactly reconstructs the true capacity factor. The choice of timestamp (beginning-of-interval vs. end-of-interval) is a mere labeling convention and has no impact on the calculated value. There is zero bias.\n\n**Analysis for Provider C (Centered instantaneous value):**\n\nProvider C supplies values $C_k$ that are instantaneous samples at the midpoint of each interval, $t_{k,cen} = (t_{k-1} + t_k) / 2$.\n$$C_k = a(t_{k,cen})$$\nThe estimated capacity factor is:\n$$\\widehat{CF}_C = \\frac{1}{N} \\sum_{k=1}^N C_k = \\frac{1}{N} \\sum_{k=1}^N a(t_{k,cen})$$\nTo compare this with the true $CF$, we rewrite the expression:\n$$\\widehat{CF}_C = \\frac{\\Delta t}{T} \\sum_{k=1}^N a(t_{k,cen})$$\nThis calculation is equivalent to approximating the true integral $\\int_0^T a(t) \\, dt$ using the midpoint rule for numerical quadrature:\n$$\\int_0^T a(t) \\, dt \\approx \\sum_{k=1}^N a(t_{k,cen}) \\Delta t$$\nThe error of the composite midpoint rule is known from numerical analysis. For a function $a(t)$ with a continuous second derivative, the error is:\n$$\\int_0^T a(t) \\, dt - \\sum_{k=1}^N a(t_{k,cen}) \\Delta t = \\frac{T (\\Delta t)^2}{24} a''(\\xi)$$\nfor some $\\xi \\in (0, T)$.\n\nConclusion: The estimate from Provider C's data is an approximation, not an exact calculation. It is biased, and the bias (error) is non-zero unless $a''(t) = 0$ for all $t$ (i.e., $a(t)$ is a linear function).\n\n**Evaluation of Options:**\n\n**A. If reported values are interval averages that tile $[0,T]$ as half-open intervals and no edge intervals are dropped or duplicated, the capacity factor estimate is invariant to whether timestamps denote beginnings, ends, or centers. Bias arises primarily when interval averages are misinterpreted as point samples or when horizon edges are mishandled. A consistent, unbiased convention is to store interval-averaged availability for half-open intervals $[t_k, t_{k+1})$, time-stamp at either $t_k$ or $t_{k+1}$ with explicit interval semantics, and compute capacity factor from the sum of interval energies divided by $P_{\\mathrm{rated}} T$. If only point samples are available, use midpoint or trapezoidal quadrature on $[0,T]$ with aligned boundaries.**\n- This statement is fully consistent with our analysis. It correctly identifies that for interval-average data (Providers A and B), the timestamp is a labeling issue and the calculation is exact (unbiased) if performed correctly. It pinpoints the true sources of error: misinterpreting the data type (e.g., treating an average as a point sample) or mishandling the summation domain (edge effects). The prescribed convention for interval averages is the mathematically exact method. It also correctly advises using numerical quadrature for point samples.\n- **Verdict: Correct.**\n\n**B. Beginning-of-interval timestamps systematically inflate capacity factors for wind and PV because availability typically increases within each hour; end-of-interval timestamps eliminate this effect. Therefore a consistent, unbiased convention is to use end-of-interval timestamps and interpret values as instantaneous.**\n- This option is flawed in multiple ways. First, the premise that availability \"typically increases within each hour\" is a weak and unsubstantiated generalization. Second, it incorrectly analyzes the bias. If we were using point samples at the beginning of the interval for an increasing function, the left-hand Riemann sum would *underestimate* (not inflate) the integral. Third, it conflates interval-average data (from Provider B) with point-sample data. As shown, for interval averages, there is no bias. Finally, it recommends misinterpreting data (\"interpret values as instantaneous\"), which is a direct cause of error.\n- **Verdict: Incorrect.**\n\n**C. Centered timestamps guarantee zero bias for any smooth availability profile when capacity factor is computed by a rectangle rule on point samples; beginning and end timestamps produce equal-magnitude, opposite-sign first-order biases proportional to the local time derivative. Therefore one should always time-stamp at interval centers to eliminate bias.**\n- The central claim that centered timestamps (midpoint rule) \"guarantee zero bias for any smooth\" profile is false. As shown in the analysis, the error is proportional to the second derivative $a''(t)$ and is non-zero for any non-linear function. While the midpoint rule is often more accurate than left- or right-hand rules, it is not generally exact.\n- **Verdict: Incorrect.**\n\n**D. To avoid double counting, with end-of-interval timestamps one should drop the first reported value and sum the remaining $N-1$ values; this removes edge effects and ensures an unbiased capacity factor.**\n- The premise of \"double counting\" is false. Provider A's data for intervals $(t_0, t_1], (t_1, t_2], \\dots, (t_{N-1}, t_N]$ are perfectly disjoint and cover the entire horizon. Dropping the first value means ignoring the energy produced in the first interval $(0, t_1]$, which introduces a significant bias. This recommendation is demonstrably wrong.\n- **Verdict: Incorrect.**\n\n**E. Timestamp choice is irrelevant under any interpretation because the sum over a fixed set of $N$ values is invariant to relabeling; therefore capacity factor cannot depend on beginning versus end versus centered conventions, and no special handling of edges is needed.**\n- The phrase \"under any interpretation\" is a fatal flaw. The interpretation of a value—as an interval average or a point sample—is critically important. The set of values from Provider C (point samples) is fundamentally different from the sets from A and B (interval averages). The claim that \"no special handling of edges is needed\" is also false and dangerous, as off-by-one errors are a common and significant source of bias. This statement promotes a dangerously simplistic view.\n- **Verdict: Incorrect.**\n\nBased on the rigorous analysis, only option A provides a scientifically sound and complete characterization of the problem.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "Energy system models frequently operate at specific temporal resolutions, such as hourly, which may not match the resolution of available raw data. Therefore, a common and essential task is to resample time series data from one frequency to another. This exercise provides hands-on practice in designing a resampling scheme that adheres to the fundamental physical principle of energy conservation . By implementing methods to convert between quarter-hour and hourly data, you will develop a core skill for preparing consistent and physically meaningful datasets for both operational and long-term planning models.",
            "id": "4115891",
            "problem": "You are tasked with designing and implementing an energy-preserving resampling and interpolation scheme between quarter-hour availability profiles and hourly availability profiles in the context of energy systems modeling. Consider a power time series represented as a function $P(t)$ measured in kilowatts (kW), where $t$ is time measured in hours. The energy over an interval $[a,b]$ is defined as $E_{[a,b]} = \\int_a^b P(t)\\, dt$ in kilowatt-hours (kWh). In discrete time, with a uniform sampling interval $\\Delta t$, a common modeling assumption is that $P(t)$ is piecewise constant on each sampling interval. Under this assumption, the energy over a collection of intervals is given by the Riemann sum $E = \\sum_i P_i \\Delta t$, where $P_i$ is the constant power over interval $i$.\n\nYour program must construct a scheme that preserves energy (i.e., the time integral of power) when converting:\n- from quarter-hour ($\\Delta t = 0.25$ hours) to hourly ($\\Delta t = 1$ hour), and\n- from hourly to quarter-hour.\n\nYou must adhere to the following scientific constraints:\n- Downsampling from quarter-hour to hourly must produce an hourly time series $\\{\\hat{P}_h\\}$ where each $\\hat{P}_h$ is the hourly average power such that the hourly energy equals the sum of the quarter-hour energies within that hour.\n- Upsampling from hourly to quarter-hour must produce a quarter-hour time series $\\{p_{h,j}\\}$ ($j=1,\\dots,4$ per hour) that preserves the hourly energy for each hour.\n\nDefinitions and requirements:\n- Let $P_k$ denote the quarter-hour power samples (in kW), with $k$ indexing quarter-hours over a day ($96$ samples).\n- Let $\\hat{P}_h$ denote the hourly power samples (in kW), with $h$ indexing hours over a day ($24$ samples).\n- Quarter-hour energy is $E_{\\text{15min}} = \\sum_{k=0}^{95} P_k \\cdot 0.25$ in kWh.\n- Hourly energy is $E_{\\text{hour}} = \\sum_{h=0}^{23} \\hat{P}_h \\cdot 1$ in kWh.\n- Angles used in any trigonometric function must be in radians.\n\nYour program must implement:\n1. A function that takes a quarter-hour series $\\{P_k\\}$ and returns the hourly series $\\{\\hat{P}_h\\}$ such that energy is preserved when aggregating from $15$ minutes to $1$ hour.\n2. A function that takes an hourly series $\\{\\hat{P}_h\\}$ and returns a quarter-hour series $\\{p_{h,j}\\}$ such that energy is preserved when disaggregating from $1$ hour to $15$ minutes.\n\nTest suite:\nImplement the following test cases to verify energy preservation. For each case, compute the absolute difference in total energy (in kWh) between the original series and the series obtained after a round-trip conversion that should preserve energy. Each test case should yield a single float (in kWh). Your final output must be a single line containing all results as a comma-separated list enclosed in square brackets.\n\nQuarter-hour to hourly to quarter-hour (15→60→15):\n- Test $1$ (solar-like quarter-hour profile): Let $P_{\\max} = 100$ (kW). For $t_k = k \\cdot 0.25$ with $k \\in \\{0,1,\\dots,95\\}$, define $P_k = \\max\\{0, P_{\\max} \\cdot \\sin\\left(\\pi \\cdot \\frac{t_k - 6}{12}\\right)\\}$ for $t_k \\in [0,24)$, with angles in radians.\n- Test $2$ (single-hour linear ramp quarter-hour profile): Define a linear ramp during hour $8$ to $9$ from $0$ (kW) to $80$ (kW). Represent $P_k$ as the average power over each quarter-hour subinterval using the midpoint of each subinterval in $[8,9)$ hours, yielding quarter-hour values of $10$, $30$, $50$, and $70$ (kW) only within $[8,9)$; elsewhere $P_k = 0$ (kW).\n- Test $3$ (constant quarter-hour profile): $P_k = 50$ (kW) for all $k$.\n- Test $4$ (deterministic pseudo-random quarter-hour profile): Use a fixed seed to generate $96$ deterministic samples uniformly in $[0,100]$ (kW).\n\nHourly to quarter-hour to hourly (60→15→60):\n- Test $5$ (triangular hourly profile): For $h \\in \\{0,1,\\dots,23\\}$, define $\\hat{P}_h = \\max\\{0, 100 \\cdot (1 - |h - 12|/6)\\}$ (kW).\n- Test $6$ (spiky hourly profile): For $h \\in \\{0,1,\\dots,23\\}$, define $\\hat{P}_h = 100$ (kW) if $h$ is even, and $\\hat{P}_h = 0$ (kW) if $h$ is odd.\n\nUnit requirements:\n- All power values must be interpreted in kilowatts (kW).\n- All energy values must be computed and reported in kilowatt-hours (kWh).\n- All angles for trigonometric functions must be in radians.\n- The program must compute the absolute total energy differences for the round-trip conversions and report them in kWh.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for the $6$ tests in order $1$ through $6$. For example, the format must be $[r_1,r_2,r_3,r_4,r_5,r_6]$, where each $r_i$ is a float (in kWh).",
            "solution": "The problem requires the design and implementation of an energy-preserving resampling scheme for power time series between quarter-hour and hourly resolutions. The core principle is the conservation of energy, defined as the time integral of power, $E = \\int P(t) dt$. For discrete time series with a piecewise constant power assumption over sampling intervals of duration $\\Delta t$, this integral becomes a sum: $E = \\sum_i P_i \\Delta t$.\n\nFirst, we establish the mathematical formalisms for the two required conversion functions.\n\n**1. Downsampling: Quarter-Hour to Hourly (15-min → 60-min)**\n\nLet $\\{P_k\\}_{k=0}^{95}$ be the quarter-hour power series with a time step of $\\Delta t_{15} = 0.25$ hours. Let $\\{\\hat{P}_h\\}_{h=0}^{23}$ be the corresponding hourly power series with a time step of $\\Delta t_{60} = 1.0$ hour.\n\nThe energy preservation constraint requires that the energy calculated over any given hour is the same, regardless of the resolution used. For an arbitrary hour $h$ (where $h \\in \\{0, 1, \\dots, 23\\}$), the energy calculated from the hourly series is:\n$$E_{h, \\text{hourly}} = \\hat{P}_h \\cdot \\Delta t_{60} = \\hat{P}_h \\cdot 1$$\n\nThe same hour $h$ corresponds to four quarter-hour intervals, with indices $k = 4h, 4h+1, 4h+2,$ and $4h+3$. The total energy over this hour, calculated from the quarter-hour series, is the sum of the energies of these four intervals:\n$$E_{h, \\text{15min}} = \\sum_{j=0}^{3} P_{4h+j} \\cdot \\Delta t_{15} = \\left( P_{4h} + P_{4h+1} + P_{4h+2} + P_{4h+3} \\right) \\cdot 0.25$$\n\nEquating the two expressions for energy ($E_{h, \\text{hourly}} = E_{h, \\text{15min}}$) yields the rule for determining the hourly power value $\\hat{P}_h$:\n$$\\hat{P}_h \\cdot 1 = \\left( P_{4h} + P_{4h+1} + P_{4h+2} + P_{4h+3} \\right) \\cdot 0.25$$\n$$\\hat{P}_h = \\frac{P_{4h} + P_{4h+1} + P_{4h+2} + P_{4h+3}}{4}$$\n\nThis shows that the energy-preserving downsampling operation is equivalent to taking the arithmetic mean of the four quarter-hour power values within each hour.\n\n**2. Upsampling: Hourly to Quarter-Hour (60-min → 15-min)**\n\nLet $\\{\\hat{P}_h\\}_{h=0}^{23}$ be the hourly power series. We need to determine a quarter-hour series $\\{P_k\\}_{k=0}^{95}$ that preserves the energy for each hour. The energy preservation equation for hour $h$ is the same as before:\n$$\\hat{P}_h = \\frac{1}{4} \\sum_{j=0}^{3} P_{4h+j}$$\n\nFor each hour $h$, this is a single linear equation with four unknowns: $\\{P_{4h}, P_{4h+1}, P_{4h+2}, P_{4h+3}\\}$. This is an underdetermined system, admitting infinitely many solutions. To select a unique solution, an additional assumption is necessary. The problem states from the outset that power $P(t)$ is modeled as being piecewise constant on each sampling interval. The simplest and most consistent choice for upsampling is to extend this assumption, postulating that the hourly power value $\\hat{P}_h$ represents a constant power level throughout that entire hour. This leads to the following rule:\n$$P_k = \\hat{P}_{\\lfloor k/4 \\rfloor}$$\nIn other words, all four quarter-hour power values within a given hour are set equal to the power value of that hour:\n$$P_{4h} = P_{4h+1} = P_{4h+2} = P_{4h+3} = \\hat{P}_h$$\nThis choice satisfies the energy conservation constraint, as $\\frac{1}{4}(\\hat{P}_h + \\hat{P}_h + \\hat{P}_h + \\hat{P}_h) = \\hat{P}_h$. This method, also known as zeroth-order hold or nearest-neighbor interpolation, is the chosen upsampling scheme.\n\n**3. Round-Trip Energy Conservation Analysis**\n\nThe problem asks to compute the absolute difference in total energy after a round-trip conversion.\n\n**Case A: 15-min → 60-min → 15-min**\n- An initial quarter-hour series $\\{P_k\\}$ is downsampled to an hourly series $\\{\\hat{P}_h\\}$, where $\\hat{P}_h = \\frac{1}{4}\\sum_{j=0}^{3} P_{4h+j}$.\n- This hourly series is then upsampled to a new quarter-hour series $\\{P'_k\\}$, where $P'_k = \\hat{P}_{\\lfloor k/4 \\rfloor}$.\n- The total energy of the original series is $E_{\\text{orig}} = \\Delta t_{15} \\sum_{k=0}^{95} P_k = 0.25 \\sum_{k=0}^{95} P_k$.\n- The total energy of the reconstructed series is $E_{\\text{recon}} = \\Delta t_{15} \\sum_{k=0}^{95} P'_k$.\nSubstituting the definitions:\n$$E_{\\text{recon}} = 0.25 \\sum_{h=0}^{23} \\sum_{j=0}^{3} P'_{4h+j} = 0.25 \\sum_{h=0}^{23} \\sum_{j=0}^{3} \\hat{P}_h = 0.25 \\sum_{h=0}^{23} 4\\hat{P}_h = \\sum_{h=0}^{23} \\hat{P}_h$$\nNow, substituting the formula for $\\hat{P}_h$:\n$$E_{\\text{recon}} = \\sum_{h=0}^{23} \\left( \\frac{1}{4}\\sum_{j=0}^{3} P_{4h+j} \\right) = 0.25 \\sum_{h=0}^{23} \\sum_{j=0}^{3} P_{4h+j} = 0.25 \\sum_{k=0}^{95} P_k = E_{\\text{orig}}$$\nThus, the total energy is analytically preserved for this round trip, and the absolute difference $|E_{\\text{orig}} - E_{\\text{recon}}|$ is $0$.\n\n**Case B: 60-min → 15-min → 60-min**\n- An initial hourly series $\\{\\hat{P}_h\\}$ is upsampled to a quarter-hour series $\\{P_k\\}$, where $P_k = \\hat{P}_{\\lfloor k/4 \\rfloor}$.\n- This quarter-hour series is then downsampled to a new hourly series $\\{\\hat{P}'_h\\}$.\n$$\\hat{P}'_h = \\frac{1}{4}\\sum_{j=0}^{3} P_{4h+j} = \\frac{1}{4}\\left(\\hat{P}_h + \\hat{P}_h + \\hat{P}_h + \\hat{P}_h\\right) = \\hat{P}_h$$\nThe reconstructed hourly series $\\{\\hat{P}'_h\\}$ is identical to the original series $\\{\\hat{P}_h\\}$. Consequently, their total energies, $E_{\\text{recon}} = \\sum \\hat{P}'_h \\cdot 1$ and $E_{\\text{orig}} = \\sum \\hat{P}_h \\cdot 1$, are identical. The absolute difference $|E_{\\text{orig}} - E_{\\text{recon}}|$ is also $0$.\n\nThe implementation will compute these differences for various test profiles. Any non-zero results will be attributable to floating-point arithmetic inaccuracies.",
            "answer": "```python\nimport numpy as np\n\ndef downsample_15_to_60(p_15min: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Downsamples a quarter-hour power series to an hourly series by averaging.\n    This preserves energy as per the derivation.\n    \"\"\"\n    if p_15min.shape != (96,):\n        raise ValueError(\"Input must be a numpy array of shape (96,).\")\n    # Reshape the 96-sample series into 24 hours of 4 quarter-hour samples\n    # and compute the mean for each hour.\n    return p_15min.reshape(24, 4).mean(axis=1)\n\ndef upsample_60_to_15(p_60min: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Upsamples an hourly power series to a quarter-hour series by repetition.\n    This preserves energy under the piecewise-constant power assumption.\n    \"\"\"\n    if p_60min.shape != (24,):\n        raise ValueError(\"Input must be a numpy array of shape (24,).\")\n    # Repeat each hourly value 4 times to create the quarter-hour series.\n    return p_60min.repeat(4)\n\ndef calculate_energy(power_series: np.ndarray, dt: float) -> float:\n    \"\"\"\n    Calculates the total energy for a time series with a given time step.\n    \"\"\"\n    return np.sum(power_series) * dt\n\ndef solve():\n    \"\"\"\n    Runs the test suite and computes the energy differences for round-trip conversions.\n    \"\"\"\n    results = []\n    \n    # --- Test Cases 1-4: 15-min -> 60-min -> 15-min ---\n\n    # Test 1: Solar-like quarter-hour profile\n    k = np.arange(96)\n    t_k = k * 0.25\n    p15_orig_1 = np.maximum(0, 100.0 * np.sin(np.pi * (t_k - 6.0) / 12.0))\n    \n    # Test 2: Single-hour linear ramp quarter-hour profile\n    p15_orig_2 = np.zeros(96)\n    p15_orig_2[32:36] = [10.0, 30.0, 50.0, 70.0]\n\n    # Test 3: Constant quarter-hour profile\n    p15_orig_3 = np.full(96, 50.0)\n\n    # Test 4: Deterministic pseudo-random quarter-hour profile\n    np.random.seed(42)\n    p15_orig_4 = np.random.uniform(0.0, 100.0, 96)\n\n    test_cases_15min = [p15_orig_1, p15_orig_2, p15_orig_3, p15_orig_4]\n\n    for p15_orig in test_cases_15min:\n        E_orig = calculate_energy(p15_orig, 0.25)\n        p60_resampled = downsample_15_to_60(p15_orig)\n        p15_recon = upsample_60_to_15(p60_resampled)\n        E_recon = calculate_energy(p15_recon, 0.25)\n        diff = np.abs(E_orig - E_recon)\n        results.append(diff)\n        \n    # --- Test Cases 5-6: 60-min -> 15-min -> 60-min ---\n\n    # Test 5: Triangular hourly profile\n    h = np.arange(24)\n    p60_orig_5 = np.maximum(0, 100.0 * (1.0 - np.abs(h - 12.0) / 6.0))\n\n    # Test 6: Spiky hourly profile\n    p60_orig_6 = 100.0 * (h % 2 == 0)\n\n    test_cases_60min = [p60_orig_5, p60_orig_6]\n\n    for p60_orig in test_cases_60min:\n        E_orig = calculate_energy(p60_orig, 1.0)\n        p15_resampled = upsample_60_to_15(p60_orig)\n        p60_recon = downsample_15_to_60(p15_resampled)\n        E_recon = calculate_energy(p60_recon, 1.0)\n        diff = np.abs(E_orig - E_recon)\n        results.append(diff)\n        \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Raw outputs from numerical weather models, while powerful, often exhibit systematic biases when compared to real-world observations. To generate a realistic renewable availability profile, these biases must be corrected. This advanced practice introduces you to quantile mapping, a sophisticated statistical method used to align a modeled data distribution with an observed one . By deriving the mapping function and applying it to correct a wind speed time series, you will learn how to produce a statistically robust and physically credible availability profile, a cornerstone of high-fidelity resource assessment and system planning.",
            "id": "4115842",
            "problem": "A wind energy analyst aims to construct an availability time series for a wind farm by correcting a modeled wind speed time series using a distribution-preserving mapping and then applying a turbine power curve. Let the modeled wind speed be a continuous random variable $v_{\\text{mod}}$ with cumulative distribution function (CDF) $F_{\\text{mod}}(v)$ and probability density function (PDF) $f_{\\text{mod}}(v)$, and let the observationally calibrated wind speed distribution at the site be described by a CDF $F_{\\text{obs}}(v)$ and a PDF $f_{\\text{obs}}(v)$. Assume all CDFs are strictly increasing on their support and continuously differentiable. The corrected wind speed $v_{\\text{corr}}$ is defined to be a monotone transformation of $v_{\\text{mod}}$ that preserves cumulative probabilities so that the corrected variable follows the observed distribution.\n\nThe wind turbine is characterized by a standard idealized power curve with cut-in speed $v_{ci} > 0$, rated speed $v_{r} > v_{ci}$, and cut-out speed $v_{co} > v_{r}$. The electrical power $P(v)$ produced at wind speed $v$ and the rated power $P_{r}$ satisfy $P(v) = 0$ for $v < v_{ci}$, $P(v) = P_{r} \\left( \\frac{v}{v_{r}} \\right)^{3}$ for $v \\in [v_{ci}, v_{r}]$, $P(v) = P_{r}$ for $v \\in [v_{r}, v_{co}]$, and $P(v) = 0$ for $v > v_{co}$. The availability at time $t$ is defined as the fraction of rated output $A(t) = \\frac{P(v(t))}{P_{r}}$.\n\nAssume the modeled and observed wind speed distributions are each Weibull with strictly positive shape and scale parameters: $F_{\\text{mod}}(v) = 1 - \\exp\\!\\left( - \\left( \\frac{v}{c_{m}} \\right)^{k_{m}} \\right)$ and $F_{\\text{obs}}(v) = 1 - \\exp\\!\\left( - \\left( \\frac{v}{c_{o}} \\right)^{k_{o}} \\right)$, with $k_{m} > 0$, $c_{m} > 0$, $k_{o} > 0$, and $c_{o} > 0$.\n\nTasks:\n1. Using fundamental definitions from probability theory, derive the unique monotone mapping $v_{\\text{corr}}$ that sends realizations of $v_{\\text{mod}}$ to realizations that follow $F_{\\text{obs}}$, by requiring equality of cumulative probabilities under the mapping. Express $v_{\\text{corr}}$ explicitly in terms of $F_{\\text{mod}}$, $F_{\\text{obs}}$, and $v_{\\text{mod}}$ without introducing any additional assumptions beyond continuity and strict monotonicity of the CDFs.\n2. Specialize your result to the case of Weibull distributions given above and obtain a closed-form expression for $v_{\\text{corr}}$ as a function of $v_{\\text{mod}}$, $k_{m}$, $c_{m}$, $k_{o}$, and $c_{o}$.\n3. Define the corrected availability function $A_{\\text{corr}}(v_{\\text{mod}})$ by composing the power curve with your mapping from part 2. Identify the thresholds in $v_{\\text{mod}}$ corresponding to $v_{ci}$, $v_{r}$, and $v_{co}$ under this mapping.\n4. Compute the expected corrected availability (i.e., the corrected capacity factor) $\\mathbb{E}[A_{\\text{corr}}(V_{\\text{mod}})]$, where $V_{\\text{mod}} \\sim \\text{Weibull}(k_{m}, c_{m})$. Express your final result as a single closed-form analytic expression in terms of $v_{ci}$, $v_{r}$, $v_{co}$, $k_{o}$, and $c_{o}$, and standard special functions if needed. Express the final answer as a dimensionless fraction (do not use a percentage sign). No numerical substitution or rounding is required.",
            "solution": "The problem statement is subjected to validation.\n\n### Step 1: Extract Givens\n- Modeled wind speed: random variable $v_{\\text{mod}}$, CDF $F_{\\text{mod}}(v)$, PDF $f_{\\text{mod}}(v)$.\n- Observed wind speed: random variable with CDF $F_{\\text{obs}}(v)$, PDF $f_{\\text{obs}}(v)$.\n- Assumption: CDFs are strictly increasing and continuously differentiable.\n- Corrected wind speed $v_{\\text{corr}}$ is a monotone transformation of $v_{\\text{mod}}$ such that the distribution of $v_{\\text{corr}}$ is described by $F_{\\text{obs}}(v)$.\n- Power curve $P(v)$ with rated power $P_r$:\n  - $v_{ci}$: cut-in speed, $v_{ci} > 0$.\n  - $v_{r}$: rated speed, $v_{r} > v_{ci}$.\n  - $v_{co}$: cut-out speed, $v_{co} > v_r$.\n  - $P(v) = 0$ for $v < v_{ci}$ or $v > v_{co}$.\n  - $P(v) = P_{r} \\left( \\frac{v}{v_{r}} \\right)^{3}$ for $v \\in [v_{ci}, v_{r}]$.\n  - $P(v) = P_{r}$ for $v \\in [v_{r}, v_{co}]$.\n- Availability: $A(t) = \\frac{P(v(t))}{P_{r}}$.\n- Modeled CDF: $F_{\\text{mod}}(v) = 1 - \\exp\\left( - \\left( \\frac{v}{c_{m}} \\right)^{k_{m}} \\right)$ with shape $k_{m} > 0$ and scale $c_{m} > 0$.\n- Observed CDF: $F_{\\text{obs}}(v) = 1 - \\exp\\left( - \\left( \\frac{v}{c_{o}} \\right)^{k_{o}} \\right)$ with shape $k_{o} > 0$ and scale $c_{o} > 0$.\n- Tasks:\n  1. Derive the general mapping $v_{\\text{corr}}(v_{\\text{mod}})$.\n  2. Specialize the mapping for the given Weibull distributions.\n  3. Define the corrected availability $A_{\\text{corr}}(v_{\\text{mod}})$ and find thresholds in $v_{\\text{mod}}$.\n  4. Compute the expected corrected availability $\\mathbb{E}[A_{\\text{corr}}(V_{\\text{mod}})]$ in terms of $v_{ci}, v_{r}, v_{co}, k_{o}, c_{o}$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed for validity.\n- **Scientifically Grounded:** The problem uses established statistical methods (quantile mapping, also known as distribution mapping or statistical calibration) commonly applied in climate science and energy systems modeling to correct biases in model outputs. The use of Weibull distributions for wind speed and an idealized power curve are standard practices in wind energy analysis. The problem is fundamentally sound.\n- **Well-Posed:** All necessary functions and parameters are defined. The condition that CDFs are strictly increasing ensures that their inverses (quantile functions) are well-defined, which is critical for the derivation. The tasks are specified clearly and lead to a unique mathematical solution.\n- **Objective:** The problem is stated using formal, objective mathematical language, free from subjectivity or ambiguity.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. It is self-contained, scientifically grounded, and well-posed. The solution process may proceed.\n\n---\n\nThe solution is developed by addressing each task sequentially.\n\n**Task 1: Derive the general monotone mapping**\nThe requirement is that the corrected variable $v_{\\text{corr}}$ follows the observed distribution $F_{\\text{obs}}$, and that the mapping from $v_{\\text{mod}}$ to $v_{\\text{corr}}$ preserves cumulative probabilities. This means that for any realization $v_{\\text{mod}}$ and its corresponding transformed value $v_{\\text{corr}}$, the probability of a random value from the modeled distribution being less than or equal to $v_{\\text{mod}}$ must equal the probability of a random value from the observed distribution being less than or equal to $v_{\\text{corr}}$.\n\nLet $V_{\\text{mod}}$ be the random variable for modeled speed and $V_{\\text{corr}}$ be for the corrected speed. The condition is:\n$$P(V_{\\text{mod}} \\le v_{\\text{mod}}) = P(V_{\\text{corr}} \\le v_{\\text{corr}})$$\nBy definition of the CDF, and the fact that $V_{\\text{corr}}$ follows the observed distribution, this becomes:\n$$F_{\\text{mod}}(v_{\\text{mod}}) = F_{\\text{obs}}(v_{\\text{corr}})$$\nTo solve for $v_{\\text{corr}}$ as a function of $v_{\\text{mod}}$, we must apply the inverse of the function $F_{\\text{obs}}$. Since $F_{\\text{obs}}$ is given as strictly increasing and continuous, its inverse function, the quantile function $F_{\\text{obs}}^{-1}$, is well-defined. Applying it to both sides of the equation yields the unique monotone mapping:\n$$v_{\\text{corr}}(v_{\\text{mod}}) = F_{\\text{obs}}^{-1}\\left( F_{\\text{mod}}(v_{\\text{mod}}) \\right)$$\nThis technique is known as quantile mapping.\n\n**Task 2: Specialize for Weibull distributions**\nFirst, we find the inverse CDF $F_{\\text{obs}}^{-1}(u)$ for the observed Weibull distribution. Let $u = F_{\\text{obs}}(v)$.\n$$u = 1 - \\exp\\left( - \\left( \\frac{v}{c_{o}} \\right)^{k_{o}} \\right)$$\nSolving for $v$:\n$$1 - u = \\exp\\left( - \\left( \\frac{v}{c_{o}} \\right)^{k_{o}} \\right)$$\n$$\\ln(1 - u) = - \\left( \\frac{v}{c_{o}} \\right)^{k_{o}}$$\n$$-\\ln(1 - u) = \\left( \\frac{v}{c_{o}} \\right)^{k_{o}}$$\n$$v = c_{o} \\left( -\\ln(1 - u) \\right)^{\\frac{1}{k_{o}}}$$\nThus, the quantile function is $F_{\\text{obs}}^{-1}(u) = c_{o} \\left( -\\ln(1 - u) \\right)^{1/k_{o}}$.\n\nNow, we substitute $u = F_{\\text{mod}}(v_{\\text{mod}})$ into this expression.\n$$u = F_{\\text{mod}}(v_{\\text{mod}}) = 1 - \\exp\\left( - \\left( \\frac{v_{\\text{mod}}}{c_{m}} \\right)^{k_{m}} \\right)$$\nThis gives $1 - u = \\exp\\left( - \\left( \\frac{v_{\\text{mod}}}{c_{m}} \\right)^{k_{m}} \\right)$.\nSubstituting this into the expression for $v_{\\text{corr}}$:\n$$v_{\\text{corr}} = c_{o} \\left( -\\ln\\left( \\exp\\left( - \\left( \\frac{v_{\\text{mod}}}{c_{m}} \\right)^{k_{m}} \\right) \\right) \\right)^{\\frac{1}{k_{o}}}$$\n$$v_{\\text{corr}} = c_{o} \\left( \\left( \\frac{v_{\\text{mod}}}{c_{m}} \\right)^{k_{m}} \\right)^{\\frac{1}{k_{o}}}$$\nThe closed-form expression for the mapping is:\n$$v_{\\text{corr}}(v_{\\text{mod}}) = c_{o} \\left( \\frac{v_{\\text{mod}}}{c_{m}} \\right)^{\\frac{k_{m}}{k_{o}}}$$\n\n**Task 3: Corrected availability and thresholds**\nThe availability function is $A(v) = P(v)/P_r$. The corrected availability $A_{\\text{corr}}(v_{\\text{mod}})$ is found by composing $A(v)$ with the mapping $v_{\\text{corr}}(v_{\\text{mod}})$.\n$$A_{\\text{corr}}(v_{\\text{mod}}) = \\frac{P(v_{\\text{corr}}(v_{\\text{mod}}))}{P_r} =\n\\begin{cases}\n0 & v_{\\text{corr}} < v_{ci} \\\\\n\\left( \\frac{v_{\\text{corr}}}{v_{r}} \\right)^{3} & v_{ci} \\le v_{\\text{corr}} < v_{r} \\\\\n1 & v_{r} \\le v_{\\text{corr}} < v_{co} \\\\\n0 & v_{\\text{corr}} \\ge v_{co}\n\\end{cases}\n$$\nThe thresholds in $v_{\\text{mod}}$ are found by inverting the mapping from Task 2 to express $v_{\\text{mod}}$ in terms of $v_{\\text{corr}}$.\nFrom $v_{\\text{corr}} = c_{o} \\left( \\frac{v_{\\text{mod}}}{c_{m}} \\right)^{k_{m}/k_{o}}$, we solve for $v_{\\text{mod}}$:\n$$\\frac{v_{\\text{corr}}}{c_{o}} = \\left( \\frac{v_{\\text{mod}}}{c_{m}} \\right)^{\\frac{k_{m}}{k_{o}}}$$\n$$\\left( \\frac{v_{\\text{corr}}}{c_{o}} \\right)^{\\frac{k_{o}}{k_{m}}} = \\frac{v_{\\text{mod}}}{c_{m}}$$\n$$v_{\\text{mod}}(v_{\\text{corr}}) = c_{m} \\left( \\frac{v_{\\text{corr}}}{c_{o}} \\right)^{\\frac{k_{o}}{k_{m}}}$$\nThe thresholds in $v_{\\text{mod}}$ are:\n- $v_{\\text{mod},ci} = c_{m} \\left( \\frac{v_{ci}}{c_{o}} \\right)^{k_{o}/k_{m}}$\n- $v_{\\text{mod},r} = c_{m} \\left( \\frac{v_{r}}{c_{o}} \\right)^{k_{o}/k_{m}}$\n- $v_{\\text{mod},co} = c_{m} \\left( \\frac{v_{co}}{c_{o}} \\right)^{k_{o}/k_{m}}$\n\n**Task 4: Expected corrected availability**\nWe need to compute $\\mathbb{E}[A_{\\text{corr}}(V_{\\text{mod}})]$, where $V_{\\text{mod}} \\sim \\text{Weibull}(k_{m}, c_{m})$. A fundamental property of this transformation is that the expectation of a function of the corrected variable can be computed over the target distribution. That is, if $g(v)$ is any function, then $\\mathbb{E}[g(V_{\\text{corr}})] = \\int g(v) f_{\\text{obs}}(v) dv$.\nBy construction, $V_{\\text{corr}}$ is a random variable following the observed distribution, $F_{\\text{obs}}$. Therefore,\n$$\\mathbb{E}[A_{\\text{corr}}(V_{\\text{mod}})] = \\mathbb{E}[A(V_{\\text{corr}})]$$\nwhere the expectation is taken over the distribution of $V_{\\text{corr}}$, which is the observed Weibull distribution with parameters $k_{o}$ and $c_{o}$. This insight simplifies the problem greatly, as the parameters $k_m$ and $c_m$ are no longer needed for this calculation, as requested by the problem statement.\nThe expected availability is given by the integral:\n$$\\mathbb{E}[A] = \\int_{0}^{\\infty} A(v) f_{\\text{obs}}(v) dv$$\nUsing the piecewise definition of $A(v)$:\n$$\\mathbb{E}[A] = \\int_{v_{ci}}^{v_{r}} \\left(\\frac{v}{v_{r}}\\right)^3 f_{\\text{obs}}(v) dv + \\int_{v_{r}}^{v_{co}} 1 \\cdot f_{\\text{obs}}(v) dv$$\nThe second integral is straightforward:\n$$\\int_{v_{r}}^{v_{co}} f_{\\text{obs}}(v) dv = F_{\\text{obs}}(v_{co}) - F_{\\text{obs}}(v_{r}) = \\left(1 - \\exp\\left(-\\left(\\frac{v_{co}}{c_{o}}\\right)^{k_{o}}\\right)\\right) - \\left(1 - \\exp\\left(-\\left(\\frac{v_{r}}{c_{o}}\\right)^{k_{o}}\\right)\\right)$$\n$$= \\exp\\left(-\\left(\\frac{v_{r}}{c_{o}}\\right)^{k_{o}}\\right) - \\exp\\left(-\\left(\\frac{v_{co}}{c_{o}}\\right)^{k_{o}}\\right)$$\nFor the first integral, let's substitute the Weibull PDF $f_{\\text{obs}}(v) = \\frac{k_{o}}{c_{o}} \\left( \\frac{v}{c_{o}} \\right)^{k_{o}-1} \\exp\\left( - \\left( \\frac{v}{c_{o}} \\right)^{k_{o}} \\right)$.\nThe integral is $\\frac{1}{v_r^3}\\int_{v_{ci}}^{v_{r}} v^3 f_{\\text{obs}}(v) dv$. We perform a change of variables. Let $x = (v/c_{o})^{k_{o}}$. Then $v = c_o x^{1/k_o}$. The differential is $dv = c_o \\frac{1}{k_o} x^{\\frac{1}{k_o}-1} dx$.\nAlternatively, and more simply, notice that $dx = \\frac{k_o}{c_o} (\\frac{v}{c_o})^{k_o-1} dv$. This means $f_{\\text{obs}}(v) dv = \\exp(-x) dx$.\nWe express $v^3$ in terms of $x$: $v^3 = (c_o x^{1/k_o})^3 = c_o^3 x^{3/k_o}$.\nThe limits of integration change from $v \\in [v_{ci}, v_{r}]$ to $x \\in [x_{ci}, x_{r}]$, where $x_{ci} = (v_{ci}/c_{o})^{k_{o}}$ and $x_{r} = (v_{r}/c_{o})^{k_{o}}$.\nThe integral becomes:\n$$\\frac{1}{v_r^3} \\int_{x_{ci}}^{x_{r}} \\left(c_o^3 x^{3/k_o}\\right) \\exp(-x) dx = \\frac{c_o^3}{v_r^3} \\int_{x_{ci}}^{x_{r}} x^{3/k_o} \\exp(-x) dx$$\nThis integral is defined in terms of the lower incomplete gamma function, $\\gamma(s, z) = \\int_0^z t^{s-1} e^{-t} dt$.\nOur integral has the form $\\int x^{a} e^{-x} dx$, where $a = 3/k_o$. We match this to the definition of $\\gamma(s,z)$ by setting $s-1 = a$, so $s = 1+a = 1 + 3/k_o$.\n$$\\int_{x_{ci}}^{x_{r}} x^{3/k_o} \\exp(-x) dx = \\gamma\\left(1+\\frac{3}{k_o}, x_r\\right) - \\gamma\\left(1+\\frac{3}{k_o}, x_{ci}\\right)$$\nSubstituting the expressions for $x_{ci}$ and $x_{r}$:\n$$\\frac{c_o^3}{v_r^3} \\left[ \\gamma\\left(1+\\frac{3}{k_o}, \\left(\\frac{v_r}{c_o}\\right)^{k_o}\\right) - \\gamma\\left(1+\\frac{3}{k_o}, \\left(\\frac{v_{ci}}{c_o}\\right)^{k_o}\\right) \\right]$$\nCombining the two parts gives the final expression for the expected availability:\n$$\\mathbb{E}[A] = \\frac{c_o^3}{v_r^3} \\left[ \\gamma\\left(1+\\frac{3}{k_o}, \\left(\\frac{v_r}{c_o}\\right)^{k_o}\\right) - \\gamma\\left(1+\\frac{3}{k_o}, \\left(\\frac{v_{ci}}{c_o}\\right)^{k_o}\\right) \\right] + \\exp\\left(-\\left(\\frac{v_{r}}{c_{o}}\\right)^{k_{o}}\\right) - \\exp\\left(-\\left(\\frac{v_{co}}{c_{o}}\\right)^{k_{o}}\\right)$$\nThis expression is the corrected capacity factor, expressed solely in terms of the observed distribution parameters and the turbine characteristics, as required.",
            "answer": "$$\n\\boxed{\n\\frac{c_o^3}{v_r^3} \\left[ \\gamma\\left(1+\\frac{3}{k_o}, \\left(\\frac{v_r}{c_o}\\right)^{k_o}\\right) - \\gamma\\left(1+\\frac{3}{k_o}, \\left(\\frac{v_{ci}}{c_o}\\right)^{k_o}\\right) \\right] + \\exp\\left(-\\left(\\frac{v_{r}}{c_{o}}\\right)^{k_{o}}\\right) - \\exp\\left(-\\left(\\frac{v_{co}}{c_{o}}\\right)^{k_{o}}\\right)\n}\n$$"
        }
    ]
}