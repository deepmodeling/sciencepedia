{
    "hands_on_practices": [
        {
            "introduction": "Before beginning a complex calibration, a critical first step is to assess which model parameters can be reliably estimated from the available data. This concept, known as parameter identifiability, can be explored using local sensitivity analysis. This practice guides you through calculating the sensitivity of a heat pump performance model's output to each of its parameters, teaching you how to use these sensitivity magnitudes to diagnose which parameters are well-posed for calibration and which might be problematic.",
            "id": "4073866",
            "problem": "A single-stage vapor-compression heat pump is characterized at steady state by a parametric performance map that predicts its heating capacity as a function of operating conditions. Let the operating-point vector be $x_{t} = (T_{h}, T_{c}, f, m)$, where $T_{h}$ is the condenser refrigerant temperature in Kelvin, $T_{c}$ is the evaporator refrigerant temperature in Kelvin, $f$ is the compressor electrical frequency in hertz, and $m$ is the refrigerant mass flow rate in kilograms per second. The model for the heating capacity is\n$$\nf_{\\theta}(x) \\;=\\; \\theta_{1} \\;+\\; \\theta_{2}\\,(T_{h} - T_{c}) \\;+\\; \\theta_{3}\\,\\ln\\!\\left(\\frac{T_{h}}{T_{c}}\\right) \\;+\\; \\theta_{4}\\,\\frac{f}{f + \\theta_{5}} \\;+\\; \\theta_{6}\\,\\exp\\!\\left(-\\frac{\\theta_{7}}{m}\\right),\n$$\nwhere $\\theta = (\\theta_{1}, \\theta_{2}, \\theta_{3}, \\theta_{4}, \\theta_{5}, \\theta_{6}, \\theta_{7})$ is the vector of unknown parameters, and $f_{\\theta}(x)$ is expressed in kilowatts. Assume the local operating point is $T_{h} = 315 \\ \\mathrm{K}$, $T_{c} = 275 \\ \\mathrm{K}$, $f = 50 \\ \\mathrm{Hz}$, and $m = 0.10 \\ \\mathrm{kg \\ s^{-1}}$. The current parameter estimate is $\\theta_{1} = 3.0 \\ \\mathrm{kW}$, $\\theta_{2} = 0.12 \\ \\mathrm{kW \\ K^{-1}}$, $\\theta_{3} = 1.5 \\ \\mathrm{kW}$, $\\theta_{4} = 2.0 \\ \\mathrm{kW}$, $\\theta_{5} = 60 \\ \\mathrm{Hz}$, $\\theta_{6} = 1.2 \\ \\mathrm{kW}$, and $\\theta_{7} = 0.05 \\ \\mathrm{kg \\ s^{-1}}$.\n\nStarting from first principles—namely, the definition of local sensitivity as the partial derivative of the output with respect to a parameter, and the role of sensitivity in parameter estimation under an additive measurement-noise model—do the following:\n\n- Compute the local sensitivities $\\frac{\\partial f_{\\theta}(x_{t})}{\\partial \\theta_{i}}$ for $i \\in \\{1,2,3,4,5,6,7\\}$ at the operating point $x_{t}$.\n- Form the dimensionless, parameter-scaled sensitivity magnitudes\n$$\n\\tilde{s}_{i} \\;=\\; \\left| \\frac{\\partial f_{\\theta}(x_{t})}{\\partial \\theta_{i}} \\cdot \\frac{\\theta_{i}}{f_{\\theta}(x_{t})} \\right|\n$$\nfor $i \\in \\{1,2,3,4,5,6,7\\}$, and then compute the infinity-norm of the scaled sensitivity vector, $\\|\\tilde{s}\\|_{\\infty} = \\max_{i} \\tilde{s}_{i}$. Round your final reported infinity-norm to four significant figures and express it as a pure number (dimensionless).\n- Explain qualitatively, using the framework of the Fisher Information Matrix (FIM) for Gaussian measurement noise, how the magnitudes of these scaled sensitivities guide which parameters can be reliably calibrated at $x_{t}$ and which require additional excitation or experimental design.\n\nYour final answer must be the single numerical value of $\\|\\tilde{s}\\|_{\\infty}$, rounded to four significant figures, expressed as a pure number with no units.",
            "solution": "The problem statement has been validated and is deemed valid. It is scientifically grounded, well-posed, objective, and contains all necessary information for a complete solution. There are no contradictions, ambiguities, or unsound premises. We may proceed with the solution.\n\nThe problem requires a three-part analysis:\n1.  Derivation and computation of local sensitivities of a heat pump model output with respect to its parameters.\n2.  Calculation of the dimensionless, parameter-scaled sensitivity magnitudes and their infinity-norm.\n3.  A qualitative explanation of the role of these sensitivities in parameter identifiability, framed by the Fisher Information Matrix (FIM).\n\nThe model for the heating capacity, $f_{\\theta}(x)$, is given by:\n$$f_{\\theta}(x) \\;=\\; \\theta_{1} \\;+\\; \\theta_{2}\\,(T_{h} - T_{c}) \\;+\\; \\theta_{3}\\,\\ln\\!\\left(\\frac{T_{h}}{T_{c}}\\right) \\;+\\; \\theta_{4}\\,\\frac{f}{f + \\theta_{5}} \\;+\\; \\theta_{6}\\,\\exp\\!\\left(-\\frac{\\theta_{7}}{m}\\right)$$\nThe operating point is $x_{t} = (T_{h}, T_{c}, f, m)$ with $T_{h} = 315 \\ \\mathrm{K}$, $T_{c} = 275 \\ \\mathrm{K}$, $f = 50 \\ \\mathrm{Hz}$, and $m = 0.10 \\ \\mathrm{kg \\ s^{-1}}$.\nThe parameter vector is $\\theta = (\\theta_{1}, \\theta_{2}, \\theta_{3}, \\theta_{4}, \\theta_{5}, \\theta_{6}, \\theta_{7})$ with values $\\theta_1 = 3.0 \\ \\mathrm{kW}$, $\\theta_2 = 0.12 \\ \\mathrm{kW \\ K^{-1}}$, $\\theta_3 = 1.5 \\ \\mathrm{kW}$, $\\theta_4 = 2.0 \\ \\mathrm{kW}$, $\\theta_5 = 60 \\ \\mathrm{Hz}$, $\\theta_6 = 1.2 \\ \\mathrm{kW}$, and $\\theta_7 = 0.05 \\ \\mathrm{kg \\ s^{-1}}$.\n\nFirst, we compute the local sensitivities, which are the partial derivatives of the model output $f_{\\theta}(x)$ with respect to each parameter $\\theta_{i}$, evaluated at the given operating point $x_t$ and parameter estimate $\\theta$.\n\nThe partial derivatives are:\n$$ \\frac{\\partial f_{\\theta}}{\\partial \\theta_{1}} = 1 $$\n$$ \\frac{\\partial f_{\\theta}}{\\partial \\theta_{2}} = T_{h} - T_{c} $$\n$$ \\frac{\\partial f_{\\theta}}{\\partial \\theta_{3}} = \\ln\\left(\\frac{T_{h}}{T_{c}}\\right) $$\n$$ \\frac{\\partial f_{\\theta}}{\\partial \\theta_{4}} = \\frac{f}{f + \\theta_{5}} $$\n$$ \\frac{\\partial f_{\\theta}}{\\partial \\theta_{5}} = \\frac{\\partial}{\\partial \\theta_{5}} \\left( \\theta_{4} f (f + \\theta_{5})^{-1} \\right) = - \\frac{\\theta_{4} f}{(f + \\theta_{5})^{2}} $$\n$$ \\frac{\\partial f_{\\theta}}{\\partial \\theta_{6}} = \\exp\\left(-\\frac{\\theta_{7}}{m}\\right) $$\n$$ \\frac{\\partial f_{\\theta}}{\\partial \\theta_{7}} = \\frac{\\partial}{\\partial \\theta_{7}} \\left( \\theta_{6} \\exp\\left(-\\frac{\\theta_{7}}{m}\\right) \\right) = \\theta_{6} \\exp\\left(-\\frac{\\theta_{7}}{m}\\right) \\left(-\\frac{1}{m}\\right) = -\\frac{\\theta_{6}}{m} \\exp\\left(-\\frac{\\theta_{7}}{m}\\right) $$\n\nNext, we evaluate these derivatives at the specified point:\n$$ \\frac{\\partial f_{\\theta}}{\\partial \\theta_{1}} = 1 $$\n$$ \\frac{\\partial f_{\\theta}}{\\partial \\theta_{2}} = 315 - 275 = 40 $$\n$$ \\frac{\\partial f_{\\theta}}{\\partial \\theta_{3}} = \\ln\\left(\\frac{315}{275}\\right) \\approx 0.135760 $$\n$$ \\frac{\\partial f_{\\theta}}{\\partial \\theta_{4}} = \\frac{50}{50 + 60} = \\frac{50}{110} = \\frac{5}{11} \\approx 0.454545 $$\n$$ \\frac{\\partial f_{\\theta}}{\\partial \\theta_{5}} = - \\frac{2.0 \\cdot 50}{(50 + 60)^{2}} = - \\frac{100}{110^{2}} = - \\frac{100}{12100} = -\\frac{1}{121} \\approx -0.00826446 $$\n$$ \\frac{\\partial f_{\\theta}}{\\partial \\theta_{6}} = \\exp\\left(-\\frac{0.05}{0.10}\\right) = \\exp(-0.5) \\approx 0.606531 $$\n$$ \\frac{\\partial f_{\\theta}}{\\partial \\theta_{7}} = -\\frac{1.2}{0.10} \\exp(-0.5) = -12 \\exp(-0.5) \\approx -7.278368 $$\n\nTo compute the dimensionless, scaled sensitivities $\\tilde{s}_{i}$, we first need the value of the model output $f_{\\theta}(x_t)$:\n$$ f_{\\theta}(x_t) = 3.0 + 0.12(315 - 275) + 1.5\\ln\\left(\\frac{315}{275}\\right) + 2.0\\left(\\frac{50}{50+60}\\right) + 1.2\\exp\\left(-\\frac{0.05}{0.1}\\right) $$\n$$ f_{\\theta}(x_t) \\approx 3.0 + 0.12(40) + 1.5(0.135760) + 2.0(0.454545) + 1.2(0.606531) $$\n$$ f_{\\theta}(x_t) \\approx 3.0 + 4.8 + 0.203640 + 0.909091 + 0.727837 $$\n$$ f_{\\theta}(x_t) \\approx 9.640568 \\ \\mathrm{kW} $$\n\nNow, we compute each scaled sensitivity $\\tilde{s}_{i} = \\left| \\frac{\\partial f_{\\theta}(x_{t})}{\\partial \\theta_{i}} \\cdot \\frac{\\theta_{i}}{f_{\\theta}(x_{t})} \\right|$:\n$$ \\tilde{s}_{1} = \\left| 1 \\cdot \\frac{3.0}{9.640568} \\right| \\approx 0.311183 $$\n$$ \\tilde{s}_{2} = \\left| 40 \\cdot \\frac{0.12}{9.640568} \\right| = \\left| \\frac{4.8}{9.640568} \\right| \\approx 0.497893 $$\n$$ \\tilde{s}_{3} = \\left| 0.135760 \\cdot \\frac{1.5}{9.640568} \\right| \\approx 0.021123 $$\n$$ \\tilde{s}_{4} = \\left| \\frac{5}{11} \\cdot \\frac{2.0}{9.640568} \\right| \\approx 0.094298 $$\n$$ \\tilde{s}_{5} = \\left| -\\frac{1}{121} \\cdot \\frac{60}{9.640568} \\right| \\approx |-0.00826446 \\cdot 6.22361| \\approx 0.051435 $$\n$$ \\tilde{s}_{6} = \\left| \\exp(-0.5) \\cdot \\frac{1.2}{9.640568} \\right| \\approx |0.606531 \\cdot 0.124474| \\approx 0.075498 $$\n$$ \\tilde{s}_{7} = \\left| -12 \\exp(-0.5) \\cdot \\frac{0.05}{9.640568} \\right| \\approx |-7.278368 \\cdot 0.0051864| \\approx 0.037749 $$\n\nThe vector of scaled sensitivity magnitudes is $\\tilde{s} \\approx (0.3112, 0.4979, 0.0211, 0.0943, 0.0514, 0.0755, 0.0377)$.\nThe infinity-norm of this vector is the maximum value among its components:\n$$ \\|\\tilde{s}\\|_{\\infty} = \\max_{i} \\tilde{s}_{i} = \\max\\{0.311183, 0.497893, 0.021123, 0.094298, 0.051435, 0.075498, 0.037749\\} $$\n$$ \\|\\tilde{s}\\|_{\\infty} = \\tilde{s}_{2} \\approx 0.497893 $$\nRounding to four significant figures, we get $\\|\\tilde{s}\\|_{\\infty} = 0.4979$.\n\nFinally, we address the qualitative role of these sensitivities. The framework begins with an additive noise model for measurements of the heating capacity, $y_t = f_{\\theta}(x_t) + \\epsilon_t$, where $\\epsilon_t$ is typically assumed to be i.i.d. Gaussian noise with zero mean and variance $\\sigma^2$, i.e., $\\epsilon_t \\sim \\mathcal{N}(0, \\sigma^2)$.\nThe Fisher Information Matrix (FIM) is a fundamental concept in parameter estimation theory. For a single data point $y_t$, its elements are given by:\n$$ \\mathcal{I}_{ij}(\\theta) = \\frac{1}{\\sigma^2} \\frac{\\partial f_{\\theta}(x_t)}{\\partial \\theta_i} \\frac{\\partial f_{\\theta}(x_t)}{\\partial \\theta_j} $$\nThe importance of the FIM stems from the Cramér-Rao Lower Bound (CRLB), which states that the variance of any unbiased estimator $\\hat{\\theta}$ is bounded below by the inverse of the FIM: $\\mathrm{Cov}(\\hat{\\theta}) \\ge \\mathcal{I}(\\theta)^{-1}$. This implies that the diagonal elements of the FIM, $\\mathcal{I}_{ii}(\\theta) = \\frac{1}{\\sigma^2} \\left(\\frac{\\partial f_{\\theta}(x_t)}{\\partial \\theta_i}\\right)^2$, are inversely related to the minimum achievable variance for the estimate of parameter $\\theta_i$. A large value of $\\mathcal{I}_{ii}(\\theta)$ corresponds to a small variance bound, indicating that $\\theta_i$ can be estimated with high precision.\n\nThe local sensitivity, $\\frac{\\partial f_{\\theta}(x_t)}{\\partial \\theta_i}$, directly determines the magnitude of these FIM diagonal elements. A larger sensitivity magnitude means that a small change in the parameter $\\theta_i$ produces a large change in the model output, making its effect easier to distinguish from measurement noise. The dimensionless scaled sensitivity, $\\tilde{s}_i$, allows for a fair comparison across parameters that have different units and magnitudes. It represents the percentage change in the output for a one-percent change in the parameter.\n\nIn this analysis:\n- Parameters $\\theta_2$ ($\\tilde{s}_2 \\approx 0.498$) and $\\theta_1$ ($\\tilde{s}_1 \\approx 0.311$) exhibit the highest scaled sensitivities. This signifies that at the operating point $x_t$, the model output is most responsive to changes in the temperature-difference coefficient and the baseline offset. Consequently, data collected at or near $x_t$ would be most informative for calibrating $\\theta_1$ and $\\theta_2$.\n- Parameters with low scaled sensitivities, such as $\\theta_3$ ($\\tilde{s}_3 \\approx 0.021$), $\\theta_7$ ($\\tilde{s}_7 \\approx 0.038$), and $\\theta_5$ ($\\tilde{s}_5 \\approx 0.051$), have a weak influence on the output at this specific operating point. Attempting to estimate these parameters using only data from this condition would be unreliable; their effects would be difficult to deconvolve from system noise, leading to large variance in their estimates (a large CRLB) and high correlation with other parameter estimates. The FIM would be ill-conditioned.\n- To reliably calibrate these less sensitive parameters, one must engage in experimental design. This involves selecting additional operating points ($x_t$) that specifically \"excite\" the model's sensitivity to these parameters. For instance, to improve the identifiability of $\\theta_3$, experiments with a wider range of temperature ratios $T_h/T_c$ are needed. To identify $\\theta_5$, the compressor frequency $f$ should be varied, particularly near the current estimate of $60 \\ \\mathrm{Hz}$. For $\\theta_7$, varying the mass flow rate $m$ is essential. This process of selecting informative experiments is known as optimal experimental design, which aims to maximize the \"information\" content of the data, often by maximizing a scalar measure of the FIM (e.g., its determinant).",
            "answer": "$$\\boxed{0.4979}$$"
        },
        {
            "introduction": "A credible model calibration provides not only the best-fit parameter values but also a robust measure of their uncertainty. This exercise introduces the profile likelihood method, a powerful statistical technique for constructing confidence intervals, especially in nonlinear models or when nuisance parameters are present. By calibrating a heat transfer coefficient for a dynamic cooling model, you will learn to move beyond simple point estimates and quantify the confidence in your calibrated parameters in a statistically rigorous way.",
            "id": "4073888",
            "problem": "Consider the calibration of a convective heat transfer coefficient in a lumped-capacitance energy system. A rigid body of known surface area and thermal capacitance exchanges heat with ambient air at a known constant temperature. The fundamental base for this model is Newton's law of cooling and a Gaussian observation model. Let the heat balance be given by Newton's law of cooling applied to a lumped capacitance:\n$$\nC \\frac{dT(t)}{dt} = -h A \\left(T(t) - T_{\\infty}\\right),\n$$\nwhere $T(t)$ is the true body temperature at time $t$, $h$ is the convective heat transfer coefficient, $A$ is the surface area, $C$ is the thermal capacitance, and $T_{\\infty}$ is the ambient temperature. The initial temperature at $t=0$ is $T(0)=T_0$. The solution to this first-order ordinary differential equation yields a temperature trajectory that decays exponentially toward $T_{\\infty}$.\n\nObservations are modeled as independent and identically distributed (IID) Gaussian measurements:\n$$\ny_i = T(t_i; h, T_0) + \\varepsilon_i,\\quad \\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2),\n$$\nfor $i=1,\\dots,n$, where $t_i$ are known sampling times, $y_i$ are observed temperatures, and $\\sigma^2$ is an unknown measurement noise variance. The parameter of interest is $h$, while $T_0$ and $\\sigma^2$ are nuisance parameters. Use the principle of Maximum Likelihood Estimation (MLE) to construct the profile likelihood for $h$ by maximizing the likelihood over the nuisance parameters $(T_0, \\sigma^2)$ for each fixed $h$. Then, based on the asymptotic likelihood ratio, compute an approximate two-sided $95\\%$ confidence interval for $h$.\n\nYour program must:\n- Implement the forward model implied by the ordinary differential equation to compute $T(t; h, T_0)$.\n- For each fixed $h$, analytically or numerically maximize the likelihood over $T_0$ and $\\sigma^2$ to construct the profile log-likelihood for $h$.\n- Compute the MLE $\\hat{h}$ by maximizing the profile log-likelihood over a physically reasonable interval of $h$ in $\\text{W}/(\\text{m}^2\\cdot\\text{K})$.\n- Use the profile likelihood ratio and the chi-square distribution with $1$ degree of freedom to determine the endpoints of the approximate $95\\%$ confidence interval for $h$ in $\\text{W}/(\\text{m}^2\\cdot\\text{K})$.\n- Express all $h$-related outputs in $\\text{W}/(\\text{m}^2\\cdot\\text{K})$ as decimal numbers rounded to exactly $6$ digits after the decimal point.\n\nTest Suite:\nFor all cases, ambient temperature is $T_{\\infty} = 293$ in Kelvin, thermal capacitance is $C = 2000$ in Joules per Kelvin, and surface area is $A = 0.5$ in square meters. Observations are generated by simulating $y_i$ according to the model with the specified true parameters and then adding Gaussian noise $\\varepsilon_i$ with the given standard deviation. Use the following cases, where seeds reference a fixed pseudorandom number generator initialization:\n\n- Case $1$ (happy path, moderate cooling): $T_0 = 353$, $h_{\\text{true}} = 15$, times $t_i$ from $0$ to $1800$ in seconds with step $60$, noise standard deviation $0.5$ Kelvin, seed $1$.\n- Case $2$ (slow cooling, limited dynamics): $T_0 = 298$, $h_{\\text{true}} = 2$, times $t_i$ from $0$ to $1800$ in seconds with step $120$, noise standard deviation $0.5$ Kelvin, seed $2$.\n- Case $3$ (fast cooling, short observation window): $T_0 = 353$, $h_{\\text{true}} = 100$, times $t_i$ from $0$ to $180$ in seconds with step $10$, noise standard deviation $0.5$ Kelvin, seed $3$.\n- Case $4$ (noisy measurements): $T_0 = 333$, $h_{\\text{true}} = 20$, times $t_i$ from $0$ to $1200$ in seconds with step $60$, noise standard deviation $2.0$ Kelvin, seed $4$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each case, output the triple $[\\hat{h}, h_{\\text{low}}, h_{\\text{high}}]$ where $\\hat{h}$ is the MLE, and $h_{\\text{low}}$ and $h_{\\text{high}}$ are the lower and upper bounds of the approximate two-sided $95\\%$ confidence interval for $h$. Aggregate these into one flat list in the order of the cases, formatted to six digits after the decimal point, for example:\n$$\n[\\hat{h}_1, h_{\\text{low},1}, h_{\\text{high},1}, \\hat{h}_2, h_{\\text{low},2}, h_{\\text{high},2}, \\hat{h}_3, h_{\\text{low},3}, h_{\\text{high},3}, \\hat{h}_4, h_{\\text{low},4}, h_{\\text{high},4}].\n$$\nAll $h$ values and interval endpoints must be in $\\text{W}/(\\text{m}^2\\cdot\\text{K})$ and printed with exactly six digits after the decimal point. Angles are not involved. Percentages must not be used; express all confidence levels implicitly by numerical bounds. No user input is required; all data should be generated internally by the program using the specified seeds.",
            "solution": "The problem requires the estimation of a convective heat transfer coefficient, $h$, from noisy temperature measurements of a cooling body. We will determine the Maximum Likelihood Estimate (MLE) for $h$ and construct an approximate $95\\%$ confidence interval using the profile likelihood method. The process involves several steps: deriving the physical model, constructing the statistical likelihood function, profiling out nuisance parameters, and applying asymptotic likelihood theory.\n\n**1. Physical Model: Newton's Law of Cooling**\n\nThe system is governed by a lumped-capacitance heat balance, described by the first-order ordinary differential equation (ODE):\n$$\nC \\frac{dT(t)}{dt} = -h A \\left(T(t) - T_{\\infty}\\right)\n$$\nwhere $T(t)$ is the body's temperature at time $t$, $C$ is its thermal capacitance, $A$ is its surface area, $h$ is the convective heat transfer coefficient, and $T_{\\infty}$ is the constant ambient temperature. The initial condition is $T(0) = T_0$.\n\nThis linear ODE can be solved by separation of variables. Let $\\theta(t) = T(t) - T_{\\infty}$. The equation becomes $C \\frac{d\\theta}{dt} = -hA\\theta$. Integration yields:\n$$\n\\int \\frac{d\\theta}{\\theta} = -\\int \\frac{hA}{C} dt \\implies \\ln(\\theta) = -\\frac{hA}{C}t + K\n$$\nwhere $K$ is the integration constant. Exponentiating gives $\\theta(t) = e^K e^{-\\frac{hA}{C}t}$.\nUsing the initial condition, $\\theta(0) = T(0) - T_{\\infty} = T_0 - T_{\\infty}$, we find the constant $e^K = T_0 - T_{\\infty}$.\nSubstituting back for $\\theta(t)$, we obtain the forward model for the temperature trajectory:\n$$\nT(t; h, T_0) = T_{\\infty} + (T_0 - T_{\\infty}) e^{-\\frac{hA}{C}t}\n$$\nThis function predicts the true temperature at any time $t$ for given parameters $h$ and $T_0$.\n\n**2. Statistical Model and Likelihood Function**\n\nThe temperature observations $\\{y_i\\}_{i=1}^n$ at known times $\\{t_i\\}_{i=1}^n$ are modeled as the true temperature plus independent and identically distributed (IID) Gaussian noise:\n$$\ny_i = T(t_i; h, T_0) + \\varepsilon_i, \\quad \\text{where} \\quad \\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)\n$$\nThe parameters of the full model are $\\theta = (h, T_0, \\sigma^2)$. The probability density of a single observation $y_i$ is:\n$$\np(y_i | h, T_0, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - T(t_i; h, T_0))^2}{2\\sigma^2}\\right)\n$$\nDue to the IID assumption, the joint likelihood of all observations $\\mathbf{y}=(y_1, \\dots, y_n)$ is the product of individual densities:\n$$\nL(h, T_0, \\sigma^2 | \\mathbf{y}) = \\prod_{i=1}^n p(y_i | h, T_0, \\sigma^2) = (2\\pi\\sigma^2)^{-n/2} \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y_i - T(t_i; h, T_0))^2\\right)\n$$\nFor maximization, it is more convenient to work with the log-likelihood function:\n$$\n\\ell(h, T_0, \\sigma^2 | \\mathbf{y}) = -\\frac{n}{2}\\ln(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y_i - T(t_i; h, T_0))^2\n$$\n\n**3. Profile Likelihood for $h$**\n\nThe parameter of interest is $h$, while $T_0$ and $\\sigma^2$ are nuisance parameters. We construct the profile log-likelihood for $h$ by maximizing $\\ell$ over the nuisance parameters for each fixed value of $h$:\n$$\n\\ell_p(h) = \\max_{T_0, \\sigma^2} \\ell(h, T_0, \\sigma^2 | \\mathbf{y})\n$$\nThis maximization is performed in two steps.\n\nFirst, we maximize with respect to $\\sigma^2$ for fixed $h$ and $T_0$. Let $S(h, T_0) = \\sum_{i=1}^n (y_i - T(t_i; h, T_0))^2$ be the sum of squared residuals.\n$$\n\\frac{\\partial \\ell}{\\partial \\sigma^2} = -\\frac{n}{2\\sigma^2} + \\frac{S(h, T_0)}{2(\\sigma^2)^2} = 0 \\implies \\hat{\\sigma}^2(h, T_0) = \\frac{S(h, T_0)}{n}\n$$\nSubstituting $\\hat{\\sigma}^2$ back into $\\ell$ gives the concentrated log-likelihood (profiled over $\\sigma^2$):\n$$\n\\ell_c(h, T_0) = -\\frac{n}{2}(\\ln(2\\pi) + 1) - \\frac{n}{2}\\ln\\left(\\frac{S(h, T_0)}{n}\\right)\n$$\nMaximizing $\\ell_c(h, T_0)$ with respect to $T_0$ is equivalent to minimizing the sum of squared residuals $S(h, T_0)$.\n\nSecond, for a fixed $h$, we find the value of $T_0$ that minimizes $S(h, T_0)$. The model $T(t; h, T_0)$ is linear in $T_0$. Let $\\beta = \\frac{hA}{C}$ and $E_i = e^{-\\beta t_i}$. The model can be written as $T(t_i) = T_{\\infty} + (T_0 - T_{\\infty})E_i = T_0 E_i + T_{\\infty}(1-E_i)$.\nThe minimization of $S(h, T_0) = \\sum_{i=1}^n (y_i - (T_0 E_i + T_{\\infty}(1-E_i)))^2$ is a linear least squares problem for $T_0$. Setting the partial derivative $\\frac{\\partial S}{\\partial T_0}$ to zero yields the analytical solution for the MLE of $T_0$ given $h$:\n$$\n\\hat{T_0}(h) = \\frac{\\sum_{i=1}^n (y_i - T_{\\infty}(1-E_i))E_i}{\\sum_{i=1}^n E_i^2}\n$$\nBy substituting $\\hat{T_0}(h)$ into $S(h, T_0)$, we obtain $S(h, \\hat{T_0}(h))$, which can then be used to compute the profile log-likelihood $\\ell_p(h)$. The MLE for $h$, denoted $\\hat{h}$, is the value that maximizes $\\ell_p(h)$, which is equivalent to minimizing $S(h, \\hat{T_0}(h))$:\n$$\n\\hat{h} = \\underset{h}{\\arg\\max} \\, \\ell_p(h) = \\underset{h}{\\arg\\min} \\, S(h, \\hat{T_0}(h))\n$$\nSince this objective function is nonlinear in $h$, $\\hat{h}$ must be found using numerical optimization.\n\n**4. Approximate Confidence Interval**\n\nBased on Wilks' theorem, an approximate $(1-\\alpha)$ confidence interval for $h$ can be constructed using the likelihood ratio test statistic. The statistic $W(h) = 2(\\ell_p(\\hat{h}) - \\ell_p(h))$ is asymptotically distributed as a chi-square distribution with one degree of freedom ($\\chi^2_1$), as only one parameter ($h$) is being constrained.\n\nThe $95\\%$ confidence interval for $h$ is the set of all values for which the test does not reject the null hypothesis $H_0: h = h_{test}$, i.e., the set of $h$ satisfying:\n$$\n2(\\ell_p(\\hat{h}) - \\ell_p(h)) \\le \\chi^2_{1, 0.95}\n$$\nwhere $\\chi^2_{1, 0.95} \\approx 3.841$ is the $95^{th}$ percentile of the $\\chi^2_1$ distribution. The endpoints of the confidence interval are the solutions to the equation:\n$$\n\\ell_p(h) = \\ell_p(\\hat{h}) - \\frac{\\chi^2_{1, 0.95}}{2}\n$$\nSubstituting the expression for $\\ell_p(h)$ in terms of the sum of squares, this equation can be simplified for enhanced numerical stability.\n$$\n-\\frac{n}{2}\\ln\\left(\\frac{S(h, \\hat{T_0}(h))}{n}\\right) = -\\frac{n}{2}\\ln\\left(\\frac{S(\\hat{h}, \\hat{T_0}(\\hat{h}))}{n}\\right) - \\frac{\\chi^2_{1, 0.95}}{2}\n$$\n$$\n\\ln(S(h, \\hat{T_0}(h))) = \\ln(S(\\hat{h}, \\hat{T_0}(\\hat{h}))) + \\frac{\\chi^2_{1, 0.95}}{n}\n$$\n$$\nS(h, \\hat{T_0}(h)) = S(\\hat{h}, \\hat{T_0}(\\hat{h})) \\cdot \\exp\\left(\\frac{\\chi^2_{1, 0.95}}{n}\\right)\n$$\nThe two roots of this equation, $h_{\\text{low}}$ and $h_{\\text{high}}$, form the lower and upper bounds of the confidence interval. These roots must be found numerically using a root-finding algorithm. The search for $h_{\\text{low}}$ is conducted in the interval $(0, \\hat{h})$ and for $h_{\\text{high}}$ in $(\\hat{h}, \\infty)$.",
            "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize_scalar, brentq\nfrom scipy.stats import chi2\n\ndef solve():\n    \"\"\"\n    Performs Maximum Likelihood Estimation to find the convective heat transfer\n    coefficient 'h' and its 95% confidence interval using profile likelihood.\n    \"\"\"\n    # Global constants for the physical model\n    T_INF = 293.0  # Ambient temperature in Kelvin\n    C = 2000.0     # Thermal capacitance in J/K\n    A = 0.5        # Surface area in m^2\n\n    # Test suite definition\n    test_cases = [\n        # (T0_true, h_true, t_start, t_end, t_step, sigma, seed)\n        (353.0, 15.0, 0, 1800, 60, 0.5, 1),\n        (298.0, 2.0, 0, 1800, 120, 0.5, 2),\n        (353.0, 100.0, 0, 180, 10, 0.5, 3),\n        (333.0, 20.0, 0, 1200, 60, 2.0, 4),\n    ]\n\n    all_results = []\n    \n    # Critical value for 95% CI from chi-square distribution with 1 df\n    crit_val = chi2.ppf(0.95, df=1)\n\n    for T0_true, h_true, t_start, t_end, t_step, sigma, seed in test_cases:\n        # 1. Generate synthetic observation data\n        ts = np.arange(t_start, t_end + 1e-9, t_step) # Use a small epsilon to ensure endpoint inclusion\n        n = len(ts)\n        rng = np.random.default_rng(seed=seed)\n\n        beta_true = h_true * A / C\n        T_model_true = T_INF + (T0_true - T_INF) * np.exp(-beta_true * ts)\n        ys = T_model_true + rng.normal(loc=0.0, scale=sigma, size=n)\n\n        # 2. Define the objective function for minimization\n        # This function computes the sum of squared residuals S(h, T0_hat(h)) for a given h.\n        def get_sum_sq_for_h(h, times, y_obs, cap, area, T_amb):\n            if h <= 0:\n                return np.inf\n\n            beta = h * area / cap\n            E = np.exp(-beta * times)\n            \n            # Analytically find the MLE for T0 given h\n            numerator_T0 = np.sum((y_obs - T_amb * (1 - E)) * E)\n            denominator_T0 = np.sum(E**2)\n            \n            # Avoid division by zero for very large h where E is almost zero.\n            if denominator_T0 < 1e-15:\n                T0_hat = y_obs[0]\n            else:\n                T0_hat = numerator_T0 / denominator_T0\n\n            # Calculate predicted temperatures and sum of squared residuals\n            T_pred = T_amb + (T0_hat - T_amb) * E\n            ssq = np.sum((y_obs - T_pred)**2)\n            return ssq\n\n        # 3. Find h_hat (MLE for h) by minimizing the sum of squares\n        h_search_bounds = (0.01, 500.0)\n        opt_result = minimize_scalar(\n            get_sum_sq_for_h,\n            args=(ts, ys, C, A, T_INF),\n            bounds=h_search_bounds,\n            method='bounded'\n        )\n        h_hat = opt_result.x\n        ssq_min = opt_result.fun\n\n        # 4. Compute the confidence interval using the likelihood ratio test\n        # The endpoints are roots of S(h) = S(h_hat) * exp(chi^2_crit / n)\n        ssq_target = ssq_min * np.exp(crit_val / n)\n\n        def ci_root_function(h, times, y_obs, cap, area, T_amb, target):\n            return get_sum_sq_for_h(h, times, y_obs, cap, area, T_amb) - target\n        \n        # Find lower bound of the CI\n        try:\n            h_low = brentq(\n                ci_root_function,\n                a=h_search_bounds[0],\n                b=h_hat,\n                args=(ts, ys, C, A, T_INF, ssq_target)\n            )\n        except ValueError:\n            h_low = np.nan # Should not happen with well-behaved data\n\n        # Find upper bound of the CI\n        try:\n            h_high = brentq(\n                ci_root_function,\n                a=h_hat,\n                b=h_search_bounds[1],\n                args=(ts, ys, C, A, T_INF, ssq_target)\n            )\n        except ValueError:\n            h_high = np.nan\n\n        all_results.extend([h_hat, h_low, h_high])\n\n    # Format the final output as a comma-separated list of strings\n    formatted_results = [f\"{x:.6f}\" for x in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "For large-scale dynamic models, such as those used in energy system planning or climate science, gradient-based optimization is essential for calibration, but computing gradients can be a major computational bottleneck. This practice introduces the discrete adjoint method, a highly efficient algorithm for calculating exact gradients of a cost function with respect to model parameters. By implementing and verifying an adjoint for a time-stepped energy storage model, you will gain hands-on experience with a cornerstone technique for modern computational modeling and large-scale optimization.",
            "id": "4073858",
            "problem": "Consider a single-state, time-stepped energy storage model derived from conservation of energy. Let $x_t$ denote the state of charge in megawatt-hours (MWh) at discrete time index $t$, let $u_t$ denote a net charging power input in megawatts (MW), and let $\\Delta t$ denote the time step in hours. The storage exhibits self-discharge modeled by a parameter $\\theta$ with units per hour ($\\mathrm{h}^{-1}$). The discrete-time dynamics are defined by the energy balance\n$$\nx_{t+1} = x_t + \\Delta t \\, \\big(-\\theta \\, x_t + u_t\\big),\n$$\nwith a given initial condition $x_0$. For calibration of model parameters in energy systems modeling, consider the tracking objective of the state of charge to a reference profile $r_t$ with nonnegative weights $w_t$ and a quadratic regularization on $\\theta$:\n$$\nJ(\\theta) = \\frac{1}{2} \\sum_{t=0}^{T} w_t \\, \\big(x_t - r_t\\big)^2 \\, \\Delta t \\;+\\; \\frac{1}{2} \\, \\alpha \\, \\theta^2.\n$$\nThe task is to implement a discrete adjoint method for the time-stepped simulator to compute the gradient $\\frac{dJ}{d\\theta}$, and to verify its accuracy by comparing to a central finite difference approximation of $\\frac{dJ}{d\\theta}$.\n\nYour program must:\n- Simulate the forward dynamics for given $(T, \\Delta t, x_0, \\theta, \\{u_t\\}_{t=0}^{T-1}, \\{r_t\\}_{t=0}^{T}, \\{w_t\\}_{t=0}^{T}, \\alpha)$.\n- Implement the discrete adjoint of the simulator to obtain $\\frac{dJ}{d\\theta}$.\n- Implement a central finite difference approximation with perturbation $\\varepsilon$:\n$$\n\\frac{dJ}{d\\theta} \\approx \\frac{J(\\theta + \\varepsilon) - J(\\theta - \\varepsilon)}{2 \\varepsilon}.\n$$\n- Compare the adjoint gradient and the finite difference gradient using a relative-error criterion. Define the absolute difference $d = \\big|\\big(\\frac{dJ}{d\\theta}\\big)_{\\mathrm{adj}} - \\big(\\frac{dJ}{d\\theta}\\big)_{\\mathrm{fd}}\\big|$ and check if $d \\le \\max\\big(\\tau_{\\mathrm{abs}}, \\tau_{\\mathrm{rel}} \\cdot \\max(1, \\big|\\big(\\frac{dJ}{d\\theta}\\big)_{\\mathrm{fd}}\\big|)\\big)$.\n\nUnits and numerical requirements:\n- Interpret $x_t$ in MWh, $u_t$ in MW, $\\Delta t$ in hours, and $\\theta$ in per hour ($\\mathrm{h}^{-1}$).\n- Express the gradient values with respect to $\\theta$ in per hour ($\\mathrm{h}^{-1}$) units as pure numeric floats in the final output.\n- Angles, if any, must be in radians; the provided test suite does not use angles.\n- Percentages, if any, must be expressed as decimals; the provided test suite does not use percentages.\n\nUse the following test suite of parameter values to exercise different regimes of the model and the adjoint:\n- Test case A (general case):\n  - $T = 48$, $\\Delta t = 1.0$, $x_0 = 1.0$, $\\theta = 0.02$, $\\alpha = 0.01$.\n  - For $t = 0, 1, \\dots, T-1$, define $u_t = 0.5 \\cdot \\sin\\!\\big(\\frac{2\\pi t}{24}\\big) + 0.1$.\n  - For $t = 0, 1, \\dots, T$, define $r_t = 1.0$ and $w_t = 1.0$.\n  - Use $\\varepsilon = 10^{-6}$, $\\tau_{\\mathrm{abs}} = 10^{-9}$, $\\tau_{\\mathrm{rel}} = 10^{-6}$.\n- Test case B (boundary case near zero self-discharge):\n  - $T = 24$, $\\Delta t = 1.0$, $x_0 = 0.0$, $\\theta = 0.0$, $\\alpha = 0.0$.\n  - For $t = 0, 1, \\dots, 11$, define $u_t = 0.1$, and for $t = 12, 13, \\dots, 23$, define $u_t = -0.1$.\n  - For $t = 0, 1, \\dots, T$, define $r_t = 0.0$ and $w_t = 1.5$.\n  - Use $\\varepsilon = 10^{-6}$, $\\tau_{\\mathrm{abs}} = 10^{-9}$, $\\tau_{\\mathrm{rel}} = 10^{-6}$.\n- Test case C (single-step edge case with larger self-discharge and nonuniform weights):\n  - $T = 1$, $\\Delta t = 2.0$, $x_0 = 0.0$, $\\theta = 0.5$, $\\alpha = 0.1$.\n  - For $t = 0$, define $u_t = 1.0$.\n  - For $t = 0, 1$, define $r_t = 1.0$, and $w_0 = 0.5$, $w_1 = 2.0$.\n  - Use $\\varepsilon = 10^{-6}$, $\\tau_{\\mathrm{abs}} = 10^{-9}$, $\\tau_{\\mathrm{rel}} = 10^{-6}$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each test case, record the finite difference gradient, the adjoint gradient, and the boolean result of the accuracy check. The final output must be of the form\n$$\n[\\;g_{\\mathrm{fd},A},\\;g_{\\mathrm{adj},A},\\;\\text{ok}_A,\\;g_{\\mathrm{fd},B},\\;g_{\\mathrm{adj},B},\\;\\text{ok}_B,\\;g_{\\mathrm{fd},C},\\;g_{\\mathrm{adj},C},\\;\\text{ok}_C\\;],\n$$\nwhere each $g_{\\mathrm{fd},\\cdot}$ and $g_{\\mathrm{adj},\\cdot}$ is a float and each $\\text{ok}_{\\cdot}$ is a boolean.",
            "solution": "The posed problem is valid. It is a well-defined task in computational science, grounded in the principles of optimal control and sensitivity analysis for discrete-time systems. All necessary data and conditions are provided, and the problem is scientifically and mathematically sound.\n\nThe core of the problem is to compute the gradient of a scalar objective function, $J(\\theta)$, with respect to a model parameter, $\\theta$, for a system governed by discrete-time dynamics. This is a common requirement in parameter estimation, calibration, and optimization of dynamic models. The discrete adjoint method is an efficient and elegant technique for this purpose, particularly for systems with many time steps, as its computational cost is largely independent of the number of parameters.\n\nThe energy storage model is described by the state-transition equation:\n$$x_{t+1} = x_t + \\Delta t \\, \\big(-\\theta \\, x_t + u_t\\big) = (1 - \\Delta t \\, \\theta) x_t + \\Delta t \\, u_t$$\nwhere $x_t$ is the state of charge at time step $t$, $u_t$ is the net power input, $\\theta$ is the self-discharge rate parameter, and $\\Delta t$ is the duration of the time step. The simulation runs from $t=0$ to $t=T$, starting from a given initial state $x_0$.\n\nThe objective function to be minimized is a quadratic cost function that penalizes the deviation of the state trajectory $\\{x_t\\}$ from a reference trajectory $\\{r_t\\}$ and includes a regularization term on the parameter $\\theta$:\n$$J(\\theta) = \\frac{1}{2} \\sum_{t=0}^{T} w_t \\, \\big(x_t - r_t\\big)^2 \\, \\Delta t \\;+\\; \\frac{1}{2} \\, \\alpha \\, \\theta^2$$\nHere, $w_t$ are non-negative weights, and $\\alpha$ is the regularization coefficient.\n\nTo compute the gradient $\\frac{dJ}{d\\theta}$ using the discrete adjoint method, we introduce a Lagrangian, $\\mathcal{L}$, which augments the objective function with the system dynamics, weighted by a sequence of Lagrange multipliers, or adjoint variables, $\\{\\lambda_t\\}$:\n$$ \\mathcal{L} = J(\\theta) + \\sum_{t=0}^{T-1} \\lambda_{t+1} \\left[ (1 - \\Delta t \\, \\theta) x_t + \\Delta t \\, u_t - x_{t+1} \\right] $$\nBy construction, for any trajectory that satisfies the system dynamics, $\\mathcal{L} = J(\\theta)$. Therefore, the total derivative of $J$ with respect to $\\theta$ is equal to the total derivative of $\\mathcal{L}$. Applying the chain rule, we have:\n$$ \\frac{dJ}{d\\theta} = \\frac{d\\mathcal{L}}{d\\theta} = \\frac{\\partial \\mathcal{L}}{\\partial \\theta} + \\sum_{t=1}^{T} \\frac{\\partial \\mathcal{L}}{\\partial x_t} \\frac{dx_t}{d\\theta} $$\nThe initial state $x_0$ is given and does not depend on $\\theta$, so $\\frac{dx_0}{d\\theta} = 0$. The terms $\\frac{\\partial\\mathcal{L}}{\\partial\\lambda_t}$ are zero by definition of the state trajectory.\n\nThe essence of the adjoint method is to select the multiplier sequence $\\{\\lambda_t\\}$ to eliminate the terms involving the state sensitivities $\\frac{dx_t}{d\\theta}$. This is achieved by setting the partial derivatives of the Lagrangian with respect to the states $x_t$ (for $t=1, \\dots, T$) to zero.\n$$ \\frac{\\partial \\mathcal{L}}{\\partial x_t} = 0 \\quad \\text{for } t=1, \\dots, T $$\nLet's compute these partial derivatives. For the terminal state $x_T$:\n$$ \\frac{\\partial \\mathcal{L}}{\\partial x_T} = w_T(x_T - r_T)\\Delta t - \\lambda_T = 0 \\implies \\lambda_T = w_T(x_T - r_T)\\Delta t $$\nThis provides the terminal condition for the adjoint system. For the intermediate states $x_t$ where $t \\in \\{1, \\dots, T-1\\}$:\n$$ \\frac{\\partial \\mathcal{L}}{\\partial x_t} = w_t(x_t - r_t)\\Delta t + \\lambda_{t+1}(1 - \\Delta t \\, \\theta) - \\lambda_t = 0 $$\nThis gives the backward recurrence relation for the adjoint variables:\n$$ \\lambda_t = (1 - \\Delta t \\, \\theta) \\lambda_{t+1} + w_t(x_t - r_t)\\Delta t \\quad \\text{for } t=T-1, \\dots, 1 $$\nBy satisfying these adjoint equations, the sum involving state sensitivities vanishes, and the gradient calculation simplifies to:\n$$ \\frac{dJ}{d\\theta} = \\frac{\\partial \\mathcal{L}}{\\partial \\theta} $$\nThe partial derivative of $\\mathcal{L}$ with respect to $\\theta$ (treating all $x_t$ and $\\lambda_t$ as independent variables) is:\n$$ \\frac{\\partial \\mathcal{L}}{\\partial \\theta} = \\alpha \\, \\theta + \\sum_{t=0}^{T-1} \\lambda_{t+1} (-\\Delta t \\, x_t) $$\nThus, the gradient is given by:\n$$ \\frac{dJ}{d\\theta}_{\\mathrm{adj}} = \\alpha \\, \\theta - \\Delta t \\sum_{t=0}^{T-1} x_t \\lambda_{t+1} $$\nThe overall algorithm is a three-step process:\n$1$. **Forward Pass**: Given the parameter $\\theta$, simulate the system dynamics forward in time from $t=0$ to $t=T$ to obtain the state trajectory $\\{x_0, x_1, \\dots, x_T\\}$.\n$2$. **Backward Pass**: Using the stored state trajectory, compute the adjoint variables backward in time from $t=T$ down to $t=1$ using the adjoint equations derived above.\n$3$. **Gradient Calculation**: Compute the gradient $\\frac{dJ}{d\\theta}$ by summing the products of states and adjoint variables, as in the final expression.\n\nTo verify the correctness of this derivation and its implementation, we compare the resulting gradient to a numerical approximation obtained via the central finite difference formula:\n$$ \\frac{dJ}{d\\theta}_{\\mathrm{fd}} \\approx \\frac{J(\\theta + \\varepsilon) - J(\\theta - \\varepsilon)}{2 \\varepsilon} $$\nfor a small perturbation $\\varepsilon$. The accuracy is confirmed if the absolute difference $d = |\\frac{dJ}{d\\theta}_{\\mathrm{adj}} - \\frac{dJ}{d\\theta}_{\\mathrm{fd}}|$ is within a specified tolerance, defined as $d \\le \\max(\\tau_{\\mathrm{abs}}, \\tau_{\\mathrm{rel}} \\cdot \\max(1, |\\frac{dJ}{d\\theta}_{\\mathrm{fd}}|))$, where $\\tau_{\\mathrm{abs}}$ and $\\tau_{\\mathrm{rel}}$ are absolute and relative tolerance thresholds, respectively.\n\nThe provided Python code implements this entire procedure, including the forward simulator, the cost function $J(\\theta)$, the adjoint gradient calculation, the finite difference approximation, and the final accuracy check, for each of the three specified test cases.",
            "answer": "```python\nimport numpy as np\n\ndef get_cost(theta, T, dt, x0, u, r, w, alpha):\n    \"\"\"\n    Computes the objective function J(theta) for a given set of parameters.\n    This involves a forward simulation of the state trajectory.\n    \"\"\"\n    x = np.zeros(T + 1)\n    x[0] = x0\n    \n    # Forward simulation to find the state trajectory x_t\n    for t in range(T):\n        x[t+1] = (1.0 - dt * theta) * x[t] + dt * u[t]\n        \n    # Cost calculation\n    cost_sum = np.sum(w * (x - r)**2)\n        \n    J = 0.5 * cost_sum * dt + 0.5 * alpha * theta**2\n    return J\n\ndef get_adjoint_gradient(theta, T, dt, x0, u, r, w, alpha):\n    \"\"\"\n    Computes the gradient dJ/dtheta using the discrete adjoint method.\n    \"\"\"\n    \n    # 1. Forward Pass: Simulate and store the state trajectory\n    x = np.zeros(T + 1)\n    x[0] = x0\n    for t in range(T):\n        x[t+1] = (1.0 - dt * theta) * x[t] + dt * u[t]\n\n    # 2. Backward Pass: Compute the adjoint variables\n    lamb = np.zeros(T + 1)  # Use 'lamb' to avoid conflict with Python's 'lambda'\n    lamb[T] = w[T] * (x[T] - r[T]) * dt\n    for t in range(T - 1, 0, -1):  # Iterate from t=T-1 down to 1\n        lamb[t] = (1.0 - dt * theta) * lamb[t+1] + w[t] * (x[t] - r[t]) * dt\n        \n    # 3. Gradient Calculation\n    # The gradient expression is dJ/d(theta) = alpha*theta - dt * sum_{t=0}^{T-1} x_t * lambda_{t+1}\n    grad_sum = np.dot(x[:T], lamb[1:])\n        \n    grad_J_theta = alpha * theta - dt * grad_sum\n    \n    return grad_J_theta\n\ndef solve():\n    \"\"\"\n    Main function to run the test cases and print the results.\n    \"\"\"\n    test_cases = [\n        # Test Case A: General case\n        {\n            \"name\": \"A\",\n            \"T\": 48, \"dt\": 1.0, \"x0\": 1.0, \"theta\": 0.02, \"alpha\": 0.01,\n            \"u_func\": lambda t: 0.5 * np.sin(2 * np.pi * t / 24.0) + 0.1,\n            \"r_val\": 1.0, \"w_val\": 1.0,\n            \"eps\": 1e-6, \"tau_abs\": 1e-9, \"tau_rel\": 1e-6\n        },\n        # Test Case B: Boundary case near zero self-discharge\n        {\n            \"name\": \"B\",\n            \"T\": 24, \"dt\": 1.0, \"x0\": 0.0, \"theta\": 0.0, \"alpha\": 0.0,\n            \"u_func\": lambda t: 0.1 if t < 12 else -0.1,\n            \"r_val\": 0.0, \"w_val\": 1.5,\n            \"eps\": 1e-6, \"tau_abs\": 1e-9, \"tau_rel\": 1e-6\n        },\n        # Test Case C: Single-step edge case\n        {\n            \"name\": \"C\",\n            \"T\": 1, \"dt\": 2.0, \"x0\": 0.0, \"theta\": 0.5, \"alpha\": 0.1,\n            \"u_func\": lambda t: 1.0,\n            \"r_val\": 1.0, \n            \"w_arr\": [0.5, 2.0], # Non-uniform weights\n            \"eps\": 1e-6, \"tau_abs\": 1e-9, \"tau_rel\": 1e-6\n        }\n    ]\n    \n    all_results = []\n    \n    for case in test_cases:\n        T = case[\"T\"]\n        dt = case[\"dt\"]\n        x0 = case[\"x0\"]\n        theta = case[\"theta\"]\n        alpha = case[\"alpha\"]\n        eps = case[\"eps\"]\n        tau_abs = case[\"tau_abs\"]\n        tau_rel = case[\"tau_rel\"]\n        \n        # Prepare inputs u, r, w as numpy arrays\n        u = np.array([case[\"u_func\"](t) for t in range(T)], dtype=float)\n        \n        if \"w_arr\" in case:\n            w = np.array(case[\"w_arr\"], dtype=float)\n        else:\n            w = np.full(T + 1, case[\"w_val\"], dtype=float)\n            \n        r = np.full(T + 1, case[\"r_val\"], dtype=float)\n        \n        # Calculate Adjoint Gradient\n        g_adj = get_adjoint_gradient(theta, T, dt, x0, u, r, w, alpha)\n        \n        # Calculate Finite Difference Gradient\n        J_plus = get_cost(theta + eps, T, dt, x0, u, r, w, alpha)\n        J_minus = get_cost(theta - eps, T, dt, x0, u, r, w, alpha)\n        g_fd = (J_plus - J_minus) / (2.0 * eps)\n        \n        # Accuracy Check\n        abs_diff = np.abs(g_adj - g_fd)\n        # Using a relative-error-like criterion as specified\n        tolerance = max(tau_abs, tau_rel * max(1.0, np.abs(g_fd)))\n        is_ok = abs_diff <= tolerance\n        \n        all_results.extend([g_fd, g_adj, is_ok])\n\n    # Format the final output string as specified in the problem\n    # [g_fd,A, g_adj,A, ok_A, g_fd,B, g_adj,B, ok_B, ...]\n    # str(bool) in Python gives \"True\" or \"False\", which is a standard representation.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```"
        }
    ]
}