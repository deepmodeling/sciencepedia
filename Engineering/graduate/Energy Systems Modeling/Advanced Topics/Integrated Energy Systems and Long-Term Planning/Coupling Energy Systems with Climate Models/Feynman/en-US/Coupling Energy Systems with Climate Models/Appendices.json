{
    "hands_on_practices": [
        {
            "introduction": "Before tackling complex physical interactions, any coupled modeling effort must ensure the integrity of the data exchanged between components. This exercise  provides a stark, quantitative illustration of how a simple unit conversion error between megatonnes of carbon and carbon dioxide can catastrophically corrupt a simulation's results. By calculating the discrepancy and proposing a validation checklist, you will engage with the foundational principle of mass conservation, a non-negotiable check for credible scientific modeling.",
            "id": "4081500",
            "problem": "An Integrated Assessment Model (IAM) couples an energy system module to a carbon-cycle climate module. The climate module expects inputs in megatonnes of carbon ($\\text{MtC}$), while the energy system module reports annual emissions in megatonnes of carbon dioxide ($\\text{MtCO}_2$). For the years $2020$ through $2024$, the energy system module reports annual $\\text{CO}_2$ emissions $E_t$ (in $\\text{MtCO}_2$) as follows: $E_{2020} = 2750$, $E_{2021} = 2820$, $E_{2022} = 2895$, $E_{2023} = 2970$, and $E_{2024} = 3050$. Due to a unit-handling bug, the coupler passed these numeric values directly to the climate module without unit conversion during $2022$ and $2023$; during the other years the conversion was correctly performed. Assume atomic masses $M_{\\text{C}} = 12\\,\\mathrm{g/mol}$ and $M_{\\text{O}} = 16\\,\\mathrm{g/mol}$, and that emissions consist solely of carbon dioxide ($\\text{CO}_2$). Using only first principles of mass conservation and molecular composition, compute the discrepancy in cumulative emissions over $2020$–$2024$ that the climate module effectively integrated, relative to the physically correct cumulative emissions, and express your final answer in gigatonnes of carbon dioxide ($\\text{GtCO}_2$). Round your answer to four significant figures.\n\nSeparately, propose a concise validation checklist of coupling diagnostics grounded in mass conservation and unit consistency that would have caught this inconsistency prior to running the simulation. Your checklist should be justified by first principles rather than ad hoc heuristics.",
            "solution": "The problem statement is evaluated as scientifically grounded, well-posed, objective, and internally consistent. It describes a realistic scenario in integrated assessment modeling, involving a unit conversion error, and requests a calculation based on first principles of chemistry and a procedural recommendation based on software validation principles. All necessary data are provided. The problem is valid.\n\nThe solution is articulated in two parts, as requested.\n\nPart 1: Calculation of the Emissions Discrepancy\n\nThe core of the problem lies in the conversion between the mass of carbon dioxide ($\\text{CO}_2$) and the mass of carbon ($\\text{C}$) it contains. This relationship is determined by the ratio of their molar masses.\n\nFirst, we establish the molar masses using the provided atomic masses, $M_{\\text{C}} = 12\\,\\mathrm{g/mol}$ and $M_{\\text{O}} = 16\\,\\mathrm{g/mol}$.\nThe molar mass of carbon is $M_{\\text{C}} = 12\\,\\mathrm{g/mol}$.\nThe molar mass of carbon dioxide, a molecule with one carbon atom and two oxygen atoms, is:\n$$M_{\\text{CO}_2} = M_{\\text{C}} + 2 \\times M_{\\text{O}} = 12 + 2 \\times 16 = 12 + 32 = 44\\,\\mathrm{g/mol}$$\n\nBy the law of definite proportions, the ratio of the mass of carbon to the mass of carbon dioxide in any sample of pure $\\text{CO}_2$ is constant and equal to the ratio of their molar masses. Let this conversion factor be $k_C$:\n$$k_C = \\frac{\\text{Mass of C}}{\\text{Mass of CO}_2} = \\frac{M_{\\text{C}}}{M_{\\text{CO}_2}} = \\frac{12}{44} = \\frac{3}{11}$$\nThis is the factor used to convert a mass of $\\text{CO}_2$ into the corresponding mass of $\\text{C}$. The climate module correctly expects inputs in megatonnes of carbon ($\\text{MtC}$), which should be obtained by multiplying the energy system's output in megatonnes of carbon dioxide ($\\text{MtCO}_2$) by $k_C$.\n\nA discrepancy occurs only in the years when the unit conversion was not performed, namely $2022$ and $2023$. In these years, the numerical value of emissions in $\\text{MtCO}_2$, let's call it $E_t$, was incorrectly interpreted by the climate module as a mass in $\\text{MtC}$.\n\nFor a year $t$ with the bug, the physically correct mass of carbon that should have been integrated is $C_t^{\\text{correct}} = E_t \\times k_C$.\nThe mass of carbon that was actually integrated is $C_t^{\\text{actual}} = E_t$.\n\nThe discrepancy, or error, in integrated carbon for that year is the difference:\n$$\\Delta C_t = C_t^{\\text{actual}} - C_t^{\\text{correct}} = E_t - E_t \\times k_C = E_t \\left(1 - k_C\\right)$$\n$$\\Delta C_t = E_t \\left(1 - \\frac{12}{44}\\right) = E_t \\left(\\frac{32}{44}\\right) = E_t \\left(\\frac{8}{11}\\right)$$\nThis represents the overestimation of carbon mass in the climate model for a given year $t$ affected by the bug.\n\nThe problem states the bug occurred in $2022$ and $2023$. The emissions data for these years are $E_{2022} = 2895\\,\\text{MtCO}_2$ and $E_{2023} = 2970\\,\\text{MtCO}_2$.\nThe total discrepancy in integrated carbon, $\\Delta C_{\\text{total}}$, over the entire period is the sum of the discrepancies from the faulty years:\n$$\\Delta C_{\\text{total}} = \\Delta C_{2022} + \\Delta C_{2023} = (E_{2022} + E_{2023}) \\left(1 - \\frac{12}{44}\\right)$$\n$$\\Delta C_{\\text{total}} = (2895 + 2970) \\left(\\frac{8}{11}\\right) = 5865 \\times \\frac{8}{11} = \\frac{46920}{11}\\,\\text{MtC}$$\nThis value, $\\Delta C_{\\text{total}} \\approx 4265.45\\,\\text{MtC}$, is the total excess mass of carbon that the climate module integrated due to the bug.\n\nThe problem requires this discrepancy to be expressed in gigatonnes of carbon dioxide ($\\text{GtCO}_2$). This means we must find the mass of $\\text{CO}_2$ that would contain the excess mass of carbon, $\\Delta C_{\\text{total}}$. This requires the inverse conversion, multiplying by $\\frac{M_{\\text{CO}_2}}{M_{\\text{C}}} = \\frac{44}{12}$.\nLet $\\Delta E_{\\text{equiv}}$ be the discrepancy expressed as an equivalent mass of $\\text{CO}_2$.\n$$\\Delta E_{\\text{equiv}} = \\Delta C_{\\text{total}} \\times \\frac{M_{\\text{CO}_2}}{M_{\\text{C}}} = \\left(5865 \\times \\frac{8}{11}\\right) \\times \\frac{44}{12}$$\n$$\\Delta E_{\\text{equiv}} = 5865 \\times \\frac{8}{11} \\times \\frac{11 \\times 4}{3 \\times 4} = 5865 \\times \\frac{8}{3}$$\n$$\\Delta E_{\\text{equiv}} = 1955 \\times 8 = 15640\\,\\text{MtCO}_2$$\nThis is the magnitude of the discrepancy in megatonnes of carbon dioxide.\n\nFinally, we convert this to gigatonnes ($\\text{GtCO}_2$) and apply the required rounding. Since $1\\,\\text{Gt} = 1000\\,\\text{Mt}$:\n$$\\Delta E_{\\text{equiv}} = \\frac{15640}{1000}\\,\\text{GtCO}_2 = 15.640\\,\\text{GtCO}_2$$\nThe problem requires rounding the answer to four significant figures. The calculated value $15.640$ has five significant figures. Rounding to four significant figures gives $15.64$.\n\nPart 2: Validation Checklist for Coupling Diagnostics\n\nA robust validation checklist, grounded in the first principles of mass conservation and unit consistency, is essential for preventing such errors in coupled model systems.\n\n1.  **Interface Unit Assertion:** All data quantities exchanged between modules must be programmatically tagged with their physical units (e.g., as part of a data structure or metadata). Before any processing, the receiving module or coupling component must assert that the unit tag of the incoming data matches the expected unit. The simulation must terminate with a descriptive error if there is a mismatch.\n    *   *First Principles Justification:* This check enforces the fundamental principle of **unit consistency**. A physical quantity is meaningless without its unit. This diagnostic treats units as an integral part of the data, preventing the erroneous conflation of numerical values that represent physically distinct quantities (e.g., mass of $\\text{CO}_2$ vs. mass of $\\text{C}$).\n\n2.  **Mass-Conserving Transformation Check:** Any module or coupler that performs a unit conversion involving a conserved substance (like converting mass of $\\text{CO}_2$ to mass of $\\text{C}$) must perform an immediate, \"round-trip\" verification. After converting an input quantity $A$ to an output quantity $B$, it must apply the inverse transformation to $B$ to compute $A'$. It must then assert that $A$ and $A'$ are equal within a small numerical tolerance, i.e., $|A-A'|/|A| < \\epsilon$.\n    *   *First Principles Justification:* This check is a direct implementation of the **conservation of mass**. The conversion between a compound's mass and the mass of one of its constituent elements is a reflection of mass conservation. The round-trip calculation creates an algebraic invariant ($A = A'$) that must hold if the transformation correctly embeds this physical law. In the given problem, this check would have failed catastrophically: $E' = C \\times \\frac{44}{12} = E \\times \\frac{44}{12} \\neq E$.\n\n3.  **System-Wide Mass Balance Audit:** At regular intervals or checkpoints in a simulation, an automated audit must be performed. This audit sums the total mass of a conserved substance (carbon) emitted from the source module (energy system) and compares it to the total mass of that substance received by the sink module (climate model). The ratio of these two cumulative quantities must equal $1$.\n    *   *First Principles Justification:* This elevates the check from an individual transformation to an integrated **system-level verification of mass conservation**. It ensures that no mass is \"created\" or \"destroyed\" anywhere in the data pipeline over the course of the simulation, guarding against more subtle bugs like dropped data packets, accumulation of floating-point errors, or errors that manifest only over time.",
            "answer": "$$\n\\boxed{15.64}\n$$"
        },
        {
            "introduction": "Energy and climate models almost always operate at different spatial resolutions, creating the challenge of mapping data from one grid to another. This practice  moves beyond simple averaging to explore a more sophisticated aggregation that preserves key statistical moments of the underlying data. You will gain hands-on experience with methods that better represent sub-grid variability by constructing a transformation to match a target variance derived from the Law of Total Variance.",
            "id": "4081536",
            "problem": "You are given a conceptual coupling task between an energy systems model with $10$ load zones and a regional climate model that provides wind speed on a $0.25^{\\circ}$ latitude-longitude grid. The goal is to design a spatial aggregation mapping that produces, for each load zone, a single aggregated wind speed time series whose time-mean and time-variance over the aggregation period exactly match those implied by a specified weighting scheme applied to the underlying grid cells.\n\nUse the following fundamental statistical and physical bases:\n- The time-mean of a sum of random variables equals the sum of their time-means. If $x_g(t)$ are time series, then for weights $\\pi_g$ satisfying $\\sum_g \\pi_g = 1$, the aggregated series is $y(t) = \\sum_g \\pi_g x_g(t)$ with mean $\\mu_y = \\sum_g \\pi_g \\mu_g$, where $\\mu_g$ is the time-mean of $x_g(t)$.\n- The Law of Total Variance states that for a mixture distribution selecting cell $g$ with probability $\\pi_g$ and sampling $X$ from the time series at that cell, the variance is $\\mathrm{Var}(X) = \\sum_g \\pi_g \\left( \\sigma_g^2 + (\\mu_g - \\mu)^2 \\right)$, where $\\sigma_g^2$ is the time-variance of $x_g(t)$ and $\\mu = \\sum_g \\pi_g \\mu_g$ is the mixture mean.\n- Linear aggregation and affine transformations are applied to construct zone-level series from grid cell series.\n- Trigonometric functions must use angles in radians.\n\nGrid and mapping:\n- Define a rectangular grid with $4$ latitude rows and $5$ longitude columns, whose midpoints are at latitude values $30.125^{\\circ}$, $30.375^{\\circ}$, $30.625^{\\circ}$, $30.875^{\\circ}$ and longitude values $-99.875^{\\circ}$, $-99.625^{\\circ}$, $-99.375^{\\circ}$, $-99.125^{\\circ}$, $-98.875^{\\circ}$. These midpoints are the centers of $0.25^{\\circ} \\times 0.25^{\\circ}$ grid cells; the underlying latitude bounds span $30.0^{\\circ}$ to $31.0^{\\circ}$ and longitude bounds span $-100.0^{\\circ}$ to $-98.75^{\\circ}$.\n- Index latitude rows by $i \\in \\{0,1,2,3\\}$ and longitude columns by $j \\in \\{0,1,2,3,4\\}$. The load zones $z \\in \\{0,1,\\dots,9\\}$ include the following cell memberships (given as $(i,j)$ pairs):\n  - $z=0$: $(0,0)$, $(0,1)$\n  - $z=1$: $(0,2)$, $(0,3)$\n  - $z=2$: $(0,4)$, $(1,0)$, $(1,1)$\n  - $z=3$: $(1,2)$, $(1,3)$\n  - $z=4$: $(1,4)$, $(2,0)$\n  - $z=5$: $(2,1)$, $(2,2)$\n  - $z=6$: $(2,3)$, $(2,4)$\n  - $z=7$: $(3,0)$, $(3,1)$\n  - $z=8$: $(3,2)$, $(3,3)$\n  - $z=9$: $(3,4)$\n\nWind speed time series:\n- For each grid cell $(i,j)$, define a deterministic $T=200$-step wind speed time series $x_{i,j}(t)$ in meters per second ($\\mathrm{m/s}$) for discrete time index $t \\in \\{0,1,\\dots,199\\}$ by the physically plausible composition\n  $$x_{i,j}(t) = 6 + a_{i,j}\\,C_1(t) + 0.2\\,C_2(t) + S_{i,j}(t) + L_{i,j},$$\n  where\n  $$C_1(t) = 2\\sin\\!\\left(\\frac{2\\pi t}{T}\\right),\\quad C_2(t) = \\sin\\!\\left(\\frac{2\\pi t}{50}\\right),$$\n  $$a_{i,j} = 1 + 0.1\\sin(\\phi_{i,j}),\\quad \\phi_{i,j} = \\mathrm{lat}_{i}\\,\\pi/180 + \\mathrm{lon}_{j}\\,\\pi/180,$$\n  $$S_{i,j}(t) = 0.3\\sin\\!\\left(\\frac{2\\pi t}{T} + \\theta_{i,j}\\right),\\quad \\theta_{i,j} = 0.3(i - j),$$\n  $$L_{i,j} = 0.3(\\mathrm{lat}_i - 30.5) + 0.1(\\mathrm{lon}_j + 99.375) + 0.5\\sin\\!\\left(2\\pi\\frac{i + j}{5}\\right),$$\n  with $\\mathrm{lat}_i$ and $\\mathrm{lon}_j$ being the latitude and longitude midpoints in degrees for indices $i$ and $j$, respectively. All trigonometric arguments are in radians.\n\nWeighting schemes:\n- For each zone $z$, you will compute aggregation weights $w_{i,j}^{(k)}$ for three weighting schemes $k \\in \\{1,2,3\\}$:\n  1. Area-based weights: $$w_{i,j}^{(1)} = \\cos\\!\\left(\\mathrm{lat}_i\\,\\frac{\\pi}{180}\\right).$$\n  2. Uniform weights: $$w_{i,j}^{(2)} = 1.$$\n  3. Custom emphasis weights: $$w_{i,j}^{(3)} = 1 + 0.2\\left(\\mathrm{lon}_j - (-99.875)\\right) + 0.1\\left(\\mathrm{lat}_i - 30.125\\right).$$\n- Normalize within each zone $z$ to obtain $\\pi_{i,j}^{(k)} = w_{i,j}^{(k)} \\Big/ \\sum_{(p,q)\\in z} w_{p,q}^{(k)}$.\n\nTarget statistics per zone and scheme:\n- For each zone $z$ and scheme $k$, let $\\mu_{i,j}$ be the time-mean of $x_{i,j}(t)$ and $\\sigma^2_{i,j}$ be the time-variance of $x_{i,j}(t)$, both computed over $t=0,\\dots,199$.\n- Define the target mixture mean\n  $$\\mu_z^{*(k)} = \\sum_{(i,j)\\in z} \\pi_{i,j}^{(k)}\\,\\mu_{i,j}.$$\n- Define the target mixture variance using the Law of Total Variance\n  $$\\left(\\sigma_z^{2}\\right)^{*(k)} = \\sum_{(i,j)\\in z} \\pi_{i,j}^{(k)}\\left(\\sigma^2_{i,j} + \\left(\\mu_{i,j} - \\mu_z^{*(k)}\\right)^2\\right).$$\n\nAggregation mapping requirement:\n- Construct the aggregated zone series $y_z^{(k)}(t) = \\sum_{(i,j)\\in z} \\pi_{i,j}^{(k)} x_{i,j}(t)$.\n- Design and apply a deterministic affine transformation to $y_z^{(k)}(t)$, using only zone-local information, so that the transformed series $\\tilde{y}_z^{(k)}(t)$ satisfies\n  $$\\frac{1}{T}\\sum_{t=0}^{T-1} \\tilde{y}_z^{(k)}(t) = \\mu_z^{*(k)}\\quad\\text{and}\\quad \\frac{1}{T}\\sum_{t=0}^{T-1} \\left(\\tilde{y}_z^{(k)}(t) - \\mu_z^{*(k)}\\right)^2 = \\left(\\sigma_z^{2}\\right)^{*(k)}.$$\n- The transformation must be spatially linear in the original cell series and must not require modifying the original climate model outputs beyond linear aggregation and zone-wise affine scaling.\n\nTest suite and output specification:\n- Implement the above for all $10$ zones and all $3$ weighting schemes, producing $30$ zone-specific dimensionless scaling coefficients (one per zone per scheme) as floats.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered by schemes $k=1,2,3$ and, within each scheme, zones $z=0,1,\\dots,9$; for example, the first $10$ numbers correspond to scheme $k=1$ for zones $0$ through $9$, the next $10$ to scheme $k=2$, and the final $10$ to scheme $k=3$.\n- The scaling coefficients are unitless (dimensionless) numbers. Use radians for all trigonometric function arguments. No other units are required in the output. The internal wind speed computations must use meters per second ($\\mathrm{m/s}$), but you must not print units.",
            "solution": "The problem requires the design of a spatial aggregation mapping to couple wind speed data from a gridded climate model to a zonal energy systems model. The core task is to construct, for each of the $10$ load zones and for $3$ different weighting schemes, an aggregated wind speed time series whose time-mean and time-variance match specific target values. This is achieved by applying a zone-specific affine transformation to a linearly aggregated time series.\n\nThe solution methodology can be decomposed into the following steps:\n\n1.  **Generation of Grid-Cell Time Series**: We first synthesize the wind speed time series for each of the $4 \\times 5 = 20$ grid cells. The time series for cell $(i,j)$ is given by the formula:\n    $$x_{i,j}(t) = 6 + a_{i,j}\\,C_1(t) + 0.2\\,C_2(t) + S_{i,j}(t) + L_{i,j}$$\n    for a time horizon of $T=200$ steps. This model represents wind speed as a superposition of a constant base value ($6$), common-mode periodic fluctuations ($C_1(t)$ and $C_2(t)$), a spatially-varying periodic component ($S_{i,j}(t)$), and a spatially-varying time-invariant offset ($L_{i,j}$). The parameters $a_{i,j}$ and $L_{i,j}$ depend on the cell's latitude ($\\mathrm{lat}_i$) and longitude ($\\mathrm{lon}_j$), introducing spatial heterogeneity. All trigonometric functions are evaluated using radians.\n\n2.  **Statistical Characterization of Grid Cells**: For each grid cell $(i,j)$, we compute the time-mean $\\mu_{i,j}$ and time-variance $\\sigma^2_{i,j}$ of its time series $x_{i,j}(t)$ over the period $T$:\n    $$\\mu_{i,j} = \\frac{1}{T}\\sum_{t=0}^{T-1} x_{i,j}(t)$$\n    $$\\sigma^2_{i,j} = \\frac{1}{T}\\sum_{t=0}^{T-1} (x_{i,j}(t) - \\mu_{i,j})^2$$\n    These statistics form the basis for constructing the target properties of the aggregated zonal series.\n\n3.  **Zonal Aggregation and Weighting**: For each load zone $z$ and weighting scheme $k \\in \\{1, 2, 3\\}$, we first compute normalized weights $\\pi_{i,j}^{(k)}$ for all cells $(i,j)$ belonging to that zone. The raw weights $w_{i,j}^{(k)}$ are given by the three schemes (Area, Uniform, Custom) and are normalized within the zone:\n    $$\\pi_{i,j}^{(k)} = \\frac{w_{i,j}^{(k)}}{\\sum_{(p,q)\\in z} w_{p,q}^{(k)}}$$\n    Using these weights, we construct a linearly aggregated time series for the zone, $y_z^{(k)}(t)$:\n    $$y_z^{(k)}(t) = \\sum_{(i,j)\\in z} \\pi_{i,j}^{(k)} x_{i,j}(t)$$\n\n4.  **Definition of Target Statistics**: The problem specifies the target statistical properties for the final, transformed zonal time series, $\\tilde{y}_z^{(k)}(t)$.\n    -   The target mean, $\\mu_z^{*(k)}$, is the weighted average of the individual cell means:\n        $$\\mu_z^{*(k)} = \\sum_{(i,j)\\in z} \\pi_{i,j}^{(k)}\\,\\mu_{i,j}$$\n        By the linearity of the expectation (mean) operator, the mean of the linearly aggregated series, $\\mathrm{Mean}(y_z^{(k)}(t))$, is inherently equal to this target mean, $\\mu_z^{*(k)}$.\n    -   The target variance, $(\\sigma_z^2)^{*(k)}$, is defined using the Law of Total Variance:\n        $$(\\sigma_z^2)^{*(k)} = \\sum_{(i,j)\\in z} \\pi_{i,j}^{(k)} \\left(\\sigma^2_{i,j} + (\\mu_{i,j} - \\mu_z^{*(k)})^2\\right)$$\n        This represents the variance of a statistical mixture model, which is conceptually different from the variance of the linearly aggregated sum, $\\sigma_{y,z,k}^2 = \\mathrm{Var}(y_z^{(k)}(t))$. The latter is calculated as $\\frac{1}{T}\\sum_{t=0}^{T-1} (y_z^{(k)}(t) - \\mu_z^{*(k)})^2$.\n\n5.  **Design of the Affine Transformation**: The final step is to find an affine transformation $\\tilde{y}(t) = A y(t) + B$ that modifies the aggregated series $y_z^{(k)}(t)$ to have the target statistics. The coefficients $A$ and $B$ are determined by the following two conditions:\n    1.  $\\mathrm{Mean}(\\tilde{y}) = \\mathrm{Mean}(A y + B) = A \\mathrm{Mean}(y) + B = \\mu_z^{*(k)}$\n    2.  $\\mathrm{Var}(\\tilde{y}) = \\mathrm{Var}(A y + B) = A^2 \\mathrm{Var}(y) = (\\sigma_z^2)^{*(k)}$\n\n    As established, $\\mathrm{Mean}(y) = \\mu_z^{*(k)}$. Substituting this into the first equation yields $A \\mu_z^{*(k)} + B = \\mu_z^{*(k)}$, which implies $B = (1-A)\\mu_z^{*(k)}$.\n    From the second equation, we can solve for the scaling coefficient $A$, which is the quantity requested by the problem. Assuming $\\mathrm{Var}(y) > 0$:\n    $$A^2 = \\frac{(\\sigma_z^2)^{*(k)}}{\\mathrm{Var}(y_z^{(k)}(t))} \\implies A_z^{(k)} = \\sqrt{\\frac{(\\sigma_z^2)^{*(k)}}{\\sigma_{y,z,k}^2}}$$\n    We take the positive root for the scaling coefficient. This dimensionless coefficient $A_z^{(k)}$ is calculated for each of the $10$ zones and $3$ weighting schemes.\n\nThe algorithm proceeds by first generating all $20$ grid-cell time series and their means and variances. Then, for each of the $30$ zone-scheme combinations, it computes the normalized weights, the target mean and variance, the variance of the linearly aggregated series, and finally the required scaling coefficient $A_z^{(k)}$. These $30$ coefficients constitute the final result.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the spatial aggregation mapping problem by calculating dimensionless\n    scaling coefficients for 10 load zones under 3 weighting schemes.\n    \"\"\"\n\n    # --- Problem Definition ---\n\n    # Grid and time parameters\n    T = 200\n    NUM_LAT_ROWS = 4\n    NUM_LON_COLS = 5\n    NUM_ZONES = 10\n\n    # Grid midpoints\n    lats = 30.125 + 0.25 * np.arange(NUM_LAT_ROWS)\n    lons = -99.875 + 0.25 * np.arange(NUM_LON_COLS)\n    \n    # Zone memberships: list of (i, j) tuples for each zone\n    zone_memberships = [\n        [(0, 0), (0, 1)],  # z=0\n        [(0, 2), (0, 3)],  # z=1\n        [(0, 4), (1, 0), (1, 1)],  # z=2\n        [(1, 2), (1, 3)],  # z=3\n        [(1, 4), (2, 0)],  # z=4\n        [(2, 1), (2, 2)],  # z=5\n        [(2, 3), (2, 4)],  # z=6\n        [(3, 0), (3, 1)],  # z=7\n        [(3, 2), (3, 3)],  # z=8\n        [(3, 4)]           # z=9\n    ]\n\n    # Time vector\n    t = np.arange(T)\n\n    # --- Step 1: Generate Grid Cell Time Series and Statistics ---\n    \n    x_series = np.zeros((NUM_LAT_ROWS, NUM_LON_COLS, T))\n    mu_cells = np.zeros((NUM_LAT_ROWS, NUM_LON_COLS))\n    var_cells = np.zeros((NUM_LAT_ROWS, NUM_LON_COLS))\n\n    # Common time-varying components\n    C1_t = 2 * np.sin(2 * np.pi * t / T)\n    C2_t = np.sin(2 * np.pi * t / 50)\n\n    for i in range(NUM_LAT_ROWS):\n        for j in range(NUM_LON_COLS):\n            lat_deg, lon_deg = lats[i], lons[j]\n            \n            # Spatially varying parameters\n            phi_ij = np.deg2rad(lat_deg) + np.deg2rad(lon_deg)\n            a_ij = 1 + 0.1 * np.sin(phi_ij)\n            \n            theta_ij = 0.3 * (i - j)\n            S_ij_t = 0.3 * np.sin(2 * np.pi * t / T + theta_ij)\n            \n            L_ij = (0.3 * (lat_deg - 30.5) +\n                    0.1 * (lon_deg + 99.375) +\n                    0.5 * np.sin(2 * np.pi * (i + j) / 5))\n            \n            # Construct time series for cell (i, j)\n            x_series[i, j, :] = 6 + a_ij * C1_t + 0.2 * C2_t + S_ij_t + L_ij\n            \n            # Pre-compute cell mean and variance\n            mu_cells[i, j] = np.mean(x_series[i, j, :])\n            # Use population variance (ddof=0 is default)\n            var_cells[i, j] = np.var(x_series[i, j, :])\n\n    # --- Step 2  3: Iterate Schemes and Zones, Calculate Coefficients ---\n\n    results = []\n    \n    for k in range(1, 4):  # Iterate through schemes k=1, 2, 3\n        for z in range(NUM_ZONES): # Iterate through zones z=0, ..., 9\n            cell_indices = zone_memberships[z]\n            num_cells_in_zone = len(cell_indices)\n            \n            # Step 3a: Calculate weights for the current zone and scheme\n            raw_weights = np.zeros(num_cells_in_zone)\n            for idx, (i, j) in enumerate(cell_indices):\n                lat_deg, lon_deg = lats[i], lons[j]\n                if k == 1:  # Area-based weights\n                    raw_weights[idx] = np.cos(np.deg2rad(lat_deg))\n                elif k == 2:  # Uniform weights\n                    raw_weights[idx] = 1.0\n                else:  # k == 3, Custom emphasis weights\n                    raw_weights[idx] = 1 + 0.2 * (lon_deg - lons[0]) + 0.1 * (lat_deg - lats[0])\n            \n            pi_weights = raw_weights / np.sum(raw_weights)\n\n            # Step 3b: Calculate target statistics\n            zone_mu_cells = np.array([mu_cells[i, j] for i, j in cell_indices])\n            zone_var_cells = np.array([var_cells[i, j] for i, j in cell_indices])\n            \n            mu_star_target = np.sum(pi_weights * zone_mu_cells)\n            \n            var_star_target = np.sum(pi_weights * (zone_var_cells + (zone_mu_cells - mu_star_target)**2))\n\n            # Step 3c: Calculate aggregated series and its variance\n            y_series_z_k = np.zeros(T)\n            for idx, (i, j) in enumerate(cell_indices):\n                y_series_z_k += pi_weights[idx] * x_series[i, j, :]\n            \n            var_y = np.var(y_series_z_k)\n\n            # Step 3d: Compute the scaling coefficient\n            if var_y > 0:\n                A = np.sqrt(var_star_target / var_y)\n            elif var_star_target == 0: # If both are 0, variance is already matched\n                A = 1.0\n            else: # var_y is 0 but target is not, indicates an issue\n                A = np.inf # Should not happen in this problem\n                \n            results.append(A)\n\n    # --- Final Output ---\n    # Format the results as a comma-separated list in brackets\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "A fundamental decision in coupled modeling is determining the frequency of data exchange between models that run at different native time steps. This problem  transforms this practical question into a formal optimization problem, balancing computational cost against scientific accuracy. By using a standard error bound for linear interpolation, you will derive a principled loose coupling schedule that minimizes communication overhead while guaranteeing that the error from time-interpolation remains below a specified tolerance.",
            "id": "4081556",
            "problem": "You are given two time-stepped simulators: an Energy Model (EM) with native time step $\\Delta t_{e} = 1 \\ \\text{hour}$ and a Climate Model (CM) with native time step $\\Delta t_{c} = 6 \\ \\text{hours}$. The goal is to design a Loose Coupling Schedule (LCS) for a one-year run that minimizes communication overhead between the EM and CM while ensuring that the interpolation error of climate inputs used by the EM remains bounded below a specified tolerance. The one-year horizon is $T = 8760 \\ \\text{hours}$.\n\nAssume each relevant climate input is a twice continuously differentiable function of time $x(t)$ with a known global bound on the magnitude of its second derivative, i.e., $\\lvert x''(t) \\rvert \\le M$ for all $t$ in $[0, T]$, where $M$ is provided. The EM uses piecewise linear interpolation of CM values between communication events. The LCS must consist of communication times that are integer multiples of the CM step, i.e., the interval $h$ between communications must satisfy $h = N \\, \\Delta t_{c}$ for some integer $N \\ge 1$. The schedule must include $t=0$ and $t=T$.\n\nFrom first principles of numerical approximation and smoothness bounds, you must determine an $N$ that yields the largest feasible communication interval $h$ (thus minimizing the number of communications) such that the maximum interpolation error of the EM’s climate inputs is bounded by the provided tolerance $\\varepsilon$. If no integer $N \\ge 1$ exists that satisfies the error bound given the CM’s native step constraint $h \\ge \\Delta t_{c}$, the schedule is infeasible.\n\nFor multiple climate inputs indexed by $i$ with bounds $\\lvert x_i''(t) \\rvert \\le M_i$ and tolerances $\\varepsilon_i$, the error constraint must hold simultaneously for all inputs over each interval; therefore the selected $h$ must satisfy the most restrictive bound across all inputs.\n\nDefine the communication overhead as the total number of communication events over $[0, T]$, counting both endpoints. For interval length $h$, the number of communication events is $\\left\\lceil \\frac{T}{h} \\right\\rceil + 1$. Your program must compute, for each parameter set in the test suite:\n- The largest feasible integer $N$ (with $h = N \\, \\Delta t_{c}$) that satisfies the error bound.\n- The corresponding total number of communication events over $[0, T]$.\nIf the problem is infeasible, output $N = -1$ and the total number of communication events as $-1$.\n\nUnits: Report time in $\\text{hours}$. The outputs are integers. Angles are not involved. Percentages are not involved.\n\nTest suite:\n- Case (i) Single-input, typical: $M = 0.0004 \\ \\text{per} \\ \\text{hour}^{2}$, $\\varepsilon = 0.02$.\n- Case (ii) Single-input, boundary feasibility: $M = 0.01 \\ \\text{per} \\ \\text{hour}^{2}$, $\\varepsilon = 0.045$.\n- Case (iii) Single-input, infeasible: $M = 0.01 \\ \\text{per} \\ \\text{hour}^{2}$, $\\varepsilon = 0.044$.\n- Case (iv) Two inputs, differing smoothness and tolerances: $M_1 = 0.0004 \\ \\text{per} \\ \\text{hour}^{2}$, $\\varepsilon_1 = 0.02$; $M_2 = 0.002 \\ \\text{per} \\ \\text{hour}^{2}$, $\\varepsilon_2 = 0.04$.\n\nYour program should produce a single line of output containing the results for the test suite as a comma-separated list enclosed in square brackets in the following format: $[[N_1,C_1],[N_2,C_2],[N_3,C_3],[N_4,C_4]]$, where $N_k$ is the selected integer and $C_k$ is the total number of communication events for case $k$. If infeasible, the pair must be $[-1,-1]$.",
            "solution": "We formalize the coupling between the Energy Model (EM) and Climate Model (CM). The EM advances with time step $\\Delta t_{e} = 1 \\ \\text{hour}$ and requires climate inputs at each EM step. The CM provides climate outputs at its native times separated by $\\Delta t_{c} = 6 \\ \\text{hours}$. Under a Loose Coupling Schedule (LCS), we select communication times separated by $h = N \\, \\Delta t_{c}$, where $N \\in \\mathbb{Z}$ and $N \\ge 1$, so that the EM can linearly interpolate climate inputs between communications. The one-year horizon is $T = 8760 \\ \\text{hours}$, and the communication times must include $t=0$ and $t=T$.\n\nTo ensure scientific realism, we model each climate input $x(t)$ as twice continuously differentiable with $\\lvert x''(t) \\rvert \\le M$ over $[0, T]$. The EM employs piecewise linear interpolation between communication points. We derive an upper bound on the interpolation error using Taylor’s theorem with the Lagrange form of the remainder and the standard error analysis for linear interpolation.\n\nLet $x(t)$ be the climate input and consider a single coupling interval $[a,b]$ of length $h = b-a$. Linear interpolation of $x(t)$ between $a$ and $b$ is the unique affine function $\\ell(t)$ such that $\\ell(a) = x(a)$ and $\\ell(b) = x(b)$. Define the interpolation error $e(t) = x(t) - \\ell(t)$. From classical numerical analysis, $e(t)$ can be related to the second derivative via the Peano kernel or by applying Taylor’s theorem about any point and subtracting the interpolant. One obtains the identity\n$$\ne(t) = \\frac{x''(\\xi_t)}{2} \\, (t-a)(t-b),\n$$\nfor some $\\xi_t \\in (a,b)$ depending on $t$. Taking absolute values and using the global bound $\\lvert x''(t) \\rvert \\le M$ yields\n$$\n\\lvert e(t) \\rvert \\le \\frac{M}{2} \\, \\lvert (t-a)(t-b) \\rvert.\n$$\nThe function $\\lvert (t-a)(t-b) \\rvert$ attains its maximum at the midpoint $t = \\frac{a+b}{2}$ with value $\\left(\\frac{h}{2}\\right)\\left(\\frac{h}{2}\\right) = \\frac{h^2}{4}$. Therefore, the maximum interpolation error over the interval satisfies\n$$\n\\max_{t \\in [a,b]} \\lvert e(t) \\rvert \\le \\frac{M h^2}{8}.\n$$\nThis inequality is a well-tested result from numerical approximation theory and provides a conservative bound on the piecewise linear interpolation error in terms of the interval length $h$ and the second derivative bound $M$.\n\nTo enforce a tolerance $\\varepsilon$ on the interpolation error, we require\n$$\n\\frac{M h^2}{8} \\le \\varepsilon.\n$$\nSolving for $h$ gives the largest admissible interval length\n$$\nh_{\\max} = \\sqrt{\\frac{8 \\varepsilon}{M}}.\n$$\nTo minimize communication overhead, we choose the largest feasible $h$ subject to the CM’s native step constraint that $h$ must be an integer multiple of $\\Delta t_{c}$, i.e., $h = N \\, \\Delta t_{c}$ with $N \\in \\mathbb{Z}$ and $N \\ge 1$. Hence,\n$$\nN_{\\max} = \\left\\lfloor \\frac{h_{\\max}}{\\Delta t_{c}} \\right\\rfloor.\n$$\nFeasibility requires $N_{\\max} \\ge 1$, which is equivalent to $h_{\\max} \\ge \\Delta t_{c}$. If $N_{\\max}  1$, then no integer multiple of $\\Delta t_{c}$ can satisfy the error bound, and the problem is infeasible with respect to the given CM step.\n\nFor multiple climate inputs indexed by $i$ with bounds $M_i$ and tolerances $\\varepsilon_i$, the error constraint must hold simultaneously. The admissible interval is therefore restricted by the most stringent input:\n$$\nh_{\\max} = \\min_{i} \\sqrt{\\frac{8 \\varepsilon_i}{M_i}},\n$$\nand the same integer multiple constraint applies:\n$$\nN_{\\max} = \\left\\lfloor \\frac{h_{\\max}}{\\Delta t_{c}} \\right\\rfloor, \\quad \\text{feasible if } N_{\\max} \\ge 1.\n$$\n\nOnce $N_{\\max}$ is determined, the communication interval is $h = N_{\\max} \\, \\Delta t_{c}$. The total number of communication events over $[0, T]$, including both endpoints, is\n$$\nC = \\left\\lceil \\frac{T}{h} \\right\\rceil + 1.\n$$\nThe inclusion of $t=T$ ensures that the final (possibly shorter) interval $[k h, T]$ has both endpoints available to the EM for interpolation. If $N_{\\max}  1$, we declare infeasibility and set $(N, C) = (-1, -1)$.\n\nWe now evaluate the test suite:\n\n- Case (i): $M = 0.0004$, $\\varepsilon = 0.02$, $\\Delta t_{c} = 6$, $T = 8760$.\n  Compute $h_{\\max} = \\sqrt{\\frac{8 \\cdot 0.02}{0.0004}} = \\sqrt{400} = 20$. Then $N_{\\max} = \\left\\lfloor \\frac{20}{6} \\right\\rfloor = 3$. Thus $h = 18$, $C = \\left\\lceil \\frac{8760}{18} \\right\\rceil + 1 = 487 + 1 = 488$.\n\n- Case (ii): $M = 0.01$, $\\varepsilon = 0.045$, $\\Delta t_{c} = 6$, $T = 8760$.\n  Compute $h_{\\max} = \\sqrt{\\frac{8 \\cdot 0.045}{0.01}} = \\sqrt{36} = 6$. Then $N_{\\max} = \\left\\lfloor \\frac{6}{6} \\right\\rfloor = 1$. Thus $h = 6$, $C = \\left\\lceil \\frac{8760}{6} \\right\\rceil + 1 = 1460 + 1 = 1461$.\n\n- Case (iii): $M = 0.01$, $\\varepsilon = 0.044$, $\\Delta t_{c} = 6$, $T = 8760$.\n  Compute $h_{\\max} = \\sqrt{\\frac{8 \\cdot 0.044}{0.01}} = \\sqrt{35.2} \\approx 5.93$. Then $N_{\\max} = \\left\\lfloor \\frac{5.93}{6} \\right\\rfloor = 0$, which is infeasible. Output $[-1, -1]$.\n\n- Case (iv): Two inputs with $(M_1, \\varepsilon_1) = (0.0004, 0.02)$ and $(M_2, \\varepsilon_2) = (0.002, 0.04)$; $\\Delta t_{c} = 6$, $T = 8760$.\n  Compute $h_{\\max,1} = \\sqrt{\\frac{8 \\cdot 0.02}{0.0004}} = \\sqrt{400} = 20$, and $h_{\\max,2} = \\sqrt{\\frac{8 \\cdot 0.04}{0.002}} = \\sqrt{160} \\approx 12.649$. Thus $h_{\\max} = \\min(20, 12.649) = 12.649$, so $N_{\\max} = \\left\\lfloor \\frac{12.649}{6} \\right\\rfloor = 2$. Therefore $h = 12$, $C = \\left\\lceil \\frac{8760}{12} \\right\\rceil + 1 = 730 + 1 = 731$.\n\nThe algorithm is:\n1) For each case, form the set of $(M_i, \\varepsilon_i)$ pairs (singletons for single-input cases).\n2) Compute $h_{\\max,i} = \\sqrt{\\frac{8 \\varepsilon_i}{M_i}}$ and take $h_{\\max} = \\min_i h_{\\max,i}$.\n3) Compute $N_{\\max} = \\left\\lfloor \\frac{h_{\\max}}{\\Delta t_{c}} \\right\\rfloor$. If $N_{\\max}  1$, return $[-1, -1]$.\n4) Else set $h = N_{\\max} \\, \\Delta t_{c}$ and $C = \\left\\lceil \\frac{T}{h} \\right\\rceil + 1$.\n5) Output the list of $[N, C]$ for all cases as a single bracketed, comma-separated line.\n\nThis procedure minimizes communication overhead by selecting the largest feasible integer multiple of the climate step while respecting the interpolation error bound derived from smoothness constraints via Taylor’s theorem.",
            "answer": "```python\n# Python 3.12\n# Libraries: numpy 1.23.5, scipy 1.11.4 (not used)\nimport numpy as np\n\ndef largest_feasible_N_and_count(M_list, eps_list, dt_c_hours, T_hours):\n    \"\"\"\n    Compute the largest feasible integer N such that h = N * dt_c_hours\n    satisfies max interpolation error = epsilon for all inputs,\n    and the corresponding total number of communication events over [0, T].\n\n    Parameters:\n        M_list: list or scalar; bound(s) on |x''(t)| in units per hour^2\n        eps_list: list or scalar; tolerance(s) in same units as x\n        dt_c_hours: float; climate model native step in hours\n        T_hours: float; total horizon in hours (one year)\n\n    Returns:\n        (N_max, C) where:\n            N_max: int; largest feasible integer multiple\n            C: int; total communications including endpoints\n        If infeasible, returns (-1, -1).\n    \"\"\"\n    # Ensure arrays for uniform handling\n    Ms = np.atleast_1d(M_list).astype(float)\n    eps = np.atleast_1d(eps_list).astype(float)\n\n    # Validate lengths\n    if Ms.shape != eps.shape:\n        raise ValueError(\"M_list and eps_list must have the same shape\")\n\n    # Compute h_max for each input: sqrt(8 * eps_i / M_i)\n    # Guard against division by zero or negative values (not expected in test suite)\n    h_max_list = np.sqrt(8.0 * eps / Ms)\n\n    # Most restrictive bound across inputs\n    h_max = float(np.min(h_max_list))\n\n    # Integer multiple constraint\n    N_max = int(np.floor(h_max / dt_c_hours))\n\n    if N_max  1:\n        return (-1, -1)\n\n    h = N_max * dt_c_hours\n    C = int(np.ceil(T_hours / h)) + 1\n    return (N_max, C)\n\ndef format_nested_list(obj):\n    \"\"\"\n    Format a Python nested list of ints into a compact string without spaces,\n    e.g., [[3,488],[1,1461],[-1,-1],[2,731]]\n    \"\"\"\n    if isinstance(obj, list):\n        return \"[\" + \",\".join(format_nested_list(x) for x in obj) + \"]\"\n    elif isinstance(obj, (int, np.integer)):\n        return str(int(obj))\n    else:\n        # Should not occur for this problem; handle gracefully\n        return str(obj)\n\ndef solve():\n    # Constants per problem statement\n    dt_e_hours = 1.0  # energy model step; not directly used in computation\n    dt_c_hours = 6.0  # climate model step\n    T_hours = 8760.0  # one-year horizon\n\n    # Test suite: four cases\n    test_cases = [\n        # Case (i) Single-input, typical\n        {\"M\": 0.0004, \"eps\": 0.02},\n        # Case (ii) Single-input, boundary feasibility\n        {\"M\": 0.01, \"eps\": 0.045},\n        # Case (iii) Single-input, infeasible\n        {\"M\": 0.01, \"eps\": 0.044},\n        # Case (iv) Two inputs, differing smoothness and tolerances\n        {\"M\": [0.0004, 0.002], \"eps\": [0.02, 0.04]},\n    ]\n\n    results = []\n    for case in test_cases:\n        M = case[\"M\"]\n        eps = case[\"eps\"]\n        N_max, C = largest_feasible_N_and_count(M, eps, dt_c_hours, T_hours)\n        results.append([N_max, C])\n\n    print(format_nested_list(results))\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}