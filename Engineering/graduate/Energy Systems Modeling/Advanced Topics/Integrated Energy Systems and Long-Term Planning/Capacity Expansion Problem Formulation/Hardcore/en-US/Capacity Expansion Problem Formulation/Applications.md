## Applications and Interdisciplinary Connections

The preceding chapters have established the fundamental principles and mathematical formulation of capacity expansion problems. We have seen how to construct an optimization model comprising an objective function, decision variables, and a set of constraints that define a feasible system. This chapter bridges the gap between these foundational concepts and their application in sophisticated, real-world energy [system analysis](@entry_id:263805). Our focus shifts from *how* to formulate a basic model to *what* can be achieved with it. We will explore how the core framework is adapted to model complex technologies, incorporate diverse policy objectives, and handle the inherent uncertainties of the future. By examining these applications, we not only demonstrate the utility of capacity expansion modeling but also highlight its deep connections to electrical engineering, economics, public policy, and computational science.

### Advanced Technology Representation

While the basic capacity expansion formulation can be applied to simple thermal generators, its true power lies in its flexibility to represent the unique physical and economic characteristics of a wide array of energy technologies. This requires tailoring the standard set of constraints to capture specific operational realities.

A paradigmatic example is energy storage. Unlike a generator, a storage asset is characterized by two distinct capacities: its power capacity ($P$, in MW), which limits the rate of charge and discharge, and its energy capacity ($E$, in MWh), which limits the total amount of energy it can hold. A model must include separate investment variables for both, as they are often decoupled in modern technologies like batteries. The operational constraints must then track the state of charge ($s_t$) over time through an energy balance equation. This equation accounts for the initial state, energy added during charging (adjusted for charging efficiency $\eta_c$), and energy withdrawn during discharging (adjusted for discharging efficiency $\eta_d$). A typical formulation for the state of charge at the end of period $t+1$ is $s_{t+1} = (1 - \sigma) s_t + \eta_c p^c_t \Delta t - \frac{1}{\eta_d} p^d_t \Delta t$, where $p^c_t$ and $p^d_t$ are the charging and discharging powers and $\sigma$ is the self-discharge rate. This ensures that the model respects the fundamental distinction between a flux limit (power) and a stock limit (energy)  .

Hydropower with reservoirs presents a different modeling challenge, shifting the focus from energy balance to mass balance. Here, the state variable is the volume of water ($x_s$) in the reservoir at the beginning of a season $s$. The intertemporal balance equation tracks this volume as a function of the previous season's storage, natural inflows ($a_s$), and outflows from turbine discharge ($u_s$) and spillage ($v_s$): $x_{s+1} = x_s + a_s - u_s - v_s$. Electricity generation ($e_s$) is then proportional to the turbinated water volume, $e_s = \xi u_s$, where $\xi$ is a conversion factor representing the [hydraulic head](@entry_id:750444) and [turbine efficiency](@entry_id:1133485). The total energy generated in a season is limited by the installed turbine power capacity ($y^T$) and the season's duration ($\tau_s$), yielding the constraint $e_s \le y^T \tau_s$. This formulation demonstrates how the capacity expansion framework can be adapted to resource-flow constraints, which are common in many renewable technologies .

The framework can also be extended to multi-product technologies like Combined Heat and Power (CHP) plants, which produce both electricity ($p$) and useful heat ($h$). The operational flexibility of such plants is described by a [feasible operating region](@entry_id:1124878) in the power-heat plane. This region is often non-convex, but for linear programming models, it is typically approximated by its convex hull. The vertices of this polygon correspond to extreme operating modes, such as maximum electricity production in condensing mode (zero heat output) and maximum co-generation in backpressure mode. The line segment connecting these vertices defines the trade-off between heat and power production. This trade-off can be represented by a single [linear inequality](@entry_id:174297) of the form $p \le \overline{p}^c K - \alpha h$, where $K$ is the plant's capacity, $\overline{p}^c$ is the maximum electricity output per unit of capacity with zero heat production, and $\alpha$ is the power loss factor, quantifying the reduction in electricity output for each unit of heat extracted. This technique allows the model to co-optimize the supply of multiple energy vectors, a crucial aspect of integrated energy [systems analysis](@entry_id:275423) .

### Incorporating Policy, Reliability, and Economic Signals

Capacity expansion models are not merely technical tools; they are essential instruments for policy analysis. They allow decision-makers to explore the system-wide consequences of different policy choices by translating those choices into mathematical constraints or objective function terms.

A primary application is the analysis of [climate policy](@entry_id:1122477). One common instrument, a carbon price ($p_c$), can be directly incorporated into the objective function. For each fossil-fueled generator, the carbon price is multiplied by its emissions intensity ($e_g$, in tCO2/MWh) to create a carbon cost component, which is added to its variable operating cost: $v_g = v_{g0} + p_c e_g$. This internalizes the social cost of emissions, potentially altering the economic merit order of dispatch and influencing long-term investment decisions away from carbon-intensive technologies. For example, a sufficiently high carbon price can make the levelized cost of a new gas turbine greater than that of a new wind farm, even if the gas turbine has lower capital costs, thereby favoring investment in wind .

An alternative to pricing emissions is to impose a quantitative limit. A cumulative emissions budget, or cap ($E^{cap}$), is a common long-term [climate policy](@entry_id:1122477) goal. This can be directly translated into a single, aggregate constraint in the model. The total emissions are calculated by summing the product of generation ($q_{k,t}$) and technology-specific emission factors ($e_k$) over all technologies $k$ and time periods $t$. The constraint is then simply $\sum_{t \in T} \sum_{k \in K} e_k q_{k,t} \le E^{cap}$. This formulation ensures that the entire development pathway of the energy system over the planning horizon complies with the overarching carbon budget .

Beyond [environmental policy](@entry_id:200785), a core function of the power system is to be reliable. Translating the inherently probabilistic nature of reliability into a deterministic optimization framework is a significant interdisciplinary challenge. Metrics like Loss of Load Expectation (LOLE, the expected hours per year of a supply shortfall) and Expected Unserved Energy (EUE, the expected MWh of unserved demand per year) are formally defined using probability and expectation. In [capacity expansion models](@entry_id:1122042), these are often approximated with simpler, deterministic constraints. The most common is the Planning Reserve Margin (PRM), which requires the total available firm capacity to exceed the peak load by a certain percentage. A more sophisticated approach uses the [load duration curve](@entry_id:1127380) to impose a constraint that available capacity must exceed a high quantile of the load distribution. For EUE, one can introduce [slack variables](@entry_id:268374) representing unserved energy in different time blocks and constrain their sum to be below a target, $\sum_b h_b s_b \le \text{EUE}^{\max}$ .

### Spatial and Network Dimensions

While single-node models are useful for strategic insights, real-world energy systems are geographically distributed networks. Capacity expansion models can be extended to incorporate these spatial dimensions, enabling decisions about not just *what* to build, but *where* to build it.

Siting decisions involve a trade-off between resource quality and network integration costs. For example, the best location for a wind farm might have a high capacity factor but be far from load centers, incurring significant costs for new transmission lines and greater energy losses during transport. A capacity expansion model can solve this trade-off by evaluating a [discrete set](@entry_id:146023) of potential sites. For each site, it calculates the net value, considering revenues from delivered energy (which accounts for transmission losses) and subtracting all costs, including fixed costs, variable costs, and distance-dependent transmission investment costs. By comparing the net value across all candidate locations, the model can identify the optimal site that balances resource richness against the tyranny of distance .

To model the entire network, rather than just point-to-point connections, planners employ power flow formulations. For large-scale models, the full alternating current (AC) power flow equations are computationally prohibitive. Instead, the linearized Direct Current (DC) power flow approximation is standard. This approach models power flow on a line $\ell$ connecting buses $i$ and $j$ as being proportional to the difference in their voltage angles ($\theta_i$, $\theta_j$) and the line's susceptance ($b_\ell$): $f_\ell = b_\ell (\theta_i - \theta_j)$. At each bus, Kirchhoff's Current Law is enforced, stating that net power injection must equal the sum of outgoing power flows. Crucially, each flow $f_\ell$ is bounded by the line's thermal capacity, which itself can be a decision variable for expansion, $ |f_\ell| \le F_\ell + \kappa_\ell $, where $F_\ell$ is existing capacity and $\kappa_\ell$ is new investment. Including these DC power flow constraints transforms the model into a network-constrained [capacity expansion problem](@entry_id:1122044), capable of identifying network congestion and endogenously deciding on optimal transmission and generation investments simultaneously .

### Frontiers in Capacity Expansion Modeling

The field continues to evolve, with researchers developing more sophisticated formulations to capture complex, long-term dynamics and uncertainties. These advanced methods represent the frontier of energy [systems analysis](@entry_id:275423) and connect the field to advanced topics in operations research, economics, and decision science.

#### Planning Under Uncertainty: Stochastic Programming

Deterministic models assume perfect foresight of future conditions, such as fuel prices, technology costs, and the availability of renewable resources. To address this limitation, planners use **[two-stage stochastic programming](@entry_id:635828)**. In this framework, decisions are split into two stages. First-stage decisions, such as capacity investments ($y_g$), are made "here and now," before uncertainty is resolved. Second-stage or "recourse" decisions, such as the dispatch of generators ($q_{gst}$), are made "wait and see," after a particular scenario $s$ from a set of possible futures $S$ has been realized. The objective is to minimize the sum of the certain first-stage investment costs and the *expected* value of the second-stage operational costs over all scenarios.

A key concept in this formulation is **non-anticipativity**: the first-stage investment decision must be the same for all scenarios, because it is made before the future is known. This is enforced by defining a single investment variable $y_g$ that applies across all scenarios. The operational constraints, however, are scenario-specific, allowing the model to find a unique optimal dispatch for each possible realization of renewable availability or demand . This framework is particularly powerful for valuing flexibility; for instance, it can determine the optimal mix of wind, solar, and storage by explicitly modeling how storage can mitigate the [intermittency](@entry_id:275330) of renewables across different weather scenarios. Within this framework, decisions like curtailing excess renewable generation or spilling unused hydro inflows are natural [recourse actions](@entry_id:634878) the model can take when generation exceeds demand .

#### Multi-Objective Optimization

The standard objective of minimizing total system cost does not capture the full range of societal goals, which may include minimizing environmental impact, maximizing energy security, or promoting social equity. **Multi-objective optimization** addresses this by seeking to simultaneously optimize two or more conflicting objectives, such as minimizing cost ($C$) and minimizing emissions ($E$). Instead of a single [optimal solution](@entry_id:171456), the result is a set of non-dominated solutions known as the **Pareto frontier**. Each point on this frontier represents a trade-off where one objective cannot be improved without worsening another.

One common method for generating this frontier is the **epsilon-constraint ($\epsilon$-constraint) method**. In this approach, one objective (e.g., cost) is chosen as the primary objective to be minimized, while the other objectives are converted into constraints. For instance, to find the lowest-cost system that achieves a certain level of environmental performance, an upper-bound constraint is placed on total emissions: $E(g) \le E^{\max}$. By systematically varying the value of $E^{\max}$ and re-solving the cost-minimization problem, the planner can trace out the entire Pareto frontier, providing policymakers with a clear menu of cost-versus-emissions options from which to choose .

#### Endogenous Technological Learning

Technology costs are not static. For many technologies, particularly modular ones like solar PV and batteries, costs tend to decrease as cumulative global production increases. This phenomenon, known as **[technological learning](@entry_id:1132886)** or the [experience curve](@entry_id:1124759), can be modeled endogenously within a capacity expansion framework. The most common formulation is **Wright's Law**, a power-law relationship where the unit capital cost ($C_t$) is a function of cumulative installed capacity ($Q_t$): $C_t = C_0 (Q_t / Q_0)^{-\lambda}$, where $\lambda$ is the learning index derived from the technology's learning rate.

Incorporating this into a model makes the cost of investment a dynamic variable that depends on the model's own investment decisions. The cost of building a technology in period $t$ depends on the cumulative capacity built up to period $t-1$. This creates a dynamic feedback loop: investing in a technology makes it cheaper, which encourages further investment. This formulation is crucial for modeling long-term energy transitions and understanding the dynamics of technology adoption and diffusion .

#### Computational Solution Methods

The advanced formulations described above, particularly stochastic models with many scenarios and long-term models with endogenous learning, can result in optimization problems of enormous scale, often with millions of variables and constraints. Solving these models is a significant computational challenge that connects capacity expansion modeling to computer science and [numerical optimization](@entry_id:138060).

For large-scale two-stage stochastic problems, **Benders decomposition** (also known as the L-shaped method) is a powerful solution algorithm. It exploits the structure of the problem by decomposing it into a [master problem](@entry_id:635509) and multiple independent subproblems, one for each scenario. The [master problem](@entry_id:635509) proposes a first-stage investment decision ($x$). The subproblems then solve the operational dispatch for that fixed investment plan under their respective scenarios. The dual solutions from these subproblems are then used to generate "cuts"—new [linear constraints](@entry_id:636966)—that are added to the [master problem](@entry_id:635509). These cuts progressively refine the master problem's approximation of the expected future costs. This iterative process of proposing an investment, evaluating its consequences, and generating cuts continues until a solution is found that is both feasible across scenarios and has the lowest expected cost . These decomposition techniques are what make the analysis of highly detailed, multi-scenario, network-constrained national or continental-scale systems computationally tractable .