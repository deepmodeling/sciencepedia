{
    "hands_on_practices": [
        {
            "introduction": "The foundational method for offline change point detection is finding an optimal segmentation that balances data fidelity with model simplicity. This hands-on coding exercise guides you through implementing a dynamic programming algorithm to solve this exact problem . You will see firsthand how adjusting a penalty term and minimum segment length allows you to navigate the critical trade-off between detecting too many spurious changes (oversegmentation) and missing true structural shifts (undersegmentation).",
            "id": "4077437",
            "problem": "Consider a univariate solar irradiance time series with intermittent cloud passages measured at a fixed sampling interval of $1$ minute, where irradiance is recorded in watts per square meter (W/m$^2$). Let the time series be denoted by $\\{x_t\\}_{t=1}^N$, where $x_t$ is modeled as a piecewise constant mean corrupted by independent Gaussian noise: $x_t = \\mu_{s(t)} + \\varepsilon_t$, with $\\varepsilon_t \\sim \\mathcal{N}(0,\\sigma^2)$ and $s(t)$ the segment index for time $t$. A change point is any index $t$ at which the segment mean changes. In energy systems modeling, change point detection in energy time series is often posed as a penalized likelihood problem with a minimum segment length constraint to reduce oversegmentation and undersegmentation.\n\nStarting from the definitions of independent Gaussian noise and the principle of model selection via minimum description length and information criteria, derive and implement a program that, given a penalty magnitude $\\lambda$ applied per segment and a minimum segment length $L_{\\min}$, returns the integer classification of oversegmentation versus undersegmentation for a fixed, synthetic solar irradiance time series with known ground truth change points. Oversegmentation is defined as detecting a strictly greater number of segments than the ground truth ($+1$), undersegmentation is defined as detecting a strictly smaller number of segments than the ground truth ($-1$), and matched segmentation is defined as detecting exactly the same number of segments as the ground truth ($0$).\n\nUse the following scientifically realistic synthetic scenario to define $\\{x_t\\}_{t=1}^N$: Let $N = 480$ minutes. The ground truth consists of $9$ constant irradiance segments with means (in W/m$^2$) and durations (in minutes) given by the ordered lists\nmeans $= [800, 300, 820, 500, 850, 450, 830, 350, 820]$,\ndurations $= [90, 30, 80, 40, 60, 10, 70, 50, 50]$,\nso that the ground-truth change points occur at cumulative indices $[90, 120, 200, 240, 300, 310, 380, 430]$. Add independent Gaussian measurement noise with standard deviation $\\sigma = 60$ W/m$^2$. Fix the random seed deterministically so that results are reproducible. The program must solve the penalized segmentation problem that minimizes the sum of within-segment squared errors plus $\\lambda$ times the number of segments, subject to the constraint that each segment has length at least $L_{\\min}$.\n\nImplement an algorithm that computes the optimal segmentation under these constraints and returns the integer classification $+1$, $0$, or $-1$ for each of the test cases below. Do not assume any external libraries beyond the specified runtime environment. Angles are not used in this problem. Time is measured in minutes and irradiance in W/m$^2$, but the final outputs are unitless integers as defined above.\n\nTest Suite:\n- Case $1$: $\\lambda = 0$, $L_{\\min} = 1$.\n- Case $2$: $\\lambda = 10^9$, $L_{\\min} = 1$.\n- Case $3$: $\\lambda = 10^6$, $L_{\\min} = 1$.\n- Case $4$: $\\lambda = 10^3$, $L_{\\min} = 20$.\n- Case $5$: $\\lambda = 5 \\times 10^6$, $L_{\\min} = 50$.\n- Case $6$: $\\lambda = 10^6$, $L_{\\min} = 50$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[$result_1,result_2,\\dots,result_6$]\"), where each $result_i$ is the integer classification for the corresponding test case in the order listed above. No other output should be produced. The final outputs are unitless integers.",
            "solution": "The user wants me to solve a change point detection problem.\n\n### Step 1: Extract Givens\n-   **Time Series Model**: A univariate time series $\\{x_t\\}_{t=1}^N$ is modeled as $x_t = \\mu_{s(t)} + \\varepsilon_t$.\n-   **Noise**: The noise term $\\varepsilon_t$ is independent and identically distributed as a Gaussian distribution, $\\varepsilon_t \\sim \\mathcal{N}(0, \\sigma^2)$.\n-   **Signal**: The mean $\\mu_{s(t)}$ is piecewise constant, where $s(t)$ is the segment index for time $t$.\n-   **Objective Function**: Find a segmentation that minimizes the sum of within-segment squared errors plus a penalty term, $\\lambda \\times (\\text{number of segments})$.\n-   **Constraint**: Each segment must have a length of at least $L_{\\min}$.\n-   **Output Classification**:\n    -   Oversegmentation ($+1$): Number of detected segments > number of ground-truth segments.\n    -   Undersegmentation ($-1$): Number of detected segments < number of ground-truth segments.\n    -   Matched segmentation ($0$): Number of detected segments = number of ground-truth segments.\n-   **Synthetic Data Parameters**:\n    -   Total length: $N = 480$ minutes.\n    -   Sampling interval: $1$ minute.\n    -   Ground-truth means (W/m$^2$): `means` $= [800, 300, 820, 500, 850, 450, 830, 350, 820]$.\n    -   Ground-truth durations (minutes): `durations` $= [90, 30, 80, 40, 60, 10, 70, 50, 50]$.\n    -   Number of ground-truth segments: $K_{true} = 9$.\n    -   Noise standard deviation: $\\sigma = 60$ W/m$^2$.\n    -   A deterministic random seed must be used.\n-   **Test Suite**:\n    -   Case 1: $\\lambda = 0$, $L_{\\min} = 1$.\n    -   Case 2: $\\lambda = 10^9$, $L_{\\min} = 1$.\n    -   Case 3: $\\lambda = 10^6$, $L_{\\min} = 1$.\n    -   Case 4: $\\lambda = 10^3$, $L_{\\min} = 20$.\n    -   Case 5: $\\lambda = 5 \\times 10^6$, $L_{\\min} = 50$.\n    -   Case 6: $\\lambda = 10^6$, $L_{\\min} = 50$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically and mathematically well-defined.\n-   **Scientifically Grounded**: The model of a piecewise constant signal corrupted by Gaussian noise is a canonical model in signal processing and statistics. The problem of change point detection is fundamental in time series analysis. Using a penalized likelihood is a standard technique for model selection, directly related to information criteria like AIC and BIC. The physical context of solar irradiance and the numerical values provided are realistic. The problem is valid.\n-   **Well-Posed**: The task is to solve a clearly defined optimization problem. An optimal solution is guaranteed to exist. The use of dynamic programming provides a concrete method to find this unique optimal solution. The problem is valid.\n-   **Objective**: The problem is specified with precise mathematical definitions, numerical data, and unambiguous terminology. The objective function and constraints are clearly stated. The problem is valid.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n### Principle-Based Solution Derivation\n\nThe problem is to find an optimal partition of the time series $\\{x_t\\}_{t=0}^{N-1}$ into segments that minimizes a penalized cost function. This is a classic change point detection problem that can be solved exactly using dynamic programming.\n\n**1. Mathematical Formulation**\n\nLet the time series be $x_0, x_1, \\dots, x_{N-1}$. A segmentation is defined by a set of change points $0 = \\tau_0 < \\tau_1 < \\dots < \\tau_K = N$, which partition the data into $K$ segments. The $k$-th segment consists of data points $\\{x_t\\}_{t=\\tau_{k-1}}^{\\tau_k-1}$.\n\nThe problem is to minimize the total cost, which is the sum of two terms: the total within-segment sum of squared errors and a penalty term proportional to the number of segments.\nThe cost function to minimize is:\n$$ \\mathcal{J}(\\tau_1, \\dots, \\tau_K) = \\sum_{k=1}^{K} \\left( \\sum_{t=\\tau_{k-1}}^{\\tau_k-1} (x_t - \\hat{\\mu}_k)^2 \\right) + \\lambda K $$\nwhere $\\hat{\\mu}_k$ is the estimated mean of the $k$-th segment and $K$ is the total number of segments.\n\nThis formulation is derived from the principle of maximum likelihood estimation under a Gaussian noise model, combined with a penalty for model complexity, akin to information criteria such as AIC or BIC. For a segment of data assumed to be drawn from $\\mathcal{N}(\\mu, \\sigma^2)$, the maximum likelihood estimate for the mean $\\mu$ is the sample mean. Minimizing the sum of squared errors $\\sum(x_t - \\hat{\\mu})^2$ is equivalent to maximizing the likelihood of the data. The term $\\lambda K$ is a penalty for model complexity, preventing overfitting (oversegmentation) by penalizing each additional segment.\n\nLet $c(i, j)$ denote the minimum sum of squared errors for a single segment of data from index $i$ to $j-1$, i.e., $\\{x_t\\}_{t=i}^{j-1}$. This cost is achieved when the mean is the sample mean $\\hat{\\mu}_{i..j-1} = \\frac{1}{j-i} \\sum_{t=i}^{j-1} x_t$. The cost is then:\n$$ c(i, j) = \\sum_{t=i}^{j-1} (x_t - \\hat{\\mu}_{i..j-1})^2 = \\left(\\sum_{t=i}^{j-1} x_t^2\\right) - \\frac{1}{j-i} \\left(\\sum_{t=i}^{j-1} x_t\\right)^2 $$\nThe total cost can be expressed as:\n$$ \\mathcal{J} = \\sum_{k=1}^{K} \\left( c(\\tau_{k-1}, \\tau_k) + \\lambda \\right) $$\n\n**2. Dynamic Programming Solution**\n\nThis optimization problem exhibits optimal substructure, making it ideal for dynamic programming. Let $F[i]$ be the minimum cost for an optimal segmentation of the first $i$ data points, $x_0, \\dots, x_{i-1}$. Our goal is to compute $F[N]$.\n\nThe base case is for an empty prefix, which has a cost of $0$. Thus, $F[0] = 0$.\n\nFor $i > 0$, the value of $F[i]$ can be computed by considering all possible end points for the previous segment. If the last segment is $x_j, \\dots, x_{i-1}$, then the total cost is the optimal cost for segmenting up to $x_{j-1}$ (which is $F[j]$) plus the cost of this new final segment (which is $c(j, i) + \\lambda$). We must minimize this over all possible start points $j$.\n\nThe recurrence relation is:\n$$ F[i] = \\min_{0 \\le j < i} \\{ F[j] + c(j, i) + \\lambda \\} $$\nWe are also given the constraint that each segment must have a length of at least $L_{\\min}$. The length of the segment from $j$ to $i-1$ is $i-j$. Therefore, we must have $i-j \\ge L_{\\min}$, which implies $j \\le i - L_{\\min}$.\n\nThe constrained recurrence relation becomes:\n$$ F[i] = \\min_{0 \\le j \\le i - L_{\\min}} \\{ F[j] + c(j, i) + \\lambda \\} $$\nfor $i = 1, \\dots, N$. To reconstruct the segmentation, we also store the optimal index $j$ that gives the minimum for each $i$. Let this be $P[i]$.\n\n**3. Efficient Implementation**\n\nA naive implementation of this DP would be slow due to the repeated calculation of $c(j, i)$. The calculation of $c(j,i)$ takes $O(i-j)$ time, making the total complexity $O(N^3)$. We can reduce this to $O(N^2)$ by pre-computing cumulative sums.\nLet $S_1[i] = \\sum_{t=0}^{i-1} x_t$ and $S_2[i] = \\sum_{t=0}^{i-1} x_t^2$.\nThen $\\sum_{t=j}^{i-1} x_t = S_1[i] - S_1[j]$ and $\\sum_{t=j}^{i-1} x_t^2 = S_2[i] - S_2[j]$.\nThe cost $c(j, i)$ can now be computed in $O(1)$ time:\n$$ c(j, i) = (S_2[i] - S_2[j]) - \\frac{(S_1[i] - S_1[j])^2}{i-j} $$\nWith this optimization, the DP algorithm runs in $O(N^2)$ time, which is efficient enough for $N=480$.\n\n**4. Reconstructing the Solution and Classification**\n\nAfter populating the DP table $F$ and the pointer table $P$ up to $N$, the optimal cost is $F[N]$. To find the number of segments, we backtrack using the pointers from $P[N]$.\nLet the current index be `idx`$=N$.\nThe start of the last segment is $P[N]$.\nThe start of the second to last segment is $P[P[N]]$, and so on, until we reach $0$.\nThe number of segments is the number of backtracking steps required to go from $N$ to $0$.\n\nLet $K_{found}$ be the number of segments found by the algorithm. The ground truth number of segments is $K_{true} = 9$. The final classification is determined by the sign of $K_{found} - K_{true}$:\n-   If $K_{found} > K_{true}$, the result is $+1$ (oversegmentation).\n-   If $K_{found} < K_{true}$, the result is $-1$ (undersegmentation).\n-   If $K_{found} = K_{true}$, the result is $0$ (matched segmentation).\n\nThis procedure is applied to each test case with its specific $\\lambda$ and $L_{\\min}$ values.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the change point detection problem for all test cases.\n    \"\"\"\n\n    # 1. Define synthetic data parameters from the problem statement\n    means = [800, 300, 820, 500, 850, 450, 830, 350, 820]\n    durations = [90, 30, 80, 40, 60, 10, 70, 50, 50]\n    N = 480\n    sigma = 60.0\n    K_true = 9\n    \n    # 2. Generate the synthetic solar irradiance time series\n    # Fix the random seed for reproducibility as required\n    np.random.seed(42)\n    \n    # Construct the piecewise constant ground-truth signal\n    x_true = np.zeros(N)\n    current_idx = 0\n    for mean_val, duration in zip(means, durations):\n        x_true[current_idx : current_idx + duration] = mean_val\n        current_idx += duration\n        \n    # Add independent Gaussian noise\n    noise = np.random.normal(loc=0.0, scale=sigma, size=N)\n    x = x_true + noise\n\n    # 3. Define test cases\n    test_cases = [\n        (0, 1),           # Case 1\n        (10**9, 1),       # Case 2\n        (10**6, 1),       # Case 3\n        (10**3, 20),      # Case 4\n        (5 * 10**6, 50),  # Case 5\n        (10**6, 50),      # Case 6\n    ]\n\n    results = []\n    \n    # 4. Process each test case\n    for lambda_val, L_min in test_cases:\n        num_segments_found = find_optimal_segments(x, lambda_val, L_min)\n        \n        # Classify the segmentation result\n        if num_segments_found > K_true:\n            classification = 1\n        elif num_segments_found < K_true:\n            classification = -1\n        else:\n            classification = 0\n        results.append(classification)\n\n    # 5. Print the final output in the required format\n    print(f\"[{','.join(map(str, results))}]\")\n\n\ndef find_optimal_segments(x, lambda_val, L_min):\n    \"\"\"\n    Finds the optimal number of segments for a given time series, penalty, and min length.\n    Uses a dynamic programming approach (Optimal Partitioning).\n    \"\"\"\n    N = len(x)\n\n    # Precompute cumulative sums for efficient cost calculation\n    # S1[i] = sum(x[0]...x[i-1]), S2[i] = sum(x[0]**2...x[i-1]**2)\n    # This allows O(1) calculation of sum of squares for any sub-segment.\n    S1 = np.zeros(N + 1)\n    S2 = np.zeros(N + 1)\n    S1[1:] = np.cumsum(x)\n    S2[1:] = np.cumsum(x**2)\n\n    def cost(j, i):\n        \"\"\"\n        Calculates the sum of squared errors for the segment x[j...i-1].\n        \"\"\"\n        if j >= i:\n            return 0\n        \n        length = float(i - j)\n        sum_x = S1[i] - S1[j]\n        sum_x2 = S2[i] - S2[j]\n        \n        # cost = sum(x^2) - (sum(x))^2 / length\n        cost_val = sum_x2 - (sum_x**2) / length\n        return cost_val\n\n    # Initialize DP arrays\n    # F[i]: minimum cost for segmenting the prefix x[0...i-1]\n    # P[i]: stores the optimal last changepoint for the prefix x[0...i-1]\n    F = np.full(N + 1, np.inf)\n    P = np.zeros(N + 1, dtype=int)\n    \n    # Base case: cost of segmenting an empty prefix is 0\n    F[0] = 0\n\n    # Fill DP table using the recurrence relation\n    for i in range(1, N + 1):\n        # The last segment x[j...i-1] must have length at least L_min.\n        # Length = i - j >= L_min  =>  j <= i - L_min\n        for j in range(i - L_min + 1):\n            \n            # This path is only possible if F[j] is not infinity\n            if F[j] != np.inf:\n                # Cost of a potential segmentation ending at i with the last segment starting at j\n                c = cost(j, i)\n                current_cost = F[j] + c + lambda_val\n                \n                if current_cost < F[i]:\n                    F[i] = current_cost\n                    P[i] = j\n    \n    # Backtrack using the P array to find the number of segments\n    count = 0\n    idx = N\n    while idx > 0:\n        idx = P[idx]\n        count += 1\n        \n    return count\n\nsolve()\n```"
        },
        {
            "introduction": "Not all noise is created equal, and energy time series often exhibit \"heavy tails\" due to sudden events like cloud cover over a solar farm. This practice delves into the crucial concept of statistical robustness by comparing a standard mean-based detector with a median-based alternative under such challenging conditions . By calculating the Asymptotic Relative Efficiency (ARE), you will derive a quantitative measure of which detector is superior for this type of non-Gaussian data.",
            "id": "4077349",
            "problem": "A utility-scale photovoltaic field produces a time series of power deviations $y_t$ from its short-horizon forecast. Cloud transients and inverter control produce heavy-tailed fluctuation residuals that are well approximated by a Laplace distribution. Consider a canonical single change point model for location (level) shifts in the residual time series: for times $t < \\tau$, $y_t = \\varepsilon_t$ and for times $t \\ge \\tau$, $y_t = \\delta + \\varepsilon_t$, where $\\varepsilon_t$ are independent and identically distributed with the Laplace distribution of location $0$ and scale $b$, having probability density function $f(x) = \\frac{1}{2 b} \\exp\\!\\left(-\\frac{|x|}{b}\\right)$. The pre-change location equals $0$ and the post-change location equals $\\delta$, with $|\\delta|$ small relative to $b$.\n\nYou are asked to compare two Cumulative Sum (CUSUM) detectors that aggregate instantaneous increments $T(y_t)$:\n\n- A mean-based CUSUM using $T_{\\mathrm{mean}}(y_t) = y_t$ to target shifts in the mean.\n- A median-based CUSUM using $T_{\\mathrm{med}}(y_t) = \\operatorname{sign}(y_t)$, motivated by the influence function of the sample median in a symmetric distribution, to target shifts in the median.\n\nDefine the Pitman asymptotic relative efficiency (ARE) of $T_{\\mathrm{med}}$ relative to $T_{\\mathrm{mean}}$ for detecting small location shifts under the Laplace noise model as the ratio\n$$\n\\mathrm{ARE} \\equiv \\frac{\\left(\\Delta_{\\mathrm{med}}\\right)^{2} / \\operatorname{Var}_{0}\\!\\left(T_{\\mathrm{med}}(Y)\\right)}{\\left(\\Delta_{\\mathrm{mean}}\\right)^{2} / \\operatorname{Var}_{0}\\!\\left(T_{\\mathrm{mean}}(Y)\\right)},\n$$\nwhere $Y$ denotes a pre-change random variable with distribution $\\text{Laplace}(0,b)$, $\\operatorname{Var}_{0}$ denotes variance under the pre-change distribution, and $\\Delta_{\\mathrm{stat}}$ is the leading-order change in the expectation of $T_{\\mathrm{stat}}(Y + \\delta)$ as $\\delta \\to 0$.\n\nStarting from the stated distributional properties and general definitions of expectation and variance, derive a closed-form expression for $\\mathrm{ARE}$ under the Laplace noise model. Express the final answer as a dimensionless number. No rounding is required.",
            "solution": "The user has asked for the derivation of the Pitman Asymptotic Relative Efficiency (ARE) for two change point detectors under a Laplace noise model.\n\n### Step 1: Extract Givens\n- The time series model for power deviations $y_t$ has a single change point at time $\\tau$.\n- For $t < \\tau$, $y_t = \\varepsilon_t$.\n- For $t \\ge \\tau$, $y_t = \\delta + \\varepsilon_t$.\n- The noise terms $\\varepsilon_t$ are independent and identically distributed (i.i.d.) random variables from a Laplace distribution with location $0$ and scale $b$.\n- The probability density function (PDF) of the noise is $f(x) = \\frac{1}{2 b} \\exp\\left(-\\frac{|x|}{b}\\right)$.\n- The first detector is a mean-based CUSUM with increment function $T_{\\mathrm{mean}}(y_t) = y_t$.\n- The second detector is a median-based CUSUM with increment function $T_{\\mathrm{med}}(y_t) = \\operatorname{sign}(y_t)$.\n- The ARE is defined as:\n$$\n\\mathrm{ARE} \\equiv \\frac{\\left(\\Delta_{\\mathrm{med}}\\right)^{2} / \\operatorname{Var}_{0}\\!\\left(T_{\\mathrm{med}}(Y)\\right)}{\\left(\\Delta_{\\mathrm{mean}}\\right)^{2} / \\operatorname{Var}_{0}\\!\\left(T_{\\mathrm{mean}}(Y)\\right)}\n$$\n- $Y$ is a pre-change random variable, so $Y \\sim \\text{Laplace}(0,b)$.\n- $\\operatorname{Var}_{0}$ denotes variance under the pre-change distribution (i.e., for $Y$).\n- $\\Delta_{\\mathrm{stat}}$ is the leading-order change in the expectation of $T_{\\mathrm{stat}}(Y + \\delta)$ as $\\delta \\to 0$. This is equivalent to the derivative $\\left. \\frac{d}{d\\delta} \\mathbb{E}[T_{\\mathrm{stat}}(Y+\\delta)] \\right|_{\\delta=0}$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded in the fields of statistical signal processing and time series analysis. The Laplace distribution, CUSUM algorithm, and Pitman ARE are all standard, well-established concepts. The problem is well-posed, providing all necessary definitions, distributions, and a clear objective. The language is precise and objective. The setup is mathematically and statistically consistent, non-contradictory, and the required calculations are feasible. Therefore, the problem is deemed **valid**.\n\n### Step 3: Derivation of the Solution\nTo compute the ARE, we must calculate four quantities: $\\operatorname{Var}_{0}\\!\\left(T_{\\mathrm{mean}}(Y)\\right)$, $\\Delta_{\\mathrm{mean}}$, $\\operatorname{Var}_{0}\\!\\left(T_{\\mathrm{med}}(Y)\\right)$, and $\\Delta_{\\mathrm{med}}$.\n\nLet $Y$ be a random variable with the pre-change Laplace distribution, $Y \\sim \\text{Laplace}(0, b)$. Its PDF is $f_Y(y) = \\frac{1}{2b} \\exp(-\\frac{|y|}{b})$. The standard properties of this distribution are that its mean is $\\mathbb{E}[Y]=0$ and its variance is $\\operatorname{Var}(Y) = 2b^2$.\n\n**Analysis of the Mean-Based Detector ($T_{\\mathrm{mean}}(Y) = Y$)**\n\n1.  **Variance under the null hypothesis ($H_0: \\delta=0$):**\n    The increment function is $T_{\\mathrm{mean}}(Y) = Y$. The variance under the pre-change distribution is:\n    $$\n    \\operatorname{Var}_{0}\\!\\left(T_{\\mathrm{mean}}(Y)\\right) = \\operatorname{Var}(Y) = 2b^2\n    $$\n\n2.  **Efficacy term ($\\Delta_{\\mathrm{mean}}$):**\n    We need to compute the derivative of the expectation of the post-change statistic, $T_{\\mathrm{mean}}(Y+\\delta) = Y+\\delta$, with respect to $\\delta$ at $\\delta=0$.\n    The expectation of the post-change statistic is:\n    $$\n    \\mathbb{E}[T_{\\mathrm{mean}}(Y+\\delta)] = \\mathbb{E}[Y+\\delta] = \\mathbb{E}[Y] + \\delta\n    $$\n    Since $\\mathbb{E}[Y]=0$, we have:\n    $$\n    \\mathbb{E}[T_{\\mathrm{mean}}(Y+\\delta)] = \\delta\n    $$\n    Now, we compute $\\Delta_{\\mathrm{mean}}$:\n    $$\n    \\Delta_{\\mathrm{mean}} = \\left. \\frac{d}{d\\delta} \\mathbb{E}[T_{\\mathrm{mean}}(Y+\\delta)] \\right|_{\\delta=0} = \\left. \\frac{d}{d\\delta}(\\delta) \\right|_{\\delta=0} = 1\n    $$\n\n**Analysis of the Median-Based Detector ($T_{\\mathrm{med}}(Y) = \\operatorname{sign}(Y)$)**\n\n1.  **Variance under the null hypothesis ($H_0: \\delta=0$):**\n    The increment function is $T_{\\mathrm{med}}(Y) = \\operatorname{sign}(Y)$. First, we compute its expectation under the pre-change distribution.\n    $$\n    \\mathbb{E}_{0}[T_{\\mathrm{med}}(Y)] = \\mathbb{E}[\\operatorname{sign}(Y)] = \\int_{-\\infty}^{\\infty} \\operatorname{sign}(y) f_Y(y) dy\n    $$\n    Since $\\operatorname{sign}(y)$ is an odd function and $f_Y(y)$ is an even function (symmetric about $0$), their product is an odd function, and the integral over a symmetric interval $(-\\infty, \\infty)$ is $0$. Thus, $\\mathbb{E}_{0}[T_{\\mathrm{med}}(Y)] = 0$.\n    The variance is then:\n    $$\n    \\operatorname{Var}_{0}\\!\\left(T_{\\mathrm{med}}(Y)\\right) = \\mathbb{E}_{0}[(T_{\\mathrm{med}}(Y))^2] - (\\mathbb{E}_{0}[T_{\\mathrm{med}}(Y)])^2 = \\mathbb{E}[(\\operatorname{sign}(Y))^2] - 0^2\n    $$\n    For any $y \\neq 0$, $(\\operatorname{sign}(y))^2=1$. Since $Y$ is a continuous random variable, $P(Y=0)=0$.\n    $$\n    \\mathbb{E}[(\\operatorname{sign}(Y))^2] = \\int_{-\\infty}^{\\infty} (\\operatorname{sign}(y))^2 f_Y(y) dy = \\int_{-\\infty}^{\\infty} 1 \\cdot f_Y(y) dy = 1\n    $$\n    Therefore, the variance is:\n    $$\n    \\operatorname{Var}_{0}\\!\\left(T_{\\mathrm{med}}(Y)\\right) = 1\n    $$\n\n2.  **Efficacy term ($\\Delta_{\\mathrm{med}}$):**\n    We need to compute $\\Delta_{\\mathrm{med}} = \\left. \\frac{d}{d\\delta} \\mathbb{E}[T_{\\mathrm{med}}(Y+\\delta)] \\right|_{\\delta=0}$. The expectation is:\n    $$\n    \\mathbb{E}[T_{\\mathrm{med}}(Y+\\delta)] = \\mathbb{E}[\\operatorname{sign}(Y+\\delta)] = \\int_{-\\infty}^{\\infty} \\operatorname{sign}(y+\\delta) f_Y(y) dy\n    $$\n    This is equivalent to $P(Y+\\delta > 0) - P(Y+\\delta < 0) = P(Y > -\\delta) - P(Y < -\\delta)$.\n    Let $F_Y(y)$ be the cumulative distribution function (CDF) of $Y \\sim \\text{Laplace}(0,b)$. Then $P(Y < -\\delta) = F_Y(-\\delta)$ and $P(Y > -\\delta) = 1 - F_Y(-\\delta)$.\n    $$\n    \\mathbb{E}[\\operatorname{sign}(Y+\\delta)] = (1 - F_Y(-\\delta)) - F_Y(-\\delta) = 1 - 2F_Y(-\\delta)\n    $$\n    The CDF of a Laplace($0,b$) distribution is:\n    $$\n    F_Y(y) = \\begin{cases} \\frac{1}{2} \\exp\\left(\\frac{y}{b}\\right) & y \\le 0 \\\\ 1 - \\frac{1}{2} \\exp\\left(-\\frac{y}{b}\\right) & y > 0 \\end{cases}\n    $$\n    We analyze the expectation for small positive and negative $\\delta$.\n    - For $\\delta > 0$, $-\\delta < 0$, so we use the first case of the CDF: $F_Y(-\\delta) = \\frac{1}{2}\\exp(-\\frac{\\delta}{b})$.\n      $$\n      \\mathbb{E}[\\operatorname{sign}(Y+\\delta)] = 1 - 2 \\left(\\frac{1}{2} \\exp\\left(-\\frac{\\delta}{b}\\right)\\right) = 1 - \\exp\\left(-\\frac{\\delta}{b}\\right)\n      $$\n    - For $\\delta < 0$, $-\\delta > 0$, so we use the second case of the CDF: $F_Y(-\\delta) = 1 - \\frac{1}{2}\\exp(-(-\\frac{\\delta}{b})) = 1 - \\frac{1}{2}\\exp(\\frac{\\delta}{b})$.\n      $$\n      \\mathbb{E}[\\operatorname{sign}(Y+\\delta)] = 1 - 2 \\left(1 - \\frac{1}{2} \\exp\\left(\\frac{\\delta}{b}\\right)\\right) = e^{\\frac{\\delta}{b}} - 1\n      $$\n    To find $\\Delta_{\\mathrm{med}}$, we differentiate with respect to $\\delta$ and evaluate at $\\delta=0$. We can use the one-sided limits.\n    For $\\delta \\to 0^+$:\n    $$\n    \\lim_{\\delta \\to 0^+} \\frac{1 - \\exp\\left(-\\frac{\\delta}{b}\\right)}{\\delta} = \\lim_{\\delta \\to 0^+} \\frac{-\\left(-\\frac{1}{b}\\right)\\exp\\left(-\\frac{\\delta}{b}\\right)}{1} = \\frac{1}{b}\n    $$\n    For $\\delta \\to 0^-$:\n    $$\n    \\lim_{\\delta \\to 0^-} \\frac{\\exp\\left(\\frac{\\delta}{b}\\right) - 1}{\\delta} = \\lim_{\\delta \\to 0^-} \\frac{\\frac{1}{b}\\exp\\left(\\frac{\\delta}{b}\\right)}{1} = \\frac{1}{b}\n    $$\n    Since both limits are equal, the derivative exists at $\\delta=0$ and its value is $\\frac{1}{b}$.\n    Alternatively, the leading-order term in the Taylor expansion of $\\mathbb{E}[\\operatorname{sign}(Y+\\delta)]$ around $\\delta = 0$ is $\\frac{\\delta}{b}$. The coefficient is $\\frac{1}{b}$.\n    Thus, $\\Delta_{\\mathrm{med}} = \\frac{1}{b}$.\n\n**Calculation of the ARE**\n\nWe now substitute the four calculated quantities into the ARE formula:\n- $\\left(\\Delta_{\\mathrm{mean}}\\right)^{2} = 1^2 = 1$\n- $\\operatorname{Var}_{0}\\!\\left(T_{\\mathrm{mean}}(Y)\\right) = 2b^2$\n- $\\left(\\Delta_{\\mathrm{med}}\\right)^{2} = (\\frac{1}{b})^2 = \\frac{1}{b^2}$\n- $\\operatorname{Var}_{0}\\!\\left(T_{\\mathrm{med}}(Y)\\right) = 1$\n\nThe efficacy of the mean-based detector is $\\mathcal{E}_{\\mathrm{mean}} = \\frac{\\left(\\Delta_{\\mathrm{mean}}\\right)^{2}}{\\operatorname{Var}_{0}\\!\\left(T_{\\mathrm{mean}}(Y)\\right)} = \\frac{1}{2b^2}$.\nThe efficacy of the median-based detector is $\\mathcal{E}_{\\mathrm{med}} = \\frac{\\left(\\Delta_{\\mathrm{med}}\\right)^{2}}{\\operatorname{Var}_{0}\\!\\left(T_{\\mathrm{med}}(Y)\\right)} = \\frac{1/b^2}{1} = \\frac{1}{b^2}$.\n\nThe ARE is the ratio of these efficacies:\n$$\n\\mathrm{ARE} = \\frac{\\mathcal{E}_{\\mathrm{med}}}{\\mathcal{E}_{\\mathrm{mean}}} = \\frac{1/b^2}{1/(2b^2)} = \\frac{1}{b^2} \\cdot \\frac{2b^2}{1} = 2\n$$\nThe Asymptotic Relative Efficiency of the median-based detector relative to the mean-based detector for a location shift in Laplace noise is $2$. This value being greater than $1$ indicates that the median-based detector is more efficient, which is expected given the heavy tails of the Laplace distribution.",
            "answer": "$$\\boxed{2}$$"
        },
        {
            "introduction": "A powerful statistical model becomes truly useful when it incorporates the physical laws of the system it describes. This advanced practice challenges you to modify the change point detection algorithm to respect the physical constraints of a battery, namely its finite capacity and non-negative state of charge . This exercise demonstrates how to fuse domain-specific engineering knowledge with statistical optimization to produce results that are not only mathematically optimal but also physically feasible.",
            "id": "4077358",
            "problem": "You are given a discrete-time series representing net power imbalance measurements for a grid-connected battery over successive, equally spaced time steps. Let the measurement sequence be denoted by $\\{y_t\\}_{t=1}^T$, with each $y_t$ in kilowatts (kW). Assume the interval length between samples is $1$ hour, so accumulated sums of power correspond to energy in kilowatt-hours (kWh). Model the system under the following foundational assumptions and definitions.\n\n1. Statistical model and segmentation. The series obeys a piecewise-constant mean model with additive noise. There exists a segmentation of the index set $\\{1,2,\\dots,T\\}$ into $K$ contiguous segments with means $\\{\\mu_k\\}_{k=1}^K$ such that\n$$\ny_t = \\mu_{\\text{seg}(t)} + \\varepsilon_t,\n$$\nwhere $\\varepsilon_t$ are independent, identically distributed Gaussian random variables with zero mean and constant variance, and $\\text{seg}(t)$ maps $t$ to its segment index. Under this model, the negative log-likelihood up to an additive constant is proportional to the sum of squared errors\n$$\n\\sum_{t=1}^T \\left(y_t - \\mu_{\\text{seg}(t)}\\right)^2.\n$$\nFor a fixed segmentation, the least-squares estimate of each segment mean $\\mu_k$ equals the arithmetic average of the samples in that segment.\n\n2. Battery state-of-charge dynamics under mean-centering. Define the battery state-of-charge sequence $\\{s_t\\}_{t=0}^T$ with $s_0$ given (in kWh). Within each segment, define the mean-centered residual sequence $r_t = y_t - \\mu_{\\text{seg}(t)}$. The state evolves as\n$$\ns_t = s_{t-1} + r_t, \\quad t = 1,2,\\dots,T,\n$$\nbecause each $r_t$ is net energy flow over one hour. Since $\\mu_{\\text{seg}(t)}$ is the sample mean within its segment, the sum of $r_t$ over any complete segment equals $0$, implying that $s_t$ returns to its value at the start of the segment after the segment ends.\n\n3. Physical constraints. The battery must respect nonnegativity and finite capacity:\n$$\n0 \\le s_t \\le C, \\quad t=0,1,\\dots,T,\n$$\nwhere $C$ is the battery capacity in kWh. Because the state-of-charge returns to the segment-start value at each segment end, feasibility of a segmentation with respect to the above constraints reduces to checking whether, for every segment, the partial sums of the mean-centered residuals, added to the common starting level $s_0$, remain in $[0,C]$ throughout that segment.\n\n4. Model selection criterion. Given a nonnegative penalty parameter $\\lambda$ (in the same units as squared errors), consider the penalized objective\n$$\nJ = \\sum_{t=1}^T \\left(y_t - \\mu_{\\text{seg}(t)}\\right)^2 + \\lambda\\cdot (K-1),\n$$\nwhich penalizes the number of change points (a change point occurs at any index $t \\in \\{1,2,\\dots,T-1\\}$ where a new segment begins at time $t+1$). The unconstrained optimal segmentation minimizes $J$ over all possible segmentations. The constrained optimal segmentation minimizes $J$ subject to the segmentwise feasibility of the state-of-charge path with respect to the battery constraints.\n\nYour tasks are:\n\nA. Derive, from the Gaussian noise assumption and the state update definition, the segmentwise cost structure and the segmentwise feasibility condition. Explain why, for least-squares segment means, the state-of-charge at the end of a segment equals the state-of-charge at the start of the segment, and why this implies that feasibility can be checked independently on each segment using the common initial level $s_0$.\n\nB. Design an algorithm based on dynamic programming (DP) to compute:\n- the unconstrained optimal segmentation that minimizes $J$,\n- the constrained optimal segmentation that minimizes $J$ with the feasibility condition enforced on every segment.\n\nYour DP must:\n- treat the total cost as the sum of additive segment costs plus the penalty,\n- for the constrained case, restrict transitions to those segments whose mean-centered cumulative sums preserve the state-of-charge within $[0,C]$ when starting from $s_0$,\n- reconstruct the list of change point indices. A change point at time $t$ means a new segment begins at $t+1$, so the reported indices must lie in $\\{1,2,\\dots,T-1\\}$.\n\nC. Implement the algorithm as a complete, runnable program that takes no user input and produces the required outputs for the following test suite. In all cases:\n- express $y_t$ in kW,\n- $s_0$ and $C$ in kWh,\n- the sampling interval is $1$ hour.\n\nTest Suite:\n1. Case $1$ (balanced variability, ample capacity):\n   - $y = [3.2, 3.3, 3.1, 3.4, 3.5, 3.4, 3.3, 3.2]$,\n   - $s_0 = 5.0$ kWh,\n   - $C = 10.0$ kWh,\n   - $\\lambda = 1.0$.\n\n2. Case $2$ (monotone ramp, tight capacity, large penalty for change points):\n   - $y = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]$,\n   - $s_0 = 2.0$ kWh,\n   - $C = 3.0$ kWh,\n   - $\\lambda = 20.0$.\n\n3. Case $3$ (alternating injections and withdrawals, near-upper initial state):\n   - $y = [3, -1, 2, -2, 4, -3, 5, -4]$,\n   - $s_0 = 1.8$ kWh,\n   - $C = 2.0$ kWh,\n   - $\\lambda = 5.0$.\n\nFinal Output Specification:\nFor each test case, your program must output a list containing four items:\n- the number of segments in the unconstrained optimal solution (an integer),\n- the list of change point indices for the unconstrained solution (a list of integers in $\\{1,2,\\dots,T-1\\}$),\n- the number of segments in the constrained optimal solution (an integer),\n- the list of change point indices for the constrained solution (a list of integers in $\\{1,2,\\dots,T-1\\}$).\n\nYour program should produce a single line of output containing the results for all cases as a comma-separated list enclosed in square brackets. For example, the format must be $[r_1,r_2,r_3]$, where each $r_i$ is the four-item list described above. The outputs are unitless indices and counts; do not print any physical units in the final line. Ensure the program computes these values exactly for the test suite and prints only that single line.",
            "solution": "The user has provided a problem statement that is scientifically sound, well-posed, and objective. It outlines a formal procedure for optimal segmentation of a time series under physical constraints, a standard task in signal processing and energy systems analysis. The problem is suitable for a solution based on dynamic programming.\n\n### A. Segment Cost Structure and Feasibility Condition\n\nThe problem asks for an optimal segmentation of a time series $\\{y_t\\}_{t=1}^T$ by minimizing a penalized least-squares objective function, subject to physical constraints on a derived state-of-charge (SOC) sequence $\\{s_t\\}_{t=0}^T$.\n\n**1. Segmentwise Cost Structure**\n\nThe objective function to minimize is given by\n$$\nJ = \\sum_{t=1}^T \\left(y_t - \\mu_{\\text{seg}(t)}\\right)^2 + \\lambda \\cdot (K-1)\n$$\nwhere $K$ is the number of segments and $\\lambda$ is a penalty parameter. A segmentation is defined by a set of change points $\\{\\tau_1, \\tau_2, \\dots, \\tau_{K-1}\\}$. This partitions the time indices $\\{1, \\dots, T\\}$ into $K$ contiguous segments. Let a generic segment run from time $i$ to $j$. The contribution of this segment to the first term of $J$ is the sum of squared errors (SSE) within that segment. The optimal mean for this segment, $\\mu_{i,j}$, under the least-squares criterion is the sample mean:\n$$\n\\mu_{i,j} = \\frac{1}{j-i+1} \\sum_{t=i}^j y_t\n$$\nThe cost for this segment, which we denote $C(i, j)$, is then:\n$$\nC(i, j) = \\sum_{t=i}^j (y_t - \\mu_{i,j})^2\n$$\nThis expression can be expanded for efficient computation:\n$$\nC(i, j) = \\sum_{t=i}^j y_t^2 - 2\\mu_{i,j} \\sum_{t=i}^j y_t + (j-i+1)\\mu_{i,j}^2\n$$\nSubstituting the expression for $\\mu_{i,j}$:\n$$\nC(i, j) = \\sum_{t=i}^j y_t^2 - 2 \\left(\\frac{\\sum_{t=i}^j y_t}{j-i+1}\\right) \\sum_{t=i}^j y_t + (j-i+1) \\left(\\frac{\\sum_{t=i}^j y_t}{j-i+1}\\right)^2\n$$\n$$\nC(i, j) = \\sum_{t=i}^j y_t^2 - \\frac{2(\\sum_{t=i}^j y_t)^2}{j-i+1} + \\frac{(\\sum_{t=i}^j y_t)^2}{j-i+1} = \\sum_{t=i}^j y_t^2 - \\frac{\\left(\\sum_{t=i}^j y_t\\right)^2}{j-i+1}\n$$\nThis allows the cost of any segment $[i, j]$ to be calculated rapidly if prefix sums of $\\{y_t\\}$ and $\\{y_t^2\\}$ are pre-computed. The total objective $J$ for a segmentation with change points $\\tau_0=0, \\tau_1, \\dots, \\tau_{K-1}, \\tau_K=T$ is then an additive sum:\n$$\nJ = \\left( \\sum_{k=1}^K C(\\tau_{k-1}+1, \\tau_k) \\right) + \\lambda(K-1)\n$$\nThis additive structure is the key to solving the problem with dynamic programming.\n\n**2. State-of-Charge Dynamics and Segmentwise Feasibility**\n\nThe SOC evolves as $s_t = s_{t-1} + r_t$, where $r_t = y_t - \\mu_{\\text{seg}(t)}$ is the mean-centered residual. The interval between samples is $1$ hour, so the power $y_t$ in kW is numerically equal to the energy in kWh over that hour.\n\nConsider a segment from time $i$ to $j$. The SOC at the end of the segment, $s_j$, can be related to the SOC at the start, $s_{i-1}$, by summing the updates:\n$$\ns_j = s_{i-1} + \\sum_{t=i}^j r_t = s_{i-1} + \\sum_{t=i}^j (y_t - \\mu_{i,j})\n$$\nBy the definition of the sample mean $\\mu_{i,j}$, the sum of residuals over the segment is zero:\n$$\n\\sum_{t=i}^j (y_t - \\mu_{i,j}) = \\left(\\sum_{t=i}^j y_t\\right) - (j-i+1)\\mu_{i,j} = \\left(\\sum_{t=i}^j y_t\\right) - (j-i+1)\\frac{\\sum_{t=i}^j y_t}{j-i+1} = 0\n$$\nTherefore, $s_j = s_{i-1}$. The SOC at the end of any complete segment returns to its value at the beginning of that segment.\n\nThis property implies that the starting SOC for any segment is always the same. If the first segment is $[1, \\tau_1]$, its starting state is $s_0$. Since $s_{\\tau_1} = s_0$, the second segment $[\\tau_1+1, \\tau_2]$ also effectively begins with state $s_0$. By induction, every segment in any valid segmentation starts with an SOC equal to the global initial state $s_0$.\n\nConsequently, the feasibility of an entire segmentation can be checked on a segment-by-segment basis. A proposed segment from time $i$ to $j$ is feasible if and only if the SOC path within that segment remains within the physical bounds $[0, C]$. The SOC at any time $k$ within this segment ($i \\le k \\le j$) is:\n$$\ns_k = s_{i-1} + \\sum_{l=i}^k r_l = s_0 + \\sum_{l=i}^k (y_t - \\mu_{i,j})\n$$\nThe feasibility condition for the segment $[i,j]$ is thus:\n$$\n0 \\le s_0 + \\sum_{l=i}^k (y_l - \\mu_{i,j}) \\le C \\quad \\forall k \\in \\{i, i+1, \\dots, j\\}\n$$\n\n### B. Dynamic Programming Algorithm Design\n\nThe problem of finding an optimal segmentation can be modeled as finding a shortest path in a directed acyclic graph where nodes are time points and edges represent segments.\n\n**1. DP Formulation**\n\nLet $\\text{Opt}[t]$ be the minimum penalized cost for an optimal segmentation of the subsequence $\\{y_1, \\dots, y_t\\}$. Our goal is to compute $\\text{Opt}[T]$.\nThe base case is for an empty sequence, which has a cost of $0$. To properly account for the penalty term $\\lambda(K-1)$, which is a penalty on the number of *change points*, we set $\\text{Opt}[0] = -\\lambda$.\n\nThe recurrence relation for $\\text{Opt}[t]$ for $t > 0$ is derived by considering all possible endpoints of the previous segment. If the last segment is $[j+1, t]$ (for $0 \\le j < t$), the total cost is the optimal cost for the subsequence $\\{y_1, \\dots, y_j\\}$ plus the cost of the new segment $C(j+1,t)$ plus the penalty $\\lambda$ for the new change point at $j$.\n$$\n\\text{Opt}[t] = \\min_{0 \\le j < t} \\{ \\text{Opt}[j] + C(j+1, t) + \\lambda \\}\n$$\nThe base case $\\text{Opt}[0] = -\\lambda$ ensures that for the very first segment $[1,t]$ (corresponding to $j=0$), the cost is correctly calculated as $\\text{Opt}[0] + C(1,t) + \\lambda = -\\lambda + C(1,t) + \\lambda = C(1,t)$, which has $K-1=0$ change points and thus no penalty.\n\nTo reconstruct the segmentation, we maintain a pointer array, $\\text{Pointers}[t]$, which stores the index $j$ that yielded the minimum cost for $\\text{Opt}[t]$.\n\n**2. Unconstrained vs. Constrained Algorithm**\n\nThe DP framework elegantly handles both cases.\n\n*   **Unconstrained Case**: The algorithm proceeds exactly as described by the recurrence relation. At each step $t$, it iterates through all possible previous change points $j$ and finds the one that minimizes the total cost.\n\n*   **Constrained Case**: The recurrence is modified by a feasibility check. When considering a potential last segment $[j+1, t]$, we first compute its mean $\\mu_{j+1,t}$ and then check if the segment is feasible according to the condition derived in Part A.\n    $$\n    0 \\le s_0 + \\sum_{l=j+1}^k (y_l - \\mu_{j+1,t}) \\le C \\quad \\forall k \\in \\{j+1, \\dots, t\\}\n    $$\n    If the segment is not feasible, it is disallowed. This can be implemented by assigning an infinite cost to the transition from $j$ to $t$. The recurrence effectively becomes:\n    $$\n    \\text{Opt}[t] = \\min_{0 \\le j < t, \\text{segment }[j+1, t]\\text{ is feasible}} \\{ \\text{Opt}[j] + C(j+1, t) + \\lambda \\}\n    $$\n\n**3. Algorithm Steps**\n\n1.  **Initialization**: Given the time series $\\{y_t\\}_{t=1}^T$ of length $T$.\n    *   Create prefix sum arrays for $\\{y_t\\}$ and $\\{y_t^2\\}$ to allow for $O(1)$ calculation of $C(i, j)$.\n    *   Initialize a cost array `dp_costs` of size $T+1$ with $\\infty$, and set `dp_costs[0] = -lambda`.\n    *   Initialize a pointer array `dp_pointers` of size $T+1$.\n\n2.  **DP Computation**:\n    *   For $t$ from $1$ to $T$:\n        *   For $j$ from $0$ to $t-1$:\n            *   Calculate the cost $C(j+1, t)$ of the segment from time $j+1$ to $t$.\n            *   **If constrained**: Check the feasibility of this segment. If not feasible, continue to the next $j$.\n            *   Calculate the total cost: `new_cost = dp_costs[j] + C(j+1, t) + lambda`.\n            *   If `new_cost < dp_costs[t]`, update `dp_costs[t] = new_cost` and `dp_pointers[t] = j`.\n\n3.  **Path Reconstruction**:\n    *   Initialize an empty list of change points.\n    *   Start with `curr = T`.\n    *   While `curr > 0`:\n        *   Get the previous change point index `prev = dp_pointers[curr]`.\n        *   If `prev > 0`, add `prev` to the list of change points (since this corresponds to a change point at time `prev`).\n        *   Set `curr = prev`.\n    *   Reverse the list of change points to get them in chronological order. The number of segments is $1$ plus the number of change points.\n\nThis procedure is executed once for the unconstrained case and once for the constrained case to obtain the four required outputs for each test scenario.",
            "answer": "```python\nimport numpy as np\n\ndef _find_segmentation(y, s0, C, lambda_val, constrained):\n    \"\"\"\n    Finds the optimal segmentation using dynamic programming.\n    \n    Args:\n        y (np.ndarray): The time series data.\n        s0 (float): Initial state of charge.\n        C (float): Battery capacity.\n        lambda_val (float): Penalty parameter for change points.\n        constrained (bool): If True, enforce SOC constraints.\n\n    Returns:\n        tuple[int, list[int]]: Number of segments and list of change point indices.\n    \"\"\"\n    T = len(y)\n    \n    # Precompute prefix sums for y and y^2 for O(1) cost calculation.\n    # We use 1-based indexing for sums to align with math notation.\n    S1 = np.zeros(T + 1)\n    S2 = np.zeros(T + 1)\n    for i in range(T):\n        S1[i+1] = S1[i] + y[i]\n        S2[i+1] = S2[i] + y[i]**2\n        \n    def get_segment_cost(j, t):\n        \"\"\"Calculates sum of squared errors for segment y[j...t-1].\"\"\"\n        # Segment corresponds to time steps j+1 to t.\n        # Python indices are j to t-1.\n        length = t - j\n        if length == 0:\n            return 0\n        sum_y = S1[t] - S1[j]\n        sum_y_sq = S2[t] - S2[j]\n        cost = sum_y_sq - (sum_y ** 2) / length\n        return cost\n\n    # DP state: dp_costs[t] = min cost for segmenting y[0...t-1]\n    dp_costs = np.full(T + 1, np.inf)\n    dp_pointers = np.zeros(T + 1, dtype=int)\n    \n    # Base case: cost for empty sequence is 0.\n    # The -lambda term correctly handles the penalty count (K-1).\n    dp_costs[0] = -lambda_val\n    \n    for t in range(1, T + 1):\n        for j in range(t):\n            # Potential segment is y[j...t-1], corresponding to times j+1 to t\n            cost = get_segment_cost(j, t)\n            \n            if constrained:\n                segment_data = y[j:t]\n                length = len(segment_data)\n                \n                # If segment is empty, skip\n                if length == 0:\n                    continue\n                \n                mean = np.mean(segment_data)\n                residuals = segment_data - mean\n                cum_residuals = np.cumsum(residuals)\n                \n                # SOC path for this segment, starting from s0\n                soc_path = s0 + cum_residuals\n                \n                # Feasibility check: 0 <= s_t <= C for all t in segment\n                if np.any(soc_path < 0) or np.any(soc_path > C):\n                    continue  # This segment is infeasible, skip\n\n            total_cost = dp_costs[j] + cost + lambda_val\n            \n            if total_cost < dp_costs[t]:\n                dp_costs[t] = total_cost\n                dp_pointers[t] = j\n\n    # Reconstruct the path\n    changepoints = []\n    curr = T\n    while curr > 0:\n        prev = dp_pointers[curr]\n        if prev > 0:\n            # Change point at time `prev` (1-based index)\n            changepoints.append(prev)\n        curr = prev\n        \n    changepoints.sort()\n    num_segments = len(changepoints) + 1\n    \n    return num_segments, changepoints\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and format output.\n    \"\"\"\n    test_cases = [\n        {\n            \"y\": np.array([3.2, 3.3, 3.1, 3.4, 3.5, 3.4, 3.3, 3.2]),\n            \"s0\": 5.0,\n            \"C\": 10.0,\n            \"lambda\": 1.0\n        },\n        {\n            \"y\": np.array([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]),\n            \"s0\": 2.0,\n            \"C\": 3.0,\n            \"lambda\": 20.0\n        },\n        {\n            \"y\": np.array([3., -1., 2., -2., 4., -3., 5., -4.]),\n            \"s0\": 1.8,\n            \"C\": 2.0,\n            \"lambda\": 5.0\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        y, s0, C, lambda_val = case[\"y\"], case[\"s0\"], case[\"C\"], case[\"lambda\"]\n        \n        k_unc, cp_unc = _find_segmentation(y, s0, C, lambda_val, constrained=False)\n        k_con, cp_con = _find_segmentation(y, s0, C, lambda_val, constrained=True)\n        \n        all_results.append([k_unc, cp_unc, k_con, cp_con])\n\n    # Manually format the output string to be compact (no spaces)\n    result_strings = []\n    for res in all_results:\n        k_u, cp_u, k_c, cp_c = res\n        cp_u_str = '[' + ','.join(map(str, cp_u)) + ']'\n        cp_c_str = '[' + ','.join(map(str, cp_c)) + ']'\n        res_str = f\"[{k_u},{cp_u_str},{k_c},{cp_c_str}]\"\n        result_strings.append(res_str)\n    \n    final_output = '[' + ','.join(result_strings) + ']'\n    print(final_output)\n\nsolve()\n\n```"
        }
    ]
}