{
    "hands_on_practices": [
        {
            "introduction": "Mastering global sensitivity analysis begins with a firm grasp of its foundational concepts. This first practice provides a direct, hands-on calculation of Sobol' sensitivity indices from their definitions in a controlled setting . By working through an analytical solution for a simple model, you will build intuition for how these indices quantify the contribution of each input to output variance and how model structure, such as additivity, is reflected in their values.",
            "id": "4133329",
            "problem": "In a stylized uncertainty quantification study of an energy systems surrogate, consider the scalar output model $Y = X_1^2 + X_2$, where $X_1$ and $X_2$ are independent uncertain inputs. Suppose $X_1 \\sim \\mathcal{U}(-1, 2)$ and $X_2 \\sim \\mathcal{U}(0, 3)$, modeling epistemic ranges for two independent drivers in an energy systems model. Using the variance-based definitions of Sobol' sensitivity indices grounded in the law of total variance and conditional expectation, derive analytically the first-order Sobol' indices $S_1$ and $S_2$ and the total-effect Sobol' index $S_{T1}$. Then interpret whether any second-order interaction between $X_1$ and $X_2$ is present and explain how this is reflected in the indices.\n\nExpress $S_1$, $S_2$, and $S_{T1}$ as exact fractions with no rounding, and report your final answer in the order $S_1$, $S_2$, $S_{T1}$.",
            "solution": "The problem is valid as it is scientifically grounded in the principles of probability theory and global sensitivity analysis, is well-posed with all necessary information provided, and is stated objectively.\n\nThe model for the scalar output is given by $Y = X_1^2 + X_2$. The inputs $X_1$ and $X_2$ are independent random variables with specified uniform distributions: $X_1 \\sim \\mathcal{U}(-1, 2)$ and $X_2 \\sim \\mathcal{U}(0, 3)$.\n\nThe first-order Sobol' index, $S_i$, is defined as the fraction of variance in the output $Y$ that is attributable to the input $X_i$ alone:\n$$S_i = \\frac{V_i}{V(Y)} = \\frac{V(E(Y|X_i))}{V(Y)}$$\nThe total-effect Sobol' index, $S_{Ti}$, is the fraction of variance in $Y$ attributable to $X_i$, including its main effect and all of its interactions with other inputs:\n$$S_{Ti} = \\frac{E(V(Y|X_{\\sim i}))}{V(Y)}$$\nwhere $X_{\\sim i}$ denotes the set of all input variables except $X_i$. For a two-variable model, the variance decomposition is $V(Y) = V_1 + V_2 + V_{12}$, which implies $1 = S_1 + S_2 + S_{12}$. The total-effect indices are $S_{T1} = S_1 + S_{12}$ and $S_{T2} = S_2 + S_{12}$.\n\nFirst, we compute the necessary moments and variances of the input variables. For a general uniform distribution $\\mathcal{U}(a, b)$, the mean is $E(X) = \\frac{a+b}{2}$ and the variance is $V(X) = \\frac{(b-a)^2}{12}$.\n\nFor $X_1 \\sim \\mathcal{U}(-1, 2)$:\nThe probability density function is $p_1(x_1) = \\frac{1}{2 - (-1)} = \\frac{1}{3}$ for $x_1 \\in [-1, 2]$.\nThe mean is $E(X_1) = \\frac{-1+2}{2} = \\frac{1}{2}$.\nThe variance is $V(X_1) = \\frac{(2-(-1))^2}{12} = \\frac{3^2}{12} = \\frac{9}{12} = \\frac{3}{4}$.\nWe require moments of $X_1^2$.\n$E(X_1^2) = \\int_{-1}^{2} x_1^2 p_1(x_1) dx_1 = \\int_{-1}^{2} x_1^2 \\frac{1}{3} dx_1 = \\frac{1}{3} \\left[\\frac{x_1^3}{3}\\right]_{-1}^{2} = \\frac{1}{9} (2^3 - (-1)^3) = \\frac{1}{9}(8+1) = 1$.\n$E(X_1^4) = \\int_{-1}^{2} x_1^4 \\frac{1}{3} dx_1 = \\frac{1}{3} \\left[\\frac{x_1^5}{5}\\right]_{-1}^{2} = \\frac{1}{15} (2^5 - (-1)^5) = \\frac{1}{15}(32+1) = \\frac{33}{15} = \\frac{11}{5}$.\nThe variance of $X_1^2$ is $V(X_1^2) = E(X_1^4) - (E(X_1^2))^2 = \\frac{11}{5} - 1^2 = \\frac{6}{5}$.\n\nFor $X_2 \\sim \\mathcal{U}(0, 3)$:\nThe probability density function is $p_2(x_2) = \\frac{1}{3-0} = \\frac{1}{3}$ for $x_2 \\in [0, 3]$.\nThe mean is $E(X_2) = \\frac{0+3}{2} = \\frac{3}{2}$.\nThe variance is $V(X_2) = \\frac{(3-0)^2}{12} = \\frac{9}{12} = \\frac{3}{4}$.\n\nNext, we calculate the total variance of the output, $V(Y)$. Since $X_1$ and $X_2$ are independent, $X_1^2$ and $X_2$ are also independent. Thus, the variance of their sum is the sum of their variances:\n$V(Y) = V(X_1^2 + X_2) = V(X_1^2) + V(X_2) = \\frac{6}{5} + \\frac{3}{4} = \\frac{24 + 15}{20} = \\frac{39}{20}$.\n\nNow we compute the terms needed for the first-order indices.\nFor $S_1$, we need $V_1 = V(E(Y|X_1))$.\n$E(Y|X_1) = E(X_1^2 + X_2 | X_1) = E(X_1^2|X_1) + E(X_2|X_1)$. Due to independence, $E(X_2|X_1) = E(X_2)$.\n$E(Y|X_1) = X_1^2 + E(X_2) = X_1^2 + \\frac{3}{2}$.\n$V_1 = V(X_1^2 + \\frac{3}{2}) = V(X_1^2) = \\frac{6}{5}$.\nSo, $S_1 = \\frac{V_1}{V(Y)} = \\frac{6/5}{39/20} = \\frac{6}{5} \\times \\frac{20}{39} = \\frac{120}{195} = \\frac{24}{39} = \\frac{8}{13}$.\n\nFor $S_2$, we need $V_2 = V(E(Y|X_2))$.\n$E(Y|X_2) = E(X_1^2 + X_2 | X_2) = E(X_1^2|X_2) + E(X_2|X_2)$. Due to independence, $E(X_1^2|X_2) = E(X_1^2)$.\n$E(Y|X_2) = E(X_1^2) + X_2 = 1 + X_2$.\n$V_2 = V(1 + X_2) = V(X_2) = \\frac{3}{4}$.\nSo, $S_2 = \\frac{V_2}{V(Y)} = \\frac{3/4}{39/20} = \\frac{3}{4} \\times \\frac{20}{39} = \\frac{60}{156} = \\frac{15}{39} = \\frac{5}{13}$.\n\nTo interpret the interaction, we sum the first-order indices: $S_1 + S_2 = \\frac{8}{13} + \\frac{5}{13} = \\frac{13}{13} = 1$.\nThe relation $S_1 + S_2 + S_{12} = 1$ implies that the second-order interaction index $S_{12} = 0$. This result is expected because the model $Y = X_1^2 + X_2$ is additive, meaning it is a sum of functions of individual inputs, $Y = f_1(X_1) + f_2(X_2)$. Additive models have no interactions by definition, so the variance of the output is fully explained by the first-order effects.\n\nFinally, we compute the total-effect index $S_{T1}$. By definition, $S_{T1} = S_1 + S_{12}$.\nSince $S_{12} = 0$, it follows that $S_{T1} = S_1$.\nTherefore, $S_{T1} = \\frac{8}{13}$.\nThis can be confirmed using the alternative definition. $S_{T1} = E(V(Y|X_2)) / V(Y)$.\n$V(Y|X_2) = V(X_1^2+X_2 | X_2) = V(X_1^2) = \\frac{6}{5}$. Since this is a constant, its expectation is $E(V(Y|X_2)) = \\frac{6}{5}$.\nThen $S_{T1} = \\frac{6/5}{39/20} = \\frac{8}{13}$, confirming the result.\n\nThe requested indices are $S_1 = \\frac{8}{13}$, $S_2 = \\frac{5}{13}$, and $S_{T1} = \\frac{8}{13}$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{8}{13} & \\frac{5}{13} & \\frac{8}{13}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "In practical energy systems analysis, not all uncertainties are alike; it is crucial to distinguish between inherent randomness (aleatory uncertainty) and reducible lack of knowledge (epistemic uncertainty). This exercise applies the law of total variance, the same mathematical principle behind Sobol' indices, to decompose the total output variance of a power system model into these two distinct components . This skill is vital for prioritizing research efforts and making robust decisions under uncertainty.",
            "id": "4133285",
            "problem": "Consider a single-zone, single-hour thermal power system with two dispatchable generators, where the system operator meets random demand by merit order: the base unit $G_{1}$ is dispatched up to its capacity, and any residual demand is met by the peaker $G_{2}$. Let the one-hour demand $D$ (in megawatts) be an aleatory random variable, and let the generator heat rates be epistemically uncertain parameters. Specifically, assume:\n- The base unit capacity is $C = 1000$ megawatts, and the one-hour demand is $D \\sim \\mathrm{Uniform}(0, 2C)$.\n- The heat rates $H_{1}$ and $H_{2}$ (in million British thermal units per megawatt-hour) are independent and epistemically uncertain with $H_{1} \\sim \\mathcal{N}(\\mu_{1}, \\sigma_{1}^{2})$ and $H_{2} \\sim \\mathcal{N}(\\mu_{2}, \\sigma_{2}^{2})$, where $\\mu_{1} = 9$, $\\sigma_{1} = 0.2$, $\\mu_{2} = 11$, and $\\sigma_{2} = 0.3$.\n- The hourly fuel consumption $F$ (in million British thermal units) is\n$$\nF = H_{1}\\min(D, C) + H_{2}\\max(D - C, 0).\n$$\n\nUsing the Law of Total Variance, decompose the total variance $\\mathrm{Var}(F)$ into its aleatory and epistemic contributions with respect to the epistemic distribution of $(H_{1}, H_{2})$:\n$$\n\\mathrm{Var}(F) = \\mathbb{E}\\left[\\mathrm{Var}(F \\mid H_{1}, H_{2})\\right] + \\mathrm{Var}\\left(\\mathbb{E}[F \\mid H_{1}, H_{2}]\\right).\n$$\nHere, $\\mathbb{E}[\\cdot]$ and $\\mathrm{Var}(\\cdot)$ without conditioning are taken over the epistemic distribution of $(H_{1}, H_{2})$.\n\nDerive expressions from first principles for both contributions and evaluate them numerically in units of $(\\text{million British thermal units})^{2}$. Finally, report the epistemic variance share\n$$\nS_{\\text{epi}} = \\frac{\\mathrm{Var}\\left(\\mathbb{E}[F \\mid H_{1}, H_{2}]\\right)}{\\mathrm{Var}(F)},\n$$\nas a dimensionless number, rounded to four significant figures. Express your final answer as a single number.",
            "solution": "The problem requires the decomposition of the total variance of the hourly fuel consumption, $F$, into its aleatory and epistemic components using the Law of Total Variance. The total variance $\\mathrm{Var}(F)$ is taken with respect to all uncertain quantities, which are the aleatory demand $D$ and the epistemic heat rates $H_1$ and $H_2$. The decomposition is given by:\n$$\n\\mathrm{Var}(F) = \\mathbb{E}_{H}[\\mathrm{Var}_{D}(F \\mid H_1, H_2)] + \\mathrm{Var}_{H}(\\mathbb{E}_{D}[F \\mid H_1, H_2])\n$$\nHere, $\\mathbb{E}_{D}[\\cdot]$ and $\\mathrm{Var}_{D}(\\cdot)$ denote expectation and variance with respect to the distribution of demand $D$, conditional on the values of $H_1$ and $H_2$. The operators $\\mathbb{E}_{H}[\\cdot]$ and $\\mathrm{Var}_{H}(\\cdot)$ denote expectation and variance with respect to the joint epistemic distribution of $(H_1, H_2)$. The first term, $\\mathbb{E}_{H}[\\mathrm{Var}_{D}(F \\mid H_1, H_2)]$, is the aleatory variance contribution, representing the average variance due to demand variability. The second term, $\\mathrm{Var}_{H}(\\mathbb{E}_{D}[F \\mid H_1, H_2])$, is the epistemic variance contribution, representing the variance due to uncertainty in the mean fuel consumption.\n\nThe fuel consumption $F$ is defined as $F = H_1 \\min(D, C) + H_2 \\max(D - C, 0)$. Given the demand $D \\sim \\mathrm{Uniform}(0, 2C)$, we can write $F$ as a piecewise function of $D$:\n$$\nF(D) = \\begin{cases} H_1 D & \\text{if } 0 \\le D \\le C \\\\ H_1 C + H_2 (D - C) & \\text{if } C < D \\le 2C \\end{cases}\n$$\nThe probability density function of $D$ is $f_D(d) = \\frac{1}{2C}$ for $d \\in [0, 2C]$ and $0$ otherwise.\n\nFirst, we compute the inner expectation, $\\mathbb{E}_{D}[F \\mid H_1, H_2]$, treating $H_1$ and $H_2$ as fixed parameters.\n$$\n\\mathbb{E}_{D}[F \\mid H_1, H_2] = \\int_{0}^{2C} F(d) f_D(d) \\,dd = \\frac{1}{2C} \\left[ \\int_{0}^{C} H_1 d \\,dd + \\int_{C}^{2C} (H_1 C + H_2 (d - C)) \\,dd \\right]\n$$\nThe first integral is:\n$$\n\\frac{1}{2C} \\int_{0}^{C} H_1 d \\,dd = \\frac{H_1}{2C} \\left[ \\frac{d^2}{2} \\right]_{0}^{C} = \\frac{H_1 C}{4}\n$$\nThe second integral is:\n$$\n\\frac{1}{2C} \\int_{C}^{2C} (H_1 C + H_2 d - H_2 C) \\,dd = \\frac{1}{2C} \\left[ (H_1-H_2)Cd + H_2 \\frac{d^2}{2} \\right]_{C}^{2C}\n$$\n$$\n= \\frac{1}{2C} \\left[ (H_1-H_2)C(2C-C) + \\frac{H_2}{2}((2C)^2 - C^2) \\right] = \\frac{1}{2C} \\left[ (H_1-H_2)C^2 + \\frac{3}{2}H_2 C^2 \\right] = \\frac{C}{2} \\left[ H_1 - H_2 + \\frac{3}{2}H_2 \\right] = \\frac{C}{2} \\left( H_1 + \\frac{1}{2}H_2 \\right) = \\frac{H_1 C}{2} + \\frac{H_2 C}{4}\n$$\nSumming the results from both integrals:\n$$\n\\mathbb{E}_{D}[F \\mid H_1, H_2] = \\frac{H_1 C}{4} + \\frac{H_1 C}{2} + \\frac{H_2 C}{4} = \\frac{3}{4}C H_1 + \\frac{1}{4}C H_2 = \\frac{C}{4}(3H_1 + H_2)\n$$\n\nNow we can compute the epistemic variance contribution, $\\mathrm{Var}_{H}(\\mathbb{E}_{D}[F \\mid H_1, H_2])$. Since $H_1$ and $H_2$ are independent, $\\mathrm{Cov}(H_1, H_2) = 0$.\n$$\nV_{\\text{epi}} = \\mathrm{Var}_{H}\\left(\\frac{C}{4}(3H_1 + H_2)\\right) = \\left(\\frac{C}{4}\\right)^2 \\mathrm{Var}_{H}(3H_1 + H_2) = \\frac{C^2}{16} (3^2 \\mathrm{Var}_H(H_1) + \\mathrm{Var}_H(H_2)) = \\frac{C^2}{16}(9\\sigma_1^2 + \\sigma_2^2)\n$$\nSubstituting numerical values: $C=1000$, $\\sigma_1 = 0.2$, $\\sigma_2 = 0.3$.\n$$\nV_{\\text{epi}} = \\frac{1000^2}{16}(9(0.2)^2 + (0.3)^2) = \\frac{10^6}{16}(9(0.04) + 0.09) = \\frac{10^6}{16}(0.36 + 0.09) = \\frac{10^6 \\times 0.45}{16} = 28125 \\, (\\text{MMBtu})^2\n$$\n\nNext, we compute the conditional variance, $\\mathrm{Var}_{D}(F \\mid H_1, H_2) = \\mathbb{E}_{D}[F^2 \\mid H_1, H_2] - (\\mathbb{E}_{D}[F \\mid H_1, H_2])^2$.\nFirst, we find $\\mathbb{E}_{D}[F^2 \\mid H_1, H_2]$:\n$$\n\\mathbb{E}_{D}[F^2 \\mid H_1, H_2] = \\frac{1}{2C} \\left[ \\int_{0}^{C} (H_1 d)^2 \\,dd + \\int_{C}^{2C} (H_1 C + H_2 (d - C))^2 \\,dd \\right]\n$$\nThe first integral is:\n$$\n\\frac{H_1^2}{2C} \\int_{0}^{C} d^2 \\,dd = \\frac{H_1^2}{2C} \\left[ \\frac{d^3}{3} \\right]_{0}^{C} = \\frac{H_1^2 C^2}{6}\n$$\nThe second integral is:\n$$\n\\frac{1}{2C} \\int_{C}^{2C} ((H_1-H_2)C + H_2d)^2 \\,dd = \\frac{C^2}{2}\\left(H_1^2 + H_1 H_2 + \\frac{1}{3}H_2^2\\right)\n$$\n(The detailed expansion and integration is omitted for brevity but was verified).\nSumming the results:\n$$\n\\mathbb{E}_{D}[F^2 \\mid H_1, H_2] = C^2\\left(\\frac{1}{6}H_1^2 + \\frac{1}{2}H_1^2 + \\frac{1}{2}H_1 H_2 + \\frac{1}{6}H_2^2\\right) = C^2\\left(\\frac{2}{3}H_1^2 + \\frac{1}{2}H_1 H_2 + \\frac{1}{6}H_2^2\\right)\n$$\nNow, we compute the conditional variance:\n$$\n\\mathrm{Var}_{D}(F \\mid H_1, H_2) = C^2\\left(\\frac{2}{3}H_1^2 + \\frac{1}{2}H_1 H_2 + \\frac{1}{6}H_2^2\\right) - \\left(\\frac{C}{4}(3H_1 + H_2)\\right)^2\n$$\n$$\n= C^2\\left(\\frac{2}{3}H_1^2 + \\frac{1}{2}H_1 H_2 + \\frac{1}{6}H_2^2 - \\frac{1}{16}(9H_1^2 + 6H_1H_2 + H_2^2)\\right)\n$$\n$$\n= C^2\\left[ \\left(\\frac{2}{3}-\\frac{9}{16}\\right)H_1^2 + \\left(\\frac{1}{2}-\\frac{6}{16}\\right)H_1H_2 + \\left(\\frac{1}{6}-\\frac{1}{16}\\right)H_2^2 \\right]\n$$\n$$\n= C^2\\left[ \\left(\\frac{32-27}{48}\\right)H_1^2 + \\left(\\frac{8-6}{16}\\right)H_1H_2 + \\left(\\frac{8-3}{48}\\right)H_2^2 \\right] = \\frac{C^2}{48}(5H_1^2 + 6H_1H_2 + 5H_2^2)\n$$\n\nNow we can compute the aleatory variance contribution, $\\mathbb{E}_{H}[\\mathrm{Var}_{D}(F \\mid H_1, H_2)]$. We use $\\mathbb{E}[H_i^2] = \\mu_i^2 + \\sigma_i^2$ for $i=1, 2$ and $\\mathbb{E}[H_1 H_2]=\\mu_1 \\mu_2$.\n$$\nV_{\\text{ale}} = \\mathbb{E}_{H}\\left[\\frac{C^2}{48}(5H_1^2 + 6H_1H_2 + 5H_2^2)\\right] = \\frac{C^2}{48}\\left(5\\mathbb{E}[H_1^2] + 6\\mathbb{E}[H_1H_2] + 5\\mathbb{E}[H_2^2]\\right)\n$$\n$$\nV_{\\text{ale}} = \\frac{C^2}{48}\\left(5(\\mu_1^2 + \\sigma_1^2) + 6\\mu_1\\mu_2 + 5(\\mu_2^2 + \\sigma_2^2)\\right)\n$$\nSubstituting numerical values: $\\mu_1=9$, $\\sigma_1^2=0.04$, $\\mu_2=11$, $\\sigma_2^2=0.09$.\n$$\nV_{\\text{ale}} = \\frac{1000^2}{48}\\left(5(9^2 + 0.04) + 6(9)(11) + 5(11^2 + 0.09)\\right)\n$$\n$$\n= \\frac{10^6}{48}\\left(5(81.04) + 594 + 5(121.09)\\right) = \\frac{10^6}{48}(405.2 + 594 + 605.45) = \\frac{10^6 \\times 1604.65}{48} \\approx 33430208.33 \\, (\\text{MMBtu})^2\n$$\n\nFinally, we calculate the total variance $\\mathrm{Var}(F) = V_{\\text{ale}} + V_{\\text{epi}}$ and the epistemic variance share $S_{\\text{epi}}$.\n$$\n\\mathrm{Var}(F) = 33430208.33 + 28125 = 33458333.33 \\, (\\text{MMBtu})^2\n$$\n$$\nS_{\\text{epi}} = \\frac{V_{\\text{epi}}}{\\mathrm{Var}(F)} = \\frac{28125}{33458333.33} \\approx 0.0008405977...\n$$\nAlternatively, using the symbolic expressions:\n$$\nS_{\\text{epi}} = \\frac{\\frac{C^2}{16}(9\\sigma_1^2 + \\sigma_2^2)}{\\frac{C^2}{48}\\left(5(\\mu_1^2 + \\sigma_1^2) + 6\\mu_1\\mu_2 + 5(\\mu_2^2 + \\sigma_2^2)\\right) + \\frac{C^2}{16}(9\\sigma_1^2 + \\sigma_2^2)}\n$$\n$$\nS_{\\text{epi}} = \\frac{3(9\\sigma_1^2 + \\sigma_2^2)}{5(\\mu_1^2 + \\sigma_1^2) + 6\\mu_1\\mu_2 + 5(\\mu_2^2 + \\sigma_2^2) + 3(9\\sigma_1^2 + \\sigma_2^2)}\n$$\n$$\n= \\frac{27\\sigma_1^2 + 3\\sigma_2^2}{5\\mu_1^2 + 6\\mu_1\\mu_2 + 5\\mu_2^2 + 32\\sigma_1^2 + 8\\sigma_2^2}\n$$\nPlugging in the parameters:\n$$\nS_{\\text{epi}} = \\frac{27(0.04) + 3(0.09)}{5(9^2) + 6(9)(11) + 5(11^2) + 32(0.04) + 8(0.09)}\n$$\n$$\n= \\frac{1.08 + 0.27}{5(81) + 594 + 5(121) + 1.28 + 0.72} = \\frac{1.35}{405 + 594 + 605 + 1.28 + 0.72} = \\frac{1.35}{1604 + 2} = \\frac{1.35}{1606} \\approx 0.000840597758\n$$\nRounding to four significant figures, we get $S_{\\text{epi}} \\approx 0.0008406$.\n\nSo the epistemic and aleatory variance contributions are $V_{\\text{epi}} = 28125 \\, (\\text{MMBtu})^2$ and $V_{\\text{ale}} \\approx 3.343 \\times 10^7 \\, (\\text{MMBtu})^2$. The epistemic variance share is extremely small, indicating that the variability in fuel consumption is dominated by the random fluctuations in demand rather than by the uncertainty in the generators' heat rates.",
            "answer": "$$\\boxed{0.0008406}$$"
        },
        {
            "introduction": "While analytical calculations are invaluable for building understanding, most real-world models are too complex for them, necessitating numerical methods. This capstone practice guides you through the process of building and validating a complete computational Global Sensitivity Analysis (GSA) pipeline using Monte Carlo methods . By calibrating a well-known benchmark function and comparing numerical estimates to analytical truths, you will develop the core skills required to apply GSA to complex energy systems models with confidence.",
            "id": "4133362",
            "problem": "Consider the benchmark Ishigami function, which is widely used for uncertainty quantification and global sensitivity analysis to stress-test nonlinear response surfaces. Let the uncertain inputs be $x_1$, $x_2$, and $x_3$, each independently and identically distributed as uniform random variables on $[-\\pi,\\pi]$. Define the model output $f$ by\n$$\nf(x_1,x_2,x_3;a,b) \\equiv \\sin(x_1) + a \\sin^2(x_2) + b\\, x_3^4 \\sin(x_1),\n$$\nwhere $a$ and $b$ are real-valued parameters to be calibrated. In an energy systems modeling context, interpret $x_1$ as a dispatchable flexibility proxy, $x_2$ as variable renewable resource variability, and $x_3$ as policy-driven structural change; the model output $f$ is a dimensionless reliability surrogate. Your tasks are:\n\n1. Starting from the fundamental definitions of expectation and variance, and the variance-based sensitivity decomposition underlying Sobol indices, derive a calibration rule to set parameters $a$ and $b$ so that:\n   - The unconditional mean satisfies $\\mathbb{E}[f] = m_{\\text{target}}$ for a given target $m_{\\text{target}}$ (dimensionless).\n   - The ratio of first-order sensitivities satisfies $R_{\\text{target}} = S_2 / S_1$, where $S_i$ denotes the first-order Sobol index for input $x_i$.\n\n   The calibration must be expressed in closed form using only $m_{\\text{target}}$ and $R_{\\text{target}}$ and constants derived from the input distributions on $[-\\pi,\\pi]$.\n\n2. Implement a Monte Carlo Global Sensitivity Analysis (GSA) pipeline using the Saltelli sampling scheme (first-order estimator) and the Jansen estimator (total-order estimator) to estimate the first-order indices $S_1$, $S_2$, $S_3$ and total-effect indices $T_1$, $T_2$, $T_3$. The pipeline must:\n   - Use independent base samples of size $N$ for matrices $A$ and $B$ in dimension $d=3$, with all coordinates sampled uniformly from $[-\\pi,\\pi]$.\n   - Construct mixed matrices $A_{B}^{(i)}$ by replacing the $i$-th column of $A$ with the $i$-th column of $B$ for $i \\in \\{1,2,3\\}$.\n   - Estimate first-order indices using the Saltelli estimator and total-effect indices using the Jansen estimator.\n   - Use an absolute error tolerance $\\epsilon = 3 \\times 10^{-2}$ to validate the pipeline against analytically derived indices: the pipeline is deemed valid for a test case if the absolute error between estimated and analytical indices is at most $\\epsilon$ for all three first-order indices and all three total-effect indices.\n\n3. For numerical reproducibility, fix the random seeds specified in the test suite below.\n\nAll outputs are dimensionless; no physical units are required. All angles are in radians. Your program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The result for each test case must be a list containing, in this exact order, the calibrated parameters and the estimated indices and validation flags: $[a,b,S_1,S_2,S_3,T_1,T_2,T_3,\\text{pass\\_first\\_order},\\text{pass\\_total}]$. Each entry must be a boolean, an integer, a float, or a list of these types.\n\nUse the following test suite, which exercises a standard case, a moderate case, a high $R_{\\text{target}}$ case, and a small-sample stress case:\n\n- Test case $1$: $m_{\\text{target}} = 3.5$, $R_{\\text{target}} = 1.409$, $N = 30000$, $\\text{seed} = 42$.\n- Test case $2$: $m_{\\text{target}} = 2.0$, $R_{\\text{target}} = 0.5$, $N = 15000$, $\\text{seed} = 123$.\n- Test case $3$: $m_{\\text{target}} = 1.0$, $R_{\\text{target}} = 4.0$, $N = 15000$, $\\text{seed} = 456$.\n- Test case $4$: $m_{\\text{target}} = 2.0$, $R_{\\text{target}} = 0.01$, $N = 4000$, $\\text{seed} = 789$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $[[\\dots],[\\dots],[\\dots],[\\dots]]$.",
            "solution": "The problem statement has been evaluated and is deemed valid. It is scientifically grounded, well-posed, and contains all necessary information to derive a unique, verifiable solution.\n\nThe problem requires a three-part solution: first, the derivation of a calibration rule for the parameters of the Ishigami function; second, the derivation of the analytical Sobol indices for validation; and third, the implementation of a Monte Carlo Global Sensitivity Analysis (GSA) pipeline to estimate these indices numerically.\n\n### 1. Analytical Derivation of the Calibration Rule\n\nThe Ishigami function is given by:\n$$\nf(x_1,x_2,x_3;a,b) = \\sin(x_1) + a \\sin^2(x_2) + b x_3^4 \\sin(x_1)\n$$\nThe inputs $x_1, x_2, x_3$ are independent and identically distributed random variables from a uniform distribution on $[-\\pi, \\pi]$, i.e., $x_i \\sim U[-\\pi, \\pi]$. The probability density function is $p(x) = \\frac{1}{2\\pi}$ for $x \\in [-\\pi, \\pi]$.\n\nWe first compute the expectations of the elementary functions involved, which will be used throughout the derivation:\n$$\n\\mathbb{E}[\\sin(x_i)] = \\frac{1}{2\\pi} \\int_{-\\pi}^{\\pi} \\sin(x) dx = 0\n$$\n$$\n\\mathbb{E}[\\sin^2(x_i)] = \\frac{1}{2\\pi} \\int_{-\\pi}^{\\pi} \\sin^2(x) dx = \\frac{1}{2\\pi} \\int_{-\\pi}^{\\pi} \\frac{1 - \\cos(2x)}{2} dx = \\frac{1}{2}\n$$\n$$\n\\mathbb{E}[x_3^4] = \\frac{1}{2\\pi} \\int_{-\\pi}^{\\pi} x^4 dx = \\frac{1}{2\\pi} \\left[ \\frac{x^5}{5} \\right]_{-\\pi}^{\\pi} = \\frac{1}{10\\pi}(\\pi^5 - (-\\pi)^5) = \\frac{\\pi^4}{5}\n$$\n\n**1.1. Unconditional Mean Condition**\n\nThe first calibration condition is $\\mathbb{E}[f] = m_{\\text{target}}$. We compute the unconditional mean of $f$ by applying the linearity of expectation:\n$$\n\\mathbb{E}[f] = \\mathbb{E}[\\sin(x_1)] + a\\mathbb{E}[\\sin^2(x_2)] + b\\mathbb{E}[x_3^4 \\sin(x_1)]\n$$\nDue to the independence of the inputs, the expectation of the product term is the product of expectations:\n$$\n\\mathbb{E}[x_3^4 \\sin(x_1)] = \\mathbb{E}[x_3^4] \\mathbb{E}[\\sin(x_1)] = \\left(\\frac{\\pi^4}{5}\\right)(0) = 0\n$$\nSubstituting the known expectations, we get:\n$$\n\\mathbb{E}[f] = 0 + a\\left(\\frac{1}{2}\\right) + 0 = \\frac{a}{2}\n$$\nEquating this to the target mean gives the first part of our rule:\n$$\n\\frac{a}{2} = m_{\\text{target}} \\implies a = 2 m_{\\text{target}}\n$$\n\n**1.2. Sensitivity Ratio Condition**\n\nThe second condition is $R_{\\text{target}} = S_2 / S_1$, where $S_i$ is the first-order Sobol index for input $x_i$. The index $S_i$ is defined as $S_i = D_i/D$, where $D = \\text{Var}(f)$ is the total variance and $D_i = \\text{Var}(\\mathbb{E}[f|x_i])$ is the first-order partial variance of $x_i$. The ratio simplifies to $R_{\\text{target}} = D_2/D_1$.\n\nWe must compute the conditional expectations $\\mathbb{E}[f|x_i]$:\nFor $x_1$:\n$$\n\\mathbb{E}[f|x_1] = \\mathbb{E}[\\sin(x_1) + a \\sin^2(x_2) + b x_3^4 \\sin(x_1) | x_1] = \\sin(x_1) + a\\mathbb{E}[\\sin^2(x_2)] + b\\mathbb{E}[x_3^4]\\sin(x_1)\n$$\n$$\n\\mathbb{E}[f|x_1] = \\sin(x_1) + \\frac{a}{2} + b\\frac{\\pi^4}{5}\\sin(x_1) = \\left(1 + b\\frac{\\pi^4}{5}\\right)\\sin(x_1) + \\frac{a}{2}\n$$\nThe partial variance $D_1$ is the variance of this expression:\n$$\nD_1 = \\text{Var}(\\mathbb{E}[f|x_1]) = \\text{Var}\\left(\\left(1 + b\\frac{\\pi^4}{5}\\right)\\sin(x_1) + \\frac{a}{2}\\right) = \\left(1 + b\\frac{\\pi^4}{5}\\right)^2 \\text{Var}(\\sin(x_1))\n$$\nSince $\\mathbb{E}[\\sin(x_1)] = 0$, $\\text{Var}(\\sin(x_1)) = \\mathbb{E}[\\sin^2(x_1)] - (\\mathbb{E}[\\sin(x_1)])^2 = 1/2$.\n$$\nD_1 = \\frac{1}{2}\\left(1 + b\\frac{\\pi^4}{5}\\right)^2\n$$\n\nFor $x_2$:\n$$\n\\mathbb{E}[f|x_2] = \\mathbb{E}[\\sin(x_1) + a \\sin^2(x_2) + b x_3^4 \\sin(x_1) | x_2] = \\mathbb{E}[\\sin(x_1)] + a\\sin^2(x_2) + b\\mathbb{E}[x_3^4]\\mathbb{E}[\\sin(x_1)]\n$$\n$$\n\\mathbb{E}[f|x_2] = 0 + a\\sin^2(x_2) + 0 = a\\sin^2(x_2)\n$$\nThe partial variance $D_2$ is:\n$$\nD_2 = \\text{Var}(a\\sin^2(x_2)) = a^2 \\text{Var}(\\sin^2(x_2)) = a^2 (\\mathbb{E}[\\sin^4(x_2)] - (\\mathbb{E}[\\sin^2(x_2)])^2)\n$$\nWe need $\\mathbb{E}[\\sin^4(x_2)] = \\frac{1}{2\\pi}\\int_{-\\pi}^{\\pi}\\sin^4(x)dx = \\frac{3}{8}$.\n$$\nD_2 = a^2 \\left(\\frac{3}{8} - \\left(\\frac{1}{2}\\right)^2\\right) = a^2 \\left(\\frac{3}{8} - \\frac{1}{4}\\right) = \\frac{a^2}{8}\n$$\n\nNow we can form the ratio $D_2/D_1$:\n$$\nR_{\\text{target}} = \\frac{D_2}{D_1} = \\frac{a^2/8}{\\frac{1}{2}(1 + b\\frac{\\pi^4}{5})^2} = \\frac{a^2}{4(1 + b\\frac{\\pi^4}{5})^2}\n$$\nSubstituting $a = 2 m_{\\text{target}}$:\n$$\nR_{\\text{target}} = \\frac{(2 m_{\\text{target}})^2}{4(1 + b\\frac{\\pi^4}{5})^2} = \\frac{m_{\\text{target}}^2}{(1 + b\\frac{\\pi^4}{5})^2}\n$$\nSolving for $b$, and assuming the positive root for a conventional solution (as the test cases with positive $m_{\\text{target}}$ imply):\n$$\n1 + b\\frac{\\pi^4}{5} = \\frac{m_{\\text{target}}}{\\sqrt{R_{\\text{target}}}} \\implies b = \\frac{5}{\\pi^4}\\left(\\frac{m_{\\text{target}}}{\\sqrt{R_{\\text{target}}}} - 1\\right)\n$$\nThis completes the calibration rule.\n\n### 2. Analytical Sobol Indices for Validation\n\nTo validate the numerical pipeline, we must derive the analytical expressions for all first-order and total-effect indices. This requires computing the total variance $D$. We use the ANOVA decomposition $D = \\sum_i D_i + \\sum_{i<j} D_{ij} + \\dots$.\n\nPartial variances already computed:\n$D_1 = \\frac{1}{2}\\left(1 + b\\frac{\\pi^4}{5}\\right)^2$\n$D_2 = \\frac{a^2}{8}$\n$D_3 = \\text{Var}(\\mathbb{E}[f|x_3]) = \\text{Var}(\\frac{a}{2}) = 0$.\n\nWe check for interaction terms. The only one is between $x_1$ and $x_3$.\n$D_{13} = \\text{Var}(\\mathbb{E}[f|x_1, x_3] - \\mathbb{E}[f|x_1] - \\mathbb{E}[f|x_3] + \\mathbb{E}[f])$.\nThe term is $f_{13} = b(x_3^4 - \\mathbb{E}[x_3^4])\\sin(x_1) = b(x_3^4 - \\frac{\\pi^4}{5})\\sin(x_1)$.\n$D_{13} = \\text{Var}(f_{13}) = b^2 \\text{Var}((x_3^4 - \\frac{\\pi^4}{5})\\sin(x_1))$. Since $x_1$ and $x_3$ are independent, and $\\mathbb{E}[\\sin(x_1)]=0$, this variance separates:\n$D_{13} = b^2 \\mathbb{E}[(x_3^4 - \\frac{\\pi^4}{5})^2] \\mathbb{E}[\\sin^2(x_1)] = b^2 \\text{Var}(x_3^4) \\mathbb{E}[\\sin^2(x_1)]$.\n$\\text{Var}(x_3^4) = \\mathbb{E}[x_3^8] - (\\mathbb{E}[x_3^4])^2 = \\frac{\\pi^8}{9} - (\\frac{\\pi^4}{5})^2 = \\pi^8(\\frac{1}{9} - \\frac{1}{25}) = \\frac{16\\pi^8}{225}$.\n$D_{13} = b^2 \\left(\\frac{16\\pi^8}{225}\\right) \\left(\\frac{1}{2}\\right) = \\frac{8b^2\\pi^8}{225}$.\nAll other interaction terms ($D_{12}, D_{23}, D_{123}$) are zero.\n\nThe total variance is $D = D_1 + D_2 + D_3 + D_{13} = \\frac{1}{2}\\left(1 + b\\frac{\\pi^4}{5}\\right)^2 + \\frac{a^2}{8} + \\frac{8b^2\\pi^8}{225}$.\n\nThe first-order indices are:\n$S_1 = D_1/D$, $S_2 = D_2/D$, $S_3 = 0$.\n\nFor total-effect indices $T_i = D_i^T/D$, the numerators $D_i^T$ sum all partial variances involving input $x_i$:\n$D_1^T = D_1 + D_{13} = \\frac{1}{2}\\left(1 + b\\frac{\\pi^4}{5}\\right)^2 + \\frac{8b^2\\pi^8}{225}$.\n$D_2^T = D_2 = \\frac{a^2}{8}$.\n$D_3^T = D_3 + D_{13} = D_{13} = \\frac{8b^2\\pi^8}{225}$.\n\nThe total-effect indices are:\n$T_1 = (D_1+D_{13})/D$, $T_2 = D_2/D = S_2$, $T_3 = D_{13}/D$.\n\nThese formulas allow for the computation of the true indices for any given $a$ and $b$.\n\n### 3. Numerical Estimation via Monte Carlo GSA\n\nThe GSA pipeline is implemented using the Saltelli sampling scheme with $N(d+2)$ total model evaluations, where $d=3$ is the number of inputs.\n\n**3.1. Sampling**\n1.  Generate two independent sample matrices, $A$ and $B$, of size $N \\times 3$, drawing all values from $U[-\\pi, \\pi]$.\n2.  For each input $i \\in \\{1, 2, 3\\}$, construct a \"mixed\" matrix $A_B^{(i)}$ by taking matrix $A$ and replacing its $i$-th column with the $i$-th column of matrix $B$.\n\n**3.2. Model Evaluation**\nThe Ishigami function $f$ is evaluated for each row of the matrices $A$, $B$, and $A_B^{(i)}$ for $i=1,2,3$, yielding five output vectors of length $N$: $y_A, y_B, y_{A_B^{(1)}}, y_{A_B^{(2)}}, y_{A_B^{(3)}}$.\n\n**3.3. Index Estimation**\nThe estimators for the partial and total variances are:\n- The total variance $D$ is estimated from the sample variance of one of the base samples:\n  $$ \\hat{D} = \\text{Var}(y_A) $$\n- The first-order variance $D_i$ is estimated using the Saltelli estimator:\n  $$ \\hat{D}_i = \\frac{1}{N} \\sum_{j=1}^{N} y_B^{(j)} (y_{A_B^{(i)}}^{(j)} - y_A^{(j)}) $$\n- The total-effect variance $D_i^T$ is estimated using the Jansen estimator:\n  $$ \\hat{D}_i^T = \\frac{1}{2N} \\sum_{j=1}^{N} (y_A^{(j)} - y_{A_B^{(i)}}^{(j)})^2 $$\n\nThe estimated Sobol indices are then calculated as $\\hat{S}_i = \\hat{D}_i / \\hat{D}$ and $\\hat{T}_i = \\hat{D}_i^T / \\hat{D}$.\n\n**3.4. Validation**\nFor each test case, the estimated indices $(\\hat{S}_i, \\hat{T}_i)$ are compared to the analytical indices $(S_i, T_i)$ derived in section 2. The pipeline is validated separately for first-order and total-effect indices. The validation passes if the absolute error is less than or equal to the tolerance $\\epsilon = 3 \\times 10^{-2}$ for all three indices in the respective group.\n\nThe final implementation will encapsulate these steps, process the provided test suite, and format the output as specified.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to execute the full GSA workflow for the given test cases.\n    \"\"\"\n    test_cases = [\n        {'m_target': 3.5, 'R_target': 1.409, 'N': 30000, 'seed': 42},\n        {'m_target': 2.0, 'R_target': 0.5, 'N': 15000, 'seed': 123},\n        {'m_target': 1.0, 'R_target': 4.0, 'N': 15000, 'seed': 456},\n        {'m_target': 2.0, 'R_target': 0.01, 'N': 4000, 'seed': 789},\n    ]\n    \n    epsilon = 3e-2\n    all_results = []\n\n    for case in test_cases:\n        m_target = case['m_target']\n        R_target = case['R_target']\n        N = case['N']\n        seed = case['seed']\n\n        # Task 1: Calibrate parameters a and b\n        a = 2 * m_target\n        # Assuming m_target > 0 as per test cases for the sqrt\n        b = (5 / np.pi**4) * (m_target / np.sqrt(R_target) - 1)\n\n        # Pre-compute analytical indices for validation\n        analytical_S, analytical_T = calculate_analytical_indices(a, b)\n\n        # Task 2: Run Monte Carlo GSA pipeline\n        estimated_S, estimated_T = monte_carlo_gsa(a, b, N, seed)\n\n        # Validate the pipeline\n        pass_first_order = np.all(np.abs(estimated_S - analytical_S) <= epsilon)\n        pass_total = np.all(np.abs(estimated_T - analytical_T) <= epsilon)\n\n        # Format results for the current test case\n        case_results = [\n            a, b,\n            estimated_S[0], estimated_S[1], estimated_S[2],\n            estimated_T[0], estimated_T[1], estimated_T[2],\n            pass_first_order, pass_total\n        ]\n        all_results.append(case_results)\n\n    # Print the final result in the exact required format\n    print(all_results)\n    \ndef ishigami_function(X, a, b):\n    \"\"\"\n    Computes the Ishigami function for a matrix of inputs.\n    X: a numpy array of shape (N, 3)\n    a, b: model parameters\n    \"\"\"\n    x1 = X[:, 0]\n    x2 = X[:, 1]\n    x3 = X[:, 2]\n    \n    term1 = np.sin(x1)\n    term2 = a * np.sin(x2)**2\n    term3 = b * x3**4 * np.sin(x1)\n    \n    return term1 + term2 + term3\n\ndef calculate_analytical_indices(a, b):\n    \"\"\"\n    Calculates the analytical first-order and total-effect Sobol indices.\n    \"\"\"\n    pi_4 = np.pi**4\n    pi_8 = np.pi**8\n\n    # Partial variances\n    D1 = 0.5 * (1 + b * pi_4 / 5)**2\n    D2 = a**2 / 8\n    D13 = 8 * b**2 * pi_8 / 225\n\n    # Total variance\n    D_total = D1 + D2 + D13\n\n    if D_total == 0:\n        return np.zeros(3), np.zeros(3)\n\n    # First-order indices\n    S1 = D1 / D_total\n    S2 = D2 / D_total\n    S3 = 0.0\n    \n    # Total-effect indices\n    T1 = (D1 + D13) / D_total\n    T2 = D2 / D_total  # T2 is equal to S2\n    T3 = D13 / D_total\n\n    S_ana = np.array([S1, S2, S3])\n    T_ana = np.array([T1, T2, T3])\n    \n    return S_ana, T_ana\n\ndef monte_carlo_gsa(a, b, N, seed):\n    \"\"\"\n    Performs Monte Carlo GSA using Saltelli sampling.\n    \"\"\"\n    d = 3  # Number of input dimensions\n    rng = np.random.default_rng(seed)\n\n    # Generate Saltelli samples\n    A = rng.uniform(-np.pi, np.pi, size=(N, d))\n    B = rng.uniform(-np.pi, np.pi, size=(N, d))\n\n    # Evaluate the model on base samples\n    y_A = ishigami_function(A, a, b)\n    y_B = ishigami_function(B, a, b)\n\n    S_est = np.zeros(d)\n    T_est = np.zeros(d)\n    \n    # Use sample variance of y_A as the estimator for total variance.\n    # Using ddof=1 for an unbiased estimator.\n    total_variance_est = np.var(y_A, ddof=1)\n    \n    if total_variance_est == 0:\n        return np.zeros(d), np.zeros(d)\n\n    for i in range(d):\n        # Create mixed matrix A_B^(i)\n        A_B_i = A.copy()\n        A_B_i[:, i] = B[:, i]\n\n        # Evaluate the model on the mixed matrix\n        y_A_B_i = ishigami_function(A_B_i, a, b)\n\n        # Estimate first-order variance (Saltelli estimator)\n        first_order_var_est = np.mean(y_B * (y_A_B_i - y_A))\n        \n        # Estimate total-effect variance (Jansen estimator)\n        total_effect_var_est = 0.5 * np.mean((y_A - y_A_B_i)**2)\n\n        S_est[i] = first_order_var_est / total_variance_est\n        T_est[i] = total_effect_var_est / total_variance_est\n        \n    return S_est, T_est\n\nif __name__ == '__main__':\n    solve()\n```"
        }
    ]
}