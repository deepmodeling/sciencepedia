## 引言
在现代能源系统中，可再生能源的[间歇性](@entry_id:275330)、市场价格的波动性以及负荷需求的不确定性给规划和运营带来了前所未有的挑战。随机优化模型是应对这些不确定性、做出[稳健决策](@entry_id:184609)的关键工具。然而，这些模型面临一个核心的内在矛盾：一方面，我们需要高保真度的[不确定性表示](@entry_id:1133583)来确保决策的可靠性；另一方面，复杂的[不确定性表示](@entry_id:1133583)会导致模型规模急剧膨胀，使其在计算上难以求解。场景生成与削减技术正是为解决这一矛盾而生。

本文将系统性地引导您掌握这一关键领域。首先，在“原理与机制”一章中，我们将深入探讨支撑场景生成与削减的[概率论基础](@entry_id:158925)，介绍从蒙特卡洛抽样、[Copula理论](@entry_id:142319)到场景质量评估和削减算法的核心技术。接着，在“应用与跨学科连接”一章中，我们将展示这些理论如何在[电力系统运行](@entry_id:1130078)、长期投资规划、风险管理以及气候科学等多个领域发挥作用，连接理论与实践。最后，“动手实践”部分将提供一系列精心设计的练习，帮助您将所学知识付诸实践，巩固对整个流程的理解。通过本文的学习，您将能够构建和运用场景分析方法，以在复杂的不确定性环境中进行严谨的建模与决策。

## 原理与机制

在[能源系统规划](@entry_id:1124498)与运行中，我们必须面对各种不确定性，如可再生能源发电量、[电力](@entry_id:264587)需求和燃料价格的波动。为了在这些不确定性下做出稳健的决策，随机优化模型成为一种不可或缺的工具。这些模型的核心挑战在于如何有效、准确地表示和处理不确定性。场景的生成与削减技术正是为应对这一挑战而生。本章将深入探讨支撑这些技术的核心原理与机制，从不确定性的基本概念出发，系统地介绍场景的生成方法、质量评估度量以及在保真度与计算可行性之间取得平衡的削减策略。

### 场景的[概率论基础](@entry_id:158925)

在深入探讨技术细节之前，我们必须首先建立一个坚实的概率论框架来理解不确定性。

#### 随机不确定性与认知不确定性

在[能源系统建模](@entry_id:1124496)中，我们遇到的不确定性通常可以分为两类：**随机不确定性 (aleatory uncertainty)** 和 **认知不确定性 (epistemic uncertainty)**。

**随机不确定性** 是系统固有的、内在的随机性。即使我们拥有了完美的模型和参数，未来的风速、光照强度或负荷需求仍然是不可预测的。例如，假设我们用一个特定参数 $\theta$ 的威布尔 (Weibull) 分布 $f(v | \theta)$ 来描述风速 $V_t$ 的变化，那么即使 $\theta$ 已知，从该分布中抽取的具体风速值 $V_t$ 依然是随机的。这种不确定性源于自然现象的内在变异性，通常无法通过收集更多数据来消除。

**认知不确定性** 则源于我们知识的局限性。这包括对模型结构或模型参数的不确定性。在我们之前的例子中，我们可能不知道参数 $\theta$ 的确切值，或者甚至不确定[威布尔分布](@entry_id:270143)是否是描述风速的最佳模型。这种不确定性是由于数据有限、测量误差或模型简化造成的。原则上，通过收集更多的数据或改进模型，认知不确定性是可以被减小的。

在实际建模中，区分这两种不确定性至关重要，因为它们需要不同的处理方法。对于随机不确定性，我们通过从一个给定的概率分布中抽样来生成场景。而对于认知不确定性，我们可以采用更复杂的方法，例如：
1.  **贝叶斯方法 (Bayesian methods)**：将模型参数 $\theta$ 视为[随机变量](@entry_id:195330)，并基于历史数据 $\mathcal{D}$ 计算其后验分布 $p(\theta | \mathcal{D})$。生成场景时，首先从后验分布中抽取一组参数 $\theta^*$，然后再从[条件分布](@entry_id:138367) $f(v | \theta^*)$ 中抽取[随机变量](@entry_id:195330)的值。最终的预测分布是通过对所有可能的参数进行积分得到的[后验预测分布](@entry_id:167931) $p(v | \mathcal{D}) = \int f(v | \theta) p(\theta | \mathcal{D}) d\theta$。这种方法系统地融合了两种不确定性，通常会导致预测分布具有更重的尾部，这对[风险评估](@entry_id:170894)（如使用风险价值 [VaR](@entry_id:140792) 或[条件风险价值](@entry_id:163580) CVaR）尤为重要 。
2.  **[分布鲁棒优化](@entry_id:636272) (Distributionally robust optimization, DRO)**：构建一个包含所有“合理”概率分布的“[模糊集](@entry_id:269080)” $\mathcal{P}$，而不是依赖单一的[经验分布](@entry_id:274074)。然后，模型的目标是寻找一个在最坏情况分布下表现最佳的决策。这种方法可以有效[对冲](@entry_id:635975)模型错误设定的风险，通常会产生比基于单一分布的优化更为保守和稳健的决策 。

#### 场景、[概率空间](@entry_id:201477)与信息结构

为了严谨地处理不确定性，我们将其置于一个**[概率空间](@entry_id:201477)** $(\Omega, \mathcal{F}, \mathbb{P})$ 的框架中。这里，$\Omega$ 是包含所有可能结果的[样本空间](@entry_id:275301)，$\mathcal{F}$ 是一个包含我们可以度量其概率的事件（$\Omega$ 的子集）的 $\sigma$-代数，而 $\mathbb{P}$ 是一个在 $\mathcal{F}$ 上定义的[概率测度](@entry_id:190821)，满足 Kolmogorov 公理。

在随机模型中，一个**场景 (scenario)** 本质上是[样本空间](@entry_id:275301) $\Omega$ 的一个子集。场景生成过程可以被看作是对[样本空间](@entry_id:275301) $\Omega$ 进行一次**有限可测划分 (finite measurable partition)** $\{S_i\}_{i=1}^N$。这意味着每个 $S_i$ 都是一个事件（即 $S_i \in \mathcal{F}$），它们互不相交（$S_i \cap S_j = \emptyset$ for $i \ne j$），并且它们的并集覆盖了整个[样本空间](@entry_id:275301)（$\bigcup_{i=1}^N S_i = \Omega$）。

对于每个这样的子集 $S_i$，我们可以赋予其一个概率 $p_i = \mathbb{P}(S_i)$。由于这是一个划分，这些概率自然满足 $p_i \ge 0$ 且 $\sum_{i=1}^N p_i = 1$。然后，我们为每个子集 $S_i$ 选择一个**代表值 (representative)** $x_i \in \mathbb{R}^T$（例如，随机向量在 $S_i$ 上的[条件期望](@entry_id:159140) $x_i = \mathbb{E}[X | S_i]$）。这样，我们就构建了一个离散的场景集 $\{ (x_i, p_i) \}_{i=1}^N$，它构成了一个有效的[离散概率](@entry_id:151843)测度 $\sum_{i=1}^N p_i \delta_{x_i}$，其中 $\delta_{x_i}$ 是集中在点 $x_i$ 处的狄拉克 (Dirac) 测度 。

在多阶段随机规划中，这种基于划分的信息结构直接引出了**非预期性约束 (non-anticipativity constraints)**。决策过程随时间 $t=0, 1, ..., T$ 展开，在每个阶段 $t$，决策 $x_t$ 只能依赖于直到该阶段已揭示的信息。这由一个**信息 filtration** $\{\mathcal{F}_t\}_{t=0}^T$ 来建模，其中 $\mathcal{F}_t$ 代表在阶段 $t$ 可获得的信息。在场景树模型中，$\mathcal{F}_t$是由阶段 $t$ 的所有节点生成的 $\sigma$-代数。

一个决策 $x_t$ 被称为是**$\mathcal{F}_t$-可测的**，这意味着它不能“预见”未来。在有限场景树的背景下，这等价于一个非常直观的约束：如果两个场景 $\omega$ 和 $\omega'$ 在阶段 $t$ 之前经历了完全相同的历史（即它们在场景树中经过同一个节点 $n \in \mathcal{N}_t$），那么在阶段 $t$ 做出的决策必须完全相同。换句话说，对于所有属于同一节点类 $S_n = \{\omega \in \Omega : \nu_t(\omega)=n\}$ 的场景，决策必须一致：
$$
x_t(\omega) = x_t(\omega') \quad \forall \omega, \omega' \in S_n
$$
这是非预期性约束的核心。在构建优化模型时，这些约束必须被明确地加入，以确保决策策略是现实可行的。一个计算上更高效的等价形式是为每个节点类 $S_n$ 选择一个代表性场景 $\omega_n$，并强制所有其他场景的决策与之相等：$x_t(\omega) - x_t(\omega_n) = 0$ 对所有 $\omega \in S_n$ 成立 。

### 场景生成：逼近真实分布

场景生成的目标是创建一个离散的、有限的场景集，其所代表的[概率测度](@entry_id:190821)能够很好地逼近不确定变量的真实（但通常是连续或非常复杂的）概率分布 $\mu$。

#### 蒙特卡洛抽样

最基础的场景生成方法是**[蒙特卡洛](@entry_id:144354) (Monte Carlo) 抽样**。其思想非常直接：如果我们能够从[目标分布](@entry_id:634522) $\mu$ 中抽取样本，我们就可以通过抽取大量[独立同分布](@entry_id:169067) (i.i.d.) 的样本 $\{X_1, X_2, \dots, X_N\}$ 来构造一个经验[概率测度](@entry_id:190821) $\hat{\mu}_N = \frac{1}{N} \sum_{i=1}^N \delta_{X_i}$。

这种方法的理论基石是**大数定律 (Law of Large Numbers)**。例如，强大数定律 (SLLN) 保证，只要[随机变量](@entry_id:195330)的一阶矩存在（即 $\mathbb{E}_\mu[|X|]  \infty$），那么样本均值 $\bar{X}_N = \frac{1}{N} \sum_{i=1}^N X_i$ 将随着样本量 $N$ 的增大而[几乎必然](@entry_id:262518)地收敛到真实期望 $\mathbb{E}_\mu[X]$ 。这为使用样本均值近似 (Sample Average Approximation, SAA) 方法求解随机优化问题提供了理论依据。

#### 基于Copula的多变量场景生成

在能源系统中，不确定性往往是多维度的，例如风电、光伏和负荷之间存在复杂的相关性。简单地对每个变量独立抽样会忽略这种依赖结构，从而导致不切实际的场景（例如，同时出现极高的风电和光伏出力，这在物理上不太可能）。

**Copula 理论** 为构建具有特定依赖结构的多变量分布提供了强大的框架。根据 **Sklar 定理**，任何一个多变量[联合分布](@entry_id:263960)函数 $F(x_1, \dots, x_d)$ 都可以被分解为一个 copula 函数 $C$ 和各自的边缘[分布函数](@entry_id:145626) $F_1, \dots, F_d$：
$$
F(x_1, \dots, x_d) = C(F_1(x_1), \dots, F_d(x_d))
$$
Copula 函数 $C$ 描述了变量之间的依赖结构，而边缘分布 $F_i$ 则描述了每个单一变量的统计特性。这一定理的美妙之处在于它将边缘分布的建模与依赖结构的建模分离开来。

一个实用的场景生成流程如下 ：
1.  **边缘分布建模**：根据历史数据为每个不确定变量（如风电、光伏、负荷）拟合最合适的边缘分布。例如，风电和光伏的[容量因子](@entry_id:1122045)常用 Beta 分布描述，而负荷则可能更符合正态分布。
2.  **依赖结构建模**：选择一个 Copula 函数族（如高斯 Copula 或 t-Copula）来描述变量间的依赖关系。Copula 的参数通常通过匹配历史数据的秩[相关系数](@entry_id:147037)（如 [Kendall's tau](@entry_id:750989) 或 [Spearman's rho](@entry_id:168402)）来校准。秩[相关系数](@entry_id:147037)的优势在于它们只依赖于 Copula，不受边缘分布变换的影响。
    例如，如果选择高斯 Copula，其参数是一个相关系数矩阵 $R$。为了匹配观测到的 [Kendall's tau](@entry_id:750989) 矩阵 $T$，我们需要利用它们之间的已知关系来校准 $R$ 中的每个元素 $R_{ij}$：
    $$
    R_{ij} = \sin\left(\frac{\pi}{2} T_{ij}\right)
    $$
3.  **抽样过程**：
    a.  从校准后的多维高斯分布 $\mathcal{N}(0, R)$ 中抽取一个向量 $Z = (Z_1, \dots, Z_d)$。
    b.  应用[标准正态分布](@entry_id:184509)的[累积分布函数 (CDF)](@entry_id:264700) $\Phi$ 将 $Z$ 的每个分量转换为在 $[0,1]$ 上均匀分布的变量 $U_i = \Phi(Z_i)$。向量 $U=(U_1, \dots, U_d)$ 的[联合分布](@entry_id:263960)即为我们所选择的 Copula。
    c.  最后，应用每个变量的**[逆累积分布函数](@entry_id:266870) (inverse CDF)**，即[分位数函数](@entry_id:271351) $F_i^{-1}$，将均匀变量 $U_i$ 转换为最终的场景值 $X_i = F_i^{-1}(U_i)$。

通过这个过程，我们生成的场景集 $\{X_i\}_{i=1}^N$ 不仅保留了每个变量各自的边缘分布特性，还精确地复现了我们想要的目标依赖结构。

### 场景质量评估：[概率测度](@entry_id:190821)间的距离

生成场景后，我们如何定量评估它对真实分布的逼近程度？这需要引入**概率度量 (probability metrics)** 的概念，即衡量两个[概率测度](@entry_id:190821)之间“距离”的函数。

#### [弱收敛](@entry_id:146650)

在概率论中，衡量一系列近似测度 $\nu_N$ 是否“趋向”于目标测度 $\mu$ 的一个基本概念是**[弱收敛](@entry_id:146650) (weak convergence)**，记为 $\nu_N \Rightarrow \mu$。其定义为：对于任意有界连续函数 $f$，期望的近似值收敛于真实值：
$$
\int f(x) d\nu_N(x) \to \int f(x) d\mu(x) \quad \text{as } N \to \infty
$$
对于由场景集 $\{(\xi_i, p_i)\}_{i=1}^N$ 诱导的[离散测度](@entry_id:183686) $\nu_N = \sum p_i \delta_{\xi_i}$，上述积分即为加权平均 $\sum_{i=1}^N p_i f(\xi_i)$。[弱收敛](@entry_id:146650)保证了对于行为良好的函数（有界且连续），基于场景的期望计算是可靠的。**Portmanteau 定理** 提供了弱收斂的多个等价刻画，例如，它要求对于所有开集 $O$，$\liminf_{N\to\infty} \nu_N(O) \ge \mu(O)$；对于所有[闭集](@entry_id:136446) $F$，$\limsup_{N\to\infty} \nu_N(F) \le \mu(F)$ 。

然而，[弱收敛](@entry_id:146650)本身是一个极限概念，它没有告诉我们在有限样本 $N$ 下近似的误差有多大。为此，我们需要一个能够量化这种收敛的度量。

#### Wasserstein 距离

**Wasserstein 距离**（或称为“Earth Mover's Distance”）是一种在随机优化领域日益重要的概率度量。$p$-阶 Wasserstein 距离 $W_p(\mu, \nu)$ 直观上衡量了将一个概率分布 $\mu$ “变换”成另一个分布 $\nu$ 所需的“最小代价”。其数学定义基于[最优输运](@entry_id:196008)理论：
$$
W_p(\mu, \nu) = \left( \inf_{\gamma \in \Pi(\mu, \nu)} \int_{\mathbb{R}^d \times \mathbb{R}^d} \|x-y\|^p d\gamma(x, y) \right)^{1/p}
$$
其中，$\Pi(\mu, \nu)$ 是所有以 $\mu$ 和 $\nu$ 为边缘分布的[联合概率](@entry_id:266356)测度（称为“耦合”）的集合。$\gamma(x,y)$ 可以被想象成一个运输计划，描述了从位置 $x$ 搬运多少“土方”（概率质量）到位置 $y$。

Wasserstein 距离之所以在[能源系统建模](@entry_id:1124496)中特别有用，是因为它与优化模型中常见的[目标函数](@entry_id:267263)和约束有着深刻的联系。**Kantorovich-Rubinstein [对偶定理](@entry_id:137804)** 指出，对于 $p=1$ 的情况，Wasserstein 距离等价于在所有 1-Lipschitz 函数上，两个分布[期望值](@entry_id:150961)的最大差异：
$$
W_1(\mu, \nu) = \sup \left\{ \int f d\mu - \int f d\nu : f \text{ is 1-Lipschitz} \right\}
$$
这个对偶公式带来了一个极其重要的结论：如果一个函数 $g$ 是 $L$-Lipschitz 连续的（即 $|g(a) - g(b)| \le L \|a-b\|$），那么使用场景测度 $\nu$ 计算其期望所产生的误差有一个明确的[上界](@entry_id:274738)：
$$
\left| \mathbb{E}_\mu[g(X)] - \mathbb{E}_\nu[g(X)] \right| \le L \cdot W_1(\mu, \nu)
$$
由于[随机规划](@entry_id:168183)模型中的成本函数（如燃料成本、[弃风](@entry_id:1134089)弃光惩罚）通常是 Lipschitz 连续的，因此，通过最小化 Wasserstein 距离来生成或削减场景，可以直接控制模型中最关心量的近似误差 。

#### 保真度与计算复杂度的权衡

理论上，增加场景数量 $N$ 可以提高近似精度。然而，这种提升并非没有代价。一方面，[蒙特卡洛近似](@entry_id:164880)的收敛速度通常是次线性的。例如，对于 Wasserstein distance，在 $d$ 维空间中，其收敛速度通常为 $\mathcal{O}(N^{-1/d})$。这意味着在高维问题中，需要指数级增长的场景数量才能有效降低误差，这就是所谓的**“[维度灾难](@entry_id:143920)” (curse of dimensionality)**。

另一方面，在两阶段或多阶段[随机规划](@entry_id:168183)的[确定性等价](@entry_id:636694)形式中，变量和约束的数量通常与场景数量 $N$ **[线性相关](@entry_id:185830)**。这意味着模型规模会随着 $N$ 的增加而迅速膨胀，导致计算上变得 intractable（难解）。

这就揭示了一个核心的权衡：
*   **高保真度 (High Fidelity)** 需要大量的场景 $N$ 来减小统计近似误差。
*   **计算可行性 (Computational Tractability)** 要求场景数量 $N$ 足够小。

**[场景削减](@entry_id:1131296) (scenario reduction)** 技术正是为了解决这一矛盾而设计的。

### [场景削减](@entry_id:1131296)：在保真度与可行性间寻求平衡

[场景削减](@entry_id:1131296)的出发点是：我们首先通过某种方法（如[蒙特卡洛](@entry_id:144354)）生成一个包含大量场景（比如 $N=10000$）的高保真度初始集，它很好地代表了真实分布。然后，我们应用一个算法，将这个大集合削减为一个规模小得多（比如 $M=50$）、但仍能“尽可能好”地保留原始集统计特性的代表性场景集。

这个过程可以被视为一个两步逼近 ：
1.  **生成**：从真实分布 $\mathbb{P}$ 到大规模[经验测度](@entry_id:181007) $\hat{\mathbb{P}}_N$ 的逼近，引入误差 $d(\mathbb{P}, \hat{\mathbb{P}}_N)$。
2.  **削减**：从大规模[经验测度](@entry_id:181007) $\hat{\mathbb{P}}_N$ 到小规模削减测度 $\hat{\mathbb{P}}_M$ 的逼近，引入误差 $d(\hat{\mathbb{P}}_N, \hat{\mathbb{P}}_M)$。

根据度量的[三角不等式](@entry_id:143750)，总误差 $d(\mathbb{P}, \hat{\mathbb{P}}_M)$ 受两者之和的约束：$d(\mathbb{P}, \hat{\mathbb{P}}_M) \le d(\mathbb{P}, \hat{\mathbb{P}}_N) + d(\hat{\mathbb{P}}_N, \hat{\mathbb{P}}_M)$。[场景削减](@entry_id:1131296)的目标就是在给定 $M$ 的情况下，最小化第二项 $d(\hat{\mathbb{P}}_N, \hat{\mathbb{P}}_M)$。

#### 基于聚类的削减方法

一类流行的[场景削减](@entry_id:1131296)方法是基于**聚类 (clustering)** 算法，例如 k-means。其基本思想是将相似的场景分组，然后用一个代表性场景（即簇心）来代替整个簇。

在处理带概率的场景时，我们需要使用**概率加权的 k-means** 算法。给定一个初始场景集 $\{(x_i, p_i)\}_{i=1}^N$ 和目标削减数量 $K$，算法的目标是找到一个将场景划分为 $K$ 个簇 $\{C_k\}_{k=1}^K$ 的方案，以及相应的 $K$ 个簇心 $c_k$，使得总的加权平方欧氏距离最小化：
$$
\min \sum_{k=1}^K \sum_{i \in C_k} p_i \|x_i - c_k\|^2
$$
对于一个给定的簇 $C_k$，可以证明，最小化该簇内部误差的 optimal centroid $c_k$ 是该簇内所有场景的**概率加权平均值**：
$$
c_k = \frac{\sum_{i \in C_k} p_i x_i}{\sum_{i \in C_k} p_i}
$$
而这个新的代表性场景的权重 $w_k$ 则是该簇内所有原始场景概率的总和：
$$
w_k = \sum_{i \in C_k} p_i
$$
例如，假设我们有两个场景 $x_1=(40,50)$ (概率 $p_1=0.1$) 和 $x_2=(45,55)$ (概率 $p_2=0.3$) 被分到同一个簇。那么，代表这个簇的新场景是：
*   权重 $w_1 = p_1 + p_2 = 0.1 + 0.3 = 0.4$
*   中心 $c_1 = \frac{0.1 \cdot (40,50) + 0.3 \cdot (45,55)}{0.4} = (43.75, 53.75)$

这种方法的一个非常重要的特性是，它能够**精确地保持原始分布的一阶矩（即[期望值](@entry_id:150961)）**。也就是说，削减后的场景集的[期望值](@entry_id:150961)与原始场景集的[期望值](@entry_id:150961)完全相等 ：
$$
\sum_{k=1}^K w_k c_k = \sum_{i=1}^N p_i x_i
$$
然而，需要注意的是，该方法通常不能保持二阶或更高阶的矩（如方差），这意味着它可能改变分布的 dispersion。

#### 向后削减法

另一类重要的方法是**向后削减法 (backward reduction)**。这类方法从完整的场景集开始，迭代地移除“最不重要”的场景，并将其概率质量重新分配给剩余的场景。

一个通用的向后削减问题可以被形式化为一个优化问题。我们的目标是选择一个大小为 $K$ 的保留场景子集 $S \subset \{1, \dots, N\}$，并确定一个最优的概率重分配方案，以最小化削减后测度 $\tilde{\mu}^K$ 与原始测度 $\mu^N$ 之间的距离 $D(\tilde{\mu}^K, \mu^N)$。

最一般的**概率重分配**方案可以由一个重分配矩阵 $A = (a_{ji})$ 描述，其中 $a_{ji}$ 代表原始场景 $j$ 的概率质量 $p_j$ 中被转移到保留场景 $i \in S$ 的比例。保留场景 $i$ 的新概率 $\tilde{p}_i$ 就是所有转移给它的概率质量的总和：
$$
\tilde{p}_i = \sum_{j=1}^N p_j a_{ji}
$$
因此，完整的向后削减问题可以表述为以下优化问题 ：
$$
\min_{S, A} \; D\left( \sum_{i \in S} \tilde{p}_i \delta_{\xi_i}, \sum_{j=1}^N p_j \delta_{\xi_j} \right)
$$
$$
\text{subject to} \quad |S| = K, \quad a_{ji} \ge 0, \quad \sum_{i \in S} a_{ji} = 1 \text{ for each } j \in \{1, \dots, N\}
$$
这个问题在组合上是 NP-hard 的，但存在高效的[启发式算法](@entry_id:176797)来求解。例如，一种常见的[启发式](@entry_id:261307)策略是，在每次迭代中，移除一个场景，并将其概率完全转移给剩余场景中“最近”的一个（根据某种[距离度量](@entry_id:636073)）。

无论是基于聚类还是向后削减，这些方法都为我们提供了在模型保真度与计算可行性之间做出明智权衡的 principled tools，使得处理大规模不确定性下的[能源系统优化](@entry_id:1124497)问题成为可能。