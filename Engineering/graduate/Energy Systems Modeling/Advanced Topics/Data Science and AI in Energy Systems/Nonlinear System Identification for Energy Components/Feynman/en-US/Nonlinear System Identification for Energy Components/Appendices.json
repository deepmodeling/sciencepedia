{
    "hands_on_practices": [
        {
            "introduction": "A foundational step in developing a state-space model is to ensure its internal states can be uniquely determined from the available sensor measurements. This property, known as observability, is crucial for both state estimation and parameter identification. This exercise  provides hands-on practice with assessing the local observability of a nonlinear fuel cell model using the powerful mathematical framework of Lie derivatives, a cornerstone of modern nonlinear control theory.",
            "id": "4108685",
            "problem": "Consider a three-state, input-driven model for a proton-exchange-membrane fuel cell stack used in energy systems modeling. The state vector is $x = \\begin{pmatrix} x_{1} & x_{2} & x_{3} \\end{pmatrix}^{\\top}$, where $x_{1}$ is the membrane water content in moles, $x_{2}$ is the stack temperature in kelvins, and $x_{3}$ is the anode hydrogen partial pressure in bar. The scalar input $u$ is the stack current in amperes, which is known and imposed.\n\nThe dynamics are given as ordinary differential equations (ODEs):\n$$\n\\dot{x}_{1} = -a\\left(x_{1} - \\alpha x_{2}\\right) + b\\,u,\\quad\n\\dot{x}_{2} = \\frac{1}{C}\\left(\\beta\\,u - \\gamma\\left(x_{2} - T_{a}\\right)\\right),\\quad\n\\dot{x}_{3} = -\\delta\\,u + \\varepsilon\\left(p_{s} - x_{3}\\right),\n$$\nwith constant parameters $a = 0.1\\,\\mathrm{s}^{-1}$, $\\alpha = 0.02\\,\\mathrm{mol/K}$, $b = 5.0\\times 10^{-3}\\,\\mathrm{mol/(A\\cdot s)}$, $C = 500\\,\\mathrm{J/K}$, $\\beta = 0.6\\,\\mathrm{W/A}$, $\\gamma = 50\\,\\mathrm{W/K}$, $T_{a} = 298\\,\\mathrm{K}$, $\\delta = 10^{-3}\\,\\mathrm{bar/(A\\cdot s)}$, $\\varepsilon = 0.2\\,\\mathrm{s}^{-1}$, $p_{s} = 2.0\\,\\mathrm{bar}$.\n\nPartial sensing is available through two outputs. The first output is the stack voltage modeled as\n$$\ny_{1} = h_{1}(x,u) = E_{0} + k_{T}\\,x_{2}\\,\\ln(x_{3}) - \\frac{r_{1}}{x_{1}} - r_{2}\\,u - r_{0},\n$$\nwhere $E_{0} = 0.95\\,\\mathrm{V}$, $r_{0} = 0.05\\,\\mathrm{V}$, $r_{1} = 2.0\\times 10^{-2}\\,\\mathrm{V\\cdot mol}$, $r_{2} = 10^{-3}\\,\\mathrm{V/A}$, and $k_{T} = \\frac{R}{2F}$ with universal gas constant $R = 8.314\\,\\mathrm{J/(mol\\cdot K)}$ and Faraday’s constant $F = 96485\\,\\mathrm{C/mol}$. The second output directly measures the stack temperature,\n$$\ny_{2} = h_{2}(x) = x_{2}.\n$$\n\nFor a fixed constant input $u = u^{\\star} = 200\\,\\mathrm{A}$ and operating point $x^{\\star} = \\begin{pmatrix} 5.0\\,\\mathrm{mol} & 350\\,\\mathrm{K} & e\\,\\mathrm{bar} \\end{pmatrix}^{\\top}$, where $e$ is the base of the natural logarithm, address the following:\n\n1. Starting from the core definition of local weak observability for nonlinear systems, define the observability rank condition using Lie derivatives. In particular, explain how to construct an observability matrix from the gradients of the outputs and their successive Lie derivatives along the system vector field.\n\n2. Apply this construction to the fuel cell stack model above. Using the rows $\\nabla h_{1}(x^{\\star})$, $\\nabla h_{2}(x^{\\star})$, and $\\nabla \\left(L_{f} h_{1}\\right)(x^{\\star})$—where $L_{f} h_{1}$ denotes the Lie derivative of $h_{1}$ along the drift vector field $f(x) = \\begin{pmatrix} \\dot{x}_{1} & \\dot{x}_{2} & \\dot{x}_{3} \\end{pmatrix}^{\\top}$ with $u$ held constant at $u^{\\star}$—evaluate these gradients at the given operating point with the given parameter values, assemble the resulting $3 \\times 3$ observability matrix, and determine its rank.\n\nExpress the final answer as the integer rank of the constructed observability matrix. No units are required for the final answer. The final answer must be a single real number.",
            "solution": "The analysis proceeds in two parts as requested by the problem statement. First, we define the observability rank condition for a general nonlinear system. Second, we apply this condition to the specific fuel cell model at the given operating point.\n\n### Part 1: The Observability Rank Condition\n\nConsider a general nonlinear, time-invariant system described by the equations:\n$$\n\\dot{x} = f(x, u) \\\\\ny = h(x, u)\n$$\nwhere $x \\in \\mathbb{R}^{n}$ is the state vector, $u \\in \\mathbb{R}^{m}$ is the input vector, and $y \\in \\mathbb{R}^{p}$ is the output vector. For the purpose of observability analysis, the input $u(t)$ is assumed to be a known function of time. We can therefore treat the system as being governed by a time-varying vector field on the state space, $\\dot{x} = f(x(t), u(t)) \\equiv f_t(x(t))$.\n\nA system is locally weakly observable at a state $x^{\\star}$ if, for some input $u(t)$, any initial state $x(0)$ in a neighborhood of $x^{\\star}$ can be uniquely determined from the output history $y(t)$ over a finite time interval.\n\nThe formal tool for analyzing observability is the Lie derivative. The Lie derivative of a scalar field (a function) $\\phi(x)$ along a vector field $g(x)$ is defined as:\n$$\nL_{g}\\phi(x) = \\nabla \\phi(x) \\cdot g(x) = \\sum_{i=1}^{n} \\frac{\\partial \\phi}{\\partial x_{i}} g_{i}(x)\n$$\nIt represents the rate of change of $\\phi$ along the flow of the vector field $g$. Successive Lie derivatives are defined recursively: $L_{g}^{0}\\phi = \\phi$ and $L_{g}^{k}\\phi = L_{g}(L_{g}^{k-1}\\phi)$ for $k \\ge 1$.\n\nThe time derivative of an output component $y_j = h_j(x)$ (where we suppress the explicit dependence on the known input $u$ for notational clarity) along a system trajectory is $\\dot{y}_j = \\frac{d}{dt}h_j(x(t)) = (\\nabla h_j) \\cdot \\dot{x} = L_f h_j(x)$. Similarly, higher time derivatives of the output can be expressed as successive Lie derivatives: $\\frac{d^k y_j}{dt^k} = L_f^k h_j(x)$.\n\nThe observability of the system is determined by the \"information\" about the state $x$ contained in the outputs and their time derivatives. This information is captured by the gradients (differentials) of the successive Lie derivatives of the output functions. The collection of all such independent gradients spans the observability codistribution.\n\nThe observability rank condition states that a system is locally weakly observable at $x^{\\star}$ if the dimension of the observability space is equal to the dimension of the state space, $n$. This is checked by constructing the observability matrix, $\\mathcal{M_O}(x)$, whose rows are the gradients of the Lie derivatives of the output components:\n$$\n\\mathcal{M_O}(x) = \\begin{pmatrix}\n\\nabla (L_f^0 h_1)(x) \\\\\n\\vdots \\\\\n\\nabla (L_f^0 h_p)(x) \\\\\n\\nabla (L_f^1 h_1)(x) \\\\\n\\vdots \\\\\n\\nabla (L_f^1 h_p)(x) \\\\\n\\vdots \\\\\n\\nabla (L_f^{n-1} h_1)(x) \\\\\n\\vdots \\\\\n\\nabla (L_f^{n-1} h_p)(x)\n\\end{pmatrix}\n$$\nThe system is locally weakly observable at $x^{\\star}$ if $\\text{rank}(\\mathcal{M_O}(x^{\\star})) = n$. The problem specifies constructing a $3 \\times 3$ matrix from a particular subset of these rows.\n\n### Part 2: Application to the Fuel Cell Model\n\nThe system has a state dimension of $n=3$ and output dimension of $p=2$. The input is held constant at $u = u^{\\star} = 200\\,\\mathrm{A}$. This makes the system autonomous with the vector field $f(x) = (\\dot{x}_1, \\dot{x}_2, \\dot{x}_3)^\\top$ given by:\n$$\nf(x) = \\begin{pmatrix}\n-a(x_1 - \\alpha x_2) + b u^{\\star} \\\\\n\\frac{1}{C}(\\beta u^{\\star} - \\gamma(x_2 - T_a)) \\\\\n-\\delta u^{\\star} + \\varepsilon(p_s - x_3)\n\\end{pmatrix}\n$$\nThe output functions are:\n$$\nh_1(x) = E_0 + k_T x_2 \\ln(x_3) - \\frac{r_1}{x_1} - r_2 u^{\\star} - r_0 \\\\\nh_2(x) = x_2\n$$\nThe problem requires constructing a $3 \\times 3$ observability matrix from the rows $\\nabla h_1(x^\\star)$, $\\nabla h_2(x^\\star)$, and $\\nabla (L_f h_1)(x^\\star)$, evaluated at the operating point $x^{\\star} = \\begin{pmatrix} 5.0 & 350 & e \\end{pmatrix}^{\\top}$.\n\nFirst, we compute the necessary gradients symbolically.\nThe gradient of $h_1(x)$ is:\n$$\n\\nabla h_1(x) = \\begin{pmatrix} \\frac{\\partial h_1}{\\partial x_1}, & \\frac{\\partial h_1}{\\partial x_2}, & \\frac{\\partial h_1}{\\partial x_3} \\end{pmatrix} = \\begin{pmatrix} \\frac{r_1}{x_1^2} & k_T \\ln(x_3) & \\frac{k_T x_2}{x_3} \\end{pmatrix}\n$$\nThe gradient of $h_2(x)$ is:\n$$\n\\nabla h_2(x) = \\begin{pmatrix} \\frac{\\partial h_2}{\\partial x_1}, & \\frac{\\partial h_2}{\\partial x_2}, & \\frac{\\partial h_2}{\\partial x_3} \\end{pmatrix} = \\begin{pmatrix} 0 & 1 & 0 \\end{pmatrix}\n$$\nNext, we compute the Lie derivative $L_f h_1$:\n$$\nL_f h_1(x) = \\nabla h_1(x) \\cdot f(x) = \\left(\\frac{r_1}{x_1^2}\\right)f_1(x) + \\left(k_T \\ln(x_3)\\right)f_2(x) + \\left(\\frac{k_T x_2}{x_3}\\right)f_3(x)\n$$\nNow, we find the gradient of $L_f h_1(x)$.\n$\\frac{\\partial}{\\partial x_1}(L_f h_1) = \\frac{\\partial}{\\partial x_1}\\left(\\frac{r_1}{x_1^2} f_1(x)\\right) = \\frac{-2r_1}{x_1^3}f_1(x) + \\frac{r_1}{x_1^2}\\frac{\\partial f_1}{\\partial x_1} = \\frac{-2r_1}{x_1^3}(-a(x_1-\\alpha x_2)+bu^{\\star}) + \\frac{r_1}{x_1^2}(-a) = \\frac{r_1}{x_1^3}(ax_1 - 2a\\alpha x_2 - 2bu^{\\star})$.\n\n$\\frac{\\partial}{\\partial x_2}(L_f h_1) = \\frac{\\partial}{\\partial x_2}\\left(\\frac{r_1}{x_1^2} f_1\\right) + \\frac{\\partial}{\\partial x_2}\\left(k_T \\ln(x_3) f_2\\right) + \\frac{\\partial}{\\partial x_2}\\left(\\frac{k_T x_2}{x_3} f_3\\right) = \\frac{r_1}{x_1^2}\\frac{\\partial f_1}{\\partial x_2} + k_T \\ln(x_3)\\frac{\\partial f_2}{\\partial x_2} + \\frac{k_T}{x_3}f_3 = \\frac{a \\alpha r_1}{x_1^2} - \\frac{\\gamma k_T}{C} \\ln(x_3) + \\frac{k_T}{x_3}(-\\delta u^{\\star} + \\varepsilon(p_s-x_3))$.\n\n$\\frac{\\partial}{\\partial x_3}(L_f h_1) = \\frac{\\partial}{\\partial x_3}\\left(k_T \\ln(x_3) f_2\\right) + \\frac{\\partial}{\\partial x_3}\\left(\\frac{k_T x_2}{x_3} f_3\\right) = \\frac{k_T}{x_3}f_2 + \\left( \\frac{-k_T x_2}{x_3^2}f_3 + \\frac{k_T x_2}{x_3}\\frac{\\partial f_3}{\\partial x_3} \\right) = \\frac{k_T}{x_3}f_2 - \\frac{k_T x_2}{x_3^2}f_3 - \\frac{k_T x_2 \\varepsilon}{x_3}$.\n\nWe evaluate these gradients at $x^{\\star} = \\begin{pmatrix} 5 & 350 & e\\end{pmatrix}^{\\top}$ with $u^{\\star}=200\\,\\mathrm{A}$.\nParameters: $a=0.1$, $\\alpha=0.02$, $b=5\\times 10^{-3}$, $C=500$, $\\beta=0.6$, $\\gamma=50$, $T_a=298$, $\\delta=10^{-3}$, $\\varepsilon=0.2$, $p_s=2$, $r_1=0.02$.\n$k_T = \\frac{R}{2F} = \\frac{8.314}{2 \\times 96485} \\approx 4.3084 \\times 10^{-5}$.\n\nRow 1: $\\nabla h_1(x^{\\star})$\n$\\frac{\\partial h_1}{\\partial x_1}\\bigg|_{x^\\star} = \\frac{0.02}{5^2} = 0.0008$.\n$\\frac{\\partial h_1}{\\partial x_2}\\bigg|_{x^\\star} = k_T \\ln(e) = k_T \\approx 4.3084 \\times 10^{-5}$.\n$\\frac{\\partial h_1}{\\partial x_3}\\bigg|_{x^\\star} = k_T \\frac{350}{e} \\approx 128.755 k_T \\approx 5.5473 \\times 10^{-3}$.\n$\\nabla h_1(x^{\\star}) \\approx \\begin{pmatrix} 8 \\times 10^{-4} & 4.3084 \\times 10^{-5} & 5.5473 \\times 10^{-3} \\end{pmatrix}$.\n\nRow 2: $\\nabla h_2(x^{\\star})$\n$\\nabla h_2(x^{\\star}) = \\begin{pmatrix} 0 & 1 & 0 \\end{pmatrix}$.\n\nRow 3: $\\nabla (L_f h_1)(x^{\\star})$\n$\\frac{\\partial}{\\partial x_1}(L_f h_1)\\bigg|_{x^\\star} = \\frac{0.02}{5^3}(0.1(5) - 2(0.1)(0.02)(350) - 2(0.005)(200)) = \\frac{0.02}{125}(0.5 - 1.4 - 2) = -0.000464$.\n$\\frac{\\partial}{\\partial x_2}(L_f h_1)\\bigg|_{x^\\star} = \\frac{0.1(0.02)(0.02)}{5^2} - \\frac{50 k_T}{500}(1) + \\frac{k_T}{e}(-10^{-3}(200) + 0.2(2-e)) = 1.6 \\times 10^{-6} - 0.1 k_T + \\frac{k_T}{e}(0.2 - 0.2e) = 1.6 \\times 10^{-6} - 0.2264 k_T \\approx -8.154 \\times 10^{-6}$.\n$\\frac{\\partial}{\\partial x_3}(L_f h_1)\\bigg|_{x^\\star} = \\frac{k_T}{500e}(0.6(200) - 50(350-298)) + \\frac{k_T(350)}{e^2}(10^{-3}(200) - 0.2(2)) = \\frac{k_T}{500e}(-2480) + \\frac{350k_T}{e^2}(-0.2) = k_T(-\\frac{4.96}{e} - \\frac{70}{e^2}) \\approx -11.298 k_T \\approx -4.8675 \\times 10^{-4}$.\n$\\nabla (L_f h_1)(x^{\\star}) \\approx \\begin{pmatrix} -4.64 \\times 10^{-4} & -8.154 \\times 10^{-6} & -4.8675 \\times 10^{-4} \\end{pmatrix}$.\n\nThe observability matrix $\\mathcal{M}$ is:\n$$\n\\mathcal{M} = \\begin{pmatrix} \\nabla h_1(x^{\\star}) \\\\ \\nabla h_2(x^{\\star}) \\\\ \\nabla (L_f h_1)(x^{\\star}) \\end{pmatrix} \\approx \\begin{pmatrix} 8 \\times 10^{-4} & 4.3084 \\times 10^{-5} & 5.5473 \\times 10^{-3} \\\\ 0 & 1 & 0 \\\\ -4.64 \\times 10^{-4} & -8.154 \\times 10^{-6} & -4.8675 \\times 10^{-4} \\end{pmatrix}\n$$\nTo find the rank, we compute the determinant of $\\mathcal{M}$. Expanding along the second row simplifies the calculation:\n$$\n\\det(\\mathcal{M}) = (1) \\cdot \\det \\begin{pmatrix} 8 \\times 10^{-4} & 5.5473 \\times 10^{-3} \\\\ -4.64 \\times 10^{-4} & -4.8675 \\times 10^{-4} \\end{pmatrix}\n$$\n$$\n\\det(\\mathcal{M}) \\approx (8 \\times 10^{-4})(-4.8675 \\times 10^{-4}) - (5.5473 \\times 10^{-3})(-4.64 \\times 10^{-4})\n$$\n$$\n\\det(\\mathcal{M}) \\approx -3.894 \\times 10^{-7} - (-2.574 \\times 10^{-6}) = -0.3894 \\times 10^{-6} + 2.574 \\times 10^{-6} = 2.1846 \\times 10^{-6}\n$$\nSince the determinant is non-zero, the matrix $\\mathcal{M}$ is of full rank. The rank of the constructed observability matrix is $3$.",
            "answer": "$$\\boxed{3}$$"
        },
        {
            "introduction": "Realistic physical models of energy components often contain numerous parameters, many of which are uncertain. To direct identification efforts efficiently, it's vital to determine which parameters most strongly influence the model's output. This practice in sensitivity analysis  demonstrates how to derive an analytical sensitivity for a chiller model, providing a quantitative basis for prioritizing parameter estimation and model refinement.",
            "id": "4108755",
            "problem": "A vapor-compression chiller’s evaporator is modeled for nonlinear system identification using first-principles heat exchange and energy balance. The model output is the cooling capacity $Q_{e}$, and a key parameter is the evaporator overall heat transfer conductance $U_{e}A_{e}$. To prioritize model refinement, you are asked to compute the local sensitivity $\\frac{\\partial Q_{e}}{\\partial (U_{e}A_{e})}$ at a specified operating point.\n\nAssume the refrigerant evaporates isothermally at saturation temperature $T_{e}$ and the chilled water flows through the evaporator with mass flow rate $\\dot{m}_{w}$ and specific heat capacity $c_{p}$. The chilled-water inlet temperature is $T_{w,\\mathrm{in}}$, and the outlet temperature is $T_{w,\\mathrm{out}}$. The evaporator heat transfer obeys the standard log-mean temperature difference (LMTD) relation for an isothermal hot or cold side and single-phase liquid on the other side. The governing relations are:\n- Energy balance on the water side: $$Q_{e} = \\dot{m}_{w} c_{p} \\bigl(T_{w,\\mathrm{in}} - T_{w,\\mathrm{out}}\\bigr).$$\n- Log-mean temperature difference (LMTD): $$\\Delta T_{\\mathrm{lm}} = \\frac{\\bigl(T_{w,\\mathrm{in}} - T_{e}\\bigr) - \\bigl(T_{w,\\mathrm{out}} - T_{e}\\bigr)}{\\ln\\!\\left(\\frac{T_{w,\\mathrm{in}} - T_{e}}{T_{w,\\mathrm{out}} - T_{e}}\\right)}.$$\n- Heat transfer rate: $$Q_{e} = (U_{e}A_{e})\\,\\Delta T_{\\mathrm{lm}}.$$\n\nUsing only these relations, derive an analytic expression for $Q_{e}$ as a function of $(U_{e}A_{e})$ by eliminating $T_{w,\\mathrm{out}}$, and from that expression obtain the sensitivity $\\frac{\\partial Q_{e}}{\\partial (U_{e}A_{e})}$ in closed form. Evaluate the sensitivity at the baseline operating point:\n$$\\dot{m}_{w} = 10~\\mathrm{kg/s},\\quad c_{p} = 4.186~\\mathrm{kJ/(kg\\,K)},\\quad T_{w,\\mathrm{in}} = 12~\\mathrm{^\\circ C},\\quad T_{e} = 5~\\mathrm{^\\circ C},\\quad (U_{e}A_{e}) = 35~\\mathrm{kW/K}.$$\n\nExpress your final numerical value for $\\frac{\\partial Q_{e}}{\\partial (U_{e}A_{e})}$ in Kelvin. Round your answer to four significant figures.",
            "solution": "We begin from first principles appropriate to energy systems modeling: conservation of energy on the water side and the well-tested log-mean temperature difference (LMTD) formulation for heat exchanger analysis.\n\nDefine the water-side heat capacity rate as $$M \\equiv \\dot{m}_{w} c_{p}.$$ The water-side energy balance gives\n$$Q_{e} = M \\bigl(T_{w,\\mathrm{in}} - T_{w,\\mathrm{out}}\\bigr),$$\nso the outlet temperature can be expressed in terms of $Q_{e}$:\n$$T_{w,\\mathrm{out}} = T_{w,\\mathrm{in}} - \\frac{Q_{e}}{M}.$$\n\nLet the inlet temperature difference relative to the evaporating refrigerant be\n$$\\Delta T_{1} \\equiv T_{w,\\mathrm{in}} - T_{e},$$\nand the outlet temperature difference be\n$$\\Delta T_{2} \\equiv T_{w,\\mathrm{out}} - T_{e} = \\bigl(T_{w,\\mathrm{in}} - \\frac{Q_{e}}{M}\\bigr) - T_{e} = \\Delta T_{1} - \\frac{Q_{e}}{M}.$$\n\nFor an isothermal refrigerant side, the LMTD between the water and refrigerant is\n$$\\Delta T_{\\mathrm{lm}} = \\frac{\\Delta T_{1} - \\Delta T_{2}}{\\ln\\!\\left(\\frac{\\Delta T_{1}}{\\Delta T_{2}}\\right)}.$$\nSubstituting $\\Delta T_{2} = \\Delta T_{1} - \\frac{Q_{e}}{M}$ gives\n$$\\Delta T_{\\mathrm{lm}} = \\frac{\\frac{Q_{e}}{M}}{\\ln\\!\\left(\\frac{\\Delta T_{1}}{\\Delta T_{1} - \\frac{Q_{e}}{M}}\\right)}.$$\n\nThe evaporator heat transfer relation $Q_{e} = (U_{e}A_{e})\\,\\Delta T_{\\mathrm{lm}}$ then yields the implicit equation\n$$Q_{e} = (U_{e}A_{e}) \\cdot \\frac{\\frac{Q_{e}}{M}}{\\ln\\!\\left(\\frac{\\Delta T_{1}}{\\Delta T_{1} - \\frac{Q_{e}}{M}}\\right)}.$$\nAssuming $Q_{e} > 0$, divide both sides by $Q_{e}$ to obtain\n$$1 = \\frac{(U_{e}A_{e})}{M} \\cdot \\frac{1}{\\ln\\!\\left(\\frac{\\Delta T_{1}}{\\Delta T_{1} - \\frac{Q_{e}}{M}}\\right)}.$$\nInvert to isolate the logarithm:\n$$\\ln\\!\\left(\\frac{\\Delta T_{1}}{\\Delta T_{1} - \\frac{Q_{e}}{M}}\\right) = \\frac{(U_{e}A_{e})}{M}.$$\nExponentiate both sides:\n$$\\frac{\\Delta T_{1}}{\\Delta T_{1} - \\frac{Q_{e}}{M}} = \\exp\\!\\left(\\frac{(U_{e}A_{e})}{M}\\right).$$\nSolve for $Q_{e}$:\n$$\\Delta T_{1} - \\frac{Q_{e}}{M} = \\Delta T_{1}\\,\\exp\\!\\left(-\\frac{(U_{e}A_{e})}{M}\\right),$$\n$$\\frac{Q_{e}}{M} = \\Delta T_{1}\\left[1 - \\exp\\!\\left(-\\frac{(U_{e}A_{e})}{M}\\right)\\right],$$\n$$Q_{e}\\bigl((U_{e}A_{e})\\bigr) = M\\,\\Delta T_{1}\\left[1 - \\exp\\!\\left(-\\frac{(U_{e}A_{e})}{M}\\right)\\right].$$\n\nThis is a closed-form nonlinear map of output $Q_{e}$ to parameter $(U_{e}A_{e})$, derived solely from energy balance and LMTD. The sensitivity needed for parameter prioritization is the partial derivative of $Q_{e}$ with respect to $(U_{e}A_{e})$:\n$$\\frac{\\partial Q_{e}}{\\partial (U_{e}A_{e})} = M\\,\\Delta T_{1} \\cdot \\frac{\\partial}{\\partial (U_{e}A_{e})}\\left[1 - \\exp\\!\\left(-\\frac{(U_{e}A_{e})}{M}\\right)\\right] = M\\,\\Delta T_{1}\\left[\\frac{1}{M}\\exp\\!\\left(-\\frac{(U_{e}A_{e})}{M}\\right)\\right].$$\nThis simplifies to\n$$\\frac{\\partial Q_{e}}{\\partial (U_{e}A_{e})} = \\Delta T_{1}\\,\\exp\\!\\left(-\\frac{(U_{e}A_{e})}{M}\\right).$$\n\nEvaluate at the given operating point. First compute\n$$M = \\dot{m}_{w} c_{p} = 10~\\mathrm{kg/s} \\times 4.186~\\mathrm{kJ/(kg\\,K)} = 41.86~\\mathrm{kW/K},$$\nand\n$$\\Delta T_{1} = T_{w,\\mathrm{in}} - T_{e} = 12 - 5 = 7~\\mathrm{K}.$$\nNow substitute $(U_{e}A_{e}) = 35~\\mathrm{kW/K}$:\n$$\\frac{\\partial Q_{e}}{\\partial (U_{e}A_{e})}\\bigg|_{(U_{e}A_{e})=35} = 7 \\,\\exp\\!\\left(-\\frac{35}{41.86}\\right).$$\nCompute the exponential factor:\n$$\\frac{35}{41.86} \\approx 0.83612,$$\nso\n$$\\exp(-0.83612) \\approx 0.43338.$$\nTherefore,\n$$\\frac{\\partial Q_{e}}{\\partial (U_{e}A_{e})}\\bigg|_{(U_{e}A_{e})=35} \\approx 7 \\times 0.43338 \\approx 3.034~\\mathrm{K}.$$\n\nThis sensitivity quantifies how many Kelvin of cooling-capacity change (in the sense of $(\\mathrm{kW})/(\\mathrm{kW/K})$ which reduces to Kelvin) arises per unit change in $(U_{e}A_{e})$ near the operating point. It provides a principled metric to prioritize refining $(U_{e}A_{e})$ within the nonlinear system identification of the chiller.",
            "answer": "$$\\boxed{3.034}$$"
        },
        {
            "introduction": "After a model has been fit to experimental data, a critical question remains: is the model truly adequate? A powerful method for answering this is residual analysis, which examines the statistical properties of the errors between the model's predictions and the measurements. This exercise  delves into the theory and application of residual autocorrelation tests, showing how to determine if the residuals behave like random noise—a hallmark of a good model—or if they contain a structure that points to uncaptured dynamics.",
            "id": "4108728",
            "problem": "In the nonlinear identification of a fired water-tube boiler within an energy conversion plant, the plant dynamics obey conservation of mass and conservation of energy. The steam drum pressure responds to fuel enthalpy input and feedwater enthalpy inflow through nonlinear storage and transport mechanisms, while sensors introduce additive noise at the measured output. An engineer fits a nonlinear output-error model, in which the predicted output $\\hat{y}(k)$ is computed from the input sequence (e.g., fuel valve command $u_1(k)$ and feedwater valve command $u_2(k)$) by a parametric nonlinear dynamical mapping and the residual sequence is defined as $e(k) = y(k) - \\hat{y}(k)$. Under the standard assumptions that the sensor noise is additive, zero-mean, and independent of the inputs and past outputs, and that the model structure is capable of representing the true input-output dynamics, adequacy of the identification can be assessed using the autocorrelation of the residuals.\n\nA data set of length $N = 900$ is collected under persistently exciting operation, and the sample autocorrelation of the residual sequence is computed for selected lags. The reported values are $\\hat{r}_e(1) = 0.08$, $\\hat{r}_e(2) = -0.02$, $\\hat{r}_e(3) = 0.05$, and $\\hat{r}_e(5) = 0.01$.\n\nWhich of the following statements about the definition of residuals and the use of residual autocorrelation to test adequacy in this boiler identification are correct?\n\nA. The residual is defined as $e(k) = y(k) - \\hat{y}(k)$; for an adequately identified nonlinear output-error boiler model with additive, zero-mean sensor noise, $e(k)$ should behave approximately like a white noise sequence, meaning the true autocorrelation is $0$ for all non-zero lags, and sample autocorrelations at non-zero lags should be statistically indistinguishable from $0$.\n\nB. Because the model is nonlinear and optimized by minimizing a sum of squared errors, a good fit implies $e(k)$ is identically $0$ or nearly $0$ at all times; any non-zero residual autocorrelation arises only from imperfections in numerical optimization, not from model structure inadequacy.\n\nC. In an output-error model, the residual autocorrelation at lag $0$ must be $0$ by definition, since $\\hat{y}(k)$ attempts to track $y(k)$; therefore any non-zero $\\hat{r}_e(0)$ contradicts model adequacy.\n\nD. Given $N = 900$ and $\\hat{r}_e(1) = 0.08$, the residual autocorrelation test indicates inadequacy at a conventional $95\\%$ significance level, because under the hypothesis that $e(k)$ is white, sample autocorrelations at non-zero lags are approximately centered at $0$ and a value of $0.08$ at lag $1$ is too large to be attributed to sampling variability alone.\n\nE. Residual autocorrelation is irrelevant for output-error structures in boiler identification, because residuals are always shaped by the model dynamics and therefore cannot reveal unmodeled dynamics; adequacy should be judged solely by the magnitude of the sum of squared residuals and not by their temporal correlation.",
            "solution": "The problem statement is first validated for scientific soundness, self-consistency, and clarity before a solution is attempted.\n\n### Step 1: Extract Givens\n- **System**: A fired water-tube boiler in an energy conversion plant.\n- **Underlying Physics**: Dynamics governed by conservation of mass and energy.\n- **Inputs**: Fuel valve command $u_1(k)$ and feedwater valve command $u_2(k)$.\n- **Output**: Steam drum pressure, measured with additive sensor noise.\n- **Model Structure**: Nonlinear output-error (OE) model.\n- **Predicted Output**: $\\hat{y}(k)$, a function of the input sequence.\n- **Residual Definition**: $e(k) = y(k) - \\hat{y}(k)$, where $y(k)$ is the measured output.\n- **Standard Assumptions**:\n    1. Sensor noise is additive, zero-mean, and independent of inputs and past outputs. In the context of time-series analysis, this implies the noise is a white noise process.\n    2. The model structure is capable of representing the true input-output dynamics.\n- **Validation Method**: Autocorrelation of the residuals, $e(k)$.\n- **Experimental Data**:\n    - Data length, $N = 900$.\n    - Data collected under persistently exciting operation.\n- **Computed Statistics**:\n    - Sample autocorrelation of residuals at lag $1$: $\\hat{r}_e(1) = 0.08$.\n    - Sample autocorrelation of residuals at lag $2$: $\\hat{r}_e(2) = -0.02$.\n    - Sample autocorrelation of residuals at lag $3$: $\\hat{r}_e(3) = 0.05$.\n    - Sample autocorrelation of residuals at lag $5$: $\\hat{r}_e(5) = 0.01$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientific Grounding**: The problem is well-grounded in the established field of system identification, a core discipline in control engineering and data-driven modeling. The use of output-error models, residual analysis, and autocorrelation tests for model validation are standard, theoretically sound practices. The physical context of a boiler is realistic for such an application.\n- **Well-Posed**: The problem is clearly stated. It provides a specific model structure (nonlinear OE), a set of standard assumptions, experimental data ($N=900$), and computed results ($\\hat{r}_e(\\tau)$). The question asks for an evaluation of several statements based on these givens, which is a well-defined task.\n- **Objective**: The problem is expressed in precise, objective, and technical language. There are no subjective or ambiguous terms.\n\n### Step 3: Verdict and Action\nThe problem statement is valid. It is scientifically sound, self-contained, and well-posed. A rigorous analysis of the options is possible based on the provided information and standard system identification theory. Proceeding to solution derivation and option analysis.\n\n### Derivation of Principles for Solution\nThe core of this problem lies in understanding the properties of the residuals for a correctly specified output-error (OE) model.\n\nThe true physical system can be represented as:\n$$y(k) = y_{true}(k) + v(k)$$\nwhere $y(k)$ is the measured output, $y_{true}(k)$ is the noise-free output of the boiler, and $v(k)$ is the additive sensor noise. The problem states that $v(k)$ is a zero-mean white noise process, meaning its autocorrelation function $R_v(\\tau) = E[v(k)v(k+\\tau)]$ is zero for all non-zero lags $\\tau \\neq 0$.\n\nThe nonlinear OE model predicts the output based solely on past and present inputs:\n$$\\hat{y}(k) = g(u(k), u(k-1), \\dots; \\theta)$$\nwhere $\\theta$ is the vector of model parameters.\n\nThe residual is defined as the difference between the measured output and the predicted output:\n$$e(k) = y(k) - \\hat{y}(k)$$\n\nSubstituting the expression for $y(k)$, we get:\n$$e(k) = (y_{true}(k) - \\hat{y}(k)) + v(k)$$\n\nThe problem makes a crucial assumption: \"the model structure is capable of representing the true input-output dynamics\". This means there exists a parameter vector, $\\theta_0$, such that the model perfectly describes the noise-free system, i.e., $g(u; \\theta_0) = y_{true}(k)$. If the identification procedure is successful, the estimated parameter vector $\\hat{\\theta}$ will converge to $\\theta_0$. In this ideal case of an \"adequately identified model\", we have $\\hat{y}(k) \\approx y_{true}(k)$.\n\nConsequently, the residual simplifies to:\n$$e(k) \\approx y_{true}(k) + v(k) - y_{true}(k) = v(k)$$\n\nThis means that for an adequate OE model, the residual sequence $e(k)$ should have the same statistical properties as the measurement noise $v(k)$. Since $v(k)$ is assumed to be white noise, the residuals $e(k)$ must also be a white noise sequence.\n\nA key property of a white noise sequence is that its theoretical autocorrelation is zero for all non-zero lags. When we compute the *sample* autocorrelation $\\hat{r}_e(\\tau)$ from a finite data set of length $N$, statistical fluctuations will cause $\\hat{r}_e(\\tau)$ to be non-zero. However, for large $N$, if $e(k)$ is truly a white noise sequence, $\\hat{r}_e(\\tau)$ (for $\\tau \\neq 0$) is approximately a normally distributed random variable with mean $0$ and variance $1/N$. A standard statistical test for whiteness involves checking if the sample autocorrelations lie within a confidence interval, typically the $95\\%$ interval, which is given by $\\pm 1.96/\\sqrt{N}$. If a value of $\\hat{r}_e(\\tau)$ falls outside this interval, we reject the hypothesis that the residuals are white, which implies the model is inadequate.\n\n### Option-by-Option Analysis\n\n**A. The residual is defined as $e(k) = y(k) - \\hat{y}(k)$; for an adequately identified nonlinear output-error boiler model with additive, zero-mean sensor noise, $e(k)$ should behave approximately like a white noise sequence, meaning the true autocorrelation is $0$ for all non-zero lags, and sample autocorrelations at non-zero lags should be statistically indistinguishable from $0$.**\nThis statement correctly defines the residual. Based on the derivation above, if the OE model structure is correct and the parameters are well-estimated, the residuals $e(k)$ should approximate the sensor noise $v(k)$. Since $v(k)$ is assumed to be white, $e(k)$ should also be white. A white noise sequence is characterized by zero autocorrelation for all non-zero lags. The phrase \"statistically indistinguishable from $0$\" correctly reflects that sample autocorrelations are random variables and must be evaluated using a statistical hypothesis test against a confidence interval. This statement is a perfect summary of the underlying theory.\n**Verdict: Correct.**\n\n**B. Because the model is nonlinear and optimized by minimizing a sum of squared errors, a good fit implies $e(k)$ is identically $0$ or nearly $0$ at all times; any non-zero residual autocorrelation arises only from imperfections in numerical optimization, not from model structure inadequacy.**\nThis statement is incorrect. A good fit does not imply $e(k)$ is zero. The model predicts the noise-free part of the output, $\\hat{y}(k) \\approx y_{true}(k)$. The residual $e(k) \\approx v(k)$ will fluctuate with the sensor noise, which is generally not zero. The primary reason for significant, structured (non-white) residual autocorrelation is model inadequacy, i.e., $y_{true}(k) - \\hat{y}(k)$ is a non-zero, dynamically structured error term. While numerical optimization errors can contribute, attributing all non-zero autocorrelation to them and excluding model inadequacy is a fundamental error.\n**Verdict: Incorrect.**\n\n**C. In an output-error model, the residual autocorrelation at lag $0$ must be $0$ by definition, since $\\hat{y}(k)$ attempts to track $y(k)$; therefore any non-zero $\\hat{r}_e(0)$ contradicts model adequacy.**\nThis statement is fundamentally flawed. The sample autocorrelation function is defined as $\\hat{r}_e(\\tau) = \\hat{R}_e(\\tau) / \\hat{R}_e(0)$, where $\\hat{R}_e(\\tau)$ is the sample autocovariance. By this definition, the value at lag $0$ is $\\hat{r}_e(0) = \\hat{R}_e(0) / \\hat{R}_e(0) = 1$, provided the variance of the residuals $\\hat{R}_e(0)$ is non-zero (which it will be in the presence of noise). The value $\\hat{r}_e(0)$ is always $1$ and carries no information about model adequacy. The test for adequacy involves examining the autocorrelations at *non-zero* lags.\n**Verdict: Incorrect.**\n\n**D. Given $N = 900$ and $\\hat{r}_e(1) = 0.08$, the residual autocorrelation test indicates inadequacy at a conventional $95\\%$ significance level, because under the hypothesis that $e(k)$ is white, sample autocorrelations at non-zero lags are approximately centered at $0$ and a value of $0.08$ at lag $1$ is too large to be attributed to sampling variability alone.**\nThis statement proposes a quantitative application of the principles discussed. Let's verify it. The null hypothesis ($H_0$) is that the residuals are white noise.\nThe data length is $N = 900$.\nThe approximate standard deviation for the sample autocorrelation $\\hat{r}_e(\\tau)$ (for $\\tau \\neq 0$) under $H_0$ is $1/\\sqrt{N}$.\nThe $95\\%$ confidence interval for $\\hat{r}_e(\\tau)$ is approximately $\\pm 1.96 \\times (1/\\sqrt{N})$.\nCalculating the bounds: $\\pm 1.96 / \\sqrt{900} = \\pm 1.96 / 30 \\approx \\pm 0.0653$.\nThe reported value is $\\hat{r}_e(1) = 0.08$.\nSince $|\\hat{r}_e(1)| = 0.08$ is greater than the confidence bound of $0.0653$, this value is statistically significant at the $95\\%$ level. We therefore reject the null hypothesis that the residuals are white noise. This indicates that the model is inadequate. The reasoning provided in the statement is sound and the conclusion is correct based on the data.\n**Verdict: Correct.**\n\n**E. Residual autocorrelation is irrelevant for output-error structures in boiler identification, because residuals are always shaped by the model dynamics and therefore cannot reveal unmodeled dynamics; adequacy should be judged solely by the magnitude of the sum of squared residuals and not by their temporal correlation.**\nThis statement is the antithesis of correct system identification practice. The very reason residual autocorrelation is a powerful validation tool for OE models is that the residuals are *not* necessarily whitened by the model structure. If the model is correct, residuals are white. If the model is incorrect (i.e., there are unmodeled dynamics), the term $y_{true}(k) - \\hat{y}(k)$ is non-zero and possesses a temporal structure, which will manifest as non-zero correlation in the residuals $e(k)$. Judging adequacy solely by the sum of squared residuals is insufficient, as it provides no information about whether the errors are systematic or random.\n**Verdict: Incorrect.**\n\nIn summary, statements A and D are correct descriptions of the theory and application of residual analysis for output-error models.",
            "answer": "$$\\boxed{AD}$$"
        }
    ]
}