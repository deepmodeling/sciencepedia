{
    "hands_on_practices": [
        {
            "introduction": "The first step in any data-driven monitoring system is to transform raw measurements into meaningful features. In power systems, the Rate of Change of Frequency (RoCoF) is a vital indicator of grid stress and instability, making it a crucial input for diagnostic models. This exercise provides hands-on practice in calculating RoCoF from a discrete time-series of Phasor Measurement Unit (PMU) data using a least-squares estimation method, a fundamental skill in signal processing for grid applications .",
            "id": "4083510",
            "problem": "A Phasor Measurement Unit (PMU) monitors grid frequency for a nominally $50\\,\\text{Hz}$ power system. The PMU sampling rate is $50$ samples per second, so each sample occurs at time $t_k = k \\times 0.02\\,\\text{s}$ for integer $k \\geq 0$. A deep learning fault diagnosis pipeline combines a Long Short-Term Memory (LSTM) network with a threshold-based early warning module. One engineered feature fed to the LSTM is the Rate of Change of Frequency (RoCoF), which, for the PMUâ€™s discrete-time measurements, is computed at each sample by the slope of the best-fit line (ordinary least squares) of frequency versus time over a trailing window of $M = 5$ samples.\n\nAn event occurs that induces a linear ramp in frequency due to a net generation trip. The PMU reports the following frequency samples $f_k$ (in $\\text{Hz}$) at times $t_k$ (in $\\text{s}$), with the ramp starting at $t = 0.10\\,\\text{s}$:\n$$\n\\begin{aligned}\n&k: &&0 &&1 &&2 &&3 &&4 &&5 &&6 &&7 &&8 &&9 &&10 &&11 &&12\\\\\n&t_k: &&0 &&0.02 &&0.04 &&0.06 &&0.08 &&0.10 &&0.12 &&0.14 &&0.16 &&0.18 &&0.20 &&0.22 &&0.24\\\\\n&f_k: &&50.000 &&50.000 &&50.000 &&50.000 &&50.000 &&50.000 &&49.988 &&49.976 &&49.964 &&49.952 &&49.940 &&49.928 &&49.916\n\\end{aligned}\n$$\nThe threshold-based early warning module triggers when the magnitude of the estimated RoCoF (computed as the slope over the most recent $M = 5$ samples) first exceeds the threshold $\\theta = 0.4\\,\\text{Hz}\\cdot \\text{s}^{-1}$.\n\nStarting from first principles, use the definition of RoCoF as the time derivative of frequency, and its discrete-time estimation via least-squares slope over the specified window, to determine:\n- The RoCoF estimate (in $\\text{Hz}\\cdot \\text{s}^{-1}$) at the earliest sample where the threshold condition is met.\n- The earliest detection time (in $\\text{s}$), defined as the time stamp $t_k$ of that sample.\n\nRound both requested quantities to four significant figures. Express the detection time in seconds.",
            "solution": "The problem requires the calculation of the Rate of Change of Frequency (RoCoF) from a discrete series of frequency measurements. The RoCoF is estimated as the slope of the best-fit line using an ordinary least squares (OLS) regression over a trailing window of $M=5$ samples. We must find the first time instant $t_k$ at which the magnitude of this estimated RoCoF exceeds a threshold $\\theta = 0.4\\,\\text{Hz}\\cdot \\text{s}^{-1}$.\n\nFirst, let's establish the formula for the OLS slope. For a set of $M$ data points $(x_i, y_i)$, the slope $b$ of the best-fit line $y = a + bx$ is given by:\n$$\nb = \\frac{M \\sum_{i=1}^{M} x_i y_i - (\\sum_{i=1}^{M} x_i) (\\sum_{i=1}^{M} y_i)}{M \\sum_{i=1}^{M} x_i^2 - (\\sum_{i=1}^{M} x_i)^2}\n$$\nIn our case, the data points are time and frequency, $(t_j, f_j)$. The time samples are uniformly spaced with a sampling period of $\\Delta t = 0.02\\,\\text{s}$. This uniformity allows for a significant simplification of the slope formula.\n\nLet us consider a window of $M=5$ samples ending at index $k$. The samples in this window are indexed from $k-4$ to $k$. To simplify the OLS formula, we can define a local, centered time variable for the window. Let the indices within the window be $j \\in \\{-2, -1, 0, 1, 2\\}$, corresponding to sample indices $\\{k-4, k-3, k-2, k-1, k\\}$. The time coordinates relative to the center of the window ($t_{k-2}$) are $\\tau_j = j \\Delta t$. With this centered time variable, $\\sum \\tau_j = 0$, and the slope formula simplifies to:\n$$\n\\text{RoCoF}_k = b = \\frac{\\sum_{j=-2}^{2} \\tau_j f_{k-2+j}}{\\sum_{j=-2}^{2} \\tau_j^2}\n$$\nThe denominator is a constant for a fixed window size and sampling rate:\n$$\n\\sum_{j=-2}^{2} \\tau_j^2 = \\sum_{j=-2}^{2} (j \\Delta t)^2 = (\\Delta t)^2 \\sum_{j=-2}^{2} j^2 = (\\Delta t)^2 ((-2)^2 + (-1)^2 + 0^2 + 1^2 + 2^2) = 10(\\Delta t)^2\n$$\nSubstituting $\\Delta t = 0.02\\,\\text{s}$, the denominator is $10 \\times (0.02)^2 = 10 \\times 0.0004 = 0.004\\,\\text{s}^2$.\n\nThe numerator is:\n$$\n\\sum_{j=-2}^{2} \\tau_j f_{k-2+j} = \\Delta t \\sum_{j=-2}^{2} j f_{k-2+j} = \\Delta t (-2f_{k-4} - 1f_{k-3} + 0f_{k-2} + 1f_{k-1} + 2f_k)\n$$\nCombining the numerator and denominator, the RoCoF at sample $k$ is:\n$$\n\\text{RoCoF}_k = \\frac{\\Delta t (-2f_{k-4} - f_{k-3} + f_{k-1} + 2f_k)}{10(\\Delta t)^2} = \\frac{-2f_{k-4} - f_{k-3} + f_{k-1} + 2f_k}{10 \\Delta t}\n$$\nSince $10 \\Delta t = 10 \\times 0.02 = 0.2\\,\\text{s}$, the formula becomes:\n$$\n\\text{RoCoF}_k = \\frac{1}{0.2} (-2f_{k-4} - f_{k-3} + f_{k-1} + 2f_k) = 5(-2f_{k-4} - f_{k-3} + f_{k-1} + 2f_k)\n$$\nWe can now apply this formula iteratively, starting from the first possible window (ending at $k=4$), until the condition $|\\text{RoCoF}_k| > 0.4\\,\\text{Hz}\\cdot \\text{s}^{-1}$ is met. The frequency data $f_k$ is provided.\n\nFor $k < 6$, all frequencies in any $5$-sample window are $50.000\\,\\text{Hz}$. The slope of a constant function is zero.\nFor $k=4$, the window is $\\{f_0, f_1, f_2, f_3, f_4\\}$, all $50.000\\,\\text{Hz}$. Thus, $\\text{RoCoF}_4 = 0$.\nFor $k=5$, the window is $\\{f_1, f_2, f_3, f_4, f_5\\}$, all $50.000\\,\\text{Hz}$. Thus, $\\text{RoCoF}_5 = 0$.\nThe magnitude $|\\text{RoCoF}_k|$ is $0$, which is not greater than $0.4\\,\\text{Hz}\\cdot \\text{s}^{-1}$.\n\nFor $k=6$: The window samples are $\\{f_2, f_3, f_4, f_5, f_6\\}$.\nFrequencies: $\\{50.000, 50.000, 50.000, 50.000, 49.988\\}$.\n$$\n\\text{RoCoF}_6 = 5 \\times (-2f_2 - f_3 + f_5 + 2f_6) = 5 \\times [-2(50.000) - 1(50.000) + 1(50.000) + 2(49.988)]\n$$\n$$\n\\text{RoCoF}_6 = 5 \\times [-100.000 + 2(49.988)] = 5 \\times [-100.000 + 99.976] = 5 \\times (-0.024) = -0.12\\,\\text{Hz}\\cdot \\text{s}^{-1}\n$$\n$|\\text{RoCoF}_6| = 0.12\\,\\text{Hz}\\cdot \\text{s}^{-1}$, which is not greater than $0.4\\,\\text{Hz}\\cdot \\text{s}^{-1}$.\n\nFor $k=7$: The window samples are $\\{f_3, f_4, f_5, f_6, f_7\\}$.\nFrequencies: $\\{50.000, 50.000, 50.000, 49.988, 49.976\\}$.\n$$\n\\text{RoCoF}_7 = 5 \\times (-2f_3 - f_4 + f_6 + 2f_7) = 5 \\times [-2(50.000) - 1(50.000) + 1(49.988) + 2(49.976)]\n$$\n$$\n\\text{RoCoF}_7 = 5 \\times [-150.000 + 49.988 + 99.952] = 5 \\times [-150.000 + 149.940] = 5 \\times (-0.060) = -0.30\\,\\text{Hz}\\cdot \\text{s}^{-1}\n$$\n$|\\text{RoCoF}_7| = 0.30\\,\\text{Hz}\\cdot \\text{s}^{-1}$, which is not greater than $0.4\\,\\text{Hz}\\cdot \\text{s}^{-1}$.\n\nFor $k=8$: The window samples are $\\{f_4, f_5, f_6, f_7, f_8\\}$.\nFrequencies: $\\{50.000, 50.000, 49.988, 49.976, 49.964\\}$.\n$$\n\\text{RoCoF}_8 = 5 \\times (-2f_4 - f_5 + f_7 + 2f_8) = 5 \\times [-2(50.000) - 1(50.000) + 1(49.976) + 2(49.964)]\n$$\n$$\n\\text{RoCoF}_8 = 5 \\times [-150.000 + 49.976 + 99.928] = 5 \\times [-150.000 + 149.904] = 5 \\times (-0.096) = -0.48\\,\\text{Hz}\\cdot \\text{s}^{-1}\n$$\n$|\\text{RoCoF}_8| = 0.48\\,\\text{Hz}\\cdot \\text{s}^{-1}$. This value is greater than the threshold of $0.4\\,\\text{Hz}\\cdot \\text{s}^{-1}$.\nThis is the first sample index at which the threshold condition is met.\n\nThe requested quantities are:\n1. The RoCoF estimate at this sample: $\\text{RoCoF}_8 = -0.48\\,\\text{Hz}\\cdot \\text{s}^{-1}$.\n2. The earliest detection time, which is the timestamp of this sample: $t_8$.\nThe times are given by $t_k = k \\times 0.02\\,\\text{s}$.\n$t_8 = 8 \\times 0.02\\,\\text{s} = 0.16\\,\\text{s}$.\n\nThe problem requires rounding both quantities to four significant figures.\n- RoCoF estimate: $-0.48\\,\\text{Hz}\\cdot \\text{s}^{-1}$ rounded to four significant figures is $-0.4800\\,\\text{Hz}\\cdot \\text{s}^{-1}$.\n- Detection time: $0.16\\,\\text{s}$ rounded to four significant figures is $0.1600\\,\\text{s}$.\nThe final answers are therefore $-0.4800$ and $0.1600$.",
            "answer": "$$\\boxed{\\begin{pmatrix} -0.4800 & 0.1600 \\end{pmatrix}}$$"
        },
        {
            "introduction": "Once features are engineered, they are fed into a predictive model. For sequential data like PMU streams, Long Short-Term Memory (LSTM) networks are a common and powerful architectural choice due to their ability to capture temporal dependencies. This practice demystifies the internal structure of an LSTM by guiding you through the calculation of its trainable parameters, which is essential for understanding model capacity, computational cost, and potential for overfitting .",
            "id": "4083509",
            "problem": "A transmission operator is deploying a deep learning pipeline for real-time grid monitoring and fault diagnosis using Phasor Measurement Unit (PMU) data streams. The core temporal encoder is a single-layer Long Short-Term Memory (LSTM) network, where Long Short-Term Memory (LSTM) denotes a specific form of recurrent neural network with gated state updates. The PMU provides synchronized time-stamped feature vectors per time step. From foundational definitions of the LSTM recurrence, each of the input, forget, output, and candidate gates computes an affine map of the current input and previous hidden state followed by a nonlinearity, and the cell and hidden states are updated via elementwise operations. Assume the following system and modeling assumptions:\n- The input at each time step is a feature vector of dimension $d=16$.\n- The LSTM hidden state has dimension $h=64$.\n- The PMU sequence length is $T=200$ time steps.\n- Each gate uses a single input-to-hidden weight matrix, a single hidden-to-hidden weight matrix, and a single bias vector; there are no peephole connections, no projection layer, and no layer normalization.\n- The same parameters are reused at all time steps, consistent with time-invariant recurrent models.\n\nStarting from the standard LSTM gating and state-update definitions and the above assumptions, derive the total number of trainable parameters in this single-layer LSTM core (counting all gates and all biases), and then compute its numerical value. Express your final answer as an integer parameter count. No rounding is necessary, and there are no physical units associated with the answer. Exclude any input embedding, output (decoder) layers, or additional heads from your count.",
            "solution": "The goal is to calculate the total number of trainable parameters in a single-layer LSTM network core, given its input dimension, hidden state dimension, and architectural constraints. The number of parameters in a neural network layer is the count of all its weights and biases. Due to the time-invariant nature of recurrent neural networks, the parameters are shared across all time steps, meaning the sequence length $T$ does not affect the total parameter count.\n\nA standard LSTM cell consists of four main components that compute transformations of the current input vector $x_t \\in \\mathbb{R}^d$ and the previous hidden state vector $h_{t-1} \\in \\mathbb{R}^h$. These components are the input gate ($i_t$), forget gate ($f_t$), output gate ($o_t$), and the candidate cell state block ($g_t$). For each of these components, an affine transformation is computed. Let us denote the weight matrices for the input $x_t$ as $W$ and the weight matrices for the previous hidden state $h_{t-1}$ as $U$. Each gate also has a bias vector $b$.\n\nThe equations for the four components are as follows:\n1.  **Input Gate**: $i_t = \\sigma(W_i x_t + U_i h_{t-1} + b_i)$\n2.  **Forget Gate**: $f_t = \\sigma(W_f x_t + U_f h_{t-1} + b_f)$\n3.  **Candidate Cell State**: $g_t = \\tanh(W_g x_t + U_g h_{t-1} + b_g)$\n4.  **Output Gate**: $o_t = \\sigma(W_o x_t + U_o h_{t-1} + b_o)$\n\nIn these equations, $\\sigma$ denotes the sigmoid activation function and $\\tanh$ denotes the hyperbolic tangent activation function. The state updates are then given by:\nCell State update: $c_t = f_t \\odot c_{t-1} + i_t \\odot g_t$\nHidden State update: $h_t = o_t \\odot \\tanh(c_t)$\nwhere $\\odot$ represents element-wise multiplication.\n\nThe trainable parameters are the weight matrices $W_i, W_f, W_g, W_o$, the recurrent weight matrices $U_i, U_f, U_g, U_o$, and the bias vectors $b_i, b_f, b_g, b_o$.\n\nLet us determine the dimensions of these parameter tensors based on the input dimension $d$ and hidden dimension $h$.\n- The input vector $x_t$ has dimension $d$.\n- The hidden state vector $h_{t-1}$ has dimension $h$.\n- The output of each gate's affine transformation (e.g., $W_i x_t + U_i h_{t-1} + b_i$) must have dimension $h$ to be combined with other states of dimension $h$.\n\nFor a single gate (e.g., the input gate):\n- The term $W_i x_t$ must result in a vector of dimension $h$. Since $x_t$ has dimension $d$, the matrix $W_i$ must have dimensions $h \\times d$. The number of parameters in $W_i$ is therefore $h \\cdot d$.\n- The term $U_i h_{t-1}$ must result in a vector of dimension $h$. Since $h_{t-1}$ has dimension $h$, the matrix $U_i$ must have dimensions $h \\times h$. The number of parameters in $U_i$ is $h^2$.\n- The bias vector $b_i$ is added to the result, so it must have dimension $h$. The number of parameters in $b_i$ is $h$.\n\nThus, the total number of parameters for a single gate component (like the input gate) is the sum of the parameters from its input weight matrix, recurrent weight matrix, and bias vector:\n$$N_{\\text{gate}} = (h \\cdot d) + (h \\cdot h) + h = hd + h^2 + h$$\n\nSince there are four such components (input, forget, candidate, and output), and each has an identical parameter structure, the total number of parameters $N_{\\text{total}}$ in the LSTM layer is four times the number of parameters for a single gate:\n$$N_{\\text{total}} = 4 \\times N_{\\text{gate}}$$\n$$N_{\\text{total}} = 4 \\times (hd + h^2 + h)$$\n\nNow, we substitute the values provided in the problem statement:\n- Input feature dimension: $d = 16$\n- Hidden state dimension: $h = 64$\n\nSubstituting these values into the derived formula:\n$$N_{\\text{total}} = 4 \\times ((64 \\cdot 16) + 64^2 + 64)$$\n\nFirst, we calculate the terms inside the parenthesis:\n- $64 \\cdot 16 = 1024$\n- $64^2 = 4096$\n- The bias term is $64$.\n\nSumming these terms:\n$$1024 + 4096 + 64 = 5120 + 64 = 5184$$\n\nFinally, we multiply by $4$:\n$$N_{\\text{total}} = 4 \\times 5184$$\n$$N_{\\text{total}} = 20736$$\n\nThe total number of trainable parameters in the single-layer LSTM core is $20736$. This counts all weight matrices and bias vectors across the four gates, while correctly accounting for the standard parameter sharing across the time dimension $T$.",
            "answer": "$$\\boxed{20736}$$"
        },
        {
            "introduction": "Building and training a model is only half the battle; we must also rigorously evaluate its performance using appropriate metrics. Fault diagnosis is a classic example of a class-imbalance problem where \"fault\" events are rare, and conventional metrics like accuracy can be dangerously misleading. This exercise demonstrates the importance of using robust evaluation metrics like the F1-score and Matthews Correlation Coefficient (MCC) to gain a true understanding of a classifier's performance in a realistic setting .",
            "id": "4083465",
            "problem": "A utility-scale distribution grid is monitored by time-synchronized Phasor Measurement Units (PMU) producing high-rate measurements. A Convolutional Neural Network (CNN) is trained to detect transient line faults from labeled PMU time windows. Over a deployment week, the CNN produces a binary decision $X \\in \\{0,1\\}$ for each time window, and the ground-truth label is $Y \\in \\{0,1\\}$, where $X=1$ and $Y=1$ denote a fault window.\n\nThe evaluation set contains $N$ independent windows with severe class imbalance typical of fault monitoring: the confusion matrix counts are\n- true positives $TP = 600$,\n- false positives $FP = 400$,\n- false negatives $FN = 1{,}400$,\n- true negatives $TN = 199{,}600$,\nso the total number of windows is $N = TP + FP + FN + TN = 202{,}000$, with a positive class base rate $2{,}000$ out of $202{,}000$.\n\nStarting only from first principles and core definitions appropriate to classification and correlation (namely, the Pearson product-moment correlation coefficient definition for binary random variables, and the definitions of precision, recall, and harmonic mean), derive the Matthews correlation coefficient (MCC) for this binary confusion matrix and the F1-score. Then compute the ratio $\\text{MCC}/\\text{F1}$ for the given counts.\n\nExpress the final ratio as a pure number with no units, and round your answer to four significant figures.",
            "solution": "### Derivation and Calculation\n\n**1. Derivation of the F1-Score**\n\nThe F1-score is defined as the harmonic mean of precision ($P$) and recall ($R$).\nFirst, we define precision and recall in terms of the confusion matrix elements:\n-   Precision ($P$): The fraction of positive predictions that are correct.\n    $$P = \\frac{TP}{TP + FP}$$\n-   Recall ($R$): The fraction of actual positives that are correctly identified. Also known as sensitivity.\n    $$R = \\frac{TP}{TP + FN}$$\nThe harmonic mean of two numbers $a$ and $b$ is given by $H = \\frac{2}{\\frac{1}{a} + \\frac{1}{b}} = \\frac{2ab}{a+b}$.\nSubstituting $P$ and $R$ for $a$ and $b$ gives the F1-score:\n$$F1 = \\frac{2PR}{P+R} = \\frac{2 \\left(\\frac{TP}{TP+FP}\\right) \\left(\\frac{TP}{TP+FN}\\right)}{\\frac{TP}{TP+FP} + \\frac{TP}{TP+FN}}$$\nTo simplify, we multiply the numerator and denominator by $(TP+FP)(TP+FN)$:\n$$F1 = \\frac{2(TP)^2}{TP(TP+FN) + TP(TP+FP)} = \\frac{2(TP)^2}{TP^2 + TP \\cdot FN + TP^2 + TP \\cdot FP}$$\nFactoring out $TP$ from the denominator (assuming $TP \\neq 0$):\n$$F1 = \\frac{2TP}{TP+FN + TP+FP} = \\frac{2TP}{2TP + FP + FN}$$\nThis is the standard formula for the F1-score.\n\n**2. Derivation of the Matthews Correlation Coefficient (MCC)**\n\nThe MCC for binary classification is equivalent to the Pearson product-moment correlation coefficient $\\rho_{X,Y}$ for the binary variables $X$ (prediction) and $Y$ (truth), where $X, Y \\in \\{0, 1\\}$.\nThe formula for $\\rho_{X,Y}$ is:\n$$\\rho_{X,Y} = \\frac{\\text{cov}(X, Y)}{\\sigma_X \\sigma_Y} = \\frac{E[XY] - E[X]E[Y]}{\\sqrt{\\text{Var}(X)} \\sqrt{\\text{Var}(Y)}}$$\nThe expectation values are estimated from the confusion matrix counts, with $N = TP+FP+FN+TN$:\n-   $E[X] = P(X=1) = \\frac{TP+FP}{N}$\n-   $E[Y] = P(Y=1) = \\frac{TP+FN}{N}$\n-   $XY=1$ only if $X=1$ and $Y=1$, so $E[XY] = P(X=1, Y=1) = \\frac{TP}{N}$\n\nThe numerator, the covariance, is:\n$$\\text{cov}(X, Y) = \\frac{TP}{N} - \\left(\\frac{TP+FP}{N}\\right)\\left(\\frac{TP+FN}{N}\\right) = \\frac{N \\cdot TP - (TP+FP)(TP+FN)}{N^2}$$\nExpanding the terms:\n$$N \\cdot TP - (TP^2 + TP \\cdot FN + FP \\cdot TP + FP \\cdot FN)$$\nSubstituting $N = TP+FP+FN+TN$:\n$$(TP+FP+FN+TN)TP - (TP^2 + TP \\cdot FN + FP \\cdot TP + FP \\cdot FN)$$\n$$= (TP^2+FP \\cdot TP+FN \\cdot TP+TN \\cdot TP) - (TP^2 + TP \\cdot FN + FP \\cdot TP + FP \\cdot FN)$$\n$$= TN \\cdot TP - FP \\cdot FN$$\nThus, the numerator is $\\frac{TP \\cdot TN - FP \\cdot FN}{N^2}$.\n\nThe denominator involves the variances. For a binary variable $A$, $\\text{Var}(A) = E[A^2] - (E[A])^2$. Since $A^2=A$ for $A \\in \\{0, 1\\}$, $\\text{Var}(A) = E[A] - (E[A])^2 = E[A](1-E[A]) = P(A=1)P(A=0)$.\n-   $\\text{Var}(X) = P(X=1)P(X=0) = \\left(\\frac{TP+FP}{N}\\right)\\left(\\frac{TN+FN}{N}\\right)$\n-   $\\text{Var}(Y) = P(Y=1)P(Y=0) = \\left(\\frac{TP+FN}{N}\\right)\\left(\\frac{TN+FP}{N}\\right)$\nThe product of the standard deviations is:\n$$\\sigma_X \\sigma_Y = \\sqrt{\\text{Var}(X)\\text{Var}(Y)} = \\sqrt{\\frac{(TP+FP)(TN+FN)}{N^2} \\frac{(TP+FN)(TN+FP)}{N^2}}$$\n$$= \\frac{\\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}{N^2}$$\n\nCombining the numerator and denominator gives the MCC:\n$$\\text{MCC} = \\rho_{X,Y} = \\frac{\\frac{TP \\cdot TN - FP \\cdot FN}{N^2}}{\\frac{\\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}{N^2}}$$\n$$\\text{MCC} = \\frac{TP \\cdot TN - FP \\cdot FN}{\\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}$$\n\n**3. Numerical Calculation**\n\nGiven values are $TP = 600$, $FP = 400$, $FN = 1{,}400$, and $TN = 199{,}600$.\n\n-   **F1-Score Calculation**:\n    $$F1 = \\frac{2 \\times 600}{2 \\times 600 + 400 + 1{,}400} = \\frac{1{,}200}{1{,}200 + 400 + 1{,}400} = \\frac{1{,}200}{3{,}000} = 0.4$$\n\n-   **MCC Calculation**:\n    -   Numerator: $TP \\cdot TN - FP \\cdot FN = (600)(199{,}600) - (400)(1{,}400) = 119{,}760{,}000 - 560{,}000 = 119{,}200{,}000$.\n    -   Denominator terms:\n        -   $TP+FP = 600 + 400 = 1{,}000$\n        -   $TP+FN = 600 + 1{,}400 = 2{,}000$\n        -   $TN+FP = 199{,}600 + 400 = 200{,}000$\n        -   $TN+FN = 199{,}600 + 1{,}400 = 201{,}000$\n    -   Denominator: $\\sqrt{(1{,}000)(2{,}000)(200{,}000)(201{,}000)} = \\sqrt{8.04 \\times 10^{16}} = \\sqrt{8.04} \\times 10^8$.\n    -   $MCC = \\frac{119{,}200{,}000}{\\sqrt{8.04} \\times 10^8} = \\frac{1.192 \\times 10^8}{\\sqrt{8.04} \\times 10^8} = \\frac{1.192}{\\sqrt{8.04}}$.\n    -   $MCC \\approx \\frac{1.192}{2.83548937...} \\approx 0.420380...$\n\n**4. Ratio Calculation**\n\nFinally, we compute the ratio $\\text{MCC}/\\text{F1}$:\n$$\\frac{\\text{MCC}}{\\text{F1}} = \\frac{0.420380...}{0.4} = 1.05095...$$\nRounding to four significant figures, the result is $1.051$.\nAlternatively, using the exact expressions:\n$$\\frac{\\text{MCC}}{\\text{F1}} = \\frac{1.192 / \\sqrt{8.04}}{0.4} = \\frac{2.98}{\\sqrt{8.04}} \\approx 1.050963...$$\nRounding to four significant figures gives $1.051$.",
            "answer": "$$\\boxed{1.051}$$"
        }
    ]
}