{
    "hands_on_practices": [
        {
            "introduction": "Environmental models often depend on inputs derived from remote sensing, such as the fractional cover of vegetation or soil within a landscape. This practice tackles a fundamental challenge: a single satellite pixel often contains a mixture of different materials. You will explore the technique of linear spectral unmixing by deriving the mathematical framework to deconstruct a pixel's measured spectrum into the proportions of its pure components, or endmembers . This exercise provides a crucial link between raw satellite data and meaningful model parameters, forming a foundational step in many modeling workflows.",
            "id": "3809793",
            "problem": "An environmental model of sub-pixel land-cover composition relies on estimating fractional abundances from multispectral remote sensing data. In a typical workflow, model conceptualization begins by defining how measured pixel reflectance is explained by endmember spectra and then formulating a physically constrained estimation problem to ensure the outputs are feasible for downstream environmental modeling.\n\nDefine linear spectral unmixing and endmembers in the context of remote sensing and environmental modeling. Starting from first principles of linear mixing and least-squares estimation under physical constraints, derive the constrained least squares abundance estimator with nonnegativity and sum-to-one constraints. Use Karush-Kuhn-Tucker (KKT) conditions to justify the structure of the solution.\n\nThen, apply the derived formulation to the following noiseless multispectral scenario (a single pixel). There are $B=5$ bands and $p=3$ endmembers: vegetation, soil, and water. The endmember matrix $E \\in \\mathbb{R}^{B \\times p}$ has columns given by the reflectance spectra of the endmembers (each value is unitless reflectance):\n- Vegetation: $\\begin{pmatrix}0.05 \\\\ 0.10 \\\\ 0.05 \\\\ 0.50 \\\\ 0.25\\end{pmatrix}$,\n- Soil: $\\begin{pmatrix}0.12 \\\\ 0.15 \\\\ 0.18 \\\\ 0.30 \\\\ 0.35\\end{pmatrix}$,\n- Water: $\\begin{pmatrix}0.02 \\\\ 0.03 \\\\ 0.02 \\\\ 0.05 \\\\ 0.03\\end{pmatrix}$.\n\nThe measured pixel reflectance vector $y \\in \\mathbb{R}^{B}$ is\n$$\ny \\;=\\; \\begin{pmatrix}0.068 \\\\ 0.108 \\\\ 0.086 \\\\ 0.395 \\\\ 0.258\\end{pmatrix}.\n$$\n\nFormulate and solve the constrained least squares abundance estimation problem\n$$\n\\min_{a \\in \\mathbb{R}^{p}} \\;\\|E\\,a - y\\|_{2}^{2}\n\\quad \\text{subject to} \\quad a \\succeq 0,\\;\\; \\mathbf{1}^{\\top} a = 1,\n$$\nwhere $a$ is the abundance vector, $a \\succeq 0$ denotes elementwise nonnegativity, and $\\mathbf{1}$ is a $p$-vector of ones. Explain each derivation step from the linear mixing model to the optimization and then to the solution characterization. Finally, compute the abundance vector for this pixel and express it as unitless fractions. Round each abundance to four significant figures, and present the final abundance vector in the form of a single row matrix using the $\\text{pmatrix}$ environment.",
            "solution": "Linear spectral unmixing is a conceptual and mathematical framework in remote sensing wherein the measured reflectance spectrum of a pixel is modeled as a linear combination of a small set of pure material spectra, called endmembers. An endmember is the characteristic reflectance spectrum of a pure land-cover type (for example, vegetation, soil, water), and the linear spectral unmixing model posits that a mixed pixel’s spectrum is a convex combination of these endmember spectra when materials are areally mixed within the instantaneous field of view and multiple scattering and nonlinear interactions are negligible.\n\nThe fundamental base for the model is the linear mixing relation with additive measurement noise,\n$$\ny \\;=\\; E\\,a \\;+\\; n,\n$$\nwhere $y \\in \\mathbb{R}^{B}$ is the measured spectrum across $B$ bands, $E \\in \\mathbb{R}^{B \\times p}$ contains $p$ endmember spectra as columns, $a \\in \\mathbb{R}^{p}$ is the abundance vector, and $n \\in \\mathbb{R}^{B}$ is additive noise (assumed zero-mean). Physical constraints on abundances for areal mixtures are nonnegativity and sum-to-one:\n$$\na \\succeq 0, \\qquad \\mathbf{1}^{\\top} a = 1.\n$$\nThese impose that the abundances form a convex combination of endmembers, consistent with the conceptual workflow in environmental modeling wherein land-cover fractions must be feasible and interpretable.\n\nGiven the linear model and Gaussian-like noise assumptions, the well-tested approach is to estimate $a$ by constrained least squares:\n$$\n\\min_{a} \\; \\|E\\,a - y\\|_{2}^{2} \\quad \\text{subject to} \\quad a \\succeq 0, \\;\\mathbf{1}^{\\top} a = 1.\n$$\nThis is a convex quadratic program over a simplex. To derive the solution structure, consider the Lagrangian\n$$\n\\mathcal{L}(a,\\mu,\\gamma) \\;=\\; \\|E\\,a - y\\|_{2}^{2} \\;+\\; \\mu \\left(\\mathbf{1}^{\\top} a - 1\\right) \\;-\\; \\gamma^{\\top} a,\n$$\nwhere $\\mu \\in \\mathbb{R}$ is the multiplier for the equality constraint and $\\gamma \\in \\mathbb{R}^{p}$ are multipliers for the inequality constraints $a \\succeq 0$ written as $-a \\preceq 0$. The Karush-Kuhn-Tucker (KKT) conditions are:\n- Stationarity:\n$$\n\\nabla_{a} \\mathcal{L} \\;=\\; 2 E^{\\top}(E\\,a - y) \\;+\\; \\mu\\,\\mathbf{1} \\;-\\; \\gamma \\;=\\; 0.\n$$\n- Primal feasibility: $a \\succeq 0$, $\\mathbf{1}^{\\top} a = 1$.\n- Dual feasibility: $\\gamma \\succeq 0$.\n- Complementary slackness: $\\gamma_{i}\\,a_{i} = 0$ for all $i \\in \\{1,\\dots,p\\}$.\n\nThese conditions characterize the constrained least squares solution. If the nonnegativity constraints are inactive (that is, all entries of $a$ are strictly positive), then $\\gamma = 0$ by complementary slackness, and the stationarity condition reduces to\n$$\n2 E^{\\top}(E\\,a - y) \\;+\\; \\mu\\,\\mathbf{1} \\;=\\; 0.\n$$\nDividing by $2$ (which does not affect the minimizer) and rearranging, we obtain\n$$\nE^{\\top} E \\, a \\;-\\; E^{\\top} y \\;+\\; \\mu \\,\\mathbf{1} \\;=\\; 0\n\\quad\\Rightarrow\\quad\na \\;=\\; (E^{\\top} E)^{-1} \\left(E^{\\top} y \\;-\\; \\mu \\,\\mathbf{1}\\right),\n$$\nassuming $E^{\\top} E$ is invertible, which holds when the endmember spectra are linearly independent across bands. Enforcing the sum-to-one constraint,\n$$\n\\mathbf{1}^{\\top} a \\;=\\; \\mathbf{1}^{\\top} (E^{\\top} E)^{-1} \\left(E^{\\top} y \\;-\\; \\mu \\,\\mathbf{1}\\right) \\;=\\; 1,\n$$\nyields a closed-form expression for the multiplier,\n$$\n\\mu \\;=\\; \\frac{\\mathbf{1}^{\\top} (E^{\\top} E)^{-1} E^{\\top} y \\;-\\; 1}{\\mathbf{1}^{\\top} (E^{\\top} E)^{-1} \\mathbf{1}}.\n$$\nSubstituting back gives the equality-constrained least squares solution\n$$\na \\;=\\; (E^{\\top} E)^{-1} E^{\\top} y \\;-\\; (E^{\\top} E)^{-1} \\mathbf{1} \\;\\frac{\\mathbf{1}^{\\top} (E^{\\top} E)^{-1} E^{\\top} y \\;-\\; 1}{\\mathbf{1}^{\\top} (E^{\\top} E)^{-1} \\mathbf{1}}.\n$$\nIf this $a$ is strictly positive elementwise, then it also satisfies the original nonnegativity constraints and the KKT system with $\\gamma = 0$, hence it is the unique minimizer due to the strict convexity of $\\|E\\,a - y\\|_{2}^{2}$.\n\nWe now apply this to the given spectral dataset. The endmember matrix is\n$$\nE \\;=\\; \\begin{pmatrix}\n0.05 & 0.12 & 0.02 \\\\\n0.10 & 0.15 & 0.03 \\\\\n0.05 & 0.18 & 0.02 \\\\\n0.50 & 0.30 & 0.05 \\\\\n0.25 & 0.35 & 0.03\n\\end{pmatrix}, \\qquad\ny \\;=\\; \\begin{pmatrix}0.068 \\\\ 0.108 \\\\ 0.086 \\\\ 0.395 \\\\ 0.258\\end{pmatrix}.\n$$\nObserve that the measured spectrum is a convex combination of the endmembers with the abundance vector\n$$\na_{\\text{true}} \\;=\\; \\begin{pmatrix}0.60 \\\\ 0.30 \\\\ 0.10\\end{pmatrix},\n$$\nsince\n$$\nE\\,a_{\\text{true}} \\;=\\; \\begin{pmatrix}\n0.05 \\cdot 0.60 + 0.12 \\cdot 0.30 + 0.02 \\cdot 0.10 \\\\\n0.10 \\cdot 0.60 + 0.15 \\cdot 0.30 + 0.03 \\cdot 0.10 \\\\\n0.05 \\cdot 0.60 + 0.18 \\cdot 0.30 + 0.02 \\cdot 0.10 \\\\\n0.50 \\cdot 0.60 + 0.30 \\cdot 0.30 + 0.05 \\cdot 0.10 \\\\\n0.25 \\cdot 0.60 + 0.35 \\cdot 0.30 + 0.03 \\cdot 0.10\n\\end{pmatrix}\n\\;=\\;\n\\begin{pmatrix}\n0.068 \\\\ 0.108 \\\\ 0.086 \\\\ 0.395 \\\\ 0.258\n\\end{pmatrix}\n\\;=\\; y.\n$$\nBecause $y$ lies exactly in the column space of $E$ and $a_{\\text{true}}$ satisfies $a_{\\text{true}} \\succeq 0$ and $\\mathbf{1}^{\\top} a_{\\text{true}} = 1$, the constrained least squares objective $\\|E\\,a - y\\|_{2}^{2}$ achieves its global minimum value of $0$ at $a_{\\text{true}}$. Strict convexity implies uniqueness of the minimizer given full column rank of $E$ (which holds here because the endmembers are linearly independent across $B$ bands), so the constrained least squares abundance estimate equals\n$$\na^{\\star} \\;=\\; a_{\\text{true}} \\;=\\; \\begin{pmatrix}0.60 \\\\ 0.30 \\\\ 0.10\\end{pmatrix}.\n$$\nSince all entries are strictly positive, the nonnegativity constraints are inactive and the KKT multipliers $\\gamma$ are $0$; the solution is consistent with the equality-constrained closed form derived above.\n\nFinally, rounding each abundance to four significant figures and expressing values as unitless fractions yields the row matrix\n$$\n\\begin{pmatrix}0.6000 & 0.3000 & 0.1000\\end{pmatrix}.\n$$",
            "answer": "$$\\boxed{\\begin{pmatrix}0.6000 & 0.3000 & 0.1000\\end{pmatrix}}$$"
        },
        {
            "introduction": "With model inputs prepared, the next step is to formulate the process model itself. This practice delves into the physics of land-atmosphere exchange by deriving the celebrated Penman-Monteith equation, a cornerstone for estimating evapotranspiration. Starting from the first principles of energy conservation, you will construct the model and then perform a sensitivity analysis to understand how its output responds to changes in key parameters . This exercise builds the essential skill of not just using models, but understanding their internal structure and behavior.",
            "id": "3809774",
            "problem": "A land surface energy balance model is being developed within a remote sensing driven workflow for a tallgrass canopy. Satellite and in situ products provide net radiation and near-surface humidity, and aerodynamic and canopy exchange parameters are supplied by a canopy-atmosphere module. You are tasked with deriving the evapotranspiration (ET) rate from first principles and quantifying its local sensitivity to canopy resistance, consistent with the Penman–Monteith conceptualization.\n\nStarting from conservation of energy at the land surface and the bulk transfer laws for sensible heat and water vapor, derive an analytic expression for the latent heat flux (latent energy) and thus for evapotranspiration, as a function of net radiation, vapor pressure deficit, aerodynamic conductance, and canopy resistance. Use the following physically based definitions in your derivation: the surface energy balance $R_{n}-G=H+\\mathrm{LE}$, the bulk transfer for sensible heat $H=\\rho c_{p} g_{a}\\left(T_{s}-T_{a}\\right)$, the linearization of saturation vapor pressure around air temperature $e_{s}\\left(T_{s}\\right)\\approx e_{s}\\left(T_{a}\\right)+s\\left(T_{s}-T_{a}\\right)$, the vapor pressure deficit $\\mathrm{VPD}=e_{s}\\left(T_{a}\\right)-e_{a}$, and the psychrometric constant $\\gamma=\\frac{c_{p}P}{\\varepsilon \\lambda}$, where $\\varepsilon$ is the ratio of the molecular weight of water to that of dry air. Express evapotranspiration $E$ as mass flux using $\\mathrm{LE}=\\lambda E$.\n\nThen, treating aerodynamic conductance and all meteorological terms as constant, compute the local log-sensitivity (elasticity) of $E$ with respect to canopy resistance $r_{c}$, defined by $\\epsilon_{r_{c}}=\\frac{\\partial E}{\\partial r_{c}}\\frac{r_{c}}{E}$, and evaluate it numerically under the following midday conditions, using strict International System of Units:\n\n- Available energy $A=R_{n}-G$ is $180\\,\\mathrm{W\\,m^{-2}}$ (use $A$ for compactness).\n- Vapor pressure deficit (VPD) is $1.2\\,\\mathrm{kPa}$.\n- Aerodynamic conductance $g_{a}$ is $0.02\\,\\mathrm{m\\,s^{-1}}$.\n- Canopy resistance $r_{c}$ is $70\\,\\mathrm{s\\,m^{-1}}$.\n- Air density $\\rho$ is $1.2\\,\\mathrm{kg\\,m^{-3}}$.\n- Specific heat of air at constant pressure $c_{p}$ is $1005\\,\\mathrm{J\\,kg^{-1}\\,K^{-1}}$.\n- Latent heat of vaporization $\\lambda$ is $2.45\\times 10^{6}\\,\\mathrm{J\\,kg^{-1}}$.\n- Slope of saturation vapor pressure curve $s$ is $0.188\\,\\mathrm{kPa\\,K^{-1}}$.\n- Psychrometric constant $\\gamma$ is $0.066\\,\\mathrm{kPa\\,K^{-1}}$.\n\nUse $1\\,\\mathrm{kPa}=1000\\,\\mathrm{Pa}$ and $1\\,\\mathrm{W}=1\\,\\mathrm{J\\,s^{-1}}$, and be explicit about how unit consistency is maintained. You may assume near-sea-level pressure and moderate temperature, so that the given $s$ and $\\gamma$ are appropriate.\n\nAs part of your reasoning, briefly justify how the structure of the derived expression reveals the competing controls of energy supply and aerodynamic–stomatal coupling on evapotranspiration, and identify which parameters most influence $E$ under the stated conditions.\n\nProvide as your single final answer the numerical value of the elasticity $\\epsilon_{r_{c}}$ as a unitless decimal, rounded to three significant figures.",
            "solution": "The problem requires the derivation of the Penman-Monteith equation for evapotranspiration, an analysis of its controlling factors, and the calculation of the sensitivity of evapotranspiration to canopy resistance.\n\n### Derivation of the Penman-Monteith Equation\n\nThe goal is to find an expression for the latent heat flux, $\\mathrm{LE}$, that eliminates the unmeasured surface temperature, $T_s$.\n\nWe begin with the provided governing equations:\n1.  Surface energy balance: $A = H + \\mathrm{LE}$\n2.  Sensible heat flux: $H = \\rho c_p g_a (T_s - T_a)$\n\nThe latent heat flux, $\\mathrm{LE}$, is the energy equivalent of the water vapor flux, $E$. This flux is driven by the vapor pressure gradient from the surface to the air, $(e_s(T_s) - e_a)$, and is resisted by two resistances in series: the aerodynamic resistance $r_a = 1/g_a$ and the canopy resistance $r_c$. The total conductance for water vapor is $g_v = 1/(r_a + r_c) = 1/(1/g_a + r_c) = g_a / (1 + g_a r_c)$. The bulk transfer formula for $\\mathrm{LE}$ is analogous to that for $H$, with the inclusion of the psychrometric constant $\\gamma$ as a conversion factor between energy and vapor pressure units:\n$$ \\mathrm{LE} = \\frac{\\rho c_p}{\\gamma} g_v (e_s(T_s) - e_a) = \\frac{\\rho c_p}{\\gamma} \\frac{g_a}{1+g_a r_c} (e_s(T_s) - e_a) $$\nThis expression for $\\mathrm{LE}$ contains the unknown surface vapor pressure $e_s(T_s)$, which depends on the unknown surface temperature $T_s$. We eliminate $T_s$ using the other equations.\n\nFrom the sensible heat flux equation (2), we isolate the surface-air temperature difference:\n$$ T_s - T_a = \\frac{H}{\\rho c_p g_a} \\quad (3) $$\nSubstitute this into the provided linearization of the saturation vapor pressure curve:\n$$ e_s(T_s) \\approx e_s(T_a) + s(T_s - T_a) = e_s(T_a) + s \\frac{H}{\\rho c_p g_a} $$\nNow, substitute this linearized $e_s(T_s)$ into the expression for $\\mathrm{LE}$:\n$$ \\mathrm{LE} = \\frac{\\rho c_p}{\\gamma} \\frac{g_a}{1+g_a r_c} \\left( \\left( e_s(T_a) + s \\frac{H}{\\rho c_p g_a} \\right) - e_a \\right) $$\nUsing the definition $\\mathrm{VPD} = e_s(T_a) - e_a$, we simplify:\n$$ \\mathrm{LE} = \\frac{\\rho c_p}{\\gamma} \\frac{g_a}{1+g_a r_c} \\left( \\mathrm{VPD} + \\frac{s H}{\\rho c_p g_a} \\right) $$\nDistributing the terms:\n$$ \\mathrm{LE} = \\frac{\\rho c_p g_a \\mathrm{VPD}}{\\gamma (1+g_a r_c)} + \\frac{s H}{\\gamma (1+g_a r_c)} $$\nWe now have a system of two linear equations with two unknowns, $\\mathrm{LE}$ and $H$:\n- $H = A - \\mathrm{LE}$\n- $\\mathrm{LE} ( \\gamma (1+g_a r_c) ) = \\rho c_p g_a \\mathrm{VPD} + s H$\n\nSubstitute the first equation into the second to eliminate $H$:\n$$ \\mathrm{LE} (\\gamma + \\gamma g_a r_c) = \\rho c_p g_a \\mathrm{VPD} + s(A - \\mathrm{LE}) $$\n$$ \\mathrm{LE} (\\gamma + \\gamma g_a r_c) = \\rho c_p g_a \\mathrm{VPD} + s A - s \\mathrm{LE} $$\nGroup all terms containing $\\mathrm{LE}$ on one side:\n$$ \\mathrm{LE} (\\gamma + \\gamma g_a r_c + s) = s A + \\rho c_p g_a \\mathrm{VPD} $$\nFinally, solve for $\\mathrm{LE}$:\n$$ \\mathrm{LE} = \\frac{s A + \\rho c_p g_a \\mathrm{VPD}}{s + \\gamma (1+g_a r_c)} $$\nThis is the Penman-Monteith equation for latent heat flux. The evapotranspiration rate $E$ (as mass flux, e.g., in $\\mathrm{kg\\,m^{-2}\\,s^{-1}}$) is obtained using $\\mathrm{LE} = \\lambda E$:\n$$ E = \\frac{1}{\\lambda} \\left( \\frac{s A + \\rho c_p g_a \\mathrm{VPD}}{s + \\gamma (1+g_a r_c)} \\right) $$\n\n### Analysis of Competing Controls\n\nThe structure of the Penman-Monteith equation reveals the competing factors controlling evapotranspiration. The numerator, $s A + \\rho c_p g_a \\mathrm{VPD}$, represents the total \"source\" for evaporation. It consists of two main components:\n1.  The **energy term**, $s A$, which represents the portion of available energy $A$ that can be converted to latent heat. The factor $s$ (the slope of the saturation vapor pressure curve) is temperature-dependent, indicating that at higher temperatures, a greater proportion of energy is partitioned into evapotranspiration.\n2.  The **aerodynamic term**, $\\rho c_p g_a \\mathrm{VPD}$, which represents the \"drying power\" of the atmosphere. It quantifies the capacity of the atmosphere to transport water vapor away from the surface, driven by the vapor pressure deficit ($\\mathrm{VPD}$) and facilitated by turbulent transport (represented by $g_a$).\n\nThe denominator, $s + \\gamma (1+g_a r_c)$, represents the total \"resistance\" to water vapor transfer. It combines the thermodynamic properties of air ($s$ and $\\gamma$) with the physical resistances of the surface ($r_c$) and the boundary layer ($r_a = 1/g_a$). The term $\\gamma g_a r_c$ explicitly couples the physiological control of the canopy ($r_c$) with the aerodynamic efficiency ($g_a$).\n\nUnder the given conditions, let's compare the magnitude of the two numerator terms in consistent SI units, with pressure in $\\mathrm{Pa}$.\n$s = 0.188\\,\\mathrm{kPa\\,K^{-1}} = 188\\,\\mathrm{Pa\\,K^{-1}}$.\n$\\mathrm{VPD} = 1.2\\,\\mathrm{kPa} = 1200\\,\\mathrm{Pa}$.\nThe energy term contribution to the numerator's driver function is $s A = 188\\,\\mathrm{Pa\\,K^{-1}} \\times 180\\,\\mathrm{W\\,m^{-2}} = 33840\\,\\mathrm{W\\,m^{-2}\\,Pa\\,K^{-1}}$.\nThe aerodynamic term contribution is $\\rho c_p g_a \\mathrm{VPD} = (1.2\\,\\mathrm{kg\\,m^{-3}})(1005\\,\\mathrm{J\\,kg^{-1}\\,K^{-1}})(0.02\\,\\mathrm{m\\,s^{-1}})(1200\\,\\mathrm{Pa}) = 28944\\,\\mathrm{W\\,m^{-2}\\,Pa\\,K^{-1}}$.\nThe two terms are of comparable magnitude ($33840$ versus $28944$), indicating that under these conditions, evapotranspiration is subject to **mixed control**. Both the available energy supply and the atmospheric vapor demand are strong, competing drivers. The canopy resistance also exerts significant control, as the term $\\gamma g_a r_c = (0.066)(0.02)(70) \\approx 0.0924$ is a substantial fraction of the total denominator term $s + \\gamma + \\gamma g_a r_c = 0.188 + 0.066 + 0.0924 = 0.3464$.\n\n### Derivation and Calculation of Elasticity\n\nThe elasticity of $E$ with respect to $r_c$ is defined as $\\epsilon_{r_{c}}=\\frac{\\partial E}{\\partial r_{c}}\\frac{r_{c}}{E}$.\nLet the expression for $E$ be written as $E = K \\frac{N}{D(r_c)}$, where $K=1/\\lambda$ and $N = s A + \\rho c_p g_a \\mathrm{VPD}$ are constants with respect to $r_c$, and the denominator is $D(r_c) = s + \\gamma (1+g_a r_c)$.\n\nFirst, we find the partial derivative $\\frac{\\partial E}{\\partial r_{c}}$ using the chain rule:\n$$ \\frac{\\partial E}{\\partial r_{c}} = K N \\frac{\\partial}{\\partial r_{c}} \\left( [D(r_c)]^{-1} \\right) = - K N [D(r_c)]^{-2} \\frac{\\partial D(r_c)}{\\partial r_c} $$\nThe derivative of the denominator is:\n$$ \\frac{\\partial D(r_c)}{\\partial r_c} = \\frac{\\partial}{\\partial r_c} (s + \\gamma + \\gamma g_a r_c) = \\gamma g_a $$\nSubstituting this back, we get:\n$$ \\frac{\\partial E}{\\partial r_{c}} = -K N \\frac{\\gamma g_a}{(s + \\gamma (1+g_a r_c))^2} $$\nNow, we form the elasticity:\n$$ \\epsilon_{r_{c}} = \\frac{\\partial E}{\\partial r_{c}}\\frac{r_{c}}{E} = \\left( -K N \\frac{\\gamma g_a}{[s + \\gamma (1+g_a r_c)]^2} \\right) \\frac{r_c}{K \\frac{N}{s + \\gamma (1+g_a r_c)}} $$\nCanceling the constant terms $K$ and $N$, and one factor of the denominator:\n$$ \\epsilon_{r_{c}} = - \\frac{\\gamma g_a r_c}{s + \\gamma (1+g_a r_c)} = - \\frac{\\gamma g_a r_c}{s + \\gamma + \\gamma g_a r_c} $$\nThis is the analytical expression for the elasticity. The negative sign correctly indicates that evapotranspiration decreases as canopy resistance increases.\n\nFinally, we substitute the given numerical values to compute $\\epsilon_{r_{c}}$:\n- $g_{a} = 0.02\\,\\mathrm{m\\,s^{-1}}$\n- $r_{c} = 70\\,\\mathrm{s\\,m^{-1}}$\n- $s = 0.188\\,\\mathrm{kPa\\,K^{-1}}$\n- $\\gamma = 0.066\\,\\mathrm{kPa\\,K^{-1}}$\n\nThe terms in the expression are:\n- Numerator: $\\gamma g_a r_c = (0.066)(0.02)(70) = 0.066 \\times 1.4 = 0.0924$\n- Denominator: $s + \\gamma + \\gamma g_a r_c = 0.188 + 0.066 + 0.0924 = 0.254 + 0.0924 = 0.3464$\n\nThe units of the numerator and denominator are both $\\mathrm{kPa\\,K^{-1}}$, so the elasticity is a dimensionless quantity as expected.\n$$ \\epsilon_{r_{c}} = - \\frac{0.0924}{0.3464} \\approx -0.2667436... $$\nRounding to three significant figures, the elasticity is $-0.267$. This means that a $1\\%$ increase in canopy resistance would lead to approximately a $0.267\\%$ decrease in evapotranspiration under these conditions.",
            "answer": "$$\\boxed{-0.267}$$"
        },
        {
            "introduction": "Neither our models nor our observations are perfect, creating uncertainty in our understanding of environmental systems. Data assimilation offers a rigorous framework to merge model predictions with incoming data, providing a more accurate and robust estimate of a system's state. This final hands-on practice guides you through the implementation of an Ensemble Kalman Filter (EnKF), a premier data assimilation method . By coding a simplified one-dimensional example for soil moisture, you will gain practical experience in dynamically updating a model's state, a technique at the heart of modern environmental forecasting.",
            "id": "3809835",
            "problem": "You are to implement a single assimilation step of a one-dimensional Ensemble Kalman Filter (EnKF) to update a soil moisture state at a single location using satellite retrievals as observations. The soil moisture state is volumetric water content, expressed as a fraction in cubic meters per cubic meter, and the satellite retrieval is assumed to directly observe this state (that is, the observation operator is the identity mapping). The program must compute the per-member analysis increments and the posterior ensemble covariance (which, in one dimension, is the ensemble variance). All numerical quantities must use International System of Units, specifically volumetric soil moisture expressed in cubic meters per cubic meter.\n\nFundamental base:\n- Begin from Bayes theorem for state estimation, where the prior state and observational noise are modeled as Gaussian random variables. Assume additive observational noise and a linear observation operator. Use the unbiased sample covariance computed from the ensemble, consistent with standard statistical practice.\n\nDefinitions and assumptions:\n- Let the prior ensemble of soil moisture states be $\\{x_i^{f}\\}_{i=1}^{N}$, where $N$ is the ensemble size and each $x_i^{f}$ is expressed in $\\mathrm{m^3/m^3}$. The prior ensemble mean and variance are computed as the unbiased sample mean and the unbiased sample variance with denominator $N-1$.\n- The observation is a single scalar $y$ (in $\\mathrm{m^3/m^3}$) and the observation error variance is $R$ (in $\\mathrm{(m^3/m^3)^2}$). The observation operator is $h(x) = x$. The observational noise is additive and Gaussian with zero mean and variance $R$.\n- Use a stochastic Ensemble Kalman Filter update with perturbed observations. For reproducibility, when generating observational perturbations, use a fixed random seed $42$ and draw $\\epsilon_i \\sim \\mathcal{N}(0, R)$ independently for each ensemble member. The perturbed observations are $y_i = y + \\epsilon_i$.\n- Compute the Kalman gain based on the unbiased sample variance of the prior ensemble and the observation error variance. If the prior sample variance is zero, set the Kalman gain to zero and the analysis increments to zero.\n- Update each ensemble member using the perturbed observations and compute the analysis increment for each member as $d_i = x_i^{a} - x_i^{f}$, where $x_i^{a}$ is the analysis state after the update. Compute the posterior ensemble variance as the unbiased sample variance of $\\{x_i^{a}\\}_{i=1}^{N}$ using denominator $N-1$.\n\nTasks:\n- Implement the described stochastic Ensemble Kalman Filter update for one-dimensional soil moisture using the specified assumptions.\n- For each provided test case, compute:\n  1. The list of per-member analysis increments $\\{d_i\\}_{i=1}^{N}$ in $\\mathrm{m^3/m^3}$.\n  2. The posterior ensemble variance (a single float) in $\\mathrm{(m^3/m^3)^2}$.\n\nUnits:\n- Soil moisture values and analysis increments must be expressed in $\\mathrm{m^3/m^3}$.\n- Posterior variance must be expressed in $\\mathrm{(m^3/m^3)^2}$.\n\nAngle units:\n- No angles are used in this problem.\n\nTest suite:\n- Case $1$ (happy path): Prior ensemble $\\left[\\,0.18,\\,0.22,\\,0.25,\\,0.19,\\,0.21,\\,0.24\\,\\right]$, observation $y = 0.23$, observation error variance $R = 0.0016$.\n- Case $2$ (observation far from prior mean): Prior ensemble $\\left[\\,0.08,\\,0.12,\\,0.09,\\,0.11,\\,0.10\\,\\right]$, observation $y = 0.20$, observation error variance $R = 0.0025$.\n- Case $3$ (zero prior spread boundary condition): Prior ensemble $\\left[\\,0.30,\\,0.30,\\,0.30,\\,0.30,\\,0.30\\,\\right]$, observation $y = 0.35$, observation error variance $R = 0.0009$.\n- Case $4$ (large observation error variance edge case): Prior ensemble $\\left[\\,0.34,\\,0.33,\\,0.35,\\,0.36,\\,0.32,\\,0.31,\\,0.37,\\,0.34\\,\\right]$, observation $y = 0.34$, observation error variance $R = 0.04$.\n\nOutput specification:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with no spaces. Each element corresponds to one test case and must be a two-element list of the form $\\left[\\,\\text{increments},\\,\\text{posterior\\_variance}\\,\\right]$, where increments is a list of floats (each in $\\mathrm{m^3/m^3}$) and posterior\\_variance is a single float (in $\\mathrm{(m^3/m^3)^2}$). For example, the output should look like $\\left[[[d_{1},d_{2},\\dots],v_{1}],[[\\dots],v_{2}],\\dots\\right]$ with no spaces, where $d_{i}$ and $v_{j}$ are decimal numbers.",
            "solution": "The problem requires the implementation of a single assimilation step for a one-dimensional stochastic Ensemble Kalman Filter (EnKF). The core of this task lies in applying the Bayesian update rule for Gaussian distributions, where the prior probability density is approximated by an ensemble of states.\n\nThe theoretical foundation is Bayes' theorem, which states that the posterior probability of the state $x$ given an observation $y$ is proportional to the product of the likelihood and the prior: $p(x|y) \\propto p(y|x) p(x)$. The problem assumes Gaussian distributions for both the prior state and the observation error.\n\nLet the prior state $x$ be a random variable with mean $x^f$ and variance $P^f$, denoted $x \\sim \\mathcal{N}(x^f, P^f)$. In the EnKF framework, this distribution is represented by a finite ensemble $\\{x_i^f\\}_{i=1}^{N}$ of $N$ state vectors. The prior statistics are estimated from this ensemble using the unbiased sample estimators:\n- Prior ensemble mean: $\\bar{x}^f = \\frac{1}{N} \\sum_{i=1}^{N} x_i^f$\n- Prior ensemble variance: $P^f = \\frac{1}{N-1} \\sum_{i=1}^{N} (x_i^f - \\bar{x}^f)^2$\n\nThe observation model is linear and involves additive Gaussian noise: $y = h(x) + \\epsilon$, where the observation operator is the identity, $h(x)=x$, and the observation error $\\epsilon$ is drawn from a zero-mean Gaussian distribution with variance $R$, i.e., $\\epsilon \\sim \\mathcal{N}(0, R)$. This defines the likelihood function $p(y|x)$ as a Gaussian with mean $h(x)=x$ and variance $R$.\n\nFor this linear-Gaussian system, the analytical solution for the posterior distribution $p(x|y)$ is also Gaussian, with an updated mean $x^a$ and variance $P^a$. The Kalman gain, $K$, is computed to provide an optimal (in the minimum mean square error sense) linear update. In this one-dimensional case, the gain is a scalar:\n$$K = \\frac{P^f}{P^f + R}$$\nThe term $P^f + R$ represents the variance of the innovation, which is the discrepancy between the observation and the model forecast.\n\nThe problem specifies a stochastic EnKF, where each ensemble member is updated independently using a perturbed observation. This method helps preserve the posterior variance consistent with the theoretical value. For each ensemble member $i$, a unique perturbed observation $y_i$ is generated:\n$$y_i = y + \\epsilon_i$$\nwhere each perturbation $\\epsilon_i$ is an independent draw from the observation error distribution, $\\epsilon_i \\sim \\mathcal{N}(0, R)$. For reproducibility, these random draws are seeded with a fixed value of $42$.\n\nThe update equation for each ensemble member $x_i^f$ to its analysis state $x_i^a$ is then:\n$$x_i^a = x_i^f + K(y_i - h(x_i^f))$$\nSince the observation operator is the identity ($h(x_i^f) = x_i^f$), this simplifies to:\n$$x_i^a = x_i^f + K(y_i - x_i^f)$$\n\nThe two required outputs are the per-member analysis increments, $\\{d_i\\}_{i=1}^{N}$, and the posterior ensemble variance, $P^a$.\nThe analysis increment for member $i$ is the change applied to the prior state:\n$$d_i = x_i^a - x_i^f = K(y_i - x_i^f)$$\n\nAfter updating all ensemble members to form the analysis ensemble $\\{x_i^a\\}_{i=1}^{N}$, the posterior variance $P^a$ is computed as the unbiased sample variance of this new ensemble:\n$$P^a = \\frac{1}{N-1} \\sum_{i=1}^{N} (x_i^a - \\bar{x}^a)^2$$\nwhere $\\bar{x}^a$ is the mean of the analysis ensemble.\n\nA specific boundary condition is defined for the case where the prior ensemble has zero variance, i.e., $P^f = 0$. In this scenario, the model has complete certainty in its prior state (relative to the ensemble representation). The Kalman gain is set to $K=0$. Consequently, all analysis increments $d_i$ become zero, the analysis ensemble is identical to the prior ensemble ($x_i^a = x_i^f$), and the posterior variance $P^a$ remains zero. This prevents the filter from being influenced by the observation if the model state ensemble indicates no uncertainty.\n\nThe implementation will process each test case by:\n1.  Calculating the prior ensemble variance $P^f$ from the input ensemble $\\{x_i^f\\}_{i=1}^{N}$.\n2.  Calculating the Kalman gain $K$ based on $P^f$ and the given observation error variance $R$, handling the $P^f=0$ case as specified.\n3.  Generating $N$ random observation perturbations $\\epsilon_i$ from $\\mathcal{N}(0, R)$ using a random number generator seeded with $42$.\n4.  Computing the perturbed observations $y_i = y + \\epsilon_i$.\n5.  Applying the update equation to calculate the analysis ensemble $\\{x_i^a\\}_{i=1}^{N}$ and the analysis increments $\\{d_i\\}_{i=1}^{N}$.\n6.  Calculating the posterior ensemble variance $P^a$ from the analysis ensemble.\n7.  Returning the list of increments and the posterior variance.\nAll calculations will be performed using vectorized operations provided by the `numpy` library for efficiency and clarity.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport json\n\ndef enkf_update_1d(x_f, y, R):\n    \"\"\"\n    Performs a single step of a 1D stochastic Ensemble Kalman Filter.\n\n    Args:\n        x_f (np.ndarray): The prior ensemble of states (N-element array).\n        y (float): The scalar observation.\n        R (float): The observation error variance.\n\n    Returns:\n        tuple: A tuple containing:\n            - list: The per-member analysis increments.\n            - float: The posterior ensemble variance.\n    \"\"\"\n    N = len(x_f)\n    if N = 1:\n        # Unbiased variance is undefined for N = 1.\n        # Problem constraints ensure N > 1, but this is good practice.\n        # For N=1, p_f would be NaN or 0 depending on numpy version.\n        # If N=1 and we set p_f = 0, K=0, d=[0], p_a=0.\n        p_f = 0.0\n    else:\n        # Calculate unbiased prior sample variance (denominator N-1)\n        p_f = np.var(x_f, ddof=1)\n\n    # Handle the boundary condition of zero prior variance\n    if p_f == 0.0:\n        K = 0.0\n        # If K=0, increments are 0, and posterior equals prior.\n        x_a = x_f\n        increments = np.zeros(N)\n    else:\n        # Compute Kalman gain\n        K = p_f / (p_f + R)\n    \n        # Generate perturbed observations for the stochastic EnKF\n        # Use a fixed seed for reproducibility for each case.\n        # A new generator is created for each call to ensure test cases are independent.\n        rng = np.random.default_rng(42)\n        # Draw perturbations from N(0, R)\n        perturbations = rng.normal(loc=0.0, scale=np.sqrt(R), size=N)\n        y_perturbed = y + perturbations\n    \n        # Update each ensemble member (vectorized)\n        # h(x) is the identity, so h(x_f) is just x_f.\n        x_a = x_f + K * (y_perturbed - x_f)\n        \n        # Calculate analysis increments\n        increments = x_a - x_f\n\n    # Calculate unbiased posterior sample variance\n    if N = 1:\n        p_a = 0.0\n    else:\n        p_a = np.var(x_a, ddof=1)\n        \n    return increments.tolist(), p_a\n\ndef solve():\n    \"\"\"\n    Processes all test cases and prints the formatted result.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (np.array([0.18, 0.22, 0.25, 0.19, 0.21, 0.24]), 0.23, 0.0016),\n        (np.array([0.08, 0.12, 0.09, 0.11, 0.10]), 0.20, 0.0025),\n        (np.array([0.30, 0.30, 0.30, 0.30, 0.30]), 0.35, 0.0009),\n        (np.array([0.34, 0.33, 0.35, 0.36, 0.32, 0.31, 0.37, 0.34]), 0.34, 0.04),\n    ]\n\n    results = []\n    for x_f, y, R in test_cases:\n        increments, posterior_variance = enkf_update_1d(x_f, y, R)\n        # Assemble the result for one case in the required format\n        results.append([increments, posterior_variance])\n\n    # Final print statement in the exact required format.\n    # Use json.dumps with separators to produce a compact string with no spaces.\n    print(json.dumps(results, separators=(',', ':')))\n\nsolve()\n```"
        }
    ]
}