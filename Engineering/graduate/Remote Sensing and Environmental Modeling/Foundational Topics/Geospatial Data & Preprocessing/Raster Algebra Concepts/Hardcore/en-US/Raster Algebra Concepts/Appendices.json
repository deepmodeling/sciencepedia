{
    "hands_on_practices": [
        {
            "introduction": "This practice explores the crucial concept of uncertainty propagation in raster algebra. Since all remote sensing measurements have inherent uncertainty, it is vital to understand how these errors are transformed when we combine raster bands to compute indices. By applying a first-order Taylor expansion, we can approximate the variance of a derived product, such as a normalized difference index, providing essential information about the reliability of our model outputs .",
            "id": "3840042",
            "problem": "Consider two co-registered per-pixel top-of-atmosphere reflectance bands, denoted by the random variables $X$ and $Y$, derived from a physically based atmospheric correction model. The normalized difference index for these two bands is defined by the differentiable mapping $I(X,Y) = \\frac{X - Y}{X + Y}$. Assume that at a particular pixel the bandwise reflectances are characterized by the following statistical summaries: expected values $\\mu_{X} = 0.52$ and $\\mu_{Y} = 0.32$, variances $\\sigma_{X}^{2} = (0.015)^{2}$ and $\\sigma_{Y}^{2} = (0.012)^{2}$, and covariance $\\sigma_{XY} = 0.00015$. Using only the definitions of variance, covariance, and a first-order Taylor expansion for a differentiable function of random variables about $(\\mu_{X}, \\mu_{Y})$, derive the expression for the variance of $I$ at this pixel and compute its numerical value. Then, explain qualitatively under what conditions ignoring covariance (i.e., treating $X$ and $Y$ as uncorrelated) severely misestimates the uncertainty of $I$.\n\nRound the final numerical variance of $I$ to four significant figures. Express the final variance as a decimal number without units (the index is dimensionless).",
            "solution": "The problem requires the derivation and calculation of the variance of a normalized difference index, $I(X,Y)$, and a qualitative analysis of the effect of covariance on this variance. The solution proceeds in three parts as requested.\n\nFirst, the problem statement is validated.\n**Step 1: Extract Givens**\n-   Random variables for reflectance bands: $X$, $Y$.\n-   Normalized difference index: $I(X,Y) = \\frac{X - Y}{X + Y}$.\n-   The mapping $I(X,Y)$ is differentiable.\n-   Expected values: $\\mu_{X} = 0.52$, $\\mu_{Y} = 0.32$.\n-   Variances: $\\sigma_{X}^{2} = (0.015)^{2}$, $\\sigma_{Y}^{2} = (0.012)^{2}$.\n-   Covariance: $\\sigma_{XY} = 0.00015$.\n-   Method: First-order Taylor expansion for a function of random variables.\n-   Required output: 1. Derived expression for $\\sigma_I^2$. 2. Numerical value of $\\sigma_I^2$ rounded to four significant figures. 3. Qualitative explanation of the impact of ignoring covariance.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, well-posed, and objective. It presents a standard application of the propagation of uncertainty (error propagation) using the Delta method, a first-order Taylor series approximation. This technique is fundamental in statistics and widely used in remote sensing to estimate the uncertainty of derived indices like NDVI. The provided reflectance values and their variances are physically realistic for top-of-atmosphere measurements. All necessary parameters are given, and the problem is free of contradictions, ambiguities, or non-scientific claims.\n\n**Step 3: Verdict and Action**\nThe problem is **valid**. A full solution will be provided.\n\n**Part 1: Derivation of the Variance Expression**\n\nThe variance of a differentiable function $I(X,Y)$ of two random variables $X$ and $Y$ can be approximated using a first-order Taylor series expansion of $I(X,Y)$ around the mean values $(\\mu_X, \\mu_Y)$. The general formula for the variance, $\\sigma_{I}^{2}$, is given by:\n$$ \\sigma_{I}^{2} \\approx \\left( \\left. \\frac{\\partial I}{\\partial X} \\right|_{(\\mu_X, \\mu_Y)} \\right)^2 \\sigma_X^2 + \\left( \\left. \\frac{\\partial I}{\\partial Y} \\right|_{(\\mu_X, \\mu_Y)} \\right)^2 \\sigma_Y^2 + 2 \\left( \\left. \\frac{\\partial I}{\\partial X} \\right|_{(\\mu_X, \\mu_Y)} \\right) \\left( \\left. \\frac{\\partial I}{\\partial Y} \\right|_{(\\mu_X, \\mu_Y)} \\right) \\sigma_{XY} $$\nThe function is $I(X,Y) = \\frac{X - Y}{X + Y}$. We must first compute the partial derivatives of $I$ with respect to $X$ and $Y$.\n\nUsing the quotient rule for differentiation, the partial derivative with respect to $X$ is:\n$$ \\frac{\\partial I}{\\partial X} = \\frac{(1)(X+Y) - (X-Y)(1)}{(X+Y)^2} = \\frac{X+Y-X+Y}{(X+Y)^2} = \\frac{2Y}{(X+Y)^2} $$\nThe partial derivative with respect to $Y$ is:\n$$ \\frac{\\partial I}{\\partial Y} = \\frac{(-1)(X+Y) - (X-Y)(1)}{(X+Y)^2} = \\frac{-X-Y-X+Y}{(X+Y)^2} = \\frac{-2X}{(X+Y)^2} $$\nNext, we evaluate these partial derivatives at the mean values $(\\mu_X, \\mu_Y)$:\n$$ \\left. \\frac{\\partial I}{\\partial X} \\right|_{(\\mu_X, \\mu_Y)} = \\frac{2\\mu_Y}{(\\mu_X + \\mu_Y)^2} $$\n$$ \\left. \\frac{\\partial I}{\\partial Y} \\right|_{(\\mu_X, \\mu_Y)} = \\frac{-2\\mu_X}{(\\mu_X + \\mu_Y)^2} $$\nSubstituting these expressions into the general variance formula yields the derived expression for the variance of $I$:\n$$ \\sigma_I^2 \\approx \\left( \\frac{2\\mu_Y}{(\\mu_X + \\mu_Y)^2} \\right)^2 \\sigma_X^2 + \\left( \\frac{-2\\mu_X}{(\\mu_X + \\mu_Y)^2} \\right)^2 \\sigma_Y^2 + 2 \\left( \\frac{2\\mu_Y}{(\\mu_X + \\mu_Y)^2} \\right) \\left( \\frac{-2\\mu_X}{(\\mu_X + \\mu_Y)^2} \\right) \\sigma_{XY} $$\nSimplifying this expression, we get:\n$$ \\sigma_I^2 \\approx \\frac{4\\mu_Y^2}{(\\mu_X + \\mu_Y)^4} \\sigma_X^2 + \\frac{4\\mu_X^2}{(\\mu_X + \\mu_Y)^4} \\sigma_Y^2 - \\frac{8\\mu_X\\mu_Y}{(\\mu_X + \\mu_Y)^4} \\sigma_{XY} $$\nThis can be written more compactly as:\n$$ \\sigma_I^2 \\approx \\frac{4}{(\\mu_X + \\mu_Y)^4} \\left( \\mu_Y^2 \\sigma_X^2 + \\mu_X^2 \\sigma_Y^2 - 2 \\mu_X \\mu_Y \\sigma_{XY} \\right) $$\nThis completes the derivation of the expression for the variance of $I$.\n\n**Part 2: Numerical Computation of the Variance**\n\nWe are given the following values:\n$\\mu_X = 0.52$\n$\\mu_Y = 0.32$\n$\\sigma_X^2 = (0.015)^2 = 0.000225$\n$\\sigma_Y^2 = (0.012)^2 = 0.000144$\n$\\sigma_{XY} = 0.00015$\n\nFirst, let's compute the denominator term:\n$$ (\\mu_X + \\mu_Y)^4 = (0.52 + 0.32)^4 = (0.84)^4 = 0.49787136 $$\nNow, let's compute the terms within the parentheses in the numerator's expression:\n$$ \\mu_Y^2 \\sigma_X^2 = (0.32)^2 (0.015)^2 = (0.1024)(0.000225) = 0.00002304 $$\n$$ \\mu_X^2 \\sigma_Y^2 = (0.52)^2 (0.012)^2 = (0.2704)(0.000144) = 0.0000389376 $$\n$$ 2 \\mu_X \\mu_Y \\sigma_{XY} = 2(0.52)(0.32)(0.00015) = (0.3328)(0.00015) = 0.00004992 $$\nCombining these terms:\n$$ \\mu_Y^2 \\sigma_X^2 + \\mu_X^2 \\sigma_Y^2 - 2 \\mu_X \\mu_Y \\sigma_{XY} = 0.00002304 + 0.0000389376 - 0.00004992 = 0.0000120576 $$\nFinally, we compute $\\sigma_I^2$:\n$$ \\sigma_I^2 \\approx \\frac{4}{0.49787136} \\times (0.0000120576) \\approx (8.034292)(0.0000120576) \\approx 0.0000968731 $$\nRounding to four significant figures, the numerical value for the variance is:\n$$ \\sigma_I^2 \\approx 0.00009687 $$\n\n**Part 3: Qualitative Explanation of the Covariance Effect**\n\nIgnoring covariance (i.e., assuming $X$ and $Y$ are uncorrelated, $\\sigma_{XY}=0$) means omitting the term $2 \\left(\\frac{\\partial I}{\\partial X}\\right) \\left(\\frac{\\partial I}{\\partial Y}\\right) \\sigma_{XY}$ from the variance calculation. For the normalized difference index $I = \\frac{X-Y}{X+Y}$, the partial derivatives have opposite signs, as $\\frac{\\partial I}{\\partial X} = \\frac{2Y}{(X+Y)^2} > 0$ and $\\frac{\\partial I}{\\partial Y} = \\frac{-2X}{(X+Y)^2} < 0$ for positive reflectances $X$ and $Y$. Consequently, their product is negative:\n$$ \\left(\\frac{\\partial I}{\\partial X}\\right) \\left(\\frac{\\partial I}{\\partial Y}\\right) < 0 $$\nThe effect of ignoring covariance thus depends critically on the sign of the covariance $\\sigma_{XY}$ itself:\n\n1.  **Positive Covariance ($\\sigma_{XY} > 0$)**: This is the typical case for reflectance bands from the same sensor, as surface properties and atmospheric conditions tend to affect them similarly. With $\\sigma_{XY} > 0$ and a negative product of partials, the full covariance term $2 (\\partial I / \\partial X) (\\partial I / \\partial Y) \\sigma_{XY}$ is negative. Neglecting this term (i.e., treating it as zero) removes a negative contribution from the total variance, thereby causing an **overestimation** of the uncertainty of $I$.\n\n2.  **Negative Covariance ($\\sigma_{XY} < 0$)**: In the less common case where bands are negatively correlated, the full covariance term becomes positive. Ignoring it would then cause an **underestimation** of the uncertainty of $I$.\n\nA severe misestimation occurs when the magnitude of the covariance term, $|2 (\\partial I / \\partial X) (\\partial I / \\partial Y) \\sigma_{XY}|$, is large relative to the sum of the variance terms, $(\\partial I / \\partial X)^2 \\sigma_X^2 + (\\partial I / \\partial Y)^2 \\sigma_Y^2$. This happens when the correlation coefficient, $\\rho_{XY} = \\frac{\\sigma_{XY}}{\\sigma_X \\sigma_Y}$, is close to $+1$ or $-1$. In summary, ignoring covariance is most problematic when the input variables are strongly correlated. For remote sensing indices like the one in this problem, where input bands are often strongly positively correlated, treating them as uncorrelated typically results in a significant and systematic overestimation of the index's uncertainty.",
            "answer": "$$\\boxed{0.00009687}$$"
        },
        {
            "introduction": "Building upon the use of normalized difference indices, this exercise tackles a practical challenge in their computational implementation: numerical stability. When the denominator of an index like $I=\\frac{R_1-R_2}{R_1+R_2}$ approaches zero, the calculation can become unstable or fail. This practice demonstrates a common solution—epsilon-regularization—and requires a rigorous analysis of the bias this technique introduces, blending robust coding practices with analytical skill .",
            "id": "3840049",
            "problem": "You are given two single-band reflectance rasters represented as arrays, denoted by $R_1$ and $R_2$, with element-wise values constrained to the interval $[0,1]$ and no physical units (dimensionless). The goal is to implement a pixel-wise normalized difference operator that computes the difference between $R_1$ and $R_2$ normalized by their sum, in a manner that is numerically robust to small sums in the denominator through the addition of a strictly positive regularization parameter $\\varepsilon$. Then, quantify the bias introduced by this regularization, and verify an upper bound on its magnitude.\n\nStart from the following fundamental bases:\n- Raster algebra operations are defined element-wise. That is, for any two rasters $A$ and $B$ having the same shape, addition, subtraction, multiplication, and division are performed independently at each pixel.\n- For any $a,b \\in [0,1]$, their sum satisfies $0 \\le a+b \\le 2$ and their difference satisfies $-1 \\le a-b \\le 1$.\n\nDefine the ideal, unregularized index at each pixel where the sum $S = R_1 + R_2$ is strictly positive as the difference of $R_1$ and $R_2$ normalized by their sum. Define the regularized index at each pixel as the same difference normalized by the sum with an added strictly positive constant $\\varepsilon$, i.e., the sum in the denominator is increased by $\\varepsilon$ to avoid division by very small numbers. The pixel-wise bias of the regularized index relative to the unregularized index is the difference between the regularized index and the unregularized index at that pixel.\n\nYour tasks:\n1. Derive, from the element-wise definitions above and algebraic manipulation, a closed-form expression for the pixel-wise bias as a function of $R_1$, $R_2$, and $\\varepsilon$, valid wherever $R_1 + R_2 > 0$ and $\\varepsilon > 0$.\n2. Prove a pointwise upper bound on the magnitude of the bias in terms of only the sum $S = R_1 + R_2$ and $\\varepsilon$, and not the difference $R_1 - R_2$. Your bound should be nonnegative and finite for all $S > 0$ and $\\varepsilon > 0$.\n3. Implement a robust, vectorized computation of the unregularized and regularized indices, the pixel-wise bias, and the verification of the bound from Task $2$ for each pixel, with a numerical tolerance $\\delta = 10^{-12}$ to accommodate floating-point arithmetic. The verification is considered successful for a pixel if the computed magnitude of the bias is less than or equal to your bound plus $\\delta$.\n\nUse the following test suite. Each test case consists of two arrays for $R_1$ and $R_2$ and a scalar $\\varepsilon$:\n- Test case $1$ (general values with moderate sums): $R_1 = [\\,0.62,\\, 0.18,\\, 0.47,\\, 0.33,\\, 0.70\\,]$, $R_2 = [\\,0.21,\\, 0.35,\\, 0.43,\\, 0.12,\\, 0.40\\,]$, $\\varepsilon = 10^{-3}$.\n- Test case $2$ (small sums near the regularization scale): $R_1 = [\\,10^{-6},\\, 3 \\times 10^{-4},\\, 5 \\times 10^{-3},\\, 2 \\times 10^{-2}\\,]$, $R_2 = [\\,2 \\times 10^{-6},\\, 8 \\times 10^{-4},\\, 5 \\times 10^{-3},\\, 10^{-2}\\,]$, $\\varepsilon = 10^{-3}$.\n- Test case $3$ (zero numerators, nonzero sums): $R_1 = [\\,10^{-6},\\, 10^{-3},\\, 10^{-1},\\, 5 \\times 10^{-1}\\,]$, $R_2 = [\\,10^{-6},\\, 10^{-3},\\, 10^{-1},\\, 5 \\times 10^{-1}\\,]$, $\\varepsilon = 10^{-3}$.\n- Test case $4$ (mixed extremes): $R_1 = [\\,1.0,\\, 0.0,\\, 0.9,\\, 2 \\times 10^{-2},\\, 0.6\\,]$, $R_2 = [\\,0.0,\\, 1.0,\\, 0.1,\\, 10^{-2},\\, 0.35\\,]$, $\\varepsilon = 5 \\times 10^{-2}$.\n\nFor each test case, compute and return the following three quantities:\n- The maximum absolute bias across the pixels (a single float).\n- The mean signed bias across the pixels (a single float).\n- A boolean indicating whether the bound from Task $2$ is satisfied at every pixel within tolerance $\\delta$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case result is itself a list of the three quantities above in the order specified. For example, the output should have the form\n\"[ [max_abs_bias_case1, mean_bias_case1, bound_ok_case1], [max_abs_bias_case2, mean_bias_case2, bound_ok_case2], [max_abs_bias_case3, mean_bias_case3, bound_ok_case3], [max_abs_bias_case4, mean_bias_case4, bound_ok_case4] ]\".",
            "solution": "The problem requires the derivation of the bias introduced by a regularization term in a normalized difference index, the proof of an upper bound for this bias, and a numerical implementation to compute the bias and verify the bound for a given set of test cases. The problem is well-posed, scientifically grounded, and provides all necessary information for a unique solution.\n\nLet the pixel-wise reflectance values of the two rasters be denoted by $r_1$ and $r_2$, where $r_1, r_2 \\in [0, 1]$. The regularization parameter is $\\varepsilon > 0$. All raster operations are performed element-wise.\n\n**Task 1: Derivation of a Closed-Form Expression for Pixel-Wise Bias**\n\nThe unregularized normalized difference index, $I_{unreg}$, is defined for $r_1 + r_2 > 0$ as:\n$$I_{unreg} = \\frac{r_1 - r_2}{r_1 + r_2}$$\n\nThe regularized index, $I_{reg}$, is defined as:\n$$I_{reg} = \\frac{r_1 - r_2}{r_1 + r_2 + \\varepsilon}$$\n\nThe pixel-wise bias, $B$, is the difference between the regularized and unregularized indices:\n$$B = I_{reg} - I_{unreg}$$\n\nSubstituting the definitions of $I_{reg}$ and $I_{unreg}$:\n$$B = \\frac{r_1 - r_2}{r_1 + r_2 + \\varepsilon} - \\frac{r_1 - r_2}{r_1 + r_2}$$\n\nTo simplify, let $D = r_1 - r_2$ and $S = r_1 + r_2$. The expression for the bias becomes:\n$$B = \\frac{D}{S + \\varepsilon} - \\frac{D}{S}$$\n\nFactoring out the term $D$:\n$$B = D \\left( \\frac{1}{S + \\varepsilon} - \\frac{1}{S} \\right)$$\n\nWe find a common denominator for the terms in the parentheses:\n$$B = D \\left( \\frac{S - (S + \\varepsilon)}{S(S + \\varepsilon)} \\right) = D \\left( \\frac{-\\varepsilon}{S(S + \\varepsilon)} \\right)$$\n\nSubstituting back $D = r_1 - r_2$ and $S = r_1 + r_2$, we obtain the closed-form expression for the bias:\n$$B = - \\varepsilon \\frac{r_1 - r_2}{(r_1 + r_2)(r_1 + r_2 + \\varepsilon)}$$\nThis expression is valid for all pixels where $S = r_1 + r_2 > 0$ and for any $\\varepsilon > 0$.\n\n**Task 2: Proof of a Pointwise Upper Bound on the Bias Magnitude**\n\nWe seek to find an upper bound for the magnitude of the bias, $|B|$, that depends only on the sum $S = r_1 + r_2$ and the regularization parameter $\\varepsilon$.\n\nStarting from the derived expression for $B$:\n$$|B| = \\left| - \\varepsilon \\frac{r_1 - r_2}{S(S + \\varepsilon)} \\right|$$\n\nSince $\\varepsilon > 0$ and $S > 0$ (as per the problem's constraints), the denominator $S(S + \\varepsilon)$ is strictly positive. We can simplify the magnitude as:\n$$|B| = \\frac{\\varepsilon |r_1 - r_2|}{S(S + \\varepsilon)}$$\n\nTo establish an upper bound in terms of $S$ and $\\varepsilon$, we must bound the term $|r_1 - r_2|$. The reflectance values are non-negative, i.e., $r_1 \\ge 0$ and $r_2 \\ge 0$. By the triangle inequality, for any two real numbers $x$ and $y$, $|x - y| \\le |x| + |y|$. Applying this to our non-negative reflectances:\n$$|r_1 - r_2| \\le |r_1| + |-r_2| = r_1 + r_2 = S$$\n\nThis inequality, $|r_1 - r_2| \\le S$, allows us to bound the magnitude of the bias. Substituting this into the expression for $|B|$:\n$$|B| \\le \\frac{\\varepsilon \\cdot S}{S(S + \\varepsilon)}$$\n\nFor $S>0$, we can cancel the $S$ term in the numerator and denominator:\n$$|B| \\le \\frac{\\varepsilon}{S + \\varepsilon}$$\n\nThis provides a valid upper bound $U(S, \\varepsilon) = \\frac{\\varepsilon}{S + \\varepsilon}$ for the magnitude of the bias. This bound is expressed solely in terms of $S$ and $\\varepsilon$, is non-negative, and is finite for all $S > 0$ and $\\varepsilon > 0$. The bound is tight, meaning equality $|B| = U(S, \\varepsilon)$ is achieved when $|r_1 - r_2| = S$, which occurs if and only if one of $r_1$ or $r_2$ is zero. This is a physically realizable condition (e.g., $r_1 \\in (0, 1]$ and $r_2 = 0$).\n\n**Task 3: Algorithmic Design for Vectorized Computation**\n\nThe implementation will be vectorized using the `numpy` library to efficiently perform element-wise operations on the raster arrays.\n\nFor each test case, consisting of arrays $R_1$, $R_2$ and a scalar $\\varepsilon$:\n1.  Compute the element-wise sum $S = R_1 + R_2$ and difference $D = R_1 - R_2$.\n2.  Compute the bias array $B$ using the numerically stable, derived closed-form expression: $B = -\\varepsilon D / (S(S + \\varepsilon))$. This avoids potential subtractive cancellation that might occur from $I_{reg} - I_{unreg}$.\n3.  Compute the array of bias magnitudes, $|B| = \\text{abs}(B)$.\n4.  Compute the array of upper bounds, $U = \\varepsilon / (S + \\varepsilon)$.\n5.  Verify the bound at each pixel. With the numerical tolerance $\\delta = 10^{-12}$, the condition for a successful verification at a pixel is $|B| \\le U + \\delta$. A boolean array is generated from this comparison.\n6.  The overall bound verification for the test case is deemed successful if the condition holds for all pixels. This is determined by applying a universal quantifier (`np.all`) to the boolean array from the previous step.\n7.  The three required output quantities for the test case are:\n    a. The maximum absolute bias: $\\max(|B|)$.\n    b. The mean signed bias: $\\text{mean}(B)$.\n    c. The overall boolean result of the bound verification.\nThese steps are repeated for each test case, and the results are aggregated for the final output.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the raster algebra problem by calculating bias and verifying bounds.\n    \"\"\"\n    \n    test_cases = [\n        (\n            [0.62, 0.18, 0.47, 0.33, 0.70],\n            [0.21, 0.35, 0.43, 0.12, 0.40],\n            1e-3\n        ),\n        (\n            [1e-6, 3e-4, 5e-3, 2e-2],\n            [2e-6, 8e-4, 5e-3, 1e-2],\n            1e-3\n        ),\n        (\n            [1e-6, 1e-3, 1e-1, 5e-1],\n            [1e-6, 1e-3, 1e-1, 5e-1],\n            1e-3\n        ),\n        (\n            [1.0, 0.0, 0.9, 2e-2, 0.6],\n            [0.0, 1.0, 0.1, 1e-2, 0.35],\n            5e-2\n        )\n    ]\n    \n    delta = 1e-12\n    all_results = []\n    \n    for r1_list, r2_list, epsilon in test_cases:\n        # Convert input lists to numpy arrays for vectorized operations\n        R1 = np.array(r1_list, dtype=float)\n        R2 = np.array(r2_list, dtype=float)\n\n        # Task 1: Compute bias using the derived closed-form expression\n        # This is more numerically stable than subtracting the indices directly.\n        S = R1 + R2  # Element-wise sum\n        D = R1 - R2  # Element-wise difference\n        \n        # Guard against division by zero in the unlikely case S is zero\n        # The problem statement guarantees S > 0 in the test data.\n        # This is for general robustness.\n        # bias = np.divide(-epsilon * D, S * (S + epsilon), where=S>0, out=np.zeros_like(S))\n        bias = -epsilon * D / (S * (S + epsilon))\n        \n        # Task 2: Compute upper bound and verify\n        abs_bias = np.abs(bias)\n        \n        # Bound U(S, epsilon) = epsilon / (S + epsilon)\n        bound = epsilon / (S + epsilon)\n        \n        # Task 3: Verification with tolerance\n        is_bound_satisfied_all_pixels = np.all(abs_bias <= bound + delta)\n\n        # Compute required output statistics\n        max_abs_bias = np.max(abs_bias)\n        mean_signed_bias = np.mean(bias)\n        \n        # Append results for the current test case\n        all_results.append([max_abs_bias, mean_signed_bias, bool(is_bound_satisfied_all_pixels)])\n\n    # Format the final output string as a list of lists.\n    # The str() representation of a list of lists matches the required spacing and format.\n    # The problem example shows lowercase booleans, but python's default is capitalized.\n    # We will produce standard Python string representation, which is unambiguous,\n    # and then convert booleans to lowercase as per the prompt's implied format.\n    print(str(all_results).replace(\"True\", \"true\").replace(\"False\", \"false\"))\n\nsolve()\n\n```"
        },
        {
            "introduction": "This final practice moves from local, pixel-wise operations to global spatial modeling by implementing a least-cost path analysis. You will translate the continuous concept of a friction surface into a discrete weighted graph and use Dijkstra's algorithm to find the most efficient route between points. This exercise is fundamental to many applications in environmental modeling, from hydrology to wildlife management, and formalizes the connection between the accumulated cost raster and the underlying shortest-path tree .",
            "id": "3840074",
            "problem": "You are given the conceptual task of computing least-cost paths on a spatial grid (a raster) where each cell stores a nonnegative per-unit-length traversal friction cost, and of formalizing the relationship between the accumulated cost raster and the shortest-path tree induced by a nonnegative weighted graph model. The raster algebra constructs in this problem must be derived from first principles that connect a continuous path integral of a spatially varying cost field to a discrete graph representation on a raster grid.\n\nBegin from the following fundamental base: a continuous cost field $c(\\mathbf{x}) \\ge 0$ over a domain $\\Omega \\subset \\mathbb{R}^2$, and a continuous path $\\gamma: [0, L] \\to \\Omega$ of total arclength $L$ with infinitesimal arclength measure $\\mathrm{d}s$. The accumulated cost of traversing $\\gamma$ is the path integral\n$$\nJ[\\gamma] = \\int_{0}^{L} c(\\gamma(s)) \\, \\mathrm{d}s,\n$$\nwhich is the spatial analog of integrating a nonnegative friction over arclength. For a raster with $m$ rows and $n$ columns, define the cost raster $C \\in \\mathbb{R}_{\\ge 0}^{m \\times n}$ such that $C_{i,j}$ approximates $c(\\mathbf{x}_{i,j})$ at the cell center $\\mathbf{x}_{i,j}$. Let the set of vertices be $V = \\{(i,j) \\mid i \\in \\{0,\\dots,m-1\\}, j \\in \\{0,\\dots,n-1\\}\\}$ and the set of edges $E$ connect each cell to its neighbors under a specified connectivity $\\mathcal{N}$, either $4$-connected or $8$-connected. Associate each edge $e = (u,v) \\in E$ with a Euclidean step length $d_{uv} \\in \\{1,\\sqrt{2}\\}$ depending on whether $u$ and $v$ are orthogonal or diagonal neighbors. Using a first-order accurate quadrature along each edge (for example, trapezoidal rule applied to the line segment between cell centers), approximate the edge weight $w_{uv}$ as the discrete analog of the integral of $c$ along that short segment. This defines a nonnegative weighted graph $(V,E,w)$, for which the accumulated cost from a source set $S \\subset V$ to any vertex $v \\in V$ is the minimum over all discrete paths from $S$ to $v$ of the sum of edge weights, and the corresponding predecessor relation defines a shortest-path tree (a forest when $|S| \\ge 2$).\n\nYour tasks are:\n- Implement a program that, given $C$, $\\mathcal{N}$, a source set $S \\subset V$, and one target $t \\in V$, constructs the weighted graph as above and computes the accumulated cost raster $A \\in \\mathbb{R}_{\\ge 0}^{m \\times n}$ and a predecessor raster $P \\in \\{-1\\} \\cup \\{0,1,\\dots,mn-1\\}^{m \\times n}$ using Dijkstra’s algorithm on $(V,E,w)$, where $A_{v}$ is the minimal sum of edge weights from any source in $S$ to $v$, and $P_{v}$ encodes the unique predecessor of $v$ chosen by the algorithm in the shortest-path tree (or $-1$ for sources). The graph must respect the following: edges exist only between valid in-bounds neighbors under $\\mathcal{N}$, and any edge incident to an impassable cell with $C_{i,j} = +\\infty$ is excluded. Use a zero-based index convention for $(i,j)$ coordinates and a flat index mapping $f(i,j) = i \\cdot n + j$ for storing predecessors.\n- Reconstruct the least-cost path from the target $t$ back to its source by following $P$, compute the number of steps along the path, and verify the edge-additive property of the accumulated cost raster along the reconstructed path: for each consecutive pair $(u,v)$ in the path, check that $A_{v} = A_{u} + w_{uv}$ within a numerical tolerance of $10^{-9}$, and that the total sum of edge weights along the path equals $A_{t}$ within the same tolerance.\n\nUnits: all costs are unitless and represent cost units per unit length; step lengths are unitless and equal to $1$ for orthogonal moves and $\\sqrt{2}$ for diagonal moves by construction.\n\nAngle units: no angles are used in this problem.\n\nPercentages: not applicable.\n\nTest suite and parameter specification:\nDefine four test cases, each with a single target:\n- Test case $1$ (general case with $8$-connectivity and uniform cost):\n    - Raster size: $m = 5$, $n = 5$.\n    - Cost raster: $C_{i,j} = 1$ for all $(i,j)$.\n    - Connectivity: $\\mathcal{N} = 8$-connected.\n    - Sources: $S = \\{(0,0)\\}$.\n    - Target: $t = (4,3)$.\n- Test case $2$ (boundary case with an impassable barrier and $4$-connectivity):\n    - Raster size: $m = 5$, $n = 5$.\n    - Cost raster: $C_{i,j} = 1$ for all $(i,j)$ except $C_{2,1} = C_{2,2} = C_{2,3} = +\\infty$ (impassable).\n    - Connectivity: $\\mathcal{N} = 4$-connected.\n    - Sources: $S = \\{(0,2)\\}$.\n    - Target: $t = (4,2)$.\n- Test case $3$ (nonuniform gradient cost with multiple sources and $8$-connectivity):\n    - Raster size: $m = 6$, $n = 6$.\n    - Cost raster: $C_{i,j} = 1 + 0.15\\, i + 0.05\\, j$ for $i \\in \\{0,\\dots,5\\}$, $j \\in \\{0,\\dots,5\\}$.\n    - Connectivity: $\\mathcal{N} = 8$-connected.\n    - Sources: $S = \\{(0,0),(5,5)\\}$.\n    - Target: $t = (5,0)$.\n- Test case $4$ (edge case with multiple sources and ties under uniform cost):\n    - Raster size: $m = 4$, $n = 4$.\n    - Cost raster: $C_{i,j} = 1$ for all $(i,j)$.\n    - Connectivity: $\\mathcal{N} = 8$-connected.\n    - Sources: $S = \\{(0,3),(3,0)\\}$.\n    - Target: $t = (3,3)$.\n\nFinal output format:\nFor each test case, produce a list of three items: the accumulated cost to the target $A_{t}$ as a float rounded to $6$ decimal places, the number of steps along the reconstructed least-cost path to reach a source as an integer, and a boolean indicating whether the edge-additive property and total path cost equality hold within tolerance $10^{-9}$. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to a test case and is itself a list in the form $[A_{t}, \\text{steps}, \\text{valid}]$. For example, an output for two test cases would look like $[[a_1,s_1,v_1],[a_2,s_2,v_2]]$ with the actual values substituted.",
            "solution": "The problem statement has been meticulously reviewed and is determined to be **valid**. It is scientifically grounded in the principles of graph theory and numerical methods as applied to spatial analysis, specifically least-cost pathfinding on a raster. The problem is well-posed, with all necessary data, constraints, and definitions provided to ensure a unique and meaningful solution. It is objective and free of ambiguity. The task is to implement a computational solution based on these principles.\n\nThe solution proceeds by first discretizing the continuous cost-path problem into a shortest-path problem on a weighted graph. We then apply Dijkstra's algorithm to solve this discrete problem, and finally, we reconstruct and verify the resulting path.\n\n**1. Discretization: From Continuous Field to Discrete Graph**\n\nThe foundational concept is the transformation of a continuous problem into a discrete one amenable to computation. The continuous accumulated cost is given by the path integral $J[\\gamma] = \\int_{\\gamma} c(\\mathbf{x}) \\, \\mathrm{d}s$, where $c(\\mathbf{x})$ is a spatially varying friction cost field.\n\nWe discretize this as follows:\n- The continuous domain $\\Omega \\subset \\mathbb{R}^2$ is represented by a grid of $m \\times n$ cells. Each cell $(i,j)$ becomes a vertex $v_{i,j}$ in a graph $G = (V, E)$.\n- The cost field $c(\\mathbf{x})$ is sampled at the center of each cell, yielding the cost raster $C$, where $C_{i,j}$ is the cost value at vertex $v_{i,j}$.\n- A continuous path $\\gamma$ is approximated by a sequence of edges connecting adjacent vertices in the graph. The set of available edges $E$ is determined by the specified connectivity, either $4$-connected (orthogonal neighbors) or $8$-connected (orthogonal and diagonal neighbors).\n- The integral $\\int c(\\mathbf{x}) \\, \\mathrm{d}s$ along a short path segment between two adjacent vertices $u$ and $v$ is approximated by a sum of discrete edge weights, $w_{uv}$.\n\n**2. Edge Weight Formulation**\n\nThe problem specifies using a first-order accurate quadrature to approximate the cost integral along the straight-line segment connecting the centers of two adjacent cells, $u$ and $v$. The trapezoidal rule is a suitable choice. The cost integral along this segment is approximated as the average of the costs at the endpoints multiplied by the segment's length:\n$$\nw_{uv} \\approx \\frac{C_u + C_v}{2} d_{uv}\n$$\nwhere $C_u$ and $C_v$ are the cost values at cells $u$ and $v$, respectively, and $d_{uv}$ is the Euclidean distance between their centers. By convention, for a unit grid spacing:\n- $d_{uv} = 1$ for orthogonal neighbors.\n- $d_{uv} = \\sqrt{2}$ for diagonal neighbors.\n\nSince all costs $C_{i,j}$ are non-negative, all resulting edge weights $w_{uv}$ are also non-negative. This is a critical prerequisite for the correctness of Dijkstra's algorithm. Edges connected to impassable cells (where $C_{i,j} = +\\infty$) are considered to have infinite weight and are effectively excluded from the graph.\n\n**3. Dijkstra's Algorithm for Least-Cost Path**\n\nWith a graph $G = (V,E,w)$ where all edge weights $w$ are non-negative, Dijkstra's algorithm correctly finds the path with the minimum accumulated cost from a set of source vertices $S$ to all other reachable vertices.\n\nThe algorithm maintains three key data structures:\n- An accumulated cost raster $A$, where $A_v$ stores the currently known shortest-path cost from any source in $S$ to vertex $v$. It is initialized to $0$ for all $s \\in S$ and to $+\\infty$ for all other vertices.\n- A predecessor raster $P$, where $P_v$ stores the index of the vertex that precedes $v$ on the shortest path from $S$. Source vertices have a special predecessor value of $-1$. We use a flat index mapping $f(i,j) = i \\cdot n + j$ for this purpose.\n- A priority queue `pq`, which stores vertices to visit, prioritized by their accumulated cost. It is initialized with all source vertices.\n\nThe algorithm proceeds as follows:\n1.  Initialize $A$, $P$, and `pq` as described above.\n2.  While the priority queue is not empty, extract the vertex $u$ with the smallest accumulated cost.\n3.  For each valid neighbor $v$ of $u$ (as defined by the connectivity $\\mathcal{N}$ and grid boundaries):\n    a. Calculate the edge weight $w_{uv} = ((C_u + C_v)/2) \\cdot d_{uv}$.\n    b. Compute the potential new cost to reach $v$ through $u$: $A_u + w_{uv}$.\n    c. If this new cost is less than the current cost $A_v$, update $A_v$ to the new, lower cost, set $P_v$ to the flat index of $u$, and add $v$ to the priority queue with its new cost.\n\nUpon termination, $A$ contains the final least-cost values from the source set $S$ to every other cell, and $P$ encodes the shortest-path tree (a forest, in the case of multiple disconnected components or multiple sources).\n\n**4. Path Reconstruction and Verification**\n\nGiven the computed predecessor raster $P$ and a target vertex $t$, the least-cost path can be reconstructed by starting at $t$ and iteratively stepping backward to its predecessor until a source vertex (with predecessor $-1$) is reached.\n\nThe number of steps is the number of edges in this reconstructed path.\n\nVerification involves two checks, performed with a numerical tolerance of $10^{-9}$:\n- **Edge-Additive Property**: For every pair of consecutive vertices $(u, v)$ on the reconstructed path, where $u$ is the predecessor of $v$, we must verify that the costs are consistent: $A_v = A_u + w_{uv}$.\n- **Total Path Cost**: The sum of all edge weights $w_{uv}$ along the entire reconstructed path must equal the final accumulated cost at the target, $A_t$.\n\nIf both properties hold true for the entire path, the implementation is considered valid.",
            "answer": "```python\nimport numpy as np\nimport heapq\n\ndef solve():\n    \"\"\"\n    Main function to define test cases and orchestrate the solution process.\n    \"\"\"\n\n    def run_least_cost_path(cost_raster, connectivity, sources, target):\n        \"\"\"\n        Computes the least-cost path, reconstructs it, and verifies its properties.\n\n        Args:\n            cost_raster (np.ndarray): The m x n raster of cost values.\n            connectivity (int): The neighborhood connectivity (4 or 8).\n            sources (list of tuples): A list of (row, col) source coordinates.\n            target (tuple): The (row, col) coordinate of the target cell.\n\n        Returns:\n            tuple: A tuple containing:\n                - float: Accumulated cost at the target, rounded.\n                - int: Number of steps in the reconstructed path.\n                - bool: True if the path properties are valid, False otherwise.\n        \"\"\"\n        m, n = cost_raster.shape\n        \n        # Initialize accumulated cost raster A and predecessor raster P\n        costs = np.full((m, n), np.inf, dtype=np.float64)\n        predecessors = np.full((m, n), -1, dtype=np.intp)\n        \n        # Priority queue stores (cost, row, col)\n        pq = []\n\n        for r_s, c_s in sources:\n            if 0 <= r_s < m and 0 <= c_s < n:\n                costs[r_s, c_s] = 0.0\n                heapq.heappush(pq, (0.0, r_s, c_s))\n\n        # Define neighbor movements and distances\n        if connectivity == 4:\n            neighbors_def = [\n                (0, 1, 1.0), (0, -1, 1.0), (1, 0, 1.0), (-1, 0, 1.0)\n            ]\n        elif connectivity == 8:\n            sqrt2 = np.sqrt(2)\n            neighbors_def = [\n                (0, 1, 1.0), (0, -1, 1.0), (1, 0, 1.0), (-1, 0, 1.0),\n                (1, 1, sqrt2), (1, -1, sqrt2), (-1, 1, sqrt2), (-1, -1, sqrt2)\n            ]\n        else:\n            raise ValueError(\"Connectivity must be 4 or 8.\")\n            \n        # Dijkstra's algorithm\n        while pq:\n            cost, r, c = heapq.heappop(pq)\n            \n            # Skip stale entries\n            if cost > costs[r, c]:\n                continue\n            \n            # Skip impassable cells\n            if np.isinf(cost_raster[r,c]):\n                continue\n\n            for dr, dc, dist in neighbors_def:\n                nr, nc = r + dr, c + dc\n                \n                if 0 <= nr < m and 0 <= nc < n:\n                    # Skip neighbors that are impassable\n                    if np.isinf(cost_raster[nr, nc]):\n                        continue\n\n                    # Calculate edge weight using trapezoidal rule\n                    edge_weight = (cost_raster[r, c] + cost_raster[nr, nc]) / 2.0 * dist\n                    new_cost = costs[r, c] + edge_weight\n                    \n                    if new_cost < costs[nr, nc]:\n                        costs[nr, nc] = new_cost\n                        predecessors[nr, nc] = r * n + c\n                        heapq.heappush(pq, (new_cost, nr, nc))\n\n        # Path reconstruction and verification\n        target_cost = costs[target]\n        if np.isinf(target_cost):\n            return [np.inf, 0, False]\n\n        path = []\n        curr_r, curr_c = target\n        num_steps = 0\n        \n        if predecessors[curr_r, curr_c] != -1 or (curr_r, curr_c) in sources:\n            while True:\n                path.append((curr_r, curr_c))\n                pred_flat_idx = predecessors[curr_r, curr_c]\n                if pred_flat_idx == -1:\n                    break\n                pred_r, pred_c = divmod(pred_flat_idx, n)\n                curr_r, curr_c = pred_r, pred_c\n                num_steps += 1\n        \n        path.reverse() # Path from source to target\n        \n        # Verification\n        path_is_valid = True\n        total_path_cost_sum = 0.0\n        tolerance = 1e-9\n\n        if num_steps > 0:\n            for i in range(num_steps):\n                u_r, u_c = path[i]\n                v_r, v_c = path[i+1]\n                \n                dr, dc = v_r - u_r, v_c - u_c\n                dist = np.sqrt(dr**2 + dc**2)\n                \n                edge_weight = (cost_raster[u_r, u_c] + cost_raster[v_r, v_c]) / 2.0 * dist\n                total_path_cost_sum += edge_weight\n                \n                # Check edge-additive property\n                if not np.isclose(costs[v_r, v_c], costs[u_r, u_c] + edge_weight, atol=tolerance):\n                    path_is_valid = False\n                    break\n            \n            # Check total path cost property\n            if path_is_valid and not np.isclose(total_path_cost_sum, target_cost, atol=tolerance):\n                path_is_valid = False\n        elif target not in sources:\n             # Target is not a source, but path has 0 steps. This is an invalid state\n             # unless the target is unreachable, which is handled earlier.\n             path_is_valid = False\n\n        return [round(target_cost, 6), num_steps, path_is_valid]\n\n\n    # Test Case 1\n    C1 = np.ones((5, 5), dtype=np.float64)\n    S1 = [(0, 0)]\n    t1 = (4, 3)\n    \n    # Test Case 2\n    C2 = np.ones((5, 5), dtype=np.float64)\n    C2[2, 1] = C2[2, 2] = C2[2, 3] = np.inf\n    S2 = [(0, 2)]\n    t2 = (4, 2)\n    \n    # Test Case 3\n    C3 = np.fromfunction(lambda i, j: 1 + 0.15 * i + 0.05 * j, (6, 6), dtype=np.float64)\n    S3 = [(0, 0), (5, 5)]\n    t3 = (5, 0)\n\n    # Test Case 4\n    C4 = np.ones((4, 4), dtype=np.float64)\n    S4 = [(0, 3), (3, 0)]\n    t4 = (3, 3)\n\n    test_cases = [\n        (C1, 8, S1, t1),\n        (C2, 4, S2, t2),\n        (C3, 8, S3, t3),\n        (C4, 8, S4, t4),\n    ]\n    \n    results = []\n    for C, N_conn, S, t in test_cases:\n        result = run_least_cost_path(C, N_conn, S, t)\n        results.append(result)\n\n    print(str(results).replace(\"True\", \"true\").replace(\"False\", \"false\"))\n\nsolve()\n```"
        }
    ]
}