{
    "hands_on_practices": [
        {
            "introduction": "In environmental modeling, we frequently compute indices like the Normalized Difference Vegetation Index (NDVI) from multiple raster bands. However, these source bands are measurements and inherently contain uncertainty. This practice guides you through one of the most fundamental tasks in quantitative analysis: propagating uncertainty from source data into a derived product. You will use a first-order Taylor expansion, a technique known as the Delta method, to approximate the variance of a normalized difference index, paying close attention to the critical and often-overlooked effect of covariance between the input bands .",
            "id": "3840042",
            "problem": "Consider two co-registered per-pixel top-of-atmosphere reflectance bands, denoted by the random variables $X$ and $Y$, derived from a physically based atmospheric correction model. The normalized difference index for these two bands is defined by the differentiable mapping $I(X,Y) = \\frac{X - Y}{X + Y}$. Assume that at a particular pixel the bandwise reflectances are characterized by the following statistical summaries: expected values $\\mu_{X} = 0.52$ and $\\mu_{Y} = 0.32$, variances $\\sigma_{X}^{2} = (0.015)^{2}$ and $\\sigma_{Y}^{2} = (0.012)^{2}$, and covariance $\\sigma_{XY} = 0.00015$. Using only the definitions of variance, covariance, and a first-order Taylor expansion for a differentiable function of random variables about $(\\mu_{X}, \\mu_{Y})$, derive the expression for the variance of $I$ at this pixel and compute its numerical value. Then, explain qualitatively under what conditions ignoring covariance (i.e., treating $X$ and $Y$ as uncorrelated) severely misestimates the uncertainty of $I$.\n\nRound the final numerical variance of $I$ to four significant figures. Express the final variance as a decimal number without units (the index is dimensionless).",
            "solution": "The problem requires the derivation and calculation of the variance of a normalized difference index, $I(X,Y)$, and a qualitative analysis of the effect of covariance on this variance. The solution proceeds in three parts as requested.\n\nFirst, the problem statement is validated.\n**Step 1: Extract Givens**\n-   Random variables for reflectance bands: $X$, $Y$.\n-   Normalized difference index: $I(X,Y) = \\frac{X - Y}{X + Y}$.\n-   The mapping $I(X,Y)$ is differentiable.\n-   Expected values: $\\mu_{X} = 0.52$, $\\mu_{Y} = 0.32$.\n-   Variances: $\\sigma_{X}^{2} = (0.015)^{2}$, $\\sigma_{Y}^{2} = (0.012)^{2}$.\n-   Covariance: $\\sigma_{XY} = 0.00015$.\n-   Method: First-order Taylor expansion for a function of random variables.\n-   Required output: 1. Derived expression for $\\sigma_I^2$. 2. Numerical value of $\\sigma_I^2$ rounded to four significant figures. 3. Qualitative explanation of the impact of ignoring covariance.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, well-posed, and objective. It presents a standard application of the propagation of uncertainty (error propagation) using the Delta method, a first-order Taylor series approximation. This technique is fundamental in statistics and widely used in remote sensing to estimate the uncertainty of derived indices like NDVI. The provided reflectance values and their variances are physically realistic for top-of-atmosphere measurements. All necessary parameters are given, and the problem is free of contradictions, ambiguities, or non-scientific claims.\n\n**Step 3: Verdict and Action**\nThe problem is **valid**. A full solution will be provided.\n\n**Part 1: Derivation of the Variance Expression**\n\nThe variance of a differentiable function $I(X,Y)$ of two random variables $X$ and $Y$ can be approximated using a first-order Taylor series expansion of $I(X,Y)$ around the mean values $(\\mu_X, \\mu_Y)$. The general formula for the variance, $\\sigma_{I}^{2}$, is given by:\n$$ \\sigma_{I}^{2} \\approx \\left( \\left. \\frac{\\partial I}{\\partial X} \\right|_{(\\mu_X, \\mu_Y)} \\right)^2 \\sigma_X^2 + \\left( \\left. \\frac{\\partial I}{\\partial Y} \\right|_{(\\mu_X, \\mu_Y)} \\right)^2 \\sigma_Y^2 + 2 \\left( \\left. \\frac{\\partial I}{\\partial X} \\right|_{(\\mu_X, \\mu_Y)} \\right) \\left( \\left. \\frac{\\partial I}{\\partial Y} \\right|_{(\\mu_X, \\mu_Y)} \\right) \\sigma_{XY} $$\nThe function is $I(X,Y) = \\frac{X - Y}{X + Y}$. We must first compute the partial derivatives of $I$ with respect to $X$ and $Y$.\n\nUsing the quotient rule for differentiation, the partial derivative with respect to $X$ is:\n$$ \\frac{\\partial I}{\\partial X} = \\frac{(1)(X+Y) - (X-Y)(1)}{(X+Y)^2} = \\frac{X+Y-X+Y}{(X+Y)^2} = \\frac{2Y}{(X+Y)^2} $$\nThe partial derivative with respect to $Y$ is:\n$$ \\frac{\\partial I}{\\partial Y} = \\frac{(-1)(X+Y) - (X-Y)(1)}{(X+Y)^2} = \\frac{-X-Y-X+Y}{(X+Y)^2} = \\frac{-2X}{(X+Y)^2} $$\nNext, we evaluate these partial derivatives at the mean values $(\\mu_X, \\mu_Y)$:\n$$ \\left. \\frac{\\partial I}{\\partial X} \\right|_{(\\mu_X, \\mu_Y)} = \\frac{2\\mu_Y}{(\\mu_X + \\mu_Y)^2} $$\n$$ \\left. \\frac{\\partial I}{\\partial Y} \\right|_{(\\mu_X, \\mu_Y)} = \\frac{-2\\mu_X}{(\\mu_X + \\mu_Y)^2} $$\nSubstituting these expressions into the general variance formula yields the derived expression for the variance of $I$:\n$$ \\sigma_I^2 \\approx \\left( \\frac{2\\mu_Y}{(\\mu_X + \\mu_Y)^2} \\right)^2 \\sigma_X^2 + \\left( \\frac{-2\\mu_X}{(\\mu_X + \\mu_Y)^2} \\right)^2 \\sigma_Y^2 + 2 \\left( \\frac{2\\mu_Y}{(\\mu_X + \\mu_Y)^2} \\right) \\left( \\frac{-2\\mu_X}{(\\mu_X + \\mu_Y)^2} \\right) \\sigma_{XY} $$\nSimplifying this expression, we get:\n$$ \\sigma_I^2 \\approx \\frac{4\\mu_Y^2}{(\\mu_X + \\mu_Y)^4} \\sigma_X^2 + \\frac{4\\mu_X^2}{(\\mu_X + \\mu_Y)^4} \\sigma_Y^2 - \\frac{8\\mu_X\\mu_Y}{(\\mu_X + \\mu_Y)^4} \\sigma_{XY} $$\nThis can be written more compactly as:\n$$ \\sigma_I^2 \\approx \\frac{4}{(\\mu_X + \\mu_Y)^4} \\left( \\mu_Y^2 \\sigma_X^2 + \\mu_X^2 \\sigma_Y^2 - 2 \\mu_X \\mu_Y \\sigma_{XY} \\right) $$\nThis completes the derivation of the expression for the variance of $I$.\n\n**Part 2: Numerical Computation of the Variance**\n\nWe are given the following values:\n$\\mu_X = 0.52$\n$\\mu_Y = 0.32$\n$\\sigma_X^2 = (0.015)^2 = 0.000225$\n$\\sigma_Y^2 = (0.012)^2 = 0.000144$\n$\\sigma_{XY} = 0.00015$\n\nFirst, let's compute the denominator term:\n$$ (\\mu_X + \\mu_Y)^4 = (0.52 + 0.32)^4 = (0.84)^4 = 0.49787136 $$\nNow, let's compute the terms within the parentheses in the numerator's expression:\n$$ \\mu_Y^2 \\sigma_X^2 = (0.32)^2 (0.015)^2 = (0.1024)(0.000225) = 0.00002304 $$\n$$ \\mu_X^2 \\sigma_Y^2 = (0.52)^2 (0.012)^2 = (0.2704)(0.000144) = 0.0000389376 $$\n$$ 2 \\mu_X \\mu_Y \\sigma_{XY} = 2(0.52)(0.32)(0.00015) = (0.3328)(0.00015) = 0.00004992 $$\nCombining these terms:\n$$ \\mu_Y^2 \\sigma_X^2 + \\mu_X^2 \\sigma_Y^2 - 2 \\mu_X \\mu_Y \\sigma_{XY} = 0.00002304 + 0.0000389376 - 0.00004992 = 0.0000120576 $$\nFinally, we compute $\\sigma_I^2$:\n$$ \\sigma_I^2 \\approx \\frac{4}{0.49787136} \\times (0.0000120576) \\approx (8.034292)(0.0000120576) \\approx 0.0000968731 $$\nRounding to four significant figures, the numerical value for the variance is:\n$$ \\sigma_I^2 \\approx 0.00009687 $$\n\n**Part 3: Qualitative Explanation of the Covariance Effect**\n\nIgnoring covariance (i.e., assuming $X$ and $Y$ are uncorrelated, $\\sigma_{XY}=0$) means omitting the term $2 \\left(\\frac{\\partial I}{\\partial X}\\right) \\left(\\frac{\\partial I}{\\partial Y}\\right) \\sigma_{XY}$ from the variance calculation. For the normalized difference index $I = \\frac{X-Y}{X+Y}$, the partial derivatives have opposite signs, as $\\frac{\\partial I}{\\partial X} = \\frac{2Y}{(X+Y)^2} > 0$ and $\\frac{\\partial I}{\\partial Y} = \\frac{-2X}{(X+Y)^2} < 0$ for positive reflectances $X$ and $Y$. Consequently, their product is negative:\n$$ \\left(\\frac{\\partial I}{\\partial X}\\right) \\left(\\frac{\\partial I}{\\partial Y}\\right) < 0 $$\nThe effect of ignoring covariance thus depends critically on the sign of the covariance $\\sigma_{XY}$ itself:\n\n1.  **Positive Covariance ($\\sigma_{XY} > 0$)**: This is the typical case for reflectance bands from the same sensor, as surface properties and atmospheric conditions tend to affect them similarly. With $\\sigma_{XY} > 0$ and a negative product of partials, the full covariance term $2 (\\partial I / \\partial X) (\\partial I / \\partial Y) \\sigma_{XY}$ is negative. Neglecting this term (i.e., treating it as zero) removes a negative contribution from the total variance, thereby causing an **overestimation** of the uncertainty of $I$.\n\n2.  **Negative Covariance ($\\sigma_{XY} < 0$)**: In the less common case where bands are negatively correlated, the full covariance term becomes positive. Ignoring it would then cause an **underestimation** of the uncertainty of $I$.\n\nA severe misestimation occurs when the magnitude of the covariance term, $|2 (\\partial I / \\partial X) (\\partial I / \\partial Y) \\sigma_{XY}|$, is large relative to the sum of the variance terms, $(\\partial I / \\partial X)^2 \\sigma_X^2 + (\\partial I / \\partial Y)^2 \\sigma_Y^2$. This happens when the correlation coefficient, $\\rho_{XY} = \\frac{\\sigma_{XY}}{\\sigma_X \\sigma_Y}$, is close to $+1$ or $-1$. In summary, ignoring covariance is most problematic when the input variables are strongly correlated. For remote sensing indices like the one in this problem, where input bands are often strongly positively correlated, treating them as uncorrelated typically results in a significant and systematic overestimation of the index's uncertainty.",
            "answer": "$$\\boxed{0.00009687}$$"
        },
        {
            "introduction": "As remote sensing datasets grow in size, the computational efficiency of our algorithms becomes paramount. A common task is computing zonal statistics, such as summarizing pixel values within predefined administrative or ecological zones. This hands-on practice challenges you to move beyond simple but slow methods by implementing a highly efficient algorithm using Summed-Area Tables (SATs), also known as integral images. By comparing the performance of a naive scanning approach against the SAT-based method, you will gain a practical understanding of algorithmic complexity and how a one-time preprocessing cost can be amortized to enable near-instantaneous queries, a cornerstone of high-performance geospatial data analysis .",
            "id": "3840046",
            "problem": "You are given a two-dimensional raster and a set of axis-aligned rectangular zones. The task is to design and implement an algorithm, grounded in raster algebra concepts, that computes zonal histograms efficiently using Summed-Area Tables (SAT; also known as integral images), and to evaluate its complexity relative to naive scanning. The raster is a function over a regular grid, and the zonal histogram counts, for each rectangular zone, the number of raster cells whose values fall into prescribed bins. The problem must be solved by deriving the method from fundamental definitions of discrete sums and set operations.\n\nDefinitions and assumptions:\n- Let the raster be a function $R : \\{0,1,\\dots,H-1\\} \\times \\{0,1,\\dots,W-1\\} \\to \\mathbb{R}$ with height $H$ and width $W$.\n- Let there be $B$ histogram bins defined by edges $\\mathbf{a} = \\big(a_0, a_1, \\dots, a_B\\big)$, where $a_0 < a_1 < \\dots < a_B$. A value $x$ belongs to bin $k \\in \\{0,1,\\dots,B-1\\}$ if $a_k \\le x < a_{k+1}$, and if $x = a_B$, then $x$ is assigned to bin $B-1$.\n- A rectangular zone is specified by inclusive indices $(r_0, r_1, c_0, c_1)$ with $0 \\le r_0 \\le r_1 < H$ and $0 \\le c_0 \\le c_1 < W$.\n- An optional NoData value $v_{\\text{nodata}} \\in \\mathbb{R}$ may be specified; cells equal to $v_{\\text{nodata}}$ are excluded from all histograms.\n\nYour program must:\n1. Quantize the raster into bin indices according to the bin edges (excluding NoData).\n2. Compute zonal histograms using two methods:\n   - Naive scanning: For each zone, iterate over its pixels and increment the appropriate bin count for non-NoData cells.\n   - Summed-Area Table (SAT)-based method: For each bin $k$, construct an integral image over the indicator function $\\phi_k(r,c) = 1$ if the cell belongs to bin $k$ and is not NoData, and $\\phi_k(r,c) = 0$ otherwise. Use inclusion-exclusion to compute counts for each rectangular zone from the SAT with constant-time queries per bin.\n3. Evaluate operation counts under the following scalar operation cost model:\n   - SAT construction cost: For each bin, computing a two-pass cumulative sum (first along rows, then along columns) costs $2 \\cdot H \\cdot W$ additions. For $B$ bins, total SAT construction cost is $2 \\cdot H \\cdot W \\cdot B$ additions.\n   - SAT query cost: Each rectangular query per bin uses inclusion-exclusion with $4$ scalar operations (three additions and one subtraction). For $Z$ zones across $B$ bins, total SAT query cost is $4 \\cdot Z \\cdot B$ scalar operations.\n   - Naive scanning cost: For each zone, count one scalar addition for each non-NoData pixel scanned. The total cost is the sum, over zones, of their non-NoData pixel counts.\n   - Quantization cost is excluded from both methods to ensure a fair comparison since both methods rely on the same pre-quantization.\n4. Verify that the SAT-based histograms match the naive histograms for each test case.\n\nYou must express all mathematical statements, symbols, variables, operators, and numbers in LaTeX. The program should generate and process the following test suite:\n\nTest Case 1 (general case without NoData):\n- Raster $R$ of size $H = 6$, $W = 5$:\n  $$\n  \\begin{bmatrix}\n  12 & 15 & 8 & 7 & 6 \\\\\n  5 & 9 & 11 & 3 & 2 \\\\\n  1 & 4 & 0 & 13 & 10 \\\\\n  8 & 8 & 8 & 14 & 16 \\\\\n  20 & 18 & 17 & 7 & 5 \\\\\n  6 & 6 & 6 & 6 & 6\n  \\end{bmatrix}\n  $$\n- Bin edges $\\mathbf{a} = (0, 5, 10, 15, 20)$ so $B = 4$.\n- Zones:\n  - Zone $1$: $(r_0, r_1, c_0, c_1) = (0, 2, 1, 3)$.\n  - Zone $2$: $(r_0, r_1, c_0, c_1) = (3, 5, 0, 4)$.\n  - Zone $3$: $(r_0, r_1, c_0, c_1) = (0, 5, 0, 4)$.\n- NoData: none.\n\nTest Case 2 (boundary and NoData handling):\n- Raster $R$ of size $H = 3$, $W = 3$:\n  $$\n  \\begin{bmatrix}\n  0 & -9999 & 10 \\\\\n  5 & 5 & 5 \\\\\n  15 & 20 & 20\n  \\end{bmatrix}\n  $$\n- Bin edges $\\mathbf{a} = (0, 5, 10, 15, 20)$ so $B = 4$.\n- Zones:\n  - Zone $1$: $(r_0, r_1, c_0, c_1) = (0, 0, 0, 0)$.\n  - Zone $2$: $(r_0, r_1, c_0, c_1) = (0, 0, 1, 1)$.\n  - Zone $3$: $(r_0, r_1, c_0, c_1) = (1, 1, 0, 2)$.\n  - Zone $4$: $(r_0, r_1, c_0, c_1) = (0, 2, 0, 2)$.\n- NoData: $v_{\\text{nodata}} = -9999$.\n\nTest Case 3 (degenerate raster with all values equal):\n- Raster $R$ of size $H = 4$, $W = 4$ with all entries $7$.\n- Bin edges $\\mathbf{a} = (0, 7, 14)$ so $B = 2$.\n- Zones:\n  - Zone $1$: $(r_0, r_1, c_0, c_1) = (0, 3, 0, 3)$.\n- NoData: none.\n\nTest Case 4 (amortization over repeated large queries):\n- Raster $R$ of size $H = 40$, $W = 40$ with $R(r,c) = r + c$ for $r \\in \\{0,1,\\dots,39\\}$ and $c \\in \\{0,1,\\dots,39\\}$.\n- Bin edges $\\mathbf{a}$ are $9$ equally spaced edges from $0$ to $80$, i.e., $\\mathbf{a} = \\big(0, 10, 20, 30, 40, 50, 60, 70, 80\\big)$ so $B = 8$.\n- Zones: $200$ identical zones covering the entire raster, i.e., each zone has $(r_0, r_1, c_0, c_1) = (0, 39, 0, 39)$.\n- NoData: none.\n\nYour program must output, for each test case, a list containing:\n- A boolean indicating whether all zonal histograms from the SAT-based method exactly match those from naive scanning.\n- The total naive operation count as an integer under the specified cost model.\n- The total SAT operation count as an integer under the specified cost model.\n- The speed ratio (naive operations divided by SAT operations) as a float.\n\nFinal output format:\n- Your program should produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets, where each test case result is itself a list in the order described above. For example: $\\big[\\,[\\text{True},\\,100,\\,48,\\,2.0833333333333335],\\dots\\big]$.",
            "solution": "The present problem requires the design, implementation, and complexity analysis of two algorithms for computing zonal histograms on a two-dimensional raster: a naive scanning method and a more efficient method based on Summed-Area Tables (SATs). The solution will be derived from the fundamental principles of discrete summation and set theory.\n\n### **1. Formal Problem Definition**\n\nLet the raster be a grid function $R(r, c)$ defined over a discrete domain $D = \\{0, 1, \\dots, H-1\\} \\times \\{0, 1, \\dots, W-1\\}$, where $H$ is the height and $W$ is the width. The value at each cell $(r,c) \\in D$ is $R(r,c) \\in \\mathbb{R}$.\n\nA set of $B$ histogram bins is defined by an ordered sequence of $B+1$ bin edges $\\mathbf{a} = \\big(a_0, a_1, \\dots, a_B\\big)$ such that $a_0 < a_1 < \\dots < a_B$. A raster value $x$ is assigned to bin $k \\in \\{0, 1, \\dots, B-1\\}$ according to the rule:\n- If $a_k \\le x < a_{k+1}$, $x$ belongs to bin $k$.\n- As a special case for the rightmost boundary, if $x = a_B$, $x$ belongs to bin $B-1$.\nCells with a specified NoData value, $v_{\\text{nodata}}$, are to be ignored in all computations.\n\nA rectangular zone $\\mathcal{Z}$ is defined by its inclusive row and column indices $(r_0, r_1, c_0, c_1)$, where $0 \\le r_0 \\le r_1 < H$ and $0 \\le c_0 \\le c_1 < W$.\n\nThe task is to compute the histogram for each zone. The histogram for a zone $\\mathcal{Z}$ is a vector of counts $\\mathbf{h}(\\mathcal{Z}) = \\big(h_0, h_1, \\dots, h_{B-1}\\big)$, where $h_k$ is the number of cells $(r,c) \\in \\mathcal{Z}$ whose value $R(r,c)$ falls into bin $k$ and is not equal to $v_{\\text{nodata}}$.\n\n### **2. Preprocessing: Raster Quantization**\n\nBoth the naive and SAT-based methods benefit from a common preprocessing step: raster quantization. This involves creating a set of $B$ binary indicator rasters, or masks, $\\Phi = \\{\\phi_0, \\phi_1, \\dots, \\phi_{B-1}\\}$. Each mask $\\phi_k(r,c)$ corresponds to a bin $k$ and is defined as:\n$$\n\\phi_k(r,c) = \\begin{cases} 1 & \\text{if } R(r,c) \\text{ is in bin } k \\text{ and } R(r,c) \\ne v_{\\text{nodata}} \\\\ 0 & \\text{otherwise} \\end{cases}\n$$\nWith these indicator functions, the count $h_k$ for a zone $\\mathcal{Z}_{(r_0, r_1, c_0, c_1)}$ is formally expressed as a discrete two-dimensional sum:\n$$\nh_k(\\mathcal{Z}) = \\sum_{r=r_0}^{r_1} \\sum_{c=c_0}^{c_1} \\phi_k(r,c)\n$$\nThe cost of this quantization step is excluded from the comparative analysis as it is a prerequisite for both methods.\n\n### **3. Method 1: Naive Scanning**\n\nThe naive scanning algorithm is a direct implementation of the summation formula. For each specified zone $\\mathcal{Z}$, we iterate through every cell $(r,c)$ within its boundaries. For each cell, we retrieve its value $R(r,c)$, determine its corresponding bin index (if not NoData), and increment the appropriate counter in the zone's histogram.\n\n**Algorithmic Steps:**\n1. For each zone $\\mathcal{Z}_i$ in the set of $Z$ zones:\n2. Initialize a histogram vector $\\mathbf{h}(\\mathcal{Z}_i)$ of size $B$ to all zeros.\n3. For each cell $(r,c)$ from $(r_{i,0}, c_{i,0})$ to $(r_{i,1}, c_{i,1})$:\n   a. Let $v = R(r,c)$.\n   b. If $v \\ne v_{\\text{nodata}}$:\n      i. Determine the bin index $k$ for the value $v$.\n      ii. Increment the count for that bin: $h_k(\\mathcal{Z}_i) \\leftarrow h_k(\\mathcal{Z}_i) + 1$.\n\n**Complexity Analysis:**\nAccording to the problem's cost model, the cost for processing a single zone $\\mathcal{Z}$ is the number of its non-NoData cells, which we denote as $N(\\mathcal{Z})$. For a set of $Z$ zones $\\{\\mathcal{Z}_1, \\dots, \\mathcal{Z}_Z\\}$, the total computational cost is the sum of the costs for each zone:\n$$\nC_{\\text{naive}} = \\sum_{i=1}^{Z} N(\\mathcal{Z}_i)\n$$\nThis cost is directly proportional to the total area scanned across all zones. If zones overlap, the cells in the overlapping regions are processed multiple times.\n\n### **4. Method 2: Summed-Area Table (SAT)**\n\nThe Summed-Area Table (SAT), or integral image, is a data structure that allows for the rapid calculation of sums over rectangular subregions. The key insight is to trade a one-time pre-computation cost for extremely fast query times.\n\n**SAT Construction:**\nFor each bin $k$, we construct a corresponding SAT, denoted $S_k$. The value of $S_k$ at any coordinate $(r,c)$ is the sum of the indicator function values $\\phi_k(i,j)$ in the rectangle from the origin $(0,0)$ to $(r,c)$:\n$$\nS_k(r,c) = \\sum_{i=0}^{r} \\sum_{j=0}^{c} \\phi_k(i,j)\n$$\nThe SAT can be computed efficiently in a single pass over the indicator mask $\\phi_k$ using the recurrence relation:\n$$\nS_k(r,c) = \\phi_k(r,c) + S_k(r-1, c) + S_k(r, c-1) - S_k(r-1, c-1)\n$$\nwith boundary conditions $S_k(r,c) = 0$ if $r < 0$ or $c < 0$. This is typically implemented as a two-pass algorithm: first, a cumulative sum is computed along each row, and then a cumulative sum is computed along each column of the intermediate result.\nFor a raster of size $H \\times W$, each pass involves approximately $H \\times W$ additions. Thus, constructing one SAT costs $2 \\cdot H \\cdot W$ additions. For all $B$ bins, the total construction cost is:\n$$\nC_{\\text{construct}} = 2 \\cdot H \\cdot W \\cdot B\n$$\n\n**SAT Querying:**\nOnce the SATs $S_0, \\dots, S_{B-1}$ are built, the sum of $\\phi_k$ over any arbitrary rectangle $\\mathcal{Z}_{(r_0, r_1, c_0, c_1)}$ can be calculated in constant time using the principle of inclusion-exclusion. The sum is the area of the rectangle anchored at $(0,0)$ and extending to $(r_1, c_1)$, from which two overlapping rectangles are subtracted, and their common intersection is added back.\n$$\nh_k(\\mathcal{Z}) = \\sum_{r=r_0}^{r_1} \\sum_{c=c_0}^{c_1} \\phi_k(r,c) = S_k(r_1, c_1) - S_k(r_1, c_0-1) - S_k(r_0-1, c_1) + S_k(r_0-1, c_0-1)\n$$\nTo simplify index handling, especially for zones touching the raster boundaries (where $r_0-1$ or $c_0-1$ would be negative), it is conventional to pad the SAT with a row and a column of zeros. A query for bin $k$ requires $4$ lookups and $3$ arithmetic operations, which the problem defines as $4$ scalar operations. The total query cost for $Z$ zones across $B$ bins is:\n$$\nC_{\\text{query}} = 4 \\cdot Z \\cdot B\n$$\n\n**Total SAT-based Cost:**\nThe total cost of the SAT-based method is the sum of the construction and query costs:\n$$\nC_{\\text{SAT}} = C_{\\text{construct}} + C_{\\text{query}} = (2 \\cdot H \\cdot W \\cdot B) + (4 \\cdot Z \\cdot B)\n$$\n\n### **5. Comparative Analysis**\n\nThe choice between naive scanning and the SAT method depends on the problem parameters: raster size $(H, W)$, number of bins $(B)$, and the number $(Z)$ and size of the query zones.\n\n-   **Naive Cost:** $C_{\\text{naive}} \\propto \\sum (\\text{Area of Zone}_i)$\n-   **SAT Cost:** $C_{\\text{SAT}} \\propto (H \\cdot W \\cdot B) + (Z \\cdot B)$\n\nThe SAT method's cost is independent of the size of the query zones. It has a significant upfront cost for building the tables but a very low, constant-time cost per query. The naive method has no upfront cost but a query cost that scales linearly with zone area.\n\nThe SAT method becomes more efficient than the naive method when the number of queries is large or the zones themselves are large, causing the total area scanned by the naive method to exceed the SAT construction cost. The term $C_{\\text{construct}}$ is amortized over many queries. Test Case 4, with $200$ large zones, is designed to demonstrate this amortization effect, where the SAT method is expected to be substantially faster.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run test cases for zonal histogram calculation.\n    \"\"\"\n    \n    def quantize_raster(raster, bin_edges, nodata_val):\n        \"\"\"\n        Quantizes the raster into B binary indicator masks.\n        \n        A value x is in bin k if bin_edges[k] <= x < bin_edges[k+1].\n        If x == bin_edges[-1], it is in the last bin (B-1).\n        \n        Returns a (B, H, W) numpy array of indicator masks.\n        \"\"\"\n        H, W = raster.shape\n        num_bins = len(bin_edges) - 1\n        \n        # Initialize with a sentinel value for pixels outside all bins or nodata\n        bin_indices = np.full(raster.shape, -1, dtype=int)\n        \n        # Assign bin indices based on the rules.\n        # Bins 0 to B-2: a_k <= x < a_{k+1}\n        for k in range(num_bins - 1):\n            mask = (raster >= bin_edges[k]) & (raster < bin_edges[k+1])\n            bin_indices[mask] = k\n        \n        # Last bin (B-1): a_{B-1} <= x <= a_B\n        mask_last_bin = (raster >= bin_edges[num_bins - 1]) & (raster <= bin_edges[num_bins])\n        bin_indices[mask_last_bin] = num_bins - 1\n\n        # Mask out NoData values\n        if nodata_val is not None:\n            bin_indices[raster == nodata_val] = -1\n\n        # Create B indicator masks from the bin indices\n        quantized_masks = np.zeros((num_bins, H, W), dtype=int)\n        for k in range(num_bins):\n            quantized_masks[k, :, :] = (bin_indices == k)\n        \n        return quantized_masks\n\n    def naive_histograms(quantized_masks, zones, original_raster, nodata_val):\n        \"\"\"\n        Computes zonal histograms using naive scanning.\n        \"\"\"\n        histograms = []\n        naive_op_count = 0\n        \n        for r0, r1, c0, c1 in zones:\n            # Calculate operation cost for this zone\n            zone_raster = original_raster[r0:r1+1, c0:c1+1]\n            if nodata_val is not None:\n                naive_op_count += np.sum(zone_raster != nodata_val)\n            else:\n                naive_op_count += zone_raster.size\n\n            # Slice the indicator masks to get the zone\n            zone_masks = quantized_masks[:, r0:r1+1, c0:c1+1]\n            # Sum over the spatial dimensions to get the histogram\n            histogram = np.sum(zone_masks, axis=(1, 2))\n            histograms.append(histogram)\n            \n        return np.array(histograms), naive_op_count\n\n    def sat_histograms(quantized_masks, zones):\n        \"\"\"\n        Computes zonal histograms using Summed-Area Tables.\n        \"\"\"\n        num_bins, H, W = quantized_masks.shape\n        num_zones = len(zones)\n        \n        # --- SAT Construction ---\n        # Cost as per problem statement\n        sat_construction_cost = 2 * H * W * num_bins\n        \n        # Pad SATs with a row and column of zeros for easy querying\n        sats = np.zeros((num_bins, H + 1, W + 1), dtype=int)\n        \n        # Compute cumulative sums twice: once along columns (axis=2), then rows (axis=1)\n        # This is a vectorized implementation of the two-pass algorithm.\n        sats[:, 1:, 1:] = np.cumsum(np.cumsum(quantized_masks, axis=2), axis=1)\n\n        # --- SAT Querying ---\n        # Cost as per problem statement\n        sat_query_cost = 4 * num_zones * num_bins\n        total_sat_cost = sat_construction_cost + sat_query_cost\n\n        histograms = []\n        for r0, r1, c0, c1 in zones:\n            # Adjust indices for padded SAT. The query area corresponds to\n            # corners (r0, c0) and (r1+1, c1+1) in the padded SAT.\n            # D = S(r1, c1), C = S(r1, c0-1), B = S(r0-1, c1), A = S(r0-1, c0-1)\n            # Sum = D - B - C + A\n            # Indices in padded SAT: S(r,c) is accessed at sats[:, r+1, c+1]\n            # So, S(r1,c1) -> sats[:, r1+1, c1+1]\n            # S(r1,c0-1) -> sats[:, r1+1, c0]\n            # S(r0-1,c1) -> sats[:, r0, c1+1]\n            # S(r0-1,c0-1) -> sats[:, r0, c0]\n            # This is vectorized across all bins simultaneously.\n            D = sats[:, r1 + 1, c1 + 1]\n            B_term = sats[:, r1 + 1, c0]\n            C_term = sats[:, r0, c1 + 1]\n            A = sats[:, r0, c0]\n            histogram = D - B_term - C_term + A\n            histograms.append(histogram)\n            \n        return np.array(histograms), total_sat_cost\n\n    test_cases = [\n        # Test Case 1\n        {\n            \"R\": np.array([\n                [12, 15, 8, 7, 6],\n                [5, 9, 11, 3, 2],\n                [1, 4, 0, 13, 10],\n                [8, 8, 8, 14, 16],\n                [20, 18, 17, 7, 5],\n                [6, 6, 6, 6, 6]\n            ]),\n            \"a\": (0, 5, 10, 15, 20),\n            \"zones\": [(0, 2, 1, 3), (3, 5, 0, 4), (0, 5, 0, 4)],\n            \"nodata\": None\n        },\n        # Test Case 2\n        {\n            \"R\": np.array([\n                [0, -9999, 10],\n                [5, 5, 5],\n                [15, 20, 20]\n            ]),\n            \"a\": (0, 5, 10, 15, 20),\n            \"zones\": [(0, 0, 0, 0), (0, 0, 1, 1), (1, 1, 0, 2), (0, 2, 0, 2)],\n            \"nodata\": -9999\n        },\n        # Test Case 3\n        {\n            \"R\": np.full((4, 4), 7),\n            \"a\": (0, 7, 14),\n            \"zones\": [(0, 3, 0, 3)],\n            \"nodata\": None\n        },\n        # Test Case 4\n        {\n            \"R\": np.array([[r + c for c in range(40)] for r in range(40)]),\n            \"a\": tuple(range(0, 81, 10)),\n            \"zones\": [(0, 39, 0, 39)] * 200,\n            \"nodata\": None\n        },\n    ]\n\n    final_results = []\n    for case in test_cases:\n        raster = case[\"R\"]\n        bin_edges = case[\"a\"]\n        zones = case[\"zones\"]\n        nodata_val = case[\"nodata\"]\n\n        # 1. Quantize the raster\n        quantized_masks = quantize_raster(raster, bin_edges, nodata_val)\n\n        # 2a. Compute histograms and cost with naive scanning\n        naive_hists, naive_cost = naive_histograms(quantized_masks, zones, raster, nodata_val)\n\n        # 2b. Compute histograms and cost with SAT\n        sat_hists, sat_cost = sat_histograms(quantized_masks, zones)\n        \n        # 3. Verify results\n        is_match = np.array_equal(naive_hists, sat_hists)\n        \n        # 4. Calculate speed ratio\n        if sat_cost > 0:\n            speed_ratio = naive_cost / sat_cost\n        else: # Should not happen with the given cost models\n            speed_ratio = float('inf') if naive_cost > 0 else 1.0\n            \n        final_results.append([is_match, int(naive_cost), int(sat_cost), speed_ratio])\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(map(str, final_results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Beyond local and zonal operations, raster algebra provides powerful tools for global modeling, where the value of an output cell can depend on the entire raster. This practice explores a classic example: finding the least-cost path across a \"friction\" surface, a fundamental problem in fields from logistics and hydrology to wildlife ecology. You will translate the continuous concept of a cost surface into a discrete graph problem and implement Dijkstra's algorithm to compute the accumulated cost raster and reconstruct the optimal path from a target back to a source. This exercise bridges the conceptual gap between the raster data model and graph-based network analysis, a vital skill for advanced spatial modeling .",
            "id": "3840074",
            "problem": "You are given the conceptual task of computing least-cost paths on a spatial grid (a raster) where each cell stores a nonnegative per-unit-length traversal friction cost, and of formalizing the relationship between the accumulated cost raster and the shortest-path tree induced by a nonnegative weighted graph model. The raster algebra constructs in this problem must be derived from first principles that connect a continuous path integral of a spatially varying cost field to a discrete graph representation on a raster grid.\n\nBegin from the following fundamental base: a continuous cost field $c(\\mathbf{x}) \\ge 0$ over a domain $\\Omega \\subset \\mathbb{R}^2$, and a continuous path $\\gamma: [0, L] \\to \\Omega$ of total arclength $L$ with infinitesimal arclength measure $\\mathrm{d}s$. The accumulated cost of traversing $\\gamma$ is the path integral\n$$\nJ[\\gamma] = \\int_{0}^{L} c(\\gamma(s)) \\, \\mathrm{d}s,\n$$\nwhich is the spatial analog of integrating a nonnegative friction over arclength. For a raster with $m$ rows and $n$ columns, define the cost raster $C \\in \\mathbb{R}_{\\ge 0}^{m \\times n}$ such that $C_{i,j}$ approximates $c(\\mathbf{x}_{i,j})$ at the cell center $\\mathbf{x}_{i,j}$. Let the set of vertices be $V = \\{(i,j) \\mid i \\in \\{0,\\dots,m-1\\}, j \\in \\{0,\\dots,n-1\\}\\}$ and the set of edges $E$ connect each cell to its neighbors under a specified connectivity $\\mathcal{N}$, either $4$-connected or $8$-connected. Associate each edge $e = (u,v) \\in E$ with a Euclidean step length $d_{uv} \\in \\{1,\\sqrt{2}\\}$ depending on whether $u$ and $v$ are orthogonal or diagonal neighbors. Using a first-order accurate quadrature along each edge (for example, trapezoidal rule applied to the line segment between cell centers), approximate the edge weight $w_{uv}$ as the discrete analog of the integral of $c$ along that short segment. This defines a nonnegative weighted graph $(V,E,w)$, for which the accumulated cost from a source set $S \\subset V$ to any vertex $v \\in V$ is the minimum over all discrete paths from $S$ to $v$ of the sum of edge weights, and the corresponding predecessor relation defines a shortest-path tree (a forest when $|S| \\ge 2$).\n\nYour tasks are:\n- Implement a program that, given $C$, $\\mathcal{N}$, a source set $S \\subset V$, and one target $t \\in V$, constructs the weighted graph as above and computes the accumulated cost raster $A \\in \\mathbb{R}_{\\ge 0}^{m \\times n}$ and a predecessor raster $P \\in \\{-1\\} \\cup \\{0,1,\\dots,mn-1\\}^{m \\times n}$ using Dijkstraâ€™s algorithm on $(V,E,w)$, where $A_{v}$ is the minimal sum of edge weights from any source in $S$ to $v$, and $P_{v}$ encodes the unique predecessor of $v$ chosen by the algorithm in the shortest-path tree (or $-1$ for sources). The graph must respect the following: edges exist only between valid in-bounds neighbors under $\\mathcal{N}$, and any edge incident to an impassable cell with $C_{i,j} = +\\infty$ is excluded. Use a zero-based index convention for $(i,j)$ coordinates and a flat index mapping $f(i,j) = i \\cdot n + j$ for storing predecessors.\n- Reconstruct the least-cost path from the target $t$ back to its source by following $P$, compute the number of steps along the path, and verify the edge-additive property of the accumulated cost raster along the reconstructed path: for each consecutive pair $(u,v)$ in the path, check that $A_{v} = A_{u} + w_{uv}$ within a numerical tolerance of $10^{-9}$, and that the total sum of edge weights along the path equals $A_{t}$ within the same tolerance.\n\nUnits: all costs are unitless and represent cost units per unit length; step lengths are unitless and equal to $1$ for orthogonal moves and $\\sqrt{2}$ for diagonal moves by construction.\n\nAngle units: no angles are used in this problem.\n\nPercentages: not applicable.\n\nTest suite and parameter specification:\nDefine four test cases, each with a single target:\n- Test case $1$ (general case with $8$-connectivity and uniform cost):\n    - Raster size: $m = 5$, $n = 5$.\n    - Cost raster: $C_{i,j} = 1$ for all $(i,j)$.\n    - Connectivity: $\\mathcal{N} = 8$-connected.\n    - Sources: $S = \\{(0,0)\\}$.\n    - Target: $t = (4,3)$.\n- Test case $2$ (boundary case with an impassable barrier and $4$-connectivity):\n    - Raster size: $m = 5$, $n = 5$.\n    - Cost raster: $C_{i,j} = 1$ for all $(i,j)$ except $C_{2,1} = C_{2,2} = C_{2,3} = +\\infty$ (impassable).\n    - Connectivity: $\\mathcal{N} = 4$-connected.\n    - Sources: $S = \\{(0,2)\\}$.\n    - Target: $t = (4,2)$.\n- Test case $3$ (nonuniform gradient cost with multiple sources and $8$-connectivity):\n    - Raster size: $m = 6$, $n = 6$.\n    - Cost raster: $C_{i,j} = 1 + 0.15\\, i + 0.05\\, j$ for $i \\in \\{0,\\dots,5\\}$, $j \\in \\{0,\\dots,5\\}$.\n    - Connectivity: $\\mathcal{N} = 8$-connected.\n    - Sources: $S = \\{(0,0),(5,5)\\}$.\n    - Target: $t = (5,0)$.\n- Test case $4$ (edge case with multiple sources and ties under uniform cost):\n    - Raster size: $m = 4$, $n = 4$.\n    - Cost raster: $C_{i,j} = 1$ for all $(i,j)$.\n    - Connectivity: $\\mathcal{N} = 8$-connected.\n    - Sources: $S = \\{(0,3),(3,0)\\}$.\n    - Target: $t = (3,3)$.\n\nFinal output format:\nFor each test case, produce a list of three items: the accumulated cost to the target $A_{t}$ as a float rounded to $6$ decimal places, the number of steps along the reconstructed least-cost path to reach a source as an integer, and a boolean indicating whether the edge-additive property and total path cost equality hold within tolerance $10^{-9}$. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to a test case and is itself a list in the form $[A_{t}, \\text{steps}, \\text{valid}]$. For example, an output for two test cases would look like $[[a_1,s_1,v_1],[a_2,s_2,v_2]]$ with the actual values substituted.",
            "solution": "The problem statement has been meticulously reviewed and is determined to be **valid**. It is scientifically grounded in the principles of graph theory and numerical methods as applied to spatial analysis, specifically least-cost pathfinding on a raster. The problem is well-posed, with all necessary data, constraints, and definitions provided to ensure a unique and meaningful solution. It is objective and free of ambiguity. The task is to implement a computational solution based on these principles.\n\nThe solution proceeds by first discretizing the continuous cost-path problem into a shortest-path problem on a weighted graph. We then apply Dijkstra's algorithm to solve this discrete problem, and finally, we reconstruct and verify the resulting path.\n\n**1. Discretization: From Continuous Field to Discrete Graph**\n\nThe foundational concept is the transformation of a continuous problem into a discrete one amenable to computation. The continuous accumulated cost is given by the path integral $J[\\gamma] = \\int_{\\gamma} c(\\mathbf{x}) \\, \\mathrm{d}s$, where $c(\\mathbf{x})$ is a spatially varying friction cost field.\n\nWe discretize this as follows:\n- The continuous domain $\\Omega \\subset \\mathbb{R}^2$ is represented by a grid of $m \\times n$ cells. Each cell $(i,j)$ becomes a vertex $v_{i,j}$ in a graph $G = (V, E)$.\n- The cost field $c(\\mathbf{x})$ is sampled at the center of each cell, yielding the cost raster $C$, where $C_{i,j}$ is the cost value at vertex $v_{i,j}$.\n- A continuous path $\\gamma$ is approximated by a sequence of edges connecting adjacent vertices in the graph. The set of available edges $E$ is determined by the specified connectivity, either $4$-connected (orthogonal neighbors) or $8$-connected (orthogonal and diagonal neighbors).\n- The integral $\\int c(\\mathbf{x}) \\, \\mathrm{d}s$ along a short path segment between two adjacent vertices $u$ and $v$ is approximated by a sum of discrete edge weights, $w_{uv}$.\n\n**2. Edge Weight Formulation**\n\nThe problem specifies using a first-order accurate quadrature to approximate the cost integral along the straight-line segment connecting the centers of two adjacent cells, $u$ and $v$. The trapezoidal rule is a suitable choice. The cost integral along this segment is approximated as the average of the costs at the endpoints multiplied by the segment's length:\n$$\nw_{uv} \\approx \\frac{C_u + C_v}{2} d_{uv}\n$$\nwhere $C_u$ and $C_v$ are the cost values at cells $u$ and $v$, respectively, and $d_{uv}$ is the Euclidean distance between their centers. By convention, for a unit grid spacing:\n- $d_{uv} = 1$ for orthogonal neighbors.\n- $d_{uv} = \\sqrt{2}$ for diagonal neighbors.\n\nSince all costs $C_{i,j}$ are non-negative, all resulting edge weights $w_{uv}$ are also non-negative. This is a critical prerequisite for the correctness of Dijkstra's algorithm. Edges connected to impassable cells (where $C_{i,j} = +\\infty$) are considered to have infinite weight and are effectively excluded from the graph.\n\n**3. Dijkstra's Algorithm for Least-Cost Path**\n\nWith a graph $G = (V,E,w)$ where all edge weights $w$ are non-negative, Dijkstra's algorithm correctly finds the path with the minimum accumulated cost from a set of source vertices $S$ to all other reachable vertices.\n\nThe algorithm maintains three key data structures:\n- An accumulated cost raster $A$, where $A_v$ stores the currently known shortest-path cost from any source in $S$ to vertex $v$. It is initialized to $0$ for all $s \\in S$ and to $+\\infty$ for all other vertices.\n- A predecessor raster $P$, where $P_v$ stores the index of the vertex that precedes $v$ on the shortest path from $S$. Source vertices have a special predecessor value of $-1$. We use a flat index mapping $f(i,j) = i \\cdot n + j$ for this purpose.\n- A priority queue `pq`, which stores vertices to visit, prioritized by their accumulated cost. It is initialized with all source vertices.\n\nThe algorithm proceeds as follows:\n1.  Initialize $A$, $P$, and `pq` as described above.\n2.  While the priority queue is not empty, extract the vertex $u$ with the smallest accumulated cost.\n3.  For each valid neighbor $v$ of $u$ (as defined by the connectivity $\\mathcal{N}$ and grid boundaries):\n    a. Calculate the edge weight $w_{uv} = ((C_u + C_v)/2) \\cdot d_{uv}$.\n    b. Compute the potential new cost to reach $v$ through $u$: $A_u + w_{uv}$.\n    c. If this new cost is less than the current cost $A_v$, update $A_v$ to the new, lower cost, set $P_v$ to the flat index of $u$, and add $v$ to the priority queue with its new cost.\n\nUpon termination, $A$ contains the final least-cost values from the source set $S$ to every other cell, and $P$ encodes the shortest-path tree (a forest, in the case of multiple disconnected components or multiple sources).\n\n**4. Path Reconstruction and Verification**\n\nGiven the computed predecessor raster $P$ and a target vertex $t$, the least-cost path can be reconstructed by starting at $t$ and iteratively stepping backward to its predecessor until a source vertex (with predecessor $-1$) is reached.\n\nThe number of steps is the number of edges in this reconstructed path.\n\nVerification involves two checks, performed with a numerical tolerance of $10^{-9}$:\n- **Edge-Additive Property**: For every pair of consecutive vertices $(u, v)$ on the reconstructed path, where $u$ is the predecessor of $v$, we must verify that the costs are consistent: $A_v = A_u + w_{uv}$.\n- **Total Path Cost**: The sum of all edge weights $w_{uv}$ along the entire reconstructed path must equal the final accumulated cost at the target, $A_t$.\n\nIf both properties hold true for the entire path, the implementation is considered valid.",
            "answer": "```python\nimport numpy as np\nimport heapq\n\ndef solve():\n    \"\"\"\n    Main function to define test cases and orchestrate the solution process.\n    \"\"\"\n\n    def run_least_cost_path(cost_raster, connectivity, sources, target):\n        \"\"\"\n        Computes the least-cost path, reconstructs it, and verifies its properties.\n\n        Args:\n            cost_raster (np.ndarray): The m x n raster of cost values.\n            connectivity (int): The neighborhood connectivity (4 or 8).\n            sources (list of tuples): A list of (row, col) source coordinates.\n            target (tuple): The (row, col) coordinate of the target cell.\n\n        Returns:\n            tuple: A tuple containing:\n                - float: Accumulated cost at the target, rounded.\n                - int: Number of steps in the reconstructed path.\n                - bool: True if the path properties are valid, False otherwise.\n        \"\"\"\n        m, n = cost_raster.shape\n        \n        # Initialize accumulated cost raster A and predecessor raster P\n        costs = np.full((m, n), np.inf, dtype=np.float64)\n        predecessors = np.full((m, n), -1, dtype=np.intp)\n        \n        # Priority queue stores (cost, row, col)\n        pq = []\n\n        for r_s, c_s in sources:\n            if 0 <= r_s < m and 0 <= c_s < n:\n                costs[r_s, c_s] = 0.0\n                heapq.heappush(pq, (0.0, r_s, c_s))\n\n        # Define neighbor movements and distances\n        if connectivity == 4:\n            neighbors_def = [\n                (0, 1, 1.0), (0, -1, 1.0), (1, 0, 1.0), (-1, 0, 1.0)\n            ]\n        elif connectivity == 8:\n            sqrt2 = np.sqrt(2)\n            neighbors_def = [\n                (0, 1, 1.0), (0, -1, 1.0), (1, 0, 1.0), (-1, 0, 1.0),\n                (1, 1, sqrt2), (1, -1, sqrt2), (-1, 1, sqrt2), (-1, -1, sqrt2)\n            ]\n        else:\n            raise ValueError(\"Connectivity must be 4 or 8.\")\n            \n        # Dijkstra's algorithm\n        while pq:\n            cost, r, c = heapq.heappop(pq)\n            \n            # Skip stale entries\n            if cost > costs[r, c]:\n                continue\n            \n            # Skip impassable cells\n            if np.isinf(cost_raster[r,c]):\n                continue\n\n            for dr, dc, dist in neighbors_def:\n                nr, nc = r + dr, c + dc\n                \n                if 0 <= nr < m and 0 <= nc < n:\n                    # Skip neighbors that are impassable\n                    if np.isinf(cost_raster[nr, nc]):\n                        continue\n\n                    # Calculate edge weight using trapezoidal rule\n                    edge_weight = (cost_raster[r, c] + cost_raster[nr, nc]) / 2.0 * dist\n                    new_cost = costs[r, c] + edge_weight\n                    \n                    if new_cost < costs[nr, nc]:\n                        costs[nr, nc] = new_cost\n                        predecessors[nr, nc] = r * n + c\n                        heapq.heappush(pq, (new_cost, nr, nc))\n\n        # Path reconstruction and verification\n        target_cost = costs[target]\n        if np.isinf(target_cost):\n            return [np.inf, 0, False]\n\n        path = []\n        curr_r, curr_c = target\n        num_steps = 0\n        \n        if predecessors[curr_r, curr_c] != -1 or (curr_r, curr_c) in sources:\n            while True:\n                path.append((curr_r, curr_c))\n                pred_flat_idx = predecessors[curr_r, curr_c]\n                if pred_flat_idx == -1:\n                    break\n                pred_r, pred_c = divmod(pred_flat_idx, n)\n                curr_r, curr_c = pred_r, pred_c\n                num_steps += 1\n        \n        path.reverse() # Path from source to target\n        \n        # Verification\n        path_is_valid = True\n        total_path_cost_sum = 0.0\n        tolerance = 1e-9\n\n        if num_steps > 0:\n            for i in range(num_steps):\n                u_r, u_c = path[i]\n                v_r, v_c = path[i+1]\n                \n                dr, dc = v_r - u_r, v_c - u_c\n                dist = np.sqrt(dr**2 + dc**2)\n                \n                edge_weight = (cost_raster[u_r, u_c] + cost_raster[v_r, v_c]) / 2.0 * dist\n                total_path_cost_sum += edge_weight\n                \n                # Check edge-additive property\n                if not np.isclose(costs[v_r, v_c], costs[u_r, u_c] + edge_weight, atol=tolerance):\n                    path_is_valid = False\n                    break\n            \n            # Check total path cost property\n            if path_is_valid and not np.isclose(total_path_cost_sum, target_cost, atol=tolerance):\n                path_is_valid = False\n        elif target not in sources:\n             # Target is not a source, but path has 0 steps. This is an invalid state\n             # unless the target is unreachable, which is handled earlier.\n             path_is_valid = False\n\n        return [round(target_cost, 6), num_steps, path_is_valid]\n\n\n    # Test Case 1\n    C1 = np.ones((5, 5), dtype=np.float64)\n    S1 = [(0, 0)]\n    t1 = (4, 3)\n    \n    # Test Case 2\n    C2 = np.ones((5, 5), dtype=np.float64)\n    C2[2, 1] = C2[2, 2] = C2[2, 3] = np.inf\n    S2 = [(0, 2)]\n    t2 = (4, 2)\n    \n    # Test Case 3\n    C3 = np.fromfunction(lambda i, j: 1 + 0.15 * i + 0.05 * j, (6, 6), dtype=np.float64)\n    S3 = [(0, 0), (5, 5)]\n    t3 = (5, 0)\n\n    # Test Case 4\n    C4 = np.ones((4, 4), dtype=np.float64)\n    S4 = [(0, 3), (3, 0)]\n    t4 = (3, 3)\n\n    test_cases = [\n        (C1, 8, S1, t1),\n        (C2, 4, S2, t2),\n        (C3, 8, S3, t3),\n        (C4, 8, S4, t4),\n    ]\n    \n    results = []\n    for C, N_conn, S, t in test_cases:\n        result = run_least_cost_path(C, N_conn, S, t)\n        results.append(result)\n\n    # Convert boolean to string 'True' or 'False' for final printing\n    formatted_results = [f\"[{res[0]}, {res[1]}, {str(res[2])}]\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}