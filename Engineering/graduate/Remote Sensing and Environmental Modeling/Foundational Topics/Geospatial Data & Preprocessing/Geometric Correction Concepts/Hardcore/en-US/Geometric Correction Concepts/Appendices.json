{
    "hands_on_practices": [
        {
            "introduction": "Before we can correct geometric distortions, we must first understand their origins. A fundamental distortion in any central perspective imagery, such as that from a frame camera, is relief displacement—the apparent leaning of tall objects away from the image center. This hands-on practice  tasks you with deriving the classic relief displacement formula from first principles, building a solid intuition for how terrain height interacts with sensor geometry. Mastering this concept is essential for visual image interpretation and for appreciating the core principle behind orthorectification.",
            "id": "3815616",
            "problem": "A frame-imaging satellite acquires a vertical photograph over terrain that can be locally approximated by a horizontal reference plane. Model the imaging as a central perspective (pinhole) camera with focal length $f$ measured in pixel units, and adopt an image coordinate system in which the principal point is at the origin and radial distance from the principal point is measured in pixels. Let the satellite be at height $H$ above the reference plane along the camera’s optical axis. Consider a small, isolated feature of height $h$ above the reference plane. The base of the feature projects in the image at radial distance $r$ from the principal point.\n\nUsing only the central perspective projection relationship that an object point at distance $Z$ from the camera center along the optical axis and lateral ground offset $R$ projects to image radial coordinate $r = f\\,R/Z$, derive from first principles an exact expression for the radial relief displacement in the image, defined as the outward radial shift between the projected positions of the feature’s top and base, in terms of $r$, $f$, $H$, and $h$. Then evaluate this displacement numerically for $h = 50\\,\\mathrm{m}$, $r = 2000\\,\\mathrm{px}$, $H = 600\\,\\mathrm{km}$, and $f = 1000\\,\\mathrm{px}$. Round your final numerical result to four significant figures. Express the answer in pixels.",
            "solution": "The problem asks for the derivation of an expression for radial relief displacement and its numerical evaluation. The problem is physically and mathematically well-defined, based on the principles of central perspective projection, and is therefore valid.\n\nWe begin from first principles. The imaging system is modeled as a pinhole camera, whose geometry is governed by similar triangles. Let the camera's center of projection (the pinhole) be the origin $O$. The optical axis is aligned with the $Z$-axis, and the image plane is parallel to the $XY$-plane at a distance $f$ from the origin. A point in object space with coordinates $(X, Y, Z)$ and a lateral offset from the optical axis $R = \\sqrt{X^2 + Y^2}$ is projected onto the image plane. The problem provides the projection relationship for the radial image coordinate $r$ as $r = f R/Z$, where $Z$ is the distance from the camera center to the object point along the optical axis. This relationship is a direct consequence of the similarity of triangles formed by the object point, the camera center, and their projections on the optical and image planes.\n\nFor a vertical photograph, the camera's optical axis is perpendicular to the reference plane. The satellite is at a height $H$ above this plane. We can set up a coordinate system where the camera center is at an altitude $H$ and the reference plane is at altitude $0$.\n\nLet's locate the base and the top of the feature in this system.\n\n1.  **The Base of the Feature:**\n    The base of the feature rests on the reference plane. Its height above this plane is $0$. The distance from the camera center to the plane containing the feature's base is therefore $Z_{\\text{base}} = H$.\n    Let the lateral ground offset of the feature's base from the axis of nadir be $R$. The problem states that the base projects to a radial distance $r$ in the image. Using the given projection formula:\n    $$r = \\frac{f R}{Z_{\\text{base}}} = \\frac{f R}{H}$$\n    This equation relates the known image coordinate $r$ to the unknown ground offset $R$. We can rearrange it to express $R$ in terms of the given quantities:\n    $$R = \\frac{rH}{f}$$\n\n2.  **The Top of the Feature:**\n    The feature has a height $h$. Assuming it is a vertical feature, its top is directly above its base, so it has the same lateral ground offset $R$. The height of the top above the reference plane is $h$.\n    The distance from the camera center to the plane containing the feature's top is $Z_{\\text{top}} = H - h$.\n    Let the radial distance of the projected top in the image be $r_{\\text{top}}$. Applying the projection formula again:\n    $$r_{\\text{top}} = \\frac{f R}{Z_{\\text{top}}} = \\frac{f R}{H - h}$$\n\n3.  **Derivation of Relief Displacement:**\n    Relief displacement, $\\Delta r$, is defined as the outward radial shift from the projected base to the projected top.\n    $$\\Delta r = r_{\\text{top}} - r$$\n    To express this in terms of the specified variables ($r$, $f$, $H$, and $h$), we substitute the expressions for $r_{\\text{top}}$ and $R$.\n    First, substitute the expression for $R$ from step 1 into the equation for $r_{\\text{top}}$:\n    $$r_{\\text{top}} = \\frac{f}{H - h} \\left( \\frac{rH}{f} \\right) = \\frac{rH}{H - h}$$\n    Note that the focal length $f$ has cancelled out. Now, we can calculate the displacement:\n    $$\\Delta r = \\frac{rH}{H - h} - r$$\n    Factoring out $r$:\n    $$\\Delta r = r \\left( \\frac{H}{H - h} - 1 \\right)$$\n    To simplify the expression in the parenthesis, we find a common denominator:\n    $$\\Delta r = r \\left( \\frac{H - (H - h)}{H - h} \\right) = r \\left( \\frac{H - H + h}{H - h} \\right)$$\n    This yields the final exact expression for the radial relief displacement:\n    $$\\Delta r = \\frac{rh}{H - h}$$\n    Although the problem requests the expression in terms of $r$, $f$, $H$, and $h$, the focal length $f$ is algebraically eliminated when relief displacement is expressed as a function of the image coordinate $r$. The focal length is fundamentally necessary to relate the image and object spaces, but it does not appear in this specific final relationship.\n\n4.  **Numerical Evaluation:**\n    We are given the following values:\n    $h = 50\\,\\mathrm{m}$\n    $r = 2000\\,\\mathrm{px}$\n    $H = 600\\,\\mathrm{km}$\n    $f = 1000\\,\\mathrm{px}$ (This value is not needed for the final calculation).\n\n    For the formula to be dimensionally consistent, $h$ and $H$ must be in the same units. We convert $H$ to meters:\n    $$H = 600\\,\\mathrm{km} = 600 \\times 1000\\,\\mathrm{m} = 600000\\,\\mathrm{m}$$\n    Now, substitute the numerical values into the derived expression for $\\Delta r$:\n    $$\\Delta r = \\frac{(2000\\,\\mathrm{px}) \\times (50\\,\\mathrm{m})}{600000\\,\\mathrm{m} - 50\\,\\mathrm{m}}$$\n    $$\\Delta r = \\frac{100000}{599950}\\,\\mathrm{px}$$\n    Performing the division:\n    $$\\Delta r \\approx 0.16667222\\,\\mathrm{px}$$\n    The problem requires the result to be rounded to four significant figures. The first four significant figures are $1$, $6$, $6$, $6$. The following digit is $7$, so we round up the last digit.\n    $$\\Delta r \\approx 0.1667\\,\\mathrm{px}$$",
            "answer": "$$\\boxed{0.1667}$$"
        },
        {
            "introduction": "While modeling specific distortions is insightful, a more common practical approach is to use a general mathematical model to correct an image. The affine transformation is a workhorse model in remote sensing, capable of correcting for combined effects of sensor tilt, scale differences, and rotation by relating image coordinates to map coordinates using a set of Ground Control Points (GCPs). This exercise  guides you through the full process of implementing a least-squares adjustment to find the best-fit affine parameters and statistically evaluating the quality of your correction using the Root Mean Square Error (RMSE).",
            "id": "3815685",
            "problem": "You are given three independent scenarios, each consisting of $12$ Ground Control Points (GCPs) with image coordinates $(x_i,y_i)$ and observed map coordinates $(X_i,Y_i)$. Assume an affine geometric correction model that linearly maps image coordinates to map coordinates. Your task is to:\n\n- Formulate the linear observation model for both axes and derive, from first principles under the classical Gauss–Markov framework, the least squares estimators of the affine parameters for the $X$-axis and $Y$-axis mappings.\n- From the estimator properties, derive the unbiased estimators of the residual variances, and the covariance matrices of the affine parameters for each axis.\n- Using the law of error propagation, predict the Root Mean Square Error (RMSE) of the residuals at the given design points by propagating the parameter covariances to the observation space at the provided $(x_i,y_i)$.\n- Compute the empirical RMSE of the residuals from the fitted model at the GCPs for comparison.\n\nFor each scenario, produce an output list of eight floating-point numbers:\n- The first six values are the affine parameters for the $X$-axis and $Y$-axis mappings, in the order $[a_0,a_1,a_2,b_0,b_1,b_2]$, where the affine models are $X = a_0 + a_1 x + a_2 y$ and $Y = b_0 + b_1 x + b_2 y$.\n- The next two values are the predicted RMSE (from parameter covariance propagation at the design points) and the empirical RMSE (from residuals), in this order.\n\nAll values should be rounded to six decimal places. No physical units are required. Angles do not appear. Percentages do not appear.\n\nFundamental base available for derivation:\n- Linear observation model with independent, homoscedastic errors.\n- Ordinary Least Squares under the Gauss–Markov assumptions.\n- Properties of least squares estimators, including unbiased variance estimation and parameter covariance.\n- Law of error propagation for linear functions.\n\nTest suite data for the three scenarios:\n\n- Scenario $1$ (well-distributed GCPs with small noise):\n    - Image coordinates $(x_i,y_i)$:\n        - $(x,y)$ list: $[(0,0),(100,0),(0,100),(100,100),(50,0),(0,50),(50,50),(150,50),(50,150),(150,150),(200,100),(100,200)]$\n    - Observed map coordinates $(X_i,Y_i)$:\n        - $[(500.8,-301.2),(699.5,-279.1),(551.1,-150.4),(749.3,-129.7),(600.3,-290.6),(524.0,-224.3),(625.2,-215.0),(824.6,-196.5),(675.0,-63.6),(875.6,-45.8),(949.1,-109.5),(800.4,18.9)]$\n\n- Scenario $2$ (near-collinear GCPs; ill-conditioned design):\n    - Image coordinates $(x_i,y_i)$ with $y_i \\approx 0.1 x_i$:\n        - $(x,y)$ list: $[(0,-0.5),(10,1.2),(20,1.9),(30,3.0),(40,4.3),(50,4.6),(60,6.1),(70,6.8),(80,8.5),(90,8.7),(100,10.2),(110,11.0)]$\n    - Observed map coordinates $(X_i,Y_i)$:\n        - $[(1000.35,-501.2),(1009.38,-497.0),(1020.21,-495.5),(1029.5,-492.0),(1039.67,-489.5),(1049.24,-488.1),(1059.39,-485.2),(1069.52,-482.9),(1079.05,-478.7),(1089.63,-478.7),(1098.58,-474.2),(1109.2,-472.3)]$\n\n- Scenario $3$ (well-distributed GCPs with larger noise):\n    - Image coordinates $(x_i,y_i)$:\n        - $(x,y)$ list: $[(0,0),(300,0),(0,300),(300,300),(150,0),(0,150),(150,150),(450,150),(150,450),(450,450),(600,300),(300,600)]$\n    - Observed map coordinates $(X_i,Y_i)$:\n        - $[(-193.5,792.0),(245.8,1015.7),(-286.7,1063.9),(153.0,1289.4),(33.2,900.4),(-250.1,942.3),(-15.3,1036.7),(426.4,1256.6),(-102.9,1304.8),(337.6,1528.8),(615.5,1482.3),(63.2,1560.5)]$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each scenario’s result is itself a comma-separated sublist enclosed in square brackets, and no spaces appear. For example, the structure must be:\n- $[[a_0,a_1,a_2,b_0,b_1,b_2,\\text{predRMSE},\\text{empRMSE}],[\\dots],[\\dots]]$",
            "solution": "The user-provided problem is valid. It is a well-posed, scientifically grounded problem in the field of geomatics and remote sensing, based on the fundamental principles of linear algebra and statistical estimation theory (Ordinary Least Squares). The data provided for all three scenarios are complete and sufficient for a unique solution, and the required derivations and computations are standard procedures in this discipline. The second scenario, while ill-conditioned, is a valid case for demonstrating the effects of poor geometric distribution of control points, a key concept in this domain.\n\n### 1. Theoretical Derivation\n\nThe problem requires performing an affine transformation to map image coordinates $(x,y)$ to map coordinates $(X,Y)$. The transformation is defined by two independent linear equations:\n$$\n\\begin{cases}\nX = a_0 + a_1 x + a_2 y \\\\\nY = b_0 + b_1 x + b_2 y\n\\end{cases}\n$$\nThese two equations can be solved independently using the method of Ordinary Least Squares (OLS). The derivation will be shown for the $X$-coordinate model, and the same logic applies to the $Y$-coordinate model.\n\n**Linear Observation Model for the X-coordinate**\n\nFor $N$ Ground Control Points (GCPs), we have a system of $N$ linear observation equations. For each point $i=1, \\dots, N$:\n$$\nX_i + v_{X,i} = a_0 + a_1 x_i + a_2 y_i\n$$\nwhere $X_i$ is the observed map coordinate and $v_{X,i}$ is the unknown random error or residual. The Gauss-Markov model assumes these errors are uncorrelated and have a constant variance, i.e., $E[v_{X,i}] = 0$ and $E[v_{X,i}^2] = \\sigma_X^2$, and $E[v_{X,i}v_{X,j}] = 0$ for $i \\neq j$.\n\nThis system can be written in matrix form as:\n$$\n\\mathbf{L}_X + \\mathbf{v}_X = \\mathbf{A} \\mathbf{p}_a\n$$\nwhere:\n- $\\mathbf{L}_X = [X_1, X_2, \\dots, X_N]^T$ is the $N \\times 1$ vector of observations.\n- $\\mathbf{v}_X = [v_{X,1}, v_{X,2}, \\dots, v_{X,N}]^T$ is the $N \\times 1$ vector of residuals.\n- $\\mathbf{p}_a = [a_0, a_1, a_2]^T$ is the $3 \\times 1$ vector of unknown affine parameters.\n- $\\mathbf{A}$ is the $N \\times 3$ design matrix, whose elements are the known image coordinates:\n$$\n\\mathbf{A} = \\begin{pmatrix}\n1 & x_1 & y_1 \\\\\n1 & x_2 & y_2 \\\\\n\\vdots & \\vdots & \\vdots \\\\\n1 & x_N & y_N\n\\end{pmatrix}\n$$\nThe covariance matrix of the observations is $\\boldsymbol{\\Sigma}_{\\mathbf{L}_X \\mathbf{L}_X} = E[\\mathbf{v}_X \\mathbf{v}_X^T] = \\sigma_X^2 \\mathbf{I}$, where $\\mathbf{I}$ is the $N \\times N$ identity matrix.\n\n**Least Squares Estimation of Parameters**\n\nThe OLS principle seeks to find the estimate $\\hat{\\mathbf{p}}_a$ that minimizes the sum of squared residuals, $S = \\mathbf{v}_X^T \\mathbf{v}_X$.\n$$\nS = (\\mathbf{A} \\mathbf{p}_a - \\mathbf{L}_X)^T (\\mathbf{A} \\mathbf{p}_a - \\mathbf{L}_X)\n$$\nTo find the minimum, we differentiate $S$ with respect to $\\mathbf{p}_a$ and set the derivative to zero:\n$$\n\\frac{\\partial S}{\\partial \\mathbf{p}_a} = 2 \\mathbf{A}^T (\\mathbf{A} \\hat{\\mathbf{p}}_a - \\mathbf{L}_X) = \\mathbf{0}\n$$\nThis leads to the **normal equations**:\n$$\n(\\mathbf{A}^T \\mathbf{A}) \\hat{\\mathbf{p}}_a = \\mathbf{A}^T \\mathbf{L}_X\n$$\nAssuming the matrix $\\mathbf{A}^T \\mathbf{A}$ is invertible (which requires the GCPs to not be collinear, i.e., $\\mathbf{A}$ has full column rank $p=3$), the least squares estimator for the parameters is:\n$$\n\\hat{\\mathbf{p}}_a = (\\mathbf{A}^T \\mathbf{A})^{-1} \\mathbf{A}^T \\mathbf{L}_X\n$$\nSimilarly, for the $Y$-coordinate parameters $\\mathbf{p}_b = [b_0, b_1, b_2]^T$, the estimator is:\n$$\n\\hat{\\mathbf{p}}_b = (\\mathbf{A}^T \\mathbf{A})^{-1} \\mathbf{A}^T \\mathbf{L}_Y\n$$\nwhere $\\mathbf{L}_Y = [Y_1, Y_2, \\dots, Y_N]^T$.\n\n**Residual Variance and Parameter Covariance**\n\nThe vector of estimated residuals is $\\hat{\\mathbf{v}}_X = \\mathbf{A} \\hat{\\mathbf{p}}_a - \\mathbf{L}_X$. The sum of squared residuals (SSR) is $\\text{SSR}_X = \\hat{\\mathbf{v}}_X^T \\hat{\\mathbf{v}}_X$.\nThe expected value of the SSR is $E[\\text{SSR}_X] = (N-p)\\sigma_X^2$, where $p=3$ is the number of parameters. The quantity $r = N-p$ is the degrees of freedom of the adjustment.\n\nThe **unbiased estimator of the residual variance** $\\sigma_X^2$ is:\n$$\n\\hat{\\sigma}_X^2 = \\frac{\\text{SSR}_X}{N-p}\n$$\nThe **covariance matrix of the estimated parameters** $\\hat{\\mathbf{p}}_a$ is derived using the law of propagation of variances on the estimator equation $\\hat{\\mathbf{p}}_a = (\\mathbf{A}^T \\mathbf{A})^{-1} \\mathbf{A}^T \\mathbf{L}_X$:\n$$\n\\boldsymbol{\\Sigma}_{\\hat{\\mathbf{p}}_a \\hat{\\mathbf{p}}_a} = \\left[(\\mathbf{A}^T \\mathbf{A})^{-1} \\mathbf{A}^T\\right] \\boldsymbol{\\Sigma}_{\\mathbf{L}_X \\mathbf{L}_X} \\left[(\\mathbf{A}^T \\mathbf{A})^{-1} \\mathbf{A}^T\\right]^T = \\sigma_X^2 (\\mathbf{A}^T \\mathbf{A})^{-1}\n$$\nSince $\\sigma_X^2$ is unknown, we use its estimate $\\hat{\\sigma}_X^2$ to obtain the estimated covariance matrix:\n$$\n\\hat{\\boldsymbol{\\Sigma}}_{\\hat{\\mathbf{p}}_a \\hat{\\mathbf{p}}_a} = \\hat{\\sigma}_X^2 (\\mathbf{A}^T \\mathbf{A})^{-1}\n$$\n\n### 2. RMSE Calculation\n\n**Empirical RMSE**\n\nThe empirical Root Mean Square Error measures the actual misfit of the model to the data points. For each point $i$, the squared residual error is $e_i^2 = \\hat{v}_{X,i}^2 + \\hat{v}_{Y,i}^2$. The empirical RMSE is the square root of the mean of these squared errors over all $N$ points:\n$$\n\\text{RMSE}_{\\text{emp}} = \\sqrt{\\frac{1}{N} \\sum_{i=1}^N (\\hat{v}_{X,i}^2 + \\hat{v}_{Y,i}^2)} = \\sqrt{\\frac{\\text{SSR}_X + \\text{SSR}_Y}{N}}\n$$\n\n**Predicted RMSE**\n\nThe predicted RMSE is derived by propagating the parameter uncertainties to the observation space. The variance of a single predicted coordinate, e.g., $\\hat{X}_i = \\mathbf{a}_i^T \\hat{\\mathbf{p}}_a$ (where $\\mathbf{a}_i^T = [1, x_i, y_i]$ is the $i$-th row of $\\mathbf{A}$), is given by:\n$$\n\\text{Var}(\\hat{X}_i) = \\mathbf{a}_i^T \\boldsymbol{\\Sigma}_{\\hat{\\mathbf{p}}_a \\hat{\\mathbf{p}}_a} \\mathbf{a}_i = \\sigma_X^2 \\, \\mathbf{a}_i^T (\\mathbf{A}^T \\mathbf{A})^{-1} \\mathbf{a}_i = \\sigma_X^2 h_{ii}\n$$\nwhere $h_{ii}$ is the $i$-th diagonal element of the \"hat\" matrix $\\mathbf{H} = \\mathbf{A}(\\mathbf{A}^T \\mathbf{A})^{-1}\\mathbf{A}^T$. The value $h_{ii}$ is the leverage of the $i$-th observation.\n\nThe predicted RMSE is the root mean square of the standard errors of the fitted values over the design points. The mean predicted variance for a point (summing X and Y components) is:\n$$\n\\frac{1}{N} \\sum_{i=1}^N \\left[ \\text{Var}(\\hat{X}_i) + \\text{Var}(\\hat{Y}_i) \\right] = \\frac{1}{N} \\sum_{i=1}^N \\left[ \\sigma_X^2 h_{ii} + \\sigma_Y^2 h_{ii} \\right] = \\frac{\\sigma_X^2 + \\sigma_Y^2}{N} \\sum_{i=1}^N h_{ii}\n$$\nThe sum of the leverages is the trace of the hat matrix, $\\text{tr}(\\mathbf{H}) = p = 3$. Therefore, the average predicted variance is $\\frac{p}{N}(\\sigma_X^2 + \\sigma_Y^2)$.\nThe predicted RMSE is the square root of this quantity, using the estimates $\\hat{\\sigma}_X^2$ and $\\hat{\\sigma}_Y^2$:\n$$\n\\text{RMSE}_{\\text{pred}} = \\sqrt{\\frac{p}{N}(\\hat{\\sigma}_X^2 + \\hat{\\sigma}_Y^2)} = \\sqrt{\\frac{p}{N} \\left( \\frac{\\text{SSR}_X}{N-p} + \\frac{\\text{SSR}_Y}{N-p} \\right)} = \\sqrt{\\frac{p (\\text{SSR}_X + \\text{SSR}_Y)}{N(N-p)}}\n$$\n\n### 3. Computational Procedure\nFor each scenario:\n1. Set $N=12$ and $p=3$. The degrees of freedom is $df = N-p=9$.\n2. Form the $12 \\times 3$ design matrix $\\mathbf{A}$ from the image coordinates $(x_i, y_i)$.\n3. Form the $12 \\times 1$ observation vectors $\\mathbf{L}_X$ and $\\mathbf{L}_Y$ from the map coordinates $(X_i, Y_i)$.\n4. Compute the normal matrix $\\mathbf{N} = \\mathbf{A}^T \\mathbf{A}$ and the vectors $\\mathbf{c}_X = \\mathbf{A}^T \\mathbf{L}_X$ and $\\mathbf{c}_Y = \\mathbf{A}^T \\mathbf{L}_Y$.\n5. Solve the linear systems $\\mathbf{N} \\hat{\\mathbf{p}}_a = \\mathbf{c}_X$ and $\\mathbf{N} \\hat{\\mathbf{p}}_b = \\mathbf{c}_Y$ to get the parameter vectors $\\hat{\\mathbf{p}}_a = [a_0, a_1, a_2]^T$ and $\\hat{\\mathbf{p}}_b = [b_0, b_1, b_2]^T$.\n6. Compute the residual vectors $\\hat{\\mathbf{v}}_X = \\mathbf{A} \\hat{\\mathbf{p}}_a - \\mathbf{L}_X$ and $\\hat{\\mathbf{v}}_Y = \\mathbf{A} \\hat{\\mathbf{p}}_b - \\mathbf{L}_Y$.\n7. Compute the sums of squared residuals $\\text{SSR}_X = \\hat{\\mathbf{v}}_X^T \\hat{\\mathbf{v}}_X$ and $\\text{SSR}_Y = \\hat{\\mathbf{v}}_Y^T \\hat{\\mathbf{v}}_Y$. Let $\\text{SSR}_{\\text{total}} = \\text{SSR}_X + \\text{SSR}_Y$.\n8. Compute $\\text{RMSE}_{\\text{pred}} = \\sqrt{\\frac{p \\cdot \\text{SSR}_{\\text{total}}}{N \\cdot df}} = \\sqrt{\\frac{3 \\cdot \\text{SSR}_{\\text{total}}}{12 \\cdot 9}} = \\sqrt{\\frac{\\text{SSR}_{\\text{total}}}{36}}$.\n9. Compute $\\text{RMSE}_{\\text{emp}} = \\sqrt{\\frac{\\text{SSR}_{\\text{total}}}{N}} = \\sqrt{\\frac{\\text{SSR}_{\\text{total}}}{12}}$.\n10. Assemble the final list of 8 floating point numbers, rounded to six decimal places.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# No other libraries are permitted, as scipy is not needed for this solution.\n\ndef solve():\n    \"\"\"\n    Solves the affine transformation problem for three scenarios using Ordinary Least Squares.\n    \"\"\"\n\n    test_cases = [\n        # Scenario 1 (well-distributed GCPs with small noise)\n        {\n            \"img_coords\": np.array([\n                [0, 0], [100, 0], [0, 100], [100, 100], [50, 0], [0, 50],\n                [50, 50], [150, 50], [50, 150], [150, 150], [200, 100], [100, 200]\n            ]),\n            \"map_coords\": np.array([\n                [500.8, -301.2], [699.5, -279.1], [551.1, -150.4], [749.3, -129.7],\n                [600.3, -290.6], [524.0, -224.3], [625.2, -215.0], [824.6, -196.5],\n                [675.0, -63.6], [875.6, -45.8], [949.1, -109.5], [800.4, 18.9]\n            ])\n        },\n        # Scenario 2 (near-collinear GCPs; ill-conditioned design)\n        {\n            \"img_coords\": np.array([\n                [0, -0.5], [10, 1.2], [20, 1.9], [30, 3.0], [40, 4.3], [50, 4.6],\n                [60, 6.1], [70, 6.8], [80, 8.5], [90, 8.7], [100, 10.2], [110, 11.0]\n            ]),\n            \"map_coords\": np.array([\n                [1000.35, -501.2], [1009.38, -497.0], [1020.21, -495.5], [1029.5, -492.0],\n                [1039.67, -489.5], [1049.24, -488.1], [1059.39, -485.2], [1069.52, -482.9],\n                [1079.05, -478.7], [1089.63, -478.7], [1098.58, -474.2], [1109.2, -472.3]\n            ])\n        },\n        # Scenario 3 (well-distributed GCPs with larger noise)\n        {\n            \"img_coords\": np.array([\n                [0, 0], [300, 0], [0, 300], [300, 300], [150, 0], [0, 150],\n                [150, 150], [450, 150], [150, 450], [450, 450], [600, 300], [300, 600]\n            ]),\n            \"map_coords\": np.array([\n                [-193.5, 792.0], [245.8, 1015.7], [-286.7, 1063.9], [153.0, 1289.4],\n                [33.2, 900.4], [-250.1, 942.3], [-15.3, 1036.7], [426.4, 1256.6],\n                [-102.9, 1304.8], [337.6, 1528.8], [615.5, 1482.3], [63.2, 1560.5]\n            ])\n        }\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        img_coords = case[\"img_coords\"]\n        map_coords = case[\"map_coords\"]\n        \n        N = img_coords.shape[0]  # Number of observations (GCPs)\n        p = 3  # Number of parameters (a0, a1, a2)\n        df = N - p # Degrees of freedom\n\n        # Construct the design matrix A\n        A = np.ones((N, p))\n        A[:, 1] = img_coords[:, 0]  # x coordinates\n        A[:, 2] = img_coords[:, 1]  # y coordinates\n\n        # Construct the observation vectors L_X and L_Y\n        L_X = map_coords[:, 0]\n        L_Y = map_coords[:, 1]\n        \n        # Form the normal equations: (A^T A) p = A^T L\n        N_matrix = A.T @ A\n        c_X = A.T @ L_X\n        c_Y = A.T @ L_Y\n        \n        # Solve for the parameter vectors p_a and p_b\n        try:\n            # np.linalg.solve is numerically more stable than using np.linalg.inv\n            p_a = np.linalg.solve(N_matrix, c_X)\n            p_b = np.linalg.solve(N_matrix, c_Y)\n        except np.linalg.LinAlgError:\n            # Fallback for singular matrix, although not expected with test data\n            inv_N = np.linalg.pinv(N_matrix)\n            p_a = inv_N @ c_X\n            p_b = inv_N @ c_Y\n            \n        a0, a1, a2 = p_a\n        b0, b1, b2 = p_b\n        \n        # Calculate residuals\n        v_X = A @ p_a - L_X\n        v_Y = A @ p_b - L_Y\n        \n        # Calculate Sum of Squared Residuals (SSR)\n        ssr_X = v_X.T @ v_X\n        ssr_Y = v_Y.T @ v_Y\n        ssr_total = ssr_X + ssr_Y\n        \n        # Calculate predicted RMSE from parameter covariance propagation\n        # RMSE_pred = sqrt( (p/N) * (sigma_hat_X^2 + sigma_hat_Y^2) )\n        #           = sqrt( (p/N) * (SSR_X/df + SSR_Y/df) )\n        #           = sqrt( p * (SSR_X + SSR_Y) / (N * df) )\n        if df > 0:\n            rmse_pred = np.sqrt(p * ssr_total / (N * df))\n        else:\n            rmse_pred = 0.0\n\n        # Calculate empirical RMSE from residuals\n        # RMSE_emp = sqrt( (1/N) * sum(v_Xi^2 + v_Yi^2) )\n        #          = sqrt( (SSR_X + SSR_Y) / N )\n        rmse_emp = np.sqrt(ssr_total / N)\n        \n        # Format results\n        scenario_result = [a0, a1, a2, b0, b1, b2, rmse_pred, rmse_emp]\n        rounded_result = [round(val, 6) for val in scenario_result]\n        all_results.append(rounded_result)\n\n    # Format the final output string exactly as required\n    output_str = \"[\"\n    for i, res in enumerate(all_results):\n        output_str += f\"[{','.join(map(str, res))}]\"\n        if i < len(all_results) - 1:\n            output_str += \",\"\n    output_str += \"]\"\n            \n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "Moving beyond empirical models, we now tackle the rigorous physical modeling required for modern active sensors like LiDAR, a process known as direct georeferencing. Here, ground coordinates are computed directly from high-frequency measurements of the platform's position and orientation, combined with the sensor's own measurements of range and scan angle. This advanced practice  immerses you in the complete LiDAR geopositioning equation, from handling 3D rotations between multiple coordinate frames to propagating input uncertainties to derive a final positional accuracy for your data points.",
            "id": "3815640",
            "problem": "You are to implement a program that computes geometrically corrected ground point coordinates from airborne Light Detection and Ranging (LiDAR) measurements and estimates their positional uncertainty by first-order error propagation. The computation must be derived from foundational principles: rigid-body rotations, coordinate frame transformations, and first-order linearization for uncertainty propagation.\n\nAssume the following coordinate frames and conventions, which you must use exactly as stated for all computations.\n\n1. Navigation frame (denoted $N$): a local East-North-Up frame, with axes $x_{N}$ pointing East, $y_{N}$ pointing North, and $z_{N}$ pointing Up. Platform positions are given in this frame.\n2. Body frame (denoted $B$): attached to the platform, with axes $x_{B}$ pointing to the right wing, $y_{B}$ pointing forward, and $z_{B}$ pointing up. The platform orientation is expressed as roll $\\phi$, pitch $\\theta$, yaw $\\psi$, using the right-handed rotations about $x$, $y$, $z$ respectively. The rotation from body to navigation is given by the composition that first rolls, then pitches, then yaws as right-handed rotations, yielding $R_{N B} = R_{z}(\\psi)\\,R_{y}(\\theta)\\,R_{x}(\\phi)$, where $R_{x}$, $R_{y}$, $R_{z}$ denote standard right-handed rotation matrices about the corresponding axes.\n3. LiDAR frame (denoted $L$): attached to the LiDAR unit. The LiDAR boresight misalignment is represented by small rotations $\\alpha$ (about $x$), $\\beta$ (about $y$), and $\\gamma$ (about $z$), again using the right-handed convention. The rotation from LiDAR to body is $R_{B L} = R_{z}(\\gamma)\\,R_{y}(\\beta)\\,R_{x}(\\alpha)$.\n\nAssume the LiDAR emits a beam that, at zero scan angle, points exactly downward in the LiDAR frame (toward $-z_{L}$). The scan angle $s$ is defined as a rotation of the beam about the LiDAR $y$-axis, with positive $s$ rotating the beam toward $+x_{L}$. Therefore, the unit beam direction in the LiDAR frame is\n$d_{L}(s) = \\begin{bmatrix} \\sin s \\\\ 0 \\\\ -\\cos s \\end{bmatrix}$.\nGiven a platform position $p_{N} \\in \\mathbb{R}^{3}$ in the navigation frame, a measured slant range $r$ in meters along the beam, a platform orientation $(\\phi,\\theta,\\psi)$, a boresight $(\\alpha,\\beta,\\gamma)$, and a scan angle $s$, the corrected ground point in the navigation frame is obtained by applying the appropriate rotations to the beam direction and translating by the platform position. Your program must compute this corrected point.\n\nFor uncertainty, assume independent, zero-mean Gaussian uncertainties in the eight scalar inputs $q = [r, s, \\phi, \\theta, \\psi, \\alpha, \\beta, \\gamma]^{\\top}$. Let $C_{q}$ be the diagonal covariance matrix whose diagonal entries are the variances of these inputs. Using a first-order linearization around the given nominal inputs, propagate the uncertainty to the position $p \\in \\mathbb{R}^{3}$ by computing the Jacobian $J = \\frac{\\partial p}{\\partial q}$ and setting $C_{p} = J\\,C_{q}\\,J^{\\top}$. The one-sigma positional uncertainties along the navigation axes are given by the square roots of the diagonal entries of $C_{p}$. You must compute $J$ numerically using central finite differences with suitably small steps to capture sensitivities while avoiding numerical cancellation.\n\nAngle units and physical units:\n- All angles provided in the inputs are in degrees. Internally, you must perform all trigonometric computations in radians. State clearly any conversions you perform.\n- All lengths are in meters. You must express the final coordinates and uncertainties in meters.\n\nYour program must, for each test case, output a list of six floats in the order $[x_{N}, y_{N}, z_{N}, \\sigma_{x}, \\sigma_{y}, \\sigma_{z}]$, where $(x_{N},y_{N},z_{N})$ are the corrected coordinates in meters and $(\\sigma_{x},\\sigma_{y},\\sigma_{z})$ are the one-sigma uncertainties in meters computed from $C_{p}$. Round each value to four decimal places.\n\nTest suite:\nImplement your program to process exactly the following three test cases. Each test case provides the platform position $p_{N}$ (meters), platform orientation $(\\phi,\\theta,\\psi)$ (degrees), boresight $(\\alpha,\\beta,\\gamma)$ (degrees), range $r$ (meters), scan angle $s$ (degrees), and the standard deviations for the uncertainties $\\sigma_{r}$ (meters), $\\sigma_{s}$ (degrees), $\\sigma_{\\phi}$ (degrees), $\\sigma_{\\theta}$ (degrees), $\\sigma_{\\psi}$ (degrees), $\\sigma_{\\alpha}$ (degrees), $\\sigma_{\\beta}$ (degrees), $\\sigma_{\\gamma}$ (degrees).\n\n- Test case $1$ (general case):\n  - $p_{N} = (\\,100.0,\\,200.0,\\,1500.0\\,)$\n  - $(\\phi,\\theta,\\psi) = (\\,1.0,\\,-2.0,\\,45.0\\,)$\n  - $(\\alpha,\\beta,\\gamma) = (\\,0.2,\\,-0.1,\\,0.3\\,)$\n  - $r = 1200.0$\n  - $s = 15.0$\n  - $(\\sigma_{r}, \\sigma_{s}, \\sigma_{\\phi}, \\sigma_{\\theta}, \\sigma_{\\psi}, \\sigma_{\\alpha}, \\sigma_{\\beta}, \\sigma_{\\gamma}) = (\\,0.05,\\,0.005,\\,0.01,\\,0.01,\\,0.01,\\,0.02,\\,0.02,\\,0.02\\,)$\n- Test case $2$ (near-nadir, identity attitude and boresight, boundary check):\n  - $p_{N} = (\\,0.0,\\,0.0,\\,1000.0\\,)$\n  - $(\\phi,\\theta,\\psi) = (\\,0.0,\\,0.0,\\,0.0\\,)$\n  - $(\\alpha,\\beta,\\gamma) = (\\,0.0,\\,0.0,\\,0.0\\,)$\n  - $r = 1000.0$\n  - $s = 0.0$\n  - $(\\sigma_{r}, \\sigma_{s}, \\sigma_{\\phi}, \\sigma_{\\theta}, \\sigma_{\\psi}, \\sigma_{\\alpha}, \\sigma_{\\beta}, \\sigma_{\\gamma}) = (\\,0.01,\\,0.001,\\,0.001,\\,0.001,\\,0.001,\\,0.001,\\,0.001,\\,0.001\\,)$\n- Test case $3$ (highly oblique scan and large yaw):\n  - $p_{N} = (\\, -500.0,\\,300.0,\\,2000.0\\,)$\n  - $(\\phi,\\theta,\\psi) = (\\, -5.0,\\,10.0,\\,170.0\\,)$\n  - $(\\alpha,\\beta,\\gamma) = (\\,0.5,\\,0.2,\\,-0.4\\,)$\n  - $r = 1500.0$\n  - $s = 80.0$\n  - $(\\sigma_{r}, \\sigma_{s}, \\sigma_{\\phi}, \\sigma_{\\theta}, \\sigma_{\\psi}, \\sigma_{\\alpha}, \\sigma_{\\beta}, \\sigma_{\\gamma}) = (\\,0.10,\\,0.02,\\,0.03,\\,0.03,\\,0.03,\\,0.05,\\,0.05,\\,0.05\\,)$\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is itself a list corresponding to a test case. Each inner list must contain the six rounded floats $[x_{N}, y_{N}, z_{N}, \\sigma_{x}, \\sigma_{y}, \\sigma_{z}]$ in meters. For example, an output with three test cases should look like\n$[[x_{1},y_{1},z_{1},\\sigma_{x1},\\sigma_{y1},\\sigma_{z1}],[x_{2},y_{2},z_{2},\\sigma_{x2},\\sigma_{y2},\\sigma_{z2}],[x_{3},y_{3},z_{3},\\sigma_{x3},\\sigma_{y3},\\sigma_{z3}]]$,\nwith every numerical entry rounded to four decimal places. No other characters or lines should be printed.",
            "solution": "The problem statement is assessed to be valid. It is scientifically grounded in the principles of rigid-body kinematics and first-order error propagation, which are standard in geodesy and remote sensing. The problem is well-posed, providing all necessary data, explicit mathematical definitions, and a clear objective. The language is unambiguous, and the required computations are feasible and lead to a unique, verifiable solution.\n\nThe core task is to compute the ground coordinates of a LiDAR point and its positional uncertainty. This is addressed in two main parts: the forward model for calculating the position, and the uncertainty propagation model.\n\n**1. Forward Model: LiDAR Geopositioning Equation**\n\nThe position of the LiDAR-illuminated point on the ground, $p$, in the navigation frame ($N$) is determined by the vector sum of the platform's position, $p_{N}$, and the vector from the platform to the ground point. This latter vector is the product of the measured slant range, $r$, and a unit vector, $d_{N}$, that describes the beam's direction in the navigation frame.\n\nThe overall equation is:\n$$ p = p_{N} + r \\cdot d_{N} $$\n\nThe directional unit vector $d_{N}$ is obtained by rotating the beam's direction from the LiDAR frame ($L$) to the body frame ($B$), and subsequently from the body frame to the navigation frame ($N$). This is achieved through a sequence of matrix multiplications:\n\n$$ d_{N} = R_{NB} \\cdot R_{BL} \\cdot d_{L} $$\n\nwhere:\n- $d_{L}$ is the beam direction in the LiDAR frame.\n- $R_{BL}$ is the rotation matrix from the LiDAR frame to the body frame.\n- $R_{NB}$ is the rotation matrix from the body frame to the navigation frame.\n\nCombining these, the complete LiDAR geopositioning equation is:\n$$ p(q) = p_{N} + r \\cdot R_{NB}(\\phi, \\theta, \\psi) \\cdot R_{BL}(\\alpha, \\beta, \\gamma) \\cdot d_{L}(s) $$\nThe vector $q = [r, s, \\phi, \\theta, \\psi, \\alpha, \\beta, \\gamma]^{\\top}$ represents the set of eight variable input parameters.\n\nThe rotation matrices are defined as compositions of standard right-handed rotations about the principal axes. Let $c(\\cdot) = \\cos(\\cdot)$ and $s(\\cdot) = \\sin(\\cdot)$.\n$R_{x}(\\omega) = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & c(\\omega) & -s(\\omega) \\\\ 0 & s(\\omega) & c(\\omega) \\end{bmatrix}$, $R_{y}(\\omega) = \\begin{bmatrix} c(\\omega) & 0 & s(\\omega) \\\\ 0 & 1 & 0 \\\\ -s(\\omega) & 0 & c(\\omega) \\end{bmatrix}$, $R_{z}(\\omega) = \\begin{bmatrix} c(\\omega) & -s(\\omega) & 0 \\\\ s(\\omega) & c(\\omega) & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}$\n\nAs specified, the rotation from body to navigation frame is a sequence of roll, then pitch, then yaw:\n$$ R_{NB}(\\phi, \\theta, \\psi) = R_{z}(\\psi) R_{y}(\\theta) R_{x}(\\phi) $$\nAnd the rotation from LiDAR to body frame, representing the boresight misalignment, is:\n$$ R_{BL}(\\alpha, \\beta, \\gamma) = R_{z}(\\gamma) R_{y}(\\beta) R_{x}(\\alpha) $$\nThe unit beam direction vector in the LiDAR frame is given by:\n$$ d_{L}(s) = \\begin{bmatrix} \\sin(s) \\\\ 0 \\\\ -\\cos(s) \\end{bmatrix} $$\nAll input angles are provided in degrees and must be converted to radians for use in trigonometric functions via the conversion: $\\text{radians} = \\text{degrees} \\cdot \\frac{\\pi}{180}$.\n\n**2. Uncertainty Propagation Model**\n\nThe uncertainty in the computed ground point position $p$ is estimated using first-order Taylor series expansion, also known as the law of propagation of uncertainty. Given a vector of input variables $q$ with a covariance matrix $C_{q}$, the covariance matrix $C_{p}$ of the output vector $p = f(q)$ is approximated by:\n\n$$ C_{p} \\approx J C_{q} J^{\\top} $$\n\nHere:\n- $J$ is the Jacobian matrix of the function $f$ with respect to the inputs $q$, evaluated at their nominal values: $J_{ij} = \\frac{\\partial p_i}{\\partial q_j}$.\n- $C_{q}$ is the covariance matrix of the input variables. As the inputs are assumed to be independent, $C_{q}$ is a diagonal matrix where the diagonal elements are the variances ($\\sigma^2$) of the respective inputs.\n$$ C_{q} = \\text{diag}(\\sigma_r^2, \\sigma_s^2, \\sigma_{\\phi}^2, \\sigma_{\\theta}^2, \\sigma_{\\psi}^2, \\sigma_{\\alpha}^2, \\sigma_{\\beta}^2, \\sigma_{\\gamma}^2) $$\nThe standard deviations of the angular inputs ($\\sigma_s, \\sigma_{\\phi}$, etc.), given in degrees, must be converted to radians before being squared to compute the variances. This ensures consistency of units (meters and radians) throughout the calculation.\n\nThe Jacobian $J$, a $3 \\times 8$ matrix, is computed numerically using the central finite difference method. For each input variable $q_j$, the corresponding column of the Jacobian is:\n$$ \\frac{\\partial p}{\\partial q_j} \\approx \\frac{p(q_0 + h \\cdot e_j) - p(q_0 - h \\cdot e_j)}{2h} $$\nwhere $q_0$ is the vector of nominal input values, $e_j$ is the $j$-th standard basis vector, and $h$ is a small perturbation step, chosen to be sufficiently small (e.g., $10^{-8}$) to ensure accuracy while avoiding numerical cancellation errors.\n\nOnce the $3 \\times 3$ output covariance matrix $C_p$ is computed, the one-sigma positional uncertainties along the navigation axes $(\\sigma_x, \\sigma_y, \\sigma_z)$ are the square roots of its diagonal elements:\n$$ \\sigma_x = \\sqrt{C_{p,11}}, \\quad \\sigma_y = \\sqrt{C_{p,22}}, \\quad \\sigma_z = \\sqrt{C_{p,33}} $$\n\n**3. Algorithm Implementation**\n\nThe solution is implemented by following these steps for each test case:\n1.  Define the platform position $p_N$ and the nominal values for the eight input parameters $q_{in} = [r, s_{deg}, \\phi_{deg}, \\theta_{deg}, \\psi_{deg}, \\alpha_{deg}, \\beta_{deg}, \\gamma_{deg}]^{\\top}$.\n2.  Define the standard deviations for the eight inputs, $\\sigma_{q,in}$, with angles in degrees.\n3.  Convert all angular components in $q_{in}$ and $\\sigma_{q,in}$ from degrees to radians to create the working vectors $q_0$ and $\\sigma_q$.\n4.  Compute the nominal ground point $p_0 = p(q_0)$ using the LiDAR geopositioning equation.\n5.  Construct the $8 \\times 8$ diagonal input covariance matrix $C_q$ by squaring the elements of $\\sigma_q$.\n6.  Numerically compute the $3 \\times 8$ Jacobian matrix $J$ using the central finite difference formula, where the function $p(q)$ is evaluated for each perturbed input.\n7.  Calculate the $3 \\times 3$ output covariance matrix $C_p = J C_q J^{\\top}$.\n8.  Extract the one-sigma uncertainties $(\\sigma_x, \\sigma_y, \\sigma_z)$ by taking the square root of the diagonal elements of $C_p$.\n9.  Combine the nominal coordinates and the uncertainties into a single list $[p_{0,x}, p_{0,y}, p_{0,z}, \\sigma_x, \\sigma_y, \\sigma_z]$.\n10. Round each of the six values to four decimal places.\n11. Collect the results from all test cases into a final list of lists for output.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes geometrically corrected ground point coordinates from airborne LiDAR\n    measurements and estimates their positional uncertainty.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"p_N\": np.array([100.0, 200.0, 1500.0]),\n            \"att\": np.array([1.0, -2.0, 45.0]),  # phi, theta, psi\n            \"boresight\": np.array([0.2, -0.1, 0.3]),  # alpha, beta, gamma\n            \"r\": 1200.0,\n            \"s\": 15.0,\n            \"sigmas\": np.array([0.05, 0.005, 0.01, 0.01, 0.01, 0.02, 0.02, 0.02])\n        },\n        {\n            \"p_N\": np.array([0.0, 0.0, 1000.0]),\n            \"att\": np.array([0.0, 0.0, 0.0]),\n            \"boresight\": np.array([0.0, 0.0, 0.0]),\n            \"r\": 1000.0,\n            \"s\": 0.0,\n            \"sigmas\": np.array([0.01, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001])\n        },\n        {\n            \"p_N\": np.array([-500.0, 300.0, 2000.0]),\n            \"att\": np.array([-5.0, 10.0, 170.0]),\n            \"boresight\": np.array([0.5, 0.2, -0.4]),\n            \"r\": 1500.0,\n            \"s\": 80.0,\n            \"sigmas\": np.array([0.10, 0.02, 0.03, 0.03, 0.03, 0.05, 0.05, 0.05])\n        }\n    ]\n\n    def calculate_ground_point(q, p_N):\n        \"\"\"\n        Calculates the ground point coordinates based on the LiDAR equation.\n        \n        Args:\n            q (np.ndarray): 8-element vector [r, s, phi, theta, psi, alpha, beta, gamma].\n                          Angles must be in radians.\n            p_N (np.ndarray): 3-element platform position vector [x, y, z].\n\n        Returns:\n            np.ndarray: 3-element ground point vector [x, y, z].\n        \"\"\"\n        r, s_rad, phi, theta, psi, alpha, beta, gamma = q\n        \n        c, s_ = np.cos, np.sin\n\n        # Rotation matrices for platform attitude\n        Rx_phi = np.array([[1, 0, 0], [0, c(phi), -s_(phi)], [0, s_(phi), c(phi)]])\n        Ry_theta = np.array([[c(theta), 0, s_(theta)], [0, 1, 0], [-s_(theta), 0, c(theta)]])\n        Rz_psi = np.array([[c(psi), -s_(psi), 0], [s_(psi), c(psi), 0], [0, 0, 1]])\n        \n        # Rotation matrices for boresight alignment\n        Rx_alpha = np.array([[1, 0, 0], [0, c(alpha), -s_(alpha)], [0, s_(alpha), c(alpha)]])\n        Ry_beta = np.array([[c(beta), 0, s_(beta)], [0, 1, 0], [-s_(beta), 0, c(beta)]])\n        Rz_gamma = np.array([[c(gamma), -s_(gamma), 0], [s_(gamma), c(gamma), 0], [0, 0, 1]])\n\n        # Composite rotation matrices\n        R_NB = Rz_psi @ Ry_theta @ Rx_phi\n        R_BL = Rz_gamma @ Ry_beta @ Rx_alpha\n        R_NL = R_NB @ R_BL\n\n        # Beam vector in LiDAR frame\n        d_L = np.array([s_(s_rad), 0, -c(s_rad)])\n\n        # LiDAR Geopositioning Equation\n        p_ground = p_N + r * (R_NL @ d_L)\n        \n        return p_ground\n\n    results = []\n    for case in test_cases:\n        # Unpack case data\n        p_N = case[\"p_N\"]\n        r, s_deg = case[\"r\"], case[\"s\"]\n        phi_deg, theta_deg, psi_deg = case[\"att\"]\n        alpha_deg, beta_deg, gamma_deg = case[\"boresight\"]\n        sigmas_mixed_units = case[\"sigmas\"]\n\n        # 1. Prepare nominal input vector q0 with angles in radians\n        q0 = np.array([\n            r,\n            np.deg2rad(s_deg),\n            np.deg2rad(phi_deg),\n            np.deg2rad(theta_deg),\n            np.deg2rad(psi_deg),\n            np.deg2rad(alpha_deg),\n            np.deg2rad(beta_deg),\n            np.deg2rad(gamma_deg)\n        ])\n\n        # 2. Prepare standard deviations vector with angular sigmas in radians\n        sigmas = np.copy(sigmas_mixed_units)\n        sigmas[1:] = np.deg2rad(sigmas_mixed_units[1:])\n        \n        # 3. Calculate nominal ground point\n        p_nominal = calculate_ground_point(q0, p_N)\n        \n        # 4. Numerically compute the Jacobian J using central finite differences\n        J = np.zeros((3, 8))\n        h = 1e-8  # Small step for differentiation\n        for i in range(8):\n            q_plus = np.copy(q0)\n            q_plus[i] += h\n            p_plus = calculate_ground_point(q_plus, p_N)\n\n            q_minus = np.copy(q0)\n            q_minus[i] -= h\n            p_minus = calculate_ground_point(q_minus, p_N)\n\n            J[:, i] = (p_plus - p_minus) / (2 * h)\n\n        # 5. Propagate uncertainty\n        Cq = np.diag(sigmas**2)\n        Cp = J @ Cq @ J.T\n\n        # 6. Extract 1-sigma uncertainties\n        sigma_p = np.sqrt(np.diag(Cp))\n\n        # 7. Combine, round, and store results\n        output_vector = np.concatenate((p_nominal, sigma_p))\n        rounded_results = [round(v, 4) for v in output_vector]\n        results.append(rounded_results)\n\n    # Format the final output string exactly as required\n    print(str(results).replace(\" \", \"\"))\n\nsolve()\n```"
        }
    ]
}