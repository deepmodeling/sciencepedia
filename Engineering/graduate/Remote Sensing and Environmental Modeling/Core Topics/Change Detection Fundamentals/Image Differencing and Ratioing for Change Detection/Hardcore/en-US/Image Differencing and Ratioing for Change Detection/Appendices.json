{
    "hands_on_practices": [
        {
            "introduction": "Before interpreting a change map, we must understand its potential artifacts. This first practice tackles a critical source of error in image differencing: subpixel misregistration. By applying multivariate calculus, you will quantify how slight misalignments between images can generate spurious change signals, especially in heterogeneous landscapes, a crucial skill for ensuring the reliability of any change detection analysis. ",
            "id": "3820730",
            "problem": "A pair of atmospherically corrected surface reflectance images $I_1(\\mathbf{x})$ and $I_2(\\mathbf{x})$ of the same scene are used for change detection by image differencing, where the differenced image is defined by $D(\\mathbf{x}) = I_2(\\mathbf{x}) - I_1(\\mathbf{x})$. Assume there is no true physical change between the acquisition times, but the second image is subject to subpixel misregistration so that samples in $I_2$ are taken at locations $\\mathbf{x} + \\delta \\mathbf{x}$, with $\\delta \\mathbf{x}$ representing the misregistration vector. In a spatially heterogeneous landscape, edges and gradients in $I_1(\\mathbf{x})$ are nonzero.\n\nStarting from the differentiability of $I_1(\\mathbf{x})$ and the physical interpretation that translational misregistration shifts the sampling location, do the following at a fixed pixel $\\mathbf{x}_0$:\n\n- Using a first-order expansion grounded in standard multivariate calculus, express the leading-order contribution to $D(\\mathbf{x}_0)$ due solely to $\\delta \\mathbf{x}$ when there is no true change.\n\n- Let $\\delta \\mathbf{x}$ be modeled as a zero-mean bivariate Gaussian random vector with covariance matrix $\\boldsymbol{\\Sigma}$. Using properties of linear transformations of Gaussian random variables, derive an analytic expression for the expected magnitude of the false change, $\\mathbb{E}\\left[|D(\\mathbf{x}_0)|\\right]$, in terms of $\\nabla I_1(\\mathbf{x}_0)$ and $\\boldsymbol{\\Sigma}$.\n\n- Evaluate the derived expression numerically for a pixel located on a land-water boundary where the local spatial gradient is measured as $\\nabla I_1(\\mathbf{x}_0) = \\begin{pmatrix} 0.022 \\\\ -0.015 \\end{pmatrix}$ in reflectance per meter, and the misregistration covariance is $\\boldsymbol{\\Sigma} = \\begin{pmatrix} 0.36 & 0.06 \\\\ 0.06 & 0.16 \\end{pmatrix}$ in $\\mathrm{m}^2$. Express the final expected magnitude of the spurious change $\\mathbb{E}\\left[|D(\\mathbf{x}_0)|\\right]$ in decimal reflectance (dimensionless), and round your answer to $4$ significant figures.",
            "solution": "The problem is validated as scientifically grounded, well-posed, and objective. It is a standard application of multivariate calculus and statistics to a practical problem in remote sensing change detection. All necessary information is provided, and the problem is free of contradictions or ambiguities.\n\nThe problem asks for a three-part derivation and calculation concerning spurious change signals in image differencing caused by subpixel misregistration.\n\nFirst, we address the derivation of the leading-order contribution to the differenced image, $D(\\mathbf{x}_0)$, at a pixel $\\mathbf{x}_0$. The differenced image is defined as $D(\\mathbf{x}) = I_2(\\mathbf{x}) - I_1(\\mathbf{x})$. The problem states that there is no true physical change in the scene, so the underlying reflectance field is described by $I_1(\\mathbf{x})$ at both times. The second image, $I_2$, is a misregistered version of the first, meaning its samples are taken at shifted locations. This is formally expressed as $I_2(\\mathbf{x}) = I_1(\\mathbf{x} + \\delta\\mathbf{x})$, where $\\delta\\mathbf{x}$ is the subpixel misregistration vector.\n\nSubstituting this into the definition of the difference image at a specific pixel $\\mathbf{x}_0$, we get:\n$$D(\\mathbf{x}_0) = I_1(\\mathbf{x}_0 + \\delta\\mathbf{x}) - I_1(\\mathbf{x}_0)$$\nThe problem assumes that the image function $I_1(\\mathbf{x})$ is differentiable. We can therefore use a first-order multivariate Taylor expansion for $I_1$ around the point $\\mathbf{x}_0$:\n$$I_1(\\mathbf{x}_0 + \\delta\\mathbf{x}) \\approx I_1(\\mathbf{x}_0) + \\nabla I_1(\\mathbf{x}_0) \\cdot \\delta\\mathbf{x}$$\nHere, $\\nabla I_1(\\mathbf{x}_0)$ is the gradient of the image intensity at $\\mathbf{x}_0$, which is a column vector, and the product is the dot product. This can also be written as $\\nabla I_1(\\mathbf{x}_0)^T \\delta\\mathbf{x}$. Substituting this expansion back into the expression for $D(\\mathbf{x}_0)$:\n$$D(\\mathbf{x}_0) \\approx \\left( I_1(\\mathbf{x}_0) + \\nabla I_1(\\mathbf{x}_0)^T \\delta\\mathbf{x} \\right) - I_1(\\mathbf{x}_0)$$\nThis simplifies to the leading-order contribution to the false change signal:\n$$D(\\mathbf{x}_0) \\approx \\nabla I_1(\\mathbf{x}_0)^T \\delta\\mathbf{x}$$\n\nSecond, we derive an analytic expression for the expected magnitude of this false change, $\\mathbb{E}\\left[|D(\\mathbf{x}_0)|\\right]$. We model the false change as the scalar random variable $D_0 = \\mathbf{g}^T \\delta\\mathbf{x}$, where we define the constant vector $\\mathbf{g} = \\nabla I_1(\\mathbf{x}_0)$. The misregistration vector $\\delta\\mathbf{x}$ is modeled as a zero-mean bivariate Gaussian random vector, $\\delta\\mathbf{x} \\sim \\mathcal{N}(\\mathbf{0}, \\boldsymbol{\\Sigma})$, where $\\mathbf{0}$ is the zero vector and $\\boldsymbol{\\Sigma}$ is the covariance matrix.\n\nA fundamental property of the multivariate Gaussian distribution is that any linear transformation of a Gaussian random vector results in another Gaussian random variable (or vector). Since $D_0$ is a scalar linear transformation of $\\delta\\mathbf{x}$, $D_0$ is a univariate Gaussian random variable. We determine its mean and variance.\nThe mean of $D_0$ is:\n$$\\mathbb{E}[D_0] = \\mathbb{E}[\\mathbf{g}^T \\delta\\mathbf{x}] = \\mathbf{g}^T \\mathbb{E}[\\delta\\mathbf{x}] = \\mathbf{g}^T \\mathbf{0} = 0$$\nThe variance of $D_0$ is given by the general formula for the variance of a linear transformation of a random vector, $\\mathrm{Var}(\\mathbf{A}\\mathbf{y}) = \\mathbf{A}\\mathrm{Var}(\\mathbf{y})\\mathbf{A}^T$. In our scalar case ($D_0 = \\mathbf{g}^T \\delta\\mathbf{x}$):\n$$\\sigma_{D_0}^2 = \\mathrm{Var}(D_0) = \\mathrm{Var}(\\mathbf{g}^T \\delta\\mathbf{x}) = \\mathbf{g}^T \\mathrm{Var}(\\delta\\mathbf{x}) \\mathbf{g} = \\mathbf{g}^T \\boldsymbol{\\Sigma} \\mathbf{g}$$\nThus, $D_0$ follows a zero-mean normal distribution, $D_0 \\sim \\mathcal{N}(0, \\sigma_{D_0}^2)$, with probability density function $f(d) = \\frac{1}{\\sqrt{2\\pi}\\sigma_{D_0}} \\exp\\left(-\\frac{d^2}{2\\sigma_{D_0}^2}\\right)$.\n\nWe need to compute the expectation of the absolute value of $D_0$:\n$$\\mathbb{E}[|D_0|] = \\int_{-\\infty}^{\\infty} |d| f(d) \\, \\mathrm{d}d = \\int_{-\\infty}^{\\infty} |d| \\frac{1}{\\sqrt{2\\pi}\\sigma_{D_0}} \\exp\\left(-\\frac{d^2}{2\\sigma_{D_0}^2}\\right) \\, \\mathrm{d}d$$\nSince the integrand is an even function, we can simplify this to:\n$$\\mathbb{E}[|D_0|] = 2 \\int_{0}^{\\infty} d \\frac{1}{\\sqrt{2\\pi}\\sigma_{D_0}} \\exp\\left(-\\frac{d^2}{2\\sigma_{D_0}^2}\\right) \\, \\mathrm{d}d$$\nThis integral is a standard result, corresponding to the mean of a half-normal distribution. It evaluates to:\n$$\\mathbb{E}[|D_0|] = \\sigma_{D_0} \\sqrt{\\frac{2}{\\pi}}$$\nSubstituting the expression for $\\sigma_{D_0}$:\n$$\\mathbb{E}\\left[|D(\\mathbf{x}_0)|\\right] = \\sqrt{\\frac{2}{\\pi}} \\sqrt{\\mathbf{g}^T \\boldsymbol{\\Sigma} \\mathbf{g}} = \\sqrt{\\frac{2}{\\pi} \\left( \\nabla I_1(\\mathbf{x}_0)^T \\boldsymbol{\\Sigma} \\nabla I_1(\\mathbf{x}_0) \\right)}$$\n\nThird, we evaluate this expression numerically. The given values are:\n$$\\nabla I_1(\\mathbf{x}_0) = \\begin{pmatrix} 0.022 \\\\ -0.015 \\end{pmatrix} \\quad \\text{and} \\quad \\boldsymbol{\\Sigma} = \\begin{pmatrix} 0.36 & 0.06 \\\\ 0.06 & 0.16 \\end{pmatrix}$$\nLet $\\mathbf{g} = \\nabla I_1(\\mathbf{x}_0)$. We first compute the variance $\\sigma_{D_0}^2 = \\mathbf{g}^T \\boldsymbol{\\Sigma} \\mathbf{g}$:\n$$\\sigma_{D_0}^2 = \\begin{pmatrix} 0.022 & -0.015 \\end{pmatrix} \\begin{pmatrix} 0.36 & 0.06 \\\\ 0.06 & 0.16 \\end{pmatrix} \\begin{pmatrix} 0.022 \\\\ -0.015 \\end{pmatrix}$$\nFirst, we multiply $\\mathbf{g}^T \\boldsymbol{\\Sigma}$:\n$$\\begin{pmatrix} 0.022 & -0.015 \\end{pmatrix} \\begin{pmatrix} 0.36 & 0.06 \\\\ 0.06 & 0.16 \\end{pmatrix} = \\begin{pmatrix} (0.022)(0.36) + (-0.015)(0.06) & (0.022)(0.06) + (-0.015)(0.16) \\end{pmatrix}$$\n$$= \\begin{pmatrix} 0.00792 - 0.0009 & 0.00132 - 0.0024 \\end{pmatrix} = \\begin{pmatrix} 0.00702 & -0.00108 \\end{pmatrix}$$\nNext, we multiply this result by $\\mathbf{g}$:\n$$\\sigma_{D_0}^2 = \\begin{pmatrix} 0.00702 & -0.00108 \\end{pmatrix} \\begin{pmatrix} 0.022 \\\\ -0.015 \\end{pmatrix} = (0.00702)(0.022) + (-0.00108)(-0.015)$$\n$$\\sigma_{D_0}^2 = 0.00015444 + 0.0000162 = 0.00017064$$\nThe standard deviation $\\sigma_{D_0}$ is:\n$$\\sigma_{D_0} = \\sqrt{0.00017064} \\approx 0.01306292$$\nFinally, we compute the expected magnitude:\n$$\\mathbb{E}\\left[|D(\\mathbf{x}_0)|\\right] = \\sigma_{D_0} \\sqrt{\\frac{2}{\\pi}} \\approx (0.01306292) \\times \\sqrt{\\frac{2}{\\pi}} \\approx (0.01306292) \\times (0.79788456)$$\n$$\\mathbb{E}\\left[|D(\\mathbf{x}_0)|\\right] \\approx 0.01042299$$\nRounding to $4$ significant figures, the expected magnitude of the spurious change is $0.01042$. This value is in units of reflectance, which is dimensionless.",
            "answer": "$$\\boxed{0.01042}$$"
        },
        {
            "introduction": "Image ratioing is a powerful alternative to differencing, particularly for handling illumination variations, but a simple ratio value is insufficient; we must also assess its statistical significance. This exercise uses the delta method to propagate measurement uncertainty through the ratio function, $R(\\mathbf{x}) = \\frac{X_2(\\mathbf{x})}{X_1(\\mathbf{x})}$. This allows you to construct confidence intervals and make statistically robust decisions about whether an observed change is real or simply due to noise. ",
            "id": "3820727",
            "problem": "A ratio-based change detection scheme uses the ratio image $R(\\mathbf{x}) = \\dfrac{X_{2}(\\mathbf{x})}{X_{1}(\\mathbf{x})}$ between atmospherically corrected surface reflectance at two acquisition dates over the same location $\\mathbf{x}$. Consider a single pixel and model the reflectances as random variables $X_{1}$ and $X_{2}$ with finite means $\\mu_{1} = \\mathbb{E}[X_{1}]$, $\\mu_{2} = \\mathbb{E}[X_{2}]$, and variances $\\sigma_{1}^{2} = \\mathrm{Var}(X_{1})$, $\\sigma_{2}^{2} = \\mathrm{Var}(X_{2})$, arising from sensor and residual correction noise. Assume the measurement errors are unbiased and independent across dates so that $\\mathrm{Cov}(X_{1}, X_{2}) = 0$, and that the true reflectance is constant within each date so the uncertainty is dominated by measurement noise. Let $g(x_{1}, x_{2}) = x_{2}/x_{1}$.\n\nStarting only from the first-order multivariate Taylor expansion of $g$ about $(\\mu_{1}, \\mu_{2})$ and the standard rules for propagation of uncertainty (also known as the delta method), do the following:\n\n1. Derive the first-order approximations for $\\mathbb{E}[R]$ and $\\mathrm{Var}(R)$ in terms of $\\mu_{1}$, $\\mu_{2}$, $\\sigma_{1}^{2}$, and $\\sigma_{2}^{2}$ under the stated independence assumption.\n2. Using a normal approximation for $R$ based on your variance expression, propose a two-sided $(1-\\alpha)$ confidence interval (CI) for $R$ with $\\alpha = 0.05$.\n3. For a near-infrared pixel with parameters $\\mu_{1} = 0.24$, $\\mu_{2} = 0.264$, $\\sigma_{1} = 0.01$, and $\\sigma_{2} = 0.012$, compute the upper endpoint of the two-sided $95\\%$ CI for $R$ implied by your derivation. Round your answer to four significant figures. Express the result as a dimensionless number (no units).",
            "solution": "We begin from the measurement model where $X_{1}$ and $X_{2}$ are random variables representing the reflectance at two dates, with means $\\mu_{1}$ and $\\mu_{2}$, variances $\\sigma_{1}^{2}$ and $\\sigma_{2}^{2}$, and $\\mathrm{Cov}(X_{1}, X_{2}) = 0$. The change metric is $R = g(X_{1}, X_{2})$ with $g(x_{1}, x_{2}) = x_{2}/x_{1}$.\n\nFirst-order multivariate Taylor expansion (delta method): For a differentiable function $g$ and a random vector $\\mathbf{X} = (X_{1}, X_{2})^{\\top}$ with mean $\\boldsymbol{\\mu} = (\\mu_{1}, \\mu_{2})^{\\top}$ and covariance matrix $\\boldsymbol{\\Sigma}$, the first-order expansion is\n$$\ng(\\mathbf{X}) \\approx g(\\boldsymbol{\\mu}) + \\nabla g(\\boldsymbol{\\mu})^{\\top}(\\mathbf{X} - \\boldsymbol{\\mu}),\n$$\nand the propagated mean and variance are\n$$\n\\mathbb{E}[g(\\mathbf{X})] \\approx g(\\boldsymbol{\\mu}), \\quad \\mathrm{Var}(g(\\mathbf{X})) \\approx \\nabla g(\\boldsymbol{\\mu})^{\\top} \\boldsymbol{\\Sigma}\\, \\nabla g(\\boldsymbol{\\mu}).\n$$\n\nCompute the gradient of $g$:\n- The partial derivative with respect to $x_{1}$ is\n$$\n\\frac{\\partial g}{\\partial x_{1}}(x_{1}, x_{2}) = -\\frac{x_{2}}{x_{1}^{2}}.\n$$\n- The partial derivative with respect to $x_{2}$ is\n$$\n\\frac{\\partial g}{\\partial x_{2}}(x_{1}, x_{2}) = \\frac{1}{x_{1}}.\n$$\nThus,\n$$\n\\nabla g(\\boldsymbol{\\mu}) =\n\\begin{pmatrix}\n-\\dfrac{\\mu_{2}}{\\mu_{1}^{2}} \\\\\n\\dfrac{1}{\\mu_{1}}\n\\end{pmatrix}.\n$$\n\nUnder the independence assumption, the covariance matrix is\n$$\n\\boldsymbol{\\Sigma} =\n\\begin{pmatrix}\n\\sigma_{1}^{2} & 0 \\\\\n0 & \\sigma_{2}^{2}\n\\end{pmatrix}.\n$$\n\nTherefore, the first-order propagated mean and variance are:\n- Mean:\n$$\n\\mathbb{E}[R] \\approx g(\\boldsymbol{\\mu}) = \\frac{\\mu_{2}}{\\mu_{1}}.\n$$\n- Variance:\n$$\n\\mathrm{Var}(R) \\approx\n\\begin{pmatrix}\n-\\dfrac{\\mu_{2}}{\\mu_{1}^{2}} & \\dfrac{1}{\\mu_{1}}\n\\end{pmatrix}\n\\begin{pmatrix}\n\\sigma_{1}^{2} & 0 \\\\\n0 & \\sigma_{2}^{2}\n\\end{pmatrix}\n\\begin{pmatrix}\n-\\dfrac{\\mu_{2}}{\\mu_{1}^{2}} \\\\\n\\dfrac{1}{\\mu_{1}}\n\\end{pmatrix}\n=\n\\left(\\frac{\\mu_{2}}{\\mu_{1}^{2}}\\right)^{2}\\sigma_{1}^{2} + \\left(\\frac{1}{\\mu_{1}}\\right)^{2}\\sigma_{2}^{2}.\n$$\nThat is,\n$$\n\\mathrm{Var}(R) \\approx \\frac{\\mu_{2}^{2}\\,\\sigma_{1}^{2}}{\\mu_{1}^{4}} + \\frac{\\sigma_{2}^{2}}{\\mu_{1}^{2}}.\n$$\n\nTo propose a two-sided $(1-\\alpha)$ confidence interval for $R$, we use the normal approximation implied by the delta method:\n$$\nR \\approx \\mathcal{N}\\!\\left(\\frac{\\mu_{2}}{\\mu_{1}},\\, \\frac{\\mu_{2}^{2}\\,\\sigma_{1}^{2}}{\\mu_{1}^{4}} + \\frac{\\sigma_{2}^{2}}{\\mu_{1}^{2}}\\right).\n$$\nLet $z_{1-\\alpha/2}$ denote the upper $(1-\\alpha/2)$ quantile of the standard normal distribution. A two-sided $(1-\\alpha)$ confidence interval (CI) is then\n$$\n\\left[\\frac{\\mu_{2}}{\\mu_{1}} - z_{1-\\alpha/2}\\,\\sqrt{\\frac{\\mu_{2}^{2}\\,\\sigma_{1}^{2}}{\\mu_{1}^{4}} + \\frac{\\sigma_{2}^{2}}{\\mu_{1}^{2}}}\\,,\\;\\;\n\\frac{\\mu_{2}}{\\mu_{1}} + z_{1-\\alpha/2}\\,\\sqrt{\\frac{\\mu_{2}^{2}\\,\\sigma_{1}^{2}}{\\mu_{1}^{4}} + \\frac{\\sigma_{2}^{2}}{\\mu_{1}^{2}}}\\right].\n$$\n\nApply to the numerical values $\\mu_{1} = 0.24$, $\\mu_{2} = 0.264$, $\\sigma_{1} = 0.01$, $\\sigma_{2} = 0.012$, with $\\alpha = 0.05$ so that $z_{1-\\alpha/2} \\approx 1.96$.\n\n1. Point estimate:\n$$\n\\widehat{R} \\approx \\frac{\\mu_{2}}{\\mu_{1}} = \\frac{0.264}{0.24} = 1.1.\n$$\n\n2. Variance components:\n$$\n\\frac{\\sigma_{2}^{2}}{\\mu_{1}^{2}} = \\frac{0.012^{2}}{0.24^{2}} = \\frac{0.000144}{0.0576} = 0.0025,\n$$\n$$\n\\frac{\\mu_{2}^{2}\\,\\sigma_{1}^{2}}{\\mu_{1}^{4}} = \\frac{0.264^{2}\\cdot 0.01^{2}}{0.24^{4}} = \\frac{0.069696\\cdot 0.0001}{0.00331776} = \\frac{0.0000069696}{0.00331776} = 0.002100694444\\ldots\n$$\nHence,\n$$\n\\mathrm{Var}(R) \\approx 0.0025 + 0.002100694444\\ldots = 0.004600694444\\ldots\n$$\nand\n$$\n\\mathrm{SD}(R) \\approx \\sqrt{0.004600694444\\ldots} = 0.067828\\ldots\n$$\n\n3. Upper endpoint of the two-sided $95\\%$ CI:\n$$\n\\text{Upper} = \\widehat{R} + z_{0.975}\\,\\mathrm{SD}(R) \\approx 1.1 + 1.96 \\times 0.067828\\ldots \\approx 1.1 + 0.132944\\ldots = 1.232944\\ldots\n$$\n\nRounded to four significant figures, the upper endpoint is $1.233$ (dimensionless).",
            "answer": "$$\\boxed{1.233}$$"
        },
        {
            "introduction": "Simple pixel-wise methods treat each pixel as a monolithic unit, but true environmental change often occurs at the sub-pixel level, such as the gradual encroachment of vegetation. This advanced practice moves beyond simple arithmetic to a physically-based spectral unmixing model. You will implement a constrained least squares algorithm to estimate the change in material abundances (fractions) within a pixel and develop a statistical threshold to determine the significance of these changes, reflecting a modern, quantitative approach to remote sensing. ",
            "id": "3820689",
            "problem": "You are given two multispectral image acquisitions of the same scene at two dates, denoted by $t = 1$ and $t = 2$. Assume the standard Linear Mixing Model (LMM), where each pixel reflectance vector $r_t(x) \\in \\mathbb{R}^B$ is expressed as a linear combination of $J$ endmember spectra $E \\in \\mathbb{R}^{B \\times J}$ with additive measurement noise $\\varepsilon_t(x)$. The endmember fractions $f_t(x) \\in \\mathbb{R}^J$ are constrained to be nonnegative and to sum to one. You will derive a constrained least squares estimator of $f_t(x)$ at each date, compute the fraction change $\\Delta f_j(x) = f_{j,2}(x) - f_{j,1}(x)$ for each endmember $j$, and threshold $\\Delta f_j(x)$ using a statistically motivated criterion derived from a noise model. Reflectance is dimensionless and unitless; all outputs must be numerical decimals or booleans as specified.\n\nFundamental base and assumptions:\n- The Linear Mixing Model states $r_t(x) = E f_t(x) + \\varepsilon_t(x)$, with $f_t(x)$ subject to $\\sum_{j=1}^J f_{j,t}(x) = 1$ and $f_{j,t}(x) \\ge 0$ for all $j$.\n- Measurement noise is modeled as zero-mean Gaussian with band-wise covariance $\\Sigma_t \\in \\mathbb{R}^{B \\times B}$, assumed diagonal and known.\n- You must derive from these assumptions a constrained estimator of $f_t(x)$ based on least squares principles and then a change detection threshold for $\\Delta f_j(x)$ that follows from propagation of uncertainty.\n\nYour program must implement the following for a provided test suite:\n- Estimate $f_t(x)$ for each date $t \\in \\{1,2\\}$ via constrained least squares under the LMM, using the endmember matrix and the noise covariance.\n- Compute $\\Delta f(x) = f_2(x) - f_1(x)$.\n- Compute a per-endmember threshold $\\tau_j$ using the noise model such that a change is flagged when $|\\Delta f_j(x)| \\ge \\tau_j$; the test suite provides the significance multiplier $k$, and you must derive an appropriate $\\tau_j$ from the covariance of the fraction estimator under Gaussian assumptions.\n- Output, for each test case, the list of floats $\\Delta f_j(x)$ for $j = 1,\\dots,J$ and the corresponding list of booleans indicating whether change is flagged for each $j$.\n\nScientifically realistic parameters:\n- Number of bands $B$ is $3$ (Blue, Red, Near-Infrared).\n- Number of endmembers $J$ is $3$, with columns corresponding to Water, Vegetation, and Soil.\n- Endmember matrix $E$ is\n$$\nE = \\begin{bmatrix}\n0.05 & 0.05 & 0.30 \\\\\n0.02 & 0.04 & 0.35 \\\\\n0.01 & 0.45 & 0.40\n\\end{bmatrix}.\n$$\n- Band noise variances (diagonal of $\\Sigma_t$) for both dates are identical:\n$$\n\\Sigma_1 = \\Sigma_2 = \\operatorname{diag}(1\\times 10^{-4},\\ 1\\times 10^{-4},\\ 4\\times 10^{-4}).\n$$\n- Significance multiplier $k$ is $3$.\n\nTest suite with three cases (each case specifies two date reflectance vectors $r_1(x)$ and $r_2(x)$):\n1. Happy path case:\n   - $r_1(x) = [0.175,\\ 0.193,\\ 0.381]$\n   - $r_2(x) = [0.1375,\\ 0.1475,\\ 0.4105]$\n2. Boundary case (small change near threshold):\n   - $r_1(x) = [0.175,\\ 0.191,\\ 0.337]$\n   - $r_2(x) = [0.175,\\ 0.1912,\\ 0.3414]$\n3. Edge case (reflectances slightly outside the convex mixing simplex requiring constraint handling):\n   - $r_1(x) = [0.38,\\ 0.20,\\ 0.10]$\n   - $r_2(x) = [0.32,\\ 0.18,\\ 0.08]$\n\nAlgorithmic requirements:\n- Your constrained least squares implementation must be mathematically justified from first principles: whiten by the inverse square root of $\\Sigma_t$, obtain the unconstrained least squares solution in the whitened space, and enforce the nonnegativity and sum-to-one constraints by projecting the unconstrained solution onto the probability simplex $\\{f \\in \\mathbb{R}^J : f_j \\ge 0,\\ \\sum_j f_j = 1\\}$.\n- For thresholding, derive the covariance of the fraction estimator under Gaussian noise and propagate to $\\Delta f(x)$ across two independent dates to obtain a per-component standard deviation. Use the provided multiplier $k$ to form $\\tau_j$ and then apply the change flag rule $|\\Delta f_j(x)| \\ge \\tau_j$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element corresponds to one test case and is itself a list of two elements: the first is the list of floats $[\\Delta f_1(x), \\Delta f_2(x), \\Delta f_3(x)]$, and the second is the list of booleans $[b_1, b_2, b_3]$ where $b_j$ is $true$ if $|\\Delta f_j(x)| \\ge \\tau_j$ and $false$ otherwise.\n- For example, the structural format is $[[[\\Delta f_1,\\Delta f_2,\\Delta f_3],[b_1,b_2,b_3]], \\dots]$ with no spaces inserted by your program.",
            "solution": "The user-provided problem is valid as it is scientifically grounded in the principles of remote sensing and statistics, is well-posed with a clear objective and sufficient data, and is free of ambiguity or contradiction. We shall proceed with a complete solution.\n\nThe problem requires a two-part statistical analysis of multispectral image data. First, we must estimate material abundances (endmember fractions) at two different times using a constrained linear model. Second, we must develop and apply a statistical test to detect significant changes in these abundances between the two times.\n\n### Part 1: Constrained Estimation of Endmember Fractions\n\nThe relationship between the measured reflectance spectrum $r_t(x) \\in \\mathbb{R}^B$ at a pixel location $x$ and time $t$, and the fractions $f_t(x) \\in \\mathbb{R}^J$ of $J$ pure materials (endmembers), is given by the Linear Mixing Model (LMM):\n$$\nr_t(x) = E f_t(x) + \\varepsilon_t(x)\n$$\nHere, $E \\in \\mathbb{R}^{B \\times J}$ is the matrix whose columns are the spectra of the endmembers, and $\\varepsilon_t(x)$ is a vector of measurement noise. The fractions are subject to two physical constraints: the sum-to-one constraint, $\\sum_{j=1}^J f_{j,t}(x) = 1$, and the non-negativity constraint, $f_{j,t}(x) \\ge 0$ for all $j$.\n\nThe noise $\\varepsilon_t(x)$ is modeled as a zero-mean Gaussian random variable, $\\varepsilon_t(x) \\sim \\mathcal{N}(0, \\Sigma_t)$, with a known diagonal covariance matrix $\\Sigma_t$. The diagonal nature of $\\Sigma_t$ implies that noise is uncorrelated between spectral bands, but may have different variances (heteroscedasticity).\n\nTo find the optimal estimate of $f_t(x)$, we employ a Generalized Least Squares (GLS) approach, which accounts for the noise covariance. The objective is to find $f_t$ that minimizes the weighted squared error (the Mahalanobis distance):\n$$\n\\min_{f_t} (r_t - E f_t)^T \\Sigma_t^{-1} (r_t - E f_t) \\quad \\text{subject to} \\quad \\mathbf{1}^T f_t = 1, \\ f_t \\ge 0\n$$\nwhere we have suppressed the spatial coordinate $x$ for notational simplicity.\n\nAs stipulated, we follow a specific procedure:\n1.  **Whitening Transformation**: We first transform the problem into an Ordinary Least Squares (OLS) framework by \"whitening\" the data and model. Let $W_t = \\Sigma_t^{-1/2}$ be the whitening matrix. Since $\\Sigma_t$ is diagonal, $W_t$ is also diagonal with elements $(\\Sigma_t)_{ii}^{-1/2}$. Applying this to the LMM:\n    $$\n    W_t r_t = W_t E f_t + W_t \\varepsilon_t\n    $$\n    Let $r'_t = W_t r_t$, $E' = W_t E$, and $\\varepsilon'_t = W_t \\varepsilon_t$. The transformed model is $r'_t = E' f_t + \\varepsilon'_t$. The transformed noise $\\varepsilon'_t$ has covariance $\\operatorname{Cov}(\\varepsilon'_t) = W_t \\Sigma_t W_t^T = \\Sigma_t^{-1/2} \\Sigma_t \\Sigma_t^{-1/2} = I$, the identity matrix. Minimizing the original objective function is equivalent to minimizing the standard Euclidean norm in the whitened space: $\\min_{f_t} \\|r'_t - E' f_t\\|_2^2$.\n\n2.  **Unconstrained Least Squares Solution**: We first find the unconstrained solution, denoted $\\hat{f}_{t, \\text{unc}}$, which minimizes $\\|r'_t - E' f_t\\|_2^2$ without constraints. This is the standard OLS solution:\n    $$\n    \\hat{f}_{t, \\text{unc}} = ((E')^T E')^{-1} (E')^T r'_t = (E^T \\Sigma_t^{-1} E)^{-1} E^T \\Sigma_t^{-1} r_t\n    $$\n\n3.  **Projection onto the Probability Simplex**: The unconstrained solution $\\hat{f}_{t, \\text{unc}}$ does not, in general, satisfy the sum-to-one and non-negativity constraints. We enforce them by finding the vector on the probability simplex $\\Delta^J = \\{f \\in \\mathbb{R}^J : \\mathbf{1}^T f = 1, f \\ge 0\\}$ that is closest to $\\hat{f}_{t, \\text{unc}}$ in the Euclidean sense. This is a quadratic programming problem:\n    $$\n    \\hat{f}_t = \\arg\\min_{f \\in \\Delta^J} \\|f - \\hat{f}_{t, \\text{unc}}\\|_2^2\n    $$\n    An efficient algorithm solves this by finding a Lagrange multiplier $\\theta$ such that the solution is $f_j = \\max(0, \\hat{f}_{j,t,\\text{unc}} - \\theta)$, where $\\theta$ is chosen to satisfy the sum-to-one constraint. This involves sorting the components of $\\hat{f}_{t, \\text{unc}}$ and finding the correct active set of non-zero fractions.\n\n### Part 2: Statistical Change Detection\n\nThe goal is to detect if the change in fractions, $\\Delta f_j = f_{j,2} - f_{j,1}$, is statistically significant. This requires deriving the uncertainty (variance) of our fraction estimates and propagating it to the difference.\n\n1.  **Covariance of the Fraction Estimator**: The statistical properties are most tractable for the unconstrained GLS estimator, $\\hat{f}_{t, \\text{unc}}$. Its covariance matrix, which we denote $C_{f,t}$, is:\n    $$\n    C_{f,t} = \\operatorname{Cov}(\\hat{f}_{t, \\text{unc}}) = \\operatorname{Cov}( (E^T \\Sigma_t^{-1} E)^{-1} E^T \\Sigma_t^{-1} r_t )\n    $$\n    Since $r_t = E f_t + \\varepsilon_t$, and the true fractions $f_t$ are constant, this simplifies to:\n    $$\n    C_{f,t} = M \\operatorname{Cov}(\\varepsilon_t) M^T \\quad \\text{where} \\quad M = (E^T \\Sigma_t^{-1} E)^{-1} E^T \\Sigma_t^{-1}\n    $$\n    A standard result for GLS estimators is that this simplifies to:\n    $$\n    C_{f,t} = (E^T \\Sigma_t^{-1} E)^{-1}\n    $$\n    We use this covariance as an approximation of the uncertainty, acknowledging that the subsequent non-linear projection step alters the true distribution. This is a common and practical simplification.\n\n2.  **Propagation of Uncertainty to the Difference**: The change in estimated fractions is $\\Delta \\hat{f} = \\hat{f}_2 - \\hat{f}_1$. Assuming the measurements at times $t=1$ and $t=2$ are independent, their estimated fractions are also independent. The covariance of the difference is the sum of their individual covariances:\n    $$\n    C_{\\Delta f} = \\operatorname{Cov}(\\Delta \\hat{f}) = \\operatorname{Cov}(\\hat{f}_2) + \\operatorname{Cov}(\\hat{f}_1)\n    $$\n    Given that the noise covariance is identical at both times, $\\Sigma_1 = \\Sigma_2 = \\Sigma$, we have $C_{f,1} = C_{f,2} = C_f = (E^T \\Sigma^{-1} E)^{-1}$. Therefore:\n    $$\n    C_{\\Delta f} = 2 C_f = 2 (E^T \\Sigma^{-1} E)^{-1}\n    $$\n    The variance of the change for a single endmember $j$, denoted $\\operatorname{Var}(\\Delta f_j)$, is the $j$-th diagonal element of this matrix: $\\operatorname{Var}(\\Delta f_j) = (C_{\\Delta f})_{jj}$. The standard deviation is $\\sigma_{\\Delta f_j} = \\sqrt{(C_{\\Delta f})_{jj}}$.\n\n3.  **Change Detection Threshold**: Under the Gaussian noise assumption, $\\Delta \\hat{f}$ is also approximately Gaussian. A change is considered significant if its magnitude exceeds a certain number of standard deviations from zero. We construct a threshold $\\tau_j$ using the provided significance multiplier $k$:\n    $$\n    \\tau_j = k \\cdot \\sigma_{\\Delta f_j} = k \\sqrt{2 [ (E^T \\Sigma^{-1} E)^{-1} ]_{jj}}\n    $$\n    A change is flagged for endmember $j$ if the computed change in its constrained fraction, $|\\Delta f_j| = |\\hat{f}_{j,2} - \\hat{f}_{j,1}|$, satisfies the condition:\n    $$\n    |\\Delta f_j| \\ge \\tau_j\n    $$\n\nThis completes the derivation of the required algorithm. The implementation will follow these steps numerically for the provided test cases.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the change detection problem based on the LMM.\n    \"\"\"\n    # Define problem parameters\n    E = np.array([\n        [0.05, 0.05, 0.30],\n        [0.02, 0.04, 0.35],\n        [0.01, 0.45, 0.40]\n    ])\n    \n    variances = np.array([1e-4, 1e-4, 4e-4])\n    k = 3.0\n    \n    # Define test cases\n    test_cases = [\n        # Case 1: Happy path\n        (np.array([0.175, 0.193, 0.381]), np.array([0.1375, 0.1475, 0.4105])),\n        # Case 2: Boundary case\n        (np.array([0.175, 0.191, 0.337]), np.array([0.175, 0.1912, 0.3414])),\n        # Case 3: Edge case\n        (np.array([0.38, 0.20, 0.10]), np.array([0.32, 0.18, 0.08]))\n    ]\n\n    # --- Derivation of Thresholds ---\n    # Noise covariance matrix and its inverse\n    Sigma = np.diag(variances)\n    inv_Sigma = np.diag(1.0 / variances)\n    \n    # Covariance of the unconstrained fraction estimator\n    # C_f = (E^T * Sigma^-1 * E)^-1\n    A = E.T @ inv_Sigma @ E\n    C_f = np.linalg.inv(A)\n    \n    # Variance of the fraction change Delta_f\n    # Var(Delta_f) = 2 * diag(C_f)\n    var_delta_f = 2.0 * np.diag(C_f)\n    std_delta_f = np.sqrt(var_delta_f)\n    \n    # Thresholds for change detection\n    thresholds = k * std_delta_f\n\n    # --- Pre-computation for Fraction Estimation ---\n    # Whitening matrix and whitened endmember matrix\n    inv_sqrt_Sigma = np.diag(1.0 / np.sqrt(variances))\n    E_prime = inv_sqrt_Sigma @ E\n    \n    # Unconstrained estimator matrix in whitened space\n    M_prime = np.linalg.inv(E_prime.T @ E_prime) @ E_prime.T\n\n    def project_simplex(v):\n        \"\"\"\n        Projects a vector v onto the probability simplex.\n        \"\"\"\n        n_features = v.shape[0]\n        # Sort v in descending order\n        u = np.sort(v)[::-1]\n        cssv = np.cumsum(u)\n        # Find rho, the largest index i such that u_i > (cumsum_i - 1) / (i + 1)\n        # This is a vectorized implementation to find the correct threshold for projection.\n        indices = np.arange(n_features) + 1\n        cond = u > (cssv - 1) / indices\n        # In rare cases, no index satisfies the condition, meaning projection is just clamping.\n        # This can happen if the sum of positive elements of v is already = 1.\n        # However, the standard algorithm is guaranteed to find a solution.\n        rho = np.where(cond)[0][-1]\n        # Calculate theta, the Lagrange multiplier\n        theta = (cssv[rho] - 1) / (rho + 1)\n        # Project by shifting and clamping at 0\n        w = np.maximum(v - theta, 0)\n        return w\n\n    def estimate_fractions(r):\n        \"\"\"\n        Estimates endmember fractions for a given reflectance vector r.\n        \"\"\"\n        # Whiten the reflectance vector\n        r_prime = inv_sqrt_Sigma @ r\n        # Compute unconstrained fractions\n        f_unc = M_prime @ r_prime\n        # Project onto the simplex to enforce constraints\n        f_constrained = project_simplex(f_unc)\n        return f_constrained\n\n    results = []\n    for r1, r2 in test_cases:\n        # Estimate fractions for time 1 and time 2\n        f1 = estimate_fractions(r1)\n        f2 = estimate_fractions(r2)\n        \n        # Compute fraction change\n        delta_f = f2 - f1\n        \n        # Apply change detection threshold\n        change_flags = np.abs(delta_f) >= thresholds\n        \n        results.append([delta_f.tolist(), change_flags.tolist()])\n\n    def format_output(data):\n        \"\"\"Custom formatter to match the required output string format.\"\"\"\n        if isinstance(data, list):\n            return f\"[{','.join(format_output(x) for x in data)}]\"\n        elif isinstance(data, (bool, np.bool_)):\n            return str(data).lower()\n        elif isinstance(data, (float, np.floating)):\n            # Using a fixed precision for consistent output\n            return f\"{data:.8f}\"\n        else:\n            return str(data)\n\n    # Print the final result in the specified format\n    print(format_output(results))\n\nsolve()\n```"
        }
    ]
}