{
    "hands_on_practices": [
        {
            "introduction": "At the core of any change detection workflow lies a fundamental decision: has a given pixel changed or has it remained stable? This decision is formally handled using statistical hypothesis testing. This exercise grounds you in the Neyman-Pearson criterion, a foundational method for establishing a decision threshold that strictly controls the probability of false alarms—a critical requirement in applications from environmental monitoring to disaster response . By working through this problem, you will solidify the essential skill of translating a desired error rate into a concrete decision threshold based on the statistical properties of your data.",
            "id": "3800392",
            "problem": "A pixel-wise change detection workflow is applied to a time series of Synthetic Aperture Radar (SAR) images in which the per-pixel log-intensity difference variable $d$ (in decibels) between two acquisition times is modeled as follows: under the no-change hypothesis $H_0$, $d$ is Gaussian with distribution $d \\sim \\mathcal{N}(\\mu_0, \\sigma^2)$; under the change hypothesis $H_1$, $d$ is Gaussian with distribution $d \\sim \\mathcal{N}(\\mu_1, \\sigma^2)$. The operational objective is to detect positive changes (increases in backscatter), so assume $\\mu_1  \\mu_0$ and adopt the Neyman–Pearson decision criterion that controls the false alarm probability at a target value. In a calibrated coastal wetland monitoring scenario, the sensor and preprocessing pipeline yield the following parameters: $\\mu_0 = -0.2$ (decibels), $\\sigma = 1.3$ (decibels), and a target false alarm probability $P_{FA} = 0.01$ (expressed as a decimal). Starting from the foundational definitions of hypothesis testing and the Neyman–Pearson principle, derive the decision threshold $\\tau$ on $d$ for an upper-tail test that achieves the specified $P_{FA}$, and then compute its numerical value using the given parameters. Express the final threshold in decibels and round your answer to four significant figures.",
            "solution": "The workflow is a binary hypothesis test on the scalar difference variable $d$. Under the no-change hypothesis $H_0$, $d \\sim \\mathcal{N}(\\mu_0, \\sigma^2)$; under the change hypothesis $H_1$, $d \\sim \\mathcal{N}(\\mu_1, \\sigma^2)$, with $\\mu_1  \\mu_0$. The Neyman–Pearson principle prescribes a most-powerful test for a fixed false alarm rate by thresholding the likelihood ratio. For two Gaussian distributions with a common variance, the likelihood ratio is a monotone function of $d$ when the means are ordered, so the most-powerful test reduces to an upper-tail threshold on $d$ when $\\mu_1  \\mu_0$. \n\nLet the decision rule be to declare change if $d \\geq \\tau$ and no change otherwise. The false alarm probability is defined as\n$$\nP_{FA} = \\mathbb{P}(d \\geq \\tau \\mid H_0).\n$$\nUnder $H_0$, $d$ is Gaussian, so we can standardize:\n$$\nZ = \\frac{d - \\mu_0}{\\sigma} \\sim \\mathcal{N}(0, 1).\n$$\nTherefore,\n$$\nP_{FA} = \\mathbb{P}\\!\\left( \\frac{d - \\mu_0}{\\sigma} \\geq \\frac{\\tau - \\mu_0}{\\sigma} \\,\\middle|\\, H_0 \\right) = \\mathbb{P}\\!\\left( Z \\geq \\frac{\\tau - \\mu_0}{\\sigma} \\right) = 1 - \\Phi\\!\\left( \\frac{\\tau - \\mu_0}{\\sigma} \\right),\n$$\nwhere $\\Phi(\\cdot)$ is the cumulative distribution function of the standard normal distribution. Solving for the threshold that achieves a specified $P_{FA}$ gives\n$$\n1 - \\Phi\\!\\left( \\frac{\\tau - \\mu_0}{\\sigma} \\right) = P_{FA}\n\\quad \\Longrightarrow \\quad\n\\Phi\\!\\left( \\frac{\\tau - \\mu_0}{\\sigma} \\right) = 1 - P_{FA}\n\\quad \\Longrightarrow \\quad\n\\frac{\\tau - \\mu_0}{\\sigma} = \\Phi^{-1}(1 - P_{FA}),\n$$\nand hence\n$$\n\\tau = \\mu_0 + \\sigma \\, \\Phi^{-1}(1 - P_{FA}).\n$$\n\nWe now compute the numerical value using the given parameters. With $\\mu_0 = -0.2$, $\\sigma = 1.3$, and $P_{FA} = 0.01$, we have $1 - P_{FA} = 0.99$ and\n$$\n\\Phi^{-1}(0.99) \\approx 2.326347874.\n$$\nThus,\n$$\n\\tau = -0.2 + 1.3 \\times 2.326347874 = -0.2 + 3.0242522362 = 2.8242522362.\n$$\nRounding to four significant figures yields\n$$\n\\tau \\approx 2.824.\n$$\nInterpreting in the application context, the threshold $2.824$ decibels defines the upper-tail decision rule for declaring change at the desired false alarm probability $P_{FA} = 0.01$.",
            "answer": "$$\\boxed{2.824}$$"
        },
        {
            "introduction": "While a single decision threshold is necessary for producing a final change map, its specific value always represents a trade-off between detecting true changes (true positives) and generating spurious ones (false positives). To move beyond a single operating point, we need a way to evaluate the intrinsic performance of our change metric across all possible thresholds. The Receiver Operating Characteristic (ROC) curve and its corresponding Area Under the Curve (AUC) provide a comprehensive framework for this evaluation . This practice will guide you through deriving the direct relationship between a detector's performance and the statistical separability of the \"change\" and \"no-change\" classes, offering deep insight into what makes a change detection algorithm effective.",
            "id": "3800442",
            "problem": "In an operational change detection workflow for multi-temporal satellite observations of a temperate forest, a scalar detection statistic $T$ is computed per pixel from the two-date imagery using a radiometrically calibrated linear index. After atmospheric correction and bidirectional reflectance normalization, assume $T$ can be modeled as Gaussian under both the null hypothesis of no-change and the alternative hypothesis of change, with equal variance. Specifically, under $H_{0}$ (no-change), $T \\sim \\mathcal{N}(\\mu_{0}, \\sigma^{2})$, and under $H_{1}$ (change), $T \\sim \\mathcal{N}(\\mu_{1}, \\sigma^{2})$, with $\\sigma  0$. A threshold detector declares change if $T \\geq \\tau$, where $\\tau \\in \\mathbb{R}$ is varied to trace the Receiver Operating Characteristic (ROC) curve. Let the separability index be defined as $d' = (\\mu_{1} - \\mu_{0})/\\sigma$.\n\nStarting from the fundamental definitions of the false positive rate $\\text{FPR}(\\tau) = \\mathbb{P}(T \\geq \\tau \\mid H_{0})$ and the true positive rate $\\text{TPR}(\\tau) = \\mathbb{P}(T \\geq \\tau \\mid H_{1})$ for a threshold detector applied to Gaussian distributions, and using only the standard properties of the Gaussian probability density function and cumulative distribution function, perform the following:\n\n1. Derive the parametric ROC relation by eliminating the threshold $\\tau$ to express $\\text{TPR}$ directly as a function of $\\text{FPR}$ for the equal-variance Gaussian case in terms of the separability $d'$.\n\n2. Using the definition of the Area Under the Curve (AUC) as the integral of $\\text{TPR}$ with respect to $\\text{FPR}$ over its domain, derive a closed-form analytic expression for the AUC that depends only on $d'$ and fundamental functions.\n\nExpress your final AUC solely as a function of $d'$. The final answer must be a single closed-form analytic expression. No numerical rounding is required, and no physical units are involved.",
            "solution": "Let $\\Phi(z)$ denote the cumulative distribution function (CDF) of the standard normal distribution $\\mathcal{N}(0, 1)$, and let $\\phi(z)$ be its probability density function (PDF).\n$$\n\\Phi(z) = \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^{z} \\exp\\left(-\\frac{x^2}{2}\\right) dx\n$$\n$$\n\\phi(z) = \\frac{d\\Phi(z)}{dz} = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{z^2}{2}\\right)\n$$\nThe complementary CDF is $Q(z) = 1 - \\Phi(z) = \\mathbb{P}(Z  z)$ for $Z \\sim \\mathcal{N}(0, 1)$.\n\nThe false positive rate (FPR) is the probability of detecting change when there is none ($H_0$). With the detection statistic $T \\sim \\mathcal{N}(\\mu_0, \\sigma^2)$ under $H_0$, the FPR for a threshold $\\tau$ is:\n$$\n\\text{FPR}(\\tau) = \\mathbb{P}(T \\geq \\tau \\mid H_0)\n$$\nTo evaluate this, we standardize the variable $T$:\n$$\n\\text{FPR}(\\tau) = \\mathbb{P}\\left(\\frac{T - \\mu_0}{\\sigma} \\geq \\frac{\\tau - \\mu_0}{\\sigma}\\right)\n$$\nSince $(T - \\mu_0)/\\sigma \\sim \\mathcal{N}(0, 1)$, we have:\n$$\n\\text{FPR}(\\tau) = Q\\left(\\frac{\\tau - \\mu_0}{\\sigma}\\right) = 1 - \\Phi\\left(\\frac{\\tau - \\mu_0}{\\sigma}\\right)\n$$\n\nThe true positive rate (TPR), or sensitivity, is the probability of detecting change when it has occurred ($H_1$). With $T \\sim \\mathcal{N}(\\mu_1, \\sigma^2)$ under $H_1$, the TPR is:\n$$\n\\text{TPR}(\\tau) = \\mathbb{P}(T \\geq \\tau \\mid H_1) = \\mathbb{P}\\left(\\frac{T - \\mu_1}{\\sigma} \\geq \\frac{\\tau - \\mu_1}{\\sigma}\\right)\n$$\nSince $(T - \\mu_1)/\\sigma \\sim \\mathcal{N}(0, 1)$, we have:\n$$\n\\text{TPR}(\\tau) = Q\\left(\\frac{\\tau - \\mu_1}{\\sigma}\\right) = 1 - \\Phi\\left(\\frac{\\tau - \\mu_1}{\\sigma}\\right)\n$$\n\n**Part 1: Derivation of the Parametric ROC Relation**\n\nTo express TPR as a function of FPR, we must eliminate the threshold parameter $\\tau$.\nFrom the FPR expression:\n$$\n\\text{FPR} = 1 - \\Phi\\left(\\frac{\\tau - \\mu_0}{\\sigma}\\right) \\implies \\Phi\\left(\\frac{\\tau - \\mu_0}{\\sigma}\\right) = 1 - \\text{FPR}\n$$\nApplying the inverse standard normal CDF, $\\Phi^{-1}(\\cdot)$:\n$$\n\\frac{\\tau - \\mu_0}{\\sigma} = \\Phi^{-1}(1 - \\text{FPR})\n$$\nUsing the property of the standard normal distribution that $\\Phi^{-1}(1 - p) = -\\Phi^{-1}(p)$:\n$$\n\\frac{\\tau - \\mu_0}{\\sigma} = -\\Phi^{-1}(\\text{FPR})\n$$\nNow, let's manipulate the argument of the TPR's CDF:\n$$\n\\frac{\\tau - \\mu_1}{\\sigma} = \\frac{(\\tau - \\mu_0) - (\\mu_1 - \\mu_0)}{\\sigma} = \\frac{\\tau - \\mu_0}{\\sigma} - \\frac{\\mu_1 - \\mu_0}{\\sigma}\n$$\nSubstituting the term for the threshold and the definition of the separability index $d' = (\\mu_1 - \\mu_0)/\\sigma$:\n$$\n\\frac{\\tau - \\mu_1}{\\sigma} = -\\Phi^{-1}(\\text{FPR}) - d'\n$$\nNow, substitute this back into the expression for TPR:\n$$\n\\text{TPR} = 1 - \\Phi\\left(\\frac{\\tau - \\mu_1}{\\sigma}\\right) = 1 - \\Phi(-\\Phi^{-1}(\\text{FPR}) - d')\n$$\nUsing the symmetry property of the normal CDF, $\\Phi(-z) = 1 - \\Phi(z)$, or equivalently $1 - \\Phi(-z) = \\Phi(z)$:\n$$\n\\text{TPR} = \\Phi(\\Phi^{-1}(\\text{FPR}) + d')\n$$\nThis is the parametric ROC relation, expressing TPR as a function of FPR and the separability $d'$.\n\n**Part 2: Derivation of the Area Under the Curve (AUC)**\n\nThe AUC is the integral of TPR with respect to FPR over the entire range of FPR, which is $[0, 1]$.\n$$\n\\text{AUC} = \\int_0^1 \\text{TPR}(\\text{FPR}) \\, d(\\text{FPR})\n$$\nThe ROC curve is parameterized by $\\tau$. As $\\tau$ varies from $-\\infty$ to $+\\infty$, FPR varies from $1$ to $0$. We can use this parameterization to evaluate the integral.\nFirst, we find the differential $d(\\text{FPR})$ in terms of $d\\tau$:\n$$\n\\text{FPR}(\\tau) = 1 - \\Phi\\left(\\frac{\\tau - \\mu_0}{\\sigma}\\right)\n$$\n$$\n\\frac{d(\\text{FPR})}{d\\tau} = -\\frac{d}{d\\tau} \\Phi\\left(\\frac{\\tau - \\mu_0}{\\sigma}\\right) = -\\phi\\left(\\frac\\tau - \\mu_0}{\\sigma}\\right) \\cdot \\frac{1}{\\sigma}\n$$\nSo, $d(\\text{FPR}) = -\\frac{1}{\\sigma} \\phi\\left(\\frac{\\tau - \\mu_0}{\\sigma}\\right) d\\tau$.\n\nThe limits of integration for $\\tau$ corresponding to $\\text{FPR}=0$ and $\\text{FPR}=1$ are $\\tau=+\\infty$ and $\\tau=-\\infty$, respectively.\nSubstituting into the AUC integral:\n$$\n\\text{AUC} = \\int_{\\tau=+\\infty}^{\\tau=-\\infty} \\text{TPR}(\\tau) \\left(-\\frac{1}{\\sigma} \\phi\\left(\\frac{\\tau - \\mu_0}{\\sigma}\\right)\\right) d\\tau\n$$\nFlipping the integration limits reverses the sign:\n$$\n\\text{AUC} = \\int_{-\\infty}^{+\\infty} \\text{TPR}(\\tau) \\frac{1}{\\sigma} \\phi\\left(\\frac{\\tau - \\mu_0}{\\sigma}\\right) d\\tau\n$$\nSubstitute the expression for $\\text{TPR}(\\tau)$:\n$$\n\\text{AUC} = \\int_{-\\infty}^{+\\infty} \\left[1 - \\Phi\\left(\\frac{\\tau - \\mu_1}{\\sigma}\\right)\\right] \\frac{1}{\\sigma} \\phi\\left(\\frac{\\tau - \\mu_0}{\\sigma}\\right) d\\tau\n$$\nTo solve this integral, we perform a change of variables. Let $u = (\\tau - \\mu_0)/\\sigma$. Then $\\tau = \\sigma u + \\mu_0$ and $d\\tau = \\sigma du$. The limits of integration for $u$ remain $(-\\infty, +\\infty)$.\nThe term $\\frac{1}{\\sigma} \\phi\\left(\\frac{\\tau - \\mu_0}{\\sigma}\\right) d\\tau$ becomes $\\phi(u) du$.\nThe argument of the $\\Phi$ function becomes:\n$$\n\\frac{\\tau - \\mu_1}{\\sigma} = \\frac{(\\sigma u + \\mu_0) - \\mu_1}{\\sigma} = u + \\frac{\\mu_0 - \\mu_1}{\\sigma} = u - d'\n$$\nThe integral transforms to:\n$$\n\\text{AUC} = \\int_{-\\infty}^{+\\infty} [1 - \\Phi(u - d')] \\phi(u) du\n$$\nThis integral has a probabilistic interpretation. If $U$ and $Z$ are two independent random variables drawn from the standard normal distribution $\\mathcal{N}(0, 1)$, their PDFs are $\\phi(u)$ and $\\phi(z)$, respectively. The integral represents:\n$$\n\\text{AUC} = \\int_{-\\infty}^{+\\infty} \\mathbb{P}(Z  u - d') \\phi(u) du = \\mathbb{P}(Z  U - d')\n$$\nwhere the probability is taken over the joint distribution of $U$ and $Z$.\nThe inequality can be rewritten as $\\mathbb{P}(U - Z  d')$.\nLet us define a new random variable $W = U - Z$. Since $U$ and $Z$ are independent Gaussian variables, $W$ is also Gaussian.\nThe mean of $W$ is $\\mathbb{E}[W] = \\mathbb{E}[U] - \\mathbb{E}[Z] = 0 - 0 = 0$.\nThe variance of $W$ is $\\text{Var}(W) = \\text{Var}(U) + \\text{Var}(-Z) = \\text{Var}(U) + (-1)^2 \\text{Var}(Z) = 1 + 1 = 2$.\nSo, $W \\sim \\mathcal{N}(0, 2)$.\nThe AUC is the probability $\\mathbb{P}(W  d')$. To calculate this, we standardize $W$:\n$$\n\\mathbb{P}(W  d') = \\mathbb{P}\\left(\\frac{W - 0}{\\sqrt{2}}  \\frac{d' - 0}{\\sqrt{2}}\\right)\n$$\nThe variable $(W-0)/\\sqrt{2}$ is a standard normal variable. Therefore, the probability is given by the standard normal CDF $\\Phi(\\cdot)$:\n$$\n\\text{AUC} = \\Phi\\left(\\frac{d'}{\\sqrt{2}}\\right)\n$$\nThis is the closed-form analytic expression for the AUC, dependent only on the separability index $d'$.",
            "answer": "$$\n\\boxed{\\Phi\\left(\\frac{d'}{\\sqrt{2}}\\right)}\n$$"
        },
        {
            "introduction": "The Neyman-Pearson approach is powerful but assumes we want to fix the false alarm rate beforehand. An alternative, common in unsupervised change detection, is to find a threshold that best separates the entire population of pixels into \"changed\" and \"unchanged\" classes. This is typically done by modeling the data as a mixture of two distributions and finding the threshold that minimizes the total classification error. This exercise challenges you to derive this optimal threshold from the first principles of Bayesian decision theory, as famously formulated in the Kittler-Illingworth algorithm . You will see how the class means, variance, and prior probabilities combine to define the ideal decision boundary that minimizes overall error.",
            "id": "3800402",
            "problem": "A bi-temporal optical reflectance dataset has been radiometrically normalized and differenced to produce a scalar change magnitude image, denoted by $m \\in \\mathbb{R}$. Empirical evidence suggests the histogram of $m$ is bimodal and can be modeled as a two-component mixture with class-conditional Gaussian densities: one mode for \"unchanged\" pixels ($U$) and one for \"changed\" pixels ($C$). Suppose the class-conditional densities are Gaussian with equal variance, that is $m \\mid U \\sim \\mathcal{N}(\\mu_{U}, \\sigma^{2})$ and $m \\mid C \\sim \\mathcal{N}(\\mu_{C}, \\sigma^{2})$, with $\\,\\mu_{U}  \\mu_{C}\\,$ due to change magnitudes being generally larger for changed pixels after normalization. Let the prior probabilities be $\\pi_{U}$ and $\\pi_{C}$ with $\\pi_{U} + \\pi_{C} = 1$.\n\nUnder equal misclassification costs, the Kittler–Illingworth minimum error threshold is the scalar decision boundary $\\tau$ such that the Bayes decision rule partitions the real line into two regions: decide $U$ for $m  \\tau$ and decide $C$ for $m \\ge \\tau$, thereby minimizing the total classification error. Starting from first principles of Bayesian decision theory and the Maximum a Posteriori (MAP) rule, derive the analytic formula for the threshold $\\tau$ in terms of $\\mu_{U}$, $\\mu_{C}$, $\\sigma^{2}$, $\\pi_{U}$, and $\\pi_{C}$.\n\nExpress your final answer as a single closed-form analytic expression for $\\tau$. No rounding is required. The threshold $\\tau$ is a scalar on the same measurement scale as $m$; do not include units in the final boxed expression.",
            "solution": "The goal is to find the optimal decision threshold $\\tau$ that minimizes the total probability of classification error. Under the specified condition of equal misclassification costs, this is achieved by the Maximum a Posteriori (MAP) decision rule. The MAP rule assigns a measurement $m$ to the class $K$ that has the maximum posterior probability, $P(K \\mid m)$.\n\nThe decision boundary $\\tau$ is the specific value of the measurement $m$ where the posterior probabilities for both classes are equal. At this point of indifference, we have:\n$$P(U \\mid m=\\tau) = P(C \\mid m=\\tau)$$\n\nAccording to Bayes' theorem, the posterior probability of a class $K \\in \\{U, C\\}$ given the measurement $m$ is:\n$$P(K \\mid m) = \\frac{p(m \\mid K) P(K)}{p(m)}$$\nHere, $p(m \\mid K)$ is the class-conditional probability density function (the likelihood), $P(K)$ is the prior probability of the class, and $p(m)$ is the evidence, which is the marginal probability density of the measurement $m$.\n\nSubstituting the Bayesian formulation into our equality at the threshold $\\tau$:\n$$\\frac{p(\\tau \\mid U) P(U)}{p(\\tau)} = \\frac{p(\\tau \\mid C) P(C)}{p(\\tau)}$$\n\nSince $p(\\tau)$ is a common, non-zero denominator, it can be cancelled from both sides, yielding a relationship between the likelihoods and priors:\n$$p(\\tau \\mid U) P(U) = p(\\tau \\mid C) P(C)$$\n\nWe are given the explicit forms for these terms:\n- Priors: $P(U) = \\pi_{U}$ and $P(C) = \\pi_{C}$.\n- Likelihoods: The class-conditional densities are Gaussian. The probability density function for a normal distribution $\\mathcal{N}(\\mu, \\sigma^2)$ is $p(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)$.\nTherefore,\n$$p(\\tau \\mid U) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(\\tau - \\mu_{U})^2}{2\\sigma^2}\\right)$$\n$$p(\\tau \\mid C) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(\\tau - \\mu_{C})^2}{2\\sigma^2}\\right)$$\n\nSubstituting these expressions into the equality:\n$$\\left[ \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(\\tau - \\mu_{U})^2}{2\\sigma^2}\\right) \\right] \\pi_{U} = \\left[ \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(\\tau - \\mu_{C})^2}{2\\sigma^2}\\right) \\right] \\pi_{C}$$\n\nThe constant term $\\frac{1}{\\sqrt{2\\pi\\sigma^2}}$ is present on both sides and can be cancelled:\n$$\\pi_{U} \\exp\\left(-\\frac{(\\tau - \\mu_{U})^2}{2\\sigma^2}\\right) = \\pi_{C} \\exp\\left(-\\frac{(\\tau - \\mu_{C})^2}{2\\sigma^2}\\right)$$\n\nTo solve for $\\tau$, we take the natural logarithm ($\\ln$) of both sides. This is a monotonic transformation that simplifies the exponential terms:\n$$\\ln\\left( \\pi_{U} \\exp\\left(-\\frac{(\\tau - \\mu_{U})^2}{2\\sigma^2}\\right) \\right) = \\ln\\left( \\pi_{C} \\exp\\left(-\\frac{(\\tau - \\mu_{C})^2}{2\\sigma^2}\\right) \\right)$$\n\nUsing the logarithm property $\\ln(ab) = \\ln(a) + \\ln(b)$:\n$$\\ln(\\pi_{U}) + \\ln\\left( \\exp\\left(-\\frac{(\\tau - \\mu_{U})^2}{2\\sigma^2}\\right) \\right) = \\ln(\\pi_{C}) + \\ln\\left( \\exp\\left(-\\frac{(\\tau - \\mu_{C})^2}{2\\sigma^2}\\right) \\right)$$\n\nUsing the property $\\ln(\\exp(x)) = x$:\n$$\\ln(\\pi_{U}) - \\frac{(\\tau - \\mu_{U})^2}{2\\sigma^2} = \\ln(\\pi_{C}) - \\frac{(\\tau - \\mu_{C})^2}{2\\sigma^2}$$\n\nTo isolate $\\tau$, we first multiply the entire equation by $2\\sigma^2$ to eliminate the denominators:\n$$2\\sigma^2 \\ln(\\pi_{U}) - (\\tau - \\mu_{U})^2 = 2\\sigma^2 \\ln(\\pi_{C}) - (\\tau - \\mu_{C})^2$$\n\nNext, expand the squared binomial terms:\n$$2\\sigma^2 \\ln(\\pi_{U}) - (\\tau^2 - 2\\tau\\mu_{U} + \\mu_{U}^2) = 2\\sigma^2 \\ln(\\pi_{C}) - (\\tau^2 - 2\\tau\\mu_{C} + \\mu_{C}^2)$$\n$$2\\sigma^2 \\ln(\\pi_{U}) - \\tau^2 + 2\\tau\\mu_{U} - \\mu_{U}^2 = 2\\sigma^2 \\ln(\\pi_{C}) - \\tau^2 + 2\\tau\\mu_{C} - \\mu_{C}^2$$\n\nThe $\\tau^2$ terms on both sides cancel out. This simplification is a direct consequence of the equal variance assumption ($\\sigma_U^2 = \\sigma_C^2 = \\sigma^2$).\n$$2\\sigma^2 \\ln(\\pi_{U}) + 2\\tau\\mu_{U} - \\mu_{U}^2 = 2\\sigma^2 \\ln(\\pi_{C}) + 2\\tau\\mu_{C} - \\mu_{C}^2$$\n\nNow, we rearrange the equation to group terms involving $\\tau$ on one side and all other terms on the other side:\n$$2\\tau\\mu_{U} - 2\\tau\\mu_{C} = \\mu_{U}^2 - \\mu_{C}^2 + 2\\sigma^2 \\ln(\\pi_{C}) - 2\\sigma^2 \\ln(\\pi_{U})$$\n\nFactor out common terms: $2\\tau$ on the left, and $2\\sigma^2$ on the right.\n$$2\\tau(\\mu_{U} - \\mu_{C}) = (\\mu_{U}^2 - \\mu_{C}^2) + 2\\sigma^2 (\\ln(\\pi_{C}) - \\ln(\\pi_{U}))$$\n\nUsing the logarithm property $\\ln(a) - \\ln(b) = \\ln(a/b)$ and factoring the difference of squares $\\mu_{U}^2 - \\mu_{C}^2 = (\\mu_{U} - \\mu_{C})(\\mu_{U} + \\mu_{C})$:\n$$2\\tau(\\mu_{U} - \\mu_{C}) = (\\mu_{U} - \\mu_{C})(\\mu_{U} + \\mu_{C}) + 2\\sigma^2 \\ln\\left(\\frac{\\pi_{C}}{\\pi_{U}}\\right)$$\n\nTo solve for $\\tau$, we divide by $2(\\mu_{U} - \\mu_{C})$. This is permissible because we are given that $\\mu_{U}  \\mu_{C}$, which implies $\\mu_{U} - \\mu_{C} \\neq 0$.\n$$\\tau = \\frac{(\\mu_{U} - \\mu_{C})(\\mu_{U} + \\mu_{C})}{2(\\mu_{U} - \\mu_{C})} + \\frac{2\\sigma^2}{2(\\mu_{U} - \\mu_{C})} \\ln\\left(\\frac{\\pi_{C}}{\\pi_{U}}\\right)$$\n\nSimplifying the expression:\n$$\\tau = \\frac{\\mu_{U} + \\mu_{C}}{2} + \\frac{\\sigma^2}{\\mu_{U} - \\mu_{C}} \\ln\\left(\\frac{\\pi_{C}}{\\pi_{U}}\\right)$$\n\nTo maintain a positive denominator, as $\\mu_{C}  \\mu_{U}$, we can rewrite $\\frac{1}{\\mu_{U} - \\mu_{C}}$ as $-\\frac{1}{\\mu_{C} - \\mu_{U}}$. Also, using the property $\\ln(a/b) = -\\ln(b/a)$:\n$$\\tau = \\frac{\\mu_{U} + \\mu_{C}}{2} + \\frac{\\sigma^2}{-(\\mu_{C} - \\mu_{U})} \\ln\\left(\\frac{\\pi_{C}}{\\pi_{U}}\\right) = \\frac{\\mu_{U} + \\mu_{C}}{2} - \\frac{\\sigma^2}{\\mu_{C} - \\mu_{U}} [-\\ln\\left(\\frac{\\pi_{U}}{\\pi_{C}}\\right)]$$\nThis leads to the final, more conventional form:\n$$\\tau = \\frac{\\mu_{U} + \\mu_{C}}{2} + \\frac{\\sigma^2}{\\mu_{C} - \\mu_{U}} \\ln\\left(\\frac{\\pi_{U}}{\\pi_{C}}\\right)$$\n\nThis analytical expression shows that the optimal threshold $\\tau$ is the midpoint between the two class means, $\\frac{\\mu_{U} + \\mu_{C}}{2}$, adjusted by a bias term. The bias depends on the common variance $\\sigma^2$ and the ratio of the prior probabilities. If the priors are equal, $\\pi_{U} = \\pi_{C}$, then $\\ln(\\pi_{U}/\\pi_{C}) = \\ln(1) = 0$, and the threshold simplifies to the midpoint $\\tau = \\frac{\\mu_{U} + \\mu_{C}}{2}$, as expected. If one class has a higher prior probability, the threshold shifts away from that class's mean to reduce misclassifications of the more probable class.",
            "answer": "$$\\boxed{\\frac{\\mu_{U} + \\mu_{C}}{2} + \\frac{\\sigma^{2}}{\\mu_{C} - \\mu_{U}} \\ln\\left(\\frac{\\pi_{U}}{\\pi_{C}}\\right)}$$"
        }
    ]
}