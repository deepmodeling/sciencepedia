{
    "hands_on_practices": [
        {
            "introduction": "Nearest neighbor resampling is the simplest method for changing a raster's grid, valued for its ability to preserve the original data values without creating new ones. However, its discontinuous, blocky nature can introduce subtle but significant geometric distortions. This exercise reveals how a systematic offset between source and target grids can lead to a consistent positional bias, a critical issue in applications where boundary location accuracy is paramount, such as monitoring deforestation or urban expansion .",
            "id": "3842123",
            "problem": "A source image on a Cartesian grid is modeled as samples of a piecewise-constant scalar field acquired at pixel centers on the integer lattice $\\mathbb{Z}^{2}$. Each source pixel with index $(i,j) \\in \\mathbb{Z}^{2}$ has its center at $(x,y)=(i,j)$ and its spatial support as $[i-\\tfrac{1}{2},i+\\tfrac{1}{2}) \\times [j-\\tfrac{1}{2},j+\\tfrac{1}{2})$. A reprojection is performed to a target grid whose pixel centers map into the source coordinate system as $(x_{t},y_{t})=(m+\\delta,n+\\delta)$ for each target pixel index $(m,n) \\in \\mathbb{Z}^{2}$, with a fixed subpixel offset $\\delta=0.49$. Nearest neighbor resampling is used, defined as choosing the source pixel whose center is closest in Euclidean distance to $(x_{t},y_{t})$, equivalently $(i_{s},j_{s})=(\\operatorname{round}(x_{t}),\\operatorname{round}(y_{t}))$, where $\\operatorname{round}(\\cdot)$ denotes rounding to the nearest integer (ties do not occur here because $\\delta \\neq 0.5$).\n  \n1) Starting from these definitions, derive the mapping $(m,n) \\mapsto (i_{s},j_{s})$ symbolically for a generic target pixel $(m,n)$ under nearest neighbor resampling with $\\delta=0.49$.\n\n2) To assess positional bias at a sharp boundary, consider the one-dimensional slice along the $x$-axis of a binary field $f(x)$ that is $0$ for $x<0$ and $1$ for $x \\ge 0$. Using only nearest neighbor resampling under the same offset $\\delta=0.49$, the target pixel at index $m \\in \\mathbb{Z}$ samples the source at $x_{t}=m+\\delta$ and is assigned label $\\ell_{m}=f\\!\\left(\\operatorname{round}(x_{t})\\right)$. Define the observed boundary location along $x$ as the midpoint between the centers of the last target pixel labeled $0$ and the first target pixel labeled $1$. Compute the signed bias of this observed boundary location relative to the true boundary at $x=0$, expressed in pixel units. Provide the exact value as a single decimal number (no rounding instruction is needed).",
            "solution": "The user-provided problem is assessed to be valid. It is scientifically grounded in the principles of digital image processing and remote sensing, well-posed with all necessary information provided for a unique solution, and stated objectively. The problem proceeds in two parts.\n\nPart 1: Derivation of the symbolic mapping\nThe problem defines the reprojection of a target pixel with index $(m,n)$ into the source coordinate system as $(x_t, y_t) = (m+\\delta, n+\\delta)$, where $\\delta=0.49$.\nThe nearest neighbor resampling rule states that the assigned source pixel index $(i_s, j_s)$ is determined by rounding the target coordinates to the nearest integer:\n$$ (i_s, j_s) = (\\operatorname{round}(x_t), \\operatorname{round}(y_t)) $$\nSubstituting the expression for $(x_t, y_t)$, we get:\n$$ (i_s, j_s) = (\\operatorname{round}(m+\\delta), \\operatorname{round}(n+\\delta)) $$\nGiven $\\delta=0.49$ and that $m$ and $n$ are integers, we analyze the rounding operation for the $x$-coordinate. The function $\\operatorname{round}(z)$ finds the integer $k$ that minimizes the distance $|z-k|$. For the coordinate $x_t=m+0.49$:\nThe distance to the integer $m$ is $|(m+0.49) - m| = |0.49| = 0.49$.\nThe distance to the next integer $m+1$ is $|(m+0.49) - (m+1)| = |-0.51| = 0.51$.\nSince $0.49 < 0.51$, the nearest integer to $m+0.49$ is $m$.\nTherefore, $\\operatorname{round}(m+0.49) = m$.\nBy identical reasoning for the $y$-coordinate, $\\operatorname{round}(n+0.49) = n$.\nSubstituting these results back into the mapping equation gives:\n$$ (i_s, j_s) = (m, n) $$\nThis shows that for the given subpixel offset $\\delta=0.49$, the nearest neighbor resampling algorithm maps each target pixel $(m,n)$ to the source pixel with the identical index $(m,n)$.\n\nPart 2: Computation of the boundary bias\nThis part considers a one-dimensional slice along the $x$-axis. The true boundary is located at $x=0$, defined by the binary field $f(x)$:\n$$ f(x) = \\begin{cases} 0 & \\text{if } x < 0 \\\\ 1 & \\text{if } x \\ge 0 \\end{cases} $$\nA target pixel at index $m \\in \\mathbb{Z}$ has its center mapped to the source coordinate $x_t = m + \\delta = m + 0.49$. The label assigned to this target pixel, $\\ell_m$, is determined by applying nearest neighbor resampling, which means evaluating the field $f(x)$ at the center of the nearest source pixel. The coordinate of the center of the nearest source pixel is found by $\\operatorname{round}(x_t)$.\n$$ \\ell_m = f(\\operatorname{round}(x_t)) = f(\\operatorname{round}(m+0.49)) $$\nFrom Part 1, we established that $\\operatorname{round}(m+0.49) = m$ for any integer $m$. Thus, the label for the target pixel $m$ is:\n$$ \\ell_m = f(m) $$\nUsing the definition of $f(x)$, we can find the labels for integer values of $m$:\nFor $m < 0$ (e.g., $m=-1, -2, \\dots$), $\\ell_m = f(m) = 0$.\nFor $m \\ge 0$ (e.g., $m=0, 1, \\dots$), $\\ell_m = f(m) = 1$.\nThe transition from label $0$ to label $1$ occurs between target pixel $m=-1$ and target pixel $m=0$.\nThe last target pixel labeled $0$ is at index $m_{last\\_0} = -1$.\nThe first target pixel labeled $1$ is at index $m_{first\\_1} = 0$.\n\nThe problem defines the observed boundary location, $x_{obs}$, as the midpoint between the centers of these two target pixels, as located in the source coordinate system.\nThe location of the center of target pixel $m$ in the source coordinate system is $m+\\delta$.\nLocation of the center of pixel $m_{last\\_0} = -1$: $x_{t,-1} = -1 + \\delta$.\nLocation of the center of pixel $m_{first\\_1} = 0$: $x_{t,0} = 0 + \\delta = \\delta$.\n\nThe observed boundary location is the midpoint:\n$$ x_{obs} = \\frac{x_{t,-1} + x_{t,0}}{2} = \\frac{(-1 + \\delta) + \\delta}{2} = \\frac{2\\delta - 1}{2} = \\delta - \\frac{1}{2} $$\nThe signed bias is the difference between the observed boundary location and the true boundary location, $x_{true}=0$.\n$$ \\text{Bias} = x_{obs} - x_{true} = \\left(\\delta - \\frac{1}{2}\\right) - 0 = \\delta - 0.5 $$\nSubstituting the given value $\\delta = 0.49$:\n$$ \\text{Bias} = 0.49 - 0.5 = -0.01 $$\nThe bias is $-0.01$ pixel units, indicating a slight shift of the observed boundary in the negative direction relative to the true boundary.",
            "answer": "$$\n\\boxed{-0.01}\n$$"
        },
        {
            "introduction": "While interpolation methods like bilinear and cubic convolution produce visually smooth results, they are fundamentally designed for continuous fields like temperature or reflectance. A common but serious mistake is to apply them to discrete, categorical data such as land cover maps. This practice quantifies the resulting misclassification error, demonstrating how interpolation artificially \"smoothes\" sharp class boundaries and leads to incorrect labeling, a crucial lesson for ensuring the integrity of thematic map analysis .",
            "id": "3842075",
            "problem": "A binary categorical land-cover map is defined on the continuous plane by the indicator function $I(x,y)$ that equals $1$ in class $A$ and $0$ in class $B$. Consider a simple boundary geometry in which the true boundary within a single original pixel cell coincides with a straight line parallel to the $y$-axis, so that within the unit square $[0,1]\\times[0,1]$ the true class is $A$ if and only if $x \\ge s$ for some offset $s \\in (0,1)$. The four original pixel-center samples are located at the corners $(0,0)$, $(1,0)$, $(0,1)$, and $(1,1)$, and each stores the categorical value $I(x,y)\\in\\{0,1\\}$ at its location.\n\nA target grid is produced by resampling this categorical map with bilinear interpolation, followed by hard assignment to the nearest label: the bilinear interpolant $b(x,y)$ is evaluated at every point $(x,y)$ within $[0,1]\\times[0,1]$, and the resampled label $\\hat{I}(x,y)$ is set to $1$ if $b(x,y) \\ge 0.5$ and to $0$ otherwise. Define the within-cell misclassification rate as the area fraction of $[0,1]\\times[0,1]$ on which $\\hat{I}(x,y)\\neq I(x,y)$.\n\nAssume that the registration between the true boundary and the original grid is random so that $S$ is uniformly distributed on $(0,1)$, representing the unknown offset of the vertical boundary within a generic unit cell. Under the foregoing assumptions and using only first principles and standard definitions of bilinear interpolation and area measure, derive the expected misclassification rate $\\mathbb{E}[\\text{misclassification rate}]$ with respect to $S$. Express your final answer as an exact, dimensionless fraction (no units, no percentage sign).",
            "solution": "The problem is first validated and found to be well-posed, scientifically grounded, and objective. It is a formalizable problem in the domain of digital image processing and remote sensing, with all necessary components defined.\n\nThe solution proceeds as follows.\n\nFirst, we determine the values of the indicator function $I(x,y)$ at the four corner sample points of the unit square $[0,1] \\times [0,1]$. These points are $(0,0)$, $(1,0)$, $(0,1)$, and $(1,1)$. The indicator function is defined as $I(x,y) = 1$ if $x \\ge s$ and $I(x,y) = 0$ if $x < s$, where $s$ is the boundary offset and is known to be in the interval $(0,1)$.\n\nLet $v_{ij} = I(i,j)$ for $i,j \\in \\{0,1\\}$ be the values at the corner points.\n- At $(0,0)$, the x-coordinate is $x=0$. Since $s \\in (0,1)$, we have $0 < s$. Therefore, $v_{00} = I(0,0) = 0$.\n- At $(0,1)$, the x-coordinate is $x=0$. Similarly, $0 < s$, so $v_{01} = I(0,1) = 0$.\n- At $(1,0)$, the x-coordinate is $x=1$. Since $s \\in (0,1)$, we have $1 > s$. Therefore, $v_{10} = I(1,0) = 1$.\n- At $(1,1)$, the x-coordinate is $x=1$. Similarly, $1 > s$, so $v_{11} = I(1,1) = 1$.\n\nNotably, for any choice of $s \\in (0,1)$, the corner values are fixed: $v_{00}=0$, $v_{01}=0$, $v_{10}=1$, and $v_{11}=1$.\n\nSecond, we write the expression for the bilinear interpolant $b(x,y)$ within the unit square. The general formula is:\n$$b(x,y) = v_{00}(1-x)(1-y) + v_{10}x(1-y) + v_{01}(1-x)y + v_{11}xy$$\nSubstituting the determined corner values:\n$$b(x,y) = 0 \\cdot (1-x)(1-y) + 1 \\cdot x(1-y) + 0 \\cdot (1-x)y + 1 \\cdot xy$$\n$$b(x,y) = x(1-y) + xy$$\n$$b(x,y) = x - xy + xy = x$$\nThe bilinear interpolant simplifies to just the x-coordinate, $b(x,y)=x$.\n\nThird, we determine the resampled categorical map $\\hat{I}(x,y)$. The rule is to assign the label $1$ if $b(x,y) \\ge 0.5$ and $0$ otherwise. Since $b(x,y)=x$, this becomes:\n$$\\hat{I}(x,y) = \\begin{cases} 1 & \\text{if } x \\ge 0.5 \\\\ 0 & \\text{if } x < 0.5 \\end{cases}$$\nThis result indicates that the resampling process creates a new boundary at the fixed location $x=0.5$, regardless of the true boundary's position $s$.\n\nFourth, we calculate the misclassification rate for a fixed boundary offset $s$. The misclassification rate is the area of the region within the unit square where the true map $I(x,y)$ and the resampled map $\\hat{I}(x,y)$ do not match. Let this rate be denoted $M(s)$.\nThe total area of the unit cell is $1$, so the rate is equal to the misclassified area.\n- The true map has a boundary at $x=s$.\n- The resampled map has a boundary at $x=0.5$.\nMisclassification occurs in the vertical strip between $x=s$ and $x=0.5$. The width of this strip is $|s-0.5|$, and its height is $1$.\nThe area of misclassification is thus $|s-0.5| \\times 1 = |s-0.5|$.\nSo, the misclassification rate as a function of $s$ is:\n$$M(s) = |s - 0.5| = \\left|s - \\frac{1}{2}\\right|$$\n\nFinally, we compute the expected misclassification rate, $\\mathbb{E}[M(S)]$, where the boundary offset $S$ is a random variable uniformly distributed on $(0,1)$. The probability density function of $S$ is $f_S(s)=1$ for $s \\in (0,1)$ and $0$ otherwise.\nThe expectation is given by the integral of $M(s)$ over its probability distribution:\n$$\\mathbb{E}[M(S)] = \\int_0^1 M(s) f_S(s) ds = \\int_0^1 \\left|s - \\frac{1}{2}\\right| ds$$\nTo evaluate this integral, we split the domain at the point where the argument of the absolute value changes sign, which is $s=1/2$:\n$$\\mathbb{E}[M(S)] = \\int_0^{1/2} \\left(\\frac{1}{2} - s\\right) ds + \\int_{1/2}^1 \\left(s - \\frac{1}{2}\\right) ds$$\nWe evaluate each integral separately.\nFor the first integral:\n$$ \\int_0^{1/2} \\left(\\frac{1}{2} - s\\right) ds = \\left[ \\frac{1}{2}s - \\frac{s^2}{2} \\right]_0^{1/2} = \\left(\\frac{1}{2}\\cdot\\frac{1}{2} - \\frac{(1/2)^2}{2}\\right) - (0) = \\frac{1}{4} - \\frac{1/4}{2} = \\frac{1}{4} - \\frac{1}{8} = \\frac{1}{8} $$\nFor the second integral:\n$$ \\int_{1/2}^1 \\left(s - \\frac{1}{2}\\right) ds = \\left[ \\frac{s^2}{2} - \\frac{1}{2}s \\right]_{1/2}^1 = \\left(\\frac{1^2}{2} - \\frac{1}{2}\\cdot1\\right) - \\left(\\frac{(1/2)^2}{2} - \\frac{1}{2}\\cdot\\frac{1}{2}\\right) = \\left(\\frac{1}{2} - \\frac{1}{2}\\right) - \\left(\\frac{1/4}{2} - \\frac{1}{4}\\right) = 0 - \\left(\\frac{1}{8} - \\frac{1}{4}\\right) = -\\left(-\\frac{1}{8}\\right) = \\frac{1}{8} $$\nThe total expected misclassification rate is the sum of these two results:\n$$\\mathbb{E}[M(S)] = \\frac{1}{8} + \\frac{1}{8} = \\frac{2}{8} = \\frac{1}{4}$$",
            "answer": "$$\\boxed{\\frac{1}{4}}$$"
        },
        {
            "introduction": "Cubic convolution represents a significant step up in quality from bilinear interpolation, producing a continuously differentiable ($C^1$) surface that avoids the artificial corners present in linearly interpolated data. This is achieved by using a more complex, piecewise-cubic kernel whose shape is carefully constrained. This advanced problem guides you through the first-principles construction of this kernel, providing a deep appreciation for the mathematical underpinnings that enable high-fidelity image resampling and superior aesthetic and analytical results .",
            "id": "3842069",
            "problem": "A one-dimensional scanline from a passive optical remote sensing instrument is resampled from its native grid with sample centers at integer coordinates $i \\in \\mathbb{Z}$ to a finer grid. Let the underlying scene radiance be sampled at four consecutive native grid locations with values $f_{-1}=5$, $f_{0}=10$, $f_{1}=15$, and $f_{2}=20$ (in arbitrary consistent units). Consider estimating the radiance at the finer-grid location $x=0.25$, measured in native-pixel units to the right of the native grid point at $x=0$.\n\nUsing the well-tested cubic convolution resampling framework, assume a stationary, symmetric, compactly supported, piecewise-cubic reconstruction kernel $h(x)$ that:\n- has support on $[-2,2]$,\n- is even, so $h(x)=h(-x)$,\n- satisfies the interpolation conditions $h(0)=1$ and $h(k)=0$ for integer $k \\in \\{\\pm 1,\\pm 2,\\pm 3,\\dots\\}$ within the support,\n- is continuously differentiable ($C^{1}$) on $\\mathbb{R}$,\n- depends on a single tension parameter $a \\in \\mathbb{R}$ that governs the third-order terms on each piece, with the commonly used value $a=-0.5$ taken for this problem.\n\nStarting from these properties, construct $h(x)$ on $[0,1)$ and $[1,2)$ as cubic polynomials with undetermined coefficients, enforce the above constraints to determine those coefficients in terms of $a$, and then, using $a=-0.5$, compute the cubic convolution estimate $\\hat{f}_{\\mathrm{cc}}(0.25)$ at $x=0.25$ from the four samples $f_{-1}, f_{0}, f_{1}, f_{2}$. In parallel, form the bilinear (one-dimensional linear) interpolation estimate $\\hat{f}_{\\mathrm{lin}}(0.25)$ from $f_{0}$ and $f_{1}$, and briefly compare the smoothness of cubic convolution versus bilinear interpolation in terms of continuity of derivatives across integer grid boundaries.\n\nReport, as your final numerical answer, the cubic convolution estimate $\\hat{f}_{\\mathrm{cc}}(0.25)$. No rounding is required, and no units should be included in the final numeric answer.",
            "solution": "The problem is first validated against the specified criteria.\n\n### Step 1: Extract Givens\n- **Discrete Sampled Data**: Four consecutive native grid samples are provided: $f_{-1}=5$, $f_{0}=10$, $f_{1}=15$, and $f_{2}=20$.\n- **Target Location**: The radiance is to be estimated at $x=0.25$, measured in native-pixel units relative to the grid point at $x=0$.\n- **Reconstruction Kernel Properties ($h(x)$)**:\n    - **Type**: Stationary, symmetric, compactly supported, piecewise-cubic reconstruction kernel.\n    - **Support**: $[-2, 2]$.\n    - **Symmetry**: Even function, $h(x)=h(-x)$.\n    - **Interpolation Conditions**: $h(0)=1$ and $h(k)=0$ for any non-zero integer $k$ within the support, which implies $h(\\pm 1)=0$ and $h(\\pm 2)=0$.\n    - **Smoothness**: Continuously differentiable ($C^1$) on $\\mathbb{R}$.\n    - **Parameterization**: Depends on a single parameter $a$, with the specific value $a=-0.5$.\n- **Tasks**:\n    1.  Construct the kernel $h(x)$ for $x \\in [0, 2)$ by determining its polynomial coefficients in terms of $a$.\n    2.  Compute the cubic convolution estimate $\\hat{f}_{\\mathrm{cc}}(0.25)$ using $a=-0.5$.\n    3.  Compute the bilinear (1D linear) interpolation estimate $\\hat{f}_{\\mathrm{lin}}(0.25)$.\n    4.  Compare the smoothness of the two methods.\n    5.  Report $\\hat{f}_{\\mathrm{cc}}(0.25)$ as the final answer.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is well-grounded in signal processing and remote sensing. Cubic convolution is a standard and widely used algorithm for image resampling. The properties described for the kernel $h(x)$ correspond to the well-known Keys family of cubic convolution kernels. The value $a=-0.5$ corresponds to the kernel that produces Catmull-Rom splines, which is a common choice.\n- **Well-Posed**: The problem is well-posed. The constraints provided are sufficient to uniquely determine the kernel $h(x)$ and, consequently, the interpolated value. The data and target location are specified precisely.\n- **Objective**: The problem is stated in precise, objective, and quantitative terms.\n\n### Step 3: Verdict and Action\nThe problem is valid. It is a standard scientific problem that is complete, consistent, and solvable. A full solution will be provided.\n\n### Solution Derivation\n\n**1. Construction of the Cubic Convolution Kernel $h(x)$**\n\nThe kernel $h(x)$ is a piecewise-cubic function. Due to its even symmetry, $h(x)=h(-x)$, we only need to define it for $x \\ge 0$. Its support is $[-2, 2]$, so $h(x)=0$ for $|x| \\ge 2$. We need to find the expressions for two cubic polynomials, one on the interval $[0, 1]$ and another on $[1, 2]$.\n\nLet's define the two pieces as:\n$h(x) = h_1(x) = A_1 x^3 + B_1 x^2 + C_1 x + D_1$ for $x \\in [0, 1]$\n$h(x) = h_2(x) = A_2 x^3 + B_2 x^2 + C_2 x + D_2$ for $x \\in [1, 2]$\n\nWe apply the given constraints to determine the eight coefficients.\n- **On $[0, 1]$:**\n    1.  $h(0)=1 \\implies h_1(0) = D_1 = 1$.\n    2.  Due to even symmetry, the derivative at the origin must be zero: $h'(0)=0$.\n        $h'_1(x) = 3A_1 x^2 + 2B_1 x + C_1$. At $x=0$, this gives $h'_1(0) = C_1 = 0$.\n    3.  $h(1)=0 \\implies h_1(1) = A_1 + B_1 + C_1 + D_1 = 0$. Substituting $C_1=0$ and $D_1=1$, we get $A_1 + B_1 + 1 = 0$, so $B_1 = -A_1 - 1$.\n\n- **Continuity at $x=1$:**\n    4.  $h_1(1) = h_2(1) = 0$. For $h_2$, this means $A_2 + B_2 + C_2 + D_2 = 0$.\n    5.  The kernel is $C^1$, so derivatives must match: $h'_1(1) = h'_2(1)$. This derivative value is the parameter that can be chosen, and it is standard practice to set $h'(1) = a$.\n        $h'_1(1) = 3A_1(1)^2 + 2B_1(1) + C_1 = 3A_1 + 2B_1 = a$.\n        Substituting $B_1 = -A_1 - 1$: $3A_1 + 2(-A_1 - 1) = a \\implies A_1 - 2 = a \\implies A_1 = a+2$.\n        Then $B_1 = -(a+2) - 1 = -a-3$.\n        This fully defines the kernel on $[0, 1]$:\n        $h_1(x) = (a+2)x^3 - (a+3)x^2 + 1$.\n\n- **On $[1, 2]$:**\n    We must also have $h'_2(1)=a$, so $3A_2(1)^2 + 2B_2(1) + C_2 = 3A_2 + 2B_2 + C_2 = a$.\n    6.  $h(2)=0 \\implies h_2(2) = 8A_2 + 4B_2 + 2C_2 + D_2 = 0$.\n    7.  For smoothness at the edge of the support, $h(x)$ must approach $0$ with a zero derivative. So, $h'(2)=0$.\n        $h'_2(x) = 3A_2 x^2 + 2B_2 x + C_2$. So $h'_2(2) = 12A_2 + 4B_2 + C_2 = 0$.\n\nWe now have a system of four linear equations for the coefficients $A_2, B_2, C_2, D_2$:\n(i) $A_2 + B_2 + C_2 + D_2 = 0$\n(ii) $8A_2 + 4B_2 + 2C_2 + D_2 = 0$\n(iii) $3A_2 + 2B_2 + C_2 = a$\n(iv) $12A_2 + 4B_2 + C_2 = 0$\n\nFrom (iv), $C_2 = -12A_2 - 4B_2$. Substituting into (iii):\n$3A_2 + 2B_2 + (-12A_2 - 4B_2) = a \\implies -9A_2 - 2B_2 = a \\implies B_2 = -\\frac{9}{2}A_2 - \\frac{a}{2}$.\nSubtracting (i) from (ii): $7A_2 + 3B_2 + C_2 = 0$.\nSubstitute expressions for $B_2$ and $C_2$:\n$7A_2 + 3(-\\frac{9}{2}A_2 - \\frac{a}{2}) + (-12A_2 - 4B_2) = 0$\n$7A_2 - \\frac{27}{2}A_2 - \\frac{3a}{2} - 12A_2 - 4(-\\frac{9}{2}A_2 - \\frac{a}{2}) = 0$\n$(7 - 13.5 - 12)A_2 - 1.5a + 18A_2 + 2a = 0$\n$(-18.5)A_2 - 1.5a + 18A_2 + 2a = 0$\n$-0.5A_2 + 0.5a = 0 \\implies A_2 = a$.\n\nThis result aligns with the problem's hint that $a$ \"governs the third-order terms.\" With $A_2=a$:\n$B_2 = -\\frac{9}{2}a - \\frac{a}{2} = -5a$.\n$C_2 = -12a - 4(-5a) = 8a$.\n$D_2 = -A_2 - B_2 - C_2 = -a - (-5a) - 8a = -4a$.\nThis defines the kernel on $[1, 2]$:\n$h_2(x) = ax^3 - 5ax^2 + 8ax - 4a = a(x^3 - 5x^2 + 8x - 4)$.\n\nFor the problem, we use $a=-0.5$:\nFor $|x| \\in [0, 1)$: $h(x) = (-0.5+2)|x|^3 - (-0.5+3)|x|^2 + 1 = 1.5|x|^3 - 2.5|x|^2 + 1$.\nFor $|x| \\in [1, 2)$: $h(x) = -0.5(|x|^3 - 5|x|^2 + 8|x| - 4) = -0.5|x|^3 + 2.5|x|^2 - 4|x| + 2$.\n\n**2. Calculation of the Cubic Convolution Estimate $\\hat{f}_{\\mathrm{cc}}(0.25)$**\nThe estimate is given by the convolution sum: $\\hat{f}_{\\mathrm{cc}}(x) = \\sum_{i} f_i h(x-i)$.\nFor $x=0.25$, the kernel $h(0.25-i)$ is non-zero only for $i \\in \\{-1, 0, 1, 2\\}$.\n$\\hat{f}_{\\mathrm{cc}}(0.25) = f_{-1}h(0.25-(-1)) + f_{0}h(0.25-0) + f_{1}h(0.25-1) + f_{2}h(0.25-2)$\n$\\hat{f}_{\\mathrm{cc}}(0.25) = f_{-1}h(1.25) + f_{0}h(0.25) + f_{1}h(-0.75) + f_{2}h(-1.75)$.\nUsing $h(-x)=h(x)$:\n$\\hat{f}_{\\mathrm{cc}}(0.25) = f_{-1}h(1.25) + f_{0}h(0.25) + f_{1}h(0.75) + f_{2}h(1.75)$.\n\nWe compute the required kernel values for $x \\ge 0$:\n- $h(0.25)$, where $|x| = 0.25 \\in [0, 1)$: $h(0.25) = 1.5(0.25)^3 - 2.5(0.25)^2 + 1 = \\frac{3}{2}(\\frac{1}{64}) - \\frac{5}{2}(\\frac{1}{16}) + 1 = \\frac{3}{128} - \\frac{20}{128} + \\frac{128}{128} = \\frac{111}{128}$.\n- $h(0.75)$, where $|x| = 0.75 \\in [0, 1)$: $h(0.75) = 1.5(0.75)^3 - 2.5(0.75)^2 + 1 = \\frac{3}{2}(\\frac{27}{64}) - \\frac{5}{2}(\\frac{9}{16}) + 1 = \\frac{81}{128} - \\frac{180}{128} + \\frac{128}{128} = \\frac{29}{128}$.\n- $h(1.25)$, where $|x| = 1.25 \\in [1, 2)$: $h(1.25) = -0.5(1.25)^3 + 2.5(1.25)^2 - 4(1.25) + 2 = -\\frac{1}{2}(\\frac{125}{64}) + \\frac{5}{2}(\\frac{25}{16}) - 5 + 2 = -\\frac{125}{128} + \\frac{500}{128} - \\frac{384}{128} = \\frac{-9}{128}$.\n- $h(1.75)$, where $|x| = 1.75 \\in [1, 2)$: $h(1.75) = -0.5(1.75)^3 + 2.5(1.75)^2 - 4(1.75) + 2 = -\\frac{1}{2}(\\frac{343}{64}) + \\frac{5}{2}(\\frac{49}{16}) - 7 + 2 = -\\frac{343}{128} + \\frac{980}{128} - \\frac{640}{128} = \\frac{-3}{128}$.\n\nNow, substitute these values and the given data ($f_{-1}=5, f_{0}=10, f_{1}=15, f_{2}=20$):\n$\\hat{f}_{\\mathrm{cc}}(0.25) = 5\\left(\\frac{-9}{128}\\right) + 10\\left(\\frac{111}{128}\\right) + 15\\left(\\frac{29}{128}\\right) + 20\\left(\\frac{-3}{128}\\right)$\n$\\hat{f}_{\\mathrm{cc}}(0.25) = \\frac{1}{128} \\left( -45 + 1110 + 435 - 60 \\right)$\n$\\hat{f}_{\\mathrm{cc}}(0.25) = \\frac{1}{128} \\left( 1545 - 105 \\right) = \\frac{1440}{128}$\n$\\hat{f}_{\\mathrm{cc}}(0.25) = \\frac{720}{64} = \\frac{360}{32} = \\frac{180}{16} = \\frac{90}{8} = \\frac{45}{4} = 11.25$.\n\n**3. Calculation of the Bilinear Interpolation Estimate $\\hat{f}_{\\mathrm{lin}}(0.25)$**\nBilinear interpolation in one dimension is linear interpolation. To estimate the value at $x=0.25$, which lies between the grid points $i=0$ and $i=1$, we use the values $f_0$ and $f_1$.\n$\\hat{f}_{\\mathrm{lin}}(0.25) = f_0 \\cdot (1-0.25) + f_1 \\cdot (0.25)$\n$\\hat{f}_{\\mathrm{lin}}(0.25) = 10 \\cdot (0.75) + 15 \\cdot (0.25) = 7.5 + 3.75 = 11.25$.\nThe result is identical to the cubic convolution estimate. This occurs because the input data $f_i = 5(i+2)$ lies on a perfect line, and the cubic convolution kernel with $a=-0.5$ (Catmull-Rom) is known to be first-order accurate, meaning it exactly reproduces linear functions.\n\n**4. Smoothness Comparison**\n- **Bilinear (Linear) Interpolation**: The reconstructed function is continuous ($C^0$) but its first derivative is discontinuous at the integer grid points. The function is composed of straight line segments, resulting in sharp corners at the sample points. The first derivative is piecewise constant.\n- **Cubic Convolution**: The reconstruction kernel $h(x)$ was constructed to be $C^1$. The reconstructed function $\\hat{f}_{\\mathrm{cc}}(x) = \\sum_i f_i h(x-i)$ is a sum of scaled and shifted versions of this $C^1$ kernel. Therefore, $\\hat{f}_{\\mathrm{cc}}(x)$ is also a $C^1$ function. Its first derivative, $\\hat{f}'_{\\mathrm{cc}}(x)=\\sum_i f_i h'(x-i)$, is continuous everywhere. This results in a much smoother reconstruction than linear interpolation, avoiding the artificial corners.",
            "answer": "$$\\boxed{11.25}$$"
        }
    ]
}