{
    "hands_on_practices": [
        {
            "introduction": "This first practice is a cornerstone of statistical classification. We will move from raw training data to a functional Maximum Likelihood Classifier by estimating the statistical parameters that define each land-cover class. This exercise  will give you hands-on experience with the Maximum Likelihood Estimators (MLEs) for the mean vector and covariance matrix of a Gaussian distribution, which form the statistical 'signature' of a class.",
            "id": "3826538",
            "problem": "A multispectral satellite image provides three calibrated reflectance bands: blue, red, and near-infrared (NIR). Consider two land-cover classes, water and vegetation, to be modeled by class-conditional trivariate normal distributions with unknown mean vectors and covariance matrices. You are provided training samples for each class, collected from homogeneous regions and preprocessed to remove atmospheric and illumination artifacts. Let the water class training samples be\n$$\n(0.05,\\,0.03,\\,0.02),\\quad\n(0.06,\\,0.03,\\,0.02),\\quad\n(0.04,\\,0.03,\\,0.02),\\quad\n(0.05,\\,0.04,\\,0.02),\\quad\n(0.05,\\,0.02,\\,0.02),\\quad\n(0.05,\\,0.03,\\,0.03),\\quad\n(0.05,\\,0.03,\\,0.01),\n$$\nand the vegetation class training samples be\n$$\n(0.06,\\,0.08,\\,0.45),\\quad\n(0.08,\\,0.08,\\,0.45),\\quad\n(0.04,\\,0.08,\\,0.45),\\quad\n(0.06,\\,0.11,\\,0.45),\\quad\n(0.06,\\,0.05,\\,0.45),\\quad\n(0.06,\\,0.08,\\,0.50),\\quad\n(0.06,\\,0.08,\\,0.40).\n$$\nA new pixel with reflectance vector\n$$\n\\mathbf{x}=(0.052,\\,0.031,\\,0.022)\n$$\nis to be classified by maximum likelihood classification (ignore any class priors and misclassification costs). Starting from the probability density function of the multivariate normal distribution and the maximum likelihood principle, derive the maximum likelihood estimators for the mean and covariance of each class, estimate these parameters from the given training data, and then compute the exact log-likelihood ratio\n$$\n\\Lambda(\\mathbf{x})=\\ln p(\\mathbf{x}\\mid \\text{water})-\\ln p(\\mathbf{x}\\mid \\text{vegetation}),\n$$\nevaluated at the estimated parameters.\n\nExpress the final value of $\\Lambda(\\mathbf{x})$ as a dimensionless real number and round your answer to four significant figures.",
            "solution": "The problem asks us to classify a new pixel using Maximum Likelihood Classification, which involves calculating the log-likelihood ratio of the pixel's reflectance vector belonging to the 'water' class versus the 'vegetation' class. The class-conditional probability distributions are modeled as trivariate normal distributions.\n\n**Step 1: Derive the Maximum Likelihood Estimators (MLEs)**\nLet a set of $N$ i.i.d. samples $\\{\\mathbf{x}_1, \\dots, \\mathbf{x}_N\\}$ be drawn from a $d$-dimensional multivariate normal distribution $N(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$. The PDF is:\n$$p(\\mathbf{x} | \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) = \\frac{1}{(2\\pi)^{d/2} |\\boldsymbol{\\Sigma}|^{1/2}} \\exp\\left(-\\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu})^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x}-\\boldsymbol{\\mu})\\right)$$\nThe log-likelihood function for the $N$ samples is:\n$$\n\\ln L(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) = \\sum_{k=1}^{N} \\ln p(\\mathbf{x}_k | \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) = -\\frac{Nd}{2}\\ln(2\\pi) - \\frac{N}{2}\\ln|\\boldsymbol{\\Sigma}| - \\frac{1}{2}\\sum_{k=1}^{N} (\\mathbf{x}_k-\\boldsymbol{\\mu})^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x}_k-\\boldsymbol{\\mu})\n$$\nTo find the MLE for the mean $\\boldsymbol{\\mu}$, we take the gradient with respect to $\\boldsymbol{\\mu}$ and set it to zero:\n$$\n\\nabla_{\\boldsymbol{\\mu}} \\ln L = \\sum_{k=1}^{N} \\boldsymbol{\\Sigma}^{-1}(\\mathbf{x}_k - \\boldsymbol{\\mu}) = \\boldsymbol{0} \\implies \\sum_{k=1}^{N} (\\mathbf{x}_k - \\boldsymbol{\\mu}) = \\boldsymbol{0} \\implies N\\hat{\\boldsymbol{\\mu}} = \\sum_{k=1}^{N} \\mathbf{x}_k\n$$\nThis gives the MLE for the mean, which is the sample mean:\n$$\\hat{\\boldsymbol{\\mu}} = \\frac{1}{N} \\sum_{k=1}^{N} \\mathbf{x}_k$$\nTo find the MLE for the covariance $\\boldsymbol{\\Sigma}$, we differentiate $\\ln L$ with respect to $\\boldsymbol{\\Sigma}^{-1}$ and set the result to zero. This yields:\n$$\\hat{\\boldsymbol{\\Sigma}} = \\frac{1}{N} \\sum_{k=1}^{N} (\\mathbf{x}_k - \\hat{\\boldsymbol{\\mu}})(\\mathbf{x}_k - \\hat{\\boldsymbol{\\mu}})^T$$\nThis is the sample covariance matrix (with $1/N$ scaling).\n\n**Step 2: Estimate Parameters for Each Class**\nLet $\\omega_1$ be the water class and $\\omega_2$ be the vegetation class. Here, $d=3$ and $N_1=N_2=7$.\n\nFor the water class ($ \\omega_1 $):\nThe sum of the sample vectors is $\\sum_{k=1}^{7} \\mathbf{x}_{1,k} = (0.35, 0.21, 0.14)^T$.\nThe MLE of the mean is:\n$$\\hat{\\boldsymbol{\\mu}}_1 = \\frac{1}{7}(0.35, 0.21, 0.14)^T = (0.05, 0.03, 0.02)^T$$\nThe centered vectors $(\\mathbf{x}_{1,k} - \\hat{\\boldsymbol{\\mu}}_1)$ are: $(0,0,0)^T$, $(0.01,0,0)^T$, $(-0.01,0,0)^T$, $(0,0.01,0)^T$, $(0,-0.01,0)^T$, $(0,0,0.01)^T$, $(0,0,-0.01)^T$.\nThe sum of the outer products is:\n$$ \\sum_{k=1}^{7} (\\mathbf{x}_{1,k} - \\hat{\\boldsymbol{\\mu}}_1)(\\mathbf{x}_{1,k} - \\hat{\\boldsymbol{\\mu}}_1)^T = \\begin{pmatrix} 2(0.01^2) & 0 & 0 \\\\ 0 & 2(0.01^2) & 0 \\\\ 0 & 0 & 2(0.01^2) \\end{pmatrix} = \\begin{pmatrix} 0.0002 & 0 & 0 \\\\ 0 & 0.0002 & 0 \\\\ 0 & 0 & 0.0002 \\end{pmatrix} $$\nThe MLE of the covariance matrix is:\n$$ \\hat{\\boldsymbol{\\Sigma}}_1 = \\frac{1}{7} \\begin{pmatrix} 0.0002 & 0 & 0 \\\\ 0 & 0.0002 & 0 \\\\ 0 & 0 & 0.0002 \\end{pmatrix} = \\begin{pmatrix} \\frac{2}{70000} & 0 & 0 \\\\ 0 & \\frac{2}{70000} & 0 \\\\ 0 & 0 & \\frac{2}{70000} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{35000} & 0 & 0 \\\\ 0 & \\frac{1}{35000} & 0 \\\\ 0 & 0 & \\frac{1}{35000} \\end{pmatrix} $$\n\nFor the vegetation class ($ \\omega_2 $):\nThe sum of the sample vectors is $\\sum_{k=1}^{7} \\mathbf{x}_{2,k} = (0.42, 0.56, 3.15)^T$.\nThe MLE of the mean is:\n$$\\hat{\\boldsymbol{\\mu}}_2 = \\frac{1}{7}(0.42, 0.56, 3.15)^T = (0.06, 0.08, 0.45)^T$$\nThe centered vectors $(\\mathbf{x}_{2,k} - \\hat{\\boldsymbol{\\mu}}_2)$ have non-zero components: $(0.02,0,0)^T$, $(-0.02,0,0)^T$, $(0,0.03,0)^T$, $(0,-0.03,0)^T$, $(0,0,0.05)^T$, $(0,0,-0.05)^T$.\nThe sum of the outer products is:\n$$ \\sum_{k=1}^{7} (\\mathbf{x}_{2,k} - \\hat{\\boldsymbol{\\mu}}_2)(\\mathbf{x}_{2,k} - \\hat{\\boldsymbol{\\mu}}_2)^T = \\begin{pmatrix} 2(0.02^2) & 0 & 0 \\\\ 0 & 2(0.03^2) & 0 \\\\ 0 & 0 & 2(0.05^2) \\end{pmatrix} = \\begin{pmatrix} 0.0008 & 0 & 0 \\\\ 0 & 0.0018 & 0 \\\\ 0 & 0 & 0.0050 \\end{pmatrix} $$\nThe MLE of the covariance matrix is:\n$$ \\hat{\\boldsymbol{\\Sigma}}_2 = \\frac{1}{7} \\begin{pmatrix} 0.0008 & 0 & 0 \\\\ 0 & 0.0018 & 0 \\\\ 0 & 0 & 0.0050 \\end{pmatrix} = \\begin{pmatrix} \\frac{8}{70000} & 0 & 0 \\\\ 0 & \\frac{18}{70000} & 0 \\\\ 0 & 0 & \\frac{50}{70000} \\end{pmatrix} = \\begin{pmatrix} \\frac{4}{35000} & 0 & 0 \\\\ 0 & \\frac{9}{35000} & 0 \\\\ 0 & 0 & \\frac{25}{35000} \\end{pmatrix} $$\n\n**Step 3: Compute the Log-Likelihood Ratio**\nThe log-likelihood ratio $\\Lambda(\\mathbf{x})$ is given by:\n$$\n\\Lambda(\\mathbf{x}) = \\ln p(\\mathbf{x}\\mid \\hat{\\boldsymbol{\\mu}}_1, \\hat{\\boldsymbol{\\Sigma}}_1) - \\ln p(\\mathbf{x}\\mid \\hat{\\boldsymbol{\\mu}}_2, \\hat{\\boldsymbol{\\Sigma}}_2)\n$$\nSubstituting the log-PDF expression:\n$$\n\\Lambda(\\mathbf{x}) = \\left[-\\frac{d}{2}\\ln(2\\pi) - \\frac{1}{2}\\ln|\\hat{\\boldsymbol{\\Sigma}}_1| - \\frac{1}{2}D_1^2(\\mathbf{x})\\right] - \\left[-\\frac{d}{2}\\ln(2\\pi) - \\frac{1}{2}\\ln|\\hat{\\boldsymbol{\\Sigma}}_2| - \\frac{1}{2}D_2^2(\\mathbf{x})\\right]\n$$\nwhere $D_i^2(\\mathbf{x}) = (\\mathbf{x}-\\hat{\\boldsymbol{\\mu}}_i)^T \\hat{\\boldsymbol{\\Sigma}}_i^{-1} (\\mathbf{x}-\\hat{\\boldsymbol{\\mu}}_i)$ is the squared Mahalanobis distance.\nSimplifying, we get:\n$$\n\\Lambda(\\mathbf{x}) = \\frac{1}{2}\\ln\\left(\\frac{|\\hat{\\boldsymbol{\\Sigma}}_2|}{|\\hat{\\boldsymbol{\\Sigma}}_1|}\\right) - \\frac{1}{2}\\left(D_1^2(\\mathbf{x}) - D_2^2(\\mathbf{x})\\right)\n$$\nThe pixel to classify is $\\mathbf{x}=(0.052, 0.031, 0.022)^T$.\n\nFirst, compute the ratio of determinants:\n$|\\hat{\\boldsymbol{\\Sigma}}_1| = (\\frac{1}{35000})^3$\n$|\\hat{\\boldsymbol{\\Sigma}}_2| = (\\frac{4}{35000})(\\frac{9}{35000})(\\frac{25}{35000}) = \\frac{900}{35000^3}$\n$\\frac{|\\hat{\\boldsymbol{\\Sigma}}_2|}{|\\hat{\\boldsymbol{\\Sigma}}_1|} = 900$. So, the first term is $\\frac{1}{2}\\ln(900) = \\ln(\\sqrt{900}) = \\ln(30)$.\n\nNext, compute the squared Mahalanobis distances. Since the covariance matrices are diagonal, $D_i^2(\\mathbf{x}) = \\sum_{j=1}^{3} \\frac{(x_j - \\hat{\\mu}_{i,j})^2}{\\hat{\\sigma}_{i,j}^2}$.\n\nFor water ($\\omega_1$):\n$\\mathbf{x} - \\hat{\\boldsymbol{\\mu}}_1 = (0.052 - 0.05, 0.031 - 0.03, 0.022 - 0.02)^T = (0.002, 0.001, 0.002)^T$.\nThe variance is constant $\\hat{\\sigma}_{1,j}^2 = 1/35000$.\n$$ D_1^2(\\mathbf{x}) = \\frac{(0.002)^2}{1/35000} + \\frac{(0.001)^2}{1/35000} + \\frac{(0.002)^2}{1/35000} = 35000 \\times (0.000004 + 0.000001 + 0.000004) = 35000 \\times 9 \\times 10^{-6} = 0.315 $$\n\nFor vegetation ($\\omega_2$):\n$\\mathbf{x} - \\hat{\\boldsymbol{\\mu}}_2 = (0.052 - 0.06, 0.031 - 0.08, 0.022 - 0.45)^T = (-0.008, -0.049, -0.428)^T$.\nThe variances are $\\hat{\\sigma}_{2,1}^2=4/35000$, $\\hat{\\sigma}_{2,2}^2=9/35000$, $\\hat{\\sigma}_{2,3}^2=25/35000$.\n$$ D_2^2(\\mathbf{x}) = \\frac{(-0.008)^2}{4/35000} + \\frac{(-0.049)^2}{9/35000} + \\frac{(-0.428)^2}{25/35000} $$\n$$ D_2^2(\\mathbf{x}) = \\frac{0.000064 \\times 35000}{4} + \\frac{0.002401 \\times 35000}{9} + \\frac{0.183184 \\times 35000}{25} $$\n$$ D_2^2(\\mathbf{x}) = 0.56 + \\frac{84.035}{9} + \\frac{6411.44}{25} = 0.56 + 9.33722... + 256.4576 = 266.35482... $$\nNow, assemble the log-likelihood ratio:\n$$ \\Lambda(\\mathbf{x}) = \\ln(30) - \\frac{1}{2}(0.315 - 266.35482...) = \\ln(30) - \\frac{1}{2}(-266.03982...) $$\n$$ \\Lambda(\\mathbf{x}) = \\ln(30) + 133.01991... \\approx 3.401197 + 133.019911 = 136.421108... $$\nRounding to four significant figures, we get $136.4$.",
            "answer": "$$\\boxed{136.4}$$"
        },
        {
            "introduction": "Building upon parameter estimation, this exercise explores the decision-making heart of the classifier: the discriminant function. You will derive the general form of the log-discriminant function from Bayesian decision theory, incorporating prior probabilities. A key insight from this practice  is understanding how assumptions about the class covariances determine whether the decision boundary is linear or a more complex quadratic surface.",
            "id": "3826536",
            "problem": "A two-class Maximum Likelihood Classification (MLC) problem is posed for a multispectral remote sensing scene in two bands, red and near-infrared. Consider the feature vector $x = (x_{\\mathrm{R}}, x_{\\mathrm{NIR}})^{\\top}$ that represents top-of-atmosphere reflectance (unitless fraction) in the red and near-infrared bands. Suppose the class-conditional distributions for emergent vegetation (class $\\mathcal{V}$) and clear water (class $\\mathcal{W}$) are modeled as multivariate Gaussian (Normal) distributions with parameters estimated from training data:\n$$\n\\mu_{\\mathcal{V}} = \\begin{pmatrix} 0.08 \\\\ 0.45 \\end{pmatrix}, \\quad \\Sigma_{\\mathcal{V}} = \\begin{pmatrix} 0.0025 & 0 \\\\ 0 & 0.0121 \\end{pmatrix},\n$$\n$$\n\\mu_{\\mathcal{W}} = \\begin{pmatrix} 0.05 \\\\ 0.06 \\end{pmatrix}, \\quad \\Sigma_{\\mathcal{W}} = \\begin{pmatrix} 0.0016 & 0 \\\\ 0 & 0.0025 \\end{pmatrix},\n$$\nand prior probabilities\n$$\nP(\\mathcal{V}) = 0.55, \\quad P(\\mathcal{W}) = 0.45.\n$$\nAssume independence across bands is a reasonable approximation so the covariance matrices are diagonal as given. A pixel with reflectance $x_{0} = (0.07, 0.40)^{\\top}$ is observed.\n\nStarting only from Bayes decision rule for minimum probability of error classification and the multivariate Gaussian probability density function, derive the difference of the log-discriminant functions $\\Delta(x) = g_{\\mathcal{V}}(x) - g_{\\mathcal{W}}(x)$ for the general heteroscedastic case (class-specific covariance matrices), making explicit any terms that depend on $x$, the means, the covariance matrices, and the priors. Then, under the homoscedastic assumption (equal covariance across classes) with a common covariance\n$$\n\\Sigma = \\begin{pmatrix} 0.0020 & 0 \\\\ 0 & 0.0040 \\end{pmatrix},\n$$\nshow that the discriminant difference reduces to a linear function in $x$ and identify the weight vector and bias in terms of $\\mu_{\\mathcal{V}}$, $\\mu_{\\mathcal{W}}$, $\\Sigma$, and the priors.\n\nFinally, evaluate the heteroscedastic discriminant difference $\\Delta(x_{0})$ numerically using the parameters provided above for $\\Sigma_{\\mathcal{V}}$ and $\\Sigma_{\\mathcal{W}}$. Express the final value of $\\Delta(x_{0})$ as a single real number, rounded to four significant figures. The answer is unitless.",
            "solution": "The problem asks for three tasks: first, to derive the difference of the log-discriminant functions $\\Delta(x) = g_{\\mathcal{V}}(x) - g_{\\mathcal{W}}(x)$ for the general heteroscedastic case; second, to show this simplifies to a linear function for the homoscedastic case and identify the constituent terms; and third, to evaluate $\\Delta(x_{0})$ numerically for a specific observation $x_{0}$.\n\n**Part 1: Derivation of the Heteroscedastic Discriminant Difference**\n\nBayes' decision rule for minimum classification error is to assign a feature vector $x$ to the class $\\omega_i$ that has the maximum a posteriori probability, $P(\\omega_i | x)$. Using Bayes' theorem, this posterior probability is given by:\n$$\nP(\\omega_i | x) = \\frac{p(x | \\omega_i) P(\\omega_i)}{p(x)}\n$$\nwhere $p(x | \\omega_i)$ is the class-conditional probability density function, $P(\\omega_i)$ is the prior probability of class $\\omega_i$, and $p(x)$ is the evidence. Since $p(x)$ is the same for all classes, maximizing the posterior is equivalent to maximizing the product $p(x | \\omega_i) P(\\omega_i)$.\n\nFor computational convenience, we work with the logarithm of this product, as the logarithm is a monotonically increasing function. This defines the discriminant function $g_i(x)$:\n$$\ng_i(x) = \\ln\\left(p(x | \\omega_i) P(\\omega_i)\\right) = \\ln(p(x | \\omega_i)) + \\ln(P(\\omega_i))\n$$\nThe problem states that the class-conditional densities are multivariate Gaussian. For a feature vector $x$ of dimension $d$, the probability density function for class $\\omega_i$ is:\n$$\np(x | \\omega_i) = \\frac{1}{(2\\pi)^{d/2} |\\Sigma_i|^{1/2}} \\exp\\left(-\\frac{1}{2} (x - \\mu_i)^{\\top} \\Sigma_i^{-1} (x - \\mu_i)\\right)\n$$\nSubstituting this into the discriminant function expression, we get:\n$$\ng_i(x) = \\ln\\left(\\frac{1}{(2\\pi)^{d/2} |\\Sigma_i|^{1/2}}\\right) - \\frac{1}{2} (x - \\mu_i)^{\\top} \\Sigma_i^{-1} (x - \\mu_i) + \\ln(P(\\omega_i))\n$$\n$$\ng_i(x) = -\\frac{d}{2}\\ln(2\\pi) - \\frac{1}{2}\\ln(|\\Sigma_i|) - \\frac{1}{2}(x - \\mu_i)^{\\top} \\Sigma_i^{-1} (x - \\mu_i) + \\ln(P(\\omega_i))\n$$\nWe are asked to find the difference of the discriminant functions for class $\\mathcal{V}$ (vegetation) and class $\\mathcal{W}$ (water), denoted as $\\Delta(x) = g_{\\mathcal{V}}(x) - g_{\\mathcal{W}}(x)$.\n$$\ng_{\\mathcal{V}}(x) = -\\frac{d}{2}\\ln(2\\pi) - \\frac{1}{2}\\ln(|\\Sigma_{\\mathcal{V}}|) - \\frac{1}{2}(x - \\mu_{\\mathcal{V}})^{\\top} \\Sigma_{\\mathcal{V}}^{-1} (x - \\mu_{\\mathcal{V}}) + \\ln(P(\\mathcal{V}))\n$$\n$$\ng_{\\mathcal{W}}(x) = -\\frac{d}{2}\\ln(2\\pi) - \\frac{1}{2}\\ln(|\\Sigma_{\\mathcal{W}}|) - \\frac{1}{2}(x - \\mu_{\\mathcal{W}})^{\\top} \\Sigma_{\\mathcal{W}}^{-1} (x - \\mu_{\\mathcal{W}}) + \\ln(P(\\mathcal{W}))\n$$\nSubtracting $g_{\\mathcal{W}}(x)$ from $g_{\\mathcal{V}}(x)$, the common term $-\\frac{d}{2}\\ln(2\\pi)$ cancels out:\n$$\n\\Delta(x) = \\left( - \\frac{1}{2}\\ln(|\\Sigma_{\\mathcal{V}}|) - \\frac{1}{2}(x - \\mu_{\\mathcal{V}})^{\\top} \\Sigma_{\\mathcal{V}}^{-1} (x - \\mu_{\\mathcal{V}}) + \\ln(P(\\mathcal{V})) \\right) - \\left( - \\frac{1}{2}\\ln(|\\Sigma_{\\mathcal{W}}|) - \\frac{1}{2}(x - \\mu_{\\mathcal{W}})^{\\top} \\Sigma_{\\mathcal{W}}^{-1} (x - \\mu_{\\mathcal{W}}) + \\ln(P(\\mathcal{W})) \\right)\n$$\nRearranging the terms, we arrive at the general heteroscedastic discriminant difference:\n$$\n\\Delta(x) = \\frac{1}{2}\\left[ (x - \\mu_{\\mathcal{W}})^{\\top} \\Sigma_{\\mathcal{W}}^{-1} (x - \\mu_{\\mathcal{W}}) - (x - \\mu_{\\mathcal{V}})^{\\top} \\Sigma_{\\mathcal{V}}^{-1} (x - \\mu_{\\mathcal{V}}) \\right] + \\frac{1}{2}\\ln\\left(\\frac{|\\Sigma_{\\mathcal{W}}|}{|\\Sigma_{\\mathcal{V}}|}\\right) + \\ln\\left(\\frac{P(\\mathcal{V})}{P(\\mathcal{W})}\\right)\n$$\nThis expression contains a quadratic term in $x$ (since $\\Sigma_{\\mathcal{V}} \\neq \\Sigma_{\\mathcal{W}}$), as well as terms dependent on the means, covariance matrices, and prior probabilities.\n\n**Part 2: Reduction to the Linear Case (Homoscedastic Assumption)**\n\nUnder the homoscedastic assumption, the covariance matrices for all classes are equal, i.e., $\\Sigma_{\\mathcal{V}} = \\Sigma_{\\mathcal{W}} = \\Sigma$. Let's analyze the effect on $\\Delta(x)$.\nThe log-determinant term becomes:\n$$\n\\frac{1}{2}\\ln\\left(\\frac{|\\Sigma|}{|\\Sigma|}\\right) = \\frac{1}{2}\\ln(1) = 0\n$$\nTo see how the quadratic term simplifies, we expand it:\n$$\n(x - \\mu)^{\\top}\\Sigma^{-1}(x - \\mu) = x^{\\top}\\Sigma^{-1}x - 2\\mu^{\\top}\\Sigma^{-1}x + \\mu^{\\top}\\Sigma^{-1}\\mu\n$$\nSubstituting this into the first part of $\\Delta(x)$ with $\\Sigma_{\\mathcal{V}}^{-1} = \\Sigma_{\\mathcal{W}}^{-1} = \\Sigma^{-1}$:\n$$\n\\frac{1}{2}\\left[ (x^{\\top}\\Sigma^{-1}x - 2\\mu_{\\mathcal{W}}^{\\top}\\Sigma^{-1}x + \\mu_{\\mathcal{W}}^{\\top}\\Sigma^{-1}\\mu_{\\mathcal{W}}) - (x^{\\top}\\Sigma^{-1}x - 2\\mu_{\\mathcal{V}}^{\\top}\\Sigma^{-1}x + \\mu_{\\mathcal{V}}^{\\top}\\Sigma^{-1}\\mu_{\\mathcal{V}}) \\right]\n$$\nThe quadratic term in $x$, $x^{\\top}\\Sigma^{-1}x$, cancels out. The expression simplifies to:\n$$\n\\frac{1}{2}\\left[ 2\\mu_{\\mathcal{V}}^{\\top}\\Sigma^{-1}x - 2\\mu_{\\mathcal{W}}^{\\top}\\Sigma^{-1}x + \\mu_{\\mathcal{W}}^{\\top}\\Sigma^{-1}\\mu_{\\mathcal{W}} - \\mu_{\\mathcal{V}}^{\\top}\\Sigma^{-1}\\mu_{\\mathcal{V}} \\right] = (\\mu_{\\mathcal{V}} - \\mu_{\\mathcal{W}})^{\\top}\\Sigma^{-1}x - \\frac{1}{2}(\\mu_{\\mathcal{V}}^{\\top}\\Sigma^{-1}\\mu_{\\mathcal{V}} - \\mu_{\\mathcal{W}}^{\\top}\\Sigma^{-1}\\mu_{\\mathcal{W}})\n$$\nCombining this with the prior probability term, the discriminant difference reduces to a linear function of $x$:\n$$\n\\Delta(x) = (\\mu_{\\mathcal{V}} - \\mu_{\\mathcal{W}})^{\\top}\\Sigma^{-1}x - \\frac{1}{2}\\mu_{\\mathcal{V}}^{\\top}\\Sigma^{-1}\\mu_{\\mathcal{V}} + \\frac{1}{2}\\mu_{\\mathcal{W}}^{\\top}\\Sigma^{-1}\\mu_{\\mathcal{W}} + \\ln\\left(\\frac{P(\\mathcal{V})}{P(\\mathcal{W})}\\right)\n$$\nThis is a linear function of the form $\\Delta(x) = w^{\\top}x + b$, where:\nThe weight vector is $w = \\Sigma^{-1}(\\mu_{\\mathcal{V}} - \\mu_{\\mathcal{W}})$.\nThe bias term is $b = -\\frac{1}{2}\\mu_{\\mathcal{V}}^{\\top}\\Sigma^{-1}\\mu_{\\mathcal{V}} + \\frac{1}{2}\\mu_{\\mathcal{W}}^{\\top}\\Sigma^{-1}\\mu_{\\mathcal{W}} + \\ln\\left(\\frac{P(\\mathcal{V})}{P(\\mathcal{W})}\\right)$.\n\n**Part 3: Numerical Evaluation of $\\Delta(x_0)$**\n\nWe now evaluate $\\Delta(x_0)$ using the heteroscedastic formula and the given parameters for a pixel $x_0 = (0.07, 0.40)^{\\top}$.\nThe parameters are:\n$$\nx_0 = \\begin{pmatrix} 0.07 \\\\ 0.40 \\end{pmatrix}, \\quad \\mu_{\\mathcal{V}} = \\begin{pmatrix} 0.08 \\\\ 0.45 \\end{pmatrix}, \\quad \\mu_{\\mathcal{W}} = \\begin{pmatrix} 0.05 \\\\ 0.06 \\end{pmatrix}\n$$\n$$\n\\Sigma_{\\mathcal{V}} = \\begin{pmatrix} 0.0025 & 0 \\\\ 0 & 0.0121 \\end{pmatrix}, \\quad \\Sigma_{\\mathcal{W}} = \\begin{pmatrix} 0.0016 & 0 \\\\ 0 & 0.0025 \\end{pmatrix}\n$$\n$$\nP(\\mathcal{V}) = 0.55, \\quad P(\\mathcal{W}) = 0.45\n$$\nFirst, we compute the inverse and determinant of the covariance matrices. Since they are diagonal, these are straightforward.\n$$\n\\Sigma_{\\mathcal{V}}^{-1} = \\begin{pmatrix} \\frac{1}{0.0025} & 0 \\\\ 0 & \\frac{1}{0.0121} \\end{pmatrix} = \\begin{pmatrix} 400 & 0 \\\\ 0 & \\frac{10000}{121} \\end{pmatrix}\n$$\n$$\n|\\Sigma_{\\mathcal{V}}| = 0.0025 \\times 0.0121 = 0.00003025\n$$\n$$\n\\Sigma_{\\mathcal{W}}^{-1} = \\begin{pmatrix} \\frac{1}{0.0016} & 0 \\\\ 0 & \\frac{1}{0.0025} \\end{pmatrix} = \\begin{pmatrix} 625 & 0 \\\\ 0 & 400 \\end{pmatrix}\n$$\n$$\n|\\Sigma_{\\mathcal{W}}| = 0.0016 \\times 0.0025 = 0.000004\n$$\nNext, we compute the Mahalanobis distance squared terms, which are the quadratic forms $(x-\\mu)^{\\top}\\Sigma^{-1}(x-\\mu)$:\n$$\nx_0 - \\mu_{\\mathcal{V}} = \\begin{pmatrix} 0.07 - 0.08 \\\\ 0.40 - 0.45 \\end{pmatrix} = \\begin{pmatrix} -0.01 \\\\ -0.05 \\end{pmatrix}\n$$\n$$\n(x_0 - \\mu_{\\mathcal{V}})^{\\top}\\Sigma_{\\mathcal{V}}^{-1}(x_0 - \\mu_{\\mathcal{V}}) = 400(-0.01)^2 + \\frac{10000}{121}(-0.05)^2 = 400(0.0001) + \\frac{10000}{121}(0.0025) = 0.04 + \\frac{25}{121} \\approx 0.24661\n$$\n$$\nx_0 - \\mu_{\\mathcal{W}} = \\begin{pmatrix} 0.07 - 0.05 \\\\ 0.40 - 0.06 \\end{pmatrix} = \\begin{pmatrix} 0.02 \\\\ 0.34 \\end{pmatrix}\n$$\n$$\n(x_0 - \\mu_{\\mathcal{W}})^{\\top}\\Sigma_{\\mathcal{W}}^{-1}(x_0 - \\mu_{\\mathcal{W}}) = 625(0.02)^2 + 400(0.34)^2 = 625(0.0004) + 400(0.1156) = 0.25 + 46.24 = 46.49\n$$\nNow, we compute the logarithmic terms:\n$$\n\\frac{1}{2}\\ln\\left(\\frac{|\\Sigma_{\\mathcal{W}}|}{|\\Sigma_{\\mathcal{V}}|}\\right) = \\frac{1}{2}\\ln\\left(\\frac{0.000004}{0.00003025}\\right) = \\frac{1}{2}\\ln\\left(\\frac{16}{121}\\right) = \\ln\\left(\\frac{4}{11}\\right) \\approx -1.01160\n$$\n$$\n\\ln\\left(\\frac{P(\\mathcal{V})}{P(\\mathcal{W})}\\right) = \\ln\\left(\\frac{0.55}{0.45}\\right) = \\ln\\left(\\frac{11}{9}\\right) \\approx 0.20067\n$$\nFinally, we assemble the parts to compute $\\Delta(x_0)$:\n$$\n\\Delta(x_0) = \\frac{1}{2}\\left[ 46.49 - \\left(0.04 + \\frac{25}{121}\\right) \\right] + \\ln\\left(\\frac{4}{11}\\right) + \\ln\\left(\\frac{11}{9}\\right)\n$$\n$$\n\\Delta(x_0) \\approx \\frac{1}{2}[46.49 - 0.24661] - 1.01160 + 0.20067\n$$\n$$\n\\Delta(x_0) \\approx \\frac{1}{2}[46.24339] - 1.01160 + 0.20067\n$$\n$$\n\\Delta(x_0) \\approx 23.12170 - 1.01160 + 0.20067 = 22.31077\n$$\nRounding the result to four significant figures gives $22.31$.\nThe large positive value of $\\Delta(x_0)$ indicates that the posterior probability for class $\\mathcal{V}$ (vegetation) is much higher than for class $\\mathcal{W}$ (water), so the pixel would be classified as vegetation.",
            "answer": "$$\\boxed{22.31}$$"
        },
        {
            "introduction": "Our final practice moves beyond classifying pixels into discrete, 'pure' categories and into the more realistic scenario of mixed pixels. This problem introduces the concept of a generalized likelihood ratio test to compare a simple (pure pixel) hypothesis against a composite (mixed pixel) one. This exercise  provides a bridge from basic classification to the foundational ideas of spectral unmixing, a critical topic in quantitative remote sensing.",
            "id": "3826529",
            "problem": "A multispectral sensor measures bidirectional reflectance factor in $2$ bands for a land pixel. The scene contains two endmembers, vegetation and soil, with known mean reflectance vectors $\\boldsymbol{\\mu}_{V} = \\begin{pmatrix} 0.42 \\\\ 0.30 \\end{pmatrix}$ and $\\boldsymbol{\\mu}_{S} = \\begin{pmatrix} 0.12 \\\\ 0.18 \\end{pmatrix}$, respectively. Instrumental and atmospheric residuals are modeled as zero-mean Gaussian noise with a known covariance matrix $\\boldsymbol{\\Sigma} = \\begin{pmatrix} 0.0025 & 0.0010 \\\\ 0.0010 & 0.0040 \\end{pmatrix}$. A single pixel observation is $\\boldsymbol{x} = \\begin{pmatrix} 0.28 \\\\ 0.24 \\end{pmatrix}$.\n\nConsider Maximum Likelihood (ML) classification between the following two hypotheses: \n- $H_{V}$ (pure vegetation): $\\boldsymbol{x} = \\boldsymbol{\\mu}_{V} + \\boldsymbol{\\varepsilon}$,\n- $H_{M}$ (mixed pixel): $\\boldsymbol{x} = f \\boldsymbol{\\mu}_{V} + (1 - f)\\boldsymbol{\\mu}_{S} + \\boldsymbol{\\varepsilon}$,\n\nwhere $f \\in [0,1]$ is an unknown sub-pixel mixture fraction and $\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\boldsymbol{0}, \\boldsymbol{\\Sigma})$ is a zero-mean Gaussian vector with covariance $\\boldsymbol{\\Sigma}$. Assume equal class priors and that $\\boldsymbol{\\Sigma}$ is the same under both hypotheses.\n\nStarting only from the definition of the multivariate Gaussian probability density function and the principle of maximum likelihood, derive the natural logarithm of the generalized likelihood ratio $\\Lambda(\\boldsymbol{x})$ that compares $H_{M}$ to $H_{V}$ by maximizing the likelihood over $f \\in [0,1]$ within $H_{M}$. Then evaluate your derived expression numerically for the given $\\boldsymbol{\\mu}_{V}$, $\\boldsymbol{\\mu}_{S}$, $\\boldsymbol{\\Sigma}$, and $\\boldsymbol{x}$, and provide $\\ln \\Lambda(\\boldsymbol{x})$ as a single closed-form analytic expression. Do not approximate numerically.",
            "solution": "The problem requires the derivation and evaluation of the natural logarithm of the generalized likelihood ratio, $\\ln \\Lambda(\\boldsymbol{x})$, for discriminating between two hypotheses, $H_V$ and $H_M$.\n\nThe likelihood of an observation vector $\\boldsymbol{x}$ for a multivariate Gaussian distribution $\\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$ in $d$ dimensions is given by the probability density function (PDF):\n$$L(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma} | \\boldsymbol{x}) = p(\\boldsymbol{x}|\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) = \\frac{1}{(2\\pi)^{d/2} |\\boldsymbol{\\Sigma}|^{1/2}} \\exp\\left(-\\frac{1}{2}(\\boldsymbol{x} - \\boldsymbol{\\mu})^T \\boldsymbol{\\Sigma}^{-1} (\\boldsymbol{x} - \\boldsymbol{\\mu})\\right)$$\nThe corresponding log-likelihood is:\n$$\\ln L = -\\frac{d}{2}\\ln(2\\pi) - \\frac{1}{2}\\ln|\\boldsymbol{\\Sigma}| - \\frac{1}{2}(\\boldsymbol{x} - \\boldsymbol{\\mu})^T \\boldsymbol{\\Sigma}^{-1} (\\boldsymbol{x} - \\boldsymbol{\\mu})$$\n\nFor hypothesis $H_V$ (pure vegetation), the model is $\\boldsymbol{x} \\sim \\mathcal{N}(\\boldsymbol{\\mu}_V, \\boldsymbol{\\Sigma})$. The likelihood $L_V$ is fixed, as all parameters are known. The log-likelihood is:\n$$\\ln L_V = -\\frac{d}{2}\\ln(2\\pi) - \\frac{1}{2}\\ln|\\boldsymbol{\\Sigma}| - \\frac{1}{2}(\\boldsymbol{x} - \\boldsymbol{\\mu}_V)^T \\boldsymbol{\\Sigma}^{-1} (\\boldsymbol{x} - \\boldsymbol{\\mu}_V)$$\n\nFor hypothesis $H_M$ (mixed pixel), the model is $\\boldsymbol{x} \\sim \\mathcal{N}(\\boldsymbol{\\mu}_M(f), \\boldsymbol{\\Sigma})$, where the mean vector $\\boldsymbol{\\mu}_M(f) = f \\boldsymbol{\\mu}_V + (1-f) \\boldsymbol{\\mu}_S$ depends on the unknown mixture fraction $f \\in [0,1]$.\nThe log-likelihood under $H_M$ is a function of $f$:\n$$\\ln L_M(f) = -\\frac{d}{2}\\ln(2\\pi) - \\frac{1}{2}\\ln|\\boldsymbol{\\Sigma}| - \\frac{1}{2}(\\boldsymbol{x} - \\boldsymbol{\\mu}_M(f))^T \\boldsymbol{\\Sigma}^{-1} (\\boldsymbol{x} - \\boldsymbol{\\mu}_M(f))$$\n\nThe generalized likelihood for $H_M$ is found by maximizing $L_M(f)$ with respect to $f$. This is equivalent to maximizing $\\ln L_M(f)$, which in turn is equivalent to minimizing the quadratic form in the exponent, which we denote by $J(f)$:\n$$J(f) = (\\boldsymbol{x} - \\boldsymbol{\\mu}_M(f))^T \\boldsymbol{\\Sigma}^{-1} (\\boldsymbol{x} - \\boldsymbol{\\mu}_M(f))$$\nWe seek $\\hat{f} = \\arg\\min_{f \\in [0,1]} J(f)$.\n\nLet's expand $J(f)$. Let $\\boldsymbol{\\delta} = \\boldsymbol{\\mu}_V - \\boldsymbol{\\mu}_S$ and $\\boldsymbol{r} = \\boldsymbol{x} - \\boldsymbol{\\mu}_S$. Then $\\boldsymbol{\\mu}_M(f) = \\boldsymbol{\\mu}_S + f\\boldsymbol{\\delta}$, and $\\boldsymbol{x} - \\boldsymbol{\\mu}_M(f) = \\boldsymbol{r} - f\\boldsymbol{\\delta}$.\n$$J(f) = (\\boldsymbol{r} - f\\boldsymbol{\\delta})^T \\boldsymbol{\\Sigma}^{-1} (\\boldsymbol{r} - f\\boldsymbol{\\delta}) = \\boldsymbol{r}^T\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{r} - 2f \\boldsymbol{\\delta}^T\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{r} + f^2 \\boldsymbol{\\delta}^T\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\delta}$$\nTo find the unconstrained minimum, we differentiate with respect to $f$ and set the result to zero:\n$$\\frac{dJ}{df} = -2\\boldsymbol{\\delta}^T\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{r} + 2f \\boldsymbol{\\delta}^T\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\delta} = 0$$\nThis yields the unconstrained maximum likelihood estimate for $f$:\n$$f_{unc} = \\frac{\\boldsymbol{\\delta}^T\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{r}}{\\boldsymbol{\\delta}^T\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\delta}} = \\frac{(\\boldsymbol{\\mu}_V - \\boldsymbol{\\mu}_S)^T\\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{x} - \\boldsymbol{\\mu}_S)}{(\\boldsymbol{\\mu}_V - \\boldsymbol{\\mu}_S)^T\\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{\\mu}_V - \\boldsymbol{\\mu}_S)}$$\nThe constrained estimate $\\hat{f}$ is found by clipping $f_{unc}$ to the interval $[0,1]$:\n$$\\hat{f} = \\max(0, \\min(1, f_{unc}))$$\nThe maximized log-likelihood for $H_M$ is then $\\ln L_M^* = \\ln L_M(\\hat{f})$.\n\nThe generalized likelihood ratio $\\Lambda(\\boldsymbol{x})$ is defined as $\\frac{L_M^*}{L_V}$. The natural logarithm is:\n$$\\ln \\Lambda(\\boldsymbol{x}) = \\ln L_M^* - \\ln L_V = \\ln L_M(\\hat{f}) - \\ln L_V$$\nSubstituting the expressions for the log-likelihoods, the constant terms cancel out:\n$$\\ln \\Lambda(\\boldsymbol{x}) = \\frac{1}{2} \\left[ (\\boldsymbol{x} - \\boldsymbol{\\mu}_V)^T \\boldsymbol{\\Sigma}^{-1} (\\boldsymbol{x} - \\boldsymbol{\\mu}_V) - (\\boldsymbol{x} - \\boldsymbol{\\mu}_M(\\hat{f}))^T \\boldsymbol{\\Sigma}^{-1} (\\boldsymbol{x} - \\boldsymbol{\\mu}_M(\\hat{f})) \\right]$$\nThis is the derived expression. Now we evaluate it numerically.\n\nGiven data:\n$\\boldsymbol{\\mu}_{V} = \\begin{pmatrix} 0.42 \\\\ 0.30 \\end{pmatrix}$, $\\boldsymbol{\\mu}_{S} = \\begin{pmatrix} 0.12 \\\\ 0.18 \\end{pmatrix}$, $\\boldsymbol{x} = \\begin{pmatrix} 0.28 \\\\ 0.24 \\end{pmatrix}$, $\\boldsymbol{\\Sigma} = \\begin{pmatrix} 0.0025 & 0.0010 \\\\ 0.0010 & 0.0040 \\end{pmatrix}$.\n\nFirst, compute the inverse of the covariance matrix $\\boldsymbol{\\Sigma}^{-1}$:\n$|\\boldsymbol{\\Sigma}| = (0.0025)(0.0040) - (0.0010)^2 = 0.000010 - 0.000001 = 0.000009 = 9 \\times 10^{-6}$.\n$$\\boldsymbol{\\Sigma}^{-1} = \\frac{1}{0.000009} \\begin{pmatrix} 0.0040 & -0.0010 \\\\ -0.0010 & 0.0025 \\end{pmatrix} = \\frac{10^6}{9} \\begin{pmatrix} 0.0040 & -0.0010 \\\\ -0.0010 & 0.0025 \\end{pmatrix} = \\frac{1000}{9} \\begin{pmatrix} 4 & -1 \\\\ -1 & 2.5 \\end{pmatrix}$$\n\nNext, calculate the vectors for $f_{unc}$:\n$\\boldsymbol{\\delta} = \\boldsymbol{\\mu}_V - \\boldsymbol{\\mu}_S = \\begin{pmatrix} 0.42 - 0.12 \\\\ 0.30 - 0.18 \\end{pmatrix} = \\begin{pmatrix} 0.30 \\\\ 0.12 \\end{pmatrix}$.\n$\\boldsymbol{r} = \\boldsymbol{x} - \\boldsymbol{\\mu}_S = \\begin{pmatrix} 0.28 - 0.12 \\\\ 0.24 - 0.18 \\end{pmatrix} = \\begin{pmatrix} 0.16 \\\\ 0.06 \\end{pmatrix}$.\n\nNow, compute the terms for $f_{unc}$:\nDenominator: $\\boldsymbol{\\delta}^T\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\delta}$\n$\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\delta} = \\frac{1000}{9} \\begin{pmatrix} 4 & -1 \\\\ -1 & 2.5 \\end{pmatrix} \\begin{pmatrix} 0.30 \\\\ 0.12 \\end{pmatrix} = \\frac{1000}{9} \\begin{pmatrix} 4(0.30) - 1(0.12) \\\\ -1(0.30) + 2.5(0.12) \\end{pmatrix} = \\frac{1000}{9} \\begin{pmatrix} 1.20 - 0.12 \\\\ -0.30 + 0.30 \\end{pmatrix} = \\frac{1000}{9} \\begin{pmatrix} 1.08 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 120 \\\\ 0 \\end{pmatrix}$.\n$\\boldsymbol{\\delta}^T\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\delta} = \\begin{pmatrix} 0.30 & 0.12 \\end{pmatrix} \\begin{pmatrix} 120 \\\\ 0 \\end{pmatrix} = (0.30)(120) = 36$.\n\nNumerator: $\\boldsymbol{\\delta}^T\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{r}$\n$\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{r} = \\frac{1000}{9} \\begin{pmatrix} 4 & -1 \\\\ -1 & 2.5 \\end{pmatrix} \\begin{pmatrix} 0.16 \\\\ 0.06 \\end{pmatrix} = \\frac{1000}{9} \\begin{pmatrix} 4(0.16) - 1(0.06) \\\\ -1(0.16) + 2.5(0.06) \\end{pmatrix} = \\frac{1000}{9} \\begin{pmatrix} 0.64 - 0.06 \\\\ -0.16 + 0.15 \\end{pmatrix} = \\frac{1000}{9} \\begin{pmatrix} 0.58 \\\\ -0.01 \\end{pmatrix} = \\frac{1}{9} \\begin{pmatrix} 580 \\\\ -10 \\end{pmatrix}$.\n$\\boldsymbol{\\delta}^T\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{r} = \\begin{pmatrix} 0.30 & 0.12 \\end{pmatrix} \\frac{1}{9} \\begin{pmatrix} 580 \\\\ -10 \\end{pmatrix} = \\frac{1}{9} (0.30 \\times 580 + 0.12 \\times (-10)) = \\frac{1}{9} (174 - 1.2) = \\frac{172.8}{9} = 19.2$.\n\nCalculate $f_{unc}$:\n$f_{unc} = \\frac{19.2}{36} = \\frac{192}{360} = \\frac{192/24}{360/24} = \\frac{8}{15}$.\nSince $0 \\le \\frac{8}{15} \\le 1$, the constrained estimate is $\\hat{f} = f_{unc} = \\frac{8}{15}$.\n\nNow, evaluate the two Mahalanobis distance terms for $\\ln \\Lambda(\\boldsymbol{x})$.\nTerm 1: $(\\boldsymbol{x} - \\boldsymbol{\\mu}_V)^T \\boldsymbol{\\Sigma}^{-1} (\\boldsymbol{x} - \\boldsymbol{\\mu}_V)$.\n$\\boldsymbol{x} - \\boldsymbol{\\mu}_V = \\begin{pmatrix} 0.28 - 0.42 \\\\ 0.24 - 0.30 \\end{pmatrix} = \\begin{pmatrix} -0.14 \\\\ -0.06 \\end{pmatrix}$.\n$\\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{x} - \\boldsymbol{\\mu}_V) = \\frac{1000}{9} \\begin{pmatrix} 4 & -1 \\\\ -1 & 2.5 \\end{pmatrix} \\begin{pmatrix} -0.14 \\\\ -0.06 \\end{pmatrix} = \\frac{1000}{9} \\begin{pmatrix} -0.56 + 0.06 \\\\ 0.14 - 0.15 \\end{pmatrix} = \\frac{1000}{9} \\begin{pmatrix} -0.50 \\\\ -0.01 \\end{pmatrix} = \\frac{1}{9}\\begin{pmatrix} -500 \\\\ -10 \\end{pmatrix}$.\n$(\\boldsymbol{x} - \\boldsymbol{\\mu}_V)^T[\\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{x} - \\boldsymbol{\\mu}_V)] = \\begin{pmatrix} -0.14 & -0.06 \\end{pmatrix}\\frac{1}{9}\\begin{pmatrix} -500 \\\\ -10 \\end{pmatrix} = \\frac{1}{9}((-0.14)(-500) + (-0.06)(-10)) = \\frac{1}{9}(70 + 0.6) = \\frac{70.6}{9}$.\n\nTerm 2: $(\\boldsymbol{x} - \\boldsymbol{\\mu}_M(\\hat{f}))^T \\boldsymbol{\\Sigma}^{-1} (\\boldsymbol{x} - \\boldsymbol{\\mu}_M(\\hat{f})) = J(\\hat{f})$.\nThis is the minimum value of $J(f)$, which can be calculated using the formula $J(f_{unc}) = \\boldsymbol{r}^T\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{r} - \\frac{(\\boldsymbol{\\delta}^T\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{r})^2}{\\boldsymbol{\\delta}^T\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\delta}}$.\nFirst, we need $\\boldsymbol{r}^T\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{r}$:\n$\\boldsymbol{r}^T[\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{r}] = \\begin{pmatrix} 0.16 & 0.06 \\end{pmatrix} \\frac{1}{9} \\begin{pmatrix} 580 \\\\ -10 \\end{pmatrix} = \\frac{1}{9}(0.16 \\times 580 - 0.06 \\times 10) = \\frac{1}{9}(92.8 - 0.6) = \\frac{92.2}{9}$.\n$J(\\hat{f}) = \\frac{92.2}{9} - \\frac{(19.2)^2}{36} = \\frac{92.2}{9} - \\frac{368.64}{36} = \\frac{4 \\times 92.2}{36} - \\frac{368.64}{36} = \\frac{368.8 - 368.64}{36} = \\frac{0.16}{36} = \\frac{16}{3600} = \\frac{1}{225}$.\n\nFinally, assemble $\\ln \\Lambda(\\boldsymbol{x})$:\n$$\\ln \\Lambda(\\boldsymbol{x}) = \\frac{1}{2}\\left( \\frac{70.6}{9} - \\frac{1}{225} \\right)$$\nTo combine the terms, we find a common denominator. The least common multiple of $9$ and $225$ is $225$.\n$$\\ln \\Lambda(\\boldsymbol{x}) = \\frac{1}{2}\\left( \\frac{70.6 \\times 25}{9 \\times 25} - \\frac{1}{225} \\right) = \\frac{1}{2}\\left( \\frac{1765}{225} - \\frac{1}{225} \\right) = \\frac{1}{2}\\left( \\frac{1764}{225} \\right) = \\frac{882}{225}$$\nThis fraction can be simplified by dividing the numerator and denominator by their greatest common divisor, which is $9$.\n$882 \\div 9 = 98$.\n$225 \\div 9 = 25$.\n$$\\ln \\Lambda(\\boldsymbol{x}) = \\frac{98}{25}$$",
            "answer": "$$\n\\boxed{\\frac{98}{25}}\n$$"
        }
    ]
}