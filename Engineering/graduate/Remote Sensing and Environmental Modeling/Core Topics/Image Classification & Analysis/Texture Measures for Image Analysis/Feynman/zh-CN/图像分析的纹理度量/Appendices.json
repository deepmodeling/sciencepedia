{
    "hands_on_practices": [
        {
            "introduction": "灰度共生矩阵（GLCM）是纹理分析中最基础且功能强大的统计方法之一。本练习将引导你从基本定义出发，构建 GLCM 并计算一系列经典的纹理特征。通过系统地改变滞后距离 $d$，你将亲身体验和量化纹理特征如何响应不同空间尺度上的图像结构，这是掌握多尺度遥感影像解译的关键一步 ()。",
            "id": "3859963",
            "problem": "考虑一个二维离散灰度图像，该图像被建模为矩形格点上的函数 $I(x,y)$，其中每个像素的取值范围为 $\\{0,1,\\dots,L-1\\}$，$L$ 为整数灰度级数。二阶纹理由空间灰度级相互作用来表征，这种相互作用可通过灰度共生矩阵 (GLCM) 进行量化。对于一个严格为正整数的滞后距离 $d$ 和一个方向角 $\\theta$（以度为单位），定义由长度为 $d$、角度为 $\\theta$ 的向量分隔的像素对之间的共生关系。基于此关系，定义共生计数矩阵，然后通过将计数除以有效像素对的总数来定义归一化联合概率函数 $p_{d,\\theta}(i,j)$。\n\n从上述基本定义出发，将标准二阶纹理度量推导为联合概率 $p_{d,\\theta}(i,j)$ 的泛函。您的推导必须基于 $p_{d,\\theta}(i,j)$ 的概率解释，使用对应于成对像素灰度级的随机变量的适当函数的期望。您必须确保您的定义对于任何整数 $L \\geq 2$、任何图像域、任何严格为正整数的滞后距离 $d$ 以及以度表示的角度 $\\theta$ 都是良定义的。对于方差为零的退化图像这一特殊情况，将相关性定义为 $0$。\n\n编写一个完整的程序，该程序：\n- 对于给定的图像，在方向角 $\\theta = 0^\\circ$（水平方向）和指定的滞后距离 $d$ 下构建 GLCM，方法是计算所有索引在边界内的有效有序像素对 $(I(x,y), I(x,y+d))$，并进行归一化以获得 $p_{d,0^\\circ}(i,j)$。\n- 从第一性原理出发，根据 $p_{d,0^\\circ}(i,j)$ 计算以下纹理特征：对比度、同质性、能量、熵和相关性，每个特征都通过关于 $p_{d,0^\\circ}(i,j)$ 的期望来定义。\n- 通过计算在 $d = 4$ 和 $d = 1$ 时的特征差异 $\\Delta$ 来量化改变滞后距离 $d$ 的影响，即对于每个特征 $f$，返回 $f(d=4) - f(d=1)$。\n\n整个过程中角度必须以度为单位处理。不适用任何物理单位。所有计算必须表示为浮点数。\n\n测试套件：\n使用以下图像和参数来测试不同的纹理区域、边界条件以及粗尺度与细尺度模式：\n- 测试用例 1：一个 $8 \\times 8$ 的二值棋盘格图像，$L=2$，由 $I(x,y) = ((x+y) \\bmod 2)$ 定义。使用滞后距离 $d \\in \\{1,4\\}$，$\\theta = 0^\\circ$。\n- 测试用例 2：一个 $32 \\times 32$ 的图像，具有 $L=8$ 个灰度级，通过使用固定种子 $s = 42$ 生成的 $\\{0,1,\\dots,7\\}$ 中的伪随机整数产生，以确保可复现性。使用滞后距离 $d \\in \\{1,4\\}$，$\\theta = 0^\\circ$。\n- 测试用例 3：一个 $16 \\times 16$ 的二值图像，包含宽度为 4 的垂直条纹，因此 $I(x,y) = ((y \\div 4) \\bmod 2)$，其中 $\\div$ 表示整数除法，$L=2$。使用滞后距离 $d \\in \\{1,4\\}$，$\\theta = 0^\\circ$。\n- 测试用例 4：一个 $16 \\times 16$ 的均匀二值图像，所有像素值均为 $1$，$L=2$。使用滞后距离 $d \\in \\{1,4\\}$，$\\theta = 0^\\circ$。\n\n对于每个测试用例，计算特征差异 $[\\Delta\\text{contrast}, \\Delta\\text{homogeneity}, \\Delta\\text{energy}, \\Delta\\text{entropy}, \\Delta\\text{correlation}]$，其中每个 $\\Delta$ 为 $f(d=4) - f(d=1)$。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个用方括号括起来、无空格的逗号分隔列表。每个测试用例的结果必须是一个包含五个浮点数的列表，这些浮点数按 $[\\Delta\\text{contrast},\\Delta\\text{homogeneity},\\Delta\\text{energy},\\Delta\\text{entropy},\\Delta\\text{correlation}]$ 的顺序排列，并四舍五入到六位小数。例如，输出结构必须类似于 $[[a_1,a_2,a_3,a_4,a_5],[b_1,b_2,b_3,b_4,b_5],[c_1,c_2,c_3,c_4,c_5],[d_1,d_2,d_3,d_4,d_5]]$，其中每个 $a_i$、$b_i$、$c_i$、$d_i$ 都是四舍五入到六位小数的浮点数。",
            "solution": "我们从基于概率的灰度共生矩阵 (GLCM) 的形式化定义开始。设 $I(x,y)$ 是一个离散灰度图像，其像素值在 $\\{0,1,\\dots,L-1\\}$ 范围内，$L \\geq 2$。固定一个以度为单位的方向角 $\\theta$ 和一个严格为正整数的滞后距离 $d$。考虑所有有序像素对 $(I(x,y), I(x',y'))$，使得从 $(x,y)$ 到 $(x',y')$ 的位移长度为 $d$，角度为 $\\theta$。对于 $\\theta = 0^\\circ$ 的水平方向，位移为 $(\\Delta x, \\Delta y) = (0,d)$，因此 $(x',y') = (x, y + d)$。\n\n共生计数矩阵 $P_{d,\\theta}(i,j)$ 定义为\n$$\nP_{d,\\theta}(i,j) = \\#\\{(x,y) \\mid I(x,y) = i,\\, I(x',y') = j,\\, (x',y') = (x,y) + (0,d),\\, \\text{in bounds}\\}.\n$$\n令 $N_{d,\\theta} = \\sum_{i=0}^{L-1} \\sum_{j=0}^{L-1} P_{d,\\theta}(i,j)$ 为有效像素对的总数。归一化的联合概率质量函数 (pmf) 为\n$$\np_{d,\\theta}(i,j) = \\frac{P_{d,\\theta}(i,j)}{N_{d,\\theta}}, \\quad \\text{for } i,j \\in \\{0,1,\\dots,L-1\\}.\n$$\n这个 $p_{d,\\theta}(i,j)$ 是一个真正的 pmf，作用于离散随机变量 $(X,Y)$，该变量代表在指定位移下像素对的灰度级，因为 $p_{d,\\theta}(i,j) \\geq 0$ 且 $\\sum_{i,j} p_{d,\\theta}(i,j) = 1$。\n\n从 $p_{d,\\theta}(i,j)$，我们获得边际 pmf $p_X(i) = \\sum_{j=0}^{L-1} p_{d,\\theta}(i,j)$ 和 $p_Y(j) = \\sum_{i=0}^{L-1} p_{d,\\theta}(i,j)$。它们的期望和方差是\n$$\n\\mu_X = \\sum_{i=0}^{L-1} i\\, p_X(i), \\quad \\mu_Y = \\sum_{j=0}^{L-1} j\\, p_Y(j),\n$$\n$$\n\\sigma_X^2 = \\sum_{i=0}^{L-1} (i - \\mu_X)^2\\, p_X(i), \\quad \\sigma_Y^2 = \\sum_{j=0}^{L-1} (j - \\mu_Y)^2\\, p_Y(j).\n$$\n\n现在我们通过关于 $p_{d,\\theta}(i,j)$ 的随机变量 $(X,Y)$ 的期望来定义标准纹理度量：\n\n- 对比度量化了灰度级差异的平均平方：\n$$\n\\text{contrast} = \\mathbb{E}\\left[(X - Y)^2\\right] = \\sum_{i=0}^{L-1} \\sum_{j=0}^{L-1} (i - j)^2\\, p_{d,\\theta}(i,j).\n$$\n\n- 同质性（逆差分矩）对较大的灰度级差异进行惩罚：\n$$\n\\text{homogeneity} = \\mathbb{E}\\left[\\frac{1}{1 + (X - Y)^2}\\right] = \\sum_{i=0}^{L-1} \\sum_{j=0}^{L-1} \\frac{p_{d,\\theta}(i,j)}{1 + (i - j)^2}.\n$$\n\n- 能量（角二阶矩）衡量 pmf 的集中程度：\n$$\n\\text{energy} = \\sum_{i=0}^{L-1} \\sum_{j=0}^{L-1} \\left(p_{d,\\theta}(i,j)\\right)^2.\n$$\n\n- 熵衡量 pmf 的不确定性：\n$$\n\\text{entropy} = - \\sum_{i=0}^{L-1} \\sum_{j=0}^{L-1} p_{d,\\theta}(i,j) \\log p_{d,\\theta}(i,j),\n$$\n约定当 $p_{d,\\theta}(i,j) = 0$ 时，该项贡献为 $0$。\n\n- 相关性衡量 $X$ 和 $Y$ 之间的线性关联：\n$$\n\\text{correlation} = \\frac{\\mathbb{E}\\left[(X - \\mu_X)(Y - \\mu_Y)\\right]}{\\sigma_X \\sigma_Y} = \\frac{\\sum_{i=0}^{L-1}\\sum_{j=0}^{L-1} (i - \\mu_X)(j - \\mu_Y)\\, p_{d,\\theta}(i,j)}{\\sigma_X \\sigma_Y}.\n$$\n对于 $\\sigma_X = 0$ 或 $\\sigma_Y = 0$ 的退化情况，我们将相关性定义为 $0$。\n\n这些度量直接从 pmf $p_{d,\\theta}(i,j)$ 推导得出，将它们与空间灰度级相互作用的基本概率表征联系起来。\n\n算法设计：\n\n1. 对于给定的图像 $I$ 和 $L$，通过计算所有有序对 $(I(x,y), I(x,y+d))$ 来构建 $P_{d,0^\\circ}(i,j)$，其中对于图像高度 $H$ 和宽度 $W$，$x \\in \\{0,\\dots,H-1\\}$，$y \\in \\{0,\\dots,W-d-1\\}$。每个观察到的对 $(i,j)$ 使 $P_{d,0^\\circ}(i,j)$ 增加 $1$。\n\n2. 将计数归一化为概率：计算 $N_{d,0^\\circ}$ 作为所有计数的总和，并设置 $p_{d,0^\\circ}(i,j) = P_{d,0^\\circ}(i,j)/N_{d,0^\\circ}$。\n\n3. 使用 $p_{d,0^\\circ}(i,j)$、其边际分布和派生统计量，如上所示计算特征。在计算熵时使用自然对数，并从求和中排除零概率项以避免未定义的表达式。\n\n4. 通过计算每个特征 $f \\in \\{\\text{contrast}, \\text{homogeneity}, \\text{energy}, \\text{entropy}, \\text{correlation}\\}$ 的差异 $\\Delta f = f(d=4) - f(d=1)$，来量化滞后距离的影响。\n\n5. 将此过程应用于指定的测试套件：\n   - 对于 $8 \\times 8$ 棋盘格图像 ($L=2$)，将 $d$ 从 $1$ 增加到 $4$ 会使共生关系从主要为跨级像素对转变为主要为同级像素对，从而降低对比度，增加同质性，能量和熵保持不变（由于对称的两状态 pmf），并将相关性从负值增加到正值。\n   - 对于 $32 \\times 32$ 的带种子的随机图像 ($L=8$)，共生关系在不同的 $d$ 值上近似独立，导致所有特征的 $\\Delta$ 值都很小。\n   - 对于 $16 \\times 16$ 的宽度为 4 的垂直条纹图像 ($L=2$)，$d=1$ 时像素对大多停留在条纹内部（同级像素对），而 $d=4$ 时则与条纹之间的过渡对齐（跨级像素对），这会增加对比度并降低同质性；能量和熵会适度调整；相关性变得更负。\n   - 对于均匀图像 ($L=2$)，对所有 $d$，pmf 都集中在 $(i,j) = (1,1)$ 上，导致特征没有变化（所有 $\\Delta$ 均为 $0$）。\n\n6. 将每个计算出的差异四舍五入到六位小数，并以指定的单行、无空格、方括号括起、逗号分隔的格式输出每个测试用例的结果。\n\n此设计遵循第一性原理，通过从 GLCM 确定的联合概率函数推导所有纹理特征，并且它分离了增加滞后距离 $d$ 对粗尺度与细尺度纹理表达的影响。",
            "answer": "```python\nimport numpy as np\n\ndef build_glcm_horizontal(image: np.ndarray, levels: int, d: int) -> np.ndarray:\n    \"\"\"\n    Build the GLCM for horizontal orientation (theta = 0 degrees) at lag distance d.\n    image: 2D numpy array of integer gray levels in {0, ..., levels-1}\n    levels: number of gray levels L\n    d: positive integer lag\n    Returns: L x L normalized joint probability matrix p(i,j)\n    \"\"\"\n    H, W = image.shape\n    if d == 0:\n        raise ValueError(\"Lag distance d must be a strictly positive integer.\")\n    if W - d <= 0:\n        # No valid pairs; return uniform zero-probability matrix\n        return np.zeros((levels, levels), dtype=float)\n\n    left = image[:, :W - d]\n    right = image[:, d:]\n\n    # Flatten pairs\n    i_vals = left.ravel()\n    j_vals = right.ravel()\n\n    # Accumulate counts\n    counts = np.zeros((levels, levels), dtype=float)\n    # Use np.add.at to handle repeated indices\n    np.add.at(counts, (i_vals, j_vals), 1.0)\n    total = counts.sum()\n    if total == 0:\n        return np.zeros((levels, levels), dtype=float)\n    return counts / total\n\ndef texture_features_from_p(p: np.ndarray) -> dict:\n    \"\"\"\n    Compute texture features from normalized joint probability matrix p(i,j).\n    Returns a dict with keys: contrast, homogeneity, energy, entropy, correlation.\n    \"\"\"\n    L = p.shape[0]\n    # Indices grid\n    i_idx = np.arange(L)\n    j_idx = np.arange(L)\n    I, J = np.meshgrid(i_idx, j_idx, indexing='ij')\n\n    # Contrast: E[(I-J)^2]\n    diff_sq = (I - J) ** 2\n    contrast = float((diff_sq * p).sum())\n\n    # Homogeneity: E[1/(1+(I-J)^2)]\n    homogeneity = float((p / (1.0 + diff_sq)).sum())\n\n    # Energy: sum p^2\n    energy = float((p ** 2).sum())\n\n    # Entropy: - sum p log p (natural log), ignoring p=0\n    nonzero = p > 0\n    entropy = float(-(p[nonzero] * np.log(p[nonzero])).sum())\n\n    # Marginals\n    p_i = p.sum(axis=1)\n    p_j = p.sum(axis=0)\n\n    mu_i = float((i_idx * p_i).sum())\n    mu_j = float((j_idx * p_j).sum())\n\n    var_i = float(((i_idx - mu_i) ** 2 * p_i).sum())\n    var_j = float(((j_idx - mu_j) ** 2 * p_j).sum())\n\n    sigma_i = np.sqrt(var_i)\n    sigma_j = np.sqrt(var_j)\n\n    # Numerator for correlation: sum (i - mu_i)(j - mu_j) p(i,j)\n    corr_num = float(((I - mu_i) * (J - mu_j) * p).sum())\n    denom = sigma_i * sigma_j\n    if denom == 0.0:\n        correlation = 0.0\n    else:\n        correlation = float(corr_num / denom)\n\n    return {\n        \"contrast\": contrast,\n        \"homogeneity\": homogeneity,\n        \"energy\": energy,\n        \"entropy\": entropy,\n        \"correlation\": correlation\n    }\n\ndef image_checkerboard(size: int) -> np.ndarray:\n    \"\"\"Generate an size x size checkerboard with values {0,1}.\"\"\"\n    x = np.arange(size).reshape(-1, 1)\n    y = np.arange(size).reshape(1, -1)\n    return ((x + y) % 2).astype(int)\n\ndef image_random_ints(h: int, w: int, levels: int, seed: int) -> np.ndarray:\n    \"\"\"Generate h x w random integers in {0, ..., levels-1} with fixed seed.\"\"\"\n    rng = np.random.default_rng(seed)\n    return rng.integers(low=0, high=levels, size=(h, w), endpoint=False).astype(int)\n\ndef image_vertical_stripes(h: int, w: int, stripe_width: int) -> np.ndarray:\n    \"\"\"Generate binary vertical stripes of given width.\"\"\"\n    y = np.arange(w)\n    stripe = ((y // stripe_width) % 2).astype(int)\n    return np.tile(stripe, (h, 1))\n\ndef image_uniform(h: int, w: int, value: int = 1) -> np.ndarray:\n    \"\"\"Generate a uniform image with a given value.\"\"\"\n    return np.full((h, w), value, dtype=int)\n\ndef compute_feature_differences(image: np.ndarray, levels: int, d_small: int = 1, d_large: int = 4) -> list:\n    \"\"\"\n    Compute feature differences f(d_large) - f(d_small) for contrast, homogeneity, energy, entropy, correlation.\n    Returns a list of five floats rounded to six decimal places.\n    \"\"\"\n    p_small = build_glcm_horizontal(image, levels, d_small)\n    p_large = build_glcm_horizontal(image, levels, d_large)\n\n    feats_small = texture_features_from_p(p_small)\n    feats_large = texture_features_from_p(p_large)\n\n    keys = [\"contrast\", \"homogeneity\", \"energy\", \"entropy\", \"correlation\"]\n    diffs = [feats_large[k] - feats_small[k] for k in keys]\n    # Round to six decimals as required\n    diffs = [round(x, 6) for x in diffs]\n    return diffs\n\ndef format_nested_list_no_spaces(nested: list) -> str:\n    \"\"\"\n    Format a nested list (list of lists of floats) into a string with no spaces,\n    as required: [[a1,a2,...],[b1,b2,...],...]\n    \"\"\"\n    inner_strs = []\n    for inner in nested:\n        inner_str = \",\".join(f\"{v:.6f}\" if isinstance(v, float) else str(v) for v in inner)\n        inner_strs.append(f\"[{inner_str}]\")\n    return f\"[{','.join(inner_strs)}]\"\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (image_generator_function, parameters_dict, levels)\n        (\"checkerboard\", {\"size\": 8}, 2),\n        (\"random\", {\"h\": 32, \"w\": 32, \"levels\": 8, \"seed\": 42}, 8),\n        (\"vertical_stripes\", {\"h\": 16, \"w\": 16, \"stripe_width\": 4}, 2),\n        (\"uniform\", {\"h\": 16, \"w\": 16, \"value\": 1}, 2),\n    ]\n\n    results = []\n    for name, params, levels in test_cases:\n        if name == \"checkerboard\":\n            img = image_checkerboard(params[\"size\"])\n        elif name == \"random\":\n            img = image_random_ints(params[\"h\"], params[\"w\"], params[\"levels\"], params[\"seed\"])\n        elif name == \"vertical_stripes\":\n            img = image_vertical_stripes(params[\"h\"], params[\"w\"], params[\"stripe_width\"])\n        elif name == \"uniform\":\n            img = image_uniform(params[\"h\"], params[\"w\"], params[\"value\"])\n        else:\n            raise ValueError(\"Unknown test case.\")\n\n        # Ensure values are within [0, levels-1]\n        img = img.astype(int)\n        if img.min() < 0 or img.max() >= levels:\n            raise ValueError(\"Image values out of expected range for specified levels.\")\n\n        diffs = compute_feature_differences(img, levels, d_small=1, d_large=4)\n        results.append(diffs)\n\n    # Final print statement in the exact required format (no spaces).\n    print(format_nested_list_no_spaces(results))\n\nsolve()\n```"
        },
        {
            "introduction": "除了统计方法，基于滤波器组的方法为纹理分析提供了另一种视角，其中 Laws 掩模是一种经典技术。该方法通过一系列特定设计的卷积核将图像分解为“水平”、“边缘”、“斑点”等基本纹理基元，并测量其局部能量。本练习不仅要求你实现 Laws 能量的计算，更重要的是通过一个受控实验，探讨局部均值归一化如何有效地提升纹理特征对光照变化的鲁棒性，这对处理不同时相或不同传感器获取的影像至关重要 ()。",
            "id": "3859964",
            "problem": "给定一个二维离散灰度图像，表示为有限网格上的实值函数 $I:\\mathbb{Z}^2\\to\\mathbb{R}$。考虑基于长度为 $5$ 的一维核（$L_5$、$E_5$、$S_5$、$W_5$ 和 $R_5$）的标准 Laws 纹理度量构建方法。通过这些核的外积形成二维掩模，得到 $25$ 个大小为 $5\\times 5$ 的掩模。对于每个掩模 $M$，将线性响应 $R_M$ 定义为图像与该掩模的二维离散卷积，记为\n$$\nR_M(x,y) = \\sum_{u=-2}^{2}\\sum_{v=-2}^{2} I(x-u,y-v)\\,M(u,v),\n$$\n以及将窗口能量 $E_M$ 定义为以 $(x,y)$ 为中心、边长为 $w$ 的方形窗口 $\\mathcal{W}_{w}(x,y)$ 内响应的平方和，\n$$\nE_M(x,y) = \\sum_{(i,j)\\in \\mathcal{W}_{w}(x,y)} \\left( R_M(i,j)\\right)^2.\n$$\n您将实现一个程序，对于给定的图像和窗口大小 $w$，计算每个掩模的窗口能量的空间平均值，并将其汇集成一个 $25$ 维的全局能量特征向量，每个掩模对应一个能量值。为了研究光照不变性，您将研究两种形式的图像形成变化：加性偏置和乘性尺度。您还将考虑在滤波前通过减去相同窗口大小 $w$ 内的局部均值来进行局部零均值归一化。形式上，局部零均值归一化的图像 $\\tilde{I}$ 为\n$$\n\\tilde{I}(x,y) = I(x,y) - \\frac{1}{w^2}\\sum_{(i,j)\\in \\mathcal{W}_{w}(x,y)} I(i,j).\n$$\n您将通过所有掩模上的最大相对偏差来量化一对图像 $I_\\text{base}$ 和 $I_\\text{mod}$ 的不变性，\n$$\n\\Delta = \\max_{M} \\frac{\\left| \\bar{E}_M(I_\\text{mod}) - \\bar{E}_M(I_\\text{base}) \\right|}{\\left|\\bar{E}_M(I_\\text{base})\\right| + \\varepsilon},\n$$\n其中 $\\bar{E}_M$ 表示 $E_M$ 在所有像素位置上的空间平均值，$\\varepsilon$ 是一个小的正数，用于避免除以零。目标是计算几种测试用例的 $\\Delta$ 值，包括使用和不使用零均值归一化的情况，从而凭经验检验零均值归一化对光照不变性的影响。\n\n将使用的基本原理和定义：\n- 离散卷积和线性移不变滤波：卷积算子是线性的，并且与平移操作是可交换的，掩模响应通过邻域强度与掩模系数的乘积之和获得。\n- 能量为窗口内聚合的幅值平方：将滤波器响应平方并在一个空间支持域上求和，可以量化局部纹理强度，其根源在于线性系统中的功率概念。\n- 局部均值作为平滑算子：窗口内的局部均值是与归一化盒式滤波器进行卷积的输出。\n\n您的程序必须严格基于这些原理实现以下任务：\n1. 构建一维 Laws 核 $L_5=\\left[1,4,6,4,1\\right]$、$E_5=\\left[-1,-2,0,2,1\\right]$、$S_5=\\left[-1,0,2,0,-1\\right]$、$W_5=\\left[-1,2,0,-2,1\\right]$ 和 $R_5=\\left[1,-4,6,-4,1\\right]$。通过一维核的外积形成所有 $25$ 个二维掩模。\n2. 对于给定的图像和窗口大小 $w$，计算每个掩模的空间平均窗口能量。具体步骤为：将图像与每个掩模进行卷积，将响应平方，然后与一个 $w\\times w$ 的全1窗口进行卷积以获得 $E_M(x,y)$，最后对所有 $(x,y)$ 的 $E_M$ 进行平均。\n3. 通过从图像中减去使用 $w\\times w$ 均匀核计算出的局部均值来实现局部零均值归一化，并在设置归一化标志时相应地重新计算能量。\n4. 对于每个测试用例，计算基础图像和修改后图像的全局能量向量之间的最大相对偏差 $\\Delta$。\n\n测试套件：\n- 用例1（加性偏置，无归一化）：基础图像大小为 $64\\times 64$，定义为\n$$\nI_\\text{base}(x,y) = 0.5\\sin\\left(\\frac{2\\pi x}{8}\\right) + 0.3\\cos\\left(\\frac{2\\pi y}{16}\\right) + 0.2\\sin\\left(\\frac{2\\pi(x+y)}{10}\\right),\n$$\n修改后的图像为 $I_\\text{mod}(x,y)=I_\\text{base}(x,y)+0.3$，窗口大小 $w=15$，禁用零均值归一化。\n- 用例2（加性偏置，带归一化）：$I_\\text{base}$ 和 $I_\\text{mod}$ 与用例1相同，窗口大小 $w=15$，启用零均值归一化。\n- 用例3（乘性尺度，无归一化）：$I_\\text{mod}(x,y)=2.0\\,I_\\text{base}(x,y)$，窗口大小 $w=15$，禁用零均值归一化。\n- 用例4（乘性尺度，带归一化）：与用例3相同，但启用零均值归一化。\n- 用例5（常数图像，加性偏置，带归一化）：在 $64\\times 64$ 网格上，对所有 $(x,y)$，$I_\\text{base}(x,y)=0.2$，修改后的图像为 $I_\\text{mod}(x,y)=I_\\text{base}(x,y)+0.1$，窗口大小 $w=15$，启用零均值归一化。\n\n数值和输出要求：\n- 在相对偏差公式中使用 $\\varepsilon=10^{-12}$ 作为小的正数。\n- 三角函数中的角度应以弧度为单位。\n- 所有输出都必须表示为无单位的实数。\n- 对于每个用例，程序必须计算一个单一的实数 $\\Delta$。\n- 您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔的结果列表（例如，$[r_1,r_2,r_3,r_4,r_5]$），顺序为用例1到5。",
            "solution": "该问题要求实现和分析 Laws 纹理度量，以凭经验研究其对于简单光照变化（即加性偏置和乘性缩放）的不变性属性。分析涉及比较从基础图像和修改后图像派生的特征向量，这些比较在有和没有前置局部零均值归一化步骤的情况下进行。解决方案是根据所提供的定义和任务构建的。\n\nLaws 方法的核心在于一组长度为5的一维零和核，这些核旨在检测如水平 (level)、边缘 (edge)、斑点 (spot)、波浪 (wave) 和波纹 (ripple) 等基本结构模式。五个基本核是：\n- 水平 (Level): $L_5 = [1, 4, 6, 4, 1]$\n- 边缘 (Edge): $E_5 = [-1, -2, 0, 2, 1]$\n- 斑点 (Spot): $S_5 = [-1, 0, 2, 0, -1]$\n- 波浪 (Wave): $W_5 = [-1, 2, 0, -2, 1]$\n- 波纹 (Ripple): $R_5 = [1, -4, 6, -4, 1]$\n\n由这五个一维核，通过计算一维核对的外积 $M_{ab} = k_a^T k_b$（其中 $k_a, k_b \\in \\{L_5, E_5, S_5, W_5, R_5\\}$），生成一组 $5 \\times 5 = 25$ 个二维卷积掩模 $M_{ab}$。\n\n对于每个测试用例，我们必须处理一个基础图像 $I_\\text{base}$ 和一个修改后图像 $I_\\text{mod}$，为每个图像生成一个25维特征向量。该向量由每个掩模（共25个）的空间平均窗口能量 $\\bar{E}_M$ 组成。对于给定图像 $I$，计算此特征向量的步骤如下。\n\n首先，可以应用可选的局部零均值归一化。启用此选项后，原始图像 $I$ 会被转换为归一化图像 $\\tilde{I}$。这种归一化旨在消除亮度的局部变化。对于给定的窗口大小 $w$，每个像素 $(x,y)$ 处的局部均值是通过对以 $(x,y)$ 为中心的 $w \\times w$ 窗口内的像素值求平均来计算的。归一化后的图像由下式给出：\n$$\n\\tilde{I}(x,y) = I(x,y) - \\frac{1}{w^2}\\sum_{(i,j)\\in \\mathcal{W}_{w}(x,y)} I(i,j)\n$$\n此操作可通过将图像 $I$ 与一个归一化的 $w \\times w$ 盒式滤波器 $K_\\text{avg}$（其中每个元素为 $1/w^2$）进行卷积，然后从原始图像中减去得到的局部均值图像来高效实现：$\\tilde{I} = I - (I * K_\\text{avg})$。如果禁用归一化，则在后续步骤中直接使用原始图像 $I$。让我们将被处理的图像（无论是 $I$ 还是 $\\tilde{I}$）表示为 $I'$。\n\n接下来，对于25个掩模中的每一个 $M$，执行以下步骤来计算 $\\bar{E}_M(I')$：\n1.  **滤波**：将图像 $I'$ 与掩模 $M$ 进行卷积，生成响应图像 $R_M$。这是一个线性滤波操作，定义为：\n    $$\n    R_M(x,y) = (I' * M)(x,y) = \\sum_{u=-2}^{2}\\sum_{v=-2}^{2} I'(x-u,y-v)\\,M(u,v)\n    $$\n2.  **能量计算**：将响应图像 $R_M$ 按元素进行平方，生成原始能量图 $(R_M(x,y))^2$。\n3.  **窗口聚合**：将平方后的响应在一个 $w \\times w$ 窗口 $\\mathcal{W}_w$ 内求和，生成窗口能量图 $E_M$。\n    $$\n    E_M(x,y) = \\sum_{(i,j)\\in \\mathcal{W}_{w}(x,y)} \\left( R_M(i,j)\\right)^2\n    $$\n    这种求和在计算上等同于将平方响应图与一个全为1的 $w \\times w$ 核（我们称之为 $K_\\text{sum}$）进行卷积：$E_M = (R_M)^2 * K_\\text{sum}$。\n4.  **空间平均**：掩模 $M$ 的最终特征（表示为 $\\bar{E}_M$）是窗口能量图 $E_M(x,y)$ 在图像网格中所有像素坐标 $(x,y)$ 上的空间平均值。\n\n此过程产生两个25维特征向量，一个用于 $I_\\text{base}$，一个用于 $I_\\text{mod}$。\n\n最后，为了量化特征的不变性，计算两个特征向量之间的最大相对偏差 $\\Delta$。设 $\\bar{E}_M(I_\\text{base})$ 和 $\\bar{E}_M(I_\\text{mod})$ 为给定掩模 $M$ 的特征。偏差定义为：\n$$\n\\Delta = \\max_{M} \\frac{\\left| \\bar{E}_M(I_\\text{mod}) - \\bar{E}_M(I_\\text{base}) \\right|}{\\left|\\bar{E}_M(I_\\text{base})\\right| + \\varepsilon}\n$$\n其中 $\\varepsilon=10^{-12}$ 是一个小的常数，用于防止除以零，特别是对于可能产生零能量的掩模。\n\n所有卷积操作的执行都应使输出图像与输入图像具有相同的维度。为了处理边界，采用了对称填充方案，这是图像处理中最小化边缘伪影的标准选择。该实现将处理五个指定的测试用例中的每一个，并计算相应的 $\\Delta$ 值。",
            "answer": "```python\nimport numpy as np\nfrom scipy.signal import convolve2d\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n\n    def compute_features(img, w, normalize):\n        \"\"\"\n        Computes the 25-dimensional Laws' texture energy feature vector for a given image.\n\n        Args:\n            img (np.ndarray): The input 2D image.\n            w (int): The window size for energy aggregation and local mean.\n            normalize (bool): Flag to enable/disable local zero-mean normalization.\n\n        Returns:\n            np.ndarray: A 25-element vector of spatially averaged texture energies.\n        \"\"\"\n        if normalize:\n            # Local zero-mean normalization\n            local_mean_kernel = np.ones((w, w)) / (w * w)\n            local_mean = convolve2d(img, local_mean_kernel, mode='same', boundary='symm')\n            proc_img = img - local_mean\n        else:\n            proc_img = img\n\n        # Define 1D Laws' kernels\n        L5 = np.array([1, 4, 6, 4, 1])\n        E5 = np.array([-1, -2, 0, 2, 1])\n        S5 = np.array([-1, 0, 2, 0, -1])\n        W5 = np.array([-1, 2, 0, -2, 1])\n        R5 = np.array([1, -4, 6, -4, 1])\n        kernels_1d = [L5, E5, S5, W5, R5]\n\n        # Generate 2D masks from outer products\n        masks_2d = [np.outer(k1, k2) for k1 in kernels_1d for k2 in kernels_1d]\n\n        energy_vector = []\n        window_sum_kernel = np.ones((w, w))\n\n        for mask in masks_2d:\n            # Step 1: Convolve image with mask\n            response = convolve2d(proc_img, mask, mode='same', boundary='symm')\n            \n            # Step 2: Square the response\n            response_sq = np.square(response)\n            \n            # Step 3: Compute windowed energy via convolution\n            energy_map = convolve2d(response_sq, window_sum_kernel, mode='same', boundary='symm')\n            \n            # Step 4: Spatially average the energy\n            avg_energy = np.mean(energy_map)\n            energy_vector.append(avg_energy)\n\n        return np.array(energy_vector)\n\n    # Common parameters for test cases\n    size = 64\n    w = 15\n    epsilon = 1e-12\n\n    # Generate base images\n    y, x = np.meshgrid(np.arange(size), np.arange(size), indexing='ij')\n    I_base_sinusoid = (0.5 * np.sin(2 * np.pi * x / 8) +\n                       0.3 * np.cos(2 * np.pi * y / 16) +\n                       0.2 * np.sin(2 * np.pi * (x + y) / 10))\n    I_base_constant = np.full((size, size), 0.2)\n\n    # Define test cases\n    test_cases = [\n        # Case 1: Additive bias, no normalization\n        {'I_base': I_base_sinusoid, 'I_mod': I_base_sinusoid + 0.3, 'w': w, 'normalize': False},\n        # Case 2: Additive bias, with normalization\n        {'I_base': I_base_sinusoid, 'I_mod': I_base_sinusoid + 0.3, 'w': w, 'normalize': True},\n        # Case 3: Multiplicative scale, no normalization\n        {'I_base': I_base_sinusoid, 'I_mod': I_base_sinusoid * 2.0, 'w': w, 'normalize': False},\n        # Case 4: Multiplicative scale, with normalization\n        {'I_base': I_base_sinusoid, 'I_mod': I_base_sinusoid * 2.0, 'w': w, 'normalize': True},\n        # Case 5: Constant image, additive bias, with normalization\n        {'I_base': I_base_constant, 'I_mod': I_base_constant + 0.1, 'w': w, 'normalize': True},\n    ]\n\n    results = []\n    for case in test_cases:\n        # Compute feature vectors for base and modified images\n        E_base = compute_features(case['I_base'], case['w'], case['normalize'])\n        E_mod = compute_features(case['I_mod'], case['w'], case['normalize'])\n        \n        # Calculate maximum relative deviation Delta\n        numerator = np.abs(E_mod - E_base)\n        denominator = np.abs(E_base) + epsilon\n        relative_deviations = numerator / denominator\n        delta = np.max(relative_deviations)\n        results.append(delta)\n\n    # Format the final output string\n    output_str = f\"[{','.join(f'{r:.10f}' for r in results)}]\"\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "在实际应用中，我们常常会计算出数十种纹理特征，这导致了特征集的高维、冗余问题。主成分分析（PCA）是解决此类问题的核心工具，它能帮助我们发现数据中的主要潜在结构。这项综合性练习将引导你对一个高维纹理特征集执行 PCA，学习如何解释各主成分所代表的物理意义（例如粗糙度或周期性），并最终根据分析结果选择一个信息量大且冗余度低的特征子集 ()。",
            "id": "3860042",
            "problem": "一位研究人员正在研究遥感影像中的纹理度量，并寻求一种有原则的方法来简化高维特征集。考虑一个数据矩阵 $\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$，其中包含在 $n$ 个影像斑块上测量的 $p$ 个纹理特征。目标是对标准化后的特征应用主成分分析（PCA），根据潜在的纹理过程来解释主成分，并提出一个既能保留方差又能最小化冗余的降维特征集。\n\n基础理论：\n- 每个特征向量 $\\mathbf{x}_i \\in \\mathbb{R}^n$ 通过 $z_{ti} = (x_{ti} - \\mu_i)/\\sigma_i$ 被标准化为零均值、单位方差的向量 $\\mathbf{z}_i$，其中 $\\mu_i$ 是特征 $i$ 的样本均值，$\\sigma_i$ 是其样本标准差。样本方差低于一个很小的阈值 $\\delta$ 的特征被视为零方差特征，必须被排除。\n- 标准化特征的样本协方差即为样本相关矩阵 $\\mathbf{S} = \\frac{1}{n-1} \\mathbf{Z}^\\top \\mathbf{Z}$，其中 $\\mathbf{Z} \\in \\mathbb{R}^{n \\times p'}$ 是排除零方差特征后的标准化矩阵（$p'$ 是保留的特征数量）。\n- 主成分被定义为使投影方差最大化的标准正交方向 $\\{\\mathbf{v}_j\\}$。这些方向通过特征分解 $\\mathbf{S}\\mathbf{v}_j = \\lambda_j \\mathbf{v}_j$ 获得，其中特征值按 $\\lambda_1 \\ge \\lambda_2 \\ge \\cdots \\ge \\lambda_{p'} \\ge 0$ 排序。成分 $j$ 的方差解释率为 $r_j = \\lambda_j / \\sum_{k=1}^{p'} \\lambda_k$。\n- 特征 $i$ 在主成分 $j$ 上的相关性载荷由 $\\ell_{ij} = \\sqrt{\\lambda_j} v_{ij}$ 给出。这些载荷量化了每个特征与每个主成分的对齐强度。\n\n成分保留规则：\n- 使用Kaiser准则保留满足 $\\lambda_j \\ge 1$ 的成分，因为 $\\mathbf{S}$ 是标准化变量的相关矩阵。同时，强制执行一个累计方差解释率阈值 $\\tau = 0.9$：保留至少使 $\\sum_{j=1}^{k} r_j \\ge \\tau$ 成立的最小数量 $k$ 个成分。令 $k$ 为Kaiser准则计数和累计阈值计数中的最大值，且上限为 $p'$。\n\n解释映射：\n- 根据四个纹理过程类别来解释保留的主成分：粗糙度、周期性、平滑度和随机性。为每个类别 $c$ 定义一个针对原始 $p$ 个特征的权重向量 $\\mathbf{w}^{(c)} \\in \\mathbb{R}^{p}$，以反映定性关联：\n    - 粗糙度：对灰度共生矩阵对比度、相异性、熵以及分形维数等特征赋予正权重；对同质性、能量以及局部二值模式均匀度赋予负权重。\n    - 周期性：对主导频率处的傅里叶谱比赋予正权重；对同质性和局部二值模式均匀度赋予中等正权重；对熵赋予负权重。\n    - 平滑度：与粗糙度相反；对同质性、能量和局部二值模式均匀度赋予正权重；对对比度、相异性、熵和分形维数赋予负权重。\n    - 随机性：对熵赋予正权重，对谱比赋予负权重；其他权重为中性。\n- 对于每个保留的成分 $j$，计算类别得分 $s_c(j) = \\sum_{i \\in \\mathcal{I}} w^{(c)}_i \\cdot |\\ell_{ij}|$，其中 $\\mathcal{I}$ 是保留特征（排除零方差特征后）的索引。将成分 $j$ 分配给得分最高的类别。如果出现平分，则按照粗糙度 $\\rightarrow$ 周期性 $\\rightarrow$ 平滑度 $\\rightarrow$ 随机性的优先顺序确定性地解决，并将这些类别分别映射到整数代码 $1,2,3,4$。\n\n降维特征集提案：\n- 从保留的成分中，为每个成分 $j$ 选择使其绝对载荷 $|\\ell_{ij}|$ 最大化的保留特征 $i$，从而挑选出一组具有最小冗余度的代表性特征。跳过任何已被先前成分选中的特征。继续选择下一个最高载荷的特征，直到为每个保留的成分都选择了一个唯一的代表。报告最终选定的特征索引集，这些索引是相对于原始 $p$ 个特征的从零开始的索引（不包括因零方差而被丢弃的任何特征）。如果在排除后唯一特征的数量少于 $k$，则相应地限制选择数量。\n\n你的程序必须实现此过程，并将其应用于以下测试套件。完整的原始特征列表（按顺序）是：\n1. 灰度共生矩阵对比度\n2. 灰度共生矩阵相异性\n3. 灰度共生矩阵同质性\n4. 灰度共生矩阵能量\n5. 灰度共生矩阵熵\n6. 主导频率处的傅里叶谱比\n7. 局部二值模式均匀度\n8. 分形维数估计\n\n使用以下针对原始八个特征（按上述顺序）的类别权重向量 $\\mathbf{w}^{(c)}$：\n- 粗糙度 ($c=1$): $\\mathbf{w}^{(\\text{roughness})} = [1.0, 1.0, -1.0, -0.8, 0.8, 0.0, -0.6, 0.9]$.\n- 周期性 ($c=2$): $\\mathbf{w}^{(\\text{periodicity})} = [0.0, 0.0, 0.3, 0.0, -0.5, 1.0, 0.3, 0.0]$.\n- 平滑度 ($c=3$): $\\mathbf{w}^{(\\text{smoothness})} = [-1.0, -0.8, 1.0, 1.0, -0.5, 0.2, 0.7, -0.5]$.\n- 随机性 ($c=4$): $\\mathbf{w}^{(\\text{randomness})} = [0.0, 0.0, 0.0, 0.0, 1.0, -0.5, 0.0, 0.0]$.\n\n测试套件：\n- 案例 A（均衡的潜在粗糙度和周期性）：$n=20$，噪声标准差 $\\sigma=0.05$，种子 $s=12345$。令潜在过程 $R_t \\sim \\mathcal{N}(0,1)$ 和 $P_t \\sim \\mathcal{N}(0,1)$ 对于 $t=1,\\dots,n$ 是独立的。生成特征：\n    1. 对比度: $0.9 R_t + 0.1 P_t + \\varepsilon_{1t}$，\n    2. 相异性: $0.8 R_t + 0.15 P_t + \\varepsilon_{2t}$，\n    3. 同质性: $-0.75 R_t - 0.1 P_t + \\varepsilon_{3t}$，\n    4. 能量: $-0.65 R_t - 0.1 P_t + \\varepsilon_{4t}$，\n    5. 熵: $0.7 R_t + 0.2 P_t + \\varepsilon_{5t}$，\n    6. 谱比: $0.1 R_t + 0.9 P_t + \\varepsilon_{6t}$，\n    7. 局部二值模式均匀度: $-0.55 R_t + 0.15 P_t + \\varepsilon_{7t}$，\n    8. 分形维数: $0.6 R_t + 0.05 P_t + \\varepsilon_{8t}$，\n    其中 $\\varepsilon_{it} \\sim \\mathcal{N}(0,\\sigma^2)$ 是独立的。\n- 案例 B（包含零方差特征的边界情况）：$n=15$，$\\sigma=0.05$，种子 $s=54321$。按案例 A 的方式生成特征，但将所有样本的能量值设为常数 $1.0$。该特征由于样本方差为零，必须在 PCA 之前被排除。\n- 案例 C（特征间高度共线性）：$n=25$，$\\sigma=0.05$，种子 $s=24680$。按案例 A 的方式生成潜在过程 $R_t, P_t$ 并构造：\n    1. 对比度: $0.85 R_t + 0.1 P_t + \\varepsilon_{1t}$，\n    2. 相异性: $\\text{contrast} + \\eta_{2t}$，其中 $\\eta_{2t} \\sim \\mathcal{N}(0,0.01^2)$，\n    3. 同质性: $-0.7 R_t - 0.1 P_t + \\varepsilon_{3t}$，\n    4. 能量: $-\\text{homogeneity} + \\eta_{4t}$，其中 $\\eta_{4t} \\sim \\mathcal{N}(0,0.01^2)$，\n    5. 熵: $0.65 R_t + 0.2 P_t + \\varepsilon_{5t}$，\n    6. 谱比: $0.1 R_t + 0.85 P_t + \\varepsilon_{6t}$，\n    7. 局部二值模式均匀度: $0.7 \\cdot \\text{spectral ratio} + \\eta_{7t}$，其中 $\\eta_{7t} \\sim \\mathcal{N}(0,0.01^2)$，\n    8. 分形维数: $\\text{entropy} + \\eta_{8t}$，其中 $\\eta_{8t} \\sim \\mathcal{N}(0,0.01^2)$。\n\n实现要求：\n- 按所述方法标准化每个特征。在 PCA 之前排除方差小于 $\\delta = 10^{-12}$ 的特征。\n- 计算 $\\mathbf{S}$、其特征值 $\\{\\lambda_j\\}$ 和特征向量 $\\{\\mathbf{v}_j\\}$，按 $\\lambda_j$ 降序排序，并计算相关性载荷 $\\ell_{ij} = \\sqrt{\\lambda_j} v_{ij}$。\n- 使用所述规则确定 $k$ 并提出降维特征集。将每个保留的成分解释为整数代码 $1$（粗糙度）、$2$（周期性）、$3$（平滑度）或 $4$（随机性）。\n- 对特征索引使用从零开始的索引，该索引相对于原始的八个特征列表。\n\n最终输出规范：\n- 对于每个测试案例，你的程序必须输出一个包含三个元素的列表：保留的成分数量 $k$（一个整数），所选特征索引的列表（一个整数列表），以及按方差解释率降序排列的每个保留成分的解释代码列表（一个整数列表）。\n- 你的程序应生成单行输出，其中包含所有三个案例的结果，结果为逗号分隔的列表，并用方括号括起来，每个案例的结果本身也用方括号括起来。例如：\"[[k1,[i1,i2,...],[c1,c2,...]], [k2,[...],[...]], [k3,[...],[...]]\"。\n- 此任务不涉及物理单位或角度。",
            "solution": "该问题要求实现一个结构化的、多阶段的流程，使用主成分分析（PCA）对纹理特征集进行降维和解释。该方法应用于代表遥感影像分析中典型情景的合成数据集。整个过程可以分解为以下几个主要步骤。\n\n**1. 数据标准化与预处理**\n\n初始数据以矩阵 $\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$ 的形式提供，包含对 $n$ 个不同影像斑块测量的 $p=8$ 个纹理特征。为确保方差较大的特征不会对 PCA 产生不成比例的影响，每个特征向量都进行了标准化。对于每个特征 $i$，其值 $\\{x_{ti}\\}_{t=1}^n$ 被转换为一个新的特征向量 $\\mathbf{z}_i$，其样本均值为零，样本标准差为一。转换公式如下：\n$$\nz_{ti} = \\frac{x_{ti} - \\mu_i}{\\sigma_i}\n$$\n其中 $\\mu_i$ 是第 $i$ 个特征的样本均值，$\\sigma_i$ 是其样本标准差（以 $n-1$ 为分母计算）。\n\n一个关键的预处理步骤是识别并排除方差接近于零的特征，因为这些特征不携带任何信息，并可能导致数值不稳定。任何样本方差低于指定阈值 $\\delta = 10^{-12}$ 的特征在进行 PCA 之前都会从数据集中移除。这将产生一个标准化的数据矩阵 $\\mathbf{Z} \\in \\mathbb{R}^{n \\times p'}$，其中 $p' \\le p$ 是保留的特征数量。\n\n**2. 通过主成分分析（PCA）进行协方差分析**\n\n应用 PCA 来揭示特征集内部的潜在结构。由于特征已经标准化，它们的样本协方差矩阵等同于样本相关矩阵 $\\mathbf{S}$。该矩阵计算如下：\n$$\n\\mathbf{S} = \\frac{1}{n-1} \\mathbf{Z}^\\top \\mathbf{Z}\n$$\nPCA 通过对这个 $p' \\times p'$ 的对称矩阵 $\\mathbf{S}$ 进行特征分解来继续：\n$$\n\\mathbf{S}\\mathbf{v}_j = \\lambda_j \\mathbf{v}_j\n$$\n特征向量 $\\{\\mathbf{v}_j\\}_{j=1}^{p'}$ 是主成分，它们代表了特征空间的一组新的标准正交基向量。相应的特征值 $\\{\\lambda_j\\}_{j=1}^{p'}$ 按降序排列 $\\lambda_1 \\ge \\lambda_2 \\ge \\cdots \\ge \\lambda_{p'} \\ge 0$，量化了数据投影到每个主成分上的方差。标准化数据中的总方差为 $\\sum_{j=1}^{p'} \\lambda_j = \\text{trace}(\\mathbf{S}) = p'$。\n\n**3. 成分保留准则**\n\n为了实现降维，只保留 $k$ 个主成分的一个子集。$k$ 的值由一个混合规则确定：\n- **Kaiser准则**：保留所有特征值 $\\lambda_j \\ge 1$ 的成分。其基本原理是，由于总方差为 $p'$，每个原始标准化特征平均贡献的方差为 $1$。一个特征值 $\\lambda_j  1$ 的主成分所解释的方差比单个原始特征还少。\n- **累计方差解释率**：保留解释至少 $\\tau = 0.9$ 比例总方差所需的最小成分数量。成分 $j$ 的方差解释率为 $r_j = \\lambda_j / \\sum_{i=1}^{p'} \\lambda_i$。我们找到最小的 $k'$ 使得 $\\sum_{j=1}^{k'} r_j \\ge \\tau$。\n\n最终保留的成分数量 $k$ 是由这两个规则确定的计数中的最大值，但不能超过可用的总成分数 $p'$。\n\n**4. 保留成分的解释**\n\n通过将保留的抽象主成分与原始特征关联起来进行解释。这通过相关性载荷 $\\ell_{ij}$ 来完成，它衡量了原始特征 $i$ 与主成分 $j$ 之间的相关性。载荷计算如下：\n$$\n\\ell_{ij} = \\sqrt{\\lambda_j} v_{ij}\n$$\n其中 $v_{ij}$ 是特征向量 $\\mathbf{v}_j$ 的第 $i$ 个元素。\n\n对于 $k$ 个保留成分中的每一个，都会计算一组分数，以将其与四个预定义纹理类别之一相关联：粗糙度 ($c=1$)、周期性 ($c=2$)、平滑度 ($c=3$) 和随机性 ($c=4$)。每个类别由一个针对原始 $p$ 个特征的权重向量 $\\mathbf{w}^{(c)}$ 定义。成分 $j$ 和类别 $c$ 的分数为：\n$$\ns_c(j) = \\sum_{i \\in \\mathcal{I}} w^{(c)}_i \\cdot |\\ell_{ij}|\n$$\n其中 $\\mathcal{I}$ 是 $p'$ 个保留特征的索引集。该成分被分配给得分最高的类别。任何平分情况都使用优先顺序确定性地解决：粗糙度 $\\rightarrow$ 周期性 $\\rightarrow$ 平滑度 $\\rightarrow$ 随机性。\n\n**5. 降维特征集提案**\n\n最后一步是提出一个降维后的原始特征集，这些特征能够代表保留的主成分，同时最小化冗余。对于每个保留的成分 $j$（按方差从大到小排序，从 $1$ 到 $k$），算法选择表现出最高绝对载荷 $|\\ell_{ij}|$ 的单个原始特征 $i$。为确保所选特征不冗余，会跳过已选择用来代表先前成分的特征，并考虑当前成分中绝对载荷次高的特征。此过程持续进行，直到为 $k$ 个成分中的每一个都选择了一个唯一的代表性特征，最终得到一个包含 $k$ 个特征索引的集合（以零为基准报告）。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef _format_result(res):\n    \"\"\"Helper to format the list results into a string without spaces.\"\"\"\n    if res is None:\n        return \"None\"\n    k, i_list, c_list = res\n    i_str = f\"[{','.join(map(str, i_list))}]\"\n    c_str = f\"[{','.join(map(str, c_list))}]\"\n    return f\"[{k},{i_str},{c_str}]\"\n\ndef analyze_pca(X, weights, delta=1e-12, tau=0.9):\n    \"\"\"\n    Performs the full PCA analysis pipeline on a data matrix X.\n    \"\"\"\n    n, p = X.shape\n\n    # Step 1: Standardization and Filtering\n    variances = np.var(X, axis=0, ddof=1)\n    retained_mask = variances > delta\n    original_indices = np.arange(p)[retained_mask]\n    \n    if not np.any(retained_mask):\n        return [0, [], []]\n\n    X_filtered = X[:, retained_mask]\n    n_filtered, p_prime = X_filtered.shape\n\n    # Standardize the filtered data using sample statistics (ddof=1)\n    means = np.mean(X_filtered, axis=0)\n    stds = np.std(X_filtered, axis=0, ddof=1)\n    \n    # Avoid division by zero if a std is extremely small\n    stds[stds  delta] = 1.0 \n    Z = (X_filtered - means) / stds\n\n    # Step 2: PCA on the correlation matrix\n    # For data Z standardized with sample stddev, its sample covariance matrix\n    # is the desired correlation matrix.\n    if p_prime == 1:\n        S = np.array([[1.0]])\n    else:\n        # Use ddof=1 for sample covariance matrix (normalization by n-1)\n        S = np.cov(Z, rowvar=False, ddof=1)\n\n    # Eigen-decomposition. eigh returns eigenvalues in ascending order.\n    eigenvalues, eigenvectors = np.linalg.eigh(S)\n    \n    # Sort in descending order\n    idx = np.argsort(eigenvalues)[::-1]\n    eigenvalues = eigenvalues[idx]\n    eigenvectors = eigenvectors[:, idx]\n\n    # Step 3: Component Retention\n    # Kaiser criterion\n    k_kaiser = np.sum(eigenvalues >= 1.0)\n\n    # Cumulative variance criterion\n    total_variance = np.sum(eigenvalues)\n    if total_variance  delta:\n        explained_variance_ratio = np.zeros_like(eigenvalues)\n    else:\n        explained_variance_ratio = eigenvalues / total_variance\n    \n    cumulative_variance_ratio = np.cumsum(explained_variance_ratio)\n    \n    k_tau_candidates = np.where(cumulative_variance_ratio >= tau)[0]\n    if k_tau_candidates.size > 0:\n        k_tau = k_tau_candidates[0] + 1\n    else:\n        k_tau = p_prime\n    \n    k = int(max(k_kaiser, k_tau))\n    k = min(k, p_prime)\n\n    if k == 0:\n        return [0, [], []]\n        \n    retained_lambdas = eigenvalues[:k]\n    retained_vectors = eigenvectors[:, :k]\n\n    # Step 4: Loadings and Interpretation\n    loadings = retained_vectors * np.sqrt(retained_lambdas) # shape (p', k)\n\n    interpretation_codes = []\n    filtered_weights = weights[:, original_indices] # shape (4, p')\n    \n    for j in range(k):\n        component_loadings = np.abs(loadings[:, j])\n        scores = np.sum(filtered_weights * component_loadings, axis=1)\n        \n        # Tie-breaking is handled by argmax due to priority order of weights\n        best_category_idx = np.argmax(scores)\n        interpretation_codes.append(best_category_idx + 1)\n        \n    # Step 5: Feature Selection\n    selected_feature_indices = []\n    used_original_indices = set()\n\n    for j in range(k):\n        component_loadings_abs = np.abs(loadings[:, j])\n        sorted_feature_idx_local = np.argsort(component_loadings_abs)[::-1]\n        \n        for local_idx in sorted_feature_idx_local:\n            original_idx = original_indices[local_idx]\n            if original_idx not in used_original_indices:\n                selected_feature_indices.append(int(original_idx))\n                used_original_indices.add(original_idx)\n                break\n    \n    return [k, selected_feature_indices, interpretation_codes]\n\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    weights = np.array([\n        [1.0, 1.0, -1.0, -0.8, 0.8, 0.0, -0.6, 0.9],   # Roughness (1)\n        [0.0, 0.0, 0.3, 0.0, -0.5, 1.0, 0.3, 0.0],    # Periodicity (2)\n        [-1.0, -0.8, 1.0, 1.0, -0.5, 0.2, 0.7, -0.5], # Smoothness (3)\n        [0.0, 0.0, 0.0, 0.0, 1.0, -0.5, 0.0, 0.0]     # Randomness (4)\n    ])\n    \n    test_cases_params = [\n        {'n': 20, 'sigma': 0.05, 'seed': 12345, 'case_type': 'A'},\n        {'n': 15, 'sigma': 0.05, 'seed': 54321, 'case_type': 'B'},\n        {'n': 25, 'sigma': 0.05, 'seed': 24680, 'case_type': 'C'},\n    ]\n    \n    results = []\n\n    for params in test_cases_params:\n        n, sigma, seed, case_type = params['n'], params['sigma'], params['seed'], params['case_type']\n        rng = np.random.default_rng(seed)\n        \n        X = np.zeros((n, 8))\n        \n        if case_type in ['A', 'B']:\n            R = rng.normal(0, 1, n)\n            P = rng.normal(0, 1, n)\n            epsilons = rng.normal(0, sigma, (n, 8))\n            \n            X[:, 0] = 0.9 * R + 0.1 * P + epsilons[:, 0]\n            X[:, 1] = 0.8 * R + 0.15 * P + epsilons[:, 1]\n            X[:, 2] = -0.75 * R - 0.1 * P + epsilons[:, 2]\n            X[:, 3] = -0.65 * R - 0.1 * P + epsilons[:, 3]\n            X[:, 4] = 0.7 * R + 0.2 * P + epsilons[:, 4]\n            X[:, 5] = 0.1 * R + 0.9 * P + epsilons[:, 5]\n            X[:, 6] = -0.55 * R + 0.15 * P + epsilons[:, 6]\n            X[:, 7] = 0.6 * R + 0.05 * P + epsilons[:, 7]\n\n            if case_type == 'B':\n                X[:, 3] = 1.0\n\n        elif case_type == 'C':\n            R = rng.normal(0, 1, n)\n            P = rng.normal(0, 1, n)\n            epsilons = rng.normal(0, sigma, (n, 8))\n            etas = rng.normal(0, 0.01, (n, 8))\n\n            contrast = 0.85 * R + 0.1 * P + epsilons[:, 0]\n            homogeneity = -0.7 * R - 0.1 * P + epsilons[:, 2]\n            entropy = 0.65 * R + 0.2 * P + epsilons[:, 4]\n            spectral_ratio = 0.1 * R + 0.85 * P + epsilons[:, 5]\n            \n            X[:, 0] = contrast\n            X[:, 1] = contrast + etas[:, 1]\n            X[:, 2] = homogeneity\n            X[:, 3] = -homogeneity + etas[:, 3]\n            X[:, 4] = entropy\n            X[:, 5] = spectral_ratio\n            X[:, 6] = 0.7 * spectral_ratio + etas[:, 6]\n            X[:, 7] = entropy + etas[:, 7]\n            \n        result = analyze_pca(X, weights)\n        results.append(result)\n\n    formatted_results = [_format_result(r) for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}