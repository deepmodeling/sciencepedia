{
    "hands_on_practices": [
        {
            "introduction": "An essential aspect of any iterative algorithm like K-means or ISODATA is determining when to stop. While a simple heuristic is to halt when centroid movements become small, this practice challenges you to derive a rigorous stopping criterion from first principles. By exploring the geometric relationship between inter-cluster separation, intra-cluster radii, and centroid displacement, you will develop a deeper, more fundamental understanding of what it means for a classification map to be stable .",
            "id": "3863354",
            "problem": "A multispectral satellite image with $d$ spectral bands has been normalized so that each feature lies in $[0,1]$. You apply unsupervised classification using the $k$-means algorithm and the Iterative Self-Organizing Data Analysis Technique Algorithm (ISODATA). Let $\\{\\mu_k^{(t)}\\}_{k=1}^K$ denote the cluster centroids at iteration $t$, and define the maximum centroid displacement between iterations $t-1$ and $t$ as\n$$\n\\Delta^{(t)} \\equiv \\max_{1 \\le k \\le K} \\left\\|\\mu_k^{(t)} - \\mu_k^{(t-1)}\\right\\|.\n$$\nClass assignments are made by nearest-centroid rule in Euclidean distance. Define the minimum inter-centroid separation at iteration $t-1$ as\n$$\ns^{(t-1)} \\equiv \\min_{i \\neq j} \\left\\|\\mu_i^{(t-1)} - \\mu_j^{(t-1)}\\right\\|,\n$$\nand the maximum within-cluster radius at iteration $t-1$ as\n$$\nr_{\\max}^{(t-1)} \\equiv \\max_{1 \\le k \\le K} \\max_{x \\in C_k^{(t-1)}} \\left\\|x - \\mu_k^{(t-1)}\\right\\|,\n$$\nwhere $C_k^{(t-1)}$ is the set of pixels assigned to centroid $\\mu_k^{(t-1)}$.\n\nStarting only from the definitions of Euclidean distance, the $k$-means assignment rule (nearest centroid), and the triangle inequality, derive a sufficient stopping threshold $\\varepsilon$ such that if $\\Delta^{(t)} \\le \\varepsilon$, then no pixelâ€™s class label can change at iteration $t$ (i.e., the map of class labels is stabilized). Your derivation must express $\\varepsilon$ solely in terms of $s^{(t-1)}$ and $r_{\\max}^{(t-1)}$ and must be valid for both $k$-means and ISODATA iterations that do not perform merges or splits.\n\nThen, for a specific scene where $s^{(t-1)} = 0.42$ and $r_{\\max}^{(t-1)} = 0.11$, compute the numerical value of $\\varepsilon$. Round your answer to four significant figures. Because the features are normalized reflectance, express the final threshold as a unitless value.",
            "solution": "The problem statement has been carefully examined and is determined to be valid. It is scientifically grounded within the domain of unsupervised machine learning and remote sensing, well-posed with all necessary definitions and constraints, objective in its formulation, and presents a solvable challenge based on fundamental mathematical principles.\n\nThe objective is to derive a sufficient stopping threshold, $\\varepsilon$, for the maximum centroid displacement, $\\Delta^{(t)}$, such that if $\\Delta^{(t)} \\le \\varepsilon$, no pixel changes its cluster assignment during the $t$-th iteration of a $k$-means or ISODATA-style algorithm (without cluster splits or merges). A pixel's cluster assignment is determined by the nearest-centroid rule in Euclidean distance. The derivation must rely solely on the definition of Euclidean distance, the assignment rule, and the triangle inequality.\n\nLet $x$ be an arbitrary pixel in the $d$-dimensional feature space. At iteration $t-1$, pixel $x$ is assigned to cluster $C_i^{(t-1)}$, with centroid $\\mu_i^{(t-1)}$. By the nearest-centroid assignment rule, this implies that for any other cluster $j \\ne i$:\n$$\n\\left\\|x - \\mu_i^{(t-1)}\\right\\| \\le \\left\\|x - \\mu_j^{(t-1)}\\right\\|\n$$\nAt iteration $t$, the centroids are updated to new positions $\\{\\mu_k^{(t)}\\}_{k=1}^K$. For the cluster assignments to remain stable, pixel $x$ must remain assigned to cluster $i$. This requires that its distance to the new centroid $\\mu_i^{(t)}$ is less than or equal to its distance to any other new centroid $\\mu_j^{(t)}$:\n$$\n\\left\\|x - \\mu_i^{(t)}\\right\\| \\le \\left\\|x - \\mu_j^{(t)}\\right\\| \\quad \\text{for all } j \\ne i\n$$\nOur goal is to find a condition on $\\Delta^{(t)} \\equiv \\max_{k} \\left\\|\\mu_k^{(t)} - \\mu_k^{(t-1)}\\right\\|$ that guarantees this inequality holds for any pixel $x$. We will establish this by finding an upper bound for the left-hand side and a lower bound for the right-hand side.\n\nFirst, we find an upper bound for $\\left\\|x - \\mu_i^{(t)}\\right\\|$. By applying the triangle inequality, we can write:\n$$\n\\left\\|x - \\mu_i^{(t)}\\right\\| = \\left\\|(x - \\mu_i^{(t-1)}) + (\\mu_i^{(t-1)} - \\mu_i^{(t)})\\right\\| \\le \\left\\|x - \\mu_i^{(t-1)}\\right\\| + \\left\\|\\mu_i^{(t-1)} - \\mu_i^{(t)}\\right\\|\n$$\nSince $x$ belongs to cluster $C_i^{(t-1)}$, its distance to the centroid $\\mu_i^{(t-1)}$ is bounded by the maximum within-cluster radius, $r_{\\max}^{(t-1)}$:\n$$\n\\left\\|x - \\mu_i^{(t-1)}\\right\\| \\le \\max_{z \\in C_i^{(t-1)}} \\left\\|z - \\mu_i^{(t-1)}\\right\\| \\le r_{\\max}^{(t-1)}\n$$\nThe displacement of centroid $i$ is bounded by the maximum displacement over all centroids, $\\Delta^{(t)}$:\n$$\n\\left\\|\\mu_i^{(t-1)} - \\mu_i^{(t)}\\right\\| = \\left\\|\\mu_i^{(t)} - \\mu_i^{(t-1)}\\right\\| \\le \\Delta^{(t)}\n$$\nSubstituting these bounds, we obtain an upper bound for the distance from $x$ to its new \"home\" centroid:\n$$\n\\left\\|x - \\mu_i^{(t)}\\right\\| \\le r_{\\max}^{(t-1)} + \\Delta^{(t)}\n$$\n\nNext, we find a lower bound for $\\left\\|x - \\mu_j^{(t)}\\right\\|$ for any $j \\ne i$. Using the reverse triangle inequality ($\\|a-b\\| \\ge \\|a\\| - \\|b\\|$), we have:\n$$\n\\left\\|x - \\mu_j^{(t)}\\right\\| = \\left\\|(x - \\mu_j^{(t-1)}) - (\\mu_j^{(t)} - \\mu_j^{(t-1)})\\right\\| \\ge \\left\\|x - \\mu_j^{(t-1)}\\right\\| - \\left\\|\\mu_j^{(t)} - \\mu_j^{(t-1)}\\right\\|\n$$\nThe second term is bounded by $\\Delta^{(t)}$, so:\n$$\n\\left\\|x - \\mu_j^{(t)}\\right\\| \\ge \\left\\|x - \\mu_j^{(t-1)}\\right\\| - \\Delta^{(t)}\n$$\nNow, we need a lower bound for $\\left\\|x - \\mu_j^{(t-1)}\\right\\|$, the distance from pixel $x$ (in cluster $i$) to a different centroid $j$ at the previous iteration. We apply the triangle inequality to the three points $x$, $\\mu_i^{(t-1)}$, and $\\mu_j^{(t-1)}$:\n$$\n\\left\\|\\mu_i^{(t-1)} - \\mu_j^{(t-1)}\\right\\| \\le \\left\\|\\mu_i^{(t-1)} - x\\right\\| + \\left\\|x - \\mu_j^{(t-1)}\\right\\|\n$$\nRearranging this gives:\n$$\n\\left\\|x - \\mu_j^{(t-1)}\\right\\| \\ge \\left\\|\\mu_i^{(t-1)} - \\mu_j^{(t-1)}\\right\\| - \\left\\|x - \\mu_i^{(t-1)}\\right\\|\n$$\nTo find the tightest possible lower bound for $\\left\\|x - \\mu_j^{(t-1)}\\right\\|$ that holds for any pixel $x \\in C_i^{(t-1)}$ and any competing cluster $j$, we must consider the worst-case values for the terms on the right-hand side. The term $\\left\\|\\mu_i^{(t-1)} - \\mu_j^{(t-1)}\\right\\|$ is minimized by the minimum inter-centroid separation, $s^{(t-1)}$. The term $\\left\\|x - \\mu_i^{(t-1)}\\right\\|$ is maximized by the maximum within-cluster radius, $r_{\\max}^{(t-1)}$. Therefore:\n$$\n\\left\\|x - \\mu_j^{(t-1)}\\right\\| \\ge s^{(t-1)} - r_{\\max}^{(t-1)}\n$$\nSubstituting this result back into our lower bound for $\\left\\|x - \\mu_j^{(t)}\\right\\|$:\n$$\n\\left\\|x - \\mu_j^{(t)}\\right\\| \\ge (s^{(t-1)} - r_{\\max}^{(t-1)}) - \\Delta^{(t)}\n$$\n\nWe have now established a worst-case upper bound on the distance to the new home centroid and a worst-case lower bound on the distance to any new competing centroid. For the cluster assignments to be stable for all pixels, the following sufficient condition must be met:\n$$\n\\text{upper bound on } \\left\\|x - \\mu_i^{(t)}\\right\\| \\le \\text{lower bound on } \\left\\|x - \\mu_j^{(t)}\\right\\|\n$$\n$$\nr_{\\max}^{(t-1)} + \\Delta^{(t)} \\le s^{(t-1)} - r_{\\max}^{(t-1)} - \\Delta^{(t)}\n$$\nWe can now solve for $\\Delta^{(t)}$ to find the condition that guarantees stability:\n$$\n2\\Delta^{(t)} \\le s^{(t-1)} - 2r_{\\max}^{(t-1)}\n$$\n$$\n\\Delta^{(t)} \\le \\frac{s^{(t-1)}}{2} - r_{\\max}^{(t-1)}\n$$\nIf the maximum centroid displacement $\\Delta^{(t)}$ satisfies this inequality, no pixel will change its cluster assignment. Therefore, the sufficient stopping threshold $\\varepsilon$ is:\n$$\n\\varepsilon = \\frac{s^{(t-1)}}{2} - r_{\\max}^{(t-1)}\n$$\n\nWe are given the specific values $s^{(t-1)} = 0.42$ and $r_{\\max}^{(t-1)} = 0.11$. We can now compute the numerical value of $\\varepsilon$:\n$$\n\\varepsilon = \\frac{0.42}{2} - 0.11 = 0.21 - 0.11 = 0.1\n$$\nThe problem requires the answer to be rounded to four significant figures. Since the calculation is exact, we express the result as $0.1000$. The quantities involved are derived from distances in a normalized feature space, so the resulting threshold $\\varepsilon$ is a unitless value.\n$$\n\\varepsilon = 0.1000\n$$",
            "answer": "$$\\boxed{0.1000}$$"
        },
        {
            "introduction": "The heart of the K-means algorithm lies in its distance calculations, which are profoundly influenced by data preprocessing. A common step in remote sensing is to standardize spectral bands to have zero mean and unit variance, but the mathematical consequences of this action are not always fully appreciated. This exercise guides you to derive the effect of z-score normalization on the K-means objective function, revealing its equivalence to using a diagonal Mahalanobis distance, which provides a formal justification for why this technique prevents high-variance bands from dominating the clustering result .",
            "id": "3863403",
            "problem": "A multispectral satellite image has $N$ pixels with $p$ spectral bands, represented as vectors $\\mathbf{x}_{i} \\in \\mathbb{R}^{p}$ for $i \\in \\{1,\\dots,N\\}$. Consider unsupervised classification via the $k$-means objective, where the dataset is partitioned into $K$ nonempty clusters $\\{C_{1},\\dots,C_{K}\\}$ and each cluster $k$ has centroid $\\boldsymbol{\\mu}_{k} \\in \\mathbb{R}^{p}$. The standard $k$-means objective is\n$$\nJ \\;=\\; \\sum_{k=1}^{K} \\sum_{i \\in C_{k}} \\left\\| \\mathbf{x}_{i} - \\boldsymbol{\\mu}_{k} \\right\\|_{2}^{2}.\n$$\nIn many remote sensing workflows and in the Iterative Self-Organizing Data Analysis Technique (ISODATA), it is common to perform per-band standardization (also called $z$-score normalization) prior to clustering, to mitigate dominance by high-variance spectral bands. Define the per-band empirical mean $\\mathbf{m} \\in \\mathbb{R}^{p}$ with components $m_{d} = \\frac{1}{N} \\sum_{i=1}^{N} x_{i d}$ and the per-band empirical standard deviations $\\boldsymbol{\\sigma} \\in \\mathbb{R}^{p}$ with components $\\sigma_{d} = \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} (x_{i d} - m_{d})^{2}}$, for $d \\in \\{1,\\dots,p\\}$. Form the diagonal scaling matrix $\\mathbf{D} = \\mathrm{diag}(\\sigma_{1},\\dots,\\sigma_{p})$ and the diagonal variance matrix $\\mathbf{S} = \\mathrm{diag}(\\sigma_{1}^{2},\\dots,\\sigma_{p}^{2})$.\n\nDefine the standardized data $\\mathbf{z}_{i} \\in \\mathbb{R}^{p}$ by $\\mathbf{z}_{i} = \\mathbf{D}^{-1}(\\mathbf{x}_{i} - \\mathbf{m})$. Let the $k$-means objective computed in this standardized space be\n$$\nJ^{(z)} \\;=\\; \\sum_{k=1}^{K} \\sum_{i \\in C_{k}} \\left\\| \\mathbf{z}_{i} - \\boldsymbol{\\nu}_{k} \\right\\|_{2}^{2},\n$$\nwhere $\\boldsymbol{\\nu}_{k}$ is the centroid of cluster $k$ in the standardized space.\n\nStarting only from these definitions, derive $J^{(z)}$ explicitly in terms of the original-space quantities $\\mathbf{x}_{i}$, $\\boldsymbol{\\mu}_{k}$, and the per-band variances encoded in $\\mathbf{S}$. Assume the cluster assignments $\\{C_{k}\\}_{k=1}^{K}$ are fixed and $\\boldsymbol{\\mu}_{k}$ is the empirical mean of $\\{\\mathbf{x}_{i} : i \\in C_{k}\\}$. Then, interpret how per-band $z$-score normalization alters the implicit weighting of within-cluster variance across bands in $J^{(z)}$ compared to $J$, and explain the connection to a diagonal Mahalanobis metric and its implications for band dominance in remote sensing imagery.\n\nProvide your final answer as a single closed-form analytic expression for $J^{(z)}$ written in terms of $\\mathbf{x}_{i}$, $\\boldsymbol{\\mu}_{k}$, and $\\mathbf{S}$ only. No numerical evaluation is required, and no units are needed for the final expression.",
            "solution": "The problem statement is evaluated to be valid. It is scientifically grounded in the principles of unsupervised machine learning and remote sensing data analysis, is well-posed with clear definitions and objectives, and is free of subjective or ambiguous language. All necessary information is provided for a rigorous derivation and interpretation.\n\nThe task is to derive an expression for the $k$-means objective function $J^{(z)}$ computed on standardized data, and to interpret its meaning. The derivation proceeds as follows.\n\nFirst, we express the centroid $\\boldsymbol{\\nu}_k$ of a cluster $C_k$ in the standardized space in terms of the original-space quantities. By definition, the centroid is the mean of the data points within the cluster. Let $|C_k|$ be the number of pixels in cluster $k$.\n$$\n\\boldsymbol{\\nu}_{k} = \\frac{1}{|C_k|} \\sum_{i \\in C_k} \\mathbf{z}_{i}\n$$\nSubstituting the definition of the standardized data, $\\mathbf{z}_{i} = \\mathbf{D}^{-1}(\\mathbf{x}_{i} - \\mathbf{m})$:\n$$\n\\boldsymbol{\\nu}_{k} = \\frac{1}{|C_k|} \\sum_{i \\in C_k} \\mathbf{D}^{-1}(\\mathbf{x}_{i} - \\mathbf{m})\n$$\nSince the scaling matrix $\\mathbf{D}^{-1}$ and the global mean vector $\\mathbf{m}$ are constant for all pixels $i$ in the cluster, we can move them outside the summation:\n$$\n\\boldsymbol{\\nu}_{k} = \\mathbf{D}^{-1} \\left( \\left( \\frac{1}{|C_k|} \\sum_{i \\in C_k} \\mathbf{x}_{i} \\right) - \\left( \\frac{1}{|C_k|} \\sum_{i \\in C_k} \\mathbf{m} \\right) \\right)\n$$\nThe first term in the parentheses is the definition of the original-space cluster centroid, $\\boldsymbol{\\mu}_{k} = \\frac{1}{|C_k|} \\sum_{i \\in C_k} \\mathbf{x}_{i}$. The second term simplifies to $\\frac{|C_k|}{|C_k|}\\mathbf{m} = \\mathbf{m}$. Therefore, the standardized centroid $\\boldsymbol{\\nu}_k$ is related to the original-space centroid $\\boldsymbol{\\mu}_k$ by the same transformation applied to the data points:\n$$\n\\boldsymbol{\\nu}_{k} = \\mathbf{D}^{-1}(\\boldsymbol{\\mu}_{k} - \\mathbf{m})\n$$\nNow we can evaluate the term inside the summation for the standardized objective function $J^{(z)}$:\n$$\n\\mathbf{z}_{i} - \\boldsymbol{\\nu}_{k} = \\mathbf{D}^{-1}(\\mathbf{x}_{i} - \\mathbf{m}) - \\mathbf{D}^{-1}(\\boldsymbol{\\mu}_{k} - \\mathbf{m}) = \\mathbf{D}^{-1} \\left( (\\mathbf{x}_{i} - \\mathbf{m}) - (\\boldsymbol{\\mu}_{k} - \\mathbf{m}) \\right) = \\mathbf{D}^{-1}(\\mathbf{x}_{i} - \\boldsymbol{\\mu}_{k})\n$$\nThe squared $L_2$ norm of this vector is then:\n$$\n\\left\\| \\mathbf{z}_{i} - \\boldsymbol{\\nu}_{k} \\right\\|_{2}^{2} = \\left\\| \\mathbf{D}^{-1}(\\mathbf{x}_{i} - \\boldsymbol{\\mu}_{k}) \\right\\|_{2}^{2}\n$$\nTo express this in terms of the matrix $\\mathbf{S}$, we use the quadratic form representation of the squared norm, $\\|\\mathbf{v}\\|_{2}^{2} = \\mathbf{v}^{T}\\mathbf{v}$.\n$$\n\\left\\| \\mathbf{D}^{-1}(\\mathbf{x}_{i} - \\boldsymbol{\\mu}_{k}) \\right\\|_{2}^{2} = \\left( \\mathbf{D}^{-1}(\\mathbf{x}_{i} - \\boldsymbol{\\mu}_{k}) \\right)^{T} \\left( \\mathbf{D}^{-1}(\\mathbf{x}_{i} - \\boldsymbol\\mu_{k}) \\right)\n$$\nUsing the transpose property $(AB)^T = B^T A^T$:\n$$\n= (\\mathbf{x}_{i} - \\boldsymbol{\\mu}_{k})^{T} (\\mathbf{D}^{-1})^{T} \\mathbf{D}^{-1} (\\mathbf{x}_{i} - \\boldsymbol{\\mu}_{k})\n$$\nThe matrix $\\mathbf{D} = \\mathrm{diag}(\\sigma_{1},\\dots,\\sigma_{p})$ is diagonal, hence its inverse $\\mathbf{D}^{-1}$ is also diagonal. Diagonal matrices are symmetric, so $(\\mathbf{D}^{-1})^{T} = \\mathbf{D}^{-1}$. The expression becomes:\n$$\n= (\\mathbf{x}_{i} - \\boldsymbol{\\mu}_{k})^{T} (\\mathbf{D}^{-1})^2 (\\mathbf{x}_{i} - \\boldsymbol{\\mu}_{k}) = (\\mathbf{x}_{i} - \\boldsymbol{\\mu}_{k})^{T} (\\mathbf{D}^2)^{-1} (\\mathbf{x}_{i} - \\boldsymbol{\\mu}_{k})\n$$\nGiven the definition $\\mathbf{S} = \\mathrm{diag}(\\sigma_{1}^{2},\\dots,\\sigma_{p}^{2}) = \\mathbf{D}^2$, we have $(\\mathbf{D}^2)^{-1} = \\mathbf{S}^{-1}$. Thus:\n$$\n\\left\\| \\mathbf{z}_{i} - \\boldsymbol{\\nu}_{k} \\right\\|_{2}^{2} = (\\mathbf{x}_{i} - \\boldsymbol{\\mu}_{k})^{T} \\mathbf{S}^{-1} (\\mathbf{x}_{i} - \\boldsymbol{\\mu}_{k})\n$$\nFinally, substituting this back into the definition of $J^{(z)}$ yields the desired expression:\n$$\nJ^{(z)} \\;=\\; \\sum_{k=1}^{K} \\sum_{i \\in C_{k}} (\\mathbf{x}_{i} - \\boldsymbol{\\mu}_{k})^{T} \\mathbf{S}^{-1} (\\mathbf{x}_{i} - \\boldsymbol{\\mu}_{k})\n$$\nThis expression reveals how per-band standardization alters the clustering objective. The standard $k$-means objective, $J = \\sum \\sum \\| \\mathbf{x}_{i} - \\boldsymbol{\\mu}_{k} \\|_{2}^{2}$, implicitly uses the squared Euclidean distance, which can be written with an identity matrix in the quadratic form: $(\\mathbf{x}_{i} - \\boldsymbol{\\mu}_{k})^{T} \\mathbf{I} (\\mathbf{x}_{i} - \\boldsymbol{\\mu}_{k})$. This weights each spectral band equally.\n\nIn contrast, the objective function $J^{(z)}$ for standardized data is equivalent to minimizing a sum of squared distances in the original data space, but with the distance metric defined by the matrix $\\mathbf{S}^{-1}$. Since $\\mathbf{S} = \\mathrm{diag}(\\sigma_{1}^{2},\\dots,\\sigma_{p}^{2})$, its inverse is $\\mathbf{S}^{-1} = \\mathrm{diag}(1/\\sigma_{1}^{2},\\dots,1/\\sigma_{p}^{2})$. The contribution of each band $d$ to the distance is now scaled by the inverse of its global variance, $1/\\sigma_{d}^2$. Specifically, the squared error for a single pixel $i$ in band $d$ becomes $\\frac{(x_{id} - \\mu_{kd})^2}{\\sigma_d^2}$.\n\nThis has a critical implication for band dominance in remote sensing. Spectral bands with high variance (large $\\sigma_d^2$) tend to have larger numerical values for squared differences and thus dominate the standard Euclidean distance calculation and the objective $J$. The clustering algorithm would preferentially form clusters that reduce variance in these high-variance bands, often ignoring the information in lower-variance bands. By standardizing the data, we introduce the factor $1/\\sigma_d^2$, which down-weights the contribution of high-variance bands and up-weights the contribution of low-variance bands. This effectively normalizes the influence of each band on the clustering process, leading to partitions that are sensitive to patterns across all bands, not just the most variable ones.\n\nThis formulation connects directly to the Mahalanobis distance. The squared Mahalanobis distance from a point $\\mathbf{x}$ to a mean $\\boldsymbol{\\mu}$ given a covariance matrix $\\mathbf{\\Sigma}$ is $(\\mathbf{x}-\\boldsymbol{\\mu})^{T}\\mathbf{\\Sigma}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu})$. The objective $J^{(z)}$ is a sum of squared Mahalanobis distances between each point $\\mathbf{x}_i$ and its cluster centroid $\\boldsymbol{\\mu}_k$. The covariance matrix used is $\\mathbf{S}$, a diagonal matrix of the global per-band variances. This specific form, a diagonal Mahalanobis distance, assumes that the spectral bands are uncorrelated. Clustering on z-standardized data is therefore mathematically equivalent to performing $k$-means on the original data using this specific Mahalanobis distance instead of Euclidean distance. This provides a formal justification for standardization as a method to account for differing scales and variances among features.",
            "answer": "$$\n\\boxed{\\sum_{k=1}^{K} \\sum_{i \\in C_{k}} (\\mathbf{x}_{i} - \\boldsymbol{\\mu}_{k})^{T} \\mathbf{S}^{-1} (\\mathbf{x}_{i} - \\boldsymbol{\\mu}_{k})}\n$$"
        },
        {
            "introduction": "Building upon the concept of weighted distances, we now address a common practical challenge in remote sensing: outliers. While global standardization adjusts for differing band variances, it does not handle localized anomalies like pixels contaminated by cloud or terrain shadow. This advanced exercise introduces a robust weighting scheme based on the Huber influence function, allowing the algorithm to selectively down-weight the influence of outliers on a per-band basis. You will design and tune this robust system for a realistic scenario involving shadow-affected vegetation, bridging the gap between theoretical algorithms and practical, high-fidelity environmental modeling .",
            "id": "3863385",
            "problem": "A push-broom imaging spectrometer acquires three reflectance bands over a vegetated watershed: blue $B$, red $R$, and Near Infrared (NIR). You will design a robust, multi-band weighting for unsupervised clustering with K-means and the Iterative Self-Organizing Data Analysis Technique Algorithm (ISODATA) that mitigates the influence of shadow pixels, which are outliers in the visible bands but not in NIR.\n\nAssume the current K-means vegetation prototype (cluster center) has reflectance means $\\boldsymbol{\\mu}_{\\text{veg}} = [\\mu_B, \\mu_R, \\mu_N] = [0.060, 0.040, 0.480]$. Empirically estimated inlier scales (per-band standard deviations for in-cluster distances) are $\\boldsymbol{s} = [s_B, s_R, s_N] = [0.008, 0.007, 0.050]$. A subset of pixels affected by cast shadows on vegetation has observed reflectance $\\boldsymbol{x}_{\\text{sh}} = [x_B, x_R, x_N] = [0.020, 0.015, 0.450]$. All reflectances are unitless fractions in $[0,1]$.\n\nYou adopt a bandwise Huber-type robust weighting for the K-means assignment and ISODATA variance computation. For each band $b \\in \\{B,R,N\\}$, define the residual $r_b = x_b - \\mu_b$, the standardized residual $u_b = r_b / s_b$, and the Huber influence function with a common threshold parameter $\\kappa > 0$ given by\n$$\n\\psi_{\\kappa}(u) = \n\\begin{cases}\nu, & |u| \\le \\kappa,\\\\\n\\kappa\\,\\mathrm{sign}(u), & |u| > \\kappa.\n\\end{cases}\n$$\nThe corresponding robust weight is $w_b(\\kappa) = \\psi_{\\kappa}(u_b)/u_b = \\min\\{1,\\kappa/|u_b|\\}$ for $u_b \\neq 0$ and $w_b(\\kappa)=1$ if $u_b=0$. This yields a robust, multi-band, scaled squared distance for assignment,\n$$\nD_{\\kappa}^2(\\boldsymbol{x},\\boldsymbol{\\mu}) = \\sum_{b \\in \\{B,R,N\\}} \\left[w_b(\\kappa)\\,\\frac{r_b}{s_b}\\right]^2,\n$$\nand the same weights $w_b(\\kappa)$ are used in computing weighted within-cluster scatter for ISODATA split-merge decisions.\n\nYour design goal is to choose a single $\\kappa$ (common across bands) that enforces the following for the given shadow pixel relative to the vegetation prototype: the visible bands $B$ and $R$ should be in the saturated regime $|u_b| > \\kappa$ (so their influence is bounded), while NIR should be in the linear regime $|u_N| < \\kappa$ (so it remains informative). Among all $\\kappa$ that satisfy these regime constraints, you will choose the one that maximizes the symmetric separation margin between $\\kappa$ and the two regime boundaries by solving the following maximin problem:\n$$\n\\max_{\\kappa > 0} \\;\\; \\min\\!\\left\\{ \\frac{\\kappa}{|u_N|}, \\; \\frac{\\min\\{|u_B|,|u_R|\\}}{\\kappa} \\right\\}\n\\quad \\text{subject to} \\quad |u_N| < \\kappa < \\min\\{|u_B|,|u_R|\\}.\n$$\n\nCompute the value of $\\kappa$ that solves this problem for the given $\\boldsymbol{\\mu}_{\\text{veg}}$, $\\boldsymbol{s}$, and $\\boldsymbol{x}_{\\text{sh}}$. Express your final answer rounded to four significant figures.",
            "solution": "The problem presents a task of designing a weighting parameter $\\kappa$ for a robust unsupervised classification algorithm in remote sensing. Before proceeding to a solution, the problem statement is validated.\n\n**Problem Validation**\n1.  **Givens Extraction**:\n    -   Bands: Blue ($B$), Red ($R$), Near Infrared ($N$).\n    -   Vegetation prototype mean reflectance: $\\boldsymbol{\\mu}_{\\text{veg}} = [\\mu_B, \\mu_R, \\mu_N] = [0.060, 0.040, 0.480]$.\n    -   Inlier scales (standard deviations): $\\boldsymbol{s} = [s_B, s_R, s_N] = [0.008, 0.007, 0.050]$.\n    -   Shadow pixel reflectance: $\\boldsymbol{x}_{\\text{sh}} = [x_B, x_R, x_N] = [0.020, 0.015, 0.450]$.\n    -   Definitions: $r_b = x_b - \\mu_b$, $u_b = r_b / s_b$.\n    -   Optimization problem: $\\max_{\\kappa > 0} \\;\\; \\min\\!\\left\\{ \\frac{\\kappa}{|u_N|}, \\; \\frac{\\min\\{|u_B|,|u_R|\\}}{\\kappa} \\right\\}$ subject to $|u_N| < \\kappa < \\min\\{|u_B|,|u_R|\\}$.\n\n2.  **Validation Check**:\n    -   **Scientific Grounding**: The problem is well-grounded in robust statistics and remote sensing. The use of a Huber-type influence function to down-weight outliers (shadow pixels) is a standard technique. The physical premise that shadows significantly reduce reflectance in visible bands ($B$, $R$) more than in the NIR band is correct for vegetated surfaces.\n    -   **Well-Posedness**: All necessary data and definitions are provided. The optimization problem is clearly stated and structured to yield a unique, meaningful solution.\n    -   **Objectivity**: The problem is formulated in precise mathematical and technical terms, free of subjective or ambiguous language.\n\n3.  **Verdict**: The problem is valid. It is a well-defined mathematical optimization problem based on sound scientific principles. A solution can be derived.\n\n**Solution Derivation**\n\nThe goal is to compute the optimal value of the threshold parameter $\\kappa$. The first step is to calculate the standardized residuals, $u_b$, for the shadow pixel $\\boldsymbol{x}_{\\text{sh}}$ with respect to the vegetation prototype $\\boldsymbol{\\mu}_{\\text{veg}}$ using the given scales $\\boldsymbol{s}$.\n\nFor each band $b \\in \\{B, R, N\\}$, the residual is $r_b = x_b - \\mu_b$, and the standardized residual is $u_b = r_b / s_b$.\n\nFor the blue band ($B$):\n$$\nr_B = x_B - \\mu_B = 0.020 - 0.060 = -0.040\n$$\n$$\nu_B = \\frac{r_B}{s_B} = \\frac{-0.040}{0.008} = -5\n$$\n\nFor the red band ($R$):\n$$\nr_R = x_R - \\mu_R = 0.015 - 0.040 = -0.025\n$$\n$$\nu_R = \\frac{r_R}{s_R} = \\frac{-0.025}{0.007} = -\\frac{25}{7}\n$$\n\nFor the Near Infrared band ($N$):\n$$\nr_N = x_N - \\mu_N = 0.450 - 0.480 = -0.030\n$$\n$$\nu_N = \\frac{r_N}{s_N} = \\frac{-0.030}{0.050} = -0.6\n$$\n\nThe optimization problem requires the absolute values of these standardized residuals:\n$$\n|u_B| = 5\n$$\n$$\n|u_R| = \\frac{25}{7} \\approx 3.5714\n$$\n$$\n|u_N| = 0.6\n$$\n\nThe optimization is subject to the constraint $|u_N| < \\kappa < \\min\\{|u_B|,|u_R|\\}$. Let's define the lower bound $L$ and upper bound $U$ for this constraint.\nThe lower bound is $L = |u_N| = 0.6$.\nThe upper bound is $U = \\min\\{|u_B|, |u_R|\\} = \\min\\left\\{5, \\frac{25}{7}\\right\\}$. Since $5 = \\frac{35}{7}$, the minimum is $\\frac{25}{7}$. Thus, $U = \\frac{25}{7}$.\nThe constraint on $\\kappa$ is $0.6 < \\kappa < \\frac{25}{7}$.\n\nThe problem is to solve:\n$$\n\\max_{\\kappa} \\min\\left\\{ \\frac{\\kappa}{L}, \\frac{U}{\\kappa} \\right\\}\n$$\nThe function $f(\\kappa) = \\min\\left\\{ \\frac{\\kappa}{L}, \\frac{U}{\\kappa} \\right\\}$ is being maximized. The term $\\frac{\\kappa}{L}$ increases with $\\kappa$, while the term $\\frac{U}{\\kappa}$ decreases with $\\kappa$. The maximum of the minimum of these two functions occurs where they are equal:\n$$\n\\frac{\\kappa}{L} = \\frac{U}{\\kappa}\n$$\nSolving for $\\kappa$ (where $\\kappa > 0$):\n$$\n\\kappa^2 = L \\cdot U\n$$\n$$\n\\kappa = \\sqrt{L \\cdot U}\n$$\nThis solution is the geometric mean of the boundaries $L$ and $U$. It is guaranteed to lie within the interval $(L, U)$ provided $L < U$, which is true in this case ($0.6 < \\frac{25}{7}$).\n\nNow, we substitute the numerical values for $L$ and $U$ to compute $\\kappa$.\n$$\nL = 0.6 = \\frac{3}{5}\n$$\n$$\nU = \\frac{25}{7}\n$$\n$$\n\\kappa = \\sqrt{\\left(\\frac{3}{5}\\right) \\cdot \\left(\\frac{25}{7}\\right)} = \\sqrt{\\frac{3 \\cdot 25}{5 \\cdot 7}} = \\sqrt{\\frac{3 \\cdot 5}{7}} = \\sqrt{\\frac{15}{7}}\n$$\nTo obtain the final answer, we calculate the numerical value and round to four significant figures as specified.\n$$\n\\kappa = \\sqrt{\\frac{15}{7}} \\approx \\sqrt{2.14285714...} \\approx 1.463850109...\n$$\nRounding to four significant figures yields $\\kappa \\approx 1.464$. This value lies within the constraint interval $(0.6, \\frac{25}{7})$, confirming its validity.",
            "answer": "$$\\boxed{1.464}$$"
        }
    ]
}