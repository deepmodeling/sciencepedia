{
    "hands_on_practices": [
        {
            "introduction": "At the heart of the Classification and Regression Trees (CART) algorithm is the principle of recursive partitioning, where data is repeatedly split to create increasingly pure subsets. To guide this process, CART employs the Gini impurity as a measure of node heterogeneity. This hands-on practice  will allow you to compute this core metric and evaluate the quality of a potential split by calculating the Gini impurity decrease, which is the primary criterion used by the algorithm to select the optimal cut-point for a feature.",
            "id": "3805182",
            "problem": "A land-cover mapping workflow for environmental modeling uses a Classification and Regression Trees (CART) classifier to partition a remote sensing image into semantic classes. Consider a single internal node $S$ containing $120$ pixels, with class counts forest $=60$, cropland $=40$, and water $=20$. A proposed axis-aligned split on a spectral feature produces a left child with counts $(50,30,10)$ and a right child with counts $(10,10,10)$.\n\nUsing the standard multi-class impurity criterion employed by CART, compute the impurity $G(S)$ of the parent node and the impurity decrease induced by the split, defined as the parent impurity minus the post-split impurity weighted by child sizes. Express both quantities as exact reduced fractions. Do not round. The final answer must present the two values in a single row, in the order $G(S)$, impurity decrease.",
            "solution": "The problem is first validated to ensure it is self-contained, scientifically sound, and well-posed.\n\n**Step 1: Extract Givens**\n- Parent node $S$ contains $N = 120$ pixels.\n- Class counts in $S$: $n_{\\text{forest}} = 60$, $n_{\\text{cropland}} = 40$, $n_{\\text{water}} = 20$.\n- A split produces a left child $S_L$ and a right child $S_R$.\n- Class counts in $S_L$: $n_{\\text{forest},L} = 50$, $n_{\\text{cropland},L} = 30$, $n_{\\text{water},L} = 10$.\n- Class counts in $S_R$: $n_{\\text{forest},R} = 10$, $n_{\\text{cropland},R} = 10$, $n_{\\text{water},R} = 10$.\n- The impurity criterion is the standard one for CART, which is the Gini impurity.\n- The impurity decrease is defined as the parent impurity minus the weighted post-split impurity.\n- The required output is the impurity of the parent node, $G(S)$, and the impurity decrease, $\\Delta G(S)$, as exact reduced fractions.\n\n**Step 2: Validate Using Extracted Givens**\n- The total number of pixels in the parent node is $60 + 40 + 20 = 120$, which is consistent.\n- The total number of pixels in the left child is $N_L = 50 + 30 + 10 = 90$.\n- The total number of pixels in the right child is $N_R = 10 + 10 + 10 = 30$.\n- The pixel conservation check yields $N_L + N_R = 90 + 30 = 120 = N$, which is correct.\n- The class count conservation check for each class is also satisfied:\n  - Forest: $50 + 10 = 60$.\n  - Cropland: $30 + 10 = 40$.\n  - Water: $10 + 10 = 20$.\n- The problem is scientifically grounded in decision tree theory, is numerically consistent, and is well-posed. No flaws are detected.\n\n**Step 3: Verdict and Action**\n- The problem is valid. The solution proceeds.\n\nThe standard impurity criterion for Classification and Regression Trees (CART) is the Gini impurity. For a node $T$ containing samples from $K$ classes, the Gini impurity $G(T)$ is defined as:\n$$G(T) = \\sum_{k=1}^{K} p_k(1-p_k) = 1 - \\sum_{k=1}^{K} p_k^2$$\nwhere $p_k$ is the proportion of samples of class $k$ in node $T$.\n\nFirst, we compute the Gini impurity of the parent node $S$, denoted $G(S)$. The node $S$ has a total of $N_S = 120$ pixels. The proportions $p_k$ for the three classes (forest, cropland, water) are:\n- $p_{\\text{forest},S} = \\frac{60}{120} = \\frac{1}{2}$\n- $p_{\\text{cropland},S} = \\frac{40}{120} = \\frac{1}{3}$\n- $p_{\\text{water},S} = \\frac{20}{120} = \\frac{1}{6}$\n\nThe Gini impurity $G(S)$ is:\n$$G(S) = 1 - \\left( p_{\\text{forest},S}^2 + p_{\\text{cropland},S}^2 + p_{\\text{water},S}^2 \\right)$$\n$$G(S) = 1 - \\left( \\left(\\frac{1}{2}\\right)^2 + \\left(\\frac{1}{3}\\right)^2 + \\left(\\frac{1}{6}\\right)^2 \\right)$$\n$$G(S) = 1 - \\left( \\frac{1}{4} + \\frac{1}{9} + \\frac{1}{36} \\right)$$\nTo sum the fractions, we find a common denominator, which is $36$:\n$$G(S) = 1 - \\left( \\frac{9}{36} + \\frac{4}{36} + \\frac{1}{36} \\right) = 1 - \\frac{9+4+1}{36} = 1 - \\frac{14}{36}$$\nReducing the fraction gives:\n$$G(S) = 1 - \\frac{7}{18} = \\frac{11}{18}$$\n\nNext, we compute the impurity decrease. The impurity decrease, $\\Delta G$, is defined as the impurity of the parent minus the weighted average of the impurities of the children nodes:\n$$\\Delta G = G(S) - \\left( \\frac{N_L}{N_S} G(S_L) + \\frac{N_R}{N_S} G(S_R) \\right)$$\nWe must first calculate the impurities of the left child, $G(S_L)$, and the right child, $G(S_R)$.\n\nFor the left child $S_L$, the total number of pixels is $N_L = 90$. The class proportions are:\n- $p_{\\text{forest},L} = \\frac{50}{90} = \\frac{5}{9}$\n- $p_{\\text{cropland},L} = \\frac{30}{90} = \\frac{3}{9} = \\frac{1}{3}$\n- $p_{\\text{water},L} = \\frac{10}{90} = \\frac{1}{9}$\nThe Gini impurity $G(S_L)$ is:\n$$G(S_L) = 1 - \\left( \\left(\\frac{5}{9}\\right)^2 + \\left(\\frac{1}{3}\\right)^2 + \\left(\\frac{1}{9}\\right)^2 \\right)$$\n$$G(S_L) = 1 - \\left( \\frac{25}{81} + \\frac{1}{9} + \\frac{1}{81} \\right) = 1 - \\left( \\frac{25}{81} + \\frac{9}{81} + \\frac{1}{81} \\right)$$\n$$G(S_L) = 1 - \\frac{25+9+1}{81} = 1 - \\frac{35}{81} = \\frac{46}{81}$$\n\nFor the right child $S_R$, the total number of pixels is $N_R = 30$. The class proportions are:\n- $p_{\\text{forest},R} = \\frac{10}{30} = \\frac{1}{3}$\n- $p_{\\text{cropland},R} = \\frac{10}{30} = \\frac{1}{3}$\n- $p_{\\text{water},R} = \\frac{10}{30} = \\frac{1}{3}$\nThe Gini impurity $G(S_R)$ is:\n$$G(S_R) = 1 - \\left( \\left(\\frac{1}{3}\\right)^2 + \\left(\\frac{1}{3}\\right)^2 + \\left(\\frac{1}{3}\\right)^2 \\right)$$\n$$G(S_R) = 1 - 3 \\times \\left(\\frac{1}{9}\\right) = 1 - \\frac{3}{9} = 1 - \\frac{1}{3} = \\frac{2}{3}$$\n\nNow we can compute the weighted impurity of the split:\n$$G_{\\text{split}} = \\frac{N_L}{N_S} G(S_L) + \\frac{N_R}{N_S} G(S_R)$$\nThe weights are $w_L = \\frac{N_L}{N_S} = \\frac{90}{120} = \\frac{3}{4}$ and $w_R = \\frac{N_R}{N_S} = \\frac{30}{120} = \\frac{1}{4}$.\n$$G_{\\text{split}} = \\left(\\frac{3}{4}\\right) \\left(\\frac{46}{81}\\right) + \\left(\\frac{1}{4}\\right) \\left(\\frac{2}{3}\\right)$$\n$$G_{\\text{split}} = \\frac{1 \\times 46}{4 \\times 27} + \\frac{2}{12} = \\frac{46}{108} + \\frac{1}{6}$$\nTo add these, we use the common denominator $108$:\n$$G_{\\text{split}} = \\frac{46}{108} + \\frac{18}{108} = \\frac{64}{108}$$\nReducing the fraction by dividing the numerator and denominator by $4$:\n$$G_{\\text{split}} = \\frac{16}{27}$$\n\nFinally, we compute the impurity decrease, $\\Delta G$:\n$$\\Delta G = G(S) - G_{\\text{split}} = \\frac{11}{18} - \\frac{16}{27}$$\nThe least common multiple of $18$ and $27$ is $54$.\n$$\\Delta G = \\frac{11 \\times 3}{18 \\times 3} - \\frac{16 \\times 2}{27 \\times 2} = \\frac{33}{54} - \\frac{32}{54} = \\frac{1}{54}$$\n\nThe parent impurity $G(S)$ is $\\frac{11}{18}$, and the impurity decrease is $\\frac{1}{54}$.",
            "answer": "$$\\boxed{\\begin{pmatrix} \\frac{11}{18} & \\frac{1}{54} \\end{pmatrix}}$$"
        },
        {
            "introduction": "While growing a decision tree until every leaf is perfectly pure can perfectly model the training data, it often leads to overfitting, creating a complex model that fails to generalize to new data. To combat this, CART employs a technique called cost-complexity pruning. This exercise  demonstrates this elegant method, where you will identify a sequence of \"weakest links\" in the tree and determine the critical values of the complexity parameter $\\alpha$ at which it becomes optimal to prune them, ultimately producing a nested sequence of robust subtrees.",
            "id": "3805108",
            "problem": "A land-cover classification decision tree was trained on a labeled set of satellite pixels to distinguish major classes (urban, vegetation, water, bare soil) using multispectral thresholds. The method is Classification and Regression Trees (CART). In this training, each terminal leaf assigns the majority class within its region and incurs a resubstitution misclassification count. The following small tree has five terminal leaves with resubstitution errors $\\left(e_{1}, e_{2}, e_{3}, e_{4}, e_{5}\\right)$ and four internal nodes whose aggregated errors (the misclassification count if the entire node’s region were labeled by a single majority class) are given.\n\nThe topology is: the root node $U$ splits into a left internal node $A$ and a right internal node $D$; node $A$ splits into leaves $1$ and $2$; node $D$ splits into an internal node $B$ and leaf $5$; and node $B$ splits into leaves $3$ and $4$.\n\nThe resubstitution errors of the leaves are\n$$\ne_{1} = 12,\\quad e_{2} = 8,\\quad e_{3} = 15,\\quad e_{4} = 10,\\quad e_{5} = 6.\n$$\nThe aggregated errors at the internal nodes, if each were pruned to a single leaf with a majority label over its region, are\n$$\nE_{A} = 24,\\quad E_{B} = 28,\\quad E_{D} = 40,\\quad E_{U} = 72.\n$$\n\nAssume cost-complexity pruning is used, with a complexity penalty parameter $\\alpha \\geq 0$, and trees are selected by minimizing a penalized empirical risk that sums the resubstitution misclassification counts over leaves plus a linear penalty in the number of leaves. Using first-principles reasoning from structural risk minimization, determine the pruning path by identifying the sequence of critical penalty values at which the optimal subtree changes and the corresponding $\\alpha$-intervals for which each subtree is optimal.\n\nReport the sequence of critical penalty values in increasing order as a single row matrix. Give exact values (do not round).",
            "solution": "The problem is well-posed, scientifically grounded, and provides all necessary information for a unique solution. The data are internally consistent. Therefore, a solution will be derived.\n\nThe problem requires a determination of the pruning path for a given decision tree using cost-complexity pruning. This method, central to the Classification and Regression Trees (CART) algorithm, aims to find a sequence of subtrees that are optimal for a continuous range of a complexity penalty parameter, $\\alpha$.\n\nThe cost-complexity of a tree $T$ is defined as:\n$$\nC_{\\alpha}(T) = R(T) + \\alpha |T_{\\text{leaves}}|\n$$\nwhere $R(T)$ is the total resubstitution misclassification count, which is the sum of errors of all terminal leaves in the tree $T$, and $|T_{\\text{leaves}}|$ is the number of terminal leaves in $T$. The parameter $\\alpha \\ge 0$ penalizes the complexity of the tree, measured by the number of its leaves.\n\nFor any internal node $t$ in a tree, we can consider the subtree $T_t$ rooted at $t$. The cost-complexity of this subtree is $C_{\\alpha}(T_t) = R(T_t) + \\alpha |(T_t)_{\\text{leaves}}|$. If we were to prune this subtree, node $t$ would become a terminal leaf. The cost-complexity of this pruned state is $C_{\\alpha}(\\{t\\}) = E_t + \\alpha \\cdot 1$, where $E_t$ is the aggregated misclassification count for the region corresponding to node $t$.\n\nA node $t$ is a candidate for pruning when the cost of making it a leaf becomes less than or equal to the cost of the unpruned subtree rooted at $t$:\n$$\nE_t + \\alpha \\le R(T_t) + \\alpha |(T_t)_{\\text{leaves}}|\n$$\nThe critical value of $\\alpha$ at which the costs are equal defines the point where node $t$ becomes the \"weakest link\" in the tree. Rearranging the equation gives the critical complexity parameter for node $t$:\n$$\n\\alpha_t = \\frac{E_t - R(T_t)}{|(T_t)_{\\text{leaves}}| - 1}\n$$\nThe cost-complexity pruning algorithm proceeds iteratively. Starting with the full tree, we calculate $\\alpha_t$ for every internal node. The node with the smallest $\\alpha_t$ is pruned first. This process is repeated on the newly pruned tree until only the root node remains.\n\nLet us denote the initial, full tree as $T_0$. Its topology consists of internal nodes $U, A, D, B$ and terminal leaves $1, 2, 3, 4, 5$.\nThe errors of the leaves are given as $e_{1} = 12$, $e_{2} = 8$, $e_{3} = 15$, $e_{4} = 10$, and $e_{5} = 6$.\nThe aggregated errors for internal nodes are $E_{A} = 24$, $E_{B} = 28$, $E_{D} = 40$, and $E_{U} = 72$.\n\nFirst, we must compute the resubstitution error $R(T_t)$ and the number of leaves $|(T_t)_{\\text{leaves}}|$ for the subtrees rooted at each internal node of the full tree $T_0$.\n\nFor node $A$:\nThe subtree $T_A$ has leaves $1$ and $2$.\n$R(T_A) = e_1 + e_2 = 12 + 8 = 20$.\n$|(T_A)_{\\text{leaves}}| = 2$.\n\nFor node $B$:\nThe subtree $T_B$ has leaves $3$ and $4$.\n$R(T_B) = e_3 + e_4 = 15 + 10 = 25$.\n$|(T_B)_{\\text{leaves}}| = 2$.\n\nFor node $D$:\nThe subtree $T_D$ has an internal node $B$ and a leaf $5$. Its leaves are the leaves of $T_B$ and leaf $5$.\n$R(T_D) = R(T_B) + e_5 = 25 + 6 = 31$.\n$|(T_D)_{\\text{leaves}}| = |(T_B)_{\\text{leaves}}| + 1 = 2 + 1 = 3$.\n\nFor the root node $U$:\nThe full tree $T_U = T_0$ has internal nodes $A$ and $D$. Its leaves are the union of leaves from $T_A$ and $T_D$.\n$R(T_U) = R(T_A) + R(T_D) = 20 + 31 = 51$.\n$|(T_U)_{\\text{leaves}}| = |(T_A)_{\\text{leaves}}| + |(T_D)_{\\text{leaves}}| = 2 + 3 = 5$.\n\nNow, we can compute the critical $\\alpha$ values for all internal nodes in $T_0$.\n\n$\\alpha_A = \\frac{E_A - R(T_A)}{|(T_A)_{\\text{leaves}}| - 1} = \\frac{24 - 20}{2 - 1} = \\frac{4}{1} = 4$.\n$\\alpha_B = \\frac{E_B - R(T_B)}{|(T_B)_{\\text{leaves}}| - 1} = \\frac{28 - 25}{2 - 1} = \\frac{3}{1} = 3$.\n$\\alpha_D = \\frac{E_D - R(T_D)}{|(T_D)_{\\text{leaves}}| - 1} = \\frac{40 - 31}{3 - 1} = \\frac{9}{2} = 4.5$.\n$\\alpha_U = \\frac{E_U - R(T_U)}{|(T_U)_{\\text{leaves}}| - 1} = \\frac{72 - 51}{5 - 1} = \\frac{21}{4} = 5.25$.\n\nThe minimum of these values is $\\alpha_B = 3$. Thus, the first critical penalty value is $\\alpha_{(1)} = 3$. For $\\alpha \\in [0, 3)$, the full tree $T_0$ is the optimal subtree. At $\\alpha = 3$, we prune node $B$.\n\nLet $T_1$ be the tree obtained by pruning node $B$. In $T_1$, $B$ becomes a leaf (let's denote it $B'$) with error $R(B') = E_B = 28$.\nThe internal nodes of $T_1$ are $A, D, U$. The leaves are $\\{1, 2, B', 5\\}$.\nWe recalculate the critical $\\alpha$ values for the internal nodes of $T_1$.\n\nFor node $A$: The subtree $T_A$ is unchanged, so $\\alpha_A = 4$.\nFor node $D$: The subtree $T_D$ in $T_1$ now has leaves $\\{B', 5\\}$.\n$R(T_D \\text{ in } T_1) = R(B') + e_5 = 28 + 6 = 34$.\n$|(T_D \\text{ in } T_1)_{\\text{leaves}}| = 2$.\n$\\alpha_D = \\frac{E_D - R(T_D \\text{ in } T_1)}{|(T_D \\text{ in } T_1)_{\\text{leaves}}| - 1} = \\frac{40 - 34}{2 - 1} = \\frac{6}{1} = 6$.\nFor node $U$: The tree $T_U$ is now $T_1$.\n$R(T_1) = R(T_A) + R(T_D \\text{ in } T_1) = (e_1+e_2) + (E_B+e_5) = 20 + 34 = 54$.\n$|T_1|_{\\text{leaves}} = 4$.\n$\\alpha_U = \\frac{E_U - R(T_1)}{|T_1|_{\\text{leaves}} - 1} = \\frac{72 - 54}{4 - 1} = \\frac{18}{3} = 6$.\n\nThe set of candidate critical values is $\\{\\alpha_A, \\alpha_D, \\alpha_U\\} = \\{4, 6, 6\\}$. The minimum is $\\alpha_A = 4$. So, the second critical value is $\\alpha_{(2)} = 4$. For $\\alpha \\in [3, 4)$, tree $T_1$ is optimal. At $\\alpha=4$, node $A$ is pruned.\n\nLet $T_2$ be the tree obtained by pruning node $A$ from $T_1$. Node $A$ becomes a leaf $A'$ with error $R(A')=E_A = 24$.\nThe internal nodes of $T_2$ are $D, U$. The leaves are $\\{A', B', 5\\}$.\nWe recalculate the critical $\\alpha$ values for the internal nodes of $T_2$.\n\nFor node $D$: The subtree $T_D$ is unaffected by pruning at $A$. Its structure in $T_2$ is the same as in $T_1$. Thus, $\\alpha_D = 6$.\nFor node $U$: The tree $T_U$ is now $T_2$.\n$R(T_2) = R(A') + R(B') + e_5 = 24 + 28 + 6 = 58$.\n$|T_2|_{\\text{leaves}} = 3$.\n$\\alpha_U = \\frac{E_U - R(T_2)}{|T_2|_{\\text{leaves}} - 1} = \\frac{72 - 58}{3 - 1} = \\frac{14}{2} = 7$.\n\nThe set of candidate values is $\\{\\alpha_D, \\alpha_U\\} = \\{6, 7\\}$. The minimum is $\\alpha_D = 6$. So, the third critical value is $\\alpha_{(3)} = 6$. For $\\alpha \\in [4, 6)$, tree $T_2$ is optimal. At $\\alpha=6$, node $D$ is pruned.\n\nLet $T_3$ be the tree obtained by pruning node $D$ from $T_2$. Node $D$ becomes a leaf $D'$ with error $R(D')=E_D = 40$.\nThe only internal node of $T_3$ is $U$. The leaves are $\\{A', D'\\}$.\nWe calculate the critical $\\alpha$ value for the remaining internal node $U$.\n\nFor node $U$: The tree $T_U$ is now $T_3$.\n$R(T_3) = R(A') + R(D') = 24 + 40 = 64$.\n$|T_3|_{\\text{leaves}} = 2$.\n$\\alpha_U = \\frac{E_U - R(T_3)}{|T_3|_{\\text{leaves}} - 1} = \\frac{72 - 64}{2 - 1} = \\frac{8}{1} = 8$.\n\nThere is only one internal node, so its $\\alpha$ value is the next critical value, $\\alpha_{(4)} = 8$. For $\\alpha \\in [6, 8)$, tree $T_3$ is optimal. At $\\alpha=8$, node $U$ is pruned.\n\nPruning node $U$ results in the trivial tree $T_4$, which consists of only the root node. This tree has $1$ leaf and total error $E_U = 72$. The pruning process is now complete. For $\\alpha \\in [8, \\infty)$, the trivial tree $T_4$ is optimal.\n\nThe sequence of critical penalty values at which the optimal subtree changes is $\\alpha_{(1)}, \\alpha_{(2)}, \\alpha_{(3)}, \\alpha_{(4)}$.\nThis sequence is $3, 4, 6, 8$.\nThe corresponding sequence of optimal subtrees for the intervals defined by these values is $T_0, T_1, T_2, T_3, T_4$.\n\nThe sequence of critical penalty values, in increasing order, is $\\{3, 4, 6, 8\\}$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n3 & 4 & 6 & 8\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "After constructing and pruning a decision tree, the final and most critical step is to evaluate its predictive performance on an independent validation dataset. The confusion matrix provides a comprehensive summary of a classifier's performance, but interpreting it requires calculating specific metrics. This practice  focuses on deriving essential class-wise and aggregated metrics—such as precision, recall, and the harmonic-mean F1-score—and explores the crucial distinction between macro and micro averaging to achieve a nuanced understanding of your model's strengths and weaknesses in a multi-class setting.",
            "id": "3805154",
            "problem": "A decision tree classifier is trained on multispectral satellite observations to map land cover into three classes relevant to environmental modeling: forest, cropland, and water. The features include physical reflectance bands and domain-standard spectral indices such as normalized difference vegetation index (NDVI) and modified normalized difference water index (MNDWI), computed from atmospherically corrected surface reflectance. The classifier outputs a single label per pixel.\n\nOn an independent validation set of $N$ pixels, the following confusion matrix $C$ is obtained. Rows correspond to the reference (ground truth) class, and columns correspond to the predicted class:\n$$\nC \\;=\\;\n\\begin{pmatrix}\n420 & 50 & 30 \\\\\n60 & 340 & 50 \\\\\n5 & 45 & 300\n\\end{pmatrix},\n$$\nwith the class order $\\{\\text{forest}, \\text{cropland}, \\text{water}\\}$ for both rows and columns.\n\nUsing fundamental definitions of class-wise precision, class-wise recall, and the harmonic-mean $\\mathrm{F1}$ score based on true positives, false positives, and false negatives for each class, compute:\n- the per-class precision, recall, and $\\mathrm{F1}$ for forest, cropland, and water;\n- the macro-averaged precision, recall, and $\\mathrm{F1}$ across the three classes;\n- the micro-averaged precision, recall, and $\\mathrm{F1}$ across the three classes.\n\nAssume a single-label multi-class setting in which each pixel belongs to exactly one reference class and receives exactly one predicted class. Provide a quantitative comparison between macro and micro averages.\n\nReport as your final answer the value of $F1_{\\mathrm{macro}} - F1_{\\mathrm{micro}}$ as a decimal fraction, rounded to four significant figures. Do not include any units or a percent sign in your final answer.",
            "solution": "The problem statement has been validated and is deemed valid. It is scientifically grounded in the principles of machine learning performance evaluation, well-posed with all necessary data provided in the confusion matrix, and objective in its formulation.\n\nThe task is to compute several performance metrics for a three-class classifier from its confusion matrix $C$. The classes are indexed as follows: $1$ for forest, $2$ for cropland, and $3$ for water. The confusion matrix is given as:\n$$\nC \\;=\\;\n\\begin{pmatrix}\nC_{11} & C_{12} & C_{13} \\\\\nC_{21} & C_{22} & C_{23} \\\\\nC_{31} & C_{32} & C_{33}\n\\end{pmatrix}\n\\;=\\;\n\\begin{pmatrix}\n420 & 50 & 30 \\\\\n60 & 340 & 50 \\\\\n5 & 45 & 300\n\\end{pmatrix}\n$$\nwhere rows represent the reference (true) class and columns represent the predicted class.\n\nFor each class $i$, we define the number of true positives ($TP_i$), false positives ($FP_i$), and false negatives ($FN_i$):\n- $TP_i = C_{ii}$: The number of pixels of class $i$ correctly classified as class $i$.\n- $FP_i = \\sum_{k \\neq i} C_{ki}$: The number of pixels of other classes incorrectly classified as class $i$ (sum of column $i$, excluding the diagonal).\n- $FN_i = \\sum_{j \\neq i} C_{ij}$: The number of pixels of class $i$ incorrectly classified as other classes (sum of row $i$, excluding the diagonal).\n\nUsing these, we define per-class precision ($P_i$), recall ($R_i$), and F1-score ($F1_i$):\n$$ P_i = \\frac{TP_i}{TP_i + FP_i} \\quad , \\quad R_i = \\frac{TP_i}{TP_i + FN_i} \\quad , \\quad F1_i = 2 \\cdot \\frac{P_i \\cdot R_i}{P_i + R_i} = \\frac{2 TP_i}{2 TP_i + FP_i + FN_i} $$\n\n**Per-Class Metrics Calculation**\n\n**Class 1: Forest**\n- $TP_1 = C_{11} = 420$\n- $FP_1 = C_{21} + C_{31} = 60 + 5 = 65$\n- $FN_1 = C_{12} + C_{13} = 50 + 30 = 80$\n- $P_1 = \\frac{420}{420 + 65} = \\frac{420}{485} = \\frac{84}{97} \\approx 0.8660$\n- $R_1 = \\frac{420}{420 + 80} = \\frac{420}{500} = \\frac{21}{25} = 0.8400$\n- $F1_1 = \\frac{2 \\cdot 420}{2 \\cdot 420 + 65 + 80} = \\frac{840}{840 + 145} = \\frac{840}{985} = \\frac{168}{197} \\approx 0.8528$\n\n**Class 2: Cropland**\n- $TP_2 = C_{22} = 340$\n- $FP_2 = C_{12} + C_{32} = 50 + 45 = 95$\n- $FN_2 = C_{21} + C_{23} = 60 + 50 = 110$\n- $P_2 = \\frac{340}{340 + 95} = \\frac{340}{435} = \\frac{68}{87} \\approx 0.7816$\n- $R_2 = \\frac{340}{340 + 110} = \\frac{340}{450} = \\frac{34}{45} \\approx 0.7556$\n- $F1_2 = \\frac{2 \\cdot 340}{2 \\cdot 340 + 95 + 110} = \\frac{680}{680 + 205} = \\frac{680}{885} = \\frac{136}{177} \\approx 0.7684$\n\n**Class 3: Water**\n- $TP_3 = C_{33} = 300$\n- $FP_3 = C_{13} + C_{23} = 30 + 50 = 80$\n- $FN_3 = C_{31} + C_{32} = 5 + 45 = 50$\n- $P_3 = \\frac{300}{300 + 80} = \\frac{300}{380} = \\frac{15}{19} \\approx 0.7895$\n- $R_3 = \\frac{300}{300 + 50} = \\frac{300}{350} = \\frac{6}{7} \\approx 0.8571$\n- $F1_3 = \\frac{2 \\cdot 300}{2 \\cdot 300 + 80 + 50} = \\frac{600}{600 + 130} = \\frac{600}{730} = \\frac{60}{73} \\approx 0.8219$\n\n**Macro-Averaged Metrics**\nMacro-averaging computes the unweighted mean of the per-class metrics. For $K=3$ classes:\n$$ P_{\\mathrm{macro}} = \\frac{1}{K} \\sum_{i=1}^K P_i \\quad , \\quad R_{\\mathrm{macro}} = \\frac{1}{K} \\sum_{i=1}^K R_i \\quad , \\quad F1_{\\mathrm{macro}} = \\frac{1}{K} \\sum_{i=1}^K F1_i $$\nWe compute $F1_{\\mathrm{macro}}$:\n$$ F1_{\\mathrm{macro}} = \\frac{1}{3} (F1_1 + F1_2 + F1_3) = \\frac{1}{3} \\left( \\frac{168}{197} + \\frac{136}{177} + \\frac{60}{73} \\right) $$\nUsing high-precision decimal values:\n$$ F1_{\\mathrm{macro}} \\approx \\frac{1}{3} (0.85279188 + 0.76836158 + 0.82191781) = \\frac{1}{3} (2.44307127) \\approx 0.81435709 $$\n\n**Micro-Averaged Metrics**\nMicro-averaging aggregates the contributions of all classes to compute the global metrics. We first sum the $TP$, $FP$, and $FN$ counts across all classes:\n- $\\sum_{i=1}^3 TP_i = TP_1 + TP_2 + TP_3 = 420 + 340 + 300 = 1060$\n- $\\sum_{i=1}^3 FP_i = FP_1 + FP_2 + FP_3 = 65 + 95 + 80 = 240$\n- $\\sum_{i=1}^3 FN_i = FN_1 + FN_2 + FN_3 = 80 + 110 + 50 = 240$\nNote that $\\sum_i FP_i = \\sum_i FN_i$, as both represent the sum of all off-diagonal elements of the confusion matrix.\n\nThe micro-averaged metrics are:\n$$ P_{\\mathrm{micro}} = \\frac{\\sum_i TP_i}{\\sum_i TP_i + \\sum_i FP_i} = \\frac{1060}{1060 + 240} = \\frac{1060}{1300} $$\n$$ R_{\\mathrm{micro}} = \\frac{\\sum_i TP_i}{\\sum_i TP_i + \\sum_i FN_i} = \\frac{1060}{1060 + 240} = \\frac{1060}{1300} $$\nSince $P_{\\mathrm{micro}} = R_{\\mathrm{micro}}$, the micro-averaged F1-score is equal to both:\n$$ F1_{\\mathrm{micro}} = P_{\\mathrm{micro}} = R_{\\mathrm{micro}} = \\frac{1060}{1300} = \\frac{106}{130} = \\frac{53}{65} $$\nThis value is also the overall accuracy of the classifier.\n$$ F1_{\\mathrm{micro}} = \\frac{53}{65} \\approx 0.81538462 $$\n\n**Quantitative Comparison and Final Answer**\nThe problem asks for a quantitative comparison, which we will provide by computing the difference $F1_{\\mathrm{macro}} - F1_{\\mathrm{micro}}$.\n$$ F1_{\\mathrm{macro}} - F1_{\\mathrm{micro}} \\approx 0.81435709 - 0.81538462 = -0.00102753 $$\nThe negative value indicates that $F1_{\\mathrm{micro}} > F1_{\\mathrm{macro}}$. Micro-averaging weights each sample equally, thus giving more influence to larger classes. Macro-averaging weights each class equally. The result suggests that the classifier performs slightly better on the more populous classes (Forest, in this case, has the highest F1 score and the largest number of samples: $500$), thereby pulling the micro-average up relative to the macro-average.\n\nRounding the result to four significant figures:\nThe first significant digit is the $1$ in the ten-thousandths place. We keep four digits: $1, 0, 2, 7$. The fifth digit is $5$, so we round the last digit up.\n$$ -0.00102753 \\rightarrow -0.001028 $$",
            "answer": "$$\\boxed{-0.001028}$$"
        }
    ]
}