{
    "hands_on_practices": [
        {
            "introduction": "The power of a decision tree lies in its ability to learn a hierarchy of rules directly from data. At the core of this learning process is the recursive partitioning of the feature space. This first practice takes you to the heart of this mechanism, challenging you to implement the algorithm for finding the optimal split point for a continuous feature, such as the Normalized Difference Vegetation Index (NDVI) used widely in remote sensing. By maximizing the information gain at each node, you will develop a foundational skill for constructing high-performance decision tree classifiers from scratch .",
            "id": "3805139",
            "problem": "You are given a binary-split decision rule for a single continuous feature, the Normalized Difference Vegetation Index (NDVI). The target is a land-cover label among three classes: forest, cropland, and water. The feature values are known to be bounded within the physically meaningful interval $[-1, 1]$ for NDVI. You will compute the best scalar threshold $t$ for the rule: assign an instance to the left child if $NDVI \\le t$ and to the right child if $NDVI > t$. The best threshold $t$ is the one that maximizes the information gain of the split with respect to the three-class label.\n\nFundamental base and definitions to use:\n- Use Shannon entropy with logarithm base $2$, defined for a discrete label $Y$ with class probabilities $\\{p_c\\}_{c \\in \\mathcal{C}}$ as\n$$\nH(Y) \\;=\\; - \\sum_{c \\in \\mathcal{C}} p_c \\log_2 p_c,\n$$\nwith the convention that $0 \\log_2 0 \\equiv 0$. For a finite dataset, $p_c$ is the class relative frequency.\n- Given a split of a dataset $S$ into disjoint subsets $S_L$ and $S_R$, the information gain is\n$$\nIG(S, S_L, S_R) \\;=\\; H(S) \\;-\\; \\left( \\frac{|S_L|}{|S|} H(S_L) \\;+\\; \\frac{|S_R|}{|S|} H(S_R) \\right),\n$$\nwhere $|\\cdot|$ denotes cardinality and $H(\\cdot)$ is the Shannon entropy computed from the empirical class distribution in the respective subset.\n- The candidate thresholds for a continuous feature are all midpoints between consecutive distinct sorted feature values. If the sorted NDVI values are $x_1 \\le x_2 \\le \\cdots \\le x_n$, a candidate threshold exists between indices $i$ and $i+1$ only if $x_i < x_{i+1}$, and is given by\n$$\nt_i \\;=\\; \\frac{x_i + x_{i+1}}{2}.\n$$\n\nAlgorithmic requirements:\n- Input for each test case is a sorted list of NDVI values and the corresponding list of class labels from the set $\\{\\text{forest}, \\text{cropland}, \\text{water}\\}$. The lists are of equal length $n \\ge 2$. You must treat the NDVI list as sorted in non-decreasing order.\n- Consider only thresholds $t_i$ between consecutive distinct NDVI values as above. Duplicate NDVI values (possibly with differing labels) cannot be separated by any scalar threshold; such equal pairs do not produce a candidate $t_i$.\n- Compute $H(S)$ from the class label frequencies in the full set, and for each candidate threshold compute the induced $S_L$ and $S_R$ by splitting at that threshold. Evaluate $IG$ for each candidate and choose the threshold $t^\\star$ that maximizes $IG$.\n- Ties: If multiple thresholds achieve the same information gain within a numerical tolerance of $\\epsilon = 10^{-12}$, select the smallest threshold numerically. If all information gains are zero, the same tie-breaking rule applies.\n- Output for each test case must be the pair $[t^\\star, IG^\\star]$, where $t^\\star$ is the maximizing threshold and $IG^\\star$ is the corresponding maximum information gain. Both $t^\\star$ and $IG^\\star$ must be rounded to $6$ decimal places. NDVI is unitless; entropy and information gain are in bits.\n\nTest suite:\nProvide solutions for the following four test cases. Each test case gives $(\\text{NDVI list}, \\text{label list})$:\n\n1. General mixed case (balanced among classes):\n   - NDVI: [ $-0.08$, $0.02$, $0.13$, $0.22$, $0.35$, $0.48$, $0.66$, $0.71$, $0.83$ ]\n   - Labels: [ $\\text{water}$, $\\text{water}$, $\\text{cropland}$, $\\text{cropland}$, $\\text{cropland}$, $\\text{cropland}$, $\\text{forest}$, $\\text{forest}$, $\\text{forest}$ ]\n2. Duplicate NDVI values with conflicting labels:\n   - NDVI: [ $-0.10$, $0.00$, $0.00$, $0.20$, $0.50$, $0.50$, $0.60$ ]\n   - Labels: [ $\\text{water}$, $\\text{water}$, $\\text{cropland}$, $\\text{cropland}$, $\\text{cropland}$, $\\text{forest}$, $\\text{forest}$ ]\n3. Imbalanced case where high NDVI likely indicates forest:\n   - NDVI: [ $-0.20$, $0.05$, $0.07$, $0.10$, $0.15$, $0.30$, $0.32$, $0.33$, $0.35$, $0.80$, $0.82$ ]\n   - Labels: [ $\\text{water}$, $\\text{water}$, $\\text{water}$, $\\text{cropland}$, $\\text{cropland}$, $\\text{cropland}$, $\\text{cropland}$, $\\text{cropland}$, $\\text{cropland}$, $\\text{forest}$, $\\text{forest}$ ]\n4. All samples same class (water):\n   - NDVI: [ $-0.20$, $-0.10$, $0.00$, $0.20$, $0.50$ ]\n   - Labels: [ $\\text{water}$, $\\text{water}$, $\\text{water}$, $\\text{water}$, $\\text{water}$ ]\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element is the list $[t^\\star, IG^\\star]$ for the corresponding test case, in the same order as above. For example, the output must look like\n\"[ [t1,ig1],[t2,ig2],[t3,ig3],[t4,ig4] ]\"\nwith each $t_k$ and $ig_k$ rounded to $6$ decimal places and printed as decimal numbers without units. Do not print anything else.",
            "solution": "The problem of identifying the optimal binary split for a continuous feature is a fundamental procedure in the construction of decision tree classifiers. The objective is to find a scalar threshold $t$ for the feature, in this case, the Normalized Difference Vegetation Index (NDVI), that maximizes the purity of the two resulting subsets (child nodes). The standard metric for measuring this increase in purity is the Information Gain ($IG$), which is based on the concept of Shannon entropy.\n\nThe Shannon entropy $H(Y)$ of a discrete random variable $Y$ with a set of outcomes $\\mathcal{C}$ having probabilities $p_c$ is defined as:\n$$\nH(Y) = - \\sum_{c \\in \\mathcal{C}} p_c \\log_2 p_c\n$$\nIn the context of a dataset $S$, the probabilities $p_c$ are estimated by the empirical relative frequencies of each class $c$. By convention, $0 \\log_2 0$ is taken to be $0$, which occurs for classes not present in a given subset. The unit of entropy with logarithm base $2$ is the bit. A dataset with a single class has an entropy of $0$ (perfectly pure), while a dataset with a uniform distribution across classes has maximum entropy (maximum impurity).\n\nGiven a dataset $S$ and a split that partitions it into two disjoint subsets, $S_L$ (left child) and $S_R$ (right child), the Information Gain is formulated as the reduction in entropy from the parent node $S$ to the weighted average of the children nodes' entropies:\n$$\nIG(S, S_L, S_R) = H(S) - \\left( \\frac{|S_L|}{|S|} H(S_L) + \\frac{|S_R|}{|S|} H(S_R) \\right)\n$$\nwhere $|S|$, $|S_L|$, and $|S_R|$ are the number of instances in the parent, left child, and right child, respectively. A higher information gain signifies a more effective split.\n\nThe algorithm to find the optimal threshold $t^\\star$ that maximizes the information gain $IG^\\star$ proceeds as follows:\n\n1.  **Preprocessing**: First, the class labels, given as strings ('water', 'cropland', 'forest'), are mapped to integer values (e.g., $0, 1, 2$) for computational efficiency. The input NDVI list is already sorted in non-decreasing order.\n\n2.  **Parent Entropy Calculation**: The entropy of the entire dataset, $H(S)$, is calculated based on the frequency of the three land-cover classes in the full label list. This value is constant for all candidate splits of a given dataset.\n\n3.  **Candidate Threshold Generation**: For a continuous feature, an exhaustive search of all possible thresholds is computationally infeasible. An efficient method, as specified, is to consider only the midpoints between consecutive, distinct feature values. Given the sorted NDVI values $x_1, x_2, \\dots, x_n$, a candidate threshold $t_i$ is generated as $t_i = (x_i + x_{i+1})/2$ for each index $i$ where $x_i < x_{i+1}$. This ensures that every possible partitioning of the sorted data is evaluated.\n\n4.  **Information Gain Evaluation**: For each candidate threshold $t_i$, the following steps are performed:\n    a. The dataset $S$ is partitioned into $S_L = \\{(x, y) \\in S \\mid x \\le t_i\\}$ and $S_R = \\{(x, y) \\in S \\mid x > t_i\\}$. Given the sorted NDVI values, this split corresponds to a simple division of the data lists.\n    b. The entropies of the child nodes, $H(S_L)$ and $H(S_R)$, are computed from the class distributions within each subset.\n    c. The information gain $IG(S, t_i)$ is calculated using the formula above.\n\n5.  **Optimal Threshold Selection**: The algorithm iterates through all candidate thresholds, calculating the information gain for each. It maintains a record of the best threshold found so far, $t^\\star$, and its corresponding maximum information gain, $IG^\\star$. A candidate threshold becomes the new optimum if its information gain is greater than the current maximum.\n\n6.  **Tie-Breaking**: If a candidate threshold yields an information gain that is equal to the current maximum (within a specified numerical tolerance of $\\epsilon = 10^{-12}$), the problem requires selecting the threshold with the smaller numerical value.\n\n7.  **Final Result**: After evaluating all candidate thresholds, the pair $[t^\\star, IG^\\star]$ represents the solution. If a dataset contains no distinct consecutive feature values, no split is possible, and the information gain is $0$. In this case, the threshold is indeterminate; a value of $0.0$ is a reasonable default. For a parent node that is already pure (all samples belong to one class), the parent entropy $H(S)$ is $0$, and consequently, the information gain for any split will also be $0$. The tie-breaking rule still applies to select the smallest candidate threshold. The final values for $t^\\star$ and $IG^\\star$ are rounded to $6$ decimal places as required.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the decision tree split problem for the given test cases.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        (\n            [-0.08, 0.02, 0.13, 0.22, 0.35, 0.48, 0.66, 0.71, 0.83],\n            ['water', 'water', 'cropland', 'cropland', 'cropland', 'cropland', 'forest', 'forest', 'forest']\n        ),\n        (\n            [-0.10, 0.00, 0.00, 0.20, 0.50, 0.50, 0.60],\n            ['water', 'water', 'cropland', 'cropland', 'cropland', 'forest', 'forest']\n        ),\n        (\n            [-0.20, 0.05, 0.07, 0.10, 0.15, 0.30, 0.32, 0.33, 0.35, 0.80, 0.82],\n            ['water', 'water', 'water', 'cropland', 'cropland', 'cropland', 'cropland', 'cropland', 'cropland', 'forest', 'forest']\n        ),\n        (\n            [-0.20, -0.10, 0.00, 0.20, 0.50],\n            ['water', 'water', 'water', 'water', 'water']\n        )\n    ]\n    \n    results = []\n    \n    label_map = {'water': 0, 'cropland': 1, 'forest': 2}\n    n_classes = len(label_map)\n    tolerance = 1e-12\n\n    def calculate_entropy(labels):\n        \"\"\"Computes the Shannon entropy for a set of integer labels.\"\"\"\n        n_labels = len(labels)\n        if n_labels <= 1:\n            return 0.0\n        \n        counts = np.bincount(labels, minlength=n_classes)\n        probs = counts[counts > 0] / n_labels\n        entropy = -np.sum(probs * np.log2(probs))\n        return entropy\n\n    for ndvis, str_labels in test_cases:\n        labels = np.array([label_map[l] for l in str_labels], dtype=int)\n        n_samples = len(ndvis)\n        \n        parent_entropy = calculate_entropy(labels)\n        \n        best_t = None\n        best_ig = -1.0\n        \n        # Iterate through data to find candidate split points\n        for i in range(n_samples - 1):\n            # A candidate threshold exists only between distinct values.\n            if ndvis[i] < ndvis[i+1]:\n                t = (ndvis[i] + ndvis[i+1]) / 2.0\n                \n                # Split the dataset based on the current data index `i`.\n                # This is efficient because the ndvi list is sorted.\n                left_labels = labels[:i+1]\n                right_labels = labels[i+1:]\n                \n                n_left = len(left_labels)\n                n_right = len(right_labels)\n                \n                h_left = calculate_entropy(left_labels)\n                h_right = calculate_entropy(right_labels)\n                \n                weighted_h = (n_left / n_samples) * h_left + (n_right / n_samples) * h_right\n                current_ig = parent_entropy - weighted_h\n                \n                # First valid split initializes best_t and best_ig\n                if best_t is None:\n                    best_t = t\n                    best_ig = current_ig\n                    continue\n                \n                # Check for a better split\n                if current_ig > best_ig + tolerance:\n                    best_ig = current_ig\n                    best_t = t\n                # Apply tie-breaking rule\n                elif abs(current_ig - best_ig) <= tolerance:\n                    if t < best_t:\n                        best_t = t\n                        # best_ig is already considered equal\n\n        # Handle case where no split is possible (all NDVI values are identical)\n        if best_t is None:\n             results.append([0.0, 0.0])\n        else:\n             results.append([best_t, best_ig])\n\n    # Final print statement in the exact required format.\n    output_str = \",\".join([f\"[{t:.6f},{ig:.6f}]\" for t, ig in results])\n    print(f\"[{output_str}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "While growing a deep tree can perfectly classify the training data, it often leads to overfitting, resulting in poor performance on new, unseen data. To create a model that generalizes well, we must prune the tree. This exercise introduces cost-complexity pruning, the principled method used in CART to systematically find the best trade-off between model simplicity and accuracy. You will trace the entire pruning path by identifying the sequence of \"weakest links,\" providing a deep, mechanical understanding of how to combat overfitting and select a final, robust model .",
            "id": "3805108",
            "problem": "A land-cover classification decision tree was trained on a labeled set of satellite pixels to distinguish major classes (urban, vegetation, water, bare soil) using multispectral thresholds. The method is Classification And Regression Trees (CART). In this training, each terminal leaf assigns the majority class within its region and incurs a resubstitution misclassification count. The following small tree has five terminal leaves with resubstitution errors $\\left(e_{1}, e_{2}, e_{3}, e_{4}, e_{5}\\right)$ and four internal nodes whose aggregated errors (the misclassification count if the entire nodeâ€™s region were labeled by a single majority class) are given.\n\nThe topology is: the root node $U$ splits into a left internal node $A$ and a right internal node $D$; node $A$ splits into leaves $1$ and $2$; node $D$ splits into an internal node $B$ and leaf $5$; and node $B$ splits into leaves $3$ and $4$.\n\nThe resubstitution errors of the leaves are\n$$\ne_{1} = 12,\\quad e_{2} = 8,\\quad e_{3} = 15,\\quad e_{4} = 10,\\quad e_{5} = 6.\n$$\nThe aggregated errors at the internal nodes, if each were pruned to a single leaf with a majority label over its region, are\n$$\nE_{A} = 24,\\quad E_{B} = 28,\\quad E_{D} = 40,\\quad E_{U} = 72.\n$$\n\nAssume cost-complexity pruning is used, with a complexity penalty parameter $\\alpha \\geq 0$, and trees are selected by minimizing a penalized empirical risk that sums the resubstitution misclassification counts over leaves plus a linear penalty in the number of leaves. Using first-principles reasoning from structural risk minimization, determine the pruning path by identifying the sequence of critical penalty values at which the optimal subtree changes and the corresponding $\\alpha$-intervals for which each subtree is optimal.\n\nReport the sequence of critical penalty values in increasing order as a single row matrix. Give exact values (do not round).",
            "solution": "The problem is well-posed, scientifically grounded, and provides all necessary information for a unique solution. The data are internally consistent. Therefore, a solution will be derived.\n\nThe problem requires a determination of the pruning path for a given decision tree using cost-complexity pruning. This method, central to the Classification and Regression Trees (CART) algorithm, aims to find a sequence of subtrees that are optimal for a continuous range of a complexity penalty parameter, $\\alpha$.\n\nThe cost-complexity of a tree $T$ is defined as:\n$$\nC_{\\alpha}(T) = R(T) + \\alpha |T_{\\text{leaves}}|\n$$\nwhere $R(T)$ is the total resubstitution misclassification count, which is the sum of errors of all terminal leaves in the tree $T$, and $|T_{\\text{leaves}}|$ is the number of terminal leaves in $T$. The parameter $\\alpha \\ge 0$ penalizes the complexity of the tree, measured by the number of its leaves.\n\nFor any internal node $t$ in a tree, we can consider the subtree $T_t$ rooted at $t$. The cost-complexity of this subtree is $C_{\\alpha}(T_t) = R(T_t) + \\alpha |(T_t)_{\\text{leaves}}|$. If we were to prune this subtree, node $t$ would become a terminal leaf. The cost-complexity of this pruned state is $C_{\\alpha}(\\{t\\}) = E_t + \\alpha \\cdot 1$, where $E_t$ is the aggregated misclassification count for the region corresponding to node $t$.\n\nA node $t$ is a candidate for pruning when the cost of making it a leaf becomes less than or equal to the cost of the unpruned subtree rooted at $t$:\n$$\nE_t + \\alpha \\le R(T_t) + \\alpha |(T_t)_{\\text{leaves}}|\n$$\nThe critical value of $\\alpha$ at which the costs are equal defines the point where node $t$ becomes the \"weakest link\" in the tree. Rearranging the equation gives the critical complexity parameter for node $t$:\n$$\n\\alpha_t = \\frac{E_t - R(T_t)}{|(T_t)_{\\text{leaves}}| - 1}\n$$\nThe cost-complexity pruning algorithm proceeds iteratively. Starting with the full tree, we calculate $\\alpha_t$ for every internal node. The node with the smallest $\\alpha_t$ is pruned first. This process is repeated on the newly pruned tree until only the root node remains.\n\nLet us denote the initial, full tree as $T_0$. Its topology consists of internal nodes $U, A, D, B$ and terminal leaves $1, 2, 3, 4, 5$.\nThe errors of the leaves are given as $e_{1} = 12$, $e_{2} = 8$, $e_{3} = 15$, $e_{4} = 10$, and $e_{5} = 6$.\nThe aggregated errors for internal nodes are $E_{A} = 24$, $E_{B} = 28$, $E_{D} = 40$, and $E_{U} = 72$.\n\nFirst, we must compute the resubstitution error $R(T_t)$ and the number of leaves $|(T_t)_{\\text{leaves}}|$ for the subtrees rooted at each internal node of the full tree $T_0$.\n\nFor node $A$:\nThe subtree $T_A$ has leaves $1$ and $2$.\n$R(T_A) = e_1 + e_2 = 12 + 8 = 20$.\n$|(T_A)_{\\text{leaves}}| = 2$.\n\nFor node $B$:\nThe subtree $T_B$ has leaves $3$ and $4$.\n$R(T_B) = e_3 + e_4 = 15 + 10 = 25$.\n$|(T_B)_{\\text{leaves}}| = 2$.\n\nFor node $D$:\nThe subtree $T_D$ has an internal node $B$ and a leaf $5$. Its leaves are the leaves of $T_B$ and leaf $5$.\n$R(T_D) = R(T_B) + e_5 = 25 + 6 = 31$.\n$|(T_D)_{\\text{leaves}}| = |(T_B)_{\\text{leaves}}| + 1 = 2 + 1 = 3$.\n\nFor the root node $U$:\nThe full tree $T_U = T_0$ has internal nodes $A$ and $D$. Its leaves are the union of leaves from $T_A$ and $T_D$.\n$R(T_U) = R(T_A) + R(T_D) = 20 + 31 = 51$.\n$|(T_U)_{\\text{leaves}}| = |(T_A)_{\\text{leaves}}| + |(T_D)_{\\text{leaves}}| = 2 + 3 = 5$.\n\nNow, we can compute the critical $\\alpha$ values for all internal nodes in $T_0$.\n\n$\\alpha_A = \\frac{E_A - R(T_A)}{|(T_A)_{\\text{leaves}}| - 1} = \\frac{24 - 20}{2 - 1} = \\frac{4}{1} = 4$.\n$\\alpha_B = \\frac{E_B - R(T_B)}{|(T_B)_{\\text{leaves}}| - 1} = \\frac{28 - 25}{2 - 1} = \\frac{3}{1} = 3$.\n$\\alpha_D = \\frac{E_D - R(T_D)}{|(T_D)_{\\text{leaves}}| - 1} = \\frac{40 - 31}{3 - 1} = \\frac{9}{2} = 4.5$.\n$\\alpha_U = \\frac{E_U - R(T_U)}{|(T_U)_{\\text{leaves}}| - 1} = \\frac{72 - 51}{5 - 1} = \\frac{21}{4} = 5.25$.\n\nThe minimum of these values is $\\alpha_B = 3$. Thus, the first critical penalty value is $\\alpha_{(1)} = 3$. For $\\alpha \\in [0, 3)$, the full tree $T_0$ is the optimal subtree. At $\\alpha = 3$, we prune node $B$.\n\nLet $T_1$ be the tree obtained by pruning node $B$. In $T_1$, $B$ becomes a leaf (let's denote it $B'$) with error $R(B') = E_B = 28$.\nThe internal nodes of $T_1$ are $A, D, U$. The leaves are $\\{1, 2, B', 5\\}$.\nWe recalculate the critical $\\alpha$ values for the internal nodes of $T_1$.\n\nFor node $A$: The subtree $T_A$ is unchanged, so $\\alpha_A = 4$.\nFor node $D$: The subtree $T_D$ in $T_1$ now has leaves $\\{B', 5\\}$.\n$R(T_D \\text{ in } T_1) = R(B') + e_5 = 28 + 6 = 34$.\n$|(T_D \\text{ in } T_1)_{\\text{leaves}}| = 2$.\n$\\alpha_D = \\frac{E_D - R(T_D \\text{ in } T_1)}{|(T_D \\text{ in } T_1)_{\\text{leaves}}| - 1} = \\frac{40 - 34}{2 - 1} = \\frac{6}{1} = 6$.\nFor node $U$: The tree $T_U$ is now $T_1$.\n$R(T_1) = R(T_A) + R(T_D \\text{ in } T_1) = (e_1+e_2) + (E_B+e_5) = 20 + 34 = 54$.\n$|T_1|_{\\text{leaves}} = 4$.\n$\\alpha_U = \\frac{E_U - R(T_1)}{|T_1|_{\\text{leaves}} - 1} = \\frac{72 - 54}{4 - 1} = \\frac{18}{3} = 6$.\n\nThe set of candidate critical values is $\\{\\alpha_A, \\alpha_D, \\alpha_U\\} = \\{4, 6, 6\\}$. The minimum is $\\alpha_A = 4$. So, the second critical value is $\\alpha_{(2)} = 4$. For $\\alpha \\in [3, 4)$, tree $T_1$ is optimal. At $\\alpha=4$, node $A$ is pruned.\n\nLet $T_2$ be the tree obtained by pruning node $A$ from $T_1$. Node $A$ becomes a leaf $A'$ with error $R(A')=E_A = 24$.\nThe internal nodes of $T_2$ are $D, U$. The leaves are $\\{A', B', 5\\}$.\nWe recalculate the critical $\\alpha$ values for the internal nodes of $T_2$.\n\nFor node $D$: The subtree $T_D$ is unaffected by pruning at $A$. Its structure in $T_2$ is the same as in $T_1$. Thus, $\\alpha_D = 6$.\nFor node $U$: The tree $T_U$ is now $T_2$.\n$R(T_2) = R(A') + R(B') + e_5 = 24 + 28 + 6 = 58$.\n$|T_2|_{\\text{leaves}} = 3$.\n$\\alpha_U = \\frac{E_U - R(T_2)}{|T_2|_{\\text{leaves}} - 1} = \\frac{72 - 58}{3 - 1} = \\frac{14}{2} = 7$.\n\nThe set of candidate values is $\\{\\alpha_D, \\alpha_U\\} = \\{6, 7\\}$. The minimum is $\\alpha_D = 6$. So, the third critical value is $\\alpha_{(3)} = 6$. For $\\alpha \\in [4, 6)$, tree $T_2$ is optimal. At $\\alpha=6$, node $D$ is pruned.\n\nLet $T_3$ be the tree obtained by pruning node $D$ from $T_2$. Node $D$ becomes a leaf $D'$ with error $R(D')=E_D = 40$.\nThe only internal node of $T_3$ is $U$. The leaves are $\\{A', D'\\}$.\nWe calculate the critical $\\alpha$ value for the remaining internal node $U$.\n\nFor node $U$: The tree $T_U$ is now $T_3$.\n$R(T_3) = R(A') + R(D') = 24 + 40 = 64$.\n$|T_3|_{\\text{leaves}} = 2$.\n$\\alpha_U = \\frac{E_U - R(T_3)}{|T_3|_{\\text{leaves}} - 1} = \\frac{72 - 64}{2 - 1} = \\frac{8}{1} = 8$.\n\nThere is only one internal node, so its $\\alpha$ value is the next critical value, $\\alpha_{(4)} = 8$. For $\\alpha \\in [6, 8)$, tree $T_3$ is optimal. At $\\alpha=8$, node $U$ is pruned.\n\nPruning node $U$ results in the trivial tree $T_4$, which consists of only the root node. This tree has $1$ leaf and total error $E_U = 72$. The pruning process is now complete. For $\\alpha \\in [8, \\infty)$, the trivial tree $T_4$ is optimal.\n\nThe sequence of critical penalty values at which the optimal subtree changes is $\\alpha_{(1)}, \\alpha_{(2)}, \\alpha_{(3)}, \\alpha_{(4)}$.\nThis sequence is $3, 4, 6, 8$.\nThe corresponding sequence of optimal subtrees for the intervals defined by these values is $T_0, T_1, T_2, T_3, T_4$.\n\nThe sequence of critical penalty values, in increasing order, is $\\{3, 4, 6, 8\\}$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n3 & 4 & 6 & 8\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "After a decision tree has been grown and pruned, the final step is to rigorously evaluate its performance. This practice shifts our focus from model building to model assessment, using the indispensable confusion matrix as our starting point. You will compute a suite of essential classification metrics, including precision, recall, and the $F_1$ score, to diagnose your model's strengths and weaknesses on a per-class basis. Furthermore, you will explore the critical distinction between macro- and micro-averaging, a key concept for fairly evaluating classifiers in the presence of class imbalance, a common challenge in environmental and remote sensing datasets .",
            "id": "3805154",
            "problem": "A decision tree classifier is trained on multispectral satellite observations to map land cover into three classes relevant to environmental modeling: forest, cropland, and water. The features include physical reflectance bands and domain-standard spectral indices such as normalized difference vegetation index (NDVI) and modified normalized difference water index (MNDWI), computed from atmospherically corrected surface reflectance. The classifier outputs a single label per pixel.\n\nOn an independent validation set of $N$ pixels, the following confusion matrix $C$ is obtained. Rows correspond to the reference (ground truth) class, and columns correspond to the predicted class:\n$$\nC \\;=\\;\n\\begin{pmatrix}\n420 & 50 & 30 \\\\\n60 & 340 & 50 \\\\\n5 & 45 & 300\n\\end{pmatrix},\n$$\nwith the class order $\\{\\text{forest}, \\text{cropland}, \\text{water}\\}$ for both rows and columns.\n\nUsing fundamental definitions of class-wise precision, class-wise recall, and the harmonic-mean $F_1$ score based on true positives, false positives, and false negatives for each class, compute:\n- the per-class precision, recall, and $F_1$ for forest, cropland, and water;\n- the macro-averaged precision, recall, and $F_1$ across the three classes;\n- the micro-averaged precision, recall, and $F_1$ across the three classes.\n\nAssume a single-label multi-class setting in which each pixel belongs to exactly one reference class and receives exactly one predicted class. Provide a quantitative comparison between macro and micro averages.\n\nReport as your final answer the value of $F_{1, \\mathrm{macro}} - F_{1, \\mathrm{micro}}$ as a decimal fraction, rounded to four significant figures. Do not include any units or a percent sign in your final answer.",
            "solution": "The problem statement has been validated and is deemed valid. It is scientifically grounded in the principles of machine learning performance evaluation, well-posed with all necessary data provided in the confusion matrix, and objective in its formulation.\n\nThe task is to compute several performance metrics for a three-class classifier from its confusion matrix $C$. The classes are indexed as follows: $1$ for forest, $2$ for cropland, and $3$ for water. The confusion matrix is given as:\n$$\nC \\;=\\;\n\\begin{pmatrix}\nC_{11} & C_{12} & C_{13} \\\\\nC_{21} & C_{22} & C_{23} \\\\\nC_{31} & C_{32} & C_{33}\n\\end{pmatrix}\n\\;=\\;\n\\begin{pmatrix}\n420 & 50 & 30 \\\\\n60 & 340 & 50 \\\\\n5 & 45 & 300\n\\end{pmatrix}\n$$\nwhere rows represent the reference (true) class and columns represent the predicted class.\n\nFor each class $i$, we define the number of true positives ($TP_i$), false positives ($FP_i$), and false negatives ($FN_i$):\n- $TP_i = C_{ii}$: The number of pixels of class $i$ correctly classified as class $i$.\n- $FP_i = \\sum_{k \\neq i} C_{ki}$: The number of pixels of other classes incorrectly classified as class $i$ (sum of column $i$, excluding the diagonal).\n- $FN_i = \\sum_{j \\neq i} C_{ij}$: The number of pixels of class $i$ incorrectly classified as other classes (sum of row $i$, excluding the diagonal).\n\nUsing these, we define per-class precision ($P_i$), recall ($R_i$), and $F_1$ score ($F_{1,i}$):\n$$ P_i = \\frac{TP_i}{TP_i + FP_i} \\quad , \\quad R_i = \\frac{TP_i}{TP_i + FN_i} \\quad , \\quad F_{1,i} = 2 \\cdot \\frac{P_i \\cdot R_i}{P_i + R_i} = \\frac{2 TP_i}{2 TP_i + FP_i + FN_i} $$\n\n**Per-Class Metrics Calculation**\n\n**Class 1: Forest**\n- $TP_1 = C_{11} = 420$\n- $FP_1 = C_{21} + C_{31} = 60 + 5 = 65$\n- $FN_1 = C_{12} + C_{13} = 50 + 30 = 80$\n- $P_1 = \\frac{420}{420 + 65} = \\frac{420}{485} = \\frac{84}{97} \\approx 0.8660$\n- $R_1 = \\frac{420}{420 + 80} = \\frac{420}{500} = \\frac{21}{25} = 0.8400$\n- $F_{1,1} = \\frac{2 \\cdot 420}{2 \\cdot 420 + 65 + 80} = \\frac{840}{840 + 145} = \\frac{840}{985} = \\frac{168}{197} \\approx 0.8528$\n\n**Class 2: Cropland**\n- $TP_2 = C_{22} = 340$\n- $FP_2 = C_{12} + C_{32} = 50 + 45 = 95$\n- $FN_2 = C_{21} + C_{23} = 60 + 50 = 110$\n- $P_2 = \\frac{340}{340 + 95} = \\frac{340}{435} = \\frac{68}{87} \\approx 0.7816$\n- $R_2 = \\frac{340}{340 + 110} = \\frac{340}{450} = \\frac{34}{45} \\approx 0.7556$\n- $F_{1,2} = \\frac{2 \\cdot 340}{2 \\cdot 340 + 95 + 110} = \\frac{680}{680 + 205} = \\frac{680}{885} = \\frac{136}{177} \\approx 0.7684$\n\n**Class 3: Water**\n- $TP_3 = C_{33} = 300$\n- $FP_3 = C_{13} + C_{23} = 30 + 50 = 80$\n- $FN_3 = C_{31} + C_{32} = 5 + 45 = 50$\n- $P_3 = \\frac{300}{300 + 80} = \\frac{300}{380} = \\frac{15}{19} \\approx 0.7895$\n- $R_3 = \\frac{300}{300 + 50} = \\frac{300}{350} = \\frac{6}{7} \\approx 0.8571$\n- $F_{1,3} = \\frac{2 \\cdot 300}{2 \\cdot 300 + 80 + 50} = \\frac{600}{600 + 130} = \\frac{600}{730} = \\frac{60}{73} \\approx 0.8219$\n\n**Macro-Averaged Metrics**\nMacro-averaging computes the unweighted mean of the per-class metrics. For $K=3$ classes:\n$$ P_{\\mathrm{macro}} = \\frac{1}{K} \\sum_{i=1}^K P_i \\quad , \\quad R_{\\mathrm{macro}} = \\frac{1}{K} \\sum_{i=1}^K R_i \\quad , \\quad F_{1, \\mathrm{macro}} = \\frac{1}{K} \\sum_{i=1}^K F_{1,i} $$\nWe compute $F_{1, \\mathrm{macro}}$:\n$$ F_{1, \\mathrm{macro}} = \\frac{1}{3} (F_{1,1} + F_{1,2} + F_{1,3}) = \\frac{1}{3} \\left( \\frac{168}{197} + \\frac{136}{177} + \\frac{60}{73} \\right) $$\nUsing high-precision decimal values:\n$$ F_{1, \\mathrm{macro}} \\approx \\frac{1}{3} (0.85279188 + 0.76836158 + 0.82191781) = \\frac{1}{3} (2.44307127) \\approx 0.81435709 $$\n\n**Micro-Averaged Metrics**\nMicro-averaging aggregates the contributions of all classes to compute the global metrics. We first sum the $TP$, $FP$, and $FN$ counts across all classes:\n- $\\sum_{i=1}^3 TP_i = TP_1 + TP_2 + TP_3 = 420 + 340 + 300 = 1060$\n- $\\sum_{i=1}^3 FP_i = FP_1 + FP_2 + FP_3 = 65 + 95 + 80 = 240$\n- $\\sum_{i=1}^3 FN_i = FN_1 + FN_2 + FN_3 = 80 + 110 + 50 = 240$\nNote that $\\sum_i FP_i = \\sum_i FN_i$, as both represent the sum of all off-diagonal elements of the confusion matrix.\n\nThe micro-averaged metrics are:\n$$ P_{\\mathrm{micro}} = \\frac{\\sum_i TP_i}{\\sum_i TP_i + \\sum_i FP_i} = \\frac{1060}{1060 + 240} = \\frac{1060}{1300} $$\n$$ R_{\\mathrm{micro}} = \\frac{\\sum_i TP_i}{\\sum_i TP_i + \\sum_i FN_i} = \\frac{1060}{1060 + 240} = \\frac{1060}{1300} $$\nSince $P_{\\mathrm{micro}} = R_{\\mathrm{micro}}$, the micro-averaged $F_1$ score is equal to both:\n$$ F_{1, \\mathrm{micro}} = P_{\\mathrm{micro}} = R_{\\mathrm{micro}} = \\frac{1060}{1300} = \\frac{106}{130} = \\frac{53}{65} $$\nThis value is also the overall accuracy of the classifier.\n$$ F_{1, \\mathrm{micro}} = \\frac{53}{65} \\approx 0.81538462 $$\n\n**Quantitative Comparison and Final Answer**\nThe problem asks for a quantitative comparison, which we will provide by computing the difference $F_{1, \\mathrm{macro}} - F_{1, \\mathrm{micro}}$.\n$$ F_{1, \\mathrm{macro}} - F_{1, \\mathrm{micro}} \\approx 0.81435709 - 0.81538462 = -0.00102753 $$\nThe negative value indicates that $F_{1, \\mathrm{micro}} > F_{1, \\mathrm{macro}}$. Micro-averaging weights each sample equally, thus giving more influence to larger classes. Macro-averaging weights each class equally. The result suggests that the classifier performs slightly better on the more populous classes (Forest, in this case, has the highest $F_1$ score and the largest number of samples: $500$), thereby pulling the micro-average up relative to the macro-average.\n\nRounding the result to four significant figures:\nThe first significant digit is the $1$ in the ten-thousandths place. We keep four digits: $1, 0, 2, 7$. The fifth digit is $5$, so we round the last digit up.\n$$ -0.00102753 \\rightarrow -0.001028 $$",
            "answer": "$$\\boxed{-0.001028}$$"
        }
    ]
}