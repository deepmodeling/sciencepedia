{
    "hands_on_practices": [
        {
            "introduction": "The first step in any supervised classification is to characterize the spectral properties of each land cover class using labeled training data. This exercise takes you through the foundational process of deriving the statistical signature—the mean vector and covariance matrix—from first principles using the method of maximum likelihood. By working through this derivation, you will not only understand how these essential parameters are estimated but also uncover their inherent statistical properties, such as bias, and the critical challenges that arise when working with a limited number of training samples relative to the data's dimensionality .",
            "id": "3853753",
            "problem": "In multispectral remote sensing for environmental modeling, spectral separability analysis relies on class-conditional training signatures estimated from labeled pixels. Consider a particular land cover class indexed by $i$, for which you have $N_i$ labeled $d$-dimensional spectral vectors $\\{\\mathbf{x}_n\\}_{n=1}^{N_i}$, assumed independent and identically distributed from a $d$-variate normal (Gaussian) distribution with mean $\\boldsymbol{\\mu}_i \\in \\mathbb{R}^d$ and positive-definite covariance matrix $\\boldsymbol{\\Sigma}_i \\in \\mathbb{R}^{d \\times d}$. The multivariate normal probability density function is\n$$\np(\\mathbf{x}\\mid \\boldsymbol{\\mu}_i,\\boldsymbol{\\Sigma}_i)=\\frac{1}{(2\\pi)^{d/2}|\\boldsymbol{\\Sigma}_i|^{1/2}}\\exp\\!\\left(-\\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu}_i)^{\\top}\\boldsymbol{\\Sigma}_i^{-1}(\\mathbf{x}-\\boldsymbol{\\mu}_i)\\right).\n$$\nStarting from the maximum likelihood principle and the above density, derive the maximum likelihood estimates for $\\boldsymbol{\\mu}_i$ and $\\boldsymbol{\\Sigma}_i$ using only the labeled samples $\\{\\mathbf{x}_n\\}_{n=1}^{N_i}$. Then, using properties of the multivariate normal distribution and the Wishart distribution, analyze the bias of the covariance estimate for finite $N_i$ and articulate a bias-corrected estimator under normality, commenting on its behavior when $N_i$ is small relative to $d$ and the implications for spectral separability analysis.\n\nYour final answer must be the pair of closed-form analytic expressions for the maximum likelihood estimates of $\\boldsymbol{\\mu}_i$ and $\\boldsymbol{\\Sigma}_i$, in that order, presented as two entries of a single row vector. Do not include any explanatory text, and do not include equality signs in your final answer. No rounding is required.",
            "solution": "The problem is scientifically grounded, well-posed, and contains all necessary information to derive the maximum likelihood estimates and analyze their properties. It is a standard problem in multivariate statistics with a direct and important application in remote sensing. Therefore, the problem is valid, and a full solution follows.\n\nThe objective is to find the maximum likelihood estimates (MLE) for the mean vector $\\boldsymbol{\\mu}_i$ and the covariance matrix $\\boldsymbol{\\Sigma}_i$ of a $d$-variate normal distribution, given $N_i$ independent and identically distributed (i.i.d.) samples $\\{\\mathbf{x}_n\\}_{n=1}^{N_i}$.\n\nFirst, we construct the likelihood function. Since the samples are i.i.d., the likelihood is the product of the probability density functions for each sample:\n$$\nL(\\boldsymbol{\\mu}_i, \\boldsymbol{\\Sigma}_i \\mid \\{\\mathbf{x}_n\\}) = \\prod_{n=1}^{N_i} p(\\mathbf{x}_n\\mid \\boldsymbol{\\mu}_i,\\boldsymbol{\\Sigma}_i) = \\prod_{n=1}^{N_i} \\frac{1}{(2\\pi)^{d/2}|\\boldsymbol{\\Sigma}_i|^{1/2}}\\exp\\!\\left(-\\frac{1}{2}(\\mathbf{x}_n-\\boldsymbol{\\mu}_i)^{\\top}\\boldsymbol{\\Sigma}_i^{-1}(\\mathbf{x}_n-\\boldsymbol{\\mu}_i)\\right)\n$$\nMaximizing the likelihood is equivalent to maximizing its natural logarithm, the log-likelihood function $\\ell = \\ln L$. Taking the logarithm simplifies the product into a sum:\n$$\n\\ell(\\boldsymbol{\\mu}_i, \\boldsymbol{\\Sigma}_i) = \\ln \\left( \\prod_{n=1}^{N_i} \\left[ (2\\pi)^{-d/2}|\\boldsymbol{\\Sigma}_i|^{-1/2} \\exp\\!\\left(-\\frac{1}{2}(\\mathbf{x}_n-\\boldsymbol{\\mu}_i)^{\\top}\\boldsymbol{\\Sigma}_i^{-1}(\\mathbf{x}_n-\\boldsymbol{\\mu}_i)\\right) \\right] \\right)\n$$\n$$\n\\ell(\\boldsymbol{\\mu}_i, \\boldsymbol{\\Sigma}_i) = \\sum_{n=1}^{N_i} \\left( -\\frac{d}{2}\\ln(2\\pi) - \\frac{1}{2}\\ln|\\boldsymbol{\\Sigma}_i| - \\frac{1}{2}(\\mathbf{x}_n-\\boldsymbol{\\mu}_i)^{\\top}\\boldsymbol{\\Sigma}_i^{-1}(\\mathbf{x}_n-\\boldsymbol{\\mu}_i) \\right)\n$$\n$$\n\\ell(\\boldsymbol{\\mu}_i, \\boldsymbol{\\Sigma}_i) = -\\frac{N_i d}{2}\\ln(2\\pi) - \\frac{N_i}{2}\\ln|\\boldsymbol{\\Sigma}_i| - \\frac{1}{2}\\sum_{n=1}^{N_i}(\\mathbf{x}_n-\\boldsymbol{\\mu}_i)^{\\top}\\boldsymbol{\\Sigma}_i^{-1}(\\mathbf{x}_n-\\boldsymbol{\\mu}_i)\n$$\nTo find the MLEs, we take the partial derivatives of $\\ell$ with respect to $\\boldsymbol{\\mu}_i$ and $\\boldsymbol{\\Sigma}_i$ and set them to zero.\n\n**Maximum Likelihood Estimate for the Mean $\\boldsymbol{\\mu}_i$**\n\nWe compute the gradient of $\\ell$ with respect to $\\boldsymbol{\\mu}_i$. Only the summation term depends on $\\boldsymbol{\\mu}_i$. Using the standard matrix calculus identity $\\frac{\\partial}{\\partial\\mathbf{v}} (\\mathbf{x}-\\mathbf{v})^{\\top}\\mathbf{A}(\\mathbf{x}-\\mathbf{v}) = -2\\mathbf{A}(\\mathbf{x}-\\mathbf{v})$ for a symmetric matrix $\\mathbf{A}$:\n$$\n\\frac{\\partial \\ell}{\\partial \\boldsymbol{\\mu}_i} = -\\frac{1}{2}\\sum_{n=1}^{N_i} \\frac{\\partial}{\\partial \\boldsymbol{\\mu}_i}\\left( (\\mathbf{x}_n-\\boldsymbol{\\mu}_i)^{\\top}\\boldsymbol{\\Sigma}_i^{-1}(\\mathbf{x}_n-\\boldsymbol{\\mu}_i) \\right) = -\\frac{1}{2}\\sum_{n=1}^{N_i} \\left( -2\\boldsymbol{\\Sigma}_i^{-1}(\\mathbf{x}_n-\\boldsymbol{\\mu}_i) \\right) = \\boldsymbol{\\Sigma}_i^{-1} \\sum_{n=1}^{N_i}(\\mathbf{x}_n-\\boldsymbol{\\mu}_i)\n$$\nSetting the gradient to the zero vector:\n$$\n\\boldsymbol{\\Sigma}_i^{-1} \\left( \\sum_{n=1}^{N_i}\\mathbf{x}_n - N_i\\boldsymbol{\\mu}_i \\right) = \\mathbf{0}\n$$\nSince $\\boldsymbol{\\Sigma}_i$ is positive-definite, its inverse $\\boldsymbol{\\Sigma}_i^{-1}$ exists and is also positive-definite, therefore it is invertible. We can multiply by $\\boldsymbol{\\Sigma}_i$ to get:\n$$\n\\sum_{n=1}^{N_i}\\mathbf{x}_n - N_i\\boldsymbol{\\mu}_i = \\mathbf{0} \\implies N_i\\boldsymbol{\\mu}_i = \\sum_{n=1}^{N_i}\\mathbf{x}_n\n$$\nThe MLE for the mean, denoted $\\hat{\\boldsymbol{\\mu}}_i$, is the sample mean:\n$$\n\\hat{\\boldsymbol{\\mu}}_i = \\frac{1}{N_i}\\sum_{n=1}^{N_i}\\mathbf{x}_n\n$$\n\n**Maximum Likelihood Estimate for the Covariance $\\boldsymbol{\\Sigma}_i$**\n\nNext, we differentiate $\\ell$ with respect to $\\boldsymbol{\\Sigma}_i$. This is more conveniently done with respect to the inverse covariance matrix, or precision matrix, $\\mathbf{\\Lambda}_i = \\boldsymbol{\\Sigma}_i^{-1}$. We rewrite the log-likelihood using $\\ln|\\boldsymbol{\\Sigma}_i| = -\\ln|\\mathbf{\\Lambda}_i|$ and the trace cyclicity property $(\\mathbf{v}^\\top\\mathbf{A}\\mathbf{v} = \\text{tr}(\\mathbf{A}\\mathbf{v}\\mathbf{v}^\\top))$:\n$$\n\\ell(\\hat{\\boldsymbol{\\mu}}_i, \\mathbf{\\Lambda}_i) = C + \\frac{N_i}{2}\\ln|\\mathbf{\\Lambda}_i| - \\frac{1}{2}\\sum_{n=1}^{N_i}(\\mathbf{x}_n-\\hat{\\boldsymbol{\\mu}}_i)^{\\top}\\mathbf{\\Lambda}_i(\\mathbf{x}_n-\\hat{\\boldsymbol{\\mu}}_i)\n$$\n$$\n\\ell(\\hat{\\boldsymbol{\\mu}}_i, \\mathbf{\\Lambda}_i) = C + \\frac{N_i}{2}\\ln|\\mathbf{\\Lambda}_i| - \\frac{1}{2}\\text{tr}\\left( \\mathbf{\\Lambda}_i \\sum_{n=1}^{N_i}(\\mathbf{x}_n-\\hat{\\boldsymbol{\\mu}}_i)(\\mathbf{x}_n-\\hat{\\boldsymbol{\\mu}}_i)^{\\top} \\right)\n$$\nWe define the scatter matrix $\\mathbf{S} = \\sum_{n=1}^{N_i}(\\mathbf{x}_n-\\hat{\\boldsymbol{\\mu}}_i)(\\mathbf{x}_n-\\hat{\\boldsymbol{\\mu}}_i)^{\\top}$.\nUsing the identities $\\frac{\\partial \\ln|\\mathbf{A}|}{\\partial \\mathbf{A}} = (\\mathbf{A}^{-1})^\\top$ and $\\frac{\\partial \\text{tr}(\\mathbf{A}\\mathbf{B})}{\\partial \\mathbf{A}} = \\mathbf{B}^\\top$:\n$$\n\\frac{\\partial \\ell}{\\partial \\mathbf{\\Lambda}_i} = \\frac{N_i}{2}(\\mathbf{\\Lambda}_i^{-1})^\\top - \\frac{1}{2}\\mathbf{S}^\\top\n$$\nSince both $\\mathbf{\\Lambda}_i$ and $\\mathbf{S}$ are symmetric, we have:\n$$\n\\frac{\\partial \\ell}{\\partial \\mathbf{\\Lambda}_i} = \\frac{N_i}{2}\\mathbf{\\Lambda}_i^{-1} - \\frac{1}{2}\\mathbf{S}\n$$\nSetting to the zero matrix and recalling $\\mathbf{\\Lambda}_i^{-1} = \\boldsymbol{\\Sigma}_i$:\n$$\n\\frac{N_i}{2}\\boldsymbol{\\Sigma}_i - \\frac{1}{2}\\mathbf{S} = \\mathbf{0} \\implies \\boldsymbol{\\Sigma}_i = \\frac{1}{N_i}\\mathbf{S}\n$$\nThe MLE for the covariance matrix, denoted $\\hat{\\boldsymbol{\\Sigma}}_i$, is:\n$$\n\\hat{\\boldsymbol{\\Sigma}}_i = \\frac{1}{N_i}\\sum_{n=1}^{N_i}(\\mathbf{x}_n-\\hat{\\boldsymbol{\\mu}}_i)(\\mathbf{x}_n-\\hat{\\boldsymbol{\\mu}}_i)^{\\top}\n$$\n\n**Bias Analysis of the Covariance Estimator**\n\nTo analyze the bias of $\\hat{\\boldsymbol{\\Sigma}}_i$, we compute its expected value. Let the true (but unknown) parameters be $\\boldsymbol{\\mu}_i$ and $\\boldsymbol{\\Sigma}_i$.\n$$\nE[\\hat{\\boldsymbol{\\Sigma}}_i] = E\\left[\\frac{1}{N_i}\\sum_{n=1}^{N_i}(\\mathbf{x}_n-\\hat{\\boldsymbol{\\mu}}_i)(\\mathbf{x}_n-\\hat{\\boldsymbol{\\mu}}_i)^{\\top}\\right]\n$$\nWe substitute $\\mathbf{x}_n-\\hat{\\boldsymbol{\\mu}}_i = (\\mathbf{x}_n-\\boldsymbol{\\mu}_i) - (\\hat{\\boldsymbol{\\mu}}_i-\\boldsymbol{\\mu}_i)$:\n$$\n\\sum_{n=1}^{N_i}(\\mathbf{x}_n-\\hat{\\boldsymbol{\\mu}}_i)(\\mathbf{x}_n-\\hat{\\boldsymbol{\\mu}}_i)^{\\top} = \\sum_{n=1}^{N_i} \\left[ (\\mathbf{x}_n-\\boldsymbol{\\mu}_i)(\\mathbf{x}_n-\\boldsymbol{\\mu}_i)^\\top - (\\mathbf{x}_n-\\boldsymbol{\\mu}_i)(\\hat{\\boldsymbol{\\mu}}_i-\\boldsymbol{\\mu}_i)^\\top - (\\hat{\\boldsymbol{\\mu}}_i-\\boldsymbol{\\mu}_i)(\\mathbf{x}_n-\\boldsymbol{\\mu}_i)^\\top + (\\hat{\\boldsymbol{\\mu}}_i-\\boldsymbol{\\mu}_i)(\\hat{\\boldsymbol{\\mu}}_i-\\boldsymbol{\\mu}_i)^\\top \\right]\n$$\nSumming the terms and using $\\sum_{n=1}^{N_i}(\\mathbf{x}_n-\\boldsymbol{\\mu}_i) = N_i(\\hat{\\boldsymbol{\\mu}}_i-\\boldsymbol{\\mu}_i)$:\n$$\n= \\sum_{n=1}^{N_i}(\\mathbf{x}_n-\\boldsymbol{\\mu}_i)(\\mathbf{x}_n-\\boldsymbol{\\mu}_i)^\\top - N_i(\\hat{\\boldsymbol{\\mu}}_i-\\boldsymbol{\\mu}_i)(\\hat{\\boldsymbol{\\mu}}_i-\\boldsymbol{\\mu}_i)^\\top - N_i(\\hat{\\boldsymbol{\\mu}}_i-\\boldsymbol{\\mu}_i)(\\hat{\\boldsymbol{\\mu}}_i-\\boldsymbol{\\mu}_i)^\\top + N_i(\\hat{\\boldsymbol{\\mu}}_i-\\boldsymbol{\\mu}_i)(\\hat{\\boldsymbol{\\mu}}_i-\\boldsymbol{\\mu}_i)^\\top\n$$\n$$\n= \\sum_{n=1}^{N_i}(\\mathbf{x}_n-\\boldsymbol{\\mu}_i)(\\mathbf{x}_n-\\boldsymbol{\\mu}_i)^\\top - N_i(\\hat{\\boldsymbol{\\mu}}_i-\\boldsymbol{\\mu}_i)(\\hat{\\boldsymbol{\\mu}}_i-\\boldsymbol{\\mu}_i)^\\top\n$$\nNow we take the expectation. The expectation of the first term is $\\sum_{n=1}^{N_i} E[(\\mathbf{x}_n-\\boldsymbol{\\mu}_i)(\\mathbf{x}_n-\\boldsymbol{\\mu}_i)^\\top] = \\sum_{n=1}^{N_i} \\boldsymbol{\\Sigma}_i = N_i \\boldsymbol{\\Sigma}_i$.\nThe expectation of the second term involves the covariance of the sample mean: $E[(\\hat{\\boldsymbol{\\mu}}_i-\\boldsymbol{\\mu}_i)(\\hat{\\boldsymbol{\\mu}}_i-\\boldsymbol{\\mu}_i)^\\top] = \\text{Cov}(\\hat{\\boldsymbol{\\mu}}_i) = \\text{Cov}(\\frac{1}{N_i}\\sum \\mathbf{x}_n) = \\frac{1}{N_i^2}\\sum\\text{Cov}(\\mathbf{x}_n) = \\frac{N_i}{N_i^2}\\boldsymbol{\\Sigma}_i = \\frac{1}{N_i}\\boldsymbol{\\Sigma}_i$.\nSo, $E[N_i(\\hat{\\boldsymbol{\\mu}}_i-\\boldsymbol{\\mu}_i)(\\hat{\\boldsymbol{\\mu}}_i-\\boldsymbol{\\mu}_i)^\\top] = N_i (\\frac{1}{N_i}\\boldsymbol{\\Sigma}_i) = \\boldsymbol{\\Sigma}_i$.\nPutting it all together:\n$$\nE[N_i \\hat{\\boldsymbol{\\Sigma}}_i] = N_i \\boldsymbol{\\Sigma}_i - \\boldsymbol{\\Sigma}_i = (N_i-1)\\boldsymbol{\\Sigma}_i\n$$\n$$\nE[\\hat{\\boldsymbol{\\Sigma}}_i] = \\frac{N_i-1}{N_i}\\boldsymbol{\\Sigma}_i\n$$\nThis shows that the MLE for the covariance is biased. The bias is $E[\\hat{\\boldsymbol{\\Sigma}}_i] - \\boldsymbol{\\Sigma}_i = -\\frac{1}{N_i}\\boldsymbol{\\Sigma}_i$. The estimator systematically underestimates the true covariance.\n\n**Bias-Corrected Estimator and Implications**\n\nFrom the bias calculation, we can construct an unbiased estimator by scaling the MLE by a factor of $\\frac{N_i}{N_i-1}$. This gives the familiar sample covariance matrix, let's call it $\\mathbf{S}_i$:\n$$\n\\mathbf{S}_i = \\frac{N_i}{N_i-1}\\hat{\\boldsymbol{\\Sigma}}_i = \\frac{1}{N_i-1}\\sum_{n=1}^{N_i}(\\mathbf{x}_n-\\hat{\\boldsymbol{\\mu}}_i)(\\mathbf{x}_n-\\hat{\\boldsymbol{\\mu}}_i)^{\\top}\n$$\nThis estimator is unbiased, i.e., $E[\\mathbf{S}_i] = \\boldsymbol{\\Sigma}_i$.\n\nWhen the number of samples $N_i$ is small relative to the dimensionality $d$, we encounter a critical issue. The scatter matrix $\\mathbf{S} = \\sum_{n=1}^{N_i}(\\mathbf{x}_n-\\hat{\\boldsymbol{\\mu}}_i)(\\mathbf{x}_n-\\hat{\\boldsymbol{\\mu}}_i)^{\\top}$ is a sum of $N_i$ matrices, each of rank at most $1$. Furthermore, the vectors $\\{\\mathbf{x}_n-\\hat{\\boldsymbol{\\mu}}_i\\}_{n=1}^{N_i}$ are linearly dependent as they sum to the zero vector, and thus span a subspace of dimension at most $N_i-1$. Consequently, the rank of the scatter matrix (and thus the rank of $\\hat{\\boldsymbol{\\Sigma}}_i$ and $\\mathbf{S}_i$) is at most $\\min(d, N_i-1)$.\nIf $N_i-1  d$ (or more generally $N_i \\le d$), the estimated covariance matrix $\\mathbf{S}_i$ will be singular (rank-deficient).\n\nThis has severe implications for spectral separability analysis:\n1.  **Invertibility Failure**: Many classifiers (e.g., Quadratic Discriminant Analysis) and separability metrics (e.g., Mahalanobis distance, Bhattacharyya distance) require computing the inverse of the covariance matrix, $\\boldsymbol{\\Sigma}_i^{-1}$. A singular estimate $\\mathbf{S}_i$ has no inverse, causing these methods to fail entirely.\n2.  **Degenerate Model**: A singular covariance matrix implies that the estimated Gaussian distribution is degenerate, with its probability mass concentrated on a lower-dimensional subspace of $\\mathbb{R}^d$. This is an artifact of insufficient data, not a true property of the underlying land cover class.\n3.  **High Variance**: Even when $N_i$ is slightly larger than $d$, the estimate $\\mathbf{S}_i$ is often numerically ill-conditioned (close to singular), and its elements have very high variance. This makes the estimated class signatures unreliable and leads to poor generalization performance of classifiers.\n\nThis is a manifestation of the \"curse of dimensionality\" in a high-dimensional, small sample size scenario, a common problem in remote sensing. To mitigate this, one must employ techniques such as regularization (e.g., shrinkage), dimensionality reduction, or using simpler covariance models (e.g., diagonal or pooled covariance).",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{1}{N_i} \\sum_{n=1}^{N_i} \\mathbf{x}_n  \\frac{1}{N_i} \\sum_{n=1}^{N_i} \\left(\\mathbf{x}_n - \\frac{1}{N_i}\\sum_{m=1}^{N_i} \\mathbf{x}_m\\right)\\left(\\mathbfx_n - \\frac{1}{N_i}\\sum_{m=1}^{N_i} \\mathbf{x}_m\\right)^{\\top} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Once we have statistical signatures for our classes, we need a way to quantify how distinguishable they are. This practice introduces the Bhattacharyya distance, a powerful metric derived from the degree of overlap between class probability distributions. By deriving its closed-form expression and analyzing how it is influenced by the covariance structure, you will develop a geometric intuition for separability, understanding that it depends not just on the distance between class means but critically on the variance and correlation within each class .",
            "id": "3853814",
            "problem": "In spectral separability analysis for supervised land-cover mapping, training signatures for two classes are commonly modeled as multivariate Gaussian distributions across $d$ spectral bands. Consider two land-cover classes, seasonally flooded grassland (SFG) and irrigated rice paddy (IRP), each represented by a $d$-dimensional multivariate normal density $p_{i}(\\mathbf{x})$ with mean $\\boldsymbol{\\mu}_{i} \\in \\mathbb{R}^{d}$ and covariance $\\boldsymbol{\\Sigma}_{i} \\in \\mathbb{R}^{d \\times d}$ for $i \\in \\{1,2\\}$. A widely used separability measure is the Bhattacharyya distance (BD), defined from the Bhattacharyya coefficient $\\mathrm{BC}$ by $B = -\\ln(\\mathrm{BC})$, where the Bhattacharyya coefficient is given by the overlap integral $\\mathrm{BC} = \\int_{\\mathbb{R}^{d}} \\sqrt{p_{1}(\\mathbf{x}) p_{2}(\\mathbf{x})} \\, d\\mathbf{x}$.\n\nStarting from these definitions and the standard form of the multivariate normal density, derive a closed-form expression for $B$ under the equal covariance assumption $\\boldsymbol{\\Sigma}_{1} = \\boldsymbol{\\Sigma}_{2} = \\boldsymbol{\\Sigma}$. Then, using the eigen-decomposition of $\\boldsymbol{\\Sigma}$, analyze how anisotropy in $\\boldsymbol{\\Sigma}$ skews separability along different spectral directions.\n\nFinally, evaluate $B$ numerically for the following three-band reflectance signatures (bands are green, red, and near-infrared in this order): $\\boldsymbol{\\mu}_{1} = \\begin{pmatrix} 0.08 \\\\ 0.06 \\\\ 0.32 \\end{pmatrix}$, $\\boldsymbol{\\mu}_{2} = \\begin{pmatrix} 0.10 \\\\ 0.05 \\\\ 0.36 \\end{pmatrix}$, and $\\boldsymbol{\\Sigma} = \\operatorname{diag}\\!\\big(0.01, 0.02, 0.09\\big)$. Express your final numerical answer for $B$ as a dimensionless real number, and round your answer to four significant figures.",
            "solution": "The problem as stated is subjected to a validation process to ensure its scientific and logical soundness.\n\n### Step 1: Extract Givens\n-   Two land-cover classes are modeled as $d$-dimensional multivariate normal densities, $p_{i}(\\mathbf{x})$, for $i \\in \\{1,2\\}$.\n-   The probability density function for class $i$ is $p_{i}(\\mathbf{x}) \\sim \\mathcal{N}(\\boldsymbol{\\mu}_{i}, \\boldsymbol{\\Sigma}_{i})$, where $\\boldsymbol{\\mu}_{i} \\in \\mathbb{R}^{d}$ is the mean vector and $\\boldsymbol{\\Sigma}_{i} \\in \\mathbb{R}^{d \\times d}$ is the covariance matrix.\n-   The Bhattacharyya distance, $B$, is defined as $B = -\\ln(\\mathrm{BC})$.\n-   The Bhattacharyya coefficient, $\\mathrm{BC}$, is defined by the integral $\\mathrm{BC} = \\int_{\\mathbb{R}^{d}} \\sqrt{p_{1}(\\mathbf{x}) p_{2}(\\mathbf{x})} \\, d\\mathbf{x}$.\n-   An assumption is made of equal covariance: $\\boldsymbol{\\Sigma}_{1} = \\boldsymbol{\\Sigma}_{2} = \\boldsymbol{\\Sigma}$.\n-   Task 1: Derive a closed-form expression for $B$ under the equal covariance assumption.\n-   Task 2: Analyze how anisotropy in $\\boldsymbol{\\Sigma}$ skews separability, using the eigen-decomposition of $\\boldsymbol{\\Sigma}$.\n-   Task 3: Evaluate $B$ numerically for the specific case:\n    -   Number of bands $d=3$.\n    -   Mean vectors: $\\boldsymbol{\\mu}_{1} = \\begin{pmatrix} 0.08 \\\\ 0.06 \\\\ 0.32 \\end{pmatrix}$ and $\\boldsymbol{\\mu}_{2} = \\begin{pmatrix} 0.10 \\\\ 0.05 \\\\ 0.36 \\end{pmatrix}$.\n    -   Covariance matrix: $\\boldsymbol{\\Sigma} = \\operatorname{diag}\\!\\big(0.01, 0.02, 0.09\\big)$.\n    -   The final numerical answer for $B$ should be a dimensionless real number, rounded to four significant figures.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is evaluated against the established criteria:\n-   **Scientifically Grounded:** The problem is firmly rooted in statistical pattern recognition and its application to remote sensing, a standard practice in environmental science and geography. The use of multivariate normal distributions to model spectral signatures and the Bhattacharyya distance as a separability metric are both well-established methods.\n-   **Well-Posed:** The problem provides all necessary definitions, data, and constraints to derive the requested expression, perform the analysis, and compute the numerical result. The question is unambiguous and leads to a unique, stable, and meaningful solution.\n-   **Objective:** The problem is stated using precise, formal mathematical and scientific language, free from any subjectivity or bias.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. It is scientifically sound, well-posed, and objective. It contains no logical contradictions, missing information, or unscientific premises. The solution process may therefore proceed.\n\n### Derivation of the Bhattacharyya Distance\n\nThe probability density function for a $d$-dimensional multivariate normal distribution for class $i$ is given by:\n$$\np_{i}(\\mathbf{x}) = \\frac{1}{(2\\pi)^{d/2} |\\boldsymbol{\\Sigma}_i|^{1/2}} \\exp\\left( -\\frac{1}{2} (\\mathbf{x} - \\boldsymbol{\\mu}_i)^T \\boldsymbol{\\Sigma}_i^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}_i) \\right)\n$$\nUnder the assumption of equal covariance matrices, $\\boldsymbol{\\Sigma}_1 = \\boldsymbol{\\Sigma}_2 = \\boldsymbol{\\Sigma}$, the densities are:\n$$\np_{1}(\\mathbf{x}) = \\frac{1}{(2\\pi)^{d/2} |\\boldsymbol{\\Sigma}|^{1/2}} \\exp\\left( -\\frac{1}{2} (\\mathbf{x} - \\boldsymbol{\\mu}_1)^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}_1) \\right)\n$$\n$$\np_{2}(\\mathbf{x}) = \\frac{1}{(2\\pi)^{d/2} |\\boldsymbol{\\Sigma}|^{1/2}} \\exp\\left( -\\frac{1}{2} (\\mathbf{x} - \\boldsymbol{\\mu}_2)^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}_2) \\right)\n$$\nThe Bhattacharyya coefficient, $\\mathrm{BC}$, requires computing the geometric mean $\\sqrt{p_{1}(\\mathbf{x}) p_{2}(\\mathbf{x})}$:\n$$\n\\sqrt{p_{1}(\\mathbf{x}) p_{2}(\\mathbf{x})} = \\frac{1}{(2\\pi)^{d/2} |\\boldsymbol{\\Sigma}|^{1/2}} \\exp\\left( -\\frac{1}{4} \\left[ (\\mathbf{x} - \\boldsymbol{\\mu}_1)^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}_1) + (\\mathbf{x} - \\boldsymbol{\\mu}_2)^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}_2) \\right] \\right)\n$$\nLet us analyze the term in the exponent. Let $Q(\\mathbf{x}) = (\\mathbf{x} - \\boldsymbol{\\mu}_1)^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}_1) + (\\mathbf{x} - \\boldsymbol{\\mu}_2)^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}_2)$. Expanding and collecting terms in $\\mathbf{x}$ gives:\n$$\nQ(\\mathbf{x}) = 2\\mathbf{x}^T \\boldsymbol{\\Sigma}^{-1} \\mathbf{x} - 2\\mathbf{x}^T \\boldsymbol{\\Sigma}^{-1} (\\boldsymbol{\\mu}_1 + \\boldsymbol{\\mu}_2) + \\boldsymbol{\\mu}_1^T \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}_1 + \\boldsymbol{\\mu}_2^T \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}_2\n$$\nWe complete the square for the terms involving $\\mathbf{x}$. Let $\\boldsymbol{\\mu}' = \\frac{\\boldsymbol{\\mu}_1 + \\boldsymbol{\\mu}_2}{2}$. The quadratic form can be expressed as:\n$$\n2(\\mathbf{x} - \\boldsymbol{\\mu}')^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}') = 2\\mathbf{x}^T \\boldsymbol{\\Sigma}^{-1} \\mathbf{x} - 2\\mathbf{x}^T \\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{\\mu}_1+\\boldsymbol{\\mu}_2) + 2(\\boldsymbol{\\mu}')^T \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}'\n$$\nSubstituting this into the expression for $Q(\\mathbf{x})$:\n$$\nQ(\\mathbf{x}) = 2(\\mathbf{x} - \\boldsymbol{\\mu}')^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}') + \\boldsymbol{\\mu}_1^T \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}_1 + \\boldsymbol{\\mu}_2^T \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}_2 - 2(\\boldsymbol{\\mu}')^T \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}'\n$$\nThe constant term simplifies to:\n$$\n\\boldsymbol{\\mu}_1^T \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}_1 + \\boldsymbol{\\mu}_2^T \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}_2 - 2\\left(\\frac{\\boldsymbol{\\mu}_1 + \\boldsymbol{\\mu}_2}{2}\\right)^T \\boldsymbol{\\Sigma}^{-1} \\left(\\frac{\\boldsymbol{\\mu}_1 + \\boldsymbol{\\mu}_2}{2}\\right)\n= \\boldsymbol{\\mu}_1^T \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}_1 + \\boldsymbol{\\mu}_2^T \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}_2 - \\frac{1}{2}(\\boldsymbol{\\mu}_1^T \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}_1 + 2\\boldsymbol{\\mu}_1^T \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}_2 + \\boldsymbol{\\mu}_2^T \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}_2)\n$$\n$$\n= \\frac{1}{2}(\\boldsymbol{\\mu}_1^T \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}_1 - 2\\boldsymbol{\\mu}_1^T \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}_2 + \\boldsymbol{\\mu}_2^T \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}_2) = \\frac{1}{2}(\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_2)^T \\boldsymbol{\\Sigma}^{-1} (\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_2)\n$$\nThe exponent in the geometric mean becomes:\n$$\n-\\frac{1}{4}Q(\\mathbf{x}) = -\\frac{1}{2}(\\mathbf{x} - \\boldsymbol{\\mu}')^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}') - \\frac{1}{8}(\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_2)^T \\boldsymbol{\\Sigma}^{-1} (\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_2)\n$$\nThus, the integral for the Bhattacharyya coefficient is:\n$$\n\\mathrm{BC} = \\int_{\\mathbb{R}^{d}} \\frac{1}{(2\\pi)^{d/2} |\\boldsymbol{\\Sigma}|^{1/2}} \\exp\\left(-\\frac{1}{8}(\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_2)^T \\boldsymbol{\\Sigma}^{-1} (\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_2)\\right) \\exp\\left(-\\frac{1}{2}(\\mathbf{x} - \\boldsymbol{\\mu}')^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}')\\right) d\\mathbf{x}\n$$\nThe constant term can be factored out of the integral:\n$$\n\\mathrm{BC} = \\exp\\left(-\\frac{1}{8}(\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_2)^T \\boldsymbol{\\Sigma}^{-1} (\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_2)\\right) \\int_{\\mathbb{R}^{d}} \\frac{1}{(2\\pi)^{d/2} |\\boldsymbol{\\Sigma}|^{1/2}} \\exp\\left(-\\frac{1}{2}(\\mathbf{x} - \\boldsymbol{\\mu}')^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}')\\right) d\\mathbf{x}\n$$\nThe remaining integral is that of a multivariate normal probability density $\\mathcal{N}(\\boldsymbol{\\mu}', \\boldsymbol{\\Sigma})$ over its entire domain $\\mathbb{R}^d$, which equals $1$. Therefore:\n$$\n\\mathrm{BC} = \\exp\\left(-\\frac{1}{8}(\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_2)^T \\boldsymbol{\\Sigma}^{-1} (\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_2)\\right)\n$$\nThe Bhattacharyya distance $B = -\\ln(\\mathrm{BC})$ is then:\n$$\nB = \\frac{1}{8}(\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_2)^T \\boldsymbol{\\Sigma}^{-1} (\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_2)\n$$\nThis expression is $\\frac{1}{8}$ of the squared Mahalanobis distance between the two mean vectors $\\boldsymbol{\\mu}_1$ and $\\boldsymbol{\\mu}_2$.\n\n### Analysis of Anisotropy\nTo analyze how anisotropy in $\\boldsymbol{\\Sigma}$ affects separability, we perform an eigen-decomposition of the symmetric, positive-definite covariance matrix $\\boldsymbol{\\Sigma}$:\n$$\n\\boldsymbol{\\Sigma} = \\mathbf{V} \\mathbf{\\Lambda} \\mathbf{V}^T\n$$\nwhere $\\mathbf{V}$ is an orthogonal matrix whose columns are the eigenvectors $\\mathbf{v}_j$ of $\\boldsymbol{\\Sigma}$, and $\\mathbf{\\Lambda}$ is a diagonal matrix of the corresponding positive eigenvalues $\\lambda_j$. The inverse is $\\boldsymbol{\\Sigma}^{-1} = \\mathbf{V} \\mathbf{\\Lambda}^{-1} \\mathbf{V}^T$. Let $\\Delta\\boldsymbol{\\mu} = \\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_2$. Substituting the decomposition into the expression for $B$:\n$$\nB = \\frac{1}{8}(\\Delta\\boldsymbol{\\mu})^T (\\mathbf{V} \\mathbf{\\Lambda}^{-1} \\mathbf{V}^T) (\\Delta\\boldsymbol{\\mu}) = \\frac{1}{8} (\\mathbf{V}^T \\Delta\\boldsymbol{\\mu})^T \\mathbf{\\Lambda}^{-1} (\\mathbf{V}^T \\Delta\\boldsymbol{\\mu})\n$$\nLet $\\mathbf{c} = \\mathbf{V}^T \\Delta\\boldsymbol{\\mu}$. The components of $\\mathbf{c}$ are $c_j = \\mathbf{v}_j^T \\Delta\\boldsymbol{\\mu}$, which represent the projection of the mean difference vector $\\Delta\\boldsymbol{\\mu}$ onto each eigenvector $\\mathbf{v}_j$. The expression for $B$ becomes:\n$$\nB = \\frac{1}{8} \\mathbf{c}^T \\mathbf{\\Lambda}^{-1} \\mathbf{c} = \\frac{1}{8} \\sum_{j=1}^{d} \\frac{c_j^2}{\\lambda_j}\n$$\nThis form reveals how anisotropy (non-uniform eigenvalues) skews separability:\n-   The total separability $B$ is a weighted sum of the squared components of the mean difference vector, with components defined along the principal axes (eigenvectors) of the covariance structure.\n-   The weight for each component is the inverse of the corresponding eigenvalue, $1/\\lambda_j$. The eigenvalue $\\lambda_j$ represents the variance of the data along the axis $\\mathbf{v}_j$.\n-   A small eigenvalue $\\lambda_j$ implies low variance (i.e., the data clusters are \"thin\") along the direction $\\mathbf{v}_j$. Its inverse $1/\\lambda_j$ is large. Thus, any separation between the means along this direction ($c_j \\neq 0$) contributes heavily to the total separability $B$.\n-   Conversely, a large eigenvalue $\\lambda_k$ implies high variance (the data clusters are \"wide\") along direction $\\mathbf{v}_k$. Its inverse $1/\\lambda_k$ is small. A separation between the means along this high-variance direction contributes minimally to $B$.\n-   Therefore, anisotropy in $\\boldsymbol{\\Sigma}$ makes separability highly dependent on direction. The spectral directions with the lowest variance are the most critical for discriminating between the classes.\n\n### Numerical Evaluation\n\nWe are given $d=3$, $\\boldsymbol{\\mu}_{1} = \\begin{pmatrix} 0.08 \\\\ 0.06 \\\\ 0.32 \\end{pmatrix}$, $\\boldsymbol{\\mu}_{2} = \\begin{pmatrix} 0.10 \\\\ 0.05 \\\\ 0.36 \\end{pmatrix}$, and $\\boldsymbol{\\Sigma} = \\operatorname{diag}\\!\\big(0.01, 0.02, 0.09\\big)$.\n\nFirst, we calculate the difference between the mean vectors, $\\Delta\\boldsymbol{\\mu}$:\n$$\n\\Delta\\boldsymbol{\\mu} = \\boldsymbol{\\mu}_{1} - \\boldsymbol{\\mu}_{2} = \\begin{pmatrix} 0.08 - 0.10 \\\\ 0.06 - 0.05 \\\\ 0.32 - 0.36 \\end{pmatrix} = \\begin{pmatrix} -0.02 \\\\ 0.01 \\\\ -0.04 \\end{pmatrix}\n$$\nNext, we find the inverse of the covariance matrix, $\\boldsymbol{\\Sigma}^{-1}$. Since $\\boldsymbol{\\Sigma}$ is diagonal, its inverse is the diagonal matrix of the reciprocals of its elements:\n$$\n\\boldsymbol{\\Sigma}^{-1} = \\operatorname{diag}\\!\\left(\\frac{1}{0.01}, \\frac{1}{0.02}, \\frac{1}{0.09}\\right) = \\operatorname{diag}\\!\\left(100, 50, \\frac{100}{9}\\right)\n$$\nNow, we compute the quadratic form $(\\Delta\\boldsymbol{\\mu})^T \\boldsymbol{\\Sigma}^{-1} (\\Delta\\boldsymbol{\\mu})$:\n$$\n(\\Delta\\boldsymbol{\\mu})^T \\boldsymbol{\\Sigma}^{-1} (\\Delta\\boldsymbol{\\mu}) = \\begin{pmatrix} -0.02  0.01  -0.04 \\end{pmatrix} \\begin{pmatrix} 100  0  0 \\\\ 0  50  0 \\\\ 0  0  \\frac{100}{9} \\end{pmatrix} \\begin{pmatrix} -0.02 \\\\ 0.01 \\\\ -0.04 \\end{pmatrix}\n$$\n$$\n= (-0.02)(100)(-0.02) + (0.01)(50)(0.01) + (-0.04)\\left(\\frac{100}{9}\\right)(-0.04)\n$$\n$$\n= \\frac{(-0.02)^2}{0.01} + \\frac{(0.01)^2}{0.02} + \\frac{(-0.04)^2}{0.09}\n$$\n$$\n= \\frac{0.0004}{0.01} + \\frac{0.0001}{0.02} + \\frac{0.0016}{0.09}\n$$\n$$\n= 0.04 + 0.005 + \\frac{16}{900} = 0.045 + \\frac{4}{225}\n$$\nNumerically, $\\frac{4}{225} \\approx 0.01777...$. So the quadratic form is:\n$$\n0.045 + 0.01777... = 0.062777...\n$$\nFinally, we calculate the Bhattacharyya distance $B$:\n$$\nB = \\frac{1}{8} \\times (0.062777...) = 0.007847222...\n$$\nRounding to four significant figures, we get $0.007847$. In standard scientific notation, this is $7.847 \\times 10^{-3}$.",
            "answer": "$$\\boxed{7.847 \\times 10^{-3}}$$"
        },
        {
            "introduction": "In modern remote sensing, we often have access to a large number of spectral bands, but using all of them may not be optimal. This advanced practice challenges you to apply your understanding of separability metrics to the practical problem of feature selection. You will implement a greedy algorithm to build a high-performing subset of bands, using the Jeffries-Matusita distance as your optimization criterion, and compare its performance against a globally optimal solution . This exercise bridges the gap between statistical analysis and algorithmic problem-solving, a crucial skill in environmental modeling.",
            "id": "3853730",
            "problem": "You are given a set of training signatures for land cover classes, each represented by a multivariate Gaussian class-conditional model over a discrete set of spectral bands. For each class, the training signature is fully specified by a mean vector and a covariance matrix across the spectral bands. The aggregated spectral separability for a subset of bands is defined as the mean of all pairwise Jeffries-Matusita (JM) distances between the class-conditional distributions restricted to that subset. The Jeffries-Matusita (JM) distance is defined via the Bhattacharyya distance between two multivariate normal class-conditional distributions. Your task is to formulate and implement a greedy algorithm that iteratively adds one band at a time to a growing subset so as to maximize the incremental increase in the aggregated separability. The algorithm must terminate when either a specified band budget is reached or the incremental increase is not strictly greater than a specified threshold. You must also compute the globally optimal subset of the same cardinality by exhaustive search to enable an empirical assessment of potential suboptimality of the greedy selection. All computations must be expressed in purely numerical terms. No physical units, angles, or percentages are involved.\n\nBase assumptions and definitions to be used:\n- Each class $c$ has a training signature characterized by a mean vector $\\boldsymbol{\\mu}_c \\in \\mathbb{R}^p$ and a positive-definite covariance matrix $\\boldsymbol{\\Sigma}_c \\in \\mathbb{R}^{p \\times p}$ over $p$ spectral bands.\n- For any subset of bands $S \\subseteq \\{0,1,\\ldots,p-1\\}$, the restriction of a class $c$ to $S$ is given by the subvector $\\boldsymbol{\\mu}_{c,S}$ and the principal submatrix $\\boldsymbol{\\Sigma}_{c,S}$.\n- The pairwise spectral separability between two classes $i$ and $j$ over $S$ is the Jeffries-Matusita distance $J_{ij}(S)$, defined through the Bhattacharyya distance between the multivariate Gaussian distributions for classes $i$ and $j$ restricted to $S$.\n- The aggregated separability over $S$ for $m$ classes is the arithmetic mean of $J_{ij}(S)$ over all $\\binom{m}{2}$ unordered class pairs.\n\nAlgorithmic requirements:\n- Implement a greedy selection procedure that begins with the empty subset $S = \\varnothing$ and iteratively adds a single band $b \\in \\{0,1,\\ldots,p-1\\} \\setminus S$ that maximizes the incremental gain $\\Delta J = J_{\\text{agg}}(S \\cup \\{b\\}) - J_{\\text{agg}}(S)$, where $J_{\\text{agg}}(\\cdot)$ denotes aggregated separability. If the maximum $\\Delta J$ is not strictly greater than a threshold $\\varepsilon$, terminate. Otherwise, add the band with the largest $\\Delta J$. Stop when either the band budget $K$ bands have been selected or the threshold condition triggers termination.\n- Separately, compute the globally optimal subset of exactly $K$ bands by exhaustive search over all subsets of cardinality $K$ that maximizes $J_{\\text{agg}}(S)$.\n\nOutput specification:\n- For each test case, produce a list containing: the list of greedily selected band indices in ascending order, the final aggregated separability as a real number for the greedy subset, the globally optimal aggregated separability for subsets of exactly $K$ bands, and a boolean indicating whether the greedy subset (of whatever size up to $K$) attains the same aggregated separability as the optimal subset of exactly $K$ bands.\n- Your program should produce a single line of output containing the results for all provided test cases as a comma-separated list enclosed in square brackets, where each test case result is itself a list. For example, the final output must look like $[\\,[\\cdots],\\,[\\cdots],\\,\\ldots\\,]$.\n\nTest suite:\n- Test case $1$ (happy path, moderate separation, diagonal covariances):\n    - Number of bands $p = 5$, number of classes $m = 3$, budget $K = 3$, threshold $\\varepsilon = 10^{-6}$.\n    - Class means:\n        - Class $1$: $[\\,0.1,\\,0.5,\\,0.3,\\,0.0,\\,0.2\\,]$\n        - Class $2$: $[\\,0.9,\\,0.1,\\,0.2,\\,0.1,\\,0.4\\,]$\n        - Class $3$: $[\\,0.3,\\,0.4,\\,0.9,\\,0.2,\\,0.1\\,]$\n    - Covariance matrices (identical across classes, diagonal):\n        - For each class $c$, $\\boldsymbol{\\Sigma}_c = \\mathrm{diag}(\\,[\\,0.02,\\,0.02,\\,0.03,\\,0.02,\\,0.03\\,]\\,)$.\n- Test case $2$ (boundary condition, single-band budget):\n    - Number of bands $p = 4$, number of classes $m = 2$, budget $K = 1$, threshold $\\varepsilon = 0$.\n    - Class means:\n        - Class $1$: $[\\,0.0,\\,0.5,\\,0.2,\\,0.1\\,]$\n        - Class $2$: $[\\,0.7,\\,0.6,\\,0.2,\\,0.1\\,]$\n    - Covariance matrices (identical across classes, diagonal):\n        - For each class $c$, $\\boldsymbol{\\Sigma}_c = \\mathrm{diag}(\\,[\\,0.04,\\,0.01,\\,0.02,\\,0.02\\,]\\,)$.\n- Test case $3$ (redundancy due to correlation, multi-class):\n    - Number of bands $p = 6$, number of classes $m = 3$, budget $K = 3$, threshold $\\varepsilon = 10^{-6}$.\n    - Class means:\n        - Class $1$: $[\\,0.2,\\,0.8,\\,0.4,\\,0.1,\\,0.6,\\,0.3\\,]$\n        - Class $2$: $[\\,0.5,\\,0.3,\\,0.45,\\,0.15,\\,0.55,\\,0.28\\,]$\n        - Class $3$: $[\\,0.3,\\,0.7,\\,0.35,\\,0.2,\\,0.5,\\,0.25\\,]$\n    - Covariance matrices (identical across classes, with correlation inducing redundancy):\n        $$\n        \\boldsymbol{\\Sigma}_c =\n        \\begin{bmatrix}\n        0.03  0.028  0  0  0  0 \\\\\n        0.028  0.03  0  0  0  0 \\\\\n        0  0  0.02  0.015  0  0 \\\\\n        0  0  0.015  0.02  0  0 \\\\\n        0  0  0  0  0.025  0 \\\\\n        0  0  0  0  0  0.025\n        \\end{bmatrix}.\n        $$\n- Test case $4$ (counterexample demonstrating suboptimal greedy due to synergistic covariance differences):\n    - Number of bands $p = 3$, number of classes $m = 2$, budget $K = 2$, threshold $\\varepsilon = 0$.\n    - Class means:\n        - Class $1$: $[\\,0.5,\\,0.5,\\,0.5\\,]$\n        - Class $2$: $[\\,0.5,\\,0.5,\\,0.0\\,]$\n    - Covariance matrices (distinct inter-band correlations across classes):\n        $$\n        \\boldsymbol{\\Sigma}_1 =\n        \\begin{bmatrix}\n        0.01  0.0099  0 \\\\\n        0.0099  0.01  0 \\\\\n        0  0  0.09\n        \\end{bmatrix}, \\quad\n        \\boldsymbol{\\Sigma}_2 =\n        \\begin{bmatrix}\n        0.01  0  0 \\\\\n        0  0.01  0 \\\\\n        0  0  0.09\n        \\end{bmatrix}.\n        $$\n    - This case is constructed so that bands $0$ and $1$ individually have negligible incremental gain, but together yield a large increase in aggregated separability due to differing covariance structure across classes, while band $2$ alone produces a moderate increase. The greedy algorithm, constrained to stepwise incremental gains, may select band $2$ first and then cannot realize the synergy between bands $0$ and $1$ within the budget $K = 2$, illustrating potential suboptimal selection.\n\nYour program must implement the greedy procedure and the exhaustive search as specified, apply them to the four test cases above, and print a single line containing a list of four results, one per test case. Each result must be a list of the form $[\\,\\text{selected\\_indices},\\,J_{\\text{greedy}},\\,J_{\\text{optimal}},\\,\\text{is\\_optimal}\\,]$, where $\\text{selected\\_indices}$ is a list of integers, $J_{\\text{greedy}}$ and $J_{\\text{optimal}}$ are real numbers, and $\\text{is\\_optimal}$ is a boolean.",
            "solution": "The problem presented is a well-defined task in the domain of statistical pattern recognition and remote sensing, specifically concerning feature selection. It is scientifically grounded, mathematically consistent, and all necessary parameters and test data are provided. The problem is therefore deemed valid and a solution will be provided.\n\nThe core of the problem is to select an optimal subset of spectral bands to maximize the separability between different land cover classes. Each class is modeled by a multivariate Gaussian distribution. The separability is quantified using the Jeffries-Matusita (JM) distance, which is derived from the Bhattacharyya distance. The task requires the implementation of two distinct algorithms for band selection: a greedy forward selection heuristic and an exhaustive search for the global optimum.\n\nFirst, let us formalize the key mathematical concepts. A class $c_i$ is described by a $p$-dimensional multivariate normal distribution, $\\mathcal{N}(\\boldsymbol{\\mu}_i, \\boldsymbol{\\Sigma}_i)$, where $\\boldsymbol{\\mu}_i \\in \\mathbb{R}^p$ is the mean vector and $\\boldsymbol{\\Sigma}_i \\in \\mathbb{R}^{p \\times p}$ is the positive-definite covariance matrix over $p$ spectral bands.\n\nFor a given subset of bands $S$ with cardinality $|S|=d$, the statistical distributions are restricted to these bands, yielding a mean subvector $\\boldsymbol{\\mu}_{i,S} \\in \\mathbb{R}^d$ and a principal submatrix $\\boldsymbol{\\Sigma}_{i,S} \\in \\mathbb{R}^{d \\times d}$. The separability between two classes, $c_i$ and $c_j$, over this subset $S$ is computed via the Bhattacharyya distance, $B_{ij}(S)$. For multivariate normal distributions, this is given by:\n\n$$\nB_{ij}(S) = \\frac{1}{8} (\\boldsymbol{\\mu}_{i,S} - \\boldsymbol{\\mu}_{j,S})^T \\left( \\frac{\\boldsymbol{\\Sigma}_{i,S} + \\boldsymbol{\\Sigma}_{j,S}}{2} \\right)^{-1} (\\boldsymbol{\\mu}_{i,S} - \\boldsymbol{\\mu}_{j,S}) + \\frac{1}{2} \\ln \\left( \\frac{\\det\\left(\\frac{\\boldsymbol{\\Sigma}_{i,S} + \\boldsymbol{\\Sigma}_{j,S}}{2}\\right)}{\\sqrt{\\det(\\boldsymbol{\\Sigma}_{i,S}) \\det(\\boldsymbol{\\Sigma}_{j,S})}} \\right)\n$$\n\nThe first term is a form of Mahalanobis distance, measuring the separation of the means, weighted by the average covariance. The second term measures the separability arising from differences in the covariance matrices themselves.\n\nThe Jeffries-Matusita (JM) distance, $J_{ij}(S)$, is a bounded function of the Bhattacharyya distance, which saturates as the classes become more separated. It is defined as:\n\n$$\nJ_{ij}(S) = 2(1 - e^{-B_{ij}(S)})\n$$\n\nThe value of $J_{ij}(S)$ ranges from $0$ (complete overlap) to $2$ (perfect separability).\n\nFor a set of $m$ classes, the aggregated separability over the band subset $S$, denoted $J_{\\text{agg}}(S)$, is the arithmetic mean of all pairwise JM distances:\n\n$$\nJ_{\\text{agg}}(S) = \\frac{1}{\\binom{m}{2}} \\sum_{i=0}^{m-2} \\sum_{j=i+1}^{m-1} J_{ij}(S) = \\frac{2}{m(m-1)} \\sum_{ij} J_{ij}(S)\n$$\n\nBy convention, the separability of the empty set is $J_{\\text{agg}}(\\varnothing) = 0$.\n\nThe problem requires two algorithmic approaches to find an optimal subset $S$:\n\n1.  **Greedy Forward Selection**: This is a constructive heuristic that builds a subset of bands iteratively. It starts with an empty set $S_0 = \\varnothing$. At each step $k$, it selects a single band $b_k$ from the set of available bands that provides the maximum incremental improvement to the aggregated separability. That is, the band $b_k$ is chosen such that:\n    $$\n    b_k = \\arg\\max_{b \\notin S_{k-1}} \\left( J_{\\text{agg}}(S_{k-1} \\cup \\{b\\}) - J_{\\text{agg}}(S_{k-1}) \\right)\n    $$\n    The process adds $b_k$ to the set, forming $S_k = S_{k-1} \\cup \\{b_k\\}$, and continues. The algorithm terminates if the selected subset reaches the specified budget size $K$, or if the maximum possible incremental gain at a step is not strictly positive and greater than a threshold $\\varepsilon$. This method is computationally efficient but is not guaranteed to find the globally optimal subset, as it can be trapped by locally optimal choices. The final subset may have a cardinality less than $K$ if the threshold condition is met.\n\n2.  **Exhaustive Search**: This algorithm guarantees finding the globally optimal solution for a fixed subset size $K$. It operates by systematically evaluating every possible subset of bands of cardinality $K$. The number of such subsets is given by the binomial coefficient $\\binom{p}{K}$. For each candidate subset $S$, $J_{\\text{agg}}(S)$ is computed. The subset yielding the highest $J_{\\text{agg}}$ value is the global optimum for that cardinality. The computational cost of this method is prohibitive for large $p$ and $K$, but it serves as the definitive benchmark for assessing the performance of the greedy heuristic.\n\nThe implementation will consist of a primary function to calculate $J_{\\text{agg}}(S)$ for any given subset. This function will extract the relevant subvectors and submatrices, then compute the Bhattacharyya distance using numerically stable methods (specifically, using the logarithm of the determinant to prevent underflow/overflow), and finally the JM distance. The greedy and exhaustive search procedures will then call this function as needed to evaluate candidate subsets. The final step is to compare the separability of the greedy-selected set, $J_{\\text{greedy}}$, with the optimal separability for a set of size $K$, $J_{\\text{optimal}}$, to determine if the greedy approach yielded the optimal result for that cardinality.",
            "answer": "```python\nimport numpy as np\nfrom itertools import combinations\n\ndef solve():\n    \"\"\"\n    Main function to define test cases and generate the final output.\n    \"\"\"\n    # Test case 1: Happy path, diagonal covariances\n    p1, m1, K1, eps1 = 5, 3, 3, 1e-6\n    means1 = np.array([\n        [0.1, 0.5, 0.3, 0.0, 0.2],\n        [0.9, 0.1, 0.2, 0.1, 0.4],\n        [0.3, 0.4, 0.9, 0.2, 0.1],\n    ])\n    covs1 = np.array([np.diag([0.02, 0.02, 0.03, 0.02, 0.03])] * 3)\n\n    # Test case 2: Single-band budget\n    p2, m2, K2, eps2 = 4, 2, 1, 0.0\n    means2 = np.array([\n        [0.0, 0.5, 0.2, 0.1],\n        [0.7, 0.6, 0.2, 0.1],\n    ])\n    covs2 = np.array([np.diag([0.04, 0.01, 0.02, 0.02])] * 2)\n\n    # Test case 3: Correlated bands\n    p3, m3, K3, eps3 = 6, 3, 3, 1e-6\n    means3 = np.array([\n        [0.2, 0.8, 0.4, 0.1, 0.6, 0.3],\n        [0.5, 0.3, 0.45, 0.15, 0.55, 0.28],\n        [0.3, 0.7, 0.35, 0.2, 0.5, 0.25],\n    ])\n    cov3_base = np.array([\n        [0.03, 0.028, 0, 0, 0, 0],\n        [0.028, 0.03, 0, 0, 0, 0],\n        [0, 0, 0.02, 0.015, 0, 0],\n        [0, 0, 0.015, 0.02, 0, 0],\n        [0, 0, 0, 0, 0.025, 0],\n        [0, 0, 0, 0, 0, 0.025],\n    ])\n    covs3 = np.array([cov3_base] * 3)\n\n    # Test case 4: Counterexample for greedy suboptimality\n    p4, m4, K4, eps4 = 3, 2, 2, 0.0\n    means4 = np.array([\n        [0.5, 0.5, 0.5],\n        [0.5, 0.5, 0.0],\n    ])\n    covs4 = np.array([\n        [[0.01, 0.0099, 0], [0.0099, 0.01, 0], [0, 0, 0.09]],\n        [[0.01, 0, 0], [0, 0.01, 0], [0, 0, 0.09]],\n    ])\n\n    test_cases = [\n        (p1, m1, K1, eps1, means1, covs1),\n        (p2, m2, K2, eps2, means2, covs2),\n        (p3, m3, K3, eps3, means3, covs3),\n        (p4, m4, K4, eps4, means4, covs4),\n    ]\n\n    results = []\n    for p, m, K, epsilon, means, covs in test_cases:\n        result = process_case(p, m, K, epsilon, means, covs)\n        results.append(result)\n    \n    formatted_strings = []\n    for res in results:\n        # Format: [[indices],j_greedy,j_optimal,is_optimal]\n        # np.isclose returns a numpy.bool_, str() converts to lowercase 'true'/'false'\n        # To get capitalized 'True' or 'False' we ensure it's a standard python bool.\n        is_optimal_bool = bool(res[3])\n        s = f\"[{str(res[0]).replace(' ', '')},{res[1]},{res[2]},{str(is_optimal_bool)}]\"\n        formatted_strings.append(s)\n    \n    print(f\"[{','.join(formatted_strings)}]\")\n\ndef calculate_j_agg(subset, classes, num_pairs):\n    \"\"\"\n    Calculates the aggregated Jeffries-Matusita distance for a given subset of bands.\n    \"\"\"\n    if not subset:\n        return 0.0\n    \n    total_j_dist = 0.0\n    subset_indices = list(subset)\n\n    for i in range(len(classes)):\n        for j in range(i + 1, len(classes)):\n            mu_i = classes[i]['mean'][subset_indices]\n            mu_j = classes[j]['mean'][subset_indices]\n            cov_i = classes[i]['cov'][np.ix_(subset_indices, subset_indices)]\n            cov_j = classes[j]['cov'][np.ix_(subset_indices, subset_indices)]\n\n            delta_mu = mu_i - mu_j\n            cov_avg = (cov_i + cov_j) / 2.0\n\n            # Bhattacharyya distance calculation\n            try:\n                # Term 1: Mahalanobis distance component\n                inv_cov_avg = np.linalg.inv(cov_avg)\n                term1 = 0.125 * delta_mu.T @ inv_cov_avg @ delta_mu\n\n                # Term 2: Covariance difference component (using slogdet for numerical stability)\n                _, logdet_i = np.linalg.slogdet(cov_i)\n                _, logdet_j = np.linalg.slogdet(cov_j)\n                _, logdet_avg = np.linalg.slogdet(cov_avg)\n                term2 = 0.5 * (logdet_avg - 0.5 * (logdet_i + logdet_j))\n                \n                b_dist = term1 + term2\n            except np.linalg.LinAlgError:\n                # This should not happen with positive-definite covariance matrices\n                # and their principal submatrices.\n                b_dist = np.inf\n\n            # Jeffries-Matusita distance\n            j_dist = 2.0 * (1.0 - np.exp(-b_dist))\n            total_j_dist += j_dist\n\n    return total_j_dist / num_pairs\n\n\ndef process_case(p, m, K, epsilon, means, covs):\n    \"\"\"\n    Processes a single test case, performing greedy and exhaustive searches.\n    \"\"\"\n    classes = [{'mean': m, 'cov': c} for m, c in zip(means, covs)]\n    num_pairs = m * (m - 1) // 2\n\n    # --- Greedy Forward Selection ---\n    selected_bands = []\n    current_j_agg = 0.0\n    available_bands = list(range(p))\n\n    for _ in range(K):\n        best_band_to_add = -1\n        max_gain = -1.0\n        \n        for band in available_bands:\n            candidate_subset = selected_bands + [band]\n            new_j_agg = calculate_j_agg(candidate_subset, classes, num_pairs)\n            gain = new_j_agg - current_j_agg\n            \n            if gain  max_gain:\n                max_gain = gain\n                best_band_to_add = band\n\n        if max_gain  epsilon:\n            selected_bands.append(best_band_to_add)\n            selected_bands.sort()\n            available_bands.remove(best_band_to_add)\n            current_j_agg += max_gain\n        else:\n            break  # Terminate if gain is below threshold\n    \n    j_greedy = calculate_j_agg(selected_bands, classes, num_pairs)\n\n    # --- Exhaustive Search for Optimal Subset of size K ---\n    j_optimal = 0.0\n    if K  0 and K = p:\n        for subset in combinations(range(p), K):\n            j_agg = calculate_j_agg(list(subset), classes, num_pairs)\n            if j_agg  j_optimal:\n                j_optimal = j_agg\n\n    # --- Comparison ---\n    is_optimal = np.isclose(j_greedy, j_optimal)\n\n    return [selected_bands, j_greedy, j_optimal, is_optimal]\n\nsolve()\n```"
        }
    ]
}