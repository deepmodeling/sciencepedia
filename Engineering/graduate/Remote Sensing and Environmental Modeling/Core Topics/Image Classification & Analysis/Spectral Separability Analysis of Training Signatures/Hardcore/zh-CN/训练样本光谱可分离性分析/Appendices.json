{
    "hands_on_practices": [
        {
            "introduction": "在进行任何光谱可分性分析之前，我们必须首先从训练样本中为每个地物类别建立统计模型。这个过程是后续所有分析的基石。本练习将引导你通过最大似然估计（MLE）这一基本统计原理，推导出多元正态分布的均值向量和协方差矩阵的估计量，它们共同构成了地物类别的“光谱特征”。通过这个推导，你将深入理解这些特征参数的来源，以及在样本量有限时估计中存在的偏差问题及其对分析的深远影响。",
            "id": "3853753",
            "problem": "在用于环境建模的多光谱遥感中，光谱可分离性分析依赖于从已标记像素估计的类别条件训练特征。考虑一个由 $i$ 索引的特定土地覆盖类别，您拥有 $N_i$ 个已标记的 $d$ 维光谱向量 $\\{\\mathbf{x}_n\\}_{n=1}^{N_i}$，这些向量被假定为独立同分布于一个均值为 $\\boldsymbol{\\mu}_i \\in \\mathbb{R}^d$、协方差矩阵为正定矩阵 $\\boldsymbol{\\Sigma}_i \\in \\mathbb{R}^{d \\times d}$ 的 $d$ 元正态（高斯）分布。该多元正态概率密度函数为\n$$\np(\\mathbf{x}\\mid \\boldsymbol{\\mu}_i,\\boldsymbol{\\Sigma}_i)=\\frac{1}{(2\\pi)^{d/2}|\\boldsymbol{\\Sigma}_i|^{1/2}}\\exp\\!\\left(-\\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu}_i)^{\\top}\\boldsymbol{\\Sigma}_i^{-1}(\\mathbf{x}-\\boldsymbol{\\mu}_i)\\right).\n$$\n从最大似然原理和上述密度函数出发，仅使用已标记的样本 $\\{\\mathbf{x}_n\\}_{n=1}^{N_i}$ 来推导 $\\boldsymbol{\\mu}_i$ 和 $\\boldsymbol{\\Sigma}_i$ 的最大似然估计。然后，利用多元正态分布和威沙特分布的性质，分析在有限 $N_i$ 下协方差估计的偏差，并阐述一个在正态性假设下的偏差校正估计量，评论当 $N_i$ 相对于 $d$ 很小时其行为，以及这对光谱可分离性分析的意义。\n\n您的最终答案必须是 $\\boldsymbol{\\mu}_i$ 和 $\\boldsymbol{\\Sigma}_i$ 最大似然估计的闭式解析表达式对，按此顺序以单个行向量的两个条目形式呈现。请勿包含任何解释性文本，且最终答案中不要包含等号。无需四舍五入。",
            "solution": "该问题具有科学依据，提法明確，并包含推导最大似然估计及其性质所需的所有信息。这是一个多元统计学中的标准问题，在遥感领域有直接且重要的应用。因此，该问题是有效的，下面给出完整解答。\n\n目标是给定 $N_i$ 个独立同分布 (i.i.d.) 的样本 $\\{\\mathbf{x}_n\\}_{n=1}^{N_i}$，求一个 $d$ 元正态分布的均值向量 $\\boldsymbol{\\mu}_i$ 和协方差矩阵 $\\boldsymbol{\\Sigma}_i$ 的最大似然估计 (MLE)。\n\n首先，我们构造似然函数。由于样本是独立同分布的，似然函数是每个样本概率密度函数的乘积：\n$$\nL(\\boldsymbol{\\mu}_i, \\boldsymbol{\\Sigma}_i \\mid \\{\\mathbf{x}_n\\}) = \\prod_{n=1}^{N_i} p(\\mathbf{x}_n\\mid \\boldsymbol{\\mu}_i,\\boldsymbol{\\Sigma}_i) = \\prod_{n=1}^{N_i} \\frac{1}{(2\\pi)^{d/2}|\\boldsymbol{\\Sigma}_i|^{1/2}}\\exp\\!\\left(-\\frac{1}{2}(\\mathbf{x}_n-\\boldsymbol{\\mu}_i)^{\\top}\\boldsymbol{\\Sigma}_i^{-1}(\\mathbf{x}_n-\\boldsymbol{\\mu}_i)\\right)\n$$\n最大化似然函数等价于最大化其自然对数，即对数似然函数 $\\ell = \\ln L$。取对数可将乘积简化为求和：\n$$\n\\ell(\\boldsymbol{\\mu}_i, \\boldsymbol{\\Sigma}_i) = \\ln \\left( \\prod_{n=1}^{N_i} \\left[ (2\\pi)^{-d/2}|\\boldsymbol{\\Sigma}_i|^{-1/2} \\exp\\!\\left(-\\frac{1}{2}(\\mathbf{x}_n-\\boldsymbol{\\mu}_i)^{\\top}\\boldsymbol{\\Sigma}_i^{-1}(\\mathbf{x}_n-\\boldsymbol{\\mu}_i)\\right) \\right] \\right)\n$$\n$$\n\\ell(\\boldsymbol{\\mu}_i, \\boldsymbol{\\Sigma}_i) = \\sum_{n=1}^{N_i} \\left( -\\frac{d}{2}\\ln(2\\pi) - \\frac{1}{2}\\ln|\\boldsymbol{\\Sigma}_i| - \\frac{1}{2}(\\mathbf{x}_n-\\boldsymbol{\\mu}_i)^{\\top}\\boldsymbol{\\Sigma}_i^{-1}(\\mathbf{x}_n-\\boldsymbol{\\mu}_i) \\right)\n$$\n$$\n\\ell(\\boldsymbol{\\mu}_i, \\boldsymbol{\\Sigma}_i) = -\\frac{N_i d}{2}\\ln(2\\pi) - \\frac{N_i}{2}\\ln|\\boldsymbol{\\Sigma}_i| - \\frac{1}{2}\\sum_{n=1}^{N_i}(\\mathbf{x}_n-\\boldsymbol{\\mu}_i)^{\\top}\\boldsymbol{\\Sigma}_i^{-1}(\\mathbf{x}_n-\\boldsymbol{\\mu}_i)\n$$\n为了求得最大似然估计，我们将 $\\ell$ 分别对 $\\boldsymbol{\\mu}_i$ 和 $\\boldsymbol{\\Sigma}_i$ 求偏导数，并令其为零。\n\n**均值 $\\boldsymbol{\\mu}_i$ 的最大似然估计**\n\n我们计算 $\\ell$ 关于 $\\boldsymbol{\\mu}_i$ 的梯度。只有求和项依赖于 $\\boldsymbol{\\mu}_i$。对于对称矩阵 $\\mathbf{A}$，使用标准矩阵微积分恒等式 $\\frac{\\partial}{\\partial\\mathbf{v}} (\\mathbf{x}-\\mathbf{v})^{\\top}\\mathbf{A}(\\mathbf{x}-\\mathbf{v}) = -2\\mathbf{A}(\\mathbf{x}-\\mathbf{v})$：\n$$\n\\frac{\\partial \\ell}{\\partial \\boldsymbol{\\mu}_i} = -\\frac{1}{2}\\sum_{n=1}^{N_i} \\frac{\\partial}{\\partial \\boldsymbol{\\mu}_i}\\left( (\\mathbf{x}_n-\\boldsymbol{\\mu}_i)^{\\top}\\boldsymbol{\\Sigma}_i^{-1}(\\mathbf{x}_n-\\boldsymbol{\\mu}_i) \\right) = -\\frac{1}{2}\\sum_{n=1}^{N_i} \\left( -2\\boldsymbol{\\Sigma}_i^{-1}(\\mathbf{x}_n-\\boldsymbol{\\mu}_i) \\right) = \\boldsymbol{\\Sigma}_i^{-1} \\sum_{n=1}^{N_i}(\\mathbf{x}_n-\\boldsymbol{\\mu}_i)\n$$\n将梯度设为零向量：\n$$\n\\boldsymbol{\\Sigma}_i^{-1} \\left( \\sum_{n=1}^{N_i}\\mathbf{x}_n - N_i\\boldsymbol{\\mu}_i \\right) = \\mathbf{0}\n$$\n由于 $\\boldsymbol{\\Sigma}_i$ 是正定的，其逆矩阵 $\\boldsymbol{\\Sigma}_i^{-1}$ 存在且也是正定的，因此是可逆的。我们可以两边乘以 $\\boldsymbol{\\Sigma}_i$ 得到：\n$$\n\\sum_{n=1}^{N_i}\\mathbf{x}_n - N_i\\boldsymbol{\\mu}_i = \\mathbf{0} \\implies N_i\\boldsymbol{\\mu}_i = \\sum_{n=1}^{N_i}\\mathbf{x}_n\n$$\n均值的最大似然估计，记为 $\\hat{\\boldsymbol{\\mu}}_i$，即样本均值：\n$$\n\\hat{\\boldsymbol{\\mu}}_i = \\frac{1}{N_i}\\sum_{n=1}^{N_i}\\mathbf{x}_n\n$$\n\n**协方差矩阵 $\\boldsymbol{\\Sigma}_i$ 的最大似然估计**\n\n接下来，我们将 $\\ell$ 对 $\\boldsymbol{\\Sigma}_i$ 求导。更方便的做法是对逆协方差矩阵（或精度矩阵）$\\mathbf{\\Lambda}_i = \\boldsymbol{\\Sigma}_i^{-1}$ 求导。我们使用 $\\ln|\\boldsymbol{\\Sigma}_i| = -\\ln|\\mathbf{\\Lambda}_i|$ 和迹的循环性质 $(\\mathbf{v}^\\top\\mathbf{A}\\mathbf{v} = \\text{tr}(\\mathbf{A}\\mathbf{v}\\mathbf{v}^\\top))$ 重写对数似然函数：\n$$\n\\ell(\\hat{\\boldsymbol{\\mu}}_i, \\mathbf{\\Lambda}_i) = C + \\frac{N_i}{2}\\ln|\\mathbf{\\Lambda}_i| - \\frac{1}{2}\\sum_{n=1}^{N_i}(\\mathbf{x}_n-\\hat{\\boldsymbol{\\mu}}_i)^{\\top}\\mathbf{\\Lambda}_i(\\mathbf{x}_n-\\hat{\\boldsymbol{\\mu}}_i)\n$$\n$$\n\\ell(\\hat{\\boldsymbol{\\mu}}_i, \\mathbf{\\Lambda}_i) = C + \\frac{N_i}{2}\\ln|\\mathbf{\\Lambda}_i| - \\frac{1}{2}\\text{tr}\\left( \\mathbf{\\Lambda}_i \\sum_{n=1}^{N_i}(\\mathbf{x}_n-\\hat{\\boldsymbol{\\mu}}_i)(\\mathbf{x}_n-\\hat{\\boldsymbol{\\mu}}_i)^{\\top} \\right)\n$$\n我们定义散布矩阵 $\\mathbf{S} = \\sum_{n=1}^{N_i}(\\mathbf{x}_n-\\hat{\\boldsymbol{\\mu}}_i)(\\mathbf{x}_n-\\hat{\\boldsymbol{\\mu}}_i)^{\\top}$。\n使用恒等式 $\\frac{\\partial \\ln|\\mathbf{A}|}{\\partial \\mathbf{A}} = (\\mathbf{A}^{-1})^\\top$ 和 $\\frac{\\partial \\text{tr}(\\mathbf{A}\\mathbf{B})}{\\partial \\mathbf{A}} = \\mathbf{B}^\\top$：\n$$\n\\frac{\\partial \\ell}{\\partial \\mathbf{\\Lambda}_i} = \\frac{N_i}{2}(\\mathbf{\\Lambda}_i^{-1})^\\top - \\frac{1}{2}\\mathbf{S}^\\top\n$$\n由于 $\\mathbf{\\Lambda}_i$ 和 $\\mathbf{S}$ 都是对称的，我们有：\n$$\n\\frac{\\partial \\ell}{\\partial \\mathbf{\\Lambda}_i} = \\frac{N_i}{2}\\mathbf{\\Lambda}_i^{-1} - \\frac{1}{2}\\mathbf{S}\n$$\n设为零矩阵并回顾 $\\mathbf{\\Lambda}_i^{-1} = \\boldsymbol{\\Sigma}_i$：\n$$\n\\frac{N_i}{2}\\boldsymbol{\\Sigma}_i - \\frac{1}{2}\\mathbf{S} = \\mathbf{0} \\implies \\boldsymbol{\\Sigma}_i = \\frac{1}{N_i}\\mathbf{S}\n$$\n协方差矩阵的最大似然估计，记为 $\\hat{\\boldsymbol{\\Sigma}}_i$，是：\n$$\n\\hat{\\boldsymbol{\\Sigma}}_i = \\frac{1}{N_i}\\sum_{n=1}^{N_i}(\\mathbf{x}_n-\\hat{\\boldsymbol{\\mu}}_i)(\\mathbf{x}_n-\\hat{\\boldsymbol{\\mu}}_i)^{\\top}\n$$\n\n**协方差估计量的偏差分析**\n\n为了分析 $\\hat{\\boldsymbol{\\Sigma}}_i$ 的偏差，我们计算其期望值。设真实（但未知）的参数为 $\\boldsymbol{\\mu}_i$ 和 $\\boldsymbol{\\Sigma}_i$。\n$$\nE[\\hat{\\boldsymbol{\\Sigma}}_i] = E\\left[\\frac{1}{N_i}\\sum_{n=1}^{N_i}(\\mathbf{x}_n-\\hat{\\boldsymbol{\\mu}}_i)(\\mathbf{x}_n-\\hat{\\boldsymbol{\\mu}}_i)^{\\top}\\right]\n$$\n我们代入 $\\mathbf{x}_n-\\hat{\\boldsymbol{\\mu}}_i = (\\mathbf{x}_n-\\boldsymbol{\\mu}_i) - (\\hat{\\boldsymbol{\\mu}}_i-\\boldsymbol{\\mu}_i)$：\n$$\n\\sum_{n=1}^{N_i}(\\mathbf{x}_n-\\hat{\\boldsymbol{\\mu}}_i)(\\mathbf{x}_n-\\hat{\\boldsymbol{\\mu}}_i)^{\\top} = \\sum_{n=1}^{N_i} \\left[ (\\mathbf{x}_n-\\boldsymbol{\\mu}_i)(\\mathbf{x}_n-\\boldsymbol{\\mu}_i)^\\top - (\\mathbf{x}_n-\\boldsymbol{\\mu}_i)(\\hat{\\boldsymbol{\\mu}}_i-\\boldsymbol{\\mu}_i)^\\top - (\\hat{\\boldsymbol{\\mu}}_i-\\boldsymbol{\\mu}_i)(\\mathbf{x}_n-\\boldsymbol{\\mu}_i)^\\top + (\\hat{\\boldsymbol{\\mu}}_i-\\boldsymbol{\\mu}_i)(\\hat{\\boldsymbol{\\mu}}_i-\\boldsymbol{\\mu}_i)^\\top \\right]\n$$\n对各项求和，并使用 $\\sum_{n=1}^{N_i}(\\mathbf{x}_n-\\boldsymbol{\\mu}_i) = N_i(\\hat{\\boldsymbol{\\mu}}_i-\\boldsymbol{\\mu}_i)$：\n$$\n= \\sum_{n=1}^{N_i}(\\mathbf{x}_n-\\boldsymbol{\\mu}_i)(\\mathbf{x}_n-\\boldsymbol{\\mu}_i)^\\top - N_i(\\hat{\\boldsymbol{\\mu}}_i-\\boldsymbol{\\mu}_i)(\\hat{\\boldsymbol{\\mu}}_i-\\boldsymbol{\\mu}_i)^\\top - N_i(\\hat{\\boldsymbol{\\mu}}_i-\\boldsymbol{\\mu}_i)(\\hat{\\boldsymbol{\\mu}}_i-\\boldsymbol{\\mu}_i)^\\top + N_i(\\hat{\\boldsymbol{\\mu}}_i-\\boldsymbol{\\mu}_i)(\\hat{\\boldsymbol{\\mu}}_i-\\boldsymbol{\\mu}_i)^\\top\n$$\n$$\n= \\sum_{n=1}^{N_i}(\\mathbf{x}_n-\\boldsymbol{\\mu}_i)(\\mathbf{x}_n-\\boldsymbol{\\mu}_i)^\\top - N_i(\\hat{\\boldsymbol{\\mu}}_i-\\boldsymbol{\\mu}_i)(\\hat{\\boldsymbol{\\mu}}_i-\\boldsymbol{\\mu}_i)^\\top\n$$\n现在我们取期望。第一项的期望是 $\\sum_{n=1}^{N_i} E[(\\mathbf{x}_n-\\boldsymbol{\\mu}_i)(\\mathbf{x}_n-\\boldsymbol{\\mu}_i)^\\top] = \\sum_{n=1}^{N_i} \\boldsymbol{\\Sigma}_i = N_i \\boldsymbol{\\Sigma}_i$。\n第二项的期望涉及样本均值的协方差： $E[(\\hat{\\boldsymbol{\\mu}}_i-\\boldsymbol{\\mu}_i)(\\hat{\\boldsymbol{\\mu}}_i-\\boldsymbol{\\mu}_i)^\\top] = \\text{Cov}(\\hat{\\boldsymbol{\\mu}}_i) = \\text{Cov}(\\frac{1}{N_i}\\sum \\mathbf{x}_n) = \\frac{1}{N_i^2}\\sum\\text{Cov}(\\mathbf{x}_n) = \\frac{N_i}{N_i^2}\\boldsymbol{\\Sigma}_i = \\frac{1}{N_i}\\boldsymbol{\\Sigma}_i$。\n所以，$E[N_i(\\hat{\\boldsymbol{\\mu}}_i-\\boldsymbol{\\mu}_i)(\\hat{\\boldsymbol{\\mu}}_i-\\boldsymbol{\\mu}_i)^\\top] = N_i (\\frac{1}{N_i}\\boldsymbol{\\Sigma}_i) = \\boldsymbol{\\Sigma}_i$。\n将所有项合并：\n$$\nE[N_i \\hat{\\boldsymbol{\\Sigma}}_i] = N_i \\boldsymbol{\\Sigma}_i - \\boldsymbol{\\Sigma}_i = (N_i-1)\\boldsymbol{\\Sigma}_i\n$$\n$$\nE[\\hat{\\boldsymbol{\\Sigma}}_i] = \\frac{N_i-1}{N_i}\\boldsymbol{\\Sigma}_i\n$$\n这表明协方差的最大似然估计是有偏的。偏差为 $E[\\hat{\\boldsymbol{\\Sigma}}_i] - \\boldsymbol{\\Sigma}_i = -\\frac{1}{N_i}\\boldsymbol{\\Sigma}_i$。该估计量系统性地低估了真实的协方差。\n\n**偏差校正估计量及其影响**\n\n根据偏差计算，我们可以通过将最大似然估计乘以因子 $\\frac{N_i}{N_i-1}$ 来构造一个无偏估计量。这就得到了我们熟悉的样本协方差矩阵，我们称之为 $\\mathbf{S}_i$：\n$$\n\\mathbf{S}_i = \\frac{N_i}{N_i-1}\\hat{\\boldsymbol{\\Sigma}}_i = \\frac{1}{N_i-1}\\sum_{n=1}^{N_i}(\\mathbf{x}_n-\\hat{\\boldsymbol{\\mu}}_i)(\\mathbf{x}_n-\\hat{\\boldsymbol{\\mu}}_i)^{\\top}\n$$\n这个估计量是无偏的，即 $E[\\mathbf{S}_i] = \\boldsymbol{\\Sigma}_i$。\n\n当样本数量 $N_i$ 相对于维度 $d$ 很小时，我们会遇到一个关键问题。散布矩阵 $\\mathbf{S} = \\sum_{n=1}^{N_i}(\\mathbf{x}_n-\\hat{\\boldsymbol{\\mu}}_i)(\\mathbf{x}_n-\\hat{\\boldsymbol{\\mu}}_i)^{\\top}$ 是 $N_i$ 个矩阵的和，每个矩阵的秩最多为 1。此外，向量集 $\\{\\mathbf{x}_n-\\hat{\\boldsymbol{\\mu}}_i\\}_{n=1}^{N_i}$ 是线性相关的，因为它们的和为零向量，因此它们张成一个维度最多为 $N_i-1$ 的子空间。因此，散布矩阵的秩（以及 $\\hat{\\boldsymbol{\\Sigma}}_i$ 和 $\\mathbf{S}_i$ 的秩）最多为 $\\min(d, N_i-1)$。\n如果 $N_i-1  d$（或更一般地，$N_i \\le d$），估计的协方差矩阵 $\\mathbf{S}_i$ 将是奇异的（秩亏的）。\n\n这对光谱可分离性分析有严重影响：\n1.  **可逆性失效**：许多分类器（如二次判别分析）和可分离性度量（如马氏距离、巴氏距离）需要计算协方差矩阵的逆 $\\boldsymbol{\\Sigma}_i^{-1}$。一个奇异的估计 $\\mathbf{S}_i$ 没有逆矩阵，导致这些方法完全失效。\n2.  **退化模型**：奇异的协方差矩阵意味着估计的高斯分布是退化的，其概率质量集中在 $\\mathbb{R}^d$ 的一个低维子空间上。这是数据不足造成的假象，而非底层土地覆盖类别的真实属性。\n3.  **高方差**：即使当 $N_i$ 略大于 $d$ 时，估计值 $\\mathbf{S}_i$ 通常也是数值病态的（接近奇异），其元素具有非常高的方差。这使得估计的类别特征不可靠，并导致分类器泛化性能不佳。\n\n这是“维度灾难”在高维、小样本场景下的一种表现，也是遥感中常见的问题。为了缓解这个问题，必须采用正则化（如收缩）、降维或使用更简单的协方差模型（如对角或池化协方差）等技术。",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{1}{N_i} \\sum_{n=1}^{N_i} \\mathbf{x}_n,  \\frac{1}{N_i} \\sum_{n=1}^{N_i} \\left(\\mathbf{x}_n - \\frac{1}{N_i}\\sum_{m=1}^{N_i} \\mathbf{x}_m\\right)\\left(\\mathbf{x}_n - \\frac{1}{N_i}\\sum_{m=1}^{N_i} \\mathbf{x}_m\\right)^{\\top} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "一旦我们为地物类别建立了光谱特征模型，下一步就是量化它们之间的可分性。巴氏距离（Bhattacharyya Distance）是遥感领域中衡量类别间重叠程度最常用的指标之一。本练习要求你推导在协方差相等这一常见简化假设下巴氏距离的闭合形式解，并分析协方差矩阵的各向异性如何影响不同光谱方向上的可分性。通过结合理论推导和数值计算，你将掌握可分性度量的核心，并直观地理解光谱空间中类别分布的几何关系。",
            "id": "3853814",
            "problem": "在监督式土地覆盖制图的光谱可分离性分析中，两个类别的训练样本特征通常被建模为在 $d$ 个光谱波段上的多元高斯分布。考虑两个土地覆盖类别，季节性淹没草地 (SFG) 和灌溉稻田 (IRP)，每个类别都由一个 $d$ 维多元正态密度 $p_{i}(\\mathbf{x})$ 表示，其均值为 $\\boldsymbol{\\mu}_{i} \\in \\mathbb{R}^{d}$，协方差为 $\\boldsymbol{\\Sigma}_{i} \\in \\mathbb{R}^{d \\times d}$，其中 $i \\in \\{1,2\\}$。一个广泛使用的可分离性度量是巴氏距离 (Bhattacharyya distance, BD)，它由巴氏系数 (Bhattacharyya coefficient, BC) 定义为 $B = -\\ln(\\mathrm{BC})$，其中巴氏系数由重叠积分 $\\mathrm{BC} = \\int_{\\mathbb{R}^{d}} \\sqrt{p_{1}(\\mathbf{x}) p_{2}(\\mathbf{x})} \\, d\\mathbf{x}$ 给出。\n\n从这些定义和多元正态密度的标准形式出发，在等协方差假设 $\\boldsymbol{\\Sigma}_{1} = \\boldsymbol{\\Sigma}_{2} = \\boldsymbol{\\Sigma}$下，推导 $B$ 的闭式表达式。然后，利用 $\\boldsymbol{\\Sigma}$ 的特征分解，分析 $\\boldsymbol{\\Sigma}$ 中的各向异性如何扭曲不同光谱方向上的可分离性。\n\n最后，对以下三波段反射率特征进行 $B$ 的数值计算（波段顺序为绿光、红光和近红外）：$\\boldsymbol{\\mu}_{1} = \\begin{pmatrix} 0.08 \\\\ 0.06 \\\\ 0.32 \\end{pmatrix}$，$\\boldsymbol{\\mu}_{2} = \\begin{pmatrix} 0.10 \\\\ 0.05 \\\\ 0.36 \\end{pmatrix}$，以及 $\\boldsymbol{\\Sigma} = \\operatorname{diag}\\!\\big(0.01, 0.02, 0.09\\big)$。将 $B$ 的最终数值答案表示为一个无量纲实数，并四舍五入到四位有效数字。",
            "solution": "所述问题需经过验证过程，以确保其科学性和逻辑合理性。\n\n### 步骤1：提取已知条件\n-   两个土地覆盖类别被建模为 $d$ 维多元正态密度 $p_{i}(\\mathbf{x})$，其中 $i \\in \\{1,2\\}$。\n-   类别 $i$ 的概率密度函数为 $p_{i}(\\mathbf{x}) \\sim \\mathcal{N}(\\boldsymbol{\\mu}_{i}, \\boldsymbol{\\Sigma}_{i})$，其中 $\\boldsymbol{\\mu}_{i} \\in \\mathbb{R}^{d}$ 是均值向量，$\\boldsymbol{\\Sigma}_{i} \\in \\mathbb{R}^{d \\times d}$ 是协方差矩阵。\n-   巴氏距离 $B$ 定义为 $B = -\\ln(\\mathrm{BC})$。\n-   巴氏系数 $\\mathrm{BC}$ 由积分 $\\mathrm{BC} = \\int_{\\mathbb{R}^{d}} \\sqrt{p_{1}(\\mathbf{x}) p_{2}(\\mathbf{x})} \\, d\\mathbf{x}$ 定义。\n-   作出等协方差假设：$\\boldsymbol{\\Sigma}_{1} = \\boldsymbol{\\Sigma}_{2} = \\boldsymbol{\\Sigma}$。\n-   任务1：在等协方差假设下，推导 $B$ 的闭式表达式。\n-   任务2：使用 $\\boldsymbol{\\Sigma}$ 的特征分解，分析 $\\boldsymbol{\\Sigma}$ 中的各向异性如何扭曲可分离性。\n-   任务3：对特定情况进行 $B$ 的数值计算：\n    -   波段数 $d=3$。\n    -   均值向量：$\\boldsymbol{\\mu}_{1} = \\begin{pmatrix} 0.08 \\\\ 0.06 \\\\ 0.32 \\end{pmatrix}$ 和 $\\boldsymbol{\\mu}_{2} = \\begin{pmatrix} 0.10 \\\\ 0.05 \\\\ 0.36 \\end{pmatrix}$。\n    -   协方差矩阵：$\\boldsymbol{\\Sigma} = \\operatorname{diag}\\!\\big(0.01, 0.02, 0.09\\big)$。\n    -   $B$ 的最终数值答案应为一个无量纲实数，并四舍五入到四位有效数字。\n\n### 步骤2：使用提取的已知条件进行验证\n根据既定标准对问题进行评估：\n-   **科学依据：** 该问题牢固地植根于统计模式识别及其在遥感中的应用，这是环境科学和地理学中的标准实践。使用多元正态分布对光谱特征进行建模，以及使用巴氏距离作为可分离性度量，都是公认的方法。\n-   **适定性：** 该问题提供了推导所要求的表达式、进行分析和计算数值结果所需的所有定义、数据和约束。问题表述清晰，可以导出一个唯一的、稳定的、有意义的解。\n-   **客观性：** 问题使用精确、形式化的数学和科学语言陈述，没有任何主观性或偏见。\n\n### 步骤3：结论与行动\n该问题被判定为**有效**。它具有科学合理性、适定性和客观性。不包含逻辑矛盾、信息缺失或不科学的前提。因此，可以进行求解过程。\n\n### 巴氏距离的推导\n\n类别 $i$ 的一个 $d$ 维多元正态分布的概率密度函数由下式给出：\n$$\np_{i}(\\mathbf{x}) = \\frac{1}{(2\\pi)^{d/2} |\\boldsymbol{\\Sigma}_i|^{1/2}} \\exp\\left( -\\frac{1}{2} (\\mathbf{x} - \\boldsymbol{\\mu}_i)^T \\boldsymbol{\\Sigma}_i^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}_i) \\right)\n$$\n在协方差矩阵相等的假设下，$\\boldsymbol{\\Sigma}_1 = \\boldsymbol{\\Sigma}_2 = \\boldsymbol{\\Sigma}$，密度函数为：\n$$\np_{1}(\\mathbf{x}) = \\frac{1}{(2\\pi)^{d/2} |\\boldsymbol{\\Sigma}|^{1/2}} \\exp\\left( -\\frac{1}{2} (\\mathbf{x} - \\boldsymbol{\\mu}_1)^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}_1) \\right)\n$$\n$$\np_{2}(\\mathbf{x}) = \\frac{1}{(2\\pi)^{d/2} |\\boldsymbol{\\Sigma}|^{1/2}} \\exp\\left( -\\frac{1}{2} (\\mathbf{x} - \\boldsymbol{\\mu}_2)^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}_2) \\right)\n$$\n巴氏系数 $\\mathrm{BC}$ 需要计算几何平均值 $\\sqrt{p_{1}(\\mathbf{x}) p_{2}(\\mathbf{x})}$：\n$$\n\\sqrt{p_{1}(\\mathbf{x}) p_{2}(\\mathbf{x})} = \\frac{1}{(2\\pi)^{d/2} |\\boldsymbol{\\Sigma}|^{1/2}} \\exp\\left( -\\frac{1}{4} \\left[ (\\mathbf{x} - \\boldsymbol{\\mu}_1)^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}_1) + (\\mathbf{x} - \\boldsymbol{\\mu}_2)^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}_2) \\right] \\right)\n$$\n我们来分析指数中的项。令 $Q(\\mathbf{x}) = (\\mathbf{x} - \\boldsymbol{\\mu}_1)^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}_1) + (\\mathbf{x} - \\boldsymbol{\\mu}_2)^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}_2)$。展开并合并关于 $\\mathbf{x}$ 的项，得到：\n$$\nQ(\\mathbf{x}) = 2\\mathbf{x}^T \\boldsymbol{\\Sigma}^{-1} \\mathbf{x} - 2\\mathbf{x}^T \\boldsymbol{\\Sigma}^{-1} (\\boldsymbol{\\mu}_1 + \\boldsymbol{\\mu}_2) + \\boldsymbol{\\mu}_1^T \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}_1 + \\boldsymbol{\\mu}_2^T \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}_2\n$$\n我们对包含 $\\mathbf{x}$ 的项进行配方。令 $\\boldsymbol{\\mu}' = \\frac{\\boldsymbol{\\mu}_1 + \\boldsymbol{\\mu}_2}{2}$。该二次型可以表示为：\n$$\n2(\\mathbf{x} - \\boldsymbol{\\mu}')^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}') = 2\\mathbf{x}^T \\boldsymbol{\\Sigma}^{-1} \\mathbf{x} - 2\\mathbf{x}^T \\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{\\mu}_1+\\boldsymbol{\\mu}_2) + 2(\\boldsymbol{\\mu}')^T \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}'\n$$\n将此代入 $Q(\\mathbf{x})$ 的表达式中：\n$$\nQ(\\mathbf{x}) = 2(\\mathbf{x} - \\boldsymbol{\\mu}')^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}') + \\boldsymbol{\\mu}_1^T \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}_1 + \\boldsymbol{\\mu}_2^T \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}_2 - 2(\\boldsymbol{\\mu}')^T \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}'\n$$\n常数项简化为：\n$$\n\\boldsymbol{\\mu}_1^T \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}_1 + \\boldsymbol{\\mu}_2^T \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}_2 - 2\\left(\\frac{\\boldsymbol{\\mu}_1 + \\boldsymbol{\\mu}_2}{2}\\right)^T \\boldsymbol{\\Sigma}^{-1} \\left(\\frac{\\boldsymbol{\\mu}_1 + \\boldsymbol{\\mu}_2}{2}\\right)\n= \\boldsymbol{\\mu}_1^T \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}_1 + \\boldsymbol{\\mu}_2^T \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}_2 - \\frac{1}{2}(\\boldsymbol{\\mu}_1^T \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}_1 + 2\\boldsymbol{\\mu}_1^T \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}_2 + \\boldsymbol{\\mu}_2^T \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}_2)\n$$\n$$\n= \\frac{1}{2}(\\boldsymbol{\\mu}_1^T \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}_1 - 2\\boldsymbol{\\mu}_1^T \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}_2 + \\boldsymbol{\\mu}_2^T \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}_2) = \\frac{1}{2}(\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_2)^T \\boldsymbol{\\Sigma}^{-1} (\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_2)\n$$\n几何平均值中的指数变为：\n$$\n-\\frac{1}{4}Q(\\mathbf{x}) = -\\frac{1}{2}(\\mathbf{x} - \\boldsymbol{\\mu}')^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}') - \\frac{1}{8}(\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_2)^T \\boldsymbol{\\Sigma}^{-1} (\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_2)\n$$\n因此，巴氏系数的积分为：\n$$\n\\mathrm{BC} = \\int_{\\mathbb{R}^{d}} \\frac{1}{(2\\pi)^{d/2} |\\boldsymbol{\\Sigma}|^{1/2}} \\exp\\left(-\\frac{1}{8}(\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_2)^T \\boldsymbol{\\Sigma}^{-1} (\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_2)\\right) \\exp\\left(-\\frac{1}{2}(\\mathbf{x} - \\boldsymbol{\\mu}')^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}')\\right) d\\mathbf{x}\n$$\n常数项可以从积分中提出：\n$$\n\\mathrm{BC} = \\exp\\left(-\\frac{1}{8}(\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_2)^T \\boldsymbol{\\Sigma}^{-1} (\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_2)\\right) \\int_{\\mathbb{R}^{d}} \\frac{1}{(2\\pi)^{d/2} |\\boldsymbol{\\Sigma}|^{1/2}} \\exp\\left(-\\frac{1}{2}(\\mathbf{x} - \\boldsymbol{\\mu}')^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}')\\right) d\\mathbf{x}\n$$\n剩余的积分是一个多元正态概率密度 $\\mathcal{N}(\\boldsymbol{\\mu}', \\boldsymbol{\\Sigma})$ 在其整个定义域 $\\mathbb{R}^d$ 上的积分，其值等于 $1$。因此：\n$$\n\\mathrm{BC} = \\exp\\left(-\\frac{1}{8}(\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_2)^T \\boldsymbol{\\Sigma}^{-1} (\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_2)\\right)\n$$\n巴氏距离 $B = -\\ln(\\mathrm{BC})$ 则为：\n$$\nB = \\frac{1}{8}(\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_2)^T \\boldsymbol{\\Sigma}^{-1} (\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_2)\n$$\n这个表达式是两个均值向量 $\\boldsymbol{\\mu}_1$ 和 $\\boldsymbol{\\mu}_2$ 之间马氏距离(Mahalanobis distance)平方的 $\\frac{1}{8}$。\n\n### 各向异性分析\n为分析 $\\boldsymbol{\\Sigma}$ 中的各向异性如何影响可分离性，我们对对称正定协方差矩阵 $\\boldsymbol{\\Sigma}$ 进行特征分解：\n$$\n\\boldsymbol{\\Sigma} = \\mathbf{V} \\mathbf{\\Lambda} \\mathbf{V}^T\n$$\n其中 $\\mathbf{V}$ 是一个正交矩阵，其列是 $\\boldsymbol{\\Sigma}$ 的特征向量 $\\mathbf{v}_j$，$\\mathbf{\\Lambda}$ 是由相应正特征值 $\\lambda_j$ 组成的对角矩阵。其逆矩阵为 $\\boldsymbol{\\Sigma}^{-1} = \\mathbf{V} \\mathbf{\\Lambda}^{-1} \\mathbf{V}^T$。令 $\\Delta\\boldsymbol{\\mu} = \\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_2$。将分解代入 $B$ 的表达式中：\n$$\nB = \\frac{1}{8}(\\Delta\\boldsymbol{\\mu})^T (\\mathbf{V} \\mathbf{\\Lambda}^{-1} \\mathbf{V}^T) (\\Delta\\boldsymbol{\\mu}) = \\frac{1}{8} (\\mathbf{V}^T \\Delta\\boldsymbol{\\mu})^T \\mathbf{\\Lambda}^{-1} (\\mathbf{V}^T \\Delta\\boldsymbol{\\mu})\n$$\n令 $\\mathbf{c} = \\mathbf{V}^T \\Delta\\boldsymbol{\\mu}$。$\\mathbf{c}$ 的分量是 $c_j = \\mathbf{v}_j^T \\Delta\\boldsymbol{\\mu}$，表示均值差向量 $\\Delta\\boldsymbol{\\mu}$ 在每个特征向量 $\\mathbf{v}_j$ 上的投影。$B$ 的表达式变为：\n$$\nB = \\frac{1}{8} \\mathbf{c}^T \\mathbf{\\Lambda}^{-1} \\mathbf{c} = \\frac{1}{8} \\sum_{j=1}^{d} \\frac{c_j^2}{\\lambda_j}\n$$\n这种形式揭示了各向异性（非均匀的特征值）如何扭曲可分离性：\n-   总可分离性 $B$ 是均值差向量的平方分量的加权和，这些分量是沿协方差结构的主轴（特征向量）定义的。\n-   每个分量的权重是相应特征值的倒数 $1/\\lambda_j$。特征值 $\\lambda_j$ 表示数据沿轴 $\\mathbf{v}_j$ 的方差。\n-   一个小的特征值 $\\lambda_j$ 意味着沿方向 $\\mathbf{v}_j$ 的方差很小（即数据簇是“瘦”的）。其倒数 $1/\\lambda_j$ 很大。因此，沿此方向的任何均值分离（$c_j \\neq 0$）都会对总可分离性 $B$ 产生很大贡献。\n-   相反，一个大的特征值 $\\lambda_k$ 意味着沿方向 $\\mathbf{v}_k$ 的方差很大（数据簇是“宽”的）。其倒数 $1/\\lambda_k$ 很小。沿此高方差方向的均值分离对 $B$ 的贡献很小。\n-   因此，$\\boldsymbol{\\Sigma}$ 中的各向异性使可分离性高度依赖于方向。方差最小的光谱方向对于区分这些类别至关重要。\n\n### 数值计算\n\n给定 $d=3$，$\\boldsymbol{\\mu}_{1} = \\begin{pmatrix} 0.08 \\\\ 0.06 \\\\ 0.32 \\end{pmatrix}$，$\\boldsymbol{\\mu}_{2} = \\begin{pmatrix} 0.10 \\\\ 0.05 \\\\ 0.36 \\end{pmatrix}$，以及 $\\boldsymbol{\\Sigma} = \\operatorname{diag}\\!\\big(0.01, 0.02, 0.09\\big)$。\n\n首先，我们计算均值向量之差 $\\Delta\\boldsymbol{\\mu}$：\n$$\n\\Delta\\boldsymbol{\\mu} = \\boldsymbol{\\mu}_{1} - \\boldsymbol{\\mu}_{2} = \\begin{pmatrix} 0.08 - 0.10 \\\\ 0.06 - 0.05 \\\\ 0.32 - 0.36 \\end{pmatrix} = \\begin{pmatrix} -0.02 \\\\ 0.01 \\\\ -0.04 \\end{pmatrix}\n$$\n接下来，我们求协方差矩阵的逆 $\\boldsymbol{\\Sigma}^{-1}$。由于 $\\boldsymbol{\\Sigma}$ 是对角矩阵，其逆矩阵是其对角元素倒数构成的对角矩阵：\n$$\n\\boldsymbol{\\Sigma}^{-1} = \\operatorname{diag}\\!\\left(\\frac{1}{0.01}, \\frac{1}{0.02}, \\frac{1}{0.09}\\right) = \\operatorname{diag}\\!\\left(100, 50, \\frac{100}{9}\\right)\n$$\n现在，我们计算二次型 $(\\Delta\\boldsymbol{\\mu})^T \\boldsymbol{\\Sigma}^{-1} (\\Delta\\boldsymbol{\\mu})$：\n$$\n(\\Delta\\boldsymbol{\\mu})^T \\boldsymbol{\\Sigma}^{-1} (\\Delta\\boldsymbol{\\mu}) = \\begin{pmatrix} -0.02  0.01  -0.04 \\end{pmatrix} \\begin{pmatrix} 100  0  0 \\\\ 0  50  0 \\\\ 0  0  \\frac{100}{9} \\end{pmatrix} \\begin{pmatrix} -0.02 \\\\ 0.01 \\\\ -0.04 \\end{pmatrix}\n$$\n$$\n= (-0.02)(100)(-0.02) + (0.01)(50)(0.01) + (-0.04)\\left(\\frac{100}{9}\\right)(-0.04)\n$$\n$$\n= \\frac{(-0.02)^2}{0.01} + \\frac{(0.01)^2}{0.02} + \\frac{(-0.04)^2}{0.09}\n$$\n$$\n= \\frac{0.0004}{0.01} + \\frac{0.0001}{0.02} + \\frac{0.0016}{0.09}\n$$\n$$\n= 0.04 + 0.005 + \\frac{16}{900} = 0.045 + \\frac{4}{225}\n$$\n数值上，$\\frac{4}{225} \\approx 0.01777...$。所以二次型是：\n$$\n0.045 + 0.01777... = 0.062777...\n$$\n最后，我们计算巴氏距离 $B$：\n$$\nB = \\frac{1}{8} \\times (0.062777...) = 0.007847222...\n$$\n四舍五入到四位有效数字，我们得到 $0.007847$。用标准科学记数法表示，即为 $7.847 \\times 10^{-3}$。",
            "answer": "$$\\boxed{7.847 \\times 10^{-3}}$$"
        },
        {
            "introduction": "在处理高维遥感数据（如高光谱图像）时，并非所有波段都有助于区分地物类别，有些甚至可能因引入噪声而降低分类精度。因此，波段选择成为优化分类性能的关键步骤。本练习将带你进入一个实际应用场景：利用前面学到的可分性度量（杰弗里斯-马图西塔距离）作为标准，设计并实现一个贪心算法来自动选择信息最丰富的波段子集。通过将贪心算法的结果与全局最优解进行比较，你将体验到在复杂优化问题中，启发式算法的效率与次优性之间的权衡。",
            "id": "3853730",
            "problem": "给定一组地物覆盖类别的训练特征，每个类别由一个在离散光谱波段集上的多元高斯类条件模型表示。对于每个类别，其训练特征由一个跨光谱波段的均值向量和一个协方差矩阵完全指定。某个波段子集的聚合光谱可分性定义为，在该子集上受限的类条件分布之间的所有成对 Jeffries-Matusita (JM) 距离的均值。Jeffries-Matusita (JM) 距离通过两个多元正态类条件分布之间的 Bhattacharyya 距离来定义。您的任务是制定并实现一个贪心算法，该算法每次向一个不断增长的子集中迭代添加一个波段，以最大化聚合可分性的增量增长。当达到指定的波段预算或增量增长不严格大于指定阈值时，算法必须终止。您还必须通过穷举搜索计算出具有相同基数的全局最优子集，以便对贪心选择的潜在次优性进行实证评估。所有计算必须以纯数值形式表示。不涉及任何物理单位、角度或百分比。\n\n使用的基本假设和定义：\n- 每个类别 $c$ 都有一个训练特征，其由一个均值向量 $\\boldsymbol{\\mu}_c \\in \\mathbb{R}^p$ 和一个在 $p$ 个光谱波段上的正定协方差矩阵 $\\boldsymbol{\\Sigma}_c \\in \\mathbb{R}^{p \\times p}$ 来表征。\n- 对于任何波段子集 $S \\subseteq \\{0,1,\\ldots,p-1\\}$，类别 $c$ 在 $S$ 上的限制由子向量 $\\boldsymbol{\\mu}_{c,S}$ 和主子矩阵 $\\boldsymbol{\\Sigma}_{c,S}$ 给出。\n- 两个类别 $i$ 和 $j$ 在 $S$ 上的成对光谱可分性是 Jeffries-Matusita 距离 $J_{ij}(S)$，它通过类别 $i$ 和 $j$ 在 $S$ 上受限的多元高斯分布之间的 Bhattacharyya 距离来定义。\n- 对于 $m$ 个类别，在 $S$ 上的聚合可分性是所有 $\\binom{m}{2}$ 个无序类别对的 $J_{ij}(S)$ 的算术平均值。\n\n算法要求：\n- 实现一个贪心选择过程，该过程从空子集 $S = \\varnothing$ 开始，并迭代地添加一个来自 $\\{0,1,\\ldots,p-1\\} \\setminus S$ 的波段 $b$，以最大化增量收益 $\\Delta J = J_{\\text{agg}}(S \\cup \\{b\\}) - J_{\\text{agg}}(S)$，其中 $J_{\\text{agg}}(\\cdot)$ 表示聚合可分性。如果最大的 $\\Delta J$ 不严格大于阈值 $\\varepsilon$，则终止。否则，添加具有最大 $\\Delta J$ 的波段。当已选择的波段达到预算数量 $K$ 或阈值条件触发终止时，停止。\n- 另外，通过对所有基数为 $K$ 的子集进行穷举搜索，计算出能使 $J_{\\text{agg}}(S)$ 最大化的、恰好包含 $K$ 个波段的全局最优子集。\n\n输出规范：\n- 对于每个测试用例，生成一个列表，其中包含：按升序排列的贪心选择的波段索引列表、贪心子集的最终聚合可分性（一个实数）、恰好包含 $K$ 个波段的子集的全局最优聚合可分性，以及一个布尔值，指示贪心子集（大小不超过 $K$）是否达到了与恰好包含 $K$ 个波段的最优子集相同的聚合可分性。\n- 您的程序应生成单行输出，其中包含所有测试用例的结果，形式为一个用方括号括起来的逗号分隔列表，其中每个测试用例的结果本身也是一个列表。例如，最终输出必须类似于 $[\\,[\\cdots],\\,[\\cdots],\\,\\ldots\\,]$。\n\n测试套件：\n- 测试用例 1（正常路径，中等可分性，对角协方差）：\n    - 波段数 $p = 5$，类别数 $m = 3$，预算 $K = 3$，阈值 $\\varepsilon = 10^{-6}$。\n    - 类别均值：\n        - 类别 1：$[\\,0.1,\\,0.5,\\,0.3,\\,0.0,\\,0.2\\,]$\n        - 类别 2：$[\\,0.9,\\,0.1,\\,0.2,\\,0.1,\\,0.4\\,]$\n        - 类别 3：$[\\,0.3,\\,0.4,\\,0.9,\\,0.2,\\,0.1\\,]$\n    - 协方差矩阵（所有类别相同，对角矩阵）：\n        - 对于每个类别 $c$，$\\boldsymbol{\\Sigma}_c = \\mathrm{diag}(\\,[\\,0.02,\\,0.02,\\,0.03,\\,0.02,\\,0.03\\,]\\,)$。\n- 测试用例 2（边界条件，单波段预算）：\n    - 波段数 $p = 4$，类别数 $m = 2$，预算 $K = 1$，阈值 $\\varepsilon = 0$。\n    - 类别均值：\n        - 类别 1：$[\\,0.0,\\,0.5,\\,0.2,\\,0.1\\,]$\n        - 类别 2：$[\\,0.7,\\,0.6,\\,0.2,\\,0.1\\,]$\n    - 协方差矩阵（所有类别相同，对角矩阵）：\n        - 对于每个类别 $c$，$\\boldsymbol{\\Sigma}_c = \\mathrm{diag}(\\,[\\,0.04,\\,0.01,\\,0.02,\\,0.02\\,]\\,)$。\n- 测试用例 3（因相关性导致的冗余，多类别）：\n    - 波段数 $p = 6$，类别数 $m = 3$，预算 $K = 3$，阈值 $\\varepsilon = 10^{-6}$。\n    - 类别均值：\n        - 类别 1：$[\\,0.2,\\,0.8,\\,0.4,\\,0.1,\\,0.6,\\,0.3\\,]$\n        - 类别 2：$[\\,0.5,\\,0.3,\\,0.45,\\,0.15,\\,0.55,\\,0.28\\,]$\n        - 类别 3：$[\\,0.3,\\,0.7,\\,0.35,\\,0.2,\\,0.5,\\,0.25\\,]$\n    - 协方差矩阵（所有类别相同，相关性导致冗余）：\n        - 对于每个类别 $c$，$\\boldsymbol{\\Sigma}_c$ 是块结构的，波段 0 和 1 之间以及波段 2 和 3 之间存在强相关性：\n        $$\n        \\boldsymbol{\\Sigma}_c =\n        \\begin{bmatrix}\n        0.03  0.028  0  0  0  0 \\\\\n        0.028  0.03  0  0  0  0 \\\\\n        0  0  0.02  0.015  0  0 \\\\\n        0  0  0.015  0.02  0  0 \\\\\n        0  0  0  0  0.025  0 \\\\\n        0  0  0  0  0  0.025\n        \\end{bmatrix}.\n        $$\n- 测试用例 4（因协方差协同差异导致贪心算法次优的反例）：\n    - 波段数 $p = 3$，类别数 $m = 2$，预算 $K = 2$，阈值 $\\varepsilon = 0$。\n    - 类别均值：\n        - 类别 1：$[\\,0.5,\\,0.5,\\,0.5\\,]$\n        - 类别 2：$[\\,0.5,\\,0.5,\\,0.0\\,]$\n    - 协方差矩阵（类别间具有不同的带间相关性）：\n        $$\n        \\boldsymbol{\\Sigma}_1 =\n        \\begin{bmatrix}\n        0.01  0.0099  0 \\\\\n        0.0099  0.01  0 \\\\\n        0  0  0.09\n        \\end{bmatrix}, \\quad\n        \\boldsymbol{\\Sigma}_2 =\n        \\begin{bmatrix}\n        0.01  0  0 \\\\\n        0  0.01  0 \\\\\n        0  0  0.09\n        \\end{bmatrix}.\n        $$\n    - 这个案例的构造使得波段 0 和 1 单独的增量收益可以忽略不计，但由于不同类别间的协方差结构差异，它们共同使用时会使聚合可分性大幅增加，而波段 2 单独使用则产生中等程度的增加。贪心算法受限于步进式的增量收益，可能会首先选择波段 2，然后在预算 $K=2$ 的限制下无法实现波段 0 和 1 之间的协同效应，从而说明了其潜在的次优选择。\n\n您的程序必须按规定实现贪心过程和穷举搜索，将它们应用于上述四个测试用例，并打印一行包含四个结果的列表，每个测试用例一个结果。每个结果必须是 $[\\,\\text{selected\\_indices},\\,J_{\\text{greedy}},\\,J_{\\text{optimal}},\\,\\text{is\\_optimal}\\,]$ 形式的列表，其中 $\\text{selected\\_indices}$ 是一个整数列表，$J_{\\text{greedy}}$ 和 $J_{\\text{optimal}}$ 是实数，$\\text{is\\_optimal}$ 是一个布尔值。",
            "solution": "所呈现的问题是统计模式识别和遥感领域中一个明确定义的任务，具体涉及特征选择。它具有科学依据，在数学上是一致的，并且提供了所有必要的参数和测试数据。因此，该问题被认为是有效的，并将提供一个解决方案。\n\n问题的核心是选择光谱波段的最优子集，以最大化不同地物覆盖类别之间的可分性。每个类别都由一个多元高斯分布建模。可分性使用 Jeffries-Matusita (JM) 距离来量化，该距离源自 Bhattacharyya 距离。该任务要求实现两种不同的波段选择算法：一种是贪心前向选择启发式算法，另一种是用于寻找全局最优解的穷举搜索。\n\n首先，让我们将关键的数学概念形式化。一个类别 $c_i$ 由一个 $p$ 维多元正态分布 $\\mathcal{N}(\\boldsymbol{\\mu}_i, \\boldsymbol{\\Sigma}_i)$ 描述，其中 $\\boldsymbol{\\mu}_i \\in \\mathbb{R}^p$ 是均值向量，$\\boldsymbol{\\Sigma}_i \\in \\mathbb{R}^{p \\times p}$ 是在 $p$ 个光谱波段上的正定协方差矩阵。\n\n对于一个给定的基数为 $|S|=d$ 的波段子集 $S$，统计分布被限制在这些波段上，从而得到一个均值子向量 $\\boldsymbol{\\mu}_{i,S} \\in \\mathbb{R}^d$ 和一个主子矩阵 $\\boldsymbol{\\Sigma}_{i,S} \\in \\mathbb{R}^{d \\times d}$。两个类别 $c_i$ 和 $c_j$ 在此子集 $S$ 上的可分性通过 Bhattacharyya 距离 $B_{ij}(S)$ 计算。对于多元正态分布，其公式为：\n\n$$\nB_{ij}(S) = \\frac{1}{8} (\\boldsymbol{\\mu}_{i,S} - \\boldsymbol{\\mu}_{j,S})^T \\left( \\frac{\\boldsymbol{\\Sigma}_{i,S} + \\boldsymbol{\\Sigma}_{j,S}}{2} \\right)^{-1} (\\boldsymbol{\\mu}_{i,S} - \\boldsymbol{\\mu}_{j,S}) + \\frac{1}{2} \\ln \\left( \\frac{\\det\\left(\\frac{\\boldsymbol{\\Sigma}_{i,S} + \\boldsymbol{\\Sigma}_{j,S}}{2}\\right)}{\\sqrt{\\det(\\boldsymbol{\\Sigma}_{i,S}) \\det(\\boldsymbol{\\Sigma}_{j,S})}} \\right)\n$$\n\n第一项是 Mahalanobis 距离的一种形式，用于衡量由平均协方差加权的均值之间的分离度。第二项衡量由协方差矩阵本身的差异引起的可分性。\n\nJeffries-Matusita (JM) 距离 $J_{ij}(S)$ 是 Bhattacharyya 距离的一个有界函数，当类别变得更加分离时，它会饱和。其定义为：\n\n$$\nJ_{ij}(S) = 2(1 - e^{-B_{ij}(S)})\n$$\n\n$J_{ij}(S)$ 的值范围从 0（完全重叠）到 2（完全可分）。\n\n对于一个包含 $m$ 个类别的集合，在波段子集 $S$ 上的聚合可分性，记作 $J_{\\text{agg}}(S)$，是所有成对 JM 距离的算术平均值：\n\n$$\nJ_{\\text{agg}}(S) = \\frac{1}{\\binom{m}{2}} \\sum_{i=0}^{m-2} \\sum_{j=i+1}^{m-1} J_{ij}(S)\n$$",
            "answer": "```python\nimport numpy as np\nfrom itertools import combinations\n\ndef solve():\n    \"\"\"\n    Main function to define test cases and generate the final output.\n    \"\"\"\n    # Test case 1: Happy path, diagonal covariances\n    p1, m1, K1, eps1 = 5, 3, 3, 1e-6\n    means1 = np.array([\n        [0.1, 0.5, 0.3, 0.0, 0.2],\n        [0.9, 0.1, 0.2, 0.1, 0.4],\n        [0.3, 0.4, 0.9, 0.2, 0.1],\n    ])\n    covs1 = np.array([np.diag([0.02, 0.02, 0.03, 0.02, 0.03])] * 3)\n\n    # Test case 2: Single-band budget\n    p2, m2, K2, eps2 = 4, 2, 1, 0.0\n    means2 = np.array([\n        [0.0, 0.5, 0.2, 0.1],\n        [0.7, 0.6, 0.2, 0.1],\n    ])\n    covs2 = np.array([np.diag([0.04, 0.01, 0.02, 0.02])] * 2)\n\n    # Test case 3: Correlated bands\n    p3, m3, K3, eps3 = 6, 3, 3, 1e-6\n    means3 = np.array([\n        [0.2, 0.8, 0.4, 0.1, 0.6, 0.3],\n        [0.5, 0.3, 0.45, 0.15, 0.55, 0.28],\n        [0.3, 0.7, 0.35, 0.2, 0.5, 0.25],\n    ])\n    cov3_base = np.array([\n        [0.03, 0.028, 0, 0, 0, 0],\n        [0.028, 0.03, 0, 0, 0, 0],\n        [0, 0, 0.02, 0.015, 0, 0],\n        [0, 0, 0.015, 0.02, 0, 0],\n        [0, 0, 0, 0, 0.025, 0],\n        [0, 0, 0, 0, 0, 0.025],\n    ])\n    covs3 = np.array([cov3_base] * 3)\n\n    # Test case 4: Counterexample for greedy suboptimality\n    p4, m4, K4, eps4 = 3, 2, 2, 0.0\n    means4 = np.array([\n        [0.5, 0.5, 0.5],\n        [0.5, 0.5, 0.0],\n    ])\n    covs4 = np.array([\n        [[0.01, 0.0099, 0], [0.0099, 0.01, 0], [0, 0, 0.09]],\n        [[0.01, 0, 0], [0, 0.01, 0], [0, 0, 0.09]],\n    ])\n\n    test_cases = [\n        (p1, m1, K1, eps1, means1, covs1),\n        (p2, m2, K2, eps2, means2, covs2),\n        (p3, m3, K3, eps3, means3, covs3),\n        (p4, m4, K4, eps4, means4, covs4),\n    ]\n\n    results = []\n    for p, m, K, epsilon, means, covs in test_cases:\n        result = process_case(p, m, K, epsilon, means, covs)\n        results.append(result)\n    \n    formatted_strings = []\n    for res in results:\n        # Format: [[indices],j_greedy,j_optimal,is_optimal]\n        # np.isclose returns a numpy.bool_, str() converts to lowercase 'true'/'false'\n        # To get capitalized 'True' or 'False' we ensure it's a standard python bool.\n        is_optimal_bool = bool(res[3])\n        s = f\"[{str(res[0]).replace(' ', '')},{res[1]},{res[2]},{str(is_optimal_bool)}]\"\n        formatted_strings.append(s)\n    \n    print(f\"[{','.join(formatted_strings)}]\")\n\ndef calculate_j_agg(subset, classes, num_pairs):\n    \"\"\"\n    Calculates the aggregated Jeffries-Matusita distance for a given subset of bands.\n    \"\"\"\n    if not subset:\n        return 0.0\n    \n    total_j_dist = 0.0\n    subset_indices = list(subset)\n\n    for i in range(len(classes)):\n        for j in range(i + 1, len(classes)):\n            mu_i = classes[i]['mean'][subset_indices]\n            mu_j = classes[j]['mean'][subset_indices]\n            cov_i = classes[i]['cov'][np.ix_(subset_indices, subset_indices)]\n            cov_j = classes[j]['cov'][np.ix_(subset_indices, subset_indices)]\n\n            delta_mu = mu_i - mu_j\n            cov_avg = (cov_i + cov_j) / 2.0\n\n            # Bhattacharyya distance calculation\n            try:\n                # Term 1: Mahalanobis distance component\n                inv_cov_avg = np.linalg.inv(cov_avg)\n                term1 = 0.125 * delta_mu.T @ inv_cov_avg @ delta_mu\n\n                # Term 2: Covariance difference component (using slogdet for numerical stability)\n                _, logdet_i = np.linalg.slogdet(cov_i)\n                _, logdet_j = np.linalg.slogdet(cov_j)\n                _, logdet_avg = np.linalg.slogdet(cov_avg)\n                term2 = 0.5 * (logdet_avg - 0.5 * (logdet_i + logdet_j))\n                \n                b_dist = term1 + term2\n            except np.linalg.LinAlgError:\n                # This should not happen with positive-definite covariance matrices\n                # and their principal submatrices.\n                b_dist = np.inf\n\n            # Jeffries-Matusita distance\n            j_dist = 2.0 * (1.0 - np.exp(-b_dist))\n            total_j_dist += j_dist\n\n    return total_j_dist / num_pairs\n\n\ndef process_case(p, m, K, epsilon, means, covs):\n    \"\"\"\n    Processes a single test case, performing greedy and exhaustive searches.\n    \"\"\"\n    classes = [{'mean': m, 'cov': c} for m, c in zip(means, covs)]\n    num_pairs = m * (m - 1) // 2\n\n    # --- Greedy Forward Selection ---\n    selected_bands = []\n    current_j_agg = 0.0\n    available_bands = list(range(p))\n\n    for _ in range(K):\n        best_band_to_add = -1\n        max_gain = -1.0\n        \n        for band in available_bands:\n            candidate_subset = selected_bands + [band]\n            new_j_agg = calculate_j_agg(candidate_subset, classes, num_pairs)\n            gain = new_j_agg - current_j_agg\n            \n            if gain > max_gain:\n                max_gain = gain\n                best_band_to_add = band\n\n        if max_gain > epsilon:\n            selected_bands.append(best_band_to_add)\n            selected_bands.sort()\n            available_bands.remove(best_band_to_add)\n            current_j_agg += max_gain\n        else:\n            break  # Terminate if gain is below threshold\n    \n    j_greedy = calculate_j_agg(selected_bands, classes, num_pairs)\n\n    # --- Exhaustive Search for Optimal Subset of size K ---\n    j_optimal = 0.0\n    if K > 0 and K = p:\n        for subset in combinations(range(p), K):\n            j_agg = calculate_j_agg(list(subset), classes, num_pairs)\n            if j_agg > j_optimal:\n                j_optimal = j_agg\n\n    # --- Comparison ---\n    is_optimal = np.isclose(j_greedy, j_optimal)\n\n    return [selected_bands, j_greedy, j_optimal, is_optimal]\n\nsolve()\n```"
        }
    ]
}