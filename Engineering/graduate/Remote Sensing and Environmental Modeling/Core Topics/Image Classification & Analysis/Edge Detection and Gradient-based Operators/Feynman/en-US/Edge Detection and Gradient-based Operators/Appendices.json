{
    "hands_on_practices": [
        {
            "introduction": "Gradient operators are the fundamental building blocks of edge detection, but their performance is inevitably affected by noise in the source imagery. This exercise explores how random sensor noise, modeled as Additive White Gaussian Noise, propagates through the linear filtering operation of a central difference gradient estimator. By deriving the variance of the gradient estimate from first principles, you will gain a quantitative understanding of the trade-off between gradient approximation and noise amplification, a foundational concept in remote sensing and signal processing .",
            "id": "3807314",
            "problem": "A satellite pushbroom sensor acquires a two-dimensional raster of top-of-atmosphere reflectance over a coastal environment used for environmental modeling of shoreline change. Let the measured reflectance field be denoted by $Z(i,j)$, where $i$ indexes the along-track direction ($x$-axis) and $j$ indexes the cross-track direction ($y$-axis). Assume the measurement model $Z(i,j) = S(i,j) + N(i,j)$, where $S(i,j)$ is the unknown true reflectance and $N(i,j)$ is Additive White Gaussian Noise (AWGN) with zero mean, variance $\\sigma^{2}$, and spatial independence across pixels. The pixel spacing along the $x$-axis is $\\Delta_{x}$, which is constant over the scene.\n\nTo detect edges aligned with the $y$-axis, an analyst computes the central difference estimate of the $x$-component of the reflectance gradient at pixel $(i,j)$ using\n$$\ng_{x}(i,j) = \\frac{Z(i+1,j) - Z(i-1,j)}{2\\,\\Delta_{x}}.\n$$\nAssume that $S(i,j)$ is deterministic but unknown and focus only on the noise-induced variability of $g_{x}(i,j)$ under the stated AWGN model. Starting from the fundamental definitions of variance and independence for Gaussian random variables, and using only the linearity of expectation and the independence structure of $N(i,j)$ across pixels, derive a closed-form expression for the variance of $g_{x}(i,j)$ due solely to $N(i,j)$.\n\nExpress your final answer as a symbolic expression in terms of $\\sigma$ and $\\Delta_{x}$. Do not substitute any numerical values. No rounding is required.",
            "solution": "The problem as stated is scientifically grounded, well-posed, and contains all necessary information for a unique solution. It is a standard problem in signal processing and error propagation. Therefore, the problem is deemed valid and a solution will be derived.\n\nThe objective is to find the variance of the gradient estimate, $\\text{Var}(g_x(i,j))$, arising solely from the noise component $N(i,j)$. The gradient estimate $g_x(i,j)$ is given by\n$$\ng_{x}(i,j) = \\frac{Z(i+1,j) - Z(i-1,j)}{2\\,\\Delta_{x}}\n$$\nThe measurement model is $Z(i,j) = S(i,j) + N(i,j)$, where $S(i,j)$ is a deterministic true signal and $N(i,j)$ is Additive White Gaussian Noise (AWGN) with the following properties:\n$1$. Zero mean: $E[N(i,j)] = 0$ for all pixel locations $(i,j)$.\n$2$. Constant variance: $\\text{Var}(N(i,j)) = E[N(i,j)^2] - (E[N(i,j)])^2 = E[N(i,j)^2] = \\sigma^2$ for all $(i,j)$.\n$3$. Spatial independence: $N(i_1, j_1)$ and $N(i_2, j_2)$ are statistically independent for $(i_1, j_1) \\neq (i_2, j_2)$. This implies that $E[N(i_1, j_1)N(i_2, j_2)] = E[N(i_1, j_1)]E[N(i_2, j_2)] = 0$.\n\nWe begin by substituting the measurement model into the expression for $g_x(i,j)$:\n$$\ng_{x}(i,j) = \\frac{[S(i+1,j) + N(i+1,j)] - [S(i-1,j) + N(i-1,j)]}{2\\,\\Delta_{x}}\n$$\nWe can separate the terms related to the true signal $S$ and the noise $N$:\n$$\ng_{x}(i,j) = \\frac{S(i+1,j) - S(i-1,j)}{2\\,\\Delta_{x}} + \\frac{N(i+1,j) - N(i-1,j)}{2\\,\\Delta_{x}}\n$$\nThe variance of a random variable $X$ is defined as $\\text{Var}(X) = E[(X - E[X])^2]$. We first compute the expected value of $g_x(i,j)$, denoted as $E[g_x(i,j)]$. Using the linearity of the expectation operator:\n$$\nE[g_{x}(i,j)] = E\\left[\\frac{S(i+1,j) - S(i-1,j)}{2\\,\\Delta_{x}} + \\frac{N(i+1,j) - N(i-1,j)}{2\\,\\Delta_{x}}\\right]\n$$\n$$\nE[g_{x}(i,j)] = E\\left[\\frac{S(i+1,j) - S(i-1,j)}{2\\,\\Delta_{x}}\\right] + E\\left[\\frac{N(i+1,j) - N(i-1,j)}{2\\,\\Delta_{x}}\\right]\n$$\nSince $S(i,j)$ is deterministic, the first term is a constant and its expectation is the term itself. The factor $\\frac{1}{2\\,\\Delta_{x}}$ can be factored out of the expectation in the second term.\n$$\nE[g_{x}(i,j)] = \\frac{S(i+1,j) - S(i-1,j)}{2\\,\\Delta_{x}} + \\frac{1}{2\\,\\Delta_{x}} E[N(i+1,j) - N(i-1,j)]\n$$\nApplying linearity of expectation to the noise term:\n$$\nE[N(i+1,j) - N(i-1,j)] = E[N(i+1,j)] - E[N(i-1,j)]\n$$\nGiven that the noise has zero mean, $E[N(i,j)] = 0$ for any $(i,j)$. Therefore, $E[N(i+1,j)] = 0$ and $E[N(i-1,j)] = 0$. This leads to:\n$$\nE[N(i+1,j) - N(i-1,j)] = 0 - 0 = 0\n$$\nSubstituting this result back, we find the expected value of the gradient estimate:\n$$\nE[g_{x}(i,j)] = \\frac{S(i+1,j) - S(i-1,j)}{2\\,\\Delta_{x}}\n$$\nNow we compute the variance, $\\text{Var}(g_x(i,j))$.\n$$\n\\text{Var}(g_x(i,j)) = E\\left[ (g_x(i,j) - E[g_x(i,j)])^2 \\right]\n$$\nSubstituting the expressions for $g_x(i,j)$ and $E[g_x(i,j)]$:\n$$\ng_x(i,j) - E[g_x(i,j)] = \\left(\\frac{S(i+1,j) - S(i-1,j)}{2\\,\\Delta_{x}} + \\frac{N(i+1,j) - N(i-1,j)}{2\\,\\Delta_{x}}\\right) - \\frac{S(i+1,j) - S(i-1,j)}{2\\,\\Delta_{x}}\n$$\n$$\ng_x(i,j) - E[g_x(i,j)] = \\frac{N(i+1,j) - N(i-1,j)}{2\\,\\Delta_{x}}\n$$\nThus, the variance is:\n$$\n\\text{Var}(g_x(i,j)) = E\\left[ \\left( \\frac{N(i+1,j) - N(i-1,j)}{2\\,\\Delta_{x}} \\right)^2 \\right]\n$$\nThe constant factor can be squared and taken outside the expectation:\n$$\n\\text{Var}(g_x(i,j)) = \\frac{1}{(2\\,\\Delta_{x})^2} E\\left[ (N(i+1,j) - N(i-1,j))^2 \\right] = \\frac{1}{4\\,\\Delta_{x}^2} E\\left[ (N(i+1,j) - N(i-1,j))^2 \\right]\n$$\nWe expand the squared term inside the expectation:\n$$\n(N(i+1,j) - N(i-1,j))^2 = N(i+1,j)^2 - 2N(i+1,j)N(i-1,j) + N(i-1,j)^2\n$$\nUsing the linearity of expectation on this expanded form:\n$$\nE\\left[ (N(i+1,j) - N(i-1,j))^2 \\right] = E[N(i+1,j)^2] - 2E[N(i+1,j)N(i-1,j)] + E[N(i-1,j)^2]\n$$\nWe evaluate each term:\n$1$. From the definition of variance, $\\text{Var}(X) = E[X^2] - (E[X])^2$. Since $E[N(i,j)] = 0$, we have $\\text{Var}(N(i,j)) = E[N(i,j)^2] = \\sigma^2$. Therefore, $E[N(i+1,j)^2] = \\sigma^2$ and $E[N(i-1,j)^2] = \\sigma^2$.\n$2$. For the cross-term $E[N(i+1,j)N(i-1,j)]$, we use the property of spatial independence. The pixels at $(i+1,j)$ and $(i-1,j)$ are distinct. Thus, the noise values $N(i+1,j)$ and $N(i-1,j)$ are independent random variables. For independent variables, the expectation of their product is the product of their expectations:\n$$\nE[N(i+1,j)N(i-1,j)] = E[N(i+1,j)] E[N(i-1,j)] = 0 \\cdot 0 = 0\n$$\nSubstituting these results back into the expanded expectation:\n$$\nE\\left[ (N(i+1,j) - N(i-1,j))^2 \\right] = \\sigma^2 - 2(0) + \\sigma^2 = 2\\sigma^2\n$$\nFinally, we substitute this into our expression for the variance of $g_x(i,j)$:\n$$\n\\text{Var}(g_x(i,j)) = \\frac{1}{4\\,\\Delta_{x}^2} (2\\sigma^2)\n$$\nSimplifying this expression yields the final result:\n$$\n\\text{Var}(g_x(i,j)) = \\frac{2\\sigma^2}{4\\,\\Delta_{x}^2} = \\frac{\\sigma^2}{2\\,\\Delta_{x}^2}\n$$\nThis is the closed-form expression for the variance of the gradient estimate $g_x(i,j)$ due solely to the additive white Gaussian noise.",
            "answer": "$$\n\\boxed{\\frac{\\sigma^{2}}{2\\Delta_{x}^{2}}}\n$$"
        },
        {
            "introduction": "While first-order gradients measure edge strength, second-order operators like the Laplacian can pinpoint edge locations. This practice delves into the crucial property of rotational isotropy—an operator's ability to respond consistently to features regardless of their orientation. You will construct and compare two common discrete Laplacian kernels, analyzing how their underlying structure affects their response to edges at different angles, which builds critical intuition for operator design .",
            "id": "3807330",
            "problem": "A panchromatic satellite image sampled on a square grid with spacing $h$ is used to delineate linear environmental features (e.g., coastlines and riverbanks). You are asked to design discrete approximations to the two-dimensional Laplacian operator $\\nabla^2 f$ for use in edge detection, and to assess their rotational isotropy by comparing their responses to ideal step edges at multiple orientations. Work on a $3 \\times 3$ stencil centered at the origin $(0,0)$ with integer coordinates $(i,j) \\in \\{-1,0,1\\} \\times \\{-1,0,1\\}$. Assume $h=1$ for simplicity.\n\nStarting only from the definition of the Laplacian $\\nabla^2 f = \\frac{\\partial^2 f}{\\partial x^2} + \\frac{\\partial^2 f}{\\partial y^2}$ and the standard central-difference principle for second derivatives on a uniform grid, first construct the $4$-neighborhood and $8$-neighborhood discrete Laplacian kernels as $3 \\times 3$ convolution masks. Next, model an ideal binary step edge of unit amplitude as an indicator function on a half-plane, with the convention that pixels exactly on the boundary belong to the bright side. Consider three canonical edge orientations that pass through the origin: horizontal along $y=0$ (bright side $j \\ge 0$), vertical along $x=0$ (bright side $i \\ge 0$), and diagonal along $y=x$ (bright side $j \\ge i$). For each kernel, compute the discrete convolution response at the center $(0,0)$ for each of the three edges. Finally, based on these responses, decide which kernel is more isotropic with respect to edge orientation and, if applicable, quantify the anisotropy by the ratio of minimum to maximum response magnitude across the tested orientations for that kernel.\n\nWhich option correctly states the two kernels, the three responses for each kernel (ordered as horizontal, vertical, diagonal), and the isotropy conclusion?\n\nA. $4$-neighborhood kernel $K_4 = \\begin{bmatrix} 0  1  0 \\\\ 1  -4  1 \\\\ 0  1  0 \\end{bmatrix}$, $8$-neighborhood kernel $K_8 = \\begin{bmatrix} 1  1  1 \\\\ 1  -8  1 \\\\ 1  1  1 \\end{bmatrix}$; responses: $K_4$ gives $(-1,-1,-2)$, $K_8$ gives $(-3,-3,-3)$; hence $K_8$ is more isotropic, while $K_4$ exhibits anisotropy with ratio $1/2$.\n\nB. $4$-neighborhood kernel $K_4 = \\begin{bmatrix} 0  1  0 \\\\ 1  -4  1 \\\\ 0  1  0 \\end{bmatrix}$, $8$-neighborhood kernel $K_8 = \\begin{bmatrix} 1  1  1 \\\\ 1  -8  1 \\\\ 1  1  1 \\end{bmatrix}$; responses: $K_4$ gives $(-1,-1,-1)$, $K_8$ gives $(-4,-4,-2)$; hence $K_4$ is more isotropic.\n\nC. $4$-neighborhood kernel $K_4 = \\begin{bmatrix} 0  1  0 \\\\ 1  -4  1 \\\\ 0  1  0 \\end{bmatrix}$, $8$-neighborhood kernel $K_8 = \\begin{bmatrix} 1  1  1 \\\\ 1  -8  1 \\\\ 1  1  1 \\end{bmatrix}$; responses: $K_4$ gives $(-2,-2,-2)$, $K_8$ gives $(-2,-2,-2)$; hence both are equally isotropic.\n\nD. $4$-neighborhood kernel $K_4 = \\begin{bmatrix} 0  1  0 \\\\ 1  -4  1 \\\\ 0  1  0 \\end{bmatrix}$, $8$-neighborhood kernel $K_8 = \\begin{bmatrix} 1  2  1 \\\\ 2  -12  2 \\\\ 1  2  1 \\end{bmatrix}$; responses: $K_4$ gives $(-1,-1,-2)$, $K_8$ gives $(-4,-4,-4)$; hence $K_8$ is more isotropic and properly normalized to match $K_4$.",
            "solution": "The user has provided a problem to validate and solve regarding the properties of discrete Laplacian operators in the context of image processing.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- **Topic**: Discrete approximations of the 2D Laplacian operator, $\\nabla^2 f = \\frac{\\partial^2 f}{\\partial x^2} + \\frac{\\partial^2 f}{\\partial y^2}$, for edge detection.\n- **Grid**: $3 \\times 3$ stencil on a square grid with spacing $h=1$, centered at $(0,0)$ with integer coordinates $(i,j) \\in \\{-1,0,1\\} \\times \\{-1,0,1\\}$.\n- **Derivation Principle**: Standard central-difference principle for second derivatives.\n- **Task 1**: Construct the $4$-neighborhood and $8$-neighborhood discrete Laplacian kernels ($K_4, K_8$).\n- **Edge Model**: Ideal binary step edge of unit amplitude ($f=1$ for bright, $f=0$ for dark).\n- **Boundary Convention**: Pixels on the boundary line belong to the bright side.\n- **Edge Orientations for Testing**:\n    1.  Horizontal: Boundary along $y=0$, bright side $j \\ge 0$.\n    2.  Vertical: Boundary along $x=0$, bright side $i \\ge 0$.\n    3.  Diagonal: Boundary along $y=x$, bright side $j \\ge i$.\n- **Task 2**: Compute the discrete convolution response at the center $(0,0)$ for each kernel and each edge.\n- **Task 3**: Compare kernels for rotational isotropy and quantify anisotropy for the less isotropic kernel by the ratio of minimum to maximum response magnitude.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded**: The problem is based on fundamental and standard concepts in numerical analysis and image processing, namely finite difference approximations of differential operators and their application in edge detection. The Laplacian operator and central-difference schemes are cornerstones of the field.\n- **Well-Posed**: The problem is clearly stated with all necessary definitions and constraints. The tasks are specified, and the methods for constructing the kernels and evaluating them are defined, leading to a unique and verifiable solution.\n- **Objective**: The problem is formulated in precise, objective mathematical language, free from subjectivity or ambiguity.\n\n**Step 3: Verdict and Action**\nThe problem statement is valid. It is scientifically sound, well-posed, objective, and self-contained. The solution can be derived following the specified steps.\n\n### Derivation and Solution\n\n**Part 1: Construction of the Laplacian Kernels**\n\nThe Laplacian operator is defined as $\\nabla^2 f = \\frac{\\partial^2 f}{\\partial x^2} + \\frac{\\partial^2 f}{\\partial y^2}$. We use the central-difference formula for the second partial derivatives on a grid with spacing $h=1$.\nThe second partial derivative with respect to $x$ at grid point $(i,j)$ is:\n$$ \\frac{\\partial^2 f}{\\partial x^2} \\approx \\frac{f(i+1, j) - 2f(i, j) + f(i-1, j)}{1^2} = f(i+1, j) + f(i-1, j) - 2f(i, j) $$\nSimilarly, the second partial derivative with respect to $y$ is:\n$$ \\frac{\\partial^2 f}{\\partial y^2} \\approx \\frac{f(i, j+1) - 2f(i, j) + f(i, j-1)}{1^2} = f(i, j+1) + f(i, j-1) - 2f(i, j) $$\n\n**$4$-neighborhood Kernel ($K_4$):**\nThis approximation is the sum of the second derivatives along the $x$ and $y$ axes.\n$$ \\nabla^2 f(i,j) \\approx [f(i+1, j) + f(i-1, j) - 2f(i, j)] + [f(i, j+1) + f(i, j-1) - 2f(i, j)] $$\n$$ \\nabla^2 f(i,j) \\approx f(i+1, j) + f(i-1, j) + f(i, j+1) + f(i, j-1) - 4f(i, j) $$\nThis expression corresponds to a convolution (or correlation, as the kernel is symmetric) with the following $3 \\times 3$ kernel, $K_4$:\n$$ K_4 = \\begin{bmatrix} 0  1  0 \\\\ 1  -4  1 \\\\ 0  1  0 \\end{bmatrix} $$\n\n**$8$-neighborhood Kernel ($K_8$):**\nThe standard $8$-neighborhood Laplacian kernel simply extends the principle by including all $8$ neighbors. Each neighbor contributes with a weight of $1$, and the center's weight is the negative sum of the neighbors' weights, which is $-8$.\n$$ \\nabla^2 f(i,j) \\approx \\sum_{k,l \\in \\{-1,0,1\\}, (k,l)\\neq(0,0)} f(i+k, j+l) - 8f(i, j) $$\nThis corresponds to the kernel $K_8$:\n$$ K_8 = \\begin{bmatrix} 1  1  1 \\\\ 1  -8  1 \\\\ 1  1  1 \\end{bmatrix} $$\nThe kernels stated in options A, B, and C match these derivations. Option D uses a different, weighted version for $K_8$, which is not the most common \"standard\" definition.\n\n**Part 2: Computing the Convolution Responses**\n\nThe convolution response $R$ at the center $(0,0)$ is computed as the sum of the element-wise product of the kernel $K$ and the $3 \\times 3$ image patch $P$ centered at $(0,0)$. The coordinates $(i,j)$ for the patch are in $\\{-1,0,1\\} \\times \\{-1,0,1\\}$, with $i$ representing the column index and $j$ the row index, and $(0,0)$ at the center. The patch is represented as $P_{ij} = f(i-1, 1-j)$ using matrix indexing convention, but we will use coordinate pairs $(i,j)$ for clarity.\n\n**Edge 1: Horizontal** ($j \\ge 0$ is bright)\nThe image patch $P_H$ has $f(i,j)=1$ for $j \\ge 0$ and $f(i,j)=0$ for $j  0$.\n$$ P_H = \\begin{bmatrix} f(-1,1)  f(0,1)  f(1,1) \\\\ f(-1,0)  f(0,0)  f(1,0) \\\\ f(-1,-1)  f(0,-1)  f(1,-1) \\end{bmatrix} = \\begin{bmatrix} 1  1  1 \\\\ 1  1  1 \\\\ 0  0  0 \\end{bmatrix} $$\n- Response for $K_4$: $R_{4,H} = (0)(1) + (1)(1) + (0)(1) + (1)(1) + (-4)(1) + (1)(1) + (0)(0) + (1)(0) + (0)(0) = 1+1-4+1 = -1$.\n- Response for $K_8$: $R_{8,H} = (1)(1) + (1)(1) + (1)(1) + (1)(1) + (-8)(1) + (1)(1) + (1)(0) + (1)(0) + (1)(0) = 3 + (1-8+1) + 0 = 3 - 6 = -3$.\n\n**Edge 2: Vertical** ($i \\ge 0$ is bright)\nThe image patch $P_V$ has $f(i,j)=1$ for $i \\ge 0$ and $f(i,j)=0$ for $i  0$.\n$$ P_V = \\begin{bmatrix} f(-1,1)  f(0,1)  f(1,1) \\\\ f(-1,0)  f(0,0)  f(1,0) \\\\ f(-1,-1)  f(0,-1)  f(1,-1) \\end{bmatrix} = \\begin{bmatrix} 0  1  1 \\\\ 0  1  1 \\\\ 0  1  1 \\end{bmatrix} $$\n- Response for $K_4$: $R_{4,V} = (0)(0) + (1)(1) + (0)(1) + (1)(0) + (-4)(1) + (1)(1) + (0)(0) + (1)(1) + (0)(1) = 1-4+1+1 = -1$.\n- Response for $K_8$: $R_{8,V} = (1)(0) + (1)(1) + (1)(1) + (1)(0) + (-8)(1) + (1)(1) + (1)(0) + (1)(1) + (1)(1) = 2 + (-8+1) + 2 = 2-7+2 = -3$.\n\n**Edge 3: Diagonal** ($j \\ge i$ is bright)\nThe image patch $P_D$ has $f(i,j)=1$ where $j \\ge i$.\n- At $(i,j)=(-1,1), 1 \\ge -1$, $f=1$. At $(0,1), 1 \\ge 0$, $f=1$. At $(1,1), 1 \\ge 1$, $f=1$.\n- At $(i,j)=(-1,0), 0 \\ge -1$, $f=1$. At $(0,0), 0 \\ge 0$, $f=1$. At $(1,0), 0 \\ge 1$, $f=0$.\n- At $(i,j)=(-1,-1), -1 \\ge -1$, $f=1$. At $(0,-1), -1 \\ge 0$, $f=0$. At $(1,-1), -1 \\ge 1$, $f=0$.\n$$ P_D = \\begin{bmatrix} 1  1  1 \\\\ 1  1  0 \\\\ 1  0  0 \\end{bmatrix} $$\n- Response for $K_4$: $R_{4,D} = (0)(1) + (1)(1) + (0)(1) + (1)(1) + (-4)(1) + (1)(0) + (0)(1) + (1)(0) + (0)(0) = 1+1-4 = -2$.\n- Response for $K_8$: $R_{8,D} = (1)(1) + (1)(1) + (1)(1) + (1)(1) + (-8)(1) + (1)(0) + (1)(1) + (1)(0) + (1)(0) = 3+(1-8)+1 = 3-7+1 = -3$.\n\n**Part 3: Isotropy Analysis**\n\n- **$K_4$ Responses**: The responses for horizontal, vertical, and diagonal edges are $(-1, -1, -2)$. The magnitudes are $(1, 1, 2)$. Since the magnitudes are not constant, this operator is anisotropic with respect to edge orientation. The ratio of minimum to maximum response magnitude is $\\frac{\\min(1,1,2)}{\\max(1,1,2)} = \\frac{1}{2}$.\n- **$K_8$ Responses**: The responses for all three edge orientations are $(-3, -3, -3)$. The magnitudes are $(3, 3, 3)$. Since the magnitude is constant across these orientations, this operator is more isotropic than $K_4$. Based on this test, it is isotropic.\n\n### Option-by-Option Analysis\n\n**A. $4$-neighborhood kernel $K_4 = \\begin{bmatrix} 0  1  0 \\\\ 1  -4  1 \\\\ 0  1  0 \\end{bmatrix}$, $8$-neighborhood kernel $K_8 = \\begin{bmatrix} 1  1  1 \\\\ 1  -8  1 \\\\ 1  1  1 \\end{bmatrix}$; responses: $K_4$ gives $(-1,-1,-2)$, $K_8$ gives $(-3,-3,-3)$; hence $K_8$ is more isotropic, while $K_4$ exhibits anisotropy with ratio $1/2$.**\n- The kernels $K_4$ and $K_8$ are correct.\n- The responses for $K_4$ are $(-1,-1,-2)$, which matches our calculation.\n- The responses for $K_8$ are $(-3,-3,-3)$, which matches our calculation.\n- The conclusion that $K_8$ is more isotropic and the anisotropy ratio of $1/2$ for $K_4$ is correct.\n- **Verdict**: **Correct**.\n\n**B. $4$-neighborhood kernel $K_4 = \\begin{bmatrix} 0  1  0 \\\\ 1  -4  1 \\\\ 0  1  0 \\end{bmatrix}$, $8$-neighborhood kernel $K_8 = \\begin{bmatrix} 1  1  1 \\\\ 1  -8  1 \\\\ 1  1  1 \\end{bmatrix}$; responses: $K_4$ gives $(-1,-1,-1)$, $K_8$ gives $(-4,-4,-2)$; hence $K_4$ is more isotropic.**\n- The kernels are correct.\n- The responses for $K_4$ are stated as $(-1,-1,-1)$, which is incorrect. Our calculation yielded $(-1,-1,-2)$.\n- The responses for $K_8$ are stated as $(-4,-4,-2)$, which is incorrect. Our calculation yielded $(-3,-3,-3)$.\n- The conclusion is based on incorrect data and is also incorrect.\n- **Verdict**: **Incorrect**.\n\n**C. $4$-neighborhood kernel $K_4 = \\begin{bmatrix} 0  1  0 \\\\ 1  -4  1 \\\\ 0  1  0 \\end{bmatrix}$, $8$-neighborhood kernel $K_8 = \\begin{bmatrix} 1  1  1 \\\\ 1  -8  1 \\\\ 1  1  1 \\end{bmatrix}$; responses: $K_4$ gives $(-2,-2,-2)$, $K_8$ gives $(-2,-2,-2)$; hence both are equally isotropic.**\n- The kernels are correct.\n- The responses for $K_4$ are stated as $(-2,-2,-2)$, which is incorrect. Our calculation yielded $(-1,-1,-2)$.\n- The responses for $K_8$ are stated as $(-2,-2,-2)$, which is incorrect. Our calculation yielded $(-3,-3,-3)$.\n- The conclusion is based on incorrect data.\n- **Verdict**: **Incorrect**.\n\n**D. $4$-neighborhood kernel $K_4 = \\begin{bmatrix} 0  1  0 \\\\ 1  -4  1 \\\\ 0  1  0 \\end{bmatrix}$, $8$-neighborhood kernel $K_8 = \\begin{bmatrix} 1  2  1 \\\\ 2  -12  2 \\\\ 1  2  1 \\end{bmatrix}$; responses: $K_4$ gives $(-1,-1,-2)$, $K_8$ gives $(-4,-4,-4)$; hence $K_8$ is more isotropic and properly normalized to match $K_4$.**\n- The kernel $K_4$ is correct.\n- The kernel $K_8$ is a different variant of the Laplacian operator, not the one most commonly referred to as the simple \"$8$-neighborhood kernel\", which is derived by summing unweighted neighbors.\n- The responses for $K_4$ are stated as $(-1,-1,-2)$, which is correct.\n- The responses for the proposed $K_8$ are stated as $(-4,-4,-4)$. Let's test this. Our prior calculation showed this kernel yields responses of $(-4,-4,-5)$, not $(-4,-4,-4)$. So, the stated responses are incorrect for the kernel given in this option.\n- The conclusion is based on incorrect data.\n- **Verdict**: **Incorrect**.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "Designing an edge detector is only half the battle; rigorously evaluating its performance is equally important for scientific comparison and model tuning. This problem introduces a formal, quantitative framework for evaluation based on the core metrics of precision, recall, and the $F_1$ score. It moves beyond simple pixel-wise comparison by incorporating a spatial tolerance for matching, which is essential for fairly assessing edge localization accuracy in real-world applications like shoreline mapping .",
            "id": "3807306",
            "problem": "A gradient-based edge detector is applied to a pan-sharpened multispectral satellite tile to delineate shoreline and riparian boundaries for hydrologic feature mapping in a coastal wetland. Let the image domain be a finite set of pixels denoted by $\\mathcal{U}$. The detector outputs a binary edge map interpreted as a predicted positive set $\\mathcal{E} \\subset \\mathcal{U}$, while a curated reference boundary annotation defines a ground-truth positive set $\\mathcal{G} \\subset \\mathcal{U}$. To account for localization uncertainty, evaluation proceeds by one-to-one bipartite matching between $\\mathcal{E}$ and $\\mathcal{G}$ under a fixed spatial tolerance $r$ (in pixels): a predicted edge pixel $e \\in \\mathcal{E}$ may be matched to at most one ground-truth pixel $g \\in \\mathcal{G}$ if and only if the Euclidean distance between their centers is at most $r$, and the matching maximizes the number of matched pairs. Let the resulting matched subsets be $\\mathcal{E}_{m} \\subset \\mathcal{E}$ and $\\mathcal{G}_{m} \\subset \\mathcal{G}$, with $|\\mathcal{E}_{m}| = |\\mathcal{G}_{m}|$.\n\nUsing only the notions of set membership, set cardinality, and the harmonic mean, do the following:\n\n1) Provide precise mathematical definitions of the quantities $N_{\\mathrm{TP}}$, $N_{\\mathrm{FP}}$, $N_{\\mathrm{FN}}$, and $N_{\\mathrm{TN}}$ in terms of $\\mathcal{U}$, $\\mathcal{E}$, $\\mathcal{G}$, $\\mathcal{E}_{m}$, and $\\mathcal{G}_{m}$.\n\n2) Starting from those definitions and the fundamental definition of the harmonic mean of two positive reals, derive expressions for precision, recall, and the $F_{1}$ score for binary edge maps in terms of $N_{\\mathrm{TP}}$, $N_{\\mathrm{FP}}$, and $N_{\\mathrm{FN}}$.\n\nFor a specific test tile evaluated with tolerance $r = 1.5$ pixels, bipartite matching yields the following counts: $N_{\\mathrm{TP}} = 12{,}350$, $N_{\\mathrm{FP}} = 3{,}650$, $N_{\\mathrm{FN}} = 4{,}100$, and $N_{\\mathrm{TN}} = 4{,}193{,}900$.\n\nCompute the $F_{1}$ score for this tile from these counts. Report only the $F_{1}$ score as a unitless decimal rounded to four significant figures.",
            "solution": "The problem statement has been validated and is deemed sound. It is scientifically grounded in the principles of digital image processing and performance evaluation, well-posed with a unique and meaningful solution, and expressed in objective, formal language. We may therefore proceed with the solution.\n\nThe problem asks for three tasks: 1) define performance metrics based on given set-theoretic concepts, 2) derive the formula for the $F_{1}$ score, and 3) compute the $F_{1}$ score for a specific case.\n\n**1. Definitions of $N_{\\mathrm{TP}}$, $N_{\\mathrm{FP}}$, $N_{\\mathrm{FN}}$, and $N_{\\mathrm{TN}}$**\n\nThe evaluation is based on a bipartite matching between predicted edge pixels ($\\mathcal{E}$) and ground-truth edge pixels ($\\mathcal{G}$). This matching yields subsets of matched pixels, $\\mathcal{E}_{m} \\subset \\mathcal{E}$ and $\\mathcal{G}_{m} \\subset \\mathcal{G}$, where $|\\mathcal{E}_{m}| = |\\mathcal{G}_{m}|$. The quantities $N_{\\mathrm{TP}}$, $N_{\\mathrm{FP}}$, $N_{\\mathrm{FN}}$, and $N_{\\mathrm{TN}}$—representing the counts of true positives, false positives, false negatives, and true negatives, respectively—are defined as follows:\n\n-   **True Positives ($N_{\\mathrm{TP}}$):** A true positive corresponds to a correctly detected edge pixel. In this matching-based framework, this is a predicted edge pixel that is successfully matched to a ground-truth edge pixel. The total number of such successful matches is the cardinality of the matched sets.\n    $$N_{\\mathrm{TP}} = |\\mathcal{E}_{m}| = |\\mathcal{G}_{m}|$$\n\n-   **False Positives ($N_{\\mathrm{FP}}$):** A false positive is a predicted edge pixel that does not correspond to any ground-truth edge pixel within the given tolerance. These are the predicted pixels left over after the matching process.\n    $$N_{\\mathrm{FP}} = |\\mathcal{E} \\setminus \\mathcal{E}_{m}| = |\\mathcal{E}| - |\\mathcal{E}_{m}|$$\n\n-   **False Negatives ($N_{\\mathrm{FN}}$):** A false negative is a ground-truth edge pixel that the detector failed to identify. These are the ground-truth pixels left over after the matching process.\n    $$N_{\\mathrm{FN}} = |\\mathcal{G} \\setminus \\mathcal{G}_{m}| = |\\mathcal{G}| - |\\mathcal{G}_{m}|$$\n\n-   **True Negatives ($N_{\\mathrm{TN}}$):** A true negative is a pixel that is correctly identified as not being an edge. This constitutes all pixels in the image domain $\\mathcal{U}$ that are neither in the predicted set $\\mathcal{E}$ nor in the ground-truth set $\\mathcal{G}$. Using set theory, this is the complement of the union of $\\mathcal{E}$ and $\\mathcal{G}$.\n    $$N_{\\mathrm{TN}} = |\\mathcal{U} \\setminus (\\mathcal{E} \\cup \\mathcal{G})|$$\n\n**2. Derivation of Precision, Recall, and $F_{1}$ Score**\n\nWe proceed from the fundamental definitions of the metrics.\n\n-   **Precision ($P$):** Precision is the fraction of predicted positives that are actually true positives. The total number of predicted positives is $|\\mathcal{E}|$. The number of true positives is $N_{\\mathrm{TP}}$.\n    $$P = \\frac{N_{\\mathrm{TP}}}{|\\mathcal{E}|}$$\n    From the definition of $N_{\\mathrm{FP}}$, we have $|\\mathcal{E}| = |\\mathcal{E}_{m}| + |\\mathcal{E} \\setminus \\mathcal{E}_{m}| = N_{\\mathrm{TP}} + N_{\\mathrm{FP}}$. Substituting this into the expression for $P$:\n    $$P = \\frac{N_{\\mathrm{TP}}}{N_{\\mathrm{TP}} + N_{\\mathrm{FP}}}$$\n\n-   **Recall ($R$):** Recall (or sensitivity) is the fraction of actual positives that are correctly identified. The total number of actual positives is the size of the ground-truth set, $|\\mathcal{G}|$. The number of true positives is $N_{\\mathrm{TP}}$.\n    $$R = \\frac{N_{\\mathrm{TP}}}{|\\mathcal{G}|}$$\n    From the definition of $N_{\\mathrm{FN}}$, we have $|\\mathcal{G}| = |\\mathcal{G}_{m}| + |\\mathcal{G} \\setminus \\mathcal{G}_{m}| = N_{\\mathrm{TP}} + N_{\\mathrm{FN}}$. Substituting this into the expression for $R$:\n    $$R = \\frac{N_{\\mathrm{TP}}}{N_{\\mathrm{TP}} + N_{\\mathrm{FN}}}$$\n\n-   **$F_{1}$ Score:** The $F_{1}$ score is defined as the harmonic mean of precision and recall. For two positive real numbers $a$ and $b$, their harmonic mean $H$ is given by $H = \\frac{2}{\\frac{1}{a} + \\frac{1}{b}}$. Setting $a=P$ and $b=R$:\n    $$F_{1} = \\frac{2}{\\frac{1}{P} + \\frac{1}{R}}$$\n    We substitute the expressions for $P$ and $R$ in terms of $N_{\\mathrm{TP}}$, $N_{\\mathrm{FP}}$, and $N_{\\mathrm{FN}}$ into this formula.\n    $$\\frac{1}{P} = \\frac{N_{\\mathrm{TP}} + N_{\\mathrm{FP}}}{N_{\\mathrm{TP}}}$$\n    $$\\frac{1}{R} = \\frac{N_{\\mathrm{TP}} + N_{\\mathrm{FN}}}{N_{\\mathrm{TP}}}$$\n    Their sum is:\n    $$\\frac{1}{P} + \\frac{1}{R} = \\frac{N_{\\mathrm{TP}} + N_{\\mathrm{FP}}}{N_{\\mathrm{TP}}} + \\frac{N_{\\mathrm{TP}} + N_{\\mathrm{FN}}}{N_{\\mathrm{TP}}} = \\frac{(N_{\\mathrm{TP}} + N_{\\mathrm{FP}}) + (N_{\\mathrm{TP}} + N_{\\mathrm{FN}})}{N_{\\mathrm{TP}}} = \\frac{2N_{\\mathrm{TP}} + N_{\\mathrm{FP}} + N_{\\mathrm{FN}}}{N_{\\mathrm{TP}}}$$\n    Substituting this back into the formula for $F_{1}$:\n    $$F_{1} = \\frac{2}{\\left(\\frac{2N_{\\mathrm{TP}} + N_{\\mathrm{FP}} + N_{\\mathrm{FN}}}{N_{\\mathrm{TP}}}\\right)} = \\frac{2N_{\\mathrm{TP}}}{2N_{\\mathrm{TP}} + N_{\\mathrm{FP}} + N_{\\mathrm{FN}}}$$\n    This is the required expression for the $F_{1}$ score.\n\n**3. Computation of the $F_{1}$ Score**\n\nWe are given the following counts for a specific test tile:\n-   $N_{\\mathrm{TP}} = 12350$\n-   $N_{\\mathrm{FP}} = 3650$\n-   $N_{\\mathrm{FN}} = 4100$\n-   $N_{\\mathrm{TN}} = 4193900$ (Note that $N_{\\mathrm{TN}}$ is not required for the $F_1$ score calculation).\n\nWe use the derived formula for the $F_{1}$ score:\n$$F_{1} = \\frac{2N_{\\mathrm{TP}}}{2N_{\\mathrm{TP}} + N_{\\mathrm{FP}} + N_{\\mathrm{FN}}}$$\nSubstituting the given values:\n$$F_{1} = \\frac{2 \\times 12350}{2 \\times 12350 + 3650 + 4100}$$\n$$F_{1} = \\frac{24700}{24700 + 3650 + 4100}$$\n$$F_{1} = \\frac{24700}{32450}$$\nTo simplify the fraction, we can remove the common factor of $10$ from the numerator and denominator:\n$$F_{1} = \\frac{2470}{3245}$$\nPerforming the division gives:\n$$F_{1} \\approx 0.7611710323...$$\nThe problem requires the answer to be rounded to four significant figures. The fifth significant figure is $7$, which is greater than or equal to $5$, so we round up the fourth significant figure.\n$$F_{1} \\approx 0.7612$$",
            "answer": "$$\\boxed{0.7612}$$"
        }
    ]
}