{
    "hands_on_practices": [
        {
            "introduction": "在遥感影像分析中，软标签不仅为每个像素分配了类别归属，更将其表示为一个概率分布。实践的第一步是学会量化这种概率表达中所包含的不确定性，这是理解和解释模糊分类结果的基础。本练习将通过计算一个给定软标签的香农熵（Shannon Entropy），让你亲手实践如何使用信息论的基本工具来衡量分类的模糊程度。",
            "id": "3814939",
            "problem": "一个海岸湿地场景中的多光谱像素被建模为 $K=3$ 种土地覆盖类别（开放水域、挺水植被和裸露沉积物）的凸混合。一个监督概率分类器输出了一个软标签向量 $p=(p_{1},p_{2},p_{3})=(0.2,0.5,0.3)$，表示该像素的后验类别隶属概率。在遥感模糊分类中，一个有效的软标签必须位于概率单纯形内，这意味着每个分量都非负，且所有分量之和为 $1$。首先，请仅使用有限样本空间上概率测度的核心定义来评估给定的 $p$ 是否满足此归一化性质。\n\n为了量化用于环境建模的分类不确定性，采用由 Shannon–Khinchin 公理（连续性、在均匀分布处取最大值、可扩展性和可加性）所刻画的不确定性泛函。使用自然对数，使得不确定性以纳特（nats）为单位度量，计算此像素软标签 $p$ 的不确定性。将最终数值结果四舍五入到四位有效数字，并以纳特（nats）表示答案。请仅提供数值作为最终答案。",
            "solution": "问题陈述首先经过严格的验证过程。\n\n已知条件如下：\n- 一个包含 $K=3$ 种不同土地覆盖类别的系统。\n- 一个表示后验类别隶属概率的软标签向量：$p=(p_{1},p_{2},p_{3})=(0.2,0.5,0.3)$。\n- 有效软标签的定义：它必须位于概率单纯形内，即对所有 $i$ 都有 $p_i \\ge 0$ 且 $\\sum_i p_i = 1$。\n- 第一个任务是根据概率测度的核心定义来评估给定的向量 $p$ 是否满足此性质。\n- 第二个任务是使用由 Shannon–Khinchin 公理定义的泛函来计算分类不确定性。\n- 不确定性将以纳特（nats）为单位度量，这意味着使用自然对数。\n- 最终数值结果必须四舍五入到四位有效数字。\n\n该问题具有科学依据、是良定的且客观的。它基于概率论（概率单纯形）和信息论（Shannon 熵，其由 Shannon-Khinchin 公理唯一刻画）的既定原则。遥感中的模糊分类上下文是一个标准且现实的应用领域。所提供的数据是完整、一致且充分的，足以得出一个唯一解。该问题不违反任何指定的无效标准。因此，该问题被视为有效，有必要给出完整解答。\n\n解答按要求分两部分进行。\n\n首先，我们验证给定的软标签向量 $p=(0.2, 0.5, 0.3)$。问题要求此验证基于有限样本空间上概率测度的核心定义。设互斥且穷尽的类别集合为 $\\Omega = \\{C_1, C_2, C_3\\}$。分配给每个类别的后验概率为 $p_i = P(C_i)$，其中 $P$ 是一个概率测度。对于有限样本空间，Kolmogorov 概率公理要求：\n1.  非负性：任何事件的概率都是非负的。对于基本事件 $C_i$，这转化为对所有 $i \\in \\{1, 2, 3\\}$ 都有 $p_i \\ge 0$。\n2.  归一性：整个样本空间的概率为 $1$。由于这些类别是穷尽的，这意味着 $P(\\Omega) = P(C_1 \\cup C_2 \\cup C_3) = P(C_1) + P(C_2) + P(C_3) = \\sum_{i=1}^{3} p_i = 1$。\n\n我们对给定的向量 $p$ 检查这两个条件：\n1.  非负性：向量的分量为 $p_1 = 0.2$，$p_2 = 0.5$ 和 $p_3 = 0.3$。这三个值都大于或等于 $0$。此条件满足。\n2.  归一性：分量之和计算如下：\n$$ \\sum_{i=1}^{3} p_i = p_1 + p_2 + p_3 = 0.2 + 0.5 + 0.3 = 1.0 $$\n此条件也满足。\n由于两个公理化要求都得到满足，向量 $p$ 是一个有效的概率分布，并且位于 $K=3$ 的概率单纯形内。\n\n其次，我们计算分类不确定性。问题指定使用由 Shannon–Khinchin 公理刻画的不确定性泛函。对于一个离散概率分布 $p = (p_1, p_2, \\dots, p_K)$，这个泛函是 Shannon 熵，$H(p)$，定义为：\n$$ H(p) = - \\sum_{i=1}^{K} p_i \\log_b(p_i) $$\n其中 $b$ 是对数的底。单位“纳特”（nats）表示必须使用自然对数（底为 $e$）。因此，公式变为：\n$$ H(p) = - \\sum_{i=1}^{K} p_i \\ln(p_i) $$\n如果任何 $p_i$ 为 $0$，则应用 $0 \\ln(0) = 0$ 的约定。\n\n对于给定的概率分布 $p=(0.2, 0.5, 0.3)$ 和 $K=3$，不确定性 $H(p)$ 为：\n$$ H(p) = - \\left( p_1 \\ln(p_1) + p_2 \\ln(p_2) + p_3 \\ln(p_3) \\right) $$\n代入数值：\n$$ H(p) = - \\left( 0.2 \\ln(0.2) + 0.5 \\ln(0.5) + 0.3 \\ln(0.3) \\right) $$\n我们计算自然对数的值：\n$\\ln(0.2) \\approx -1.6094379$\n$\\ln(0.5) \\approx -0.6931472$\n$\\ln(0.3) \\approx -1.2039728$\n\n现在，我们计算和中的每一项：\n$0.2 \\ln(0.2) \\approx 0.2 \\times (-1.6094379) \\approx -0.32188758$\n$0.5 \\ln(0.5) \\approx 0.5 \\times (-0.6931472) \\approx -0.34657360$\n$0.3 \\ln(0.3) \\approx 0.3 \\times (-1.2039728) \\approx -0.36119184$\n\n将这些项相加：\n$$ \\sum_{i=1}^{3} p_i \\ln(p_i) \\approx -0.32188758 - 0.34657360 - 0.36119184 \\approx -1.02965302 $$\n不确定性是这个和的负值：\n$$ H(p) \\approx -(-1.02965302) \\approx 1.02965302 \\text{ nats} $$\n问题要求结果四舍五入到四位有效数字。前四位有效数字是 $1$、$0$、$2$ 和 $9$。第五位有效数字是 $6$，大于或等于 $5$，所以我们将最后一位有效数字向上取整。因此，$1.02965...$ 四舍五入为 $1.030$。",
            "answer": "$$\\boxed{1.030}$$"
        },
        {
            "introduction": "评估模糊分类器的性能不能简单套用硬分类的精度指标，因为这会丢失部分隶属度信息。我们需要能够直接比较两个概率分布（即预测软标签和参考软标签）的度量。本练习将引导你使用海林格距离（Hellinger distance）——一种稳健且对称的度量方法——来评估分类器在像素级别和场景级别的性能，这是对软分类结果进行严格验证的关键一步。",
            "id": "3814924",
            "problem": "一幅多光谱地球观测图像被用于土地覆盖分析，分析使用一个模糊分类器，该分类器为每个像素生成软标签向量。每个向量是一组类别隶属度值，其总和为一。考虑四个土地覆盖类别（水体、植被、裸土、建筑区）。一张高分辨率分数图为相同的类别提供了作为亚像素分数的参考软标签。对于一个场景中的三个代表性像素，分类器的预测值和参考分数如下：\n- 像素 $1$：预测值 $p^{(1)} = [0.50,\\ 0.30,\\ 0.15,\\ 0.05]$，参考值 $q^{(1)} = [0.40,\\ 0.35,\\ 0.20,\\ 0.05]$。\n- 像素 $2$：预测值 $p^{(2)} = [0.05,\\ 0.25,\\ 0.60,\\ 0.10]$，参考值 $q^{(2)} = [0.10,\\ 0.20,\\ 0.65,\\ 0.05]$。\n- 像素 $3$：预测值 $p^{(3)} = [0.20,\\ 0.50,\\ 0.25,\\ 0.05]$，参考值 $q^{(3)} = [0.25,\\ 0.45,\\ 0.25,\\ 0.05]$。\n\n假设这三个像素具有可靠性权重（例如，从云筛选和信噪比估计中得出）$w^{(1)} = 0.5$, $w^{(2)} = 0.3$, $w^{(3)} = 0.2$，且 $\\sum_{i=1}^{3} w^{(i)} = 1$。\n\n从离散分布的Bhattacharyya系数的基本定义以及在概率单纯形上导出的Hellinger距离出发，完成以下任务：\n1. 推导单个像素的预测软标签向量与参考软标签向量之间的Hellinger距离的离散像素表达式。\n2. 解释如何使用权重函数以及空间域上的归一化积分（或期望）概念，来聚合整个空间场景的像素级Hellinger距离。\n3. 使用给定的权重和你的推导，计算上述三个像素的场景级聚合Hellinger距离。\n\n将最终的场景级聚合Hellinger距离以小数表示，并将答案四舍五入到四位有效数字。由于此距离是无量纲的，因此不需要物理单位。",
            "solution": "用户提供的问题经评估为有效且适定的。唯一解所需的所有数据和定义均已给出且自洽。该问题在遥感、信息论和统计学领域具有科学依据。\n\n解答将按问题陈述的要求分三部分进行。\n\n### 1. 单个像素的Hellinger距离的推导\n\n问题要求从Bhattacharyya系数出发，推导离散Hellinger距离。设 $p = \\{p_j\\}_{j=1}^K$ 和 $q = \\{q_j\\}_{j=1}^K$ 是在同一组 $K$ 个结果上的两个离散概率分布。在此情境下，$p$ 是单个像素的预测软标签向量，$q$ 是参考软标签向量，$K=4$ 是土地覆盖类别的数量。分量 $p_j$ 和 $q_j$ 代表第 $j$ 个类别的隶属度值，且满足 $\\sum_{j=1}^K p_j = 1$ 和 $\\sum_{j=1}^K q_j = 1$。\n\n对于这些离散分布，Bhattacharyya系数 $BC(p, q)$ 定义为相应概率的几何平均数之和：\n$$\nBC(p, q) = \\sum_{j=1}^{K} \\sqrt{p_j q_j}\n$$\n该系数衡量了两个分布之间的重叠程度。其取值范围从 $0$（对于不相交的分布）到 $1$（对于相同的分布）。\n\nHellinger距离 $H(p, q)$ 是定义在概率单纯形上的一个度量，由Bhattacharyya系数导出。其定义为：\n$$\nH(p, q) = \\sqrt{1 - BC(p, q)}\n$$\n将Bhattacharyya系数的表达式代入此定义，即可得到预测软标签向量 $p$ 与参考软标签向量 $q$ 之间的Hellinger距离的离散像素表达式：\n$$\nH(p, q) = \\sqrt{1 - \\sum_{j=1}^{K} \\sqrt{p_j q_j}}\n$$\n该表达式给出了单个像素的距离。\n\n### 2. 像素级Hellinger距离的聚合\n\n为了获得由多个像素组成的整个场景的单一性能度量，必须对像素级的Hellinger距离进行聚合。问题指明应使用权重函数来完成此操作。对于一个由 $i=1, \\dots, N$ 索引的 $N$ 个像素的离散集合，这对应于一个加权平均。\n\n设 $H^{(i)} = H(p^{(i)}, q^{(i)})$ 为第 $i$ 个像素的Hellinger距离，并设 $w^{(i)}$ 为与该像素相关联的可靠性权重。给定的权重是归一化的，即 $\\sum_{i=1}^N w^{(i)} = 1$。\n\n场景级聚合Hellinger距离 $H_{agg}$ 是各个像素级距离的加权算术平均值：\n$$\nH_{agg} = \\sum_{i=1}^{N} w^{(i)} H^{(i)}\n$$\n该公式等价于Hellinger距离的期望，其中像素集被视为一个样本空间，权重 $w^{(i)}$ 定义了该空间上的概率质量函数。在连续情况下，这将是空间域 $\\Omega$ 上的一个归一化积分，$H_{agg} = \\int_{\\Omega} w(x) H(p(x), q(x)) dx / \\int_{\\Omega} w(x) dx$，其中 $x$ 是一个空间坐标。对于给定的具有 $N=3$ 个像素的问题，离散求和是其直接的对应形式。\n\n### 3. 聚合Hellinger距离的计算\n\n我们现在使用推导出的公式计算给定三个像素的聚合Hellinger距离。类别数量为 $K=4$。\n\n**对于像素 1：**\n$p^{(1)} = [0.50, 0.30, 0.15, 0.05]$\n$q^{(1)} = [0.40, 0.35, 0.20, 0.05]$\n$w^{(1)} = 0.5$\n\nBhattacharyya系数为：\n$$\nBC^{(1)} = \\sqrt{0.50 \\times 0.40} + \\sqrt{0.30 \\times 0.35} + \\sqrt{0.15 \\times 0.20} + \\sqrt{0.05 \\times 0.05}\n$$\n$$\nBC^{(1)} = \\sqrt{0.20} + \\sqrt{0.105} + \\sqrt{0.03} + 0.05\n$$\n$$\nBC^{(1)} \\approx 0.4472136 + 0.3240370 + 0.1732051 + 0.05 = 0.9944557\n$$\n像素 1 的Hellinger距离为：\n$$\nH^{(1)} = \\sqrt{1 - BC^{(1)}} \\approx \\sqrt{1 - 0.9944557} = \\sqrt{0.0055443} \\approx 0.0744600\n$$\n\n**对于像素 2：**\n$p^{(2)} = [0.05, 0.25, 0.60, 0.10]$\n$q^{(2)} = [0.10, 0.20, 0.65, 0.05]$\n$w^{(2)} = 0.3$\n\nBhattacharyya系数为：\n$$\nBC^{(2)} = \\sqrt{0.05 \\times 0.10} + \\sqrt{0.25 \\times 0.20} + \\sqrt{0.60 \\times 0.65} + \\sqrt{0.10 \\times 0.05}\n$$\n$$\nBC^{(2)} = \\sqrt{0.005} + \\sqrt{0.05} + \\sqrt{0.39} + \\sqrt{0.005}\n$$\n$$\nBC^{(2)} \\approx 0.0707107 + 0.2236068 + 0.6244998 + 0.0707107 = 0.9895280\n$$\n像素 2 的Hellinger距离为：\n$$\nH^{(2)} = \\sqrt{1 - BC^{(2)}} \\approx \\sqrt{1 - 0.9895280} = \\sqrt{0.0104720} \\approx 0.1023330\n$$\n\n**对于像素 3：**\n$p^{(3)} = [0.20, 0.50, 0.25, 0.05]$\n$q^{(3)} = [0.25, 0.45, 0.25, 0.05]$\n$w^{(3)} = 0.2$\n\nBhattacharyya系数为：\n$$\nBC^{(3)} = \\sqrt{0.20 \\times 0.25} + \\sqrt{0.50 \\times 0.45} + \\sqrt{0.25 \\times 0.25} + \\sqrt{0.05 \\times 0.05}\n$$\n$$\nBC^{(3)} = \\sqrt{0.05} + \\sqrt{0.225} + 0.25 + 0.05\n$$\n$$\nBC^{(3)} \\approx 0.2236068 + 0.4743416 + 0.25 + 0.05 = 0.9979484\n$$\n像素 3 的Hellinger距离为：\n$$\nH^{(3)} = \\sqrt{1 - BC^{(3)}} \\approx \\sqrt{1 - 0.9979484} = \\sqrt{0.0020516} \\approx 0.0452946\n$$\n\n**最终聚合：**\n现在，我们使用权重 $w^{(1)}=0.5$, $w^{(2)}=0.3$ 和 $w^{(3)}=0.2$ 计算聚合的Hellinger距离。\n$$\nH_{agg} = w^{(1)} H^{(1)} + w^{(2)} H^{(2)} + w^{(3)} H^{(3)}\n$$\n在中间计算中使用更精确的值：\n$H^{(1)} \\approx 0.0744600047$\n$H^{(2)} \\approx 0.1023329969$\n$H^{(3)} \\approx 0.0452940756$\n$$\nH_{agg} \\approx (0.5 \\times 0.0744600047) + (0.3 \\times 0.1023329969) + (0.2 \\times 0.0452940756)\n$$\n$$\nH_{agg} \\approx 0.0372300024 + 0.0306998991 + 0.0090588151\n$$\n$$\nH_{agg} \\approx 0.0769887166\n$$\n问题要求将最终答案四舍五入到四位有效数字。第一位有效数字是 $7$，其后是 $6$、$9$ 和 $8$。第五位有效数字是 $8$，即 $\\ge 5$，因此我们将第四位有效数字向上取整。数字 $0.07698$ 变为 $0.07699$。\n$$\nH_{agg} \\approx 0.07699\n$$",
            "answer": "$$\n\\boxed{0.07699}\n$$"
        },
        {
            "introduction": "在环境建模中，地物类别通常存在层级结构（例如，“森林”是“植被”的子类）。一个先进的分类模型其输出应当尊重这种逻辑关系。本练习将带你深入模型训练的核心，通过设计一个结构化损失函数，将类别分类学知识直接嵌入到神经网络的优化过程中，从而确保模型生成不仅准确而且逻辑自洽的软标签。",
            "id": "3814912",
            "problem": "考虑一个用于土地覆盖制图的卫星影像中的多光谱像素，其分类属于一个层级分类体系。该像素的类别成员资格被建模为模糊软标签，这与遥感中常见的混合效应一致，即单个像素可能部分属于多个类别。设该分类体系为一棵树，具有一个父类别“植被”（$\\text{Vegetation}$），以及两个子类别“森林”（$\\text{Forest}$）和“草地”（$\\text{Grassland}$）。将模型预测的软成员资格表示为向量 $y = (y_{\\text{veg}}, y_{\\text{for}}, y_{\\text{gra}}) \\in [0,1]^3$，其中 $y_{\\text{veg}}$ 是“植被”的成员资格，$y_{\\text{for}}$ 是“森林”的成员资格，$y_{\\text{gra}}$ 是“草地”的成员资格。假设预测来自一个卷积神经网络（CNN），该网络使用从专家标注的亚像素比例中导出的软标签进行训练。\n\n层级模糊集的基本约束要求“子类 $\\leq$ 父类”的单调性，并且在假设同级子类是其父类别内覆盖不相交部分的互斥子类型的前提下，要求“同级子类之和 $\\leq$ 父类”。从这些原则出发，构建一个结构化损失函数，该函数使用一个仅在约束被违反时才激活的凸惩罚项，来惩罚对这些层级约束的违反。推导此结构化损失相对于软输出 $y$ 的梯度。\n\n然后，对于具体数值案例 $y = (0.55, 0.60, 0.10)$，边惩罚权重 $\\lambda_{\\text{edge}} = 1.2$，和惩罚权重 $\\lambda_{\\text{sum}} = 2.5$，计算结构化损失相对于 $(y_{\\text{veg}}, y_{\\text{for}}, y_{\\text{gra}})$ 的梯度。\n\n讨论在遥感中的类别分类体系下，此结构化惩罚在训练过程中如何与基础分类损失相互作用，包括对稳定性、收敛性以及软标签影响的考量。\n\n将计算出的梯度四舍五入至四位有效数字。将最终的梯度向量表示为一行矩阵，并且在最终报告的答案中不要包含单位。",
            "solution": "本問題要求在模糊分類設置下，構建一個結構化損失函數以強制層級一致性，推導其梯度，對該梯度進行具體的數值計算，並討論其影響。\n\n本問題經核實具有科學依據、定義明確、客觀且一致。它描述了機器學習在遙感應用中的一種標準且重要的技術。\n\n### 第1部分：結構化損失的構建\n\n層級分類體系包含一個父類別，植被（veg），以及兩個子類別，森林（for）和草地（gra）。預測的軟成員資格由向量 $y = (y_{\\text{veg}}, y_{\\text{for}}, y_{\\text{gra}})$ 給出。\n\n需要強制執行的約束是：\n1.  **子類-父類單調性**：子類別的成員資格不能超過其父類別的成員資格。\n    - $y_{\\text{for}} \\leq y_{\\text{veg}}$\n    - $y_{\\text{gra}} \\leq y_{\\text{veg}}$\n2.  **同級子類求和約束**：假設子類別代表互斥的子類型，它們的成員資格之和不能超過父類別的成員資格。\n    - $y_{\\text{for}} + y_{\\text{gra}} \\leq y_{\\text{veg}}$\n\n我們需要構建一個損失函數 $L_{struct}$，來懲罰對這些約束的違反。該懲罰必須是凸的，並且僅在約束被違反時才激活。一個合適的選擇是平方鉸鏈損失（squared hinge loss），它既是凸的又是連續可微的。對於一個約束 $g(y) \\leq 0$，平方鉸鏈損失的一般形式為 $\\max(0, g(y))^2$。\n\n違反情況可以表示為：\n- $y_{\\text{for}} - y_{\\text{veg}} > 0$\n- $y_{\\text{gra}} - y_{\\text{veg}} > 0$\n- $y_{\\text{for}} + y_{\\text{gra}} - y_{\\text{veg}} > 0$\n\n問題指定了不同的懲罰權重，$\\lambda_{\\text{edge}}$ 用於子類-父類（邊）約束，$\\lambda_{\\text{sum}}$ 用於同級子類求和約束。這意味著我們應該分別懲罰每種類型的違反。\n\n結構化損失 $L_{struct}$ 被構建為對每種約束違反的懲罰的加權和：\n$$\nL_{struct}(y) = \\lambda_{\\text{edge}} \\left[ \\max(0, y_{\\text{for}} - y_{\\text{veg}})^2 + \\max(0, y_{\\text{gra}} - y_{\\text{veg}})^2 \\right] + \\lambda_{\\text{sum}} \\max(0, y_{\\text{for}} + y_{\\text{gra}} - y_{\\text{veg}})^2\n$$\n如果所有約束都得到滿足，此損失函數為零；若有任何違反，損失會隨違反程度的量級呈二次方增長，這滿足了問題的要求。\n\n### 第2部分：梯度的推導\n\n為了推導 $L_{struct}$ 相對於輸出向量 $y = (y_{\\text{veg}}, y_{\\text{for}}, y_{\\text{gra}})$ 的梯度，我們需要計算偏導數 $\\frac{\\partial L_{struct}}{\\partial y_{\\text{veg}}}$，$\\frac{\\partial L_{struct}}{\\partial y_{\\text{for}}}$ 和 $\\frac{\\partial L_{struct}}{\\partial y_{\\text{gra}}}$。\n\n我們使用鏈式法則。$\\max(0, z)^2$ 相對於 $z$ 的導數是 $2 \\cdot \\max(0, z)$。\n\n**相對於 $y_{\\text{veg}}$ 的偏導數：**\n$y_{\\text{veg}}$ 出現在所有三個懲罰項中，其係數均為 $-1$。\n$$\n\\frac{\\partial L_{struct}}{\\partial y_{\\text{veg}}} = \\lambda_{\\text{edge}} \\left[ 2\\max(0, y_{\\text{for}} - y_{\\text{veg}}) \\cdot (-1) + 2\\max(0, y_{\\text{gra}} - y_{\\text{veg}}) \\cdot (-1) \\right] + \\lambda_{\\text{sum}} \\left[ 2\\max(0, y_{\\text{for}} + y_{\\text{gra}} - y_{\\text{veg}}) \\cdot (-1) \\right]\n$$\n$$\n\\frac{\\partial L_{struct}}{\\partial y_{\\text{veg}}} = -2\\lambda_{\\text{edge}} \\left[ \\max(0, y_{\\text{for}} - y_{\\text{veg}}) + \\max(0, y_{\\text{gra}} - y_{\\text{veg}}) \\right] - 2\\lambda_{\\text{sum}} \\max(0, y_{\\text{for}} + y_{\\text{gra}} - y_{\\text{veg}})\n$$\n\n**相對於 $y_{\\text{for}}$ 的偏導數：**\n$y_{\\text{for}}$ 出現在第一個邊約束項和求和約束項中，其係數均為 $+1$。\n$$\n\\frac{\\partial L_{struct}}{\\partial y_{\\text{for}}} = \\lambda_{\\text{edge}} \\left[ 2\\max(0, y_{\\text{for}} - y_{\\text{veg}}) \\cdot (1) \\right] + \\lambda_{\\text{sum}} \\left[ 2\\max(0, y_{\\text{for}} + y_{\\text{gra}} - y_{\\text{veg}}) \\cdot (1) \\right]\n$$\n$$\n\\frac{\\partial L_{struct}}{\\partial y_{\\text{for}}} = 2\\lambda_{\\text{edge}} \\max(0, y_{\\text{for}} - y_{\\text{veg}}) + 2\\lambda_{\\text{sum}} \\max(0, y_{\\text{for}} + y_{\\text{gra}} - y_{\\text{veg}})\n$$\n\n**相對於 $y_{\\text{gra}}$ 的偏導數：**\n$y_{\\text{gra}}$ 出現在第二個邊約束項和求和約束項中，其係數均為 $+1$。\n$$\n\\frac{\\partial L_{struct}}{\\partial y_{\\text{gra}}} = \\lambda_{\\text{edge}} \\left[ 2\\max(0, y_{\\text{gra}} - y_{\\text{veg}}) \\cdot (1) \\right] + \\lambda_{\\text{sum}} \\left[ 2\\max(0, y_{\\text{for}} + y_{\\text{gra}} - y_{\\text{veg}}) \\cdot (1) \\right]\n$$\n$$\n\\frac{\\partial L_{struct}}{\\partial y_{\\text{gra}}} = 2\\lambda_{\\text{edge}} \\max(0, y_{\\text{gra}} - y_{\\text{veg}}) + 2\\lambda_{\\text{sum}} \\max(0, y_{\\text{for}} + y_{\\text{gra}} - y_{\\text{veg}})\n$$\n\n梯度向量為 $\\nabla_y L_{struct} = \\left( \\frac{\\partial L_{struct}}{\\partial y_{\\text{veg}}}, \\frac{\\partial L_{struct}}{\\partial y_{\\text{for}}}, \\frac{\\partial L_{struct}}{\\partial y_{\\text{gra}}} \\right)$。\n\n### 第3部分：梯度的數值計算\n\n給定的具體案例為：\n- $y = (y_{\\text{veg}}, y_{\\text{for}}, y_{\\text{gra}}) = (0.55, 0.60, 0.10)$\n- $\\lambda_{\\text{edge}} = 1.2$\n- $\\lambda_{\\text{sum}} = 2.5$\n\n首先，我們通過計算 $\\max$ 函數的參數來檢查哪些約束被違反了：\n- $y_{\\text{for}} - y_{\\text{veg}} = 0.60 - 0.55 = 0.05$。（違反）\n- $y_{\\text{gra}} - y_{\\text{veg}} = 0.10 - 0.55 = -0.45$。（未違反）\n- $y_{\\text{for}} + y_{\\text{gra}} - y_{\\text{veg}} = 0.60 + 0.10 - 0.55 = 0.70 - 0.55 = 0.15$。（違反）\n\n現在，我們計算在梯度計算中將被激活的 $\\max$ 項：\n- $\\max(0, 0.05) = 0.05$\n- $\\max(0, -0.45) = 0$\n- $\\max(0, 0.15) = 0.15$\n\n使用這些值，我們計算梯度的各個分量：\n- $\\frac{\\partial L_{struct}}{\\partial y_{\\text{veg}}} = -2(1.2)[0.05 + 0] - 2(2.5)(0.15) = -2.4(0.05) - 5.0(0.15) = -0.12 - 0.75 = -0.87$\n- $\\frac{\\partial L_{struct}}{\\partial y_{\\text{for}}} = 2(1.2)(0.05) + 2(2.5)(0.15) = 2.4(0.05) + 5.0(0.15) = 0.12 + 0.75 = 0.87$\n- $\\frac{\\partial L_{struct}}{\\partial y_{\\text{gra}}} = 2(1.2)(0) + 2(2.5)(0.15) = 0 + 5.0(0.15) = 0.75$\n\n梯度向量為 $\\nabla_y L_{struct} = (-0.87, 0.87, 0.75)$。\n將每個分量四捨五入到四位有效數字，得到 $(-0.8700, 0.8700, 0.7500)$。\n\n### 第4部分：討論\n\n用於訓練神經網絡的總損失函數是 $L_{total} = L_{base} + L_{struct}$，其中 $L_{base}$ 是一個基礎分類損失（例如，對於軟標籤的均方誤差或KL散度），它衡量預測 $y$ 與真實值軟標籤 $t$ 之間的差異。\n\n**與基礎損失的相互作用**：$L_{struct}$ 作為一個正則化項。當 $L_{base}$ 驅動模型的輸出 $y$ 去匹配目標標籤 $t$ 時，$L_{struct}$ 則懲罰那些與預定義類別層級邏輯上不一致的預測。$L_{total}$ 的梯度是這兩項梯度之和。因此，在反向傳播過程中對網絡權重的更新是來自 $L_{base}$ 的「數據擬合」信號和來自 $L_{struct}$ 的「一致性強制」信號的組合。上面計算的梯度顯示了這種一致性信號將如何調整輸出：它會推高 $y_{\\text{veg}}$ 並拉低 $y_{\\text{for}}$ 和 $y_{\\text{gra}}$ 以解決違反約束的問題。\n\n**穩定性與收斂性**：增加凸的 $L_{struct}$ 項不會損害損失相對於最後一層輸出的凸性（假設 $L_{base}$ 是凸的），這對於最終的優化步驟是一個理想的屬性。然而，在端到端的訓練中，總損失景觀相對於網絡權重仍然是高度非凸的。懲罰梯度僅在違反約束時才被激活，這可能導致稀疏但可能很大的梯度，尤其是在訓練初期。懲罰權重 $\\lambda_{\\text{edge}}$ 和 $\\lambda_{\\text{sum}}$ 必須仔細調整，以平衡兩個損失分量並確保訓練穩定。如果 $\\lambda$ 值過高，它們可能會壓倒基礎損失，阻礙模型擬合數據的能力。通過編碼關於問題結構的強先驗知識，該懲罰可以正則化模型，防止過擬合，並引導優化朝向更有意義的解決方案，从而可能加速收斂到一個實際有用的結果。\n\n**軟標籤和遙感背景的影響**：在遙感中，真實值軟標籤 $t$ 通常代表土地覆蓋類型的亞像素比例。理想情況下，這些標籤本身應該是層級一致的（例如，$t_{\\text{for}} + t_{\\text{gra}} \\le t_{\\text{veg}}$）。如果是這樣，隨著模型學習預測 $y \\approx t$，結構化懲罰 $L_{struct}$ 將自然地減小至零。在這種情況下，懲罰項的作用是引導學習過程朝向這個一致的狀態。如果真實值標籤存在噪聲並包含不一致性，$L_{struct}$ 將與 $L_{base}$ 發生衝突。模型將會找到一個受懲罰項相對強度影響的折衷解。在這種情況下，$L_{struct}$ 實際上執行了一種標籤噪聲校正，將已知的結構邏輯強加於可能存在缺陷的數據之上。這對於生成物理上和語義上都合理的土地覆蓋圖至關重要，而這是環境建模的一個主要目標。",
            "answer": "$$\n\\boxed{\\begin{pmatrix} -0.8700  0.8700  0.7500 \\end{pmatrix}}\n$$"
        }
    ]
}