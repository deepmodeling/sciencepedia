{
    "hands_on_practices": [
        {
            "introduction": "Large-scale environmental models often require single effective parameters to represent physically complex, heterogeneous systems like layered soils. This practice demonstrates a foundational principle of upscaling: the correct averaging method is dictated by the underlying physics, not simple arithmetic. By deriving the effective hydraulic conductivity for vertical flow through stratified soil, you will discover the importance of the harmonic mean and its physical interpretation as a system of series resistances .",
            "id": "3844282",
            "problem": "A one-dimensional saturated soil column represents the vertical component of a land surface model grid cell for a hydrologic simulation. The column comprises $N$ horizontally extensive layers, each with thickness $\\Delta z_{i}$ and intrinsic saturated hydraulic conductivity $K_{i}$, where $i \\in \\{1,\\dots,N\\}$. The top and bottom boundaries of the column are held at fixed total hydraulic heads, establishing a constant total head drop $\\Delta h_{\\text{tot}}$ across the total thickness $L = \\sum_{i=1}^{N} \\Delta z_{i}$. Assume steady-state, isothermal, incompressible, one-dimensional flow; the volumetric flux $q$ is spatially uniform and mass conservation holds. The effective saturated hydraulic conductivity $K_{\\text{eff}}$ of the entire column is defined by the relation $q = -K_{\\text{eff}} \\, (\\Delta h_{\\text{tot}}/L)$, meaning $K_{\\text{eff}}$ is the single-parameter representation that reproduces the same flux under the same boundary head drop as the layered system. \n\nStarting only from the fundamental definitions and laws appropriate to this context—specifically that for each layer the Darcy flux satisfies $q = -K_{i} \\, (\\Delta h_{i}/\\Delta z_{i})$ and the total head drop satisfies $\\Delta h_{\\text{tot}} = \\sum_{i=1}^{N} \\Delta h_{i}$—derive an analytic expression for $K_{\\text{eff}}$ in terms of $\\{\\Delta z_{i}, K_{i}\\}_{i=1}^{N}$ and explain, in physical terms, why this expression embodies the concept of series resistance to flow. Then, apply your expression to the following scientifically plausible layered profile, representative of a remotely sensed soil stratigraphy aggregated to a model grid cell:\n\n- Layer $1$: $\\Delta z_{1} = 0.4 \\, \\text{m}$, $K_{1} = 1.5 \\times 10^{-5} \\, \\text{m} \\, \\text{s}^{-1}$.\n- Layer $2$: $\\Delta z_{2} = 0.7 \\, \\text{m}$, $K_{2} = 3.0 \\times 10^{-6} \\, \\text{m} \\, \\text{s}^{-1}$.\n- Layer $3$: $\\Delta z_{3} = 0.3 \\, \\text{m}$, $K_{3} = 8.0 \\times 10^{-5} \\, \\text{m} \\, \\text{s}^{-}^{-1}$.\n\nCompute the resulting $K_{\\text{eff}}$ for this column. Round your final numeric value to four significant figures and express it in $\\text{m} \\, \\text{s}^{-1}$. Your final answer must be a single real-valued number.",
            "solution": "The problem is valid as it is scientifically grounded in the principles of fluid dynamics in porous media (Darcy's Law), is well-posed with all necessary information provided, and is expressed in objective, formal language. We may proceed with a solution.\n\nThe objective is to derive an expression for the effective saturated hydraulic conductivity, $K_{\\text{eff}}$, for a series of $N$ soil layers and then apply it to a specific case.\n\n**Part 1: Derivation of the analytic expression for $K_{\\text{eff}}$**\n\nWe are given the governing equations for steady-state, one-dimensional flow through the layered system.\nFor each layer $i \\in \\{1,\\dots,N\\}$, Darcy's Law relates the volumetric flux $q$ to the head drop $\\Delta h_{i}$ across the layer of thickness $\\Delta z_{i}$ with saturated hydraulic conductivity $K_{i}$:\n$$q = -K_{i} \\frac{\\Delta h_{i}}{\\Delta z_{i}} \\quad (1)$$\nDue to the assumption of steady-state, incompressible flow, the flux $q$ must be constant through all layers.\n\nThe total head drop across the entire column, $\\Delta h_{\\text{tot}}$, is the sum of the head drops across the individual layers:\n$$\\Delta h_{\\text{tot}} = \\sum_{i=1}^{N} \\Delta h_{i} \\quad (2)$$\n\nThe effective hydraulic conductivity, $K_{\\text{eff}}$, is defined for the entire column of total thickness $L = \\sum_{i=1}^{N} \\Delta z_{i}$ as:\n$$q = -K_{\\text{eff}} \\frac{\\Delta h_{\\text{tot}}}{L} \\quad (3)$$\n\nOur goal is to find an expression for $K_{\\text{eff}}$ in terms of $\\{ \\Delta z_i, K_i \\}$. To do this, we must relate the total head drop $\\Delta h_{\\text{tot}}$ to the constant flux $q$ using the properties of the individual layers.\n\nFirst, we rearrange equation $(1)$ to solve for the head drop in each layer, $\\Delta h_{i}$:\n$$\\Delta h_{i} = -q \\frac{\\Delta z_{i}}{K_{i}}$$\nNext, we substitute this expression for $\\Delta h_{i}$ into the summation for the total head drop, equation $(2)$:\n$$\\Delta h_{\\text{tot}} = \\sum_{i=1}^{N} \\left( -q \\frac{\\Delta z_{i}}{K_{i}} \\right)$$\nSince the flux $q$ is constant for all layers, it can be factored out of the summation:\n$$\\Delta h_{\\text{tot}} = -q \\sum_{i=1}^{N} \\frac{\\Delta z_{i}}{K_{i}} \\quad (4)$$\nThis equation now relates the total head drop to the flux and the properties of the layered system. We can rearrange it to express $q$:\n$$q = -\\frac{\\Delta h_{\\text{tot}}}{\\sum_{i=1}^{N} \\frac{\\Delta z_{i}}{K_{i}}}$$\nNow, we equate this expression for $q$ with the definition of the effective system from equation $(3)$:\n$$-K_{\\text{eff}} \\frac{\\Delta h_{\\text{tot}}}{L} = -\\frac{\\Delta h_{\\text{tot}}}{\\sum_{i=1}^{N} \\frac{\\Delta z_{i}}{K_{i}}}$$\nAssuming a non-zero head drop ($\\Delta h_{\\text{tot}} \\neq 0$), which is required for flow to occur, we can cancel the term $-\\Delta h_{\\text{tot}}$ from both sides:\n$$\\frac{K_{\\text{eff}}}{L} = \\frac{1}{\\sum_{i=1}^{N} \\frac{\\Delta z_{i}}{K_{i}}}$$\nSolving for $K_{\\text{eff}}$ and substituting the definition of the total thickness $L = \\sum_{i=1}^{N} \\Delta z_{i}$, we obtain the final expression:\n$$K_{\\text{eff}} = \\frac{L}{\\sum_{i=1}^{N} \\frac{\\Delta z_{i}}{K_{i}}} = \\frac{\\sum_{i=1}^{N} \\Delta z_{i}}{\\sum_{i=1}^{N} \\frac{\\Delta z_{i}}{K_{i}}}$$\nThis expression represents the thickness-weighted harmonic mean of the individual layer conductivities.\n\n**Part 2: Physical Interpretation as Series Resistance**\n\nThe derived expression for $K_{\\text{eff}}$ embodies the concept of series resistance. This can be understood through an analogy with electrical circuits. Ohm's Law states that current $I$ is the ratio of voltage drop $\\Delta V$ to resistance $R$, i.e., $I = \\Delta V/R$.\n\nDarcy's Law, as written in equation $(1)$, can be rearranged as:\n$$q = \\frac{-\\Delta h_{i}}{\\left(\\frac{\\Delta z_{i}}{K_{i}}\\right)}$$\nBy analogy:\n- The volumetric flux $q$ is analogous to electrical current $I$.\n- The negative of the head drop, $-\\Delta h_{i}$, is the driving potential, analogous to voltage drop $\\Delta V$.\n- The quantity $R_{h,i} = \\frac{\\Delta z_{i}}{K_{i}}$ is the hydraulic resistance of layer $i$, analogous to electrical resistance $R$.\n\nThe condition of constant flux $q$ through all layers is analogous to a constant current flowing through resistors in series. The condition that the total head drop is the sum of individual head drops ($\\Delta h_{\\text{tot}} = \\sum \\Delta h_{i}$) is analogous to the total voltage drop being the sum of individual voltage drops across series resistors ($\\Delta V_{\\text{tot}} = \\sum \\Delta V_{i}$).\n\nFrom equation $(4)$, we have $-\\Delta h_{\\text{tot}} = q \\sum_{i=1}^{N} \\frac{\\Delta z_{i}}{K_{i}}$. Substituting the definition of hydraulic resistance, we get:\n$$-\\Delta h_{\\text{tot}} = q \\sum_{i=1}^{N} R_{h,i}$$\nThe total hydraulic resistance of the column is $R_{h,\\text{tot}} = \\sum_{i=1}^{N} R_{h,i}$. This is the defining characteristic of a series system: the total resistance is the sum of the individual resistances. The water must flow sequentially through each layer, and each layer contributes to the total opposition to flow.\n\nFor the effective system, the total resistance is $R_{h,\\text{eff}} = \\frac{L}{K_{\\text{eff}}}$. Equating the total resistance representations, we have:\n$$\\frac{L}{K_{\\text{eff}}} = \\sum_{i=1}^{N} R_{h,i} = \\sum_{i=1}^{N} \\frac{\\Delta z_{i}}{K_{i}}$$\nThis directly yields the derived formula for $K_{\\text{eff}}$. Physically, this means that layers with low conductivity (high resistance) dominate the overall flow characteristics, significantly lowering the effective conductivity of the entire column, a known property of the harmonic mean.\n\n**Part 3: Numerical Calculation**\n\nWe apply the derived formula to the given $3$-layer system.\nThe data are:\n- Layer $1$: $\\Delta z_{1} = 0.4 \\, \\text{m}$, $K_{1} = 1.5 \\times 10^{-5} \\, \\text{m} \\, \\text{s}^{-1}$.\n- Layer $2$: $\\Delta z_{2} = 0.7 \\, \\text{m}$, $K_{2} = 3.0 \\times 10^{-6} \\, \\text{m} \\, \\text{s}^{-1}$.\n- Layer $3$: $\\Delta z_{3} = 0.3 \\, \\text{m}$, $K_{3} = 8.0 \\times 10^{-5} \\, \\text{m} \\, \\text{s}^{-1}$.\n\nFirst, we calculate the total thickness $L$:\n$$L = \\Delta z_{1} + \\Delta z_{2} + \\Delta z_{3} = 0.4 \\, \\text{m} + 0.7 \\, \\text{m} + 0.3 \\, \\text{m} = 1.4 \\, \\text{m}$$\n\nNext, we calculate the sum of the hydraulic resistances in the denominator:\n$$\\sum_{i=1}^{3} \\frac{\\Delta z_{i}}{K_{i}} = \\frac{\\Delta z_{1}}{K_{1}} + \\frac{\\Delta z_{2}}{K_{2}} + \\frac{\\Delta z_{3}}{K_{3}}$$\n$$\\sum_{i=1}^{3} \\frac{\\Delta z_{i}}{K_{i}} = \\frac{0.4}{1.5 \\times 10^{-5}} + \\frac{0.7}{3.0 \\times 10^{-6}} + \\frac{0.3}{8.0 \\times 10^{-5}} \\, \\text{s}$$\nCalculating each term:\n- $R_{h,1} = \\frac{0.4}{1.5 \\times 10^{-5}} = 26666.\\bar{6} \\, \\text{s}$\n- $R_{h,2} = \\frac{0.7}{3.0 \\times 10^{-6}} = 233333.\\bar{3} \\, \\text{s}$\n- $R_{h,3} = \\frac{0.3}{8.0 \\times 10^{-5}} = 3750 \\, \\text{s}$\n\nThe sum of the resistances is:\n$$\\sum_{i=1}^{3} \\frac{\\Delta z_{i}}{K_{i}} = 26666.\\bar{6} + 233333.\\bar{3} + 3750 = 263750 \\, \\text{s}$$\nNow, we compute $K_{\\text{eff}}$:\n$$K_{\\text{eff}} = \\frac{L}{\\sum_{i=1}^{3} \\frac{\\Delta z_{i}}{K_{i}}} = \\frac{1.4 \\, \\text{m}}{263750 \\, \\text{s}}$$\n$$K_{\\text{eff}} \\approx 5.30803791... \\times 10^{-6} \\, \\text{m} \\, \\text{s}^{-1}$$\nThe problem requires rounding the final value to four significant figures.\n$$K_{\\text{eff}} \\approx 5.308 \\times 10^{-6} \\, \\text{m} \\, \\text{s}^{-1}$$",
            "answer": "$$\\boxed{5.308 \\times 10^{-6}}$$"
        },
        {
            "introduction": "Environmental processes are frequently nonlinear, meaning their response to averaged environmental conditions is not the same as the average of their responses to variable conditions. This discrepancy, a manifestation of Jensen's inequality, is a primary source of scaling error in models. This exercise introduces a versatile and widely used tool—a second-order Taylor series approximation—to quantify this bias, providing a practical method to account for sub-grid heterogeneity when an exact analytical solution is unavailable .",
            "id": "3844255",
            "problem": "Consider a coarse-resolution satellite pixel covering a heterogeneous forest stand. The within-pixel variability of the Leaf Area Index (LAI) is modeled as a random variable $X$ representing the fine-scale LAI sampled across the pixel. The canopy gap fraction (transmittance) at a location with LAI $x$ is modeled by the Beer–Lambert law $f(x) = \\exp(-k x)$, where $k$ is the extinction coefficient that depends on leaf angle distribution and wavelength. Assume statistical stationarity across the pixel so that the spatial average equals the ensemble expectation.\n\nStarting from the definitions of expectation, mean, and variance, and from the Taylor expansion of a sufficiently smooth function in a neighborhood of the mean, derive a second-order moment closure that approximates $\\mathbb{E}[f(X)]$ in terms of the mean $\\mu = \\mathbb{E}[X]$, the variance $\\sigma^{2} = \\mathbb{V}\\mathrm{ar}(X)$, and derivatives of $f$ evaluated at $\\mu$. State the regularity and statistical conditions on $f$ and on the distribution of $X$ under which this approximation is accurate when upscaling nonlinear responses in environmental systems.\n\nThen, apply your approximation to estimate the areal average gap fraction for parameters $k = 0.6$, $\\mu = 3.0$, and $\\sigma = 0.4$ (assume $X$ is approximately Gaussian so that the third central moment is negligible). Round your final numerical answer to four significant figures and express it as a dimensionless quantity.",
            "solution": "The problem requires the derivation of a second-order moment closure approximation for the expectation of a nonlinear function of a random variable, an analysis of the conditions for its validity, and its application to a specific problem in environmental remote sensing.\n\nFirst, we derive the general second-order approximation. Let $X$ be a random variable with mean $\\mu = \\mathbb{E}[X]$ and variance $\\sigma^{2} = \\mathbb{V}\\mathrm{ar}(X) = \\mathbb{E}[(X-\\mu)^{2}]$. Let $f(x)$ be a function that is at least twice continuously differentiable in a neighborhood of $\\mu$. The Taylor series expansion of $f(x)$ around the point $x=\\mu$ is given by:\n$$f(x) = f(\\mu) + f'(\\mu)(x-\\mu) + \\frac{f''(\\mu)}{2!}(x-\\mu)^{2} + \\frac{f'''(\\mu)}{3!}(x-\\mu)^{3} + \\dots$$\nWe can write this using a random variable $X$ instead of the deterministic variable $x$:\n$$f(X) = f(\\mu) + f'(\\mu)(X-\\mu) + \\frac{f''(\\mu)}{2}(X-\\mu)^{2} + R_{2}$$\nwhere $R_{2}$ is the remainder term, which contains all terms of order three and higher.\n\nTo find the expected value of $f(X)$, we take the expectation of both sides of the expansion. By the linearity of the expectation operator $\\mathbb{E}$:\n$$\\mathbb{E}[f(X)] = \\mathbb{E}\\left[f(\\mu) + f'(\\mu)(X-\\mu) + \\frac{f''(\\mu)}{2}(X-\\mu)^{2} + R_{2}\\right]$$\n$$\\mathbb{E}[f(X)] = \\mathbb{E}[f(\\mu)] + \\mathbb{E}[f'(\\mu)(X-\\mu)] + \\mathbb{E}\\left[\\frac{f''(\\mu)}{2}(X-\\mu)^{2}\\right] + \\mathbb{E}[R_{2}]$$\nThe terms $f(\\mu)$, $f'(\\mu)$, and $f''(\\mu)$ are constants because they are evaluated at the constant mean $\\mu$. Therefore, they can be factored out of the expectations:\n$$\\mathbb{E}[f(X)] = f(\\mu) + f'(\\mu)\\mathbb{E}[X-\\mu] + \\frac{f''(\\mu)}{2}\\mathbb{E}[(X-\\mu)^{2}] + \\mathbb{E}[R_{2}]$$\nBy definition, the first central moment is $\\mathbb{E}[X-\\mu] = \\mathbb{E}[X] - \\mu = \\mu - \\mu = 0$.\nThe second central moment is the variance, $\\mathbb{E}[(X-\\mu)^{2}] = \\sigma^{2}$.\nSubstituting these definitions into the equation yields:\n$$\\mathbb{E}[f(X)] = f(\\mu) + f'(\\mu)(0) + \\frac{f''(\\mu)}{2}\\sigma^{2} + \\mathbb{E}[R_{2}]$$\n$$\\mathbb{E}[f(X)] = f(\\mu) + \\frac{1}{2}f''(\\mu)\\sigma^{2} + \\mathbb{E}[R_{2}]$$\nTruncating the series at the second order (i.e., assuming $\\mathbb{E}[R_{2}]$ is negligible) gives the second-order moment closure approximation:\n$$\\mathbb{E}[f(X)] \\approx f(\\mu) + \\frac{1}{2}f''(\\mu)\\sigma^{2}$$\n\nNext, we state the conditions under which this approximation is accurate.\n1.  Regularity conditions on the function $f$: The function $f(x)$ must be sufficiently smooth, meaning it should be at least twice continuously differentiable (of class $C^2$) over the range where the probability mass of $X$ is significant. The accuracy of the approximation depends on the magnitude of the remainder term $\\mathbb{E}[R_{2}]$, which is determined by higher-order derivatives of $f$. The approximation is more accurate if the function is only weakly nonlinear, meaning its third and higher derivatives are small in magnitude near $\\mu$.\n2.  Statistical conditions on the random variable $X$: The Taylor expansion is an approximation centered at the mean $\\mu$. It is most accurate for values of $X$ close to $\\mu$. This implies that the variance $\\sigma^{2}$ must be small. Additionally, the remainder term $\\mathbb{E}[R_{2}]$ involves higher-order central moments of $X$, such as skewness ($\\mathbb{E}[(X-\\mu)^3]$) and kurtosis. For the second-order approximation to be accurate, these higher-order moments must be small, or the corresponding higher-order derivatives of $f$ must be small. A distribution with low variance and low skewness (i.e., it is relatively symmetric and concentrated around its mean) is ideal.\n\nNow, we apply this approximation to the specific problem. The function is the canopy gap fraction, $f(x) = \\exp(-k x)$, where $x$ is the LAI. We need its first two derivatives:\nFirst derivative: $f'(x) = \\frac{d}{dx}\\exp(-kx) = -k\\exp(-kx)$.\nSecond derivative: $f''(x) = \\frac{d}{dx}(-k\\exp(-kx)) = (-k)(-k)\\exp(-kx) = k^{2}\\exp(-kx)$.\n\nWe evaluate the function and its second derivative at the mean LAI, $\\mu$:\n$f(\\mu) = \\exp(-k\\mu)$\n$f''(\\mu) = k^{2}\\exp(-k\\mu)$\n\nSubstitute these expressions into the second-order approximation formula:\n$$\\mathbb{E}[f(X)] \\approx \\exp(-k\\mu) + \\frac{1}{2} (k^{2}\\exp(-k\\mu)) \\sigma^{2}$$\nWe can factor out the term $\\exp(-k\\mu)$:\n$$\\mathbb{E}[f(X)] \\approx \\exp(-k\\mu) \\left(1 + \\frac{1}{2}k^{2}\\sigma^{2}\\right)$$\nThis is the specific form of the approximation for the Beer-Lambert law. The problem states that the third central moment is negligible, which justifies the truncation at the second order.\n\nFinally, we substitute the given numerical values: $k = 0.6$, $\\mu = 3.0$, and $\\sigma = 0.4$. The variance is $\\sigma^{2} = (0.4)^{2} = 0.16$.\nFirst, calculate the product $k\\mu$:\n$$k\\mu = 0.6 \\times 3.0 = 1.8$$\nNext, calculate the correction term inside the parentheses:\n$$\\frac{1}{2}k^{2}\\sigma^{2} = \\frac{1}{2}(0.6)^{2}(0.4)^{2} = \\frac{1}{2}(0.36)(0.16) = \\frac{1}{2}(0.0576) = 0.0288$$\nNow substitute these values back into the approximation for the areal average gap fraction:\n$$\\mathbb{E}[f(X)] \\approx \\exp(-1.8) (1 + 0.0288) = \\exp(-1.8) \\times 1.0288$$\nUsing a calculator for the value of the exponential:\n$$\\exp(-1.8) \\approx 0.16529888$$\n$$\\mathbb{E}[f(X)] \\approx 0.16529888 \\times 1.0288 \\approx 0.1700684$$\nRounding the final numerical answer to four significant figures, we get $0.1701$. This value is dimensionless, as gap fraction is a ratio.\nAs a check, Jensen's inequality for a convex function like $f(x)=\\exp(-kx)$ (since $f''(x)  0$) states that $\\mathbb{E}[f(X)] \\ge f(\\mathbb{E}[X])$. Here, $f(\\mu) = \\exp(-1.8) \\approx 0.1653$. Our result, $0.1701$, is indeed greater than $f(\\mu)$, consistent with the theory.",
            "answer": "$$\\boxed{0.1701}$$"
        },
        {
            "introduction": "While many scaling problems involve aggregating fine-scale properties to a coarser scale (upscaling), a central challenge in remote sensing is the inverse: inferring sub-pixel composition from a coarse-scale measurement (downscaling). This exercise tackles the classic problem of linear spectral unmixing, where you will use constrained optimization to estimate the fractional abundances of different materials within a single pixel. This practice develops crucial skills in applying linear algebra to inverse problems and provides insight into the concept of identifiability, which determines whether a unique solution is possible .",
            "id": "3844212",
            "problem": "A hyperspectral pixel is modeled under the Linear Mixing Model (LMM), where the measured reflectance vector at the pixel scale, denoted by $y \\in \\mathbb{R}^{L}$, is a linear combination of $p$ endmember spectra collected at a different characteristic scale and assembled in the matrix $E \\in \\mathbb{R}^{L \\times p}$, with the abundance vector $a \\in \\mathbb{R}^{p}$. The LMM posits $y = E a + \\varepsilon$, where $\\varepsilon$ is an additive noise vector. To ensure mass conservation across scales, abundances satisfy the sum-to-one constraint $c^{\\top} a = 1$, where $c \\in \\mathbb{R}^{p}$ is the vector of ones, and nonnegativity $a \\geq 0$ arises from physical interpretability.\n\nAssume negligible noise and that $E$ has full column rank so that $E^{\\top} E$ is invertible. Consider the equality-constrained least-squares problem of estimating $a$ by minimizing the residual energy across spectral bands subject to the sum-to-one constraint:\n$$\n\\min_{a \\in \\mathbb{R}^{p}} \\ \\|y - E a\\|_{2}^{2} \\quad \\text{subject to} \\quad c^{\\top} a = 1.\n$$\nUsing Lagrange multipliers and starting from first principles, derive the closed-form expression for the abundance estimate $a^{*}$ in terms of $E$, $y$, and $c$.\n\nThen, using the same fundamental base, reason about identifiability when endmembers are collinear in spectral space at the pixel scale; that is, when the columns of $E$ lie on a single line (so the columns are linearly dependent). Specifically, explain whether the equality-constrained least-squares solution is unique and how the sum-to-one constraint interacts with collinearity. Frame your discussion using rank, null space, and convex hull arguments that are grounded in the LMM and the properties of least-squares projection.\n\nProvide the final estimate $a^{*}$ as a single closed-form analytic expression in terms of $E$, $y$, and $c$. No rounding is required, and no units are needed for the final expression.",
            "solution": "The Linear Mixing Model (LMM) posits $y = E a + \\varepsilon$, where $y \\in \\mathbb{R}^{L}$, $E \\in \\mathbb{R}^{L \\times p}$, $a \\in \\mathbb{R}^{p}$, and $\\varepsilon \\in \\mathbb{R}^{L}$ is additive noise. Under negligible noise and equality constraints, the abundance estimation problem becomes the equality-constrained least-squares optimization:\n$$\n\\min_{a \\in \\mathbb{R}^{p}} \\ \\|y - E a\\|_{2}^{2} \\quad \\text{subject to} \\quad c^{\\top} a = 1,\n$$\nwhere $c \\in \\mathbb{R}^{p}$ is the vector of ones. The fundamental base is the definition of least squares as orthogonal projection in Euclidean space and the method of Lagrange multipliers for equality-constrained optimization.\n\nDefine the objective function $f(a) = \\|y - E a\\|_{2}^{2} = (y - E a)^{\\top}(y - E a)$. Introduce the Lagrangian\n$$\n\\mathcal{L}(a,\\lambda) = (y - E a)^{\\top}(y - E a) + 2 \\lambda \\left(c^{\\top} a - 1\\right),\n$$\nwhere $\\lambda \\in \\mathbb{R}$ is the Lagrange multiplier associated with the sum-to-one constraint. The factor of $2$ is conventional and simplifies derivatives.\n\nStationarity with respect to $a$ requires $\\nabla_{a} \\mathcal{L} = 0$. Compute\n$$\n\\nabla_{a} \\left[(y - E a)^{\\top}(y - E a)\\right] = -2 E^{\\top} (y - E a) = -2 E^{\\top} y + 2 E^{\\top} E a,\n$$\nand $\\nabla_{a} \\left[2 \\lambda (c^{\\top} a - 1)\\right] = 2 \\lambda c$. Setting the gradient to zero gives\n$$\n-2 E^{\\top} y + 2 E^{\\top} E a + 2 \\lambda c = 0 \\quad \\Longrightarrow \\quad E^{\\top} E \\, a + \\lambda c = E^{\\top} y.\n$$\nStationarity with respect to $\\lambda$ recovers the constraint $c^{\\top} a = 1$. Let $G = E^{\\top} E \\in \\mathbb{R}^{p \\times p}$ and $b = E^{\\top} y \\in \\mathbb{R}^{p}$. Under the assumption that $E$ has full column rank, $G$ is invertible. Solve the linear system:\n$$\na = G^{-1} (b - \\lambda c).\n$$\nEnforce the constraint:\n$$\nc^{\\top} a = c^{\\top} G^{-1} (b - \\lambda c) = 1.\n$$\nSolve for $\\lambda$:\n$$\nc^{\\top} G^{-1} b - \\lambda \\, c^{\\top} G^{-1} c = 1 \\quad \\Longrightarrow \\quad \\lambda = \\frac{c^{\\top} G^{-1} b - 1}{c^{\\top} G^{-1} c}.\n$$\nSubstitute back into $a$:\n$$\na^{*} = G^{-1} b - G^{-1} c \\, \\frac{c^{\\top} G^{-1} b - 1}{c^{\\top} G^{-1} c}.\n$$\nRewriting in terms of $E$ and $y$ gives\n$$\na^{*} = (E^{\\top} E)^{-1} E^{\\top} y \\;-\\; (E^{\\top} E)^{-1} c \\, \\frac{c^{\\top} (E^{\\top} E)^{-1} E^{\\top} y - 1}{c^{\\top} (E^{\\top} E)^{-1} c}.\n$$\nThis is the equality-constrained least-squares abundance estimate derived from first principles via Lagrange multipliers.\n\nNext, we discuss identifiability when endmembers are collinear in spectral space at the pixel scale, meaning the columns of $E$ lie on a single line (they are linearly dependent and $\\operatorname{rank}(E) = 1$). In this case, $E^{\\top} E$ is singular, so the previous derivation relying on invertibility fails. Conceptually, collinearity implies that all endmember spectra $e_{1},\\dots,e_{p}$ lie on a line in $\\mathbb{R}^{L}$; the convex hull of the endmembers is then a line segment. Under the sum-to-one constraint, any convex combination of the endmembers lies on that line segment. The measurement $y$ (neglecting noise) lies on the same line segment if the LMM holds.\n\nIdentifiability examines whether the abundance vector $a$ is uniquely determined by $y$ under the constraints. With $\\operatorname{rank}(E)=1$, write the columns as $e_{i} = \\gamma_{i} u + d$, where $u \\in \\mathbb{R}^{L}$ spans the line, $\\gamma_{i} \\in \\mathbb{R}$ are scalars, and $d \\in \\mathbb{R}^{L}$ is a fixed offset capturing an affine shift. Then the mixture is\n$$\nE a = \\sum_{i=1}^{p} a_{i} e_{i} = \\left(\\sum_{i=1}^{p} \\gamma_{i} a_{i}\\right) u + \\left(\\sum_{i=1}^{p} a_{i}\\right) d = \\left(\\sum_{i=1}^{p} \\gamma_{i} a_{i}\\right) u + d,\n$$\nusing $c^{\\top} a = 1$. Thus, the data $y$ constrains only the scalar $\\sum_{i=1}^{p} \\gamma_{i} a_{i}$ along $u$, plus the offset $d$. For $p \\geq 3$, there are infinitely many $a$ that satisfy both $c^{\\top} a = 1$ and the same value of $\\sum_{i=1}^{p} \\gamma_{i} a_{i}$, leading to non-uniqueness. Geometrically, any interior endmember lying on the line segment introduces redundancy: a point on the segment can be represented as a convex combination of the two extreme endmembers, or as a convex combination including additional interior endmembers. Therefore, identifiability fails because the null space of $E$ is nontrivial, and multiple abundance vectors map to the same $y$ subject to $c^{\\top} a = 1$.\n\nWhen $p=2$ and the two endmembers are collinear (which is necessarily true for any two points), identifiability can still be recovered under $c^{\\top} a = 1$ because $y$ lies on the line segment between the two spectra, and the representation as a convex combination of the two extremes is unique. In that special case, the system reduces to two independent scalar equations: $a_{1}+a_{2}=1$ and $\\gamma_{1} a_{1} + \\gamma_{2} a_{2} = \\text{scalar}$ along $u$, yielding a unique solution under $a \\geq 0$. However, for $p \\geq 3$ with collinear columns, the equality-constrained least-squares solution is not unique: one can add any vector in the null space of $E$ that also satisfies $c^{\\top} z = 0$ to a solution and obtain another solution with the same fit.\n\nFrom a scale perspective, collinearity often reflects that endmembers are not sufficiently distinct at the spectral resolution or spatial scale considered, for example due to subpixel mixing, measurement smoothing, or scale mismatch between laboratory spectra and scene-level materials. This degrades the effective rank of $E$, increases the condition number of $E^{\\top} E$, and undermines identifiability. In practice, one may regularize, reduce endmember sets to extreme points, or incorporate inequality constraints and prior information to recover stability, but strictly under the equality-constrained least-squares framework with collinear endmembers and $p \\geq 3$, the abundance solution is non-unique.\n\nTherefore, under the stated full-rank assumption, the abundance estimate is given by the closed-form expression above; under collinearity (rank-deficiency), that expression is undefined because $(E^{\\top} E)^{-1}$ does not exist, and identifiability fails except in special cases such as $p=2$ where a unique convex combination exists along the line segment.",
            "answer": "$$\\boxed{(E^{\\top} E)^{-1} E^{\\top} y \\;-\\; (E^{\\top} E)^{-1} c \\,\\frac{c^{\\top} (E^{\\top} E)^{-1} E^{\\top} y - 1}{c^{\\top} (E^{\\top} E)^{-1} c}}$$"
        }
    ]
}