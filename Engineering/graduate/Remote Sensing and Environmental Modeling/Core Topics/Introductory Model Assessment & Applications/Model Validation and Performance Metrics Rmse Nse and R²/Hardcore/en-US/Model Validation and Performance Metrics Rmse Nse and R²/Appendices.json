{
    "hands_on_practices": [
        {
            "introduction": "This first exercise grounds our understanding of model efficiency by starting with the most basic building block: the residual, which is the difference between an observation and a prediction. We will calculate the Nash–Sutcliffe Efficiency ($NSE$) not by simply plugging values into a formula, but by deriving it from the principle of variance decomposition. This hands-on practice  reinforces the fundamental interpretation of $NSE$ as a measure of a model's skill relative to the performance of a simple baseline predictor, a crucial concept for any modeler.",
            "id": "3829019",
            "problem": "A satellite-based energy balance algorithm is validated against tower-based daily evapotranspiration for a small set of coincident days at a semiarid site. Let the observed values be $\\mathbf{y} = [\\,2.0,\\; 3.5,\\; 3.0,\\; 4.2\\,]$ (in $\\mathrm{mm\\,d^{-1}}$) and the model estimates be $\\hat{\\mathbf{y}} = [\\,1.8,\\; 3.6,\\; 3.4,\\; 3.9\\,]$ (in $\\mathrm{mm\\,d^{-1}}$). Starting from core statistical definitions appropriate for model validation in remote sensing and environmental modeling, proceed as follows:\n\n- Using the definition of residuals as the discrepancy between observations and estimates, write the residual vector $\\mathbf{r}$ and compute its sample mean and sample variance. Throughout, treat this as a finite sample of size $n=4$ and adhere to the conventional unbiased definition of sample variance.\n\n- Interpreting model skill as a reduction in average squared error relative to a baseline that predicts the sample mean of the observations, and using variance-decomposition reasoning consistent with this baseline, derive an explicit finite-sample expression for the Nash–Sutcliffe Efficiency (NSE) in terms of sums over this sample, then evaluate it numerically for the given data.\n\nReport only the final Nash–Sutcliffe Efficiency (NSE) value as your answer. Round your final answer to four significant figures. Express it as a pure number (unitless).",
            "solution": "The problem statement is evaluated as valid. It is scientifically grounded, well-posed, and objective. It presents a standard model validation task using established statistical metrics, provides all necessary data, and is free of ambiguity or contradiction.\n\nThe problem requires the calculation of the Nash–Sutcliffe Efficiency (NSE) for a given set of observed and modeled data. This will be accomplished in three stages as specified: first, analyzing the residuals; second, deriving the formula for NSE from first principles; and third, performing the numerical evaluation.\n\nThe observed values are given by the vector $\\mathbf{y}$ and the model estimates by the vector $\\hat{\\mathbf{y}}$, for a sample of size $n=4$.\n$$ \\mathbf{y} = \\begin{pmatrix} 2.0 \\\\ 3.5 \\\\ 3.0 \\\\ 4.2 \\end{pmatrix}, \\quad \\hat{\\mathbf{y}} = \\begin{pmatrix} 1.8 \\\\ 3.6 \\\\ 3.4 \\\\ 3.9 \\end{pmatrix} $$\n\nFirst, we address the computation of the residual vector and its statistics. The residual $r_i$ is defined as the difference between the $i$-th observation $y_i$ and the $i$-th estimate $\\hat{y}_i$: $r_i = y_i - \\hat{y}_i$.\n\nThe components of the residual vector $\\mathbf{r}$ are:\n$$ r_1 = 2.0 - 1.8 = 0.2 $$\n$$ r_2 = 3.5 - 3.6 = -0.1 $$\n$$ r_3 = 3.0 - 3.4 = -0.4 $$\n$$ r_4 = 4.2 - 3.9 = 0.3 $$\nThus, the residual vector is $\\mathbf{r} = [\\,0.2,\\; -0.1,\\; -0.4,\\; 0.3\\,]$.\n\nThe sample mean of the residuals, $\\bar{r}$, is given by:\n$$ \\bar{r} = \\frac{1}{n} \\sum_{i=1}^{n} r_i = \\frac{1}{4} (0.2 - 0.1 - 0.4 + 0.3) = \\frac{0.0}{4} = 0.0 $$\n\nThe unbiased sample variance of the residuals, $s_r^2$, is defined as:\n$$ s_r^2 = \\frac{1}{n-1} \\sum_{i=1}^{n} (r_i - \\bar{r})^2 $$\nSince $\\bar{r} = 0$, this simplifies to:\n$$ s_r^2 = \\frac{1}{4-1} \\sum_{i=1}^{4} r_i^2 = \\frac{1}{3} \\left( (0.2)^2 + (-0.1)^2 + (-0.4)^2 + (0.3)^2 \\right) $$\n$$ s_r^2 = \\frac{1}{3} (0.04 + 0.01 + 0.16 + 0.09) = \\frac{1}{3} (0.30) = 0.10 $$\n\nNext, we derive the expression for the Nash–Sutcliffe Efficiency (NSE). The NSE is interpreted as a measure of model skill, representing the reduction in squared error relative to a baseline model. The baseline model is one that consistently predicts the sample mean of the observations, $\\bar{y}$.\n\nThe sum of squared errors (SSE) of the model is the sum of the squared residuals:\n$$ \\text{SSE} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 $$\nThis quantity represents the \"unexplained\" variance by the model.\n\nThe sum of squared errors of the baseline model is the total sum of squares (SST), which quantifies the total variance in the observed data around its mean:\n$$ \\text{SST} = \\sum_{i=1}^{n} (y_i - \\bar{y})^2 $$\n\nThe NSE is then defined as the fraction of the total variance that is explained by the model. It is expressed as $1$ minus the ratio of the unexplained variance to the total variance:\n$$ \\text{NSE} = 1 - \\frac{\\text{SSE}}{\\text{SST}} $$\nSubstituting the definitions of SSE and SST gives the explicit finite-sample expression for NSE:\n$$ \\text{NSE} = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2} $$\nAn NSE value of $1$ indicates a perfect match, an NSE of $0$ indicates the model is no better than the mean, and a negative NSE indicates the model is worse than the mean.\n\nFinally, we evaluate the NSE for the given data.\nFirst, we calculate the numerator, SSE. This is the sum of the squared residuals we computed earlier:\n$$ \\text{SSE} = \\sum_{i=1}^{4} r_i^2 = (0.2)^2 + (-0.1)^2 + (-0.4)^2 + (0.3)^2 = 0.30 $$\n\nNext, we calculate the denominator, SST. This requires the mean of the observed values, $\\bar{y}$:\n$$ \\bar{y} = \\frac{1}{n} \\sum_{i=1}^{n} y_i = \\frac{1}{4} (2.0 + 3.5 + 3.0 + 4.2) = \\frac{12.7}{4} = 3.175 $$\nNow we compute SST:\n$$ \\text{SST} = \\sum_{i=1}^{4} (y_i - \\bar{y})^2 = (2.0 - 3.175)^2 + (3.5 - 3.175)^2 + (3.0 - 3.175)^2 + (4.2 - 3.175)^2 $$\n$$ \\text{SST} = (-1.175)^2 + (0.325)^2 + (-0.175)^2 + (1.025)^2 $$\n$$ \\text{SST} = 1.380625 + 0.105625 + 0.030625 + 1.050625 = 2.5675 $$\n\nWith SSE and SST, we can compute the NSE:\n$$ \\text{NSE} = 1 - \\frac{\\text{SSE}}{\\text{SST}} = 1 - \\frac{0.30}{2.5675} $$\n$$ \\text{NSE} \\approx 1 - 0.1168453466... = 0.8831546533... $$\nThe problem requires the final answer to be rounded to four significant figures.\n$$ \\text{NSE} \\approx 0.8832 $$\nThis value is unitless, as it is a ratio of two quantities with the same units (squared units of evapotranspiration).",
            "answer": "$$\\boxed{0.8832}$$"
        },
        {
            "introduction": "In practice, environmental modelers often compare results across different study sites or time periods, where the natural variability of the data can differ dramatically. This exercise tackles a critical and common pitfall: assuming that a model with a lower Root Mean Squared Error ($RMSE$) is always superior. By working through a hypothetical scenario involving two hydrological basins , we will see how an absolute error metric like $RMSE$ and a relative metric like the coefficient of determination ($R^2$) can rank models differently, underscoring the importance of considering data variance when interpreting model performance.",
            "id": "3829028",
            "problem": "A hydrological model driven by satellite-based precipitation estimates is validated against in situ streamflow for two independent basins, $\\mathcal{A}$ and $\\mathcal{B}$, each with $5$ daily pairs of observed and predicted discharge. You are to examine why a lower Root Mean Squared Error (RMSE) does not necessarily imply a higher coefficient of determination ($R^2$) when the variance of the observed discharge differs between basins.\n\nStarting only from the foundational definitions of the sample mean, sample variance, and squared error, and without invoking any pre-stated formulas for Root Mean Squared Error (RMSE), Nash–Sutcliffe Efficiency (NSE), or the coefficient of determination ($R^2$), argue from first principles why a metric that normalizes by the observed variance can rank model performance differently across datasets that have different observed variance, even if the absolute errors are smaller in one dataset.\n\nThen, using the following observed $\\{y\\}$ and predicted $\\{\\hat{y}\\}$ daily discharges (in $\\mathrm{m^3\\,s^{-1}}$) for each basin, compute both RMSE and $R^2$ for each basin to concretely illustrate the claim. Finally, report the numerical value of the difference $\\Delta = R^2_{\\mathcal{B}} - R^2_{\\mathcal{A}}$.\n\n- Basin $\\mathcal{A}$: observed $\\{y^{(\\mathcal{A})}\\} = \\{10, 12, 11, 9, 10\\}$, predicted $\\{\\hat{y}^{(\\mathcal{A})}\\} = \\{9.5, 12.5, 10.0, 9.5, 10.5\\}$.\n- Basin $\\mathcal{B}$: observed $\\{y^{(\\mathcal{B})}\\} = \\{50, 70, 90, 110, 130\\}$, predicted $\\{\\hat{y}^{(\\mathcal{B})}\\} = \\{58, 80, 85, 100, 125\\}$.\n\nInstructions:\n- Use the core definitions to derive the expressions you need and justify the reasoning about variance normalization.\n- Compute RMSE and $R^2$ for each basin and verify that Basin $\\mathcal{A}$ has the lower RMSE while Basin $\\mathcal{B}$ has the higher $R^2$.\n- Report $\\Delta = R^2_{\\mathcal{B}} - R^2_{\\mathcal{A}}$ as a pure number with no units.\n- Round your final reported $\\Delta$ to four significant figures.",
            "solution": "The problem requires a demonstration, from first principles, of why a lower Root Mean Squared Error (RMSE) does not necessarily imply a better model performance as measured by the coefficient of determination ($R^2$), particularly when comparing models across datasets with different variances. This will be illustrated with a specific calculation for two hydrological basins, $\\mathcal{A}$ and $\\mathcal{B}$.\n\nFirst, we establish the required metrics from foundational definitions. Let there be a set of $n$ observed data points $\\{y_i\\}_{i=1}^n$ and a corresponding set of model predictions $\\{\\hat{y}_i\\}_{i=1}^n$.\n\nThe fundamental definitions are:\n1.  The error for the $i$-th prediction is $e_i = y_i - \\hat{y}_i$.\n2.  The sample mean of the observed data is $\\bar{y} = \\frac{1}{n} \\sum_{i=1}^n y_i$.\n3.  The sum of squared errors ($SSE$) is the sum of the squares of the individual errors:\n    $$SSE = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2$$\n4.  The total sum of squares ($SST$) is the sum of the squared differences between each observation and the sample mean. This quantity is proportional to the variance of the observed data and represents the error of a baseline model that constantly predicts the mean.\n    $$SST = \\sum_{i=1}^n (y_i - \\bar{y})^2$$\n\nFrom these, we derive the expressions for RMSE and $R^2$.\n\nThe Mean Squared Error ($MSE$) is the average of the squared errors: $MSE = \\frac{1}{n} SSE$. The Root Mean Squared Error ($RMSE$) is the square root of the $MSE$, which returns the error metric to the original units of the data.\n$$RMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2} = \\sqrt{\\frac{SSE}{n}}$$\n$RMSE$ is an absolute measure of model fit. A smaller $RMSE$ indicates that the model's predictions are, on average, closer to the observed values.\n\nThe coefficient of determination, $R^2$, is a relative measure of model fit. It quantifies the proportion of the variance in the observed data that is predictable from the model. It is defined by comparing the model's error ($SSE$) to the error of the baseline mean-predictor model ($SST$).\n$$R^2 = 1 - \\frac{SSE}{SST} = 1 - \\frac{\\sum_{i=1}^n (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^n (y_i - \\bar{y})^2}$$\nIn the context of hydrology, this definition is identical to the Nash–Sutcliffe Efficiency (NSE). An $R^2$ value of $1$ signifies a perfect match, while an $R^2$ of $0$ indicates the model is no better than simply predicting the mean of the observations. A negative $R^2$ indicates the model is worse than predicting the mean.\n\nThe core of the problem is to explain why it is possible to have $RMSE_{\\mathcal{A}}  RMSE_{\\mathcal{B}}$ while simultaneously having $R^2_{\\mathcal{A}}  R^2_{\\mathcal{B}}$ for two datasets $\\mathcal{A}$ and $\\mathcal{B}$.\n\nFrom $RMSE_{\\mathcal{A}}  RMSE_{\\mathcal{B}}$, we have $\\sqrt{\\frac{SSE_{\\mathcal{A}}}{n}}  \\sqrt{\\frac{SSE_{\\mathcal{B}}}{n}}$, which implies $SSE_{\\mathcal{A}}  SSE_{\\mathcal{B}}$. This means the sum of squared errors is smaller for dataset $\\mathcal{A}$.\n\nFrom $R^2_{\\mathcal{A}}  R^2_{\\mathcal{B}}$, we have $1 - \\frac{SSE_{\\mathcal{A}}}{SST_{\\mathcal{A}}}  1 - \\frac{SSE_{\\mathcal{B}}}{SST_{\\mathcal{B}}}$. This inequality simplifies to $-\\frac{SSE_{\\mathcal{A}}}{SST_{\\mathcal{A}}}  -\\frac{SSE_{\\mathcal{B}}}{SST_{\\mathcal{B}}}$, which is equivalent to $\\frac{SSE_{\\mathcal{A}}}{SST_{\\mathcal{A}}}  \\frac{SSE_{\\mathcal{B}}}{SST_{\\mathcal{B}}}$.\n\nWe must satisfy two conditions:\n1.  $SSE_{\\mathcal{A}}  SSE_{\\mathcal{B}}$\n2.  $\\frac{SSE_{\\mathcal{A}}}{SST_{\\mathcal{A}}}  \\frac{SSE_{\\mathcal{B}}}{SST_{\\mathcal{B}}}$\n\nThese conditions can coexist if the total sum of squares ($SST$, representing the variance of observations) differs significantly between the two datasets. Specifically, if $SST_{\\mathcal{A}}$ is substantially smaller than $SST_{\\mathcal{B}}$, then even a small absolute error $SSE_{\\mathcal{A}}$ can result in a large error ratio $\\frac{SSE_{\\mathcal{A}}}{SST_{\\mathcal{A}}}$. Conversely, a larger absolute error $SSE_{\\mathcal{B}}$ on a dataset with a much larger variance $SST_{\\mathcal{B}}$ can result in a smaller error ratio $\\frac{SSE_{\\mathcal{B}}}{SST_{\\mathcal{B}}}$. This demonstrates that $RMSE$, an absolute metric, measures the typical magnitude of model error, whereas $R^2$, a normalized metric, measures the model's explanatory power relative to the inherent variability of the data. A model can have small absolute errors (low $RMSE$) but explain little of the (already small) variance, leading to a low $R^2$.\n\nNow, we will demonstrate this with the provided data. For both basins, the number of data points is $n=5$.\n\n**Calculations for Basin $\\mathcal{A}$**\nObserved data: $\\{y^{(\\mathcal{A})}\\} = \\{10, 12, 11, 9, 10\\}$\nPredicted data: $\\{\\hat{y}^{(\\mathcal{A})}\\} = \\{9.5, 12.5, 10.0, 9.5, 10.5\\}$\n\n1.  Calculate the mean of the observed data:\n    $$\\bar{y}^{(\\mathcal{A})} = \\frac{10 + 12 + 11 + 9 + 10}{5} = \\frac{52}{5} = 10.4$$\n2.  Calculate the sum of squared errors ($SSE_{\\mathcal{A}}$):\n    $$SSE_{\\mathcal{A}} = (10-9.5)^2 + (12-12.5)^2 + (11-10.0)^2 + (9-9.5)^2 + (10-10.5)^2$$\n    $$SSE_{\\mathcal{A}} = (0.5)^2 + (-0.5)^2 + (1.0)^2 + (-0.5)^2 + (-0.5)^2 = 0.25 + 0.25 + 1.0 + 0.25 + 0.25 = 2.0$$\n3.  Calculate $RMSE_{\\mathcal{A}}$:\n    $$RMSE_{\\mathcal{A}} = \\sqrt{\\frac{SSE_{\\mathcal{A}}}{n}} = \\sqrt{\\frac{2.0}{5}} = \\sqrt{0.4}$$\n4.  Calculate the total sum of squares ($SST_{\\mathcal{A}}$):\n    $$SST_{\\mathcal{A}} = (10-10.4)^2 + (12-10.4)^2 + (11-10.4)^2 + (9-10.4)^2 + (10-10.4)^2$$\n    $$SST_{\\mathcal{A}} = (-0.4)^2 + (1.6)^2 + (0.6)^2 + (-1.4)^2 + (-0.4)^2 = 0.16 + 2.56 + 0.36 + 1.96 + 0.16 = 5.2$$\n5.  Calculate $R^2_{\\mathcal{A}}$:\n    $$R^2_{\\mathcal{A}} = 1 - \\frac{SSE_{\\mathcal{A}}}{SST_{\\mathcal{A}}} = 1 - \\frac{2.0}{5.2} = 1 - \\frac{5}{13} = \\frac{8}{13}$$\n\n**Calculations for Basin $\\mathcal{B}$**\nObserved data: $\\{y^{(\\mathcal{B})}\\} = \\{50, 70, 90, 110, 130\\}$\nPredicted data: $\\{\\hat{y}^{(\\mathcal{B})}\\} = \\{58, 80, 85, 100, 125\\}$\n\n1.  Calculate the mean of the observed data:\n    $$\\bar{y}^{(\\mathcal{B})} = \\frac{50 + 70 + 90 + 110 + 130}{5} = \\frac{450}{5} = 90$$\n2.  Calculate the sum of squared errors ($SSE_{\\mathcal{B}}$):\n    $$SSE_{\\mathcal{B}} = (50-58)^2 + (70-80)^2 + (90-85)^2 + (110-100)^2 + (130-125)^2$$\n    $$SSE_{\\mathcal{B}} = (-8)^2 + (-10)^2 + (5)^2 + (10)^2 + (5)^2 = 64 + 100 + 25 + 100 + 25 = 314$$\n3.  Calculate $RMSE_{\\mathcal{B}}$:\n    $$RMSE_{\\mathcal{B}} = \\sqrt{\\frac{SSE_{\\mathcal{B}}}{n}} = \\sqrt{\\frac{314}{5}} = \\sqrt{62.8}$$\n4.  Calculate the total sum of squares ($SST_{\\mathcal{B}}$):\n    $$SST_{\\mathcal{B}} = (50-90)^2 + (70-90)^2 + (90-90)^2 + (110-90)^2 + (130-90)^2$$\n    $$SST_{\\mathcal{B}} = (-40)^2 + (-20)^2 + (0)^2 + (20)^2 + (40)^2 = 1600 + 400 + 0 + 400 + 1600 = 4000$$\n5.  Calculate $R^2_{\\mathcal{B}}$:\n    $$R^2_{\\mathcal{B}} = 1 - \\frac{SSE_{\\mathcal{B}}}{SST_{\\mathcal{B}}} = 1 - \\frac{314}{4000} = 1 - 0.0785 = 0.9215$$\n\n**Comparison and Final Calculation**\nWe compare the metrics for the two basins:\n- $RMSE$: $RMSE_{\\mathcal{A}} = \\sqrt{0.4} \\approx 0.632$ and $RMSE_{\\mathcal{B}} = \\sqrt{62.8} \\approx 7.925$. We confirm that $RMSE_{\\mathcal{A}}  RMSE_{\\mathcal{B}}$.\n- $R^2$: $R^2_{\\mathcal{A}} = \\frac{8}{13} \\approx 0.6154$ and $R^2_{\\mathcal{B}} = 0.9215$. We confirm that $R^2_{\\mathcal{A}}  R^2_{\\mathcal{B}}$.\n\nThe numerical results validate the theoretical argument: Basin $\\mathcal{A}$ has lower absolute errors ($RMSE_{\\mathcal{A}}  RMSE_{\\mathcal{B}}$), but the model for Basin $\\mathcal{B}$ provides a much better explanation of the observed variance ($R^2_{\\mathcal{B}}  R^2_{\\mathcal{A}}$). This is because the variance of observations in Basin $\\mathcal{B}$ ($SST_{\\mathcal{B}}=4000$) is vastly larger than in Basin $\\mathcal{A}$ ($SST_{\\mathcal{A}}=5.2$).\n\nFinally, we compute the difference $\\Delta = R^2_{\\mathcal{B}} - R^2_{\\mathcal{A}}$:\n$$\\Delta = 0.9215 - \\frac{8}{13} \\approx 0.9215 - 0.6153846... = 0.30611538...$$\nRounding to four significant figures, we get $\\Delta \\approx 0.3061$.",
            "answer": "$$\n\\boxed{0.3061}\n$$"
        },
        {
            "introduction": "Standard metrics based on squared errors, such as the Nash-Sutcliffe Efficiency ($NSE$), can be heavily influenced by errors in high-magnitude values, often masking poor performance at the lower end of the data range. This is a significant issue in fields like hydrology, where variables like streamflow can span several orders of magnitude. This practice  explores how a logarithmic transformation can be used to construct an efficiency metric that evaluates relative errors, forcing us to think critically about which aspects of model performance are most relevant to our scientific goals.",
            "id": "3829057",
            "problem": "A research team is validating a rainfall–runoff model driven by satellite-derived precipitation in a small semi-arid basin. The observed daily discharges are $\\mathbf{y} = [\\,1,\\,10,\\,100\\,]$ and the model’s predictions are $\\hat{\\mathbf{y}} = [\\,1.2,\\,9,\\,110\\,]$. All values are strictly positive, so logarithmic transformations are well-defined.\n\nStarting from the fundamental notions that (i) the mean squared error compares predictions to observations via squared deviations and (ii) a naive baseline predictor uses the sample mean of the observations, derive an efficiency metric that quantifies the relative improvement of the model over the baseline. Then, construct its logarithmic analogue by applying the same reasoning to the log-transformed observations and predictions using the natural logarithm $\\ln(\\cdot)$.\n\nUsing these derivations:\n- compute the standard efficiency on the original scale for $\\mathbf{y}$ and $\\hat{\\mathbf{y}}$;\n- compute the logarithmic efficiency on the log scale for $\\ln(\\mathbf{y})$ and $\\ln(\\hat{\\mathbf{y}})$ (use $\\ln$ to base $\\exp(1)$);\n- briefly interpret which flow regimes (low, medium, or high) each metric emphasizes given these data.\n\nReport as your final numeric result the difference between the two efficiencies (standard minus logarithmic). Round your final reported number to four significant figures. The result is dimensionless; do not include units in your final numeric report.",
            "solution": "The problem is valid as it is scientifically grounded, well-posed, objective, and internally consistent. It presents a standard task in hydrological model evaluation.\n\nWe begin by deriving the required efficiency metrics from the fundamental principles provided. Let the set of observed values be $y_i$ and the corresponding model predictions be $\\hat{y}_i$, for $i = 1, \\dots, n$. In this problem, $n=3$.\n\nThe first principle states that the performance of a model can be assessed by its mean squared error ($MSE$), which averages the squared deviations between predictions and observations.\n$$MSE_{model} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$$\nThe second principle introduces a naive baseline predictor, which is the sample mean of the observations, $\\bar{y} = \\frac{1}{n} \\sum_{i=1}^{n} y_i$. The error of this baseline model is its own $MSE$.\n$$MSE_{baseline} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\bar{y})^2$$\nThis $MSE_{baseline}$ is, by definition, the variance of the observed data. An efficiency metric quantifies the model's performance relative to this baseline. A standard formulation for efficiency, $E$, is to compare the model's error to the baseline's error. A perfect model would have $MSE_{model} = 0$, yielding a maximum efficiency of $1$. A model that performs no better than the mean has $MSE_{model} = MSE_{baseline}$, yielding an efficiency of $0$. The metric is defined as:\n$$E = 1 - \\frac{MSE_{model}}{MSE_{baseline}}$$\nSubstituting the expressions for $MSE$ and simplifying by canceling the $\\frac{1}{n}$ term gives the formula for the Nash-Sutcliffe Efficiency ($NSE$):\n$$E = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}$$\nThis is the standard efficiency metric.\n\nTo construct the logarithmic analogue, we apply the exact same reasoning to the natural logarithm of the observations and predictions. Let $z_i = \\ln(y_i)$ and $\\hat{z}_i = \\ln(\\hat{y}_i)$. The baseline predictor is now the mean of the log-transformed observations, $\\bar{z} = \\overline{\\ln(y)} = \\frac{1}{n} \\sum_{i=1}^{n} \\ln(y_i)$. The logarithmic efficiency, $E_{log}$, is then:\n$$E_{log} = 1 - \\frac{\\sum_{i=1}^{n} (z_i - \\hat{z}_i)^2}{\\sum_{i=1}^{n} (z_i - \\bar{z})^2} = 1 - \\frac{\\sum_{i=1}^{n} (\\ln(y_i) - \\ln(\\hat{y}_i))^2}{\\sum_{i=1}^{n} (\\ln(y_i) - \\overline{\\ln(y)})^2}$$\n\nNow, we compute these two metrics for the given data.\nThe observed data are $\\mathbf{y} = [\\,1,\\,10,\\,100\\,]$ and the predicted data are $\\hat{\\mathbf{y}} = [\\,1.2,\\,9,\\,110\\,]$.\n\nFirst, we compute the standard efficiency, $E$.\nThe mean of the observations is:\n$$\\bar{y} = \\frac{1+10+100}{3} = \\frac{111}{3} = 37$$\nThe sum of squared errors (the numerator term for the error ratio) is:\n$$\\sum_{i=1}^{3} (y_i - \\hat{y}_i)^2 = (1 - 1.2)^2 + (10 - 9)^2 + (100 - 110)^2 = (-0.2)^2 + (1)^2 + (-10)^2 = 0.04 + 1 + 100 = 101.04$$\nThe sum of squared deviations from the mean (the denominator term) is:\n$$\\sum_{i=1}^{3} (y_i - \\bar{y})^2 = (1 - 37)^2 + (10 - 37)^2 + (100 - 37)^2 = (-36)^2 + (-27)^2 + (63)^2 = 1296 + 729 + 3969 = 5994$$\nThe standard efficiency is:\n$$E = 1 - \\frac{101.04}{5994} \\approx 1 - 0.01685687 \\approx 0.98314313$$\n\nNext, we compute the logarithmic efficiency, $E_{log}$.\nWe first transform the data:\n$\\ln(\\mathbf{y}) = [\\,\\ln(1),\\, \\ln(10),\\, \\ln(100)\\,] = [\\,0,\\, \\ln(10),\\, 2\\ln(10)\\,]$\n$\\ln(\\hat{\\mathbf{y}}) = [\\,\\ln(1.2),\\, \\ln(9),\\, \\ln(110)\\,]$\nThe mean of the log-transformed observations is:\n$$\\overline{\\ln(y)} = \\frac{\\ln(1) + \\ln(10) + \\ln(100)}{3} = \\frac{0 + \\ln(10) + 2\\ln(10)}{3} = \\frac{3\\ln(10)}{3} = \\ln(10)$$\nThe sum of squared errors in log space is:\n$$\\sum_{i=1}^{3} (\\ln(y_i) - \\ln(\\hat{y}_i))^2 = (\\ln(1) - \\ln(1.2))^2 + (\\ln(10) - \\ln(9))^2 + (\\ln(100) - \\ln(110))^2$$\n$$= (-\\ln(1.2))^2 + (\\ln(10/9))^2 + (-\\ln(1.1))^2 \\approx (0.18232)^2 + (0.10536)^2 + (0.09531)^2$$\n$$\\approx 0.033241 + 0.011101 + 0.009084 = 0.053426$$\nThe sum of squared deviations from the mean in log space is:\n$$\\sum_{i=1}^{3} (\\ln(y_i) - \\overline{\\ln(y)})^2 = (\\ln(1) - \\ln(10))^2 + (\\ln(10) - \\ln(10))^2 + (\\ln(100) - \\ln(10))^2$$\n$$= (-\\ln(10))^2 + (0)^2 + (2\\ln(10) - \\ln(10))^2 = (\\ln(10))^2 + (\\ln(10))^2 = 2(\\ln(10))^2$$\n$$ \\approx 2 \\times (2.302585)^2 \\approx 10.603797$$\nThe logarithmic efficiency is:\n$$E_{log} = 1 - \\frac{0.053426}{10.603797} \\approx 1 - 0.0050383 \\approx 0.9949617$$\n\nInterpretation of metric emphasis:\nThe standard efficiency $E$ is based on squared differences $(y_i - \\hat{y}_i)^2$. For the given data, the squared error for the high-flow value, $(100-110)^2=100$, is dramatically larger than the errors for low and medium flows, $(1-1.2)^2=0.04$ and $(10-9)^2=1$. Consequently, $E$ is dominated by the model's performance during high-flow events and is less sensitive to errors in low-flow prediction.\nThe logarithmic efficiency $E_{log}$ is based on squared differences of logarithms, $(\\ln(y_i) - \\ln(\\hat{y}_i))^2 = (\\ln(y_i/\\hat{y}_i))^2$. This term evaluates the squared logarithm of the ratio of observed to predicted values, which is sensitive to relative errors. For these data, the error contributions in log space are more balanced: low flow $(\\approx 0.033)$, medium flow $(\\approx 0.011)$, and high flow $(\\approx 0.009)$. The largest error contribution is from the low-flow value. $E_{log}$ therefore places greater emphasis on capturing the relative accuracy across all magnitudes, and particularly penalizes large relative errors in low flows. In this case, the model's relatively small absolute errors but large relative errors at low flows are penalized more, but its overall good relative performance across the range results in a very high $E_{log}$.\n\nFinally, we compute the difference between the two efficiencies, $E - E_{log}$.\n$$E - E_{log} \\approx 0.98314313 - 0.9949617 = -0.01181857$$\nRounding this result to four significant figures gives $-0.01182$.",
            "answer": "$$ \\boxed{-0.01182} $$"
        }
    ]
}