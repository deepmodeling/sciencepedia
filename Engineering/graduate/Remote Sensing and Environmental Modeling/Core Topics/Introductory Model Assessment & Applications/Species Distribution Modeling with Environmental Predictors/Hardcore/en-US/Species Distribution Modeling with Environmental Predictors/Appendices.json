{
    "hands_on_practices": [
        {
            "introduction": "In species distribution modeling, raw environmental data like temperature may not fully capture the factors limiting a species, as it is often the extreme events—a sudden heatwave or cold snap—that drive population dynamics. This practice guides you through transforming a time-series of raster data into standardized anomalies, which measure how much a current observation deviates from its historical norm. By doing so, you will learn to create powerful predictors that represent environmental stress, a key component in building more accurate and ecologically relevant models .",
            "id": "3852186",
            "problem": "You are provided with small, synthetic rasters representing Land Surface Temperature (LST) in degrees Celsius. Treat each raster as a matrix of pixel values. The goal is to transform a target temperature raster (for a specific time) into a dimensionless standardized anomaly raster by comparing each pixel value $x$ to a baseline distribution at the same pixel across a stack of historical rasters. Use the following foundational definitions: the sample mean $\\mu$ of a pixel across $n$ baseline rasters is the arithmetic average of its values, and the unbiased sample standard deviation $\\sigma$ is the square root of the sample variance computed with $n-1$ in the denominator. To avoid division by zero or numerical instability when $\\sigma$ is near zero, implement a regularization strategy by using a modified standard deviation $\\sigma_{\\epsilon}$ obtained by combining the baseline dispersion $\\sigma$ with a small positive constant $\\epsilon$ in the same physical units as temperature. The standardized anomaly should be a dimensionless quantity that maps the deviation of $x$ from $\\mu$ relative to $\\sigma_{\\epsilon}$. After computing the standardized anomaly raster, derive a binary stress mask where a pixel is flagged as stressed if the magnitude of its standardized anomaly exceeds a given threshold.\n\nPhysical units: temperatures are in degrees Celsius (°C). The standardized anomaly is dimensionless. The regularization constant must be in °C. The stress threshold is dimensionless.\n\nAlgorithmic requirements:\n- Compute the per-pixel sample mean $\\mu$ and unbiased sample standard deviation $\\sigma$ from the stack of baseline rasters.\n- Define the stabilized dispersion as $\\sigma_{\\epsilon} = \\sqrt{\\sigma^2 + \\epsilon^2}$ with $\\epsilon = 0.01$ °C.\n- Transform each target pixel value $x$ to a dimensionless standardized anomaly using $\\mu$ and $\\sigma_{\\epsilon}$.\n- Create a boolean stress mask per pixel where stress is flagged if the absolute standardized anomaly magnitude exceeds a threshold $t = 2$.\n- For each test case, return three outputs:\n  1. The flattened standardized anomaly raster (row-major order) as a list of floats rounded to three decimals.\n  2. The flattened stress mask (row-major order) as a list of booleans where True indicates $|z| \\ge t$.\n  3. The fraction of stressed pixels as a float in the closed interval $[0,1]$, rounded to three decimals.\n\nTest suite:\nUse the following test cases, each with a baseline stack of $n=4$ rasters and one target raster. All temperatures are in °C. Each raster is $2 \\times 3$.\n\n- Case 1 (typical variability):\n  Baseline stack:\n  $$\n  B_1 = \\begin{bmatrix} 20 & 21 & 22 \\\\ 23 & 24 & 25 \\end{bmatrix},\\;\n  B_2 = \\begin{bmatrix} 19 & 21 & 23 \\\\ 22 & 24 & 26 \\end{bmatrix},\\;\n  B_3 = \\begin{bmatrix} 20 & 22 & 22 \\\\ 23 & 25 & 25 \\end{bmatrix},\\;\n  B_4 = \\begin{bmatrix} 21 & 21 & 24 \\\\ 23 & 24 & 27 \\end{bmatrix}.\n  $$\n  Target:\n  $$\n  X = \\begin{bmatrix} 22 & 23 & 21 \\\\ 24 & 26 & 24 \\end{bmatrix}.\n  $$\n\n- Case 2 (zero-variance pixels in baseline):\n  Baseline stack:\n  $$\n  B_1 = \\begin{bmatrix} 15 & 30 & 5 \\\\ 10 & 0 & 12 \\end{bmatrix},\\;\n  B_2 = \\begin{bmatrix} 15 & 32 & 5 \\\\ 11 & 0 & 12 \\end{bmatrix},\\;\n  B_3 = \\begin{bmatrix} 15 & 28 & 5 \\\\ 9 & 0 & 12 \\end{bmatrix},\\;\n  B_4 = \\begin{bmatrix} 15 & 31 & 5 \\\\ 10 & 0 & 12 \\end{bmatrix}.\n  $$\n  Target:\n  $$\n  X = \\begin{bmatrix} 15 & 33 & 7 \\\\ 13 & 0 & 12 \\end{bmatrix}.\n  $$\n\n- Case 3 (extreme heatwave relative to baseline):\n  Baseline stack:\n  $$\n  B_1 = \\begin{bmatrix} 18 & 18 & 18 \\\\ 18 & 18 & 18 \\end{bmatrix},\\;\n  B_2 = \\begin{bmatrix} 19 & 19 & 19 \\\\ 19 & 19 & 19 \\end{bmatrix},\\;\n  B_3 = \\begin{bmatrix} 20 & 20 & 20 \\\\ 20 & 20 & 20 \\end{bmatrix},\\;\n  B_4 = \\begin{bmatrix} 19 & 19 & 19 \\\\ 19 & 19 & 19 \\end{bmatrix}.\n  $$\n  Target:\n  $$\n  X = \\begin{bmatrix} 35 & 35 & 35 \\\\ 35 & 35 & 35 \\end{bmatrix}.\n  $$\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain one entry per test case, and each entry must be a list with the three outputs for the case in the order specified above. For example:\n\"[case1_result,case2_result,case3_result]\"\nwhere each \"caseX_result\" is itself a list of the three outputs described above.",
            "solution": "We begin from the statistical characterization of temperature at a pixel as a random variable $X$ with a baseline empirical distribution derived from historical satellite observations of Land Surface Temperature (LST). For a given pixel, let the baseline observations be $\\{x_1, x_2, \\dots, x_n\\}$ with $n \\ge 2$. The sample mean is defined as\n$$\n\\mu = \\frac{1}{n} \\sum_{j=1}^{n} x_j,\n$$\nand the unbiased sample variance is\n$$\ns^2 = \\frac{1}{n-1} \\sum_{j=1}^{n} (x_j - \\mu)^2,\n$$\nwith the unbiased sample standard deviation\n$$\n\\sigma = \\sqrt{s^2}.\n$$\nTo form a dimensionless standardized anomaly for a new observation $x$, we scale the deviation $x - \\mu$ by the dispersion $\\sigma$. However, when the baseline variability $\\sigma$ is zero or very small (for example, in pixels whose baseline observations do not vary), the ratio can be undefined or numerically unstable. A widely used stabilization strategy is to combine $\\sigma$ with a small positive constant $\\epsilon$ possessing the same physical units as $X$ to obtain a strictly positive effective dispersion\n$$\n\\sigma_{\\epsilon} = \\sqrt{\\sigma^2 + \\epsilon^2}.\n$$\nThis is analogous to Tikhonov regularization: it prevents singularities while preserving the units in the denominator. The standardized anomaly $z$ is then derived from first principles of normalization by dispersion as\n$$z = \\frac{x - \\mu}{\\sigma_{\\epsilon}}.$$\nBy construction, $z$ is dimensionless: an anomaly of $z = 2$ indicates that $x$ lies two dispersion units above the baseline mean when measured with $\\sigma_{\\epsilon}$.\n\nStress events relevant to species distributions are often episodic deviations in environmental conditions that exceed physiological or ecological tolerances. In species distribution modeling, extreme anomalies in LST can indicate heat stress or cold stress that alters survival, reproduction, movement, or resource availability. Standardized anomalies $z$ quantify departures in comparable units across space. Pixels with $|z|$ exceeding a threshold $t$ (here $t = 2$) correspond to events whose magnitude is large relative to typical baseline variability. Identifying such events provides predictors that capture acute stress episodes not reflected by long-term climatological means alone. For example, anomalously high LST detected by thermal infrared remote sensing may coincide with heatwaves that reduce habitat suitability or push conditions beyond species-specific thermal niches. Using $z$ incorporates both the magnitude of departure and the local variability context, enabling cross-region comparability that is critical for modeling distributions at landscape to continental scales.\n\nAlgorithmic design:\n1. For each test case, compute per-pixel $\\mu$ and $\\sigma$ across the baseline stack using the unbiased estimator ($n-1$ in the denominator).\n2. Set $\\epsilon = 0.01$ °C and compute $\\sigma_{\\epsilon} = \\sqrt{\\sigma^2 + \\epsilon^2}$ per pixel.\n3. Compute the standardized anomaly raster $z$ per pixel using $z = (x - \\mu)/\\sigma_{\\epsilon}$.\n4. Create the boolean stress mask per pixel using the condition $|z| \\ge t$ with $t = 2$.\n5. Compute the fraction of stressed pixels as the average of the boolean mask over all pixels, which yields a value in $[0, 1]$.\n6. Flatten both the $z$ raster and the stress mask in row-major order, round $z$ values and the stressed fraction to three decimals, and aggregate the outputs per test case as specified.\n\nThis procedure adheres to fundamental statistical definitions and provides a principled approach to quantifying and detecting environmental stress events as predictors in species distribution modeling. Because $z$ is dimensionless, it is appropriate for integration with other standardized predictors and for threshold-based feature engineering in advanced ecological niche models, including generalized linear models, generalized additive models, and machine learning frameworks. The use of a small $\\epsilon$ ensures numerical stability without materially altering $z$ when $\\sigma$ is sufficiently large, while preventing divergence in regions with negligible baseline variability (e.g., persistently uniform surfaces or coarse climatologies).",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef compute_standardized_anomaly(baseline_stack, target_raster, epsilon=0.01, threshold=2.0):\n    \"\"\"\n    Compute per-pixel standardized anomaly z and stress mask.\n    baseline_stack: numpy array of shape (n, rows, cols)\n    target_raster: numpy array of shape (rows, cols)\n    epsilon: small positive constant in same units as temperature (°C)\n    threshold: dimensionless threshold for stress detection on |z|\n    Returns:\n        z_flat: list of floats (rounded to 3 decimals), flattened row-major\n        stress_flat: list of booleans, flattened row-major\n        stressed_fraction: float rounded to 3 decimals\n    \"\"\"\n    # Compute per-pixel unbiased sample mean and standard deviation across baseline rasters\n    mu = np.mean(baseline_stack, axis=0)\n    sigma = np.std(baseline_stack, axis=0, ddof=1)\n\n    # Stabilize the denominator using epsilon in °C\n    sigma_eps = np.sqrt(sigma**2 + epsilon**2)\n\n    # Compute standardized anomaly (dimensionless)\n    z = (target_raster - mu) / sigma_eps\n\n    # Stress mask: absolute z exceeding threshold\n    stress_mask = np.abs(z) >= threshold\n\n    # Flatten in row-major order\n    z_flat = np.round(z.flatten(), 3).tolist()\n    stress_flat = stress_mask.flatten().tolist()\n\n    # Fraction of stressed pixels\n    stressed_fraction = np.round(np.mean(stress_mask), 3)\n\n    return z_flat, stress_flat, float(stressed_fraction)\n\ndef solve():\n    # Define the test cases from the problem statement.\n\n    # Case 1 (typical variability)\n    B1_1 = np.array([[20,21,22],[23,24,25]], dtype=float)\n    B1_2 = np.array([[19,21,23],[22,24,26]], dtype=float)\n    B1_3 = np.array([[20,22,22],[23,25,25]], dtype=float)\n    B1_4 = np.array([[21,21,24],[23,24,27]], dtype=float)\n    baseline1 = np.stack([B1_1, B1_2, B1_3, B1_4], axis=0)\n    target1 = np.array([[22,23,21],[24,26,24]], dtype=float)\n\n    # Case 2 (zero-variance pixels in baseline)\n    B2_1 = np.array([[15,30,5],[10,0,12]], dtype=float)\n    B2_2 = np.array([[15,32,5],[11,0,12]], dtype=float)\n    B2_3 = np.array([[15,28,5],[9,0,12]], dtype=float)\n    B2_4 = np.array([[15,31,5],[10,0,12]], dtype=float)\n    baseline2 = np.stack([B2_1, B2_2, B2_3, B2_4], axis=0)\n    target2 = np.array([[15,33,7],[13,0,12]], dtype=float)\n\n    # Case 3 (extreme heatwave relative to baseline)\n    B3_1 = np.array([[18,18,18],[18,18,18]], dtype=float)\n    B3_2 = np.array([[19,19,19],[19,19,19]], dtype=float)\n    B3_3 = np.array([[20,20,20],[20,20,20]], dtype=float)\n    B3_4 = nparray([[19,19,19],[19,19,19]], dtype=float)\n    baseline3 = np.stack([B3_1, B3_2, B3_3, B3_4], axis=0)\n    target3 = np.array([[35,35,35],[35,35,35]], dtype=float)\n\n    test_cases = [\n        (baseline1, target1, 0.01, 2.0),\n        (baseline2, target2, 0.01, 2.0),\n        (baseline3, target3, 0.01, 2.0),\n    ]\n\n    results = []\n    for baseline, target, eps, thr in test_cases:\n        z_flat, stress_flat, stressed_fraction = compute_standardized_anomaly(\n            baseline, target, epsilon=eps, threshold=thr\n        )\n        # Each case result is [z_flattened, stress_flattened, stressed_fraction]\n        results.append([z_flat, stress_flat, stressed_fraction])\n\n    # Final print statement in the exact required format.\n    print(f\"{results}\")\n\nsolve()\n```"
        },
        {
            "introduction": "A fundamental task in preparing data for an SDM is linking species occurrence points to environmental conditions represented by raster grids. However, a naive extraction of the raster value at a recorded coordinate ignores the inherent positional uncertainty of the observation, such as error from a GPS device. This exercise provides a hands-on approach to move beyond simple point-on-raster extraction by explicitly modeling this uncertainty, allowing you to quantify its impact and perform a more statistically robust data integration .",
            "id": "3852193",
            "problem": "You are given a two-dimensional raster predictor field defined on a regular grid and a set of point occurrences whose recorded positions are affected by Global Positioning System (GPS) error. The objective is to reconcile point-on-raster extraction with the positional uncertainty and to quantify the impact of this uncertainty when the positional error is comparable to the raster cell size. All spatial coordinates must be in meters, all raster cell sizes must be in meters, and the predictor values are dimensionless (unitless). Your program must compute, for each test case, the positional uncertainty impact quantified as two quantities: the bias between a naive extraction and an uncertainty-aware expected extraction, and the standard deviation induced by positional uncertainty. The output must be a single line containing a nested list of floats rounded to six decimal places.\n\nThe setting is as follows. Consider a raster defined on a rectangular grid of $N_{r} \\times N_{c}$ cells, with cell size $s$ meters, origin at the lower-left corner at coordinates $(x_{0}, y_{0})$. The cell indexed by row $i$ and column $j$ (with $i \\in \\{0,\\dots,N_{r}-1\\}$ and $j \\in \\{0,\\dots,N_{c}-1\\}$) covers the closed-open rectangle $[x_{0} + j s, x_{0} + (j+1)s) \\times [y_{0} + i s, y_{0} + (i+1)s)$. The raster predictor is piecewise-constant per cell with value $v_{i,j}$ on that cell, and predictor values are unitless.\n\nA single measured occurrence location $(x_{m}, y_{m})$ is affected by isotropic, independent, zero-mean Gaussian location error in each axis. Specifically, the true location $(X, Y)$ relative to $(x_{m}, y_{m})$ follows a bivariate normal distribution with independent axes, standard deviation $\\sigma$ meters in both $x$ and $y$, and zero mean. The two-dimensional probability density function is $$p(x,y) = \\frac{1}{2\\pi \\sigma^{2}} \\exp\\left(-\\frac{x^{2} + y^{2}}{2\\sigma^{2}}\\right).$$ The naive point-on-raster extraction takes the predictor value $v_{i_{0}, j_{0}}$ in the single raster cell $(i_{0}, j_{0})$ containing $(x_{m}, y_{m})$. The uncertainty-aware expected extraction is the conditional expectation of the predictor given the Gaussian error and the finite raster extent, obtained by integrating the piecewise-constant raster field against the location error density over the raster domain and renormalizing when the Gaussian mass falls partially outside the raster. The uncertainty-induced variance is the conditional variance of the predictor value under the same model.\n\nStarting from the fundamental definitions of probability density and expectation, implement a method to compute for each test case:\n- The naive extraction $v_{\\text{naive}}$, obtained by cell lookup at $(x_{m}, y_{m})$.\n- The uncertainty-aware expected value $E[V]$ computed as the conditional expectation of the raster value under the Gaussian location model and the finite grid domain.\n- The uncertainty-induced variance $\\mathrm{Var}[V] = E[V^{2}] - (E[V])^{2}$ with the same conditioning as above.\n- The bias $b = v_{\\text{naive}} - E[V]$ and the standard deviation $s_V = \\sqrt{\\mathrm{Var}[V]}$.\n\nEnsure scientific realism by correctly handling the finite extent of the raster: when $\\sigma$ is large relative to $s$ and the Gaussian mass extends beyond the raster, renormalize by the total probability mass inside the raster domain so that the conditional expectation and variance are computed with respect to the truncated distribution over the raster support. The axes are independent and the raster is piecewise-constant per cell.\n\nYour program must compute the above quantities for the following test suite. In all cases the origin is $(x_{0}, y_{0}) = (0, 0)$ meters, all coordinates $(x_{m}, y_{m})$ are in meters, cell size $s$ is in meters, and the predictor values are unitless by construction:\n\n- Test case $1$ (happy path, $\\sigma$ comparable to cell size): $N_{r} = 7$, $N_{c} = 7$, $s = 100.0$, $\\sigma = 100.0$, $(x_{m}, y_{m}) = (3.3 s, 2.7 s)$. The raster is defined by $v_{i,j} = 0.2 + 0.01 i + 0.02 j + 0.005 i j$ for all integer $i, j$.\n- Test case $2$ (boundary condition, substantial truncation): $N_{r} = 5$, $N_{c} = 5$, $s = 100.0$, $\\sigma = 150.0$, $(x_{m}, y_{m}) = (0.05 s, 0.1 s)$. The raster is defined by $v_{i,j} = 1.0 + 0.1 i - 0.05 j$.\n- Test case $3$ (near-zero uncertainty): $N_{r} = 6$, $N_{c} = 6$, $s = 100.0$, $\\sigma = 1.0$, $(x_{m}, y_{m}) = (2.4 s, 1.6 s)$. The raster is defined by $v_{i,j} = 0.5 + 0.03 i + 0.04 j$.\n- Test case $4$ (large uncertainty): $N_{r} = 5$, $N_{c} = 5$, $s = 100.0$, $\\sigma = 300.0$, $(x_{m}, y_{m}) = (2.5 s, 2.5 s)$. The raster is defined by $v_{i,j} = 0.2 + 0.02 i + 0.02 j + 0.01 (i - j)$.\n\nYour implementation must:\n- Use the independence of axes and the definition of expectation to compute exact rectangular-cell probabilities under the Gaussian model and aggregate them over all cells to obtain $E[V]$ and $E[V^{2}]$. Apply renormalization by the in-raster probability mass when necessary.\n- Compute $v_{\\text{naive}}$ as the raster value of the cell containing $(x_{m}, y_{m})$.\n- Report, for each test case, the bias $b$ and the standard deviation $s_{V}$. These two numbers must be expressed in unitless predictor units.\n- Round each reported float to six decimal places.\n\nFinal output format: Your program should produce a single line of output containing the results as a nested comma-separated list enclosed in square brackets, where each inner list corresponds to a test case in order and contains two floats $[b, s_{V}]$. For example, a syntactically valid output looks like $[[b_{1}, s_{V,1}],[b_{2}, s_{V,2}],[b_{3}, s_{V,3}],[b_{4}, s_{V,4}]]$ with each float rounded to six decimal places.",
            "solution": "The user-provided problem statement is rigorously analyzed and determined to be valid. It is scientifically grounded in spatial statistics and well-posed, with all necessary parameters and definitions provided for a unique and meaningful solution. The problem is objective and free of ambiguities or contradictions.\n\n### Principle-Based Solution Design\n\nThe problem requires us to quantify the impact of positional uncertainty on the extraction of a predictor value from a raster grid. The core of the problem lies in contrasting a naive point extraction with a probabilistic, uncertainty-aware approach. We will derive the solution from first principles of probability theory.\n\n#### 1. Mathematical Model\n\nLet the measured location of an occurrence be $(x_m, y_m)$. The true location is a random variable $(\\tilde{X}, \\tilde{Y})$. The problem states that the error is governed by an isotropic, independent, zero-mean Gaussian distribution with standard deviation $\\sigma$ in each axis. This means the true location $(\\tilde{X}, \\tilde{Y})$ follows a bivariate normal distribution centered at $(x_m, y_m)$, with the probability density function (PDF):\n$$f(\\tilde{x}, \\tilde{y}) = \\frac{1}{2\\pi\\sigma^2} \\exp\\left(-\\frac{(\\tilde{x} - x_m)^2 + (\\tilde{y} - y_m)^2}{2\\sigma^2}\\right)$$\nDue to the independence of the axes, this PDF is separable: $f(\\tilde{x}, \\tilde{y}) = f_X(\\tilde{x}) f_Y(\\tilde{y})$, where\n$$f_X(\\tilde{x}) = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left(-\\frac{(\\tilde{x} - x_m)^2}{2\\sigma^2}\\right)$$\nand $f_Y(\\tilde{y})$ is analogous.\n\nThe predictor variable is a piecewise-constant field $V(\\tilde{x}, \\tilde{y})$ defined on a raster grid. The grid has $N_r \\times N_c$ cells of size $s \\times s$. A cell with index $(i, j)$, where $i \\in \\{0, \\dots, N_r-1\\}$ and $j \\in \\{0, \\dots, N_c-1\\}$, corresponds to the spatial domain $[j s, (j+1)s) \\times [i s, (i+1)s)$ (assuming origin at $(0,0)$). Within this cell, the predictor value is constant, $V(\\tilde{x}, \\tilde{y}) = v_{i,j}$.\n\n#### 2. Naive Extraction\nThe naive extraction, $v_{\\text{naive}}$, is the predictor value of the single cell that contains the measured location $(x_m, y_m)$. The indices $(i_0, j_0)$ of this cell are determined by:\n$$j_0 = \\lfloor \\frac{x_m}{s} \\rfloor, \\quad i_0 = \\lfloor \\frac{y_m}{s} \\rfloor$$\nThus, $v_{\\text{naive}} = v_{i_0, j_0}$.\n\n#### 3. Uncertainty-Aware Expectation and Variance\n\nThe uncertainty-aware approach computes the conditional expectation and variance of the predictor value $V$, given the error model and the finite extent of the raster grid. The raster domain is $\\mathcal{D} = [0, N_c s) \\times [0, N_r s)$.\n\nFirst, we must calculate the total probability mass of the location distribution that falls within the raster domain $\\mathcal{D}$:\n$$P_{\\mathcal{D}} = \\iint_{\\mathcal{D}} f(\\tilde{x}, \\tilde{y}) \\,d\\tilde{x} \\,d\\tilde{y}$$\nUsing the separability of the PDF, this becomes:\n$$P_{\\mathcal{D}} = \\left( \\int_{0}^{N_c s} f_X(\\tilde{x}) \\,d\\tilde{x} \\right) \\left( \\int_{0}^{N_r s} f_Y(\\tilde{y}) \\,d\\tilde{y} \\right)$$\nThese one-dimensional integrals can be expressed using the cumulative distribution function (CDF) of the standard normal distribution, $\\Phi(z) = \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^{z} e^{-t^2/2} dt$.\n$$\\int_{a}^{b} f_X(\\tilde{x}) \\,d\\tilde{x} = \\Phi\\left(\\frac{b - x_m}{\\sigma}\\right) - \\Phi\\left(\\frac{a - x_m}{\\sigma}\\right)$$\nThus, $P_{\\mathcal{D}}$ is the product of two such terms, one for each axis. This value $P_{\\mathcal{D}}$ serves as the normalization constant for our conditional probability space.\n\nThe conditional expectation of the predictor, $E[V]$, is given by:\n$$E[V] = \\frac{1}{P_{\\mathcal{D}}} \\iint_{\\mathcal{D}} V(\\tilde{x}, \\tilde{y}) f(\\tilde{x}, \\tilde{y}) \\,d\\tilde{x} \\,d\\tilde{y}$$\nSince $V$ is piecewise constant, we can decompose the integral into a sum over all cells:\n$$E[V] = \\frac{1}{P_{\\mathcal{D}}} \\sum_{i=0}^{N_r-1} \\sum_{j=0}^{N_c-1} v_{i,j} \\iint_{\\text{cell}_{i,j}} f(\\tilde{x}, \\tilde{y}) \\,d\\tilde{x} \\,d\\tilde{y}$$\nLet $P_{i,j}$ be the probability mass within cell $(i,j)$:\n$$P_{i,j} = \\left( \\int_{js}^{(j+1)s} f_X(\\tilde{x}) \\,d\\tilde{x} \\right) \\left( \\int_{is}^{(i+1)s} f_Y(\\tilde{y}) \\,d\\tilde{y} \\right)$$\nEach integral is again computed using the normal CDF. Then, the expectation is a weighted average of cell values:\n$$E[V] = \\frac{1}{P_{\\mathcal{D}}} \\sum_{i=0}^{N_r-1} \\sum_{j=0}^{N_c-1} v_{i,j} P_{i,j}$$\nSimilarly, the conditional second moment, $E[V^2]$, is:\n$$E[V^2] = \\frac{1}{P_{\\mathcal{D}}} \\sum_{i=0}^{N_r-1} \\sum_{j=0}^{N_c-1} v_{i,j}^2 P_{i,j}$$\nThe conditional variance, $\\mathrm{Var}[V]$, is then found using the standard formula:\n$$\n\\mathrm{Var}[V] = E[V^2] - (E[V])^2\n$$\n\n#### 4. Final Quantities\n\nThe two quantities required are the bias, $b$, and the uncertainty-induced standard deviation, $s_V$:\n$$\nb = v_{\\text{naive}} - E[V]\n$$\n$$\ns_V = \\sqrt{\\mathrm{Var}[V]}\n$$\nTo prevent numerical issues, if the computed variance is a small negative number due to floating-point error, it is treated as $0$.\n\n#### Algorithmic Summary\nFor each test case:\n1.  Define the raster parameters ($N_r, N_c, s$) and the error model parameters ($\\sigma, x_m, y_m$).\n2.  Calculate $v_{\\text{naive}}$ by identifying the cell $(i_0, j_0) = (\\lfloor y_m/s \\rfloor, \\lfloor x_m/s \\rfloor)$ and retrieving its value $v_{i_0, j_0}$.\n3.  Calculate the total probability mass $P_{\\mathcal{D}}$ within the raster's full extent using the normal CDF.\n4.  Iterate through each cell $(i, j)$ of the raster:\n    a. Calculate the probability mass $P_{i,j}$ within that cell.\n    b. Retrieve the cell value $v_{i,j}$.\n    c. Accumulate the sums $\\sum v_{i,j} P_{i,j}$ and $\\sum v_{i,j}^2 P_{i,j}$.\n5.  Compute $E[V]$ and $E[V^2]$ by dividing the accumulated sums by $P_{\\mathcal{D}}$.\n6.  Calculate $\\mathrm{Var}[V] = E[V^2] - (E[V])^2$.\n7.  Calculate the final metrics $b = v_{\\text{naive}} - E[V]$ and $s_V = \\sqrt{\\max(0, \\mathrm{Var}[V])}$.\n8.  Round and report the results as required.\n\nThis method correctly accounts for the piecewise-constant nature of the raster, the continuous Gaussian error model, and the finite boundaries of the spatial data, providing a rigorous solution to the problem.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test cases.\n    \"\"\"\n\n    def compute_metrics(nr, nc, s, sigma, xm, ym, v_func):\n        \"\"\"\n        Computes bias and standard deviation for a single test case.\n\n        Args:\n            nr (int): Number of rows in the raster.\n            nc (int): Number of columns in the raster.\n            s (float): Cell size in meters.\n            sigma (float): Standard deviation of Gaussian error in meters.\n            xm (float): Measured x-coordinate in meters.\n            ym (float): Measured y-coordinate in meters.\n            v_func (callable): Function v_func(i, j) to get raster value.\n\n        Returns:\n            tuple: A tuple containing (bias, std_dev).\n        \"\"\"\n        # 1. Naive extraction\n        # The cell definition is [js, (j+1)s) x [is, (i+1)s), so floor is correct.\n        j0 = int(np.floor(xm / s))\n        i0 = int(np.floor(ym / s))\n        \n        # Ensure indices are within bounds in case xm/ym fall on the boundary.\n        # e.g., if xm = nc * s, floor would give nc.\n        j0 = min(j0, nc - 1)\n        i0 = min(i0, nr - 1)\n        \n        v_naive = v_func(i0, j0)\n\n        # 2. Uncertainty-aware calculation\n        \n        def prob_1d(a, b, loc, scale):\n            # Computes integral of 1D normal PDF from a to b.\n            if scale = 1e-9: # Effectively a dirac delta\n                if a = loc  b:\n                    return 1.0\n                else:\n                    return 0.0\n            # Use scipy's highly accurate CDF implementation\n            return norm.cdf(b, loc=loc, scale=scale) - norm.cdf(a, loc=loc, scale=scale)\n\n        # 2a. Total probability mass within the grid domain\n        total_prob_x = prob_1d(0, nc * s, xm, sigma)\n        total_prob_y = prob_1d(0, nr * s, ym, sigma)\n        total_prob = total_prob_x * total_prob_y\n\n        # Handle edge case where no probability mass is in the grid\n        if total_prob = 1e-12:\n            # Conditional distribution is undefined. Assume E[V]=0, Var[V]=0.\n            # This implies the point is extremely far from the grid.\n            # This choice makes bias = v_naive and s_V = 0.\n            E_V = 0.0\n            Var_V = 0.0\n        else:\n            # 2b. Loop over cells to compute moments\n            sum_v_p = 0.0\n            sum_v2_p = 0.0\n            \n            # Precompute cell probabilities for each axis to optimize the loops\n            # This avoids recomputing the same 1D integrals.\n            cell_probs_x = np.array([prob_1d(j * s, (j + 1) * s, xm, sigma) for j in range(nc)])\n            cell_probs_y = np.array([prob_1d(i * s, (i + 1) * s, ym, sigma) for i in range(nr)])\n\n            for i in range(nr):\n                p_y = cell_probs_y[i]\n                if p_y == 0: continue # Optimization\n                for j in range(nc):\n                    p_x = cell_probs_x[j]\n                    if p_x == 0: continue # Optimization\n\n                    v_ij = v_func(i, j)\n                    p_ij = p_y * p_x\n                    \n                    sum_v_p += v_ij * p_ij\n                    sum_v2_p += v_ij**2 * p_ij\n            \n            # 2c. Renormalize to get conditional moments\n            E_V = sum_v_p / total_prob\n            E_V2 = sum_v2_p / total_prob\n            \n            # Ensure variance is non-negative due to potential floating point errors\n            Var_V = E_V2 - E_V**2\n            if Var_V  0:\n                Var_V = 0.0\n\n        # 3. Final quantities\n        bias = v_naive - E_V\n        s_V = np.sqrt(Var_V)\n        \n        return bias, s_V\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: Happy path\n        {'nr': 7, 'nc': 7, 's': 100.0, 'sigma': 100.0, 'xm': 3.3 * 100.0, 'ym': 2.7 * 100.0, 'v_func': lambda i, j: 0.2 + 0.01 * i + 0.02 * j + 0.005 * i * j},\n        # Case 2: Boundary condition\n        {'nr': 5, 'nc': 5, 's': 100.0, 'sigma': 150.0, 'xm': 0.05 * 100.0, 'ym': 0.1 * 100.0, 'v_func': lambda i, j: 1.0 + 0.1 * i - 0.05 * j},\n        # Case 3: Near-zero uncertainty\n        {'nr': 6, 'nc': 6, 's': 100.0, 'sigma': 1.0, 'xm': 2.4 * 100.0, 'ym': 1.6 * 100.0, 'v_func': lambda i, j: 0.5 + 0.03 * i + 0.04 * j},\n        # Case 4: Large uncertainty\n        {'nr': 5, 'nc': 5, 's': 100.0, 'sigma': 300.0, 'xm': 2.5 * 100.0, 'ym': 2.5 * 100.0, 'v_func': lambda i, j: 0.2 + 0.02 * i + 0.02 * j + 0.01 * (i - j)}\n    ]\n\n    results = []\n    for case in test_cases:\n        bias, s_V = compute_metrics(**case)\n        results.append([bias, s_V])\n\n    # Final print statement in the exact required format.\n    formatted_results = [f\"[{res[0]:.6f},{res[1]:.6f}]\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The concept of scale is central to ecology, as the relationship between a species and its environment can change depending on the spatial resolution of the analysis. This practice simulates a common research scenario where you will fit and evaluate species distribution models using environmental predictors at two different resolutions. By comparing the performance metrics of the fine-scale and coarse-scale models, you will develop a practical understanding of how scale mismatch can affect model outcomes and ecological interpretation .",
            "id": "3852180",
            "problem": "You are to implement a complete, runnable program that evaluates species distribution models at two spatial resolutions using occurrence data and environmental predictors. The program must compute quantitative performance metrics for each resolution and then encode an interpretation of performance differences in terms of scale mismatch and ecological process representation.\n\nThe fundamental base to use is the definition of the Bernoulli distribution for binary outcomes, the framework of Generalized Linear Models (GLMs) for binomial data, the canonical link for binomial GLMs, the principle of Maximum Likelihood Estimation, and the operational definitions of Receiver Operating Characteristic (ROC) Area Under the Curve (AUC) and log loss for probabilistic predictions. Scale concepts from remote sensing, specifically spatial resolution and support, should be used to reason about how predictor aggregation affects model performance without assuming any specific shortcut formulas.\n\nYour program must:\n- Fit a binary response model at each resolution using a binomial Generalized Linear Model with the canonical link and an intercept term.\n- Estimate parameters by maximizing the likelihood, with a small ridge (quadratic) penalty on coefficients excluding the intercept to ensure numerical stability in near-separable cases.\n- Compute the ROC Area Under the Curve and the average log loss for both resolutions.\n- Encode an interpretation as an integer code based on the difference in performance between resolutions, reflecting hypotheses about ecological process scales and scale mismatch.\n\nDefinitions to be used:\n- Binary species occurrence is modeled as $Y \\sim \\text{Bernoulli}(p)$, where $p$ is the probability of presence.\n- The canonical link of the binomial GLM relates the linear predictor and $p$.\n- Maximum Likelihood Estimation finds parameter values that maximize the likelihood of observed binary outcomes under the model.\n- ROC Area Under the Curve quantifies ranking performance of predicted $p$ values between presences and absences.\n- Log loss quantifies average negative log-likelihood per observation given predicted $p$.\n\nData are provided as arrays of environmental predictors and occurrence labels for the fine and coarse resolutions. Each test case contains:\n- Fine-resolution predictors $X_{\\text{fine}} \\in \\mathbb{R}^{n_f \\times d}$.\n- Fine-resolution labels $y_{\\text{fine}} \\in \\{0,1\\}^{n_f}$.\n- Coarse-resolution predictors $X_{\\text{coarse}} \\in \\mathbb{R}^{n_c \\times d}$.\n- Coarse-resolution labels $y_{\\text{coarse}} \\in \\{0,1\\}^{n_c}$.\n- Predictors include vegetation index (unitless) and land surface temperature in degrees Celsius.\n\nPerformance metrics:\n- Compute $AUC_{\\text{fine}}$ and $AUC_{\\text{coarse}}$ as floats in $[0,1]$.\n- Compute $L_{\\text{fine}}$ and $L_{\\text{coarse}}$ as average log loss (floats), where smaller values indicate better performance.\n\nInterpretation code:\n- Let $\\Delta = AUC_{\\text{fine}} - AUC_{\\text{coarse}}$ and a threshold $\\tau_{\\text{AUC}} = 0.1$.\n- If the predicted probabilities are effectively constant (standard deviation less than $10^{-12}$) or all predictors are constant (zero variance) at a resolution, treat that resolution as degenerate.\n- If both resolutions are degenerate or both yield effectively constant predictions, set the interpretation code to $3$ (indeterminate due to lack of information).\n- Else if $\\Delta \\ge \\tau_{\\text{AUC}}$, set the interpretation code to $1$ (evidence that finer-scale processes are better represented at the fine resolution and that coarse aggregation introduces scale mismatch).\n- Else if $\\Delta \\le -\\tau_{\\text{AUC}}$, set the interpretation code to $2$ (evidence that broad-scale processes are better captured at the coarse resolution and fine-scale noise hinders performance).\n- Else set the interpretation code to $0$ (no substantial difference; ambiguous with respect to scale mismatch).\n\nUnits:\n- The land surface temperature predictor is in degrees Celsius. The vegetation index is unitless. The final outputs (AUC and log loss) are unitless floats. No angle unit is involved. All decimals must be printed as numeric floats, not percentages.\n\nYour program must run without user input and produce a single line of output containing a list of per-case results, where each case result is a list of five values: $[AUC_{\\text{fine}},AUC_{\\text{coarse}},L_{\\text{fine}},L_{\\text{coarse}},\\text{code}]$. The numbers must be rounded to four decimal places in the printed output. The final output format must be a single line: a comma-separated list enclosed in square brackets with each case result enclosed in square brackets, for example, $[[0.8123,0.7431,0.4021,0.5012,1],[\\dots]]$.\n\nTest suite:\n- Case $1$ (finer resolution expected to perform better due to fine-scale vegetation index pattern):\n  - $X_{\\text{fine},1} = [\\,(0.1,15.0),(0.2,16.0),(0.3,17.0),(0.4,18.0),(0.5,19.0),(0.6,20.0),(0.7,21.0),(0.8,22.0),(0.35,15.0),(0.55,19.0),(0.75,23.0),(0.25,16.0)\\,]$\n  - $y_{\\text{fine},1} = [\\,0,0,0,0,1,1,1,1,0,1,1,0\\,]$\n  - $X_{\\text{coarse},1} = [\\,(0.3,17.0),(0.55,20.0),(0.7,22.0),(0.25,16.0)\\,]$\n  - $y_{\\text{coarse},1} = [\\,0,1,1,0\\,]$\n- Case $2$ (coarser resolution expected to perform better due to broad-scale temperature gradient):\n  - $X_{\\text{fine},2} = [\\,(0.1,10.0),(0.9,10.5),(0.2,11.0),(0.8,11.5),(0.3,12.0),(0.7,12.5),(0.4,13.0),(0.6,13.5),(0.5,14.0),(0.5,10.2)\\,]$\n  - $y_{\\text{fine},2} = [\\,0,0,0,1,0,1,1,1,1,0\\,]$\n  - $X_{\\text{coarse},2} = [\\,(0.45,11.0),(0.65,12.5),(0.5,13.5),(0.6,14.0),(0.3,10.5)\\,]$\n  - $y_{\\text{coarse},2} = [\\,0,1,1,1,0\\,]$\n- Case $3$ (near-separation at both resolutions; regularization ensures finite estimates):\n  - $X_{\\text{fine},3} = [\\,(0.05,10.0),(0.1,10.0),(0.9,10.0),(0.95,10.0),(0.85,10.0),(0.15,10.0)\\,]$\n  - $y_{\\text{fine},3} = [\\,0,0,1,1,1,0\\,]$\n  - $X_{\\text{coarse},3} = [\\,(0.5,10.0),(0.8,10.0),(0.2,10.0)\\,]$\n  - $y_{\\text{coarse},3} = [\\,1,1,0\\,]$\n- Case $4$ (degenerate: constant predictors; expect $AUC \\approx 0.5$ and indeterminate interpretation):\n  - $X_{\\text{fine},4} = [\\,(0.5,20.0),(0.5,20.0),(0.5,20.0),(0.5,20.0)\\,]$\n  - $y_{\\text{fine},4} = [\\,0,1,0,1\\,]$\n  - $X_{\\text{coarse},4} = [\\,(0.5,20.0),(0.5,20.0)\\,]$\n  - $y_{\\text{coarse},4} = [\\,0,1\\,]$\n\nAlgorithmic requirements:\n- Use an intercept term in the GLM and do not apply the penalty to the intercept.\n- Standardize predictor features to zero mean and unit variance within each resolution before fitting to stabilize optimization; leave zero-variance features centered at zero without scaling.\n- Use a small ridge penalty coefficient $\\lambda = 10^{-2}$ for stability.\n- Clip predicted probabilities to $[10^{-12}, 1 - 10^{-12}]$ when computing log loss.\n- Compute AUC using a method that handles ties correctly.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case result is a list of $5$ values: $[AUC_{\\text{fine}},AUC_{\\text{coarse}},L_{\\text{fine}},L_{\\text{coarse}},\\text{code}]$. All numeric values must be rounded to $4$ decimal places in the printed output. No other text must be printed.",
            "solution": "The problem requires the implementation and evaluation of species distribution models at two distinct spatial resolutions, fine and coarse. The modeling framework is a Generalized Linear Model (GLM) for binary occurrence data, specifically a logistic regression model, with parameter estimation performed via penalized maximum likelihood. The core of the task is to fit these models, calculate their performance using Area Under the Receiver Operating Characteristic Curve ($AUC$) and log loss, and interpret the performance difference in the context of ecological scale.\n\nFirst, we formalize the statistical model. The presence or absence of a species at a given location $i$, denoted by the binary random variable $Y_i$, is assumed to follow a Bernoulli distribution:\n$$\nY_i \\sim \\text{Bernoulli}(p_i)\n$$\nwhere $p_i$ is the probability of presence ($Y_i=1$).\n\nA GLM relates the expected value of the response variable, $E[Y_i] = p_i$, to a linear combination of predictor variables through a link function. For the binomial family, the canonical link function is the logit function:\n$$\n\\eta_i = g(p_i) = \\log\\left(\\frac{p_i}{1-p_i}\\right)\n$$\nThe linear predictor, $\\eta_i$, is a linear function of the predictor variables $\\mathbf{x}_i \\in \\mathbb{R}^d$ for that location:\n$$\n\\eta_i = \\beta_0 + \\mathbf{x}_i^T \\boldsymbol{\\beta}\n$$\nHere, $\\beta_0$ is the intercept term, and $\\boldsymbol{\\beta} \\in \\mathbb{R}^d$ is the vector of coefficients for the $d$ predictors. To facilitate matrix notation, we can define an augmented parameter vector $\\boldsymbol{\\theta} = [\\beta_0, \\beta_1, \\dots, \\beta_d]^T$ and an augmented predictor vector $\\mathbf{x}_{\\text{aug}, i} = [1, x_{i1}, \\dots, x_{id}]^T$, so that $\\eta_i = \\mathbf{x}_{\\text{aug}, i}^T \\boldsymbol{\\theta}$.\n\nThe inverse of the logit link function is the logistic sigmoid function, $\\sigma(\\eta)$, which allows us to recover the probability $p_i$:\n$$\np_i = \\sigma(\\eta_i) = \\frac{1}{1 + e^{-\\eta_i}}\n$$\nThe model parameters $\\boldsymbol{\\theta}$ are estimated by maximizing the likelihood of the observed data. For a set of $n$ independent observations $(\\mathbf{y}, \\mathbf{X})$, the log-likelihood function is:\n$$\n\\ell(\\boldsymbol{\\theta}; \\mathbf{y}, \\mathbf{X}) = \\sum_{i=1}^n \\left[ y_i \\log(p_i) + (1-y_i)\\log(1-p_i) \\right]\n$$\nSubstituting $p_i = \\sigma(\\mathbf{x}_{\\text{aug}, i}^T \\boldsymbol{\\theta})$, we get:\n$$\n\\ell(\\boldsymbol{\\theta}) = \\sum_{i=1}^n \\left[ y_i (\\mathbf{x}_{\\text{aug}, i}^T \\boldsymbol{\\theta}) - \\log(1 + e^{\\mathbf{x}_{\\text{aug}, i}^T \\boldsymbol{\\theta}}) \\right]\n$$\nTo ensure numerical stability and prevent issues with perfect separation, a ridge (L2) penalty is added to the log-likelihood. The penalty is applied only to the slope coefficients $\\boldsymbol{\\beta}$, not the intercept $\\beta_0$. The penalized log-likelihood to be maximized is:\n$$\n\\ell_p(\\boldsymbol{\\theta}) = \\ell(\\boldsymbol{\\theta}) - \\frac{\\lambda}{2} \\sum_{j=1}^d \\beta_j^2\n$$\nwhere $\\lambda$ is the regularization parameter, specified as $\\lambda=10^{-2}$. Since numerical optimization routines typically find a minimum, we will minimize the negative penalized log-likelihood, $- \\ell_p(\\boldsymbol{\\theta})$. This is a convex optimization problem, guaranteeing a unique solution. We use a gradient-based optimizer for this task.\n\nBefore model fitting, the predictor variables $X$ at each resolution are standardized to have a mean of $0$ and a standard deviation of $1$. For a feature $j$ with mean $\\mu_j$ and standard deviation $\\sigma_j$, a value $x_{ij}$ is transformed to $z_{ij} = (x_{ij} - \\mu_j) / \\sigma_j$. If a predictor has zero variance ($\\sigma_j=0$), its standardized values are set to $0$. This standardization places predictors on a comparable scale, which is beneficial for the stability and convergence of the optimization algorithm.\n\nAfter fitting the models for both fine and coarse resolutions and obtaining the optimal parameter vectors ($\\boldsymbol{\\theta}_{\\text{fine}}$ and $\\boldsymbol{\\theta}_{\\text{coarse}}$), we compute performance metrics.\n\n1.  **Log Loss**: This metric quantifies the average negative log-likelihood of the predictions. For a set of $n$ observations, it is defined as:\n    $$\n    L = -\\frac{1}{n} \\sum_{i=1}^n \\left[ y_i \\log(\\hat{p}_i) + (1-y_i)\\log(1-\\hat{p}_i) \\right]\n    $$\n    where $\\hat{p}_i$ is the model's predicted probability for observation $i$. To avoid numerical errors from $\\log(0)$, the predicted probabilities are clipped to a small-epsilon bounded interval, specifically $[10^{-12}, 1 - 10^{-12}]$. Lower log loss values indicate a better fit.\n\n2.  **ROC Area Under the Curve ($AUC$)**: This metric evaluates the model's ability to discriminate between positive ($y=1$) and negative ($y=0$) cases. It is equivalent to the probability that the model assigns a higher predicted probability to a randomly chosen positive instance than to a randomly chosen negative instance. To compute $AUC$ while correctly handling ties in predicted probabilities, we can relate it to the Mann-Whitney U statistic. Let $\\mathcal{P} = \\{i | y_i=1\\}$ be the set of indices of positive instances and $\\mathcal{A} = \\{i | y_i=0\\}$ be the set of indices of negative instances, with $n_1 = |\\mathcal{P}|$ and $n_0 = |\\mathcal{A}|$. The $AUC$ is calculated as:\n    $$\n    AUC = \\frac{1}{n_1 n_0} \\sum_{i \\in \\mathcal{P}} \\sum_{j \\in \\mathcal{A}} \\left( \\mathbf{1}_{\\hat{p}_i > \\hat{p}_j} + 0.5 \\cdot \\mathbf{1}_{\\hat{p}_i = \\hat{p}_j} \\right)\n    $$\n    where $\\mathbf{1}$ is the indicator function. An $AUC$ of $1.0$ represents a perfect classifier, while an $AUC$ of $0.5$ corresponds to a model with no better-than-random discrimination ability.\n\nFinally, an interpretation code is assigned based on the performance difference. We define $\\Delta = AUC_{\\text{fine}} - AUC_{\\text{coarse}}$ and a threshold $\\tau_{AUC} = 0.1$.\n-   A resolution is considered degenerate if its predictors are all constant or if the model produces effectively constant probabilities (standard deviation  $10^{-12}$). If both resolutions are degenerate, the code is $3$.\n-   If $\\Delta \\ge \\tau_{AUC}$, the code is $1$, suggesting the fine-resolution model better captures the underlying ecological processes.\n-   If $\\Delta \\le -\\tau_{AUC}$, the code is $2$, suggesting the coarse-resolution model performs better, possibly by filtering out fine-scale noise or by better matching the scale of the dominant ecological driver.\n-   Otherwise, the code is $0$, indicating no substantial performance difference.\n\nThe overall algorithm proceeds by applying this entire sequence—preprocessing, model fitting, performance evaluation, and interpretation—to each test case, first for the fine-resolution data and then for the coarse-resolution data, before comparing the results.",
            "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Main function to run the species distribution modeling and evaluation for all test cases.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"fine\": (\n                np.array([(0.1, 15.0), (0.2, 16.0), (0.3, 17.0), (0.4, 18.0), (0.5, 19.0), (0.6, 20.0), (0.7, 21.0), (0.8, 22.0), (0.35, 15.0), (0.55, 19.0), (0.75, 23.0), (0.25, 16.0)]),\n                np.array([0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0])\n            ),\n            \"coarse\": (\n                np.array([(0.3, 17.0), (0.55, 20.0), (0.7, 22.0), (0.25, 16.0)]),\n                np.array([0, 1, 1, 0])\n            )\n        },\n        {\n            \"fine\": (\n                np.array([(0.1, 10.0), (0.9, 10.5), (0.2, 11.0), (0.8, 11.5), (0.3, 12.0), (0.7, 12.5), (0.4, 13.0), (0.6, 13.5), (0.5, 14.0), (0.5, 10.2)]),\n                np.array([0, 0, 0, 1, 0, 1, 1, 1, 1, 0])\n            ),\n            \"coarse\": (\n                np.array([(0.45, 11.0), (0.65, 12.5), (0.5, 13.5), (0.6, 14.0), (0.3, 10.5)]),\n                np.array([0, 1, 1, 1, 0])\n            )\n        },\n        {\n            \"fine\": (\n                np.array([(0.05, 10.0), (0.1, 10.0), (0.9, 10.0), (0.95, 10.0), (0.85, 10.0), (0.15, 10.0)]),\n                np.array([0, 0, 1, 1, 1, 0])\n            ),\n            \"coarse\": (\n                np.array([(0.5, 10.0), (0.8, 10.0), (0.2, 10.0)]),\n                np.array([1, 1, 0])\n            )\n        },\n        {\n            \"fine\": (\n                np.array([(0.5, 20.0), (0.5, 20.0), (0.5, 20.0), (0.5, 20.0)]),\n                np.array([0, 1, 0, 1])\n            ),\n            \"coarse\": (\n                np.array([(0.5, 20.0), (0.5, 20.0)]),\n                np.array([0, 1])\n            )\n        }\n    ]\n\n    lambda_penalty = 1e-2\n    prob_clip_val = 1e-12\n    const_prob_std_thresh = 1e-12\n    auc_diff_thresh = 0.1\n\n    def _sigmoid(z):\n        # Clip to avoid overflow in exp\n        z = np.clip(z, -500, 500)\n        return 1 / (1 + np.exp(-z))\n\n    def _compute_auc(y_true, y_pred):\n        pos_preds = y_pred[y_true == 1]\n        neg_preds = y_pred[y_true == 0]\n        \n        n_pos = len(pos_preds)\n        n_neg = len(neg_preds)\n\n        if n_pos == 0 or n_neg == 0:\n            return 0.5 # Return a neutral value if no pairs to compare\n\n        total_pairs = n_pos * n_neg\n        \n        # Broadcasting for efficient pair-wise comparison\n        comparison_matrix = pos_preds[:, np.newaxis] > neg_preds\n        tie_matrix = pos_preds[:, np.newaxis] == neg_preds\n        \n        score = np.sum(comparison_matrix) + 0.5 * np.sum(tie_matrix)\n        \n        return score / total_pairs\n\n    def _compute_log_loss(y_true, y_pred):\n        y_pred_clipped = np.clip(y_pred, prob_clip_val, 1 - prob_clip_val)\n        loss = -np.mean(y_true * np.log(y_pred_clipped) + (1 - y_true) * np.log(1 - y_pred_clipped))\n        return loss\n\n    def fit_and_evaluate(X, y, l2_lambda):\n        n_samples, n_features = X.shape\n        \n        # Standardize predictors\n        mean = np.mean(X, axis=0)\n        std = np.std(X, axis=0)\n        \n        is_degenerate_predictor = np.all(std  1e-9)\n\n        X_std = np.zeros_like(X)\n        non_zero_std_mask = std > 1e-9\n        if np.any(non_zero_std_mask):\n            X_std[:, non_zero_std_mask] = (X[:, non_zero_std_mask] - mean[non_zero_std_mask]) / std[non_zero_std_mask]\n\n        X_aug = np.hstack([np.ones((n_samples, 1)), X_std])\n\n        def neg_penalized_log_likelihood(theta, X_a, y_obs, lmbd):\n            eta = X_a @ theta\n            eta = np.clip(eta, -500, 500) # Clip for numerical stability\n            log_lik = np.sum(y_obs * eta - np.log(1 + np.exp(eta)))\n            penalty = (lmbd / 2) * np.sum(theta[1:]**2) # No penalty on intercept theta[0]\n            return -(log_lik - penalty)\n\n        theta_initial = np.zeros(n_features + 1)\n        res = minimize(neg_penalized_log_likelihood, theta_initial, args=(X_aug, y, l2_lambda), method='BFGS')\n        theta_opt = res.x\n        \n        # Predict probabilities\n        p_hat = _sigmoid(X_aug @ theta_opt)\n        \n        is_degenerate_prediction = np.std(p_hat)  const_prob_std_thresh\n        is_degenerate = is_degenerate_predictor or is_degenerate_prediction\n\n        # Compute metrics\n        auc = _compute_auc(y, p_hat)\n        log_loss = _compute_log_loss(y, p_hat)\n        \n        return auc, log_loss, is_degenerate\n\n    final_results = []\n    for case in test_cases:\n        X_fine, y_fine = case[\"fine\"]\n        X_coarse, y_coarse = case[\"coarse\"]\n\n        auc_fine, ll_fine, deg_fine = fit_and_evaluate(X_fine, y_fine, lambda_penalty)\n        auc_coarse, ll_coarse, deg_coarse = fit_and_evaluate(X_coarse, y_coarse, lambda_penalty)\n\n        # Interpretation code logic\n        if deg_fine and deg_coarse:\n            code = 3\n        else:\n            delta_auc = auc_fine - auc_coarse\n            if delta_auc >= auc_diff_thresh:\n                code = 1\n            elif delta_auc = -auc_diff_thresh:\n                code = 2\n            else:\n                code = 0\n                \n        result_list = [auc_fine, auc_coarse, ll_fine, ll_coarse, code]\n        final_results.append(result_list)\n\n    # Format the final output string\n    formatted_cases = []\n    for res in final_results:\n        # Round all numeric values to 4 decimal places\n        formatted_res = [f\"{val:.4f}\" if isinstance(val, float) else str(val) for val in res]\n        formatted_cases.append(f\"[{','.join(formatted_res)}]\")\n    \n    print(f\"[{','.join(formatted_cases)}]\")\n\nsolve()\n```"
        }
    ]
}