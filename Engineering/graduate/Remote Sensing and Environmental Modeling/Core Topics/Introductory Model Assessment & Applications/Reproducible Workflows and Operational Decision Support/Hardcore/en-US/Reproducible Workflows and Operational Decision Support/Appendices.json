{
    "hands_on_practices": [
        {
            "introduction": "Long-running simulations, such as those in climate and environmental modeling, are vulnerable to interruptions. A robust operational workflow must be able to pause and resume these models without compromising the integrity of the results. This practice guides you through the fundamental principles of creating a checkpointing system that guarantees bitwise-identical resumption by capturing the complete simulation state, including model variables, random number generator state, and environment configuration . Mastering this is the first step toward building verifiable and fault-tolerant decision support systems.",
            "id": "3841878",
            "problem": "You are tasked with designing and implementing a reproducible checkpointing and resumption workflow for a long-running climate model used in operational decision support in remote sensing and environmental modeling. The workflow must guarantee exact resumption, including identical stochastic behavior and environment-dependent forcing, producing bitwise-identical outputs when resumed at any valid checkpoint. You must also implement a rigorous equivalence validation procedure. The final output of your program must be a single line that aggregates the results of a predefined test suite in the specified format.\n\nBegin from the following fundamental base:\n\n- A discrete-time, finite-dimensional, stochastic dynamical system evolves by a rule that is a function of its current state and forcing. For a system state $S_t \\in \\mathbb{R}^{m \\times n}$ at integer time $t$, there exists a deterministic update function $F$ and a stochastic innovation process $\\varepsilon_t$ such that\n$$\nS_{t+1} = F(S_t, t, \\Theta, E) + \\varepsilon_t\n$$\nwhere $\\Theta$ denotes model parameters and $E$ denotes environment configuration.\n\n- A Random Number Generator (RNG) is a deterministic state machine. Given an RNG internal state $R_t$, the next output $\\xi_t$ and next state $R_{t+1}$ satisfy\n$$\n(\\xi_t, R_{t+1}) = \\Psi(R_t)\n$$\nfor a deterministic transition $\\Psi$. Therefore, if the RNG state $R_t$ is exactly restored at any checkpoint time $t$, and the same sequence of requests to the RNG is made thereafter, the stochastic realization $\\{\\varepsilon_{t}, \\varepsilon_{t+1}, \\dots\\}$ is exactly reproduced.\n\n- The discrete Laplacian operator on a two-dimensional grid with periodic boundary conditions is defined by\n$$\n(\\Delta S)_{i,j} = -4 S_{i,j} + S_{i-1,j} + S_{i+1,j} + S_{i,j-1} + S_{i,j+1}\n$$\nwith indices interpreted modulo the grid dimensions.\n\nYour climate model is specified as follows:\n\n- Let the model grid be of size $m \\times n$ and the state $S_t$ represent surface air temperature in Kelvin (K). The model evolves according to\n$$\nS_{t+1} = S_t + \\Delta t \\left( \\kappa \\, \\Delta S_t + \\alpha \\, \\mathcal{F}(t, E) \\right) + \\beta \\, \\eta_t\n$$\nwhere $\\Delta t$ is the time step, $\\kappa$ is the diffusion coefficient, $\\alpha$ is the deterministic forcing coefficient, $\\beta$ scales the stochastic innovation, and $\\eta_t$ is a zero-mean Gaussian random field. The deterministic forcing $\\mathcal{F}(t, E)$ is defined by\n$$\n\\mathcal{F}(t, E) = A(E) \\sin\\left( \\frac{2\\pi t}{T_{\\text{season}}} \\right) P\n$$\nwhere $A(E)$ is an environment-dependent amplitude, $T_{\\text{season}}$ is a seasonal period in steps, and $P$ is a fixed spatial pattern with all entries equal to $1$.\n\n- The environment $E$ contains a scenario key with two valid values: $E.\\text{SCENARIO} \\in \\{\\text{\"A\"}, \\text{\"B\"}\\}$. The amplitude map is $A(\\text{\"A\"}) = 0.5$ and $A(\\text{\"B\"}) = 0.8$ (both in Kelvin per step). The seasonal period is $T_{\\text{season}} = 24$ steps.\n\n- The stochastic innovation $\\eta_t$ is drawn independently at each step and grid cell as $\\eta_t \\sim \\mathcal{N}(0, \\sigma^2)$ with $\\sigma = 1$ (in Kelvin). All additions and multiplications are carried out in double precision.\n\nA checkpoint at time $t = k$ must contain:\n\n- The exact model state $S_k$.\n- The exact RNG internal state $R_k$.\n- The environment snapshot $E_k$.\n- The time index $k$.\n- The immutable model parameters $\\Theta$ used throughout the run.\n\nResumption from a checkpoint must set $(S_k, R_k, E_k)$ and continue the time loop for $t = k, k+1, \\dots, N-1$, producing a final state $S_N$ that is bitwise-identical to the state obtained by running the model uninterrupted for $N$ steps with the same initial conditions, RNG seed, parameters, and environment.\n\nValidation of equivalence must include:\n\n- Bitwise equality check of the final states $S_N^{\\text{full}}$ and $S_N^{\\text{resumed}}$,\n$$\n\\text{equal} = \\left( S_N^{\\text{full}} \\equiv S_N^{\\text{resumed}} \\right)\n$$\nwhere $\\equiv$ denotes elementwise exact equality in binary representation.\n- A norm-based diagnostic, such as the infinity norm of the difference,\n$$\n\\| S_N^{\\text{full}} - S_N^{\\text{resumed}} \\|_\\infty\n$$\nwhich must be exactly $0$ when the resumption is correct.\n\nYour implementation must adhere to the following constraints:\n\n- Use a single RNG and ensure all stochastic draws are exclusively derived from it, with its internal state captured and restored at checkpoints.\n- Use periodic boundary conditions for the Laplacian.\n- Use double-precision floating-point arithmetic for all computations.\n- Treat the environment $E$ as part of the deterministic model configuration; exact resumption must restore the environment snapshot taken at the checkpoint and use it for all subsequent steps.\n\nTest suite:\n\nImplement your program to run the following four test cases. In each case, produce a boolean indicating whether exact equivalence is achieved.\n\n- Case $1$ (happy path): $m = 8$, $n = 8$, $N = 50$, $k = 20$, seed $= 12345$, $E.\\text{SCENARIO} = \\text{\"A\"}$. Perform checkpoint-resume with environment restoration. Expected output is a boolean.\n- Case $2$ (boundary: immediate resume): $m = 8$, $n = 8$, $N = 50$, $k = 0$, seed $= 54321$, $E.\\text{SCENARIO} = \\text{\"A\"}$. Perform checkpoint-resume with environment restoration. Expected output is a boolean.\n- Case $3$ (boundary: final-step checkpoint): $m = 8$, $n = 8$, $N = 50$, $k = 50$, seed $= 11111$, $E.\\text{SCENARIO} = \\text{\"A\"}$. Perform checkpoint-resume with environment restoration. Expected output is a boolean.\n- Case $4$ (edge: environment drift after checkpoint): $m = 8$, $n = 8$, $N = 50$, $k = 25$, seed $= 22222$, initial $E.\\text{SCENARIO} = \\text{\"A\"}$, but after checkpoint the environment is changed to $E.\\text{SCENARIO} = \\text{\"B\"}$ and not restored from the checkpoint snapshot. Perform checkpoint-resume without environment restoration to demonstrate detection of inequality. Expected output is a boolean.\n\nModel parameters are the same for all cases: $\\Delta t = 0.1$, $\\kappa = 0.2$, $\\alpha = 1.0$, $\\beta = 0.05$, $\\sigma = 1.0$, $T_{\\text{season}} = 24$, base temperature $T_0 = 288.0$ K, and initial condition\n$$\nS_0 = T_0 \\mathbf{1} + 0.01 \\cdot \\zeta,\n$$\nwhere $\\zeta$ is an independent standard normal field generated by the same RNG.\n\nOutput specification:\n\n- Your program should produce a single line of output containing the four booleans for the test suite in order, as a comma-separated list enclosed in square brackets (for example, $[\\text{True},\\text{True},\\text{True},\\text{False}]$). No other text should be printed.",
            "solution": "The problem requires the design and implementation of a reproducible checkpointing and resumption workflow for a stochastic climate model. The core challenge is to ensure that a simulation resumed from a checkpoint at time $t=k$ produces a final state $S_N$ that is bitwise-identical to the final state of an uninterrupted simulation. This is a critical requirement for verifiable and reproducible science, particularly in operational contexts where long-running models may need to be paused and resumed.\n\nThe solution is based on the principle that a deterministic computer simulation, even one modeling stochastic processes, is fundamentally a deterministic state machine. Reproducibility is guaranteed if the entire state of the simulation is captured and restored perfectly.\n\n**1. State Definition and Encapsulation**\n\nThe complete state of the simulation at any integer time step $t$ consists of several components:\n-   The model state variable, $S_t \\in \\mathbb{R}^{m \\times n}$, which is the grid of surface air temperatures.\n-   The internal state of the Random Number Generator (RNG), denoted as $R_t$. Since the RNG is a deterministic algorithm, restoring its state $R_t$ ensures that the subsequent sequence of pseudo-random numbers is identical to the original sequence.\n-   The environment configuration, $E_t$. In this problem, this is the scenario key, $E.\\text{SCENARIO}$, which affects the deterministic forcing.\n-   The current time index, $t$.\n-   The immutable model parameters, $\\Theta = \\{\\Delta t, \\kappa, \\alpha, \\beta, \\sigma, T_{\\text{season}}, T_0 \\}$. While immutable, they are part of the simulation's definition.\n\nA checkpoint at time $t=k$ is a snapshot containing $\\{S_k, R_k, E_k, k\\}$. For resumption to be exact, every one of these components must be restored before continuing the simulation.\n\n**2. Simulation Engine**\n\nThe model evolves according to the discrete-time equation:\n$$\nS_{t+1} = S_t + \\Delta t \\left( \\kappa \\, \\Delta S_t + \\alpha \\, \\mathcal{F}(t, E) \\right) + \\beta \\, \\eta_t\n$$\nAn `update_step` function is implemented to advance the state from $S_t$ to $S_{t+1}$.\n\n-   **Discrete Laplacian ($\\Delta S_t$)**: The Laplacian operator with periodic boundary conditions is computed using `numpy.roll` for efficiency and clarity. For a state grid $S$, the operation is:\n    $$\n    (\\Delta S)_{i,j} = S_{i-1,j} + S_{i+1,j} + S_{i,j-1} + S_{i,j+1} - 4 S_{i,j}\n    $$\n    This is implemented by shifting the array $S$ along its axes.\n\n-   **Deterministic Forcing ($\\mathcal{F}(t, E)$)**: The forcing term is calculated as:\n    $$\n    \\mathcal{F}(t, E) = A(E) \\sin\\left( \\frac{2\\pi t}{T_{\\text{season}}} \\right) P\n    $$\n    where the amplitude $A(E)$ depends on the current environment $E$. For $E.\\text{SCENARIO} = \\text{\"A\"}$, $A=0.5$, and for $E.\\text{SCENARIO} = \\text{\"B\"}$, $A=0.8$. The spatial pattern $P$ is a matrix of ones.\n\n-   **Stochastic Innovation ($\\eta_t$)**: The random field $\\eta_t$ is generated at each step by drawing $m \\times n$ values from a Gaussian distribution $\\mathcal{N}(0, \\sigma^2)$ with $\\sigma=1$. This operation must use the single, stateful RNG instance for the simulation to ensure the sequence of draws is reproducible.\n\n**3. Checkpointing and Resumption Workflow**\n\nTo validate the workflow, we employ a dual-run strategy for each test case:\n\n-   **Full Run**: The simulation is initialized at $t=0$ with a given `seed` and environment $E_0$. It runs uninterrupted for $N$ steps, from $t=0$ to $t=N-1$, producing a final state $S_N^{\\text{full}}$. The RNG is seeded once at the beginning, and its state evolves throughout the simulation.\n\n-   **Resumed Run**:\n    1.  **Run to Checkpoint**: A separate simulation is initialized with the identical seed and $E_0$. It runs from $t=0$ to $t=k-1$.\n    2.  **Create Checkpoint**: At step $t=k$, before the state is updated, the complete simulation state is saved. This includes a copy of the model grid $S_k$, the RNG's internal state (obtained via `rng.bit_generator.state`), the time index $k$, and a copy of the environment configuration $E_k$.\n    3.  **Simulate Environment Drift (for Case 4)**: To test robustness, the 'live' environment state can be altered after the checkpoint is saved.\n    4.  **Resume from Checkpoint**: The saved checkpoint is loaded. The model grid $S$ is set to $S_k$. A new RNG instance has its internal state overwritten with the saved state $R_k$. The environment $E$ is restored to $E_k$ (unless intentionally omitted, as in Case 4).\n    5.  **Run to Completion**: The simulation continues from $t=k$ to $t=N-1$, using the restored state. This produces the final state $S_N^{\\text{resumed}}$.\n\n**4. Equivalence Validation**\n\nThe core of the validation is a bitwise comparison of the final states from both runs:\n$$\n\\text{equal} = \\left( S_N^{\\text{full}} \\equiv S_N^{\\text{resumed}} \\right)\n$$\nThis is implemented using `numpy.array_equal`, which returns `True` if and only if two arrays have the same shape and elements, which for floating-point numbers implies they are bit-for-bit identical. A `True` result confirms that the checkpoint-resume process was flawless. The infinity norm $\\|S_N^{\\text{full}} - S_N^{\\text{resumed}}\\|_\\infty$ must be exactly $0$.\n\n**5. Test Case Analysis**\n\n-   **Case 1 ($k=20$)**: The standard scenario validates that the workflow functions correctly for a checkpoint mid-simulation.\n-   **Case 2 ($k=0$)**: A boundary case where the checkpoint is taken at the initial step. The 'resumed' run effectively starts from the initial state, testing that the checkpointing logic correctly handles this edge case.\n-   **Case 3 ($k=50=N$)**: Another boundary case where the checkpoint is at the final step. The resumed run has no steps to execute, so the validation compares the final state of the full run with the state saved in the checkpoint.\n-   **Case 4 (Environment Drift)**: This is a negative control. The environment is altered after the checkpoint is created, and critically, the environment from the checkpoint is *not* restored during resumption. The simulation continues with the altered environment, leading to a different forcing term. This will cause the resumed simulation to diverge from the full run, correctly resulting in an inequality (`False`), demonstrating that the validation can detect failures in state restoration.\n\nThe successful execution of this test suite provides strong evidence that the designed workflow guarantees reproducibility under the specified conditions.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport copy\nfrom typing import Dict, Any, Tuple, Optional\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite for the climate model checkpointing workflow.\n    \"\"\"\n\n    def update_step(\n        S: np.ndarray,\n        t: int,\n        env: Dict[str, Any],\n        rng: np.random.Generator,\n        params: Dict[str, Any],\n        amplitude_map: Dict[str, float]\n    ) -> np.ndarray:\n        \"\"\"\n        Advances the model state by one time step.\n        \"\"\"\n        m, n = S.shape\n\n        # Laplacian with periodic boundary conditions\n        lap_S = (np.roll(S, 1, axis=0) + np.roll(S, -1, axis=0) +\n                 np.roll(S, 1, axis=1) + np.roll(S, -1, axis=1) - 4 * S)\n\n        # Deterministic forcing term\n        A = amplitude_map[env['SCENARIO']]\n        P = np.ones_like(S)\n        F_t_E = A * np.sin(2 * np.pi * t / params['T_season']) * P\n\n        # Stochastic innovation\n        eta_t = rng.normal(loc=0.0, scale=params['sigma'], size=(m, n))\n\n        # Full update equation\n        S_next = S + params['dt'] * (params['kappa'] * lap_S + params['alpha'] * F_t_E) + params['beta'] * eta_t\n        \n        return S_next\n\n    def run_simulation(\n        start_time: int,\n        end_time: int,\n        initial_S: np.ndarray,\n        rng: np.random.Generator,\n        env: Dict[str, Any],\n        params: Dict[str, Any],\n        amplitude_map: Dict[str, float]\n    ) -> np.ndarray:\n        \"\"\"\n        Runs the simulation for a given time interval.\n        \"\"\"\n        S = initial_S.copy()\n        for t in range(start_time, end_time):\n            S = update_step(S, t, env, rng, params, amplitude_map)\n        return S\n\n    def run_test_case(\n        m: int,\n        n: int,\n        N: int,\n        k: int,\n        seed: int,\n        initial_scenario: str,\n        restore_env: bool,\n        drift_scenario: Optional[str]\n    ) -> bool:\n        \"\"\"\n        Executes a single test case with a full run and a resumed run, then validates equivalence.\n        \"\"\"\n        # Define model parameters\n        params = {\n            'dt': 0.1, 'kappa': 0.2, 'alpha': 1.0, 'beta': 0.05,\n            'sigma': 1.0, 'T_season': 24, 'T0': 288.0\n        }\n        amplitude_map = {'A': 0.5, 'B': 0.8}\n\n        # --- Full Run ---\n        rng_full = np.random.default_rng(seed)\n        zeta_full = rng_full.normal(size=(m, n))\n        S0_full = params['T0'] + 0.01 * zeta_full\n        env_full = {'SCENARIO': initial_scenario}\n        S_N_full = run_simulation(0, N, S0_full, rng_full, env_full, params, amplitude_map)\n\n        # --- Resumed Run ---\n        # Part 1: Run up to checkpoint k\n        rng_resume = np.random.default_rng(seed)\n        zeta_resume = rng_resume.normal(size=(m, n))\n        S0_resume = params['T0'] + 0.01 * zeta_resume\n        env_live = {'SCENARIO': initial_scenario}\n        \n        S_at_k = run_simulation(0, k, S0_resume, rng_resume, env_live, params, amplitude_map)\n\n        # Part 2: Save checkpoint at t=k\n        checkpoint = {\n            'S': S_at_k.copy(),\n            'rng_state': copy.deepcopy(rng_resume.bit_generator.state),\n            't': k,\n            'env': copy.deepcopy(env_live)\n        }\n\n        # Part 3: Simulate environment drift after checkpoint (for Case 4)\n        if drift_scenario:\n            env_live['SCENARIO'] = drift_scenario\n            \n        # Part 4: Resume from checkpoint\n        S_from_cp = checkpoint['S']\n        t_from_cp = checkpoint['t']\n        \n        rng_from_cp = np.random.default_rng()\n        rng_from_cp.bit_generator.state = checkpoint['rng_state']\n        \n        if restore_env:\n            env_from_cp = checkpoint['env']\n        else:\n            env_from_cp = env_live # Use the potentially drifted live environment\n\n        # Part 5: Run from k to N\n        S_N_resumed = run_simulation(t_from_cp, N, S_from_cp, rng_from_cp, env_from_cp, params, amplitude_map)\n\n        # --- Validation ---\n        return np.array_equal(S_N_full, S_N_resumed)\n\n    # Test suite definition\n    test_cases = [\n        # Case 1: Happy path\n        {'m': 8, 'n': 8, 'N': 50, 'k': 20, 'seed': 12345, 'initial_scenario': 'A', 'restore_env': True, 'drift_scenario': None},\n        # Case 2: Boundary - immediate resume\n        {'m': 8, 'n': 8, 'N': 50, 'k': 0, 'seed': 54321, 'initial_scenario': 'A', 'restore_env': True, 'drift_scenario': None},\n        # Case 3: Boundary - final-step checkpoint\n        {'m': 8, 'n': 8, 'N': 50, 'k': 50, 'seed': 11111, 'initial_scenario': 'A', 'restore_env': True, 'drift_scenario': None},\n        # Case 4: Edge - environment drift detected\n        {'m': 8, 'n': 8, 'N': 50, 'k': 25, 'seed': 22222, 'initial_scenario': 'A', 'restore_env': False, 'drift_scenario': 'B'},\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_test_case(**case)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "While ensuring a single model run is reproducible is crucial, operational systems face continuous change from data updates, algorithm revisions, and evolving software environments. This exercise moves beyond a static view of reproducibility to a dynamic, risk-based perspective. You will construct a quantitative model that uses Poisson processes to estimate the probability of a reproducibility failure over time and evaluate how different control mechanisms, such as versioning and containerization, can mitigate these risks . This practice develops a strategic mindset for maintaining reproducibility throughout the entire lifecycle of a decision support system.",
            "id": "3841845",
            "problem": "A regional land cover classification workflow in remote sensing must be reproducible for operational decision support over a fixed deployment horizon. Reproducibility is compromised by three sources of variability: data updates, algorithm changes, and environment changes. A set of controls is maintained to mitigate these sources: data versioning, semantic versioning, container locks, and seed management. Construct a reproducibility requirement matrix that maps sources of variability to controls via control efficacy coefficients, then use it to determine whether the workflow configuration meets a quantitative reproducibility requirement under well-defined assumptions. The final program must compute the decision outcome for a specified test suite and produce a single-line aggregated output.\n\nFundamental base:\n- Model the occurrence of at least one reproducibility failure over a time horizon as a realization of a Poisson process (PP). For a single PP with rate $\\lambda$ in units $1/\\mathrm{month}$ over time $T$ in units $\\mathrm{month}$, the probability of at least one event is $1 - \\exp(-\\lambda T)$. Assume independence across sources so that hazards add.\n- Controls have source-specific efficacy, represented as fractions in $[0,1]$ indicating the proportion of variability suppressed when the control is fully adopted. Adoption levels are fractions $\\boldsymbol{u}$ in $[0,1]$ for each control. Controls act independently and multiplicatively on each source, so residual hazard for a source depends on the product of residual factors contributed by each control.\n\nDefinitions and requirements:\n- Let the sources index set be $i \\in \\{1,2,3\\}$ for data updates ($i=1$), algorithm changes ($i=2$), and environment changes ($i=3$).\n- Let the controls index set be $j \\in \\{1,2,3,4\\}$ for data versioning ($j=1$), semantic versioning ($j=2$), container locks ($j=3$), and seed management ($j=4$).\n- Let $\\boldsymbol{\\lambda} = (\\lambda_1,\\lambda_2,\\lambda_3)$ be the baseline hazard rates in units $1/\\mathrm{month}$ for each source.\n- Let $\\boldsymbol{u} = (u_1,u_2,u_3,u_4)$ be the adoption levels for the controls, with $u_j \\in [0,1]$.\n- Let $E \\in [0,1]^{3 \\times 4}$ be the reproducibility requirement matrix whose entry $E_{ij}$ is the efficacy of control $j$ on source $i$. Use the following matrix (rows are sources $i=1,2,3$; columns are controls $j=1,2,3,4$):\n$$\nE =\n\\begin{bmatrix}\n0.9 & 0.1 & 0.05 & 0.0 \\\\\n0.05 & 0.8 & 0.2 & 0.7 \\\\\n0.05 & 0.1 & 0.9 & 0.1\n\\end{bmatrix}.\n$$\n- The residual hazard for each source must be derived from the baseline hazard and the controls under the independence assumptions. The combined hazard over all sources must be obtained, and the probability of at least one failure over horizon $T$ must be computed. The workflow meets the reproducibility requirement if this probability is less than or equal to the threshold $\\tau$.\n- Use threshold $\\tau = 0.1$ (as a decimal) with no percentage sign.\n\nYour task:\n- Derive from first principles, using the properties of the Poisson process and independence of multiplicative control effects, a function that computes the probability of at least one reproducibility failure over horizon $T$ given $(\\boldsymbol{\\lambda}, \\boldsymbol{u}, E, T)$.\n- Implement a complete, runnable program that constructs $E$, evaluates the function for each test case below, and returns a boolean decision per case indicating whether the requirement is met (probability $\\le \\tau$).\n- Units: hazard rates in $1/\\mathrm{month}$ and time in $\\mathrm{month}$ must be respected.\n- Angle units are not applicable.\n- Express the outputs as booleans; do not use a percentage sign.\n\nTest suite:\n- Case $1$: $\\boldsymbol{\\lambda} = (0.2, 0.15, 0.1)$, $\\boldsymbol{u} = (1.0, 1.0, 1.0, 1.0)$, $T = 3.0$.\n- Case $2$: $\\boldsymbol{\\lambda} = (0.2, 0.15, 0.1)$, $\\boldsymbol{u} = (0.8, 0.7, 0.9, 0.5)$, $T = 1.0$.\n- Case $3$: $\\boldsymbol{\\lambda} = (0.2, 0.15, 0.1)$, $\\boldsymbol{u} = (0.0, 0.0, 0.0, 0.0)$, $T = 3.0$.\n- Case $4$: $\\boldsymbol{\\lambda} = (0.2, 0.15, 0.5)$, $\\boldsymbol{u} = (0.0, 0.0, 1.0, 0.0)$, $T = 2.0$.\n- Case $5$: $\\boldsymbol{\\lambda} = (0.05, 0.4, 0.05)$, $\\boldsymbol{u} = (0.0, 1.0, 0.5, 1.0)$, $T = 1.0$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $[x_1,x_2,x_3,x_4,x_5]$, where each $x_k$ is a boolean indicating whether the corresponding test case meets the reproducibility requirement.",
            "solution": "The problem as stated is subjected to validation against the established criteria.\n\n### Problem Validation\n\n**Step 1: Extracted Givens**\n- **Domain**: Reproducible workflows for operational decision support in remote sensing and environmental modeling.\n- **Variability Sources ($i$)**: $1$: data updates, $2$: algorithm changes, $3$: environment changes.\n- **Controls ($j$)**: $1$: data versioning, $2$: semantic versioning, $3$: container locks, $4$: seed management.\n- **Mathematical Model**:\n    - Failure occurrence is modeled as a Poisson process (PP).\n    - Probability of at least one event for a single PP with rate $\\lambda$ over time $T$ is $1 - \\exp(-\\lambda T)$.\n    - Sources are independent, so their hazard rates add.\n    - Controls act independently and multiplicatively.\n- **Parameters and Variables**:\n    - $\\boldsymbol{\\lambda} = (\\lambda_1, \\lambda_2, \\lambda_3)$: vector of baseline hazard rates in units $1/\\mathrm{month}$.\n    - $\\boldsymbol{u} = (u_1, u_2, u_3, u_4)$: vector of control adoption levels, where $u_j \\in [0,1]$.\n    - $E$: a $3 \\times 4$ reproducibility requirement matrix where $E_{ij}$ is the efficacy of control $j$ on source $i$, $E_{ij} \\in [0,1]$.\n- **Given Matrix $E$**:\n$$\nE =\n\\begin{bmatrix}\n0.9 & 0.1 & 0.05 & 0.0 \\\\\n0.05 & 0.8 & 0.2 & 0.7 \\\\\n0.05 & 0.1 & 0.9 & 0.1\n\\end{bmatrix}\n$$\n- **Decision Criterion**: The workflow meets the reproducibility requirement if the total probability of at least one failure over horizon $T$ is less than or equal to a threshold $\\tau$.\n- **Threshold**: $\\tau = 0.1$.\n- **Test Suite**:\n    - Case $1$: $\\boldsymbol{\\lambda} = (0.2, 0.15, 0.1)$, $\\boldsymbol{u} = (1.0, 1.0, 1.0, 1.0)$, $T = 3.0$.\n    - Case $2$: $\\boldsymbol{\\lambda} = (0.2, 0.15, 0.1)$, $\\boldsymbol{u} = (0.8, 0.7, 0.9, 0.5)$, $T = 1.0$.\n    - Case $3$: $\\boldsymbol{\\lambda} = (0.2, 0.15, 0.1)$, $\\boldsymbol{u} = (0.0, 0.0, 0.0, 0.0)$, $T = 3.0$.\n    - Case $4$: $\\boldsymbol{\\lambda} = (0.2, 0.15, 0.5)$, $\\boldsymbol{u} = (0.0, 0.0, 1.0, 0.0)$, $T = 2.0$.\n    - Case $5$: $\\boldsymbol{\\lambda} = (0.05, 0.4, 0.05)$, $\\boldsymbol{u} = (0.0, 1.0, 0.5, 1.0)$, $T = 1.0$.\n\n**Step 2: Validation Using Extracted Givens**\nThe problem is scientifically and mathematically well-defined. It uses the Poisson process, a standard stochastic model for event counting, and combines it with a clear, rule-based model for risk mitigation (multiplicative control effects, additive independent risks). The parameters are specified, the objective is unambiguous, and all required data are provided. The problem is a formalizable modeling exercise directly relevant to the specified domain. It does not violate any of the specified invalidity criteria.\n\n**Step 3: Verdict and Action**\nThe problem is deemed **valid**. A solution will be derived and implemented.\n\n### Derivation of the Solution\n\nThe objective is to derive a function for the probability of at least one reproducibility failure, $P(\\text{failure})$, given the baseline hazard rates $\\boldsymbol{\\lambda}$, control adoption levels $\\boldsymbol{u}$, control efficacy matrix $E$, and time horizon $T$. This derivation proceeds in three steps, based on the principles provided.\n\n**Step 1: Residual Hazard Rate for a Single Source**\n\nLet $\\lambda_i$ be the baseline hazard rate for source $i \\in \\{1, 2, 3\\}$. A set of controls $j \\in \\{1, 2, 3, 4\\}$ is applied to mitigate these hazards. The efficacy of control $j$ on source $i$ is $E_{ij}$, defined as the proportion of variability suppressed by the control at full adoption. The adoption level of control $j$ is $u_j \\in [0,1]$.\n\nThe effective reduction in hazard for source $i$ by control $j$ is proportional to both its efficacy and its adoption level, given by the product $u_j E_{ij}$. This represents the fraction of the hazard that is mitigated. Consequently, the fraction of the hazard that *remains* after applying control $j$ is $(1 - u_j E_{ij})$. This term is the residual factor for control $j$ on source $i$.\n\nThe problem states that controls act independently and multiplicatively. Therefore, to find the total residual hazard rate for source $i$, denoted $\\lambda'_i$, we must multiply the baseline rate $\\lambda_i$ by the product of all residual factors from each control.\n\n$$\n\\lambda'_i = \\lambda_i \\prod_{j=1}^{4} (1 - u_j E_{ij})\n$$\n\n**Step 2: Total Residual Hazard Rate**\n\nThe three sources of variability (data, algorithm, environment) are assumed to be independent. A fundamental property of independent Poisson processes is that their superposition (sum) is also a Poisson process. The rate of the resulting process is the sum of the rates of the individual processes.\n\nTherefore, the total residual hazard rate, $\\lambda'_{\\text{total}}$, which represents the rate of reproducibility failures from all sources combined, is the sum of the individual residual hazard rates $\\lambda'_i$.\n\n$$\n\\lambda'_{\\text{total}} = \\sum_{i=1}^{3} \\lambda'_i\n$$\n\nSubstituting the expression for $\\lambda'_i$ from Step 1 gives the complete formula for the total residual hazard rate:\n\n$$\n\\lambda'_{\\text{total}} = \\sum_{i=1}^{3} \\left[ \\lambda_i \\prod_{j=1}^{4} (1 - u_j E_{ij}) \\right]\n$$\n\n**Step 3: Probability of Failure over the Time Horizon**\n\nThe occurrence of at least one reproducibility failure over a time horizon $T$ is an event in a Poisson process with a constant rate $\\lambda'_{\\text{total}}$. For a Poisson process with rate $\\Lambda$, the probability of observing exactly $k$ events in an interval of length $T$ is given by the probability mass function:\n\n$$\nP(N(T) = k) = \\frac{(\\Lambda T)^k e^{-\\Lambda T}}{k!}\n$$\n\nHere, our rate is $\\Lambda = \\lambda'_{\\text{total}}$. We are interested in the probability of at least one event, $P(N(T) \\geq 1)$. This is the complement of observing zero events, $P(N(T) = 0)$.\n\n$$\nP(N(T) \\geq 1) = 1 - P(N(T) = 0)\n$$\n\nThe probability of zero events is:\n\n$$\nP(N(T) = 0) = \\frac{(\\lambda'_{\\text{total}} T)^0 e^{-\\lambda'_{\\text{total}} T}}{0!} = e^{-\\lambda'_{\\text{total}} T}\n$$\n\nTherefore, the final expression for the probability of at least one failure, $P(\\text{failure})$, is:\n\n$$\nP(\\text{failure}) = 1 - e^{-\\lambda'_{\\text{total}} T}\n$$\n\nSubstituting the full expression for $\\lambda'_{\\text{total}}$ yields the comprehensive function we must evaluate:\n\n$$\nP(\\text{failure} \\mid \\boldsymbol{\\lambda}, \\boldsymbol{u}, E, T) = 1 - \\exp\\left(-T \\sum_{i=1}^{3} \\left[ \\lambda_i \\prod_{j=1}^{4} (1 - u_j E_{ij}) \\right]\\right)\n$$\n\n**Decision Rule**\n\nThe workflow meets the reproducibility requirement if the computed probability of failure is less than or equal to the specified threshold $\\tau = 0.1$.\n\n$$\n\\text{Requirement Met} \\iff P(\\text{failure}) \\le 0.1\n$$\n\nThis completes the derivation. The logic will now be implemented to evaluate the provided test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the reproducibility requirement problem for a set of test cases.\n    \"\"\"\n\n    # Define the fundamental constants and matrices from the problem statement.\n    \n    # Efficacy matrix E (rows are sources i=1,2,3; columns are controls j=1,2,3,4).\n    # Sources i: 1=data, 2=algorithm, 3=environment\n    # Controls j: 1=data versioning, 2=semantic versioning, 3=container locks, 4=seed mgmt\n    E_matrix = np.array([\n        [0.9, 0.1, 0.05, 0.0],\n        [0.05, 0.8, 0.2, 0.7],\n        [0.05, 0.1, 0.9, 0.1]\n    ])\n\n    # Reproducibility failure probability threshold.\n    tau = 0.1\n\n    # Define the test suite. Each case is a tuple containing:\n    # 1. lambda_vec (baseline hazard rates)\n    # 2. u_vec (control adoption levels)\n    # 3. T (time horizon in months)\n    test_cases = [\n        # Case 1\n        (np.array([0.2, 0.15, 0.1]), np.array([1.0, 1.0, 1.0, 1.0]), 3.0),\n        # Case 2\n        (np.array([0.2, 0.15, 0.1]), np.array([0.8, 0.7, 0.9, 0.5]), 1.0),\n        # Case 3\n        (np.array([0.2, 0.15, 0.1]), np.array([0.0, 0.0, 0.0, 0.0]), 3.0),\n        # Case 4\n        (np.array([0.2, 0.15, 0.5]), np.array([0.0, 0.0, 1.0, 0.0]), 2.0),\n        # Case 5\n        (np.array([0.05, 0.4, 0.05]), np.array([0.0, 1.0, 0.5, 1.0]), 1.0),\n    ]\n\n    results = []\n    for lambda_vec, u_vec, T in test_cases:\n        # Step 1: Calculate the multiplicative residual factors.\n        # The term u_j * E_ij is computed by broadcasting u_vec (shape (4,))\n        # across the E_matrix (shape (3, 4)).\n        # The result is a (3, 4) matrix of (1 - u_j * E_ij) terms.\n        residual_factors_matrix = 1.0 - (u_vec * E_matrix)\n\n        # Step 2: Calculate the total residual multiplier for each source.\n        # This is the product of residual factors across controls (axis=1).\n        # The result is a vector of shape (3,) where each element i is Prod_j(1 - u_j * E_ij).\n        total_residual_multipliers = np.prod(residual_factors_matrix, axis=1)\n\n        # Step 3: Calculate the residual hazard rate for each source.\n        # This is an element-wise product of the baseline rates and the multipliers.\n        # lambda_prime_vec has shape (3,).\n        lambda_prime_vec = lambda_vec * total_residual_multipliers\n\n        # Step 4: Calculate the total residual hazard rate.\n        # This is the sum of independent Poisson process rates.\n        lambda_total_prime = np.sum(lambda_prime_vec)\n\n        # Step 5: Calculate the probability of at least one failure.\n        # P(failure) = 1 - exp(-lambda' * T)\n        prob_failure = 1.0 - np.exp(-lambda_total_prime * T)\n\n        # Step 6: Apply the decision rule.\n        # The requirement is met if the probability is less than or equal to the threshold tau.\n        decision = prob_failure <= tau\n        results.append(decision)\n\n    # Format the final output as a comma-separated list of booleans in brackets.\n    # The str() of a boolean in Python is 'True' or 'False'.\n    output_str = f\"[{','.join(map(str, results))}]\"\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "Ultimately, a key function of reproducible workflows is to provide a trusted basis for comparing scientific data products and making operational decisions. This hands-on exercise challenges you to design a rigorous statistical framework to determine if two NDVI products, derived from different software stacks, can be considered replicable. You will move beyond simple difference metrics by applying formal equivalence testing (TOST) and confidence bounds on spatial agreement, grounded in a clear error budget . This provides a defensible and quantitative methodology for product validation, a critical task in any operational environmental monitoring center.",
            "id": "3841888",
            "problem": "An environmental monitoring center must make operational decisions based on agreement between two Normalized Difference Vegetation Index (NDVI) products computed for the same geographic footprint and time window but using different software stacks. NDVI, the Normalized Difference Vegetation Index (NDVI), is dimensionless and bounded between $-1$ and $+1$. The goal is to design and implement a rigorous, reproducible workflow that decides whether the two NDVI products are replicable within a tolerance defined by an explicit error budget and a statistical equivalence criterion.\n\nConstruct a decision procedure grounded in first principles that simultaneously enforces the following two conditions.\n\n1. Statistical equivalence of mean bias: Use the Two One-Sided Tests (TOST) for equivalence of the mean pixelwise difference. Let $d_i = a_i - b_i$ denote the per-pixel difference, where $a_i$ and $b_i$ are the corresponding NDVI pixels from product $\\mathcal{A}$ and product $\\mathcal{B}$ for $i = 1, \\dots, n$. Let the per-case decision significance be $\\alpha$. The equivalence margin for the mean is $\\epsilon_{\\mu}$, which must be derived from the tolerance budget below and a specified policy factor. The test must conclude equivalence if and only if the $100 \\cdot (1 - 2\\alpha)$ percent confidence interval for the mean difference lies entirely within the interval $\\left[-\\epsilon_{\\mu}, \\epsilon_{\\mu}\\right]$.\n\n2. Operational tolerance on spatial agreement: Let $\\epsilon$ be the overall per-pixel absolute difference tolerance derived from the error budget below. Define $p$ as the fraction of pixels satisfying $|d_i| \\le \\epsilon$. Require operational replicability if and only if the one-sided $1 - \\alpha$ confidence lower bound for $p$ (computed exactly using the Clopper–Pearson method) is at least a target fraction $p_0$, where $p_0$ is set by policy.\n\nTolerance definition and error budget: The per-pixel tolerance $\\epsilon$ must be computed from root-sum-of-squares (RSS) combination of independent, well-characterized sources of uncertainty, consistent with propagation of independent random errors. For the three contributors below, define $\\epsilon$ as\n$$\n\\epsilon = \\sqrt{\\epsilon_{\\text{inst}}^2 + \\epsilon_{\\text{atm}}^2 + \\epsilon_{\\text{proc}}^2},\n$$\nwhere $\\epsilon_{\\text{inst}}$ is the contribution from instrument radiometric uncertainty mapped to NDVI, $\\epsilon_{\\text{atm}}$ is the contribution from residual atmospheric correction uncertainty mapped to NDVI, and $\\epsilon_{\\text{proc}}$ is the contribution from geolocation, resampling, and interpolation mismatches mapped to NDVI. The mean-equivalence margin is $\\epsilon_{\\mu} = \\lambda \\, \\epsilon$ for a specified policy factor $\\lambda \\in (0, 1]$.\n\nYour program must implement the above decision procedure for the following test suite. For each test case, construct products $\\mathcal{A}$ and $\\mathcal{B}$ deterministically using the given formulas, then decide replicability as a boolean. No physical units are required because NDVI is dimensionless. Angles appearing in trigonometric functions are in radians.\n\nAcross all test cases, use $\\alpha = 0.05$ and $\\lambda = 0.8$.\n\n- Case 1 (general “happy path”): Let $n = 200$. For $i = 0, 1, \\dots, n-1$,\n  $$\n  a_i = 0.6 + 0.2 \\sin\\left(\\frac{2\\pi i}{n}\\right) - 0.1 \\sin\\left(\\frac{4\\pi i}{n}\\right),\n  $$\n  $$\n  b_i = a_i + 0.005 + 0.01 \\cos\\left(\\frac{2\\pi i}{n}\\right).\n  $$\n  Error budget components are $\\epsilon_{\\text{inst}} = 0.020$, $\\epsilon_{\\text{atm}} = 0.015$, $\\epsilon_{\\text{proc}} = 0.010$. The operational fraction target is $p_0 = 0.90$.\n\n- Case 2 (stringent tolerance, should fail): Let $n = 200$. For $i = 0, 1, \\dots, n-1$,\n  $$\n  a_i = 0.5 + 0.25 \\sin\\left(\\frac{2\\pi i}{n}\\right),\n  $$\n  $$\n  b_i = a_i + 0.040 + 0.020 \\sin\\left(\\frac{6\\pi i}{n}\\right).\n  $$\n  Error budget components are $\\epsilon_{\\text{inst}} = 0.010$, $\\epsilon_{\\text{atm}} = 0.005$, $\\epsilon_{\\text{proc}} = 0.005$. The operational fraction target is $p_0 = 0.95$.\n\n- Case 3 (small sample size, should pass): Let $n = 12$. For $i = 0, 1, \\dots, n-1$,\n  $$\n  a_i = 0.4 + 0.1 \\sin\\left(\\frac{2\\pi i}{n}\\right),\n  $$\n  $$\n  b_i = a_i + 0.002 + 0.003 \\sin\\left(\\frac{4\\pi i}{n}\\right).\n  $$\n  Error budget components are $\\epsilon_{\\text{inst}} = 0.020$, $\\epsilon_{\\text{atm}} = 0.008$, $\\epsilon_{\\text{proc}} = 0.005$. The operational fraction target is $p_0 = 0.85$.\n\n- Case 4 (borderline systematic bias): Let $n = 300$. For $i = 0, 1, \\dots, n-1$,\n  $$\n  a_i = 0.55 + 0.15 \\sin\\left(\\frac{2\\pi i}{n}\\right) - 0.05 \\cos\\left(\\frac{8\\pi i}{n}\\right),\n  $$\n  $$\n  b_i = a_i + 0.020.\n  $$\n  Error budget components are $\\epsilon_{\\text{inst}} = 0.010$, $\\epsilon_{\\text{atm}} = 0.015$, $\\epsilon_{\\text{proc}} = 0.010$. The operational fraction target is $p_0 = 0.95$.\n\nDecision and output specification: For each case, return a boolean value that is true if and only if both conditions 1 and 2 above are satisfied. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result_1, result_2, result_3, result_4]$), where each $result_k$ is either $True$ or $False$ for the corresponding test case in the exact order listed above. No other text should be printed.",
            "solution": "The problem requires the design and implementation of a reproducible decision-making workflow to assess the replicability of two Normalized Difference Vegetation Index (NDVI) products, designated $\\mathcal{A}$ and $\\mathcal{B}$. Replicability is determined by simultaneously satisfying two distinct statistical criteria: one concerning the mean bias between the products and another concerning the spatial agreement of per-pixel differences. The entire procedure is performed at a specified significance level $\\alpha$.\n\nLet the two NDVI products be represented by arrays of pixel values $\\{a_i\\}_{i=1}^n$ and $\\{b_i\\}_{i=1}^n$ for $n$ corresponding pixels. The per-pixel difference is defined as $d_i = a_i - b_i$.\n\nThe core of the decision procedure is a two-part hypothesis test. A finding of replicability requires both conditions to be met.\n\n**Condition 1: Statistical Equivalence of Mean Bias**\n\nThis condition evaluates whether the average difference between the two products is negligibly small. We use the Two One-Sided Tests (TOST) procedure for equivalence. The null hypotheses for the two tests are that the true mean difference, $\\mu_d$, is outside an equivalence margin $[-\\epsilon_{\\mu}, \\epsilon_{\\mu}]$. We test $H_{0,1}: \\mu_d \\ge \\epsilon_{\\mu}$ and $H_{0,2}: \\mu_d \\le -\\epsilon_{\\mu}$. We can conclude equivalence (i.e., reject both nulls) if the $100 \\cdot (1 - 2\\alpha)\\%$ confidence interval for $\\mu_d$ is contained entirely within the interval $[-\\epsilon_{\\mu}, \\epsilon_{\\mu}]$.\n\nThe equivalence margin for the mean, $\\epsilon_{\\mu}$, is derived from a total per-pixel tolerance, $\\epsilon$, and a policy factor, $\\lambda \\in (0, 1]$. The total tolerance $\\epsilon$ is computed by combining independent sources of error—instrumental ($\\epsilon_{\\text{inst}}$), atmospheric ($\\epsilon_{\\text{atm}}$), and processing ($\\epsilon_{\\text{proc}}$)—using the root-sum-of-squares (RSS) method, which is standard for the propagation of independent random errors:\n$$\n\\epsilon = \\sqrt{\\epsilon_{\\text{inst}}^2 + \\epsilon_{\\text{atm}}^2 + \\epsilon_{\\text{proc}}^2}\n$$\nThe mean equivalence margin is then a fraction of this total tolerance:\n$$\n\\epsilon_{\\mu} = \\lambda \\, \\epsilon\n$$\nLet $\\bar{d}$ be the sample mean of the differences $d_i$, and let $s_d$ be the sample standard deviation. For a sample size $n > 1$, the standard error of the mean is $SE = s_d / \\sqrt{n}$. The $100 \\cdot (1 - 2\\alpha)\\%$ confidence interval for $\\mu_d$ is given by:\n$$\nCI_{1-2\\alpha} = \\left[ \\bar{d} - t_{1-\\alpha, n-1} \\frac{s_d}{\\sqrt{n}}, \\bar{d} + t_{1-\\alpha, n-1} \\frac{s_d}{\\sqrt{n}} \\right]\n$$\nwhere $t_{1-\\alpha, n-1}$ is the critical value of the Student's t-distribution with $n-1$ degrees of freedom that is exceeded with probability $\\alpha$. The condition for equivalence is met if and only if:\n$$\n\\left( \\bar{d} + t_{1-\\alpha, n-1} \\frac{s_d}{\\sqrt{n}} \\right) < \\epsilon_{\\mu} \\quad \\text{AND} \\quad \\left( \\bar{d} - t_{1-\\alpha, n-1} \\frac{s_d}{\\sqrt{n}} \\right) > -\\epsilon_{\\mu}\n$$\nIf $s_d = 0$, the confidence interval collapses to the point estimate $[\\bar{d}, \\bar{d}]$, and the test correctly judges equivalence based on whether this point falls within $(-\\epsilon_{\\mu}, \\epsilon_{\\mu})$.\n\n**Condition 2: Operational Tolerance on Spatial Agreement**\n\nThis condition ensures that a sufficiently large fraction of individual pixels agree within the overall tolerance $\\epsilon$. Let $p$ be the true proportion of pixels in the population for which the absolute difference is within tolerance, i.e., $|d_i| \\le \\epsilon$. We estimate this proportion from the sample of $n$ pixels. Let $k$ be the number of pixels in the sample that satisfy $|d_i| \\le \\epsilon$. The sample proportion is $\\hat{p} = k/n$.\n\nTo account for sampling uncertainty, the decision is based not on the point estimate $\\hat{p}$, but on a conservative lower bound for the true proportion $p$. We compute the one-sided $1-\\alpha$ confidence lower bound for $p$, denoted $p_{\\text{lower}}$. The problem specifies the use of the exact Clopper-Pearson method. This method defines the lower bound $p_{\\text{lower}}$ as the value of $p$ for which the probability of observing $k$ or more \"successes\" (pixels within tolerance) in $n$ trials is equal to $\\alpha$. This bound is computed using the quantile function (inverse CDF) of the Beta distribution:\n$$\np_{\\text{lower}} = \\text{Beta.ppf}(\\alpha; k, n - k + 1)\n$$\nwhere $\\text{Beta.ppf}(\\cdot; a, b)$ is the quantile function of the Beta distribution with shape parameters $a$ and $b$. A special case arises if $k=0$, for which the lower bound is correctly defined as $p_{\\text{lower}} = 0$.\n\nThe condition for operational replicability is met if and only if this lower bound meets or exceeds a predefined policy target fraction, $p_0$:\n$$\np_{\\text{lower}} \\ge p_0\n$$\n\n**Final Decision**\nFor each test case, replicability is affirmed (the result is `True`) if and only if both Condition 1 and Condition 2 are satisfied. If either condition fails, the result is `False`. The implementation will proceed by calculating the necessary quantities for each test case and applying these two decision rules.",
            "answer": "```python\nimport numpy as np\nfrom scipy import stats\n\ndef solve():\n    \"\"\"\n    Main function to execute the decision procedure for all test cases.\n    \"\"\"\n\n    def evaluate_replicability(\n        a_vals, b_vals, n, eps_inst, eps_atm, eps_proc, p0, alpha, lambda_val\n    ):\n        \"\"\"\n        Evaluates replicability for a single test case based on the two conditions.\n\n        Args:\n            a_vals (np.ndarray): NDVI values for product A.\n            b_vals (np.ndarray): NDVI values for product B.\n            n (int): Number of pixels.\n            eps_inst (float): Instrument error component.\n            eps_atm (float): Atmospheric error component.\n            eps_proc (float): Processing error component.\n            p0 (float): Target fraction for spatial agreement.\n            alpha (float): Significance level.\n            lambda_val (float): Policy factor for mean equivalence margin.\n\n        Returns:\n            bool: True if both conditions are met, False otherwise.\n        \"\"\"\n        # Calculate per-pixel differences\n        d = a_vals - b_vals\n\n        # --- Condition 1: Statistical equivalence of mean bias (TOST) ---\n\n        # Calculate total tolerance and mean equivalence margin\n        epsilon = np.sqrt(eps_inst**2 + eps_atm**2 + eps_proc**2)\n        epsilon_mu = lambda_val * epsilon\n\n        # Calculate sample statistics for the differences\n        d_mean = np.mean(d)\n        d_std = np.std(d, ddof=1)\n        \n        # Handle the case of n<=1, although not present in test cases\n        if n <= 1:\n            cond1_passed = False\n        # Handle the case where all differences are identical (std dev is 0)\n        elif d_std == 0.0:\n            ci_lower = d_mean\n            ci_upper = d_mean\n            cond1_passed = (ci_lower > -epsilon_mu) and (ci_upper < epsilon_mu)\n        else:\n            # Calculate standard error and critical t-value\n            se_mean = d_std / np.sqrt(n)\n            t_crit = stats.t.ppf(1 - alpha, df=n - 1)\n            \n            # Calculate the 100*(1-2*alpha)% confidence interval\n            ci_half_width = t_crit * se_mean\n            ci_lower = d_mean - ci_half_width\n            ci_upper = d_mean + ci_half_width\n            \n            # Check if CI is within the equivalence margin\n            cond1_passed = (ci_lower > -epsilon_mu) and (ci_upper < epsilon_mu)\n\n        # --- Condition 2: Operational tolerance on spatial agreement ---\n\n        # Count pixels within the absolute difference tolerance epsilon\n        k = np.sum(np.abs(d) <= epsilon)\n\n        # Calculate the one-sided 1-alpha Clopper-Pearson lower bound for the proportion p\n        if k == 0:\n            p_lower = 0.0\n        else:\n            p_lower = stats.beta.ppf(alpha, k, n - k + 1)\n        \n        # Check if the lower bound meets the target fraction p0\n        cond2_passed = (p_lower >= p0)\n\n        # Final decision: replicable only if both conditions pass\n        return cond1_passed and cond2_passed\n\n    # Global parameters for all cases\n    ALPHA = 0.05\n    LAMBDA = 0.8\n\n    # Define test cases\n    test_cases = [\n        {\n            \"n\": 200, \"p0\": 0.90, \"eps_inst\": 0.020, \"eps_atm\": 0.015, \"eps_proc\": 0.010,\n            \"a_func\": lambda i, n: 0.6 + 0.2 * np.sin(2 * np.pi * i / n) - 0.1 * np.sin(4 * np.pi * i / n),\n            \"b_func\": lambda a_i, i, n: a_i + 0.005 + 0.01 * np.cos(2 * np.pi * i / n),\n        },\n        {\n            \"n\": 200, \"p0\": 0.95, \"eps_inst\": 0.010, \"eps_atm\": 0.005, \"eps_proc\": 0.005,\n            \"a_func\": lambda i, n: 0.5 + 0.25 * np.sin(2 * np.pi * i / n),\n            \"b_func\": lambda a_i, i, n: a_i + 0.040 + 0.020 * np.sin(6 * np.pi * i / n),\n        },\n        {\n            \"n\": 12, \"p0\": 0.85, \"eps_inst\": 0.020, \"eps_atm\": 0.008, \"eps_proc\": 0.005,\n            \"a_func\": lambda i, n: 0.4 + 0.1 * np.sin(2 * np.pi * i / n),\n            \"b_func\": lambda a_i, i, n: a_i + 0.002 + 0.003 * np.sin(4 * np.pi * i / n),\n        },\n        {\n            \"n\": 300, \"p0\": 0.95, \"eps_inst\": 0.010, \"eps_atm\": 0.015, \"eps_proc\": 0.010,\n            \"a_func\": lambda i, n: 0.55 + 0.15 * np.sin(2 * np.pi * i / n) - 0.05 * np.cos(8 * np.pi * i / n),\n            \"b_func\": lambda a_i, i, n: a_i + 0.020,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        n = case[\"n\"]\n        i_vals = np.arange(n)\n        a_vals = case[\"a_func\"](i_vals, n)\n        b_vals = case[\"b_func\"](a_vals, i_vals, n)\n        \n        result = evaluate_replicability(\n            a_vals, b_vals, n, case[\"eps_inst\"], case[\"eps_atm\"], \n            case[\"eps_proc\"], case[\"p0\"], ALPHA, LAMBDA\n        )\n        results.append(result)\n\n    # Print final results in the required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}