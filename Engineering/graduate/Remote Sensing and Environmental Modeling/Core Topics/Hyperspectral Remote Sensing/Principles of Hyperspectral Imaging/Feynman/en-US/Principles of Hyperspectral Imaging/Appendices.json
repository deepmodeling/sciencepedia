{
    "hands_on_practices": [
        {
            "introduction": "A sensor does not measure a spectrum at an infinitesimally sharp wavelength, but rather over a finite range described by the Instrument Line Spread Function (ILS). This exercise explores the crucial impact of the ILS shape on the measured signal by comparing a realistic Gaussian model with a simplified rectangular one. By deriving the measurement bias for a curved spectrum, you will gain a deeper appreciation for why precise instrument characterization is fundamental to quantitative analysis .",
            "id": "3835466",
            "problem": "A hyperspectral pushbroom sensor samples the upwelling surface reflectance spectrum with a finite spectral response described by the Instrument Line Spread Function (ILS). The measured reflectance at a band center wavelength $\\lambda_{0}$ is modeled as a convolution of the high-resolution library spectrum $R(\\lambda)$ with the ILS, assuming linear system response and energy conservation across the spectral response. Let $R(\\lambda)$ be locally smooth around $\\lambda_{0}$ and consider its second-order Taylor expansion $R(\\lambda) = R_{0} + a(\\lambda - \\lambda_{0}) + b(\\lambda - \\lambda_{0})^{2}$, where $R_{0}$, $a$, and $b$ are constants. Suppose the true ILS is Gaussian, given by\n$$\nI_{G}(\\Delta\\lambda) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\!\\left(-\\frac{\\Delta\\lambda^{2}}{2\\sigma^{2}}\\right),\n$$\nwith full width at half maximum $\\mathrm{FWHM}_{G} = 2\\sqrt{2\\ln 2}\\,\\sigma$. Consider an approximate processing approach that replaces the Gaussian ILS with a rectangular ILS of identical full width at half maximum,\n$$\nI_{R}(\\Delta\\lambda) = \\begin{cases}\n\\frac{1}{W}, & |\\Delta\\lambda| \\leq \\frac{W}{2}, \\\\\n0, & \\text{otherwise},\n\\end{cases}\n$$\nwhere $W = \\mathrm{FWHM}_{G}$.\n\nStarting from the measurement model of hyperspectral sensing as the convolution of $R(\\lambda)$ with a normalized ILS, derive the measured reflectance at $\\lambda_{0}$ for both the Gaussian and the rectangular ILS using the given local quadratic representation of $R(\\lambda)$. Then, using the same $\\mathrm{FWHM}$ for both ILS models, derive an analytic expression for the bias $\\Delta R = R_{\\mathrm{meas},R}(\\lambda_{0}) - R_{\\mathrm{meas},G}(\\lambda_{0})$ introduced by the rectangular approximation. Finally, evaluate the bias numerically for the following parameters:\n- $\\lambda_{0} = 650\\,\\mathrm{nm}$,\n- $R_{0} = 0.35$ (dimensionless reflectance),\n- $a = -1.2 \\times 10^{-3}\\,\\mathrm{nm}^{-1}$,\n- $b = 8.0 \\times 10^{-6}\\,\\mathrm{nm}^{-2}$,\n- $\\mathrm{FWHM}_{G} = 6.0\\,\\mathrm{nm}$.\n\nAssume both ILS functions are perfectly normalized (unit area) and centered at $\\lambda_{0}$. Express the final bias $\\Delta R$ as a dimensionless number and round your answer to four significant figures. Provide the final answer without a percentage sign or any units.",
            "solution": "The problem requires the derivation and calculation of the bias introduced when approximating a Gaussian Instrument Line Spread Function (ILS) with a rectangular ILS of the same full width at half maximum (FWHM).\n\nFirst, we formalize the measurement model. The measured reflectance at a specific band center wavelength $\\lambda_{0}$, denoted $R_{\\mathrm{meas}}(\\lambda_{0})$, is the convolution of the true surface reflectance spectrum $R(\\lambda)$ with the instrument's ILS, $I(\\Delta\\lambda)$, where $\\Delta\\lambda = \\lambda - \\lambda_{0}$. The convolution at $\\lambda_{0}$ is given by:\n$$\nR_{\\mathrm{meas}}(\\lambda_{0}) = \\int_{-\\infty}^{\\infty} R(\\lambda') I(\\lambda_{0} - \\lambda') \\,d\\lambda'\n$$\nWe perform a change of variables, letting $\\Delta\\lambda = \\lambda' - \\lambda_{0}$, so $\\lambda' = \\lambda_{0} + \\Delta\\lambda$ and $d\\lambda' = d(\\Delta\\lambda)$. The argument of the ILS becomes $\\lambda_{0} - (\\lambda_{0} + \\Delta\\lambda) = -\\Delta\\lambda$. Since both the Gaussian and rectangular ILS models provided are even functions, $I(-\\Delta\\lambda) = I(\\Delta\\lambda)$. The integral becomes:\n$$\nR_{\\mathrm{meas}}(\\lambda_{0}) = \\int_{-\\infty}^{\\infty} R(\\lambda_{0} + \\Delta\\lambda) I(\\Delta\\lambda) \\,d(\\Delta\\lambda)\n$$\nThe problem provides a local quadratic approximation for the true spectrum $R(\\lambda)$ around $\\lambda_{0}$:\n$$\nR(\\lambda_0 + \\Delta\\lambda) = R_{0} + a(\\Delta\\lambda) + b(\\Delta\\lambda)^{2}\n$$\nSubstituting this into the measurement equation yields:\n$$\nR_{\\mathrm{meas}}(\\lambda_{0}) = \\int_{-\\infty}^{\\infty} [R_{0} + a(\\Delta\\lambda) + b(\\Delta\\lambda)^{2}] I(\\Delta\\lambda) \\,d(\\Delta\\lambda)\n$$\nBy linearity of the integral, we can separate the terms:\n$$\nR_{\\mathrm{meas}}(\\lambda_{0}) = R_{0} \\int_{-\\infty}^{\\infty} I(\\Delta\\lambda) \\,d(\\Delta\\lambda) + a \\int_{-\\infty}^{\\infty} \\Delta\\lambda \\, I(\\Delta\\lambda) \\,d(\\Delta\\lambda) + b \\int_{-\\infty}^{\\infty} (\\Delta\\lambda)^{2} I(\\Delta\\lambda) \\,d(\\Delta\\lambda)\n$$\nThese integrals represent the moments of the ILS distribution.\nThe first integral, $\\int_{-\\infty}^{\\infty} I(\\Delta\\lambda) \\,d(\\Delta\\lambda)$, is the zeroth moment (total area). The problem states the ILS is normalized, so this integral equals $1$.\nThe second integral, $\\int_{-\\infty}^{\\infty} \\Delta\\lambda \\, I(\\Delta\\lambda) \\,d(\\Delta\\lambda)$, is the first raw moment (the mean). The problem states the ILS is centered at $\\lambda_{0}$, meaning its distribution with respect to $\\Delta\\lambda$ has a mean of $0$.\nThe third integral, $\\int_{-\\infty}^{\\infty} (\\Delta\\lambda)^{2} I(\\Delta\\lambda) \\,d(\\Delta\\lambda)$, is the second raw moment. Since the mean is $0$, this is also the second central moment, or the variance, which we denote as $M_{2}$.\n\nThus, the general expression for the measured reflectance simplifies to:\n$$\nR_{\\mathrm{meas}}(\\lambda_{0}) = R_{0} \\cdot 1 + a \\cdot 0 + b \\cdot M_{2} = R_{0} + b M_{2}\n$$\nThis expression shows that for a symmetric ILS, the measured reflectance is the true reflectance at the band center, $R_{0}$, plus a term proportional to the spectral curvature, $b$, and the variance of the ILS, $M_{2}$.\n\nNow, we calculate the variance for each ILS model.\n\nFor the Gaussian ILS, $I_{G}(\\Delta\\lambda) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp(-\\frac{\\Delta\\lambda^{2}}{2\\sigma^{2}})$. This is a normal distribution with mean $0$ and variance $\\sigma^{2}$. So, $M_{2,G} = \\sigma^{2}$. The FWHM is given as $\\mathrm{FWHM}_{G} = 2\\sqrt{2\\ln 2}\\,\\sigma$. We solve for $\\sigma^{2}$:\n$$\n\\sigma = \\frac{\\mathrm{FWHM}_{G}}{2\\sqrt{2\\ln 2}} \\implies M_{2,G} = \\sigma^{2} = \\frac{\\mathrm{FWHM}_{G}^{2}}{(2\\sqrt{2\\ln 2})^{2}} = \\frac{\\mathrm{FWHM}_{G}^{2}}{8\\ln 2}\n$$\nSo, the measured reflectance with the Gaussian ILS is:\n$$\nR_{\\mathrm{meas},G}(\\lambda_{0}) = R_{0} + b \\frac{\\mathrm{FWHM}_{G}^{2}}{8\\ln 2}\n$$\nFor the rectangular ILS, $I_{R}(\\Delta\\lambda)$ is a uniform distribution over the interval $[-\\frac{W}{2}, \\frac{W}{2}]$, where the width $W$ is set to be equal to $\\mathrm{FWHM}_{G}$. The variance $M_{2,R}$ of a uniform distribution over $[c, d]$ is $\\frac{(d-c)^2}{12}$. Here, $c = -\\frac{W}{2}$ and $d = \\frac{W}{2}$, so the variance is:\n$$\nM_{2,R} = \\frac{(\\frac{W}{2} - (-\\frac{W}{2}))^{2}}{12} = \\frac{W^{2}}{12}\n$$\nAlternatively, by direct integration:\n$$\nM_{2,R} = \\int_{-W/2}^{W/2} (\\Delta\\lambda)^{2} \\left(\\frac{1}{W}\\right) d(\\Delta\\lambda) = \\frac{1}{W} \\left[\\frac{(\\Delta\\lambda)^{3}}{3}\\right]_{-W/2}^{W/2} = \\frac{1}{3W} \\left[\\left(\\frac{W}{2}\\right)^{3} - \\left(-\\frac{W}{2}\\right)^{3}\\right] = \\frac{1}{3W} \\left(\\frac{W^3}{8} + \\frac{W^3}{8}\\right) = \\frac{W^{2}}{12}\n$$\nSince $W = \\mathrm{FWHM}_{G}$, we have $M_{2,R} = \\frac{\\mathrm{FWHM}_{G}^{2}}{12}$.\nThe measured reflectance with the rectangular ILS is:\n$$\nR_{\\mathrm{meas},R}(\\lambda_{0}) = R_{0} + b \\frac{\\mathrm{FWHM}_{G}^{2}}{12}\n$$\nThe bias $\\Delta R$ is the difference between the reflectance measured with the rectangular approximation and the true Gaussian ILS:\n$$\n\\Delta R = R_{\\mathrm{meas},R}(\\lambda_{0}) - R_{\\mathrm{meas},G}(\\lambda_{0}) = \\left(R_{0} + b \\frac{\\mathrm{FWHM}_{G}^{2}}{12}\\right) - \\left(R_{0} + b \\frac{\\mathrm{FWHM}_{G}^{2}}{8\\ln 2}\\right)\n$$\nSimplifying, we obtain the analytic expression for the bias:\n$$\n\\Delta R = b \\, \\mathrm{FWHM}_{G}^{2} \\left(\\frac{1}{12} - \\frac{1}{8\\ln 2}\\right)\n$$\nNow, we substitute the given numerical values:\n$b = 8.0 \\times 10^{-6}\\,\\mathrm{nm}^{-2}$\n$\\mathrm{FWHM}_{G} = 6.0\\,\\mathrm{nm}$\n\nFirst, calculate $\\mathrm{FWHM}_{G}^{2}$:\n$$\n\\mathrm{FWHM}_{G}^{2} = (6.0\\,\\mathrm{nm})^{2} = 36.0\\,\\mathrm{nm}^{2}\n$$\nNext, evaluate the constant factor in parentheses:\n$$\n\\frac{1}{12} - \\frac{1}{8\\ln 2} \\approx \\frac{1}{12} - \\frac{1}{8 \\times 0.693147} \\approx 0.083333 - \\frac{1}{5.545177} \\approx 0.083333 - 0.180337 = -0.097004\n$$\nFinally, compute $\\Delta R$:\n$$\n\\Delta R = (8.0 \\times 10^{-6}\\,\\mathrm{nm}^{-2}) \\times (36.0\\,\\mathrm{nm}^{2}) \\times \\left(\\frac{1}{12} - \\frac{1}{8\\ln 2}\\right)\n$$\n$$\n\\Delta R = (2.88 \\times 10^{-4}) \\times (-0.097004) \\approx -0.000027937152\n$$\n$$\n\\Delta R \\approx -2.7937152 \\times 10^{-5}\n$$\nThe problem requires the answer to be rounded to four significant figures.\n$$\n\\Delta R \\approx -2.794 \\times 10^{-5}\n$$\nThe result is a dimensionless quantity, as expected. The negative sign indicates that for a positive spectral curvature ($b>0$, i.e., a concave up spectrum), the rectangular ILS approximation results in a lower measured reflectance value compared to the true Gaussian ILS.",
            "answer": "$$\n\\boxed{-2.794 \\times 10^{-5}}\n$$"
        },
        {
            "introduction": "Hyperspectral data often requires a trade-off between signal quality and spectral detail. This practice guides you through a first-principles derivation of the effects of spectral binning, a common technique to improve the Signal-to-Noise Ratio ($SNR$) at the cost of coarser spectral resolution. Understanding this relationship is essential for optimizing data acquisition and processing strategies for specific applications .",
            "id": "3835489",
            "problem": "A pushbroom hyperspectral imaging spectrometer with evenly spaced spectral channels is used to observe a target with radiance that is approximately constant over small wavelength intervals. Each spectral channel has a top-hat spectral response with full width at half maximum equal to a bandwidth denoted by $\\Delta \\lambda_{1}$, and channels do not overlap. For a single channel, the expected detected signal in photoelectrons is $S$ and the noise is zero-mean with standard deviation $\\sigma$, where noise contributions across distinct channels are uncorrelated. The Signal-to-Noise Ratio (SNR) is defined as $\\mathrm{SNR} \\equiv \\text{mean signal} \\,/\\, \\text{noise standard deviation}$. Spectral binning is implemented by summing $n$ adjacent channels to form one output band in post-processing.\n\nStarting only from the definition of $\\mathrm{SNR}$ and the variance-addition rule for sums of independent random variables, derive expressions for:\n- The factor by which $\\mathrm{SNR}$ changes when binning $n$ adjacent channels, relative to the unbinned single-channel $\\mathrm{SNR}$.\n- The factor by which the effective spectral resolution degrades, quantified as the ratio of the effective bandwidth $\\Delta \\lambda_{n}$ after binning to the original bandwidth $\\Delta \\lambda_{1}$, under the top-hat and non-overlap assumptions stated above and assuming the target radiance is constant over the $n$ binned channels.\n\nThen, evaluate these two factors for $n=3$. Report the $\\mathrm{SNR}$ change factor and the spectral resolution degradation factor as dimensionless quantities. Do not approximate; provide exact values. The final numerical pair must be reported without units. If you perform any intermediate rounding, round only the final values to four significant figures; otherwise, leave them in exact form.",
            "solution": "The problem is well-posed and scientifically grounded. We shall proceed with the derivation based on the provided givens and principles.\n\nLet $S$ be the mean signal in photoelectrons for a single, unbinned spectral channel and $\\sigma$ be the standard deviation of the associated zero-mean noise. The single-channel Signal-to-Noise Ratio, denoted as $\\mathrm{SNR}_{1}$, is defined as:\n$$ \\mathrm{SNR}_{1} = \\frac{S}{\\sigma} $$\n\nThe problem states that spectral binning is performed by summing $n$ adjacent, non-overlapping channels. We are to derive the new Signal-to-Noise Ratio, $\\mathrm{SNR}_{n}$, for the binned channel and the corresponding change in spectral resolution.\n\n**1. Derivation of the SNR Change Factor**\n\nLet $S_{i}$ be the signal random variable for the $i$-th channel, for $i=1, 2, \\dots, n$. The problem states that the target radiance is constant over the $n$ binned channels. Since the channels are identical (evenly spaced, same bandwidth), this implies that the expected signal for each channel is the same. Let $\\mathbb{E}[S_{i}] = S$. The total signal for the binned channel, $S_{n}$, is the sum of the signals from the $n$ individual channels:\n$$ S_{n} = \\sum_{i=1}^{n} S_{i} $$\nThe mean of the binned signal is the sum of the individual means:\n$$ \\text{mean}(S_{n}) = \\mathbb{E}[S_{n}] = \\mathbb{E}\\left[\\sum_{i=1}^{n} S_{i}\\right] = \\sum_{i=1}^{n} \\mathbb{E}[S_{i}] = \\sum_{i=1}^{n} S = nS $$\n\nLet $N_{i}$ be the noise random variable for the $i$-th channel. The problem states that the noise is zero-mean with standard deviation $\\sigma$. Thus, $\\mathbb{E}[N_i] = 0$ and the standard deviation is $\\mathrm{StdDev}(N_i) = \\sigma$. The variance of the noise for a single channel is $\\mathrm{Var}(N_i) = \\sigma^2$.\n\nThe total noise for the binned channel, $N_{n}$, is the sum of the noise from the $n$ individual channels:\n$$ N_{n} = \\sum_{i=1}^{n} N_{i} $$\nThe problem specifies that noise contributions across distinct channels are uncorrelated. According to the variance-addition rule for uncorrelated random variables, the variance of a sum of such variables is the sum of their variances. Therefore, the variance of the binned noise, $\\mathrm{Var}(N_{n})$, is:\n$$ \\mathrm{Var}(N_{n}) = \\mathrm{Var}\\left(\\sum_{i=1}^{n} N_{i}\\right) = \\sum_{i=1}^{n} \\mathrm{Var}(N_{i}) $$\nSince the noise standard deviation is the same for each channel, the variance is also the same, $\\sigma^2$.\n$$ \\mathrm{Var}(N_{n}) = \\sum_{i=1}^{n} \\sigma^2 = n\\sigma^2 $$\nThe standard deviation of the binned noise, $\\sigma_{n}$, is the square root of its variance:\n$$ \\sigma_{n} = \\sqrt{\\mathrm{Var}(N_{n})} = \\sqrt{n\\sigma^2} = \\sqrt{n}\\,\\sigma $$\nThe Signal-to-Noise Ratio for the binned channel, $\\mathrm{SNR}_{n}$, is the ratio of the mean binned signal to the standard deviation of the binned noise:\n$$ \\mathrm{SNR}_{n} = \\frac{\\text{mean}(S_{n})}{\\sigma_{n}} = \\frac{nS}{\\sqrt{n}\\,\\sigma} = \\frac{\\sqrt{n}\\,S}{\\sigma} $$\nThe factor by which the SNR changes is the ratio of $\\mathrm{SNR}_{n}$ to $\\mathrm{SNR}_{1}$:\n$$ \\text{Factor}_{\\mathrm{SNR}} = \\frac{\\mathrm{SNR}_{n}}{\\mathrm{SNR}_{1}} = \\frac{\\frac{\\sqrt{n}\\,S}{\\sigma}}{\\frac{S}{\\sigma}} = \\sqrt{n} $$\n\n**2. Derivation of the Spectral Resolution Degradation Factor**\n\nThe spectral resolution is related to the bandwidth of a spectral channel. The initial bandwidth of a single channel is given as $\\Delta\\lambda_{1}$. The channels are described as having a top-hat spectral response and being non-overlapping. Binning is performed by summing $n$ adjacent channels.\n\nConsider a set of $n$ adjacent, non-overlapping channels, each with a top-hat response of width $\\Delta\\lambda_{1}$. Since they are adjacent and do not overlap, the total spectral interval they cover is the sum of their individual widths. The effective bandwidth of the binned channel, $\\Delta\\lambda_{n}$, is therefore the sum of the bandwidths of the $n$ constituent channels:\n$$ \\Delta\\lambda_{n} = \\sum_{i=1}^{n} \\Delta\\lambda_{1,i} $$\nwhere $\\Delta\\lambda_{1,i}$ is the bandwidth of the $i$-th channel. Since all channels have the same bandwidth $\\Delta\\lambda_1$, this simplifies to:\n$$ \\Delta\\lambda_{n} = n \\Delta\\lambda_{1} $$\nThe degradation of spectral resolution is quantified by the factor $\\frac{\\Delta\\lambda_{n}}{\\Delta\\lambda_{1}}$:\n$$ \\text{Factor}_{\\text{Res}} = \\frac{\\Delta\\lambda_{n}}{\\Delta\\lambda_{1}} = \\frac{n \\Delta\\lambda_{1}}{\\Delta\\lambda_{1}} = n $$\n\n**3. Evaluation for $n=3$**\n\nWe must now evaluate these two derived factors for the case of binning $n=3$ adjacent channels. The problem requests exact values.\n\nThe SNR change factor is $\\sqrt{n}$. For $n=3$, this factor is:\n$$ \\text{Factor}_{\\mathrm{SNR}} = \\sqrt{3} $$\nThe spectral resolution degradation factor is $n$. For $n=3$, this factor is:\n$$ \\text{Factor}_{\\text{Res}} = 3 $$\nThe two factors are dimensionless quantities. The requested pair of values, in order (SNR change factor, spectral resolution degradation factor), is $(\\sqrt{3}, 3)$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\sqrt{3} & 3 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "A primary goal of hyperspectral imaging is to \"unmix\" pixels into their constituent components, but how reliable are these estimates? This exercise delves into the heart of quantitative analysis by propagating measurement noise through a constrained linear unmixing model to determine the uncertainty of the derived material abundances. This practice provides a rigorous framework for understanding the limits of detection and the confidence we can place in our analytical results .",
            "id": "3835479",
            "problem": "Consider a Hyperspectral Imaging (HSI) linear mixing model in which a measured reflectance spectrum $\\mathbf{y} \\in \\mathbb{R}^{M}$ with $M = 4$ spectral bands is represented as $\\mathbf{y} = \\mathbf{E}\\mathbf{a} + \\mathbf{n}$, where $\\mathbf{E} \\in \\mathbb{R}^{4 \\times 3}$ is a known endmember matrix with $p = 3$ endmembers, $\\mathbf{a} \\in \\mathbb{R}^{3}$ is the abundance vector, and $\\mathbf{n} \\in \\mathbb{R}^{4}$ is zero-mean measurement noise with covariance matrix $\\mathbf{C} \\in \\mathbb{R}^{4 \\times 4}$. The abundance vector satisfies the sum-to-one constraint $\\mathbf{1}^{\\top}\\mathbf{a} = 1$, where $\\mathbf{1} \\in \\mathbb{R}^{3}$ denotes the vector of ones. Assume that the nonnegativity constraints on abundances are inactive in the neighborhood of the solution (i.e., the solution lies in the interior, so inequality constraints do not affect the local sensitivity).\n\nThe abundance estimate $\\widehat{\\mathbf{a}}$ is obtained via equality-constrained weighted least squares using the inverse noise covariance as the weight, that is, by minimizing the quadratic form $(\\mathbf{y} - \\mathbf{E}\\mathbf{a})^{\\top}\\mathbf{C}^{-1}(\\mathbf{y} - \\mathbf{E}\\mathbf{a})$ subject to $\\mathbf{1}^{\\top}\\mathbf{a} = 1$.\n\nStarting from first principles, use the Karush–Kuhn–Tucker (KKT) optimality conditions for the equality-constrained problem to:\n- derive the Jacobian $\\mathbf{J} = \\frac{\\partial \\widehat{\\mathbf{a}}}{\\partial \\mathbf{y}}$ of the constrained least squares solution with respect to the measurement $\\mathbf{y}$,\n- and then propagate the measurement noise covariance $\\mathbf{C}$ to the abundance estimator covariance via the linear error propagation rule.\n\nUse the following concrete, scientifically consistent specifications:\n- The endmember matrix is\n$$\n\\mathbf{E} = \\begin{pmatrix}\n\\frac{1}{2} & \\frac{1}{\\sqrt{2}} & 0 \\\\\n\\frac{1}{2} & -\\frac{1}{\\sqrt{2}} & 0 \\\\\n\\frac{1}{2} & 0 & \\frac{1}{\\sqrt{2}} \\\\\n\\frac{1}{2} & 0 & -\\frac{1}{\\sqrt{2}}\n\\end{pmatrix}.\n$$\n- The noise covariance is isotropic: $\\mathbf{C} = \\sigma^{2}\\mathbf{I}_{4}$ with $\\sigma^{2} = 10^{-3}$.\n\nCompute the variance of the first abundance component $\\widehat{a}_{1}$ implied by this propagation, and present your final result as a pure number (no units). Round your answer to four significant figures.",
            "solution": "The problem asks for the variance of the first abundance component, $\\widehat{a}_{1}$, which is derived from a constrained weighted least squares estimation. The process involves first deriving a general expression for the abundance estimator's covariance matrix and then substituting the specific values provided.\n\nThe problem is validated as being scientifically sound, well-posed, and objective. It is a standard problem in linear estimation theory applied to hyperspectral imaging. All necessary information is provided, and there are no contradictions.\n\nThe optimization problem is to minimize the objective function $f(\\mathbf{a}) = (\\mathbf{y} - \\mathbf{E}\\mathbf{a})^{\\top}\\mathbf{C}^{-1}(\\mathbf{y} - \\mathbf{E}\\mathbf{a})$ subject to the equality constraint $\\mathbf{1}^{\\top}\\mathbf{a} = 1$. The nonnegativity constraints are assumed to be inactive, which means they do not bind at the solution and can be ignored for this local analysis.\n\nTo solve this constrained optimization problem, we use the method of Lagrange multipliers. The Lagrangian function $\\mathcal{L}(\\mathbf{a}, \\lambda)$ is:\n$$\n\\mathcal{L}(\\mathbf{a}, \\lambda) = (\\mathbf{y} - \\mathbf{E}\\mathbf{a})^{\\top}\\mathbf{C}^{-1}(\\mathbf{y} - \\mathbf{E}\\mathbf{a}) + \\lambda(\\mathbf{1}^{\\top}\\mathbf{a} - 1)\n$$\nwhere $\\lambda$ is the Lagrange multiplier associated with the sum-to-one constraint.\n\nThe Karush–Kuhn–Tucker (KKT) conditions for optimality require that the gradient of the Lagrangian with respect to $\\mathbf{a}$ and $\\lambda$ be zero. First, we compute the gradient with respect to $\\mathbf{a}$:\n$$\n\\nabla_{\\mathbf{a}} \\mathcal{L} = \\nabla_{\\mathbf{a}} (\\mathbf{y}^{\\top}\\mathbf{C}^{-1}\\mathbf{y} - 2\\mathbf{y}^{\\top}\\mathbf{C}^{-1}\\mathbf{E}\\mathbf{a} + \\mathbf{a}^{\\top}\\mathbf{E}^{\\top}\\mathbf{C}^{-1}\\mathbf{E}\\mathbf{a} + \\lambda\\mathbf{1}^{\\top}\\mathbf{a} - \\lambda)\n$$\nUsing standard rules for vector calculus (e.g., $\\nabla_{\\mathbf{x}} (\\mathbf{b}^{\\top}\\mathbf{x}) = \\mathbf{b}$ and $\\nabla_{\\mathbf{x}} (\\mathbf{x}^{\\top}\\mathbf{A}\\mathbf{x}) = 2\\mathbf{A}\\mathbf{x}$ for symmetric $\\mathbf{A}$), and noting that $\\mathbf{E}^{\\top}\\mathbf{C}^{-1}\\mathbf{E}$ is symmetric, we get:\n$$\n\\nabla_{\\mathbf{a}} \\mathcal{L} = -2\\mathbf{E}^{\\top}\\mathbf{C}^{-1}\\mathbf{y} + 2(\\mathbf{E}^{\\top}\\mathbf{C}^{-1}\\mathbf{E})\\mathbf{a} + \\lambda\\mathbf{1}\n$$\nSetting the gradient to zero for the optimal abundance vector $\\widehat{\\mathbf{a}}$, we obtain the first KKT condition:\n$$\n-2\\mathbf{E}^{\\top}\\mathbf{C}^{-1}\\mathbf{y} + 2(\\mathbf{E}^{\\top}\\mathbf{C}^{-1}\\mathbf{E})\\widehat{\\mathbf{a}} + \\lambda\\mathbf{1} = \\mathbf{0}\n$$\nThe second KKT condition is simply the original constraint:\n$$\n\\mathbf{1}^{\\top}\\widehat{\\mathbf{a}} = 1\n$$\nLet us define the matrix $\\mathbf{M} = \\mathbf{E}^{\\top}\\mathbf{C}^{-1}\\mathbf{E}$. The first condition can be rewritten as:\n$$\n2\\mathbf{M}\\widehat{\\mathbf{a}} = 2\\mathbf{E}^{\\top}\\mathbf{C}^{-1}\\mathbf{y} - \\lambda\\mathbf{1}\n$$\nAssuming $\\mathbf{M}$ is invertible, we can solve for $\\widehat{\\mathbf{a}}$:\n$$\n\\widehat{\\mathbf{a}} = \\mathbf{M}^{-1}\\mathbf{E}^{\\top}\\mathbf{C}^{-1}\\mathbf{y} - \\frac{\\lambda}{2}\\mathbf{M}^{-1}\\mathbf{1}\n$$\nNow, substitute this expression into the constraint $\\mathbf{1}^{\\top}\\widehat{\\mathbf{a}} = 1$:\n$$\n\\mathbf{1}^{\\top}(\\mathbf{M}^{-1}\\mathbf{E}^{\\top}\\mathbf{C}^{-1}\\mathbf{y} - \\frac{\\lambda}{2}\\mathbf{M}^{-1}\\mathbf{1}) = 1\n$$\nSolving for $\\frac{\\lambda}{2}$:\n$$\n\\frac{\\lambda}{2}(\\mathbf{1}^{\\top}\\mathbf{M}^{-1}\\mathbf{1}) = \\mathbf{1}^{\\top}\\mathbf{M}^{-1}\\mathbf{E}^{\\top}\\mathbf{C}^{-1}\\mathbf{y} - 1\n$$\n$$\n\\frac{\\lambda}{2} = \\frac{\\mathbf{1}^{\\top}\\mathbf{M}^{-1}\\mathbf{E}^{\\top}\\mathbf{C}^{-1}\\mathbf{y} - 1}{\\mathbf{1}^{\\top}\\mathbf{M}^{-1}\\mathbf{1}}\n$$\nSubstituting this back into the expression for $\\widehat{\\mathbf{a}}$:\n$$\n\\widehat{\\mathbf{a}} = \\mathbf{M}^{-1}\\mathbf{E}^{\\top}\\mathbf{C}^{-1}\\mathbf{y} - \\mathbf{M}^{-1}\\mathbf{1} \\left( \\frac{\\mathbf{1}^{\\top}\\mathbf{M}^{-1}\\mathbf{E}^{\\top}\\mathbf{C}^{-1}\\mathbf{y} - 1}{\\mathbf{1}^{\\top}\\mathbf{M}^{-1}\\mathbf{1}} \\right)\n$$\nWe can rearrange this to separate terms that depend on $\\mathbf{y}$ from constant terms:\n$$\n\\widehat{\\mathbf{a}} = \\left( \\mathbf{M}^{-1}\\mathbf{E}^{\\top}\\mathbf{C}^{-1} - \\frac{\\mathbf{M}^{-1}\\mathbf{1}\\mathbf{1}^{\\top}\\mathbf{M}^{-1}\\mathbf{E}^{\\top}\\mathbf{C}^{-1}}{\\mathbf{1}^{\\top}\\mathbf{M}^{-1}\\mathbf{1}} \\right)\\mathbf{y} + \\frac{\\mathbf{M}^{-1}\\mathbf{1}}{\\mathbf{1}^{\\top}\\mathbf{M}^{-1}\\mathbf{1}}\n$$\nThe Jacobian of the estimator $\\widehat{\\mathbf{a}}$ with respect to the measurement $\\mathbf{y}$ is the matrix multiplying $\\mathbf{y}$:\n$$\n\\mathbf{J} = \\frac{\\partial \\widehat{\\mathbf{a}}}{\\partial \\mathbf{y}} = \\mathbf{M}^{-1}\\mathbf{E}^{\\top}\\mathbf{C}^{-1} - \\frac{\\mathbf{M}^{-1}\\mathbf{1}\\mathbf{1}^{\\top}\\mathbf{M}^{-1}\\mathbf{E}^{\\top}\\mathbf{C}^{-1}}{\\mathbf{1}^{\\top}\\mathbf{M}^{-1}\\mathbf{1}}\n$$\n$$\n\\mathbf{J} = \\left( \\mathbf{I} - \\frac{\\mathbf{M}^{-1}\\mathbf{1}\\mathbf{1}^{\\top}}{\\mathbf{1}^{\\top}\\mathbf{M}^{-1}\\mathbf{1}} \\right) \\mathbf{M}^{-1}\\mathbf{E}^{\\top}\\mathbf{C}^{-1}\n$$\nThis completes the first part of the required derivation.\n\nNext, we propagate the measurement noise covariance $\\mathbf{C}$ through this linear relationship. The estimator $\\widehat{\\mathbf{a}}$ is an affine transformation of $\\mathbf{y}$, $\\widehat{\\mathbf{a}} = \\mathbf{J}\\mathbf{y} + \\mathbf{k}$, where $\\mathbf{k}$ is a constant vector. The covariance of $\\widehat{\\mathbf{a}}$, denoted $\\mathbf{C}_{\\widehat{\\mathbf{a}}}$, is given by the linear error propagation rule:\n$$\n\\mathbf{C}_{\\widehat{\\mathbf{a}}} = \\mathbf{J}\\text{Cov}(\\mathbf{y})\\mathbf{J}^{\\top} = \\mathbf{J}\\mathbf{C}\\mathbf{J}^{\\top}\n$$\nSubstituting the expression for $\\mathbf{J}$:\n$$\n\\mathbf{C}_{\\widehat{\\mathbf{a}}} = \\left[ \\left( \\mathbf{I} - \\frac{\\mathbf{M}^{-1}\\mathbf{1}\\mathbf{1}^{\\top}}{\\mathbf{1}^{\\top}\\mathbf{M}^{-1}\\mathbf{1}} \\right) \\mathbf{M}^{-1}\\mathbf{E}^{\\top}\\mathbf{C}^{-1} \\right] \\mathbf{C} \\left[ \\left( \\mathbf{I} - \\frac{\\mathbf{M}^{-1}\\mathbf{1}\\mathbf{1}^{\\top}}{\\mathbf{1}^{\\top}\\mathbf{M}^{-1}\\mathbf{1}} \\right) \\mathbf{M}^{-1}\\mathbf{E}^{\\top}\\mathbf{C}^{-1} \\right]^{\\top}\n$$\nUsing the properties of the transpose and the symmetry of $\\mathbf{C}^{-1}$ and $\\mathbf{M}^{-1}$:\n$$\n\\mathbf{J}^{\\top} = \\mathbf{C}^{-1}\\mathbf{E}\\mathbf{M}^{-1} \\left( \\mathbf{I} - \\frac{\\mathbf{1}\\mathbf{1}^{\\top}\\mathbf{M}^{-1}}{\\mathbf{1}^{\\top}\\mathbf{M}^{-1}\\mathbf{1}} \\right)\n$$\nLet's define a projection-like operator $\\mathbf{P}_{\\text{constr}} = \\mathbf{I} - \\frac{\\mathbf{M}^{-1}\\mathbf{1}\\mathbf{1}^{\\top}}{\\mathbf{1}^{\\top}\\mathbf{M}^{-1}\\mathbf{1}}$.\nThen $\\mathbf{J} = \\mathbf{P}_{\\text{constr}} \\mathbf{M}^{-1}\\mathbf{E}^{\\top}\\mathbf{C}^{-1}$.\n$$\n\\mathbf{C}_{\\widehat{\\mathbf{a}}} = (\\mathbf{P}_{\\text{constr}} \\mathbf{M}^{-1}\\mathbf{E}^{\\top}\\mathbf{C}^{-1}) \\mathbf{C} (\\mathbf{C}^{-1}\\mathbf{E}\\mathbf{M}^{-1} \\mathbf{P}_{\\text{constr}}^{\\top}) = \\mathbf{P}_{\\text{constr}} \\mathbf{M}^{-1}(\\mathbf{E}^{\\top}\\mathbf{C}^{-1}\\mathbf{E})\\mathbf{M}^{-1} \\mathbf{P}_{\\text{constr}}^{\\top}\n$$\nSince $\\mathbf{M} = \\mathbf{E}^{\\top}\\mathbf{C}^{-1}\\mathbf{E}$, this simplifies to:\n$$\n\\mathbf{C}_{\\widehat{\\mathbf{a}}} = \\mathbf{P}_{\\text{constr}} \\mathbf{M}^{-1}\\mathbf{M}\\mathbf{M}^{-1} \\mathbf{P}_{\\text{constr}}^{\\top} = \\mathbf{P}_{\\text{constr}} \\mathbf{M}^{-1} \\mathbf{P}_{\\text{constr}}^{\\top}\n$$\nExpanding this expression:\n$$\n\\mathbf{C}_{\\widehat{\\mathbf{a}}} = \\left(\\mathbf{I} - \\frac{\\mathbf{M}^{-1}\\mathbf{1}\\mathbf{1}^{\\top}}{c}\\right) \\mathbf{M}^{-1} \\left(\\mathbf{I} - \\frac{\\mathbf{1}\\mathbf{1}^{\\top}\\mathbf{M}^{-1}}{c}\\right), \\quad \\text{where } c = \\mathbf{1}^{\\top}\\mathbf{M}^{-1}\\mathbf{1}\n$$\n$$\n= \\left(\\mathbf{M}^{-1} - \\frac{\\mathbf{M}^{-1}\\mathbf{1}\\mathbf{1}^{\\top}\\mathbf{M}^{-1}}{c}\\right) \\left(\\mathbf{I} - \\frac{\\mathbf{1}\\mathbf{1}^{\\top}\\mathbf{M}^{-1}}{c}\\right)\n$$\n$$\n= \\mathbf{M}^{-1} - \\frac{\\mathbf{M}^{-1}\\mathbf{1}\\mathbf{1}^{\\top}\\mathbf{M}^{-1}}{c} - \\frac{\\mathbf{M}^{-1}\\mathbf{1}\\mathbf{1}^{\\top}\\mathbf{M}^{-1}}{c} + \\frac{(\\mathbf{M}^{-1}\\mathbf{1}\\mathbf{1}^{\\top}\\mathbf{M}^{-1})(\\mathbf{1}\\mathbf{1}^{\\top}\\mathbf{M}^{-1})}{c^2}\n$$\nThe numerator of the last term contains $\\mathbf{1}^{\\top}\\mathbf{M}^{-1}\\mathbf{1} = c$:\n$$\n\\frac{\\mathbf{M}^{-1}\\mathbf{1}(\\mathbf{1}^{\\top}\\mathbf{M}^{-1}\\mathbf{1})\\mathbf{1}^{\\top}\\mathbf{M}^{-1}}{c^2} = \\frac{\\mathbf{M}^{-1}\\mathbf{1}c\\mathbf{1}^{\\top}\\mathbf{M}^{-1}}{c^2} = \\frac{\\mathbf{M}^{-1}\\mathbf{1}\\mathbf{1}^{\\top}\\mathbf{M}^{-1}}{c}\n$$\nThus, the expression for the covariance matrix simplifies to:\n$$\n\\mathbf{C}_{\\widehat{\\mathbf{a}}} = \\mathbf{M}^{-1} - \\frac{\\mathbf{M}^{-1}\\mathbf{1}\\mathbf{1}^{\\top}\\mathbf{M}^{-1}}{\\mathbf{1}^{\\top}\\mathbf{M}^{-1}\\mathbf{1}}\n$$\nNow we substitute the given numerical values. The noise covariance is $\\mathbf{C} = \\sigma^2 \\mathbf{I}_{4}$ with $\\sigma^2 = 10^{-3}$, so its inverse is $\\mathbf{C}^{-1} = \\frac{1}{\\sigma^2}\\mathbf{I}_{4}$.\nThe matrix $\\mathbf{M}$ is:\n$$\n\\mathbf{M} = \\mathbf{E}^{\\top}\\mathbf{C}^{-1}\\mathbf{E} = \\mathbf{E}^{\\top}(\\frac{1}{\\sigma^2}\\mathbf{I}_{4})\\mathbf{E} = \\frac{1}{\\sigma^2}\\mathbf{E}^{\\top}\\mathbf{E}\n$$\nLet's compute $\\mathbf{E}^{\\top}\\mathbf{E}$. The columns of $\\mathbf{E}$ are $\\mathbf{e}_1 = (\\frac{1}{2}, \\frac{1}{2}, \\frac{1}{2}, \\frac{1}{2})^{\\top}$, $\\mathbf{e}_2 = (\\frac{1}{\\sqrt{2}}, -\\frac{1}{\\sqrt{2}}, 0, 0)^{\\top}$, and $\\mathbf{e}_3 = (0, 0, \\frac{1}{\\sqrt{2}}, -\\frac{1}{\\sqrt{2}})^{\\top}$.\n$$\n\\mathbf{e}_1^{\\top}\\mathbf{e}_1 = 4(\\frac{1}{2})^2 = 1, \\quad \\mathbf{e}_2^{\\top}\\mathbf{e}_2 = (\\frac{1}{\\sqrt{2}})^2 + (-\\frac{1}{\\sqrt{2}})^2 = 1, \\quad \\mathbf{e}_3^{\\top}\\mathbf{e}_3 = (\\frac{1}{\\sqrt{2}})^2 + (-\\frac{1}{\\sqrt{2}})^2 = 1\n$$\n$$\n\\mathbf{e}_1^{\\top}\\mathbf{e}_2 = \\frac{1}{2\\sqrt{2}} - \\frac{1}{2\\sqrt{2}} = 0, \\quad \\mathbf{e}_1^{\\top}\\mathbf{e}_3 = \\frac{1}{2\\sqrt{2}} - \\frac{1}{2\\sqrt{2}} = 0, \\quad \\mathbf{e}_2^{\\top}\\mathbf{e}_3 = 0\n$$\nSince the columns of $\\mathbf{E}$ are orthonormal, $\\mathbf{E}^{\\top}\\mathbf{E} = \\mathbf{I}_{3}$.\nThis greatly simplifies $\\mathbf{M}$:\n$$\n\\mathbf{M} = \\frac{1}{\\sigma^2}\\mathbf{I}_{3} \\quad \\implies \\quad \\mathbf{M}^{-1} = \\sigma^2\\mathbf{I}_{3}\n$$\nWe substitute this into the formula for $\\mathbf{C}_{\\widehat{\\mathbf{a}}}$:\n$$\n\\mathbf{C}_{\\widehat{\\mathbf{a}}} = \\sigma^2\\mathbf{I}_{3} - \\frac{(\\sigma^2\\mathbf{I}_{3})\\mathbf{1}\\mathbf{1}^{\\top}(\\sigma^2\\mathbf{I}_{3})}{\\mathbf{1}^{\\top}(\\sigma^2\\mathbf{I}_{3})\\mathbf{1}} = \\sigma^2\\mathbf{I}_{3} - \\frac{\\sigma^4 \\mathbf{1}\\mathbf{1}^{\\top}}{\\sigma^2 \\mathbf{1}^{\\top}\\mathbf{1}}\n$$\nSince $\\mathbf{1} \\in \\mathbb{R}^3$, $\\mathbf{1}^{\\top}\\mathbf{1} = 1^2 + 1^2 + 1^2 = 3$.\n$$\n\\mathbf{C}_{\\widehat{\\mathbf{a}}} = \\sigma^2\\mathbf{I}_{3} - \\frac{\\sigma^2}{3}\\mathbf{1}\\mathbf{1}^{\\top}\n$$\nIn matrix form:\n$$\n\\mathbf{C}_{\\widehat{\\mathbf{a}}} = \\sigma^2 \\left( \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} - \\frac{1}{3} \\begin{pmatrix} 1 & 1 & 1 \\\\ 1 & 1 & 1 \\\\ 1 & 1 & 1 \\end{pmatrix} \\right) = \\sigma^2 \\begin{pmatrix} 2/3 & -1/3 & -1/3 \\\\ -1/3 & 2/3 & -1/3 \\\\ -1/3 & -1/3 & 2/3 \\end{pmatrix}\n$$\nThe variance of the first abundance component, $\\text{Var}(\\widehat{a}_1)$, is the first diagonal element of $\\mathbf{C}_{\\widehat{\\mathbf{a}}}$:\n$$\n\\text{Var}(\\widehat{a}_1) = (\\mathbf{C}_{\\widehat{\\mathbf{a}}})_{11} = \\frac{2}{3}\\sigma^2\n$$\nGiven $\\sigma^2 = 10^{-3}$, the numerical value is:\n$$\n\\text{Var}(\\widehat{a}_1) = \\frac{2}{3} \\times 10^{-3} = 0.0006666...\n$$\nRounding to four significant figures gives $0.0006667$.",
            "answer": "$$\\boxed{0.0006667}$$"
        }
    ]
}