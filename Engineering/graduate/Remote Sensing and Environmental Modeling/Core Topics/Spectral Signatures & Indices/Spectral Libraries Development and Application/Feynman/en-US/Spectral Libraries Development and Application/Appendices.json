{
    "hands_on_practices": [
        {
            "introduction": "Spectral measurements are inevitably contaminated by instrument noise, which can obscure subtle features and degrade the performance of analytical models. While smoothing filters can reduce high-frequency noise, they risk distorting the underlying signal, particularly narrow diagnostic absorption features. This exercise provides a hands-on opportunity to quantitatively explore this fundamental trade-off by implementing and evaluating the widely used Savitzky-Golay filter, helping you build an intuition for how filter parameters like window size and polynomial order impact data fidelity. ",
            "id": "3853543",
            "problem": "You are tasked with designing and executing a quantitative evaluation of how Savitzky–Golay (SG) filtering affects the preservation of narrow absorption features in reflectance spectra used in spectral libraries for remote sensing and environmental modeling. The evaluation must be conducted on a synthetic spectrum that is scientifically plausible and constructed from first principles. The final program must compute a quantitative measure of feature attenuation for specified SG filter configurations and output the results in a single, machine-readable line. All wavelength values must be treated in nanometers (nm). All attenuation values must be expressed as decimal fractions rounded to $6$ decimal places.\n\nBegin with the following foundations and definitions:\n\n1. Construct a wavelength axis $\\lambda$ sampled uniformly from $400$ nm to $800$ nm in steps of $1$ nm. Represent $\\lambda$ as the integer grid $\\{400, 401, \\dots, 800\\}$ in nanometers.\n\n2. Define a smooth, slowly varying continuum reflectance $R_{\\text{cont}}(\\lambda)$ by\n$$\nR_{\\text{cont}}(\\lambda) = 0.3 + 0.0002\\left(\\lambda - 400\\right) + 0.03 \\sin\\left( \\frac{2\\pi}{300} \\left(\\lambda - 400\\right) \\right).\n$$\nThis is a unitless reflectance model incorporating a linear slope and a broad sinusoidal component.\n\n3. Superimpose a single narrow absorption feature centered at $\\lambda_0 = 550$ nm with Gaussian shape, fractional depth $A = 0.12$, and standard deviation $\\sigma_\\lambda = 2$ nm, yielding the feature modulation\n$$\nM(\\lambda) = 1 - A \\exp\\left(-\\frac{\\left(\\lambda - \\lambda_0\\right)^2}{2\\sigma_\\lambda^2}\\right).\n$$\n\n4. Model additive instrument noise $\\epsilon(\\lambda)$ as independent identically distributed Gaussian random variables with zero mean and standard deviation $\\sigma_\\epsilon = 0.005$, namely $\\epsilon(\\lambda) \\sim \\mathcal{N}\\left(0, \\sigma_\\epsilon^2\\right)$. Fix the random seed to $42$ to ensure reproducibility.\n\n5. The observed reflectance spectrum is defined as\n$$\nR(\\lambda) = R_{\\text{cont}}(\\lambda) \\cdot M(\\lambda) + \\epsilon(\\lambda).\n$$\n\n6. Define continuum removal about the feature using a linear continuum $C(\\lambda)$ connecting the two shoulders at $\\lambda_L = 535$ nm and $\\lambda_R = 565$ nm. For any signal $S(\\lambda)$ (either $R(\\lambda)$ or its filtered version), construct\n$$\nC_S(\\lambda) = S(\\lambda_L) + \\left(\\lambda - \\lambda_L\\right) \\cdot \\frac{S(\\lambda_R) - S(\\lambda_L)}{\\lambda_R - \\lambda_L}.\n$$\nThe continuum-removed depth of the feature at the center for signal $S$ is then\n$$\nd_S = 1 - \\frac{S(\\lambda_0)}{C_S(\\lambda_0)}.\n$$\n\n7. The Savitzky–Golay (SG) filter of window length $W$ (odd integer) and polynomial order $P$ is a local least squares polynomial smoother: within each window of $W$ samples, find coefficients of a polynomial of degree $P$ that minimize the squared error to the data, and use the value of that polynomial at the window center as the filtered output. Apply the SG filter to $R(\\lambda)$ to produce $R_f^{(W,P)}(\\lambda)$ for each test case $(W,P)$.\n\n8. Define the attenuation measure for each test case $(W,P)$ by the fractional loss of feature depth due to smoothing:\n$$\na^{(W,P)} = \\frac{d_{R} - d_{R_f^{(W,P)}}}{d_{R}},\n$$\nwhere $d_R$ is the depth computed from the original noisy spectrum $R(\\lambda)$ and $d_{R_f^{(W,P)}}$ is the depth computed from the filtered spectrum $R_f^{(W,P)}(\\lambda)$. A positive value indicates attenuation (loss) of the feature; a negative value indicates apparent enhancement due to noise mitigation.\n\nYour program must implement the construction of $R(\\lambda)$, apply the SG filter for each test case, compute $a^{(W,P)}$, and print the results in the exact specified format. No external inputs are permitted.\n\nTest Suite:\n- Case $1$: $(W,P) = (7,2)$\n- Case $2$: $(W,P) = (21,2)$\n- Case $3$: $(W,P) = (11,4)$\n- Case $4$: $(W,P) = (5,4)$\n- Case $5$: $(W,P) = (51,3)$\n\nConstraints:\n- $W$ must be an odd integer strictly greater than $P$.\n- All computations are on the defined wavelength grid in nanometers.\n- Reflectance is unitless.\n\nAnswer Specification:\n- For each test case, compute $a^{(W,P)}$ and round to $6$ decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test suite, for example, \"[a1,a2,a3,a4,a5]\".",
            "solution": "The problem statement is evaluated for validity prior to attempting a solution.\n\n**Problem Validation**\n\n1.  **Extract Givens**:\n    -   Wavelength axis: $\\lambda$ is a uniform integer grid from $400$ nm to $800$ nm, i.e., $\\lambda \\in \\{400, 401, \\dots, 800\\}$.\n    -   Continuum reflectance model: $R_{\\text{cont}}(\\lambda) = 0.3 + 0.0002\\left(\\lambda - 400\\right) + 0.03 \\sin\\left( \\frac{2\\pi}{300} \\left(\\lambda - 400\\right) \\right)$.\n    -   Absorption feature modulation: $M(\\lambda) = 1 - A \\exp\\left(-\\frac{\\left(\\lambda - \\lambda_0\\right)^2}{2\\sigma_\\lambda^2}\\right)$, with center $\\lambda_0 = 550$ nm, fractional depth $A = 0.12$, and standard deviation $\\sigma_\\lambda = 2$ nm.\n    -   Additive noise model: $\\epsilon(\\lambda) \\sim \\mathcal{N}\\left(0, \\sigma_\\epsilon^2\\right)$ with $\\sigma_\\epsilon = 0.005$. The random seed is fixed at $42$.\n    -   Observed reflectance model: $R(\\lambda) = R_{\\text{cont}}(\\lambda) \\cdot M(\\lambda) + \\epsilon(\\lambda)$.\n    -   Continuum removal: A linear continuum $C_S(\\lambda)$ is defined between shoulder wavelengths $\\lambda_L = 535$ nm and $\\lambda_R = 565$ nm.\n    -   Feature depth definition: $d_S = 1 - \\frac{S(\\lambda_0)}{C_S(\\lambda_0)}$.\n    -   Savitzky-Golay (SG) filter application: The filter with window length $W$ and polynomial order $P$ is applied to $R(\\lambda)$ to get $R_f^{(W,P)}(\\lambda)$.\n    -   Attenuation measure: $a^{(W,P)} = \\frac{d_{R} - d_{R_f^{(W,P)}}}{d_{R}}$.\n    -   Test Suite: $(W,P)$ pairs are $(7,2)$, $(21,2)$, $(11,4)$, $(5,4)$, and $(51,3)$.\n    -   Constraints: $W$ must be an odd integer and $W > P$.\n\n2.  **Validate Using Extracted Givens**:\n    -   **Scientifically Grounded**: The problem is well-grounded in the principles of spectroscopy and signal processing. The construction of a synthetic spectrum from a continuum, an absorption feature, and noise is a standard methodology for testing algorithms in remote sensing. The Savitzky-Golay filter and linear continuum removal are established, widely-used techniques.\n    -   **Well-Posed**: The problem is fully specified, providing all necessary equations, parameters, and procedural steps. The use of a fixed random seed ensures a unique, reproducible solution.\n    -   **Objective**: The problem is stated using precise, mathematical, and objective language, free from any subjective elements.\n    -   **Completeness and Consistency**: The definitions are self-consistent. The test cases provided, e.g., $(W,P) = (5,4)$, satisfy the specified constraints ($5$ is odd and $5>4$). The problem is complete and contains no contradictions.\n\n3.  **Verdict and Action**:\n    -   The problem is deemed **valid**. A step-by-step solution will be developed.\n\n**Solution Derivation**\n\nThe solution requires implementing a computational workflow to quantify how Savitzky-Golay (SG) filtering affects a narrow spectral absorption feature. The process is broken down into the following steps.\n\n**Step 1: Synthetic Spectrum Generation**\nFirst, we construct the synthetic reflectance spectrum. This provides a controlled signal for our analysis. The wavelength axis $\\lambda$ is defined as a uniform integer grid from $400$ nm to $800$ nm.\n$$ \\lambda = \\{400, 401, \\dots, 800\\} $$\nThe smooth, slowly varying continuum reflectance, $R_{\\text{cont}}(\\lambda)$, is computed for each wavelength in $\\lambda$ using the specified equation:\n$$ R_{\\text{cont}}(\\lambda) = 0.3 + 0.0002\\left(\\lambda - 400\\right) + 0.03 \\sin\\left( \\frac{2\\pi}{300} \\left(\\lambda - 400\\right) \\right) $$\nA single Gaussian absorption feature is superimposed on this continuum. The feature is described by a multiplicative modulation factor $M(\\lambda)$, determined by its center $\\lambda_0 = 550$ nm, fractional depth $A = 0.12$, and standard deviation $\\sigma_\\lambda = 2$ nm.\n$$ M(\\lambda) = 1 - A \\exp\\left(-\\frac{\\left(\\lambda - \\lambda_0\\right)^2}{2\\sigma_\\lambda^2}\\right) $$\nTo simulate a realistic measurement, additive instrument noise, $\\epsilon(\\lambda)$, is generated. For reproducibility, the pseudo-random number generator is seeded with the value $42$. The noise for each wavelength is an independent draw from a normal distribution with mean $0$ and standard deviation $\\sigma_\\epsilon = 0.005$.\nThe final observed reflectance spectrum, $R(\\lambda)$, is the combination of these three components:\n$$ R(\\lambda) = R_{\\text{cont}}(\\lambda) \\cdot M(\\lambda) + \\epsilon(\\lambda) $$\n\n**Step 2: Feature Depth Calculation**\nThe quantification of the absorption feature requires measuring its depth relative to the local continuum. This is a standard procedure in quantitative spectroscopy. A linear continuum, $C_S(\\lambda)$, for any given spectrum $S(\\lambda)$, is established by connecting the spectral values at two shoulder points, $\\lambda_L = 535$ nm and $\\lambda_R = 565$ nm.\n$$ C_S(\\lambda) = S(\\lambda_L) + \\left(\\lambda - \\lambda_L\\right) \\cdot \\frac{S(\\lambda_R) - S(\\lambda_L)}{\\lambda_R - \\lambda_L} $$\nThe continuum-removed depth of the feature, $d_S$, is then computed at the feature center, $\\lambda_0 = 550$ nm. It is defined as\n$$ d_S = 1 - \\frac{S(\\lambda_0)}{C_S(\\lambda_0)} $$\nThis metric provides a normalized measure of the feature's strength. This calculation is first applied to the original noisy spectrum $R(\\lambda)$ to establish the baseline feature depth, $d_R$.\n\n**Step 3: Application of Savitzky–Golay Filter**\nFor each test case defined by a window length $W$ and polynomial order $P$, the SG filter is applied to the entire noisy spectrum $R(\\lambda)$. The SG filter is a local polynomial regression smoother that is effective at reducing high-frequency noise while preserving the general shape of spectral features better than a simple moving average. The filtering operation generates a new, smoothed spectrum denoted as $R_f^{(W,P)}(\\lambda)$.\n\n**Step 4: Attenuation Measurement**\nAfter filtering, the feature depth is re-calculated for the smoothed spectrum $R_f^{(W,P)}(\\lambda)$ using the exact same continuum removal and depth calculation procedure as in Step 2. This yields the post-filtering depth, $d_{R_f^{(W,P)}}$.\nThe attenuation, $a^{(W,P)}$, is the key metric for this problem. It quantifies the fractional loss of feature depth resulting from the filtering process. It is computed as:\n$$ a^{(W,P)} = \\frac{d_{R} - d_{R_f^{(W,P)}}}{d_{R}} $$\nA positive value signifies attenuation (the feature becomes shallower), which is the expected outcome of applying a smoothing filter to a feature that is narrower than the filter window. A negative value would indicate an apparent enhancement of the feature depth.\n\n**Step 5: Execution and Formatting**\nThe procedure is executed by first computing the reference spectrum $R(\\lambda)$ and its depth $d_R$. Then, for each $(W,P)$ pair in the test suite—$(7,2)$, $(21,2)$, $(11,4)$, $(5,4)$, and $(51,3)$—the corresponding filtered spectrum $R_f^{(W,P)}(\\lambda)$ is generated, its depth $d_{R_f^{(W,P)}}$ is calculated, and the attenuation $a^{(W,P)}$ is determined. Each resulting attenuation value is rounded to $6$ decimal places. Finally, the list of results is formatted into the specified single-line string.",
            "answer": "```python\nimport numpy as np\nfrom scipy.signal import savgol_filter\n\ndef solve():\n    \"\"\"\n    Computes the attenuation of a synthetic spectral feature due to Savitzky-Golay\n    filtering for a set of filter parameters.\n    \"\"\"\n    \n    # Step 1: Define constants and generate the synthetic spectrum\n    \n    # Wavelength grid and feature parameters\n    lambdas = np.arange(400, 801, dtype=np.float64)\n    lambda_0 = 550.0\n    A = 0.12\n    sigma_lambda = 2.0\n    \n    # Noise parameters\n    sigma_eps = 0.005\n    seed = 42\n    \n    # Continuum removal parameters\n    lambda_L = 535.0\n    lambda_R = 565.0\n\n    # Map specific wavelengths to their corresponding array indices\n    # index = wavelength - start_wavelength\n    idx_0 = int(lambda_0 - lambdas[0])\n    idx_L = int(lambda_L - lambdas[0])\n    idx_R = int(lambda_R - lambdas[0])\n\n    # Construct the continuum reflectance\n    R_cont = 0.3 + 0.0002 * (lambdas - 400.0) + 0.03 * np.sin((2 * np.pi / 300.0) * (lambdas - 400.0))\n\n    # Construct the feature modulation\n    M = 1.0 - A * np.exp(-((lambdas - lambda_0)**2) / (2.0 * sigma_lambda**2))\n\n    # Generate reproducible additive noise\n    np.random.seed(seed)\n    epsilon = np.random.normal(0.0, sigma_eps, size=lambdas.shape)\n\n    # Combine components to create the final observed spectrum\n    R = R_cont * M + epsilon\n\n    # Step 2: Define a function for feature depth calculation\n    def calculate_depth(S, l_0, l_L, l_R, i_0, i_L, i_R):\n        \"\"\"\n        Calculates the continuum-removed depth of a feature in a spectrum S.\n        \"\"\"\n        S_L = S[i_L]\n        S_R = S[i_R]\n        S_0 = S[i_0]\n\n        # Calculate the value of the linear continuum at the feature center lambda_0\n        C_S_at_lambda_0 = S_L + (l_0 - l_L) * ((S_R - S_L) / (l_R - l_L))\n        \n        # Calculate the normalized depth\n        d_S = 1.0 - S_0 / C_S_at_lambda_0\n        return d_S\n\n    # Calculate the depth of the feature in the original noisy spectrum\n    d_R = calculate_depth(R, lambda_0, lambda_L, lambda_R, idx_0, idx_L, idx_R)\n\n    # Step 3 & 4: Apply filters and compute attenuation for each test case\n    test_cases = [\n        (7, 2),   # Case 1\n        (21, 2),  # Case 2\n        (11, 4),  # Case 3\n        (5, 4),   # Case 4\n        (51, 3)   # Case 5\n    ]\n    \n    results = []\n    for W, P in test_cases:\n        # Apply the Savitzky-Golay filter to the original spectrum\n        R_f = savgol_filter(R, window_length=W, polyorder=P)\n        \n        # Calculate the depth of the feature in the filtered spectrum\n        d_Rf = calculate_depth(R_f, lambda_0, lambda_L, lambda_R, idx_0, idx_L, idx_R)\n        \n        # Calculate the attenuation measure\n        attenuation = (d_R - d_Rf) / d_R\n        \n        results.append(round(attenuation, 6))\n\n    # Step 5: Format and print the final output\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Hyperspectral datasets are characterized by high dimensionality, with reflectance measured in hundreds of contiguous bands. This often leads to significant data redundancy and computational challenges, a phenomenon known as the 'curse of dimensionality'. Principal Component Analysis (PCA) is a cornerstone technique for mitigating this issue by transforming the data into a new, smaller set of uncorrelated variables, or principal components. This practice moves beyond a simple application of PCA, challenging you to determine the minimal number of components needed to represent a spectral library while satisfying a suite of practical constraints, including total variance, overall spectral shape, and the preservation of specific diagnostic features. ",
            "id": "3853533",
            "problem": "You are given three small spectral libraries represented as matrices of unitless reflectance values. Each library consists of several material spectra sampled at a finite number of discrete wavelength bands. Let there be $N$ spectra and $B$ bands; represent the library as a matrix $\\mathbf{L} \\in \\mathbb{R}^{N \\times B}$ whose rows $\\mathbf{x}_i \\in \\mathbb{R}^{B}$ are the individual spectra. The goal is to apply Principal Component Analysis (PCA) to each library, quantify explained variance, and select the minimal number of principal components that preserves diagnostic information necessary for subsequent spectral matching. Use the following fundamental base:\n\n- Centering data: define the column mean vector $\\boldsymbol{\\mu} \\in \\mathbb{R}^{B}$ and the centered data matrix $\\mathbf{X} = \\mathbf{L} - \\mathbf{1}\\boldsymbol{\\mu}^{\\top}$, where $\\mathbf{1} \\in \\mathbb{R}^{N}$ is the vector of ones.\n- Covariance: $\\mathbf{C} = \\frac{1}{N-1}\\mathbf{X}^{\\top}\\mathbf{X}$.\n- Eigen-decomposition: $\\mathbf{C}\\mathbf{u}_j = \\lambda_j \\mathbf{u}_j$, with eigenvalues $\\lambda_j \\ge 0$ and orthonormal eigenvectors $\\mathbf{u}_j$.\n- Explained variance ratio: $r_j = \\frac{\\lambda_j}{\\sum_{k=1}^{B} \\lambda_k}$. The cumulative explained variance for $K$ components is $R(K) = \\sum_{j=1}^{K} r_j$.\n- Reconstruction using the top $K$ components: for a spectrum $\\mathbf{x}$, define $\\hat{\\mathbf{x}}(K) = \\boldsymbol{\\mu} + \\mathbf{U}_K \\mathbf{U}_K^{\\top}(\\mathbf{x}-\\boldsymbol{\\mu})$, where $\\mathbf{U}_K$ stacks the first $K$ eigenvectors as columns.\n- Diagnostic preservation criteria:\n  1. Spectral Angle Mapper (SAM) between $\\mathbf{x}$ and $\\hat{\\mathbf{x}}(K)$ is $\\theta(\\mathbf{x},\\hat{\\mathbf{x}}(K)) = \\arccos\\left(\\frac{\\mathbf{x}^{\\top}\\hat{\\mathbf{x}}(K)}{\\|\\mathbf{x}\\|_2 \\,\\|\\hat{\\mathbf{x}}(K)\\|_2}\\right)$ and must be less than a given threshold $\\tau_{\\text{sam}}$. Express angles in radians.\n  2. Band depth preservation for specified diagnostic absorption bands. For a diagnostic specification $(w,\\ell,r)$ with indices $w$ (absorption band) and continuum endpoints $\\ell$ and $r$, define the local linear continuum at $w$ as $C_{\\mathbf{x}}(w) = x_{\\ell} + \\frac{w-\\ell}{r-\\ell}(x_{r}-x_{\\ell})$. The band depth is $D_{\\mathbf{x}}(w) = 1 - \\frac{x_w}{C_{\\mathbf{x}}(w)}$. The absolute reconstruction error $|D_{\\mathbf{x}}(w) - D_{\\hat{\\mathbf{x}}(K)}(w)|$ must not exceed a tolerance $\\tau_{\\text{bd}}$.\n\nFor each library, your program must:\n\n- Compute PCA as described above.\n- Determine the smallest integer $K \\in \\{1, 2, \\dots, B\\}$ such that both $R(K) \\ge \\tau_{\\text{ev}}$ and the diagnostic preservation criteria hold for all spectra in the library across all specified diagnostic band triples $(w,\\ell,r)$. If no such $K$ satisfies both criteria for $K<B$, return $B$.\n\nTest suite and parameters:\n\n- Case $1$: Library $\\mathbf{L}_1 \\in \\mathbb{R}^{4 \\times 7}$ with rows\n  - Material $1$: $[\\,0.5,\\,0.55,\\,0.6,\\,0.55,\\,0.7,\\,0.75,\\,0.8\\,]$\n  - Material $2$: $[\\,0.3,\\,0.37,\\,0.44,\\,0.51,\\,0.43,\\,0.65,\\,0.72\\,]$\n  - Material $3$: $[\\,0.6,\\,0.57,\\,0.46,\\,0.51,\\,0.48,\\,0.45,\\,0.42\\,]$\n  - Material $4$: $[\\,0.4,\\,0.42,\\,0.44,\\,0.46,\\,0.48,\\,0.38,\\,0.52\\,]$\n  Diagnostic set $\\mathcal{D}_1 = \\{(2,1,3),(3,2,4),(4,3,5)\\}$, thresholds $\\tau_{\\text{ev}} = 0.98$, $\\tau_{\\text{sam}} = 0.05$ radians, $\\tau_{\\text{bd}} = 0.03$ (unitless).\n- Case $2$: Library $\\mathbf{L}_2 \\in \\mathbb{R}^{3 \\times 6}$ with rows\n  - Material $A$: $[\\,0.3,\\,0.33,\\,0.31,\\,0.31,\\,0.42,\\,0.375\\,]$\n  - Material $B$: $[\\,0.5,\\,0.48,\\,0.45,\\,0.45,\\,0.33,\\,0.405\\,]$\n  - Material $C$: $[\\,0.4,\\,0.37,\\,0.46,\\,0.46,\\,0.5,\\,0.435\\,]$\n  Diagnostic set $\\mathcal{D}_2 = \\{(2,1,3),(4,3,5)\\}$, thresholds $\\tau_{\\text{ev}} = 0.95$, $\\tau_{\\text{sam}} = 0.02$ radians, $\\tau_{\\text{bd}} = 0.02$ (unitless).\n- Case $3$: Library $\\mathbf{L}_3 \\in \\mathbb{R}^{4 \\times 7}$ formed by adding fixed small noise to $\\mathbf{L}_1$:\n  - Material $1$: $[\\,0.5,\\,0.555,\\,0.596,\\,0.553,\\,0.698,\\,0.754,\\,0.797\\,]$\n  - Material $2$: $[\\,0.302,\\,0.367,\\,0.444,\\,0.508,\\,0.433,\\,0.646,\\,0.721\\,]$\n  - Material $3$: $[\\,0.597,\\,0.572,\\,0.458,\\,0.511,\\,0.479,\\,0.453,\\,0.418\\,]$\n  - Material $4$: $[\\,0.401,\\,0.418,\\,0.443,\\,0.456,\\,0.482,\\,0.379,\\,0.522\\,]$\n  Diagnostic set $\\mathcal{D}_3 = \\{(2,1,3),(3,2,4),(4,3,5)\\}$, thresholds $\\tau_{\\text{ev}} = 0.97$, $\\tau_{\\text{sam}} = 0.06$ radians, $\\tau_{\\text{bd}} = 0.035$ (unitless).\n\nAngle unit requirement: All angles must be computed and compared in radians.\n\nFinal output format: Your program should produce a single line of output containing the minimal numbers of components for the above three cases as a comma-separated list enclosed in square brackets, for example $[K_1,K_2,K_3]$, where each $K_i$ is an integer.\n\nYour program must be a complete, runnable program that implements the PCA, explained variance computation, reconstruction, SAM metric, and band depth preservation checks exactly as defined. No external input is allowed; the program must use the data and parameters provided above.",
            "solution": "The problem requires us to determine the minimum number of principal components, $K$, needed to represent spectral libraries while preserving a specified amount of total variance and critical diagnostic features. This is a common dimensionality reduction task in hyperspectral data analysis, where the goal is to reduce data volume and computational cost without losing scientifically relevant information. The solution involves performing Principal Component Analysis (PCA) on each library and then iteratively checking a set of criteria for an increasing number of components.\n\nThe overall procedure for each given spectral library $\\mathbf{L} \\in \\mathbb{R}^{N \\times B}$ is as follows:\n\n1.  **Data Preprocessing and Covariance Calculation**:\n    The first step in PCA is to center the data. The mean spectrum, $\\boldsymbol{\\mu} \\in \\mathbb{R}^{B}$, is computed by averaging the reflectance values for each wavelength band across all $N$ spectra in the library:\n    $$ \\boldsymbol{\\mu} = \\frac{1}{N}\\sum_{i=1}^{N} \\mathbf{x}_i^{\\top} $$\n    where $\\mathbf{x}_i$ are the row vectors representing each spectrum in $\\mathbf{L}$. The data matrix is then centered by subtracting this mean spectrum from each spectrum:\n    $$ \\mathbf{X} = \\mathbf{L} - \\mathbf{1}\\boldsymbol{\\mu}^{\\top} $$\n    where $\\mathbf{1} \\in \\mathbb{R}^{N}$ is a column vector of ones. The sample covariance matrix, $\\mathbf{C} \\in \\mathbb{R}^{B \\times B}$, is then computed from the centered data. This matrix quantifies the variance within each band and the covariance between different bands.\n    $$ \\mathbf{C} = \\frac{1}{N-1}\\mathbf{X}^{\\top}\\mathbf{X} $$\n\n2.  **Eigendecomposition**:\n    The principal components are found by performing an eigendecomposition of the covariance matrix $\\mathbf{C}$.\n    $$ \\mathbf{C}\\mathbf{u}_j = \\lambda_j \\mathbf{u}_j, \\quad j = 1, \\dots, B $$\n    This yields a set of eigenvalues $\\lambda_j$ and their corresponding eigenvectors $\\mathbf{u}_j \\in \\mathbb{R}^{B}$. Each eigenvector $\\mathbf{u}_j$ represents a principal component, which is a new orthogonal basis vector in the spectral space. The corresponding eigenvalue $\\lambda_j$ represents the amount of variance in the data along that principal component direction. By convention, the eigenvalues and their eigenvectors are sorted in descending order, such that $\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_B \\ge 0$.\n\n3.  **Iterative Search for Minimal $K$**:\n    We search for the smallest integer $K \\in \\{1, 2, \\dots, B\\}$ that satisfies all specified criteria. The search proceeds by testing $K=1$, then $K=2$, and so on, until a valid $K$ is found. For each $K$, we perform three checks.\n\n    3.1. **Cumulative Explained Variance (CEV) Criterion**:\n    The first criterion is that the top $K$ components must account for a minimum fraction, $\\tau_{\\text{ev}}$, of the total variance. The cumulative explained variance ratio, $R(K)$, is calculated as:\n    $$ R(K) = \\frac{\\sum_{j=1}^{K} \\lambda_j}{\\sum_{j=1}^{B} \\lambda_k} $$\n    The condition is $R(K) \\ge \\tau_{\\text{ev}}$. If this condition is not met, the current $K$ is insufficient, and we proceed to test $K+1$.\n\n    3.2. **Diagnostic Preservation Criteria**:\n    If the CEV criterion is met, we must then verify that the reconstruction based on $K$ components preserves specific diagnostic features. This requires reconstructing each original spectrum $\\mathbf{x}$ using the top $K$ principal components. The reconstructed spectrum, $\\hat{\\mathbf{x}}(K)$, is given by:\n    $$ \\hat{\\mathbf{x}}(K) = \\boldsymbol{\\mu} + \\mathbf{U}_K \\mathbf{U}_K^{\\top}(\\mathbf{x}-\\boldsymbol{\\mu}) $$\n    Here, $\\mathbf{U}_K = [\\mathbf{u}_1, \\dots, \\mathbf{u}_K]$ is the matrix whose columns are the first $K$ eigenvectors. This reconstruction is then compared against the original spectrum $\\mathbf{x}$ using two metrics. These checks must hold for all spectra $\\mathbf{x}_i$ in the library $\\mathbf{L}$.\n\n    a. **Spectral Angle Mapper (SAM)**: The SAM metric measures the angle between two spectra, treated as vectors. It is a measure of spectral shape similarity that is invariant to illumination. The angle must be below a threshold $\\tau_{\\text{sam}}$.\n    $$ \\theta(\\mathbf{x}, \\hat{\\mathbf{x}}(K)) = \\arccos\\left(\\frac{\\mathbf{x}^{\\top}\\hat{\\mathbf{x}}(K)}{\\|\\mathbf{x}\\|_2 \\,\\|\\hat{\\mathbf{x}}(K)\\|_2}\\right) < \\tau_{\\text{sam}} $$\n\n    b. **Band Depth (BD) Preservation**: This metric ensures that the depths of specific diagnostic absorption features are preserved. For a diagnostic feature specified by a triple of band indices $(w, \\ell, r)$—representing the absorption minimum, the left continuum point, and the right continuum point, respectively—the band depth $D_{\\mathbf{x}}(w)$ is calculated relative to a local linear continuum $C_{\\mathbf{x}}(w)$.\n    $$ C_{\\mathbf{x}}(w) = x_{\\ell} + \\frac{w-\\ell}{r-\\ell}(x_{r}-x_{\\ell}) $$\n    $$ D_{\\mathbf{x}}(w) = 1 - \\frac{x_w}{C_{\\mathbf{x}}(w)} $$\n    The absolute error in the reconstructed band depth must be below a tolerance $\\tau_{\\text{bd}}$. This must be verified for all specified diagnostic triples $(w, \\ell, r) \\in \\mathcal{D}$.\n    $$ |D_{\\mathbf{x}}(w) - D_{\\hat{\\mathbf{x}}(K)}(w)| < \\tau_{\\text{bd}} $$\n\n4.  **Final Determination**:\n    The first value of $K$ for which the CEV criterion and both diagnostic preservation criteria (SAM and BD) are satisfied for all spectra in the library and for all specified diagnostic bands is the minimal number of components required. If no value of $K < B$ satisfies all conditions, the result is $B$, as a reconstruction with all $B$ components is perfect (i.e., $\\hat{\\mathbf{x}}(B) = \\mathbf{x}$), resulting in zero error for the SAM and BD metrics and a CEV of $1$.\n\nThe implementation will apply this complete procedure to each of the three test cases provided.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of finding the minimal number of principal components\n    for three given spectral libraries based on specified criteria.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"L\": np.array([\n                [0.5, 0.55, 0.6, 0.55, 0.7, 0.75, 0.8],\n                [0.3, 0.37, 0.44, 0.51, 0.43, 0.65, 0.72],\n                [0.6, 0.57, 0.46, 0.51, 0.48, 0.45, 0.42],\n                [0.4, 0.42, 0.44, 0.46, 0.48, 0.38, 0.52]\n            ]),\n            \"D\": [(2, 1, 3), (3, 2, 4), (4, 3, 5)],\n            \"tau_ev\": 0.98,\n            \"tau_sam\": 0.05,\n            \"tau_bd\": 0.03\n        },\n        {\n            \"L\": np.array([\n                [0.3, 0.33, 0.31, 0.31, 0.42, 0.375],\n                [0.5, 0.48, 0.45, 0.45, 0.33, 0.405],\n                [0.4, 0.37, 0.46, 0.46, 0.5, 0.435]\n            ]),\n            \"D\": [(2, 1, 3), (4, 3, 5)],\n            \"tau_ev\": 0.95,\n            \"tau_sam\": 0.02,\n            \"tau_bd\": 0.02\n        },\n        {\n            \"L\": np.array([\n                [0.5, 0.555, 0.596, 0.553, 0.698, 0.754, 0.797],\n                [0.302, 0.367, 0.444, 0.508, 0.433, 0.646, 0.721],\n                [0.597, 0.572, 0.458, 0.511, 0.479, 0.453, 0.418],\n                [0.401, 0.418, 0.443, 0.456, 0.482, 0.379, 0.522]\n            ]),\n            \"D\": [(2, 1, 3), (3, 2, 4), (4, 3, 5)],\n            \"tau_ev\": 0.97,\n            \"tau_sam\": 0.06,\n            \"tau_bd\": 0.035\n        }\n    ]\n\n    def calculate_band_depth(spectrum, w, l, r):\n        \"\"\"\n        Calculates band depth for a given spectrum and diagnostic indices.\n        Indices w, l, r are 1-based.\n        \"\"\"\n        w_idx, l_idx, r_idx = w - 1, l - 1, r - 1\n        x_w, x_l, x_r = spectrum[w_idx], spectrum[l_idx], spectrum[r_idx]\n        \n        # Continuum calculation\n        if r == l:\n            # Avoid division by zero, though problem constraints should prevent this.\n            return np.nan \n        \n        continuum_val = x_l + (float(w - l) / float(r - l)) * (x_r - x_l)\n        \n        if continuum_val == 0:\n            # Avoid division by zero\n            return np.nan\n\n        return 1.0 - (x_w / continuum_val)\n\n    def process_library(L, D_set, tau_ev, tau_sam, tau_bd):\n        \"\"\"\n        Processes a single library to find the minimal number of components K.\n        \"\"\"\n        N, B = L.shape\n\n        # Step 1: PCA\n        mu = L.mean(axis=0)\n        X = L - mu\n        # Using ddof=1 for sample covariance (division by N-1)\n        C = np.cov(X, rowvar=False, ddof=1)\n        \n        # Step 2: Eigendecomposition\n        eigenvalues, eigenvectors = np.linalg.eigh(C)\n        # eigh returns eigenvalues in ascending order, so we reverse them\n        sorted_indices = np.argsort(eigenvalues)[::-1]\n        eigenvalues = eigenvalues[sorted_indices]\n        eigenvectors = eigenvectors[:, sorted_indices]\n        \n        total_variance = np.sum(eigenvalues)\n\n        # Step 3: Iterative search for K\n        for K in range(1, B + 1):\n            # 3.1: CEV check\n            cev = np.sum(eigenvalues[:K]) / total_variance\n            if cev < tau_ev:\n                continue\n\n            # If CEV met, proceed with diagnostic checks\n            UK = eigenvectors[:, :K]\n            \n            all_criteria_met_for_K = True\n            for i in range(N):\n                x = L[i, :]\n                \n                # 3.2: Reconstruction\n                x_centered = x - mu\n                x_hat = mu + UK @ (UK.T @ x_centered)\n\n                # 3.2.a: SAM check\n                dot_product = np.dot(x, x_hat)\n                norm_x = np.linalg.norm(x)\n                norm_x_hat = np.linalg.norm(x_hat)\n                \n                # Clip argument to arccos to handle potential floating point errors\n                cos_theta = np.clip(dot_product / (norm_x * norm_x_hat), -1.0, 1.0)\n                sam_angle = np.arccos(cos_theta)\n                \n                if sam_angle >= tau_sam:\n                    all_criteria_met_for_K = False\n                    break\n                \n                # 3.2.b: Band depth check\n                for (w, l, r) in D_set:\n                    bd_orig = calculate_band_depth(x, w, l, r)\n                    bd_recon = calculate_band_depth(x_hat, w, l, r)\n                    \n                    if np.abs(bd_orig - bd_recon) >= tau_bd:\n                        all_criteria_met_for_K = False\n                        break\n                \n                if not all_criteria_met_for_K:\n                    break\n\n            if all_criteria_met_for_K:\n                return K\n        \n        # This case is technically handled by the loop which will always find K=B\n        # if no smaller K works, as reconstruction is perfect.\n        return B\n\n    results = []\n    for case in test_cases:\n        K = process_library(\n            case[\"L\"], case[\"D\"], case[\"tau_ev\"], case[\"tau_sam\"], case[\"tau_bd\"]\n        )\n        results.append(K)\n    \n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "A primary goal of developing spectral libraries is to enable the identification of unknown materials. This task goes beyond simple template matching, as it requires a robust understanding of the natural intra-class variability within a material category. This practice guides you in translating a spectral library's statistical properties—its mean and covariance—into a powerful decision-making tool. You will implement a foundational Mahalanobis classifier, which uses the squared Mahalanobis distance $D^2(x) = (x - \\hat{\\mu})^\\top \\boldsymbol{\\Sigma}^{-1} (x - \\hat{\\mu})$ to account for inter-band correlations and create a statistically-principled threshold for classifying new spectra. ",
            "id": "3853576",
            "problem": "You are given a collection of spectral reflectance vectors representing entries in a spectral library for a single mineral class. Each entry is a reflectance spectrum sampled at fixed wavelengths in nanometers (nm). Reflectance is unitless and bounded between $0$ and $1$. Your task is to quantify intra-class variability using the within-class covariance and to compute classifier thresholds under a multivariate normal assumption.\n\nStarting from core definitions and well-tested formulas in statistical pattern recognition and radiometry:\n\n- Let $X \\in \\mathbb{R}^{n \\times p}$ denote the matrix of $n$ library entries, each with $p$ spectral bands (wavelengths). The sample mean vector is $\\hat{\\mu} = \\frac{1}{n}\\sum_{i=1}^{n} x_i$, and the sample covariance matrix is $\\mathbf{S} = \\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\hat{\\mu})(x_i - \\hat{\\mu})^\\top$.\n- To ensure numerical stability and model instrument noise, use a Tikhonov-style regularization $\\boldsymbol{\\Sigma}_\\lambda = \\mathbf{S} + \\lambda \\mathbf{I}_p$, where $\\lambda > 0$ is a scalar regularization parameter and $\\mathbf{I}_p$ is the $p \\times p$ identity matrix.\n- Define a scalar intra-class variability metric as the average per-band variance $v_{\\text{avg}} = \\frac{\\operatorname{tr}(\\boldsymbol{\\Sigma}_\\lambda)}{p}$, which has units of reflectance squared (unitless squared).\n- Under the multivariate normal assumption, the squared Mahalanobis distance $D^2(x) = (x - \\hat{\\mu})^\\top \\boldsymbol{\\Sigma}_\\lambda^{-1} (x - \\hat{\\mu})$ follows a chi-square distribution with $p$ degrees of freedom when $x$ belongs to the class. Thus, the decision rule \"$D^2(x) \\le \\tau_\\alpha$\" achieves a nominal false positive rate (Type I error) $\\alpha$ with $\\tau_\\alpha = \\chi^2_{p, 1-\\alpha}$, the $(1-\\alpha)$ quantile of the chi-square distribution with $p$ degrees of freedom.\n\nImplement a program that, for each provided test case:\n1. Computes $\\hat{\\mu}$, $\\mathbf{S}$, and $\\boldsymbol{\\Sigma}_\\lambda$.\n2. Computes $v_{\\text{avg}} = \\frac{\\operatorname{tr}(\\boldsymbol{\\Sigma}_\\lambda)}{p}$ (a float).\n3. Computes the squared Mahalanobis distance threshold $\\tau_\\alpha = \\chi^2_{p, 1-\\alpha}$ (a float).\n4. For each query spectrum $q$, computes $D^2(q)$ and returns a boolean decision indicating whether $D^2(q) \\le \\tau_\\alpha$.\n\nAnswer requirements:\n- Reflectance is unitless; wavelengths must be treated as given in nanometers (nm) but do not change the unitless nature of reflectance. Report $v_{\\text{avg}}$ in unitless reflectance squared. Report $\\tau_\\alpha$ as a unitless float. Report classification decisions as booleans.\n- Angles are not involved; do not convert units.\n- The final output for each test case must be a list containing $[v_{\\text{avg}}, \\tau_\\alpha, \\text{decisions}]$, where $\\text{decisions}$ is a list of booleans, one per query spectrum.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For example: \"[result1,result2,result3]\".\n\nUse the following test suite of three cases, each described by $(\\text{wavelengths in nm}, X, Q, \\alpha, \\lambda)$:\n\n- Case A (general \"happy path\"):\n  - Wavelengths (nm): $[450, 550, 650, 850, 1600, 2200]$.\n  - Library $X$ (unitless reflectance):\n    - $x_1 = [0.08, 0.12, 0.14, 0.18, 0.24, 0.28]$\n    - $x_2 = [0.09, 0.11, 0.15, 0.17, 0.23, 0.27]$\n    - $x_3 = [0.085, 0.115, 0.145, 0.175, 0.235, 0.275]$\n    - $x_4 = [0.082, 0.118, 0.142, 0.182, 0.238, 0.279]$\n    - $x_5 = [0.087, 0.113, 0.147, 0.178, 0.236, 0.276]$\n  - Queries $Q$ (unitless reflectance):\n    - $q_1 = [0.086, 0.114, 0.146, 0.177, 0.235, 0.276]$\n    - $q_2 = [0.10, 0.12, 0.16, 0.19, 0.25, 0.30]$\n  - False positive rate $\\alpha = 0.05$.\n  - Regularization $\\lambda = 0.00001$.\n\n- Case B (boundary case: extremely low variability within class, stricter false positive rate):\n  - Wavelengths (nm): $[450, 550, 650, 850, 1600, 2200]$.\n  - Library $X$ (unitless reflectance):\n    - $x_1 = [0.30, 0.28, 0.26, 0.24, 0.22, 0.20]$\n    - $x_2 = [0.3005, 0.2795, 0.2605, 0.2405, 0.2205, 0.2005]$\n    - $x_3 = [0.2995, 0.2805, 0.2595, 0.2395, 0.2195, 0.1995]$\n    - $x_4 = [0.30, 0.28, 0.26, 0.24, 0.22, 0.20]$\n  - Queries $Q$ (unitless reflectance):\n    - $q_1 = [0.30, 0.28, 0.26, 0.24, 0.22, 0.20]$\n    - $q_2 = [0.31, 0.29, 0.27, 0.25, 0.23, 0.21]$\n  - False positive rate $\\alpha = 0.01$.\n  - Regularization $\\lambda = 0.000001$.\n\n- Case C (edge case: small sample size with near-singular covariance, requiring regularization):\n  - Wavelengths (nm): $[450, 550, 650, 850, 1600, 2200]$.\n  - Library $X$ (unitless reflectance):\n    - $x_1 = [0.12, 0.18, 0.25, 0.31, 0.38, 0.44]$\n    - $x_2 = [0.13, 0.195, 0.27, 0.335, 0.41, 0.475]$\n  - Queries $Q$ (unitless reflectance):\n    - $q_1 = [0.125, 0.1875, 0.2575, 0.3225, 0.395, 0.4575]$\n    - $q_2 = [0.10, 0.22, 0.20, 0.30, 0.45, 0.40]$\n  - False positive rate $\\alpha = 0.05$.\n  - Regularization $\\lambda = 0.0001$.\n\nYour program must compute the specified quantities for each case and produce a single line: a top-level list containing one sublist per case, each sublist of the form $[v_{\\text{avg}}, \\tau_\\alpha, \\text{decisions}]$. All values are unitless; report floats and booleans exactly as computed by the program.",
            "solution": "The problem requires the implementation of a statistical classifier for spectral data based on the multivariate normal distribution and Mahalanobis distance. This involves calculating key statistics from a library of known spectra, establishing a decision threshold based on a specified false positive rate, and then classifying new query spectra. The process is grounded in fundamental principles of statistical pattern recognition.\n\nFirst, let's formalize the given data for a single class. We are provided with a set of $n$ spectral reflectance vectors, each measured at $p$ distinct wavelengths. This dataset can be represented as a matrix $X \\in \\mathbb{R}^{n \\times p}$, where each row $x_i$ for $i \\in \\{1, \\dots, n\\}$ is a single spectrum.\n\nThe first step is to model the central tendency and dispersion of the data. Assuming the spectral vectors for the given mineral class are samples from a multivariate distribution, we can estimate its parameters.\nThe central tendency is estimated by the sample mean vector $\\hat{\\mu} \\in \\mathbb{R}^p$, calculated as the average of the sample vectors:\n$$\n\\hat{\\mu} = \\frac{1}{n} \\sum_{i=1}^{n} x_i\n$$\nThe dispersion and correlation between spectral bands are captured by the sample covariance matrix $\\mathbf{S} \\in \\mathbb{R}^{p \\times p}$. It is computed using the unbiased estimator:\n$$\n\\mathbf{S} = \\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\hat{\\mu})(x_i - \\hat{\\mu})^\\top\n$$\nwhere $(x_i - \\hat{\\mu})$ is a column vector. The division by $n-1$ represents Bessel's correction for an unbiased estimate.\n\nIn practice, the sample covariance matrix $\\mathbf{S}$ can be ill-conditioned or singular, especially when the number of samples $n$ is not significantly larger than the number of dimensions $p$. This is particularly true in Case C where $n=2$ and $p=6$, ensuring $\\mathbf{S}$ is rank-deficient. To ensure numerical stability for matrix inversion and to model potential sensor noise not captured in the library spectra, a Tikhonov-style regularization is applied. The regularized covariance matrix, $\\boldsymbol{\\Sigma}_\\lambda$, is defined as:\n$$\n\\boldsymbol{\\Sigma}_\\lambda = \\mathbf{S} + \\lambda \\mathbf{I}_p\n$$\nwhere $\\lambda > 0$ is a small scalar regularization parameter and $\\mathbf{I}_p$ is the $p \\times p$ identity matrix. This operation adds $\\lambda$ to the diagonal elements of $\\mathbf{S}$, effectively increasing the variance in each band by a small amount, which guarantees that $\\boldsymbol{\\Sigma}_\\lambda$ is positive definite and thus invertible.\n\nNext, we quantify the intra-class variability. A simple scalar metric for this is the average per-band variance, $v_{\\text{avg}}$, which is defined as the mean of the diagonal elements (the trace) of the regularized covariance matrix:\n$$\nv_{\\text{avg}} = \\frac{\\operatorname{tr}(\\boldsymbol{\\Sigma}_\\lambda)}{p}\n$$\nThe trace of $\\boldsymbol{\\Sigma}_\\lambda$ is the sum of the regularized variances of each spectral band.\n\nThe core of the classifier is the decision rule. We model the class with a multivariate normal distribution $\\mathcal{N}(\\hat{\\mu}, \\boldsymbol{\\Sigma}_\\lambda)$. For a query spectrum $q$, its \"distance\" from this class distribution can be measured by the squared Mahalanobis distance, $D^2(q)$:\n$$\nD^2(q) = (q - \\hat{\\mu})^\\top \\boldsymbol{\\Sigma}_\\lambda^{-1} (q - \\hat{\\mu})\n$$\nThis distance metric is statistically powerful because it accounts for the variance of each band and the covariance between bands. It measures the distance in units of standard deviation, effectively creating an elliptical decision boundary that conforms to the shape of the data cloud.\n\nA key theorem in multivariate statistics states that if a vector $x$ is drawn from a $p$-variate normal distribution with mean $\\mu$ and covariance $\\Sigma$, then the squared Mahalanobis distance $(x - \\mu)^\\top \\Sigma^{-1} (x - \\mu)$ follows a chi-square distribution with $p$ degrees of freedom, denoted $\\chi^2_p$.\n\nThis property allows us to construct a decision rule with a statistically controlled error rate. We want to accept a query $q$ as a member of the class if its Mahalanobis distance is not unusually large. For a specified nominal false positive rate (Type I error) $\\alpha$, we find a threshold $\\tau_\\alpha$ such that the probability of a true member having a distance greater than the threshold is $\\alpha$. This threshold is the $(1-\\alpha)$ quantile of the chi-square distribution with $p$ degrees of freedom:\n$$\nP(D^2(x) > \\tau_\\alpha) = \\alpha \\quad \\implies \\quad \\tau_\\alpha = \\chi^2_{p, 1-\\alpha}\n$$\nThe decision rule is then: classify query $q$ as belonging to the class if $D^2(q) \\le \\tau_\\alpha$.\n\nThe implementation will proceed by calculating these quantities for each test case.\n1.  Read the input data $(X, Q, \\alpha, \\lambda)$ and determine dimensions $n$ and $p$.\n2.  Compute $\\hat{\\mu}$ using `numpy.mean`.\n3.  Compute $\\mathbf{S}$ using `numpy.cov` with `ddof=1`.\n4.  Compute $\\boldsymbol{\\Sigma}_\\lambda = \\mathbf{S} + \\lambda \\mathbf{I}_p$.\n5.  Compute $v_{\\text{avg}} = \\frac{\\operatorname{tr}(\\boldsymbol{\\Sigma}_\\lambda)}{p}$.\n6.  Compute $\\tau_\\alpha$ using the percent point function (`ppf`) from `scipy.stats.chi2` with $p$ degrees of freedom and a quantile of $1-\\alpha$.\n7.  For each query spectrum $q \\in Q$, compute $D^2(q)$ using `numpy.linalg.inv` to find $\\boldsymbol{\\Sigma}_\\lambda^{-1}$ and perform the matrix-vector multiplications.\n8.  Compare each $D^2(q)$ to $\\tau_\\alpha$ to get a boolean decision.\n9.  Assemble the results $[v_{\\text{avg}}, \\tau_\\alpha, \\text{decisions}]$ for each case and format them into the specified final output string.",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import chi2\n\ndef solve():\n    \"\"\"\n    Solves the spectral classification problem for all given test cases.\n    \"\"\"\n    \n    test_cases = [\n        {\n            \"id\": \"A\",\n            \"X\": np.array([\n                [0.08, 0.12, 0.14, 0.18, 0.24, 0.28],\n                [0.09, 0.11, 0.15, 0.17, 0.23, 0.27],\n                [0.085, 0.115, 0.145, 0.175, 0.235, 0.275],\n                [0.082, 0.118, 0.142, 0.182, 0.238, 0.279],\n                [0.087, 0.113, 0.147, 0.178, 0.236, 0.276]\n            ]),\n            \"Q\": np.array([\n                [0.086, 0.114, 0.146, 0.177, 0.235, 0.276],\n                [0.10, 0.12, 0.16, 0.19, 0.25, 0.30]\n            ]),\n            \"alpha\": 0.05,\n            \"lambda_reg\": 0.00001\n        },\n        {\n            \"id\": \"B\",\n            \"X\": np.array([\n                [0.30, 0.28, 0.26, 0.24, 0.22, 0.20],\n                [0.3005, 0.2795, 0.2605, 0.2405, 0.2205, 0.2005],\n                [0.2995, 0.2805, 0.2595, 0.2395, 0.2195, 0.1995],\n                [0.30, 0.28, 0.26, 0.24, 0.22, 0.20]\n            ]),\n            \"Q\": np.array([\n                [0.30, 0.28, 0.26, 0.24, 0.22, 0.20],\n                [0.31, 0.29, 0.27, 0.25, 0.23, 0.21]\n            ]),\n            \"alpha\": 0.01,\n            \"lambda_reg\": 0.000001\n        },\n        {\n            \"id\": \"C\",\n            \"X\": np.array([\n                [0.12, 0.18, 0.25, 0.31, 0.38, 0.44],\n                [0.13, 0.195, 0.27, 0.335, 0.41, 0.475]\n            ]),\n            \"Q\": np.array([\n                [0.125, 0.1875, 0.2575, 0.3225, 0.395, 0.4575],\n                [0.10, 0.22, 0.20, 0.30, 0.45, 0.40]\n            ]),\n            \"alpha\": 0.05,\n            \"lambda_reg\": 0.0001\n        }\n    ]\n\n    def process_case(X, Q, alpha, lambda_reg):\n        \"\"\"\n        Computes variability, threshold, and classification decisions for a single case.\n        \"\"\"\n        n, p = X.shape\n\n        # 1. Compute mu_hat, S, and Sigma_lambda\n        mu_hat = np.mean(X, axis=0)\n        \n        # In the case of n=1, np.cov returns 0, but the formula requires undefined or NaN.\n        # The problem constraints ensure n > 1 for all cases, so this is safe.\n        S = np.cov(X, rowvar=False, ddof=1)\n        \n        # For a single sample (n=1), S would be an array of NaNs if dimensions are > 1, \n        # or 0 for 1-D data. As n>=2, S is well-defined.\n        # In case p=1 and n > 1, np.cov returns a 0-d array (scalar), need to reshape.\n        if p == 1:\n            S = np.array([[S]])\n\n        I_p = np.identity(p)\n        Sigma_lambda = S + lambda_reg * I_p\n\n        # 2. Compute v_avg\n        v_avg = np.trace(Sigma_lambda) / p\n\n        # 3. Compute tau_alpha\n        tau_alpha = chi2.ppf(1 - alpha, df=p)\n\n        # 4. Compute D^2(q) and decisions for each query\n        try:\n            Sigma_lambda_inv = np.linalg.inv(Sigma_lambda)\n        except np.linalg.LinAlgError:\n            # This should not happen due to regularization\n            return [v_avg, tau_alpha, [False] * len(Q)]\n        \n        decisions = []\n        for q in Q:\n            delta = q - mu_hat\n            # D^2(q) = (q - mu_hat)^T * Sigma_lambda_inv * (q - mu_hat)\n            d2 = delta @ Sigma_lambda_inv @ delta\n            decisions.append(d2 <= tau_alpha)\n            \n        return [v_avg, tau_alpha, decisions]\n\n    all_results = []\n    for case in test_cases:\n        result = process_case(case[\"X\"], case[\"Q\"], case[\"alpha\"], case[\"lambda_reg\"])\n        all_results.append(result)\n\n    # Format the final output string to precisely match \"[result1,result2,...]\"\n    # with no spaces.\n    def format_result(item):\n        if isinstance(item, list):\n            return f\"[{','.join(format_result(x) for x in item)}]\"\n        elif isinstance(item, bool):\n            return str(item)  # 'True' or 'False'\n        else: # float\n            return str(item)\n\n    results_str_list = [format_result(res) for res in all_results]\n    final_output = f\"[{','.join(results_str_list)}]\"\n    \n    print(final_output)\n\nsolve()\n```"
        }
    ]
}