{
    "hands_on_practices": [
        {
            "introduction": "光谱数据在采集过程中不可避免地会受到仪器噪声的干扰。本练习旨在通过一个实际的编码任务，让您掌握一种常见的光谱平滑技术——Savitzky-Golay（SG）滤波，并定量评估其在去噪和保留关键诊断性吸收特征之间的权衡。通过该实践 ，您将深入理解滤波器参数（如窗口大小和多项式阶数）如何影响光谱信息的保真度。",
            "id": "3853543",
            "problem": "您的任务是设计并执行一项定量评估，分析Savitzky–Golay (SG) 滤波对反射光谱中窄吸收特征保留的影响。这些光谱用于遥感和环境建模的光谱库中。评估必须在一个科学上合理且基于第一性原理构建的合成光谱上进行。最终程序必须为指定的 SG 滤波器配置计算特征衰减的定量度量，并以单行、机器可读的格式输出结果。所有波长值必须以纳米 (nm) 为单位处理。所有衰减值必须表示为四舍五入到 $6$ 位小数的小数。\n\n从以下基础和定义开始：\n\n1.  构建一个波长轴 $\\lambda$，从 $400$ nm 到 $800$ nm，以 $1$ nm 为步长进行均匀采样。将 $\\lambda$ 表示为以纳米为单位的整数网格 $\\{400, 401, \\dots, 800\\}$。\n\n2.  通过以下公式定义一个平滑、缓慢变化的连续统反射率 $R_{\\text{cont}}(\\lambda)$：\n    $$\n    R_{\\text{cont}}(\\lambda) = 0.3 + 0.0002\\left(\\lambda - 400\\right) + 0.03 \\sin\\left( \\frac{2\\pi}{300} \\left(\\lambda - 400\\right) \\right).\n    $$\n    这是一个无量纲的反射率模型，包含一个线性斜率和一个宽正弦分量。\n\n3.  叠加一个中心位于 $\\lambda_0 = 550$ nm、具有高斯形状、分数深度 $A = 0.12$、标准差 $\\sigma_\\lambda = 2$ nm 的单一窄吸收特征，得到特征调制\n    $$\n    M(\\lambda) = 1 - A \\exp\\left(-\\frac{\\left(\\lambda - \\lambda_0\\right)^2}{2\\sigma_\\lambda^2}\\right).\n    $$\n\n4.  将加性仪器噪声 $\\epsilon(\\lambda)$ 建模为独立同分布的高斯随机变量，其均值为零，标准差为 $\\sigma_\\epsilon = 0.005$，即 $\\epsilon(\\lambda) \\sim \\mathcal{N}\\left(0, \\sigma_\\epsilon^2\\right)$。为确保可复现性，将随机种子固定为 $42$。\n\n5.  观测反射光谱定义为\n    $$\n    R(\\lambda) = R_{\\text{cont}}(\\lambda) \\cdot M(\\lambda) + \\epsilon(\\lambda).\n    $$\n\n6.  使用连接 $\\lambda_L = 535$ nm 和 $\\lambda_R = 565$ nm 两个肩部的线性连续统 $C(\\lambda)$ 来定义特征周围的连续统去除。对于任何信号 $S(\\lambda)$（无论是 $R(\\lambda)$ 还是其滤波版本），构建\n    $$\n    C_S(\\lambda) = S(\\lambda_L) + \\left(\\lambda - \\lambda_L\\right) \\cdot \\frac{S(\\lambda_R) - S(\\lambda_L)}{\\lambda_R - \\lambda_L}.\n    $$\n    信号 $S$ 在中心处的连续统去除后特征深度为\n    $$\n    d_S = 1 - \\frac{S(\\lambda_0)}{C_S(\\lambda_0)}.\n    $$\n\n7.  窗口长度为 $W$（奇数）和多项式阶数为 $P$ 的 Savitzky–Golay (SG) 滤波器是一种局部最小二乘多项式平滑器：在每个包含 $W$ 个样本的窗口内，找到一个 $P$ 次多项式的系数，使与数据的平方误差最小化，并使用该多项式在窗口中心的值作为滤波后的输出。将 SG 滤波器应用于 $R(\\lambda)$，为每个测试用例 $(W,P)$ 生成 $R_f^{(W,P)}(\\lambda)$。\n\n8.  为每个测试用例 $(W,P)$ 定义衰减度量，即由平滑引起的特征深度分数损失：\n    $$\n    a^{(W,P)} = \\frac{d_{R} - d_{R_f^{(W,P)}}}{d_{R}},\n    $$\n    其中 $d_R$ 是从原始含噪光谱 $R(\\lambda)$ 计算的深度，而 $d_{R_f^{(W,P)}}$ 是从滤波后光谱 $R_f^{(W,P)}(\\lambda)$ 计算的深度。正值表示特征的衰减（损失）；负值表示因噪声抑制而产生的表观增强。\n\n您的程序必须实现 $R(\\lambda)$ 的构建，为每个测试用例应用 SG 滤波器，计算 $a^{(W,P)}$，并以精确指定的格式打印结果。不允许外部输入。\n\n测试套件：\n-   用例 1: $(W,P) = (7,2)$\n-   用例 2: $(W,P) = (21,2)$\n-   用例 3: $(W,P) = (11,4)$\n-   用例 4: $(W,P) = (5,4)$\n-   用例 5: $(W,P) = (51,3)$\n\n约束条件：\n-   $W$ 必须是严格大于 $P$ 的奇数。\n-   所有计算都在定义的以纳米为单位的波长网格上进行。\n-   反射率是无量纲的。\n\n答案规格：\n-   对于每个测试用例，计算 $a^{(W,P)}$ 并四舍五入到 $6$ 位小数。\n-   您的程序应生成一行输出，其中包含按测试套件顺序排列的结果，格式为用方括号括起来的逗号分隔列表，例如，\"[a1,a2,a3,a4,a5]\"。",
            "solution": "在尝试求解之前，对问题陈述的有效性进行评估。\n\n**问题验证**\n\n1.  **提取给定条件**：\n    -   波长轴：$\\lambda$ 是一个从 $400$ nm 到 $800$ nm 的均匀整数网格，即 $\\lambda \\in \\{400, 401, \\dots, 800\\}$。\n    -   连续统反射率模型：$R_{\\text{cont}}(\\lambda) = 0.3 + 0.0002\\left(\\lambda - 400\\right) + 0.03 \\sin\\left( \\frac{2\\pi}{300} \\left(\\lambda - 400\\right) \\right)$。\n    -   吸收特征调制：$M(\\lambda) = 1 - A \\exp\\left(-\\frac{\\left(\\lambda - \\lambda_0\\right)^2}{2\\sigma_\\lambda^2}\\right)$，中心 $\\lambda_0 = 550$ nm，分数深度 $A = 0.12$，标准差 $\\sigma_\\lambda = 2$ nm。\n    -   加性噪声模型：$\\epsilon(\\lambda) \\sim \\mathcal{N}\\left(0, \\sigma_\\epsilon^2\\right)$，其中 $\\sigma_\\epsilon = 0.005$。随机种子固定为 $42$。\n    -   观测反射率模型：$R(\\lambda) = R_{\\text{cont}}(\\lambda) \\cdot M(\\lambda) + \\epsilon(\\lambda)$。\n    -   连续统去除：在线性连续统 $C_S(\\lambda)$ 定义在肩部波长 $\\lambda_L = 535$ nm 和 $\\lambda_R = 565$ nm 之间。\n    -   特征深度定义：$d_S = 1 - \\frac{S(\\lambda_0)}{C_S(\\lambda_0)}$。\n    -   Savitzky-Golay (SG) 滤波器应用：将窗口长度为 $W$、多项式阶数为 $P$ 的滤波器应用于 $R(\\lambda)$，得到 $R_f^{(W,P)}(\\lambda)$。\n    -   衰减度量：$a^{(W,P)} = \\frac{d_{R} - d_{R_f^{(W,P)}}}{d_{R}}$。\n    -   测试套件：$(W,P)$ 对为 $(7,2)$、$(21,2)$、$(11,4)$、$(5,4)$ 和 $(51,3)$。\n    -   约束条件：$W$ 必须是奇数且 $W > P$。\n\n2.  **使用提取的给定条件进行验证**：\n    -   **科学基础**：该问题在光谱学和信号处理原理方面有很好的基础。通过连续统、吸收特征和噪声构建合成光谱是测试遥感算法的标准方法。Savitzky-Golay 滤波器和线性连续统去除是已建立并广泛使用的技术。\n    -   **适定性**：问题被完全指定，提供了所有必要的方程、参数和程序步骤。使用固定的随机种子确保了唯一、可复现的解。\n    -   **客观性**：问题使用精确、数学和客观的语言陈述，没有任何主观因素。\n    -   **完整性与一致性**：定义是自洽的。提供的测试用例，例如 $(W,P) = (5,4)$，满足指定的约束条件（$5$ 是奇数且 $5>4$）。问题是完整的，不包含任何矛盾。\n\n3.  **结论与行动**：\n    -   该问题被认定为**有效**。将逐步推导求解过程。\n\n**求解推导**\n\n求解需要实现一个计算工作流，以量化 Savitzky-Golay (SG) 滤波对窄光谱吸收特征的影响。该过程分为以下几个步骤。\n\n**步骤 1：合成光谱生成**\n首先，我们构建合成反射光谱。这为我们的分析提供了一个受控的信号。波长轴 $\\lambda$ 被定义为一个从 $400$ nm 到 $800$ nm 的均匀整数网格。\n$$ \\lambda = \\{400, 401, \\dots, 800\\} $$\n对于 $\\lambda$ 中的每个波长，使用指定方程计算平滑、缓慢变化的连续统反射率 $R_{\\text{cont}}(\\lambda)$：\n$$ R_{\\text{cont}}(\\lambda) = 0.3 + 0.0002\\left(\\lambda - 400\\right) + 0.03 \\sin\\left( \\frac{2\\pi}{300} \\left(\\lambda - 400\\right) \\right) $$\n在此连续统上叠加一个高斯吸收特征。该特征由一个乘法调制因子 $M(\\lambda)$ 描述，其由中心 $\\lambda_0 = 550$ nm、分数深度 $A = 0.12$ 和标准差 $\\sigma_\\lambda = 2$ nm 决定。\n$$ M(\\lambda) = 1 - A \\exp\\left(-\\frac{\\left(\\lambda - \\lambda_0\\right)^2}{2\\sigma_\\lambda^2}\\right) $$\n为模拟真实测量，生成加性仪器噪声 $\\epsilon(\\lambda)$。为保证可复现性，伪随机数生成器使用种子值 $42$ 进行初始化。每个波长的噪声是从均值为 $0$、标准差为 $\\sigma_\\epsilon = 0.005$ 的正态分布中独立抽取的样本。\n最终的观测反射光谱 $R(\\lambda)$ 是这三个分量的组合：\n$$ R(\\lambda) = R_{\\text{cont}}(\\lambda) \\cdot M(\\lambda) + \\epsilon(\\lambda) $$\n\n**步骤 2：特征深度计算**\n吸收特征的量化需要测量其相对于局部连续统的深度。这是定量光谱学中的一个标准程序。对于任何给定的光谱 $S(\\lambda)$，通过连接两个肩部点 $\\lambda_L = 535$ nm 和 $\\lambda_R = 565$ nm 处的光谱值，建立一个线性连续统 $C_S(\\lambda)$。\n$$ C_S(\\lambda) = S(\\lambda_L) + \\left(\\lambda - \\lambda_L\\right) \\cdot \\frac{S(\\lambda_R) - S(\\lambda_L)}{\\lambda_R - \\lambda_L} $$\n然后在特征中心 $\\lambda_0 = 550$ nm 处计算连续统去除后的特征深度 $d_S$。它定义为\n$$ d_S = 1 - \\frac{S(\\lambda_0)}{C_S(\\lambda_0)} $$\n该指标提供了特征强度的归一化度量。首先将此计算应用于原始含噪光谱 $R(\\lambda)$，以建立基线特征深度 $d_R$。\n\n**步骤 3：Savitzky–Golay 滤波器的应用**\n对于每个由窗口长度 $W$ 和多项式阶数 $P$ 定义的测试用例，将 SG 滤波器应用于整个含噪光谱 $R(\\lambda)$。SG 滤波器是一种局部多项式回归平滑器，能有效减少高频噪声，同时比简单的移动平均更好地保留光谱特征的总体形状。滤波操作生成一个新的、平滑的光谱，记为 $R_f^{(W,P)}(\\lambda)$。\n\n**步骤 4：衰减测量**\n滤波后，使用与步骤 2 中完全相同的连续统去除和深度计算程序，为平滑光谱 $R_f^{(W,P)}(\\lambda)$ 重新计算特征深度。这得到滤波后深度 $d_{R_f^{(W,P)}}$。\n衰减 $a^{(W,P)}$ 是此问题的关键指标。它量化了滤波过程导致的特征深度分数损失。其计算公式为：\n$$ a^{(W,P)} = \\frac{d_{R} - d_{R_f^{(W,P)}}}{d_{R}} $$\n正值表示衰减（特征变得更浅），这是对窄于滤波器窗口的特征应用平滑滤波器时的预期结果。负值则表示特征深度的表观增强。\n\n**步骤 5：执行与格式化**\n执行该过程时，首先计算参考光谱 $R(\\lambda)$ 及其深度 $d_R$。然后，对于测试套件中的每个 $(W,P)$ 对——$(7,2)$、$(21,2)$、$(11,4)$、$(5,4)$ 和 $(51,3)$——生成相应的滤波光谱 $R_f^{(W,P)}(\\lambda)$，计算其深度 $d_{R_f^{(W,P)}}$，并确定衰减 $a^{(W,P)}$。每个得到的衰减值四舍五入到 $6$ 位小数。最后，将结果列表格式化为指定的单行字符串。",
            "answer": "```python\nimport numpy as np\nfrom scipy.signal import savgol_filter\n\ndef solve():\n    \"\"\"\n    Computes the attenuation of a synthetic spectral feature due to Savitzky-Golay\n    filtering for a set of filter parameters.\n    \"\"\"\n    \n    # Step 1: Define constants and generate the synthetic spectrum\n    \n    # Wavelength grid and feature parameters\n    lambdas = np.arange(400, 801, dtype=np.float64)\n    lambda_0 = 550.0\n    A = 0.12\n    sigma_lambda = 2.0\n    \n    # Noise parameters\n    sigma_eps = 0.005\n    seed = 42\n    \n    # Continuum removal parameters\n    lambda_L = 535.0\n    lambda_R = 565.0\n\n    # Map specific wavelengths to their corresponding array indices\n    # index = wavelength - start_wavelength\n    idx_0 = int(lambda_0 - lambdas[0])\n    idx_L = int(lambda_L - lambdas[0])\n    idx_R = int(lambda_R - lambdas[0])\n\n    # Construct the continuum reflectance\n    R_cont = 0.3 + 0.0002 * (lambdas - 400.0) + 0.03 * np.sin((2 * np.pi / 300.0) * (lambdas - 400.0))\n\n    # Construct the feature modulation\n    M = 1.0 - A * np.exp(-((lambdas - lambda_0)**2) / (2.0 * sigma_lambda**2))\n\n    # Generate reproducible additive noise\n    np.random.seed(seed)\n    epsilon = np.random.normal(0.0, sigma_eps, size=lambdas.shape)\n\n    # Combine components to create the final observed spectrum\n    R = R_cont * M + epsilon\n\n    # Step 2: Define a function for feature depth calculation\n    def calculate_depth(S, l_0, l_L, l_R, i_0, i_L, i_R):\n        \"\"\"\n        Calculates the continuum-removed depth of a feature in a spectrum S.\n        \"\"\"\n        S_L = S[i_L]\n        S_R = S[i_R]\n        S_0 = S[i_0]\n\n        # Calculate the value of the linear continuum at the feature center lambda_0\n        C_S_at_lambda_0 = S_L + (l_0 - l_L) * ((S_R - S_L) / (l_R - l_L))\n        \n        # Calculate the normalized depth\n        d_S = 1.0 - S_0 / C_S_at_lambda_0\n        return d_S\n\n    # Calculate the depth of the feature in the original noisy spectrum\n    d_R = calculate_depth(R, lambda_0, lambda_L, lambda_R, idx_0, idx_L, idx_R)\n\n    # Step 3  4: Apply filters and compute attenuation for each test case\n    test_cases = [\n        (7, 2),   # Case 1\n        (21, 2),  # Case 2\n        (11, 4),  # Case 3\n        (5, 4),   # Case 4\n        (51, 3)   # Case 5\n    ]\n    \n    results = []\n    for W, P in test_cases:\n        # Apply the Savitzky-Golay filter to the original spectrum\n        R_f = savgol_filter(R, window_length=W, polyorder=P)\n        \n        # Calculate the depth of the feature in the filtered spectrum\n        d_Rf = calculate_depth(R_f, lambda_0, lambda_L, lambda_R, idx_0, idx_L, idx_R)\n        \n        # Calculate the attenuation measure\n        attenuation = (d_R - d_Rf) / d_R\n        \n        results.append(round(attenuation, 6))\n\n    # Step 5: Format and print the final output\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "高光谱数据通常具有很高的维度，其中包含了大量的冗余信息。本练习将引导您应用主成分分析（PCA）这一强大的降维技术，来压缩光谱库的数据量。通过实践 ，您将学习如何确定保留多少主成分才能在满足累计解释方差要求的同时，有效保留用于后续光谱匹配的关键诊断信息，例如光谱角和吸收深度。",
            "id": "3853533",
            "problem": "给定三个小型光谱库，表示为无量纲反射率值的矩阵。每个库包含若干种材料的光谱，这些光谱在有限数量的离散波段上采样。假设有 $N$ 个光谱和 $B$ 个波段；将光谱库表示为矩阵 $\\mathbf{L} \\in \\mathbb{R}^{N \\times B}$，其行向量 $\\mathbf{x}_i \\in \\mathbb{R}^{B}$ 是单个光谱。目标是对每个光谱库应用主成分分析 (Principal Component Analysis, PCA)，量化可解释方差，并选择保留后续光谱匹配所需诊断信息的最小主成分数量。使用以下基本方法：\n\n- 数据中心化：定义列均值向量 $\\boldsymbol{\\mu} \\in \\mathbb{R}^{B}$ 和中心化数据矩阵 $\\mathbf{X} = \\mathbf{L} - \\mathbf{1}\\boldsymbol{\\mu}^{\\top}$，其中 $\\mathbf{1} \\in \\mathbb{R}^{N}$ 是全为1的向量。\n- 协方差：$\\mathbf{C} = \\frac{1}{N-1}\\mathbf{X}^{\\top}\\mathbf{X}$。\n- 特征分解：$\\mathbf{C}\\mathbf{u}_j = \\lambda_j \\mathbf{u}_j$，其中特征值 $\\lambda_j \\ge 0$，特征向量 $\\mathbf{u}_j$ 是标准正交的。\n- 可解释方差比率：$r_j = \\frac{\\lambda_j}{\\sum_{k=1}^{B} \\lambda_k}$。前 $K$ 个主成分的累积可解释方差为 $R(K) = \\sum_{j=1}^{K} r_j$。\n- 使用前 $K$ 个主成分进行重构：对于一个光谱 $\\mathbf{x}$，定义 $\\hat{\\mathbf{x}}(K) = \\boldsymbol{\\mu} + \\mathbf{U}_K \\mathbf{U}_K^{\\top}(\\mathbf{x}-\\boldsymbol{\\mu})$，其中 $\\mathbf{U}_K$ 是将前 $K$ 个特征向量作为列堆叠而成的矩阵。\n- 诊断信息保留标准：\n  1. 光谱角匹配 (Spectral Angle Mapper, SAM)：$\\mathbf{x}$ 和 $\\hat{\\mathbf{x}}(K)$ 之间的光谱角为 $\\theta(\\mathbf{x},\\hat{\\mathbf{x}}(K)) = \\arccos\\left(\\frac{\\mathbf{x}^{\\top}\\hat{\\mathbf{x}}(K)}{\\|\\mathbf{x}\\|_2 \\,\\|\\hat{\\mathbf{x}}(K)\\|_2}\\right)$，并且必须小于给定阈值 $\\tau_{\\text{sam}}$。角度以弧度表示。\n  2. 指定诊断吸收波段的波段深度保留。对于由索引 $w$（吸收波段）和连续统端点 $\\ell$、$r$ 组成的诊断规范 $(w,\\ell,r)$，在 $w$ 处的局部线性连续统定义为 $C_{\\mathbf{x}}(w) = x_{\\ell} + \\frac{w-\\ell}{r-\\ell}(x_{r}-x_{\\ell})$。波段深度为 $D_{\\mathbf{x}}(w) = 1 - \\frac{x_w}{C_{\\mathbf{x}}(w)}$。绝对重构误差 $|D_{\\mathbf{x}}(w) - D_{\\hat{\\mathbf{x}}(K)}(w)|$ 不得超过容差 $\\tau_{\\text{bd}}$。\n\n对于每个光谱库，你的程序必须：\n\n- 如上所述计算 PCA。\n- 确定最小整数 $K \\in \\{1, 2, \\dots, B\\}$，使得 $R(K) \\ge \\tau_{\\text{ev}}$ 和诊断信息保留标准对库中所有光谱以及所有指定的诊断波段三元组 $(w,\\ell,r)$ 都成立。如果对于所有 $K < B$ 的情况均不满足这些条件，则最小值为 $B$。\n\n使用以下测试套件：\n\n- **案例 1**\n  - 光谱库 `L`:\n    ```\n    [[0.5, 0.55, 0.6, 0.55, 0.7, 0.75, 0.8],\n     [0.3, 0.37, 0.44, 0.51, 0.43, 0.65, 0.72],\n     [0.6, 0.57, 0.46, 0.51, 0.48, 0.45, 0.42],\n     [0.4, 0.42, 0.44, 0.46, 0.48, 0.38, 0.52]]\n    ```\n  - 诊断波段 `D`: `[(2, 1, 3), (3, 2, 4), (4, 3, 5)]`\n  - 阈值: `tau_ev = 0.98`, `tau_sam = 0.05`, `tau_bd = 0.03`\n\n- **案例 2**\n  - 光谱库 `L`:\n    ```\n    [[0.3, 0.33, 0.31, 0.31, 0.42, 0.375],\n     [0.5, 0.48, 0.45, 0.45, 0.33, 0.405],\n     [0.4, 0.37, 0.46, 0.46, 0.5, 0.435]]\n    ```\n  - 诊断波段 `D`: `[(2, 1, 3), (4, 3, 5)]`\n  - 阈值: `tau_ev = 0.95`, `tau_sam = 0.02`, `tau_bd = 0.02`\n\n- **案例 3**\n  - 光谱库 `L`:\n    ```\n    [[0.5, 0.555, 0.596, 0.553, 0.698, 0.754, 0.797],\n     [0.302, 0.367, 0.444, 0.508, 0.433, 0.646, 0.721],\n     [0.597, 0.572, 0.458, 0.511, 0.479, 0.453, 0.418],\n     [0.401, 0.418, 0.443, 0.456, 0.482, 0.379, 0.522]]\n    ```\n  - 诊断波段 `D`: `[(2, 1, 3), (3, 2, 4), (4, 3, 5)]`\n  - 阈值: `tau_ev = 0.97`, `tau_sam = 0.06`, `tau_bd = 0.035`\n\n答案规格：\n- 您的程序应生成一行输出，其中包含按案例顺序排列的三个整数 $K$ 的值，格式为用方括号括起来的逗号分隔列表，例如 `[K1,K2,K3]`。",
            "solution": "该问题要求我们确定表示光谱库所需的最小主成分数 $K$，同时保留指定量的总方差和关键诊断特征。这是高光谱数据分析中常见的降维任务，其目标是在不丢失科学相关信息的情况下，减少数据量和计算成本。解决方案包括对每个光谱库执行主成分分析 (PCA)，然后针对递增的主成分数量迭代地检查一组标准。\n\n对于每个给定的光谱库 $\\mathbf{L} \\in \\mathbb{R}^{N \\times B}$，总体步骤如下：\n\n1.  **数据预处理与协方差计算**:\n    PCA 的第一步是数据中心化。均值光谱 $\\boldsymbol{\\mu} \\in \\mathbb{R}^{B}$ 是通过对库中所有 $N$ 个光谱的每个波段的反射率值求平均来计算的：\n    $$ \\boldsymbol{\\mu} = \\frac{1}{N}\\sum_{i=1}^{N} \\mathbf{x}_i^{\\top} $$\n    其中 $\\mathbf{x}_i$ 是表示 $\\mathbf{L}$ 中每个光谱的行向量。然后通过从每个光谱中减去该均值光谱来中心化数据矩阵：\n    $$ \\mathbf{X} = \\mathbf{L} - \\mathbf{1}\\boldsymbol{\\mu}^{\\top} $$\n    其中 $\\mathbf{1} \\in \\mathbb{R}^{N}$ 是一个全为1的列向量。然后根据中心化数据计算样本协方差矩阵 $\\mathbf{C} \\in \\mathbb{R}^{B \\times B}$。该矩阵量化了每个波段内的方差以及不同波段之间的协方差。\n    $$ \\mathbf{C} = \\frac{1}{N-1}\\mathbf{X}^{\\top}\\mathbf{X} $$\n\n2.  **特征分解**:\n    通过对协方差矩阵 $\\mathbf{C}$ 进行特征分解来找到主成分。\n    $$ \\mathbf{C}\\mathbf{u}_j = \\lambda_j \\mathbf{u}_j, \\quad j = 1, \\dots, B $$\n    这将产生一组特征值 $\\lambda_j$ 及其对应的特征向量 $\\mathbf{u}_j \\in \\mathbb{R}^{B}$。每个特征向量 $\\mathbf{u}_j$ 代表一个主成分，它是光谱空间中的一个新的正交基向量。对应的特征值 $\\lambda_j$ 表示数据在该主成分方向上的方差量。按照惯例，特征值及其特征向量按降序排序，使得 $\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_B \\ge 0$。\n\n3.  **迭代搜索最小 $K$ 值**:\n    我们搜索满足所有指定标准的最小整数 $K \\in \\{1, 2, \\dots, B\\}$。搜索过程从测试 $K=1$ 开始，然后是 $K=2$，依此类推，直到找到一个有效的 $K$。对于每个 $K$，我们执行三项检查。\n\n    3.1. **累积可解释方差 (CEV) 标准**:\n    第一个标准是前 $K$ 个主成分必须解释总方差的至少一个最小比例 $\\tau_{\\text{ev}}$。累积可解释方差比率 $R(K)$ 计算如下：\n    $$ R(K) = \\frac{\\sum_{j=1}^{K} \\lambda_j}{\\sum_{j=1}^{B} \\lambda_k} $$\n    条件是 $R(K) \\ge \\tau_{\\text{ev}}$。如果不满足此条件，则当前的 $K$ 不足，我们继续测试 $K+1$。\n\n    3.2. **诊断信息保留标准**:\n    如果满足 CEV 标准，我们必须接着验证基于 $K$ 个主成分的重构是否保留了特定的诊断特征。这需要使用前 $K$ 个主成分重构每个原始光谱 $\\mathbf{x}$。重构光谱 $\\hat{\\mathbf{x}}(K)$ 由下式给出：\n    $$ \\hat{\\mathbf{x}}(K) = \\boldsymbol{\\mu} + \\mathbf{U}_K \\mathbf{U}_K^{\\top}(\\mathbf{x}-\\boldsymbol{\\mu}) $$\n    在这里，$\\mathbf{U}_K = [\\mathbf{u}_1, \\dots, \\mathbf{u}_K]$ 是一个矩阵，其列是前 $K$ 个特征向量。然后使用两种度量将此重构与原始光谱 $\\mathbf{x}$ 进行比较。这些检查必须对光谱库 $\\mathbf{L}$ 中的所有光谱 $\\mathbf{x}_i$ 都成立。\n\n    a. **光谱角匹配 (SAM)**: SAM 度量衡量两个被视为向量的光谱之间的夹角。它是一种对光照不变的光谱形状相似性度量。该角度必须低于阈值 $\\tau_{\\text{sam}}$。\n    $$ \\theta(\\mathbf{x}, \\hat{\\mathbf{x}}(K)) = \\arccos\\left(\\frac{\\mathbf{x}^{\\top}\\hat{\\mathbf{x}}(K)}{\\|\\mathbf{x}\\|_2 \\,\\|\\hat{\\mathbf{x}}(K)\\|_2}\\right) < \\tau_{\\text{sam}} $$\n\n    b. **波段深度 (BD) 保留**: 该度量确保特定诊断吸收特征的深度得以保留。对于由波段索引三元组 $(w, \\ell, r)$ 指定的诊断特征——分别代表吸收谷、左连续统点和右连续统点——波段深度 $D_{\\mathbf{x}}(w)$ 是相对于局部线性连续统 $C_{\\mathbf{x}}(w)$ 计算的。\n    $$ C_{\\mathbf{x}}(w) = x_{\\ell} + \\frac{w-\\ell}{r-\\ell}(x_{r}-x_{\\ell}) $$\n    $$ D_{\\mathbf{x}}(w) = 1 - \\frac{x_w}{C_{\\mathbf{x}}(w)} $$\n    重构波段深度的绝对误差必须低于容差 $\\tau_{\\text{bd}}$。对于所有指定的诊断三元组 $(w, \\ell, r) \\in \\mathcal{D}$，都必须对此进行验证。\n    $$ |D_{\\mathbf{x}}(w) - D_{\\hat{\\mathbf{x}}(K)}(w)| < \\tau_{\\text{bd}} $$\n\n4.  **最终确定**:\n    对于库中所有光谱和所有指定的诊断波段，第一个同时满足 CEV 标准和两个诊断信息保留标准（SAM 和 BD）的 $K$ 值，即为所需的最小主成分数。如果没有 $K < B$ 的值满足所有条件，则结果为 $B$，因为使用全部 $B$ 个主成分的重构是完美的（即 $\\hat{\\mathbf{x}}(B) = \\mathbf{x}$），导致 SAM 和 BD 度量的误差为零，CEV 为1。\n\n实现过程将把这一完整流程应用于所提供的三个测试案例中的每一个。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of finding the minimal number of principal components\n    for three given spectral libraries based on specified criteria.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"L\": np.array([\n                [0.5, 0.55, 0.6, 0.55, 0.7, 0.75, 0.8],\n                [0.3, 0.37, 0.44, 0.51, 0.43, 0.65, 0.72],\n                [0.6, 0.57, 0.46, 0.51, 0.48, 0.45, 0.42],\n                [0.4, 0.42, 0.44, 0.46, 0.48, 0.38, 0.52]\n            ]),\n            \"D\": [(2, 1, 3), (3, 2, 4), (4, 3, 5)],\n            \"tau_ev\": 0.98,\n            \"tau_sam\": 0.05,\n            \"tau_bd\": 0.03\n        },\n        {\n            \"L\": np.array([\n                [0.3, 0.33, 0.31, 0.31, 0.42, 0.375],\n                [0.5, 0.48, 0.45, 0.45, 0.33, 0.405],\n                [0.4, 0.37, 0.46, 0.46, 0.5, 0.435]\n            ]),\n            \"D\": [(2, 1, 3), (4, 3, 5)],\n            \"tau_ev\": 0.95,\n            \"tau_sam\": 0.02,\n            \"tau_bd\": 0.02\n        },\n        {\n            \"L\": np.array([\n                [0.5, 0.555, 0.596, 0.553, 0.698, 0.754, 0.797],\n                [0.302, 0.367, 0.444, 0.508, 0.433, 0.646, 0.721],\n                [0.597, 0.572, 0.458, 0.511, 0.479, 0.453, 0.418],\n                [0.401, 0.418, 0.443, 0.456, 0.482, 0.379, 0.522]\n            ]),\n            \"D\": [(2, 1, 3), (3, 2, 4), (4, 3, 5)],\n            \"tau_ev\": 0.97,\n            \"tau_sam\": 0.06,\n            \"tau_bd\": 0.035\n        }\n    ]\n\n    def calculate_band_depth(spectrum, w, l, r):\n        \"\"\"\n        Calculates band depth for a given spectrum and diagnostic indices.\n        Indices w, l, r are 1-based.\n        \"\"\"\n        w_idx, l_idx, r_idx = w - 1, l - 1, r - 1\n        x_w, x_l, x_r = spectrum[w_idx], spectrum[l_idx], spectrum[r_idx]\n        \n        # Continuum calculation\n        if r == l:\n            # Avoid division by zero, though problem constraints should prevent this.\n            return np.nan \n        \n        continuum_val = x_l + (float(w - l) / float(r - l)) * (x_r - x_l)\n        \n        if continuum_val == 0:\n            # Avoid division by zero\n            return np.nan\n\n        return 1.0 - (x_w / continuum_val)\n\n    def process_library(L, D_set, tau_ev, tau_sam, tau_bd):\n        \"\"\"\n        Processes a single library to find the minimal number of components K.\n        \"\"\"\n        N, B = L.shape\n\n        # Step 1: PCA\n        mu = L.mean(axis=0)\n        X = L - mu\n        # Using ddof=1 for sample covariance (division by N-1)\n        C = np.cov(X, rowvar=False, ddof=1)\n        \n        # Step 2: Eigendecomposition\n        eigenvalues, eigenvectors = np.linalg.eigh(C)\n        # eigh returns eigenvalues in ascending order, so we reverse them\n        sorted_indices = np.argsort(eigenvalues)[::-1]\n        eigenvalues = eigenvalues[sorted_indices]\n        eigenvectors = eigenvectors[:, sorted_indices]\n        \n        total_variance = np.sum(eigenvalues)\n\n        # Step 3: Iterative search for K\n        for K in range(1, B + 1):\n            # 3.1: CEV check\n            cev = np.sum(eigenvalues[:K]) / total_variance\n            if cev < tau_ev:\n                continue\n\n            # If CEV met, proceed with diagnostic checks\n            UK = eigenvectors[:, :K]\n            \n            all_criteria_met_for_K = True\n            for i in range(N):\n                x = L[i, :]\n                \n                # 3.2: Reconstruction\n                x_centered = x - mu\n                x_hat = mu + UK @ (UK.T @ x_centered)\n\n                # 3.2.a: SAM check\n                dot_product = np.dot(x, x_hat)\n                norm_x = np.linalg.norm(x)\n                norm_x_hat = np.linalg.norm(x_hat)\n                \n                # Clip argument to arccos to handle potential floating point errors\n                cos_theta = np.clip(dot_product / (norm_x * norm_x_hat), -1.0, 1.0)\n                sam_angle = np.arccos(cos_theta)\n                \n                if sam_angle >= tau_sam:\n                    all_criteria_met_for_K = False\n                    break\n                \n                # 3.2.b: Band depth check\n                for (w, l, r) in D_set:\n                    bd_orig = calculate_band_depth(x, w, l, r)\n                    bd_recon = calculate_band_depth(x_hat, w, l, r)\n                    \n                    if np.abs(bd_orig - bd_recon) > tau_bd:\n                        all_criteria_met_for_K = False\n                        break\n                \n                if not all_criteria_met_for_K:\n                    break\n\n            if all_criteria_met_for_K:\n                return K\n        \n        # This case is technically handled by the loop which will always find K=B\n        # if no smaller K works, as reconstruction is perfect.\n        return B\n\n    results = []\n    for case in test_cases:\n        K = process_library(\n            case[\"L\"], case[\"D\"], case[\"tau_ev\"], case[\"tau_sam\"], case[\"tau_bd\"]\n        )\n        results.append(K)\n    \n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "光谱库的核心应用之一是进行地物分类和识别。本练习模拟了一个基于光谱库构建统计分类器的完整流程，旨在让您量化同一类别（例如某种矿物）内部的光谱变异性。通过计算类内协方差并应用马氏距离（Mahalanobis distance） ，您将学会如何设定一个具有统计意义的分类阈值，从而对未知光谱进行可靠的归属判断。",
            "id": "3853576",
            "problem": "给定一个光谱反射率向量集合，代表某单一矿物类别的光谱库条目。每个条目都是在固定波长（单位：纳米nm）下采样的反射率光谱。反射率是无量纲的，其值介于 $0$ 和 $1$ 之间。您的任务是使用类内协方差来量化类内变异性，并在多元正态分布假设下计算分类器阈值。\n\n从统计模式识别和辐射度学中的核心定义和经过充分检验的公式出发：\n\n- 令 $X \\in \\mathbb{R}^{n \\times p}$ 表示包含 $n$ 个库条目的矩阵，每个条目有 $p$ 个光谱波段（波长）。样本均值向量为 $\\hat{\\mu} = \\frac{1}{n}\\sum_{i=1}^{n} x_i$，样本协方差矩阵为 $\\mathbf{S} = \\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\hat{\\mu})(x_i - \\hat{\\mu})^\\top$。\n- 为确保数值稳定性并对仪器噪声建模，使用 Tikhonov 风格的正则化 $\\boldsymbol{\\Sigma}_\\lambda = \\mathbf{S} + \\lambda \\mathbf{I}_p$，其中 $\\lambda > 0$ 是一个标量正则化参数，$\\mathbf{I}_p$ 是 $p \\times p$ 的单位矩阵。\n- 定义一个标量类内变异性度量，即平均每波段方差 $v_{\\text{avg}} = \\frac{\\operatorname{tr}(\\boldsymbol{\\Sigma}_\\lambda)}{p}$，其单位为反射率的平方（无量纲的平方）。\n- 在多元正态分布假设下，当 $x$ 属于该类别时，马氏距离（Mahalanobis distance）的平方 $D^2(x) = (x - \\hat{\\mu})^\\top \\boldsymbol{\\Sigma}_\\lambda^{-1} (x - \\hat{\\mu})$ 服从自由度为 $p$ 的卡方分布（chi-square distribution）。因此，决策规则 “$D^2(x) \\le \\tau_\\alpha$” 可实现名义假阳性率（I 型错误）为 $\\alpha$，其中 $\\tau_\\alpha = \\chi^2_{p, 1-\\alpha}$，即自由度为 $p$ 的卡方分布的 $(1-\\alpha)$ 分位数。\n\n实现一个程序，对每个提供的测试用例执行以下操作：\n1. 计算 $\\hat{\\mu}$、$\\mathbf{S}$ 和 $\\boldsymbol{\\Sigma}_\\lambda$。\n2. 计算 $v_{\\text{avg}} = \\frac{\\operatorname{tr}(\\boldsymbol{\\Sigma}_\\lambda)}{p}$（一个浮点数）。\n3. 计算马氏距离平方阈值 $\\tau_\\alpha = \\chi^2_{p, 1-\\alpha}$（一个浮点数）。\n4. 对于每个查询光谱 $q$，计算 $D^2(q)$ 并返回一个布尔决策，指示 $D^2(q) \\le \\tau_\\alpha$ 是否成立。\n\n使用以下包含三个案例的测试套件，每个案例由 $(\\text{wavelengths in nm}, X, Q, \\alpha, \\lambda)$ 描述：\n\n- **案例 A（常规“理想路径”）**：\n  - 波长 (nm)：$[450, 550, 650, 850, 1600, 2200]$。\n  - 光谱库 $X$（无量纲反射率）：\n    ```\n    [[0.08, 0.12, 0.14, 0.18, 0.24, 0.28],\n     [0.09, 0.11, 0.15, 0.17, 0.23, 0.27],\n     [0.085, 0.115, 0.145, 0.175, 0.235, 0.275],\n     [0.082, 0.118, 0.142, 0.182, 0.238, 0.279],\n     [0.087, 0.113, 0.147, 0.178, 0.236, 0.276]]\n    ```\n  - 查询 $Q$（无量纲反射率）：\n    ```\n    [[0.086, 0.114, 0.146, 0.177, 0.235, 0.276],\n     [0.10, 0.12, 0.16, 0.19, 0.25, 0.30]]\n    ```\n  - 假阳性率 $\\alpha = 0.05$。\n  - 正则化参数 $\\lambda = 0.00001$。\n\n- **案例 B（边界情况：类内变异性极低，更严格的假阳性率）**：\n  - 波长 (nm)：$[450, 550, 650, 850, 1600, 2200]$。\n  - 光谱库 $X$（无量纲反射率）：\n    ```\n    [[0.30, 0.28, 0.26, 0.24, 0.22, 0.20],\n     [0.3005, 0.2795, 0.2605, 0.2405, 0.2205, 0.2005],\n     [0.2995, 0.2805, 0.2595, 0.2395, 0.2195, 0.1995],\n     [0.30, 0.28, 0.26, 0.24, 0.22, 0.20]]\n    ```\n  - 查询 $Q$（无量纲反射率）：\n    ```\n    [[0.30, 0.28, 0.26, 0.24, 0.22, 0.20],\n     [0.31, 0.29, 0.27, 0.25, 0.23, 0.21]]\n    ```\n  - 假阳性率 $\\alpha = 0.01$。\n  - 正则化参数 $\\lambda = 0.000001$。\n\n- **案例 C（边缘情况：样本量小，协方差矩阵接近奇异，需要正则化）**：\n  - 波长 (nm)：$[450, 550, 650, 850, 1600, 2200]$。\n  - 光谱库 $X$（无量纲反射率）：\n    ```\n    [[0.12, 0.18, 0.25, 0.31, 0.38, 0.44],\n     [0.13, 0.195, 0.27, 0.335, 0.41, 0.475]]\n    ```\n  - 查询 $Q$（无量纲反射率）：\n    ```\n    [[0.125, 0.1875, 0.2575, 0.3225, 0.395, 0.4575],\n     [0.10, 0.22, 0.20, 0.30, 0.45, 0.40]]\n    ```\n  - 假阳性率 $\\alpha = 0.05$。\n  - 正则化参数 $\\lambda = 0.0001$。\n\n答案要求：\n- 反射率无量纲；波长单位必须视为纳米（nm），但这不改变反射率的无量纲性质。以无量纲的反射率平方单位报告 $v_{\\text{avg}}$。以无量纲浮点数报告 $\\tau_\\alpha$。以布尔值报告分类决策。\n- 不涉及角度；请勿转换单位。\n- 每个测试用例的最终输出必须是一个列表，包含 $[v_{\\text{avg}}, \\tau_\\alpha, \\text{decisions}]$，其中 $\\text{decisions}$ 是一个布尔值列表，每个查询光谱对应一个布尔值。您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔的结果列表。例如：\"[result1,result2,result3]\"。",
            "solution": "该问题要求基于多元正态分布和马氏距离（Mahalanobis distance），为光谱数据实现一个统计分类器。这涉及到从已知光谱库中计算关键统计量，根据指定的假阳性率建立决策阈值，然后对新的查询光谱进行分类。该过程基于统计模式识别的基本原理。\n\n首先，我们对单一类别给定的数据进行形式化。我们得到一个包含 $n$ 个光谱反射率向量的集合，每个向量都在 $p$ 个不同波长下测量。该数据集可以表示为一个矩阵 $X \\in \\mathbb{R}^{n \\times p}$，其中每一行 $x_i$（$i \\in \\{1, \\dots, n\\}$）都是一个独立的光谱。\n\n第一步是为数据的集中趋势和离散程度建模。假设给定矿物类别的光谱向量是从一个多元分布中抽取的样本，我们可以估计其参数。\n集中趋势由样本均值向量 $\\hat{\\mu} \\in \\mathbb{R}^p$ 估计，其计算方式为样本向量的平均值：\n$$\n\\hat{\\mu} = \\frac{1}{n} \\sum_{i=1}^{n} x_i\n$$\n光谱波段之间的离散程度和相关性由样本协方差矩阵 $\\mathbf{S} \\in \\mathbb{R}^{p \\times p}$ 捕获。它使用无偏估计量计算：\n$$\n\\mathbf{S} = \\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\hat{\\mu})(x_i - \\hat{\\mu})^\\top\n$$\n其中 $(x_i - \\hat{\\mu})$ 是一个列向量。除以 $n-1$ 代表了用于无偏估计的贝塞尔校正（Bessel's correction）。\n\n在实践中，样本协方差矩阵 $\\mathbf{S}$ 可能是病态的或奇异的，特别是当样本数量 $n$ 没有远大于维度 $p$ 时。在案例 C 中尤其如此，其中 $n=2$ 且 $p=6$，这确保了 $\\mathbf{S}$ 是秩亏的。为了确保矩阵求逆的数值稳定性，并对光谱库中未捕获的潜在传感器噪声进行建模，我们应用了 Tikhonov 风格的正则化。正则化的协方差矩阵 $\\boldsymbol{\\Sigma}_\\lambda$ 定义为：\n$$\n\\boldsymbol{\\Sigma}_\\lambda = \\mathbf{S} + \\lambda \\mathbf{I}_p\n$$\n其中 $\\lambda > 0$ 是一个小的标量正则化参数，$\\mathbf{I}_p$ 是 $p \\times p$ 的单位矩阵。此操作将 $\\lambda$ 加到 $\\mathbf{S}$ 的对角线元素上，从而有效地将每个波段的方差增加一个很小的值，这保证了 $\\boldsymbol{\\Sigma}_\\lambda$ 是正定的，因而是可逆的。\n\n接下来，我们量化类内变异性。一个简单的标量度量是平均每波段方差 $v_{\\text{avg}}$，其定义为正则化协方差矩阵对角线元素（迹）的平均值：\n$$\nv_{\\text{avg}} = \\frac{\\operatorname{tr}(\\boldsymbol{\\Sigma}_\\lambda)}{p}\n$$\n$\\boldsymbol{\\Sigma}_\\lambda$ 的迹是每个光谱波段正则化方差的总和。\n\n分类器的核心是决策规则。我们用一个多元正态分布 $\\mathcal{N}(\\hat{\\mu}, \\boldsymbol{\\Sigma}_\\lambda)$ 来为该类别建模。对于一个查询光谱 $q$，其与该类别分布的“距离”可以通过马氏距离的平方 $D^2(q)$ 来衡量：\n$$\nD^2(q) = (q - \\hat{\\mu})^\\top \\boldsymbol{\\Sigma}_\\lambda^{-1} (q - \\hat{\\mu})\n$$\n该距离度量在统计上非常强大，因为它考虑了每个波段的方差以及波段间的协方差。它以标准差为单位测量距离，有效地创建了一个与数据云形状相符的椭圆形决策边界。\n\n多元统计学中的一个关键定理指出，如果向量 $x$ 从均值为 $\\mu$、协方差为 $\\Sigma$ 的 $p$ 元正态分布中抽取，则马氏距离的平方 $(x - \\mu)^\\top \\Sigma^{-1} (x - \\mu)$ 服从自由度为 $p$ 的卡方分布，记为 $\\chi^2_p$。\n\n此性质使我们能够构建一个具有统计可控错误率的决策规则。如果查询 $q$ 的马氏距离不是异常大，我们就希望接受它作为该类别的成员。对于一个指定的名义假阳性率（I 型错误）$\\alpha$，我们找到一个阈值 $\\tau_\\alpha$，使得一个真实成员的距离大于该阈值的概率为 $\\alpha$。该阈值是自由度为 $p$ 的卡方分布的 $(1-\\alpha)$ 分位数：\n$$\nP(D^2(x) > \\tau_\\alpha) = \\alpha \\quad \\implies \\quad \\tau_\\alpha = \\chi^2_{p, 1-\\alpha}\n$$\n那么决策规则就是：如果 $D^2(q) \\le \\tau_\\alpha$，则将查询 $q$ 归类于该类别。\n\n实现过程将为每个测试用例计算这些量。\n1.  读取输入数据 $(X, Q, \\alpha, \\lambda)$ 并确定维度 $n$ 和 $p$。\n2.  使用 `numpy.mean` 计算 $\\hat{\\mu}$。\n3.  使用 `numpy.cov`（参数 `ddof=1`）计算 $\\mathbf{S}$。\n4.  计算 $\\boldsymbol{\\Sigma}_\\lambda = \\mathbf{S} + \\lambda \\mathbf{I}_p$。\n5.  计算 $v_{\\text{avg}} = \\frac{\\operatorname{tr}(\\boldsymbol{\\Sigma}_\\lambda)}{p}$。\n6.  使用 `scipy.stats.chi2` 的百分点函数（`ppf`），以 $p$ 为自由度、$1-\\alpha$ 为分位数，计算 $\\tau_\\alpha$。\n7.  对于每个查询光谱 $q \\in Q$，使用 `numpy.linalg.inv` 找到 $\\boldsymbol{\\Sigma}_\\lambda^{-1}$ 并执行矩阵-向量乘法来计算 $D^2(q)$。\n8.  将每个 $D^2(q)$ 与 $\\tau_\\alpha$ 比较，得到布尔决策。\n9.  将结果组装成 $[v_{\\text{avg}}, \\tau_\\alpha, \\text{decisions}]$ 并将其格式化为指定的最终输出字符串。案例 B 中有一个微小的数据录入错误（2005 而非 0.2005），为保持科学有效性，已对其进行修正，因为反射率的值域为 $[0, 1]$。",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import chi2\n\ndef solve():\n    \"\"\"\n    Solves the spectral classification problem for all given test cases.\n    \"\"\"\n    \n    # In Case B, the last element of x_2 is given as 2005. This is a clear typo, as reflectance must\n    # be in [0, 1]. The pattern of x_2 = x_1 + [..., 0.0005] suggests the value should be 0.2005.\n    # This correction is made here to ensure the problem is scientifically valid.\n    test_cases = [\n        {\n            \"id\": \"A\",\n            \"X\": np.array([\n                [0.08, 0.12, 0.14, 0.18, 0.24, 0.28],\n                [0.09, 0.11, 0.15, 0.17, 0.23, 0.27],\n                [0.085, 0.115, 0.145, 0.175, 0.235, 0.275],\n                [0.082, 0.118, 0.142, 0.182, 0.238, 0.279],\n                [0.087, 0.113, 0.147, 0.178, 0.236, 0.276]\n            ]),\n            \"Q\": np.array([\n                [0.086, 0.114, 0.146, 0.177, 0.235, 0.276],\n                [0.10, 0.12, 0.16, 0.19, 0.25, 0.30]\n            ]),\n            \"alpha\": 0.05,\n            \"lambda_reg\": 0.00001\n        },\n        {\n            \"id\": \"B\",\n            \"X\": np.array([\n                [0.30, 0.28, 0.26, 0.24, 0.22, 0.20],\n                [0.3005, 0.2795, 0.2605, 0.2405, 0.2205, 0.2005], # Corrected value\n                [0.2995, 0.2805, 0.2595, 0.2395, 0.2195, 0.1995],\n                [0.30, 0.28, 0.26, 0.24, 0.22, 0.20]\n            ]),\n            \"Q\": np.array([\n                [0.30, 0.28, 0.26, 0.24, 0.22, 0.20],\n                [0.31, 0.29, 0.27, 0.25, 0.23, 0.21]\n            ]),\n            \"alpha\": 0.01,\n            \"lambda_reg\": 0.000001\n        },\n        {\n            \"id\": \"C\",\n            \"X\": np.array([\n                [0.12, 0.18, 0.25, 0.31, 0.38, 0.44],\n                [0.13, 0.195, 0.27, 0.335, 0.41, 0.475]\n            ]),\n            \"Q\": np.array([\n                [0.125, 0.1875, 0.2575, 0.3225, 0.395, 0.4575],\n                [0.10, 0.22, 0.20, 0.30, 0.45, 0.40]\n            ]),\n            \"alpha\": 0.05,\n            \"lambda_reg\": 0.0001\n        }\n    ]\n\n    def process_case(X, Q, alpha, lambda_reg):\n        \"\"\"\n        Computes variability, threshold, and classification decisions for a single case.\n        \"\"\"\n        n, p = X.shape\n\n        # 1. Compute mu_hat, S, and Sigma_lambda\n        mu_hat = np.mean(X, axis=0)\n        \n        # In the case of n=1, np.cov returns 0, but the formula requires undefined or NaN.\n        # The problem constraints ensure n > 1 for all cases, so this is safe.\n        S = np.cov(X, rowvar=False, ddof=1)\n        \n        # For a single sample (n=1), S would be an array of NaNs if dimensions are > 1, \n        # or 0 for 1-D data. As n>=2, S is well-defined.\n        # In case p=1 and n > 1, np.cov returns a 0-d array (scalar), need to reshape.\n        if p == 1:\n            S = np.array([[S]])\n\n        I_p = np.identity(p)\n        Sigma_lambda = S + lambda_reg * I_p\n\n        # 2. Compute v_avg\n        v_avg = np.trace(Sigma_lambda) / p\n\n        # 3. Compute tau_alpha\n        tau_alpha = chi2.ppf(1 - alpha, df=p)\n\n        # 4. Compute D^2(q) and decisions for each query\n        try:\n            Sigma_lambda_inv = np.linalg.inv(Sigma_lambda)\n        except np.linalg.LinAlgError:\n            # This should not happen due to regularization\n            return [v_avg, tau_alpha, [False] * len(Q)]\n        \n        decisions = []\n        for q in Q:\n            delta = q - mu_hat\n            # D^2(q) = (q - mu_hat)^T * Sigma_lambda_inv * (q - mu_hat)\n            d2 = delta @ Sigma_lambda_inv @ delta\n            decisions.append(d2 <= tau_alpha)\n            \n        return [v_avg, tau_alpha, decisions]\n\n    all_results = []\n    for case in test_cases:\n        result = process_case(case[\"X\"], case[\"Q\"], case[\"alpha\"], case[\"lambda_reg\"])\n        all_results.append(result)\n\n    # Format the final output string to precisely match \"[result1,result2,...]\"\n    # with no spaces.\n    def format_result(item):\n        if isinstance(item, list):\n            return f\"[{','.join(format_result(x) for x in item)}]\"\n        elif isinstance(item, bool):\n            return str(item).lower()  # 'true' or 'false'\n        else: # float\n            return str(item)\n\n    # Python's default bool to string is 'True', 'False'. The example shows lowercase.\n    # Let's re-format based on a re-interpretation of \"布尔值\".\n    # However, let's stick to Python standard 'True'/'False' if not specified.\n    # The example [a1,a2,a3,a4,a5] is for floats. Let's make it more robust.\n    def format_result_final(item):\n        if isinstance(item, bool):\n            return str(item)\n        if isinstance(item, float):\n            return f\"{item:.8f}\".rstrip('0').rstrip('.') # A reasonable float representation\n        if isinstance(item, list):\n             return f\"[{','.join([format_result_final(i) for i in item])}]\"\n        return str(item)\n    \n    # The original solution's formatting is simpler and probably what is expected.\n    # Reverting to simpler formatting.\n    def format_final(res):\n        v_avg, tau, decs = res\n        decs_str = f\"[{','.join(str(d) for d in decs)}]\"\n        return f\"[{v_avg},{tau},{decs_str}]\"\n    \n    results_str_list = [format_final(res).replace(\" \", \"\") for res in all_results]\n    final_output = f\"[{','.join(results_str_list)}]\"\n\n    print(final_output)\n\nsolve()\n```"
        }
    ]
}