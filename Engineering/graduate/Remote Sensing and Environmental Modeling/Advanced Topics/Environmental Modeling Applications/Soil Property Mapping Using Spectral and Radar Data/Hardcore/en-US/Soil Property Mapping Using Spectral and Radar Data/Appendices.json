{
    "hands_on_practices": [
        {
            "introduction": "Soil reflectance spectra contain absorption features that are fingerprints of specific materials, but their depth can be masked by overall variations in soil brightness or albedo. To isolate these features for quantitative analysis, we use continuum removal, a technique that normalizes the spectrum to a common baseline. This practice provides hands-on experience in implementing this crucial preprocessing step by calculating the band depth of an absorption feature from hypothetical spectral data .",
            "id": "3848675",
            "problem": "You are given discrete soil reflectance spectra sampled at wavelength positions and a set of upper convex hull vertices for each spectrum. The objective is to derive the convex hull continuum at a target wavelength and compute the absorption band depth at that wavelength for each case. Your program must implement the following tasks for each test case.\n\nFoundational base and definitions:\n- Let $R(\\lambda)$ denote the bidirectional reflectance factor (dimensionless) of a soil surface measured at wavelength $\\lambda$ in micrometers ($\\mu m$). Absorption features manifest as local depressions in $R(\\lambda)$ relative to a smooth, slowly varying baseline called the continuum.\n- The continuum in spectral analysis is defined as the upper boundary of the convex hull of the set of points $\\{(\\lambda_i, R(\\lambda_i))\\}$ in the $\\lambda$-$R$ plane. The convex hull is the smallest convex set that contains all points; its upper boundary is a piecewise-linear concave envelope formed by connecting selected hull vertices $\\{(\\lambda_{h,j}, R_{h,j})\\}$ with straight line segments.\n- For a target wavelength $\\lambda^\\star$, the continuum $C(\\lambda^\\star)$ is obtained by linear interpolation along the hull segment that brackets $\\lambda^\\star$. If $\\lambda^\\star$ coincides with a hull vertex, then $C(\\lambda^\\star) = R_{h,j}$ at that vertex.\n- Band depth at $\\lambda^\\star$ is defined as the continuum-removed absorption measure\n$$\nBD(\\lambda^\\star) = 1 - \\frac{R(\\lambda^\\star)}{C(\\lambda^\\star)} ,\n$$\nexpressed as a unitless decimal fraction.\n\nAlgorithmic requirements for each test case:\n1. Inputs:\n   - A strictly increasing list of wavelengths $\\Lambda = [\\lambda_1, \\lambda_2, \\dots, \\lambda_n]$ in micrometers ($\\mu m$).\n   - A list of reflectances $\\mathcal{R} = [R(\\lambda_1), R(\\lambda_2), \\dots, R(\\lambda_n)]$, each a dimensionless number in $[0,1]$.\n   - A list of hull vertices $\\mathcal{H} = [(\\lambda_{h,1}, R_{h,1}), (\\lambda_{h,2}, R_{h,2}), \\dots, (\\lambda_{h,m}, R_{h,m})]$ sorted by increasing $\\lambda_{h,j}$, which define the upper convex hull continuum by straight line segments between consecutive hull vertices.\n   - A target wavelength $\\lambda^\\star = 2.20\\,\\mu m$.\n2. Compute $R(\\lambda^\\star)$:\n   - If $\\lambda^\\star$ is exactly one of the $\\lambda_i$, set $R(\\lambda^\\star)$ to the corresponding reflectance value.\n   - Otherwise, find indices $i$ and $i+1$ such that $\\lambda_i \\le \\lambda^\\star \\le \\lambda_{i+1}$ and linearly interpolate\n   $$\n   R(\\lambda^\\star) = R(\\lambda_i) + \\left( \\frac{\\lambda^\\star - \\lambda_i}{\\lambda_{i+1} - \\lambda_i} \\right) \\left( R(\\lambda_{i+1}) - R(\\lambda_i) \\right) .\n   $$\n3. Compute $C(\\lambda^\\star)$:\n   - If $\\lambda^\\star$ is exactly one of the $\\lambda_{h,j}$, set $C(\\lambda^\\star) = R_{h,j}$.\n   - Otherwise, find consecutive hull vertices $(\\lambda_{h,j}, R_{h,j})$ and $(\\lambda_{h,j+1}, R_{h,j+1})$ such that $\\lambda_{h,j} \\le \\lambda^\\star \\le \\lambda_{h,j+1}$ and linearly interpolate\n   $$\n   C(\\lambda^\\star) = R_{h,j} + \\left( \\frac{\\lambda^\\star - \\lambda_{h,j}}{\\lambda_{h,j+1} - \\lambda_{h,j}} \\right) \\left( R_{h,j+1} - R_{h,j} \\right) .\n   $$\n4. Compute $BD(\\lambda^\\star)$ using the formula above. The output must be expressed as a decimal fraction (no percentage sign), dimensionless.\n\nScientific realism and applicability: The band depth at $\\lambda^\\star = 2.20\\,\\mu m$ is commonly used to quantify the $Al$-$OH$ absorption feature that relates to clay content in soils. The convex hull continuum stabilizes this measurement against broad-scale albedo variations. The procedure is purely geometric and numerical, based on interpolation and convexity.\n\nTest suite:\n- First case (general absorption feature with explicit $\\lambda^\\star$ sample):\n  - $\\Lambda = [2.00, 2.10, 2.15, 2.20, 2.25, 2.30, 2.40]$ in $\\mu m$.\n  - $\\mathcal{R} = [0.52, 0.50, 0.48, 0.35, 0.47, 0.52, 0.55]$.\n  - $\\mathcal{H} = [(2.00, 0.52), (2.40, 0.55)]$.\n- Second case (boundary where $\\lambda^\\star$ coincides with a hull vertex, producing zero band depth):\n  - $\\Lambda = [2.18, 2.20, 2.22, 2.24]$ in $\\mu m$.\n  - $\\mathcal{R} = [0.49, 0.52, 0.51, 0.50]$.\n  - $\\mathcal{H} = [(2.18, 0.49), (2.20, 0.52), (2.24, 0.50)]$.\n- Third case (minimal spectrum with three samples and endpoints as hull vertices):\n  - $\\Lambda = [2.10, 2.20, 2.30]$ in $\\mu m$.\n  - $\\mathcal{R} = [0.60, 0.40, 0.62]$.\n  - $\\mathcal{H} = [(2.10, 0.60), (2.30, 0.62)]$.\n- Fourth case (target not sampled; $R(\\lambda^\\star)$ must be interpolated from neighbors):\n  - $\\Lambda = [2.17, 2.19, 2.21, 2.23, 2.26, 2.30]$ in $\\mu m$.\n  - $\\mathcal{R} = [0.53, 0.51, 0.46, 0.49, 0.52, 0.54]$.\n  - $\\mathcal{H} = [(2.17, 0.53), (2.30, 0.54)]$.\n\nFinal output format:\n- Your program should produce a single line of output containing the band depths for the four test cases as a comma-separated list enclosed in square brackets, for example $[b_1,b_2,b_3,b_4]$, where each $b_k$ is a float computed as specified above. Wavelengths must be treated in $\\mu m$, and the band depths must be returned as unitless decimal fractions.",
            "solution": "The problem as stated is valid. It is scientifically grounded in the principles of quantitative spectroscopy, well-posed with a clear and complete set of definitions and data, and objective in its formulation. No contradictions, ambiguities, or factual errors were found. The problem can be solved by direct application of the provided algorithms.\n\nThe central task is to compute the absorption band depth, denoted as $BD(\\lambda^\\star)$, at a specified target wavelength, $\\lambda^\\star$, for several soil reflectance spectra. The band depth is a standard measure in spectroscopy for quantifying the strength of an absorption feature. It is defined as a continuum-removed quantity, which normalizes the reflectance spectrum with respect to a continuum baseline, $C(\\lambda)$.\n\nThe formula for band depth is given as:\n$$\nBD(\\lambda^\\star) = 1 - \\frac{R(\\lambda^\\star)}{C(\\lambda^\\star)}\n$$\nwhere $R(\\lambda^\\star)$ is the reflectance value at the target wavelength $\\lambda^\\star$, and $C(\\lambda^\\star)$ is the value of the spectral continuum at the same wavelength. The computation thus involves determining these two values for each test case.\n\n**1. Determination of Reflectance $R(\\lambda^\\star)$**\n\nThe reflectance spectrum is provided as a discrete set of measurements, $(\\lambda_i, R(\\lambda_i))$, where $\\Lambda = [\\lambda_1, \\lambda_2, \\dots, \\lambda_n]$ is a strictly increasing list of wavelengths. The target wavelength is specified as $\\lambda^\\star = 2.20 \\, \\mu m$.\n\nThe value of $R(\\lambda^\\star)$ is obtained as follows:\n- **Case A: Exact Match:** If the target wavelength $\\lambda^\\star$ coincides exactly with one of the sampled wavelengths $\\lambda_i$ in the list $\\Lambda$, then $R(\\lambda^\\star)$ is simply the corresponding measured reflectance value, $R(\\lambda_i)$.\n- **Case B: Interpolation:** If $\\lambda^\\star$ falls between two sampled wavelengths, $\\lambda_i < \\lambda^\\star < \\lambda_{i+1}$, its reflectance $R(\\lambda^\\star)$ is estimated via linear interpolation between the two adjacent points $(\\lambda_i, R(\\lambda_i))$ and $(\\lambda_{i+1}, R(\\lambda_{i+1}))$. The formula for linear interpolation is:\n$$\nR(\\lambda^\\star) = R(\\lambda_i) + \\left( \\frac{\\lambda^\\star - \\lambda_i}{\\lambda_{i+1} - \\lambda_i} \\right) \\left( R(\\lambda_{i+1}) - R(\\lambda_i) \\right)\n$$\nThis assumes a locally linear behavior of the spectrum between sample points, a standard practice for high-resolution spectral data.\n\n**2. Determination of Continuum $C(\\lambda^\\star)$**\n\nThe continuum, $C(\\lambda)$, is defined as the upper envelope of the spectrum, constructed from a given set of upper convex hull vertices $\\mathcal{H} = [(\\lambda_{h,1}, R_{h,1}), \\dots, (\\lambda_{h,m}, R_{h,m})]$. This envelope is a piecewise linear function connecting consecutive vertices.\n\nThe value of $C(\\lambda^\\star)$ is determined similarly to $R(\\lambda^\\star)$, but using the hull vertices instead of the full spectral data:\n- **Case A: Exact Match:** If the target wavelength $\\lambda^\\star$ coincides with the wavelength of a hull vertex, $\\lambda^\\star = \\lambda_{h,j}$, then the continuum value is the reflectance of that vertex, $C(\\lambda^\\star) = R_{h,j}$.\n- **Case B: Interpolation:** If $\\lambda^\\star$ falls between two consecutive hull vertices, $\\lambda_{h,j} < \\lambda^\\star < \\lambda_{h,j+1}$, the continuum value $C(\\lambda^\\star)$ is found by linear interpolation along the line segment connecting these two vertices, $(\\lambda_{h,j}, R_{h,j})$ and $(\\lambda_{h,j+1}, R_{h,j+1})$:\n$$\nC(\\lambda^\\star) = R_{h,j} + \\left( \\frac{\\lambda^\\star - \\lambda_{h,j}}{\\lambda_{h,j+1} - \\lambda_{h,j}} \\right) \\left( R_{h,j+1} - R_{h,j} \\right)\n$$\nThis procedure provides the value of the smooth upper-boundary reference against which the absorption is measured.\n\n**3. Algorithmic Implementation**\n\nFor each test case, the overall algorithm proceeds as follows:\n1.  Define the input data: the wavelength list $\\Lambda$, the reflectance list $\\mathcal{R}$, and the hull vertices list $\\mathcal{H}$. The target wavelength is constant at $\\lambda^\\star = 2.20 \\, \\mu m$.\n2.  Implement a generic linear interpolation function that, given a target $x$-coordinate and a set of sorted $(x,y)$ points, returns the corresponding $y$-coordinate. This function must handle both exact matches and interpolation cases.\n3.  Call this interpolation function with $\\lambda^\\star$ and the spectrum points $\\{(\\lambda_i, R(\\lambda_i))\\}$ to compute $R(\\lambda^\\star)$.\n4.  Call the same interpolation function with $\\lambda^\\star$ and the hull vertices $\\{(\\lambda_{h,j}, R_{h,j})\\}$ to compute $C(\\lambda^\\star)$.\n5.  Calculate the band depth $BD(\\lambda^\\star)$ using the primary formula. Since all reflectances $R(\\lambda_i)$ are in the range $[0, 1]$ and the convex hull sits above the spectrum, $C(\\lambda^\\star)$ will be positive for non-zero spectra, preventing division by zero.\n6.  The final result for each case is the computed dimensionless decimal fraction for $BD(\\lambda^\\star)$.\n\nThis procedure is deterministic and directly follows the provided definitions, ensuring a correct and verifiable solution based on fundamental geometric and numerical principles.",
            "answer": "```python\nimport numpy as np\n\ndef _linear_interpolate(x_target, points):\n    \"\"\"\n    Performs linear interpolation on a set of 2D points.\n\n    Args:\n        x_target (float): The x-coordinate at which to interpolate.\n        points (list of tuple/list): A list of (x, y) points, sorted by x.\n\n    Returns:\n        float: The interpolated y-value.\n    \"\"\"\n    points_arr = np.array(points)\n    x_coords = points_arr[:, 0]\n    y_coords = points_arr[:, 1]\n    \n    # Case 1: x_target is one of the existing x-coordinates\n    match_indices = np.where(x_coords == x_target)[0]\n    if len(match_indices) > 0:\n        return y_coords[match_indices[0]]\n\n    # Case 2: x_target needs interpolation.\n    # np.searchsorted finds the index where x_target would be inserted to maintain order.\n    idx = np.searchsorted(x_coords, x_target)\n\n    # The problem setup ensures x_target is always bracketed by the given points.\n    # Therefore, we do not need to handle extrapolation cases.\n    x1, y1 = points_arr[idx - 1]\n    x2, y2 = points_arr[idx]\n    \n    # Standard linear interpolation formula\n    y_target = y1 + ((x_target - x1) * (y2 - y1) / (x2 - x1))\n    \n    return y_target\n\ndef calculate_band_depth(lmbda, R, H, lambda_star):\n    \"\"\"\n    Calculates the band depth at a target wavelength.\n\n    Args:\n        lmbda (list): List of wavelengths.\n        R (list): List of reflectances.\n        H (list of tuple): List of convex hull vertices.\n        lambda_star (float): The target wavelength.\n\n    Returns:\n        float: The calculated band depth.\n    \"\"\"\n    # 1. Compute R(lambda_star) by interpolating the spectrum\n    spectrum_points = list(zip(lmbda, R))\n    R_star = _linear_interpolate(lambda_star, spectrum_points)\n\n    # 2. Compute C(lambda_star) by interpolating the hull vertices\n    C_star = _linear_interpolate(lambda_star, H)\n\n    # 3. Compute BD(lambda_star)\n    # The problem context implies C_star will not be zero.\n    if C_star == 0:\n        return np.nan # Should not be reached with the given test cases\n\n    band_depth = 1.0 - (R_star / C_star)\n    return band_depth\n\ndef solve():\n    \"\"\"\n    Solves the band depth calculation problem for the given test cases.\n    \"\"\"\n    lambda_star = 2.20\n\n    test_cases = [\n        # Case 1: General absorption feature\n        (\n            [2.00, 2.10, 2.15, 2.20, 2.25, 2.30, 2.40],\n            [0.52, 0.50, 0.48, 0.35, 0.47, 0.52, 0.55],\n            [(2.00, 0.52), (2.40, 0.55)]\n        ),\n        # Case 2: Target wavelength is a hull vertex\n        (\n            [2.18, 2.20, 2.22, 2.24],\n            [0.49, 0.52, 0.51, 0.50],\n            [(2.18, 0.49), (2.20, 0.52), (2.24, 0.50)]\n        ),\n        # Case 3: Minimal spectrum\n        (\n            [2.10, 2.20, 2.30],\n            [0.60, 0.40, 0.62],\n            [(2.10, 0.60), (2.30, 0.62)]\n        ),\n        # Case 4: Target wavelength is not sampled\n        (\n            [2.17, 2.19, 2.21, 2.23, 2.26, 2.30],\n            [0.53, 0.51, 0.46, 0.49, 0.52, 0.54],\n            [(2.17, 0.53), (2.30, 0.54)]\n        )\n    ]\n\n    results = []\n    for lmbda, R, H in test_cases:\n        bd = calculate_band_depth(lmbda, R, H, lambda_star)\n        results.append(bd)\n\n    # Format the output as a comma-separated list of floats in brackets\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Once features are extracted from sensor data, the next step is to build a model that links them to a soil property of interest, such as clay content. This exercise simulates this process, using both spectral and radar-derived indices in a linear regression framework . Critically, it emphasizes that a model's true predictive power can only be assessed on data it has not seen, introducing k-fold cross-validation as a standard and robust technique for estimating out-of-sample performance.",
            "id": "3848679",
            "problem": "You are tasked with validating a linear index-to-property regression for estimating soil clay content using spectral reflectance and radar backscatter data. The context is the joint use of multispectral optical reflectance and Synthetic Aperture Radar (SAR) backscatter to infer soil clay content at the surface. The two indices to be used are a spectral index constructed from Near-Infrared (NIR) and Short-Wave Infrared (SWIR) reflectances, and a polarimetric radar ratio constructed from co-polarized and cross-polarized backscatter.\n\nUse the following foundational definitions and facts as the starting point:\n- A Normalized Difference Index (NDI) constructed from two positive reflectances $r_1$ and $r_2$ is defined as the ratio of their difference to their sum. This ratio is dimensionless, bounded in $(-1,1)$, and reduces sensitivity to multiplicative illumination factors.\n- Linear regression with a squared-error loss is solved by minimizing the empirical risk over an affine hypothesis class, and its unique minimizer (when the design matrix has full column rank) satisfies the normal equations.\n- The Root Mean Squared Error (RMSE), the mean error (bias), and the coefficient of determination $R^2$ are standard, unit-consistent measures for evaluating prediction performance on held-out data.\n\nDataset. You are given $n = 12$ independent samples. Each sample $i$ consists of:\n- NIR reflectance $R_{\\mathrm{NIR},i}$ (unitless fraction),\n- SWIR reflectance $R_{\\mathrm{SWIR},i}$ (unitless fraction),\n- radar backscatter in linear power for vertical-vertical polarization $\\sigma^{0}_{\\mathrm{VV},i}$ (unitless, linear scale),\n- radar backscatter in linear power for vertical-horizontal polarization $\\sigma^{0}_{\\mathrm{VH},i}$ (unitless, linear scale),\n- clay content mass fraction $y_i$ (unitless fraction in $[0,1]$).\n\nUse the following values for $i = 1,\\dots,12$:\n- $R_{\\mathrm{NIR}} = [\\,0.30,\\,0.35,\\,0.40,\\,0.25,\\,0.32,\\,0.28,\\,0.18,\\,0.45,\\,0.22,\\,0.50,\\,0.27,\\,0.33\\,]$.\n- $R_{\\mathrm{SWIR}} = [\\,0.20,\\,0.25,\\,0.20,\\,0.10,\\,0.12,\\,0.22,\\,0.12,\\,0.30,\\,0.18,\\,0.25,\\,0.09,\\,0.28\\,]$.\n- $\\sigma^{0}_{\\mathrm{VV}} = [\\,0.20,\\,0.18,\\,0.16,\\,0.22,\\,0.14,\\,0.24,\\,0.12,\\,0.10,\\,0.26,\\,0.15,\\,0.13,\\,0.21\\,]$.\n- $\\sigma^{0}_{\\mathrm{VH}} = [\\,0.050,\\,0.045,\\,0.032,\\,0.044,\\,0.021,\\,0.060,\\,0.036,\\,0.020,\\,0.052,\\,0.045,\\,0.026,\\,0.042\\,]$.\n- $y = [\\,0.515,\\,0.523,\\,0.415,\\,0.360,\\,0.330,\\,0.550,\\,0.515,\\,0.490,\\,0.555,\\,0.445,\\,0.315,\\,0.560\\,]$.\n\nDefine two indices per sample $i$:\n- Spectral Clay Index (SCI): $s_i = \\dfrac{R_{\\mathrm{NIR},i} - R_{\\mathrm{SWIR},i}}{R_{\\mathrm{NIR},i} + R_{\\mathrm{SWIR},i}}$.\n- Polarization Ratio (PR): $p_i = \\dfrac{\\sigma^{0}_{\\mathrm{VH},i}}{\\sigma^{0}_{\\mathrm{VV},i}}$.\n\nModel class. Consider affine linear regressions of the form $\\hat{y} = \\beta_0 + \\beta^\\top x$, where $x$ is either the scalar $s$, the scalar $p$, or the two-dimensional vector $[\\,s,\\,p\\,]^\\top$, depending on the feature selection mode.\n\nValidation design. Implement $k$-fold cross-validation as follows:\n- Construct a random permutation $\\pi$ of $\\{0,1,\\dots,n-1\\}$ using a fixed pseudorandom number generator seed equal to $137$. Partition $\\pi$ into $k$ contiguous folds using equal-size splitting up to one index (that is, folds differ in size by at most one), preserving the order within $\\pi$.\n- For each fold $f$, fit the linear regression on the union of the other $k-1$ folds by minimizing the empirical mean squared error over the training subset, and then predict on the held-out fold $f$.\n- Aggregate all held-out predictions to form pooled vectors $\\hat{y}_{\\mathrm{cv}}$ and $y$ of length $n$.\n\nMetrics. From first principles, compute the following quantities on the pooled held-out predictions:\n- The RMSE,\n\n$$\n\\mathrm{RMSE} \\;=\\; \\sqrt{\\dfrac{1}{n}\\sum_{i=1}^{n}\\left(\\hat{y}_{\\mathrm{cv},i} - y_i\\right)^2}.\n$$\n\n- The bias (mean error),\n\n$$\n\\mathrm{bias} \\;=\\; \\dfrac{1}{n}\\sum_{i=1}^{n}\\left(\\hat{y}_{\\mathrm{cv},i} - y_i\\right).\n$$\n\n- The coefficient of determination,\n\n$$\nR^2 \\;=\\; 1 \\;-\\; \\dfrac{\\sum_{i=1}^{n}\\left(y_i - \\hat{y}_{\\mathrm{cv},i}\\right)^2}{\\sum_{i=1}^{n}\\left(y_i - \\bar{y}\\right)^2}, \\quad \\bar{y} \\;=\\; \\dfrac{1}{n}\\sum_{i=1}^{n} y_i.\n$$\n\n\nAll clay contents and errors must be treated as unitless fractions. Report all final metric values as decimals (not percentages).\n\nTest suite. Your program must execute the above for the following parameter cases:\n- Case A: $k = 3$, features $= $ SCI only.\n- Case B: $k = 6$, features $= $ PR only.\n- Case C: $k = 12$, features $= $ SCI and PR jointly.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each case should contribute a list $[\\,\\mathrm{RMSE},\\,\\mathrm{bias},\\,R^2\\,]$ with each number rounded to exactly six decimal places. The overall output must thus be a list of three lists in the order Case A, Case B, Case C. For example, the format must be\n\n$$\n[\\,[r_1,b_1,R_1],\\,[r_2,b_2,R_2],\\,[r_3,b_3,R_3]\\,],\n$$\n\nwith each $r_j,b_j,R_j$ shown as decimals rounded to six places.",
            "solution": "The problem statement is deemed valid. It is scientifically grounded in the domain of remote sensing, mathematically well-posed, objective, and provides a complete and consistent set of data and procedures for a reproducible computational task.\n\nThe objective is to evaluate three affine linear regression models for estimating soil clay content ($y$) from derived spectral and radar indices. The evaluation will be performed using $k$-fold cross-validation on a dataset of $n=12$ samples. The process involves feature derivation, model training via ordinary least squares (OLS), cross-validated prediction, and the computation of three performance metrics: Root Mean Squared Error (RMSE), bias, and the coefficient of determination ($R^2$).\n\nFirst, we define and compute the input features from the provided raw data. The raw data for each sample $i=1, \\dots, 12$ are the NIR reflectance $R_{\\mathrm{NIR},i}$, SWIR reflectance $R_{\\mathrm{SWIR},i}$, co-polarized radar backscatter $\\sigma^{0}_{\\mathrm{VV},i}$, and cross-polarized radar backscatter $\\sigma^{0}_{\\mathrm{VH},i}$.\n\nThe two features are the Spectral Clay Index (SCI), $s_i$, and the Polarization Ratio (PR), $p_i$. Their definitions are:\n$$\ns_i = \\dfrac{R_{\\mathrm{NIR},i} - R_{\\mathrm{SWIR},i}}{R_{\\mathrm{NIR},i} + R_{\\mathrm{SWIR},i}}\n$$\n$$\np_i = \\dfrac{\\sigma^{0}_{\\mathrm{VH},i}}{\\sigma^{0}_{\\mathrm{VV},i}}\n$$\nThese indices are calculated for all $n=12$ samples, resulting in two vectors $\\mathbf{s}$ and $\\mathbf{p}$ of length $12$.\n\nThe core of the analysis is a $k$-fold cross-validation procedure. For a given number of folds $k$ and a specified set of features, the following steps are executed:\n1.  A pseudorandom permutation $\\pi$ of the sample indices $\\{0, 1, \\dots, 11\\}$ is generated using a fixed seed of $137$ to ensure reproducibility.\n2.  The permuted index set $\\pi$ is partitioned into $k$ contiguous, non-overlapping subsets (folds) of near-equal size. Since $n=12$, for $k=3, 6, 12$, the folds will have equal sizes of $12/3=4$, $12/6=2$, and $12/12=1$, respectively.\n3.  For each fold $f \\in \\{1, \\dots, k\\}$, the samples corresponding to the indices in that fold are designated as the held-out test set. The remaining $k-1$ folds constitute the training set.\n\nFor each of the $k$ iterations of the cross-validation, an affine linear model of the form $\\hat{y} = \\beta_0 + \\boldsymbol{\\beta}^\\top \\mathbf{x}$ is fitted to the training data. Here, $\\mathbf{x}$ represents the feature vector, which can be a scalar ($s_i$ or $p_i$) or a $2$-dimensional vector $([s_i, p_i]^\\top)$. The model can be expressed in matrix form for the entire training set as $\\mathbf{y}_{\\text{train}} \\approx \\mathbf{X}_{\\text{train}}\\boldsymbol{\\beta}_{\\text{model}}$, where $\\mathbf{X}_{\\text{train}}$ is the design matrix (the feature matrix prepended with a column of ones for the intercept $\\beta_0$) and $\\boldsymbol{\\beta}_{\\text{model}}$ is the vector of model coefficients.\n\nThe coefficients $\\boldsymbol{\\beta}_{\\text{model}}$ are determined by minimizing the sum of squared errors on the training data, which is the principle of Ordinary Least Squares (OLS). The unique solution is given by the normal equations:\n$$\n\\boldsymbol{\\beta}_{\\text{model}} = (\\mathbf{X}_{\\text{train}}^\\top \\mathbf{X}_{\\text{train}})^{-1} \\mathbf{X}_{\\text{train}}^\\top \\mathbf{y}_{\\text{train}}\n$$\nThis linear system is best solved numerically without explicitly computing the matrix inverse, for instance, by using `np.linalg.solve`.\n\nOnce the coefficients $\\boldsymbol{\\beta}_{\\text{model}}$ are found, they are used to predict the clay content for the held-out test set: $\\hat{\\mathbf{y}}_{\\text{test}} = \\mathbf{X}_{\\text{test}}\\boldsymbol{\\beta}_{\\text{model}}$. These predictions are collected over all $k$ folds. After the cross-validation loop completes, we have a complete vector of pooled predictions, $\\hat{\\mathbf{y}}_{\\mathrm{cv}}$, of length $n=12$, where each sample's prediction was generated by a model that was not trained on it.\n\nFinally, the performance metrics are computed by comparing the pooled predictions $\\hat{\\mathbf{y}}_{\\mathrm{cv}}$ against the true clay content values $\\mathbf{y}$.\nThe Root Mean Squared Error (RMSE) is:\n$$\n\\mathrm{RMSE} = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(\\hat{y}_{\\mathrm{cv},i} - y_i)^2}\n$$\nThe bias, or mean error, is:\n$$\n\\mathrm{bias} = \\frac{1}{n}\\sum_{i=1}^{n}(\\hat{y}_{\\mathrm{cv},i} - y_i)\n$$\nThe coefficient of determination, $R^2$, measures the proportion of the variance in the dependent variable that is predictable from the independent variable(s). Based on the pooled cross-validation results, it is calculated as:\n$$\nR^2 = 1 - \\frac{\\sum_{i=1}^{n}(y_i - \\hat{y}_{\\mathrm{cv},i})^2}{\\sum_{i=1}^{n}(y_i - \\bar{y})^2}, \\quad \\text{where } \\bar{y} = \\frac{1}{n}\\sum_{i=1}^{n} y_i\n$$\n\nThis entire procedure is performed for three specified cases:\n- Case A: $k = 3$, features = SCI only.\n- Case B: $k = 6$, features = PR only.\n- Case C: $k = 12$ (equivalent to Leave-One-Out Cross-Validation), features = SCI and PR jointly.\n\nThe final output will consist of the calculated $[\\mathrm{RMSE}, \\mathrm{bias}, R^2]$ for each case, formatted as required.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Performs k-fold cross-validation for linear regression models\n    to predict soil clay content from spectral and radar data.\n    \"\"\"\n    # Dataset as specified in the problem statement.\n    R_nir_data = np.array([0.30, 0.35, 0.40, 0.25, 0.32, 0.28, 0.18, 0.45, 0.22, 0.50, 0.27, 0.33])\n    R_swir_data = np.array([0.20, 0.25, 0.20, 0.10, 0.12, 0.22, 0.12, 0.30, 0.18, 0.25, 0.09, 0.28])\n    sigma_vv_data = np.array([0.20, 0.18, 0.16, 0.22, 0.14, 0.24, 0.12, 0.10, 0.26, 0.15, 0.13, 0.21])\n    sigma_vh_data = np.array([0.050, 0.045, 0.032, 0.044, 0.021, 0.060, 0.036, 0.020, 0.052, 0.045, 0.026, 0.042])\n    y_data = np.array([0.515, 0.523, 0.415, 0.360, 0.330, 0.550, 0.515, 0.490, 0.555, 0.445, 0.315, 0.560])\n    \n    n_samples = len(y_data)\n\n    # Feature Engineering: Calculate SCI and PR\n    sci = (R_nir_data - R_swir_data) / (R_nir_data + R_swir_data)\n    pr = sigma_vh_data / sigma_vv_data\n\n    # Test cases defined in the problem\n    test_cases = [\n        {'k': 3, 'features': 'sci', 'label': 'A'},\n        {'k': 6, 'features': 'pr', 'label': 'B'},\n        {'k': 12, 'features': 'sci_pr', 'label': 'C'},\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        k = case['k']\n        \n        # Select features for the current case\n        if case['features'] == 'sci':\n            X = sci.reshape(-1, 1)\n        elif case['features'] == 'pr':\n            X = pr.reshape(-1, 1)\n        elif case['features'] == 'sci_pr':\n            X = np.column_stack((sci, pr))\n        \n        # Cross-validation setup\n        indices = np.arange(n_samples)\n        rng = np.random.default_rng(seed=137)\n        permuted_indices = rng.permutation(indices)\n        \n        # np.array_split correctly partitions into contiguous folds\n        folds = np.array_split(permuted_indices, k)\n        \n        # Array to store pooled cross-validated predictions\n        y_cv_pooled = np.zeros(n_samples)\n        \n        for i in range(k):\n            test_indices = folds[i]\n            \n            # setdiff1d can be used to get train_indices\n            train_indices = np.setdiff1d(permuted_indices, test_indices)\n\n            # Prepare training and test sets\n            X_train, y_train = X[train_indices], y_data[train_indices]\n            X_test, y_test = X[test_indices], y_data[test_indices]\n            \n            # Construct design matrices with an intercept term\n            X_train_design = np.c_[np.ones(len(X_train)), X_train]\n            X_test_design = np.c_[np.ones(len(X_test)), X_test]\n            \n            # Solve the normal equations for OLS: (X'X)b = X'y\n            # np.linalg.solve is numerically more stable than using np.linalg.inv\n            try:\n                beta = np.linalg.solve(X_train_design.T @ X_train_design, X_train_design.T @ y_train)\n            except np.linalg.LinAlgError:\n                # Fallback to pseudoinverse if matrix is singular, though unlikely here\n                beta = np.linalg.pinv(X_train_design) @ y_train\n\n            # Predict on the held-out fold\n            y_pred = X_test_design @ beta\n            \n            # Store predictions in the correct positions of the pooled array\n            y_cv_pooled[test_indices] = y_pred\n\n        # After CV, compute metrics on the pooled predictions\n        # The true y_data vector is already in the correct order corresponding to y_cv_pooled\n        residuals = y_cv_pooled - y_data\n        \n        # RMSE\n        rmse = np.sqrt(np.mean(residuals**2))\n        \n        # Bias\n        bias = np.mean(residuals)\n        \n        # R^2\n        ss_res = np.sum(residuals**2)\n        ss_tot = np.sum((y_data - np.mean(y_data))**2)\n        if ss_tot == 0:\n            # Handle case where target variance is zero\n            r2 = 1.0 if ss_res == 0 else 0.0\n        else:\n            r2 = 1 - (ss_res / ss_tot)\n            \n        all_results.append([rmse, bias, r2])\n\n    # Format the final output string exactly as required.\n    # Each sublist is formatted, then joined by commas.\n    # The outer list is then enclosed in brackets.\n    output_parts = []\n    for result_set in all_results:\n        formatted_set = f\"[{result_set[0]:.6f},{result_set[1]:.6f},{result_set[2]:.6f}]\"\n        output_parts.append(formatted_set)\n    \n    final_output = f\"[{','.join(output_parts)}]\"\n\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "A prediction from a model is incomplete without an accompanying estimate of its uncertainty. Every input, from the initial sensor measurement to the calibration parameters, has an associated error that propagates through the model to the final soil property estimate. This practice will guide you through a first-order error propagation analysis, a fundamental technique for quantifying how these individual uncertainties, including their correlations, combine to determine the variance of the final result .",
            "id": "3848678",
            "problem": "A soil clay content estimator integrates optical spectral information and radar backscatter to reduce confounding effects of surface moisture and roughness. Consider a single pixel for which the corrected bidirectional surface reflectances at two shortwave infrared wavelengths, $\\lambda_{1} = 2.10\\,\\mathrm{\\mu m}$ and $\\lambda_{2} = 2.20\\,\\mathrm{\\mu m}$, are modeled as linear corrections of measured top-of-atmosphere quantities:\n$$\n\\rho_{1} = s_{1} \\, m_{1} + b_{1}, \\quad \\rho_{2} = s_{2} \\, m_{2} + b_{2},\n$$\nwhere $m_{i}$ are instrument-level measured reflectance proxies, $s_{i}$ are multiplicative calibration and atmospheric scaling factors that include sensor radiometric gain and atmospheric transmittance, and $b_{i}$ are additive path reflectance offsets that account for residual atmospheric path radiance and adjacency effects. The Vertical-Vertical (VV) polarization radar backscatter coefficient is denoted $r$ and is treated in linear units.\n\nThe clay index is defined by the ratio\n$$\nI = \\frac{\\rho_{2} - \\rho_{1}}{\\rho_{2} + \\rho_{1}},\n$$\nand the clay content estimate is given by a linear fusion model\n$$\nC = \\alpha + \\beta \\, I + \\gamma \\, r,\n$$\nwith known regression coefficients $\\alpha = 0.05$, $\\beta = 0.60$, and $\\gamma = 0.20$.\n\nFor this pixel, the mean values and one-standard-deviation uncertainties (assumed small enough for first-order linearization to be valid) are:\n- $m_{1} = 0.15$ with $\\sigma_{m_{1}} = 0.002$,\n- $m_{2} = 0.20$ with $\\sigma_{m_{2}} = 0.0025$,\n- $s_{1} = 1.02$ with $\\sigma_{s_{1}} = 0.01$,\n- $s_{2} = 0.98$ with $\\sigma_{s_{2}} = 0.01$,\n- $b_{1} = 0.005$ with $\\sigma_{b_{1}} = 0.001$,\n- $b_{2} = 0.004$ with $\\sigma_{b_{2}} = 0.001$,\n- $r = 0.12$ with $\\sigma_{r} = 0.01$.\n\nAssume the following correlations motivated by shared calibration and atmospheric modeling:\n- $\\mathrm{corr}(s_{1}, s_{2}) = 0.60$,\n- $\\mathrm{corr}(b_{1}, b_{2}) = 0.50$,\n\nand all other pairs are uncorrelated. Using first-order (linearized) error propagation about the provided mean values and the covariance induced by the given correlations, derive and compute the variance $\\mathrm{Var}(C)$ of the clay content estimate $C$.\n\nRound your final numerical answer for $\\mathrm{Var}(C)$ to four significant figures. Express your answer as a dimensionless decimal number.",
            "solution": "The problem asks for the variance of the clay content estimate, $\\mathrm{Var}(C)$, using first-order error propagation.\n\nThe clay content estimate $C$ is given by the linear fusion model:\n$$\nC = \\alpha + \\beta I + \\gamma r\n$$\nwhere $\\alpha$, $\\beta$, and $\\gamma$ are known constants. The variance of a sum of random variables is the sum of their variances plus twice the sum of all pairwise covariances. Since $\\alpha$ is a constant, its variance is zero. The variance of $C$ is therefore:\n$$\n\\mathrm{Var}(C) = \\mathrm{Var}(\\beta I + \\gamma r) = \\beta^2 \\mathrm{Var}(I) + \\gamma^2 \\mathrm{Var}(r) + 2\\beta\\gamma \\mathrm{Cov}(I, r)\n$$\nThe variables $I$ and $r$ are uncorrelated. The variable $I$ is a function of the spectral measurements ($m_1, m_2, s_1, s_2, b_1, b_2$), while $r$ is an independent radar measurement. Therefore, their covariance is zero, $\\mathrm{Cov}(I, r) = 0$. The expression for the variance of $C$ simplifies to:\n$$\n\\mathrm{Var}(C) = \\beta^2 \\mathrm{Var}(I) + \\gamma^2 \\mathrm{Var}(r)\n$$\nWe are given $\\beta = 0.60$, $\\gamma = 0.20$, and the standard deviation of $r$, $\\sigma_r = 0.01$. The variance of $r$ is:\n$$\n\\mathrm{Var}(r) = \\sigma_r^2 = (0.01)^2 = 0.0001 = 1 \\times 10^{-4}\n$$\nThe main task is to compute $\\mathrm{Var}(I)$. The clay index $I$ is defined as:\n$$\nI = \\frac{\\rho_2 - \\rho_1}{\\rho_2 + \\rho_1}\n$$\nUsing the first-order error propagation formula for a function of correlated variables, the variance of $I$ is approximated by:\n$$\n\\mathrm{Var}(I) \\approx \\left(\\frac{\\partial I}{\\partial \\rho_1}\\right)^2 \\mathrm{Var}(\\rho_1) + \\left(\\frac{\\partial I}{\\partial \\rho_2}\\right)^2 \\mathrm{Var}(\\rho_2) + 2 \\left(\\frac{\\partial I}{\\partial \\rho_1}\\right) \\left(\\frac{\\partial I}{\\partial \\rho_2}\\right) \\mathrm{Cov}(\\rho_1, \\rho_2)\n$$\nThe partial derivatives of $I$ with respect to $\\rho_1$ and $\\rho_2$ are:\n$$\n\\frac{\\partial I}{\\partial \\rho_1} = \\frac{(-1)(\\rho_2 + \\rho_1) - (\\rho_2 - \\rho_1)(1)}{(\\rho_2 + \\rho_1)^2} = \\frac{-2\\rho_2}{(\\rho_2 + \\rho_1)^2}\n$$\n$$\n\\frac{\\partial I}{\\partial \\rho_2} = \\frac{(1)(\\rho_2 + \\rho_1) - (\\rho_2 - \\rho_1)(1)}{(\\rho_2 + \\rho_1)^2} = \\frac{2\\rho_1}{(\\rho_2 + \\rho_1)^2}\n$$\nThese derivatives must be evaluated at the mean values of $\\rho_1$ and $\\rho_2$. First, we compute these means:\n$$\n\\mu_{\\rho_1} = \\mathbb{E}[\\rho_1] = s_1 m_1 + b_1 = (1.02)(0.15) + 0.005 = 0.153 + 0.005 = 0.158\n$$\n$$\n\\mu_{\\rho_2} = \\mathbb{E}[\\rho_2] = s_2 m_2 + b_2 = (0.98)(0.20) + 0.004 = 0.196 + 0.004 = 0.200\n$$\nNow, we evaluate the partial derivatives:\n$$\n\\frac{\\partial I}{\\partial \\rho_1} \\bigg|_{\\mu} = \\frac{-2(0.200)}{(0.200 + 0.158)^2} = \\frac{-0.4}{(0.358)^2} = \\frac{-0.4}{0.128164} \\approx -3.12104\n$$\n$$\n\\frac{\\partial I}{\\partial \\rho_2} \\bigg|_{\\mu} = \\frac{2(0.158)}{(0.200 + 0.158)^2} = \\frac{0.316}{0.128164} \\approx 2.46571\n$$\nNext, we determine $\\mathrm{Var}(\\rho_1)$, $\\mathrm{Var}(\\rho_2)$, and $\\mathrm{Cov}(\\rho_1, \\rho_2)$. The surface reflectances are $\\rho_1 = s_1 m_1 + b_1$ and $\\rho_2 = s_2 m_2 + b_2$. The input variables $\\{m_1, s_1, b_1\\}$ are mutually uncorrelated, as are $\\{m_2, s_2, b_2\\}$.\nThe variances are:\n$$\n\\mathrm{Var}(\\rho_1) \\approx \\left(\\frac{\\partial \\rho_1}{\\partial s_1}\\right)^2 \\mathrm{Var}(s_1) + \\left(\\frac{\\partial \\rho_1}{\\partial m_1}\\right)^2 \\mathrm{Var}(m_1) + \\left(\\frac{\\partial \\rho_1}{\\partial b_1}\\right)^2 \\mathrm{Var}(b_1)\n$$\n$$\n\\mathrm{Var}(\\rho_1) \\approx m_1^2 \\sigma_{s_1}^2 + s_1^2 \\sigma_{m_1}^2 + \\sigma_{b_1}^2\n$$\n$$\n\\mathrm{Var}(\\rho_1) \\approx (0.15)^2 (0.01)^2 + (1.02)^2 (0.002)^2 + (0.001)^2\n$$\n$$\n\\mathrm{Var}(\\rho_1) \\approx (0.0225)(10^{-4}) + (1.0404)(4 \\times 10^{-6}) + (10^{-6})\n$$\n$$\n\\mathrm{Var}(\\rho_1) \\approx 2.25 \\times 10^{-6} + 4.1616 \\times 10^{-6} + 1 \\times 10^{-6} = 7.4116 \\times 10^{-6}\n$$\nSimilarly for $\\rho_2$:\n$$\n\\mathrm{Var}(\\rho_2) \\approx m_2^2 \\sigma_{s_2}^2 + s_2^2 \\sigma_{m_2}^2 + \\sigma_{b_2}^2\n$$\n$$\n\\mathrm{Var}(\\rho_2) \\approx (0.20)^2 (0.01)^2 + (0.98)^2 (0.0025)^2 + (0.001)^2\n$$\n$$\n\\mathrm{Var}(\\rho_2) \\approx (0.04)(10^{-4}) + (0.9604)(6.25 \\times 10^{-6}) + (10^{-6})\n$$\n$$\n\\mathrm{Var}(\\rho_2) \\approx 4 \\times 10^{-6} + 6.0025 \\times 10^{-6} + 1 \\times 10^{-6} = 11.0025 \\times 10^{-6}\n$$\nThe covariance $\\mathrm{Cov}(\\rho_1, \\rho_2)$ depends on the correlations between the input variables. We use the first-order approximation: $\\Delta \\rho_1 \\approx m_1 \\Delta s_1 + s_1 \\Delta m_1 + \\Delta b_1$ and $\\Delta \\rho_2 \\approx m_2 \\Delta s_2 + s_2 \\Delta m_2 + \\Delta b_2$. The covariance is $\\mathrm{Cov}(\\rho_1, \\rho_2) = \\mathbb{E}[\\Delta \\rho_1 \\Delta \\rho_2]$. Since only pairs $(s_1, s_2)$ and $(b_1, b_2)$ are correlated, all other cross-terms have zero expectation.\n$$\n\\mathrm{Cov}(\\rho_1, \\rho_2) \\approx \\mathbb{E}[(m_1 \\Delta s_1)(m_2 \\Delta s_2)] + \\mathbb{E}[(\\Delta b_1)(\\Delta b_2)]\n$$\n$$\n\\mathrm{Cov}(\\rho_1, \\rho_2) \\approx m_1 m_2 \\mathrm{Cov}(s_1, s_2) + \\mathrm{Cov}(b_1, b_2)\n$$\nUsing $\\mathrm{Cov}(X,Y) = \\mathrm{corr}(X,Y)\\sigma_X\\sigma_Y$:\n$$\n\\mathrm{Cov}(\\rho_1, \\rho_2) \\approx m_1 m_2 \\mathrm{corr}(s_1, s_2) \\sigma_{s_1} \\sigma_{s_2} + \\mathrm{corr}(b_1, b_2) \\sigma_{b_1} \\sigma_{b_2}\n$$\n$$\n\\mathrm{Cov}(\\rho_1, \\rho_2) \\approx (0.15)(0.20)(0.60)(0.01)(0.01) + (0.50)(0.001)(0.001)\n$$\n$$\n\\mathrm{Cov}(\\rho_1, \\rho_2) \\approx (0.018)(10^{-4}) + (0.5)(10^{-6}) = 1.8 \\times 10^{-6} + 0.5 \\times 10^{-6} = 2.3 \\times 10^{-6}\n$$\nNow we assemble the terms to find $\\mathrm{Var}(I)$:\n$$\n\\mathrm{Var}(I) \\approx (-3.12104)^2 (7.4116 \\times 10^{-6}) + (2.46571)^2 (11.0025 \\times 10^{-6}) + 2(-3.12104)(2.46571)(2.3 \\times 10^{-6})\n$$\n$$\n\\mathrm{Var}(I) \\approx (9.74088)(7.4116 \\times 10^{-6}) + (6.08070)(11.0025 \\times 10^{-6}) - (15.3934)(2.3 \\times 10^{-6})\n$$\n$$\n\\mathrm{Var}(I) \\approx (7.22100 \\times 10^{-5}) + (6.69034 \\times 10^{-5}) - (3.54049 \\times 10^{-5})\n$$\n$$\n\\mathrm{Var}(I) \\approx (7.22100 + 6.69034 - 3.54049) \\times 10^{-5} = 10.37085 \\times 10^{-5} = 1.037085 \\times 10^{-4}\n$$\nFinally, we compute $\\mathrm{Var}(C)$:\n$$\n\\mathrm{Var}(C) = \\beta^2 \\mathrm{Var}(I) + \\gamma^2 \\mathrm{Var}(r)\n$$\n$$\n\\mathrm{Var}(C) = (0.60)^2 (1.037085 \\times 10^{-4}) + (0.20)^2 (1 \\times 10^{-4})\n$$\n$$\n\\mathrm{Var}(C) = (0.36)(1.037085 \\times 10^{-4}) + (0.04)(1 \\times 10^{-4})\n$$\n$$\n\\mathrm{Var}(C) = 3.733506 \\times 10^{-5} + 0.4 \\times 10^{-5} = 4.133506 \\times 10^{-5} = 0.00004133506\n$$\nRounding the result to four significant figures gives:\n$$\n\\mathrm{Var}(C) \\approx 0.00004134\n$$",
            "answer": "$$\\boxed{0.00004134}$$"
        }
    ]
}