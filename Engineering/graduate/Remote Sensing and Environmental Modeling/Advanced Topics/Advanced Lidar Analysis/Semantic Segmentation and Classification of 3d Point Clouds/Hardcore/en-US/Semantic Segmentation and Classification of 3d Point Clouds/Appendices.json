{
    "hands_on_practices": [
        {
            "introduction": "The foundation of many point cloud processing algorithms, including feature extraction for semantic segmentation, lies in the analysis of local neighborhoods. This exercise demonstrates why a fixed-radius neighborhood is suboptimal for data with varying point densities and guides you through designing an adaptive radius selection scheme . By ensuring a consistent number of points per neighborhood, you can stabilize feature statistics and improve the robustness of your classification models.",
            "id": "3844909",
            "problem": "You are designing an adaptive neighborhood selection scheme for semantic segmentation and classification in a three-dimensional point cloud collected over terrain with spatially varying point densities. Assume that, in the vicinity of any query location, the point sampling is locally well modeled by an inhomogeneous spatial Poisson process with local intensity (points per cubic meter) denoted by $\\hat{\\lambda}(p)$, which is a local estimator of the true intensity $\\lambda(p)$. Your goal is to choose, for each query location $p$, a spherical neighborhood of radius $r(p)$ such that the expected number of points inside the sphere is equal to a prescribed target $m_{\\mathrm{target}}$. The rationale is that, for Principal Component Analysis (PCA)-based local geometric descriptors commonly used in semantic segmentation and classification of three-dimensional point clouds, the variance of sample covariance entries scales approximately like $1/(n-1)$ where $n$ is the neighborhood size. Keeping the expected $n$ approximately constant across space helps maintain robust feature statistics under spatially varying density.\n\nFundamental base facts for this problem:\n- For a homogeneous spatial Poisson process of intensity $\\lambda$ in three dimensions, the expected number of points in a ball of radius $r$ is $\\mathbb{E}[N(B_{r})] = \\lambda \\, V(B_{r})$, where $V(B_{r})$ is the volume of the ball.\n- The volume of a ball of radius $r$ in three dimensions is $V(B_r) = \\frac{4}{3}\\pi r^3$.\n\nDesign an adaptive radius rule $r(p)$ as a function of $\\hat{\\lambda}(p)$ and $m_{\\mathrm{target}}$ so that $\\mathbb{E}[N(B_{r(p)}) \\mid \\hat{\\lambda}(p)] = m_{\\mathrm{target}}$. To keep the method numerically stable and physically plausible, apply the following clamping to handle edge cases:\n- Replace $\\hat{\\lambda}(p)$ with $\\hat{\\lambda}_{\\mathrm{eff}}(p) = \\min\\{\\max\\{\\hat{\\lambda}(p), \\lambda_{\\min}\\}, \\lambda_{\\max}\\}$ where $\\lambda_{\\min} > 0$ and $\\lambda_{\\max} > 0$ are user-chosen bounds.\n- Clamp the final radius to $[r_{\\min}, r_{\\max}]$ via $r_{\\mathrm{final}}(p) = \\min\\{\\max\\{r(p), r_{\\min}\\}, r_{\\max}\\}$.\n\nImplement a program that computes $r_{\\mathrm{final}}(p)$ for a set of query locations given their $\\hat{\\lambda}(p)$ values. Use the following constants and test suite. All intensities are in points per cubic meter and all radii must be expressed in meters.\n\nConstants:\n- $m_{\\mathrm{target}} = 50$\n- $r_{\\min} = 0.05$\n- $r_{\\max} = 10.0$\n- $\\lambda_{\\min} = 10^{-3}$\n- $\\lambda_{\\max} = 10^{5}$\n\nTest suite (each test case provides a single $\\hat{\\lambda}(p)$):\n- Case A: $\\hat{\\lambda}(p) = 10^{-1}$\n- Case B: $\\hat{\\lambda}(p) = 1$\n- Case C: $\\hat{\\lambda}(p) = 10$\n- Case D: $\\hat{\\lambda}(p) = 10^{-6}$\n- Case E: $\\hat{\\lambda}(p) = 10^{4}$\n\nYour program must:\n- Derive $r(p)$ from first principles based on the fundamental base facts above and the design objective.\n- Apply the clamping of $\\hat{\\lambda}(p)$ and of $r(p)$ as described.\n- Produce a single line of output containing the list of five radii $[r_{\\mathrm{final}}^{A}, r_{\\mathrm{final}}^{B}, r_{\\mathrm{final}}^{C}, r_{\\mathrm{final}}^{D}, r_{\\mathrm{final}}^{E}]$ as a comma-separated list enclosed in square brackets, with each radius expressed in meters and rounded to $6$ decimal places. For example, the output format must be exactly like $[x_{A},x_{B},x_{C},x_{D},x_{E}]$ where each $x_{\\cdot}$ is a decimal number with $6$ digits after the decimal point.\n\nNote: Angles are not involved in this task; no angle unit is required. All numerical answers must be in meters as specified above.",
            "solution": "The problem proposed is scientifically grounded, well-posed, and objective. It presents a standard task in the analysis of three-dimensional point cloud data, rooted in the statistical properties of spatial Poisson processes and the practical requirements of feature extraction algorithms. All necessary data, models, and constraints are provided, and they are internally consistent and physically plausible. The problem is therefore deemed valid and a full solution can be derived.\n\nThe core objective is to determine an adaptive radius $r(p)$ for a spherical neighborhood centered at a query point $p$ such that the expected number of points within this sphere, $\\mathbb{E}[N(B_{r(p)})]$, equals a target value $m_{\\mathrm{target}}$. The point cloud is modeled as an inhomogeneous spatial Poisson process with a locally estimated intensity $\\hat{\\lambda}(p)$.\n\nThe fundamental principle for a homogeneous spatial Poisson process with constant intensity $\\lambda$ is that the expected number of points in a volume $V$ is given by $\\lambda V$. For the adaptive radius selection, it is a standard and valid assumption to approximate the inhomogeneous process within a sufficiently small sphere by a homogeneous process with an intensity equal to the local estimate, $\\hat{\\lambda}(p)$. Thus, we can write:\n$$\n\\mathbb{E}[N(B_{r(p)}) \\mid \\hat{\\lambda}(p)] \\approx \\hat{\\lambda}(p) \\, V(B_{r(p)})\n$$\nThe volume of a three-dimensional ball of radius $r(p)$ is $V(B_{r(p)}) = \\frac{4}{3}\\pi [r(p)]^3$. Setting the expected number of points equal to the target $m_{\\mathrm{target}}$ yields the governing equation:\n$$\nm_{\\mathrm{target}} = \\hat{\\lambda}(p) \\left( \\frac{4}{3}\\pi [r(p)]^3 \\right)\n$$\nThis equation must be solved for the radius $r(p)$.\n\nThe complete algorithm involves three main steps: intensity clamping, radius calculation, and radius clamping.\n\nStep 1: Effective Intensity Calculation\nTo ensure numerical stability and handle physically unrealistic scenarios of extremely low or high point densities, the estimated intensity $\\hat{\\lambda}(p)$ is first clamped to a predefined range $[\\lambda_{\\min}, \\lambda_{\\max}]$. The effective intensity, $\\hat{\\lambda}_{\\mathrm{eff}}(p)$, is given by:\n$$\n\\hat{\\lambda}_{\\mathrm{eff}}(p) = \\min\\{\\max\\{\\hat{\\lambda}(p), \\lambda_{\\min}\\}, \\lambda_{\\max}\\}\n$$\nThis step prevents division by a near-zero intensity (which would yield an infinitely large radius) and limits the influence of excessively high-density regions.\n\nStep 2: Ideal Radius Calculation\nWe substitute $\\hat{\\lambda}_{\\mathrm{eff}}(p)$ into our governing equation and solve for the ideal radius, which we denote simply as $r(p)$:\n$$\nm_{\\mathrm{target}} = \\hat{\\lambda}_{\\mathrm{eff}}(p) \\left( \\frac{4}{3}\\pi [r(p)]^3 \\right)\n$$\nRearranging the terms to isolate $[r(p)]^3$:\n$$\n[r(p)]^3 = \\frac{3 \\, m_{\\mathrm{target}}}{4 \\pi \\hat{\\lambda}_{\\mathrm{eff}}(p)}\n$$\nTaking the cube root of both sides gives the expression for the ideal radius:\n$$\nr(p) = \\left( \\frac{3 \\, m_{\\mathrm{target}}}{4 \\pi \\hat{\\lambda}_{\\mathrm{eff}}(p)} \\right)^{1/3}\n$$\n\nStep 3: Final Radius Clamping\nThe calculated ideal radius $r(p)$ might still be smaller or larger than what is practical for subsequent processing steps (e.g., PCA). Therefore, a final clamping step is applied to constrain the radius to the range $[r_{\\min}, r_{\\max}]$. The final adaptive radius, $r_{\\mathrm{final}}(p)$, is:\n$$\nr_{\\mathrm{final}}(p) = \\min\\{\\max\\{r(p), r_{\\min}\\}, r_{\\max}\\}\n$$\n\nApplication to Test Cases\nThe constants are given as $m_{\\mathrm{target}} = 50$, $r_{\\min} = 0.05$ meters, $r_{\\max} = 10.0$ meters, $\\lambda_{\\min} = 10^{-3}$ points/m³, and $\\lambda_{\\max} = 10^{5}$ points/m³.\n\nCase A: $\\hat{\\lambda}(p) = 10^{-1}$\n1. $\\hat{\\lambda}_{\\mathrm{eff}}(p) = \\min\\{\\max\\{0.1, 10^{-3}\\}, 10^{5}\\} = 0.1$.\n2. $r(p) = \\left( \\frac{3 \\cdot 50}{4 \\pi \\cdot 0.1} \\right)^{1/3} = \\left( \\frac{150}{0.4\\pi} \\right)^{1/3} \\approx 4.923970$ m.\n3. $r_{\\mathrm{final}}(p) = \\min\\{\\max\\{4.923970, 0.05\\}, 10.0\\} = 4.923970$ m.\n\nCase B: $\\hat{\\lambda}(p) = 1$\n1. $\\hat{\\lambda}_{\\mathrm{eff}}(p) = \\min\\{\\max\\{1, 10^{-3}\\}, 10^{5}\\} = 1$.\n2. $r(p) = \\left( \\frac{3 \\cdot 50}{4 \\pi \\cdot 1} \\right)^{1/3} = \\left( \\frac{150}{4\\pi} \\right)^{1/3} \\approx 2.285580$ m.\n3. $r_{\\mathrm{final}}(p) = \\min\\{\\max\\{2.285580, 0.05\\}, 10.0\\} = 2.285580$ m.\n\nCase C: $\\hat{\\lambda}(p) = 10$\n1. $\\hat{\\lambda}_{\\mathrm{eff}}(p) = \\min\\{\\max\\{10, 10^{-3}\\}, 10^{5}\\} = 10$.\n2. $r(p) = \\left( \\frac{3 \\cdot 50}{4 \\pi \\cdot 10} \\right)^{1/3} = \\left( \\frac{150}{40\\pi} \\right)^{1/3} \\approx 1.060790$ m.\n3. $r_{\\mathrm{final}}(p) = \\min\\{\\max\\{1.060790, 0.05\\}, 10.0\\} = 1.060790$ m.\n\nCase D: $\\hat{\\lambda}(p) = 10^{-6}$\n1. The initial value is below $\\lambda_{\\min}$. $\\hat{\\lambda}_{\\mathrm{eff}}(p) = \\min\\{\\max\\{10^{-6}, 10^{-3}\\}, 10^{5}\\} = 10^{-3}$.\n2. $r(p) = \\left( \\frac{3 \\cdot 50}{4 \\pi \\cdot 10^{-3}} \\right)^{1/3} = \\left( \\frac{150}{0.004\\pi} \\right)^{1/3} \\approx 22.855802$ m.\n3. The calculated radius exceeds $r_{\\max}$. $r_{\\mathrm{final}}(p) = \\min\\{\\max\\{22.855802, 0.05\\}, 10.0\\} = 10.0$ m.\n\nCase E: $\\hat{\\lambda}(p) = 10^{4}$\n1. $\\hat{\\lambda}_{\\mathrm{eff}}(p) = \\min\\{\\max\\{10^{4}, 10^{-3}\\}, 10^{5}\\} = 10^{4}$.\n2. $r(p) = \\left( \\frac{3 \\cdot 50}{4 \\pi \\cdot 10^{4}} \\right)^{1/3} = \\left( \\frac{150}{40000\\pi} \\right)^{1/3} \\approx 0.106079$ m.\n3. $r_{\\mathrm{final}}(p) = \\min\\{\\max\\{0.106079, 0.05\\}, 10.0\\} = 0.106079$ m.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the adaptive neighborhood radius for 3D point clouds based on local density.\n    \"\"\"\n    # Define the constants from the problem statement.\n    m_target = 50\n    r_min = 0.05\n    r_max = 10.0\n    lambda_min = 1e-3\n    lambda_max = 1e5\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        1e-1,  # Case A\n        1.0,   # Case B\n        10.0,  # Case C\n        1e-6,  # Case D\n        1e4,   # Case E\n    ]\n\n    results = []\n    for lambda_hat in test_cases:\n        # Step 1: Apply clamping to the estimated intensity lambda_hat.\n        # This gives the effective intensity lambda_eff.\n        lambda_eff = min(max(lambda_hat, lambda_min), lambda_max)\n        \n        # Step 2: Calculate the ideal radius r(p) that yields m_target expected points.\n        # The formula is r(p) = (3 * m_target / (4 * pi * lambda_eff))^(1/3).\n        # Numerator of the fraction inside the cube root.\n        numerator = 3 * m_target\n        # Denominator of the fraction inside the cube root.\n        denominator = 4 * np.pi * lambda_eff\n        # The ideal radius calculation.\n        r_ideal = (numerator / denominator)**(1/3)\n        \n        # Step 3: Clamp the final radius to the range [r_min, r_max].\n        r_final = min(max(r_ideal, r_min), r_max)\n        \n        # Format the result to 6 decimal places and add to the list.\n        results.append(f\"{r_final:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Real-world environmental datasets are often highly imbalanced, containing many points from common classes (like ground) and very few from rare but important classes (like powerlines). This practice explores how standard training objectives can fail in such scenarios and introduces the focal loss, an advanced technique that dynamically re-weights examples to focus the model's attention on harder-to-classify points . Understanding this concept is key to building high-performance models for imbalanced data.",
            "id": "3844882",
            "problem": "Consider semantic segmentation of a $3$-dimensional Light Detection and Ranging (LiDAR) point cloud with $C$ semantic classes, including rare environmental classes such as powerlines. Let each point $i$ have logits $\\mathbf{z}_i \\in \\mathbb{R}^C$ and predicted class probabilities $p_{ic} = \\exp(z_{ic}) / \\sum_{k=1}^C \\exp(z_{ik})$, with a ground-truth class index $y_i \\in \\{1,\\dots,C\\}$. From maximum likelihood under a categorical model, the standard negative log-likelihood (cross-entropy) loss is $L_{\\mathrm{CE}} = -\\sum_i \\log p_{i y_i}$. Due to severe class imbalance (e.g., many ground or vegetation points and few powerline points), we seek a difficulty-modulated loss that attenuates the contribution of well-classified examples while preserving the maximum likelihood structure, controlled by a focusing parameter $\\gamma \\ge 0$. Starting from the definitions above and the principle that attenuation should vanish for misclassified or low-confidence examples and increase smoothly as the true-class probability $p_{i y_i}$ approaches $1$, derive such a difficulty-modulated loss. Then, obtain the gradient with respect to the true-class logit $z_{i y_i}$ to characterize how the focusing parameter $\\gamma$ changes gradient magnitudes for easy versus hard examples, and analyze the implications for training on rare classes like powerlines in $3$-dimensional LiDAR point clouds.\n\nWhich of the following statements are correct?\n\nA. Introducing a monotone attenuation factor that is $0$ at $p_{i y_i} = 1$, equals $1$ at $p_{i y_i} = 0$, and reduces to the standard negative log-likelihood when $\\gamma = 0$ leads to a difficulty-modulated loss that down-weights easy examples; its $\\gamma$ controls the rate of attenuation, and setting $\\gamma = 0$ recovers $L_{\\mathrm{CE}}$.\n\nB. For rare classes such as powerlines, increasing $\\gamma$ typically enlarges the relative gradient for misclassified or low-confidence rare points compared to correctly classified majority points, which can improve recall; however, excessively large $\\gamma$ can slow convergence or cause underfitting by nearly nulling gradients for most examples.\n\nC. Because difficulty modulation inherently resolves class-frequency imbalance, explicit per-class weighting (e.g., an $\\alpha_c$ factor) is unnecessary in highly imbalanced point clouds.\n\nD. The gradient of the difficulty-modulated loss with respect to the true-class logit $z_{i y_i}$ can be written as the cross-entropy gradient $(p_{i y_i} - 1)$ multiplied by a factor that depends on $(1 - p_{i y_i})$ and $\\log p_{i y_i}$; as $p_{i y_i} \\to 1$, the gradient magnitude decays on the order of $(1 - p_{i y_i})^{\\gamma + 1}$.\n\nE. Setting $\\gamma < 0$ is recommended because it emphasizes easy examples and stabilizes training under class imbalance.\n\nSelect all that apply.",
            "solution": "We begin from maximum likelihood for multi-class classification under the categorical distribution. For each point $i$, with true class $y_i$ and predicted probabilities $p_{ic}$ produced by the softmax of logits $\\mathbf{z}_i$, the negative log-likelihood is $L_{\\mathrm{CE}} = -\\sum_i \\log p_{i y_i}$. This loss assigns each example a contribution proportional to $-\\log p_{i y_i}$, which becomes small when $p_{i y_i}$ is close to $1$ (easy examples) and large when $p_{i y_i}$ is small (hard examples).\n\nTo attenuate easy examples while preserving the maximum likelihood structure, we introduce a multiplicative modulation function $M(p)$ applied to the per-example negative log-likelihood. We impose the following fundamental design constraints derived from the stated principles:\n- $M(p)$ is nonnegative and smooth on $p \\in (0,1)$.\n- $M(p)$ is monotone decreasing in $p$, with $M(0) = 1$ so that misclassified or low-confidence examples are not attenuated, and $M(1) = 0$ so that perfectly classified examples contribute no loss.\n- The modulation is governed by a focusing parameter $\\gamma \\ge 0$ that controls the rate of attenuation of easy examples.\n- Setting $\\gamma = 0$ must recover the standard negative log-likelihood, implying $M(p) \\equiv 1$ when $\\gamma = 0$.\n\nA simplest choice that satisfies these constraints is $M(p) = (1 - p)^\\gamma$. This choice is widely used and arises by selecting a power-law form that is $1$ at $p = 0$, vanishes at $p = 1$, is monotone decreasing, and smoothly tunable by $\\gamma$. The resulting difficulty-modulated (focal) loss is\n$$\nL = -\\sum_i (1 - p_{i y_i})^\\gamma \\log p_{i y_i}.\n$$\nThis loss reduces to $L_{\\mathrm{CE}}$ when $\\gamma = 0$ since $(1 - p_{i y_i})^0 = 1$.\n\nNext, we derive the gradient of the per-example loss $\\ell_i = -(1 - p)^\\gamma \\log p$ with respect to the true-class logit $z_{y}$, denoting $p \\equiv p_{y}$ for brevity. First compute the derivative of $\\ell_i$ with respect to $p$:\n$$\n\\frac{d\\ell_i}{dp} = -\\left[\\gamma (1 - p)^{\\gamma - 1} (-1) \\log p + (1 - p)^\\gamma \\frac{1}{p}\\right]\n= \\gamma (1 - p)^{\\gamma - 1} \\log p - (1 - p)^\\gamma \\frac{1}{p}.\n$$\nFor the softmax,\n$$\n\\frac{\\partial p}{\\partial z_{y}} = p(1 - p), \\quad \\frac{\\partial p}{\\partial z_{k}} = -p\\, p_k \\quad \\text{for } k \\ne y.\n$$\nTherefore,\n$$\n\\frac{\\partial \\ell_i}{\\partial z_{y}} = \\frac{d\\ell_i}{dp} \\cdot \\frac{\\partial p}{\\partial z_{y}}\n= p(1 - p)\\left[\\gamma (1 - p)^{\\gamma - 1} \\log p - (1 - p)^\\gamma \\frac{1}{p}\\right]\n= p(1 - p)^\\gamma\\left[\\gamma \\log p - \\frac{1 - p}{p}\\right].\n$$\nNotice that for $\\gamma = 0$,\n$$\n\\frac{\\partial \\ell_i}{\\partial z_{y}} \\bigg|_{\\gamma = 0} = p\\left[0 - \\frac{1 - p}{p}\\right] = p - 1,\n$$\nwhich matches the cross-entropy gradient for the true class. For $k \\ne y$,\n$$\n\\frac{\\partial \\ell_i}{\\partial z_{k}} = \\frac{d\\ell_i}{dp} \\cdot \\frac{\\partial p}{\\partial z_{k}}\n= -p\\, p_k\\left[\\gamma (1 - p)^{\\gamma - 1} \\log p - (1 - p)^\\gamma \\frac{1}{p}\\right]\n= -p_k (1 - p)^{\\gamma - 1}\\left[\\gamma p \\log p - (1 - p)\\right],\n$$\nand for $\\gamma = 0$, this reduces to $p_k$, the standard cross-entropy gradient.\n\nAsymptotic behavior for easy examples: let $p = 1 - \\varepsilon$ with small $\\varepsilon > 0$. Then $\\log p \\approx -\\varepsilon$, $p \\approx 1$, and $\\frac{1 - p}{p} \\approx \\varepsilon$. The bracketed term in $\\partial \\ell_i / \\partial z_{y}$ becomes\n$$\n\\gamma \\log p - \\frac{1 - p}{p} \\approx -\\gamma \\varepsilon - \\varepsilon = -(\\gamma + 1)\\varepsilon,\n$$\nand $(1 - p)^\\gamma = \\varepsilon^\\gamma$. Thus,\n$$\n\\left|\\frac{\\partial \\ell_i}{\\partial z_{y}}\\right| \\approx (\\gamma + 1)\\,\\varepsilon^{\\gamma + 1},\n$$\nwhich shows that the gradient magnitude decays on the order of $(1 - p)^{\\gamma + 1}$ as $p \\to 1$, increasingly suppressing the contribution of easy examples when $\\gamma$ grows.\n\nImplications for rare classes such as powerlines: rare structures tend to have lower initial $p_{i y_i}$ because the model is biased toward frequent classes. The modulation factor $(1 - p_{i y_i})^\\gamma$ remains close to $1$ for hard examples with small $p_{i y_i}$, so their loss and gradients are not attenuated, while easy majority examples with large $p_{i y_i}$ are sharply down-weighted, especially for larger $\\gamma$. This shifts optimization focus toward hard or misclassified points, which often include rare classes, potentially improving recall. However, if $\\gamma$ is too large, gradients for the bulk of examples become extremely small (as shown by the $(1 - p)^{\\gamma + 1}$ scaling), which can slow convergence or cause underfitting. Furthermore, difficulty modulation operates on example hardness, not class frequency. In highly imbalanced settings, explicit per-class weighting (introducing an $\\alpha_c$ factor) or sampling strategies are often needed in addition to the focusing parameter to counter prior bias.\n\nOption-by-option analysis:\n- Option A: The described attenuation factor conditions and the recovery of $L_{\\mathrm{CE}}$ at $\\gamma = 0$ align with the derivation $M(p) = (1 - p)^\\gamma$, yielding a loss that down-weights easy examples. This is consistent with the principles and the resulting form. Verdict: Correct.\n- Option B: Increasing $\\gamma$ raises the relative emphasis on hard examples (small $p$), which frequently belong to rare classes like powerlines, thereby improving recall. The asymptotics show that large $\\gamma$ strongly attenuates gradients when $p \\to 1$, and in practice excessive $\\gamma$ can slow training or underfit. Verdict: Correct.\n- Option C: Difficulty modulation targets example hardness, not class priors. In severe imbalance, per-class weighting (e.g., an $\\alpha_c$ factor) remains beneficial or necessary. Thus, the claim that class weighting is unnecessary is not generally true. Verdict: Incorrect.\n- Option D: We explicitly derived $\\partial \\ell_i / \\partial z_y$ and showed it can be written as the cross-entropy gradient $(p - 1)$ times a factor depending on $(1 - p)$ and $\\log p$; the asymptotic decay order $(1 - p)^{\\gamma + 1}$ as $p \\to 1$ is correct. Verdict: Correct.\n- Option E: Standard focal loss uses $\\gamma \\ge 0$. Setting $\\gamma < 0$ would amplify easy examples and is not recommended for addressing class imbalance; it undermines the intended focus on hard examples. Verdict: Incorrect.",
            "answer": "$$\\boxed{ABD}$$"
        },
        {
            "introduction": "A model is only as good as our ability to measure its performance. This final practice moves from training to evaluation, using the confusion matrix as the cornerstone for a deep analysis of classifier behavior . You will learn to compute essential metrics like precision, recall, and the $F1$-score, and explore how real-world challenges, such as ambiguity at class boundaries, tangibly affect these evaluation measures.",
            "id": "3844924",
            "problem": "A Light Detection and Ranging (LiDAR) semantic segmentation pipeline is deployed to classify a three-dimensional point cloud acquired over a mixed urban–estuarine landscape into four semantic classes: ground, vegetation, building, and water. Consider a single, temporally consistent acquisition such that sampling density and occlusion patterns are stable across the scene. You are given the following scientifically plausible setup grounded in standard definitions of classification events:\n\n- There are $4$ classes: $C_{1}$ (ground), $C_{2}$ (vegetation), $C_{3}$ (building), and $C_{4}$ (water).\n- The true class counts are: $N_{1} = 5000$, $N_{2} = 8000$, $N_{3} = 3000$, $N_{4} = 4000$.\n- A baseline classifier produces the following counts of predicted labels for each true class (rows index true classes and columns index predicted classes in the order $C_{1},C_{2},C_{3},C_{4}$):\n  - For $C_{1}$: $[4600, 200, 150, 50]$.\n  - For $C_{2}$: $[300, 7200, 200, 300]$.\n  - For $C_{3}$: $[100, 120, 2700, 80]$.\n  - For $C_{4}$: $[50, 250, 100, 3600]$.\n\nEnvironmental boundaries (e.g., vegetation–water ecotones, ground–building edges) induce additional label ambiguity for points whose local neighborhoods straddle heterogeneous materials. Empirical field mapping indicates that boundary effects manifest as a redistribution of some diagonal (correct) predictions into the most confusable neighboring classes. Model this boundary-induced redistribution as the following per-row reassignments from the diagonal of the baseline matrix to its off-diagonals:\n\n- From $C_{1}$ correctly predicted points, reassign $60$ to $C_{2}$, $40$ to $C_{3}$, and $20$ to $C_{4}$.\n- From $C_{2}$ correctly predicted points, reassign $80$ to $C_{1}$, $60$ to $C_{3}$, and $60$ to $C_{4}$.\n- From $C_{3}$ correctly predicted points, reassign $30$ to $C_{1}$, $40$ to $C_{2}$, and $20$ to $C_{4}$.\n- From $C_{4}$ correctly predicted points, reassign $40$ to $C_{1}$, $80$ to $C_{2}$, and $40$ to $C_{3}$.\n\nTasks:\n1. Construct the baseline confusion matrix and the boundary-affected confusion matrix, ensuring that each row sum equals the corresponding true class count $N_{k}$ and that the grand total equals $N_{1}+N_{2}+N_{3}+N_{4}$.\n2. Using only core definitions of classification events and metrics, derive for the boundary-affected confusion matrix the per-class precision, recall, and $F1$-score for each $C_{k}$, where precision, recall, and $F1$-score are defined from true positives, false positives, and false negatives.\n3. Explain, using the confusion matrix structure, why boundary points change these metrics for each class in terms of increases in off-diagonal entries and corresponding changes in false positives and false negatives.\n4. Finally, compute the macro-averaged $F1$-score (the arithmetic mean of the per-class $F1$-scores) for the boundary-affected confusion matrix. Express your final answer as a single exact fraction or a closed-form analytic expression. Do not round. The final answer must be this macro-averaged $F1$-score.",
            "solution": "The problem statement is subjected to validation.\n\nGivens are extracted as follows:\n- There are $4$ classes, denoted $C_{k}$ for $k \\in \\{1, 2, 3, 4\\}$.\n- The true class counts (number of points per class) are $N_{1} = 5000$, $N_{2} = 8000$, $N_{3} = 3000$, and $N_{4} = 4000$. The total number of points is $N = N_{1}+N_{2}+N_{3}+N_{4} = 20000$.\n- A baseline classifier's performance is given by the rows of a confusion matrix, where rows are true classes and columns are predicted classes:\n  - For true class $C_{1}$: $[4600, 200, 150, 50]$. Row sum: $4600+200+150+50 = 5000 = N_{1}$.\n  - For true class $C_{2}$: $[300, 7200, 200, 300]$. Row sum: $300+7200+200+300 = 8000 = N_{2}$.\n  - For true class $C_{3}$: $[100, 120, 2700, 80]$. Row sum: $100+120+2700+80 = 3000 = N_{3}$.\n  - For true class $C_{4}$: $[50, 250, 100, 3600]$. Row sum: $50+250+100+3600 = 4000 = N_{4}$.\n- Boundary effects cause a redistribution of correctly classified points (diagonal entries) to off-diagonal entries within the same row:\n  - From $C_{1}$: $60$ points to $C_{2}$, $40$ to $C_{3}$, $20$ to $C_{4}$. Total moved: $120$.\n  - From $C_{2}$: $80$ points to $C_{1}$, $60$ to $C_{3}$, $60$ to $C_{4}$. Total moved: $200$.\n  - From $C_{3}$: $30$ points to $C_{1}$, $40$ to $C_{2}$, $20$ to $C_{4}$. Total moved: $90$.\n  - From $C_{4}$: $40$ points to $C_{1}$, $80$ to $C_{2}$, $40$ to $C_{3}$. Total moved: $160$.\n\nThe validation check confirms that the problem is scientifically grounded in the domain of remote sensing and machine learning classification. The provided data is self-consistent, as the row sums of the baseline predictions match the true class counts. The boundary effect is described as a set of operations that conserve the row sums, which is logically consistent. The problem is well-posed, objective, and contains sufficient information for a unique solution. No flaws are detected. The problem is deemed valid.\n\nThe solution proceeds as follows.\n\n**1. Construction of Confusion Matrices**\n\nA confusion matrix $M$ is a square matrix of size $K \\times K$ for a $K$-class problem, where the entry $M_{ij}$ is the number of observations of true class $C_i$ that are predicted as class $C_j$.\n\nThe baseline confusion matrix, $M_{base}$, is constructed directly from the provided counts:\n$$\nM_{base} = \\begin{pmatrix}\n4600 & 200 & 150 & 50 \\\\\n300 & 7200 & 200 & 300 \\\\\n100 & 120 & 2700 & 80 \\\\\n50 & 250 & 100 & 3600\n\\end{pmatrix}\n$$\nAs verified during validation, the sum of each row $k$ equals the corresponding true class count $N_k$. The grand total of all entries is $N = 20000$.\n\nThe boundary-affected confusion matrix, $M_{bound}$, is obtained by applying the specified redistributions to $M_{base}$. For each row $k$, we subtract the total number of reassigned points from the diagonal element $M_{kk}$ and add them to the corresponding off-diagonal elements $M_{kj}$ ($j \\neq k$).\n\nFor Row 1 (True $C_1$):\n- Diagonal: $4600 - (60+40+20) = 4600 - 120 = 4480$.\n- Off-diagonals: $M_{12} \\to 200+60=260$; $M_{13} \\to 150+40=190$; $M_{14} \\to 50+20=70$.\n- New Row 1: $[4480, 260, 190, 70]$. Sum: $5000 = N_1$.\n\nFor Row 2 (True $C_2$):\n- Diagonal: $7200 - (80+60+60) = 7200 - 200 = 7000$.\n- Off-diagonals: $M_{21} \\to 300+80=380$; $M_{23} \\to 200+60=260$; $M_{24} \\to 300+60=360$.\n- New Row 2: $[380, 7000, 260, 360]$. Sum: $8000 = N_2$.\n\nFor Row 3 (True $C_3$):\n- Diagonal: $2700 - (30+40+20) = 2700 - 90 = 2610$.\n- Off-diagonals: $M_{31} \\to 100+30=130$; $M_{32} \\to 120+40=160$; $M_{34} \\to 80+20=100$.\n- New Row 3: $[130, 160, 2610, 100]$. Sum: $3000 = N_3$.\n\nFor Row 4 (True $C_4$):\n- Diagonal: $3600 - (40+80+40) = 3600 - 160 = 3440$.\n- Off-diagonals: $M_{41} \\to 50+40=90$; $M_{42} \\to 250+80=330$; $M_{43} \\to 100+40=140$.\n- New Row 4: $[90, 330, 140, 3440]$. Sum: $4000 = N_4$.\n\nThe resulting boundary-affected confusion matrix is:\n$$\nM_{bound} = \\begin{pmatrix}\n4480 & 260 & 190 & 70 \\\\\n380 & 7000 & 260 & 360 \\\\\n130 & 160 & 2610 & 100 \\\\\n90 & 330 & 140 & 3440\n\\end{pmatrix}\n$$\nThe row sums are preserved, and the grand total remains $20000$.\n\n**2. Per-Class Classification Metrics for $M_{bound}$**\n\nFor each class $C_k$, we compute the true positives ($TP_k$), false positives ($FP_k$), and false negatives ($FN_k$).\n- $TP_k = M_{bound, kk}$ (diagonal element).\n- $FP_k = (\\sum_{i=1}^{4} M_{bound, ik}) - TP_k$ (column sum minus diagonal).\n- $FN_k = (\\sum_{j=1}^{4} M_{bound, kj}) - TP_k = N_k - TP_k$ (row sum minus diagonal).\n\nThe metrics are defined as:\n- Precision: $P_k = \\frac{TP_k}{TP_k + FP_k}$\n- Recall: $R_k = \\frac{TP_k}{TP_k + FN_k}$\n- $F1$-score: $F1_k = 2 \\frac{P_k R_k}{P_k + R_k} = \\frac{2TP_k}{2TP_k + FP_k + FN_k}$\n\nFirst, we compute the column sums of $M_{bound}$:\n- Col 1 sum: $4480+380+130+90 = 5080$.\n- Col 2 sum: $260+7000+160+330 = 7750$.\n- Col 3 sum: $190+260+2610+140 = 3200$.\n- Col 4 sum: $70+360+100+3440 = 3970$.\n\nNow, we calculate the metrics for each class:\n\n- **Class $C_1$ (ground):**\n  $TP_1 = 4480$.\n  $FP_1 = 5080 - 4480 = 600$.\n  $FN_1 = 5000 - 4480 = 520$.\n  $P_1 = \\frac{4480}{5080} = \\frac{112}{127}$.\n  $R_1 = \\frac{4480}{5000} = \\frac{112}{125}$.\n  $F1_1 = \\frac{2 \\times 4480}{2 \\times 4480 + 600 + 520} = \\frac{8960}{8960 + 1120} = \\frac{8960}{10080} = \\frac{896}{1008} = \\frac{8}{9}$.\n\n- **Class $C_2$ (vegetation):**\n  $TP_2 = 7000$.\n  $FP_2 = 7750 - 7000 = 750$.\n  $FN_2 = 8000 - 7000 = 1000$.\n  $P_2 = \\frac{7000}{7750} = \\frac{28}{31}$.\n  $R_2 = \\frac{7000}{8000} = \\frac{7}{8}$.\n  $F1_2 = \\frac{2 \\times 7000}{2 \\times 7000 + 750 + 1000} = \\frac{14000}{14000 + 1750} = \\frac{14000}{15750} = \\frac{1400}{1575} = \\frac{56}{63} = \\frac{8}{9}$.\n\n- **Class $C_3$ (building):**\n  $TP_3 = 2610$.\n  $FP_3 = 3200 - 2610 = 590$.\n  $FN_3 = 3000 - 2610 = 390$.\n  $P_3 = \\frac{2610}{3200} = \\frac{261}{320}$.\n  $R_3 = \\frac{2610}{3000} = \\frac{261}{300} = \\frac{87}{100}$.\n  $F1_3 = \\frac{2 \\times 2610}{2 \\times 2610 + 590 + 390} = \\frac{5220}{5220 + 980} = \\frac{5220}{6200} = \\frac{522}{620} = \\frac{261}{310}$.\n\n- **Class $C_4$ (water):**\n  $TP_4 = 3440$.\n  $FP_4 = 3970 - 3440 = 530$.\n  $FN_4 = 4000 - 3440 = 560$.\n  $P_4 = \\frac{3440}{3970} = \\frac{344}{397}$.\n  $R_4 = \\frac{3440}{4000} = \\frac{344}{400} = \\frac{43}{50}$.\n  $F1_4 = \\frac{2 \\times 3440}{2 \\times 3440 + 530 + 560} = \\frac{6880}{6880 + 1090} = \\frac{6880}{7970} = \\frac{688}{797}$.\n\n**3. Explanation of Metric Changes Due to Boundary Effects**\n\nThe boundary-induced redistribution exclusively moves points from a correct classification to an incorrect one. Let us analyze this for an arbitrary class $C_k$.\n- The counts of **True Positives**, $TP_k=M_{kk}$, are explicitly reduced as points are reassigned from the diagonal.\n- The counts of **False Negatives**, $FN_k$, are the sum of off-diagonal elements in row $k$ ($FN_k = \\sum_{j \\neq k} M_{kj}$). Since points from $M_{kk}$ are moved to $M_{kj}$ ($j \\neq k$), $FN_k$ increases by the exact amount that $TP_k$ decreases.\n- The counts of **False Positives**, $FP_k$, are the sum of off-diagonal elements in column $k$ ($FP_k = \\sum_{i \\neq k} M_{ik}$). The problem states that for every class $C_i$, some of its correctly classified points are reassigned to other classes, including $C_k$. This means that the entries $M_{ik}$ for $i \\neq k$ increase. Consequently, $FP_k$ increases for all classes.\n\nThese changes directly impact the metrics:\n- **Recall**, $R_k = \\frac{TP_k}{TP_k + FN_k} = \\frac{TP_k}{N_k}$. Since $TP_k$ decreases and the denominator $N_k$ is constant, recall for each class decreases.\n- **Precision**, $P_k = \\frac{TP_k}{TP_k + FP_k}$. The numerator $TP_k$ decreases while the denominator term $FP_k$ increases. Both factors contribute to a decrease in the ratio, so precision for each class decreases.\n- **$F1$-score**, $F1_k$, is the harmonic mean of precision and recall. Since both $P_k$ and $R_k$ decrease, their harmonic mean must also decrease. The boundary effects, as modeled, unambiguously degrade the classifier's performance for every class.\n\n**4. Macro-Averaged F1-Score**\n\nThe macro-averaged $F1$-score, $F1_{macro}$, is the arithmetic mean of the per-class $F1$-scores.\n$$\nF1_{macro} = \\frac{1}{4} \\sum_{k=1}^{4} F1_k = \\frac{1}{4} (F1_1 + F1_2 + F1_3 + F1_4)\n$$\nSubstituting the calculated values:\n$$\nF1_{macro} = \\frac{1}{4} \\left( \\frac{8}{9} + \\frac{8}{9} + \\frac{261}{310} + \\frac{688}{797} \\right) = \\frac{1}{4} \\left( \\frac{16}{9} + \\frac{261}{310} + \\frac{688}{797} \\right)\n$$\nTo combine these into a single fraction, we find a common denominator. The denominators are $9=3^2$, $310=2 \\times 5 \\times 31$, and $797$ (which is a prime number). The least common multiple is $9 \\times 310 \\times 797 = 2223630$.\n$$\nF1_{macro} = \\frac{1}{4} \\left( \\frac{16 \\times (310 \\times 797)}{2223630} + \\frac{261 \\times (9 \\times 797)}{2223630} + \\frac{688 \\times (9 \\times 310)}{2223630} \\right)\n$$\n$$\nF1_{macro} = \\frac{1}{4} \\left( \\frac{16 \\times 247070}{2223630} + \\frac{261 \\times 7173}{2223630} + \\frac{688 \\times 2790}{2223630} \\right)\n$$\n$$\nF1_{macro} = \\frac{1}{4} \\left( \\frac{3953120 + 1872153 + 1919520}{2223630} \\right)\n$$\n$$\nF1_{macro} = \\frac{1}{4} \\left( \\frac{7744793}{2223630} \\right)\n$$\n$$\nF1_{macro} = \\frac{7744793}{4 \\times 2223630} = \\frac{7744793}{8894520}\n$$\nThe numerator and denominator have no common prime factors, so this fraction is in its simplest form.",
            "answer": "$$\n\\boxed{\\frac{7744793}{8894520}}\n$$"
        }
    ]
}