{
    "hands_on_practices": [
        {
            "introduction": "The first step in any parameter estimation task is to construct a well-posed objective function that faithfully represents the physics of the measurement and our prior knowledge. In a Bayesian framework, this leads to maximizing the posterior probability, or equivalently, minimizing the negative log-posterior. This exercise  guides you through this foundational process, showing how to combine a likelihood function, prior distributions, and a parameter transformation to handle physical constraints, resulting in an unconstrained optimization problem ready for numerical solution.",
            "id": "3831139",
            "problem": "A hyperspectral imaging spectrometer acquires radiance measurements at $m$ wavelengths over a vegetated surface. A simplified linearized forward model relates the measured radiance vector $y \\in \\mathbb{R}^{m}$ to a set of $n$ strictly positive biophysical parameters $\\theta \\in \\mathbb{R}_{0}^{n}$ (e.g., nonnegative abundance-like scaling coefficients of radiative transfer basis functions) via a known sensitivity matrix $G \\in \\mathbb{R}^{m \\times n}$, with additive, independent Gaussian measurement errors. Specifically, the data model is the conditional distribution $y \\,|\\, \\theta \\sim \\mathcal{N}(G \\theta, \\sigma^{2} I_{m})$, where $\\mathcal{N}(\\mu, \\Sigma)$ denotes the multivariate normal distribution with mean $\\mu$ and covariance $\\Sigma$, $\\sigma^{2}  0$ is a known noise variance, and $I_{m}$ is the $m \\times m$ identity matrix. To encode physical positivity and prior knowledge, assume the components of $\\theta$ are a priori independent and follow Gamma distributions with shape $k_{i}  0$ and rate $\\lambda_{i}  0$, i.e., for each $i \\in \\{1,\\dots,n\\}$, $\\theta_{i} \\sim \\mathrm{Gamma}(k_{i}, \\lambda_{i})$ with density $p(\\theta_{i}) = \\frac{\\lambda_{i}^{k_{i}}}{\\Gamma(k_{i})} \\theta_{i}^{k_{i}-1} \\exp(-\\lambda_{i} \\theta_{i})$ for $\\theta_{i}  0$, where $\\Gamma(\\cdot)$ is the Gamma function.\n\nUsing Bayes’ theorem, the posterior density satisfies $p(\\theta \\,|\\, y) \\propto p(y \\,|\\, \\theta) \\, p(\\theta)$. Consider Maximum A Posteriori (MAP) estimation in an unconstrained space by introducing the log-transform $\\phi \\in \\mathbb{R}^{n}$ defined componentwise by $\\phi_{i} = \\ln(\\theta_{i})$ for $i = 1,\\dots,n$, so that $\\theta = \\exp(\\phi)$ with the exponential applied elementwise. Treat $\\phi$ as the optimization variable and use the change-of-variables rule for densities to write the posterior density $p(\\phi \\,|\\, y)$ in terms of $p(\\theta \\,|\\, y)$ and the Jacobian of the transformation.\n\nDerive and simplify the negative log-posterior objective $J_{\\phi}(\\phi)$ to be minimized with respect to $\\phi$, expressed entirely in terms of $\\phi$, $y$, $G$, $\\sigma^{2}$, $k_{i}$, and $\\lambda_{i}$. Ignore additive constants that do not depend on $\\phi$. Your final answer must be a single closed-form analytical expression. No numerical approximation is required.",
            "solution": "The user wants to solve the problem. I will now proceed with the solution.\nThe problem is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\n### Step 1: Extract Givens\n-   Measured radiance vector: $y \\in \\mathbb{R}^{m}$\n-   Biophysical parameters: $\\theta \\in \\mathbb{R}_{0}^{n}$\n-   Sensitivity matrix: $G \\in \\mathbb{R}^{m \\times n}$\n-   Data model (likelihood): $y \\,|\\, \\theta \\sim \\mathcal{N}(G \\theta, \\sigma^{2} I_{m})$\n-   Known noise variance: $\\sigma^{2}  0$\n-   Prior on $\\theta$ components: Independent, with $\\theta_{i} \\sim \\mathrm{Gamma}(k_{i}, \\lambda_{i})$ for $i \\in \\{1,\\dots,n\\}$\n-   Gamma density: $p(\\theta_{i}) = \\frac{\\lambda_{i}^{k_{i}}}{\\Gamma(k_{i})} \\theta_{i}^{k_{i}-1} \\exp(-\\lambda_{i} \\theta_{i})$ for $\\theta_i  0$, with shape $k_i  0$ and rate $\\lambda_i  0$\n-   Bayesian posterior: $p(\\theta \\,|\\, y) \\propto p(y \\,|\\, \\theta) \\, p(\\theta)$\n-   Log-transformation: $\\phi_{i} = \\ln(\\theta_{i})$ for $i=1, \\dots, n$, so $\\theta = \\exp(\\phi)$ (elementwise)\n-   Objective: Derive the negative log-posterior objective function $J_{\\phi}(\\phi)$ to be minimized with respect to $\\phi$, ignoring additive constants independent of $\\phi$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is a standard Bayesian inference task. The formulation uses a linear forward model with additive Gaussian noise, which is common in many physical sciences. The use of Gamma priors for strictly positive quantities is a standard and well-justified choice in statistical modeling. The reparameterization via a log-transform to convert a constrained optimization problem ($\\theta_i  0$) into an unconstrained one is a widely used and mathematically sound technique. All definitions are precise, and all necessary information is provided. The problem is scientifically grounded, well-posed, objective, and complete.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete, reasoned solution will be provided.\n\n### Derivation of the Objective Function\nThe goal is to find the Maximum A Posteriori (MAP) estimate of the parameters in the transformed space, which is equivalent to minimizing the negative log-posterior density. We will derive this objective function, $J_{\\phi}(\\phi)$.\n\nFirst, we establish the posterior probability density of $\\theta$ using Bayes' theorem:\n$$\np(\\theta \\,|\\, y) \\propto p(y \\,|\\, \\theta) p(\\theta)\n$$\nThe likelihood term $p(y \\,|\\, \\theta)$ is given by the multivariate normal distribution $y \\,|\\, \\theta \\sim \\mathcal{N}(G \\theta, \\sigma^{2} I_{m})$. The probability density function is:\n$$\np(y \\,|\\, \\theta) = \\frac{1}{(2\\pi\\sigma^{2})^{m/2}} \\exp\\left(-\\frac{1}{2\\sigma^{2}} (y - G\\theta)^{T}(y - G\\theta)\\right) = \\frac{1}{(2\\pi\\sigma^{2})^{m/2}} \\exp\\left(-\\frac{1}{2\\sigma^{2}} \\|y - G\\theta\\|_2^2\\right)\n$$\nDropping the normalization constant, which is independent of $\\theta$, we have:\n$$\np(y \\,|\\, \\theta) \\propto \\exp\\left(-\\frac{1}{2\\sigma^{2}} \\|y - G\\theta\\|_2^2\\right)\n$$\nThe prior term $p(\\theta)$ is given by the product of the independent Gamma distributions for each component $\\theta_{i}$:\n$$\np(\\theta) = \\prod_{i=1}^{n} p(\\theta_{i}) = \\prod_{i=1}^{n} \\frac{\\lambda_{i}^{k_{i}}}{\\Gamma(k_{i})} \\theta_{i}^{k_{i}-1} \\exp(-\\lambda_{i} \\theta_{i})\n$$\nAgain, dropping the normalization constants $\\frac{\\lambda_{i}^{k_{i}}}{\\Gamma(k_{i})}$, we get:\n$$\np(\\theta) \\propto \\left( \\prod_{i=1}^{n} \\theta_{i}^{k_{i}-1} \\right) \\exp\\left(-\\sum_{i=1}^{n} \\lambda_{i} \\theta_{i}\\right)\n$$\nThis can be rewritten as:\n$$\np(\\theta) \\propto \\exp\\left(\\sum_{i=1}^{n} (k_{i}-1)\\ln(\\theta_{i})\\right) \\exp\\left(-\\sum_{i=1}^{n} \\lambda_{i} \\theta_{i}\\right)\n$$\nCombining the likelihood and prior, the posterior for $\\theta$ is:\n$$\np(\\theta \\,|\\, y) \\propto \\exp\\left(-\\frac{1}{2\\sigma^{2}} \\|y - G\\theta\\|_2^2 - \\sum_{i=1}^{n} \\lambda_{i} \\theta_{i} + \\sum_{i=1}^{n} (k_{i}-1)\\ln(\\theta_{i})\\right)\n$$\nNext, we perform the change of variables from $\\theta$ to $\\phi$ where $\\phi_{i} = \\ln(\\theta_{i})$, or $\\theta_{i} = \\exp(\\phi_{i})$. The probability density functions are related by:\n$$\np_{\\phi}(\\phi \\,|\\, y) = p_{\\theta}(\\theta(\\phi) \\,|\\, y) \\left| \\det\\left(J_{\\phi \\to \\theta}\\right) \\right|\n$$\nwhere $J_{\\phi \\to \\theta}$ is the Jacobian matrix of the transformation from $\\phi$ to $\\theta$. The elements of this Jacobian are $(J_{\\phi \\to \\theta})_{ij} = \\frac{\\partial \\theta_i}{\\partial \\phi_j}$. Since $\\theta_i = \\exp(\\phi_i)$, the transformation is component-wise, and the Jacobian is a diagonal matrix:\n$$\n\\frac{\\partial \\theta_i}{\\partial \\phi_j} = \\delta_{ij} \\frac{d}{d\\phi_i}(\\exp(\\phi_i)) = \\delta_{ij} \\exp(\\phi_i)\n$$\nwhere $\\delta_{ij}$ is the Kronecker delta. The Jacobian matrix is:\n$$\nJ_{\\phi \\to \\theta} = \\mathrm{diag}(\\exp(\\phi_1), \\exp(\\phi_2), \\dots, \\exp(\\phi_n))\n$$\nIts determinant is the product of its diagonal elements:\n$$\n\\det(J_{\\phi \\to \\theta}) = \\prod_{i=1}^{n} \\exp(\\phi_i) = \\exp\\left(\\sum_{i=1}^{n} \\phi_i\\right)\n$$\nSince $\\exp(\\phi_i)  0$, the absolute value is redundant.\n\nNow, we can write the posterior for $\\phi$. We substitute $\\theta = \\exp(\\phi)$ into the expression for $p(\\theta \\,|\\, y)$ and multiply by the Jacobian determinant.\n$$\np_{\\phi}(\\phi \\,|\\, y) \\propto \\exp\\left(-\\frac{1}{2\\sigma^{2}} \\|y - G\\exp(\\phi)\\|_2^2 - \\sum_{i=1}^{n} \\lambda_{i} \\exp(\\phi_{i}) + \\sum_{i=1}^{n} (k_{i}-1)\\ln(\\exp(\\phi_{i}))\\right) \\cdot \\exp\\left(\\sum_{i=1}^{n} \\phi_i\\right)\n$$\nUsing $\\ln(\\exp(\\phi_{i})) = \\phi_i$ and combining all the terms in the exponent:\n$$\np_{\\phi}(\\phi \\,|\\, y) \\propto \\exp\\left(-\\frac{1}{2\\sigma^{2}} \\|y - G\\exp(\\phi)\\|_2^2 - \\sum_{i=1}^{n} \\lambda_{i} \\exp(\\phi_{i}) + \\sum_{i=1}^{n} (k_{i}-1)\\phi_{i} + \\sum_{i=1}^{n} \\phi_i\\right)\n$$\nSimplifying the sum over the $\\phi_i$ terms:\n$$\n\\sum_{i=1}^{n} (k_{i}-1)\\phi_{i} + \\sum_{i=1}^{n} \\phi_i = \\sum_{i=1}^{n} (k_{i}-1+1)\\phi_{i} = \\sum_{i=1}^{n} k_{i}\\phi_{i}\n$$\nThe posterior for $\\phi$ is thus:\n$$\np_{\\phi}(\\phi \\,|\\, y) \\propto \\exp\\left(-\\frac{1}{2\\sigma^{2}} \\|y - G\\exp(\\phi)\\|_2^2 - \\sum_{i=1}^{n} \\lambda_{i} \\exp(\\phi_{i}) + \\sum_{i=1}^{n} k_{i}\\phi_{i}\\right)\n$$\nThe log-posterior is the natural logarithm of this expression:\n$$\n\\ln(p_{\\phi}(\\phi \\,|\\, y)) = C - \\frac{1}{2\\sigma^{2}} \\|y - G\\exp(\\phi)\\|_2^2 - \\sum_{i=1}^{n} \\lambda_{i} \\exp(\\phi_{i}) + \\sum_{i=1}^{n} k_{i}\\phi_{i}\n$$\nwhere $C$ is a constant that does not depend on $\\phi$.\n\nThe objective function to be minimized, $J_{\\phi}(\\phi)$, is the negative of the log-posterior. We can ignore the additive constant $C$:\n$$\nJ_{\\phi}(\\phi) = - \\left( \\ln(p_{\\phi}(\\phi \\,|\\, y)) - C \\right)\n$$\n$$\nJ_{\\phi}(\\phi) = \\frac{1}{2\\sigma^{2}} \\|y - G\\exp(\\phi)\\|_2^2 + \\sum_{i=1}^{n} \\lambda_{i} \\exp(\\phi_{i}) - \\sum_{i=1}^{n} k_{i}\\phi_{i}\n$$\nThis expression is the final simplified negative log-posterior objective function, expressed entirely in terms of the required variables. The vector $\\exp(\\phi)$ is defined component-wise as $(\\exp(\\phi))_i = \\exp(\\phi_i)$.",
            "answer": "$$ \\boxed{ \\frac{1}{2\\sigma^{2}} \\|y - G\\exp(\\phi)\\|_2^2 + \\sum_{i=1}^{n} \\lambda_{i} \\exp(\\phi_{i}) - \\sum_{i=1}^{n} k_{i}\\phi_{i} } $$"
        },
        {
            "introduction": "Once an optimization algorithm yields a set of best-fit parameters, a critical question arises: how well does the model actually fit the data? A small value of the objective function is not sufficient proof of a good fit. The reduced chi-squared ($\\chi^2_\\nu$) statistic provides a rigorous answer by normalizing the sum of squared, weighted residuals by the number of degrees of freedom. This practice  delves into the derivation and calculation of this essential goodness-of-fit metric, which helps diagnose model misspecification or incorrect error assumptions.",
            "id": "3831121",
            "problem": "In a satellite-based remote sensing retrieval of land surface properties, suppose a forward model maps a scalar atmospheric parameter $x$ (e.g., aerosol optical depth) to $n$ multispectral reflectance observations via a differentiable function $f(x)$, and the measurement errors are additive, zero-mean, and multivariate Gaussian with covariance matrix $\\Sigma \\in \\mathbb{R}^{n \\times n}$. The parameter $x$ is estimated by minimizing the negative log-likelihood, equivalently by Generalized Least Squares (GLS), producing an estimate $\\hat{x}$ and a residual vector $r = y - f(\\hat{x})$, where $y \\in \\mathbb{R}^{n}$ is the observed reflectance vector. \n\nStarting from the fundamental definition of the multivariate normal density and the notion of Maximum Likelihood Estimation (MLE), derive the goodness-of-fit statistic that averages the normalized squared residuals over the available degrees of freedom. Clearly identify the role of the number of observations $n$ and the number of fitted parameters $p$ in this statistic, and state its interpretation in terms of expected magnitude when the model and the noise covariance are correctly specified.\n\nThen, for a concrete retrieval with $n = 3$ channels and $p = 1$ fitted parameter, suppose the residual vector and covariance matrix are\n$$\nr = \\begin{pmatrix} 3 \\\\ -3 \\\\ 6 \\end{pmatrix}, \n\\quad\n\\Sigma = \\begin{pmatrix}\n4  2  0 \\\\\n2  4  0 \\\\\n0  0  9\n\\end{pmatrix}.\n$$\nCompute the goodness-of-fit statistic you derived. Express your final value as a unitless quantity and round your answer to $4$ significant figures.",
            "solution": "The problem asks for the derivation of a goodness-of-fit statistic for a parameter estimation problem, its interpretation, and its computation for a specific case.\n\n### Step 1: Validation of the Problem Statement\nThe problem statement is first validated against the required criteria.\n\n**1. Extraction of Givens:**\n- A forward model maps a scalar parameter $x$ to $n$ observations: $f(x)$.\n- The function $f(x)$ is differentiable.\n- Measurement errors are additive, zero-mean, and multivariate Gaussian with covariance matrix $\\Sigma \\in \\mathbb{R}^{n \\times n}$.\n- The parameter $x$ is estimated by minimizing the negative log-likelihood (MLE/GLS).\n- The estimate is denoted as $\\hat{x}$.\n- The residual vector is $r = y - f(\\hat{x})$, where $y \\in \\mathbb{R}^{n}$ is the observation vector.\n- For a specific case:\n  - Number of observations (channels): $n = 3$.\n  - Number of fitted parameters: $p = 1$.\n  - Residual vector: $r = \\begin{pmatrix} 3 \\\\ -3 \\\\ 6 \\end{pmatrix}$.\n  - Covariance matrix: $\\Sigma = \\begin{pmatrix} 4  2  0 \\\\ 2  4  0 \\\\ 0  0  9 \\end{pmatrix}$.\n\n**2. Validation:**\n- **Scientific Grounding:** The problem is firmly grounded in statistical theory and its application to inverse problems, which is a core methodology in remote sensing and other physical sciences. The concepts of Maximum Likelihood Estimation (MLE), Generalized Least Squares (GLS), multivariate normal distribution, and chi-squared goodness-of-fit are standard and rigorously defined.\n- **Well-Posedness:** The problem is well-posed. It asks for a standard derivation followed by a calculation for which all necessary data ($n$, $p$, $r$, $\\Sigma$) are provided. The covariance matrix $\\Sigma$ is symmetric. Its determinant is $\\det(\\Sigma) = (4 \\times 4 - 2 \\times 2) \\times 9 = 12 \\times 9 = 108  0$, and its leading principal minors are $4  0$ and $12  0$, confirming it is positive definite and thus a valid covariance matrix.\n- **Objectivity:** The problem is stated in precise, objective, mathematical language, free from ambiguity or subjective claims.\n- **Conclusion:** The problem is valid as it is scientifically sound, well-posed, objective, and complete.\n\n### Step 2: Derivation and Solution\n\n**Derivation of the Goodness-of-Fit Statistic**\nThe measurement model states that the observation vector $y$ is related to the parameter $x$ by $y = f(x) + \\epsilon$, where the error term $\\epsilon$ follows a multivariate normal distribution with zero mean and covariance matrix $\\Sigma$. This is denoted as $\\epsilon \\sim \\mathcal{N}(0, \\Sigma)$. Consequently, the observation vector $y$ for a given $x$ is also normally distributed: $y \\sim \\mathcal{N}(f(x), \\Sigma)$.\n\nThe probability density function (PDF) for the observation vector $y$ given the parameter $x$ is:\n$$\np(y|x) = \\frac{1}{\\sqrt{(2\\pi)^n \\det(\\Sigma)}} \\exp\\left(-\\frac{1}{2} (y - f(x))^T \\Sigma^{-1} (y - f(x))\\right)\n$$\nThe log-likelihood function, $L(x|y) = \\ln(p(y|x))$, is:\n$$\nL(x|y) = -\\frac{n}{2}\\ln(2\\pi) - \\frac{1}{2}\\ln(\\det(\\Sigma)) - \\frac{1}{2} (y - f(x))^T \\Sigma^{-1} (y - f(x))\n$$\nMaximum Likelihood Estimation (MLE) seeks the value of $x$ that maximizes $L(x|y)$. This is equivalent to minimizing the negative log-likelihood, $-L(x|y)$. Since the first two terms are constant with respect to $x$, this is equivalent to minimizing the quadratic form in the exponent:\n$$\n\\min_{x} \\left\\{ (y - f(x))^T \\Sigma^{-1} (y - f(x)) \\right\\}\n$$\nThis minimization problem is known as Generalized Least Squares (GLS). The quantity being minimized is the sum of squared, normalized residuals, often denoted as the chi-squared ($\\chi^2$) cost function.\n\nLet $\\hat{x}$ be the estimate of $x$ that minimizes this cost function. The residual vector at the solution is $r = y - f(\\hat{x})$. The minimized value of the cost function is:\n$$\n\\chi^2 = r^T \\Sigma^{-1} r\n$$\nThis scalar value is a measure of the total misfit between the model and the data, weighted by the measurement uncertainty. If the model $f(x)$ is correct and the covariance matrix $\\Sigma$ is accurately specified, the statistic $\\chi^2$ follows a chi-squared distribution. The number of degrees of freedom, $\\nu$, for this distribution is the number of observations, $n$, minus the number of parameters being estimated, $p$. Thus, $\\nu = n - p$.\n\nThe goodness-of-fit statistic that averages this total squared error over the available degrees of freedom is the reduced chi-squared statistic, denoted $\\chi^2_\\nu$ or $\\chi^2_\\text{red}$:\n$$\n\\chi^2_\\nu = \\frac{\\chi^2}{\\nu} = \\frac{r^T \\Sigma^{-1} r}{n - p}\n$$\nThis is the required statistic.\n\n**Role of $n$ and $p$ and Interpretation**\n- $n$: The number of observations or data points available for the fit.\n- $p$: The number of parameters in the model that are adjusted to fit the data.\n- The difference, $\\nu = n - p$, represents the degrees of freedom of the fit. It is the number of independent data points remaining to assess the quality of the fit after $p$ parameters have been constrained by the data.\n\nThe expected value of a chi-squared distribution with $\\nu$ degrees of freedom is $E[\\chi^2] = \\nu$. Therefore, the expected value of the reduced chi-squared statistic is:\n$$\nE[\\chi^2_\\nu] = E\\left[\\frac{\\chi^2}{\\nu}\\right] = \\frac{E[\\chi^2]}{\\nu} = \\frac{\\nu}{\\nu} = 1\n$$\n- **Interpretation:** If the model is correct and the error covariance $\\Sigma$ is properly estimated, the value of the goodness-of-fit statistic $\\chi^2_\\nu$ is expected to be close to $1$. A value significantly greater than $1$ suggests a poor model fit or an underestimation of measurement errors. A value significantly less than $1$ may suggest overfitting the data (i.e., the model is too complex for the data's noise level) or an overestimation of measurement errors.\n\n**Computation for the Specific Case**\nWe are given:\n- $n = 3$, $p = 1$. The degrees of freedom are $\\nu = n - p = 3 - 1 = 2$.\n- $r = \\begin{pmatrix} 3 \\\\ -3 \\\\ 6 \\end{pmatrix}$.\n- $\\Sigma = \\begin{pmatrix} 4  2  0 \\\\ 2  4  0 \\\\ 0  0  9 \\end{pmatrix}$.\n\nFirst, we must compute the inverse of the covariance matrix, $\\Sigma^{-1}$. The matrix $\\Sigma$ is block-diagonal. Let the top-left $2 \\times 2$ block be $A = \\begin{pmatrix} 4  2 \\\\ 2  4 \\end{pmatrix}$. Its inverse is:\n$$\nA^{-1} = \\frac{1}{\\det(A)} \\begin{pmatrix} 4  -2 \\\\ -2  4 \\end{pmatrix} = \\frac{1}{4 \\times 4 - 2 \\times 2} \\begin{pmatrix} 4  -2 \\\\ -2  4 \\end{pmatrix} = \\frac{1}{12} \\begin{pmatrix} 4  -2 \\\\ -2  4 \\end{pmatrix} = \\begin{pmatrix} 1/3  -1/6 \\\\ -1/6  1/3 \\end{pmatrix}\n$$\nThe bottom-right block is a scalar $9$, so its inverse is $1/9$. Thus, the inverse of $\\Sigma$ is:\n$$\n\\Sigma^{-1} = \\begin{pmatrix}\n1/3  -1/6  0 \\\\\n-1/6  1/3  0 \\\\\n0  0  1/9\n\\end{pmatrix}\n$$\nNow, we compute the $\\chi^2$ statistic, $\\chi^2 = r^T \\Sigma^{-1} r$:\n$$\n\\chi^2 = \\begin{pmatrix} 3  -3  6 \\end{pmatrix} \\begin{pmatrix}\n1/3  -1/6  0 \\\\\n-1/6  1/3  0 \\\\\n0  0  1/9\n\\end{pmatrix} \\begin{pmatrix} 3 \\\\ -3 \\\\ 6 \\end{pmatrix}\n$$\nFirst, calculate the product $r^T \\Sigma^{-1}$:\n$$\nr^T \\Sigma^{-1} = \\begin{pmatrix} (3)(\\frac{1}{3}) + (-3)(-\\frac{1}{6})  (3)(-\\frac{1}{6}) + (-3)(\\frac{1}{3})  (6)(\\frac{1}{9}) \\end{pmatrix}\n$$\n$$\nr^T \\Sigma^{-1} = \\begin{pmatrix} 1 + \\frac{1}{2}  -\\frac{1}{2} - 1  \\frac{6}{9} \\end{pmatrix} = \\begin{pmatrix} \\frac{3}{2}  -\\frac{3}{2}  \\frac{2}{3} \\end{pmatrix}\n$$\nNext, multiply this result by $r$:\n$$\n\\chi^2 = (r^T \\Sigma^{-1}) r = \\begin{pmatrix} \\frac{3}{2}  -\\frac{3}{2}  \\frac{2}{3} \\end{pmatrix} \\begin{pmatrix} 3 \\\\ -3 \\\\ 6 \\end{pmatrix}\n$$\n$$\n\\chi^2 = \\left(\\frac{3}{2}\\right)(3) + \\left(-\\frac{3}{2}\\right)(-3) + \\left(\\frac{2}{3}\\right)(6) = \\frac{9}{2} + \\frac{9}{2} + \\frac{12}{3} = 9 + 4 = 13\n$$\nFinally, we compute the goodness-of-fit statistic $\\chi^2_\\nu$ by dividing by the degrees of freedom $\\nu = 2$:\n$$\n\\chi^2_\\nu = \\frac{\\chi^2}{\\nu} = \\frac{13}{2} = 6.5\n$$\nThe problem requires the answer to be rounded to $4$ significant figures.\n$$\n\\chi^2_\\nu = 6.500\n$$\nThis value is significantly larger than $1$, suggesting that either the model is a poor fit for the data or the assumed measurement errors are underestimated.",
            "answer": "$$\\boxed{6.500}$$"
        },
        {
            "introduction": "A parameter estimate without an uncertainty quantification is incomplete. The goal of Bayesian inference is not just to find the single most likely parameter vector, but to characterize the entire posterior distribution, which describes all we know about the parameters after observing the data. This exercise  demonstrates the Laplace approximation, a powerful method for approximating the posterior distribution as a Gaussian centered at the Maximum A Posteriori (MAP) estimate. You will compute the posterior covariance matrix, which provides the variances (uncertainties) and covariances (correlations) of the estimated parameters.",
            "id": "3831132",
            "problem": "A multispectral satellite sensor is used to infer two nondimensionalized land-surface parameters, $x \\in \\mathbb{R}^{2}$, representing a normalized leaf biochemistry index and a normalized soil moisture index, from three top-of-canopy reflectance measurements, $y \\in \\mathbb{R}^{3}$. Around the Maximum A Posteriori (MAP) point, the radiative transfer forward model is linearized so that the data misfit is controlled by the Jacobian matrix $J \\in \\mathbb{R}^{3 \\times 2}$ evaluated at the MAP. Assume the measurement noise is zero-mean Gaussian with covariance $\\Sigma \\in \\mathbb{R}^{3 \\times 3}$, and the prior on $x$ is Gaussian with zero mean and covariance $\\Gamma \\in \\mathbb{R}^{2 \\times 2}$. All quantities are nondimensional.\n\nThe matrices are\n$$\nJ \\;=\\; \\begin{pmatrix}\n1  0 \\\\\n0  1 \\\\\n1  1\n\\end{pmatrix}, \n\\qquad\n\\Sigma \\;=\\; \\begin{pmatrix}\n\\frac{1}{4}  0  0 \\\\\n0  \\frac{1}{9}  0 \\\\\n0  0  \\frac{1}{16}\n\\end{pmatrix},\n\\qquad\n\\Gamma \\;=\\; \\begin{pmatrix}\n\\frac{1}{2}  0 \\\\\n0  \\frac{1}{3}\n\\end{pmatrix}.\n$$\n\nUsing the standard Gaussian (Laplace) approximation of the posterior around the Maximum A Posteriori (MAP) estimate arising from a second-order Taylor expansion of the negative log-posterior and the Gauss–Newton approximation to the data misfit Hessian, compute the approximate posterior covariance matrix for $x$ at the MAP. Express the final covariance matrix entries exactly as fractions, with no rounding. Since all quantities are nondimensional, do not include units in your answer. The final answer must be a single matrix expression.",
            "solution": "The Bayesian inverse problem for parameter estimation begins with Bayes’ rule, which states that the posterior density is proportional to the product of the likelihood and the prior. For additive zero-mean Gaussian measurement noise with covariance $\\Sigma$ and a Gaussian prior with mean zero and covariance $\\Gamma$, the negative log-posterior, up to an additive constant independent of $x$, is\n$$\n\\Phi(x) \\;=\\; \\frac{1}{2}\\,\\|y - f(x)\\|_{\\Sigma^{-1}}^{2} \\;+\\; \\frac{1}{2}\\,\\|x\\|_{\\Gamma^{-1}}^{2},\n$$\nwhere $f(x)$ is the forward model, and $\\|v\\|_{A}^{2} \\equiv v^{\\top} A v$. Around the Maximum A Posteriori (MAP) point, one linearizes the forward model as $f(x) \\approx f(x_{\\text{MAP}}) + J (x - x_{\\text{MAP}})$, where $J$ is the Jacobian at the MAP. Under the Gauss–Newton approximation, the Hessian of the negative log-posterior at the MAP is approximated by\n$$\nH_{\\text{GN}} \\;\\approx\\; J^{\\top} \\Sigma^{-1} J \\;+\\; \\Gamma^{-1}.\n$$\nThe Laplace (Gaussian) approximation to the posterior around the MAP is then $\\mathcal{N}(x_{\\text{MAP}}, \\Gamma_{\\text{post}})$ with covariance\n$$\n\\Gamma_{\\text{post}} \\;\\approx\\; H_{\\text{GN}}^{-1} \\;=\\; \\left(J^{\\top} \\Sigma^{-1} J \\;+\\; \\Gamma^{-1}\\right)^{-1}.\n$$\n\nWe now compute $J^{\\top} \\Sigma^{-1} J$ and $\\Gamma^{-1}$ using the provided matrices. First, compute $\\Sigma^{-1}$:\n$$\n\\Sigma \\;=\\; \\operatorname{diag}\\!\\left(\\frac{1}{4},\\, \\frac{1}{9},\\, \\frac{1}{16}\\right)\n\\;\\;\\Longrightarrow\\;\\;\n\\Sigma^{-1} \\;=\\; \\operatorname{diag}\\!\\left(4,\\, 9,\\, 16\\right).\n$$\nWrite the rows of $J$ as\n$$\nr_{1} \\;=\\; \\begin{pmatrix} 1  0 \\end{pmatrix}, \n\\quad\nr_{2} \\;=\\; \\begin{pmatrix} 0  1 \\end{pmatrix},\n\\quad\nr_{3} \\;=\\; \\begin{pmatrix} 1  1 \\end{pmatrix}.\n$$\nThen\n$$\nJ^{\\top} \\Sigma^{-1} J \n\\;=\\; \\sum_{i=1}^{3} w_{i}\\, r_{i}^{\\top} r_{i}\n\\quad\\text{with}\\quad\n(w_{1}, w_{2}, w_{3}) \\;=\\; (4, 9, 16).\n$$\nCompute each term:\n$$\n4\\, r_{1}^{\\top} r_{1} \\;=\\; 4 \\begin{pmatrix} 1  0 \\\\ 0  0 \\end{pmatrix}\n\\;=\\; \\begin{pmatrix} 4  0 \\\\ 0  0 \\end{pmatrix},\n$$\n$$\n9\\, r_{2}^{\\top} r_{2} \\;=\\; 9 \\begin{pmatrix} 0  0 \\\\ 0  1 \\end{pmatrix}\n\\;=\\; \\begin{pmatrix} 0  0 \\\\ 0  9 \\end{pmatrix},\n$$\n$$\n16\\, r_{3}^{\\top} r_{3} \\;=\\; 16 \\begin{pmatrix} 1  1 \\\\ 1  1 \\end{pmatrix}\n\\;=\\; \\begin{pmatrix} 16  16 \\\\ 16  16 \\end{pmatrix}.\n$$\nSumming yields\n$$\nJ^{\\top} \\Sigma^{-1} J \n\\;=\\; \\begin{pmatrix} 4  0 \\\\ 0  0 \\end{pmatrix}\n\\;+\\; \\begin{pmatrix} 0  0 \\\\ 0  9 \\end{pmatrix}\n\\;+\\; \\begin{pmatrix} 16  16 \\\\ 16  16 \\end{pmatrix}\n\\;=\\; \\begin{pmatrix} 20  16 \\\\ 16  25 \\end{pmatrix}.\n$$\n\nNext, compute $\\Gamma^{-1}$. Given\n$$\n\\Gamma \\;=\\; \\begin{pmatrix} \\frac{1}{2}  0 \\\\ 0  \\frac{1}{3} \\end{pmatrix},\n$$\nwe have\n$$\n\\Gamma^{-1} \\;=\\; \\begin{pmatrix} 2  0 \\\\ 0  3 \\end{pmatrix}.\n$$\nTherefore,\n$$\nH_{\\text{GN}} \\;=\\; J^{\\top} \\Sigma^{-1} J \\;+\\; \\Gamma^{-1}\n\\;=\\; \\begin{pmatrix} 20  16 \\\\ 16  25 \\end{pmatrix} \\;+\\; \\begin{pmatrix} 2  0 \\\\ 0  3 \\end{pmatrix}\n\\;=\\; \\begin{pmatrix} 22  16 \\\\ 16  28 \\end{pmatrix}.\n$$\n\nFinally, invert the $2 \\times 2$ matrix to obtain $\\Gamma_{\\text{post}}$:\n$$\n\\Gamma_{\\text{post}} \\;\\approx\\; H_{\\text{GN}}^{-1}\n\\;=\\; \\left(\\begin{pmatrix} 22  16 \\\\ 16  28 \\end{pmatrix}\\right)^{-1}.\n$$\nFor a symmetric $2 \\times 2$ matrix $\\begin{pmatrix} a  b \\\\ b  d \\end{pmatrix}$, the inverse is $\\frac{1}{ad - b^{2}} \\begin{pmatrix} d  -b \\\\ -b  a \\end{pmatrix}$. Here, $a \\,=\\, 22$, $b \\,=\\, 16$, $d \\,=\\, 28$, so\n$$\n\\det(H_{\\text{GN}}) \\;=\\; 22 \\cdot 28 \\;-\\; 16^{2} \\;=\\; 616 \\;-\\; 256 \\;=\\; 360.\n$$\nThus,\n$$\n\\Gamma_{\\text{post}} \\;\\approx\\; \\frac{1}{360} \\begin{pmatrix} 28  -16 \\\\ -16  22 \\end{pmatrix}\n\\;=\\; \\begin{pmatrix} \\frac{28}{360}  -\\frac{16}{360} \\\\ -\\frac{16}{360}  \\frac{22}{360} \\end{pmatrix}\n\\;=\\; \\begin{pmatrix} \\frac{7}{90}  -\\frac{2}{45} \\\\ -\\frac{2}{45}  \\frac{11}{180} \\end{pmatrix}.\n$$\n\nThis is the exact fractional form of the approximate posterior covariance matrix under the Laplace approximation around the Maximum A Posteriori (MAP) estimate.",
            "answer": "$$\\boxed{\\begin{pmatrix} \\frac{7}{90}  -\\frac{2}{45} \\\\ -\\frac{2}{45}  \\frac{11}{180} \\end{pmatrix}}$$"
        }
    ]
}