{
    "hands_on_practices": [
        {
            "introduction": "After running a K-fold cross-validation, we obtain a set of performance scores, one for each fold. Simply averaging these scores provides a point estimate of the model's performance but fails to capture the variability and uncertainty in this estimate. This exercise demonstrates a fundamental statistical procedure: constructing a confidence interval for a cross-validated metric. By treating the fold-wise errors as a sample, you will learn to quantify the uncertainty of your model's expected performance, a critical step for robustly comparing models or reporting results .",
            "id": "3804427",
            "problem": "A remote sensing team is modeling Leaf Area Index (LAI) from multispectral satellite reflectance using a regression model that is evaluated by $K$-fold cross-validation. In each fold $k \\in \\{1,\\dots,K\\}$ the model is trained on the remaining $K-1$ folds and evaluated on the held-out fold $k$, producing a fold-specific Root Mean Squared Error (RMSE), denoted $e_{k}$. The RMSE on fold $k$ is defined by $e_{k} = \\sqrt{\\frac{1}{n_{k}}\\sum_{i=1}^{n_{k}} r_{k,i}^{2}}$, where $r_{k,i}$ is the prediction residual for test observation $i$ in fold $k$ and $n_{k}$ is the number of test observations in fold $k$. Assume that the fold-wise RMSE values $\\{e_{k}\\}_{k=1}^{K}$ are realizations from an unknown distribution with finite variance and that the folds are designed so that the $e_{k}$ are approximately independent and identically distributed.\n\nStarting from the definitions of the sample mean and sample variance for $\\{e_{k}\\}_{k=1}^{K}$, and using the fact that the Student’s $t$-distribution arises when the population variance is unknown and estimated from the sample, derive a $0.95$ confidence interval for the cross-validated mean RMSE. Explicitly show how the $t$-distribution with $K-1$ degrees of freedom enters the derivation and state the assumptions under which the interval is valid, with particular attention to the independence of folds in a spatially structured remote sensing context.\n\nThen, compute this interval for $K = 5$ with the following fold-wise RMSE values (LAI is dimensionless, so express your answer as decimal fractions with no unit): $e_{1} = 0.82$, $e_{2} = 0.79$, $e_{3} = 0.85$, $e_{4} = 0.88$, $e_{5} = 0.81$. Use the two-sided $t$-critical value $t_{0.975,4} = 2.776$. Round your final interval endpoints to four significant figures.",
            "solution": "The problem as stated is scientifically grounded, well-posed, objective, and internally consistent. It presents a standard statistical task within a realistic remote sensing context and provides all necessary data and definitions for a unique solution. Therefore, the problem is deemed valid. We proceed with the solution.\n\nThe objective is to derive and compute a $0.95$ confidence interval for the true mean cross-validated Root Mean Squared Error (RMSE), denoted $\\mu_e$, based on a sample of $K$ fold-wise RMSE values, $\\{e_k\\}_{k=1}^{K}$.\n\nFirst, we derive the general formula for the confidence interval. Let $\\{e_1, e_2, \\dots, e_K\\}$ be a sample of $K$ independent and identically distributed (i.i.d.) random variables representing the fold-wise RMSE values, drawn from a distribution with mean $\\mu_e$ and finite variance $\\sigma_e^2$.\n\nThe sample mean, $\\bar{e}$, is the point estimator for $\\mu_e$:\n$$\n\\bar{e} = \\frac{1}{K} \\sum_{k=1}^{K} e_k\n$$\nSince the population variance $\\sigma_e^2$ is unknown, we must estimate it from the sample. The unbiased estimator for the population variance is the sample variance, $s_e^2$:\n$$\ns_e^2 = \\frac{1}{K-1} \\sum_{k=1}^{K} (e_k - \\bar{e})^2\n$$\nThe sample standard deviation is $s_e = \\sqrt{s_e^2}$.\n\nThe standard error of the mean (SEM), which is the standard deviation of the sampling distribution of $\\bar{e}$, is estimated by $s_{\\bar{e}}$:\n$$\ns_{\\bar{e}} = \\frac{s_e}{\\sqrt{K}}\n$$\nWhen the population variance is unknown and estimated from a sample of size $K$ drawn from a normal distribution, the standardized test statistic\n$$\nT = \\frac{\\bar{e} - \\mu_e}{s_{\\bar{e}}} = \\frac{\\bar{e} - \\mu_e}{s_e / \\sqrt{K}}\n$$\nfollows a Student's $t$-distribution with $\\nu = K-1$ degrees of freedom. This is the key step where the $t$-distribution enters the derivation.\n\nTo construct a $1-\\alpha$ confidence interval, we find the critical values $\\pm t_{\\alpha/2, K-1}$ from the $t$-distribution that bound the central $1-\\alpha$ area of the distribution. The probability statement is:\n$$\nP\\left(-t_{\\alpha/2, K-1} \\le \\frac{\\bar{e} - \\mu_e}{s_e / \\sqrt{K}} \\le t_{\\alpha/2, K-1}\\right) = 1-\\alpha\n$$\nWe rearrange the inequalities to isolate the population mean $\\mu_e$:\n$$\n-t_{\\alpha/2, K-1} \\frac{s_e}{\\sqrt{K}} \\le \\bar{e} - \\mu_e \\le t_{\\alpha/2, K-1} \\frac{s_e}{\\sqrt{K}}\n$$\n$$\n-\\bar{e} - t_{\\alpha/2, K-1} \\frac{s_e}{\\sqrt{K}} \\le -\\mu_e \\le -\\bar{e} + t_{\\alpha/2, K-1} \\frac{s_e}{\\sqrt{K}}\n$$\nMultiplying by $-1$ reverses the inequalities:\n$$\n\\bar{e} - t_{\\alpha/2, K-1} \\frac{s_e}{\\sqrt{K}} \\le \\mu_e \\le \\bar{e} + t_{\\alpha/2, K-1} \\frac{s_e}{\\sqrt{K}}\n$$\nThus, the $1-\\alpha$ confidence interval for $\\mu_e$ is given by:\n$$\n\\left[ \\bar{e} - t_{\\alpha/2, K-1} \\frac{s_e}{\\sqrt{K}}, \\quad \\bar{e} + t_{\\alpha/2, K-1} \\frac{s_e}{\\sqrt{K}} \\right]\n$$\nThis can be written compactly as $\\bar{e} \\pm t_{\\alpha/2, K-1} \\cdot s_{\\bar{e}}$.\n\nThe validity of this interval rests on several key assumptions:\n1.  **Normality**: The derivation assumes the population of fold-wise RMSEs, $\\{e_k\\}$, from which the sample is drawn is normally distributed. For a small sample size like $K=5$, this assumption is important. The $t$-distribution is known to be robust to moderate departures from normality, but strong skewness or heavy tails in the distribution of $e_k$ would compromise the accuracy of the confidence level. For larger $K$, the Central Limit Theorem would ensure that the sampling distribution of $\\bar{e}$ is approximately normal, even if the underlying distribution of $e_k$ is not.\n2.  **Independence**: The derivation assumes that the fold-wise errors $e_k$ are independent. In a remote sensing context involving spatially structured data, this assumption is often violated due to spatial autocorrelation. If data points are geographically close, their prediction residuals may be correlated. If the folds are created without regard to this spatial structure (e.g., simple random sampling of pixels), a test set for one fold may contain points that are spatially close to points in a training set for another fold, which can lead to overly optimistic (narrower) error estimates. The problem states that \"the folds are designed so that the $e_{k}$ are approximately independent,\" which implies a method like spatial blocking or buffered cross-validation was used. Such methods create folds from spatially contiguous blocks of data, ensuring that the training and testing sets in any given fold iteration are spatially separate, thereby reducing the dependency between folds and making the resulting $e_k$ values more independent.\n3.  **Identical Distribution**: The $e_k$ values are assumed to be identically distributed, meaning each fold is an equally representative sample of the overall data landscape. This is typically achieved through stratified sampling on the target variable (LAI) or on geographic strata to ensure that each fold has a similar distribution of environmental conditions.\n\nNow, we compute the $0.95$ confidence interval for the given data.\nThe number of folds is $K=5$. The fold-wise RMSE values are $\\{e_k\\} = \\{0.82, 0.79, 0.85, 0.88, 0.81\\}$.\n\n1.  Calculate the sample mean $\\bar{e}$:\n    $$\n    \\bar{e} = \\frac{1}{5} (0.82 + 0.79 + 0.85 + 0.88 + 0.81) = \\frac{4.15}{5} = 0.83\n    $$\n\n2.  Calculate the sample variance $s_e^2$:\n    $$\n    \\sum_{k=1}^{5} (e_k - \\bar{e})^2 = (0.82-0.83)^2 + (0.79-0.83)^2 + (0.85-0.83)^2 + (0.88-0.83)^2 + (0.81-0.83)^2\n    $$\n    $$\n    = (-0.01)^2 + (-0.04)^2 + (0.02)^2 + (0.05)^2 + (-0.02)^2\n    $$\n    $$\n    = 0.0001 + 0.0016 + 0.0004 + 0.0025 + 0.0004 = 0.0050\n    $$\n    $$\n    s_e^2 = \\frac{1}{K-1} \\sum_{k=1}^{K} (e_k - \\bar{e})^2 = \\frac{0.0050}{5-1} = \\frac{0.0050}{4} = 0.00125\n    $$\n\n3.  Calculate the sample standard deviation $s_e$:\n    $$\n    s_e = \\sqrt{0.00125}\n    $$\n\n4.  Calculate the estimated standard error of the mean $s_{\\bar{e}}$:\n    $$\n    s_{\\bar{e}} = \\frac{s_e}{\\sqrt{K}} = \\frac{\\sqrt{0.00125}}{\\sqrt{5}} = \\sqrt{\\frac{0.00125}{5}} = \\sqrt{0.00025}\n    $$\n    $$\n    s_{\\bar{e}} \\approx 0.015811388...\n    $$\n\n5.  Determine the critical value. For a $0.95$ confidence interval, $\\alpha = 0.05$, so we need the critical value for a two-tailed probability of $\\alpha/2 = 0.025$. The degrees of freedom are $\\nu = K-1 = 4$. The problem provides this value as $t_{0.975,4} = 2.776$.\n\n6.  Calculate the margin of error (ME):\n    $$\n    \\text{ME} = t_{\\alpha/2, K-1} \\cdot s_{\\bar{e}} = 2.776 \\times \\sqrt{0.00025} \\approx 2.776 \\times 0.015811388... \\approx 0.04389439\n    $$\n\n7.  Construct the confidence interval, $\\bar{e} \\pm \\text{ME}$:\n    *   Lower bound: $0.83 - 0.04389439 = 0.78610561$\n    *   Upper bound: $0.83 + 0.04389439 = 0.87389439$\n\nFinally, we round the interval endpoints to four significant figures as requested.\n*   Lower bound rounded: $0.7861$\n*   Upper bound rounded: $0.8739$\n\nThe $0.95$ confidence interval for the cross-validated mean RMSE is $[0.7861, 0.8739]$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 0.7861 & 0.8739 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "In remote sensing and environmental modeling, data points are rarely independent due to spatial autocorrelation. Standard K-fold cross-validation violates the independence assumption, leading to overly optimistic performance estimates. A common remedy is to enforce a spatial buffer between training and validation sets. This practice challenges you to think both algorithmically and analytically about implementing a buffered cross-validation scheme, quantifying the critical trade-off between reducing spatial leakage and retaining statistical power .",
            "id": "3804495",
            "problem": "A remote sensing team is building an environmental model to predict topsoil moisture from satellite features over a contiguous study region of area $A$ (square kilometers). Ground-truth pixels were selected by simple random spatial sampling at a uniform areal density $\\rho$ (pixels per square kilometer), with $A \\rho \\gg 1$ so that stochastic approximations are justified. To mitigate spatial autocorrelation leakage in model validation, the team will use $K$-fold cross-validation with a spatial buffer: in each fold, any training pixel lying within a geodesic distance $d$ (kilometers) of any validation pixel is excluded from that fold’s training set. Validation pixels are always retained as validation.\n\nTask 1 (algorithm design): Propose a principled algorithm that, for each fold, enforces the minimum buffer distance $d$ between training and validation pixels while preserving the fold-wise validation set and dropping only those training pixels that violate the buffer. The algorithm must be applicable on the sphere or a projected plane and must specify how to implement the exclusion efficiently.\n\nTask 2 (derivation): Assume the sampled pixel locations are a realization of a homogeneous Poisson point process of intensity $\\rho$ over area $A$, and that in any fixed fold the assignment of pixels to training or validation is obtained by independent thinning into two independent point processes with probabilities $(K-1)/K$ for training and $1/K$ for validation. Ignore edge effects by assuming $A \\gg \\pi d^{2}$. Under these assumptions and in a single fold:\n- Validation pixels are all usable;\n- A training pixel is usable only if it is at geodesic distance strictly greater than $d$ from every validation pixel.\n\nDerive the expected total number of usable pixels per fold, $N_{\\mathrm{use}}(d)$, as a closed-form analytic function of $A$, $\\rho$, $K$, and $d$. Express your final answer as a single closed-form expression in terms of $A$, $\\rho$, $K$, and $d$. Do not include units in your final expression.",
            "solution": "The problem statement is assessed to be valid. It is scientifically grounded, well-posed, objective, and contains sufficient information to formulate a principled algorithm and derive the requested analytical expression. The problem is a standard, non-trivial application of spatial statistics and algorithmic design relevant to environmental modeling and machine learning.\n\n### Task 1: Algorithm for Buffered Spatial Cross-Validation\n\nThe objective is to implement $K$-fold cross-validation where, for each fold, training points within a geodesic distance $d$ of any validation point are excluded.\n\nAn efficient algorithm to achieve this is as follows:\n\nLet $S$ be the set of all $N$ sampled pixel locations, where the total number of pixels is approximately $N = A \\rho$.\n\n1.  **Partition Data**: Randomly partition the set $S$ into $K$ disjoint subsets of roughly equal size, $S_1, S_2, \\ldots, S_K$. These subsets represent the folds.\n\n2.  **Iterate Through Folds**: For each fold $k \\in \\{1, 2, \\ldots, K\\}$:\n    a.  **Define Initial Sets**:\n        i.  The validation set is $V_k = S_k$.\n        ii. The initial training set is $T'_k = S \\setminus V_k = \\bigcup_{j \\neq k} S_j$.\n\n    b.  **Build Spatial Index**: To enable efficient distance queries, construct a spatial index (e.g., a k-d tree, R-tree, or quadtree for planar data; or a ball tree for spherical data) on the set of validation points $V_k$. Let this index be denoted $\\mathcal{I}_{V_k}$. Building this index is a one-time cost per fold, typically taking $O(|V_k| \\log |V_k|)$ time.\n\n    c.  **Filter Training Set**: Initialize the final, usable training set for the current fold, $T_k$, as an empty set. Iterate through each point $p_t \\in T'_k$:\n        i.  Perform a **fixed-radius near-neighbor search** on the spatial index $\\mathcal{I}_{V_k}$. The query seeks to determine if there exists any point $p_v \\in V_k$ such that the geodesic distance $D(p_t, p_v) \\le d$.\n        ii. If the search query returns an empty set (i.e., no validation point is found within the buffer distance $d$), then the point $p_t$ is a usable training point. Add $p_t$ to the final training set $T_k$.\n        iii. If the search returns one or more neighbors, $p_t$ is discarded for this fold.\n\n    d.  **Model Training and Validation**: The model for fold $k$ is trained using the data from the filtered set $T_k$ and evaluated on the set $V_k$.\n\nThis algorithm ensures that the exclusion condition is met precisely while being computationally efficient. A naive search would compare every training point to every validation point, resulting in a complexity of $O(|T'_k| \\cdot |V_k|)$ per fold. By using a spatial index, the search for each training point becomes significantly faster, with an average time complexity of $O(\\log |V_k|)$, leading to an overall filtering complexity of approximately $O(|T'_k| \\log |V_k|)$ per fold. The choice of distance metric (e.g., Euclidean for a projected plane or Haversine for a sphere) is an interchangeable component of the near-neighbor search.\n\n### Task 2: Derivation of Expected Number of Usable Pixels\n\nThe problem models the pixel locations as a realization of a homogeneous Poisson point process (PPP), $\\Phi$, with intensity $\\rho$ over a region of area $A$. The total expected number of pixels is $E[|\\Phi|] = \\rho A$.\n\nIn any given fold of the $K$-fold cross-validation, the set of all pixels $\\Phi$ is partitioned into a validation set and a training set by independent thinning.\n- A point belongs to the validation process, $\\Phi_{\\text{val}}$, with probability $p_{\\text{val}} = 1/K$.\n- A point belongs to the training process, $\\Phi_{\\text{train}}$, with probability $p_{\\text{train}} = (K-1)/K$.\n\nThis results in two independent homogeneous PPPs:\n- The validation process $\\Phi_{\\text{val}}$ has an intensity of $\\rho_{\\text{val}} = \\rho \\cdot p_{\\text{val}} = \\rho/K$.\n- The training process $\\Phi_{\\text{train}}$ has an intensity of $\\rho_{\\text{train}} = \\rho \\cdot p_{\\text{train}} = \\rho (K-1)/K$.\n\nThe total number of usable pixels per fold, $N_{\\text{use}}(d)$, is the sum of the number of usable validation pixels, $N_{\\text{usable\\_val}}$, and the number of usable training pixels, $N_{\\text{usable\\_train}}$. By linearity of expectation, the expected total is:\n$$E[N_{\\text{use}}(d)] = E[N_{\\text{usable\\_val}}] + E[N_{\\text{usable\\_train}}]$$\n\n**1. Expected Number of Usable Validation Pixels**\n\nThe problem states that all validation pixels are usable. Therefore, the number of usable validation pixels is simply the total number of validation pixels, $|\\Phi_{\\text{val}}|$. The expected number of points in a homogeneous PPP with intensity $\\lambda$ over an area $A$ is $\\lambda A$.\n$$E[N_{\\text{usable\\_val}}] = E[|\\Phi_{\\text{val}}|] = \\rho_{\\text{val}} A = \\frac{\\rho A}{K}$$\n\n**2. Expected Number of Usable Training Pixels**\n\nA training pixel at location $x$ is usable if and only if it is at a geodesic distance strictly greater than $d$ from *every* validation pixel. This means the open disk of radius $d$ centered at $x$, denoted $B(x, d)$, must contain zero points from the validation process $\\Phi_{\\text{val}}$. The area of this disk is $\\pi d^2$.\n\nLet's find the probability that a specific training point is usable. Due to the properties of a PPP (specifically, Slivnyak's theorem), conditioning on the existence of a training point at location $x$ does not alter the distribution of the independent validation process $\\Phi_{\\text{val}}$. Therefore, the probability that a training point at $x$ is usable is equal to the void probability of the validation process in the disk $B(x, d)$.\n\nThe number of points from $\\Phi_{\\text{val}}$ that fall within $B(x, d)$, let's call it $N_{\\text{val}}(B(x,d))$, follows a Poisson distribution with mean $\\lambda = \\rho_{\\text{val}} \\cdot \\text{Area}(B(x,d)) = (\\rho/K) \\pi d^2$. The probability of this count being zero is:\n$$P(N_{\\text{val}}(B(x,d)) = 0) = \\frac{\\lambda^0 e^{-\\lambda}}{0!} = e^{-\\lambda} = \\exp\\left(-\\frac{\\rho \\pi d^2}{K}\\right)$$\nThis probability, let's call it $p_{\\text{usable}}$, is constant for any location $x$ since we ignore edge effects (as per the assumption $A \\gg \\pi d^2$).\n\nThe process of usable training pixels, $\\Phi_{\\text{usable\\_train}}$, is a thinned version of the original training process $\\Phi_{\\text{train}}$. Each point in $\\Phi_{\\text{train}}$ is kept with probability $p_{\\text{usable}}$. The resulting process is also a homogeneous PPP with intensity:\n$$\\rho_{\\text{usable\\_train}} = \\rho_{\\text{train}} \\cdot p_{\\text{usable}} = \\left(\\rho \\frac{K-1}{K}\\right) \\exp\\left(-\\frac{\\rho \\pi d^2}{K}\\right)$$\nThe expected number of usable training pixels is the integral of this intensity over the area $A$:\n$$E[N_{\\text{usable\\_train}}] = \\int_A \\rho_{\\text{usable\\_train}} \\, da = A \\cdot \\rho_{\\text{usable\\_train}}$$\n$$E[N_{\\text{usable\\_train}}] = A \\rho \\frac{K-1}{K} \\exp\\left(-\\frac{\\rho \\pi d^2}{K}\\right)$$\n\n**3. Total Expected Number of Usable Pixels**\n\nCombining the expected numbers for validation and training pixels:\n$$E[N_{\\text{use}}(d)] = E[N_{\\text{usable\\_val}}] + E[N_{\\text{usable\\_train}}]$$\n$$E[N_{\\text{use}}(d)] = \\frac{\\rho A}{K} + A \\rho \\frac{K-1}{K} \\exp\\left(-\\frac{\\rho \\pi d^2}{K}\\right)$$\nFactoring out the common term $\\frac{A \\rho}{K}$:\n$$N_{\\text{use}}(d) = \\frac{A \\rho}{K} \\left(1 + (K-1) \\exp\\left(-\\frac{\\pi \\rho d^2}{K}\\right)\\right)$$\nThis is the final closed-form expression for the expected total number of usable pixels per fold.",
            "answer": "$$\\boxed{\\frac{A \\rho}{K} \\left(1 + (K-1) \\exp\\left(-\\frac{\\pi \\rho d^2}{K}\\right)\\right)}$$"
        },
        {
            "introduction": "Information leakage can occur in subtle ways that go beyond the direct overlap of training and validation data points. One of the most common and insidious forms of leakage arises during preprocessing, such as when feature normalization parameters are calculated using the entire dataset before splitting it for validation. This simulation-based exercise makes the abstract concept of preprocessing leakage concrete by having you quantify its impact on model performance. By comparing a correct workflow with a \"leaky\" one, you will gain a practical understanding of why preprocessing steps must be treated as part of the model and fitted only on training data to obtain a valid performance estimate .",
            "id": "3804423",
            "problem": "Consider a binary classification task in remote sensing where each pixel is represented by a multivariate feature vector derived from radiance bands, and the goal is to detect a target class (for example, a particular land cover type) from background. You will study the impact of information leakage on holdout validation performance, focusing on Area Under the Receiver Operating Characteristic Curve (ROC AUC). Leakage occurs here when validation pixels share preprocessing parameters (feature normalization statistics) with training pixels, violating the independence assumption between training and validation sets.\n\nStart from the following fundamental base:\n\n- The Area Under the Receiver Operating Characteristic Curve (ROC AUC) is defined as the probability that a randomly chosen score for the positive class exceeds a randomly chosen score for the negative class. Formally, if $S^{+}$ and $S^{-}$ are the random classifier scores for a positive and a negative pixel respectively, then the ROC AUC is\n$$\\mathrm{AUC} = \\mathbb{P}\\left(S^{+} > S^{-}\\right) + \\frac{1}{2}\\mathbb{P}\\left(S^{+} = S^{-}\\right).$$\n- In a holdout strategy, the training and validation datasets must be statistically independent conditional on the data-generation process. Any preprocessing parameter (such as mean and standard deviation used for normalization) estimated using validation data and subsequently applied during training or scoring introduces information leakage, which can bias the estimated performance upward.\n\nYou will simulate two scenes: a training scene and a validation scene. Pixels are generated from a parametric Gaussian model with class-conditional means and a shared covariance per scene. Let the feature dimension be $d = 2$. For class label $Y \\in \\{0,1\\}$, define the raw feature distribution in the training scene as\n$$X \\mid (Y=1,\\ \\mathrm{train}) \\sim \\mathcal{N}\\left(\\boldsymbol{\\mu}_{1} + \\boldsymbol{b}_{\\mathrm{train}},\\ \\boldsymbol{\\Sigma}_{\\mathrm{train}}\\right),\\quad X \\mid (Y=0,\\ \\mathrm{train}) \\sim \\mathcal{N}\\left(\\boldsymbol{\\mu}_{0} + \\boldsymbol{b}_{\\mathrm{train}},\\ \\boldsymbol{\\Sigma}_{\\mathrm{train}}\\right),$$\nand in the validation scene as\n$$X \\mid (Y=1,\\ \\mathrm{val}) \\sim \\mathcal{N}\\left(\\boldsymbol{\\mu}_{1} + \\boldsymbol{b}_{\\mathrm{val}},\\ \\boldsymbol{\\Sigma}_{\\mathrm{val}}\\right),\\quad X \\mid (Y=0,\\ \\mathrm{val}) \\sim \\mathcal{N}\\left(\\boldsymbol{\\mu}_{0} + \\boldsymbol{b}_{\\mathrm{val}},\\ \\boldsymbol{\\Sigma}_{\\mathrm{val}}\\right).$$\nHere $\\boldsymbol{\\mu}_{1}, \\boldsymbol{\\mu}_{0} \\in \\mathbb{R}^{2}$ are class means, $\\boldsymbol{b}_{\\mathrm{train}}, \\boldsymbol{b}_{\\mathrm{val}} \\in \\mathbb{R}^{2}$ are scene-specific offsets, and $\\boldsymbol{\\Sigma}_{\\mathrm{train}}, \\boldsymbol{\\Sigma}_{\\mathrm{val}} \\in \\mathbb{R}^{2\\times 2}$ are scene-specific covariance matrices with unit variances and correlation $\\rho_{\\mathrm{train}}$ and $\\rho_{\\mathrm{val}}$, respectively. Let the per-class prevalence (fraction of positive pixels) in training be $\\pi_{\\mathrm{train}}$ and in validation be $\\pi_{\\mathrm{val}}$.\n\nPreprocessing consists of per-feature $z$-score normalization $Z = (X - \\boldsymbol{m}) \\oslash \\boldsymbol{s}$, where $\\oslash$ denotes element-wise division, and $\\boldsymbol{m}, \\boldsymbol{s} \\in \\mathbb{R}^{2}$ are the mean and standard deviation vectors estimated from a reference pool. Two strategies are contrasted:\n- No leakage: $\\boldsymbol{m}, \\boldsymbol{s}$ are estimated using only the training pixels, and both training and validation are normalized with these training-only parameters.\n- Leakage: $\\boldsymbol{m}, \\boldsymbol{s}$ are estimated using the union of training and validation pixels (shared parameters), and both sets are normalized with these shared parameters.\n\nA classifier is trained using Linear Discriminant Analysis (LDA) on the normalized training data, under the equal class-covariance assumption. The LDA scoring function for a pixel with normalized feature vector $\\boldsymbol{z}$ is $S(\\boldsymbol{z}) = \\boldsymbol{w}^{\\top}\\boldsymbol{z}$, where\n$$\\boldsymbol{w} = \\boldsymbol{\\Sigma}_{\\text{pooled}}^{-1}\\left(\\boldsymbol{\\hat{\\mu}}_{1} - \\boldsymbol{\\hat{\\mu}}_{0}\\right),$$\nwith $\\boldsymbol{\\hat{\\mu}}_{1}, \\boldsymbol{\\hat{\\mu}}_{0}$ the empirical class means in the normalized training set and $\\boldsymbol{\\Sigma}_{\\text{pooled}}$ the pooled covariance matrix estimated from the normalized training set. The intercept may be omitted because ROC AUC depends only on the ranking of scores.\n\nFor each parameter set, perform Monte Carlo simulation with $R$ independent replicates. In each replicate, generate $n_{\\mathrm{train}}$ training pixels and $n_{\\mathrm{val}}$ validation pixels according to the model, apply preprocessing under both strategies, train LDA on the training data, compute ROC AUC on the validation data, and record the difference between the leakage AUC and the no-leakage AUC. The expected upward bias is the empirical mean of these differences over $R$ replicates.\n\nYour task is to implement a program that, for the following test suite, outputs the expected upward bias in ROC AUC for each case:\n\n- Case $1$ (moderate domain shift, happy path): $d=2$, $\\boldsymbol{\\mu}_{1} = [0.5, 0.5]$, $\\boldsymbol{\\mu}_{0} = [-0.5, -0.5]$, $\\rho_{\\mathrm{train}} = 0.3$, $\\rho_{\\mathrm{val}} = 0.3$, $\\boldsymbol{b}_{\\mathrm{train}} = [0.0, 0.0]$, $\\boldsymbol{b}_{\\mathrm{val}} = [1.0, 1.0]$, $\\pi_{\\mathrm{train}} = 0.5$, $\\pi_{\\mathrm{val}} = 0.5$, $n_{\\mathrm{train}} = 600$, $n_{\\mathrm{val}} = 600$, $R = 200$.\n- Case $2$ (no domain shift, boundary condition): same as Case $1$ but with $\\boldsymbol{b}_{\\mathrm{val}} = [0.0, 0.0]$ and $\\rho_{\\mathrm{val}} = 0.3$. Expect the bias to be near $0$.\n- Case $3$ (covariance shift edge case): same as Case $1$ but with $\\rho_{\\mathrm{train}} = 0.0$, $\\rho_{\\mathrm{val}} = 0.8$, $n_{\\mathrm{train}} = 400$, $n_{\\mathrm{val}} = 400$, $R = 200$.\n- Case $4$ (prevalence shift edge case): same as Case $1$ but with $\\pi_{\\mathrm{train}} = 0.3$, $\\pi_{\\mathrm{val}} = 0.7$, $n_{\\mathrm{train}} = 600$, $n_{\\mathrm{val}} = 600$, $R = 200$.\n\nAll random sampling must be reproducible with a fixed seed. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[r_{1},r_{2},r_{3},r_{4}]$). Each $r_{i}$ must be a floating point number representing the expected upward bias (leakage AUC minus no-leakage AUC) for the corresponding case, rounded to exactly six decimal places. No physical units or angles are involved in this problem, and any proportions or prevalences must be expressed as decimals.\n\nYour implementation must:\n- Construct $\\boldsymbol{\\Sigma}_{\\mathrm{train}}$ and $\\boldsymbol{\\Sigma}_{\\mathrm{val}}$ as $2\\times 2$ covariance matrices with unit variances and specified correlation $\\rho$:\n$$\\boldsymbol{\\Sigma}(\\rho) = \\begin{bmatrix}1 & \\rho \\\\ \\rho & 1\\end{bmatrix}.$$\n- Perform per-feature normalization by subtracting the empirical mean $\\boldsymbol{m}$ and dividing by the empirical standard deviation $\\boldsymbol{s}$ computed on the designated reference pool, with a small stabilization constant added to each component of $\\boldsymbol{s}$ to avoid division by zero.\n- Train Linear Discriminant Analysis (LDA) from the normalized training data using the pooled covariance estimated from training only.\n- Compute ROC AUC on the normalized validation data by its definition $\\mathrm{AUC} = \\mathbb{P}\\left(S^{+} > S^{-}\\right) + \\frac{1}{2}\\mathbb{P}\\left(S^{+} = S^{-}\\right)$ via counting comparisons through efficient search on sorted negative scores.\n\nYour final output must be a single line exactly in the form $[r_{1},r_{2},r_{3},r_{4}]$ with each $r_{i}$ rounded to six decimal places.",
            "solution": "The problem requires an analysis of information leakage in a holdout validation scheme for a binary classification task, a common scenario in remote sensing. We are asked to quantify the optimistic bias in the Area Under the ROC Curve (AUC) that arises when feature normalization statistics are computed from a pool of data containing both training and validation samples. This violates the principle of independence between the sets used for model training and model evaluation. The analysis will be conducted via a Monte Carlo simulation.\n\nThe overall methodological approach is as follows:\n1.  For each of the four specified cases, we will execute $R$ independent simulation replicates.\n2.  In each replicate, we generate a training dataset of size $n_{\\mathrm{train}}$ and a validation dataset of size $n_{\\mathrm{val}}$. The data points are drawn from class-conditional multivariate Gaussian distributions, parameterized as specified for each scene (training and validation).\n3.  We then simulate two distinct validation workflows:\n    a. **No-Leakage Workflow**: Feature normalization parameters (mean $\\boldsymbol{m}$ and standard deviation $\\boldsymbol{s}$) are estimated exclusively from the training data. These parameters are then used to normalize both the training and validation data.\n    b. **Leakage Workflow**: The normalization parameters are estimated from the combined set of all training and validation data. These shared parameters are then used to normalize both datasets.\n4.  For each workflow, a Linear Discriminant Analysis (LDA) classifier is trained on the respective normalized training data.\n5.  The trained classifier is then applied to the corresponding normalized validation data to obtain classification scores. The performance is quantified by computing the ROC AUC.\n6.  The bias for a single replicate is the difference: $\\mathrm{AUC}_{\\text{leakage}} - \\mathrm{AUC}_{\\text{no-leakage}}$.\n7.  The final result for each case is the expected upward bias, estimated by averaging these differences over all $R$ replicates.\n\nLet's formalize the key computational steps.\n\n**1. Data Generation**\nFor a given scene (training or validation), we generate $n$ samples. The number of positive samples (class $Y=1$) is $n_1 = \\text{round}(n \\cdot \\pi)$, and the number of negative samples (class $Y=0$) is $n_0 = n - n_1$.\nThe feature vectors for each class are drawn from a $d=2$ dimensional normal distribution $\\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$.\n-   Positive class features: $X_1 \\sim \\mathcal{N}(\\boldsymbol{\\mu}_1 + \\boldsymbol{b}, \\boldsymbol{\\Sigma})$\n-   Negative class features: $X_0 \\sim \\mathcal{N}(\\boldsymbol{\\mu}_0 + \\boldsymbol{b}, \\boldsymbol{\\Sigma})$\nThe covariance matrix $\\boldsymbol{\\Sigma}$ is constructed based on the specified correlation $\\rho$ as:\n$$\n\\boldsymbol{\\Sigma}(\\rho) = \\begin{bmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{bmatrix}\n$$\nThis generation process is performed independently for the training and validation sets, using their respective parameters ($\\boldsymbol{b}_{\\mathrm{train}}, \\boldsymbol{\\Sigma}_{\\mathrm{train}}, n_{\\mathrm{train}}, \\pi_{\\mathrm{train}}$ and $\\boldsymbol{b}_{\\mathrm{val}}, \\boldsymbol{\\Sigma}_{\\mathrm{val}}, n_{\\mathrm{val}}, \\pi_{\\mathrm{val}}$).\n\n**2. Feature Normalization**\nGiven a set of raw feature vectors $X$ and a reference pool of vectors $X_{\\text{ref}}$, we apply per-feature $z$-score normalization. The mean vector $\\boldsymbol{m}$ and standard deviation vector $\\boldsymbol{s}$ are computed from $X_{\\text{ref}}$:\n$$\n\\boldsymbol{m} = \\mathbb{E}[X_{\\text{ref}}] \\quad \\quad \\boldsymbol{s} = \\sqrt{\\mathrm{Var}(X_{\\text{ref}})}\n$$\nThe normalized features $\\boldsymbol{z}$ for any sample $\\boldsymbol{x}$ from $X$ are then:\n$$\n\\boldsymbol{z} = (\\boldsymbol{x} - \\boldsymbol{m}) \\oslash (\\boldsymbol{s} + \\epsilon)\n$$\nwhere $\\oslash$ denotes element-wise division and $\\epsilon$ is a small constant (e.g., $10^{-8}$) to prevent division by zero.\n-   In the **no-leakage** case, $X_{\\text{ref}} = X_{\\mathrm{train}}$.\n-   In the **leakage** case, $X_{\\text{ref}} = X_{\\mathrm{train}} \\cup X_{\\mathrm{val}}$.\n\n**3. Linear Discriminant Analysis (LDA) Training**\nThe LDA classifier learns a projection vector $\\boldsymbol{w}$ that maximizes the separability between classes. Given the normalized training data $Z_{\\mathrm{train}}$ and corresponding labels $y_{\\mathrm{train}}$, we compute $\\boldsymbol{w}$ as:\n$$\n\\boldsymbol{w} = \\boldsymbol{\\hat{\\Sigma}}_{\\text{pooled}}^{-1} (\\boldsymbol{\\hat{\\mu}}_1 - \\boldsymbol{\\hat{\\mu}}_0)\n$$\nHere, $\\boldsymbol{\\hat{\\mu}}_1$ and $\\boldsymbol{\\hat{\\mu}}_0$ are the empirical means of the normalized positive and negative samples in the training set, respectively. The pooled covariance matrix, $\\boldsymbol{\\hat{\\Sigma}}_{\\text{pooled}}$, is a weighted average of the individual class covariance matrices:\n$$\n\\boldsymbol{\\hat{\\Sigma}}_{\\text{pooled}} = \\frac{(n_1 - 1)\\boldsymbol{\\hat{\\Sigma}}_1 + (n_0 - 1)\\boldsymbol{\\hat{\\Sigma}}_0}{n_1 + n_0 - 2}\n$$\nwhere $n_1, n_0$ are the number of positive and negative samples in the training set, and $\\boldsymbol{\\hat{\\Sigma}}_1, \\boldsymbol{\\hat{\\Sigma}}_0$ are their respective sample covariance matrices.\n\n**4. ROC AUC Calculation**\nThe classification score for a normalized sample $\\boldsymbol{z}$ is $S(\\boldsymbol{z}) = \\boldsymbol{w}^{\\top}\\boldsymbol{z}$. To compute the AUC on the validation set, we first separate the scores into those from positive samples, $\\{S_i^+\\}$, and those from negative samples, $\\{S_j^-\\}$. The AUC is estimated from its probabilistic definition:\n$$\n\\mathrm{AUC} = \\frac{1}{N^+ N^-} \\left( \\sum_{i=1}^{N^+} \\sum_{j=1}^{N^-} \\mathbb{I}(S_i^+ > S_j^-) + \\frac{1}{2} \\sum_{i=1}^{N^+} \\sum_{j=1}^{N^-} \\mathbb{I}(S_i^+ = S_j^-) \\right)\n$$\nwhere $N^+$ and $N^-$ are the number of positive and negative samples in the validation set, and $\\mathbb{I}(\\cdot)$ is the indicator function. A computationally efficient method to calculate this sum involves sorting the negative scores $\\{S_j^-\\}$ and, for each positive score $S_i^+$, using a binary search to count the number of negative scores that are less than or equal to it.\n\nThe simulation will be implemented in Python using the `NumPy` library for all numerical computations. A fixed random seed will ensure the reproducibility of the results. The final output is the average bias for each of the four specified experimental conditions, rounded to six decimal places. The expected bias in Case 2, where there is no domain shift between training and validation data, should be close to $0$, providing a sanity check for the implementation. For other cases where a domain shift exists (in mean, covariance, or prevalence), the leakage of validation data statistics into the training pipeline is expected to artificially reduce the apparent domain shift, resulting in a positive bias, i.e., $\\mathrm{AUC}_{\\text{leakage}} > \\mathrm{AUC}_{\\text{no-leakage}}$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation for all test cases and print the results.\n    \"\"\"\n    # A fixed seed ensures that the random sampling is reproducible.\n    np.random.seed(42)\n\n    # Define the parameter sets for the four test cases.\n    # Each tuple contains: (mu1, mu0, rho_train, rho_val, b_train, b_val, \n    #                       pi_train, pi_val, n_train, n_val, R)\n    test_cases = [\n        # Case 1 (moderate domain shift, happy path)\n        (np.array([0.5, 0.5]), np.array([-0.5, -0.5]), 0.3, 0.3, \n         np.array([0.0, 0.0]), np.array([1.0, 1.0]), 0.5, 0.5, 600, 600, 200),\n        # Case 2 (no domain shift, boundary condition)\n        (np.array([0.5, 0.5]), np.array([-0.5, -0.5]), 0.3, 0.3, \n         np.array([0.0, 0.0]), np.array([0.0, 0.0]), 0.5, 0.5, 600, 600, 200),\n        # Case 3 (covariance shift edge case)\n        (np.array([0.5, 0.5]), np.array([-0.5, -0.5]), 0.0, 0.8, \n         np.array([0.0, 0.0]), np.array([1.0, 1.0]), 0.5, 0.5, 400, 400, 200),\n        # Case 4 (prevalence shift edge case)\n        (np.array([0.5, 0.5]), np.array([-0.5, -0.5]), 0.3, 0.3, \n         np.array([0.0, 0.0]), np.array([1.0, 1.0]), 0.3, 0.7, 600, 600, 200),\n    ]\n\n    results = []\n    for case_params in test_cases:\n        expected_bias = run_simulation(*case_params)\n        results.append(expected_bias)\n\n    # Format the final output string as specified.\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\ndef generate_data(n, pi, mu1, mu0, b, rho):\n    \"\"\"\n    Generates feature data and labels from a 2D Gaussian mixture model.\n    \"\"\"\n    n_pos = int(round(n * pi))\n    n_neg = n - n_pos\n\n    mean_pos = mu1 + b\n    mean_neg = mu0 + b\n    \n    cov_matrix = np.array([[1, rho], [rho, 1]])\n\n    X_pos = np.random.multivariate_normal(mean_pos, cov_matrix, n_pos)\n    X_neg = np.random.multivariate_normal(mean_neg, cov_matrix, n_neg)\n    \n    if n_pos > 0 and n_neg > 0:\n        X = np.vstack((X_pos, X_neg))\n        y = np.hstack((np.ones(n_pos), np.zeros(n_neg)))\n        # Shuffle to mix positive and negative samples\n        perm = np.random.permutation(n)\n        X, y = X[perm], y[perm]\n    elif n_pos > 0:\n        X, y = X_pos, np.ones(n_pos)\n    else: # n_neg > 0\n        X, y = X_neg, np.zeros(n_neg)\n\n    return X, y\n\ndef train_lda(Z_train, y_train):\n    \"\"\"\n    Trains a Linear Discriminant Analysis classifier.\n    \"\"\"\n    Z1 = Z_train[y_train == 1]\n    Z0 = Z_train[y_train == 0]\n\n    n1, n0 = Z1.shape[0], Z0.shape[0]\n\n    if n1  2 or n0  2:\n        # Handle degenerate cases, though unlikely with problem parameters\n        return np.zeros(Z_train.shape[1])\n\n    mu1_hat = np.mean(Z1, axis=0)\n    mu0_hat = np.mean(Z0, axis=0)\n    \n    S1 = np.cov(Z1, rowvar=False, ddof=1)\n    S0 = np.cov(Z0, rowvar=False, ddof=1)\n    \n    # an edge case can cause S1 or S0 to be a scalar 0.0 if there is one sample\n    if S1.ndim == 0: S1 = np.zeros((Z_train.shape[1], Z_train.shape[1]))\n    if S0.ndim == 0: S0 = np.zeros((Z_train.shape[1], Z_train.shape[1]))\n    \n    Sigma_pooled = ((n1 - 1) * S1 + (n0 - 1) * S0) / (n1 + n0 - 2)\n    \n    try:\n        Sigma_pooled_inv = np.linalg.inv(Sigma_pooled)\n    except np.linalg.LinAlgError:\n         Sigma_pooled_inv = np.linalg.pinv(Sigma_pooled)\n\n    w = Sigma_pooled_inv @ (mu1_hat - mu0_hat)\n    return w\n\ndef calculate_auc(scores, y_val):\n    \"\"\"\n    Calculates the ROC AUC from scores and true labels.\n    \"\"\"\n    scores_pos = scores[y_val == 1]\n    scores_neg = scores[y_val == 0]\n    \n    n_pos = len(scores_pos)\n    n_neg = len(scores_neg)\n\n    if n_pos == 0 or n_neg == 0:\n        return 0.5  # Convention for undefined AUC\n\n    # Sort negative scores for efficient comparison\n    scores_neg_sorted = np.sort(scores_neg)\n    \n    # Use searchsorted for vectorized counting of pairs\n    # Number of S_neg  S_pos\n    num_greater = np.sum(np.searchsorted(scores_neg_sorted, scores_pos, side='left'))\n    # Number of S_neg == S_pos\n    num_equal = np.sum(np.searchsorted(scores_neg_sorted, scores_pos, side='right') - \n                       np.searchsorted(scores_neg_sorted, scores_pos, side='left'))\n\n    auc = (num_greater + 0.5 * num_equal) / (n_pos * n_neg)\n    return auc\n\ndef run_simulation(mu1, mu0, rho_train, rho_val, b_train, b_val, \n                   pi_train, pi_val, n_train, n_val, R):\n    \"\"\"\n    Runs one full Monte Carlo simulation for a given set of parameters.\n    \"\"\"\n    auc_diffs = []\n    STABILIZATION_CONST = 1e-8\n    \n    for _ in range(R):\n        # 1. Generate data\n        X_train, y_train = generate_data(n_train, pi_train, mu1, mu0, b_train, rho_train)\n        X_val, y_val = generate_data(n_val, pi_val, mu1, mu0, b_val, rho_val)\n\n        # --- No-leakage workflow ---\n        # 2a. Normalize using training set stats\n        m_train = np.mean(X_train, axis=0)\n        s_train = np.std(X_train, axis=0) + STABILIZATION_CONST\n        Z_train_nl = (X_train - m_train) / s_train\n        Z_val_nl = (X_val - m_train) / s_train\n\n        # 3a. Train LDA on no-leakage normalized data\n        w_nl = train_lda(Z_train_nl, y_train)\n\n        # 4a. Evaluate on no-leakage validation data\n        scores_nl = Z_val_nl @ w_nl\n        auc_nl = calculate_auc(scores_nl, y_val)\n\n        # --- Leakage workflow ---\n        X_all = np.vstack((X_train, X_val))\n        \n        # 2b. Normalize using combined set stats\n        m_all = np.mean(X_all, axis=0)\n        s_all = np.std(X_all, axis=0) + STABILIZATION_CONST\n        Z_train_l = (X_train - m_all) / s_all\n        Z_val_l = (X_val - m_all) / s_all\n\n        # 3b. Train LDA on leakage normalized data\n        w_l = train_lda(Z_train_l, y_train)\n        \n        # 4b. Evaluate on leakage validation data\n        scores_l = Z_val_l @ w_l\n        auc_l = calculate_auc(scores_l, y_val)\n\n        # 5. Record the difference\n        auc_diffs.append(auc_l - auc_nl)\n\n    # 6. Return the expected bias (mean of differences)\n    return np.mean(auc_diffs)\n\nsolve()\n```"
        }
    ]
}