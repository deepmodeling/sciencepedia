{
    "hands_on_practices": [
        {
            "introduction": "A common preprocessing step in many statistical analyses, including Independent Component Analysis (ICA), is whitening the data, which results in variables that are uncorrelated. This exercise  is a crucial conceptual practice that demonstrates through a concrete counterexample that zero correlation does not guarantee statistical independence. Understanding this distinction is fundamental to appreciating why ICA, which leverages higher-order statistics to maximize non-Gaussianity, is necessary for true feature separation beyond what second-order methods like PCA can offer.",
            "id": "3822180",
            "problem": "A hyperspectral imaging retrieval in a remote sensing and environmental modeling workflow seeks to separate latent biophysical drivers via Independent Component Analysis (ICA). Whitening in ICA is used to enforce zero mean and identity covariance for the observed feature vector. However, zero correlation does not imply independence. To demonstrate this within a scientifically realistic setup, consider a single latent standardized driver $S$ representing a normalized leaf water content anomaly, modeled as a continuous random variable with a symmetric probability density function (PDF). Field engineers apply nonlinear preprocessing to derive two engineered features from $S$ to mitigate sensor nonlinearity:\n- A saturating index defined by $X_{1} = g_{1}(S) = S - S^{3}$,\n- A quadratic index centered to zero mean defined by $X_{2} = g_{2}(S) = S^{2} - \\mathbb{E}[S^{2}]$.\nAssume $S \\sim \\mathrm{Uniform}([-1,1])$, which models a bounded normalized anomaly typical of biophysical indices after min-max normalization, with PDF $f_{S}(s) = \\frac{1}{2}$ for $s \\in [-1,1]$ and zero otherwise.\n\nStarting from the fundamental definitions that (i) independence means $\\mathbb{E}[h(X_{1})\\,k(X_{2})] = \\mathbb{E}[h(X_{1})]\\mathbb{E}[k(X_{2})]$ for all bounded measurable functions $h$ and $k$, and (ii) zero correlation means $\\mathrm{Cov}(X_{1},X_{2}) = \\mathbb{E}[X_{1}X_{2}] - \\mathbb{E}[X_{1}]\\mathbb{E}[X_{2}] = 0$, analyze this counterexample. First, verify that the pair $(X_{1},X_{2})$ is decorrelated (zero covariance) under the stated model. Then, to explicitly demonstrate that decorrelation does not imply independence, compute the fourth-order mixed moment discrepancy\n$$\n\\Delta \\equiv \\mathbb{E}\\!\\left[X_{1}^{2}X_{2}^{2}\\right] - \\mathbb{E}\\!\\left[X_{1}^{2}\\right]\\mathbb{E}\\!\\left[X_{2}^{2}\\right].\n$$\nA nonzero value of $\\Delta$ certifies statistical dependence. Provide $\\Delta$ as a single exact rational number. No rounding is required. Express the final answer without units.",
            "solution": "The user wants to analyze a specific counterexample to the notion that decorrelation implies independence, within the context of Independent Component Analysis (ICA). The problem is to first verify that two engineered features, $X_{1}$ and $X_{2}$, derived from a latent variable $S$, are decorrelated. Second, it requires the calculation of a fourth-order mixed moment discrepancy, $\\Delta$, to demonstrate their statistical dependence.\n\nThe latent variable $S$ is uniformly distributed on the interval $[-1, 1]$, denoted $S \\sim \\mathrm{Uniform}([-1,1])$. Its probability density function (PDF) is $f_{S}(s) = \\frac{1}{2}$ for $s \\in [-1,1]$ and $f_S(s)=0$ otherwise.\n\nThe expected value of a function $g(S)$ is given by the integral:\n$$\n\\mathbb{E}[g(S)] = \\int_{-1}^{1} g(s) f_{S}(s) \\, ds = \\frac{1}{2} \\int_{-1}^{1} g(s) \\, ds\n$$\nWe first compute the general moments of $S$, $\\mathbb{E}[S^n]$, for integer $n \\ge 0$.\nIf $n$ is an odd integer, $s^n$ is an odd function. Since the integration interval $[-1, 1]$ is symmetric about $0$, the integral is zero.\n$$\n\\mathbb{E}[S^n] = \\frac{1}{2} \\int_{-1}^{1} s^n \\, ds = 0 \\quad (\\text{for odd } n)\n$$\nIf $n$ is an even integer, $s^n$ is an even function. The integral becomes:\n$$\n\\mathbb{E}[S^n] = \\frac{1}{2} \\int_{-1}^{1} s^n \\, ds = \\frac{1}{2} \\cdot 2 \\int_{0}^{1} s^n \\, ds = \\left[ \\frac{s^{n+1}}{n+1} \\right]_{0}^{1} = \\frac{1}{n+1} \\quad (\\text{for even } n)\n$$\nWe will need the following specific moments:\n$\\mathbb{E}[S] = 0$\n$\\mathbb{E}[S^2] = \\frac{1}{3}$\n$\\mathbb{E}[S^3] = 0$\n$\\mathbb{E}[S^4] = \\frac{1}{5}$\n$\\mathbb{E}[S^5] = 0$\n$\\mathbb{E}[S^6] = \\frac{1}{7}$\n$\\mathbb{E}[S^8] = \\frac{1}{9}$\n$\\mathbb{E}[S^{10}] = \\frac{1}{11}$\n\nThe two engineered features are defined as:\n$X_{1} = S - S^{3}$\n$X_{2} = S^{2} - \\mathbb{E}[S^{2}] = S^{2} - \\frac{1}{3}$\n\n**Part 1: Verify Decorrelation**\nTwo random variables $X_1$ and $X_2$ are decorrelated if their covariance is zero: $\\mathrm{Cov}(X_{1},X_{2}) = 0$. The covariance is defined as $\\mathrm{Cov}(X_{1},X_{2}) = \\mathbb{E}[X_{1}X_{2}] - \\mathbb{E}[X_{1}]\\mathbb{E}[X_{2}]$.\n\nFirst, we compute the expected values of $X_1$ and $X_2$:\n$$\n\\mathbb{E}[X_{1}] = \\mathbb{E}[S - S^{3}] = \\mathbb{E}[S] - \\mathbb{E}[S^{3}] = 0 - 0 = 0\n$$\n$$\n\\mathbb{E}[X_{2}] = \\mathbb{E}\\left[S^{2} - \\frac{1}{3}\\right] = \\mathbb{E}[S^{2}] - \\frac{1}{3} = \\frac{1}{3} - \\frac{1}{3} = 0\n$$\nSince $\\mathbb{E}[X_{1}] = 0$ and $\\mathbb{E}[X_{2}] = 0$, the covariance simplifies to $\\mathrm{Cov}(X_{1},X_{2}) = \\mathbb{E}[X_{1}X_{2}]$.\nWe compute the expectation of the product $X_{1}X_{2}$:\n$$\n\\mathbb{E}[X_{1}X_{2}] = \\mathbb{E}\\left[(S-S^3)\\left(S^2 - \\frac{1}{3}\\right)\\right] = \\mathbb{E}\\left[S^3 - \\frac{1}{3}S - S^5 + \\frac{1}{3}S^3\\right] = \\mathbb{E}\\left[- \\frac{1}{3}S + \\frac{4}{3}S^3 - S^5\\right]\n$$\nBy linearity of expectation:\n$$\n\\mathbb{E}[X_{1}X_{2}] = -\\frac{1}{3}\\mathbb{E}[S] + \\frac{4}{3}\\mathbb{E}[S^3] - \\mathbb{E}[S^5] = -\\frac{1}{3}(0) + \\frac{4}{3}(0) - 0 = 0\n$$\nSince $\\mathrm{Cov}(X_{1},X_{2}) = 0$, the variables $X_1$ and $X_2$ are decorrelated. This completes the verification.\n\n**Part 2: Compute the Discrepancy $\\Delta$**\nThe discrepancy is defined as $\\Delta = \\mathbb{E}[X_{1}^{2}X_{2}^{2}] - \\mathbb{E}[X_{1}^{2}]\\mathbb{E}[X_{2}^{2}]$. A non-zero $\\Delta$ indicates that $X_1^2$ and $X_2^2$ are correlated, which in turn proves that $X_1$ and $X_2$ are not independent. We calculate each term separately.\n\nFirst, calculate $\\mathbb{E}[X_{1}^{2}]$:\n$X_{1}^{2} = (S-S^{3})^2 = S^2 - 2S^4 + S^6$\n$$\n\\mathbb{E}[X_{1}^{2}] = \\mathbb{E}[S^2 - 2S^4 + S^6] = \\mathbb{E}[S^2] - 2\\mathbb{E}[S^4] + \\mathbb{E}[S^6]\n$$\n$$\n\\mathbb{E}[X_{1}^{2}] = \\frac{1}{3} - 2\\left(\\frac{1}{5}\\right) + \\frac{1}{7} = \\frac{1}{3} - \\frac{2}{5} + \\frac{1}{7} = \\frac{35 - 42 + 15}{105} = \\frac{8}{105}\n$$\nNext, calculate $\\mathbb{E}[X_{2}^{2}]$:\n$X_{2}^{2} = \\left(S^2 - \\frac{1}{3}\\right)^2 = S^4 - \\frac{2}{3}S^2 + \\frac{1}{9}$\n$$\n\\mathbb{E}[X_{2}^{2}] = \\mathbb{E}\\left[S^4 - \\frac{2}{3}S^2 + \\frac{1}{9}\\right] = \\mathbb{E}[S^4] - \\frac{2}{3}\\mathbb{E}[S^2] + \\frac{1}{9}\n$$\n$$\n\\mathbb{E}[X_{2}^{2}] = \\frac{1}{5} - \\frac{2}{3}\\left(\\frac{1}{3}\\right) + \\frac{1}{9} = \\frac{1}{5} - \\frac{2}{9} + \\frac{1}{9} = \\frac{1}{5} - \\frac{1}{9} = \\frac{9-5}{45} = \\frac{4}{45}\n$$\nNow, we calculate the product term of $\\Delta$:\n$$\n\\mathbb{E}[X_{1}^{2}]\\mathbb{E}[X_{2}^{2}] = \\left(\\frac{8}{105}\\right)\\left(\\frac{4}{45}\\right) = \\frac{32}{4725}\n$$\nFinally, we calculate $\\mathbb{E}[X_{1}^{2}X_{2}^{2}]$:\n$X_{1}^{2}X_{2}^{2} = (S^2 - 2S^4 + S^6)\\left(S^4 - \\frac{2}{3}S^2 + \\frac{1}{9}\\right)$\nExpanding this product gives:\n$S^6 - \\frac{2}{3}S^4 + \\frac{1}{9}S^2 - 2S^8 + \\frac{4}{3}S^6 - \\frac{2}{9}S^4 + S^{10} - \\frac{2}{3}S^8 + \\frac{1}{9}S^6$\nCollecting terms by power of $S$:\n$X_{1}^{2}X_{2}^{2} = S^{10} + \\left(-2-\\frac{2}{3}\\right)S^8 + \\left(1+\\frac{4}{3}+\\frac{1}{9}\\right)S^6 + \\left(-\\frac{2}{3}-\\frac{2}{9}\\right)S^4 + \\frac{1}{9}S^2$\n$X_{1}^{2}X_{2}^{2} = S^{10} - \\frac{8}{3}S^8 + \\frac{22}{9}S^6 - \\frac{8}{9}S^4 + \\frac{1}{9}S^2$\nTaking the expectation:\n$$\n\\mathbb{E}[X_{1}^{2}X_{2}^{2}] = \\mathbb{E}[S^{10}] - \\frac{8}{3}\\mathbb{E}[S^8] + \\frac{22}{9}\\mathbb{E}[S^6] - \\frac{8}{9}\\mathbb{E}[S^4] + \\frac{1}{9}\\mathbb{E}[S^2]\n$$\n$$\n\\mathbb{E}[X_{1}^{2}X_{2}^{2}] = \\frac{1}{11} - \\frac{8}{3}\\left(\\frac{1}{9}\\right) + \\frac{22}{9}\\left(\\frac{1}{7}\\right) - \\frac{8}{9}\\left(\\frac{1}{5}\\right) + \\frac{1}{9}\\left(\\frac{1}{3}\\right)\n$$\n$$\n= \\frac{1}{11} - \\frac{8}{27} + \\frac{22}{63} - \\frac{8}{45} + \\frac{1}{27} = \\frac{1}{11} - \\frac{7}{27} + \\frac{22}{63} - \\frac{8}{45}\n$$\nThe least common multiple of the denominators $(11, 27, 63, 45)$ is $10395$.\n$$\n\\mathbb{E}[X_{1}^{2}X_{2}^{2}] = \\frac{945}{10395} - \\frac{7 \\cdot 385}{10395} + \\frac{22 \\cdot 165}{10395} - \\frac{8 \\cdot 231}{10395}\n$$\n$$\n= \\frac{945 - 2695 + 3630 - 1848}{10395} = \\frac{4575 - 4543}{10395} = \\frac{32}{10395}\n$$\nNow we compute $\\Delta$:\n$$\n\\Delta = \\mathbb{E}[X_{1}^{2}X_{2}^{2}] - \\mathbb{E}[X_{1}^{2}]\\mathbb{E}[X_{2}^{2}] = \\frac{32}{10395} - \\frac{32}{4725}\n$$\nTo subtract these fractions, we find a common denominator.\n$10395 = 3^3 \\cdot 5 \\cdot 7 \\cdot 11$\n$4725 = 3^3 \\cdot 5^2 \\cdot 7$\nThe least common denominator is $3^3 \\cdot 5^2 \\cdot 7 \\cdot 11 = 51975$.\n$$\n\\Delta = 32\\left(\\frac{1}{10395} - \\frac{1}{4725}\\right) = 32\\left(\\frac{5}{51975} - \\frac{11}{51975}\\right) = 32\\left(\\frac{-6}{51975}\\right) = \\frac{-192}{51975}\n$$\nWe simplify this fraction. The numerator is $192 = 3 \\cdot 64$. The denominator $51975$ is divisible by $3$ (sum of digits is $27$).\n$51975 \\div 3 = 17325$.\n$$\n\\Delta = \\frac{-64}{17325}\n$$\nSince $17325$ is odd, the fraction is in simplest form. The non-zero value of $\\Delta$ confirms that $X_1$ and $X_2$ are statistically dependent, despite being decorrelated.",
            "answer": "$$\\boxed{-\\frac{64}{17325}}$$"
        },
        {
            "introduction": "After establishing the theoretical need to look beyond correlation, this practice delves into the mechanics of how an ICA algorithm operates. This problem  offers a hands-on look inside the FastICA algorithm, one of the most efficient and widely used methods for source separation. By implementing a single fixed-point iteration, you will gain a concrete understanding of the numerical optimization process that maximizes a measure of non-Gaussianity to extract an independent component from a mixture of signals.",
            "id": "3822211",
            "problem": "You are given a set of whitened remote sensing pixel feature vectors and an initial direction vector. Consider Independent Component Analysis (ICA) for feature separation using a single-unit extraction with a fixed-point iteration. The data are whitened so that each feature has zero mean and identity covariance. Use the hyperbolic tangent nonlinearity. Your task is to apply one iteration of the fixed-point update on the initial direction vector for each test case, using sample averages to approximate expectations, and then renormalize the updated vector to unit norm.\n\nContext and definitions:\n- Independent Component Analysis (ICA) seeks a direction $w$ such that the projection $u = w^\\top x$ has maximal non-Gaussianity under appropriate constraints, with $x$ being the whitened data vector. Whitening implies zero mean and identity covariance, i.e., $\\mathbb{E}[x] = 0$ and $\\mathbb{E}[x x^\\top] = I$ for the population, and sample analogues for the finite data matrices considered here.\n- Fast Independent Component Analysis (FastICA) uses a fixed-point iteration with a chosen nonlinearity to update $w$. Use the hyperbolic tangent nonlinearity $g(u) = \\tanh(u)$ and its sample-based application on the provided data matrices.\n- The data matrices are provided as $X \\in \\mathbb{R}^{p \\times n}$, with $p$ features (rows) and $n$ samples (columns). The initial direction vector is $w_0 \\in \\mathbb{R}^p$. All expectations are to be approximated by sample averages across the $n$ samples. After computing the update, enforce $\\|w\\|_2 = 1$.\n\nTest suite:\n- Dataset $\\mathcal{D}_1$: A $2 \\times 4$ whitened matrix whose columns are $[\\sqrt{2}, 0]^\\top$, $[-\\sqrt{2}, 0]^\\top$, $[0, \\sqrt{2}]^\\top$, $[0, -\\sqrt{2}]^\\top$.\n- Dataset $\\mathcal{D}_2$: A $2 \\times 8$ whitened matrix whose columns are $[a, 0]^\\top$, $[-a, 0]^\\top$, $[b, 0]^\\top$, $[-b, 0]^\\top$, $[0, c]^\\top$, $[0, -c]^\\top$, $[0, d]^\\top$, $[0, -d]^\\top$, with $a = \\sqrt{3}$, $b = 1$, $c = \\sqrt{3}$, $d = 1$. This produces heavier-tailed projections while maintaining whitening.\n- Use the sample mean across columns for all expectations. A single iteration means compute the updated direction once from $w_0$, followed by normalization to unit norm.\n\nApply this to the following $5$ test cases:\n1. $(X = \\mathcal{D}_1, w_0 = [0.6, 0.8]^\\top)$\n2. $(X = \\mathcal{D}_1, w_0 = [1.0, 0.0]^\\top)$\n3. $(X = \\mathcal{D}_2, w_0 = [-0.3, 0.95]^\\top)$\n4. $(X = \\mathcal{D}_2, w_0 = [10.0, -10.0]^\\top)$\n5. Edge case $(X = \\mathcal{D}_1, w_0 = [10^{-9}, -10^{-9}]^\\top)$\n\nOutput specification:\n- For each test case, output the updated and normalized direction vector $w$ as a list of two floats rounded to $6$ decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the two-element list for the corresponding test case, in order. For example, the format should be $[[w_{11}, w_{12}], [w_{21}, w_{22}], \\dots]$ with no additional text.\n\nNo physical units or angle units are involved; all outputs are dimensionless. All mathematical operations must be implemented using standard floating-point arithmetic.",
            "solution": "The problem has been validated and is deemed scientifically sound, well-posed, and self-contained. The provided data and conditions are sufficient and consistent for a unique solution.\n\nThe core of this problem lies in applying a single iterative step of the Fast Independent Component Analysis (FastICA) algorithm for one unit (i.e., to find one independent component). The objective of ICA is to find a linear transformation of a signal that maximizes the statistical independence of its components. For a single component, this is equivalent to finding a direction vector $w$ such that the projection $u = w^\\top x$ of the data $x$ is maximally non-Gaussian.\n\nThe FastICA algorithm provides an efficient fixed-point iteration scheme to find such a direction $w$. The data $x$ is assumed to be whitened, meaning it has a zero mean, $\\mathbb{E}[x] = 0$, and an identity covariance matrix, $\\mathbb{E}[xx^\\top] = I$. The problem states that these population expectations should be replaced by their corresponding sample averages over the provided data matrices.\n\nThe general one-unit FastICA update rule, for an an initial vector $w$, is given by:\n$$\nw^+ \\leftarrow \\mathbb{E}[x g(w^\\top x)] - \\mathbb{E}[g'(w^\\top x)] w\n$$\nwhere $g(u)$ is a suitable non-quadratic function, and $g'(u)$ is its derivative. After this update, the new vector $w^+$ is normalized to have a unit norm:\n$$\nw_{new} \\leftarrow \\frac{w^+}{\\|w^+\\|_2}\n$$\n\nThe problem specifies the use of the hyperbolic tangent nonlinearity, $g(u) = \\tanh(u)$. Its derivative is $g'(u) = 1 - \\tanh^2(u)$.\n\nGiven a data matrix $X \\in \\mathbb{R}^{p \\times n}$ with $n$ samples (columns) $x_1, \\dots, x_n$, and an initial direction vector $w_0 \\in \\mathbb{R}^p$, the sample-based update equations for a single iteration are:\n1.  Compute the projections: $u_i = w_0^\\top x_i$ for each sample $i = 1, \\dots, n$.\n2.  Compute the unnormalized updated vector $w^+$:\n    $$\n    w^+ = \\left( \\frac{1}{n} \\sum_{i=1}^n x_i \\tanh(w_0^\\top x_i) \\right) - \\left( \\frac{1}{n} \\sum_{i=1}^n (1 - \\tanh^2(w_0^\\top x_i)) \\right) w_0\n    $$\n3.  Normalize the vector:\n    $$\n    w = \\frac{w^+}{\\|w^+\\|_2}\n    $$\nWe will now apply this procedure to each of the $5$ test cases.\n\n**Case 1: $(X = \\mathcal{D}_1, w_0 = [0.6, 0.8]^\\top)$**\nThe data matrix is $X_1 = \\begin{pmatrix} \\sqrt{2}  -\\sqrt{2}  0  0 \\\\ 0  0  \\sqrt{2}  -\\sqrt{2} \\end{pmatrix}$, with $n=4$ samples. The initial vector is $w_0 = \\begin{pmatrix} 0.6 \\\\ 0.8 \\end{pmatrix}$.\n\n1.  Projections $u_i = w_0^\\top x_i$:\n    $u_1 = 0.6\\sqrt{2}$, $u_2 = -0.6\\sqrt{2}$, $u_3 = 0.8\\sqrt{2}$, $u_4 = -0.8\\sqrt{2}$.\n2.  Update $w^+$:\n    The first term is $\\frac{1}{4} \\sum_{i=1}^4 x_i \\tanh(u_i)$. By symmetry of $\\tanh$, this simplifies to:\n    $$\n    \\frac{1}{4} \\left( 2 \\begin{pmatrix} \\sqrt{2} \\\\ 0 \\end{pmatrix} \\tanh(0.6\\sqrt{2}) + 2 \\begin{pmatrix} 0 \\\\ \\sqrt{2} \\end{pmatrix} \\tanh(0.8\\sqrt{2}) \\right) = \\begin{pmatrix} \\frac{\\sqrt{2}}{2} \\tanh(0.6\\sqrt{2}) \\\\ \\frac{\\sqrt{2}}{2} \\tanh(0.8\\sqrt{2}) \\end{pmatrix} \\approx \\begin{pmatrix} 0.487732 \\\\ 0.573490 \\end{pmatrix}\n    $$\n    The second term's scalar coefficient is $\\frac{1}{4} \\sum_{i=1}^4 (1 - \\tanh^2(u_i))$. By symmetry of $\\tanh^2$, this is:\n    $$\n    \\frac{1}{4} \\left( 2(1 - \\tanh^2(0.6\\sqrt{2})) + 2(1 - \\tanh^2(0.8\\sqrt{2})) \\right) = 1 - \\frac{1}{2}(\\tanh^2(0.6\\sqrt{2}) + \\tanh^2(0.8\\sqrt{2})) \\approx 0.433244\n    $$\n    The second term vector is $0.433244 \\times w_0 \\approx \\begin{pmatrix} 0.259946 \\\\ 0.346595 \\end{pmatrix}$.\n    $w^+ \\approx \\begin{pmatrix} 0.487732 \\\\ 0.573490 \\end{pmatrix} - \\begin{pmatrix} 0.259946 \\\\ 0.346595 \\end{pmatrix} = \\begin{pmatrix} 0.227786 \\\\ 0.226895 \\end{pmatrix}$.\n3.  Normalization:\n    $\\|w^+\\|_2 \\approx \\sqrt{0.227786^2 + 0.226895^2} \\approx 0.321506$.\n    $w = \\frac{w^+}{\\|w^+\\|_2} \\approx \\begin{pmatrix} 0.708496 \\\\ 0.705725 \\end{pmatrix}$.\n\n**Case 2: $(X = \\mathcal{D}_1, w_0 = [1.0, 0.0]^\\top)$**\nThe data matrix is $X_1$ and $w_0 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$. This initial vector is already aligned with one of the axes of the data distribution.\n1.  Projections: $u_1 = \\sqrt{2}$, $u_2 = -\\sqrt{2}$, $u_3 = 0$, $u_4 = 0$.\n2.  Update $w^+$:\n    First term: $\\frac{1}{4} \\left( 2 \\begin{pmatrix} \\sqrt{2} \\\\ 0 \\end{pmatrix} \\tanh(\\sqrt{2}) \\right) = \\begin{pmatrix} \\frac{\\sqrt{2}}{2}\\tanh(\\sqrt{2}) \\\\ 0 \\end{pmatrix}$.\n    Second term coefficient: $\\frac{1}{4} (2(1-\\tanh^2(\\sqrt{2})) + 2(1-\\tanh^2(0))) = 1 - \\frac{1}{2}\\tanh^2(\\sqrt{2})$.\n    $w^+$ has a second component of $0$. The first component is $\\frac{\\sqrt{2}}{2}\\tanh(\\sqrt{2}) - (1 - \\frac{1}{2}\\tanh^2(\\sqrt{2})) \\approx 0.62813 - 0.60547 = 0.02266 > 0$.\n3.  Normalization:\n    Since $w^+$ is a positive multiple of $\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$, normalization yields $w = \\begin{pmatrix} 1.0 \\\\ 0.0 \\end{pmatrix}$. This direction is a fixed point of the algorithm.\n\n**Case 3: $(X = \\mathcal{D}_2, w_0 = [-0.3, 0.95]^\\top)$**\nThe data matrix $X_2$ has $n=8$ samples with $a=c=\\sqrt{3}, b=d=1$. $w_0 = \\begin{pmatrix} -0.3 \\\\ 0.95 \\end{pmatrix}$.\n1.  Projections: $u_1 = -0.3\\sqrt{3}, u_2 = 0.3\\sqrt{3}, u_3 = -0.3, u_4 = 0.3, u_5 = 0.95\\sqrt{3}, u_6 = -0.95\\sqrt{3}, u_7 = 0.95, u_8 = -0.95$.\n2.  Update $w^+$:\n    First term: $\\frac{1}{8} \\sum x_i \\tanh(u_i) = \\frac{1}{4} \\begin{pmatrix} -\\sqrt{3}\\tanh(0.3\\sqrt{3}) - \\tanh(0.3) \\\\ \\sqrt{3}\\tanh(0.95\\sqrt{3}) + \\tanh(0.95) \\end{pmatrix} \\approx \\begin{pmatrix} -0.279670 \\\\ 0.586812 \\end{pmatrix}$.\n    Second term coefficient: $\\frac{1}{8} \\sum (1-\\tanh^2(u_i)) = 1 - \\frac{1}{4}(\\tanh^2(0.3\\sqrt{3}) + \\tanh^2(0.3) + \\tanh^2(0.95\\sqrt{3}) + \\tanh^2(0.95)) \\approx 0.569615$.\n    Second term vector: $0.569615 \\times w_0 \\approx \\begin{pmatrix} -0.170885 \\\\ 0.541134 \\end{pmatrix}$.\n    $w^+ \\approx \\begin{pmatrix} -0.279670 \\\\ 0.586812 \\end{pmatrix} - \\begin{pmatrix} -0.170885 \\\\ 0.541134 \\end{pmatrix} = \\begin{pmatrix} -0.108785 \\\\ 0.045678 \\end{pmatrix}$.\n3.  Normalization:\n    $\\|w^+\\|_2 \\approx \\sqrt{(-0.108785)^2 + 0.045678^2} \\approx 0.117986$.\n    $w = \\frac{w^+}{\\|w^+\\|_2} \\approx \\begin{pmatrix} -0.922020 \\\\ 0.387147 \\end{pmatrix}$.\n\n**Case 4: $(X = \\mathcal{D}_2, w_0 = [10.0, -10.0]^\\top)$**\nThe data matrix is $X_2$, and $w_0 = 10 \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$. The components of $w_0$ are large.\n1.  Projections: The values of $u_i = w_0^\\top x_i$ are all large in magnitude (e.g., $10\\sqrt{3} \\approx 17.32$, $10$).\n2.  Update $w^+$:\n    For large $|u|$, $\\tanh(u) \\approx \\text{sign}(u)$ and its derivative $1-\\tanh^2(u) \\approx 0$.\n    The second term $\\mathbb{E}[g'(w^\\top x)]w$ will be close to zero.\n    The first term $\\mathbb{E}[x g(w^\\top x)]$ dominates. $g(u_i) = \\tanh(w_0^\\top x_i) \\approx \\text{sign}(w_0^\\top x_i)$.\n    The signs are $\\text{sign}([10,-10]^\\top x_i)$.\n    $w^+ \\approx \\frac{1}{8} \\sum x_i \\text{sign}(w_0^\\top x_i) = \\frac{1}{8} \\begin{pmatrix} 2(\\sqrt{3}+1) \\\\ -2(\\sqrt{3}+1) \\end{pmatrix} = \\frac{\\sqrt{3}+1}{4} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$.\n3.  Normalization:\n    $w^+$ is a positive multiple of $\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$. Normalizing this vector gives $\\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} \\approx \\begin{pmatrix} 0.707107 \\\\ -0.707107 \\end{pmatrix}$.\n    This is a fixed-point direction for the algorithm.\n\n**Case 5: $(X = \\mathcal{D}_1, w_0 = [10^{-9}, -10^{-9}]^\\top)$**\nThe data matrix is $X_1$, and $w_0 = \\epsilon \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$ with $\\epsilon = 10^{-9}$. The initial vector is very close to the origin.\n1.  Projections: The values of $u_i=w_0^\\top x_i$ are extremely small (of order $\\epsilon$).\n2.  Update $w^+$:\n    For small $u$, $\\tanh(u) \\approx u - u^3/3$ and $1-\\tanh^2(u) \\approx 1 - u^2$.\n    The update rule simplifies to $w^+ \\approx (\\|w_0\\|^2 w_0 - \\frac{1}{3} \\mathbb{E}[x(w_0^\\top x)^3])$.\n    We calculate $\\frac{1}{4} \\sum x_i(w_0^\\top x_i)^3 = 2\\epsilon^3 \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$.\n    $\\|w_0\\|^2 = 2\\epsilon^2$.\n    $w^+ \\approx 2\\epsilon^2 (\\epsilon \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}) - \\frac{1}{3} (2\\epsilon^3 \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}) = (2\\epsilon^3 - \\frac{2}{3}\\epsilon^3) \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\frac{4}{3}\\epsilon^3 \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$.\n3.  Normalization:\n    Again, $w^+$ is a positive multiple of $\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$. Normalizing yields $w = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} \\approx \\begin{pmatrix} 0.707107 \\\\ -0.707107 \\end{pmatrix}$. This demonstrates that even starting near the origin, the algorithm is driven by higher-order statistics (related to kurtosis) towards a non-Gaussian direction.\n\nThe results, rounded to $6$ decimal places, are:\n1.  $[0.708496, 0.705725]$\n2.  $[1.000000, 0.000000]$\n3.  $[-0.922020, 0.387147]$\n4.  $[0.707107, -0.707107]$\n5.  $[0.707107, -0.707107]$",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the ICA single-step update problem for a suite of test cases.\n    \"\"\"\n\n    # Define the datasets\n    sqrt2 = np.sqrt(2)\n    sqrt3 = np.sqrt(3)\n\n    D1 = np.array([\n        [sqrt2, -sqrt2, 0, 0],\n        [0, 0, sqrt2, -sqrt2]\n    ])\n\n    a, b, c, d = sqrt3, 1.0, sqrt3, 1.0\n    D2 = np.array([\n        [a, -a, b, -b, 0, 0, 0, 0],\n        [0, 0, 0, 0, c, -c, d, -d]\n    ])\n\n    # Define the test cases\n    test_cases = [\n        {'X': D1, 'w0': np.array([0.6, 0.8])},\n        {'X': D1, 'w0': np.array([1.0, 0.0])},\n        {'X': D2, 'w0': np.array([-0.3, 0.95])},\n        {'X': D2, 'w0': np.array([10.0, -10.0])},\n        # Edge case w0 is very small\n        {'X': D1, 'w0': np.array([1e-9, -1e-9])},\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        X = case['X']\n        w0 = case['w0']\n        p, n = X.shape  # p features, n samples\n        \n        # Ensure w0 is a column vector for matrix operations\n        w0 = w0.reshape(p, 1)\n\n        # 1. Compute projections u = w0^T * X\n        # w0.T is (1, p), X is (p, n) -> u is (1, n)\n        u = w0.T @ X\n\n        # Apply nonlinearity g(u) = tanh(u)\n        g_u = np.tanh(u)\n\n        # Apply derivative of nonlinearity g'(u) = 1 - tanh^2(u)\n        g_prime_u = 1 - g_u**2\n\n        # 2. Compute the unnormalized updated vector w+\n        # This corresponds to the FastICA update rule:\n        # w+ = E[x * g(w^T*x)] - E[g'(w^T*x)] * w\n        # where expectations E are sample averages.\n\n        # First term: E[x * g(w^T*x)] = (1/n) * sum(xi * g(ui))\n        # In matrix form: (1/n) * X @ g_u.T\n        # X is (p, n), g_u.T is (n, 1) -> term1 is (p, 1)\n        term1 = (X @ g_u.T) / n\n\n        # Second term: E[g'(w^T*x)] * w = (1/n) * sum(g'(ui)) * w0\n        # In matrix form: np.mean(g_prime_u) * w0\n        term2 = np.mean(g_prime_u) * w0\n\n        w_plus = term1 - term2\n\n        # 3. Normalize the new vector to unit norm\n        norm_w_plus = np.linalg.norm(w_plus)\n        \n        # Handle the case where w_plus is the zero vector, although unlikely here.\n        if norm_w_plus == 0:\n            w_new = np.zeros_like(w_plus)\n        else:\n            w_new = w_plus / norm_w_plus\n            \n        # Format the result: flatten to a 1D array, round to 6 decimal places, and convert to list\n        result_vector = np.round(w_new.flatten(), 6).tolist()\n        results.append(result_vector)\n\n    # Final print statement in the exact required format.\n    # Convert each inner list to its string representation and join with commas.\n    str_results = [str(r).replace(\" \", \"\") for r in results]\n    print(f\"[{','.join(str_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Once an ICA algorithm has been applied to separate mixed signals, a critical final step is to evaluate how well it performed. This exercise  provides practical experience with the quantitative assessment of source separation quality, a common task in validating algorithm performance in a controlled setting where ground-truth sources are available. By calculating the Signal-to-Interference Ratio (SIR) and Signal-to-Distortion Ratio (SDR), you will learn to objectively measure the success of the separation, distinguishing contamination from other sources from distortion caused by noise and other artifacts.",
            "id": "3822206",
            "problem": "A remote sensing instrument records two spectral channels that linearly mix two independent environmental features: aerosol optical depth anomaly and vegetation index anomaly. Assume an instantaneous linear mixture with additive measurement noise, modeled as $x = A s + n$, where $x \\in \\mathbb{R}^{2 \\times T}$ is the observed data across $T$ spatial samples, $A \\in \\mathbb{R}^{2 \\times 2}$ is the mixing matrix, $s \\in \\mathbb{R}^{2 \\times T}$ stacks the two independent source signals, and $n \\in \\mathbb{R}^{2 \\times T}$ is additive noise. The ground-truth source signals across $T = 4$ samples are\n$$\ns_1 = [\\,2,\\,-2,\\,2,\\,-2\\,], \\quad s_2 = [\\,1,\\,1,\\,-1,\\,-1\\,],\n$$\nand the mixing matrix and noise are\n$$\nA = \\begin{pmatrix} 1.0  0.4 \\\\ 0.2  1.2 \\end{pmatrix}, \\quad\nn = \\begin{pmatrix}\n0.1  -0.1  0.0  0.05 \\\\\n-0.05  0.0  0.1  -0.05\n\\end{pmatrix}.\n$$\nAn Independent Component Analysis (ICA) algorithm estimates a demixing matrix $W \\in \\mathbb{R}^{2 \\times 2}$, yielding recovered components $y = W x$, with\n$$\nW = \\begin{pmatrix} 1.02  -0.35 \\\\ -0.15  0.88 \\end{pmatrix}.\n$$\nUsing energy-based blind source separation evaluation grounded in orthogonal projections onto the source subspace spanned by $\\{s_1, s_2\\}$, compute the Signal-to-Interference Ratio (SIR) and the Signal-to-Distortion Ratio (SDR) in decibels for the recovered component corresponding to $s_1$, that is, for the first recovered source $y_1$. Express both SIR and SDR in decibels and round your answers to three significant figures. Provide the final pair as a row matrix in the order $[\\,\\text{SIR},\\,\\text{SDR}\\,]$.",
            "solution": "The problem requires the computation of the Signal-to-Interference Ratio (SIR) and Signal-to-Distortion Ratio (SDR) for the first recovered component in an Independent Component Analysis (ICA) context.\n\nFirst, we establish the relationship between the recovered signals $y$, the source signals $s$, and the noise $n$. The observed signal is given by the linear mixing model:\n$$x = As + n$$\nThe recovered signal is obtained by applying the estimated demixing matrix $W$:\n$$y = Wx = W(As + n) = WAs + Wn$$\nLet's define the global transfer matrix $G = WA$ and the filtered noise $n' = Wn$. The equation becomes:\n$$y = Gs + n'$$\nThe problem specifies the matrices $A$ and $W$:\n$$A = \\begin{pmatrix} 1.0  0.4 \\\\ 0.2  1.2 \\end{pmatrix}, \\quad W = \\begin{pmatrix} 1.02  -0.35 \\\\ -0.15  0.88 \\end{pmatrix}$$\nWe compute the global matrix $G$:\n$$G = WA = \\begin{pmatrix} 1.02  -0.35 \\\\ -0.15  0.88 \\end{pmatrix} \\begin{pmatrix} 1.0  0.4 \\\\ 0.2  1.2 \\end{pmatrix}$$\n$$G = \\begin{pmatrix} (1.02)(1.0) + (-0.35)(0.2)  (1.02)(0.4) + (-0.35)(1.2) \\\\ (-0.15)(1.0) + (0.88)(0.2)  (-0.15)(0.4) + (0.88)(1.2) \\end{pmatrix}$$\n$$G = \\begin{pmatrix} 1.02 - 0.07  0.408 - 0.42 \\\\ -0.15 + 0.176  -0.06 + 1.056 \\end{pmatrix} = \\begin{pmatrix} 0.95  -0.012 \\\\ 0.026  0.996 \\end{pmatrix}$$\nThe recovered signals $y_1$ and $y_2$ (the rows of matrix $y$) can be expressed in terms of the source signals $s_1$ and $s_2$ (the rows of matrix $s$) and the filtered noise. We are interested in $y_1$:\n$$y_1 = G_{11}s_1 + G_{12}s_2 + n'_1$$\nwhere $n'_1$ is the first row of $n' = Wn$. Since $G_{11} \\approx 1$ and $G_{12} \\approx 0$, $y_1$ is the recovered component corresponding to the source $s_1$.\n\nThe evaluation metrics SIR and SDR are based on the decomposition of the recovered signal $y_1$ into three orthogonal components: a target signal component, an interference component, and an artifact (noise) component. This decomposition is achieved by orthogonal projection onto the source signal subspace.\nThe source signals are given as:\n$$s_1 = [\\,2,\\,-2,\\,2,\\,-2\\,], \\quad s_2 = [\\,1,\\,1,\\,-1,\\,-1\\,]$$\nWe check for orthogonality by computing their dot product:\n$$s_1 \\cdot s_2 = (2)(1) + (-2)(1) + (2)(-1) + (-2)(-1) = 2 - 2 - 2 + 2 = 0$$\nSince the sources are orthogonal, they form an orthogonal basis for the source subspace. The decomposition of $y_1$ is:\n- Target component: $s_{\\text{target}} = \\text{proj}_{s_1}(y_1) = \\frac{y_1 \\cdot s_1}{\\|s_1\\|^2} s_1$\n- Interference component: $e_{\\text{interf}} = \\text{proj}_{s_2}(y_1) = \\frac{y_1 \\cdot s_2}{\\|s_2\\|^2} s_2$\n- Artifact component: $e_{\\text{artif}} = y_1 - s_{\\text{target}} - e_{\\text{interf}}$\n\nThe SIR and SDR in decibels (dB) are defined by the energy (squared L2-norm) of these components:\n$$\\text{SIR} = 10 \\log_{10} \\left( \\frac{\\|s_{\\text{target}}\\|^2}{\\|e_{\\text{interf}}\\|^2} \\right)$$\n$$\\text{SDR} = 10 \\log_{10} \\left( \\frac{\\|s_{\\text{target}}\\|^2}{\\|e_{\\text{interf}} + e_{\\text{artif}}\\|^2} \\right)$$\nThe total distortion term is $e_{\\text{interf}} + e_{\\text{artif}} = y_1 - s_{\\text{target}}$.\n\nTo calculate these values, we first need to compute the recovered signal vector $y_1$.\nFirst, we find the observed signal matrix $x = As+n$.\n$$As = \\begin{pmatrix} 1.0  0.4 \\\\ 0.2  1.2 \\end{pmatrix} \\begin{pmatrix} 2  -2  2  -2 \\\\ 1  1  -1  -1 \\end{pmatrix} = \\begin{pmatrix} 2.4  -1.6  1.6  -2.4 \\\\ 1.6  0.8  -0.8  -1.6 \\end{pmatrix}$$\n$$n = \\begin{pmatrix} 0.1  -0.1  0.0  0.05 \\\\ -0.05  0.0  0.1  -0.05 \\end{pmatrix}$$\n$$x = As + n = \\begin{pmatrix} 2.5  -1.7  1.6  -2.35 \\\\ 1.55  0.8  -0.7  -1.65 \\end{pmatrix}$$\nLet $x_1$ and $x_2$ be the rows of $x$. Now we compute $y_1$, the first row of $y=Wx$.\n$$y_1 = W_{11}x_1 + W_{12}x_2 = 1.02 x_1 - 0.35 x_2$$\n$$y_1 = 1.02 [\\,2.5, -1.7, 1.6, -2.35\\,] - 0.35 [\\,1.55, 0.8, -0.7, -1.65\\,]$$\n$$y_1 = [\\,2.55, -1.734, 1.632, -2.397\\,] - [\\,0.5425, 0.28, -0.245, -0.5775\\,]$$\n$$y_1 = [\\,2.0075, -2.014, 1.877, -1.8195\\,]$$\nNext, we compute the necessary dot products and norms.\n$$\\|s_1\\|^2 = 2^2 + (-2)^2 + 2^2 + (-2)^2 = 16$$\n$$\\|s_2\\|^2 = 1^2 + 1^2 + (-1)^2 + (-1)^2 = 4$$\n$$y_1 \\cdot s_1 = (2.0075)(2) + (-2.014)(-2) + (1.877)(2) + (-1.8195)(-2) = 4.015 + 4.028 + 3.754 + 3.639 = 15.436$$\n$$y_1 \\cdot s_2 = (2.0075)(1) + (-2.014)(1) + (1.877)(-1) + (-1.8195)(-1) = 2.0075 - 2.014 - 1.877 + 1.8195 = -0.064$$\nNow we compute the energy of the target and interference components.\n$$\\|s_{\\text{target}}\\|^2 = \\left\\| \\frac{y_1 \\cdot s_1}{\\|s_1\\|^2} s_1 \\right\\|^2 = \\left(\\frac{15.436}{16}\\right)^2 \\|s_1\\|^2 = \\frac{15.436^2}{16^2} \\times 16 = \\frac{15.436^2}{16} = \\frac{238.270096}{16} = 14.891881$$\n$$\\|e_{\\text{interf}}\\|^2 = \\left\\| \\frac{y_1 \\cdot s_2}{\\|s_2\\|^2} s_2 \\right\\|^2 = \\left(\\frac{-0.064}{4}\\right)^2 \\|s_2\\|^2 = \\frac{(-0.064)^2}{4^2} \\times 4 = \\frac{0.004096}{4} = 0.001024$$\nWe can now calculate the SIR.\n$$\\text{SIR} = 10 \\log_{10} \\left( \\frac{14.891881}{0.001024} \\right) = 10 \\log_{10}(14542.8525...)= 41.6265... \\text{ dB}$$\nRounding to three significant figures, $\\text{SIR} = 41.6 \\text{ dB}$.\n\nFor the SDR, we need the energy of the total distortion, $\\|y_1 - s_{\\text{target}}\\|^2$. By the properties of orthogonal projection, $s_{\\text{target}}$ and $y_1 - s_{\\text{target}}$ are orthogonal. Therefore, by the Pythagorean theorem: $\\|y_1\\|^2 = \\|s_{\\text{target}}\\|^2 + \\|y_1 - s_{\\text{target}}\\|^2$.\nThe distortion energy is $\\|y_1 - s_{\\text{target}}\\|^2 = \\|y_1\\|^2 - \\|s_{\\text{target}}\\|^2$.\nFirst, calculate $\\|y_1\\|^2$:\n$$\\|y_1\\|^2 = 2.0075^2 + (-2.014)^2 + 1.877^2 + (-1.8195)^2$$\n$$\\|y_1\\|^2 = 4.03005625 + 4.056196 + 3.523129 + 3.31058025 = 14.9199615$$\nThe total distortion energy is:\n$$\\|y_1 - s_{\\text{target}}\\|^2 = 14.9199615 - 14.891881 = 0.0280805$$\nNow we calculate the SDR.\n$$\\text{SDR} = 10 \\log_{10} \\left( \\frac{\\|s_{\\text{target}}\\|^2}{\\|y_1 - s_{\\text{target}}\\|^2} \\right) = 10 \\log_{10} \\left( \\frac{14.891881}{0.0280805} \\right) = 10 \\log_{10}(530.324...)= 27.2454... \\text{ dB}$$\nRounding to three significant figures, $\\text{SDR} = 27.2 \\text{ dB}$.\n\nThe final answer is the pair $[\\,\\text{SIR},\\,\\text{SDR}\\,]$ with values rounded to three significant figures.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n41.6  27.2\n\\end{pmatrix}\n}\n$$"
        }
    ]
}