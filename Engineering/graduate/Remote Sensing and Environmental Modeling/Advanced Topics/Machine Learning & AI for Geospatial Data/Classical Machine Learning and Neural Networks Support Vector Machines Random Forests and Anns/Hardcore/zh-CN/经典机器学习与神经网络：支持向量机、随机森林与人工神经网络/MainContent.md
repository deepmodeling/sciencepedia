## 引言
机器学习，特别是[支持向量机](@entry_id:172128)（SVM）、[随机森林](@entry_id:146665)（RF）和人工神经网络（ANN）等经典模型，已成为现代数据科学的基石，彻底改变了我们从复杂数据中提取洞见与模式的方式。然而，将这些通用算法应用于[环境遥感](@entry_id:1124564)等特定科学领域时，我们面临着独特的挑战：数据的空间特性、传感器的物理限制以及科学发现对[模型可解释性](@entry_id:637866)的高要求，构成了理论与实践之间的巨大鸿沟。本文旨在弥合这一差距，为读者提供一个从理论基础到前沿应用的完整知识框架。

在接下来的内容中，我们将分三个章节展开探索。**第一章“原理与机制”**将深入剖析SVM、RF和ANN的核心数学原理，并探讨偏差-方差权衡、[空间自相关](@entry_id:177050)等所有模型共同面临的根本性问题。**第二章“应用与交叉学科联系”**将展示这些模型如何被创造性地应用于解决真实的遥感与[环境科学](@entry_id:187998)问题，从特征工程到高级集成，再到与物理学、地理学等学科的深度融合。最后，**第三章“动手实践”**将通过具体的计算问题，巩固您对核心概念的理解。通过本次学习，您将不仅掌握这些强大模型的运作方式，更能洞悉如何将它们有效地应用于充满挑战的科学研究中。

## 原理与机制

本章将深入探讨三种主流[机器学习算法](@entry_id:751585)的核心原理与机制：[支持向量机](@entry_id:172128) (Support Vector Machines, SVM)、[随机森林](@entry_id:146665) (Random Forests, RF) 和[人工神经网络](@entry_id:140571) (Artificial Neural Networks, ANNs)。我们将从每个模型的基[本构建模](@entry_id:183370)块出发，逐步解析其学习策略，并最终探讨在[环境遥感](@entry_id:1124564)建模中普遍存在的共通挑战，例如[模型评估](@entry_id:164873)、[泛化理论](@entry_id:635655)以及[数据异质性](@entry_id:918115)问题。

### [支持向量机](@entry_id:172128)：[最大间隔](@entry_id:633974)原理

[支持向量机](@entry_id:172128)是一种功能强大且理论完备的[分类与回归](@entry_id:898818)方法，其核心思想是**最大化间隔 (maximization of the margin)**。

#### 从线性分隔器到最优超平面

对于一个线性可分的数据集，[线性分类器](@entry_id:637554)的目标是找到一个超平面，将不同类别的样本分开。例如，一个简单的单层[感知器](@entry_id:143922)可以通过迭代学习算法找到这样一个[超平面](@entry_id:268044)，其决策函数形式为 $f(\mathbf{x}) = \mathrm{sign}(\mathbf{w}^\top \mathbf{x} + b)$。然而，对于一个给定的线性可分数据集，满足分离条件的[超平面](@entry_id:268044)通常有无数个。[感知器](@entry_id:143922)算法收敛时，可能会返回这些[可行解](@entry_id:634783)中的任何一个，但并非所有解的泛化性能都一样好。

[支持向量机](@entry_id:172128)通过引入一个更严格的优化目标来解决这个问题：在所有可行的[分离超平面](@entry_id:273086)中，寻找那个“最优”的超平面，即距离两个类别中最近的样本点最远的那个。直观上，这个[超平面](@entry_id:268044)位于两个类别之间的“街道”的正中央，这条“街道”越宽，模型对未知样本的分类就越有信心，其泛化能力和对噪声的鲁棒性也越强 。这个被最大化的“街道”宽度，就是**间隔 (margin)**。

#### 间隔的形式化定义：几何间隔与函数间隔

为了精确地定义和优化间隔，我们需要区分两种类型的间隔 。

给定一个样本点 $\mathbf{x}_i$ 和一个由 $(\mathbf{w}, b)$ 定义的[超平面](@entry_id:268044)，我们可以计算该点到超平面的有符号距离。如果我们将[超平面](@entry_id:268044)的法向量 $\mathbf{w}$ 归一化，就可以得到点 $\mathbf{x}_i$ 到超平面的**几何间隔 (geometric margin)** $\gamma_i$：

$$
\gamma_i = \frac{y_i(\mathbf{w}^\top \mathbf{x}_i + b)}{\lVert \mathbf{w} \rVert}
$$

其中 $y_i \in \{-1, +1\}$ 是样本的类别标签。几何间隔表示了点到[超平面](@entry_id:268044)的真实欧氏距离，它具有一个重要特性：**尺度不变性**。如果我们对参数 $(\mathbf{w}, b)$ 进行任意正标量 $c$ 的缩放，即变为 $(c\mathbf{w}, cb)$，超平面的位置不会改变，几何间隔的值也保持不变。

然而，在优化问题中直接处理带有 $\lVert \mathbf{w} \rVert$ 的分式会很麻烦。为此，我们引入**函数间隔 (functional margin)** $\hat{\gamma}_i$：

$$
\hat{\gamma}_i = y_i(\mathbf{w}^\top \mathbf{x}_i + b)
$$

函数间隔不具有尺度不变性，它会随着参数 $(\mathbf{w}, b)$ 的缩放而线性变化。它与几何间隔的关系是 $\gamma_i = \hat{\gamma}_i / \lVert \mathbf{w} \rVert$。

#### 硬间隔[支持向量机](@entry_id:172128)的优化问题

利用函数间隔的尺度可变性，我们可以简化优化问题。尽[管存](@entry_id:1127299)在无限多组参数 $(\mathbf{w}, b)$ 对应同一个超平面，但我们可以通过缩放来选择一个“规范”的代表。具体而言，我们可以设定数据集中所有样本的最小函数间隔 $\hat{\gamma} = \min_i \hat{\gamma}_i$ 为 $1$。

在这个规范化条件下，即对于所有样本都满足 $y_i(\mathbf{w}^\top \mathbf{x}_i + b) \ge 1$，最大化几何间隔 $\gamma = \hat{\gamma} / \lVert \mathbf{w} \rVert = 1 / \lVert \mathbf{w} \rVert$ 就等价于最小化 $\lVert \mathbf{w} \rVert$。为了数学上的便利（将问题转化为一个凸二次规划问题），我们通常最小化 $\frac{1}{2}\lVert \mathbf{w} \rVert^2$。

因此，**硬间隔 (hard-margin)** 线性[支持向量机](@entry_id:172128)的**原始优化问题 (primal optimization problem)** 可以形式化地表述为 ：

$$
\min_{\mathbf{w}, b} \frac{1}{2}\lVert \mathbf{w} \rVert^2 \quad \text{subject to} \quad y_i(\mathbf{w}^\top \mathbf{x}_i + b) \geq 1 \quad \text{for all } i=1, \dots, n
$$

那些恰好落在间隔边界上（即满足 $y_i(\mathbf{w}^\top \mathbf{x}_i + b) = 1$）的训练样本被称为**[支持向量](@entry_id:638017) (support vectors)**，它们是唯一对最终超平面位置起决定性作用的数据点。

#### 泛化能力与间隔的角色

最大化间隔不仅在直觉上是合理的，其优越性更有坚实的[统计学习理论](@entry_id:274291)作为支撑。一个关键的理论结果是，对于[线性分类器](@entry_id:637554)，其[泛化误差](@entry_id:637724)的界限可以不依赖于[特征空间](@entry_id:638014)的维度 $d$，而是依赖于间隔的大小 。

根据经典的VC理论 (Vapnik-Chervonenkis theory)，$\mathbb{R}^d$ 空间中仿射线性分隔器（即[超平面](@entry_id:268044)）的[VC维](@entry_id:636849)是 $d+1$。这意味着，基于[VC维](@entry_id:636849)的[泛化误差](@entry_id:637724)界通常会随着维度 $d$ 的增加而变差。然而，对于在半径为 $R$ 的球内的数据，如果一个线性SVM能够以几何间隔 $\gamma$ 将其分开，那么其[泛化误差](@entry_id:637724)的界限大致与 $(\frac{R}{\gamma})^2$ 成正比，而与维度 $d$ 无关。

这个**维度无关 (dimension-independent)** 的特性解释了为什么SVM即使在遥感等具有非常高维特征（如[高光谱数据](@entry_id:1126305)）的领域中也能表现出色。只要能找到一个大间隔的解，模型就能有效避免“维度灾难”并获得良好的泛化性能。此外，更大的几何间隔也意味着需要更大的扰动才能使一个样本被错误分类，这表明模型对输入特征的噪声（如[传感器噪声](@entry_id:1131486)）具有更强的鲁棒性 。

#### 处理多类别问题

SVM本质上是一个[二元分类器](@entry_id:911934)。在遥感中，土地覆盖分类等任务通常涉及多个类别 ($K > 2$)。为了将SVM应用于多类别问题，需要采用特定的策略将其分解为多个[二元分类](@entry_id:142257)子问题。两种最经典的策略是“一对余” (One-vs-Rest, OvR) 和“一对一” (One-vs-One, OvO) 。

-   **一对余 (OvR)**：该策略为 $K$ 个类别中的每一个类别训练一个[二元分类器](@entry_id:911934)。第 $k$ 个分类器将类别 $k$ 的样本视为正类，将所有其他 $K-1$ 个类别的样本视为负类。因此，这需要训练 $K$ 个分类器。在预测时，一个新样本会被输入到所有 $K$ 个分类器中，最终被赋予得分最高的那个分类器所对应的类别。OvR的一个潜在问题是，每个[二元分类器](@entry_id:911934)的训练集可能存在严重的[类别不平衡](@entry_id:636658)。

-   **一对一 (OvO)**：该策略为每一对类别 $(i, j)$ 训练一个[二元分类器](@entry_id:911934)。因此，总共需要训练 $\binom{K}{2} = \frac{K(K-1)}{2}$ 个分类器。每个分类器仅使用来自这两个类别的样本进行训练。在预测时，一个新样本会被输入到所有 $\frac{K(K-1)}{2}$ 个分类器中，每个分类器为其区分的两个类别之一“投票”。最终，获得最多投票的类别成为预测结果。OvO需要训练更多的分类器，但在每个子问题上训练的数据量更少，且不存在[类别不平衡](@entry_id:636658)问题。

在实践中，当类别数 $K$ 较大时，OvO的训练成本（训练分类器的数量）可能很高，但其预测速度可能比OvR慢，因为需要进行更多次的评估。

### [随机森林](@entry_id:146665)：通过决策树进行[集成学习](@entry_id:1124521)

[随机森林](@entry_id:146665)是一种强大而灵活的集成学习方法，它通过构建并结合大量的**[决策树](@entry_id:265930) (decision trees)** 来提升预测性能和鲁棒性。

#### 构建模块：决策树与分裂准则

单棵决策树通过对[特征空间](@entry_id:638014)进行递归的、轴对齐的划分来工作。在每个内部节点，算法选择一个[特征和](@entry_id:189446)一个阈值，将该节点的数据集分成两个子集，目标是使分裂后的子节点尽可能“纯净”。

那么，如何量化一个节点的“纯度”呢？一个常用的度量是**[基尼不纯度](@entry_id:147776) (Gini impurity)**。我们可以从第一性原理出发来理解它 。假设在一个节点中，类别 $k$ 的样本比例为 $p_k$。如果我们从该节点随机抽取一个样本，其真实类别为 $k$ 的概率是 $p_k$。现在，我们构建一个随机分类器，它也根据节点内的类别分布 $p_k$ 来独立地预测一个标签。那么，这个随机分类器预测标签为 $j$ 的概率是 $p_j$。由于真实标签的抽取和预测标签的抽取是独立的，分类正确的概率是真实类别和预测类别恰好相同的概率之和：

$$
P(\text{correct}) = \sum_{k=1}^{K} P(\text{true}=k \text{ and } \text{predicted}=k) = \sum_{k=1}^{K} p_k \cdot p_k = \sum_{k=1}^{K} p_k^2
$$

[基尼不纯度](@entry_id:147776)被定义为分类错误的概率，即：

$$
I_G(\{p_k\}) = 1 - P(\text{correct}) = 1 - \sum_{k=1}^{K} p_k^2
$$

在构建决策树时，算法会评估所有可能的分裂（即所有[特征和](@entry_id:189446)所有可能的阈值），并选择那个能够最大化**不纯度减少量 (reduction in impurity)** 的分裂。这个减少量，也称为**基尼增益 (Gini gain)**，计算为父节点的不纯度减去其子节点不纯度的加权平均值。例如，在一个关于[植被指数](@entry_id:1133751)NDVI的[分类任务](@entry_id:635433)中，通过计算不同NDVI阈值分裂带来的基尼增益，算法可以找到最优的分[割点](@entry_id:637448)来区分浓密植被、稀疏植被和非植被区域 。

#### 从单棵树到森林：[自助聚合](@entry_id:902297)与随机特征子空间

单棵深度决策树虽然能够学习复杂的[决策边界](@entry_id:146073)，但它对训练数据的微小变化非常敏感，容易过拟合，即具有高方差 (high variance)。[随机森林](@entry_id:146665)通过集成大量[决策树](@entry_id:265930)来解决这个问题，其核心是引入随机性以降低树之间的相关性。这主要通过两种方式实现：

1.  **[自助聚合](@entry_id:902297) (Bootstrap Aggregating, [Bagging](@entry_id:145854))**：对于要构建的每棵树，算法不是使用全部训练数据，而是从原始训练集中进行有放回的随机抽样，创建一个与原始数据集大小相同的自助样本集 (bootstrap sample)。这意味着每棵树都在一个略有不同的数据集上进行训练。

2.  **随机特征子空间 (Random Feature Subspace)**：在每个节点寻找最佳分裂时，算法不会在所有 $d$ 个特征中进行搜索，而是首先随机选择一个由 $m_{try}$ 个特征组成的子集 ($m_{try} \ll d$)，然后仅在这个子集中寻找最佳分裂点。

#### 偏差、方差与平均的力量

随机森林的强大威力源于**偏差-方差权衡 (bias-variance tradeoff)** 的巧妙处理 。单棵[决策树](@entry_id:265930)（尤其是深度较大的树）通常是低偏差但高方差的模型。通过[Bagging](@entry_id:145854)和随机[特征选择](@entry_id:177971)，[随机森林](@entry_id:146665)生成了大量（数百或数千棵）近似无偏但彼此之间相关性较弱的树。

在进行预测时，随机森林将所有树的预测结果进行平均（对于回归任务）或投票（对于[分类任务](@entry_id:635433)）。根据统计学原理，对大量近似独立（或弱相关）的[随机变量](@entry_id:195330)求平均，其结果的方差会显著降低。具体来说，如果单棵树预测的方差为 $\sigma^2$，树之间的平均相关系数为 $\rho$，那么由 $M$ 棵树组成的森林的预测方差约为 $\rho\sigma^2 + \frac{1-\rho}{M}\sigma^2$。随着树的数量 $M$ 增加，方差的第二项会趋近于零，从而有效降低整体模型的方差。与此同时，由于单棵树是低偏差的，其平均结果的偏差也保持在较低水平。因此，增加森林中树的数量 ($M$) 主要是为了降低方差，而对偏差影响不大。

### 人工神经网络：学习层次化表征

[人工神经网络](@entry_id:140571)（ANNs）受到生物神经系统的启发，它通过连接大量简单的处理单元（神经元）来构建复杂的、可学习的函数映射。

#### 神经元模型与单层[感知器](@entry_id:143922)

ANN的[基本单位](@entry_id:148878)是人工神经元。一个简单的神经元模型，如**[感知器](@entry_id:143922) (perceptron)**，接收一组输入信号，计算它们的加权和，然后通过一个**[激活函数](@entry_id:141784) (activation function)**（如[符号函数](@entry_id:167507)或[Sigmoid函数](@entry_id:137244)）产生输出 。一个单层[感知器](@entry_id:143922)本质上就是一个[线性分类器](@entry_id:637554)，其能力有限，只能解决线性可分问题。

#### 从浅层到深层：多层网络

ANN的强大能力来自于将神经元组织成多个层次。一个典型的**[多层感知器](@entry_id:636847) (Multi-Layer Perceptron, MLP)** 由一个输入层、一个或多个隐藏层和一个输出层组成。通过在层与层之间引入[非线性激活函数](@entry_id:635291)（如ReLU, Rectified Linear Unit），[多层网络](@entry_id:261728)能够学习输入特征之间高度复杂的[非线性](@entry_id:637147)关系。每一层可以被看作是对其前一层输出的再加工，从而形成一个**层次化表征 (hierarchical representation)**。浅层可能学习边缘、颜色等简单特征，而深层则能组合这些简单特征，学习物体的部件、纹理乃至更抽象的概念。

#### [卷积神经网络](@entry_id:178973)（CNNs）在[空间数据](@entry_id:924273)中的应用

在处理遥感影像这类具有网格结构的[空间数据](@entry_id:924273)时，标准的全连接ANN存在参数过多、计算效率低且无法有效利用[空间局部性](@entry_id:637083)等问题。**卷积神经网络 (Convolutional Neural Networks, CNNs)** 是一种专门为此[类数](@entry_id:156164)据设计的ANN架构。其核心思想包括：

-   **局部连接 (Local Connectivity)**：每个神经元只与前一层的一个小局部区域（感受野）相连，而不是所有神经元。
-   **[权值共享](@entry_id:633885) (Weight Sharing)**：一个卷积核（或滤波器）在整个输入图像上滑动，用同一组权重在不同位置提取相同的特征，这大大减少了模型参数。
-   **[平移等变性](@entry_id:636340) (Translation Equivariance)**：由于[权值共享](@entry_id:633885)，如果输入中的一个模式（如一棵树）发生平移，其在[特征图](@entry_id:637719)中的响应也会相应平移，这使得CNN能自然地处理物体在空间中的位置变化。

一个关键概念是**感受野 (receptive field)**，它指的是输出[特征图](@entry_id:637719)上一个单元的激活值所依赖的输入图像的区域大小。[感受野](@entry_id:636171)的大小由网络中卷积层的**核大小 (kernel size)** 和**步长 (stride)** 共同决定。例如，一个两层CNN，第一层使用 $5 \times 5$ 的核和步长为 $2$，第二层使用 $3 \times 3$ 的核和步长为 $2$，那么第二层输出的一个单元的[感受野](@entry_id:636171)将覆盖输入图像上 $9 \times 9$ 的像素区域。这意味着该网络能够从一个 $90 \text{ m} \times 90 \text{ m}$ 的地面区域（假设像素分辨率为 $10 \text{ m}$）中聚合信息来生成一个高级特征 。

#### 应对ANNs的[过拟合](@entry_id:139093)：[正则化技术](@entry_id:261393)

由于参数量巨大，深度神经网络极易发生[过拟合](@entry_id:139093)。为了提高其泛化能力，必须采用[正则化技术](@entry_id:261393)。这些技术通常通过限制[模型复杂度](@entry_id:145563)来降低方差，代价是可能轻微增加偏差 。三种最常用的[正则化方法](@entry_id:150559)是 ：

1.  **$L_2$ 权值衰减 (Weight Decay)**：在损失函数中加入一个与网络权重[平方和](@entry_id:161049)成正比的惩罚项 $\lambda \lVert \mathbf{w} \rVert_2^2$。这会驱使[优化算法](@entry_id:147840)学习到更小的权重值，使得模型输出对输入特征的微小变化不那么敏感，从而降低了模型的方差。

2.  **Dropout**：在训练过程的每次前向传播中，以一定的概率 $p$ 随机地将一部分神经元的输出“丢弃”（置为零）。这迫使网络不能依赖于任何一个特定的神经元，而是要学习更加鲁棒和分布式的特征。在效果上，Dropout近似于对大量共享权重的“稀疏化”子网络进行集成平均，是一种非常有效的方差降低技术。

3.  **[早停](@entry_id:633908) (Early Stopping)**：在训练过程中，除了监控训练集上的损失外，还同时在一个独立的[验证集](@entry_id:636445)上监控性能。[训练误差](@entry_id:635648)通常会持续下降，但验证误差在下降到某个点后会开始上升，这标志着模型开始过拟合。[早停](@entry_id:633908)策略就是在验证误差达到最小值时停止训练，从而选择了一个在参数空间中尚未完全收敛到训练集最优解、但对[验证集](@entry_id:636445)泛化能力更好的模型。这本质上是一种隐式的复杂度控制。

### 跨模型挑战与遥感应用考量

在将这些模型应用于[环境遥感](@entry_id:1124564)建模时，我们必须面对一些超越特定[模型选择](@entry_id:155601)的、源于数据本身特性的共同挑战。

#### 偏差-方差权衡的统一视角

所[有监督学习](@entry_id:161081)模型都在试图解决[偏差-方差权衡](@entry_id:138822)问题 。
-   对于**SVM**，模型复杂度由核函数类型及其超参数（如高斯核的带宽 $\gamma$）和[正则化参数](@entry_id:162917) $C$ 控制。增加 $C$ 或 $\gamma$ 会增加[模型容量](@entry_id:634375)，通常会降低偏差但增加方差，在小样本或高噪声情况下容易导致过拟合。
-   对于**随机森林**，复杂度主要由树的深度和节点分裂的规则决定。增加树的深度会降低偏差但增加方差，而增加森林中树的数量 $M$ 主要用于降低方差。
-   对于**ANN**，复杂度由网络的大小（层数和神经元数量）决定。[正则化技术](@entry_id:261393)（如$L_2$衰减、Dropout）是控制有效复杂度、降低方差的关键手段。

在遥感应用中，训练[样本量](@entry_id:910360)、特征维度和噪声水平共同决定了哪种复杂度的模型能够达到最佳的泛化性能。

#### 空间自相关的挑战

遥感数据的一个固有特性是**空间自相关 (spatial autocorrelation)**，即地理上邻近的观测值比远离的观测值更相似（“地理学第一定律”）。例如，在[土地覆盖](@entry_id:1127047)分类中，一个森林像元极有可能被其他森林像元包围 。

这种特性严重违反了大多数机器学习算法及其标准验证方法所依赖的**[独立同分布](@entry_id:169067) (Independent and Identically Distributed, IID)** 假设。当我们使用标准的**随机k折[交叉验证](@entry_id:164650)**时，训练集中的像元与其空间邻近的[测试集](@entry_id:637546)像元之间存在信息泄露。模型可以轻易地通过“记住”邻近的训练样本来“预测”测试样本，而不是真正学习到从光谱特征到地物类别的泛化规律。这会导致对模型精度的**系统性高估**。值得注意的是，即便是[随机森林](@entry_id:146665)的**袋外 (Out-of-Bag, OOB)** 误差估计也无法幸免，因为它同样受到[空间自相关](@entry_id:177050)的影响。

评估[空间自相关](@entry_id:177050)的常用指标是**[莫兰指数](@entry_id:192667) ([Moran's I](@entry_id:192667))**，其值接近+1表示强烈的正[空间[自相](@entry_id:177050)关](@entry_id:138991)。为了获得更可靠、无偏的性能评估，必须采用**[空间交叉验证](@entry_id:1132035) (spatial cross-validation)**。这种方法通过将数据划分为空间上连续的块（例如，地理区块或缓冲区），并确保训练集和[测试集](@entry_id:637546)在空间上是分离的，从而模拟模型在全新地理区域的泛化能力。

#### 跨传感器泛化的挑战：域偏移

遥感研究常常需要整合来自不同传感器（如Landsat-8和Sentinel-2）的数据。尽管它们观测的是同一地物，但由于**光谱[响应函数](@entry_id:142629) (Spectral Response Functions, SRFs)**、[空间分辨率](@entry_id:904633)、[信噪比](@entry_id:271861)等物理特性的差异，它们生成的[特征向量](@entry_id:151813)分布是不同的 。

这种训练数据（源域）和测试数据（目标域）之间的分布差异被称为**域偏移 (domain shift)**。在概率上，这意味着源域的[联合分布](@entry_id:263960) $P_s(X,Y)$ 与目标域的[联合分布](@entry_id:263960) $P_t(X,Y)$ 不同。

-   一个特殊情况是**[协变量偏移](@entry_id:636196) (covariate shift)**，即只有特征的[边际分布](@entry_id:264862)发生变化 ($P_s(X) \neq P_t(X)$)，而特征与标签之间的条件关系保持不变 ($P_s(Y|X) = P_t(Y|X)$)。在这种情况下，可以通过对训练样本进行[重要性加权](@entry_id:636441)等方法来修正模型。

-   然而，在跨传感器应用中，情况通常更复杂。不同的SRF意味着相同地物在不同传感器下的光谱特征不同，这不仅导致了 $P(X)$ 的变化，也可能改变了从特征推断类别的关系，即 $P(Y|X)$ 也发生了变化（这有时被称为**概念漂移 (concept drift)**）。例如，一种在Sentinel-2上光谱特征鲜明的植被，在Landsat-8较宽的波段下可能与另一种植被难以区分。

这种更普遍的域偏移对所有模型都构成了根本性挑战。一个在Landsat-8数据上训练的SVM、[随机森林](@entry_id:146665)或ANN，其学习到的决策边界、分裂规则或权重参数对于Sentinel-2数据来说是次优甚至错误的。简单地对数据进行[标准化](@entry_id:637219)或重加权往往不足以解决问题，需要更高级的**[域适应](@entry_id:637871) (domain adaptation)** 技术来学习跨传感器不变的特征表示。