{
    "hands_on_practices": [
        {
            "introduction": "This first exercise takes us back to the foundational principles of sequential data assimilation. We will derive the celebrated Kalman Filter equations not from memory, but from the bedrock of Bayesian inference for a simple linear Gaussian system. By tracking the stage of a river with a single satellite observation, you will perform a full prediction-update cycle and explore the filter's long-term behavior, gaining a concrete understanding of how it balances model forecasts with new data to reduce uncertainty .",
            "id": "3804790",
            "problem": "A river reach is monitored by satellite altimetry, providing remote sensing measurements of river stage. Consider a scalar discrete-time linear Gaussian state-space model for river stage in meters, where the state $x_k$ represents the true stage at time step $k$, governed by a random-walk dynamic and a direct measurement with noise:\n$$\nx_k = x_{k-1} + w_{k-1}, \\quad w_{k-1} \\sim \\mathcal{N}(0, Q),\n$$\n$$\ny_k = x_k + v_k, \\quad v_k \\sim \\mathcal{N}(0, R),\n$$\nwith known parameters $A=1$, $H=1$, $Q=0.1\\ \\mathrm{m}^2$, and $R=0.5\\ \\mathrm{m}^2$. Assume the initial posterior (analysis) at $k=0$ is $\\hat{x}_0^{+} = 2.00\\ \\mathrm{m}$ and $P_0^{+} = 1.00\\ \\mathrm{m}^2$. At $k=1$, a single altimetric observation $y_1 = 2.30\\ \\mathrm{m}$ is received.\n\nStarting from the definition of the linear Gaussian state-space model and Bayesâ€™ rule for Gaussian random variables, derive the scalar Kalman Filter (KF) prediction and update steps for the mean and variance without invoking any pre-memorized formulas. Execute one full KF cycle at $k=1$ to obtain $\\hat{x}_1^{-}$, $P_1^{-}$, the Kalman gain $K_1$, and the updated $\\hat{x}_1^{+}$, $P_1^{+}$.\n\nThen, using only the covariance recursion implied by this model (and independent of any particular measurement values), assess whether the posterior variance $P_k^{+}$ converges to a steady-state as $k \\to \\infty$. If it does, derive the closed-form steady-state posterior variance $P^{*}$ as a function of $Q$ and $R$, and evaluate it numerically for the given $Q$ and $R$.\n\nExpress the final steady-state posterior variance in $\\mathrm{m}^2$ and round your answer to four significant figures.",
            "solution": "The problem statement has been validated and is deemed sound. It is scientifically grounded in standard estimation theory (Kalman filtering), well-posed with a unique and meaningful solution, objective, and self-contained. Therefore, a full solution is warranted.\n\nThe problem asks for a derivation of the scalar Kalman Filter (KF) equations from first principles for a linear Gaussian state-space model, a single-step execution of the filter, and a steady-state analysis of the posterior covariance.\n\nThe state-space model is given by:\nState (process) model:\n$$x_k = x_{k-1} + w_{k-1}, \\quad w_{k-1} \\sim \\mathcal{N}(0, Q)$$\nMeasurement model:\n$$y_k = x_k + v_k, \\quad v_k \\sim \\mathcal{N}(0, R)$$\nwhere $x_k$ is the true state at time step $k$, $y_k$ is the measurement, and $w_k$ and $v_k$ are independent, zero-mean Gaussian noise processes with variances $Q$ and $R$, respectively.\n\nThe KF operates in a two-step cycle: prediction and update. We start from the posterior distribution at time $k-1$, which is Gaussian: $p(x_{k-1} | y_{1:k-1}) = \\mathcal{N}(x_{k-1}; \\hat{x}_{k-1}^{+}, P_{k-1}^{+})$, where $\\hat{x}_{k-1}^{+}$ is the posterior mean (analysis estimate) and $P_{k-1}^{+}$ is the posterior variance.\n\n**1. Derivation of the Prediction (Time Update) Step**\n\nThe goal of the prediction step is to compute the prior distribution for the state at time $k$, conditioned on all measurements up to time $k-1$, i.e., $p(x_k | y_{1:k-1})$. This is found by marginalizing over $x_{k-1}$:\n$$p(x_k | y_{1:k-1}) = \\int p(x_k | x_{k-1}, y_{1:k-1}) p(x_{k-1} | y_{1:k-1}) dx_{k-1}$$\nGiven the Markovian nature of the system, $p(x_k | x_{k-1}, y_{1:k-1}) = p(x_k | x_{k-1})$. The state model gives this as $p(x_k | x_{k-1}) = \\mathcal{N}(x_k; x_{k-1}, Q)$.\nThe integral represents the convolution of two Gaussian distributions: $p(x_k | y_{1:k-1})$ is the distribution of the sum of two independent random variables, $x_{k-1} \\sim \\mathcal{N}(\\hat{x}_{k-1}^{+}, P_{k-1}^{+})$ and $w_{k-1} \\sim \\mathcal{N}(0, Q)$.\nThe mean of the sum is the sum of the means, and the variance of the sum is the sum of the variances.\nThe resulting distribution is a Gaussian, $p(x_k | y_{1:k-1}) = \\mathcal{N}(x_k; \\hat{x}_k^{-}, P_k^{-})$, where $\\hat{x}_k^{-}$ is the prior mean (forecast estimate) and $P_k^{-}$ is the prior variance.\n\nThe prior mean is:\n$$\\hat{x}_k^{-} = \\mathbb{E}[x_k | y_{1:k-1}] = \\mathbb{E}[x_{k-1} + w_{k-1} | y_{1:k-1}] = \\mathbb{E}[x_{k-1} | y_{1:k-1}] + \\mathbb{E}[w_{k-1}] = \\hat{x}_{k-1}^{+} + 0$$\n$$\\hat{x}_k^{-} = \\hat{x}_{k-1}^{+}$$\nThe prior variance is:\n$$P_k^{-} = \\text{Var}(x_k | y_{1:k-1}) = \\text{Var}(x_{k-1} + w_{k-1} | y_{1:k-1}) = \\text{Var}(x_{k-1} | y_{1:k-1}) + \\text{Var}(w_{k-1}) = P_{k-1}^{+} + Q$$\n$$P_k^{-} = P_{k-1}^{+} + Q$$\n\n**2. Derivation of the Update (Measurement Update) Step**\n\nThe update step incorporates the new measurement $y_k$ to refine the prior estimate, computing the posterior distribution $p(x_k | y_{1:k})$. Using Bayes' rule:\n$$p(x_k | y_{1:k}) = p(x_k | y_k, y_{1:k-1}) \\propto p(y_k | x_k, y_{1:k-1}) p(x_k | y_{1:k-1})$$\nThe first term is the likelihood, $p(y_k | x_k) = \\mathcal{N}(y_k; x_k, R)$, and the second is the prior, $p(x_k | y_{1:k-1}) = \\mathcal{N}(x_k; \\hat{x}_k^{-}, P_k^{-})$.\nThe product of two Gaussian probability density functions is proportional to another Gaussian. We can find its parameters by working with the log-likelihood:\n$$\\ln p(x_k | y_{1:k}) = C - \\frac{1}{2R}(y_k - x_k)^2 - \\frac{1}{2P_k^{-}}(x_k - \\hat{x}_k^{-})^2$$\nExpanding the quadratic terms in $x_k$:\n$$\\ln p(x_k | y_{1:k}) = C' - \\frac{1}{2} \\left[ x_k^2 \\left(\\frac{1}{R} + \\frac{1}{P_k^{-}}\\right) - 2x_k \\left(\\frac{y_k}{R} + \\frac{\\hat{x}_k^{-}}{P_k^{-}}\\right) \\right]$$\nThe posterior distribution is $p(x_k | y_{1:k}) = \\mathcal{N}(x_k; \\hat{x}_k^{+}, P_k^{+})$, whose log-form is $C'' - \\frac{1}{2P_k^{+}}(x_k - \\hat{x}_k^{+})^2 = C'' - \\frac{1}{2} \\left(\\frac{x_k^2}{P_k^{+}} - \\frac{2x_k \\hat{x}_k^{+}}{P_k^{+}}\\right)$.\nBy comparing coefficients of $x_k^2$:\n$$\\frac{1}{P_k^{+}} = \\frac{1}{R} + \\frac{1}{P_k^{-}} \\implies P_k^{+} = \\left( (P_k^{-})^{-1} + R^{-1} \\right)^{-1} = \\frac{P_k^{-} R}{P_k^{-} + R}$$\nBy comparing coefficients of $x_k$:\n$$\\frac{\\hat{x}_k^{+}}{P_k^{+}} = \\frac{y_k}{R} + \\frac{\\hat{x}_k^{-}}{P_k^{-}} \\implies \\hat{x}_k^{+} = P_k^{+} \\left(\\frac{y_k}{R} + \\frac{\\hat{x}_k^{-}}{P_k^{-}}\\right)$$\n$$\\hat{x}_k^{+} = \\frac{P_k^{-} R}{P_k^{-} + R} \\left(\\frac{y_k P_k^{-} + \\hat{x}_k^{-} R}{R P_k^{-}}\\right) = \\frac{y_k P_k^{-} + \\hat{x}_k^{-} R}{P_k^{-} + R}$$\nTo get the standard form, we rearrange:\n$$\\hat{x}_k^{+} = \\frac{\\hat{x}_k^{-}(P_k^{-} + R) - \\hat{x}_k^{-} R + y_k P_k^{-}}{P_k^{-} + R} = \\hat{x}_k^{-} + \\frac{P_k^{-}}{P_k^{-} + R}(y_k - \\hat{x}_k^{-})$$\nWe define the Kalman Gain $K_k = \\frac{P_k^{-}}{P_k^{-} + R}$. The update equations become:\n$$\\hat{x}_k^{+} = \\hat{x}_k^{-} + K_k (y_k - \\hat{x}_k^{-})$$\n$$P_k^{+} = (1 - K_k)P_k^{-}$$\nThis completes the derivation.\n\n**3. Execution of One KF Cycle at $k=1$**\n\nGiven: $\\hat{x}_0^{+} = 2.00$, $P_0^{+} = 1.00$, $Q = 0.1$, $R = 0.5$, and $y_1 = 2.30$.\n\n**Prediction step:**\n$$\\hat{x}_1^{-} = \\hat{x}_0^{+} = 2.00$$\n$$P_1^{-} = P_0^{+} + Q = 1.00 + 0.1 = 1.10$$\n\n**Update step:**\nKalman Gain:\n$$K_1 = \\frac{P_1^{-}}{P_1^{-} + R} = \\frac{1.10}{1.10 + 0.5} = \\frac{1.10}{1.60} = \\frac{11}{16} = 0.6875$$\nUpdated mean:\n$$\\hat{x}_1^{+} = \\hat{x}_1^{-} + K_1(y_1 - \\hat{x}_1^{-}) = 2.00 + 0.6875(2.30 - 2.00) = 2.00 + 0.6875(0.30) = 2.00 + 0.20625 = 2.20625$$\nUpdated variance:\n$$P_1^{+} = (1 - K_1)P_1^{-} = (1 - 0.6875)(1.10) = (0.3125)(1.10) = 0.34375$$\n\nThe results for the first cycle are: $\\hat{x}_1^{-} = 2.00\\ \\mathrm{m}$, $P_1^{-} = 1.10\\ \\mathrm{m}^2$, $K_1=0.6875$, $\\hat{x}_1^{+}=2.20625\\ \\mathrm{m}$, and $P_1^{+}=0.34375\\ \\mathrm{m}^2$.\n\n**4. Steady-State Analysis**\n\nThe posterior variance $P_k^{+}$ evolves according to a recursion independent of the measurements:\n$$P_k^{-} = P_{k-1}^{+} + Q$$\n$$P_k^{+} = \\frac{R P_k^{-}}{P_k^{-} + R}$$\nSubstituting the first equation into the second gives a single recurrence relation for $P_k^{+}$:\n$$P_k^{+} = \\frac{R(P_{k-1}^{+} + Q)}{(P_{k-1}^{+} + Q) + R}$$\nThis is a convergent sequence because the mapping function $f(p) = \\frac{R(p+Q)}{p+Q+R}$ is a contraction mapping for non-negative $p$, $Q$, and $R$. Thus, the posterior variance converges to a steady-state value $P^{*} = \\lim_{k \\to \\infty} P_k^{+}$.\nAt steady-state, $P_k^{+} = P_{k-1}^{+} = P^{*}$. We solve the following equation for $P^{*}$:\n$$P^{*} = \\frac{R(P^{*} + Q)}{P^{*} + Q + R}$$\n$$P^{*}(P^{*} + Q + R) = R(P^{*} + Q)$$\n$$(P^{*})^2 + P^{*}Q + P^{*}R = P^{*}R + QR$$\n$$(P^{*})^2 + Q P^{*} - QR = 0$$\nThis is a quadratic equation for $P^{*}$. Using the quadratic formula, $P^{*} = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$, with $a=1$, $b=Q$, and $c=-QR$:\n$$P^{*} = \\frac{-Q \\pm \\sqrt{Q^2 - 4(1)(-QR)}}{2} = \\frac{-Q \\pm \\sqrt{Q^2 + 4QR}}{2}$$\nSince variance must be a non-negative quantity ($P^{*} \\ge 0$), we must take the positive root:\n$$P^{*} = \\frac{-Q + \\sqrt{Q^2 + 4QR}}{2}$$\n\n**Numerical Evaluation of Steady-State Variance**\n\nSubstituting the given values $Q=0.1\\ \\mathrm{m}^2$ and $R=0.5\\ \\mathrm{m}^2$ into the closed-form expression for $P^{*}$:\n$$P^{*} = \\frac{-0.1 + \\sqrt{(0.1)^2 + 4(0.1)(0.5)}}{2}$$\n$$P^{*} = \\frac{-0.1 + \\sqrt{0.01 + 0.2}}{2}$$\n$$P^{*} = \\frac{-0.1 + \\sqrt{0.21}}{2}$$\n$$P^{*} = \\frac{-0.1 + 0.458257569...}{2}$$\n$$P^{*} = \\frac{0.358257569...}{2}$$\n$$P^{*} = 0.17912878... \\ \\mathrm{m}^2$$\nRounding to four significant figures, the steady-state posterior variance is $0.1791\\ \\mathrm{m}^2$.",
            "answer": "$$\n\\boxed{0.1791}\n$$"
        },
        {
            "introduction": "Moving from a scalar state to a multivariate environment, this practice introduces the Three-Dimensional Variational (3D-Var) framework. Here, we tackle a common scenario in remote sensing: a single coarse-resolution observation must inform multiple sub-grid model states. This exercise demonstrates the crucial role of the background error covariance matrix ($B$) in spreading the observational information realistically across the state variables, a core concept in geophysical data assimilation .",
            "id": "3804738",
            "problem": "A coarse-resolution microwave radiometer observes the aggregate surface water equivalent over a pixel that contains two subgrid cells. Let the environmental model state be the two-dimensional vector $x = \\begin{pmatrix} x_{1} \\\\ x_{2} \\end{pmatrix}$, where $x_{1}$ and $x_{2}$ denote the surface water equivalent of the two subgrid cells, expressed in kilograms per square meter ($\\mathrm{kg}\\ \\mathrm{m}^{-2}$). A background estimate $x^{b} = \\begin{pmatrix} 10 \\\\ 5 \\end{pmatrix}$ is available, and its background error covariance matrix is\n$$\nB = \\begin{pmatrix}\n4 & 2 \\\\\n2 & 3\n\\end{pmatrix},\n$$\nwith entries in $\\mathrm{kg}^{2}\\ \\mathrm{m}^{-4}$. The observation operator for the radiometer is the linear map $H = \\begin{pmatrix} 1 & 1 \\end{pmatrix}$ that returns the pixel aggregate, and the single observation is $y = 28$ in $\\mathrm{kg}\\ \\mathrm{m}^{-2}$. The observation error variance is $R = 1$ in $\\mathrm{kg}^{2}\\ \\mathrm{m}^{-4}$.\n\nUsing the Bayesian linear-Gaussian framework for Three-Dimensional Variational (3D-Var) data assimilation, start from the definitions of the Gaussian prior, the linear observation model, and the quadratic cost function, and derive the analysis increment $\\delta x^{a} = x^{a} - x^{b}$ and the analysis error covariance $A$. Then compute the fractional reduction in total error variance defined by\n$$\n\\text{reduction} \\;=\\; \\frac{\\operatorname{tr}(B) - \\operatorname{tr}(A)}{\\operatorname{tr}(B)},\n$$\nwhere $\\operatorname{tr}(\\cdot)$ denotes the matrix trace.\n\nProvide exact values (do not round). Express the analysis increment components in $\\mathrm{kg}\\ \\mathrm{m}^{-2}$ and the variance reduction as a decimal number or a fraction with no percent sign. Your final answer must be a single row matrix containing, in order, $\\delta x^{a}_{1}$, $\\delta x^{a}_{2}$, and the variance reduction.",
            "solution": "The problem will first be validated against the specified criteria.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\nThe givens extracted verbatim from the problem statement are:\n-   Model state vector: $x = \\begin{pmatrix} x_{1} \\\\ x_{2} \\end{pmatrix}$, where $x_{1}$ and $x_{2}$ are surface water equivalents.\n-   Background estimate: $x^{b} = \\begin{pmatrix} 10 \\\\ 5 \\end{pmatrix}$ in $\\mathrm{kg}\\ \\mathrm{m}^{-2}$.\n-   Background error covariance matrix: $B = \\begin{pmatrix} 4 & 2 \\\\ 2 & 3 \\end{pmatrix}$ in $\\mathrm{kg}^{2}\\ \\mathrm{m}^{-4}$.\n-   Observation operator: $H = \\begin{pmatrix} 1 & 1 \\end{pmatrix}$.\n-   Observation: $y = 28$ in $\\mathrm{kg}\\ \\mathrm{m}^{-2}$.\n-   Observation error variance: $R = 1$ in $\\mathrm{kg}^{2}\\ \\mathrm{m}^{-4}$.\n-   Required outputs: analysis increment $\\delta x^{a} = x^{a} - x^{b}$, analysis error covariance $A$, and the fractional reduction in total error variance, $\\frac{\\operatorname{tr}(B) - \\operatorname{tr}(A)}{\\operatorname{tr}(B)}$.\n\n**Step 2: Validate Using Extracted Givens**\n\nThe problem is assessed for validity:\n-   **Scientifically Grounded**: The problem describes a standard application of Three-Dimensional Variational (3D-Var) data assimilation, a fundamental technique in environmental modeling and geosciences. The quantities, units, and mathematical framework are standard and scientifically correct.\n-   **Well-Posed**: The problem is well-posed. All necessary inputs ($x^b$, $B$, $H$, $y$, $R$) are provided. The background error covariance matrix $B$ is symmetric with a positive determinant ($\\det(B) = 4 \\times 3 - 2 \\times 2 = 8 > 0$) and positive diagonal elements, making it positive definite. The observation error variance $R=1$ is also positive. These conditions ensure that the 3D-Var cost function is a strictly convex quadratic function, which guarantees the existence of a unique minimum, corresponding to a unique analysis state $x^a$.\n-   **Objective**: The problem is stated using precise, quantitative, and unbiased language.\n-   **Completeness**: The problem provides a complete set of information required to derive the requested quantities. There are no missing parameters or contradictory constraints.\n\n**Step 3: Verdict and Action**\n\nThe problem is valid. It is a standard, well-defined problem in data assimilation. I will proceed to provide a complete solution.\n\n### Derivation and Solution\n\nThe core of Three-Dimensional Variational (3D-Var) data assimilation is the minimization of a cost function $J(x)$ that balances the distance to the background estimate $x^b$ and the distance to the observations $y$, weighted by their respective error covariances. The cost function is defined as:\n$$\nJ(x) = J_b(x) + J_o(x) = \\frac{1}{2}(x - x^b)^T B^{-1} (x - x^b) + \\frac{1}{2}(y - Hx)^T R^{-1} (y - Hx)\n$$\nThe analysis state $x^a$ is the state vector $x$ that minimizes $J(x)$. To find this minimum, we compute the gradient of $J(x)$ with respect to $x$ and set it to zero. For a linear observation operator $H$, the gradient is:\n$$\n\\nabla_x J(x) = B^{-1}(x - x^b) - H^T R^{-1}(y - Hx)\n$$\nSetting the gradient to zero for $x = x^a$:\n$$\n\\nabla_x J(x^a) = B^{-1}(x^a - x^b) - H^T R^{-1}(y - Hx^a) = 0\n$$\nThe problem asks for the analysis increment, $\\delta x^a = x^a - x^b$. Substituting $x^a = x^b + \\delta x^a$ into the equation:\n$$\nB^{-1}\\delta x^a - H^T R^{-1}(y - H(x^b + \\delta x^a)) = 0\n$$\n$$\nB^{-1}\\delta x^a - H^T R^{-1}(y - Hx^b - H\\delta x^a) = 0\n$$\nRearranging the terms to solve for $\\delta x^a$:\n$$\n(B^{-1} + H^T R^{-1} H)\\delta x^a = H^T R^{-1}(y - Hx^b)\n$$\nAn algebraically equivalent and often more computationally convenient formulation is through the Kalman gain matrix, $K$. The analysis increment is given by:\n$$\n\\delta x^a = K (y - Hx^b)\n$$\nwhere the Kalman gain $K$ is defined as:\n$$\nK = B H^T (H B H^T + R)^{-1}\n$$\nThe corresponding analysis error covariance matrix $A$ is given by:\n$$\nA = (I - KH)B\n$$\nWe now compute these quantities using the provided data.\n\nFirst, we calculate the innovation, also called the departure, $d = y - Hx^b$:\n$$\nHx^b = \\begin{pmatrix} 1 & 1 \\end{pmatrix} \\begin{pmatrix} 10 \\\\ 5 \\end{pmatrix} = 1 \\cdot 10 + 1 \\cdot 5 = 15\n$$\n$$\nd = y - Hx^b = 28 - 15 = 13\n$$\n\nNext, we compute the Kalman gain $K$:\nThe transpose of the observation operator is $H^T = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$.\nWe compute the term $B H^T$:\n$$\nB H^T = \\begin{pmatrix} 4 & 2 \\\\ 2 & 3 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 4+2 \\\\ 2+3 \\end{pmatrix} = \\begin{pmatrix} 6 \\\\ 5 \\end{pmatrix}\n$$\nNow we compute the term $H B H^T$:\n$$\nH B H^T = \\begin{pmatrix} 1 & 1 \\end{pmatrix} \\begin{pmatrix} 6 \\\\ 5 \\end{pmatrix} = 6+5 = 11\n$$\nThis quantity is a scalar since the observation is a scalar. Now we can compute $(H B H^T + R)^{-1}$:\n$$\n(H B H^T + R)^{-1} = (11 + 1)^{-1} = 12^{-1} = \\frac{1}{12}\n$$\nFinally, we can compute the Kalman gain matrix $K$:\n$$\nK = B H^T (H B H^T + R)^{-1} = \\begin{pmatrix} 6 \\\\ 5 \\end{pmatrix} \\left( \\frac{1}{12} \\right) = \\begin{pmatrix} 6/12 \\\\ 5/12 \\end{pmatrix} = \\begin{pmatrix} 1/2 \\\\ 5/12 \\end{pmatrix}\n$$\n\nWith the Kalman gain and the innovation, we compute the analysis increment $\\delta x^a$:\n$$\n\\delta x^a = K d = \\begin{pmatrix} 1/2 \\\\ 5/12 \\end{pmatrix} (13) = \\begin{pmatrix} 13/2 \\\\ 65/12 \\end{pmatrix}\n$$\nSo, the components of the analysis increment are $\\delta x^a_1 = \\frac{13}{2}$ and $\\delta x^a_2 = \\frac{65}{12}$.\n\nNext, we derive and compute the analysis error covariance matrix $A$:\n$$\nA = (I - KH)B\n$$\nFirst, we compute the product $KH$:\n$$\nKH = \\begin{pmatrix} 1/2 \\\\ 5/12 \\end{pmatrix} \\begin{pmatrix} 1 & 1 \\end{pmatrix} = \\begin{pmatrix} 1/2 & 1/2 \\\\ 5/12 & 5/12 \\end{pmatrix}\n$$\nNext, we compute $I - KH$:\n$$\nI - KH = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} - \\begin{pmatrix} 1/2 & 1/2 \\\\ 5/12 & 5/12 \\end{pmatrix} = \\begin{pmatrix} 1 - 1/2 & -1/2 \\\\ -5/12 & 1 - 5/12 \\end{pmatrix} = \\begin{pmatrix} 1/2 & -1/2 \\\\ -5/12 & 7/12 \\end{pmatrix}\n$$\nFinally, we compute $A$ by post-multiplying by $B$:\n$$\nA = (I - KH)B = \\begin{pmatrix} 1/2 & -1/2 \\\\ -5/12 & 7/12 \\end{pmatrix} \\begin{pmatrix} 4 & 2 \\\\ 2 & 3 \\end{pmatrix}\n$$\n$$\nA = \\begin{pmatrix}\n\\frac{1}{2}(4) - \\frac{1}{2}(2) & \\frac{1}{2}(2) - \\frac{1}{2}(3) \\\\\n-\\frac{5}{12}(4) + \\frac{7}{12}(2) & -\\frac{5}{12}(2) + \\frac{7}{12}(3)\n\\end{pmatrix}\n= \\begin{pmatrix}\n2 - 1 & 1 - \\frac{3}{2} \\\\\n-\\frac{20}{12} + \\frac{14}{12} & -\\frac{10}{12} + \\frac{21}{12}\n\\end{pmatrix}\n= \\begin{pmatrix}\n1 & -1/2 \\\\\n-6/12 & 11/12\n\\end{pmatrix}\n$$\nSimplifying the off-diagonal term:\n$$\nA = \\begin{pmatrix} 1 & -1/2 \\\\ -1/2 & 11/12 \\end{pmatrix}\n$$\n\nThe last step is to compute the fractional reduction in total error variance. The total variance is the trace of the error covariance matrix.\nThe total background error variance is $\\operatorname{tr}(B)$:\n$$\n\\operatorname{tr}(B) = 4 + 3 = 7\n$$\nThe total analysis error variance is $\\operatorname{tr}(A)$:\n$$\n\\operatorname{tr}(A) = 1 + \\frac{11}{12} = \\frac{12}{12} + \\frac{11}{12} = \\frac{23}{12}\n$$\nThe fractional reduction in total error variance is:\n$$\n\\text{reduction} = \\frac{\\operatorname{tr}(B) - \\operatorname{tr}(A)}{\\operatorname{tr}(B)} = \\frac{7 - \\frac{23}{12}}{7} = \\frac{\\frac{84 - 23}{12}}{7} = \\frac{61/12}{7} = \\frac{61}{12 \\times 7} = \\frac{61}{84}\n$$\nThe quantities to be reported are $\\delta x_1^a = \\frac{13}{2}$, $\\delta x_2^a = \\frac{65}{12}$, and the variance reduction $\\frac{61}{84}$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{13}{2} & \\frac{65}{12} & \\frac{61}{84} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Our final practice advances to Four-Dimensional Variational (4D-Var) assimilation, the powerhouse behind many operational weather and environmental forecasting systems. Unlike 3D-Var which provides a static snapshot, 4D-Var optimizes the model's initial state to best fit observations distributed across a time window. By working with a linear advection model, you will implement the 4D-Var machinery to find the optimal initial conditions that yield a physically consistent and observationally constrained model trajectory over time .",
            "id": "3804743",
            "problem": "Consider a one-dimensional, periodic, linear advection model on a discrete grid with $N$ grid points, with state vector $\\mathbf{x}_k \\in \\mathbb{R}^N$ at discrete times $t_k$ for $k \\in \\{0,1,2\\}$. The dynamics are deterministic and linear with a constant Courant number $\\alpha \\in [0,1]$, given by\n$$\n\\mathbf{x}_{k+1} = \\mathbf{M}\\,\\mathbf{x}_k, \\quad \\text{where} \\quad \\mathbf{M} = (1-\\alpha)\\,\\mathbf{I} + \\alpha\\,\\mathbf{S}.\n$$\nHere $\\mathbf{I}$ is the $N \\times N$ identity matrix and $\\mathbf{S}$ is the $N \\times N$ cyclic right-shift matrix defined by $(\\mathbf{S}\\mathbf{x})_i = x_{(i-1)\\bmod N}$ for each component index $i \\in \\{0,\\dots,N-1\\}$, enforcing periodic boundary conditions. The background (prior) distribution on the initial state $\\mathbf{x}_0$ is Gaussian with mean $\\mathbf{x}_b \\in \\mathbb{R}^N$ and covariance $\\mathbf{B} = \\sigma_b^2\\,\\mathbf{I}$, where $\\sigma_b^2 > 0$. There is a single scalar observation $y \\in \\mathbb{R}$ at time $t_2$, which measures the model state at grid index $j \\in \\{0,\\dots,N-1\\}$ according to\n$$\ny = \\mathbf{H}\\,\\mathbf{x}_2 + \\varepsilon, \\quad \\varepsilon \\sim \\mathcal{N}(0, \\sigma_o^2),\n$$\nwhere $\\mathbf{H}$ is the $1 \\times N$ observation operator that extracts the component at index $j$, i.e., $\\mathbf{H} = \\mathbf{e}_j^\\top$ with $\\mathbf{e}_j$ the $j$-th standard basis vector in $\\mathbb{R}^N$, and $\\sigma_o^2 > 0$ is the observation error variance. The Four-Dimensional Variational assimilation (4D-Var) analysis seeks the maximum a posteriori estimate $\\mathbf{x}_0^a$ of the initial state $\\mathbf{x}_0$ given $\\mathbf{x}_b$ and $y$, and then the analyzed trajectory $\\mathbf{x}_1^a = \\mathbf{M}\\,\\mathbf{x}_0^a$ and $\\mathbf{x}_2^a = \\mathbf{M}\\,\\mathbf{x}_1^a$.\n\nStarting from the definitions of linear Gaussian priors and likelihoods for the deterministic linear model above, and the periodic linear advection operator $\\mathbf{M}$ constructed from $\\alpha$, $\\mathbf{I}$, and $\\mathbf{S}$ as specified, derive the expression for the analysis initial state $\\mathbf{x}_0^a$ and compute the analyzed initial state and trajectory $(\\mathbf{x}_0^a,\\mathbf{x}_1^a,\\mathbf{x}_2^a)$ for each of the following independent test cases. All numeric inputs are dimensionless, and no physical units are required. Angles do not appear. Express all outputs as decimal floats.\n\nTest suite definition (each case specifies $(N,\\alpha,\\mathbf{x}_b,\\sigma_b^2,j,y,\\sigma_o^2)$):\n- Case $1$: $N=5$, $\\alpha=0.4$, $\\mathbf{x}_b = (0.2, 0.0, 1.0, 0.0, 0.0)^\\top$, $\\sigma_b^2 = 0.09$, $j=2$, $y=0.7$, $\\sigma_o^2=0.01$.\n- Case $2$: $N=5$, $\\alpha=0.0$, $\\mathbf{x}_b = (0.0, 0.5, 0.0, 0.0, 0.0)^\\top$, $\\sigma_b^2 = 0.04$, $j=1$, $y=0.6$, $\\sigma_o^2=0.01$.\n- Case $3$: $N=5$, $\\alpha=1.0$, $\\mathbf{x}_b = (0.0, 0.0, 0.0, 1.0, 0.0)^\\top$, $\\sigma_b^2 = 0.09$, $j=0$, $y=0.3$, $\\sigma_o^2=0.04$.\n- Case $4$: $N=5$, $\\alpha=0.6$, $\\mathbf{x}_b = (0.0, 0.0, 0.2, 0.0, 0.8)^\\top$, $\\sigma_b^2 = 0.16$, $j=4$, $y=0.1$, $\\sigma_o^2=25.0$.\n\nYour program must, for each case, construct $\\mathbf{S}$ and $\\mathbf{M}$, compute $\\mathbf{M}^2$, derive and solve for $\\mathbf{x}_0^a$ using the appropriate linear-Gaussian maximum a posteriori conditions, then compute $\\mathbf{x}_1^a$ and $\\mathbf{x}_2^a$. The final output for each case must be a list of length $3N$ formed by concatenating $\\mathbf{x}_0^a$, $\\mathbf{x}_1^a$, and $\\mathbf{x}_2^a$ in that order, with each entry formatted as a decimal float rounded to six digits after the decimal point.\n\nFinal output format: Your program should produce a single line of output containing the results for all four cases as a comma-separated Python-style list of lists, e.g., $[r_1,r_2,r_3,r_4]$, where each $r_i$ is the list of length $3N$ for Case $i$, and each scalar in each $r_i$ is printed with exactly six digits after the decimal point.",
            "solution": "### Derivation\n\nThe goal of 4D-Var is to find the initial state $\\mathbf{x}_0$ that maximizes the posterior probability density, which for Gaussian errors is equivalent to minimizing the cost function $J(\\mathbf{x}_0)$:\n$$\nJ(\\mathbf{x}_0) = J_b(\\mathbf{x}_0) + J_o(\\mathbf{x}_0) = \\frac{1}{2} (\\mathbf{x}_0 - \\mathbf{x}_b)^\\top \\mathbf{B}^{-1} (\\mathbf{x}_0 - \\mathbf{x}_b) + \\frac{1}{2} (y - \\mathbf{H}\\mathbf{x}_2)^\\top R^{-1} (y - \\mathbf{H}\\mathbf{x}_2)\n$$\nThe model is linear and deterministic, so the state at time $t_2$ is linearly related to the initial state $\\mathbf{x}_0$:\n$$\n\\mathbf{x}_2 = \\mathbf{M} \\mathbf{x}_1 = \\mathbf{M} (\\mathbf{M} \\mathbf{x}_0) = \\mathbf{M}^2 \\mathbf{x}_0\n$$\nLet's define a generalized observation operator $\\mathcal{H} = \\mathbf{H} \\mathbf{M}^2$. This operator maps the initial state $\\mathbf{x}_0$ directly to the observation space at time $t_2$. Since $\\mathbf{H} = \\mathbf{e}_j^\\top$, $\\mathcal{H}$ is the $j$-th row of the matrix $\\mathbf{M}^2$.\n\nWith this, the cost function can be written solely in terms of $\\mathbf{x}_0$:\n$$\nJ(\\mathbf{x}_0) = \\frac{1}{2} (\\mathbf{x}_0 - \\mathbf{x}_b)^\\top \\mathbf{B}^{-1} (\\mathbf{x}_0 - \\mathbf{x}_b) + \\frac{1}{2} (y - \\mathcal{H}\\mathbf{x}_0)^\\top R^{-1} (y - \\mathcal{H}\\mathbf{x}_0)\n$$\nSubstituting the given error covariances, $\\mathbf{B} = \\sigma_b^2 \\mathbf{I}$ and $R = \\sigma_o^2$:\n$$\nJ(\\mathbf{x}_0) = \\frac{1}{2\\sigma_b^2} \\|\\mathbf{x}_0 - \\mathbf{x}_b\\|^2_2 + \\frac{1}{2\\sigma_o^2} (y - \\mathcal{H}\\mathbf{x}_0)^2\n$$\nTo find the minimum, we compute the gradient of $J$ with respect to $\\mathbf{x}_0$ and set it to zero. The analysis state $\\mathbf{x}_0^a$ is the value of $\\mathbf{x}_0$ at this minimum.\n$$\n\\nabla_{\\mathbf{x}_0} J(\\mathbf{x}_0^a) = \\frac{1}{\\sigma_b^2} (\\mathbf{x}_0^a - \\mathbf{x}_b) - \\frac{1}{\\sigma_o^2} \\mathcal{H}^\\top (y - \\mathcal{H} \\mathbf{x}_0^a) = \\mathbf{0}\n$$\nRearranging terms to solve for $\\mathbf{x}_0^a$:\n$$\n\\left( \\frac{1}{\\sigma_b^2} \\mathbf{I} + \\frac{1}{\\sigma_o^2} \\mathcal{H}^\\top \\mathcal{H} \\right) \\mathbf{x}_0^a = \\frac{1}{\\sigma_b^2} \\mathbf{x}_b + \\frac{1}{\\sigma_o^2} \\mathcal{H}^\\top y\n$$\nThis is a linear system for $\\mathbf{x}_0^a$. For this specific linear-Gaussian problem, we can also use the equivalent and computationally efficient Kalman gain formulation. The analysis update is:\n$$\n\\mathbf{x}_0^a = \\mathbf{x}_b + \\mathbf{K} (y - \\mathcal{H} \\mathbf{x}_b)\n$$\nHere, $(y - \\mathcal{H} \\mathbf{x}_b)$ is the innovation (a scalar), and $\\mathbf{K}$ is the Kalman gain vector, given by:\n$$\n\\mathbf{K} = \\mathbf{B} \\mathcal{H}^\\top (\\mathcal{H} \\mathbf{B} \\mathcal{H}^\\top + R)^{-1}\n$$\nSubstituting the specific forms of $\\mathbf{B}$ and $R$:\n$$\n\\mathbf{K} = (\\sigma_b^2 \\mathbf{I}) \\mathcal{H}^\\top (\\mathcal{H} (\\sigma_b^2 \\mathbf{I}) \\mathcal{H}^\\top + \\sigma_o^2)^{-1} = \\sigma_b^2 \\mathcal{H}^\\top (\\sigma_b^2 (\\mathcal{H} \\mathcal{H}^\\top) + \\sigma_o^2)^{-1}\n$$\nThe term in the parentheses is a scalar, making the inverse a simple reciprocal.\n\n### Computation Steps\nThe algorithm to solve for each test case is as follows:\n1.  Construct the $N \\times N$ matrices: identity $\\mathbf{I}$ and cyclic right-shift $\\mathbf{S}$.\n2.  Construct the model operator $\\mathbf{M} = (1-\\alpha)\\mathbf{I} + \\alpha\\mathbf{S}$ and compute $\\mathbf{M}^2$.\n3.  Construct the generalized observation operator $\\mathcal{H} = \\mathbf{e}_j^\\top \\mathbf{M}^2$, which is the $j$-th row of $\\mathbf{M}^2$.\n4.  Calculate the scalar innovation variance $s = \\mathcal{H} \\mathbf{B} \\mathcal{H}^\\top + R = \\sigma_b^2 (\\mathcal{H} \\mathcal{H}^\\top) + \\sigma_o^2$.\n5.  Calculate the gain vector $\\mathbf{K} = \\frac{\\sigma_b^2}{s} \\mathcal{H}^\\top$.\n6.  Calculate the scalar innovation $d = y - \\mathcal{H} \\mathbf{x}_b$.\n7.  Compute the analysis initial state: $\\mathbf{x}_0^a = \\mathbf{x}_b + \\mathbf{K} d$.\n8.  Compute the analyzed trajectory by propagating the analysis forward: $\\mathbf{x}_1^a = \\mathbf{M} \\mathbf{x}_0^a$ and $\\mathbf{x}_2^a = \\mathbf{M} \\mathbf{x}_1^a$.\n9.  Concatenate the results $\\mathbf{x}_0^a$, $\\mathbf{x}_1^a$, and $\\mathbf{x}_2^a$ and format as required.",
            "answer": "$$\n\\boxed{\n\\begin{array}{l}\n\\texttt{[[0.299177,0.297530,1.223148,0.000000,0.000000,0.179506,0.298287,0.852899,0.489259,0.000000,0.107704,0.290575,0.700000,0.527375,0.195704],} \\\\\n\\texttt{[0.000000,0.520000,0.000000,0.000000,0.000000,0.000000,0.520000,0.000000,0.000000,0.000000,0.000000,0.520000,0.000000,0.000000,0.000000],} \\\\\n\\texttt{[0.000000,0.000000,0.000000,0.569231,0.000000,0.000000,0.000000,0.569231,0.000000,0.000000,0.000000,0.569231,0.000000,0.000000,0.000000],} \\\\\n\\texttt{[0.000002,-0.000001,0.199997,-0.000001,0.800003,0.320002,0.000001,0.079998,0.120000,0.480001,0.192002,0.192001,0.048000,0.192000,0.368001]]}\n\\end{array}\n}\n$$"
        }
    ]
}