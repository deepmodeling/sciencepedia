{
    "hands_on_practices": [
        {
            "introduction": "In Monte Carlo simulations of nuclear reactors, raw tally results represent relative event frequencies. To be physically meaningful, these tallies must be normalized. This exercise explores the crucial distinction between two common schemes in $k$-eigenvalue calculations: normalization per source particle and per fission event. Mastering the conversion between these normalizations, as practiced in this problem, is fundamental for correctly interpreting simulation outputs and comparing them with experimental data or results from other codes .",
            "id": "4224230",
            "problem": "A steady-state neutron transport problem is solved using a Monte Carlo $k$-eigenvalue simulation for a homogeneous reactor with no external source, where the fission source drives the population from generation to generation. In such simulations, two common flux normalizations are used: per-source-particle normalization and per-fission-event normalization.\n\nStart from the $k$-eigenvalue form of the steady-state neutron balance, which can be written symbolically as\n$$ \\mathcal{L}\\,\\phi = \\frac{1}{k_{\\text{eff}}}\\,\\mathcal{F}\\,\\phi, $$\nwhere $\\mathcal{L}$ is the streaming-and-loss operator, $\\mathcal{F}$ is the fission production operator containing the average number of neutrons produced per fission $\\nu$, and $k_{\\text{eff}}$ is the effective neutron multiplication factor. Let the total fission rate be $F \\equiv \\int_{V}\\Sigma_{f}(\\mathbf{r},E)\\,\\phi(\\mathbf{r},E)\\,\\mathrm{d}V\\,\\mathrm{d}E$ and the total fission neutron production rate be $P \\equiv \\int_{V}\\nu(\\mathbf{r},E)\\,\\Sigma_{f}(\\mathbf{r},E)\\,\\phi(\\mathbf{r},E)\\,\\mathrm{d}V\\,\\mathrm{d}E$. Let $R_{r} \\equiv \\int_{V}\\Sigma_{r}(\\mathbf{r},E)\\,\\phi(\\mathbf{r},E)\\,\\mathrm{d}V\\,\\mathrm{d}E$ denote the volume-integrated reaction rate for some reaction $r$.\n\nExplain the conceptual difference between per-source-particle normalization and per-fission-event normalization in terms of how $\\phi$ is scaled and how $F$ and $P$ relate to $k_{\\text{eff}}$. Then, using first-principles definitions, derive a general expression to convert a reaction rate tally $R_{r}$ reported per-source-particle into the corresponding per-fission-event normalized reaction rate.\n\nFinally, apply your derivation to the following measured tallies, all reported per-source-particle in the same $k$-eigenvalue simulation:\n- Effective multiplication factor $k_{\\text{eff}} = 1.0150$,\n- Volume-integrated fission rate $F_{\\text{s}} = 0.4150$,\n- Volume-integrated fission neutron production rate $P_{\\text{s}} = 1.0150$,\n- Volume-integrated reaction rate for reaction $r$ (a specific capture reaction in an absorber material) $R_{r,\\text{s}} = 0.00790$.\n\nCompute the per-fission-event normalized value of $R_{r}$ and express your final result in events per fission event. Round your answer to four significant figures.",
            "solution": "The posed problem is scientifically grounded, self-contained, and well-posed. The provided definitions and equations are standard in the field of nuclear reactor physics and Monte Carlo methods. The numerical values are consistent with the theoretical framework, particularly the identity between the reported $k_{\\text{eff}}$ and the fission neutron production rate per source particle, which is a known consequence of the chosen normalization. The problem is therefore deemed valid and a solution may be constructed.\n\nThe problem asks for three components: a conceptual explanation of two common normalization schemes, a derivation of a conversion formula between them, and the application of this formula to a specific set of tallies.\n\nFirst, we address the conceptual difference between the normalization schemes. The fundamental solution to the steady-state $k$-eigenvalue neutron balance equation,\n$$ \\mathcal{L}\\,\\phi = \\frac{1}{k_{\\text{eff}}}\\,\\mathcal{F}\\,\\phi $$\nis the eigenfunction $\\phi$, which represents the neutron flux distribution. As with any eigenfunction, its absolute magnitude is arbitrary; only its shape is determined by the equation. To obtain quantitative reaction rates, the flux must be normalized. This is achieved by imposing a constraint on some integrated property of the flux.\n\nIn per-source-particle normalization, typically used in Monte Carlo $k$-eigenvalue simulations, the flux magnitude is scaled such that the total source strength for a subsequent generation is unity. In a simulation driven by a fission source, the source of neutrons for generation $n+1$ is the population of neutrons produced by fissions in generation $n$. The operator for this source is $\\frac{1}{k_{\\text{eff}}}\\,\\mathcal{F}$. Let the flux normalized per-source-particle be denoted $\\phi_{\\text{s}}$. The total source strength is the integral of the source term over all phase space. The normalization condition is thus:\n$$ \\int_{V}\\int_{E} \\frac{1}{k_{\\text{eff}}}\\,\\mathcal{F}\\,\\phi_{\\text{s}}(\\mathbf{r},E) \\,\\mathrm{d}E\\,\\mathrm{d}V = 1 $$\nBy definition, the total fission neutron production rate is $P_{\\text{s}} = \\int_{V}\\int_{E} \\mathcal{F}\\,\\phi_{\\text{s}}(\\mathbf{r},E) \\,\\mathrm{d}E\\,\\mathrm{d}V$. Substituting this into the normalization condition gives:\n$$ \\frac{P_{\\text{s}}}{k_{\\text{eff}}} = 1 \\implies P_{\\text{s}} = k_{\\text{eff}} $$\nThis identity confirms that in a per-source-particle normalized system, the tally for the total fission neutron production rate is numerically equal to the effective multiplication factor, $k_{\\text{eff}}$. The problem statement provides $k_{\\text{eff}} = 1.0150$ and $P_{\\text{s}} = 1.0150$, which is consistent with this principle. Any reaction rate tally $R_{r,\\text{s}}$ is then reported in units of \"events of type $r$ per simulated source particle.\"\n\nIn per-fission-event normalization, the flux magnitude is scaled such that the total number of fission events occurring in the system is unity. Let this flux be denoted $\\phi_{\\text{f}}$. The total fission rate is defined as $F \\equiv \\int_{V}\\int_{E}\\Sigma_{f}(\\mathbf{r},E)\\,\\phi(\\mathbf{r},E)\\,\\mathrm{d}E\\,\\mathrm{d}V$. The normalization condition is therefore:\n$$ F_{\\text{f}} = \\int_{V}\\int_{E}\\Sigma_{f}(\\mathbf{r},E)\\,\\phi_{\\text{f}}(\\mathbf{r},E)\\,\\mathrm{d}E\\,\\mathrm{d}V = 1 $$\nWith this normalization, a reaction rate tally $R_{r,\\text{f}}$ is reported in units of \"events of type $r$ per fission event in the system.\" The total fission neutron production rate $P_{\\text{f}}$ would be the average number of neutrons produced per fission event, commonly denoted $\\bar{\\nu}$.\n\nNext, we derive the conversion formula. Since $\\phi_{\\text{s}}$ and $\\phi_{\\text{f}}$ represent the same fundamental mode (eigenfunction), they must be proportional to each other. We can write:\n$$ \\phi_{\\text{f}} = C \\cdot \\phi_{\\text{s}} $$\nwhere $C$ is a constant conversion factor. To find $C$, we use the definition of the per-fission-event normalization.\n$$ F_{\\text{f}} = 1 = \\int_{V}\\int_{E}\\Sigma_{f}\\,\\phi_{\\text{f}}\\,\\mathrm{d}E\\,\\mathrm{d}V = \\int_{V}\\int_{E}\\Sigma_{f}\\,(C \\cdot \\phi_{\\text{s}})\\,\\mathrm{d}E\\,\\mathrm{d}V $$\nFactoring out the constant $C$:\n$$ 1 = C \\cdot \\int_{V}\\int_{E}\\Sigma_{f}\\,\\phi_{\\text{s}}\\,\\mathrm{d}E\\,\\mathrm{d}V $$\nThe integral on the right is, by definition, the per-source-particle fission rate, $F_{\\text{s}}$.\n$$ 1 = C \\cdot F_{\\text{s}} \\implies C = \\frac{1}{F_{\\text{s}}} $$\nNow we can convert any arbitrary reaction rate, $R_{r}$. The per-fission-event rate is $R_{r,\\text{f}}$ and the per-source-particle rate is $R_{r,\\text{s}}$.\n$$ R_{r,\\text{f}} = \\int_{V}\\int_{E}\\Sigma_{r}\\,\\phi_{\\text{f}}\\,\\mathrm{d}E\\,\\mathrm{d}V = \\int_{V}\\int_{E}\\Sigma_{r}\\,\\left(\\frac{1}{F_{\\text{s}}}\\phi_{\\text{s}}\\right)\\,\\mathrm{d}E\\,\\mathrm{d}V $$\n$$ R_{r,\\text{f}} = \\frac{1}{F_{\\text{s}}} \\int_{V}\\int_{E}\\Sigma_{r}\\,\\phi_{\\text{s}}\\,\\mathrm{d}E\\,\\mathrm{d}V $$\nThe integral is the definition of $R_{r,\\text{s}}$. Therefore, the general expression to convert a reaction rate tally from per-source-particle to per-fission-event is:\n$$ R_{r,\\text{f}} = \\frac{R_{r,\\text{s}}}{F_{\\text{s}}} $$\nThis shows that to convert any tally from a per-source-particle basis to a per-fission-event basis, one must divide the tally value by the total fission rate that was calculated on the per-source-particle basis.\n\nFinally, we apply this derivation to compute the requested value. The problem provides the following tallies, all reported per-source-particle:\n- Volume-integrated fission rate: $F_{\\text{s}} = 0.4150$\n- Volume-integrated reaction rate for reaction $r$: $R_{r,\\text{s}} = 0.00790$\n\nUsing the derived formula:\n$$ R_{r,\\text{f}} = \\frac{R_{r,\\text{s}}}{F_{\\text{s}}} = \\frac{0.00790}{0.4150} $$\nCalculating the numerical value:\n$$ R_{r,\\text{f}} \\approx 0.019036144... $$\nThe number of significant figures in the input data are three for $R_{r,\\text{s}}$ ($0.00790$) and four for $F_{\\text{s}}$ ($0.4150$). Standard scientific practice for division dictates that the result should be reported to the lesser number of significant figures, which is three. However, the problem provides an explicit instruction to \"round your answer to four significant figures.\" Adhering to this direct mandate, we round the result to four significant figures.\n$$ R_{r,\\text{f}} \\approx 0.01904 $$\nThis value represents the number of events of reaction $r$ per fission event in the reactor.",
            "answer": "$$\\boxed{0.01904}$$"
        },
        {
            "introduction": "After ensuring our tallies are correctly normalized, the next goal is to improve their statistical precision. This problem delves into stratified sampling, a powerful variance reduction technique that divides the sampling domain into distinct strata to ensure a more representative sampling of rare but important events. By working through the derivation of optimal sample allocation and the combination of estimators, you will gain a deep understanding of how to design more efficient Monte Carlo simulations and rigorously quantify their improvements .",
            "id": "4224231",
            "problem": "Consider a homogeneous, isotropically scattering medium in which a Monte Carlo (MC) particle history produces a per-history score for an absorption reaction rate tally via the collision estimator. Let the per-history tally score be a random variable $X$, and let $K \\in \\{1,2,3\\}$ denote the collision order of the history at which the reaction contribution is scored. The unconditional target is the reaction rate per source history $R = \\mathbb{E}[X]$. Suppose that under the natural, unstratified sampling of histories: $\\mathbb{P}(K = 1) = p_{1} = \\frac{1}{2}$, $\\mathbb{P}(K = 2) = p_{2} = \\frac{3}{10}$, $\\mathbb{P}(K = 3) = p_{3} = \\frac{1}{5}$; and the conditional moments of $X$ given $K = k$ are $\\mathbb{E}[X \\mid K = 1] = \\mu_{1} = \\frac{4}{5}$, $\\mathbb{E}[X \\mid K = 2] = \\mu_{2} = \\frac{1}{2}$, $\\mathbb{E}[X \\mid K = 3] = \\mu_{3} = \\frac{1}{5}$, and $\\operatorname{Var}(X \\mid K = 1) = \\sigma_{1}^{2} = 2$, $\\operatorname{Var}(X \\mid K = 2) = \\sigma_{2}^{2} = 1$, $\\operatorname{Var}(X \\mid K = 3) = \\sigma_{3}^{2} = \\frac{1}{2}$. You may assume that histories are independent.\n\n1. Using only the law of total expectation and the law of total variance as the foundational base, deduce the variance of the simple random sampling estimator $\\bar{X}_{N}$ of $R$, formed from $N$ independent histories without stratification. Then, derive the variance of the stratified estimator $\\hat{R}_{\\text{strat}} = \\sum_{k=1}^{3} p_{k} \\bar{X}_{k}$, where $\\bar{X}_{k}$ is the sample mean of the scores from stratum $K = k$, under a fixed allocation $n_{k}$ with $\\sum_{k=1}^{3} n_{k} = N$. Specialize this to proportional allocation $n_{k} = N p_{k}$ and show explicitly, for $N = 900$, how stratification over collision order reduces variance relative to simple random sampling, quantifying the reduction.\n\n2. Derive from first principles the optimal allocation $n_{k}$ (as a closed-form expression in $N$, $p_{k}$, and $\\sigma_{k}$) that minimizes the variance of $\\hat{R}_{\\text{strat}}$ over all allocations with $\\sum_{k=1}^{3} n_{k} = N$, assuming equal computational cost per history in each stratum.\n\n3. Now consider three independent, unbiased estimators $\\hat{R}_{1}$, $\\hat{R}_{2}$, and $\\hat{R}_{3}$ of the same reaction rate $R$, each constructed by conditioning histories to collide exactly at order $K = 1$, $K = 2$, and $K = 3$, respectively (for example, via Russian roulette and splitting or path conditioning) so that each $\\hat{R}_{k}$ is unbiased for $R$ but with different variances $v_{1} = 2$, $v_{2} = 1$, and $v_{3} = \\frac{1}{2}$. Derive the optimal variance-minimizing weights $w_{1}$, $w_{2}$, and $w_{3}$ for the linear unbiased combination $\\hat{R}_{\\text{comb}} = w_{1} \\hat{R}_{1} + w_{2} \\hat{R}_{2} + w_{3} \\hat{R}_{3}$ subject to $w_{1} + w_{2} + w_{3} = 1$, and compute their numerical values. Express the final weights as a three-entry row vector with exact fractions, and do not include units.\n\nRound any numerical values you are asked to compute to exact fractions wherever possible; no rounding by significant figures is required in this problem. The final reported answer must be the requested row vector.",
            "solution": "The problem statement has been rigorously validated and is found to be scientifically sound, well-posed, and contains sufficient information for a unique solution. The problem is a standard application of statistical principles to Monte Carlo estimators in particle transport, and all provided data are internally consistent. We shall proceed with the solution.\n\nThe problem is divided into three parts. We will address each in sequence.\n\n**Part 1: Variance of Simple Random Sampling (SRS) vs. Stratified Sampling**\n\nFirst, we calculate the unconditional mean $R = \\mathbb{E}[X]$ and variance $\\operatorname{Var}(X)$ of a per-history score $X$. The givens are the stratum probabilities $p_k = \\mathbb{P}(K=k)$ and the conditional moments $\\mu_k = \\mathbb{E}[X \\mid K=k]$ and $\\sigma_k^2 = \\operatorname{Var}(X \\mid K=k)$ for strata $k \\in \\{1, 2, 3\\}$.\n\nAccording to the law of total expectation, the unconditional mean is:\n$$\nR = \\mathbb{E}[X] = \\sum_{k=1}^{3} \\mathbb{E}[X \\mid K=k] \\mathbb{P}(K=k) = \\sum_{k=1}^{3} \\mu_k p_k\n$$\nSubstituting the given values:\n$$\nR = \\left(\\frac{4}{5}\\right)\\left(\\frac{1}{2}\\right) + \\left(\\frac{1}{2}\\right)\\left(\\frac{3}{10}\\right) + \\left(\\frac{1}{5}\\right)\\left(\\frac{1}{5}\\right) = \\frac{4}{10} + \\frac{3}{20} + \\frac{1}{25} = \\frac{40+15+4}{100} = \\frac{59}{100}\n$$\n\nNext, we use the law of total variance to find the unconditional variance $\\operatorname{Var}(X)$:\n$$\n\\operatorname{Var}(X) = \\mathbb{E}[\\operatorname{Var}(X \\mid K)] + \\operatorname{Var}(\\mathbb{E}[X \\mid K])\n$$\nThe first term is the expectation of the conditional variance:\n$$\n\\mathbb{E}[\\operatorname{Var}(X \\mid K)] = \\sum_{k=1}^{3} \\operatorname{Var}(X \\mid K=k) \\mathbb{P}(K=k) = \\sum_{k=1}^{3} \\sigma_k^2 p_k\n$$\n$$\n\\mathbb{E}[\\operatorname{Var}(X \\mid K)] = (2)\\left(\\frac{1}{2}\\right) + (1)\\left(\\frac{3}{10}\\right) + \\left(\\frac{1}{2}\\right)\\left(\\frac{1}{5}\\right) = 1 + \\frac{3}{10} + \\frac{1}{10} = 1 + \\frac{4}{10} = \\frac{14}{10} = \\frac{7}{5}\n$$\nThe second term is the variance of the conditional expectation:\n$$\n\\operatorname{Var}(\\mathbb{E}[X \\mid K]) = \\mathbb{E}[(\\mathbb{E}[X \\mid K])^2] - (\\mathbb{E}[\\mathbb{E}[X \\mid K]])^2 = \\left(\\sum_{k=1}^{3} \\mu_k^2 p_k\\right) - R^2\n$$\nWe calculate the sum of squares:\n$$\n\\sum_{k=1}^{3} \\mu_k^2 p_k = \\left(\\frac{4}{5}\\right)^2\\left(\\frac{1}{2}\\right) + \\left(\\frac{1}{2}\\right)^2\\left(\\frac{3}{10}\\right) + \\left(\\frac{1}{5}\\right)^2\\left(\\frac{1}{5}\\right) = \\left(\\frac{16}{25}\\right)\\left(\\frac{1}{2}\\right) + \\left(\\frac{1}{4}\\right)\\left(\\frac{3}{10}\\right) + \\left(\\frac{1}{25}\\right)\\left(\\frac{1}{5}\\right)\n$$\n$$\n\\sum_{k=1}^{3} \\mu_k^2 p_k = \\frac{16}{50} + \\frac{3}{40} + \\frac{1}{125} = \\frac{320+75+8}{1000} = \\frac{403}{1000}\n$$\nSo, the variance of the conditional expectation is:\n$$\n\\operatorname{Var}(\\mathbb{E}[X \\mid K]) = \\frac{403}{1000} - \\left(\\frac{59}{100}\\right)^2 = \\frac{4030}{10000} - \\frac{3481}{10000} = \\frac{549}{10000}\n$$\nThe total variance is therefore:\n$$\n\\operatorname{Var}(X) = \\frac{7}{5} + \\frac{549}{10000} = \\frac{14000}{10000} + \\frac{549}{10000} = \\frac{14549}{10000}\n$$\nFor the simple random sampling estimator $\\bar{X}_{N} = \\frac{1}{N}\\sum_{i=1}^{N} X_i$, comprised of $N$ independent histories, the variance is:\n$$\n\\operatorname{Var}(\\bar{X}_{N}) = \\frac{\\operatorname{Var}(X)}{N} = \\frac{1}{N}\\left(\\frac{14549}{10000}\\right)\n$$\nNow, consider the stratified estimator $\\hat{R}_{\\text{strat}} = \\sum_{k=1}^{3} p_{k} \\bar{X}_{k}$, where $\\bar{X}_{k}$ is the sample mean of $n_k$ scores from stratum $k$. The samples from different strata are independent, so the variance is:\n$$\n\\operatorname{Var}(\\hat{R}_{\\text{strat}}) = \\operatorname{Var}\\left(\\sum_{k=1}^{3} p_{k} \\bar{X}_{k}\\right) = \\sum_{k=1}^{3} p_{k}^2 \\operatorname{Var}(\\bar{X}_{k})\n$$\nThe variance of the sample mean in stratum $k$ is $\\operatorname{Var}(\\bar{X}_{k}) = \\frac{\\operatorname{Var}(X \\mid K=k)}{n_k} = \\frac{\\sigma_k^2}{n_k}$. Thus, the variance of the stratified estimator for a fixed allocation $\\{n_k\\}$ is:\n$$\n\\operatorname{Var}(\\hat{R}_{\\text{strat}}) = \\sum_{k=1}^{3} \\frac{p_{k}^2 \\sigma_k^2}{n_k}\n$$\nFor proportional allocation, we set $n_k = N p_k$. Substituting this into the variance expression:\n$$\n\\operatorname{Var}(\\hat{R}_{\\text{strat, prop}}) = \\sum_{k=1}^{3} \\frac{p_{k}^2 \\sigma_k^2}{N p_k} = \\frac{1}{N} \\sum_{k=1}^{3} p_k \\sigma_k^2\n$$\nThis is precisely the within-strata variance component, scaled by $1/N$:\n$$\n\\operatorname{Var}(\\hat{R}_{\\text{strat, prop}}) = \\frac{1}{N} \\mathbb{E}[\\operatorname{Var}(X \\mid K)] = \\frac{1}{N}\\left(\\frac{7}{5}\\right)\n$$\nTo quantify the variance reduction for $N=900$, we compare the two variances:\n$$\n\\operatorname{Var}(\\bar{X}_{900}) = \\frac{1}{900}\\left(\\frac{14549}{10000}\\right) = \\frac{14549}{9000000}\n$$\n$$\n\\operatorname{Var}(\\hat{R}_{\\text{strat, prop}}) = \\frac{1}{900}\\left(\\frac{7}{5}\\right) = \\frac{1}{900}\\left(\\frac{14000}{10000}\\right) = \\frac{14000}{9000000}\n$$\nThe absolute reduction in variance is:\n$$\n\\operatorname{Var}(\\bar{X}_{900}) - \\operatorname{Var}(\\hat{R}_{\\text{strat, prop}}) = \\frac{14549 - 14000}{9000000} = \\frac{549}{9000000}\n$$\nThis reduction is precisely the between-strata variance component $\\frac{\\operatorname{Var}(\\mathbb{E}[X \\mid K])}{N}$, which stratification with proportional allocation eliminates.\n\n**Part 2: Optimal Allocation for Stratified Sampling**\n\nWe seek to minimize the variance of the stratified estimator, $V(n_1, n_2, n_3) = \\sum_{k=1}^{3} \\frac{p_{k}^2 \\sigma_k^2}{n_k}$, subject to the constraint that the total number of samples is fixed: $g(n_1, n_2, n_3) = \\sum_{k=1}^{3} n_k - N = 0$. We employ the method of Lagrange multipliers. The Lagrangian is:\n$$\nL(n_1, n_2, n_3, \\lambda) = \\sum_{k=1}^{3} \\frac{p_k^2 \\sigma_k^2}{n_k} + \\lambda\\left(\\sum_{k=1}^{3} n_k - N\\right)\n$$\nTo find the minimum, we set the partial derivatives with respect to each $n_k$ to zero:\n$$\n\\frac{\\partial L}{\\partial n_k} = -\\frac{p_k^2 \\sigma_k^2}{n_k^2} + \\lambda = 0 \\quad \\implies \\quad n_k^2 = \\frac{p_k^2 \\sigma_k^2}{\\lambda}\n$$\nTaking the positive square root, as sample sizes must be positive, we get $n_k = \\frac{p_k \\sigma_k}{\\sqrt{\\lambda}}$.\nWe substitute this into the constraint equation to solve for $\\sqrt{\\lambda}$:\n$$\n\\sum_{k=1}^{3} n_k = \\sum_{k=1}^{3} \\frac{p_k \\sigma_k}{\\sqrt{\\lambda}} = \\frac{1}{\\sqrt{\\lambda}} \\sum_{j=1}^{3} p_j \\sigma_j = N\n$$\nThis yields $\\frac{1}{\\sqrt{\\lambda}} = \\frac{N}{\\sum_{j=1}^{3} p_j \\sigma_j}$.\nSubstituting this back into the expression for $n_k$, we obtain the optimal allocation, known as Neyman allocation:\n$$\nn_k = N \\frac{p_k \\sigma_k}{\\sum_{j=1}^{3} p_j \\sigma_j}\n$$\nThis expression gives the optimal number of samples $n_k$ for each stratum $k$ as a function of the total number of samples $N$, the stratum probability $p_k$, and the stratum standard deviation $\\sigma_k$.\n\n**Part 3: Optimal Combination of Independent Estimators**\n\nWe are given three independent, unbiased estimators $\\hat{R}_1, \\hat{R}_2, \\hat{R}_3$ for the same quantity $R$, with variances $v_1, v_2, v_3$. We form a linear combination $\\hat{R}_{\\text{comb}} = \\sum_{k=1}^{3} w_k \\hat{R}_k$.\nThe estimator must be unbiased, which means $\\mathbb{E}[\\hat{R}_{\\text{comb}}] = R$.\n$$\n\\mathbb{E}\\left[\\sum_{k=1}^{3} w_k \\hat{R}_k\\right] = \\sum_{k=1}^{3} w_k \\mathbb{E}[\\hat{R}_k] = \\sum_{k=1}^{3} w_k R = R \\sum_{k=1}^{3} w_k\n$$\nFor this to equal $R$, we must have the constraint $\\sum_{k=1}^{3} w_k = 1$.\nThe variance of the combined estimator, due to the independence of the $\\hat{R}_k$, is:\n$$\n\\operatorname{Var}(\\hat{R}_{\\text{comb}}) = \\operatorname{Var}\\left(\\sum_{k=1}^{3} w_k \\hat{R}_k\\right) = \\sum_{k=1}^{3} w_k^2 \\operatorname{Var}(\\hat{R}_k) = \\sum_{k=1}^{3} w_k^2 v_k\n$$\nOur objective is to minimize this variance subject to the unbiasedness constraint. We again use the method of Lagrange multipliers. The Lagrangian is:\n$$\nL(w_1, w_2, w_3, \\lambda) = \\sum_{k=1}^{3} w_k^2 v_k - \\lambda\\left(\\sum_{k=1}^{3} w_k - 1\\right)\n$$\nSetting the partial derivatives with respect to each $w_k$ to zero:\n$$\n\\frac{\\partial L}{\\partial w_k} = 2 w_k v_k - \\lambda = 0 \\quad \\implies \\quad w_k = \\frac{\\lambda}{2 v_k}\n$$\nThis shows that the optimal weights are inversely proportional to the variances of the individual estimators.\nWe substitute this result into the constraint equation to find $\\lambda$:\n$$\n\\sum_{k=1}^{3} w_k = \\sum_{k=1}^{3} \\frac{\\lambda}{2 v_k} = \\frac{\\lambda}{2} \\sum_{j=1}^{3} \\frac{1}{v_j} = 1\n$$\nThis gives $\\frac{\\lambda}{2} = \\left(\\sum_{j=1}^{3} \\frac{1}{v_j}\\right)^{-1}$.\nSubstituting this back into the expression for $w_k$, we find the optimal weights:\n$$\nw_k = \\frac{1/v_k}{\\sum_{j=1}^{3} (1/v_j)}\n$$\nNow we compute the numerical values using the given variances: $v_1=2$, $v_2=1$, and $v_3=1/2$.\nThe inverse variances are:\n$1/v_1 = 1/2$\n$1/v_2 = 1/1 = 1$\n$1/v_3 = 1/(1/2) = 2$\nThe sum of the inverse variances is:\n$$\n\\sum_{j=1}^{3} \\frac{1}{v_j} = \\frac{1}{2} + 1 + 2 = \\frac{1+2+4}{2} = \\frac{7}{2}\n$$\nThe optimal weights are therefore:\n$$\nw_1 = \\frac{1/2}{7/2} = \\frac{1}{7}\n$$\n$$\nw_2 = \\frac{1}{7/2} = \\frac{2}{7}\n$$\n$$\nw_3 = \\frac{2}{7/2} = \\frac{4}{7}\n$$\nThe final weights are $(w_1, w_2, w_3) = (1/7, 2/7, 4/7)$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{1}{7}  \\frac{2}{7}  \\frac{4}{7} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Many key parameters in reactor analysis, such as albedo or spectral indices, are defined as ratios of two tallied quantities. Estimating the uncertainty of such a ratio requires careful treatment, as the numerator and denominator tallies are often correlated within a single particle history. This exercise guides you through the application of first-order uncertainty propagation, also known as the Delta Method, to derive the variance of a ratio estimator. This is an essential skill for reporting statistically sound results for any quantity that is not tallied directly but calculated from other Monte Carlo estimates .",
            "id": "4224240",
            "problem": "Consider steady-state, one-speed neutron transport in a planar slab with a planar surface $\\mathcal{S}$ separating two regions. Let $\\mathbf{n}$ be the unit normal on $\\mathcal{S}$ pointing outward from the region of interest. The angular flux is $\\psi(\\mathbf{r},\\mathbf{\\Omega})$, where $\\mathbf{r}$ is position and $\\mathbf{\\Omega}$ is direction. Define the outgoing and incoming partial currents through $\\mathcal{S}$ by\n$$\nJ_{+} \\equiv \\int_{\\mathcal{S}} \\int_{\\mathbf{\\Omega}\\cdot \\mathbf{n}  0} (\\mathbf{\\Omega}\\cdot \\mathbf{n}) \\,\\psi(\\mathbf{r},\\mathbf{\\Omega})\\, \\mathrm{d}\\mathbf{\\Omega}\\, \\mathrm{d}S,\\qquad\nJ_{-} \\equiv \\int_{\\mathcal{S}} \\int_{\\mathbf{\\Omega}\\cdot \\mathbf{n}  0} |\\mathbf{\\Omega}\\cdot \\mathbf{n}| \\,\\psi(\\mathbf{r},\\mathbf{\\Omega})\\, \\mathrm{d}\\mathbf{\\Omega}\\, \\mathrm{d}S.\n$$\nThe albedo at $\\mathcal{S}$ is the dimensionless ratio $A \\equiv J_{+}/J_{-}$.\n\nA Monte Carlo (MC) simulation is run with $N$ independent source histories. Each history $i$ generates a random weighted set of particle surface-crossing events at $\\mathcal{S}$ due to transport, collisions, and any population control techniques used. For each crossing event $j$ in history $i$, let $w_{ij}$ be the particle statistical weight at crossing and let $\\mu_{ij} \\equiv \\mathbf{\\Omega}_{ij}\\cdot \\mathbf{n}$ be its direction cosine relative to $\\mathbf{n}$. Define the per-history outgoing and incoming current contributions by\n$$\nX_{i} \\equiv \\sum_{j \\in \\text{outgoing}} w_{ij}\\, \\mu_{ij},\\qquad\nY_{i} \\equiv \\sum_{j \\in \\text{incoming}} w_{ij}\\, |\\mu_{ij}|.\n$$\nAssume that for each history $i$, the pair $(X_{i},Y_{i})$ is a square-integrable random vector with finite first and second moments, and that the pairs are independent and identically distributed across histories but may be correlated within a history due to the shared transport realization and variance reduction operations. Let $\\mu_{X} \\equiv \\mathbb{E}[X_{i}]$, $\\mu_{Y} \\equiv \\mathbb{E}[Y_{i}]$, $\\sigma_{X}^{2} \\equiv \\mathrm{Var}(X_{i})$, $\\sigma_{Y}^{2} \\equiv \\mathrm{Var}(Y_{i})$, and $\\sigma_{XY} \\equiv \\mathrm{Cov}(X_{i},Y_{i})$.\n\nDefine the sample means $\\bar{X} \\equiv \\frac{1}{N}\\sum_{i=1}^{N} X_{i}$ and $\\bar{Y} \\equiv \\frac{1}{N}\\sum_{i=1}^{N} Y_{i}$, and the ratio estimator of albedo $\\hat{A} \\equiv \\bar{X}/\\bar{Y}$. Starting from the definitions of $J_{+}$ and $J_{-}$ and the unbiased surface-current crossing estimator for partial current in Monte Carlo, show that $\\hat{A}$ targets $A$ in the sense that $\\hat{A}$ is a consistent estimator for $A$ as $N \\to \\infty$. Then, using the joint asymptotic normality of $(\\bar{X},\\bar{Y})$ and first-order uncertainty propagation appropriate for correlated random variables, derive the asymptotic variance $\\mathrm{Var}(\\hat{A})$ in closed form as a function of $\\mu_{X}$, $\\mu_{Y}$, $\\sigma_{X}^{2}$, $\\sigma_{Y}^{2}$, $\\sigma_{XY}$, and $N$.\n\nExpress your final answer as a single closed-form analytic expression. No numerical rounding is required, and the albedo and its uncertainty are dimensionless.",
            "solution": "The problem asks for two results concerning the Monte Carlo estimation of albedo. First, we must demonstrate that the ratio estimator $\\hat{A} = \\bar{X}/\\bar{Y}$ is a consistent estimator for the true albedo $A = J_{+}/J_{-}$. Second, we must derive the asymptotic variance of this estimator, $\\mathrm{Var}(\\hat{A})$.\n\n**Part 1: Consistency of the Estimator $\\hat{A}$**\n\nThe consistency of an estimator means that the estimator converges in probability to the true value of the quantity being estimated as the number of samples tends to infinity. Here, we must show that $\\hat{A} \\xrightarrow{p} A$ as $N \\to \\infty$.\n\nThe quantities $J_{+}$ and $J_{-}$ are the true outgoing and incoming partial currents, defined as integrals of the angular neutron flux $\\psi(\\mathbf{r},\\mathbf{\\Omega})$:\n$$J_{+} = \\int_{\\mathcal{S}} \\int_{\\mathbf{\\Omega}\\cdot \\mathbf{n}  0} (\\mathbf{\\Omega}\\cdot \\mathbf{n}) \\,\\psi(\\mathbf{r},\\mathbf{\\Omega})\\, \\mathrm{d}\\mathbf{\\Omega}\\, \\mathrm{d}S$$\n$$J_{-} = \\int_{\\mathcal{S}} \\int_{\\mathbf{\\Omega}\\cdot \\mathbf{n}  0} |\\mathbf{\\Omega}\\cdot \\mathbf{n}| \\,\\psi(\\mathbf{r},\\mathbf{\\Omega})\\, \\mathrm{d}\\mathbf{\\Omega}\\, \\mathrm{d}S$$\nThe true albedo is the ratio $A = J_{+}/J_{-}$.\n\nIn a Monte Carlo simulation, the scores $X_{i}$ and $Y_{i}$ are per-history tallies for the outgoing and incoming currents, respectively. The surface-crossing estimator is constructed to be unbiased. This means the expectation of the tallied score is proportional to the physical quantity of interest. Let $\\mu_X = \\mathbb{E}[X_i]$ and $\\mu_Y = \\mathbb{E}[Y_i]$ be the expected scores per history. Due to the unbiased nature of the surface-crossing estimator, these expected scores are directly proportional to the true partial currents, with the same constant of proportionality $C$ related to the source normalization:\n$$ \\mu_X = C J_{+} $$\n$$ \\mu_Y = C J_{-} $$\nTherefore, the ratio of the expected scores is exactly the albedo:\n$$ \\frac{\\mu_X}{\\mu_Y} = \\frac{C J_{+}}{C J_{-}} = \\frac{J_{+}}{J_{-}} = A $$\nWe assume that $J_{-} \\neq 0$, and thus $\\mu_Y \\neq 0$, which is required for the albedo to be well-defined.\n\nThe problem states that the pairs $(X_i, Y_i)$ for $i=1, \\dots, N$ are independent and identically distributed (i.i.d.) random vectors with finite first moments $\\mu_X$ and $\\mu_Y$. The sample means are $\\bar{X} = \\frac{1}{N}\\sum_{i=1}^{N} X_{i}$ and $\\bar{Y} = \\frac{1}{N}\\sum_{i=1}^{N} Y_{i}$.\n\nAccording to the Law of Large Numbers (LLN), for i.i.d. random variables with a finite mean, the sample mean converges in probability to the true mean. Applying the LLN to our estimators, we have:\n$$ \\bar{X} \\xrightarrow{p} \\mu_X \\quad \\text{as } N \\to \\infty $$\n$$ \\bar{Y} \\xrightarrow{p} \\mu_Y \\quad \\text{as } N \\to \\infty $$\nThe aledbo estimator is $\\hat{A} = \\bar{X}/\\bar{Y}$. We can write this as a function $g(\\bar{X}, \\bar{Y})$, where $g(x,y) = x/y$. The function $g$ is continuous at all points $(x,y)$ where $y \\neq 0$. Since we assumed $\\mu_Y \\neq 0$, the function is continuous at $(\\mu_X, \\mu_Y)$.\n\nBy the Continuous Mapping Theorem (a consequence of Slutsky's Theorem), if a sequence of random vectors converges in probability to a constant vector, then any continuous function of that sequence converges in probability to the function of the constant vector. Therefore:\n$$ \\hat{A} = g(\\bar{X}, \\bar{Y}) \\xrightarrow{p} g(\\mu_X, \\mu_Y) = \\frac{\\mu_X}{\\mu_Y} $$\nSince we established that $\\mu_X / \\mu_Y = A$, it follows that:\n$$ \\hat{A} \\xrightarrow{p} A $$\nThis demonstrates that $\\hat{A}$ is a consistent estimator for the albedo $A$.\n\n**Part 2: Asymptotic Variance of $\\hat{A}$**\n\nTo find the asymptotic variance of $\\hat{A}$, we use a first-order Taylor series expansion of the function $g(\\bar{X}, \\bar{Y}) = \\bar{X}/\\bar{Y}$ around the point of the means $(\\mu_X, \\mu_Y)$. This technique is also known as the Delta Method.\n$$ \\hat{A} \\approx g(\\mu_X, \\mu_Y) + \\frac{\\partial g}{\\partial x}\\bigg|_{(\\mu_X, \\mu_Y)} (\\bar{X} - \\mu_X) + \\frac{\\partial g}{\\partial y}\\bigg|_{(\\mu_X, \\mu_Y)} (\\bar{Y} - \\mu_Y) $$\nThe partial derivatives of $g(x,y) = x/y$ are:\n$$ \\frac{\\partial g}{\\partial x} = \\frac{1}{y} \\quad \\implies \\quad \\frac{\\partial g}{\\partial x}\\bigg|_{(\\mu_X, \\mu_Y)} = \\frac{1}{\\mu_Y} $$\n$$ \\frac{\\partial g}{\\partial y} = -\\frac{x}{y^2} \\quad \\implies \\quad \\frac{\\partial g}{\\partial y}\\bigg|_{(\\mu_X, \\mu_Y)} = -\\frac{\\mu_X}{\\mu_Y^2} $$\nSubstituting these into the Taylor expansion gives:\n$$ \\hat{A} \\approx \\frac{\\mu_X}{\\mu_Y} + \\frac{1}{\\mu_Y}(\\bar{X} - \\mu_X) - \\frac{\\mu_X}{\\mu_Y^2}(\\bar{Y} - \\mu_Y) $$\nThe asymptotic variance of $\\hat{A}$ is the variance of this linear approximation. Since $\\mu_X$ and $\\mu_Y$ are constants, the variance is given by:\n$$ \\mathrm{Var}(\\hat{A}) \\approx \\mathrm{Var}\\left( \\frac{1}{\\mu_Y}\\bar{X} - \\frac{\\mu_X}{\\mu_Y^2}\\bar{Y} \\right) $$\nWe use the general formula for the variance of a linear combination of two correlated random variables $U$ and $V$, $\\mathrm{Var}(aU+bV) = a^2\\mathrm{Var}(U) + b^2\\mathrm{Var}(V) + 2ab\\mathrm{Cov}(U,V)$. Here, $U=\\bar{X}$, $V=\\bar{Y}$, $a=1/\\mu_Y$, and $b=-\\mu_X/\\mu_Y^2$.\n$$ \\mathrm{Var}(\\hat{A}) \\approx \\left(\\frac{1}{\\mu_Y}\\right)^2 \\mathrm{Var}(\\bar{X}) + \\left(-\\frac{\\mu_X}{\\mu_Y^2}\\right)^2 \\mathrm{Var}(\\bar{Y}) + 2\\left(\\frac{1}{\\mu_Y}\\right)\\left(-\\frac{\\mu_X}{\\mu_Y^2}\\right)\\mathrm{Cov}(\\bar{X}, \\bar{Y}) $$\nNext, we find the variances and covariance of the sample means. Since the per-history scores $(X_i, Y_i)$ are i.i.d. with variances $\\sigma_X^2 = \\mathrm{Var}(X_i)$, $\\sigma_Y^2 = \\mathrm{Var}(Y_i)$, and covariance $\\sigma_{XY} = \\mathrm{Cov}(X_i, Y_i)$:\n$$ \\mathrm{Var}(\\bar{X}) = \\mathrm{Var}\\left(\\frac{1}{N}\\sum_{i=1}^{N} X_i\\right) = \\frac{1}{N^2} \\sum_{i=1}^{N} \\mathrm{Var}(X_i) = \\frac{N\\sigma_X^2}{N^2} = \\frac{\\sigma_X^2}{N} $$\n$$ \\mathrm{Var}(\\bar{Y}) = \\mathrm{Var}\\left(\\frac{1}{N}\\sum_{i=1}^{N} Y_i\\right) = \\frac{1}{N^2} \\sum_{i=1}^{N} \\mathrm{Var}(Y_i) = \\frac{N\\sigma_Y^2}{N^2} = \\frac{\\sigma_Y^2}{N} $$\n$$ \\mathrm{Cov}(\\bar{X}, \\bar{Y}) = \\mathrm{Cov}\\left(\\frac{1}{N}\\sum_{i=1}^{N} X_i, \\frac{1}{N}\\sum_{j=1}^{N} Y_j\\right) = \\frac{1}{N^2} \\sum_{i=1}^{N} \\mathrm{Cov}(X_i, Y_i) = \\frac{N\\sigma_{XY}}{N^2} = \\frac{\\sigma_{XY}}{N} $$\nIn the covariance calculation, we use the fact that $\\mathrm{Cov}(X_i, Y_j) = 0$ for $i \\neq j$ because histories are independent.\n\nSubstituting these into the expression for $\\mathrm{Var}(\\hat{A})$:\n$$ \\mathrm{Var}(\\hat{A}) \\approx \\frac{1}{\\mu_Y^2} \\left(\\frac{\\sigma_X^2}{N}\\right) + \\frac{\\mu_X^2}{\\mu_Y^4} \\left(\\frac{\\sigma_Y^2}{N}\\right) - \\frac{2\\mu_X}{\\mu_Y^3} \\left(\\frac{\\sigma_{XY}}{N}\\right) $$\nWe can factor out $1/N$ and combine the terms:\n$$ \\mathrm{Var}(\\hat{A}) \\approx \\frac{1}{N} \\left( \\frac{\\sigma_X^2}{\\mu_Y^2} + \\frac{\\mu_X^2 \\sigma_Y^2}{\\mu_Y^4} - \\frac{2\\mu_X \\sigma_{XY}}{\\mu_Y^3} \\right) $$\nTo obtain a single fraction, we place all terms over the common denominator $N \\mu_Y^4$:\n$$ \\mathrm{Var}(\\hat{A}) \\approx \\frac{1}{N\\mu_Y^4} \\left( \\mu_Y^2 \\sigma_X^2 + \\mu_X^2 \\sigma_Y^2 - 2\\mu_X \\mu_Y \\sigma_{XY} \\right) $$\nThis is the final closed-form expression for the asymptotic variance of the albedo estimator $\\hat{A}$.",
            "answer": "$$\\boxed{\\frac{\\mu_Y^2 \\sigma_X^2 + \\mu_X^2 \\sigma_Y^2 - 2\\mu_X \\mu_Y \\sigma_{XY}}{N\\mu_Y^4}}$$"
        }
    ]
}