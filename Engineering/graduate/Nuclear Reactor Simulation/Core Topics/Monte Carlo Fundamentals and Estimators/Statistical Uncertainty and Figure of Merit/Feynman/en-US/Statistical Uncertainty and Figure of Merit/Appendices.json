{
    "hands_on_practices": [
        {
            "introduction": "A critical first step in analyzing any Monte Carlo criticality simulation is to ensure the fission source has converged to its fundamental eigenmode. Failing to discard the initial, non-stationary 'burn-in' cycles will bias the final results. This practice  moves beyond subjective visual inspection by implementing a rigorous, automatable algorithm to detect stationarity. By simultaneously monitoring statistical trends in both the multiplication factor ($k_{\\mathrm{eff}}$) and the source entropy using a sliding window, you will develop a robust tool for determining the correct number of inactive cycles.",
            "id": "4251796",
            "problem": "You are given a Monte Carlo criticality monitoring task for a nuclear reactor simulation. The goal is to determine the number of initial inactive (burn-in) cycles to discard by simultaneously monitoring the drift of the effective multiplication factor $k_{\\mathrm{eff}}$ and the source entropy of the fission source distribution. After a justified burn-in, you must compute the Figure of Merit (FOM), defined for the $k_{\\mathrm{eff}}$ estimator.\n\nYou must start from fundamental principles: convergence of Monte Carlo sample means by the law of large numbers, linear trend detection by least-squares regression, and Shannon entropy for discrete probability distributions. Design and implement a test that uses the following principles:\n\n- For the most recent window of $W$ consecutive cycles, assess the stability of two time series: the cycle-wise $k_{\\mathrm{eff}}$ estimates $\\{k_i\\}$ and the source entropy values $\\{H_i\\}$. For each series, fit a simple linear regression of the form $y_i = a + b x_i$ on the window using ordinary least squares, where $x_i$ is a cycle index within the window. Using a two-sided confidence interval at confidence level $1-\\alpha$, determine whether the slope $b$ is statistically indistinguishable from zero. Also require that $\\lvert b \\rvert$ be below a specified absolute tolerance for the corresponding metric.\n- Declare convergence at the first cycle index $i^\\star \\ge W$ such that both metrics (the $k_{\\mathrm{eff}}$ series and the source entropy series) pass the zero-slope confidence interval test and the absolute slope tolerance test on the window of the last $W$ cycles ending at $i^\\star$. The chosen burn-in count is the index $i^\\star$.\n- After determining $i^\\star$, compute the Figure of Merit (FOM) for the $k_{\\mathrm{eff}}$ estimator using the active cycles $i^\\star+1,\\dots,N$. Use the standard relative standard error definition for an average of batch means: if $\\bar{k}$ is the sample mean of $k_{\\mathrm{eff}}$ across active cycles, and $s^2$ is the unbiased sample variance of the per-cycle $k_{\\mathrm{eff}}$ batch means across the active cycles, and $t_c$ is the time per cycle in seconds, then the relative standard error is $R = \\frac{s/\\sqrt{n}}{\\bar{k}}$ where $n$ is the number of active cycles. The Figure of Merit is $F = \\frac{1}{R^2 \\, T}$ with $T = n \\, t_c$. Express the final FOM in $\\mathrm{s}^{-1}$ (per second) without rounding.\n\nNotes and requirements:\n- The Shannon entropy for a discrete distribution $p \\in \\mathbb{R}^M$ with nonnegative components summing to one is $H(p) = -\\sum_{j=1}^{M} p_j \\ln p_j$ (natural logarithm). The entropy unit is nats.\n- The least-squares slope and its two-sided confidence interval must be computed using ordinary least squares and Student’s $t$ distribution with $n-2$ degrees of freedom, where $n$ is the number of points in the window. Use a confidence level $1-\\alpha$ as specified in each test case.\n- If no cycle satisfies the convergence criteria by the end of the simulation (i.e., no $i^\\star$ exists), then set the burn-in count to $N$ and set the FOM to $0.0$.\n\nTest suite:\nImplement and evaluate your method on the following $4$ test cases. In all cases, the time per cycle $t_c$ is given in seconds and the final FOM must be reported in $\\mathrm{s}^{-1}$.\n\nFor each case, you must programmatically generate $\\{k_i\\}_{i=1}^{N}$ and the source probability vectors $\\{p_i\\}_{i=1}^{N}$ as specified, and then compute $H_i = -\\sum_{j=1}^{M} p_{i,j} \\ln p_{i,j}$.\n\n- Case $1$ (happy path):\n  - Parameters: $N=120$, $M=10$, $t_c=0.4$, $W=20$, $\\alpha=0.05$, $k$-slope tolerance $\\tau_k = 3\\times 10^{-4}$ per cycle, entropy slope tolerance $\\tau_H = 1.5\\times 10^{-3}$ nats per cycle.\n  - $k_{\\mathrm{eff}}$ sequence: for cycle index $i \\in \\{1,\\dots,N\\}$,\n    $$k_i = 0.96 + 0.0012 \\cdot \\min(i, 35) + 0.0004 \\sin(0.31 i).$$\n  - Source distributions: define $\\alpha_i = 3.0 \\exp(-i/18.0)$ and raw scores $r_{i,j} = \\exp\\big(-\\alpha_i \\cdot (j-1)\\big)$ for $j \\in \\{1,\\dots,M\\}$. Normalize to $p_{i,j} = r_{i,j} / \\sum_{\\ell=1}^{M} r_{i,\\ell}$.\n\n- Case $2$ (slow drift near tolerance):\n  - Parameters: $N=150$, $M=12$, $t_c=0.6$, $W=25$, $\\alpha=0.05$, $\\tau_k = 2\\times 10^{-4}$ per cycle, $\\tau_H = 9\\times 10^{-4}$ nats per cycle.\n  - $k_{\\mathrm{eff}}$ sequence:\n    $$k_i = 0.98 + 0.00018 \\cdot \\min(i, 70) + 0.00025 \\sin(0.2 i).$$\n  - Source distributions: define $\\alpha_i = 2.5 \\exp(-i/40.0)$, $r_{i,j} = \\exp\\big(-\\alpha_i \\cdot (j-1)\\big)$, $p_{i,j} = r_{i,j} / \\sum_{\\ell=1}^{M} r_{i,\\ell}$.\n\n- Case $3$ (never stabilizes; edge case):\n  - Parameters: $N=80$, $M=8$, $t_c=0.5$, $W=20$, $\\alpha=0.05$, $\\tau_k = 1\\times 10^{-4}$ per cycle, $\\tau_H = 5\\times 10^{-4}$ nats per cycle.\n  - $k_{\\mathrm{eff}}$ sequence:\n    $$k_i = 0.90 + 0.0005 \\cdot i + 0.0001 \\sin(0.37 i).$$\n  - Source distributions: define $\\alpha_i = 3.0 \\exp(-i/200.0)$, $r_{i,j} = \\exp\\big(-\\alpha_i \\cdot (j-1)\\big)$, $p_{i,j} = r_{i,j} / \\sum_{\\ell=1}^{M} r_{i,\\ell}$.\n\n- Case $4$ (high-frequency noise around stationarity):\n  - Parameters: $N=100$, $M=16$, $t_c=0.3$, $W=20$, $\\alpha=0.05$, $\\tau_k = 3\\times 10^{-4}$ per cycle, $\\tau_H = 1.0\\times 10^{-3}$ nats per cycle.\n  - $k_{\\mathrm{eff}}$ sequence:\n    $$k_i = 1.00 + 0.0006 \\sin(0.5 i).$$\n  - Source distributions: define $\\alpha_i = 1.8 \\exp(-i/5.0)$, $r_{i,j} = \\exp\\big(-\\alpha_i \\cdot (j-1)\\big)$, $p_{i,j} = r_{i,j} / \\sum_{\\ell=1}^{M} r_{i,\\ell}$.\n\nFinal output format:\n- Your program must produce a single line of output containing the results for all $4$ test cases aggregated as a single Python-style list of lists, where each inner list contains two values: the chosen burn-in count and the computed FOM in $\\mathrm{s}^{-1}$. The required format is\n  $$[[b_1, f_1], [b_2, f_2], [b_3, f_3], [b_4, f_4]],$$\n  where each $b_i$ is an integer and each $f_i$ is a floating-point number. Do not include any other text. Do not round the floating-point numbers.\n\nAngle units are not applicable. There are no percentages to report. All physical times must be treated in seconds. The FOM must be expressed in $\\mathrm{s}^{-1}$.",
            "solution": "The problem requires the implementation of an algorithm to determine the burn-in period for a Monte Carlo nuclear reactor simulation and subsequently to calculate the Figure of Merit (FOM). The solution is structured into three main phases: data generation, convergence analysis, and FOM computation.\n\n### 1. Data Generation\nFor each test case, two time series are generated over $N$ cycles: the effective multiplication factor, $\\{k_i\\}_{i=1}^N$, and the source entropy, $\\{H_i\\}_{i=1}^N$.\n\nThe $k_{\\mathrm{eff}}$ values, $k_i$, for each cycle $i \\in \\{1, \\dots, N\\}$ are generated directly from the analytical function provided for each specific case.\n\nThe source entropy values, $H_i$, are derived from the source probability distributions, $\\{p_{i,j}\\}_{j=1}^M$. For each cycle $i$, a parameter $\\alpha_i$ is first calculated using its case-specific formula. This parameter is then used to define a set of raw scores $r_{i,j} = \\exp\\big(-\\alpha_i \\cdot (j-1)\\big)$ for $j \\in \\{1, \\dots, M\\}$. These scores are normalized to form a probability distribution $p_i = \\{p_{i,1}, \\dots, p_{i,M}\\}$ where $p_{i,j} = r_{i,j} / \\sum_{\\ell=1}^{M} r_{i,\\ell}$. The Shannon entropy for cycle $i$ is then computed using the natural logarithm, conformant with the definition provided:\n$$\nH_i = H(p_i) = -\\sum_{j=1}^{M} p_{i,j} \\ln p_{i,j}\n$$\nThe unit of entropy is nats.\n\n### 2. Convergence Detection\nThe core of the problem is to find the optimal number of burn-in cycles, $i^\\star$. This is achieved by iteratively checking for the statistical stationarity of both the $\\{k_i\\}$ and $\\{H_i\\}$ time series. The search for a suitable $i^\\star$ begins at cycle $W$ and proceeds up to cycle $N$, where $W$ is the specified window size.\n\nFor each candidate cycle $i \\in \\{W, \\dots, N\\}$, we analyze the data within the window of the last $W$ cycles, i.e., cycles $\\{i-W+1, \\dots, i\\}$. For a time series $\\{y_j\\}$ within this window, we perform a stability analysis consisting of two tests.\n\nFirst, a simple linear regression model, $y_j = a + b x_j$, is fitted to the data points $(x_j, y_j)$, where $\\{y_j\\}$ are the $W$ values from the window and $\\{x_j\\}$ are corresponding indices, which can be conveniently set to $\\{0, 1, \\dots, W-1\\}$. The slope $b$ of this regression line indicates the trend in the data.\n\nThe stability analysis imposes two conditions on the estimated slope, $\\hat{b}$:\n\n1.  **Zero-Slope Confidence Interval Test**: The slope must be statistically indistinguishable from zero. This is verified by checking if the value $0$ lies within the two-sided confidence interval for the true slope $b$. The interval is given by $\\hat{b} \\pm t_{\\alpha/2, W-2} \\cdot SE(\\hat{b})$, where $SE(\\hat{b})$ is the standard error of the slope estimate, and $t_{\\alpha/2, W-2}$ is the critical value from the Student's $t$-distribution with $W-2$ degrees of freedom at a confidence level of $1-\\alpha$. This condition is equivalent to checking if $|\\hat{b}| \\le t_{\\alpha/2, W-2} \\cdot SE(\\hat{b})$. For implementation, the `scipy.stats.linregress` function is used to obtain $\\hat{b}$ and $SE(\\hat{b})$, and `scipy.stats.t.ppf` is used to find the critical $t$-value.\n\n2.  **Absolute Slope Tolerance Test**: The magnitude of the slope must be smaller than a given practical tolerance, i.e., $|\\hat{b}| < \\tau$. This ensures that any residual trend is not only statistically insignificant but also practically negligible. Separate tolerances, $\\tau_k$ and $\\tau_H$, are specified for the $k_{\\mathrm{eff}}$ and entropy series, respectively.\n\nConvergence is declared at the first cycle $i^\\star$ for which **both** the $\\{k_i\\}$ and $\\{H_i\\}$ time series satisfy **both** stability conditions over the window ending at $i^\\star$. If no such cycle is found by the end of the simulation, the burn-in count is set to $i^\\star = N$.\n\n### 3. Figure of Merit (FOM) Calculation\nOnce the burn-in count $i^\\star$ is determined, the remaining cycles, from $i^\\star+1$ to $N$, are considered \"active\" cycles. Let $n = N - i^\\star$ be the number of active cycles.\n\nIf $n  2$, a meaningful sample variance cannot be computed. In this scenario, as well as the case where no convergence is found ($i^\\star = N$, so $n=0$), the FOM is set to $0.0$.\n\nOtherwise, the FOM is calculated for the $k_{\\mathrm{eff}}$ estimator over the $n$ active cycles. Let the set of $k_{\\mathrm{eff}}$ values for these cycles be $\\{k_{\\text{active}}\\}$. We first compute the sample mean, $\\bar{k}$, and the unbiased sample variance, $s^2$, of these values:\n$$\n\\bar{k} = \\frac{1}{n} \\sum_{k_j \\in \\{k_{\\text{active}}\\}} k_j\n$$\n$$\ns^2 = \\frac{1}{n-1} \\sum_{k_j \\in \\{k_{\\text{active}}\\}} (k_j - \\bar{k})^2\n$$\nThe relative standard error of the mean, $R$, is given by $R = \\frac{s/\\sqrt{n}}{\\bar{k}}$. The total computational time for the active cycles is $T = n \\cdot t_c$, where $t_c$ is the time per cycle. The Figure of Merit is defined as:\n$$\nF = \\frac{1}{R^2 T}\n$$\nSubstituting the expressions for $R$ and $T$, we can simplify the formula for $F$:\n$$\nF = \\frac{1}{\\left(\\frac{s^2/n}{\\bar{k}^2}\\right) (n \\cdot t_c)} = \\frac{\\bar{k}^2}{s^2 \\cdot t_c}\n$$\nThis simplified formula is used for the final calculation. The resulting FOM has units of $\\mathrm{s}^{-1}$. The final output for each case is a pair containing the integer burn-in count $i^\\star$ and the floating-point FOM value.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import t, linregress\n\ndef solve():\n    \"\"\"\n    Main solver function that processes all test cases and prints the final result.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"id\": 1,\n            \"N\": 120, \"M\": 10, \"tc\": 0.4, \"W\": 20, \"alpha\": 0.05,\n            \"tau_k\": 3e-4, \"tau_H\": 1.5e-3,\n            \"k_func\": lambda i: 0.96 + 0.0012 * min(i, 35) + 0.0004 * np.sin(0.31 * i),\n            \"alpha_func\": lambda i: 3.0 * np.exp(-i / 18.0)\n        },\n        {\n            \"id\": 2,\n            \"N\": 150, \"M\": 12, \"tc\": 0.6, \"W\": 25, \"alpha\": 0.05,\n            \"tau_k\": 2e-4, \"tau_H\": 9e-4,\n            \"k_func\": lambda i: 0.98 + 0.00018 * min(i, 70) + 0.00025 * np.sin(0.2 * i),\n            \"alpha_func\": lambda i: 2.5 * np.exp(-i / 40.0)\n        },\n        {\n            \"id\": 3,\n            \"N\": 80, \"M\": 8, \"tc\": 0.5, \"W\": 20, \"alpha\": 0.05,\n            \"tau_k\": 1e-4, \"tau_H\": 5e-4,\n            \"k_func\": lambda i: 0.90 + 0.0005 * i + 0.0001 * np.sin(0.37 * i),\n            \"alpha_func\": lambda i: 3.0 * np.exp(-i / 200.0)\n        },\n        {\n            \"id\": 4,\n            \"N\": 100, \"M\": 16, \"tc\": 0.3, \"W\": 20, \"alpha\": 0.05,\n            \"tau_k\": 3e-4, \"tau_H\": 1.0e-3,\n            \"k_func\": lambda i: 1.00 + 0.0006 * np.sin(0.5 * i),\n            \"alpha_func\": lambda i: 1.8 * np.exp(-i / 5.0)\n        }\n    ]\n\n    all_results = []\n    for params in test_cases:\n        result_pair = _process_case(params)\n        all_results.append(result_pair)\n\n    # Use str() on the list of lists to get the Python-style representation\n    # which matches the required output format including spaces.\n    print(str(all_results))\n\ndef _check_stability(y_window, x_reg, W, alpha, tolerance):\n    \"\"\"\n    Checks if a time series window is stable based on linear regression.\n    \"\"\"\n    # Perform Ordinary Least Squares regression\n    regression = linregress(x_reg, y_window)\n    slope = regression.slope\n    std_err_slope = regression.stderr\n\n    # Degrees of freedom for t-distribution is n-2\n    df = W - 2\n    \n    # Critical t-value for a two-sided test\n    t_crit = t.ppf(1 - alpha / 2, df)\n    \n    # Condition 1: Slope is statistically indistinguishable from zero\n    # This checks if 0 is within the confidence interval of the slope.\n    is_zero_slope = abs(slope) = t_crit * std_err_slope\n    \n    # Condition 2: Slope is below an absolute tolerance\n    is_small_slope = abs(slope) = tolerance\n    \n    return is_zero_slope and is_small_slope\n\ndef _process_case(params):\n    \"\"\"\n    Executes the full analysis for a single test case.\n    \"\"\"\n    N = params['N']\n    M = params['M']\n    tc = params['tc']\n    W = params['W']\n    alpha = params['alpha']\n    tau_k = params['tau_k']\n    tau_H = params['tau_H']\n    k_func = params['k_func']\n    alpha_func = params['alpha_func']\n\n    # 1. Data Generation\n    # Use 1-based cycle indices as per the problem description for function evaluation\n    cycle_indices_1_based = np.arange(1, N + 1)\n    \n    k_series = np.array([k_func(i) for i in cycle_indices_1_based])\n    \n    h_series = np.zeros(N)\n    for i_idx, i_1b in enumerate(cycle_indices_1_based):\n        alpha_i = alpha_func(i_1b)\n        j_indices = np.arange(1, M + 1)\n        r_ij = np.exp(-alpha_i * (j_indices - 1))\n        p_ij = r_ij / np.sum(r_ij)\n        # Shannon entropy calculation. Use a mask for p_ij=0 to avoid log(0).\n        # Although for the given formulae, p_ij is always > 0.\n        valid_p = p_ij[p_ij > 0]\n        h_series[i_idx] = -np.sum(valid_p * np.log(valid_p))\n\n    # 2. Convergence Detection\n    burn_in_cycle = N  # Default value if convergence is not found\n    x_reg = np.arange(W)\n    \n    # Iterate through possible end-of-window cycles (1-based index)\n    for i_star in range(W, N + 1):\n        # Window indices for array slicing (0-based)\n        start_idx = i_star - W\n        end_idx = i_star\n        \n        # Check stability for k_eff\n        k_window = k_series[start_idx:end_idx]\n        k_stable = _check_stability(k_window, x_reg, W, alpha, tau_k)\n        \n        if not k_stable:\n            continue\n            \n        # Check stability for entropy\n        h_window = h_series[start_idx:end_idx]\n        h_stable = _check_stability(h_window, x_reg, W, alpha, tau_H)\n        \n        if h_stable:\n            burn_in_cycle = i_star\n            break\n            \n    # 3. Figure of Merit (FOM) Calculation\n    n_active = N - burn_in_cycle\n    \n    # FOM requires at least 2 data points to compute variance\n    if n_active  2:\n        fom = 0.0\n    else:\n        # Active cycles are from burn_in_cycle + 1 to N.\n        # In 0-based indexing, slice is from burn_in_cycle to N.\n        active_k_series = k_series[burn_in_cycle:N]\n        \n        k_bar = np.mean(active_k_series)\n        s_sq = np.var(active_k_series, ddof=1)\n        \n        # If variance is zero (all values are identical), FOM is undefined (infinite).\n        # We set it to 0.0 as a practical matter, though it's unlikely with given functions.\n        if s_sq == 0:\n            fom = 0.0\n        else:\n            # FOM = k_bar^2 / (s^2 * t_c)\n            fom = (k_bar**2) / (s_sq * tc)\n            \n    return [burn_in_cycle, fom]\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "Having established a method for identifying convergence, we now explore the consequences of getting it wrong. What is the quantitative impact on our uncertainty estimate if we mistakenly include transient cycles in our analysis? This exercise  uses a hypothetical but physically realistic model to dissect this exact issue. By applying the law of total variance, you will derive how the combined effects of a biased mean and different variances during the transient phase alter the final confidence interval, providing a clear understanding of why proper burn-in diagnostics are essential.",
            "id": "4251733",
            "problem": "A Monte Carlo criticality simulation estimates the effective neutron multiplication factor $k_{\\mathrm{eff}}$ cycle by cycle. Let the random variable $X_i$ denote the per-cycle estimate of $k_{\\mathrm{eff}}$. In practice, the initial cycles are transient (burn-in) and are discarded to avoid bias due to fission source convergence. Consider a scenario in which an analyst incorrectly includes $n_t$ transient cycles together with $n_s$ steady-state cycles when forming the final estimate and uncertainty. Assume the following physically realistic and scientifically consistent model:\n- The transient cycles are independent and identically distributed with mean $m_t$ and variance $\\sigma_t^2$.\n- The steady-state cycles are independent and identically distributed with mean $m_s$ and variance $\\sigma_s^2$.\n- Successive cycles are correlated due to fission source coupling, characterized by a constant Integrated Autocorrelation Time (IAT) $\\tau \\ge 1$ for all cycles. This implies that the variance of the sample mean of a stationary sequence scales as $\\sigma^2 \\tau / n$ for $n$ samples.\n- The mixture of transient and steady-state cycles is non-stationary with a different mean in the transient phase. Treat the overall variance across all included cycles using the law of total variance across the two phases.\n\nStarting from the following fundamental base:\n- The Central Limit Theorem (CLT) implies that the sample mean of a weakly dependent sequence converges in distribution to a normal random variable with variance inflated by the Integrated Autocorrelation Time (IAT).\n- The confidence interval for the mean of a normal distribution at confidence level $0.95$ uses the $0.975$-quantile of the standard normal distribution.\n- The law of total variance for a mixture of two sub-populations with probabilities $w$ and $1-w$ states that the total variance equals the weighted average of the within-group variances plus the variance of the group means.\n\nYour task is to derive, from first principles, an algorithm that computes the effect on the $0.95$-level confidence interval width for $k_{\\mathrm{eff}}$ when transient cycles are incorrectly included. Specifically, the program must:\n1. Compute the width of the $0.95$ confidence interval for the sample mean using all cycles (transient plus steady-state), $W_{\\mathrm{all}}$.\n2. Compute the width of the $0.95$ confidence interval for the sample mean using only the steady-state cycles (the correct practice), $W_{\\mathrm{steady}}$.\n3. Report the ratio $R$ of these widths and the signed relative difference $D$ defined as $(W_{\\mathrm{all}} - W_{\\mathrm{steady}})/W_{\\mathrm{steady}}$.\n4. Return a boolean that is true if the inclusion of transient cycles leads to an overestimation of uncertainty (i.e., $R > 1$), and false otherwise.\n\nAll quantities involved are dimensionless. No physical units are required. Express the confidence level as the decimal $0.95$ in all reasoning. Angles are not involved. Percentages must be expressed as decimals as already specified.\n\nTest Suite:\nProvide results for the following parameter sets. For each case, the program shall take $(n_t, n_s, m_t, m_s, \\sigma_t, \\sigma_s, \\tau)$ as inputs.\n\n- Case A (general realistic case):\n  $n_t = 50$, $n_s = 950$, $m_t = 1.010$, $m_s = 1.000$, $\\sigma_t = 0.008$, $\\sigma_s = 0.006$, $\\tau = 3.0$.\n- Case B (equal means, higher transient variance):\n  $n_t = 200$, $n_s = 800$, $m_t = 1.000$, $m_s = 1.000$, $\\sigma_t = 0.010$, $\\sigma_s = 0.006$, $\\tau = 2.5$.\n- Case C (equal means, lower transient variance):\n  $n_t = 100$, $n_s = 900$, $m_t = 1.000$, $m_s = 1.000$, $\\sigma_t = 0.004$, $\\sigma_s = 0.006$, $\\tau = 3.5$.\n- Case D (boundary: no transient cycles):\n  $n_t = 0$, $n_s = 1000$, $m_t = 1.000$, $m_s = 1.000$, $\\sigma_t = 0.006$, $\\sigma_s = 0.006$, $\\tau = 2.0$.\n- Case E (large transient fraction and significant bias):\n  $n_t = 400$, $n_s = 600$, $m_t = 1.050$, $m_s = 1.000$, $\\sigma_t = 0.010$, $\\sigma_s = 0.006$, $\\tau = 4.0$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case result should be a list of the form $[W_{\\mathrm{all}}, W_{\\mathrm{steady}}, R, D, \\text{boolean}]$, where $W_{\\mathrm{all}}$ and $W_{\\mathrm{steady}}$ are floats, $R$ and $D$ are floats, and $\\text{boolean}$ is either true or false. For example: $[[0.001,0.002,0.5,-0.5,false],[\\dots]]$.",
            "solution": "The objective is to derive an algorithm to quantify the effect of incorrectly including $n_t$ transient cycles in a Monte Carlo estimation of the effective multiplication factor, $k_{\\mathrm{eff}}$, which should only be based on $n_s$ steady-state cycles. We will compute the width of the $0.95$ confidence interval for two scenarios: the correct analysis using only steady-state cycles ($W_{\\mathrm{steady}}$), and the incorrect analysis using all cycles ($W_{\\mathrm{all}}$). From these, we will determine the ratio $R = W_{\\mathrm{all}}/W_{\\mathrm{steady}}$, the relative difference $D = R-1$, and whether the uncertainty has been overestimated.\n\nFirst, we establish the general formula for the width of a two-sided confidence interval for a mean. Based on the Central Limit Theorem, the sample mean $\\bar{X}$ of $n$ observations from a distribution with mean $\\mu$ and variance $\\sigma^2$ is approximately normally distributed, $\\bar{X} \\sim N(\\mu, \\sigma_{\\bar{X}}^2)$, where $\\sigma_{\\bar{X}}^2 = \\mathrm{Var}(\\bar{X})$. The width $W$ of a confidence interval at confidence level $1-\\alpha$ is given by:\n$$ W = 2 \\cdot z_{1-\\alpha/2} \\cdot \\sigma_{\\bar{X}} $$\nwhere $z_{1-\\alpha/2}$ is the $(1-\\alpha/2)$-quantile of the standard normal distribution. For the specified confidence level of $0.95$, we have $\\alpha=0.05$, and we use the quantile $z_{0.975}$.\n\nThe problem states that for a stationary sequence of $n$ cycles with per-cycle variance $\\sigma^2$ and Integrated Autocorrelation Time (IAT) $\\tau$, the variance of the sample mean is inflated by the IAT:\n$$ \\mathrm{Var}(\\bar{X}) = \\frac{\\sigma^2 \\tau}{n} $$\n\n**1. Correct Analysis: Steady-State Cycles Only**\n\nIn the correct analysis, we consider only the $n_s$ steady-state cycles. This is a stationary sequence.\nThe given parameters are the per-cycle mean $m_s$ and per-cycle variance $\\sigma_s^2$.\nThe number of samples is $n_s$.\nThe variance of the sample mean, $\\bar{X}_s$, is:\n$$ \\mathrm{Var}(\\bar{X}_s) = \\frac{\\sigma_s^2 \\tau}{n_s} $$\nThe standard error of the mean is the square root of this variance:\n$$ \\sigma_{\\bar{X}_s} = \\sqrt{\\frac{\\sigma_s^2 \\tau}{n_s}} = \\sigma_s \\sqrt{\\frac{\\tau}{n_s}} $$\nThe width of the $0.95$ confidence interval, which we denote $W_{\\mathrm{steady}}$, is therefore:\n$$ W_{\\mathrm{steady}} = 2 \\cdot z_{0.975} \\cdot \\sigma_s \\sqrt{\\frac{\\tau}{n_s}} $$\nThis formula is valid for $n_s  0$.\n\n**2. Incorrect Analysis: All Cycles (Transient + Steady-State)**\n\nIn the incorrect analysis, the analyst combines $n_t$ transient cycles with $n_s$ steady-state cycles, for a total of $N = n_t + n_s$ cycles. This combined set is non-stationary because the mean differs between the two phases ($m_t$ vs. $m_s$).\n\nTo determine the variance for this mixed population, we use the law of total variance as specified. Let $X$ be a random variable representing a single cycle's $k_{\\mathrm{eff}}$ drawn from the combined pool of $N$ cycles. Let $Y$ be an indicator variable for the cycle type (transient or steady-state). The total variance of $X$ is $\\mathrm{Var}(X) = E[\\mathrm{Var}(X|Y)] + \\mathrm{Var}(E[X|Y])$.\n\nThe weights of the two sub-populations are $w_t = n_t/N$ and $w_s = n_s/N$.\n\nThe first term, the expected value of the conditional variance (within-group variance), is the weighted average of the variances of the two phases:\n$$ E[\\mathrm{Var}(X|Y)] = w_t \\cdot \\mathrm{Var}(X|Y=\\text{transient}) + w_s \\cdot \\mathrm{Var}(X|Y=\\text{steady-state}) = w_t \\sigma_t^2 + w_s \\sigma_s^2 $$\n\nThe second term, the variance of the conditional expectation (between-group variance), captures the variability due to the different means:\n$$ E[X|Y] = \\begin{cases} m_t  \\text{with probability } w_t \\\\ m_s  \\text{with probability } w_s \\end{cases} $$\nThe variance of this discrete random variable is:\n$$ \\mathrm{Var}(E[X|Y]) = w_t w_s (m_t - m_s)^2 $$\n\nCombining these terms, the effective per-cycle variance of the mixed population, $\\sigma_{\\mathrm{all}}^2$, is:\n$$ \\sigma_{\\mathrm{all}}^2 = (w_t \\sigma_t^2 + w_s \\sigma_s^2) + w_t w_s (m_t - m_s)^2 $$\nSubstituting the weights $w_t=n_t/N$ and $w_s=n_s/N$:\n$$ \\sigma_{\\mathrm{all}}^2 = \\left(\\frac{n_t}{N}\\sigma_t^2 + \\frac{n_s}{N}\\sigma_s^2\\right) + \\frac{n_t n_s}{N^2}(m_t - m_s)^2 $$\n\nThe analyst, incorrectly treating this non-stationary sequence as stationary, would use this effective per-cycle variance to compute the variance of the sample mean over $N$ cycles. Applying the IAT correction with the given constant $\\tau$:\n$$ \\mathrm{Var}(\\bar{X}_{\\mathrm{all}}) = \\frac{\\sigma_{\\mathrm{all}}^2 \\tau}{N} $$\nThe corresponding standard error of the mean is:\n$$ \\sigma_{\\bar{X}_{\\mathrm{all}}} = \\sqrt{\\frac{\\sigma_{\\mathrm{all}}^2 \\tau}{N}} = \\sigma_{\\mathrm{all}} \\sqrt{\\frac{\\tau}{N}} $$\nThe width of the $0.95$ confidence interval for this incorrect analysis, $W_{\\mathrm{all}}$, is:\n$$ W_{\\mathrm{all}} = 2 \\cdot z_{0.975} \\cdot \\sigma_{\\mathrm{all}} \\sqrt{\\frac{\\tau}{N}} = 2 \\cdot z_{0.975} \\cdot \\sqrt{\\sigma_{\\mathrm{all}}^2} \\sqrt{\\frac{\\tau}{n_t+n_s}} $$\n\n**3. Final Quantities**\n\nWith the expressions for $W_{\\mathrm{steady}}$ and $W_{\\mathrm{all}}$, we can compute the required metrics.\n\nThe ratio $R$ of the widths is:\n$$ R = \\frac{W_{\\mathrm{all}}}{W_{\\mathrm{steady}}} = \\frac{2 \\cdot z_{0.975} \\cdot \\sigma_{\\mathrm{all}} \\sqrt{\\tau/N}}{2 \\cdot z_{0.975} \\cdot \\sigma_s \\sqrt{\\tau/n_s}} = \\frac{\\sigma_{\\mathrm{all}}}{\\sigma_s} \\sqrt{\\frac{n_s}{N}} = \\frac{\\sigma_{\\mathrm{all}}}{\\sigma_s} \\sqrt{\\frac{n_s}{n_t+n_s}} $$\n\nThe signed relative difference $D$ is:\n$$ D = \\frac{W_{\\mathrm{all}} - W_{\\mathrm{steady}}}{W_{\\mathrm{steady}}} = \\frac{W_{\\mathrm{all}}}{W_{\\mathrm{steady}}} - 1 = R - 1 $$\n\nFinally, the boolean indicator for an overestimation of uncertainty is true if the analyst reports a larger uncertainty (i.e., a wider confidence interval), which corresponds to the condition $W_{\\mathrm{all}} > W_{\\mathrm{steady}}$. This is equivalent to $R > 1$.\n\nThe algorithm consists of computing these quantities using the provided parameters for each test case.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Computes the effect on the confidence interval for k-eff when transient\n    cycles are incorrectly included in a Monte Carlo simulation.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (n_t, n_s, m_t, m_s, sigma_t, sigma_s, tau)\n    test_cases = [\n        # Case A (general realistic case)\n        (50, 950, 1.010, 1.000, 0.008, 0.006, 3.0),\n        # Case B (equal means, higher transient variance)\n        (200, 800, 1.000, 1.000, 0.010, 0.006, 2.5),\n        # Case C (equal means, lower transient variance)\n        (100, 900, 1.000, 1.000, 0.004, 0.006, 3.5),\n        # Case D (boundary: no transient cycles)\n        (0, 1000, 1.000, 1.000, 0.006, 0.006, 2.0),\n        # Case E (large transient fraction and significant bias)\n        (400, 600, 1.050, 1.000, 0.010, 0.006, 4.0),\n    ]\n\n    results = []\n    \n    # Quantile for 95% confidence interval (two-sided)\n    z_0975 = norm.ppf(0.975)\n\n    for case in test_cases:\n        n_t, n_s, m_t, m_s, sigma_t, sigma_s, tau = case\n\n        # Ensure variances are used, not standard deviations, in formulas\n        var_t = sigma_t**2\n        var_s = sigma_s**2\n\n        # 1. Compute W_steady (correct case)\n        # Width of the 0.95 CI using only steady-state cycles\n        if n_s = 0:\n            # Avoid division by zero; physically n_s must be > 0\n            W_steady = np.inf\n        else:\n            std_err_steady = sigma_s * np.sqrt(tau / n_s)\n            W_steady = 2 * z_0975 * std_err_steady\n\n        # 2. Compute W_all (incorrect case)\n        # Width of the 0.95 CI using all (transient + steady) cycles\n        N = n_t + n_s\n        if N == 0:\n            # No cycles, width is undefined or infinite\n            W_all, R, D, is_overestimated = np.nan, np.nan, np.nan, False\n        else:\n            w_t = n_t / N\n            w_s = n_s / N\n\n            # Effective per-cycle variance from the law of total variance\n            # sigma_all^2 = E[Var(X|Y)] + Var(E[X|Y])\n            within_group_var = (w_t * var_t) + (w_s * var_s)\n            between_group_var = w_t * w_s * (m_t - m_s)**2\n            var_all = within_group_var + between_group_var\n            \n            sigma_all = np.sqrt(var_all)\n\n            std_err_all = sigma_all * np.sqrt(tau / N)\n            W_all = 2 * z_0975 * std_err_all\n\n            # 3. Compute ratio R and difference D\n            if W_steady == 0 or np.isinf(W_steady):\n                R = np.inf if W_all > 0 else np.nan\n            else:\n                R = W_all / W_steady\n            \n            D = R - 1\n            \n            # 4. Determine if uncertainty is overestimated\n            is_overestimated = R > 1\n\n        results.append([W_all, W_steady, R, D, is_overestimated])\n\n    # Helper function to format each result list according to the spec\n    def format_result(res_list):\n        return f\"[{res_list[0]},{res_list[1]},{res_list[2]},{res_list[3]},{str(res_list[4]).lower()}]\"\n\n    # Final print statement in the exact required format\n    formatted_results = [format_result(r) for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Most standard statistical analyses, including the confidence intervals for Monte Carlo results, are built upon the foundation of the Central Limit Theorem (CLT), which requires a finite variance in the underlying data. This final practice explores a more advanced and subtle challenge where this fundamental assumption breaks down. This exercise  delves into the topic of 'heavy-tailed' tally distributions, a pathology that can arise in difficult shielding problems where rare events contribute enormous scores. You will investigate how these distributions can lead to infinite variance, invalidating standard CLT-based statistics and rendering the Figure of Merit unstable.",
            "id": "4251719",
            "problem": "A nuclear reactor Monte Carlo neutron transport simulation is used to estimate a spatially localized reaction rate in a deeply shielded detector region. Population control is performed with weight windows, using splitting of particles whose weight is above a prescribed upper bound and Russian roulette of particles whose weight is below a prescribed lower bound. The tally uses a path-length estimator with per-event contribution equal to particle weight times a geometry-dependent factor. The observed run shows a highly skewed distribution of per-history tallies with occasional extremely large scores.\n\nStarting from the definition of the Monte Carlo estimator of the mean tally, the requirement for applicability of the Central Limit Theorem (CLT), and standard definitions of heavy-tailed distributions from probability theory, determine which statements are correct about heavy-tailed tallies and how occasional large weights arising from population control can violate finite-variance assumptions needed for CLT-based confidence intervals.\n\nSelect all that apply.\n\nA. A distribution is heavy-tailed if its upper tail decays more slowly than any exponential function; one canonical class is the regularly varying tail with $$\\mathbb{P}(Xx)\\sim L(x)\\,x^{-\\alpha}$$ for $x\\to\\infty$, where $L(x)$ is slowly varying and $\\alpha0$. For such a distribution, if $\\alpha\\leq 2$ then $\\mathrm{Var}(X)=\\infty$, and this invalidates CLT-based confidence intervals for the sample mean.\n\nB. Splitting alone can produce occasional very large particle weights, leading to infinite-variance tallies even with well-tuned weight windows; therefore CLT-based intervals always fail whenever splitting is used.\n\nC. In weight window population control, Russian roulette applied to under-weight particles can produce a surviving particle weight multiplied by $1/p$, where $p$ is the survival probability. If the random survival probability $p$ can be arbitrarily small with non-negligible frequency, then the induced weight distribution may have a heavy tail (e.g., $$\\mathbb{P}(Wx)\\propto x^{-\\alpha}$$ with $\\alpha\\leq 2$), potentially violating the finite-variance condition required by the CLT.\n\nD. Even if $\\mathrm{Var}(X)=\\infty$ but $\\mathbb{E}[X]$ exists, the Strong Law of Large Numbers (SLLN) ensures $\\bar{X}_n\\to\\mathbb{E}[X]$ almost surely as $n\\to\\infty$, while the normalized fluctuations of $\\bar{X}_n$ are typically governed by non-Gaussian stable laws rather than by a normal law; hence standard Gaussian confidence intervals based on the sample variance are unreliable.\n\nE. The Figure of Merit (FOM), defined as $$\\mathrm{FOM}=\\frac{1}{\\hat{\\sigma}^2\\,T}$$ with $\\hat{\\sigma}^2$ the sample variance estimate and $T$ the computational time, remains robust even under infinite-variance heavy tails, because the sample variance can be used in place of the true variance without affecting the interpretation of FOM.",
            "solution": "The user requires an analysis of statements concerning statistical issues in a Monte Carlo neutron transport simulation, specifically regarding heavy-tailed tally distributions arising from population control methods.\n\nFirst, we establish the theoretical background. The Monte Carlo method estimates an expected value $\\mu = \\mathbb{E}[X]$ by computing the sample mean $\\bar{X}_N = \\frac{1}{N} \\sum_{i=1}^{N} X_i$ over $N$ independent and identically distributed (i.i.d.) random variables $X_i$ (the tally scores from each history). The uncertainty of this estimate is typically quantified using a confidence interval derived from the Central Limit Theorem (CLT). The standard CLT requires the underlying distribution of $X_i$ to have a finite mean $\\mu$ and a finite variance $\\sigma^2 = \\mathrm{Var}(X)  \\infty$. When these conditions hold, the distribution of the sample mean is asymptotically normal, and its variance is $\\mathrm{Var}(\\bar{X}_N) = \\sigma^2/N$.\n\nThe problem describes a deep penetration simulation where a path-length estimator is used. The tally contribution is proportional to the particle weight. Population control is performed using weight windows, which involve splitting particles with high weight and applying Russian roulette to particles with low weight. This context is crucial for understanding the origin of statistical anomalies.\n\nNow, we evaluate each statement.\n\n### Analysis of Option A\n\nThis statement provides a definition of heavy-tailed distributions and relates their properties to the validity of the CLT.\n1.  **Definition of a heavy-tailed distribution**: The statement defines a heavy-tailed distribution as one whose tail decays more slowly than any exponential function. This is a correct, standard definition in probability theory.\n2.  **Regularly varying tail**: The statement presents the canonical class of distributions with a regularly varying tail, where the tail probability is $\\mathbb{P}(X  x) \\sim L(x) x^{-\\alpha}$ for $x \\to \\infty$. Here, $\\alpha  0$ is the tail index, and $L(x)$ is a slowly varying function (e.g., a constant or a logarithm). This is a precise and correct mathematical characterization of power-law-like tails.\n3.  **Condition for infinite variance**: For a non-negative random variable $X$, the $k$-th moment $\\mathbb{E}[X^k]$ is finite if and only if $\\alpha > k$. The variance, $\\mathrm{Var}(X) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2$, is finite if and only if $\\mathbb{E}[X^2]$ is finite. This requires the tail index $\\alpha$ to be greater than $2$. Consequently, if $\\alpha \\leq 2$, the second moment is infinite, and thus $\\mathrm{Var}(X) = \\infty$. This part of the statement is mathematically correct.\n4.  **Implication for CLT**: The standard CLT has a prerequisite of finite variance ($\\sigma^2  \\infty$). If the variance is infinite, this condition is violated. Therefore, CLT-based confidence intervals, which rely on the normality of the sample mean and a finite, estimable variance, are invalidated. This conclusion is correct.\n\nThe entire statement is a correct and logical progression from definition to implication.\n\n**Verdict**: **Correct**.\n\n### Analysis of Option B\n\nThis statement claims that splitting is the cause of large particle weights and infinite-variance tallies.\n1.  **Mechanism of splitting**: In Monte Carlo particle transport, splitting is a variance reduction technique applied to particles deemed \"important.\" A particle of weight $W$ is replaced by $m$ identical particles, each with a new weight of $W/m$. This action explicitly *reduces* particle weights. It is designed to increase the sample size in important phase-space regions without biasing the result.\n2.  **Origin of large weights**: Large particle weights are generated by the complementary technique, Russian roulette. In Russian roulette, a particle of weight $W$ is killed with probability $1-p$ and survives with probability $p$. To maintain an unbiased simulation, the weight of a surviving particle is increased to $W/p$. If the survival probability $p$ is small, the resulting weight can be very large.\n3.  **Conclusion**: The premise that \"Splitting alone can produce occasional very large particle weights\" is factually incorrect. Therefore, the conclusion that \"CLT-based intervals always fail whenever splitting is used\" is unfounded and false. Properly implemented weight windows, which use both splitting and Russian roulette, are intended to *control* weight fluctuations and prevent infinite variance.\n\nThe statement misidentifies the source of large particle weights.\n\n**Verdict**: **Incorrect**.\n\n### Analysis of Option C\n\nThis statement identifies Russian roulette as a potential source of heavy-tailed distributions for particle weights.\n1.  **Russian roulette mechanism**: The description is accurate. An under-weight particle is played against a survival probability $p$, and if it survives, its weight is multiplied by the factor $1/p$. This is fundamental to unbiased weight control.\n2.  **Origin of heavy tails**: The statement correctly posits that if the survival probability $p$ can be \"arbitrarily small with non-negligible frequency,\" the resulting weight $W' \\propto 1/p$ can be arbitrarily large. This can happen in practice. For instance, if a particle scatters into a region of very low importance, its weight may be far above the local weight window. To correct this, the particle's weight must be reduced, often by playing Russian roulette with a low survival probability. If the distribution of these survival probabilities $p$ near zero is not sufficiently suppressed, the resulting distribution of survivor weights $W'$ will have a power-law tail.\n3.  **Mathematical consequence**: Let's assume the probability density of $p$ near $0$ is $f_P(p) \\propto p^\\beta$. The tail probability of the resulting weight $W' \\propto 1/p$ is $\\mathbb{P}(W'  x) = \\mathbb{P}(p  c/x) \\propto \\int_0^{c/x} p^\\beta dp \\propto x^{-(\\beta+1)}$. So, the tail index of the weight distribution is $\\alpha = \\beta+1$. If the phase-space transport allows for processes where $\\beta \\leq 1$, then $\\alpha \\leq 2$, leading to infinite variance. The statement correctly identifies this possibility and its consequence: the potential violation of the finite-variance condition for the CLT.\n\nThe statement correctly describes a known pathological mechanism in Monte Carlo simulations.\n\n**Verdict**: **Correct**.\n\n### Analysis of Option D\n\nThis statement discusses the behavior of the sample mean when the variance is infinite but the mean is finite.\n1.  **Strong Law of Large Numbers (SLLN)**: The SLLN states that the sample mean $\\bar{X}_n$ converges almost surely to the true mean $\\mathbb{E}[X]$ as $n \\to \\infty$, provided that $\\mathbb{E}[|X|]$ is finite. For a power-law tail $\\mathbb{P}(X  x) \\propto x^{-\\alpha}$, the mean is finite if $\\alpha  1$. The variance is infinite if $1  \\alpha \\leq 2$. In this regime, the SLLN still holds. The statement is correct that even if $\\mathrm{Var}(X) = \\infty$, the sample mean will converge to the true mean, as long as the true mean exists.\n2.  **Generalized Central Limit Theorem**: When the variance is infinite, the classical CLT does not apply. Instead, for distributions with regularly varying tails with index $\\alpha \\in (0, 2)$, the fluctuations of the sample mean are described by the Generalized CLT. This theorem states that the suitably normalized sample mean converges in distribution to a non-Gaussian stable distribution (or Lévy alpha-stable distribution). The statement that fluctuations are \"governed by non-Gaussian stable laws\" is correct for the infinite variance case (where $\\alpha  2$).\n3.  **Reliability of Gaussian confidence intervals**: Since the limiting distribution is not normal (Gaussian), the very foundation of standard confidence intervals (which assume normality) is removed. Furthermore, the sample variance $\\hat{\\sigma}^2$ does not converge to a constant but instead tends to grow with the sample size $n$, making it a useless estimator of population variance. Therefore, \"standard Gaussian confidence intervals based on the sample variance are unreliable.\" This is an accurate and important conclusion.\n\nThe statement provides a correct and sophisticated summary of the consequences of an infinite-variance tally distribution.\n\n**Verdict**: **Correct**.\n\n### Analysis of Option E\n\nThis statement claims that the Figure of Merit (FOM) is a robust metric even when the tally distribution has infinite variance.\n1.  **Definition of FOM**: The FOM is defined as $\\mathrm{FOM} = \\frac{1}{\\hat{\\sigma}^2\\,T}$, where $\\hat{\\sigma}^2$ is the estimated variance of the mean tally and $T$ is the total computation time. Since $\\hat{\\sigma}^2 = \\hat{s}^2/N$, where $\\hat{s}^2$ is the sample variance of the individual tallies and $N$ is the number of histories, and $T \\propto N$, the FOM is inversely proportional to the sample variance per history: $\\mathrm{FOM} \\propto 1/\\hat{s}^2$. The purpose of the FOM is to provide a stable metric to compare the efficiency of different simulation methods.\n2.  **Behavior of estimators under infinite variance**: When the true variance is infinite, the sample variance $\\hat{s}^2$ is not a consistent estimator. It does not converge to a finite value as $N \\to \\infty$. Instead, it is subject to large, unpredictable jumps whenever a history produces an extremely large tally score. The value of $\\hat{s}^2$ tends to grow with $N$.\n3.  **Robustness of FOM**: Because $\\hat{s}^2$ is unstable and does not converge, the FOM, which is proportional to $1/\\hat{s}^2$, is also unstable and not robust. Its value will fluctuate wildly and generally trend downwards as the simulation progresses. An unstable FOM is, in fact, a primary diagnostic indicator that the simulation suffers from an infinite-variance problem.\n4.  **Flawed reasoning**: The justification given, \"...because the sample variance can be used in place of the true variance without affecting the interpretation of FOM,\" is false. The interpretation is critically affected because the sample variance is no longer a meaningful or well-behaved quantity in this context.\n\nThe statement makes a claim that is the opposite of what is observed in practice and predicted by theory.\n\n**Verdict**: **Incorrect**.",
            "answer": "$$\\boxed{ACD}$$"
        }
    ]
}