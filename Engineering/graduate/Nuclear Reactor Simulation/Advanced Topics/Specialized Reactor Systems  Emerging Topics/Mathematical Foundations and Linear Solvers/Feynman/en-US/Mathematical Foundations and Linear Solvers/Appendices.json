{
    "hands_on_practices": [
        {
            "introduction": "The discretization of diffusion equations in reactor physics typically results in large, sparse linear systems where direct solvers become prohibitively expensive, making iterative methods essential. This practice delves into the mechanics of the Conjugate Gradient (CG) method, the workhorse solver for the symmetric positive definite (SPD) systems common in this field. By deriving the CG recurrence from the principle of minimizing an energy functional , you will gain a fundamental understanding of how $A$-conjugate directions are built and why they guarantee convergence, culminating in a concrete calculation on a simple system to see the theory in action.",
            "id": "4234482",
            "problem": "Consider a steady-state, one-energy-group neutron diffusion model in a one-dimensional slab of length $L$ with homogeneous Dirichlet boundary conditions, where the scalar flux $\\phi(x)$ satisfies the differential equation $- \\frac{\\mathrm{d}}{\\mathrm{d}x}\\!\\left(D \\frac{\\mathrm{d}\\phi}{\\mathrm{d}x}\\right) + \\Sigma_{a}\\,\\phi = S(x)$. Assume constant diffusion coefficient $D0$, positive macroscopic absorption cross section $\\Sigma_{a}0$, and a spatially uniform source $S(x)=S_{0}$. Under these conditions, the diffusion operator is self-adjoint and positive definite on the appropriate Hilbert space with the induced energy inner product. A second-order finite-volume discretization on a uniform two-cell partition of the slab yields a linear system $A x = b$ for the cell-averaged flux vector $x \\in \\mathbb{R}^{2}$, where $A \\in \\mathbb{R}^{2\\times 2}$ is symmetric positive definite (SPD) and $b \\in \\mathbb{R}^{2}$ is the discrete source.\n\nDefine the $A$-inner product $\\langle x, y \\rangle_{A} = x^{T} A y$ and the induced $A$-norm $\\|x\\|_{A} = \\sqrt{x^{T} A x}$. Starting from the quadratic functional $J(x) = \\frac{1}{2}\\, x^{T} A x - x^{T} b$ and the fact that $A$ is SPD, derive from first principles the Conjugate Gradient (CG) recurrence that ensures that the search directions are $A$-conjugate and that the error decreases monotonically in the $A$-norm at each iteration. Specifically, justify the line-search step and the recurrence coefficients required to enforce $A$-conjugacy and explain why the $A$-norm of the error strictly decreases unless the exact solution is attained.\n\nThen, consider a physically consistent two-cell discretization with uniform cell width $h$, constant $D$ and $\\Sigma_{a}$, and zero Dirichlet boundary conditions, leading to the SPD matrix\n$$\nA = \\begin{pmatrix}\n\\frac{2D}{h} + \\Sigma_{a} h  -\\frac{D}{h} \\\\\n-\\frac{D}{h}  \\frac{2D}{h} + \\Sigma_{a} h\n\\end{pmatrix},\n$$\nwith a uniform discrete source vector $b = \\begin{pmatrix} S_{0}\\, h \\\\ S_{0}\\, h \\end{pmatrix}.$\nFor the parameter values $D = 1$, $\\Sigma_{a} = 0.2$, $h = 1$, and $S_{0} = 1$, take the initial guess $x_{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ and perform one CG iteration using your derived recurrence. Compute the contraction factor of the squared $A$-norm of the error after the first iteration, namely the ratio\n$$\n\\frac{\\|e_{1}\\|_{A}^{2}}{\\|e_{0}\\|_{A}^{2}},\n$$\nwhere $e_{k} = x^{\\ast} - x_{k}$ and $x^{\\ast}$ solves $A x^{\\ast} = b$. Express your final numerical answer as a single real number and round your answer to four significant figures.",
            "solution": "The steady-state, one-energy-group neutron diffusion operator $-\\frac{\\mathrm{d}}{\\mathrm{d}x}\\!\\left(D \\frac{\\mathrm{d}\\phi}{\\mathrm{d}x}\\right) + \\Sigma_{a}\\,\\phi$ with homogeneous Dirichlet boundary conditions is self-adjoint with respect to the energy inner product and coercive for $D0$ and $\\Sigma_{a}0$. A standard second-order finite-volume discretization on a uniform grid yields an SPD matrix $A$ consistent with this operator; the discrete energy is $J(x) = \\frac{1}{2} x^{T} A x - x^{T} b$, whose unique minimizer is the solution $x^{\\ast}$ of $A x^{\\ast} = b$.\n\nWe define the $A$-inner product and induced norm by $\\langle x, y \\rangle_{A} = x^{T} A y$ and $\\|x\\|_{A} = \\sqrt{x^{T} A x}$. The gradient of $J$ at $x$ is $\\nabla J(x) = A x - b = -r$, where $r = b - A x$ is the residual. Consider an iterative method $x_{k+1} = x_{k} + \\alpha_{k} p_{k}$ with a search direction $p_{k}$. To ensure a decrease of $J$ and, equivalently, of $\\|e_{k}\\|_{A}$, we perform an exact line search along $p_{k}$:\n$$\n\\alpha_{k} = \\arg\\min_{\\alpha \\in \\mathbb{R}} J(x_{k} + \\alpha p_{k}).\n$$\nExpanding $J(x_{k} + \\alpha p_{k})$ gives\n$$\nJ(x_{k} + \\alpha p_{k}) = \\frac{1}{2} (x_{k} + \\alpha p_{k})^{T} A (x_{k} + \\alpha p_{k}) - (x_{k} + \\alpha p_{k})^{T} b,\n$$\nwhose derivative with respect to $\\alpha$ is\n$$\n\\frac{\\mathrm{d}}{\\mathrm{d}\\alpha} J(x_{k} + \\alpha p_{k}) = p_{k}^{T} A x_{k} + \\alpha p_{k}^{T} A p_{k} - p_{k}^{T} b = -p_{k}^{T} r_{k} + \\alpha p_{k}^{T} A p_{k}.\n$$\nSetting the derivative to zero yields the optimal step length\n$$\n\\alpha_{k} = \\frac{p_{k}^{T} r_{k}}{p_{k}^{T} A p_{k}}.\n$$\nA natural initial choice is $p_{0} = r_{0}$, and the subsequent directions are constructed to be $A$-conjugate, that is, $\\langle p_{i}, p_{j} \\rangle_{A} = p_{i}^{T} A p_{j} = 0$ for $i \\neq j$. We consider a recurrence of the form\n$$\np_{k+1} = r_{k+1} + \\beta_{k} p_{k},\n$$\nand choose $\\beta_{k}$ to enforce $A$-conjugacy with $p_{k}$. Imposing $p_{k+1}^{T} A p_{k} = 0$ gives\n$$\n(r_{k+1} + \\beta_{k} p_{k})^{T} A p_{k} = 0 \\quad \\Rightarrow \\quad r_{k+1}^{T} A p_{k} + \\beta_{k}\\, p_{k}^{T} A p_{k} = 0,\n$$\nwhich yields\n$$\n\\beta_{k} = -\\frac{r_{k+1}^{T} A p_{k}}{p_{k}^{T} A p_{k}}.\n$$\nTo obtain a computationally simpler formula, we exploit properties of the residuals. From the update $x_{k+1} = x_{k} + \\alpha_{k} p_{k}$ and the residual $r_{k+1} = b - A x_{k+1}$, we have\n$$\nr_{k+1} = r_{k} - \\alpha_{k} A p_{k}.\n$$\nUsing the optimal $\\alpha_{k}$, one can show that successive residuals are orthogonal in the Euclidean inner product, namely $r_{k+1}^{T} r_{k} = 0$. This orthogonality, combined with the recurrence and the $A$-conjugacy requirement, leads to the classic Conjugate Gradient (CG) choice\n$$\n\\beta_{k} = \\frac{r_{k+1}^{T} r_{k+1}}{r_{k}^{T} r_{k}}.\n$$\nThus the CG recurrence is\n$$\n\\begin{aligned}\n\\text{Initialize } x_{0} \\text{ given, } r_{0} = b - A x_{0}, \\; p_{0} = r_{0}. \\\\\n\\alpha_{k} = \\frac{r_{k}^{T} p_{k}}{p_{k}^{T} A p_{k}}, \\quad x_{k+1} = x_{k} + \\alpha_{k} p_{k}, \\quad r_{k+1} = r_{k} - \\alpha_{k} A p_{k}, \\\\\n\\beta_{k} = \\frac{r_{k+1}^{T} r_{k+1}}{r_{k}^{T} r_{k}}, \\quad p_{k+1} = r_{k+1} + \\beta_{k} p_{k}.\n\\end{aligned}\n$$\nThe $A$-conjugacy $p_{i}^{T} A p_{j} = 0$ for $i \\neq j$ ensures that each step minimizes $J$ over the affine space $x_{0} + \\operatorname{span}\\{p_{0}, \\dots, p_{k}\\}$, and the exact line search guarantees that $J(x_{k+1})  J(x_{k})$ unless $r_{k} = 0$. Since $J(x) - J(x^{\\ast}) = \\frac{1}{2} \\|x - x^{\\ast}\\|_{A}^{2}$, monotonic decrease of $J$ implies monotonic decrease of $\\|e_{k}\\|_{A}$.\n\nWe now compute the requested contraction factor for the specified two-cell discretization. With $D = 1$, $\\Sigma_{a} = 0.2$, $h = 1$, the SPD matrix and source are\n$$\nA = \\begin{pmatrix} 2.2  -1 \\\\ -1  2.2 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}.\n$$\nWe take $x_{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$. The exact solution is $x^{\\ast} = A^{-1} b$, and the initial error is $e_{0} = x^{\\ast} - x_{0} = x^{\\ast}$. Because $A$ is $2 \\times 2$ SPD, we can compute $A^{-1}$ explicitly. The determinant is $\\det(A) = 2.2 \\cdot 2.2 - (-1)\\cdot(-1) = 4.84 - 1 = 3.84$, and\n$$\nA^{-1} = \\frac{1}{3.84} \\begin{pmatrix} 2.2  1 \\\\ 1  2.2 \\end{pmatrix}.\n$$\nTherefore,\n$$\ne_{0} = x^{\\ast} = A^{-1} b = \\frac{1}{3.84} \\begin{pmatrix} 3.2 \\\\ 3.2 \\end{pmatrix} = \\begin{pmatrix} \\frac{5}{6} \\\\ \\frac{5}{6} \\end{pmatrix}.\n$$\nWe perform one CG step. With $x_{0} = 0$, we have $r_{0} = b - A x_{0} = b$, and we choose $p_{0} = r_{0} = b$. The optimal step length is\n$$\n\\alpha_{0} = \\frac{r_{0}^{T} p_{0}}{p_{0}^{T} A p_{0}} = \\frac{b^{T} b}{b^{T} A b}.\n$$\nCompute the necessary scalars:\n$$\nb^{T} b = \\begin{pmatrix} 1  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = 2,\n$$\n$$\nA b = \\begin{pmatrix} 2.2  -1 \\\\ -1  2.2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1.2 \\\\ 1.2 \\end{pmatrix}, \\quad b^{T} A b = \\begin{pmatrix} 1  1 \\end{pmatrix} \\begin{pmatrix} 1.2 \\\\ 1.2 \\end{pmatrix} = 2.4.\n$$\nThus\n$$\n\\alpha_{0} = \\frac{2}{2.4} = \\frac{5}{6}.\n$$\nThe new iterate is $x_{1} = x_{0} + \\alpha_{0} p_{0} = \\alpha_{0} b = \\begin{pmatrix} \\frac{5}{6} \\\\ \\frac{5}{6} \\end{pmatrix}$, which coincides with $x^{\\ast}$ computed above. Therefore the new error is\n$$\ne_{1} = x^{\\ast} - x_{1} = 0.\n$$\nConsequently, the contraction factor of the squared $A$-norm of the error after the first iteration is\n$$\n\\frac{\\|e_{1}\\|_{A}^{2}}{\\|e_{0}\\|_{A}^{2}} = \\frac{0}{\\|e_{0}\\|_{A}^{2}} = 0.\n$$\nFor completeness, one can also express this ratio symbolically in terms of $s_{1} = e_{0}^{T} A e_{0}$, $s_{2} = e_{0}^{T} A^{2} e_{0} = r_{0}^{T} r_{0}$, and $s_{3} = e_{0}^{T} A^{3} e_{0} = r_{0}^{T} A r_{0}$, using $p_{0} = r_{0}$ and $\\alpha_{0} = s_{2}/s_{3}$:\n$$\n\\frac{\\|e_{1}\\|_{A}^{2}}{\\|e_{0}\\|_{A}^{2}} = 1 - \\frac{s_{2}^{2}}{s_{1} s_{3}},\n$$\nwhich evaluates here to $1 - \\frac{4}{\\left(\\frac{5}{3}\\right) \\cdot 2.4} = 1 - \\frac{4}{4} = 0$. Rounded to four significant figures, the numeric value remains $0$.",
            "answer": "$$\\boxed{0}$$"
        },
        {
            "introduction": "The efficiency of the Conjugate Gradient method is intrinsically linked to the condition number of the system matrix $A$, with convergence slowing dramatically for ill-conditioned problems. In reactor physics, large variations in material properties, such as the diffusion coefficient across different core components, are a common source of ill-conditioning. This exercise  provides a practical demonstration of how preconditioning addresses this challenge, showing that even a simple diagonal scaling (Jacobi) preconditioner can neutralize the effect of high-contrast coefficients and drastically reduce the predicted number of iterations.",
            "id": "4234506",
            "problem": "In a one-dimensional slab model of one-group neutron diffusion with vacuum boundary conditions, consider a layered medium with two materials: for $x \\in [0,1/2]$, the diffusion coefficient is $D_{1}$, and for $x \\in (1/2,1]$, the diffusion coefficient is $D_{2}$. The steady-state diffusion equation is $- \\frac{d}{dx}\\!\\left(D(x)\\,\\frac{d\\phi}{dx}\\right) = q(x)$ with $\\phi(0) = \\phi(1) = 0$, where $D(x)$ is piecewise constant as specified and $q(x)$ is a bounded source. Discretize the interval $[0,1]$ using a uniform mesh with $N=128$ cells and cell width $h = 1/(N+1)$, and form the symmetric positive definite linear system $A \\phi = b$ by a conservative finite-volume scheme with harmonic averaging of diffusion coefficients at cell interfaces. The resulting stiffness matrix $A$ is a symmetric tridiagonal $M$-matrix with entries constructed from the interface conductances $D_{i+1/2}$, where $D_{i+1/2}$ is the harmonic mean of the adjacent cell diffusion coefficients.\n\nLet $M = \\operatorname{diag}(A)$ denote the diagonal (Jacobi) preconditioner. Using first principles of finite-volume discretization, Rayleigh quotient bounds, and spectral properties of the discrete Dirichlet Laplacian on a uniform grid, estimate the condition numbers $\\kappa(A)$ and $\\kappa(M^{-1}A)$ up to multiplicative constants that do not depend on the diffusion contrast $D_{\\max}/D_{\\min}$, where $D_{\\min} = \\min\\{D_{1},D_{2}\\}$ and $D_{\\max} = \\max\\{D_{1},D_{2}\\}$. Then, using the standard energy-norm convergence bound for the Conjugate Gradient (CG) method relating the iteration count $m$, the condition number $\\kappa$, and the desired error reduction factor $\\varepsilon$ in the $A$-norm, determine the predicted ratio\n$R = \\frac{m_{\\text{unpre}}}{m_{\\text{pre}}}$,\nwhere $m_{\\text{unpre}}$ and $m_{\\text{pre}}$ are the number of CG iterations required without preconditioning and with diagonal preconditioning, respectively, to reduce the $A$-norm of the error by a factor $\\varepsilon = 10^{-8}$. Take $D_{1} = 1$ and $D_{2} = 100$. Express your final answer as a single real number and round your answer to four significant figures.",
            "solution": "We begin by recalling the finite-volume discretization for a one-dimensional second-order operator with variable coefficients on a uniform grid. Let $x_{i}$ denote the interior cell centers, $i=1,\\dots,N$, with mesh width $h = 1/(N+1)$. The conservative flux balance with harmonic averaging at interfaces yields a symmetric tridiagonal stiffness matrix $A$ satisfying\n$$\nv^{\\top} A v \\;=\\; \\frac{1}{h^{2}} \\sum_{i=0}^{N} D_{i+1/2}\\,\\big(v_{i+1} - v_{i}\\big)^{2},\n$$\nfor any vector $v \\in \\mathbb{R}^{N}$ with the conventions $v_{0}=v_{N+1}=0$ due to Dirichlet boundary conditions, and where $D_{i+1/2}$ is the harmonic mean of $D$ on the adjacent cells. For the layered medium with $D_{1}$ on $[0,1/2]$ and $D_{2}$ on $(1/2,1]$, the interface conductances satisfy $D_{\\min} \\le D_{i+1/2} \\le D_{\\max}$ for all $i$, with $D_{\\min} = \\min\\{D_{1},D_{2}\\}$ and $D_{\\max} = \\max\\{D_{1},D_{2}\\}$.\n\nDefine the standard discrete Dirichlet Laplacian template matrix $T \\in \\mathbb{R}^{N \\times N}$ by\n$$\nw^{\\top} \\left(\\frac{1}{h^{2}} T\\right) w \\;=\\; \\frac{1}{h^{2}} \\sum_{i=0}^{N} \\big(w_{i+1} - w_{i}\\big)^{2},\n$$\nso the eigenvalues of $(1/h^{2})T$ are\n$$\n\\lambda_{k}\\!\\left(\\frac{1}{h^{2}} T\\right) \\;=\\; \\frac{4}{h^{2}} \\sin^{2}\\!\\left(\\frac{k\\pi}{2(N+1)}\\right), \\quad k=1,\\dots,N.\n$$\nHence\n$$\n\\lambda_{\\min}\\!\\left(\\frac{1}{h^{2}} T\\right) \\;=\\; \\frac{4}{h^{2}} \\sin^{2}\\!\\left(\\frac{\\pi}{2(N+1)}\\right), \n\\quad\n\\lambda_{\\max}\\!\\left(\\frac{1}{h^{2}} T\\right) \\;=\\; \\frac{4}{h^{2}} \\sin^{2}\\!\\left(\\frac{N\\pi}{2(N+1)}\\right).\n$$\n\nWe now bound the condition number $\\kappa(A)$ using Rayleigh quotient comparisons. For any $v \\in \\mathbb{R}^{N}$,\n$$\n\\frac{D_{\\min}}{h^{2}} \\sum_{i=0}^{N} \\big(v_{i+1} - v_{i}\\big)^{2}\n\\;\\le\\;\nv^{\\top} A v\n\\;\\le\\;\n\\frac{D_{\\max}}{h^{2}} \\sum_{i=0}^{N} \\big(v_{i+1} - v_{i}\\big)^{2}.\n$$\nBy the Courant–Fischer min–max characterization and the eigenpairs of $(1/h^{2})T$, this yields\n$$\n\\lambda_{\\min}(A) \\;\\ge\\; D_{\\min} \\,\\lambda_{\\min}\\!\\left(\\frac{1}{h^{2}} T\\right),\n\\qquad\n\\lambda_{\\max}(A) \\;\\le\\; D_{\\max} \\,\\lambda_{\\max}\\!\\left(\\frac{1}{h^{2}} T\\right),\n$$\nand therefore\n$$\n\\kappa(A) \\;=\\; \\frac{\\lambda_{\\max}(A)}{\\lambda_{\\min}(A)}\n\\;\\le\\;\n\\frac{D_{\\max}}{D_{\\min}} \\cdot \n\\frac{\\lambda_{\\max}\\!\\left(\\frac{1}{h^{2}} T\\right)}{\\lambda_{\\min}\\!\\left(\\frac{1}{h^{2}} T\\right)}\n\\;=\\;\n\\frac{D_{\\max}}{D_{\\min}} \\cdot\n\\frac{\\sin^{2}\\!\\left(\\frac{N\\pi}{2(N+1)}\\right)}{\\sin^{2}\\!\\left(\\frac{\\pi}{2(N+1)}\\right)}.\n$$\nFor $N=128$, this asymptotically behaves like\n$$\n\\kappa(A) \\;\\approx\\; \\frac{D_{\\max}}{D_{\\min}} \\cdot \\frac{4}{4\\sin^{2}\\!\\left(\\frac{\\pi}{2(N+1)}\\right)} \n\\;\\sim\\; \\frac{D_{\\max}}{D_{\\min}} \\cdot \\frac{1}{\\sin^{2}\\!\\left(\\frac{\\pi}{2\\cdot 129}\\right)},\n$$\nwhich scales like $\\mathcal{O}\\!\\left(\\frac{D_{\\max}}{D_{\\min}} N^{2}\\right)$.\n\nNext, consider diagonal (Jacobi) preconditioning $M = \\operatorname{diag}(A)$ and the symmetric preconditioned matrix $H = M^{-1/2} A M^{-1/2}$. Its extremal eigenvalues admit the Rayleigh quotient representation\n$$\n\\lambda(H) \\;=\\; \n\\frac{y^{\\top} A y}{y^{\\top} M y}\n\\;=\\;\n\\frac{\\frac{1}{h^{2}} \\sum_{i=0}^{N} D_{i+1/2}\\,(y_{i+1}-y_{i})^{2}}{\\frac{1}{h^{2}} \\sum_{i=1}^{N} (D_{i-1/2}+D_{i+1/2}) \\, y_{i}^{2}}.\n$$\nOn each homogeneous subdomain where $D_{i+1/2}$ is constant, the numerator and denominator scale with the same diffusion factor, so the local symbol of $H$ is that of the normalized one-dimensional Laplacian, independent of $D_{1}$ and $D_{2}$. The only departure from constant coefficients occurs at the single material interface, which perturbs the spectrum by a lower-order effect that does not introduce dependence on the contrast $D_{\\max}/D_{\\min}$ for fixed $N$. Consequently, $H$ is spectrally equivalent to the normalized Dirichlet Laplacian on the same grid, with\n$$\n\\kappa\\!\\big(M^{-1} A\\big) \\;\\approx\\; \n\\frac{\\sin^{2}\\!\\left(\\frac{N\\pi}{2(N+1)}\\right)}{\\sin^{2}\\!\\left(\\frac{\\pi}{2(N+1)}\\right)}\n\\;\\sim\\; \\kappa\\!\\left(\\frac{1}{h^{2}} T\\right),\n$$\nup to multiplicative constants that depend on $N$ and mesh regularity but are independent of the contrast $D_{\\max}/D_{\\min}$. In particular, for the present uniform mesh and a single interface, the leading-order dependence on $D_{\\max}/D_{\\min}$ is removed by the diagonal scaling.\n\nWe now relate the Conjugate Gradient (CG) iteration count to the condition number. A standard, well-tested convergence bound for CG in the $A$-norm states that the error after $m$ iterations satisfies\n$$\n\\frac{\\|e_{m}\\|_{A}}{\\|e_{0}\\|_{A}} \n\\;\\le\\;\n2 \\left(\\frac{\\sqrt{\\kappa} - 1}{\\sqrt{\\kappa} + 1}\\right)^{m},\n$$\nwhere $\\kappa$ is the condition number of the (preconditioned) coefficient matrix in the relevant inner product. Solving for $m$ to achieve a target reduction factor $\\varepsilon$ yields the estimate\n$$\nm \\;\\ge\\; \\frac{1}{2} \\sqrt{\\kappa} \\,\\ln\\!\\left(\\frac{2}{\\varepsilon}\\right)\n\\quad \\text{for large } \\kappa,\n$$\nwhich captures the dominant dependence of CG on $\\sqrt{\\kappa}$. Therefore, the ratio of iteration counts for unpreconditioned and diagonally preconditioned systems that both use the same grid and tolerance is predicted by\n$$\nR \\;=\\; \\frac{m_{\\text{unpre}}}{m_{\\text{pre}}}\n\\;\\approx\\;\n\\frac{\\sqrt{\\kappa(A)}}{\\sqrt{\\kappa\\!\\big(M^{-1}A\\big)}}.\n$$\n\nFrom the condition number estimates derived above,\n$$\n\\sqrt{\\kappa(A)} \\;\\approx\\; \\sqrt{\\frac{D_{\\max}}{D_{\\min}}}\\,\\sqrt{\\kappa\\!\\left(\\frac{1}{h^{2}} T\\right)},\n\\qquad\n\\sqrt{\\kappa\\!\\big(M^{-1}A\\big)} \\;\\approx\\; \\sqrt{\\kappa\\!\\left(\\frac{1}{h^{2}} T\\right)}.\n$$\nThus, for $D_{1}=1$ and $D_{2}=100$,\n$$\nR \\;\\approx\\; \\sqrt{\\frac{D_{\\max}}{D_{\\min}}} \\;=\\; \\sqrt{100} \\;=\\; 10.\n$$\n\nBecause the logarithmic factor $\\ln\\!\\left(2/\\varepsilon\\right)$ is the same for both runs and cancels in the ratio, and because the dominant $N$-dependence due to the discrete Laplacian also cancels under diagonal scaling, the predicted ratio is set by the square root of the diffusion contrast. Therefore, rounded to four significant figures, the requested ratio is $10.00$.",
            "answer": "$$\\boxed{10.00}$$"
        },
        {
            "introduction": "While diagonal scaling is effective for certain problems, more complex systems often require more powerful preconditioners. This practice explores the Incomplete LU (ILU) factorization, a popular technique that approximates the true LU decomposition of a matrix while preserving sparsity. You will investigate the structure of an ILU(0) preconditioner for a classic 2D five-point stencil  and use Local Fourier Analysis (LFA) to analyze its properties as a smoother, a key role for preconditioners within advanced multigrid solvers. This exercise connects the abstract theory of preconditioning to both practical implementation details and sophisticated performance analysis.",
            "id": "4234460",
            "problem": "Consider the steady-state one-group neutron diffusion equation in a homogeneous, rectangular reactor core with isotropic scattering neglected and homogeneous Dirichlet boundary conditions, given by $-\\nabla \\cdot (D \\nabla \\phi) + \\Sigma_{a} \\phi = q$, where $D  0$ is the diffusion coefficient, $\\Sigma_{a} \\ge 0$ is the macroscopic absorption cross section, $\\phi$ is the neutron flux, and $q$ is a source term. Discretize this equation over a square domain using a cell-centered finite volume method on a uniform $n \\times n$ grid of interior cells with mesh spacing $h$, and natural lexicographic (row-major) ordering of unknowns. The resulting linear system $A \\phi = b$ has a symmetric positive definite five-point stencil, with diagonal entries $a_{pp} = \\Sigma_{a} + \\frac{4D}{h^{2}}$ and off-diagonal entries $a_{pq} = -\\frac{D}{h^{2}}$ for the four nearest neighbors $q$ of any interior point $p$.\n\nDefine the Incomplete Lower-Upper factorization with zero fill (ILU(0)) for this matrix $A$ under the given ordering, with the convention that the incomplete lower factor $L$ is unit lower triangular and its unit diagonal is not stored, while the incomplete upper factor $U$ contains the diagonal. Assume the usual restriction that ILU(0) permits nonzero entries only at positions where $A$ is nonzero.\n\nStarting from these foundations:\n1) Determine the sparsity (fill-in) pattern of the incomplete lower factor $L$ and the incomplete upper factor $U$ produced by ILU(0) for this five-point stencil under the given ordering.\n2) Compute the total number of stored nonzero entries in $L$ and $U$ combined, counting the diagonal of $U$ once and not storing the unit diagonal of $L$. Express your answer as a closed-form analytic expression in terms of $n$.\n3) Using Local Fourier Analysis (LFA) on the infinite periodic-grid surrogate for the same five-point operator, characterize the forward Gauss–Seidel iteration matrix constructed from the standard splitting $A = L_{A} + D_{A} + U_{A}$, where $L_{A}$ and $U_{A}$ are the strictly lower and strictly upper parts of $A$ in the same ordering and $D_{A}$ is the diagonal of $A$. Derive the Fourier symbol of its error-propagation operator and evaluate its high-frequency amplification at the mode with $(\\theta_{x}, \\theta_{y}) = (\\pi, \\pi)$ when $\\Sigma_{a} = 0$. Briefly compare this action to that of ILU(0) used as a left preconditioner within a stationary Richardson iteration in the context of multigrid cycles.\n\nYour final recorded answer must be the expression from part (2) for the total number of stored nonzero entries as a function of $n$. No rounding is required and no units should be used in the final expression.",
            "solution": "The problem asks for three distinct analyses based on the discretization of the one-group neutron diffusion equation on a uniform $n \\times n$ grid of interior cells. The resulting linear system $A \\phi = b$ involves a symmetric positive definite matrix $A$ with a five-point stencil structure arising from a cell-centered finite volume method with homogeneous Dirichlet boundary conditions. The total number of unknowns is $N=n^2$.\n\n### Part 1: Sparsity Pattern of ILU(0) Factors\n\nThe Incomplete Lower-Upper factorization with zero fill-in, denoted ILU($0$), produces factors $\\tilde{L}$ and $\\tilde{U}$ such that $A \\approx \\tilde{L}\\tilde{U}$. The defining constraint of ILU($0$) is that the sparsity pattern of the factors is a subset of the sparsity pattern of the original matrix $A$. That is, if $a_{ij} = 0$, then $\\tilde{l}_{ij}$ (for $ij$) and $\\tilde{u}_{ij}$ (for $i \\ge j$) are also forced to be zero.\n\nThe matrix $A$ is a five-point stencil matrix. For a grid point $p$ with lexicographic index $p = (i-1)n+j$, where $i,j \\in \\{1,...,n\\}$ are the row and column indices in the grid, the non-zero entries in row $p$ of $A$ are:\n-   $a_{p,p}$: The diagonal entry.\n-   $a_{p, p-1}$: Connection to the West neighbor $(i, j-1)$, if $j1$.\n-   $a_{p, p+1}$: Connection to the East neighbor $(i, j+1)$, if $jn$.\n-   $a_{p, p-n}$: Connection to the North neighbor $(i-1, j)$, if $i1$.\n-   $a_{p, p+n}$: Connection to the South neighbor $(i+1, j)$, if $in$.\n\nThe ILU($0$) algorithm proceeds by variants of Gaussian elimination, but with any new non-zero entry (fill-in) being discarded. For the five-point stencil, fill-in would typically occur at positions corresponding to neighbors of neighbors, for example, at an index corresponding to the North-West neighbor. Specifically, the product of a non-zero in the $-n$ sub-diagonal of $\\tilde{L}$ and a non-zero in the $-1$ sub-diagonal of an intermediate matrix could create fill-in. In ILU(0), this is prevented.\n\nAs a result, the non-zero structure of the factors $\\tilde{L}$ and $\\tilde{U}$ is identical to the non-zero structure of the lower and upper parts of $A$, respectively.\n-   The incomplete lower factor $\\tilde{L}$ is a unit lower triangular matrix. Its non-zero entries (apart from the unit diagonal) are located on the sub-diagonals with offsets $-1$ and $-n$, corresponding to the West and North connections.\n-   The incomplete upper factor $\\tilde{U}$ is an upper triangular matrix. Its non-zero entries are located on the main diagonal and on the super-diagonals with offsets $+1$ and $+n$, corresponding to the center, East, and South connections.\n\nThus, the sparsity pattern of $\\tilde{L}$ and $\\tilde{U}$ combined is the same as the sparsity pattern of $A$.\n\n### Part 2: Total Number of Stored Nonzero Entries\n\nWe need to count the total number of stored entries in $\\tilde{L}$ and $\\tilde{U}$. According to the problem's convention:\n1.  For $\\tilde{L}$, the unit diagonal is not stored. We only store its strictly lower triangular part.\n2.  For $\\tilde{U}$, we store all its entries (diagonal and strictly upper triangular part).\n\nThe total number of stored entries is therefore the number of non-zero entries in the strictly lower part of $A$ plus the number of non-zero entries in the upper part of $A$ (including the diagonal). This is simply the total number of non-zero entries in the original matrix $A$.\n\nLet's calculate the number of non-zero entries in $A$, an $n^2 \\times n^2$ matrix.\n-   **Diagonal entries**: There is one diagonal entry for each of the $n^2$ grid points. Total: $n^2$.\n-   **Off-diagonal entries**: These correspond to connections between adjacent interior grid points.\n    -   **Horizontal connections**: In each of the $n$ rows of the grid, there are $n-1$ connections between adjacent cells. This gives a total of $n(n-1)$ horizontal connections.\n    -   **Vertical connections**: In each of the $n$ columns of the grid, there are $n-1$ connections between adjacent cells. This gives a total of $n(n-1)$ vertical connections.\n\nThe total number of unique connections (or edges in the grid graph) is $n(n-1) + n(n-1) = 2n(n-1)$.\nSince the matrix $A$ is symmetric, each connection corresponds to two non-zero off-diagonal entries (e.g., $a_{pq}$ and $a_{qp}$).\nThe total number of off-diagonal non-zero entries is $2 \\times (\\text{number of connections}) = 2 \\times [2n(n-1)] = 4n(n-1) = 4n^2 - 4n$.\n\nThe total number of non-zero entries in $A$ is the sum of diagonal and off-diagonal entries:\n$NZ(A) = n^2 + (4n^2 - 4n) = 5n^2 - 4n$.\n\nThis is the total number of stored entries for the ILU($0$) factorization:\n-   Number of stored entries in $\\tilde{L}$ (strictly lower) = number of connections = $2n^2 - 2n$.\n-   Number of stored entries in $\\tilde{U}$ (diagonal + upper) = (diagonal entries) + (number of connections) = $n^2 + (2n^2 - 2n) = 3n^2 - 2n$.\n-   Total stored entries = $(2n^2 - 2n) + (3n^2 - 2n) = 5n^2 - 4n$.\n\n### Part 3: Local Fourier Analysis and Comparison\n\nLocal Fourier Analysis (LFA) is used to study the properties of an iterative method on an infinite grid, which approximates the behavior away from boundaries. We analyze the action of the iteration operator on a Fourier mode $\\phi_{j,k} = \\exp(i(j\\theta_x + k\\theta_y))$.\n\n**Gauss-Seidel Iteration:**\nThe matrix $A$ is split as $A = L_A + D_A + U_A$. The forward Gauss-Seidel error propagation operator is $M_{GS} = -(L_A + D_A)^{-1}U_A$. For lexicographic ordering, $L_A$ represents connections to \"past\" nodes (North and West), and $U_A$ represents connections to \"future\" nodes (South and East). Their Fourier symbols are:\n-   $\\tilde{L}_A(\\theta_x, \\theta_y) = -\\frac{D}{h^2}(\\exp(-i\\theta_x) + \\exp(-i\\theta_y))$\n-   $\\tilde{U}_A(\\theta_x, \\theta_y) = -\\frac{D}{h^2}(\\exp(i\\theta_x) + \\exp(i\\theta_y))$\n-   $\\tilde{D}_A(\\theta_x, \\theta_y) = \\Sigma_a + \\frac{4D}{h^2}$\n\nThe Fourier symbol of the Gauss-Seidel operator (amplification factor) is $\\tilde{M}_{GS} = - \\tilde{U}_A / (\\tilde{L}_A + \\tilde{D}_A)$:\n$$ \\tilde{M}_{GS}(\\theta_x, \\theta_y) = \\frac{\\frac{D}{h^2}(\\exp(i\\theta_x) + \\exp(i\\theta_y))}{\\Sigma_a + \\frac{4D}{h^2} - \\frac{D}{h^2}(\\exp(-i\\theta_x) + \\exp(-i\\theta_y))} $$\nFor the case $\\Sigma_a=0$ and the high-frequency mode $(\\theta_x, \\theta_y) = (\\pi, \\pi)$, we have $\\exp(i\\pi)=-1$ and $\\exp(-i\\pi)=-1$. The amplification factor is:\n$$ \\tilde{M}_{GS}(\\pi, \\pi) = \\frac{-1 - 1}{4 - (-1 - 1)} = \\frac{-2}{6} = -\\frac{1}{3} $$\nThe absolute value, $|-1/3| = 1/3$, is the high-frequency amplification factor. Since this value is significantly less than $1$, Gauss-Seidel is an effective smoother for this problem, as it strongly damps high-frequency error components.\n\n**Comparison with ILU(0) Preconditioning:**\nThe ILU($0$) factorization can be used as a preconditioner $M = \\tilde{L}\\tilde{U}$ in a stationary Richardson iteration. The error propagation operator is $E = I - M^{-1}A$.\n-   **Smoothing Mechanism:** Both Gauss-Seidel and ILU($0$) act as smoothers in a multigrid context. A smoother's role is to reduce high-frequency error components.\n-   **Gauss-Seidel:** Its smoothing property arises from the asymmetric nature of the update, where information from different directions is treated differently (old vs. new values). It is computationally simple but its effectiveness can degrade for problems with strong anisotropy, and its convergence can be slow due to its explicit, point-wise nature and dependence on ordering.\n-   **ILU(0):** This is a more implicit smoother. The error in the factorization, $R = \\tilde{L}\\tilde{U} - A$, consists of the fill-in that was discarded. This error is largest for high-frequency modes. Therefore, the preconditioner inverse $M^{-1} = (\\tilde{L}\\tilde{U})^{-1}$ acts very differently from $A^{-1}$ on these modes. When used in a Richardson iteration, this leads to strong damping of high-frequency errors. ILU-based smoothers are generally more robust than point-wise smoothers like Gauss-Seidel, especially for more complex problems with anisotropic coefficients, as they capture more of the couplings within the operator $A$. For the simple isotropic problem here, both methods are effective smoothers.",
            "answer": "$$\\boxed{5n^2 - 4n}$$"
        }
    ]
}