{
    "hands_on_practices": [
        {
            "introduction": "A cornerstone of the Best Estimate Plus Uncertainty (BEPU) framework is the ability to make robust statements about safety margins without making strong assumptions about the underlying distribution of a simulated quantity. This practice guides you through the derivation of the one-sided Wilks' tolerance interval, a powerful nonparametric tool. By applying it to a common nuclear safety parameter, the Departure from Nucleate Boiling Ratio (DNBR), you will see how to connect statistical confidence ($\\gamma$) and population coverage ($p$) to the required number of code runs ($N$) to meet a specific safety goal .",
            "id": "4251480",
            "problem": "A Best Estimate Plus Uncertainty (BEPU) analysis is performed for a pressurized water reactor core to assess the Departure from Nucleate Boiling Ratio (DNBR), defined as the ratio of the critical heat flux to the actual local heat flux. Because higher DNBR implies greater thermal margin, the risk-relevant variable is taken as the inverse DNBR, $Y = 1/\\mathrm{DNBR}$, so that larger values of $Y$ are more adverse. Assume $N = 59$ independent and identically distributed BEPU realizations of $Y$ are generated under steady, rated conditions, and that the true distribution of $Y$ is continuous.\n\nIn a one-sided Wilks nonparametric tolerance approach, an upper tolerance limit for $Y$ is defined by the largest observed value in the sample. The regulatory safety goal for DNBR is specified as follows: the one-sided tolerance statement must ensure that at least a fraction $p_{\\mathrm{goal}} = 0.95$ of the population lies below the upper tolerance limit with confidence $\\gamma_{\\mathrm{goal}} = 0.95$. In terms of $Y$, this means that with confidence at least $0.95$, at least $0.95$ of operating states will have $Y$ below the bound, equivalently DNBR above the inverse of the bound.\n\nStarting from first principles of order statistics and the probability integral transform, derive the relationship between the tolerance content $p$ and confidence $\\gamma$ for the one-sided upper Wilks bound built from the sample maximum. Then, using $N = 59$, compute the achievable pair $(p,\\gamma)$ when $p$ is set equal to $p_{\\mathrm{goal}}$. Express $p$ and $\\gamma$ as decimals and round $\\gamma$ to four significant figures. Finally, state whether the resulting pair satisfies the specified safety goal for DNBR.",
            "solution": "The user wants me to solve the following problem:\nA Best Estimate Plus Uncertainty (BEPU) analysis is performed for a pressurized water reactor core to assess the Departure from Nucleate Boiling Ratio (DNBR), defined as the ratio of the critical heat flux to the actual local heat flux. Because higher DNBR implies greater thermal margin, the risk-relevant variable is taken as the inverse DNBR, $Y = 1/\\mathrm{DNBR}$, so that larger values of $Y$ are more adverse. Assume $N = 59$ independent and identically distributed BEPU realizations of $Y$ are generated under steady, rated conditions, and that the true distribution of $Y$ is continuous.\n\nIn a one-sided Wilks nonparametric tolerance approach, an upper tolerance limit for $Y$ is defined by the largest observed value in the sample. The regulatory safety goal for DNBR is specified as follows: the one-sided tolerance statement must ensure that at least a fraction $p_{\\mathrm{goal}} = 0.95$ of the population lies below the upper tolerance limit with confidence $\\gamma_{\\mathrm{goal}} = 0.95$. In terms of $Y$, this means that with confidence at least $0.95$, at least $0.95$ of operating states will have $Y$ below the bound, equivalently DNBR above the inverse of the bound.\n\nStarting from first principles of order statistics and the probability integral transform, derive the relationship between the tolerance content $p$ and confidence $\\gamma$ for the one-sided upper Wilks bound built from the sample maximum. Then, using $N = 59$, compute the achievable pair $(p,\\gamma)$ when $p$ is set equal to $p_{\\mathrm{goal}}$. Express $p$ and $\\gamma$ as decimals and round $\\gamma$ to four significant figures. Finally, state whether the resulting pair satisfies the specified safety goal for DNBR.\n\n### Step 1: Extract Givens\n-   Variable of interest: $Y = 1/\\mathrm{DNBR}$.\n-   Sample size: $N = 59$.\n-   Sample properties: independent and identically distributed (i.i.d.).\n-   Distribution of $Y$: continuous.\n-   Tolerance limit: one-sided upper limit defined by the largest observed value in the sample (sample maximum).\n-   Safety goal for population fraction (content): $p_{\\mathrm{goal}} = 0.95$.\n-   Safety goal for confidence: $\\gamma_{\\mathrm{goal}} = 0.95$.\n-   Task 1: Derive the relationship between content $p$ and confidence $\\gamma$ for the one-sided upper Wilks bound from first principles.\n-   Task 2: Compute the achievable pair $(p, \\gamma)$ for $N=59$ when $p$ is set to $p_{\\mathrm{goal}}$.\n-   Task 3: Round the computed $\\gamma$ to four significant figures.\n-   Task 4: State whether the computed $(p, \\gamma)$ pair meets the safety goal.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, well-posed, and objective. It is rooted in standard statistical methods (nonparametric tolerance intervals, specifically Wilks' formula) applied to a real-world problem in nuclear engineering (BEPU analysis of DNBR). The concepts of order statistics and the probability integral transform are fundamental principles in mathematical statistics. The provided data are complete and consistent, and the problem is structured to have a unique, verifiable solution. There are no scientific flaws, ambiguities, or contradictions.\n\n### Step 3: Verdict and Action\nThe problem is valid. A full solution will be provided.\n\n### Solution Derivation and Calculation\n\nThe problem requires the derivation of the relationship between the tolerance content $p$ and the confidence level $\\gamma$ for a one-sided upper tolerance limit constructed from the maximum value of a sample of size $N$.\n\nLet $Y_1, Y_2, \\dots, Y_N$ be the $N$ independent and identically distributed (i.i.d.) random variables representing the BEPU realizations of $Y = 1/\\mathrm{DNBR}$. Let their common, unknown, continuous cumulative distribution function (CDF) be $F_Y(y) = P(Y \\le y)$.\n\nThe order statistics of the sample are denoted by $Y_{(1)} \\le Y_{(2)} \\le \\dots \\le Y_{(N)}$. The problem states that the one-sided upper tolerance limit is the sample maximum, $T_U = Y_{(N)}$. A one-sided upper tolerance limit $T_U$ is a statistic such that we can claim with a certain confidence $\\gamma$ that at least a fraction $p$ of the population is less than $T_U$.\n\nThe fraction of the population covered by the statistical interval $(-\\infty, T_U]$ is given by the random variable $C = F_Y(T_U) = F_Y(Y_{(N)})$. This variable $C$ is the *coverage* or *content* of the tolerance interval.\n\nThe confidence level $\\gamma$ is the probability that this random coverage $C$ is at least the desired population fraction $p$. Mathematically, this is expressed as:\n$$ \\gamma = P(C \\ge p) $$\nSubstituting the expression for $C$:\n$$ \\gamma = P(F_Y(Y_{(N)}) \\ge p) $$\n\nTo proceed from first principles, we utilize the property that for a continuous and monotonically increasing CDF $F_Y$, the inequality can be inverted. Let $y_p$ be the true $p$-th quantile of the distribution of $Y$, which is defined by the relation $F_Y(y_p) = p$. Applying the inverse function $F_Y^{-1}$ to both sides of the inequality inside the probability statement, we get:\n$$ Y_{(N)} \\ge F_Y^{-1}(p) = y_p $$\nThus, the confidence $\\gamma$ is the probability that the sample maximum is greater than or equal to the true $p$-th quantile of the population:\n$$ \\gamma = P(Y_{(N)} \\ge y_p) $$\nThis can be more conveniently calculated using the complementary event:\n$$ \\gamma = 1 - P(Y_{(N)} < y_p) $$\nThe event $Y_{(N)} < y_p$ means that the maximum value in the sample is less than $y_p$. This occurs if and only if all $N$ observations in the sample are less than $y_p$:\n$$ Y_{(N)} < y_p \\iff (Y_1 < y_p) \\cap (Y_2 < y_p) \\cap \\dots \\cap (Y_N < y_p) $$\nSince the observations $Y_i$ are i.i.d., the probability of this joint event is the product of the individual probabilities:\n$$ P(Y_{(N)} < y_p) = \\prod_{i=1}^{N} P(Y_i < y_p) $$\nBy the definition of the CDF, $P(Y_i < y_p) = F_Y(y_p)$ for any $i$. Since $y_p$ is the $p$-th quantile, $F_Y(y_p) = p$. Therefore:\n$$ P(Y_{(N)} < y_p) = \\prod_{i=1}^{N} p = p^N $$\nSubstituting this result back into the expression for the confidence $\\gamma$, we arrive at the desired relationship:\n$$ \\gamma = 1 - p^N $$\nThis is the well-known one-sided Wilks' formula for the largest order statistic. It relates the sample size $N$, the population coverage $p$, and the statistical confidence $\\gamma$. The derivation relies only on the i.i.d. nature of the continuous random variables and the fundamental definitions of CDFs and quantiles, consistent with the requirement to start from first principles. The probability integral transform is implicitly used in the step where $P(Y_i < y_p)$ is identified as $p$, as this is the core idea of transforming a random variable by its own CDF to a uniform variable.\n\nWith the derived formula, we can now perform the required calculation. We are given:\n-   Sample size, $N = 59$.\n-   Desired population fraction, $p$, which is set to the goal value $p_{\\mathrm{goal}} = 0.95$.\n\nWe need to compute the achievable confidence $\\gamma$ for this pair of $(N, p)$:\n$$ \\gamma = 1 - (0.95)^{59} $$\nThe calculation is as follows:\n$$ \\gamma \\approx 1 - 0.0484964603 $$\n$$ \\gamma \\approx 0.9515035397 $$\nThe problem requires rounding the result for $\\gamma$ to four significant figures.\n$$ \\gamma \\approx 0.9515 $$\nSo, the achievable pair is $(p, \\gamma) = (0.95, 0.9515)$.\n\nFinally, we must assess whether this result satisfies the specified safety goal. The regulatory goal requires that the tolerance statement holds for a population fraction of at least $p_{\\mathrm{goal}} = 0.95$ with a confidence of at least $\\gamma_{\\mathrm{goal}} = 0.95$. This means we must satisfy two conditions:\n$1$. $p \\ge p_{\\mathrm{goal}}$\n$2$. $\\gamma \\ge \\gamma_{\\mathrm{goal}}$\n\nOur achieved pair is $(p, \\gamma) = (0.95, 0.9515)$. Let's check the conditions:\n$1$. $p = 0.95$, so $0.95 \\ge 0.95$. This condition is met.\n$2$. $\\gamma \\approx 0.9515$, so $0.9515 \\ge 0.95$. This condition is also met.\n\nSince both conditions are satisfied, the resulting pair $(p, \\gamma)$ successfully meets the specified safety goal. The choice of $N=59$ is a standard practice in the nuclear industry precisely because it is the smallest integer sample size that satisfies the \"95/95\" one-sided tolerance criterion. For $N=58$, $\\gamma = 1-0.95^{58} \\approx 0.9489$, which would fail to meet the confidence requirement.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.95 & 0.9515\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "In any realistic uncertainty analysis, input parameters are rarely independent. This computational exercise explores the critical impact of input correlations on the final uncertainty quantification for a key safety parameter, the Peak Cladding Temperature (PCT). Using a response surface model, you will implement a Monte Carlo simulation to directly compare the output distribution under assumptions of independence versus correlation, gaining tangible insight into how these statistical dependencies can alter the estimated safety margins and why they must be carefully considered in a BEPU framework .",
            "id": "4251476",
            "problem": "Consider a Best-Estimate Plus Uncertainty (BEPU) analysis of a pressurized water reactor, where the quantity of interest is the Peak Cladding Temperature (PCT). Assume that the uncertain input vector has dimension $d=5$ and is modeled as a zero-mean multivariate normal random vector $\\mathbf{z}\\in\\mathbb{R}^5$ with covariance matrix $\\boldsymbol{\\Sigma}\\in\\mathbb{R}^{5\\times 5}$. The PCT response is represented by a second-order response surface\n$$\nT(\\mathbf{z}) \\;=\\; T_0 \\;+\\; \\mathbf{c}^\\top \\mathbf{z} \\;+\\; \\tfrac{1}{2}\\,\\mathbf{z}^\\top \\mathbf{H}\\,\\mathbf{z},\n$$\nwhere $T_0\\in\\mathbb{R}$ is the baseline PCT, $\\mathbf{c}\\in\\mathbb{R}^5$ is a sensitivity vector, and $\\mathbf{H}\\in\\mathbb{R}^{5\\times 5}$ is a symmetric curvature matrix. All temperatures are expressed in Kelvin (K). The BEPU margin with respect to a safety limit $T_{\\text{limit}}$ is defined as\n$$\nM \\;=\\; T_{\\text{limit}} \\;-\\; Q_{0.95}[T(\\mathbf{z})],\n$$\nwhere $Q_{0.95}[\\cdot]$ denotes the $0.95$ quantile of the PCT distribution.\n\nYour task is to compute, for several specified covariance matrices, the change in the output $0.95$ quantile of $T(\\mathbf{z})$ when replacing independent sampling (i.e., using $\\boldsymbol{\\Sigma}=\\mathbf{I}$) with correlated sampling (i.e., using the specified $\\boldsymbol{\\Sigma}$), and from it the change in BEPU margin. You must implement the following from first principles:\n\n- Use the definition of a multivariate normal distribution and the Cholesky factorization. Specifically, if $\\mathbf{e}\\sim \\mathcal{N}(\\mathbf{0},\\mathbf{I})$, then generate correlated samples via $\\mathbf{z} = \\mathbf{L}\\mathbf{e}$, where $\\mathbf{L}$ satisfies $\\mathbf{L}\\mathbf{L}^\\top = \\boldsymbol{\\Sigma}$.\n- Construct a fixed sample matrix $\\mathbf{E}\\in\\mathbb{R}^{d\\times N}$ to ensure reproducibility and variance reduction by antithetic pairing. Let $N=2M$ with $M=100{,}000$. Generate $\\mathbf{G}\\in\\mathbb{R}^{d\\times M}$ with independent standard normal entries using a pseudo-random generator seeded with $12345$, then set $\\mathbf{E} = [\\mathbf{G},\\,-\\mathbf{G}]$ (column-wise concatenation). This ensures that the independent baseline and all correlated cases are driven by the same underlying standard normal variates.\n- For each covariance matrix $\\boldsymbol{\\Sigma}$ in the test suite, compute the $0.95$ quantile $Q_{0.95}[T(\\mathbf{z})]$ under that $\\boldsymbol{\\Sigma}$ and under the independent case $\\mathbf{I}$ using the same $\\mathbf{E}$, then report the change in quantile\n$$\n\\Delta Q \\;=\\; Q_{0.95}[T(\\mathbf{z});\\,\\boldsymbol{\\Sigma}] \\;-\\; Q_{0.95}[T(\\mathbf{z});\\,\\mathbf{I}]\n$$\nin Kelvin, rounded to three decimals. Also compute the corresponding change in margin $\\Delta M$, rounded to three decimals, where\n$$\n\\Delta M \\;=\\; -\\,\\Delta Q.\n$$\n\nUse the following fixed model coefficients and safety limit:\n- Baseline PCT: $T_0 = 1100~\\text{K}$.\n- Sensitivity vector: $\\mathbf{c} = [30,\\,-20,\\,15,\\,-10,\\,25]^\\top~\\text{K}$.\n- Symmetric curvature matrix (units of K in the quadratic form):\n$$\n\\mathbf{H} \\;=\\; \\begin{bmatrix}\n3.0 & 1.0 & 0.0 & 0.0 & 0.0 \\\\\n1.0 & 2.0 & 0.5 & 0.0 & 0.0 \\\\\n0.0 & 0.5 & 1.5 & 0.3 & 0.0 \\\\\n0.0 & 0.0 & 0.3 & 1.0 & 0.2 \\\\\n0.0 & 0.0 & 0.0 & 0.2 & 1.2\n\\end{bmatrix}.\n$$\n- Safety limit: $T_{\\text{limit}} = 1477~\\text{K}$.\n- Quantile level: $0.95$.\n\nFor the covariance matrices, use the following test suite, each specified as a $5\\times 5$ correlation matrix (i.e., with unit variances). In each case, the independent baseline is $\\mathbf{I}$.\n1. Identity (baseline-equivalent):\n$$\n\\boldsymbol{\\Sigma}_1 \\;=\\; \\mathbf{I}_{5}.\n$$\n2. Equicorrelated positive case ($\\rho=0.6$):\n$$\n\\boldsymbol{\\Sigma}_2 \\;=\\; (1-\\rho)\\mathbf{I}_5 \\;+\\; \\rho\\,\\mathbf{1}\\mathbf{1}^\\top,\\quad \\rho=0.6,\n$$\nwhere $\\mathbf{1}\\in\\mathbb{R}^5$ is the vector of ones.\n3. Single strong negative pair:\n$$\n\\boldsymbol{\\Sigma}_3 \\;=\\; \\mathbf{I}_5 \\;+\\; \\rho(\\mathbf{e}_1\\mathbf{e}_2^\\top+\\mathbf{e}_2\\mathbf{e}_1^\\top),\\quad \\rho=-0.6,\n$$\nwhere $\\mathbf{e}_i$ is the $i$th standard basis vector.\n4. Near-singular equicorrelated positive case ($\\rho=0.95$):\n$$\n\\boldsymbol{\\Sigma}_4 \\;=\\; (1-\\rho)\\mathbf{I}_5 \\;+\\; \\rho\\,\\mathbf{1}\\mathbf{1}^\\top,\\quad \\rho=0.95.\n$$\n\nImplementation requirements:\n- Compute the PCT values using the response surface for all $N$ samples in each case.\n- Use the same $\\mathbf{E}$ for the independent and correlated cases to eliminate sampling noise in the difference.\n- Compute empirical $0.95$ quantiles using a standard definition of sample quantiles consistent with linear interpolation between order statistics.\n\nYour program must produce a single line of output containing the results for the four test cases aggregated into a single list in the following order:\n$$\n[\\Delta Q_1,\\Delta M_1,\\Delta Q_2,\\Delta M_2,\\Delta Q_3,\\Delta M_3,\\Delta Q_4,\\Delta M_4],\n$$\nwith each entry expressed in Kelvin (K) as a decimal number rounded to three decimals. For the identity case, $\\Delta Q_1$ and $\\Delta M_1$ should be exactly $0.000$ when computed as specified.\n\nYour solution must be purely computational and must not require any user input or external data. The numerical answer must be printed exactly as a single line in the specified format. Angles are not used in this problem. Percentages must be treated as decimals (for example, $0.95$ rather than $95\\%$).",
            "solution": "The problem is valid. It is a well-posed and scientifically sound problem in the field of nuclear reactor safety analysis, specifically concerning Best-Estimate Plus Uncertainty (BEPU) methods. All required data, models, and computational procedures are fully specified and consistent.\n\nThe core task is to quantify the impact of input parameter correlations on the $0.95$ quantile of the Peak Cladding Temperature (PCT), which is modeled by a quadratic response surface. This is achieved through a Monte Carlo simulation. The change in the quantile, $\\Delta Q$, directly translates into a change in the safety margin, $\\Delta M$.\n\nThe methodology is founded on the principles of uncertainty propagation and statistical analysis. The steps are as follows:\n\nFirst, we define the response surface model for the PCT, $T(\\mathbf{z})$, as a function of a $d$-dimensional standard normal input vector $\\mathbf{z}$:\n$$\nT(\\mathbf{z}) \\;=\\; T_0 \\;+\\; \\mathbf{c}^\\top \\mathbf{z} \\;+\\; \\tfrac{1}{2}\\,\\mathbf{z}^\\top \\mathbf{H}\\,\\mathbf{z}\n$$\nThe model parameters are given as:\n- Dimension of input vector: $d=5$.\n- Baseline PCT: $T_0 = 1100$ K.\n- Sensitivity vector: $\\mathbf{c} = [30,\\,-20,\\,15,\\,-10,\\,25]^\\top$ K.\n- Symmetric curvature matrix:\n$$\n\\mathbf{H} \\;=\\; \\begin{bmatrix}\n3.0 & 1.0 & 0.0 & 0.0 & 0.0 \\\\\n1.0 & 2.0 & 0.5 & 0.0 & 0.0 \\\\\n0.0 & 0.5 & 1.5 & 0.3 & 0.0 \\\\\n0.0 & 0.0 & 0.3 & 1.0 & 0.2 \\\\\n0.0 & 0.0 & 0.0 & 0.2 & 1.2\n\\end{bmatrix}\n$$\n\nSecond, a fixed set of $N=200,000$ independent standard normal samples, $\\mathbf{e} \\sim \\mathcal{N}(\\mathbf{0},\\mathbf{I})$, is generated to serve as the common underlying source of randomness for all calculations. This is crucial for isolating the effect of correlation. These samples are structured in a matrix $\\mathbf{E} \\in \\mathbb{R}^{d \\times N}$. To reduce sampling variance, this matrix is constructed using antithetic pairing. A matrix $\\mathbf{G} \\in \\mathbb{R}^{d \\times M}$ (where $M=N/2=100,000$) of independent standard normal deviates is generated using a pseudo-random number generator seeded with $12345$. The final sample matrix is then $\\mathbf{E} = [\\mathbf{G}, -\\mathbf{G}]$.\n\nThird, the baseline case assumes a covariance matrix $\\boldsymbol{\\Sigma} = \\mathbf{I}$, meaning the input parameters are independent. In this case, the samples are directly $\\mathbf{z} = \\mathbf{E}$. The corresponding PCT values are calculated, and the empirical $0.95$ quantile, $Q_{0.95}[T(\\mathbf{z}); \\mathbf{I}]$, is computed. This quantile is found using linear interpolation between the order statistics of the $N$ PCT values.\n\nFourth, for each specified covariance matrix $\\boldsymbol{\\Sigma}_k$ in the test suite, we introduce correlations. Correlated random vectors $\\mathbf{z}_k \\sim \\mathcal{N}(\\mathbf{0}, \\boldsymbol{\\Sigma}_k)$ are generated from the base independent samples $\\mathbf{E}$ via a linear transformation using the Cholesky decomposition. We find a lower triangular matrix $\\mathbf{L}_k$ such that $\\boldsymbol{\\Sigma}_k = \\mathbf{L}_k \\mathbf{L}_k^\\top$. The correlated samples are then given by the matrix $\\mathbf{Z}_k = \\mathbf{L}_k \\mathbf{E}$.\n\nThe PCT response is then evaluated for this new set of samples, $\\mathbf{Z}_k$, and the corresponding $0.95$ quantile, $Q_{0.95}[T(\\mathbf{z}); \\boldsymbol{\\Sigma}_k]$, is computed.\n\nFinally, for each case, the change in quantile is calculated as:\n$$\n\\Delta Q_k \\;=\\; Q_{0.95}[T(\\mathbf{z});\\,\\boldsymbol{\\Sigma}_k] \\;-\\; Q_{0.95}[T(\\mathbf{z});\\,\\mathbf{I}]\n$$\nThe corresponding change in the BEPU margin is simply its negative:\n$$\n\\Delta M_k \\;=\\; - \\Delta Q_k\n$$\n\nThe required calculations are performed for the four specified covariance matrices:\n1.  $\\boldsymbol{\\Sigma}_1 = \\mathbf{I}_5$.\n2.  $\\boldsymbol{\\Sigma}_2$ with equicorrelation $\\rho=0.6$.\n3.  $\\boldsymbol{\\Sigma}_3$ with a single negative correlation $\\rho=-0.6$ between inputs $1$ and $2$.\n4.  $\\boldsymbol{\\Sigma}_4$ with equicorrelation $\\rho=0.95$.\n\nFor efficient computation, the PCT values for all $N$ samples are calculated simultaneously using vectorized operations. For a sample matrix $\\mathbf{Z} \\in \\mathbb{R}^{d \\times N}$, the vector of PCTs is computed as:\n$$\n\\mathbf{T} = T_0 + \\mathbf{c}^\\top \\mathbf{Z} + \\frac{1}{2} \\text{diag}(\\mathbf{Z}^\\top \\mathbf{H} \\mathbf{Z})\n$$\nwhere the quadratic term can be implemented by summing the element-wise product of $\\mathbf{Z}$ and $\\mathbf{H}\\mathbf{Z}$ along the feature dimension. The final numerical results are rounded to three decimal places and presented in the specified list format.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import cholesky\n\ndef solve():\n    \"\"\"\n    Solves the BEPU analysis problem by calculating changes in the PCT quantile\n    due to input parameter correlations.\n    \"\"\"\n    # ====== Step 1: Define Constants and Model Parameters ======\n    d = 5\n    M = 100_000\n    N = 2 * M\n    seed = 12345\n    q_level = 0.95\n    \n    # Model coefficients\n    T0 = 1100.0\n    c = np.array([[30.0], [-20.0], [15.0], [-10.0], [25.0]]) # shape (d, 1)\n    H = np.array([\n        [3.0, 1.0, 0.0, 0.0, 0.0],\n        [1.0, 2.0, 0.5, 0.0, 0.0],\n        [0.0, 0.5, 1.5, 0.3, 0.0],\n        [0.0, 0.0, 0.3, 1.0, 0.2],\n        [0.0, 0.0, 0.0, 0.2, 1.2]\n    ])\n\n    # ====== Step 2: Generate Base Samples ======\n    # Use a fixed seed for reproducibility\n    rng = np.random.default_rng(seed)\n    # Generate G with shape (d, M)\n    G = rng.normal(size=(d, M))\n    # Construct E with antithetic pairing, shape (d, N)\n    E = np.concatenate([G, -G], axis=1)\n\n    # ====== Helper function to calculate PCT ======\n    def calculate_pct(Z, T0_val, c_vec, H_mat):\n        \"\"\"\n        Calculates PCT values for a matrix of samples Z.\n        Z is expected to have shape (d, N).\n        \"\"\"\n        linear_term = c_vec.T @ Z\n        # The quadratic term z.T @ H @ z is computed efficiently for all samples\n        # by summing the element-wise product of Z and (H @ Z)\n        quadratic_term = 0.5 * np.sum(Z * (H_mat @ Z), axis=0, keepdims=True)\n        return (T0_val + linear_term + quadratic_term).flatten()\n\n    # ====== Step 3: Calculate Baseline Quantile (Independent Case) ======\n    # For the independent case, Sigma = I, so L = I and Z = E.\n    pct_independent = calculate_pct(E, T0, c, H)\n    q_independent = np.quantile(pct_independent, q_level, method='linear')\n\n    # ====== Step 4: Define Test Cases for Covariance Matrices ======\n    I5 = np.identity(d)\n    ones_mat = np.ones((d, d))\n    \n    rho_2 = 0.6\n    Sigma_2 = (1 - rho_2) * I5 + rho_2 * ones_mat\n    \n    rho_3 = -0.6\n    E12 = np.zeros((d, d))\n    E12[0, 1] = E12[1, 0] = 1.0\n    Sigma_3 = I5 + rho_3 * E12\n    \n    rho_4 = 0.95\n    Sigma_4 = (1 - rho_4) * I5 + rho_4 * ones_mat\n    \n    test_cases = [\n        I5,\n        Sigma_2,\n        Sigma_3,\n        Sigma_4\n    ]\n\n    results = []\n\n    # ====== Step 5: Process Each Correlated Case ======\n    for Sigma in test_cases:\n        # Perform Cholesky decomposition: Sigma = L * L.T\n        try:\n            L = cholesky(Sigma, lower=True)\n        except np.linalg.LinAlgError:\n            # Handle cases where Sigma might not be positive definite\n            # though all provided matrices are.\n            results.extend([\"NaN\", \"NaN\"])\n            continue\n\n        # Generate correlated samples: Z_corr = L * E\n        Z_correlated = L @ E\n\n        # Calculate PCTs for correlated samples\n        pct_correlated = calculate_pct(Z_correlated, T0, c, H)\n        \n        # Compute the 0.95 quantile for the correlated case\n        q_correlated = np.quantile(pct_correlated, q_level, method='linear')\n\n        # Calculate change in quantile and margin\n        delta_Q = q_correlated - q_independent\n        delta_M = -delta_Q\n\n        results.append(f\"{delta_Q:.3f}\")\n        results.append(f\"{delta_M:.3f}\")\n\n    # ====== Final Output ======\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The total uncertainty in a BEPU analysis is a composite of multiple sources, including the numerical error inherent in the computer simulation itself. This practice shifts focus from propagated input uncertainty to the quantification of discretization error, a key aspect of code verification. By applying the Grid Convergence Index (GCI) method to a set of simulation results on different computational meshes, you will learn a standardized procedure to estimate the uncertainty of your \"best estimate\" solution and calculate an extrapolated, more accurate value .",
            "id": "4251472",
            "problem": "A Pressurized Water Reactor (PWR) core thermal-hydraulics simulation is performed to predict the core outlet temperature under nominal power at steady state as part of a Best Estimate Plus Uncertainty (BEPU) assessment. To quantify discretization uncertainty using the Grid Convergence Index (GCI) framework, consider three successively coarsened computational meshes with a constant refinement ratio $r$ defined by $r = h_{2}/h_{1} = h_{3}/h_{2}$, where $h_{i}$ denotes the representative cell size on grid $i$. The simulation produces the following outlet temperatures on the fine, medium, and coarse grids, respectively:\n- Fine grid ($i=1$): $\\phi_{1} = 594.5~\\text{K}$,\n- Medium grid ($i=2$): $\\phi_{2} = 595.5~\\text{K}$,\n- Coarse grid ($i=3$): $\\phi_{3} = 599.5~\\text{K}$,\nwith refinement ratio $r = 2$.\nAssume the solution is in the asymptotic range of grid convergence (monotone and non-oscillatory), and adopt the classical leading-order truncation error model $\\phi(h) = \\phi_{0} + C h^{p}$, where $\\phi_{0}$ is the mesh-independent solution, $C$ is a constant, and $p$ is the observed order of accuracy. Using the above model as the fundamental base, do the following:\n1. From the three-grid data, determine the observed order $p$.\n2. Using the same model, obtain the Richardson extrapolated estimate of the outlet temperature $\\phi_{0}$.\n3. Estimate the fine-grid Grid Convergence Index per Roache (Grid Convergence Index (GCI)) using a safety factor $F_{s} = 1.25$.\nExpress the extrapolated outlet temperature in Kelvin. Express the GCI as a decimal (not a percentage). Round the observed order $p$ to three significant figures, the extrapolated temperature $\\phi_{0}$ to four significant figures, and the GCI to three significant figures. Your final answer must be a single row vector containing, in order, the rounded values of $p$, $\\phi_{0}$, and the GCI.",
            "solution": "The user-provided problem statement has been validated and is deemed sound, well-posed, and objective. It describes a standard application of the Grid Convergence Index (GCI) method for quantifying discretization uncertainty in computational simulations, a cornerstone of verification and validation procedures in engineering and applied physics. All necessary data and a clear theoretical framework are provided.\n\nThe solution proceeds by applying the principles of Richardson extrapolation and the GCI methodology as defined by Roache and adopted in standard practices (e.g., by ASME). The fundamental assumption is the leading-order truncation error model, given as:\n$$\n\\phi(h) = \\phi_{0} + C h^{p}\n$$\nwhere $\\phi(h)$ is the solution on a grid with characteristic cell size $h$, $\\phi_{0}$ is the exact solution for the given partial differential equations (estimated by extrapolation to $h=0$), $C$ is a constant, and $p$ is the observed order of accuracy of the numerical method.\n\nThe provided data consists of three solutions on successively refined grids: $\\phi_{1} = 594.5$ on the fine grid ($h_1$), $\\phi_{2} = 595.5$ on the medium grid ($h_2$), and $\\phi_{3} = 599.5$ on the coarse grid ($h_3$). The grid refinement ratio is constant, $r = h_{2}/h_{1} = h_{3}/h_{2} = 2$.\n\nThe solution is determined in three stages as requested.\n\n1. Determination of the Observed Order of Accuracy, $p$:\n\nWe can write the error model for the three grids:\n$$\n\\phi_{1} = \\phi_{0} + C h_1^{p}\n$$\n$$\n\\phi_{2} = \\phi_{0} + C h_2^{p} = \\phi_{0} + C (r h_1)^{p}\n$$\n$$\n\\phi_{3} = \\phi_{0} + C h_3^{p} = \\phi_{0} + C (r h_2)^{p} = \\phi_{0} + C (r^2 h_1)^{p}\n$$\nConsider the differences between solutions on successive grids:\n$$\n\\epsilon_{21} = \\phi_{2} - \\phi_{1} = C (h_2^{p} - h_1^{p}) = C h_1^{p} (r^{p} - 1)\n$$\n$$\n\\epsilon_{32} = \\phi_{3} - \\phi_{2} = C (h_3^{p} - h_2^{p}) = C ( (r h_2)^{p} - h_2^{p} ) = C h_2^{p} (r^{p} - 1) = C (r h_1)^{p} (r^{p} - 1)\n$$\nThe ratio of these differences isolates the term $r^p$:\n$$\n\\frac{\\epsilon_{32}}{\\epsilon_{21}} = \\frac{C (r h_1)^{p} (r^{p} - 1)}{C h_1^{p} (r^{p} - 1)} = r^p\n$$\nThus, $p$ can be calculated from the three grid solutions:\n$$\np = \\frac{\\ln\\left(\\frac{\\epsilon_{32}}{\\epsilon_{21}}\\right)}{\\ln(r)} = \\frac{\\ln\\left(\\frac{\\phi_{3} - \\phi_{2}}{\\phi_{2} - \\phi_{1}}\\right)}{\\ln(r)}\n$$\nSubstituting the given values:\n$$\n\\phi_{2} - \\phi_{1} = 595.5 - 594.5 = 1.0\n$$\n$$\n\\phi_{3} - \\phi_{2} = 599.5 - 595.5 = 4.0\n$$\n$$\np = \\frac{\\ln\\left(\\frac{4.0}{1.0}\\right)}{\\ln(2)} = \\frac{\\ln(4)}{\\ln(2)} = \\frac{\\ln(2^2)}{\\ln(2)} = \\frac{2 \\ln(2)}{\\ln(2)} = 2\n$$\nThe problem requires rounding $p$ to three significant figures, which yields $p = 2.00$.\n\n2. Richardson Extrapolation for the Mesh-Independent Solution, $\\phi_{0}$:\n\nUsing the first two equations of the error model, we can eliminate the unknown term $C h_1^{p}$ to solve for $\\phi_0$.\nFrom $\\phi_1 = \\phi_0 + C h_1^p$ and $\\phi_2 = \\phi_0 + C h_1^p r^p$, we can express $C h_1^p = \\phi_1 - \\phi_0$ and substitute this into the second equation:\n$$\n\\phi_2 = \\phi_0 + (\\phi_1 - \\phi_0) r^p\n$$\nSolving for $\\phi_0$:\n$$\n\\phi_2 = \\phi_0(1 - r^p) + \\phi_1 r^p\n$$\n$$\n\\phi_0(r^p - 1) = \\phi_1 r^p - \\phi_2\n$$\n$$\n\\phi_0 = \\frac{\\phi_1 r^p - \\phi_2}{r^p - 1} = \\frac{\\phi_1 (r^p - 1) + \\phi_1 - \\phi_2}{r^p - 1} = \\phi_1 + \\frac{\\phi_1 - \\phi_2}{r^p - 1}\n$$\nThis is the standard Richardson extrapolation formula. Substituting the values for $\\phi_1$, $\\phi_2$, $r$, and the calculated value of $p$:\n$$\n\\phi_{0} = 594.5 + \\frac{594.5 - 595.5}{2^{2} - 1} = 594.5 + \\frac{-1.0}{3} = 594.5 - 0.3333... = 594.1666...\n$$\nRounding the extrapolated temperature to four significant figures gives $\\phi_{0} = 594.2$.\n\n3. Estimation of the Fine-Grid Grid Convergence Index (GCI):\n\nThe GCI provides an uncertainty estimate for the fine-grid solution. The GCI for the fine grid, based on the solutions from grids $1$ and $2$, is defined as:\n$$\n\\text{GCI}_{\\text{fine}} = F_s \\left| \\frac{e_a}{\\phi_1} \\right|\n$$\nwhere $F_s$ is a factor of safety and $e_a$ is the estimated error in the fine-grid solution $\\phi_1$. The error $e_a = \\phi_1 - \\phi_0$ is estimated using the Richardson extrapolation formula:\n$$\ne_a = \\phi_1 - \\phi_0 = \\phi_1 - \\left( \\phi_1 + \\frac{\\phi_1 - \\phi_2}{r^p - 1} \\right) = - \\left( \\frac{\\phi_1 - \\phi_2}{r^p - 1} \\right) = \\frac{\\phi_2 - \\phi_1}{r^p - 1}\n$$\nThe GCI is then expressed as a relative error, here normalized by $\\phi_1$:\n$$\n\\text{GCI}_{\\text{fine}} = F_s \\frac{|e_a|}{|\\phi_1|} = \\frac{F_s}{r^p - 1} \\left| \\frac{\\phi_2 - \\phi_1}{\\phi_1} \\right|\n$$\nThe problem specifies a safety factor $F_s = 1.25$. Substituting the known values:\n$$\n\\text{GCI}_{\\text{fine}} = \\frac{1.25}{2^{2} - 1} \\left| \\frac{595.5 - 594.5}{594.5} \\right| = \\frac{1.25}{3} \\left| \\frac{1.0}{594.5} \\right|\n$$\n$$\n\\text{GCI}_{\\text{fine}} = \\frac{1.25}{3 \\times 594.5} = \\frac{1.25}{1783.5} \\approx 0.000700869...\n$$\nThe problem asks for the GCI to be expressed as a decimal and rounded to three significant figures.\n$$\n\\text{GCI}_{\\text{fine}} \\approx 0.000701\n$$\n\nIn summary, the final rounded results are:\n- Observed order, $p = 2.00$\n- Extrapolated outlet temperature, $\\phi_0 = 594.2$ K\n- Fine-grid GCI = $0.000701$\n\nThese values will be presented in a single row vector as requested.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n2.00 & 594.2 & 0.000701\n\\end{pmatrix}\n}\n$$"
        }
    ]
}