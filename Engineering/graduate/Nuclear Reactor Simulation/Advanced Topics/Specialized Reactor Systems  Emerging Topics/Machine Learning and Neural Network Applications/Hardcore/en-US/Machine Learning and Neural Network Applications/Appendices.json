{
    "hands_on_practices": [
        {
            "introduction": "Physics-Informed Neural Networks (PINNs) represent a paradigm shift, enabling the solution of differential equations by embedding them directly into the network's loss function. This practice guides you through the foundational step of constructing a composite loss function for the time-dependent point reactor kinetics equations. Mastering this allows you to build models that learn the dynamics of a reactor system from its governing physical laws, bridging the gap between deep learning and core reactor physics principles. ",
            "id": "4234282",
            "problem": "Consider the time-dependent point reactor kinetics model with $m$ delayed neutron precursor groups. The neutron population $n(t)$ and the delayed neutron precursor concentrations $C_i(t)$, for $i \\in \\{1,2,\\dots,m\\}$, evolve according to the widely accepted point kinetics ordinary differential equations derived from neutron balance and precursor decay-production processes:\n$$\n\\dot{n}(t) = \\frac{\\rho(t) - \\beta}{\\Lambda}\\, n(t) + \\sum_{i=1}^{m} \\lambda_i\\, C_i(t),\n$$\n$$\n\\dot{C}_i(t) = \\frac{\\beta_i}{\\Lambda}\\, n(t) - \\lambda_i\\, C_i(t),\n$$\nwhere $\\rho(t)$ is the reactivity, $\\beta = \\sum_{i=1}^{m} \\beta_i$ is the total delayed neutron fraction, $\\Lambda$ is the prompt neutron generation time, and $\\lambda_i$ are the precursor decay constants. Assume $\\rho(t)$ is a known differentiable function on the interval $[0,T]$, and all parameters $\\beta_i$, $\\lambda_i$, $\\Lambda$ are known positive constants. Initial conditions $n(0)=n_0$ and $C_i(0)=C_{i0}$ are given.\n\nA Physics-Informed Neural Network (PINN) uses Automatic Differentiation (AD) to approximate the solution by parameterizing $n(t)$ and $C_i(t)$ as $n_{\\theta}(t)$ and $C_{i,\\theta}(t)$, where $\\theta$ are the trainable network parameters. Let $\\{t_j\\}_{j=1}^{N}$ be interior collocation points in $(0,T)$ to enforce the physics, and optionally let $\\{t_k\\}_{k=1}^{K}$ be measurement times at which $n(t)$ has observed values $n^{\\mathrm{obs}}(t_k)$.\n\nUsing only the principles of neutron balance encoded in the point kinetics equations and the definition of a Physics-Informed Neural Network residual as the mismatch between the time derivative (computed by Automatic Differentiation) and the model right-hand side, derive the explicit analytic expression for a weighted mean-squared composite loss function $L(\\theta)$ that penalizes:\n- the neutron kinetics residual over $\\{t_j\\}$,\n- each precursor residual over $\\{t_j\\}$ and all $i \\in \\{1,\\dots,m\\}$,\n- the initial condition mismatches at $t=0$,\n- and the data mismatch at the measurement times $\\{t_k\\}$.\n\nYour loss must be a single closed-form expression in terms of sums over the collocation and measurement sets, with nonnegative weights $w_n$, $w_{C_i}$, $w_{\\mathrm{ic},n}$, $w_{\\mathrm{ic},C_i}$, and $w_d$ explicitly included, and it must use the residuals defined from the given differential equations. Express the final loss $L(\\theta)$ symbolically, without substituting numerical values. The final answer must be a single closed-form analytic expression. Do not include any units in your final answer.",
            "solution": "The problem statement is a valid scientific problem. It is grounded in the standard model of point reactor kinetics and the well-established methodology of Physics-Informed Neural Networks (PINNs). The problem is self-contained, well-posed, objective, and asks for a formal mathematical derivation based on the provided principles. All necessary information is given, and there are no internal contradictions, scientific inaccuracies, or ambiguities.\n\nWe are tasked with deriving a composite loss function, $L(\\theta)$, for a PINN designed to solve the point reactor kinetics equations. The loss function is a weighted sum of mean-squared errors that penalize deviations from physical laws, initial conditions, and observed data. The neural network approximations for the neutron population and precursor concentrations are denoted by $n_{\\theta}(t)$ and $C_{i,\\theta}(t)$, respectively, where $\\theta$ represents the trainable network parameters. The time derivatives of these network outputs, $\\frac{d n_{\\theta}}{dt}$ and $\\frac{d C_{i,\\theta}}{dt}$, are computed using Automatic Differentiation (AD).\n\nThe derivation proceeds by constructing each component of the loss function separately and then combining them into a single expression.\n\nFirst, we define the residuals for the governing ordinary differential equations (ODEs). The residual represents the extent to which the neural network approximation fails to satisfy the differential equation.\n\nThe residual for the neutron population equation, $r_n(t; \\theta)$, is defined by substituting the network approximations $n_{\\theta}(t)$ and $C_{i,\\theta}(t)$ into the first ODE:\n$$\nr_n(t; \\theta) = \\frac{d n_{\\theta}}{dt}(t) - \\left( \\frac{\\rho(t) - \\beta}{\\Lambda}\\, n_{\\theta}(t) + \\sum_{i=1}^{m} \\lambda_i\\, C_{i,\\theta}(t) \\right)\n$$\n\nSimilarly, for each of the $m$ delayed neutron precursor groups, the residual $r_{C_i}(t; \\theta)$ is defined by substituting the network approximations into the second set of ODEs:\n$$\nr_{C_i}(t; \\theta) = \\frac{d C_{i,\\theta}}{dt}(t) - \\left( \\frac{\\beta_i}{\\Lambda}\\, n_{\\theta}(t) - \\lambda_i\\, C_{i,\\theta}(t) \\right) \\quad \\text{for } i \\in \\{1, 2, \\dots, m\\}\n$$\n\nThe total loss function $L(\\theta)$ is a weighted sum of four distinct components:\n$1$. The physics loss for the neutron kinetics equation.\n$2$. The physics loss for the precursor concentration equations.\n$3$. The loss for the initial conditions.\n$4$. The loss for the data mismatch.\n\nWe construct each component as a mean-squared error (or squared error for single-point conditions).\n\n$1$. **Neutron Kinetics Residual Loss ($L_n$)**: This term penalizes the neutron kinetics residual over the set of $N$ interior collocation points $\\{t_j\\}_{j=1}^{N}$. It is the weighted mean-squared residual:\n$$\nL_n(\\theta) = w_n \\frac{1}{N} \\sum_{j=1}^{N} \\left( r_n(t_j; \\theta) \\right)^2\n$$\nSubstituting the expression for $r_n(t_j; \\theta)$:\n$$\nL_n(\\theta) = w_n \\frac{1}{N} \\sum_{j=1}^{N} \\left( \\frac{d n_{\\theta}}{dt}(t_j) - \\frac{\\rho(t_j) - \\beta}{\\Lambda}\\, n_{\\theta}(t_j) - \\sum_{i=1}^{m} \\lambda_i C_{i,\\theta}(t_j) \\right)^2\n$$\n\n$2$. **Precursor Residuals Loss ($L_C$)**: This term penalizes the residuals for all $m$ precursor equations over the same collocation points. The problem specifies weights $w_{C_i}$ for each precursor group $i$. The most direct interpretation is to compute a weighted mean-squared error for each group and sum these contributions.\n$$\nL_C(\\theta) = \\sum_{i=1}^{m} \\left( w_{C_i} \\frac{1}{N} \\sum_{j=1}^{N} \\left( r_{C_i}(t_j; \\theta) \\right)^2 \\right)\n$$\nSubstituting the expression for $r_{C_i}(t_j; \\theta)$:\n$$\nL_C(\\theta) = \\sum_{i=1}^{m} w_{C_i} \\frac{1}{N} \\sum_{j=1}^{N} \\left( \\frac{d C_{i,\\theta}}{dt}(t_j) - \\frac{\\beta_i}{\\Lambda}\\, n_{\\theta}(t_j) + \\lambda_i C_{i,\\theta}(t_j) \\right)^2\n$$\n\n$3$. **Initial Condition Loss ($L_{\\mathrm{ic}}$)**: This term penalizes the mismatch between the network's prediction at $t=0$ and the given initial conditions $n(0)=n_0$ and $C_i(0)=C_{i0}$. This is a sum of weighted squared errors (not a mean, as it's evaluated at a single point).\n$$\nL_{\\mathrm{ic}}(\\theta) = w_{\\mathrm{ic},n} \\left( n_{\\theta}(0) - n_0 \\right)^2 + \\sum_{i=1}^{m} w_{\\mathrm{ic},C_i} \\left( C_{i,\\theta}(0) - C_{i0} \\right)^2\n$$\n\n$4$. **Data Mismatch Loss ($L_d$)**: This term penalizes the deviation of the network's prediction for the neutron population, $n_{\\theta}(t_k)$, from the observed values, $n^{\\mathrm{obs}}(t_k)$, at the $K$ measurement times $\\{t_k\\}_{k=1}^{K}$. This is the weighted mean-squared error of the data mismatch.\n$$\nL_d(\\theta) = w_d \\frac{1}{K} \\sum_{k=1}^{K} \\left( n_{\\theta}(t_k) - n^{\\mathrm{obs}}(t_k) \\right)^2\n$$\n\nFinally, the total composite loss function $L(\\theta)$ is the sum of these individual components:\n$$\nL(\\theta) = L_n(\\theta) + L_C(\\theta) + L_{\\mathrm{ic}}(\\theta) + L_d(\\theta)\n$$\nCombining all the terms yields the final explicit analytic expression for the loss function:\n$$\nL(\\theta) = w_n \\frac{1}{N} \\sum_{j=1}^{N} \\left( \\frac{d n_{\\theta}}{dt}(t_j) - \\frac{\\rho(t_j) - \\beta}{\\Lambda} n_{\\theta}(t_j) - \\sum_{i=1}^{m} \\lambda_i C_{i,\\theta}(t_j) \\right)^2 + \\sum_{i=1}^{m} w_{C_i} \\frac{1}{N} \\sum_{j=1}^{N} \\left( \\frac{d C_{i,\\theta}}{dt}(t_j) - \\frac{\\beta_i}{\\Lambda} n_{\\theta}(t_j) + \\lambda_i C_{i,\\theta}(t_j) \\right)^2 + w_{\\mathrm{ic},n} (n_{\\theta}(0) - n_0)^2 + \\sum_{i=1}^{m} w_{\\mathrm{ic},C_i} (C_{i,\\theta}(0) - C_{i0})^2 + w_d \\frac{1}{K} \\sum_{k=1}^{K} (n_{\\theta}(t_k) - n^{\\mathrm{obs}}(t_k))^2\n$$\nThis expression represents the complete objective function to be minimized during the training of the Physics-Informed Neural Network to find the optimal parameters $\\theta$.",
            "answer": "$$\n\\boxed{\nL(\\theta) = w_n \\frac{1}{N} \\sum_{j=1}^{N} \\left( \\frac{d n_{\\theta}}{dt}(t_j) - \\frac{\\rho(t_j) - \\beta}{\\Lambda} n_{\\theta}(t_j) - \\sum_{i=1}^{m} \\lambda_i C_{i,\\theta}(t_j) \\right)^2 + \\sum_{i=1}^{m} \\left( w_{C_i} \\frac{1}{N} \\sum_{j=1}^{N} \\left( \\frac{d C_{i,\\theta}}{dt}(t_j) - \\frac{\\beta_i}{\\Lambda} n_{\\theta}(t_j) + \\lambda_i C_{i,\\theta}(t_j) \\right)^2 \\right) + w_{\\mathrm{ic},n} (n_{\\theta}(0) - n_0)^2 + \\sum_{i=1}^{m} w_{\\mathrm{ic},C_i} (C_{i,\\theta}(0) - C_{i0})^2 + w_d \\frac{1}{K} \\sum_{k=1}^{K} (n_{\\theta}(t_k) - n^{\\mathrm{obs}}(t_k))^2\n}\n$$"
        },
        {
            "introduction": "For a machine learning model to be physically meaningful, its predictions must adhere to fundamental constraints, such as the non-negativity of neutron flux and temperature. This exercise explores how to enforce these bounds directly within the network's architecture using a parameterized softplus activation function. You will derive the optimal parameterization to ensure physical validity without causing vanishing gradients, a crucial and practical skill for developing stable and reliable physics-based neural networks. ",
            "id": "4234298",
            "problem": "A reactor-physics surrogate uses a Neural Network (NN) to map spatial-material features to two physically constrained outputs: the steady-state scalar neutron flux, denoted by $\\phi(\\mathbf{r})$, and the coolant temperature, denoted by $T(\\mathbf{r})$. Physical realizability requires $\\phi(\\mathbf{r}) \\ge 0$ and $T(\\mathbf{r}) \\ge 0$. To encode these non-negativity constraints at the network outputs while retaining smoothness needed for gradient-based training on Partial Differential Equation (PDE)-informed losses, the model employs a parameterized softplus activation at the final layer,\n$$\ns_{\\beta}(z) = \\frac{1}{\\beta}\\,\\ln\\!\\bigl(1 + \\exp(\\beta z)\\bigr),\n$$\nwith a positive scale parameter $\\beta > 0$. The outputs are given by $\\phi(\\mathbf{r}) = s_{\\beta}(a_{\\phi}(\\mathbf{r}))$ and $T(\\mathbf{r}) = s_{\\beta}(a_{T}(\\mathbf{r}))$, where $a_{\\phi}$ and $a_{T}$ are the corresponding pre-activations.\n\nAssume that, due to Batch Normalization (BN), the pre-activation variables are approximately Gaussian with zero mean and unit variance at initialization, that is, $a_{\\phi} \\sim \\mathcal{N}(0,1)$ and $a_{T} \\sim \\mathcal{N}(0,1)$. To mitigate vanishing gradients in physics-informed training, you require that with probability $p = 0.95$ over the pre-activation distribution, the output-layer local gradient magnitude with respect to its input satisfies\n$$\n\\delta \\le \\frac{d}{dz} s_{\\beta}(z) \\le 1 - \\delta,\n$$\nwith $\\delta = 0.05$. Starting from fundamental definitions and standard Gaussian properties, derive a closed-form expression for the required $\\beta$ in terms of the standard normal cumulative distribution function and its inverse, and then evaluate $\\beta$ numerically for the given $p$ and $\\delta$ under the stated Gaussian assumption. Round your numerical answer to four significant figures. The final answer must be a single real number with no units.",
            "solution": "The problem requires the derivation of a parameter $\\beta$ for a parameterized softplus activation function, $s_{\\beta}(z)$, used in a neural network surrogate for reactor physics simulation. The derivation is subject to a probabilistic constraint on the gradient of this activation function.\n\nFirst, we identify the given function and its properties. The parameterized softplus activation function is defined as:\n$$s_{\\beta}(z) = \\frac{1}{\\beta}\\,\\ln\\bigl(1 + \\exp(\\beta z)\\bigr)$$\nwhere $\\beta > 0$. The local gradient, or the derivative of $s_{\\beta}(z)$ with respect to its input $z$, is found using the chain rule:\n$$\n\\frac{d}{dz} s_{\\beta}(z) = \\frac{d}{dz} \\left[ \\frac{1}{\\beta}\\,\\ln\\bigl(1 + \\exp(\\beta z)\\bigr) \\right] = \\frac{1}{\\beta} \\cdot \\frac{1}{1 + \\exp(\\beta z)} \\cdot \\frac{d}{dz}\\bigl(1 + \\exp(\\beta z)\\bigr)\n$$\n$$\n\\frac{d}{dz} s_{\\beta}(z) = \\frac{1}{\\beta} \\cdot \\frac{1}{1 + \\exp(\\beta z)} \\cdot \\bigl(\\beta \\exp(\\beta z)\\bigr) = \\frac{\\exp(\\beta z)}{1 + \\exp(\\beta z)}\n$$\nThis expression is the standard logistic sigmoid function, often denoted as $\\sigma(u) = \\frac{1}{1 + \\exp(-u)}$. We can see this by rewriting our expression:\n$$\n\\frac{\\exp(\\beta z)}{1 + \\exp(\\beta z)} = \\frac{1}{\\frac{1 + \\exp(\\beta z)}{\\exp(\\beta z)}} = \\frac{1}{\\exp(-\\beta z) + 1} = \\sigma(\\beta z)\n$$\nThe problem specifies a constraint on this gradient to prevent it from vanishing or saturating. The constraint is:\n$$\n\\delta \\le \\frac{d}{dz} s_{\\beta}(z) \\le 1 - \\delta\n$$\nSubstituting the expression for the derivative, we have:\n$$\n\\delta \\le \\frac{\\exp(\\beta z)}{1 + \\exp(\\beta z)} \\le 1 - \\delta\n$$\nThis compound inequality can be split into two separate inequalities. Let's solve the first one:\n$$\n\\delta \\le \\frac{\\exp(\\beta z)}{1 + \\exp(\\beta z)}\n$$\nSince $1 + \\exp(\\beta z)$ is always positive, we can multiply both sides by it:\n$$\n\\delta(1 + \\exp(\\beta z)) \\le \\exp(\\beta z) \\implies \\delta + \\delta\\exp(\\beta z) \\le \\exp(\\beta z)\n$$\n$$\n\\delta \\le \\exp(\\beta z)(1 - \\delta) \\implies \\exp(\\beta z) \\ge \\frac{\\delta}{1 - \\delta}\n$$\nTaking the natural logarithm of both sides, which is a monotonically increasing function:\n$$\n\\beta z \\ge \\ln\\left(\\frac{\\delta}{1 - \\delta}\\right)\n$$\nSince $\\beta > 0$, we can divide by $\\beta$ without changing the inequality direction:\n$$\nz \\ge \\frac{1}{\\beta} \\ln\\left(\\frac{\\delta}{1 - \\delta}\\right)\n$$\nNow, we solve the second inequality:\n$$\n\\frac{\\exp(\\beta z)}{1 + \\exp(\\beta z)} \\le 1 - \\delta\n$$\n$$\n\\exp(\\beta z) \\le (1 - \\delta)(1 + \\exp(\\beta z)) \\implies \\exp(\\beta z) \\le 1 - \\delta + (1 - \\delta)\\exp(\\beta z)\n$$\n$$\n\\exp(\\beta z) - (1 - \\delta)\\exp(\\beta z) \\le 1 - \\delta \\implies \\exp(\\beta z)(1 - (1 - \\delta)) \\le 1 - \\delta\n$$\n$$\n\\delta \\exp(\\beta z) \\le 1 - \\delta \\implies \\exp(\\beta z) \\le \\frac{1 - \\delta}{\\delta}\n$$\nTaking the natural logarithm of both sides:\n$$\n\\beta z \\le \\ln\\left(\\frac{1 - \\delta}{\\delta}\\right)\n$$\nSince $\\beta > 0$:\n$$\nz \\le \\frac{1}{\\beta} \\ln\\left(\\frac{1 - \\delta}{\\delta}\\right)\n$$\nCombining both results, the pre-activation $z$ must lie in the interval:\n$$\n\\frac{1}{\\beta} \\ln\\left(\\frac{\\delta}{1 - \\delta}\\right) \\le z \\le \\frac{1}{\\beta} \\ln\\left(\\frac{1 - \\delta}{\\delta}\\right)\n$$\nUsing the property $\\ln(1/x) = -\\ln(x)$, we can write $\\ln\\left(\\frac{\\delta}{1 - \\delta}\\right) = -\\ln\\left(\\frac{1 - \\delta}{\\delta}\\right)$. Let us define $z_{max} = \\frac{1}{\\beta} \\ln\\left(\\frac{1 - \\delta}{\\delta}\\right)$. The interval for $z$ becomes $[-z_{max}, z_{max}]$.\n\nThe problem states that the pre-activation $z$ follows a standard normal distribution, $z \\sim \\mathcal{N}(0,1)$, and that the probability of $z$ falling into this interval is $p$.\n$$\nP(-z_{max} \\le z \\le z_{max}) = p\n$$\nLet $\\Phi(x)$ denote the cumulative distribution function (CDF) of the standard normal distribution. The probability can be expressed as:\n$$\nP(-z_{max} \\le z \\le z_{max}) = \\Phi(z_{max}) - \\Phi(-z_{max})\n$$\nUsing the symmetry property of the standard normal distribution, $\\Phi(-x) = 1 - \\Phi(x)$, we have:\n$$\n\\Phi(z_{max}) - (1 - \\Phi(z_{max})) = 2\\Phi(z_{max}) - 1 = p\n$$\nSolving for $\\Phi(z_{max})$:\n$$\n\\Phi(z_{max}) = \\frac{1 + p}{2}\n$$\nTo find $z_{max}$, we apply the inverse of the standard normal CDF, $\\Phi^{-1}$:\n$$\nz_{max} = \\Phi^{-1}\\left(\\frac{1 + p}{2}\\right)\n$$\nWe now have two expressions for $z_{max}$. Equating them allows us to solve for $\\beta$:\n$$\n\\frac{1}{\\beta} \\ln\\left(\\frac{1 - \\delta}{\\delta}\\right) = \\Phi^{-1}\\left(\\frac{1 + p}{2}\\right)\n$$\nSolving for $\\beta$, we obtain the required closed-form expression:\n$$\n\\beta = \\frac{\\ln\\left(\\frac{1 - \\delta}{\\delta}\\right)}{\\Phi^{-1}\\left(\\frac{1 + p}{2}\\right)}\n$$\nNow, we substitute the numerical values provided in the problem: $p = 0.95$ and $\\delta = 0.05$.\nFirst, we compute the argument of the logarithm:\n$$\n\\frac{1 - \\delta}{\\delta} = \\frac{1 - 0.05}{0.05} = \\frac{0.95}{0.05} = 19\n$$\nThe numerator of the expression for $\\beta$ is $\\ln(19)$.\nNext, we compute the argument of the inverse CDF:\n$$\n\\frac{1 + p}{2} = \\frac{1 + 0.95}{2} = \\frac{1.95}{2} = 0.975\n$$\nThe denominator is $\\Phi^{-1}(0.975)$. This is the upper critical value of the standard normal distribution for a confidence level of $95\\%$. Standard statistical tables or calculation give this value as approximately $1.959964$.\nNow we compute $\\beta$:\n$$\n\\beta = \\frac{\\ln(19)}{\\Phi^{-1}(0.975)} \\approx \\frac{2.944438979}{1.959963985} \\approx 1.50229204\n$$\nThe problem requires the answer to be rounded to four significant figures. The first four significant figures are $1$, $5$, $0$, and $2$. The fifth significant digit is $2$, which is less than $5$, so we round down.\nThe final numerical value for $\\beta$ is $1.502$.",
            "answer": "$$\\boxed{1.502}$$"
        },
        {
            "introduction": "A truly effective surrogate model must not only predict values but also capture the underlying physical trends of the system it represents. This problem focuses on encoding such knowledge, specifically the monotonic relationship between $k_{\\mathrm{eff}}$ and control parameters like enrichment and absorber concentration, by creating a custom regularizer. By deriving the analytical gradient of this penalty term, you will gain hands-on experience in implementing \"soft\" physical constraints that guide the model towards more generalizable and trustworthy behavior. ",
            "id": "4234321",
            "problem": "A deep neural network parameterized by weights and biases $\\boldsymbol{\\theta}$, denoted $f_{\\boldsymbol{\\theta}}:\\mathbb{R}^{d}\\to\\mathbb{R}$, is trained as a surrogate for the effective neutron multiplication factor $k_{\\mathrm{eff}}$ in a pressurized water reactor steady-state core model. The input feature vector $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ includes uranium-$^{235}\\mathrm{U}$ enrichment $e$ (mass fraction) and soluble boron absorber concentration $a$ (moles per unit volume), along with other thermohydraulic and geometric features held fixed for this task. From reactor physics, $k_{\\mathrm{eff}}$ increases monotonically with $e$ and decreases monotonically with $a$, because enrichment increases the average number of fissions per neutron generation while absorber content increases neutron capture without fission. The training set consists of $N$ base states $\\{\\boldsymbol{x}_{i}\\}_{i=1}^{N}$, where each $\\boldsymbol{x}_{i}$ contains $(e_{i},a_{i})$ and fixed contextual features. For each base state, define two augmented states by small controlled perturbations $\\Delta e>0$ and $\\Delta a>0$ applied exclusively to the enrichment and absorber components:\n$$\n\\boldsymbol{x}_{i}^{e+} := \\boldsymbol{x}_{i}\\ \\text{with $e$ replaced by $e_{i}+\\Delta e$},\\qquad\n\\boldsymbol{x}_{i}^{a+} := \\boldsymbol{x}_{i}\\ \\text{with $a$ replaced by $a_{i}+\\Delta a$}.\n$$\nYou are tasked to construct a differentiable monotonicity regularizer $R(\\boldsymbol{\\theta})$ that penalizes deviations from the physically expected local monotonic behavior, namely $f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{e+})\\ge f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i})$ and $f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{a+})\\le f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i})$ for each $i$. Use separate nonnegative weights $\\lambda_{e}$ and $\\lambda_{a}$ to control the strength of the enrichment and absorber monotonic components. Formulate $R(\\boldsymbol{\\theta})$ using a squared hinge penalty on the signed finite-difference violations so that the penalty is zero when the inequalities are satisfied and grows quadratically otherwise. Then, starting only from the definition of this regularizer, derive a closed-form analytical expression for the gradient $\\nabla_{\\boldsymbol{\\theta}} R(\\boldsymbol{\\theta})$ in terms of $\\nabla_{\\boldsymbol{\\theta}} f_{\\boldsymbol{\\theta}}(\\boldsymbol{x})$ evaluated at $\\boldsymbol{x}_{i}$, $\\boldsymbol{x}_{i}^{e+}$, and $\\boldsymbol{x}_{i}^{a+}$. Express the final gradient using standard functions and clearly indicate any indicator behavior through a Heaviside step function.\n\nYour final answer must be a single closed-form analytic expression for $\\nabla_{\\boldsymbol{\\theta}} R(\\boldsymbol{\\theta})$. No numerical evaluation is required.",
            "solution": "The user wants me to act as a meticulous STEM professor. I will first validate the problem statement according to the specified criteria.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- A deep neural network is defined as $f_{\\boldsymbol{\\theta}}:\\mathbb{R}^{d}\\to\\mathbb{R}$, parameterized by weights and biases $\\boldsymbol{\\theta}$.\n- The network $f_{\\boldsymbol{\\theta}}$ is a surrogate for the effective neutron multiplication factor $k_{\\mathrm{eff}}$.\n- The input vector $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ includes uranium-$^{235}\\mathrm{U}$ enrichment $e$ and soluble boron absorber concentration $a$.\n- Physical principles dictate that $k_{\\mathrm{eff}}$ increases monotonically with $e$ and decreases monotonically with $a$.\n- The training set is $\\{\\boldsymbol{x}_{i}\\}_{i=1}^{N}$. Each $\\boldsymbol{x}_{i}$ contains components $(e_{i},a_{i})$.\n- Augmented states are defined with small perturbations $\\Delta e>0$ and $\\Delta a>0$:\n  - $\\boldsymbol{x}_{i}^{e+} := \\boldsymbol{x}_{i}$ with $e$ replaced by $e_{i}+\\Delta e$.\n  - $\\boldsymbol{x}_{i}^{a+} := \\boldsymbol{x}_{i}$ with $a$ replaced by $a_{i}+\\Delta a$.\n- The desired monotonic behavior for the surrogate model is:\n  - $f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{e+})\\ge f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i})$\n  - $f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{a+})\\le f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i})$\n- A differentiable monotonicity regularizer $R(\\boldsymbol{\\theta})$ must be constructed.\n- The regularizer uses nonnegative weights $\\lambda_{e}$ and $\\lambda_{a}$.\n- The penalty function is a squared hinge loss on the signed finite-difference violations, which is zero when inequalities are satisfied and grows quadratically otherwise.\n- The final task is to derive a closed-form analytical expression for the gradient $\\nabla_{\\boldsymbol{\\theta}} R(\\boldsymbol{\\theta})$ in terms of the network's gradient $\\nabla_{\\boldsymbol{\\theta}} f_{\\boldsymbol{\\theta}}(\\boldsymbol{x})$ and to use a Heaviside step function to show indicator behavior.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem is sound. It describes a common and valid technique in physics-informed machine learning, where physical constraints are encoded into the loss function as regularizers. The underlying reactor physics principles (monotonicity of $k_{\\mathrm{eff}}$ with enrichment and absorber concentration) are correct.\n- **Well-Posed:** The problem is well-posed. The task is to construct a specific mathematical object ($R(\\boldsymbol{\\theta})$) and derive its gradient based on clear and unambiguous rules (squared hinge penalty, specific violation conditions). This ensures a unique and meaningful solution exists.\n- **Objective:** The problem is stated in precise, objective, and formal mathematical language, free from any subjective or opinion-based content.\n\nThe problem does not violate any of the invalidity criteria. It is scientifically sound, well-posed, complete, realistic, and represents a non-trivial mathematical derivation relevant to the specified field.\n\n**Step 3: Verdict and Action**\nThe problem is **valid**. The solution process will now proceed.\n\n### Solution Derivation\n\nThe goal is to construct a monotonicity regularizer $R(\\boldsymbol{\\theta})$ and derive its gradient $\\nabla_{\\boldsymbol{\\theta}}R(\\boldsymbol{\\theta})$.\n\n**1. Formulation of the Regularizer $R(\\boldsymbol{\\theta})$**\n\nThe regularizer penalizes violations of the two monotonicity conditions. We use a squared hinge penalty, which for a condition that a variable $v$ should be non-negative ($v \\ge 0$), is given by $(\\max(0, -v))^2$. This penalty is zero if $v \\ge 0$ and $(v)^2$ if $v < 0$.\n\nFirst, consider the enrichment monotonicity: $f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{e+})\\ge f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i})$. This can be rewritten as $f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{e+}) - f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}) \\ge 0$. A violation occurs when this quantity is negative. The quantity to penalize is $f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}) - f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{e+})$. The penalty term for sample $i$ is therefore $(\\max(0, f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}) - f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{e+})))^2$.\n\nSecond, consider the absorber monotonicity: $f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{a+})\\le f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i})$. This can be rewritten as $f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}) - f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{a+}) \\ge 0$. A violation occurs when this is negative. The quantity to penalize is $f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{a+}) - f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i})$. The penalty term for sample $i$ is $(\\max(0, f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{a+}) - f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i})))^2$.\n\nThe total regularizer $R(\\boldsymbol{\\theta})$ is the weighted sum of these penalties over all $N$ training samples:\n$$\nR(\\boldsymbol{\\theta}) = \\sum_{i=1}^{N} \\left[ \\lambda_{e} \\left( \\max(0, f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}) - f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{e+})) \\right)^2 + \\lambda_{a} \\left( \\max(0, f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{a+}) - f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i})) \\right)^2 \\right]\n$$\nThis function is differentiable almost everywhere, which is sufficient for gradient-based optimization methods.\n\n**2. Derivation of the Gradient $\\nabla_{\\boldsymbol{\\theta}} R(\\boldsymbol{\\theta})$**\n\nTo find the gradient of $R(\\boldsymbol{\\theta})$ with respect to the parameters $\\boldsymbol{\\theta}$, we can differentiate term by term. We need the gradient of a generic squared hinge loss term of the form $L(v) = (\\max(0, v))^2$, where $v$ is a function of $\\boldsymbol{\\theta}$.\n\nUsing the chain rule, $\\nabla_{\\boldsymbol{\\theta}} L(v(\\boldsymbol{\\theta})) = \\frac{dL}{dv} \\nabla_{\\boldsymbol{\\theta}} v(\\boldsymbol{\\theta})$.\nThe derivative of $L(v)$ with respect to $v$ is:\n- If $v > 0$, $L(v) = v^2$, so $\\frac{dL}{dv} = 2v = 2\\max(0, v)$.\n- If $v < 0$, $L(v) = 0$, so $\\frac{dL}{dv} = 0 = 2\\max(0, v)$.\n- At $v=0$, the derivative is $0$.\nSo, we can write $\\frac{d}{dv} (\\max(0, v))^2 = 2\\max(0, v)$.\n\nLet us define the violation terms:\n- For enrichment: $v_{e,i}(\\boldsymbol{\\theta}) = f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}) - f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{e+})$\n- For absorber: $v_{a,i}(\\boldsymbol{\\theta}) = f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{a+}) - f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i})$\n\nThe gradient $\\nabla_{\\boldsymbol{\\theta}} R(\\boldsymbol{\\theta})$ is given by:\n$$\n\\nabla_{\\boldsymbol{\\theta}} R(\\boldsymbol{\\theta}) = \\sum_{i=1}^{N} \\left[ \\lambda_{e} \\nabla_{\\boldsymbol{\\theta}} \\left( \\max(0, v_{e,i}) \\right)^2 + \\lambda_{a} \\nabla_{\\boldsymbol{\\theta}} \\left( \\max(0, v_{a,i}) \\right)^2 \\right]\n$$\nApplying the chain rule:\n$$\n\\nabla_{\\boldsymbol{\\theta}} R(\\boldsymbol{\\theta}) = \\sum_{i=1}^{N} \\left[ \\lambda_{e} \\cdot 2 \\max(0, v_{e,i}) \\nabla_{\\boldsymbol{\\theta}} v_{e,i} + \\lambda_{a} \\cdot 2 \\max(0, v_{a,i}) \\nabla_{\\boldsymbol{\\theta}} v_{a,i} \\right]\n$$\nNext, we compute the gradients of the violation terms:\n- $\\nabla_{\\boldsymbol{\\theta}} v_{e,i} = \\nabla_{\\boldsymbol{\\theta}} (f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}) - f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{e+})) = \\nabla_{\\boldsymbol{\\theta}}f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}) - \\nabla_{\\boldsymbol{\\theta}}f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{e+})$\n- $\\nabla_{\\boldsymbol{\\theta}} v_{a,i} = \\nabla_{\\boldsymbol{\\theta}} (f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{a+}) - f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i})) = \\nabla_{\\boldsymbol{\\theta}}f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{a+}) - \\nabla_{\\boldsymbol{\\theta}}f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i})$\n\nSubstituting these expressions back into the gradient of $R(\\boldsymbol{\\theta})$:\n$$\n\\nabla_{\\boldsymbol{\\theta}} R(\\boldsymbol{\\theta}) = 2 \\sum_{i=1}^{N} \\left[ \\lambda_{e} \\max(0, v_{e,i}) (\\nabla_{\\boldsymbol{\\theta}}f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}) - \\nabla_{\\boldsymbol{\\theta}}f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{e+})) + \\lambda_{a} \\max(0, v_{a,i}) (\\nabla_{\\boldsymbol{\\theta}}f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{a+}) - \\nabla_{\\boldsymbol{\\theta}}f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i})) \\right]\n$$\nThe problem requires expressing the indicator behavior using the Heaviside step function $H(z)$, defined as $H(z)=1$ for $z>0$ and $H(z)=0$ for $z \\le 0$. The function $\\max(0, v)$ can be rewritten as $v \\cdot H(v)$.\n\nApplying this transformation:\n- $\\max(0, v_{e,i}) = v_{e,i} H(v_{e,i}) = (f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}) - f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{e+})) H(f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}) - f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{e+}))$\n- $\\max(0, v_{a,i}) = v_{a,i} H(v_{a,i}) = (f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{a+}) - f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i})) H(f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{a+}) - f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}))$\n\nSubstituting these into the gradient expression yields the final closed-form result:\n$$\n\\nabla_{\\boldsymbol{\\theta}} R(\\boldsymbol{\\theta}) = 2 \\sum_{i=1}^{N} \\left[ \\lambda_{e} (f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}) - f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{e+})) H(f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}) - f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{e+})) (\\nabla_{\\boldsymbol{\\theta}} f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}) - \\nabla_{\\boldsymbol{\\theta}} f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{e+})) \\right.\n$$\n$$\n\\left. + \\lambda_{a} (f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{a+}) - f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i})) H(f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{a+}) - f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i})) (\\nabla_{\\boldsymbol{\\theta}} f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{a+}) - \\nabla_{\\boldsymbol{\\theta}} f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i})) \\right]\n$$\nThis expression is the complete analytical gradient of the specified monotonicity regularizer, formulated in terms of the network's output and its gradient with respect to the parameters at the base and perturbed states.",
            "answer": "$$\n\\boxed{\n2 \\sum_{i=1}^{N} \\left[ \\lambda_{e} (f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}) - f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{e+})) H(f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}) - f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{e+})) (\\nabla_{\\boldsymbol{\\theta}} f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}) - \\nabla_{\\boldsymbol{\\theta}} f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{e+})) + \\lambda_{a} (f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{a+}) - f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i})) H(f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{a+}) - f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i})) (\\nabla_{\\boldsymbol{\\theta}} f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{a+}) - \\nabla_{\\boldsymbol{\\theta}} f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i})) \\right]\n}\n$$"
        }
    ]
}