{
    "hands_on_practices": [
        {
            "introduction": "物理信息神经网络（PINNs）已经成为利用机器学习求解微分方程的一种强大方法。本练习将带你深入PINN的核心，通过为一个经典的反应堆点动力学模型构建损失函数来进行实践。你将学习如何将描述中子布居数和缓发中子先驱核浓度随时间演化的常微分方程组，转化为神经网络可以学习和最小化的目标函数。这项实践是掌握如何将物理定律直接编码到机器学习模型训练过程中的关键一步。",
            "id": "4234282",
            "problem": "考虑一个包含 $m$ 个缓发中子先驱核群的含时点堆动力学模型。中子布居 $n(t)$ 和缓发中子先驱核浓度 $C_i(t)$ (其中 $i \\in \\{1,2,\\dots,m\\}$) 的演化遵循从“中子平衡”和“先驱核衰变-产生”过程推导出的、被广泛接受的点堆动力学常微分方程：\n$$\n\\dot{n}(t) = \\frac{\\rho(t) - \\beta}{\\Lambda}\\, n(t) + \\sum_{i=1}^{m} \\lambda_i\\, C_i(t),\n$$\n$$\n\\dot{C}_i(t) = \\frac{\\beta_i}{\\Lambda}\\, n(t) - \\lambda_i\\, C_i(t),\n$$\n其中 $\\rho(t)$ 是反应性，$\\beta = \\sum_{i=1}^{m} \\beta_i$ 是总缓发中子份额，$\\Lambda$ 是瞬发中子代时间，$\\lambda_i$ 是先驱核衰变常数。假设 $\\rho(t)$ 在区间 $[0,T]$ 上是已知的可微函数，且所有参数 $\\beta_i$、$\\lambda_i$、$\\Lambda$ 均为已知的正常数。初始条件 $n(0)=n_0$ 和 $C_i(0)=C_{i0}$ 已给定。\n\n物理信息神经网络 (PINN) 使用自动微分 (AD) 通过将解 $n(t)$ 和 $C_i(t)$ 参数化为 $n_{\\theta}(t)$ 和 $C_{i,\\theta}(t)$ 来进行近似，其中 $\\theta$ 是可训练的网络参数。设 $\\{t_j\\}_{j=1}^{N}$ 为 $(0,T)$ 内的内部配置点，用于施加物理约束；并可选地，设 $\\{t_k\\}_{k=1}^{K}$ 为测量时间点，在这些时间点上 $n(t)$ 具有观测值 $n^{\\mathrm{obs}}(t_k)$。\n\n仅利用点堆动力学方程中包含的中子平衡原理，以及物理信息神经网络残差的定义（即时间导数（通过自动微分计算）与模型右端项之间的不匹配），推导一个加权均方复合损失函数 $L(\\theta)$ 的显式解析表达式，该函数对以下各项进行惩罚：\n- 在 $\\{t_j\\}$ 上的中子动力学残差，\n- 对于所有 $i \\in \\{1,\\dots,m\\}$，在 $\\{t_j\\}$ 上的每个先驱核残差，\n- 在 $t=0$ 处的初始条件不匹配，\n- 以及在测量时间点 $\\{t_k\\}$ 处的数据不匹配。\n\n您给出的损失函数必须是一个单一的闭式表达式，以关于配置点集和测量点集的求和形式表示，并显式地包含非负权重 $w_n$、$w_{C_i}$、$w_{\\mathrm{ic},n}$、$w_{\\mathrm{ic},C_i}$ 和 $w_d$。它必须使用由给定微分方程定义的残差。请以符号形式表示最终损失 $L(\\theta)$，不要代入数值。最终答案必须是单一的闭式解析表达式。最终答案中不要包含任何单位。",
            "solution": "该问题陈述是一个有效的科学问题。它基于标准的点堆动力学模型和物理信息神经网络（PINN）的成熟方法论。该问题是自洽的、适定的、客观的，并要求基于所提供的原理进行形式化的数学推导。所有必要信息均已给出，不存在内部矛盾、科学不准确性或歧义。\n\n我们的任务是为一个旨在求解点堆动力学方程的 PINN 推导一个复合损失函数 $L(\\theta)$。该损失函数是均方误差的加权和，用以惩罚对物理定律、初始条件和观测数据的偏离。中子布居和先驱核浓度的神经网络近似分别表示为 $n_{\\theta}(t)$ 和 $C_{i,\\theta}(t)$，其中 $\\theta$ 代表可训练的网络参数。这些网络输出的时间导数 $\\frac{d n_{\\theta}}{dt}$ 和 $\\frac{d C_{i,\\theta}}{dt}$ 是使用自动微分（AD）计算的。\n\n推导过程是先分别构建损失函数的每个分量，然后将它们组合成一个单一的表达式。\n\n首先，我们为控制性常微分方程（ODEs）定义残差。残差表示神经网络近似未能满足微分方程的程度。\n\n中子布居方程的残差 $r_n(t; \\theta)$，通过将网络近似 $n_{\\theta}(t)$ 和 $C_{i,\\theta}(t)$ 代入第一个 ODE 来定义：\n$$\nr_n(t; \\theta) = \\frac{d n_{\\theta}}{dt}(t) - \\left( \\frac{\\rho(t) - \\beta}{\\Lambda}\\, n_{\\theta}(t) + \\sum_{i=1}^{m} \\lambda_i\\, C_{i,\\theta}(t) \\right)\n$$\n\n类似地，对于 $m$ 个缓发中子先驱核群中的每一个，残差 $r_{C_i}(t; \\theta)$ 通过将网络近似代入第二组 ODEs 来定义：\n$$\nr_{C_i}(t; \\theta) = \\frac{d C_{i,\\theta}}{dt}(t) - \\left( \\frac{\\beta_i}{\\Lambda}\\, n_{\\theta}(t) - \\lambda_i\\, C_{i,\\theta}(t) \\right) \\quad \\text{对于 } i \\in \\{1, 2, \\dots, m\\}\n$$\n\n总损失函数 $L(\\theta)$ 是四个不同分量的加权和：\n1. 中子动力学方程的物理损失。\n2. 先驱核浓度方程的物理损失。\n3. 初始条件的损失。\n4. 数据不匹配的损失。\n\n我们将每个分量构建为均方误差（或针对单点条件的平方误差）。\n\n1. **中子动力学残差损失 ($L_n$)**: 此项惩罚在 $N$ 个内部配置点 $\\{t_j\\}_{j=1}^{N}$ 集合上的中子动力学残差。它是加权的均方残差：\n$$\nL_n(\\theta) = w_n \\frac{1}{N} \\sum_{j=1}^{N} \\left( r_n(t_j; \\theta) \\right)^2\n$$\n代入 $r_n(t_j; \\theta)$ 的表达式：\n$$\nL_n(\\theta) = w_n \\frac{1}{N} \\sum_{j=1}^{N} \\left( \\frac{d n_{\\theta}}{dt}(t_j) - \\frac{\\rho(t_j) - \\beta}{\\Lambda}\\, n_{\\theta}(t_j) - \\sum_{i=1}^{m} \\lambda_i C_{i,\\theta}(t_j) \\right)^2\n$$\n\n2. **先驱核残差损失 ($L_C$)**: 此项惩罚在相同的配置点上所有 $m$ 个先驱核方程的残差。问题为每个先驱核群 $i$ 指定了权重 $w_{C_i}$。最直接的解释是为每个群组计算一个加权均方误差，然后将这些贡献相加。\n$$\nL_C(\\theta) = \\sum_{i=1}^{m} \\left( w_{C_i} \\frac{1}{N} \\sum_{j=1}^{N} \\left( r_{C_i}(t_j; \\theta) \\right)^2 \\right)\n$$\n代入 $r_{C_i}(t_j; \\theta)$ 的表达式：\n$$\nL_C(\\theta) = \\sum_{i=1}^{m} w_{C_i} \\frac{1}{N} \\sum_{j=1}^{N} \\left( \\frac{d C_{i,\\theta}}{dt}(t_j) - \\frac{\\beta_i}{\\Lambda}\\, n_{\\theta}(t_j) + \\lambda_i C_{i,\\theta}(t_j) \\right)^2\n$$\n\n3. **初始条件损失 ($L_{\\mathrm{ic}}$)**: 此项惩罚网络在 $t=0$ 时的预测值与给定初始条件 $n(0)=n_0$ 和 $C_i(0)=C_{i0}$ 之间的不匹配。这是一个加权平方误差的和（不是均值，因为它是在单个点上评估的）。\n$$\nL_{\\mathrm{ic}}(\\theta) = w_{\\mathrm{ic},n} \\left( n_{\\theta}(0) - n_0 \\right)^2 + \\sum_{i=1}^{m} w_{\\mathrm{ic},C_i} \\left( C_{i,\\theta}(0) - C_{i0} \\right)^2\n$$\n\n4. **数据不匹配损失 ($L_d$)**: 此项惩罚在 $K$ 个测量时间点 $\\{t_k\\}_{k=1}^{K}$ 上，网络对中子布居的预测值 $n_{\\theta}(t_k)$ 与观测值 $n^{\\mathrm{obs}}(t_k)$ 之间的偏离。这是数据不匹配的加权均方误差。\n$$\nL_d(\\theta) = w_d \\frac{1}{K} \\sum_{k=1}^{K} \\left( n_{\\theta}(t_k) - n^{\\mathrm{obs}}(t_k) \\right)^2\n$$\n\n最后，总复合损失函数 $L(\\theta)$ 是这些单个分量的和：\n$$\nL(\\theta) = L_n(\\theta) + L_C(\\theta) + L_{\\mathrm{ic}}(\\theta) + L_d(\\theta)\n$$\n将所有项组合起来，得到损失函数的最终显式解析表达式：\n$$\nL(\\theta) = w_n \\frac{1}{N} \\sum_{j=1}^{N} \\left( \\frac{d n_{\\theta}}{dt}(t_j) - \\frac{\\rho(t_j) - \\beta}{\\Lambda} n_{\\theta}(t_j) - \\sum_{i=1}^{m} \\lambda_i C_{i,\\theta}(t_j) \\right)^2 + \\sum_{i=1}^{m} w_{C_i} \\frac{1}{N} \\sum_{j=1}^{N} \\left( \\frac{d C_{i,\\theta}}{dt}(t_j) - \\frac{\\beta_i}{\\Lambda} n_{\\theta}(t_j) + \\lambda_i C_{i,\\theta}(t_j) \\right)^2 + w_{\\mathrm{ic},n} (n_{\\theta}(0) - n_0)^2 + \\sum_{i=1}^{m} w_{\\mathrm{ic},C_i} (C_{i,\\theta}(0) - C_{i0})^2 + w_d \\frac{1}{K} \\sum_{k=1}^{K} (n_{\\theta}(t_k) - n^{\\mathrm{obs}}(t_k))^2\n$$\n该表达式代表了在训练物理信息神经网络以找到最优参数 $\\theta$ 的过程中需要被最小化的完整目标函数。",
            "answer": "$$\n\\boxed{\nL(\\theta) = w_n \\frac{1}{N} \\sum_{j=1}^{N} \\left( \\frac{d n_{\\theta}}{dt}(t_j) - \\frac{\\rho(t_j) - \\beta}{\\Lambda} n_{\\theta}(t_j) - \\sum_{i=1}^{m} \\lambda_i C_{i,\\theta}(t_j) \\right)^2 + \\sum_{i=1}^{m} \\left( w_{C_i} \\frac{1}{N} \\sum_{j=1}^{N} \\left( \\frac{d C_{i,\\theta}}{dt}(t_j) - \\frac{\\beta_i}{\\Lambda} n_{\\theta}(t_j) + \\lambda_i C_{i,\\theta}(t_j) \\right)^2 \\right) + w_{\\mathrm{ic},n} (n_{\\theta}(0) - n_0)^2 + \\sum_{i=1}^{m} w_{\\mathrm{ic},C_i} (C_{i,\\theta}(0) - C_{i0})^2 + w_d \\frac{1}{K} \\sum_{k=1}^{K} (n_{\\theta}(t_k) - n^{\\mathrm{obs}}(t_k))^2\n}\n$$"
        },
        {
            "introduction": "一个真正具有物理意义的代理模型不仅应能准确预测，还应遵守已知的物理原理，例如单调性。本练习将探讨如何通过在损失函数中添加一个自定义的正则化项，来强制神经网络模型学习这种行为。你将为一个预测反应堆有效中子增殖因子 $k_{\\mathrm{eff}}$ 的模型设计一个单调性正则化器，以确保模型的输出能正确反映物理规律，例如 $k_{\\mathrm{eff}}$ 随燃料富集度的增加而增加。这个练习展示了如何将超越核心控制方程的领域知识注入模型，从而显著提升其泛化能力和物理真实性。",
            "id": "4234321",
            "problem": "一个由权重和偏置 $\\boldsymbol{\\theta}$ 参数化的深度神经网络，表示为 $f_{\\boldsymbol{\\theta}}:\\mathbb{R}^{d}\\to\\mathbb{R}$，被训练作为压水反应堆稳态堆芯模型中有效中子倍增因子 $k_{\\mathrm{eff}}$ 的代理模型。输入特征向量 $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ 包括铀-${}^{235}\\text{U}$ 富集度 $e$ (质量分数) 和可溶性硼吸收剂浓度 $a$ (单位体积摩尔数)，以及在此任务中保持不变的其他热工水力和几何特征。根据反应堆物理学，$k_{\\mathrm{eff}}$ 随 $e$ 单调增加，随 $a$ 单调减少，因为富集度增加了每代中子的平均裂变数，而吸收剂含量增加了无裂变的中子俘获。训练集由 $N$ 个基态 $\\{\\boldsymbol{x}_{i}\\}_{i=1}^{N}$ 组成，其中每个 $\\boldsymbol{x}_{i}$ 包含 $(e_{i},a_{i})$ 和固定的上下文特征。对于每个基态，通过仅应用于富集度和吸收剂分量的小的可控扰动 $\\Delta e>0$ 和 $\\Delta a>0$ 来定义两个增广状态：\n$$\n\\boldsymbol{x}_{i}^{e+} := \\boldsymbol{x}_{i}\\ \\text{with $e$ replaced by $e_{i}+\\Delta e$},\\qquad\n\\boldsymbol{x}_{i}^{a+} := \\boldsymbol{x}_{i}\\ \\text{with $a$ replaced by $a_{i}+\\Delta a$}.\n$$\n你的任务是构建一个可微的单调性正则化器 $R(\\boldsymbol{\\theta})$，该正则化器惩罚与物理预期的局部单调行为的偏差，即对于每个 $i$，有 $f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{e+})\\ge f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i})$ 和 $f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{a+})\\le f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i})$。使用独立的非负权重 $\\lambda_{e}$ 和 $\\lambda_{a}$ 来控制富集度和吸收剂单调分量的强度。使用平方合页惩罚（squared hinge penalty）对有符号有限差分违例进行建模，来构建 $R(\\boldsymbol{\\theta})$，使得当不等式满足时惩罚为零，否则呈二次方增长。然后，仅从此正则化器的定义出发，推导其梯度 $\\nabla_{\\boldsymbol{\\theta}} R(\\boldsymbol{\\theta})$ 的闭式解析表达式，该表达式用在 $\\boldsymbol{x}_{i}$、$\\boldsymbol{x}_{i}^{e+}$ 和 $\\boldsymbol{x}_{i}^{a+}$ 处求值的 $\\nabla_{\\boldsymbol{\\theta}} f_{\\boldsymbol{\\theta}}(\\boldsymbol{x})$ 来表示。使用标准函数表示最终梯度，并通过亥维赛德阶跃函数清楚地指示任何指示函数行为。\n\n你的最终答案必须是 $\\nabla_{\\boldsymbol{\\theta}} R(\\boldsymbol{\\theta})$ 的单个闭式解析表达式。不需要进行数值计算。",
            "solution": "### 解题推导\n\n目标是构建一个单调性正则化器 $R(\\boldsymbol{\\theta})$ 并推导其梯度 $\\nabla_{\\boldsymbol{\\theta}}R(\\boldsymbol{\\theta})$。\n\n**1. 正则化器 $R(\\boldsymbol{\\theta})$ 的构建**\n\n正则化器惩罚对两个单调性条件的违反。我们使用平方合页惩罚，对于一个变量 $v$ 应为非负（$v \\ge 0$）的条件，其惩罚由 $(\\max(0, -v))^2$ 给出。如果 $v \\ge 0$，此惩罚为零；如果 $v  0$，则为 $(-v)^2$。\n\n首先，考虑富集度的单调性：$f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{e+})\\ge f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i})$。这可以重写为 $f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{e+}) - f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}) \\ge 0$。当此量为负时发生违例。要惩罚的量是 $f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}) - f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{e+})$。因此，样本 $i$ 的惩罚项是 $(\\max(0, f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}) - f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{e+})))^2$。\n\n其次，考虑吸收剂的单调性：$f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{a+})\\le f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i})$。这可以重写为 $f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}) - f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{a+}) \\ge 0$。当此量为负时发生违例。要惩罚的量是 $f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{a+}) - f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i})$。因此，样本 $i$ 的惩罚项是 $(\\max(0, f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{a+}) - f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i})))^2$。\n\n总正则化器 $R(\\boldsymbol{\\theta})$ 是所有 $N$ 个训练样本上这些惩罚的加权和：\n$$\nR(\\boldsymbol{\\theta}) = \\sum_{i=1}^{N} \\left[ \\lambda_{e} \\left( \\max(0, f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}) - f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{e+})) \\right)^2 + \\lambda_{a} \\left( \\max(0, f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{a+}) - f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i})) \\right)^2 \\right]\n$$\n该函数几乎处处可微，这对于基于梯度的优化方法来说是足够的。\n\n**2. 梯度 $\\nabla_{\\boldsymbol{\\theta}} R(\\boldsymbol{\\theta})$ 的推导**\n\n为了找到 $R(\\boldsymbol{\\theta})$ 关于参数 $\\boldsymbol{\\theta}$ 的梯度，我们可以逐项求导。我们需要形式为 $L(v) = (\\max(0, v))^2$ 的通用平方合页损失项的梯度，其中 $v$ 是 $\\boldsymbol{\\theta}$ 的函数。\n\n使用链式法则，$\\nabla_{\\boldsymbol{\\theta}} L(v(\\boldsymbol{\\theta})) = \\frac{dL}{dv} \\nabla_{\\boldsymbol{\\theta}} v(\\boldsymbol{\\theta})$。\n$L(v)$ 对 $v$ 的导数是：\n- 如果 $v  0$，$L(v) = v^2$，所以 $\\frac{dL}{dv} = 2v = 2\\max(0, v)$。\n- 如果 $v  0$，$L(v) = 0$，所以 $\\frac{dL}{dv} = 0 = 2\\max(0, v)$。\n- 在 $v=0$ 处，导数为 $0$。\n所以，我们可以写出 $\\frac{d}{dv} (\\max(0, v))^2 = 2\\max(0, v)$。\n\n我们定义违例项：\n- 对于富集度：$v_{e,i}(\\boldsymbol{\\theta}) = f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}) - f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{e+})$\n- 对于吸收剂：$v_{a,i}(\\boldsymbol{\\theta}) = f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{a+}) - f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i})$\n\n梯度 $\\nabla_{\\boldsymbol{\\theta}} R(\\boldsymbol{\\theta})$ 由下式给出：\n$$\n\\nabla_{\\boldsymbol{\\theta}} R(\\boldsymbol{\\theta}) = \\sum_{i=1}^{N} \\left[ \\lambda_{e} \\nabla_{\\boldsymbol{\\theta}} \\left( \\max(0, v_{e,i}) \\right)^2 + \\lambda_{a} \\nabla_{\\boldsymbol{\\theta}} \\left( \\max(0, v_{a,i}) \\right)^2 \\right]\n$$\n应用链式法则：\n$$\n\\nabla_{\\boldsymbol{\\theta}} R(\\boldsymbol{\\theta}) = \\sum_{i=1}^{N} \\left[ \\lambda_{e} \\cdot 2 \\max(0, v_{e,i}) \\nabla_{\\boldsymbol{\\theta}} v_{e,i} + \\lambda_{a} \\cdot 2 \\max(0, v_{a,i}) \\nabla_{\\boldsymbol{\\theta}} v_{a,i} \\right]\n$$\n接下来，我们计算违例项的梯度：\n- $\\nabla_{\\boldsymbol{\\theta}} v_{e,i} = \\nabla_{\\boldsymbol{\\theta}} (f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}) - f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{e+})) = \\nabla_{\\boldsymbol{\\theta}}f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}) - \\nabla_{\\boldsymbol{\\theta}}f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{e+})$\n- $\\nabla_{\\boldsymbol{\\theta}} v_{a,i} = \\nabla_{\\boldsymbol{\\theta}} (f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{a+}) - f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i})) = \\nabla_{\\boldsymbol{\\theta}}f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{a+}) - \\nabla_{\\boldsymbol{\\theta}}f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i})$\n\n将这些表达式代回 $R(\\boldsymbol{\\theta})$ 的梯度中：\n$$\n\\nabla_{\\boldsymbol{\\theta}} R(\\boldsymbol{\\theta}) = 2 \\sum_{i=1}^{N} \\left[ \\lambda_{e} \\max(0, v_{e,i}) (\\nabla_{\\boldsymbol{\\theta}}f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}) - \\nabla_{\\boldsymbol{\\theta}}f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{e+})) + \\lambda_{a} \\max(0, v_{a,i}) (\\nabla_{\\boldsymbol{\\theta}}f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{a+}) - \\nabla_{\\boldsymbol{\\theta}}f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i})) \\right]\n$$\n问题要求使用亥维赛德阶跃函数 $H(z)$ 来表示指示函数行为，定义为当 $z0$ 时 $H(z)=1$，当 $z \\le 0$ 时 $H(z)=0$。函数 $\\max(0, v)$ 可以重写为 $v \\cdot H(v)$。\n\n应用此变换：\n- $\\max(0, v_{e,i}) = v_{e,i} H(v_{e,i}) = (f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}) - f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{e+})) H(f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}) - f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{e+}))$\n- $\\max(0, v_{a,i}) = v_{a,i} H(v_{a,i}) = (f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{a+}) - f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i})) H(f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{a+}) - f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}))$\n\n将这些代入梯度表达式，得到最终的闭式结果：\n$$\n\\nabla_{\\boldsymbol{\\theta}} R(\\boldsymbol{\\theta}) = 2 \\sum_{i=1}^{N} \\left[ \\lambda_{e} (f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}) - f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{e+})) H(f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}) - f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{e+})) (\\nabla_{\\boldsymbol{\\theta}} f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}) - \\nabla_{\\boldsymbol{\\theta}} f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{e+})) \\right.\n$$\n$$\n\\left. + \\lambda_{a} (f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{a+}) - f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i})) H(f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{a+}) - f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i})) (\\nabla_{\\boldsymbol{\\theta}} f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{a+}) - \\nabla_{\\boldsymbol{\\theta}} f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i})) \\right]\n$$\n这个表达式是指定单调性正则化器的完整解析梯度，用网络在基态和扰动状态下的输出及其关于参数的梯度来表示。",
            "answer": "$$\n\\boxed{\n2 \\sum_{i=1}^{N} \\left[ \\lambda_{e} (f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}) - f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{e+})) H(f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}) - f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{e+})) (\\nabla_{\\boldsymbol{\\theta}} f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}) - \\nabla_{\\boldsymbol{\\theta}} f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{e+})) + \\lambda_{a} (f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{a+}) - f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i})) H(f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{a+}) - f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i})) (\\nabla_{\\boldsymbol{\\theta}} f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{a+}) - \\nabla_{\\boldsymbol{\\theta}} f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i})) \\right]\n}\n$$"
        },
        {
            "introduction": "开发快速准确的机器学习代理模型的一个主要动机是加速成本高昂的科学计算。本练习将你置于一个实际的工程优化场景中：在多保真度蒙特卡罗模拟中，如何有效地分配计算资源。你将推导出一个最优样本分配策略，该策略利用一个低成本的机器学习模型（作为低保真度模型）来辅助高成本的高保真度模拟，从而在满足给定精度目标的前提下最小化总计算成本。这项实践揭示了机器学习模型在不确定性量化和计算加速方面所扮演的关键角色，是连接理论模型与实际应用的重要桥梁。",
            "id": "4234263",
            "problem": "在压水反应堆堆芯瞬态模拟中，假设目标是估计一个标量响应泛函 $R$ 的期望值，例如时间平均总裂变功率 $R = \\mathbb{E}[g(X)]$，其中 $X$ 编码了不确定性输入（例如，截面、热工水力边界条件），$g(\\cdot)$ 是高保真度连续能量输运模型的输出。为减少计算开销，使用了一种具有 $L+1$ 个层级的多保真度控制变量构造，通过一系列精度递增的模型来展开期望。具体而言，定义一个最低保真度层级 $l=0$，由一个在反应堆物理数据上训练的神经网络代理模型 $f_0$ 给出（例如，一个在棒状功率图谱上训练的图神经网络 GNN）；一个中等保真度层级 $l=1$，由一个确定性粗网格扩散求解器 $f_1$ 给出；以及一个最高保真度层级 $l=2$，由一个蒙特卡洛 (MC) 中子输运求解器 $f_2$ 给出。伸缩恒等式为\n$$\n\\mathbb{E}[f_2(X)] \\;=\\; \\mathbb{E}[f_0(X)] \\;+\\; \\sum_{l=1}^{2} \\mathbb{E}\\big[f_l(X) - f_{l-1}(X)\\big].\n$$\n通过在每个层级上抽取独立的批次并求平均，构建一个无偏多保真度估计量：\n$$\n\\widehat{Y} \\;=\\; \\frac{1}{N_0}\\sum_{i=1}^{N_0} f_0(X^{(0)}_i) \\;+\\; \\sum_{l=1}^{2} \\frac{1}{N_l}\\sum_{i=1}^{N_l} \\big(f_l(X^{(l)}_i) - f_{l-1}(X^{(l)}_i)\\big),\n$$\n其中对于每个层级 $l$，$\\{X^{(l)}_i\\}_{i=1}^{N_l}$ 是与 $X$ 来自相同输入分布的独立同分布抽样，且各层级间的批次是独立的。\n\n假设以下基本性质成立：\n- 每个层级的估计量都是无偏的。\n- 各层级的批次在 $l$ 上是独立的，因此和的方差等于方差的和。\n- 对于每个层级 $l$，单样本层级增量的方差是一个已知常数，$V_l = \\operatorname{Var}\\big(f_l(X) - f_{l-1}(X)\\big)$（对于 $l \\ge 1$），且 $V_0 = \\operatorname{Var}\\big(f_0(X)\\big)$。因此，层级 $l$ 的方差贡献为 $V_l/N_l$。\n- 期望计算成本与样本数量成线性关系：层级 $l$ 的每个样本成本是一个已知常数 $c_l  0$，因此总期望成本为 $\\sum_{l=0}^{2} c_l N_l$。\n\n您的任务是设计最优的实数值分配 $(N_0,N_1,N_2)$，以最小化期望成本，同时满足固定的估计量方差目标 $\\mathbb{V}\\mathrm{ar}[\\widehat{Y}] \\le \\tau^2$。\n\n1. 仅从层级批次的独立性、独立和的方差可加性、独立同分布样本的均值方差恒等式 $\\operatorname{Var}\\big(\\frac{1}{N}\\sum_{i=1}^N Z_i\\big) = \\operatorname{Var}(Z_1)/N$ 以及期望成本的线性性出发，推导出在约束条件 $\\sum_{l=0}^{2} V_l/N_l \\le \\tau^2$ 和 $N_l  0$ 下最小化 $\\sum_{l=0}^{2} c_l N_l$ 的闭式最优解 $(N_0^{\\star},N_1^{\\star},N_2^{\\star})$。\n\n2. 然后，对于从相同瞬态情景的先前运行中获得的以下科学上合理的初步研究摘要，\n$$\nV_0 \\,=\\, 9 \\times 10^{-6},\\quad V_1 \\,=\\, 16 \\times 10^{-6},\\quad V_2 \\,=\\, 25 \\times 10^{-6},\n$$\n和单位样本成本\n$$\nc_0 \\,=\\, 1,\\quad c_1 \\,=\\, 4,\\quad c_2 \\,=\\, 9,\n$$\n以及方差目标\n$$\n\\tau^2 \\,=\\, 1 \\times 10^{-6},\n$$\n计算您推导出的表达式，以获得最优实数值分配。将您的最终答案表示为行向量 $(N_0^{\\star},N_1^{\\star},N_2^{\\star})$。不要四舍五入；报告精确值。由于样本数是无单位的，答案不需要物理单位。",
            "solution": "该问题具有科学依据、是良定的、客观的，并包含推导唯一且有意义解所需的所有信息。这是多保真度不确定性量化领域一个标准的约束优化问题。\n\n任务是找到最优的样本分配 $(N_0, N_1, N_2)$，以在多保真度估计量总方差的约束下，最小化总计算成本。\n\n**第1部分：最优分配的推导**\n\n总期望计算成本由以下线性函数给出：\n$$\nC(N_0, N_1, N_2) = \\sum_{l=0}^{2} c_l N_l = c_0 N_0 + c_1 N_1 + c_2 N_2\n$$\n由于各层级间的样本是独立的，多保真度估计量 $\\widehat{Y}$ 的方差由各层级方差之和给出：\n$$\n\\mathbb{V}\\mathrm{ar}[\\widehat{Y}] = \\mathbb{V}\\mathrm{ar}\\left[\\frac{1}{N_0}\\sum_{i=1}^{N_0} f_0(X^{(0)}_i)\\right] + \\sum_{l=1}^{2} \\mathbb{V}\\mathrm{ar}\\left[\\frac{1}{N_l}\\sum_{i=1}^{N_l} \\big(f_l(X^{(l)}_i) - f_{l-1}(X^{(l)}_i)\\big)\\right]\n$$\n使用对于独立同分布样本的恒等式 $\\operatorname{Var}\\big(\\frac{1}{N}\\sum_{i=1}^N Z_i\\big) = \\operatorname{Var}(Z_1)/N$ 以及给定的定义 $V_0 = \\operatorname{Var}\\big(f_0(X)\\big)$ 和 $V_l = \\operatorname{Var}\\big(f_l(X) - f_{l-1}(X)\\big)$（对于 $l \\ge 1$），总方差简化为：\n$$\n\\mathbb{V}\\mathrm{ar}[\\widehat{Y}] = \\frac{V_0}{N_0} + \\frac{V_1}{N_1} + \\frac{V_2}{N_2}\n$$\n优化问题是在约束 $\\mathbb{V}\\mathrm{ar}[\\widehat{Y}] \\le \\tau^2$ 下最小化成本 $C(N_0, N_1, N_2)$。由于成本函数 $C$ 对每个 $N_l$ 都是严格递增的，而方差对每个 $N_l$ 都是严格递减的，因此对于给定的方差目标，最小成本将在约束取等号时出现，即当方差恰好等于目标 $\\tau^2$ 时。\n\n因此，问题被表述为：\n最小化 $C(N_0, N_1, N_2) = \\sum_{l=0}^{2} c_l N_l$\n约束条件为 $g(N_0, N_1, N_2) = \\sum_{l=0}^{2} \\frac{V_l}{N_l} - \\tau^2 = 0$，且 $N_l  0$。\n\n我们使用拉格朗日乘子法来解决这个问题。拉格朗日函数 $\\mathcal{L}$ 是：\n$$\n\\mathcal{L}(N_0, N_1, N_2, \\lambda) = \\sum_{l=0}^{2} c_l N_l + \\lambda \\left( \\sum_{l=0}^{2} \\frac{V_l}{N_l} - \\tau^2 \\right)\n$$\n为了找到驻点，我们将关于每个 $N_k$（对于 $k \\in \\{0, 1, 2\\}$）的偏导数设为零：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial N_k} = c_k - \\lambda \\frac{V_k}{N_k^2} = 0\n$$\n由于 $c_k  0$ 和 $V_k \\ge 0$，我们必须有 $\\lambda  0$。对 $N_k$ 进行整理，我们得到：\n$$\nN_k^2 = \\lambda \\frac{V_k}{c_k} \\implies N_k = \\sqrt{\\lambda} \\sqrt{\\frac{V_k}{c_k}}\n$$\n其中我们取正平方根，因为样本数 $N_k$ 必须为正。\n\n现在，我们将 $N_k$ 的这个表达式代入约束方程：\n$$\n\\sum_{l=0}^{2} \\frac{V_l}{N_l} = \\sum_{l=0}^{2} \\frac{V_l}{\\sqrt{\\lambda} \\sqrt{V_l/c_l}} = \\tau^2\n$$\n$$\n\\frac{1}{\\sqrt{\\lambda}} \\sum_{l=0}^{2} \\frac{V_l \\sqrt{c_l}}{\\sqrt{V_l}} = \\frac{1}{\\sqrt{\\lambda}} \\sum_{l=0}^{2} \\sqrt{V_l c_l} = \\tau^2\n$$\n求解乘子项 $\\sqrt{\\lambda}$：\n$$\n\\sqrt{\\lambda} = \\frac{1}{\\tau^2} \\sum_{l=0}^{2} \\sqrt{V_l c_l}\n$$\n最后，我们将其代回 $N_k$ 的表达式中，以找到最优分配 $N_k^{\\star}$：\n$$\nN_k^{\\star} = \\left( \\frac{1}{\\tau^2} \\sum_{l=0}^{2} \\sqrt{V_l c_l} \\right) \\sqrt{\\frac{V_k}{c_k}}\n$$\n这就是每个层级 $k \\in \\{0, 1, 2\\}$ 的最优实数值样本分配的闭式表达式。\n\n**第2部分：数值计算**\n\n给定以下数值：\n方差：$V_0 = 9 \\times 10^{-6}$，$V_1 = 16 \\times 10^{-6}$，$V_2 = 25 \\times 10^{-6}$。\n成本：$c_0 = 1$，$c_1 = 4$，$c_2 = 9$。\n方差目标：$\\tau^2 = 1 \\times 10^{-6}$。\n\n首先，我们为每个层级 $l$ 计算项 $\\sqrt{V_l c_l}$：\n$$\n\\sqrt{V_0 c_0} = \\sqrt{(9 \\times 10^{-6}) \\times 1} = 3 \\times 10^{-3}\n$$\n$$\n\\sqrt{V_1 c_1} = \\sqrt{(16 \\times 10^{-6}) \\times 4} = \\sqrt{64 \\times 10^{-6}} = 8 \\times 10^{-3}\n$$\n$$\n\\sqrt{V_2 c_2} = \\sqrt{(25 \\times 10^{-6}) \\times 9} = \\sqrt{225 \\times 10^{-6}} = 15 \\times 10^{-3}\n$$\n接下来，我们计算和 $\\sum_{l=0}^{2} \\sqrt{V_l c_l}$：\n$$\n\\sum_{l=0}^{2} \\sqrt{V_l c_l} = (3 + 8 + 15) \\times 10^{-3} = 26 \\times 10^{-3}\n$$\n这使我们能够计算乘子项 $\\sqrt{\\lambda}$：\n$$\n\\sqrt{\\lambda} = \\frac{1}{\\tau^2} \\sum_{l=0}^{2} \\sqrt{V_l c_l} = \\frac{26 \\times 10^{-3}}{1 \\times 10^{-6}} = 26 \\times 10^3\n$$\n现在，我们为每个层级 $k$ 计算项 $\\sqrt{V_k/c_k}$：\n$$\n\\sqrt{\\frac{V_0}{c_0}} = \\sqrt{\\frac{9 \\times 10^{-6}}{1}} = 3 \\times 10^{-3}\n$$\n$$\n\\sqrt{\\frac{V_1}{c_1}} = \\sqrt{\\frac{16 \\times 10^{-6}}{4}} = \\sqrt{4 \\times 10^{-6}} = 2 \\times 10^{-3}\n$$\n$$\n\\sqrt{\\frac{V_2}{c_2}} = \\sqrt{\\frac{25 \\times 10^{-6}}{9}} = \\frac{5}{3} \\times 10^{-3}\n$$\n最后，我们使用推导出的公式 $N_k^{\\star} = \\sqrt{\\lambda} \\sqrt{V_k/c_k}$ 计算最优分配 $N_k^{\\star}$：\n$$\nN_0^{\\star} = (26 \\times 10^3) \\times (3 \\times 10^{-3}) = 78\n$$\n$$\nN_1^{\\star} = (26 \\times 10^3) \\times (2 \\times 10^{-3}) = 52\n$$\n$$\nN_2^{\\star} = (26 \\times 10^3) \\times \\left(\\frac{5}{3} \\times 10^{-3}\\right) = \\frac{26 \\times 5}{3} = \\frac{130}{3}\n$$\n最优实数值样本分配为 $(N_0^{\\star}, N_1^{\\star}, N_2^{\\star}) = (78, 52, 130/3)$。",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 78  52  \\frac{130}{3} \\end{pmatrix}}\n$$"
        }
    ]
}