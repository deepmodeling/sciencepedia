{
    "hands_on_practices": [
        {
            "introduction": "A key aspect of source convergence diagnosis is monitoring the evolution of the fission source's spatial shape. This exercise provides foundational practice by having you calculate higher-order statistical moments—specifically skewness and kurtosis—for a representative, non-symmetric source distribution. Mastering these calculations  is essential for understanding and interpreting diagnostics that track how the source distribution's shape changes from its initial guess to its final, converged form.",
            "id": "4250579",
            "problem": "In neutron transport Monte Carlo (MC) eigenvalue simulations for nuclear reactors, the fission source distribution is iteratively updated and should converge to the fundamental-mode stationary source. One way to diagnose source convergence is to monitor standardized shape descriptors of the spatial source distribution. Consider a one-dimensional slab model with spatial coordinate $x \\in [0,1]$, where the late-iteration fission source shape is modeled as a normalized probability density $S(x)$ on $[0,1]$. Due to material heterogeneity and boundary leakage, assume the source density can be represented by a Beta-family shape:\n$$\nS(x) = C\\, x^{a-1}(1-x)^{b-1}, \\quad x \\in [0,1],\n$$\nwith parameters $a = 4$ and $b = 2$, and normalization constant $C$ determined by $\\int_{0}^{1} S(x)\\,\\mathrm{d}x = 1$. Let $X$ be a random variable with probability density function $S(x)$. Define the mean $\\mu = \\mathbb{E}[X]$, the variance $\\sigma^{2} = \\mathbb{E}[(X-\\mu)^{2}]$, the third central moment $\\mu_{3} = \\mathbb{E}[(X-\\mu)^{3}]$, and the fourth central moment $\\mu_{4} = \\mathbb{E}[(X-\\mu)^{4}]$. The skewness is $\\gamma_{1} = \\mu_{3}/\\sigma^{3}$, and the excess kurtosis is $\\gamma_{2} = \\mu_{4}/\\sigma^{4} - 3$.\n\nA combined normality-based diagnostic used in source convergence assessment is the Jarque–Bera (JB) statistic, defined for a sample size $n$ as\n$$\nJ = \\frac{n}{6}\\,\\gamma_{1}^{2} + \\frac{n}{24}\\,\\gamma_{2}^{2}.\n$$\nAssume the source bank contains $n = 2.0 \\times 10^{5}$ fission sites sampled from $S(x)$. Using only fundamental definitions of moments and normalization, compute the value of $J$ for the given $a$ and $b$. Round your final result to four significant figures. Express your answer as a dimensionless number with no units.",
            "solution": "The problem requires the computation of the Jarque-Bera (JB) statistic for a given probability density modeling a fission source distribution. The problem is well-posed and scientifically grounded in statistical methods used in nuclear engineering simulations.\n\nThe fission source density is given by $S(x) = C\\, x^{a-1}(1-x)^{b-1}$ for $x \\in [0,1]$, with parameters $a=4$ and $b=2$. This is the probability density function (PDF) of a Beta distribution, $X \\sim \\text{Beta}(a,b)$.\n\nFirst, we determine the normalization constant $C$ from the condition $\\int_{0}^{1} S(x)\\,\\mathrm{d}x = 1$. The integral is the Beta function, $B(a,b)$.\n$$\nB(a,b) = \\int_{0}^{1} x^{a-1}(1-x)^{b-1}\\,\\mathrm{d}x\n$$\nFor $a=4$ and $b=2$:\n$$\nB(4,2) = \\int_{0}^{1} x^{4-1}(1-x)^{2-1}\\,\\mathrm{d}x = \\int_{0}^{1} x^3(1-x)\\,\\mathrm{d}x = \\int_{0}^{1} (x^3 - x^4)\\,\\mathrm{d}x\n$$\n$$\nB(4,2) = \\left[ \\frac{x^4}{4} - \\frac{x^5}{5} \\right]_{0}^{1} = \\frac{1}{4} - \\frac{1}{5} = \\frac{5-4}{20} = \\frac{1}{20}\n$$\nThe normalization constant is $C = 1/B(4,2) = 20$. Thus, the PDF is $S(x) = 20x^3(1-x)$ for $x \\in [0,1]$.\n\nNext, we compute the first four raw moments, $\\mathbb{E}[X^k] = \\int_{0}^{1} x^k S(x)\\,\\mathrm{d}x$.\n\nThe mean, $\\mu = \\mathbb{E}[X]$:\n$$\n\\mu = \\mathbb{E}[X] = \\int_{0}^{1} x (20x^3(1-x))\\,\\mathrm{d}x = 20 \\int_{0}^{1} (x^4 - x^5)\\,\\mathrm{d}x = 20 \\left[ \\frac{x^5}{5} - \\frac{x^6}{6} \\right]_{0}^{1} = 20 \\left( \\frac{1}{5} - \\frac{1}{6} \\right) = 20 \\left( \\frac{1}{30} \\right) = \\frac{2}{3}\n$$\nThe second raw moment, $\\mathbb{E}[X^2]$:\n$$\n\\mathbb{E}[X^2] = \\int_{0}^{1} x^2 (20x^3(1-x))\\,\\mathrm{d}x = 20 \\int_{0}^{1} (x^5 - x^6)\\,\\mathrm{d}x = 20 \\left[ \\frac{x^6}{6} - \\frac{x^7}{7} \\right]_{0}^{1} = 20 \\left( \\frac{1}{6} - \\frac{1}{7} \\right) = 20 \\left( \\frac{1}{42} \\right) = \\frac{10}{21}\n$$\nThe third raw moment, $\\mathbb{E}[X^3]$:\n$$\n\\mathbb{E}[X^3] = \\int_{0}^{1} x^3 (20x^3(1-x))\\,\\mathrm{d}x = 20 \\int_{0}^{1} (x^6 - x^7)\\,\\mathrm{d}x = 20 \\left[ \\frac{x^7}{7} - \\frac{x^8}{8} \\right]_{0}^{1} = 20 \\left( \\frac{1}{7} - \\frac{1}{8} \\right) = 20 \\left( \\frac{1}{56} \\right) = \\frac{5}{14}\n$$\nThe fourth raw moment, $\\mathbb{E}[X^4]$:\n$$\n\\mathbb{E}[X^4] = \\int_{0}^{1} x^4 (20x^3(1-x))\\,\\mathrm{d}x = 20 \\int_{0}^{1} (x^7 - x^8)\\,\\mathrm{d}x = 20 \\left[ \\frac{x^8}{8} - \\frac{x^9}{9} \\right]_{0}^{1} = 20 \\left( \\frac{1}{8} - \\frac{1}{9} \\right) = 20 \\left( \\frac{1}{72} \\right) = \\frac{5}{18}\n$$\nNow, we compute the central moments.\nThe variance, $\\sigma^2 = \\mu_2 = \\mathbb{E}[(X-\\mu)^2] = \\mathbb{E}[X^2] - \\mu^2$:\n$$\n\\sigma^2 = \\frac{10}{21} - \\left(\\frac{2}{3}\\right)^2 = \\frac{10}{21} - \\frac{4}{9} = \\frac{30 - 28}{63} = \\frac{2}{63}\n$$\nThe third central moment, $\\mu_3 = \\mathbb{E}[(X-\\mu)^3] = \\mathbb{E}[X^3] - 3\\mu\\mathbb{E}[X^2] + 2\\mu^3$:\n$$\n\\mu_3 = \\frac{5}{14} - 3\\left(\\frac{2}{3}\\right)\\left(\\frac{10}{21}\\right) + 2\\left(\\frac{2}{3}\\right)^3 = \\frac{5}{14} - \\frac{20}{21} + \\frac{16}{27}\n$$\nThe least common denominator of $14$, $21$, and $27$ is $378$.\n$$\n\\mu_3 = \\frac{5 \\cdot 27}{378} - \\frac{20 \\cdot 18}{378} + \\frac{16 \\cdot 14}{378} = \\frac{135 - 360 + 224}{378} = \\frac{-1}{378}\n$$\nThe fourth central moment, $\\mu_4 = \\mathbb{E}[(X-\\mu)^4] = \\mathbb{E}[X^4] - 4\\mu\\mathbb{E}[X^3] + 6\\mu^2\\mathbb{E}[X^2] - 3\\mu^4$:\n$$\n\\mu_4 = \\frac{5}{18} - 4\\left(\\frac{2}{3}\\right)\\left(\\frac{5}{14}\\right) + 6\\left(\\frac{2}{3}\\right)^2\\left(\\frac{10}{21}\\right) - 3\\left(\\frac{2}{3}\\right)^4 = \\frac{5}{18} - \\frac{20}{21} + \\frac{80}{63} - \\frac{16}{27}\n$$\nThe least common denominator of $18$, $21$, $63$, and $27$ is $378$.\n$$\n\\mu_4 = \\frac{5 \\cdot 21}{378} - \\frac{20 \\cdot 18}{378} + \\frac{80 \\cdot 6}{378} - \\frac{16 \\cdot 14}{378} = \\frac{105 - 360 + 480 - 224}{378} = \\frac{1}{378}\n$$\nNext, we calculate the skewness $\\gamma_1$ and excess kurtosis $\\gamma_2$.\nThe skewness is $\\gamma_1 = \\mu_3 / \\sigma^3$.\n$$\n\\gamma_1^2 = \\frac{\\mu_3^2}{\\sigma^6} = \\frac{(-1/378)^2}{(2/63)^3} = \\frac{1/378^2}{8/63^3} = \\frac{63^3}{8 \\cdot 378^2} = \\frac{63^3}{8 \\cdot (6 \\cdot 63)^2} = \\frac{63^3}{8 \\cdot 36 \\cdot 63^2} = \\frac{63}{288} = \\frac{7}{32}\n$$\nThe excess kurtosis is $\\gamma_2 = \\mu_4 / \\sigma^4 - 3$.\n$$\n\\frac{\\mu_4}{\\sigma^4} = \\frac{1/378}{(2/63)^2} = \\frac{1/378}{4/3969} = \\frac{3969}{4 \\cdot 378} = \\frac{3969}{1512}\n$$\nSince $3969 = 63^2$ and $1512 = 24 \\cdot 63$, the ratio is $63/24 = 21/8$.\n$$\n\\gamma_2 = \\frac{21}{8} - 3 = \\frac{21 - 24}{8} = -\\frac{3}{8}\n$$\nSo, $\\gamma_2^2 = (-3/8)^2 = 9/64$.\n\nFinally, we compute the Jarque-Bera statistic $J$ with $n = 2.0 \\times 10^5$.\n$$\nJ = \\frac{n}{6}\\gamma_1^2 + \\frac{n}{24}\\gamma_2^2 = \\frac{2.0 \\times 10^5}{6}\\left(\\frac{7}{32}\\right) + \\frac{2.0 \\times 10^5}{24}\\left(\\frac{9}{64}\\right)\n$$\n$$\nJ = (2.0 \\times 10^5) \\left[ \\frac{7}{192} + \\frac{9}{1536} \\right]\n$$\nThe common denominator is $1536 = 8 \\cdot 192$.\n$$\nJ = (2.0 \\times 10^5) \\left[ \\frac{7 \\cdot 8}{1536} + \\frac{9}{1536} \\right] = (2.0 \\times 10^5) \\left[ \\frac{56 + 9}{1536} \\right] = (2.0 \\times 10^5) \\left( \\frac{65}{1536} \\right)\n$$\n$$\nJ = \\frac{200000 \\times 65}{1536} = \\frac{13000000}{1536} = \\frac{203125}{24} \\approx 8463.541666...\n$$\nThe problem requires rounding the result to four significant figures. The fifth significant figure is $5$, so we round up the fourth digit.\n$$\nJ \\approx 8464\n$$",
            "answer": "$$\\boxed{8464}$$"
        },
        {
            "introduction": "In Monte Carlo simulations, convergence metrics are not single values but rather a time series of estimates, one for each cycle. These sequential estimates are inherently correlated, and failing to account for this correlation leads to incorrect conclusions about statistical uncertainty. This practice  demonstrates the proper method for calculating confidence intervals from such autocorrelated data using the integrated autocorrelation time, a critical skill for rigorous analysis of any Monte Carlo output.",
            "id": "4250569",
            "problem": "In a Monte Carlo $k$-eigenvalue simulation of a pressurized water reactor core, the fission source distribution is updated each cycle via power iteration, producing a correlated sequence of cycle-wise source metrics. One diagnostic used to assess source stabilization is the Shannon entropy of the normalized fission source histogram, defined for each cycle $t$ as $H_{t} = -\\sum_{i=1}^{B} p_{t,i} \\ln p_{t,i}$, where $p_{t,i}$ is the fraction of fission neutrons born in spatial bin $i$ during cycle $t$ and $\\sum_{i=1}^{B} p_{t,i} = 1$. After discarding inactive cycles to remove initial transients, suppose $N = 1000$ active cycles are collected and the stationary entropy process $\\{H_{t}\\}_{t=1}^{N}$ is treated as a realization from a stationary ergodic sequence generated by the source iteration.\n\nFrom these cycles, the following diagnostics are computed: the sample mean $\\bar{H} = 3.182$ (in natural logarithm units), the sample variance $s^{2} = 3.6 \\times 10^{-3}$ of $\\{H_{t}\\}$, and an estimate of the integrated autocorrelation time $\\tau_{\\text{int}} = 8.2$ for $\\{H_{t}\\}$ based on the empirical autocorrelation function. Using a derivation grounded in the definition of $H_{t}$ and appropriate asymptotic theory for correlated stationary sequences, construct an asymptotic two-sided $95\\%$ confidence interval for the stationary mean entropy level and compute the corresponding $95\\%$ half-width. Report only the numerical value of the half-width, rounded to four significant figures, and express the half-width in natural logarithm units.",
            "solution": "The user wants to find the half-width of the $95\\%$ confidence interval for the mean of a correlated time series.\n\n### Step 1: Extract Givens\n-   The number of active cycles (samples) is $N = 1000$.\n-   The process is a stationary ergodic sequence of Shannon entropy values, $\\{H_{t}\\}_{t=1}^{N}$.\n-   The sample mean of the entropy is $\\bar{H} = 3.182$.\n-   The sample variance of the entropy sequence is $s^{2} = 3.6 \\times 10^{-3}$.\n-   The estimated integrated autocorrelation time for the sequence is $\\tau_{\\text{int}} = 8.2$.\n-   The desired confidence level is $95\\%$.\n\n### Step 2: Validate Using Extracted Givens\n-   **Scientifically Grounded:** The problem is set within the standard framework of Monte Carlo nuclear reactor analysis. The use of Shannon entropy as a source convergence diagnostic, power iteration, active cycles, and the statistical treatment of the resulting time series (including autocorrelation) are all established practices in the field. The provided values are physically and numerically plausible. The problem is scientifically sound.\n-   **Well-Posed:** All necessary data ($N$, $s^2$, $\\tau_{\\text{int}}$, and the confidence level) are provided to compute the requested quantity. The problem asks for a unique numerical value, the half-width of the confidence interval, which can be determined using standard statistical theory for time series.\n-   **Objective:** The problem is stated using precise and unambiguous technical language common to statistics and nuclear engineering. There are no subjective or opinion-based elements.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n### Solution Derivation\nThe problem requires the construction of a confidence interval for the true mean, $\\mu_H = E[H_t]$, of a stationary and ergodic, but autocorrelated, stochastic process $\\{H_t\\}$. We are given a finite sample of this process of size $N = 1000$.\n\nAccording to the Central Limit Theorem for stationary sequences with sufficient decay of correlations, the sample mean $\\bar{H} = \\frac{1}{N}\\sum_{t=1}^{N} H_t$ is asymptotically normally distributed. The mean of this distribution is the true mean of the process, $\\mu_H$, and its variance, $\\sigma^2_{\\bar{H}}$, is given by:\n$$ \\sigma^2_{\\bar{H}} = \\text{Var}(\\bar{H}) = \\frac{\\sigma^2}{N} \\left( 1 + 2\\sum_{k=1}^{N-1} \\left(1 - \\frac{k}{N}\\right) \\rho_k \\right) $$\nwhere $\\sigma^2 = \\text{Var}(H_t)$ is the true variance of the process and $\\rho_k$ is the autocorrelation of the process at lag $k$.\n\nFor large $N$, this expression can be approximated using the integrated autocorrelation time, $\\tau_{\\text{int}}$, which is defined as:\n$$ \\tau_{\\text{int}} = 1 + 2\\sum_{k=1}^{\\infty} \\rho_k $$\nWith this definition, the variance of the sample mean for large $N$ becomes:\n$$ \\sigma^2_{\\bar{H}} \\approx \\frac{\\sigma^2 \\tau_{\\text{int}}}{N} $$\nThis expression shows that the correlation increases the variance of the sample mean by a factor of $\\tau_{\\text{int}}$ compared to an uncorrelated process. The quantity $N_{\\text{eff}} = N/\\tau_{\\text{int}}$ can be interpreted as the effective number of independent samples.\n\nTo compute the confidence interval, we must first estimate the standard error of the mean, $s_{\\bar{H}}$. This is the square root of the estimated variance of the sample mean, $s^2_{\\bar{H}}$. We estimate $\\sigma^2_{\\bar{H}}$ by using the given sample variance $s^2$ as an estimator for the true process variance $\\sigma^2$.\n$$ s^2_{\\bar{H}} = \\frac{s^2 \\tau_{\\text{int}}}{N} $$\nSubstituting the given values:\n$N = 1000$\n$s^2 = 3.6 \\times 10^{-3}$\n$\\tau_{\\text{int}} = 8.2$\n$$ s^2_{\\bar{H}} = \\frac{(3.6 \\times 10^{-3}) \\cdot (8.2)}{1000} = \\frac{0.02952}{1000} = 2.952 \\times 10^{-5} $$\nThe standard error of the mean is the square root of this value:\n$$ s_{\\bar{H}} = \\sqrt{2.952 \\times 10^{-5}} \\approx 0.005433231 $$\n\nA two-sided $95\\%$ confidence interval for the true mean $\\mu_H$ is given by:\n$$ \\bar{H} \\pm z_{\\alpha/2} s_{\\bar{H}} $$\nFor a $95\\%$ confidence level, the significance level is $\\alpha = 0.05$, which means $\\alpha/2 = 0.025$. The critical value $z_{0.025}$ from the standard normal distribution is the value such that the cumulative probability is $1 - 0.025 = 0.975$. This value is:\n$$ z_{0.025} = 1.96 $$\nThe use of the normal distribution is justified because the sample size ($N=1000$) is large.\n\nThe half-width ($HW$) of the confidence interval is the quantity that is added to and subtracted from the sample mean. It is defined as:\n$$ HW = z_{\\alpha/2} s_{\\bar{H}} $$\nSubstituting the values for $z_{0.025}$ and $s_{\\bar{H}}$:\n$$ HW = 1.96 \\times \\sqrt{\\frac{s^2 \\tau_{\\text{int}}}{N}} $$\n$$ HW = 1.96 \\times \\sqrt{2.952 \\times 10^{-5}} $$\n$$ HW \\approx 1.96 \\times 0.005433231 \\approx 0.010649133 $$\nThe problem requires the answer to be rounded to four significant figures.\n$$ HW \\approx 0.01065 $$\nThe half-width is expressed in the same units as the quantity being measured, which are natural logarithm units.",
            "answer": "$$\n\\boxed{0.01065}\n$$"
        },
        {
            "introduction": "While single metrics are useful, a comprehensive diagnostic must monitor the source distribution across many spatial regions simultaneously. This hands-on coding problem  guides you through the implementation of a multivariate control chart, a powerful tool for detecting statistically significant shifts in the high-dimensional source vector. You will learn to build a robust, automated monitoring system that is applicable to large-scale, practical reactor simulations.",
            "id": "4250525",
            "problem": "A Monte Carlo eigenvalue simulation for neutron transport in a nuclear reactor core generates a sequence of batchwise estimates of the fission source distribution across a fixed spatial partition. Let the spatial partition consist of $p$ regions, and let the batchwise vector of normalized fission source tallies be denoted by $x_t \\in \\mathbb{R}^p$ for batch index $t$. Under source convergence, with sufficiently many particle histories per batch, the Central Limit Theorem implies that successive batch means are approximately independent realizations from a multivariate normal distribution with an unknown mean vector $\\mu \\in \\mathbb{R}^p$ and an unknown positive semidefinite covariance matrix $\\Sigma \\in \\mathbb{R}^{p \\times p}$. The purpose of source convergence diagnostics is to detect departures from this stationary distribution, such as mean-shifts or structural changes, by monitoring $x_t$.\n\nDesign a multivariate control chart that uses a quadratic-form decision rule derived from the distributional properties of a centered multivariate normal vector, and express its threshold as a function of a user-specified false-alarm probability expressed as a decimal $\\alpha$. Estimate the unknown parameters from an \"in-control\" training segment. To robustly handle singular or ill-conditioned covariance estimates, include a ridge regularization parameter $\\lambda \\ge 0$ that modifies the covariance estimate by adding $\\lambda$ times the identity matrix. Use the Moore–Penrose pseudoinverse when needed. The chart must process a subsequent detection segment and return the smallest positive batch index in the detection segment at which an out-of-control signal is detected, or return $0$ if no signal is detected.\n\nYour program must implement the following steps for each test case:\n- Given a training matrix $X_0 \\in \\mathbb{R}^{m \\times p}$ whose rows are training vectors and a detection matrix $X_1 \\in \\mathbb{R}^{n \\times p}$ whose rows are detection vectors, compute the sample mean $\\hat{\\mu} \\in \\mathbb{R}^p$ of the rows of $X_0$ and the unbiased sample covariance $\\hat{\\Sigma} \\in \\mathbb{R}^{p \\times p}$ of the rows of $X_0$.\n- Form the regularized covariance $\\hat{\\Sigma}_\\lambda = \\hat{\\Sigma} + \\lambda I_p$, where $I_p$ is the $p \\times p$ identity matrix.\n- For each detection vector $x \\in \\mathbb{R}^p$ (processed in order of appearance in $X_1$), compute the quadratic form statistic using $\\hat{\\mu}$ and the pseudoinverse of $\\hat{\\Sigma}_\\lambda$. Derive the threshold corresponding to the specified false-alarm probability $\\alpha$ based on the distribution of this statistic under stationarity. Signal an out-of-control condition on the first detection vector whose statistic exceeds the threshold. If none exceed the threshold, return $0$.\n- The final output for the program must be a single line containing the results for all test cases aggregated into a comma-separated list enclosed in square brackets, for example $[r_1,r_2,r_3]$, where each $r_i$ is an integer indicating the earliest detection index or $0$.\n\nAll quantities are dimensionless. Angles do not appear in this problem. The false-alarm probability must be handled as a decimal $\\alpha$, not as a percentage symbol.\n\nUse the following test suite. Each test case specifies $(X_0, X_1, \\alpha, \\lambda)$ explicitly.\n\nTest case $1$ (in-control, no signal expected):\n- $p = 3$, $m = 5$, $n = 2$,\n- $X_0$ rows: $\\left[1,2,3\\right], \\left[2,3,4\\right], \\left[1,3,5\\right], \\left[2,2,2\\right], \\left[0,1,2\\right]$,\n- $X_1$ rows: equal to the sample mean of $X_0$ repeated twice,\n- $\\alpha = 0.01$, $\\lambda = 0$.\n\nTest case $2$ (mean shift, clear signal expected):\n- $p = 3$, $m = 6$, $n = 3$,\n- $X_0$ rows: $\\left[1,0,0\\right], \\left[-1,0,0\\right], \\left[0,1,0\\right], \\left[0,-1,0\\right], \\left[0,0,1\\right], \\left[0,0,-1\\right]$,\n- $X_1$ rows: $\\left[2,2,2\\right], \\left[2,2,2\\right], \\left[2,2,2\\right]$,\n- $\\alpha = 0.01$, $\\lambda = 0$.\n\nTest case $3$ (singular covariance, regularization prevents false signal):\n- $p = 2$, $m = 3$, $n = 1$,\n- $X_0$ rows: $\\left[1,1\\right], \\left[1,1\\right], \\left[1,1\\right]$,\n- $X_1$ rows: $\\left[1.2,1.2\\right]$,\n- $\\alpha = 0.01$, $\\lambda = 0.1$.\n\nTest case $4$ (high dimension with rank-deficient covariance; shift in an unobserved direction yields no signal):\n- $p = 5$, $m = 4$, $n = 2$,\n- $X_0$ rows: $\\left[1,0,0,0,0\\right], \\left[-1,0,0,0,0\\right], \\left[0,2,0,0,0\\right], \\left[0,-2,0,0,0\\right]$,\n- $X_1$ rows: $\\left[0,0,1,0,0\\right], \\left[0,0,1,0,0\\right]$,\n- $\\alpha = 0.01$, $\\lambda = 0$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test cases listed above, for example $[r_1,r_2,r_3,r_4]$ where each $r_i$ is an integer.",
            "solution": "The problem requires the design and implementation of a multivariate statistical process control chart for monitoring source convergence in Monte Carlo nuclear reactor simulations. The chart is based on a quadratic form statistic and must be capable of handling potentially singular covariance matrices through regularization and the use of the Moore-Penrose pseudoinverse.\n\nThe fundamental principle of this control chart is to test whether new observations (batchwise fission source vectors) are statistically consistent with a baseline \"in-control\" process characterized by a multivariate normal distribution.\n\nLet the sequence of $p$-dimensional batchwise fission source vectors be denoted by $\\{x_t\\}_{t \\ge 1}$. Under stationary, \"in-control\" conditions, the problem states that these vectors are approximately independent and identically distributed realizations from a multivariate normal distribution, $x_t \\sim \\mathcal{N}(\\mu, \\Sigma)$, where the mean vector $\\mu \\in \\mathbb{R}^p$ and the covariance matrix $\\Sigma \\in \\mathbb{R}^{p \\times p}$ are unknown.\n\nThe parameters of this in-control distribution are estimated from a training data set, given as a matrix $X_0 \\in \\mathbb{R}^{m \\times p}$ containing $m$ observations. The standard unbiased estimators for the mean and covariance are:\n-   Sample Mean: $\\hat{\\mu} = \\frac{1}{m} \\sum_{i=1}^{m} x_{0,i}$\n-   Unbiased Sample Covariance: $\\hat{\\Sigma} = \\frac{1}{m-1} \\sum_{i=1}^{m} (x_{0,i} - \\hat{\\mu})(x_{0,i} - \\hat{\\mu})^T$\nwhere $x_{0,i}$ denotes the $i$-th row of $X_0$.\n\nThe sample covariance matrix $\\hat{\\Sigma}$ can be singular or ill-conditioned, especially if the number of training samples $m$ is not much larger than the dimension $p$. To ensure robustness, a regularized covariance matrix is formed:\n$$\n\\hat{\\Sigma}_\\lambda = \\hat{\\Sigma} + \\lambda I_p\n$$\nwhere $\\lambda \\ge 0$ is a user-specified regularization parameter and $I_p$ is the $p \\times p$ identity matrix. This technique, known as ridge regularization or Tikhonov regularization, adds a small positive value to the diagonal elements of $\\hat{\\Sigma}$, making the resulting matrix better conditioned and ensuring it is positive definite for $\\lambda  0$.\n\nFor each new observation vector $x$ from the detection set $X_1 \\in \\mathbb{R}^{n \\times p}$, we test the null hypothesis $H_0$ that $x$ is drawn from the same distribution as the training data. This is done by computing a test statistic that measures the \"distance\" of $x$ from the estimated center of the training data, $\\hat{\\mu}$, adjusted for the data's covariance structure. This is a generalized Mahalanobis distance. The statistic, a quadratic form, is defined as:\n$$\nT^2 = (x - \\hat{\\mu})^T (\\hat{\\Sigma}_\\lambda)^+ (x - \\hat{\\mu})\n$$\nwhere $(\\hat{\\Sigma}_\\lambda)^+$ denotes the Moore-Penrose pseudoinverse of the regularized covariance matrix. The pseudoinverse is used to gracefully handle cases where $\\hat{\\Sigma}_\\lambda$ might still be singular (i.e., if $\\lambda=0$ and $\\hat{\\Sigma}$ is singular).\n\nTo decide if an observed value of $T^2$ is significantly large, we must know its probability distribution under the null hypothesis. For a sufficiently large training set ($m \\to \\infty$), $\\hat{\\mu} \\to \\mu$ and $\\hat{\\Sigma} \\to \\Sigma$. In this asymptotic limit, the statistic $(x-\\mu)^T \\Sigma^{-1} (x-\\mu)$ follows a chi-squared ($\\chi^2$) distribution with $p$ degrees of freedom. When using estimated parameters from a finite sample, and particularly when using a pseudoinverse for a potentially rank-deficient covariance matrix, the reference distribution for $T^2$ is more accurately approximated as a $\\chi^2$ distribution whose degrees of freedom equal the rank of the covariance matrix.\nLet $r = \\text{rank}(\\hat{\\Sigma}_\\lambda)$. We approximate the null distribution of our statistic as:\n$$\nT^2 \\sim \\chi^2_r\n$$\nThe decision rule for the control chart is to signal an \"out-of-control\" condition if the computed $T^2$ statistic exceeds a critical threshold $C$. This threshold is determined by the specified false-alarm probability $\\alpha$, which is the probability of raising a signal when the process is actually in control (a Type I error). We set $C$ such that $P(T^2  C | H_0) = \\alpha$. Based on our distributional approximation, the threshold $C$ is the upper $\\alpha$-quantile of the $\\chi^2_r$ distribution:\n$$\nC = F_{\\chi^2_r}^{-1}(1-\\alpha)\n$$\nwhere $F_{\\chi^2_r}^{-1}$ is the inverse cumulative distribution function (percent-point function) of the chi-squared distribution with $r$ degrees of freedom.\n\nThe overall algorithm for each test case is as follows:\n1.  From the training matrix $X_0$, compute the sample mean $\\hat{\\mu}$ and the unbiased sample covariance $\\hat{\\Sigma}$.\n2.  Form the regularized covariance matrix $\\hat{\\Sigma}_\\lambda = \\hat{\\Sigma} + \\lambda I_p$.\n3.  Determine the rank $r$ of $\\hat{\\Sigma}_\\lambda$.\n4.  Calculate the detection threshold $C = F_{\\chi^2_r}^{-1}(1-\\alpha)$.\n5.  Compute the pseudoinverse $(\\hat{\\Sigma}_\\lambda)^+$.\n6.  Iterate through the rows $x_k$ of the detection matrix $X_1$ for $k=1, 2, ..., n$.\n    a. For each $x_k$, compute the deviation vector $d_k = x_k - \\hat{\\mu}$.\n    b. Calculate the statistic $T_k^2 = d_k^T (\\hat{\\Sigma}_\\lambda)^+ d_k$.\n    c. If $T_k^2  C$, an out-of-control condition is detected. The process terminates, and the current batch index $k$ is returned.\n7.  If the loop completes without any statistic exceeding the threshold, the detection segment is deemed in-control, and $0$ is returned.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import chi2\n\ndef solve():\n    \"\"\"\n    Solves the multivariate control chart problem for a given suite of test cases.\n    \"\"\"\n    \n    # Test case 1 (in-control, no signal expected)\n    X0_1 = np.array([\n        [1, 2, 3],\n        [2, 3, 4],\n        [1, 3, 5],\n        [2, 2, 2],\n        [0, 1, 2]\n    ])\n    mu_hat_1 = np.mean(X0_1, axis=0)\n    X1_1 = np.array([mu_hat_1, mu_hat_1])\n    alpha_1 = 0.01\n    lambda_1 = 0.0\n    case_1 = (X0_1, X1_1, alpha_1, lambda_1)\n\n    # Test case 2 (mean shift, clear signal expected)\n    X0_2 = np.array([\n        [1, 0, 0],\n        [-1, 0, 0],\n        [0, 1, 0],\n        [0, -1, 0],\n        [0, 0, 1],\n        [0, 0, -1]\n    ])\n    X1_2 = np.array([\n        [2, 2, 2],\n        [2, 2, 2],\n        [2, 2, 2]\n    ])\n    alpha_2 = 0.01\n    lambda_2 = 0.0\n    case_2 = (X0_2, X1_2, alpha_2, lambda_2)\n\n    # Test case 3 (singular covariance, regularization prevents false signal)\n    X0_3 = np.array([\n        [1, 1],\n        [1, 1],\n        [1, 1]\n    ])\n    X1_3 = np.array([\n        [1.2, 1.2]\n    ])\n    alpha_3 = 0.01\n    lambda_3 = 0.1\n    case_3 = (X0_3, X1_3, alpha_3, lambda_3)\n\n    # Test case 4 (high dimension with rank-deficient covariance; shift in an unobserved direction yields no signal)\n    X0_4 = np.array([\n        [1, 0, 0, 0, 0],\n        [-1, 0, 0, 0, 0],\n        [0, 2, 0, 0, 0],\n        [0, -2, 0, 0, 0]\n    ])\n    X1_4 = np.array([\n        [0, 0, 1, 0, 0],\n        [0, 0, 1, 0, 0]\n    ])\n    alpha_4 = 0.01\n    lambda_4 = 0.0\n    case_4 = (X0_4, X1_4, alpha_4, lambda_4)\n\n    test_cases = [case_1, case_2, case_3, case_4]\n\n    results = []\n    for case in test_cases:\n        X0, X1, alpha, lam = case\n        \n        # Determine dimensions\n        m, p = X0.shape\n        \n        # Step 1: Compute sample mean and unbiased sample covariance\n        mu_hat = np.mean(X0, axis=0)\n        \n        # np.cov with rowvar=False treats columns as variables. ddof=1 for unbiased estimator.\n        # This requires m  1, which is true for all test cases.\n        if m > 1:\n            sigma_hat = np.cov(X0, rowvar=False, ddof=1)\n        else: # Handle case m = 1, covariance is zero\n            sigma_hat = np.zeros((p, p))\n\n        # Step 2: Form regularized covariance\n        sigma_lambda = sigma_hat + lam * np.identity(p)\n        \n        # Step 3: Determine rank for degrees of freedom\n        # Use a small tolerance for numerical stability\n        rank = np.linalg.matrix_rank(sigma_lambda)\n        \n        # Step 4: Calculate threshold\n        # If rank is 0, the chi-squared distribution is a point mass at 0.\n        # ppf(1-alpha, 0) is 0 for alpha  1.\n        # The statistic will also be 0, so no signal is possible.\n        if rank == 0:\n            threshold = 0.0\n        else:\n            threshold = chi2.ppf(1 - alpha, df=rank)\n            \n        # Step 5: Compute pseudoinverse\n        pinv_sigma_lambda = np.linalg.pinv(sigma_lambda)\n\n        detected_index = 0\n        # Step 6: Process detection segment\n        for i, x_detect in enumerate(X1):\n            # a. Deviation vector\n            d = x_detect - mu_hat\n            # b. Quadratic form statistic\n            T_squared = d.T @ pinv_sigma_lambda @ d\n            # c. Check against threshold\n            if T_squared > threshold:\n                detected_index = i + 1\n                break\n        \n        results.append(detected_index)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}