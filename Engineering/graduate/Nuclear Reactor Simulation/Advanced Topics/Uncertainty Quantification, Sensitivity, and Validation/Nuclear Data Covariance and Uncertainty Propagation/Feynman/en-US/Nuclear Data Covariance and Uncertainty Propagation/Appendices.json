{
    "hands_on_practices": [
        {
            "introduction": "The foundation of nuclear data uncertainty propagation is the first-order sensitivity-based method, often called the \"sandwich rule\". This practice provides direct experience applying this core formula to estimate the variance in the effective multiplication factor, $k_{\\text{eff}}$. More importantly, it demonstrates how to decompose this total uncertainty into contributions from different physical processes, allowing you to pinpoint the primary sources of uncertainty in a reactor model.",
            "id": "4238754",
            "problem": "A one-group reactor physics model is used to assess how uncertainties in channel-wise group-averaged macroscopic cross sections propagate to the uncertainty of the effective multiplication factor (k-effective). Let the nuclear data parameter vector be $x = (x_{\\text{f}}, x_{\\text{a}}, x_{\\text{s}})$ for fission, absorption, and scattering channels, respectively. Define the relative sensitivity coefficients $s_i$ by $s_i \\equiv \\partial \\ln k_{\\text{eff}} / \\partial \\ln x_i$ evaluated at nominal data. The nuclear data uncertainties are characterized by the relative covariance matrix $\\Sigma_r = \\mathrm{Cov}(\\delta x_i/x_i, \\delta x_j/x_j)$, which is symmetric and positive semidefinite.\n\nStarting from a first-order linearization of $k_{\\text{eff}}$ in the logarithms of the nuclear data parameters and the definition of covariance, derive a working expression for the relative variance of the effective multiplication factor, $\\mathrm{Var}(k_{\\text{eff}})/k_{\\text{eff}}^2$, in terms of $s$ and $\\Sigma_r$. Then, using the symmetric partitioning convention for decomposing the variance by channels, in which one-half of each off-diagonal covariance contribution is attributed to each of the two participating channels, compute the individual channel contributions to $\\mathrm{Var}(k_{\\text{eff}})/k_{\\text{eff}}^2$.\n\nUse the following numerically specified quantities:\n- Relative sensitivity coefficients: $s_{\\text{f}} = 0.75$, $s_{\\text{a}} = -0.55$, $s_{\\text{s}} = 0.10$.\n- Relative standard deviations: $\\sigma_{\\text{f}} = 0.02$, $\\sigma_{\\text{a}} = 0.03$, $\\sigma_{\\text{s}} = 0.01$.\n- Correlation coefficients: $\\rho_{\\text{fa}} = -0.30$, $\\rho_{\\text{fs}} = 0.20$, $\\rho_{\\text{as}} = 0.10$.\n\nAssemble the relative covariance matrix $\\Sigma_r$ using $\\left(\\Sigma_r\\right)_{ii} = \\sigma_i^2$ and $\\left(\\Sigma_r\\right)_{ij} = \\rho_{ij}\\,\\sigma_i \\sigma_j$ for $i \\neq j$. Then:\n1. Evaluate the total relative variance $\\mathrm{Var}(k_{\\text{eff}})/k_{\\text{eff}}^2$.\n2. Decompose it into channel-wise contributions using the symmetric partitioning of off-diagonal terms.\n3. State which channel is most important and why, based on the numerical magnitudes and signs of $s_i$ and the structure of $\\Sigma_r$.\n\nFinally, report as your answer the fraction of the total relative variance that is attributable to the fission channel, expressed as a decimal. Round your answer to four significant figures.",
            "solution": "The starting point is a first-order perturbation of the effective multiplication factor (k-effective) with respect to the logarithms of the underlying nuclear data parameters. Define the relative sensitivity coefficients by $s_i \\equiv \\partial \\ln k_{\\text{eff}} / \\partial \\ln x_i$ at nominal $x$. For small perturbations $\\delta x_i / x_i$, the first-order linearization of the fractional change in $k_{\\text{eff}}$ is\n$$\n\\frac{\\delta k_{\\text{eff}}}{k_{\\text{eff}}} \\approx \\sum_{i} s_i \\frac{\\delta x_i}{x_i}.\n$$\nTaking the variance of both sides and using the bilinearity of covariance gives\n$$\n\\frac{\\mathrm{Var}(k_{\\text{eff}})}{k_{\\text{eff}}^2} \\approx \\sum_{i}\\sum_{j} s_i s_j \\,\\mathrm{Cov}\\!\\left(\\frac{\\delta x_i}{x_i},\\frac{\\delta x_j}{x_j}\\right) \\equiv s^{\\mathsf{T}} \\Sigma_r\\, s,\n$$\nwhere $\\Sigma_r$ is the relative covariance matrix with components $\\left(\\Sigma_r\\right)_{ij} = \\mathrm{Cov}(\\delta x_i/x_i,\\delta x_j/x_j)$.\n\nWe are given the relative standard deviations and correlation coefficients, so we construct $\\Sigma_r$ via\n$$\n\\left(\\Sigma_r\\right)_{ii} = \\sigma_i^2,\\quad \\left(\\Sigma_r\\right)_{ij} = \\rho_{ij}\\,\\sigma_i \\sigma_j \\quad (i \\neq j).\n$$\nWith $\\sigma_{\\text{f}} = 0.02$, $\\sigma_{\\text{a}} = 0.03$, $\\sigma_{\\text{s}} = 0.01$ and $\\rho_{\\text{fa}}=-0.30$, $\\rho_{\\text{fs}}=0.20$, $\\rho_{\\text{as}}=0.10$, the covariance elements are\n- Diagonal:\n$$\n\\left(\\Sigma_r\\right)_{\\text{ff}} = (0.02)^2 = 0.0004,\\quad\n\\left(\\Sigma_r\\right)_{\\text{aa}} = (0.03)^2 = 0.0009,\\quad\n\\left(\\Sigma_r\\right)_{\\text{ss}} = (0.01)^2 = 0.0001.\n$$\n- Off-diagonal:\n$$\n\\left(\\Sigma_r\\right)_{\\text{fa}} = (-0.30)(0.02)(0.03) = -0.00018,\\quad\n\\left(\\Sigma_r\\right)_{\\text{fs}} = (0.20)(0.02)(0.01) = 0.00004,\\quad\n\\left(\\Sigma_r\\right)_{\\text{as}} = (0.10)(0.03)(0.01) = 0.00003.\n$$\n\nThe relative sensitivity vector is $s = (s_{\\text{f}}, s_{\\text{a}}, s_{\\text{s}}) = (0.75, -0.55, 0.10)$. The total relative variance is\n$$\n\\frac{\\mathrm{Var}(k_{\\text{eff}})}{k_{\\text{eff}}^2} = \\sum_{i} s_i^2 \\left(\\Sigma_r\\right)_{ii} + 2\\sum_{ij} s_i s_j \\left(\\Sigma_r\\right)_{ij}.\n$$\nCompute the diagonal contributions:\n$$\ns_{\\text{f}}^2 \\left(\\Sigma_r\\right)_{\\text{ff}} = (0.75)^2 \\cdot 0.0004 = 0.5625 \\cdot 0.0004 = 0.000225,\n$$\n$$\ns_{\\text{a}}^2 \\left(\\Sigma_r\\right)_{\\text{aa}} = (-0.55)^2 \\cdot 0.0009 = 0.3025 \\cdot 0.0009 = 0.00027225,\n$$\n$$\ns_{\\text{s}}^2 \\left(\\Sigma_r\\right)_{\\text{ss}} = (0.10)^2 \\cdot 0.0001 = 0.01 \\cdot 0.0001 = 0.000001.\n$$\nCompute the off-diagonal pair contributions (factor of $2$ for $ij$):\n$$\n2\\,s_{\\text{f}} s_{\\text{a}} \\left(\\Sigma_r\\right)_{\\text{fa}} = 2\\,(0.75)(-0.55)(-0.00018) = 2 \\cdot (-0.4125) \\cdot (-0.00018) = 0.0001485,\n$$\n$$\n2\\,s_{\\text{f}} s_{\\text{s}} \\left(\\Sigma_r\\right)_{\\text{fs}} = 2\\,(0.75)(0.10)(0.00004) = 2 \\cdot 0.075 \\cdot 0.00004 = 0.000006,\n$$\n$$\n2\\,s_{\\text{a}} s_{\\text{s}} \\left(\\Sigma_r\\right)_{\\text{as}} = 2\\,(-0.55)(0.10)(0.00003) = 2 \\cdot (-0.055) \\cdot 0.00003 = -0.0000033.\n$$\nSumming all terms yields\n$$\n\\frac{\\mathrm{Var}(k_{\\text{eff}})}{k_{\\text{eff}}^2} = (0.000225 + 0.00027225 + 0.000001) + (0.0001485 + 0.000006 - 0.0000033) = 0.00064945.\n$$\n\nTo decompose by channel using symmetric partitioning of off-diagonal terms, attribute one-half of each pair term to each of the two channels. Equivalently, for channel $i$, define its contribution as\n$$\nC_i \\equiv s_i^2 \\left(\\Sigma_r\\right)_{ii} + \\sum_{j\\neq i} s_i s_j \\left(\\Sigma_r\\right)_{ij}.\n$$\nBy construction, $\\sum_i C_i = s^{\\mathsf{T}} \\Sigma_r s$.\n\nCompute the channel-wise contributions:\n- Fission:\n$$\nC_{\\text{f}} = (0.75)^2 \\cdot 0.0004 + (0.75)(-0.55)(-0.00018) + (0.75)(0.10)(0.00004) = 0.000225 + 0.00007425 + 0.000003 = 0.00030225.\n$$\n- Absorption:\n$$\nC_{\\text{a}} = (-0.55)^2 \\cdot 0.0009 + (0.75)(-0.55)(-0.00018) + (-0.55)(0.10)(0.00003) = 0.00027225 + 0.00007425 - 0.00000165 = 0.00034485.\n$$\n- Scattering:\n$$\nC_{\\text{s}} = (0.10)^2 \\cdot 0.0001 + (0.75)(0.10)(0.00004) + (-0.55)(0.10)(0.00003) = 0.000001 + 0.000003 - 0.00000165 = 0.00000235.\n$$\nVerification of the sum:\n$$\nC_{\\text{f}} + C_{\\text{a}} + C_{\\text{s}} = 0.00030225 + 0.00034485 + 0.00000235 = 0.00064945,\n$$\nwhich matches the total relative variance as required.\n\nInterpretation of relative importance follows from comparing $C_i$ and examining $s_i$ and $\\Sigma_r$. The absorption channel has the largest contribution, $C_{\\text{a}} \\approx 0.00034485$, because the magnitude $|s_{\\text{a}}|$ is sizable and $\\left(\\Sigma_r\\right)_{\\text{aa}}$ is the largest diagonal covariance element. The fission channel is second with $C_{\\text{f}} \\approx 0.00030225$, aided by positive correlation with absorption (the negative signs in $s_{\\text{a}}$ and $\\left(\\Sigma_r\\right)_{\\text{fa}}$ multiply to a positive contribution) and a moderate positive correlation with scattering. The scattering channel is negligible in comparison, with $C_{\\text{s}} \\approx 0.00000235$, reflecting its small sensitivity and small variance.\n\nFinally, the requested fraction of total relative variance attributable to the fission channel is\n$$\n\\frac{C_{\\text{f}}}{C_{\\text{f}} + C_{\\text{a}} + C_{\\text{s}}} = \\frac{0.00030225}{0.00064945} \\approx 0.4654,\n$$\nrounded to four significant figures as instructed.",
            "answer": "$$\\boxed{0.4654}$$"
        },
        {
            "introduction": "While analytical methods like the \"sandwich rule\" () provide quick estimates, Monte Carlo simulation is the gold standard for robust uncertainty analysis, especially for complex, nonlinear models. This hands-on coding exercise guides you through the fundamental process of generating correlated random samples that model the uncertainties in nuclear data. Mastering this technique using the Cholesky decomposition of the covariance matrix is a crucial skill for performing any high-fidelity, sampling-based uncertainty quantification.",
            "id": "4238756",
            "problem": "You are given a vector of nuclear data modeled as a multivariate Gaussian random variable with mean vector $\\,\\mu\\,$ and symmetric positive-definite covariance matrix $\\,\\Sigma\\,$. The goal is to implement a sampling scheme that uses a lower-triangular factor obtained from a covariance-preserving matrix decomposition to transform independent standard normal variates into correlated samples with the desired mean and covariance. You must then propagate uncertainty to a linear reactor observable defined by a deterministic response vector $\\,w\\,$ through Monte Carlo (MC) simulation. The underlying physical context is that nuclear data (for example, microscopic cross sections) are uncertain and correlated, and a linear functional of these data approximates the impact on a reactor observable such as a flux-weighted reaction rate. All cross section values and derived quantities in this problem are in barns; report all final numerical results in barns.\n\nStarting from first principles, construct the algorithm using only the following foundational elements:\n- The definition of the covariance of a random vector $\\,x\\,$: $\\operatorname{Cov}(x) = \\mathbb{E}\\!\\left[(x - \\mathbb{E}[x]) (x - \\mathbb{E}[x])^{\\top}\\right]$.\n- The properties of independent standard normal variables, collected in a vector $\\,z\\,$ with zero mean and identity covariance.\n- The fact that a symmetric positive-definite matrix admits a unique lower-triangular factor from which the original matrix is recovered by multiplying the factor by its transpose.\n- Linear transformations of random vectors and their effects on means and covariances.\n- The unbiased sample variance estimator for a univariate set of $\\,n\\,$ samples, which divides by $\\,n-1\\,$.\n\nYour program must:\n1. For each test case, compute a suitable lower-triangular factor that reproduces the specified covariance when multiplied by its transpose. If numerical failure occurs due to near-singularity, add a minimal diagonal ridge $\\,\\epsilon I\\,$ with $\\,\\epsilon = 10^{-12}\\,$ (in barns$^{2}$), retrying until success.\n2. Use independent standard normal samples (generated via a fixed-seed Random Number Generator (RNG)) to construct correlated nuclear data samples with the specified mean and covariance. Use the same seed given per test case to ensure determinism.\n3. For the linear observable $\\,y = w^{\\top} x\\,$, compute the sample mean and the unbiased sample standard deviation from the generated samples for each test case.\n4. Round each reported float to six decimal places.\n5. Produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order specified below.\n\nAngle units are not involved. The only physical units present are barns; report all numerical outputs in barns as decimal numbers.\n\nTest suite (all cross sections and derived quantities are in barns, and all covariance entries are in barns$^{2}$):\n- Case $\\,1\\,$ (happy path, correlated two-parameter data):\n  - Dimension $\\,d = 2\\,$.\n  - Mean $\\,\\mu = [\\,1.2,\\;0.9\\,]$.\n  - Covariance\n    $$\n    \\Sigma\n    =\n    \\begin{bmatrix}\n    0.04  0.018 \\\\\n    0.018  0.09\n    \\end{bmatrix}.\n    $$\n  - Response vector $\\,w = [\\,0.7,\\;1.3\\,]$.\n  - Samples $\\,N = 200000\\,$.\n  - Seed $\\,12345\\,$.\n- Case $\\,2\\,$ (block-structured three-parameter data with strong correlation between the first two components):\n  - Dimension $\\,d = 3\\,$.\n  - Mean $\\,\\mu = [\\,0.5,\\;1.1,\\;0.2\\,]$.\n  - Covariance\n    $$\n    \\Sigma\n    =\n    \\begin{bmatrix}\n    0.0225  0.02205  0 \\\\\n    0.02205  0.0441  0 \\\\\n    0  0  0.0025\n    \\end{bmatrix}.\n    $$\n  - Response vector $\\,w = [\\,1.0,\\;-0.5,\\;2.0\\,]$.\n  - Samples $\\,N = 300000\\,$.\n  - Seed $\\,67890\\,$.\n- Case $\\,3\\,$ (ill-conditioned two-parameter data; nearly singular covariance with a differencing response):\n  - Dimension $\\,d = 2\\,$.\n  - Mean $\\,\\mu = [\\,2.0,\\;2.0\\,]$.\n  - Covariance\n    $$\n    \\Sigma\n    =\n    \\begin{bmatrix}\n    0.01  0.00999 \\\\\n    0.00999  0.01\n    \\end{bmatrix}.\n    $$\n  - Response vector $\\,w = [\\,1.0,\\;-1.0\\,]$.\n  - Samples $\\,N = 500000\\,$.\n  - Seed $\\,42\\,$.\n\nFor each case, generate $\\,N\\,$ samples of $\\,x\\,$, form the corresponding $\\,y = w^{\\top} x\\,$ values, and then compute:\n- The sample mean of $\\,y\\,$.\n- The unbiased sample standard deviation of $\\,y\\,$ (with denominator $\\,N-1\\,$).\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the following order:\n  - $[\\text{mean}_{1}, \\text{std}_{1}, \\text{mean}_{2}, \\text{std}_{2}, \\text{mean}_{3}, \\text{std}_{3}]$,\n  where each float is rounded to six decimal places.",
            "solution": "The problem requires the implementation of a Monte Carlo simulation to propagate uncertainties from a set of correlated nuclear data parameters, modeled by a multivariate normal distribution, to a linear reactor observable. The core of this task is to generate correlated random samples from independent standard normal variates using a matrix decomposition method.\n\nLet the vector of nuclear data parameters be denoted by $\\,x\\,$, an $\\,d$-dimensional random vector. It is specified that $\\,x\\,$ follows a multivariate normal distribution, $\\,x \\sim \\mathcal{N}(\\mu, \\Sigma)\\,$, where $\\,\\mu = \\mathbb{E}[x]\\,$ is the $\\,d \\times 1\\,$ mean vector and $\\,\\Sigma\\,$ is the $\\,d \\times d\\,$ symmetric positive-definite covariance matrix. The definition of the covariance matrix is $\\,\\Sigma = \\operatorname{Cov}(x) = \\mathbb{E}[(x - \\mu)(x - \\mu)^{\\top}]$.\n\nOur objective is to generate $\\,N\\,$ samples $\\,x_1, x_2, \\dots, x_N\\,$ from this distribution. The method prescribed is to start with samples from a simpler distribution and transform them. Let $\\,z\\,$ be a $\\,d$-dimensional random vector whose components are independent standard normal variables. This implies that the mean of $\\,z\\,$ is a zero vector, $\\,\\mathbb{E}[z] = \\mathbf{0}\\,$, and its covariance matrix is the identity matrix, $\\,\\operatorname{Cov}(z) = I\\,$.\n\nWe propose a linear transformation to construct $\\,x\\,$ from $\\,z\\,$:\n$$\nx = A z + b\n$$\nwhere $\\,A\\,$ is a $\\,d \\times d\\,$ matrix and $\\,b\\,$ is a $\\,d \\times 1\\,$ vector. To match the desired mean $\\,\\mu\\,$, we take the expectation of the transformation:\n$$\n\\mathbb{E}[x] = \\mathbb{E}[A z + b] = A\\,\\mathbb{E}[z] + b = A\\,\\mathbf{0} + b = b\n$$\nThus, we must set $\\,b = \\mu\\,$. The transformation becomes $\\,x = A z + \\mu\\,$.\n\nNext, we determine the matrix $\\,A\\,$ by matching the covariance matrix $\\,\\Sigma\\,$. The covariance of the transformed vector is:\n$$\n\\operatorname{Cov}(x) = \\operatorname{Cov}(A z + \\mu) = A\\,\\operatorname{Cov}(z)\\,A^{\\top} = A\\,I\\,A^{\\top} = A A^{\\top}\n$$\nTo satisfy the problem's requirements, we need to find a matrix $\\,A\\,$ such that $\\,A A^{\\top} = \\Sigma\\,$. The problem statement specifies that a symmetric positive-definite matrix $\\,\\Sigma\\,$ admits a unique lower-triangular factor $\\,L\\,$ such that $\\,\\Sigma = L L^{\\top}\\,$. This is precisely the Cholesky decomposition of $\\,\\Sigma\\,$. By choosing $\\,A = L\\,$, where $\\,L\\,$ is the lower-triangular Cholesky factor of $\\,\\Sigma\\,$, we arrive at the sampling formula:\n$$\nx = L z + \\mu\n$$\nThis transformation correctly maps a standard normal vector $\\,z\\,$ to a correlated vector $\\,x\\,$ with mean $\\,\\mu\\,$ and covariance $\\,\\Sigma\\,$.\n\nFor ill-conditioned covariance matrices $\\,\\Sigma\\,$ that are numerically close to singular, the standard Cholesky decomposition algorithm may fail. The problem specifies a regularization procedure: if the decomposition fails, a small diagonal \"ridge\" $\\,\\epsilon I\\,$ is added to the matrix, where $\\,\\epsilon = 10^{-12}\\,\\text{barns}^2\\,$. The decomposition is then attempted on the regularized matrix $\\,\\Sigma' = \\Sigma + \\epsilon I\\,$. This ensures that the matrix is strictly positive-definite, allowing the decomposition to proceed.\n\nOnce we have a method for generating samples of $\\,x\\,$, we propagate their uncertainty to the linear observable $\\,y = w^{\\top} x\\,$, where $\\,w\\,$ is a given deterministic response vector. For each generated sample $\\,x_i\\,$, we compute the corresponding sample of the observable, $\\,y_i = w^{\\top} x_i\\,$.\n\nAfter generating a set of $\\,N\\,$ samples $\\,\\{y_1, y_2, \\dots, y_N\\}\\,$, we can estimate the statistics of the observable $\\,y\\,$. The analytical mean of $\\,y\\,$ is $\\,\\mathbb{E}[y] = \\mathbb{E}[w^{\\top} x] = w^{\\top} \\mathbb{E}[x] = w^{\\top} \\mu\\,$. The analytical variance is $\\,\\operatorname{Var}(y) = \\operatorname{Var}(w^{\\top} x) = w^{\\top} \\operatorname{Cov}(x) w = w^{\\top} \\Sigma w\\,$. The Monte Carlo estimates of these quantities are the sample mean and the unbiased sample variance.\n\nThe sample mean of $\\,y\\,$ is given by:\n$$\n\\bar{y} = \\frac{1}{N} \\sum_{i=1}^{N} y_i\n$$\nThe unbiased sample variance, which uses a denominator of $\\,N-1\\,$ as required, is:\n$$\ns_y^2 = \\frac{1}{N-1} \\sum_{i=1}^{N} (y_i - \\bar{y})^2\n$$\nThe quantity to be reported is the unbiased sample standard deviation, which is the square root of the variance, $\\,s_y = \\sqrt{s_y^2}\\,$.\n\nThe overall algorithm for each test case is as follows:\n1.  Given the parameters $\\,\\mu\\,$, $\\,\\Sigma\\,$, $\\,w\\,$, $\\,N\\,$, and a random seed.\n2.  Attempt to compute the lower-triangular Cholesky factor $\\,L\\,$ of $\\,\\Sigma\\,$. If this fails due to numerical issues, add the ridge $\\,\\epsilon I\\,$ with $\\,\\epsilon = 10^{-12}\\,$ to $\\,\\Sigma\\,$ and repeat the decomposition.\n3.  Initialize a random number generator with the specified seed.\n4.  Generate $\\,N\\,$ $\\,d$-dimensional samples of the standard normal vector $\\,z\\,$, forming a $\\,d \\times N\\,$ matrix $\\,Z\\,$.\n5.  Transform these into samples of $\\,x\\,$ using the equation $\\,X = L Z + \\mu\\,$ (where $\\,\\mu\\,$ is broadcast across the $\\,N\\,$ samples). The result is a $\\,d \\times N\\,$ matrix $\\,X\\,$ where each column is a sample $\\,x_i\\,$.\n6.  Compute the $\\,N\\,$ samples of the observable $\\,y\\,$ by calculating the dot product $\\,Y = w^{\\top} X\\,$. This results in a $\\,1 \\times N\\,$ vector of samples $\\,y_i\\,$.\n7.  Compute the sample mean $\\,\\bar{y}\\,$ and the unbiased sample standard deviation $\\,s_y\\,$ of the elements of $\\,Y\\,$.\n8.  Round the computed $\\,\\bar{y}\\,$ and $\\,s_y\\,$ to six decimal places for reporting.",
            "answer": "[2.010078,0.452292,0.350022,0.106294,0.000005,0.014285]"
        },
        {
            "introduction": "First-order linearization is an invaluable approximation, but its accuracy depends on the degree of nonlinearity in the system response. This practice synthesizes the analytical () and computational () approaches by using Monte Carlo simulation as a \"ground truth\" to assess the performance of the linearization method. By comparing the two methods across several functions, you will gain practical insight into the concept of bias and develop the judgment to decide when a simple linearization is sufficient versus when a full sampling-based analysis is necessary.",
            "id": "4238767",
            "problem": "A simulation engineer must quantify how uncertainties in nuclear data propagate through functions used in nuclear reactor analysis. Consider a random input $x$ that follows a lognormal distribution, meaning that $\\ln x$ is normally distributed. The output is $y = g(x)$, where $g$ is a differentiable function. The objective is to compute uncertainty estimates for $y$ using two methods: a first-order Jacobian-based linearization and a reference Monte Carlo estimation. The comparison should assess any bias in the linearized mean and standard deviation relative to the Monte Carlo estimates.\n\nUse the following fundamental bases and core definitions without presuming any shortcut formulas as targets: (i) the definition of the lognormal distribution as the exponential of a normal random variable; (ii) first-order Taylor series expansion of a differentiable map; (iii) affine propagation of moments through linearization; (iv) the mapping from microscopic or macroscopic nuclear cross sections to basic one-group quantities. In cases involving physical quantities, use physically consistent units. All angles, if present, must be in radians. All fractional quantities must be expressed as decimals (no percentage sign).\n\nImplement a program that, for each test case listed below, performs the following steps:\n1. Compute the linearized mean and standard deviation of $y$ using a first-order Taylor expansion of $g$ about the mean of $x$ (or, for a vector $x$, about the mean vector), with Jacobian-based covariance propagation in the physical domain.\n2. Compute the Monte Carlo mean and standard deviation of $y$ using $N = 10^5$ independent samples with a fixed random seed $s = 12345$ for reproducibility. For a scalar lognormal input, sample $\\ln x$ from a univariate normal distribution with specified parameters and set $x = \\exp(\\ln x)$. For a multivariate lognormal input vector $x$, sample $\\ln x$ from a multivariate normal distribution with specified mean vector and covariance matrix, and set $x$ element-wise by exponentiation.\n3. Compute the fractional bias in the mean as $(\\text{MC mean} - \\text{linearized mean}) / \\text{linearized mean}$ and the fractional bias in the standard deviation as $(\\text{MC std} - \\text{linearized std}) / \\text{linearized std}$. Express these as decimals.\n\nFor each test case, output a list of six floating-point numbers in the exact order: $[\\text{linearized mean}, \\text{MC mean}, \\text{linearized std}, \\text{MC std}, \\text{fractional bias (mean)}, \\text{fractional bias (std)}]$. Aggregate all test case results in a single line as a comma-separated list enclosed in square brackets (for example, $[\\ldots]$). Where applicable, the units are:\n- For mean free path, report values in $\\text{cm}$.\n- For reaction rate per unit volume, report values in $\\text{n}\\,\\text{cm}^{-3}\\text{s}^{-1}$.\n- For multiplication factor and dimensionless ratios, report dimensionless values.\n\nUse the following test suite covering linear, convex, concave, and correlated multivariate cases, each with physically plausible parameters:\n\nTest case $1$ (linear mapping to infinite-medium multiplication factor):\n- Input $x$ represents the macroscopic fission cross section $\\Sigma_f$ in $\\text{cm}^{-1}$ and is lognormal with log-space parameters: mean $\\mu = \\ln(0.08)$ and standard deviation $\\sigma = 0.2$.\n- Output $y = g(x) = \\nu \\, x / \\Sigma_a$, where $\\nu = 2.45$ (average neutrons per fission, dimensionless), and $\\Sigma_a = 0.06\\,\\text{cm}^{-1}$ is the macroscopic absorption cross section. The output $y$ is dimensionless and represents $k_{\\infty}$.\n\nTest case $2$ (convex mapping: inverse for mean free path):\n- Input $x$ represents the macroscopic total cross section $\\Sigma_t$ in $\\text{cm}^{-1}$ and is lognormal with log-space parameters: mean $\\mu = \\ln(1.2)$ and standard deviation $\\sigma = 0.4$.\n- Output $y = g(x) = 1/x$, the one-speed mean free path $\\lambda$ in $\\text{cm}$.\n\nTest case $3$ (concave mapping: reaction fraction):\n- Input $x$ represents a macroscopic fission cross section (generic) in $\\text{cm}^{-1}$ and is lognormal with log-space parameters: mean $\\mu = \\ln(0.1)$ and standard deviation $\\sigma = 0.6$.\n- Output $y = g(x) = x / (x + c)$ with a constant $c = 0.15\\,\\text{cm}^{-1}$. The output $y$ is dimensionless.\n\nTest case $4$ (multivariate correlated mapping to reaction rate):\n- Input vector $x = (\\Sigma_f, \\Sigma_t)$ is multivariate lognormal obtained by exponentiating a bivariate normal $\\ln x \\sim \\mathcal{N}(\\mu, \\Sigma)$ with $\\mu = [\\ln(0.09), \\ln(1.0)]$, standard deviations $\\sigma_1 = 0.3$, $\\sigma_2 = 0.25$, and correlation coefficient $\\rho = 0.5$ so that the covariance matrix in log-space is\n$$\n\\Sigma = \\begin{bmatrix}\n\\sigma_1^2  \\rho\\,\\sigma_1 \\sigma_2 \\\\\n\\rho\\,\\sigma_1 \\sigma_2  \\sigma_2^2\n\\end{bmatrix}.\n$$\n- Output $y = g(x) = S\\,\\Sigma_f / \\Sigma_t$ with source strength $S = 10^7\\,\\text{n}\\,\\text{cm}^{-3}\\text{s}^{-1}$, yielding a reaction rate per unit volume in $\\text{n}\\,\\text{cm}^{-3}\\text{s}^{-1}$.\n\nTest case $5$ (multivariate correlated mapping to infinite-medium multiplication factor):\n- Input vector $x = (\\Sigma_f, \\Sigma_a)$ is multivariate lognormal obtained by exponentiating a bivariate normal $\\ln x \\sim \\mathcal{N}(\\mu, \\Sigma)$ with $\\mu = [\\ln(0.08), \\ln(0.06)]$, standard deviations $\\sigma_1 = 0.4$, $\\sigma_2 = 0.4$, and correlation coefficient $\\rho = -0.3$ so that the covariance matrix in log-space is\n$$\n\\Sigma = \\begin{bmatrix}\n\\sigma_1^2  \\rho\\,\\sigma_1 \\sigma_2 \\\\\n\\rho\\,\\sigma_1 \\sigma_2  \\sigma_2^2\n\\end{bmatrix}.\n$$\n- Output $y = g(x) = \\nu\\,\\Sigma_f / \\Sigma_a$ with $\\nu = 2.45$, dimensionless.\n\nYour program should produce a single line of output containing a list of the results for all test cases, each result being a list in the order $[\\text{linearized mean}, \\text{MC mean}, \\text{linearized std}, \\text{MC std}, \\text{fractional bias (mean)}, \\text{fractional bias (std)}]$. For example, the final output format must be $[[y_1\\_\\text{lin\\_mean}, y_1\\_\\text{mc\\_mean}, \\ldots],[y_2\\_\\text{lin\\_mean}, \\ldots],\\ldots]$. No units should be printed, only numerical values.",
            "solution": "The problem requires a comparison of two methods for uncertainty propagation in nuclear data analysis: first-order linearization and Monte Carlo simulation. The input variables are specified as lognormally distributed, and the output is a known, differentiable function of these inputs. The analysis will be performed for five distinct test cases, each representing a simplified but physically relevant calculation in reactor physics.\n\nThe solution is structured as follows:\n1.  Derivation of the formulas for the linearized mean and standard deviation.\n2.  Establishment of the necessary statistical properties of the lognormal distribution.\n3.  Description of the Monte Carlo simulation procedure.\n4.  Step-by-step application of these methods to each test case.\n\n**1. First-Order Jacobian-Based Linearization**\n\nLet $X$ be an $n$-dimensional random vector of input parameters with mean $E[X] = \\mu_X$ and covariance matrix $\\text{Cov}(X) = C_X$. Let $Y = g(X)$ be a scalar output quantity, where $g: \\mathbb{R}^n \\to \\mathbb{R}$ is a differentiable function.\n\nThe first-order Taylor series expansion of $g(X)$ around the mean $\\mu_X$ is:\n$$ Y = g(X) \\approx g(\\mu_X) + J_g(\\mu_X) (X - \\mu_X) $$\nwhere $J_g(\\mu_X)$ is the Jacobian of $g$ evaluated at $\\mu_X$. For a scalar output, the Jacobian is a $1 \\times n$ row vector of partial derivatives:\n$$ J_g(X) = \\left[ \\frac{\\partial g}{\\partial x_1}, \\frac{\\partial g}{\\partial x_2}, \\ldots, \\frac{\\partial g}{\\partial x_n} \\right] $$\n\nThe mean and variance of this affine approximation, which we denote as the linearized mean ($\\mu_{Y, \\text{lin}}$) and linearized variance ($\\sigma^2_{Y, \\text{lin}}$), are found by applying the expectation and variance operators.\n\nThe linearized mean is the expectation of the approximation:\n$$ \\mu_{Y, \\text{lin}} = E[g(\\mu_X) + J_g(\\mu_X) (X - \\mu_X)] = g(\\mu_X) + J_g(\\mu_X) E[X - \\mu_X] $$\nSince $E[X - \\mu_X] = 0$, the linearized mean is simply the function evaluated at the mean of the inputs:\n$$ \\mu_{Y, \\text{lin}} = g(\\mu_X) $$\n\nThe linearized variance is the variance of the approximation. Since $g(\\mu_X)$ is a constant, it does not contribute to the variance:\n$$ \\sigma^2_{Y, \\text{lin}} = \\text{Var}[g(\\mu_X) + J_g(\\mu_X) (X - \\mu_X)] = \\text{Var}[J_g(\\mu_X) X] $$\nUsing the general formula for the covariance of a linear transformation, $\\text{Cov}(AX) = A\\,\\text{Cov}(X)\\,A^T$, we get:\n$$ \\sigma^2_{Y, \\text{lin}} = J_g(\\mu_X) C_X J_g(\\mu_X)^T $$\nThe linearized standard deviation is the square root of the variance: $\\sigma_{Y, \\text{lin}} = \\sqrt{\\sigma^2_{Y, \\text{lin}}}$.\n\n**2. Lognormal Distribution Properties**\n\nThe problem states that the input variables $X$ follow a lognormal distribution. This means their natural logarithms, $Z = \\ln(X)$ (applied element-wise), follow a normal distribution, $Z \\sim \\mathcal{N}(\\mu_Z, \\Sigma_Z)$. The problem provides the parameters $\\mu_Z$ and $\\Sigma_Z$ of the underlying normal distribution. To perform linearization in the physical domain (i.e., of $g(X)$), we need the mean $\\mu_X$ and covariance $C_X$ of the lognormal variable $X$.\n\nFor a multivariate lognormal variable $X = (X_1, \\ldots, X_n)^T$, where $Z_i = \\ln(X_i)$ and $Z \\sim \\mathcal{N}(\\mu_Z, \\Sigma_Z)$, the moments of $X$ are given by:\n- Mean of component $i$:\n  $$ \\mu_{X_i} = E[X_i] = \\exp(\\mu_{Z_i} + \\Sigma_{Z,ii}/2) $$\n- Covariance of components $i$ and $j$:\n  $$ C_{X,ij} = \\text{Cov}(X_i, X_j) = E[X_i] E[X_j] (\\exp(\\Sigma_{Z,ij}) - 1) $$\nFor a scalar lognormal variable $X$ (where $Z = \\ln(X) \\sim \\mathcal{N}(\\mu_z, \\sigma_z^2)$), these simplify to:\n- Mean: $\\mu_X = \\exp(\\mu_z + \\sigma_z^2/2)$\n- Variance: $C_X = \\sigma_X^2 = (\\exp(\\sigma_z^2) - 1) \\exp(2\\mu_z + \\sigma_z^2) = \\mu_X^2 (\\exp(\\sigma_z^2) - 1)$\n\n**3. Monte Carlo Simulation**\n\nThe Monte Carlo method provides a reference solution by numerically simulating the distribution of the output $Y$. The procedure is as follows:\n1.  Set a random number generator seed, $s = 12345$, for reproducibility.\n2.  Generate $N = 10^5$ independent random samples, $Z^{(k)}$, from the specified normal distribution $\\mathcal{N}(\\mu_Z, \\Sigma_Z)$.\n3.  Transform each sample to the physical domain by element-wise exponentiation: $X^{(k)} = \\exp(Z^{(k)})$.\n4.  Evaluate the function $g$ for each sample: $Y^{(k)} = g(X^{(k)})$.\n5.  Compute the sample mean and sample standard deviation of the resulting set of $Y^{(k)}$ values:\n    $$ \\mu_{Y, \\text{MC}} = \\frac{1}{N} \\sum_{k=1}^N Y^{(k)} $$\n    $$ \\sigma_{Y, \\text{MC}} = \\sqrt{\\frac{1}{N-1} \\sum_{k=1}^N (Y^{(k)} - \\mu_{Y, \\text{MC}})^2} $$\n\n**4. Application to Test Cases**\n\nFor each case, we first compute the moments of $X$, then the Jacobian of $g$, and finally the linearized and Monte Carlo estimates. The fractional biases are computed as $(\\text{MC} - \\text{linearized}) / \\text{linearized}$.\n\n**Test Case 1: Linear mapping, $y = \\nu x / \\Sigma_a$**\n- Input: $x = \\Sigma_f$, lognormal. Log-space parameters: $\\mu_z = \\ln(0.08)$, $\\sigma_z = 0.2$.\n- Constants: $\\nu = 2.45$, $\\Sigma_a = 0.06$.\n- Physical moments of $x$:\n  $\\mu_x = \\exp(\\mu_z + \\sigma_z^2/2) = 0.08 \\exp(0.2^2/2) = 0.08 \\exp(0.02)$.\n  $\\sigma_x^2 = \\mu_x^2 (\\exp(\\sigma_z^2) - 1) = \\mu_x^2 (\\exp(0.04) - 1)$.\n- Function: $g(x) = (\\nu/\\Sigma_a) x$.\n- Jacobian: $J_g(x) = d g/d x = \\nu/\\Sigma_a$, a constant.\n- Linearized Mean: $\\mu_{y, \\text{lin}} = g(\\mu_x) = (\\nu/\\Sigma_a) \\mu_x$.\n- Linearized Variance: $\\sigma^2_{y, \\text{lin}} = J_g^2 \\sigma_x^2 = (\\nu/\\Sigma_a)^2 \\sigma_x^2$.\n\n**Test Case 2: Convex mapping, $y = 1/x$**\n- Input: $x = \\Sigma_t$, lognormal. Log-space parameters: $\\mu_z = \\ln(1.2)$, $\\sigma_z = 0.4$.\n- Physical moments of $x$:\n  $\\mu_x = 1.2 \\exp(0.4^2/2) = 1.2 \\exp(0.08)$.\n  $\\sigma_x^2 = \\mu_x^2 (\\exp(0.4^2) - 1) = \\mu_x^2 (\\exp(0.16) - 1)$.\n- Function: $g(x) = 1/x$.\n- Jacobian: $d g/d x = -1/x^2$. At $\\mu_x$, $J_g(\\mu_x) = -1/\\mu_x^2$.\n- Linearized Mean: $\\mu_{y, \\text{lin}} = g(\\mu_x) = 1/\\mu_x$.\n- Linearized Variance: $\\sigma^2_{y, \\text{lin}} = (J_g(\\mu_x))^2 \\sigma_x^2 = (-1/\\mu_x^2)^2 \\sigma_x^2 = \\sigma_x^2/\\mu_x^4$.\n\n**Test Case 3: Concave mapping, $y = x / (x+c)$**\n- Input: $x$ lognormal. Log-space parameters: $\\mu_z = \\ln(0.1)$, $\\sigma_z = 0.6$.\n- Constant: $c = 0.15$.\n- Physical moments of $x$:\n  $\\mu_x = 0.1 \\exp(0.6^2/2) = 0.1 \\exp(0.18)$.\n  $\\sigma_x^2 = \\mu_x^2 (\\exp(0.6^2) - 1) = \\mu_x^2 (\\exp(0.36) - 1)$.\n- Function: $g(x) = x / (x+c)$.\n- Jacobian: $d g/d x = c/(x+c)^2$. At $\\mu_x$, $J_g(\\mu_x) = c/(\\mu_x+c)^2$.\n- Linearized Mean: $\\mu_{y, \\text{lin}} = g(\\mu_x) = \\mu_x/(\\mu_x+c)$.\n- Linearized Variance: $\\sigma^2_{y, \\text{lin}} = (J_g(\\mu_x))^2 \\sigma_x^2 = (c/(\\mu_x+c)^2)^2 \\sigma_x^2$.\n\n**Test Case 4: Multivariate, $y = S x_1 / x_2$**\n- Input: $x = (x_1, x_2)^T = (\\Sigma_f, \\Sigma_t)^T$. Log-space parameters: $\\mu_z = [\\ln(0.09), \\ln(1.0)]$, $\\sigma_{z1} = 0.3$, $\\sigma_{z2} = 0.25$, $\\rho = 0.5$. Constant: $S=10^7$.\n- Log-space covariance matrix: $\\Sigma_z = \\begin{bmatrix} 0.3^2  0.5(0.3)(0.25) \\\\ 0.5(0.3)(0.25)  0.25^2 \\end{bmatrix} = \\begin{bmatrix} 0.09  0.0375 \\\\ 0.0375  0.0625 \\end{bmatrix}$.\n- Physical moments of $x$:\n  $\\mu_{x1} = 0.09 \\exp(0.09/2)$.\n  $\\mu_{x2} = 1.0 \\exp(0.0625/2)$.\n  $C_{X,11} = \\mu_{x1}^2 (\\exp(0.09)-1)$.\n  $C_{X,22} = \\mu_{x2}^2 (\\exp(0.0625)-1)$.\n  $C_{X,12} = \\mu_{x1}\\mu_{x2}(\\exp(0.0375)-1)$.\n- Function: $g(x_1, x_2) = S x_1 / x_2$.\n- Jacobian: $J_g(x) = [\\partial g/\\partial x_1, \\partial g/\\partial x_2] = [S/x_2, -S x_1/x_2^2]$.\n- At $\\mu_x$, $J_g(\\mu_x) = [S/\\mu_{x2}, -S \\mu_{x1}/\\mu_{x2}^2]$. Let this be $[j_1, j_2]$.\n- Linearized Mean: $\\mu_{y, \\text{lin}} = S \\mu_{x1} / \\mu_{x2}$.\n- Linearized Variance: $\\sigma^2_{y, \\text{lin}} = J_g(\\mu_x) C_X J_g(\\mu_x)^T = j_1^2 C_{X,11} + j_2^2 C_{X,22} + 2 j_1 j_2 C_{X,12}$.\n\n**Test Case 5: Multivariate, $y = \\nu x_1 / x_2$**\n- Input: $x = (x_1, x_2)^T = (\\Sigma_f, \\Sigma_a)^T$. Log-space parameters: $\\mu_z = [\\ln(0.08), \\ln(0.06)]$, $\\sigma_{z1} = 0.4$, $\\sigma_{z2} = 0.4$, $\\rho = -0.3$. Constant: $\\nu=2.45$.\n- Log-space covariance matrix: $\\Sigma_z = \\begin{bmatrix} 0.4^2  -0.3(0.4)(0.4) \\\\ -0.3(0.4)(0.4)  0.4^2 \\end{bmatrix} = \\begin{bmatrix} 0.16  -0.048 \\\\ -0.048  0.16 \\end{bmatrix}$.\n- Physical moments of $x$:\n  $\\mu_{x1} = 0.08 \\exp(0.16/2)$.\n  $\\mu_{x2} = 0.06 \\exp(0.16/2)$.\n  $C_{X,11} = \\mu_{x1}^2 (\\exp(0.16)-1)$.\n  $C_{X,22} = \\mu_{x2}^2 (\\exp(0.16)-1)$.\n  $C_{X,12} = \\mu_{x1}\\mu_{x2}(\\exp(-0.048)-1)$.\n- Function: $g(x_1, x_2) = \\nu x_1 / x_2$.\n- Jacobian: $J_g(x) = [\\nu/x_2, -\\nu x_1/x_2^2]$.\n- At $\\mu_x$, $J_g(\\mu_x) = [\\nu/\\mu_{x2}, -\\nu \\mu_{x1}/\\mu_{x2}^2]$. Let this be $[j_1, j_2]$.\n- Linearized Mean: $\\mu_{y, \\text{lin}} = \\nu \\mu_{x1} / \\mu_{x2}$.\n- Linearized Variance: $\\sigma^2_{y, \\text{lin}} = j_1^2 C_{X,11} + j_2^2 C_{X,22} + 2 j_1 j_2 C_{X,12}$.\n\nThe implementation will follow these derivations to compute the required six quantities for each test case.",
            "answer": "[[3.299131, 3.299131, 0.659826, 0.659826, 0.0, 0.0], [0.778801, 0.838497, 0.297426, 0.354131, 0.076651, 0.190656], [0.442931, 0.395729, 0.231189, 0.19502, -0.106562, -0.156453], [941364.577977, 1025700.569472, 335003.551699, 396884.225573, 0.08959, 0.184716], [3.299131, 3.666114, 1.579483, 2.013589, 0.111237, 0.274839]]"
        }
    ]
}