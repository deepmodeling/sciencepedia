{
    "hands_on_practices": [
        {
            "introduction": "In an ideal solid solution, the average lattice parameter should vary linearly with composition, a principle known as Vegard's law. However, in real materials like high-entropy alloys, differences in atomic size and bonding lead to local strains that cause the average lattice parameter to deviate from this linear behavior. This exercise () provides hands-on practice in modeling this deviation, or \"bowing,\" by fitting a physically-motivated quadratic model to lattice parameter data, allowing you to extract a single parameter that quantifies the magnitude of the mixing-induced distortion.",
            "id": "3730433",
            "problem": "Consider a substitutional, single-phase pseudo-binary solid solution in a High-Entropy Alloy (HEA) or complex alloy, where the composition variable $x \\in [0,1]$ denotes the atomic fraction of species $B$ replacing species $A$. Under small-strain isotropic elasticity, the hydrostatic strain $\\varepsilon$ and the change in lattice parameter $a$ obey the linear relation $\\Delta a / a \\approx \\varepsilon$. Empirically, the composition dependence of the lattice parameter often follows a linear mixing rule (Vegard's law) plus a systematic deviation attributable to elastic and size mismatch-driven atomic-level lattice distortion. The deviation must vanish at the end-members ($x=0$ and $x=1$), be symmetric with respect to exchanging $A$ and $B$ while swapping $x \\leftrightarrow 1-x$, and scale with the mixing ($x$ and $1-x$). These constraints imply a second-order term in composition that is proportional to $x(1-x)$.\n\nYour task is to determine a set of parameters that best fit the measured lattice parameters at several compositions, subject to the following model structure and physical constraints:\n- The lattice parameter $a(x)$ is represented by three unknown parameters: the end-member lattice parameters $a_A$ and $a_B$ (at $x=0$ and $x=1$, respectively) and a single distortion coefficient $b$ multiplying a second-order composition term that vanishes at both end-members.\n- The model must be linear in the unknown parameters, satisfy $a(0)=a_A$ and $a(1)=a_B$, and include a single scalar $b$ that multiplies a function of $x$ that is zero at $x=0$ and $x=1$ and symmetric under $x \\leftrightarrow 1-x$.\n\nFrom first principles of linear elasticity and mixture symmetry, argue how such a form captures atomic-level lattice distortion arising from elastic and size mismatch. Then, for the provided test suite, compute the best-fit parameters $(a_A,a_B,b)$ by minimizing the sum of squared residuals between the measured lattice parameter values and the model predictions, and report the fitted distortion coefficient $b$ for each case. Interpret the sign and magnitude of $b$ in terms of mismatch and distortion: explain how $b>0$ or $b<0$ relates to expansion or contraction relative to linear mixing, and why the curvature scales with $x(1-x)$.\n\nPhysical units requirement: Express all lattice parameters $a$ and the fitted distortion coefficient $b$ in angstrom units. In the output, provide $b$ in angstroms as floating-point values. Angles are not involved. Do not use percentage signs; any ratios discussed in the reasoning should be expressed as decimals.\n\nTest suite (each case provides composition values $x$ and measured lattice parameter values $a$ in angstroms):\n- Case $1$ (well-defined positive curvature, overdetermined, no measurement noise):\n  - $x$ values: $[0, 0.25, 0.5, 0.75, 1.0]$\n  - measured $a$ values: $[3.60, 3.65375, 3.705, 3.75375, 3.80]$\n- Case $2$ (nearly linear mixing, overdetermined, small measurement noise):\n  - $x$ values: $[0.0, 0.2, 0.4, 0.6, 0.8, 1.0]$\n  - measured $a$ values: $[3.6005, 3.679, 3.7607, 3.8397, 3.9202, 3.9994]$\n- Case $3$ (strong negative curvature, overdetermined, small measurement noise):\n  - $x$ values: $[0.0, 0.3, 0.5, 0.7, 1.0]$\n  - measured $a$ values: $[3.60, 3.6752, 3.729, 3.7947, 3.90]$\n- Case $4$ (boundary, exactly determined with the minimal number of points):\n  - $x$ values: $[0.0, 0.5, 1.0]$\n  - measured $a$ values: $[3.50, 3.6125, 3.70]$\n\nAlgorithmic specification:\n- Construct a linear design in the unknown parameters $(a_A,a_B,b)$ consistent with the constraints above by using basis functions of $x$ that enforce $a(0)=a_A$ and $a(1)=a_B$ and a symmetric second-order term vanishing at the end-members.\n- Solve for $(a_A,a_B,b)$ by minimizing the sum of squared residuals over the provided data points in each case. Any numerically stable method is acceptable, such as using the Moore–Penrose pseudoinverse or a standard least-squares solver.\n\nFinal output format:\n- Your program should produce a single line of output containing the fitted distortion coefficients $b$ for all four cases, in angstrom units, as a comma-separated list enclosed in square brackets. Each value must be rounded to six decimal places. For example, the output format is: $[b_1,b_2,b_3,b_4]$ where $b_i$ are floats rounded to six decimal places, expressed in angstroms.",
            "solution": "The problem statement is subjected to validation before a solution is attempted.\n\nThe givens are as follows:\n- A pseudo-binary solid solution with composition variable $x \\in [0,1]$ representing the atomic fraction of species $B$ replacing species $A$.\n- A model for the lattice parameter $a(x)$ is to be fitted, parameterized by three unknowns: $a_A = a(0)$, $a_B = a(1)$, and a distortion coefficient $b$.\n- The model must be linear in the parameters $(a_A, a_B, b)$.\n- The model must satisfy the boundary conditions $a(0)=a_A$ and $a(1)=a_B$.\n- The model must include a term multiplied by $b$ that is a function of $x$, is zero at $x=0$ and $x=1$, and is symmetric under the transformation $x \\leftrightarrow 1-x$.\n- The fitting procedure is to minimize the sum of squared residuals.\n- Four test cases are provided, each consisting of a set of composition values $x$ and corresponding measured lattice parameter values $a$ in angstroms.\n- The final output must be the fitted distortion coefficient $b$ for each case, rounded to six decimal places, in a specific list format.\n\nThe problem is assessed to be **valid**. It is scientifically grounded in the established materials science principles of solid solution theory, including Vegard's law and deviations thereof. The model form is a standard representation for bowing effects in alloys. The problem is well-posed, providing sufficient data and a clear objective (linear least-squares minimization) to find a unique set of best-fit parameters for each case. The language is objective and precise, and all necessary information for a full solution is provided. The problem is neither trivial nor ill-posed and represents a standard data analysis task in a scientific context.\n\nThe problem requires a derivation of the model form from first principles, followed by the computation of the model parameters.\n\n**1. Theoretical Formulation of the Lattice Parameter Model**\n\nIn a binary solid solution of species $A$ and $B$, the simplest model for the lattice parameter $a(x)$ as a function of the composition $x$ of species $B$ is a linear interpolation between the lattice parameters of the pure end-members, $a_A$ and $a_B$. This is known as Vegard's law:\n$$ a_{veg}(x) = (1-x)a_A + x a_B $$\nThis law assumes an ideal mixture, where the effective atomic volume scales linearly with composition. However, in real alloys, particularly complex concentrated alloys or High-Entropy Alloys (HEAs), factors such as atomic size mismatch, differing electronic structures, and variations in bond energies lead to local lattice distortions. These distortions cause the average lattice parameter to deviate from linear behavior.\n\nTo account for this deviation, we introduce- a correction term, $\\Delta a_{dist}(x)$, to the linear model. The total lattice parameter is then $a(x) = a_{veg}(x) + \\Delta a_{dist}(x)$. The problem statement imposes physically motivated constraints on this deviation term:\n1.  The deviation must vanish for the pure elements. At $x=0$ (pure $A$) and $x=1$ (pure $B$), there is no mixing-induced distortion, so $\\Delta a_{dist}(0) = 0$ and $\\Delta a_{dist}(1) = 0$.\n2.  The physics of distortion should be symmetric with respect to labeling species $A$ or $B$ as the solute. This means exchanging $A$ and $B$ (which corresponds to the transformation $x \\leftrightarrow 1-x$) should not change the functional form of the deviation.\n3.  The deviation is expected to be maximal where the \"disorder\" or mixing is maximal, which occurs at intermediate compositions, away from the pure end-members.\n\nThe simplest polynomial function of $x$ that satisfies these constraints is $f(x) = C \\cdot x(1-x)$, where $C$ is a constant. This function is zero at $x=0$ and $x=1$, and it is symmetric since $(1-x)(1-(1-x)) = (1-x)x$. This quadratic term is often called a \"bowing\" parameter in semiconductor physics and is widely used to model deviations from Vegard's law in metallic alloys.\n\nThus, we adopt the model for the lattice parameter $a(x)$ as:\n$$ a(x) = (1-x)a_A + x a_B + b \\cdot x(1-x) $$\nHere, $b$ is the single distortion coefficient to be determined, which quantifies the magnitude of the deviation from linearity. This model is linear in its three parameters $(a_A, a_B, b)$, as requested. The basis functions for the parameters are $(1-x)$, $x$, and $x(1-x)$, respectively.\n\n**2. Parameter Estimation via Linear Least Squares**\n\nGiven a set of $N$ experimental data points $(x_i, a_i)$, where $i=1, \\dots, N$, we seek the parameters $(a_A, a_B, b)$ that \"best fit\" the model. This is achieved by minimizing the sum of the squared residuals (SSR), $S$, between the measured values $a_i$ and the model's predictions $a(x_i)$:\n$$ S(a_A, a_B, b) = \\sum_{i=1}^{N} [a_i - a(x_i)]^2 = \\sum_{i=1}^{N} [a_i - ((1-x_i)a_A + x_i a_B + x_i(1-x_i)b)]^2 $$\nThis is a standard linear least-squares problem, which can be formulated in matrix algebra. Let $\\boldsymbol{\\beta}$ be the vector of parameters, $\\mathbf{y}$ be the vector of observed lattice parameters, and $\\mathbf{X}$ be the design matrix.\n$$ \\boldsymbol{\\beta} = \\begin{pmatrix} a_A \\\\ a_B \\\\ b \\end{pmatrix}, \\quad \\mathbf{y} = \\begin{pmatrix} a_1 \\\\ a_2 \\\\ \\vdots \\\\ a_N \\end{pmatrix} $$\nThe design matrix $\\mathbf{X}$ is constructed such that each row corresponds to a data point $x_i$, and each column corresponds to a basis function evaluated at $x_i$:\n$$ \\mathbf{X} = \\begin{pmatrix}\n1-x_1 & x_1 & x_1(1-x_1) \\\\\n1-x_2 & x_2 & x_2(1-x_2) \\\\\n\\vdots  & \\vdots  & \\vdots \\\\\n1-x_N & x_N & x_N(1-x_N)\n\\end{pmatrix} $$\nThe problem is to find the vector $\\hat{\\boldsymbol{\\beta}}$ that minimizes the squared Euclidean norm $\\|\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}\\|^2$. The solution is given by the normal equations:\n$$ (\\mathbf{X}^T \\mathbf{X}) \\hat{\\boldsymbol{\\beta}} = \\mathbf{X}^T \\mathbf{y} $$\nAssuming $\\mathbf{X}^T \\mathbf{X}$ is invertible (which is true if the columns of $\\mathbf{X}$ are linearly independent, a condition met by our choice of basis functions and non-degenerate $x_i$ values), the unique solution is:\n$$ \\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y} $$\nThe matrix $(\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T$ is the Moore-Penrose pseudoinverse of $\\mathbf{X}$. Numerically, this system is solved using robust methods like QR decomposition or Singular Value Decomposition (SVD) to ensure stability, as implemented in standard scientific computing libraries.\n\n**3. Interpretation of the Distortion Coefficient $b$**\n\nThe sign and magnitude of the fitted coefficient $b$ provide physical insight into the nature of the atomic interactions in the alloy.\n- If $b > 0$, the term $b \\cdot x(1-x)$ is positive for $x \\in (0,1)$. This means the actual lattice parameter $a(x)$ is larger than that predicted by the linear mixing rule (Vegard's law). This is known as a positive deviation or positive bowing. It typically suggests that the repulsive forces are dominant or that the bonding between unlike atoms (A-B) is weaker than the average of like-atom bonds (A-A and B-B), leading to an effective expansion of the lattice to accommodate the strain.\n- If $b < 0$, the deviation term is negative. The lattice parameter is smaller than the linear average, a phenomenon known as negative deviation or negative bowing. This often indicates stronger attractive interactions between unlike atoms (A-B) compared to the average of the constituents. This can be due to effects like charge transfer or a tendency towards chemical ordering, which pull the atoms closer together.\n- The magnitude $|b|$ quantifies the strength of this non-ideal interaction. A larger $|b|$ implies a more significant deviation from ideal mixing. The scaling with $x(1-x)$ ensures that this deviation effect is most pronounced at the composition of maximum atomic heterogeneity, $x=0.5$, and correctly disappears at the pure end-members.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves for the distortion coefficient 'b' for four test cases\n    of lattice parameter vs. composition data in a pseudo-binary alloy.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: Well-defined positive curvature, overdetermined, no noise\n        {\n            \"x\": [0.0, 0.25, 0.5, 0.75, 1.0],\n            \"a\": [3.60, 3.65375, 3.705, 3.75375, 3.80]\n        },\n        # Case 2: Nearly linear mixing, overdetermined, small noise\n        {\n            \"x\": [0.0, 0.2, 0.4, 0.6, 0.8, 1.0],\n            \"a\": [3.6005, 3.679, 3.7607, 3.8397, 3.9202, 3.9994]\n        },\n        # Case 3: Strong negative curvature, overdetermined, small noise\n        {\n            \"x\": [0.0, 0.3, 0.5, 0.7, 1.0],\n            \"a\": [3.60, 3.6752, 3.729, 3.7947, 3.90]\n        },\n        # Case 4: Exactly determined with minimal points\n        {\n            \"x\": [0.0, 0.5, 1.0],\n            \"a\": [3.50, 3.6125, 3.70]\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        x_vals = np.array(case[\"x\"])\n        a_vals = np.array(case[\"a\"])\n\n        # The model is a(x) = a_A*(1-x) + a_B*x + b*x*(1-x).\n        # This is a linear system of the form X * beta = y, where:\n        # y is the vector of measured lattice parameters 'a'.\n        # beta is the vector of unknown parameters [a_A, a_B, b].\n        # X is the design matrix with columns corresponding to the basis functions\n        # of the parameters: [(1-x), x, x*(1-x)].\n        \n        # Construct the design matrix X.\n        # The number of rows is the number of data points.\n        # The number of columns is the number of parameters (3).\n        X = np.zeros((len(x_vals), 3))\n        \n        # Column 0: Basis function for a_A is (1-x)\n        X[:, 0] = 1 - x_vals\n        \n        # Column 1: Basis function for a_B is x\n        X[:, 1] = x_vals\n        \n        # Column 2: Basis function for b is x*(1-x)\n        X[:, 2] = x_vals * (1 - x_vals)\n        \n        # Solve the linear least-squares problem X * beta = a_vals for beta.\n        # np.linalg.lstsq is a numerically stable solver.\n        # It returns the solution vector, residuals, rank, and singular values.\n        # We only need the solution vector, which contains [a_A, a_B, b].\n        params = np.linalg.lstsq(X, a_vals, rcond=None)[0]\n        \n        # The distortion coefficient 'b' is the third element of the solution vector.\n        b_fit = params[2]\n        \n        results.append(b_fit)\n\n    # Format the final output string as a comma-separated list of floats\n    # rounded to six decimal places, enclosed in square brackets.\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\n# Execute the main function.\nsolve()\n\n```"
        },
        {
            "introduction": "While measurements of the average lattice parameter reveal overall expansion or contraction, they do not capture the distribution of local strains within the material. X-ray diffraction (XRD) provides a powerful window into this heterogeneity, as both finite crystallite size and microscopic strain variations broaden the diffraction peaks. This practice () guides you through the application of the Williamson-Hall analysis, a cornerstone technique used to deconvolve these two contributions from experimental peak broadening data and extract a quantitative measure of microstrain.",
            "id": "3730439",
            "problem": "You are modeling line broadening in X-ray diffraction (XRD) patterns of high-entropy alloys to quantify atomic-level lattice distortion. The total integral breadth broadening is assumed to arise from two physically justified sources: finite crystallite size and distributed microstrain. Starting from the definitions that (i) convolution of independent broadening sources yields additive integral breadth, (ii) the crystallite size broadening scales inversely with a characteristic domain size as constrained by diffraction geometry, and (iii) small lattice parameter fluctuations produce an angular broadening proportional to the tangent of the Bragg angle, derive a linear regression framework that estimates the root-mean-square microstrain and the equivalent crystallite size from measured peak breadths at multiple Bragg angles. Then, propose and implement a strategy that incorporates reflection-dependent anisotropy using a direction-sensitive weighting factor for the strain contribution, consistent with the modified Williamson–Hall approach.\n\nUse the following conditions and definitions:\n- Use angles in radians.\n- Use the wavelength $\\lambda$ and shape factor $K$ as constants; convert $\\lambda$ from Angstroms to nanometers inside your program to maintain unit consistency.\n- Estimate the microstrain $\\epsilon$ (dimensionless) and the crystallite size $D$ (in nanometers). Your program must output $D$ in nanometers.\n- For anisotropic strain separation, use provided reflection-dependent contrast factors as dimensionless weights.\n\nAlgorithmic tasks to implement:\n- From the base assumptions, construct a linear relation of the form $y_i = b + m x_i$ where $y_i$ is a function of the measured integral breadth $ \\beta_i $ at angle $\\theta_i$, and $x_i$ is a function of $\\theta_i$ (and, for anisotropy, also of a reflection-dependent factor). Do not use any non-linear fitting; use ordinary least squares with an explicit intercept.\n- Recover the parameters $ \\epsilon $ and $ D $ in closed form from the slope $ m $ and intercept $ b $, ensuring unit consistency.\n- For anisotropy, use a modified predictor that incorporates the provided contrast factor for each reflection; estimate the anisotropic microstrain and the corresponding $ D $ using the same linear regression framework.\n\nConstants to use in all test cases:\n- X-ray wavelength $ \\lambda = 1.5406 $ Angstroms.\n- Shape factor $ K = 0.9 $.\n\nTest suite and data:\n- Test Case $1$ (general case: simultaneous size and isotropic microstrain):\n  - Bragg angles $ \\theta $ in degrees: $[15,20,25,30,35,40,45,50]$.\n  - Measured integral breadths $ \\beta $ in radians: $[0.005015, 0.005863, 0.006790, 0.007822, 0.008986, 0.010333, 0.011922, 0.013848]$.\n- Test Case $2$ (boundary condition: negligible microstrain, size-dominated broadening):\n  - Bragg angles $ \\theta $ in degrees: $[15,20,25,30,35,40,45,50]$.\n  - Measured integral breadths $ \\beta $ in radians: $[0.004786, 0.004919, 0.005099, 0.005339, 0.005643, 0.006037, 0.006536, 0.007190]$.\n- Test Case $3$ (anisotropic microstrain using modified predictor with contrast factors):\n  - Bragg angles $ \\theta $ in degrees: $[15,20,25,30,35,40,45,50]$.\n  - Measured integral breadths $ \\beta $ in radians: $[0.002994, 0.003457, 0.003922, 0.004139, 0.005086, 0.004767, 0.006318, 0.006796]$.\n  - Reflection-dependent contrast factors $ C_{hkl} $ (dimensionless): $[0.14, 0.21, 0.24, 0.18, 0.29, 0.12, 0.26, 0.20]$.\n\nOutput requirements:\n- Your program should compute:\n  - For Test Case $1$: the isotropic microstrain $ \\epsilon_1 $ and size $ D_1 $.\n  - For Test Case $2$: the isotropic microstrain $ \\epsilon_2 $ and size $ D_2 $.\n  - For Test Case $3$: the isotropic microstrain fit from the anisotropic dataset $ \\epsilon_{3,\\text{iso}} $, the anisotropic microstrain using the modified predictor $ \\epsilon_{3,\\text{aniso}} $, and the corresponding size $ D_3 $ from the modified fit.\n- Round all reported floating-point outputs to $6$ decimal places.\n- Final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order $[\\epsilon_1,D_1,\\epsilon_2,D_2,\\epsilon_{3,\\text{iso}},\\epsilon_{3,\\text{aniso}},D_3]$.\n\nAll numerical values in this statement are given in the units specified. Ensure angles are converted from degrees to radians prior to use; report $D$ in nanometers and microstrain as a decimal (not a percentage).",
            "solution": "The modeling begins from two well-tested facts. First, when two independent broadening mechanisms contribute to the diffraction peak profile, the integral breadths add: if $ \\beta_{\\text{size}} $ and $ \\beta_{\\text{strain}} $ are the integral breadth contributions due to finite crystallite size and microstrain respectively, then the measured integral breadth $ \\beta $ satisfies $ \\beta = \\beta_{\\text{size}} + \\beta_{\\text{strain}} $. Second, the kinematic diffraction geometry relates the angular breadth to crystallite size through a geometric inverse dependence and relates small lattice spacing fluctuations to an angular dependence via the tangent of the Bragg angle. These two relationships are widely used in line-profile analysis.\n\nFor the crystallite size contribution, the Scherrer-type relation provides $ \\beta_{\\text{size}} $ proportional to $ 1 / (D \\cos \\theta) $ with proportionality constant given by the shape factor and wavelength. The factor $ \\cos \\theta $ arises from the projection of coherent domain size onto the scattering vector orientation. For the microstrain contribution, a small fractional change in lattice spacing $ \\Delta d / d $ translates to an angular broadening that scales as $ \\tan \\theta $. When we model microstrain statistically using a root-mean-square value $ \\epsilon $, the integral breadth due to strain is proportional to $ \\epsilon \\tan \\theta $ with a multiplicative constant derived from the relation between full-width and integral breadth and the specific line-profile conventions; the standard Williamson–Hall formulation uses a factor of $ 4 $ for integral breadths.\n\nCombining these contributions and multiplying by $ \\cos \\theta $ yields a linear relation in $ \\sin \\theta $. Define\n$$ y_i \\equiv \\beta_i \\cos \\theta_i, \\quad x_i \\equiv 4 \\sin \\theta_i. $$\nUnder the above assumptions, one obtains\n$$ y_i = b + m x_i, $$\nwith intercept $ b $ equal to $ K \\lambda / D $ and slope $ m $ equal to $ \\epsilon $. This linearization follows from $ \\beta_{\\text{size}} = K \\lambda / (D \\cos \\theta) $ and $ \\beta_{\\text{strain}} = 4 \\epsilon \\tan \\theta $, and thus\n$$ \\beta_i \\cos \\theta_i = \\frac{K \\lambda}{D} + 4 \\epsilon \\sin \\theta_i. $$\nTherefore, ordinary least squares on $ (x_i, y_i) $ with an intercept estimates $ m $ and $ b $, and we recover\n$$ \\epsilon = m, \\quad D = \\frac{K \\lambda}{b}. $$\nThe wavelength $ \\lambda $ must be expressed in nanometers to obtain $ D $ in nanometers; if $ \\lambda $ is provided in Angstroms, we convert via $ \\lambda_{\\text{nm}} = \\lambda_{\\text{\\AA}} / 10 $.\n\nTo incorporate anisotropy in the modified Williamson–Hall method, we weight the strain predictor with a reflection-dependent factor that captures direction sensitivity of the strain fields. A common approach uses the dislocation contrast factor $ C_{hkl} $ (dimensionless), which depends on the crystallographic indices and elastic anisotropy and modulates the effective strain seen by a given reflection. In this framework, we define the modified predictor\n$$ x_i^{\\ast} \\equiv 4 \\sqrt{C_{hkl,i}} \\sin \\theta_i, $$\nand retain the same response $ y_i = \\beta_i \\cos \\theta_i $. The modified linear relation becomes\n$$ y_i = b^{\\ast} + m^{\\ast} x_i^{\\ast}, $$\nwith $ b^{\\ast} = K \\lambda / D $ and $ m^{\\ast} = \\epsilon_{\\text{aniso}} $, the anisotropic microstrain normalized by the contrast factor scaling. Consequently, we perform ordinary least squares using $ x_i^{\\ast} $ to estimate $ \\epsilon_{\\text{aniso}} $ and $ D $. This strategy separates direction-dependent microstrain contributions: reflections with larger $ C_{hkl} $ carry a larger weight in the strain predictor, and the slope $ m^{\\ast} $ corresponds directly to the microstrain parameter after accounting for anisotropy.\n\nAlgorithmic steps implemented in code:\n- Convert provided angles $ \\theta_i $ from degrees to radians: $ \\theta_i^{\\text{rad}} = (\\pi/180) \\times \\theta_i^{\\circ} $.\n- Compute $ y_i = \\beta_i \\cos \\theta_i^{\\text{rad}} $.\n- For isotropic analysis, compute $ x_i = 4 \\sin \\theta_i^{\\text{rad}} $ and solve the ordinary least squares problem for $ y_i = b + m x_i $. Using means,\n$$ m = \\frac{\\sum_i (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_i (x_i - \\bar{x})^2}, \\quad b = \\bar{y} - m \\bar{x}. $$\n- Recover $ \\epsilon = m $ and $ D = K \\lambda_{\\text{nm}} / b $, checking that $ b > 0 $.\n- For anisotropic analysis, compute $ x_i^{\\ast} = 4 \\sqrt{C_{hkl,i}} \\sin \\theta_i^{\\text{rad}} $ and repeat the regression to obtain $ m^{\\ast} $ and $ b^{\\ast} $, then $ \\epsilon_{\\text{aniso}} = m^{\\ast} $ and $ D = K \\lambda_{\\text{nm}} / b^{\\ast} $.\n- For completeness, also fit the isotropic model to the anisotropic dataset to obtain $ \\epsilon_{3,\\text{iso}} $; this quantifies the bias if anisotropy is ignored.\n\nAll outputs are rounded to $ 6 $ decimal places and printed as a single comma-separated list enclosed in square brackets, in the specified order. The approach is scientifically realistic: it starts from additive integral breadths and well-established scaling laws, constructs a linear regression consistent with the Williamson–Hall method, and extends it via reflection-dependent contrast factors to separate anisotropic microstrain within the modified Williamson–Hall framework.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef ordinary_least_squares(x, y):\n    \"\"\"\n    Perform ordinary least squares for y = b + m * x.\n    Returns (m, b).\n    \"\"\"\n    x = np.asarray(x, dtype=float)\n    y = np.asarray(y, dtype=float)\n    x_mean = np.mean(x)\n    y_mean = np.mean(y)\n    # Compute slope and intercept using the covariance/variance method\n    cov = np.sum((x - x_mean) * (y - y_mean))\n    var = np.sum((x - x_mean) ** 2)\n    # Guard against degenerate var (shouldn't occur with diverse theta)\n    if var == 0.0:\n        m = 0.0\n    else:\n        m = cov / var\n    b = y_mean - m * x_mean\n    return m, b\n\ndef radians(degrees):\n    return np.deg2rad(np.asarray(degrees, dtype=float))\n\ndef isotropic_fit(beta, theta_rad, K, lambda_nm):\n    \"\"\"\n    Implements isotropic Williamson-Hall linearization:\n    y = beta * cos(theta), x = 4 * sin(theta)\n    Returns (epsilon, D_nm).\n    \"\"\"\n    theta = np.asarray(theta_rad, dtype=float)\n    beta = np.asarray(beta, dtype=float)\n    y = beta * np.cos(theta)\n    x = 4.0 * np.sin(theta)\n    m, b = ordinary_least_squares(x, y)\n    epsilon = m\n    # Intercept should be positive; if not, set D to np.inf to reflect nonphysical value\n    if b <= 0.0:\n        D_nm = float('inf')\n    else:\n        D_nm = K * lambda_nm / b\n    return epsilon, D_nm\n\ndef anisotropic_fit(beta, theta_rad, contrast_factors, K, lambda_nm):\n    \"\"\"\n    Implements modified Williamson-Hall with contrast factor:\n    y = beta * cos(theta), x* = 4 * sqrt(C_hkl) * sin(theta)\n    Returns (epsilon_aniso, D_nm).\n    \"\"\"\n    theta = np.asarray(theta_rad, dtype=float)\n    beta = np.asarray(beta, dtype=float)\n    C = np.asarray(contrast_factors, dtype=float)\n    y = beta * np.cos(theta)\n    x_star = 4.0 * np.sqrt(C) * np.sin(theta)\n    m_star, b_star = ordinary_least_squares(x_star, y)\n    epsilon_aniso = m_star\n    if b_star <= 0.0:\n        D_nm = float('inf')\n    else:\n        D_nm = K * lambda_nm / b_star\n    return epsilon_aniso, D_nm\n\ndef solve():\n    # Constants\n    lambda_angstrom = 1.5406  # Angstroms\n    lambda_nm = lambda_angstrom / 10.0  # Convert to nanometers\n    K = 0.9\n\n    # Test Case 1 data\n    theta_deg_1 = [15, 20, 25, 30, 35, 40, 45, 50]\n    beta_1 = [0.005015, 0.005863, 0.006790, 0.007822, 0.008986, 0.010333, 0.011922, 0.013848]\n\n    # Test Case 2 data (size-dominated, negligible microstrain)\n    theta_deg_2 = [15, 20, 25, 30, 35, 40, 45, 50]\n    beta_2 = [0.004786, 0.004919, 0.005099, 0.005339, 0.005643, 0.006037, 0.006536, 0.007190]\n\n    # Test Case 3 data (anisotropic)\n    theta_deg_3 = [15, 20, 25, 30, 35, 40, 45, 50]\n    beta_3 = [0.002994, 0.003457, 0.003922, 0.004139, 0.005086, 0.004767, 0.006318, 0.006796]\n    contrast_C_3 = [0.14, 0.21, 0.24, 0.18, 0.29, 0.12, 0.26, 0.20]\n\n    # Convert angles to radians\n    theta_rad_1 = radians(theta_deg_1)\n    theta_rad_2 = radians(theta_deg_2)\n    theta_rad_3 = radians(theta_deg_3)\n\n    # Perform fits\n    eps1, D1 = isotropic_fit(beta_1, theta_rad_1, K, lambda_nm)\n    eps2, D2 = isotropic_fit(beta_2, theta_rad_2, K, lambda_nm)\n    # Isotropic fit on anisotropic data\n    eps3_iso, D3_iso = isotropic_fit(beta_3, theta_rad_3, K, lambda_nm)\n    # Modified Williamson-Hall anisotropic fit\n    eps3_aniso, D3 = anisotropic_fit(beta_3, theta_rad_3, contrast_C_3, K, lambda_nm)\n\n    # Prepare results, rounded to 6 decimals\n    results = [\n        round(eps1, 6), round(D1, 6),\n        round(eps2, 6), round(D2, 6),\n        round(eps3_iso, 6), round(eps3_aniso, 6), round(D3, 6)\n    ]\n\n    # Final print statement in the exact required format.\n    # Ensure no spaces in the comma-separated list.\n    print(f\"[{','.join(map(lambda v: ('inf' if v == float('inf') else f'{v:.6f}'), results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "To gain a fundamental understanding of lattice distortion, we must connect macroscopic properties back to their microscopic origins—the diverse local atomic environments. Static lattice distortion arises directly from the distribution of bond lengths between different pairs of atomic species in a chemically disordered alloy. This exercise () challenges you to construct a first-principles statistical model for this bond-length distribution using atomic radii and a random-alloy assumption, and to compare your model's predictions with experimental data from Pair Distribution Function (PDF) analysis.",
            "id": "3730402",
            "problem": "You are modeling atomic-level lattice distortion in multi-component metallic systems using a probabilistic description of local nearest-neighbor bond lengths. The aim is to quantify how compositional size mismatch contributes to the broadening of the first peak in the Pair Distribution Function (PDF) and to compare your modeled broadening to experimental measurements. You are provided compositions and metallic atomic radii in ångström units, along with crystal structures and experimental first-peak widths. Your task is to compute the deviation of local bond lengths from the average lattice parameter, convert this to a predicted PDF peak width by including thermal and instrumental broadening, and then compare these predictions to the experimental data.\n\nFundamental base assumptions to use:\n- For cubic lattices, the nearest-neighbor distance is related to the lattice parameter through geometry: in a face-centered cubic lattice, the nearest-neighbor separation equals the lattice parameter divided by the square root of two, and in a body-centered cubic lattice, it equals the lattice parameter times the square root of three divided by two. Use these geometric relations to convert between an average nearest-neighbor bond length and an average lattice parameter.\n- Metallic atomic radii provide a baseline estimate of nearest-neighbor separations in random substitutional alloys, and random site occupancy implies that the probability of a bond between two species is proportional to the product of their concentrations, with appropriate normalization over unordered pairs.\n- Independent Gaussian broadening mechanisms add in quadrature at the level of variances. Use this to combine compositional size mismatch broadening with thermal and instrument contributions.\n\nDefinitions and required procedure:\n- The Pair Distribution Function (PDF) is a real-space measure of atomic pair correlations. The first peak corresponds to the nearest-neighbor shell. You must model the distribution of nearest-neighbor bond lengths by considering all unordered species pairs from the given composition, where each species has a metallic atomic radius. For an unordered pair, its bond length is represented by the sum of the radii of the two species.\n- Compute the probability-weighted mean of the bond-length distribution under random occupancy and its standard deviation. Interpret the standard deviation of bond lengths as the static distortion due to compositional size mismatch.\n- Convert the mean bond length to the average lattice parameter using the appropriate cubic lattice geometry. Use this only to establish the reference average lattice parameter; the deviation to be modeled is the bond-length fluctuation relative to this mean.\n- Combine the compositional size-mismatch contribution with fixed thermal and instrument broadening values to obtain a predicted observed width for the first PDF peak. Assume that the thermal broadening and instrument resolution are constant across the test suite and are independent of composition. Use the following constants: thermal broadening standard deviation is $0.040$ ångström, instrument broadening standard deviation is $0.010$ ångström.\n- Compare the predicted observed width to the provided experimental width for each test case, and compute the difference (predicted minus experimental).\n\nPhysical units and output requirements:\n- All lengths and widths must be treated and reported in ångström (Å). Your program must produce differences in ångström.\n- Your program must output a single line containing a comma-separated list enclosed in square brackets that aggregates the differences for all provided test cases as decimal floats. For example: $[x_1,x_2,x_3,x_4]$, where each $x_i$ is the difference in ångström for test case $i$.\n- Express numerical values as decimal floats; do not use a percentage sign.\n\nTest suite:\nUse the following metallic atomic radii (in ångström):\n- Chromium: $1.28$\n- Manganese: $1.27$\n- Iron: $1.26$\n- Cobalt: $1.25$\n- Nickel: $1.24$\n- Aluminum: $1.43$\n- Copper: $1.28$\n- Vanadium: $1.34$\n- Niobium: $1.43$\n- Tantalum: $1.43$\n- Tungsten: $1.37$\n\nUse the following test cases, each specified by composition, crystal structure, and experimental first-peak width:\n- Test case $1$ (happy path, multi-component face-centered cubic alloy): composition $\\{\\text{Cr}:0.20,\\text{Mn}:0.20,\\text{Fe}:0.20,\\text{Co}:0.20,\\text{Ni}:0.20\\}$, structure fcc, experimental width $0.055$ ångström.\n- Test case $2$ (binary face-centered cubic alloy with large atomic size mismatch): composition $\\{\\text{Ni}:0.50,\\text{Al}:0.50\\}$, structure fcc, experimental width $0.130$ ångström.\n- Test case $3$ (boundary case, pure element): composition $\\{\\text{Cu}:1.00\\}$, structure fcc, experimental width $0.045$ ångström.\n- Test case $4$ (multi-component body-centered cubic refractory alloy): composition $\\{\\text{V}:0.25,\\text{Nb}:0.25,\\text{Ta}:0.25,\\text{W}:0.25\\}$, structure bcc, experimental width $0.085$ ångström.\n\nYour program must:\n- Construct the nearest-neighbor bond-length distribution for each test case as outlined above under random occupancy.\n- Compute the probability-weighted mean and standard deviation of bond lengths, convert the mean to the lattice parameter via cubic geometry, and use the standard deviation as the static distortion magnitude.\n- Combine the static distortion with the fixed thermal and instrument contributions by adding variances to produce a predicted observed width.\n- Compute the difference between the predicted observed width and the provided experimental width for each test case as a float number in ångström.\n- Produce a single line of output containing the list of differences with six decimal places precision, comma-separated, enclosed in square brackets, in the exact order of test cases $1$ through $4$, for example: $[d_1,d_2,d_3,d_4]$.\n\nIn addition to numerical computation, consider in your reasoning that Density Functional Theory (DFT) predictions of relaxed local bond lengths can depend on the choice of Exchange-Correlation (XC) functional, which can alter the expected static distortion. Your program will not perform electronic structure calculations, but your solution should discuss how XC functional choice could lead to disagreement between modeled and experimental widths, even when the compositional size mismatch is computed correctly.",
            "solution": "The problem statement has been critically reviewed and is deemed valid. It is scientifically grounded, well-posed, objective, and internally consistent, providing all necessary data and a clear procedure for calculation.\n\nThe objective is to model the broadening of the first peak in the Pair Distribution Function (PDF) for several multi-component alloys. This broadening arises from three primary sources: static lattice distortion due to atomic size mismatch, thermal vibrations, and instrumental resolution limitations. The problem requires a quantitative comparison between the total predicted broadening and provided experimental measurements.\n\nThe solution proceeds by first constructing a probabilistic model for the distribution of nearest-neighbor bond lengths, based on the assumption of a random solid solution. From this distribution, the variance due to compositional size mismatch is computed. This variance is then combined with the given thermal and instrumental variances to yield a total predicted variance, and its square root gives the predicted peak width.\n\nLet the alloy consist of $N$ elemental species, indexed by $i = 1, \\dots, N$. Each species $i$ is characterized by its atomic concentration $c_i$ and its metallic atomic radius $r_i$. The set of concentrations must satisfy the condition $\\sum_{i=1}^{N} c_i = 1$.\n\nThe model defines the bond length $d_{ij}$ for a nearest-neighbor pair of atoms of species $i$ and $j$ as the sum of their metallic radii:\n$$d_{ij} = r_i + r_j$$\nThis is a first-order approximation that treats atoms as hard spheres.\n\nIn a random alloy, the probability of finding an atom of species $i$ at any given lattice site is $c_i$. Consequently, the probability of finding a specific ordered pair of atoms, $i$ followed by $j$, is $c_i c_j$. The model considers unordered pairs, which represent a single physical bond. The probability $P_{ij}$ of a bond being between species $i$ and $j$ is given by:\n$$\nP_{ij} =\n\\begin{cases}\n    c_i^2      & \\text{if } i = j \\\\\n    2 c_i c_j  & \\text{if } i \\neq j\n\\end{cases}\n$$\nThe sum of these probabilities over all unique unordered pairs $(i, j)$ with $i \\le j$ is $(\\sum_i c_i)^2 = 1$, confirming the normalization.\n\nThe mean bond length, $\\bar{d}$, is the expectation value $E[d]$ of this distribution:\n$$ \\bar{d} = E[d] = \\sum_{i} \\sum_{j} c_i c_j d_{ij} = \\sum_{i} \\sum_{j} c_i c_j (r_i + r_j) $$\nThis expression simplifies to twice the concentration-weighted average atomic radius:\n$$ \\bar{d} = 2 \\sum_{i=1}^{N} c_i r_i $$\nThis mean bond length $\\bar{d}$ can be related to an average lattice parameter $\\bar{a}$ using the geometric relations for cubic lattices provided: $\\bar{d} = \\bar{a} / \\sqrt{2}$ for face-centered cubic (fcc) and $\\bar{d} = \\bar{a} \\sqrt{3} / 2$ for body-centered cubic (bcc). While this establishes the average structure, the key quantity for broadening is the variance.\n\nThe standard deviation of this bond-length distribution, $\\sigma_{dist}$, quantifies the static lattice distortion due to atomic size mismatch. It is derived from the variance, $\\sigma_{dist}^2 = E[d^2] - (E[d])^2$. The term $E[d^2]$ is calculated as:\n$$ E[d^2] = \\sum_{i} \\sum_{j} c_i c_j d_{ij}^2 = \\sum_{i} \\sum_{j} c_i c_j (r_i + r_j)^2 $$\nTherefore, the variance from static distortion is:\n$$ \\sigma_{dist}^2 = \\left( \\sum_{i} \\sum_{j} c_i c_j (r_i + r_j)^2 \\right) - \\bar{d}^2 $$\nFor a pure element, all radii are identical, so all $d_{ij}$ are the same. The distribution has only one value, and thus $\\sigma_{dist}^2 = 0$, as expected.\n\nThe problem states that independent Gaussian broadening contributions add in quadrature. This means their variances are additive. The total predicted variance of the PDF peak, $\\sigma_{pred}^2$, is the sum of the variances from static distortion ($\\sigma_{dist}^2$), thermal effects ($\\sigma_{therm}^2$), and instrumental resolution ($\\sigma_{inst}^2$):\n$$ \\sigma_{pred}^2 = \\sigma_{dist}^2 + \\sigma_{therm}^2 + \\sigma_{inst}^2 $$\nThe given constant values are $\\sigma_{therm} = 0.040$ Å and $\\sigma_{inst} = 0.010$ Å.\nThe predicted observed width of the PDF peak is the standard deviation:\n$$ \\sigma_{pred} = \\sqrt{\\sigma_{dist}^2 + \\sigma_{therm}^2 + \\sigma_{inst}^2} $$\nFinally, for each test case, we compute the difference $\\Delta$ between the predicted width and the experimental width $\\sigma_{exp}$:\n$$ \\Delta = \\sigma_{pred} - \\sigma_{exp} $$\n\nThe discrepancy between this model's prediction and experimental reality can be significant. The model's assumption that $d_{ij} = r_i + r_j$ neglects local electronic and structural relaxations. Atoms do not behave as rigid spheres; their bond lengths adjust to minimize the total energy of the system, influenced by the local chemical environment and charge transfer effects. More sophisticated methods like Density Functional Theory (DFT) can capture these relaxations. However, DFT calculations themselves depend on the choice of the exchange-correlation (XC) functional (e.g., Local Density Approximation (LDA) vs. Generalized Gradient Approximation (GGA)). Different XC functionals can yield different equilibrium bond lengths and, consequently, different predictions for the magnitude of static lattice distortion ($\\sigma_{dist}$). Therefore, any disagreement between a theoretical model (be it the simple one used here or a complex DFT one) and experimental data can stem from both the inherent approximations of the model and the specific parameters chosen within that model (like the XC functional in DFT).\n\nThe procedure will now be applied to each test case. All length units are in ångström (Å).\n\n**Test Case 1**: $\\{\\text{Cr}:0.20,\\text{Mn}:0.20,\\text{Fe}:0.20,\\text{Co}:0.20,\\text{Ni}:0.20\\}$, structure fcc, $\\sigma_{exp} = 0.055$ Å.\nRadii: $r_{Cr}=1.28$, $r_{Mn}=1.27$, $r_{Fe}=1.26$, $r_{Co}=1.25$, $r_{Ni}=1.24$.\n$\\sigma_{dist}^2 = 0.0004$ Å$^2$.\n$\\sigma_{pred}^2 = 0.0004 + (0.040)^2 + (0.010)^2 = 0.0004 + 0.0016 + 0.0001 = 0.0021$ Å$^2$.\n$\\sigma_{pred} = \\sqrt{0.0021} \\approx 0.045826$ Å.\n$\\Delta_1 = 0.045826 - 0.055 = -0.009174$ Å.\n\n**Test Case 2**: $\\{\\text{Ni}:0.50,\\text{Al}:0.50\\}$, structure fcc, $\\sigma_{exp} = 0.130$ Å.\nRadii: $r_{Ni}=1.24$, $r_{Al}=1.43$.\n$\\sigma_{dist}^2 \\approx 0.01805$ Å$^2$.\n$\\sigma_{pred}^2 = 0.01805 + 0.0016 + 0.0001 = 0.01975$ Å$^2$.\n$\\sigma_{pred} = \\sqrt{0.01975} \\approx 0.140535$ Å.\n$\\Delta_2 = 0.140535 - 0.130 = 0.010535$ Å.\n\n**Test Case 3**: $\\{\\text{Cu}:1.00\\}$, structure fcc, $\\sigma_{exp} = 0.045$ Å.\nRadii: $r_{Cu}=1.28$.\nSince this is a pure element, there is no compositional size mismatch, so $\\sigma_{dist}^2 = 0$.\n$\\sigma_{pred}^2 = 0 + (0.040)^2 + (0.010)^2 = 0.0016 + 0.0001 = 0.0017$ Å$^2$.\n$\\sigma_{pred} = \\sqrt{0.0017} \\approx 0.041231$ Å.\n$\\Delta_3 = 0.041231 - 0.045 = -0.003769$ Å.\n\n**Test Case 4**: $\\{\\text{V}:0.25,\\text{Nb}:0.25,\\text{Ta}:0.25,\\text{W}:0.25\\}$, structure bcc, $\\sigma_{exp} = 0.085$ Å.\nRadii: $r_{V}=1.34$, $r_{Nb}=1.43$, $r_{Ta}=1.43$, $r_{W}=1.37$.\n$\\sigma_{dist}^2 \\approx 0.0021375$ Å$^2$.\n$\\sigma_{pred}^2 = 0.0021375 + 0.0016 + 0.0001 = 0.0038375$ Å$^2$.\n$\\sigma_{pred} = \\sqrt{0.0038375} \\approx 0.061948$ Å.\n$\\Delta_4 = 0.061948 - 0.085 = -0.023052$ Å.\n\nThese differences will be computed and formatted by the accompanying program.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the difference between predicted and experimental PDF first-peak widths\n    for several multi-component alloys, based on a model of lattice distortion.\n    \"\"\"\n\n    # Define atomic radii and fixed broadening contributions in ångström.\n    atomic_radii = {\n        'Cr': 1.28, 'Mn': 1.27, 'Fe': 1.26, 'Co': 1.25, 'Ni': 1.24,\n        'Al': 1.43, 'Cu': 1.28, 'V': 1.34, 'Nb': 1.43, 'Ta': 1.43,\n        'W': 1.37\n    }\n    sigma_therm = 0.040  # Thermal broadening standard deviation\n    sigma_inst = 0.010   # Instrument broadening standard deviation\n\n    # Test cases: (composition dictionary, experimental width in ångström)\n    # The crystal structure is for context and not used in the width calculation.\n    test_cases = [\n        ({'Cr': 0.20, 'Mn': 0.20, 'Fe': 0.20, 'Co': 0.20, 'Ni': 0.20}, 0.055),\n        ({'Ni': 0.50, 'Al': 0.50}, 0.130),\n        ({'Cu': 1.00}, 0.045),\n        ({'V': 0.25, 'Nb': 0.25, 'Ta': 0.25, 'W': 0.25}, 0.085)\n    ]\n\n    differences = []\n\n    for composition, sigma_exp in test_cases:\n        elements = list(composition.keys())\n        concentrations = np.array(list(composition.values()))\n        radii = np.array([atomic_radii[el] for el in elements])\n        num_species = len(elements)\n\n        # Step 1: Calculate the mean bond length (d_bar)\n        # d_bar = E[d] = 2 * sum(c_i * r_i)\n        d_bar = 2 * np.dot(concentrations, radii)\n\n        # Step 2: Calculate the variance due to static distortion (sigma_dist^2)\n        # sigma_dist^2 = E[d^2] - (E[d])^2\n        # E[d^2] = sum_i sum_j c_i * c_j * (r_i + r_j)^2\n        e_d_squared = 0.0\n        for i in range(num_species):\n            for j in range(num_species):\n                e_d_squared += concentrations[i] * concentrations[j] * (radii[i] + radii[j])**2\n        \n        sigma_dist_sq = e_d_squared - d_bar**2\n\n        # Step 3: Combine variances to get the predicted total variance\n        # Variances add in quadrature.\n        sigma_pred_sq = sigma_dist_sq + sigma_therm**2 + sigma_inst**2\n\n        # Step 4: Calculate the predicted total width (standard deviation)\n        sigma_pred = np.sqrt(sigma_pred_sq)\n\n        # Step 5: Compute the difference between predicted and experimental widths\n        difference = sigma_pred - sigma_exp\n        differences.append(difference)\n\n    # Final print statement in the exact required format with 6 decimal places.\n    formatted_differences = [f\"{d:.6f}\" for d in differences]\n    print(f\"[{','.join(formatted_differences)}]\")\n\nsolve()\n```"
        }
    ]
}