{
    "hands_on_practices": [
        {
            "introduction": "在开发高熵合金（HEA）的机器学习预测模型时，准确评估其性能至关重要。本练习超越了简单计算误差指标的范畴，引导我们深入探讨在更真实的异方差噪声（即测量误差随属性值大小变化）情境下，如何解读平均绝对误差（MAE）、均方根误差（RMSE）和决定系数（$R^2$）之间的差异。通过这个实践，你将学会如何从模型的残差分布中洞察其在不同预测范围内的性能表现。",
            "id": "3750198",
            "problem": "在预测高熵合金（HEAs）机械性能的背景下，考虑一个监督学习模型，该模型根据成分和热力学描述符进行训练，以预测屈服强度。在一个包含8种等原子比高熵合金的独立测试集上，测量的屈服强度 $y_i$ 和模型预测值 $\\hat{y}_i$（单位均为吉帕斯卡 (GPa)）如下：\n\n$$(y_1, y_2, y_3, y_4, y_5, y_6, y_7, y_8) = (0.85, 1.10, 1.40, 1.80, 2.20, 2.60, 3.20, 3.80),$$\n$$(\\hat{y}_1, \\hat{y}_2, \\hat{y}_3, \\hat{y}_4, \\hat{y}_5, \\hat{y}_6, \\hat{y}_7, \\hat{y}_8) = (0.87, 1.07, 1.45, 1.72, 2.30, 2.48, 3.38, 3.55).$$\n\n测量过程是异方差的：其噪声标准差随数值大小而增加，对于每种合金 $i$，其唯象估计为 $\\sigma_i = 0.04\\, y_i + 0.02$ GPa。仅使用上述测试集数值以及回归分析中平均绝对误差（MAE）、均方根误差（RMSE）和决定系数 $R^2$ 的标准定义，计算该模型的这三个指标。\n\n然后，根据异方差噪声下残差的量值分布，解释MAE和RMSE之间的任何差异，并阐明当残差方差随 $y_i$ 增加时，$R^2$ 的值如何与预测所解释的方差分数相关。你的解释应基于统计误差和方差的核心定义，不使用除这些定义之外的任何快捷公式。\n\n以吉帕斯卡（GPa）为单位报告MAE和RMSE的数值，并将 $R^2$ 报告为纯数（无量纲）。将所有三个数值四舍五入到四位有效数字。以 $(\\text{MAE}, \\text{RMSE}, R^2)$ 的顺序，将最终答案表示为单行矩阵。",
            "solution": "该问题陈述具有科学依据，提法恰当，并包含获得唯一解所需的所有信息。它要求计算标准回归指标，并基于基本统计原理解释。该问题是有效的。\n\n解题过程分两部分：首先，计算平均绝对误差（MAE）、均方根误差（RMSE）和决定系数（$R^2$）；其次，在所提供数据和指定的异方差噪声模型的背景下解释这些指标。\n\n测试集中的数据点数量为 $N=8$。测量的屈服强度由向量 $y$ 给出，模型预测值由向量 $\\hat{y}$ 给出。\n$$y = (0.85, 1.10, 1.40, 1.80, 2.20, 2.60, 3.20, 3.80)$$\n$$\\hat{y} = (0.87, 1.07, 1.45, 1.72, 2.30, 2.48, 3.38, 3.55)$$\n所有值的单位都是吉帕斯卡（GPa）。\n\n首先，我们计算残差向量 $e$，其中每个元素为 $e_i = y_i - \\hat{y}_i$。\n$$e_1 = 0.85 - 0.87 = -0.02$$\n$$e_2 = 1.10 - 1.07 = 0.03$$\n$$e_3 = 1.40 - 1.45 = -0.05$$\n$$e_4 = 1.80 - 1.72 = 0.08$$\n$$e_5 = 2.20 - 2.30 = -0.10$$\n$$e_6 = 2.60 - 2.48 = 0.12$$\n$$e_7 = 3.20 - 3.38 = -0.18$$\n$$e_8 = 3.80 - 3.55 = 0.25$$\n\n残差向量为 $e = (-0.02, 0.03, -0.05, 0.08, -0.10, 0.12, -0.18, 0.25)$。\n\n**计算平均绝对误差（MAE）**\nMAE定义为残差绝对值的平均值：\n$$\\text{MAE} = \\frac{1}{N} \\sum_{i=1}^{N} |y_i - \\hat{y}_i|$$\n我们对残差向量 $e$ 中各元素的绝对值求和：\n$$\\sum_{i=1}^{8} |e_i| = |-0.02| + |0.03| + |-0.05| + |0.08| + |-0.10| + |0.12| + |-0.18| + |0.25|$$\n$$\\sum_{i=1}^{8} |e_i| = 0.02 + 0.03 + 0.05 + 0.08 + 0.10 + 0.12 + 0.18 + 0.25 = 0.83$$\n$$\\text{MAE} = \\frac{0.83}{8} = 0.10375$$\n四舍五入到四位有效数字，$\\text{MAE} \\approx 0.1038$ GPa。\n\n**计算均方根误差（RMSE）**\nRMSE定义为残差平方的均值的平方根：\n$$\\text{RMSE} = \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2}$$\n首先，我们计算残差平方和 $\\text{SS}_{\\text{res}}$：\n$$\\text{SS}_{\\text{res}} = \\sum_{i=1}^{8} e_i^2 = (-0.02)^2 + (0.03)^2 + (-0.05)^2 + (0.08)^2 + (-0.10)^2 + (0.12)^2 + (-0.18)^2 + (0.25)^2$$\n$$\\text{SS}_{\\text{res}} = 0.0004 + 0.0009 + 0.0025 + 0.0064 + 0.0100 + 0.0144 + 0.0324 + 0.0625 = 0.1295$$\n均方误差（MSE）为 $\\frac{\\text{SS}_{\\text{res}}}{N} = \\frac{0.1295}{8} = 0.0161875$。\n$$\\text{RMSE} = \\sqrt{0.0161875} \\approx 0.1272299$$\n四舍五入到四位有效数字，$\\text{RMSE} \\approx 0.1272$ GPa。\n\n**计算决定系数（$R^2$）**\n$R^2$ 值定义为：\n$$R^2 = 1 - \\frac{\\text{SS}_{\\text{res}}}{\\text{SS}_{\\text{tot}}}$$\n其中 $\\text{SS}_{\\text{res}} = \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2$ 是残差平方和（上文计算为 $0.1295$），而 $\\text{SS}_{\\text{tot}} = \\sum_{i=1}^{N} (y_i - \\bar{y})^2$ 是总平方和。首先，我们必须计算测量值的平均值 $\\bar{y}$。\n$$\\bar{y} = \\frac{1}{N} \\sum_{i=1}^{N} y_i = \\frac{0.85 + 1.10 + 1.40 + 1.80 + 2.20 + 2.60 + 3.20 + 3.80}{8} = \\frac{16.95}{8} = 2.11875$$\n接下来，我们计算 $\\text{SS}_{\\text{tot}}$：\n$$\\text{SS}_{\\text{tot}} = (0.85 - 2.11875)^2 + (1.10 - 2.11875)^2 + \\dots + (3.80 - 2.11875)^2$$\n$$\\text{SS}_{\\text{tot}} \\approx 1.609766 + 1.037852 + 0.516602 + 0.101602 + 0.006602 + 0.231602 + 1.169102 + 2.826602 = 7.49975$$\n现在我们可以计算 $R^2$：\n$$R^2 = 1 - \\frac{0.1295}{7.49975} \\approx 1 - 0.01726727 = 0.98273273$$\n四舍五入到四位有效数字，$R^2 \\approx 0.9827$。\n\n**指标解释**\n\n*   **MAE与RMSE之间的差异：** 我们观察到 $\\text{RMSE} \\approx 0.1272$ 大于 $\\text{MAE} \\approx 0.1038$。这是一个普遍性质，因为RMSE计算中的平方运算会给较大的残差不成比例的更大权重。而MAE对所有误差进行线性处理。在这个数据集中，与较大屈服强度相关的残差（$e_7 = -0.18$ 和 $e_8 = 0.25$）在量值上显著大于其他残差。这几个大的误差夸大了平方和，导致RMSE明显大于MAE。这表明残差分布不均匀，特别是存在一些“离群”或大量值的误差。这一观察结果与所提供的异方差噪声模型 $\\sigma_i = 0.04\\, y_i + 0.02$ 一致，该模型预测误差大小应随测量值 $y_i$ 的增加而增加。较大的残差确实出现在具有较高屈服强度的合金上。\n\n*   **异方差性下的 $R^2$ 解释：** 决定系数 $R^2 \\approx 0.9827$ 表明，测量的屈服强度（$y$）的总方差中约有 $98.27\\%$ 可由模型的预测值（$\\hat{y}$）解释。虽然这是一个非常高的值，表明拟合效果极好，但由于误差的异方差性质，对其解释必须更加细致。基本定义 $R^2 = 1 - \\text{SS}_{\\text{res}}/\\text{SS}_{\\text{tot}}$ 在求和 $\\text{SS}_{\\text{res}}$ 时对所有平方残差同等加权。然而，由于 $y_i$ 值较大时残差也较大，因此对 $\\text{SS}_{\\text{res}}$ 的贡献主要来自高强度合金的误差。因此，$R^2$ 指标对模型在高强度合金上的性能更敏感，而对在低强度合金上的性能不那么敏感。一个高的 $R^2$ 值可能掩盖在低方差区域的较差相对性能，或者像本例中一样，表明总体拟合良好，但这种良好性正因数据范围上端的误差增加而降低。$R^2$ 不再是衡量 $y$ 值整个范围内拟合优度的无偏度量；它变成了一个由数据点的局部方差加权的度量。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.1038  0.1272  0.9827\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "物理定律不应依赖于我们描述系统时任意选择的标签顺序。对于合金而言，这意味着其预测属性不应随其组分元素的排列顺序而改变——这就是置换不变性。本练习将引导你构建并测试一个满足此基本对称性要求的模型，并与一个不满足该要求的基线模型进行对比，从而加深对如何设计具有物理意义的机器学习模型架构的理解。",
            "id": "3750170",
            "problem": "您的任务是为高熵合金（HEA）性能的预测模型设计并实现一个成分不变性测试框架。高熵合金（HEA）由多种元素以近等原子比混合而成；在对此类系统进行建模时，一个核心的物理要求是，一种成分的预测属性必须与其组成元素的任意排序无关。用机器学习的术语来说，从元素描述符及其分数的多重集到标量属性的映射，应该在组分索引的排列下保持不变。您的程序必须是一个完整的、可运行的实现，它能够构建两个预测器，建立一个排列不变性测试框架，并报告针对指定测试套件的自动化单元测试结果。\n\n基本原理和物理真实性：对于理想混合物行为和材料科学中使用的许多混合法则，目标宏观属性是通过对特定组分描述符的聚合统计量来建模的。一个典型的例子是混合法则，其一种形式表明，属性 $P$ 由加权和 $P = \\sum_{i=1}^{n} w_i P_i$ 给出，其中 $w_i$ 是一个具有物理意义的权重（如摩尔分数），$P_i$ 是一个特定于组分的量。这种聚合是对称的：它只取决于组分及其权重的集合，而不取决于它们的枚举顺序。在与这些定律一致的学习系统中，排列不变性可以通过对称操作（例如对每个组分的嵌入向量求和）在结构上被强制执行。\n\n您的任务：\n1. 实现两个标量输出的预测器，它们以元素描述符和分数定义的成分作为输入：\n   - 一个基于对称聚合器的顺序不变预测器，该聚合器与理想混合物成分一致。此模型必须使用一个共享的逐元素嵌入，然后对元素进行对称规约，最后进行读出。具体来说，使用共享的逐元素变换和按元素分数加权的元素求和来获得中间表示，然后通过线性读出产生标量属性。此设计不得依赖于元素的位置索引。\n   - 一个有意设计为顺序敏感的基线预测器，它使用位置特定的参数来形成每个元素贡献的线性组合；该模型必须依赖于索引位置，因此对于多元素输入，它应该无法通过不变性检查。\n\n2. 构建一个测试框架，对于给定的成分，该框架会枚举组分索引的所有排列，并检查预测器输出在所有排列中是否在指定容差范围内保持不变。测试框架必须为每种成分报告一个布尔值，指示排列不变性测试是否通过。\n\n3. 使用以下数值指定的参数和测试套件。所有描述符都是三维向量，代表物理上合理的特征：Pauling 电负性（无量纲）、共价半径（单位为埃）和类整数的价电子数。分数是表示为小数的摩尔分数；它们的总和必须为 $1$。\n   - 不变模型的共享逐元素嵌入参数（对每个元素使用相同的 $\\phi$）：\n     - 权重矩阵 $W_{\\phi}$:\n       $$\n       W_{\\phi} = \\begin{bmatrix}\n       0.5  -0.2  0.1 \\\\\n       0.3  0.4  -0.1 \\\\\n       -0.25  0.15  0.6\n       \\end{bmatrix}\n       $$\n     - 偏置向量 $b_{\\phi}$: $[0.05, -0.03, 0.02]$.\n     - 读出权重 $w_{\\rho}$: $[1.2, -0.7, 0.5]$.\n     - 读出偏置 $c_{\\rho}$: $-0.1$.\n   - 顺序敏感基线模型的位置特定权重向量（对于 $n$ 元素成分，按其出现的顺序使用前 $n$ 个向量）：\n     - $b_1 = [0.1, 0.2, 0.3]$, $b_2 = [-0.2, 0.1, 0.4]$, $b_3 = [0.3, -0.1, 0.2]$, $b_4 = [0.4, 0.5, -0.3]$.\n     - 基线偏置 $c_0 = 0.0$.\n   - 成分测试套件（每个案例是一对分数和描述符矩阵；行对应于元素）：\n     1. 案例 A（三个元素，典型的类 HEA 成分）：\n        - 分数：$[0.40, 0.35, 0.25]$。\n        - 描述符：$\\big[ [1.83, 1.26, 8.00], [1.91, 1.24, 10.00], [1.66, 1.28, 6.00] \\big]$。\n     2. 案例 B（单元素边界情况）：\n        - 分数：$[1.00]$。\n        - 描述符：$\\big[ [1.61, 1.43, 3.00] \\big]$。\n     3. 案例 C（四个元素，微小和零分数的边缘情况）：\n        - 分数：$[0.001, 0.499, 0.500, 0.000]$。\n        - 描述符：$\\big[ [1.63, 1.34, 5.00], [1.88, 1.25, 9.00], [1.54, 1.47, 4.00], [2.16, 1.39, 6.00] \\big]$。\n     4. 案例 D（三个元素，相等分数且有重复描述符）：\n        - 分数：$[\\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3}]$。\n        - 描述符：$\\big[ [1.55, 1.27, 7.00], [1.55, 1.27, 7.00], [1.90, 1.28, 11.00] \\big]$。\n   - 不变性检查的容差：$\\varepsilon = 10^{-12}$。\n\n4. 程序逻辑：\n   - 使用共享的元素映射 $\\phi$，然后按分数进行加权对称求和，最后进行线性读出，来定义不变预测器。输出必须是单个标量。\n   - 对于一个 $n$ 元素输入，按给定顺序使用前 $n$ 个位置特定的权重 $b_i$ 来定义非不变基线预测器；输出必须是单个标量。\n   - 实现一个函数，对于每个测试案例，检查元素索引的所有排列下的预测器输出，并返回一个布尔值，指示所有输出是否在 $\\varepsilon$ 容差内相等。\n\n5. 最终输出格式：\n   - 对于每个测试案例，生成一对布尔值 $[b_{\\text{inv}}, b_{\\text{noninv}}]$，其中 $b_{\\text{inv}}$ 表示不变模型是否通过了排列不变性测试，$b_{\\text{noninv}}$ 表示基线模型是否通过了排列不变性测试。\n   - 您的程序应该生成一行输出，其中包含所有测试案例的结果，结果是一个用方括号括起来的逗号分隔列表，列表中的每个元素是对应一个测试案例的结果对。例如：$[[\\text{True},\\text{False}],[\\text{True},\\text{True}],\\dots]$。\n\n角度单位不适用。中间描述符的物理单位如前所述，但最终输出是布尔值，因此无单位。测试套件涵盖了典型、边界和边缘情况，包括多元素成分、单元素成分、微小和零分数，以及在相等分数下的重复描述符，以探究排列不变性的不同方面。",
            "solution": "我们首先将设计植根于物理驱动的对称性和聚合原则。混合法则提供了一个典型的例子，其中宏观属性 $P$ 由总和 $P = \\sum_{i=1}^{n} w_i P_i$ 建模，其中 $w_i$ 表示物理权重（如摩尔分数），$P_i$ 表示特定于组分的量。该表达式在索引集 $\\{1,2,\\dots,n\\}$ 的排列下是不变的，因为加法满足交换律和结合律。在用于成分的学习系统中，一种尊重这种对称性的结构是 Deep Sets 公式，其中多重集 $\\{x_i\\}_{i=1}^n$ 上的函数被建模为 $f(\\{x_i\\}) = \\rho\\left(\\sum_{i=1}^n \\phi(x_i)\\right)$，其中 $\\phi$ 和 $\\rho$ 是合适的函数。对于包含分数 $x_i$（这里，$x_i$ 表示摩尔分数，不要与描述符 $d_i$ 混淆）的 HEA 成分，一个一致的推广是 $f(\\{(x_i,d_i)\\}) = \\rho\\left(\\sum_{i=1}^n x_i \\, \\phi(d_i)\\right)$。\n\n排列不变性推导：设 $\\pi$ 为 $\\{1,\\dots,n\\}$ 的任意排列。考虑不变模型\n$$\ny_{\\text{inv}} = \\rho\\left(\\sum_{i=1}^n x_i \\, \\phi(d_i)\\right),\n$$\n其中 $\\phi$ 对每个元素描述符 $d_i$ 的应用是相同的。在排列下，输入序列 $(x_i, d_i)$ 变为 $(x_{\\pi(i)}, d_{\\pi(i)})$；对称聚合器产生\n$$\n\\sum_{i=1}^n x_{\\pi(i)} \\, \\phi(d_{\\pi(i)}) = \\sum_{j=1}^n x_j \\, \\phi(d_j),\n$$\n这是在进行索引变换 $j = \\pi(i)$ 之后得到的结果，因为 $\\pi$ 是一个双射。因此，$y_{\\text{inv}}$ 在任何 $\\pi$ 下都是相同的。\n\n基线模型的顺序敏感性推导：设基线预测器使用位置特定的参数 $b_i$ 来形成一个标量\n$$\ny_{\\text{base}} = c_0 + \\sum_{i=1}^n x_i \\, \\langle b_i, d_i \\rangle,\n$$\n其中 $\\langle \\cdot, \\cdot \\rangle$ 表示 $\\mathbb{R}^3$ 中的欧几里得内积。在排列 $\\pi$ 下，这变为\n$$\ny_{\\text{base}}^{(\\pi)} = c_0 + \\sum_{i=1}^n x_{\\pi(i)} \\, \\langle b_i, d_{\\pi(i)} \\rangle.\n$$\n除非所有的 $b_i$ 都相同，或者对于特定的描述符和分数发生了特殊的抵消，$y_{\\text{base}}^{(\\pi)} \\neq y_{\\text{base}}$。因此，该预测器通常是顺序敏感的，并且对于多元素输入，它将无法通过排列不变性测试。\n\n算法设计：\n- 实现 $\\phi(d) = W_{\\phi} d + b_{\\phi}$，其中 $W_{\\phi} \\in \\mathbb{R}^{3 \\times 3}$ 和 $b_{\\phi} \\in \\mathbb{R}^3$ 如给定。这是一个应用于每个元素描述符 $d_i$ 的共享仿射映射。\n- 实现 $\\rho(v) = \\langle w_{\\rho}, v \\rangle + c_{\\rho}$，其中 $w_{\\rho} \\in \\mathbb{R}^3$ 和 $c_{\\rho} \\in \\mathbb{R}$ 如给定。\n- 不变预测器计算 $v = \\sum_{i=1}^n x_i \\, \\phi(d_i)$，然后计算 $y_{\\text{inv}} = \\rho(v)$。\n- 基线预测器使用当前顺序中的前 $n$ 个位置特定的向量 $b_i$ 来计算 $y_{\\text{base}} = c_0 + \\sum_{i=1}^n x_i \\, \\langle b_i, d_i \\rangle$。\n- 不变性测试框架：对于每个具有分数 $(x_1,\\dots,x_n)$ 和描述符 $(d_1,\\dots,d_n)$ 的成分，枚举 $\\{1,\\dots,n\\}$ 的所有排列 $\\pi$，计算每个排列安排下的模型输出，并检查所有输出是否在容差 $\\varepsilon = 10^{-12}$ 内与参考输出相等。形式上，对于输出 $y^{(\\pi)}$，计算 $\\max_{\\pi} \\left| y^{(\\pi)} - y^{(\\text{id})} \\right|$，如果该最大值小于或等于 $\\varepsilon$，则声明测试通过。\n\n测试套件覆盖范围：\n- 案例 A 探究了一个典型的多元素类 HEA 成分，具有现实的特征和异构的分数；不变模型应通过，基线模型应失败。\n- 案例 B 是一个单元素边界情况；两个模型都应通过，因为只有一个排列。\n- 案例 C 包括微小和零分数，以检验数值稳定性并确保分数为零的元素没有影响；不变模型应通过，而基线模型应失败，除非在所有排列中出现巧合的相等，这极不可能。\n- 案例 D 包括相等分数和重复描述符，以探究即使在交换相同元素时不变性是否仍然成立；不变模型应通过，但基线模型仍应失败，因为位置特定的权重产生了在所有排列中都不会消失的依赖性。\n\n数值稳定性：对浮点值求和可能会引入微小的、与顺序相关的舍入差异。所选的容差 $\\varepsilon = 10^{-12}$ 远大于双精度浮点数的机器精度，同时又足够严格，可以捕捉到真正的顺序依赖性。不变模型在结构上是对称的；任何排列间的差异最多应为舍入误差，因此低于 $\\varepsilon$。基线模型在多元素情况下预计会超过 $\\varepsilon$。\n\n最终输出：对于每个案例，生成 $[b_{\\text{inv}}, b_{\\text{noninv}}]$，并将所有案例的列表打印在单行上：$[[b_{\\text{inv}}^{A}, b_{\\text{noninv}}^{A}], [b_{\\text{inv}}^{B}, b_{\\text{noninv}}^{B}], [b_{\\text{inv}}^{C}, b_{\\text{noninv}}^{C}], [b_{\\text{inv}}^{D}, b_{\\text{noninv}}^{D}]]$。\n\n该设计将基础物理对称性与算法检查相结合，以在 HEA 属性预测模型中强制执行和验证排列不变性，这符合复杂材料建模中对机器学习模型正确性和鲁棒性的高等研究生水平的期望。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport itertools\n\ndef deep_sets_predict(fractions, descriptors):\n    \"\"\"\n    Order-invariant predictor using shared element-wise embedding phi and\n    symmetric aggregation followed by a linear readout rho.\n    \"\"\"\n    W_phi = np.array([\n        [0.5,  -0.2,  0.1],\n        [0.3,   0.4, -0.1],\n        [-0.25, 0.15, 0.6]\n    ], dtype=float)\n    b_phi = np.array([0.05, -0.03, 0.02], dtype=float)\n    w_rho = np.array([1.2, -0.7, 0.5], dtype=float)\n    c_rho = -0.1\n\n    fractions = np.asarray(fractions, dtype=float)\n    descriptors = np.asarray(descriptors, dtype=float)\n    # Apply shared embedding phi to each descriptor\n    phi_vals = (descriptors @ W_phi.T) + b_phi  # shape (n, 3)\n    # Symmetric aggregation weighted by fractions\n    v = np.einsum('i,ij->j', fractions, phi_vals)  # shape (3,)\n    # Linear readout\n    y = float(np.dot(w_rho, v) + c_rho)\n    return y\n\ndef baseline_order_sensitive_predict(fractions, descriptors):\n    \"\"\"\n    Order-sensitive baseline predictor using position-specific weights b_i.\n    Uses first n weight vectors for an n-element composition in given order.\n    \"\"\"\n    b_list = [\n        np.array([0.1, 0.2, 0.3], dtype=float),\n        np.array([-0.2, 0.1, 0.4], dtype=float),\n        np.array([0.3, -0.1, 0.2], dtype=float),\n        np.array([0.4, 0.5, -0.3], dtype=float),\n    ]\n    c0 = 0.0\n\n    fractions = np.asarray(fractions, dtype=float)\n    descriptors = np.asarray(descriptors, dtype=float)\n    n = len(fractions)\n    assert descriptors.shape == (n, 3), \"Descriptors shape must be (n, 3)\"\n    # Use the first n b_i vectors in order\n    contrib = 0.0\n    for i in range(n):\n        contrib += fractions[i] * float(np.dot(b_list[i], descriptors[i]))\n    y = c0 + contrib\n    return y\n\ndef invariance_test(model_fn, fractions, descriptors, eps=1e-12):\n    \"\"\"\n    Returns True if model_fn output is invariant within eps across all permutations\n    of elements (jointly permuting fractions and descriptors).\n    \"\"\"\n    fractions = np.asarray(fractions, dtype=float)\n    descriptors = np.asarray(descriptors, dtype=float)\n    n = len(fractions)\n    # Reference output on identity ordering\n    y_ref = model_fn(fractions, descriptors)\n    max_diff = 0.0\n    # Enumerate all permutations\n    for perm in itertools.permutations(range(n)):\n        perm = np.array(perm, dtype=int)\n        f_perm = fractions[perm]\n        d_perm = descriptors[perm]\n        y_perm = model_fn(f_perm, d_perm)\n        diff = abs(y_perm - y_ref)\n        if diff > max_diff:\n            max_diff = diff\n        # Early exit if above tolerance to save time\n        if max_diff > eps:\n            return False\n    return True\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A: 3 elements, typical\n        (\n            [0.40, 0.35, 0.25],\n            [\n                [1.83, 1.26,  8.00],\n                [1.91, 1.24, 10.00],\n                [1.66, 1.28,  6.00],\n            ],\n        ),\n        # Case B: single element boundary\n        (\n            [1.00],\n            [\n                [1.61, 1.43, 3.00],\n            ],\n        ),\n        # Case C: 4 elements with tiny and zero fractions\n        (\n            [0.001, 0.499, 0.500, 0.000],\n            [\n                [1.63, 1.34,  5.00],\n                [1.88, 1.25,  9.00],\n                [1.54, 1.47,  4.00],\n                [2.16, 1.39,  6.00],\n            ],\n        ),\n        # Case D: 3 elements, equal fractions with duplicate descriptors\n        (\n            [1.0/3.0, 1.0/3.0, 1.0/3.0],\n            [\n                [1.55, 1.27,  7.00],\n                [1.55, 1.27,  7.00],\n                [1.90, 1.28, 11.00],\n            ],\n        ),\n    ]\n\n    eps = 1e-12\n    results = []\n    for fractions, descriptors in test_cases:\n        inv_pass = invariance_test(deep_sets_predict, fractions, descriptors, eps)\n        base_pass = invariance_test(baseline_order_sensitive_predict, fractions, descriptors, eps)\n        results.append([inv_pass, base_pass])\n\n    # Final print statement in the exact required format.\n    # Single line, comma-separated list enclosed in square brackets.\n    # We format booleans as their Python literals True/False.\n    print(f\"[{','.join([str(pair).replace(' ', '') for pair in results])}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在材料科学中，我们常常面对包含大量潜在相关描述符的高维特征空间。本练习将引导你完成一个完整的机器学习流程：使用LASSO（最小绝对收缩和选择算子）方法从众多描述符中为高熵合金的硬度预测筛选出一个简约而有效的特征子集。此外，你将通过自助法（bootstrap）重采样来评估所选特征的稳定性，这是验证从模型中得出的科学结论是否稳健的关键一步。",
            "id": "3750182",
            "problem": "您需要实现一个完整的算法流程，以使用最小绝对收缩和选择算子 (LASSO) 为高熵合金 (HEAs) 的硬度预测选择一个最小描述符集，并通过自助重采样 (bootstrap resampling) 来测试所选特征的稳定性。请考虑一个用于 HEAs 的合成但科学上合理的描述符空间和一个硬度的线性响应模型。该任务必须纯粹用数学术语来表述，并且必须根据指定的格式产生可量化的输出。\n\n假设设置如下。设有 $n$ 个合金样本和每个样本的 $p$ 个描述符，收集到一个设计矩阵 $X \\in \\mathbb{R}^{n \\times p}$ 和一个硬度响应向量 $y \\in \\mathbb{R}^{n}$ 中。硬度是一个物理量，必须以吉帕斯卡 (GPa) 为单位表示。虽然您的程序将只输出不带单位的索引和列表，但请确保内部模拟的硬度值使用 GPa 单位。定义 $p = 8$ 个描述符，对应于典型 HEA 设计特征：原子尺寸失配度 $\\delta$、价电子浓度 $\\mathrm{VEC}$、平均电负性 $\\chi_{\\mathrm{avg}}$、混合焓 $\\Delta H_{\\mathrm{mix}}$、平均熔点 $T_{m,\\mathrm{avg}}$、平均体积模量 $K_{\\mathrm{avg}}$、平均原子质量 $M_{\\mathrm{avg}}$ 和平均剪切模量 $G_{\\mathrm{avg}}$。矩阵 $X$ 必须从相关的潜在变量生成，以反映现实中的 HEA 描述符关系。\n\n使用以下带有加性噪声的生成式硬度模型：\n$$\ny = \\beta_0 + X \\beta^\\star + \\varepsilon,\n$$\n其中 $\\beta_0 \\in \\mathbb{R}$ 是一个截距，$\\beta^\\star \\in \\mathbb{R}^{p}$ 是一个稀疏系数向量，其非零项位于描述符的一个子集上，$\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I)$ 是独立噪声。在拟合之前，描述符矩阵 $X$ 必须按列进行标准化，使其均值为零，方差为一，响应向量 $y$ 必须中心化至均值为零；截距应与中心化一致处理。\n\n定义系数向量 $\\beta \\in \\mathbb{R}^{p}$ 的 LASSO 目标函数：\n$$\n\\min_{\\beta \\in \\mathbb{R}^{p}} \\; \\frac{1}{2n}\\,\\|y - X \\beta\\|_2^2 \\; + \\; \\lambda \\|\\beta\\|_1,\n$$\n其中 $\\lambda \\ge 0$ 是正则化强度，$\\|\\cdot\\|_1$ 表示 $\\ell_1$ 范数。通过基于第一性原理和凸优化的坐标下降法实现 LASSO 求解器。截距项必须不被惩罚，并通过中心化 $y$ 来处理。特征选择由拟合系数 $\\hat{\\beta}_j$ 非零的索引 $j$ 定义。\n\n为了测试特征稳定性，执行自助重采样：通过对 $(X,y)$ 的 $n$ 行进行有放回抽样来生成 $B$ 个自助数据集，在相同的 $\\lambda$ 下对每个自助重采样副本重新拟合 LASSO，并记录特征被选中（非零系数）的指示符。特征 $j$ 的选择频率是 $j$ 被选中的自助重采样副本所占的比例。将在阈值 $\\tau \\in [0,1]$ 下的稳定集定义为那些频率至少为 $\\tau$ 的索引 $j$。\n\n您的程序必须：\n- 根据测试套件给定的参数，按照指定的受 HEA 启发的 correlated latent structure（相关潜在结构）生成 $X$ 和 $y$。\n- 对每次拟合和每个自助重采样副本，标准化 $X$ 并中心化 $y$。\n- 实现用于 LASSO 目标的坐标下降法，并使用软阈值更新，确保在预定容差内收敛到极小值点。\n- 计算所有自助重采样副本的选择频率，并为每个测试用例返回稳定的特征索引。\n\n测试套件参数化和数据生成：\n- 对于每个测试用例，使用 $n = 240$ 个样本， $p = 8$ 个描述符，一个截距 $\\beta_0 = 2.0$ (单位 GPa)，以及一个稀疏的真实系数向量\n$$\n\\beta^\\star = [0.9,\\; 0.7,\\; 0.0,\\; 0.0,\\; 0.0,\\; 0.2,\\; 0.0,\\; 0.5],\n$$\n这分别对应于 $\\delta$、$\\mathrm{VEC}$、$K_{\\mathrm{avg}}$ 和 $G_{\\mathrm{avg}}$ 的非零效应。将 $X$ 生成为潜在变量 $(u_1,u_2,u_3,u_4)$ 的线性组合，相关强度为 $c \\in [0,1]$：\n- $X_{\\cdot,0} = u_1 + c\\,u_2 + 0.1\\,u_4$,\n- $X_{\\cdot,1} = u_2 + c\\,u_1 + 0.1\\,u_4$,\n- $X_{\\cdot,2} = 0.5\\,u_2 + 0.5\\,u_3 + 0.1\\,u_4$,\n- $X_{\\cdot,3} = -0.8\\,u_1 + 0.2\\,u_4$,\n- $X_{\\cdot,4} = 0.1\\,u_1 + u_3 + 0.1\\,u_4$,\n- $X_{\\cdot,5} = u_3 + 0.2\\,u_2 + 0.1\\,u_4$,\n- $X_{\\cdot,6} = 0.3\\,u_1 - 0.1\\,u_3 + 0.1\\,u_4$,\n- $X_{\\cdot,7} = u_3 + 0.3\\,u_1 + 0.1\\,u_4$,\n其中每个样本的 $(u_1,u_2,u_3,u_4)$ 均独立地从 $\\mathcal{N}(0,1)$ 中抽取。设噪声标准差为 $\\sigma$ (单位 GPa)。\n\n测试套件：\n- 情况 1 (一般情况)：$\\lambda = 0.2$，$B = 200$，$\\tau = 0.6$，$c = 0.3$，$\\sigma = 0.3$。\n- 情况 2 (强正则化边界)：$\\lambda = 1.2$，$B = 200$，$\\tau = 0.6$，$c = 0.3$，$\\sigma = 0.3$。\n- 情况 3 (强描述符相关性边缘)：$\\lambda = 0.25$，$B = 200$，$\\tau = 0.9$，$c = 0.9$，$\\sigma = 0.3$。\n\n答案规范：\n- 对于每个测试用例，返回按升序排序的稳定特征索引列表（从零开始）。每个列表必须格式化为方括号内由逗号分隔的整数，无空格，例如 $[0,1,7]$。\n- 最终的程序输出必须是单行，包含三个用例的结果，格式为由逗号分隔并用方括号括起来的列表，例如 $[[i_{1,1},\\dots],[i_{2,1},\\dots],[i_{3,1},\\dots]]$。",
            "solution": "该问题陈述是有效的。它科学上基于统计机器学习的既定原则（LASSO 回归、坐标下降优化、自助重采样）及其在一个合理的材料科学场景（预测高熵合金的硬度）中的应用。该问题是适定的，提供了一套完整且一致的给定条件，包括一个生成式数据模型、一个精确的优化目标和一个清晰的算法规范。所有参数均已定义，且任务在计算上是可行的。因此，我们可以着手解决。\n\n目标是实现一个完整的算法流程，以识别用于预测高熵合金 (HEA) 硬度的稳定描述符集。这涉及三个主要阶段：1) 生成一个合成但现实的数据集；2) 使用坐标下降法实现最小绝对收缩和选择算子 (LASSO) 以执行特征选择；以及 3) 使用自助重采样评估所选特征的稳定性。\n\n**1. 数据生成模型**\n\n问题定义了一个合成数据生成过程，该过程模拟了 HEA 中物理描述符的相关性。给定 $n=240$ 个样本和 $p=8$ 个描述符。设计矩阵 $X \\in \\mathbb{R}^{n \\times p}$ 由 4 个潜在变量 $u_1, u_2, u_3, u_4$ 构建，其中每个潜在变量是来自标准正态分布 $\\mathcal{N}(0,1)$ 的 $n$ 个独立抽样的向量。$X$ 的列代表 $p=8$ 个 HEA 描述符，是这些潜在变量的线性组合，其中参数 $c$ 控制相关强度，特别是前两个描述符之间的相关强度。具体的线性组合如下：\n$$\n\\begin{aligned}\nX_{\\cdot,0} = u_1 + c\\,u_2 + 0.1\\,u_4 \\\\\nX_{\\cdot,1} = u_2 + c\\,u_1 + 0.1\\,u_4 \\\\\nX_{\\cdot,2} = 0.5\\,u_2 + 0.5\\,u_3 + 0.1\\,u_4 \\\\\nX_{\\cdot,3} = -0.8\\,u_1 + 0.2\\,u_4 \\\\\nX_{\\cdot,4} = 0.1\\,u_1 + u_3 + 0.1\\,u_4 \\\\\nX_{\\cdot,5} = u_3 + 0.2\\,u_2 + 0.1\\,u_4 \\\\\nX_{\\cdot,6} = 0.3\\,u_1 - 0.1\\,u_3 + 0.1\\,u_4 \\\\\nX_{\\cdot,7} = u_3 + 0.3\\,u_1 + 0.1\\,u_4\n\\end{aligned}\n$$\n响应变量，即硬度 $y \\in \\mathbb{R}^n$，由一个带有稀疏真实系数向量 $\\beta^\\star \\in \\mathbb{R}^p$、截距 $\\beta_0 \\in \\mathbb{R}$ 和加性高斯噪声 $\\varepsilon \\in \\mathbb{R}^n$ 的线性模型生成：\n$$\ny = \\beta_0 + X \\beta^\\star + \\varepsilon\n$$\n给定的参数包括截距 $\\beta_0 = 2.0$ GPa，噪声标准差 $\\sigma$，以及一个稀疏系数向量：\n$$\n\\beta^\\star = [0.9,\\; 0.7,\\; 0.0,\\; 0.0,\\; 0.0,\\; 0.2,\\; 0.0,\\; 0.5]\n$$\n该向量表明描述符 0、1、5 和 7 存在真实效应，它们分别对应于 $\\delta$、$\\mathrm{VEC}$、$K_{\\mathrm{avg}}$ 和 $G_{\\mathrm{avg}}$。噪声向量 $\\varepsilon$ 从 $\\mathcal{N}(0, \\sigma^2 I)$ 中抽取，其中 $I$ 是 $n \\times n$ 单位矩阵。\n\n**2. 通过坐标下降法求解 LASSO**\n\n特征选择的核心是 LASSO，我们通过最小化以下关于系数向量 $\\beta \\in \\mathbb{R}^p$ 的目标函数来求解：\n$$\n\\min_{\\beta \\in \\mathbb{R}^{p}} \\; \\frac{1}{2n}\\,\\|y - X \\beta\\|_2^2 \\; + \\; \\lambda \\|\\beta\\|_1\n$$\n这里，$\\lambda \\ge 0$ 是一个超参数，控制 $\\ell_1$ 范数惩罚项 $\\|\\beta\\|_1 = \\sum_{j=0}^{p-1} |\\beta_j|$ 的强度。此惩罚项鼓励稀疏性，将一些系数精确地驱动到零。\n\n在最小化之前，必须对数据进行预处理。设计矩阵 $X$ 按列进行标准化，以获得零均值和单位方差。响应向量 $y$ 进行中心化，以获得零均值。这种中心化隐式地处理了截距项 $\\beta_0$，在后续对 $\\beta$ 的 LASSO 拟合中不再考虑该截距。\n\n我们使用坐标下降法实现求解器。这种迭代算法一次只针对单个系数 $\\beta_j$ 优化目标函数，同时保持所有其他系数 $\\beta_{k \\ne j}$ 固定。对于一个标准化的 $X$（其中每列的平方 $\\ell_2$ 范数为 $n$），关于 $\\beta_j$ 的目标函数为：\n$$\nf(\\beta_j) = \\frac{1}{2n} \\sum_{i=1}^{n} \\left(r_i^{(j)} - X_{ij}\\beta_j\\right)^2 + \\lambda |\\beta_j|  + \\text{const}\n$$\n其中 $r_i^{(j)} = y_i - \\sum_{k \\ne j} X_{ik}\\beta_k$ 是部分残差的第 $i$ 个分量。依赖于 $\\beta_j$ 的目标函数部分简化为 $\\frac{1}{2}\\beta_j^2 - \\rho_j\\beta_j + \\lambda|\\beta_j|$，其中 $\\rho_j = \\frac{1}{n} \\sum_{i=1}^n X_{ij} r_i^{(j)}$。使该式最小化的 $\\beta_j$ 值由软阈值算子 $\\mathcal{S}_{\\lambda}(\\cdot)$ 给出：\n$$\n\\hat{\\beta}_j \\leftarrow \\mathcal{S}_{\\lambda}(\\rho_j) = \\text{sign}(\\rho_j) \\max(|\\rho_j| - \\lambda, 0)\n$$\n$\\rho_j$ 的值可以高效计算，而无需在每一步重新构建部分残差。它等于 $\\frac{1}{n}X_j^T(y - X\\beta) + \\beta_j$，其中 $y-X\\beta$ 是使用当前 $\\beta$ 估计的完整残差向量。\n\n该算法首先初始化 $\\beta = 0$，然后重复遍历所有特征 $j=0, \\dots, p-1$，应用更新规则，直到 $\\beta$ 向量在两次迭代之间的变化量小于一个很小的容差为止。\n\n**3. 自助法稳定性分析**\n\nLASSO 的特征选择可能对数据的特定实现很敏感，尤其是在存在相关预测变量时。为了评估我们选择的稳定性，我们采用自助重采样。过程如下：\n1. 生成 $B$ 个自助数据集。每个数据集 $(X_b, y_b)$ 都是通过从原始数据集 $(X, y)$ 中有放回地抽样 $n$ 行而形成的。\n2. 对于每个自助法副本 $b = 1, \\dots, B$：\n    a. 预处理自助样本：标准化 $X_b$ 的列并中心化 $y_b$。请注意，用于标准化的均值和标准差是*从当前自助样本* $(X_b, y_b)$ 计算的。\n    b. 使用给定的正则化参数 $\\lambda$ 在处理后的 $(X'_b, y'_b)$ 上拟合 LASSO 模型，以获得系数估计 $\\hat{\\beta}^{(b)}$。\n    c. 如果系数 $\\hat{\\beta}_j^{(b)}$ 非零（即 $|\\hat{\\beta}_j^{(b)}| > \\epsilon$，对于某个小的数值容差 $\\epsilon > 0$），则记录一个指示符 $I_j^{(b)} = 1$，否则 $I_j^{(b)} = 0$。\n3. 所有自助拟合完成后，计算每个特征 $j$ 的选择频率，即其指示符的平均值：$F_j = \\frac{1}{B} \\sum_{b=1}^{B} I_j^{(b)}$。\n4. 最终的稳定特征集定义为所有选择频率 $F_j$ 达到或超过稳定性阈值 $\\tau$ 的索引 $j$ 的集合，即 $\\{j \\mid F_j \\ge \\tau\\}$。\n\n这个完整的流程将应用于每个测试用例，每个用例指定了模型参数 $(\\lambda, B, \\tau, c, \\sigma)$ 的唯一组合，以生成一个排序后的稳定特征索引列表。",
            "answer": "```python\nimport numpy as np\n\ndef soft_threshold(z, t):\n    \"\"\"Soft-thresholding operator.\"\"\"\n    return np.sign(z) * np.maximum(np.abs(z) - t, 0)\n\ndef lasso_cd(X, y, lambda_val, tol=1e-5, max_iter=1000):\n    \"\"\"\n    Solves the LASSO problem using coordinate descent.\n    Assumes X is standardized and y is centered.\n    Objective: min_beta 1/(2n) * ||y - X*beta||_2^2 + lambda_val * ||beta||_1\n    \"\"\"\n    n, p = X.shape\n    beta = np.zeros(p)\n\n    for _ in range(max_iter):\n        beta_old = beta.copy()\n        for j in range(p):\n            # The term rho_j is the simple least squares coefficient for feature j\n            # on the partial residual y - sum_{k!=j} X_k*beta_k.\n            # It can be computed efficiently from the full residual.\n            # rho_j = X_j^T * (y - X*beta + X_j*beta_j) / n\n            #       = (X_j^T * (y - X*beta) + X_j^T*X_j*beta_j) / n\n            # Since X_j is standardized, X_j^T*X_j = n.\n            # So, rho_j = (X_j^T * (y - X*beta))/n + beta_j\n            rho_j = (X[:, j].T @ (y - X @ beta)) / n + beta[j]\n            beta[j] = soft_threshold(rho_j, lambda_val)\n        \n        if np.max(np.abs(beta - beta_old))  tol:\n            break\n            \n    return beta\n\ndef generate_data(n, p, beta_0, beta_star, c, sigma):\n    \"\"\"Generates synthetic HEA descriptor and hardness data.\"\"\"\n    # Latent variables\n    u = np.random.randn(n, 4)\n    u1, u2, u3, u4 = u[:, 0], u[:, 1], u[:, 2], u[:, 3]\n\n    X = np.zeros((n, p))\n    X[:, 0] = u1 + c * u2 + 0.1 * u4\n    X[:, 1] = u2 + c * u1 + 0.1 * u4\n    X[:, 2] = 0.5 * u2 + 0.5 * u3 + 0.1 * u4\n    X[:, 3] = -0.8 * u1 + 0.2 * u4\n    X[:, 4] = 0.1 * u1 + u3 + 0.1 * u4\n    X[:, 5] = u3 + 0.2 * u2 + 0.1 * u4\n    X[:, 6] = 0.3 * u1 - 0.1 * u3 + 0.1 * u4\n    X[:, 7] = u3 + 0.3 * u1 + 0.1 * u4\n    \n    # Generate response variable y\n    epsilon = np.random.randn(n) * sigma\n    y = beta_0 + X @ beta_star + epsilon\n    \n    return X, y\n\ndef run_case(n, p, beta_0, beta_star, lambda_val, B, tau, c, sigma):\n    \"\"\"Runs the full pipeline for one test case.\"\"\"\n    # 1. Generate the original, full dataset for this case\n    X_orig, y_orig = generate_data(n, p, beta_0, beta_star, c, sigma)\n    \n    selection_counts = np.zeros(p)\n    \n    # 2. Bootstrap stability analysis\n    for _ in range(B):\n        # Create bootstrap sample\n        indices = np.random.choice(n, size=n, replace=True)\n        X_b, y_b = X_orig[indices], y_orig[indices]\n\n        # Pre-processing for the bootstrap sample\n        y_b_centered = y_b - np.mean(y_b)\n        \n        X_b_mean = np.mean(X_b, axis=0)\n        X_b_std = np.std(X_b, axis=0)\n        # Handle columns with zero variance to avoid division by zero\n        X_b_std[X_b_std == 0] = 1.0\n        X_b_stdized = (X_b - X_b_mean) / X_b_std\n        \n        # Fit LASSO model\n        beta_hat = lasso_cd(X_b_stdized, y_b_centered, lambda_val)\n\n        # Record selected features (non-zero coefficients)\n        selected_features = np.where(np.abs(beta_hat)  1e-7)[0]\n        selection_counts[selected_features] += 1\n        \n    # 3. Determine stable set\n    frequencies = selection_counts / B\n    stable_indices = np.where(frequencies = tau)[0]\n    \n    return sorted(stable_indices.tolist())\n\ndef solve():\n    \"\"\"Main solver function.\"\"\"\n    # Common parameters defined in the problem\n    n = 240\n    p = 8\n    beta_0 = 2.0\n    beta_star = np.array([0.9, 0.7, 0.0, 0.0, 0.0, 0.2, 0.0, 0.5])\n\n    # Test suite from the problem statement\n    test_cases = [\n        # (lambda_val, B, tau, c, sigma)\n        (0.2, 200, 0.6, 0.3, 0.3),    # Case 1\n        (1.2, 200, 0.6, 0.3, 0.3),    # Case 2\n        (0.25, 200, 0.9, 0.9, 0.3),   # Case 3\n    ]\n\n    # A fixed random seed is used to ensure the stochastic parts of the\n    # algorithm (data generation, bootstrapping) are reproducible.\n    np.random.seed(42)\n\n    case_results = []\n    for case_params in test_cases:\n        lambda_val, B, tau, c, sigma = case_params\n        stable_set = run_case(n, p, beta_0, beta_star, lambda_val, B, tau, c, sigma)\n        \n        # Format each list as a string \"[i1,i2,...]\" with no spaces\n        list_as_string = f\"[{','.join(map(str, stable_set))}]\"\n        case_results.append(list_as_string)\n\n    # Final output must be a single line, formatted as a list of lists.\n    # e.g., [[0,1,7],[0],[5,7]]\n    print(f\"[{','.join(case_results)}]\")\n\nsolve()\n```"
        }
    ]
}