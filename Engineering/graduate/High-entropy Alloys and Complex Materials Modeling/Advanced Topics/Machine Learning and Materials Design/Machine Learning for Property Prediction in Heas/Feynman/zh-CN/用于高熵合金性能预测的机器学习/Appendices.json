{
    "hands_on_practices": [
        {
            "introduction": "在信任任何机器学习模型之前，我们必须严格评估其性能。本练习介绍平均绝对误差（$MAE$）、均方根误差（$RMSE$）和决定系数（$R^2$）等基本评估指标，并要求您在异方差噪声（材料数据中的常见特征）的真实情景下解释这些指标。通过这个练习，您将为模型评估建立关键的基础。",
            "id": "3750198",
            "problem": "在预测高熵合金 (HEAs) 机械性能的背景下，考虑一个监督学习模型，该模型经过训练，可根据成分和热力学描述符预测屈服强度。在一个包含 $8$ 种等原子比高熵合金的独立测试集上，测得的屈服强度 $y_i$ 和模型预测值 $\\hat{y}_i$（单位均为吉帕斯卡 (GPa)）如下：\n\n$$(y_1, y_2, y_3, y_4, y_5, y_6, y_7, y_8) = (0.85, 1.10, 1.40, 1.80, 2.20, 2.60, 3.20, 3.80),$$\n$$(\\hat{y}_1, \\hat{y}_2, \\hat{y}_3, \\hat{y}_4, \\hat{y}_5, \\hat{y}_6, \\hat{y}_7, \\hat{y}_8) = (0.87, 1.07, 1.45, 1.72, 2.30, 2.48, 3.38, 3.55).$$\n\n测量过程是异方差的：其噪声标准差随数值大小而增加，对于每种合金 $i$，其唯象估计为 $\\sigma_i = 0.04\\, y_i + 0.02$ GPa。仅使用上述测试集数值以及回归分析中平均绝对误差 (MAE)、均方根误差 (RMSE) 和决定系数 $R^2$ 的标准定义，计算这三个指标。\n\n然后，根据异方差噪声下残差大小的分布，解释 MAE 和 RMSE 之间的任何差异，并阐明当残差方差随 $y_i$ 增加时，$R^2$ 的值如何与预测所解释的方差比例相关联。你的解释应基于统计误差和方差的核心定义，不使用这些定义之外的任何快捷公式。\n\n报告 MAE 和 RMSE 的数值（单位为吉帕斯卡 (GPa)），以及 $R^2$ 的数值（纯数字，无量纲）。将所有三个数值四舍五入到四位有效数字。以 $(\\text{MAE}, \\text{RMSE}, R^2)$ 的顺序，将最终答案表示为单行矩阵。",
            "solution": "该问题陈述具有科学依据，提法明确，并包含了唯一解所需的所有信息。它要求计算标准的回归指标，并基于基本统计原理进行解释。该问题是有效的。\n\n解答过程分为两部分：首先，计算平均绝对误差 (MAE)、均方根误差 (RMSE) 和决定系数 ($R^2$)；其次，在所提供的数据和指定的异方差噪声模型背景下解释这些指标。\n\n测试集中的数据点数量为 $N=8$。测量的屈服强度由向量 $y$ 给出，模型预测由向量 $\\hat{y}$ 给出。\n$$y = (0.85, 1.10, 1.40, 1.80, 2.20, 2.60, 3.20, 3.80)$$\n$$\\hat{y} = (0.87, 1.07, 1.45, 1.72, 2.30, 2.48, 3.38, 3.55)$$\n所有值的单位均为吉帕斯卡 (GPa)。\n\n首先，我们计算残差向量 $e$，其中每个元素为 $e_i = y_i - \\hat{y}_i$。\n$$e_1 = 0.85 - 0.87 = -0.02$$\n$$e_2 = 1.10 - 1.07 = 0.03$$\n$$e_3 = 1.40 - 1.45 = -0.05$$\n$$e_4 = 1.80 - 1.72 = 0.08$$\n$$e_5 = 2.20 - 2.30 = -0.10$$\n$$e_6 = 2.60 - 2.48 = 0.12$$\n$$e_7 = 3.20 - 3.38 = -0.18$$\n$$e_8 = 3.80 - 3.55 = 0.25$$\n\n残差向量为 $e = (-0.02, 0.03, -0.05, 0.08, -0.10, 0.12, -0.18, 0.25)$。\n\n**平均绝对误差 (MAE) 的计算**\nMAE 定义为残差绝对值的平均值：\n$$\\text{MAE} = \\frac{1}{N} \\sum_{i=1}^{N} |y_i - \\hat{y}_i|$$\n我们对残差向量 $e$ 中元素的绝对值求和：\n$$\\sum_{i=1}^{8} |e_i| = |-0.02| + |0.03| + |-0.05| + |0.08| + |-0.10| + |0.12| + |-0.18| + |0.25|$$\n$$\\sum_{i=1}^{8} |e_i| = 0.02 + 0.03 + 0.05 + 0.08 + 0.10 + 0.12 + 0.18 + 0.25 = 0.83$$\n$$\\text{MAE} = \\frac{0.83}{8} = 0.10375$$\n四舍五入到四位有效数字，$\\text{MAE} \\approx 0.1038$ GPa。\n\n**均方根误差 (RMSE) 的计算**\nRMSE 是残差平方均值的平方根：\n$$\\text{RMSE} = \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2}$$\n首先，我们计算残差平方和 $\\text{SS}_{\\text{res}}$：\n$$\\text{SS}_{\\text{res}} = \\sum_{i=1}^{8} e_i^2 = (-0.02)^2 + (0.03)^2 + (-0.05)^2 + (0.08)^2 + (-0.10)^2 + (0.12)^2 + (-0.18)^2 + (0.25)^2$$\n$$\\text{SS}_{\\text{res}} = 0.0004 + 0.0009 + 0.0025 + 0.0064 + 0.0100 + 0.0144 + 0.0324 + 0.0625 = 0.1295$$\n均方误差 (MSE) 为 $\\frac{\\text{SS}_{\\text{res}}}{N} = \\frac{0.1295}{8} = 0.0161875$。\n$$\\text{RMSE} = \\sqrt{0.0161875} \\approx 0.1272299$$\n四舍五入到四位有效数字，$\\text{RMSE} \\approx 0.1272$ GPa。\n\n**决定系数 ($R^2$) 的计算**\n$R^2$ 值定义为：\n$$R^2 = 1 - \\frac{\\text{SS}_{\\text{res}}}{\\text{SS}_{\\text{tot}}}$$\n其中 $\\text{SS}_{\\text{res}} = \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2$ 是残差平方和（上面已计算为 $0.1295$），而 $\\text{SS}_{\\text{tot}} = \\sum_{i=1}^{N} (y_i - \\bar{y})^2$ 是总平方和。首先，我们必须计算测量值的均值 $\\bar{y}$。\n$$\\bar{y} = \\frac{1}{N} \\sum_{i=1}^{N} y_i = \\frac{0.85 + 1.10 + 1.40 + 1.80 + 2.20 + 2.60 + 3.20 + 3.80}{8} = \\frac{16.95}{8} = 2.11875$$\n接下来，我们计算 $\\text{SS}_{\\text{tot}}$：\n$$\\text{SS}_{\\text{tot}} = (0.85 - 2.11875)^2 + (1.10 - 2.11875)^2 + \\dots + (3.80 - 2.11875)^2$$\n$$\\text{SS}_{\\text{tot}} \\approx 1.609766 + 1.037852 + 0.516602 + 0.101602 + 0.006602 + 0.231602 + 1.169102 + 2.826602 = 7.49975$$\n现在我们可以计算 $R^2$：\n$$R^2 = 1 - \\frac{0.1295}{7.49975} \\approx 1 - 0.01726727 = 0.98273273$$\n四舍五入到四位有效数字，$R^2 \\approx 0.9827$。\n\n**指标解释**\n\n*   **MAE 与 RMSE 之间的差异：** 我们观察到 $\\text{RMSE} \\approx 0.1272$ 大于 $\\text{MAE} \\approx 0.1038$。这是一个普遍性质，因为 RMSE 计算中的平方运算对较大的残差给予了不成比例的更大权重。MAE 对所有误差都进行线性处理。在这个数据集中，与较大屈服强度相关的残差（$e_7 = -0.18$ 和 $e_8 = 0.25$）在量级上显著大于其他残差。这几个大的误差增大了平方和，导致 RMSE 明显大于 MAE。这表明残差分布不均匀，特别是存在一些“离群”或大数值的误差。这一观察结果与给出的异方差噪声模型 $\\sigma_i = 0.04\\, y_i + 0.02$ 一致，该模型预测误差大小应随测量值 $y_i$ 的增加而增加。较大的残差确实出现在具有较高屈服强度的合金上。\n\n*   **异方差性下 $R^2$ 的解释：** 决定系数 $R^2 \\approx 0.9827$ 表明，测量屈服强度 ($y$) 的总方差中约有 $98.27\\%$ 可由模型的预测 ($\\hat{y}$) 解释。虽然这是一个非常高的值，表明拟合效果极佳，但由于误差的异方差性，其解释必须细致地理解。基本定义 $R^2 = 1 - \\text{SS}_{\\text{res}}/\\text{SS}_{\\text{tot}}$ 在求和 $\\text{SS}_{\\text{res}}$ 时对所有平方残差给予相同的权重。然而，由于较大 $y_i$ 值的残差也较大，对 $\\text{SS}_{\\text{res}}$ 的贡献主要来自高强度合金的误差。因此，$R^2$ 指标对模型在高强度合金上的性能更敏感，而对在低强度合金上的性能不那么敏感。一个高的 $R^2$ 值可能掩盖了在低方差区域的相对较差的性能，或者像本例中一样，表明总体拟合良好，但由于数据范围高端的误差不断增大而有所降低。$R^2$ 不再是衡量整个 $y$ 值范围拟合优度的无偏度量；它变成了一个受数据点局部方差加权的度量。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.1038  0.1272  0.9827\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "一个强大的预测模型必须尊重其所模拟系统的基本物理原理。对于合金而言，一个核心原则是材料的性质不应依赖于我们列出其组成元素的任意顺序。本练习将指导您实现并测试这一被称为“置换不变性”的关键属性，从而确保您开发的模型在物理上是合理的。",
            "id": "3750170",
            "problem": "您的任务是为高熵合金（HEA）性能的预测模型设计并实现一个成分不变性测试框架。高熵合金（HEA）由多种元素以近乎等原子比混合而成；在对此类系统进行建模时，一个核心的物理要求是，一种成分的预测属性必须与其组成元素的任意排序无关。用机器学习的术语来说，从元素描述符及其分数的多元集到标量属性的映射，应在组成元素索引的排列下保持不变。您的程序必须是一个完整、可运行的实现，它需要构建两个预测器，建立一个排列不变性测试框架，并报告指定测试套件的自动化单元测试结果。\n\n基本原理与物理真实性：对于理想混合物行为和材料科学中使用的许多混合法则，目标宏观属性是通过对特定组分描述符的聚合统计来建模的。一个典型的例子是混合法则，其一种形式是，属性 $P$ 由加权和 $P = \\sum_{i=1}^{n} w_i P_i$ 给出，其中 $w_i$ 是一个具有物理意义的权重（如摩尔分数），$P_i$ 是一个特定于组分的量。这种聚合是对称的：它只取决于组分的集合及其权重，而不取决于它们的枚举顺序。在与这些定律一致的学习系统中，排列不变性可以通过对称操作（例如对各组分嵌入的求和）在结构上被强制执行。\n\n您的任务：\n1. 实现两个以元素描述符和分数定义的成分作为输入的标量输出预测器：\n   - 一个基于对称聚合器的顺序不变预测器，该聚合器与理想混合物成分一致。该模型必须使用一个共享的逐元素嵌入，然后是对元素进行对称规约和一个最终的读出层。具体来说，使用共享的逐元素变换和按元素分数加权的元素求和来获得中间表示，然后通过线性读出层产生标量属性。此设计不得依赖于元素的位置索引。\n   - 一个有意设计的顺序敏感基线预测器，它使用位置特定的参数来形成各元素贡献的线性组合；该模型必须依赖于索引位置，因此对于多元素输入，它应该无法通过不变性检查。\n\n2. 构建一个测试框架，对于给定的成分，该框架会枚举组成元素索引的所有排列，并检查预测器输出在所有排列中的变化是否在指定容差范围内。测试框架必须为每个成分报告一个布尔值，指示其排列不变性测试是否通过。\n\n3. 使用以下数值指定的参数和测试套件。所有描述符都是三维向量，代表物理上合理的特征：Pauling 电负性（无量纲）、共价半径（单位为埃）和一个类整数的价电子数。分数是表示为小数的摩尔分数；它们的总和必须为 $1$。\n   - 不变模型的共享逐元素嵌入参数（对每个元素使用相同的 $\\phi$）：\n     - 权重矩阵 $W_{\\phi}$：\n       $$\n       W_{\\phi} = \\begin{bmatrix}\n       0.5  -0.2  0.1 \\\\\n       0.3  0.4  -0.1 \\\\\n       -0.25  0.15  0.6\n       \\end{bmatrix}\n       $$\n     - 偏置向量 $b_{\\phi}$：$[0.05, -0.03, 0.02]$。\n     - 读出权重 $w_{\\rho}$：$[1.2, -0.7, 0.5]$。\n     - 读出偏置 $c_{\\rho}$：$-0.1$。\n   - 顺序敏感基线的特定位置权重向量（对于一个 n 元素成分，按出现顺序使用前 n 个向量）：\n     - $b_1 = [0.1, 0.2, 0.3]$， $b_2 = [-0.2, 0.1, 0.4]$， $b_3 = [0.3, -0.1, 0.2]$， $b_4 = [0.4, 0.5, -0.3]$。\n     - 基线偏置 $c_0 = 0.0$。\n   - 成分测试套件（每个案例是一对分数和描述符矩阵；行对应于元素）：\n     1. 案例 A（三个元素，典型的类 HEA）：\n        - 分数：$[0.40, 0.35, 0.25]$。\n        - 描述符：$\\big[ [1.83, 1.26, 8.00], [1.91, 1.24, 10.00], [1.66, 1.28, 6.00] \\big]$。\n     2. 案例 B（单元素边界情况）：\n        - 分数：$[1.00]$。\n        - 描述符：$\\big[ [1.61, 1.43, 3.00] \\big]$。\n     3. 案例 C（四个元素，含极小和零分数的边缘情况）：\n        - 分数：$[0.001, 0.499, 0.500, 0.000]$。\n        - 描述符：$\\big[ [1.63, 1.34, 5.00], [1.88, 1.25, 9.00], [1.54, 1.47, 4.00], [2.16, 1.39, 6.00] \\big]$。\n     4. 案例 D（三个元素，等分数且含重复描述符）：\n        - 分数：$[\\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3}]$。\n        - 描述符：$\\big[ [1.55, 1.27, 7.00], [1.55, 1.27, 7.00], [1.90, 1.28, 11.00] \\big]$。\n   - 不变性检查的容差：$\\varepsilon = 10^{-12}$。\n\n4. 程序逻辑：\n   - 定义不变预测器，使用共享的元素映射 $\\phi$，然后是按分数加权的对称求和以及一个线性读出层。输出必须是一个标量。\n   - 定义非不变基线预测器，对于一个 n 元素输入，按给定顺序使用前 n 个特定位置权重 $b_i$；输出必须是一个标量。\n   - 实现一个函数，对每个测试案例，检查预测器在元素索引的所有排列下的输出，并返回一个布尔值，指示所有输出是否在容差 $\\varepsilon$ 内相等。\n\n5. 最终输出格式：\n   - 对于每个测试案例，生成一对布尔值 $[b_{\\text{inv}}, b_{\\text{noninv}}]$，其中 $b_{\\text{inv}}$ 表示不变模型是否通过排列不变性测试，而 $b_{\\text{noninv}}$ 表示基线模型是否通过排列不变性测试。\n   - 您的程序应生成单行输出，其中包含所有测试案例的结果，格式为一个由方括号括起来的逗号分隔列表，列表中的每个元素是对应一个测试案例的布尔值对。例如：$[[\\text{True},\\text{False}],[\\text{True},\\text{True}],\\dots]$。\n\n角度单位不适用。中间描述符的物理单位如上所述，但最终输出是布尔值，因此无单位。该测试套件涵盖了典型、边界和边缘情况，包括多元素成分、单元素成分、极小和零分数，以及在等分数下的重复描述符，以探究排列不变性的不同方面。",
            "solution": "我们首先将设计植根于具有物理动机的对称性和聚合原理。混合法则提供了一个典型的例子，其中宏观属性 $P$ 通过求和 $P = \\sum_{i=1}^{n} w_i P_i$ 来建模，其中 $w_i$ 表示物理权重（如摩尔分数），$P_i$ 表示特定组分的量。该表达式在索引集 $\\{1,2,\\dots,n\\}$ 的排列下是不变的，因为求和运算满足交换律和结合律。在用于成分的学习系统中，一种尊重这种对称性的结构是 Deep Sets 公式，其中多元集 $\\{x_i\\}_{i=1}^n$ 上的函数被建模为 $f(\\{x_i\\}) = \\rho\\left(\\sum_{i=1}^n \\phi(x_i)\\right)$，其中 $\\phi$ 和 $\\rho$ 是合适的函数。对于包含分数 $x_i$ 的高熵合金成分（这里，$x_i$ 表示摩尔分数，不要与描述符 $d_i$ 混淆），一个一致的推广是 $f(\\{(x_i,d_i)\\}) = \\rho\\left(\\sum_{i=1}^n x_i \\, \\phi(d_i)\\right)$。\n\n排列不变性推导：设 $\\pi$ 为 $\\{1,\\dots,n\\}$ 的任意一个排列。考虑不变模型\n$$\ny_{\\text{inv}} = \\rho\\left(\\sum_{i=1}^n x_i \\, \\phi(d_i)\\right),\n$$\n其中 $\\phi$ 相同地应用于每个元素描述符 $d_i$。在排列下，输入序列 $(x_i, d_i)$ 被转换为 $(x_{\\pi(i)}, d_{\\pi(i)})$；对称聚合器产生\n$$\n\\sum_{i=1}^n x_{\\pi(i)} \\, \\phi(d_{\\pi(i)}) = \\sum_{j=1}^n x_j \\, \\phi(d_j),\n$$\n在进行索引变换 $j = \\pi(i)$ 之后，因为 $\\pi$ 是一个双射。因此，$y_{\\text{inv}}$ 在任何 $\\pi$ 下都是相同的。\n\n基线模型的顺序敏感性推导：设基线预测器使用位置特定的参数 $b_i$ 来形成一个标量\n$$\ny_{\\text{base}} = c_0 + \\sum_{i=1}^n x_i \\, \\langle b_i, d_i \\rangle,\n$$\n其中 $\\langle \\cdot, \\cdot \\rangle$ 表示 $\\mathbb{R}^3$ 中的欧几里得内积。在一个排列 $\\pi$ 下，这变为\n$$\ny_{\\text{base}}^{(\\pi)} = c_0 + \\sum_{i=1}^n x_{\\pi(i)} \\, \\langle b_i, d_{\\pi(i)} \\rangle.\n$$\n除非所有的 $b_i$ 都相同，或者对于特定的描述符和分数发生了特殊的抵消，$y_{\\text{base}}^{(\\pi)} \\neq y_{\\text{base}}$。因此，该预测器通常是顺序敏感的，对于多元素输入将无法通过排列不变性测试。\n\n算法设计：\n- 实现 $\\phi(d) = W_{\\phi} d + b_{\\phi}$，其中 $W_{\\phi} \\in \\mathbb{R}^{3 \\times 3}$ 和 $b_{\\phi} \\in \\mathbb{R}^3$ 如给定。这是一个应用于每个元素描述符 $d_i$ 的共享仿射映射。\n- 实现 $\\rho(v) = \\langle w_{\\rho}, v \\rangle + c_{\\rho}$，其中 $w_{\\rho} \\in \\mathbb{R}^3$ 和 $c_{\\rho} \\in \\mathbb{R}$ 如给定。\n- 不变预测器计算 $v = \\sum_{i=1}^n x_i \\, \\phi(d_i)$，然后计算 $y_{\\text{inv}} = \\rho(v)$。\n- 基线预测器使用当前顺序中的前 $n$ 个特定位置向量 $b_i$ 来计算 $y_{\\text{base}} = c_0 + \\sum_{i=1}^n x_i \\, \\langle b_i, d_i \\rangle$。\n- 不变性测试框架：对于每个具有分数 $(x_1,\\dots,x_n)$ 和描述符 $(d_1,\\dots,d_n)$ 的成分，枚举 $\\{1,\\dots,n\\}$ 的所有排列 $\\pi$，为每个排列后的排列计算模型输出，并检查所有输出是否在容差 $\\varepsilon = 10^{-12}$ 内与参考输出相等。形式上，对于输出 $y^{(\\pi)}$，计算 $\\max_{\\pi} \\left| y^{(\\pi)} - y^{(\\text{id})} \\right|$，如果该最大值小于或等于 $\\varepsilon$，则判定为通过。\n\n测试套件覆盖范围：\n- 案例 A 探究了一个典型的多元素类 HEA 成分，具有现实的特征和非均匀的分数；不变模型应该通过，而基线模型应该失败。\n- 案例 B 是一个单元素边界情况；两个模型都应该通过，因为只有一个排列。\n- 案例 C 包括极小和零分数，用于检验数值稳定性并确保零分数元素没有影响；不变模型应该通过，而基线模型应该失败，除非在所有排列中发生巧合的相等，而这极不可能。\n- 案例 D 包括等分数和重复的描述符，用于探究即使在交换相同元素时不变性是否仍然成立；不变模型应该通过，而基线模型仍然会失败，因为特定位置的权重创建了不会在所有排列中消失的依赖性。\n\n数值稳定性：对浮点值求和会引入微小的、与顺序相关的舍入差异。选择容差 $\\varepsilon = 10^{-12}$ 是为了安全地高于双精度浮点数的机器精度，同时又足够严格以捕捉真正的顺序依赖性。不变模型根据其构造是对称的；不同排列之间的任何差异最多应为舍入误差，因此低于 $\\varepsilon$。预计基线模型在多元素情况下会超过 $\\varepsilon$。\n\n最终输出：对于每个案例，生成 $[b_{\\text{inv}}, b_{\\text{noninv}}]$，并将所有案例的列表打印在单行上：$[[b_{\\text{inv}}^{A}, b_{\\text{noninv}}^{A}], [b_{\\text{inv}}^{B}, b_{\\text{noninv}}^{B}], [b_{\\text{inv}}^{C}, b_{\\text{noninv}}^{C}], [b_{\\text{inv}}^{D}, b_{\\text{noninv}}^{D}]]$。\n\n该设计将基础物理对称性与算法检查相结合，以强制执行和验证 HEA 属性预测模型中的排列不变性，这符合高等研究生水平对复杂材料建模中机器学习模型的正确性和鲁棒性的期望。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport itertools\n\ndef deep_sets_predict(fractions, descriptors):\n    \"\"\"\n    Order-invariant predictor using shared element-wise embedding phi and\n    symmetric aggregation followed by a linear readout rho.\n    \"\"\"\n    W_phi = np.array([\n        [0.5,  -0.2,  0.1],\n        [0.3,   0.4, -0.1],\n        [-0.25, 0.15, 0.6]\n    ], dtype=float)\n    b_phi = np.array([0.05, -0.03, 0.02], dtype=float)\n    w_rho = np.array([1.2, -0.7, 0.5], dtype=float)\n    c_rho = -0.1\n\n    fractions = np.asarray(fractions, dtype=float)\n    descriptors = np.asarray(descriptors, dtype=float)\n    # Apply shared embedding phi to each descriptor\n    phi_vals = (descriptors @ W_phi.T) + b_phi  # shape (n, 3)\n    # Symmetric aggregation weighted by fractions\n    v = np.einsum('i,ij->j', fractions, phi_vals)  # shape (3,)\n    # Linear readout\n    y = float(np.dot(w_rho, v) + c_rho)\n    return y\n\ndef baseline_order_sensitive_predict(fractions, descriptors):\n    \"\"\"\n    Order-sensitive baseline predictor using position-specific weights b_i.\n    Uses first n weight vectors for an n-element composition in given order.\n    \"\"\"\n    b_list = [\n        np.array([0.1, 0.2, 0.3], dtype=float),\n        np.array([-0.2, 0.1, 0.4], dtype=float),\n        np.array([0.3, -0.1, 0.2], dtype=float),\n        np.array([0.4, 0.5, -0.3], dtype=float),\n    ]\n    c0 = 0.0\n\n    fractions = np.asarray(fractions, dtype=float)\n    descriptors = np.asarray(descriptors, dtype=float)\n    n = len(fractions)\n    assert descriptors.shape == (n, 3), \"Descriptors shape must be (n, 3)\"\n    # Use the first n b_i vectors in order\n    contrib = 0.0\n    for i in range(n):\n        contrib += fractions[i] * float(np.dot(b_list[i], descriptors[i]))\n    y = c0 + contrib\n    return y\n\ndef invariance_test(model_fn, fractions, descriptors, eps=1e-12):\n    \"\"\"\n    Returns True if model_fn output is invariant within eps across all permutations\n    of elements (jointly permuting fractions and descriptors).\n    \"\"\"\n    fractions = np.asarray(fractions, dtype=float)\n    descriptors = np.asarray(descriptors, dtype=float)\n    n = len(fractions)\n    # Reference output on identity ordering\n    y_ref = model_fn(fractions, descriptors)\n    max_diff = 0.0\n    # Enumerate all permutations\n    for perm in itertools.permutations(range(n)):\n        perm = np.array(perm, dtype=int)\n        f_perm = fractions[perm]\n        d_perm = descriptors[perm]\n        y_perm = model_fn(f_perm, d_perm)\n        diff = abs(y_perm - y_ref)\n        if diff > max_diff:\n            max_diff = diff\n        # Early exit if above tolerance to save time\n        if max_diff > eps:\n            return False\n    return True\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A: 3 elements, typical\n        (\n            [0.40, 0.35, 0.25],\n            [\n                [1.83, 1.26,  8.00],\n                [1.91, 1.24, 10.00],\n                [1.66, 1.28,  6.00],\n            ],\n        ),\n        # Case B: single element boundary\n        (\n            [1.00],\n            [\n                [1.61, 1.43, 3.00],\n            ],\n        ),\n        # Case C: 4 elements with tiny and zero fractions\n        (\n            [0.001, 0.499, 0.500, 0.000],\n            [\n                [1.63, 1.34,  5.00],\n                [1.88, 1.25,  9.00],\n                [1.54, 1.47,  4.00],\n                [2.16, 1.39,  6.00],\n            ],\n        ),\n        # Case D: 3 elements, equal fractions with duplicate descriptors\n        (\n            [1.0/3.0, 1.0/3.0, 1.0/3.0],\n            [\n                [1.55, 1.27,  7.00],\n                [1.55, 1.27,  7.00],\n                [1.90, 1.28, 11.00],\n            ],\n        ),\n    ]\n\n    eps = 1e-12\n    results = []\n    for fractions, descriptors in test_cases:\n        inv_pass = invariance_test(deep_sets_predict, fractions, descriptors, eps)\n        base_pass = invariance_test(baseline_order_sensitive_predict, fractions, descriptors, eps)\n        results.append([inv_pass, base_pass])\n\n    # Final print statement in the exact required format.\n    # Single line, comma-separated list enclosed in square brackets.\n    # We format booleans as their Python literals True/False.\n    print(f\"[{','.join([str(pair).replace(' ', '') for pair in results])}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "从预测走向科学洞见，往往需要识别出最具影响力的物理因素。本练习使用 LASSO 回归这一强大的特征选择技术，来构建一个更简洁、更具可解释性的高熵合金硬度模型。通过将其与自助法重采样相结合，您还将学会评估特征选择的稳定性，这是获得稳健科学发现的关键一步。",
            "id": "3750182",
            "problem": "要求您实现一个完整的算法流程，使用最小绝对收缩和选择算子 (LASSO) 来选择一个最小描述符集，以预测高熵合金 (HEA) 的硬度，并通过自助法重采样 (bootstrap resampling) 来测试所选特征的稳定性。考虑一个用于 HEA 的合成但科学上合理的描述符空间和一个硬度的线性响应模型。该任务必须完全用数学术语来表述，并且必须根据指定的格式产生可量化的输出。\n\n假设有以下设定。设有 $n$ 个合金样本，每个样本有 $p$ 个描述符，这些描述符被收集到一个设计矩阵 $X \\in \\mathbb{R}^{n \\times p}$ 和一个硬度响应向量 $y \\in \\mathbb{R}^{n}$ 中。硬度是一个物理量，必须以吉帕斯卡 (GPa) 表示。虽然您的程序将只输出不带单位的索引和列表，但请确保内部模拟的硬度值使用 GPa 单位。定义 $p = 8$ 个描述符，对应于典型的高熵合金设计特征：原子尺寸失配度 $\\delta$、价电子浓度 $\\mathrm{VEC}$、平均电负性 $\\chi_{\\mathrm{avg}}$、混合焓 $\\Delta H_{\\mathrm{mix}}$、平均熔点 $T_{m,\\mathrm{avg}}$、平均体积模量 $K_{\\mathrm{avg}}$、平均原子质量 $M_{\\mathrm{avg}}$ 和平均剪切模量 $G_{\\mathrm{avg}}$。设矩阵 $X$ 由相关的潜在变量生成，以反映现实的高熵合金描述符关系。\n\n使用以下带有加性噪声的生成式硬度模型：\n$$\ny = \\beta_0 + X \\beta^\\star + \\varepsilon,\n$$\n其中 $\\beta_0 \\in \\mathbb{R}$ 是一个截距，$\\beta^\\star \\in \\mathbb{R}^{p}$ 是一个稀疏系数向量，其非零项位于描述符的一个子集上，而 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I)$ 是独立噪声。在拟合之前，描述符矩阵 $X$ 必须按列标准化为零均值和单位方差，响应向量 $y$ 必须中心化为零均值；截距应通过中心化来一致地处理。\n\n定义系数向量 $\\beta \\in \\mathbb{R}^{p}$ 的 LASSO 目标函数：\n$$\n\\min_{\\beta \\in \\mathbb{R}^{p}} \\; \\frac{1}{2n}\\,\\|y - X \\beta\\|_2^2 \\; + \\; \\lambda \\|\\beta\\|_1,\n$$\n其中 $\\lambda \\ge 0$ 是正则化强度，$\\|\\cdot\\|_1$ 表示 $\\ell_1$ 范数。基于基本原理和凸优化，通过坐标下降法实现 LASSO 求解器。截距项必须不受惩罚，并通过中心化 $y$ 来处理。特征选择由拟合系数 $\\hat{\\beta}_j$ 非零的索引 $j$ 定义。\n\n为了测试特征稳定性，执行自助法重采样：通过从 $(X,y)$ 中有放回地抽取 $n$ 行来生成 $B$ 个自助法数据集，对于每个自助法复制样本，在相同的 $\\lambda$ 下重新拟合 LASSO，并记录特征被选中（系数非零）的指示符。特征 $j$ 的选择频率是 $j$ 被选中的自助法复制样本所占的比例。将阈值 $\\tau \\in [0,1]$ 下的稳定集定义为那些频率至少为 $\\tau$ 的索引 $j$。\n\n您的程序必须：\n- 根据测试套件给定的参数，按照指定的高熵合金启发的相关潜在结构生成 $X$ 和 $y$。\n- 对每次拟合和每个自助法复制样本，将 $X$ 标准化并将 $y$ 中心化。\n- 为 LASSO 目标函数实现坐标下降法，使用软阈值更新，并确保在规定的容差内收敛到最小值点。\n- 计算所有自助法复制样本的选择频率，并为每个测试用例返回稳定的特征索引。\n\n测试套件参数化和数据生成：\n- 对于每个测试用例，使用 $n = 240$ 个样本，$p = 8$ 个描述符，截距 $\\beta_0 = 2.0$ (GPa)，以及一个稀疏的真实系数向量\n$$\n\\beta^\\star = [0.9,\\; 0.7,\\; 0.0,\\; 0.0,\\; 0.0,\\; 0.2,\\; 0.0,\\; 0.5],\n$$\n这对应于 $\\delta$、$\\mathrm{VEC}$、$K_{\\mathrm{avg}}$ 和 $G_{\\mathrm{avg}}$ 分别存在非零效应。将 $X$ 生成为潜在变量 $(u_1,u_2,u_3,u_4)$ 的线性组合，相关强度为 $c \\in [0,1]$：\n- $X_{\\cdot,0} = u_1 + c\\,u_2 + 0.1\\,u_4$,\n- $X_{\\cdot,1} = u_2 + c\\,u_1 + 0.1\\,u_4$,\n- $X_{\\cdot,2} = 0.5\\,u_2 + 0.5\\,u_3 + 0.1\\,u_4$,\n- $X_{\\cdot,3} = -0.8\\,u_1 + 0.2\\,u_4$,\n- $X_{\\cdot,4} = 0.1\\,u_1 + u_3 + 0.1\\,u_4$,\n- $X_{\\cdot,5} = u_3 + 0.2\\,u_2 + 0.1\\,u_4$,\n- $X_{\\cdot,6} = 0.3\\,u_1 - 0.1\\,u_3 + 0.1\\,u_4$,\n- $X_{\\cdot,7} = u_3 + 0.3\\,u_1 + 0.1\\,u_4$,\n其中每个样本的 $(u_1,u_2,u_3,u_4)$ 均独立地从 $\\mathcal{N}(0,1)$ 中抽取。设噪声标准差为 $\\sigma$ (GPa)。\n\n测试套件：\n- 情况 1（一般情况）：$\\lambda = 0.2$, $B = 200$, $\\tau = 0.6$, $c = 0.3$, $\\sigma = 0.3$。\n- 情况 2（强正则化边界）：$\\lambda = 1.2$, $B = 200$, $\\tau = 0.6$, $c = 0.3$, $\\sigma = 0.3$。\n- 情况 3（强描述符相关性边缘）：$\\lambda = 0.25$, $B = 200$, $\\tau = 0.9$, $c = 0.9$, $\\sigma = 0.3$。\n\n答案规范：\n- 为每个测试用例返回稳定特征索引（从零开始）的列表，按升序排序。每个列表的格式必须是方括号和逗号分隔的整数，不含空格，例如 $[0,1,7]$。\n- 最终程序输出必须是一行，包含三个测试用例的结果，格式为用方括号括起来的逗号分隔列表，例如 $[[i_{1,1},\\dots],[i_{2,1},\\dots],[i_{3,1},\\dots]]$。",
            "solution": "该问题陈述是有效的。它在科学上基于统计机器学习的既定原则（LASSO 回归、坐标下降优化、自助法重采样）及其在一个可信的材料科学场景（预测高熵合金的硬度）中的应用。该问题是适定的，提供了一套完整且一致的给定条件，包括一个生成式数据模型、一个精确的优化目标和一个清晰的算法规范。所有参数都已定义，并且任务在计算上是可行的。因此，我们可以着手解决。\n\n目标是实现一个完整的算法流程，以识别用于预测高熵合金 (HEA) 硬度的稳定描述符集。这涉及三个主要阶段：1) 生成一个合成但现实的数据集；2) 使用坐标下降法实现最小绝对收缩和选择算子 (LASSO) 以执行特征选择；以及 3) 使用自助法重采样评估所选特征的稳定性。\n\n**1. 数据生成模型**\n\n该问题定义了一个合成数据生成过程，该过程模仿了 HEA 中物理描述符的相关性。给定 $n=240$ 个样本和 $p=8$ 个描述符。设计矩阵 $X \\in \\mathbb{R}^{n \\times p}$ 由 4 个潜在变量 $u_1, u_2, u_3, u_4$ 构建，其中每个潜在变量都是从标准正态分布 $\\mathcal{N}(0,1)$ 中独立抽取的 $n$ 个值的向量。$X$ 的列代表 $p=8$ 个 HEA 描述符，是这些潜在变量的线性组合，其中参数 $c$ 控制相关强度，特别是前两个描述符之间的相关强度。具体的线性组合如下：\n$$\n\\begin{aligned}\nX_{\\cdot,0} = u_1 + c\\,u_2 + 0.1\\,u_4 \\\\\nX_{\\cdot,1} = u_2 + c\\,u_1 + 0.1\\,u_4 \\\\\nX_{\\cdot,2} = 0.5\\,u_2 + 0.5\\,u_3 + 0.1\\,u_4 \\\\\nX_{\\cdot,3} = -0.8\\,u_1 + 0.2\\,u_4 \\\\\nX_{\\cdot,4} = 0.1\\,u_1 + u_3 + 0.1\\,u_4 \\\\\nX_{\\cdot,5} = u_3 + 0.2\\,u_2 + 0.1\\,u_4 \\\\\nX_{\\cdot,6} = 0.3\\,u_1 - 0.1\\,u_3 + 0.1\\,u_4 \\\\\nX_{\\cdot,7} = u_3 + 0.3\\,u_1 + 0.1\\,u_4\n\\end{aligned}\n$$\n响应变量，即硬度 $y \\in \\mathbb{R}^n$，由一个带有稀疏真实系数向量 $\\beta^\\star \\in \\mathbb{R}^p$、截距 $\\beta_0 \\in \\mathbb{R}$ 和加性高斯噪声 $\\varepsilon \\in \\mathbb{R}^n$ 的线性模型生成：\n$$\ny = \\beta_0 + X \\beta^\\star + \\varepsilon\n$$\n给定的参数包括截距 $\\beta_0 = 2.0$ GPa，噪声标准差 $\\sigma$，以及一个稀疏系数向量：\n$$\n\\beta^\\star = [0.9,\\; 0.7,\\; 0.0,\\; 0.0,\\; 0.0,\\; 0.2,\\; 0.0,\\; 0.5]\n$$\n该向量表明，描述符 0、1、5 和 7 存在真实效应，它们分别对应于 $\\delta$、$\\mathrm{VEC}$、$K_{\\mathrm{avg}}$ 和 $G_{\\mathrm{avg}}$。噪声向量 $\\varepsilon$ 从 $\\mathcal{N}(0, \\sigma^2 I)$ 中抽取，其中 $I$ 是 $n \\times n$ 的单位矩阵。\n\n**2. 通过坐标下降法求解 LASSO**\n\n特征选择的核心是 LASSO，我们通过最小化以下关于系数向量 $\\beta \\in \\mathbb{R}^p$ 的目标函数来求解：\n$$\n\\min_{\\beta \\in \\mathbb{R}^{p}} \\; \\frac{1}{2n}\\,\\|y - X \\beta\\|_2^2 \\; + \\; \\lambda \\|\\beta\\|_1\n$$\n这里，$\\lambda \\ge 0$ 是一个超参数，用于控制 $\\ell_1$ 范数惩罚项 $\\|\\beta\\|_1 = \\sum_{j=0}^{p-1} |\\beta_j|$ 的强度。该惩罚项鼓励稀疏性，将一些系数驱动到恰好为零。\n\n在最小化之前，必须对数据进行预处理。设计矩阵 $X$ 按列进行标准化，使其均值为零，方差为单位。响应向量 $y$ 进行中心化，使其均值为零。这种中心化隐式地处理了截距项 $\\beta_0$，之后在对 $\\beta$ 的 LASSO 拟合中会忽略该项。\n\n我们使用坐标下降法实现求解器。这种迭代算法一次只针对单个系数 $\\beta_j$ 优化目标函数，同时保持所有其他系数 $\\beta_{k \\ne j}$ 固定。对于标准化的 $X$（其中每列的 $\\ell_2$ 范数平方为 $n$），$\\beta_j$ 的目标函数为：\n$$\nf(\\beta_j) = \\frac{1}{2n} \\sum_{i=1}^{n} \\left(r_i^{(j)} - X_{ij}\\beta_j\\right)^2 + \\lambda |\\beta_j|  + \\text{const}\n$$\n其中 $r_i^{(j)} = y_i - \\sum_{k \\ne j} X_{ik}\\beta_k$ 是部分残差的第 $i$ 个分量。目标函数中依赖于 $\\beta_j$ 的部分简化为 $\\frac{1}{2}\\beta_j^2 - \\rho_j\\beta_j + \\lambda|\\beta_j|$，其中 $\\rho_j = \\frac{1}{n} \\sum_{i=1}^n X_{ij} r_i^{(j)}$。最小化此表达式的 $\\beta_j$ 值由软阈值算子 $\\mathcal{S}_{\\lambda}(\\cdot)$ 给出：\n$$\n\\hat{\\beta}_j \\leftarrow \\mathcal{S}_{\\lambda}(\\rho_j) = \\text{sign}(\\rho_j) \\max(|\\rho_j| - \\lambda, 0)\n$$\n值 $\\rho_j$ 可以高效计算，无需在每一步重新构建部分残差。它等于 $\\frac{1}{n}X_j^T(y - X\\beta) + \\beta_j$，其中 $y-X\\beta$ 是使用当前 $\\beta$ 估计值的完整残差向量。\n\n算法首先初始化 $\\beta = 0$，然后重复遍历所有特征 $j=0, \\dots, p-1$，应用更新规则，直到两次循环之间 $\\beta$ 向量的变化量低于一个很小的容差。\n\n**3. 自助法稳定性分析**\n\nLASSO 的特征选择可能对数据的具体实现敏感，尤其是在存在相关预测变量的情况下。为了评估我们所选特征的稳定性，我们采用自助法重采样。步骤如下：\n1. 生成 $B$ 个自助法数据集。每个数据集 $(X_b, y_b)$ 都是通过从原始数据集 $(X, y)$ 中有放回地抽取 $n$ 行形成的。\n2. 对于每个自助法复制样本 $b = 1, \\dots, B$：\n    a. 预处理自助法样本：将 $X_b$ 的列标准化并中心化 $y_b$。注意，用于标准化的均值和标准差是*从当前的自助法样本* $(X_b, y_b)$ 计算的。\n    b. 在处理后的 $(X'_b, y'_b)$ 上使用给定的正则化参数 $\\lambda$ 拟合 LASSO 模型，以获得系数估计 $\\hat{\\beta}^{(b)}$。\n    c. 如果系数 $\\hat{\\beta}_j^{(b)}$ 非零（即 $|\\hat{\\beta}_j^{(b)}| > \\epsilon$，对于某个小的数值容差 $\\epsilon > 0$），则记录一个指示符 $I_j^{(b)} = 1$，否则 $I_j^{(b)} = 0$。\n3. 在所有自助法拟合完成后，计算每个特征 $j$ 的选择频率，即其指示符的平均值：$F_j = \\frac{1}{B} \\sum_{b=1}^{B} I_j^{(b)}$。\n4. 最终的稳定特征集定义为所有选择频率 $F_j$ 达到或超过稳定性阈值 $\\tau$ 的索引 $j$ 的集合，即 $\\{j \\mid F_j \\ge \\tau\\}$。\n\n这个完整的流程应用于每个测试用例，每个用例指定了模型参数 $(\\lambda, B, \\tau, c, \\sigma)$ 的唯一组合，以生成一个排序后的稳定特征索引列表。",
            "answer": "```python\nimport numpy as np\n\ndef soft_threshold(z, t):\n    \"\"\"Soft-thresholding operator.\"\"\"\n    return np.sign(z) * np.maximum(np.abs(z) - t, 0)\n\ndef lasso_cd(X, y, lambda_val, tol=1e-5, max_iter=1000):\n    \"\"\"\n    Solves the LASSO problem using coordinate descent.\n    Assumes X is standardized and y is centered.\n    Objective: min_beta 1/(2n) * ||y - X*beta||_2^2 + lambda_val * ||beta||_1\n    \"\"\"\n    n, p = X.shape\n    beta = np.zeros(p)\n\n    for _ in range(max_iter):\n        beta_old = beta.copy()\n        for j in range(p):\n            # The term rho_j is the simple least squares coefficient for feature j\n            # on the partial residual y - sum_{k!=j} X_k*beta_k.\n            # It can be computed efficiently from the full residual.\n            # rho_j = X_j^T * (y - X*beta + X_j*beta_j) / n\n            #       = (X_j^T * (y - X*beta) + X_j^T*X_j*beta_j) / n\n            # Since X_j is standardized, X_j^T*X_j = n.\n            # So, rho_j = (X_j^T * (y - X*beta))/n + beta_j\n            rho_j = (X[:, j].T @ (y - X @ beta)) / n + beta[j]\n            beta[j] = soft_threshold(rho_j, lambda_val)\n        \n        if np.max(np.abs(beta - beta_old)) < tol:\n            break\n            \n    return beta\n\ndef generate_data(n, p, beta_0, beta_star, c, sigma):\n    \"\"\"Generates synthetic HEA descriptor and hardness data.\"\"\"\n    # Latent variables\n    u = np.random.randn(n, 4)\n    u1, u2, u3, u4 = u[:, 0], u[:, 1], u[:, 2], u[:, 3]\n\n    X = np.zeros((n, p))\n    X[:, 0] = u1 + c * u2 + 0.1 * u4\n    X[:, 1] = u2 + c * u1 + 0.1 * u4\n    X[:, 2] = 0.5 * u2 + 0.5 * u3 + 0.1 * u4\n    X[:, 3] = -0.8 * u1 + 0.2 * u4\n    X[:, 4] = 0.1 * u1 + u3 + 0.1 * u4\n    X[:, 5] = u3 + 0.2 * u2 + 0.1 * u4\n    X[:, 6] = 0.3 * u1 - 0.1 * u3 + 0.1 * u4\n    X[:, 7] = u3 + 0.3 * u1 + 0.1 * u4\n    \n    # Generate response variable y\n    epsilon = np.random.randn(n) * sigma\n    y = beta_0 + X @ beta_star + epsilon\n    \n    return X, y\n\ndef run_case(n, p, beta_0, beta_star, lambda_val, B, tau, c, sigma):\n    \"\"\"Runs the full pipeline for one test case.\"\"\"\n    # 1. Generate the original, full dataset for this case\n    X_orig, y_orig = generate_data(n, p, beta_0, beta_star, c, sigma)\n    \n    selection_counts = np.zeros(p)\n    \n    # 2. Bootstrap stability analysis\n    for _ in range(B):\n        # Create bootstrap sample\n        indices = np.random.choice(n, size=n, replace=True)\n        X_b, y_b = X_orig[indices], y_orig[indices]\n\n        # Pre-processing for the bootstrap sample\n        y_b_centered = y_b - np.mean(y_b)\n        \n        X_b_mean = np.mean(X_b, axis=0)\n        X_b_std = np.std(X_b, axis=0)\n        # Handle columns with zero variance to avoid division by zero\n        X_b_std[X_b_std == 0] = 1.0\n        X_b_stdized = (X_b - X_b_mean) / X_b_std\n        \n        # Fit LASSO model\n        beta_hat = lasso_cd(X_b_stdized, y_b_centered, lambda_val)\n\n        # Record selected features (non-zero coefficients)\n        selected_features = np.where(np.abs(beta_hat) > 1e-7)[0]\n        selection_counts[selected_features] += 1\n        \n    # 3. Determine stable set\n    frequencies = selection_counts / B\n    stable_indices = np.where(frequencies >= tau)[0]\n    \n    return sorted(stable_indices.tolist())\n\ndef solve():\n    \"\"\"Main solver function.\"\"\"\n    # Common parameters defined in the problem\n    n = 240\n    p = 8\n    beta_0 = 2.0\n    beta_star = np.array([0.9, 0.7, 0.0, 0.0, 0.0, 0.2, 0.0, 0.5])\n\n    # Test suite from the problem statement\n    test_cases = [\n        # (lambda_val, B, tau, c, sigma)\n        (0.2, 200, 0.6, 0.3, 0.3),    # Case 1\n        (1.2, 200, 0.6, 0.3, 0.3),    # Case 2\n        (0.25, 200, 0.9, 0.9, 0.3),   # Case 3\n    ]\n\n    # A fixed random seed is used to ensure the stochastic parts of the\n    # algorithm (data generation, bootstrapping) are reproducible.\n    np.random.seed(42)\n\n    case_results = []\n    for case_params in test_cases:\n        lambda_val, B, tau, c, sigma = case_params\n        stable_set = run_case(n, p, beta_0, beta_star, lambda_val, B, tau, c, sigma)\n        \n        # Format each list as a string \"[i1,i2,...]\" with no spaces\n        list_as_string = f\"[{','.join(map(str, stable_set))}]\"\n        case_results.append(list_as_string)\n\n    # Final output must be a single line, formatted as a list of lists.\n    # e.g., [[0,1,7],[0],[5,7]]\n    print(f\"[{','.join(case_results)}]\")\n\nsolve()\n```"
        }
    ]
}