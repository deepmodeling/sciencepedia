{
    "hands_on_practices": [
        {
            "introduction": "In materials informatics, we often begin with a large number of potential physical descriptors. A key task is to identify a minimal, yet predictive, subset of these features to build interpretable and robust models. This practice  guides you through implementing the LASSO regression algorithm to perform automated feature selection and using bootstrap resampling to assess the stability of your results, a crucial step for building reliable scientific models.",
            "id": "3750182",
            "problem": "You are asked to implement a complete algorithmic pipeline to select a minimal descriptor set for predicting hardness in High-Entropy Alloys (HEAs) using the Least Absolute Shrinkage and Selection Operator (LASSO), and to test the stability of the selected features via bootstrap resampling. Consider a synthetic but scientifically plausible descriptor space for HEAs and a linear response model for hardness. The task must be framed purely in mathematical terms and must produce quantifiable outputs according to the specified format.\n\nAssume the following setting. Let there be $n$ alloy samples and $p$ descriptors for each sample collected into a design matrix $X \\in \\mathbb{R}^{n \\times p}$ and a hardness response vector $y \\in \\mathbb{R}^{n}$. Hardness is a physical quantity and must be expressed in gigapascals (GPa). While your program will only output indices and lists without units, ensure that simulated hardness values internally use the unit GPa. Define $p = 8$ descriptors corresponding to typical HEA design features: atomic size mismatch $\\delta$, valence electron concentration $\\mathrm{VEC}$, average electronegativity $\\chi_{\\mathrm{avg}}$, mixing enthalpy $\\Delta H_{\\mathrm{mix}}$, average melting point $T_{m,\\mathrm{avg}}$, average bulk modulus $K_{\\mathrm{avg}}$, average atomic mass $M_{\\mathrm{avg}}$, and average shear modulus $G_{\\mathrm{avg}}$. Let the matrix $X$ be generated from correlated latent variables to reflect realistic HEA descriptor relationships.\n\nUse the following generative hardness model with additive noise:\n$$\ny = \\beta_0 + X \\beta^\\star + \\varepsilon,\n$$\nwhere $\\beta_0 \\in \\mathbb{R}$ is an intercept, $\\beta^\\star \\in \\mathbb{R}^{p}$ is a sparse coefficient vector with nonzero entries on a subset of descriptors, and $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I)$ is independent noise. The descriptor matrix $X$ must be standardized columnwise to zero mean and unit variance, and the response $y$ must be centered to zero mean before fitting; the intercept should be handled consistently with centering.\n\nDefine the LASSO objective for coefficient vector $\\beta \\in \\mathbb{R}^{p}$:\n$$\n\\min_{\\beta \\in \\mathbb{R}^{p}} \\; \\frac{1}{2n}\\,\\|y - X \\beta\\|_2^2 \\; + \\; \\lambda \\|\\beta\\|_1,\n$$\nwhere $\\lambda \\ge 0$ is the regularization strength and $\\|\\cdot\\|_1$ denotes the $\\ell_1$ norm. Implement the LASSO solver via coordinate descent based on first principles and convex optimization. The intercept term must be unpenalized and treated by centering $y$. Feature selection is defined by the indices $j$ for which the fitted coefficient $\\hat{\\beta}_j$ is nonzero.\n\nTo test feature stability, perform bootstrap resampling: generate $B$ bootstrap datasets by sampling $n$ rows of $(X,y)$ with replacement, refit the LASSO for each bootstrap replicate at the same $\\lambda$, and record the indicator that a feature is selected (nonzero coefficient). The selection frequency for feature $j$ is the fraction of bootstrap replicates in which $j$ is selected. Define the stable set at threshold $\\tau \\in [0,1]$ as those indices $j$ whose frequency is at least $\\tau$.\n\nYour program must:\n- Generate $X$ and $y$ according to the specified HEA-inspired correlated latent structure with parameters given by the test suite.\n- Standardize $X$ and center $y$ for each fit and each bootstrap replicate.\n- Implement coordinate descent for the LASSO objective with a soft-thresholding update, ensuring convergence to a minimizer within a prescribed tolerance.\n- Compute selection frequencies across bootstrap replicates and return the stable feature indices for each test case.\n\nTest suite parameterization and data generation:\n- For each test case, use $n = 240$ samples, $p = 8$ descriptors, an intercept $\\beta_0 = 2.0$ (in GPa), and a sparse true coefficient vector\n$$\n\\beta^\\star = [0.9,\\; 0.7,\\; 0.0,\\; 0.0,\\; 0.0,\\; 0.2,\\; 0.0,\\; 0.5],\n$$\nwhich corresponds to nonzero effects on $\\delta$, $\\mathrm{VEC}$, $K_{\\mathrm{avg}}$, and $G_{\\mathrm{avg}}$ respectively. Generate $X$ as linear combinations of latent variables $(u_1,u_2,u_3,u_4)$ with correlation strength $c \\in [0,1]$:\n- $X_{\\cdot,0} = u_1 + c\\,u_2 + 0.1\\,u_4$,\n- $X_{\\cdot,1} = u_2 + c\\,u_1 + 0.1\\,u_4$,\n- $X_{\\cdot,2} = 0.5\\,u_2 + 0.5\\,u_3 + 0.1\\,u_4$,\n- $X_{\\cdot,3} = -0.8\\,u_1 + 0.2\\,u_4$,\n- $X_{\\cdot,4} = 0.1\\,u_1 + u_3 + 0.1\\,u_4$,\n- $X_{\\cdot,5} = u_3 + 0.2\\,u_2 + 0.1\\,u_4$,\n- $X_{\\cdot,6} = 0.3\\,u_1 - 0.1\\,u_3 + 0.1\\,u_4$,\n- $X_{\\cdot,7} = u_3 + 0.3\\,u_1 + 0.1\\,u_4$,\nwith $(u_1,u_2,u_3,u_4)$ drawn independently from $\\mathcal{N}(0,1)$ per sample. Let noise standard deviation be $\\sigma$ (in GPa).\n\nTest suite:\n- Case $1$ (general case): $\\lambda = 0.2$, $B = 200$, $\\tau = 0.6$, $c = 0.3$, $\\sigma = 0.3$.\n- Case $2$ (strong regularization boundary): $\\lambda = 1.2$, $B = 200$, $\\tau = 0.6$, $c = 0.3$, $\\sigma = 0.3$.\n- Case $3$ (strong descriptor correlation edge): $\\lambda = 0.25$, $B = 200$, $\\tau = 0.9$, $c = 0.9$, $\\sigma = 0.3$.\n\nAnswer specification:\n- For each test case, return the list of stable feature indices (zero-based) sorted in ascending order. Each list must be formatted with brackets and comma-separated integers, with no spaces, for example $[0,1,7]$.\n- The final program output must be a single line containing the three case results as a comma-separated list enclosed in square brackets, for example $[[i_{1,1},\\dots],[i_{2,1},\\dots],[i_{3,1},\\dots]]$.",
            "solution": "The problem statement is valid. It is scientifically grounded in established principles of statistical machine learning (LASSO regression, coordinate descent optimization, bootstrap resampling) and their application to a plausible materials science scenario (predicting hardness in high-entropy alloys). The problem is well-posed, providing a complete and consistent set of givens, including a generative data model, a precise optimization objective, and a clear algorithmic specification. All parameters are defined, and the task is computationally feasible. We may therefore proceed with a solution.\n\nThe objective is to implement a complete algorithmic pipeline to identify a stable set of descriptors for predicting High-Entropy Alloy (HEA) hardness. This involves three main stages: $1$) generating a synthetic, yet realistic, dataset; $2$) implementing the Least Absolute Shrinkage and Selection Operator (LASSO) using coordinate descent to perform feature selection; and $3$) assessing the stability of the selected features using bootstrap resampling.\n\n**1. Data Generation Model**\n\nThe problem defines a synthetic data generation process that mimics the correlated nature of physical descriptors in HEAs. We are given $n=240$ samples and $p=8$ descriptors. The design matrix $X \\in \\mathbb{R}^{n \\times p}$ is constructed from $4$ latent variables, $u_1, u_2, u_3, u_4$, where each latent variable is a vector of $n$ independent draws from a standard normal distribution, $\\mathcal{N}(0,1)$. The columns of $X$, representing the $p=8$ HEA descriptors, are linear combinations of these latent variables, with a parameter $c$ controlling the correlation strength, particularly between the first two descriptors. The specific linear combinations are:\n$$\n\\begin{aligned}\nX_{\\cdot,0} &= u_1 + c\\,u_2 + 0.1\\,u_4 \\\\\nX_{\\cdot,1} &= u_2 + c\\,u_1 + 0.1\\,u_4 \\\\\nX_{\\cdot,2} &= 0.5\\,u_2 + 0.5\\,u_3 + 0.1\\,u_4 \\\\\nX_{\\cdot,3} &= -0.8\\,u_1 + 0.2\\,u_4 \\\\\nX_{\\cdot,4} &= 0.1\\,u_1 + u_3 + 0.1\\,u_4 \\\\\nX_{\\cdot,5} &= u_3 + 0.2\\,u_2 + 0.1\\,u_4 \\\\\nX_{\\cdot,6} &= 0.3\\,u_1 - 0.1\\,u_3 + 0.1\\,u_4 \\\\\nX_{\\cdot,7} &= u_3 + 0.3\\,u_1 + 0.1\\,u_4\n\\end{aligned}\n$$\nThe response variable, hardness $y \\in \\mathbb{R}^n$, is generated by a linear model with a sparse true coefficient vector $\\beta^\\star \\in \\mathbb{R}^p$, an intercept $\\beta_0 \\in \\mathbb{R}$, and additive Gaussian noise $\\varepsilon \\in \\mathbb{R}^n$:\n$$\ny = \\beta_0 + X \\beta^\\star + \\varepsilon\n$$\nThe given parameters are an intercept $\\beta_0 = 2.0$ GPa, a noise standard deviation $\\sigma$, and a sparse coefficient vector:\n$$\n\\beta^\\star = [0.9,\\; 0.7,\\; 0.0,\\; 0.0,\\; 0.0,\\; 0.2,\\; 0.0,\\; 0.5]\n$$\nThis vector indicates that true effects are present for descriptors $0, 1, 5$, and $7$, which correspond to $\\delta$, $\\mathrm{VEC}$, $K_{\\mathrm{avg}}$, and $G_{\\mathrm{avg}}$, respectively. The noise vector $\\varepsilon$ is drawn from $\\mathcal{N}(0, \\sigma^2 I)$, where $I$ is the $n \\times n$ identity matrix.\n\n**2. LASSO via Coordinate Descent**\n\nThe core of the feature selection is the LASSO, which we solve by minimizing the following objective function for the coefficient vector $\\beta \\in \\mathbb{R}^p$:\n$$\n\\min_{\\beta \\in \\mathbb{R}^{p}} \\; \\frac{1}{2n}\\,\\|y - X \\beta\\|_2^2 \\; + \\; \\lambda \\|\\beta\\|_1\n$$\nHere, $\\lambda \\ge 0$ is a hyperparameter that controls the strength of the $\\ell_1$-norm penalty, $\\|\\beta\\|_1 = \\sum_{j=0}^{p-1} |\\beta_j|$. This penalty encourages sparsity, driving some coefficients to exactly zero.\n\nBefore minimization, the data must be pre-processed. The design matrix $X$ is standardized column-wise to have zero mean and unit variance. The response vector $y$ is centered to have zero mean. This centering implicitly handles the intercept term $\\beta_0$, which is thereafter ignored in the LASSO fit for $\\beta$.\n\nWe implement the solver using coordinate descent. This iterative algorithm optimizes the objective function with respect to a single coefficient $\\beta_j$ at a time, holding all other coefficients $\\beta_{k \\ne j}$ fixed. For a standardized $X$ (where each column has a squared $\\ell_2$-norm of $n$), the objective for $\\beta_j$ is:\n$$\nf(\\beta_j) = \\frac{1}{2n} \\sum_{i=1}^{n} \\left(r_i^{(j)} - X_{ij}\\beta_j\\right)^2 + \\lambda |\\beta_j|  + \\text{const}\n$$\nwhere $r_i^{(j)} = y_i - \\sum_{k \\ne j} X_{ik}\\beta_k$ is the $i$-th component of the partial residual. The part of the objective dependent on $\\beta_j$ simplifies to $\\frac{1}{2}\\beta_j^2 - \\rho_j\\beta_j + \\lambda|\\beta_j|$, where $\\rho_j = \\frac{1}{n} \\sum_{i=1}^n X_{ij} r_i^{(j)}$. The value of $\\beta_j$ that minimizes this is given by the soft-thresholding operator $\\mathcal{S}_{\\lambda}(\\cdot)$:\n$$\n\\hat{\\beta}_j \\leftarrow \\mathcal{S}_{\\lambda}(\\rho_j) = \\text{sign}(\\rho_j) \\max(|\\rho_j| - \\lambda, 0)\n$$\nThe value $\\rho_j$ can be efficiently computed without re-forming the partial residual at each step. It is equal to $\\frac{1}{n}X_j^T(y - X\\beta) + \\beta_j$, where $y-X\\beta$ is the full residual vector using the current $\\beta$ estimate.\n\nThe algorithm proceeds by initializing $\\beta = 0$ and repeatedly cycling through all features $j=0, \\dots, p-1$, applying the update rule until the change in the $\\beta$ vector between cycles falls below a small tolerance.\n\n**3. Bootstrap Stability Analysis**\n\nLASSO's feature selection can be sensitive to the specific realization of the data, especially with correlated predictors. To assess the stability of our selections, we employ bootstrap resampling. The procedure is as follows:\n1. Generate $B$ bootstrap datasets. Each dataset $(X_b, y_b)$ is formed by sampling $n$ rows with replacement from the original dataset $(X, y)$.\n2. For each bootstrap replicate $b = 1, \\dots, B$:\n    a. Pre-process the bootstrap sample: standardize the columns of $X_b$ and center $y_b$. Note that the mean and standard deviation for standardization are computed *from the current bootstrap sample* $(X_b, y_b)$.\n    b. Fit the LASSO model on the processed $(X'_b, y'_b)$ using the given regularization parameter $\\lambda$ to obtain the coefficient estimate $\\hat{\\beta}^{(b)}$.\n    c. Record an indicator $I_j^{(b)} = 1$ if the coefficient $\\hat{\\beta}_j^{(b)}$ is non-zero (i.e., $|\\hat{\\beta}_j^{(b)}| > \\epsilon$ for some small numerical tolerance $\\epsilon > 0$), and $I_j^{(b)} = 0$ otherwise.\n3. After all bootstrap fits are complete, calculate the selection frequency for each feature $j$ as the average of its indicators: $F_j = \\frac{1}{B} \\sum_{b=1}^{B} I_j^{(b)}$.\n4. The final stable feature set is defined as the collection of all indices $j$ for which the selection frequency $F_j$ meets or exceeds a stability threshold $\\tau$, i.e., $\\{j \\mid F_j \\ge \\tau\\}$.\n\nThis full pipeline is applied to each test case, which specifies a unique combination of model parameters $(\\lambda, B, \\tau, c, \\sigma)$, to produce a sorted list of stable feature indices.",
            "answer": "```python\nimport numpy as np\n\ndef soft_threshold(z, t):\n    \"\"\"Soft-thresholding operator.\"\"\"\n    return np.sign(z) * np.maximum(np.abs(z) - t, 0)\n\ndef lasso_cd(X, y, lambda_val, tol=1e-5, max_iter=1000):\n    \"\"\"\n    Solves the LASSO problem using coordinate descent.\n    Assumes X is standardized and y is centered.\n    Objective: min_beta 1/(2n) * ||y - X*beta||_2^2 + lambda_val * ||beta||_1\n    \"\"\"\n    n, p = X.shape\n    beta = np.zeros(p)\n\n    for _ in range(max_iter):\n        beta_old = beta.copy()\n        for j in range(p):\n            # The term rho_j is the simple least squares coefficient for feature j\n            # on the partial residual y - sum_{k!=j} X_k*beta_k.\n            # It can be computed efficiently from the full residual.\n            # rho_j = X_j^T * (y - X*beta + X_j*beta_j) / n\n            #       = (X_j^T * (y - X*beta) + X_j^T*X_j*beta_j) / n\n            # Since X_j is standardized, X_j^T*X_j = n.\n            # So, rho_j = (X_j^T * (y - X*beta))/n + beta_j\n            rho_j = (X[:, j].T @ (y - X @ beta)) / n + beta[j]\n            beta[j] = soft_threshold(rho_j, lambda_val)\n        \n        if np.max(np.abs(beta - beta_old)) < tol:\n            break\n            \n    return beta\n\ndef generate_data(n, p, beta_0, beta_star, c, sigma):\n    \"\"\"Generates synthetic HEA descriptor and hardness data.\"\"\"\n    # Latent variables\n    u = np.random.randn(n, 4)\n    u1, u2, u3, u4 = u[:, 0], u[:, 1], u[:, 2], u[:, 3]\n\n    X = np.zeros((n, p))\n    X[:, 0] = u1 + c * u2 + 0.1 * u4\n    X[:, 1] = u2 + c * u1 + 0.1 * u4\n    X[:, 2] = 0.5 * u2 + 0.5 * u3 + 0.1 * u4\n    X[:, 3] = -0.8 * u1 + 0.2 * u4\n    X[:, 4] = 0.1 * u1 + u3 + 0.1 * u4\n    X[:, 5] = u3 + 0.2 * u2 + 0.1 * u4\n    X[:, 6] = 0.3 * u1 - 0.1 * u3 + 0.1 * u4\n    X[:, 7] = u3 + 0.3 * u1 + 0.1 * u4\n    \n    # Generate response variable y\n    epsilon = np.random.randn(n) * sigma\n    y = beta_0 + X @ beta_star + epsilon\n    \n    return X, y\n\ndef run_case(n, p, beta_0, beta_star, lambda_val, B, tau, c, sigma):\n    \"\"\"Runs the full pipeline for one test case.\"\"\"\n    # 1. Generate the original, full dataset for this case\n    X_orig, y_orig = generate_data(n, p, beta_0, beta_star, c, sigma)\n    \n    selection_counts = np.zeros(p)\n    \n    # 2. Bootstrap stability analysis\n    for _ in range(B):\n        # Create bootstrap sample\n        indices = np.random.choice(n, size=n, replace=True)\n        X_b, y_b = X_orig[indices], y_orig[indices]\n\n        # Pre-processing for the bootstrap sample\n        y_b_centered = y_b - np.mean(y_b)\n        \n        X_b_mean = np.mean(X_b, axis=0)\n        X_b_std = np.std(X_b, axis=0)\n        # Handle columns with zero variance to avoid division by zero\n        X_b_std[X_b_std == 0] = 1.0\n        X_b_stdized = (X_b - X_b_mean) / X_b_std\n        \n        # Fit LASSO model\n        beta_hat = lasso_cd(X_b_stdized, y_b_centered, lambda_val)\n\n        # Record selected features (non-zero coefficients)\n        selected_features = np.where(np.abs(beta_hat) > 1e-7)[0]\n        selection_counts[selected_features] += 1\n        \n    # 3. Determine stable set\n    frequencies = selection_counts / B\n    stable_indices = np.where(frequencies >= tau)[0]\n    \n    return sorted(stable_indices.tolist())\n\ndef solve():\n    \"\"\"Main solver function.\"\"\"\n    # Common parameters defined in the problem\n    n = 240\n    p = 8\n    beta_0 = 2.0\n    beta_star = np.array([0.9, 0.7, 0.0, 0.0, 0.0, 0.2, 0.0, 0.5])\n\n    # Test suite from the problem statement\n    test_cases = [\n        # (lambda_val, B, tau, c, sigma)\n        (0.2, 200, 0.6, 0.3, 0.3),    # Case 1\n        (1.2, 200, 0.6, 0.3, 0.3),    # Case 2\n        (0.25, 200, 0.9, 0.9, 0.3),   # Case 3\n    ]\n\n    # A fixed random seed is used to ensure the stochastic parts of the\n    # algorithm (data generation, bootstrapping) are reproducible.\n    np.random.seed(42)\n\n    case_results = []\n    for case_params in test_cases:\n        lambda_val, B, tau, c, sigma = case_params\n        stable_set = run_case(n, p, beta_0, beta_star, lambda_val, B, tau, c, sigma)\n        \n        # Format each list as a string \"[i1,i2,...]\" with no spaces\n        list_as_string = f\"[{','.join(map(str, stable_set))}]\"\n        case_results.append(list_as_string)\n\n    # Final output must be a single line, formatted as a list of lists.\n    # e.g., [[0,1,7],[0],[5,7]]\n    print(f\"[{','.join(case_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A fundamental principle of materials science is that the physical properties of a composition are independent of the arbitrary order in which we list its constituent elements. Machine learning models for High-Entropy Alloys (HEAs) must respect this permutation invariance to be physically meaningful. In this exercise , you will construct a test harness to enforce this symmetry, building both a correctly designed invariant model and an incorrect, order-dependent baseline to solidify your understanding of physically-informed model architecture.",
            "id": "3750170",
            "problem": "You are tasked with designing and implementing a compositional invariance test harness for predictive models of High-Entropy Alloy (HEA) properties. A High-Entropy Alloy (HEA) consists of multiple elements mixed in near-equiatomic proportions; a core physical requirement in modeling such systems is that the predicted property of a composition must be independent of the arbitrary ordering of its constituent elements. In machine learning terms, the mapping from a multiset of element descriptors and their fractions to a scalar property should be invariant under permutations of the constituent index. Your program must be a complete, runnable implementation that constructs two predictors, builds a permutation-invariance test harness, and reports automated unit test results for a specified test suite.\n\nFundamental base and physical realism: For ideal mixture behavior and many mixture rules used in materials science, the target macroscopic property is modeled by aggregated statistics over constituent-specific descriptors. A canonical example is the rule of mixtures, which in one form states that a property $P$ is given by a weighted sum $P = \\sum_{i=1}^{n} w_i P_i$, where $w_i$ is a physically meaningful weight such as a mole fraction and $P_i$ is a constituent-specific quantity. This aggregation is symmetric: it depends only on the set of constituents and their weights, not their enumeration. In learning systems consistent with these laws, permutation invariance can be structurally enforced by symmetric operations such as summation over per-constituent embeddings.\n\nYour tasks:\n1. Implement two scalar-output predictors that take a composition defined by element descriptors and fractions as input:\n   - An order-invariant predictor based on a symmetric aggregator consistent with ideal mixture composition. This model must use a shared per-element embedding followed by a symmetric reduction over elements and a final readout. Specifically, use a shared element-wise transformation and a summation over elements weighted by the element fractions to obtain an intermediate representation, followed by a linear readout to produce the scalar property. This design must not depend on the positional index of the elements.\n   - An intentionally order-sensitive baseline predictor that uses position-specific parameters to form a linear combination of per-element contributions; this model must depend on the index position and therefore should fail invariance checks for multi-element inputs.\n\n2. Construct a test harness that, for a given composition, enumerates all permutations of the constituent index and checks whether the predictor output is unchanged within a specified tolerance across permutations. The test harness must report a boolean indicating pass or fail of permutation invariance for each composition.\n\n3. Use the following numerically specified parameters and test suite. All descriptors are three-dimensional vectors representing physically plausible features: Pauling electronegativity (dimensionless), covalent radius in angstroms, and an integer-like valence electron count. Fractions are mole fractions expressed as decimals; they must sum to $1$.\n   - Shared element-wise embedding parameters for the invariant model (use the same $\\phi$ for every element):\n     - Weight matrix $W_{\\phi}$:\n       $$\n       W_{\\phi} = \\begin{bmatrix}\n       0.5 & -0.2 & 0.1 \\\\\n       0.3 & 0.4 & -0.1 \\\\\n       -0.25 & 0.15 & 0.6\n       \\end{bmatrix}\n       $$\n     - Bias vector $b_{\\phi}$: $[0.05, -0.03, 0.02]$.\n     - Readout weight $w_{\\rho}$: $[1.2, -0.7, 0.5]$.\n     - Readout bias $c_{\\rho}$: $-0.1$.\n   - Position-specific weight vectors for the order-sensitive baseline (use the first $n$ vectors for an $n$-element composition in the order they appear):\n     - $b_1 = [0.1, 0.2, 0.3]$, $b_2 = [-0.2, 0.1, 0.4]$, $b_3 = [0.3, -0.1, 0.2]$, $b_4 = [0.4, 0.5, -0.3]$.\n     - Baseline bias $c_0 = 0.0$.\n   - Test suite of compositions (each case is a pair of fractions and a descriptor matrix; rows correspond to elements):\n     1. Case A (three elements, typical HEA-like):\n        - Fractions: $[0.40, 0.35, 0.25]$.\n        - Descriptors: $\\big[ [1.83, 1.26, 8.00], [1.91, 1.24, 10.00], [1.66, 1.28, 6.00] \\big]$.\n     2. Case B (single-element boundary):\n        - Fractions: $[1.00]$.\n        - Descriptors: $\\big[ [1.61, 1.43, 3.00] \\big]$.\n     3. Case C (four elements, tiny and zero fractions edge case):\n        - Fractions: $[0.001, 0.499, 0.500, 0.000]$.\n        - Descriptors: $\\big[ [1.63, 1.34, 5.00], [1.88, 1.25, 9.00], [1.54, 1.47, 4.00], [2.16, 1.39, 6.00] \\big]$.\n     4. Case D (three elements, equal fractions with duplicate descriptors):\n        - Fractions: $[\\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3}]$.\n        - Descriptors: $\\big[ [1.55, 1.27, 7.00], [1.55, 1.27, 7.00], [1.90, 1.28, 11.00] \\big]$.\n   - Tolerance for invariance checks: $\\varepsilon = 10^{-12}$.\n\n4. Program logic:\n   - Define the invariant predictor using a shared element mapping $\\phi$ followed by a symmetric sum weighted by the fractions and a linear readout. The output must be a single scalar.\n   - Define the non-invariant baseline predictor using the first $n$ position-specific weights $b_i$ for an $n$-element input in the given order; the output must be a single scalar.\n   - Implement a function that, for each test case, checks the predictor outputs across all permutations of the element index and returns a boolean indicating whether all outputs are equal within $\\varepsilon$.\n\n5. Final output format:\n   - For each test case, produce a pair of booleans $[b_{\\text{inv}}, b_{\\text{noninv}}]$ where $b_{\\text{inv}}$ indicates whether the invariant model passed permutation invariance and $b_{\\text{noninv}}$ indicates whether the baseline model passed permutation invariance.\n   - Your program should produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets, where each element is the pair for a test case. For example: $[[\\text{True},\\text{False}],[\\text{True},\\text{True}],\\dots]$.\n\nAngle units are not applicable. Physical units for intermediate descriptors are as stated, but the final outputs are booleans and therefore unitless. The test suite covers typical, boundary, and edge cases, including multi-element compositions, a single-element composition, tiny and zero fractions, and duplicate descriptors under equal fractions, to probe different facets of permutation invariance.",
            "solution": "We begin by grounding the design in physically motivated symmetry and aggregation principles. The rule of mixtures provides a canonical example where a macroscopic property $P$ is modeled by the sum $P = \\sum_{i=1}^{n} w_i P_i$, with $w_i$ denoting a physical weight such as a mole fraction and $P_i$ denoting a constituent-specific quantity. This expression is invariant under permutations of the indexing set $\\{1,2,\\dots,n\\}$, because the sum is commutative and associative. In learning systems for compositions, a structure that respects this symmetry is the Deep Sets formulation, where the function over a multiset $\\{x_i\\}_{i=1}^n$ is modeled as $f(\\{x_i\\}) = \\rho\\left(\\sum_{i=1}^n \\phi(x_i)\\right)$ for suitable functions $\\phi$ and $\\rho$. For HEA compositions, including fractions $x_i$ (here, $x_i$ denotes mole fractions, not to be confused with the descriptor $d_i$), a consistent generalization is $f(\\{(x_i,d_i)\\}) = \\rho\\left(\\sum_{i=1}^n x_i \\, \\phi(d_i)\\right)$.\n\nPermutation invariance derivation: Let $\\pi$ be any permutation of $\\{1,\\dots,n\\}$. Consider the invariant model\n$$\ny_{\\text{inv}} = \\rho\\left(\\sum_{i=1}^n x_i \\, \\phi(d_i)\\right),\n$$\nwith $\\phi$ applied identically to each element descriptor $d_i$. Under permutation, the input sequence $(x_i, d_i)$ is transformed to $(x_{\\pi(i)}, d_{\\pi(i)})$; the symmetric aggregator yields\n$$\n\\sum_{i=1}^n x_{\\pi(i)} \\, \\phi(d_{\\pi(i)}) = \\sum_{j=1}^n x_j \\, \\phi(d_j),\n$$\nafter the change of index $j = \\pi(i)$, since $\\pi$ is a bijection. Therefore $y_{\\text{inv}}$ is identical under any $\\pi$.\n\nOrder sensitivity derivation for the baseline: Let the baseline predictor use position-specific parameters $b_i$ to form a scalar\n$$\ny_{\\text{base}} = c_0 + \\sum_{i=1}^n x_i \\, \\langle b_i, d_i \\rangle,\n$$\nwhere $\\langle \\cdot, \\cdot \\rangle$ denotes the Euclidean inner product in $\\mathbb{R}^3$. Under a permutation $\\pi$, this becomes\n$$\ny_{\\text{base}}^{(\\pi)} = c_0 + \\sum_{i=1}^n x_{\\pi(i)} \\, \\langle b_i, d_{\\pi(i)} \\rangle.\n$$\nUnless the $b_i$ are all identical or special cancellations occur for the specific descriptors and fractions, $y_{\\text{base}}^{(\\pi)} \\neq y_{\\text{base}}$. Therefore, this predictor is generally order-sensitive and will fail permutation-invariance tests for multi-element inputs.\n\nAlgorithmic design:\n- Implement $\\phi(d) = W_{\\phi} d + b_{\\phi}$ with $W_{\\phi} \\in \\mathbb{R}^{3 \\times 3}$ and $b_{\\phi} \\in \\mathbb{R}^3$ as given. This is a shared affine map applied to each element descriptor $d_i$.\n- Implement $\\rho(v) = \\langle w_{\\rho}, v \\rangle + c_{\\rho}$ with $w_{\\rho} \\in \\mathbb{R}^3$ and $c_{\\rho} \\in \\mathbb{R}$ as given.\n- The invariant predictor computes $v = \\sum_{i=1}^n x_i \\, \\phi(d_i)$ and then $y_{\\text{inv}} = \\rho(v)$.\n- The baseline predictor computes $y_{\\text{base}} = c_0 + \\sum_{i=1}^n x_i \\, \\langle b_i, d_i \\rangle$ using the first $n$ position-specific vectors $b_i$ in the current order.\n- The invariance test harness: For each composition with fractions $(x_1,\\dots,x_n)$ and descriptors $(d_1,\\dots,d_n)$, enumerate all permutations $\\pi$ of $\\{1,\\dots,n\\}$, compute the model output for each permuted arrangement, and check that all outputs are equal to the reference output within a tolerance $\\varepsilon = 10^{-12}$. Formally, for outputs $y^{(\\pi)}$, compute $\\max_{\\pi} \\left| y^{(\\pi)} - y^{(\\text{id})} \\right|$ and declare pass if this maximum is less than or equal to $\\varepsilon$.\n\nTest suite coverage:\n- Case A probes a typical multi-element HEA-like composition with realistic features and heterogeneous fractions; the invariant model should pass and the baseline should fail.\n- Case B is a single-element boundary case; both models should pass because there is only one permutation.\n- Case C includes tiny and zero fractions, exercising numerical stability and ensuring elements with zero fraction have no effect; the invariant model should pass and the baseline should fail unless there is coincidental equality across all permutations, which is highly unlikely.\n- Case D includes equal fractions and duplicate descriptors, probing whether invariance holds even when swapping identical elements; the invariant model should pass and the baseline should still fail because position-specific weights create dependence that does not vanish across all permutations.\n\nNumerical stability: Summation over floating-point values can introduce minor ordering-dependent round-off differences. The tolerance $\\varepsilon = 10^{-12}$ is selected to be safely above machine epsilon for double precision while being strict enough to catch genuine order dependence. The invariant model, by construction, is symmetric; any differences across permutations should be at most round-off, hence below $\\varepsilon$. The baseline model is expected to exceed $\\varepsilon$ in multi-element cases.\n\nFinal output: For each case, produce $[b_{\\text{inv}}, b_{\\text{noninv}}]$, and print the list over all cases on a single line: $[[b_{\\text{inv}}^{A}, b_{\\text{noninv}}^{A}], [b_{\\text{inv}}^{B}, b_{\\text{noninv}}^{B}], [b_{\\text{inv}}^{C}, b_{\\text{noninv}}^{C}], [b_{\\text{inv}}^{D}, b_{\\text{noninv}}^{D}]]$.\n\nThis design integrates foundational physical symmetry with algorithmic checks to enforce and verify permutation invariance in HEA property prediction models, aligning with advanced graduate-level expectations for the correctness and robustness of machine learning models in complex materials modeling.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport itertools\n\ndef deep_sets_predict(fractions, descriptors):\n    \"\"\"\n    Order-invariant predictor using shared element-wise embedding phi and\n    symmetric aggregation followed by a linear readout rho.\n    \"\"\"\n    W_phi = np.array([\n        [0.5,  -0.2,  0.1],\n        [0.3,   0.4, -0.1],\n        [-0.25, 0.15, 0.6]\n    ], dtype=float)\n    b_phi = np.array([0.05, -0.03, 0.02], dtype=float)\n    w_rho = np.array([1.2, -0.7, 0.5], dtype=float)\n    c_rho = -0.1\n\n    fractions = np.asarray(fractions, dtype=float)\n    descriptors = np.asarray(descriptors, dtype=float)\n    # Apply shared embedding phi to each descriptor\n    phi_vals = (descriptors @ W_phi.T) + b_phi  # shape (n, 3)\n    # Symmetric aggregation weighted by fractions\n    v = np.einsum('i,ij->j', fractions, phi_vals)  # shape (3,)\n    # Linear readout\n    y = float(np.dot(w_rho, v) + c_rho)\n    return y\n\ndef baseline_order_sensitive_predict(fractions, descriptors):\n    \"\"\"\n    Order-sensitive baseline predictor using position-specific weights b_i.\n    Uses first n weight vectors for an n-element composition in given order.\n    \"\"\"\n    b_list = [\n        np.array([0.1, 0.2, 0.3], dtype=float),\n        np.array([-0.2, 0.1, 0.4], dtype=float),\n        np.array([0.3, -0.1, 0.2], dtype=float),\n        np.array([0.4, 0.5, -0.3], dtype=float),\n    ]\n    c0 = 0.0\n\n    fractions = np.asarray(fractions, dtype=float)\n    descriptors = np.asarray(descriptors, dtype=float)\n    n = len(fractions)\n    assert descriptors.shape == (n, 3), \"Descriptors shape must be (n, 3)\"\n    # Use the first n b_i vectors in order\n    contrib = 0.0\n    for i in range(n):\n        contrib += fractions[i] * float(np.dot(b_list[i], descriptors[i]))\n    y = c0 + contrib\n    return y\n\ndef invariance_test(model_fn, fractions, descriptors, eps=1e-12):\n    \"\"\"\n    Returns True if model_fn output is invariant within eps across all permutations\n    of elements (jointly permuting fractions and descriptors).\n    \"\"\"\n    fractions = np.asarray(fractions, dtype=float)\n    descriptors = np.asarray(descriptors, dtype=float)\n    n = len(fractions)\n    # Reference output on identity ordering\n    y_ref = model_fn(fractions, descriptors)\n    max_diff = 0.0\n    # Enumerate all permutations\n    for perm in itertools.permutations(range(n)):\n        perm = np.array(perm, dtype=int)\n        f_perm = fractions[perm]\n        d_perm = descriptors[perm]\n        y_perm = model_fn(f_perm, d_perm)\n        diff = abs(y_perm - y_ref)\n        if diff > max_diff:\n            max_diff = diff\n        # Early exit if above tolerance to save time\n        if max_diff > eps:\n            return False\n    return True\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A: 3 elements, typical\n        (\n            [0.40, 0.35, 0.25],\n            [\n                [1.83, 1.26,  8.00],\n                [1.91, 1.24, 10.00],\n                [1.66, 1.28,  6.00],\n            ],\n        ),\n        # Case B: single element boundary\n        (\n            [1.00],\n            [\n                [1.61, 1.43, 3.00],\n            ],\n        ),\n        # Case C: 4 elements with tiny and zero fractions\n        (\n            [0.001, 0.499, 0.500, 0.000],\n            [\n                [1.63, 1.34,  5.00],\n                [1.88, 1.25,  9.00],\n                [1.54, 1.47,  4.00],\n                [2.16, 1.39,  6.00],\n            ],\n        ),\n        # Case D: 3 elements, equal fractions with duplicate descriptors\n        (\n            [1.0/3.0, 1.0/3.0, 1.0/3.0],\n            [\n                [1.55, 1.27,  7.00],\n                [1.55, 1.27,  7.00],\n                [1.90, 1.28, 11.00],\n            ],\n        ),\n    ]\n\n    eps = 1e-12\n    results = []\n    for fractions, descriptors in test_cases:\n        inv_pass = invariance_test(deep_sets_predict, fractions, descriptors, eps)\n        base_pass = invariance_test(baseline_order_sensitive_predict, fractions, descriptors, eps)\n        results.append([inv_pass, base_pass])\n\n    # Final print statement in the exact required format.\n    # Single line, comma-separated list enclosed in square brackets.\n    # We format booleans as their Python literals True/False.\n    print(f\"[{','.join([str(pair).replace(' ', '') for pair in results])}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "While a single point prediction from a model is useful, a prediction accompanied by a measure of confidence is far more powerful for scientific discovery and engineering applications. Uncertainty quantification (UQ) provides a rigorous framework for assessing the reliability of a model's output by propagating uncertainties from its inputs and parameters. This advanced practice  challenges you to analytically propagate Gaussian uncertainties through a machine learning-augmented model for HEA mixing enthalpy, providing direct experience with the statistical foundations of predictive reliability.",
            "id": "3750165",
            "problem": "A High-Entropy Alloy (HEA) is defined as an alloy composed of multiple principal elements with near-equimolar compositions. Consider a quinary High-Entropy Alloy (HEA) with elements indexed as $0,1,2,3,4$ corresponding to cobalt (Co), chromium (Cr), iron (Fe), nickel (Ni), and manganese (Mn). The mixing enthalpy $\\Delta H_{\\text{mix}}$ is modeled using a Miedema-like pairwise interaction approximation, where the contribution of each binary pair is weighted by the product of their molar fractions. Specifically, under the ideal solution approximation, the mixing enthalpy is modeled as the sum of binary interaction enthalpies:\n$$\n\\Delta H_{\\text{mix}} = \\sum_{0 \\leq i &lt; j \\leq 4} x_i x_j \\, \\Omega_{ij},\n$$\nwhere $x_i$ is the molar fraction of element $i$ (dimensionless), and $\\Omega_{ij}$ is the binary interaction enthalpy (in $\\text{kJ/mol}$) between elements $i$ and $j$. The binary interaction enthalpy $\\Omega_{ij}$ is predicted via a linear Machine Learning (ML) model trained on prior binary alloy data:\n$$\n\\Omega_{ij} = \\mathbf{w}^\\top \\boldsymbol{\\phi}_{ij} + b,\n$$\nwhere $\\boldsymbol{\\phi}_{ij} \\in \\mathbb{R}^3$ is a feature vector of pairwise descriptors, $\\mathbf{w} \\in \\mathbb{R}^3$ are the model coefficients, and $b$ is the bias term. The model parameters and features are uncertain and modeled as Gaussian random variables. All uncertainties are expressed as means and covariances, and independence assumptions are specified below.\n\nModel parameters:\n- Mean coefficient vector $\\boldsymbol{\\mu}_{\\mathbf{w}} = [ -7.5, 18.0, -2.5 ]$ (units: $\\text{kJ/mol}$ per unit of each feature component).\n- Coefficient covariance matrix $\\boldsymbol{\\Sigma}_{\\mathbf{w}} = \\begin{bmatrix} 1.96 &amp; 0.20 &amp; -0.10 \\\\ 0.20 &amp; 4.00 &amp; 0.15 \\\\ -0.10 &amp; 0.15 &amp; 0.81 \\end{bmatrix}$.\n- Bias mean $\\mu_b = 0.3$ (units: $\\text{kJ/mol}$).\n- Bias variance $\\sigma_b^2 = 0.04$ (units: $(\\text{kJ/mol})^2$).\n\nPairwise features:\nEach binary pair $(i,j)$ is associated with a $3$-component feature vector $\\boldsymbol{\\phi}_{ij} = [\\Delta \\chi, \\Delta r / r, \\Delta \\text{VEC}]$, consisting of electronegativity difference (dimensionless), relative atomic size mismatch (dimensionless), and valence electron concentration difference (dimensionless). The feature means for the ten unique pairs are:\n- $(0,1)$: $[0.22, 0.0237, 3.0]$\n- $(0,2)$: $[0.05, 0.0080, 1.0]$\n- $(0,3)$: $[0.03, 0.0080, 1.0]$\n- $(0,4)$: $[0.33, 0.0159, 2.0]$\n- $(1,2)$: $[0.17, 0.0157, 2.0]$\n- $(1,3)$: $[0.25, 0.0317, 4.0]$\n- $(1,4)$: $[0.11, 0.0078, 1.0]$\n- $(2,3)$: $[0.08, 0.0160, 2.0]$\n- $(2,4)$: $[0.28, 0.0079, 1.0]$\n- $(3,4)$: $[0.36, 0.0239, 3.0]$\n\nFeature covariance matrices $\\boldsymbol{\\Sigma}_{\\boldsymbol{\\phi}_{ij}}$ (dimensionless) are specified as follows. For most pairs, use the base diagonal covariance:\n- Base: $\\begin{bmatrix} 2.5\\times 10^{-5} &amp; 0 &amp; 0 \\\\ 0 &amp; 4.0\\times 10^{-6} &amp; 0 \\\\ 0 &amp; 0 &amp; 4.0\\times 10^{-2} \\end{bmatrix}$.\nFor specific pairs with correlated uncertainties:\n- $(1,3)$: $\\begin{bmatrix} 2.5\\times 10^{-5} &amp; 1.0\\times 10^{-6} &amp; 1.0\\times 10^{-3} \\\\ 1.0\\times 10^{-6} &amp; 4.0\\times 10^{-6} &amp; -5.0\\times 10^{-4} \\\\ 1.0\\times 10^{-3} &amp; -5.0\\times 10^{-4} &amp; 4.0\\times 10^{-2} \\end{bmatrix}$.\n- $(0,4)$: $\\begin{bmatrix} 2.5\\times 10^{-5} &amp; -5.0\\times 10^{-7} &amp; 8.0\\times 10^{-4} \\\\ -5.0\\times 10^{-7} &amp; 4.0\\times 10^{-6} &amp; 3.0\\times 10^{-4} \\\\ 8.0\\times 10^{-4} &amp; 3.0\\times 10^{-4} &amp; 4.0\\times 10^{-2} \\end{bmatrix}$.\nAll other pairs use the base covariance.\n\nStatistical assumptions:\n- The coefficient vector $\\mathbf{w}$ is Gaussian with mean $\\boldsymbol{\\mu}_{\\mathbf{w}}$ and covariance $\\boldsymbol{\\Sigma}_{\\mathbf{w}}$.\n- The bias $b$ is Gaussian with mean $\\mu_b$ and variance $\\sigma_b^2$.\n- The feature vectors $\\boldsymbol{\\phi}_{ij}$ are Gaussian with means given above and covariances $\\boldsymbol{\\Sigma}_{\\boldsymbol{\\phi}_{ij}}$.\n- The random variables $\\mathbf{w}$, $b$, and all $\\boldsymbol{\\phi}_{ij}$ are mutually independent, and different $\\boldsymbol{\\phi}_{ij}$ across pairs are independent of each other.\n- For test variation, feature covariances may be isotropically scaled by a scalar factor $s \\ge 0$, which multiplies each $\\boldsymbol{\\Sigma}_{\\boldsymbol{\\phi}_{ij}}$ as $s \\, \\boldsymbol{\\Sigma}_{\\boldsymbol{\\phi}_{ij}}$.\n\nTask:\n- Implement a program that computes the mean and standard deviation of $\\Delta H_{\\text{mix}}$ in $\\text{kJ/mol}$ for each test case, by analytically propagating the Gaussian uncertainties from $\\mathbf{w}$, $b$, and $\\boldsymbol{\\phi}_{ij}$ to $\\Delta H_{\\text{mix}}$ using the independence assumptions listed above.\n- The program must validate that the composition vector $\\mathbf{x} = [x_0,x_1,x_2,x_3,x_4]$ satisfies $\\sum_{i=0}^4 x_i = 1$ within numerical tolerance. If not, the program should normalize $\\mathbf{x}$ by its sum to enforce $\\sum_i x_i = 1$; this normalization step must be deterministic and documented in the code comments.\n\nRequired outputs:\n- For each test case, output a pair consisting of the mean and the standard deviation of $\\Delta H_{\\text{mix}}$ in $\\text{kJ/mol}$, rounded to three decimal places, as floats.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each item is a two-element list representing one test case. For example: $[[\\text{mean}_1,\\text{std}_1],[\\text{mean}_2,\\text{std}_2],\\dots]$.\n\nTest suite:\nProvide results for the following five test cases. Each case is given by a composition vector $\\mathbf{x}$ and a covariance scaling factor $s$.\n- Case $1$: $\\mathbf{x} = [0.2, 0.2, 0.2, 0.2, 0.2]$, $s = 1.0$.\n- Case $2$: $\\mathbf{x} = [0.35, 0.25, 0.2, 0.15, 0.05]$, $s = 1.0$.\n- Case $3$: $\\mathbf{x} = [0.4, 0.1, 0.2, 0.25, 0.05]$, $s = 1.0$.\n- Case $4$: $\\mathbf{x} = [0.2, 0.2, 0.2, 0.2, 0.2]$, $s = 0.0$.\n- Case $5$: $\\mathbf{x} = [0.2, 0.2, 0.2, 0.2, 0.2]$, $s = 2.0$.\n\nAngle units are not involved in this computation. Express all final numerical results for $\\Delta H_{\\text{mix}}$ and its uncertainty in $\\text{kJ/mol}$, rounded to three decimal places.",
            "solution": "The problem requires the calculation of the mean and standard deviation of the mixing enthalpy, $\\Delta H_{\\text{mix}}$, for a quinary high-entropy alloy. The uncertainty in the model parameters and feature descriptors must be propagated analytically.\n\nThe mixing enthalpy is given by\n$$\n\\Delta H_{\\text{mix}} = \\sum_{0 \\leq i < j \\leq 4} x_i x_j \\, \\Omega_{ij}\n$$\nwhere $x_i$ are the deterministic molar fractions and $\\Omega_{ij}$ are random variables representing the binary interaction enthalpies. Each $\\Omega_{ij}$ is modeled by a linear function of a feature vector $\\boldsymbol{\\phi}_{ij}$:\n$$\n\\Omega_{ij} = \\mathbf{w}^\\top \\boldsymbol{\\phi}_{ij} + b\n$$\nThe model coefficients $\\mathbf{w}$, bias $b$, and feature vectors $\\boldsymbol{\\phi}_{ij}$ are all modeled as mutually independent Gaussian random variables.\n\nWe can rewrite the expression for $\\Delta H_{\\text{mix}}$ by substituting the model for $\\Omega_{ij}$:\n$$\n\\Delta H_{\\text{mix}} = \\sum_{i<j} x_i x_j (\\mathbf{w}^\\top \\boldsymbol{\\phi}_{ij} + b)\n$$\nBy distributing the summation, we can separate the terms involving $\\mathbf{w}$, $\\boldsymbol{\\phi}_{ij}$, and $b$:\n$$\n\\Delta H_{\\text{mix}} = \\left( \\sum_{i<j} x_i x_j \\mathbf{w}^\\top \\boldsymbol{\\phi}_{ij} \\right) + \\left( \\sum_{i<j} x_i x_j b \\right)\n$$\nWe can factor out the common terms $\\mathbf{w}$ and $b$:\n$$\n\\Delta H_{\\text{mix}} = \\mathbf{w}^\\top \\left( \\sum_{i<j} x_i x_j \\boldsymbol{\\phi}_{ij} \\right) + b \\left( \\sum_{i<j} x_i x_j \\right)\n$$\nLet us define two quantities that depend only on the composition $\\mathbf{x}$ and the feature vectors $\\boldsymbol{\\phi}_{ij}$:\n$C_x = \\sum_{i<j} x_i x_j$\n$\\boldsymbol{\\Phi}_x = \\sum_{i<j} x_i x_j \\boldsymbol{\\phi}_{ij}$\n\nThe expression for the mixing enthalpy simplifies to:\n$$\n\\Delta H_{\\text{mix}} = \\mathbf{w}^\\top \\boldsymbol{\\Phi}_x + b C_x\n$$\nHere, $C_x$ is a scalar constant for a given composition $\\mathbf{x}$. $\\boldsymbol{\\Phi}_x$ is a random vector, since it is a linear combination of the random vectors $\\boldsymbol{\\phi}_{ij}$. Based on the problem's independence assumptions, the random variables $\\mathbf{w}$, $b$, and $\\boldsymbol{\\Phi}_x$ are mutually independent.\n\n**1. Mean of $\\Delta H_{\\text{mix}}$**\n\nUsing the linearity of the expectation operator, we can compute the mean of $\\Delta H_{\\text{mix}}$:\n$$\nE[\\Delta H_{\\text{mix}}] = E[\\mathbf{w}^\\top \\boldsymbol{\\Phi}_x + b C_x] = E[\\mathbf{w}^\\top \\boldsymbol{\\Phi}_x] + E[b C_x]\n$$\nDue to the independence of $\\mathbf{w}$ and $\\boldsymbol{\\Phi}_x$, and since $C_x$ is a constant:\n$$\nE[\\Delta H_{\\text{mix}}] = E[\\mathbf{w}]^\\top E[\\boldsymbol{\\Phi}_x] + E[b] C_x\n$$\nLet $\\boldsymbol{\\mu}_{\\mathbf{w}} = E[\\mathbf{w}]$ and $\\mu_b = E[b]$. The mean of $\\boldsymbol{\\Phi}_x$ is:\n$$\n\\boldsymbol{\\mu}_{\\boldsymbol{\\Phi}_x} = E[\\boldsymbol{\\Phi}_x] = E\\left[\\sum_{i<j} x_i x_j \\boldsymbol{\\phi}_{ij}\\right] = \\sum_{i<j} x_i x_j E[\\boldsymbol{\\phi}_{ij}] = \\sum_{i<j} x_i x_j \\boldsymbol{\\mu}_{\\boldsymbol{\\phi}_{ij}}\n$$\nSubstituting these into the expression for the mean gives:\n$$\nE[\\Delta H_{\\text{mix}}] = \\boldsymbol{\\mu}_{\\mathbf{w}}^\\top \\boldsymbol{\\mu}_{\\boldsymbol{\\Phi}_x} + \\mu_b C_x\n$$\n\n**2. Variance of $\\Delta H_{\\text{mix}}$**\n\nThe total variance is the variance of the sum of two independent random variables, $A = \\mathbf{w}^\\top \\boldsymbol{\\Phi}_x$ and $B = b C_x$:\n$$\n\\text{Var}[\\Delta H_{\\text{mix}}] = \\text{Var}[A + B] = \\text{Var}[A] + \\text{Var}[B]\n$$\nThe variance of term $B$ is straightforward:\n$$\n\\text{Var}[B] = \\text{Var}[b C_x] = C_x^2 \\text{Var}[b] = C_x^2 \\sigma_b^2\n$$\nTo find the variance of term $A = \\mathbf{w}^\\top \\boldsymbol{\\Phi}_x$, which is the dot product of two independent random vectors, we use the law of total variance: $\\text{Var}(Y) = E[\\text{Var}(Y|X)] + \\text{Var}[E(Y|X)]$. Let $Y=A$ and $X=\\mathbf{w}$.\n\nFirst, we compute the conditional expectation and its variance:\n$$\nE[A | \\mathbf{w}] = E[\\mathbf{w}^\\top \\boldsymbol{\\Phi}_x | \\mathbf{w}] = \\mathbf{w}^\\top E[\\boldsymbol{\\Phi}_x] = \\mathbf{w}^\\top \\boldsymbol{\\mu}_{\\boldsymbol{\\Phi}_x}\n$$\nThe variance of this term is:\n$$\n\\text{Var}[E[A | \\mathbf{w}]] = \\text{Var}[\\mathbf{w}^\\top \\boldsymbol{\\mu}_{\\boldsymbol{\\Phi}_x}] = \\boldsymbol{\\mu}_{\\boldsymbol{\\Phi}_x}^\\top \\text{Var}[\\mathbf{w}] \\boldsymbol{\\mu}_{\\boldsymbol{\\Phi}_x} = \\boldsymbol{\\mu}_{\\boldsymbol{\\Phi}_x}^\\top \\boldsymbol{\\Sigma}_{\\mathbf{w}} \\boldsymbol{\\mu}_{\\boldsymbol{\\Phi}_x}\n$$\nwhere $\\boldsymbol{\\Sigma}_{\\mathbf{w}}$ is the covariance matrix of $\\mathbf{w}$.\n\nNext, we compute the conditional variance and its expectation:\n$$\n\\text{Var}[A | \\mathbf{w}] = \\text{Var}[\\mathbf{w}^\\top \\boldsymbol{\\Phi}_x | \\mathbf{w}] = \\mathbf{w}^\\top \\text{Var}[\\boldsymbol{\\Phi}_x] \\mathbf{w} = \\mathbf{w}^\\top \\boldsymbol{\\Sigma}_{\\boldsymbol{\\Phi}_x} \\mathbf{w}\n$$\nwhere $\\boldsymbol{\\Sigma}_{\\boldsymbol{\\Phi}_x}$ is the covariance matrix of $\\boldsymbol{\\Phi}_x$. Since the feature vectors $\\boldsymbol{\\phi}_{ij}$ are independent for different pairs $(i,j)$, the covariance of their weighted sum is:\n$$\n\\boldsymbol{\\Sigma}_{\\boldsymbol{\\Phi}_x} = \\text{Var}\\left[\\sum_{i<j} x_i x_j \\boldsymbol{\\phi}_{ij}\\right] = \\sum_{i<j} (x_i x_j)^2 \\text{Var}[\\boldsymbol{\\phi}_{ij}] = \\sum_{i<j} (x_i x_j)^2 (s \\boldsymbol{\\Sigma}_{\\boldsymbol{\\phi}_{ij}}) = s \\sum_{i<j} (x_i x_j)^2 \\boldsymbol{\\Sigma}_{\\boldsymbol{\\phi}_{ij}}\n$$\nwhere $s$ is the given scaling factor.\nThe expectation of the conditional variance is the expectation of the quadratic form $\\mathbf{w}^\\top \\boldsymbol{\\Sigma}_{\\boldsymbol{\\Phi}_x} \\mathbf{w}$:\n$$\nE[\\text{Var}[A | \\mathbf{w}]] = E[\\mathbf{w}^\\top \\boldsymbol{\\Sigma}_{\\boldsymbol{\\Phi}_x} \\mathbf{w}] = \\text{Tr}(\\boldsymbol{\\Sigma}_{\\boldsymbol{\\Phi}_x} \\boldsymbol{\\Sigma}_{\\mathbf{w}}) + \\boldsymbol{\\mu}_{\\mathbf{w}}^\\top \\boldsymbol{\\Sigma}_{\\boldsymbol{\\Phi}_x} \\boldsymbol{\\mu}_{\\mathbf{w}}\n$$\nCombining the two parts, the variance of $A$ is:\n$$\n\\text{Var}[A] = \\boldsymbol{\\mu}_{\\boldsymbol{\\Phi}_x}^\\top \\boldsymbol{\\Sigma}_{\\mathbf{w}} \\boldsymbol{\\mu}_{\\boldsymbol{\\Phi}_x} + \\boldsymbol{\\mu}_{\\mathbf{w}}^\\top \\boldsymbol{\\Sigma}_{\\boldsymbol{\\Phi}_x} \\boldsymbol{\\mu}_{\\mathbf{w}} + \\text{Tr}(\\boldsymbol{\\Sigma}_{\\mathbf{w}} \\boldsymbol{\\Sigma}_{\\boldsymbol{\\Phi}_x})\n$$\nFinally, the total variance of $\\Delta H_{\\text{mix}}$ is:\n$$\n\\text{Var}[\\Delta H_{\\text{mix}}] = \\underbrace{\\boldsymbol{\\mu}_{\\boldsymbol{\\Phi}_x}^\\top \\boldsymbol{\\Sigma}_{\\mathbf{w}} \\boldsymbol{\\mu}_{\\boldsymbol{\\Phi}_x} + \\boldsymbol{\\mu}_{\\mathbf{w}}^\\top \\boldsymbol{\\Sigma}_{\\boldsymbol{\\Phi}_x} \\boldsymbol{\\mu}_{\\mathbf{w}} + \\text{Tr}(\\boldsymbol{\\Sigma}_{\\mathbf{w}} \\boldsymbol{\\Sigma}_{\\boldsymbol{\\Phi}_x})}_{\\text{Var}[\\mathbf{w}^\\top \\boldsymbol{\\Phi}_x]} + \\underbrace{C_x^2 \\sigma_b^2}_{\\text{Var}[b C_x]}\n$$\nThe standard deviation is $\\text{std}[\\Delta H_{\\text{mix}}] = \\sqrt{\\text{Var}[\\Delta H_{\\text{mix}}]}$.\n\nThese formulae allow for the analytical computation of the mean and standard deviation of $\\Delta H_{\\text{mix}}$ for each test case.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the mean and standard deviation of HEA mixing enthalpy\n    by analytically propagating Gaussian uncertainties.\n    \"\"\"\n    # Model parameters from the problem statement\n    mu_w = np.array([-7.5, 18.0, -2.5])\n    Sigma_w = np.array([\n        [1.96, 0.20, -0.10],\n        [0.20, 4.00, 0.15],\n        [-0.10, 0.15, 0.81]\n    ])\n    mu_b = 0.3\n    sigma_b_sq = 0.04\n\n    # Pairwise feature means\n    mu_phi_data = {\n        (0, 1): np.array([0.22, 0.0237, 3.0]),\n        (0, 2): np.array([0.05, 0.0080, 1.0]),\n        (0, 3): np.array([0.03, 0.0080, 1.0]),\n        (0, 4): np.array([0.33, 0.0159, 2.0]),\n        (1, 2): np.array([0.17, 0.0157, 2.0]),\n        (1, 3): np.array([0.25, 0.0317, 4.0]),\n        (1, 4): np.array([0.11, 0.0078, 1.0]),\n        (2, 3): np.array([0.08, 0.0160, 2.0]),\n        (2, 4): np.array([0.28, 0.0079, 1.0]),\n        (3, 4): np.array([0.36, 0.0239, 3.0])\n    }\n\n    # Pairwise feature covariances\n    Sigma_phi_base = np.array([\n        [2.5e-5, 0.0, 0.0],\n        [0.0, 4.0e-6, 0.0],\n        [0.0, 0.0, 4.0e-2]\n    ])\n    Sigma_phi_special = {\n        (1, 3): np.array([\n            [2.5e-5, 1.0e-6, 1.0e-3],\n            [1.0e-6, 4.0e-6, -5.0e-4],\n            [1.0e-3, -5.0e-4, 4.0e-2]\n        ]),\n        (0, 4): np.array([\n            [2.5e-5, -5.0e-7, 8.0e-4],\n            [-5.0e-7, 4.0e-6, 3.0e-4],\n            [8.0e-4, 3.0e-4, 4.0e-2]\n        ])\n    }\n\n    # Generate a complete dictionary of feature covariance matrices for all pairs\n    all_pairs = [(i, j) for i in range(5) for j in range(i + 1, 5)]\n    Sigma_phi_data = {pair: Sigma_phi_special.get(pair, Sigma_phi_base) for pair in all_pairs}\n\n    # Test cases defined in the problem\n    test_cases = [\n        (np.array([0.2, 0.2, 0.2, 0.2, 0.2]), 1.0),\n        (np.array([0.35, 0.25, 0.2, 0.15, 0.05]), 1.0),\n        (np.array([0.4, 0.1, 0.2, 0.25, 0.05]), 1.0),\n        (np.array([0.2, 0.2, 0.2, 0.2, 0.2]), 0.0),\n        (np.array([0.2, 0.2, 0.2, 0.2, 0.2]), 2.0)\n    ]\n    \n    results = []\n    \n    for x_raw, s in test_cases:\n        # Normalize the composition vector x to ensure it sums to 1\n        # This is a deterministic step as required by the problem.\n        x_sum = np.sum(x_raw)\n        if not np.isclose(x_sum, 1.0):\n            x = x_raw / x_sum\n        else:\n            x = x_raw\n\n        # Calculate composition-dependent intermediates C_x, mu_Phi_x, and Sigma_Phi_x\n        C_x = 0.0\n        mu_Phi_x = np.zeros(3)\n        Sigma_Phi_x = np.zeros((3, 3))\n        \n        for i in range(5):\n            for j in range(i + 1, 5):\n                pair = (i, j)\n                xixj = x[i] * x[j]\n                \n                C_x += xixj\n                mu_Phi_x += xixj * mu_phi_data[pair]\n                Sigma_Phi_x += (xixj**2) * Sigma_phi_data[pair]\n\n        # Apply the scalar scaling factor to the feature covariance\n        Sigma_Phi_x *= s\n\n        # 1. Calculate the mean of Delta H_mix\n        mean_H = mu_w @ mu_Phi_x + C_x * mu_b\n        \n        # 2. Calculate the variance of Delta H_mix\n        # Variance of the term involving w and phi\n        term1_var_w_phi = mu_Phi_x.T @ Sigma_w @ mu_Phi_x\n        term2_var_w_phi = mu_w.T @ Sigma_Phi_x @ mu_w\n        term3_var_w_phi = np.trace(Sigma_w @ Sigma_Phi_x)\n        var_w_phi = term1_var_w_phi + term2_var_w_phi + term3_var_w_phi\n        \n        # Variance of the term involving b\n        var_b = (C_x**2) * sigma_b_sq\n        \n        # Total variance\n        var_H = var_w_phi + var_b\n        \n        # 3. Calculate the standard deviation\n        std_H = np.sqrt(var_H)\n        \n        # Append rounded results for the current test case\n        results.append([round(mean_H, 3), round(std_H, 3)])\n\n    # Format the final output string\n    output_str = \"[\" + \",\".join([f\"[{r[0]},{r[1]}]\" for r in results]) + \"]\"\n    print(output_str)\n\nsolve()\n```"
        }
    ]
}