## 引言
在[材料科学](@entry_id:152226)领域，发现具有特定性能的新材料是推动技术进步的核心驱动力。传统上，这一过程依赖于耗时耗力的实验试错法。然而，随着计算能力的飞速发展和海量数据的积累，一个被称为“数据驱动[材料发现](@entry_id:159066)”的革命性[范式](@entry_id:161181)正在兴起。它融合了物理学、化学、计算机科学和[材料科学](@entry_id:152226)，旨在通过机器学习和自动化计算来系统性地、高效地探索广阔的化学空间，从而以前所未有的速度预测和设计新材料。

尽管前景广阔，但要成功实施[数据驱动的发现](@entry_id:274863)，需要掌握一套复杂的跨学科知识体系。如何从量子力学层面定义和计算材料的稳定性？如何将[原子结构](@entry_id:137190)转化为机器可以理解的语言？如何构建能够捕捉复杂物理规律的预测模型？又如何智能地指导计算资源，以在近乎无限的候选者中找到“宝藏”？本文旨在系统性地回答这些问题，为读者构建一个关于数据驱动[材料发现](@entry_id:159066)的完整知识框架。

在接下来的内容中，我们将分三步深入这一领域。第一章 **“原理与机制”** 将为您揭开数据驱动发现的底层逻辑，从作为筛选基础的[热力学稳定性](@entry_id:142877)，到描述材料的数学语言，再到学习原子结构的现代机器学习架构，最后是驱动整个发现循环的[主动学习](@entry_id:157812)策略。第二章 **“应用与[交叉](@entry_id:147634)学科联系”** 将通过一系列真实的研究案例，展示这些原理如何应用于解决实际问题，如构建多保真度预测模型、设计复杂的[高通量筛选](@entry_id:271166)流程以及管理实验数据。最后，在 **“动手实践”** 部分，您将有机会通过具体的编程练习，将理论知识转化为实践技能。让我们一同开启这场加速材料创新的智慧之旅。

## 原理与机制

本章深入探讨了数据驱动[材料发现](@entry_id:159066)背后的核心科学原理与计算机制。在前一章介绍背景之后，我们现在将系统地剖析构成这一跨学科领域基础的四个关键支柱：首先，我们将阐述如何利用[第一性原理计算](@entry_id:198754)来定义和评估材料的热力学稳定性，这是[高通量筛选](@entry_id:271166)的根本目标。其次，我们将探讨如何将原子结构转化为[机器学习模型](@entry_id:262335)可以理解的数学语言，即描述符，并强调对称性与不变性在其中的核心作用。再次，我们将深入研究为原子系统量身定制的现代机器学习架构，特别是图神经网络，并区分不变性与[等变性](@entry_id:636671)在预测不同物理量时的重要性。最后，我们将讨论驱动整个发现循环的引擎：一方面是确保数据可靠性与可复现性的自动化工作流与[数据溯源](@entry_id:175012)，另一方面是指导我们高效探索广阔化学空间的主动学习与[多目标优化](@entry_id:637420)策略。

### 计算材料筛选的基础：[热力学稳定性](@entry_id:142877)

在[材料科学](@entry_id:152226)中，一个核心问题是确定给定[化学成分](@entry_id:138867)下哪些[晶体结构](@entry_id:140373)是[热力学](@entry_id:141121)稳定的。在低温（接近 $0 \ \mathrm{K}$）和环境压力下，一个系统的稳定性由其能量决定。[数据驱动的发现](@entry_id:274863)流程通常利用量子力学计算，如**[密度泛函理论](@entry_id:139027) (DFT)**，来估算材料的总能量。然而，绝对总能量值本身依赖于计算细节且不具备直接的物理意义。更有意义的物理量是**生成能 (formation energy)**，它量化了由其构成元素形成一摩尔化合物时释放或吸收的能量。

对于一个[化学式](@entry_id:136318)为 $\mathrm{A}_{n_{\mathrm{A}}}\mathrm{B}_{n_{\mathrm{B}}}\dots$ 的化合物，其每个原子的生成能 $E_f$ 定义为：

$$
E_f = \frac{E_{\mathrm{tot}} - \sum_i n_i \mu_i}{\sum_i n_i}
$$

其中，$E_{\mathrm{tot}}$ 是通过 DFT 计算得到的化合物的总能量，$n_i$ 是元素 $i$ 在[化学式](@entry_id:136318)中的原子数，而 $\mu_i$ 是元素 $i$ 的**化学势 (chemical potential)**。化学势代表了元素在其最稳定相（即[参考态](@entry_id:151465)）中的每个原子的能量。为了确保系统误差的最大程度抵消，化合物的 $E_{\mathrm{tot}}$ 和其所有组成元素的化学势 $\mu_i$ 必须在完全相同的计算设置（例如，相同的交换关联泛函、[赝势](@entry_id:170389)、[截断能](@entry_id:177594)等）下计算 [@problem_id:2479701]。

通过定义，$E_f$ 为负值表示该化合物相对于其组成元素是稳定的，其形成过程是放热的。$E_f$ 越负，化合物的[热力学稳定性](@entry_id:142877)越高。

为了比较不同[化学计量](@entry_id:137450)比的化合物的[相对稳定性](@entry_id:262615)，我们引入了**凸包 (convex hull)** 的概念。在一个二元体系（如 A-B 体系）中，我们可以绘制一个二维图，其中 x 轴代表成分（例如，B 的原子分数 $x_{\mathrm{B}}$），y 轴代表每个原子的生成能 $E_f$。将所有已知的或假设的化合物以及纯元素 A ($x_{\mathrm{B}}=0, E_f=0$) 和 B ($x_{\mathrm{B}}=1, E_f=0$) 在这个图上标记出来。所有这些点的最下方边界构成了一个[凸多边形](@entry_id:165008)，即**能量凸包**。

根据[热力学第一定律](@entry_id:146485)，任何位于凸包上的相都是**[热力学](@entry_id:141121)稳定相 (thermodynamically stable phase)**。它们不能自发分解为任何其他相或相的混合物来降低能量。相反，任何位于[凸包](@entry_id:262864)上方的相都是**[亚稳相](@entry_id:184907) (metastable phase)** 或**不稳定相 (unstable phase)**。它在能量上倾向于分解为位于其正下方[凸包](@entry_id:262864)上的两个或多个稳定相的混合物。一个相的点与其正下方[凸包](@entry_id:262864)上的连线（称为**[连接线](@entry_id:196944) (tie-line)**）之间的垂直能量差，被称为**分解能 (decomposition energy)** 或**距[凸包](@entry_id:262864)的能量 (energy above hull)**，它量化了该相的[热力学](@entry_id:141121)不稳定性程度 [@problem_id:2479751]。

例如，考虑一个假想的 A-B 二元体系，我们计算了三种化合物的生成能 [@problem_id:2479751]：
- $\mathrm{A}_2\mathrm{B}$ ($x_{\mathrm{B}} = 1/3$)：$E_f \approx -0.567 \ \mathrm{eV/atom}$
- $\mathrm{AB}$ ($x_{\mathrm{B}} = 1/2$)：$E_f = -0.400 \ \mathrm{eV/atom}$
- $\mathrm{AB}_2$ ($x_{\mathrm{B}} = 2/3$)：$E_f = -0.400 \ \mathrm{eV/atom}$

通过构建[凸包](@entry_id:262864)，我们发现 $\mathrm{A}_2\mathrm{B}$ 和 $\mathrm{AB}_2$ 位于[凸包](@entry_id:262864)上，是稳定相。而 $\mathrm{AB}$ 相位于连接 $\mathrm{A}_2\mathrm{B}$ 和 $\mathrm{AB}_2$ 两点的连接线上方。通过线性插值计算，$\mathrm{AB}$ 的分解能约为 $0.0835 \ \mathrm{eV/atom}$，表明它会倾向于分解为 $\mathrm{A}_2\mathrm{B}$ 和 $\mathrm{AB}_2$ 的混合物。

在机器学习驱动的[高通量筛选](@entry_id:271166)中，我们训练模型来预测成千上万个候[选材](@entry_id:161179)料的 $E_f$。然后，我们可以自动构建预测的[凸包](@entry_id:262864)来快速识别潜在的稳定相。一个有趣且实用的策略是，不仅关注预测为稳定的材料（分解能为零或负值），也关注那些具有很小正分解能（例如，小于 $50-100 \ \mathrm{meV/atom}$）的亚稳材料。这是因为我们的模型是基于 $0 \ \mathrm{K}$ 的焓，忽略了有限温度下的熵稳定效应（$G=H-TS$）以及阻止分解的动力学能垒。因此，这些轻微亚稳的材料在实际实验条件下可能仍然可以被合成并长期存在 [@problem_id:2479751]。

### [材料信息学](@entry_id:197429)的语言：描述符与[不变性](@entry_id:140168)

为了让[机器学习模型](@entry_id:262335)能够从原子结构中学习，我们必须首先将这些结构——本质上是原子种类和其三维坐标的列表——转化为固定长度的数字向量，即**描述符 (descriptor)** 或**[特征向量](@entry_id:151813) (feature vector)**。一个好的描述符必须编码与目标属性相关的化学和几何信息，同时尊重物理定律所要求的**[不变性](@entry_id:140168) (invariances)**。

对于像生成能这样的标量、内禀属性，描述符必须满足以下[基本对称性](@entry_id:161256)要求 [@problem_id:2479726]：
1.  **平移不变性 (Translational Invariance)**：将整个材料在空间中平移不应改变其能量，因此描述符也必须对平移不敏感。
2.  **[旋转不变性](@entry_id:137644) (Rotational Invariance)**：将整个材料在空间中旋转不应改变其能量，描述符也必须对旋转不敏感。
3.  **[置换不变性](@entry_id:753356) (Permutational Invariance)**：重新标记同种类的原子（例如，交换两个碳原子的索引）不应改变其能量，描述符也必须对这种标记的任意[置换](@entry_id:136432)不敏感。

描述符大致可分为两类：

**基于成分的描述符 (Composition-based descriptors)** 是最简单的形式。它们只依赖于材料的[化学计量](@entry_id:137450)比，完全忽略原子坐标。例如，我们可以使用元素的原子分数，或者计算材料中所有原子某些 tabulated 属性（如[电负性](@entry_id:147633)、原子半径）的平均值、[方差](@entry_id:200758)等统计量。这些描述符天生满足所有上述[不变性](@entry_id:140168)，因为它们不使用坐标信息，并且[统计计算](@entry_id:637594)（如求和、求平均）不依赖于原子顺序。然而，它们的局限性也显而易见：它们无法区分**[同分异构体](@entry_id:268311) (polymorphs)**，例如金刚石和石墨，它们都由纯碳构成，但具有截然不同的结构和性质。因此，基于成分的描述符仅适用于属性主要由化学成分决定的情况 [@problem_id:2479726]。

**基于结构的描述符 (Structure-based descriptors)** 则将几何信息编码进来，这对于预测结构敏感的属性至关重要。
- 一个早期的例子是**库仑矩阵 (Coulomb matrix)**。它是一个[对称矩阵](@entry_id:143130)，其对角线元素代表单个原子的能量，非对角[线元](@entry_id:196833)素 $C_{ij}$ 表示原子 $i$ 和 $j$ 之间的[静电排斥](@entry_id:162128)能，通常形式为 $Z_i Z_j / r_{ij}$，其中 $Z$ 是[原子序数](@entry_id:139400)，$r_{ij}$ 是原子间距。由于原子间距对全局平移和旋转是不变的，库仑矩阵本身满足这两种不变性。然而，一个原始的库仑矩阵对于原子索引的[置换](@entry_id:136432)并**不是**不变的——交换两个原子的标签会交换矩阵的相应行和列，产生一个不同的矩阵。为了强制实现[置换不变性](@entry_id:753356)，需要对矩阵进行后处理，例如，通过某种规范化的方式（如按行范数）对行和列进行排序，或者直接使用其**[本征值](@entry_id:154894)谱 (eigenvalue spectrum)** 作为最终描述符，因为矩阵的[本征值](@entry_id:154894)在行/列[置换](@entry_id:136432)下是不变的 [@problem_id:2479726]。
- 现代方法，如**平滑原子位置重叠 (Smooth Overlap of Atomic Positions, SOAP)**，则从构建每个原子的**局部原子环境 (local atomic environment)** 描述符入手。SOAP 将一个中心原子周围的邻居密度表示为一系列[高斯函数](@entry_id:261394)的和，然后将其在球谐函数基上展开。通过对展开系数进行特定的积分（形成所谓的**[功率谱](@entry_id:159996) (power spectrum)**），可以构建出对局部环境的旋转不敏感的特征。由于邻居密度是邻居原子的总和，它天然地对邻居的[置换](@entry_id:136432)不敏感。最后，通过在材料的所有原子上对这些局部 SOAP 描述符进行池化操作（如求平均），可以得到一个全局的、满足所有[不变性](@entry_id:140168)且大小不随系统[原子数](@entry_id:746561)变化的描述符，非常适合预测内禀属性 [@problem_id:2479726]。

### 学习[原子结构](@entry_id:137190)的现代架构

虽然手工设计的描述符（如 SOAP）非常强大，但[现代机器学习](@entry_id:637169)，特别是**[图神经网络](@entry_id:136853) (Graph Neural Networks, GNNs)**，提供了一种端到端的方式，可以直接从[原子结构](@entry_id:137190)中学习有效的表示。

在这种[范式](@entry_id:161181)中，一个晶体或分子被视为一个**图 (graph)**，其中原子是**节点 (nodes)**，原子间的键或邻近关系是**边 (edges)**。对于周期性晶体，必须正确处理[周期性边界条件](@entry_id:147809) (PBC)。这通常通过**[最小镜像约定](@entry_id:142070) (minimum image convention)** 来实现：两个原子之间的距离和[位移矢量](@entry_id:262782)是连接一个原子到另一个原子的所有周期性镜像中最短的那一个 [@problem_id:2479723]。边的特征可以包含原子间距离、方向等信息。

在设计用于原子系统的[神经网](@entry_id:276355)络时，一个至关重要的概念是区分**不变性 (invariance)** 和**[等变性](@entry_id:636671) (equivariance)** [@problem_id:2479779]。
- 如果一个函数 $f$ 的输出在输入 $x$ 经过变换 $g$ 后保持不变，即 $f(g(x)) = f(x)$，则称 $f$ 是**不变的 (invariant)**。
- 如果函数 $f$ 的输出以一种与输入变换相关联的、可预测的方式进行变换，即 $f(g(x)) = g'(f(x))$，其中 $g'$ 是与 $g$ 对应的变换，则称 $f$ 是**等变的 (equivariant)**。

这里的变换群是欧几里得群 $E(3)$，它包括三维空间中的平移、旋转和反射。
- **预测标量属性**：对于像总能量这样的标量属性，它本身是 $E(3)$ 不变的。因此，用于预测能量的模型其最终输出必须是 $E(3)$ 不变的。这可以通过两种主要方式实现：
    1.  **[不变性](@entry_id:140168)网络 (Invariant Networks)**：构建一个其所有中间层和输出都由不变特征构成的网络。例如，网络只使用原子间距离 $r_{ij}$ 和键角（表示为位移单位矢量间的[点积](@entry_id:149019) $\hat{\mathbf{r}}_{ij}\cdot \hat{\mathbf{r}}_{ik}$）等标量作为输入和信息传递的内容。这种设计天然保证了输出的[旋转不变性](@entry_id:137644) [@problem_id:2479736]。
    2.  **[等变性](@entry_id:636671)网络 (Equivariant Networks)**：构建一个其特征（如矢量、张量）在网络层间传递时遵循 $E(3)$ 群的变换规则的网络。例如，使用球谐函数 $Y_{\ell m}(\hat{\mathbf{r}}_{ij})$ 来编码方向信息，并使用[张量积](@entry_id:140694)来组合这些特征。网络的最终层通过一个特殊的操作将等变特征收缩为一个不变的标量输出（例如，只取 $\ell=0$ 的分量）[@problem_id:2479736]。

- **预测矢量属性**：对于像作用在原子上的**力 (force)** 这样的矢量属性，它本身是 $E(3)$ **等变的**——当整个系统旋转时，力矢量也必须随之旋转。因此，直接预测力的模型必须是 $E(3)$ 等变的架构。一个只使用不变特征的网络无法直接输出正确的、随系统旋转的力矢量 [@problem_id:2479779]。

这两种策略之间有一个深刻的物理联系。根据经典力学，[力是势能的负梯度](@entry_id:168705)：$\mathbf{F}_i = -\nabla_{\mathbf{r}_i} E$。一个重要的数学事实是，如果一个[标量场](@entry_id:151443) $E$ 是 $E(3)$ 不变的，那么它的[梯度场](@entry_id:264143) $\nabla E$ 必然是 $E(3)$ 等变的 [@problem_id:2479779]。这意味着，如果我们训练一个精确的、不变的能量模型，然后通过对其进行解析或[自动微分](@entry_id:144512)来计算力，得到的[力场](@entry_id:147325)将自动满足正确的[等变性](@entry_id:636671)要求。这构成了第一种策略（所谓的“[能量守恒](@entry_id:140514)势”）的理论基础。

反之，如果我们直接训练一个等变模型来预测力（策略二），虽然它在数据效率上可能更有优势（因为它被硬编码了正确的物理对称性，不需要从数据中学习旋转的不同变体），但其预测的[力场](@entry_id:147325)不一定**保守 (conservative)**，即不一定能保证存在一个唯一的标量势能函数。为了强制能量与力的一致性，通常需要额外的约束或正则化项 [@problem_id:2479779]。

### 发现的引擎：数据生成与[主动学习](@entry_id:157812)

[数据驱动的发现](@entry_id:274863)不仅依赖于强大的模型，更依赖于高质量的数据和智能的搜索策略。这个过程可以被看作一个“闭环”系统，包括数据生成、模型训练和决策制定三个阶段。

#### 系统化地生成高[质量数](@entry_id:142580)据

[高通量筛选](@entry_id:271166)活动能够产生海量数据，但只有当这些数据是可靠、可复现且被妥善管理时，它们才是有价值的。这需要严谨的计算工作流和[数据管理](@entry_id:635035)实践。

首先，计算流程应被设计为**有向无环图 (Directed Acyclic Graph, DAG)**。一个典型的 DFT 计算生成能的工作流可能包括以下节点：输入结构标准化 → 几何[结构弛豫](@entry_id:263707) → 静态总能量计算 → 参考元素能量计算 → 生成能后处理。DAG 结构确保了计算步骤的逻辑顺序和无[循环依赖](@entry_id:273976)，便于自动化、并行化和错误处理 [@problem_id:2479731]。

其次，**[数据溯源](@entry_id:175012) (provenance)** 是保证科学可复现性的基石。对于每一次计算，我们必须记录所有可能影响结果的细节。这构成了一个计算的“上下文”。一个完整的上下文记录应包括：
-   **软件信息**：所用计算代码的确切版本号（例如，git commit hash）。
-   **输入参数**：所有 DFT 参数，如交换关联泛函、[截断能](@entry_id:177594)、[k点](@entry_id:168686)网格密度、收敛阈值等。
-   **输入数据**：所用赝势文件的确切版本和校验和。
-   **[初始条件](@entry_id:152863)**：任何随机或不确定的选择，如初始磁矩的设置、随机种子等。

为了便于管理，可以将整个上下文信息通过[哈希函数](@entry_id:636237)压缩成一个唯一的**上下文哈希 (context hash)**。这个哈希值就像是计算的指纹。来自不同上下文（例如，使用不同泛函或[赝势](@entry_id:170389)库）的计算结果，即使是针对完全相同的原子结构，其能量值也通常是不可直接比较的 [@problem_id:2479701]。例如，来自两个使用不同 GGA 泛函的计算活动 $\mathcal{A}$ 和 $\mathcal{B}$ 的[生成焓](@entry_id:139204) $\Delta H_f^{\mathcal{A}}$ 和 $\Delta H_f^{\mathcal{B}}$ 是不同的标签，直接混合它们会给模型带来[标签噪声](@entry_id:636605)。

最后，在训练和评估[机器学习模型](@entry_id:262335)时，必须严格防止**[数据泄漏](@entry_id:260649) (data leakage)**。一个常见的泄漏源是在[训练集](@entry_id:636396)和[测试集](@entry_id:637546)中包含了代表相同物理结构的不同数据点。为避免这种情况，除了上下文哈希，我们还应该为每个独特的[晶体结构](@entry_id:140373)（由[晶格参数](@entry_id:191810)和原子分数坐标定义）计算一个**结构哈希 (structure hash)**。在划分数据集时，应采用**基于组的分割 (group-aware splitting)**，确保所有具有相同结构哈希的数据点（无论其计算上下文如何）都完全落在同一个数据[子集](@entry_id:261956)（训练集、验证集或[测试集](@entry_id:637546)）中。这确保了模型评估的是其对**全新**结构的泛化能力，而不是记忆已知结构在不同计算设置下的表现 [@problem_id:2479701]。

#### 智能搜索与优化

在[高通量筛选](@entry_id:271166)中，我们面对的是一个几乎无限的化学可能性空间。即使有高效的计算方法，穷尽所有可能性也是不现实的。因此，我们需要一种智能的策略来决定下一步应该评估哪个候[选材](@entry_id:161179)料，以便尽快找到满足我们目标的最佳材料。这就是**主动学习 (active learning)** 和**[贝叶斯优化](@entry_id:175791) (Bayesian Optimization, BO)** 发挥作用的地方。

BO 将[材料发现](@entry_id:159066)问题框架化为一个[优化问题](@entry_id:266749)：寻找能最小化（或最大化）某个昂贵的[目标函数](@entry_id:267263) $f(\mathbf{x})$ 的输入 $\mathbf{x}$。它通过维护一个关于 $f$ 的概率代理模型（通常是**高斯过程 (Gaussian Process, GP)**）来实现这一点。GP 不仅提供对每个点 $\mathbf{x}$ 的预测均值 $\mu(\mathbf{x})$（我们对 $f(\mathbf{x})$ 的最佳猜测），还提供预测[标准差](@entry_id:153618) $\sigma(\mathbf{x})$（我们对该猜测的不确定性）。

决策制定的关键在于**[采集函数](@entry_id:168889) (acquisition function)**，它利用 GP 的预测均值和不确定性来量化评估每个候选点的“价值”。一个广泛使用的[采集函数](@entry_id:168889)是**置信下界 (Lower Confidence Bound, LCB)**，用于最小化问题：

$$
\alpha(\mathbf{x}) = \mu(\mathbf{x}) - \sqrt{\beta_t} \sigma(\mathbf{x})
$$

在每一轮，我们选择使 $\alpha(\mathbf{x})$ 最小化的点进行下一次昂贵的评估。这个函数巧妙地平衡了**利用 (exploitation)** 和**探索 (exploration)**：
-   **利用**：选择具有较低预测均值 $\mu(\mathbf{x})$ 的点，因为它们很可能就是我们寻找的最优解。
-   **探索**：选择具有较高不确定性 $\sigma(\mathbf{x})$ 的点，因为这些区域我们知之甚少，可能隐藏着未被发现的更优解。

参数 $\beta_t$ 控制着这种平衡。理论分析表明，为了保证算法最终能收敛到全局最优解（即实现**次线性累积遗憾 (sublinear cumulative regret)**），$\beta_t$ 必须随着迭代次数 $t$ 的增加而缓慢增长（例如，$\beta_t \propto \log t$）。一个恒定或递减的 $\beta_t$ 可能会导致算法过[早停](@entry_id:633908)止探索，陷入局部最优 [@problem_id:2479741]。

在 BO 框架中，对不确定性的严谨量化至关重要。我们需要区分两种不确定性 [@problem_id:2479744]：
-   **偶然不确定性 (Aleatoric Uncertainty)**：源于数据本身的固有随机性或噪声，例如实验测量误差或 DFT 计算中的随机数值噪声。即使我们拥有无限数据，这种不确定性也无法消除。
-   **认知不确定性 (Epistemic Uncertainty)**：源于我们对模型的无知，例如模型参数的不确定性或模型形式的错误设定（如选用了不合适的 DFT 泛函）。这种不确定性可以通过收集更多数据来降低。

在贝叶斯模型（如 GP）中，认知不确定性正由[后验分布](@entry_id:145605)的[方差](@entry_id:200758)（即 $\sigma^2(\mathbf{x})$）来体现。因此，BO 正是利用[认知不确定性](@entry_id:149866)来指导探索。

最后，现实世界的[材料发现](@entry_id:159066)通常涉及多个相互冲突的目标，例如，我们可能同时希望材料既稳定（低 $E_f$），又便宜（低成本 $C$），还具有高催化活性。这是一个**[多目标优化](@entry_id:637420) (multi-objective optimization)** 问题。在这种情况下，通常不存在单个“最佳”解，而是一组**[帕累托最优](@entry_id:636539) (Pareto optimal)** 解的集合。一个解被称为[帕累托最优](@entry_id:636539)，当且仅当在不牺牲任何一个目标性能的前提下，无法再改进任何其他目标。这些最优解构成了**[帕累托前沿](@entry_id:634123) (Pareto front)**。在[数据驱动的发现](@entry_id:274863)中，我们的目标就是识别出这个前沿，为决策者提供一组最佳的权衡选项。对于一个有限的候选集，可以通过高效的算法（例如，基于排序和扫描）在 $O(n \log n)$ 时间内提取出帕累托前沿 [@problem_id:2479725]。