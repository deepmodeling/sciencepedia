{"hands_on_practices": [{"introduction": "参数估计的实践始于对其基本原理的深刻理解。本练习将引导你从最大似然估计 (MLE) 的第一性原理出发，推导一个简单零级反应模型的速率常数和初始浓度的解析解。通过这个过程，你将亲身体会到，在线性模型和高斯噪声的假设下，最大似然估计如何自然地等价于我们所熟知的最小二乘法 [@problem_id:2660590]。", "problem": "考虑一个间歇反应，在等温、恒容条件下，物质 $[A]$ 的浓度遵循零级消耗模型 $[A](t) = [A]_{0} - k t$。您收集了 $n$ 个独立测量值 $\\{(t_{i}, y_{i})\\}_{i=1}^{n}$，其中 $t_{i}$ 表示已知的观测时间，$y_{i}$ 表示观测到的浓度。假设测量误差是独立同分布的高斯分布，均值为 $0$，已知方差为 $\\sigma^{2}$，并且设计时间存在非零变异性，因此 $\\sum_{i=1}^{n} (t_{i} - \\bar{t})^{2} \\neq 0$，其中 $\\bar{t} = \\frac{1}{n}\\sum_{i=1}^{n} t_{i}$。\n\n从独立高斯误差下的似然定义和模型 $y_{i} = [A]_{0} - k t_{i} + \\varepsilon_{i}$（其中 $\\varepsilon_{i} \\sim \\mathcal{N}(0,\\sigma^{2})$）出发，推导 $[A]_{0}$ 和 $k$ 的闭式最大似然估计。用求和项 $\\sum_{i=1}^{n} y_{i}$、$\\sum_{i=1}^{n} t_{i}$、$\\sum_{i=1}^{n} t_{i}^{2}$、$\\sum_{i=1}^{n} t_{i} y_{i}$ 以及样本大小 $n$ 来明确表示您的最终结果。\n\n您的最终答案必须是一个单一的解析表达式，以行矩阵的形式按顺序 $\\big(\\widehat{[A]}_{0}, \\widehat{k}\\big)$ 给出两个估计值。不需要进行数值计算或四舍五入，最终表达式中也不应包含任何单位。", "solution": "在尝试任何解答之前，首先对问题陈述的有效性进行严格审查。\n\n**步骤1：提取已知条件**\n- **反应模型：** 零级消耗，$[A](t) = [A]_{0} - k t$。\n- **系统条件：** 等温、恒容的间歇式反应器。\n- **数据：** $n$ 个独立测量值 $\\{(t_{i}, y_{i})\\}_{i=1}^{n}$。\n- **测量误差：** 误差是独立同分布 (i.i.d.) 的，服从均值为 $0$、已知方差为 $\\sigma^{2}$ 的高斯分布。\n- **统计模型：** $y_{i} = [A]_{0} - k t_{i} + \\varepsilon_{i}$，其中 $\\varepsilon_{i} \\sim \\mathcal{N}(0,\\sigma^{2})$。\n- **约束条件：** 观测时间存在非零变异性，$\\sum_{i=1}^{n} (t_{i} - \\bar{t})^{2} \\neq 0$，其中 $\\bar{t} = \\frac{1}{n}\\sum_{i=1}^{n} t_{i}$。\n- **目标：** 推导参数 $[A]_{0}$ 和 $k$ 的闭式最大似然估计 (MLE)。结果需用求和项 $\\sum_{i=1}^{n} y_{i}$、$\\sum_{i=1}^{n} t_{i}$、$\\sum_{i=1}^{n} t_{i}^{2}$、$\\sum_{i=1}^{n} t_{i} y_{i}$ 以及样本大小 $n$ 来表示。\n\n**步骤2：使用提取的已知条件进行验证**\n根据所需标准对问题进行评估。\n- **科学依据：** 问题使用了标准的零级动力学模型和经典的统计框架（带高斯误差的线性回归）。这些是物理化学和统计学中的基本概念。该问题在科学上是合理的。\n- **适定性：** 该问题是适定的。模型对于参数 $[A]_{0}$ 和 $k$ 是线性的。独立同分布高斯误差的假设是标准的。明确的约束条件 $\\sum_{i=1}^{n} (t_{i} - \\bar{t})^{2} \\neq 0$ 确保了设计矩阵是满秩的，从而保证了参数存在唯一解。\n- **目标：** 语言精确、量化，没有任何主观或含糊的术语。\n\n**步骤3：结论与行动**\n该问题被判定为**有效**。这是一个统计参数估计中的标准、定义明确的问题。将推导其解。\n\n**最大似然估计的推导**\n\n单个观测值 $y_i$ 的统计模型由 $y_{i} = [A]_{0} - k t_{i} + \\varepsilon_{i}$ 给出。由于 $\\varepsilon_{i} \\sim \\mathcal{N}(0,\\sigma^{2})$，因此观测值 $y_i$ 是一个随机变量，服从均值为 $\\mu_i = E[y_i] = [A]_{0} - k t_{i}$、方差为 $\\sigma^{2}$ 的高斯分布。单个观测值 $y_i$ 的概率密度函数 (PDF) 为：\n$$p(y_i \\mid [A]_0, k) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - ([A]_0 - k t_i))^2}{2\\sigma^2}\\right)$$\n各测量值是独立的，因此整个数据集 $\\{(t_i, y_i)\\}_{i=1}^n$ 的似然函数 $L([A]_0, k)$ 是各个 PDF 的乘积：\n$$L([A]_0, k) = \\prod_{i=1}^{n} p(y_i \\mid [A]_0, k) = \\left(\\frac{1}{2\\pi\\sigma^2}\\right)^{n/2} \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (y_i - [A]_0 + k t_i)^2\\right)$$\n为了找到最大似然估计 (MLE)，我们最大化 $L$，这等价于最大化其自然对数，即对数似然函数 $\\ln L$：\n$$\\ln L([A]_0, k) = -\\frac{n}{2}\\ln(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (y_i - [A]_0 + k t_i)^2$$\n相对于 $[A]_0$ 和 $k$ 最大化 $\\ln L([A]_0, k)$ 等价于最小化误差平方和项 $S([A]_0, k)$，因为所有其他项相对于这些参数都是常数：\n$$S([A]_0, k) = \\sum_{i=1}^{n} (y_i - [A]_0 + k t_i)^2$$\n注意模型 $E[y_i] = [A]_0 - k t_i$ 意味着残差为 $r_i = y_i - E[y_i] = y_i - ([A]_0 - k t_i) = y_i - [A]_0 + k t_i$。\n\n为最小化 $S$，我们对其求关于 $[A]_0$ 和 $k$ 的偏导数，并令其为零。得到的估计值将表示为 $\\widehat{[A]}_0$ 和 $\\widehat{k}$。\n\n首先，关于 $[A]_0$ 的偏导数：\n$$\\frac{\\partial S}{\\partial [A]_0} = \\sum_{i=1}^{n} 2(y_i - [A]_0 + k t_i)(-1) = -2 \\left( \\sum_{i=1}^{n} y_i - n[A]_0 + k \\sum_{i=1}^{n} t_i \\right)$$\n将此导数设为零，得到第一个正规方程：\n$$n \\widehat{[A]}_0 - \\widehat{k} \\sum_{i=1}^{n} t_i = \\sum_{i=1}^{n} y_i \\quad (1)$$\n\n其次，关于 $k$ 的偏导数：\n$$\\frac{\\partial S}{\\partial k} = \\sum_{i=1}^{n} 2(y_i - [A]_0 + k t_i)(t_i) = 2 \\left( \\sum_{i=1}^{n} y_i t_i - [A]_0 \\sum_{i=1}^{n} t_i + k \\sum_{i=1}^{n} t_i^2 \\right)$$\n将此导数设为零，得到第二个正规方程：\n$$\\widehat{[A]}_0 \\sum_{i=1}^{n} t_i - \\widehat{k} \\sum_{i=1}^{n} t_i^2 = \\sum_{i=1}^{n} y_i t_i \\quad (2)$$\n\n我们现在得到了一个包含两个未知数 $\\widehat{[A]}_0$ 和 $\\widehat{k}$ 的二元线性方程组：\n$$\n\\begin{cases}\n    n \\widehat{[A]}_0 - \\left(\\sum_{i=1}^{n} t_i\\right) \\widehat{k} = \\sum_{i=1}^{n} y_i \\\\\n    \\left(\\sum_{i=1}^{n} t_i\\right) \\widehat{[A]}_0 - \\left(\\sum_{i=1}^{n} t_i^2\\right) \\widehat{k} = \\sum_{i=1}^{n} y_i t_i\n\\end{cases}\n$$\n该方程组可以使用多种方法求解，例如代入法或克莱姆法则。使用克莱姆法则，系数矩阵的行列式为：\n$$\\Delta = (n)\\left(-\\sum_{i=1}^{n} t_i^2\\right) - \\left(-\\sum_{i=1}^{n} t_i\\right)\\left(\\sum_{i=1}^{n} t_i\\right) = -\\left(n \\sum_{i=1}^{n} t_i^2 - \\left(\\sum_{i=1}^{n} t_i\\right)^2\\right)$$\n估计值 $\\widehat{[A]}_0$ 由下式给出：\n$$\\widehat{[A]}_0 = \\frac{\\begin{vmatrix} \\sum_{i=1}^{n} y_i  -\\sum_{i=1}^{n} t_i \\\\ \\sum_{i=1}^{n} y_i t_i  -\\sum_{i=1}^{n} t_i^2 \\end{vmatrix}}{\\Delta} = \\frac{\\left(\\sum_{i=1}^{n} y_i\\right)\\left(-\\sum_{i=1}^{n} t_i^2\\right) - \\left(-\\sum_{i=1}^{n} t_i\\right)\\left(\\sum_{i=1}^{n} y_i t_i\\right)}{-\\left(n \\sum_{i=1}^{n} t_i^2 - \\left(\\sum_{i=1}^{n} t_i\\right)^2\\right)}$$\n$$\\widehat{[A]}_0 = \\frac{-\\left(\\sum_{i=1}^{n} y_i\\right)\\left(\\sum_{i=1}^{n} t_i^2\\right) + \\left(\\sum_{i=1}^{n} t_i\\right)\\left(\\sum_{i=1}^{n} y_i t_i\\right)}{-\\left(n \\sum_{i=1}^{n} t_i^2 - \\left(\\sum_{i=1}^{n} t_i\\right)^2\\right)} = \\frac{\\left(\\sum_{i=1}^{n} y_i\\right)\\left(\\sum_{i=1}^{n} t_i^2\\right) - \\left(\\sum_{i=1}^{n} t_i\\right)\\left(\\sum_{i=1}^{n} y_i t_i\\right)}{n \\sum_{i=1}^{n} t_i^2 - \\left(\\sum_{i=1}^{n} t_i\\right)^2}$$\n估计值 $\\widehat{k}$ 由下式给出：\n$$\\widehat{k} = \\frac{\\begin{vmatrix} n  \\sum_{i=1}^{n} y_i \\\\ \\sum_{i=1}^{n} t_i  \\sum_{i=1}^{n} y_i t_i \\end{vmatrix}}{\\Delta} = \\frac{n\\left(\\sum_{i=1}^{n} y_i t_i\\right) - \\left(\\sum_{i=1}^{n} t_i\\right)\\left(\\sum_{i=1}^{n} y_i\\right)}{-\\left(n \\sum_{i=1}^{n} t_i^2 - \\left(\\sum_{i=1}^{n} t_i\\right)^2\\right)}$$\n$$\\widehat{k} = \\frac{\\left(\\sum_{i=1}^{n} t_i\\right)\\left(\\sum_{i=1}^{n} y_i\\right) - n \\sum_{i=1}^{n} y_i t_i}{n \\sum_{i=1}^{n} t_i^2 - \\left(\\sum_{i=1}^{n} t_i\\right)^2}$$\n这些就是 $[A]_0$ 和 $k$ 的最大似然估计的闭式解。", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{\\left(\\sum_{i=1}^{n} y_i\\right)\\left(\\sum_{i=1}^{n} t_i^2\\right) - \\left(\\sum_{i=1}^{n} t_i\\right)\\left(\\sum_{i=1}^{n} t_i y_i\\right)}{n \\sum_{i=1}^{n} t_i^2 - \\left(\\sum_{i=1}^{n} t_i\\right)^2} & \\frac{\\left(\\sum_{i=1}^{n} t_i\\right)\\left(\\sum_{i=1}^{n} y_i\\right) - n \\sum_{i=1}^{n} y_i t_i}{n \\sum_{i=1}^{n} t_i^2 - \\left(\\sum_{i=1}^{n} t_i\\right)^2}\n\\end{pmatrix}\n}\n$$", "id": "2660590"}, {"introduction": "现实世界中的许多动力学模型本质上是非线性的，无法通过简单的线性回归求解。本练习将带你进入计算实践的核心，你将为一个典型的一级反应模型编写代码，使用加权非线性最小二乘法来拟合实验数据。此练习不仅能锻炼你使用数值优化工具的能力，还将教会你如何计算参数的协方差矩阵，从而对估计结果的置信度进行量化评估 [@problem_id:2660532]。", "problem": "给定一个单步、不可逆、一级衰变反应的动态模型，其中物种 $A$ 的浓度遵循常微分方程 $d[A]/dt = -k[A]$，初始条件为 $[A](0) = [A]_0$。浓度的测量在离散时间点进行，并受到独立加性噪声的干扰。目标是通过在高斯噪声模型下使用加权最小二乘法 (WLS) 将积分速率方程与数据进行拟合，来估计参数向量 $\\theta = ([A]_0, k)$，并计算估计量的近似协方差矩阵。\n\n基本原理和假设：\n- 确定性动力学由一级过程的质量作用定律控制。测量模型为 $y_i = [A](t_i) + \\varepsilon_i$，其中 $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma_i^2)$ 在 $i$ 上独立。\n- 自变量是时间 $t$，单位为秒，可观测浓度 $[A]$ 的单位为摩尔/升。\n- 在所述假设下，最大似然估计量与 WLS 估计量一致，其中权重为 $w_i = 1/\\sigma_i^2$。\n\n你的程序必须为每个测试用例执行的任务：\n1. 从基本速率方程和初始条件出发，推导出 $[A](t)$ 作为 $[A]_0$ 和 $k$ 的函数的表达式。\n2. 根据已知方差 $\\sigma_i^2$ 的高斯噪声模型，构建 WLS 目标函数。通过最小化此目标函数来估计 $\\theta = ([A]_0, k)$，其中 $[A]_0 > 0$ 且 $k > 0$。\n3. 使用与 WLS 公式一致的一阶（高斯-牛顿）近似，在最优点计算近似协方差矩阵 $\\mathrm{Cov}(\\hat{\\theta})$。报告条目 $\\mathrm{Var}([A]_0)$、$\\mathrm{Var}(k)$ 和 $\\mathrm{Cov}([A]_0,k)$。\n4. 实现一个数值稳定的求解器，该求解器强制 $[A]_0$ 和 $k$ 的正性，并在构建近似时，使用模型相对于参数的解析正确的敏感度（雅可比矩阵）。\n5. 使用下方的精确测试套件，包括指定的伪随机种子来生成合成观测值 $y_i$，以及给定的已知标准差 $\\sigma_i$ 来定义 WLS 权重。\n\n使用的物理和数值单位：\n- 报告 $[A]_0$ 的单位为 $\\mathrm{mol\\,L^{-1}}$， $k$ 的单位为 $\\mathrm{s^{-1}}$。方差和协方差必须使用相应的平方单位和混合单位。\n- 最终程序输出必须为不带单位符号的数值；但是，所有计算都必须使用规定的单位进行。\n\n测试套件（三个数据集）。对于每个数据集 $j \\in \\{1,2,3\\}$：\n- 使用 $y_i^{(j)} = [A]_0^{\\mathrm{true}(j)} \\exp(-k^{\\mathrm{true}(j)} t_i^{(j)}) + \\varepsilon_i^{(j)}$ 生成合成观测值，其中 $\\varepsilon_i^{(j)} \\sim \\mathcal{N}(0, (\\sigma_i^{(j)})^2)$，独立。为每个数据集使用指定的种子初始化伪随机数生成器，以使生成的 $y_i^{(j)}$ 是确定性和可复现的。\n\n数据集 1（理想路径，同方差）：\n- 真实参数：$[A]_0^{\\mathrm{true}(1)} = 1.25\\,\\mathrm{mol\\,L^{-1}}$，$k^{\\mathrm{true}(1)} = 0.0075\\,\\mathrm{s^{-1}}$。\n- 时间点（秒）：$t^{(1)} = (0, 50, 100, 150, 200, 250, 300)$。\n- 已知标准差（$\\mathrm{mol\\,L^{-1}}$）：对于所有 $i$，$\\sigma_i^{(1)} \\equiv 0.02$。\n- 种子：$314159$。\n\n数据集 2（随时间增加的异方差噪声）：\n- 真实参数：$[A]_0^{\\mathrm{true}(2)} = 0.8\\,\\mathrm{mol\\,L^{-1}}$，$k^{\\mathrm{true}(2)} = 0.01\\,\\mathrm{s^{-1}}$。\n- 时间点（秒）：$t^{(2)} = (0, 40, 80, 120, 160, 200, 240, 280, 320)$。\n- 已知标准差（$\\mathrm{mol\\,L^{-1}}$）：$\\sigma_i^{(2)} = 0.01 + 0.00005\\, t_i^{(2)}$。\n- 种子：$271828$。\n\n数据集 3（在短时间窗口内关于 $k$ 的信息较弱的边界情况）：\n- 真实参数：$[A]_0^{\\mathrm{true}(3)} = 1.0\\,\\mathrm{mol\\,L^{-1}}$，$k^{\\mathrm{true}(3)} = 0.001\\,\\mathrm{s^{-1}}$。\n- 时间点（秒）：$t^{(3)} = (0, 5, 10, 15, 20, 25, 30)$。\n- 已知标准差（$\\mathrm{mol\\,L^{-1}}$）：对于所有 $i$，$\\sigma_i^{(3)} \\equiv 0.005$。\n- 种子：$161803$。\n\n算法要求：\n- 使用带有 WLS 残差的非线性最小二乘求解器。向求解器提供模型关于 $[A]_0$ 和 $k$ 的解析雅可比矩阵。强制执行边界 $[A]_0 > 0$ 和 $k > 0$。\n- 在最优点，使用与 WLS 权重和已知 $\\sigma_i$ 的高斯假设一致的一阶信息近似来计算近似协方差矩阵。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含三个数据集的串联结果，并严格遵循以下顺序\n$[\\hat{[A]}_0^{(1)}$, $\\hat{k}^{(1)}$, $\\mathrm{Var}^{(1)}([A]_0)$, $\\mathrm{Var}^{(1)}(k)$, $\\mathrm{Cov}^{(1)}([A]_0,k)$, $\\hat{[A]}_0^{(2)}$, $\\hat{k}^{(2)}$, $\\mathrm{Var}^{(2)}([A]_0)$, $\\mathrm{Var}^{(2)}(k)$, $\\mathrm{Cov}^{(2)}([A]_0,k)$, $\\hat{[A]}_0^{(3)}$, $\\hat{k}^{(3)}$, $\\mathrm{Var}^{(3)}([A]_0)$, $\\mathrm{Var}^{(3)}(k)$, $\\mathrm{Cov}^{(3)}([A]_0,k)]$，无空格。\n- 每个数值必须精确四舍五入到小数点后 $6$ 位。\n- 不涉及角度。所有量均为实值浮点数。\n\n您的程序必须是完整且可直接运行的，无需任何外部输入，并且必须严格遵守指定的输出格式。唯一允许使用的库是 Python 标准库、NumPy 和 SciPy。", "solution": "所提出的问题是化学动力学中动态系统参数估计的标准练习。它具有科学依据，定义明确，并包含确定性和可验证解决方案所需的所有信息。因此，该问题被认为是有效的。\n\n解决方案分为四个主要步骤：\n1.  从控制微分方程推导分析模型。\n2.  为参数估计构建加权最小二乘法 (WLS) 目标函数。\n3.  为优化算法推导解析雅可比矩阵。\n4.  构建参数协方差矩阵近似。\n\n**1. 积分速率方程**\n\n问题始于一级不可逆衰变过程的基本速率方程：\n$$\n\\frac{d[A]}{dt} = -k[A]\n$$\n这是一个一阶线性常微分方程。我们通过分离变量法求解，并满足初始条件 $[A](t=0) = [A]_0$。\n$$\n\\frac{d[A]}{[A]} = -k \\, dt\n$$\n将两边从初始状态 $([A]_0, 0)$ 积分到状态 $([A], t)$ 得到：\n$$\n\\int_{[A]_0}^{[A]} \\frac{1}{[A]'} \\, d[A]' = \\int_0^t -k \\, dt'\n$$\n$$\n\\ln([A]) - \\ln([A]_0) = -k(t - 0)\n$$\n$$\n\\ln\\left(\\frac{[A]}{[A]_0}\\right) = -kt\n$$\n对两边取指数，得到积分速率方程，这是我们在时间 $t$ 时物种 $A$ 浓度的模型函数：\n$$\n[A](t) = [A]_0 e^{-kt}\n$$\n这个函数，我们记为 $f(t; \\theta)$，依赖于参数向量 $\\theta = ([A]_0, k)^T$。\n\n**2. 加权最小二乘法 (WLS) 公式**\n\n在时间 $t_i$ 的测量值 $y_i$ 被建模为 $y_i = f(t_i; \\theta) + \\varepsilon_i$，其中误差 $\\varepsilon_i$ 是独立的且服从正态分布 $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma_i^2)$，其方差 $\\sigma_i^2$ 已知。在这些假设下，$\\theta$ 的最大似然估计量是使加权残差平方和最小的那个。这就是加权最小二乘法 (WLS) 估计量。\n\nWLS 目标函数 $S(\\theta)$ 由下式给出：\n$$\nS(\\theta) = \\sum_{i=1}^{N} \\left( \\frac{y_i - f(t_i; \\theta)}{\\sigma_i} \\right)^2 = \\sum_{i=1}^{N} \\left( \\frac{y_i - [A]_0 e^{-kt_i}}{\\sigma_i} \\right)^2\n$$\n其中 $N$ 是观测点的数量。我们寻求参数值 $\\hat{\\theta} = (\\hat{[A]}_0, \\hat{k})^T$ 以最小化此函数，并满足物理约束条件 $[A]_0 > 0$ 和 $k > 0$。\n\n**3. 非线性优化与解析雅可比矩阵**\n\n最小化 $S(\\theta)$ 是一个非线性最小二乘问题。我们使用一个数值求解器，具体来说是 `scipy.optimize.least_squares`，它非常适合这个任务。通过提供残差向量关于参数的解析雅可比矩阵，可以大大提高求解器的效率和可靠性。\n\n该求解器最小化一个向量函数的平方和。设这个向量函数为 $\\mathbf{r}_w(\\theta)$，即加权残差向量，其第 $i$ 个分量为：\n$$\nr_{w,i}(\\theta) = \\frac{f(t_i; \\theta) - y_i}{\\sigma_i} = \\frac{[A]_0 e^{-kt_i} - y_i}{\\sigma_i}\n$$\n雅可比矩阵 $J_w$ 是一个 $N \\times 2$ 的矩阵，其中 $(J_w)_{ij} = \\frac{\\partial r_{w,i}}{\\partial \\theta_j}$。这两个列对应于关于 $[A]_0$ 和 $k$ 的偏导数。\n\n对于第一个参数 $[A]_0$：\n$$\n\\frac{\\partial r_{w,i}}{\\partial [A]_0} = \\frac{1}{\\sigma_i} \\frac{\\partial}{\\partial [A]_0} \\left( [A]_0 e^{-kt_i} - y_i \\right) = \\frac{e^{-kt_i}}{\\sigma_i}\n$$\n对于第二个参数 $k$：\n$$\n\\frac{\\partial r_{w,i}}{\\partial k} = \\frac{1}{\\sigma_i} \\frac{\\partial}{\\partial k} \\left( [A]_0 e^{-kt_i} - y_i \\right) = \\frac{[A]_0}{\\sigma_i} \\frac{\\partial}{\\partial k} \\left( e^{-kt_i} \\right) = \\frac{[A]_0}{\\sigma_i} \\left( -t_i e^{-kt_i} \\right) = -\\frac{[A]_0 t_i e^{-kt_i}}{\\sigma_i}\n$$\n这些偏导数的表达式构成了提供给优化程序的雅可比矩阵的列。\n\n**4. 估计量的协方差矩阵**\n\n估计参数 $\\hat{\\theta}$ 的不确定性由其协方差矩阵 $\\mathrm{Cov}(\\hat{\\theta})$ 来表征。对于非线性最小二乘问题，一个常用且有效的近似方法源自高斯-牛顿法。这个近似将协方差矩阵与费雪信息矩阵的逆矩阵联系起来，而费雪信息矩阵本身则通过在解 $\\hat{\\theta}$ 处计算的 $J_w^T J_w$ 来近似。\n$$\n\\mathrm{Cov}(\\hat{\\theta}) \\approx \\left( J_w(\\hat{\\theta})^T J_w(\\hat{\\theta}) \\right)^{-1}\n$$\n这个 $2 \\times 2$ 矩阵的对角线上是估计量的方差，非对角线上是它们之间的协方差：\n$$\n\\mathrm{Cov}(\\hat{\\theta}) = \\begin{pmatrix} \\mathrm{Var}(\\hat{[A]}_0)  \\mathrm{Cov}(\\hat{[A]}_0, \\hat{k}) \\\\ \\mathrm{Cov}(\\hat{[A]}_0, \\hat{k})  \\mathrm{Var}(\\hat{k}) \\end{pmatrix}\n$$\n`scipy.optimize.least_squares` 函数方便地返回在最终参数估计值处计算的雅可比矩阵 $J_w$，从而可以直接计算该协方差矩阵。\n\n实现将精确遵循这些推导。对于每个数据集，使用提供的真实参数和一个带种子的随机数生成器来生成合成数据，以确保可复现性。然后执行 WLS 优化以找到 $\\hat{[A]}_0$ 和 $\\hat{k}$，接着计算所需的方差和协方差项。", "answer": "```python\n# Final runnable Python code for the specified environment.\nimport numpy as np\nfrom scipy.optimize import least_squares\n\ndef solve():\n    \"\"\"\n    Solves for the parameters of a first-order decay model for three\n    different datasets using Weighted Least Squares and computes the\n    covariance matrix of the estimators.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"name\": \"Dataset 1\",\n            \"true_params\": [1.25, 0.0075], # [A0_true, k_true]\n            \"times\": np.array([0., 50., 100., 150., 200., 250., 300.]),\n            \"sigma\": lambda t: np.full_like(t, 0.02),\n            \"seed\": 314159\n        },\n        {\n            \"name\": \"Dataset 2\",\n            \"true_params\": [0.8, 0.01],\n            \"times\": np.array([0., 40., 80., 120., 160., 200., 240., 280., 320.]),\n            \"sigma\": lambda t: 0.01 + 0.00005 * t,\n            \"seed\": 271828\n        },\n        {\n            \"name\": \"Dataset 3\",\n            \"true_params\": [1.0, 0.001],\n            \"times\": np.array([0., 5., 10., 15., 20., 25., 30.]),\n            \"sigma\": lambda t: np.full_like(t, 0.005),\n            \"seed\": 161803\n        }\n    ]\n\n    # --- Model and Objective Function Definitions ---\n\n    def model(params, t_vals):\n        \"\"\"\n        Calculates concentration based on first-order decay model.\n        params: [A0, k]\n        \"\"\"\n        A0, k = params\n        return A0 * np.exp(-k * t_vals)\n\n    def residuals_wls(params, t_vals, y_obs, sigma_vals):\n        \"\"\"\n        Calculates weighted residuals for WLS fitting.\n        (model - y) / sigma\n        \"\"\"\n        return (model(params, t_vals) - y_obs) / sigma_vals\n\n    def jacobian_wls(params, t_vals, y_obs, sigma_vals):\n        \"\"\"\n        Calculates the analytical Jacobian of the weighted residuals.\n        \"\"\"\n        A0, k = params\n        jac = np.zeros((len(t_vals), 2))\n        exp_term = np.exp(-k * t_vals)\n        \n        # d(residual)/d(A0) = (1/sigma) * d(model)/d(A0)\n        jac[:, 0] = exp_term / sigma_vals\n        \n        # d(residual)/d(k) = (1/sigma) * d(model)/d(k)\n        jac[:, 1] = -A0 * t_vals * exp_term / sigma_vals\n        \n        return jac\n\n    all_results = []\n    \n    for case in test_cases:\n        A0_true, k_true = case[\"true_params\"]\n        t = case[\"times\"]\n        seed = case[\"seed\"]\n        sigma = case[\"sigma\"](t)\n        \n        # 1. Generate synthetic data\n        rng = np.random.default_rng(seed)\n        noise = rng.normal(0, sigma)\n        y_obs = model([A0_true, k_true], t) + noise\n        \n        # 2. Perform nonlinear least-squares optimization\n        # Use true parameters as initial guess for robust convergence in this test setup\n        initial_guess = case[\"true_params\"]\n        # Enforce positivity constraints [A0 > 0, k > 0]\n        bounds = ([1e-9, 1e-9], [np.inf, np.inf])\n        \n        lsq_result = least_squares(\n            residuals_wls,\n            x0=initial_guess,\n            jac=jacobian_wls,\n            bounds=bounds,\n            method='trf',\n            args=(t, y_obs, sigma)\n        )\n        \n        A0_est, k_est = lsq_result.x\n        \n        # 3. Compute the approximate covariance matrix\n        # Cov(theta) approx (J^T J)^-1, where J is Jacobian of weighted residuals\n        J = lsq_result.jac\n        try:\n            cov_matrix = np.linalg.inv(J.T @ J)\n            var_A0 = cov_matrix[0, 0]\n            var_k = cov_matrix[1, 1]\n            cov_A0_k = cov_matrix[0, 1]\n        except np.linalg.LinAlgError:\n            # Handle cases where the matrix is singular (e.g., poor data)\n            var_A0, var_k, cov_A0_k = (np.inf, np.inf, np.inf)\n\n        # 4. Store the results in the required order\n        all_results.extend([A0_est, k_est, var_A0, var_k, cov_A0_k])\n\n    # 5. Format and print the final output string\n    # E.g. [1.249123,0.007505,...]\n    output_str = \",\".join([f\"{val:.6f}\" for val in all_results])\n    print(f\"[{output_str}]\")\n\nif __name__ == '__main__':\n    solve()\n```", "id": "2660532"}, {"introduction": "在一个反应过程中，我们常常可以同时监测反应物和产物的浓度变化。本高级练习旨在展示全局分析 (global analysis) 的强大威力，这是一种现代且稳健的建模方法。通过对一个简单不可逆反应中的反应物和产物数据进行联合拟合，并强制执行质量守恒等物理约束，你将亲身体会到这种方法相比于独立拟合，如何显著提高参数估计的准确性和可靠性 [@problem_id:2660601]。", "problem": "考虑一个由质量作用动力学控制的不可逆单分子反应，其中物种 $A$ 以一级速率常数 $k$ 转化为物种 $B$：\n$$ A \\xrightarrow{k} B. $$\n设 $[A](t)$ 和 $[B](t)$ 分别表示在时间 $t$ 时物种 $A$ 和 $B$ 的浓度。在充分混合和恒定体积的条件下，其控制常微分方程 (ODE) 为\n$$ \\frac{d[A]}{dt} = -k [A], \\quad \\frac{d[B]}{dt} = k [A], $$\n以及守恒定律（质量平衡）\n$$ [A](t) + [B](t) = M, $$\n其中 $M$ 是一个常数，等于 $[A](0) + [B](0)$。\n\n您将通过在指定时间点计算 ODE 所隐含的精确轨迹，然后添加具有指定标准差的独立高斯测量噪声，来生成 $[A](t)$ 和 $[B](t)$ 的合成测量数据。然后，您将通过三种方式估计动力学参数 $k$：\n- 仅对 $[A](t)$ 单独拟合：通过最小化测量的 $[A](t_i)$ 与相应模型预测值之间的加权残差平方和来估计 $k_A$。将 $[A](0)$ 视为一个待估计的未知参数。对所有浓度参数和 $k_A$ 强制施加非负约束。\n- 仅对 $[B](t)$ 单独拟合：通过最小化测量的 $[B](t_i)$ 与相应模型预测值之间的加权残差平方和来估计 $k_B$。将 $[B](0)$、$[A](0)$ 和 $k_B$ 视为待估计的未知参数。对所有浓度参数和 $k_B$ 强制施加非负约束。\n- 对 $[A](t)$ 和 $[B](t)$ 进行联合拟合，强制使用共同的速率常数和质量平衡：通过同时最小化两个序列的组合加权残差平方和来估计单一的 $k_{\\mathrm{joint}}$，使用一个与 ODE 一致且在所有时间 $t$ 都满足守恒定律的模型，未知参数为 $[A](0)$、$[B](0)$ 和 $k_{\\mathrm{joint}}$。对所有浓度参数和 $k_{\\mathrm{joint}}$ 强制施加非负约束。\n\n在所有三个估计任务中，假设测量误差是独立的，服从高斯分布，其标准差 $\\sigma_A$（对于 $[A](t)$）和 $\\sigma_B$（对于 $[B](t)$）是已知的，并使用等于方差倒数的权重，即在平方和求和之前将残差除以相应的标准差。使用具有指定种子的确定性伪随机数生成以确保可复现性。\n\n单位和报告要求：\n- 时间 $t$ 的单位必须是秒。\n- 浓度 $[A]$ 和 $[B]$ 的单位必须是摩尔/升。\n- 速率常数 $k_A$、$k_B$ 和 $k_{\\mathrm{joint}}$ 必须以秒的倒数，即 $\\mathrm{s}^{-1}$ 报告。\n- 所有报告的速率常数以 $\\mathrm{s}^{-1}$ 为单位，表示为浮点小数，并精确到小数点后六位。\n\n测试套件规范。对于下面的每个测试用例，生成时间网格，计算与 ODE 和守恒定律一致的无噪声轨迹，添加具有给定标准差和种子的含高斯分布的噪声，然后执行上述三种估计过程。使用以下测试用例：\n\n- 用例 $1$（通用，良态）：\n  - 真实参数：$k_\\text{true} = 0.35 \\ \\mathrm{s}^{-1}$，$[A](0) = 1.1 \\ \\mathrm{mol \\cdot L^{-1}}$，$[B](0) = 0.3 \\ \\mathrm{mol \\cdot L^{-1}}$。\n  - 时间网格：从 $t=0$ 到 $t_\\max = 8.0 \\ \\mathrm{s}$ 的 $N = 41$ 个等距点（含端点）。\n  - 噪声水平：$\\sigma_A = 0.01 \\ \\mathrm{mol \\cdot L^{-1}}$，$\\sigma_B = 0.01 \\ \\mathrm{mol \\cdot L^{-1}}$。\n  - 随机种子：$13579$。\n\n- 用例 $2$（慢动力学，长观测窗口）：\n  - 真实参数：$k_\\text{true} = 0.02 \\ \\mathrm{s}^{-1}$，$[A](0) = 2.0 \\ \\mathrm{mol \\cdot L^{-1}}$，$[B](0) = 0.4 \\ \\mathrm{mol \\cdot L^{-1}}$。\n  - 时间网格：从 $t=0$ 到 $t_\\max = 200.0 \\ \\mathrm{s}$ 的 $N = 51$ 个等距点（含端点）。\n  - 噪声水平：$\\sigma_A = 0.01 \\ \\mathrm{mol \\cdot L^{-1}}$，$\\sigma_B = 0.015 \\ \\mathrm{mol \\cdot L^{-1}}$。\n  - 随机种子：$24680$。\n\n- 用例 $3$（稀疏采样，快动力学）：\n  - 真实参数：$k_\\text{true} = 1.5 \\ \\mathrm{s}^{-1}$，$[A](0) = 0.5 \\ \\mathrm{mol \\cdot L^{-1}}$，$[B](0) = 0.0 \\ \\mathrm{mol \\cdot L^{-1}}$。\n  - 时间网格：明确指定为 $t \\in \\{0.0, 0.2, 0.5, 1.0, 1.5\\} \\ \\mathrm{s}$。\n  - 噪声水平：$\\sigma_A = 0.02 \\ \\mathrm{mol \\cdot L^{-1}}$，$\\sigma_B = 0.02 \\ \\mathrm{mol \\cdot L^{-1}}$。\n  - 随机种子：$97531$。\n\n您的程序必须为每个用例执行以下步骤：\n- 使用真实参数，根据 ODE 和守恒定律生成 $[A](t)$ 和 $[B](t)$ 的轨迹。\n- 添加具有指定标准差和种子的独立高斯噪声，以获得含噪声的测量数据。\n- 拟合单独的 $[A](t)$ 模型以估计 $k_A$；拟合单独的 $[B](t)$ 模型以估计 $k_B$；拟合强制使用共同速率常数和质量平衡的联合模型以估计 $k_{\\mathrm{joint}}$。\n- 将三个估计出的速率常数四舍五入到小数点后六位，单位为 $\\mathrm{s}^{-1}$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个逗号分隔的列表的列表形式的结果，每个内部列表对应一个测试用例，顺序与上述用例一致。每个内部列表必须包含该用例的三个四舍五入后的浮点数 $[k_A, k_B, k_{\\mathrm{joint}}]$，单位为 $\\mathrm{s}^{-1}$。例如，一个用例的输出应如 `[[0.123456,0.123457,0.123450]]`。对于本问题中的三个用例，要求的输出格式必须为 `[[k_{A,1},k_{B,1},k_{\\mathrm{joint},1}],[k_{A,2},k_{B,2},k_{\\mathrm{joint},2}],[k_{A,3},k_{B,3},k_{\\mathrm{joint},3}]]`，且行内无任何空格。", "solution": "所述问题在逻辑上是一致的，在科学上是合理的，并为获得唯一且可验证的解提供了所有必要信息。它代表了化学动力学中一个标准的适定问题，特别是从时间序列数据中进行参数估计。因此，我们将着手提供一个完整的解决方案。\n\n问题的核心是使用三种不同的统计模型从合成数据中估计一个动力学参数，并比较结果。该反应是物种 $A$ 到物种 $B$ 的单分子不可逆转化，表示为 $A \\xrightarrow{k} B$。\n\n首先，我们建立物种浓度的解析模型。$A$ 的消失速率由一级速率定律给出：\n$$ \\frac{d[A]}{dt} = -k [A] $$\n这是一个线性一阶常微分方程。在初始条件 $[A](t=0) = [A]_0$ 下，该方程积分后得到 $A$ 浓度的指数衰减函数：\n$$ [A](t) = [A]_0 e^{-kt} $$\n物种 $A$ 和 $B$ 的总浓度是守恒的。质量平衡方程为：\n$$ [A](t) + [B](t) = M = \\text{constant} $$\n常数 $M$ 是总初始浓度，$M = [A]_0 + [B]_0$。利用这个守恒定律，我们推导出物种 $B$ 的浓度随时间变化的函数：\n$$ [B](t) = M - [A](t) = ([A]_0 + [B]_0) - [A]_0 e^{-kt} $$\n这可以被重新整理成一个更常见的形式：\n$$ [B](t) = [B]_0 + [A]_0 (1 - e^{-kt}) $$\n这两个关于 $[A](t)$ 和 $[B](t)$ 的方程构成了我们模型的确定性部分。它们将用于生成合成数据和拟合参数。\n\n合成数据的生成分两步进行。首先，对于每个测试用例，使用提供的真实参数（$k_{\\text{true}}$、$[A]_{0, \\text{true}}$、$[B]_{0, \\text{true}}$）在一组离散时间点 $\\{t_i\\}$ 上计算“真实”的无噪声轨迹 $[A]_{\\text{true}}(t_i)$ 和 $[B]_{\\text{true}}(t_i)$。其次，将独立的、零均值的高斯测量噪声添加到这些真实浓度中：\n$$ [A]_{\\text{meas}}(t_i) = [A]_{\\text{true}}(t_i) + \\epsilon_{A,i}, \\quad \\text{where} \\quad \\epsilon_{A,i} \\sim \\mathcal{N}(0, \\sigma_A^2) $$\n$$ [B]_{\\text{meas}}(t_i) = [B]_{\\text{true}}(t_i) + \\epsilon_{B,i}, \\quad \\text{where} \\quad \\epsilon_{B,i} \\sim \\mathcal{N}(0, \\sigma_B^2) $$\n噪声的标准差 $\\sigma_A$ 和 $\\sigma_B$ 对每个用例都有指定。为确保可复现性，每个用例都使用一个特定的种子来初始化伪随机数生成器。\n\n参数估计是通过最小化加权残差平方和 (WSSR) 来执行的，该方法也称为卡方 ($\\chi^2$) 目标函数。给定一组测量数据点 $(t_i, y_i)$、它们的不确定度 $\\sigma_i$ 以及一个依赖于参数 $\\boldsymbol{\\theta}$ 的模型函数 $y_{\\text{model}}(t; \\boldsymbol{\\theta})$，目标是找到使 $\\chi^2$ 最小化的参数 $\\boldsymbol{\\theta}^*$：\n$$ \\boldsymbol{\\theta}^* = \\arg\\min_{\\boldsymbol{\\theta}} \\chi^2(\\boldsymbol{\\theta}) = \\arg\\min_{\\boldsymbol{\\theta}} \\sum_{i} \\left( \\frac{y_i - y_{\\text{model}}(t_i; \\boldsymbol{\\theta})}{\\sigma_i} \\right)^2 $$\n这个最小化过程将使用 `scipy.optimize.minimize` 函数进行数值计算，并设置边界以强制对所有参数（浓度和速率常数）施加物理上的非负约束。\n\n我们将把这个框架应用于三种不同的估计场景：\n\n1.  **仅对 $[A](t)$ 单独拟合**：\n    模型是 $[A]_{\\text{model}}(t) = [A]_0 e^{-k_A t}$。未知参数向量是 $\\boldsymbol{\\theta}_A = ([A]_0, k_A)$。要最小化的目标函数是：\n    $$ \\chi^2_A([A]_0, k_A) = \\sum_{i} \\left( \\frac{[A]_{\\text{meas}}(t_i) - [A]_0 e^{-k_A t_i}}{\\sigma_A} \\right)^2 $$\n    参数 $[A]_0$ 和 $k_A$ 的估计受到 $[A]_0 \\ge 0$ 和 $k_A \\ge 0$ 约束。得到的速率常数估计值记为 $k_A$。\n\n2.  **仅对 $[B](t)$ 单独拟合**：\n    模型是 $[B]_{\\text{model}}(t) = [B]_0 + [A]_0(1 - e^{-k_B t})$。在这里，未知参数向量是 $\\boldsymbol{\\theta}_B = ([B]_0, [A]_0, k_B)$。目标函数是：\n    $$ \\chi^2_B([B]_0, [A]_0, k_B) = \\sum_{i} \\left( \\frac{[B]_{\\text{meas}}(t_i) - \\left( [B]_0 + [A]_0(1 - e^{-k_B t_i}) \\right)}{\\sigma_B} \\right)^2 $$\n    所有三个参数都从 $[B]$ 的单个时间序列中估计得出，并受到 $[B]_0 \\ge 0$、$[A]_0 \\ge 0$ 和 $k_B \\ge 0$ 约束。速率常数估计值为 $k_B$。\n\n3.  **对 $[A](t)$ 和 $[B](t)$ 进行联合拟合**：\n    这种方法同时使用所有可用信息，并在两个数据集上一致地强制执行物理模型。未知参数向量是 $\\boldsymbol{\\theta}_{\\text{joint}} = ([A]_0, [B]_0, k_{\\text{joint}})$。关于 $[A](t)$ 和 $[B](t)$ 的模型通过这些共享参数耦合在一起：\n    $$ [A]_{\\text{model}}(t) = [A]_0 e^{-k_{\\text{joint}} t} $$\n    $$ [B]_{\\text{model}}(t) = [B]_0 + [A]_0(1 - e^{-k_{\\text{joint}} t}) $$\n    目标函数是每个序列的 $\\chi^2$ 值之和：\n    $$ \\chi^2_{\\text{joint}}(\\boldsymbol{\\theta}_{\\text{joint}}) = \\sum_{i} \\left( \\frac{[A]_{\\text{meas}}(t_i) - [A]_0 e^{-k_{\\text{joint}} t_i}}{\\sigma_A} \\right)^2 + \\sum_{i} \\left( \\frac{[B]_{\\text{meas}}(t_i) - \\left( [B]_0 + [A]_0(1 - e^{-k_{\\text{joint}} t_i}) \\right)}{\\sigma_B} \\right)^2 $$\n    参数的估计受到 $[A]_0 \\ge 0$、$[B]_0 \\ge 0$ 和 $k_{\\text{joint}} \\ge 0$ 约束。得到的估计值为 $k_{\\text{joint}}$。由于联合拟合整合了所有数据并强制执行所有已知的物理约束，预计它将产生统计上最稳健的估计。\n\n对于每个测试用例，我们将执行这三种拟合过程，提取估计出的速率常数 $k_A$、$k_B$ 和 $k_{\\text{joint}}$，并在四舍五入到指定精度后报告它们。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Main solver function that processes all test cases for parameter estimation.\n    \"\"\"\n    test_cases = [\n        # Case 1 (general, well-conditioned)\n        {\n            \"k_true\": 0.35, \"A0_true\": 1.1, \"B0_true\": 0.3,\n            \"t_grid\": np.linspace(0, 8.0, 41),\n            \"sigma_A\": 0.01, \"sigma_B\": 0.01,\n            \"seed\": 13579\n        },\n        # Case 2 (slow kinetics, long observation window)\n        {\n            \"k_true\": 0.02, \"A0_true\": 2.0, \"B0_true\": 0.4,\n            \"t_grid\": np.linspace(0, 200.0, 51),\n            \"sigma_A\": 0.01, \"sigma_B\": 0.015,\n            \"seed\": 24680\n        },\n        # Case 3 (sparse sampling, fast kinetics)\n        {\n            \"k_true\": 1.5, \"A0_true\": 0.5, \"B0_true\": 0.0,\n            \"t_grid\": np.array([0.0, 0.2, 0.5, 1.0, 1.5]),\n            \"sigma_A\": 0.02, \"sigma_B\": 0.02,\n            \"seed\": 97531\n        }\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        # Step 1: Generate synthetic data\n        k_true, A0_true, B0_true = case[\"k_true\"], case[\"A0_true\"], case[\"B0_true\"]\n        t = case[\"t_grid\"]\n        sigma_A, sigma_B = case[\"sigma_A\"], case[\"sigma_B\"]\n        seed = case[\"seed\"]\n\n        A_true = A0_true * np.exp(-k_true * t)\n        B_true = B0_true + A0_true * (1 - np.exp(-k_true * t))\n        \n        rng = np.random.default_rng(seed)\n        noise_A = rng.normal(0, sigma_A, size=t.shape)\n        noise_B = rng.normal(0, sigma_B, size=t.shape)\n        \n        A_meas = A_true + noise_A\n        B_meas = B_true + noise_B\n\n        # --- Fitting Procedure A ---\n        def model_A(params, t):\n            A0, k = params\n            return A0 * np.exp(-k * t)\n\n        def objective_A(params, t, A_meas, sigma_A):\n            residuals = (A_meas - model_A(params, t)) / sigma_A\n            return np.sum(residuals**2)\n\n        # Initial guesses and bounds\n        A0_guess_A = A_meas[0] if A_meas[0] > 1e-6 else 1e-6\n        k_guess_A = 0.1 \n        initial_guess_A = [A0_guess_A, k_guess_A]\n        bounds_A = [(0, None), (0, None)]\n\n        res_A = minimize(objective_A, initial_guess_A, args=(t, A_meas, sigma_A),\n                         method='L-BFGS-B', bounds=bounds_A)\n        k_A = res_A.x[1]\n\n        # --- Fitting Procedure B ---\n        def model_B(params, t):\n            B0, A0, k = params\n            return B0 + A0 * (1 - np.exp(-k * t))\n\n        def objective_B(params, t, B_meas, sigma_B):\n            residuals = (B_meas - model_B(params, t)) / sigma_B\n            return np.sum(residuals**2)\n\n        # Initial guesses and bounds\n        B0_guess_B = B_meas[0] if B_meas[0] > 1e-6 else 1e-6\n        A0_guess_B = (B_meas[-1] - B_meas[0]) if (B_meas[-1] - B_meas[0]) > 1e-6 else 1e-6\n        k_guess_B = 0.1\n        initial_guess_B = [B0_guess_B, A0_guess_B, k_guess_B]\n        bounds_B = [(0, None), (0, None), (0, None)]\n        \n        res_B = minimize(objective_B, initial_guess_B, args=(t, B_meas, sigma_B),\n                         method='L-BFGS-B', bounds=bounds_B)\n        k_B = res_B.x[2]\n\n        # --- Fitting Procedure Joint ---\n        def model_joint_A(params, t):\n            A0, _, k = params # A0, B0, k\n            return A0 * np.exp(-k * t)\n\n        def model_joint_B(params, t):\n            A0, B0, k = params\n            return B0 + A0 * (1 - np.exp(-k * t))\n\n        def objective_joint(params, t, A_meas, B_meas, sigma_A, sigma_B):\n            res_A = (A_meas - model_joint_A(params, t)) / sigma_A\n            res_B = (B_meas - model_joint_B(params, t)) / sigma_B\n            return np.sum(res_A**2) + np.sum(res_B**2)\n\n        # Initial guesses and bounds\n        A0_guess_joint = A_meas[0] if A_meas[0] > 1e-6 else 1e-6\n        B0_guess_joint = B_meas[0] if B_meas[0] > 1e-6 else 1e-6\n        k_guess_joint = 0.1\n        initial_guess_joint = [A0_guess_joint, B0_guess_joint, k_guess_joint]\n        bounds_joint = [(0, None), (0, None), (0, None)]\n\n        res_joint = minimize(objective_joint, initial_guess_joint, \n                             args=(t, A_meas, B_meas, sigma_A, sigma_B),\n                             method='L-BFGS-B', bounds=bounds_joint)\n        k_joint = res_joint.x[2]\n\n        # Format and store results\n        k_A_rounded = round(k_A, 6)\n        k_B_rounded = round(k_B, 6)\n        k_joint_rounded = round(k_joint, 6)\n        \n        all_results.append([k_A_rounded, k_B_rounded, k_joint_rounded])\n\n    # Final print statement in the exact required format.\n    # Convert each inner list to a string without spaces, then join them\n    inner_lists_str = [f\"[{','.join(f'{x:.6f}' for x in res)}]\" for res in all_results]\n    print(f\"[{','.join(inner_lists_str)}]\")\n\nsolve()\n```", "id": "2660601"}]}