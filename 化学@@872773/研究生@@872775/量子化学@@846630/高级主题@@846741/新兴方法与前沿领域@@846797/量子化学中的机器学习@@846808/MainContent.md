## 引言
在[量子化学](@entry_id:140193)领域，研究者们始终面临着一个根本性的权衡：计算精度与计算成本。高精度的从头算方法，如[耦合簇理论](@entry_id:141746)（[CCSD(T)](@entry_id:271595)），能够提供“金标准”的预测，但其高昂的计算开销限制了其在较[大分子](@entry_id:150543)体系和[长时间尺度模拟](@entry_id:751459)中的应用。另一方面，计算成本较低的方法，如密度泛函理论（DFT）的某些近似或[经典力场](@entry_id:747367)，虽然速度快，但其精度和普适性往往难以满足前沿研究的需求。机器学习（ML）的崛起，为打破这一困境提供了革命性的途径，它有望以较低的计算成本实现高精度的预测，从而极大地扩展[计算化学](@entry_id:143039)的疆域。

然而，将机器学习应用于分子体系并非简单的[曲线拟合](@entry_id:144139)。分子是遵循量子力学规律的物理系统，成功的机器学习模型必须内在地理解并尊重这些规律。本文旨在系统性地介绍如何构建和应用物理知情的机器学习模型来解决[量子化学](@entry_id:140193)中的挑战。读者将通过本文学习到一套完整的知识体系，从基本原理到前沿应用，再到动手实践。

在第一章“原理与机制”中，我们将深入探讨构建这些模型的基础。我们将从如何将三维[分子结构](@entry_id:140109)转化为机器可读的表征开始，学习如何设计能够自动满足旋转、平移和[置换不变性](@entry_id:753356)等基本对称性的[神经网络架构](@entry_id:637524)，并确保模型能遵守[能量守恒](@entry_id:140514)等关键物理约束。

随后的第二章“应用与[交叉](@entry_id:147634)学科联系”将展示这些原理在实际科研中的威力。我们将探索如何利用机器学习构建高保真[势能面](@entry_id:147441)，以开展前所未有的大规模[分子动力学模拟](@entry_id:160737)；如何预测偶极矩、极化率等分子性质以模拟[光谱](@entry_id:185632)；以及如何将[机器学习与密度泛函理论](@entry_id:180942)等[第一性原理方法](@entry_id:268553)深度融合，以期从根本上改进理论本身。

最后，为了将理论付诸实践，第三章“动手实践”提供了一系列精心设计的编程练习，引导读者亲手实现[径向基函数](@entry_id:754004)、通过力匹配训练模型，并验证所学[力场](@entry_id:147325)的物理保守性。

通过这三个层次的递进学习，本文将为您揭示机器学习如何不仅仅是加速计算的工具，更是一种能够与物理第一性原理深度融合、共同推动科学发现的强大[范式](@entry_id:161181)。现在，让我们从构建这些智能模型的核心——其原理与机制——开始我们的探索之旅。

## 原理与机制

在将机器学习应用于[量子化学](@entry_id:140193)的复杂任务中，其核心挑战不仅仅是简单地拟合数据，而是要构建能够体现并遵循底层物理规律的模型。本章旨在深入阐述构建这些物理知情（physics-informed）机器学习模型所依据的核心原理与关键机制。我们将从如何将分子的三维结构编码为机器可读的格式开始，探索如何设计能捕捉[量子力学对称性](@entry_id:140809)的[神经网络架构](@entry_id:637524)，并讨论确保模型预测（如能量和力）在物理上自洽的先进策略。我们还将通过具体的应用案例，如开发[交换相关泛函](@entry_id:142042)和高精度[势能面](@entry_id:147441)，来说明这些原理的实际效用，并最终探讨如何量化和理解我们模型的预测不确定性。

### [分子表征](@entry_id:752125)与[基本对称性](@entry_id:161256)

将机器学习应用于分子系统的第一个基本步骤是**表征 (representation)**，即将一个由[原子核](@entry_id:167902)[电荷](@entry_id:275494) $\{Z_i\}$ 和三维坐标 $\{\mathbf{R}_i\}$ 定义的分子，转化为机器学习模型能够处理的固定格式的输入特征。一个有效的表征必须尊重分子固有的物理对称性，即当分子整体平移、旋转或交换相同种类原子的标签时，其物理性质（如总能量）应保持不变。

一个经典且极具启发性的[分子表征](@entry_id:752125)是**库仑矩阵 (Coulomb matrix)** [@problem_id:2903792]。对于一个包含 $N$ 个原子的分子，其库仑矩阵 $M$ 是一个 $N \times N$ 的对称矩阵。其非对角元素 $M_{ij}$ ($i \neq j$) 描述[原子核](@entry_id:167902) $i$ 和 $j$ 之间的[静电排斥](@entry_id:162128)，定义为 $M_{ij} = \frac{Z_i Z_j}{\|\mathbf{R}_i - \mathbf{R}_j\|}$。对角元素 $M_{ii}$ 则编码了单个原子的信息，通常选择为与自由原子能量相关的关于核[电荷](@entry_id:275494) $Z_i$ 的[多项式拟合](@entry_id:178856)，一个常见的选择是 $M_{ii} = \frac{1}{2} Z_i^{2.4}$。

库仑矩阵的设计巧妙地解决了**平移不变性 (translational invariance)** 和**[旋转不变性](@entry_id:137644) (rotational invariance)**。由于[矩阵元](@entry_id:186505)素仅依赖于原子间的距离 $\|\mathbf{R}_i - \mathbf{R}_j\|$，而距离在刚性平移和旋转下是不变的，因此矩阵 $M$ 本身不随分子的整体平移和旋转而改变 [@problem_id:2903792]。

然而，库仑矩阵并未完全解决**[置换不变性](@entry_id:753356) (permutational invariance)**。如果我们将原子 $i$ 和 $j$ 的标签互换，这相当于同时交换矩阵的第 $i$ 行与第 $j$ 行，以及第 $i$ 列与第 $j$ 列。用矩阵语言来说，如果一个[置换](@entry_id:136432)操作由[置换矩阵](@entry_id:136841) $P$ 表示，则库仑矩阵会发生一个相似变换：$M \mapsto P M P^{\top}$。这意味着库仑矩阵对于原子标签的[置换](@entry_id:136432)是**协变 (covariant)** 的，而非不变的。

为了从[协变](@entry_id:634097)的库仑矩阵中获得一个真正不变的表征，需要进行后处理。常见策略包括：
1.  **排序 (Sorting)**：通过一个确定性的规则对矩阵的行和列进行排序。例如，可以按照每行范数（如 $\ell_2$ 范数）的大小降序[排列](@entry_id:136432)。这样可以为任何分子生成一个唯一的、规范化的矩阵。然而，这种方法存在一个致命缺陷：**不连续性 (discontinuity)**。当两个原子的行范数非常接近时，[分子构象](@entry_id:163456)的微小扰动可能会导致它们的排序发生交换，从而使得最终的表征向量发生剧烈、非物理的跳变 [@problem_id:2903792]。
2.  **[谱方法](@entry_id:141737) (Spectral method)**：使用矩阵的[本征值](@entry_id:154894)集合（谱）作为表征。由于[相似变换](@entry_id:152935)不改变矩阵的[本征值](@entry_id:154894)，因此库仑矩阵的谱天然地满足[置换不变性](@entry_id:753356)，并且相对于分子坐标的变化是连续的。但这种方法也并非完美，其主要问题在于**唯一性 (uniqueness)**。在数学上，存在“同谱非同构”的图，即结构不同但具有相同[本征值](@entry_id:154894)谱的图。类似地，两个不同的[分子构象](@entry_id:163456)也可能（尽管很罕见）产生相同的库仑矩阵谱，这会导致模型无法区分它们。

库仑矩阵的例子深刻地揭示了构建[分子表征](@entry_id:752125)的核心困境，并推动了更先进的、能够端到端学习满足对称性要求的表征的现代[神经网络架构](@entry_id:637524)的发展。

### [几何深度学习](@entry_id:636472)架构

现代方法倾向于直接从原子坐标和种类中学习满足物理对称性的特征，而不是依赖于手工设计的描述符。**[消息传递神经网络](@entry_id:751916) (Message Passing Neural Networks, MPNNs)** 及其变体，如 SchNet 和 DimeNet，已成为这一领域的标准架构。

#### 物理一致的消息传递机制

MPNN 的核心思想是，每个原子（或节点）的特征表示是通过迭代地聚合其邻近原子（邻居）传递来的“消息”来更新的。一个典型的 MPNN 包含一系列相互作用层，在第 $t$ 层，原子 $i$ 的隐藏态（[特征向量](@entry_id:151813)）$h_i^{(t)}$ 会根据其自身和邻居的信息进行更新，得到 $h_i^{(t+1)}$。

要构建一个物理上合理的 MPNN [势能面](@entry_id:147441)模型，必须在其架构设计中精心嵌入核心的对称性原理 [@problem_id:2903834]。一个通用的消息传递更新规则可以写为：
$$
h_i^{(t+1)} = \phi\Big(h_i^{(t)}, \sum_{j \ne i} c(r_{ij})\, \psi\big(h_i^{(t)}, h_j^{(t)}, r_{ij}\big)\Big)
$$
其中 $r_{ij} = \|\mathbf{r}_i - \mathbf{r}_j\|$ 是原子间距。这个公式的每一部分都服务于一个特定的物理约束：
- **[置换不变性](@entry_id:753356)**：通过对来自所有邻居 $j$ 的消息进行**求和 ($\sum$)** 来实现。求和是一个对称的聚合操作，不依赖于邻居的[排列](@entry_id:136432)顺序。这确保了如果交换两个邻居原子的标签，总的聚合消息保持不变，从而使得原子特征的更新与原子索引无关，最终得到的总能量 $E = \sum_i E_i(h_i^{(T)})$ 也是[置换](@entry_id:136432)不变的。
- **平移与[旋转不变性](@entry_id:137644)**：通过确保消息函数 $\psi$ 仅依赖于[旋转和平移](@entry_id:175994)[不变量](@entry_id:148850)来实现，最基本的就是原子间**距离 $r_{ij}$**。只要初始原子特征 $h_i^{(0)}$ 是标量（如仅依赖于[原子序数](@entry_id:139400) $Z_i$），并且所有后续更新都只使用标量距离，那么所有隐藏态 $h_i^{(t)}$ 和最终的能量都将是平移和旋转不变的。
- **局域性 (Locality)**：物理上，原子间的相互作用是“近视”的，即一个原子的电[子环](@entry_id:154194)境主要由其近邻决定。这通过引入一个**截断函数 (cutoff function)** $c(r_{ij})$ 来实现。该函数在一个有限的[截断半径](@entry_id:136708) $r_c$ 之外为零 ($c(r_{ij})=0$ for $r_{ij} \ge r_c$)，从而确保只有在 $r_c$ 范围内的邻居才能对中心原子 $i$ 产生直接影响。
- **[可微性](@entry_id:140863) (Differentiability)**：为了从学到的[势能面](@entry_id:147441) $E$ 中计算出物理上有意义且连续的力 $F_i = -\nabla_{\mathbf{r}_i} E$，能量函数必须对原子坐标是可微的。截断函数 $c(r_{ij})$ 在截断边界 $r_c$ 处的行为至关重要。一个突然的截断（如[阶跃函数](@entry_id:159192)）会在原子穿越 $r_c$ 时导致能量和力的不连续跳变。因此，必须使用**平滑的截断函数**，该函数及其一阶导数在 $r_c$ 处平滑地衰减至零（即 $c(r_c) = 0$ 和 $c'(r_c) = 0$）。

#### 不变性与[等变性](@entry_id:636671)：一个关键的区别

对于要预测的物理量，我们需要仔细区分两种对称性行为：**[不变性](@entry_id:140168) (invariance)** 和**[等变性](@entry_id:636671) (equivariance)**。
- **[不变性](@entry_id:140168)**：当对输入施加一个[对称操作](@entry_id:143398)（如旋转）时，输出**保持不变**。标量性质，如总能量，是旋转不变的。
- **[等变性](@entry_id:636671)**：当对输入施加一个对称操作时，输出以一种**可预测的、相应的方式**进行变换。矢量和张量性质，如力、偶极矩和极化率，是旋转等变的。例如，如果将分子旋转一个矩阵 $\mathbf{Q}$，其偶极矩矢量 $\boldsymbol{\mu}$ 也会同样旋转：$\boldsymbol{\mu}(\mathbf{Q}\mathbf{X}) = \mathbf{Q}\boldsymbol{\mu}(\mathbf{X})$。

混淆这两种性质会导致模型设计的根本性错误 [@problem_id:2903793]。假设我们要训练一个模型来预测偶极矩 $\boldsymbol{\mu}$。如果我们错误地使用一个**不变的 (invariant)** [神经网络架构](@entry_id:637524)，该架构的设计保证了对于任何旋转 $\mathbf{Q}$，其输出 $g(\mathbf{Q}\mathbf{X}) = g(\mathbf{X})$。然而，训练数据告诉我们，真实的标签应该满足 $g(\mathbf{X}) \approx \boldsymbol{\mu}$ 和 $g(\mathbf{Q}\mathbf{X}) \approx \mathbf{Q}\boldsymbol{\mu}$。将这两个条件结合，模型必须满足 $\boldsymbol{\mu} \approx \mathbf{Q}\boldsymbol{\mu}$。对于一个非零的偶极矩 $\boldsymbol{\mu} \neq \mathbf{0}$ 和任意的旋转 $\mathbf{Q}$，这个条件是不可能成立的。唯一能满足所有旋转的向量是[零向量](@entry_id:156189)。因此，一个不变的架构在面对非零向量预测任务时，其唯一能做出的自洽预测就是输出零，这使其无法学习到任何有用的信息。

正确的做法是使用一个**等变的 (equivariant)** 架构。这种架构被设计为先天满足 $f(\mathbf{Q}\mathbf{X}) = \mathbf{Q}f(\mathbf{X})$ 的变换规则。这样，模型就内置了正确的物理**[归纳偏置](@entry_id:137419) (inductive bias)**。它不需要从数据中“学习”旋转是什么，而是将学习任务简化为在任意一个[参考系](@entry_id:169232)下找到分子几何与其偶极矩之间的内在关系。这种方法极大地提高了数据效率和模型的泛化能力 [@problem_id:2903793] [@problem_id:2903830]。对于像偶极矩这样的非[零向量](@entry_id:156189)性质，只有当分子本身缺乏反演对称中心时才会出现。如果数据集中所有分子的偶极矩都为零（例如，都具有反演对称性），那么不变和等变模型都可以同样好地拟合数据，因为预测[零向量](@entry_id:156189)是两种模型都能做到的 [@problem_id:2903793]。

#### 基于群论构建等变层

构建[等变神经网络](@entry_id:137437)层需要借助[群表示论](@entry_id:141930)的数学工具 [@problem_id:2903794]。其核心思想是将原子上的特征表示为**球张量 (spherical tensors)**。一个 $\ell$ 阶的球张量特征 $x^{(\ell)}$ 是一组包含 $2\ell+1$ 个分量 $x^{(\ell)}_m$（其中 $m = -\ell, \dots, \ell$）的集合，它在[三维特殊正交群](@entry_id:138200) $\mathrm{SO}(3)$ 的旋转 $R$ 下，按照 $\ell$ 阶的 [Wigner D-矩阵](@entry_id:187739) $D^{(\ell)}(R)$ 进行变换：
$$
x^{(\ell)}_{m} \mapsto x^{(\ell)\prime}_{m} = \sum_{n=-\ell}^{\ell} D^{(\ell)}_{mn}(R)\, x^{(\ell)}_{n}
$$
标量是 $\ell=0$ 的张量，矢量是 $\ell=1$ 的张量，依此类推。

当我们将两个球张量特征 $x^{(\ell_1)}$ 和 $x^{(\ell_2)}$（例如，通过一个[双线性](@entry_id:146819)层）相互作用时，它们的乘积 $x^{(\ell_1)}_{m_1} x^{(\ell_2)}_{m_2}$ 的变换性质由两个 D-[矩阵的张量积](@entry_id:182766) $D^{(\ell_1)} \otimes D^{(\ell_2)}$ 决定，这是一个[可约表示](@entry_id:137110)。为了得到一个具有良好变换性质的输出特征（即一个 $\mathrm{SO}(3)$ 的[不可约表示](@entry_id:263310)），我们需要使用**克莱布施-戈登系数 (Clebsch-Gordan coefficients)** $C^{L M}_{\ell_{1} m_{1}, \ell_{2} m_{2}}$ 来进行耦合。这些系数正是量子力学中用于耦合两个角动量的工具。

一个通用的、等变的双线性层，它将两个输入特征 $x^{(\ell_1)}$ 和 $x^{(\ell_2)}$ 耦合，以产生一个特定阶数 $L$ 的输出特征 $y^{(L)}$，其形式如下：
$$
y^{(L)}_{M} = \sum_{\ell_{1},m_{1}} \sum_{\ell_{2},m_{2}} w_{\ell_{1}\ell_{2}L} \; C^{L M}_{\ell_{1} m_{1}, \ell_{2} m_{2}} \; x^{(\ell_{1})}_{m_{1}} \; x^{(\ell_{2})}_{m_{2}}
$$
这里的 $w_{\ell_{1}\ell_{2}L}$ 是可学习的权重。至关重要的是，这些权重必须是标量，即它们只能依赖于角动量阶数 $(\ell_1, \ell_2, L)$，而不能依赖于其磁量子数分量 $(m_1, m_2, M)$。任何对[磁量子数](@entry_id:145584)的依赖都会破坏[旋转对称](@entry_id:137077)性。这种基于群论的构建方式，是保证[神经网](@entry_id:276355)络层严格满足 $\mathrm{SO}(3)$ [等变性](@entry_id:636671)的根本方法 [@problem_id:2903794]。

### 实施物理约束

除了基本的[几何对称性](@entry_id:189059)，机器学习模型还必须遵守更深层次的物理定律。例如，分子动力学模拟所依赖的[力场](@entry_id:147325)必须是**保守的 (conservative)**，即力应该是某个势能函数的梯度的负值。

#### 能量-力的自洽性与[保守力场](@entry_id:164320)

在玻恩-奥本海默近似下，[原子核](@entry_id:167902)在电子云产生的[势能面](@entry_id:147441) $E(\mathbf{R})$ 上运动。作用在[原子核](@entry_id:167902)上的力 $\mathbf{F}(\mathbf{R})$ 是[势能](@entry_id:748988)的负梯度：
$$
\mathbf{F}(\mathbf{R}) = -\nabla_{\mathbf{R}}E(\mathbf{R})
$$
这个关系定义了一个**[保守力场](@entry_id:164320)**。根据矢量微积分的基本定理，一个[保守力场](@entry_id:164320)具有以下等价的数学性质 [@problem_id:2903797]：
1.  **路径无关性**：沿任意闭合路径 $\mathcal{C}$ [对力](@entry_id:159909)做线积分，结果为零，即 $\oint_{\mathcal{C}}\mathbf{F}(\mathbf{R})\cdot d\mathbf{l}=0$。这意味着系统从一个状态到另一个状态的能量变化与所经历的路径无关。
2.  **无旋性**：[力场的旋度](@entry_id:174409)处处为零，即 $\nabla_{\mathbf{R}} \times \mathbf{F}(\mathbf{R}) = \mathbf{0}$。
3.  **对称雅可比矩阵**：力的雅可比矩阵 $J_{ij}(\mathbf{R})=\partial F_i/\partial R_j$ 是对称的。这源于[克莱罗定理](@entry_id:139814)（Clairaut's theorem），因为 $J_{ij} = -\partial^2 E/(\partial R_j \partial R_i)$，对于一个足够平滑的[势能函数](@entry_id:200753) $E$，其[混合偏导数](@entry_id:139334)与求导次序无关。

在机器学习中，确保力-能量自洽性的最直接、最有力的方法是**“能量主导 (energy-based)”** 的建模策略 [@problem_id:2903797]。即，我们只训练一个模型来预测标量势能 $E_\theta(\mathbf{R})$。然后，力被定义为该模型势能的解析梯度，可以通过**[自动微分](@entry_id:144512) (automatic differentiation)** 精确计算出来，即 $\mathbf{F}_\theta(\mathbf{R}) = -\nabla_{\mathbf{R}}E_\theta(\mathbf{R})$。通过这种**“构造即保证 (by construction)”**的方式，无论模型的参数 $\theta$ 如何，预测的[力场](@entry_id:147325) $\mathbf{F}_\theta$ 都天然地是一个[保守力场](@entry_id:164320)，完美地满足了上述所有物理条件。值得注意的是，虽然在生成参考数据时，从[量子化学](@entry_id:140193)程序中计算精确的力可能需要包含复杂的**普莱项 (Pulay terms)** 来修正[基组](@entry_id:160309)随核坐标变化带来的影响，但一旦我们训练好一个自洽的[机器学习势](@entry_id:183033)能模型 $E_\theta$，其预测的力和能量之间的关系是内在一致的 [@problem_id:2903797]。

#### 硬约束 vs. 软约束

将物理定律融入[机器学习模型](@entry_id:262335)有两种主要策略：**硬约束 (hard constraints)** 和**软约束 (soft constraints)** [@problem_id:2903828]。

- **硬约束**是将物理定律直接构建到模型的架构中。前述的能量主导方法就是硬约束的一个绝佳例子，它通过架构设计保证了[力场](@entry_id:147325)的保守性。类似地，使用[等变网络](@entry_id:143881)架构来保证正确的对称性也是一种硬约束。
- **软约束**则是通过在[损失函数](@entry_id:634569)中添加惩罚项来鼓励模型学习物理定律。例如，如果我们独立地训练一个矢量模型来预测力 $\mathbf{F}_\theta$，我们可以添加一个惩罚项 $\lambda_{\mathrm{curl}} \, \|\nabla_{\mathbf{R}} \times \mathbf{F}_\theta\|^2$ 来使其趋向于无旋。

这两种方法各有优劣 [@problem_id:2903828]：
- **保证与近似**：硬约束能**精确**地满足物理定律。软约束只能在惩罚权重 $\lambda \to \infty$ 的极限下才能完全满足约束，对于任何有限的 $\lambda$，它只是一个近似，模型会在拟合数据和遵守物理定律之间做出权衡。
- **表达能力与泛化**：如果物理定律是确定无疑的（如对称性和力-能量自洽性），施加硬约束可以有效地缩小[假设空间](@entry_id:635539)，排除掉所有不物理的模型。根据[统计学习理论](@entry_id:274291)，减小[假设空间](@entry_id:635539)的复杂度（例如，通过降低其 [Rademacher 复杂度](@entry_id:634858)）通常会带来更好的**泛化能力**，即在有限的训练数据下表现得更好，而不会增加额外的**近似误差**（只要真实物理函数本身就在被约束的[假设空间](@entry_id:635539)内）。
- **优化**：硬约束和软约束对应着不同的[优化问题](@entry_id:266749)。软约[束方法](@entry_id:636307)通过修改损失[函数的曲率](@entry_id:173664)来引导优化，而硬约束则从一开始就将优化过程限制在一个满足约束的[子流形](@entry_id:159439)上。声称硬约束必然会恶化训练的说法是错误的；实际上，通过排除与对称性相关的冗余自由度，硬约束常常能简化优化过程。

总而言之，当物理定律是严格且普适的，采用硬约束通常是更优越的选择，因为它提供了精确的物理保证和更好的数据效率。

### 先进学习策略与应用

除了架构设计，一些特定的学习策略和应用场景也极大地推动了机器学习在[量子化学](@entry_id:140193)中的成功。

#### 案例研究一：学习[交换相关泛函](@entry_id:142042)

密度泛函理论 (DFT) 的核心是找到精确的**交换相关 (exchange-correlation, XC) 泛函** $E_{xc}[n]$。机器学习为此提供了一个强大的数据驱动[范式](@entry_id:161181)。

在 [Kohn-Sham DFT](@entry_id:172809) 框架中，一个多电子体系的总能量被精确地分解为：
$$
E[n] = T_{s}[n] + E_{H}[n] + E_{ext}[n] + E_{xc}[n]
$$
其中 $T_s[n]$ 是非相互作用参考体系的动能，$E_H[n]$ 是经典的哈特里[静电能](@entry_id:267406)，$E_{ext}[n]$ 是外部势能。$E_{xc}[n]$ 则囊括了所有剩余的、复杂的量子[多体效应](@entry_id:173569)。其严格定义为 $E_{xc}[n] = (T[n] - T_s[n]) + (E_{ee}[n] - E_H[n])$，即真实动能与非相互作用动能之差，加上真实电子互作用能与经典[哈特里能量](@entry_id:167303)之差 [@problem_id:2903784]。

对于常用的半局域 (semilocal) 或 [meta-GGA](@entry_id:191648) 泛函，XC 能量被写成一个积分形式 $E_{xc}[n] = \int n(\mathbf{r}) e_{xc}(\mathbf{r}) d\mathbf{r}$，其中 $e_{xc}$ 是单位粒子的[交换相关能](@entry_id:138029)量密度。机器学习的任务，就是学习从局域的电子密度描述符（如密度 $n(\mathbf{r})$、密度梯度 $\nabla n(\mathbf{r})$ 和动能密度 $\tau(\mathbf{r})$）到 $e_{xc}$ 这个标量的映射关系：
$$
\big(n(\mathbf{r}), \nabla n(\mathbf{r}), \tau(\mathbf{r})\big) \mapsto e_{xc}(\mathbf{r})
$$
然而，一个在有限数据上训练的 ML-XC 泛函面临着严峻的**迁移性 (transferability)** 挑战 [@problem_id:2903830]。如果一个泛函只在小的、[电中性](@entry_id:157680)的、闭壳层分子数据上训练，它将很难正确地描述性质迥异的体系，如开壳层[自由基](@entry_id:164363)和分子离子。这是因为它未能从训练数据中学到普适的物理约束：
- **[自相互作用误差](@entry_id:139981) (Self-Interaction Error)**：泛函未能满足单电子体系中[交换能](@entry_id:137069)与[哈特里能量](@entry_id:167303)精确抵消的条件。
- **[导数不连续性](@entry_id:136336) (Derivative Discontinuity)**：泛函未能体现总能量 $E$ 关于电子数 $N$ 在整数点处的[导数不连续性](@entry_id:136336)，导致对分数[电荷](@entry_id:275494)体系的描述出现严重偏差。
- **正确的渐进行为**：泛函产生的[交换相关势](@entry_id:180254) $v_{xc}(\mathbf{r})$ 在 $r \to \infty$ 时通常衰减过快，而不是正确的 $-1/r$ 行为，这使得它无法准确束缚额外的电子，从而严重低估[电子亲和能](@entry_id:147520)。

单纯增加更多同类型（中性、闭壳层）的训练数据并不能解决这些问题。根本的解决方案在于将这些物理约束作为先验知识引入学习过程，例如：在[训练集](@entry_id:636396)中包含单电子和分数[电荷](@entry_id:275494)体系的数据来强制学习正确的行为；或者采用**范围分离 (range-separation)** 的方法，将长程交换部分用精确的 Hartree-Fock 交换来处理，以保证正确的渐进行为，而只让[机器学习模型](@entry_id:262335)负责复杂的短程部分 [@problem_id:2903830]。

#### 案例研究二：通过 $\Delta$-学习构建高保真[势能面](@entry_id:147441)

另一个强大的策略是**$\Delta$-学习 ($\Delta$-learning)**，或称**[残差学习](@entry_id:634200) (residual learning)** [@problem_id:2903824]。其目标是预测一个高精度、高成本的能量（如[耦合簇理论](@entry_id:141746) $E^{\mathrm{CC}}$），但不是直接学习 $E^{\mathrm{CC}}$，而是学习它与一个低成本、中等精度的基线方法（如 DFT，$E^{\mathrm{DFT}}$）之间的差值：
$$
E^{\mathrm{CC}}(\mathbf{R}) = E^{\mathrm{DFT}}(\mathbf{R}) + \Delta_{\theta}(\mathbf{R})
$$
模型 $ \Delta_{\theta}(\mathbf{R})$ 的学习目标是残差 $\Delta(\mathbf{R}) = E^{\mathrm{CC}}(\mathbf{R}) - E^{\mathrm{DFT}}(\mathbf{R})$。

这种策略之所以非常有效，其原因植根于物理和[统计学习理论](@entry_id:274291)：
- **简化学习目标**：$E^{\mathrm{DFT}}$ 通常已经捕捉了 $E^{\mathrm{CC}}$ 的绝大部分信息（如化学键的形成、原子排斥等“低频”的、大幅度的能量变化）。残差 $\Delta$ 则代表了更精细的、高阶的电子相关效应，其[绝对值](@entry_id:147688)远小于总能量，并且随几何构象的变化通常更为平滑或更具“高频”特征。学习这个“小而精”的残差函数比学习庞大而复杂的总能量函数要容易得多。
- **改善样本效率**：从[统计学习理论](@entry_id:274291)的角度看，一个“更简单”的[目标函数](@entry_id:267263)通常在某个合适的函数空间（如[再生核希尔伯特空间](@entry_id:633928), RKHS）中具有更小的范数。学习一个范数更小的函数所需的样本数量更少。因此，通过让 DFT 基线处理大部[分工](@entry_id:190326)作，$\Delta$-学习显著提高了达到给定精度所需的训练样本效率 [@problem_id:2903824]。
- **保持物理性质**：如果基线方法和高精度方法都满足某个物理性质（如**[广延性](@entry_id:144932) (size-extensivity)**，即能量对非相互作用子体系是可加的），那么它们的差值 $\Delta$ 也将保持该性质。这对于模型正确地外推到比[训练集](@entry_id:636396)更大的系统至关重要 [@problem_id:2903824]。

### 量化预测不确定性

一个负责任的机器学习模型不仅应给出预测值，还应提供对其预测可靠性的评估。预测的不确定性主要分为两类 [@problem_id:2903781]：

1.  **[偶然不确定性](@entry_id:154011) (Aleatoric Uncertainty)**：也称为**数据不确定性 (data uncertainty)**。它源于数据生成过程中的内在随机性或噪声。例如，实验测量总会伴随[测量误差](@entry_id:270998)。这种不确定性是**不可约减的**，即使拥有无限多的数据也无法消除。

2.  **认知不确定性 (Epistemic Uncertainty)**：也称为**[模型不确定性](@entry_id:265539) (model uncertainty)**。它源于模型自身的局限性和训练数据的不足。当模型对输入空间中未见过或采样稀疏的区域进行预测时，它会表现出较高的认知不确定性。这种不确定性是**可约减的**，通过增加更多（特别是多样化的）训练数据可以降低。

在为[势能面](@entry_id:147441)建模的典型场景中，参考能量通常由**确定性的 (deterministic)** [量子化学](@entry_id:140193)方法（如 DFT 或 CC）在严格的收敛阈值下计算得出。这意味着对于给定的原子构型 $\mathbf{R}$，计算出的能量 $E(\mathbf{R})$ 是一个固定的值，不存在统计噪声。因此，在这种设定下，**[偶然不确定性](@entry_id:154011)可以忽略不计** [@problem_id:2903781]。

主导的误差来源是**[认知不确定性](@entry_id:149866)**。由于分子的[构型空间](@entry_id:149531)极其广阔，任何有限的训练集都只能覆盖其极小的一部分。当模型需要对远离训练数据的构型进行外推时，其预测的可靠性就会下降。

评估认知不确定性的常用方法包括**模型集成 (model ensembling)**（训练多个具有不同初始化的模型并考察其预测的[分歧](@entry_id:193119)度）和**[贝叶斯神经网络](@entry_id:746725) (Bayesian Neural Networks)**（对模型参数进行[概率建模](@entry_id:168598)）。这些方法主要针对的是[认知不确定性](@entry_id:149866)。如果数据中确实存在显著的[偶然不确定性](@entry_id:154011)（例如，当使用随机的[量子蒙特卡洛方法](@entry_id:753887)生成标签时），则需要采用能输出[预测分布](@entry_id:165741)的显式[噪声模型](@entry_id:752540)（如异[方差](@entry_id:200758)[似然](@entry_id:167119)模型）来捕捉它 [@problem_id:2903781]。