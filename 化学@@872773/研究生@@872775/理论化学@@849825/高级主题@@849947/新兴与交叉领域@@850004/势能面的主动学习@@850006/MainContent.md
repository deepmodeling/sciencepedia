## 引言
在现代化学与[材料科学](@entry_id:152226)中，[势能面](@entry_id:147441)（Potential Energy Surface, PES）是连接微观原子构型与宏观体系性质的桥梁，是理解和预测分子行为的基石。然而，通过高精度的[量子化学](@entry_id:140193)方法（如密度泛函理论或[耦合簇理论](@entry_id:141746)）来全面绘制一个高维[势能面](@entry_id:147441)，其计算成本往往高得令人望而却步，这极大地限制了我们对大规模、长时间尺度[分子动力学](@entry_id:147283)过程的研究。这一根本性的挑战催生了机器学习在这一领域的应用，旨在以数据驱动的方式构建出兼具[量子化学](@entry_id:140193)精度和[经典力场](@entry_id:747367)效率的代理模型。

本文聚焦于一种名为“主动学习”（Active Learning）的先进方法，它被证明是解决数据稀缺性问题、高效构建高质量[机器学习势](@entry_id:183033)能面的关键策略。与被动地使用随机或预先生成的数据集不同，主动学习让模型能够智能地识别其知识的“边界”，并主动请求在最不确定的、信息量最大的[构型空间](@entry_id:149531)区域进行新的[高精度计算](@entry_id:200567)。这种“按需学习”的[范式](@entry_id:161181)，旨在以最少的计算资源投入，达到预设的模型精度目标。

在接下来的内容中，我们将系统地、深入地剖析[主动学习](@entry_id:157812)在[势能面](@entry_id:147441)构建中的应用。**第一章：原理与机制**，将奠定理论基础，阐明[势能面](@entry_id:147441)必须满足的基本物理对称性、克服[维度灾难](@entry_id:143920)的[局域性原理](@entry_id:753741)，并详细拆解[主动学习](@entry_id:157812)的完整循环，从模型训练、[不确定性量化](@entry_id:138597)到[数据采集](@entry_id:273490)与[停止准则](@entry_id:136282)。**第二章：应用与跨学科[交叉](@entry_id:147634)**，将展示这些原理如何转化为解决前沿科学问题的强大工具，涵盖从增强分子动力学模拟到探索[化学反应](@entry_id:146973)机理和非绝热现象的广泛应用。最后，**第三章：动手实践**，将提供一系列精心设计的编码练习，帮助读者将理论知识转化为实际操作能力。通过这一完整的学习路径，读者将能够全面掌握构建与应用[主动学习](@entry_id:157812)[势能面](@entry_id:147441)的核心知识与技能。

## 原理与机制

本章深入探讨了构建[机器学习势](@entry_id:183033)能面（Potential Energy Surfaces, PES）的核心原理与关键机制，特别聚焦于[主动学习](@entry_id:157812)（Active Learning）框架。我们将从[势能面](@entry_id:147441)的基本物理对称性出发，阐明为何必须在模型中引入这些对称性。随后，我们将讨论“维度灾难”这一根本性挑战，并揭示局域性分解（local decomposition）如何成为克服该挑战的有力策略。在此基础上，我们将详细介绍主动学习的完整循环，包括模型训练、[不确定性量化](@entry_id:138597)、数据获取策略，以及确保模型可靠性的校准与[停止准则](@entry_id:136282)。本章旨在为读者构建一个系统、严谨的理论框架，以理解和应用先进的[势能面](@entry_id:147441)构建方法。

### [势能面](@entry_id:147441)及其[基本对称性](@entry_id:161256)

在玻恩–奥本海默（Born–Oppenheimer）近似下，一个分子体系的[势能面](@entry_id:147441) $E(\mathbf{R})$ 是一个标量函数，它将[原子核](@entry_id:167902)的构型 $\mathbf{R} = (\mathbf{R}_1, \dots, \mathbf{R}_N) \in \mathbb{R}^{3N}$ 映射到体系的[基态](@entry_id:150928)电子能量与[原子核](@entry_id:167902)间[库仑排斥](@entry_id:181876)能之和。这个函数是[分子模拟](@entry_id:182701)的基石，其精确描述是[理论化学](@entry_id:199050)的核心任务之一。

从物理第一性原理出发，任何一个有效的[势能面](@entry_id:147441)模型都必须尊重其所描述的物理体系的内在对称性。这些对称性源于[欧几里得空间](@entry_id:138052)的基本性质以及[全同粒子](@entry_id:142755)的不可区分性。具体而言，[势能面](@entry_id:147441) $E(\mathbf{R})$ 必须满足以下三种基本的[不变性](@entry_id:140168)：

1.  **[平移不变性](@entry_id:195885)（Translational Invariance）**：将整个分子体系在空间中进行任意平移，其能量保持不变。即对于任意矢量 $\mathbf{v} \in \mathbb{R}^3$，有 $E(\mathbf{R}_1 + \mathbf{v}, \dots, \mathbf{R}_N + \mathbf{v}) = E(\mathbf{R}_1, \dots, \mathbf{R}_N)$。

2.  **[旋转不变性](@entry_id:137644)（Rotational Invariance）**：将整个分子体系在空间中进行任意刚性旋转，其能量保持不变。即对于任意[旋转矩阵](@entry_id:140302) $\mathcal{O} \in SO(3)$，有 $E(\mathcal{O}\mathbf{R}_1, \dots, \mathcal{O}\mathbf{R}_N) = E(\mathbf{R}_1, \dots, \mathbf{R}_N)$。

3.  **[置换不变性](@entry_id:753356)（Permutational Invariance）**：交换体系中任意两个相同种类（即原子序数 $Z$ 相同）的[原子核](@entry_id:167902)的标签，其能量保持不变。这源于量子力学中全同粒子的[不可区分原理](@entry_id:150314)，即使在[原子核](@entry_id:167902)被“钳定”的玻恩–奥本海默近似中，这一原理依然成立。[@problem_id:2760102]

在机器学习中，我们通常不直接将 $3N$ 维的[笛卡尔坐标](@entry_id:167698) $\mathbf{R}$ 作为模型的输入，而是通过一个**描述符（descriptor）**或**表示（representation）** $\phi(\mathbf{R})$ 来表征原子构型。上述物理[不变性](@entry_id:140168)对描述符的设计提出了严格的要求。为了使能量预测函数 $f$ 能够成为一个从描述符到能量的单值、光滑的映射，即 $\phi(\mathbf{R}) \mapsto E(\mathbf{R})$，描述符本身必须对这些对称性操作保持不变。[@problem_id:2760105]

换言之，如果两个构型 $\mathbf{R}_1$ 和 $\mathbf{R}_2$ 在物理上是等价的（例如，$\mathbf{R}_2$ 是 $\mathbf{R}_1$ 经过平移、旋转或相同原子[置换](@entry_id:136432)后得到的），那么它们的描述符也必须完全相同，即 $\phi(\mathbf{R}_1) = \phi(\mathbf{R}_2)$。只有这样，[机器学习模型](@entry_id:262335)才能“理解”这两种构型对应同一个能量值，而无需从数据中“学习”这些基本物理定律。

在[主动学习](@entry_id:157812)的背景下，这一点尤为关键。[主动学习](@entry_id:157812)依赖于模型的不确定性来指导采样。如果[不确定性估计](@entry_id:191096)（例如，高斯过程（GP）回归中使用的[核函数](@entry_id:145324)）不具备这些不变性，那么对于一个已经采样过的构型，简单地将其旋转或重排原子标签，就会在原始坐标空间中产生一个新的、看似不同的点。模型会错误地认为这个新点具有高不确定性，从而浪费宝贵的计算资源去重复采样物理上等价的构型。因此，构建一个满足平移、旋转和[置换不变性](@entry_id:753356)的描述符是开发数据高效、物理可靠的[机器学习势](@entry_id:183033)能面的第一步，也是最重要的一步。[@problem_id:2760102]

### 克服维度灾难：[局域性原理](@entry_id:753741)

直接学习一个将 $3N$ 维笛卡尔坐标映射到能量的函数 $E(\mathbf{R})$ 面临着一个巨大的挑战——**[维度灾难](@entry_id:143920)（curse of dimensionality）**。假设[势能面](@entry_id:147441)函数 $E$ 在其定义域（一个边长为 $W$ 的 $3N$ 维超立方体）上是 $L$-Lipschitz 连续的。为了保证在整个[构型空间](@entry_id:149531)中预测误差的上限不超过 $\varepsilon$，我们需要用半径为 $h \approx \varepsilon/L$ 的小球覆盖整个定义域。所需的样本点数量将与 $(\text{定义域体积}) / (\text{小球体积})$ 成正比，大致按 $(\varepsilon^{-1})^{3N}$ 的指数形式增长。对于一个包含数十个原子的体系，这是一个天文数字，使得通过均匀采样来构建精确[势能面](@entry_id:147441)变得完全不可行。[@problem_id:2760112]

幸运的是，物理学为我们提供了克服这一挑战的钥匙：**[局域性原理](@entry_id:753741)（principle of locality）**，或称为电子物质的“近视性”（nearsightedness of electronic matter）。[@problem_id:2760103] 对于绝缘体和许多其他体系，一个原子的电子结构及其对总能量的贡献主要由其临近的化学环境决定，而远处原子的影响会迅速衰减。

这一物理洞察启发了一种强大的建模策略：将总[能量分解](@entry_id:193582)为各个原子能量贡献的总和。这种**原子[能量分解](@entry_id:193582)（atomic energy decomposition）**是现代机器学习[势能面](@entry_id:147441)的核心思想之一，其形式如下：
$$
E(\mathbf{R}) = \sum_{i=1}^{N} \varepsilon^{(Z_i)} \left( \mathcal{N}_i(\mathbf{R}; r_c) \right)
$$
这里，$Z_i$ 是原子 $i$ 的原子序数，$\varepsilon^{(Z_i)}$ 是一个特定于元素种类的[神经网](@entry_id:276355)络（或其它学习模型），它将原子 $i$ 的**局域环境描述符** $\mathcal{N}_i$ 映射到一个能量贡献值。该描述符 $\mathcal{N}_i$ 只依赖于原子 $i$ 周围一个有限**[截断半径](@entry_id:136708)（cutoff radius）** $r_c$ 内的邻近原子。[@problem_id:2760129]

这种局域分解架构具有两个深远的优势：

1.  **内禀的尺寸[外延](@entry_id:161930)性（Size Extensivity）**：尺寸外延性是指对于两个互不相互作用的子体系 $\mathcal{A}$ 和 $\mathcal{B}$，总能量等于子体系能量之和，即 $E(\mathcal{A} \cup \mathcal{B}) = E(\mathcal{A}) + E(\mathcal{B})$。在局域分解模型中，如果两个子体系的原子间距都大于[截断半径](@entry_id:136708) $r_c$，那么一个体系中任何原子的局域环境都不会受到另一个体系的影响。因此，总能量的求和自然地分离为两个子体系的能量求和，模型天生就满足尺寸[外延](@entry_id:161930)性。这一性质对于模型从小的训练体系（如分子）迁移到大的凝聚相体系（如晶体或液体）至关重要。[@problem_id:2760129]

2.  **降低[有效维度](@entry_id:146824)**：学习任务从一个高维（$3N$）全局函数转变为一个低维（$d_{\mathrm{loc}}$）的局域能量函数 $\varepsilon$。局域环境描述符的维度 $d_{\mathrm{loc}}$ 通常是一个不依赖于总原子数 $N$ 的小常数。这极大地缓解了[维度灾难](@entry_id:143920)。从[最坏情况分析](@entry_id:168192)来看，为保证总能量误差小于 $\varepsilon$，每个原子能量贡献的误差需要控制在 $\varepsilon/N$ 的量级，导致样本复杂度按 $(N/\varepsilon)^{d_{\mathrm{loc}}}$ 增长，这只是关于 $N$ 的[多项式增长](@entry_id:177086)。而从更现实的统计角度看，如果各原子能量的预测误差是独立且零均值的，那么总能量的[均方根误差](@entry_id:170440)（RMSE）与单个原子能量的RMSE之间存在 $\sqrt{N}$ 的关系。这意味着，要达到总能量误差 $\varepsilon$ 的目标，每个原子能量的误差仅需控制在 $\varepsilon/\sqrt{N}$ 的量级。相比于 $1/N$ 的最坏情况，这种 $1/\sqrt{N}$ 的误差容忍度放宽，使得所需训练样本数量大幅减少，从而有力地支持了局域分解模型的数据高效性。[@problem_id:2760112]

### 构建不变性局域表示

局域分解模型的成功依赖于能够有效编码局域化学环境并满足[基本对称性](@entry_id:161256)的描述符。目前已发展出多种成熟的方案，其中最著名的是[原子中心对称函数](@entry_id:174796)（Atom-Centered Symmetry Functions, ACSF）和原子位置光滑重叠（Smooth Overlap of Atomic Positions, SOAP）。

**[原子中心对称函数](@entry_id:174796)（ACSF）** [@problem_id:2760105] 的构建思想是直接利用体系的内禀几何量（原子间距和键角），这些量本身就是平移和旋转不变的。ACSF通常包括两种类型：

-   **[径向对称](@entry_id:141658)函数**：描述中心原子 $i$ 周围的径向[分布](@entry_id:182848)，通常是关于邻近原子 $j$ 与中心原子 $i$ 之间距离 $R_{ij}$ 的高斯函数之和，并乘以一个截断函数以保证局域性。
-   **角向[对称函数](@entry_id:177113)**：描述中心原子 $i$ 周围的角度[分布](@entry_id:182848)，通常是关于邻近原子 $j,k$ 和中心原子 $i$ 构成的夹角 $\theta_{ijk}$ 的余弦函数的多项式，同样乘以截断函数。

通过对所有邻居原子对或原子三元组的贡献进行求和，ACSF可以实现对邻居原子[置换](@entry_id:136432)的不变性。最后，将不同种类原子的贡献分别输入到[神经网](@entry_id:276355)络的不同通道，整个描述符就具备了对全同原子[置换](@entry_id:136432)的[不变性](@entry_id:140168)。

**原子位置光滑重叠（SOAP）** [@problem_id:2760105] 则采用了另一种更系统的方法。它首先在中心原子周围构建一个邻近原子密度场（用[高斯函数](@entry_id:261394)表示每个邻居原子），这个密度场本身就是平移和邻居[置换](@entry_id:136432)不变的。然后，将这个密度场在[径向基函数](@entry_id:754004)和球谐函数的乘积基上展开，得到一组展开系数 $c_{nlm}$。这些系数会随体系的旋转而变化。为了获得[旋转不变性](@entry_id:137644)，SOAP通过计算这些系数的**功率谱（power spectrum）**，即对磁量子数 $m$ 进行求和，例如 $p_{nn'l} = \sum_{m=-l}^{l} c_{nlm}^* c_{n'lm}$。这个功率谱向量就是最终的[旋转不变量](@entry_id:170459)描述符。

这些现代描述符的共同特点是，它们将依赖于方向的、各向异性的局域几何信息，编码成一个高维的、但对全局旋转保持不变的[特征向量](@entry_id:151813)。这与某些错误观点相反，后者认为强制[旋转不变性](@entry_id:137644)会丢失方向信息。恰恰相反，ACSF和SOAP等方法正是为了在保持全局[旋转不变性](@entry_id:137644)的同时，精确地捕捉局域的[各向异性相互作用](@entry_id:161673)。[@problem_id:2760105] 相比之下，一些看似简单的方法，如使用未经排序的库仑矩阵或简单对齐主轴后的笛卡尔坐标，都无法完全满足所有对称性要求，因而在实践中效果不佳。[@problem_id:2760102]

### [主动学习](@entry_id:157812)循环

拥有了合适的模型架构和描述符后，我们就可以通过主动学习来高效地构建训练数据集。主动学习是一个迭代过程，旨在以最少的计算成本（即调用昂贵的[量子化学](@entry_id:140193)计算的次数）达到期望的模型精度。一个完整、严谨的[主动学习](@entry_id:157812)循环包括以下几个关键环节。[@problem_id:2760110]

#### 模型训练：能量-力联合损失函数

在每个主动学习迭代步中，我们使用当前已有的标注数据集 $\mathcal{D}_t$ 来训练[势能面](@entry_id:147441)模型。由于[势能面](@entry_id:147441)对原子坐标的负梯度就是原子受力，即 $\mathbf{F} = -\nabla_{\mathbf{R}} E$，我们可以同时利用[量子化学](@entry_id:140193)计算得到的能量和力来进行训练。这极大地增加了每个数据点提供的[信息量](@entry_id:272315)，因为一个 $N$ 原子体系的力数据提供了 $3N$ 个额外的约束。

训练的目标是最小化一个联合了能量误差和力误差的损失函数 $L(\boldsymbol{\theta})$：
$$
L(\boldsymbol{\theta}) = \lambda_E \sum_{a} \left(E_a^{\text{pred}}-E_a^{\text{ref}}\right)^2 + \lambda_F \sum_{a}\sum_{i=1}^{N_a}\left\|\mathbf{F}_{a,i}^{\text{pred}}-\mathbf{F}_{a,i}^{\text{ref}}\right\|^2
$$
其中，$a$ 遍历所有训练构型，$\boldsymbol{\theta}$ 是模型参数，$\lambda_E$ 和 $\lambda_F$ 是超参数，用于平衡能量和力在总损失中的权重。

从统计学的角度看，如果假设参考的能量和力数据分别被独立、零均值的高斯噪声（[方差](@entry_id:200758)为 $\sigma_E^2$ 和 $\sigma_F^2$）所污染，那么最小化上述损失函数等价于最大化数据的[似然](@entry_id:167119)。此时，最优的权重应与噪声[方差](@entry_id:200758)的倒数成正比，即 $\lambda_E \propto 1/\sigma_E^2$ 和 $\lambda_F \propto 1/\sigma_F^2$。这意味着权重比 $\lambda_E/\lambda_F$ 反映了我们对能量和力数据相对可靠性的[先验信念](@entry_id:264565)。[@problem_id:2760121]

值得注意的是，如果只使用力进行训练（即 $\lambda_E = 0$），[势能面](@entry_id:147441)只能被确定到一个任意的附加常数，因为力的计算只涉及能量的梯度。引入能量项（$\lambda_E > 0$）可以消除这种不确定性，将[势能面](@entry_id:147441)的[绝对值](@entry_id:147688)固定下来。[@problem_id:2760121]

#### 不确定性量化：区分模型的“已知”与“未知”

[主动学习](@entry_id:157812)的核心在于让模型判断自己“知道什么”和“不知道什么”。这通过**不确定性量化（Uncertainty Quantification, UQ）**来实现。在机器学习中，不确定性主要分为两类：[@problem_id:2760138]

-   **认知不确定性（Epistemic Uncertainty）**：源于模型本身和有限的训练数据。它反映了模型在未见过的输入区域的不确定性。这种不确定性是**可约减的**，通过增加更多相关数据可以降低。[主动学习](@entry_id:157812)主要就是为了高效地降低[认知不确定性](@entry_id:149866)。

-   **[偶然不确定性](@entry_id:154011)（Aleatoric Uncertainty）**：源于数据生成过程中的内在随机性或噪声。例如，[量子化学](@entry_id:140193)计算由于收敛不完全而引入的数值噪声。这种不确定性是**不可约减的**，即使在数据无限多的情况下也依然存在。

为了量化[认知不确定性](@entry_id:149866)，常用的方法有两种：[@problem_id:2760107]
1.  **贝叶斯方法**：如高斯过程（GP），它直接为每个预测提供一个完整的[后验概率](@entry_id:153467)[分布](@entry_id:182848)。其后验[方差](@entry_id:200758)可以作为[认知不确定性](@entry_id:149866)的直接度量。
2.  **[集成方法](@entry_id:635588)（Ensemble）**：训练一组（例如 $M$ 个）具有不同初始化或在不同数据[子集](@entry_id:261956)（通过[自举法](@entry_id:139281)/bootstrap得到）上训练的[神经网](@entry_id:276355)络。对于一个新的输入，这 $M$ 个模型会给出 $M$ 个不同的预测，这些预测值的[方差](@entry_id:200758)就可以作为[认知不确定性](@entry_id:149866)的度量。

一个更精细的模型还可以对偶然不确定性进行建模。例如，[量子化学](@entry_id:140193)计算通常会报告一个衡量[自洽场](@entry_id:136549)（SCF）收敛程度的残差 $r_i$。残差越大，能量标签的噪声可能就越大。我们可以构建一个**异[方差](@entry_id:200758)（heteroscedastic）**的似然模型，让观测噪声的[方差](@entry_id:200758)依赖于 $r_i$，例如 $\sigma_i^2 = \sigma_0^2 + \alpha r_i^2$。使用诸如[学生t分布](@entry_id:267063)这样的[重尾分布](@entry_id:142737)作为似然，还可以更好地处理偶尔出现的、由收敛失败导致的大误差离群点。[@problem_id:2760138]

#### 数据获取：化学空间的智能探索

有了不确定性的度量，我们就可以设计**[采集函数](@entry_id:168889)（acquisition function）**来从一个大的未标注构型池（例如，通过[分子动力学模拟](@entry_id:160737)生成）中挑选最有价值的候选点进行标注。一个好的采集策略通常需要平衡**探索（exploration）**和**利用（exploitation）**。

最简单的[采集函数](@entry_id:168889)就是选择[认知不确定性](@entry_id:149866)最高的点。然而，当一次需要选择一个批次（batch）的数据点时，仅仅贪婪地选择不确定性最高的几个点可能会导致所选的点在构型空间中非常相似，造成信息冗余。因此，一个更优的策略是在不确定性的基础上引入**多样性（diversity）**的考量。例如，可以采用一种混合策略，结合不确定性得分和点与已选点之间的距离（如在描述符空间中的最远点采样）。[@problem_id:2760110]

此外，对于可变尺寸的体系，一个简单的总不确定性求和[采集函数](@entry_id:168889)会天然地偏向于选择更大的体系。为了消除这种尺寸偏见，通常会使用一个尺寸无关的准则，例如平均每个原子的不确定性或最大的单个原子不确定性。[@problem_id:2760129]

选出的候选构型在经过昂贵的[量子化学](@entry_id:140193)计算获得能量和力标签后，需要经过严格的**去重（deduplication）**。这一步必须是物理上严谨的，即通过对齐和平移并考虑全同原子[置换](@entry_id:136432)后，计算构型间的[均方根偏差](@entry_id:170440)（RMSD），只有当RMSD大于某个阈值时，才认为是一个新的、有价值的数据点。[@problemid:2760110]

### 确保模型可靠性：校准与[停止准则](@entry_id:136282)

一个能够报告自身不确定性的模型，只有当其报告的不确定性真实反映其预测误差时，才是可靠的。这一性质被称为**校准（calibration）**。在主动学习中，由于训练数据的[分布](@entry_id:182848)（偏向高不确定性区域）与模型最终应用的[分布](@entry_id:182848)（如特定温度下的玻尔兹曼分布，偏向低能区域）存在**[协变量偏移](@entry_id:636196)（covariate shift）**，模型的校准性评估尤为重要。一个严谨的校准实验必须在一个独立的、从目标分布中抽样的[测试集](@entry_id:637546)上进行。通过比较不同[置信水平](@entry_id:182309)下的[预测区间](@entry_id:635786)（nominal coverage）与真实值落入该区间的频率（empirical coverage），可以评估模型的校准程度。[@problem_id:2760107]

最后，一个实际问题是：[主动学习](@entry_id:157812)过程何时可以停止？我们需要一个有原则的**[停止准则](@entry_id:136282)（stopping criterion）**，而不仅仅是观察验证误差是否收敛。一个统计上稳健的准则是，当我们有足够的信心（例如，置信度 $1-\delta$）断定模型的真实[泛化误差](@entry_id:637724)已经低于我们预设的容忍度 $\varepsilon$ 时，就可以停止。

要实现这一点，需要一种能够可靠估计[泛化误差](@entry_id:637724)及其[置信区间](@entry_id:142297)的方法。考虑到[主动学习](@entry_id:157812)数据中可能存在的相关性（如[分子动力学轨迹](@entry_id:752118)中相邻的帧）以及超参数选择对误差评估的影响，**嵌套的、分组的K折[交叉验证](@entry_id:164650)（nested, grouped K-fold cross-validation）**是当前最可靠的方案之一。[@problem_id:2760148]

-   **分组（Grouping）**：将相关的数据点（如一个短轨迹）作为一个整体放入同一个折中，以保证不同折之间的近似独立性，从而得到更无偏的误差估计。
-   **嵌套（Nesting）**：在外层[交叉验证](@entry_id:164650)的每一次划分中，使用内层交叉验证在训练集部分上完成完整的[超参数调优](@entry_id:143653)，然后用得到的模型在外层留出的验证折上进行评估。这可以公正地评估整个模型选择流程的性能。

通过这种方式，我们可以得到 $K$ 个近似独立的误差估计值 $e_k$。基于它们的均值和标准差，可以构建真实[泛化误差](@entry_id:637724)的单侧[置信上界](@entry_id:178122) $U$（通常使用学生t分布）。当 $U \le \varepsilon$ 时，我们就有统计上的把握认为模型已经达标，可以停止主动学习循环。