{"hands_on_practices": [{"introduction": "Analyzing divide-and-conquer algorithms often involves solving recurrence relations. While many textbook examples feature subproblems of equal size, real-world scenarios can be more complex. This practice problem [@problem_id:3279209] challenges you to analyze a recurrence where the problem is split into two subproblems of unequal fractional sizes. Solving it requires a return to first principles, such as the substitution method, to establish a tight asymptotic bound, thereby sharpening your foundational analysis skills.", "problem": "An algorithm on an input of size $n$ proceeds by the divide-and-conquer principle: it splits the problem into two subproblems of sizes $\\lfloor n/2 \\rfloor$ and $\\lfloor n/3 \\rfloor$, recursively solves each subproblem, and then performs a combination step whose cost is linear in the size of the current problem. Let $T(n)$ denote the worst-case number of primitive operations for an input of size $n$, and assume a constant-time base case $T(1)=d$ for some fixed $d0$. The combination step incurs $c n$ operations for some fixed $c0$. \n\nStarting from the core definitions of divide-and-conquer recurrences and the meaning of Big-Theta ($\\Theta$) notation as an asymptotic growth classification up to constant factors, first formulate a mathematically precise recurrence for $T(n)$ that captures this algorithm. Then, by analyzing this recurrence from first principles and well-tested results in recurrence analysis, determine the dominant-term growth function $f(n)$ such that $T(n)$ is bounded above and below by constant multiples of $f(n)$ for sufficiently large $n$. \n\nExpress your final answer as a single closed-form analytic expression $f(n)$ in terms of $n$ only, without using any asymptotic classification symbols (e.g., do not write $\\Theta(\\cdot)$, $\\mathcal{O}(\\cdot)$, or $\\Omega(\\cdot)$). No rounding is required.", "solution": "The problem asks for the formulation and asymptotic analysis of a divide-and-conquer recurrence relation.\n\nFirst, we must formulate the recurrence for the worst-case number of primitive operations, $T(n)$, for an input of size $n$. According to the problem statement, the algorithm divides the problem into two subproblems of sizes $\\lfloor n/2 \\rfloor$ and $\\lfloor n/3 \\rfloor$. The time to solve these subproblems is $T(\\lfloor n/2 \\rfloor)$ and $T(\\lfloor n/3 \\rfloor)$, respectively. The combination step requires a number of operations linear in the size of the current problem, which is given as $cn$ for some constant $c  0$. The base case for the recursion is specified as $T(1) = d$ for some constant $d  0$.\n\nCombining these components, the recurrence relation for $T(n)$ is:\n$$\nT(n) = \\begin{cases}\n    d  \\text{if } n=1 \\\\\n    T(\\lfloor n/2 \\rfloor) + T(\\lfloor n/3 \\rfloor) + cn  \\text{if } n1\n\\end{cases}\n$$\nWe are asked to find the dominant-term growth function $f(n)$ such that $T(n) = \\Theta(f(n))$, which means we must find functions $f(n)$ and positive constants $k_1, k_2, n_0$ such that $k_2 f(n) \\le T(n) \\le k_1 f(n)$ for all $n \\ge n_0$.\n\nWe can form a hypothesis about the growth of $T(n)$ by examining the work done at each level of the recursion tree. At the root (level $0$), the work is $cn$. At level $1$, the work is $c(\\lfloor n/2 \\rfloor) + c(\\lfloor n/3 \\rfloor) \\approx c(n/2 + n/3) = cn(5/6)$. At level $2$, the work is approximately $cn(1/2+1/3)^2 = cn(5/6)^2$. The total work is the sum of the work at all levels. This sum forms a geometric series with a ratio of $5/6$, which is less than $1$. The sum is therefore dominated by the first term, $cn$. This suggests that $T(n)$ grows linearly with $n$, so we hypothesize that $T(n) = \\Theta(n)$. We will now prove this hypothesis formally using the substitution method.\n\nTo prove $T(n) = \\Theta(n)$, we must prove both $T(n) = O(n)$ and $T(n) = \\Omega(n)$.\n\n**1. Upper Bound: $T(n) = O(n)$**\n\nWe must show that there exists a constant $k_1  0$ and an integer $n_0$ such that for all $n \\ge n_0$, $T(n) \\le k_1 n$. We use proof by induction.\nInductive Hypothesis (I.H.): Assume that for all $m  n$, $T(m) \\le k_1 m$.\n\nFor $n  1$, we have:\n$$\nT(n) = T(\\lfloor n/2 \\rfloor) + T(\\lfloor n/3 \\rfloor) + cn\n$$\nApplying the inductive hypothesis to $T(\\lfloor n/2 \\rfloor)$ and $T(\\lfloor n/3 \\rfloor)$ (since $\\lfloor n/2 \\rfloor  n$ and $\\lfloor n/3 \\rfloor  n$ for $n1$):\n$$\nT(n) \\le k_1 \\lfloor n/2 \\rfloor + k_1 \\lfloor n/3 \\rfloor + cn\n$$\nUsing the property $\\lfloor x \\rfloor \\le x$:\n$$\nT(n) \\le k_1 (n/2) + k_1 (n/3) + cn = k_1 n \\left(\\frac{1}{2} + \\frac{1}{3}\\right) + cn = k_1 n \\left(\\frac{5}{6}\\right) + cn\n$$\nTo complete the proof, we need to show that this expression is less than or equal to $k_1 n$ for a suitable choice of $k_1$:\n$$\nk_1 n \\left(\\frac{5}{6}\\right) + cn \\le k_1 n\n$$\n$$\ncn \\le k_1 n - k_1 n \\left(\\frac{5}{6}\\right)\n$$\n$$\ncn \\le k_1 n \\left(1 - \\frac{5}{6}\\right)\n$$\n$$\ncn \\le k_1 n \\left(\\frac{1}{6}\\right)\n$$\nSince $n0$, we can divide by $n$:\n$$\nc \\le \\frac{k_1}{6} \\implies k_1 \\ge 6c\n$$\nSince $c  0$, we can choose a constant $k_1$, for example $k_1 = 6c$. For this choice to be valid, the base case must also be satisfied. We need $T(1) \\le k_1(1)$, which means $d \\le 6c$. If this condition does not hold (i.e., if $d  6c$), we can choose a larger $k_1$, for example $k_1 = \\max(6c, d)$, which would satisfy both the inductive step and the base case for $n=1$. Alternatively, we select $n_0$ large enough so that the effects of the base case are overcome. By choosing $k_1$ appropriately (e.g., $k_1=7c$ and handling base cases for small $n$ by adjusting $n_0$), the inequality holds. Thus, $T(n) = O(n)$.\n\n**2. Lower Bound: $T(n) = \\Omega(n)$**\n\nWe must show that there exists a constant $k_2  0$ and an integer $n_0$ such that for all $n \\ge n_0$, $T(n) \\ge k_2 n$.\nInductive Hypothesis (I.H.): Assume that for all $m  n$, $T(m) \\ge k_2 m$.\n\nFor $n  1$:\n$$\nT(n) = T(\\lfloor n/2 \\rfloor) + T(\\lfloor n/3 \\rfloor) + cn\n$$\nApplying the inductive hypothesis:\n$$\nT(n) \\ge k_2 \\lfloor n/2 \\rfloor + k_2 \\lfloor n/3 \\rfloor + cn\n$$\nThis time, to establish a lower bound, we use the property $\\lfloor x \\rfloor  x - 1$:\n$$\nT(n)  k_2 (n/2 - 1) + k_2 (n/3 - 1) + cn = k_2 n \\left(\\frac{5}{6}\\right) - 2k_2 + cn\n$$\nWe want to show this is greater than or equal to $k_2 n$:\n$$\nk_2 n \\left(\\frac{5}{6}\\right) - 2k_2 + cn \\ge k_2 n\n$$\n$$\ncn \\ge k_2 n - k_2 n \\left(\\frac{5}{6}\\right) + 2k_2\n$$\n$$\ncn \\ge k_2 n \\left(\\frac{1}{6}\\right) + 2k_2\n$$\nDividing by $n$ (for $n0$):\n$$\nc \\ge \\frac{k_2}{6} + \\frac{2k_2}{n}\n$$\nThis inequality must hold for all $n \\ge n_0$. As $n$ increases, the term $2k_2/n$ decreases. So if the inequality holds for $n_0$, it holds for all $n  n_0$. The condition approaches $c \\ge k_2/6$ as $n \\to \\infty$. This implies we must choose $k_2$ such that $k_2 \\le 6c$.\nLet's choose $k_2 = c$. Since $c0$, $k_20$. The condition becomes:\n$$\nc \\ge \\frac{c}{6} + \\frac{2c}{n} \\implies 1 \\ge \\frac{1}{6} + \\frac{2}{n} \\implies \\frac{5}{6} \\ge \\frac{2}{n} \\implies 5n \\ge 12 \\implies n \\ge \\frac{12}{5} = 2.4\n$$\nSo, the inductive step holds for all $n \\ge 3$. We must check the base cases for $n=1, 2$.\nFor $n=1$, we need $T(1) = d \\ge k_2(1) = c$. If $d  c$, we can choose a smaller $k_2$ (e.g., $k_2 = \\min(c, d)$ as long as $d0$) or simply note that the asymptotic bound holds for $n \\ge n_0$ where $n_0$ is large enough. For simplicity, and since asymptotic analysis concerns large $n$, we can select $n_0 \\ge 3$. A simple argument is that $T(n) \\ge cn$ for all $n \\ge 1$, which immediately implies $T(n) = \\Omega(n)$ with $k_2=c$. This is because $T(n)$ is a sum of non-negative terms $T(\\lfloor n/2 \\rfloor)$, $T(\\lfloor n/3 \\rfloor)$, and a positive term $cn$.\n\nSince we have shown that $T(n) = O(n)$ and $T(n) = \\Omega(n)$, we can conclude that $T(n) = \\Theta(n)$.\nThe problem asks for the dominant-term growth function $f(n)$ that describes this asymptotic behavior. This function is $f(n) = n$.", "answer": "$$\n\\boxed{n}\n$$", "id": "3279209"}, {"introduction": "The Master Theorem provides a powerful shortcut for solving many common recurrence relations, but it doesn't cover all cases. This exercise [@problem_id:3279114] presents a recurrence whose non-recursive work term, $f(n) = n / \\ln n$, falls into a well-known gap in the theorem. To find the solution, you must employ the recursion-tree method and carefully sum the work done at each level, offering a deeper understanding of asymptotic analysis beyond rote formula application.", "problem": "Consider a divide-and-conquer (DC) procedure whose running time is captured by the recurrence\n$$T(n)=2\\,T\\!\\left(\\frac{n}{2}\\right)+\\frac{n}{\\ln n},$$\nfor all integer inputs $n \\geq 2$ that are powers of $2$, with the base case $T(1)=\\Theta(1)$. Here $\\ln$ denotes the natural logarithm. In standard analyses, the Master Theorem (MT) does not directly handle the case where the non-recursive work term is $f(n)=n/\\ln n$.\n\nStarting from the core definitions of asymptotic growth and the structure of DC recurrences, and using only well-tested analysis tools (such as rigorous recursion-tree summation and integral approximation arguments), derive the tight asymptotic order of growth of $T(n)$ as $n \\to \\infty$. Express your final answer as a single standard complexity-class expression in terms of $n$ and elementary functions. No numerical rounding is required.", "solution": "We are tasked with finding the tight asymptotic order of growth for the recurrence relation\n$$T(n) = 2 T\\left(\\frac{n}{2}\\right) + \\frac{n}{\\ln n}$$\nwith the base case $T(1) = \\Theta(1)$, for values of $n$ that are integer powers of $2$, i.e., $n=2^k$ for some integer $k \\ge 0$.\n\nWe will use the recursion-tree method to solve this recurrence. Let $n=2^k$, which implies $k = \\log_2(n)$. The depth of the recursion tree is $k$.\n\nAt each level $i$ of the recursion tree, where the root is at level $i=0$, there are $2^i$ subproblems. The size of each subproblem at level $i$ is $n_i = n/2^i$. The work done at each node at level $i$ (excluding the recursive calls) is $f(n_i) = \\frac{n_i}{\\ln n_i}$.\n\nThe total work done at level $i$ is the number of nodes times the work per node:\n$$\\text{Work at level } i = 2^i \\times f\\left(\\frac{n}{2^i}\\right) = 2^i \\times \\frac{n/2^i}{\\ln(n/2^i)} = \\frac{n}{\\ln(n) - i \\ln(2)}$$\nThe total running time $T(n)$ is the sum of the work done at all levels of the tree, from the root (level $0$) to the level just before the leaves (level $k-1$), plus the work done at the leaves (level $k$).\n\nThe number of leaves is $2^k = n$. The size of the problem at each leaf is $n/2^k = 1$. The cost at each leaf is $T(1) = \\Theta(1)$. Thus, the total cost at the leaves is $n \\times T(1) = \\Theta(n)$.\n\nThe total non-recursive work is the sum of the work at levels $i=0$ to $k-1$:\n$$\\sum_{i=0}^{k-1} \\frac{n}{\\ln(n) - i \\ln(2)}$$\nTo simplify this sum, we substitute $\\ln(n) = \\ln(2^k) = k \\ln(2)$:\n$$\\text{Sum} = \\sum_{i=0}^{k-1} \\frac{n}{k \\ln(2) - i \\ln(2)} = \\frac{n}{\\ln(2)} \\sum_{i=0}^{k-1} \\frac{1}{k-i}$$\nLet us perform a change of index in the summation. Let $j = k-i$. As $i$ goes from $0$ to $k-1$, $j$ goes from $k$ down to $1$. The sum becomes:\n$$\\text{Sum} = \\frac{n}{\\ln(2)} \\sum_{j=1}^{k} \\frac{1}{j}$$\nThe summation $\\sum_{j=1}^{k} \\frac{1}{j}$ is the $k$-th harmonic number, denoted by $H_k$. It is a well-established result that for large $k$, the harmonic series has the following asymptotic behavior:\n$$H_k = \\ln(k) + \\gamma + O\\left(\\frac{1}{k}\\right)$$\nwhere $\\gamma$ is the Euler-Mascheroni constant. Therefore, $H_k = \\Theta(\\ln k)$.\n\nSubstituting this back into the expression for the sum of non-recursive work:\n$$\\text{Sum} = \\frac{n}{\\ln(2)} H_k = \\frac{n}{\\ln(2)} \\Theta(\\ln k) = \\Theta(n \\ln k)$$\nThe total running time $T(n)$ is the sum of the leaf costs and the non-recursive work:\n$$T(n) = \\Theta(n) + \\Theta(n \\ln k)$$\nAs $n \\to \\infty$, $k = \\log_2(n) \\to \\infty$. Since $\\ln(k)$ is a growing function of $k$, the term $\\Theta(n \\ln k)$ dominates the term $\\Theta(n)$.\nThus, the asymptotic behavior of $T(n)$ is given by:\n$$T(n) = \\Theta(n \\ln k)$$\nFinally, we must express this result in terms of $n$. We substitute $k = \\log_2(n)$:\n$$T(n) = \\Theta(n \\ln(\\log_2 n))$$\nWe can simplify the logarithmic term. Using the change of base formula for logarithms, $\\log_2(n) = \\frac{\\ln(n)}{\\ln(2)}$:\n$$\\ln(\\log_2 n) = \\ln\\left(\\frac{\\ln n}{\\ln 2}\\right) = \\ln(\\ln n) - \\ln(\\ln 2)$$\nAs $n \\to \\infty$, $\\ln(\\ln n) \\to \\infty$, while $\\ln(\\ln 2)$ is a negative constant. Therefore, the term $\\ln(\\ln n)$ dominates. This means that $\\ln(\\log_2 n) = \\Theta(\\ln(\\ln n))$.\n\nSubstituting this final simplification into our expression for $T(n)$:\n$$T(n) = \\Theta(n \\cdot \\Theta(\\ln(\\ln n))) = \\Theta(n \\ln(\\ln n))$$\nThis is the tight asymptotic order of growth for the given recurrence.", "answer": "$$\\boxed{\\Theta(n \\ln(\\ln n))}$$", "id": "3279114"}, {"introduction": "Beyond analyzing the efficiency of a single algorithm, a deeper question in computer science is determining the inherent complexity of a problem itself. This practice problem [@problem_id:3279201] guides you through deriving a tight lower bound on the number of comparisons needed to find the second-smallest element in an array. By using a tournament analogy and an information-theoretic argument, you will prove the absolute minimum number of operations required, providing fundamental insights into the limits of computation.", "problem": "Consider an algorithm that is allowed to access an array of $n$ elements only through pairwise comparisons, each query asking whether one element is less than another. Assume the elements are all distinct and come from a strict total order, and the algorithm is adaptive: it may choose which pair to compare next based on the outcomes of prior comparisons. In this comparison-only setting, we measure query complexity as the worst-case number of pairwise comparisons needed to guarantee correctness on all inputs.\n\nStarting from the fundamental basis of the comparison decision tree (CDT), where each comparison yields one bit of information and an algorithm corresponds to a binary decision tree whose leaves represent outcomes consistent with correct identification, and the well-tested facts that a binary tree of height $h$ has at most $2^{h}$ leaves and that selecting the minimum among $k$ elements by comparisons alone requires at least $k-1$ comparisons, derive the tight worst-case minimum number of pairwise comparisons required to identify the second-smallest element of the array. Your answer must be a single closed-form analytic expression in terms of $n$. No rounding is required.", "solution": "The derivation proceeds in two parts: first, establishing an upper bound by constructing an algorithm, and second, proving a matching lower bound that any comparison-based algorithm must respect.\n\n#### Upper Bound\n\nWe can find the second-smallest element, which we denote as $S_2$, by first identifying the smallest element, $S_1$.\n\n1.  **Finding the Minimum ($S_1$)**: We can model the process of finding the minimum element as a single-elimination tournament. The $n$ elements are the participants. We pair them up and compare them. The winners advance to the next round. This continues until a single overall winner remains. This winner is the minimum element, $S_1$. In such a tournament, every element except the winner must lose exactly one match. Since each comparison produces one loser, exactly $n-1$ comparisons are performed to determine $S_1$.\n\n2.  **Identifying Candidates for the Second-Minimum ($S_2$)**: The second-smallest element, $S_2$, must have lost a comparison to some other element. If $S_2$ lost a comparison to an element $x$, it must be that $x = S_1$. If $x$ were any other element, we would have the relation $S_1  x  S_2$, which would mean $S_2$ is at least the third-smallest element, a contradiction. Therefore, $S_2$ must be one of the elements that was directly compared with $S_1$ and lost.\n\n3.  **Finding the Second-Minimum ($S_2$)**: Let $K$ be the set of elements that lost directly to $S_1$ during the tournament. The second-smallest element $S_2$ is the minimum element within the set $K$.\n    The number of elements in $K$, which we denote as $|K|$, is the number of opponents $S_1$ faced and defeated on its path to becoming the winner of the tournament. In a balanced tournament structure, the winner participates in $\\lceil \\log_2(n) \\rceil$ comparisons. Thus, in the worst case (for the size of $K$), $|K| = \\lceil \\log_2(n) \\rceil$.\n    To find the minimum element of the set $K$, we require $|K|-1$ additional comparisons.\n\n4.  **Total Comparisons (Upper Bound)**: The total number of comparisons for this algorithm is the sum of comparisons from step 1 and step 3:\n    $$ C_{\\text{upper}} = (n-1) + (|K|-1) $$\n    In the worst case for this algorithm, $|K| = \\lceil \\log_2(n) \\rceil$.\n    $$ C_{\\text{upper}} = (n-1) + (\\lceil \\log_2(n) \\rceil - 1) = n + \\lceil \\log_2(n) \\rceil - 2 $$\n    This establishes an upper bound on the number of comparisons required.\n\n#### Lower Bound\n\nWe now derive a lower bound on the worst-case complexity for *any* comparison-based algorithm that finds the second-smallest element.\n\n1.  **Information Required**: Any correct algorithm must acquire sufficient information to certify its output. To certify that an element $y$ is $S_2$, the algorithm must have established that:\n    a. Exactly one element, $S_1$, is smaller than $y$.\n    b. All other $n-2$ elements are larger than $y$.\n\n2.  **Counting Necessary Comparison Outcomes**:\n    - **Finding $S_1$**: To establish that an element $S_1$ is the minimum, every other element $x$ must be shown to be larger than at least one other element (i.e., must lose at least one comparison). Each comparison has only one loser, so to ensure $n-1$ distinct elements have lost, at least $n-1$ comparisons are necessary.\n    - **Finding $S_2$**: As established previously, $S_2$ must be in the set $K$ of elements that lost directly to $S_1$. Let $|K| = k$. To identify $S_2$ as the minimum of the set $K$, the algorithm must establish that $S_2$ is smaller than the other $k-1$ elements in $K$. Proving this requires at least $k-1$ comparisons among the elements of $K$.\n\n3.  **Combining the Requirements**: Let's account for the total number of comparisons, $C$.\n    - A comparison $(x,y)$ where neither $x$ nor $y$ is $S_1$ establishes a loss for one element. To ensure the $n-1-k$ elements not in $\\{S_1\\} \\cup K$ are identified as non-minimal, at least $n-1-k$ such comparisons must occur where they lose.\n    - The $k$ elements in $K$ must lose to $S_1$. This requires exactly $k$ comparisons of the form $(S_1, y)$ where $y \\in K$.\n    - The $k-1$ elements in $K \\setminus \\{S_2\\}$ must be shown to be larger than $S_2$. This requires at least $k-1$ comparisons among the elements of $K$.\n\n    The information gained from these three types of necessary events is distinct. A comparison of type $(S_1, y)$ confirms $y \\in K$. A comparison of type $(y_i, y_j)$ where $y_i, y_j \\in K$ establishes ordering within $K$, which is necessary to find its minimum. This ordering information is not provided by the comparisons involving $S_1$. Therefore, we can sum the minimum number of comparisons required for these distinct informational needs to get a lower bound on the total number of comparisons.\n    $$ C_{\\text{lower}} \\ge (n-1-k) + k + (k-1) = n + k - 2 $$\n    This lower bound depends on $k$, the number of elements that are direct losers to the eventual minimum $S_1$.\n\n4.  **Lower Bounding $k$**: The expression $n+k-2$ provides a lower bound for a given $k$. To obtain a universal lower bound on complexity, we must determine the smallest possible value of $k$ that an adversary can force, regardless of the algorithm's strategy.\n    The number of elements $k$ corresponds to the number of comparisons won by $S_1$. After $k$ wins, an element can be certified as being smaller than at most $2^k-1$ other elements (this occurs if the element wins a perfectly balanced tournament of size $2^k$). To be certified as the minimum of $n$ elements, an element must be shown to be smaller than all $n-1$ other elements. Abstractly, the set of $n-1$ losers must be partitioned among the $k$ direct losers to $S_1$. Each of these $k$ direct losers is the \"winner\" of a subtree of defeated elements. A single element can be the winner of a group of at most $2^0=1$ element (itself) without any wins. With one win, it can be the winner of $2^1=2$ elements. With $c$ wins, it can be the winner of at most $2^c$ elements. The winner $S_1$ must have \"conquered\" a hierarchy containing all $n$ elements. The number of direct opponents, $k$, is the number of sub-tournaments whose winners were defeated by $S_1$. To cover $n$ elements, we must have $2^k \\ge n$.\n    This implies $k \\ge \\log_2(n)$. Since $k$ must be an integer, we have:\n    $$ k \\ge \\lceil \\log_2(n) \\rceil $$\n    Any algorithm can be forced by an adversary to make its eventual winner play at least $\\lceil \\log_2(n) \\rceil$ matches.\n\n5.  **Final Lower Bound**: Substituting the minimum possible value of $k$ into our lower bound for $C$:\n    $$ C_{\\text{lower}} \\ge n + \\lceil \\log_2(n) \\rceil - 2 $$\n\n#### Conclusion\n\nThe upper bound on the number of comparisons is $n + \\lceil \\log_2(n) \\rceil - 2$, and the worst-case lower bound for any algorithm is also $n + \\lceil \\log_2(n) \\rceil - 2$. Since the upper and lower bounds match, this is the tight worst-case complexity.", "answer": "$$\n\\boxed{n + \\lceil \\log_2(n) \\rceil - 2}\n$$", "id": "3279201"}]}