{"hands_on_practices": [{"introduction": "Comparing the growth rates of functions is a cornerstone of algorithm analysis. This first practice challenges you to rank a diverse set of functions, including factorials and exponential forms, that do not lend themselves to simple eyeball comparisons [@problem_id:3222399]. To solve this, you will employ a powerful technique: analyzing the growth of the functions' logarithms, a method that transforms complex multiplicative structures into more manageable additive ones, sharpened by the use of Stirling's approximation for factorials.", "problem": "An algorithm design team is comparing four asymptotic running-time candidates to understand their relative growth rates as the input size $n$ tends to infinity. The four functions are defined by\n$f_1(n) = n!$, $f_2(n) = (\\log n)!$, $f_3(n) = n^n$, and $f_4(n) = n^{\\log n}$, where $\\log n$ denotes the natural logarithm. Using the core definitions of asymptotic comparison from the theory of algorithms (for example, the notions embodied in little-oh), and without relying on any shortcut ranking, determine the strict total order of these four functions from the slowest-growing to the fastest-growing as $n \\to \\infty$. Encode your final ranking as a row vector of indices $(i_1, i_2, i_3, i_4)$, where each $i_k \\in \\{1,2,3,4\\}$ identifies the function $f_{i_k}$ and the sequence is in increasing order of growth rate. Express your final answer as a single row matrix using the $\\operatorname{pmatrix}$ environment. No rounding is required.", "solution": "The problem requires determining the strict total order of four functions based on their asymptotic growth rates as the input size $n$ tends to infinity. The functions are given as $f_1(n) = n!$, $f_2(n) = (\\log n)!$, $f_3(n) = n^n$, and $f_4(n) = n^{\\log n}$. The notation $\\log n$ refers to the natural logarithm.\n\nTo establish the strict ordering, we must arrange the functions $f_{i_k}$ such that $f_{i_1}(n) = o(f_{i_2}(n))$, $f_{i_2}(n) = o(f_{i_3}(n))$, and $f_{i_3}(n) = o(f_{i_4}(n))$. The little-oh notation, $f(n) = o(g(n))$, signifies that $g(n)$ grows strictly faster than $f(n)$, which is formally defined by the limit $\\lim_{n \\to \\infty} \\frac{f(n)}{g(n)} = 0$.\n\nWe will perform a series of pairwise comparisons to establish this order. A powerful method for comparing functions is to analyze the asymptotic behavior of their logarithms. Since the logarithm is a monotonically increasing function, the growth order of the functions is preserved in their logarithms. Specifically, if $\\lim_{n \\to \\infty} (\\log g(n) - \\log f(n)) = \\infty$, it follows that $\\lim_{n \\to \\infty} \\log\\left(\\frac{g(n)}{f(n)}\\right) = \\infty$, which implies $\\lim_{n \\to \\infty} \\frac{g(n)}{f(n)} = \\infty$. This is equivalent to $\\lim_{n \\to \\infty} \\frac{f(n)}{g(n)} = 0$, thus proving $f(n) = o(g(n))$.\n\nThe logarithms of the four functions are:\n$\\log(f_1(n)) = \\log(n!)$\n$\\log(f_2(n)) = \\log((\\log n)!)$\n$\\log(f_3(n)) = \\log(n^n) = n \\log n$\n$\\log(f_4(n)) = \\log(n^{\\log n}) = (\\log n) \\log n = (\\log n)^2$\n\nFor analyzing the factorial terms, we use Stirling's approximation for the logarithm of a factorial, which states that for large $k$, $\\log(k!) = k \\log k - k + O(\\log k)$. This provides the asymptotic behavior of $\\log(f_1(n))$ and $\\log(f_2(n))$.\n\n**Comparison 1: $f_2(n)$ versus $f_4(n)$**\n\nWe compare $f_2(n) = (\\log n)!$ and $f_4(n) = n^{\\log n}$. Let us analyze their logarithms.\n$\\log(f_4(n)) = (\\log n)^2$.\nFor $\\log(f_2(n))$, we apply Stirling's approximation with $k = \\log n$. As $n \\to \\infty$, $k \\to \\infty$.\n$\\log(f_2(n)) = \\log((\\log n)!) = (\\log n)\\log(\\log n) - \\log n + O(\\log(\\log n))$.\nNow, we examine the difference of the logarithms as $n \\to \\infty$:\n$$ \\log(f_4(n)) - \\log(f_2(n)) = (\\log n)^2 - \\left((\\log n)\\log(\\log n) - \\log n + O(\\log(\\log n))\\right) $$\n$$ = (\\log n)^2 - (\\log n)\\log(\\log n) + \\log n - O(\\log(\\log n)) $$\nWe can factor out the dominant term of this expression, which is $(\\log n)^2$, as we are interested in the limit. Alternatively, we can see that $(\\log n)^2$ grows faster than $(\\log n)\\log(\\log n)$. Let's factor out $\\log n$:\n$$ \\log n \\left(\\log n - \\log(\\log n) + 1 - O\\left(\\frac{\\log(\\log n)}{\\log n}\\right)\\right) $$\nAs $n \\to \\infty$, the term $\\log n - \\log(\\log n)$ tends to infinity. The trailing terms are either constant or tend to $0$. Thus, the entire expression tends to infinity.\n$$ \\lim_{n \\to \\infty} (\\log(f_4(n)) - \\log(f_2(n))) = \\infty $$\nThis implies that $f_2(n) = o(f_4(n))$.\n\n**Comparison 2: $f_4(n)$ versus $f_1(n)$**\n\nWe compare $f_4(n) = n^{\\log n}$ and $f_1(n) = n!$. Again, we examine their logarithms.\n$\\log(f_4(n)) = (\\log n)^2$.\nFor $\\log(f_1(n))$, we use Stirling's approximation with $k = n$:\n$\\log(f_1(n)) = \\log(n!) = n \\log n - n + O(\\log n)$.\nWe examine the difference of the logarithms as $n \\to \\infty$:\n$$ \\log(f_1(n)) - \\log(f_4(n)) = (n \\log n - n + O(\\log n)) - (\\log n)^2 $$\n$$ = n \\log n - n - (\\log n)^2 + O(\\log n) $$\nAs $n \\to \\infty$, the term $n \\log n$ grows faster than all other terms in the expression ($n$, $(\\log n)^2$, and $O(\\log n)$). Therefore, the limit of the difference is determined by the dominant term $n \\log n$.\n$$ \\lim_{n \\to \\infty} (\\log(f_1(n)) - \\log(f_4(n))) = \\infty $$\nThis implies that $f_4(n) = o(f_1(n))$.\n\n**Comparison 3: $f_1(n)$ versus $f_3(n)$**\n\nWe compare $f_1(n) = n!$ and $f_3(n) = n^n$. For this pair, it is straightforward to compute the limit of their ratio directly.\n$$ \\lim_{n \\to \\infty} \\frac{f_1(n)}{f_3(n)} = \\lim_{n \\to \\infty} \\frac{n!}{n^n} = \\lim_{n \\to \\infty} \\frac{1 \\cdot 2 \\cdot 3 \\cdots n}{n \\cdot n \\cdot n \\cdots n} $$\nWe can write the ratio as a product:\n$$ \\frac{n!}{n^n} = \\left(\\frac{1}{n}\\right) \\left(\\frac{2}{n}\\right) \\left(\\frac{3}{n}\\right) \\cdots \\left(\\frac{n}{n}\\right) $$\nFor any $n \\ge 2$, we can establish an upper bound for this product. Each term $\\frac{k}{n} \\le 1$ for $k=1, \\dots, n$. The first term is $\\frac{1}{n}$.\n$$ 0 < \\frac{n!}{n^n} = \\frac{1}{n} \\cdot \\left(\\frac{2}{n} \\cdots \\frac{n}{n}\\right) \\le \\frac{1}{n} \\cdot (1 \\cdots 1) = \\frac{1}{n} $$\nSince we have $0 < \\frac{n!}{n^n} \\le \\frac{1}{n}$ and $\\lim_{n \\to \\infty} \\frac{1}{n} = 0$, the Squeeze Theorem implies that:\n$$ \\lim_{n \\to \\infty} \\frac{n!}{n^n} = 0 $$\nThis proves that $f_1(n) = o(f_3(n))$.\n\n**Conclusion**\n\nFrom our pairwise comparisons, we have established the following strict ordering:\n$1.$ $f_2(n) = o(f_4(n))$\n$2.$ $f_4(n) = o(f_1(n))$\n$3.$ $f_1(n) = o(f_3(n))$\n\nBy the transitivity of the little-oh relationship, we can combine these results into a single strict total order:\n$$ f_2(n) \\prec f_4(n) \\prec f_1(n) \\prec f_3(n) $$\nwhere $\\prec$ denotes \"grows strictly slower than\".\n\nThe problem requires the answer to be encoded as a row vector of indices $(i_1, i_2, i_3, i_4)$ corresponding to this ordering. The slowest-growing function is $f_2(n)$, followed by $f_4(n)$, then $f_1(n)$, and the fastest-growing is $f_3(n)$. The sequence of indices is therefore $(2, 4, 1, 3)$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n2 & 4 & 1 & 3\n\\end{pmatrix}\n}\n$$", "id": "3222399"}, {"introduction": "The efficiency of an algorithm is often the sum of the costs of its individual steps. This exercise moves from comparing static functions to analyzing the cumulative cost of iterative processes, modeled by recurrence relations that unroll into summations [@problem_id:3222276]. You will practice a fundamental skill by approximating these discrete sums using definite integrals, a first-principles approach that elegantly bridges the gap between discrete steps and continuous growth to reveal the dominant asymptotic behavior.", "problem": "Consider two algorithms, Algorithm $A$ and Algorithm $B$, that process an input of size $n$ using iterative refinements. At iteration $i$, Algorithm $A$ incurs an incremental cost equal to the natural logarithm $\\ln i$, while Algorithm $B$ incurs an incremental cost equal to the square root $\\sqrt{i}$. The total costs satisfy the recurrences\n$$T_{A}(1) = 0,\\quad T_{A}(n) = T_{A}(n-1) + \\ln n,$$\n$$T_{B}(1) = 0,\\quad T_{B}(n) = T_{B}(n-1) + \\sqrt{n},$$\nfor all integers $n \\geq 2$. Starting from core definitions for summations and properties of the natural logarithm and using first-principles approximations justified by monotonicity and integration, determine the exact value of the limit\n$$L \\;=\\; \\lim_{n \\to \\infty} \\frac{T_{B}(n) - T_{A}(n)}{n^{3/2}}.$$\nYour final answer must be a single real number. No rounding is required and no physical units are involved.", "solution": "The total costs $T_{A}(n)$ and $T_{B}(n)$ are defined by the recurrence relations:\n$$T_{A}(1) = 0, \\quad T_{A}(n) = T_{A}(n-1) + \\ln n \\quad \\text{for } n \\geq 2$$\n$$T_{B}(1) = 0, \\quad T_{B}(n) = T_{B}(n-1) + \\sqrt{n} \\quad \\text{for } n \\geq 2$$\nBy unrolling these recurrences, we can express $T_{A}(n)$ and $T_{B}(n)$ as summations:\n$$T_{A}(n) = T_{A}(1) + \\sum_{k=2}^{n} \\ln k = 0 + \\sum_{k=2}^{n} \\ln k = \\sum_{k=2}^{n} \\ln k$$\n$$T_{B}(n) = T_{B}(1) + \\sum_{k=2}^{n} \\sqrt{k} = 0 + \\sum_{k=2}^{n} \\sqrt{k} = \\sum_{k=2}^{n} \\sqrt{k}$$\nThe problem requires computing the limit:\n$$L = \\lim_{n \\to \\infty} \\frac{T_{B}(n) - T_{A}(n)}{n^{3/2}}$$\nWe can split the limit if the individual limits exist:\n$$L = \\lim_{n \\to \\infty} \\left( \\frac{T_{B}(n)}{n^{3/2}} - \\frac{T_{A}(n)}{n^{3/2}} \\right) = \\left( \\lim_{n \\to \\infty} \\frac{T_{B}(n)}{n^{3/2}} \\right) - \\left( \\lim_{n \\to \\infty} \\frac{T_{A}(n)}{n^{3/2}} \\right)$$\nWe will evaluate each limit separately using integral bounds, as suggested by the problem's requirement for a first-principles approach based on monotonicity and integration.\n\nFirst, let us analyze the term involving $T_{A}(n)$.\nThe function $f(x) = \\ln x$ is continuous and monotonically increasing for $x > 0$. We can therefore bound the sum $T_{A}(n) = \\sum_{k=2}^{n} \\ln k$ using definite integrals.\nFor an increasing function $f(x)$, the following inequalities hold:\n$$\\int_{a-1}^{b} f(x) \\, dx \\leq \\sum_{k=a}^{b} f(k) \\leq \\int_{a}^{b+1} f(x) \\, dx$$\nFor $T_{A}(n)$, we have $a=2$ and $b=n$.\nThe lower bound is:\n$$T_{A}(n) \\geq \\int_{2-1}^{n} \\ln x \\, dx = \\int_{1}^{n} \\ln x \\, dx$$\nThe integral of $\\ln x$ is found using integration by parts: $\\int \\ln x \\, dx = x \\ln x - x + C$.\n$$T_{A}(n) \\geq [x \\ln x - x]_{1}^{n} = (n \\ln n - n) - (1 \\ln 1 - 1) = n \\ln n - n + 1$$\nThe upper bound is:\n$$T_{A}(n) \\leq \\int_{2}^{n+1} \\ln x \\, dx = [x \\ln x - x]_{2}^{n+1} = ((n+1)\\ln(n+1) - (n+1)) - (2\\ln 2 - 2)$$\nSo we have the bounds for $T_{A}(n)$:\n$$n \\ln n - n + 1 \\leq T_{A}(n) \\leq (n+1)\\ln(n+1) - n + 1 - 2\\ln 2$$\nNow, we can find the limit of $\\frac{T_{A}(n)}{n^{3/2}}$ by applying the Squeeze Theorem.\n$$\\frac{n \\ln n - n + 1}{n^{3/2}} \\leq \\frac{T_{A}(n)}{n^{3/2}} \\leq \\frac{(n+1)\\ln(n+1) - n + 1 - 2\\ln 2}{n^{3/2}}$$\nLet's evaluate the limit of the lower bound:\n$$\\lim_{n \\to \\infty} \\frac{n \\ln n - n + 1}{n^{3/2}} = \\lim_{n \\to \\infty} \\left( \\frac{\\ln n}{n^{1/2}} - \\frac{1}{n^{1/2}} + \\frac{1}{n^{3/2}} \\right)$$\nThe term $\\lim_{n \\to \\infty} \\frac{\\ln n}{n^{1/2}}$ can be evaluated using L'Hôpital's Rule:\n$$\\lim_{n \\to \\infty} \\frac{\\ln n}{n^{1/2}} = \\lim_{n \\to \\infty} \\frac{1/n}{\\frac{1}{2}n^{-1/2}} = \\lim_{n \\to \\infty} \\frac{2}{n^{1/2}} = 0$$\nThus, the limit of the lower bound is $0 - 0 + 0 = 0$.\nThe limit of the upper bound is asymptotically equivalent to $\\lim_{n \\to \\infty} \\frac{n \\ln n}{n^{3/2}} = \\lim_{n \\to \\infty} \\frac{\\ln n}{n^{1/2}} = 0$.\nBy the Squeeze Theorem, we conclude:\n$$\\lim_{n \\to \\infty} \\frac{T_{A}(n)}{n^{3/2}} = 0$$\n\nNext, we analyze the term involving $T_{B}(n)$.\nThe function $g(x) = \\sqrt{x}$ is continuous and monotonically increasing for $x \\geq 0$. We bound the sum $T_{B}(n) = \\sum_{k=2}^{n} \\sqrt{k}$ using the same integral inequality.\nThe lower bound for $T_B(n)$ is:\n$$T_{B}(n) \\geq \\int_{2-1}^{n} \\sqrt{x} \\, dx = \\int_{1}^{n} x^{1/2} \\, dx$$\nThe integral is:\n$$T_{B}(n) \\geq \\left[ \\frac{2}{3}x^{3/2} \\right]_{1}^{n} = \\frac{2}{3}n^{3/2} - \\frac{2}{3}$$\nThe upper bound is:\n$$T_{B}(n) \\leq \\int_{2}^{n+1} \\sqrt{x} \\, dx = \\left[ \\frac{2}{3}x^{3/2} \\right]_{2}^{n+1} = \\frac{2}{3}(n+1)^{3/2} - \\frac{2}{3}(2)^{3/2} = \\frac{2}{3}(n+1)^{3/2} - \\frac{4\\sqrt{2}}{3}$$\nSo we have the bounds for $T_{B}(n)$:\n$$\\frac{2}{3}n^{3/2} - \\frac{2}{3} \\leq T_{B}(n) \\leq \\frac{2}{3}(n+1)^{3/2} - \\frac{4\\sqrt{2}}{3}$$\nApplying the Squeeze Theorem to $\\frac{T_{B}(n)}{n^{3/2}}$:\n$$\\frac{\\frac{2}{3}n^{3/2} - \\frac{2}{3}}{n^{3/2}} \\leq \\frac{T_{B}(n)}{n^{3/2}} \\leq \\frac{\\frac{2}{3}(n+1)^{3/2} - \\frac{4\\sqrt{2}}{3}}{n^{3/2}}$$\n$$\\frac{2}{3} - \\frac{2}{3n^{3/2}} \\leq \\frac{T_{B}(n)}{n^{3/2}} \\leq \\frac{2}{3}\\left(\\frac{n+1}{n}\\right)^{3/2} - \\frac{4\\sqrt{2}}{3n^{3/2}}$$\nTaking the limit as $n \\to \\infty$:\nThe limit of the lower bound is $\\lim_{n \\to \\infty} \\left(\\frac{2}{3} - \\frac{2}{3n^{3/2}}\\right) = \\frac{2}{3}$.\nThe limit of the upper bound is $\\lim_{n \\to \\infty} \\left(\\frac{2}{3}\\left(1+\\frac{1}{n}\\right)^{3/2} - \\frac{4\\sqrt{2}}{3n^{3/2}}\\right) = \\frac{2}{3}(1)^{3/2} - 0 = \\frac{2}{3}$.\nBy the Squeeze Theorem, we conclude:\n$$\\lim_{n \\to \\infty} \\frac{T_{B}(n)}{n^{3/2}} = \\frac{2}{3}$$\n\nFinally, we substitute these results back into the expression for $L$:\n$$L = \\left( \\lim_{n \\to \\infty} \\frac{T_{B}(n)}{n^{3/2}} \\right) - \\left( \\lim_{n \\to \\infty} \\frac{T_{A}(n)}{n^{3/2}} \\right) = \\frac{2}{3} - 0 = \\frac{2}{3}$$\nThe exact value of the limit is $\\frac{2}{3}$.", "answer": "$$ \\boxed{\\frac{2}{3}} $$", "id": "3222276"}, {"introduction": "Divide-and-conquer algorithms, ubiquitous in computer science, are naturally described by recurrence relations. While the Master Theorem offers a valuable shortcut for many cases, a deeper understanding comes from analyzing recurrences from first principles [@problem_id:3222269]. This final practice contrasts two classic divide-and-conquer recurrences, demonstrating how a subtle change in the non-recursive work—from $O(n)$ to $O(n/\\ln n)$—profoundly impacts the overall time complexity, requiring you to unroll the recurrences and analyze the resulting series.", "problem": "Two divide-and-conquer procedures on arrays of size $n$ have running times defined for $n=2^{k}$, where $k \\in \\mathbb{Z}_{\\ge 0}$, by the recurrences\n$$T_{1}(n)=2\\,T_{1}\\!\\left(\\frac{n}{2}\\right)+\\frac{n}{\\ln n}\\quad\\text{and}\\quad T_{2}(n)=2\\,T_{2}\\!\\left(\\frac{n}{2}\\right)+n,$$\nfor all $n \\ge 2$, with base conditions $T_{1}(1)=1$ and $T_{2}(1)=1$. Assume $T_{1}(n)$ and $T_{2}(n)$ are defined only on powers of $2$. Using only fundamental definitions from asymptotic analysis and well-tested facts about divide-and-conquer recurrences, determine the exact value of the limit\n$$L=\\lim_{n \\to \\infty}\\frac{T_{1}(n)}{T_{2}(n)}.$$\nYour final answer must be a single real number. No rounding is required.", "solution": "The task is to evaluate the limit $L=\\lim_{n \\to \\infty}\\frac{T_{1}(n)}{T_{2}(n)}$, where $T_1(n)$ and $T_2(n)$ are defined for $n=2^k$ ($k \\in \\mathbb{Z}_{\\ge 0}$) by the recurrences:\n$$T_{1}(n)=2\\,T_{1}\\!\\left(\\frac{n}{2}\\right)+\\frac{n}{\\ln n}, \\quad T_{1}(1)=1$$\n$$T_{2}(n)=2\\,T_{2}\\!\\left(\\frac{n}{2}\\right)+n, \\quad T_{2}(1)=1$$\n\nFirst, we find an exact closed-form expression for $T_{2}(n)$.\nLet $n = 2^k$ for some integer $k \\ge 0$. Then $k = \\log_2 n$. The recurrence for $T_2$ is defined for $n \\ge 2$, which corresponds to $k \\ge 1$.\nWe unroll the recurrence:\n\\begin{align*} T_2(n) &= 2 T_2\\left(\\frac{n}{2}\\right) + n \\\\ &= 2 \\left( 2 T_2\\left(\\frac{n}{4}\\right) + \\frac{n}{2} \\right) + n = 4 T_2\\left(\\frac{n}{4}\\right) + n + n \\\\ &= 4 \\left( 2 T_2\\left(\\frac{n}{8}\\right) + \\frac{n}{4} \\right) + 2n = 8 T_2\\left(\\frac{n}{8}\\right) + 3n \\\\ & \\quad \\vdots \\\\ &= 2^i T_2\\left(\\frac{n}{2^i}\\right) + i \\cdot n \\end{align*}\nWe unroll this until we reach the base case $T_2(1)$, which occurs when $\\frac{n}{2^k}=1$, or $i=k$.\n$$T_2(n) = 2^k T_2\\left(\\frac{n}{2^k}\\right) + k \\cdot n = 2^k T_2(1) + k n$$\nSince $n=2^k$ and $T_2(1)=1$, we have:\n$$T_2(n) = n \\cdot 1 + k n = n(1+k)$$\nSubstituting $k = \\log_2 n$, we obtain the closed form for $T_2(n)$:\n$$T_2(n) = n(1 + \\log_2 n)$$\n\nNext, we find a similar expression for $T_1(n)$. Let $n = 2^k$, so $k = \\log_2 n$. The recurrence is $T_1(n) = 2 T_1(n/2) + \\frac{n}{\\ln n}$ for $n \\ge 2$ ($k \\ge 1$).\nWe unroll this recurrence:\n\\begin{align*} T_1(n) &= 2 T_1\\left(\\frac{n}{2}\\right) + \\frac{n}{\\ln n} \\\\ &= 2 \\left( 2 T_1\\left(\\frac{n}{4}\\right) + \\frac{n/2}{\\ln(n/2)} \\right) + \\frac{n}{\\ln n} \\\\ &= 4 T_1\\left(\\frac{n}{4}\\right) + \\frac{n}{\\ln(n/2)} + \\frac{n}{\\ln n} \\\\ & \\quad \\vdots \\\\ &= 2^k T_1\\left(\\frac{n}{2^k}\\right) + \\sum_{i=0}^{k-1} 2^i \\frac{n/2^i}{\\ln(n/2^i)} \\end{align*}\nUsing the base case $T_1(1)=1$ and $n=2^k$:\n$$T_1(n) = 2^k T_1(1) + \\sum_{i=0}^{k-1} \\frac{n}{\\ln(2^{k-i})}$$\n$$T_1(n) = n + n \\sum_{i=0}^{k-1} \\frac{1}{(k-i)\\ln 2}$$\nLet $j=k-i$. As $i$ goes from $0$ to $k-1$, $j$ goes from $k$ to $1$.\n$$T_1(n) = n + \\frac{n}{\\ln 2} \\sum_{j=1}^{k} \\frac{1}{j}$$\nThe sum $\\sum_{j=1}^{k} \\frac{1}{j}$ is the $k$-th Harmonic number, denoted $H_k$. So, with $k = \\log_2 n$:\n$$T_1(n) = n \\left( 1 + \\frac{H_{\\log_2 n}}{\\ln 2} \\right)$$\n\nNow we can evaluate the limit $L$:\n$$L = \\lim_{n \\to \\infty} \\frac{T_1(n)}{T_2(n)} = \\lim_{n \\to \\infty} \\frac{n \\left( 1 + \\frac{H_{\\log_2 n}}{\\ln 2} \\right)}{n (1 + \\log_2 n)}$$\nWe can cancel the factor of $n$:\n$$L = \\lim_{n \\to \\infty} \\frac{1 + \\frac{H_{\\log_2 n}}{\\ln 2}}{1 + \\log_2 n}$$\nLet $k = \\log_2 n$. As $n \\to \\infty$, we have $k \\to \\infty$. The limit becomes:\n$$L = \\lim_{k \\to \\infty} \\frac{1 + \\frac{H_k}{\\ln 2}}{1 + k}$$\nWe can split the fraction:\n$$L = \\lim_{k \\to \\infty} \\left( \\frac{1}{1+k} + \\frac{H_k}{(1+k)\\ln 2} \\right)$$\nThe first term clearly goes to $0$:\n$$\\lim_{k \\to \\infty} \\frac{1}{1+k} = 0$$\nFor the second term, we analyze the limit $\\lim_{k \\to \\infty} \\frac{H_k}{1+k}$. To do this rigorously, we can use the Squeeze Theorem. The Harmonic number $H_k = \\sum_{i=1}^k \\frac{1}{i}$ can be bounded by integrals of the function $f(x) = 1/x$:\n$$\\int_1^{k+1} \\frac{1}{x} \\, dx \\le \\sum_{i=1}^k \\frac{1}{i} \\le 1 + \\int_1^k \\frac{1}{x} \\, dx$$\nEvaluating the integrals gives:\n$$\\ln(k+1) \\le H_k \\le 1 + \\ln(k)$$\nNow divide the entire inequality by $1+k$:\n$$\\frac{\\ln(k+1)}{1+k} \\le \\frac{H_k}{1+k} \\le \\frac{1+\\ln(k)}{1+k}$$\nWe take the limits of the lower and upper bounds as $k \\to \\infty$. Using L'Hôpital's rule for the lower bound:\n$$\\lim_{k \\to \\infty} \\frac{\\ln(k+1)}{1+k} = \\lim_{k \\to \\infty} \\frac{1/(k+1)}{1} = 0$$\nAnd for the upper bound:\n$$\\lim_{k \\to \\infty} \\frac{1+\\ln(k)}{1+k} = \\lim_{k \\to \\infty} \\frac{1/(k)}{1} = 0$$\nBy the Squeeze Theorem, we conclude:\n$$\\lim_{k \\to \\infty} \\frac{H_k}{1+k} = 0$$\nSubstituting this back into the expression for $L$:\n$$L = 0 + \\lim_{k \\to \\infty} \\frac{1}{\\ln 2} \\left( \\frac{H_k}{1+k} \\right) = 0 + \\frac{1}{\\ln 2} \\cdot 0 = 0$$\nThus, the exact value of the limit is $0$.", "answer": "$$\n\\boxed{0}\n$$", "id": "3222269"}]}