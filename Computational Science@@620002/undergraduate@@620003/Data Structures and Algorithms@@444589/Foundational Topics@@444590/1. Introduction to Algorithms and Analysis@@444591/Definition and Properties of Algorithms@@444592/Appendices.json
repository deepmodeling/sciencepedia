{"hands_on_practices": [{"introduction": "Before we can analyze an algorithm's efficiency, we must be certain of its correctness. This exercise explores the formal method of proving correctness using loop invariants, a cornerstone of algorithm design. You will analyze a flawed algorithm to understand why establishing an invariant's initialization and maintenance is not enough; the termination condition is the crucial final piece that guarantees the postcondition is met [@problem_id:3226962].", "problem": "You are given an array $A[0\\,..\\,n-1]$ with $n \\ge 1$, and the following algorithm is claimed to compute $m = \\min\\{A[j] \\mid 0 \\le j \\le n-1\\}$.\n\nAlgorithm description (using ordinary sequencing and conditional control, with integer indices and comparisons):\n- Precondition: $n \\ge 1$.\n- Initialize $m \\leftarrow A[0]$, $i \\leftarrow 1$.\n- Define the loop invariant $I(i,m)$ to be: $m = \\min\\{A[j] \\mid 0 \\le j  i\\}$ and $1 \\le i \\le n-1$.\n- While $i  n-1$ do:\n  - If $A[i]  m$ then set $m \\leftarrow A[i]$.\n  - Set $i \\leftarrow i + 1$.\n- Return $m$.\n\nTake as fundamental base the standard definitions from the method of proving correctness of iterative algorithms by loop invariants: a loop invariant $I$ must be established by initialization, preserved by maintenance, and combined with a well-defined termination state to imply the postcondition; partial correctness means that, under the precondition, if the algorithm terminates then the postcondition holds; total correctness additionally requires termination.\n\nAssume you have already formally proved both the initialization and the maintenance of $I(i,m)$ for the above algorithm from these base definitions, so that at the start of each loop test $I(i,m)$ holds. However, it is observed that the algorithm can return an incorrect result when $A[n-1]$ is strictly less than all $A[0], A[1], \\dots, A[n-2]$.\n\nWhich additional property, beyond the fact that $I(i,m)$ is initialized and maintained, must be verified to complete a correct proof that the algorithm returns $m = \\min\\{A[j] \\mid 0 \\le j \\le n-1\\}$ under the precondition $n \\ge 1$?\n\nA. The invariant $I(i,m)$ alone implies the postcondition at every iteration without reference to how or when the loop terminates.\n\nB. The initialization must set $i \\leftarrow 0$ instead of $i \\leftarrow 1$.\n\nC. The termination and boundary condition must ensure that the loop exits precisely when $i = n$, so that at termination $I(i,m)$ implies $m = \\min\\{A[j] \\mid 0 \\le j \\le n-1\\}$.\n\nD. The algorithm must be made non-deterministic so that it can explore all possible index orders, thereby covering boundary cases implicitly.\n\nE. It must be shown that $m$ is monotonically non-increasing as $i$ advances, ensuring progress toward the postcondition regardless of the termination state.", "solution": "The user requires a critical validation of the provided problem statement about an algorithm's correctness, followed by a formal derivation of the solution and evaluation of all options.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n- **Array:** $A[0\\,..\\,n-1]$, with $n \\ge 1$.\n- **Algorithm Goal:** Compute $m = \\min\\{A[j] \\mid 0 \\le j \\le n-1\\}$.\n- **Algorithm Steps:**\n    1. Precondition: $n \\ge 1$.\n    2. Initialization: $m \\leftarrow A[0]$, $i \\leftarrow 1$.\n    3. Loop Invariant $I(i,m)$: $m = \\min\\{A[j] \\mid 0 \\le j  i\\}$ and $1 \\le i \\le n-1$.\n    4. Loop: `while` $i  n-1$ `do`:\n       - `if` $A[i]  m$ `then` set $m \\leftarrow A[i]$.\n       - Set $i \\leftarrow i + 1$.\n    5. Return: $m$.\n- **Provided Information:**\n    1. The initialization and maintenance properties of the loop invariant $I(i,m)$ have been formally proved.\n    2. The algorithm can return an incorrect result when $A[n-1]$ is strictly less than all other elements $A[0], \\dots, A[n-2]$.\n- **Question:** Which additional property must be verified to complete a correct proof that the algorithm returns $m = \\min\\{A[j] \\mid 0 \\le j \\le n-1\\}$?\n\n**Step 2: Validate Using Extracted Givens**\n\n- **Scientific Grounding:** The problem is based on the formal method of proving algorithm correctness using loop invariants, a cornerstone of theoretical computer science and software engineering. The concepts of precondition, postcondition, loop invariant, initialization, maintenance, and termination are standard and well-defined.\n- **Well-Posedness:** The problem presents a flawed algorithm and asks to identify the missing element in its formal proof of correctness. This is a standard and effective pedagogical structure. The problem is well-posed as it directs the analysis towards the three-part structure of a loop invariant proof.\n- **Objectivity:** The problem is stated in objective, formal language.\n- **Consistency and Completeness:** The problem statement is internally consistent. It provides an algorithm, a loop invariant, and a key observation about its failure. The observation (failure when $A[n-1]$ is minimum) directly points to the flaw in the algorithm's logic, which is the loop's boundary condition. The provided invariant $I(i,m)$ is defined with the constraint $1 \\le i \\le n-1$. For the case $n=1$, this constraint becomes $1 \\le i \\le 0$, which is impossible. This implies the analysis is intended for $n \\ge 2$, which is consistent with the loop structure and the observation. The problem is not underspecified for its purpose, which is to diagnose a failure in the proof structure.\n\n**Step 3: Verdict and Action**\n\nThe problem statement is **valid**. It is a well-formed question about the formal verification of an algorithm using loop invariants. I will now proceed to the solution.\n\n### Solution Derivation\n\nThe standard method for proving the partial correctness of an algorithm with a loop using a loop invariant requires proving three properties:\n\n1.  **Initialization:** The loop invariant is true before the first iteration of the loop.\n2.  **Maintenance:** If the loop invariant is true at the start of an iteration, it remains true at the end of that iteration.\n3.  **Termination:** When the loop terminates, the combination of the loop invariant and the termination condition implies the desired postcondition of the algorithm.\n\nThe problem states that Initialization and Maintenance are already proven. The task is to identify the missing piece for a complete and correct proof. This directs our attention to the third property: Termination.\n\nLet's analyze the state of the algorithm at termination.\n- The loop condition is $i  n-1$.\n- The loop terminates when this condition is false, i.e., when $i \\ge n-1$.\n- The variable $i$ is initialized to $1$ and incremented by $1$ in each iteration. Therefore, the loop will terminate precisely when $i$ first equals $n-1$.\n- The loop invariant $I(i,m)$ is given to hold at the start of each loop test. This means that when the final test $i  n-1$ is performed and fails, the invariant from the end of the previous iteration still holds. At this point, $i = n-1$.\n- The invariant is stated as $I(i,m): m = \\min\\{A[j] \\mid 0 \\le j  i\\}$ and $1 \\le i \\le n-1$.\n- So, at termination, we have $i = n-1$ and the invariant holds for this value of $i$. This gives us the property: $m = \\min\\{A[j] \\mid 0 \\le j  n-1\\}$.\n\nThe algorithm then returns this value of $m$. The desired postcondition, however, is that $m = \\min\\{A[j] \\mid 0 \\le j \\le n-1\\}$.\n\nThe property derived at termination is $m = \\min\\{A[0], A[1], \\dots, A[n-2]\\}$.\nThe desired postcondition is $m = \\min\\{A[0], A[1], \\dots, A[n-1]\\}$.\n\nClearly, the property at termination does not imply the postcondition. Specifically, the element $A[n-1]$ is never compared, so if it is the minimum, the algorithm will return an incorrect result. This matches the observation given in the problem statement.\n\nThe \"additional property\" that must be verified to complete a correct proof is precisely this third step: showing that the invariant and the termination condition together imply the postcondition. The given algorithm fails this verification step. A correct proof would necessitate an algorithm where this step is satisfied. This would typically involve changing the loop to run until all elements are processed, for instance, `while` $i  n$. This would cause termination at $i=n$, and if the invariant holds, we would get $m = \\min\\{A[j] \\mid 0 \\le j  n\\}$, which is the correct postcondition.\n\n### Evaluation of Options\n\n**A. The invariant $I(i,m)$ alone implies the postcondition at every iteration without reference to how or when the loop terminates.**\nThis statement misrepresents the function of a loop invariant. A loop invariant establishes a property that holds for a partial execution of the algorithm (up to iteration $i$). It is only upon termination, when $i$ has reached its final value, that the invariant can be used to establish the final postcondition. The invariant for an intermediate $i  n-1$ cannot imply the result for the entire array.\n**Verdict: Incorrect.**\n\n**B. The initialization must set $i \\leftarrow 0$ instead of $i \\leftarrow 1$.**\nChanging the initialization to $i \\leftarrow 0$ while keeping the rest of the algorithm the same does not solve the fundamental problem. The loop condition is still $i  n-1$, so the loop will terminate before processing $A[n-1]$. This change does not address the error in the termination condition.\n**Verdict: Incorrect.**\n\n**C. The termination and boundary condition must ensure that the loop exits precisely when $i = n$, so that at termination $I(i,m)$ implies $m = \\min\\{A[j] \\mid 0 \\le j \\le n-1\\}$.**\nThis option correctly identifies the missing part of the proof. The third step of a loop invariant proof requires showing that the invariant, combined with the negation of the loop condition, implies the postcondition. This option accurately describes what is required for this step to succeed: the loop must process all elements, which for this indexing scheme means terminating at $i=n$. At $i=n$, the invariant $m = \\min\\{A[j] \\mid 0 \\le j  i\\}$ would become $m = \\min\\{A[j] \\mid 0 \\le j  n\\}$, which is the desired postcondition. The current algorithm's failure is precisely its inability to satisfy this termination property.\n**Verdict: Correct.**\n\n**D. The algorithm must be made non-deterministic so that it can explore all possible index orders, thereby covering boundary cases implicitly.**\nIntroducing non-determinism is an unrelated and excessively complex solution to a simple off-by-one error. The algorithm can be corrected deterministically. This approach is not relevant to the standard method of proof by loop invariants for this type of problem.\n**Verdict: Incorrect.**\n\n**E. It must be shown that $m$ is monotonically non-increasing as $i$ advances, ensuring progress toward the postcondition regardless of the termination state.**\nWhile it is true that $m$ is monotonically non-increasing in this algorithm (it is only ever updated to a smaller value), this property is insufficient to prove correctness. The algorithm can make \"progress\" towards a value that is not the correct final answer. The termination state is critical; without processing all elements, the non-increasing nature of $m$ does not guarantee it reaches the global minimum. The proof structure of loop invariants requires a specific argument at termination, which this property does not provide.\n**Verdict: Incorrect.**", "answer": "$$\\boxed{C}$$", "id": "3226962"}, {"introduction": "Asymptotic notation gives us a powerful lens for viewing an algorithm's long-term behavior, but it can be misleading for practical input sizes where constant factors dominate. This problem provides a hands-on comparison between a polynomial-time algorithm and an exponential-time one, demonstrating that an asymptotically \"worse\" algorithm can be faster in a specific, practical range. This analysis is fundamental to making informed decisions when choosing between algorithms [@problem_id:3226973].", "problem": "In the Random Access Machine (RAM) model, an algorithm is a finite, well-defined procedure that maps inputs to outputs and executes a sequence of primitive operations whose count can be expressed as a function of the input size. For input size $N \\in \\mathbb{N}$ with $N \\geq 1$, consider two algorithms $A$ and $B$ whose exact time complexities (primitive operation counts) are given by $C_{A}(N)$ and $C_{B}(N)$, respectively. Assume every primitive operation has unit cost.\n\nAlgorithm $A$ has exact time complexity $C_{A}(N) = 1000 N^{2}$ and algorithm $B$ has exact time complexity $C_{B}(N) = \\frac{2^{N}}{1000}$. Although algorithm $B$ is asymptotically worse than algorithm $A$ in the sense of growth rates, it may be faster for some finite $N$ due to smaller leading constants and the detailed structure of the exact cost functions.\n\nUsing the foundational definition that, for a fixed input size $N$, the algorithm with the smaller exact primitive operation count is faster, determine the largest integer $N$ for which algorithm $B$ is faster than algorithm $A$; that is, find the maximum $N \\in \\mathbb{N}$ such that $C_{B}(N)  C_{A}(N)$. Express your final answer as a single integer.", "solution": "The problem requires finding the largest integer $N \\in \\mathbb{N}$, with $N \\geq 1$, for which algorithm $B$ is faster than algorithm $A$. According to the problem's definition, this means we must find the largest integer $N$ that satisfies the inequality $C_{B}(N)  C_{A}(N)$.\n\nFirst, we state the givens for the exact time complexities of algorithms $A$ and $B$:\n$$ C_{A}(N) = 1000 N^{2} $$\n$$ C_{B}(N) = \\frac{2^{N}}{1000} $$\n\nThe condition that algorithm $B$ is faster than algorithm $A$ translates to the following inequality:\n$$ \\frac{2^{N}}{1000}  1000 N^{2} $$\n\nTo analyze this inequality, we can rearrange the terms. Multiplying both sides by $1000$ yields:\n$$ 2^{N}  1000^{2} N^{2} $$\n$$ 2^{N}  (10^3)^{2} N^{2} $$\n$$ 2^{N}  10^6 N^{2} $$\n\nThis is a transcendental inequality, which cannot be solved for $N$ using elementary algebraic operations. We must analyze the behavior of the functions on both sides of the inequality. Let $g(N) = 2^{N}$ and $h(N) = 10^6 N^{2}$. It is a fundamental principle of algorithm complexity that exponential functions like $g(N)$ grow significantly faster than polynomial functions like $h(N)$ for sufficiently large $N$. This implies that the inequality $g(N)  h(N)$ will be true for a certain range of initial integer values of $N$, but will eventually become false and remain false as $N$ increases beyond a \"crossover\" point. The problem asks for the largest integer $N$ for which the inequality still holds.\n\nTo locate this crossover point, we can investigate the approximate real-valued solution to the equation $2^{x} = 10^6 x^{2}$. Taking the base-$2$ logarithm of both sides gives:\n$$ \\log_{2}(2^{x}) = \\log_{2}(10^6 x^{2}) $$\n$$ x = \\log_{2}(10^6) + \\log_{2}(x^{2}) $$\n$$ x = 6 \\log_{2}(10) + 2 \\log_{2}(x) $$\n\nUsing the known value $\\log_{2}(10) \\approx 3.321928$, we can write the equation as:\n$$ x \\approx 6(3.321928) + 2 \\log_{2}(x) $$\n$$ x \\approx 19.931568 + 2 \\log_{2}(x) $$\n\nThis equation can be solved numerically or by iteration. The solution represents the point where the costs are equal. The integer we seek will be the floor of this value, or an integer very close to it. Let's test integer values of $N$ in the vicinity of the expected crossover. Based on a quick iterative estimation, the solution lies between $N=29$ and $N=30$. We will now test these two integer values by direct substitution into the inequality $2^{N}  10^6 N^{2}$.\n\nCase 1: Test $N=29$\nWe need to determine if $2^{29}  10^6 (29)^{2}$.\nFirst, calculate the left-hand side (LHS):\n$$ \\text{LHS} = 2^{29} = 536,870,912 $$\nNext, calculate the right-hand side (RHS):\n$$ \\text{RHS} = 10^6 \\times (29)^{2} = 1,000,000 \\times 841 = 841,000,000 $$\nComparing the two values, we find:\n$$ 536,870,912  841,000,000 $$\nThe inequality holds true for $N=29$. Thus, for an input size of $N=29$, algorithm $B$ is faster than algorithm $A$.\n\nCase 2: Test $N=30$\nWe need to determine if $2^{30}  10^6 (30)^{2}$.\nFirst, calculate the LHS:\n$$ \\text{LHS} = 2^{30} = 2 \\times 2^{29} = 2 \\times 536,870,912 = 1,073,741,824 $$\nNext, calculate the RHS:\n$$ \\text{RHS} = 10^6 \\times (30)^{2} = 1,000,000 \\times 900 = 900,000,000 $$\nComparing the two values, we find:\n$$ 1,073,741,824 \\not 900,000,000 $$\nThe inequality does not hold for $N=30$. For an input size of $N=30$, algorithm $A$ is faster than algorithm $B$.\n\nSince the exponential function $2^N$ continues to grow faster than the polynomial function $10^6 N^2$ for all $N$ greater than their crossover point (which we have determined is between $29$ and $30$), the inequality $C_B(N)  C_A(N)$ will not hold for any integer $N \\ge 30$.\n\nTherefore, the largest integer $N$ for which algorithm $B$ is faster than algorithm $A$ is $29$.", "answer": "$$\\boxed{29}$$", "id": "3226973"}, {"introduction": "Recursive algorithms often lead to recurrence relations that describe their time complexity, but not all recurrences fit simple patterns. This exercise challenges you to analyze a recursive algorithm whose complexity is described by a non-standard recurrence, $T(N) = \\sqrt{N} T(\\sqrt{N}) + N$. By employing a change of variables, you will master a versatile technique for solving complex recurrences and deriving a tight asymptotic bound on performance [@problem_id:3227010].", "problem": "Consider a deterministic algorithm whose running time on inputs of size $N$ is governed by the recurrence\n$$\nT(N) = \\sqrt{N}\\, T\\!\\left(\\sqrt{N}\\right) + N,\n$$\nfor all $N$ of the form $N = 2^{2^{k}}$ with $k \\in \\mathbb{N}$, and with base condition $T(2) = c_{0}$ for some fixed constant $c_{0} \\ge 0$. Work within the standard word-Random Access Machine (word-RAM) cost model where each arithmetic or comparison operation on constant-word-size operands has unit cost, and interpret $T(N)$ as the worst-case number of primitive operations as a function of input size $N$.\n\nUsing only foundational definitions of asymptotic growth and recurrences arising from recursive algorithms, derive from first principles a tight asymptotic rate $f(N)$ such that $T(N) \\in \\Theta\\!\\big(f(N)\\big)$. Report only the dominant-order function $f(N)$ (that is, suppress multiplicative constant factors), and use the natural logarithm $\\ln$ in your final expression. Your final answer must be a single closed-form analytic expression $f(N)$.", "solution": "The problem requires finding a tight asymptotic bound $f(N)$ for a function $T(N)$ defined by the recurrence relation\n$$\nT(N) = \\sqrt{N}\\, T\\!\\left(\\sqrt{N}\\right) + N\n$$\nThe recurrence holds for input sizes $N$ of the form $N = 2^{2^{k}}$ where $k \\in \\mathbb{N}$. The base case is given as $T(2) = c_{0}$ for some constant $c_{0} \\ge 0$. The base case $N=2$ corresponds to $2 = 2^{2^k}$, which implies $1 = 2^k$, so $k=0$. This establishes that the set of natural numbers is taken to be $\\mathbb{N} = \\{0, 1, 2, \\ldots\\}$.\n\nThe given recurrence relation is not in a standard form amenable to the Master Theorem due to the coefficient $\\sqrt{N}$ of the recursive term. We will solve this recurrence from first principles using a change of variables.\n\nFirst, let's normalize the recurrence by dividing both sides by $N$:\n$$\n\\frac{T(N)}{N} = \\frac{\\sqrt{N}\\, T\\!\\left(\\sqrt{N}\\right)}{N} + \\frac{N}{N}\n$$\n$$\n\\frac{T(N)}{N} = \\frac{T\\!\\left(\\sqrt{N}\\right)}{\\sqrt{N}} + 1\n$$\nThis transformation reveals a simpler structure. Let us define a new function $G(N)$ as:\n$$\nG(N) = \\frac{T(N)}{N}\n$$\nSubstituting $G(N)$ into the transformed recurrence, we get:\n$$\nG(N) = G\\!\\left(\\sqrt{N}\\right) + 1\n$$\nThis is a much simpler recurrence relation for $G(N)$. We can solve this by unrolling it, taking advantage of the specified structure of $N$. The problem states that $N$ is of the form $N=2^{2^k}$.\nLet's apply the recurrence for $G(N)$ repeatedly:\n$$\n\\begin{aligned}\nG(N) = G\\!\\left(N^{1/2}\\right) + 1 \\\\\n= \\left(G\\!\\left(\\left(N^{1/2}\\right)^{1/2}\\right) + 1\\right) + 1 = G\\!\\left(N^{1/4}\\right) + 2 \\\\\n= \\left(G\\!\\left(\\left(N^{1/4}\\right)^{1/2}\\right) + 1\\right) + 2 = G\\!\\left(N^{1/8}\\right) + 3\n\\end{aligned}\n$$\nBy continuing this process $m$ times, we can see a pattern:\n$$\nG(N) = G\\!\\left(N^{1/2^m}\\right) + m\n$$\nThe unrolling stops when the argument of $G$ reaches the base case, which is $N=2$. So we set $N^{1/2^m} = 2$.\nTo solve for $m$, we take the logarithm base $2$ of both sides:\n$$\n\\log_{2}\\left(N^{1/2^m}\\right) = \\log_{2}(2)\n$$\n$$\n\\frac{1}{2^m} \\log_{2}(N) = 1\n$$\n$$\n2^m = \\log_{2}(N)\n$$\nTaking the logarithm base $2$ again:\n$$\n\\log_{2}\\left(2^m\\right) = \\log_{2}(\\log_{2}(N))\n$$\n$$\nm = \\log_{2}(\\log_{2}(N))\n$$\nNow, we substitute this expression for $m$ back into the equation for $G(N)$:\n$$\nG(N) = G(2) + \\log_{2}(\\log_{2}(N))\n$$\nThe value of $G(2)$ can be found from the base case for $T(N)$:\n$$\nG(2) = \\frac{T(2)}{2} = \\frac{c_{0}}{2}\n$$\nSo, the exact expression for $G(N)$ is:\n$$\nG(N) = \\frac{c_{0}}{2} + \\log_{2}(\\log_{2}(N))\n$$\nTo find $T(N)$, we reverse the initial substitution $G(N) = T(N)/N$:\n$$\nT(N) = N \\cdot G(N) = N \\left(\\frac{c_{0}}{2} + \\log_{2}(\\log_{2}(N))\\right)\n$$\n$$\nT(N) = \\frac{c_{0}}{2}N + N\\log_{2}(\\log_{2}(N))\n$$\nWe are asked for a tight asymptotic rate $f(N)$ such that $T(N) \\in \\Theta(f(N))$. We must identify the dominant term in the expression for $T(N)$ as $N \\to \\infty$. The two terms are $\\frac{c_{0}}{2}N$ and $N\\log_{2}(\\log_{2}(N))$. As $N \\to \\infty$, the function $\\log_{2}(\\log_{2}(N))$ grows without bound. Therefore, the term $N\\log_{2}(\\log_{2}(N))$ grows asymptotically faster than the linear term $N$.\nThus, the asymptotic behavior of $T(N)$ is governed by the second term:\n$$\nT(N) \\in \\Theta\\left(N\\log_{2}(\\log_{2}(N))\\right)\n$$\nThe problem requires the final expression to use the natural logarithm, $\\ln$. We can convert the base of the logarithm using the change of base formula $\\log_{b}(x) = \\frac{\\ln(x)}{\\ln(b)}$.\n$$\n\\log_{2}(\\log_{2}(N)) = \\frac{\\ln(\\log_{2}(N))}{\\ln(2)} = \\frac{\\ln\\left(\\frac{\\ln(N)}{\\ln(2)}\\right)}{\\ln(2)} = \\frac{\\ln(\\ln(N)) - \\ln(\\ln(2))}{\\ln(2)}\n$$\nFor large $N$, $\\ln(\\ln(N))$ is the dominant part of this expression. The term $\\ln(\\ln(2))$ is a constant. Therefore:\n$$\n\\log_{2}(\\log_{2}(N)) = \\frac{1}{\\ln(2)}\\ln(\\ln(N)) - \\frac{\\ln(\\ln(2))}{\\ln(2)}\n$$\nAsymptotically, $\\log_{2}(\\log_{2}(N)) \\in \\Theta(\\ln(\\ln(N)))$, since constant factors and lower-order additive constants are ignored in $\\Theta$-notation.\nSubstituting this into the asymptotic bound for $T(N)$:\n$$\nT(N) \\in \\Theta(N \\cdot \\ln(\\ln(N)))\n$$\nThe problem asks for the dominant-order function $f(N)$, suppressing multiplicative constant factors. This function is:\n$$\nf(N) = N \\ln(\\ln(N))\n$$", "answer": "$$\n\\boxed{N \\ln(\\ln(N))}\n$$", "id": "3227010"}]}