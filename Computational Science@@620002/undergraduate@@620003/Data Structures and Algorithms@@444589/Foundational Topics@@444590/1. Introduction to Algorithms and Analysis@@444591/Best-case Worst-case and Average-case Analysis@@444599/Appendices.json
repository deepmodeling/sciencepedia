{"hands_on_practices": [{"introduction": "Worst-case analysis can feel abstract, but it is a critical tool for understanding an algorithm's performance limits. In this first exercise, you will step into the role of an adversary to design an input that intentionally subverts the Lempel-Ziv-Welch (LZW) compression algorithm, causing it to produce an output that is actually larger than the original. This practice [@problem_id:3214412] demonstrates that worst-case scenarios are not just theoretical possibilities but can be practically constructed, revealing important boundary conditions of an algorithm.", "problem": "Consider the Lempel–Ziv–Welch (LZW) compression algorithm (LZW) defined as follows for a byte-oriented file. The input alphabet is the set of all byte values, so the alphabet size is $256$. The encoder initializes its dictionary to contain exactly the $256$ single-byte strings, each mapped to a distinct code. The encoder uses a fixed-width code of $12$ bits for every output code throughout the entire run. There are no special control codes (no clear code and no end-of-information code). At each step, the encoder outputs the code for the longest dictionary phrase that matches the current input prefix, then adds to the dictionary the concatenation of that phrase with the next input symbol (if any remains), and advances the input cursor past the matched phrase.\n\nUsing the core definitions of worst-case analysis in data structures and algorithms (that is, the maximum resource use over all inputs of a given size) together with the above LZW specification, design a file of length $N = 50{,}000$ bytes that becomes larger after LZW compression. Your design must ensure that, during encoding, the longest match at every step has length $1$. You may assume the existence of sequences over the byte alphabet in which all adjacent pairs are distinct; for example, any length-$N$ prefix of a sequence that enumerates distinct adjacent pairs without repetition is acceptable. Justify why your design indeed forces the encoder to output exactly one code per input byte. Then, compute the exact number of extra bits produced by the compression relative to the original file size. Express the final size difference in bits. No rounding is required, and you must provide the final answer as a single real-valued number.", "solution": "The fundamental base for the analysis is the definition of worst-case behavior in algorithm analysis: for a given input size, the worst case is the maximum cost (here, the maximum compressed size) over all valid inputs. We also rely on the standard operation of Lempel–Ziv–Welch (LZW) compression with a fixed-width code of $12$ bits, an initial dictionary of all $256$ single-byte strings, and the greedy longest-match rule. Our goal is to exhibit an input whose structure forces the encoder to behave in a way that yields maximal output length per input byte.\n\nLet the input be a sequence $S$ of $N = 50{,}000$ bytes over the $256$-symbol alphabet. We design $S$ so that every adjacent byte pair $(S[i], S[i+1])$ for $i \\in \\{1, 2, \\dots, N-1\\}$ is distinct; that is, no ordered pair of consecutive symbols repeats anywhere in $S$. Such a sequence exists because the number of distinct ordered pairs over a $256$-symbol alphabet is $256^{2} = 65{,}536$, which exceeds $N-1 = 49{,}999$, so one can select any length-$N$ prefix of a sequence that enumerates distinct pairs without repetition. One constructive viewpoint is to consider a directed multigraph whose vertices are the $256$ symbols and whose edges represent ordered pairs; an Eulerian traversal can visit edges without repeating, yielding a cyclic sequence in which each pair appears exactly once. Taking any length-$N$ segment of this cycle achieves the desired property that all adjacent pairs are distinct within the segment.\n\nWe now prove that, under this design, the LZW encoder outputs exactly one code per input byte. The encoder begins with a dictionary containing all single-byte strings. At the start of encoding, consider the pointer to the current position $i$ in $S$. Because the dictionary initially contains only single-byte phrases, the longest dictionary phrase matching the input prefix at position $i$ is the single symbol $S[i]$. The encoder outputs the code for $S[i]$, and then (by LZW’s update rule) inserts into the dictionary the phrase formed by concatenating this matched phrase $S[i]$ with the next symbol $S[i+1]$, namely the two-byte string $S[i]S[i+1]$, provided that $i+1 \\le N$. The encoder then advances to the next position $i' = i+1$.\n\nAt position $i'$, the dictionary now contains the newly added two-byte phrase $S[i]S[i+1]$. However, the encoder attempts to match the longest phrase starting at $S[i'] = S[i+1]$. The only two-byte phrase that could match starting at $i'$ is $S[i+1]S[i+2]$. By the design of $S$, all adjacent pairs are distinct. Therefore, unless the immediately preceding pair $S[i]S[i+1]$ happens to equal $S[i+1]S[i+2]$, which is impossible when pairs are all distinct, the two-byte phrase $S[i+1]S[i+2]$ is not yet in the dictionary. Consequently, the longest match at position $i'$ again has length $1$, namely the single-byte $S[i+1]$. The encoder outputs the code for $S[i+1]$ and inserts the new two-byte phrase $S[i+1]S[i+2]$ into the dictionary. This argument iterates for each successive position because the design guarantees that no two adjacent pairs are equal anywhere in $S$. By induction over $i$ from $1$ to $N$, the longest match at every step has length $1$, and the encoder outputs exactly one code for each input byte. Therefore, the total number of output codes is exactly $N$.\n\nGiven the fixed-width $12$-bit codes, the compressed size $C$ in bits is\n$$\nC = 12 \\cdot N.\n$$\nThe original file consists of $N$ bytes, so its size $O$ in bits is\n$$\nO = 8 \\cdot N.\n$$\nThe extra bits produced by compression relative to the original are\n$$\n\\Delta = C - O = 12 \\cdot N - 8 \\cdot N = 4 \\cdot N.\n$$\nSubstituting $N = 50{,}000$ yields\n$$\n\\Delta = 4 \\cdot 50{,}000 = 200{,}000.\n$$\nThus, the designed input triggers the worst-case behavior for this fixed-width LZW variant, producing $200{,}000$ more bits than the original. The final answer, expressed in bits, is $200{,}000$.", "answer": "$$\\boxed{200000}$$", "id": "3214412"}, {"introduction": "Beyond static inputs, the performance of dynamic data structures often depends on the entire history of operations performed on them. This exercise [@problem_id:3214330] explores how a splay tree, a data structure known for its excellent long-term (amortized) performance, can be forced into a highly inefficient, list-like structure by a specific sequence of insertions. You will first construct this worst-case state and then analyze the high cost of a subsequent operation, highlighting the important distinction between amortized guarantees and single-operation worst-case performance.", "problem": "You are given an initially empty splay tree that supports insertion and search by performing a standard binary search tree (BST) insertion followed by splaying the accessed or newly inserted node to the root using the canonical bottom-up splay operations: single rotation (zig), double rotation with the same turn direction (zig-zig), and double rotation with opposite turn directions (zig-zag). A rotation is the standard local BST rotation that maintains the BST order property. Assume the exact unit-cost model in which the cost of a single access (a search operation) is the total number of key comparisons performed during the descent plus the total number of rotations performed during the splay that brings the accessed node to the root. All keys are distinct.\n\nYour tasks are:\n- Design an explicit sequence of $n$ distinct insertions that, immediately after the $n$-th insertion and its concluding splay, yields a splay tree whose underlying BST shape is a simple path (a degenerate tree, structurally identical to a linked list).\n- Justify that your construction indeed yields such a degenerate shape.\n- Using the cost model above, compute the exact cost of a single access to the minimum key performed immediately after the $n$-th insertion completes. Express your final answer as a closed-form function of $n$.\n\nYour final answer must be a single closed-form expression in $n$. No rounding is required. Do not include units in your final answer.", "solution": "The user requests a three-part solution concerning splay trees: 1) an insertion sequence of $n$ keys that results in a degenerate tree, 2) a justification for this sequence, and 3) the cost of accessing the minimum key in the resulting tree. The cost is defined by a specific model.\n\nFirst, we design the sequence of insertions. We propose inserting the keys $1, 2, 3, \\ldots, n$ in a strictly increasing order into an initially empty splay tree. We will now justify that this sequence produces a tree structure that is a simple path. We use proof by mathematical induction.\n\nLet $P(k)$ be the proposition that after inserting the keys $1, 2, \\ldots, k$ in sequence, the resulting splay tree is a \"left spine,\" where node $k$ is the root, node $k-1$ is the left child of node $k$, and generally, node $i-1$ is the left child of node $i$ for all $i \\in \\{2, \\ldots, k\\}$. This structure is a simple path.\n\n**Base Case:** For $k=1$, we insert the key $1$ into an empty tree. The resulting tree is a single node containing the key $1$. This is a simple path of length $1$, so $P(1)$ holds.\n\n**Inductive Hypothesis:** Assume that for some integer $k \\geq 1$, the proposition $P(k)$ is true. That is, after inserting keys $1, 2, \\ldots, k$ in order, the splay tree is a left spine with root $k$.\n\n**Inductive Step:** We now insert the key $k+1$.\n$1$. **BST Insertion:** The insertion process starts at the root of the current tree, which is node $k$. Since the key to be inserted, $k+1$, is greater than the key at the root, $k$, the algorithm attempts to proceed to the right child. According to the inductive hypothesis, the root $k$ has no right child. Therefore, a new node containing the key $k+1$ is created and becomes the right child of node $k$.\n$2$. **Splaying:** After the insertion, the newly added node, $k+1$, is splayed to the root. At this moment, the node $k+1$ is the right child of the root $k$. Since its parent is the root of the tree, it has no grandparent. This configuration triggers a **zig** operation.\n$3$. **Rotation:** A `zig` operation on a right child consists of a single left rotation around its parent. We perform a left rotation at node $k$. This operation makes $k+1$ the new root of the tree. The former root, $k$, becomes the left child of the new root, $k+1$. The original left subtree of $k$, which by the inductive hypothesis was the left spine $1 \\leftarrow 2 \\leftarrow \\dots \\leftarrow (k-1)$, remains as the left subtree of node $k$.\n$4$. **Resulting Structure:** The final tree has $k+1$ at the root. Its left child is $k$, whose left child is $k-1$, and so on, down to the leaf node $1$. This configuration is a left spine, $1 \\leftarrow 2 \\leftarrow \\dots \\leftarrow (k+1)$. Thus, the proposition $P(k+1)$ holds.\n\nBy the principle of mathematical induction, inserting the keys $1, 2, \\ldots, n$ in increasing order results in a splay tree that is a simple path (a left spine) with root $n$.\n\nNext, we compute the cost of a single access to the minimum key, which is $1$, performed on this tree. The cost is the sum of the number of key comparisons during the search and the number of rotations during the subsequent splay.\n\n**1. Number of Key Comparisons:**\nThe tree is a left spine with root $n$. To locate the key $1$, the search algorithm starts at the root $n$ and follows the chain of left-child pointers until it reaches node $1$. The search path is $n \\rightarrow (n-1) \\rightarrow (n-2) \\rightarrow \\dots \\rightarrow 2 \\rightarrow 1$. A key comparison is performed at each of these $n$ nodes.\nTherefore, the number of key comparisons is $n$.\n\n**2. Number of Rotations:**\nAfter finding node $1$, it is splayed to the root. Initially, node $1$ is at a depth of $n-1$. The path from node $1$ to the root consists entirely of \"left-child of\" relationships. The splaying process involves a sequence of steps moving node $1$ towards the root.\nLet $x$ be the node being splayed ($x=1$), $p$ be its parent, and $g$ be its grandparent.\n-   As long as $x$ has a grandparent, the configuration will always be that $x$ is the left child of $p$, and $p$ is the left child of $g$. This is the **zig-zig** case, which involves two rotations. Each `zig-zig` step moves $x$ two levels closer to the root. This is repeated $\\lfloor \\frac{n-1}{2} \\rfloor$ times.\n-   After the series of `zig-zig` steps, node $1$ will either be at the root (if $n-1$ is even) or it will be a direct child of the root (if $n-1$ is odd). If it is a child of the root, its parent is the root, so one final **zig** step is performed, involving a single rotation.\n\nThe total number of rotations is the sum of rotations from all `zig-zig` and potential `zig` steps:\n$$ \\text{Number of rotations} = 2 \\times \\left\\lfloor \\frac{n-1}{2} \\right\\rfloor + ((n-1) \\pmod 2) $$\nThis expression is a standard identity that simplifies to $n-1$ for any integer $n \\geq 1$.\n-   If $n-1$ is even, let $n-1 = 2k$ for some integer $k$. The number of rotations is $2 \\times k + 0 = 2k = n-1$.\n-   If $n-1$ is odd, let $n-1 = 2k+1$ for some integer $k$. The number of rotations is $2 \\times k + 1 = 2k+1 = n-1$.\nThus, the total number of rotations performed during the splay is exactly $n-1$.\n\n**3. Total Cost:**\nThe total cost of the access is the sum of the number of key comparisons and the number of rotations.\n$$ \\text{Cost} = (\\text{Number of Comparisons}) + (\\text{Number of Rotations}) $$\n$$ \\text{Cost} = n + (n-1) $$\n$$ \\text{Cost} = 2n - 1 $$\nThis is the required closed-form expression for the cost as a function of $n$.", "answer": "$$\\boxed{2n-1}$$", "id": "3214330"}, {"introduction": "An algorithm's complete story is told by comparing its best, worst, and average-case performance. The Quickselect algorithm provides a classic and illuminating case study, as its worst-case behavior is drastically different from its typical performance. In this comprehensive analysis [@problem_id:3214466], you will derive the performance bounds for all three scenarios from first principles, ultimately explaining through average-case analysis why this algorithm is so fast and widely used in practice despite its potential pitfalls.", "problem": "An array of $n$ distinct keys is given. Consider the Quickselect algorithm to find the $k$-th smallest key, where each partition operation compares the chosen pivot against every other element in the current subarray exactly once, and the pivot for each subproblem is chosen uniformly at random from the elements of the subproblem. Assume the Random Access Machine (RAM) model where counting the number of key comparisons is the cost measure.\n\nStarting from the core definitions of Quickselect and the expectation operator, perform the following:\n\n- Formally define the number of comparisons made in one partition step on a subarray of size $m$.\n- Derive, from first principles, the best-case and worst-case number of comparisons needed by Quickselect to find the $k$-th smallest element in an array of size $n$, expressed as functions of $n$ (you may assume $1 \\leq k \\leq n$ is fixed).\n- Let $A(n)$ denote the expected number of comparisons made by Quickselect to find the $k$-th smallest when $k$ is itself chosen uniformly at random from $\\{1,2,\\dots,n\\}$ and the pivot is chosen uniformly at random at each recursion. Using only fundamental definitions and linearity of expectation, derive a recurrence for $A(n)$ and determine the leading constant $c$ in the linear asymptotic $A(n) \\sim c n$ as $n \\to \\infty$.\n\nReport the value of $c$ as your final answer. No rounding is required and no units should be included in the final answer.", "solution": "Quickselect works by choosing a pivot, partitioning the current subarray around the pivot, and then recursively continuing in the half that can still contain the $k$-th smallest element. We count only key comparisons. The foundational facts we use are: for a subarray of size $m$, partition compares the pivot against each of the other $m-1$ elements exactly once, and linearity of expectation for the average-case analysis.\n\nOne partition step on a subarray of size $m$ makes exactly $m-1$ comparisons. This follows from the definition of the partition routine that scans the subarray, comparing each non-pivot element to the pivot once.\n\nBest-case analysis: On an input of size $n$, the best case occurs when the pivot chosen in the first call has rank exactly $k$. In that case, the algorithm performs a single partition of the whole array, costing $n-1$ comparisons, and halts. Therefore, the best-case number of comparisons is $n-1$.\n\nWorst-case analysis: The worst case occurs when the pivot selection consistently forces the algorithm to recurse on the largest possible subarray, reducing its size by only 1 at each step. This results in partition calls on subarrays of size $n, n-1, \\dots, 2$. The total number of comparisons is the sum of the costs of these partitions:\n$$\n(n-1) + (n-2) + \\dots + 1 = \\frac{n(n-1)}{2}.\n$$\nThus, the worst-case number of comparisons is $\\frac{n(n-1)}{2}$.\n\nAverage-case setup: Let $A(n)$ be the expected number of comparisons to find the $k$-th smallest when $k$ is uniformly random in $\\{1,2,\\dots,n\\}$ and the pivot is uniformly random at each recursion. We derive a recurrence for $A(n)$ using the law of total expectation, conditioning on the pivot’s rank.\n\nLet the pivot’s rank be $r \\in \\{1,2,\\dots,n\\}$, chosen uniformly at random. The top-level partition costs $n-1$ comparisons regardless of $r$ or $k$. If $r=k$, the algorithm stops. If $r<k$, Quickselect recurses on the right subarray of size $n-r$ seeking the $(k-r)$-th smallest in that subarray. If $r>k$, it recurses on the left subarray of size $r-1$ seeking the $k$-th smallest in that subarray. With $k$ uniform in $\\{1,\\dots,n\\}$ and independent of $r$, the conditional probabilities are\n$$\n\\Pr(r<k \\mid r) \\;=\\; \\frac{n-r}{n}, \\qquad\n\\Pr(r>k \\mid r) \\;=\\; \\frac{r-1}{n}, \\qquad\n\\Pr(r=k \\mid r) \\;=\\; \\frac{1}{n}.\n$$\nTherefore, conditioning on $r$ and averaging over $k$,\n$$\n\\mathbb{E}[\\text{recursive cost} \\mid r] \\;=\\; \\frac{n-r}{n}\\,A(n-r) \\;+\\; \\frac{r-1}{n}\\,A(r-1),\n$$\nand averaging uniformly over $r$ yields\n$$\nA(n) \\;=\\; (n-1) \\;+\\; \\frac{1}{n}\\sum_{r=1}^{n} \\left( \\frac{n-r}{n}\\,A(n-r) + \\frac{r-1}{n}\\,A(r-1) \\right).\n$$\nChange variables $m=n-r$ in the first sum and $m=r-1$ in the second sum; both $m$ range from $0$ to $n-1$. We obtain\n$$\nA(n) \\;=\\; (n-1) \\;+\\; \\frac{1}{n}\\left( \\frac{1}{n}\\sum_{m=0}^{n-1} m\\,A(m) \\;+\\; \\frac{1}{n}\\sum_{m=0}^{n-1} m\\,A(m) \\right)\n\\;=\\; (n-1) \\;+\\; \\frac{2}{n^{2}} \\sum_{m=0}^{n-1} m\\,A(m).\n$$\n\nAsymptotic leading constant: Suppose $A(n)$ is asymptotically linear, $A(n) \\sim c n$ as $n \\to \\infty$, for some constant $c>0$. Substitute $A(m) \\sim c m$ into the recurrence and use the well-tested asymptotic for the quadratic sum $\\sum_{m=0}^{n-1} m^{2} = \\frac{(n-1)n(2n-1)}{6} \\sim \\frac{n^{3}}{3}$:\n$$\nA(n) \\;\\sim\\; n \\;+\\; \\frac{2}{n^{2}} \\sum_{m=0}^{n-1} m \\cdot (c m)\n\\;=\\; n \\;+\\; \\frac{2c}{n^{2}} \\sum_{m=0}^{n-1} m^{2}\n\\;\\sim\\; n \\;+\\; \\frac{2c}{n^{2}} \\cdot \\frac{n^{3}}{3}\n\\;=\\; n \\;+\\; \\frac{2c}{3}\\,n.\n$$\nEquating the leading terms with $A(n) \\sim c n$ gives\n$$\nc \\, n \\;=\\; \\left( 1 + \\frac{2c}{3} \\right) n,\n$$\nso\n$$\nc \\;=\\; 1 + \\frac{2c}{3} \\quad \\Longrightarrow \\quad \\frac{c}{3} \\;=\\; 1 \\quad \\Longrightarrow \\quad c \\;=\\; 3.\n$$\n\nThus, the expected number of comparisons satisfies $A(n) \\sim 3n$, and the leading constant is $c=3$.", "answer": "$$\\boxed{3}$$", "id": "3214466"}]}