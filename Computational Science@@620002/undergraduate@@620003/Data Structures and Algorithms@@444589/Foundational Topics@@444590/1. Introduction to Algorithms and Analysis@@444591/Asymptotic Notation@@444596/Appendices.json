{"hands_on_practices": [{"introduction": "Understanding asymptotic notation begins with a firm grasp of its formal definitions. The definitions, particularly the roles and constraints on the constants $c$ and $n_0$, can be subtle and are a common source of error. This first exercise invites you to act as a proof-checker, analyzing a deliberately flawed argument to pinpoint logical fallacies [@problem_id:3210000]. Sharpening your ability to spot these common mistakes is one of the most effective ways to solidify your own understanding of the foundational principles.", "problem": "Consider the claim that the function $f(n) = n$ belongs to the set $\\mathcal{O}(1)$ and the following purported proof.\n\nClaim: $f(n) \\in \\mathcal{O}(1)$.\n\nPurported proof: By the definition, it suffices to find constants $c  0$ and $n_0 \\in \\mathbb{N}$ such that $0 \\le f(n) \\le c \\cdot 1$ for all $n \\ge n_0$. First, for each $n \\ge 1$, pick $c = n$. Then $f(n) = n \\le c \\cdot 1$ holds, so the required inequality is satisfied. Second, observe that $\\lim_{n \\to \\infty} \\frac{f(n)}{1} = \\lim_{n \\to \\infty} n$ “exists,” so the growth is “well-behaved,” and therefore bounded by a constant in the sense of asymptotic notation. Finally, choose $n_0 = 10$; for all $n \\ge n_0$ we have $n \\le n_0$, so $f(n) \\le n_0 \\cdot 1$. Hence $f(n) \\in \\mathcal{O}(1)$.\n\nSelect all options that correctly identify logical errors in the purported proof.\n\nA. The argument allows the constant $c$ to depend on $n$, using $c = n$, which violates the quantifier structure in the definition of $\\mathcal{O}$; the constant must be chosen independently of $n$.\n\nB. The use of the limit $\\lim_{n \\to \\infty} \\frac{f(n)}{1}$ to conclude $\\mathcal{O}(1)$ is invalid; not only is this limit infinite, but existence of a limit does not by itself establish a uniform constant bound required by $\\mathcal{O}(1)$.\n\nC. The step asserting that for $n \\ge n_0$ one has $n \\le n_0$ reverses the inequality and misinterprets the role of $n_0$; $n_0$ is a threshold beyond which a bound must hold, not an upper bound for $n$ itself.\n\nD. The choice $c = n$ in the first step is acceptable because it makes $f(n) \\le c \\cdot 1$ true for each $n$, so the proof correctly satisfies the definition of $\\mathcal{O}(1)$.\n\nE. The proof uses the inequality $f(n) \\le c \\cdot 1$, but the definition of $\\mathcal{O}(1)$ actually requires $f(n) \\ge c \\cdot 1$; the direction of inequality is wrong.\n\nF. The only issue is the specific choice $n_0 = 10$; picking a sufficiently large $n_0$ would make the inequality $n \\le n_0$ true for all $n \\ge n_0$ and thus would fix the proof.", "solution": "The claim that $n \\in \\mathcal{O}(1)$ is false, as an unbounded function cannot be asymptotically bounded by a constant. The proof is therefore flawed. Let's analyze its errors against the formal definition, which requires finding fixed constants $c > 0$ and $n_0$ such that $n \\le c$ for all $n \\ge n_0$.\n\nThe purported proof contains three distinct, flawed arguments:\n1.  It sets $c=n$, violating the rule that $c$ must be a constant independent of $n$.\n2.  It misinterprets the limit test. The fact that $\\lim_{n \\to \\infty} n/1 = \\infty$ proves that $n \\notin \\mathcal{O}(1)$.\n3.  It makes the contradictory claim that for all $n \\ge n_0$, $n \\le n_0$ holds, fundamentally misunderstanding the role of $n_0$.\n\nEvaluating the options based on these fallacies:\n*   **A:** Correct. It identifies that allowing $c$ to depend on $n$ is a violation of the definition.\n*   **B:** Correct. It identifies that the infinite limit is used to draw an incorrect conclusion.\n*   **C:** Correct. It identifies the logical contradiction and misinterpretation of $n_0$.\n*   **D:** Incorrect. It defends the invalid choice of $c=n$.\n*   **E:** Incorrect. It misstates the definition of $\\mathcal{O}$ notation, which uses $\\le$ for an upper bound. The lower bound inequality $\\ge$ is for $\\Omega$ notation.\n*   **F:** Incorrect. It wrongly claims the $n_0$ error is the only one and that it is fixable.\n\nThe options that correctly identify logical errors are A, B, and C.", "answer": "$$\\boxed{ABC}$$", "id": "3210000"}, {"introduction": "With a solid understanding of the definitions, a core application of asymptotic notation is to compare the long-term growth rates of different functions. This skill is essential for algorithm analysis, allowing us to formally state that one algorithm is more efficient than another for large inputs. This practice challenges you to establish a strict growth hierarchy for a set of common functions using little-o notation, which represents a stronger upper bound than Big-O [@problem_id:3209962]. The rigorous justifications required will hone your skills in applying calculus, particularly limits and L'Hôpital's rule, to the analysis of algorithms.", "problem": "Consider the functions $f_1(n) = n \\log n$, $f_2(n) = n^{3/2}$, $f_3(n) = n!$, $f_4(n) = 2^n$, and $f_5(n) = (\\log n)^2$. Using the rigorous definitions of little-$o$ and little-$\\omega$, construct the tightest possible chain of asymptotic relationships among these functions as $n \\to \\infty$. That is, determine a permutation $\\pi$ of $\\{1,2,3,4,5\\}$ such that\n$$\nf_{\\pi(1)}(n) \\in o\\!\\left(f_{\\pi(2)}(n)\\right), \\quad\nf_{\\pi(2)}(n) \\in o\\!\\left(f_{\\pi(3)}(n)\\right), \\quad\nf_{\\pi(3)}(n) \\in o\\!\\left(f_{\\pi(4)}(n)\\right), \\quad\nf_{\\pi(4)}(n) \\in o\\!\\left(f_{\\pi(5)}(n)\\right),\n$$\nand equivalently $f_{\\pi(i+1)}(n) \\in \\omega\\!\\left(f_{\\pi(i)}(n)\\right)$ for each $i \\in \\{1,2,3,4\\}$. Justify every link in the chain from first principles.\n\nLet $p_k$ denote the $k$-th prime number, with $p_1 = 2$, $p_2 = 3$, $p_3 = 5$, $p_4 = 7$, and $p_5 = 11$. After you have determined the correct permutation $\\pi$, compute the exact integer\n$$\nE \\;=\\; \\prod_{i=1}^{5} p_{\\pi(i)}^{\\,i}.\n$$\nExpress the final value as an exact integer with no rounding.", "solution": "The problem requires two main tasks. First, we must establish a strict asymptotic ordering for a given set of five functions using the definition of little-$o$ notation. Second, based on this ordering, we must compute an integer value $E$ derived from a product of prime numbers raised to certain powers.\n\nThe functions to be ordered are:\n$f_1(n) = n \\log n$\n$f_2(n) = n^{3/2}$\n$f_3(n) = n!$\n$f_4(n) = 2^n$\n$f_5(n) = (\\log n)^2$\n\nThe definition of little-$o$ is as follows: a function $g(n)$ is in $o(h(n))$ if for all constants $c  0$, there exists a constant $n_0$ such that $0 \\le g(n)  c \\cdot h(n)$ for all $n  n_0$. For functions that are positive for sufficiently large $n$, this is equivalent to the limit definition:\n$$\ng(n) \\in o(h(n)) \\iff \\lim_{n \\to \\infty} \\frac{g(n)}{h(n)} = 0\n$$\nWe will use this limit-based definition to justify each step of the ordering. The base of the logarithm does not affect the asymptotic ordering, as logarithm bases are related by a constant factor: $\\log_a n = \\frac{\\log_b n}{\\log_b a}$. For the purposes of calculus (specifically, L'Hôpital's Rule), we will use the natural logarithm, denoted as $\\ln(n)$.\n\nThe goal is to find a permutation $\\pi$ of $\\{1,2,3,4,5\\}$ such that $f_{\\pi(1)}(n) \\in o(f_{\\pi(2)}(n))$, $f_{\\pi(2)}(n) \\in o(f_{\\pi(3)}(n))$, and so on.\n\nLet's perform the pairwise comparisons.\n\n1.  Comparison of $f_5(n) = (\\ln n)^2$ and $f_1(n) = n \\ln n$:\n    We evaluate the limit of the ratio:\n    $$\n    \\lim_{n \\to \\infty} \\frac{f_5(n)}{f_1(n)} = \\lim_{n \\to \\infty} \\frac{(\\ln n)^2}{n \\ln n} = \\lim_{n \\to \\infty} \\frac{\\ln n}{n}\n    $$\n    This limit is of the indeterminate form $\\frac{\\infty}{\\infty}$, so we apply L'Hôpital's Rule:\n    $$\n    \\lim_{n \\to \\infty} \\frac{\\ln n}{n} = \\lim_{n \\to \\infty} \\frac{\\frac{d}{dn}(\\ln n)}{\\frac{d}{dn}(n)} = \\lim_{n \\to \\infty} \\frac{1/n}{1} = 0\n    $$\n    Since the limit is $0$, we have $f_5(n) \\in o(f_1(n))$.\n\n2.  Comparison of $f_1(n) = n \\ln n$ and $f_2(n) = n^{3/2}$:\n    We evaluate the limit of the ratio:\n    $$\n    \\lim_{n \\to \\infty} \\frac{f_1(n)}{f_2(n)} = \\lim_{n \\to \\infty} \\frac{n \\ln n}{n^{3/2}} = \\lim_{n \\to \\infty} \\frac{\\ln n}{n^{1/2}}\n    $$\n    Again, we have the form $\\frac{\\infty}{\\infty}$ and apply L'Hôpital's Rule:\n    $$\n    \\lim_{n \\to \\infty} \\frac{\\ln n}{n^{1/2}} = \\lim_{n \\to \\infty} \\frac{\\frac{d}{dn}(\\ln n)}{\\frac{d}{dn}(n^{1/2})} = \\lim_{n \\to \\infty} \\frac{1/n}{\\frac{1}{2}n^{-1/2}} = \\lim_{n \\to \\infty} \\frac{2n^{1/2}}{n} = \\lim_{n \\to \\infty} \\frac{2}{n^{1/2}} = 0\n    $$\n    Thus, $f_1(n) \\in o(f_2(n))$. This establishes that any polylogarithmic factor grows slower than any polynomial factor (where the exponent is positive).\n\n3.  Comparison of $f_2(n) = n^{3/2}$ and $f_4(n) = 2^n$:\n    We evaluate the limit of the ratio:\n    $$\n    \\lim_{n \\to \\infty} \\frac{f_2(n)}{f_4(n)} = \\lim_{n \\to \\infty} \\frac{n^{3/2}}{2^n}\n    $$\n    This is of the form $\\frac{\\infty}{\\infty}$. Applying L'Hôpital's Rule:\n    $$\n    \\lim_{n \\to \\infty} \\frac{n^{3/2}}{2^n} = \\lim_{n \\to \\infty} \\frac{\\frac{3}{2}n^{1/2}}{2^n \\ln 2}\n    $$\n    This is still of the form $\\frac{\\infty}{\\infty}$. Applying the rule again:\n    $$\n    \\lim_{n \\to \\infty} \\frac{\\frac{3}{2} \\cdot \\frac{1}{2} n^{-1/2}}{2^n (\\ln 2)^2} = \\lim_{n \\to \\infty} \\frac{3}{4 n^{1/2} 2^n (\\ln 2)^2} = 0\n    $$\n    The denominator grows to infinity while the numerator is constant. Thus, $f_2(n) \\in o(f_4(n))$. This confirms the general principle that polynomial functions grow slower than exponential functions.\n\n4.  Comparison of $f_4(n) = 2^n$ and $f_3(n) = n!$:\n    We evaluate the limit of the ratio:\n    $$\n    \\lim_{n \\to \\infty} \\frac{f_4(n)}{f_3(n)} = \\lim_{n \\to \\infty} \\frac{2^n}{n!}\n    $$\n    Let's consider the term $a_n = \\frac{2^n}{n!}$. We can write it as:\n    $$\n    a_n = \\frac{2}{1} \\cdot \\frac{2}{2} \\cdot \\frac{2}{3} \\cdot \\frac{2}{4} \\cdots \\frac{2}{n}\n    $$\n    For $n  2$, the terms $\\frac{2}{k}$ for $k=3, \\dots, n$ are all less than or equal to $\\frac{2}{3}$.\n    So for $n \\ge 3$, we have $0  \\frac{2^n}{n!} = \\frac{2^2}{2!} \\cdot \\frac{2}{3} \\cdots \\frac{2}{n} \\le 2 \\cdot \\left(\\frac{2}{3}\\right)^{n-2}$.\n    As $n \\to \\infty$, $\\left(\\frac{2}{3}\\right)^{n-2} \\to 0$. By the squeeze theorem, $\\lim_{n \\to \\infty} \\frac{2^n}{n!} = 0$.\n    Thus, $f_4(n) \\in o(f_3(n))$.\n\nCombining these results, we have the following strict asymptotic ordering:\n$$\nf_5(n) \\in o(f_1(n)), \\quad f_1(n) \\in o(f_2(n)), \\quad f_2(n) \\in o(f_4(n)), \\quad f_4(n) \\in o(f_3(n))\n$$\nThis corresponds to the chain required by the problem: $f_{\\pi(1)}(n), f_{\\pi(2)}(n), f_{\\pi(3)}(n), f_{\\pi(4)}(n), f_{\\pi(5)}(n)$.\nBy matching the functions, we determine the permutation $\\pi$:\n- $f_{\\pi(1)}(n)$ is the slowest growing function, which is $f_5(n)$. So, $\\pi(1) = 5$.\n- $f_{\\pi(2)}(n)$ is the next in the sequence, $f_1(n)$. So, $\\pi(2) = 1$.\n- $f_{\\pi(3)}(n)$ is $f_2(n)$. So, $\\pi(3) = 2$.\n- $f_{\\pi(4)}(n)$ is $f_4(n)$. So, $\\pi(4) = 4$.\n- $f_{\\pi(5)}(n)$ is the fastest growing function, $f_3(n)$. So, $\\pi(5) = 3$.\n\nThe permutation is $\\pi(1)=5$, $\\pi(2)=1$, $\\pi(3)=2$, $\\pi(4)=4$, $\\pi(5)=3$.\n\nNow we compute the value of $E$. The primes given are $p_1 = 2$, $p_2 = 3$, $p_3 = 5$, $p_4 = 7$, and $p_5 = 11$.\nThe formula for $E$ is:\n$$\nE \\;=\\; \\prod_{i=1}^{5} p_{\\pi(i)}^{\\,i} = p_{\\pi(1)}^1 \\cdot p_{\\pi(2)}^2 \\cdot p_{\\pi(3)}^3 \\cdot p_{\\pi(4)}^4 \\cdot p_{\\pi(5)}^5\n$$\nSubstituting the values of $\\pi(i)$:\n$$\nE = p_5^1 \\cdot p_1^2 \\cdot p_2^3 \\cdot p_4^4 \\cdot p_3^5\n$$\nSubstituting the prime number values:\n$$\nE = (11)^1 \\cdot (2)^2 \\cdot (3)^3 \\cdot (7)^4 \\cdot (5)^5\n$$\nNow we calculate the value of each term:\n- $11^1 = 11$\n- $2^2 = 4$\n- $3^3 = 27$\n- $7^4 = (7^2)^2 = 49^2 = 2401$\n- $5^5 = 3125$\n\nWe multiply these values together:\n$$\nE = 11 \\cdot 4 \\cdot 27 \\cdot 2401 \\cdot 3125\n$$\nTo simplify the calculation, we can rearrange the terms. Let's group terms that are easy to multiply.\n$$\nE = (4 \\cdot 3125) \\cdot (11 \\cdot 27) \\cdot 2401\n$$\nCalculating the parentheses:\n- $4 \\cdot 3125 = 12500$\n- $11 \\cdot 27 = 297$\nSo, the expression becomes:\n$$\nE = 12500 \\cdot 297 \\cdot 2401\n$$\nNow multiply $12500$ by $297$:\n$$\n12500 \\cdot 297 = 12500 \\cdot (300 - 3) = 12500 \\cdot 300 - 12500 \\cdot 3 = 3750000 - 37500 = 3712500\n$$\nThe expression is now:\n$$\nE = 3712500 \\cdot 2401\n$$\nThis can be calculated as $3712500 \\cdot (2400 + 1)$:\n$$\nE = 3712500 \\cdot 2400 + 3712500 \\cdot 1\n$$\nLet's compute $37125 \\cdot 24$:\n$37125 \\cdot 24 = 37125 \\cdot (20 + 4) = 742500 + 148500 = 891000$.\nSo, $3712500 \\cdot 2400 = 37125 \\cdot 100 \\cdot 24 \\cdot 100 = 891000 \\cdot 10000 = 8910000000$.\nFinally, we add the remaining part:\n$$\nE = 8910000000 + 3712500 = 8913712500\n$$\nThe exact integer value is $8,913,712,500$.\nLet's re-verify the calculation by a different grouping:\n$E = (2^2 \\cdot 5^5) \\cdot (3^3 \\cdot 7^4) \\cdot 11^1 = (4 \\cdot 3125) \\cdot (27 \\cdot 2401) \\cdot 11$.\n- $4 \\cdot 3125 = 12500$.\n- $27 \\cdot 2401 = 27 \\cdot (2400+1) = 64800 + 27 = 64827$.\n$E = 12500 \\cdot 64827 \\cdot 11$.\n$E = 12500 \\cdot (64827 \\cdot 11) = 12500 \\cdot 713097$.\nTo calculate $125 \\cdot 713097$, we can use the property $125 = 1000/8$:\n$125 \\cdot 713097 = \\frac{713097000}{8} = 89137125$.\nSo, $E = 12500 \\cdot 713097 = 100 \\cdot (125 \\cdot 713097) = 100 \\cdot 89137125 = 8913712500$.\nBoth calculation methods yield the same result, confirming the final answer.\n$$\nE = 8913712500\n$$", "answer": "$$\n\\boxed{8913712500}\n$$", "id": "3209962"}, {"introduction": "Asymptotic notation is a powerful tool for predicting performance as input size $n$ approaches infinity, but what happens for practical, finite values of $n$? The constants and lower-order terms that we ignore in asymptotic analysis can significantly influence an algorithm's real-world performance. This final exercise bridges the gap between theory and practice by calculating the \"crossover point\" for two algorithms [@problem_id:3210023]. This is the specific input size at which an asymptotically superior algorithm overcomes its larger constant factors and becomes genuinely faster, a crucial consideration in practical software engineering.", "problem": "You are comparing two algorithms that solve the same problem on an input of size $n$, with exact runtime models $T_{A}(n) = 500\\,n\\,\\ln n$ and $T_{B}(n) = 0.1\\,n^{2}$, where $\\ln$ denotes the natural logarithm. In asymptotic terms, $T_{A}(n)$ belongs to the class $\\Theta(n \\ln n)$ and $T_{B}(n)$ belongs to the class $\\Theta(n^{2})$. Using the foundational definitions of asymptotic notation and the fact that $n \\ln n \\in o(n^{2})$, there exists a threshold value $n_{0}$ beyond which algorithm $A$ has strictly smaller running time than algorithm $B$. Define the crossover point $n_{0}$ to be the larger positive solution to the equation $T_{A}(n) = T_{B}(n)$, so that for all $n \\ge n_{0}$, algorithm $A$ is faster than algorithm $B$. Compute an approximation to this $n_{0}$. Assume $n$ is treated as a real number for the purpose of approximation. Round your final numeric answer to four significant figures.", "solution": "The crossover point $n_{0}$ is the larger positive solution to the equation $T_{A}(n) = T_{B}(n)$.\n$$500\\,n\\,\\ln n = 0.1\\,n^{2}$$\nSince we are looking for a solution where $n > 0$, we can safely divide both sides by $n$:\n$$500\\,\\ln n = 0.1\\,n$$\nMultiplying by $10$ to simplify the constants gives:\n$$5000\\,\\ln n = n$$\nThis is a transcendental equation, which does not have a closed-form solution using elementary functions. We must use a numerical method to find an approximate solution. Fixed-point iteration is a suitable method. The equation can be written as $n = h(n)$, where $h(n) = 5000\\,\\ln n$. The iterative process is given by:\n$$n_{k+1} = 5000\\,\\ln(n_k)$$\nWe seek the larger of two roots for this equation. For the iteration to converge to this root, the derivative $|h'(n)|$ must be less than 1 near the root. Since $h'(n) = 5000/n$, this condition holds for any $n > 5000$, which will include the larger root. We can start with an initial guess of $n_1 = 10000$.\n\nThe iteration proceeds as follows:\n- $n_1 = 10000$\n- $n_2 = 5000\\,\\ln(10000) \\approx 46051.7$\n- $n_3 = 5000\\,\\ln(46051.7) \\approx 53687.8$\n- $n_4 = 5000\\,\\ln(53687.8) \\approx 54454.2$\n- $n_5 = 5000\\,\\ln(54454.2) \\approx 54525.5$\n- $n_6 = 5000\\,\\ln(54525.5) \\approx 54532.0$\n- $n_7 = 5000\\,\\ln(54532.0) \\approx 54532.5$\n- $n_8 = 5000\\,\\ln(54532.5) \\approx 54532.6$\n\nThe sequence converges to approximately $54532.6$. Rounding this to four significant figures gives $54530$. To express this unambiguously, we use scientific notation: $5.453 \\times 10^{4}$. For all $n$ greater than this value, algorithm A will be faster than algorithm B.", "answer": "$$\n\\boxed{5.453 \\times 10^{4}}\n$$", "id": "3210023"}]}