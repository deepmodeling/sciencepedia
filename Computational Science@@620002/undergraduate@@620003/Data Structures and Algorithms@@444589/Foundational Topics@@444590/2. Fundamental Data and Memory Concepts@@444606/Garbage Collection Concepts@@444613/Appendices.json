{"hands_on_practices": [{"introduction": "Reference counting offers a simple and immediate way to reclaim memory: an object is deleted as soon as no references point to it. However, this elegant approach has a critical weakness—it cannot handle \"islands of isolation,\" or cyclic data structures where objects reference each other but are unreachable from the rest of the program. This exercise challenges you to implement a solution based on the \"trial deletion\" algorithm, using graph reachability to precisely identify and reclaim these otherwise uncollectible cycles [@problem_id:3236414].", "problem": "You are given the conceptual framework of reference counting in automatic memory management and the challenge of collecting cyclic garbage when using reference counts alone. Start from the following foundational base: a heap object graph is modeled as a directed graph $G = (V, E)$ with $|V| = n$, where each vertex $v \\in V$ is an object, and each directed edge $(u,v) \\in E$ is a reference from object $u$ to object $v$. The reference count $rc(v)$ of an object $v$ equals the total number of references to $v$ from all objects plus the number of references from outside the heap (for example, from roots such as stacks or registers), which are modeled as an external-reference vector $ext: V \\to \\mathbb{N}$. A set of candidate objects $S \\subseteq V$ is provided by the mutator as “suspect” objects that may participate in cycles.\n\nUsing only these base notions, derive the semantics of trial deletion restricted to $S$: it must conservatively detect precisely those objects in $S$ that are not reachable from outside $S$. Formally, let $ext\\_in(v)$ denote the number of references into $v$ from outside $S$, given by $ext\\_in(v) = ext(v) + |\\{(u,v) \\in E \\mid u \\notin S\\}|$. Define the base set $B = \\{ v \\in S \\mid ext\\_in(v) > 0 \\}$ and the survivor set $R \\subseteq S$ as all vertices in $S$ reachable from $B$ by following only edges within $S$. The recyclable garbage set is $G_S = S \\setminus R$. Your program must implement these semantics algorithmically and must behave as a cycle detector for a reference counting system that uses trial deletion on the limited candidate set $S$.\n\nInput format and execution model: There is no runtime input. Instead, your program must hard-code a small suite of test cases. Each test case is a quadruple $(n, E, ext, S)$, where $n$ is the number of objects labeled as integers from $0$ to $n-1$, $E$ is a list of ordered pairs $(u,v)$, $ext$ is a list of length $n$ giving $ext(i)$ for $i \\in \\{0,\\dots,n-1\\}$, and $S$ is a list of candidate integers. For each test case, compute the sorted list of object identifiers in $G_S$.\n\nTest suite:\n- Test case $1$: $n = 5$, $E = [(0,1),(1,2),(2,0),(2,3)]$, $ext = [0,0,0,1,0]$, $S = [0,1,2]$.\n- Test case $2$: $n = 3$, $E = [(0,1)]$, $ext = [0,0,0]$, $S = []$.\n- Test case $3$: $n = 3$, $E = [(0,1),(1,2)]$, $ext = [1,0,0]$, $S = [0,1,2]$.\n- Test case $4$: $n = 3$, $E = [(0,1),(1,2),(2,0)]$, $ext = [0,0,0]$, $S = [0,1]$.\n- Test case $5$: $n = 1$, $E = [(0,0)]$, $ext = [0]$, $S = [0]$.\n- Test case $6$: $n = 6$, $E = [(0,1),(1,2),(2,0),(3,4),(4,5),(5,3)]$, $ext = [0,1,0,0,0,0]$, $S = [0,1,2,3,4,5]$.\n\nRequirements:\n- Derive and implement a correct and efficient algorithm consistent with trial deletion semantics limited to $S$ as defined using the foundational notions above. Your program must not rely on any external input.\n- For each test case, output the sorted list of object identifiers that would be reclaimed by trial deletion on $S$.\n- Final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with no whitespace. Each element is the per-test-case sorted list. For example, the final output should look like $[[\\dots],[\\dots],\\dots]$.\n\nAnswer specification:\n- For each test case, the answer is a list of integers (possibly empty).\n- The single-line output must aggregate the lists for all test cases in the order they are presented above and contain no whitespace characters anywhere in the line.", "solution": "The user-provided problem statement has been analyzed and validated against the specified criteria.\n\n### Step 1: Extract Givens\n\n-   **Heap Model**: A directed graph $G = (V, E)$ with $|V| = n$ vertices (objects) and a set of directed edges $E$ (references).\n-   **Object Identifiers**: Integers from $0$ to $n-1$.\n-   **External References**: A vector $ext: V \\to \\mathbb{N}$, where $ext(v)$ is the number of references to object $v$ from outside the heap.\n-   **Candidate Set**: A subset of objects $S \\subseteq V$, suspected to be part of garbage cycles.\n-   **External-in References to S**: For an object $v \\in S$, the number of references from outside $S$ is defined as $ext\\_in(v) = ext(v) + |\\{(u,v) \\in E \\mid u \\notin S\\}|$.\n-   **Base Set**: $B = \\{ v \\in S \\mid ext\\_in(v) > 0 \\}$. These are objects in $S$ that are directly reachable from outside $S$.\n-   **Survivor Set**: $R \\subseteq S$, defined as the set of all vertices in $S$ that are reachable from the base set $B$ by paths consisting entirely of edges within $S$.\n-   **Recyclable Garbage Set**: $G_S = S \\setminus R$.\n-   **Task**: For a given suite of test cases, compute the sorted list of object identifiers in $G_S$.\n-   **Test Suite**:\n    1.  Test case 1: $n = 5$, $E = [(0,1),(1,2),(2,0),(2,3)]$, $ext = [0,0,0,1,0]$, $S = [0,1,2]$.\n    2.  Test case 2: $n = 3$, $E = [(0,1)]$, $ext = [0,0,0]$, $S = []$.\n    3.  Test case 3: $n = 3$, $E = [(0,1),(1,2)]$, $ext = [1,0,0]$, $S = [0,1,2]$.\n    4.  Test case 4: $n = 3$, $E = [(0,1),(1,2),(2,0)]$, $ext = [0,0,0]$, $S = [0,1]$.\n    5.  Test case 5: $n = 1$, $E = [(0,0)]$, $ext = [0]$, $S = [0]$.\n    6.  Test case 6: $n = 6$, $E = [(0,1),(1,2),(2,0),(3,4),(4,5),(5,3)]$, $ext = [0,1,0,0,0,0]$, $S = [0,1,2,3,4,5]$.\n-   **Output Format**: A single-line string `[[...],[...],...]` representing the list of results for all test cases, with no whitespace.\n\n### Step 2: Validate Using Extracted Givens\n\nThe problem is evaluated against the validation criteria:\n\n-   **Scientifically Grounded**: The problem is well-grounded in the field of computer science, specifically in the domain of data structures and algorithms concerning automatic memory management (garbage collection). The graph-based model of object heaps, the concept of reference counting, and the problem of cyclic garbage are fundamental topics in this field. The \"trial deletion\" semantics are rigorously defined and represent a valid approach to cycle detection. The problem is directly relevant to the topic of *garbage collection concepts*.\n-   **Well-Posed**: The problem is mathematically well-posed. All terms ($G, E, S, ext, ext\\_in, B, R, G_S$) are defined unambiguously. The objective—to compute $G_S$—is clear. The process involves standard graph algorithms (reachability), for which unique and stable solutions exist. All necessary data are provided for each test case.\n-   **Objective**: The problem statement uses precise, formal, and objective language, free from any subjectivity or ambiguity.\n\nThe problem does not exhibit any of the listed flaws (Scientific Unsoundness, Non-Formalizable, Incomplete Setup, Unrealistic, Ill-Posed, Pseudo-Profound, Outside Verifiability).\n\n### Step 3: Verdict and Action\n\nThe problem statement is **valid**. A solution will be derived and implemented.\n\n### Derivation of the Algorithm\n\nThe task is to compute the recyclable garbage set $G_S = S \\setminus R$ for each test case. The algorithm directly implements the formal definitions provided. The process is decomposed into three main computational steps.\n\n1.  **Computation of the Base Set $B$**: The base set $B$ consists of all objects in the candidate set $S$ that are reachable from outside $S$. An object is reachable from outside $S$ if it has a non-zero external reference count ($ext(v) > 0$) or if there is at least one reference to it from an object not in $S$. This corresponds to the condition $ext\\_in(v) > 0$.\n\n    To compute $B$, we first must calculate $ext\\_in(v)$ for every $v \\in S$. The algorithm proceeds as follows:\n    - For each object $v \\in S$, initialize a counter for $ext\\_in(v)$ with the value from the external reference vector, $ext(v)$.\n    - Iterate through all edges $(u, w) \\in E$. If the source $u$ is not in $S$ ($u \\notin S$) and the destination $w$ is in $S$ ($w \\in S$), increment the counter for $ext\\_in(w)$.\n    - After evaluating all edges, the base set $B$ is formed by collecting all objects $v \\in S$ for which the final computed $ext\\_in(v)$ is greater than $0$.\n\n2.  **Computation of the Survivor Set $R$**: The survivor set $R$ contains all objects in $S$ that are reachable from the base set $B$ using paths that lie entirely within $S$. This is a classical graph reachability problem on the subgraph induced by $S$. The nodes of this subgraph are the objects in $S$, and its edges are all edges $(u, v) \\in E$ where both $u$ and $v$ are in $S$.\n\n    An efficient algorithm to find all reachable nodes from a starting set of nodes ($B$) is the Breadth-First Search (BFS). The algorithm proceeds as follows:\n    - Initialize the survivor set $R$ as a copy of the base set $B$.\n    - Initialize a queue for the traversal with all elements of $B$.\n    - While the queue is not empty:\n        - Dequeue an object $u$.\n        - For each object $v$ that $u$ references (i.e., for each edge $(u, v) \\in E$):\n            - If $v$ is in the candidate set $S$ and has not yet been added to the survivor set $R$, add $v$ to $R$ and enqueue it.\n    - Upon termination of the BFS, the set $R$ contains precisely all objects in $S$ reachable from $B$ through paths within $S$.\n\n3.  **Computation of the Recyclable Garbage Set $G_S$**: The problem defines the recyclable garbage set as $G_S = S \\setminus R$. This is the set of objects that are in the candidate set $S$ but are not in the survivor set $R$. These are the objects in $S$ that are not reachable from any external source, either directly or indirectly through a path of other survivor objects.\n\n    This final step is a simple set difference operation. The resulting set of object identifiers is then sorted in ascending order as required by the problem specification.\n\nThis three-step procedure correctly and efficiently implements the specified semantics of trial deletion on the candidate set $S$. The implementation will apply this logic to each of the provided test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the cyclic garbage collection problem based on trial deletion\n    semantics for a predefined suite of test cases.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1: A simple cycle [0,1,2] that is garbage.\n        {\n            \"n\": 5,\n            \"E\": [(0, 1), (1, 2), (2, 0), (2, 3)],\n            \"ext\": [0, 0, 0, 1, 0],\n            \"S\": [0, 1, 2]\n        },\n        # Test case 2: Empty candidate set.\n        {\n            \"n\": 3,\n            \"E\": [(0, 1)],\n            \"ext\": [0, 0, 0],\n            \"S\": []\n        },\n        # Test case 3: A chain kept alive by an external reference.\n        {\n            \"n\": 3,\n            \"E\": [(0, 1), (1, 2)],\n            \"ext\": [1, 0, 0],\n            \"S\": [0, 1, 2]\n        },\n        # Test case 4: A cycle where S is a subset of the cycle, and an\n        #              external reference (from node 2) saves the S-subset.\n        {\n            \"n\": 3,\n            \"E\": [(0, 1), (1, 2), (2, 0)],\n            \"ext\": [0, 0, 0],\n            \"S\": [0, 1]\n        },\n        # Test case 5: A single-node cycle with no external refs.\n        {\n            \"n\": 1,\n            \"E\": [(0, 0)],\n            \"ext\": [0],\n            \"S\": [0]\n        },\n        # Test case 6: Two disjoint cycles. One is saved by an ext ref, one is not.\n        {\n            \"n\": 6,\n            \"E\": [(0, 1), (1, 2), (2, 0), (3, 4), (4, 5), (5, 3)],\n            \"ext\": [0, 1, 0, 0, 0, 0],\n            \"S\": [0, 1, 2, 3, 4, 5]\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        n, E, ext, S = case[\"n\"], case[\"E\"], case[\"ext\"], case[\"S\"]\n        \n        # For efficient lookups, convert S to a set.\n        S_set = set(S)\n        if not S_set:\n            results.append([])\n            continue\n\n        # Step 1: Compute the Base Set B\n        # B = { v in S | ext_in(v) > 0 }\n        # where ext_in(v) = ext(v) + |{(u,v) in E | u not in S}|\n        \n        ext_in = {v: ext[v] for v in S_set}\n        \n        # To find references from outside S, we can build a predecessor graph\n        # or iterate through all edges. Iterating through edges is straightforward.\n        for u, v in E:\n            if v in S_set and u not in S_set:\n                ext_in[v] += 1\n        \n        B = {v for v, count in ext_in.items() if count > 0}\n        \n        # Step 2: Compute the Survivor Set R\n        # R is the set of all vertices in S reachable from B using only edges within S.\n        # This is a graph traversal (BFS) starting from all nodes in B.\n        \n        adj = {i: [] for i in range(n)}\n        for u, v in E:\n            adj[u].append(v)\n            \n        R = set(B)\n        queue = list(B)\n        head = 0\n        \n        while head < len(queue):\n            u = queue[head]\n            head += 1\n            \n            for v in adj[u]:\n                # Traversal is restricted to paths within S\n                if v in S_set and v not in R:\n                    R.add(v)\n                    queue.append(v)\n\n        # Step 3: Compute the Recyclable Garbage Set G_S = S \\ R\n        G_S = S_set - R\n        \n        # Sort the results as required by the problem statement.\n        sorted_garbage = sorted(list(G_S))\n        results.append(sorted_garbage)\n\n    # Final print statement in the exact required format.\n    # The string representation of a list of lists, with no whitespace.\n    print(str(results).replace(' ', ''))\n\nsolve()\n```", "id": "3236414"}, {"introduction": "While tracing garbage collectors like mark-and-sweep can correctly identify all unreachable objects, they introduce a different challenge: memory fragmentation. After reclaiming dead objects, the heap can become a patchwork of small, non-contiguous free blocks, potentially preventing large allocation requests even when total free memory is sufficient. This practice guides you through a detailed simulation to quantitatively compare the impact of `first-fit` versus `best-fit` allocation strategies on external fragmentation, demonstrating how allocator choices and compaction interact to determine heap efficiency [@problem_id:3236476].", "problem": "You must write a complete, runnable program that simulates external fragmentation under two allocation strategies (first-fit and best-fit) within a mark-sweep-compact collection scheme and compares their average external fragmentation on the same workload. The simulation must be designed from fundamental definitions, not by using prepackaged results.\n\nDefinitions and model assumptions:\n- Managed heap and layout. The heap is a one-dimensional array of bytes of total size $H$. A free list $\\mathcal{F}=\\{(s_i,\\ell_i)\\}_{i=1}^m$ represents $m$ free blocks, where $s_i$ is the start address and $\\ell_i$ is the length in bytes, with blocks sorted by address and non-overlapping. The alignment is $a$, and every requested size $r$ is rounded up to the smallest multiple of $a$ greater than or equal to $r$.\n- Allocation strategies. On an allocation request of aligned size $R$, the first-fit strategy selects the first block $(s_i,\\ell_i)$ in address order with $\\ell_i\\ge R$. The best-fit strategy selects the block with minimal $\\ell_i$ such that $\\ell_i\\ge R$; ties are broken by smaller $s_i$. In either case, if $\\ell_i=R$, the block is removed from $\\mathcal{F}$; if $\\ell_i>R$, the block is split into an allocated segment of size $R$ at $s_i$ and a remainder free block $(s_i+R,\\ell_i-R)$.\n- External fragmentation metric. For a given free list $\\mathcal{F}$, define total free memory $F=\\sum_{i=1}^{m}\\ell_i$ and largest block $L=\\max_{1\\le i\\le m}\\ell_i$ when $m\\ge 1$, and set $F=0$ and $L=0$ when $m=0$. The external fragmentation at that moment is\n$$\nE=\\begin{cases}\n0,& \\text{if }F=0,\\\\\n1-\\dfrac{L}{F},& \\text{if }F>0.\n\\end{cases}\n$$\n- Mark-sweep-compact scheme and collection cadence. Time is divided into $C$ cycles. In each cycle, exactly $A$ allocation attempts are made sequentially. Garbage collection occurs after each cycle (at its end). A compaction is applied to that cycle if and only if the cycle index is a multiple of $P$. At the end of a cycle without compaction, the scheme performs mark-sweep without relocating objects; freed spaces are coalesced with adjacent free regions to form the new $\\mathcal{F}$ as the complement of live segments in $[0,H)$. At the end of a cycle with compaction, all live objects are relocated contiguously starting at address $0$ in their address-order, and a single free block $(U,H-U)$ is created where $U$ is the total size of live objects after compaction.\n- Liveness. Each allocated object independently has a lifetime measured in the number of future collections it will survive. Let $d$ be the per-collection death probability and $s=1-d$ the per-collection survival probability. For each newly allocated object, draw $S\\sim\\mathrm{Geometric}(d)$ on support $\\{1,2,\\dots\\}$ and set its number of survivals to $S-1\\in\\{0,1,2,\\dots\\}$. At each collection, an object with current survival counter $k$ dies if $k=0$ and otherwise updates its counter to $k-1$.\n- Allocation failure handling. During a cycle, if an allocation request cannot be satisfied by the current free list $\\mathcal{F}$, the request is dropped (not allocated) and the simulation proceeds to the next request. There are no collections triggered within a cycle other than the end-of-cycle collection.\n- Measurement protocol. Immediately before each allocation attempt, compute the current external fragmentation $E$ from $\\mathcal{F}$ and add it to a running list for that run. The average external fragmentation is the arithmetic mean $\\bar{E}=\\dfrac{1}{T}\\sum_{t=1}^{T} E_t$ where $T=C\\cdot A$ is the total number of allocation attempts, and $E_t$ is computed immediately before the $t$-th allocation attempt of the run. The reported numbers are $\\bar{E}$ for first-fit and best-fit separately, measured on the same workload with the same randomness.\n- Randomness and reproducibility. For a given test case, the sizes of requests and the lifetimes of requests are drawn with a specified seed and reused identically for both strategies to ensure a fair comparison. Request sizes are drawn uniformly at random as integers in $[r_{\\min},r_{\\max}]$ and then aligned up to $a$.\n\nYour task:\n- Implement a simulator following the above, with two allocation strategies: first-fit and best-fit. For each provided test case, generate the sequence of $C\\cdot A$ request sizes and lifetimes using the provided seed, then compute the average external fragmentation $\\bar{E}$ for first-fit and best-fit.\n- For each test case, output a list with two floats $[\\bar{E}_{\\mathrm{first}},\\bar{E}_{\\mathrm{best}}]$ in this exact order. All floats must be rounded to $6$ decimal places using standard rounding to nearest, with ties to away from zero acceptable as in typical floating-point formatting.\n\nTest suite:\nUse the following parameter sets, each completely specified by a tuple $(H,C,P,A,r_{\\min},r_{\\max},a,d,\\text{seed})$.\n- Test $1$: $(H,C,P,A,r_{\\min},r_{\\max},a,d,\\text{seed})=(300000,20,5,800,8,128,8,0.3,42)$.\n- Test $2$: $(H,C,P,A,r_{\\min},r_{\\max},a,d,\\text{seed})=(300000,10,1,800,8,128,8,0.4,12345)$.\n- Test $3$: $(H,C,P,A,r_{\\min},r_{\\max},a,d,\\text{seed})=(250000,25,7,600,8,128,8,0.2,2024)$.\n- Test $4$: $(H,C,P,A,r_{\\min},r_{\\max},a,d,\\text{seed})=(50000,10,3,500,8,64,8,1.0,777)$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element of this top-level list corresponds to one test case, in the same order as listed above, and is itself a two-element list $[\\bar{E}_{\\mathrm{first}},\\bar{E}_{\\mathrm{best}}]$. For example, a valid output would look like\n\"[ [0.012345,0.009876],[0.000000,0.000000],[0.023456,0.019999],[0.000000,0.000000] ]\"\nbut with the actual numbers determined by your simulation and with no spaces unless your programming language inserts them; your actual program must print exactly one line in the specified format.", "solution": "The problem requires the implementation of a discrete-time simulation of a memory-managed heap to compare the external fragmentation produced by first-fit and best-fit allocation strategies. The simulation must adhere to a specific set of rules for memory layout, allocation, garbage collection (including periodic compaction), object liveness, and measurement.\n\nThe model is based on established principles of memory management in computer science. The heap is a contiguous block of memory of size $H$. The state of the heap at any time is represented by the set of allocated objects and the set of free blocks. For this simulation, these are maintained as two separate lists, both kept sorted by starting memory address.\n\n- The list of allocated objects contains tuples of the form $(s, \\ell, k)$, representing an object at starting address $s$ with length $\\ell$ and a survival counter $k$.\n- The free list, denoted $\\mathcal{F}$, contains tuples $(s_i, \\ell_i)$, representing a free block at address $s_i$ with length $\\ell_i$.\n\nMaintaining these lists in sorted order is crucial. It ensures that the first-fit strategy (which selects the first available block in address order) can be implemented by a simple linear scan. It also correctly handles the tie-breaking rule for the best-fit strategy (choosing the block with the lower start address among those with the minimum suitable size). Furthermore, sorted lists of live objects are essential for both the sweep and compaction phases of garbage collection. The Python `bisect` module is used to efficiently maintain this sorted order during insertion operations.\n\nThe simulation proceeds over $C$ cycles, with each cycle comprising $A$ allocation attempts. The workload, consisting of $C \\times A$ pairs of (request size, lifetime), is pre-generated using a specified random seed to ensure both allocation strategies are tested against the identical sequence of requests, enabling a fair comparison.\n- **Request Generation**: Raw request sizes $r$ are drawn uniformly from $[r_{\\min}, r_{\\max}]$. Each size is then aligned up to the nearest multiple of the alignment value $a$, yielding the actual requested size $R$. This is calculated as $R = \\lceil r/a \\rceil \\times a$.\n- **Liveness Model**: Object lifetimes are determined by a per-collection death probability $d$. The number of collections an object survives, $S$, follows a Geometric distribution, $S \\sim \\mathrm{Geometric}(d)$ on support {$1, 2, \\dots$}. A newly allocated object is assigned a survival counter of $S-1$.\n\nFor each of the $T = C \\times A$ total allocation attempts, the simulation follows these steps:\n$1$. **Measure Fragmentation**: Immediately before an allocation attempt, the external fragmentation $E$ is calculated from the current free list $\\mathcal{F}$. Using the total free memory $F = \\sum \\ell_i$ and the largest free block size $L = \\max \\ell_i$, the fragmentation is $E = 1 - L/F$ for $F>0$, and $E=0$ for $F=0$. This value is recorded.\n$2$. **Allocation**: An aligned request of size $R$ is processed.\n   - **First-Fit**: The free list $\\mathcal{F}$ is scanned in address order, and the first block $(s_i, \\ell_i)$ with $\\ell_i \\ge R$ is chosen.\n   - **Best-Fit**: The free list $\\mathcal{F}$ is scanned completely to find a block that satisfies $\\ell_i \\ge R$ and minimizes $\\ell_i$.\nIf a suitable block is found, it is used for allocation. If its size $\\ell_i$ is greater than $R$, the block is split, creating a new allocated object of size $R$ and a smaller free block of size $\\ell_i - R$. If $\\ell_i = R$, the entire block is used. If no suitable block is found, the allocation request fails and is dropped.\n\n$3$. **Garbage Collection**: After every $A$ allocations (at the end of each cycle), a garbage collection is performed.\n   - **Liveness Update**: For each allocated object with survival counter $k$, if $k=0$, the object is marked as dead. If $k>0$, its counter is decremented to $k-1$.\n   - **Compaction**: If the cycle index is a multiple of the compaction period $P$, a compaction phase is executed. All live objects are relocated to the beginning of the heap, starting at address $0$, in their existing relative address order. This eliminates all fragmentation, resulting in a single contiguous block of live data followed by a single large free block.\n   - **Sweep (No Compaction)**: If it is not a compaction cycle, a sweep operation is performed. The memory occupied by dead objects is reclaimed. A new free list $\\mathcal{F}$ is constructed by identifying the gaps between the remaining (sorted) live objects and the heap boundaries. This process implicitly coalesces adjacent free spaces.\n\nThe final reported value for each strategy is the arithmetic mean $\\bar{E}$ of all the fragmentation values recorded just before each of the $T$ allocation attempts.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport bisect\n\nclass MemorySimulator:\n    \"\"\"\n    Simulates heap memory management with first-fit or best-fit allocation,\n    and a mark-sweep-compact garbage collector.\n    \"\"\"\n    def __init__(self, params, workload, strategy):\n        \"\"\"\n        Initializes the simulator with given parameters.\n        \n        Args:\n            params (tuple): Simulation parameters (H, C, P, A, ...).\n            workload (list): A list of (request_size, survival_counter) tuples.\n            strategy (str): Allocation strategy, 'first_fit' or 'best_fit'.\n        \"\"\"\n        (self.H, self.C, self.P, self.A, \n         self.r_min, self.r_max, self.a, self.d, self.seed) = params\n        self.workload = workload\n        self.strategy = strategy\n        \n        # State variables\n        self.allocated_objects = []  # List of (start, size, survival_counter)\n        self.free_list = [(0, self.H)] # List of (start, size)\n        self.fragmentation_scores = []\n\n    def run_simulation(self):\n        \"\"\"\n        Executes the full simulation over all cycles and allocation attempts.\n        \n        Returns:\n            float: The average external fragmentation over the simulation run.\n        \"\"\"\n        workload_idx = 0\n        for c in range(1, self.C + 1):\n            for _ in range(self.A):\n                self._measure_fragmentation()\n                \n                request_size, survival_counter = self.workload[workload_idx]\n                self._allocate(request_size, survival_counter)\n                workload_idx += 1\n                \n            self._collect_garbage(c)\n        \n        if not self.fragmentation_scores:\n            return 0.0\n        return np.mean(self.fragmentation_scores)\n\n    def _measure_fragmentation(self):\n        \"\"\"\n        Calculates and records the current external fragmentation.\n        \"\"\"\n        if not self.free_list:\n            total_free, largest_block = 0, 0\n        else:\n            free_block_sizes = [block[1] for block in self.free_list]\n            total_free = sum(free_block_sizes)\n            largest_block = max(free_block_sizes)\n        \n        fragmentation = 0.0 if total_free == 0 else 1.0 - (largest_block / total_free)\n        self.fragmentation_scores.append(fragmentation)\n\n    def _find_block_idx(self, size):\n        \"\"\"Finds a suitable free block index based on the allocation strategy.\"\"\"\n        if self.strategy == 'first_fit':\n            for i, (_, length) in enumerate(self.free_list):\n                if length >= size:\n                    return i\n            return None\n        \n        elif self.strategy == 'best_fit':\n            best_idx = -1\n            min_size = float('inf')\n            # The free list is sorted by address, so the first match in case of a\n            # size tie will have the smallest start address, satisfying the\n            # tie-breaking rule.\n            for i, (_, length) in enumerate(self.free_list):\n                if length >= size:\n                    if length < min_size:\n                        min_size = length\n                        best_idx = i\n            return best_idx if best_idx != -1 else None\n\n    def _allocate(self, size, survival_counter):\n        \"\"\"Attempts to allocate a block of memory of a given size.\"\"\"\n        block_idx = self._find_block_idx(size)\n        if block_idx is not None:\n            start, length = self.free_list.pop(block_idx)\n            \n            # Add to allocated list, keeping it sorted by start address.\n            new_obj = (start, size, survival_counter)\n            bisect.insort_left(self.allocated_objects, new_obj)\n            \n            # If the block was larger than needed, return remainder to free list.\n            if length > size:\n                new_free_block = (start + size, length - size)\n                bisect.insort_left(self.free_list, new_free_block)\n\n    def _collect_garbage(self, cycle_num):\n        \"\"\"Performs garbage collection at the end of a cycle.\"\"\"\n        # Identify live objects and decrement their survival counters.\n        live_objects = []\n        for obj in self.allocated_objects:\n            if obj[2] > 0:  # Object survives if counter > 0\n                live_objects.append((obj[0], obj[1], obj[2] - 1))\n        \n        self.allocated_objects = live_objects\n\n        # Perform compaction or sweep.\n        if cycle_num % self.P == 0:\n            # Compaction: Relocate all live objects to the start of the heap.\n            current_addr = 0\n            compacted_objects = []\n            for _, size, counter in self.allocated_objects:\n                compacted_objects.append((current_addr, size, counter))\n                current_addr += size\n            self.allocated_objects = compacted_objects\n            self.free_list = [] if current_addr >= self.H else [(current_addr, self.H - current_addr)]\n        else:\n            # Sweep: Rebuild free list from gaps between live objects.\n            self.free_list = []\n            last_addr = 0\n            for start, size, _ in self.allocated_objects:\n                if start > last_addr:\n                    self.free_list.append((last_addr, start - last_addr))\n                last_addr = start + size\n            if last_addr < self.H:\n                self.free_list.append((last_addr, self.H - last_addr))\n                \ndef solve():\n    \"\"\"\n    Main function to run simulations for all test cases and print results.\n    \"\"\"\n    test_cases = [\n        (300000, 20, 5, 800, 8, 128, 8, 0.3, 42),\n        (300000, 10, 1, 800, 8, 128, 8, 0.4, 12345),\n        (250000, 25, 7, 600, 8, 128, 8, 0.2, 2024),\n        (50000, 10, 3, 500, 8, 64, 8, 1.0, 777),\n    ]\n\n    all_results = []\n    for params in test_cases:\n        H, C, P, A, r_min, r_max, a, d, seed = params\n        \n        # 1. Generate a single, reproducible workload for the test case.\n        rng = np.random.default_rng(seed)\n        total_attempts = C * A\n        \n        request_sizes = rng.integers(r_min, r_max + 1, size=total_attempts)\n        aligned_sizes = np.ceil(request_sizes / a) * a\n        aligned_sizes = aligned_sizes.astype(int)\n        \n        # S ~ Geometric(d) on {1, 2, ...}, survival counter is S-1\n        survival_draws = rng.geometric(p=d, size=total_attempts)\n        survival_counters = survival_draws - 1\n\n        workload = list(zip(aligned_sizes, survival_counters))\n\n        # 2. Run simulation for First-Fit\n        sim_ff = MemorySimulator(params, workload, 'first_fit')\n        avg_frag_ff = sim_ff.run_simulation()\n\n        # 3. Run simulation for Best-Fit using the same workload\n        sim_bf = MemorySimulator(params, workload, 'best_fit')\n        avg_frag_bf = sim_bf.run_simulation()\n\n        all_results.append([avg_frag_ff, avg_frag_bf])\n\n    # 4. Format and print the final output string exactly as required.\n    result_strings = [f\"[{ff:.6f},{bf:.6f}]\" for ff, bf in all_results]\n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n```", "id": "3236476"}, {"introduction": "Scanning the entire heap for live objects during every collection cycle can be prohibitively expensive. Modern garbage collectors employ a powerful optimization based on an empirical observation known as the \"weak generational hypothesis\": most objects die young. This hands-on simulation explores the mechanics of a two-generation collector and asks you to design a workload that intentionally violates this hypothesis [@problem_id:3236439]. By observing the resulting inefficiency, you will gain a concrete understanding of why generational collection is a cornerstone of high-performance automatic memory management.", "problem": "You must write a complete, runnable program that simulates a simplified two-generation Garbage Collector (GC) and evaluates a concrete allocation pattern that intentionally violates the weak generational hypothesis. The weak generational hypothesis asserts that most objects die young; the violating pattern instead produces many medium-lived objects that survive enough minor collections to be promoted, and then die soon after in the old generation, thereby forcing inefficient promotions and costly major collections.\n\nStart from the following foundational base:\n- Garbage Collector (GC) terminology and core definitions:\n  - A generational GC partitions heap objects into disjoint generations. The young generation is collected frequently (minor collections), the old generation less frequently (major collections).\n  - Promotion occurs when an object survives a prescribed number of minor collections.\n  - A collection scans a set of objects to identify which are dead (unreachable) and reclaims them.\n- The weak generational hypothesis is an empirical observation that most objects die young.\n- Mark-and-sweep collection scans and removes dead objects without moving survivors; in this problem, objects are abstract and reachability is modeled deterministically by a lifetime counter, not by graph traversal.\n\nModel to implement:\n- Heap model:\n  - Two generations: young and old.\n  - Capacities: young capacity $C_y$ and old capacity $C_o$ measured in object counts.\n- Object model:\n  - Each object has:\n    - Remaining lifetime $L \\in \\mathbb{Z}_{\\ge 0}$ in time steps. An object is considered dead when $L \\le 0$.\n    - Minor-survival age $a \\in \\mathbb{Z}_{\\ge 0}$, the number of minor collections it has survived while resident in the young generation.\n    - A generation tag, either young or old.\n- Allocation pattern:\n  - At each discrete time step $t \\in \\{1,2,\\dots,T\\}$, exactly $A$ new objects are allocated into the young generation.\n  - The $i$-th object allocated in a test case receives a deterministic lifetime assigned by cycling through the inclusive integer range $[L_{\\min}, L_{\\max}]$: the $k$-th allocation in the whole run is assigned\n    $$L = L_{\\min} + \\left((k - 1) \\bmod (L_{\\max} - L_{\\min} + 1)\\right).$$\n  - This cycling removes randomness and makes the simulation deterministic.\n- Time and GC events per step (strict order):\n  1. Allocate $A$ new young objects, as above.\n  2. While the number of young objects exceeds $C_y$, run a minor collection:\n     - Scan all current young objects (costed, see below).\n     - Remove dead young objects (those with $L \\le 0$).\n     - For each remaining young object, increment its minor-survival age $a \\leftarrow a + 1$.\n     - If $a \\ge p$ (promotion threshold), promote the object to the old generation and reset its age bookkeeping (you may discard $a$ after promotion).\n     - Count every promotion.\n  3. Decrement $L \\leftarrow L - 1$ for every object in both generations (this models the passage of one time step).\n  4. While the number of old objects exceeds $C_o$, run a major collection:\n     - Scan all objects in both generations (costed, see below).\n     - Remove dead old objects (those with $L \\le 0$).\n     - Young objects are not promoted by major collections in this model.\n- Cost model:\n  - Assign scalar costs to approximate work:\n    - Minor collection cost is $\\alpha$ times the number of young objects scanned at the start of the collection.\n    - Major collection cost is $\\beta$ times the total number of objects scanned across both generations at the start of the collection.\n    - Promotion cost is $\\gamma$ per promoted object.\n  - Total GC cost is the sum of all scan costs and promotion costs over the entire run.\n- Inefficiency index:\n  - Define the inefficiency index as\n    $$I = \\frac{\\text{total GC cost}}{\\text{total allocations}} = \\frac{\\text{total GC cost}}{T \\cdot A}.$$\n\nConcrete pattern that violates the weak generational hypothesis:\n- Choose lifetimes so that most objects live just long enough to survive enough minor collections to be promoted (by meeting or exceeding the promotion threshold $p$), but then die shortly after in the old generation. This creates frequent promotions and costly major collections because many promoted objects do not remain long-lived.\n- The frequency of minor collections is governed by load; if the young generation fills after approximately $s = \\left\\lceil \\frac{C_y}{A} \\right\\rceil$ steps, then setting lifetimes near $p \\cdot s$ ensures many objects will be promoted and soon die. In the test suite below, the provided $(L_{\\min}, L_{\\max})$ values are chosen to concretely instantiate this pattern.\n\nConstants to use across all tests:\n- Minor scan weight $\\alpha = 1.0$.\n- Major scan weight $\\beta = 5.0$.\n- Promotion cost $\\gamma = 0.2$.\n\nYour task:\n- Implement the simulator exactly as specified.\n- For each test case, compute the inefficiency index $I$ defined above.\n- Your program must execute with no input and must print a single line containing a list of decimal numbers in the specified order.\n\nTest suite:\n- Each test case is given as a tuple $(C_y, C_o, p, A, T, L_{\\min}, L_{\\max})$.\n- Use the following four test cases that exercise different behaviors:\n  1. Happy-path violation of the weak generational hypothesis:\n     - $(C_y, C_o, p, A, T, L_{\\min}, L_{\\max}) = (100, 500, 2, 30, 200, 9, 10)$.\n  2. Conforming workload where most objects die young:\n     - $(C_y, C_o, p, A, T, L_{\\min}, L_{\\max}) = (100, 500, 2, 30, 200, 1, 3)$.\n  3. Boundary case with extremely frequent minor collections and immediate promotions:\n     - $(C_y, C_o, p, A, T, L_{\\min}, L_{\\max}) = (30, 60, 1, 40, 120, 2, 2)$.\n  4. Large old-generation capacity delaying major collections even under violating pattern:\n     - $(C_y, C_o, p, A, T, L_{\\min}, L_{\\max}) = (50, 10000, 2, 25, 300, 5, 6)$.\n\nRequired final output format:\n- Your program should produce a single line of output containing the inefficiency indices for the four test cases as a comma-separated list enclosed in square brackets, in the same order as listed above. For example, the format must look like\n  $[I_1, I_2, I_3, I_4].$\n- Each $I_k$ must be printed as a decimal number. You may format them to a fixed number of fractional digits if you wish, but they must be valid decimal numerals.", "solution": "The problem statement is valid. It presents a well-defined, self-contained, and scientifically grounded simulation task within the domain of computer science, specifically concerning memory management algorithms. The parameters, rules, and objectives are specified with sufficient precision to permit a unique, deterministic solution.\n\n### Step 1: Extraction of Givens\n\n- **Generational GC Model**: A garbage collector (GC) with two generations: young and old.\n- **Heap Capacities**: Young generation capacity $C_y$, old generation capacity $C_o$.\n- **Object State**: Each object possesses a remaining lifetime $L \\in \\mathbb{Z}_{\\ge 0}$, a minor-survival age $a \\in \\mathbb{Z}_{\\ge 0}$, and a generation tag (young or old). An object is dead if $L \\le 0$.\n- **Promotion Threshold**: An object is promoted from the young to the old generation if its minor-survival age $a$ meets or exceeds the threshold $p$, i.e., $a \\ge p$.\n- **Simulation Time**: The simulation runs for $T$ discrete time steps, from $t=1$ to $t=T$.\n- **Allocation Pattern**: At each time step $t$, $A$ new objects are allocated into the young generation.\n- **Lifetime Assignment**: The $k$-th object allocated during the simulation is assigned a lifetime $L = L_{\\min} + \\left((k - 1) \\bmod (L_{\\max} - L_{\\min} + 1)\\right)$.\n- **Order of Operations per Time Step**:\n    1.  Allocate $A$ new objects into the young generation.\n    2.  Execute minor collections in a loop as long as the number of young objects exceeds $C_y$.\n    3.  Decrement the lifetime $L$ of all objects in both generations by $1$.\n    4.  Execute major collections in a loop as long as the number of old objects exceeds $C_o$.\n- **Minor Collection Process**:\n    - The cost incurred is $\\alpha$ times the number of objects in the young generation at the start of the collection.\n    - All dead objects ($L \\le 0$) in the young generation are removed.\n    - The minor-survival age $a$ of each surviving young object is incremented.\n    - Surviving young objects with $a \\ge p$ are promoted to the old generation. The cost of each promotion is $\\gamma$.\n- **Major Collection Process**:\n    - The cost incurred is $\\beta$ times the total number of objects in both generations at the start of the collection.\n    - All dead objects ($L \\le 0$) in the old generation are removed.\n- **Cost Model Constants**:\n    - Minor scan weight: $\\alpha = 1.0$.\n    - Major scan weight: $\\beta = 5.0$.\n    - Promotion cost: $\\gamma = 0.2$.\n- **Inefficiency Index**: The metric for evaluation is $I = \\frac{\\text{total GC cost}}{\\text{total allocations}} = \\frac{\\text{total GC cost}}{T \\cdot A}$.\n- **Test Suite**:\n    1.  $(C_y, C_o, p, A, T, L_{\\min}, L_{\\max}) = (100, 500, 2, 30, 200, 9, 10)$.\n    2.  $(C_y, C_o, p, A, T, L_{\\min}, L_{\\max}) = (100, 500, 2, 30, 200, 1, 3)$.\n    3.  $(C_y, C_o, p, A, T, L_{\\min}, L_{\\max}) = (30, 60, 1, 40, 120, 2, 2)$.\n    4.  $(C_y, C_o, p, A, T, L_{\\min}, L_{\\max}) = (50, 10000, 2, 25, 300, 5, 6)$.\n\n### Step 2: Validation of Givens\n\nThe provided problem statement is valid.\n- **Scientifically Grounded**: The model is a simplified but standard representation of a two-generation garbage collector, a fundamental concept in systems programming and computer science. The weak generational hypothesis is a well-established empirical principle that motivates this design. The problem is a deterministic simulation based on these ideas.\n- **Well-Posed**: All parameters and operational rules are explicitly defined, ensuring that the state of the simulation at each step is unique and computable. The problem is structured to yield a single, meaningful numerical result ($I$) for each test case.\n- **Objective**: The problem uses precise, technical language and avoids subjectivity. All quantities are formally defined.\n\nThe problem formulation exhibits none of the specified flaws (e.g., scientific unsoundness, incompleteness, ambiguity). It is a formal, solvable problem.\n\n### Step 3: Solution Procedure\n\nA deterministic simulation will be implemented according to the specified model. The state of the system at any time is defined by the set of objects in the young and old generations, their respective lifetimes and ages, and the cumulative GC cost.\n\n**State Representation**\nThe heap will be represented by two collections of objects, one for the young generation and one for the old. Each object will be an instance of a class encapsulating its state: its remaining lifetime $L$ and its minor-survival age $a$.\n\n- `young_generation`: A list of objects in the young generation.\n- `old_generation`: A list of objects in the old generation.\n- `total_gc_cost`: A floating-point number tracking the cumulative cost.\n- `total_allocations_count`: An integer $k$ tracking the total number of objects allocated so far, starting from $1$.\n\n**Simulation Algorithm**\nThe simulation proceeds through discrete time steps $t$ from $1$ to $T$.\n\nFor each test case with parameters $(C_y, C_o, p, A, T, L_{\\min}, L_{\\max})$:\n1.  Initialize `young_generation` and `old_generation` to be empty. Initialize `total_gc_cost = 0.0` and `total_allocations_count = 0$.\n2.  Begin the main loop: for $t$ from $1$ to $T$:\n    a.  **Allocation**:\n        - Generate $A$ new objects. For each $j \\in \\{1, \\dots, A\\}$:\n            - Increment `total_allocations_count`. Let the current value be $k$.\n            - Calculate the object lifetime: $L = L_{\\min} + \\left((k - 1) \\bmod (L_{\\max} - L_{\\min} + 1)\\right)$.\n            - Create a new object with this lifetime $L$ and initial age $a=0$.\n            - Add the object to `young_generation`.\n\n    b.  **Minor Collection Phase**:\n        - Loop `while` the number of objects in `young_generation` is greater than $C_y$:\n            i.   Add $\\alpha \\times |\\text{young\\_generation}|$ to `total_gc_cost`.\n            ii.  Create three temporary lists: `survivors`, `promoted`, `next_young_generation`.\n            iii. For each object in `young_generation`:\n                 - If its lifetime $L > 0$, it is a survivor. Increment its age: $a \\leftarrow a + 1$.\n                 - If its new age $a \\ge p$, add it to the `promoted` list.\n                 - Otherwise (if $L > 0$ and $a < p$), add it to the `next_young_generation` list.\n            iv.  Add the `promoted` objects to `old_generation`.\n            v.   Add $\\gamma \\times |\\text{promoted}|$ to `total_gc_cost`.\n            vi.  Replace `young_generation` with `next_young_generation`.\n\n    c.  **Time Advancement**:\n        - For each object in `young_generation`, decrement its lifetime: $L \\leftarrow L - 1$.\n        - For each object in `old_generation`, decrement its lifetime: $L \\leftarrow L - 1$.\n\n    d.  **Major Collection Phase**:\n        - Loop `while` the number of objects in `old_generation` is greater than $C_o$:\n            i.   Add $\\beta \\times (|\\text{young\\_generation}| + |\\text{old\\_generation}|)$ to `total_gc_cost`.\n            ii.  Filter `old_generation` in place, keeping only objects with $L > 0$.\n\n3.  **Final Calculation**:\n    - After the loop over $T$ steps is complete, compute the inefficiency index:\n      $$I = \\frac{\\text{total\\_gc\\_cost}}{T \\cdot A}$$\n    - Store this value.\n\nThis procedure is executed for each of the four test cases provided, and the resulting indices are collected.", "answer": "```python\nimport numpy as np\n\n# Constants from the problem statement\nALPHA = 1.0  # Minor scan weight\nBETA = 5.0   # Major scan weight\nGAMMA = 0.2  # Promotion cost\n\nclass Object:\n    \"\"\"Represents an object in the heap with its state.\"\"\"\n    def __init__(self, lifetime):\n        self.lifetime = lifetime\n        self.age = 0  # Minor-survival age\n\ndef run_simulation(Cy, Co, p, A, T, Lmin, Lmax):\n    \"\"\"\n    Runs a single simulation for a given set of GC parameters.\n    \n    Args:\n        Cy (int): Young generation capacity.\n        Co (int): Old generation capacity.\n        p (int): Promotion threshold (age).\n        A (int): Allocations per time step.\n        T (int): Total time steps for the simulation.\n        Lmin (int): Minimum lifetime for allocated objects.\n        Lmax (int): Maximum lifetime for allocated objects.\n    \n    Returns:\n        float: The calculated inefficiency index.\n    \"\"\"\n    young_gen = []\n    old_gen = []\n    total_gc_cost = 0.0\n    total_allocations_count = 0\n    \n    lifetime_range_size = Lmax - Lmin + 1\n\n    for _ in range(1, T + 1):\n        # 1. Allocate A new objects\n        for _ in range(A):\n            total_allocations_count += 1\n            lifetime_offset = (total_allocations_count - 1) % lifetime_range_size\n            lifetime = Lmin + lifetime_offset\n            new_obj = Object(lifetime)\n            young_gen.append(new_obj)\n\n        # 2. Minor collection phase\n        while len(young_gen) > Cy:\n            # Add cost for scanning young generation\n            total_gc_cost += ALPHA * len(young_gen)\n            \n            survivors = []\n            promoted_count = 0\n            \n            # Identify survivors and objects to be promoted\n            for obj in young_gen:\n                if obj.lifetime > 0:\n                    obj.age += 1\n                    if obj.age >= p:\n                        # This object gets promoted. Resetting its age is not\n                        # strictly necessary as it's no longer used.\n                        old_gen.append(obj)\n                        promoted_count += 1\n                    else:\n                        survivors.append(obj)\n            \n            # Add promotion cost\n            total_gc_cost += GAMMA * promoted_count\n            \n            # Update the young generation\n            young_gen = survivors\n\n        # 3. Decrement lifetime for all objects\n        for obj in young_gen:\n            obj.lifetime -= 1\n        for obj in old_gen:\n            obj.lifetime -= 1\n\n        # 4. Major collection phase\n        while len(old_gen) > Co:\n            # Add cost for scanning both generations\n            total_gc_cost += BETA * (len(young_gen) + len(old_gen))\n            \n            # Reclaim dead objects in the old generation\n            old_gen = [obj for obj in old_gen if obj.lifetime > 0]\n\n    # Calculate the inefficiency index\n    total_allocations = T * A\n    if total_allocations == 0:\n        return 0.0\n    \n    inefficiency_index = total_gc_cost / total_allocations\n    return inefficiency_index\n\ndef solve():\n    \"\"\"\n    Executes the simulation for all test cases and prints the results.\n    \"\"\"\n    test_cases = [\n        # (Cy, Co, p, A, T, Lmin, Lmax)\n        (100, 500, 2, 30, 200, 9, 10),    # 1. Weak generational hypothesis violation\n        (100, 500, 2, 30, 200, 1, 3),     # 2. Conforming workload (objects die young)\n        (30, 60, 1, 40, 120, 2, 2),       # 3. Frequent GCs and immediate promotions\n        (50, 10000, 2, 25, 300, 5, 6),    # 4. Large old gen delaying major collections\n    ]\n\n    results = []\n    for params in test_cases:\n        result = run_simulation(*params)\n        results.append(result)\n\n    # Format output as a comma-separated list in brackets\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3236439"}]}