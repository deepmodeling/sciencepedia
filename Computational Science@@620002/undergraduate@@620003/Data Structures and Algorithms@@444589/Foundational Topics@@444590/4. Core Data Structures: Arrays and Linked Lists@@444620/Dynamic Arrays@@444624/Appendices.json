{"hands_on_practices": [{"introduction": "While we often talk about the *amortized* constant time of appending to a dynamic array, it's insightful to consider the \"worst-case\" journey of a single element. This exercise challenges you to quantify the maximum number of times any individual element might be copied during a series of appends. By analyzing the experience of the very first element added, you'll gain a concrete understanding of how the doubling strategy contains costs and why geometric growth is so effective. [@problem_id:3206830]", "problem": "Consider a dynamic array that initially has capacity $1$ and is empty. The array supports an append operation that places a new element at the end in constant time when unused capacity is available. When an append finds the array full, the array performs a resize: it allocates a new contiguous block whose capacity is exactly double the current capacity, and then copies all current elements from the old block into the new block in order, after which the new element is appended. Assume no deletions occur. Define a “copy” of an element to mean a movement of that element during a resize (the initial placement of an element when it is appended does not count as a copy).\n\nAfter performing exactly $N \\ge 1$ appends, determine, as a closed-form expression in terms of $N$, the maximum possible number of times that any single element has been copied due solely to resizes during this sequence of $N$ appends. Provide your answer as an exact expression; no numerical approximation or rounding is required.", "solution": "The user wants me to solve a problem about dynamic arrays.\n\n### Step 1: Extract Givens\n- Initial array capacity: $1$.\n- Initial array size: $0$.\n- Append operation: Places a new element at the end.\n- Resize condition: Occurs when an append is attempted on a full array.\n- Resize mechanism:\n    1. New capacity is allocated as $2 \\times$ current capacity.\n    2. All existing elements are copied to the new block.\n    3. The new element is then appended.\n- Definition of \"copy\": Movement of an element during a resize. The initial insertion of an element is not a copy.\n- Total operations: Exactly $N$ appends are performed, with $N \\ge 1$.\n- Objective: Determine the maximum possible number of times any single element has been copied.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientific Grounding**: The problem is a standard exercise in the analysis of algorithms, specifically concerning the amortized analysis of a dynamic array (also known as a vector or ArrayList). The model described (doubling capacity on resize) is a classic implementation strategy. The problem is scientifically and mathematically sound.\n- **Well-Posedness**: The problem is well-posed. The initial conditions, the rules for operation, and the quantity to be determined are all specified precisely. A unique, deterministic solution exists.\n- **Objectivity**: The problem statement is objective and uses formal, unambiguous language.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. It is a well-defined question in the field of computer science about data structures and algorithms. I will now proceed with a formal solution.\n\n### Solution Derivation\nThe problem asks for the maximum number of times any single element has been copied after a sequence of $N$ append operations. To maximize the number of copies for an element, we should consider an element that has been present in the array for the longest possible duration, as it would be subject to every resize operation that occurs after its insertion. The first element to be appended, let us call it $E_1$, satisfies this condition. Therefore, the maximum number of copies for any element is the number of times $E_1$ is copied.\n\nLet's analyze the state of the dynamic array, specifically its capacity. The capacity, let's denote it by $C$, changes only during resize operations. According to the problem statement, the initial capacity is $C_0 = 1$. A resize operation doubles the capacity. Thus, the sequence of capacities the array can have are powers of $2$: $1, 2, 4, 8, \\dots, 2^k$ for non-negative integers $k$. A capacity of $C=2^k$ can hold up to $2^k$ elements.\n\nA resize is triggered when an append operation is attempted on an array that is full. That is, if the array has a capacity of $C$ and already contains $C$ elements, the next append causes a resize.\n\nLet us trace the process:\n1.  **Initial State**: Capacity $C=1$, size $S=0$.\n2.  **Append 1**: We append element $E_1$. The array has space, so no resize occurs. The state becomes $C=1$, $S=1$. The number of copies for $E_1$ is $0$.\n3.  **Append 2**: We try to append $E_2$. The array is full ($S=C=1$). A resize is triggered.\n    - A new array of capacity $2 \\times 1 = 2$ is allocated.\n    - Element $E_1$ is copied from the old array to the new one. This is the **first copy** of $E_1$.\n    - Element $E_2$ is appended.\n    - The state becomes $C=2$, $S=2$.\n4.  **Append 3**: We try to append $E_3$. The array is full ($S=C=2$). A resize is triggered.\n    - A new array of capacity $2 \\times 2 = 4$ is allocated.\n    - Elements $E_1$ and $E_2$ are copied. This is the **second copy** of $E_1$.\n    - Element $E_3$ is appended.\n    - The state becomes $C=4$, $S=3$.\n5.  **Append 4**: We append $E_4$. The array is not full ($S=3, C=4$). No resize. State becomes $C=4, S=4$. $E_1$ has still been copied twice.\n6.  **Append 5**: We try to append $E_5$. The array is full ($S=C=4$). A resize is triggered to capacity $2 \\times 4 = 8$. This involves the **third copy** of $E_1$.\n\nFrom this analysis, it is clear that $E_1$ is copied on every resize event that happens after its initial placement. The number of times $E_1$ is copied is therefore equal to the total number of resizes that occur during the $N$ appends.\n\nAfter $N$ appends, the array must hold $N$ elements. The capacity of the array, $C_N$, must be at least $N$. Since capacities are powers of two, $C_N$ must be the smallest power of two that is greater than or equal to $N$. Let this capacity be $C_N = 2^k$.\nThe condition $C_N = 2^k \\ge N$ with $k$ being the smallest such integer implies that the previous capacity, $2^{k-1}$, was insufficient, i.e., $2^{k-1} < N$.\nSo, we have the inequality:\n$$2^{k-1} < N \\le 2^k$$\nThis inequality holds for $N > 1$. If $N=1$, the capacity is $1=2^0$, so $k=0$.\nFor $N > 1$, we can take the logarithm base $2$ of the inequality:\n$$\\log_2(2^{k-1}) < \\log_2(N) \\le \\log_2(2^k)$$\n$$k-1 < \\log_2(N) \\le k$$\nThis relationship is the definition of the ceiling function. Thus, for $N > 1$, the exponent $k$ of the final capacity is given by:\n$$k = \\lceil \\log_2(N) \\rceil$$\nLet's check this for $N=1$. The final capacity is $1 = 2^0$, so $k=0$. The formula gives $\\lceil \\log_2(1) \\rceil = \\lceil 0 \\rceil = 0$. So the formula holds for all $N \\ge 1$.\n\nThe sequence of capacities that the array must adopt to reach a capacity of $2^k$ is $2^0, 2^1, 2^2, \\ldots, 2^k$.\nEach step in this sequence from $2^j$ to $2^{j+1}$ for $j=0, 1, \\ldots, k-1$ corresponds to one resize operation.\nThe first element, $E_1$, is present in the array from the point where the capacity is $C=2^0$. It will be copied in every subsequent resize. The resizes are:\n- from $2^0 \\to 2^1$\n- from $2^1 \\to 2^2$\n- ...\n- from $2^{k-1} \\to 2^k$\n\nThe total number of these resize events is $(k-1) - 0 + 1 = k$.\nTherefore, the maximum number of times any single element is copied is $k$. Substituting the expression for $k$:\n$$\\text{Maximum copies} = \\lceil \\log_2(N) \\rceil$$\nThis expression is a closed form in terms of $N$ and is valid for all $N \\ge 1$.", "answer": "$$\\boxed{\\lceil \\log_2(N) \\rceil}$$", "id": "3206830"}, {"introduction": "Real-world algorithms rarely exist in isolation; they are built upon data structures whose own performance characteristics must be considered. This practice moves beyond simple appends to a more complex scenario: merging two sorted arrays into a new, sorted dynamic array. You will develop a precise cost model that accounts for every comparison, write, allocation, and data movement, forcing a rigorous, quantitative analysis of the entire process. This exercise demonstrates how to integrate the costs of an algorithm with the overhead of the underlying dynamic data structure. [@problem_id:3230162]", "problem": "Consider two sorted arrays implemented as dynamic arrays, denoted $A$ and $B$, whose lengths are $m$ and $n$, respectively, where $m \\geq 1$ and $n \\geq 1$. Both $A$ and $B$ are sorted in nondecreasing order. You will merge $A$ and $B$ into a new dynamic array $D$ by repeatedly selecting the smallest remaining element among the two arrays and appending it to $D$ (a stable merge). The dynamic array $D$ is created with an initial capacity $C_{0} \\in \\mathbb{N}$ and uses geometric resizing with a fixed integer growth factor $\\alpha \\in \\mathbb{N}$, where $\\alpha \\geq 2$. The resizing rule is as follows: whenever appending would cause the number of stored elements in $D$ to exceed its current capacity, $D$ allocates a new buffer of capacity multiplied by $\\alpha$ and moves all currently stored elements into the new buffer.\n\nUse the following additive cost model:\n- Each element-to-element comparison costs $c_{\\mathrm{cmp}}$.\n- Each append of a single element into $D$ (a write into $D$) costs $c_{\\mathrm{write}}$.\n- Each allocation (including the initial allocation of $D$ with capacity $C_{0}$ and every subsequent reallocation) incurs an overhead cost $c_{\\mathrm{alloc}}$.\n- During each reallocation, moving each already-stored element from the old buffer to the new buffer costs $c_{\\mathrm{move}}$ per element.\n\nStarting only from the core definitions of dynamic array resizing and the standard stable merge procedure, derive the exact worst-case total cost of producing the merged, sorted $D$ with $m+n$ elements under this model. Express your final answer as a single closed-form analytic expression in terms of $m$, $n$, $C_{0}$, $\\alpha$, $c_{\\mathrm{cmp}}$, $c_{\\mathrm{write}}$, $c_{\\mathrm{alloc}}$, and $c_{\\mathrm{move}}$. Do not provide an asymptotic bound; provide the exact expression. No rounding is required.", "solution": "The total cost, $T$, of merging the two arrays is the sum of the costs of all operations performed. According to the provided cost model, the total cost can be decomposed into four components: the cost of comparisons ($T_{\\mathrm{cmp}}$), the cost of writes/appends ($T_{\\mathrm{write}}$), the cost of allocations ($T_{\\mathrm{alloc}}$), and the cost of moving elements during reallocations ($T_{\\mathrm{move}}$).\n$$T = T_{\\mathrm{cmp}} + T_{\\mathrm{write}} + T_{\\mathrm{alloc}} + T_{\\mathrm{move}}$$\nWe are asked to find the exact worst-case total cost. Of these components, the write, allocation, and move costs are uniquely determined by the total number of elements $m+n$ and the dynamic array parameters $C_0$ and $\\alpha$. The comparison cost, however, depends on the specific interleaving of element values between arrays $A$ and $B$. Therefore, to find the worst-case total cost, we must determine the maximum possible comparison cost while the other costs remain fixed.\n\nLet $N = m+n$ be the total number of elements in the final merged array $D$.\n\n**1. Comparison Cost ($T_{\\mathrm{cmp}}$)**\nThe standard stable merge algorithm proceeds by repeatedly comparing the elements at the current heads of arrays $A$ and $B$, and appending the smaller of the two to the output array $D$. A comparison is performed for each element appended to $D$, as long as neither $A$ nor $B$ is exhausted. Once one array is exhausted, the remaining elements of the other array are appended to $D$ without any further comparisons.\n\nLet $i$ and $j$ be the indices of the current heads of $A$ and $B$, respectively. The merge loop `while (i < m && j < n)` performs one comparison per iteration. The number of comparisons is maximized when this loop runs for the maximum possible number of iterations. This occurs when the elements of $A$ and $B$ are interleaved in such a way that both arrays remain non-empty for as long as possible. The loop terminates when we have exhausted one array, i.e., when we have performed $m$ appends from $A$ or $n$ appends from $B$.\n\nThe maximum number of comparisons occurs when the final element of one array is selected just before the final element of the other. For example, consider the case where we have appended $m$ elements from $A$ and $n-1$ elements from $B$. This requires $m + (n-1) = m+n-1$ comparisons. The last element, from $B$, is then appended without a comparison, as $A$ is now exhausted. This scenario is always possible if $m \\geq 1$ and $n \\geq 1$. For instance, if all elements in $A$ and the first $n-1$ elements in $B$ are smaller than the last element of $B$. Therefore, the worst-case number of comparisons is $m+n-1$.\n\nThe worst-case comparison cost is:\n$$T_{\\mathrm{cmp, WC}} = (m+n-1) c_{\\mathrm{cmp}}$$\n\n**2. Write Cost ($T_{\\mathrm{write}}$)**\nThe final merged array $D$ will contain all $m+n$ elements from $A$ and $B$. The merge process involves exactly one append operation for each of these elements.\nThe total write cost is:\n$$T_{\\mathrm{write}} = (m+n) c_{\\mathrm{write}}$$\n\n**3. Allocation ($T_{\\mathrm{alloc}}$) and Move ($T_{\\mathrm{move}}$) Costs**\nThese costs are associated with the management of the dynamic array $D$.\nThe array $D$ is created with an initial capacity of $C_0$. This is the first allocation, incurring a cost of $c_{\\mathrm{alloc}}$.\nSubsequent reallocations occur whenever an append operation would cause the number of elements to exceed the current capacity. A resize is triggered when the array contains $k$ elements and its capacity is also $k$. The resize must happen before the $(k+1)$-th element is appended. The new capacity becomes $\\alpha$ times the old capacity.\n\nLet's trace the capacities and resizes. The initial capacity is $C_0$.\nIf $N \\le C_0$, no reallocations are needed. The array is allocated once.\nIf $N > C_0$, reallocations will occur. The sequence of capacities of the array will be $C_0, C_1, C_2, \\ldots$, where $C_i = C_0 \\alpha^i$.\nA reallocation is triggered when the number of elements to be stored is $C_0+1, C_1+1, \\ldots$. This happens when the array is full, i.e., when it holds $C_0, C_1, \\ldots$ elements respectively.\n\nLet $K$ be the number of reallocations (resizing events).\n- If $N \\le C_0$, no reallocations are needed, so $K=0$.\n- If $N > C_0$, reallocations are necessary. Let the sequence of capacities after each resize be $C_1, C_2, \\ldots, C_K$. The capacity just before the final resize was $C_{K-1} = C_0 \\alpha^{K-1}$. This capacity was insufficient for $N$ elements, meaning $C_{K-1} < N$. The final resize creates a new buffer of capacity $C_K = C_0 \\alpha^K$, which must be sufficient, so $C_K \\geq N$.\nThis gives the condition: $C_0 \\alpha^{K-1} < N \\le C_0 \\alpha^K$.\nDividing by $C_0$ (since $C_0 \\ge 1$): $\\alpha^{K-1} < \\frac{N}{C_0} \\le \\alpha^K$.\nTaking the logarithm base $\\alpha$ (since $\\alpha \\ge 2$): $K-1 < \\log_{\\alpha}\\left(\\frac{N}{C_0}\\right) \\le K$.\nThis implies that for $N>C_0$, the number of reallocations is $K = \\left\\lceil \\log_{\\alpha}\\left(\\frac{N}{C_0}\\right) \\right\\rceil$.\n\nWe can combine the cases $N \\le C_0$ and $N > C_0$ into a single expression for $K$. If $N \\le C_0$, then $\\frac{N}{C_0} \\le 1$ and $\\log_{\\alpha}(\\frac{N}{C_0}) \\le 0$. The ceiling of a non-positive number is an integer less than or equal to $0$. Since $K$ cannot be negative, we define $K$ as:\n$$K = \\max\\left(0, \\left\\lceil \\log_{\\alpha}\\left(\\frac{m+n}{C_0}\\right) \\right\\rceil\\right)$$\n\nThe total number of allocations is $1$ (for the initial array) plus $K$ (for the reallocations).\n$$T_{\\mathrm{alloc}} = (1+K) c_{\\mathrm{alloc}}$$\n\nThe move cost is incurred during each of the $K$ reallocations.\n- At the 1st reallocation, the array size is $C_0$. We move $C_0$ elements.\n- At the 2nd reallocation, the array size is $C_1=C_0\\alpha$. We move $C_0\\alpha$ elements.\n- At the $j$-th reallocation, the array size is $C_{j-1}=C_0\\alpha^{j-1}$. We move $C_0\\alpha^{j-1}$ elements.\nThe total number of elements moved is the sum over all $K$ reallocations:\n$$\\text{Total elements moved} = \\sum_{j=1}^{K} C_0\\alpha^{j-1} = \\sum_{i=0}^{K-1} C_0\\alpha^{i}$$\nThis is a geometric series. If $K=0$, the sum is empty and equals $0$. If $K \\ge 1$:\n$$\\sum_{i=0}^{K-1} C_0\\alpha^{i} = C_0 \\left(\\frac{\\alpha^K - 1}{\\alpha - 1}\\right)$$\nThis formula also correctly yields $0$ if we substitute $K=0$.\nThe total move cost is:\n$$T_{\\mathrm{move}} = C_0 \\left(\\frac{\\alpha^K - 1}{\\alpha - 1}\\right) c_{\\mathrm{move}}$$\n\n**Total Worst-Case Cost**\nCombining all components, the total worst-case cost $T_{WC}$ is:\n$$T_{WC} = (m+n-1)c_{\\mathrm{cmp}} + (m+n)c_{\\mathrm{write}} + (1+K)c_{\\mathrm{alloc}} + C_0 \\left(\\frac{\\alpha^K - 1}{\\alpha - 1}\\right)c_{\\mathrm{move}}$$\nSubstituting the expression for $K$:\n$$T_{WC} = (m+n-1)c_{\\mathrm{cmp}} + (m+n)c_{\\mathrm{write}} + \\left(1 + \\max\\left(0, \\left\\lceil \\log_{\\alpha}\\left(\\frac{m+n}{C_0}\\right) \\right\\rceil\\right)\\right) c_{\\mathrm{alloc}} + C_0 \\frac{\\alpha^{\\max\\left(0, \\left\\lceil \\log_{\\alpha}\\left(\\frac{m+n}{C_0}\\right) \\right\\rceil\\right)}-1}{\\alpha - 1} c_{\\mathrm{move}}$$\nThis is the final closed-form expression for the exact worst-case total cost.", "answer": "$$\n\\boxed{(m+n-1)c_{\\mathrm{cmp}} + (m+n)c_{\\mathrm{write}} + \\left(1 + \\max\\left(0, \\left\\lceil \\log_{\\alpha}\\left(\\frac{m+n}{C_0}\\right) \\right\\rceil\\right)\\right) c_{\\mathrm{alloc}} + C_0 \\frac{\\alpha^{\\max\\left(0, \\left\\lceil \\log_{\\alpha}\\left(\\frac{m+n}{C_0}\\right) \\right\\rceil\\right)}-1}{\\alpha - 1} c_{\\mathrm{move}}}\n$$", "id": "3230162"}, {"introduction": "A key aspect of data structure design is matching the structure's strengths to the application's needs. A standard dynamic array is optimized for adding and removing elements at the end, but what happens when you need to efficiently remove elements from the front? This hands-on problem contrasts the linear-time inefficiency of a naive `pop_front` operation with the amortized constant-time performance of a more sophisticated design using a circular buffer. By implementing both, you will directly experience the profound impact of algorithmic design on practical performance. [@problem_id:3230221]", "problem": "You are given the task of reasoning from first principles about dynamic arrays and amortized analysis, and to validate your reasoning by implementing two concrete array-backed structures. Use only the following foundational base: array indexing in Random Access Memory (RAM) is constant time; copying or assigning a single element has unit cost; copying $k$ elements has cost proportional to $k$; creating a new array and copying existing elements are the only sources of data movement aside from explicit assignments; the dynamic array capacity is always a power of $2$ when grown by doubling.\n\nYour tasks are:\n\n- Implement a standard dynamic array that supports $push\\_back$ in amortized constant time using doubling when full. Implement $pop\\_front$ by shifting all remaining elements one position to the left. Analyze the time cost of $pop\\_front$ purely by counting the number of element assignments it performs within the $pop\\_front$ operation. Ignore any cost incurred by reallocation due to $push\\_back$ when answering about $pop\\_front$.\n\n- Design and implement an array-backed queue with both $push\\_back$ and $pop\\_front$ that achieves amortized $\\mathcal{O}(1)$ for both operations. Use a circular buffer (ring) over a dynamic array with doubling on overflow and halving when, after an operation, the size is at most one quarter of the capacity. When resizing (either growing or shrinking), copy all elements into a new array in logical order starting at index $0$. Perform at most one resize per operation. Count the number of element copies done during resizing, and only during resizing, to quantify cost.\n\nFrom the above base and your implementations, derive why the naive $pop\\_front$ has linear cost in the current size, and why the ring buffer achieves amortized constant time for both $push\\_back$ and $pop\\_front$ under the specified doubling/halving policy.\n\nTest suite and required outputs:\n\nImplement your program to execute the following four test cases, using exact sequences of operations. For the naive dynamic array, count only the number of element assignments performed inside $pop\\_front$ due to shifting. For the ring buffer, count only the number of element copies performed during resizing. For any $pop\\_front$ on an empty structure, do nothing and return a distinguished value (such as a null) internally; such a no-op should not affect counters.\n\n- Test case $1$ (naive dynamic array shift cost):\n  - Start empty.\n  - Perform $push\\_back$ of the integers $1$ through $10$ in increasing order.\n  - Perform $pop\\_front$ exactly $3$ times.\n  - Perform $push\\_back$ of the integers $11$ through $15$ in increasing order.\n  - Perform $pop\\_front$ exactly $4$ times.\n  - Perform $push\\_back$ of the integers $16$ through $20$ in increasing order.\n  - Output for this case: a single integer equal to the total number of element assignments performed by all $pop\\_front$ operations (ignore any reallocations caused by $push\\_back$).\n\n- Test case $2$ (ring buffer resize copy count under mixed operations):\n  - Start empty.\n  - Perform $push\\_back$ of $1$ through $10$.\n  - Perform $pop\\_front$ $3$ times.\n  - Perform $push\\_back$ of $11$ through $15$.\n  - Perform $pop\\_front$ $4$ times.\n  - Perform $push\\_back$ of $16$ through $20$.\n  - Output for this case: a single integer equal to the total number of element copies performed during all resizes (both grow and shrink).\n\n- Test case $3$ (ring buffer correctness and wrap-around behavior):\n  - Start empty.\n  - Perform $push\\_back$ of $1$ through $6$.\n  - Perform $pop\\_front$ $4$ times.\n  - Perform $push\\_back$ of $7$, $8$, $9$, $10$.\n  - Perform $pop\\_front$ $2$ times.\n  - Output for this case: the final content of the ring buffer as a list of integers in logical order from front to back.\n\n- Test case $4$ (ring buffer growth and shrink copy count):\n  - Start empty.\n  - Perform $push\\_back$ of $1$ through $16$.\n  - Perform $pop\\_front$ $12$ times.\n  - Perform $push\\_back$ of $17$ through $24$.\n  - Perform $pop\\_front$ $8$ times.\n  - Output for this case: a single integer equal to the total number of element copies performed during all resizes (both grow and shrink).\n\nFinal output format:\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with the four test case results in order: $[$result\\_1,result\\_2,result\\_3,result\\_4$]$. The first, second, and fourth results are integers. The third result is a list of integers. For example, a syntactically valid output looks like $[42,7,[3,5,8],19]$ where the specific numbers must match the correct results for the test suite above.", "solution": "The problem requires an analysis and implementation of two array-backed data structures to compare their performance for queue-like operations, specifically `push_back` and `pop_front`. The analysis must be derived from first principles, grounded in a specified cost model where element assignments and copies are the primary measure of work.\n\n**Foundational Principles and Cost Model**\n\nThe analysis is based on the following axioms:\n1.  Random Access Memory (RAM) allows for constant time, $\\mathcal{O}(1)$, access to any array element by its index.\n2.  The cost of a single element assignment or copy is a unit cost, which we can denote as $\\mathcal{O}(1)$.\n3.  The cost of copying a block of $k$ contiguous elements is $\\mathcal{O}(k)$.\n4.  Data movement is restricted to explicit assignments or block copies during array resizing.\n5.  The capacity of a dynamic array, when grown, is always a power of $2$, achieved by doubling the current capacity.\n\n**Part 1: Naive Dynamic Array with Shifting `pop_front`**\n\nA standard dynamic array supports `push_back` by appending an element to the end. If the underlying array is full, a new, larger array is allocated (in this case, of double the capacity), all existing elements are copied to it, and the old array is discarded. This doubling strategy ensures that the amortized cost of `push_back` is $\\mathcal{O}(1)$.\n\nThe problem specifies a naive implementation for `pop_front`. This operation must remove the element at index $0$. To maintain the contiguity of the array with its first logical element at index $0$, all subsequent elements must be shifted one position to the left.\n\n**Cost Analysis of Naive `pop_front`**\nLet the dynamic array contain $N$ elements, indexed from $0$ to $N-1$.\n1.  The element at index $0$ is removed.\n2.  The element at index $1$ is copied to index $0$.\n3.  The element at index $2$ is copied to index $1$.\n4.  This process continues until the last element at index $N-1$ is copied to index $N-2$.\n\nThis sequence involves $N-1$ individual element assignment operations. According to the stated cost model, where each assignment has a unit cost, the total cost of a single `pop_front` operation is directly proportional to $N-1$. Therefore, the time complexity of the naive `pop_front` is $\\mathcal{O}(N)$, where $N$ is the current number of elements in the array. This cost is incurred for every `pop_front` call, so there is no opportunity for amortization; the worst-case and average-case costs are both linear in the size of the array.\n\n**Part 2: Array-Backed Queue with a Circular Buffer (Ring Buffer)**\n\nTo achieve efficient `push_back` and `pop_front` operations simultaneously, a circular buffer (or ring buffer) is employed. This structure is implemented over a dynamic array but treats the array's memory as a circle. Two pointers, `head` and `tail` (or an equivalent representation like `head` and `size`), are used to mark the logical beginning and end of the queue.\n\n-   `push_back`: Adds an element at the `tail` position and advances the `tail`.\n-   `pop_front`: Removes an element from the `head` position and advances the `head`.\n\nBoth pointers wrap around to the beginning of the array when they pass the end, typically using the modulo operator: `new_index = (old_index + 1) % capacity`. In the absence of resizing, both `push_back` and `pop_front` only require updating a pointer and a single element read/write. These are $\\mathcal{O}(1)$ operations.\n\n**Amortized Analysis of Ring Buffer Resizing**\nThe cost of this structure is dominated by its resizing operations. The specified policy is:\n-   **Growth:** If `size == capacity` upon a `push_back`, double the capacity ($C \\to 2C$).\n-   **Shrinking:** If `size <= capacity / 4` after an operation, halve the capacity ($C \\to C/2$).\n\nWhen a resize occurs, all $N$ elements are copied from the (potentially wrapped) old array into a new, contiguous array starting at index $0$. This \"unwraps\" the queue and costs $\\mathcal{O}(N)$. We can use a potential function analysis to show that the amortized cost of both operations is $\\mathcal{O}(1)$.\n\nLet $N$ be the number of elements and $C$ be the capacity. We define a potential function $\\Phi$ that stores \"credit\" to pay for future expensive resizes. A suitable potential function for this scheme is:\n$$ \\Phi = \\begin{cases} 2N - C & \\text{if } N/C \\ge 1/2 \\\\ C/2 - N & \\text{if } N/C < 1/2 \\end{cases} $$\nThe amortized cost $\\hat{c}$ of an operation is its actual cost $c$ plus the change in potential, $\\Delta\\Phi$.\n\n1.  **`push_back` causes growth**: Suppose we have $N=C$ elements and perform a `push_back`. The actual cost $c$ is $1$ (for the element write) $+ C$ (for copying $C$ elements to the new array). The total is $c = C+1$. The capacity changes from $C$ to $C' = 2C$, and the size becomes $N' = C+1$.\n    -   Old potential (with $N=C$, so $N/C=1 \\ge 1/2$): $\\Phi_{old} = 2N - C = 2C - C = C$.\n    -   New potential (with $N' = C+1, C'=2C$, so $N'/C' \\approx 1/2$): $\\Phi_{new} = 2N' - C' = 2(C+1) - 2C = 2$.\n    -   $\\Delta\\Phi = \\Phi_{new} - \\Phi_{old} = 2 - C$.\n    -   Amortized cost: $\\hat{c} = c + \\Delta\\Phi = (C+1) + (2-C) = 3$. This is $\\mathcal{O}(1)$.\n\n2.  **`pop_front` causes shrinking**: Suppose a `pop_front` reduces the size to $N = C/4$. The actual cost $c$ is $1$ (for the pop logic) $+ N$ (for copying $N$ elements). The total is $c = C/4+1$. The capacity changes from $C$ to $C' = C/2$, and the size remains $N' = N = C/4$.\n    -   The state just before shrinking was $N_{pre} = C/4+1$ elements in a capacity $C$ array. Since $N_{pre}/C < 1/2$, the potential was $\\Phi_{old} = C/2 - N_{pre} = C/2 - (C/4+1) = C/4 - 1$.\n    -   New potential (with $N' = C/4, C' = C/2$, so $N'/C' = 1/2$): $\\Phi_{new} = 2N' - C' = 2(C/4) - C/2 = 0$.\n    -   $\\Delta\\Phi = \\Phi_{new} - \\Phi_{old} = 0 - (C/4-1) = 1-C/4$.\n    -   Amortized cost: $\\hat{c} = c + \\Delta\\Phi = (C/4+1) + (1-C/4) = 2$. This is $\\mathcal{O}(1)$.\n\nIn all cases (including non-resizing ones, which can be similarly analyzed), the amortized cost is bounded by a small constant. The resizing strategy ensures that after a resize, there must be a number of operations proportional to the new capacity before another resize is required. This gap allows sufficient \"credit\" (potential) to accumulate to pay for the next linear-cost resize, rendering the amortized cost for every single operation $\\mathcal{O}(1)$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Executes the four test cases defined in the problem statement\n    and prints the results in the specified format.\n    \"\"\"\n\n    # --- Test Case 1: Naive Dynamic Array Shift Cost ---\n    \n    class NaiveDynamicArray:\n        def __init__(self):\n            self.size = 0\n            self.capacity = 0\n            self.data = np.empty(self.capacity, dtype=object)\n            self.pop_front_cost = 0\n\n        def _resize(self, new_capacity):\n            new_data = np.empty(new_capacity, dtype=object)\n            if self.size > 0:\n                new_data[0:self.size] = self.data[0:self.size]\n            self.data = new_data\n            self.capacity = new_capacity\n\n        def push_back(self, value):\n            if self.size == self.capacity:\n                new_cap = 1 if self.capacity == 0 else self.capacity * 2\n                self._resize(new_cap)\n            self.data[self.size] = value\n            self.size += 1\n\n        def pop_front(self):\n            if self.size == 0:\n                return None\n            \n            value = self.data[0]\n            # Cost is the number of element assignments for the shift.\n            shift_cost = self.size - 1\n            self.pop_front_cost += shift_cost\n            \n            # Shift all elements to the left.\n            if shift_cost > 0:\n                self.data[0:shift_cost] = self.data[1:self.size]\n            \n            self.size -= 1\n            return value\n\n    nda = NaiveDynamicArray()\n    for i in range(1, 11): nda.push_back(i)\n    for _ in range(3): nda.pop_front()\n    for i in range(11, 16): nda.push_back(i)\n    for _ in range(4): nda.pop_front()\n    for i in range(16, 21): nda.push_back(i)\n    result_1 = nda.pop_front_cost\n\n    # --- Test Cases 2, 3, 4: Ring Buffer ---\n    \n    class RingBuffer:\n        def __init__(self):\n            self.size = 0\n            self.capacity = 0\n            self.head = 0\n            self.data = np.empty(self.capacity, dtype=object)\n            self.resize_cost = 0\n\n        def _resize(self, new_capacity):\n            # Cost is the number of elements copied.\n            self.resize_cost += self.size\n            \n            new_data = np.empty(new_capacity, dtype=object)\n            # Unwraps the circular buffer into a linear array.\n            for i in range(self.size):\n                old_idx = (self.head + i) % self.capacity\n                new_data[i] = self.data[old_idx]\n            \n            self.data = new_data\n            self.capacity = new_capacity\n            self.head = 0\n\n        def push_back(self, value):\n            if self.size == self.capacity:\n                new_cap = 1 if self.capacity == 0 else self.capacity * 2\n                self._resize(new_cap)\n            \n            tail_idx = (self.head + self.size) % self.capacity\n            self.data[tail_idx] = value\n            self.size += 1\n            # Per problem spec, shrink check is done after an operation.\n            # A push_back will never cause size = capacity / 4 if the\n            # invariant held before, so no shrink check needed here.\n            \n        def pop_front(self):\n            if self.size == 0:\n                return None\n            \n            value = self.data[self.head]\n            self.head = (self.head + 1) % self.capacity\n            self.size -= 1\n            \n            # Shrink condition: size is at most one quarter of capacity.\n            # Must also ensure capacity > 0 to avoid division by zero.\n            if self.capacity > 0 and self.size = self.capacity // 4:\n                # Halve capacity, minimum capacity is 1 if not empty.\n                new_cap = max(1, self.capacity // 2)\n                if new_cap  self.capacity:\n                    self._resize(new_cap)\n\n            return value\n\n        def get_contents(self):\n            contents = []\n            for i in range(self.size):\n                idx = (self.head + i) % self.capacity\n                contents.append(self.data[idx])\n            return contents\n\n    # Test Case 2\n    rb2 = RingBuffer()\n    for i in range(1, 11): rb2.push_back(i)\n    for _ in range(3): rb2.pop_front()\n    for i in range(11, 16): rb2.push_back(i)\n    for _ in range(4): rb2.pop_front()\n    for i in range(16, 21): rb2.push_back(i)\n    result_2 = rb2.resize_cost\n\n    # Test Case 3\n    rb3 = RingBuffer()\n    for i in range(1, 7): rb3.push_back(i)\n    for _ in range(4): rb3.pop_front()\n    for i in range(7, 11): rb3.push_back(i)\n    for _ in range(2): rb3.pop_front()\n    result_3 = rb3.get_contents()\n\n    # Test Case 4\n    rb4 = RingBuffer()\n    for i in range(1, 17): rb4.push_back(i)\n    for _ in range(12): rb4.pop_front()\n    for i in range(17, 25): rb4.push_back(i)\n    for _ in range(8): rb4.pop_front()\n    result_4 = rb4.resize_cost\n    \n    # Final output formatting\n    # The f-string conversion of a list automatically includes spaces, e.g., '[7, 8]'.\n    # We must remove them to match a strict list representation like '[7,8,9,10]'.\n    result_3_str = str(result_3).replace(\" \", \"\")\n\n    print(f\"[{result_1},{result_2},{result_3_str},{result_4}]\")\n\n\nsolve()\n```", "id": "3230221"}]}