{"hands_on_practices": [{"introduction": "The cost of array operations is often non-uniform and depends heavily on the context. This exercise reveals how the *order* of a series of deletions can dramatically impact the total computational cost. By analyzing the work required to delete elements at specific indices—in this case, perfect squares—you will learn to derive an optimal strategy from first principles to minimize the total number of expensive element shifts [@problem_id:3208390].", "problem": "Consider a contiguous array of length $N$ indexed from $1$ to $N$. You are tasked with deleting, one by one, exactly those entries whose original indices are perfect squares, that is, indices $i$ such that $i = k^{2}$ for some integer $k \\ge 1$. At any point, deleting the element currently at position $j$ requires shifting each element strictly to the right of position $j$ one position to the left, and each individual element shift costs $1$ unit. There is no batching: each deletion is performed as a standalone operation, and its cost is the number of elements shifted during that operation. You may choose the order in which to perform the deletions.\n\nStarting only from the core definition of array deletion as the shifting of all subsequent elements and the fact that each shift costs $1$ unit, derive from first principles the minimum possible total cost (measured in units of element shifts) to complete all deletions. Express your final answer as a single closed-form analytic expression in terms of $N$, using only standard arithmetic operators and, if needed, the floor function. No rounding is required, and the final answer must be a single expression without units.", "solution": "The user wants me to act as a meticulous STEM professor and solve the given problem.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- An array of length $N$, with indices from $1$ to $N$.\n- Task: Delete entries whose original indices are perfect squares, i.e., $i = k^2$ for any integer $k \\ge 1$.\n- Deletion process: Deleting an element at current position $j$ causes all elements to its right to shift one position to the left.\n- Cost: The cost of a deletion is the number of elements shifted, which is $1$ unit per shift.\n- Deletions are sequential and not batched.\n- The order of deletions is selectable.\n- Objective: Derive the minimum possible total cost from first principles.\n- Final answer: A single closed-form analytic expression in terms of $N$, using standard arithmetic operators and the floor function.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem is a well-defined exercise in algorithm analysis, specifically concerning the operational cost of array manipulation. It is firmly based on the principles of data structures and algorithms.\n- **Well-Posed:** The problem provides all necessary information. It seeks to minimize a cost function over a finite set of possible deletion orderings. A minimum cost is guaranteed to exist.\n- **Objective:** The problem is stated in precise mathematical and algorithmic terms, free from ambiguity or subjective content.\n- **Other criteria:** The problem is self-contained, consistent, and formalizable. It poses a non-trivial but solvable reasoning challenge.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A full solution will be provided.\n\n### Solution Derivation\n\nThe process to find the minimum cost is as follows:\n1.  Identify the set of elements to be deleted.\n2.  Formulate the cost of a single deletion operation.\n3.  Express the total cost as a function of the order of deletions.\n4.  Determine the optimal order of deletions that minimizes the total cost.\n5.  Calculate the total cost for this optimal order and simplify it into a closed-form expression.\n\nLet the initial array be of length $N$. The elements to be deleted are those at original indices that are perfect squares. These indices are $1^2, 2^2, 3^2, \\dots, K^2$ such that $K^2 \\le N$. The largest such integer $K$ is $K = \\lfloor\\sqrt{N}\\rfloor$. Let us denote this value by $M$. Thus, there are $M = \\lfloor\\sqrt{N}\\rfloor$ elements to be deleted. Let the set of these original indices, sorted in increasing order, be $S = \\{s_1, s_2, \\dots, s_M\\}$, where $s_k = k^2$ for $k \\in \\{1, 2, \\dots, M\\}$.\n\nThe cost of deleting an element at a current position $j$ in an array of current length $L$ is the number of elements that must be shifted. These are the elements at positions $j+1, j+2, \\dots, L$. The number of such elements is $L - j$. So the cost is $L - j$.\n\nThe total cost depends on the order in which the $M$ elements are deleted. Let $\\pi$ be a permutation of the set $\\{1, 2, \\dots, M\\}$, representing the order of deletion. At step $t$ (for $t=1, \\dots, M$), we delete the element that was originally at index $s_{\\pi(t)}$.\n\nLet's analyze the cost of the $t$-th deletion.\nAt the beginning of step $t$, $t-1$ elements have already been deleted. The current length of the array is $L_t = N - (t-1)$.\nThe element to be deleted was originally at index $s_{\\pi(t)}$. Its current position, $P_t$, is its original index minus the number of elements that were located at original positions smaller than $s_{\\pi(t)}$ and have already been deleted. The set of elements deleted in steps $1, \\dots, t-1$ are those from original indices $\\{s_{\\pi(1)}, s_{\\pi(2)}, \\dots, s_{\\pi(t-1)}\\}$.\nThe number of these deleted elements that were originally positioned before $s_{\\pi(t)}$ is $d_t = |\\{j \\in \\{1, \\dots, t-1\\} \\mid s_{\\pi(j)} < s_{\\pi(t)}\\}|$.\nThe current position of the element to be deleted is therefore $P_t = s_{\\pi(t)} - d_t$.\nThe cost of this $t$-th deletion is $C_t = L_t - P_t = (N - t + 1) - (s_{\\pi(t)} - d_t)$.\n$C_t = N + 1 - t - s_{\\pi(t)} + d_t$.\n\nThe total cost for the deletion sequence defined by $\\pi$ is the sum of the costs of the individual deletions:\n$C_{total}(\\pi) = \\sum_{t=1}^{M} C_t = \\sum_{t=1}^{M} (N + 1 - t - s_{\\pi(t)} + d_t)$.\nWe can separate the terms:\n$C_{total}(\\pi) = \\sum_{t=1}^{M} (N+1) - \\sum_{t=1}^{M} t - \\sum_{t=1}^{M} s_{\\pi(t)} + \\sum_{t=1}^{M} d_t$.\n\nLet's analyze each term in the sum:\n- $\\sum_{t=1}^{M} (N+1) = M(N+1)$. This term is independent of the deletion order $\\pi$.\n- $\\sum_{t=1}^{M} t = \\frac{M(M+1)}{2}$. This term is also independent of $\\pi$.\n- $\\sum_{t=1}^{M} s_{\\pi(t)} = \\sum_{k=1}^{M} s_k = \\sum_{k=1}^{M} k^2 = \\frac{M(M+1)(2M+1)}{6}$. This is the sum over the set of indices to be deleted, and is independent of the order $\\pi$.\n- $\\sum_{t=1}^{M} d_t = \\sum_{t=1}^{M} |\\{j \\in \\{1, \\dots, t-1\\} \\mid s_{\\pi(j)} < s_{\\pi(t)}\\}|$. This is the only term that depends on the chosen permutation $\\pi$.\n\nTo minimize the total cost $C_{total}(\\pi)$, we must minimize the term $\\sum_{t=1}^{M} d_t$. Since $d_t$ is a count of elements, $d_t \\ge 0$. Therefore, the sum is minimized when each $d_t$ is minimized. The smallest possible value for each $d_t$ is $0$.\nThe condition $d_t = 0$ for all $t \\in \\{1, \\dots, M\\}$ means that for every step $t$, there are no previously deleted elements (from steps $j < t$) whose original index $s_{\\pi(j)}$ is smaller than the current element's original index $s_{\\pi(t)}$. This is achieved if $s_{\\pi(j)} > s_{\\pi(t)}$ for all $j < t$. This implies the sequence of original indices for deletion must be strictly decreasing:\n$s_{\\pi(1)} > s_{\\pi(2)} > \\dots > s_{\\pi(M)}$.\nSince $s_k = k^2$ is a strictly increasing function of $k$, this requires $\\pi(1) > \\pi(2) > \\dots > \\pi(M)$. The unique permutation that satisfies this is the reverse-sorted one: $\\pi(t) = M - t + 1$.\nThis means the optimal strategy is to delete the elements in decreasing order of their original indices: $M^2$, then $(M-1)^2$, and so on, down to $1^2$.\n\nNow, we calculate the minimum cost, $C_{min}$, using this optimal order. With this order, $d_t = 0$ for all $t$.\n$C_{min} = \\sum_{t=1}^{M} (N + 1 - t - s_{\\pi(t)})$.\nUsing $\\pi(t) = M-t+1$, we get $s_{\\pi(t)} = s_{M-t+1} = (M-t+1)^2$.\n$C_{min} = \\sum_{t=1}^{M} [ (N+1) - t - (M-t+1)^2 ]$.\n$C_{min} = \\sum_{t=1}^{M} (N+1) - \\sum_{t=1}^{M} t - \\sum_{t=1}^{M} (M-t+1)^2$.\n- $\\sum_{t=1}^{M} (N+1) = M(N+1)$.\n- $\\sum_{t=1}^{M} t = \\frac{M(M+1)}{2}$.\n- For the third term, let $k = M-t+1$. As $t$ ranges from $1$ to $M$, $k$ ranges from $M$ to $1$.\n  $\\sum_{t=1}^{M} (M-t+1)^2 = \\sum_{k=1}^{M} k^2 = \\frac{M(M+1)(2M+1)}{6}$.\n\nSubstituting these back into the expression for $C_{min}$:\n$C_{min} = M(N+1) - \\frac{M(M+1)}{2} - \\frac{M(M+1)(2M+1)}{6}$.\nWe can factor out $\\frac{M}{6}$ to simplify:\n$C_{min} = \\frac{M}{6} [6(N+1) - 3(M+1) - (M+1)(2M+1)]$.\nExpanding the terms inside the brackets:\n$C_{min} = \\frac{M}{6} [ (6N+6) - (3M+3) - (2M^2+3M+1) ]$.\n$C_{min} = \\frac{M}{6} [ 6N+6 - 3M-3 - 2M^2-3M-1 ]$.\nCombining like terms:\n$C_{min} = \\frac{M}{6} [ 6N + (6-3-1) - (3M+3M) - 2M^2 ]$.\n$C_{min} = \\frac{M}{6} [ 6N + 2 - 6M - 2M^2 ]$.\nFactoring out a $2$ from the bracket:\n$C_{min} = \\frac{M \\cdot 2}{6} [ 3N + 1 - 3M - M^2 ]$.\n$C_{min} = \\frac{M}{3} (3N + 1 - 3M - M^2)$.\n\nFinally, we substitute back $M = \\lfloor\\sqrt{N}\\rfloor$. The minimum total cost is:\n$C_{min} = \\frac{\\lfloor\\sqrt{N}\\rfloor}{3} (3N + 1 - 3\\lfloor\\sqrt{N}\\rfloor - (\\lfloor\\sqrt{N}\\rfloor)^2)$.\nThis is the required single closed-form analytic expression derived from first principles.", "answer": "$$\n\\boxed{\\frac{\\lfloor\\sqrt{N}\\rfloor}{3} (3N + 1 - 3\\lfloor\\sqrt{N}\\rfloor - \\lfloor\\sqrt{N}\\rfloor^2)}\n$$", "id": "3208390"}, {"introduction": "Once we understand the cost of individual operations, the next step is to compare high-level algorithmic strategies. This practice explores a classic trade-off: is it more efficient to insert multiple elements into a sorted array one by one, or as a single batch? By modeling the costs of both CPU comparisons and memory moves, you will derive a quantitative threshold that determines the superior strategy, a vital skill for designing high-performance systems [@problem_id:3208573].", "problem": "You are given a sorted array of distinct keys of length $N$ and $k$ new keys to insert. The new keys are independent and identically distributed draws from a continuous distribution such that their insertion ranks among the existing $N$ keys are independent and uniformly distributed over the $N+1$ possible slots. Consider two strategies under a cost model with a unit cost $c_{\\mathrm{cmp}} > 0$ for each comparison and a unit cost $c_{\\mathrm{mov}} > 0$ for moving (writing) a single element position in memory.\n\nStrategy A (one-by-one insertion): Insert each new key sequentially. For each insertion into a sorted array of current length $L$, find the insertion index using binary search and perform the needed right-shifts to make room. Assume binary search uses $\\lceil \\log_{2} L \\rceil$ comparisons. Model the shift as moving each element that lies strictly to the right of the insertion index by one position, and count each such move with cost $c_{\\mathrm{mov}}$.\n\nStrategy B (batch insertion by sort-then-merge): First sort the $k$ new keys using a comparison-based sorting algorithm that uses at most $\\beta k \\log_{2} k$ comparisons for some constant $\\beta \\ge 1$, then merge the two sorted sequences (the original array of length $N$ and the newly sorted keys of length $k$) into a fresh array of length $N+k$ using the classical two-way merge, which performs at most $N+k-1$ comparisons and exactly $N+k$ moves. You may assume no additional overhead costs beyond comparisons and moves as specified.\n\nWork in the asymptotic regime where $1 \\ll k \\ll N$. Starting from first principles and standard definitions in the analysis of algorithms, derive the leading-order expected total costs of both strategies under the uniform rank assumption, identify the smallest $k$ for which the expected cost of Strategy B is no greater than that of Strategy A, and provide the resulting leading-order asymptotic expression for this threshold $k$ in terms of $c_{\\mathrm{cmp}}$ and $c_{\\mathrm{mov}}$. Your final answer must be a single closed-form analytic expression for the leading-order threshold $k$; do not provide an inequality. No rounding is required.", "solution": "The problem asks for a comparison of two strategies for inserting $k$ new keys into a sorted array of size $N$, under the asymptotic regime $1 \\ll k \\ll N$. We are to find the threshold value of $k$ where Strategy B becomes more cost-effective than Strategy A. The cost is a function of comparisons ($c_{\\mathrm{cmp}}$ per comparison) and memory moves ($c_{\\mathrm{mov}}$ per move).\n\nFirst, we derive the leading-order expected total cost for Strategy A.\n\n**Strategy A: One-by-One Insertion**\n\nStrategy A consists of inserting each of the $k$ keys sequentially. Let us analyze the cost of inserting the $i$-th key, for $i \\in \\{1, 2, \\dots, k\\}$. At the time of the $i$-th insertion, the array has a length of $L = N+i-1$.\n\nThe cost for this insertion is the sum of the comparison cost and the movement cost.\n1.  **Comparison Cost:** The problem states that a binary search is used to find the insertion point, which requires $\\lceil \\log_{2} L \\rceil$ comparisons. The cost is $c_{\\mathrm{cmp}} \\lceil \\log_{2} (N+i-1) \\rceil$.\n\n2.  **Movement Cost:** The insertion rank of the new key is uniformly distributed over the $L+1$ possible slots. Let the insertion index be $j$, where $j \\in \\{0, 1, \\dots, L\\}$. Inserting at index $j$ requires shifting all elements from index $j$ to $L-1$ one position to the right. This constitutes $L-j$ moves. Since each index $j$ is equally likely with probability $P(j) = \\frac{1}{L+1}$, the expected number of moves is:\n    $$E[\\text{moves}] = \\sum_{j=0}^{L} (L-j) P(j) = \\frac{1}{L+1} \\sum_{j=0}^{L} (L-j)$$\n    Let $m = L-j$. As $j$ goes from $0$ to $L$, $m$ goes from $L$ to $0$.\n    $$E[\\text{moves}] = \\frac{1}{L+1} \\sum_{m=0}^{L} m = \\frac{1}{L+1} \\frac{L(L+1)}{2} = \\frac{L}{2}$$\n    The expected movement cost for the $i$-th insertion is $c_{\\mathrm{mov}} \\frac{L}{2} = c_{\\mathrm{mov}} \\frac{N+i-1}{2}$.\n\nThe total expected cost for Strategy A, $E[C_A]$, is the sum of the expected costs for each of the $k$ insertions:\n$$E[C_A] = \\sum_{i=1}^{k} \\left( c_{\\mathrm{cmp}} \\lceil \\log_{2} (N+i-1) \\rceil + c_{\\mathrm{mov}} \\frac{N+i-1}{2} \\right)$$\n\nFor the asymptotic analysis where $1 \\ll k \\ll N$, we can approximate the terms.\nFor the comparison cost, $N \\le N+i-1 < N+k$. Since $k \\ll N$, $\\log_{2}(N+i-1) = \\log_{2} N + \\log_{2}(1+\\frac{i-1}{N}) \\approx \\log_2 N$. Thus, $\\lceil \\log_{2} (N+i-1) \\rceil \\approx \\log_2 N$. The total comparison cost is approximately $k c_{\\mathrm{cmp}} \\log_{2} N$.\nThe total movement cost is:\n$$c_{\\mathrm{mov}} \\sum_{i=1}^{k} \\frac{N+i-1}{2} = \\frac{c_{\\mathrm{mov}}}{2} \\left( \\sum_{i=1}^{k} N + \\sum_{i=1}^{k} (i-1) \\right) = \\frac{c_{\\mathrm{mov}}}{2} \\left( kN + \\frac{k(k-1)}{2} \\right) = \\frac{c_{\\mathrm{mov}} k N}{2} + \\frac{c_{\\mathrm{mov}} k(k-1)}{4}$$\nSo, the total expected cost is:\n$$E[C_A] \\approx k c_{\\mathrm{cmp}} \\log_{2} N + \\frac{c_{\\mathrm{mov}} k N}{2} + \\frac{c_{\\mathrm{mov}} (k^2 - k)}{4}$$\nIn the regime $1 \\ll k \\ll N$, the term proportional to $N$ is dominant. The term $k^2$ is much smaller than $kN$, and the term $k \\log_2 N$ is also much smaller than $kN$. Therefore, the leading-order expected cost of Strategy A is:\n$$E[C_A] \\approx \\frac{c_{\\mathrm{mov}} k N}{2}$$\n\n**Strategy B: Batch Insertion by Sort-Then-Merge**\n\nStrategy B involves two steps: sorting the $k$ new keys and then merging them with the original array of $N$ keys.\n1.  **Sorting Cost:** The $k$ new keys are sorted using an algorithm with at most $\\beta k \\log_{2} k$ comparisons. The problem statement does not specify a move cost for this step; we assume it is subsumed into lower-order terms relative to the merge step which involves $N$ elements. The comparison cost is at most $c_{\\mathrm{cmp}} \\beta k \\log_{2} k$.\n2.  **Merging Cost:** The sorted list of $k$ keys is merged with the sorted array of $N$ keys into a new array of size $N+k$. This operation takes at most $N+k-1$ comparisons and exactly $N+k$ moves. The cost is $c_{\\mathrm{cmp}} (N+k-1) + c_{\\mathrm{mov}} (N+k)$.\n\nThe total cost for Strategy B, $C_B$, is bounded by:\n$$C_B \\le c_{\\mathrm{cmp}} (\\beta k \\log_{2} k) + c_{\\mathrm{cmp}} (N+k-1) + c_{\\mathrm{mov}} (N+k)$$\n$$C_B \\le (c_{\\mathrm{cmp}} + c_{\\mathrm{mov}})N + (c_{\\mathrm{cmp}} (\\beta \\log_{2} k + 1) + c_{\\mathrm{mov}})k - c_{\\mathrm{cmp}}$$\nFor the asymptotic analysis where $1 \\ll k \\ll N$, the terms proportional to $N$ are dominant. The terms proportional to $k \\log_2 k$ and $k$ are of lower order. Thus, the leading-order cost of Strategy B is:\n$$C_B \\approx (c_{\\mathrm{cmp}} + c_{\\mathrm{mov}}) N$$\n\n**Threshold Identification**\n\nWe are looking for the smallest $k$ such that the expected cost of Strategy B is no greater than that of Strategy A. This corresponds to the inequality $C_B \\le E[C_A]$. To find the leading-order asymptotic expression for the threshold $k$, we compare the leading-order costs of the two strategies:\n$$(c_{\\mathrm{cmp}} + c_{\\mathrm{mov}}) N \\le \\frac{c_{\\mathrm{mov}} k N}{2}$$\nSince $N$ is a large positive length, we can divide both sides by $N$:\n$$c_{\\mathrm{cmp}} + c_{\\mathrm{mov}} \\le \\frac{c_{\\mathrm{mov}} k}{2}$$\nNow, we solve for $k$:\n$$k \\ge \\frac{2(c_{\\mathrm{cmp}} + c_{\\mathrm{mov}})}{c_{\\mathrm{mov}}}$$\n$$k \\ge 2\\left(\\frac{c_{\\mathrm{cmp}}}{c_{\\mathrm{mov}}} + 1\\right)$$\nThe smallest value of $k$ for which Strategy B is preferable is given by this threshold. The problem asks for the single closed-form analytic expression for this leading-order threshold. This is the value at which the leading-order costs are equal.\n$$k_{\\text{threshold}} = 2\\left(\\frac{c_{\\mathrm{cmp}}}{c_{\\mathrm{mov}}} + 1\\right)$$\nThis result is independent of $N$ and consistent with the asymptotic regime $1 \\ll k \\ll N$, as the threshold $k$ is a constant, which for large $N$ satisfies $k \\ll N$. The condition $k \\gg 1$ depends on the ratio $c_{\\mathrm{cmp}}/c_{\\mathrm{mov}}$. For typical processor architectures where memory access ($c_{\\mathrm{mov}}$) is more expensive than a register/cache comparison ($c_{\\mathrm{cmp}}$), the ratio is small but the threshold can still be greater than 1.", "answer": "$$\\boxed{2\\left(\\frac{c_{\\mathrm{cmp}}}{c_{\\mathrm{mov}}} + 1\\right)}$$", "id": "3208573"}, {"introduction": "Having established the potential benefits of batch processing, we now focus on an elegant and efficient implementation. This exercise introduces the prefix sum (or scan) operation, a powerful parallel-friendly primitive, to solve the \"stream compaction\" problem: calculating the final positions of all elements that survive a batch deletion. You will derive the core index-mapping function and outline an algorithm that performs this task in linear time, showcasing a fundamental technique in data processing and parallel computing [@problem_id:3208395].", "problem": "You are given an abstract array $A$ of length $n$ with indices $0$ through $n-1$. A single batch deletion removes the elements of $A$ located at a specified set of indices $D \\subseteq \\{0,1,\\ldots,n-1\\}$ (duplicates in the specification of $D$ have no additional effect). After deleting these elements, the remaining elements of $A$ are compacted left in their original relative order to form a new array $A'$. For any original index $i \\in \\{0,1,\\ldots,n-1\\}$, define an index-mapping function that returns the position of $A[i]$ in $A'$ if $A[i]$ survives, or returns a sentinel if $A[i]$ is deleted.\n\nFundamental base. Use only the following core definitions to derive the mapping and the algorithm:\n- Array indices and order: an array is an ordered sequence; deletions preserve the relative order of surviving elements. \n- Indicator function: define $x[j] = 1$ if $j \\in D$ and $x[j] = 0$ otherwise for each $j \\in \\{0,1,\\ldots,n-1\\}$.\n- Prefix sum: for a sequence $x[0],x[1],\\ldots,x[n-1]$, a prefix sum array $S$ is defined by $S[i] = \\sum_{j=0}^{i} x[j]$ for each $i \\in \\{0,1,\\ldots,n-1\\}$.\n\nTask. Starting from the fundamental base above and no other specialized formulas, do the following.\n1. Derive a mathematically precise function that maps each original index $i$ to its new position after the deletions are applied and the array is compacted, or indicates that $i$ was deleted. You must express the mapping in terms of the indicator function and its prefix sums.\n2. Design an algorithm that constructs this mapping for a given array length $n$, a deletion set $D$ with size $k = |D|$, and a list of $q$ query indices $Q = [q_0,q_1,\\ldots,q_{q-1}]$. Your algorithm must run in time $O(n + k + q)$ and use $O(n)$ additional space. Use the integer $-1$ as the sentinel to indicate that an original index was deleted. You must justify the time complexity from first principles.\n3. Implement your algorithm as a complete program that evaluates the mapping for each provided test case below.\n\nInput constraints for each test case:\n- The array length $n$ is a positive integer.\n- The deletion list $D$ contains indices in $\\{0,1,\\ldots,n-1\\}$ and may include duplicates; only set membership matters.\n- The query list $Q$ contains indices in $\\{0,1,\\ldots,n-1\\}$.\n\nTest suite. Your program must compute the mapped indices for each test case’s query list $Q$, using $-1$ for deleted positions, in the following five cases:\n- Case $1$: $n = 10$, $D = [2,5,6]$, $Q = [0,1,2,3,6,7,9]$.\n- Case $2$: $n = 5$, $D = [0]$, $Q = [0,1]$.\n- Case $3$: $n = 5$, $D = [0,1,2,3,4]$, $Q = [0,2,4]$.\n- Case $4$: $n = 5$, $D = [3,3,1]$, $Q = [1,2,3,4]$.\n- Case $5$: $n = 1$, $D = []$, $Q = [0]$.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where the $i$-th element corresponds to the $i$-th test case and is itself the comma-separated list of mapped indices for that test case’s $Q$. For example, if there were two test cases with results $[a_0,a_1]$ and $[b_0,b_1,b_2]$, then the output would be $[[a_0,a_1],[b_0,b_1,b_2]]$ in a single line.", "solution": "The problem requires the derivation of an index-mapping function for array deletion, the design of an efficient algorithm based on this function, and its implementation. The derivation must be based on first principles as specified.\n\n**Part 1: Derivation of the Index-Mapping Function**\n\nLet $A$ be the original array of length $n$ with indices in $\\{0, 1, \\ldots, n-1\\}$. Let $D$ be the set of indices to be deleted. The problem defines an indicator sequence $x$ of length $n$ where $x[j] = 1$ if the index $j$ is in the deletion set $D$, and $x[j] = 0$ otherwise. We are tasked with finding a function, $\\text{map}(i)$, that gives the new index of the element originally at index $i$, or a sentinel value if it is deleted. The specified sentinel value is $-1$.\n\nFirst, consider an element at an original index $i$ that is to be deleted. This occurs if and only if $i \\in D$, which is equivalent to the condition $x[i] = 1$. For any such index, the mapping function must return the sentinel value.\n$$ \\text{if } x[i] = 1, \\text{then } \\text{map}(i) = -1 $$\n\nNext, consider an element at an original index $i$ that survives the deletion. This occurs if and only if $i \\notin D$, which is equivalent to $x[i] = 0$. After deletion, the remaining elements are compacted, preserving their relative order. The new index of the surviving element $A[i]$ is determined by the number of other surviving elements that appeared before it in the original array. The elements preceding $A[i]$ are those at indices $\\{0, 1, \\ldots, i-1\\}$. The total number of such elements is $i$.\n\nTo find the number of surviving elements before $A[i]$, we can count the number of *deleted* elements before $A[i]$ and subtract this from the total number of preceding elements, $i$. The number of deleted elements with an index $j < i$ is given by the sum of the indicator function values for those indices:\n$$ \\text{Number of deleted elements before } i = \\sum_{j=0}^{i-1} x[j] $$\nThe problem defines the prefix sum sequence $S$ as $S[k] = \\sum_{j=0}^{k} x[j]$ for $k \\in \\{0, 1, \\ldots, n-1\\}$. Using this definition, the number of deleted elements before index $i$ is $S[i-1]$ (where we can define $S[-1] = 0$ for the case $i=0$).\n\nThe new $0$-based index of $A[i]$ is the count of surviving elements before it. This count is:\n$$ \\text{New index} = (\\text{total elements before } i) - (\\text{deleted elements before } i) = i - \\sum_{j=0}^{i-1} x[j] $$\nFor $i>0$, this is $i - S[i-1]$. For $i=0$, the sum is empty and equals $0$, so the new index is $0-0=0$. Therefore, if $x[i]=0$, $\\text{map}(i) = i - S[i-1]$ (with $S[-1]=0$).\n\nAn equivalent and more compact formulation can be derived. The number of surviving elements up to and including index $i$ is the total number of elements $(i+1)$ minus the number of deleted elements up to and including index $i$, which is $S[i]$. So, the number of survivors in $\\{0, 1, \\ldots, i\\}$ is $(i+1) - S[i]$. If the element at index $i$ survives (i.e., $x[i]=0$), its new $0$-based index will be one less than this count.\n$$ \\text{if } x[i] = 0, \\text{then } \\text{map}(i) = ((i+1) - S[i]) - 1 = i - S[i] $$\nThis formulation holds for all $i \\in \\{0, \\ldots, n-1\\}$ without needing a special case for $S[-1]$. It is also equivalent to the previous formulation, since if $x[i]=0$, then $S[i] = S[i-1] + x[i] = S[i-1]$.\n\nCombining the cases for deletion and survival, the complete index-mapping function is:\n$$ \\text{map}(i) = \\begin{cases} i - S[i] & \\text{if } x[i] = 0 \\\\ -1 & \\text{if } x[i] = 1 \\end{cases} $$\nwhere $S[i] = \\sum_{j=0}^{i} x[j]$. This function is expressed purely in terms of the original index $i$, the indicator sequence $x$, and its prefix sums $S$, as required.\n\n**Part 2: Algorithm Design and Complexity Analysis**\n\nBased on the derived mapping function, we can design an algorithm to compute the mapping for a given array length $n$, a deletion list $D$ of size $k$, and a query list $Q$ of size $q$. The algorithm must operate within $O(n + k + q)$ time and $O(n)$ additional space.\n\nThe algorithm proceeds in three main stages:\n1.  **Construct Indicator and Prefix Sum Arrays:** First, we precompute the necessary information for the mapping: the indicator array $x$ and the prefix sum array $S$.\n    a.  Initialize an integer array `x_arr` of size $n$ with all elements set to $0$. This takes $O(n)$ time.\n    b.  Iterate through the input deletion list $D$, which has length $k$. For each index $d \\in D$, set `x_arr[d] = 1`. This step takes $O(k)$ time.\n    c.  Compute the prefix sum array `S_arr` of `x_arr`. This can be done by initializing `S_arr[0] = x_arr[0]` and then iterating from $i=1$ to $n-1$, computing `S_arr[i] = S_arr[i-1] + x_arr[i]`. This stage takes $O(n)$ time.\n\n2.  **Compute Full Index Map (Optional but Clean):** To answer queries efficiently, we can precompute the entire mapping from original to new indices.\n    a.  Create a mapping array `map_arr` of size $n$.\n    b.  Iterate from $i=0$ to $n-1$. For each index $i$:\n        i. If `x_arr[i] == 1`, set `map_arr[i] = -1`.\n        ii. If `x_arr[i] == 0`, set `map_arr[i] = i - S_arr[i]`.\n    This step takes $O(n)$ time.\n\n3.  **Process Queries:** Iterate through the query list $Q$, which has length $q$. For each query index $q_j \\in Q$, the result is simply `map_arr[q_j]`. This is an $O(1)$ lookup for each query. This stage takes a total of $O(q)$ time.\n\n**Complexity Analysis:**\n*   **Time Complexity:** The total time is the sum of the time for each step: $O(n)$ (for `x_arr` init) + $O(k)$ (for populating `x_arr`) + $O(n)$ (for `S_arr` computation) + $O(n)$ (for `map_arr` computation) + $O(q)$ (for queries). This sums to $O(n + k + q)$, which meets the problem's requirement.\n*   **Space Complexity:** The algorithm uses three arrays of size $n$: `x_arr`, `S_arr`, and `map_arr`. The total additional space is $O(n) + O(n) + O(n) = O(n)$, which also meets the requirement.\n\nThis algorithm correctly implements the derived mapping function and satisfies the given performance constraints.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the index mapping problem for a suite of test cases.\n    The core logic is implemented in the compute_mapping_for_case function.\n    \"\"\"\n\n    test_cases = [\n        # Case 1: n = 10, D = [2, 5, 6], Q = [0, 1, 2, 3, 6, 7, 9]\n        (10, [2, 5, 6], [0, 1, 2, 3, 6, 7, 9]),\n        # Case 2: n = 5, D = [0], Q = [0, 1]\n        (5, [0], [0, 1]),\n        # Case 3: n = 5, D = [0, 1, 2, 3, 4], Q = [0, 2, 4]\n        (5, [0, 1, 2, 3, 4], [0, 2, 4]),\n        # Case 4: n = 5, D = [3, 3, 1], Q = [1, 2, 3, 4]\n        (5, [3, 3, 1], [1, 2, 3, 4]),\n        # Case 5: n = 1, D = [], Q = [0]\n        (1, [], [0]),\n    ]\n\n    def compute_mapping_for_case(n, D, Q):\n        \"\"\"\n        Computes the new indices for a single test case based on the derived formula.\n        \n        Args:\n            n (int): The length of the abstract array.\n            D (list[int]): A list of original indices to delete.\n            Q (list[int]): A list of original indices to query.\n\n        Returns:\n            list[int]: A list of mapped indices, with -1 for deleted elements.\n        \"\"\"\n        \n        # Step 1: Create the indicator array `x`.\n        # `x[i] = 1` if index `i` is deleted, `0` otherwise.\n        # This takes O(n + |D|) time.\n        x = np.zeros(n, dtype=np.int32)\n        for del_idx in D:\n            if 0 <= del_idx < n:\n                x[del_idx] = 1\n\n        # Step 2: Compute the prefix sum array `S`.\n        # `S[i]` stores the number of deleted elements in the range `[0, i]`.\n        # np.cumsum is an efficient O(n) operation.\n        S = np.cumsum(x, dtype=np.int32)\n\n        # Step 3: Respond to queries.\n        # For each query index q, apply the mapping function:\n        # map(q) = -1 if x[q] == 1\n        # map(q) = q - S[q] if x[q] == 0\n        # This takes O(|Q|) time.\n        results = []\n        for q_idx in Q:\n            if x[q_idx] == 1:\n                results.append(-1)\n            else:\n                # The new index is the original index minus the number of\n                # deleted items up to that point.\n                new_idx = q_idx - S[q_idx]\n                results.append(new_idx)\n        \n        return results\n\n    all_results = []\n    for case in test_cases:\n        n, D, Q = case\n        result = compute_mapping_for_case(n, D, Q)\n        all_results.append(result)\n\n    # Format the final output string to match the problem specification exactly.\n    # Produces a string like \"[[r1_1,r1_2],[r2_1,r2_2],...]\"\n    inner_strings = [f\"[{','.join(map(str, res))}]\" for res in all_results]\n    final_output = f\"[{','.join(inner_strings)}]\"\n    \n    print(final_output)\n\nsolve()\n```", "id": "3208395"}]}