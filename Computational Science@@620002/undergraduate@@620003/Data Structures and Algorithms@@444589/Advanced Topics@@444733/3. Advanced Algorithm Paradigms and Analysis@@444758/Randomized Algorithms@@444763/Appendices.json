{"hands_on_practices": [{"introduction": "A cornerstone of analyzing Las Vegas algorithms is calculating the expected runtime. This exercise provides a foundational practice in this skill using the clear and intuitive model of a one-dimensional random walk. By setting up and solving a recurrence relation for the expected number of steps until termination, you will gain hands-on experience with a fundamental technique used to analyze the performance of many randomized algorithms [@problem_id:3263292].", "problem": "Consider a path graph with $n$ labeled nodes $\\{1,2,\\ldots,n\\}$. A robot implements a Las Vegas randomized algorithm (LVA), meaning it always produces a correct result but its runtime is a random variable determined by its random choices. The robot starts at node $i$ with $1 \\leq i \\leq n$. At each discrete time step, if it is at an interior node $j$ with $1jn$, it moves to node $j-1$ with probability $1/2$ and to node $j+1$ with probability $1/2$. Nodes $1$ and $n$ are absorbing: once the robot reaches either end, the algorithm terminates. Each move incurs exactly one unit of time. Treat the robot’s movement as an unbiased simple random walk on the path graph with absorbing boundaries.\n\nUsing only fundamental definitions from probability and randomized algorithms, derive a closed-form expression for the expected runtime of this Las Vegas algorithm, that is, the expected number of time steps until the robot first reaches node $1$ or node $n$, as a function of $n$ and $i$. Express your final answer as a single closed-form symbolic expression in terms of $n$ and $i$. No rounding is required and no physical units are to be included.", "solution": "The problem of determining the expected number of steps for a random walk on a path graph to reach an absorbing boundary is a classic problem in probability theory, often known as a form of the Gambler's Ruin problem. The problem statement is well-posed, scientifically grounded, and contains all necessary information for a formal derivation.\n\nLet $E_i$ be the expected number of time steps (moves) until the robot reaches either node $1$ or node $n$, given that it starts at node $i$, where $1 \\leq i \\leq n$.\n\nThe nodes $1$ and $n$ are absorbing states. If the robot starts at one of these nodes, the algorithm terminates immediately, having taken $0$ steps. Therefore, we have the boundary conditions:\n$$E_1 = 0$$\n$$E_n = 0$$\n\nFor any interior node $j$, where $1  j  n$, we can formulate a recurrence relation for $E_j$. The robot makes one move, which takes $1$ unit of time. From node $j$, it moves to node $j-1$ with probability $p = 1/2$ or to node $j+1$ with probability $1-p = 1/2$. After this first step, the expected number of additional steps from the new position is $E_{j-1}$ or $E_{j+1}$, respectively.\n\nBy the law of total expectation, the expected number of steps from node $j$ is the sum of the first step and the weighted average of the expected future steps:\n$$E_j = 1 + \\left(\\frac{1}{2}\\right)E_{j-1} + \\left(\\frac{1}{2}\\right)E_{j+1}$$\n\nThis equation holds for $j = 2, 3, \\ldots, n-1$. Our goal is to solve this system of linear equations for $E_i$ as a function of $i$ and $n$. We can rearrange the equation into the form of a second-order linear non-homogeneous difference equation. Multiplying by $2$ gives:\n$$2E_j = 2 + E_{j-1} + E_{j+1}$$\nRearranging the terms, we get:\n$$E_{j+1} - 2E_j + E_{j-1} = -2$$\n\nTo solve this equation, we first find the general solution to the corresponding homogeneous equation:\n$$E_{j+1} - 2E_j + E_{j-1} = 0$$\nThe characteristic equation is obtained by substituting $E_j = r^j$:\n$$r^{j+1} - 2r^j + r^{j-1} = 0$$\nDividing by $r^{j-1}$ (assuming $r \\neq 0$), we get:\n$$r^2 - 2r + 1 = 0$$\n$$(r-1)^2 = 0$$\nThis equation has a repeated root $r=1$. For a repeated root, the general solution to the homogeneous equation is of the form:\n$$E_j^{(h)} = A \\cdot (1)^j + B \\cdot j \\cdot (1)^j = A + Bj$$\nwhere $A$ and $B$ are constants.\n\nNext, we find a particular solution to the non-homogeneous equation $E_{j+1} - 2E_j + E_{j-1} = -2$. Since the right-hand side is a constant, and the homogeneous solution contains a constant term ($A$) and a linear term ($Bj$), we must try a particular solution of the form $E_j^{(p)} = Cj^2$. Substituting this into the non-homogeneous equation:\n$$C(j+1)^2 - 2C(j^2) + C(j-1)^2 = -2$$\n$$C(j^2+2j+1) - 2Cj^2 + C(j^2-2j+1) = -2$$\n$$Cj^2 + 2Cj + C - 2Cj^2 + Cj^2 - 2Cj + C = -2$$\n$$2C = -2$$\n$$C = -1$$\nSo, a particular solution is $E_j^{(p)} = -j^2$.\n\nThe general solution for $E_j$ is the sum of the homogeneous and particular solutions:\n$$E_j = E_j^{(h)} + E_j^{(p)} = A + Bj - j^2$$\n\nNow, we use the boundary conditions $E_1=0$ and $E_n=0$ to determine the constants $A$ and $B$.\nFor $j=1$:\n$$E_1 = A + B(1) - (1)^2 = 0 \\implies A + B = 1$$\nFor $j=n$:\n$$E_n = A + B(n) - (n)^2 = 0 \\implies A + Bn = n^2$$\n\nWe have a system of two linear equations for $A$ and $B$:\n1) $A + B = 1$\n2) $A + Bn = n^2$\n\nFrom equation (1), we have $A = 1 - B$. Substituting this into equation (2):\n$$(1-B) + Bn = n^2$$\n$$1 + B(n-1) = n^2$$\n$$B(n-1) = n^2 - 1$$\n$$B(n-1) = (n-1)(n+1)$$\nSince the path graph has at least two nodes ($1$ and $n$), we have $n \\ge 2$, so $n-1 \\neq 0$. We can divide by $(n-1)$:\n$$B = n+1$$\n\nNow, we find $A$:\n$$A = 1 - B = 1 - (n+1) = -n$$\n\nSubstituting the values of $A$ and $B$ back into the general solution, we obtain the expression for the expected runtime $E_j$:\n$$E_j = -n + (n+1)j - j^2$$\n\nThe problem asks for the solution in terms of the starting node $i$, so we replace $j$ with $i$:\n$$E_i = (n+1)i - i^2 - n$$\nThis expression can be simplified by factoring:\n$$E_i = ni + i - i^2 - n$$\n$$E_i = n(i-1) - i^2 + i$$\n$$E_i = n(i-1) - i(i-1)$$\n$$E_i = (n-i)(i-1)$$\n\nThis is the final closed-form expression for the expected number of steps starting from node $i$.", "answer": "$$\n\\boxed{(n-i)(i-1)}\n$$", "id": "3263292"}, {"introduction": "While Las Vegas algorithms are always correct, Monte Carlo algorithms trade certainty for speed, introducing a probability of error. A crucial aspect of algorithm design is understanding the robustness of these error bounds, especially under adversarial conditions. This problem challenges you to analyze Freivalds' algorithm for matrix product verification—a classic Monte Carlo method—in a scenario where an adversary can manipulate the random input, providing a sharp insight into the algorithm's failure modes and the importance of its underlying assumptions [@problem_id:3263395].", "problem": "Let $A$, $B$, and $C$ be $n \\times n$ matrices over the integers with $n \\geq 2$. Consider the classical matrix product verification test commonly known as Freivalds' algorithm, which is a Monte Carlo (MC) randomized algorithm: draw a random vector $r \\in \\{0,1\\}^{n}$ uniformly at random, and accept the claim $AB = C$ if and only if $A(Br) = Cr$. Define the error event for a single run of the test as accepting when $AB \\neq C$. Let $D = AB - C$, so that the test accepts if and only if $D r = 0$. In the standard, uncorrupted setting, the error probability for a single run is known to be bounded above by a fixed constant less than $1$.\n\nNow suppose that, after you draw $r$, an adversary who observes $A$, $B$, $C$, and $r$ is allowed to corrupt the random vector by changing the value of at most one coordinate of $r$, producing a corrupted vector $r' \\in \\{0,1\\}^{n}$ satisfying $|\\{i \\in \\{1,\\dots,n\\} : r'_{i} \\neq r_{i}\\}| \\leq 1$. The algorithm then proceeds using $r'$ and accepts if and only if $D r' = 0$. The adversary acts to maximize the error probability.\n\nAssuming $AB \\neq C$ (equivalently, $D \\neq 0$), determine the worst-case error probability of a single run of the corrupted test over the random choice of $r$. Express your final answer as a single real number. No rounding is required.", "solution": "Let the given $n \\times n$ integer matrices be $A$, $B$, and $C$. We are given that $AB \\neq C$. Let $D = AB - C$. Since $AB \\neq C$, the matrix $D$ is a non-zero matrix, $D \\neq 0$. The entries of $D$ are integers.\n\nThe algorithm draws a random vector $r \\in \\{0,1\\}^{n}$ uniformly at random. There are $2^n$ such vectors, each with probability $\\frac{1}{2^n}$.\n\nAfter $r$ is chosen, an adversary, who knows $D$ and $r$, can corrupt $r$ into a new vector $r'$. The corruption is limited such that $r'$ differs from $r$ in at most one coordinate. This means the Hamming distance between $r$ and $r'$ is at most $1$. Let $S(r)$ be the set of vectors the adversary can choose from:\n$$S(r) = \\{ x \\in \\{0,1\\}^n : |\\{i \\in \\{1,\\dots,n\\} : x_i \\neq r_i \\}| \\leq 1 \\}$$\nThis set consists of the vector $r$ itself (0 changes) and all vectors obtained by flipping exactly one bit of $r$ (1 change). There are $n$ such vectors. Thus, $|S(r)| = n+1$.\n\nThe algorithm accepts if $Dr' = 0$. Since we assume $D \\neq 0$ ($AB \\neq C$), an acceptance is an error. The adversary's goal is to choose an $r' \\in S(r)$ such that $Dr' = 0$, if such a vector exists.\n\nAn error occurs for a given random vector $r$ if the adversary can force acceptance. This happens if there exists at least one vector $r' \\in S(r)$ such that $Dr' = 0$.\nThe error event $E$ is the set of all such random vectors $r$ for which the adversary can succeed:\n$$E = \\{r \\in \\{0,1\\}^n \\mid \\exists r' \\in S(r), Dr' = 0\\}$$\nThe probability of error for a given matrix $D$ is $P(\\text{error} | D) = \\frac{|E|}{2^n}$.\n\nThe problem asks for the worst-case error probability. This is the maximum possible error probability over all valid choices for the non-zero matrix $D$.\n$$\\text{Worst-case error probability} = \\sup_{D \\neq 0} P(\\text{error} | D)$$\n\nTo find this supremum, we can construct a specific non-zero $n \\times n$ integer matrix $D_0$ and calculate the error probability for it. If we can show this probability is $1$, then we have found the maximum possible value.\n\nLet us construct such a matrix $D_0$. Let $n \\ge 2$. Consider the matrix $D_0$ which has its first column equal to the first standard basis vector $e_1$, and all other columns equal to the zero vector.\n$$d_{0,1} = e_1 = \\begin{pmatrix} 1 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{pmatrix}, \\quad d_{0,j} = \\mathbf{0} \\quad \\text{for } j \\in \\{2, \\ldots, n\\}$$\nThis matrix $D_0$ is non-zero, and its entries are integers. Such a matrix can be formed from $A, B, C$ over integers, for example, by setting $A=e_1e_1^T$, $B=I$ (the identity matrix), and $C=0$ (the zero matrix). Then $D_0 = AB-C = (e_1e_1^T)I - 0 = e_1e_1^T$.\n\nNow, let's analyze the condition $D_0r' = 0$ for an arbitrary vector $r' = (r'_1, r'_2, \\ldots, r'_n)^T \\in \\{0,1\\}^n$.\n$$D_0 r' = \\sum_{j=1}^n r'_j d_{0,j} = r'_1 d_{0,1} + \\sum_{j=2}^n r'_j d_{0,j} = r'_1 e_1 + \\sum_{j=2}^n r'_j \\mathbf{0} = r'_1 e_1$$\nSince $e_1$ is not the zero vector, the condition $D_0r' = 0$ is equivalent to its scalar coefficient being zero:\n$$r'_1 e_1 = 0 \\iff r'_1 = 0$$\n\nSo, for this specific matrix $D_0$, the adversary's task simplifies: given $r$, find an $r' \\in S(r)$ such that the first component of $r'$, $r'_1$, is $0$.\n\nLet's examine if this is always possible for any randomly chosen initial vector $r \\in \\{0,1\\}^n$. We consider the two possibilities for the first component of $r$, $r_1$.\n\nCase 1: $r_1 = 0$.\nThe initial random vector $r$ has a zero in its first component. The adversary can simply choose not to corrupt the vector, i.e., choose $r' = r$. This is a valid choice since $r' \\in S(r)$ (zero coordinates changed). For this choice, $r'_1 = r_1 = 0$, so $D_0r' = 0$. The adversary succeeds.\n\nCase 2: $r_1 = 1$.\nThe initial random vector $r$ has a one in its first component. The adversary can choose to corrupt $r$ by flipping its first bit. Let this new vector be $r'$.\n$$r' = (1-r_1, r_2, \\ldots, r_n)^T = (0, r_2, \\ldots, r_n)^T$$\nThis vector $r'$ differs from $r$ in exactly one position (the first one). Thus, $r' \\in S(r)$ and is a valid choice for the adversary.\nThe first component of this $r'$ is $r'_1 = 0$. Therefore, $D_0r' = 0$. The adversary succeeds.\n\nIn both cases, regardless of the initial random vector $r$, the adversary is always able to select a vector $r' \\in S(r)$ that satisfies the acceptance condition $D_0r' = 0$.\nThis means that for our chosen matrix $D_0$, the error event $E$ includes all possible random vectors $r$.\n$$E = \\{r \\in \\{0,1\\}^n \\mid \\exists r' \\in S(r), D_0r' = 0\\} = \\{0,1\\}^n$$\nThe number of elements in $E$ is $|E| = 2^n$.\n\nThe error probability for this specific matrix $D_0$ is:\n$$P(\\text{error} | D_0) = \\frac{|E|}{2^n} = \\frac{2^n}{2^n} = 1$$\n\nWe have found a valid non-zero matrix $D_0$ for which the error probability is $1$. Since a probability cannot exceed $1$, this must be the maximum possible error probability.\nTherefore, the worst-case error probability is $1$.", "answer": "$$\\boxed{1}$$", "id": "3263395"}, {"introduction": "This practice moves from analysis to practical design and implementation, tackling the important problem of detecting negative-weight cycles in a graph. You are tasked with developing a Las Vegas algorithm that leverages randomization to achieve better expected performance than its classic deterministic counterpart, the Bellman-Ford algorithm. This exercise requires you to synthesize concepts of correctness from deterministic algorithms with the performance-enhancing potential of randomness, culminating in a concrete implementation that solves a common computational problem [@problem_id:3263300].", "problem": "You are given a finite weighted directed graph specified by a vertex set $V$ and an edge multiset $E \\subseteq V \\times V \\times \\mathbb{R}$, where each edge $(u,v,w) \\in E$ has a real-valued weight $w$. A cycle is a sequence of vertices $(v_0,v_1,\\dots,v_k)$ with $k \\ge 1$ such that $(v_i,v_{i+1},w_i) \\in E$ for all $i \\in \\{0,1,\\dots,k-1\\}$ and $v_0 = v_k$. A cycle is negative if $\\sum_{i=0}^{k-1} w_i  0$. The classical deterministic Bellman-Ford algorithm detects a negative cycle reachable from a source in time that is upper bounded by $O(|V||E|)$.\n\nDesign and implement a Las Vegas randomized algorithm (always correct, randomness only affects runtime) that detects whether there exists any negative cycle in the graph (in any weakly connected component), using expected time faster than the deterministic Bellman-Ford algorithm on typical sparse inputs. Use a queue-based randomized relaxation strategy that processes vertices in a randomized order and visits outgoing edges in a randomized order. To ensure detection of negative cycles in any component (not only those reachable from a particular source), use the super-source initialization model: initialize distances $d(v)$ for all $v \\in V$ to $0$, and place all vertices into the initial processing queue. Maintain an array `relax_count(v)` counting the number of times $d(v)$ strictly decreases. If some `relax_count(v)` reaches $|V|$, declare that a negative cycle exists. This termination condition is required to ensure correctness while preventing non-termination.\n\nBase your derivation and design on the following fundamental definitions and well-tested facts:\n- A weighted directed graph $G=(V,E)$ is defined as above.\n- A negative cycle is a cycle whose total weight is less than $0$.\n- The Bellman-Ford algorithm performs repeated relaxations and detects a negative cycle if an improvement occurs in the $|V|$-th pass. A relaxation reduces a tentative distance estimate.\n- A Las Vegas algorithm always outputs a correct answer; its expected runtime is measured over its internal randomness.\n- A Monte Carlo algorithm may have a nonzero probability of error; its runtime is typically bounded.\n\nYour program must implement the described Las Vegas randomized algorithm and produce the required outputs on the following test suite. In each test case, vertices are numbered consecutively as integers from $0$ to $n-1$ and edges are given as triples $(u,v,w)$ with $u,v \\in \\{0,1,\\dots,n-1\\}$ and $w \\in \\mathbb{R}$.\n\nTest Suite:\n- Case A (general negative cycle): $n=5$, $E=\\{(0,1,1.0),(1,2,1.0),(2,3,1.0),(3,1,-4.0),(0,4,2.0),(4,3,2.0)\\}$. This has a negative cycle $1 \\to 2 \\to 3 \\to 1$ with total weight $1.0 + 1.0 - 4.0 = -2.0$.\n- Case B (no negative cycle, directed acyclic core): $n=4$, $E=\\{(0,1,2.0),(1,2,2.0),(2,3,2.0),(0,3,7.0)\\}$. No cycle exists.\n- Case C (self-loop negative cycle): $n=1$, $E=\\{(0,0,-1.0)\\}$. The edge is a single negative cycle of weight $-1.0$.\n- Case D (disconnected graph with a negative cycle in a separate component): $n=5$, $E=\\{(0,1,1.0),(1,2,1.0),(3,4,-2.0),(4,3,-2.0)\\}$. Component $\\{3,4\\}$ contains a negative cycle $3 \\to 4 \\to 3$ with total weight $-4.0$, while component $\\{0,1,2\\}$ has no cycle.\n- Case E (long chain, no cycles): $n=10$, $E=\\{(0,1,1.0),(1,2,1.0),(2,3,1.0),(3,4,1.0),(4,5,1.0),(5,6,1.0),(6,7,1.0),(7,8,1.0),(8,9,1.0)\\}$. This graph is acyclic.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result\\_A,result\\_B,result\\_C,result\\_D,result\\_E]$), where each $result\\_X$ is a boolean indicating whether a negative cycle was detected in the corresponding case. The required output types for each test case are booleans. No physical units or angle units are involved in this problem, and no percentages are required. Ensure your algorithm is randomized by using a random initial order of vertices and random order of edge relaxations, and ensure that it terminates with the correct boolean answer for every test case.", "solution": "The problem statement has been validated and is deemed sound. It is scientifically grounded, well-posed, and contains all necessary information to formulate and implement a solution. The definitions of graphs, cycles, and negative cycles are standard. The described randomized algorithm is a valid variant of the Shortest Path Faster Algorithm (SPFA), and its correctness relies on the well-established principles of the Bellman-Ford algorithm.\n\nThe core task is to design a Las Vegas randomized algorithm to detect the presence of any negative-weight cycles in a given weighted directed graph $G=(V,E)$. A negative cycle is a path $(v_0, v_1, \\dots, v_{k-1}, v_k=v_0)$ where the sum of edge weights $\\sum_{i=0}^{k-1} w(v_i, v_{i+1})$ is less than $0$.\n\nThe fundamental principle for detecting negative cycles originates from the Bellman-Ford algorithm. In a graph with $|V|$ vertices and no negative cycles, the shortest path from any source vertex to any other vertex contains at most $|V|-1$ edges. The Bellman-Ford algorithm iteratively relaxes edges, and after $|V|-1$ passes, it finds all such shortest paths. If a distance estimate can still be improved on the $|V|$-th pass, it implies the existence of a path with $|V|$ edges that is \"shorter\" than any path with fewer edges. Such a path must contain a cycle, and for its total weight to enable a further decrease in distance, this cycle must have a negative total weight.\n\nTo detect a negative cycle anywhere in the graph, not just those reachable from a specific source, we employ the \"super-source\" model. This is conceptually equivalent to adding a new vertex $s$ with a zero-weight directed edge to every vertex $v \\in V$. Running a shortest path algorithm from $s$ would then explore all parts of the graph. In practice, this is implemented by initializing the distance estimate for every vertex $v \\in V$ to $0$, i.e., $d(v) \\leftarrow 0$, and placing all vertices into the initial set of vertices to be processed.\n\nThe specified algorithm is a queue-based, randomized variant of Bellman-Ford. Unlike the deterministic Bellman-Ford which relaxes all $|E|$ edges in each of its $|V|-1$ passes, the queue-based approach only re-processes vertices whose distance estimates have been successfully relaxed (decreased). This can lead to significant performance gains on average. The algorithm maintains a queue of vertices that are candidates for relaxing their neighbors.\n\nThe algorithm proceeds as follows:\n$1$. **Initialization**: For each vertex $v \\in V$, initialize its distance estimate $d(v) \\leftarrow 0$ and a relaxation counter $\\text{relax\\_count}(v) \\leftarrow 0$. Create a queue and add all vertices from $V$ to it. To introduce randomness as required, the set of all vertices $V$ is shuffled before being added to the queue. An auxiliary data structure (e.g., a hash set) is used to keep track of which vertices are currently in the queue to avoid duplicates and allow for constant-time membership checking.\n\n$2$. **Randomized Relaxation Loop**: While the queue is not empty, perform the following steps:\n    a. Dequeue a vertex $u$.\n    b. The list of outgoing edges from $u$ is retrieved. To satisfy the second requirement for randomization, this list of edges is shuffled.\n    c. For each outgoing edge $(u, v, w)$ in the shuffled list, a relaxation attempt is made. If $d(u) + w  d(v)$, the condition for relaxation is met.\n    d. If relaxation occurs, update $d(v) \\leftarrow d(u) + w$ and increment the relaxation counter $\\text{relax\\_count}(v) \\leftarrow \\text{relax\\_count}(v) + 1$.\n    e. **Termination Check**: Immediately after incrementing $\\text{relax\\_count}(v)$, check if $\\text{relax\\_count}(v) \\ge |V|$. If this condition is met, it serves as conclusive proof of a negative cycle's existence. The algorithm terminates and reports `True`.\n    f. If a relaxation occurred and the algorithm has not terminated, vertex $v$ must be processed again as its new, shorter distance might lead to further relaxations of its neighbors. If $v$ is not already in the queue, it is enqueued.\n\n$3$. **Termination (No Cycle Found)**: If the main loop completes (i.e., the queue becomes empty), it means a stable state has been reached where for all edges $(u, v, w)$, the condition $d(v) \\le d(u) + w$ holds. Since no vertex had its relaxation counter reach $|V|$, it is guaranteed that no negative cycles exist in the graph. The algorithm terminates and reports `False`.\n\nThis algorithm is of the Las Vegas type because its correctness is guaranteed. If it reports a negative cycle, the condition on $\\text{relax\\_count}(v)$ provides a definitive proof. If it reports no negative cycle, it's because the relaxation process converged, which is only possible in their absence. The randomization of vertex and edge processing order affects only the sequence of operations and thus the runtime, not the final outcome. The expected runtime is often better than the worst-case $O(|V||E|)$ of the standard Bellman-Ford algorithm, especially for sparse graphs where worst-case structures for SPFA-like algorithms are not present.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport collections\n\ndef solve():\n    \"\"\"\n    Solves the negative cycle detection problem for a suite of test cases\n    using a randomized Las Vegas algorithm.\n    \"\"\"\n\n    def detect_negative_cycle_randomized(n, edges, rng):\n        \"\"\"\n        Implements a randomized Las Vegas algorithm to detect negative cycles.\n\n        This algorithm is a queue-based variant of Bellman-Ford (similar to SPFA).\n        It uses a \"super-source\" model by initializing all distances to 0.\n        Randomness is introduced by shuffling the initial vertex processing order\n        and shuffling the outgoing edges for each processed vertex.\n\n        A negative cycle is detected if any vertex's distance is relaxed n or more times.\n\n        Args:\n            n (int): The number of vertices, |V|.\n            edges (list): A list of tuples (u, v, w) representing edges.\n            rng (np.random.Generator): A random number generator for shuffling.\n\n        Returns:\n            bool: True if a negative cycle is detected, False otherwise.\n        \"\"\"\n        if n == 0:\n            return False\n\n        # Adjacency list representation of the graph\n        adj = collections.defaultdict(list)\n        for u, v, w in edges:\n            adj[u].append((v, w))\n\n        # Initialize distances to 0 (super-source model)\n        dist = np.zeros(n, dtype=float)\n\n        # Count the number of times each vertex's distance is relaxed\n        relax_count = np.zeros(n, dtype=int)\n\n        # Queue for vertices to be processed\n        queue = collections.deque()\n        \n        # Set for O(1) checking of queue membership\n        in_queue = set()\n\n        # Initial population of the queue with all vertices in a random order\n        initial_nodes = list(range(n))\n        rng.shuffle(initial_nodes)\n        for i in initial_nodes:\n            queue.append(i)\n            in_queue.add(i)\n\n        while queue:\n            u = queue.popleft()\n            in_queue.remove(u)\n\n            # Randomize the order of edge relaxation\n            outgoing_edges = adj[u]\n            rng.shuffle(outgoing_edges)\n\n            for v, w in outgoing_edges:\n                # Relaxation step\n                if dist[u] + w  dist[v]:\n                    dist[v] = dist[u] + w\n                    relax_count[v] += 1\n\n                    # Check for negative cycle detection\n                    # A path can have at most n-1 edges without a cycle.\n                    # If a vertex is relaxed n times, it implies a path of\n                    # length n, which must contain a negative cycle.\n                    if relax_count[v] >= n:\n                        return True\n\n                    if v not in in_queue:\n                        queue.append(v)\n                        in_queue.add(v)\n        \n        # If the queue is empty and no cycle was detected, none exists.\n        return False\n\n    # Instantiate a random number generator. Seeding is not required by the problem.\n    rng = np.random.default_rng()\n\n    # Test Suite\n    test_cases = {\n        'A': {\n            \"n\": 5, \n            \"edges\": [(0, 1, 1.0), (1, 2, 1.0), (2, 3, 1.0), (3, 1, -4.0), (0, 4, 2.0), (4, 3, 2.0)]\n        },\n        'B': {\n            \"n\": 4, \n            \"edges\": [(0, 1, 2.0), (1, 2, 2.0), (2, 3, 2.0), (0, 3, 7.0)]\n        },\n        'C': {\n            \"n\": 1, \n            \"edges\": [(0, 0, -1.0)]\n        },\n        'D': {\n            \"n\": 5, \n            \"edges\": [(0, 1, 1.0), (1, 2, 1.0), (3, 4, -2.0), (4, 3, -2.0)]\n        },\n        'E': {\n            \"n\": 10, \n            \"edges\": [(0, 1, 1.0), (1, 2, 1.0), (2, 3, 1.0), (3, 4, 1.0), (4, 5, 1.0), \n                      (5, 6, 1.0), (6, 7, 1.0), (7, 8, 1.0), (8, 9, 1.0)]\n        }\n    }\n    \n    results = []\n    # The order of execution must match the problem statement's order for the final output.\n    case_order = ['A', 'B', 'C', 'D', 'E']\n    for case_id in case_order:\n        case = test_cases[case_id]\n        n, edges = case[\"n\"], case[\"edges\"]\n        result = detect_negative_cycle_randomized(n, edges, rng)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3263300"}]}