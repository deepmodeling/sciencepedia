{"hands_on_practices": [{"introduction": "Many algorithmic problems have intuitive, \"greedy\" solutions. This exercise serves as a crucial cautionary tale, demonstrating that a plausible-sounding heuristic may perform abysmally in the worst case. By analyzing a \"reverse greedy\" strategy for the Knapsack problem, you will learn to construct a worst-case instance that reveals the algorithm's fundamental flaws and pushes its approximation ratio to the lowest possible value. [@problem_id:3207612]", "problem": "Consider the standard $0$-$1$ knapsack problem: given $n$ items, where item $i$ has positive value $v_i$ and positive weight $w_i$, and a knapsack capacity $C0$, the goal is to choose a subset $S \\subseteq \\{1,\\dots,n\\}$ maximizing $\\sum_{i \\in S} v_i$ subject to $\\sum_{i \\in S} w_i \\le C$. Let the value-to-weight ratio of item $i$ be $r_i = \\frac{v_i}{w_i}$.\n\nAnalyze the following \"reverse greedy\" algorithm for $0$-$1$ knapsack: start with the set $S$ containing all items, and iteratively remove one item at a time, always removing an item with the smallest current ratio $r_i$ among the remaining items in $S$, until the total weight $\\sum_{i \\in S} w_i \\le C$. The algorithm then returns the remaining set $S$.\n\nUsing the core definitions of $0$-$1$ knapsack, value-to-weight ratio, and approximation ratio, derive the worst-case approximation ratio of this algorithm. The approximation ratio of an algorithm is defined as $\\inf_{I} \\frac{\\mathrm{ALG}(I)}{\\mathrm{OPT}(I)}$ (a value between 0 and 1 for maximization problems), where $\\mathrm{ALG}(I)$ is the value produced by the algorithm on instance $I$ and $\\mathrm{OPT}(I)$ is the optimal value for $I$. Your derivation must start from these definitions and construct a scientifically sound family of instances demonstrating the bound. Express your final answer as a single real number. No rounding is required.", "solution": "The problem statement is subjected to validation.\n\n### Step 1: Extract Givens\n- **Problem Type**: $0$-$1$ knapsack problem.\n- **Inputs**: A set of $n$ items, where item $i \\in \\{1, \\dots, n\\}$ has a positive value $v_i > 0$ and a positive weight $w_i > 0$. A knapsack capacity $C > 0$.\n- **Objective**: Maximize the total value $\\sum_{i \\in S} v_i$ for a subset of items $S$ subject to the constraint $\\sum_{i \\in S} w_i \\le C$.\n- **Definition**: The value-to-weight ratio for item $i$ is $r_i = \\frac{v_i}{w_i}$.\n- **Algorithm to Analyze (\"Reverse Greedy\")**:\n    1. Initialize the solution set $S$ to contain all items, $S = \\{1, \\dots, n\\}$.\n    2. While the total weight of items in $S$ exceeds the capacity, i.e., $\\sum_{i \\in S} w_i  C$:\n        a. Identify an item $j \\in S$ that has the minimum value-to-weight ratio among all items currently in $S$, so $r_j = \\min_{k \\in S} \\{r_k\\}$.\n        b. Remove item $j$ from $S$.\n    3. The algorithm returns the final set $S$.\n- **Evaluation Metric**: The worst-case approximation ratio, defined as $\\inf_{I} \\frac{\\mathrm{ALG}(I)}{\\mathrm{OPT}(I)}$, where $I$ represents an instance of the problem, $\\mathrm{ALG}(I)$ is the value obtained by the reverse greedy algorithm, and $\\mathrm{OPT}(I)$ is the optimal value. The problem specifies this ratio is in $[0, 1]$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientific Grounding**: The problem is situated within the well-established field of theoretical computer science, specifically the analysis of approximation algorithms. The $0$-$1$ knapsack problem is a canonical NP-hard optimization problem. All concepts used—value, weight, capacity, approximation ratio—are standard and rigorously defined. The proposed \"reverse greedy\" algorithm is a plausible heuristic whose performance can be analyzed mathematically. The problem is scientifically sound.\n- **Well-Posedness**: The problem is clearly stated. It specifies the algorithm's logic, the objective function, the constraints, and the precise definition of the quantity to be derived (the worst-case approximation ratio). A unique, meaningful numerical answer is expected.\n- **Objectivity**: The problem is described using precise, formal language devoid of subjective or ambiguous terminology.\n- **Conclusion**: The problem is self-contained, consistent, and formally structured. It does not violate any of the invalidity criteria.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A full solution will be provided.\n\n### Solution Derivation\nLet $\\mathrm{ALG}(I)$ be the total value of the items selected by the reverse greedy algorithm for a given problem instance $I$, and let $\\mathrm{OPT}(I)$ be the maximum possible value for that instance (the optimal solution). The approximation ratio of the algorithm is the infimum of the ratio $\\frac{\\mathrm{ALG}(I)}{\\mathrm{OPT}(I)}$ over all possible instances $I$.\n\nTo determine the worst-case approximation ratio, we seek to construct a family of instances for which this ratio is minimized. The reverse greedy algorithm's strategy is to preserve items with high value-to-weight ratios ($r_i = v_i/w_i$) by removing those with low ratios first. A failure case for this algorithm would likely involve a scenario where an item with a low ratio is nonetheless extremely valuable and is a crucial part of the optimal solution. The algorithm, by its design, might discard this item.\n\nConsider the following family of problem instances, parameterized by a small positive real number $\\epsilon$. Let the knapsack capacity $C$ be any value greater than $1$. The instance $I_\\epsilon$ consists of two items:\n\n- **Item 1**: $v_1 = 1$, $w_1 = C$.\n- **Item 2**: $v_2 = \\epsilon$, $w_2 = \\epsilon$.\n\nWe require $v_i > 0$, $w_i > 0$, and $C > 0$. By choosing $C > 1$ and $0  \\epsilon  1$, these conditions are satisfied.\n\nLet us analyze the behavior of the reverse greedy algorithm and determine the optimal solution for this instance $I_\\epsilon$.\n\n**1. Algorithm's Performance ($\\mathrm{ALG}(I_\\epsilon)$):**\nFirst, we calculate the value-to-weight ratios for the two items:\n- $r_1 = \\frac{v_1}{w_1} = \\frac{1}{C}$\n- $r_2 = \\frac{v_2}{w_2} = \\frac{\\epsilon}{\\epsilon} = 1$\n\nSince we chose $C > 1$, it follows that $\\frac{1}{C}  1$, and therefore $r_1  r_2$.\n\nThe algorithm starts with the set $S = \\{1, 2\\}$. The total weight is $W_{total} = w_1 + w_2 = C + \\epsilon$.\nSince $W_{total} > C$, the algorithm must remove an item. According to its definition, it removes the item with the smallest ratio. As $r_1  r_2$, item $1$ is removed.\n\nThe resulting set is $S_{ALG} = \\{2\\}$. The total weight of this set is $w_2 = \\epsilon$. Since we chose $\\epsilon  1$ and $C > 1$, we have $w_2  C$, so the weight constraint is satisfied, and the algorithm terminates.\nThe value obtained by the algorithm is the value of item $2$:\n$$ \\mathrm{ALG}(I_\\epsilon) = v_2 = \\epsilon $$\n\n**2. Optimal Solution ($\\mathrm{OPT}(I_\\epsilon)$):**\nWe now find the optimal solution by considering all feasible subsets of items.\n- **Subset {1}**: The weight is $w_1 = C$, which satisfies the capacity constraint $w_1 \\le C$. The value is $v_1 = 1$. This is a feasible solution.\n- **Subset {2}**: The weight is $w_2 = \\epsilon$. Since $\\epsilon  C$, this is also a feasible solution. The value is $v_2 = \\epsilon$.\n- **Subset {1, 2}**: The weight is $w_1 + w_2 = C + \\epsilon$, which is greater than $C$. This subset is not feasible.\n- **Subset $\\emptyset$**: The value is $0$.\n\nThe optimal value is the maximum value among all feasible subsets:\n$$ \\mathrm{OPT}(I_\\epsilon) = \\max(\\{1, \\epsilon\\}) $$\nSince we chose $0  \\epsilon  1$, the maximum value is $1$.\n$$ \\mathrm{OPT}(I_\\epsilon) = 1 $$\n\n**3. Approximation Ratio Calculation:**\nFor the instance $I_\\epsilon$, the ratio is:\n$$ \\frac{\\mathrm{ALG}(I_\\epsilon)}{\\mathrm{OPT}(I_\\epsilon)} = \\frac{\\epsilon}{1} = \\epsilon $$\n\nWe are looking for the worst-case approximation ratio, which is the infimum of this ratio over all possible instances. Our constructed family of instances $I_\\epsilon$ shows that for any arbitrarily small positive number $\\delta$, we can choose $\\epsilon = \\delta$ (provided $\\delta  1$) to create an instance where the approximation ratio is $\\delta$.\n\nFormally, the set of possible ratios for our family of instances is $\\{ \\epsilon \\mid 0  \\epsilon  1 \\}$. The infimum of this set is:\n$$ \\inf_{\\epsilon \\in (0,1)} \\{\\epsilon\\} = 0 $$\nSince the approximation ratio for any instance must be non-negative (as all values $v_i$ are positive), and we have shown that the ratio can be made arbitrarily close to $0$, the worst-case approximation ratio of the algorithm is $0$.\n$$ \\inf_I \\frac{\\mathrm{ALG}(I)}{\\mathrm{OPT}(I)} = 0 $$\nThis demonstrates that there is no constant $c > 0$ for which $\\mathrm{ALG}(I) \\ge c \\cdot \\mathrm{OPT}(I)$ holds for all instances $I$. The algorithm can perform arbitrarily poorly compared to the optimal solution.", "answer": "$$\\boxed{0}$$", "id": "3207612"}, {"introduction": "Unlike the flawed heuristic in our previous exercise, the standard greedy algorithm for Set Cover is a classic example of a successful approximation algorithm whose performance guarantee is a cornerstone result in the field. This practice challenges you to engage with the celebrated tight analysis of this algorithm, related to the harmonic number $H_m$. You will work through the specific family of instances that proves this guarantee is the best possible, demonstrating that the algorithm's performance can indeed approach this theoretical limit. [@problem_id:3207647]", "problem": "You will analyze the greedy algorithm for the Weighted Set Cover problem to construct a family of instances on which its approximation ratio matches, in the limit, the $m$-th harmonic number. Begin from the foundational definitions.\n\nDefinitions:\n- In Weighted Set Cover, you are given a finite universe $U$ of $m$ elements, a collection $\\mathcal{S}$ of subsets of $U$, and a cost function $c : \\mathcal{S} \\to \\mathbb{R}_{0}$. The goal is to find a subcollection $\\mathcal{C} \\subseteq \\mathcal{S}$ that covers $U$ (that is, $\\bigcup_{S \\in \\mathcal{C}} S = U$) with minimum total cost $\\sum_{S \\in \\mathcal{C}} c(S)$.\n- The greedy algorithm repeatedly chooses a set $S \\in \\mathcal{S}$ that minimizes the ratio $c(S) / |S \\setminus C|$, where $C$ denotes the set of elements already covered, breaking ties arbitrarily, until all elements of $U$ are covered.\n- The approximation ratio on an instance is the ratio of the greedy algorithm’s total cost to the optimal total cost on that instance.\n- The $m$-th harmonic number is $H_m \\triangleq \\sum_{i=1}^{m} \\frac{1}{i}$.\n\nFor a given integer $m \\geq 2$ and a parameter $\\delta$ with $0  \\delta  1$, consider the following instance $(U,\\mathcal{S},c)$:\n- Universe $U = \\{u_1, u_2, \\ldots, u_m\\}$.\n- Family $\\mathcal{S}$ consists of two types of sets:\n  1. For each $i \\in \\{1,2,\\ldots,m\\}$, a singleton set $T_i \\triangleq \\{u_i\\}$ with cost $c(T_i) \\triangleq \\frac{1}{i}$.\n  2. For each $j \\in \\{1,2,\\ldots,m\\}$, a prefix set $P_j \\triangleq \\{u_1, u_2, \\ldots, u_j\\}$ with cost $c(P_j) \\triangleq 1 + \\delta$.\n\nTasks:\n1. Determine, with proof from the above definitions, the exact sequence of sets chosen by the greedy algorithm on this instance, and compute its total cost as a function of $m$ and $\\delta$.\n2. Compute the optimal total cost as a function of $m$ and $\\delta$ and justify optimality from first principles.\n3. Let $R(m,\\delta)$ denote the greedy-to-optimal cost ratio for this instance. Compute the limit $\\lim_{\\delta \\to 0^{+}} R(m,\\delta)$ and express your final answer as a single closed-form analytic expression in terms of $m$ and $H_m$.\n\nYour final answer must be a single closed-form expression. No rounding is required.", "solution": "The problem provides a specific instance of the Weighted Set Cover problem and asks for an analysis of the greedy algorithm's performance on it. The validation process confirms that the problem is well-posed, mathematically sound, and all definitions are complete and consistent. We proceed with the solution in three parts as requested.\n\nPart 1: Analysis of the Greedy Algorithm\n\nThe greedy algorithm for Weighted Set Cover iteratively selects the set $S$ that minimizes the ratio of its cost to the number of newly covered elements. Let $C_{k-1}$ be the set of elements covered before iteration $k$. The algorithm chooses $S \\in \\mathcal{S}$ to minimize the value $\\frac{c(S)}{|S \\setminus C_{k-1}|}$.\n\nLet us trace the algorithm's execution on the given instance.\nLet $\\mathcal{C}_{greedy}$ be the set of sets chosen by the algorithm.\n\n**Iteration $k=1$**:\nInitially, the set of covered elements is empty, $C_0 = \\emptyset$. The number of new elements covered by any set $S$ is simply its cardinality, $|S|$. We calculate the cost-effectiveness ratio for each set in $\\mathcal{S}$.\nFor the singleton sets $T_i = \\{u_i\\}$, for $i \\in \\{1, 2, \\ldots, m\\}$:\nThe ratio is $\\frac{c(T_i)}{|T_i \\setminus C_0|} = \\frac{c(T_i)}{|T_i|} = \\frac{1/i}{1} = \\frac{1}{i}$.\nFor the prefix sets $P_j = \\{u_1, u_2, \\ldots, u_j\\}$, for $j \\in \\{1, 2, \\ldots, m\\}$:\nThe ratio is $\\frac{c(P_j)}{|P_j \\setminus C_0|} = \\frac{c(P_j)}{|P_j|} = \\frac{1+\\delta}{j}$.\n\nThe algorithm selects the set with the minimum ratio. We need to find the minimum value in the set of all possible ratios: $\\{\\frac{1}{1}, \\frac{1}{2}, \\ldots, \\frac{1}{m}\\} \\cup \\{\\frac{1+\\delta}{1}, \\frac{1+\\delta}{2}, \\ldots, \\frac{1+\\delta}{m}\\}$.\nComparing the ratios for sets of the same effective size, for any $k \\in \\{1, \\ldots, m\\}$, the ratio for $T_k$ is $\\frac{1}{k}$ and the ratio for $P_k$ is $\\frac{1+\\delta}{k}$. Since $\\delta > 0$, we have $1  1+\\delta$, which implies $\\frac{1}{k}  \\frac{1+\\delta}{k}$. This holds for all $k$.\nThe minimum ratio among all singleton sets is $\\frac{1}{m}$ (for $T_m$). The minimum ratio among all prefix sets is $\\frac{1+\\delta}{m}$ (for $P_m$). The overall minimum ratio is therefore $\\frac{1}{m}$, which is achieved uniquely by the set $S_1 = T_m$.\nThus, the first set chosen is $T_m$. The set of covered elements becomes $C_1 = \\{u_m\\}$.\n\n**Inductive Proof for the sequence of choices**:\nWe claim that the greedy algorithm selects the singleton sets in the sequence $T_m, T_{m-1}, \\ldots, T_1$. We prove this by induction.\nBase Case: For $k=1$, we have shown that the algorithm selects $T_m$. This establishes the base of our induction corresponding to the element $u_m$.\nInductive Hypothesis: Assume that for the first $k$ steps ($1 \\le k  m$), the algorithm has selected the sets $T_m, T_{m-1}, \\ldots, T_{m-k+1}$. The set of covered elements after $k$ steps is $C_k = \\{u_m, u_{m-1}, \\ldots, u_{m-k+1}\\}$. The set of uncovered elements is $U \\setminus C_k = \\{u_1, u_2, \\ldots, u_{m-k}\\}$.\n\nInductive Step (Iteration $k+1$):\nWe now determine the set to be chosen. The cost-effectiveness ratio for any set $S$ is $\\frac{c(S)}{|S \\setminus C_k|}$.\nFor any remaining singleton set $T_i$ with $i \\in \\{1, 2, \\ldots, m-k\\}$, the element $u_i$ is uncovered. Thus, $|T_i \\setminus C_k| = 1$. The ratio is $\\frac{c(T_i)}{1} = \\frac{1}{i}$. The minimum of these ratios is $\\frac{1}{m-k}$, achieved by $T_{m-k}$.\nFor any prefix set $P_j$ with $j \\in \\{1, 2, \\ldots, m\\}$, the number of new elements it covers is $|P_j \\setminus C_k| = |P_j \\cap (U \\setminus C_k)| = |\\{u_1, \\ldots, u_j\\} \\cap \\{u_1, \\ldots, u_{m-k}\\}|$. This cardinality is $\\min(j, m-k)$.\nThe ratio for $P_j$ is $\\frac{c(P_j)}{|P_j \\setminus C_k|} = \\frac{1+\\delta}{\\min(j, m-k)}$.\nTo find the minimum ratio among all prefix sets, we must maximize the denominator, $\\min(j, m-k)$. The maximum value of $\\min(j, m-k)$ is $m-k$, which is achieved for any $j \\ge m-k$. So, the minimum ratio for any prefix set is $\\frac{1+\\delta}{m-k}$.\nNow we compare the minimum ratio from a singleton set, $\\frac{1}{m-k}$ (from $T_{m-k}$), with the minimum ratio from a prefix set, $\\frac{1+\\delta}{m-k}$.\nSince $\\delta > 0$, we have $1  1+\\delta$, and thus $\\frac{1}{m-k}  \\frac{1+\\delta}{m-k}$.\nThe uniquely minimum ratio is $\\frac{1}{m-k}$, achieved by the set $T_{m-k}$. Therefore, at step $k+1$, the algorithm selects the set $S_{k+1}=T_{m-k}$.\n\nBy the principle of mathematical induction, the greedy algorithm will choose the sets $T_m, T_{m-1}, \\ldots, T_1$ in this sequence. This covers all elements of $U$.\nThe total cost of the cover found by the greedy algorithm, $\\text{Cost}_{greedy}$, is the sum of the costs of these sets:\n$$ \\text{Cost}_{greedy} = c(T_m) + c(T_{m-1}) + \\cdots + c(T_1) = \\sum_{i=1}^{m} c(T_i) = \\sum_{i=1}^{m} \\frac{1}{i} $$\nThis sum is, by definition, the $m$-th harmonic number, $H_m$.\n$$ \\text{Cost}_{greedy} = H_m $$\n\nPart 2: Calculation of the Optimal Cost\n\nWe must find a valid cover $\\mathcal{C} \\subseteq \\mathcal{S}$ with the minimum possible total cost, $\\text{Cost}_{opt}$.\nThere are two primary candidates for an optimal cover:\n1. A cover consisting only of singleton sets. To cover the entire universe $U=\\{u_1, \\ldots, u_m\\}$, this cover must be $\\mathcal{C}_1 = \\{T_1, T_2, \\ldots, T_m\\}$. The cost is $\\text{Cost}(\\mathcal{C}_1) = \\sum_{i=1}^m c(T_i) = H_m$.\n2. A cover using one or more prefix sets. If a cover contains any prefix set $P_j$, its cost is at least $c(P_j)=1+\\delta$. The most efficient cover of this type is the single set $P_m = \\{u_1, \\ldots, u_m\\}$, which covers all of $U$ by itself. The cost of this cover, $\\mathcal{C}_2 = \\{P_m\\}$, is $\\text{Cost}(\\mathcal{C}_2) = c(P_m) = 1+\\delta$. Any other cover containing a prefix set would have a cost of at least $1+\\delta$, so $\\{P_m\\}$ is the best among covers that use prefix sets.\n\nThe optimal cost must be the minimum of the costs of all possible covers. From the analysis above, any cover is either of type 1 (only singletons) or contains at least one prefix set (leading to a cost of at least $1+\\delta$). Therefore, the optimal cost is the minimum of the costs of $\\mathcal{C}_1$ and $\\mathcal{C}_2$:\n$$ \\text{Cost}_{opt}(m, \\delta) = \\min(H_m, 1+\\delta) $$\nThe problem requires analysis in the limit as $\\delta \\to 0^+$. For $m \\ge 2$, the harmonic number $H_m = 1 + \\frac{1}{2} + \\cdots + \\frac{1}{m} > 1$. Thus, $H_m-1 > 0$. We can choose $\\delta$ such that $0  \\delta  H_m-1$, which implies $1+\\delta  H_m$. Since the limit considers arbitrarily small positive $\\delta$, this condition will hold.\nTherefore, for the purpose of this analysis, the optimal solution is the cover $\\{P_m\\}$ and its cost is:\n$$ \\text{Cost}_{opt}(m, \\delta) = 1+\\delta $$\n\nPart 3: Calculation of the Limit of the Approximation Ratio\n\nThe approximation ratio $R(m,\\delta)$ is the ratio of the greedy algorithm's cost to the optimal cost for this instance.\n$$ R(m,\\delta) = \\frac{\\text{Cost}_{greedy}}{\\text{Cost}_{opt}} = \\frac{H_m}{1+\\delta} $$\nWe are asked to compute the limit of this ratio as $\\delta$ approaches $0$ from the positive side.\n$$ \\lim_{\\delta \\to 0^+} R(m,\\delta) = \\lim_{\\delta \\to 0^+} \\frac{H_m}{1+\\delta} $$\nSince $H_m$ is a constant with respect to $\\delta$ and the function $f(\\delta) = \\frac{H_m}{1+\\delta}$ is continuous at $\\delta=0$, we can evaluate the limit by direct substitution:\n$$ \\lim_{\\delta \\to 0^+} \\frac{H_m}{1+\\delta} = \\frac{H_m}{1+0} = H_m $$\nThe limit asked for is the $m$-th harmonic number, $H_m$.", "answer": "$$ \\boxed{H_m} $$", "id": "3207647"}, {"introduction": "An algorithm's worst-case approximation ratio tells only part of its story; its performance on \"typical\" or random instances is often much better. This final hands-on practice moves from theoretical analysis to empirical investigation by having you implement an experiment. You will compare the performance of the famous 2-approximation algorithm for Vertex Cover against the true optimal solution on random graphs, allowing you to estimate its average-case performance and appreciate the gap between theoretical guarantees and practical reality. [@problem_id:3207616]", "problem": "You are asked to implement an experiment to estimate the average-case approximation ratio of a classical two-approximation algorithm for the Vertex Cover problem on Erdős–Rényi random graphs. The goal is to design a complete, runnable program that generates random graphs, applies the approximation algorithm, computes the exact optimal vertex cover to measure the ratio, and aggregates the results over multiple trials.\n\nYou must use the following fundamental base:\n- A vertex cover of an undirected graph is a subset of vertices that touches every edge. Formally, for an undirected graph $G = (V, E)$, a set $C \\subseteq V$ is a vertex cover if for every edge $(u, v) \\in E$, either $u \\in C$ or $v \\in C$ holds.\n- The Erdős–Rényi random graph model $G(n, p)$ draws each potential edge independently with probability $p$, on a vertex set of size $n$.\n- An approximation algorithm for an optimization problem is evaluated by its approximation ratio. Given an instance $I$, with optimal value $\\operatorname{OPT}(I)$ and algorithm output value $\\operatorname{ALG}(I)$ for a minimization problem, its approximation ratio on $I$ is defined as $\\rho(I) = \\frac{\\operatorname{ALG}(I)}{\\operatorname{OPT}(I)}$, with the convention that if $\\operatorname{OPT}(I) = 0$, the ratio is defined to be $1$ (this applies to graphs with no edges, where both the optimal and the algorithmic vertex cover sizes are $0$).\n- The classical two-approximation for Vertex Cover based on maximal matching constructs any maximal matching $M$ and returns the set of all endpoints of edges in $M$. This set has size $2 \\lvert M \\rvert$ and is a valid vertex cover.\n\nYour program must:\n- Implement the two-approximation algorithm based on maximal matching as described above, where the matching is constructed greedily by scanning edges in a fixed deterministic order.\n- Compute the exact optimal vertex cover size for each generated graph instance. You must do this exactly (not approximately), using any correct exact method. Hints are forbidden; however, an exact backtracking or branch-and-bound search is acceptable, provided it is correct.\n- For each random graph instance $G \\sim G(n, p)$, compute the ratio $\\rho(G)$ as specified above.\n- For each parameter tuple, estimate the average-case approximation ratio by the Monte Carlo estimator, that is, the arithmetic mean of $\\rho(G)$ over $s$ independent samples $G$ drawn from $G(n, p)$ with a fixed random seed to ensure reproducibility.\n- If a generated graph has no edges (so the optimal vertex cover size is $0$), define $\\rho(G) = 1$.\n- All computations are unitless and purely combinatorial; no physical units are involved.\n\nDesign details to implement from foundational definitions:\n- The two-approximation based on maximal matching is justified from the fact that any matching $M$ is a lower bound on the size of any vertex cover. Since each edge in a matching requires a distinct vertex to cover it, $\\lvert M \\rvert \\le \\operatorname{OPT}(G)$. The algorithm returns a vertex cover of size $2 \\lvert M \\rvert$, so it is at most twice the optimal size.\n- The Monte Carlo estimator uses the arithmetic mean as an estimator of the expected approximation ratio under $G(n, p)$ due to the law of large numbers, which states that the sample average converges to the expectation as the number of samples grows.\n\nInput is implicit; you must hardcode and run the following test suite of parameter values. Each test case is a tuple $(n, p, s, \\text{seed})$, where $n$ is the number of vertices, $p$ is the edge probability, $s$ is the number of independent graph samples, and $\\text{seed}$ is the seed for the random number generator:\n- Test $1$: $(n, p, s, \\text{seed}) = (10, 0.2, 80, 101)$.\n- Test $2$: $(n, p, s, \\text{seed}) = (12, 0.5, 30, 202)$.\n- Test $3$: $(n, p, s, \\text{seed}) = (8, 0.0, 60, 303)$.\n- Test $4$: $(n, p, s, \\text{seed}) = (9, 0.9, 40, 404)$.\n- Test $5$: $(n, p, s, \\text{seed}) = (1, 0.7, 10, 505)$.\n\nAlgorithmic requirements and constraints:\n- Graph generation for $G(n, p)$ must include each unordered pair $\\{i, j\\}$ for $0 \\le i  j  n$ independently with probability $p$, and must not include self-loops or multiple edges.\n- The exact optimal vertex cover must be computed by a correct exact algorithm. An acceptable approach is a branch-and-bound search that repeatedly picks an uncovered edge $(u, v)$ and branches on including $u$ or including $v$ into the cover, with memoization or pruning to keep the computation tractable on the small graphs used here.\n- The maximal matching must be constructed deterministically by scanning edges in lexicographic order, adding an edge if both endpoints are currently unmatched, until no more edges can be added.\n\nOutput:\n- For each test case, output a single floating-point number equal to the sample average of $\\rho(G)$ over the $s$ graphs, rounded to $6$ decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test suite above, for example, $[r_1, r_2, r_3, r_4, r_5]$, where each $r_i$ is the rounded floating-point result for test $i$.\n\nThere is no user input. The final output must be exactly one line in the specified format. All randomization must use the provided seeds. Ensure reproducibility by seeding the random number generator for each test case independently using the given seed. The answers are unitless real numbers. No angles or physical units are involved.", "solution": "We derive the complete solution by starting from core definitions in graph theory and approximation analysis, then designing the algorithmic components required for the experiment.\n\n1. Definitions and target quantity\n- Let $G = (V, E)$ be an undirected simple graph with $\\lvert V \\rvert = n$ and edge set $E$. A vertex cover $C \\subseteq V$ satisfies that for every $(u, v) \\in E$, $u \\in C$ or $v \\in C$.\n- In the Erdős–Rényi model $G(n, p)$, every potential edge $\\{i, j\\}$ for $0 \\le i  j  n$ is independently present with probability $p$.\n- For a minimization problem instance $I$ with optimal value $\\operatorname{OPT}(I)$ and algorithm output $\\operatorname{ALG}(I)$, the approximation ratio is $\\rho(I) = \\frac{\\operatorname{ALG}(I)}{\\operatorname{OPT}(I)}$. For our experiment, we define $\\rho(G) = 1$ when $\\operatorname{OPT}(G) = 0$ (i.e., no edges), because both optimum and algorithm outputs are $0$, and we assign a neutral ratio of $1$.\n- The average-case approximation ratio under $G(n, p)$ is $\\mathbb{E}_{G \\sim G(n, p)}[\\rho(G)]$. We estimate it via the Monte Carlo estimator $\\hat{\\rho} = \\frac{1}{s} \\sum_{i=1}^{s} \\rho(G_i)$, where $G_i$ are independent samples from $G(n, p)$.\n\n2. Two-approximation algorithm and its guarantee\n- A matching $M \\subseteq E$ is a set of disjoint edges. Any vertex cover must include at least one endpoint of each edge in $M$, so $\\lvert M \\rvert \\le \\operatorname{OPT}(G)$.\n- The classical two-approximation constructs any maximal matching $M$ and returns the set $C$ of all endpoints of edges in $M$. Then $\\lvert C \\rvert = 2 \\lvert M \\rvert$. Since $\\lvert M \\rvert \\le \\operatorname{OPT}(G)$, we have $\\lvert C \\rvert \\le 2 \\operatorname{OPT}(G)$, hence the algorithm achieves a worst-case factor of $2$.\n- We implement a deterministic greedy maximal matching: iterate the edges in lexicographic order and add an edge if both endpoints are unmatched. The returned cover size is $2 \\lvert M \\rvert$.\n\n3. Exact optimal vertex cover computation\n- Exact computation is required to measure $\\rho(G)$ precisely. For the small graphs in the test suite, a branch-and-bound search suffices.\n- Preprocessing step: index the edges as $e_0, e_1, \\dots, e_{m-1}$, with $m = \\lvert E \\rvert$. For each vertex $v \\in \\{0, 1, \\dots, n-1\\}$, precompute a bit mask $B_v \\in \\{0, 1\\}^m$ (stored as an integer) indicating which edges are incident to $v$. Represent the current set of uncovered edges by a bit mask $X \\in \\{0, 1\\}^m$, where bit $i$ is $1$ if $e_i$ is uncovered.\n- Recurrence: If $X = 0$, all edges are covered and the additional required size is $0$. Otherwise, pick any uncovered edge $e_i = (u, v)$ whose bit is $1$ in $X$. To cover $e_i$, we must include $u$ or $v$. Branch:\n  - Include $u$: new uncovered-edge mask $X' = X \\text{ \\ } (\\sim B_u)$ and cost $1$ plus the minimal cover cost for $X'$.\n  - Include $v$: new uncovered-edge mask $X'' = X \\text{ \\ } (\\sim B_v)$ and cost $1$ plus the minimal cover cost for $X''$.\n  Take the minimum of the two results.\n- Pruning and memoization:\n  - Upper bound initialization: compute a feasible cover size using the two-approximation to initialize an upper bound $U$.\n  - Maintain the best solution value found so far, and abandon any recursive branch that has already selected at least $U$ vertices.\n  - Memoize the minimal number of selections encountered for a given uncovered-edge mask $X$; if a recursive call reaches the same $X$ with a greater or equal number of selections, prune the call since it cannot lead to a better solution.\n  - To improve pruning, when branching on $(u, v)$, first explore the vertex among $\\{u, v\\}$ that covers more currently uncovered edges, i.e., compare the popcounts of $X \\text{ \\ } B_u$ and $X \\text{ \\ } B_v$ and recurse on the larger first.\n- Correctness follows by induction on the number of uncovered edges: at each step, at least one endpoint of an uncovered edge must be chosen, and the algorithm explores both choices, ensuring that the minimal solution is found. Pruning and memoization do not remove any potentially optimal solution because they only avoid exploring branches that cannot improve on current best or are dominated by previously seen states.\n\n4. Monte Carlo average-case estimation\n- For each test case $(n, p, s, \\text{seed})$, initialize a pseudorandom number generator with the given seed. Independently sample $s$ graphs from $G(n, p)$ by including each potential edge with probability $p$.\n- For each sampled graph $G$, compute $\\operatorname{ALG}(G)$ via the two-approximation and $\\operatorname{OPT}(G)$ via the exact method, define $\\rho(G)$ as $1$ if $\\operatorname{OPT}(G) = 0$, and otherwise $\\rho(G) = \\frac{\\operatorname{ALG}(G)}{\\operatorname{OPT}(G)}$.\n- The estimated average-case approximation ratio is $\\hat{\\rho} = \\frac{1}{s} \\sum_{i=1}^{s} \\rho(G_i)$.\n\n5. Test suite and output\n- Implement the five test cases:\n  - $(n, p, s, \\text{seed}) = (10, 0.2, 80, 101)$.\n  - $(n, p, s, \\text{seed}) = (12, 0.5, 30, 202)$.\n  - $(n, p, s, \\text{seed}) = (8, 0.0, 60, 303)$.\n  - $(n, p, s, \\text{seed}) = (9, 0.9, 40, 404)$.\n  - $(n, p, s, \\text{seed}) = (1, 0.7, 10, 505)$.\n- For each test case, output one floating-point number equal to the sample average of $\\rho(G)$, rounded to $6$ decimal places.\n- The final program must print a single line containing the list of the five results, in order, as a comma-separated list enclosed in square brackets, for example, $[1.234567,1.111111,1.000000,1.050000,1.000000]$.\n\nThis design is grounded in core definitions of graph theory and approximation analysis, employs a rigorously correct exact computation for the optimal solution on small instances, leverages the provable two-approximation method as the experimental subject, and uses the Monte Carlo method to estimate the expected approximation ratio under the Erdős–Rényi model.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom typing import List, Tuple, Dict\n\ndef generate_er_graph(n: int, p: float, rng: np.random.RandomState) - List[Tuple[int, int]]:\n    \"\"\"\n    Generate an undirected simple graph G(n, p) as a list of edges (u, v) with u  v.\n    \"\"\"\n    edges = []\n    if n = 1 or p = 0.0:\n        return edges\n    # Iterate lexicographically over pairs (i, j), i  j\n    for i in range(n - 1):\n        # Generate a vector of randoms for edges (i, j) for j  i\n        # However, generating one by one is simpler and avoids large arrays.\n        for j in range(i + 1, n):\n            if rng.rand()  p:\n                edges.append((i, j))\n    return edges\n\ndef approx_vertex_cover_size(edges: List[Tuple[int, int]], n: int) - int:\n    \"\"\"\n    Two-approximation via maximal matching: greedily add edges in lex order if both endpoints free,\n    then return the number of endpoints in the matching (i.e., 2*|M|).\n    \"\"\"\n    matched = [False] * n\n    cover_size = 0\n    # Edges already lexicographically ordered by construction in generate_er_graph\n    for u, v in edges:\n        if not matched[u] and not matched[v]:\n            matched[u] = True\n            matched[v] = True\n            cover_size += 2\n    return cover_size\n\ndef exact_min_vertex_cover_size(edges: List[Tuple[int, int]], n: int, approx_upper_bound: int) - int:\n    \"\"\"\n    Exact minimum vertex cover using branch-and-bound over uncovered-edge bitmasks.\n    - edges: list of (u, v) with u  v\n    - n: number of vertices\n    - approx_upper_bound: an initial upper bound for pruning (e.g., 2-approx size)\n    Returns the exact size of a minimum vertex cover.\n    \"\"\"\n    m = len(edges)\n    if m == 0:\n        return 0\n\n    # Map each vertex to the bitmask of incident edges\n    incident_masks = [0] * n\n    for idx, (u, v) in enumerate(edges):\n        incident_masks[u] |= (1  idx)\n        incident_masks[v] |= (1  idx)\n\n    # Initial uncovered edges mask: all edges uncovered\n    full_mask = (1  m) - 1\n\n    # Best found so far (upper bound). Initialize with approx_upper_bound, but it might be 0 if no edges.\n    best = [approx_upper_bound if approx_upper_bound > 0 else m]  # wrap in list to allow mutation in closure\n\n    # Memoization: for a given uncovered-edge mask X, store the minimum number of selections (taken) encountered.\n    seen: Dict[int, int] = {}\n\n    # Precompute an order of edges to select the first uncovered edge quickly by index.\n    # We simply keep the list and will find the first set bit when needed.\n\n    def dfs(uncovered_mask: int, taken: int) - None:\n        # Branch-and-bound pruning by current best\n        if taken = best[0]:\n            return\n\n        if uncovered_mask == 0:\n            # All edges covered\n            if taken  best[0]:\n                best[0] = taken\n            return\n\n        # Memoization pruning: if we've seen this mask with = taken, no need to proceed\n        prev = seen.get(uncovered_mask)\n        if prev is not None and taken = prev:\n            return\n        seen[uncovered_mask] = taken\n\n        # Pick one uncovered edge: get index of a set bit in uncovered_mask\n        # Use least significant set bit\n        lsb = uncovered_mask  -uncovered_mask\n        edge_index = (lsb.bit_length() - 1)\n        u, v = edges[edge_index]\n\n        # Compute degrees with respect to uncovered edges to decide branch order\n        deg_u = (uncovered_mask  incident_masks[u]).bit_count()\n        deg_v = (uncovered_mask  incident_masks[v]).bit_count()\n\n        # Explore the vertex that covers more uncovered edges first to prune faster\n        if deg_u = deg_v:\n            # Include u\n            dfs(uncovered_mask  ~incident_masks[u], taken + 1)\n            # Include v\n            dfs(uncovered_mask  ~incident_masks[v], taken + 1)\n        else:\n            # Include v\n            dfs(uncovered_mask  ~incident_masks[v], taken + 1)\n            # Include u\n            dfs(uncovered_mask  ~incident_masks[u], taken + 1)\n\n    dfs(full_mask, 0)\n    return best[0]\n\ndef estimate_average_ratio(n: int, p: float, samples: int, seed: int) - float:\n    \"\"\"\n    For given (n, p), estimate the average approximation ratio over 'samples' graphs,\n    using the provided seed for reproducibility.\n    \"\"\"\n    rng = np.random.RandomState(seed)\n    ratios_sum = 0.0\n    for _ in range(samples):\n        edges = generate_er_graph(n, p, rng)\n        # Approximate cover size\n        approx_size = approx_vertex_cover_size(edges, n)\n        # Exact optimal cover size\n        opt_size = exact_min_vertex_cover_size(edges, n, approx_size)\n        if opt_size == 0:\n            ratio = 1.0  # by convention for edgeless graphs\n        else:\n            ratio = approx_size / opt_size\n        ratios_sum += ratio\n    return ratios_sum / samples if samples  0 else float('nan')\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each tuple is (n, p, samples, seed)\n    test_cases = [\n        (10, 0.2, 80, 101),\n        (12, 0.5, 30, 202),\n        (8, 0.0, 60, 303),\n        (9, 0.9, 40, 404),\n        (1, 0.7, 10, 505),\n    ]\n\n    results = []\n    for n, p, s, seed in test_cases:\n        avg_ratio = estimate_average_ratio(n, p, s, seed)\n        results.append(f\"{avg_ratio:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3207616"}]}