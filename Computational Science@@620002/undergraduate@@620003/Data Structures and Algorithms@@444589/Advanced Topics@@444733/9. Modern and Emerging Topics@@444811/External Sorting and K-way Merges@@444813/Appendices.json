{"hands_on_practices": [{"introduction": "The efficiency of any external sorting algorithm is dominated by its Input/Output (I/O) cost. This exercise provides a foundational practice in quantifying this cost by meticulously tracking block reads and writes through each pass of an external mergesort. By analyzing a common optimization—on-the-fly deduplication—you will develop a concrete understanding of how algorithmic choices directly impact performance and lead to significant I/O savings. [@problem_id:3232889]", "problem": "You are given a large file of records stored on disk, consisting of exactly $D$ disk blocks, with each block fully utilized by fixed-length records. There is a main memory of $M$ block buffers (each buffer holds one disk block). An external mergesort is performed using the standard model where, in the initial run generation pass, contiguous chunks of size $M$ blocks are read into memory, sorted in memory, and written back as initial runs; then the algorithm performs a $k$-way merge pass repeatedly until a single fully sorted run remains, where $k = M - 1$ (one output buffer and one input buffer per run). Assume that all disk Input/Output (I/O) is sequential and block-aligned and that in-memory processing cost is negligible compared to disk I/O. Also assume that replacement selection is not used, so each initial run has exactly $M$ blocks.\n\nYou are asked to modify the merging phase so that duplicate records are removed on the fly. Specifically, during each $k$-way merge, the merger emits a record only if its key differs from the last key that was written to the output; ties across input streams are resolved by emitting a single representative and discarding all subsequent equal keys before they reach disk. Assume that equal keys are byte-identical and compare equal under the sort order. This modification does not change the number of input blocks read in any pass, but it can reduce the number of blocks written in the pass where duplicates are first eliminated, and it reduces both reads and writes in subsequent passes due to the smaller data volume.\n\nConsider the following concrete instance under these assumptions:\n- The number of memory buffers is $M = 101$, so $k = 100$.\n- The dataset occupies $D = 1010000$ disk blocks.\n- Initial runs are of length $M$ blocks, so the number of initial runs is $R_{0} = D / M$.\n- Because $R_{0}$ is an exact power of $k$, the baseline (no deduplication) sort completes in exactly two $k$-way merge passes after the initial run generation pass.\n- The dataset consists of records with exactly $r = 5$ duplicates per key, i.e., every distinct key appears exactly $5$ times in the dataset. Assume the deduplication during the first $k$-way merge pass removes all redundant copies so that only one representative per key is written. This reduces the data volume by a factor of $r$ in all subsequent materializations. Assume this reduction translates exactly into a factor of $r$ fewer blocks written or read (no padding or partial-block effects).\n\nCompute the exact number of disk block I/Os saved by performing this on-the-fly deduplication during merging, compared to the baseline external mergesort that performs the same sequence of passes but writes all records at every pass (i.e., no deduplication until after sorting). Count both reads and writes across all passes, including the initial run generation pass and both merge passes, and assume the final sorted output is written to disk.\n\nProvide your final answer as a single integer giving the total number of block I/Os saved. No rounding is necessary. Do not include any unit in your final answer.", "solution": "Let's denote the total number of blocks in the initial dataset as $D = 1010000$, the number of memory buffers as $M = 101$, and the duplication factor as $r=5$. The merge fan-in is $k = M - 1 = 100$. The number of initial runs of size $M$ is $R_0 = D/M = 1010000 / 101 = 10000$. Since $R_0 = 100^2 = k^2$, exactly two merge passes are required after the initial run generation pass.\n\nWe calculate the total I/O cost (reads + writes) for both the baseline and the deduplication scenarios. The total number of passes is 3 (1 for run generation, 2 for merging).\n\n**1. Baseline Scenario (No Deduplication)**\nIn each pass, the entire dataset of size $D$ is read from disk and written back to disk.\n- **Pass 0 (Run Generation):** Reads $D$ blocks, writes $D$ blocks. Cost: $2D$.\n- **Pass 1 (First Merge):** Reads $D$ blocks, writes $D$ blocks. Cost: $2D$.\n- **Pass 2 (Second Merge):** Reads $D$ blocks, writes $D$ blocks. Cost: $2D$.\nThe total I/O cost for the baseline scenario is:\n$$I/O_{\\text{baseline}} = 2D + 2D + 2D = 6D$$\n\n**2. Deduplication Scenario**\nDeduplication occurs during the merge passes.\n- **Pass 0 (Run Generation):** Same as baseline. Reads $D$ blocks, writes $D$ blocks. Cost: $2D$.\n- **Pass 1 (First Merge):** Reads the initial runs of total size $D$. Duplicates are removed on the fly, reducing the output data volume by a factor of $r=5$. The size of the written data is $D/r$. Cost: $D_{\\text{read}} + (D/r)_{\\text{write}} = D + D/r$.\n- **Pass 2 (Second Merge):** Reads the intermediate runs from Pass 1, which now have a total size of $D/r$. The data is already unique, so the output size is also $D/r$. Cost: $(D/r)_{\\text{read}} + (D/r)_{\\text{write}} = 2(D/r)$.\nThe total I/O cost for the deduplication scenario is:\n$$I/O_{\\text{dedup}} = 2D + \\left(D + \\frac{D}{r}\\right) + 2\\left(\\frac{D}{r}\\right) = 3D + \\frac{3D}{r}$$\n\n**3. I/O Savings**\nThe total I/O savings is the difference between the two scenarios:\n$$I/O_{\\text{saved}} = I/O_{\\text{baseline}} - I/O_{\\text{dedup}} = 6D - \\left(3D + \\frac{3D}{r}\\right) = 3D - \\frac{3D}{r} = 3D\\left(1 - \\frac{1}{r}\\right)$$\n\nSubstituting the given values:\n$$I/O_{\\text{saved}} = 3 \\cdot 1010000 \\cdot \\left(1 - \\frac{1}{5}\\right) = 3 \\cdot 1010000 \\cdot \\frac{4}{5} = 2424000$$\n\nThe total number of block I/Os saved is $2,424,000$.", "answer": "$$\\boxed{2424000}$$", "id": "3232889"}, {"introduction": "Beyond simple sorting, the external merge framework is powerful for large-scale data processing tasks like filtering. This problem challenges you to think as an algorithm designer, evaluating different strategies to integrate dynamic data filtering directly into the merge process. By considering how to handle records with a \"time-to-live\" (TTL), you will weigh the trade-offs between correctness, implementation complexity, and I/O efficiency, learning to favor designs that avoid processing data that will ultimately be discarded. [@problem_id:3233058]", "problem": "An external merge sort is used to sort $N$ records on disk by a primary key under the External Memory Model (EMM), where the main memory size is $M$ bytes, disk block size is $B$ bytes, and the merge uses a $k$-way selection heap across $k$ input runs. Each record $r$ carries a Time-To-Live (TTL) timestamp, interpreted as an absolute expiration time $e(r)$, and a record is considered valid at current time $t$ if and only if $e(r) \\ge t$. The Input/Output (I/O) cost of a standard external merge sort is driven by reading and writing $\\Theta(N/B)$ blocks per pass, with the number of passes determined by the run count and the merge fan-in $k$. The goal is to drop expired records dynamically during merge, without changing the sorted order of the remaining valid records by the primary key and without increasing the asymptotic I/O complexity.\n\nWhich modification to the $k$-way external merge algorithm is most appropriate to ensure that only valid records are output, the output is the same as if one sorted only the valid records by the primary key using a stable sort, and the asymptotic I/O complexity in terms of blocks remains unchanged except for potentially reduced writes due to dropped records?\n\nA. Modify the comparator in the $k$-way heap to order by expiration time $e(r)$ first and the primary key second, so that records with smaller $e(r)$ are popped earlier; when a popped record is expired (i.e., $e(r) < t$), discard it instead of writing.\n\nB. Leave the comparator as the primary key and introduce a per-run validity filter: whenever a run’s input buffer is refilled, scan its front records in memory and, for each record with $e(r) < t$, discard it immediately without pushing it into the $k$-way heap; only push valid records keyed by the primary key. When a run is exhausted of valid records, it contributes no further elements to the heap.\n\nC. Perform the full external sort ignoring TTL, then run an additional full pass over the final sorted output to drop all records with $e(r) < t$; this ensures valid output while leaving the merge unmodified.\n\nD. Partition the memory into two heaps: one heap for expired records and one heap for valid records. During merge, push every record into one of the heaps based on $e(r) \\ge t$, and write expired records to a separate discard file to keep them out of the final output. Keep the primary key comparator inside each heap.", "solution": "The objective is to modify a $k$-way external merge sort to filter out expired records, defined by $e(r)  t$, without altering the final sorted order of valid records and without increasing the asymptotic I/O complexity. The total number of records is $N$, main memory is $M$ bytes, and disk block size is $B$ bytes. The standard I/O cost of external merge sort is $\\Theta((N/B) \\log_{M/B}(N/B))$ block transfers. This cost arises from approximately $\\log_{k}(N/M)$ merge passes, where each pass reads and writes the entire dataset. The fan-in $k$ is typically chosen to be on the order of $M/B$.\n\nAn ideal modification would integrate the filtering logic into the merge passes seamlessly. This means expired records should be identified and discarded as early as possible to avoid the I/O cost of writing them to intermediate runs and reading them back in subsequent passes.\n\nLet's evaluate each option against these criteria.\n\nA. Modify the comparator in the $k$-way heap to order by expiration time $e(r)$ first and the primary key second, so that records with smaller $e(r)$ are popped earlier; when a popped record is expired (i.e., $e(r)  t$), discard it instead of writing.\nThis modification is fundamentally flawed. The primary goal is to produce an output file sorted by the primary key. By changing the heap's comparator to prioritize $e(r)$, the merge process will produce an output sorted by expiration time, not the primary key. This violates a core requirement of the problem.\nVerdict: **Incorrect**.\n\nB. Leave the comparator as the primary key and introduce a per-run validity filter: whenever a run’s input buffer is refilled, scan its front records in memory and, for each record with $e(r)  t$, discard it immediately without pushing it into the $k$-way heap; only push valid records keyed by the primary key. When a run is exhausted of valid records, it contributes no further elements to the heap.\nThis approach integrates the filtering logic correctly. The $k$-way heap maintains its standard function: ordering elements by the primary key. Before an element from an input run is considered for the heap, its validity is checked using the condition $e(r) \\ge t$. This check occurs in main memory after a block is read from disk. If a record is invalid, it is simply discarded, and the next record from its run is considered. This continues until a valid record is found or the run is exhausted. This process ensures that only valid records are ever placed on the heap and, subsequently, written to the merge output. The I/O complexity is not increased. In fact, it is reduced. An expired record is read from disk during a merge pass but is immediately discarded in memory and not written to the output of that pass. Consequently, it is not read or written in any subsequent merge passes. The number of passes remains the same, $\\lceil \\log_k (\\lceil N/M \\rceil) \\rceil$, and the number of blocks read per pass is at most what it was, while the number of blocks written is reduced. The asymptotic I/O complexity thus remains unchanged or improves. This method correctly produces a stable-sorted list of valid records by primary key.\nVerdict: **Correct**.\n\nC. Perform the full external sort ignoring TTL, then run an additional full pass over the final sorted output to drop all records with $e(r)  t$; this ensures valid output while leaving the merge unmodified.\nThis approach produces the correct output. A full external sort followed by a filtering pass will result in a list of valid records sorted by the primary key. However, it is highly inefficient from an I/O perspective. It requires all $N$ records, including the expired ones, to be read and written during every single merge pass. Only after the entire sort is complete does it perform an additional pass to filter, which costs $\\Theta(N/B)$ more I/Os for reading the sorted data and writing the final filtered result. Compared to option B, which eliminates expired records early in the process, this method performs a substantial amount of avoidable I/O. The problem asks for the \"most appropriate\" modification, implying efficiency is a key consideration. Adding an entire pass is not the most efficient solution, even if the overall asymptotic complexity class, $\\Theta((N/B) \\log_{M/B}(N/B))$, remains the same.\nVerdict: **Incorrect**.\n\nD. Partition the memory into two heaps: one heap for expired records and one heap for valid records. During merge, push every record into one of the heaps based on $e(r) \\ge t$, and write expired records to a separate discard file to keep them out of the final output. Keep the primary key comparator inside each heap.\nThis solution is unnecessarily complex and inefficient. First, there is no reason to maintain a sorted heap of expired records. Their order is irrelevant as they are being discarded. This is wasted computational effort. Second, writing the expired records to a separate discard file generates unnecessary write I/Os. The goal is to drop them, not to store them elsewhere. This directly contradicts the goal of minimizing I/O. Compared to option B, which simply discards expired records in memory with no extra I/O, this option performs more work (maintaining a second heap) and more I/O (writing to a discard file).\nVerdict: **Incorrect**.\n\nBased on this analysis, Option B is the only approach that satisfies all stated conditions correctly and efficiently. It integrates filtering seamlessly into the merge process, maintains the required sort order, and avoids increasing the asymptotic I/O complexity by eliminating expired records at the earliest possible stage during the merge.", "answer": "$$\\boxed{B}$$", "id": "3233058"}, {"introduction": "To predict real-world performance, we must advance from counting I/O blocks to building sophisticated throughput models. This exercise guides you through modeling a $k$-way merge as a pipeline, accounting for modern hardware features like asynchronous I/O and the distinct performance characteristics of SSDs versus HDDs. By deriving the time for each stage—reading, CPU merging, and writing—you will learn to identify the system's bottleneck and calculate its sustained throughput, bridging the gap between theoretical complexity and practical performance engineering. [@problem_id:3232934]", "problem": "You are asked to model and implement the sustained throughput of an external $k$-way merge driven by multi-stream asynchronous Input/Output (I/O) with double buffering. The objective is to derive the steady-state throughput under two device classes, Solid-State Drive (SSD) and Hard Disk Drive (HDD), using a principled performance model. You must then write a complete, runnable program that computes the predicted throughput for a fixed set of test cases.\n\nAssumptions and context:\n- External sorting merges runs stored on secondary storage where the data far exceeds Random Access Memory (RAM) capacity. A $K$-way merge reads from $K$ sorted input streams and produces a single sorted output stream.\n- The algorithm uses two buffers per input stream and two buffers for the output stream (double buffering). While a buffer is being processed by the Central Processing Unit (CPU), the other buffer is filled or flushed by asynchronous I/O.\n- The merge uses a min-heap of size $K$ to choose the next output element, with per-element CPU work that grows logarithmically in $K$.\n- Each round processes exactly one per-stream input chunk of size $C$ bytes, for a total of $K \\cdot C$ bytes of output per round.\n- Element size is $s$ bytes and each comparison costs $t_{\\mathrm{comp}}$ seconds. Assume comparisons dominate the CPU time for merging.\n- I/O timing model for a single blocking request of size $X$ bytes on a single device with access latency $L$ seconds and streaming bandwidth $B$ bytes per second is defined by the fundamental device performance relation: the time for the request equals latency plus transfer time. The specific device-class assumptions per round are:\n  - SSD model: each input read request of size $C$ costs one access latency plus transfer time; there are $K$ such reads. The output write request of size $K \\cdot C$ costs one access latency plus transfer time.\n  - HDD model: each input read request of size $C$ incurs average seek and rotation latency plus transfer time; there are $K$ such reads. The output write request of size $K \\cdot C$ similarly incurs one latency plus transfer time. Treat the $K$ input reads as serialized at the device due to mechanical movement. Treat the input device and the output device as independent, so reading and writing can overlap.\n- Double buffering with asynchronous I/O yields a pipeline where reading the next round’s input, writing the current round’s output, and CPU merging of the current round overlap in steady state. The round time equals the time of the bottleneck stage in this pipeline, and the sustained throughput equals bytes per round divided by the round time.\n\nYour tasks:\n- Derive from first principles the steady-state round time using the above assumptions. Start only from the definitions above and the fundamental device performance relation that a single blocking I/O of size $X$ takes latency plus transfer time.\n- Implement a program that computes the sustained throughput in mebibytes per second (MiB/s), where one mebibyte equals $2^{20}$ bytes, rounded to three decimal places. Use double buffering overlap as described for steady-state throughput.\n- For heap-based merging cost, use binary logarithm. You may assume the number of comparisons per output element is proportional to $\\log_{2}(K)$.\n\nInput and output specification:\n- There is no runtime input. Your program must embed and evaluate the following test suite of parameter sets. For each test case, compute the sustained throughput as a single floating-point number in MiB/s rounded to three decimals. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[x,y,z]\").\n\nTest suite:\nEach test case is a tuple of the form (device-type, $K$, $C$, $s$, $t_{\\mathrm{comp}}$, $B_{\\mathrm{in}}$, $B_{\\mathrm{out}}$, $L_{\\mathrm{in}}$, $L_{\\mathrm{out}}$), where:\n- device-type is either \"SSD\" or \"HDD\"; it determines the latency interpretation but uses the same computation structure,\n- $K$ is the number of input streams,\n- $C$ is the per-stream chunk size in bytes,\n- $s$ is the element size in bytes,\n- $t_{\\mathrm{comp}}$ is the time per comparison in seconds,\n- $B_{\\mathrm{in}}$ and $B_{\\mathrm{out}}$ are input and output bandwidths in bytes per second,\n- $L_{\\mathrm{in}}$ and $L_{\\mathrm{out}}$ are input and output access latencies in seconds.\n\nUse the following seven cases:\n- Case A (SSD, general throughput): (\"SSD\", $8$, $4\\,000\\,000$, $8$, $1 \\times 10^{-9}$, $1.5 \\times 10^{9}$, $1.5 \\times 10^{9}$, $5.0 \\times 10^{-5}$, $5.0 \\times 10^{-5}$).\n- Case B (HDD, seek-dominated): (\"HDD\", $8$, $4\\,000\\,000$, $8$, $1 \\times 10^{-9}$, $1.5 \\times 10^{8}$, $1.5 \\times 10^{8}$, $1.0 \\times 10^{-2}$, $1.0 \\times 10^{-2}$).\n- Case C (SSD, compute-bound): (\"SSD\", $8$, $4\\,000\\,000$, $8$, $5.0 \\times 10^{-8}$, $1.5 \\times 10^{9}$, $1.5 \\times 10^{9}$, $5.0 \\times 10^{-5}$, $5.0 \\times 10^{-5}$).\n- Case D (SSD, latency-dominated small chunks): (\"SSD\", $8$, $4096$, $8$, $1 \\times 10^{-9}$, $1.5 \\times 10^{9}$, $1.5 \\times 10^{9}$, $5.0 \\times 10^{-5}$, $5.0 \\times 10^{-5}$).\n- Case E (SSD, large $K$): (\"SSD\", $64$, $4\\,000\\,000$, $8$, $1 \\times 10^{-9}$, $1.5 \\times 10^{9}$, $1.5 \\times 10^{9}$, $5.0 \\times 10^{-5}$, $5.0 \\times 10^{-5}$).\n- Case F (HDD, $K = 1$ boundary): (\"HDD\", $1$, $4\\,000\\,000$, $8$, $1 \\times 10^{-9}$, $1.5 \\times 10^{8}$, $1.5 \\times 10^{8}$, $1.0 \\times 10^{-2}$, $1.0 \\times 10^{-2}$).\n- Case G (SSD, write-bandwidth bottleneck): (\"SSD\", $8$, $4\\,000\\,000$, $8$, $1 \\times 10^{-9}$, $1.5 \\times 10^{9}$, $3.0 \\times 10^{8}$, $5.0 \\times 10^{-5}$, $5.0 \\times 10^{-5}$).\n\nOutput unit and format:\n- Express each throughput in mebibytes per second (MiB/s), rounded to three decimal places.\n- Your program must print exactly one line in the format \"[v1,v2,v3,v4,v5,v6,v7]\" where each $v_{i}$ is the rounded throughput for the corresponding case in the same order as above.", "solution": "The objective is to derive the steady-state throughput of a $K$-way merge algorithm using double buffering and asynchronous I/O. The system's performance is modeled as a three-stage pipeline. In steady state, the time per round is dictated by the slowest stage (the bottleneck), and the throughput is the amount of data processed per round divided by this round time.\n\nA single round of the merge process involves reading one chunk of data of size $C$ from each of the $K$ input streams and merging them to produce one output chunk of size $K \\cdot C$. The total amount of data processed per round is therefore $N_{\\text{bytes\\_round}} = K \\cdot C$.\n\nThe pipeline consists of three stages that operate in parallel on different rounds of data:\n1.  **CPU Merge**: The CPU merges the input chunks from round $i$ to generate the output for round $i$.\n2.  **Input Read**: The I/O system reads the $K$ input chunks for the subsequent round, $i+1$.\n3.  **Output Write**: The I/O system writes the merged output chunk from the previous round, $i-1$. In steady state, we consider this the output of the current round $i$.\n\nThe time to complete one round in steady state, $T_{\\text{round}}$, is the maximum of the time taken by each of these three stages.\n$$T_{\\text{round}} = \\max(T_{\\text{CPU}}, T_{\\text{Read}}, T_{\\text{Write}})$$\n\nWe now derive the expression for the time taken by each stage.\n\n**1. CPU Time ($T_{\\text{CPU}}$)**\nThe CPU time is dominated by the element comparisons required for the heap-based merge. A min-heap of size $K$ is used to find the next smallest element among all input streams.\n- The number of elements processed in one round is $N_{\\text{elem}} = \\frac{N_{\\text{bytes\\_round}}}{s} = \\frac{K \\cdot C}{s}$, where $s$ is the size of each element in bytes.\n- The problem states that the number of comparisons per output element is proportional to $\\log_{2}(K)$. We assume a standard binary heap implementation where replacing the minimum element requires a number of comparisons on the order of $\\log_{2}(K)$. We take the proportionality constant to be $1$, so the number of comparisons per element is $\\log_{2}(K)$.\n- The time to perform one comparison is given as $t_{\\text{comp}}$.\n- Therefore, the total CPU time for merging all elements in one round is:\n$$T_{\\text{CPU}} = N_{\\text{elem}} \\cdot (\\text{comparisons per element}) \\cdot t_{\\text{comp}} = \\left(\\frac{K \\cdot C}{s}\\right) \\cdot \\log_{2}(K) \\cdot t_{\\text{comp}}$$\nFor the special case where $K=1$, no merging is required, and $\\log_{2}(1) = 0$, which correctly yields $T_{\\text{CPU}} = 0$.\n\n**2. Input Read Time ($T_{\\text{Read}}$)**\nThis is the time required to read the $K$ input chunks for the next round. The problem specifies that this involves $K$ distinct read requests, each of size $C$. The fundamental I/O performance relation for a single request of size $X$ is given as $T_{\\text{I/O}}(X) = L + \\frac{X}{B}$, where $L$ is latency and $B$ is bandwidth.\n- For both SSD and HDD models, the problem implies that the $K$ input reads are effectively serialized from the perspective of total time calculation. For HDD, this is explicit due to mechanical head movement. For SSD, the instruction \"each input read request of size $C$ costs one access latency plus transfer time\" leads to an additive model.\n- Time for a single input read of size $C$: $L_{\\text{in}} + \\frac{C}{B_{\\text{in}}}$.\n- Total time to read all $K$ chunks for one round:\n$$T_{\\text{Read}} = K \\cdot \\left(L_{\\text{in}} + \\frac{C}{B_{\\text{in}}}\\right)$$\n\n**3. Output Write Time ($T_{\\text{Write}}$)**\nThis is the time to write the single, consolidated output chunk of size $K \\cdot C$.\n- The total size of the write is $X = K \\cdot C$.\n- The write is performed as a single I/O request.\n- Using the fundamental I/O relation, the total write time is:\n$$T_{\\text{Write}} = L_{\\text{out}} + \\frac{K \\cdot C}{B_{\\text{out}}}$$\n\n**4. Sustained Throughput ($\\Theta$)**\nThe sustained throughput is the total data processed per round divided by the round time.\n- Throughput in bytes per second:\n$$\\Theta_{\\text{B/s}} = \\frac{N_{\\text{bytes\\_round}}}{T_{\\text{round}}} = \\frac{K \\cdot C}{\\max\\left(\\frac{K \\cdot C \\cdot t_{\\text{comp}} \\cdot \\log_{2}(K)}{s}, K \\cdot \\left(L_{\\text{in}} + \\frac{C}{B_{\\text{in}}}\\right), L_{\\text{out}} + \\frac{K \\cdot C}{B_{\\text{out}}}\\right)}$$\n- The problem requires the result in mebibytes per second (MiB/s), where $1 \\text{ MiB} = 2^{20}$ bytes.\n$$\\Theta_{\\text{MiB/s}} = \\frac{\\Theta_{\\text{B/s}}}{2^{20}}$$\n\nThis model applies to both SSD and HDD device types, with the performance differences captured by the specific values of the parameters $L_{\\text{in}}$, $L_{\\text{out}}$, $B_{\\text{in}}$, and $B_{\\text{out}}$. The derived formulas will now be implemented to compute the throughputs for the given test suite.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the sustained throughput for a k-way merge based on a\n    pipelined performance model.\n    \"\"\"\n\n    # Test suite of parameter sets. Each tuple is of the form:\n    # (device-type, K, C, s, t_comp, B_in, B_out, L_in, L_out)\n    test_cases = [\n        # Case A (SSD, general throughput)\n        (\"SSD\", 8, 4_000_000, 8, 1e-9, 1.5e9, 1.5e9, 5.0e-5, 5.0e-5),\n        # Case B (HDD, seek-dominated)\n        (\"HDD\", 8, 4_000_000, 8, 1e-9, 1.5e8, 1.5e8, 1.0e-2, 1.0e-2),\n        # Case C (SSD, compute-bound)\n        (\"SSD\", 8, 4_000_000, 8, 5.0e-8, 1.5e9, 1.5e9, 5.0e-5, 5.0e-5),\n        # Case D (SSD, latency-dominated small chunks)\n        (\"SSD\", 8, 4096, 8, 1e-9, 1.5e9, 1.5e9, 5.0e-5, 5.0e-5),\n        # Case E (SSD, large K)\n        (\"SSD\", 64, 4_000_000, 8, 1e-9, 1.5e9, 1.5e9, 5.0e-5, 5.0e-5),\n        # Case F (HDD, K = 1 boundary)\n        (\"HDD\", 1, 4_000_000, 8, 1e-9, 1.5e8, 1.5e8, 1.0e-2, 1.0e-2),\n        # Case G (SSD, write-bandwidth bottleneck)\n        (\"SSD\", 8, 4_000_000, 8, 1e-9, 1.5e9, 3.0e8, 5.0e-5, 5.0e-5),\n    ]\n\n    results = []\n    \n    # Conversion factor from bytes to mebibytes\n    BYTES_PER_MIB = 2**20\n\n    for case in test_cases:\n        _device_type, K, C, s, t_comp, B_in, B_out, L_in, L_out = case\n\n        # Total bytes processed per round\n        bytes_per_round = K * C\n\n        # 1. Calculate T_CPU: Time for CPU merge stage\n        if K > 1:\n            log2_K = np.log2(K)\n            T_cpu = (K * C * t_comp * log2_K) / s\n        else:\n            # For K=1, no comparisons are needed.\n            T_cpu = 0.0\n\n        # 2. Calculate T_Read: Time for input read stage\n        # This models K serialized read requests of size C.\n        T_read = K * (L_in + C / B_in)\n\n        # 3. Calculate T_Write: Time for output write stage\n        # This models a single write request of size K*C.\n        T_write = L_out + (K * C) / B_out\n\n        # Determine the round time (bottleneck of the pipeline)\n        T_round = max(T_cpu, T_read, T_write)\n\n        # Calculate throughput in bytes per second\n        if T_round > 0:\n            throughput_bps = bytes_per_round / T_round\n        else:\n            throughput_bps = float('inf')\n\n        # Convert throughput to MiB/s and round to 3 decimal places\n        throughput_mibs = throughput_bps / BYTES_PER_MIB\n        rounded_throughput = round(throughput_mibs, 3)\n        \n        results.append(rounded_throughput)\n\n    # Format the final output string as specified\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3232934"}]}