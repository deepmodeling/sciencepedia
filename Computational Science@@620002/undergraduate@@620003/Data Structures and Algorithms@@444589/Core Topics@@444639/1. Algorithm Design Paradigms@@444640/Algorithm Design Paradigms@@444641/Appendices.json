{"hands_on_practices": [{"introduction": "This exercise challenges you to apply the Divide and Conquer paradigm to a problem beyond simple sorting. Counting inversions in a sequence is not only a common technical challenge but also serves as a fundamental technique in fields like computational biology and recommender systems. By modifying the standard Merge Sort algorithm, you will learn how the \"combine\" step can be cleverly leveraged to gather information that solves a seemingly unrelated problem, reinforcing the power and flexibility of this design approach. [@problem_id:3205394]", "problem": "You are given the task of designing and implementing a divide and conquer algorithm to compute the number of inversions in a finite sequence of distinct integers. An inversion is defined as a pair of indices $(i,j)$ such that $i<j$ and $A[i]>A[j]$. You must implement an algorithm whose worst-case running time is $O(n \\log n)$, where $n$ denotes the length of the input sequence.\n\nDefinitions and assumptions:\n- A sequence $A$ of length $n$ is a permutation if it contains $n$ distinct integers (not necessarily the integers from $1$ to $n$).\n- The inversion count of $A$ is the number of index pairs $(i,j)$ with $i<j$ and $A[i]>A[j]$.\n\nYour program must:\n- Implement a divide and conquer algorithm to compute the inversion count, using only comparisons of integers and basic arithmetic. Naive quadratic-time enumeration of all pairs is not allowed.\n- Use the following fixed test suite, which is a list of $7$ sequences, each represented as a Python-style list. For the purposes of this problem, treat these lists as the entire input to your program, and do not read any external input.\n    1. $[]$\n    2. $[1]$\n    3. $[1,2,3,4,5]$\n    4. $[5,4,3,2,1]$\n    5. $[2,3,8,6,1]$\n    6. $[1,3,5,2,4,6]$\n    7. $[10,3,7,4,9,1,8,2,5,6]$\n\nOutput specification:\n- For each of the $7$ sequences above, compute its inversion count as a nonnegative integer.\n- Your program should produce a single line of output containing the $7$ results as a comma-separated list enclosed in square brackets, with no spaces. For example, if the results for the $7$ cases were $r_1,r_2,\\dots,r_7$, the output should be exactly $[r_1,r_2,\\dots,r_7]$ on one line.\n\nNotes:\n- There are no physical or angular units involved, and no percentages are required. All outputs are integers.\n- The algorithm design paradigm to use is Divide and Conquer, and the time complexity target is $O(n \\log n)$.", "solution": "We begin with core definitions from discrete mathematics and algorithm analysis. Let a sequence $A$ of length $n$ be a list of $n$ distinct integers. An inversion is a pair of indices $(i,j)$ with $i<j$ and $A[i]>A[j]$. The inversion count is the cardinality of the set $\\{(i,j)\\mid 1\\le i<j\\le n,\\ A[i]>A[j]\\}$.\n\nWe seek an algorithm in the Divide and Conquer paradigm with worst-case time $O(n \\log n)$. The fundamental base we rely on is:\n- The divide and conquer methodology: solve a problem of size $n$ by dividing it into subproblems of size approximately $n/2$, solving these recursively, and combining their solutions in $O(n)$ time.\n- The Master Theorem for recurrences of the form $T(n)=a\\,T(n/b)+f(n)$, which yields $T(n)=O(n \\log n)$ when $a=2$, $b=2$, and $f(n)=\\Theta(n)$.\n\nDesign:\n1. Divide step: Split $A$ into two halves, $L$ (left) and $R$ (right), of sizes $\\lfloor n/2\\rfloor$ and $\\lceil n/2\\rceil$.\n2. Conquer step: Recursively compute the inversion counts of $L$ and $R$, denoted $I_L$ and $I_R$, respectively, and also produce the sorted versions of $L$ and $R$. Because the integers are distinct, sorting defines a strict total order.\n3. Combine step: Merge the two sorted halves into a single sorted sequence while counting cross inversions $I_C$, which are inversions $(i,j)$ with $i$ in the left half and $j$ in the right half. During a standard stable merge, maintain indices $p$ into $L$ and $q$ into $R$. When $L[p]\\le R[q]$, we append $L[p]$ and increment $p$. When $L[p]>R[q]$, we append $R[q]$ and increment $q$, and we add $(|L|-p)$ to the inversion count, since $R[q]$ is smaller than all remaining elements $L[p],L[p+1],\\dots,L[|L|-1]$, each contributing one cross inversion.\n\nCorrectness:\nWe prove by induction on $n$ that the algorithm returns the exact inversion count and a sorted sequence.\n- Base cases: For $n=0$ and $n=1$, there are no inversions, so the count is $0$, and the sequence is trivially sorted. The algorithm returns $0$ and the original sequence, which is correct.\n- Inductive step: Assume correctness for all sequences of length less than $n$. Consider a sequence $A$ of length $n$. After dividing into $L$ and $R$, by the inductive hypothesis, the recursive calls correctly return $I_L$ and the sorted $L$, and $I_R$ and the sorted $R$. Every inversion in $A$ is either entirely within $L$, entirely within $R$, or a cross inversion with the first index in $L$ and the second in $R$. The merge procedure counts exactly the cross inversions: whenever an element from $R$ precedes remaining elements in $L$ in the merged order, it contributes exactly one inversion against each such remaining element, and when an element from $L$ precedes an element from $R$, no new cross inversion is formed. Therefore, the total inversion count is $I_L+I_R+I_C$, which the algorithm computes. The merge also produces the correctly sorted sequence from the two sorted halves. Thus, the algorithm is correct.\n\nComplexity:\nLet $T(n)$ denote the running time on length $n$. The recurrence satisfies\n$$\nT(n)=\n\\begin{cases}\n\\Theta(1), & \\text{if } n\\le 1,\\\\\n2\\,T(n/2)+\\Theta(n), & \\text{if } n>1,\n\\end{cases}\n$$\nsince we make two recursive calls on halves and perform a linear-time merge. By the Master Theorem with $a=2$, $b=2$, and $f(n)=\\Theta(n)$, we obtain $T(n)=\\Theta(n\\log n)$. Therefore, the algorithm runs in $O(n\\log n)$ time.\n\nApplication to the test suite:\nWe apply the algorithm to the $7$ sequences specified. For each sequence, the algorithm outputs an integer inversion count. The final output must be a single line in the format $[r_1,r_2,\\dots,r_7]$ with no spaces, where $r_k$ is the inversion count for the $k$-th test case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef count_inversions(arr):\n    # Returns (sorted_arr, inversion_count)\n    def sort_count(a):\n        n = len(a)\n        if n = 1:\n            return a[:], 0\n        mid = n // 2\n        left_sorted, left_inv = sort_count(a[:mid])\n        right_sorted, right_inv = sort_count(a[mid:])\n        merged = []\n        i = j = 0\n        cross_inv = 0\n        len_left = len(left_sorted)\n        len_right = len(right_sorted)\n        # Merge with inversion counting\n        while i  len_left and j  len_right:\n            if left_sorted[i] = right_sorted[j]:\n                merged.append(left_sorted[i])\n                i += 1\n            else:\n                merged.append(right_sorted[j])\n                # All remaining items in left_sorted form inversions with right_sorted[j]\n                cross_inv += (len_left - i)\n                j += 1\n        if i  len_left:\n            merged.extend(left_sorted[i:])\n        if j  len_right:\n            merged.extend(right_sorted[j:])\n        return merged, left_inv + right_inv + cross_inv\n\n    _, inv = sort_count(arr)\n    return inv\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        [],  # 1: empty\n        [1],  # 2: single element\n        [1,2,3,4,5],  # 3: already sorted\n        [5,4,3,2,1],  # 4: reverse sorted\n        [2,3,8,6,1],  # 5: mixed distinct integers\n        [1,3,5,2,4,6],  # 6: interleaved\n        [10,3,7,4,9,1,8,2,5,6],  # 7: length 10 permutation\n    ]\n\n    results = []\n    for case in test_cases:\n        result = count_inversions(case)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3205394"}, {"introduction": "We now transition to Dynamic Programming (DP), a powerful technique for solving optimization problems. This practice introduces DP in one of its most intuitive forms: finding the minimum cost path on a grid. You will learn to identify the core properties of optimal substructure and overlapping subproblems that make a problem suitable for DP. Formulating the recurrence relation and building the solution table for this problem will equip you with the foundational skills needed to tackle a wide range of dynamic programming challenges. [@problem_id:3205401]", "problem": "Consider an $(n+1) \\times (m+1)$ grid with integer coordinates, where each cell $(i,j)$ for $0 \\le i \\le n$ and $0 \\le j \\le m$ has an associated nonnegative or negative entry cost given by a function $c(i,j)$. A path starts by entering cell $(0,0)$ and ends upon entering cell $(n,m)$. At each step, the path may only move to a cell with either $(i+1,j)$ or $(i,j+1)$, ensuring that movement is monotone nondecreasing in both indices. The total cost of a path is defined to be the sum of the costs of all entered cells along the path, including the starting cell and the ending cell. Your task is to formulate a dynamic programming algorithm that computes the minimum possible total cost to reach $(n,m)$ from $(0,0)$ under these rules, and then implement a program that applies this algorithm to the specified test suite.\n\nThe design must be grounded in fundamental principles of algorithm design for directed acyclic graphs, specifically the notion of optimal substructure and overlapping subproblems based on the structure of the grid and allowed moves. The grid with allowed right and down moves forms a directed acyclic graph, and the path cost is additive. No heuristic shortcuts are permitted; the method must be derived from principled reasoning about subproblems.\n\nThere are no physical units involved. All outputs in the test suite must be recorded as integers.\n\nUse the following test suite, where each case is specified by $(n,m)$ and the cost function $c(i,j)$:\n\n- Test case $1$: $n=2$, $m=3$, with $c(i,j)=i^2+2j+1$ for all $i,j$ in range.\n- Test case $2$: $n=0$, $m=0$, with $c(i,j)=5$ for all $i,j$ in range.\n- Test case $3$: $n=3$, $m=3$, with $c(i,j)=1$ for all $i,j$ except $c(1,1)=1000$ and $c(2,2)=1000$.\n- Test case $4$: $n=5$, $m=0$, with $c(i,j)=i$ for all $i,j$ in range.\n- Test case $5$: $n=3$, $m=5$, with $c(i,j)=(-1)^{i+j}$ for all $i,j$ in range.\n\nYour program should compute the minimum total cost for each test case and produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[x_1,x_2,x_3,x_4,x_5]$), where $x_k$ is the minimum total cost for test case $k$.", "solution": "The problem is well-posed and scientifically grounded in the principles of algorithm design for directed acyclic graphs. It is a classic minimum cost path problem on a grid, which can be solved efficiently using dynamic programming.\n\nThe problem can be modeled as finding a minimum weight path in a directed acyclic graph (DAG). The cells of the $(n+1) \\times (m+1)$ grid represent the vertices $(i,j)$ of the graph, for $0 \\le i \\le n$ and $0 \\le j \\le m$. The allowed movements, from a cell $(i,j)$ to either $(i+1,j)$ or $(i,j+1)$, define the directed edges of the graph. Specifically, for each cell $(i,j)$, there exists a directed edge to $(i+1,j)$ if $i  n$, and a directed edge to $(i,j+1)$ if $j  m$. Since movements are always to cells with a greater or equal index in both coordinates (monotonically non-decreasing), no cycles can be formed, confirming the graph is a DAG. The cost $c(i,j)$ of entering a cell can be treated as the weight associated with each vertex. The total cost of a path is the sum of the weights of all vertices along it.\n\nThis problem structure exhibits two key properties that make it suitable for dynamic programming:\n$1$. **Optimal Substructure**: A minimum cost path from the start cell $(0,0)$ to any cell $(i,j)$ must be formed by extending a minimum cost path from one of its immediate predecessors, either $(i-1,j)$ or $(i,j-1)$.\n$2$. **Overlapping Subproblems**: The minimum cost to reach a particular cell $(i,j)$ is referenced multiple times when computing the minimum costs for subsequent cells, specifically $(i+1,j)$ and $(i,j+1)$.\n\nWe define a subproblem as computing the minimum cost to reach an arbitrary cell $(i,j)$ from the starting cell $(0,0)$. Let $DP(i,j)$ denote this minimum cost. Our ultimate goal is to find $DP(n,m)$.\n\nThe recurrence relation for $DP(i,j)$ is derived as follows:\n\n**Base Case**: The path to the starting cell $(0,0)$ involves only that cell. Therefore, the minimum cost to reach $(0,0)$ is simply its own cost.\n$$DP(0,0) = c(0,0)$$\n\n**Recursive Step**: For any other cell $(i,j)$, a path must arrive from one of its valid predecessors.\n- For cells in the top row where $i=0$ and $j0$, the only valid predecessor is $(0,j-1)$. The minimum cost path to $(0,j)$ is the minimum cost path to $(0,j-1)$ followed by a move to $(0,j)$. Thus:\n$$DP(0,j) = DP(0,j-1) + c(0,j) \\quad \\text{for } j  0$$\n- For cells in the first column where $i0$ and $j=0$, the only valid predecessor is $(i-1,0)$. Similarly:\n$$DP(i,0) = DP(i-1,0) + c(i,0) \\quad \\text{for } i  0$$\n- For any other cell $(i,j)$ where $i0$ and $j0$, the path must come from either $(i-1,j)$ (a move down) or $(i,j-1)$ (a move right). According to the principle of optimality, we must choose the predecessor with the minimum path cost. The total cost to reach $(i,j)$ is its own cost plus the minimum of the costs to reach its predecessors:\n$$DP(i,j) = c(i,j) + \\min(DP(i-1,j), DP(i,j-1)) \\quad \\text{for } i0, j0$$\n\nWe can compute the values of $DP(i,j)$ iteratively in a bottom-up manner. We create an $(n+1) \\times (m+1)$ table to store the $DP$ values. The computation proceeds as follows:\n$1$. Initialize $DP(0,0)$ using the base case.\n$2$. Fill the first row ($i=0$) of the table using the recurrence for $DP(0,j)$.\n$3$. Fill the first column ($j=0$) of the table using the recurrence for $DP(i,0)$.\n$4$. Fill the rest of the table for $i$ from $1$ to $n$ and $j$ from $1$ to $m$, using the general recurrence for $DP(i,j)$. Each $DP(i,j)$ depends only on values that have already been computed (i.e., $DP(i-1,j)$ and $DP(i,j-1)$).\n\nThe final answer to the problem is the value stored in the table at the destination cell, $DP(n,m)$. The time complexity of this algorithm is $O(nm)$ because each cell in the $(n+1) \\times (m+1)$ table is visited exactly once. The space complexity is also $O(nm)$ to store the DP table.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef min_cost_path(n: int, m: int, cost_func) -> int:\n    \"\"\"\n    Computes the minimum cost path on a grid using dynamic programming.\n\n    Args:\n        n: The maximum row index (grid is (n+1) x (m+1)).\n        m: The maximum column index.\n        cost_func: A function c(i, j) that returns the cost of cell (i, j).\n\n    Returns:\n        The minimum cost to travel from (0,0) to (n,m).\n    \"\"\"\n    # Grid dimensions are (n+1) x (m+1)\n    rows, cols = n + 1, m + 1\n\n    # Create the cost matrix C using the provided cost function.\n    # np.vectorize is used to apply the element-wise function over the indices.\n    indices = np.indices((rows, cols))\n    C = np.vectorize(cost_func)(indices[0], indices[1])\n\n    # DP table to store the minimum cost to reach cell (i, j)\n    DP = np.zeros((rows, cols), dtype=int)\n\n    # Base case: cost to reach (0,0) is just its own cost\n    DP[0, 0] = C[0, 0]\n\n    # Fill the first row (can only come from the left)\n    for j in range(1, cols):\n        DP[0, j] = C[0, j] + DP[0, j - 1]\n\n    # Fill the first column (can only come from above)\n    for i in range(1, rows):\n        DP[i, 0] = C[i, 0] + DP[i - 1, 0]\n    \n    # Fill the rest of the DP table\n    for i in range(1, rows):\n        for j in range(1, cols):\n            DP[i, j] = C[i, j] + min(DP[i - 1, j], DP[i, j - 1])\n\n    # The result is the minimum cost to reach the bottom-right cell\n    return DP[n, m]\n\ndef solve():\n    \"\"\"\n    Solves the problem for the given test suite and prints the results.\n    \"\"\"\n    test_cases = [\n        {'n': 2, 'm': 3, 'cost_func': lambda i, j: i**2 + 2*j + 1},\n        {'n': 0, 'm': 0, 'cost_func': lambda i, j: 5},\n        {'n': 3, 'm': 3, 'cost_func': lambda i, j: 1000 if (i, j) in [(1, 1), (2, 2)] else 1},\n        {'n': 5, 'm': 0, 'cost_func': lambda i, j: i},\n        {'n': 3, 'm': 5, 'cost_func': lambda i, j: (-1)**(i + j)},\n    ]\n\n    results = []\n    for case in test_cases:\n        result = min_cost_path(case['n'], case['m'], case['cost_func'])\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3205401"}, {"introduction": "Having mastered DP on a grid, this next practice extends the paradigm to a hierarchical structure: trees. While finding a minimum vertex cover is NP-hard for general graphs, it can be solved efficiently on trees using a technique often called \"DP on trees.\" This exercise will teach you how to define DP states based on the local choices available at each node—in this case, whether to include a vertex in the cover—and how to formulate a recurrence that combines optimal solutions from subtrees. This demonstrates the adaptability of dynamic programming to complex combinatorial problems on non-linear data structures. [@problem_id:3205276]", "problem": "You are given the task of computing the minimum vertex cover in an undirected, unweighted tree. A vertex cover of a graph is a set of vertices such that every edge has at least one endpoint in the set. Formally, given a graph $G = (V, E)$, a set $C \\subseteq V$ is a vertex cover if for every edge $(u, v) \\in E$, at least one of $u$ or $v$ belongs to $C$. A tree is a connected, acyclic graph with $|V| = n$ and $|E| = n - 1$, where $n \\geq 1$. Your task is to design a program that, for each provided tree, computes the size (cardinality) of a minimum vertex cover.\n\nFundamental base to use: the definitions of graph, tree, vertex cover, and the principle of optimal substructure as used in dynamic programming. Do not assume any pre-derived formula for the minimum vertex cover; instead, derive an algorithm from first principles based on these definitions.\n\nInput specification: There is no external input. Instead, your program must internally use the following test suite of trees, each specified by the number of nodes $n$ and a set of undirected edges $E$ represented as unordered pairs of distinct vertices labeled from $1$ to $n$.\n\nTest suite of trees:\n- Test $1$: $n = 1$, $E = \\{\\}$.\n- Test $2$: $n = 2$, $E = \\{(1, 2)\\}$.\n- Test $3$: $n = 6$, $E = \\{(1, 2), (1, 3), (1, 4), (1, 5), (1, 6)\\}$.\n- Test $4$: $n = 5$, $E = \\{(1, 2), (2, 3), (3, 4), (4, 5)\\}$.\n- Test $5$: $n = 7$, $E = \\{(1, 2), (1, 3), (2, 4), (2, 5), (3, 6), (3, 7)\\}$.\n- Test $6$: $n = 9$, $E = \\{(1, 2), (1, 3), (2, 4), (2, 5), (3, 6), (6, 7), (6, 8), (8, 9)\\}$.\n\nAlgorithmic requirements:\n- Your algorithm must be based on a rigorous derivation from the definitions and must run in time $\\mathcal{O}(n)$ per tree using a suitable algorithm design paradigm for trees.\n- Since the input graphs are trees, you may root each tree at any arbitrary vertex, but the result must be the same regardless of the chosen root.\n\nOutput specification:\n- For each test tree, compute the size of its minimum vertex cover as an integer.\n- Your program should produce a single line of output containing the results for tests $1$ through $6$ in order, as a comma-separated list of integers enclosed in square brackets (e.g., \"[result1,result2,result3,result4,result5,result6]\"), with no spaces.\n\nThere are no physical quantities, angles, or percentages in this problem. The outputs are integers with no units.", "solution": "The problem of finding the size of a minimum vertex cover (MVC) is NP-hard for general graphs, but for trees, it can be solved efficiently in linear time using dynamic programming. The acyclic structure of trees allows for a solution based on the principle of optimal substructure.\n\nFirst, we root the tree arbitrarily at some node $r$. This imposes a parent-child structure on the edges, allowing us to think in terms of subtrees. The solution for a subtree rooted at a node $u$ can be constructed from the solutions for the subtrees of its children. This suggests a post-order traversal (e.g., via DFS) for computation.\n\nFor each node $u$, we need to decide whether to include it in the vertex cover. The optimal choice for the subtree at $u$ depends on this decision. We therefore define two states for our dynamic programming formulation:\n- $DP(u, 1)$: The size of the minimum vertex cover for the subtree rooted at $u$, given that node $u$ **is included** in the vertex cover.\n- $DP(u, 0)$: The size of the minimum vertex cover for the subtree rooted at $u$, given that node $u$ **is not included** in the vertex cover.\n\nWe can now formulate the recurrence relations for these states. For a given node $u$, let $\\text{children}(u)$ be the set of its children.\n\n**Case 1: Node $u$ is included in the vertex cover ($DP(u, 1)$).**\nIf we include $u$ in the cover, we add $1$ to the count for $u$ itself. By including $u$, every edge $(u,v)$ to a child $v \\in \\text{children}(u)$ is covered. This means for each child's subtree, we are free to make the most optimal choice, which is to take the minimum of including or not including $v$.\n$$DP(u, 1) = 1 + \\sum_{v \\in \\text{children}(u)} \\min(DP(v, 0), DP(v, 1))$$\n\n**Case 2: Node $u$ is not included in the vertex cover ($DP(u, 0)$).**\nIf we do not include $u$ in the cover, we add $0$ to the count for $u$. However, to cover the edge $(u, v)$ for each child $v$, the child node $v$ **must** be included in the vertex cover. This is not a choice but a requirement.\n$$DP(u, 0) = \\sum_{v \\in \\text{children}(u)} DP(v, 1)$$\n\n**Base Case:**\nFor a leaf node $u$, which has no children, the summations are empty (equal to 0).\n- $DP(u, 1) = 1 + 0 = 1$.\n- $DP(u, 0) = 0 + 0 = 0$. (An empty set is a valid vertex cover for a graph with no edges).\n\n**Final Answer:**\nAfter the post-order traversal computes the DP values for all nodes up to the root $r$, the size of the MVC for the entire tree is the minimum of the two possibilities for the root node:\n$$\\text{Size of MVC} = \\min(DP(r, 0), DP(r, 1))$$\n\nThis algorithm performs a single traversal (e.g., DFS) over the tree, visiting each node and edge once. At each node, a constant number of arithmetic operations are performed. Thus, the total time complexity is $\\mathcal{O}(|V| + |E|) = \\mathcal{O}(n)$ for a tree with $n$ nodes, satisfying the problem's requirements.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport sys\n\ndef solve():\n    \"\"\"\n    Solves the minimum vertex cover problem for a suite of test trees.\n    \"\"\"\n    # Setting a higher recursion limit is necessary for deep trees,\n    # though not strictly required for the given test cases.\n    sys.setrecursionlimit(2000)\n\n    test_cases = [\n        (1, []),\n        (2, [(1, 2)]),\n        (6, [(1, 2), (1, 3), (1, 4), (1, 5), (1, 6)]),\n        (5, [(1, 2), (2, 3), (3, 4), (4, 5)]),\n        (7, [(1, 2), (1, 3), (2, 4), (2, 5), (3, 6), (3, 7)]),\n        (9, [(1, 2), (1, 3), (2, 4), (2, 5), (3, 6), (6, 7), (6, 8), (8, 9)]),\n    ]\n\n    results = []\n    \n    for n, edges in test_cases:\n        if n == 0:\n            results.append(0)\n            continue\n        if n == 1:\n            # A single node has no edges, so the minimum vertex cover is the empty set.\n            results.append(0)\n            continue\n\n        # Adjacency list representation for the tree. Nodes are 1-indexed.\n        adj = [[] for _ in range(n + 1)]\n        for u, v in edges:\n            adj[u].append(v)\n            adj[v].append(u)\n\n        # DP table: dp[u][0] -> cost if u is not in VC, dp[u][1] -> cost if u is in VC.\n        dp = np.zeros((n + 1, 2), dtype=int)\n\n        # We can pick any node as the root. Let's pick node 1.\n        root = 1\n        # The parent of the root can be an invalid node index like 0.\n        parent_of_root = 0\n        \n        # Perform DFS from the root to compute DP values in a post-order fashion.\n        _dfs_vertex_cover(root, parent_of_root, adj, dp)\n        \n        # The final answer is the minimum of including or not including the root in the VC.\n        result = min(dp[root][0], dp[root][1])\n        results.append(result)\n\n    # Format the final output as specified.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef _dfs_vertex_cover(u, p, adj, dp):\n    \"\"\"\n    Performs a post-order traversal to calculate DP values for minimum vertex cover.\n    \n    Args:\n        u (int): The current node being visited.\n        p (int): The parent of the current node u.\n        adj (list of lists): The adjacency list of the tree.\n        dp (numpy.ndarray): The dynamic programming table.\n    \"\"\"\n    # Base case for the recurrence:\n    # DP(u, 1) starts at 1 for including u itself.\n    dp[u][1] = 1\n    # DP(u, 0) starts at 0 for not including u.\n    dp[u][0] = 0\n\n    # Visit all children of u.\n    for v in adj[u]:\n        if v == p:\n            continue  # Don't go back up the tree.\n        \n        # Recursively call for the child node.\n        _dfs_vertex_cover(v, u, adj, dp)\n        \n        # Apply the recurrence relations after child's DP values are computed.\n        \n        # If u is included (dp[u][1]), we can choose the minimum for its children.\n        dp[u][1] += min(dp[v][0], dp[v][1])\n        \n        # If u is not included (dp[u][0]), its children must be included.\n        dp[u][0] += dp[v][1]\n\nsolve()\n```", "id": "3205276"}]}