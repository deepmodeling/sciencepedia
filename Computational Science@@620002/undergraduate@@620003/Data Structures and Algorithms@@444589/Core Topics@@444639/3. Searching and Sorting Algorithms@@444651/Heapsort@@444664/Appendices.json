{"hands_on_practices": [{"introduction": "Before diving into Heapsort itself, it's crucial to master its core component: the heap. This exercise challenges you to use a binary min-heap as a priority queue to solve the classic problem of merging $k$ sorted arrays. By determining the most efficient way to repeatedly find the smallest element among all array heads, you will build a strong intuition for how heaps provide an optimal solution for managing a dynamic set of candidates, a principle that lies at the heart of Heapsort [@problem_id:3239740].", "problem": "You are given $k$ sorted arrays $A_1, A_2, \\dots, A_k$ containing a total of $N = \\sum_{i=1}^{k} n_i$ keys, where $n_i$ is the length of $A_i$. You must design an algorithm, grounded in the principles of heapsort, to merge these arrays into a single nondecreasing output array. Assume a standard binary heap with the heap-order property and complete binary tree shape is available. Your goal is to reason from first principles to identify an algorithmic strategy and derive its worst-case running time and auxiliary space (excluding the space of the output array). Among the options below, choose the single option that both correctly specifies a valid algorithm derived from heapsort principles and achieves asymptotically optimal running time under the comparison model of computation, with its stated time and space complexities.\n\nA. Initialize a binary min-heap of size $k$ with one key from each nonempty array in the form of a tuple $(\\text{key}, \\text{array\\_id}, \\text{index})$. Repeatedly extract the minimum tuple, append its key to the output, and if the tuple came from array $A_j$ and there remains an unmerged key in $A_j$, insert the next tuple from $A_j$. This yields worst-case time $\\Theta(N \\log k)$ and auxiliary space $\\Theta(k)$.\n\nB. Concatenate all $k$ arrays into a single array of length $N$ and apply in-place heapsort: build a single binary heap over all $N$ keys and repeatedly extract the extremum to sort. This yields worst-case time $\\Theta(N \\log N)$ and auxiliary space $\\Theta(1)$.\n\nC. Maintain a binary max-heap of size $k$ over the current first unmerged keys of each array. Repeatedly extract the maximum and append it to the output, reinserting the next key from the same array if it exists. This yields worst-case time $\\Theta(N \\log k)$ and auxiliary space $\\Theta(k)$.\n\nD. Use a $k$-ary heap on the current first unmerged keys, so that the heap height is $1$; thus each extract-min and insert takes $\\Theta(1)$ time, giving total time $\\Theta(N)$ and auxiliary space $\\Theta(k)$.\n\nSelect the single correct option.", "solution": "The problem statement is subjected to validation prior to any attempt at a solution.\n\n**Problem Validation**\n\n**Step 1: Extract Givens**\n- There are $k$ sorted arrays denoted as $A_1, A_2, \\dots, A_k$.\n- The total number of keys is $N = \\sum_{i=1}^{k} n_i$, where $n_i$ is the length of array $A_i$.\n- The objective is to merge these arrays into a single nondecreasing output array.\n- The algorithm must be grounded in the principles of heapsort.\n- A standard binary heap with the heap-order property and complete binary tree shape is available.\n- The task is to identify a valid algorithmic strategy, derive its worst-case running time and auxiliary space (excluding the output array), and select the option that correctly specifies this algorithm, its optimal running time, and its complexities.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is a classic and well-defined problem in computer science known as $k$-way merge.\n- **Scientifically Grounded:** The problem is based on fundamental concepts of algorithms and data structures, specifically sorting, merging, and priority queues (heaps). It is entirely free of pseudoscience or factual inaccuracies.\n- **Well-Posed:** The objective is clear: design and analyze an efficient merging algorithm. A unique, stable, and meaningful solution exists within the comparison model of computation.\n- **Objective:** The problem is stated using precise, unambiguous technical language.\n\n**Step 3: Verdict and Action**\nThe problem statement is valid. It is a well-posed, scientifically grounded problem from the field of algorithm design and analysis. We may proceed with the solution derivation.\n\n**Derivation from First Principles**\n\nThe task is to merge $k$ sorted arrays into a single sorted output array. To construct the output array in nondecreasing order, we must repeatedly identify and append the smallest element among all keys not yet merged.\n\nAt any step of the merging process, the candidates for the next overall smallest element are the current first (i.e., smallest) unmerged keys from each of the $k$ arrays. This is because each input array $A_i$ is already sorted. The core of the problem is thus to efficiently manage a dynamic set of up to $k$ candidate keys and repeatedly extract the minimum from this set.\n\nThe problem requires an approach \"grounded in the principles of heapsort.\" Heapsort utilizes a heap data structure to efficiently sort an array by repeatedly extracting the extremum element. A heap is a specialized tree-based data structure that satisfies the heap property. For our purpose of finding the smallest element, a **min-heap** is the appropriate choice. It allows for efficient extraction of the minimum element and insertion of new elements.\n\nLet us formalize the algorithm using a min-heap:\n1.  **Initialization**: A binary min-heap is created. For each of the $k$ arrays $A_i$ that is not empty, its first key is inserted into the heap. To track the origin of each key, we store a tuple of the form $(\\text{key}, \\text{array\\_id}, \\text{index})$ in the heap. The heap will contain at most $k$ elements.\n2.  **Iteration**: The following process is repeated until the heap is empty, which will occur after all $N$ keys have been processed.\n    a.  Use `extract-min` to remove the tuple with the smallest key from the heap. Let this tuple be $(v, i, j)$, where $v$ is the key value, $i$ is the identifier of its source array $A_i$, and $j$ is its index within $A_i$.\n    b.  Append the key $v$ to the single output array.\n    c.  If array $A_i$ has more keys (i.e., if index $j+1$ is less than $n_i$, the length of $A_i$), insert the next key from $A_i$ into the heap. This new tuple will be $(A_i[j+1], i, j+1)$.\n\n**Complexity Analysis**\n-   **Time Complexity**:\n    -   The initialization step involves inserting up to $k$ keys into the min-heap. With a binary heap, each insertion takes $O(\\log k)$ time. The total time for initialization is $O(k \\log k)$.\n    -   The main loop executes $N$ times, once for each key in the total input. Each iteration involves one `extract-min` operation and at most one `insert` operation. On a binary heap of size at most $k$, both operations have a worst-case time complexity of $O(\\log k)$.\n    -   The total worst-case time complexity is therefore $O(k \\log k) + N \\times O(\\log k) = O((N+k)\\log k)$. Since any practical application will have $N \\ge k$, this simplifies to $\\Theta(N \\log k)$.\n    -   This running time is known to be asymptotically optimal in the comparison model of computation. The lower bound for merging $k$ lists is $\\Omega(N \\log k)$.\n\n-   **Auxiliary Space Complexity**:\n    -   The primary auxiliary data structure is the min-heap. At any point, the heap stores at most one key from each of the $k$ arrays.\n    -   Therefore, the maximum size of the heap is $k$. Each element in the heap is a small tuple of constant size.\n    -   The auxiliary space required is $\\Theta(k)$. The problem statement specifies that the space for the output array (of size $N$) is to be excluded.\n\n**Option-by-Option Analysis**\n\n**A. Initialize a binary min-heap of size $k$ with one key from each nonempty array in the form of a tuple $(\\text{key}, \\text{array\\_id}, \\text{index})$. Repeatedly extract the minimum tuple, append its key to the output, and if the tuple came from array $A_j$ and there remains an unmerged key in $A_j$, insert the next tuple from $A_j$. This yields worst-case time $\\Theta(N \\log k)$ and auxiliary space $\\Theta(k)$.**\n\nThis option precisely describes the optimal algorithm derived from first principles. It correctly identifies the use of a min-heap to manage the candidates, the iterative process of extracting the minimum and replenishing the heap, and the data stored in the heap. The stated time complexity of $\\Theta(N \\log k)$ and auxiliary space complexity of $\\Theta(k)$ are both correct as per our analysis. The algorithm is asymptotically optimal.\n**Verdict: Correct**\n\n**B. Concatenate all $k$ arrays into a single array of length $N$ and apply in-place heapsort: build a single binary heap over all $N$ keys and repeatedly extract the extremum to sort. This yields worst-case time $\\Theta(N \\log N)$ and auxiliary space $\\Theta(1)$.**\n\nThis approach constitutes a valid sorting algorithm. However, it fails to exploit the pre-sorted nature of the $k$ input arrays, which is the crux of a *merging* problem. Its time complexity is $\\Theta(N \\log N)$, which is asymptotically slower than the optimal $\\Theta(N \\log k)$ whenever $k < N$. For instance, if $k$ is a small constant, the optimal time is $\\Theta(N)$, whereas this option gives $\\Theta(N \\log N)$. Thus, it is not an asymptotically optimal algorithm for this problem. Furthermore, the claim of $\\Theta(1)$ auxiliary space is questionable; the \"concatenate\" step requires an auxiliary array of size $N$ to hold the combined elements before sorting can begin, leading to $\\Theta(N)$ auxiliary space, not $\\Theta(1)$.\n**Verdict: Incorrect**\n\n**C. Maintain a binary max-heap of size $k$ over the current first unmerged keys of each array. Repeatedly extract the maximum and append it to the output, reinserting the next key from the same array if it exists. This yields worst-case time $\\Theta(N \\log k)$ and auxiliary space $\\Theta(k)$.**\n\nThis option is fundamentally flawed in its logic. The goal is to produce a \"nondecreasing\" (i.e., ascending order) output array. To do this, one must repeatedly select the *minimum* available key. This option proposes using a **max-heap** and extracting the *maximum* key. This would produce a nonincreasing (descending order) sequence of keys, which contradicts the problem requirement. While the complexity analysis for a heap-based method is stated with the correct orders of magnitude, the proposed algorithm itself is incorrect for the stated goal.\n**Verdict: Incorrect**\n\n**D. Use a $k$-ary heap on the current first unmerged keys, so that the heap height is $1$; thus each extract-min and insert takes $\\Theta(1)$ time, giving total time $\\Theta(N)$ and auxiliary space $\\Theta(k)$.**\n\nThis option contains a critical flaw in its analysis of $k$-ary heap operations. A $k$-ary heap with $k$ items does have a height of $1$ (a root and $k-1$ children). An `insert` operation, which involves bubbling up one level, would indeed take $O(1)$ time. However, an `extract-min` operation on a $k$-ary heap requires replacing the root and then sifting it down. The sift-down process at each node involves finding the minimum among its up to $k$ children. This takes $\\Theta(k)$ comparisons at the root level. Therefore, the `extract-min` operation does not take $\\Theta(1)$ time; it takes $\\Theta(k)$ time. The resulting total time complexity would be $N \\times \\Theta(k) = \\Theta(Nk)$, which is significantly worse than the $\\Theta(N \\log k)$ achieved with a binary heap. The premise that operations take $\\Theta(1)$ is false, invalidating the claimed total time of $\\Theta(N)$.\n**Verdict: Incorrect**", "answer": "$$\\boxed{A}$$", "id": "3239740"}, {"introduction": "This practice moves from the heap data structure to the full Heapsort algorithm, challenging you to implement it for a non-trivial data type: strings. You will adapt Heapsort to use lexicographical comparison and instrument your code to perform a detailed performance analysis, tracking metrics like character versus string comparisons and swaps. This exercise not only solidifies your understanding of the algorithm's mechanics but also provides practical experience in handling complex data and evaluating algorithmic efficiency from first principles [@problem_id:3239748].", "problem": "You are asked to adapt the heapsort algorithm to operate on string data using lexicographical comparison and to analyze its behavior using principled measurements. The task must be solved from first principles, starting only from core definitions and well-tested facts, without presuming or using any specialized shortcuts.\n\nFundamental base you may assume:\n- A binary heap is a complete binary tree stored in an array. For an array index $i$ with $0 \\le i < n$, the parent index is $\\left\\lfloor \\frac{i-1}{2} \\right\\rfloor$, the left child index is $2i+1$, and the right child index is $2i+2$.\n- The height of a binary heap with $n$ elements is at most $\\left\\lfloor \\log_2 n \\right\\rfloor$.\n- The sift-down operation on a heap takes at most $O(\\log n)$ comparisons and swaps per call, and building a heap by sifting down from the middle to the root costs $O(n)$ comparisons and swaps in total.\n- With a comparator that defines a total order, heapsort performs $O(n \\log n)$ comparisons and swaps in the worst case under the Landau symbol $O(\\cdot)$.\n\nLexicographical order to implement:\n- For strings $s$ and $t$, define $s \\prec t$ if and only if, at the first index $j$ where $s[j] \\ne t[j]$, the Unicode code point $ord(s[j])$ is less than $ord(t[j])$; if one is a strict prefix of the other, the shorter string is considered smaller. This is the standard lexicographical order induced by Unicode code points and is a total order on finite strings.\n\nCounting model to use:\n- A string comparison is a single invocation of the lexicographical comparator between two strings; count each invocation as one unit toward the string-comparison count $C^{(str)}$.\n- A character comparison is one step that compares two characters at the same position in two strings; count each such positional character check as one unit toward the character-comparison count $C^{(char)}$. If two strings are equal up to the end of the shorter string and differ only by length, do not count a character comparison for the final length check.\n- A swap is the exchange of two array elements; count each array-element swap as one unit toward the swap count $S$.\n- When you verify properties (such as the heap property after the build phase), do not add to $C^{(str)}$, $C^{(char)}$, or $S$; those verification steps must not contaminate the algorithmic counts.\n\nStability measurement:\n- Heapsort is known not to be stable in general. To empirically test stability per input, augment each string with its original index and sort pairs $(\\text{string}, \\text{index})$ using comparisons that only inspect the string component. After sorting, for any maximal contiguous block of equal strings, check whether their original indices are in nondecreasing order. Report a boolean $Stab$ that is true if and only if the relative order of equal strings is preserved.\n\nHash of the sorted output:\n- To avoid printing strings while still verifying the exact sorted order, compute a polynomial rolling hash $H$ of the resulting sorted sequence of strings. Let $B = 911382323$ and $M = 2^{64}$. Initialize $H \\gets 0$. For each string in sorted order, process a separator value $0$ first, then each character $c$ in the string as the integer $ord(c)+1$, updating\n$$\nH \\gets \\big(H \\cdot B + x\\big) \\bmod M\n$$\nfor each processed integer $x$. The separator $0$ cannot collide with any character because characters are mapped to $ord(c)+1 \\ge 1$. Use arithmetic modulo $M$.\n\nProgram requirements:\n- Implement in-place heapsort for strings, using the lexicographical order defined above.\n- Instrument the algorithm to return, for each input array, the tuple $[H, C^{(char)}, C^{(str)}, S, H^{(build)}, Stab]$, where:\n  - $H$ is the $64$-bit hash of the sorted sequence computed as specified,\n  - $C^{(char)}$ is the total number of character comparisons performed by the comparator during heapsort,\n  - $C^{(str)}$ is the total number of string-comparator invocations during heapsort,\n  - $S$ is the total number of swaps during heapsort,\n  - $H^{(build)}$ is a boolean indicating whether the array satisfied the max-heap property immediately after the heap-build phase under the specified comparator, checked without affecting the counts,\n  - $Stab$ is the boolean stability result defined above.\n- The heapsort should sort in ascending lexicographical order (smallest first).\n\nTest suite to implement and evaluate:\n- Case A (general mix): [\"pear\",\"apple\",\"banana\",\"peach\",\"apricot\",\"grape\"]\n- Case B (duplicates for stability): [\"bob\",\"alice\",\"bob\",\"alice\",\"bob\"]\n- Case C (empty): []\n- Case D (singleton): [\"Z\"]\n- Case E (mixed case and non-ASCII Unicode): [\"éclair\",\"eclair\",\"Éclair\",\"ECLAIR\"]\n- Case F (prefix relations and empty strings): [\"\", \"a\", \"aa\", \"ab\", \"\"]\n\nFor each case, run the instrumented heapsort and report the metrics as above.\n\nRequired final output format:\n- Your program should produce a single line of output containing the list of per-case results as a comma-separated list enclosed in square brackets, with each per-case result itself formatted as a bracketed comma-separated list, and with no spaces anywhere. For example, a line of the form \"[[1,2,3,4,True,False],[...],...]\" is acceptable, where booleans must appear as True or False.\n\nDesign-for-coverage explanation:\n- Case A exercises typical behavior with varied first letters.\n- Case B stresses duplicates and exposes (lack of) stability.\n- Case C validates correct handling of $n=0$ corner case with zero cost and vacuous properties.\n- Case D validates $n=1$ behavior.\n- Case E ensures correct Unicode code point ordering, including uppercase and lowercase with accented letters.\n- Case F tests prefix ordering and the presence of empty strings.\n\nAll numeric values and symbols above are formal mathematical quantities and must be handled exactly as specified. There are no physical units, angles, or percentages involved in this task; all outputs are either integers or booleans as described. The final answer must be code that computes and prints the required single line. Do not read any input and do not print anything else.", "solution": "The user has provided a well-defined computational problem that is scientifically grounded in the principles of data structures and algorithms. The problem is self-contained, consistent, and requires a rigorous implementation of the heapsort algorithm with specific instrumentation for performance and property analysis. All definitions, constants, and test cases are explicitly provided, making the problem fully formalizable and verifiable. Therefore, the problem is deemed **valid**.\n\nThe solution involves creating a detailed implementation of heapsort for string data, adhering to the specified lexicographical order, and instrumenting it to measure several key metrics. The implementation will be encapsulated within a class to manage state, including the input array and the various counters.\n\n**1. Overall Structure**\nAn analysis class, let's call it `HeapsortAnalysis`, is defined to handle the sorting and analysis for a single input array. This class stores the counters for character comparisons ($C^{(char)}$), string comparisons ($C^{(str)}$), and swaps ($S$). It also manages the augmented data structure required for stability analysis.\n\n**2. Data Augmentation and Stability**\nHeapsort is not a stable sorting algorithm by nature. To empirically measure its stability for a given input, each string in the input array is augmented with its original index. We create a list of pairs $(\\text{string}, \\text{original\\_index})$. The heapsort algorithm will sort this list of pairs, but the comparison logic will only consider the string component. After the sort is complete, we can check if the relative order of elements with equal string keys has been preserved by inspecting their original indices. If for every maximal contiguous block of equal strings, the original indices are in non-decreasing order, the sort for that input is deemed stable, and the boolean flag $Stab$ is set to true.\n\n**3. Instrumented Lexicographical Comparator**\nA custom comparator function is implemented to perform lexicographical comparisons as defined: for two strings $s$ and $t$, $s \\prec t$ if at the first differing character position $j$, $\\mathrm{ord}(s[j]) < \\mathrm{ord}(t[j])$, or if $s$ is a strict prefix of $t$. Since we are sorting in ascending order, the heapsort algorithm will build a max-heap, which requires a \"greater than\" comparison. Our comparator will implement this logic.\n\nCrucially, this comparator is instrumented. Each time it is invoked to compare two strings, the string-comparison counter $C^{(str)}$ is incremented by $1$. Within a single invocation, for each pair of characters compared at a given position, the character-comparison counter $C^{(char)}$ is incremented by $1$. As per the problem specification, the final check of string lengths when one is a prefix of another does not count as a character comparison.\n\n**4. Heapsort Algorithm Implementation**\nThe heapsort algorithm proceeds in two main phases:\n\n*   **Phase 1: Heap Build ($H^{(build)}$)**\n    The algorithm first converts the augmented array into a max-heap in-place. A max-heap is a binary tree where the value of each node is greater than or equal to the values of its children. This is achieved by iterating backwards from the last non-leaf node (at index $\\left\\lfloor \\frac{n-1}{2} \\right\\rfloor$) down to the root (index $0$) and applying a `sift-down` operation at each node. The `sift-down` operation ensures the heap property is maintained by repeatedly swapping a node with its largest child until it is no longer smaller than its children. After this phase, a one-time, non-instrumented check is performed to verify that the array correctly represents a max-heap, setting the boolean flag $H^{(build)}$.\n\n*   **Phase 2: Sorting**\n    Once the max-heap is built, the largest element is at the root of the heap (index $0$). The algorithm repeatedly extracts the maximum element and places it at the end of the array. In each step, for a heap of current size $k$:\n    1.  The root element (the maximum) is swapped with the element at index $k-1$. The swap counter $S$ is incremented.\n    2.  The size of the heap is notionally reduced to $k-1$.\n    3.  The new root element (which came from the end of the heap) is sifted down to its correct position to restore the max-heap property for the reduced heap.\n    This process is repeated until the heap size becomes $1$, at which point the array is fully sorted in ascending order.\n\n**5. Hashing the Output ($H$)**\nAfter sorting, a $64$-bit polynomial rolling hash $H$ of the sequence of sorted strings is computed to provide a compact verification of the final state. The hash is calculated iteratively using the formula $H \\gets \\big(H \\cdot B + x\\big) \\bmod M$, with base $B = 911382323$ and modulus $M = 2^{64}$. For each string in the sorted array, a separator value $x=0$ is processed first, followed by integer values $x = \\mathrm{ord}(c)+1$ for each character $c$ in the string. The use of $64$-bit unsigned integer arithmetic naturally handles the modulo $M$ operation.\n\n**6. Edge Cases**\nThe implementation correctly handles edge cases:\n*   An empty array ($n=0$): This results in zero costs ($C^{(char)}=0, C^{(str)}=0, S=0$), a hash of $0$, and vacuously true properties ($H^{(build)}=\\text{True}, Stab=\\text{True}$).\n*   A single-element array ($n=1$): This also results in zero sorting costs, as the array is already sorted. The hash is computed for the single element, and properties are trivially true.\n\nBy integrating these components into a single class, the required metrics $[H, C^{(char)}, C^{(str)}, S, H^{(build)}, Stab]$ are systematically computed for each test case provided in the problem statement.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the heapsort analysis on all test cases and print the results.\n    \"\"\"\n\n    class HeapsortAnalysis:\n        \"\"\"\n        Encapsulates the heapsort algorithm and its instrumentation for a single array.\n        This includes tracking comparison and swap counts, checking stability,\n        and computing a final hash.\n        \"\"\"\n        def __init__(self, arr):\n            self.original_arr = list(arr)\n            self.n = len(self.original_arr)\n            self.c_char = 0\n            self.c_str = 0\n            self.s = 0\n            # Use uint64 for modulus 2^64 arithmetic, as specified by M = 2^64.\n            self.B = np.uint64(911382323)\n\n        def _compare_greater(self, s1, s2):\n            \"\"\"\n            Instrumented lexicographical comparison for s1 > s2.\n            Increments string and character comparison counters according to the rules.\n            Returns True if s1 is lexicographically greater than s2.\n            \"\"\"\n            self.c_str += 1\n            min_len = min(len(s1), len(s2))\n            for i in range(min_len):\n                self.c_char += 1\n                if s1[i] != s2[i]:\n                    return ord(s1[i]) > ord(s2[i])\n            # One string is a prefix of the other, or they are equal.\n            # The longer string is considered greater. The length check itself\n            # does not count as a character comparison.\n            return len(s1) > len(s2)\n\n        def _sift_down(self, aug_arr, start, end):\n            \"\"\"\n            Restores the max-heap property for a subtree rooted at `start`.\n            The heap is bounded by the exclusive index `end`.\n            \"\"\"\n            root = start\n            while True:\n                child = 2 * root + 1\n                if child >= end:\n                    break\n                \n                swap = root\n                # Find the largest among root, left child, and right child\n                # using the instrumented comparator.\n                if self._compare_greater(aug_arr[child][0], aug_arr[swap][0]):\n                    swap = child\n                \n                right_child = child + 1\n                if right_child < end and self._compare_greater(aug_arr[right_child][0], aug_arr[swap][0]):\n                    swap = right_child\n\n                if swap == root:\n                    break\n                else:\n                    aug_arr[root], aug_arr[swap] = aug_arr[swap], aug_arr[root]\n                    self.s += 1\n                    root = swap\n\n        def _build_max_heap(self, aug_arr):\n            \"\"\"Converts the array `aug_arr` into a max-heap in-place.\"\"\"\n            start = (self.n // 2) - 1\n            for i in range(start, -1, -1):\n                self._sift_down(aug_arr, i, self.n)\n\n        def _check_heap_property(self, aug_arr):\n            \"\"\"\n            Verifies if `aug_arr` is a valid max-heap without affecting counters.\n            Uses standard un-instrumented string comparison.\n            \"\"\"\n            if self.n <= 1:\n                return True\n            # Only need to check non-leaf nodes\n            for i in range(self.n // 2):\n                left = 2 * i + 1\n                right = 2 * i + 2\n                if left < self.n and aug_arr[i][0] < aug_arr[left][0]:\n                    return False\n                if right < self.n and aug_arr[i][0] < aug_arr[right][0]:\n                    return False\n            return True\n\n        def _check_stability(self, sorted_aug_arr):\n            \"\"\"\n            Checks if the sort was stable by examining original indices of equal elements.\n            \"\"\"\n            if self.n < 2:\n                return True\n            i = 0\n            while i < self.n:\n                j = i + 1\n                while j < self.n and sorted_aug_arr[j][0] == sorted_aug_arr[i][0]:\n                    j += 1\n                # A block of equal elements exists from index i to j-1\n                if j > i + 1:\n                    # Check if original indices are in non-decreasing order\n                    for k in range(i, j - 1):\n                        if sorted_aug_arr[k][1] > sorted_aug_arr[k+1][1]:\n                            return False\n                i = j\n            return True\n\n        def _compute_hash(self, sorted_aug_arr):\n            \"\"\"\n            Computes the specified polynomial rolling hash of the sorted sequence of strings.\n            \"\"\"\n            h = np.uint64(0)\n            for s_item, _ in sorted_aug_arr:\n                # Process separator value 0\n                h *= self.B\n                # Process each character, mapping it to ord(c) + 1\n                for char in s_item:\n                    h = h * self.B + np.uint64(ord(char) + 1)\n            return int(h)\n\n        def run(self):\n            \"\"\"\n            Executes the instrumented heapsort and returns all specified metrics.\n            \"\"\"\n            if self.n == 0:\n                # Handle empty array case\n                return [0, 0, 0, 0, True, True]\n            \n            # Augment array with original indices for stability check\n            aug_arr = [(self.original_arr[i], i) for i in range(self.n)]\n\n            if self.n == 1:\n                # Handle single-element array case\n                h = self._compute_hash(aug_arr)\n                return [h, 0, 0, 0, True, True]\n\n            # Phase 1: Build a max-heap\n            self._build_max_heap(aug_arr)\n            \n            # After building, check if it's a valid max-heap\n            h_build = self._check_heap_property(aug_arr)\n            \n            # Phase 2: Sort by repeatedly extracting the max element\n            for i in range(self.n - 1, 0, -1):\n                # Move current root (max element) to the end of the unsorted part\n                aug_arr[0], aug_arr[i] = aug_arr[i], aug_arr[0]\n                self.s += 1\n                # Restore heap property on the reduced heap\n                self._sift_down(aug_arr, 0, i)\n                \n            # Post-processing and metric collection\n            h = self._compute_hash(aug_arr)\n            stab = self._check_stability(aug_arr)\n\n            return [h, self.c_char, self.c_str, self.s, h_build, stab]\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        [\"pear\",\"apple\",\"banana\",\"peach\",\"apricot\",\"grape\"],\n        [\"bob\",\"alice\",\"bob\",\"alice\",\"bob\"],\n        [],\n        [\"Z\"],\n        [\"éclair\",\"eclair\",\"Éclair\",\"ECLAIR\"],\n        [\"\", \"a\", \"aa\", \"ab\", \"\"]\n    ]\n\n    results = []\n    for case in test_cases:\n        analyzer = HeapsortAnalysis(case)\n        result = analyzer.run()\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    result_str = \",\".join([f\"[{','.join(map(str, r))}]\" for r in results])\n    print(f\"[{result_str}]\")\n\nsolve()\n```", "id": "3239748"}, {"introduction": "Real-world systems are not always perfect; data structures can become corrupted. This advanced problem places you in a scenario where a valid max-heap has been damaged, with $k$ elements arbitrarily changed. Your task is to think like an algorithm designer and determine the most efficient strategy to repair the heap, weighing the trade-offs between a full rebuild and targeted local fixes [@problem_id:3239861]. This exercise sharpens your skills in asymptotic analysis and algorithmic strategy, pushing you to justify why one approach is optimal over another under different conditions.", "problem": "A binary max-heap is a complete binary tree stored in an array so that for every index $i$ with $i > 1$, the heap property $A\\!\\left(\\left\\lfloor \\frac{i}{2} \\right\\rfloor\\right) \\ge A(i)$ holds, where $A(i)$ denotes the key at index $i$. Heapsort relies on first arranging the $n$ keys into such a heap and then repeatedly extracting the maximum while maintaining the heap property. Consider an array $A(1), A(2), \\dots, A(n)$ that previously satisfied the max-heap property but has been corrupted by a transient fault that arbitrarily changed the values of exactly $k$ distinct positions, potentially violating the heap property at those positions and along related ancestor or descendant paths.\n\nYou must restore the max-heap property before continuing heapsort. In some scenarios the indices of the $k$ corrupted positions are known (for example, a fault log recorded their locations), and in other scenarios they are unknown. Using only the core definitions of a complete binary tree and the heap property, and reasoning from the structure of array-based heaps, select all statements that correctly describe an asymptotically optimal strategy (tight up to constant factors in the worst case) to repair the heap, including the correct running-time dependence on $n$ and $k$ and the regime in which the strategy is optimal.\n\nA. If the $k$ corrupted indices are known, perform a local sift (either upward or downward as dictated by comparisons with the parent and children) starting from each corrupted position; the total running time is $O(k \\log n)$, and this is asymptotically optimal when $k = o\\!\\left(\\frac{n}{\\log n}\\right)$.\n\nB. If the corrupted indices are unknown or if $k$ is large relative to $n$, rebuild the heap over all $n$ elements using bottom-up heap construction (Floyd’s method), which runs in $O(n)$ time and is asymptotically optimal when $k = \\Omega\\!\\left(\\frac{n}{\\log n}\\right)$ or when the set of corrupted positions is unknown.\n\nC. Independently run a sift from every internal node to the leaves so that each node is re-validated; the running time is $O(n \\log n)$, and this is optimal because it touches all nodes that could have been affected.\n\nD. First sort the entire array using heapsort in $O(n \\log n)$ time and then rebuild the heap over the sorted array; the total running time is $O(n \\log n)$, and this is asymptotically optimal regardless of $k$.", "solution": "The user wants a critical validation of the problem statement, followed by a rigorous derivation of the solution and an evaluation of all provided options.\n\n### Problem Validation\n\nFirst, I will deconstruct and validate the problem statement.\n\n**Step 1: Extract Givens**\n- **Data Structure:** A binary max-heap represented as an array $A(1), A(2), \\dots, A(n)$.\n- **Heap Property Definition:** For every index $i$ such that $i > 1$, the property $A(\\lfloor \\frac{i}{2} \\rfloor) \\ge A(i)$ holds.\n- **Initial Condition:** The heap was initially valid.\n- **Event:** A fault arbitrarily changed the values at exactly $k$ distinct positions.\n- **Goal:** Restore the max-heap property.\n- **Scenarios:**\n    1.  The indices of the $k$ corrupted positions are known.\n    2.  The indices of the $k$ corrupted positions are unknown.\n- **Question:** Identify all statements that correctly describe an asymptotically optimal strategy to repair the heap, including the running time and the regime of optimality.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientific Grounding:** The problem is set within the well-established field of computer science, specifically data structures and algorithms. The definitions of a max-heap, its array representation, the heap property, and the concept of asymptotic complexity are all standard and rigorously defined. The problem is scientifically sound.\n- **Well-Posedness:** The problem is well-posed. It provides clear parameters ($n$, $k$), distinct scenarios (known vs. unknown indices), and a precise objective (restore the max-heap property optimally). A solution can be logically derived.\n- **Objectivity:** The language is formal and unambiguous. Terms like \"asymptotically optimal\" and \"running-time dependence\" are objective and have precise meanings in the context of algorithm analysis.\n- **Consistency and Completeness:** The problem statement is self-contained and consistent. It defines the necessary terms and provides all information required to analyze the algorithmic complexities.\n- **Other Flaws:** The problem does not suffer from any of the other listed flaws such as being unrealistic, trivial, or unverifiable. It is a classic and practical problem in algorithmic analysis.\n\n**Step 3: Verdict and Action**\nThe problem is **valid**. I will now proceed with a full derivation and analysis.\n\n### Solution Derivation\n\nThe core of the problem is to find the most efficient way to restore the max-heap property after $k$ elements have been altered. An element's value change can violate the max-heap property in two ways:\n1.  The new value is too large, possibly exceeding its parent's value. This requires a \"sift-up\" (or \"bubble-up\") operation to restore the property by swapping the element with its parent repeatedly until it reaches a valid position. The complexity of a single sift-up is bounded by the depth of the node, which is at most $O(\\log n)$.\n2.  The new value is too small, possibly being less than one of its children's values. This requires a \"sift-down\" (or \"heapify\") operation to restore the property by swapping the element with its largest child repeatedly. The complexity of a single sift-down is bounded by the height of the node, which is at most $O(\\log n)$.\n\nThere are two primary strategies to repair the heap:\n- **Strategy 1: Localized Repair.** If the locations of the $k$ corruptions are known, we can visit each of the $k$ indices and perform a local fix. For each corrupted element, we check if it violates the heap property with its parent (requiring a sift-up) or its children (requiring a sift-down). The total cost for this strategy is $k$ times the cost of a single fix, which is $k \\times O(\\log n) = O(k \\log n)$.\n- **Strategy 2: Global Rebuild.** Regardless of whether the indices are known, we can always rebuild the entire heap from scratch. The most efficient algorithm for this is the bottom-up construction method (often called Floyd's algorithm or simply Build-Heap), which iterates through all non-leaf nodes from bottom to top and applies a sift-down operation at each. The total time complexity for this standard algorithm is proven to be $O(n)$, which is more efficient than the naive approach of $O(n \\log n)$.\n\nWe must now determine which strategy is asymptotically optimal under which conditions.\n\n**Scenario A: The $k$ corrupted indices are known.**\nWe have a choice between Strategy $1$ (local repairs) with a cost of $O(k \\log n)$ and Strategy $2$ (global rebuild) with a cost of $O(n)$.\n- The local repair strategy is asymptotically faster if $k \\log n$ is asymptotically smaller than $n$. This corresponds to the regime $k = o(\\frac{n}{\\log n})$.\n- The global rebuild strategy is asymptotically faster or equivalent if $k \\log n$ is asymptotically greater than or equal to $n$. This corresponds to the regime $k = \\Omega(\\frac{n}{\\log n})$. In this case, the $O(k \\log n)$ cost becomes $\\Omega(n)$, so the $O(n)$ rebuild is optimal.\n\n**Scenario B: The $k$ corrupted indices are unknown.**\nIf we do not know where the faults occurred, we cannot use the targeted local repair strategy. We must verify the entire heap structure. To guarantee that the heap property holds everywhere, any algorithm must, in the worst case, inspect all $n$ elements. For instance, a single corruption at a leaf node can only be detected by comparing it to its parent, and checking all such parent-child relationships requires accessing a linear number of nodes. Therefore, there is a lower bound of $\\Omega(n)$ on the complexity of any algorithm for this scenario. The global rebuild strategy (Strategy $2$) runs in $O(n)$ time, matching this lower bound. Thus, it is asymptotically optimal.\n\n### Option-by-Option Analysis\n\n**A. If the $k$ corrupted indices are known, perform a local sift (either upward or downward as dictated by comparisons with the parent and children) starting from each corrupted position; the total running time is $O(k \\log n)$, and this is asymptotically optimal when $k = o\\!\\left(\\frac{n}{\\log n}\\right)$.**\n- **Analysis:** This statement accurately describes Strategy $1$. The running time for applying $k$ local sifts (each taking $O(\\log n)$) is indeed $O(k \\log n)$. As derived above, this strategy is superior to the $O(n)$ global rebuild precisely when $k = o(\\frac{n}{\\log n})$. In this regime, it is the asymptotically optimal approach.\n- **Verdict:** Correct.\n\n**B. If the corrupted indices are unknown or if $k$ is large relative to $n$, rebuild the heap over all $n$ elements using bottom-up heap construction (Floyd’s method), which runs in $O(n)$ time and is asymptotically optimal when $k = \\Omega\\!\\left(\\frac{n}{\\log n}\\right)$ or when the set of corrupted positions is unknown.**\n- **Analysis:** This statement accurately describes the conditions under which Strategy $2$ is optimal.\n    - If indices are unknown, the $\\Omega(n)$ lower bound makes the $O(n)$ rebuild optimal.\n    - If $k$ is large, defined as $k = \\Omega(\\frac{n}{\\log n})$, the local repair cost $O(k \\log n)$ becomes $\\Omega(n)$, making the $O(n)$ global rebuild optimal.\nThe use of Floyd's method and its $O(n)$ time complexity are correctly stated.\n- **Verdict:** Correct.\n\n**C. Independently run a sift from every internal node to the leaves so that each node is re-validated; the running time is $O(n \\log n)$, and this is optimal because it touches all nodes that could have been affected.**\n- **Analysis:** Running a sift-down operation from every internal node is a valid way to construct a heap. There are $\\lfloor n/2 \\rfloor$ internal nodes. A sift-down from a node near the root can take $O(\\log n)$ time. Performing this for all $\\approx n/2$ internal nodes leads to a worst-case running time of $O(n \\log n)$. However, this strategy is not optimal. Floyd's method, which processes nodes in a bottom-up order, achieves the same result in $O(n)$ time. Therefore, an $O(n \\log n)$ approach is suboptimal.\n- **Verdict:** Incorrect.\n\n**D. First sort the entire array using heapsort in $O(n \\log n)$ time and then rebuild the heap over the sorted array; the total running time is $O(n \\log n)$, and this is asymptotically optimal regardless of $k$.**\n- **Analysis:** This strategy is a significant overkill. The goal is to restore the heap property, not to fully sort the array. Sorting takes $O(n \\log n)$ time, after which building a heap takes an additional $O(n)$ time. The total complexity is dominated by the sort, resulting in $O(n \\log n)$. This is never optimal, as there is always an $O(n)$ solution (global rebuild) and sometimes an even better $O(k \\log n)$ solution. The claim that it is \"asymptotically optimal regardless of $k$\" is false.\n- **Verdict:** Incorrect.", "answer": "$$\\boxed{AB}$$", "id": "3239861"}]}