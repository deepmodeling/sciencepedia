{"hands_on_practices": [{"introduction": "The efficiency of an adaptive sorting algorithm is not a universal constant; it is fundamentally tied to the specific measure of 'presortedness' it is designed to exploit. This practice will challenge you to explore this critical concept by constructing an input that is considered 'nearly sorted' by one measure but highly disordered by another. By analyzing the performance of two hypothetical algorithms sensitive to different properties—the number of runs $R(\\pi)$ versus the number of inversions $I(\\pi)$—you will gain a deeper appreciation for the trade-offs and design principles behind adaptive sorting.", "problem": "Consider a set of $n$ distinct keys with the intended sorted order $1,2,\\dots,n$, where $n$ is an even integer and $n \\geq 4$. Two adaptive comparison-based sorting algorithms, $A_1$ and $A_2$, exploit different measures of presortedness. Define the number of inversions $I(\\pi)$ of a permutation $\\pi$ as the number of index pairs $(i,j)$ with $i<j$ and $\\pi(i)>\\pi(j)$, and define the number of ascending runs $R(\\pi)$ as the number of maximal contiguous non-decreasing subsequences in $\\pi$. Use the following well-tested facts as the fundamental base for modeling cost: an algorithm that merges existing runs executes a number of comparisons proportional to $n \\log R(\\pi)$, and an insertion-based algorithm executes a number of comparisons proportional to $n + I(\\pi)$. For this problem, model the comparison counts exactly as $C_1(n,R) = n \\ln R$ and $C_2(n,I) = n + I$, where $\\ln$ denotes the natural logarithm.\n\nConstruct a permutation $\\pi$ that is nearly sorted by the run measure, in the sense that $R(\\pi)$ is small, but far from sorted by the inversion measure, in the sense that $I(\\pi)$ is large. Specifically, construct $\\pi$ by concatenating two strictly increasing contiguous blocks of length $n/2$ each such that every key in the first block is larger than every key in the second block. Then, using the definitions of $I(\\pi)$ and $R(\\pi)$ and the given cost models $C_1$ and $C_2$, derive the exact closed-form expression (in terms of $n$ only) for the speedup factor $\\rho(n)$ defined as the ratio of $A_2$’s comparisons to $A_1$’s comparisons on this input:\n$$\n\\rho(n) = \\frac{C_2\\!\\left(n, I(\\pi)\\right)}{C_1\\!\\left(n, R(\\pi)\\right)}.\n$$\nProvide your final answer as a single simplified analytic expression. No rounding is required, and no physical units apply.", "solution": "The problem statement is validated as sound. It is well-posed, self-contained, and grounded in standard concepts from the analysis of algorithms. The definitions for the number of inversions $I(\\pi)$ and the number of runs $R(\\pi)$ are standard measures of presortedness. The cost models $C_1(n,R) = n \\ln R$ and $C_2(n,I) = n + I$ are explicitly given, removing any ambiguity. The construction of the permutation $\\pi$ is precisely defined, and the task is to derive a specific analytical expression. All conditions are internally consistent and allow for the derivation of a unique, meaningful solution. We may therefore proceed with the formal derivation.\n\nThe problem requires constructing a specific permutation $\\pi$ of the keys $\\{1, 2, \\dots, n\\}$, where $n$ is an even integer and $n \\ge 4$. The permutation is formed by concatenating two strictly increasing contiguous blocks of length $n/2$ each, with the additional constraint that every key in the first block is larger than every key in the second block.\n\nTo satisfy these constraints, the set of keys $\\{1, 2, \\dots, n\\}$ must be partitioned into two sets: the smaller keys $\\{1, 2, \\dots, n/2\\}$ and the larger keys $\\{n/2+1, \\dots, n\\}$. For all keys in the first block to be larger than all keys in the second, the first block must be comprised of the larger keys and the second block of the smaller keys. Since each block is also strictly increasing, they must be sorted internally.\nThe first block, of length $n/2$, is therefore the sequence $(n/2+1, n/2+2, \\dots, n)$.\nThe second block, of length $n/2$, is the sequence $(1, 2, \\dots, n/2)$.\n\nConcatenating these two blocks gives the permutation $\\pi$:\n$$\n\\pi = \\left(\\frac{n}{2}+1, \\frac{n}{2}+2, \\dots, n, 1, 2, \\dots, \\frac{n}{2}\\right)\n$$\n\nNext, we must determine the number of ascending runs, $R(\\pi)$, and the number of inversions, $I(\\pi)$, for this specific permutation.\n\n1.  **Calculation of the number of ascending runs, $R(\\pi)$**:\n    An ascending run is a maximal contiguous non-decreasing subsequence.\n    The first part of the sequence, $(\\frac{n}{2}+1, \\frac{n}{2}+2, \\dots, n)$, is strictly increasing and thus forms a single ascending run.\n    The sequence breaks at the junction between the element $n$ and the element $1$, since $n > 1$.\n    The second part of the sequence, $(1, 2, \\dots, \\frac{n}{2})$, is also strictly increasing and forms a second ascending run.\n    Therefore, the permutation $\\pi$ consists of exactly two ascending runs.\n    $$\n    R(\\pi) = 2\n    $$\n    This permutation is nearly sorted with respect to the run measure, as $R(\\pi)$ is a small constant.\n\n2.  **Calculation of the number of inversions, $I(\\pi)$**:\n    An inversion is a pair of indices $(i, j)$ such that $i < j$ and $\\pi(i) > \\pi(j)$. Let us analyze the possible pairs of indices.\n    - If both indices $i$ and $j$ are within the first block (i.e., $1 \\le i < j \\le n/2$), then $\\pi(i) < \\pi(j)$ because the first block is strictly increasing. This contributes $0$ inversions.\n    - If both indices $i$ and $j$ are within the second block (i.e., $n/2+1 \\le i < j \\le n$), then $\\pi(i) < \\pi(j)$ because the second block is strictly increasing. This contributes $0$ inversions.\n    - If index $i$ is in the first block ($1 \\le i \\le n/2$) and index $j$ is in the second block ($n/2+1 \\le j \\le n$), then $\\pi(i)$ is a key from the set $\\{\\frac{n}{2}+1, \\dots, n\\}$ and $\\pi(j)$ is a key from the set $\\{1, \\dots, \\frac{n}{2}\\}$. By construction, every key in the first block is larger than every key in the second block. Therefore, for every such pair $(i, j)$, it is true that $\\pi(i) > \\pi(j)$, creating an inversion.\n\n    To find the total number of such inversions, we multiply the number of possible choices for $i$ by the number of possible choices for $j$. The number of elements in the first block is $n/2$, and the number of elements in the second block is also $n/2$.\n    The total number of inversions is:\n    $$\n    I(\\pi) = \\left(\\frac{n}{2}\\right) \\times \\left(\\frac{n}{2}\\right) = \\frac{n^2}{4}\n    $$\n    This permutation is far from sorted with respect to the inversion measure, as $I(\\pi)$ grows quadratically with $n$.\n\nNow, we use the given cost models to find the number of comparisons for each algorithm.\nThe cost for algorithm $A_1$, which is based on merging runs, is:\n$$\nC_1(n, R(\\pi)) = n \\ln(R(\\pi)) = n \\ln(2)\n$$\nThe cost for algorithm $A_2$, which is an insertion-based algorithm, is:\n$$\nC_2(n, I(\\pi)) = n + I(\\pi) = n + \\frac{n^2}{4}\n$$\n\nFinally, we compute the speedup factor $\\rho(n)$, defined as the ratio of $C_2$ to $C_1$:\n$$\n\\rho(n) = \\frac{C_2(n, I(\\pi))}{C_1(n, R(\\pi))} = \\frac{n + \\frac{n^2}{4}}{n \\ln(2)}\n$$\nWe can simplify this expression. Factoring out $n$ from the numerator (permissible since $n \\ge 4$):\n$$\n\\rho(n) = \\frac{n \\left(1 + \\frac{n}{4}\\right)}{n \\ln(2)} = \\frac{1 + \\frac{n}{4}}{\\ln(2)}\n$$\nTo present the expression in a more standard form without a fraction in the numerator, we can write:\n$$\n\\rho(n) = \\frac{\\frac{4+n}{4}}{\\ln(2)} = \\frac{n+4}{4\\ln(2)}\n$$\nThis is the final, simplified, closed-form expression for the speedup factor $\\rho(n)$.", "answer": "$$\n\\boxed{\\frac{n+4}{4\\ln(2)}}\n$$", "id": "3203325"}, {"introduction": "Having explored how different measures of presortedness can lead to performance trade-offs, we now turn our attention to a concrete algorithm: Natural Mergesort. This algorithm is adaptive to the number of sorted 'runs' present in the input data. This exercise will guide you through a step-by-step analysis of Natural Mergesort's performance on a specific, structured input, allowing you to calculate the exact number of comparisons and see firsthand how identifying just two runs can reduce the sorting complexity to linear time.", "problem": "Consider an array $A$ of length $n$, where $n \\geq 3$ is odd, whose contents form a bitonic pattern given explicitly by $A = \\langle 1, 3, 5, \\dots, n, n-1, \\dots, 2 \\rangle$. An adaptive sorting algorithm is one whose running time depends on a formal measure of presortedness of the input rather than solely on $n$. Natural mergesort is an adaptive sorting technique that operates according to the following rules:\n- It scans the array from left to right to partition it into maximal monotonic runs, where a run is a contiguous subsequence that is either strictly nondecreasing or strictly nonincreasing.\n- Any strictly nonincreasing run is reversed in place to become strictly nondecreasing before merging.\n- It then repeatedly performs a stable two-way merge of adjacent runs until a single sorted run covering the entire array remains.\nAssume the standard comparison model in which the cost of the algorithm is measured by counting only key-to-key comparisons. In particular, a scan that tests monotonicity of adjacent elements uses exactly one comparison per adjacent pair, a reversal uses no comparisons, and the stable two-way merge of two strictly increasing runs of lengths $a$ and $b$ uses exactly $a + b - 1$ comparisons when all keys are distinct.\n\nUnder these assumptions, derive an exact closed-form expression, as a function of $n$, for the total number of key comparisons that natural mergesort performs on the array $A$ described above, including both the initial run detection phase and the merging phase. Express your final answer as a single symbolic expression. No rounding is required.", "solution": "The total number of key comparisons, $C_T$, is the sum of comparisons from the initial run detection phase, $C_{\\text{scan}}$, and the subsequent merging phase, $C_{\\text{merge}}$. We have $C_T = C_{\\text{scan}} + C_{\\text{merge}}$.\n\nFirst, we analyze the **run detection phase**.\nThe input is an array $A$ of length $n$, where $n \\geq 3$ is an odd integer. The array's elements form the bitonic sequence $A = \\langle 1, 3, 5, \\dots, n, n-1, \\dots, 2 \\rangle$.\nThe algorithm scans the array from left to right, comparing adjacent elements to find maximal monotonic runs. This requires one comparison for each adjacent pair. For an array of length $n$, there are $n-1$ adjacent pairs. Therefore, the cost of the scan is:\n$$\nC_{\\text{scan}} = n-1\n$$\nThis scan identifies two maximal monotonic runs.\n1.  The first part of the array, $\\langle 1, 3, 5, \\dots, n \\rangle$, is strictly increasing. This is the first run, $R_1$. The number of elements in this sequence of odd numbers from $1$ to $n$ is $\\frac{n-1}{2} + 1 = \\frac{n+1}{2}$. So, the length of $R_1$ is $L_1 = \\frac{n+1}{2}$.\n2.  The break in monotonicity occurs between the element $n$ and the next element, $n-1$. The rest of the array forms the second run, $R_2 = \\langle n-1, n-3, \\dots, 2 \\rangle$. This is a strictly decreasing run. Its length is $L_2 = n - L_1 = n - \\frac{n+1}{2} = \\frac{n-1}{2}$.\n\nNext, we analyze the **run preparation and merging phases**.\n-   Run $R_1$ is already nondecreasing, so no action is taken.\n-   Run $R_2$ is strictly nonincreasing and must be reversed in place to become nondecreasing. According to the problem statement, this reversal operation costs $0$ comparisons. After reversal, the run becomes $R'_2 = \\langle 2, 4, \\dots, n-1 \\rangle$.\n-   The algorithm now has a list of two adjacent, strictly increasing runs: $R_1$ and $R'_2$. It performs a single stable two-way merge on these runs.\n\nThe cost of merging two strictly increasing runs of lengths $a=L_1$ and $b=L_2$ is given as $a + b - 1$. The elements of $A$ are all distinct integers, so this cost model applies.\nThe number of comparisons for the merge is:\n$$\nC_{\\text{merge}} = L_1 + L_2 - 1 = \\left(\\frac{n+1}{2}\\right) + \\left(\\frac{n-1}{2}\\right) - 1\n$$\n$$\nC_{\\text{merge}} = \\frac{(n+1) + (n-1)}{2} - 1 = \\frac{2n}{2} - 1 = n - 1\n$$\n\nFinally, we compute the total number of comparisons by summing the costs of the scan and the merge.\n$$\nC_T = C_{\\text{scan}} + C_{\\text{merge}} = (n-1) + (n-1) = 2n-2\n$$\nThis is the exact closed-form expression for the total number of comparisons.", "answer": "$$\n\\boxed{2n - 2}\n$$", "id": "3203381"}, {"introduction": "This final practice moves from analysis to practical design, tackling a common scenario where data is 'almost' in place. You are tasked with developing an efficient algorithm for arrays where every element is guaranteed to be at most $k$ positions away from its final sorted location. By leveraging a min-priority queue to maintain a 'sliding window' of candidates, you will construct an elegant adaptive sort that achieves an impressive $O(n \\log k)$ time complexity, showcasing how the right data structure can be used to exploit a specific type of presortedness.", "problem": "You are given arrays over a totally ordered domain with the guarantee that, for each element, its displacement from its final position in the sorted order is at most $k$ indices. Formally, let the input be an array $A$ of length $n$ over a set with a total order relation $\\leq$. For each element $A[i]$, let $pos(A[i])$ denote the index of $A[i]$ in the array obtained by sorting $A$ in nondecreasing order (with ties resolved by any deterministic rule). The guarantee is that for all $i \\in \\{0,1,\\dots,n-1\\}$, the displacement satisfies $\\lvert pos(A[i]) - i \\rvert \\leq k$, where $k$ is a nonnegative integer. Your task is to design and implement an adaptive sorting technique that exploits this displacement bound to achieve worst-case time complexity $O(n \\log k)$ and space complexity $O(k)$, while returning the array sorted in nondecreasing order.\n\nBase definitions you may use:\n- A total order is a binary relation $\\leq$ on a set $S$ such that for all $x,y \\in S$, exactly one of $x < y$, $x = y$, or $x > y$ holds, and the relation is transitive and antisymmetric.\n- A priority queue (PQ) is an abstract data type supporting insertion of an element and extraction of the minimum element under $\\leq$, both in logarithmic time with respect to the number of elements currently stored.\n- A binary heap is a data structure implementing a priority queue with insertion and extract-min operations running in $O(\\log m)$ time where $m$ is the current number of elements in the heap.\n\nYour program must implement the described adaptive sorting algorithm, apply it to the following test suite, and produce the results as specified.\n\nTest suite (each test case is the pair $(A,k)$):\n1. $A = [12,3,5,9,24,15,18,21,30,27]$, $k = 3$.\n2. $A = [1,2,2,3]$, $k = 0$.\n3. $A = [2,1,4,3,6,5]$, $k = 1$.\n4. $A = []$, $k = 2$.\n5. $A = [42]$, $k = 10$.\n6. $A = [-2,-5,-2,3,0,3,7]$, $k = 2$.\n7. $A = [23,42,4,8,15,16]$, $k = 5$.\n8. $A = [30,40,50,10,20]$, $k = 100$.\n9. $A = [3,1,2,6,4,5,9,7,8,10]$, $k = 2$.\n\nOutput specification:\n- For each test case, the program must output the sorted array (a list of integers) corresponding to $A$ sorted in nondecreasing order.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test suite above. For example, the output format must be $[result_1,result_2,\\dots,result_9]$, where each $result_i$ is the sorted list for test case $i$.\n- All numeric values in the arrays are plain integers; no physical units or angles apply.\n\nConstraints:\n- $n$ is any nonnegative integer.\n- $k$ is any nonnegative integer (when $k \\geq n-1$, the guarantee is vacuous but the algorithm must still run and return the correct sorted order).", "solution": "The problem requires the design of an adaptive sorting algorithm for arrays that are \"nearly sorted.\" The specific property of near-sortedness is that any element at index $i$ in the input array $A$ will be at an index $j$ in the final sorted array such that its displacement $\\lvert i - j \\rvert$ is at most $k$, a given nonnegative integer. The algorithm must achieve a worst-case time complexity of $O(n \\log k)$ and an auxiliary space complexity of $O(k)$, where $n$ is the number of elements in the array.\n\n### Fundamental Principle\nThe core of the adaptive approach rests on a critical deduction from the given property: $\\lvert pos(A[i]) - i \\rvert \\leq k$, where $pos(A[i])$ is the final sorted index of element $A[i]$. This inequality can be rearranged to analyze the origin of elements in the sorted sequence. Let $S$ be the array $A$ sorted in nondecreasing order. For an element $S[j]$ at index $j$ in the sorted array, its original index $i$ in $A$ must satisfy $\\lvert j - i \\rvert \\leq k$. This implies $j-k \\leq i \\leq j+k$.\n\nThis is a powerful constraint. It tells us that the element that belongs at position $j$ in the sorted output, $S[j]$, must be located in the input array $A$ within the index range $[\\max(0, j-k), \\min(n-1, j+k)]$. More importantly for a constructive algorithm, it guarantees that the smallest overall element, $S[0]$, must originate from an index $i \\in [0, k]$ in $A$. The second smallest element, $S[1]$, must originate from an index $i \\in [0, 1+k]$ in $A$. In general, the $j$-th element of the sorted array, $S[j]$, must be present in the prefix $A[0 \\dots j+k]$ of the input array.\n\nThis locality suggests that we do not need to consider the entire array to find the next smallest element. Instead, we can maintain a \"window\" of candidate elements and repeatedly extract the minimum from this window. A min-priority queue, implemented as a min-heap, is the ideal data structure for this task, as it allows for efficient extraction of the minimum and insertion of new elements.\n\n### Algorithmic Design\nThe algorithm proceeds as follows, using a min-heap to manage a sliding window of candidates:\n1.  Initialize an empty list, `result`, which will store the sorted elements.\n2.  Initialize a min-priority queue, `pq`.\n3.  Populate the `pq` with the first `min(n, k+1)` elements from the input array $A$. This forms the initial window of candidates. In the case where $n \\le k$, all elements of $A$ are added. This step takes $O(k)$ time using a linear-time heapify algorithm.\n4.  Iterate $n$ times to build the sorted `result` array. In each iteration:\n    a. Extract the minimum element from the `pq`. By the principle established above, this element is guaranteed to be the next smallest element for the sorted sequence. Append it to `result`.\n    b. To maintain the sliding window, if there is a next uninspected element in the input array $A$, insert it into the `pq`. The element to be considered after processing the initial `min(n, k+1)` elements and extracting one is at index `min(n, k+1)`, and so on.\n\nThe size of the priority queue, `pq`, is crucial for the algorithm's performance. It is initialized with at most $k+1$ elements. In each step of the main loop, one element is removed and one is added (until the input array is exhausted). Therefore, the number of elements in `pq` never exceeds $k+1$.\n\n### Correctness Argument\nThe algorithm's correctness hinges on the loop invariant that before each extraction, the `pq` contains the smallest element among all yet-to-be-sorted elements.\nLet's formalize this. Suppose we are about to determine the $j$-th element of the sorted array, $S[j]$. At this point, the algorithm has already placed $S[0], \\dots, S[j-1]$ into the `result` array. The elements from the input array $A$ that have been inserted into the `pq` are from indices $0$ up to at least $j+k-1$ (or fewer if $n$ is smaller).\nThe element $S[j]$ must come from an original index $i \\le j+k$. Because all elements from $A[0 \\dots j+k-1]$ have been considered (either placed in `result` or are still in `pq`), and we are about to add $A[j+k]$ if it exists, the true $S[j]$ is guaranteed to be in the `pq`. Since `pq` is a min-priority queue, the `extract-min` operation will correctly return $S[j]$. This holds for all $j$ from $0$ to $n-1$.\n\n### Complexity Analysis\n-   **Time Complexity**: The initialization of the `pq` with $\\min(n, k+1)$ elements takes $O(\\min(n, k)) = O(k)$ time (since if $n \\le k$, it's $O(n)$, and if we assume $k < n$ for an adaptive sort, it's $O(k)$). The main loop runs $n$ times. Inside the loop, we perform one `extract-min` and at most one `insert` operation. The size of the `pq` is bounded by $k+1$. Thus, each heap operation takes $O(\\log k)$ time. The total time complexity is $O(k) + n \\times O(\\log k) = O(n \\log k)$. This meets the specified requirement. If $k=0$, the complexity is $O(n)$, and if $k \\geq n-1$, it becomes $O(n \\log n)$, behaving like a standard heapsort.\n-   **Space Complexity**: The auxiliary space is dominated by the priority queue. As its size is bounded by $k+1$, the space complexity is $O(k)$. The output array `result` requires $O(n)$ space, which is typically classified as output space rather than auxiliary space. The algorithm fulfills the $O(k)$ space complexity requirement for auxiliary storage.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport heapq\n\ndef adaptive_sort(A, k):\n    \"\"\"\n    Sorts a k-nearly-sorted array in O(n log k) time and O(k) auxiliary space.\n\n    An array is k-nearly-sorted if for every element, its final sorted position\n    is at most k indices away from its current position.\n\n    Args:\n        A (list): The input array of numbers.\n        k (int): The maximum displacement of any element.\n\n    Returns:\n        list: The sorted array.\n    \"\"\"\n    n = len(A)\n    if n == 0:\n        return []\n\n    # The size of the initial heap and the sliding window.\n    # If k >= n-1, this will be n, correctly turning the algorithm into a heapsort.\n    window_size = min(n, k + 1)\n    \n    # Python's heapq implements a min-heap.\n    # Initialize the heap with the first `window_size` elements.\n    pq = A[:window_size]\n    heapq.heapify(pq)  # This operation takes O(window_size) time.\n\n    result = []\n    # Index of the next element from A to be added to the heap.\n    next_elem_idx = window_size\n\n    # The loop runs n times, once for each element to be placed in the result.\n    for _ in range(n):\n        # The smallest element in the heap is the next element in the sorted sequence.\n        # We can extract it.\n        min_val = heapq.heappop(pq)\n        result.append(min_val)\n\n        # If there are more elements in the input array, add the next one to the heap\n        # to maintain the sliding window of candidates.\n        if next_elem_idx  n:\n            heapq.heappush(pq, A[next_elem_idx])\n            next_elem_idx += 1\n            \n    return result\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        ([12, 3, 5, 9, 24, 15, 18, 21, 30, 27], 3),\n        ([1, 2, 2, 3], 0),\n        ([2, 1, 4, 3, 6, 5], 1),\n        ([], 2),\n        ([42], 10),\n        ([-2, -5, -2, 3, 0, 3, 7], 2),\n        ([23, 42, 4, 8, 15, 16], 5),\n        ([30, 40, 50, 10, 20], 100),\n        ([3, 1, 2, 6, 4, 5, 9, 7, 8, 10], 2)\n    ]\n\n    results = []\n    for A, k in test_cases:\n        sorted_A = adaptive_sort(list(A), k) # Use a copy to not modify original\n        results.append(sorted_A)\n\n    # Format the final list of results into the specified string format.\n    # e.g., [[1, 2], [3, 4]] -> \"[[1, 2], [3, 4]]\"\n    # Using str() on a list automatically includes spaces, e.g., '[1, 2, 3]'.\n    # To match a compact format, we might need custom string formatting.\n    # The prompt's example code `','.join(map(str, results))` suggests `str()` is fine.\n    # Let's verify: `str([1,2])` is `[1, 2]`. This is list-like. Let's make it compact.\n    formatted_results = [f\"[{','.join(map(str, res))}]\" for res in results]\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3203352"}]}