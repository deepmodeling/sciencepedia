{"hands_on_practices": [{"introduction": "To master open addressing, we must first understand the fundamental costs of its operations. This exercise challenges you to think like a hash table architect, working backward from a target configuration to find the most efficient sequence of insertions and deletions. By carefully choosing keys and operations, you will gain a hands-on understanding of how linear probing, deletions, and tombstones interact, and how to minimize the total number of probes—the fundamental measure of performance [@problem_id:3227311].", "problem": "Consider a hash table that uses open addressing with linear probing. The table has size $m=7$ with slots indexed $0,1,2,3,4,5,6$. Keys are integers and the hash function is $h(k)=k \\bmod 7$. A successful deletion marks the deleted slot with a tombstone (denoted $\\dagger$) rather than making it empty; a search for a key starts at $h(k)$ and linearly probes successive indices modulo $7$ until it finds the key or an empty slot, and a tombstone does not terminate the search. An insertion of a key $k$ starts at $h(k)$ and linearly probes until it finds either an empty slot or the first tombstone and stores the key there. The cost model is the standard one for open addressing: each slot inspection during search, insertion, or deletion counts as one probe.\n\nStarting from an empty table, you are allowed to choose any integer keys and perform any sequence of insertions and deletions. Your goal is to reach a final state in which the table has tombstones exactly at slots $2$, $4$, and $6$, and all other slots are empty with no active keys remaining in the table. Under the rules stated above, what is the minimal total number of slot inspections (probes) required over the entire sequence of operations to achieve such a final state? Give your answer as an exact integer with no units.", "solution": "The problem asks for the minimum total number of slot inspections (probes) required to transform an empty hash table of size $m=7$ into a state with tombstones at slots $2$, $4$, and $6$, and all other slots empty. The hash table uses open addressing with linear probing, and the hash function is $h(k) = k \\bmod 7$.\n\nFirst, let us analyze the process of creating a single tombstone. A tombstone is created at a slot $i$ by first inserting a key into that slot and subsequently deleting that same key. This requires a sequence of at least one insertion and one deletion operation.\n\nThe cost of any operation (insertion, search, or deletion) is defined as the number of slots inspected. Since every operation must inspect at least one slot, the cost of any single operation is at least $1$. Therefore, the minimum cost to create one tombstone is at least $1$ (for insertion) $+ 1$ (for deletion) $= 2$ probes.\n\nOur goal is to create three distinct tombstones at slots $2$, $4$, and $6$. Since each tombstone requires at least two probes to create, and the operations for creating one do not eliminate the need for operations for another, the total number of probes for creating three tombstones must be at least $3 \\times 2 = 6$. This establishes a lower bound on the total cost.\n\nNow, we must determine if this lower bound of $6$ probes is achievable. To do this, we need to construct a sequence of operations that achieves the target state and has a total cost of exactly $6$. A total cost of $6$ requires that each of the three necessary insertions and three necessary deletions costs exactly $1$ probe each.\n\nLet's analyze the cost of an insertion and a deletion.\nThe cost of inserting a key $k$ is the number of slots probed starting from index $h(k)$ until an available slot is found. An available slot is defined as either an empty slot or the first slot containing a tombstone. The cost is minimized to $1$ if the slot at index $h(k)$ is itself available.\nThe cost of deleting a key $k$ that resides at slot $i$ is the number of slots probed starting from $h(k)$ until slot $i$ is reached. The cost is minimized to $1$ if the key $k$ was placed such that $h(k)=i$.\n\nTo achieve a cost of $1$ for both insertion and deletion for a tombstone at slot $i$, we should choose a key $k$ such that $h(k)=i$. Let's devise a sequence of operations based on this principle. We need to create tombstones at slots $i_1=2$, $i_2=4$, and $i_3=6$.\n\nWe can choose three distinct keys, $k_1, k_2, k_3$, such that their hash values correspond to the desired tombstone locations. Let us choose $k_1=2$, $k_2=4$, and $k_3=6$.\n$h(k_1) = 2 \\bmod 7 = 2$\n$h(k_2) = 4 \\bmod 7 = 4$\n$h(k_3) = 6 \\bmod 7 = 6$\n\nConsider the following sequence of operations, starting with an empty table.\n\n1.  **Insert key $k_1=2$**: The probe starts at $h(2)=2$. Slot $2$ is empty. The key $2$ is placed in slot $2$.\n    Number of probes: $1$.\n    Table state: `[ , , 2, , , , ]`.\n    Cumulative probes: $1$.\n\n2.  **Insert key $k_2=4$**: The probe starts at $h(4)=4$. Slot $4$ is empty. The key $4$ is placed in slot $4$.\n    Number of probes: $1$.\n    Table state: `[ , , 2, , 4, , ]`.\n    Cumulative probes: $1+1=2$.\n\n3.  **Insert key $k_3=6$**: The probe starts at $h(6)=6$. Slot $6$ is empty. The key $6$ is placed in slot $6$.\n    Number of probes: $1$.\n    Table state: `[ , , 2, , 4, , 6]`.\n    Cumulative probes: $2+1=3$.\n\nAt this point, all necessary keys are in the table. Now we delete them to create tombstones.\n\n4.  **Delete key $k_1=2$**: The search starts at $h(2)=2$. The key $2$ is found at the first probed slot, index $2$. The slot is marked with a tombstone ($\\dagger$).\n    Number of probes: $1$.\n    Table state: `[ , , †, , 4, , 6]`.\n    Cumulative probes: $3+1=4$.\n\n5.  **Delete key $k_2=4$**: The search starts at $h(4)=4$. The key $4$ is found at slot $4$. The slot is marked with a tombstone.\n    Number of probes: $1$.\n    Table state: `[ , , †, , †, , 6]`.\n    Cumulative probes: $4+1=5$.\n\n6.  **Delete key $k_3=6$**: The search starts at $h(6)=6$. The key $6$ is found at slot $6$. The slot is marked with a tombstone.\n    Number of probes: $1$.\n    Table state: `[ , , †, , †, , †]`.\n    Cumulative probes: $5+1=6$.\n\nThe final state of the table has tombstones at slots $2$, $4$, and $6$, with all other slots ($0, 1, 3, 5$) remaining empty. This perfectly matches the problem's requirement. The total number of probes for this sequence is $6$.\n\nSince we have established a lower bound of $6$ probes and have demonstrated a sequence of operations that achieves this cost, the minimal total number of probes must be $6$. Any other choice of keys or order of operations could not result in a lower cost. For instance, using a key $k$ that hashes to a value different from its final position $i$ (i.e., $h(k) \\neq i$) would result in a probe count greater than $1$ for both insertion (due to collisions or an initially occupied slot) and deletion, thus increasing the total cost. The specific insertion rule (stopping at the first tombstone) does not alter this optimal strategy, as the minimal-cost path involves inserting into empty slots.", "answer": "$$\\boxed{6}$$", "id": "3227311"}, {"introduction": "While individual operations may seem efficient, their collective behavior can reveal serious performance pitfalls. This practice explores primary clustering, a critical weakness of linear probing, by investigating how batch insertion order impacts performance. You will implement an experiment comparing sorted and random key insertions to demonstrate empirically how non-random patterns can cause clusters to form and merge, leading to drastically increased operation costs [@problem_id:3257202].", "problem": "You are to implement and analyze an open addressing hash table with linear probing to investigate how batch insertion order affects performance. The investigation must compare two batch insertion orders for the same set of integer keys: ascending sorted order and a pseudorandom order. The comparison must quantify the effect using well-defined metrics and a fixed hash function that creates a monotone mapping from key values to home addresses, leading to potentially non-uniform collisions when keys are not uniformly distributed.\n\nFundamental base and core definitions:\n- Open addressing inserts each key into a single array of capacity $m$ by probing a sequence of candidate indices until finding an empty slot.\n- Linear probing defines the probe sequence for a key $k$ as successive indices $h(k), h(k)+1, h(k)+2, \\dots$ modulo $m$, where $h(k)$ is the hash of $k$.\n- The hash function to implement is the division method $h(k) = \\left\\lfloor \\dfrac{m \\cdot k}{U} \\right\\rfloor$, where $U$ is a positive integer defining the key universe scale, and $k$ is an integer key in the range $[0, U-1]$.\n- The number of probes for inserting a single key is defined as the count of examined slots, including the final successful slot.\n- The maximum contiguous occupied cluster length at the end of a batch is defined as the maximum number of consecutive occupied slots encountered when traversing the table cyclically (wrap-around is considered; the traversal must avoid double-counting by starting immediately after some empty slot).\n\nTwo insertion orders:\n- Sorted order: keys inserted in ascending numerical order.\n- Random order: keys inserted in the order induced by sorting a pseudorandom score for each key generated by a Linear Congruential Generator (LCG). The Linear Congruential Generator (LCG) is defined by $X_{i+1} = (a X_i + c) \\bmod M$ with parameters $a = 1664525$, $c = 1013904223$, and $M = 2^{32}$. A fixed seed $X_0$ is given per test case. To obtain a permutation, generate a score $X_i$ per key and sort keys by these scores.\n\nKey set construction:\n- For each test case, keys are constructed to induce a bimodal density in home addresses: a low-index group and a high-index group.\n- Let $L$ and $t$ be positive integers. The low group consists of $t$ keys for each bucket index $i \\in \\{0, 1, \\dots, L-1\\}$; the high group consists of $t$ keys for each bucket index $j \\in \\{m-L, m-L+1, \\dots, m-1\\}$.\n- For bucket index $b$, choose $t$ distinct keys $k$ from the interval $I_b = \\left[\\left\\lfloor \\dfrac{U \\cdot b}{m} \\right\\rfloor, \\left\\lfloor \\dfrac{U \\cdot (b+1)}{m} \\right\\rfloor - 1\\right]$ so that $h(k) = b$. Use the first $t$ integers in $I_b$ (in ascending order). The total number of keys is $n = 2Lt$, and the load factor is $n/m$. All test cases satisfy $n < m$ to allow successful insertion.\n\nMetrics to compute per test case:\n- The average number of probes for sorted order, denoted $\\overline{P}_{\\mathrm{sorted}}$.\n- The average number of probes for random order, denoted $\\overline{P}_{\\mathrm{random}}$.\n- The final maximum cluster length for sorted order, denoted $C_{\\mathrm{sorted}}$.\n- The final maximum cluster length for random order, denoted $C_{\\mathrm{random}}$.\n- Report the ratios $R_P = \\dfrac{\\overline{P}_{\\mathrm{sorted}}}{\\overline{P}_{\\mathrm{random}}}$ and $R_C = \\dfrac{C_{\\mathrm{sorted}}}{C_{\\mathrm{random}}}$.\n\nTest suite:\nFor each parameter tuple $(m, U, L, t, \\text{seed})$, construct keys and measure the two ratios above.\n1. $(m, U, L, t, \\text{seed}) = (1024, 1024000, 240, 2, 123456789)$ so that $n = 2 \\cdot 240 \\cdot 2 = 960$.\n2. $(m, U, L, t, \\text{seed}) = (512, 512000, 120, 2, 987654321)$ so that $n = 480$.\n3. $(m, U, L, t, \\text{seed}) = (257, 257000, 64, 1, 20231102)$ so that $n = 128$.\n4. $(m, U, L, t, \\text{seed}) = (128, 128000, 24, 2, 31415926)$ so that $n = 96$.\n\nRequired final output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain, in order for each test case, first $R_P$ then $R_C$, both as floating-point numbers. For the entire test suite above, the output must be:\n$[R_{P,1}, R_{C,1}, R_{P,2}, R_{C,2}, R_{P,3}, R_{C,3}, R_{P,4}, R_{C,4}]$.\n\nConstraints and expectations:\n- Implement the hash table and metrics exactly as defined.\n- No external input is permitted; the program must embed the test suite and print the single output line.\n- Use the specified LCG to generate the pseudorandom order per test case.\n- The program must be complete and runnable and must strictly adhere to the final output format described above.", "solution": "The user provided a detailed problem statement to analyze the performance of an open addressing hash table with linear probing. The analysis focuses on the impact of insertion order by comparing a sorted insertion of keys against a pseudorandom insertion order.\n\n### Step 1: Problem Validation\n\nThe problem is subjected to a rigorous validation process.\n\n**Givens Extracted from the Problem Statement:**\n\n*   **Hash Table Method:** Open addressing with linear probing.\n*   **Table Capacity:** $m$.\n*   **Key Universe:** Integers $k$ in the range $[0, U-1]$.\n*   **Hash Function:** Division method, $h(k) = \\left\\lfloor \\dfrac{m \\cdot k}{U} \\right\\rfloor$.\n*   **Probe Sequence:** For a key $k$, the sequence is $h(k), (h(k)+1) \\pmod m, (h(k)+2) \\pmod m, \\dots$.\n*   **Probe Count Definition:** The number of slots examined for an insertion, including the final empty slot where the key is placed.\n*   **Maximum Cluster Length Definition:** The maximum length of a contiguous block of occupied slots, considering cyclic wrap-around. The measurement starts after an empty slot to handle the wrap-around case unambiguously.\n*   **Key Set Construction:**\n    *   Parameters $L$ and $t$ are positive integers.\n    *   A bimodal distribution of keys is generated.\n    *   Low-index group: For each bucket index $b \\in \\{0, 1, \\dots, L-1\\}$, select the first $t$ integers $k$ from the interval $I_b = \\left[\\left\\lfloor \\dfrac{U \\cdot b}{m} \\right\\rfloor, \\left\\lfloor \\dfrac{U \\cdot (b+1)}{m} \\right\\rfloor - 1\\right]$ such that $h(k)=b$.\n    *   High-index group: For each bucket index $b \\in \\{m-L, m-L+1, \\dots, m-1\\}$, select keys similarly.\n    *   Total number of keys is $n = 2Lt$. It is stated that $n < m$.\n*   **Insertion Orders:**\n    *   **Sorted:** Keys are inserted in ascending numerical order.\n    *   **Random:** Keys are inserted according to an order determined by a Linear Congruential Generator (LCG).\n*   **LCG Specification:** $X_{i+1} = (a X_i + c) \\pmod M$, with $a = 1664525$, $c = 1013904223$, $M = 2^{32}$, and a specific seed $X_0$ for each test case. Keys are sorted based on their generated LCG scores.\n*   **Metrics for Comparison:**\n    *   Average probes for sorted insertion: $\\overline{P}_{\\mathrm{sorted}}$.\n    *   Average probes for random insertion: $\\overline{P}_{\\mathrm{random}}$.\n    *   Maximum final cluster length for sorted insertion: $C_{\\mathrm{sorted}}$.\n    *   Maximum final cluster length for random insertion: $C_{\\mathrm{random}}$.\n*   **Ratios to be Computed:** $R_P = \\dfrac{\\overline{P}_{\\mathrm{sorted}}}{\\overline{P}_{\\mathrm{random}}}$ and $R_C = \\dfrac{C_{\\mathrm{sorted}}}{C_{\\mathrm{random}}}$.\n*   **Test Cases:** Four tuples of $(m, U, L, t, \\text{seed})$ are provided.\n    1.  $(1024, 1024000, 240, 2, 123456789)$\n    2.  $(512, 512000, 120, 2, 987654321)$\n    3.  $(257, 257000, 64, 1, 20231102)$\n    4.  $(128, 128000, 24, 2, 31415926)$\n\n**Validation Verdict:**\n\nThe problem is **valid**. It is scientifically grounded in the principles of data structures and algorithms, specifically hash table analysis. It is well-posed, with all necessary parameters, initial conditions, and definitions provided explicitly. The terminology is precise, and the objectives are objective and quantifiable. The problem describes a deterministic procedure for which a unique solution exists and is computationally feasible. The setup is self-contained and free of contradictions; the condition $n < m$ is satisfied for all test cases, ensuring that all insertions can succeed. The experiment is well-designed to demonstrate the phenomenon of primary clustering in linear probing under specific non-uniform conditions.\n\n### Step 2: Solution Derivation\n\nThe solution involves implementing the specified hash table, key generation, insertion orders, and metric calculations for each test case.\n\n**1. Algorithmic Framework**\n\nFor each test case defined by the parameters $(m, U, L, t, \\text{seed})$:\n1.  Generate the set of $n=2Lt$ integer keys.\n2.  Create two insertion sequences: one sorted, one pseudorandom.\n3.  For each sequence:\n    a.  Initialize an empty hash table of size $m$.\n    b.  Insert all keys, calculating the total number of probes.\n    c.  Compute the average number of probes, $\\overline{P}$.\n    d.  After all insertions, compute the maximum cluster length, $C$.\n4.  Calculate the ratios $R_P$ and $R_C$.\n\n**2. Key Generation**\n\nThe set of keys is the union of two groups:\n*   A low-index group: for each bucket index $b$ from $0$ to $L-1$, we generate $t$ keys. The keys for a given $b$ are the first $t$ integers in the interval $I_b$. The starting key for bucket $b$ is $k_{b,0} = \\lfloor (U \\cdot b) / m \\rfloor$. The $t$ keys are thus $\\{k_{b,0}, k_{b,0}+1, \\dots, k_{b,0}+t-1\\}$.\n*   A high-index group: for each bucket index $b$ from $m-L$ to $m-1$, we generate $t$ keys in the same manner.\nThe total number of keys is $n=2Lt$.\n\n**3. Insertion Order Generation**\n\n*   **Sorted Order:** The entire set of $n$ keys is sorted numerically in ascending order.\n*   **Random Order:** A pseudorandom permutation of the keys is generated.\n    1.  Initialize the LCG state with the given seed: $X_0 = \\text{seed}$.\n    2.  Generate $n$ pseudorandom numbers, $X_1, X_2, \\dots, X_n$, using the recurrence $X_{i+1} = (a X_i + c) \\pmod M$, where $a=1664525$, $c=1013904223$, and $M=2^{32}$.\n    3.  Create pairs of (key, random score).\n    4.  Sort these pairs based on the random score. The resulting order of keys is the pseudorandom insertion sequence.\n\n**4. Hash Table Operations and Metrics**\n\nThe hash table is an array of size $m$, where each cell can be empty or occupied.\n\n*   **Insertion and Probe Counting:** To insert key $k$:\n    1.  Compute the home address $j_0 = h(k) = \\lfloor (m \\cdot k) / U \\rfloor$.\n    2.  Initialize probe count $p=1$.\n    3.  Examine table slot $j = (j_0 + p - 1) \\pmod m$.\n    4.  If the slot is empty, place $k$ there and terminate the insertion for this key. The number of probes is $p$.\n    5.  If the slot is occupied, increment $p$ and repeat from step $3$.\n    The total number of probes is accumulated over all $n$ insertions. The average is $\\overline{P} = (\\sum_{i=1}^{n} p_i) / n$.\n\n*   **Maximum Cluster Length Calculation:**\n    1.  After all $n$ keys are inserted, find the first empty slot in the table array at index $j_{empty}$. If no slot is empty, the cluster length is $m$. The problem guarantees $n < m$, so at least one empty slot exists.\n    2.  Initialize `max_length = 0` and `current_length = 0`.\n    3.  Iterate through the table cyclically for $m$ positions, starting from index $(j_{empty} + 1) \\pmod m$.\n    4.  For each slot, if it is occupied, increment `current_length`.\n    5.  If it is empty, update `max_length = max(max_length, current_length)` and reset `current_length = 0$.\n    6.  After the loop completes, perform one final update `max_length = max(max_length, current_length)` to account for any cluster that wraps around the table's end. This final value is $C$.\n\n**5. Analysis of Expected Behavior**\n\nThe hash function $h(k) = \\lfloor (m \\cdot k) / U \\rfloor$ is order-preserving. Keys that are numerically close are mapped to the same or adjacent hash table slots.\n\n*   **Sorted Insertion:** Keys are inserted in increasing order. Keys for bucket $0$ are inserted, then keys for bucket $1$, and so on. When inserting keys for bucket $b$, they will likely collide with the cluster already formed by keys from buckets $0, \\ldots, b-1$. This sequential insertion into adjacent home slots will cause primary clustering to become severe, creating very large clusters and long probe sequences. We expect $\\overline{P}_{\\mathrm{sorted}}$ and $C_{\\mathrm{sorted}}$ to be large.\n\n*   **Random Insertion:** The pseudorandom order ensures that consecutive insertions are unlikely to be for keys that are numerically close. An insertion into the low-index region might be followed by an insertion into the high-index region. This effectively breaks the pattern that causes catastrophic primary clustering, distributing the insertions more evenly over time across the collision-prone regions. We expect $\\overline{P}_{\\mathrm{random}}$ and $C_{\\mathrm{random}}$ to be significantly smaller than their sorted-order counterparts.\n\nConsequently, the ratios $R_P = \\overline{P}_{\\mathrm{sorted}} / \\overline{P}_{\\mathrm{random}}$ and $R_C = C_{\\mathrm{sorted}} / C_{\\mathrm{random}}$ are expected to be significantly greater than $1$, quantifying the performance degradation caused by the sorted insertion order under these specific conditions.\nThe implementation will now proceed based on this formal design.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the hash table analysis problem.\n    It iterates through predefined test cases, runs the insertion experiments,\n    and prints the final results in the required format.\n    \"\"\"\n    test_cases = [\n        (1024, 1024000, 240, 2, 123456789),\n        (512, 512000, 120, 2, 987654321),\n        (257, 257000, 64, 1, 20231102),\n        (128, 128000, 24, 2, 31415926),\n    ]\n\n    # LCG parameters\n    LCG_A = 1664525\n    LCG_C = 1013904223\n    LCG_M_MASK = 0xFFFFFFFF  # (2**32 - 1)\n\n    EMPTY_SLOT = -1 # Sentinel for an empty slot in the hash table\n\n    def generate_keys(m, U, L, t):\n        \"\"\"Generates the bimodal key set based on problem parameters.\"\"\"\n        keys = []\n        # Low-index group\n        for b in range(L):\n            start_key = (U * b) // m\n            keys.extend(range(start_key, start_key + t))\n        # High-index group\n        for b in range(m - L, m):\n            start_key = (U * b) // m\n            keys.extend(range(start_key, start_key + t))\n        return keys\n\n    def get_random_order(keys, n, seed):\n        \"\"\"Permutes keys based on scores from a Linear Congruential Generator.\"\"\"\n        lcg_state = seed\n        scores = []\n        for _ in range(n):\n            lcg_state = (LCG_A * lcg_state + LCG_C) & LCG_M_MASK\n            scores.append(lcg_state)\n        \n        # Pair keys with scores and sort by score\n        keyed_scores = list(zip(keys, scores))\n        keyed_scores.sort(key=lambda x: x[1])\n        \n        return [key for key, score in keyed_scores]\n\n    def run_experiment(m, U, n, key_order):\n        \"\"\"\n        Runs a single experiment for a given key insertion order.\n        Returns the average number of probes and the final max cluster length.\n        \"\"\"\n        table = np.full(m, EMPTY_SLOT, dtype=np.int64)\n        total_probes = 0\n\n        for key in key_order:\n            home_address = (m * key) // U\n            probes = 0\n            while True:\n                probes += 1\n                current_pos = (home_address + probes - 1) % m\n                if table[current_pos] == EMPTY_SLOT:\n                    table[current_pos] = key\n                    total_probes += probes\n                    break\n        \n        avg_probes = total_probes / n\n\n        # Calculate max cluster length\n        max_len = 0\n        current_len = 0\n        \n        # Per problem spec, start traversal after an empty slot to handle wrap-around\n        empty_indices = np.where(table == EMPTY_SLOT)[0]\n        if len(empty_indices) == 0:\n            # This case is not expected as n < m\n            max_len = m\n        else:\n            start_index = (empty_indices[0] + 1) % m\n            for i in range(m):\n                idx = (start_index + i) % m\n                if table[idx] != EMPTY_SLOT:\n                    current_len += 1\n                else:\n                    max_len = max(max_len, current_len)\n                    current_len = 0\n            # Final check for wrap-around cluster\n            max_len = max(max_len, current_len)\n            \n        return avg_probes, max_len\n\n    final_results = []\n    for m, U, L, t, seed in test_cases:\n        n = 2 * L * t\n        \n        keys = generate_keys(m, U, L, t)\n        \n        # Sorted order experiment\n        sorted_keys = sorted(keys)\n        p_sorted, c_sorted = run_experiment(m, U, n, sorted_keys)\n        \n        # Random order experiment\n        random_keys = get_random_order(keys, n, seed)\n        p_random, c_random = run_experiment(m, U, n, random_keys)\n        \n        # Calculate and store ratios\n        R_P = p_sorted / p_random\n        # Handle division by zero for cluster length, though not expected here\n        R_C = (c_sorted / c_random) if c_random != 0 else float('inf')\n        \n        final_results.append(R_P)\n        final_results.append(R_C)\n\n    # Format the final output string\n    output_str = \"[\" + \",\".join(map(str, final_results)) + \"]\"\n    print(output_str)\n\nsolve()\n```", "id": "3257202"}, {"introduction": "Now that we've seen how clustering can arise, it's crucial to quantify its precise impact on performance. This exercise provides a controlled environment to measure the direct relationship between cluster size and search cost in linear probing. By synthetically creating a single large cluster and measuring search times, you will derive and empirically verify the linear scaling of probe counts, solidifying your understanding of why clustering is so detrimental to hash table efficiency [@problem_id:3257276].", "problem": "Consider a hash table implemented with open addressing and linear probing. The hash table has capacity $m$, a hash function $h(k)$, and probe sequence $p_j(k) = (h(k) + j) \\bmod m$ for $j = 0, 1, \\dots, m-1$. The cost model defines search time as the count of examined slots (probes), which is a unit-free measure in \"probes.\"\n\nStarting from the fundamental definitions:\n- Open addressing places all keys into the table itself; collisions are resolved by finding the next available slot using a probe sequence.\n- Linear probing uses an arithmetic progression probe sequence, which creates contiguous runs of occupied slots (clusters).\n- A search terminates when either the key is found in a probed slot, or an empty slot is encountered. A successful search cost equals the number of probes until the matching key is examined. An unsuccessful search cost equals the number of probes until the first empty slot is examined.\n\nPrimary clustering arises when contiguous runs of occupied slots form and grow; probing into such a cluster forces a sequential scan across the entire cluster. You will construct a synthetic benchmark to isolate this phenomenon.\n\nBenchmark design:\n- Use a modulus-based hash function $h(k) = k \\bmod m$.\n- Fix a start index $s$ and create a single contiguous cluster of length $C$ occupying slots $s, s+1, \\dots, s+C-1$ without wrap-around, with slot $s+C$ empty. Achieve this by inserting keys $k_i = s + i \\cdot m$ for $i = 0, 1, \\dots, C-1$. Under linear probing and the given hash function, each such key initially hashes to $s$ and is placed at the next available slot, filling the contiguous block.\n- For each cluster length $C$, measure:\n  1. The average successful search cost within the cluster, obtained by searching for all $C$ inserted keys and averaging their probe counts.\n  2. The unsuccessful search cost when the initial probe is at the start of the cluster, obtained by searching for a key $k^\\star = s + C \\cdot m$ that hashes to $s$ but is not present.\n  3. The average unsuccessful search cost when the initial probe is uniformly distributed over the cluster offsets, obtained by searching for $C$ distinct keys $u_\\delta = (s + \\delta) + M \\cdot m$ for $\\delta = 0, 1, \\dots, C-1$, where $M$ is a sufficiently large integer ensuring none of these keys is equal to an inserted key. Each such key starts probing at index $s + \\delta$ and is not present.\n\nScientific requirements:\n- Derive the expected dependence of these three costs on the cluster length $C$ from the stated definitions (no shortcut formulas). Your derivation must explain why search time scales linearly with $C$ when probing into a single contiguous cluster.\n- Treat boundary cases carefully. For $C = 0$, define the average successful search cost as $0$ by convention (no successful searches), the unsuccessful search cost from the start as $1$ probe (the start slot is empty), and the average unsuccessful search cost under uniform offsets as $1$ probe.\n\nTest suite:\n- Use a fixed table size $m = 4096$ and start index $s = 123$.\n- Evaluate cluster lengths $C$ in the set $\\{0, 1, 8, 64, 512, 2048, 3968\\}$, which covers an empty cluster, tiny clusters, moderate clusters, and near-maximum clusters without wrap-around since $s + C \\le m - 1$ for all listed values of $C$.\n\nProgram requirements:\n- Implement linear probing exactly as defined.\n- For each test case cluster length $C$, construct the cluster and compute the three metrics specified above.\n- Express all three metrics in probes (unit-free counts).\n- Aggregate the results for all test cases into a single line of output as a comma-separated list enclosed in square brackets. Each element of this list must itself be a bracketed comma-separated list of the form $[C,\\ \\text{avg\\_succ},\\ \\text{unsucc\\_start},\\ \\text{avg\\_unsucc\\_uniform}]$, where $C$ is an integer and the three costs are floats or integers in probes. For example, the top-level format is $[[\\dots],[\\dots],\\dots]$.", "solution": "The fundamental base is the definition of open addressing with linear probing and the probe-count cost model. Let the table capacity be $m$, the hash function be $h(k) = k \\bmod m$, and the probe sequence be $p_j(k) = (h(k) + j) \\bmod m$. A search examines slots sequentially along the probe sequence; the cost equals the total number of examined slots. A successful search stops when the examined slot contains the target key. An unsuccessful search stops when the examined slot is empty.\n\nConstruct one giant cluster by inserting $C$ keys that all hash to the same start index $s$:\n- Insert keys $k_i = s + i \\cdot m$ for $i \\in \\{0, 1, \\dots, C-1\\}$.\n- Each insertion begins at $h(k_i) = s$ and uses linear probing to find the next empty slot, so the inserted keys occupy $s, s+1, \\dots, s+C-1$ without wrap-around, provided $s + C \\le m - 1$. Slot $s+C$ remains empty.\n\nWe now derive search costs as functions of cluster length $C$.\n\nSuccessful search within the cluster:\n- Consider the key placed at offset $\\delta \\in \\{0, 1, \\dots, C-1\\}$, which lies in slot $s+\\delta$.\n- Its probe sequence begins at $s$, and the search scans $s, s+1, \\dots, s+\\delta$ until it finds the key. The number of probes equals $\\delta + 1$.\n- Averaging over all keys in the cluster, the average successful search cost is\n$$\n\\frac{1}{C} \\sum_{\\delta=0}^{C-1} (\\delta + 1) = \\frac{1}{C} \\left( \\sum_{\\delta=0}^{C-1} \\delta + \\sum_{\\delta=0}^{C-1} 1 \\right) = \\frac{1}{C} \\left( \\frac{(C-1)C}{2} + C \\right) = \\frac{C+1}{2}.\n$$\n- For the boundary case $C=0$, there are no successful searches to average; by convention we define the average as $0$.\n\nUnsuccessful search starting at the cluster start:\n- Consider a missing key $k^\\star$ with $h(k^\\star) = s$ (for example $k^\\star = s + C \\cdot m$).\n- The probe sequence begins at $s$ and scans the cluster slots $s, s+1, \\dots, s+C-1$, finding each occupied. The first empty slot is at $s+C$.\n- The number of probes equals the number of occupied slots examined plus the empty slot examination, which is $C + 1$.\n- For the boundary case $C=0$, the first probed slot $s$ is empty, so the cost is $1$.\n\nUnsuccessful search with a uniform starting offset over the cluster:\n- Consider missing keys $u_\\delta$ with $h(u_\\delta) = s + \\delta$ for $\\delta \\in \\{0, 1, \\dots, C-1\\}$ and $u_\\delta$ different from any inserted key.\n- The probe sequence begins at $s + \\delta$ and scans $s + \\delta, s + \\delta + 1, \\dots, s+C - 1$, then the first empty slot at $s+C$.\n- The number of probes for offset $\\delta$ is $(C - \\delta) + 1$.\n- Averaging over all offsets,\n$$\n\\frac{1}{C} \\sum_{\\delta=0}^{C-1} \\big( (C - \\delta) + 1 \\big) = \\frac{1}{C} \\left( \\sum_{\\delta=0}^{C-1} (C - \\delta) + \\sum_{\\delta=0}^{C-1} 1 \\right) = \\frac{1}{C} \\left( \\frac{C(C+1)}{2} + C \\right) = \\frac{C+3}{2}.\n$$\n- For the boundary case $C=0$, there are no offsets to average; the cost reduces to the start case, which is $1$.\n\nThese derivations demonstrate linear sensitivity to clustering under linear probing:\n- The average successful search cost grows as $\\frac{C+1}{2}$.\n- The unsuccessful search cost from the cluster start grows as $C+1$.\n- The average unsuccessful search cost with uniform starting offsets over the cluster grows as $\\frac{C+3}{2}$.\nEach expression is an affine function of the cluster length $C$, so creating one giant cluster causes search times to scale linearly with the cluster length.\n\nAlgorithmic realization:\n- Implement insertion and search with linear probing according to the stated definitions.\n- Build the cluster by inserting $C$ colliding keys with initial index $s$ using the modulus-based hash function.\n- Measure probe counts directly by instrumenting the search procedure to count examined slots.\n- For each test case $C \\in \\{0, 1, 8, 64, 512, 2048, 3968\\}$ with $m = 4096$ and $s = 123$, compute:\n  - The average successful search cost across all inserted keys (defined as $0$ for $C=0$).\n  - The unsuccessful search cost from the cluster start using a missing key hashing to $s$.\n  - The average unsuccessful search cost with uniform starting offsets using one missing key per offset.\n- Print the aggregated results as a single line in the specified nested list format. All reported values are in probes.\n\nThis principle-based design directly reflects how linear probing behaves under contiguous occupancy, without relying on shortcut formulas beyond the stated definitions, and produces measurements that match the derived linear scaling.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\nclass LinearProbingTable:\n    def __init__(self, m: int):\n        self.m = m\n        # Use None to denote empty slot\n        self.table = [None] * m\n\n    def h(self, k: int) -> int:\n        # Modulus-based hash function\n        return k % self.m\n\n    def insert(self, k: int) -> bool:\n        \"\"\"Insert key k using linear probing. Returns True if inserted, False if table is full.\"\"\"\n        idx = self.h(k)\n        for _ in range(self.m):\n            if self.table[idx] is None:\n                self.table[idx] = k\n                return True\n            idx = (idx + 1) % self.m\n        return False  # Table full\n\n    def probe_count_search(self, k: int) -> int:\n        \"\"\"Return number of probes used to search for k (successful or unsuccessful).\"\"\"\n        idx = self.h(k)\n        probes = 0\n        for _ in range(self.m):\n            probes += 1\n            slot = self.table[idx]\n            if slot is None:\n                return probes  # unsuccessful: empty slot terminates\n            if slot == k:\n                return probes  # successful: found the key\n            idx = (idx + 1) % self.m\n        # If full cycle without finding empty or the key (should not happen with our construction),\n        # return the maximum probes m.\n        return probes\n\ndef build_cluster_and_measure(m: int, s: int, C: int):\n    \"\"\"\n    Build a single contiguous cluster starting at index s of length C without wrap-around,\n    and measure:\n      - average successful search probes over all inserted keys,\n      - unsuccessful search probes starting from s (key hashing to s but missing),\n      - average unsuccessful search probes when initial probe is uniformly distributed over cluster offsets.\n    \"\"\"\n    # Preconditions: ensure the cluster fits without wrap-around\n    assert s + C <= m - 1, \"Cluster must not wrap around.\"\n\n    table = LinearProbingTable(m)\n\n    # Insert C keys that all hash to s to form the cluster in slots s..s+C-1\n    inserted_keys = []\n    for i in range(C):\n        k = s + i * m  # all hash to s\n        ok = table.insert(k)\n        if not ok:\n            raise RuntimeError(\"Insertion failed: table unexpectedly full.\")\n        inserted_keys.append(k)\n\n    # Average successful search over inserted keys\n    if C == 0:\n        avg_succ = 0.0  # by convention\n    else:\n        succ_probes = [table.probe_count_search(k) for k in inserted_keys]\n        avg_succ = float(sum(succ_probes)) / float(len(succ_probes))\n\n    # Unsuccessful search from start (key hashing to s but not present)\n    missing_start_key = s + C * m  # hashes to s, not inserted\n    unsucc_start = float(table.probe_count_search(missing_start_key))\n\n    # Average unsuccessful search with uniform starting offsets over the cluster\n    if C == 0:\n        avg_unsucc_uniform = 1.0  # degenerate case: same as start case\n    else:\n        # For each offset delta in 0..C-1, choose a missing key hashing to s+delta\n        # Ensure keys do not equal any inserted key: pick a large M to avoid collisions with existing keys\n        M = C + 12345  # any large integer suffices\n        unsucc_uniform_probes = []\n        for delta in range(C):\n            u = (s + delta) + M * m\n            unsucc_uniform_probes.append(table.probe_count_search(u))\n        avg_unsucc_uniform = float(sum(unsucc_uniform_probes)) / float(len(unsucc_uniform_probes))\n\n    return [C, avg_succ, unsucc_start, avg_unsucc_uniform]\n\ndef format_list(obj):\n    \"\"\"Format nested lists without spaces, as required.\"\"\"\n    if isinstance(obj, list):\n        return \"[\" + \",\".join(format_list(x) for x in obj) + \"]\"\n    elif isinstance(obj, (int, float)):\n        # Ensure consistent float formatting (no trailing .0 removal for ints)\n        return str(obj)\n    elif isinstance(obj, bool):\n        return \"True\" if obj else \"False\"\n    else:\n        raise TypeError(\"Unsupported type for formatting\")\n\ndef solve():\n    # Define the test cases from the problem statement.\n    m = 4096\n    s = 123\n    test_cases = [\n        # cluster lengths C covering empty, small, moderate, and near-maximum cluster sizes without wrap-around\n        0, 1, 8, 64, 512, 2048, 3968\n    ]\n\n    results = []\n    for C in test_cases:\n        results.append(build_cluster_and_measure(m, s, C))\n\n    # Final print statement in the exact required format: a single line with nested lists, comma-separated, no spaces.\n    print(format_list(results))\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3257276"}]}