{"hands_on_practices": [{"introduction": "To appreciate the power of Huffman coding, let's begin with its end result: a compressed message. This exercise places you in the role of a receiver tasked with decoding a binary stream using a given Huffman codebook. This practice demonstrates the critical role of the prefix-free property in ensuring a message can be read without any ambiguity [@problem_id:1630289].", "problem": "A deep-space probe sends observational data back to Earth using a stream of four distinct markers, which we will denote as Alpha ($\\alpha$), Beta ($\\beta$), Gamma ($\\gamma$), and Delta ($\\delta$). To conserve transmission bandwidth, the data stream is compressed using a Huffman code. The codebook mapping each marker to its binary representation is as follows:\n*   $\\alpha$: `0`\n*   $\\beta$: `10`\n*   $\\gamma$: `110`\n*   $\\delta$: `111`\n\nA segment of a received transmission contains the binary sequence `11010011100`. Your task is to decode this sequence. Which of the following options represents the original sequence of markers?\n\nA. $\\gamma\\beta\\delta\\alpha\\alpha$\n\nB. $\\gamma\\beta\\alpha\\gamma\\alpha\\alpha$\n\nC. $\\delta\\alpha\\beta\\beta\\alpha\\gamma$\n\nD. $\\gamma\\beta\\alpha\\delta\\alpha\\alpha$\n\nE. $\\gamma\\alpha\\beta\\delta\\alpha\\alpha$", "solution": "The problem requires us to decode a binary sequence using a given Huffman codebook. The fundamental property of a Huffman code is that it is a prefix code. This means no codeword is a prefix of any other codeword, which guarantees that any encoded sequence can be decoded unambiguously into a unique sequence of symbols.\n\nThe decoding algorithm proceeds by reading the binary stream from left to right. We accumulate bits one by one until the current string of bits matches a codeword in the dictionary. Once a match is found, we record the corresponding symbol and then continue the process from the very next bit in the stream.\n\nThe provided codebook is:\n*   $\\alpha$: `0`\n*   $\\beta$: `10`\n*   $\\gamma$: `110`\n*   $\\delta$: `111`\n\nThe binary sequence to be decoded is `11010011100`.\n\nLet's apply the decoding procedure step-by-step:\n\n1.  We start at the beginning of the sequence: `11010011100`.\n    -   We read the first bit, `1`. This is not a complete codeword in our codebook.\n    -   We read the second bit, forming the string `11`. This is also not in the codebook.\n    -   We read the third bit, forming `110`. This string matches the code for the symbol $\\gamma$. We record $\\gamma$ as the first symbol of our decoded sequence.\n    -   The remaining bit sequence is `10011100`.\n\n2.  We continue decoding from the start of the remaining sequence: `10011100`.\n    -   We read the first bit, `1`. This is not a codeword.\n    -   We read the second bit, forming `10`. This string matches the code for $\\beta$. We record $\\beta$ as the second symbol.\n    -   The remaining bit sequence is `011100`.\n\n3.  We proceed with the new sequence: `011100`.\n    -   We read the first bit, `0`. This string matches the code for $\\alpha$. We record $\\alpha$ as the third symbol.\n    -   The remaining bit sequence is `11100`.\n\n4.  We continue with the sequence `11100`.\n    -   We read the first bit, `1`. Not a codeword.\n    -   We read the second bit, forming `11`. Not a codeword.\n    -   We read the third bit, forming `111`. This string matches the code for $\\delta$. We record $\\delta$ as the fourth symbol.\n    -   The remaining bit sequence is `00`.\n\n5.  We proceed with the sequence `00`.\n    -   We read the first bit, `0`. This matches the code for $\\alpha$. We record $\\alpha$ as the fifth symbol.\n    -   The remaining bit sequence is `0`.\n\n6.  Finally, we decode the last part of the sequence: `0`.\n    -   We read the bit, `0`. This matches the code for $\\alpha$. We record $\\alpha$ as the sixth and final symbol.\n    -   The remaining sequence is now empty, so the decoding is complete.\n\nBy concatenating the symbols we identified in order, we find the original sequence to be $\\gamma\\beta\\alpha\\delta\\alpha\\alpha$.\n\nWe now compare this result with the given multiple-choice options:\nA. $\\gamma\\beta\\delta\\alpha\\alpha$\nB. $\\gamma\\beta\\alpha\\gamma\\alpha\\alpha$\nC. $\\delta\\alpha\\beta\\beta\\alpha\\gamma$\nD. $\\gamma\\beta\\alpha\\delta\\alpha\\alpha$\nE. $\\gamma\\alpha\\beta\\delta\\alpha\\alpha$\n\nOur decoded sequence, $\\gamma\\beta\\alpha\\delta\\alpha\\alpha$, matches option D.", "answer": "$$\\boxed{D}$$", "id": "1630289"}, {"introduction": "After seeing how to decode a message, the next logical step is to learn how the codebook itself is created. This practice will guide you through the classic Huffman algorithm, a greedy approach that builds an optimal code tree by iteratively merging the least frequent symbols. You will also measure the code's efficiency by calculating the average codeword length, $L$, and comparing it to the theoretical minimum given by the source entropy, $H$ [@problem_id:1630316].", "problem": "A deep-space probe is monitoring the atmosphere of a distant exoplanet. It is equipped to detect six specific types of molecules, which it labels with the symbols $\\{S_1, S_2, S_3, S_4, S_5, S_6\\}$. Due to the planet's atmospheric chemistry, these molecules are detected with different frequencies. Over a long observation period, the probe has established the following stable probabilities of detecting each molecule type in any given measurement:\n$P(S_1) = 0.1$\n$P(S_2) = 0.1$\n$P(S_3) = 0.1$\n$P(S_4) = 0.1$\n$P(S_5) = 0.3$\n$P(S_6) = 0.3$\n\nTo conserve bandwidth for transmissions back to Earth, the data stream of detected symbols is to be encoded using an optimal binary prefix code. You are tasked with analyzing the efficiency of this encoding scheme.\n\nCalculate two fundamental quantities for this source:\n1. The average codeword length, $L$, of a Huffman code constructed for these symbols.\n2. The entropy, $H$, of the source.\n\nWhen constructing the Huffman code, if a tie in probabilities occurs when selecting nodes to combine, the choice between the tied nodes may be made arbitrarily.\n\nExpress your answers for $L$ and $H$, in that order, in units of bits per symbol. Round your final numerical answers to four significant figures.", "solution": "We are given six symbols with probabilities $p_{1}=0.1$, $p_{2}=0.1$, $p_{3}=0.1$, $p_{4}=0.1$, $p_{5}=0.3$, and $p_{6}=0.3$. To find the average codeword length $L$ of a Huffman code, we construct the code by iteratively combining the two least probable nodes:\n- Combine $0.1$ and $0.1$ to form $0.2$ (twice), giving multiset $\\{0.2,0.2,0.3,0.3\\}$.\n- Combine $0.2$ and $0.2$ to form $0.4$, giving $\\{0.3,0.3,0.4\\}$.\n- Combine $0.3$ and $0.3$ to form $0.6$, giving $\\{0.4,0.6\\}$.\n- Combine $0.4$ and $0.6$ to form $1.0$.\nTracing back, each symbol of probability $0.1$ attains codeword length $3$, and each symbol of probability $0.3$ attains codeword length $2$. Therefore, the average length is\n$$\nL=\\sum_{i=1}^{6}p_{i}l_{i}=4\\cdot 0.1\\cdot 3+2\\cdot 0.3\\cdot 2=1.2+1.2=2.4\\ \\text{bits per symbol}.\n$$\n\nThe entropy in bits per symbol is\n$$\nH=-\\sum_{i=1}^{6}p_{i}\\log_{2}p_{i}=-4\\cdot 0.1\\log_{2}(0.1)-2\\cdot 0.3\\log_{2}(0.3).\n$$\nUsing $\\log_{2}(0.1)\\approx -3.3219280949$ and $\\log_{2}(0.3)\\approx -1.7369655942$, we obtain\n$$\nH\\approx -0.4(-3.3219280949)-0.6(-1.7369655942)=1.3287712379+1.0421793565\\approx 2.3709505945.\n$$\nRounding to four significant figures gives $L=2.400$ and $H=2.371$, both in bits per symbol.", "answer": "$$\\boxed{\\begin{pmatrix}2.400 & 2.371\\end{pmatrix}}$$", "id": "1630316"}, {"introduction": "While the average length of a Huffman code is uniquely optimal for a given set of probabilities, the structure of the code itself is not always unique. This final practice explores the nuances that arise from ties during the algorithm's execution. You are challenged to construct different, yet equally optimal, Huffman trees for the same source and analyze how these structural variations impact the statistical distribution of the codeword lengths [@problem_id:1630317].", "problem": "A discrete memoryless source emits symbols from the alphabet $\\mathcal{S} = \\{S_1, S_2, S_3, S_4, S_5, S_6\\}$. The probabilities of these symbols are given by the set $P = \\{0.3, 0.2, 0.2, 0.1, 0.1, 0.1\\}$, where $p(S_1)=0.3$, $p(S_2)=0.2$, $p(S_3)=0.2$, and $p(S_4)=p(S_5)=p(S_6)=0.1$.\n\nThe Huffman coding algorithm is used to generate an optimal prefix-free binary code for this source. The execution of the algorithm involves iteratively merging the two nodes (symbols or groups of symbols) with the lowest probabilities. When ties in probabilities occur, different choices can be made, potentially leading to different valid Huffman trees. While all resulting codes are optimal and share the same minimum average codeword length, the distribution of individual codeword lengths can differ.\n\nYour task is to determine the maximum possible variance of the codeword length, $L$, for an optimal code generated for this source. The variance is defined as $\\text{Var}(L) = \\sum_{i=1}^{6} p(S_i) (l_i - \\bar{L})^2 = E[L^2] - (E[L])^2$, where $l_i$ is the length of the codeword for symbol $S_i$, and $\\bar{L} = E[L]$ is the average codeword length.\n\nExpress your answer as a single decimal number, rounded to three significant figures.", "solution": "Let the source probabilities be $p_{1}=0.3$, $p_{2}=0.2$, $p_{3}=0.2$, $p_{4}=0.1$, $p_{5}=0.1$, $p_{6}=0.1$. The Huffman merge sequence by weights is fixed:\n- Merge two $0.1$ to $0.2$.\n- Merge $0.1$ with $0.2$ to $0.3$.\n- Merge $0.2$ with $0.2$ to $0.4$.\n- Merge $0.3$ with $0.3$ to $0.6$.\n- Merge $0.6$ with $0.4$ to $1$.\n\nTies allow two structurally different optimal assignments of codeword lengths.\n\nCase I (do not merge the initial $0.1+0.1$ pair with the remaining $0.1$ at the next step): At step 2, merge the remaining $0.1$ with a singleton $0.2$. The resulting lengths are\n- $l_{1}=2$,\n- among $p=0.2$, one gets $l=2$ and one gets $l=3$,\n- all three $p=0.1$ get $l=3$.\nThus the length distribution is $L=2$ with total probability $0.5$ and $L=3$ with total probability $0.5$. Hence\n$$\nE[L]=2\\cdot 0.5+3\\cdot 0.5=2.5,\\quad\nE[L^{2}]=4\\cdot 0.5+9\\cdot 0.5=6.5,\n$$\nso\n$$\n\\operatorname{Var}(L)=E[L^{2}]-(E[L])^{2}=6.5-(2.5)^{2}=0.25.\n$$\n\nCase II (merge the initial $0.1+0.1$ pair with the remaining $0.1$ at the next step): At step 2, merge the remaining $0.1$ with the $0.2$ formed by two $0.1$’s. The resulting lengths are\n- $l_{1}=2$,\n- both $p=0.2$ symbols get $l=2$,\n- one $p=0.1$ symbol gets $l=3$,\n- the other two $p=0.1$ symbols get $l=4$.\nThus the length distribution is $L=2$ with probability $0.7$, $L=3$ with probability $0.1$, and $L=4$ with probability $0.2$. Hence\n$$\nE[L]=2\\cdot 0.7+3\\cdot 0.1+4\\cdot 0.2=2.5,\n$$\n$$\nE[L^{2}]=4\\cdot 0.7+9\\cdot 0.1+16\\cdot 0.2=6.9,\n$$\nso\n$$\n\\operatorname{Var}(L)=6.9-(2.5)^{2}=6.9-6.25=0.65.\n$$\n\nBoth cases are optimal with the same minimal average length $E[L]=2.5$. The variance is maximized in Case II. Therefore, the maximum possible variance is $0.65$, which to three significant figures is $0.650$.", "answer": "$$\\boxed{0.650}$$", "id": "1630317"}]}