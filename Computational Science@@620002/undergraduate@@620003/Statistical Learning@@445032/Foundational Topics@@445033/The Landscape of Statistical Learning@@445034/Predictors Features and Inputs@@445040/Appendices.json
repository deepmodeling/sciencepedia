{"hands_on_practices": [{"introduction": "The performance of many statistical models, particularly linear regression, can be sensitive to the distribution of the input features. This practice explores the Box-Cox transformation, a powerful and systematic method for reducing skewness in positive predictors, often making them more closely resemble a normal distribution. By programmatically applying these transformations, you will investigate the crucial link between a feature's statistical properties (like skewness) and the predictive quality of a model, revealing that the two are related but not always perfectly aligned [@problem_id:3160298].", "problem": "You are given the task of programmatically investigating how nonlinear monotone transformations of a single positive predictor affect both its skewness and the quality of a linear model fit. You will use the Box–Cox family of transformations applied to the predictor (feature, input), and you will compare outcomes across a grid of transformation parameters.\n\nFundamental base for the task:\n- Define the Box–Cox transformation for positive inputs as follows. For any $x \\in \\mathbb{R}_{+}$ and parameter $\\lambda \\in \\mathbb{R}$, define\n$$\ng_{\\lambda}(x) = \n\\begin{cases}\n\\dfrac{x^{\\lambda}-1}{\\lambda} & \\text{if } \\lambda \\ne 0, \\\\\n\\log(x) & \\text{if } \\lambda = 0.\n\\end{cases}\n$$\n- The sample skewness $\\gamma_{1}(z)$ of a real-valued sample $z_{1},\\dots,z_{n}$ is defined by\n$$\n\\gamma_{1}(z) = \\frac{\\frac{1}{n}\\sum_{i=1}^{n}\\left(z_{i}-\\bar{z}\\right)^{3}}{\\left(\\frac{1}{n}\\sum_{i=1}^{n}\\left(z_{i}-\\bar{z}\\right)^{2}\\right)^{3/2}},\n\\quad \\bar{z}=\\frac{1}{n}\\sum_{i=1}^{n} z_{i}.\n$$\n- For a linear model with intercept,\n$$\ny_{i} = \\beta_{0} + \\beta_{1} z_{i} + \\varepsilon_{i}, \\quad \\varepsilon_{i} \\stackrel{\\text{iid}}{\\sim} \\mathcal{N}(0,\\sigma^{2}),\n$$\nthe ordinary least squares (OLS) fit yields residuals $r_{i}$ and residual sum of squares $\\text{RSS}=\\sum_{i=1}^{n} r_{i}^{2}$. Under the Gaussian error model with $\\hat{\\sigma}^{2}=\\text{RSS}/n$, the maximized log-likelihood is\n$$\n\\ell = -\\frac{n}{2}\\left(\\log\\left(2\\pi \\hat{\\sigma}^{2}\\right)+1\\right).\n$$\n\nYour program must do the following for each specified test case:\n1. Data generation. Generate a positive predictor $X_{i}$ and a response $Y_{i}$ by\n$$\nX_{i} \\sim \\text{LogNormal}(\\mu,\\sigma_{X}^{2}), \\quad \nY_{i} = \\beta_{0} + \\beta_{1}\\, g_{\\lambda^{\\star}}(X_{i}) + \\varepsilon_{i}, \\quad \\varepsilon_{i} \\sim \\mathcal{N}(0,\\sigma_{\\varepsilon}^{2}),\n$$\nfor $i=1,\\dots,n$, using the specified random seed for reproducibility. All random draws must be independent and identically distributed.\n\n2. Transformation grid. Consider the candidate set\n$$\n\\Lambda = \\left\\{-2.0,-1.0,-0.5,-0.25,0.0,0.25,0.5,1.0,2.0\\right\\}.\n$$\n\n3. Skewness equalization. For each $\\lambda \\in \\Lambda$, compute $Z_{i}^{(\\lambda)}=g_{\\lambda}(X_{i})$ and compute the absolute skewness $|\\gamma_{1}(Z^{(\\lambda)})|$. Let $\\hat{\\lambda}_{\\text{skew}}$ be any minimizer over $\\Lambda$. In the event of ties, choose the first minimizer encountered when $\\Lambda$ is scanned in ascending order.\n\n4. Model-fit optimization. For each $\\lambda \\in \\Lambda$, fit the OLS model $Y_{i}=\\beta_{0}^{(\\lambda)}+\\beta_{1}^{(\\lambda)} Z_{i}^{(\\lambda)}+\\varepsilon_{i}^{(\\lambda)}$ with intercept, compute the residual sum of squares $\\text{RSS}(\\lambda)$ and the corresponding maximized log-likelihood\n$$\n\\ell(\\lambda) = -\\frac{n}{2}\\left(\\log\\left(2\\pi \\cdot \\frac{\\text{RSS}(\\lambda)}{n}\\right) + 1\\right).\n$$\nLet $\\hat{\\lambda}_{\\text{fit}}$ be any maximizer of $\\ell(\\lambda)$ over $\\Lambda$. In the event of ties, choose the first maximizer encountered when $\\Lambda$ is scanned in ascending order.\n\n5. Improvement over identity. Define the identity transformation by $\\lambda=1.0$. Let $\\Delta_{\\text{skew}} = \\ell(\\hat{\\lambda}_{\\text{skew}}) - \\ell(1.0)$. Define a boolean improvement indicator $I$ that is true if and only if $\\Delta_{\\text{skew}} > 0$.\n\n6. Output per test case. For each test case, output the list $[\\hat{\\lambda}_{\\text{skew}}, \\hat{\\lambda}_{\\text{fit}}, I, \\Delta_{\\text{skew}}]$, where the two $\\lambda$ values and $\\Delta_{\\text{skew}}$ must be rounded to four decimal places. The boolean $I$ must be either true or false.\n\nTest suite:\nUse the following four test cases. For each case, use the specified parameters $(\\text{seed},n,\\mu,\\sigma_{X},\\beta_{0},\\beta_{1},\\lambda^{\\star},\\sigma_{\\varepsilon})$:\n- Case $1$: $(0,\\,400,\\,0.0,\\,1.0,\\,1.0,\\,2.0,\\,0.0,\\,1.0)$.\n- Case $2$: $(1,\\,400,\\,0.0,\\,0.25,\\,-0.5,\\,1.5,\\,1.0,\\,1.0)$.\n- Case $3$: $(2,\\,400,\\,0.0,\\,1.25,\\,0.0,\\,1.0,\\,0.5,\\,1.5)$.\n- Case $4$: $(3,\\,400,\\,0.5,\\,1.0,\\,0.5,\\,1.0,\\,-0.5,\\,1.0)$.\n\nAngle units are not involved. There are no physical units in this problem.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list of the four per-case results, each itself a list in the form $[\\hat{\\lambda}_{\\text{skew}},\\hat{\\lambda}_{\\text{fit}},I,\\Delta_{\\text{skew}}]$, and the outer list enclosed in square brackets. For example, an output with two cases would look like\n$[[\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot]]$.\nAll three real-valued entries within each inner list must be rounded to four decimal places in the textual output.", "solution": "The user wants to investigate the effects of Box-Cox transformations on a predictor variable. I will first validate the problem statement.\n\n### Step 1: Extract Givens\n\n- **Box–Cox Transformation**: For a positive input $x \\in \\mathbb{R}_{+}$ and a parameter $\\lambda \\in \\mathbb{R}$, the transformation is defined as:\n$$\ng_{\\lambda}(x) = \n\\begin{cases}\n\\dfrac{x^{\\lambda}-1}{\\lambda} & \\text{if } \\lambda \\ne 0, \\\\\n\\log(x) & \\text{if } \\lambda = 0.\n\\end{cases}\n$$\n\n- **Sample Skewness**: For a sample $z = \\{z_{1}, \\dots, z_{n}\\}$, the sample skewness is:\n$$\n\\gamma_{1}(z) = \\frac{\\frac{1}{n}\\sum_{i=1}^{n}\\left(z_{i}-\\bar{z}\\right)^{3}}{\\left(\\frac{1}{n}\\sum_{i=1}^{n}\\left(z_{i}-\\bar{z}\\right)^{2}\\right)^{3/2}}, \\quad \\text{where } \\bar{z}=\\frac{1}{n}\\sum_{i=1}^{n} z_{i}.\n$$\n\n- **Linear Model and Log-Likelihood**: For a simple linear regression model $y_{i} = \\beta_{0} + \\beta_{1} z_{i} + \\varepsilon_{i}$ with i.i.d. Gaussian errors $\\varepsilon_{i} \\sim \\mathcal{N}(0,\\sigma^{2})$, the maximized log-likelihood under OLS is:\n$$\n\\ell = -\\frac{n}{2}\\left(\\log\\left(2\\pi \\hat{\\sigma}^{2}\\right)+1\\right),\n$$\nwhere $\\hat{\\sigma}^{2}=\\text{RSS}/n$ is the maximum likelihood estimate of the error variance, with $\\text{RSS}$ being the residual sum of squares.\n\n- **Data Generation Model**:\n$$\nX_{i} \\sim \\text{LogNormal}(\\mu,\\sigma_{X}^{2}), \\quad \nY_{i} = \\beta_{0} + \\beta_{1}\\, g_{\\lambda^{\\star}}(X_{i}) + \\varepsilon_{i}, \\quad \\varepsilon_{i} \\sim \\mathcal{N}(0,\\sigma_{\\varepsilon}^{2})\n$$\nfor $i=1,\\dots,n$, with a specified random seed.\n\n- **Candidate Transformation Parameters**:\n$$\n\\Lambda = \\left\\{-2.0,-1.0,-0.5,-0.25,0.0,0.25,0.5,1.0,2.0\\right\\}\n$$\n\n- **Procedure**:\n    1.  **Skewness Equalization**: Find $\\hat{\\lambda}_{\\text{skew}} \\in \\Lambda$ that minimizes the absolute sample skewness $|\\gamma_{1}(g_{\\lambda}(X))|$.\n    2.  **Model-Fit Optimization**: Find $\\hat{\\lambda}_{\\text{fit}} \\in \\Lambda$ that maximizes the log-likelihood $\\ell(\\lambda)$ of the model $Y \\sim g_{\\lambda}(X)$.\n    3.  **Tie-breaking**: For both optimizations, the first value encountered in an ascending scan of $\\Lambda$ is chosen.\n    4.  **Improvement over Identity**: Calculate $\\Delta_{\\text{skew}} = \\ell(\\hat{\\lambda}_{\\text{skew}}) - \\ell(1.0)$ and a boolean indicator $I = (\\Delta_{\\text{skew}} > 0)$.\n\n- **Output**: For each test case, a list $[\\hat{\\lambda}_{\\text{skew}}, \\hat{\\lambda}_{\\text{fit}}, I, \\Delta_{\\text{skew}}]$ with floating-point numbers rounded to four decimal places.\n\n- **Test Cases**:\n    - Case 1: $(\\text{seed},n,\\mu,\\sigma_{X},\\beta_{0},\\beta_{1},\\lambda^{\\star},\\sigma_{\\varepsilon}) = (0, 400, 0.0, 1.0, 1.0, 2.0, 0.0, 1.0)$.\n    - Case 2: $(\\text{seed},n,\\mu,\\sigma_{X},\\beta_{0},\\beta_{1},\\lambda^{\\star},\\sigma_{\\varepsilon}) = (1, 400, 0.0, 0.25, -0.5, 1.5, 1.0, 1.0)$.\n    - Case 3: $(\\text{seed},n,\\mu,\\sigma_{X},\\beta_{0},\\beta_{1},\\lambda^{\\star},\\sigma_{\\varepsilon}) = (2, 400, 0.0, 1.25, 0.0, 1.0, 0.5, 1.5)$.\n    - Case 4: $(\\text{seed},n,\\mu,\\sigma_{X},\\beta_{0},\\beta_{1},\\lambda^{\\star},\\sigma_{\\varepsilon}) = (3, 400, 0.5, 1.0, 0.5, 1.0, -0.5, 1.0)$.\n\n### Step 2: Validate Using Extracted Givens\n\nThe problem is coherent and well-defined.\n- **Scientifically Grounded**: The problem is built upon fundamental and standard concepts in statistical modeling and data analysis, including the Box-Cox transformation, linear regression, skewness, and likelihood-based model evaluation. The data simulation protocol is a common approach for studying statistical methods.\n- **Well-Posed**: The sequence of operations is clearly specified. The use of a fixed random seed ensures reproducibility. The search for optimal $\\lambda$ values is conducted over a finite, discrete set $\\Lambda$, and the explicit tie-breaking rule guarantees a unique solution for each test case.\n- **Objective**: All definitions and tasks are stated with mathematical precision, leaving no room for subjective interpretation.\n- **Flaw Analysis**:\n    1.  **Scientific/Factual Unsoundness**: None. All formulas and concepts are standard in statistics.\n    2.  **Non-Formalizable/Irrelevant**: The problem is entirely formalizable and highly relevant to its stated topic in statistical learning.\n    3.  **Incomplete/Contradictory**: The problem specification is complete, providing all necessary parameters, data generation procedures, formulas, and criteria for evaluation. There are no contradictions.\n    4.  **Unrealistic/Infeasible**: The specified parameters are reasonable for a simulation study. The generation of $X$ from a LogNormal distribution ensures $X>0$, making the Box-Cox transformation well-defined.\n    5.  **Ill-Posed/Poorly Structured**: None. The problem structure is logical and leads to a unique, meaningful solution.\n    6.  **Pseudo-Profound/Trivial**: The problem requires the integration of several non-trivial statistical computations into a simulation framework to explore a conceptually important trade-off in feature engineering. It is a substantive task.\n    7.  **Outside Scientific Verifiability**: The results are computationally deterministic and verifiable given the random seeds.\n\n### Step 3: Verdict and Action\nThe problem is valid. I will proceed to construct the solution.\n\nThe solution will be implemented as a single Python script. For each test case defined in the problem, the script will execute the following sequence of steps:\n1.  Initialize a random number generator with the specified seed for reproducibility.\n2.  Generate the predictor data $X$ from a LogNormal distribution, $\\text{LogNormal}(\\mu, \\sigma_X^2)$, and the noise term $\\varepsilon$ from a Normal distribution, $\\mathcal{N}(0, \\sigma_\\varepsilon^2)$. The parameters $\\mu$, $\\sigma_X$, and $\\sigma_\\varepsilon$ are provided in each test case.\n3.  Construct the response variable $Y$ using the true underlying relationship: $Y_{i} = \\beta_{0} + \\beta_{1}\\, g_{\\lambda^{\\star}}(X_{i}) + \\varepsilon_{i}$.\n4.  Iterate through the candidate set of transformation parameters $\\Lambda$. For each $\\lambda \\in \\Lambda$:\n    a. Apply the Box-Cox transformation to the predictor: $Z^{(\\lambda)} = g_{\\lambda}(X)$.\n    b. Compute the sample skewness of the transformed predictor, $\\gamma_{1}(Z^{(\\lambda)})$, using the provided formula. The absolute value of this skewness is stored.\n    c. Fit a simple linear regression model of $Y$ on $Z^{(\\lambda)}$ using Ordinary Least Squares (OLS) to obtain coefficient estimates.\n    d. From the OLS fit, calculate the Residual Sum of Squares, $\\text{RSS}(\\lambda)$.\n    e. Use $\\text{RSS}(\\lambda)$ to compute the maximized log-likelihood, $\\ell(\\lambda)$, according to the given formula.\n5.  After iterating through all $\\lambda \\in \\Lambda$, determine $\\hat{\\lambda}_{\\text{skew}}$ by finding the $\\lambda$ that minimizes the stored absolute skewness values. The specified tie-breaking rule (first in ascending order) is handled by using `numpy.argmin`.\n6.  Similarly, determine $\\hat{\\lambda}_{\\text{fit}}$ by finding the $\\lambda$ that maximizes the stored log-likelihood values. The tie-breaking rule is handled by using `numpy.argmax`.\n7.  Calculate the improvement metric $\\Delta_{\\text{skew}} = \\ell(\\hat{\\lambda}_{\\text{skew}}) - \\ell(1.0)$. The value $\\ell(1.0)$ corresponds to the log-likelihood for the model using the identity-like transformation.\n8.  Define the boolean indicator $I$ as true if $\\Delta_{\\text{skew}} > 0$, and false otherwise.\n9.  Assemble the final result for the test case, which is the list $[\\hat{\\lambda}_{\\text{skew}}, \\hat{\\lambda}_{\\text{fit}}, I, \\Delta_{\\text{skew}}]$, ensuring numerical values are rounded to four decimal places.\n10. After processing all test cases, the results are formatted into a single string as a list of lists and printed to standard output.\nHelper functions will be defined for the Box-Cox transformation and other repeated calculations to ensure code clarity and correctness. The `scipy.stats.skew` function will be used for computing skewness as it correctly implements the biased estimator specified in the problem statement.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import skew\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation for all test cases and print the final result.\n    \"\"\"\n    # Define the test cases as per the problem statement.\n    # Format: (seed, n, mu, sigma_X, beta_0, beta_1, lambda_star, sigma_epsilon)\n    test_cases = [\n        (0, 400, 0.0, 1.0, 1.0, 2.0, 0.0, 1.0),\n        (1, 400, 0.0, 0.25, -0.5, 1.5, 1.0, 1.0),\n        (2, 400, 0.0, 1.25, 0.0, 1.0, 0.5, 1.5),\n        (3, 400, 0.5, 1.0, 0.5, 1.0, -0.5, 1.0),\n    ]\n\n    # Candidate set for the transformation parameter lambda.\n    Lambda = np.array([-2.0, -1.0, -0.5, -0.25, 0.0, 0.25, 0.5, 1.0, 2.0])\n\n    all_results = []\n\n    def box_cox_transform(x, lambda_val):\n        \"\"\"\n        Applies the Box-Cox transformation to the input data x.\n        \"\"\"\n        if lambda_val == 0.0:\n            return np.log(x)\n        else:\n            return (np.power(x, lambda_val) - 1.0) / lambda_val\n\n    for case in test_cases:\n        seed, n, mu, sigma_x, beta_0, beta_1, lambda_star, sigma_eps = case\n        \n        # Initialize random number generator for reproducibility.\n        rng = np.random.default_rng(seed)\n        \n        # Step 1: Data generation.\n        # X is from a LogNormal distribution, ensuring it's positive.\n        X = rng.lognormal(mean=mu, sigma=sigma_x, size=n)\n        # Epsilon is from a Normal distribution.\n        eps = rng.normal(loc=0.0, scale=sigma_eps, size=n)\n        \n        # The true transformed predictor and the response variable Y.\n        X_transformed_true = box_cox_transform(X, lambda_star)\n        Y = beta_0 + beta_1 * X_transformed_true + eps\n        \n        abs_skewness_list = []\n        log_likelihood_list = []\n        \n        # Step 2-4: Iterate through the transformation grid.\n        for lambda_val in Lambda:\n            # Transform the predictor X using the current lambda.\n            Z = box_cox_transform(X, lambda_val)\n            \n            # Compute absolute skewness. scipy.stats.skew with bias=True (default)\n            # matches the formula provided in the problem.\n            current_abs_skew = np.abs(skew(Z))\n            abs_skewness_list.append(current_abs_skew)\n            \n            # Fit an OLS model: Y = b0 + b1*Z.\n            # np.polyfit returns coefficients [b1, b0].\n            b1_fit, b0_fit = np.polyfit(Z, Y, 1)\n            \n            # Calculate predicted Y values and residuals.\n            Y_pred = b0_fit + b1_fit * Z\n            \n            # Compute residual sum of squares (RSS).\n            rss = np.sum((Y - Y_pred)**2)\n            \n            # The MLE for variance is RSS/n.\n            sigma2_hat = rss / float(n)\n            \n            # Compute the maximized log-likelihood.\n            # A check for non-positive variance to avoid math errors, though unlikely here.\n            if sigma2_hat > 0:\n                log_lik = -n / 2.0 * (np.log(2.0 * np.pi * sigma2_hat) + 1.0)\n            else:\n                log_lik = -np.inf\n            log_likelihood_list.append(log_lik)\n\n        # Find the lambda that minimizes absolute skewness.\n        # np.argmin respects the tie-breaking rule (first occurrence).\n        idx_skew = np.argmin(abs_skewness_list)\n        hat_lambda_skew = Lambda[idx_skew]\n        \n        # Find the lambda that maximizes log-likelihood.\n        # np.argmax also respects the tie-breaking rule.\n        idx_fit = np.argmax(log_likelihood_list)\n        hat_lambda_fit = Lambda[idx_fit]\n        \n        # Step 5: Improvement over identity.\n        # Find the log-likelihood for the identity transformation (lambda = 1.0).\n        idx_1 = np.where(Lambda == 1.0)[0][0]\n        log_lik_at_1 = log_likelihood_list[idx_1]\n        \n        # Get the log-likelihood at the skewness-minimizing lambda.\n        log_lik_at_skew = log_likelihood_list[idx_skew]\n        \n        Delta_skew = log_lik_at_skew - log_lik_at_1\n        I = Delta_skew > 0.0\n        \n        # Step 6: Assemble the output for the current case.\n        case_result = [\n            hat_lambda_skew, \n            hat_lambda_fit, \n            bool(I),\n            Delta_skew\n        ]\n        all_results.append(case_result)\n\n    # Format the final output string as a list of lists.\n    result_strings = []\n    for res in all_results:\n        # Format: [float, float, bool, float] with specified rounding and boolean case.\n        s = f\"[{res[0]:.4f},{res[1]:.4f},{str(res[2]).lower()},{res[3]:.4f}]\"\n        result_strings.append(s)\n    \n    # Final print statement must be on a single line in the exact specified format.\n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n```", "id": "3160298"}, {"introduction": "When building models with multiple predictors, it's essential to understand how they relate to one another. This exercise focuses on multicollinearity, an issue that arises when features carry redundant information, and examines the extreme case of perfect collinearity where Ordinary Least Squares (OLS) fails to find a unique solution. Through a carefully constructed hands-on example, you will see why the OLS parameters become non-identifiable and how Ridge Regression, by adding an $\\ell_2$ penalty, elegantly resolves this instability to provide a single, stable set of coefficients [@problem_id:3160401].", "problem": "Consider a data construction that highlights the role of predictors, features, and inputs in linear models with and without regularization. Let there be $n$ samples with a single base feature $x_1 \\in \\mathbb{R}^n$ defined by $x_1 = [-2,-1,0,1,2]^\\top$, and define a perfectly collinear second feature $x_2 = 3 x_1$. Let the target be noiseless and given by $y = 6 x_1$, and take the intercept to be $0$. Define the design matrix $X \\in \\mathbb{R}^{n \\times p}$ by stacking columns of features as needed.\n\nThe foundational base for this problem is:\n- The definition of Ordinary Least Squares (OLS): minimize the sum of squared residuals, equivalently solve the normal equations $X^\\top X \\beta = X^\\top y$ when a unique solution exists.\n- The definition of Ridge Regression: minimize the sum of squared residuals with an $\\ell_2$ penalty, equivalently solve $(X^\\top X + \\lambda I)\\beta = X^\\top y$ for $\\lambda \\ge 0$, where $I$ is the identity matrix.\n- The Moore-Penrose Pseudoinverse (MPP): when $X^\\top X$ is singular, the minimum-norm least squares solution is $\\beta = X^+ y$, where $X^+$ denotes the pseudoinverse.\n- Identifiability principle: parameters are identifiable if a unique solution exists; otherwise, there can be infinitely many parameter vectors $\\beta$ yielding identical predictions $X\\beta$.\n\nTasks:\n1. Using only the base feature $x_1$ ($p = 1$), compute the OLS coefficient $\\beta_1$.\n2. Using both features $[x_1, x_2]$ ($p = 2$) with perfect collinearity $x_2 = 3 x_1$, compute the unregularized least squares coefficients $(\\beta_1, \\beta_2)$ using the Moore-Penrose Pseudoinverse.\n3. For the collinear two-feature case, compute the ridge regression coefficients $(\\beta_1, \\beta_2)$ for the following regularization strengths $\\lambda$: $0$, $0.1$, $1$, $10$, and $1,000,000$. When $\\lambda = 0$, use the Moore-Penrose Pseudoinverse.\n4. Discuss identifiability and coefficient paths: derive the form of the ridge solution in this specific rank-$1$ setting and explain how the ratio $\\beta_2 / \\beta_1$ behaves as $\\lambda$ varies.\n\nNumerical details:\n- All computations are purely mathematical and unitless.\n- Each reported coefficient must be rounded to $6$ decimal places.\n\nTest suite:\n- Case $1$: OLS with a single predictor $x_1$ (unregularized), return $[\\beta_1]$.\n- Case $2$: OLS with predictors $[x_1, x_2]$ (unregularized using MPP), return $[\\beta_1, \\beta_2]$.\n- Case $3$: Ridge with predictors $[x_1, x_2]$ and $\\lambda = 0$, return $[\\beta_1, \\beta_2]$ via MPP.\n- Case $4$: Ridge with predictors $[x_1, x_2]$ and $\\lambda = 0.1$, return $[\\beta_1, \\beta_2]$.\n- Case $5$: Ridge with predictors $[x_1, x_2]$ and $\\lambda = 1$, return $[\\beta_1, \\beta_2]$.\n- Case $6$: Ridge with predictors $[x_1, x_2]$ and $\\lambda = 10$, return $[\\beta_1, \\beta_2]$.\n- Case $7$: Ridge with predictors $[x_1, x_2]$ and $\\lambda = 1,000,000$, return $[\\beta_1, \\beta_2]$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is an inner bracketed list of the rounded coefficients for a test case, in the order of the test suite. For example, the output should look like\n$[[c_{1,1},c_{1,2},\\dots],[c_{2,1},c_{2,2}],\\dots]$\nand must be printed exactly as a single line with no extra text.", "solution": "The problem is validated as follows.\n\nGivens:\n- Number of samples: $n = 5$.\n- Base feature vector: $x_1 = [-2, -1, 0, 1, 2]^\\top \\in \\mathbb{R}^n$.\n- Second feature vector: $x_2 = 3 x_1$.\n- Target vector: $y = 6 x_1$.\n- Intercept: $0$.\n- Design matrix: $X \\in \\mathbb{R}^{n \\times p}$, composed of feature columns.\n- Ordinary Least Squares (OLS) definition: Minimize sum of squared residuals, solve $X^\\top X \\beta = X^\\top y$.\n- Ridge Regression definition: Minimize sum of squared residuals plus an $\\ell_2$ penalty, solve $(X^\\top X + \\lambda I)\\beta = X^\\top y$ for $\\lambda \\ge 0$.\n- Moore-Penrose Pseudoinverse (MPP): For singular $X^\\top X$, the minimum-norm least squares solution is $\\beta = X^+ y$.\n- Task 1: Compute OLS coefficient $\\beta_1$ for $p=1$ using feature $x_1$.\n- Task 2: Compute OLS coefficients $(\\beta_1, \\beta_2)$ for $p=2$ using features $[x_1, x_2]$ via MPP.\n- Task 3: Compute Ridge coefficients $(\\beta_1, \\beta_2)$ for $p=2$ with $\\lambda \\in \\{0, 0.1, 1, 10, 1000000\\}$.\n- Task 4: Discuss identifiability and the behavior of the coefficient ratio $\\beta_2 / \\beta_1$ as $\\lambda$ changes.\n- Numerical precision: Round coefficients to $6$ decimal places.\n\nValidation:\n- **Scientifically Grounded:** The problem is a standard exercise in statistical learning and linear algebra, dealing with fundamental concepts like OLS, ridge regression, multicollinearity, and parameter identifiability. All principles are well-established.\n- **Well-Posed:** The problem provides all necessary data and definitions. The instruction to use the Moore-Penrose Pseudoinverse for the singular case ensures that a unique solution is defined for every task. Thus, the problem is well-posed.\n- **Objective:** The problem is stated in precise, mathematical language, free from subjectivity or ambiguity.\n\nVerdict: The problem is valid.\n\nWe proceed with the solution.\n\nFirst, we define the given vectors and compute some preliminary quantities.\nThe base feature vector is $x_1 = [-2, -1, 0, 1, 2]^\\top$.\nThe number of samples is $n=5$.\nThe second feature is $x_2 = 3x_1 = [-6, -3, 0, 3, 6]^\\top$.\nThe target vector is $y = 6x_1 = [-12, -6, 0, 6, 12]^\\top$.\n\nWe calculate the following inner products:\n$x_1^\\top x_1 = (-2)^2 + (-1)^2 + 0^2 + 1^2 + 2^2 = 4 + 1 + 0 + 1 + 4 = 10$.\n$x_1^\\top y = x_1^\\top (6x_1) = 6(x_1^\\top x_1) = 6(10) = 60$.\n\nTask 1: OLS with a single predictor $x_1$\nThe design matrix is $X = x_1$, which is a $5 \\times 1$ matrix. The OLS coefficient $\\beta_1$ is found by solving the normal equation $(X^\\top X) \\beta_1 = X^\\top y$.\n$$\n\\beta_1 = (x_1^\\top x_1)^{-1} (x_1^\\top y) = (10)^{-1} (60) = 6\n$$\nThe coefficient for the single-feature model is $\\beta_1 = 6$.\n\nTask 2: OLS with predictors $[x_1, x_2]$ using the Moore-Penrose Pseudoinverse\nThe design matrix is $X = [x_1, x_2] = [x_1, 3x_1]$. The columns are perfectly collinear, so $X$ has rank $1$. The matrix $X^\\top X$ will be singular.\n$$\nX^\\top X = \\begin{bmatrix} x_1^\\top x_1 & x_1^\\top x_2 \\\\ x_2^\\top x_1 & x_2^\\top x_2 \\end{bmatrix} = \\begin{bmatrix} x_1^\\top x_1 & x_1^\\top (3x_1) \\\\ (3x_1)^\\top x_1 & (3x_1)^\\top (3x_1) \\end{bmatrix} = \\begin{bmatrix} x_1^\\top x_1 & 3(x_1^\\top x_1) \\\\ 3(x_1^\\top x_1) & 9(x_1^\\top x_1) \\end{bmatrix} = \\begin{bmatrix} 10 & 30 \\\\ 30 & 90 \\end{bmatrix}\n$$\nThe determinant of $X^\\top X$ is $10 \\times 90 - 30 \\times 30 = 0$, confirming singularity. The right-hand side of the normal equations is:\n$$\nX^\\top y = \\begin{bmatrix} x_1^\\top y \\\\ x_2^\\top y \\end{bmatrix} = \\begin{bmatrix} x_1^\\top y \\\\ (3x_1)^\\top y \\end{bmatrix} = \\begin{bmatrix} 60 \\\\ 3(60) \\end{bmatrix} = \\begin{bmatrix} 60 \\\\ 180 \\end{bmatrix}\n$$\nThe system $X^\\top X \\beta = X^\\top y$ has infinite solutions satisfying $10\\beta_1 + 30\\beta_2 = 60$, which simplifies to $\\beta_1 + 3\\beta_2 = 6$. The Moore-Penrose Pseudoinverse provides the minimum $\\ell_2$-norm solution. This solution vector $\\beta=[\\beta_1, \\beta_2]^\\top$ must be orthogonal to the null space of the constraint, meaning it must be proportional to the vector of coefficients $[1, 3]^\\top$. So, $\\beta=k[1, 3]^\\top = [k, 3k]^\\top$ for some scalar $k$.\nSubstituting this into the constraint equation:\n$$\nk + 3(3k) = 6 \\implies 10k = 6 \\implies k = 0.6\n$$\nThus, the minimum-norm solution is $\\beta_1 = 0.6$ and $\\beta_2 = 3(0.6) = 1.8$.\n\nTask 3: Ridge regression for various $\\lambda$\nThe ridge regression solution is given by $\\beta_\\lambda = (X^\\top X + \\lambda I)^{-1} X^\\top y$. The matrix to be inverted is:\n$$\nX^\\top X + \\lambda I = \\begin{bmatrix} 10 & 30 \\\\ 30 & 90 \\end{bmatrix} + \\begin{bmatrix} \\lambda & 0 \\\\ 0 & \\lambda \\end{bmatrix} = \\begin{bmatrix} 10+\\lambda & 30 \\\\ 30 & 90+\\lambda \\end{bmatrix}\n$$\nFor $\\lambda > 0$, this matrix is invertible. Its determinant is $(10+\\lambda)(90+\\lambda) - 900 = 100\\lambda + \\lambda^2 = \\lambda(\\lambda+100)$.\nThe inverse is:\n$$\n(X^\\top X + \\lambda I)^{-1} = \\frac{1}{\\lambda(\\lambda+100)} \\begin{bmatrix} 90+\\lambda & -30 \\\\ -30 & 10+\\lambda \\end{bmatrix}\n$$\nThe ridge coefficients are then:\n$$\n\\beta_\\lambda = \\begin{bmatrix} \\beta_{\\lambda,1} \\\\ \\beta_{\\lambda,2} \\end{bmatrix} = \\frac{1}{\\lambda(\\lambda+100)} \\begin{bmatrix} 90+\\lambda & -30 \\\\ -30 & 10+\\lambda \\end{bmatrix} \\begin{bmatrix} 60 \\\\ 180 \\end{bmatrix}\n$$\nCalculating the components:\n$\\beta_{\\lambda,1} = \\frac{1}{\\lambda(\\lambda+100)} [60(90+\\lambda) - 30(180)] = \\frac{60\\lambda}{\\lambda(\\lambda+100)} = \\frac{60}{\\lambda+100}$.\n$\\beta_{\\lambda,2} = \\frac{1}{\\lambda(\\lambda+100)} [-30(60) + 180(10+\\lambda)] = \\frac{180\\lambda}{\\lambda(\\lambda+100)} = \\frac{180}{\\lambda+100}$.\n\n- For $\\lambda=0$: The problem states to use the MPP, so the result is the same as in Task 2: $(\\beta_1, \\beta_2) = (0.6, 1.8)$.\n- For $\\lambda=0.1$: $(\\beta_1, \\beta_2) = (\\frac{60}{100.1}, \\frac{180}{100.1}) \\approx (0.599401, 1.798202)$.\n- For $\\lambda=1$: $(\\beta_1, \\beta_2) = (\\frac{60}{101}, \\frac{180}{101}) \\approx (0.594059, 1.782178)$.\n- For $\\lambda=10$: $(\\beta_1, \\beta_2) = (\\frac{60}{110}, \\frac{180}{110}) \\approx (0.545455, 1.636364)$.\n- For $\\lambda=1,000,000$: $(\\beta_1, \\beta_2) = (\\frac{60}{1000100}, \\frac{180}{1000100}) \\approx (0.000060, 0.000180)$.\n\nTask 4: Discussion on identifiability and coefficient paths\n- Identifiability: In the OLS case ($\\lambda=0$) with collinear features $x_1$ and $x_2$, the matrix $X^\\top X$ is singular. This leads to a system of normal equations with infinitely many solutions, all lying on the line $\\beta_1 + 3\\beta_2 = 6$. Since no unique solution for the parameter vector $\\beta$ exists, the parameters are non-identifiable. However, the predicted values $\\hat{y} = X\\beta$ are unique for all solutions, since $\\hat{y} = \\beta_1 x_1 + \\beta_2 x_2 = \\beta_1 x_1 + \\beta_2 (3x_1) = (\\beta_1 + 3\\beta_2)x_1 = 6x_1 = y$. Ridge regression, by adding the term $\\lambda I$ with $\\lambda > 0$, makes the matrix $(X^\\top X + \\lambda I)$ invertible, thereby ensuring a unique solution for $\\beta_\\lambda$ and making the parameters identifiable for any positive regularization strength.\n- Coefficient Paths: The Ridge regression solution we derived is $\\beta_\\lambda = (\\frac{60}{\\lambda+100}, \\frac{180}{\\lambda+100})$.\nThe ratio of the coefficients is:\n$$\n\\frac{\\beta_{\\lambda,2}}{\\beta_{\\lambda,1}} = \\frac{180/(\\lambda+100)}{60/(\\lambda+100)} = \\frac{180}{60} = 3\n$$\nThis ratio is constant and equal to $3$ for all $\\lambda > 0$. As $\\lambda \\to 0^+$, the ridge solution converges to $(0.6, 1.8)$, which is the minimum-norm OLS solution, and the ratio is $1.8/0.6=3$. As $\\lambda \\to \\infty$, both coefficients are shrunk towards $0$, but their ratio remains fixed at $3$. The coefficient path is a straight line from the minimum-norm solution $(0.6, 1.8)$ to the origin $(0,0)$. This occurs because the predictors $x_1$ and $x_2$ are perfectly correlated, defining a one-dimensional subspace spanned by the vector $[1, 3]^\\top$ in the feature space. The data variance exists only along this direction. Ridge regression penalizes coefficients, and in this case, it shrinks them along the principal component direction of the data covariance matrix $X^\\top X$. The principal eigenvector of $X^\\top X$ is proportional to $[1, 3]^\\top$, which governs the fixed ratio of the coefficients.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the linear regression problem under different conditions as specified.\n    \"\"\"\n    # Define base data using floating-point numbers for precision.\n    x1_vec = np.array([-2.0, -1.0, 0.0, 1.0, 2.0])\n    x1 = x1_vec.reshape(-1, 1)\n    y = 6.0 * x1\n\n    all_results = []\n\n    # Case 1: OLS with a single predictor x1 (p=1)\n    # beta_1 = (X1.T @ X1)^-1 @ X1.T @ y\n    X1 = x1\n    beta_case1 = np.linalg.inv(X1.T @ X1) @ (X1.T @ y)\n    all_results.append(beta_case1.flatten())\n\n    # Define data for two-predictor cases (p=2)\n    x2 = 3.0 * x1\n    X2 = np.hstack((x1, x2))\n    \n    # Case 2: OLS with predictors [x1, x2] using Moore-Penrose Pseudoinverse\n    # beta = pinv(X2) @ y\n    beta_mpp = np.linalg.pinv(X2) @ y\n    all_results.append(beta_mpp.flatten())\n\n    # Case 3: Ridge with lambda = 0, using MPP as specified\n    # This is identical to Case 2.\n    all_results.append(beta_mpp.flatten())\n\n    # Cases 4-7: Ridge with predictors [x1, x2] and lambda > 0\n    lambdas = [0.1, 1.0, 10.0, 1000000.0]\n    p = X2.shape[1]\n    identity_matrix = np.identity(p)\n    X2T_X2 = X2.T @ X2\n    X2T_y = X2.T @ y\n\n    for lam in lambdas:\n        # beta_ridge = (X2.T @ X2 + lam * I)^-1 @ X2.T @ y\n        beta_ridge = np.linalg.inv(X2T_X2 + lam * identity_matrix) @ X2T_y\n        all_results.append(beta_ridge.flatten())\n    \n    # Format the final output string as per requirements.\n    # Each coefficient is formatted to 6 decimal places.\n    formatted_results = []\n    for res_vector in all_results:\n        # Using f-string formatting handles rounding and ensures 6 decimal places.\n        s_res = [f\"{x:.6f}\" for x in res_vector]\n        formatted_results.append(f\"[{','.join(s_res)}]\")\n    \n    final_output_string = f\"[{','.join(formatted_results)}]\"\n    print(final_output_string)\n\nsolve()\n```", "id": "3160401"}, {"introduction": "Selecting the most relevant predictors from a large pool is a core task in machine learning, and a simple \"filter\" method based on correlation with the target is often a tempting first step. This practice reveals a critical flaw in this approach by demonstrating how a hidden confounding variable can create a statistical illusion known as Simpson's paradox. You will simulate a dataset where a genuinely useful predictor appears useless—or even harmful—when data is aggregated, showing how a naive feature selection rule can be dangerously misled and emphasizing the need for deeper analysis beyond marginal correlations [@problem_id:3160360].", "problem": "You will write a complete program that constructs synthetic data to demonstrate when a simple filter method for feature selection, based on the Pearson Product-Moment Correlation Coefficient (PPMCC), fails due to Simpson’s paradox in the presence of a binary confounder. Use the following foundational definitions and facts as the base: a predictor (feature) $x_j$ is a measurable function used as an input to predict a response $y$; the PPMCC between random variables $X$ and $Y$ is $\\rho(X,Y) = \\frac{\\mathrm{Cov}(X,Y)}{\\sqrt{\\mathrm{Var}(X)}\\sqrt{\\mathrm{Var}(Y)}}$; the law of total covariance for a confounder $Z$ is $\\mathrm{Cov}(X,Y) = \\mathbb{E}[\\mathrm{Cov}(X,Y\\mid Z)] + \\mathrm{Cov}(\\mathbb{E}[X\\mid Z],\\mathbb{E}[Y\\mid Z])$. A naive filter method that ranks features by $|\\rho(x_j,y)|$ on pooled data can be misled when $\\mathrm{Cov}(\\mathbb{E}[X\\mid Z],\\mathbb{E}[Y\\mid Z])$ reverses the sign of $\\mathbb{E}[\\mathrm{Cov}(X,Y\\mid Z)]$, which is precisely Simpson’s paradox for correlation. Your task is to implement a simulator that, for specified parameters, generates a dataset with two predictors $x_1$ and $x_2$, a binary confounder $z\\in\\{0,1\\}$, and a response $y$, then determines if the naive filter fails because of Simpson’s paradox. Work in purely mathematical terms; no physical units are involved.\n\nSimulation model for a given parameter set:\n- Let $n_0$ and $n_1$ be the sample sizes in groups $z=0$ and $z=1$, respectively. Construct $z$ by taking $n_0$ zeros and $n_1$ ones.\n- For each group $g\\in\\{0,1\\}$, draw $x_1\\mid z=g \\sim \\mathcal{N}(\\mu^{(x_1)}_g, \\sigma_{x_1}^2)$ independently across samples.\n- Construct $x_2$ in two ways depending on a boolean flag:\n  - If $x_2$ is “confounded,” set $x_2\\mid z=g = \\alpha_g + \\varepsilon^{(2)}$ with $\\varepsilon^{(2)}\\sim \\mathcal{N}(0,\\sigma_{x_2}^2)$ independent of everything else.\n  - If $x_2$ is “unconfounded,” draw $x_2 \\sim \\mathcal{N}(0,\\sigma_{x_2}^2)$ independently of $z$ and all other variables.\n- For each group $g\\in\\{0,1\\}$, generate the response by $y\\mid z=g = \\alpha_g + b\\,x_1 + \\varepsilon^{(y)}$ with $\\varepsilon^{(y)}\\sim \\mathcal{N}(0,\\sigma_y^2)$ independent of all predictors given $z$.\n\nFeature selection rule and failure definition:\n- Compute pooled-sample PPMCCs $r_{1,\\mathrm{all}}=\\rho(x_1,y)$ and $r_{2,\\mathrm{all}}=\\rho(x_2,y)$ across all samples.\n- The naive filter selects the index $j^\\star \\in \\{1,2\\}$ that maximizes $|r_{j,\\mathrm{all}}|$, with ties broken in favor of the smaller index.\n- Compute within-group PPMCCs $r_{1,0}=\\rho(x_1,y\\mid z=0)$ and $r_{1,1}=\\rho(x_1,y\\mid z=1)$.\n- Let $\\epsilon$ be a numerical tolerance equal to $10^{-6}$ to avoid ambiguous zero correlations. Declare that Simpson’s paradox holds for $(x_1,y)$ if $\\mathrm{sign}(r_{1,0})=\\mathrm{sign}(r_{1,1})\\neq 0$, $|r_{1,0}|>\\epsilon$, $|r_{1,1}|>\\epsilon$, $|r_{1,\\mathrm{all}}|>\\epsilon$, and $\\mathrm{sign}(r_{1,\\mathrm{all}})=-\\mathrm{sign}(r_{1,0})$. Declare a filter failure due to Simpson’s paradox if and only if Simpson’s paradox holds for $(x_1,y)$ and $j^\\star\\neq 1$.\n\nImplementation requirements:\n- Use a fixed random seed equal to $12345$ for reproducibility.\n- Use the standard PPMCC formula on finite samples: for vectors $u$ and $v$, $r(u,v)=\\frac{\\sum_i (u_i-\\bar u)(v_i-\\bar v)}{\\sqrt{\\sum_i (u_i-\\bar u)^2}\\sqrt{\\sum_i (v_i-\\bar v)^2}}$, provided both standard deviations are nonzero.\n\nTest suite of parameter sets to cover a strong paradox case, a no-confounding case, and a boundary case:\n- Case A (strong Simpson failure expected):\n  - $n_0=600$, $n_1=600$,\n  - $\\mu^{(x_1)}_0=-2.0$, $\\mu^{(x_1)}_1=2.0$,\n  - $\\alpha_0=2.0$, $\\alpha_1=-2.0$,\n  - $b=0.5$,\n  - $\\sigma_{x_1}=0.6$, $\\sigma_y=0.6$,\n  - $x_2$ confounded, $\\sigma_{x_2}=0.4$.\n- Case B (no failure; aligned groups; $x_2$ uninformative):\n  - $n_0=600$, $n_1=600$,\n  - $\\mu^{(x_1)}_0=-2.0$, $\\mu^{(x_1)}_1=2.0$,\n  - $\\alpha_0=-2.0$, $\\alpha_1=2.0$,\n  - $b=0.5$,\n  - $\\sigma_{x_1}=0.6$, $\\sigma_y=0.6$,\n  - $x_2$ unconfounded, $\\sigma_{x_2}=1.0$.\n- Case C (boundary; identical groups; no confounding):\n  - $n_0=600$, $n_1=600$,\n  - $\\mu^{(x_1)}_0=0.0$, $\\mu^{(x_1)}_1=0.0$,\n  - $\\alpha_0=0.0$, $\\alpha_1=0.0$,\n  - $b=0.5$,\n  - $\\sigma_{x_1}=1.0$, $\\sigma_y=1.0$,\n  - $x_2$ unconfounded, $\\sigma_{x_2}=1.0$.\n\nYour program should, for each case in the test suite, output a boolean indicating whether the naive filter fails due to Simpson’s paradox according to the above definition. The final output must be a single line containing the three booleans in order for Cases A, B, and C, formatted as a comma-separated list enclosed in square brackets, for example, $[\\mathrm{True},\\mathrm{False},\\mathrm{False}]$.", "solution": "The problem requires the creation of a computational simulation to demonstrate a specific failure mode of a naive feature selection method. The failure occurs when Simpson's paradox, induced by a binary confounding variable, misleads a filter method based on the Pearson Product-Moment Correlation Coefficient (PPMCC). The task is to construct the data, apply the selection logic, and determine if a failure occurs according to a precise definition for three distinct parameter sets.\n\nFirst, we establish the theoretical underpinnings. A predictor $x_j$ is used to predict a response $y$. A simple filter method for feature selection might rank predictors based on the magnitude of their pooled correlation with the response, $|\\rho(x_j, y)|$. This approach, however, can be flawed in the presence of a confounding variable $z$. The relationship between the pooled (marginal) correlation and the conditional correlations (within strata of $z$) is governed by the law of total covariance:\n$$\n\\mathrm{Cov}(X,Y) = \\mathbb{E}[\\mathrm{Cov}(X,Y\\mid Z)] + \\mathrm{Cov}(\\mathbb{E}[X\\mid Z],\\mathbb{E}[Y\\mid Z])\n$$\nHere, $\\mathbb{E}[\\mathrm{Cov}(X,Y\\mid Z)]$ is the average of the covariances within the subpopulations defined by $Z$, while $\\mathrm{Cov}(\\mathbb{E}[X\\mid Z],\\mathbb{E}[Y\\mid Z])$ is the covariance of the subpopulation means. Simpson's paradox for correlation arises when the second term, the \"confounding\" term, is sufficiently large and has the opposite sign of the first, \"within-group\" term, causing $\\mathrm{sign}(\\mathrm{Cov}(X,Y))$ to be opposite to $\\mathrm{sign}(\\mathrm{Cov}(X,Y\\mid Z=g))$ for all groups $g$.\n\nThe simulation model is defined as follows:\n- A binary confounder $z$ with $n_0$ samples in group $g=0$ and $n_1$ samples in group $g=1$.\n- A predictor $x_1$ generated from a Normal distribution conditional on $z$: $x_1 \\mid (z=g) \\sim \\mathcal{N}(\\mu^{(x_1)}_g, \\sigma_{x_1}^2)$.\n- A response variable $y$ generated via a linear model conditional on $z$: $y \\mid (z=g) = \\alpha_g + b\\,x_1 + \\varepsilon^{(y)}$, where $\\varepsilon^{(y)} \\sim \\mathcal{N}(0, \\sigma_y^2)$. In this model, $x_1$ has a direct relationship with $y$ within each group, governed by the coefficient $b$.\n- A second predictor $x_2$, which can be either \"confounded\" or \"unconfounded\".\n  - If confounded: $x_2 \\mid (z=g) = \\alpha_g + \\varepsilon^{(2)}$, with $\\varepsilon^{(2)} \\sim \\mathcal{N}(0, \\sigma_{x_2}^2)$. This predictor has no direct link to $y$ but is associated with it through the common term $\\alpha_g$, which depends on the confounder $z$.\n  - If unconfounded: $x_2 \\sim \\mathcal{N}(0, \\sigma_{x_2}^2)$, making it independent of all other variables.\n\nThe program implements a function to generate a dataset according to these specifications for a given set of parameters. A fixed random seed of $12345$ ensures reproducibility. The program then proceeds with the analysis:\n\n1.  **Correlation Calculation**: The sample PPMCC, $r(u,v) = \\frac{\\sum_i (u_i-\\bar{u})(v_i-\\bar{v})}{\\sqrt{\\sum_i (u_i-\\bar{u})^2 \\sum_i (v_i-\\bar{v})^2}}$, is calculated for several pairs of variables. A custom implementation is used to handle cases where a variable is constant, returning a correlation of $0$. The following correlations are computed:\n    - Pooled correlations: $r_{1,\\mathrm{all}} = r(x_1, y)$ and $r_{2,\\mathrm{all}} = r(x_2, y)$.\n    - Within-group correlations for $x_1$: $r_{1,0} = r(x_1, y \\mid z=0)$ and $r_{1,1} = r(x_1, y \\mid z=1)$.\n\n2.  **Feature Selection**: The naive filter selects the feature $j^\\star \\in \\{1, 2\\}$ that maximizes the absolute pooled correlation $|r_{j,\\mathrm{all}}|$. Ties are broken in favor of index $j=1$.\n\n3.  **Failure Analysis**: A \"filter failure due to Simpson's paradox\" is declared if two conditions are met simultaneously:\n    a. Simpson's paradox holds for the pair $(x_1, y)$. This is defined as:\n       - The within-group correlations have the same non-zero sign: $\\mathrm{sign}(r_{1,0}) = \\mathrm{sign}(r_{1,1}) \\neq 0$.\n       - The pooled correlation has the opposite sign: $\\mathrm{sign}(r_{1,\\mathrm{all}}) = -\\mathrm{sign}(r_{1,0})$.\n       - All relevant correlations are non-trivial in magnitude, i.e., $|r_{1,0}|, |r_{1,1}|, |r_{1,\\mathrm{all}}| > \\epsilon$, where $\\epsilon = 10^{-6}$.\n    b. The filter selects the wrong feature: $j^\\star \\neq 1$. An ideal feature selector should identify $x_1$ as the relevant predictor due to its direct link to $y$ via the coefficient $b$.\n\nThe program evaluates these conditions for three test cases designed to explore different scenarios:\n- **Case A**: Parameters are chosen to create a strong confounding effect that reverses the sign of the correlation for $x_1$, while making the confounded predictor $x_2$ appear strongly correlated with $y$. A failure is expected.\n- **Case B**: The confounding effect is structured to reinforce, not reverse, the underlying correlation. No Simpson's paradox and no failure are expected.\n- **Case C**: A null case where there are no group differences. This serves as a baseline where no paradox should occur.\n\nThe final output is a list of booleans indicating the outcome of the failure test for each of the three cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef ppmcc(u, v):\n    \"\"\"\n    Computes the Pearson Product-Moment Correlation Coefficient between two vectors.\n    This is a direct implementation of the formula provided in the problem statement.\n    \"\"\"\n    if len(u)  2 or len(v)  2:\n        return 0.0\n\n    u_mean = np.mean(u)\n    v_mean = np.mean(v)\n    \n    u_dev = u - u_mean\n    v_dev = v - v_mean\n    \n    numerator = np.sum(u_dev * v_dev)\n    \n    u_ss = np.sum(u_dev**2)\n    v_ss = np.sum(v_dev**2)\n    \n    denominator = np.sqrt(u_ss * v_ss)\n    \n    if denominator == 0:\n        return 0.0\n        \n    return numerator / denominator\n\ndef run_simulation(params, rng):\n    \"\"\"\n    Runs a single simulation for a given set of parameters.\n    \n    Args:\n        params (dict): A dictionary containing the parameters for the simulation.\n        rng (numpy.random.Generator): The random number generator instance.\n        \n    Returns:\n        bool: True if the naive filter fails due to Simpson's paradox, False otherwise.\n    \"\"\"\n    # Unpack parameters\n    n0, n1 = params['n0'], params['n1']\n    mu_x1_0, mu_x1_1 = params['mu_x1_0'], params['mu_x1_1']\n    alpha0, alpha1 = params['alpha0'], params['alpha1']\n    b = params['b']\n    sigma_x1, sigma_y = params['sigma_x1'], params['sigma_y']\n    is_x2_confounded = params['is_x2_confounded']\n    sigma_x2 = params['sigma_x2']\n    \n    # 1. Generate the binary confounder z\n    z = np.concatenate([np.zeros(n0, dtype=int), np.ones(n1, dtype=int)])\n    \n    # 2. Generate predictor x1\n    x1_group0 = rng.normal(loc=mu_x1_0, scale=sigma_x1, size=n0)\n    x1_group1 = rng.normal(loc=mu_x1_1, scale=sigma_x1, size=n1)\n    x1 = np.concatenate([x1_group0, x1_group1])\n    \n    # 3. Generate predictor x2\n    if is_x2_confounded:\n        eps2 = rng.normal(loc=0, scale=sigma_x2, size=n0 + n1)\n        z_based_alpha_x2 = np.concatenate([np.full(n0, alpha0), np.full(n1, alpha1)])\n        x2 = z_based_alpha_x2 + eps2\n    else: # unconfounded\n        x2 = rng.normal(loc=0, scale=sigma_x2, size=n0 + n1)\n\n    # 4. Generate the response variable y\n    eps_y = rng.normal(loc=0, scale=sigma_y, size=n0 + n1)\n    z_based_alpha_y = np.concatenate([np.full(n0, alpha0), np.full(n1, alpha1)])\n    y = z_based_alpha_y + b * x1 + eps_y\n\n    # 5. Calculate all required correlations\n    r_1_all = ppmcc(x1, y)\n    r_2_all = ppmcc(x2, y)\n    \n    mask0 = (z == 0)\n    mask1 = (z == 1)\n    \n    r_1_0 = ppmcc(x1[mask0], y[mask0])\n    r_1_1 = ppmcc(x1[mask1], y[mask1])\n\n    # 6. Apply the failure definition from the problem statement\n    epsilon = 1e-6\n    \n    sign_r10 = np.sign(r_1_0)\n    sign_r11 = np.sign(r_1_1)\n    sign_r1all = np.sign(r_1_all)\n    \n    # Condition for Simpson's paradox holding for (x1, y)\n    simpson_paradox_holds = (\n        sign_r10 == sign_r11 and\n        sign_r10 != 0 and\n        abs(r_1_0) > epsilon and\n        abs(r_1_1) > epsilon and\n        abs(r_1_all) > epsilon and\n        sign_r1all == -sign_r10\n    )\n    \n    # Naive filter selection rule\n    # j_star = argmax |r_j_all|, tie-break to index 1\n    if abs(r_2_all) > abs(r_1_all):\n        j_star = 2\n    else:\n        j_star = 1\n    \n    # Final definition of filter failure due to Simpson's paradox\n    filter_failure = simpson_paradox_holds and (j_star != 1)\n    \n    return filter_failure\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A: Strong Simpson failure expected\n        {'n0': 600, 'n1': 600, 'mu_x1_0': -2.0, 'mu_x1_1': 2.0,\n         'alpha0': 2.0, 'alpha1': -2.0, 'b': 0.5, 'sigma_x1': 0.6,\n         'sigma_y': 0.6, 'is_x2_confounded': True, 'sigma_x2': 0.4},\n        # Case B: No failure; aligned groups; x2 uninformative\n        {'n0': 600, 'n1': 600, 'mu_x1_0': -2.0, 'mu_x1_1': 2.0,\n         'alpha0': -2.0, 'alpha1': 2.0, 'b': 0.5, 'sigma_x1': 0.6,\n         'sigma_y': 0.6, 'is_x2_confounded': False, 'sigma_x2': 1.0},\n        # Case C: Boundary; identical groups; no confounding\n        {'n0': 600, 'n1': 600, 'mu_x1_0': 0.0, 'mu_x1_1': 0.0,\n         'alpha0': 0.0, 'alpha1': 0.0, 'b': 0.5, 'sigma_x1': 1.0,\n         'sigma_y': 1.0, 'is_x2_confounded': False, 'sigma_x2': 1.0}\n    ]\n\n    # Initialize a single random number generator with the fixed seed for reproducibility.\n    # This generator is passed to each simulation run to ensure sequential consumption\n    # of random numbers, as implied by a single global seed.\n    rng = np.random.default_rng(12345)\n    \n    results = []\n    for params in test_cases:\n        failure = run_simulation(params, rng)\n        results.append(failure)\n        \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3160360"}]}