{"hands_on_practices": [{"introduction": "Understanding model flexibility begins with being able to quantify it. This exercise focuses on polynomial regression, where model capacity can be directly measured by counting the number of features (monomials) it uses. By deriving this count from first principles and relating it to a model's generalization performance, you will gain a concrete understanding of how excessive flexibility leads to overfitting and compromises predictive power on new data. [@problem_id:3148660]", "problem": "A team is comparing polynomial-kernel models for a regression task with $p=20$ input variables and $n=500$ independent and identically distributed samples. The polynomial kernel corresponds to a linear model in a feature space consisting of all monomials in the original inputs up to total degree $d$. The team observes that both degree $d=2$ and degree $d=4$ models achieve essentially zero training error, while the test mean squared error is approximately $0.08$ for $d=2$ and $0.20$ for $d=4$.\n\nStarting from first principles in counting and statistical learning, do the following.\n\n1) Using only the definition of a monomial as a product $x_{1}^{\\alpha_{1}} x_{2}^{\\alpha_{2}} \\cdots x_{p}^{\\alpha_{p}}$ with nonnegative integer exponents $\\alpha_{j}$ whose total degree satisfies $\\sum_{j=1}^{p} \\alpha_{j} \\le d$, derive the closed-form expression for the number of distinct monomials in $p$ variables of total degree at most $d$. Your derivation must not assume any pre-existing counting formula and should proceed from a valid combinatorial argument.\n\n2) Evaluate your expression from part (1) at $p=20$ for $d=2$ and $d=4$.\n\n3) It is a well-tested fact in statistical learning theory that for linear predictors in an $M$-dimensional bounded feature space with norm constraints, data-independent generalization bounds based on complexity measures such as Rademacher complexity scale proportionally to $\\sqrt{M/n}$. Using this scaling as a proxy, compute the predicted ratio $r$ of the generalization gaps (degree $d=4$ divided by degree $d=2$) for this problem. Express $r$ as a unitless number and round your answer to three significant figures.\n\n4) Briefly explain, using your counts from part (2), how the change in feature dimensionality relates to the observed increase in test error and what this implies about model flexibility, capacity, and interpretability.\n\nOnly the numerical value of $r$ is to be submitted as the final answer.", "solution": "The problem is assessed to be valid as it is scientifically grounded in statistical learning theory, well-posed, objective, and contains all necessary information for a solution.\n\nThe problem asks for four tasks: 1) derive a formula for the number of monomials, 2) evaluate this formula for specific parameters, 3) compute a ratio of generalization gaps based on a given scaling law, and 4) provide a conceptual explanation of the results. The final answer is specified to be only the numerical value from part (3).\n\n**Part 1: Derivation of the Number of Monomials**\n\nWe are asked to find the number of distinct monomials in $p$ variables, $x_1, x_2, \\dots, x_p$, of total degree at most $d$. A monomial is of the form $x_{1}^{\\alpha_{1}} x_{2}^{\\alpha_{2}} \\cdots x_{p}^{\\alpha_{p}}$, where the exponents $\\alpha_j$ are non-negative integers. The condition is that the total degree, $\\sum_{j=1}^{p} \\alpha_{j}$, is less than or equal to $d$.\n$$ \\sum_{j=1}^{p} \\alpha_{j} \\le d, \\quad \\alpha_j \\in \\{0, 1, 2, \\dots\\} $$\nThis problem is equivalent to counting the number of non-negative integer solutions to the inequality above. To transform this inequality into a more manageable equality, we introduce a non-negative integer \"slack\" variable, $\\alpha_{p+1}$. We define $\\alpha_{p+1}$ as:\n$$ \\alpha_{p+1} = d - \\sum_{j=1}^{p} \\alpha_{j} $$\nSince $\\sum_{j=1}^{p} \\alpha_{j} \\le d$, it is guaranteed that $\\alpha_{p+1} \\ge 0$. The original inequality is now perfectly equivalent to finding the number of non-negative integer solutions to the equation:\n$$ \\alpha_1 + \\alpha_2 + \\cdots + \\alpha_p + \\alpha_{p+1} = d $$\nThis is a classic combinatorial problem that can be solved using a \"stars and bars\" argument, as required. Imagine we have $d$ identical items (stars, $\\star$) to be distributed among $p+1$ distinct bins (the variables $\\alpha_1, \\dots, \\alpha_{p+1}$). We can represent this arrangement by placing the $d$ stars in a row and using $p$ bars ($|$) to partition them into $p+1$ groups. The number of stars in the $j$-th group corresponds to the value of $\\alpha_j$.\n\nFor example, with $d=3$ stars and $p=2$ variables (meaning $p+1=3$ bins), the arrangement $\\star|\\star\\star|$ corresponds to $\\alpha_1=1$, $\\alpha_2=2$, and the slack $\\alpha_3=0$. This corresponds to the monomial $x_1^1 x_2^2$ of degree $3$. The arrangement $| \\star | \\star\\star$ corresponds to $\\alpha_1=0$, $\\alpha_2=1$, and $\\alpha_3=2$. This implies $\\alpha_1+\\alpha_2 = 1 \\le 3$, representing the monomial $x_2^1$ of degree $1$.\n\nTo count the number of such arrangements, we consider a sequence of $d+p$ total positions. We need to choose $p$ of these positions to place the bars (the remaining $d$ positions will be filled by stars). The number of ways to do this is given by the binomial coefficient \"($d+p$) choose $p$\".\n\nLet $M(p, d)$ be the number of monomials. The number of ways to choose $p$ positions for the bars from $d+p$ total positions is:\n$$ M(p, d) = \\binom{p+d}{p} $$\nAlternatively, we could choose $d$ positions for the stars, which gives the equivalent expression:\n$$ M(p, d) = \\binom{p+d}{d} = \\frac{(p+d)!}{d! p!} $$\nThis is the closed-form expression for the number of distinct monomials in $p$ variables of total degree at most $d$. This count includes the constant term (the monomial of degree $0$), which corresponds to the solution $\\alpha_1 = \\alpha_2 = \\dots = \\alpha_p = 0$.\n\n**Part 2: Evaluation of the Expression**\n\nWe are given $p=20$ variables and are asked to evaluate the number of features for models of degree $d=2$ and $d=4$.\n\nFor $d=2$, let the number of features be $M_2$.\n$$ M_2 = M(20, 2) = \\binom{20+2}{2} = \\binom{22}{2} $$\n$$ M_2 = \\frac{22!}{2!(22-2)!} = \\frac{22!}{2!20!} = \\frac{22 \\times 21}{2 \\times 1} = 11 \\times 21 = 231 $$\n\nFor $d=4$, let the number of features be $M_4$.\n$$ M_4 = M(20, 4) = \\binom{20+4}{4} = \\binom{24}{4} $$\n$$ M_4 = \\frac{24!}{4!(24-4)!} = \\frac{24!}{4!20!} = \\frac{24 \\times 23 \\times 22 \\times 21}{4 \\times 3 \\times 2 \\times 1} $$\nSince $4 \\times 3 \\times 2 \\times 1 = 24$, we can simplify:\n$$ M_4 = 23 \\times 22 \\times 21 = 506 \\times 21 = 10626 $$\nSo, the number of features is $M_2 = 231$ for the degree-2 model and $M_4 = 10626$ for the degree-4 model.\n\n**Part 3: Computation of the Ratio of Generalization Gaps**\n\nWe are told that for a linear predictor in an $M$-dimensional feature space, a typical data-independent generalization bound scales proportionally to $\\sqrt{M/n}$. The generalization gap, $G$, is the difference between the test error and the training error. We are asked to use this scaling as a proxy.\nLet $G_d$ be the generalization gap for a model of degree $d$.\n$$ G_d \\propto \\sqrt{\\frac{M_d}{n}} $$\nThis implies $G_d = C \\sqrt{M_d/n}$ for some constant of proportionality $C$ that depends on factors not relevant to this ratio, such as norm constraints on the weights and properties of the data distribution.\n\nThe predicted ratio, $r$, of the generalization gaps is:\n$$ r = \\frac{G_4}{G_2} = \\frac{C \\sqrt{M_4/n}}{C \\sqrt{M_2/n}} $$\nThe constant $C$ and the sample size $n$ cancel out:\n$$ r = \\sqrt{\\frac{M_4}{M_2}} $$\nUsing the values calculated in Part 2:\n$$ r = \\sqrt{\\frac{10626}{231}} $$\nFirst, we simplify the fraction:\n$$ \\frac{10626}{231} = 46 $$\nTherefore, the ratio $r$ is:\n$$ r = \\sqrt{46} $$\nTo express this as a number rounded to three significant figures:\n$$ r \\approx 6.7823... $$\nRounding to three significant figures gives $r \\approx 6.78$.\n\n**Part 4: Explanation of the Observed Increase in Test Error**\n\nThe number of features, $M$, is a direct measure of a model's capacity or flexibility. A model with a larger $M$ can represent a more complex class of functions.\nIn this problem, we have $p=20$ inputs and $n=500$ samples.\n- The degree $d=2$ model has $M_2=231$ features. Since $M_2  n$, this is a regime where a good fit is possible but not guaranteed to be perfect interpolation.\n- The degree $d=4$ model has $M_4=10626$ features. Here, $M_4 \\gg n$, placing the model in a heavily over-parameterized regime.\n\nThe observation that both models achieve \"essentially zero training error\" is explained by their high capacity. With $231$ features for $500$ data points, the $d=2$ model is already very flexible. The $d=4$ model, being in the over-parameterized regime ($M_4 > n$), has more parameters than data points, which generally allows it to interpolate the training data, thus driving the training error to zero.\n\nThe test error, however, is a measure of how well the model generalizes to new, unseen data. It is influenced by the bias-variance tradeoff.\n- **Flexibility and Capacity**: Moving from $d=2$ to $d=4$ increases the feature space dimensionality from $M_2=231$ to $M_4=10626$, a dramatic increase in model capacity.\n- **Overfitting and Variance**: While this increased capacity helps to reduce the training error (and potentially model bias), it comes at the cost of much higher variance. The $d=4$ model, with its vast number of features, is not only fitting the underlying signal in the training data but is also fitting the random noise specific to that sample. This is overfitting. The model becomes highly sensitive to the particularities of the training set.\n- **Test Error**: The increase in test MSE from $0.08$ for $d=2$ to $0.20$ for $d=4$ is a classic symptom of overfitting. The increased variance of the $d=4$ model dominates any potential reduction in bias, leading to poorer generalization performance. The $d=2$ model represents a better balance between bias and variance for this specific problem and dataset size.\n- **Interpretability**: Interpretability is inversely related to model complexity. A linear model on $M_2=231$ features is already very difficult to interpret. A model on $M_4=10626$ features, which involve complex products of up to four original variables, is for all practical purposes a \"black box.\" It is impossible for a human to analyze the individual effects of tens of thousands of coefficients to understand the model's decision process. The simpler $d=2$ model is comparatively more interpretable.\n\nIn summary, the transition from $d=2$ to $d=4$ massively increases model capacity, leading to severe overfitting, which manifests as an increase in test error due to high variance, and a stark decrease in model interpretability.", "answer": "$$\n\\boxed{6.78}\n$$", "id": "3148660"}, {"introduction": "While changing the feature set is one way to alter model capacity, a more common technique is regularization, which allows us to tune flexibility continuously. In this hands-on coding exercise, you will investigate the effect of L2 regularization on a logistic regression model by tracking how its coefficients change as the penalty strength $\\lambda$ varies. This analysis of the \"regularization path\" provides powerful insights into model stability and interpretability, revealing how a well-chosen penalty can lead to more robust conclusions, especially in the presence of correlated features or noise. [@problem_id:3148628]", "problem": "You are given the task of analyzing how the signs of logistic regression coefficients change as the regularization strength varies, and to connect this behavior to the model’s flexibility, capacity, and interpretability. Consider binary classification with logistic regression under a squared Euclidean norm (also called L2) penalty within Empirical Risk Minimization (ERM). The model parameters are a weight vector $w \\in \\mathbb{R}^d$ and an intercept $b \\in \\mathbb{R}$, with predictions $p_i = \\sigma(z_i)$ where $z_i = w^\\top x_i + b$ and $\\sigma(t) = \\frac{1}{1 + e^{-t}}$. For a dataset $\\{(x_i, y_i)\\}_{i=1}^n$ with $y_i \\in \\{0,1\\}$, the regularized objective for a given regularization parameter $\\lambda \\ge 0$ is\n$$\nJ_\\lambda(w,b) = \\frac{1}{n} \\sum_{i=1}^n \\left( \\log\\left(1 + e^{z_i}\\right) - y_i z_i \\right) + \\frac{\\lambda}{2} \\|w\\|_2^2,\n$$\nwhere $\\|w\\|_2$ is the Euclidean norm, and the intercept $b$ is not penalized.\n\nDefinitions to be used:\n- The sign of a coefficient with a tolerance $\\varepsilon  0$ is defined by the function $S_\\varepsilon:\\mathbb{R}\\to\\{-1,0,1\\}$,\n$$\nS_\\varepsilon(u) =\n\\begin{cases}\n-1,  \\text{if } u  -\\varepsilon, \\\\\n0,  \\text{if } |u| \\le \\varepsilon, \\\\\n1,  \\text{if } u  \\varepsilon.\n\\end{cases}\n$$\n- Given a grid of regularization parameters $\\Lambda = \\{\\lambda_1, \\lambda_2, \\dots, \\lambda_m\\}$ ordered increasingly, for feature index $j \\in \\{1, \\dots, d\\}$, the sign-change count across the path is\n$$\nC_j = \\sum_{k=1}^{m-1} \\mathbf{1}\\left\\{ S_\\varepsilon\\big(w_j(\\lambda_k)\\big) \\ne S_\\varepsilon\\big(w_j(\\lambda_{k+1})\\big) \\right\\},\n$$\nwhere $\\mathbf{1}\\{\\cdot\\}$ denotes the indicator function, and $w_j(\\lambda)$ is the coefficient for feature $j$ fitted at regularization $\\lambda$.\n- The per-feature stability over the path is the fraction of adjacent pairs without a sign change:\n$$\n\\text{stab}_j = \\frac{1}{m-1} \\sum_{k=1}^{m-1} \\mathbf{1}\\left\\{ S_\\varepsilon\\big(w_j(\\lambda_k)\\big) = S_\\varepsilon\\big(w_j(\\lambda_{k+1})\\big) \\right\\}.\n$$\n- The overall stability index is the average stability across features:\n$$\n\\text{Stability} = \\frac{1}{d} \\sum_{j=1}^d \\text{stab}_j.\n$$\n\nStarting from the fundamental definitions of the logistic model, its loss, and the regularization penalty, derive and implement a solver for the regularized objective $J_\\lambda(w,b)$ that fits $(w(\\lambda), b(\\lambda))$ for each $\\lambda \\in \\Lambda$ using gradient-based optimization. Then, compute $C_j$ for each feature $j$ and the overall Stability for each test case described below.\n\nThe regularization grid is fixed as $\\Lambda = \\{0.001, 0.01, 0.1, 1.0, 10.0, 100.0\\}$, the tolerance is $\\varepsilon = 10^{-6}$, and the intercept $b$ must not be penalized.\n\nTest suite (three cases) to ensure coverage:\n- Case A (general, clear signal): Generate data with seed $0$. Let $n=120$, $d=3$. Draw $X \\in \\mathbb{R}^{n \\times d}$ with entries independently from a standard normal distribution. Define the ground-truth parameters $w^* = (3, -2, 0)$ and $b^* = 0$. For each $i$, compute $z_i^* = x_i^\\top w^* + b^*$ and set $y_i = 1$ if $z_i^* \\ge 0$ and $y_i = 0$ otherwise.\n- Case B (high collinearity, ambiguous attribution): Generate data with seed $1$. Let $n=100$, $d=3$. Draw $x^{(1)} \\in \\mathbb{R}^n$ from a standard normal distribution. Create $x^{(2)} = x^{(1)} + 0.01 \\cdot \\xi$ where $\\xi$ is standard normal with the same seed progression. Draw $x^{(3)}$ independently from a standard normal distribution. Form $X$ by stacking $(x^{(1)}, x^{(2)}, x^{(3)})$. Define $w^* = (1, -1, 0)$, $b^* = 0$. For each $i$, set $y_i = 1$ if $x_i^{(1)} - x_i^{(2)} \\ge 0$ and $y_i = 0$ otherwise.\n- Case C (low signal-to-noise, regularization-dominated): Generate data with seed $2$. Let $n=200$, $d=3$. Draw $X$ as standard normal. Define $w^* = (0.2, -0.2, 0)$ and $b^* = 0$. Draw independent noise $\\eta \\in \\mathbb{R}^n$ from a standard normal distribution with the same seed. Compute $z_i^* = x_i^\\top w^* + b^* + 0.1 \\eta_i$ and set $y_i = 1$ if $z_i^* \\ge 0$ and $y_i = 0$ otherwise.\n\nProgram requirements:\n- Fit $(w(\\lambda), b(\\lambda))$ for each $\\lambda \\in \\Lambda$ by minimizing $J_\\lambda(w,b)$ using a gradient-based optimizer with an exact gradient; ensure the intercept $b$ is not penalized.\n- Compute, for each test case, the list $[C_1, C_2, C_3]$ of sign-change counts across $\\Lambda$ for the $d=3$ features, and the overall Stability as defined above, expressed as a decimal rounded to three digits after the decimal point.\n- Final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a two-element list of the form $[[C_1,C_2,C_3],\\text{Stability}]$. For example, the final printed line must look like $[[[c_{A1},c_{A2},c_{A3}],s_A],[[c_{B1},c_{B2},c_{B3}],s_B],[[c_{C1},c_{C2},c_{C3}],s_C]]$, with all $c$ values as integers and all $s$ values as decimals rounded to three digits.\n\nNo physical units or angles are involved, and no percentages should be used. All numerical outputs must be integers or decimals as specified. The datasets must be generated exactly as described to ensure reproducibility. The implementation must not require any external input and must run as-is.", "solution": "The user wants to analyze the stability of logistic regression coefficients under varying L2 regularization strength. This involves implementing a logistic regression solver, generating three distinct datasets, fitting the model for a range of regularization parameters ($\\lambda$), and then quantifying the stability of the coefficient signs across this regularization path.\n\n### 1. Mathematical Formulation\n\nThe problem is to minimize the regularized objective function for logistic regression:\n$$\nJ_\\lambda(w,b) = \\frac{1}{n} \\sum_{i=1}^n \\left( \\log\\left(1 + e^{w^\\top x_i + b}\\right) - y_i (w^\\top x_i + b) \\right) + \\frac{\\lambda}{2} \\|w\\|_2^2\n$$\nThis objective function is convex, ensuring that a gradient-based optimization method can find the unique global minimum for any $\\lambda  0$. To use such an optimizer, we need the gradient of $J_\\lambda$ with respect to the parameters $w$ and $b$.\n\nLet $z_i = w^\\top x_i + b$. The sigmoid function is $\\sigma(z_i) = p_i = \\frac{1}{1 + e^{-z_i}}$.\nThe gradient of the loss term for a single data point with respect to $z_i$ is:\n$$\n\\frac{\\partial}{\\partial z_i} \\left( \\log(1 + e^{z_i}) - y_i z_i \\right) = \\frac{e^{z_i}}{1 + e^{z_i}} - y_i = \\sigma(z_i) - y_i = p_i - y_i\n$$\nUsing the chain rule, we can find the gradients with respect to each component of $w$ and $b$:\n$$\n\\frac{\\partial z_i}{\\partial w_j} = x_{ij} \\quad \\text{and} \\quad \\frac{\\partial z_i}{\\partial b} = 1\n$$\nThe gradient of the objective function $J_\\lambda$ with respect to a weight $w_j$ is:\n$$\n\\nabla_{w_j} J_\\lambda = \\frac{1}{n} \\sum_{i=1}^n \\frac{\\partial J_\\lambda}{\\partial z_i} \\frac{\\partial z_i}{\\partial w_j} + \\frac{\\partial}{\\partial w_j} \\left( \\frac{\\lambda}{2} \\|w\\|_2^2 \\right) = \\frac{1}{n} \\sum_{i=1}^n (p_i - y_i)x_{ij} + \\lambda w_j\n$$\nIn vector form, the gradient with respect to the entire weight vector $w$ is:\n$$\n\\nabla_w J_\\lambda = \\frac{1}{n} X^\\top (p - y) + \\lambda w\n$$\nwhere $p$ is the vector of predicted probabilities and $y$ is the vector of true labels.\n\nThe intercept $b$ is not penalized. Its gradient is:\n$$\n\\nabla_b J_\\lambda = \\frac{1}{n} \\sum_{i=1}^n \\frac{\\partial J_\\lambda}{\\partial z_i} \\frac{\\partial z_i}{\\partial b} = \\frac{1}{n} \\sum_{i=1}^n (p_i - y_i)\n$$\n\nThese exact gradients will be supplied to a quasi-Newton optimization algorithm (L-BFGS-B) to find the optimal $(w(\\lambda), b(\\lambda))$ for each $\\lambda$ in the specified grid.\n\n### 2. Implementation Strategy\n\nThe solution is implemented in Python, adhering to the specified environment. The overall process for each test case is as follows:\n\n1.  **Data Generation**: For each of the three test cases (A, B, C), a specific dataset $(X, y)$ is generated according to the problem's rules, using `numpy.random.default_rng` with the specified seed for reproducibility.\n\n2.  **Model Fitting**: A loop iterates through the fixed regularization grid $\\Lambda = \\{0.001, 0.01, 0.1, 1.0, 10.0, 100.0\\}$.\n    *   For each $\\lambda \\in \\Lambda$, the objective function $J_\\lambda(w,b)$ and its gradient are passed to `scipy.optimize.minimize` with the 'L-BFGS-B' method. The parameters to be optimized are a single vector concatenating $w$ and $b$.\n    *   The implementation uses \"warm starts\": the solution $(w, b)$ found for a given $\\lambda_k$ is used as the initial guess for the optimization at the next value, $\\lambda_{k+1}$. This improves efficiency as solutions for adjacent $\\lambda$ values are typically close.\n    *   Numerical stability is ensured by using the `log-sum-exp` trick for the loss term and `scipy.special.expit` for a stable sigmoid function.\n\n3.  **Metrics Calculation**: After fitting the model for all $\\lambda$ values, the resulting coefficient path (a sequence of $w$ vectors) is analyzed.\n    *   The sign of each coefficient $w_j(\\lambda)$ is determined using the provided sign function $S_\\varepsilon(u)$ with $\\varepsilon = 10^{-6}$.\n    *   The number of sign changes, $C_j$, for each feature $j$ is counted by comparing the signs of coefficients at adjacent $\\lambda$ values in the path.\n    *   The `Stability` index is calculated as the average per-feature stability, $\\text{stab}_j$, where $\\text{stab}_j$ is the fraction of adjacent $\\lambda$ pairs for which the sign of $w_j$ does not change. This is equivalent to $\\text{stab}_j = 1 - C_j / (m-1)$, where $m$ is the number of points in the $\\lambda$ grid.\n\n4.  **Output Formatting**: The results for each test case, consisting of the list of sign-change counts $[C_1, C_2, C_3]$ and the overall `Stability` (rounded to three decimal places), are formatted into a precise string representation without extra spaces, as specified. The final output is a single line containing a list of these results.\n\n### 3. Expected Behavior for Test Cases\n\n*   **Case A (Clear Signal)**: With strong underlying coefficients for features 1 and 2, their signs should be correctly identified and remain stable until high regularization shrinks them to zero. Feature 3 has no signal, so its coefficient should remain near zero. We expect low sign-change counts and high stability.\n*   **Case B (High Collinearity)**: Features 1 and 2 are nearly identical ($x^{(2)} \\approx x^{(1)}$), while the outcome depends on their difference ($x^{(1)} - x^{(2)}$). This creates ambiguity, as many combinations of $w_1$ and $w_2$ where $w_1 \\approx -w_2$ can produce a good fit. The L2 penalty will select a unique solution, but the coefficient path might be less smooth than in Case A, potentially leading to instability and more sign changes for $w_1$ and $w_2$.\n*   **Case C (Low Signal-to-Noise)**: The true coefficients are small and significant noise is added to the linear predictor. The model may struggle to distinguish the weak signal from the noise, especially at low regularization. As $\\lambda$ increases, the penalty will quickly dominate, shrinking the already small coefficients to zero. This scenario is expected to exhibit the most instability and the lowest stability score.", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.special import expit\n\ndef solve():\n    \"\"\"\n    Solves the logistic regression regularization path problem for three test cases.\n    \"\"\"\n\n    # --- Constants from the problem statement ---\n    LAMBDA_GRID = np.array([0.001, 0.01, 0.1, 1.0, 10.0, 100.0])\n    EPSILON = 1e-6\n\n    # --- Nested Helper Functions ---\n    def _generate_data(case_spec):\n        \"\"\"Generates dataset (X, y) based on the case specification.\"\"\"\n        case_id = case_spec['id']\n        seed = case_spec['seed']\n        n = case_spec['n']\n        d = case_spec['d']\n        rng = np.random.default_rng(seed)\n\n        if case_id == 'A':\n            w_star = np.array(case_spec['w_star'])\n            b_star = case_spec['b_star']\n            X = rng.standard_normal(size=(n, d))\n            z_star = X @ w_star + b_star\n            y = (z_star = 0).astype(int)\n        elif case_id == 'B':\n            # Collinear features\n            x1 = rng.standard_normal(size=n)\n            xi = rng.standard_normal(size=n)\n            x2 = x1 + 0.01 * xi\n            x3 = rng.standard_normal(size=n)\n            X = np.stack([x1, x2, x3], axis=1)\n            y = ((x1 - x2) = 0).astype(int)\n        elif case_id == 'C':\n            w_star = np.array(case_spec['w_star'])\n            b_star = case_spec['b_star']\n            noise_std = case_spec['noise_std']\n            X = rng.standard_normal(size=(n, d))\n            eta = rng.standard_normal(size=n)\n            z_star = X @ w_star + b_star + noise_std * eta\n            y = (z_star = 0).astype(int)\n        \n        return X, y\n\n    def _objective_and_grad(params, X, y, lambda_val):\n        \"\"\"Computes the objective function J and its gradient.\"\"\"\n        n, d = X.shape\n        w = params[:d]\n        b = params[d]\n\n        z = X @ w + b\n\n        # Numerically stable calculation of log-likelihood part\n        log_1_plus_exp_z = np.maximum(0, z) + np.log(1 + np.exp(-np.abs(z)))\n        loss = np.mean(log_1_plus_exp_z - y * z)\n\n        # L2 regularization term (only on w)\n        reg_term = (lambda_val / 2.0) * np.dot(w, w)\n        J = loss + reg_term\n\n        # Gradient calculation\n        p = expit(z)\n        p_minus_y = p - y\n        grad_w = (1.0 / n) * X.T @ p_minus_y + lambda_val * w\n        grad_b = np.mean(p_minus_y)\n        grad = np.append(grad_w, grad_b)\n        \n        return J, grad\n\n    def _s_epsilon_vec(u_vec, epsilon):\n        \"\"\"Vectorized sign function with tolerance.\"\"\"\n        return np.select([u_vec  -epsilon, u_vec  epsilon], [-1, 1], default=0)\n\n    def _calculate_metrics(w_path, m, d, epsilon):\n        \"\"\"Calculates sign-change counts and stability.\"\"\"\n        w_path_np = np.array(w_path)\n        sign_path = _s_epsilon_vec(w_path_np, epsilon)\n        \n        # Sign-change count C_j\n        sign_diffs = np.diff(sign_path, axis=0)\n        sign_changes = (sign_diffs != 0).astype(int)\n        c_counts = np.sum(sign_changes, axis=0)\n        \n        # Stability\n        num_transitions = m - 1\n        if num_transitions == 0:\n            return c_counts, 1.0\n        \n        stab_j = (num_transitions - c_counts) / num_transitions\n        overall_stability = np.mean(stab_j)\n        \n        return c_counts, overall_stability\n\n    # --- Main Logic ---\n    test_cases = [\n        {'id': 'A', 'seed': 0, 'n': 120, 'd': 3, 'w_star': [3, -2, 0], 'b_star': 0},\n        {'id': 'B', 'seed': 1, 'n': 100, 'd': 3},\n        {'id': 'C', 'seed': 2, 'n': 200, 'd': 3, 'w_star': [0.2, -0.2, 0], 'b_star': 0, 'noise_std': 0.1},\n    ]\n\n    all_results_str = []\n\n    for case_spec in test_cases:\n        X, y = _generate_data(case_spec)\n        d = X.shape[1]\n        \n        w_path = []\n        # Initial guess for the optimization (cold start for the first lambda)\n        params_0 = np.zeros(d + 1)\n        \n        for lambda_val in LAMBDA_GRID:\n            result = minimize(\n                fun=_objective_and_grad,\n                x0=params_0,\n                args=(X, y, lambda_val),\n                method='L-BFGS-B',\n                jac=True\n            )\n            # Use the current solution as a warm start for the next iteration\n            params_0 = result.x\n            w_path.append(result.x[:d])\n\n        c_counts, stability = _calculate_metrics(w_path, len(LAMBDA_GRID), d, EPSILON)\n        \n        c_counts_list = list(c_counts.astype(int))\n        \n        # Format the result string for this case\n        result_str = f\"[[{','.join(map(str, c_counts_list))}],{stability:.3f}]\"\n        all_results_str.append(result_str)\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(all_results_str)}]\")\n\nsolve()\n```", "id": "3148628"}, {"introduction": "The most sophisticated models often build flexibility adaptively, adding complexity only where it is most needed to capture patterns in the data. This practice introduces this concept through regression splines, a powerful tool for flexible, non-linear modeling. You will implement a greedy algorithm that iteratively adds knots at locations of high residual error, allowing you to directly observe the trade-off between reducing training error and increasing model complexity, measured by the degrees of freedom. [@problem_id:3157197]", "problem": "You are to implement an algorithmic procedure for adaptive knot placement in a regression spline model, and report the evolution of model complexity and error. Begin from the following foundational base: Ordinary Least Squares (OLS) regression minimizes the sum of squared residuals, piecewise polynomials are represented via truncated power bases, and the residual at a sample is the difference between the observed and fitted value. You must not use any shortcut formulas beyond these base definitions.\n\nConsider a one-dimensional input domain with samples $\\{(x_i, y_i)\\}_{i=1}^n$ where $x_i \\in [0,1]$ and $y_i \\in \\mathbb{R}$. A piecewise polynomial of degree $m$ with knots $\\{t_j\\}_{j=1}^K$ can be represented using the truncated power basis as a linear combination of the functions $1, x, x^2, \\dots, x^m$ and $(x - t_j)_+^m$ for each knot $t_j$, where $(u)_+ = \\max(u, 0)$. For cubic splines ($m = 3$), this yields a model\n$$\n\\hat{y}(x) = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3 + \\sum_{j=1}^K \\gamma_j (x - t_j)_+^3.\n$$\nIn OLS, the coefficients are chosen to minimize\n$$\n\\sum_{i=1}^n \\left( y_i - \\hat{y}(x_i) \\right)^2.\n$$\nDefine the residuals as $r_i = y_i - \\hat{y}(x_i)$ and the Root Mean Squared Error (RMSE) as\n$$\n\\mathrm{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n r_i^2}.\n$$\nDefine the degrees of freedom (df) of the fitted model as the number of estimated coefficients, which for the cubic truncated power basis is $4 + K$ (one for each of $1, x, x^2, x^3$ and one per knot).\n\nImplement the following multi-resolution knot addition algorithm:\n- Initialize with no knots ($K = 0$) and fit the cubic model using OLS on the basis $\\{1, x, x^2, x^3\\}$.\n- At each iteration $k = 1, 2, \\dots$, compute the current residuals $\\{r_i\\}_{i=1}^n$, identify an index $i^\\star$ that maximizes $|r_{i^\\star}|$ (break ties by choosing the smallest index), and propose a new knot at $t_k = x_{i^\\star}$.\n- If $t_k$ is already in the knot set, skip to the next largest $|r_i|$ not yet used; if all $x_i$ are already knots, terminate.\n- Add the knot $t_k$, refit the model including the new basis function $(x - t_k)_+^3$, then record the updated $df$ and $\\mathrm{RMSE}$.\n- Continue until a specified maximum number of knots is reached or until adding a knot would exceed $n$ basis functions (to avoid degeneracy), i.e., terminate if $4 + K \\ge n$.\n\nData generation for each test case must follow:\n- Draw $n$ points $x_i$ uniformly in $[0,1]$.\n- Use the deterministic function $f(x) = \\sin(2\\pi x) + \\frac{1}{2} x$.\n- Generate noise $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$ and set $y_i = f(x_i) + \\varepsilon_i$.\n- Use a specified random seed for reproducibility.\n\nYour program must implement this algorithm and report, for each test case, the sequence of $df$ and $\\mathrm{RMSE}$ values starting from iteration $k = 0$ (no knots) up to the final iteration. The output for each test case must be a list containing two lists: the list of $df$ values and the list of $\\mathrm{RMSE}$ values, both ordered by iteration. Aggregate the results for all test cases into a single list.\n\nTest Suite:\n- Case $1$: $n = 100$, $\\sigma = 0.1$, maximum knots $= 5$, seed $= 0$ (general case).\n- Case $2$: $n = 50$, $\\sigma = 0$, maximum knots $= 3$, seed $= 1$ (no-noise boundary case).\n- Case $3$: $n = 200$, $\\sigma = 0.3$, maximum knots $= 0$, seed $= 2$ (zero iterations boundary case).\n- Case $4$: $n = 30$, $\\sigma = 0.05$, maximum knots $= 25$, seed $= 3$ (stress test with many potential knots; ensure termination by the $df \\ge n$ rule as needed).\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the pair of lists for one test case in order: \n$[ [\\text{df\\_sequence\\_case1}, \\text{rmse\\_sequence\\_case1}], [\\text{df\\_sequence\\_case2}, \\text{rmse\\_sequence\\_case2}], [\\text{df\\_sequence\\_case3}, \\text{rmse\\_sequence\\_case3}], [\\text{df\\_sequence\\_case4}, \\text{rmse\\_sequence\\_case4}] ]$.\nAll numbers must be standard floating-point or integer literals, and no additional text may be printed.", "solution": "### Problem Validation\n\n**Step 1: Extract Givens**\n\nThe problem provides the following data, definitions, and procedures:\n-   **Data Samples**: $\\{(x_i, y_i)\\}_{i=1}^n$ with $x_i \\in [0,1]$ and $y_i \\in \\mathbb{R}$.\n-   **Model**: A piecewise cubic spline ($m=3$) represented using a truncated power basis. The model function is $\\hat{y}(x) = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3 + \\sum_{j=1}^K \\gamma_j (x - t_j)_+^3$, where $\\{t_j\\}_{j=1}^K$ are the knots and $(u)_+ = \\max(u, 0)$.\n-   **Fitting Objective**: Minimize the sum of squared residuals using Ordinary Least Squares (OLS), i.e., minimize $\\sum_{i=1}^n \\left( y_i - \\hat{y}(x_i) \\right)^2$.\n-   **Metrics**:\n    -   Residuals: $r_i = y_i - \\hat{y}(x_i)$.\n    -   Root Mean Squared Error (RMSE): $\\mathrm{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n r_i^2}$.\n    -   Degrees of Freedom (df): $df = 4 + K$.\n-   **Knot Addition Algorithm**:\n    1.  **Initialize**: Start with $K=0$ knots. Fit the global cubic model.\n    2.  **Iterate**: For $k=1, 2, \\dots$:\n        -   Compute residuals $\\{r_i\\}$.\n        -   Find index $i^\\star$ that maximizes $|r_{i^\\star}|$. Ties are broken by choosing the smallest index.\n        -   Propose a new knot $t_k = x_{i^\\star}$.\n        -   If $t_k$ is already a knot, use the $x_i$ from the next largest $|r_i|$ not yet used.\n        -   Add knot $t_k$, refit the model, and record the new $df$ and $\\mathrm{RMSE}$.\n-   **Termination Conditions**: The algorithm terminates upon the first of these conditions being met:\n    1.  The specified maximum number of knots is reached.\n    2.  Adding a new knot would result in the number of basis functions being greater than or equal to the number of samples, i.e., $4 + (K_{current}+1) \\ge n$.\n    3.  All unique sample locations $x_i$ have been used as knots.\n-   **Data Generation**:\n    -   $x_i \\sim U([0,1])$ for $i=1, \\dots, n$.\n    -   True function: $f(x) = \\sin(2\\pi x) + \\frac{1}{2} x$.\n    -   Observations: $y_i = f(x_i) + \\varepsilon_i$, where $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$.\n    -   Specific random seeds are provided for reproducibility.\n-   **Test Suite**:\n    -   Case 1: $n = 100$, $\\sigma = 0.1$, max knots $= 5$, seed $= 0$.\n    -   Case 2: $n = 50$, $\\sigma = 0$, max knots $= 3$, seed $= 1$.\n    -   Case 3: $n = 200$, $\\sigma = 0.3$, max knots $= 0$, seed $= 2$.\n    -   Case 4: $n = 30$, $\\sigma = 0.05$, max knots $= 25$, seed $= 3$.\n\n**Step 2: Validate Using Extracted Givens**\n\n-   **Scientifically Grounded**: The problem is firmly rooted in the field of statistical learning and non-parametric regression. Regression splines, the truncated power basis, Ordinary Least Squares, and forward stepwise model selection are all standard, well-established concepts. The procedure is a valid, if specific, method for adaptive model fitting.\n-   **Well-Posed**: The problem is well-posed. The data generation process is fully specified. The algorithm is deterministic, including explicit rules for initialization, iteration, tie-breaking in knot selection, and termination. This ensures that for a given test case, a unique sequence of models and corresponding metrics exists. The termination condition $4+K \\ge n$ prevents the linear system in the OLS fit from becoming underdetermined, ensuring a meaningful solution can be found at each step.\n-   **Objective**: The problem is stated in precise, objective mathematical and algorithmic language. There are no subjective claims or ambiguities.\n\nThe problem statement does not exhibit any flaws such as scientific unsoundness, incompleteness, contradiction, or infeasibility. It is a well-defined computational problem in applied mathematics.\n\n**Step 3: Verdict and Action**\n\nThe problem is **valid**. A solution will be provided.\n\n### Solution\n\nThe problem requires the implementation of a forward stepwise algorithm for building a regression spline model. The core idea is to iteratively add knots at locations where the existing model shows the poorest fit, thereby increasing the model's flexibility in a targeted manner.\n\nThe specified model is a cubic spline with $K$ knots, which can be expressed as a linear combination of basis functions:\n$$\n\\hat{y}(x) = \\sum_{j=0}^{3} \\beta_j x^j + \\sum_{j=1}^{K} \\gamma_j (x - t_j)_+^3\n$$\nThis is a linear model in the coefficients $(\\beta_0, \\beta_1, \\beta_2, \\beta_3, \\gamma_1, \\dots, \\gamma_K)$. Given a set of data points $\\{(x_i, y_i)\\}_{i=1}^n$ and a set of knots $\\{t_j\\}_{j=1}^K$, we can construct an $n \\times (4+K)$ design matrix $\\mathbf{X}$. Each row $i$ of $\\mathbf{X}$ corresponds to a data point $x_i$ and each column corresponds to a basis function. Specifically, row $i$ is:\n$$\n\\mathbf{X}_{i,:} = [1, x_i, x_i^2, x_i^3, (x_i - t_1)_+^3, \\dots, (x_i - t_K)_+^3]\n$$\nThe vector of all coefficients is $\\mathbf{b} = [\\beta_0, \\beta_1, \\beta_2, \\beta_3, \\gamma_1, \\dots, \\gamma_K]^T$. The vector of predicted values is then given by $\\hat{\\mathbf{y}} = \\mathbf{X}\\mathbf{b}$.\n\nAccording to the principle of Ordinary Least Squares (OLS), the optimal coefficient vector $\\mathbf{b}$ is the one that minimizes the sum of squared residuals (SSR), defined as $SSR = ||\\mathbf{y} - \\hat{\\mathbf{y}}||_2^2 = ||\\mathbf{y} - \\mathbf{X}\\mathbf{b}||_2^2$. This is a standard linear algebra problem, the solution to which can be found using numerically stable methods like QR decomposition, as implemented in `numpy.linalg.lstsq`.\n\nThe algorithm proceeds as follows:\n\n1.  **Initialization ($k=0$)**: We begin with an empty set of knots, $K=0$. The model is a simple global cubic polynomial $\\hat{y}(x) = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3$. The design matrix $\\mathbf{X}$ has $4$ columns. We solve for the initial coefficients, compute the residuals $r_i = y_i - \\hat{y}(x_i)$, and then calculate the initial degrees of freedom ($df=4$) and RMSE. These initial values form the first entry in our result sequences.\n\n2.  **Iterative Knot Addition (for $k = 1, 2, \\dots$)**:\n    a.  **Termination Checks**: Before adding a new knot, we check two termination conditions. First, if the maximum number of knots has been reached, the process stops. Second, to prevent an underdetermined system, we check if adding another knot would make the number of basis functions ($4 + K_{current} + 1$) greater than or equal to the number of data points $n$. If so, we terminate.\n    b.  **Knot Selection**: The core of the adaptive strategy lies in choosing the next knot. We compute the absolute residuals $|r_i|$ from the current model fit. The location $x_{i^\\star}$ corresponding to the maximum absolute residual is chosen as the new knot candidate. This greedily places the next knot where the model's error is largest. The problem specifies that ties in $|r_i|$ should be broken by selecting the observation with the smallest index $i$. Furthermore, we must ensure the new knot is not a duplicate of an existing knot. We search through the $x_i$ in descending order of their corresponding $|r_i|$ until a unique knot location is found. If all potential locations $x_i$ are already in the knot set, the algorithm terminates.\n    c.  **Model Refitting**: Once a new knot $t_k$ is selected, it is added to the set of knots. The design matrix $\\mathbf{X}$ is augmented with a new column representing the new basis function, $(x_i - t_k)_+^3$. The entire model, with its $4+K$ coefficients, is then refit to the data using OLS.\n    d.  **Recording Metrics**: After refitting, the updated degrees of freedom ($df=4+K$) and the new, presumably lower, RMSE are calculated and appended to their respective sequences.\n\nThis iterative process continues until one of the termination conditions is met. The procedure generates a sequence of models of increasing complexity ($df$) and their corresponding goodness-of-fit (RMSE), providing insight into the trade-off between model complexity and prediction error. For each test case, we will generate the data as specified and execute this algorithm, collecting the `df` and `RMSE` at each step (including the initial $K=0$ step).", "answer": "```python\nimport numpy as np\n\ndef adaptive_knot_placement(n: int, sigma: float, max_knots: int, seed: int) - list:\n    \"\"\"\n    Implements the adaptive knot placement algorithm for regression splines.\n\n    Args:\n        n (int): Number of data points.\n        sigma (float): Standard deviation of the Gaussian noise.\n        max_knots (int): Maximum number of knots to add.\n        seed (int): Random seed for reproducibility.\n\n    Returns:\n        list: A list containing two lists: the sequence of degrees of freedom (df)\n              and the sequence of Root Mean Squared Errors (RMSE).\n    \"\"\"\n    # 1. Data Generation\n    rng = np.random.default_rng(seed)\n    x = rng.uniform(0, 1, n)\n    f_x = np.sin(2 * np.pi * x) + 0.5 * x\n    epsilon = rng.normal(0, sigma, n)\n    y = f_x + epsilon\n\n    # Initialization\n    knots = []\n    df_sequence = []\n    rmse_sequence = []\n\n    # 2. Initial Fit (k=0, no knots)\n    K = 0\n    # Design matrix: columns for 1, x, x^2, x^3\n    X_poly = np.power(x[:, np.newaxis], np.arange(4))\n    X = X_poly\n\n    # Solve OLS\n    coeffs = np.linalg.lstsq(X, y, rcond=None)[0]\n    y_hat = X @ coeffs\n    residuals = y - y_hat\n    rmse = np.sqrt(np.mean(residuals**2))\n    df = 4 + K\n    \n    df_sequence.append(df)\n    rmse_sequence.append(rmse)\n\n    # 3. Iterative Knot Addition\n    for k in range(1, max_knots + 1):\n        # Termination condition: model would become degenerate\n        if 4 + len(knots) + 1 = n:\n            break\n\n        # Find new knot\n        # Sort indices by descending |residual|, with ascending index as tie-breaker\n        sort_keys = (np.arange(n), -np.abs(residuals))\n        sorted_indices = np.lexsort(sort_keys)\n        \n        new_knot = None\n        knot_found = False\n        knot_set = set(knots) # Use a set for efficient checking\n        for idx in sorted_indices:\n            candidate_knot = x[idx]\n            if candidate_knot not in knot_set:\n                new_knot = candidate_knot\n                knot_found = True\n                break\n        \n        # Termination condition: no new unique knots can be added\n        if not knot_found:\n            break\n\n        # Add knot and prepare for refitting\n        knots.append(new_knot)\n        K = len(knots)\n\n        # Re-build design matrix\n        new_basis_col = np.maximum(0, x - new_knot)**3\n        X = np.hstack([X, new_basis_col[:, np.newaxis]])\n\n        # Refit model\n        coeffs = np.linalg.lstsq(X, y, rcond=None)[0]\n        y_hat = X @ coeffs\n        residuals = y - y_hat\n        rmse = np.sqrt(np.mean(residuals**2))\n        df = 4 + K\n\n        df_sequence.append(df)\n        rmse_sequence.append(rmse)\n        \n    return [df_sequence, rmse_sequence]\n\ndef solve():\n    \"\"\"\n    Runs the test suite and prints the final results in the specified format.\n    \"\"\"\n    test_cases = [\n        # (n, sigma, max_knots, seed)\n        (100, 0.1, 5, 0),\n        (50, 0.0, 3, 1),\n        (200, 0.3, 0, 2),\n        (30, 0.05, 25, 3),\n    ]\n\n    all_results = []\n    for case in test_cases:\n        n, sigma, max_knots, seed = case\n        result = adaptive_knot_placement(n, sigma, max_knots, seed)\n        all_results.append(result)\n\n    # Format the final output string to be a compact list representation\n    # e.g., [[[df1s],[rmse1s]],[[df2s],[rmse2s]]]\n    # repr() gives a standard representation, and .replace(\" \", \"\") removes whitespace.\n    output_str = repr(all_results).replace(\" \", \"\")\n    print(output_str)\n\nsolve()\n\n```", "id": "3157197"}]}