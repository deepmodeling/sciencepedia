{"hands_on_practices": [{"introduction": "Understanding Type I and Type II errors goes beyond memorizing definitions; it's about recognizing their real-world consequences. This first practice places you in the role of a data scientist checking the assumptions of a linear regression model. By analyzing the potential outcomes of a normality test on model residuals, you will gain a concrete understanding of what these statistical errors represent in a common data analysis workflow. [@problem_id:3130871]", "problem": "A researcher fits a linear regression to a dataset with $n=60$ observations and obtains residuals $\\{e_i\\}_{i=1}^{60}$. To assess whether the residuals are approximately Gaussian, the researcher plans to apply the Shapiro–Wilk normality test to the residuals. The null hypothesis is $H_0$: “the residuals are sampled from a Gaussian distribution,” and the alternative hypothesis is $H_1$: “the residuals are not Gaussian.” The researcher will use a significance level $\\alpha=0.05$ and the following decision rule: reject $H_0$ if the test’s $p$-value is at most $\\alpha$. If $H_0$ is rejected, the researcher will adjust the model (for example, by applying a variance-stabilizing transformation to the response or by using inference methods that are less sensitive to non-Gaussian errors); if $H_0$ is not rejected, the researcher will proceed with ordinary least squares (OLS) inference as initially specified. Suppose the procedure outputs a $p$-value $p=0.03$ on the current dataset. Consider also the possibility that the true residual distribution is mildly right-skewed (a small deviation from Gaussianity).\n\nWhich of the following statements are correct?\n\nA. Using $\\alpha=0.05$, a Type I error is rejecting $H_0$ when it is true; in this context, that corresponds to performing an unnecessary model adjustment when the residuals are in fact Gaussian.\n\nB. If the true residuals have mild right-skewness (so $H_0$ is false) but the test fails to reject $H_0$ at $\\alpha=0.05$, that is a Type II error; in this context, it risks overstating the robustness of OLS inference to non-Gaussianity.\n\nC. Observing $p=0.03$ means there is a $0.03$ probability that $H_0$ is true for this dataset.\n\nD. Under $H_0$ for a valid continuous test, the distribution of the $p$-value is uniform on $[0,1]$; therefore, if $H_0$ is true, the probability that the $p$-value is at most $0.12$ is $0.12$.\n\nE. Lowering the significance level from $\\alpha=0.05$ to $\\alpha=0.01$ simultaneously reduces both the probabilities of Type I and Type II errors for the same test and sample size.", "solution": "The validity of the problem statement must first be assessed.\n\n### Step 1: Extract Givens\n-   **Sample Size**: $n=60$ observations.\n-   **Model**: Linear regression.\n-   **Data for test**: Residuals $\\{e_i\\}_{i=1}^{60}$.\n-   **Test**: Shapiro–Wilk normality test.\n-   **Null Hypothesis ($H_0$)**: “the residuals are sampled from a Gaussian distribution.”\n-   **Alternative Hypothesis ($H_1$)**: “the residuals are not Gaussian.”\n-   **Significance Level**: $\\alpha=0.05$.\n-   **Decision Rule**: Reject $H_0$ if the $p$-value is at most $\\alpha$.\n-   **Action on Rejection**: Adjust the model.\n-   **Action on Non-rejection**: Proceed with ordinary least squares (OLS) inference.\n-   **Observed Data**: A specific instance yields a $p$-value of $p=0.03$.\n-   **Hypothetical Scenario**: The true residual distribution is mildly right-skewed.\n\n### Step 2: Validate Using Extracted Givens\nThe problem describes a standard scenario in applied statistics: assessing the assumptions of a linear regression model using hypothesis testing on a model-fitting diagnostic (the residuals). The concepts presented—null/alternative hypotheses, Type I and Type II errors, $p$-values, significance levels, and the Shapiro-Wilk test—are all fundamental and well-defined in statistics. The setup is self-contained and internally consistent.\n\nA subtle point is that residuals from OLS are not strictly independent and identically distributed (i.i.d.), even if the true model errors are. They possess a covariance structure, $\\mathrm{Cov}(\\boldsymbol{e}) = \\sigma^2(\\boldsymbol{I} - \\boldsymbol{H})$, where $\\boldsymbol{H}$ is the hat matrix. Standard normality tests like Shapiro-Wilk assume an i.i.d. sample. However, applying such tests to residuals is a very common, albeit approximate, practice in regression diagnostics. The problem statement itself is not invalidated by this nuance; rather, this subtlety may be relevant to evaluating the options. The phrasing \"assess whether the residuals are *approximately* Gaussian\" shows awareness of this practical context.\n\nThe problem is scientifically grounded, well-posed, objective, and does not violate any of the criteria for invalidity.\n\n### Step 3: Verdict and Action\nThe problem is valid. I will now proceed with the analysis of each statement.\n\n### Analysis of the Statements\n\nThe core principles of frequentist hypothesis testing are central here.\n-   A **Type I error** is the rejection of a true null hypothesis ($H_0$). The probability of a Type I error is controlled by the significance level, $P(\\text{Type I error}) = \\alpha$.\n-   A **Type II error** is the failure to reject a false null hypothesis ($H_0$). The probability of a Type II error is denoted by $\\beta$.\n-   The **$p$-value** is the probability, assuming $H_0$ is true, of observing a test statistic at least as extreme as the one computed from the sample data. It is $P(\\text{data as extreme or more extreme} | H_0)$.\n\n**A. Using $\\alpha=0.05$, a Type I error is rejecting $H_0$ when it is true; in this context, that corresponds to performing an unnecessary model adjustment when the residuals are in fact Gaussian.**\n\nThis statement first provides the correct definition of a Type I error: rejecting $H_0$ when it is true. It then applies this definition to the specific context of the problem. Here, $H_0$ is the hypothesis that the residuals are Gaussian. The consequence of rejecting $H_0$ is to \"adjust the model.\" Therefore, if $H_0$ were true (residuals are Gaussian) and we rejected it (a Type I error), we would perform a model adjustment that is unnecessary because the initial model's error assumption was correct. The statement's definition and contextual application are both accurate.\n\nVerdict: **Correct**.\n\n**B. If the true residuals have mild right-skewness (so $H_0$ is false) but the test fails to reject $H_0$ at $\\alpha=0.05$, that is a Type II error; in this context, it risks overstating the robustness of OLS inference to non-Gaussianity.**\n\nThis statement considers a scenario where $H_0$ is false because the true distribution is skewed. It correctly identifies the failure to reject a false $H_0$ as a Type II error. The consequence of not rejecting $H_0$ is to \"proceed with ordinary least squares (OLS) inference.\" Standard OLS inference, such as t-tests and confidence intervals for coefficients, relies on the assumption of normally distributed errors for finite-sample validity. If this assumption is violated, the inference may be unreliable. By failing to detect the non-normality and proceeding with standard inference, the researcher acts as if the method is robust to this violation. This introduces a genuine risk that the conclusions drawn from the inference are incorrect. The phrasing \"risks overstating the robustness\" is a precise and accurate description of the practical danger of a Type II error in this diagnostic context.\n\nVerdict: **Correct**.\n\n**C. Observing $p=0.03$ means there is a $0.03$ probability that $H_0$ is true for this dataset.**\n\nThis is a common and fundamental misinterpretation of a $p$-value. The $p$-value is the probability of the observed data (or more extreme data) given that the null hypothesis is true, i.e., $P(\\text{data}|H_0)$. The statement claims the $p$-value is the probability of the null hypothesis given the data, i.e., $P(H_0|\\text{data})$. This latter quantity is a Bayesian concept (a posterior probability), which cannot be calculated without specifying a prior probability for $H_0$. The frequentist $p$-value does not provide the probability of the hypothesis being true.\n\nVerdict: **Incorrect**.\n\n**D. Under $H_0$ for a valid continuous test, the distribution of the $p$-value is uniform on $[0,1]$; therefore, if $H_0$ is true, the probability that the $p$-value is at most $0.12$ is $0.12$.**\n\nThis statement describes a fundamental property of $p$-values. For any hypothesis test where the test statistic has a continuous distribution under the null hypothesis $H_0$, the distribution of the $p$-value random variable is the uniform distribution on the interval $[0,1]$. If a random variable $P$ follows a $U(0,1)$ distribution, its cumulative distribution function is $F_P(c) = P(P \\le c) = c$ for any $c \\in [0,1]$. Thus, the deduction that $P(p\\text{-value} \\le 0.12) = 0.12$ is mathematically sound. The entire statement is a correct theorem from mathematical statistics.\n\nVerdict: **Correct**.\n\n**E. Lowering the significance level from $\\alpha=0.05$ to $\\alpha=0.01$ simultaneously reduces both the probabilities of Type I and Type II errors for the same test and sample size.**\n\nThe probability of a Type I error, $P(\\text{Type I error})$, is equal to the significance level $\\alpha$. Lowering $\\alpha$ from $0.05$ to $0.01$ directly reduces the probability of a Type I error. However, lowering $\\alpha$ makes the criterion for rejecting $H_0$ more stringent (the rejection region becomes smaller). This makes it harder to reject $H_0$. Consequently, if $H_0$ is false, the probability of failing to reject it increases. This means the probability of a Type II error, $\\beta$, increases. For a fixed sample size and test, there is an inherent trade-off between $\\alpha$ and $\\beta$. It is not possible to reduce both simultaneously by only changing the significance level.\n\nVerdict: **Incorrect**.\n\nRevisiting the correct options A, B, and D. All three are correct statements regarding the principles of hypothesis testing. Statement A correctly defines a Type I error in context. Statement B correctly defines a Type II error in context. Statement D correctly states a fundamental theorem about the null distribution of p-values. There are no inconsistencies or inaccuracies in these three statements.", "answer": "$$\\boxed{ABD}$$", "id": "3130871"}, {"introduction": "Moving from interpretation to design, we now use our understanding of $\\alpha$ and $\\beta$ to plan an experiment. A fundamental question in any study is, \"How much data do we need?\" This practice guides you through the process of power analysis to determine the minimum sample size required to reliably detect an effect of a certain magnitude, a crucial skill for designing efficient and effective experiments. [@problem_id:3130913]", "problem": "A security operations team monitors system logs, counting the number of anomaly flags per fixed time window. It is scientifically reasonable to model the count per window under a constant-rate regime as independent draws from a Poisson distribution with rate parameter $\\lambda$ per window. The team wishes to test whether the anomaly rate has increased relative to a known baseline $\\lambda_0$.\n\nFormulate a one-sided hypothesis test with null hypothesis $H_0\\!:\\ \\lambda=\\lambda_0$ versus alternative $H_1\\!:\\ \\lambda=\\lambda_1$ with $\\lambda_1=\\lambda_0(1+\\delta)$ for a small $\\delta0$. Let $Y_1,\\dots,Y_n$ be the counts observed over $n$ independent windows, and define the total count $X=\\sum_{i=1}^{n} Y_i$. The test will reject $H_0$ when $X$ exceeds a threshold $c$ chosen to control the Type I error rate $\\alpha$ (the probability of rejecting $H_0$ when $H_0$ is true). The team plans to later validate the achieved $\\alpha$ by simulating draws from the Poisson model under $H_0$, but for planning purposes they need an analytical expression for the sample size.\n\nStarting from the core definitions of Type I error $\\alpha$ and Type II error $\\beta$ (the probability of failing to reject $H_0$ when $H_1$ is true), and using only well-tested facts about the Poisson distribution (mean and variance equal to its rate) together with the Central Limit Theorem (CLT) approximation that $X$ is approximately normal for large $n$ with mean $n\\lambda$ and variance $n\\lambda$, derive an expression for the minimal integer sample size $n$ that achieves a Type I error rate $\\alpha$ and a Type II error rate $\\beta$ for detecting the increase from $\\lambda_0$ to $\\lambda_1=\\lambda_0(1+\\delta)$.\n\nThen, evaluate this expression numerically for the planning parameters $\\lambda_0=5$, $\\delta=0.1$, $\\alpha=0.05$, and $\\beta=0.2$, and report the minimal integer $n$ that meets the criteria. Do not use any continuity correction. The answer must be a single integer. No rounding specification is needed beyond taking the minimal integer.", "solution": "The problem requires the derivation of the minimal sample size $n$ for a one-sided hypothesis test comparing two Poisson rates, followed by a numerical evaluation. The derivation will be based on the definitions of Type I and Type II errors, using the Central Limit Theorem (CLT) for approximation.\n\nFirst, let's formalize the problem setup. We are given $n$ independent and identically distributed random variables $Y_1, \\dots, Y_n$, where $Y_i \\sim \\text{Poisson}(\\lambda)$. The test statistic is their sum, $X = \\sum_{i=1}^{n} Y_i$. A property of the Poisson distribution is that the sum of independent Poisson variables is also a Poisson variable. Thus, $X \\sim \\text{Poisson}(n\\lambda)$.\n\nThe hypothesis test is:\nNull hypothesis $H_0: \\lambda = \\lambda_0$\nAlternative hypothesis $H_1: \\lambda = \\lambda_1$, where $\\lambda_1 = \\lambda_0(1+\\delta)$ and $\\delta  0$.\n\nThe problem states to use the CLT approximation. For a large sum $n$, the distribution of $X$ can be approximated by a normal distribution.\nUnder $H_0$, $X$ has mean $E[X|H_0] = n\\lambda_0$ and variance $\\text{Var}(X|H_0) = n\\lambda_0$. So, $X \\stackrel{\\text{approx}}{\\sim} \\mathcal{N}(n\\lambda_0, n\\lambda_0)$.\nUnder $H_1$, $X$ has mean $E[X|H_1] = n\\lambda_1$ and variance $\\text{Var}(X|H_1) = n\\lambda_1$. So, $X \\stackrel{\\text{approx}}{\\sim} \\mathcal{N}(n\\lambda_1, n\\lambda_1)$.\n\nThe decision rule is to reject $H_0$ if $X  c$, where $c$ is a critical value.\n\nThe Type I error rate, $\\alpha$, is the probability of rejecting $H_0$ when it is true.\n$$ \\alpha = P(\\text{reject } H_0 | H_0) = P(X  c | \\lambda = \\lambda_0) $$\nUsing the normal approximation for $X$ under $H_0$, we standardize the variable:\n$$ \\alpha = P\\left( \\frac{X - n\\lambda_0}{\\sqrt{n\\lambda_0}}  \\frac{c - n\\lambda_0}{\\sqrt{n\\lambda_0}} \\right) $$\nLet $Z$ be a standard normal random variable, $Z \\sim \\mathcal{N}(0,1)$. Let $z_\\alpha$ be the upper $\\alpha$-quantile of the standard normal distribution, defined such that $P(Z  z_\\alpha) = \\alpha$. We then have:\n$$ \\frac{c - n\\lambda_0}{\\sqrt{n\\lambda_0}} = z_\\alpha $$\nSolving for the critical value $c$, we get our first expression:\n$$ c = n\\lambda_0 + z_\\alpha \\sqrt{n\\lambda_0} \\quad (1) $$\n\nThe Type II error rate, $\\beta$, is the probability of failing to reject $H_0$ when $H_1$ is true.\n$$ \\beta = P(\\text{fail to reject } H_0 | H_1) = P(X \\le c | \\lambda = \\lambda_1) $$\nUsing the normal approximation for $X$ under $H_1$ (and ignoring the continuity correction as per the instructions, so $P(X \\le c) \\approx P(X  c)$), we standardize the variable:\n$$ \\beta = P\\left( \\frac{X - n\\lambda_1}{\\sqrt{n\\lambda_1}} \\le \\frac{c - n\\lambda_1}{\\sqrt{n\\lambda_1}} \\right) $$\nLet $z_\\beta$ be the upper $\\beta$-quantile of the standard normal distribution, $P(Z  z_\\beta) = \\beta$. Due to the symmetry of the normal distribution, $P(Z \\le -z_\\beta) = \\beta$. Thus:\n$$ \\frac{c - n\\lambda_1}{\\sqrt{n\\lambda_1}} = -z_\\beta $$\nSolving for the critical value $c$, we get our second expression:\n$$ c = n\\lambda_1 - z_\\beta \\sqrt{n\\lambda_1} \\quad (2) $$\n\nNow, we equate the two expressions for $c$ from equations $(1)$ and $(2)$ to find the required sample size $n$.\n$$ n\\lambda_0 + z_\\alpha \\sqrt{n\\lambda_0} = n\\lambda_1 - z_\\beta \\sqrt{n\\lambda_1} $$\nRearranging the terms to solve for $n$:\n$$ n\\lambda_1 - n\\lambda_0 = z_\\alpha \\sqrt{n\\lambda_0} + z_\\beta \\sqrt{n\\lambda_1} $$\n$$ n(\\lambda_1 - \\lambda_0) = \\sqrt{n} (z_\\alpha \\sqrt{\\lambda_0} + z_\\beta \\sqrt{\\lambda_1}) $$\nAssuming $n  0$, we can divide by $\\sqrt{n}$:\n$$ \\sqrt{n}(\\lambda_1 - \\lambda_0) = z_\\alpha \\sqrt{\\lambda_0} + z_\\beta \\sqrt{\\lambda_1} $$\nIsolating $\\sqrt{n}$:\n$$ \\sqrt{n} = \\frac{z_\\alpha \\sqrt{\\lambda_0} + z_\\beta \\sqrt{\\lambda_1}}{\\lambda_1 - \\lambda_0} $$\nSquaring both sides gives the expression for $n$:\n$$ n = \\left( \\frac{z_\\alpha \\sqrt{\\lambda_0} + z_\\beta \\sqrt{\\lambda_1}}{\\lambda_1 - \\lambda_0} \\right)^2 $$\nWe are given $\\lambda_1 = \\lambda_0(1+\\delta)$, so $\\lambda_1 - \\lambda_0 = \\lambda_0 \\delta$. Substituting this into the formula:\n$$ n = \\frac{(z_\\alpha \\sqrt{\\lambda_0} + z_\\beta \\sqrt{\\lambda_0(1+\\delta)})^2}{(\\lambda_0 \\delta)^2} = \\frac{(\\sqrt{\\lambda_0}(z_\\alpha + z_\\beta \\sqrt{1+\\delta}))^2}{\\lambda_0^2 \\delta^2} $$\n$$ n = \\frac{\\lambda_0 (z_\\alpha + z_\\beta \\sqrt{1+\\delta})^2}{\\lambda_0^2 \\delta^2} = \\frac{(z_\\alpha + z_\\beta \\sqrt{1+\\delta})^2}{\\lambda_0 \\delta^2} $$\nThis is the general analytical expression for the sample size $n$.\n\nNext, we evaluate this expression for the given parameters: $\\lambda_0=5$, $\\delta=0.1$, $\\alpha=0.05$, and $\\beta=0.2$.\nFirst, we find the corresponding $z$-scores for the standard normal distribution.\nFor a Type I error rate $\\alpha=0.05$, $z_{0.05}$ is the value such that $P(Z  z_{0.05}) = 0.05$. This value is $z_{0.05} \\approx 1.64485$.\nFor a Type II error rate $\\beta=0.2$, $z_{0.2}$ is the value such that $P(Z  z_{0.2}) = 0.2$. This value is $z_{0.2} \\approx 0.84162$.\n\nNow, we substitute these values into the derived formula for $n$:\n$$ n = \\frac{(1.64485 + 0.84162 \\sqrt{1+0.1})^2}{5 \\times (0.1)^2} $$\n$$ n = \\frac{(1.64485 + 0.84162 \\sqrt{1.1})^2}{5 \\times 0.01} $$\nLet's compute the terms:\n$$ \\sqrt{1.1} \\approx 1.048809 $$\n$$ n \\approx \\frac{(1.64485 + 0.84162 \\times 1.048809)^2}{0.05} $$\n$$ n \\approx \\frac{(1.64485 + 0.88277)^2}{0.05} $$\n$$ n \\approx \\frac{(2.52762)^2}{0.05} $$\n$$ n \\approx \\frac{6.38886}{0.05} $$\n$$ n \\approx 127.777 $$\nSince the sample size $n$ must be an integer, and the calculated value is the minimum required to achieve the desired error rates, we must take the ceiling of this value. A sample size of $127$ would result in slightly higher error rates than specified. Therefore, the minimal integer sample size is the next integer greater than the calculated value.\n$$ n_{\\text{min}} = \\lceil 127.777 \\rceil = 128 $$", "answer": "$$\\boxed{128}$$", "id": "3130913"}, {"introduction": "The principles of hypothesis testing are essential for critically evaluating the output of complex machine learning algorithms. This final practice explores the popular technique of permutation-based feature importance in Random Forests. You will investigate how correlations between predictors can distort the results of this test, leading to misleading conclusions about which features are truly important. [@problem_id:3130912]", "problem": "A data scientist uses Random Forest to predict a response $Y$ from predictors $(X_1,\\dots,X_p)$ with $n$ independent and identically distributed observations. To assess the importance of a particular predictor $X_j$, they use a permutation-based test statistic $T_j$, defined as the increase in out-of-bag predictive loss when $X_j$ is permuted across the out-of-bag samples while the trained forest is kept fixed. They consider the one-sided hypothesis test with null hypothesis $H_0$: permuting $X_j$ has no effect on the expected generalization loss, versus $H_1$: permuting $X_j$ increases expected loss. The test is carried out at significance level $\\alpha$, and the decision rule rejects $H_0$ when $T_j$ is sufficiently large relative to its reference distribution. Type I error is the probability of rejecting $H_0$ when $H_0$ is true, denoted by $\\alpha$, and Type II error is the probability of failing to reject $H_0$ when $H_1$ is true, denoted by $\\beta$.\n\nSuppose now that there is a second predictor $X_k$ that is strongly correlated with $X_j$ (for instance, $\\operatorname{corr}(X_j,X_k)$ is close to $1$ in magnitude), and the Random Forest has had the opportunity to split on either $X_j$ or $X_k$ during training. The scientist is concerned with how correlation among predictors affects the validity of the permutation test and the rates $\\alpha$ and $\\beta$. They also consider a conditional permutation scheme in which $X_j$ is permuted conditionally on $X_k$ (for example, by shuffling $X_j$ within narrow bins of $X_k$, or by sampling $X_j$ from an estimate of the conditional distribution $P(X_j \\mid X_k)$) so that the dependence between $X_j$ and $X_k$ is preserved under the permutation.\n\nWhich of the following statements are correct? Select all that apply.\n\nA. If $X_j$ is independent of $Y$ but strongly correlated with a truly predictive $X_k$, then under the standard (unconditional) permutation of $X_j$ the test maintains the nominal Type I error $\\alpha$, because the permutation leaves the joint distribution of $(X_j,X_k,Y)$ unchanged under $H_0$.\n\nB. If $X_j$ is independent of $Y$ but strongly correlated with a truly predictive $X_k$, then under the standard (unconditional) permutation of $X_j$ the test can have inflated Type I error, with $\\mathbb{P}(\\text{reject }H_0 \\mid H_0)  \\alpha$, because the trained forest may use $X_j$ as a proxy for $X_k$ and the permutation breaks that proxy relationship.\n\nC. If $X_j$ truly affects $Y$ but is strongly correlated with another predictor $X_k$ that also carries the same signal, then the standard (unconditional) permutation of $X_j$ can reduce power, increasing $\\beta$, because the remaining correlated predictor compensates for the permuted one, leading to a small observed $T_j$.\n\nD. A conditional permutation scheme that permutes $X_j$ conditionally on $X_k$ aims to preserve the dependence structure between $X_j$ and $X_k$ under $H_0$ and thereby restores control of the Type I error at the nominal level $\\alpha$ (at least asymptotically under regularity conditions).\n\nE. Conditional permutation simultaneously guarantees maximal power and control of Type I error for any sample size and any dependency structure among predictors; thus, relative to standard permutation, it strictly decreases $\\beta$ in all cases while holding $\\alpha$ fixed at the nominal level.", "solution": "The user wants me to validate the problem statement and, if valid, solve the problem by evaluating each option.\n\n### Step 1: Extract Givens\n-   **Model:** Random Forest predicting a response $Y$ from predictors $(X_1, \\dots, X_p)$.\n-   **Data:** $n$ independent and identically distributed observations.\n-   **Test Statistic:** $T_j$, the increase in out-of-bag (OOB) predictive loss when predictor $X_j$ is permuted.\n-   **Hypotheses:**\n    -   Null Hypothesis $H_0$: Permuting $X_j$ has no effect on the expected generalization loss.\n    -   Alternative Hypothesis $H_1$: Permuting $X_j$ increases expected loss.\n-   **Significance Level:** $\\alpha$.\n-   **Decision Rule:** Reject $H_0$ if $T_j$ is sufficiently large.\n-   **Error Types:**\n    -   Type I Error: $\\mathbb{P}(\\text{reject } H_0 \\mid H_0 \\text{ is true}) = \\alpha$.\n    -   Type II Error: $\\mathbb{P}(\\text{fail to reject } H_0 \\mid H_1 \\text{ is true}) = \\beta$.\n-   **Scenario:** A predictor $X_k$ is strongly correlated with $X_j$ (e.g., $|\\operatorname{corr}(X_j,X_k)| \\approx 1$).\n-   **Alternative Test:** A conditional permutation scheme where $X_j$ is permuted conditionally on $X_k$ to preserve their dependence.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is evaluated against the validation criteria.\n\n-   **Scientifically Grounded:** The problem describes permutation-based feature importance for Random Forests, a standard technique introduced by Leo Breiman. The issues raised—the effect of multicollinearity on Type I and Type II errors of this test—are well-documented and represent a central challenge in machine learning interpretability. The concept of conditional permutation is also a recognized approach proposed in the statistical literature (e.g., by Strobl et al.) to address these very issues. The problem is scientifically sound.\n-   **Well-Posed:** The problem asks to evaluate several statements regarding the statistical properties (Type I and Type II errors) of a specific hypothesis test under specific conditions (correlated predictors). The question is structured to have a definite set of correct answers based on established statistical principles.\n-   **Objective:** The language is technical, precise, and free from subjective or ambiguous terminology.\n-   **Completeness:** The problem provides all necessary conceptual information to reason about the qualitative behavior of the error rates $\\alpha$ and $\\beta$. No specific numerical values are needed.\n-   **Consistency:** The provided definitions and scenario are internally consistent.\n\n### Step 3: Verdict and Action\nThe problem statement is **valid**. It is a well-formulated question about a non-trivial topic in statistical learning. I will now proceed to the solution and option-by-option analysis.\n\n***\n\n### Principle-Based Derivation\n\nThe core of this problem lies in understanding what a standard permutation importance test actually measures, particularly in the presence of correlated predictors.\n\nThe standard procedure for permutation feature importance for a predictor $X_j$ involves the following steps:\n1.  Train a Random Forest model on the training data.\n2.  Calculate the OOB prediction error.\n3.  Randomly permute the values of the predictor $X_j$ in the OOB samples. This breaks the association between $X_j$ and the response $Y$. Crucially, it also breaks the association between $X_j$ and all other predictors $X_k$ (for $k \\neq j$).\n4.  Calculate the OOB prediction error on this permuted dataset.\n5.  The test statistic $T_j$ is the increase in error from step 2 to step 4.\n\nThe null hypothesis that this procedure tests is that the predictor $X_j$ has no association with the response $Y$. If $Y$ is independent of $X_j$, then permuting $X_j$ should not, on average, change the prediction error.\n\nThe complication arises when predictors are correlated. Let's consider two correlated predictors, $X_j$ and $X_k$.\n\n-   **Impact on Type I Error:** Suppose $X_j$ is a \"null\" predictor (it has no causal effect on $Y$ and provides no information about $Y$ that isn't already in other predictors), but it is strongly correlated with a truly predictive variable $X_k$. Formally, this can be stated as $Y$ being conditionally independent of $X_j$ given the other predictors: $Y \\perp X_j \\mid X_{(-j)}$. In this case, the null hypothesis of \"no *additional* predictive value\" is true. However, due to the correlation, $X_j$ is not marginally independent of $Y$. When we unconditionally permute $X_j$, we break its correlation with $X_k$. The model, which was trained on data where $X_j$ and $X_k$ were related, now receives OOB data points where the $(X_j, X_k)$ pair is highly unlikely under the true data-generating distribution. This can lead to poor predictions and a large increase in error, $T_j$. Consequently, the test will incorrectly reject $H_0$ and flag $X_j$ as important, leading to an inflated Type I error rate.\n\n-   **Impact on Type II Error (Power):** Suppose both $X_j$ and $X_k$ are truly predictive and are also strongly correlated. They carry redundant information. The Random Forest algorithm, which selects predictors greedily at each split, might choose $X_j$ for some trees and $X_k$ for others. The total \"importance\" of the signal carried by these two predictors is thus diluted or split between them. When we permute $X_j$ to measure its importance, the information from $X_j$ is lost. However, the model can still rely on the highly correlated predictor $X_k$ to make its predictions. Because $X_k$ compensates for the loss of $X_j$, the increase in prediction error, $T_j$, will be smaller than it would have been if $X_j$ were the sole carrier of its information. A smaller value of $T_j$ makes it less likely to exceed the rejection threshold. Therefore, the test is less likely to detect that $X_j$ is truly important, which means the test has lower power, or equivalently, an increased Type II error rate, $\\beta$.\n\n-   **Conditional Permutation:** This technique is designed to specifically address the Type I error inflation. By permuting the values of $X_j$ only within groups of observations that have similar values of the other predictors (e.g., $X_k$), the scheme attempts to preserve the correlation structure of the data. The test then measures the impact of breaking the link between $X_j$ and $Y$ *while holding the information from its correlated peers constant*. This more accurately tests the conditional independence hypothesis $Y \\perp X_j \\mid X_{(-j)}$ and, as a result, can bring the Type I error rate back to its nominal level $\\alpha$.\n\n### Option-by-Option Analysis\n\n**A. If $X_j$ is independent of $Y$ but strongly correlated with a truly predictive $X_k$, then under the standard (unconditional) permutation of $X_j$ the test maintains the nominal Type I error $\\alpha$, because the permutation leaves the joint distribution of $(X_j,X_k,Y)$ unchanged under $H_0$.**\n\nThe premise \"independent of $Y$\" is best interpreted in the conditional sense ($Y \\perp X_j \\mid X_k$) for $X_j$ to be a null variable in this context. The reasoning in the statement is flawed. The unconditional permutation of $X_j$ does *not* leave the joint distribution of $(X_j, X_k, Y)$ unchanged. It specifically changes the joint distribution of the predictors by making $X_j$ and $X_k$ independent, i.e., it replaces $P(X_j, X_k)$ with $P(X_j)P(X_k)$. Since they are correlated, $P(X_j, X_k) \\neq P(X_j)P(X_k)$. This creates unrealistic data points for the model, which was trained on the original correlated structure. The model's error increases, leading to a large $T_j$ and a rejection of $H_0$. This means the Type I error is inflated, not maintained at $\\alpha$.\n**Verdict: Incorrect.**\n\n**B. If $X_j$ is independent of $Y$ but strongly correlated with a truly predictive $X_k$, then under the standard (unconditional) permutation of $X_j$ the test can have inflated Type I error, with $\\mathbb{P}(\\text{reject }H_0 \\mid H_0)  \\alpha$, because the trained forest may use $X_j$ as a proxy for $X_k$ and the permutation breaks that proxy relationship.**\n\nThis statement accurately describes the mechanism for Type I error inflation. The Random Forest may learn to use $X_j$ as a proxy for the truly predictive $X_k$ because of their strong correlation. The unconditional permutation severs this proxy relationship by decorrelating $X_j$ from $X_k$. The model now sees feature combinations it was not trained on (e.g., a value of $X_j$ that \"disagrees\" with the value of $X_k$), causing its predictive performance to degrade significantly. This results in a large test statistic $T_j$, leading to a frequent rejection of the null hypothesis even when $X_j$ adds no information beyond $X_k$. Thus, the Type I error rate exceeds the nominal level $\\alpha$.\n**Verdict: Correct.**\n\n**C. If $X_j$ truly affects $Y$ but is strongly correlated with another predictor $X_k$ that also carries the same signal, then the standard (unconditional) permutation of $X_j$ can reduce power, increasing $\\beta$, because the remaining correlated predictor compensates for the permuted one, leading to a small observed $T_j$.**\n\nThis statement correctly identifies the problem of power loss. When two predictors are redundant, the model's reliance on any single one of them is diminished. The importance is \"shared\". When $X_j$ is permuted, the model can still use $X_k$, which contains similar information. This compensation by $X_k$ means that the drop in performance (and thus the value of $T_j$) is less pronounced than it would be if $X_k$ were not in the model. A smaller $T_j$ means the test is less likely to correctly identify $X_j$ as important, thus reducing the test's power ($1-\\beta$) and increasing the Type II error rate $\\beta$.\n**Verdict: Correct.**\n\n**D. A conditional permutation scheme that permutes $X_j$ conditionally on $X_k$ aims to preserve the dependence structure between $X_j$ and $X_k$ under $H_0$ and thereby restores control of the Type I error at the nominal level $\\alpha$ (at least asymptotically under regularity conditions).**\n\nThis statement correctly describes the purpose and effect of conditional permutation. By shuffling $X_j$ only among data points with similar values of $X_k$, the correlation between them is maintained. The test is no longer affected by the creation of \"unrealistic\" data points. Instead, it isolates the unique contribution of $X_j$ after accounting for $X_k$. This procedure is specifically designed to test for conditional importance and remedy the Type I error inflation of the standard permutation test. The qualification \"at least asymptotically under regularity conditions\" is appropriate, as finite-sample performance depends on the quality of the conditioning (e.g., bin sizes or the accuracy of estimated conditional distributions).\n**Verdict: Correct.**\n\n**E. Conditional permutation simultaneously guarantees maximal power and control of Type I error for any sample size and any dependency structure among predictors; thus, relative to standard permutation, it strictly decreases $\\beta$ in all cases while holding $\\alpha$ fixed at the nominal level.**\n\nThis statement is far too strong and absolute. Statistical methods rarely, if ever, provide such universal guarantees.\n1.  **\"Guarantees... for any sample size\"**: Control of Type I error and power properties are typically asymptotic. In finite samples, the effectiveness of conditioning is imperfect.\n2.  **\"Maximal power\"**: There is no guarantee of maximal power. In fact, by testing a more specific, conditional hypothesis, the test might have less power to detect a marginal effect if that is what is of interest.\n3.  **\"Strictly decreases $\\beta$ in all cases\"**: This is false. A procedure that reduces inflated Type I error often involves a trade-off with power. It is not guaranteed that conditional permutation will be more powerful than standard permutation in all situations. For example, in a setting with no correlation, the added complexity of conditioning might introduce noise and slightly reduce power compared to the simpler, and in that case valid, standard permutation test. The claim is an overstatement of the method's benefits.\n**Verdict: Incorrect.**", "answer": "$$\\boxed{BCD}$$", "id": "3130912"}]}