{"hands_on_practices": [{"introduction": "In the world of statistical estimation, a common misconception is that an unbiased estimator is always superior. This exercise challenges that notion by exploring the fundamental bias-variance trade-off. By working through the mathematics of ridge regression [@problem_id:3155654], you will see how intentionally introducing a small amount of bias can dramatically reduce an estimator's variance, leading to a more accurate estimate overall as measured by the expected squared error.", "problem": "Consider a supervised learning linear model with a fixed design matrix $X \\in \\mathbb{R}^{3 \\times 2}$, an unknown parameter vector $\\beta \\in \\mathbb{R}^{2}$, and a response vector $Y \\in \\mathbb{R}^{3}$ satisfying\n$$\nY = X \\beta + \\varepsilon,\n$$\nwhere $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{3})$ and $I_{3}$ denotes the $3 \\times 3$ identity matrix. The parameter of interest is the mean response vector $\\mu \\in \\mathbb{R}^{3}$ defined by $\\mu = X \\beta$. Consider the ridge shrinkage estimator for $\\beta$,\n$$\n\\hat{\\beta}_{\\lambda} = (X^{\\top} X + \\lambda I_{2})^{-1} X^{\\top} Y,\n$$\nwith shrinkage parameter $\\lambda \\geq 0$, and the induced estimator for the mean response\n$$\n\\hat{\\mu}_{\\lambda} = X \\hat{\\beta}_{\\lambda}.\n$$\n\nWork from first principles using definitions of expectation, variance, and the linear model stated above. Do not assume any results beyond those definitions. Your tasks are:\n1. Derive the bias vector of $\\hat{\\mu}_{\\lambda}$ with respect to $\\mu$ as a function of $\\lambda$ and $X$, and derive the covariance matrix of $\\hat{\\mu}_{\\lambda}$ as a function of $\\lambda$, $X$, and $\\sigma^{2}$.\n2. Using your derivations, express the expected squared error $E\\!\\left[\\|\\hat{\\mu}_{\\lambda} - \\mu\\|^{2}\\right]$ as a function of $\\lambda$ for the fixed design matrix\n$$\nX = \\begin{pmatrix}\n1 & 0 \\\\\n0 & 1 \\\\\n0 & 0\n\\end{pmatrix}.\n$$\n3. For $\\beta = \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix}$ and $\\sigma^{2} = 3$, determine the value of $\\lambda \\geq 0$ that minimizes $E\\!\\left[\\|\\hat{\\mu}_{\\lambda} - \\mu\\|^{2}\\right]$. Provide the final answer as an exact value. No rounding is required.", "solution": "The problem is valid as it is scientifically grounded, well-posed, and objective. It consists of standard derivations in the theory of regularized linear models. We will proceed by addressing each of the three tasks in order.\n\nThe linear model is given by $Y = X \\beta + \\varepsilon$, where $Y \\in \\mathbb{R}^{3}$, $X \\in \\mathbb{R}^{3 \\times 2}$, $\\beta \\in \\mathbb{R}^{2}$, and the error term $\\varepsilon \\in \\mathbb{R}^{3}$ has a multivariate normal distribution $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{3})$. This implies $E[\\varepsilon] = 0$ and $\\text{Cov}(\\varepsilon) = E[\\varepsilon \\varepsilon^{\\top}] = \\sigma^{2} I_{3}$. The mean response vector is $\\mu = X \\beta$. The ridge estimator for $\\beta$ and the induced estimator for $\\mu$ are given by:\n$$\n\\hat{\\beta}_{\\lambda} = (X^{\\top} X + \\lambda I_{2})^{-1} X^{\\top} Y\n$$\n$$\n\\hat{\\mu}_{\\lambda} = X \\hat{\\beta}_{\\lambda} = X (X^{\\top} X + \\lambda I_{2})^{-1} X^{\\top} Y\n$$\nwhere $\\lambda \\geq 0$.\n\n**1. Derivation of the Bias Vector and Covariance Matrix of $\\hat{\\mu}_{\\lambda}$**\n\nFirst, we derive the bias of the estimator $\\hat{\\mu}_{\\lambda}$. The bias is defined as $\\text{Bias}(\\hat{\\mu}_{\\lambda}) = E[\\hat{\\mu}_{\\lambda}] - \\mu$.\nTo find the expectation of $\\hat{\\mu}_{\\lambda}$, we use the linearity of the expectation operator. The expectation of the response vector $Y$ is:\n$$\nE[Y] = E[X \\beta + \\varepsilon] = X \\beta + E[\\varepsilon] = X \\beta + 0 = X \\beta\n$$\nsince $X$ and $\\beta$ are considered fixed (non-random).\nNow we compute the expectation of $\\hat{\\mu}_{\\lambda}$:\n$$\nE[\\hat{\\mu}_{\\lambda}] = E[X (X^{\\top} X + \\lambda I_{2})^{-1} X^{\\top} Y]\n$$\nThe matrix term is constant with respect to the expectation, so we can pull it out:\n$$\nE[\\hat{\\mu}_{\\lambda}] = X (X^{\\top} X + \\lambda I_{2})^{-1} X^{\\top} E[Y] = X (X^{\\top} X + \\lambda I_{2})^{-1} X^{\\top} (X \\beta)\n$$\nSubstituting this into the bias definition:\n$$\n\\text{Bias}(\\hat{\\mu}_{\\lambda}) = X (X^{\\top} X + \\lambda I_{2})^{-1} X^{\\top} X \\beta - \\mu = X (X^{\\top} X + \\lambda I_{2})^{-1} X^{\\top} X \\beta - X \\beta\n$$\nFactoring out $X$ on the left and $\\beta$ on the right:\n$$\n\\text{Bias}(\\hat{\\mu}_{\\lambda}) = X \\left[ (X^{\\top} X + \\lambda I_{2})^{-1} X^{\\top} X - I_{2} \\right] \\beta\n$$\nTo simplify the expression in the brackets, let $A = X^{\\top} X$. The expression becomes $(A + \\lambda I_{2})^{-1} A - I_{2}$. We can write $I_{2} = (A + \\lambda I_{2})^{-1} (A + \\lambda I_{2})$.\n$$\n(A + \\lambda I_{2})^{-1} A - (A + \\lambda I_{2})^{-1} (A + \\lambda I_{2}) = (A + \\lambda I_{2})^{-1} (A - (A + \\lambda I_{2})) = (A + \\lambda I_{2})^{-1} (-\\lambda I_{2}) = -\\lambda (A + \\lambda I_{2})^{-1}\n$$\nSubstituting back $A = X^{\\top} X$, the bias vector is:\n$$\n\\text{Bias}(\\hat{\\mu}_{\\lambda}) = X \\left[ -\\lambda (X^{\\top} X + \\lambda I_{2})^{-1} \\right] \\beta = -\\lambda X (X^{\\top} X + \\lambda I_{2})^{-1} \\beta\n$$\nNext, we derive the covariance matrix, $\\text{Cov}(\\hat{\\mu}_{\\lambda})$. We use the property that for a random vector $Z$ and a constant matrix $M$, $\\text{Cov}(MZ) = M \\text{Cov}(Z) M^{\\top}$.\nLet $H_{\\lambda} = X (X^{\\top} X + \\lambda I_{2})^{-1} X^{\\top}$. Then $\\hat{\\mu}_{\\lambda} = H_{\\lambda} Y$.\n$$\n\\text{Cov}(\\hat{\\mu}_{\\lambda}) = \\text{Cov}(H_{\\lambda} Y) = H_{\\lambda} \\text{Cov}(Y) H_{\\lambda}^{\\top}\n$$\nThe covariance of $Y$ is:\n$$\n\\text{Cov}(Y) = \\text{Cov}(X \\beta + \\varepsilon) = \\text{Cov}(\\varepsilon) = \\sigma^{2} I_{3}\n$$\nSubstituting this into the covariance expression for $\\hat{\\mu}_{\\lambda}$:\n$$\n\\text{Cov}(\\hat{\\mu}_{\\lambda}) = H_{\\lambda} (\\sigma^{2} I_{3}) H_{\\lambda}^{\\top} = \\sigma^{2} H_{\\lambda} H_{\\lambda}^{\\top}\n$$\nThe matrix $H_{\\lambda}$ is symmetric because $(X^{\\top}X + \\lambda I_2)$ is symmetric, and thus its inverse is also symmetric.\n$H_{\\lambda}^{\\top} = (X (X^{\\top} X + \\lambda I_{2})^{-1} X^{\\top})^{\\top} = (X^{\\top})^{\\top} ((X^{\\top} X + \\lambda I_{2})^{-1})^{\\top} X^{\\top} = X (X^{\\top} X + \\lambda I_{2})^{-1} X^{\\top} = H_{\\lambda}$.\nSo, $\\text{Cov}(\\hat{\\mu}_{\\lambda}) = \\sigma^{2} H_{\\lambda} H_{\\lambda} = \\sigma^{2} H_{\\lambda}^{2}$.\nSubstituting the definition of $H_{\\lambda}$:\n$$\n\\text{Cov}(\\hat{\\mu}_{\\lambda}) = \\sigma^{2} \\left( X (X^{\\top} X + \\lambda I_{2})^{-1} X^{\\top} \\right)^{2} = \\sigma^{2} X (X^{\\top} X + \\lambda I_{2})^{-1} X^{\\top} X (X^{\\top} X + \\lambda I_{2})^{-1} X^{\\top}\n$$\n\n**2. Expected Squared Error for the specific $X$**\n\nThe expected squared error, or Mean Squared Error (MSE), of $\\hat{\\mu}_{\\lambda}$ is $E[\\|\\hat{\\mu}_{\\lambda} - \\mu\\|^{2}]$. This can be decomposed into a sum of the squared norm of the bias and the trace of the covariance matrix (the total variance).\n$E[\\|\\hat{\\mu}_{\\lambda} - \\mu\\|^{2}] = E[(\\hat{\\mu}_{\\lambda} - \\mu)^{\\top}(\\hat{\\mu}_{\\lambda} - \\mu)] = \\|\\text{Bias}(\\hat{\\mu}_{\\lambda})\\|^{2} + \\text{Tr}(\\text{Cov}(\\hat{\\mu}_{\\lambda}))$.\n\nWe are given the specific design matrix:\n$$\nX = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 0 & 0 \\end{pmatrix}\n$$\nFirst, we compute $X^{\\top}X$:\n$$\nX^{\\top} X = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = I_{2}\n$$\nNow, we specialize the bias and covariance expressions using $X^{\\top}X = I_{2}$.\n\nFor the bias term:\n$$\n\\text{Bias}(\\hat{\\mu}_{\\lambda}) = -\\lambda X (I_{2} + \\lambda I_{2})^{-1} \\beta = -\\lambda X ((1+\\lambda)I_{2})^{-1} \\beta = -\\lambda X \\frac{1}{1+\\lambda} I_{2} \\beta = -\\frac{\\lambda}{1+\\lambda} X \\beta = -\\frac{\\lambda}{1+\\lambda} \\mu\n$$\nThe squared norm of the bias is:\n$$\n\\|\\text{Bias}(\\hat{\\mu}_{\\lambda})\\|^{2} = \\left\\| -\\frac{\\lambda}{1+\\lambda} \\mu \\right\\|^{2} = \\frac{\\lambda^{2}}{(1+\\lambda)^{2}} \\|\\mu\\|^{2}\n$$\nWe note that for this specific $X$, $\\mu = X\\beta = \\begin{pmatrix} \\beta_{1} \\\\ \\beta_{2} \\\\ 0 \\end{pmatrix}$. Thus, $\\|\\mu\\|^{2} = \\beta_{1}^{2} + \\beta_{2}^{2} = \\|\\beta\\|^{2}$.\n$$\n\\|\\text{Bias}(\\hat{\\mu}_{\\lambda})\\|^{2} = \\frac{\\lambda^{2}}{(1+\\lambda)^{2}} \\|\\beta\\|^{2}\n$$\n\nFor the variance term, we first simplify the covariance matrix expression:\n$$\n\\text{Cov}(\\hat{\\mu}_{\\lambda}) = \\sigma^{2} X(I_{2} + \\lambda I_{2})^{-1}I_{2}(I_{2} + \\lambda I_{2})^{-1}X^{\\top} = \\sigma^{2} X \\left(\\frac{1}{1+\\lambda}I_{2}\\right) I_{2} \\left(\\frac{1}{1+\\lambda}I_{2}\\right) X^{\\top} = \\frac{\\sigma^{2}}{(1+\\lambda)^{2}} X X^{\\top}\n$$\nLet's compute $XX^{\\top}$:\n$$\nXX^{\\top} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix}\n$$\nThe variance term is the trace of the covariance matrix:\n$$\n\\text{Tr}(\\text{Cov}(\\hat{\\mu}_{\\lambda})) = \\text{Tr}\\left( \\frac{\\sigma^{2}}{(1+\\lambda)^{2}} X X^{\\top} \\right) = \\frac{\\sigma^{2}}{(1+\\lambda)^{2}} \\text{Tr}(XX^{\\top})\n$$\nThe trace of $XX^{\\top}$ is $1+1+0=2$.\n$$\n\\text{Tr}(\\text{Cov}(\\hat{\\mu}_{\\lambda})) = \\frac{2\\sigma^{2}}{(1+\\lambda)^{2}}\n$$\nCombining the bias and variance terms, the expected squared error is:\n$$\nE[\\|\\hat{\\mu}_{\\lambda} - \\mu\\|^{2}] = \\frac{\\lambda^{2}}{(1+\\lambda)^{2}} \\|\\beta\\|^{2} + \\frac{2\\sigma^{2}}{(1+\\lambda)^{2}} = \\frac{\\lambda^{2}\\|\\beta\\|^{2} + 2\\sigma^{2}}{(1+\\lambda)^{2}}\n$$\n\n**3. Minimizing the Expected Squared Error**\n\nWe are given $\\beta = \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix}$ and $\\sigma^{2} = 3$. First, we compute $\\|\\beta\\|^{2}$:\n$$\n\\|\\beta\\|^{2} = 2^{2} + (-1)^{2} = 4 + 1 = 5\n$$\nSubstituting $\\|\\beta\\|^{2}=5$ and $\\sigma^{2}=3$ into the expected squared error expression:\n$$\nE[\\|\\hat{\\mu}_{\\lambda} - \\mu\\|^{2}] = \\frac{5\\lambda^{2} + 2(3)}{(1+\\lambda)^{2}} = \\frac{5\\lambda^{2} + 6}{(1+\\lambda)^{2}}\n$$\nLet $f(\\lambda) = \\frac{5\\lambda^{2} + 6}{(1+\\lambda)^{2}}$. We want to find the value of $\\lambda \\geq 0$ that minimizes $f(\\lambda)$. We do this by finding the derivative of $f(\\lambda)$ with respect to $\\lambda$ and setting it to zero. Using the quotient rule for differentiation, $[u/v]' = (u'v - uv')/v^2$:\n$$\nf'(\\lambda) = \\frac{(10\\lambda)(1+\\lambda)^{2} - (5\\lambda^{2} + 6)(2(1+\\lambda))}{((1+\\lambda)^{2})^{2}}\n$$\nFor $\\lambda \\geq 0$, $1+\\lambda \\neq 0$, so we can simplify by dividing the numerator and denominator by $1+\\lambda$:\n$$\nf'(\\lambda) = \\frac{10\\lambda(1+\\lambda) - 2(5\\lambda^{2} + 6)}{(1+\\lambda)^{3}} = \\frac{10\\lambda^{2} + 10\\lambda - 10\\lambda^{2} - 12}{(1+\\lambda)^{3}} = \\frac{10\\lambda - 12}{(1+\\lambda)^{3}}\n$$\nSetting the derivative to zero to find critical points:\n$$\nf'(\\lambda) = 0 \\implies 10\\lambda - 12 = 0 \\implies 10\\lambda = 12 \\implies \\lambda = \\frac{12}{10} = \\frac{6}{5}\n$$\nThe critical point is $\\lambda = \\frac{6}{5}$. This value satisfies the constraint $\\lambda \\geq 0$. To confirm this is a minimum, we can examine the sign of the derivative. For $\\lambda \\geq 0$, the denominator $(1+\\lambda)^{3}$ is positive. The sign of $f'(\\lambda)$ is determined by the numerator $10\\lambda - 12$.\n- For $0 \\leq \\lambda < \\frac{6}{5}$, $10\\lambda - 12 < 0$, so $f'(\\lambda) < 0$. The function $f(\\lambda)$ is decreasing.\n- For $\\lambda > \\frac{6}{5}$, $10\\lambda - 12 > 0$, so $f'(\\lambda) > 0$. The function $f(\\lambda)$ is increasing.\nSince the function decreases to $\\lambda=\\frac{6}{5}$ and increases thereafter, $\\lambda = \\frac{6}{5}$ is the global minimum for $\\lambda \\geq 0$.\n\nThe value of $\\lambda$ that minimizes the expected squared error is $\\frac{6}{5}$.", "answer": "$$\n\\boxed{\\frac{6}{5}}\n$$", "id": "3155654"}, {"introduction": "An estimator's performance is not absolute; it is deeply tied to the nature of the data it is applied to. This computational practice [@problem_id:3155655] dives into a scenario where the familiar sample mean, $\\bar{X}$, becomes an unreliable estimator due to heavy-tailed noise. You will implement a Monte Carlo simulation to compare its performance against a robust alternative, the median-of-means estimator, and gain hands-on experience in why robustness is a critical consideration in modern data analysis.", "problem": "Consider independent and identically distributed (i.i.d.) data points $X_1, X_2, \\dots, X_n$ generated from a heavy-tailed noise model defined by $X_i = \\mu + Z_i$, where $Z_i \\sim \\text{Student-}t_\\nu$ and $\\nu \\in (1,2)$. Let the parameter of interest be the location parameter $\\mu$, and fix $\\mu = 1.0$ and $\\nu = 1.5$. The sample mean statistic is $\\bar{X} = \\frac{1}{n} \\sum_{i=1}^n X_i$, which is the canonical estimator of $\\mu$ but is known to be sensitive to heavy tails. Define the median-of-means estimator as follows: for a given $n$, choose $k = \\lfloor \\sqrt{n} \\rfloor$ groups of equal size $m = \\lfloor n/k \\rfloor$. Partition the first $k \\cdot m$ observations into $k$ disjoint groups $G_1, G_2, \\dots, G_k$ with $|G_j| = m$ for all $j$, compute each group mean $M_j = \\frac{1}{m} \\sum_{i \\in G_j} X_i$, and define the estimator $\\widehat{\\mu}_{\\mathrm{MoM}} = \\operatorname{median}(M_1, M_2, \\dots, M_k)$. Any leftover observations beyond the first $k \\cdot m$ should be ignored for the median-of-means estimator. The sample mean $\\bar{X}$ should be computed using all $n$ observations.\n\nYour task is to implement a program that, for a given test suite of sample sizes $n$, compares the empirical concentration of $\\bar{X}$ and $\\widehat{\\mu}_{\\mathrm{MoM}}$ around $\\mu$ via Monte Carlo simulation. For each $n$, perform $T$ independent trials with $T = 2000$, and in each trial generate a fresh i.i.d. sample of size $n$ from the specified heavy-tailed model. For each estimator, compute the empirical mean absolute error with respect to $\\mu$:\n$$\\mathrm{MAE}_{\\mathrm{mean}}(n) = \\frac{1}{T} \\sum_{t=1}^T \\left| \\bar{X}^{(t)} - \\mu \\right|, \\quad \\mathrm{MAE}_{\\mathrm{MoM}}(n) = \\frac{1}{T} \\sum_{t=1}^T \\left| \\widehat{\\mu}_{\\mathrm{MoM}}^{(t)} - \\mu \\right|.$$\nReport, for each $n$, the ratio\n$$R(n) = \\frac{\\mathrm{MAE}_{\\mathrm{mean}}(n)}{\\mathrm{MAE}_{\\mathrm{MoM}}(n)}.$$\nLarger values of $R(n)$ indicate better concentration for the median-of-means estimator relative to the sample mean. Use a fixed pseudorandom number generator seed equal to $12345$ so that the results are reproducible.\n\nTest suite:\n- Sample sizes $n$ to evaluate: $n \\in \\{1, 10, 30, 100, 300, 1000\\}$.\n- Location parameter: $\\mu = 1.0$.\n- Degrees of freedom: $\\nu = 1.5$.\n- Trials: $T = 2000$.\n\nEdge case coverage:\n- When $n = 1$, one has $k = 1$ and $m = 1$, so $\\widehat{\\mu}_{\\mathrm{MoM}}$ equals $\\bar{X}$ exactly, and $R(1)$ should equal $1$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test suite $n$ values. Express each float rounded to $6$ decimal places. For example, the output format must be exactly like $[r_1,r_2,\\dots,r_6]$, where each $r_j$ corresponds to $R(n)$ for the matching $n$ in the test suite.", "solution": "The problem is assessed to be valid. It is scientifically grounded in the principles of statistical estimation, well-posed with a clear and complete set of definitions and constraints, and objective in its formulation. The problem addresses a non-trivial concept in robust statistics: the estimation of a location parameter for a heavy-tailed distribution, where standard estimators like the sample mean perform poorly. The comparison between the sample mean and the median-of-means estimator is a standard and informative exercise in this domain. All parameters, procedures, and evaluation metrics are specified without ambiguity, ensuring a unique and verifiable solution.\n\n### Theoretical Framework\n\nThe problem requires a comparison of two estimators for the location parameter $\\mu$ of a distribution defined by $X_i = \\mu + Z_i$, where the noise term $Z_i$ follows a Student's t-distribution with $\\nu$ degrees of freedom. The specific value for the degrees of freedom is $\\nu = 1.5$. A critical property of the Student's t-distribution is that its moments depend on $\\nu$. Specifically, the $k$-th moment is finite if and only if $k < \\nu$.\n\nFor the given $\\nu = 1.5$, which lies in the interval $(1, 2)$, the following properties hold:\n1.  The mean of the distribution is well-defined and finite because $1 < \\nu$. The expected value is $E[X_i] = E[\\mu + Z_i] = \\mu + E[Z_i] = \\mu + 0 = \\mu$.\n2.  The variance of the distribution is infinite because $2 \\ge \\nu$.\n\nThe infinite variance of the underlying data distribution has profound consequences for the behavior of the sample mean, $\\bar{X} = \\frac{1}{n} \\sum_{i=1}^n X_i$. The standard Central Limit Theorem, which states that the distribution of the sample mean approaches a normal distribution as the sample size $n$ grows, requires the underlying distribution to have a finite variance. Since this condition is violated, the distribution of $\\bar{X}$ does not converge to a Gaussian. Instead, its distribution remains heavy-tailed, implying that extreme values of $\\bar{X}$ occur with non-negligible probability, even for large $n$. Consequently, the sample mean is not a robust estimator for $\\mu$ in this context, as it is highly sensitive to the frequent large-magnitude observations (outliers) generated by the heavy-tailed noise.\n\n### The Median-of-Means Estimator\n\nThe median-of-means (MoM) estimator, $\\widehat{\\mu}_{\\mathrm{MoM}}$, is a robust alternative designed to perform well in the presence of heavy tails. Its construction involves a two-stage process:\n1.  **Averaging:** The data is partitioned into $k = \\lfloor \\sqrt{n} \\rfloor$ smaller groups of size $m = \\lfloor n/k \\rfloor$. The sample mean, $M_j$, is computed for each group $j \\in \\{1, \\dots, k\\}$. Averaging within a group still produces a variable with heavy tails, but it is an estimate of $\\mu$.\n2.  **Median:** The final estimate, $\\widehat{\\mu}_{\\mathrm{MoM}}$, is the median of these $k$ group means: $\\widehat{\\mu}_{\\mathrm{MoM}} = \\operatorname{median}(M_1, M_2, \\dots, M_k)$.\n\nThe robustness of the MoM estimator stems from the median operator. The median is insensitive to the magnitude of extreme values. If a few of the group means $M_j$ are exceptionally large or small (due to one or more outliers from the Student's t-distribution falling into that group), they will not significantly influence the median, as long as fewer than half of the group means are corrupted. This property provides a strong defense against the impact of heavy tails, leading to an estimator that concentrates more tightly around the true parameter $\\mu$.\n\n### Monte Carlo Simulation\n\nTo empirically evaluate and compare the performance of $\\bar{X}$ and $\\widehat{\\mu}_{\\mathrm{MoM}}$, we employ a Monte Carlo simulation. For each sample size $n$ in the test suite, we perform a large number of trials, $T = 2000$. In each trial $t \\in \\{1, \\dots, T\\}$:\n1.  A new dataset of $n$ i.i.d. points is generated from the specified Student's t-distribution with $\\nu = 1.5$ and location $\\mu = 1.0$.\n2.  The sample mean $\\bar{X}^{(t)}$ is computed using all $n$ points.\n3.  The MoM estimate $\\widehat{\\mu}_{\\mathrm{MoM}}^{(t)}$ is computed according to its definition.\n4.  The absolute error for each estimator, $|\\bar{X}^{(t)} - \\mu|$ and $|\\widehat{\\mu}_{\\mathrm{MoM}}^{(t)} - \\mu|$, is recorded.\n\nAfter all $T$ trials, the Mean Absolute Error (MAE) for each estimator is calculated by averaging the absolute errors across the trials:\n$$ \\mathrm{MAE}_{\\mathrm{mean}}(n) = \\frac{1}{T} \\sum_{t=1}^T \\left| \\bar{X}^{(t)} - \\mu \\right| $$\n$$ \\mathrm{MAE}_{\\mathrm{MoM}}(n) = \\frac{1}{T} \\sum_{t=1}^T \\left| \\widehat{\\mu}_{\\mathrm{MoM}}^{(t)} - \\mu \\right| $$\n\nThe final metric for comparison is the ratio $R(n) = \\frac{\\mathrm{MAE}_{\\mathrm{mean}}(n)}{\\mathrm{MAE}_{\\mathrm{MoM}}(n)}$. A value of $R(n) > 1$ indicates that the MoM estimator has a smaller MAE and thus exhibits better concentration around $\\mu$ than the sample mean. Theory predicts that as $n$ increases, the superiority of the MoM estimator will become more pronounced, leading to an increasing trend in $R(n)$.\n\nFor the edge case $n=1$, we have $k = \\lfloor\\sqrt{1}\\rfloor = 1$ and $m = \\lfloor 1/1 \\rfloor = 1$. The MoM procedure uses one group of size one, so $M_1 = X_1$. The estimator is $\\widehat{\\mu}_{\\mathrm{MoM}} = \\operatorname{median}(M_1) = X_1$. The sample mean is also $\\bar{X} = X_1$. Since both estimators are identical, their MAEs will be identical, and thus $R(1)$ must equal $1$.\n\n### Algorithm Implementation\n\nThe simulation will be implemented in Python using the `numpy` and `scipy` libraries.\n1.  Initialize a pseudorandom number generator with a fixed seed of $12345$ for reproducibility.\n2.  Define the constants: $\\mu = 1.0$, $\\nu = 1.5$, $T = 2000$.\n3.  Iterate through each sample size $n$ in the test suite $\\{1, 10, 30, 100, 300, 1000\\}$.\n4.  For each $n$, initialize accumulators for the absolute errors of both estimators to zero.\n5.  Begin the Monte Carlo loop for $t$ from $1$ to $T$:\n    a. Generate a sample of size $n$ using `scipy.stats.t.rvs(df=nu, loc=mu, size=n, random_state=rng)`.\n    b. Calculate the sample mean $\\bar{X}$ of the full sample.\n    c. Calculate the MoM parameters: $k = \\lfloor \\sqrt{n} \\rfloor$ and $m = \\lfloor n/k \\rfloor$. Handle the casewhere $k=0$ if $n=0$ by design, although not required for the given test suite. If $k>0$, proceed.\n    d. Determine the number of data points to use for MoM: `num_pts_mom = k * m`.\n    e. Truncate the sample to the first `num_pts_mom` points.\n    f. Reshape this truncated sample into a matrix of shape $(k, m)$.\n    g. Compute the means of each row to get the $k$ group means.\n    h. Compute the median of the group means to get $\\widehat{\\mu}_{\\mathrm{MoM}}$.\n    i. Calculate the absolute errors $|\\bar{X} - \\mu|$ and $|\\widehat{\\mu}_{\\mathrm{MoM}} - \\mu|$ and add them to the respective accumulators.\n6.  After the Monte Carlo loop, compute the MAEs for both estimators by dividing the accumulated errors by $T$.\n7.  Compute the ratio $R(n) = \\mathrm{MAE}_{\\mathrm{mean}}(n) / \\mathrm{MAE}_{\\mathrm{MoM}}(n)$.\n8.  Store the computed ratio.\n9.  After iterating through all $n$, format the list of ratios into the specified output string.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import t as student_t\n\ndef solve():\n    \"\"\"\n    Performs a Monte Carlo simulation to compare the mean absolute error of the\n    sample mean and the median-of-means estimator for a heavy-tailed distribution.\n    \"\"\"\n    \n    # Problem parameters\n    mu = 1.0\n    nu = 1.5\n    T = 2000\n    seed = 12345\n    \n    # Test suite of sample sizes\n    test_cases = [1, 10, 30, 100, 300, 1000]\n\n    # Initialize the random number generator for reproducibility\n    rng = np.random.default_rng(seed)\n\n    results = []\n    \n    for n in test_cases:\n        # Accumulators for absolute errors for each estimator\n        total_abs_error_mean = 0.0\n        total_abs_error_mom = 0.0\n\n        for _ in range(T):\n            # 1. Generate a fresh i.i.d. sample of size n\n            samples = student_t.rvs(df=nu, loc=mu, scale=1, size=n, random_state=rng)\n\n            # 2. Calculate the sample mean estimator\n            x_bar = np.mean(samples)\n            \n            # 3. Calculate the median-of-means estimator\n            if n == 0:\n                # Handle n=0 case, although not in test suite.\n                # Estimators are undefined, but we can set to a default.\n                mu_mom = np.nan \n            else:\n                k = int(np.floor(np.sqrt(n)))\n                if k == 0:\n                    # This branch is hit for n=1, 2, 3 where k=1, but if n was 0, k=0.\n                    # Or if n=0, sqrt(n)=0 floor(0)=0.\n                    # As k=0, m is division by zero. Let's handle this case for completeness.\n                    # For n in {1,2,3}, k=1, so m = floor(n/1) = n.\n                    # num_pts=n, reshape(1,n). mean across axis=1 is just the row.\n                    # median of one value is the value itself.\n                    # Effectively, for k=1, mu_mom is just mean of first m=n points.\n                    # This logic should be robust. k will not be 0 for n>=1.\n                     mu_mom = np.nan\n                else: \n                     m = int(np.floor(n / k))\n                \n                if m > 0: # Ensure groups are non-empty\n                    num_points_for_mom = k * m\n                    \n                    # Partition the first k*m observations\n                    # and reshape into k groups of size m\n                    groups = samples[:num_points_for_mom].reshape(k, m)\n                    \n                    # Compute mean for each group\n                    group_means = np.mean(groups, axis=1)\n                    \n                    # Compute the median of the group means\n                    mu_mom = np.median(group_means)\n                else:\n                    # This case happens for small n where floor(n/k) = 0.\n                    # E.g., for n=2, k=1, m=2. No problem.\n                    # E.g., for n=3, k=1, m=3. No problem.\n                    # No n in the test suite causes this.\n                    # If it happened, MoM is not well-defined.\n                    mu_mom = np.nan\n\n            # 4. Calculate absolute errors and accumulate\n            total_abs_error_mean += np.abs(x_bar - mu)\n            if not np.isnan(mu_mom):\n                total_abs_error_mom += np.abs(mu_mom - mu)\n\n        # 5. Compute Mean Absolute Error (MAE) for both estimators\n        mae_mean = total_abs_error_mean / T\n        mae_mom = total_abs_error_mom / T\n\n        # 6. Compute the ratio R(n)\n        # For n=1, k=1, m=1, so mu_mom = x_bar and ratio should be 1.\n        if mae_mom == 0:\n            # Avoid division by zero, though unlikely in this context\n            ratio = 1.0 if mae_mean == 0 else np.inf\n        else:\n            ratio = mae_mean / mae_mom\n            \n        results.append(ratio)\n\n    # Final print statement in the exact required format.\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3155655"}, {"introduction": "While analytical derivations are powerful, they are not always feasible, especially for complex, nonlinear estimators. This practice introduces two powerful computational techniques, the jackknife and the bootstrap, that allow us to estimate an estimator's properties like bias directly from the data. By implementing these resampling methods [@problem_id:3155706], you will learn how to assess and correct for bias in situations where purely theoretical approaches fall short, a common challenge in applied statistics.", "problem": "Consider a dataset of independent and identically distributed (i.i.d.) positive observations $X_1, X_2, \\dots, X_n$. The parameter of interest is the logarithm of the population mean, denoted by $\\theta = \\log\\left(E[X]\\right)$. A statistic is a function of the data, denoted here by $T(D)$ for dataset $D$, and an estimator is the statistic viewed as a function intended to estimate a parameter; its realized value on a dataset is an estimate. In this problem, use the nonlinear estimator $\\hat{\\theta} = T(D) = \\log\\left(\\bar{X}\\right)$ where $\\bar{X}$ is the sample mean. The bias of $\\hat{\\theta}$ for $\\theta$ is defined as $B(\\hat{\\theta}) = E[\\hat{\\theta}] - \\theta$. Your task is to estimate the bias using the jackknife method and to compute a bootstrap-based bias correction, then to compare the two approaches across different data-generating scenarios.\n\nStarting from the core definitions above, implement the following operations without relying on any pre-derived or shortcut formulas:\n- Use the jackknife based on leave-one-out samples to estimate the bias in $\\hat{\\theta}$.\n- Use the nonparametric bootstrap (resampling with replacement from the empirical distribution of the observed data) to estimate the bias in $\\hat{\\theta}$ and form a bootstrap bias-corrected estimator.\n- For each dataset, compute the true parameter value $\\theta$ using the known data-generating distribution, and compare the jackknife bias-corrected estimate and the bootstrap bias-corrected estimate to $\\theta$.\n\nYou must generate the datasets exactly as specified in the test suite below, using the given random seeds for reproducibility. All angles, if any, must be treated as dimensionless real numbers; no physical units are involved. All outputs must be real numbers or integers, expressed in plain decimal notation.\n\nTest Suite:\n- Case $1$ (happy path, moderate sample size, light skew):\n  - Distribution: Exponential with rate $\\lambda = 1.0$, so $E[X] = 1/\\lambda$ and $\\theta = \\log(1/\\lambda) = -\\log(\\lambda)$.\n  - Sample size: $n = 30$.\n  - Bootstrap replicates: $B = 500$.\n  - Random seed: $123$.\n- Case $2$ (boundary condition, very small sample):\n  - Distribution: Exponential with rate $\\lambda = 2.0$.\n  - Sample size: $n = 3$.\n  - Bootstrap replicates: $B = 1000$.\n  - Random seed: $456$.\n- Case $3$ (happy path, moderate sample size, heavier skew):\n  - Distribution: Lognormal with underlying normal parameters $\\mu = 0.0$, $\\sigma = 1.0$, so $E[X] = \\exp(\\mu + \\sigma^2/2)$ and $\\theta = \\log(E[X]) = \\mu + \\sigma^2/2$.\n  - Sample size: $n = 50$.\n  - Bootstrap replicates: $B = 1000$.\n  - Random seed: $789$.\n- Case $4$ (edge case, heavy tail):\n  - Distribution: Lognormal with underlying normal parameters $\\mu = -0.5$, $\\sigma = 2.0$.\n  - Sample size: $n = 20$.\n  - Bootstrap replicates: $B = 2000$.\n  - Random seed: $321$.\n\nFor each case, perform the following:\n- Generate data $X_1, \\dots, X_n$ from the specified distribution using the given seed.\n- Compute the full-sample estimate $\\hat{\\theta} = \\log(\\bar{X})$.\n- Compute the jackknife bias estimate from the leave-one-out datasets and the corresponding jackknife bias-corrected estimator.\n- Compute the bootstrap bias estimate using $B$ resamples and the corresponding bootstrap bias-corrected estimator.\n- Compute the true parameter $\\theta$ from the known distributional parameters for the case.\n- Compute which correction is closer to the true parameter by comparing absolute errors.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element corresponds to one case and must itself be a bracketed list in the following order:\n$[\\hat{\\theta}, \\theta, b_{\\text{jack}}, b_{\\text{boot}}, \\hat{\\theta}_{\\text{jack,corr}}, \\hat{\\theta}_{\\text{boot,corr}}, I_{\\text{boot-better}}]$,\nwhere $b_{\\text{jack}}$ is the jackknife bias estimate, $b_{\\text{boot}}$ is the bootstrap bias estimate, $\\hat{\\theta}_{\\text{jack,corr}}$ is the jackknife bias-corrected estimate, $\\hat{\\theta}_{\\text{boot,corr}}$ is the bootstrap bias-corrected estimate, and $I_{\\text{boot-better}}$ is an integer indicator equal to $1$ if the bootstrap bias-corrected estimate has a smaller absolute error relative to $\\theta$ than the jackknife bias-corrected estimate, and $0$ otherwise. For example, the final output should look like $[[\\dots],[\\dots],[\\dots],[\\dots]]$ with all values in decimal or integer form, no percentage signs, and no additional text.", "solution": "The user has provided a problem in the domain of computational statistics, requiring the implementation and comparison of two resampling-based bias estimation techniques: the jackknife and the nonparametric bootstrap.\n\n### 1. Problem Validation\n\nThe problem statement has been rigorously validated and is determined to be a valid scientific problem.\n\n*   **Givens Extracted**: The problem provides all necessary parameters for four distinct test cases, including the data-generating distributions (Exponential, Lognormal), their parameters ($\\lambda$, $\\mu$, $\\sigma$), sample sizes ($n$), number of bootstrap replicates ($B$), and random seeds for reproducibility. It clearly defines the parameter of interest $\\theta = \\log(E[X])$, the estimator $\\hat{\\theta} = \\log(\\bar{X})$, and the required outputs.\n\n*   **Validation Verdict**:\n    1.  **Scientifically Grounded**: The problem is based on fundamental and well-established principles of statistical inference. The jackknife and bootstrap are canonical methods for estimating properties of an estimator, such as bias and variance. The estimator $\\hat{\\theta} = \\log(\\bar{X})$ is a classic example demonstrating bias in nonlinear transformations of sample moments. The bias arises due to Jensen's inequality, as the logarithm function is strictly concave. Specifically, $E[\\log(\\bar{X})] \\le \\log(E[\\bar{X}]) = \\log(E[X]) = \\theta$. Thus, a negative bias is expected, which these methods aim to estimate. The problem is scientifically sound.\n    2.  **Well-Posed**: The problem is fully specified. For each case, the combination of distribution, parameters, sample size, and random seed uniquely determines the dataset to be generated. The procedures for calculating the jackknife and bootstrap estimates are standard and unambiguous. A unique, stable, and meaningful numerical solution exists for each test case.\n    3.  **Objective**: The problem is stated using precise, formal mathematical and statistical language. The tasks are objective computational procedures, and the final comparison is based on a clear, quantitative criterion (absolute error).\n\nThe problem is valid as it passes all criteria. The subsequent sections will detail the solution methodology.\n\n### 2. Methodology\n\nThe solution will be implemented by systematically performing the required calculations for each test case.\n\n#### 2.1. True Parameter Calculation ($\\theta$)\nThe true parameter $\\theta = \\log(E[X])$ is calculated based on the known properties of the specified distributions.\n*   For an Exponential distribution with rate parameter $\\lambda$, the expected value is $E[X] = 1/\\lambda$. Therefore, $\\theta = \\log(1/\\lambda) = -\\log(\\lambda)$.\n*   For a Lognormal distribution with underlying normal distribution parameters $\\mu$ and $\\sigma$, the expected value is $E[X] = \\exp(\\mu + \\sigma^2/2)$. Therefore, $\\theta = \\log(E[X]) = \\mu + \\sigma^2/2$.\n\n#### 2.2. Data Generation and Full-Sample Estimate ($\\hat{\\theta}$)\nFor each case, a dataset $D = \\{X_1, \\dots, X_n\\}$ of size $n$ is generated from the specified distribution using the given random seed. The full-sample estimate is then computed as $\\hat{\\theta} = \\log(\\bar{X})$, where $\\bar{X} = \\frac{1}{n}\\sum_{i=1}^n X_i$.\n\n#### 2.3. Jackknife Bias Estimation\nThe jackknife method estimates bias by systematically re-computing the statistic on leave-one-out subsamples.\n1.  For each observation $i \\in \\{1, \\dots, n\\}$, create a subsample $D_{(-i)}$ by removing $X_i$ from the original dataset $D$.\n2.  Calculate the sample mean of this subsample, $\\bar{X}_{(-i)} = \\frac{1}{n-1}\\sum_{j \\ne i} X_j$. This can be efficiently computed as $\\bar{X}_{(-i)} = \\frac{n\\bar{X} - X_i}{n-1}$.\n3.  Compute the \"leave-one-out\" estimate $\\hat{\\theta}_{(-i)} = \\log(\\bar{X}_{(-i)})$.\n4.  The jackknife bias estimate, $b_{\\text{jack}}$, is given by the formula:\n    $$b_{\\text{jack}} = (n-1) \\left( \\left( \\frac{1}{n} \\sum_{i=1}^n \\hat{\\theta}_{(-i)} \\right) - \\hat{\\theta} \\right)$$\n5.  The jackknife bias-corrected estimate is then $\\hat{\\theta}_{\\text{jack,corr}} = \\hat{\\theta} - b_{\\text{jack}}$.\n\n#### 2.4. Bootstrap Bias Estimation\nThe nonparametric bootstrap estimates bias by simulating the sampling distribution of the estimator from the empirical distribution of the data.\n1.  Generate $B$ bootstrap samples, $D^{\\ast 1}, \\dots, D^{\\ast B}$. Each sample $D^{\\ast b}$ is created by drawing $n$ observations from the original dataset $D$ with replacement.\n2.  For each bootstrap sample $D^{\\ast b}$, compute the bootstrap replicate of the statistic: $\\hat{\\theta}^{\\ast b} = \\log(\\bar{X}^{\\ast b})$, where $\\bar{X}^{\\ast b}$ is the mean of the sample $D^{\\ast b}$.\n3.  The bootstrap estimate of bias, $b_{\\text{boot}}$, is the difference between the average of the bootstrap replicates and the original full-sample estimate:\n    $$b_{\\text{boot}} = \\left( \\frac{1}{B} \\sum_{b=1}^B \\hat{\\theta}^{\\ast b} \\right) - \\hat{\\theta}$$\n4.  The bootstrap bias-corrected estimate is $\\hat{\\theta}_{\\text{boot,corr}} = \\hat{\\theta} - b_{\\text{boot}}$. This can be written as:\n    $$\\hat{\\theta}_{\\text{boot,corr}} = 2\\hat{\\theta} - \\frac{1}{B} \\sum_{b=1}^B \\hat{\\theta}^{\\ast b}$$\n\n#### 2.5. Comparison and Final Output\nFor each test case, the absolute error of each corrected estimate with respect to the true parameter $\\theta$ is calculated:\n*   Error (Jackknife): $\\text{err}_{\\text{jack}} = |\\hat{\\theta}_{\\text{jack,corr}} - \\theta|$\n*   Error (Bootstrap): $\\text{err}_{\\text{boot}} = |\\hat{\\theta}_{\\text{boot,corr}} - \\theta|$\n\nAn indicator variable, $I_{\\text{boot-better}}$, is set to $1$ if $\\text{err}_{\\text{boot}} < \\text{err}_{\\text{jack}}$, and $0$ otherwise. The final result for each case is an ordered list of seven values: $[\\hat{\\theta}, \\theta, b_{\\text{jack}}, b_{\\text{boot}}, \\hat{\\theta}_{\\text{jack,corr}}, \\hat{\\theta}_{\\text{boot,corr}}, I_{\\text{boot-better}}]$. The results from all four cases are aggregated into a list of lists for the final output.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by computing jackknife and bootstrap bias estimates\n    for the estimator log(sample mean) across four test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"dist\": \"exp\",\n            \"params\": {\"lambda\": 1.0},\n            \"n\": 30,\n            \"B\": 500,\n            \"seed\": 123\n        },\n        {\n            \"dist\": \"exp\",\n            \"params\": {\"lambda\": 2.0},\n            \"n\": 3,\n            \"B\": 1000,\n            \"seed\": 456\n        },\n        {\n            \"dist\": \"lognormal\",\n            \"params\": {\"mu\": 0.0, \"sigma\": 1.0},\n            \"n\": 50,\n            \"B\": 1000,\n            \"seed\": 789\n        },\n        {\n            \"dist\": \"lognormal\",\n            \"params\": {\"mu\": -0.5, \"sigma\": 2.0},\n            \"n\": 20,\n            \"B\": 2000,\n            \"seed\": 321\n        }\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        n = case[\"n\"]\n        B = case[\"B\"]\n        seed = case[\"seed\"]\n        dist = case[\"dist\"]\n        params = case[\"params\"]\n\n        # Use a random number generator for reproducibility.\n        rng = np.random.default_rng(seed)\n\n        # 1. Generate data and compute true theta.\n        if dist == \"exp\":\n            rate = params[\"lambda\"]\n            X = rng.exponential(scale=1.0/rate, size=n)\n            theta_true = -np.log(rate)\n        elif dist == \"lognormal\":\n            mu, sigma = params[\"mu\"], params[\"sigma\"]\n            X = rng.lognormal(mean=mu, sigma=sigma, size=n)\n            theta_true = mu + (sigma**2) / 2.0\n            \n        # 2. Compute the full-sample estimate.\n        x_bar = np.mean(X)\n        theta_hat = np.log(x_bar)\n\n        # 3. Compute jackknife bias estimate and corrected estimator.\n        # Efficiently calculate leave-one-out means\n        x_bar_j = (n * x_bar - X) / (n - 1)\n        theta_hat_j = np.log(x_bar_j)\n        \n        # Jackknife bias\n        theta_hat_j_mean = np.mean(theta_hat_j)\n        b_jack = (n - 1) * (theta_hat_j_mean - theta_hat)\n        \n        # Jackknife bias-corrected estimate\n        theta_hat_jack_corr = theta_hat - b_jack\n\n        # 4. Compute bootstrap bias estimate and corrected estimator.\n        bootstrap_replicates = rng.choice(X, size=(B, n), replace=True)\n        x_bar_b = np.mean(bootstrap_replicates, axis=1)\n        theta_hat_b = np.log(x_bar_b)\n        \n        # Bootstrap bias\n        theta_hat_b_mean = np.mean(theta_hat_b)\n        b_boot = theta_hat_b_mean - theta_hat\n        \n        # Bootstrap bias-corrected estimate\n        theta_hat_boot_corr = theta_hat - b_boot\n        \n        # 5. Compare the corrected estimators.\n        err_jack = np.abs(theta_hat_jack_corr - theta_true)\n        err_boot = np.abs(theta_hat_boot_corr - theta_true)\n        I_boot_better = 1 if err_boot < err_jack else 0\n\n        # Assemble the results for the current case.\n        case_result = [\n            theta_hat, \n            theta_true, \n            b_jack, \n            b_boot, \n            theta_hat_jack_corr, \n            theta_hat_boot_corr, \n            I_boot_better\n        ]\n        all_results.append(case_result)\n\n    # Format the final output string exactly as required.\n    result_strings = []\n    for res in all_results:\n        # Convert each number in the list to its string representation.\n        # The integer indicator will be correctly formatted as an integer string.\n        s = \"[\" + \",\".join(map(str, res)) + \"]\"\n        result_strings.append(s)\n    \n    final_output = \"[\" + \",\".join(result_strings) + \"]\"\n    print(final_output)\n\nsolve()\n```", "id": "3155706"}]}