{"hands_on_practices": [{"introduction": "Estimating a quantile from a sample is a fundamental statistical task, but what is the uncertainty associated with that estimate? This exercise guides you through propagating the statistical uncertainty of an empirical quantile to a downstream risk metric using the delta method. Mastering this process is crucial for applications like financial risk management, where quantifying the reliability of a high-quantile estimate is as important as the estimate itself. [@problem_id:3177964]", "problem": "A dataset of $n$ normalized daily loss magnitudes is given by\n$0.80,\\, 0.55,\\, 1.20,\\, 0.30,\\, 0.95,\\, 0.67,\\, 1.50,\\, 0.40,\\, 0.72,\\, 0.88,\\, 1.10,\\, 0.33,\\, 0.60,\\, 0.77,\\, 0.25,\\, 1.30,\\, 0.52,\\, 0.66,\\, 0.90,\\, 0.45$.\nYou are tasked to estimate a high-quantile risk and quantify its uncertainty using fundamental definitions, then validate the magnitude of your uncertainty estimate against a nonparametric bootstrap summary.\n\nUsing the empirical cumulative distribution function (CDF; cumulative distribution function) inversion, estimate the $p$-quantile with $p = 0.90$ from the sample. Define the downstream risk metric as $R(q) = \\exp(\\beta q)$ with $\\beta = 0.50$, evaluated at your empirical quantile estimate. Starting from first principles, use the delta method to derive and compute the standard error of $\\widehat{R}$ by propagating the uncertainty in the empirical quantile through $R(\\cdot)$. To estimate the density $f(q)$ at the quantile for the variance of the sample quantile, use the symmetric spacing of nearest order statistics (that is, approximate $f(q)$ locally by the finite difference of the empirical CDF across $x_{(r-1)}$ and $x_{(r+1)}$, where $r$ is the index of the empirical $p$-quantile). As a magnitude check, a nonparametric bootstrap with $B = 1000$ resamples yields a sample standard deviation of the risk metric of $0.121$.\n\nCompute and report the delta-method standard error of $\\widehat{R}$, and round your answer to four significant figures. No physical units are involved; provide a unitless numerical value.", "solution": "The objective is to compute the standard error of an estimated risk metric, $\\widehat{R} = R(\\widehat{q}_p)$, using the delta method. The process involves several steps: finding the sample quantile, deriving the form of the delta method approximation for the standard error, estimating the components of this approximation, and finally computing the numerical value.\n\nFirst, we must determine the sample size and sort the given data to find the order statistics, $x_{(i)}$. The sample size is $n = 20$. The sorted dataset is:\n$x_{(1)} = 0.25$, $x_{(2)} = 0.30$, $x_{(3)} = 0.33$, $x_{(4)} = 0.40$, $x_{(5)} = 0.45$,\n$x_{(6)} = 0.52$, $x_{(7)} = 0.55$, $x_{(8)} = 0.60$, $x_{(9)} = 0.66$, $x_{(10)} = 0.67$,\n$x_{(11)} = 0.72$, $x_{(12)} = 0.77$, $x_{(13)} = 0.80$, $x_{(14)} = 0.88$, $x_{(15)} = 0.90$,\n$x_{(16)} = 0.95$, $x_{(17)} = 1.10$, $x_{(18)} = 1.20$, $x_{(19)} = 1.30$, $x_{(20)} = 1.50$.\n\nThe problem specifies estimating the $p=0.90$ quantile, $\\widehat{q}_{0.90}$, using empirical CDF inversion. A common definition for the sample $p$-quantile is the $r$-th order statistic, where $r = \\lceil n \\cdot p \\rceil$.\nFor $n=20$ and $p=0.90$, the index is:\n$$r = \\lceil 20 \\times 0.90 \\rceil = \\lceil 18 \\rceil = 18$$\nThus, the estimated $0.90$-quantile is the $18$-th order statistic:\n$$\\widehat{q}_{0.90} = x_{(18)} = 1.20$$\n\nThe risk metric is $R(q) = \\exp(\\beta q)$ with $\\beta=0.50$. The estimated risk is:\n$$\\widehat{R} = R(\\widehat{q}_{0.90}) = \\exp(0.50 \\times 1.20) = \\exp(0.60)$$\n\nThe delta method approximates the variance of a function $g(\\widehat{\\theta})$ of an estimator $\\widehat{\\theta}$. For a univariate estimator, the approximation is $\\text{Var}(g(\\widehat{\\theta})) \\approx [g'(\\theta)]^2 \\text{Var}(\\widehat{\\theta})$.\nIn our context, $\\widehat{\\theta}$ is the sample quantile $\\widehat{q}_p$ and $g(\\cdot)$ is the risk function $R(\\cdot)$. Thus, the variance of $\\widehat{R}$ is:\n$$\\text{Var}(\\widehat{R}) \\approx [R'(q_p)]^2 \\text{Var}(\\widehat{q}_p)$$\nwhere $q_p$ is the true population quantile. The standard error is the square root of the variance.\n\nFirst, we find the derivative of $R(q)$:\n$$R'(q) = \\frac{d}{dq} \\exp(\\beta q) = \\beta \\exp(\\beta q)$$\nWe evaluate this using our estimates:\n$$R'(\\widehat{q}_{0.90}) = 0.50 \\times \\exp(0.50 \\times 1.20) = 0.50 \\exp(0.60)$$\n\nNext, we need the variance of the sample quantile, $\\text{Var}(\\widehat{q}_p)$. The large-sample approximation for this variance is:\n$$\\text{Var}(\\widehat{q}_p) \\approx \\frac{p(1-p)}{n [f(q_p)]^2}$$\nwhere $f(q_p)$ is the value of the probability density function at the true quantile $q_p$. We need to estimate $f(q_p)$. The problem directs us to use the symmetric spacing of nearest order statistics around our estimated quantile $x_{(r)}$. This gives the density estimate:\n$$\\widehat{f}(x_{(r)}) = \\frac{(r+1)/n - (r-1)/n}{x_{(r+1)} - x_{(r-1)}} = \\frac{2/n}{x_{(r+1)} - x_{(r-1)}}$$\nFor our quantile $\\widehat{q}_{0.90} = x_{(18)}$, we have $r=18$. We need the adjacent order statistics, $x_{(17)}$ and $x_{(19)}$.\nFrom the sorted list, $x_{(17)} = 1.10$ and $x_{(19)} = 1.30$.\nSubstituting these values with $n=20$:\n$$\\widehat{f}(\\widehat{q}_{0.90}) = \\frac{2/20}{x_{(19)} - x_{(17)}} = \\frac{0.1}{1.30 - 1.10} = \\frac{0.1}{0.2} = 0.50$$\n\nNow we can estimate the variance of the sample quantile:\n$$\\widehat{\\text{Var}}(\\widehat{q}_{0.90}) = \\frac{p(1-p)}{n [\\widehat{f}(\\widehat{q}_{0.90})]^2} = \\frac{0.90 \\times (1-0.90)}{20 \\times (0.50)^2} = \\frac{0.09}{20 \\times 0.25} = \\frac{0.09}{5} = 0.018$$\n\nFinally, we combine these results to find the variance of the estimated risk metric $\\widehat{R}$:\n$$\\widehat{\\text{Var}}(\\widehat{R}) \\approx [R'(\\widehat{q}_{0.90})]^2 \\widehat{\\text{Var}}(\\widehat{q}_{0.90})$$\n$$\\widehat{\\text{Var}}(\\widehat{R}) \\approx [0.50 \\exp(0.60)]^2 \\times 0.018$$\nThe standard error, $\\text{SE}(\\widehat{R})$, is the square root of the variance:\n$$\\text{SE}(\\widehat{R}) = \\sqrt{\\widehat{\\text{Var}}(\\widehat{R})} \\approx \\sqrt{[0.50 \\exp(0.60)]^2 \\times 0.018}$$\n$$\\text{SE}(\\widehat{R}) \\approx |0.50 \\exp(0.60)| \\sqrt{0.018} = 0.50 \\exp(0.60) \\sqrt{0.018}$$\nNow we compute the numerical value:\n$\\exp(0.60) \\approx 1.8221188$\n$\\sqrt{0.018} \\approx 0.1341640786$\n$\\text{SE}(\\widehat{R}) \\approx 0.50 \\times 1.8221188 \\times 0.1341640786 \\approx 0.122235$\nRounding to four significant figures, we get $0.1222$. This value is consistent with the provided bootstrap estimate of $0.121$, confirming the reasonableness of our calculation.", "answer": "$$\\boxed{0.1222}$$", "id": "3177964"}, {"introduction": "While the concept of a population quantile is unique, its estimation from a finite sample is not. This hands-on practice explores how different, equally valid, interpolation methods for calculating empirical quantiles can lead to different threshold values. You will implement these methods and observe their tangible impact on a downstream fairness metric, demonstrating that seemingly small methodological choices can have significant real-world consequences. [@problem_id:3177907]", "problem": "You are given two finite sets of real-valued scores representing two demographic groups, and a global decision rule that accepts an individual if and only if their score is greater than or equal to a threshold defined by a specified empirical quantile of the pooled scores across both groups. The primary goal is to compare empirical quantiles computed under different interpolation rules and quantify their downstream effect on a fairness metric that uses percentile thresholds.\n\nStarting point and definitions:\n- Let $X$ be a real-valued random variable with cumulative distribution function (CDF) $F(x) = \\mathbb{P}(X \\le x)$. For a quantile level $q \\in [0,1]$, the $q$-quantile is any value $x_q$ such that $F(x_q) \\ge q$ and $F(x_q^{-}) \\le q$, where $x_q^{-}$ denotes the limit from the left.\n- Given a finite sample of size $n$, the empirical cumulative distribution function (ECDF) is $$F_n(x) = \\frac{1}{n} \\sum_{i=1}^{n} \\mathbf{1}\\{X_i \\le x\\},$$ where $\\mathbf{1}\\{\\cdot\\}$ is the indicator function and $X_i$ are the observed scores. The empirical $q$-quantile is defined using $F_n$ and an interpolation rule that specifies how to handle non-integer ranks between neighboring order statistics.\n- Interpolation rules to be compared are the following widely used conventions: \"lower\" (choose the greatest observed value not exceeding the target rank), \"higher\" (choose the smallest observed value not less than the target rank), \"nearest\" (choose the closest observed rank), \"midpoint\" (average of the neighboring observed values bracketing the target rank), and \"linear\" (linear interpolation between the two neighboring observed values according to the fractional part of the target rank). All rules operate on the sorted pooled scores and differ only in how they resolve fractional ranks between order statistics.\n\nFairness metric to evaluate:\n- The fairness metric is the Demographic Parity (DP) difference. For groups $\\mathcal{A}$ and $\\mathcal{B}$ with acceptance rates $r_{\\mathcal{A}}$ and $r_{\\mathcal{B}}$, define the DP difference as $d = |r_{\\mathcal{A}} - r_{\\mathcal{B}}|$. Acceptance rates must be expressed as decimal fractions (for example, $0.6$ for sixty percent).\n- Acceptance is determined by a global threshold $T_q$ computed as the empirical $q$-quantile of the pooled scores from both groups. An individual is accepted if and only if their score is greater than or equal to $T_q$.\n\nTask:\n- For each test case, compute $T_q$ under each interpolation rule among $\\{\\text{linear}, \\text{lower}, \\text{higher}, \\text{nearest}, \\text{midpoint}\\}$ on the pooled scores of the two groups.\n- For each rule, compute the acceptance rates $r_{\\mathcal{A}}$ and $r_{\\mathcal{B}}$, then the DP difference $d$.\n- Let $d_{\\text{linear}}$ be the DP difference when using the \"linear\" rule. Quantify the effect of interpolation on the fairness metric for that test case by computing\n$$\\Delta = \\max_{m \\in \\{\\text{lower}, \\text{higher}, \\text{nearest}, \\text{midpoint}\\}} \\left| d_{m} - d_{\\text{linear}} \\right|,$$\nwhere $d_{m}$ is the DP difference under method $m$.\n- The final output for the entire test suite is a single list of the $\\Delta$ values, one per test case, printed on a single line as a comma-separated list enclosed in square brackets, for example $[0.0,0.1,0.05]$.\n\nTest suite:\n- Case $1$ (general case):\n  - Group $\\mathcal{A}$ scores: [$0.10$, $0.20$, $0.35$, $0.40$, $0.50$]\n  - Group $\\mathcal{B}$ scores: [$0.05$, $0.25$, $0.30$, $0.45$, $0.55$]\n  - Quantile level $q$: $0.70$\n- Case $2$ (boundary with small sample and median threshold):\n  - Group $\\mathcal{A}$ scores: [$0.20$, $0.80$, $0.90$]\n  - Group $\\mathcal{B}$ scores: [$0.10$, $0.70$, $0.95$]\n  - Quantile level $q$: $0.50$\n- Case $3$ (ties and repeated values):\n  - Group $\\mathcal{A}$ scores: [$0.30$, $0.30$, $0.30$, $0.60$, $0.60$]\n  - Group $\\mathcal{B}$ scores: [$0.40$, $0.40$, $0.40$, $0.40$, $0.70$]\n  - Quantile level $q$: $0.60$\n- Case $4$ (extreme upper quantile):\n  - Group $\\mathcal{A}$ scores: [$0.20$, $0.40$, $0.60$, $0.80$]\n  - Group $\\mathcal{B}$ scores: [$0.10$, $0.30$, $0.50$, $0.70$]\n  - Quantile level $q$: $0.95$\n\nPrecise output format requirement:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[r_1,r_2,r_3,r_4]$), where each $r_i$ is the computed $\\Delta$ for the $i$-th test case.", "solution": "The problem requires an analysis of the effect of different empirical quantile interpolation methods on a fairness metric, specifically the Demographic Parity (DP) difference. For each provided test case, we must calculate a metric $\\Delta$, which represents the maximum deviation in the DP difference caused by using alternative interpolation methods ('lower', 'higher', 'nearest', 'midpoint') relative to a baseline 'linear' interpolation method.\n\nThe process for each test case is as follows:\n1.  Combine the scores from Group $\\mathcal{A}$ and Group $\\mathcal{B}$ into a single pooled dataset.\n2.  For each of the five specified interpolation methods ($m \\in \\{\\text{linear}, \\text{lower}, \\text{higher}, \\text{nearest}, \\text{midpoint}\\}$), calculate the decision threshold $T_q^{(m)}$. This threshold is the empirical $q$-quantile of the pooled scores, computed using method $m$.\n3.  For each threshold $T_q^{(m)}$, determine the acceptance rates for each group, $r_{\\mathcal{A}}^{(m)}$ and $r_{\\mathcal{B}}^{(m)}$. An individual is accepted if their score is greater than or equal to the threshold. The acceptance rate is the fraction of individuals in a group who are accepted.\n4.  Calculate the DP difference for each method, $d_m = |r_{\\mathcal{A}}^{(m)} - r_{\\mathcal{B}}^{(m)}|$.\n5.  Finally, compute the target metric $\\Delta = \\max_{m \\in \\{\\text{lower}, \\text{higher}, \\text{nearest}, \\text{midpoint}\\}} |d_m - d_{\\text{linear}}|$.\n\nThe core of the calculation is the computation of the empirical $q$-quantile. Let the pooled and sorted sample of size $n$ be denoted by the 0-indexed sequence $S = (x_0, x_1, \\dots, x_{n-1})$. The quantile value is determined by an index $k = q(n-1)$. Let $i = \\lfloor k \\rfloor$ be the floor and $j = \\lceil k \\rceil$ be the ceiling of this index. The thresholds $T_q$ for the different interpolation methods are then calculated as:\n- Linear: $T_q = x_i + (k-i)(x_j - x_i)$\n- Lower: $T_q = x_i$\n- Higher: $T_q = x_j$\n- Midpoint: $T_q = (x_i + x_j) / 2$\n- Nearest: $T_q = x_{\\text{round}(k)}$, where halves are rounded to the nearest even integer.\n\nWe now apply this methodology to each test case.\n\n**Case 1:**\n- Group $\\mathcal{A}$ scores: [$0.10, 0.20, 0.35, 0.40, 0.50$] ($n_{\\mathcal{A}} = 5$)\n- Group $\\mathcal{B}$ scores: [$0.05, 0.25, 0.30, 0.45, 0.55$] ($n_{\\mathcal{B}} = 5$)\n- Quantile level $q = 0.70$\nThe pooled and sorted scores are $S = (0.05, 0.10, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.50, 0.55)$, with size $n=10$.\nThe index is $k = 0.70 \\times (10-1) = 6.3$. Thus, $i=6$ and $j=7$. The values are $x_6 = 0.40$ and $x_7 = 0.45$.\n- $T_q^{(\\text{linear})} = 0.40 + (6.3-6)(0.45-0.40) = 0.415$.\n  $r_{\\mathcal{A}} = 1/5 = 0.2$, $r_{\\mathcal{B}} = 2/5 = 0.4$. $d_{\\text{linear}} = |0.2 - 0.4| = 0.2$.\n- $T_q^{(\\text{lower})} = x_6 = 0.40$.\n  $r_{\\mathcal{A}} = 2/5 = 0.4$, $r_{\\mathcal{B}} = 2/5 = 0.4$. $d_{\\text{lower}} = |0.4 - 0.4| = 0.0$.\n- $T_q^{(\\text{higher})} = x_7 = 0.45$.\n  $r_{\\mathcal{A}} = 1/5 = 0.2$, $r_{\\mathcal{B}} = 2/5 = 0.4$. $d_{\\text{higher}} = |0.2 - 0.4| = 0.2$.\n- $T_q^{(\\text{nearest})} = x_{\\text{round}(6.3)} = x_6 = 0.40$.\n  $d_{\\text{nearest}} = d_{\\text{lower}} = 0.0$.\n- $T_q^{(\\text{midpoint})} = (0.40+0.45)/2 = 0.425$.\n  $r_{\\mathcal{A}} = 1/5 = 0.2$, $r_{\\mathcal{B}} = 2/5 = 0.4$. $d_{\\text{midpoint}} = |0.2 - 0.4| = 0.2$.\nThe differences from $d_{\\text{linear}}$ are $|0.0 - 0.2| = 0.2$, $|0.2 - 0.2| = 0.0$, $|0.0 - 0.2| = 0.2$, and $|0.2 - 0.2| = 0.0$.\n$\\Delta = \\max(0.2, 0.0, 0.2, 0.0) = 0.2$.\n\n**Case 2:**\n- Group $\\mathcal{A}$ scores: [$0.20, 0.80, 0.90$] ($n_{\\mathcal{A}} = 3$)\n- Group $\\mathcal{B}$ scores: [$0.10, 0.70, 0.95$] ($n_{\\mathcal{B}} = 3$)\n- Quantile level $q = 0.50$\nThe pooled and sorted scores are $S = (0.10, 0.20, 0.70, 0.80, 0.90, 0.95)$, with size $n=6$.\nThe index is $k = 0.50 \\times (6-1) = 2.5$. Thus, $i=2$ and $j=3$. The values are $x_2 = 0.70$ and $x_3 = 0.80$.\n- $T_q^{(\\text{linear})} = 0.70 + (2.5-2)(0.80-0.70) = 0.75$.\n  $r_{\\mathcal{A}} = 2/3$, $r_{\\mathcal{B}} = 1/3$. $d_{\\text{linear}} = |2/3 - 1/3| = 1/3$.\n- $T_q^{(\\text{lower})} = x_2 = 0.70$.\n  $r_{\\mathcal{A}} = 2/3$, $r_{\\mathcal{B}} = 2/3$. $d_{\\text{lower}} = |2/3 - 2/3| = 0.0$.\n- $T_q^{(\\text{higher})} = x_3 = 0.80$.\n  $r_{\\mathcal{A}} = 2/3$, $r_{\\mathcal{B}} = 1/3$. $d_{\\text{higher}} = |2/3 - 1/3| = 1/3$.\n- $T_q^{(\\text{nearest})} = x_{\\text{round}(2.5)} = x_2 = 0.70$.\n  $d_{\\text{nearest}} = d_{\\text{lower}} = 0.0$.\n- $T_q^{(\\text{midpoint})} = (0.70+0.80)/2 = 0.75$.\n  $d_{\\text{midpoint}} = d_{\\text{linear}} = 1/3$.\nThe differences from $d_{\\text{linear}}$ are $|0.0 - 1/3| = 1/3$, $|1/3 - 1/3| = 0.0$, $|0.0 - 1/3| = 1/3$, and $|1/3 - 1/3| = 0.0$.\n$\\Delta = \\max(1/3, 0.0, 1/3, 0.0) = 1/3 \\approx 0.333...$.\n\n**Case 3:**\n- Group $\\mathcal{A}$ scores: [$0.30, 0.30, 0.30, 0.60, 0.60$] ($n_{\\mathcal{A}} = 5$)\n- Group $\\mathcal{B}$ scores: [$0.40, 0.40, 0.40, 0.40, 0.70$] ($n_{\\mathcal{B}} = 5$)\n- Quantile level $q = 0.60$\nThe pooled and sorted scores are $S = (0.30, 0.30, 0.30, 0.40, 0.40, 0.40, 0.40, 0.60, 0.60, 0.70)$, with size $n=10$.\nThe index is $k = 0.60 \\times (10-1) = 5.4$. Thus, $i=5$ and $j=6$. The values are $x_5 = 0.40$ and $x_6 = 0.40$.\nSince $x_i = x_j$, all interpolation methods yield the same threshold $T_q = 0.40$.\nConsequently, $d_m$ is the same for all methods $m$.\nFor $T_q = 0.40$:\n$r_{\\mathcal{A}} = 2/5 = 0.4$.\n$r_{\\mathcal{B}} = 5/5 = 1.0$.\n$d = |0.4 - 1.0| = 0.6$.\nSince $d_m=0.6$ for all $m$, $d_m - d_{\\text{linear}} = 0$ for all other methods.\n$\\Delta = 0.0$.\n\n**Case 4:**\n- Group $\\mathcal{A}$ scores: [$0.20, 0.40, 0.60, 0.80$] ($n_{\\mathcal{A}} = 4$)\n- Group $\\mathcal{B}$ scores: [$0.10, 0.30, 0.50, 0.70$] ($n_{\\mathcal{B}} = 4$)\n- Quantile level $q = 0.95$\nThe pooled and sorted scores are $S = (0.10, 0.20, 0.30, 0.40, 0.50, 0.60, 0.70, 0.80)$, with size $n=8$.\nThe index is $k = 0.95 \\times (8-1) = 6.65$. Thus, $i=6$ and $j=7$. The values are $x_6 = 0.70$ and $x_7 = 0.80$.\n- $T_q^{(\\text{linear})} = 0.70 + (6.65-6)(0.80-0.70) = 0.765$.\n  $r_{\\mathcal{A}} = 1/4 = 0.25$, $r_{\\mathcal{B}} = 0/4 = 0.0$. $d_{\\text{linear}} = |0.25 - 0.0| = 0.25$.\n- $T_q^{(\\text{lower})} = x_6 = 0.70$.\n  $r_{\\mathcal{A}} = 1/4 = 0.25$, $r_{\\mathcal{B}} = 1/4 = 0.25$. $d_{\\text{lower}} = |0.25 - 0.25| = 0.0$.\n- $T_q^{(\\text{higher})} = x_7 = 0.80$.\n  $r_{\\mathcal{A}} = 1/4 = 0.25$, $r_{\\mathcal{B}} = 0/4 = 0.0$. $d_{\\text{higher}} = |0.25 - 0.0| = 0.25$.\n- $T_q^{(\\text{nearest})} = x_{\\text{round}(6.65)} = x_7 = 0.80$.\n  $d_{\\text{nearest}} = d_{\\text{higher}} = 0.25$.\n- $T_q^{(\\text{midpoint})} = (0.70+0.80)/2 = 0.75$.\n  $r_{\\mathcal{A}} = 1/4 = 0.25$, $r_{\\mathcal{B}} = 0/4 = 0.0$. $d_{\\text{midpoint}} = |0.25 - 0.0| = 0.25$.\nThe differences from $d_{\\text{linear}}$ are $|0.0 - 0.25| = 0.25$, $|0.25 - 0.25| = 0.0$, $|0.25 - 0.25| = 0.0$, and $|0.25 - 0.25| = 0.0$.\n$\\Delta = \\max(0.25, 0.0, 0.0, 0.0) = 0.25$.\n\nThe final calculated $\\Delta$ values for the four test cases are $[0.2, 0.333..., 0.0, 0.25]$.", "answer": "[0.2, 0.3333333333333333, 0.0, 0.25]", "id": "3177907"}, {"introduction": "Percentiles are not just for summarizing distributions; they are a powerful tool for robust data transformation. This coding exercise demonstrates how converting raw prediction scores to their empirical percentiles can normalize them and make them comparable across different scales, a common challenge in machine learning. You will confirm the rank-based nature of the Area Under the Curve (AUC) metric and explore how group-wise percentile normalization can improve a model's predictive power. [@problem_id:3177941]", "problem": "You are given binary classification datasets consisting of a vector of labels and a corresponding vector of real-valued prediction scores. Your tasks are to (i) derive and implement an empirical percentile transformation of prediction scores, (ii) compute the Area Under the Receiver Operating Characteristic Curve (AUC; defined below) for both raw scores and percentile-transformed scores, and (iii) assess the stability of AUC under this transformation. Additionally, in a multi-group setting where raw score scales differ by group, implement a groupwise percentile normalization to explore whether percentile-based ranking is more informative than raw scores.\n\nFundamental base and definitions to use:\n- Let $N$ be the number of observations, indexed by $i \\in \\{1,\\dots,N\\}$. Each observation has a binary label $Y_i \\in \\{0,1\\}$ and a real-valued prediction score $S_i \\in \\mathbb{R}$.\n- The empirical cumulative distribution function (ECDF) of the scores is defined as $$F_N(s) = \\frac{1}{N}\\sum_{j=1}^{N} \\mathbf{1}\\{S_j \\le s\\},$$ where $\\mathbf{1}\\{\\cdot\\}$ is the indicator function. A percentile is a quantile expressed as a fraction in $[0,1]$; in this task, express percentiles as decimals in $[0,1]$ (not with a percentage sign).\n- Use midranks to handle ties: if the sorted positions of tied scores span integer ranks $r_{\\text{low}},\\dots,r_{\\text{high}}$, assign each tied score the midrank $r_{\\text{mid}} = (r_{\\text{low}} + r_{\\text{high}})/2$. Define the global empirical percentile of $S_i$ as $$P_i = \\frac{r_i - 0.5}{N},$$ where $r_i$ is the midrank of $S_i$ among all $N$ scores. In the multi-group case with groups $G_i \\in \\{0,1,\\dots\\}$, define the groupwise percentile as $$P^{(g)}_i = \\frac{r^{(g)}_i - 0.5}{N_g},$$ where $r^{(g)}_i$ is the midrank of $S_i$ within its group $g = G_i$, and $N_g$ is the number of observations in group $g$.\n- The Receiver Operating Characteristic (ROC) curve (defined here as the set of achievable true positive rate and false positive rate pairs by thresholding scores) has Area Under the Curve (AUC), which can be defined as the probability that a randomly chosen positive-labeled score exceeds a randomly chosen negative-labeled score, with ties broken by assigning half credit. Using ranks, let $m = \\sum_{i=1}^{N} \\mathbf{1}\\{Y_i = 1\\}$ and $n = \\sum_{i=1}^{N} \\mathbf{1}\\{Y_i = 0\\}$, and let $R^+ = \\sum_{i: Y_i=1} r_i$ be the sum of midranks of the positive-labeled observations; then the AUC is given by $$\\text{AUC} = \\frac{R^+ - \\frac{m(m+1)}{2}}{m \\cdot n}.$$\n\nYour program must, for each test case below:\n1. Compute the raw-score AUC using the midrank-based formula.\n2. Compute the global percentile scores $P_i$ and the corresponding AUC, and then the absolute difference $$\\Delta_{\\text{global}} = \\left|\\text{AUC}_{\\text{raw}} - \\text{AUC}_{\\text{global}}\\right|.$$\n3. Compute the groupwise percentile scores $P^{(g)}_i$ and the corresponding AUC, and the improvement $$\\Delta_{\\text{group}} = \\text{AUC}_{\\text{group}} - \\text{AUC}_{\\text{raw}}.$$\n4. Report all AUCs and differences as decimals.\n\nTest suite (each case provides labels $Y$, scores $S$, and integer group labels $G$):\n- Case $1$ (continuous, mostly unique scores; single group):\n  - $Y = [0,0,0,0,1,1,1,1,1,0,1,0]$\n  - $S = [0.10,0.15,0.20,0.25,0.30,0.22,0.27,0.35,0.40,0.12,0.33,0.18]$\n  - $G = [0,0,0,0,0,0,0,0,0,0,0,0]$\n- Case $2$ (many ties; single group):\n  - $Y = [0,0,1,1,0,1,0,1]$\n  - $S = [0.50,0.50,0.50,0.50,0.70,0.70,0.20,0.20]$\n  - $G = [0,0,0,0,0,0,0,0]$\n- Case $3$ (extreme outlier; single group):\n  - $Y = [0,0,0,0,1,1,1,1,1,0]$\n  - $S = [0.01,0.02,0.03,100.00,0.04,0.05,0.06,0.07,0.08,0.90]$\n  - $G = [0,0,0,0,0,0,0,0,0,0]$\n- Case $4$ (degenerate constant scores; single group):\n  - $Y = [0,0,0,1,1,1,0,1]$\n  - $S = [0.30,0.30,0.30,0.30,0.30,0.30,0.30,0.30]$\n  - $G = [0,0,0,0,0,0,0,0]$\n- Case $5$ (two groups with different raw score scales; percentile ranking expected to be more informative):\n  - $Y = [0,0,0,0,1,1,1,1,0,0,0,0,1,1,1,1]$\n  - $S = [20,22,18,24,35,37,33,39,1030,1050,1100,1080,1005,1010,995,1020]$\n  - $G = [0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1]$\n\nFinal output format:\n- Your program should produce a single line of output containing a list of lists. For each test case, output the list $[\\text{AUC}_{\\text{raw}}, \\text{AUC}_{\\text{global}}, \\Delta_{\\text{global}}, \\text{AUC}_{\\text{group}}, \\Delta_{\\text{group}}]$ in this order, all as decimals. Aggregate the five case results into a single list and print it exactly as one line, for example, $$[[a_1,b_1,c_1,d_1,e_1],[a_2,b_2,c_2,d_2,e_2],\\dots].$$", "solution": "The problem statement has been critically validated and is deemed valid. It is scientifically grounded in established statistical principles, well-posed with clear definitions and data, and objective in its formulation. All necessary components, including definitions for empirical percentiles using midranks and the rank-based formula for the Area Under the ROC Curve (AUC), are provided. The test cases are well-defined and cover a range of scenarios relevant to the concepts being tested.\n\nThe solution proceeds by first implementing a function to calculate the AUC according to the provided formula. This function will be the core computational block. Subsequently, for each test case, we will compute the required statistical measures in three distinct steps: once for the raw prediction scores, once for the globally transformed percentile scores, and once for the groupwise transformed percentile scores.\n\n### 1. AUC Calculation\nThe AUC is given by the formula:\n$$\n\\text{AUC} = \\frac{R^+ - \\frac{m(m+1)}{2}}{m \\cdot n}\n$$\nwhere $m$ is the count of positive labels ($Y_i=1$), $n$ is the count of negative labels ($Y_i=0$), and $R^+$ is the sum of the midranks of the scores $S_i$ corresponding to the positive labels. Midranks are used to handle ties in score values. If a set of tied scores occupies ranks from $r_{\\text{low}}$ to $r_{\\text{high}}$, each score is assigned the average rank $r_{\\text{mid}} = (r_{\\text{low}} + r_{\\text{high}})/2$. The implementation will use `scipy.stats.rankdata` with `method='average'` to compute these midranks efficiently and correctly.\n\n### 2. Global Percentile Transformation\nThe global percentile $P_i$ for a score $S_i$ is defined as:\n$$\nP_i = \\frac{r_i - 0.5}{N}\n$$\nwhere $r_i$ is the global midrank of $S_i$ and $N$ is the total number of observations. This transformation is a strictly order-preserving (monotonic) function of the ranks. Since the AUC formula is based entirely on ranks, any such transformation will not change the ranks of the scores relative to each other. Consequently, the AUC calculated on the raw scores, $\\text{AUC}_{\\text{raw}}$, must be identical to the AUC calculated on the global percentile scores, $\\text{AUC}_{\\text{global}}$. Therefore, the absolute difference, $\\Delta_{\\text{global}} = |\\text{AUC}_{\\text{raw}} - \\text{AUC}_{\\text{global}}|$, is theoretically expected to be $0$.\n\n### 3. Groupwise Percentile Transformation\nFor an observation $i$ belonging to group $g = G_i$, the groupwise percentile is:\n$$\nP^{(g)}_i = \\frac{r^{(g)}_i - 0.5}{N_g}\n$$\nHere, $r^{(g)}_i$ is the midrank of $S_i$ calculated **only** among the scores within group $g$, and $N_g$ is the number of observations in that group. This transformation rescales scores within each group to a common $[0, 1]$ range. Unlike the global transformation, this does **not** preserve the overall rank ordering of scores across different groups. A high score from a low-scale group might receive a higher percentile rank than a low score from a high-scale group. This can alter the overall AUC. We will compute the new set of scores, $P^{(g)}_i$, and then calculate $\\text{AUC}_{\\text{group}}$ using these new scores. The improvement is measured by $\\Delta_{\\text{group}} = \\text{AUC}_{\\text{group}} - \\text{AUC}_{\\text{raw}}$.\n\n### 4. Algorithmic Implementation\nFor each test case, the algorithm is as follows:\n1.  Receive the vectors of labels $Y$, scores $S$, and group assignments $G$.\n2.  **Raw AUC**: Compute midranks for all scores $S$ to find the ranks of the positive samples, sum them to get $R^+$, and calculate $\\text{AUC}_{\\text{raw}}$ using the provided formula.\n3.  **Global Percentile AUC**:\n    a. Use the same global midranks $r_i$ from the previous step.\n    b. Compute the global percentile scores $P_i = (r_i - 0.5) / N$.\n    c. Calculate $\\text{AUC}_{\\text{global}}$ using labels $Y$ and scores $P$.\n    d. Compute $\\Delta_{\\text{global}} = |\\text{AUC}_{\\text{raw}} - \\text{AUC}_{\\text{global}}|$.\n4.  **Groupwise Percentile AUC**:\n    a. Initialize a new score vector $P_{\\text{group}}$.\n    b. For each unique group $g$ in $G$:\n        i. Isolate the scores $S_g$ and labels $Y_g$ for that group.\n        ii. Compute midranks $r^{(g)}_i$ for the scores $S_g$.\n        iii. Calculate the group-specific percentiles $P^{(g)}_i = (r^{(g)}_i - 0.5) / N_g$.\n        iv. Place these percentile scores back into the corresponding positions in $P_{\\text{group}}$.\n    c. Calculate $\\text{AUC}_{\\text{group}}$ using labels $Y$ and scores $P_{\\text{group}}$.\n    d. Compute $\\Delta_{\\text{group}} = \\text{AUC}_{\\text{group}} - \\text{AUC}_{\\text{raw}}$.\n5.  Collect the five resulting values: $[\\text{AUC}_{\\text{raw}}, \\text{AUC}_{\\text{global}}, \\Delta_{\\text{global}}, \\text{AUC}_{\\text{group}}, \\Delta_{\\text{group}}]$.\n\nThis procedure is systematically applied to all test cases, and the results are aggregated into a final list for output.", "answer": "[[0.9444444444444444, 0.9444444444444444, 0.0, 0.9444444444444444, 0.0], [0.625, 0.625, 0.0, 0.625, 0.0], [0.8, 0.8, 0.0, 0.8, 0.0], [0.5, 0.5, 0.0, 0.5, 0.0], [0.5625, 0.5625, 0.0, 0.9375, 0.375]]", "id": "3177941"}]}