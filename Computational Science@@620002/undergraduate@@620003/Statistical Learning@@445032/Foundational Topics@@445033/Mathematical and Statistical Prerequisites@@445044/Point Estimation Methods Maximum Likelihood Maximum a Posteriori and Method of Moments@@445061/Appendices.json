{"hands_on_practices": [{"introduction": "We begin our practice with a foundational model: the Normal distribution. In this exercise [@problem_id:3157673], you will estimate the variance parameter $\\sigma^2$ under the simplifying assumption that the mean $\\mu$ is known. This allows for a focused and clear application of the Method of Moments (MoM), Maximum Likelihood Estimation (MLE), and Maximum a Posteriori (MAP) estimation, providing a direct comparison of their results. Furthermore, this problem guides you through an analysis of the MAP estimator's bias, a critical concept for evaluating the quality of any point estimator.", "problem": "Consider independent and identically distributed observations $X_{1},\\dots,X_{n}$ from a Normal (Gaussian) distribution with known mean $\\,\\mu\\,$ and unknown variance $\\,\\sigma^{2}\\,$, that is $\\,X_{i}\\sim \\mathcal{N}(\\mu,\\sigma^{2})\\,$ for $\\,i=1,\\dots,n\\,$. Define the centered sum of squares $\\,Q=\\sum_{i=1}^{n}(X_{i}-\\mu)^{2}\\,$ and the empirical second central moment $\\,S^{2}=\\frac{1}{n}\\sum_{i=1}^{n}(X_{i}-\\mu)^{2}=\\frac{Q}{n}\\,$.\n\nUsing only fundamental principles of likelihood and moments, do the following:\n\n1) Using the definition of the likelihood for the Normal model and the rule for maximizing a differentiable function via its derivative, derive the maximum likelihood estimator for $\\,\\sigma^{2}\\,$. Using the definition of the method of moments, equate the sample second central moment to its population counterpart and derive the method-of-moments estimator for $\\,\\sigma^{2}\\,$ based on $\\,S^{2}\\,$. State the relationship between these two estimators.\n\n2) Place a prior on $\\,\\sigma^{2}\\,$ given by an Inverse-Gamma distribution with shape $\\,\\alpha>0\\,$ and scale $\\,\\beta>0\\,$, whose density is $\\,p(\\sigma^{2})=\\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)}(\\sigma^{2})^{-(\\alpha+1)}\\exp\\!\\big(-\\beta/\\sigma^{2}\\big)\\,$ for $\\,\\sigma^{2}>0\\,$, where $\\,\\Gamma(\\cdot)\\,$ is the Gamma function. Using Bayesâ€™ rule and calculus on the log-posterior, derive the maximum a posteriori estimator (posterior mode) of $\\,\\sigma^{2}\\,$ as a function of $\\,Q\\,$, $\\,\\alpha\\,$, $\\,\\beta\\,$, and $\\,n\\,$.\n\n3) Treating the data as random under the true variance $\\,\\sigma^{2}\\,$, use the distributional property of $\\,Q\\,$ for Normal data with known mean to compute the bias of the maximum a posteriori estimator relative to the true parameter, defined as $\\,\\mathbb{E}_{\\sigma^{2}}[\\widehat{\\sigma^{2}}_{\\mathrm{MAP}}]-\\sigma^{2}\\,$, in closed form as a symbolic expression in $\\,\\alpha\\,$, $\\,\\beta\\,$, $\\,n\\,$, and $\\,\\sigma^{2}\\,$.\n\nProvide only the final symbolic expression for the bias as your final answer. No numerical rounding is required. The final answer must be a single analytic expression.", "solution": "The problem statement is scientifically sound, self-contained, and well-posed, setting forth a standard problem in statistical inference that can be solved using established principles. We proceed with the derivation.\n\nLet the observations be $X_{1}, \\dots, X_{n}$, which are independent and identically distributed (i.i.d.) from a Normal distribution $\\mathcal{N}(\\mu, \\sigma^{2})$ where the mean $\\mu$ is known and the variance $\\sigma^{2}$ is unknown.\n\n**1) Maximum Likelihood and Method-of-Moments Estimators**\n\nFirst, we derive the Maximum Likelihood Estimator (MLE) for $\\sigma^{2}$. The probability density function for a single observation $X_{i}$ is:\n$$f(x_{i} | \\sigma^{2}) = \\frac{1}{\\sqrt{2\\pi\\sigma^{2}}} \\exp\\left(-\\frac{(x_{i}-\\mu)^{2}}{2\\sigma^{2}}\\right)$$\nSince the observations are i.i.d., the likelihood function for the entire sample $\\mathbf{x} = (x_{1}, \\dots, x_{n})$ is the product of the individual densities:\n$$L(\\sigma^{2} | \\mathbf{x}) = \\prod_{i=1}^{n} f(x_{i} | \\sigma^{2}) = \\left(\\frac{1}{2\\pi\\sigma^{2}}\\right)^{n/2} \\exp\\left(-\\frac{1}{2\\sigma^{2}}\\sum_{i=1}^{n}(x_{i}-\\mu)^{2}\\right)$$\nUsing the definition $Q = \\sum_{i=1}^{n}(X_{i}-\\mu)^{2}$, the likelihood can be written as:\n$$L(\\sigma^{2} | \\mathbf{x}) = (2\\pi\\sigma^{2})^{-n/2} \\exp\\left(-\\frac{Q}{2\\sigma^{2}}\\right)$$\nTo find the maximum, we work with the log-likelihood function, $\\ell(\\sigma^{2} | \\mathbf{x}) = \\ln L(\\sigma^{2} | \\mathbf{x})$:\n$$\\ell(\\sigma^{2} | \\mathbf{x}) = -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln(\\sigma^{2}) - \\frac{Q}{2\\sigma^{2}}$$\nWe differentiate $\\ell$ with respect to $\\sigma^{2}$ and set the result to zero. Let $\\theta = \\sigma^{2}$ for notational convenience.\n$$\\frac{\\partial\\ell}{\\partial\\theta} = -\\frac{n}{2\\theta} + \\frac{Q}{2\\theta^{2}}$$\nSetting the derivative to zero to find the critical point:\n$$-\\frac{n}{2\\hat{\\theta}} + \\frac{Q}{2\\hat{\\theta}^{2}} = 0 \\implies \\frac{Q}{2\\hat{\\theta}^{2}} = \\frac{n}{2\\hat{\\theta}}$$\nAssuming $\\hat{\\theta} \\neq 0$, we can multiply by $2\\hat{\\theta}^{2}$ to get $Q = n\\hat{\\theta}$. Solving for $\\hat{\\theta}$:\n$$\\widehat{\\sigma^{2}}_{\\mathrm{MLE}} = \\hat{\\theta} = \\frac{Q}{n} = \\frac{1}{n}\\sum_{i=1}^{n}(X_{i}-\\mu)^{2}$$\nThis is the sample second central moment, $S^{2}$.\n\nNext, we derive the Method-of-Moments (MoM) estimator. The method equates population moments to sample moments. The first relevant population moment is the second central moment, $\\mathbb{E}[(X-\\mu)^{2}]$. By definition, for a random variable $X$, this is its variance.\n$$\\mathbb{E}[(X-\\mu)^{2}] = \\mathrm{Var}(X) = \\sigma^{2}$$\nThe corresponding sample second central moment is given in the problem as $S^{2} = \\frac{1}{n}\\sum_{i=1}^{n}(X_{i}-\\mu)^{2}$.\nEquating the population moment to the sample moment gives the MoM estimator for $\\sigma^{2}$:\n$$\\widehat{\\sigma^{2}}_{\\mathrm{MoM}} = S^{2} = \\frac{1}{n}\\sum_{i=1}^{n}(X_{i}-\\mu)^{2}$$\nComparing the two results, we find that the Maximum Likelihood Estimator and the Method-of-Moments estimator for $\\sigma^{2}$ are identical for this model:\n$$\\widehat{\\sigma^{2}}_{\\mathrm{MLE}} = \\widehat{\\sigma^{2}}_{\\mathrm{MoM}} = S^{2}$$\n\n**2) Maximum a Posteriori (MAP) Estimator**\n\nThe MAP estimator maximizes the posterior distribution. By Bayes' rule, the posterior density $p(\\sigma^{2} | \\mathbf{x})$ is proportional to the product of the likelihood and the prior:\n$$p(\\sigma^{2} | \\mathbf{x}) \\propto L(\\sigma^{2} | \\mathbf{x}) p(\\sigma^{2})$$\nThe prior on $\\sigma^{2}$ is an Inverse-Gamma$(\\alpha, \\beta)$ distribution:\n$$p(\\sigma^{2}) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)}(\\sigma^{2})^{-(\\alpha+1)}\\exp\\left(-\\frac{\\beta}{\\sigma^{2}}\\right)$$\nWe work with the log-posterior, which is proportional to the sum of the log-likelihood and the log-prior:\n$$\\ln p(\\sigma^{2} | \\mathbf{x}) = \\ln L(\\sigma^{2} | \\mathbf{x}) + \\ln p(\\sigma^{2}) + C$$\nwhere $C$ is a constant that does not depend on $\\sigma^{2}$.\n$$\\ln p(\\sigma^{2} | \\mathbf{x}) = \\left(-\\frac{n}{2}\\ln(\\sigma^{2}) - \\frac{Q}{2\\sigma^{2}}\\right) + \\left(-(\\alpha+1)\\ln(\\sigma^{2}) - \\frac{\\beta}{\\sigma^{2}}\\right) + C'$$\nCombining terms:\n$$\\ln p(\\sigma^{2} | \\mathbf{x}) = -\\left(\\frac{n}{2} + \\alpha + 1\\right)\\ln(\\sigma^{2}) - \\frac{1}{\\sigma^{2}}\\left(\\frac{Q}{2} + \\beta\\right) + C'$$\nTo find the mode of the posterior (the MAP estimator), we differentiate with respect to $\\sigma^{2}$ and set the derivative to zero. Let $\\theta = \\sigma^{2}$:\n$$\\frac{\\partial}{\\partial\\theta}\\ln p(\\theta | \\mathbf{x}) = -\\left(\\frac{n}{2} + \\alpha + 1\\right)\\frac{1}{\\theta} + \\left(\\frac{Q}{2} + \\beta\\right)\\frac{1}{\\theta^{2}}$$\nSetting to zero:\n$$\\left(\\frac{Q}{2} + \\beta\\right)\\frac{1}{\\hat{\\theta}^{2}} = \\left(\\frac{n}{2} + \\alpha + 1\\right)\\frac{1}{\\hat{\\theta}}$$\n$$\\frac{Q + 2\\beta}{2\\hat{\\theta}^{2}} = \\frac{n + 2\\alpha + 2}{2\\hat{\\theta}}$$\nSolving for $\\hat{\\theta}$ gives the MAP estimator:\n$$\\widehat{\\sigma^{2}}_{\\mathrm{MAP}} = \\hat{\\theta} = \\frac{Q + 2\\beta}{n + 2\\alpha + 2}$$\n\n**3) Bias of the MAP Estimator**\n\nThe bias of an estimator $\\hat{\\theta}$ for a parameter $\\theta$ is defined as $\\mathrm{Bias}(\\hat{\\theta}) = \\mathbb{E}[\\hat{\\theta}] - \\theta$. Here, we need to compute the bias of $\\widehat{\\sigma^{2}}_{\\mathrm{MAP}}$ relative to the true variance $\\sigma^{2}$. The expectation is taken over the sampling distribution of the data, where $Q$ is a random variable.\n$$\\mathrm{Bias}(\\widehat{\\sigma^{2}}_{\\mathrm{MAP}}) = \\mathbb{E}_{\\sigma^{2}}\\left[\\widehat{\\sigma^{2}}_{\\mathrm{MAP}}\\right] - \\sigma^{2}$$\nFirst, we find the expectation of the estimator:\n$$\\mathbb{E}_{\\sigma^{2}}\\left[\\widehat{\\sigma^{2}}_{\\mathrm{MAP}}\\right] = \\mathbb{E}_{\\sigma^{2}}\\left[\\frac{Q + 2\\beta}{n + 2\\alpha + 2}\\right]$$\nUsing the linearity of expectation, and noting that $n, \\alpha, \\beta$ are constants:\n$$\\mathbb{E}_{\\sigma^{2}}\\left[\\widehat{\\sigma^{2}}_{\\mathrm{MAP}}\\right] = \\frac{\\mathbb{E}_{\\sigma^{2}}[Q] + 2\\beta}{n + 2\\alpha + 2}$$\nTo find $\\mathbb{E}_{\\sigma^{2}}[Q]$, we use the distributional property of $Q$. For $X_{i} \\sim \\mathcal{N}(\\mu, \\sigma^{2})$, the standardized variable $Z_{i} = \\frac{X_{i}-\\mu}{\\sigma}$ follows a standard normal distribution, $\\mathcal{N}(0, 1)$. The square of a standard normal variable, $Z_{i}^{2}$, follows a chi-squared distribution with $1$ degree of freedom, $\\chi^{2}_{1}$.\nThe quantity $Q$ is defined as $Q = \\sum_{i=1}^{n}(X_{i}-\\mu)^{2}$. We can write this as:\n$$Q = \\sum_{i=1}^{n} \\left(\\sigma \\frac{X_{i}-\\mu}{\\sigma}\\right)^{2} = \\sigma^{2} \\sum_{i=1}^{n} Z_{i}^{2}$$\nSince the $X_{i}$ are independent, the $Z_{i}$ are also independent. The sum of $n$ independent $\\chi^{2}_{1}$ random variables is a $\\chi^{2}$ random variable with $n$ degrees of freedom. Thus:\n$$\\frac{Q}{\\sigma^{2}} = \\sum_{i=1}^{n} Z_{i}^{2} \\sim \\chi^{2}_{n}$$\nThe expected value of a $\\chi^{2}_{n}$ random variable is its degrees of freedom, $n$.\n$$\\mathbb{E}\\left[\\frac{Q}{\\sigma^{2}}\\right] = n$$\nBy linearity of expectation, $\\frac{1}{\\sigma^{2}}\\mathbb{E}_{\\sigma^{2}}[Q] = n$, which implies:\n$$\\mathbb{E}_{\\sigma^{2}}[Q] = n\\sigma^{2}$$\nSubstituting this back into the expression for the expected value of the MAP estimator:\n$$\\mathbb{E}_{\\sigma^{2}}\\left[\\widehat{\\sigma^{2}}_{\\mathrm{MAP}}\\right] = \\frac{n\\sigma^{2} + 2\\beta}{n + 2\\alpha + 2}$$\nFinally, we compute the bias:\n$$\\mathrm{Bias}(\\widehat{\\sigma^{2}}_{\\mathrm{MAP}}) = \\frac{n\\sigma^{2} + 2\\beta}{n + 2\\alpha + 2} - \\sigma^{2}$$\nTo simplify, we put everything over a common denominator:\n$$\\mathrm{Bias}(\\widehat{\\sigma^{2}}_{\\mathrm{MAP}}) = \\frac{n\\sigma^{2} + 2\\beta - \\sigma^{2}(n + 2\\alpha + 2)}{n + 2\\alpha + 2}$$\n$$\\mathrm{Bias}(\\widehat{\\sigma^{2}}_{\\mathrm{MAP}}) = \\frac{n\\sigma^{2} + 2\\beta - n\\sigma^{2} - (2\\alpha + 2)\\sigma^{2}}{n + 2\\alpha + 2}$$\n$$\\mathrm{Bias}(\\widehat{\\sigma^{2}}_{\\mathrm{MAP}}) = \\frac{2\\beta - (2\\alpha + 2)\\sigma^{2}}{n + 2\\alpha + 2} = \\frac{2\\beta - 2(\\alpha + 1)\\sigma^{2}}{n + 2(\\alpha + 1)}$$\nThis is the final symbolic expression for the bias of the MAP estimator.", "answer": "$$\n\\boxed{\\frac{2\\beta - 2(\\alpha + 1)\\sigma^{2}}{n + 2\\alpha + 2}}\n$$", "id": "3157673"}, {"introduction": "Moving from continuous to discrete data, we now explore the Poisson distribution, a cornerstone for modeling count data. This problem [@problem_id:3157635] introduces a practical layer of complexity by incorporating variable \"exposures\" $e_i$ into the model, a common scenario in fields like epidemiology and insurance. You will derive the MoM, MLE, and MAP estimators for the underlying rate $\\mu$ and, importantly, go a step further by comparing the efficiency of the MoM and MLE estimators through a direct variance analysis. This exercise highlights that different methods can lead to estimators with different statistical properties.", "problem": "Suppose that for $i=1,\\dots,n$ we observe independent counts $X_i$ with $X_i \\sim \\operatorname{Poisson}(e_i \\mu)$, where the exposures $e_i>0$ are known constants and $\\mu>0$ is an unknown rate parameter per unit exposure. Work from first principles (definitions of likelihood, Bayes' Rule, and moment-based estimation) to answer the following.\n\n- Using the definition of the joint likelihood for independent Poisson observations, derive the maximum likelihood estimate (MLE) of $\\mu$ expressed in terms of $\\{X_i\\}_{i=1}^{n}$ and $\\{e_i\\}_{i=1}^{n}$.\n- Assume a Gamma prior on $\\mu$ with shape-rate parameterization $\\mu \\sim \\operatorname{Gamma}(a,b)$, with density $p(\\mu)=\\dfrac{b^{a}}{\\Gamma(a)} \\mu^{a-1} \\exp(-b\\mu)$ for $a>0$ and $b>0$. Using Bayes' Rule and the definition of a mode, derive the maximum a posteriori (MAP) estimate of $\\mu$ in terms of $a$, $b$, $\\{X_i\\}_{i=1}^{n}$, and $\\{e_i\\}_{i=1}^{n}$.\n- Using the method of moments (MoM) and the identifying moment $E[X_i/e_i] = \\mu$, derive a MoM estimator for $\\mu$ based on $\\{X_i\\}_{i=1}^{n}$ and $\\{e_i\\}_{i=1}^{n}$.\n- Analyze and compare the variances of the MLE and the MoM estimator under variable exposures. Derive closed-form expressions for $\\operatorname{Var}(\\hat{\\mu}_{\\mathrm{MLE}})$ and $\\operatorname{Var}(\\hat{\\mu}_{\\mathrm{MoM}})$ as functions of $\\mu$ and $\\{e_i\\}_{i=1}^{n}$, starting from the variance property of the Poisson distribution and independence of the $X_i$.\n- Now consider a concrete dataset with $n=5$, exposures $(e_i)_{i=1}^{5}=(2.0,\\,0.5,\\,1.5,\\,3.0,\\,0.8)$ and observations $(X_i)_{i=1}^{5}=(3,\\,0,\\,2,\\,7,\\,1)$. For prior parameters $a=2$ and $b=1$, compute the numerical values of the MLE, the MoM, and the MAP estimates of $\\mu$.\n- Finally, using only the exposures in the previous bullet, compute the ratio $R=\\dfrac{\\operatorname{Var}(\\hat{\\mu}_{\\mathrm{MoM}})}{\\operatorname{Var}(\\hat{\\mu}_{\\mathrm{MLE}})}$. Report $R$ as a dimensionless real number and round your answer to four significant figures.", "solution": "We begin with the fundamental definitions for the Poisson model, likelihood for independent observations, the Gamma prior, and the method of moments.\n\nFor $i=1,\\dots,n$, the model is $X_i \\sim \\operatorname{Poisson}(e_i \\mu)$ with probability mass function\n$$\n\\Pr(X_i=x_i \\mid \\mu) \\;=\\; \\frac{(e_i \\mu)^{x_i} \\exp(-e_i \\mu)}{x_i!}, \\quad x_i \\in \\{0,1,2,\\dots\\}.\n$$\nBy independence, the joint likelihood is\n$$\nL(\\mu; x_1,\\dots,x_n) \\;=\\; \\prod_{i=1}^{n} \\frac{(e_i \\mu)^{x_i} \\exp(-e_i \\mu)}{x_i!}.\n$$\n\nMaximum likelihood estimate (MLE). Consider the log-likelihood,\n$$\n\\ell(\\mu) \\;=\\; \\sum_{i=1}^{n} \\bigl[ x_i \\ln(e_i \\mu) - e_i \\mu - \\ln(x_i!) \\bigr]\n\\;=\\; \\sum_{i=1}^{n} x_i \\ln \\mu + \\sum_{i=1}^{n} x_i \\ln e_i - \\mu \\sum_{i=1}^{n} e_i - \\sum_{i=1}^{n} \\ln(x_i!).\n$$\nDifferentiate with respect to $\\mu$:\n$$\n\\frac{\\partial \\ell(\\mu)}{\\partial \\mu} \\;=\\; \\frac{1}{\\mu} \\sum_{i=1}^{n} x_i \\;-\\; \\sum_{i=1}^{n} e_i.\n$$\nSet the derivative to zero to obtain the critical point:\n$$\n\\frac{1}{\\mu} \\sum_{i=1}^{n} x_i \\;-\\; \\sum_{i=1}^{n} e_i \\;=\\; 0\n\\;\\;\\Longrightarrow\\;\\;\n\\hat{\\mu}_{\\mathrm{MLE}} \\;=\\; \\frac{\\sum_{i=1}^{n} x_i}{\\sum_{i=1}^{n} e_i}.\n$$\nThe second derivative is $\\frac{\\partial^2 \\ell(\\mu)}{\\partial \\mu^2} = -\\frac{1}{\\mu^2} \\sum_{i=1}^{n} x_i \\le 0$ for $\\mu>0$ when at least one $x_i>0$, confirming a maximum.\n\nMaximum a posteriori (MAP) estimate. Assume the Gamma prior $\\mu \\sim \\operatorname{Gamma}(a,b)$ with density $p(\\mu)=\\frac{b^{a}}{\\Gamma(a)} \\mu^{a-1} \\exp(-b \\mu)$, for $a>0$ and $b>0$. The posterior density is proportional to prior times likelihood:\n$$\np(\\mu \\mid x_{1:n}) \\;\\propto\\; \\mu^{a-1} \\exp(-b\\mu) \\prod_{i=1}^{n} (e_i \\mu)^{x_i} \\exp(-e_i \\mu)\n\\;\\propto\\; \\mu^{a-1+\\sum_{i=1}^{n} x_i} \\exp\\!\\bigl(-(b+\\sum_{i=1}^{n} e_i)\\mu\\bigr).\n$$\nThus $\\mu \\mid x_{1:n} \\sim \\operatorname{Gamma}\\!\\bigl(a+\\sum_{i=1}^{n} x_i,\\, b+\\sum_{i=1}^{n} e_i\\bigr)$ in the same shape-rate parameterization. For a Gamma distribution with shape $k>1$ and rate $\\theta>0$, the mode is $(k-1)/\\theta$. Therefore, provided $a+\\sum_{i=1}^{n} x_i>1$, the MAP is\n$$\n\\hat{\\mu}_{\\mathrm{MAP}} \\;=\\; \\frac{a-1+\\sum_{i=1}^{n} x_i}{b+\\sum_{i=1}^{n} e_i}.\n$$\n\nMethod of moments (MoM). From the model, $E[X_i]=e_i \\mu$, so\n$$\nE\\!\\left[\\frac{X_i}{e_i}\\right] \\;=\\; \\mu.\n$$\nUsing the sample analogue of this moment condition, a natural MoM estimator is the sample mean of $\\frac{X_i}{e_i}$:\n$$\n\\hat{\\mu}_{\\mathrm{MoM}} \\;=\\; \\frac{1}{n} \\sum_{i=1}^{n} \\frac{X_i}{e_i}.\n$$\n\nVariance analysis under variable exposures. We use $\\operatorname{Var}(X_i)=e_i \\mu$ for a Poisson random variable with mean $e_i \\mu$, and independence of the $X_i$.\n\n- For the MLE,\n$$\n\\hat{\\mu}_{\\mathrm{MLE}} \\;=\\; \\frac{\\sum_{i=1}^{n} X_i}{\\sum_{i=1}^{n} e_i}.\n$$\nBecause $\\sum_{i=1}^{n} X_i \\sim \\operatorname{Poisson}\\!\\bigl(\\mu \\sum_{i=1}^{n} e_i\\bigr)$ by additivity of independent Poisson variables, we have\n$$\n\\operatorname{Var}\\!\\bigl(\\hat{\\mu}_{\\mathrm{MLE}}\\bigr)\n\\;=\\; \\frac{\\operatorname{Var}(\\sum_{i=1}^{n} X_i)}{\\bigl(\\sum_{i=1}^{n} e_i\\bigr)^2}\n\\;=\\; \\frac{\\mu \\sum_{i=1}^{n} e_i}{\\bigl(\\sum_{i=1}^{n} e_i\\bigr)^2}\n\\;=\\; \\frac{\\mu}{\\sum_{i=1}^{n} e_i}.\n$$\n\n- For the MoM estimator,\n$$\n\\hat{\\mu}_{\\mathrm{MoM}} \\;=\\; \\frac{1}{n} \\sum_{i=1}^{n} \\frac{X_i}{e_i}.\n$$\nIndependence yields\n$$\n\\operatorname{Var}\\!\\bigl(\\hat{\\mu}_{\\mathrm{MoM}}\\bigr)\n\\;=\\; \\frac{1}{n^2} \\sum_{i=1}^{n} \\operatorname{Var}\\!\\left(\\frac{X_i}{e_i}\\right)\n\\;=\\; \\frac{1}{n^2} \\sum_{i=1}^{n} \\frac{\\operatorname{Var}(X_i)}{e_i^2}\n\\;=\\; \\frac{1}{n^2} \\sum_{i=1}^{n} \\frac{e_i \\mu}{e_i^2}\n\\;=\\; \\frac{\\mu}{n^2} \\sum_{i=1}^{n} \\frac{1}{e_i}.\n$$\nConsequently, the variance ratio is\n$$\nR \\;\\equiv\\; \\frac{\\operatorname{Var}(\\hat{\\mu}_{\\mathrm{MoM}})}{\\operatorname{Var}(\\hat{\\mu}_{\\mathrm{MLE}})}\n\\;=\\; \\frac{\\mu \\, \\frac{1}{n^2} \\sum_{i=1}^{n} \\frac{1}{e_i}}{\\mu \\, \\frac{1}{\\sum_{i=1}^{n} e_i}}\n\\;=\\; \\frac{\\bigl(\\sum_{i=1}^{n} e_i\\bigr)\\bigl(\\sum_{i=1}^{n} \\frac{1}{e_i}\\bigr)}{n^2}.\n$$\nBy the Cauchyâ€“Schwarz inequality, $\\bigl(\\sum_{i=1}^{n} e_i\\bigr)\\bigl(\\sum_{i=1}^{n} \\frac{1}{e_i}\\bigr) \\ge n^2$ with equality if and only if all $e_i$ are equal, so $R \\ge 1$ and the MLE is at least as efficient in this sense, with strict inequality when the $e_i$ vary.\n\nNumerical computations for the given data. Let $n=5$, exposures $(e_i)=(2.0,\\,0.5,\\,1.5,\\,3.0,\\,0.8)$, and observations $(X_i)=(3,\\,0,\\,2,\\,7,\\,1)$. Compute the following:\n\n- MLE:\n$$\n\\sum_{i=1}^{5} X_i \\;=\\; 3+0+2+7+1 \\;=\\; 13, \n\\quad \\sum_{i=1}^{5} e_i \\;=\\; 2.0+0.5+1.5+3.0+0.8 \\;=\\; 7.8,\n$$\nso\n$$\n\\hat{\\mu}_{\\mathrm{MLE}} \\;=\\; \\frac{13}{7.8} \\;=\\; \\frac{65}{39} \\;\\approx\\; 1.666\\overline{6}.\n$$\n\n- MoM:\n$$\n\\frac{1}{5}\\sum_{i=1}^{5} \\frac{X_i}{e_i} \\;=\\; \\frac{1}{5}\\left(\\frac{3}{2.0} + \\frac{0}{0.5} + \\frac{2}{1.5} + \\frac{7}{3.0} + \\frac{1}{0.8}\\right)\n\\;=\\; \\frac{1}{5}\\left(\\frac{3}{2} + 0 + \\frac{4}{3} + \\frac{7}{3} + \\frac{5}{4}\\right)\n\\;=\\; \\frac{77}{60}\n\\;\\approx\\; 1.2833.\n$$\n\n- MAP with $a=2$ and $b=1$:\n$$\n\\hat{\\mu}_{\\mathrm{MAP}} \\;=\\; \\frac{a-1+\\sum X_i}{b+\\sum e_i}\n\\;=\\; \\frac{2-1+13}{1+7.8}\n\\;=\\; \\frac{14}{8.8}\n\\;=\\; \\frac{35}{22}\n\\;\\approx\\; 1.5909.\n$$\n\nVariance ratio using only exposures. Compute\n$$\n\\sum_{i=1}^{5} e_i \\;=\\; 7.8, \n\\quad \\sum_{i=1}^{5} \\frac{1}{e_i} \\;=\\; \\frac{1}{2.0} + \\frac{1}{0.5} + \\frac{1}{1.5} + \\frac{1}{3.0} + \\frac{1}{0.8}\n\\;=\\; \\frac{1}{2} + 2 + \\frac{2}{3} + \\frac{1}{3} + \\frac{5}{4}\n\\;=\\; \\frac{19}{4}\n\\;=\\; 4.75.\n$$\nHence\n$$\nR \\;=\\; \\frac{\\bigl(\\sum e_i\\bigr)\\bigl(\\sum \\frac{1}{e_i}\\bigr)}{n^2}\n\\;=\\; \\frac{(7.8)(4.75)}{5^2}\n\\;=\\; \\frac{37.05}{25}\n\\;=\\; 1.482.\n$$\nRounded to four significant figures, $R=1.482$.", "answer": "$$\\boxed{1.482}$$", "id": "3157635"}, {"introduction": "Our final practice problem tackles the Lognormal distribution, which is essential for modeling positive, skewed data found in finance, biology, and many other sciences. This exercise [@problem_id:3157621] cleverly demonstrates the power of transformation, as you will perform MLE and MAP estimation on the log-scale where the data become Normal. You will then contrast this with the Method of Moments on the original data scale. The true value of this problem lies in its exploration of bias, particularly how applying a non-linear function like the exponential map to an unbiased log-scale estimate can induce bias on the original scale, a direct consequence of Jensen's inequality.", "problem": "A positive-valued random sample $\\{X_{1},\\dots,X_{n}\\}$ is drawn independently and identically distributed from a lognormal model with parameters $(\\mu,\\sigma^{2})$, meaning that the logarithms $Y_{i}=\\ln X_{i}$ are independently and identically distributed as normal with mean $\\mu$ and variance $\\sigma^{2}$. Work on the log-scale by defining $Y_{i}=\\ln X_{i}$ for $i=1,\\dots,n$, and denote the sample mean $\\bar{Y}=\\frac{1}{n}\\sum_{i=1}^{n}Y_{i}$ and the empirical second central moment on the log-scale $S^{2}_{Y}=\\frac{1}{n}\\sum_{i=1}^{n}(Y_{i}-\\bar{Y})^{2}$. On the original scale, denote the sample mean $\\bar{X}=\\frac{1}{n}\\sum_{i=1}^{n}X_{i}$ and the empirical second central moment $S^{2}=\\frac{1}{n}\\sum_{i=1}^{n}(X_{i}-\\bar{X})^{2}$.\n\nUsing only foundational facts and definitions from statistical learning and probability theory, complete the following tasks.\n\n1) Derive the maximum likelihood estimators for $(\\mu,\\sigma^{2})$ on the log-scale by maximizing the likelihood of the normal model for $\\{Y_{i}\\}_{i=1}^{n}$.\n\n2) Adopt conjugate priors for the normal model with unknown mean and variance on the log-scale: conditionally, $\\mu\\mid\\sigma^{2}\\sim\\mathcal{N}\\!\\big(\\mu_{0},\\sigma^{2}/\\kappa_{0}\\big)$ with known hyperparameters $\\mu_{0}\\in\\mathbb{R}$ and $\\kappa_{0}>0$, and marginally $\\sigma^{2}\\sim\\text{Inverse-Gamma}(\\alpha_{0},\\beta_{0})$ with density proportional to $(\\sigma^{2})^{-(\\alpha_{0}+1)}\\exp\\!\\big(-\\beta_{0}/\\sigma^{2}\\big)$ for known $\\alpha_{0}>0$ and $\\beta_{0}>0$. Derive the maximum a posteriori estimators on the log-scale, and express the posterior mode for $\\mu$ in closed form using $(n,\\bar{Y},\\mu_{0},\\kappa_{0})$.\n\n3) Using the method of moments on the original scale, employ the population identities $E[X]=\\exp(\\mu+\\sigma^{2}/2)$ and $\\operatorname{Var}(X)=(\\exp(\\sigma^{2})-1)\\exp(2\\mu+\\sigma^{2})$ and equate them to the empirical moments $(\\bar{X},S^{2})$ to obtain method-of-moments estimators $\\widehat{\\mu}_{\\text{MoM}}$ and $\\widehat{\\sigma}^{2}_{\\text{MoM}}$ as explicit functions of $(\\bar{X},S^{2})$.\n\n4) Briefly discuss (analytically, not numerically) the finite-sample bias properties of the estimators on the log-scale obtained in part $1)$, and comment on the direction of bias one should expect when estimating $E[X]$ by a plug-in transformation of log-scale estimators.\n\nProvide complete derivations. For your final boxed answer, report only the closed-form expression for the maximum a posteriori estimator of $\\mu$ from part $2)$ as a function of $(n,\\bar{Y},\\mu_{0},\\kappa_{0})$. No numerical rounding is required, and no units are involved.", "solution": "The problem statement has been validated and found to be self-contained, scientifically grounded in statistical theory, and well-posed. All componentsâ€”the lognormal model, definitions of estimators (MLE, MAP, MoM), and properties like biasâ€”are standard concepts in statistics. Sufficient information is provided to complete all tasks. Therefore, a complete solution is warranted.\n\nThe solution is presented in four parts as requested by the problem statement.\n\n**1) Maximum Likelihood Estimators (MLE) on the Log-Scale**\n\nLet the sample be $\\{Y_1, \\dots, Y_n\\}$, where $Y_i = \\ln X_i$. The model assumes $Y_i$ are independent and identically distributed (i.i.d.) draws from a normal distribution $\\mathcal{N}(\\mu, \\sigma^2)$. The probability density function (PDF) for a single observation $Y_i$ is\n$$\nf(y_i \\mid \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( -\\frac{(y_i - \\mu)^2}{2\\sigma^2} \\right)\n$$\nThe likelihood function for the entire sample $\\{Y_1, \\dots, Y_n\\}$ is the product of the individual densities, given their independence:\n$$\nL(\\mu, \\sigma^2 \\mid \\{Y_i\\}) = \\prod_{i=1}^{n} f(Y_i \\mid \\mu, \\sigma^2) = \\left(\\frac{1}{2\\pi\\sigma^2}\\right)^{n/2} \\exp\\left( -\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (Y_i - \\mu)^2 \\right)\n$$\nIt is more convenient to work with the log-likelihood function, $\\ell(\\mu, \\sigma^2) = \\ln L(\\mu, \\sigma^2)$:\n$$\n\\ell(\\mu, \\sigma^2) = -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln(\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (Y_i - \\mu)^2\n$$\nTo find the maximum likelihood estimators (MLEs), we take the partial derivatives of $\\ell$ with respect to $\\mu$ and $\\sigma^2$ and set them to zero.\n\nFirst, with respect to $\\mu$:\n$$\n\\frac{\\partial \\ell}{\\partial \\mu} = -\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} 2(Y_i - \\mu)(-1) = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} (Y_i - \\mu)\n$$\nSetting this derivative to zero (assuming $\\sigma^2 > 0$):\n$$\n\\frac{1}{\\sigma^2} \\sum_{i=1}^{n} (Y_i - \\widehat{\\mu}_{\\text{MLE}}) = 0 \\implies \\sum_{i=1}^{n} Y_i - n\\widehat{\\mu}_{\\text{MLE}} = 0 \\implies \\widehat{\\mu}_{\\text{MLE}} = \\frac{1}{n}\\sum_{i=1}^{n} Y_i = \\bar{Y}\n$$\nNext, with respect to $\\sigma^2$:\n$$\n\\frac{\\partial \\ell}{\\partial \\sigma^2} = -\\frac{n}{2\\sigma^2} + \\frac{1}{2(\\sigma^2)^2} \\sum_{i=1}^{n} (Y_i - \\mu)^2\n$$\nSetting this derivative to zero and substituting $\\mu = \\widehat{\\mu}_{\\text{MLE}} = \\bar{Y}$:\n$$\n-\\frac{n}{2\\widehat{\\sigma^2}_{\\text{MLE}}} + \\frac{1}{2(\\widehat{\\sigma^2}_{\\text{MLE}})^2} \\sum_{i=1}^{n} (Y_i - \\bar{Y})^2 = 0\n$$\nMultiplying by $2(\\widehat{\\sigma^2}_{\\text{MLE}})^2$ yields:\n$$\n-n\\widehat{\\sigma^2}_{\\text{MLE}} + \\sum_{i=1}^{n} (Y_i - \\bar{Y})^2 = 0 \\implies \\widehat{\\sigma^2}_{\\text{MLE}} = \\frac{1}{n} \\sum_{i=1}^{n} (Y_i - \\bar{Y})^2 = S^2_Y\n$$\nThus, the maximum likelihood estimators for $(\\mu, \\sigma^2)$ are the sample mean and the empirical second central moment of the log-transformed data: $(\\widehat{\\mu}_{\\text{MLE}}, \\widehat{\\sigma^2}_{\\text{MLE}}) = (\\bar{Y}, S^2_Y)$.\n\n**2) Maximum a Posteriori (MAP) Estimators on the Log-Scale**\n\nThe posterior distribution is proportional to the product of the likelihood and the prior distribution: $p(\\mu, \\sigma^2 \\mid \\{Y_i\\}) \\propto L(\\{Y_i\\} \\mid \\mu, \\sigma^2) p(\\mu, \\sigma^2)$. The prior is specified as $p(\\mu, \\sigma^2) = p(\\mu \\mid \\sigma^2) p(\\sigma^2)$, with:\n$$\np(\\mu \\mid \\sigma^2) \\propto (\\sigma^2)^{-1/2} \\exp\\left( -\\frac{\\kappa_0(\\mu-\\mu_0)^2}{2\\sigma^2} \\right)\n$$\n$$\np(\\sigma^2) \\propto (\\sigma^2)^{-(\\alpha_0+1)} \\exp\\left( -\\frac{\\beta_0}{\\sigma^2} \\right)\n$$\nThe log-posterior, up to an additive constant, is $\\ln p(\\mu, \\sigma^2 \\mid \\{Y_i\\}) \\propto \\ell(\\mu, \\sigma^2) + \\ln p(\\mu, \\sigma^2)$.\n$$\n\\ln p(\\mu, \\sigma^2 \\mid \\{Y_i\\}) \\propto -\\frac{n}{2}\\ln(\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^n(Y_i-\\mu)^2 - \\frac{1}{2}\\ln(\\sigma^2) - \\frac{\\kappa_0}{2\\sigma^2}(\\mu-\\mu_0)^2 - (\\alpha_0+1)\\ln(\\sigma^2) - \\frac{\\beta_0}{\\sigma^2}\n$$\nTo find the MAP estimator for $\\mu$, we differentiate the log-posterior with respect to $\\mu$ and set the result to zero. Only terms involving $\\mu$ are relevant:\n$$\n\\frac{\\partial}{\\partial \\mu} \\left( -\\frac{1}{2\\sigma^2}\\sum_{i=1}^{n}(Y_i-\\mu)^2 - \\frac{\\kappa_0}{2\\sigma^2}(\\mu-\\mu_0)^2 \\right) = 0\n$$\n$$\n\\frac{1}{\\sigma^2}\\sum_{i=1}^{n}(Y_i - \\mu) - \\frac{\\kappa_0}{\\sigma^2}(\\mu-\\mu_0) = 0\n$$\nAssuming $\\sigma^2 > 0$, we can multiply by $\\sigma^2$:\n$$\n\\sum_{i=1}^{n} Y_i - n\\mu - \\kappa_0\\mu + \\kappa_0\\mu_0 = 0\n$$\n$$\nn\\bar{Y} + \\kappa_0\\mu_0 = (n+\\kappa_0)\\mu\n$$\nSolving for $\\mu$ gives the MAP estimator:\n$$\n\\widehat{\\mu}_{\\text{MAP}} = \\frac{n\\bar{Y} + \\kappa_0\\mu_0}{n + \\kappa_0}\n$$\nTo find the MAP estimator for $\\sigma^2$, we differentiate the log-posterior with respect to $\\sigma^2$. Let's collect all terms involving $\\sigma^2$:\n$$\n\\ln p(\\dots \\mid \\sigma^2) \\propto -\\left(\\frac{n+1}{2} + \\alpha_0+1\\right)\\ln(\\sigma^2) - \\frac{1}{\\sigma^2}\\left(\\frac{1}{2}\\sum(Y_i-\\mu)^2 + \\frac{\\kappa_0}{2}(\\mu-\\mu_0)^2 + \\beta_0\\right)\n$$\nLet $\\tau=\\sigma^2$. Differentiating with respect to $\\tau$ and setting to zero:\n$$\n-\\frac{1}{\\tau}\\left(\\frac{n}{2} + \\frac{1}{2} + \\alpha_0+1\\right) + \\frac{1}{\\tau^2}\\left(\\frac{1}{2}\\sum(Y_i-\\mu)^2 + \\frac{\\kappa_0}{2}(\\mu-\\mu_0)^2 + \\beta_0\\right) = 0\n$$\nSubstituting $\\mu = \\widehat{\\mu}_{\\text{MAP}}$ and solving for $\\tau = \\widehat{\\sigma^2}_{\\text{MAP}}$:\n$$\n\\widehat{\\sigma^2}_{\\text{MAP}} = \\frac{\\sum(Y_i-\\widehat{\\mu}_{\\text{MAP}})^2 + \\kappa_0(\\widehat{\\mu}_{\\text{MAP}}-\\mu_0)^2 + 2\\beta_0}{n+2\\alpha_0+3}\n$$\n\n**3) Method of Moments (MoM) Estimators on the Original Scale**\n\nThe method of moments equates population moments to their corresponding sample moments. We are given the population mean and variance of a lognormal random variable $X$:\n$$\nE[X] = \\exp(\\mu + \\sigma^2/2)\n$$\n$$\n\\operatorname{Var}(X) = (\\exp(\\sigma^2) - 1)\\exp(2\\mu + \\sigma^2)\n$$\nThe corresponding empirical moments are the sample mean $\\bar{X} = \\frac{1}{n}\\sum X_i$ and the empirical second central moment $S^2 = \\frac{1}{n}\\sum (X_i - \\bar{X})^2$. Note that we equate $\\operatorname{Var}(X)$ to $S^2$, not the unbiased sample variance.\nThe system of equations is:\n$$\n(1) \\quad \\bar{X} = \\exp(\\widehat{\\mu}_{\\text{MoM}} + \\widehat{\\sigma}^2_{\\text{MoM}}/2)\n$$\n$$\n(2) \\quad S^2 = (\\exp(\\widehat{\\sigma}^2_{\\text{MoM}}) - 1)\\exp(2\\widehat{\\mu}_{\\text{MoM}} + \\widehat{\\sigma}^2_{\\text{MoM}})\n$$\nFrom equation (1), we can square both sides to get $\\bar{X}^2 = \\exp(2\\widehat{\\mu}_{\\text{MoM}} + \\widehat{\\sigma}^2_{\\text{MoM}})$. Substituting this into equation (2):\n$$\nS^2 = (\\exp(\\widehat{\\sigma}^2_{\\text{MoM}}) - 1)\\bar{X}^2\n$$\nSolving for $\\widehat{\\sigma}^2_{\\text{MoM}}$:\n$$\n\\frac{S^2}{\\bar{X}^2} = \\exp(\\widehat{\\sigma}^2_{\\text{MoM}}) - 1\n$$\n$$\n\\exp(\\widehat{\\sigma}^2_{\\text{MoM}}) = 1 + \\frac{S^2}{\\bar{X}^2}\n$$\n$$\n\\widehat{\\sigma}^2_{\\text{MoM}} = \\ln\\left(1 + \\frac{S^2}{\\bar{X}^2}\\right)\n$$\nNow, from equation (1), take the natural logarithm:\n$$\n\\ln(\\bar{X}) = \\widehat{\\mu}_{\\text{MoM}} + \\widehat{\\sigma}^2_{\\text{MoM}}/2\n$$\nSolving for $\\widehat{\\mu}_{\\text{MoM}}$ and substituting the expression for $\\widehat{\\sigma}^2_{\\text{MoM}}$:\n$$\n\\widehat{\\mu}_{\\text{MoM}} = \\ln(\\bar{X}) - \\frac{1}{2}\\widehat{\\sigma}^2_{\\text{MoM}} = \\ln(\\bar{X}) - \\frac{1}{2}\\ln\\left(1 + \\frac{S^2}{\\bar{X}^2}\\right)\n$$\n\n**4) Finite-Sample Bias Properties**\n\nWe analyze the bias of the estimators found in part 1.\nFor $\\widehat{\\mu}_{\\text{MLE}} = \\bar{Y}$:\n$$\nE[\\widehat{\\mu}_{\\text{MLE}}] = E[\\bar{Y}] = E\\left[\\frac{1}{n}\\sum_{i=1}^{n} Y_i\\right] = \\frac{1}{n}\\sum_{i=1}^{n} E[Y_i] = \\frac{1}{n}(n\\mu) = \\mu\n$$\nThe bias is $E[\\widehat{\\mu}_{\\text{MLE}}] - \\mu = 0$. Thus, $\\widehat{\\mu}_{\\text{MLE}}$ is an unbiased estimator of $\\mu$.\n\nFor $\\widehat{\\sigma^2}_{\\text{MLE}} = S^2_Y = \\frac{1}{n}\\sum(Y_i - \\bar{Y})^2$:\nIt is a standard result that for a normal sample, the quantity $\\frac{n S^2_Y}{\\sigma^2} = \\frac{\\sum(Y_i - \\bar{Y})^2}{\\sigma^2}$ follows a chi-squared distribution with $n-1$ degrees of freedom, i.e., $\\chi^2_{n-1}$. The expected value of a $\\chi^2_{n-1}$ random variable is $n-1$.\n$$\nE\\left[\\frac{n S^2_Y}{\\sigma^2}\\right] = n-1 \\implies E[S^2_Y] = \\frac{n-1}{n}\\sigma^2\n$$\nThe bias is $E[\\widehat{\\sigma^2}_{\\text{MLE}}] - \\sigma^2 = \\frac{n-1}{n}\\sigma^2 - \\sigma^2 = -\\frac{1}{n}\\sigma^2$. Since $\\sigma^2 > 0$, the bias is negative. $\\widehat{\\sigma^2}_{\\text{MLE}}$ systematically underestimates the true variance $\\sigma^2$.\n\nNow, consider the plug-in estimator for the mean on the original scale, $\\widehat{E[X]} = \\exp(\\widehat{\\mu}_{\\text{MLE}} + \\widehat{\\sigma^2}_{\\text{MLE}}/2) = \\exp(\\bar{Y} + S_Y^2/2)$.\nFor a sample from a normal distribution, the sample mean $\\bar{Y}$ and sample variance $S^2_Y$ are independent (a consequence of Basu's theorem). Therefore, the expectation of their product of functions is the product of their expectations:\n$$\nE[\\widehat{E[X]}] = E[\\exp(\\bar{Y} + S_Y^2/2)] = E[\\exp(\\bar{Y})] E[\\exp(S_Y^2/2)]\n$$\nThe term $E[\\exp(\\bar{Y})]$ is the moment-generating function (MGF) of $\\bar{Y} \\sim \\mathcal{N}(\\mu, \\sigma^2/n)$ evaluated at $t=1$. The MGF of a $\\mathcal{N}(m, s^2)$ distribution is $M(t)=\\exp(mt + s^2t^2/2)$.\n$$\nE[\\exp(\\bar{Y})] = \\exp\\left(\\mu \\cdot 1 + \\frac{(\\sigma^2/n) \\cdot 1^2}{2}\\right) = \\exp\\left(\\mu + \\frac{\\sigma^2}{2n}\\right)\n$$\nThe term $E[\\exp(S_Y^2/2)]$ involves the MGF of a chi-squared distribution. Let $V = \\frac{n S_Y^2}{\\sigma^2} \\sim \\chi^2_{n-1}$. Then $S_Y^2 = \\frac{\\sigma^2}{n}V$.\n$$\nE[\\exp(S_Y^2/2)] = E\\left[\\exp\\left(\\frac{\\sigma^2}{2n}V\\right)\\right]\n$$\nThis is the MGF of $V \\sim \\chi^2_{n-1}$ evaluated at $t = \\frac{\\sigma^2}{2n}$. The MGF of a $\\chi^2_k$ distribution is $M_V(t) = (1-2t)^{-k/2}$.\n$$\nE[\\exp(S_Y^2/2)] = \\left(1 - 2\\frac{\\sigma^2}{2n}\\right)^{-(n-1)/2} = \\left(1 - \\frac{\\sigma^2}{n}\\right)^{-(n-1)/2}\n$$\nCombining these results:\n$$\nE[\\widehat{E[X]}] = \\exp\\left(\\mu + \\frac{\\sigma^2}{2n}\\right) \\left(1 - \\frac{\\sigma^2}{n}\\right)^{-(n-1)/2}\n$$\nThis expression is not equal to the true mean $E[X] = \\exp(\\mu + \\sigma^2/2)$. The estimator is biased. To determine the direction of bias, we can analyze the ratio $\\frac{E[\\widehat{E[X]}]}{E[X]}$:\n$$\n\\frac{E[\\widehat{E[X]}]}{E[X]} = \\frac{\\exp\\left(\\mu + \\frac{\\sigma^2}{2n}\\right) \\left(1 - \\frac{\\sigma^2}{n}\\right)^{-(n-1)/2}}{\\exp(\\mu + \\sigma^2/2)} = \\exp\\left(-\\frac{\\sigma^2}{2} + \\frac{\\sigma^2}{2n}\\right) \\left(1-\\frac{\\sigma^2}{n}\\right)^{-(n-1)/2}\n$$\n$$\n= \\exp\\left(-\\frac{(n-1)\\sigma^2}{2n}\\right) \\left(1-\\frac{\\sigma^2}{n}\\right)^{-(n-1)/2} = \\left[\\exp\\left(-\\frac{\\sigma^2}{n}\\right)\\left(1-\\frac{\\sigma^2}{n}\\right)^{-1}\\right]^{(n-1)/2}\n$$\nLet $u = \\sigma^2/n$. We analyze the term $h(u) = e^{-u}(1-u)^{-1}$. The Taylor series expansion of $\\ln(h(u))$ around $u=0$ is $\\ln(h(u)) = -u - \\ln(1-u) = -u - \\left(-u - \\frac{u^2}{2} - \\frac{u^3}{3} - \\dots\\right) = \\frac{u^2}{2} + \\frac{u^3}{3} + \\dots$.\nFor $u > 0$, we have $\\ln(h(u)) > 0$, which implies $h(u) > 1$.\nSince the base is greater than $1$ and the exponent $(n-1)/2$ is positive (for $n>1$), the ratio is greater than $1$.\n$$\nE[\\widehat{E[X]}] > E[X]\n$$\nTherefore, the plug-in estimator for $E[X]$ has a positive bias; it tends to overestimate the true mean. This is a common consequence of applying a convex function (the exponential) to unbiased or nearly unbiased estimators of its arguments, an effect related to Jensen's inequality.", "answer": "$$\\boxed{\\frac{n\\bar{Y} + \\kappa_{0}\\mu_{0}}{n + \\kappa_{0}}}$$", "id": "3157621"}]}