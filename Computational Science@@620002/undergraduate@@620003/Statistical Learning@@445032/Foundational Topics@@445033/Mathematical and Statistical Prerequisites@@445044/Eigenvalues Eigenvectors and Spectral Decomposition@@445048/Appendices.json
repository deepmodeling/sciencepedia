{"hands_on_practices": [{"introduction": "This first practice provides a direct application of spectral decomposition as a powerful diagnostic tool in statistical modeling. You will learn to detect multicollinearity—a common issue where features are highly correlated—by analyzing the eigenvalues of the data's correlation matrix [@problem_id:3117789]. A small eigenvalue signals a near-linear dependency, and its corresponding eigenvector reveals precisely which features are entangled, allowing you to make informed decisions about feature selection.", "problem": "You are given a sequence of multivariate datasets represented as real matrices and asked to use spectral decomposition to diagnose multicollinearity by identifying nearly collinear features through small eigenvalues of the empirical correlation matrix, and then propose feature drops aligned with the corresponding eigenvectors. Work in the context of statistical learning where feature multicollinearity degrades model interpretability and numerical stability. The task requires formal derivation from core definitions and well-tested facts and then an algorithmic realization.\n\nStart from the following fundamental base:\n- A dataset is a matrix $X \\in \\mathbb{R}^{n \\times p}$ with $n$ observations and $p$ features, with columns $x_{1}, \\dots, x_{p}$.\n- The empirical mean of feature $j$ is $\\bar{x}_{j} = \\frac{1}{n} \\sum_{i=1}^{n} X_{ij}$.\n- The unbiased empirical variance of feature $j$ is $s_{j}^{2} = \\frac{1}{n-1} \\sum_{i=1}^{n} \\left(X_{ij} - \\bar{x}_{j}\\right)^{2}$. The empirical standard deviation is $s_{j} = \\sqrt{s_{j}^{2}}$.\n- The standardized design matrix is $Z \\in \\mathbb{R}^{n \\times p}$ with $Z_{ij} = \\frac{X_{ij} - \\bar{x}_{j}}{s_{j}}$ for all features with $s_{j} > 0$.\n- The empirical correlation matrix is defined by\n$$\nR = \\frac{1}{n - 1} Z^{\\top} Z \\in \\mathbb{R}^{p \\times p}.\n$$\nThis $R$ is symmetric, positive semi-definite, and admits a spectral decomposition $R = V \\Lambda V^{\\top}$, where $V \\in \\mathbb{R}^{p \\times p}$ has orthonormal columns (eigenvectors) and $\\Lambda = \\mathrm{diag}(\\lambda_{1}, \\dots, \\lambda_{p})$ contains the eigenvalues.\n- In the context of Principal Component Analysis (PCA), each eigenvalue $\\lambda_{k}$ equals the variance of $Z$ along eigenvector $v_{k}$. Small eigenvalues indicate directions of small variance, which correspond to nearly redundant linear relationships among features.\n\nYour program must implement the following procedure for each test case:\n1. Standardize each feature to zero mean and unit variance using the definitions above. If any feature has empirical standard deviation $s_{j} = 0$, mark it for dropping immediately and exclude it from subsequent spectral analysis. Use the unbiased denominator $n-1$ in computing both $s_{j}^{2}$ and $R$.\n2. Compute the empirical correlation matrix $R = \\frac{1}{n-1} Z^{\\top} Z$ over the remaining features.\n3. Compute the spectral decomposition $R = V \\Lambda V^{\\top}$ and obtain eigenvalues $\\lambda_{k}$ and eigenvectors $v_{k}$.\n4. Given a small-eigenvalue threshold $\\tau > 0$, identify all indices $k$ such that $\\lambda_{k} < \\tau$. For each such $k$, find the feature index $j^{\\star}$ with the largest absolute loading $|v_{k,j}|$ on $v_{k}$, and propose dropping feature $j^{\\star}$. If multiple features tie for the largest loading within a numerical tolerance $\\epsilon$, choose the highest index among those ties to break the tie deterministically.\n5. Aggregate all proposed drops from step $4$ together with any zero-variance features detected in step $1$, deduplicate, and sort the resulting list in ascending order. Indices must be reported using zero-based indexing relative to the original columns of $X$.\n\nScientific realism and applicability: This procedure is grounded in the fact that small eigenvalues of a correlation (or covariance) matrix arise from directions in feature space with near-zero variance, which is consistent with near-linear dependencies across features, a hallmark of multicollinearity in statistical learning.\n\nAngle units and physical units are not involved in this problem. No percentages are required.\n\nTest suite:\nFor each test case, the program must use the provided $X$, the threshold $\\tau$, and the tie tolerance $\\epsilon$. The matrices are:\n\n- Test case $1$ (near-duplicate features; happy path):\n$$\nX^{(1)} =\n\\begin{bmatrix}\n0 & 0.00 + 0.00 & 1.00 \\\\\n1 & 1.00 + 0.05 & -0.50 \\\\\n2 & 2.00 - 0.02 & 0.70 \\\\\n3 & 3.00 + 0.10 & -1.20 \\\\\n4 & 4.00 - 0.05 & 0.30 \\\\\n5 & 5.00 + 0.03 & 0.80 \\\\\n6 & 6.00 - 0.08 & -0.90 \\\\\n7 & 7.00 + 0.02 & 1.10 \\\\\n8 & 8.00 + 0.00 & -0.40 \\\\\n9 & 9.00 - 0.07 & 0.20\n\\end{bmatrix}\n\\quad\n\\tau^{(1)} = 0.05\n\\quad\n\\epsilon^{(1)} = 10^{-12}.\n$$\nInterpretation: The second feature is nearly the first plus small noise; the third is unrelated.\n\n- Test case $2$ (approximately independent features; boundary condition with no detected multicollinearity):\n$$\nX^{(2)} =\n\\begin{bmatrix}\n0 & 1.10 & 0.50 \\\\\n1 & -0.90 & 0.30 \\\\\n2 & 0.70 & -0.60 \\\\\n3 & -0.30 & 0.70 \\\\\n4 & 0.20 & -0.80 \\\\\n5 & -0.40 & 0.10 \\\\\n6 & 0.80 & 0.20 \\\\\n7 & -0.50 & -0.30 \\\\\n8 & 0.60 & 0.40 \\\\\n9 & -0.70 & -0.20 \\\\\n10 & 0.90 & 0.00 \\\\\n11 & -1.00 & 0.50\n\\end{bmatrix}\n\\quad\n\\tau^{(2)} = 0.10\n\\quad\n\\epsilon^{(2)} = 10^{-12}.\n$$\n\n- Test case $3$ (one feature is a near-linear combination of two others; edge case with small noise):\n$$\nX^{(3)} =\n\\begin{bmatrix}\n2.00 & -1.00 & 2.00 + (-1.00) + 0.01 \\\\\n3.00 & -1.10 & 3.00 + (-1.10) - 0.02 \\\\\n4.00 & -0.90 & 4.00 + (-0.90) + 0.00 \\\\\n5.50 & -1.20 & 5.50 + (-1.20) + 0.03 \\\\\n6.10 & -0.80 & 6.10 + (-0.80) - 0.01 \\\\\n7.00 & -1.05 & 7.00 + (-1.05) + 0.02 \\\\\n8.20 & -1.00 & 8.20 + (-1.00) - 0.02 \\\\\n9.00 & -0.95 & 9.00 + (-0.95) + 0.01\n\\end{bmatrix}\n\\quad\n\\tau^{(3)} = 0.10\n\\quad\n\\epsilon^{(3)} = 10^{-12}.\n$$\n\n- Test case $4$ (four features with two near-dependencies and mild noise; stress test):\nLet the features be\n$$\nx_{1} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\\\ 5 \\\\ 6 \\\\ 7 \\\\ 8 \\\\ 9 \\end{bmatrix},\n\\quad\nx_{2} = \\begin{bmatrix} 9.00 \\\\ 7.95 \\\\ 7.00 \\\\ 6.05 \\\\ 5.00 \\\\ 3.95 \\\\ 3.00 \\\\ 2.05 \\\\ 1.00 \\end{bmatrix},\n\\quad\nx_{3} = \\begin{bmatrix} 1.50 \\\\ 0.00 \\\\ 1.70 \\\\ 0.20 \\\\ 1.40 \\\\ 0.30 \\\\ 1.80 \\\\ 0.10 \\\\ 1.60 \\end{bmatrix},\n$$\nand\n$$\nx_{4} = 0.50 \\, x_{1} + 0.50 \\, x_{3} + \\begin{bmatrix} 0.01 \\\\ -0.02 \\\\ 0.00 \\\\ 0.03 \\\\ -0.01 \\\\ 0.02 \\\\ -0.02 \\\\ 0.01 \\\\ 0.00 \\end{bmatrix}.\n$$\nThen\n$$\nX^{(4)} = \\begin{bmatrix} x_{1} & x_{2} & x_{3} & x_{4} \\end{bmatrix}\n\\quad\n\\tau^{(4)} = 0.08\n\\quad\n\\epsilon^{(4)} = 10^{-12}.\n$$\n\nFinal output specification:\n- For each test case, output a list of zero-based feature indices proposed for dropping, based on the aggregation rule described above.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each entry is itself a list of integers for the corresponding test case. For example, an output with four test cases could look like $[ [0,2], [], [3], [1] ]$, but with no spaces in the actual output string. The exact required format is a single line string of the form $[[\\dots],[\\dots],[\\dots],[\\dots]]$ with no spaces anywhere.", "solution": "The user-provided problem is assessed to be valid. It is scientifically grounded in the principles of multivariate statistics and linear algebra, well-posed with a clear and deterministic algorithmic procedure, and objective in its formulation. The problem addresses a standard technique in statistical learning for diagnosing multicollinearity.\n\nThe solution is derived from the geometric interpretation of the spectral decomposition of a correlation matrix. Multicollinearity signifies that one feature vector in a dataset can be closely approximated as a linear combination of other feature vectors. The algorithm specified in the problem formalizes a procedure to detect and resolve such dependencies.\n\nLet the dataset be represented by a matrix $X \\in \\mathbb{R}^{n \\times p}$, comprising $n$ observations of $p$ features, denoted by columns $x_1, \\dots, x_p$. The first step towards diagnosing multicollinearity involves standardizing the data. For each feature $j \\in \\{1, \\dots, p\\}$, the empirical mean $\\bar{x}_j$ and unbiased empirical standard deviation $s_j$ are calculated as:\n$$\n\\bar{x}_{j} = \\frac{1}{n} \\sum_{i=1}^{n} X_{ij}\n$$\n$$\ns_{j} = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^{n} \\left(X_{ij} - \\bar{x}_{j}\\right)^{2}}\n$$\nA feature $j$ with $s_j = 0$ is constant across all observations and carries no variance. Such a feature is non-informative and is immediately flagged for removal. For all features with $s_j > 0$, we create a standardized feature vector $z_j$ whose elements are $Z_{ij} = \\frac{X_{ij} - \\bar{x}_{j}}{s_{j}}$. These standardized vectors form the columns of the standardized design matrix $Z \\in \\mathbb{R}^{n \\times p'}$, where $p'$ is the number of non-constant features.\n\nThe empirical correlation matrix $R \\in \\mathbb{R}^{p' \\times p'}$ is computed from the standardized data as:\n$$\nR = \\frac{1}{n - 1} Z^{\\top} Z\n$$\nBy construction, $R$ is a symmetric and positive semi-definite matrix. Its diagonal elements $R_{jj}$ are all equal to $1$, and its off-diagonal elements $R_{jk}$ represent the sample correlation between features $j$ and $k$.\n\nThe cornerstone of the diagnostic procedure is the spectral decomposition of $R$:\n$$\nR = V \\Lambda V^{\\top}\n$$\nwhere $V$ is an orthogonal matrix whose columns $v_1, \\dots, v_{p'}$ are the eigenvectors of $R$, and $\\Lambda = \\mathrm{diag}(\\lambda_1, \\dots, \\lambda_{p'})$ is a diagonal matrix containing the corresponding non-negative eigenvalues, sorted in non-decreasing order.\n\nEach eigenvector $v_k$ represents a principal axis in the feature space, and the corresponding eigenvalue $\\lambda_k$ represents the variance of the standardized data when projected onto this axis. A near-linear dependency among the features translates to a direction in the feature space along which the data exhibits very little variance. Consequently, a very small eigenvalue, $\\lambda_k \\approx 0$, indicates the existence of multicollinearity. The corresponding eigenvector $v_k$ reveals the nature of this linear relationship. Specifically, if $\\lambda_k \\approx 0$, then the variance of the linear combination of standardized features $Z v_k$ is close to zero. This implies:\n$$\nZ v_k = \\sum_{j=1}^{p'} Z_{:,j} v_{k,j} \\approx 0\n$$\nwhere $v_{k,j}$ is the $j$-th component of the eigenvector $v_k$. This equation represents a near-linear dependency among the columns of $Z$.\n\nThe prescribed algorithm operationalizes this principle. It identifies all eigenvalues $\\lambda_k$ that fall below a given threshold $\\tau > 0$. For each such small eigenvalue, the associated eigenvector $v_k$ is inspected. The components $v_{k,j}$ of the eigenvector are the \"loadings\" of the original features in the linear dependency. To resolve the multicollinearity, one of the involved features must be removed. A common heuristic, adopted here, is to identify the feature that contributes most to the dependency—the one with the largest absolute loading $|v_{k,j}|$. Let this feature index be $j^{\\star}$. If multiple features exhibit the same maximal loading (within a numerical tolerance $\\epsilon$), the tie is broken by selecting the feature with the highest index. This feature $j^{\\star}$ is proposed for elimination.\n\nThe final set of features to be dropped is the union of the set of features with zero variance (identified initially) and the set of features proposed for elimination based on the small-eigenvalue analysis. The indices are deduplicated and sorted in ascending order to provide a deterministic final recommendation.\n\nThe algorithmic procedure is as follows:\n1.  For the input matrix $X$, compute the standard deviation for each feature column using a denominator of $n-1$. Identify the set of indices of columns with a standard deviation of $0$. These are the first candidates for removal.\n2.  Filter out the zero-variance columns from $X$ to form a new matrix $X'$. Keep a mapping of column indices from $X'$ back to $X$. If $X'$ has $1$ or fewer columns, the analysis stops, and only the zero-variance indices are reported.\n3.  Standardize $X'$ to obtain the matrix $Z$, where each column has a mean of $0$ and a standard deviation of $1$.\n4.  Compute the correlation matrix $R = \\frac{1}{n-1} Z^{\\top} Z$.\n5.  Perform an eigendecomposition of $R$ to obtain eigenvalues $\\lambda_k$ and eigenvectors $v_k$.\n6.  Initialize a set for features to be dropped with the indices from step $1$.\n7.  Iterate through each eigenvalue $\\lambda_k$. If $\\lambda_k < \\tau$:\n    a. Extract the corresponding eigenvector $v_k$.\n    b. Find the maximum absolute loading, $m = \\max_j |v_{k,j}|$.\n    c. Identify all indices $j'$ in the reduced feature set where $|v_{k,j'}|$ is within a tolerance $\\epsilon$ of $m$ (i.e., $m - |v_{k,j'}| \\le \\epsilon$).\n    d. From these tied indices, select the largest one, $j'_{\\text{drop}}$.\n    e. Map this reduced index $j'_{\\text{drop}}$ back to its original index in $X$ and add it to the set of features to be dropped.\n8.  Convert the final set of indices to a sorted list and return as the result for the given test case. This process is repeated for all test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the multicollinearity diagnosis problem for a suite of test cases.\n    \"\"\"\n\n    # Test case 1: Near-duplicate features\n    X1 = np.array([\n        [0.0, 0.00, 1.00], [1.0, 1.00 + 0.05, -0.50], [2.0, 2.00 - 0.02, 0.70],\n        [3.0, 3.00 + 0.10, -1.20], [4.0, 4.00 - 0.05, 0.30], [5.0, 5.00 + 0.03, 0.80],\n        [6.0, 6.00 - 0.08, -0.90], [7.0, 7.00 + 0.02, 1.10], [8.0, 8.00 + 0.00, -0.40],\n        [9.0, 9.00 - 0.07, 0.20]\n    ])\n    tau1 = 0.05\n    eps1 = 1e-12\n\n    # Test case 2: Approximately independent features\n    X2 = np.array([\n        [0.0, 1.10, 0.50], [1.0, -0.90, 0.30], [2.0, 0.70, -0.60], [3.0, -0.30, 0.70],\n        [4.0, 0.20, -0.80], [5.0, -0.40, 0.10], [6.0, 0.80, 0.20], [7.0, -0.50, -0.30],\n        [8.0, 0.60, 0.40], [9.0, -0.70, -0.20], [10.0, 0.90, 0.00], [11.0, -1.00, 0.50]\n    ])\n    tau2 = 0.10\n    eps2 = 1e-12\n\n    # Test case 3: One feature is a near-linear combination of two others\n    X3 = np.array([\n        [2.00, -1.00, 2.00 + (-1.00) + 0.01], [3.00, -1.10, 3.00 + (-1.10) - 0.02],\n        [4.00, -0.90, 4.00 + (-0.90) + 0.00], [5.50, -1.20, 5.50 + (-1.20) + 0.03],\n        [6.10, -0.80, 6.10 + (-0.80) - 0.01], [7.00, -1.05, 7.00 + (-1.05) + 0.02],\n        [8.20, -1.00, 8.20 + (-1.00) - 0.02], [9.00, -0.95, 9.00 + (-0.95) + 0.01],\n    ])\n    tau3 = 0.10\n    eps3 = 1e-12\n\n    # Test case 4: Four features with two near-dependencies\n    x1_4 = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9]).reshape(-1, 1)\n    x2_4 = np.array([9.00, 7.95, 7.00, 6.05, 5.00, 3.95, 3.00, 2.05, 1.00]).reshape(-1, 1)\n    x3_4 = np.array([1.50, 0.00, 1.70, 0.20, 1.40, 0.30, 1.80, 0.10, 1.60]).reshape(-1, 1)\n    delta4 = np.array([0.01, -0.02, 0.00, 0.03, -0.01, 0.02, -0.02, 0.01, 0.00]).reshape(-1, 1)\n    x4_4 = 0.5 * x1_4 + 0.5 * x3_4 + delta4\n    X4 = np.hstack([x1_4, x2_4, x3_4, x4_4])\n    tau4 = 0.08\n    eps4 = 1e-12\n\n    test_cases = [\n        (X1, tau1, eps1),\n        (X2, tau2, eps2),\n        (X3, tau3, eps3),\n        (X4, tau4, eps4),\n    ]\n\n    all_results = []\n\n    for X, tau, epsilon in test_cases:\n        n, p = X.shape\n        drops_to_propose = set()\n\n        if n  2:\n            # Cannot compute variance, add all features to drop if p > 0\n            if p > 0:\n                drops_to_propose.update(range(p))\n            all_results.append(sorted(list(drops_to_propose)))\n            continue\n\n        # Step 1: Standardize and handle zero-variance features\n        stds = np.std(X, axis=0, ddof=1)\n        zero_var_indices = np.where(stds == 0)[0]\n        drops_to_propose.update(zero_var_indices)\n\n        non_zero_var_mask = stds > 0\n        original_indices_map = np.arange(p)[non_zero_var_mask]\n        \n        # If 1 or 0 features remain, no multicollinearity to analyze\n        if len(original_indices_map) = 1:\n            all_results.append(sorted(list(drops_to_propose)))\n            continue\n        \n        X_filtered = X[:, non_zero_var_mask]\n        means_filtered = np.mean(X_filtered, axis=0)\n        stds_filtered = stds[non_zero_var_mask]\n\n        Z = (X_filtered - means_filtered) / stds_filtered\n\n        # Step 2: Compute the empirical correlation matrix R\n        R = (Z.T @ Z) / (n - 1)\n        \n        # Step 3: Compute the spectral decomposition\n        # np.linalg.eigh is for symmetric matrices, returns sorted eigenvalues\n        eigenvalues, eigenvectors = np.linalg.eigh(R)\n\n        # Step 4: Identify features to drop based on small eigenvalues\n        small_eigenvalue_indices = np.where(eigenvalues  tau)[0]\n        \n        for k in small_eigenvalue_indices:\n            eigenvector = eigenvectors[:, k]\n            abs_loadings = np.abs(eigenvector)\n            max_abs_loading = np.max(abs_loadings)\n            \n            # Identify indices of features tied for the largest loading\n            # A feature's loading is considered 'tied' if it's within epsilon of the max\n            tie_indices_filtered = np.where(max_abs_loading - abs_loadings = epsilon)[0]\n            \n            # Break tie by choosing the highest index\n            drop_candidate_filtered_idx = np.max(tie_indices_filtered)\n            \n            # Map back to original feature index\n            original_idx = original_indices_map[drop_candidate_filtered_idx]\n            drops_to_propose.add(original_idx)\n\n        # Step 5: Aggregate, deduplicate, and sort\n        final_drops = sorted(list(drops_to_propose))\n        all_results.append(final_drops)\n\n    # Final print statement in the exact required format\n    print(str(all_results).replace(\" \", \"\"))\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3117789"}, {"introduction": "Principal Component Analysis (PCA) is a cornerstone of dimensionality reduction, but is it always beneficial? This exercise challenges that assumption by exploring scenarios where PCA can actually harm the performance of a classifier [@problem_id:3117809]. By examining the alignment between the principal components (directions of high variance) and the directions that best separate classes, you will discover that maximizing variance does not always mean preserving the most useful information for prediction.", "problem": "Consider two classes modeled as multivariate normal distributions with equal prior probabilities and a common, symmetric positive-definite covariance matrix. Let the common covariance matrix be written in its spectral decomposition as an orthonormal eigenbasis with eigenpairs $\\{(\\lambda_i,\\mathbf{u}_i)\\}_{i=1}^p$, ordered so that $\\lambda_1 \\ge \\lambda_2 \\ge \\cdots \\ge \\lambda_p  0$. Let the mean difference be $\\Delta\\boldsymbol{\\mu} = \\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_0$, and define its coordinates in the covariance eigenbasis by $c_i = \\mathbf{u}_i^\\top \\Delta\\boldsymbol{\\mu}$ for $i \\in \\{1,\\dots,p\\}$. Principal Component Analysis (PCA) denoising with $k$ components corresponds to projecting onto the subspace spanned by $\\{\\mathbf{u}_1,\\dots,\\mathbf{u}_k\\}$ and then classifying in that subspace.\n\nStarting only from the following well-tested facts and core definitions:\n- For two multivariate normal classes with a common covariance and equal priors, the Linear Discriminant Analysis (LDA) decision rule is Bayes optimal.\n- The Mahalanobis distance between two means under a covariance matrix is a key quantity determining Bayes error.\n- Spectral decomposition expresses a covariance matrix as an orthonormal basis with nonnegative eigenvalues and allows orthogonal projection onto principal subspaces,\n\nderive how the optimal misclassification probability depends on the Mahalanobis distance, first in the full space, and then after PCA projection onto the top $k$ eigenvectors. Use this to show precisely when PCA denoising can harm classification by removing discriminative low-variance directions, in terms of $\\{(\\lambda_i,c_i)\\}$.\n\nYour program must, for each test case below, compute the change in the optimal misclassification probability due to PCA denoising, defined as\n- post-PCA optimal misclassification probability minus full-space optimal misclassification probability,\n\nand return this change as a real number. A positive value means PCA denoising increased the error (harmed classification), zero means no change, and a negative value would mean an improvement. You must:\n- treat PCA as keeping the top $k$ eigenvectors $\\{\\mathbf{u}_1,\\dots,\\mathbf{u}_k\\}$ associated with the $k$ largest eigenvalues $\\{\\lambda_1,\\dots,\\lambda_k\\}$,\n- assume $k \\in \\{0,1,\\dots,p\\}$,\n- work entirely in the eigenbasis using the given $(\\lambda_i)$ and $(c_i)$,\n- express all final numeric outputs rounded to $6$ decimal places.\n\nTest suite (each case is a triple $(\\boldsymbol{\\lambda},\\mathbf{c},k)$ with $\\boldsymbol{\\lambda}$ the list $(\\lambda_1,\\dots,\\lambda_p)$ and $\\mathbf{c}$ the list $(c_1,\\dots,c_p)$):\n- Case $1$ (discriminative low-variance direction removed): $\\boldsymbol{\\lambda} = (\\,10.0,\\,3.0,\\,1.0,\\,0.2\\,)$, $\\mathbf{c} = (\\,0.0,\\,0.0,\\,0.0,\\,1.0\\,)$, $k = 1$.\n- Case $2$ (no harm because signal lies in top direction): $\\boldsymbol{\\lambda} = (\\,10.0,\\,3.0,\\,1.0,\\,0.2\\,)$, $\\mathbf{c} = (\\,1.0,\\,0.0,\\,0.0,\\,0.0\\,)$, $k = 1$.\n- Case $3$ (isotropic covariance; dropping some signal components causes mild harm): $\\boldsymbol{\\lambda} = (\\,2.0,\\,2.0,\\,2.0,\\,2.0\\,)$, $\\mathbf{c} = (\\,1.0,\\,0.5,\\,0.3,\\,0.2\\,)$, $k = 2$.\n- Case $4$ (boundary; keep all components so no change): $\\boldsymbol{\\lambda} = (\\,5.0,\\,1.0,\\,0.5\\,)$, $\\mathbf{c} = (\\,0.4,\\,0.6,\\,0.8\\,)$, $k = 3$.\n- Case $5$ (boundary; keep $0$ components so classification becomes chance): $\\boldsymbol{\\lambda} = (\\,4.0,\\,1.0\\,)$, $\\mathbf{c} = (\\,1.0,\\,0.0\\,)$, $k = 0$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,\\dots,r_5]$), where $r_j$ is the change in optimal misclassification probability for the $j$-th test case, rounded to $6$ decimal places.", "solution": "We consider two multivariate normal classes with equal priors, common covariance, and means $\\boldsymbol{\\mu}_0$ and $\\boldsymbol{\\mu}_1$. Let $\\Delta\\boldsymbol{\\mu} = \\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_0$. Denote the common covariance matrix by $\\boldsymbol{\\Sigma}$ with spectral decomposition $\\boldsymbol{\\Sigma} = \\sum_{i=1}^p \\lambda_i \\mathbf{u}_i \\mathbf{u}_i^\\top$, where $\\{\\mathbf{u}_i\\}_{i=1}^p$ is an orthonormal basis and $\\lambda_i  0$ for all $i$. Define the coordinates $c_i = \\mathbf{u}_i^\\top \\Delta\\boldsymbol{\\mu}$.\n\nPrinciple-based derivation:\n- For two multivariate normal distributions with equal covariance and equal prior probabilities, the Linear Discriminant Analysis (LDA) rule is Bayes optimal. The discriminant direction is proportional to $\\boldsymbol{\\Sigma}^{-1} \\Delta\\boldsymbol{\\mu}$. The key scalar that determines the Bayes error is the Mahalanobis distance between the means under $\\boldsymbol{\\Sigma}$, defined by\n$$\nd^2 \\;=\\; \\Delta\\boldsymbol{\\mu}^\\top \\boldsymbol{\\Sigma}^{-1} \\Delta\\boldsymbol{\\mu}.\n$$\n- Using the spectral decomposition of $\\boldsymbol{\\Sigma}$, we have $\\boldsymbol{\\Sigma}^{-1} = \\sum_{i=1}^p \\lambda_i^{-1} \\mathbf{u}_i \\mathbf{u}_i^\\top$. Therefore,\n$$\nd^2 \\;=\\; \\Delta\\boldsymbol{\\mu}^\\top \\left( \\sum_{i=1}^p \\lambda_i^{-1} \\mathbf{u}_i \\mathbf{u}_i^\\top \\right) \\Delta\\boldsymbol{\\mu}\n\\;=\\; \\sum_{i=1}^p \\lambda_i^{-1} \\left( \\mathbf{u}_i^\\top \\Delta\\boldsymbol{\\mu} \\right)^2\n\\;=\\; \\sum_{i=1}^p \\frac{c_i^2}{\\lambda_i}.\n$$\n- The Bayes misclassification probability for equal priors in this setting is a monotone decreasing function of $d$, and is given by the Standard Normal Cumulative Distribution Function (CDF) evaluated at $-d/2$:\n$$\nP_{\\text{err, full}} \\;=\\; \\Phi\\!\\left( -\\,\\frac{d}{2} \\right),\n$$\nwhere $\\Phi$ is the Standard Normal CDF.\n\nNow consider Principal Component Analysis (PCA) denoising that keeps the top $k$ eigenvectors $\\{\\mathbf{u}_1,\\dots,\\mathbf{u}_k\\}$. Let the orthogonal projector onto this subspace be $\\mathbf{P}_k = \\sum_{i=1}^k \\mathbf{u}_i \\mathbf{u}_i^\\top$. In the projected space, the effective mean difference is $\\mathbf{P}_k \\Delta\\boldsymbol{\\mu}$ and the effective covariance is $\\mathbf{P}_k \\boldsymbol{\\Sigma} \\mathbf{P}_k = \\sum_{i=1}^k \\lambda_i \\mathbf{u}_i \\mathbf{u}_i^\\top$ restricted to the $k$-dimensional subspace. The corresponding Mahalanobis distance in this subspace becomes\n$$\nd_k^2 \\;=\\; (\\mathbf{P}_k \\Delta\\boldsymbol{\\mu})^\\top \\left( \\mathbf{P}_k \\boldsymbol{\\Sigma} \\mathbf{P}_k \\right)^{\\!-1} (\\mathbf{P}_k \\Delta\\boldsymbol{\\mu})\n\\;=\\; \\sum_{i=1}^k \\frac{c_i^2}{\\lambda_i}.\n$$\nTherefore, the optimal misclassification probability after PCA denoising is\n$$\nP_{\\text{err, PCA}(k)} \\;=\\; \\Phi\\!\\left( -\\,\\frac{d_k}{2} \\right).\n$$\nThe change in error due to PCA denoising is\n$$\n\\Delta P_{\\text{err}}(k) \\;=\\; P_{\\text{err, PCA}(k)} \\;-\\; P_{\\text{err, full}}\n\\;=\\; \\Phi\\!\\left( -\\,\\frac{1}{2}\\sqrt{\\sum_{i=1}^k \\frac{c_i^2}{\\lambda_i}} \\right)\n\\;-\\; \\Phi\\!\\left( -\\,\\frac{1}{2}\\sqrt{\\sum_{i=1}^p \\frac{c_i^2}{\\lambda_i}} \\right).\n$$\nFrom this expression, it is immediate that:\n- If all discriminative energy lies in dropped low-variance directions (i.e., $c_i = 0$ for $i \\le k$ and $c_j \\ne 0$ for some $j  k$), then $d_k = 0$ and $P_{\\text{err, PCA}(k)} = \\Phi(0) = 0.5$, while $P_{\\text{err, full}}  0.5$; hence $\\Delta P_{\\text{err}}(k)  0$ and PCA harms classification.\n- If all discriminative energy lies within the top $k$ directions (i.e., $c_i = 0$ for $i  k$), then $d_k = d$ and $\\Delta P_{\\text{err}}(k) = 0$; PCA does not change performance.\n- If some discriminative energy lies outside the top $k$ directions, then $0 \\le d_k  d$ and $\\Delta P_{\\text{err}}(k)  0$; PCA harms classification by removing useful directions. The harm is generally larger when removed directions have small $\\lambda_i$ (low variance) but nonnegligible $|c_i|$ (high discriminative content), since they contribute strongly to $d^2$ via $c_i^2/\\lambda_i$.\n\nAlgorithmic design for computation:\n- For each test case, compute $d^2 = \\sum_{i=1}^p c_i^2/\\lambda_i$ and $d_k^2 = \\sum_{i=1}^k c_i^2/\\lambda_i$ (with the convention that $d_0^2 = 0$).\n- Compute $P_{\\text{err, full}} = \\Phi(-\\sqrt{d^2}/2)$ and $P_{\\text{err, PCA}(k)} = \\Phi(-\\sqrt{d_k^2}/2)$, using $\\Phi(x) = \\tfrac{1}{2}\\left(1 + \\operatorname{erf}\\!\\left(\\tfrac{x}{\\sqrt{2}}\\right)\\right)$.\n- Output $\\Delta P_{\\text{err}}(k) = P_{\\text{err, PCA}(k)} - P_{\\text{err, full}}$, rounded to $6$ decimal places.\n\nQualitative expectations for the provided cases:\n- Case $1$: Strong harm because all signal lies in the smallest-variance direction, which is dropped when $k = 1$.\n- Case $2$: No harm because the signal lies entirely in the top direction kept by PCA.\n- Case $3$: Mild harm because isotropic covariance means each dropped component removes proportional discriminative content.\n- Case $4$: No harm because $k = p$ so nothing is dropped.\n- Case $5$: Maximal harm for the given setup because $k = 0$ leads to chance-level error $0.5$.\n\nThe program implements these computations and prints the rounded changes as a single bracketed list.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport math\n\ndef normal_cdf(x: float) -> float:\n    # Standard normal CDF using the error function\n    return 0.5 * (1.0 + math.erf(x / math.sqrt(2.0)))\n\ndef bayes_error_change(lambdas, c, k) -> float:\n    lambdas = np.array(lambdas, dtype=float)\n    c = np.array(c, dtype=float)\n    # Ensure inputs are consistent\n    assert lambdas.ndim == 1 and c.ndim == 1 and lambdas.shape == c.shape\n    p = lambdas.size\n    assert 0 = k = p\n\n    # Compute squared Mahalanobis distances\n    inv_terms = (c ** 2) / lambdas\n    d2_full = float(np.sum(inv_terms))\n    d2_k = float(np.sum(inv_terms[:k])) if k > 0 else 0.0\n\n    # Convert to Bayes errors using Phi(-d/2)\n    err_full = normal_cdf(-math.sqrt(d2_full) / 2.0) if d2_full > 0.0 else 0.5\n    err_k = normal_cdf(-math.sqrt(d2_k) / 2.0) if d2_k > 0.0 else 0.5\n\n    # Change in error (post-PCA minus full)\n    return err_k - err_full\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case: (lambdas, c, k)\n    test_cases = [\n        # Case 1\n        ([10.0, 3.0, 1.0, 0.2], [0.0, 0.0, 0.0, 1.0], 1),\n        # Case 2\n        ([10.0, 3.0, 1.0, 0.2], [1.0, 0.0, 0.0, 0.0], 1),\n        # Case 3\n        ([2.0, 2.0, 2.0, 2.0], [1.0, 0.5, 0.3, 0.2], 2),\n        # Case 4\n        ([5.0, 1.0, 0.5], [0.4, 0.6, 0.8], 3),\n        # Case 5\n        ([4.0, 1.0], [1.0, 0.0], 0),\n    ]\n\n    results = []\n    for lambdas, c, k in test_cases:\n        delta_err = bayes_error_change(lambdas, c, k)\n        results.append(f\"{delta_err:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3117809"}, {"introduction": "In our final practice, we venture beyond covariance matrices to apply spectral decomposition to graphs, a technique central to modern machine learning. You will construct a similarity graph from data points and then use the eigenvectors of the graph Laplacian to find a low-dimensional embedding that reveals latent clusters [@problem_id:3117759]. This powerful \"spectral embedding\" allows you to classify unlabeled data points by leveraging the discovered structure, providing a hands-on introduction to semi-supervised learning.", "problem": "You are given small labeled and unlabeled data sets intended to illustrate spectral methods in statistical learning. The goal is to construct a weighted graph from samples using cosine similarity, build a graph Laplacian, and use a spectral embedding based on the Laplacian’s eigenvectors to perform semi-supervised classification by nearest prototypes in the embedding space.\n\nStarting point: Use only fundamental definitions of similarity graphs and spectral properties of symmetric matrices. In particular:\n- The cosine similarity between two vectors $\\mathbf{x}_i \\in \\mathbb{R}^d$ and $\\mathbf{x}_j \\in \\mathbb{R}^d$ is defined as $\\cos(\\theta_{ij}) = \\dfrac{\\mathbf{x}_i^\\top \\mathbf{x}_j}{\\|\\mathbf{x}_i\\|\\|\\mathbf{x}_j\\|}$, with the convention that if either norm is zero the similarity is set to $0$.\n- Given nonnegative symmetric weights $w_{ij}$ for $i \\neq j$ and $w_{ii} = 0$, define the degree $d_i = \\sum_{j=1}^n w_{ij}$ and the unnormalized graph Laplacian $L = D - W$, where $D = \\mathrm{diag}(d_1,\\dots,d_n)$ and $W = [w_{ij}]$.\n- For any nonzero vector $\\mathbf{f} \\in \\mathbb{R}^n$, the Rayleigh quotient of $L$ is $R(\\mathbf{f}) = \\dfrac{\\mathbf{f}^\\top L \\mathbf{f}}{\\mathbf{f}^\\top \\mathbf{f}}$. The symmetric matrix $L$ admits an orthonormal basis of eigenvectors associated with real eigenvalues ordered as $0 = \\lambda_1 \\le \\lambda_2 \\le \\dots \\le \\lambda_n$. Minimizers of $R(\\mathbf{f})$ under orthogonality constraints correspond to eigenvectors of $L$.\n\nTask:\n- For each data set, construct a fully connected similarity graph on the $n$ samples using the cosine similarity but clipping negative values to zero: for $i \\neq j$, define $w_{ij} = \\max\\{0, \\cos(\\theta_{ij})\\}$ and $w_{ii} = 0$.\n- Form the unnormalized Laplacian $L = D - W$.\n- Compute the $k$-dimensional spectral embedding as follows. Let $k$ be the known number of classes. Compute all eigenpairs of $L$, sort eigenvalues in ascending order, discard the eigenvector corresponding to the smallest eigenvalue, and then take the next $k$ eigenvectors as columns of the embedding matrix $Y \\in \\mathbb{R}^{n \\times k}$. The embedded coordinate of sample $i$ is the $i$-th row $\\mathbf{y}_i^\\top$ of $Y$.\n- Semi-supervised classification: For each class $c \\in \\{0,\\dots,k-1\\}$, compute a class prototype in the embedding as the arithmetic mean of $\\mathbf{y}_i$ over all labeled samples with class $c$. Then assign each unlabeled sample to the class whose prototype is closest in Euclidean distance. Break distance ties by selecting the smallest class index.\n- Compute the accuracy on the unlabeled subset as the fraction of unlabeled samples whose predicted class equals the provided ground-truth class. Express accuracy as a decimal in $[0,1]$.\n\nImplement the above for the following test suite. In each case, $X$ is the data matrix with samples as rows, $y$ is the label vector with $-1$ denoting unlabeled, and $y^{\\mathrm{true}}$ is the ground-truth label vector. All numerical entries are real numbers and should be treated as dimensionless.\n\nTest case A (two latent classes, moderate separation in $\\mathbb{R}^3$):\n- Number of samples $n = 8$, number of features $d = 3$, number of classes $k = 2$.\n- Data matrix $X_A$ with rows\n  $\\big(1.0, 0.0, 0.0\\big)$,\n  $\\big(0.9, 0.1, 0.0\\big)$,\n  $\\big(0.95, 0.0, 0.1\\big)$,\n  $\\big(0.85, 0.2, 0.1\\big)$,\n  $\\big(0.0, 1.0, 0.0\\big)$,\n  $\\big(0.1, 0.9, 0.0\\big)$,\n  $\\big(0.0, 0.95, 0.1\\big)$,\n  $\\big(0.2, 0.85, 0.1\\big)$.\n- Ground-truth labels $y^{\\mathrm{true}}_A = [0, 0, 0, 0, 1, 1, 1, 1]$.\n- Semi-supervised labels $y_A = [0, -1, -1, -1, 1, -1, -1, -1]$.\n\nTest case B (three latent classes at approximately $120^\\circ$ separation in $\\mathbb{R}^2$):\n- Number of samples $n = 9$, number of features $d = 2$, number of classes $k = 3$.\n- Let $s = \\sqrt{3} / 2 \\approx 0.8660254$.\n- Data matrix $X_B$ with rows\n  $\\big(1.0, 0.0\\big)$,\n  $\\big(0.95, 0.1\\big)$,\n  $\\big(0.9, -0.1\\big)$,\n  $\\big(-0.5, s\\big)$,\n  $\\big(-0.6, 0.8\\big)$,\n  $\\big(-0.4, 0.9\\big)$,\n  $\\big(-0.5, -s\\big)$,\n  $\\big(-0.6, -0.8\\big)$,\n  $\\big(-0.4, -0.9\\big)$.\n- Ground-truth labels $y^{\\mathrm{true}}_B = [0, 0, 0, 1, 1, 1, 2, 2, 2]$.\n- Semi-supervised labels $y_B = [0, -1, -1, 1, -1, -1, 2, -1, -1]$.\n\nTest case C (two latent classes at approximately $30^\\circ$ separation in $\\mathbb{R}^3$):\n- Number of samples $n = 8$, number of features $d = 3$, number of classes $k = 2$.\n- Let $c = \\cos(\\pi/6) \\approx 0.8660254$ and $t = \\sin(\\pi/6) = 0.5$.\n- Data matrix $X_C$ with rows\n  $\\big(1.0, 0.0, 0.0\\big)$,\n  $\\big(0.95, 0.1, 0.0\\big)$,\n  $\\big(1.0, 0.05, 0.0\\big)$,\n  $\\big(0.9, 0.2, 0.0\\big)$,\n  $\\big(c, t, 0.0\\big)$,\n  $\\big(0.8, 0.55, 0.0\\big)$,\n  $\\big(0.9, 0.45, 0.0\\big)$,\n  $\\big(0.85, 0.52, 0.05\\big)$.\n- Ground-truth labels $y^{\\mathrm{true}}_C = [0, 0, 0, 0, 1, 1, 1, 1]$.\n- Semi-supervised labels $y_C = [0, -1, -1, -1, 1, -1, -1, -1]$.\n\nProgram requirements:\n- For each test case, build $W$, $D$, and $L$ exactly as described; compute the spectral embedding $Y$ using the $k$ eigenvectors obtained by discarding the eigenvector corresponding to the smallest eigenvalue and taking the next $k$ eigenvectors in ascending order; compute class prototypes in the embedding and classify unlabeled points by nearest prototype with Euclidean distance and the specified tie-breaking rule; and compute the unlabeled accuracy as a decimal in $[0,1]$ rounded to six decimal places.\n- Your program should produce a single line of output containing the three accuracies for Test case A, Test case B, and Test case C, in that order, as a comma-separated list enclosed in square brackets, for example, $[0.500000,1.000000,0.666667]$.\n\nNo additional input is provided to the program, and no external data sources are allowed. All computations are dimensionless, so no physical units are required. The angle in trigonometric constants is in radians. Ensure all numeric constants are implemented with the values shown above where applicable.", "solution": "The problem requires the implementation of a semi-supervised classification algorithm based on spectral graph theory. The procedure is applied to three distinct test cases, and the accuracy on the unlabeled portion of each dataset is to be reported. The method can be broken down into five primary steps: graph construction, Laplacian formation, spectral embedding, classification, and evaluation.\n\n### Step 1: Adjacency Matrix Construction\nFirst, we model the relationships between the $n$ data samples, represented as vectors $\\mathbf{x}_1, \\dots, \\mathbf{x}_n \\in \\mathbb{R}^d$, by constructing a weighted adjacency matrix $W \\in \\mathbb{R}^{n \\times n}$. The graph is fully connected. The weight $w_{ij}$ between two distinct samples $\\mathbf{x}_i$ and $\\mathbf{x}_j$ is derived from their cosine similarity, which measures the cosine of the angle between them:\n$$\n\\cos(\\theta_{ij}) = \\frac{\\mathbf{x}_i^\\top \\mathbf{x}_j}{\\|\\mathbf{x}_i\\|_2 \\|\\mathbf{x}_j\\|_2}\n$$\nThe problem specifies a convention that if either vector has a norm of zero, the similarity is $0$. As per the problem's instructions, negative similarity values are clipped to zero. The diagonal elements of the weight matrix are set to zero. Thus, the weights are defined as:\n$$\nw_{ij} = \\begin{cases} \\max\\{0, \\cos(\\theta_{ij})\\}  \\text{if } i \\neq j \\\\ 0  \\text{if } i = j \\end{cases}\n$$\nThe resulting matrix $W$ is symmetric with non-negative entries.\n\n### Step 2: Graph Laplacian Formation\nFrom the weight matrix $W$, we construct the unnormalized graph Laplacian $L$. This requires the degree matrix $D$, which is a diagonal matrix whose entries $D_{ii} = d_i$ are the sum of weights of all edges connected to vertex $i$:\n$$\nd_i = \\sum_{j=1}^n w_{ij}\n$$\nThe unnormalized graph Laplacian is then given by:\n$$\nL = D - W\n$$\n$L$ is a symmetric positive semi-definite matrix, a key property for the subsequent spectral analysis.\n\n### Step 3: Spectral Embedding\nThe core of the spectral method lies in analyzing the eigensystem of the graph Laplacian $L$. Since $L$ is real and symmetric, it has a full set of $n$ real eigenvalues, $0 = \\lambda_1 \\le \\lambda_2 \\le \\dots \\le \\lambda_n$, and a corresponding orthonormal basis of eigenvectors, $\\mathbf{v}_1, \\mathbf{v}_2, \\dots, \\mathbf{v}_n$. The eigenvectors corresponding to the smallest non-zero eigenvalues (known as Fiedler vectors) capture the global structure of the graph, effectively partitioning it into clusters.\n\nThe problem specifies the construction of a $k$-dimensional embedding, where $k$ is the number of classes. We compute all eigenpairs of $L$. The eigenvector $\\mathbf{v}_1$ associated with the smallest eigenvalue $\\lambda_1 = 0$ is discarded, as it is a constant vector (for a connected graph) and contains no information for clustering. The embedding matrix $Y \\in \\mathbb{R}^{n \\times k}$ is formed by taking the next $k$ eigenvectors as its columns:\n$$\nY = [\\mathbf{v}_2, \\mathbf{v}_3, \\dots, \\mathbf{v}_{k+1}]\n$$\nThe $i$-th row of $Y$, denoted $\\mathbf{y}_i^\\top$, serves as the new $k$-dimensional coordinate vector for the original sample $\\mathbf{x}_i$. This mapping from $\\mathbb{R}^d$ to $\\mathbb{R}^k$ is the spectral embedding.\n\n### Step 4: Semi-Supervised Classification\nWith the data embedded in the lower-dimensional space, we perform classification. The dataset provides labels for a small subset of samples, while the rest are unlabeled. For each class $c \\in \\{0, \\dots, k-1\\}$, we compute a class prototype, $\\mathbf{p}_c$, by taking the arithmetic mean of the embedded vectors of all labeled samples belonging to that class. Let $I_c$ be the set of indices of samples labeled with class $c$. The prototype is:\n$$\n\\mathbf{p}_c = \\frac{1}{|I_c|} \\sum_{i \\in I_c} \\mathbf{y}_i\n$$\nEach unlabeled sample $j$ is then classified by assigning it to the class of the nearest prototype in the embedding space. The distance metric is Euclidean distance. The predicted label, $\\hat{y}_j$, is:\n$$\n\\hat{y}_j = \\arg\\min_{c \\in \\{0, \\dots, k-1\\}} \\|\\mathbf{y}_j - \\mathbf{p}_c\\|_2\n$$\nTies in distance are broken by selecting the class with the smallest index.\n\n### Step 5: Accuracy Evaluation\nThe final step is to evaluate the performance of this semi-supervised classifier. The accuracy is calculated as the fraction of unlabeled samples for which the predicted label $\\hat{y}_j$ matches the provided ground-truth label $y^{\\mathrm{true}}_j$. Let $S_U$ be the set of indices of unlabeled samples. The accuracy is:\n$$\n\\text{Accuracy} = \\frac{1}{|S_U|} \\sum_{j \\in S_U} \\mathbb{I}(\\hat{y}_j = y^{\\mathrm{true}}_j)\n$$\nwhere $\\mathbb{I}(\\cdot)$ is the indicator function, which is $1$ if its argument is true and $0$ otherwise. This procedure is applied systematically to the three test cases provided.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef compute_accuracy(X, y, y_true, k):\n    \"\"\"\n    Computes semi-supervised classification accuracy using spectral methods.\n    \n    Args:\n        X (np.ndarray): Data matrix (n_samples, n_features).\n        y (np.ndarray): Label vector (-1 for unlabeled).\n        y_true (np.ndarray): Ground-truth label vector.\n        k (int): Number of classes.\n\n    Returns:\n        float: Accuracy on the unlabeled subset.\n    \"\"\"\n    n = X.shape[0]\n\n    # Step 1: Construct the Adjacency Matrix W\n    # Normalize rows of X to compute cosine similarity more efficiently.\n    norms = np.linalg.norm(X, axis=1, keepdims=True)\n    X_normalized = np.divide(X, norms, out=np.zeros_like(X, dtype=float), where=norms!=0)\n    \n    # Cosine similarity matrix is the dot product of normalized vectors.\n    cos_sim_matrix = X_normalized @ X_normalized.T\n    \n    # Build W by clipping negative values and setting diagonal to zero.\n    W = np.maximum(0, cos_sim_matrix)\n    np.fill_diagonal(W, 0)\n    \n    # Step 2: Construct the Graph Laplacian L\n    d = np.sum(W, axis=1)\n    D = np.diag(d)\n    L = D - W\n    \n    # Step 3: Compute the Spectral Embedding Y\n    # eigh returns eigenvalues in ascending order and corresponding eigenvectors.\n    eigenvalues, eigenvectors = np.linalg.eigh(L)\n    \n    # Discard the eigenvector for the smallest eigenvalue and take the next k.\n    Y = eigenvectors[:, 1:k+1]\n    \n    # Step 4: Semi-supervised Classification\n    labeled_mask = (y != -1)\n    unlabeled_mask = ~labeled_mask\n    \n    # Compute class prototypes in the embedding space.\n    prototypes = []\n    for c in range(k):\n        class_mask = (y == c)\n        class_embeddings = Y[class_mask, :]\n        \n        # This check is for robustness; problem data guarantees labeled samples for each class.\n        if class_embeddings.shape[0] > 0:\n            prototype = np.mean(class_embeddings, axis=0)\n            prototypes.append(prototype)\n        else:\n            # Handle case where a class has no labeled samples (not in this problem)\n            # A possible strategy is to place the prototype at the origin.\n            prototypes.append(np.zeros(k))\n\n    prototypes = np.array(prototypes)\n    \n    # Classify unlabeled samples by nearest prototype.\n    unlabeled_embeddings = Y[unlabeled_mask, :]\n    predictions = []\n    for emb in unlabeled_embeddings:\n        # Calculate Euclidean distance to all prototypes\n        distances = np.linalg.norm(prototypes - emb, axis=1)\n        # argmin breaks ties by choosing the smallest index, as required.\n        predicted_class = np.argmin(distances)\n        predictions.append(predicted_class)\n    \n    predictions = np.array(predictions)\n    \n    # Step 5: Calculate Accuracy\n    true_labels_unlabeled = y_true[unlabeled_mask]\n    \n    correct_count = np.sum(predictions == true_labels_unlabeled)\n    total_unlabeled = len(true_labels_unlabeled)\n    \n    if total_unlabeled == 0:\n        accuracy = 1.0 #Convention for no unlabeled points\n    else:\n        accuracy = correct_count / total_unlabeled\n        \n    return accuracy\n\ndef solve():\n    \"\"\"\n    Main function to define test cases, run the algorithm, and print results.\n    \"\"\"\n    # Define constants for test cases B and C\n    s_b = np.sqrt(3) / 2\n    c_c = np.cos(np.pi / 6)\n    t_c = np.sin(np.pi / 6)\n\n    test_cases = [\n        {\n            \"X\": np.array([\n                [1.0, 0.0, 0.0], [0.9, 0.1, 0.0], [0.95, 0.0, 0.1], [0.85, 0.2, 0.1],\n                [0.0, 1.0, 0.0], [0.1, 0.9, 0.0], [0.0, 0.95, 0.1], [0.2, 0.85, 0.1]\n            ]),\n            \"y\": np.array([0, -1, -1, -1, 1, -1, -1, -1]),\n            \"y_true\": np.array([0, 0, 0, 0, 1, 1, 1, 1]),\n            \"k\": 2\n        },\n        {\n            \"X\": np.array([\n                [1.0, 0.0], [0.95, 0.1], [0.9, -0.1],\n                [-0.5, s_b], [-0.6, 0.8], [-0.4, 0.9],\n                [-0.5, -s_b], [-0.6, -0.8], [-0.4, -0.9]\n            ]),\n            \"y\": np.array([0, -1, -1, 1, -1, -1, 2, -1, -1]),\n            \"y_true\": np.array([0, 0, 0, 1, 1, 1, 2, 2, 2]),\n            \"k\": 3\n        },\n        {\n            \"X\": np.array([\n                [1.0, 0.0, 0.0], [0.95, 0.1, 0.0], [1.0, 0.05, 0.0], [0.9, 0.2, 0.0],\n                [c_c, t_c, 0.0], [0.8, 0.55, 0.0], [0.9, 0.45, 0.0], [0.85, 0.52, 0.05]\n            ]),\n            \"y\": np.array([0, -1, -1, -1, 1, -1, -1, -1]),\n            \"y_true\": np.array([0, 0, 0, 0, 1, 1, 1, 1]),\n            \"k\": 2\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        accuracy = compute_accuracy(case[\"X\"], case[\"y\"], case[\"y_true\"], case[\"k\"])\n        results.append(accuracy)\n\n    # Format output to six decimal places as specified.\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3117759"}]}