{"hands_on_practices": [{"introduction": "The Naive Bayes classifier is a powerful tool, but its strength is rooted in a strong assumption: that all features are conditionally independent given the class label. This exercise explores the consequences of violating this assumption. By constructing a scenario with a latent variable $Z$ that induces correlation between features $X_1$ and $X_2$, you will use the law of total probability to compute the true posterior probability and compare it to the Naive Bayes estimate, thereby quantifying the model's bias [@problem_id:3184732]. This practice is fundamental for understanding the importance of model assumptions and the role of latent variables in generative modeling.", "problem": "A binary classification problem is modeled generatively with a binary label $Y \\in \\{0,1\\}$, a binary latent variable $Z \\in \\{0,1\\}$, and two binary observed features $X_1, X_2 \\in \\{0,1\\}$. The joint data-generating process is as follows. The prior probabilities are $P(Y=1)=0.4$ and $P(Y=0)=0.6$. The latent variable depends on the label via $P(Z=1 \\mid Y=1)=0.7$, $P(Z=0 \\mid Y=1)=0.3$, $P(Z=1 \\mid Y=0)=0.2$, and $P(Z=0 \\mid Y=0)=0.8$. The observed features are conditionally independent given the pair $(Y,Z)$, that is, $X_1 \\perp X_2 \\mid (Y,Z)$, with conditional probabilities specified by\n$$P(X_1=1 \\mid Y=1, Z=1)=0.95,\\quad P(X_1=1 \\mid Y=1, Z=0)=0.30,$$\n$$P(X_1=1 \\mid Y=0, Z=1)=0.85,\\quad P(X_1=1 \\mid Y=0, Z=0)=0.10,$$\n$$P(X_2=1 \\mid Y=1, Z=1)=0.90,\\quad P(X_2=1 \\mid Y=1, Z=0)=0.25,$$\n$$P(X_2=1 \\mid Y=0, Z=1)=0.80,\\quad P(X_2=1 \\mid Y=0, Z=0)=0.05.$$\n\nYou observe $X_1=1$ and $X_2=1$. Using only the axioms of probability, the law of total probability, and Bayes' theorem, and starting from these generative assumptions, perform the following:\n\n1. Derive, from first principles, the expression for the posterior $P(Y=1 \\mid X_1=1, X_2=1)$ that correctly accounts for the latent variable $Z$, and compute its exact numerical value.\n\n2. Under the Naive Bayes (NB) assumption that $X_1$ and $X_2$ are conditionally independent given $Y$ alone (ignoring $Z$), derive the corresponding posterior $P_{\\mathrm{NB}}(Y=1 \\mid X_1=1, X_2=1)$ and compute its exact numerical value.\n\n3. Report the multiplicative bias factor defined as $\\dfrac{P(Y=1 \\mid X_1=1, X_2=1)}{P_{\\mathrm{NB}}(Y=1 \\mid X_1=1, X_2=1)}$ as a single exact value.\n\nExpress the final bias factor exactly; do not round.", "solution": "The user wants to compute a multiplicative bias factor comparing a posterior probability from a full generative model with one from a simplified Naive Bayes model.\n\nThe problem asks for three quantities:\n1. The posterior probability $P(Y=1 \\mid X_1=1, X_2=1)$ from the full generative model.\n2. The posterior probability $P_{\\mathrm{NB}}(Y=1 \\mid X_1=1, X_2=1)$ under a Naive Bayes assumption.\n3. The multiplicative bias factor, defined as the ratio of the first quantity to the second.\n\nLet the event of interest be $A = \\{X_1=1, X_2=1\\}$.\n\n### Step 1: Extract Givens\nThe verbatim givens are:\n-   Binary label $Y \\in \\{0,1\\}$, latent variable $Z \\in \\{0,1\\}$, features $X_1, X_2 \\in \\{0,1\\}$.\n-   Priors on $Y$: $P(Y=1)=0.4$, $P(Y=0)=0.6$.\n-   Conditional probabilities for $Z$ given $Y$:\n    -   $P(Z=1 \\mid Y=1)=0.7$, $P(Z=0 \\mid Y=1)=0.3$.\n    -   $P(Z=1 \\mid Y=0)=0.2$, $P(Z=0 \\mid Y=0)=0.8$.\n-   Conditional probabilities for $X_1$ given $(Y,Z)$:\n    -   $P(X_1=1 \\mid Y=1, Z=1)=0.95$.\n    -   $P(X_1=1 \\mid Y=1, Z=0)=0.30$.\n    -   $P(X_1=1 \\mid Y=0, Z=1)=0.85$.\n    -   $P(X_1=1 \\mid Y=0, Z=0)=0.10$.\n-   Conditional probabilities for $X_2$ given $(Y,Z)$:\n    -   $P(X_2=1 \\mid Y=1, Z=1)=0.90$.\n    -   $P(X_2=1 \\mid Y=1, Z=0)=0.25$.\n    -   $P(X_2=1 \\mid Y=0, Z=1)=0.80$.\n    -   $P(X_2=1 \\mid Y=0, Z=0)=0.05$.\n-   Conditional Independence: $X_1 \\perp X_2 \\mid (Y,Z)$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded in probability theory and statistical learning. It describes a standard generative model with a latent variable. All probabilities provided are valid (i.e., in the range $[0, 1]$) and consistent (e.g., $P(Y=1)+P(Y=0)=1$). The problem is well-posed, providing all necessary information to answer the questions posed. The language is objective and precise. The problem is a valid exercise in applying the law of total probability and Bayes' theorem.\n\n### Step 3: Verdict and Action\nThe problem is valid. We proceed to the solution.\n\n### Part 1: Derivation of the Correct Posterior $P(Y=1 \\mid X_1=1, X_2=1)$\n\nWe wish to compute $P(Y=1 \\mid A)$. Using Bayes' theorem:\n$$P(Y=1 \\mid A) = \\frac{P(A \\mid Y=1) P(Y=1)}{P(A)}$$\nThe denominator, $P(A)$, is the marginal probability of the evidence, which can be expanded using the law of total probability:\n$$P(A) = P(A \\mid Y=1)P(Y=1) + P(A \\mid Y=0)P(Y=0)$$\nThe core task is to compute the class-conditional likelihoods $P(A \\mid Y=y) = P(X_1=1, X_2=1 \\mid Y=y)$ for $y \\in \\{0, 1\\}$. Since the model includes the latent variable $Z$, we must marginalize it out using the law of total probability:\n$$P(A \\mid Y=y) = \\sum_{z \\in \\{0,1\\}} P(X_1=1, X_2=1, Z=z \\mid Y=y)$$\nUsing the chain rule of probability, $P(B,C \\mid D) = P(B \\mid C,D)P(C \\mid D)$, we get:\n$$P(A \\mid Y=y) = \\sum_{z \\in \\{0,1\\}} P(X_1=1, X_2=1 \\mid Y=y, Z=z) P(Z=z \\mid Y=y)$$\nThe problem states that $X_1 \\perp X_2 \\mid (Y,Z)$, so $P(X_1=1, X_2=1 \\mid Y=y, Z=z) = P(X_1=1 \\mid Y=y, Z=z) P(X_2=1 \\mid Y=y, Z=z)$.\nSubstituting this into the summation gives the full expression for the likelihood:\n$$P(A \\mid Y=y) = \\sum_{z \\in \\{0,1\\}} P(X_1=1 \\mid Y=y, Z=z) P(X_2=1 \\mid Y=y, Z=z) P(Z=z \\mid Y=y)$$\n\nNow, we compute the numerical values.\nFor $Y=1$:\n$$P(A \\mid Y=1) = P(X_1=1|Y=1,Z=1)P(X_2=1|Y=1,Z=1)P(Z=1|Y=1) + P(X_1=1|Y=1,Z=0)P(X_2=1|Y=1,Z=0)P(Z=0|Y=1)$$\n$$P(A \\mid Y=1) = (0.95)(0.90)(0.7) + (0.30)(0.25)(0.3) = 0.5985 + 0.0225 = 0.621$$\n\nFor $Y=0$:\n$$P(A \\mid Y=0) = P(X_1=1|Y=0,Z=1)P(X_2=1|Y=0,Z=1)P(Z=1|Y=0) + P(X_1=1|Y=0,Z=0)P(X_2=1|Y=0,Z=0)P(Z=0|Y=0)$$\n$$P(A \\mid Y=0) = (0.85)(0.80)(0.2) + (0.10)(0.05)(0.8) = 0.136 + 0.004 = 0.140$$\n\nNow we can compute the terms for Bayes' theorem.\nThe joint probability $P(A, Y=1) = P(A \\mid Y=1)P(Y=1) = 0.621 \\times 0.4 = 0.2484$.\nThe joint probability $P(A, Y=0) = P(A \\mid Y=0)P(Y=0) = 0.140 \\times 0.6 = 0.084$.\nThe marginal evidence is $P(A) = P(A, Y=1) + P(A, Y=0) = 0.2484 + 0.084 = 0.3324$.\n\nThe posterior is:\n$$P(Y=1 \\mid A) = \\frac{0.2484}{0.3324} = \\frac{2484}{3324} = \\frac{621}{831} = \\frac{207}{277}$$\n\n### Part 2: Derivation of the Naive Bayes Posterior $P_{\\mathrm{NB}}(Y=1 \\mid X_1=1, X_2=1)$\n\nThe Naive Bayes (NB) model assumes that the features $X_1$ and $X_2$ are conditionally independent given the label $Y$ alone: $X_1 \\perp X_2 \\mid Y$. The posterior is given by:\n$$P_{\\mathrm{NB}}(Y=1 \\mid A) = \\frac{P_{\\mathrm{NB}}(A \\mid Y=1) P(Y=1)}{P_{\\mathrm{NB}}(A)}$$\nUnder the NB assumption, the likelihood is $P_{\\mathrm{NB}}(A \\mid Y=y) = P(X_1=1 \\mid Y=y) P(X_2=1 \\mid Y=y)$.\n\nThe required marginal conditional probabilities $P(X_i=1 \\mid Y=y)$ are not directly given. They must be derived from the full generative model by marginalizing out the latent variable $Z$:\n$$P(X_i=1 \\mid Y=y) = \\sum_{z \\in \\{0,1\\}} P(X_i=1 \\mid Y=y, Z=z) P(Z=z \\mid Y=y)$$\n\nWe compute these marginals:\n$$P(X_1=1 \\mid Y=1) = (0.95)(0.7) + (0.30)(0.3) = 0.665 + 0.09 = 0.755$$\n$$P(X_2=1 \\mid Y=1) = (0.90)(0.7) + (0.25)(0.3) = 0.63 + 0.075 = 0.705$$\n$$P(X_1=1 \\mid Y=0) = (0.85)(0.2) + (0.10)(0.8) = 0.17 + 0.08 = 0.25$$\n$$P(X_2=1 \\mid Y=0) = (0.80)(0.2) + (0.05)(0.8) = 0.16 + 0.04 = 0.20$$\n\nNow, we compute the NB likelihoods:\n$$P_{\\mathrm{NB}}(A \\mid Y=1) = P(X_1=1 \\mid Y=1)P(X_2=1 \\mid Y=1) = 0.755 \\times 0.705 = 0.532275$$\n$$P_{\\mathrm{NB}}(A \\mid Y=0) = P(X_1=1 \\mid Y=0)P(X_2=1 \\mid Y=0) = 0.25 \\times 0.20 = 0.05$$\n\nNext, we compute the terms for the NB version of Bayes' theorem.\nThe joint probability $P_{\\mathrm{NB}}(A, Y=1) = P_{\\mathrm{NB}}(A \\mid Y=1)P(Y=1) = 0.532275 \\times 0.4 = 0.21291$.\nThe joint probability $P_{\\mathrm{NB}}(A, Y=0) = P_{\\mathrm{NB}}(A \\mid Y=0)P(Y=0) = 0.05 \\times 0.6 = 0.03$.\nThe NB marginal evidence is $P_{\\mathrm{NB}}(A) = P_{\\mathrm{NB}}(A, Y=1) + P_{\\mathrm{NB}}(A, Y=0) = 0.21291 + 0.03 = 0.24291$.\n\nThe NB posterior is:\n$$P_{\\mathrm{NB}}(Y=1 \\mid A) = \\frac{0.21291}{0.24291} = \\frac{21291}{24291}$$\nSimplifying this fraction by dividing the numerator and denominator by their greatest common divisor, which is $3$:\n$$P_{\\mathrm{NB}}(Y=1 \\mid A) = \\frac{7097}{8097}$$\n\n### Part 3: Calculation of the Multiplicative Bias Factor\n\nThe multiplicative bias factor is the ratio of the true posterior to the NB posterior:\n$$\\text{Bias Factor} = \\frac{P(Y=1 \\mid A)}{P_{\\mathrm{NB}}(Y=1 \\mid A)} = \\frac{207/277}{21291/24291}$$\n$$\\text{Bias Factor} = \\frac{207}{277} \\times \\frac{24291}{21291}$$\nWe can work with the un-simplified fraction for the NB posterior to see if cancellations occur:\n$$\\text{Bias Factor} = \\frac{P(A, Y=1) / P(A)}{P_{\\mathrm{NB}}(A, Y=1) / P_{\\mathrm{NB}}(A)} = \\frac{0.2484 / 0.3324}{0.21291 / 0.24291}$$\n$$\\text{Bias Factor} = \\frac{2484/10000}{3324/10000} \\times \\frac{24291/100000}{21291/100000} = \\frac{2484}{3324} \\times \\frac{24291}{21291}$$\nLet's express the components as fractions:\n$P(A \\mid Y=1) = \\frac{621}{1000}$\n$P_{\\mathrm{NB}}(A \\mid Y=1) = \\frac{755}{1000} \\times \\frac{705}{1000} = \\frac{532275}{1000000} = \\frac{21291}{40000}$\n$P(A) = \\frac{3324}{10000}$\n$P_{\\mathrm{NB}}(A) = \\frac{24291}{100000}$\n\nThe bias factor can be written as $\\frac{P(A|Y=1)}{P_{\\mathrm{NB}}(A|Y=1)} \\times \\frac{P_{\\mathrm{NB}}(A)}{P(A)}$.\n$$\\text{Bias Factor} = \\frac{621/1000}{21291/40000} \\times \\frac{24291/100000}{3324/10000}$$\n$$\\text{Bias Factor} = \\left(\\frac{621}{1000} \\times \\frac{40000}{21291}\\right) \\times \\left(\\frac{24291}{100000} \\times \\frac{10000}{3324}\\right)$$\n$$\\text{Bias Factor} = \\left(\\frac{621 \\times 40}{21291}\\right) \\times \\left(\\frac{24291}{10 \\times 3324}\\right)$$\nWe know $621 = 3 \\times 207$ and $21291 = 3 \\times 7097$. Also $24291 = 9 \\times 2699$ and $3324 = 12 \\times 277$.\n$$\\text{Bias Factor} = \\left(\\frac{207 \\times 40}{7097}\\right) \\times \\left(\\frac{9 \\times 2699}{10 \\times 12 \\times 277}\\right)$$\n$$\\text{Bias Factor} = \\left(\\frac{207 \\times 40}{7097}\\right) \\times \\left(\\frac{3 \\times 2699}{40 \\times 277}\\right)$$\nThe factor of $40$ cancels:\n$$\\text{Bias Factor} = \\frac{207 \\times 3 \\times 2699}{277 \\times 7097} = \\frac{621 \\times 2699}{1965869} = \\frac{1676079}{1965869}$$", "answer": "$$\\boxed{\\frac{1676079}{1965869}}$$", "id": "3184732"}, {"introduction": "Statistical analysis can sometimes yield counter-intuitive results, and one of the most famous is Simpson's paradox, where a trend that appears in different groups of data reverses when the groups are combined. In this exercise, you will engineer a scenario demonstrating this paradox by manipulating the distribution of a feature $X$ across two subgroups, indicated by a variable $Z$ [@problem_id:3184667]. By applying the law of total probability to marginalize the subgroup variable $Z$, you will discover the precise conditions under which an aggregate conclusion can be the opposite of the conclusion within every single subgroup, a vital lesson in cautious data interpretation.", "problem": "A binary classification task involves a binary feature $X \\in \\{0,1\\}$, a binary outcome $Y \\in \\{0,1\\}$, and a binary subgroup indicator $Z \\in \\{0,1\\}$. Within each subgroup $Z=z$, the association between $X$ and $Y$ favors $X=1$ in the sense that $P(Y=1 \\mid X=1, Z=z) > P(Y=1 \\mid X=0, Z=z)$ for both $z=0$ and $z=1$. Concretely, the subgroup-conditional success rates are:\n- For $Z=1$: $P(Y=1 \\mid X=1, Z=1) = 0.62$ and $P(Y=1 \\mid X=0, Z=1) = 0.60$.\n- For $Z=0$: $P(Y=1 \\mid X=1, Z=0) = 0.52$ and $P(Y=1 \\mid X=0, Z=0) = 0.50$.\n\nThe subgroup prevalence is $P(Z=1) = 0.50$. The feature propensity within subgroups is $P(X=1 \\mid Z=1) = 0.20$ and $P(X=1 \\mid Z=0) = b$, where $b \\in [0,1]$ is an unknown design parameter under your control.\n\nUsing only the definition of conditional probability, the law of total probability, and Bayes’ theorem, derive $P(Y=1 \\mid X=1)$ and $P(Y=1 \\mid X=0)$ as functions of $b$ by appropriately marginalizing over $Z$. Then, determine the exact threshold value $b^{\\star} \\in [0,1]$ at which the aggregated association between $X$ and $Y$ reverses in the sense that $P(Y=1 \\mid X=1) = P(Y=1 \\mid X=0)$; for $b > b^{\\star}$ the aggregated association exhibits a Simpson’s paradox reversal, i.e., $P(Y=1 \\mid X=1) < P(Y=1 \\mid X=0)$, even though within each subgroup $X=1$ has the higher success rate.\n\nProvide your final answer as the exact closed-form value of $b^{\\star}$. Do not round.", "solution": "The problem is first validated to ensure it is self-contained, consistent, and scientifically sound.\n\n### Step 1: Extract Givens\nThe problem provides the following data and definitions for a binary classification task with a feature $X \\in \\{0,1\\}$, an outcome $Y \\in \\{0,1\\}$, and a subgroup indicator $Z \\in \\{0,1\\}$:\n- Subgroup-conditional success rates for $Z=1$:\n  - $P(Y=1 \\mid X=1, Z=1) = 0.62$\n  - $P(Y=1 \\mid X=0, Z=1) = 0.60$\n- Subgroup-conditional success rates for $Z=0$:\n  - $P(Y=1 \\mid X=1, Z=0) = 0.52$\n  - $P(Y=1 \\mid X=0, Z=0) = 0.50$\n- Subgroup prevalence:\n  - $P(Z=1) = 0.50$, which implies $P(Z=0) = 1 - 0.50 = 0.50$.\n- Feature propensity within subgroups:\n  - $P(X=1 \\mid Z=1) = 0.20$\n  - $P(X=1 \\mid Z=0) = b$, where $b \\in [0,1]$.\n- Objective: Find the threshold value $b^{\\star}$ where $P(Y=1 \\mid X=1) = P(Y=1 \\mid X=0)$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is a well-defined exercise in probability theory, specifically concerning the law of total probability, Bayes' theorem, and the statistical phenomenon known as Simpson's paradox.\n- **Scientifically Grounded:** The problem is based on the standard axioms and theorems of probability theory. Simpson's paradox is a well-documented and fundamental concept in statistics.\n- **Well-Posed:** All necessary probabilities are provided to define the system. The objective is to solve for a single parameter $b$ based on a single equation, which is expected to yield a unique solution within the specified domain $b \\in [0,1]$.\n- **Objective:** The problem is stated using precise mathematical notation and objective language, free of any subjective or ambiguous claims.\n- **Consistency:** The provided conditional probabilities $P(Y=1 \\mid X=x, Z=z)$ are all valid probabilities in $[0,1]$. For both subgroups ($z=0$ and $z=1$), the association is positive: $P(Y=1 \\mid X=1, Z=z) > P(Y=1 \\mid X=0, Z=z)$.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be derived.\n\nThe goal is to find the value of $b$, denoted $b^{\\star}$, for which the aggregated success rates are equal: $P(Y=1 \\mid X=1) = P(Y=1 \\mid X=0)$. To do this, we must first express these two conditional probabilities as functions of $b$ by marginalizing over the subgroup variable $Z$.\n\nWe can find $P(Y=1 \\mid X=x)$ using the definition of conditional probability, $P(A \\mid B) = \\frac{P(A, B)}{P(B)}$. Thus, we need to calculate the joint probabilities $P(Y=1, X=x)$ and the marginal probabilities $P(X=x)$.\n\nFirst, we calculate the marginal probability of $X=1$ using the law of total probability, conditioning on $Z$:\n$$P(X=1) = P(X=1 \\mid Z=1)P(Z=1) + P(X=1 \\mid Z=0)P(Z=0)$$\nSubstituting the given values:\n$$P(X=1) = (0.20)(0.50) + (b)(0.50) = 0.10 + 0.50b$$\nThe probability of $X=0$ is the complement:\n$$P(X=0) = 1 - P(X=1) = 1 - (0.10 + 0.50b) = 0.90 - 0.50b$$\n\nNext, we calculate the joint probability $P(Y=1, X=1)$ by marginalizing over $Z$:\n$$P(Y=1, X=1) = P(Y=1, X=1 \\mid Z=1)P(Z=1) + P(Y=1, X=1 \\mid Z=0)P(Z=0)$$\nUsing the chain rule for conditional probability, $P(A,B \\mid C) = P(A \\mid B,C)P(B \\mid C)$:\n$$P(Y=1, X=1) = P(Y=1 \\mid X=1, Z=1)P(X=1 \\mid Z=1)P(Z=1) + P(Y=1 \\mid X=1, Z=0)P(X=1 \\mid Z=0)P(Z=0)$$\nSubstituting the given values:\n$$P(Y=1, X=1) = (0.62)(0.20)(0.50) + (0.52)(b)(0.50) = 0.062 + 0.26b$$\n\nSimilarly, we calculate the joint probability $P(Y=1, X=0)$:\n$$P(Y=1, X=0) = P(Y=1 \\mid X=0, Z=1)P(X=0 \\mid Z=1)P(Z=1) + P(Y=1 \\mid X=0, Z=0)P(X=0 \\mid Z=0)P(Z=0)$$\nFirst, we find the necessary conditional probabilities for $X=0$:\n$P(X=0 \\mid Z=1) = 1 - P(X=1 \\mid Z=1) = 1 - 0.20 = 0.80$\n$P(X=0 \\mid Z=0) = 1 - P(X=1 \\mid Z=0) = 1 - b$\nSubstituting these and the other given values:\n$$P(Y=1, X=0) = (0.60)(0.80)(0.50) + (0.50)(1-b)(0.50)$$\n$$P(Y=1, X=0) = 0.24 + 0.25(1-b) = 0.24 + 0.25 - 0.25b = 0.49 - 0.25b$$\n\nNow we can formulate the aggregated conditional probabilities:\n$$P(Y=1 \\mid X=1) = \\frac{P(Y=1, X=1)}{P(X=1)} = \\frac{0.062 + 0.26b}{0.10 + 0.50b}$$\n$$P(Y=1 \\mid X=0) = \\frac{P(Y=1, X=0)}{P(X=0)} = \\frac{0.49 - 0.25b}{0.90 - 0.50b}$$\n\nWe are looking for the threshold $b^{\\star}$ where these two expressions are equal:\n$$\\frac{0.062 + 0.26b}{0.10 + 0.50b} = \\frac{0.49 - 0.25b}{0.90 - 0.50b}$$\nTo solve for $b$, we cross-multiply:\n$$(0.062 + 0.26b)(0.90 - 0.50b) = (0.49 - 0.25b)(0.10 + 0.50b)$$\nExpanding both sides of the equation:\nLHS: $(0.062)(0.90) - (0.062)(0.50b) + (0.26b)(0.90) - (0.26b)(0.50b) = 0.0558 - 0.031b + 0.234b - 0.13b^2$\n$$0.0558 + 0.203b - 0.13b^2$$\nRHS: $(0.49)(0.10) + (0.49)(0.50b) - (0.25b)(0.10) - (0.25b)(0.50b) = 0.049 + 0.245b - 0.025b - 0.125b^2$\n$$0.049 + 0.22b - 0.125b^2$$\nEquating the two expressions:\n$$0.0558 + 0.203b - 0.13b^2 = 0.049 + 0.22b - 0.125b^2$$\nRearranging the terms to form a standard quadratic equation of the form $ax^2+bx+c=0$:\n$$(-0.13 + 0.125)b^2 + (0.203 - 0.22)b + (0.0558 - 0.049) = 0$$\n$$-0.005b^2 - 0.017b + 0.0068 = 0$$\nTo simplify, we can multiply the entire equation by $-2000$:\n$$10b^2 + 34b - 13.6 = 0$$\nOr, to work with integers, we can multiply the original quadratic by $-10000$:\n$$50b^2 + 170b - 68 = 0$$\nDividing by $2$:\n$$25b^2 + 85b - 34 = 0$$\nWe solve this quadratic equation for $b$ using the quadratic formula $b = \\frac{-B \\pm \\sqrt{B^2 - 4AC}}{2A}$, with $A=25$, $B=85$, and $C=-34$.\n$$b^{\\star} = \\frac{-85 \\pm \\sqrt{85^2 - 4(25)(-34)}}{2(25)}$$\n$$b^{\\star} = \\frac{-85 \\pm \\sqrt{7225 - 100(-34)}}{50}$$\n$$b^{\\star} = \\frac{-85 \\pm \\sqrt{7225 + 3400}}{50}$$\n$$b^{\\star} = \\frac{-85 \\pm \\sqrt{10625}}{50}$$\nTo simplify the square root, we find the prime factors of $10625$: $10625 = 5^4 \\times 17 = 625 \\times 17$.\nTherefore, $\\sqrt{10625} = \\sqrt{625 \\times 17} = 25\\sqrt{17}$.\nSubstituting this back into the solution for $b^{\\star}$:\n$$b^{\\star} = \\frac{-85 \\pm 25\\sqrt{17}}{50}$$\nWe can divide the numerator and the denominator by their greatest common divisor, which is $5$:\n$$b^{\\star} = \\frac{-17 \\pm 5\\sqrt{17}}{10}$$\nThis gives two possible solutions:\n$$b_1 = \\frac{-17 - 5\\sqrt{17}}{10} \\quad \\text{and} \\quad b_2 = \\frac{-17 + 5\\sqrt{17}}{10}$$\nThe problem states that $b \\in [0,1]$.\nThe first solution, $b_1$, is clearly negative, as both terms in the numerator are negative. So, $b_1$ is not a valid solution.\nFor the second solution, $b_2$, we note that $\\sqrt{16} < \\sqrt{17} < \\sqrt{25}$, so $4 < \\sqrt{17} < 5$.\nThe numerator is approximately $-17 + 5(4.123) = -17 + 20.615 = 3.615$.\nSo, $b_2 \\approx \\frac{3.615}{10} = 0.3615$, which is within the valid range $[0,1]$.\nThus, the exact threshold value is:\n$$b^{\\star} = \\frac{-17 + 5\\sqrt{17}}{10}$$\nThis can be written as:\n$$b^{\\star} = \\frac{5\\sqrt{17} - 17}{10}$$\nFor $b > b^{\\star}$, the aggregated association reverses, yielding Simpson's paradox.", "answer": "$$\\boxed{\\frac{5\\sqrt{17} - 17}{10}}$$", "id": "3184667"}, {"introduction": "A machine learning model trained in one environment may not perform optimally in another, especially if the underlying class frequencies shift. This problem tackles a common challenge in model deployment known as prior probability shift. You will use Bayes' theorem and the law of total probability to derive an optimal decision threshold for a classifier that accounts for changes in the class prior $P(Y=1)$ and asymmetric misclassification costs [@problem_id:3184639]. This hands-on derivation provides a crucial link between probabilistic theory and the practical realities of building robust and reliable decision-making systems.", "problem": "A binary classifier is trained on data with class label $Y \\in \\{0,1\\}$ and feature vector $X \\in \\mathcal{X}$. The model outputs the training-posterior score $\\tilde{\\eta}(x) = P_{\\text{train}}(Y=1 \\mid X=x)$, where the training prior is $\\tilde{p} = P_{\\text{train}}(Y=1) \\in (0,1)$. At deployment, the prior may differ due to class imbalance shift, with $p = P_{\\text{deploy}}(Y=1) \\in (0,1)$, while the class-conditional feature distributions remain unchanged across training and deployment. Let $f_{1}(x)$ and $f_{0}(x)$ denote the class-conditional densities of $X$ given $Y=1$ and $Y=0$, respectively, which are assumed to be identical between training and deployment. The misclassification costs are: $c_{10} > 0$ for predicting $Y=0$ when the true label is $Y=1$ (false negative), and $c_{01} > 0$ for predicting $Y=1$ when the true label is $Y=0$ (false positive). The cost of correct classifications is zero.\n\nConsider the family of threshold decision rules that predict $Y=1$ if and only if $\\tilde{\\eta}(x) \\geq t$, for some threshold $t \\in (0,1)$ that is chosen to minimize expected deployment-time risk under the given costs. Using only the following foundational principles:\n- the definition of conditional risk under a given posterior $P(Y=1 \\mid X=x)$,\n- the Law of Total Probability for $P(X)$,\n- Bayes' theorem for $P(Y \\mid X)$,\nderive from first principles a closed-form expression for the Bayes-optimal threshold $t(p)$ as a function of $p$, $\\tilde{p}$, $c_{10}$, and $c_{01}$, under the prior probability shift assumption that $f_{1}(x)$ and $f_{0}(x)$ are invariant between training and deployment. Then, analytically compute the sensitivity of this optimal threshold to changes in the deployment prior by finding $\\frac{d}{dp} t(p)$.\n\nYour final answer must be a single closed-form analytical expression, or a pair of closed-form analytical expressions organized as a single row matrix. No numerical approximation is required.", "solution": "The problem is evaluated to be valid as it is scientifically grounded, well-posed, objective, and internally consistent. It presents a standard problem in statistical decision theory concerning Bayes-optimal classification under prior probability shift. We shall now proceed with a derivation from first principles.\n\nOur objective is twofold: first, to derive the Bayes-optimal threshold $t(p)$ for a decision rule based on the training posterior $\\tilde{\\eta}(x)$, and second, to compute the sensitivity of this threshold to the deployment prior, $\\frac{d}{dp} t(p)$.\n\nThe foundation of the Bayes-optimal decision is the minimization of expected risk. For any given feature vector $X=x$, we must choose an action $\\alpha(x) \\in \\{0, 1\\}$ (predict class $0$ or class $1$) that minimizes the conditional risk. The conditional risk, $R(\\alpha \\mid x)$, is the expected cost of taking action $\\alpha$ given the observation $x$. The expectations are taken over the true label $Y$ with respect to the deployment distribution.\n\nLet $\\eta(x) = P_{\\text{deploy}}(Y=1 \\mid X=x)$ be the true posterior probability of class $1$ at deployment time. Consequently, $P_{\\text{deploy}}(Y=0 \\mid X=x) = 1 - \\eta(x)$. The costs are given as $c_{10}$ for a false negative (predict $0$, true is $1$) and $c_{01}$ for a false positive (predict $1$, true is $0$). The cost of correct classification is $0$.\n\nThe conditional risk of predicting class $1$ is:\n$$R(\\alpha=1 \\mid x) = \\text{cost}(\\text{predict } 1, \\text{true } 1) \\cdot P_{\\text{deploy}}(Y=1 \\mid X=x) + \\text{cost}(\\text{predict } 1, \\text{true } 0) \\cdot P_{\\text{deploy}}(Y=0 \\mid X=x)$$\n$$R(\\alpha=1 \\mid x) = 0 \\cdot \\eta(x) + c_{01} \\cdot (1 - \\eta(x)) = c_{01}(1 - \\eta(x))$$\n\nThe conditional risk of predicting class $0$ is:\n$$R(\\alpha=0 \\mid x) = \\text{cost}(\\text{predict } 0, \\text{true } 1) \\cdot P_{\\text{deploy}}(Y=1 \\mid X=x) + \\text{cost}(\\text{predict } 0, \\text{true } 0) \\cdot P_{\\text{deploy}}(Y=0 \\mid X=x)$$\n$$R(\\alpha=0 \\mid x) = c_{10} \\cdot \\eta(x) + 0 \\cdot (1 - \\eta(x)) = c_{10}\\eta(x)$$\n\nThe Bayes-optimal decision rule is to predict class $1$ if and only if the risk of doing so is less than the risk of predicting class $0$. We handle the equality case by assigning it to one of the classes; the choice does not affect the total risk for continuous feature distributions. We predict $Y=1$ if $R(\\alpha=1 \\mid x) < R(\\alpha=0 \\mid x)$:\n$$c_{01}(1 - \\eta(x)) < c_{10}\\eta(x)$$\n$$c_{01} < c_{10}\\eta(x) + c_{01}\\eta(x) = (c_{10} + c_{01})\\eta(x)$$\n$$\\eta(x) > \\frac{c_{01}}{c_{10} + c_{01}}$$\nLet this deployment posterior threshold be $\\tau = \\frac{c_{01}}{c_{10} + c_{01}}$. The optimal rule is to predict $Y=1$ if $\\eta(x) > \\tau$.\n\nThe classifier's output is the training posterior $\\tilde{\\eta}(x) = P_{\\text{train}}(Y=1 \\mid X=x)$, not the deployment posterior $\\eta(x)$. We must relate $\\eta(x)$ to $\\tilde{\\eta}(x)$ using the information provided. The core assumption is that the class-conditional densities $f_1(x) = f(x \\mid Y=1)$ and $f_0(x) = f(x \\mid Y=0)$ are invariant between training and deployment. The priors, however, shift from $\\tilde{p}=P_{\\text{train}}(Y=1)$ to $p=P_{\\text{deploy}}(Y=1)$.\n\nUsing Bayes' theorem and the Law of Total Probability:\nThe deployment posterior is:\n$$\\eta(x) = P_{\\text{deploy}}(Y=1 \\mid X=x) = \\frac{f(x \\mid Y=1)P_{\\text{deploy}}(Y=1)}{f_{\\text{deploy}}(x)} = \\frac{f_1(x)p}{f_1(x)p + f_0(x)(1-p)}$$\nThe training posterior is:\n$$\\tilde{\\eta}(x) = P_{\\text{train}}(Y=1 \\mid X=x) = \\frac{f(x \\mid Y=1)P_{\\text{train}}(Y=1)}{f_{\\text{train}}(x)} = \\frac{f_1(x)\\tilde{p}}{f_1(x)\\tilde{p} + f_0(x)(1-\\tilde{p})}$$\n\nTo connect $\\eta(x)$ and $\\tilde{\\eta}(x)$, we first express the likelihood ratio $\\frac{f_1(x)}{f_0(x)}$ in terms of the training quantities. From the expression for $\\tilde{\\eta}(x)$:\n$$\\tilde{\\eta}(x) [f_1(x)\\tilde{p} + f_0(x)(1-\\tilde{p})] = f_1(x)\\tilde{p}$$\n$$\\tilde{\\eta}(x) f_0(x)(1-\\tilde{p}) = f_1(x)\\tilde{p} - \\tilde{\\eta}(x) f_1(x)\\tilde{p} = f_1(x)\\tilde{p}(1-\\tilde{\\eta}(x))$$\n$$\\frac{f_1(x)}{f_0(x)} = \\frac{\\tilde{\\eta}(x)(1-\\tilde{p})}{\\tilde{p}(1-\\tilde{\\eta}(x))}$$\n\nNow, we substitute this likelihood ratio into the expression for the deployment posterior $\\eta(x)$. Dividing the numerator and denominator of the expression for $\\eta(x)$ by $f_0(x)$:\n$$\\eta(x) = \\frac{\\frac{f_1(x)}{f_0(x)} p}{\\frac{f_1(x)}{f_0(x)} p + (1-p)} = \\frac{\\frac{\\tilde{\\eta}(x)(1-\\tilde{p})}{\\tilde{p}(1-\\tilde{\\eta}(x))} p}{\\frac{\\tilde{\\eta}(x)(1-\\tilde{p})}{\\tilde{p}(1-\\tilde{\\eta}(x))} p + (1-p)}$$\nMultiplying the numerator and denominator by $\\tilde{p}(1-\\tilde{\\eta}(x))$ to clear the fractions:\n$$\\eta(x) = \\frac{\\tilde{\\eta}(x)(1-\\tilde{p})p}{\\tilde{\\eta}(x)(1-\\tilde{p})p + \\tilde{p}(1-\\tilde{\\eta}(x))(1-p)}$$\n\nThe optimal decision rule is $\\eta(x) > \\tau$. Substituting the expression for $\\eta(x)$:\n$$\\frac{\\tilde{\\eta}(x)(1-\\tilde{p})p}{\\tilde{\\eta}(x)(1-\\tilde{p})p + \\tilde{p}(1-\\tilde{\\eta}(x))(1-p)} > \\tau$$\nSince the denominator is a sum of non-negative terms and is positive for $p, \\tilde{p} \\in (0,1)$, we can multiply both sides by it:\n$$\\tilde{\\eta}(x)(1-\\tilde{p})p > \\tau [\\tilde{\\eta}(x)(1-\\tilde{p})p + \\tilde{p}(1-\\tilde{\\eta}(x))(1-p)]$$\nCollecting terms with $\\tilde{\\eta}(x)$:\n$$\\tilde{\\eta}(x)(1-\\tilde{p})p - \\tau \\tilde{\\eta}(x)(1-\\tilde{p})p > \\tau \\tilde{p}(1-\\tilde{\\eta}(x))(1-p)$$\n$$\\tilde{\\eta}(x)[(1-\\tilde{p})p(1-\\tau)] > \\tau \\tilde{p}(1-p) - \\tau \\tilde{p}\\tilde{\\eta}(x)(1-p)$$\n$$\\tilde{\\eta}(x)[(1-\\tilde{p})p(1-\\tau) + \\tau \\tilde{p}(1-p)] > \\tau \\tilde{p}(1-p)$$\nThe term in the square brackets is positive since $p, \\tilde{p}, \\tau \\in (0,1)$. We can divide by it:\n$$\\tilde{\\eta}(x) > \\frac{\\tau \\tilde{p}(1-p)}{(1-\\tilde{p})p(1-\\tau) + \\tau \\tilde{p}(1-p)}$$\nThis inequality defines the decision rule in terms of $\\tilde{\\eta}(x)$. The problem states the decision rule is $\\tilde{\\eta}(x) \\geq t$. The Bayes-optimal threshold $t(p)$ must therefore be the right-hand side of the inequality.\n$$t(p) = \\frac{\\tau \\tilde{p}(1-p)}{(1-\\tilde{p})p(1-\\tau) + \\tau \\tilde{p}(1-p)}$$\nNow, substitute $\\tau = \\frac{c_{01}}{c_{10} + c_{01}}$ and $1-\\tau = \\frac{c_{10}}{c_{10} + c_{01}}$:\n$$t(p) = \\frac{\\frac{c_{01}}{c_{10}+c_{01}} \\tilde{p}(1-p)}{(1-\\tilde{p})p\\frac{c_{10}}{c_{10}+c_{01}} + \\frac{c_{01}}{c_{10}+c_{01}} \\tilde{p}(1-p)}$$\nMultiplying the numerator and denominator by $(c_{10} + c_{01})$ gives the final expression for the optimal threshold $t(p)$:\n$$t(p) = \\frac{c_{01}\\tilde{p}(1-p)}{c_{10}p(1-\\tilde{p}) + c_{01}\\tilde{p}(1-p)}$$\n\nNext, we compute the sensitivity $\\frac{d}{dp}t(p)$. Let's define constants for clarity: $A = c_{01}\\tilde{p}$ and $B = c_{10}(1-\\tilde{p})$. The expression becomes:\n$$t(p) = \\frac{A(1-p)}{Bp + A(1-p)} = \\frac{A - Ap}{Bp + A - Ap} = \\frac{A - Ap}{(B-A)p + A}$$\nWe use the quotient rule for differentiation, $(\\frac{u}{v})' = \\frac{u'v - uv'}{v^2}$, with $u(p) = A - Ap$ and $v(p) = (B-A)p + A$.\nThe derivatives are $u'(p) = -A$ and $v'(p) = B-A$.\n$$\\frac{d}{dp}t(p) = \\frac{(-A)((B-A)p + A) - (A-Ap)(B-A)}{((B-A)p + A)^2}$$\nLet's simplify the numerator:\n$$\\text{Numerator} = -A[(B-A)p + A] - (A-Ap)(B-A) = -A[(B-A)p + A + (1-p)(B-A)]$$\n$$= -A[(B-A)p + A + B - A - p(B-A)] = -A[B]$$\n$$\\text{Numerator} = -AB$$\nSubstituting the expressions for $A$ and $B$:\n$$\\text{Numerator} = - (c_{01}\\tilde{p})(c_{10}(1-\\tilde{p})) = -c_{10}c_{01}\\tilde{p}(1-\\tilde{p})$$\nThe denominator is the square of the denominator of $t(p)$:\n$$\\text{Denominator} = (c_{10}p(1-\\tilde{p}) + c_{01}\\tilde{p}(1-p))^2$$\nThus, the sensitivity is:\n$$\\frac{d}{dp}t(p) = -\\frac{c_{10}c_{01}\\tilde{p}(1-\\tilde{p})}{(c_{10}p(1-\\tilde{p}) + c_{01}\\tilde{p}(1-p))^2}$$\nThe negative sign indicates that as the deployment prior for class $1$ increases, the threshold on the training posterior must decrease to make the classifier more likely to predict class $1$, which is consistent with risk minimization.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{c_{01}\\tilde{p}(1-p)}{c_{10}p(1-\\tilde{p}) + c_{01}\\tilde{p}(1-p)} & -\\frac{c_{10}c_{01}\\tilde{p}(1-\\tilde{p})}{\\left(c_{10}p(1-\\tilde{p}) + c_{01}\\tilde{p}(1-p)\\right)^2}\n\\end{pmatrix}\n}\n$$", "id": "3184639"}]}