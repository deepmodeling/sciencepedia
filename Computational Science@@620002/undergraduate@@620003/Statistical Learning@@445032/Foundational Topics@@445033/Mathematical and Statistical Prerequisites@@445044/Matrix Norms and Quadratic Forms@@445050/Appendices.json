{"hands_on_practices": [{"introduction": "Quadratic penalties are a cornerstone of regularization in machine learning, forming the basis of methods like Ridge regression. To solve the optimization problems these regularizers create, modern algorithms often rely on a key component called the proximal operator. This exercise provides fundamental practice in deriving these operators for both standard isotropic and more general anisotropic quadratic penalties, revealing how their behavior is governed by the underlying matrix structure. [@problem_id:3146432]", "problem": "Consider a parameter vector $w \\in \\mathbb{R}^{d}$ and two regularization functions frequently used in statistical learning: the isotropic ridge penalty $\\phi(w) = \\frac{\\lambda}{2}\\|w\\|_{2}^{2}$ and the anisotropic quadratic penalty $\\psi(w) = \\frac{1}{2} w^{\\top} Q w$, where $Q \\in \\mathbb{R}^{d \\times d}$ is symmetric positive semidefinite and $\\lambda > 0$. Starting from the definition of the proximal operator for a proper closed convex function $f$,\n$$\n\\operatorname{prox}_{\\tau f}(v) = \\arg\\min_{w \\in \\mathbb{R}^{d}} \\left\\{ \\frac{1}{2}\\|w - v\\|_{2}^{2} + \\tau f(w) \\right\\},\n$$\nderive the closed-form expressions for $\\operatorname{prox}_{\\tau \\phi}(v)$ and $\\operatorname{prox}_{\\tau \\psi}(v)$ for any $\\tau > 0$ and $v \\in \\mathbb{R}^{d}$. Then, using the Euclidean norm $\\|\\cdot\\|_{2}$ and the induced operator norm on matrices, analyze the contraction factors (Lipschitz constants) of these proximal mappings by expressing them in terms of $\\tau$, $\\lambda$, and spectral properties of $Q$. In particular, express the Lipschitz constant of $\\operatorname{prox}_{\\tau \\psi}$ using the eigenvalues of $Q$, and discuss how the spectral norm $\\|Q\\|_{2}$ can be used to bound this constant.\n\nFinally, for the specific choices\n$$\nQ = \\begin{pmatrix} 3 & 1 \\\\ 1 & 3 \\end{pmatrix}, \\quad \\lambda = 3, \\quad \\tau = \\frac{1}{2},\n$$\ncompute the ratio of the exact contraction factor of $\\operatorname{prox}_{\\tau \\psi}$ to that of $\\operatorname{prox}_{\\tau \\phi}$. Express your final answer as a single exact number. No rounding is required.", "solution": "The problem is valid as it is scientifically grounded in the principles of convex optimization, well-posed with all necessary information provided, and stated objectively. We proceed with the solution.\n\nThe proximal operator of a function $\\tau f$ is defined as:\n$$\n\\operatorname{prox}_{\\tau f}(v) = \\arg\\min_{w \\in \\mathbb{R}^{d}} \\left\\{ \\frac{1}{2}\\|w - v\\|_{2}^{2} + \\tau f(w) \\right\\}\n$$\n\nFirst, we derive the closed-form expression for the proximal operator of the ridge penalty, $\\phi(w) = \\frac{\\lambda}{2}\\|w\\|_{2}^{2}$. The objective function to minimize is:\n$$\ng(w) = \\frac{1}{2}\\|w - v\\|_{2}^{2} + \\tau \\phi(w) = \\frac{1}{2}\\|w - v\\|_{2}^{2} + \\frac{\\tau \\lambda}{2}\\|w\\|_{2}^{2}\n$$\nThis function is strictly convex and differentiable for $\\lambda > 0$ and $\\tau > 0$. The minimum can be found by setting the gradient with respect to $w$ to zero.\n$$\n\\nabla_w g(w) = \\nabla_w \\left( \\frac{1}{2}(w - v)^{\\top}(w - v) + \\frac{\\tau \\lambda}{2} w^{\\top}w \\right) = (w - v) + \\tau \\lambda w\n$$\nSetting the gradient to the zero vector:\n$$\n(w - v) + \\tau \\lambda w = 0\n$$\n$$\n(1 + \\tau \\lambda)w = v\n$$\nSince $\\tau > 0$ and $\\lambda > 0$, the scalar $(1 + \\tau \\lambda)$ is greater than $1$, so we can solve for $w$:\n$$\nw = \\frac{1}{1 + \\tau \\lambda} v\n$$\nThus, the proximal operator for the ridge penalty is:\n$$\n\\operatorname{prox}_{\\tau \\phi}(v) = \\frac{1}{1 + \\tau \\lambda} v\n$$\n\nNext, we derive the closed-form expression for the proximal operator of the anisotropic quadratic penalty, $\\psi(w) = \\frac{1}{2} w^{\\top} Q w$. The objective function to minimize is:\n$$\nh(w) = \\frac{1}{2}\\|w - v\\|_{2}^{2} + \\tau \\psi(w) = \\frac{1}{2}\\|w - v\\|_{2}^{2} + \\frac{\\tau}{2} w^{\\top} Q w\n$$\nSince $Q$ is symmetric positive semidefinite, $h(w)$ is a convex function. The use of the Euclidean norm makes it strictly convex. The function is also differentiable. We find the minimum by setting the gradient to zero.\n$$\n\\nabla_w h(w) = \\nabla_w \\left( \\frac{1}{2}(w - v)^{\\top}(w - v) + \\frac{\\tau}{2} w^{\\top} Q w \\right) = (w - v) + \\tau Q w\n$$\nSetting the gradient to the zero vector:\n$$\n(w - v) + \\tau Q w = 0\n$$\n$$\n(I + \\tau Q)w = v\n$$\nThe matrix $I + \\tau Q$ is invertible. To see this, let $\\mu_i$ be an eigenvalue of $Q$. Since $Q$ is positive semidefinite, $\\mu_i \\ge 0$. The eigenvalues of $I + \\tau Q$ are $1 + \\tau \\mu_i$. Since $\\tau > 0$, we have $1 + \\tau \\mu_i \\ge 1$, so all eigenvalues are positive. A symmetric matrix with all positive eigenvalues is positive definite and therefore invertible. Solving for $w$:\n$$\nw = (I + \\tau Q)^{-1} v\n$$\nThus, the proximal operator for the anisotropic quadratic penalty is:\n$$\n\\operatorname{prox}_{\\tau \\psi}(v) = (I + \\tau Q)^{-1} v\n$$\n\nNow, we analyze the contraction factors (Lipschitz constants) of these proximal mappings. For a linear mapping $P(v) = Av$, the Lipschitz constant with respect to the Euclidean norm $\\|\\cdot\\|_{2}$ is the induced operator norm $\\|A\\|_{2}$. For a symmetric matrix $A$, $\\|A\\|_{2}$ is its spectral radius, i.e., the maximum absolute value of its eigenvalues.\n\nFor $\\operatorname{prox}_{\\tau \\phi}(v) = \\frac{1}{1 + \\tau \\lambda} v$, the mapping is linear with matrix $A_{\\phi} = \\frac{1}{1 + \\tau \\lambda} I$. The Lipschitz constant $L_{\\phi}$ is:\n$$\nL_{\\phi} = \\left\\| \\frac{1}{1 + \\tau \\lambda} I \\right\\|_{2} = \\left| \\frac{1}{1 + \\tau \\lambda} \\right| = \\frac{1}{1 + \\tau \\lambda}\n$$\nThe last equality holds because $\\tau > 0$ and $\\lambda > 0$.\n\nFor $\\operatorname{prox}_{\\tau \\psi}(v) = (I + \\tau Q)^{-1} v$, the mapping is linear with matrix $A_{\\psi} = (I + \\tau Q)^{-1}$. Since $Q$ is symmetric, $A_{\\psi}$ is also symmetric. Its Lipschitz constant $L_{\\psi}$ is its spectral radius. Let the eigenvalues of $Q$ be $\\mu_1, \\mu_2, \\ldots, \\mu_d$, with $\\mu_i \\ge 0$. The eigenvalues of $A_{\\psi} = (I + \\tau Q)^{-1}$ are $\\frac{1}{1 + \\tau \\mu_i}$. The Lipschitz constant is the maximum of the absolute values of these eigenvalues:\n$$\nL_{\\psi} = \\max_{i} \\left| \\frac{1}{1 + \\tau \\mu_i} \\right| = \\max_{i} \\frac{1}{1 + \\tau \\mu_i}\n$$\nThe function $f(x) = \\frac{1}{1+ax}$ for $a > 0$ is a decreasing function for $x \\ge 0$. Therefore, the maximum value is attained when $\\mu_i$ is at its minimum. Let $\\mu_{\\min}(Q)$ denote the minimum eigenvalue of $Q$.\n$$\nL_{\\psi} = \\frac{1}{1 + \\tau \\mu_{\\min}(Q)}\n$$\nThe spectral norm of $Q$ is $\\|Q\\|_{2} = \\mu_{\\max}(Q)$, the largest eigenvalue of $Q$. Since $0 \\le \\mu_{\\min}(Q) \\le \\mu_{\\max}(Q) = \\|Q\\|_{2}$, we can bound $L_{\\psi}$. The constant is bounded below by $\\frac{1}{1 + \\tau \\|Q\\|_{2}}$ and above by $1$. Specifically, $\\frac{1}{1 + \\tau \\|Q\\|_{2}} \\le L_{\\psi} \\le 1$. If $\\mu_{\\min}(Q) = 0$, the upper bound of $1$ is tight. Thus, knowledge of $\\|Q\\|_{2}$ alone provides a lower bound on the contraction factor.\n\nFinally, we compute the ratio $\\frac{L_{\\psi}}{L_{\\phi}}$ for the specific values:\n$$\nQ = \\begin{pmatrix} 3 & 1 \\\\ 1 & 3 \\end{pmatrix}, \\quad \\lambda = 3, \\quad \\tau = \\frac{1}{2}\n$$\nFirst, we find the contraction factor for the ridge penalty:\n$$\nL_{\\phi} = \\frac{1}{1 + \\tau \\lambda} = \\frac{1}{1 + (\\frac{1}{2})(3)} = \\frac{1}{1 + \\frac{3}{2}} = \\frac{1}{\\frac{5}{2}} = \\frac{2}{5}\n$$\nNext, we find the contraction factor for the anisotropic penalty. We need the minimum eigenvalue of $Q$. The characteristic equation is $\\det(Q - \\mu I) = 0$:\n$$\n\\det \\begin{pmatrix} 3 - \\mu & 1 \\\\ 1 & 3 - \\mu \\end{pmatrix} = (3 - \\mu)^{2} - 1 = 0\n$$\n$$\n(3 - \\mu)^{2} = 1 \\implies 3 - \\mu = \\pm 1\n$$\nThe eigenvalues are $\\mu = 3 - 1 = 2$ and $\\mu = 3 + 1 = 4$. The minimum eigenvalue is $\\mu_{\\min}(Q) = 2$.\nThe contraction factor $L_{\\psi}$ is:\n$$\nL_{\\psi} = \\frac{1}{1 + \\tau \\mu_{\\min}(Q)} = \\frac{1}{1 + (\\frac{1}{2})(2)} = \\frac{1}{1 + 1} = \\frac{1}{2}\n$$\nThe ratio of the contraction factors is:\n$$\n\\frac{L_{\\psi}}{L_{\\phi}} = \\frac{\\frac{1}{2}}{\\frac{2}{5}} = \\frac{1}{2} \\cdot \\frac{5}{2} = \\frac{5}{4}\n$$", "answer": "$$\\boxed{\\frac{5}{4}}$$", "id": "3146432"}, {"introduction": "Data preprocessing techniques like whitening are crucial for the performance of many learning algorithms, but they can become numerically unstable when the feature covariance matrix is rank-deficient. A common and practical solution is to add a small multiple of the identity matrix, a technique known as Tikhonov regularization. This exercise challenges you to use the spectral theorem to analytically quantify the \"distortion\" this practical fix introduces compared to an idealized whitening procedure, offering deep insight into the geometry of regularization. [@problem_id:3146475]", "problem": "In a feature preprocessing pipeline for statistical learning, an estimated covariance matrix $\\hat{\\Sigma} \\in \\mathbb{R}^{4 \\times 4}$ is used for whitening. However, due to limited data, $\\hat{\\Sigma}$ is rank-deficient. To stabilize whitening, practitioners add a small ridge regularization and use $\\hat{\\Sigma}_{\\epsilon} = \\hat{\\Sigma} + \\epsilon I$ with $\\epsilon > 0$. The resulting approximate whitening transform is $\\hat{\\Sigma}_{\\epsilon}^{-1/2}$, and the corresponding whitened squared norm for a feature vector $x \\in \\mathbb{R}^{4}$ is $x^{\\top} \\hat{\\Sigma}_{\\epsilon}^{-1} x$ by the identity $\\| \\hat{\\Sigma}_{\\epsilon}^{-1/2} x \\|_{2}^{2} = x^{\\top} \\hat{\\Sigma}_{\\epsilon}^{-1} x$.\n\nAssume $\\hat{\\Sigma}$ is symmetric positive semidefinite with orthonormal eigenvectors $\\{u_{1}, u_{2}, u_{3}, u_{4}\\}$ and eigenvalues $\\lambda_{1} = 5$, $\\lambda_{2} = 2$, $\\lambda_{3} = 0$, $\\lambda_{4} = 0$. Consider a feature vector $x$ with spectral decomposition $x = 3 u_{1} + 2 u_{2} + 1 u_{3} + 2 u_{4}$.\n\nAn idealized notion of whitening that ignores directions in the nullspace of $\\hat{\\Sigma}$ uses the Moore–Penrose pseudoinverse (MPP) $\\hat{\\Sigma}^{\\dagger}$, yielding the ideal whitened squared norm $x^{\\top} \\hat{\\Sigma}^{\\dagger} x$.\n\nStarting only from the spectral theorem for real symmetric matrices and the definitions above, derive an exact analytic expression for the distortion\n$$\nD(\\epsilon) \\equiv x^{\\top} \\hat{\\Sigma}_{\\epsilon}^{-1} x \\;-\\; x^{\\top} \\hat{\\Sigma}^{\\dagger} x\n$$\nas a function of $\\epsilon$. Express your final answer as a single closed-form analytic expression in terms of $\\epsilon$. No numerical approximation is required.", "solution": "The problem is validated as being scientifically grounded, well-posed, and objective. It is a standard problem in linear algebra applied to statistical learning concepts. All necessary information is provided, and the problem is free of contradictions or ambiguities. I will now proceed with the solution.\n\nThe solution rests on the spectral theorem for real symmetric matrices. The matrix $\\hat{\\Sigma} \\in \\mathbb{R}^{4 \\times 4}$ is symmetric and thus has a spectral decomposition. We are given its orthonormal eigenvectors $\\{u_{1}, u_{2}, u_{3}, u_{4}\\}$ and corresponding eigenvalues $\\lambda_{1} = 5$, $\\lambda_{2} = 2$, $\\lambda_{3} = 0$, and $\\lambda_{4} = 0$. The spectral decomposition of $\\hat{\\Sigma}$ can be written as:\n$$\n\\hat{\\Sigma} = \\sum_{i=1}^{4} \\lambda_i u_i u_i^{\\top}\n$$\n\nFirst, we analyze the regularized matrix $\\hat{\\Sigma}_{\\epsilon} = \\hat{\\Sigma} + \\epsilon I$, where $I$ is the $4 \\times 4$ identity matrix and $\\epsilon > 0$. The eigenvectors $u_i$ of $\\hat{\\Sigma}$ are also the eigenvectors of $\\hat{\\Sigma}_{\\epsilon}$:\n$$\n\\hat{\\Sigma}_{\\epsilon} u_i = (\\hat{\\Sigma} + \\epsilon I) u_i = \\hat{\\Sigma} u_i + \\epsilon I u_i = \\lambda_i u_i + \\epsilon u_i = (\\lambda_i + \\epsilon) u_i\n$$\nThus, the eigenvalues of $\\hat{\\Sigma}_{\\epsilon}$ are $\\lambda_i' = \\lambda_i + \\epsilon$. For the given eigenvalues of $\\hat{\\Sigma}$, the eigenvalues of $\\hat{\\Sigma}_{\\epsilon}$ are $5+\\epsilon$, $2+\\epsilon$, $\\epsilon$, and $\\epsilon$. Since $\\epsilon > 0$, all eigenvalues of $\\hat{\\Sigma}_{\\epsilon}$ are strictly positive, which means $\\hat{\\Sigma}_{\\epsilon}$ is invertible.\n\nThe inverse matrix $\\hat{\\Sigma}_{\\epsilon}^{-1}$ has the same eigenvectors $u_i$ with eigenvalues that are the reciprocal of the eigenvalues of $\\hat{\\Sigma}_{\\epsilon}$. The spectral decomposition of $\\hat{\\Sigma}_{\\epsilon}^{-1}$ is:\n$$\n\\hat{\\Sigma}_{\\epsilon}^{-1} = \\sum_{i=1}^{4} \\frac{1}{\\lambda_i + \\epsilon} u_i u_i^{\\top}\n$$\n\nNext, we compute the quadratic form $x^{\\top} \\hat{\\Sigma}_{\\epsilon}^{-1} x$. The vector $x$ is given in its spectral decomposition with respect to the basis $\\{u_i\\}$:\n$$\nx = 3 u_{1} + 2 u_{2} + 1 u_{3} + 2 u_{4} = \\sum_{i=1}^{4} c_i u_i\n$$\nwhere the coefficients are $c_1=3$, $c_2=2$, $c_3=1$, and $c_4=2$. Substituting the expansions for $x$ and $\\hat{\\Sigma}_{\\epsilon}^{-1}$ into the quadratic form:\n$$\nx^{\\top} \\hat{\\Sigma}_{\\epsilon}^{-1} x = \\left( \\sum_{j=1}^{4} c_j u_j^{\\top} \\right) \\left( \\sum_{i=1}^{4} \\frac{1}{\\lambda_i + \\epsilon} u_i u_i^{\\top} \\right) \\left( \\sum_{k=1}^{4} c_k u_k \\right)\n$$\nUsing the orthonormality property of the eigenvectors, $u_j^{\\top} u_i = \\delta_{ji}$ (the Kronecker delta), the expression simplifies significantly:\n$$\nx^{\\top} \\hat{\\Sigma}_{\\epsilon}^{-1} x = \\sum_{i=1}^{4} \\frac{c_i^2}{\\lambda_i + \\epsilon}\n$$\nSubstituting the given values for $\\lambda_i$ and $c_i$:\n$$\nx^{\\top} \\hat{\\Sigma}_{\\epsilon}^{-1} x = \\frac{3^2}{5 + \\epsilon} + \\frac{2^2}{2 + \\epsilon} + \\frac{1^2}{0 + \\epsilon} + \\frac{2^2}{0 + \\epsilon} = \\frac{9}{5 + \\epsilon} + \\frac{4}{2 + \\epsilon} + \\frac{1}{\\epsilon} + \\frac{4}{\\epsilon} = \\frac{9}{5 + \\epsilon} + \\frac{4}{2 + \\epsilon} + \\frac{5}{\\epsilon}\n$$\n\nNow, we consider the Moore-Penrose pseudoinverse (MPP), $\\hat{\\Sigma}^{\\dagger}$. For a symmetric matrix, its MPP is found by taking the reciprocal of the non-zero eigenvalues and keeping the zero eigenvalues as zero. So, $\\hat{\\Sigma}^{\\dagger}$ has the same eigenvectors $u_i$ with eigenvalues $\\lambda_i^{\\dagger}$ defined as $\\lambda_i^{\\dagger} = 1/\\lambda_i$ if $\\lambda_i \\neq 0$, and $\\lambda_i^{\\dagger}=0$ if $\\lambda_i=0$.\nThe eigenvalues of $\\hat{\\Sigma}^{\\dagger}$ are:\n$\\lambda_1^{\\dagger} = 1/5$, $\\lambda_2^{\\dagger} = 1/2$, $\\lambda_3^{\\dagger} = 0$, $\\lambda_4^{\\dagger} = 0$.\nThe quadratic form $x^{\\top} \\hat{\\Sigma}^{\\dagger} x$ is computed similarly:\n$$\nx^{\\top} \\hat{\\Sigma}^{\\dagger} x = \\sum_{i=1}^{4} c_i^2 \\lambda_i^{\\dagger} = c_1^2 \\lambda_1^{\\dagger} + c_2^2 \\lambda_2^{\\dagger} + c_3^2 \\lambda_3^{\\dagger} + c_4^2 \\lambda_4^{\\dagger}\n$$\nSubstituting the values:\n$$\nx^{\\top} \\hat{\\Sigma}^{\\dagger} x = (3^2) \\left(\\frac{1}{5}\\right) + (2^2) \\left(\\frac{1}{2}\\right) + (1^2)(0) + (2^2)(0) = \\frac{9}{5} + \\frac{4}{2} = \\frac{9}{5} + 2\n$$\n\nFinally, we compute the distortion $D(\\epsilon) = x^{\\top} \\hat{\\Sigma}_{\\epsilon}^{-1} x - x^{\\top} \\hat{\\Sigma}^{\\dagger} x$:\n$$\nD(\\epsilon) = \\left( \\frac{9}{5 + \\epsilon} + \\frac{4}{2 + \\epsilon} + \\frac{5}{\\epsilon} \\right) - \\left( \\frac{9}{5} + 2 \\right)\n$$\nTo obtain a single closed-form expression, we combine these terms into a single rational function. We can group the terms as follows:\n$$\nD(\\epsilon) = \\left( \\frac{9}{5 + \\epsilon} - \\frac{9}{5} \\right) + \\left( \\frac{4}{2 + \\epsilon} - 2 \\right) + \\frac{5}{\\epsilon}\n$$\n$$\nD(\\epsilon) = \\frac{9 \\cdot 5 - 9(5 + \\epsilon)}{5(5 + \\epsilon)} + \\frac{4 - 2(2 + \\epsilon)}{2 + \\epsilon} + \\frac{5}{\\epsilon}\n$$\n$$\nD(\\epsilon) = \\frac{45 - 45 - 9\\epsilon}{5(5 + \\epsilon)} + \\frac{4 - 4 - 2\\epsilon}{2 + \\epsilon} + \\frac{5}{\\epsilon}\n$$\n$$\nD(\\epsilon) = \\frac{-9\\epsilon}{5(\\epsilon + 5)} - \\frac{2\\epsilon}{\\epsilon + 2} + \\frac{5}{\\epsilon}\n$$\nNow, we find a common denominator, which is $5\\epsilon(\\epsilon+2)(\\epsilon+5)$:\n$$\nD(\\epsilon) = \\frac{-9\\epsilon \\cdot \\epsilon(\\epsilon+2) - 2\\epsilon \\cdot 5\\epsilon(\\epsilon+5) + 5 \\cdot 5(\\epsilon+2)(\\epsilon+5)}{5\\epsilon(\\epsilon+2)(\\epsilon+5)}\n$$\n$$\nD(\\epsilon) = \\frac{-9\\epsilon^2(\\epsilon+2) - 10\\epsilon^2(\\epsilon+5) + 25(\\epsilon^2+7\\epsilon+10)}{5\\epsilon(\\epsilon+2)(\\epsilon+5)}\n$$\n$$\nD(\\epsilon) = \\frac{(-9\\epsilon^3 - 18\\epsilon^2) - (10\\epsilon^3 + 50\\epsilon^2) + (25\\epsilon^2 + 175\\epsilon + 250)}{5\\epsilon(\\epsilon+2)(\\epsilon+5)}\n$$\nCombining terms in the numerator by powers of $\\epsilon$:\n$$\nD(\\epsilon) = \\frac{\\epsilon^3(-9-10) + \\epsilon^2(-18-50+25) + \\epsilon(175) + 250}{5\\epsilon(\\epsilon+2)(\\epsilon+5)}\n$$\n$$\nD(\\epsilon) = \\frac{-19\\epsilon^3 - 43\\epsilon^2 + 175\\epsilon + 250}{5\\epsilon(\\epsilon+2)(\\epsilon+5)}\n$$\nExpanding the denominator gives the final expression:\n$$\nD(\\epsilon) = \\frac{-19\\epsilon^3 - 43\\epsilon^2 + 175\\epsilon + 250}{5\\epsilon^3 + 35\\epsilon^2 + 50\\epsilon}\n$$\nThis is the required single closed-form analytic expression for the distortion $D(\\epsilon)$.", "answer": "$$\n\\boxed{\\frac{-19\\epsilon^{3} - 43\\epsilon^{2} + 175\\epsilon + 250}{5\\epsilon^{3} + 35\\epsilon^{2} + 50\\epsilon}}\n$$", "id": "3146475"}, {"introduction": "Batch Normalization (BN) is a powerful and widely used technique for accelerating and stabilizing the training of deep neural networks, whose effectiveness stems from re-normalizing activations within each mini-batch. This process profoundly impacts the feature covariance landscape that subsequent layers observe. This computational exercise invites you to implement BN and use the spectral norm and quadratic forms as diagnostic tools to analyze its effect on covariance structure and training stability across different scenarios. [@problem_id:3146421]", "problem": "Consider a mini-batch feature matrix $X \\in \\mathbb{R}^{n \\times d}$ with rows $x_i^\\top \\in \\mathbb{R}^d$. In statistical learning, Batch Normalization (BN) transforms each feature using batch statistics. Let the column-wise sample mean be $\\mu \\in \\mathbb{R}^d$ and the column-wise sample variance be $v \\in \\mathbb{R}^d$, where for each feature index $j \\in \\{1,\\dots,d\\}$,\n$$\n\\mu_j = \\frac{1}{n}\\sum_{i=1}^n X_{ij}, \\quad v_j = \\frac{1}{n}\\sum_{i=1}^n \\left( X_{ij} - \\mu_j \\right)^2.\n$$\nGiven parameters $\\gamma \\in \\mathbb{R}^d$, $\\beta \\in \\mathbb{R}^d$, and a small positive constant $\\varepsilon \\in \\mathbb{R}$, BN produces an output $Y \\in \\mathbb{R}^{n \\times d}$ whose entries are\n$$\nY_{ij} = \\gamma_j \\cdot \\frac{X_{ij} - \\mu_j}{\\sqrt{v_j + \\varepsilon}} + \\beta_j.\n$$\nLet the sample covariance estimator be\n$$\n\\hat{\\Sigma}(Z) = \\frac{1}{n}\\left(Z - \\mathbf{1}\\bar{z}^\\top\\right)^\\top \\left(Z - \\mathbf{1}\\bar{z}^\\top\\right),\n$$\nwhere $\\bar{z} \\in \\mathbb{R}^d$ is the column-wise mean of $Z$, and $\\mathbf{1} \\in \\mathbb{R}^n$ is the vector of ones. For a symmetric matrix $A \\in \\mathbb{R}^{d \\times d}$, define the spectral norm\n$$\n\\|A\\|_2 = \\max_{\\|u\\|_2 = 1} \\|Au\\|_2,\n$$\nand the quadratic form\n$$\nq_A(u) = u^\\top A u.\n$$\nYour task is to, for each provided training snapshot, compute $Y$ via BN, then compute $\\hat{\\Sigma}_{\\text{BN}} = \\hat{\\Sigma}(Y)$ and analyze the stability of training via changes in $\\|\\hat{\\Sigma}_{\\text{BN}}\\|_2$ across snapshots. Additionally, for a fixed direction $u \\in \\mathbb{R}^d$, compare the effect of BN on the quadratic form by taking the ratio\n$$\nr = \\frac{u^\\top \\hat{\\Sigma}_{\\text{BN}} u}{u^\\top \\hat{\\Sigma}(X) u}.\n$$\n\nStarting from the definitions above and without assuming any shortcut identities, implement a program that, for each snapshot, computes:\n- The value $\\|\\hat{\\Sigma}_{\\text{BN}}\\|_2$,\n- A boolean stability indicator relative to the previous snapshot defined as\n$$\n\\text{stable} = \n\\begin{cases}\n\\text{True}, & \\text{for the first snapshot},\\\\\n\\left( \\left| \\|\\hat{\\Sigma}_{\\text{BN}}^{(t)}\\|_2 - \\|\\hat{\\Sigma}_{\\text{BN}}^{(t-1)}\\|_2 \\right| \\le \\tau \\right), & \\text{for } t \\ge 2,\n\\end{cases}\n$$\nwhere $\\tau \\in \\mathbb{R}_{>0}$ is a given threshold,\n- The ratio $r$ defined above.\n\nUse the following test suite of three snapshots, each with $n = 6$ samples and $d = 3$ features, along with the same direction $u$ and stability threshold $\\tau$:\n- Snapshot $1$:\n  - $X^{(1)} = \\begin{bmatrix}\n  1.2 & -0.3 & 2.0 \\\\\n  0.9 & -0.1 & 2.2 \\\\\n  1.1 & -0.4 & 1.9 \\\\\n  1.0 & 0.0 & 2.1 \\\\\n  1.3 & -0.2 & 2.3 \\\\\n  0.8 & -0.1 & 2.0\n  \\end{bmatrix}$,\n  - $\\gamma^{(1)} = \\begin{bmatrix} 0.9 & 1.1 & 1.0 \\end{bmatrix}$,\n  - $\\beta^{(1)} = \\begin{bmatrix} 0.1 & -0.05 & 0.0 \\end{bmatrix}$,\n  - $\\varepsilon^{(1)} = 10^{-5}$.\n- Snapshot $2$ (near-constant second feature):\n  - $X^{(2)} = \\begin{bmatrix}\n  2.0 & 5.0 & -1.0 \\\\\n  2.1 & 5.0 & -0.9 \\\\\n  1.9 & 5.0 & -1.1 \\\\\n  2.2 & 5.0 & -1.0 \\\\\n  2.0 & 5.0 & -1.2 \\\\\n  2.1 & 5.0 & -1.0\n  \\end{bmatrix}$,\n  - $\\gamma^{(2)} = \\begin{bmatrix} 1.0 & 0.5 & 1.2 \\end{bmatrix}$,\n  - $\\beta^{(2)} = \\begin{bmatrix} 0.0 & 0.0 & 0.1 \\end{bmatrix}$,\n  - $\\varepsilon^{(2)} = 10^{-4}$.\n- Snapshot $3$ (extreme scaling on the third feature):\n  - $X^{(3)} = \\begin{bmatrix}\n  -0.5 & 1.0 & 0.2 \\\\\n  -0.6 & 0.9 & 0.3 \\\\\n  -0.4 & 1.1 & 0.4 \\\\\n  -0.5 & 1.05 & 0.25 \\\\\n  -0.55 & 0.95 & 0.35 \\\\\n  -0.45 & 1.02 & 0.28\n  \\end{bmatrix}$,\n  - $\\gamma^{(3)} = \\begin{bmatrix} 0.7 & 1.0 & 2.5 \\end{bmatrix}$,\n  - $\\beta^{(3)} = \\begin{bmatrix} -0.1 & 0.0 & 0.0 \\end{bmatrix}$,\n  - $\\varepsilon^{(3)} = 10^{-5}$.\n\nUse the fixed direction and threshold:\n- $u = \\begin{bmatrix} 1.0 & -0.5 & 0.2 \\end{bmatrix}$,\n- $\\tau = 0.15$.\n\nYour program must:\n1. Implement the BN transformation using the definitions above.\n2. Compute $\\hat{\\Sigma}(X)$ and $\\hat{\\Sigma}_{\\text{BN}} = \\hat{\\Sigma}(Y)$ for each snapshot.\n3. Compute $\\|\\hat{\\Sigma}_{\\text{BN}}\\|_2$ using only the definition of the spectral norm as the maximum singular value and facts about symmetric matrices.\n4. Compute the stability boolean and the ratio $r$ for each snapshot.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each snapshot’s result is a list $[\\|\\hat{\\Sigma}_{\\text{BN}}\\|_2, \\text{stable}, r]$. For example, the printed output should look like $[[\\text{float},\\text{boolean},\\text{float}],\\dots]$. No units are required as all quantities are dimensionless.", "solution": "The user has requested an analysis of the Batch Normalization (BN) transformation's effect on the sample covariance matrix of feature data across several snapshots. I will first validate the problem statement and then provide a principled, step-by-step solution.\n\n### Problem Validation\n\n**Step 1: Extraction of Givens**\nThe problem provides the following definitions and data:\n-   A feature matrix $X \\in \\mathbb{R}^{n \\times d}$.\n-   Column-wise sample mean: $\\mu_j = \\frac{1}{n}\\sum_{i=1}^n X_{ij}$.\n-   Column-wise sample variance: $v_j = \\frac{1}{n}\\sum_{i=1}^n \\left( X_{ij} - \\mu_j \\right)^2$.\n-   BN parameters: $\\gamma \\in \\mathbb{R}^d$, $\\beta \\in \\mathbb{R}^d$, $\\varepsilon \\in \\mathbb{R}_{>0}$.\n-   BN output: $Y_{ij} = \\gamma_j \\cdot \\frac{X_{ij} - \\mu_j}{\\sqrt{v_j + \\varepsilon}} + \\beta_j$.\n-   Sample covariance estimator: $\\hat{\\Sigma}(Z) = \\frac{1}{n}\\left(Z - \\mathbf{1}\\bar{z}^\\top\\right)^\\top \\left(Z - \\mathbf{1}\\bar{z}^\\top\\right)$, where $\\bar{z}$ is the column-wise mean of $Z$.\n-   Spectral norm for a symmetric matrix $A$: $\\|A\\|_2 = \\max_{\\|u\\|_2 = 1} \\|Au\\|_2$.\n-   Quadratic form: $q_A(u) = u^\\top A u$.\n-   Tasks for each snapshot:\n    1.  Compute $\\|\\hat{\\Sigma}_{\\text{BN}}\\|_2$, where $\\hat{\\Sigma}_{\\text{BN}} = \\hat{\\Sigma}(Y)$.\n    2.  Compute a stability indicator: $\\text{stable} = (\\text{True for } t=1)$ or $(\\left| \\|\\hat{\\Sigma}_{\\text{BN}}^{(t)}\\|_2 - \\|\\hat{\\Sigma}_{\\text{BN}}^{(t-1)}\\|_2 \\right| \\le \\tau \\text{ for } t \\ge 2)$.\n    3.  Compute the ratio $r = \\frac{u^\\top \\hat{\\Sigma}_{\\text{BN}} u}{u^\\top \\hat{\\Sigma}(X) u}$.\n-   Numerical data for three snapshots ($X, \\gamma, \\beta, \\varepsilon$) are provided with $n=6, d=3$.\n-   A fixed direction vector $u = \\begin{bmatrix} 1.0 & -0.5 & 0.2 \\end{bmatrix}$ and threshold $\\tau = 0.15$ are given.\n\n**Step 2: Validation of Givens**\nThe problem is evaluated against the established criteria:\n1.  **Scientific Soundness**: The definitions for mean, variance, Batch Normalization, and sample covariance are standard in the field of statistical learning. The definition of sample variance and covariance consistently uses a factor of $1/n$. The definition of the spectral norm is correct. The problem is scientifically and mathematically sound.\n2.  **Well-Posedness**: The problem is well-posed. All necessary data and definitions are provided to compute a unique numerical solution. The tasks are specified unambiguously.\n3.  **Objectivity**: The problem is stated in precise, objective mathematical language, free from subjective claims.\n\n**Step 3: Verdict**\nThe problem is **valid**. It is self-contained, mathematically rigorous, and grounded in established principles of linear algebra and statistics as applied to machine learning. A complete solution will now be developed.\n\n### Solution Derivation\n\nThe analysis for each snapshot involves a sequence of calculations. We will outline the methodology for a generic snapshot, which will then be applied to the provided data.\n\n**1. Batch Normalization Transformation**\nGiven a mini-batch feature matrix $X \\in \\mathbb{R}^{n \\times d}$, parameters $\\gamma, \\beta \\in \\mathbb{R}^d$, and $\\varepsilon \\in \\mathbb{R}_{>0}$:\n\na. **Compute Batch Statistics**: First, we compute the column-wise sample mean $\\mu \\in \\mathbb{R}^d$ and variance $v \\in \\mathbb{R}^d$ of $X$.\n$$ \\mu = \\frac{1}{n} X^\\top \\mathbf{1}_n \\quad (\\text{or } \\mu_j = \\frac{1}{n} \\sum_{i=1}^n X_{ij}) $$\n$$ v_j = \\frac{1}{n} \\sum_{i=1}^n (X_{ij} - \\mu_j)^2 $$\n\nb. **Normalize and Transform**: We use these statistics to compute the output matrix $Y \\in \\mathbb{R}^{n \\times d}$. The transformation can be expressed using matrix operations for efficient computation. Let $\\mathbf{1}_n$ be the $n$-dimensional vector of ones.\n-   The centered matrix is $X_c = X - \\mathbf{1}_n \\mu^\\top$.\n-   The normalization step for each element is $\\hat{X}_{ij} = \\frac{X_{ij} - \\mu_j}{\\sqrt{v_j + \\varepsilon}}$.\n-   The final affine transformation is $Y_{ij} = \\gamma_j \\hat{X}_{ij} + \\beta_j$.\nIn matrix form:\n$$ Y = (X - \\mathbf{1}_n \\mu^\\top) \\text{diag}(\\frac{1}{\\sqrt{v_1+\\varepsilon}}, \\dots, \\frac{1}{\\sqrt{v_d+\\varepsilon}}) \\text{diag}(\\gamma_1, \\dots, \\gamma_d) + \\mathbf{1}_n \\beta^\\top $$\nwhere $\\text{diag}(\\cdot)$ creates a diagonal matrix. In practice, this is implemented using broadcasting.\n\n**2. Covariance Matrix Computation**\nThe problem defines the sample covariance estimator for a matrix $Z \\in \\mathbb{R}^{n \\times d}$ as $\\hat{\\Sigma}(Z) = \\frac{1}{n} Z_c^\\top Z_c$, where $Z_c$ is the centered matrix $Z_c = Z - \\mathbf{1}_n \\bar{z}^\\top$ and $\\bar{z}$ is the column-wise mean of $Z$.\n\na. **Covariance of Input**: We compute $\\hat{\\Sigma}(X)$. The mean $\\mu$ is already known. The centered matrix is $X_c = X - \\mathbf{1}_n \\mu^\\top$.\n$$ \\hat{\\Sigma}(X) = \\frac{1}{n} X_c^\\top X_c $$\n\nb. **Covariance of BN Output**: We compute $\\hat{\\Sigma}_{\\text{BN}} = \\hat{\\Sigma}(Y)$. This requires first calculating the mean of $Y$, let's call it $\\bar{y}$. Note that we must compute this directly from $Y$ and not rely on any theoretical properties of BN.\n$$ \\bar{y} = \\frac{1}{n} Y^\\top \\mathbf{1}_n $$\nThen, we form the centered output matrix $Y_c = Y - \\mathbf{1}_n \\bar{y}^\\top$.\n$$ \\hat{\\Sigma}_{\\text{BN}} = \\frac{1}{n} Y_c^\\top Y_c $$\n\n**3. Spectral Norm Computation**\nThe spectral norm $\\|A\\|_2$ of a symmetric matrix $A$ is its largest singular value, which is also equal to its largest absolute eigenvalue. Since $\\hat{\\Sigma}_{\\text{BN}}$ is a sample covariance matrix, it is symmetric and positive semi-definite, meaning its eigenvalues are non-negative.\n$$ \\|\\hat{\\Sigma}_{\\text{BN}}\\|_2 = \\lambda_{\\max}(\\hat{\\Sigma}_{\\text{BN}}) = \\sigma_{\\max}(\\hat{\\Sigma}_{\\text{BN}}) $$\nWe will compute the singular values of $\\hat{\\Sigma}_{\\text{BN}}$ and take the maximum.\n\n**4. Stability Indicator**\nFor each snapshot $t \\in \\{1, 2, 3\\}$, we compute the stability indicator. Let $s^{(t)} = \\|\\hat{\\Sigma}_{\\text{BN}}^{(t)}\\|_2$.\n-   For $t=1$: $\\text{stable}^{(1)} = \\text{True}$.\n-   For $t \\ge 2$: $\\text{stable}^{(t)} = \\left| s^{(t)} - s^{(t-1)} \\right| \\le \\tau$.\nWe use the given threshold $\\tau = 0.15$.\n\n**5. Quadratic Form Ratio**\nUsing the given vector $u \\in \\mathbb{R}^d$, we compute the ratio $r$.\na. **Quadratic Form for X**: $q_X = u^\\top \\hat{\\Sigma}(X) u$.\nb. **Quadratic Form for Y**: $q_Y = u^\\top \\hat{\\Sigma}_{\\text{BN}} u$.\nc. **Ratio**: $r = \\frac{q_Y}{q_X}$.\nThis ratio quantifies how the variance in the direction of $u$ is altered by the BN transformation. The denominator is non-zero for the given data, so the ratio is well-defined.\n\nThis sequence of operations is repeated for each of the three snapshots, with the spectral norm from the previous snapshot being retained to compute the stability indicator for the current one.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by processing three snapshots of training data.\n    For each snapshot, it applies Batch Normalization, computes covariance matrices,\n    and calculates stability metrics as per the problem description.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    # Each case is a dictionary containing X, gamma, beta, and epsilon.\n    test_cases = [\n        {\n            \"X\": np.array([\n                [1.2, -0.3, 2.0],\n                [0.9, -0.1, 2.2],\n                [1.1, -0.4, 1.9],\n                [1.0,  0.0, 2.1],\n                [1.3, -0.2, 2.3],\n                [0.8, -0.1, 2.0]\n            ]),\n            \"gamma\": np.array([0.9, 1.1, 1.0]),\n            \"beta\": np.array([0.1, -0.05, 0.0]),\n            \"epsilon\": 1e-5\n        },\n        {\n            \"X\": np.array([\n                [2.0, 5.0, -1.0],\n                [2.1, 5.0, -0.9],\n                [1.9, 5.0, -1.1],\n                [2.2, 5.0, -1.0],\n                [2.0, 5.0, -1.2],\n                [2.1, 5.0, -1.0]\n            ]),\n            \"gamma\": np.array([1.0, 0.5, 1.2]),\n            \"beta\": np.array([0.0, 0.0, 0.1]),\n            \"epsilon\": 1e-4\n        },\n        {\n            \"X\": np.array([\n                [-0.5,  1.0,   0.2],\n                [-0.6,  0.9,   0.3],\n                [-0.4,  1.1,   0.4],\n                [-0.5,  1.05,  0.25],\n                [-0.55, 0.95,  0.35],\n                [-0.45, 1.02,  0.28]\n            ]),\n            \"gamma\": np.array([0.7, 1.0, 2.5]),\n            \"beta\": np.array([-0.1, 0.0, 0.0]),\n            \"epsilon\": 1e-5\n        }\n    ]\n\n    # Fixed parameters\n    u = np.array([1.0, -0.5, 0.2])\n    tau = 0.15\n\n    results = []\n    prev_spectral_norm_bn = None\n\n    def calculate_covariance_matrix(Z):\n        \"\"\"\n        Computes the sample covariance matrix for a given data matrix Z.\n        Sigma_hat(Z) = (1/n) * (Z - 1*z_bar^T)^T * (Z - 1*z_bar^T)\n        \"\"\"\n        n = Z.shape[0]\n        # Calculate column-wise mean z_bar\n        z_bar = np.mean(Z, axis=0)\n        # Center the matrix Z\n        Z_centered = Z - z_bar\n        # Compute the covariance matrix\n        cov_Z = (1/n) * Z_centered.T @ Z_centered\n        return cov_Z\n\n    for i, case in enumerate(test_cases):\n        X = case[\"X\"]\n        gamma = case[\"gamma\"]\n        beta = case[\"beta\"]\n        epsilon = case[\"epsilon\"]\n        n = X.shape[0]\n\n        # 1. Batch Normalization Transformation\n        # a. Compute batch statistics (mean and variance)\n        mu = np.mean(X, axis=0)\n        # np.var uses 1/n divisor by default (ddof=0), which matches the problem spec.\n        var = np.var(X, axis=0)\n        \n        # b. Normalize and transform X to get Y\n        X_normalized = (X - mu) / np.sqrt(var + epsilon)\n        Y = gamma * X_normalized + beta\n\n        # 2. Covariance Matrix Computation\n        # a. Covariance of input X\n        sigma_X = calculate_covariance_matrix(X)\n        \n        # b. Covariance of BN output Y\n        sigma_bn = calculate_covariance_matrix(Y)\n\n        # 3. Spectral Norm Computation\n        # The spectral norm is the largest singular value. For a symmetric PSD matrix,\n        # it is also the largest eigenvalue. We compute singular values.\n        singular_values = np.linalg.svd(sigma_bn, compute_uv=False)\n        spectral_norm_bn = singular_values[0]\n\n        # 4. Stability Indicator\n        if i == 0:\n            # For the first snapshot, stability is True by definition.\n            is_stable = True\n        else:\n            # For subsequent snapshots, compare with the previous norm.\n            is_stable = np.abs(spectral_norm_bn - prev_spectral_norm_bn) <= tau\n        \n        # Store for the next iteration\n        prev_spectral_norm_bn = spectral_norm_bn\n\n        # 5. Quadratic Form Ratio\n        # q_A(u) = u^T * A * u\n        q_X = u.T @ sigma_X @ u\n        q_BN = u.T @ sigma_bn @ u\n        \n        # Handle potential division by zero, though not expected for this data\n        if q_X == 0:\n            # If the variance in direction u is zero for X, the ratio is ill-defined.\n            # We can set it to a placeholder like infinity or NaN depending on context.\n            # For this problem, we assume q_X will be non-zero.\n            r = np.inf if q_BN != 0 else np.nan\n        else:\n            r = q_BN / q_X\n\n        # Store the results for this snapshot\n        results.append([spectral_norm_bn, is_stable, r])\n\n    # Final print statement in the exact required format.\n    # Convert boolean to Python's True/False string representation\n    formatted_results = []\n    for res in results:\n        # Format: [float, boolean, float]\n        formatted_results.append(f\"[{res[0]},{str(res[1])},{res[2]}]\")\n\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3146421"}]}