{"hands_on_practices": [{"introduction": "To begin, we ground our understanding of conditional distributions with a foundational exercise. This practice focuses on deriving the conditional expectation for continuous random variables from their joint probability density function (PDF) [@problem_id:2490]. By working through this problem, you will master the essential mechanics of calculating marginal and conditional distributions from a joint PDF defined over a non-rectangular region, a crucial skill for building predictive models.", "problem": "Two continuous random variables, $X$ and $Y$, have a joint probability density function (PDF) $f_{X,Y}(x,y)$ defined over the $xy$-plane. The PDF is non-zero only within a specific triangular region $R$ and is zero everywhere else. The region $R$ is defined by the vertices at the points $(0,0)$, $(1,1)$, and $(0,1)$.\n\nThe joint PDF is given by:\n$$\nf_{X,Y}(x,y) =\n\\begin{cases}\n  2  \\text{if } (x,y) \\in R \\\\\n  0  \\text{otherwise}\n\\end{cases}\n$$\n\nYour task is to derive an expression for the conditional expectation of the random variable $Y$ given that the random variable $X$ has taken the value $x$. This is denoted as $E[Y|X=x]$. The derived expression should be a function of $x$ and be valid for all values of $x$ for which the conditional expectation is defined.", "solution": "To find the conditional expectation $E[Y|X=x]$, we first need to determine the marginal probability density function of $X$, denoted $f_X(x)$, and then the conditional probability density function of $Y$ given $X=x$, denoted $f_{Y|X}(y|x)$.\n\n**Step 1: Define the region of support and find the marginal PDF $f_X(x)$**\n\nThe region of support $R$ is the triangle with vertices $(0,0)$, $(1,1)$, and $(0,1)$. This region is bounded by the lines $x=0$, $y=1$, and $y=x$. Therefore, the region $R$ can be described by the inequalities $0 \\le x \\le y \\le 1$.\n\nThe marginal PDF of $X$, $f_X(x)$, is obtained by integrating the joint PDF $f_{X,Y}(x,y)$ with respect to $y$ over its entire range.\n\n$$\nf_X(x) = \\int_{-\\infty}^{\\infty} f_{X,Y}(x,y) \\, dy\n$$\n\nFor a fixed value of $x$ in the interval $[0, 1]$, the variable $y$ ranges from $x$ to $1$. For any $x$ outside this interval, $f_{X,Y}(x,y)=0$ for all $y$, so $f_X(x)=0$.\n\nFor $x \\in [0, 1]$:\n$$\nf_X(x) = \\int_{x}^{1} 2 \\, dy\n$$\n\nEvaluating the integral:\n$$\nf_X(x) = 2 [y]_{y=x}^{y=1} = 2(1 - x)\n$$\n\nSo, the marginal PDF of $X$ is:\n$$\nf_X(x) =\n\\begin{cases}\n  2(1-x)  \\text{if } 0 \\le x \\le 1 \\\\\n  0  \\text{otherwise}\n\\end{cases}\n$$\n\n**Step 2: Find the conditional PDF $f_{Y|X}(y|x)$**\n\nThe conditional PDF of $Y$ given $X=x$ is defined as the ratio of the joint PDF to the marginal PDF of $X$:\n$$\nf_{Y|X}(y|x) = \\frac{f_{X,Y}(x,y)}{f_X(x)}\n$$\nThis is defined for $f_X(x) > 0$, which corresponds to the interval $0 \\le x  1$.\n\nIn the region where both PDFs are non-zero (i.e., for $0 \\le x  1$ and $x \\le y \\le 1$), we have:\n$$\nf_{Y|X}(y|x) = \\frac{2}{2(1-x)} = \\frac{1}{1-x}\n$$\n\nSo, the conditional PDF is:\n$$\nf_{Y|X}(y|x) =\n\\begin{cases}\n  \\frac{1}{1-x}  \\text{if } x \\le y \\le 1, \\text{ for a given } x \\in [0,1) \\\\\n  0  \\text{otherwise}\n\\end{cases}\n$$\nThis is a uniform distribution on the interval $[x, 1]$.\n\n**Step 3: Calculate the conditional expectation $E[Y|X=x]$**\n\nThe conditional expectation of $Y$ given $X=x$ is calculated by integrating the product of $y$ and the conditional PDF $f_{Y|X}(y|x)$ over all possible values of $y$.\n\n$$\nE[Y|X=x] = \\int_{-\\infty}^{\\infty} y \\cdot f_{Y|X}(y|x) \\, dy\n$$\n\nUsing the conditional PDF we found, the integral is non-zero only for $y$ in the interval $[x, 1]$.\n$$\nE[Y|X=x] = \\int_{x}^{1} y \\left( \\frac{1}{1-x} \\right) dy\n$$\n\nSince $\\frac{1}{1-x}$ is a constant with respect to the integration variable $y$, we can pull it out of the integral:\n$$\nE[Y|X=x] = \\frac{1}{1-x} \\int_{x}^{1} y \\, dy\n$$\n\nNow, we evaluate the integral of $y$:\n$$\n\\int_{x}^{1} y \\, dy = \\left[ \\frac{y^2}{2} \\right]_{y=x}^{y=1} = \\frac{1^2}{2} - \\frac{x^2}{2} = \\frac{1-x^2}{2}\n$$\n\nSubstitute this result back into the expression for the expectation:\n$$\nE[Y|X=x] = \\frac{1}{1-x} \\left( \\frac{1-x^2}{2} \\right)\n$$\n\nWe can simplify the expression by factoring the term $(1-x^2)$ as $(1-x)(1+x)$:\n$$\nE[Y|X=x] = \\frac{(1-x)(1+x)}{2(1-x)}\n$$\n\nFor $x \\neq 1$, we can cancel the $(1-x)$ term:\n$$\nE[Y|X=x] = \\frac{1+x}{2}\n$$\nThis expression is the conditional expectation for $Y$ given $X=x$, valid for $0 \\le x  1$.", "answer": "$$\n\\boxed{\\frac{1+x}{2}}\n$$", "id": "2490"}, {"introduction": "Having mastered the basic mechanics, we now explore a fascinating and counter-intuitive consequence of conditioning. This problem demonstrates how two initially independent variables can become dependent when we condition on a third variable that is a common effect of the first two—a phenomenon known as collider bias or Berkson's paradox [@problem_id:3134074]. This exercise is invaluable for understanding how selection effects can create spurious associations, a critical lesson for anyone interpreting data from non-random samples or performing feature selection in machine learning.", "problem": "A data curation pipeline in a statistical learning task uses an outcome-based filter to retain only “promising” cases. Suppose two binary features $X \\in \\{0,1\\}$ and $Y \\in \\{0,1\\}$ represent whether two independent detectors fire on an item, with $\\mathbb{P}(X=1)=\\mathbb{P}(Y=1)=\\frac{1}{2}$ and $X$ independent of $Y$ in the sense that $\\mathbb{P}(X=x, Y=y)=\\mathbb{P}(X=x)\\mathbb{P}(Y=y)$ for all $x,y \\in \\{0,1\\}$. The pipeline’s filter variable $Z \\in \\{0,1\\}$ is defined deterministically by $Z=\\mathbf{1}\\{X+Y \\geq 1\\}$, meaning an item is kept if at least one detector fires. Only items with $Z=1$ are included in the training set.\n\nStarting from the definitions of joint distribution, conditional probability $\\mathbb{P}(A \\mid B)=\\frac{\\mathbb{P}(A \\cap B)}{\\mathbb{P}(B)}$, and independence, derive the conditional joint distribution $\\mathbb{P}(X=x, Y=y \\mid Z=1)$ for all $x,y \\in \\{0,1\\}$. Use this to prove that $X$ and $Y$ are not conditionally independent given $Z=1$ by comparing $\\mathbb{P}(X=1, Y=1 \\mid Z=1)$ with $\\mathbb{P}(X=1 \\mid Z=1)\\mathbb{P}(Y=1 \\mid Z=1)$. Then, compute the conditional correlation $\\operatorname{Corr}(X,Y \\mid Z=1)$, where $\\operatorname{Corr}(X,Y \\mid Z=1)=\\frac{\\operatorname{Cov}(X,Y \\mid Z=1)}{\\sqrt{\\operatorname{Var}(X \\mid Z=1)\\operatorname{Var}(Y \\mid Z=1)}}$, $\\operatorname{Cov}(X,Y \\mid Z=1)=\\mathbb{E}[XY \\mid Z=1]-\\mathbb{E}[X \\mid Z=1]\\mathbb{E}[Y \\mid Z=1]$, and $\\operatorname{Var}(X \\mid Z=1)=\\mathbb{E}[X^{2} \\mid Z=1]-\\left(\\mathbb{E}[X \\mid Z=1]\\right)^{2}$.\n\nExplain briefly why conditioning on the outcome filter $Z$ (which is a function of both $X$ and $Y$) induces dependence between $X$ and $Y$, even though $X$ and $Y$ are independent marginally, and connect this to the potential pitfalls of feature selection that retains items based on outcomes. Provide your final numerical answer for $\\operatorname{Corr}(X,Y \\mid Z=1)$. No rounding is required.", "solution": "First, we establish the marginal and joint probabilities of the binary features $X$ and $Y$. We are given that $\\mathbb{P}(X=1) = \\frac{1}{2}$ and $\\mathbb{P}(Y=1) = \\frac{1}{2}$. Since $X$ and $Y$ are binary variables, it follows that $\\mathbb{P}(X=0) = 1 - \\mathbb{P}(X=1) = 1 - \\frac{1}{2} = \\frac{1}{2}$ and $\\mathbb{P}(Y=0) = 1 - \\mathbb{P}(Y=1) = 1 - \\frac{1}{2} = \\frac{1}{2}$.\n\nThe problem states that $X$ and $Y$ are independent, meaning $\\mathbb{P}(X=x, Y=y) = \\mathbb{P}(X=x)\\mathbb{P}(Y=y)$ for all $x, y \\in \\{0, 1\\}$. We can compute the full joint distribution:\n$$ \\mathbb{P}(X=0, Y=0) = \\mathbb{P}(X=0)\\mathbb{P}(Y=0) = \\frac{1}{2} \\times \\frac{1}{2} = \\frac{1}{4} $$\n$$ \\mathbb{P}(X=0, Y=1) = \\mathbb{P}(X=0)\\mathbb{P}(Y=1) = \\frac{1}{2} \\times \\frac{1}{2} = \\frac{1}{4} $$\n$$ \\mathbb{P}(X=1, Y=0) = \\mathbb{P}(X=1)\\mathbb{P}(Y=0) = \\frac{1}{2} \\times \\frac{1}{2} = \\frac{1}{4} $$\n$$ \\mathbb{P}(X=1, Y=1) = \\mathbb{P}(X=1)\\mathbb{P}(Y=1) = \\frac{1}{2} \\times \\frac{1}{2} = \\frac{1}{4} $$\nThe sum of these probabilities is $\\frac{1}{4} + \\frac{1}{4} + \\frac{1}{4} + \\frac{1}{4} = 1$, as expected.\n\nThe filter variable $Z$ is defined as $Z = \\mathbf{1}\\{X+Y \\geq 1\\}$. This means $Z=1$ if at least one detector fires, and $Z=0$ otherwise. The event $Z=1$ is the union of the disjoint events $\\{X=0, Y=1\\}$, $\\{X=1, Y=0\\}$, and $\\{X=1, Y=1\\}$. Alternatively, the event $Z=1$ is the complement of the event $\\{X=0, Y=0\\}$. We can calculate its probability:\n$$ \\mathbb{P}(Z=1) = \\mathbb{P}(X+Y \\geq 1) = 1 - \\mathbb{P}(X+Y  1) = 1 - \\mathbb{P}(X=0, Y=0) = 1 - \\frac{1}{4} = \\frac{3}{4} $$\n\nNow, we derive the conditional joint distribution $\\mathbb{P}(X=x, Y=y \\mid Z=1)$ using the formula for conditional probability, $\\mathbb{P}(A \\mid B) = \\frac{\\mathbb{P}(A \\cap B)}{\\mathbb{P}(B)}$.\n$$ \\mathbb{P}(X=x, Y=y \\mid Z=1) = \\frac{\\mathbb{P}(\\{X=x, Y=y\\} \\cap \\{Z=1\\})}{\\mathbb{P}(Z=1)} $$\nIf $x+y  1$ (i.e., $(x,y)=(0,0)$), the event $\\{X=x, Y=y\\}$ is incompatible with $\\{Z=1\\}$, so their intersection is the empty set, and the probability is $0$.\nIf $x+y \\geq 1$, the event $\\{X=x, Y=y\\}$ implies that $Z=1$, so the intersection $\\{X=x, Y=y\\} \\cap \\{Z=1\\}$ is simply $\\{X=x, Y=y\\}$.\nThus, for all $(x,y)$ such that $x+y \\geq 1$:\n$$ \\mathbb{P}(X=x, Y=y \\mid Z=1) = \\frac{\\mathbb{P}(X=x, Y=y)}{\\mathbb{P}(Z=1)} $$\nWe compute the conditional joint probabilities:\n$$ \\mathbb{P}(X=0, Y=0 \\mid Z=1) = 0 $$\n$$ \\mathbb{P}(X=0, Y=1 \\mid Z=1) = \\frac{\\mathbb{P}(X=0, Y=1)}{\\mathbb{P}(Z=1)} = \\frac{1/4}{3/4} = \\frac{1}{3} $$\n$$ \\mathbb{P}(X=1, Y=0 \\mid Z=1) = \\frac{\\mathbb{P}(X=1, Y=0)}{\\mathbb{P}(Z=1)} = \\frac{1/4}{3/4} = \\frac{1}{3} $$\n$$ \\mathbb{P}(X=1, Y=1 \\mid Z=1) = \\frac{\\mathbb{P}(X=1, Y=1)}{\\mathbb{P}(Z=1)} = \\frac{1/4}{3/4} = \\frac{1}{3} $$\n\nTo prove that $X$ and $Y$ are not conditionally independent given $Z=1$, we must show that $\\mathbb{P}(X=x, Y=y \\mid Z=1) \\neq \\mathbb{P}(X=x \\mid Z=1)\\mathbb{P}(Y=y \\mid Z=1)$ for at least one pair $(x,y)$. Let's use $(x,y)=(1,1)$. First, we find the conditional marginal probabilities:\n$$ \\mathbb{P}(X=1 \\mid Z=1) = \\sum_{y \\in \\{0,1\\}} \\mathbb{P}(X=1, Y=y \\mid Z=1) = \\mathbb{P}(X=1, Y=0 \\mid Z=1) + \\mathbb{P}(X=1, Y=1 \\mid Z=1) = \\frac{1}{3} + \\frac{1}{3} = \\frac{2}{3} $$\nBy symmetry,\n$$ \\mathbb{P}(Y=1 \\mid Z=1) = \\sum_{x \\in \\{0,1\\}} \\mathbb{P}(X=x, Y=1 \\mid Z=1) = \\mathbb{P}(X=0, Y=1 \\mid Z=1) + \\mathbb{P}(X=1, Y=1 \\mid Z=1) = \\frac{1}{3} + \\frac{1}{3} = \\frac{2}{3} $$\nNow we compare $\\mathbb{P}(X=1, Y=1 \\mid Z=1)$ with the product of the conditional marginals:\n$$ \\mathbb{P}(X=1, Y=1 \\mid Z=1) = \\frac{1}{3} $$\n$$ \\mathbb{P}(X=1 \\mid Z=1)\\mathbb{P}(Y=1 \\mid Z=1) = \\frac{2}{3} \\times \\frac{2}{3} = \\frac{4}{9} $$\nSince $\\frac{1}{3} \\neq \\frac{4}{9}$, we have proven that $X$ and $Y$ are not conditionally independent given $Z=1$.\n\nNext, we compute the conditional correlation $\\operatorname{Corr}(X,Y \\mid Z=1)$. We need to calculate the conditional expectations, variances, and covariance.\nThe conditional expectation of a Bernoulli variable is its conditional probability of being $1$.\n$$ \\mathbb{E}[X \\mid Z=1] = \\mathbb{P}(X=1 \\mid Z=1) = \\frac{2}{3} $$\n$$ \\mathbb{E}[Y \\mid Z=1] = \\mathbb{P}(Y=1 \\mid Z=1) = \\frac{2}{3} $$\nFor a Bernoulli variable, $X^2=X$, so $\\mathbb{E}[X^2 \\mid Z=1] = \\mathbb{E}[X \\mid Z=1] = \\frac{2}{3}$.\nThe conditional variance of $X$ is:\n$$ \\operatorname{Var}(X \\mid Z=1) = \\mathbb{E}[X^2 \\mid Z=1] - \\left(\\mathbb{E}[X \\mid Z=1]\\right)^2 = \\frac{2}{3} - \\left(\\frac{2}{3}\\right)^2 = \\frac{2}{3} - \\frac{4}{9} = \\frac{6}{9} - \\frac{4}{9} = \\frac{2}{9} $$\nBy symmetry, $\\operatorname{Var}(Y \\mid Z=1) = \\frac{2}{9}$.\n\nThe product $XY$ is also a Bernoulli variable, which is $1$ if and only if $X=1$ and $Y=1$.\n$$ \\mathbb{E}[XY \\mid Z=1] = \\mathbb{P}(XY=1 \\mid Z=1) = \\mathbb{P}(X=1, Y=1 \\mid Z=1) = \\frac{1}{3} $$\nThe conditional covariance is:\n$$ \\operatorname{Cov}(X,Y \\mid Z=1) = \\mathbb{E}[XY \\mid Z=1] - \\mathbb{E}[X \\mid Z=1]\\mathbb{E}[Y \\mid Z=1] = \\frac{1}{3} - \\left(\\frac{2}{3}\\right)\\left(\\frac{2}{3}\\right) = \\frac{1}{3} - \\frac{4}{9} = \\frac{3}{9} - \\frac{4}{9} = -\\frac{1}{9} $$\nFinally, the conditional correlation is:\n$$ \\operatorname{Corr}(X,Y \\mid Z=1) = \\frac{\\operatorname{Cov}(X,Y \\mid Z=1)}{\\sqrt{\\operatorname{Var}(X \\mid Z=1)\\operatorname{Var}(Y \\mid Z=1)}} = \\frac{-1/9}{\\sqrt{(\\frac{2}{9})(\\frac{2}{9})}} = \\frac{-1/9}{2/9} = -\\frac{1}{2} $$\n\nThe phenomenon where two independent variables become dependent when conditioned on a common effect is known as Berkson's paradox or collider bias. Here, $Z$ is a \"collider\" because its value is determined by both $X$ and $Y$. We are selecting a sub-population where $Z=1$, i.e., at least one detector fired. Within this selected group, information about one variable provides information about the other. For instance, if we are in the $Z=1$ group and we observe $X=0$, we know with certainty that $Y$ must be $1$ to satisfy the condition $X+Y \\geq 1$. This creates a dependency. The calculated negative correlation $\\left(-\\frac{1}{2}\\right)$ indicates that within the filtered dataset, if one detector fires, it's less likely that the other one also fired compared to the baseline conditional probability. Specifically, $\\mathbb{P}(Y=1 \\mid X=1, Z=1) = \\frac{\\mathbb{P}(X=1, Y=1 \\mid Z=1)}{\\mathbb{P}(X=1 \\mid Z=1)} = \\frac{1/3}{2/3} = \\frac{1}{2}$, which is less than $\\mathbb{P}(Y=1 \\mid Z=1) = \\frac{2}{3}$. In statistical learning, if feature selection or data curation is performed by filtering on a variable that is a function of the outcome (or the outcome itself), this can induce spurious correlations between predictor variables that are independent in the general population. This may lead to incorrect model specifications and flawed scientific conclusions about the relationships between predictors.", "answer": "$$\\boxed{-\\frac{1}{2}}$$", "id": "3134074"}, {"introduction": "In our final practice, we apply the principles of joint and conditional distributions to a core task in machine learning: classification. This exercise guides you through the derivation of the decision boundary for a Gaussian Discriminant Analysis (GDA) classifier from first principles [@problem_id:3134073]. By modeling the class-conditional distributions $p(\\mathbf{x} | y)$ and applying Bayes' rule to find the posterior $p(y | \\mathbf{x})$, you will see how to construct a powerful generative model and understand why different assumptions about the data's covariance structure lead to different types of decision boundaries.", "problem": "Consider a binary classification problem in which the feature vector $\\mathbf{x} \\in \\mathbb{R}^{2}$ and the class label $y \\in \\{1,2\\}$. In Gaussian Discriminant Analysis (GDA), we posit that the class-conditional distributions are multivariate normal and the joint distribution factorizes as $p(\\mathbf{x}, y) = p(y)\\,p(\\mathbf{x}\\,|\\,y)$. Suppose you simulate data from the following generative model (no simulation needs to be performed; this specification is provided solely to ground the derivation):\n\n- Class priors: $p(y=1) = \\pi_{1} = 0.6$ and $p(y=2) = \\pi_{2} = 0.4$.\n- Class-conditional distributions: $p(\\mathbf{x}\\,|\\,y=k) = \\mathcal{N}(\\boldsymbol{\\mu}_{k}, \\boldsymbol{\\Sigma}_{k})$ for $k \\in \\{1,2\\}$, with parameters\n$$\n\\boldsymbol{\\mu}_{1} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}, \\quad \\boldsymbol{\\Sigma}_{1} = \\begin{pmatrix} 2  1 \\\\ 1  2 \\end{pmatrix}, \\qquad\n\\boldsymbol{\\mu}_{2} = \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix}, \\quad \\boldsymbol{\\Sigma}_{2} = \\begin{pmatrix} 1  0 \\\\ 0  3 \\end{pmatrix}.\n$$\n\nStarting only from the fundamental definitions of joint and conditional distributions and Bayes’ rule, derive the decision rule that compares the posterior probabilities $p(y=1\\,|\\,\\mathbf{x})$ and $p(y=2\\,|\\,\\mathbf{x})$ and show that it yields a quadratic decision boundary. Then, using the specified parameters, explicitly compute the discriminant boundary function $g(\\mathbf{x})$ whose zero level set $g(\\mathbf{x}) = 0$ separates the classes, where $g(\\mathbf{x})$ is the log-posterior difference $g(\\mathbf{x}) = \\ln p(y=1\\,|\\,\\mathbf{x}) - \\ln p(y=2\\,|\\,\\mathbf{x})$ up to an additive constant common to both classes.\n\nExpress your final $g(\\mathbf{x})$ as a single analytic expression in terms of $x_{1}$ and $x_{2}$ (with no equation or inequality), simplified as much as possible. No numerical rounding is required.", "solution": "The goal is to find the decision boundary that separates the two classes. This boundary is defined by the set of points $\\mathbf{x}$ where the posterior probabilities are equal: $p(y=1\\,|\\,\\mathbf{x}) = p(y=2\\,|\\,\\mathbf{x})$. This is equivalent to the log-posterior probabilities being equal, $\\ln p(y=1\\,|\\,\\mathbf{x}) = \\ln p(y=2\\,|\\,\\mathbf{x})$, or their difference being zero: $g(\\mathbf{x}) = \\ln p(y=1\\,|\\,\\mathbf{x}) - \\ln p(y=2\\,|\\,\\mathbf{x}) = 0$.\n\nUsing Bayes' rule, the posterior probability is $p(y=k\\,|\\,\\mathbf{x}) = \\frac{p(\\mathbf{x}\\,|\\,y=k)p(y=k)}{p(\\mathbf{x})}$.\nThe log-posterior is $\\ln p(y=k\\,|\\,\\mathbf{x}) = \\ln p(\\mathbf{x}\\,|\\,y=k) + \\ln p(y=k) - \\ln p(\\mathbf{x})$.\n\nThe discriminant function $g(\\mathbf{x})$ is:\n$$g(\\mathbf{x}) = (\\ln p(\\mathbf{x}\\,|\\,y=1) + \\ln p(y=1) - \\ln p(\\mathbf{x})) - (\\ln p(\\mathbf{x}\\,|\\,y=2) + \\ln p(y=2) - \\ln p(\\mathbf{x}))$$\n$$g(\\mathbf{x}) = \\ln p(\\mathbf{x}\\,|\\,y=1) - \\ln p(\\mathbf{x}\\,|\\,y=2) + \\ln \\pi_1 - \\ln \\pi_2$$\nThe term $\\ln p(\\mathbf{x})$ cancels out, which corresponds to the problem's statement about ignoring an additive constant common to both classes when defining the discriminant scores.\n\nThe class-conditional distribution $p(\\mathbf{x}\\,|\\,y=k)$ is a multivariate normal distribution with dimension $d=2$:\n$$p(\\mathbf{x}\\,|\\,y=k) = \\frac{1}{(2\\pi)^{d/2}|\\boldsymbol{\\Sigma}_k|^{1/2}} \\exp\\left(-\\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu}_k)^T \\boldsymbol{\\Sigma}_k^{-1} (\\mathbf{x}-\\boldsymbol{\\mu}_k)\\right)$$\nTaking the natural logarithm gives:\n$$\\ln p(\\mathbf{x}\\,|\\,y=k) = -\\frac{d}{2}\\ln(2\\pi) - \\frac{1}{2}\\ln|\\boldsymbol{\\Sigma}_k| - \\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu}_k)^T \\boldsymbol{\\Sigma}_k^{-1} (\\mathbf{x}-\\boldsymbol{\\mu}_k)$$\nThe term $(\\mathbf{x}-\\boldsymbol{\\mu}_k)^T \\boldsymbol{\\Sigma}_k^{-1} (\\mathbf{x}-\\boldsymbol{\\mu}_k)$ can be expanded as $\\mathbf{x}^T\\boldsymbol{\\Sigma}_k^{-1}\\mathbf{x} - 2\\boldsymbol{\\mu}_k^T\\boldsymbol{\\Sigma}_k^{-1}\\mathbf{x} + \\boldsymbol{\\mu}_k^T\\boldsymbol{\\Sigma}_k^{-1}\\boldsymbol{\\mu}_k$. This is a quadratic function of $\\mathbf{x}$. The decision boundary is determined by setting the difference of two such log-probabilities to a constant, which results in a quadratic equation in the components of $\\mathbf{x}$, unless the covariance matrices are equal ($\\boldsymbol{\\Sigma}_1 = \\boldsymbol{\\Sigma}_2$), in which case the quadratic terms $\\mathbf{x}^T\\boldsymbol{\\Sigma}^{-1}\\mathbf{x}$ cancel, leaving a linear boundary. Since $\\boldsymbol{\\Sigma}_1 \\neq \\boldsymbol{\\Sigma}_2$ in this problem, the boundary is quadratic.\n\nWe define the discriminant score for each class, omitting the common constant $-\\frac{d}{2}\\ln(2\\pi)$:\n$$\\delta_k(\\mathbf{x}) = -\\frac{1}{2}\\ln|\\boldsymbol{\\Sigma}_k| - \\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu}_k)^T \\boldsymbol{\\Sigma}_k^{-1} (\\mathbf{x}-\\boldsymbol{\\mu}_k) + \\ln\\pi_k$$\nThe required function is $g(\\mathbf{x}) = \\delta_1(\\mathbf{x}) - \\delta_2(\\mathbf{x})$.\n\nFirst, we compute the necessary components from the given parameters:\n-   Determinants: $|\\boldsymbol{\\Sigma}_1| = (2)(2)-(1)(1) = 3$. $|\\boldsymbol{\\Sigma}_2| = (1)(3)-(0)(0) = 3$. Since $|\\boldsymbol{\\Sigma}_1|=|\\boldsymbol{\\Sigma}_2|$, the terms $-\\frac{1}{2}\\ln|\\boldsymbol{\\Sigma}_k|$ will cancel in $g(\\mathbf{x})$.\n-   Inverse covariance matrices:\n    $$\\boldsymbol{\\Sigma}_1^{-1} = \\frac{1}{3}\\begin{pmatrix} 2  -1 \\\\ -1  2 \\end{pmatrix} = \\begin{pmatrix} \\frac{2}{3}  -\\frac{1}{3} \\\\ -\\frac{1}{3}  \\frac{2}{3} \\end{pmatrix}$$\n    $$\\boldsymbol{\\Sigma}_2^{-1} = \\frac{1}{3}\\begin{pmatrix} 3  0 \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} 1  0 \\\\ 0  \\frac{1}{3} \\end{pmatrix}$$\n-   Log of priors: $\\ln \\pi_1 = \\ln(0.6)$, $\\ln \\pi_2 = \\ln(0.4)$. Their difference is $\\ln(0.6) - \\ln(0.4) = \\ln(\\frac{0.6}{0.4}) = \\ln(\\frac{3}{2})$.\n\nNow, we expand the quadratic forms.\nFor class 1, with $\\mathbf{x} = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}$ and $\\boldsymbol{\\mu}_1 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$:\n$$(\\mathbf{x}-\\boldsymbol{\\mu}_1)^T \\boldsymbol{\\Sigma}_1^{-1} (\\mathbf{x}-\\boldsymbol{\\mu}_1) = \\begin{pmatrix} x_1-1  x_2 \\end{pmatrix} \\begin{pmatrix} \\frac{2}{3}  -\\frac{1}{3} \\\\ -\\frac{1}{3}  \\frac{2}{3} \\end{pmatrix} \\begin{pmatrix} x_1-1 \\\\ x_2 \\end{pmatrix}$$\n$$= \\frac{1}{3} \\left( 2(x_1-1)^2 - 2x_2(x_1-1) + 2x_2^2 \\right)$$\n$$= \\frac{1}{3} \\left( 2(x_1^2 - 2x_1 + 1) - 2x_1x_2 + 2x_2 + 2x_2^2 \\right)$$\n$$= \\frac{2}{3}x_1^2 - \\frac{2}{3}x_1x_2 + \\frac{2}{3}x_2^2 - \\frac{4}{3}x_1 + \\frac{2}{3}x_2 + \\frac{2}{3}$$\nThe contribution to $\\delta_1(\\mathbf{x})$ is $-\\frac{1}{2}$ of this:\n$$-\\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu}_1)^T \\boldsymbol{\\Sigma}_1^{-1} (\\mathbf{x}-\\boldsymbol{\\mu}_1) = -\\frac{1}{3}x_1^2 + \\frac{1}{3}x_1x_2 - \\frac{1}{3}x_2^2 + \\frac{2}{3}x_1 - \\frac{1}{3}x_2 - \\frac{1}{3}$$\n\nFor class 2, with $\\boldsymbol{\\mu}_2 = \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix}$:\n$$(\\mathbf{x}-\\boldsymbol{\\mu}_2)^T \\boldsymbol{\\Sigma}_2^{-1} (\\mathbf{x}-\\boldsymbol{\\mu}_2) = \\begin{pmatrix} x_1+1  x_2-1 \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 0  \\frac{1}{3} \\end{pmatrix} \\begin{pmatrix} x_1+1 \\\\ x_2-1 \\end{pmatrix}$$\n$$= (x_1+1)^2(1) + (x_2-1)^2\\left(\\frac{1}{3}\\right)$$\n$$= (x_1^2 + 2x_1 + 1) + \\frac{1}{3}(x_2^2 - 2x_2 + 1)$$\n$$= x_1^2 + \\frac{1}{3}x_2^2 + 2x_1 - \\frac{2}{3}x_2 + 1 + \\frac{1}{3} = x_1^2 + \\frac{1}{3}x_2^2 + 2x_1 - \\frac{2}{3}x_2 + \\frac{4}{3}$$\nThe contribution to $\\delta_2(\\mathbf{x})$ is $-\\frac{1}{2}$ of this:\n$$-\\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu}_2)^T \\boldsymbol{\\Sigma}_2^{-1} (\\mathbf{x}-\\boldsymbol{\\mu}_2) = -\\frac{1}{2}x_1^2 - \\frac{1}{6}x_2^2 - x_1 + \\frac{1}{3}x_2 - \\frac{2}{3}$$\n\nNow we compute $g(\\mathbf{x}) = \\delta_1(\\mathbf{x}) - \\delta_2(\\mathbf{x})$. The $\\ln|\\boldsymbol{\\Sigma}_k|$ terms cancel.\n$$g(\\mathbf{x}) = \\left(-\\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu}_1)^T \\boldsymbol{\\Sigma}_1^{-1} (\\mathbf{x}-\\boldsymbol{\\mu}_1) + \\ln\\pi_1\\right) - \\left(-\\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu}_2)^T \\boldsymbol{\\Sigma}_2^{-1} (\\mathbf{x}-\\boldsymbol{\\mu}_2) + \\ln\\pi_2\\right)$$\nSubstituting the expanded forms:\n$$g(\\mathbf{x}) = \\left(-\\frac{1}{3}x_1^2 + \\frac{1}{3}x_1x_2 - \\frac{1}{3}x_2^2 + \\frac{2}{3}x_1 - \\frac{1}{3}x_2 - \\frac{1}{3}\\right) - \\left(-\\frac{1}{2}x_1^2 - \\frac{1}{6}x_2^2 - x_1 + \\frac{1}{3}x_2 - \\frac{2}{3}\\right) + \\ln\\left(\\frac{3}{2}\\right)$$\nWe collect terms by powers of $x_1$ and $x_2$:\n-   $x_1^2$ terms: $(-\\frac{1}{3} + \\frac{1}{2})x_1^2 = \\frac{1}{6}x_1^2$\n-   $x_2^2$ terms: $(-\\frac{1}{3} + \\frac{1}{6})x_2^2 = -\\frac{1}{6}x_2^2$\n-   $x_1x_2$ term: $\\frac{1}{3}x_1x_2$\n-   $x_1$ terms: $(\\frac{2}{3} - (-1))x_1 = \\frac{5}{3}x_1$\n-   $x_2$ terms: $(-\\frac{1}{3} - \\frac{1}{3})x_2 = -\\frac{2}{3}x_2$\n-   Constant terms: $(-\\frac{1}{3} - (-\\frac{2}{3})) + \\ln(\\frac{3}{2}) = \\frac{1}{3} + \\ln(\\frac{3}{2})$\n\nCombining all terms gives the final expression for the discriminant function $g(\\mathbf{x})$:\n$$g(\\mathbf{x}) = \\frac{1}{6}x_1^2 - \\frac{1}{6}x_2^2 + \\frac{1}{3}x_1x_2 + \\frac{5}{3}x_1 - \\frac{2}{3}x_2 + \\frac{1}{3} + \\ln\\left(\\frac{3}{2}\\right)$$", "answer": "$$\\boxed{\\frac{1}{6}x_{1}^{2} - \\frac{1}{6}x_{2}^{2} + \\frac{1}{3}x_{1}x_{2} + \\frac{5}{3}x_{1} - \\frac{2}{3}x_{2} + \\frac{1}{3} + \\ln\\left(\\frac{3}{2}\\right)}$$", "id": "3134073"}]}