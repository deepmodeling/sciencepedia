{"hands_on_practices": [{"introduction": "When features in a dataset are exact linear combinations of each other, we encounter perfect multicollinearity, a condition that undermines the stability of a linear model. This exercise provides a direct, hands-on look into the consequences by asking you to work with a matrix where such dependencies are built-in [@problem_id:3140085]. By computing the null space from first principles, you will uncover the precise reason why model parameters become unidentifiable, providing a foundational understanding of model stability.", "problem": "Consider a linear regression design matrix $X \\in \\mathbb{R}^{4 \\times 5}$ used in statistical learning, with columns denoted by $c_{1}, c_{2}, c_{3}, c_{4}, c_{5} \\in \\mathbb{R}^{4}$ as follows:\n$$\nc_{1} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 2 \\\\ -1 \\end{pmatrix},\\quad\nc_{2} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 3 \\\\ 2 \\end{pmatrix},\\quad\nc_{4} = \\begin{pmatrix} 2 \\\\ -1 \\\\ 0 \\\\ 3 \\end{pmatrix},\n$$\nand define two additional features by exact sums:\n$$\nc_{3} = c_{1} + c_{2},\\quad c_{5} = c_{2} + c_{4}.\n$$\nLet $X = [\\,c_{1}\\; c_{2}\\; c_{3}\\; c_{4}\\; c_{5}\\,]$.\n\nUsing only fundamental definitions from linear algebra appropriate to statistical learning (definitions of linear independence, rank, null space, and the identity matrix), do the following:\n\n- Verify the specified linear dependencies among the columns and determine whether $c_{1}, c_{2}, c_{4}$ are linearly independent.\n- From first principles, determine the rank of $X$ and compute a basis for the null space $\\mathcal{N}(X) = \\{\\,w \\in \\mathbb{R}^{5} : Xw = 0\\,\\}$.\n- In the context of the linear model $y = X\\beta + \\varepsilon$ with zero-mean noise $\\varepsilon$, discuss the implications of any nontrivial null space for parameter interpretability, and explain conceptually how adding a positive multiple of the identity matrix $I_{5}$ to a Gram-type matrix relates to identifiability.\n\nYour final answer must be a single analytical expression giving a basis for the null space of $X$ (you may present the basis vectors as the columns of a single matrix). Do not include explanatory text in the final answer box.", "solution": "The problem is assessed to be valid as it is mathematically and scientifically sound, self-contained, well-posed, and directly relevant to the specified topics of linear algebra and statistical learning. We may proceed with a full solution.\n\nThe design matrix is $X \\in \\mathbb{R}^{4 \\times 5}$, with columns $c_1, c_2, c_3, c_4, c_5 \\in \\mathbb{R}^4$.\nThe columns are given as:\n$c_{1} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 2 \\\\ -1 \\end{pmatrix}$, $c_{2} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 3 \\\\ 2 \\end{pmatrix}$, $c_{4} = \\begin{pmatrix} 2 \\\\ -1 \\\\ 0 \\\\ 3 \\end{pmatrix}$.\nAnd the remaining columns are defined by linear combinations:\n$c_{3} = c_{1} + c_{2}$\n$c_{5} = c_{2} + c_{4}$\n\nFirst, we verify if the vectors $c_1, c_2, c_4$ are linearly independent. By definition, a set of vectors is linearly independent if the only linear combination of them that equals the zero vector is the trivial combination where all coefficients are zero. We seek to find the solutions for scalars $a_1, a_2, a_4 \\in \\mathbb{R}$ to the equation:\n$$a_1 c_1 + a_2 c_2 + a_4 c_4 = 0$$\nThis vector equation corresponds to a homogeneous system of linear equations:\n$$\n\\begin{pmatrix} 1 & 0 & 2 \\\\ 0 & 1 & -1 \\\\ 2 & 3 & 0 \\\\ -1 & 2 & 3 \\end{pmatrix}\n\\begin{pmatrix} a_1 \\\\ a_2 \\\\ a_4 \\end{pmatrix} =\n\\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nWe perform Gaussian elimination on the coefficient matrix:\n$$\n\\begin{pmatrix} 1 & 0 & 2 \\\\ 0 & 1 & -1 \\\\ 2 & 3 & 0 \\\\ -1 & 2 & 3 \\end{pmatrix}\n\\xrightarrow[R_4 \\to R_4 + R_1]{R_3 \\to R_3 - 2R_1}\n\\begin{pmatrix} 1 & 0 & 2 \\\\ 0 & 1 & -1 \\\\ 0 & 3 & -4 \\\\ 0 & 2 & 5 \\end{pmatrix}\n\\xrightarrow[R_4 \\to R_4 - 2R_2]{R_3 \\to R_3 - 3R_2}\n\\begin{pmatrix} 1 & 0 & 2 \\\\ 0 & 1 & -1 \\\\ 0 & 0 & -1 \\\\ 0 & 0 & 7 \\end{pmatrix}\n\\xrightarrow{R_4 \\to R_4 + 7R_3}\n\\begin{pmatrix} 1 & 0 & 2 \\\\ 0 & 1 & -1 \\\\ 0 & 0 & -1 \\\\ 0 & 0 & 0 \\end{pmatrix}\n$$\nThe matrix in row echelon form has $3$ pivots. This indicates that the only solution to the system is the trivial solution $a_1 = 0$, $a_2 = 0$, $a_4 = 0$. Therefore, the vectors $c_1, c_2, c_4$ are linearly independent.\n\nNext, we determine the rank of $X$. The rank of a matrix is the dimension of its column space, which is equivalent to the maximum number of linearly independent columns. The column space of $X$ is $\\mathcal{C}(X) = \\text{span}\\{c_1, c_2, c_3, c_4, c_5\\}$. Using the given definitions for $c_3$ and $c_5$:\n$$\\mathcal{C}(X) = \\text{span}\\{c_1, c_2, c_1+c_2, c_4, c_2+c_4\\}$$\nSince $c_3$ and $c_5$ are linear combinations of other vectors in the set, they do not increase the dimension of the span. Thus, the spanning set can be reduced to:\n$$\\mathcal{C}(X) = \\text{span}\\{c_1, c_2, c_4\\}$$\nAs we have just demonstrated that $\\{c_1, c_2, c_4\\}$ is a linearly independent set, it forms a basis for the column space of $X$. The dimension of the column space is the number of vectors in its basis, which is $3$. Therefore, the rank of $X$ is $\\text{rank}(X) = 3$.\n\nNow, we compute a basis for the null space $\\mathcal{N}(X)$. The null space is the set of all vectors $w \\in \\mathbb{R}^5$ such that $Xw = 0$. Let $w = (w_1, w_2, w_3, w_4, w_5)^T$. The condition $Xw = 0$ can be written as a linear combination of the columns of $X$:\n$$w_1 c_1 + w_2 c_2 + w_3 c_3 + w_4 c_4 + w_5 c_5 = 0$$\nSubstituting the definitions of $c_3$ and $c_5$:\n$$w_1 c_1 + w_2 c_2 + w_3 (c_1 + c_2) + w_4 c_4 + w_5 (c_2 + c_4) = 0$$\nWe group the terms by the linearly independent vectors $c_1, c_2, c_4$:\n$$(w_1 + w_3)c_1 + (w_2 + w_3 + w_5)c_2 + (w_4 + w_5)c_4 = 0$$\nSince $c_1, c_2, c_4$ are linearly independent, their coefficients must all be zero:\n\\begin{align*} w_1 + w_3 &= 0 \\\\ w_2 + w_3 + w_5 &= 0 \\\\ w_4 + w_5 &= 0 \\end{align*}\nThis is a system of $3$ equations with $5$ unknowns. The number of free variables is $5 - 3 = 2$, which corresponds to the nullity of $X$. This is consistent with the Rank-Nullity Theorem: $\\text{rank}(X) + \\text{nullity}(X) = 5$. Let's choose $w_3$ and $w_5$ as the free parameters. Let $w_3 = s$ and $w_5 = t$, for any $s, t \\in \\mathbb{R}$.\nFrom the first equation: $w_1 = -w_3 = -s$.\nFrom the third equation: $w_4 = -w_5 = -t$.\nFrom the second equation: $w_2 = -w_3 - w_5 = -s - t$.\nThe general solution vector $w$ is:\n$$w = \\begin{pmatrix} w_1 \\\\ w_2 \\\\ w_3 \\\\ w_4 \\\\ w_5 \\end{pmatrix} = \\begin{pmatrix} -s \\\\ -s-t \\\\ s \\\\ -t \\\\ t \\end{pmatrix} = s \\begin{pmatrix} -1 \\\\ -1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix} + t \\begin{pmatrix} 0 \\\\ -1 \\\\ 0 \\\\ -1 \\\\ 1 \\end{pmatrix}$$\nA basis for the null space is formed by the vectors corresponding to setting one free parameter to $1$ and the other to $0$.\nLet $v_1 = (-1, -1, 1, 0, 0)^T$ (for $s=1, t=0$) and $v_2 = (0, -1, 0, -1, 1)^T$ (for $s=0, t=1$).\nThe set $\\{v_1, v_2\\}$ is a basis for $\\mathcal{N}(X)$.\n\nIn the context of the linear model $y = X\\beta + \\varepsilon$, a non-trivial null space for $X$ (meaning $\\mathcal{N}(X) \\neq \\{0\\}$) signifies perfect multicollinearity among the predictor variables. This has severe implications for parameter interpretability. The Ordinary Least Squares (OLS) method seeks a coefficient vector $\\beta$ that minimizes the sum of squared residuals, which solves the normal equations $(X^\\top X)\\beta = X^\\top y$. If the columns of $X$ are linearly dependent, the Gram matrix $X^\\top X$ is singular and cannot be inverted. Consequently, there is no unique solution for $\\beta$.\nIf $\\hat{\\beta}$ is any particular solution, then for any vector $w \\in \\mathcal{N}(X)$, the vector $\\beta^* = \\hat{\\beta} + w$ is also a solution, since $X\\beta^* = X(\\hat{\\beta} + w) = X\\hat{\\beta} + Xw = X\\hat{\\beta} + 0 = X\\hat{\\beta}$. This means that an infinite number of different coefficient vectors $\\beta$ yield the exact same predictions. This makes it impossible to identify the unique \"effect\" of each predictor. For instance, using the basis vector $v_1$, we see that decreasing $\\beta_1$ and $\\beta_2$ by some amount $k$ while increasing $\\beta_3$ by $k$ results in an observationally identical model. The individual coefficients $\\beta_j$ are not identifiable.\n\nAdding a positive multiple of the identity matrix, $\\lambda I_5$ where $\\lambda > 0$, to the Gram matrix $X^\\top X$ is the core mechanism of ridge regression. The ridge regression solution is $\\hat{\\beta}_{\\text{ridge}} = (X^\\top X + \\lambda I_5)^{-1} X^\\top y$. This procedure ensures identifiability by guaranteeing that the matrix $(X^\\top X + \\lambda I_5)$ is invertible, even when $X^\\top X$ is singular.\nConceptually, this works because $X^\\top X$ is a positive semi-definite matrix, meaning all its eigenvalues $\\mu_i$ are non-negative ($\\mu_i \\ge 0$). Since $X$ has linearly dependent columns, $X^\\top X$ is singular, which implies at least one of its eigenvalues is exactly $0$. Let $v$ be an eigenvector of $X^\\top X$ with eigenvalue $\\mu$. Then $(X^\\top X + \\lambda I_5)v = (X^\\top X)v + (\\lambda I_5)v = \\mu v + \\lambda v = (\\mu + \\lambda)v$. This shows that the eigenvalues of the regularized matrix $(X^\\top X + \\lambda I_5)$ are $(\\mu_i + \\lambda)$. Since $\\mu_i \\ge 0$ and $\\lambda > 0$, all eigenvalues $(\\mu_i + \\lambda)$ are strictly positive. A square matrix is invertible if and only if all its eigenvalues are non-zero. By shifting all eigenvalues to be strictly positive, the matrix $(X^\\top X + \\lambda I_5)$ becomes invertible, which provides a unique solution for $\\beta$. This regularization resolves the multicollinearity problem mathematically, allowing for the stable estimation of a unique (though biased) set of parameters.", "answer": "$$ \\boxed{ \\begin{pmatrix} -1 & 0 \\\\ -1 & -1 \\\\ 1 & 0 \\\\ 0 & -1 \\\\ 0 & 1 \\end{pmatrix} } $$", "id": "3140085"}, {"introduction": "In practice, perfect multicollinearity is rare, but nearly-collinear predictors are very common and can still severely destabilize a model's coefficient estimates. This practice moves from the qualitative problem of non-identifiability to a quantitative measure of instability: the Variance Inflation Factor (VIF) [@problem_id:3140092]. You will derive the connection between VIF and the geometry of the design matrix, learning how this crucial diagnostic quantifies the inflation in an estimator's variance due to its correlation with other predictors.", "problem": "In a linear regression used in statistical learning, consider the model $y = X\\beta + \\varepsilon$ with $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$, where $I_{n}$ is the $n \\times n$ identity matrix, $X$ is an $n \\times p$ design matrix with $p=3$ predictors, and the columns of $X$ have been mean-centered and standardized to unit variance. Suppose the sample size is $n=120$ and the Gram matrix is\n$$\nX^\\top X = \\begin{pmatrix}\n120 & 108 & 24 \\\\\n108 & 120 & 12 \\\\\n24 & 12 & 120\n\\end{pmatrix}.\n$$\nUsing only foundational definitions, begin from the Ordinary Least Squares (OLS) estimator and its sampling variance, and the definition of the Variance Inflation Factor (VIF) as the ratio of the sampling variance of an OLS coefficient under correlated predictors to the sampling variance it would have under orthogonal predictors with the same marginal scale. Derive a relationship that links the VIFs to the diagonal entries of $(X^\\top X)^{-1}$ in the standardized-predictor setting, and then compute the largest VIF for the three predictors implied by the given $X^\\top X$. Round your final answer to four significant figures. Express the final answer as a single number without units.", "solution": "The problem is first assessed for validity. All givens are extracted verbatim as required.\n- Model: $y = X\\beta + \\varepsilon$\n- Error distribution: $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$\n- Identity matrix: $I_{n}$ is the $n \\times n$ identity matrix.\n- Design matrix: $X$ is an $n \\times p$ matrix.\n- Number of predictors: $p=3$.\n- Sample size: $n=120$.\n- Standardization: Columns of $X$ are mean-centered and standardized to unit variance.\n- Gram matrix: $X^\\top X = \\begin{pmatrix} 120 & 108 & 24 \\\\ 108 & 120 & 12 \\\\ 24 & 12 & 120 \\end{pmatrix}$.\n- VIF definition: Ratio of the sampling variance of an OLS coefficient under correlated predictors to the sampling variance it would have under orthogonal predictors with the same marginal scale.\n- Task: Derive a relationship between VIFs and the diagonal entries of $(X^\\top X)^{-1}$, and then compute the largest VIF.\n- Rounding: Round final answer to four significant figures.\n\nThe problem is valid. It is a standard, well-posed problem in regression analysis, a topic within statistical learning. The provided data are internally consistent. The condition that predictors are standardized to unit variance implies the diagonal elements of the Gram matrix $X^\\top X$ are $X_j^\\top X_j$. The problem provides diagonal entries of $120$, which equals the sample size $n=120$. This corresponds to a common definition of standardization in this context where $\\frac{1}{n} \\sum_{i=1}^n X_{ij}^2 = 1$, hence $X_j^\\top X_j = \\sum_{i=1}^n X_{ij}^2 = n$. The given Gram matrix is also symmetric and positive definite, as required.\n\nThe solution proceeds in two parts as requested: first a derivation, then a computation.\n\n**Part 1: Derivation of the VIF relationship**\n\nThe Ordinary Least Squares (OLS) estimator for the coefficient vector $\\beta$ is given by:\n$$\n\\hat{\\beta} = (X^\\top X)^{-1} X^\\top y\n$$\nThe sampling variance-covariance matrix of $\\hat{\\beta}$ is derived as follows, treating the design matrix $X$ as fixed:\n$$\n\\text{Var}(\\hat{\\beta}) = \\text{Var}((X^\\top X)^{-1} X^\\top y) = (X^\\top X)^{-1} X^\\top \\text{Var}(y) ((X^\\top X)^{-1} X^\\top)^\\top\n$$\nGiven the model assumption $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$, the variance of the response vector $y$ is $\\text{Var}(y) = \\text{Var}(X\\beta + \\varepsilon) = \\text{Var}(\\varepsilon) = \\sigma^2 I_n$. Substituting this into the expression for $\\text{Var}(\\hat{\\beta})$:\n$$\n\\text{Var}(\\hat{\\beta}) = (X^\\top X)^{-1} X^\\top (\\sigma^2 I_n) X ((X^\\top X)^{-1})^\\top\n$$\nSince $X^\\top X$ is symmetric, its inverse is also symmetric. Thus, $((X^\\top X)^{-1})^\\top = (X^\\top X)^{-1}$.\n$$\n\\text{Var}(\\hat{\\beta}) = \\sigma^2 (X^\\top X)^{-1} X^\\top I_n X (X^\\top X)^{-1} = \\sigma^2 (X^\\top X)^{-1} (X^\\top X) (X^\\top X)^{-1} = \\sigma^2 (X^\\top X)^{-1}\n$$\nThe sampling variance of a single coefficient estimator, $\\hat{\\beta}_j$, is the $j$-th diagonal element of this covariance matrix:\n$$\n\\text{Var}(\\hat{\\beta}_j) = \\sigma^2 \\left( (X^\\top X)^{-1} \\right)_{jj}\n$$\nThis is the numerator in the VIF ratio.\n\nFor the denominator, we consider the hypothetical case where the predictors are orthogonal but have the same \"marginal scale\". For standardized predictors, the scale is defined by the column norms. As established, standardization implies $X_j^\\top X_j = n$. An orthogonal design matrix, which we denote $X_{ortho}$, with the same marginal scale would satisfy $(X_{ortho, j})^\\top (X_{ortho, k}) = 0$ for $j \\neq k$ and $(X_{ortho, j})^\\top (X_{ortho, j}) = n$. This leads to a diagonal Gram matrix:\n$$\nX_{ortho}^\\top X_{ortho} = \\text{diag}(n, n, \\dots, n) = n I_p\n$$\nThe inverse of this Gram matrix is:\n$$\n(X_{ortho}^\\top X_{ortho})^{-1} = (n I_p)^{-1} = \\frac{1}{n} I_p\n$$\nThe sampling variance for the $j$-th coefficient estimator in this idealized orthogonal case, denoted $\\hat{\\beta}_{j, ortho}$, would be:\n$$\n\\text{Var}(\\hat{\\beta}_{j, ortho}) = \\sigma^2 \\left( (X_{ortho}^\\top X_{ortho})^{-1} \\right)_{jj} = \\sigma^2 \\left( \\frac{1}{n} I_p \\right)_{jj} = \\frac{\\sigma^2}{n}\n$$\nThe Variance Inflation Factor for the $j$-th predictor, VIF$_j$, is defined as the ratio of these two variances:\n$$\n\\text{VIF}_j = \\frac{\\text{Var}(\\hat{\\beta}_j)}{\\text{Var}(\\hat{\\beta}_{j, ortho})} = \\frac{\\sigma^2 \\left( (X^\\top X)^{-1} \\right)_{jj}}{\\sigma^2 / n}\n$$\nThe $\\sigma^2$ terms cancel, yielding the desired relationship for standardized predictors:\n$$\n\\text{VIF}_j = n \\left( (X^\\top X)^{-1} \\right)_{jj}\n$$\n\n**Part 2: Computation of the largest VIF**\n\nWe must compute the diagonal entries of the inverse of the given Gram matrix $A = X^\\top X$:\n$$\nA = \\begin{pmatrix}\n120 & 108 & 24 \\\\\n108 & 120 & 12 \\\\\n24 & 12 & 120\n\\end{pmatrix}\n$$\nWe use the formula for the inverse of a matrix, $A^{-1} = \\frac{1}{\\det(A)}\\text{adj}(A)$, where $\\text{adj}(A)$ is the adjugate matrix (transpose of the cofactor matrix). The diagonal entries of $A^{-1}$ are given by $(A^{-1})_{jj} = \\frac{C_{jj}}{\\det(A)}$, where $C_{jj}$ is the $(j,j)$-th cofactor.\n\nFirst, we compute the determinant of $A$:\n$$\n\\det(A) = 120(120 \\cdot 120 - 12 \\cdot 12) - 108(108 \\cdot 120 - 12 \\cdot 24) + 24(108 \\cdot 12 - 120 \\cdot 24)\n$$\n$$\n\\det(A) = 120(14400 - 144) - 108(12960 - 288) + 24(1296 - 2880)\n$$\n$$\n\\det(A) = 120(14256) - 108(12672) + 24(-1584) = 1710720 - 1368576 - 38016 = 304128\n$$\nNext, we compute the diagonal cofactors, $C_{jj} = (-1)^{j+j}M_{jj} = M_{jj}$, where $M_{jj}$ is the determinant of the submatrix obtained by removing row $j$ and column $j$.\n$$\nC_{11} = \\det \\begin{pmatrix} 120 & 12 \\\\ 12 & 120 \\end{pmatrix} = 120^2 - 12^2 = 14400 - 144 = 14256\n$$\n$$\nC_{22} = \\det \\begin{pmatrix} 120 & 24 \\\\ 24 & 120 \\end{pmatrix} = 120^2 - 24^2 = 14400 - 576 = 13824\n$$\n$$\nC_{33} = \\det \\begin{pmatrix} 120 & 108 \\\\ 108 & 120 \\end{pmatrix} = 120^2 - 108^2 = 14400 - 11664 = 2736\n$$\nNow we find the diagonal elements of the inverse matrix $A^{-1}$:\n$$\n(A^{-1})_{11} = \\frac{C_{11}}{\\det(A)} = \\frac{14256}{304128}\n$$\n$$\n(A^{-1})_{22} = \\frac{C_{22}}{\\det(A)} = \\frac{13824}{304128}\n$$\n$$\n(A^{-1})_{33} = \\frac{C_{33}}{\\det(A)} = \\frac{2736}{304128}\n$$\nUsing the derived formula $\\text{VIF}_j = n \\left( (X^\\top X)^{-1} \\right)_{jj}$ with $n=120$:\n$$\n\\text{VIF}_1 = 120 \\times \\frac{14256}{304128} = \\frac{1710720}{304128}\n$$\nTo simplify, note that $A = 12 B$ where $B = \\begin{pmatrix} 10 & 9 & 2 \\\\ 9 & 10 & 1 \\\\ 2 & 1 & 10 \\end{pmatrix}$. We have $(A^{-1})_{11} = \\frac{14256}{304128} = \\frac{99 \\cdot 144}{176 \\cdot 1728} = \\frac{99 \\cdot 144}{176 \\cdot 12 \\cdot 144} = \\frac{99}{2112} = \\frac{9}{192} = \\frac{3}{64}$.\nSo, $\\text{VIF}_1 = 120 \\times \\frac{3}{64} = \\frac{30 \\times 3}{16} = \\frac{90}{16} = \\frac{45}{8} = 5.625$.\n\nFor the other VIFs:\n$(A^{-1})_{22} = \\frac{13824}{304128} = \\frac{96 \\cdot 144}{176 \\cdot 1728} = \\frac{96}{2112} = \\frac{6}{132} = \\frac{1}{22}$.\n$\\text{VIF}_2 = 120 \\times \\frac{1}{22} = \\frac{60}{11} \\approx 5.4545...$\n\n$(A^{-1})_{33} = \\frac{2736}{304128} = \\frac{19 \\cdot 144}{176 \\cdot 1728} = \\frac{19}{2112}$.\n$\\text{VIF}_3 = 120 \\times \\frac{19}{2112} = \\frac{10 \\times 19}{176} = \\frac{190}{176} = \\frac{95}{88} \\approx 1.0795...$\n\nComparing the three values:\n$\\text{VIF}_1 = 5.625$\n$\\text{VIF}_2 = 5.\\overline{45}$\n$\\text{VIF}_3 \\approx 1.0795$\nThe largest VIF is $\\text{VIF}_1 = 5.625$. The problem requests the answer rounded to four significant figures. The number $5.625$ already has four significant figures.", "answer": "$$\n\\boxed{5.625}\n$$", "id": "3140092"}, {"introduction": "Linear dependence is not just an abstract mathematical concern; it often arises directly from how we construct features for our models, a process known as feature engineering. This coding exercise simulates a common scenario in statistical learning: encoding categorical variables and their interactions into a design matrix [@problem_id:3140137]. By building matrices with both redundant and well-posed encodings, you will gain practical experience in identifying and preventing structural rank deficiency, a critical skill for building reliable regression models.", "problem": "Consider a regression design matrix in statistical learning that includes categorical encodings, polynomial terms, and interactions. Use the following foundational definitions as the basis for your reasoning and construction: the columns of a matrix $X$ are linearly independent if the only solution to $X\\alpha = 0$ is $\\alpha = 0$; the column rank $\\operatorname{rank}(X)$ is the dimension of the column space of $X$; the identity matrix $I_p$ is the $p \\times p$ matrix with ones on the diagonal and zeros elsewhere; in one-hot encoding of a categorical variable, exactly one indicator is $1$ per observation, and $0$ otherwise; for any dummy variable $d \\in \\{0,1\\}$ and any integer $m \\ge 1$, the polynomial equality $d^m = d$ holds. Construct the design matrices $X$ using only these base facts without assuming any additional shortcut formulas.\n\nYou are given $n = 12$ observations with two categorical variables $C$ and $D$. The variable $C$ has $3$ levels labeled $\\{0,1,2\\}$, and the variable $D$ has $2$ levels labeled $\\{0,1\\}$. The observation sequence covers each pair $(C,D)$ twice in the following order:\n$[(0,0),(0,1),(1,0),(1,1),(2,0),(2,1),(0,0),(0,1),(1,0),(1,1),(2,0),(2,1)]$.\nFrom these, construct one-hot indicator columns for $C$ (denote them $C0$, $C1$, $C2$) and for $D$ (denote them $D0$, $D1$). For interactions, denote the cell indicators $Jcd$ for the pair $(C=c,D=d)$ in lexicographic order by $J00$, $J01$, $J10$, $J11$, $J20$, $J21$. Denote the intercept column by $I$ (the column vector of ones).\n\nYou must write a complete program that builds the following three test-case design matrices $X$ exactly in the specified column order, computes the column rank $\\operatorname{rank}(X)$ from first principles, and identifies structural rank deficiency due to redundant coding. A column is structurally redundant if it is a linear combination of other columns in $X$. Report the zero-based indices of all structurally redundant columns, defined as those not belonging to a maximal set of linearly independent columns.\n\nTest Suite:\n- Test Case $1$ (happy path with intentional redundancy via full coding, interactions, and polynomial duplication):\n  Construct $X$ with columns in this exact order:\n  $[I, C0, C1, C2, D0, D1, J00, J01, J10, J11, J20, J21, C0^2, C1^2, C2^2, J00^2, J01^2, J10^2, J11^2, J20^2, J21^2]$.\n  Here $Ck^2$ and $Jcd^2$ denote elementwise squares of the corresponding dummy columns.\n- Test Case $2$ (boundary case with reference-level coding to avoid redundancy):\n  Construct $X$ with no intercept and reference-level coding for main effects. Keep $C0$ and $C1$ (drop $C2$ as the reference) and keep $D0$ (drop $D1$ as the reference). Include only interactions formed by products of the retained main-effect dummies. The exact column order is:\n  $[C0, C1, D0, C0 \\cdot D0, C1 \\cdot D0]$.\n- Test Case $3$ (edge case with explicit duplicates and a zero column):\n  Construct $X$ with columns in this exact order:\n  $[I, C0, C1, C2, D0, D1, I_{\\text{dup}}, C1_{\\text{dup}}, Z]$,\n  where $I_{\\text{dup}}$ is a direct duplicate of $I$, $C1_{\\text{dup}}$ is a direct duplicate of $C1$, and $Z$ is the elementwise difference $D1 - D1^2$.\n\nAlgorithmic Requirements:\n- Compute $\\operatorname{rank}(X)$ using a numerically robust linear-algebra procedure justified from the base definitions (for example, a column-pivoted factorization that selects a maximal set of independent columns).\n- Identify a maximal set of linearly independent columns and report the zero-based indices of all remaining columns as structurally redundant.\n\nFinal Output Format:\nYour program should produce a single line of output containing the aggregated results for the three test cases as a list of lists, where each inner list has the form $[p, r, \\text{dep}]$ with $p$ the number of columns of $X$ (an integer), $r$ the computed column rank (an integer), and $\\text{dep}$ the list of zero-based indices of structurally redundant columns. For example, the printed output must look like $[[p_1,r_1,[\\dots]],[p_2,r_2,[\\dots]],[p_3,r_3,[\\dots]]]$ with no extra text.\n\nNo physical units or angle units are involved in this task. All numerical values should be reported as integers or lists of integers. Ensure scientific realism by constructing $X$ exactly from the specified one-hot encodings, interactions, and elementwise polynomials as described, and by computing rank and redundancy strictly via linear algebra.", "solution": "The problem requires the construction and analysis of three design matrices, $X$, derived from categorical and polynomial features. The core tasks are to determine the column rank of each matrix and to identify all structurally redundant columns. This analysis must be based on first principles of linear algebra.\n\n### Foundational Principles\n\n1.  **Linear Independence and Column Rank**: The columns of a matrix $X \\in \\mathbb{R}^{n \\times p}$, denoted $x_1, \\dots, x_p$, are linearly independent if the only solution to the vector equation $\\sum_{j=1}^{p} \\alpha_j x_j = 0$ (or $X\\alpha = 0$) is the trivial solution $\\alpha_1 = \\dots = \\alpha_p = 0$. The column rank of $X$, denoted $\\operatorname{rank}(X)$, is the size of the largest possible subset of columns of $X$ that are linearly independent. This is equivalent to the dimension of the column space of $X$, which is the vector space spanned by its columns.\n\n2.  **Design Matrix Construction**: The problem provides $n=12$ observations of two categorical variables, $C \\in \\{0, 1, 2\\}$ and $D \\in \\{0, 1\\}$. The design matrices are constructed from these variables.\n    - **Intercept**: The intercept column, $I$, is a vector of ones, representing a constant offset in a regression model.\n    - **One-Hot Encoding**: For a categorical variable with $k$ levels, $k$ binary indicator (dummy) columns are created. For each observation, the column corresponding to the observed level is set to $1$, and all other $k-1$ indicator columns are set to $0$. For variable $C$, this yields columns $C0, C1, C2$. For variable $D$, this yields $D0, D1$.\n    - **Interactions**: Interaction terms capture effects that are specific to combinations of levels of different variables. The interaction column for the cell $(C=c, D=d)$, denoted $Jcd$, is $1$ for observations where $C=c$ and $D=d$, and $0$ otherwise. This is equivalent to the elementwise product of the corresponding main effect indicators: $Jcd = Cc \\cdot Dd$.\n    - **Polynomial Terms**: For a dummy variable $d \\in \\{0, 1\\}$, any integer power $d^m$ for $m \\ge 1$ is equal to $d$ itself, since $0^m = 0$ and $1^m = 1$. This property, $d^m=d$, is a key insight for identifying certain redundancies.\n\n### Algorithmic Method for Rank and Redundancy Detection\n\nTo determine the rank and identify redundant columns, a numerically stable method is required. Column-pivoted QR factorization is an excellent choice. For a matrix $X \\in \\mathbb{R}^{n \\times p}$, this factorization computes:\n$$\nXP = QR\n$$\nwhere $P$ is a $p \\times p$ permutation matrix that reorders the columns of $X$, $Q$ is an $n \\times n$ orthogonal matrix, and $R$ is a $p \\times p$ upper trapezoidal matrix whose diagonal elements $|R_{11}| \\ge |R_{22}| \\ge \\dots \\ge |R_{pp}|$ are non-increasing in magnitude.\n\nThe rank $r$ of $X$ is the number of non-zero diagonal entries in $R$. Numerically, an entry $R_{kk}$ is considered non-zero if its magnitude is greater than a small tolerance, $\\tau$. A robust choice for $\\tau$ is relative to the largest diagonal element and the matrix dimensions, e.g., $\\tau = \\max(n, p) \\cdot |R_{11}| \\cdot \\epsilon$, where $\\epsilon$ is machine epsilon.\n\nThe permutation matrix $P$ (represented as a vector of pivot indices) effectively selects a maximally linearly independent set of columns from $X$. The first $r$ indices in the pivot vector $P$ correspond to the columns of $X$ that form a basis for the column space. The remaining $p-r$ indices in $P$ correspond to the columns that are linearly dependent on the chosen basis columns. These are the structurally redundant columns.\n\n### Analysis of Test Cases\n\nFirst, we construct the base column vectors from the given observation sequence. Let $n=12$.\nThe sequence of $(C, D)$ pairs is $[(0,0), (0,1), (1,0), (1,1), (2,0), (2,1), (0,0), (0,1), (1,0), (1,1), (2,0), (2,1)]$.\nThis defines the following columns (as $12 \\times 1$ vectors):\n- $I = [1,1,1,1,1,1,1,1,1,1,1,1]^T$\n- $C0 = [1,1,0,0,0,0,1,1,0,0,0,0]^T$\n- $C1 = [0,0,1,1,0,0,0,0,1,1,0,0]^T$\n- $C2 = [0,0,0,0,1,1,0,0,0,0,1,1]^T$\n- $D0 = [1,0,1,0,1,0,1,0,1,0,1,0]^T$\n- $D1 = [0,1,0,1,0,1,0,1,0,1,0,1]^T$\n- The interaction columns $Jcd = Cc \\cdot Dd$ are indicators for the $3 \\times 2 = 6$ unique cells.\n\n**Test Case 1**:\n$X_1$ has $p_1=21$ columns: $[I, C0, C1, C2, D0, D1, J00, \\dots, J21, C0^2, \\dots, C2^2, J00^2, \\dots, J21^2]$.\nThis matrix has several forms of structural redundancy:\n1.  **Full dummy set redundancy**: The one-hot columns for a variable sum to the intercept column. Thus, $C0+C1+C2=I$ and $D0+D1=I$. This introduces linear dependencies. For instance, $C2 = I - C0 - C1$.\n2.  **Interaction redundancy**: The main effect columns are sums of interaction columns. For example, $C0 = J00 + J01$ and $D0 = J00 + J10 + J20$.\n3.  **Polynomial redundancy**: Since all indicator columns contain only $0$s and $1$s, squaring them results in the identical column, e.g., $C0^2 = C0$. Columns $12$ through $20$ are exact duplicates of earlier columns.\nThe fundamental dimension of the space spanned by all these columns is the number of unique $(C,D)$ pairs, which is $6$. The set of interaction columns $\\{J00, \\dots, J21\\}$ forms an orthogonal basis for this $6$-dimensional space. Therefore, the rank must be $r_1=6$. A greedy column selection procedure (like QR with pivoting) progressing from left to right would select the first $6$ linearly independent columns it encounters. A likely basis would be $\\{I, C0, C1, D0, J00, J10\\}$. All other $21-6=15$ columns are redundant.\n\n**Test Case 2**:\n$X_2$ has $p_2=5$ columns: $[C0, C1, D0, C0 \\cdot D0, C1 \\cdot D0]$.\nThis matrix corresponds to a reference-level coding scheme (often called dummy coding) without an intercept. $C2$ and $D1$ are implicitly the reference levels. This construction is designed to avoid collinearity. The columns are $\\{C0, C1, D0, J00, J10\\}$. We can verify their linear independence by setting their linear combination to zero, $a_1 C0 + a_2 C1 + a_3 D0 + a_4 J00 + a_5 J10 = 0$, and showing all coefficients must be zero by evaluating the equation at different observation types. For example, at an observation where $(C,D)=(0,1)$, only $C0$ is non-zero, forcing $a_1=0$. Proceeding this way confirms all coefficients must be zero. Thus, the columns are linearly independent. The rank will be equal to the number of columns, so $r_2=5$, and there are no redundant columns.\n\n**Test Case 3**:\n$X_3$ has $p_3=9$ columns: $[I, C0, C1, C2, D0, D1, I_{\\text{dup}}, C1_{\\text{dup}}, Z]$.\nThis case introduces explicit redundancies and a zero column.\n1.  **Full dummy set redundancy**: As in Case 1, we have $C2 = I - C0 - C1$ and $D1 = I - D0$.\n2.  **Explicit duplication**: $I_{\\text{dup}}$ is identical to $I$ (column 0), and $C1_{\\text{dup}}$ is identical to $C1$ (column 2).\n3.  **Zero column**: The column $Z = D1 - D1^2$ is a zero vector because for the dummy variable $D1$, $D1^2=D1$. A zero column is always linearly dependent unless it is the only column in the matrix.\nThe rank is determined by the number of linearly independent columns among $\\{I, C0, C1, C2, D0, D1\\}$. Based on the dependencies, a basis for this set is $\\{I, C0, C1, D0\\}$. This set has $4$ vectors, so the rank is $r_3=4$. All other columns ($C2$, $D1$, $I_{\\text{dup}}$, $C1_{\\text{dup}}$, $Z$) are redundant. This results in $9-4=5$ redundant columns.", "answer": "```python\nimport numpy as np\nfrom scipy import linalg\n\ndef solve():\n    \"\"\"\n    Solves the problem by constructing three design matrices, computing their\n    ranks, and identifying redundant columns using column-pivoted QR factorization.\n    \"\"\"\n    \n    # Define the observation data for C and D variables for n=12 observations.\n    obs = np.array([\n        (0, 0), (0, 1), (1, 0), (1, 1), (2, 0), (2, 1),\n        (0, 0), (0, 1), (1, 0), (1, 1), (2, 0), (2, 1)\n    ])\n    c_obs = obs[:, 0]\n    d_obs = obs[:, 1]\n    n_obs = len(c_obs)\n\n    # Construct all necessary base column vectors as (n, 1) numpy arrays.\n    I = np.ones((n_obs, 1))\n    C0 = (c_obs == 0).astype(int).reshape(-1, 1)\n    C1 = (c_obs == 1).astype(int).reshape(-1, 1)\n    C2 = (c_obs == 2).astype(int).reshape(-1, 1)\n    D0 = (d_obs == 0).astype(int).reshape(-1, 1)\n    D1 = (d_obs == 1).astype(int).reshape(-1, 1)\n    \n    # Interaction columns Jcd = Cc * Dd\n    J00 = C0 * D0\n    J01 = C0 * D1\n    J10 = C1 * D0\n    J11 = C1 * D1\n    J20 = C2 * D0\n    J21 = C2 * D1\n\n    # Dictionary of all base columns for easy access\n    columns = {\n        'I': I, 'C0': C0, 'C1': C1, 'C2': C2, 'D0': D0, 'D1': D1,\n        'J00': J00, 'J01': J01, 'J10': J10, 'J11': J11, 'J20': J20, 'J21': J21,\n        'C0^2': C0**2, 'C1^2': C1**2, 'C2^2': C2**2,\n        'J00^2': J00**2, 'J01^2': J01**2, 'J10^2': J10**2, 'J11^2': J11**2,\n        'J20^2': J20**2, 'J21^2': J21**2,\n        'C0*D0': C0 * D0,\n        'C1*D0': C1 * D0,\n        'I_dup': I,\n        'C1_dup': C1,\n        'Z': D1 - D1**2\n    }\n\n    # Define the column sets for each test case\n    test_case_defs = {\n        1: ['I', 'C0', 'C1', 'C2', 'D0', 'D1', 'J00', 'J01', 'J10', 'J11', \n            'J20', 'J21', 'C0^2', 'C1^2', 'C2^2', 'J00^2', 'J01^2', 'J10^2', \n            'J11^2', 'J20^2', 'J21^2'],\n        2: ['C0', 'C1', 'D0', 'C0*D0', 'C1*D0'],\n        3: ['I', 'C0', 'C1', 'C2', 'D0', 'D1', 'I_dup', 'C1_dup', 'Z']\n    }\n\n    test_cases = []\n    for i in sorted(test_case_defs.keys()):\n        col_names = test_case_defs[i]\n        X = np.hstack([columns[name] for name in col_names])\n        test_cases.append(X)\n        \n    def analyze_matrix(X):\n        \"\"\"\n        Computes rank and redundant columns for a matrix X.\n        \n        Uses column-pivoted QR factorization. The rank is the number of\n        diagonal elements of R with magnitude above a tolerance. Redundant\n        columns are those not selected as pivots for the basis.\n        \"\"\"\n        n, p = X.shape\n        if p == 0:\n            return 0, 0, []\n        \n        Q, R, P = linalg.qr(X, pivoting=True)\n        \n        # Determine rank using a relative tolerance\n        diag_R = np.abs(np.diag(R))\n        tol = diag_R[0] * max(n, p) * np.finfo(R.dtype).eps if p > 0 else 0\n        \n        rank = np.sum(diag_R > tol)\n        \n        # Pivots P[:rank] are independent, P[rank:] are redundant\n        redundant_indices = sorted(P[rank:])\n        \n        return p, rank, redundant_indices\n\n    results = []\n    for X in test_cases:\n        p, r, dep = analyze_matrix(X)\n        results.append([p, r, dep])\n\n    # The string representation of list of lists is required.\n    # The template's suggestion `f\"[{','.join(map(str, results))}]\"`\n    # creates the desired format.\n    print(f\"[{','.join(map(str, results))}]\".replace(\" \", \"\"))\n\nsolve()\n```", "id": "3140137"}]}